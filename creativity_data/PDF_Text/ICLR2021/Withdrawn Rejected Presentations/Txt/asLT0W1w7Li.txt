Under review as a conference paper at ICLR 2021
Efficient Exploration for Model-based Rein-
forcement Learning with Continuous States
and Actions
Anonymous authors
Paper under double-blind review
Ab stract
Balancing exploration and exploitation is crucial in reinforcement learning (RL). In
this paper, we study the model-based posterior sampling algorithm in continuous
state-action spaces theoretically and empirically. First, we improve the regret
bound: with the assumption that reward and transition functions can be modeled
as Gaussian Processes with linear kernels, We develop a Bayesian regret bound
of O(H3/2dVT), where H is the episode length, d is the dimension of the state-
action space, and T indicates the total time steps. Our bound can be extended to
nonlinear cases as well: using linear kernels on the feature representation φ, the
Bayesian regret bound becomes O(H3/2dg√T), where dφ is the dimension of the
representation space. Moreover, we present MPC-PSRL, a model-based posterior
sampling algorithm with model predictive control for action selection. To capture
the uncertainty in models and realize posterior sampling, we use Bayesian linear
regression on the penultimate layer (the feature representation layer φ) of neural
networks. Empirical results show that our algorithm achieves the best sample
efficiency in benchmark control tasks compared to prior model-based algorithms,
and matches the asymptotic performance of model-free algorithms.
1	Introduction
In reinforcement learning (RL), an agent interacts with an unknown environment which is typically
modeled as a Markov Decision Process (MDP). Efficient exploration has been one of the main
challenges in RL: the agent is expected to balance between exploring unseen state-action pairs to
gain more knowledge about the environment, and exploiting existing knowledge to optimize rewards
in the presence of known data.
To achieve efficient exploration, Bayesian reinforcement learning is proposed, where the MDP itself
is treated as a random variable with a prior distribution. This prior distribution of the MDP provides
an initial uncertainty estimate of the environment, which generally contains distributions of transition
dynamics and reward functions. The epistemic uncertainty (subjective uncertainty due to limited
data) in reinforcement learning can be captured by posterior distributions given the data collected by
the agent.
Posterior sampling reinforcement learning (PSRL), motivated by Thompson sampling in bandit
problems (Thompson, 1933), serves as a provably efficient algorithm under Bayesian settings. In
PSRL, the agent maintains a posterior distribution for the MDP and follows an optimal policy with
respect to a single MDP sampled from the posterior distribution for interaction in each episode.
Appealing results of PSRL in tabular RL were presented by both model-based (Osband et al., 2013;
Osband & Van Roy, 2017) and model free approaches (Osband et al., 2019) in terms of the Bayesian
regret. For H-horizon episodic RL, PSRL was proved to achieve a regret bound of O(H，SAT),
where S and A denote the number of states and actions, respectively. However, in continuous
state-action spaces S and Acan be infinite, hence the above results do not apply.
Although PSRL in continuous spaces has also been studied in episodic RL, existing results either
provide no guarantee or suffer from an exponential order of H. In this paper, we achieve the first
Bayesian regret bound for posterior sampling algorithms that is near optimal in T (i.e. TT) and
1
Under review as a conference paper at ICLR 2021
polynomial in the episode length H for continuous state-action spaces. We will explain the limitations
of previous works in Section 1.1, then summarize our approach and contributions in Section 1.2.
1.1	Limitations of previous Bayesian regrets in continuous spaces
The exponetial order of H: In model-based settings, Osband & Van Roy (2014) derive a regret bound
of O(σR/dκ(R)“e(R)T + E[L*]σ?PdK(P)dχ(P)), where L* is a global LiPschitz constant for
the future value function defined in their eq. (3). However, L* is dependent on H: the difference
between inPut states will ProPagate in H stePs, which results in a term dePendent of H in the value
function. The authors do not mention this dependency, so there is no clear dependency on H in
their regret. Moreover, they use the Lipschitz constant of the underlying value function as an upper
bound of L* in the corollaries, which yields an exponential order in H. Take their Corollary 2 of
linear quadratic systems as an example: the regret bound is (J(σCλιn1 2 √T), where λι is the largest
eigenvalue of the matrix Q in the optimal value function V1 (s) = sT Qs. 1 However, the largest
eigenvalue of Q is actually exponential in H 2. Even if we change the reward function from quadratic
to linear,the Lipschitz constant of the optimal value function is still exponential in H 3. Chowdhury
& Gopalan (2019) maintains the assumption of this Lipschitz property, thus there exists E[L*] with
no clear dependency on H in their regret, and in their Corollary 2 of LQR, they follow the same steps
as Osband & Van Roy (2014), and still maintain a term with λ1 , which is actually exponential in
H as discussed. Although Osband & Van Roy (2014) mentions that system noise helps to smooth
future values, but they do not explore it although the noise is assumed to be subgaussian. The authors
directly use the Lipschitz continuity of the underlying function in the analysis of LQR, thus they
cannot avoid the exponential term in H. Chowdhury & Gopalan (2019) do not explore how the
system noise can improve the theoretical bound either. In model-free settings, Azizzadenesheli et al.
(2018) develops a regret bound of O(dφ√T) using a linear function approximator in the Q-network,
where dφ is the dimension of the feature representation vector of the state-action space, but their
bound is still exponential in H as mentioned in their paper.
High dimensionality: The eluder dimension of neural networks in Osband & Van Roy (2014) can
be infinite, and the information gain (Srinivas et al., 2012) used in Chowdhury & Gopalan (2019)
yields exponential order of the state-action spaces dimension d if nonlinear kernels are used, such as
SE kernels. However, linear kernels can only model linear functions, thus the representation power is
highly restricted if the polynomial order of d is desired.
1.2	Our Approach and Main Contributions
To further imporve the regret bound for PSRL in continuous spaces, especially with explicit de-
pendency on H, we study model-based posterior sampling algorithms in episodic RL. We assume
that rewards and transitions can be modeled as Gaussian Processes with linear kernels, and extend
the assumption to non-linear settings utilizing features extracted by neural networks. For the linear
case, we develop a Bayesian regret bound of O(H3/2d√T). Using feature embedding technique as
mentioned in Yang & Wang (2019), we derive a bound of O(H3/2dg√T). Our Bayesian regret is the
best-known Bayesian regret for posterior sampling algorithms in continuous state-action spaces, and
it also matches the best-known frequentist regret (Zanette et al. (2020), will be discussed in Section
2). Explicitly dependent on d, H, T, our result achieves a significant improvement in terms of the
Bayesian regret of PSRL algorithms compared to previous works:
1.	We significantly improved the order of H to polynomial: In our analysis, we use the property
of subgaussian noise, which is already assumed in Osband & Van Roy (2014) and Chowdhury &
Gopalan (2019), to develop a bound with clear polynomial dependency on H, without assuming the
Lipschitz continuity of the underlying value function. More specifically, we prove Lemma 1, and use
1 V1 denotes the value function counting from step 1 to H within an episode, s is the initial state, reward at the
i-th step ri = siTPsi + aiT Rai + P,i, and the state at the i + 1-th step si+1 = Asi + Bai + P,i , i ∈ [H].
2Recall the Bellman equation we have Vi(si) = minai siTPsi + aiT Rai + P,i + Vi+1 (Asi + Bai + P,i),
VH+1 (s) = 0 . Thus in V1(s), there is a term of (AH-1s)TP (AH-1s), and the eigenvalue of the matrix
(AH-1)TPAH-1 isexponentialinH.
3For example, if ri = siTP + aiTR + P,i, there would still exist term of (AH-1s)T P in V1 (s).
2
Under review as a conference paper at ICLR 2021
it to develop a clear dependency on H, thus we can avoid handling the Lipschitz continuity of the
underlying value function.
2.	Lower dimensionality compared to Osband & Van Roy (2014) and Chowdhury & Gopalan
(2019): We first derive results for linear kernels, and increase the representation power of the linear
model by building a Bayesian linear regression model on the feature representation space instead
of the original state-action space. As a result, we can use the result of linear kernels to derive a
bound linear in the feature dimension. The feature dimension, which in practice is dimension of the
last hidden layers in the neural networks required for learning, is much lower than exponential of
the input dimension, so we avoid the exponential order of the dimension from the use of nonlinear
kernels in Chowdhury & Gopalan (2019).
3.	Fewer assumptions and different proof strategy compared to Chowdhury & Gopalan (2019):
Although we also use kernelized MDPs like Chowdhury & Gopalan (2019), we omit their assumption
A1 (Lipschitz assumption) and A2 (Regularity assumption), only use A3 (subgaussian noise). We
avoid A1 since it could be derived from our Lemma 1. Moreover, We directly analyze the regret
bound of PSRL using the fact that the sampled and the real unknown MDP share the same distribution
conditioned on history. In contrast, Chowdhury & Gopalan (2019) first analyze UCRL (Upper
confidence bound in RL) with an extra assumption A2, then transfer it to PSRL.
Empirically, we implement PSRL using Bayesian linear regression (BLR) on the penultimate layer
(for feature representation) of neural networks when fitting transition and reward models. We use
model predictive control (MPC,Camacho & Alba (2013)) to optimize the policy under the sampled
models in each episode as an approximate solution of the sampled MDP as described in Section 5.
Experiments show that our algorithm achieves more efficient exploration compared with previous
model-based algorithms in control benchmark tasks.
2	Related Work on Frequenist Regrets
Besides the aforementioned works on Bayesian regret bounds, the majority of papers in efficient RL
choose the non-Bayesian perspective and develop frequentist regret bounds where the regret for any
MDP M* ∈ M is bounded and M* ∈ M holds with high probability. frequentist regret bounds can
be expressed in the Bayesian view: for a given confidence set M, the frequentist regret bound implies
an identical Bayes regret bound for any prior distribution with support on M. Note that frequentist
regret is extensively studied in tabular RL (see Jaksch et al. (2010), Azar et al. (2017), and Jin et al.
(2018) as examples), among which the best bound for episodic settings is O(H,SAT).
There is also a line of work that develops frequentist bounds with feature representation. Most
recently, MatrixRL proposed by (Yang & Wang, 2019) uses low dimensional representation and
achieves a regret bound of O(H2dφ√T), which is the best-known frequentist bound in model based
settings. While our method is also model-based, we achieve a tighter regret bound when compared in
the Bayesian view. In model-free settings, Jin et al. (2020) developed a bound of O(H 3/2^^/2 √T).
Zanette et al. (2020) further improved the regret to O(H3/2d/√T) by the proposed an algorithm
called ELEANOR, which achieves the best-known frequentist bound in model-free settings. They
showed that it is unimprovable with the help of a lower bound established in the bandit literature.
Despite that our regret is developed in model-based settings, it matches their bound with the same
order of H, dφ and T in the Bayesian view. Moreover, their algorithm involves optimization
over all MDPs in the confidence set, and thus can be computationally prohibitive. Our method is
computationally tractable as it is much easier to optimize a single sampled MDP, while matching
their regret bound in the Bayesian view.
3	Preliminaries
3.1	Problem Formulation
We model an episodic finite-horizon Markov Decision Process (MDP) M as
{S, A, RM, PM , H, σr, σf , Rmax , ρ}, where S ⊂ Rds and A ⊂ Rda denote state and ac-
tion spaces, respectively. Each episode with length H has an initial state distribution ρ. At time
step i ∈ [1, H] within an episode, the agent observes si ∈ S, selects ai ∈ A, receives a noised
3
Under review as a conference paper at ICLR 2021
reward r 〜RM(si, ai) and transitions to a noised new state si+ι 〜PM(s%, aQ. More specifically,
r(si, a) = rM(si, ai) + 金 and Si+ι = f M(si, a/ + Ef, where 金〜N(0,σ;2), Ef 〜N(0, σf Ids).
Variances σr2 and σf2 are fixed to control the noise level. Without loss of generality, we assume the
expected reward an agent receives at a single step is bounded 尸M (s, a)| ≤ Rmax, ∀s ∈ S, a ∈ A.Let
μ: S → A be a deterministic policy. Here we define the value function for state S at time step i with
policy μ as VMi (S) = E[∑H=i[rM(sj, aj)∣Si = s], where Sj+ι 〜PM(sj, a,j) and a,j = μ(sj). With
the bound expected reward, we have that |V (s)| ≤ HRmax, ∀s.
We use M* to indicate the real unknown MDP which includes R* and P*, and M* itself is treated
as a random variable. Thus, we can treat the real noiseless reward function r* and transition
function f* as random processes as well. In the posterior sampling algorithm πPS, Mk is a random
sample from the posterior distribution of the real unknown MDP M * in the kth episode, which
includes the posterior samples of Rk and Pk , given history prior to the kth episode: Hk :=
{s1,1,a1,1,r1,1,…，Sk-ι,H,aι,H,rk-ι,H}, where Sk,i,a%i and rk,i indicate the state, action,
and reward at time step i in episode k. We define the the optimal policy under M as μM ∈argmaχμ
VM(s) for all S ∈ S and i ∈ [H]. In particular, μ* indicates the optimal policy under M* and μk
represents the optimal policy under Mk. Let ∆k denote the regret over the kth episode:
∆k = /P(sι)(¾(sι) - VM,1(sι))dsι	(1)
Then we can express the regret of πps up to time step T as:
Regret(T,∏s, M*) := ∑kT]△的,	⑵
Let BayesRegret(T, πps, φ) denote the Beyesian regret of πps as defined in Osband & Van Roy
(2017), where φ is the prior distribution of M*:
BayesRegret(T, πps, φ) = E[Reg r et(T, πps, M* )].	(3)
3.2	Assumptions
Generally, we consider modeling an unknown target function g : Rd → R. We are given a set of noisy
samples y = [y1., yT]T at points X = [x1, ..., xT]T, X ⊂ D, where D is compact and convex,
yi = g(xi) + Ei with Ei 〜N(0, σ2) i.i.d. Gaussian noise ∀i ∈ {1,…，T}.
We model g as a sample from a Gaussian Process GP(μ(χ), K(χ, χ0)), specified by the mean function
μ(x) = E[g(x)] and the covariance (kernel) function K(x, x0) = E[(g(x) - μ(x)(g(x0) - μ(x0)].
Let the prior distribution without any data as GP (0, K(x, x0)). Then the posterior distribu-
tion over g given X and y is also a Gp with mean μτ(x), covariance KT(χ,χ0), and variance
σT(x): μτ(x) = K(x,X)(K(X,X) + σ2I)-1y, KT(X,x0) = K(x,x0) -K(X, x)T(K(X, X) +
σ2I)-1K(X, x), σT2 (x) = KT (x, x), where K(X, x) = [K(x1, x), ..., K(xT, x)]T, K(X,X) =
[K(xi, xj )]1≤i≤T,1≤j≤T.
We model our reward function rM as a Gaussian Process with noise σJ2. For transition models, we
treat each dimension independently: each fi(s, a), i = 1, .., dS is modeled independently as above,
and with the same noise level σf2 in each dimension. Thus it corresponds to our formulation in the
RL setting. Since the posterior covariance matrix is only dependent on the input rather than the target
value, the distribution of each fi (s, a) shares the same covariance matrice and only differs in the
mean function.
4	Bayesian Regret Analysis
4.1	Linear case
Theorem 1 In the RL Problemformulated in Section 3.1, under the assumption ofSection 3.2 with
linear kernels4, we have BayeSRegret(T, πps,M*) = O(H3/2dVT), where d is the dimension of
the state-action space, H is the episode length, and T is the time elapsed.
4GP with linear kernel correspond to Bayesian linear regression f(x) = wTx, where the prior distribution
of the weight is W 〜N(0, Σp).
4
Under review as a conference paper at ICLR 2021
Proof The regret in episode k can be rearranged as:
∆k = Z ρ(sι)(VM (Si) - VM,1(sι)) + (VMk1(sι) - VM1(sι)))dsι
(4)
Note that conditioned upon history Hk for any k, M k and M* are identically distributed. Osband &
Van Roy (2014) showed that VfM^ - VM ι is zero in expectation, and that only the second part of the
regret decomposition need to be bounded when deriving the Bayesian regret of PSRL. Thus we can
focus on the policy μk, the sampled Mk and real environment data generated by M*. For clarity, the
value function VMM 1 is simplified to Vk 1 and VM； to Vkc 1. It suffices to derive bounds for any initial
state S1 as the regret bound will still hold through integration of the initial distribution ρ(S1).
We can rewrite the regret from concentration via the Bellman operator (see Section 5.1 in Osband
et al. (2013)):
E[∆kHk] ：= E[Vk1(s1)- Vk*,1(s1)∣Hk]
=E[rk (s1,a1) - r*(sι ,aι) + / Pk (SlSI ,aι )v^)ds0 - / P * (s0, ∣s1,a1)V*,2(s0)ds0∣Hk]
=E[∑H=Irk(si, ai) - r* (si, ai) + ∑H=ι(∕(Pk(s0∣Si, ai) - P*(s0∣Si, ai))V⅛+ι(s0)ds0)∣Hk]
〜
〜
E[∆ k (r) + ∆ k (f )|Hk ]
(5)
where ai = μk (Si),si+1 〜P*(si+1 lsi,ai), ∆k(r) = ^^i=ir^(si, ai) - r*(si,ai), ∆k(f) =
ΣiH=1( (Pk(S0ISi, ai) -P*(S0ISi,ai))Vkk,i+1(S0)dS0). Thus, here (Si, ai) is the state-action pair that
the agent encounters in the kth episode while using μk for interaction in the real MDP M*. We
can define Vk,H +1 = 0 to keep consistency. Note that we cannot treat Si and ai as determin-
istic and only take the expectation directly on random reward and transition functions. Instead,
we need to bound the difference using concentration properties of reward and transition func-
tions modeled as Gaussian Processes (which also applies to any state-action pair), and then derive
bounds of this expectation. For all i, we have R (Pk(S0ISi, ai) - P* (S0ISi, ai))Vkk,i+1(S0)dS0 ≤
maxs IVkk,i+1(S)I R IPk(S0ISi, ai)-P*(S0ISi, ai)IdS0 ≤ H Rmax R IPk(S0ISi, ai)-P*(S0ISi, ai)IdS0.
Now we present a lemma which enables us to derive a regret bound with explicit dependency on the
episode length H .
Lemma 1 For two multivariate Gaussian distribution N(μ,σ2I), N(μ0,σ21) with probability
density function p1(x) and p2(x) respectively, x ∈ Rd ,
/ IpI(X) - P2(X)Idx ≤ ∏ ∏2σ2 ”μ -从0||2.
The proof is in Appendix A.1. Clearly, this result can also be extended to sub-Gaussian noises.
Recall that Pk(s0|si, ai) = N(fk(si, ai), σf2I) and P*(s0|si, ai) = N(f* (si, ai), σf2I). By Lemma
1 we have
Z IP k (s0lsi, ai) - P *(s0lsi, ai)|ds0 ≤ ∏ ∏σ2 ||fk (Si, ai) - f * (Si,ai)g
(6)
Lemma 2 (Rigollet & Hutter, 2015) Let X∖,…,Xn be N Sub-Gaussian random variables with
variance σ2 (not required to be independent). Then for any t > 0, P(max1≤i≤N IXiI > t) ≤
t2
2Ne-2σ2.
Given history Hk, let fk (s, a) indicate the posterior mean of fk(s, a) in episode k, and σk (s, a)
denotes the posterior variance of fk in each dimension. Note that f * and fk share the same variance
in each dimension given history Hk, as described in Section 3. Consider all dimensions of the
state space, by Lemma 2, we have that with probability at least 1 - δ, max1≤i≤ds |fik(s, a) -
5
Under review as a conference paper at ICLR 2021
fk Ga)I ≤ y2σ2(s,a)∕og 2δds
. Also, we can derive an upper bound for the norm of the state
difference ∣∣fk (s,a)- fk (s,a)∣∣2 ≤ √dS maxι≤i≤ds ∣fk (s,a)- fk (s,a)∣,and so does ∣∣f *(s,a)-
fk(s, a)∣∣2 since f * and fk share the same posterior distribution. By the union bound, We have that
with probability at least 1 - 2δ ∣∣fk(s, a) - f *(s, a)∣∣2 ≤ 2
y 2dsσ2(s, a)l0g2δds.
Then we look at the sum of the differences over horizon H, without requiring each variable in the
sum to be independent:
P3H=i||fk (Si,a∕ - f * (Si ,ai ) || 2 > ςhh=12∖I 2dsσ2(si, ai)log-δ^ )
≤ P( [ {||fk (Si, ai) - f * (Si, ai) ||2 > 2J2dsσ2(si, ai)log-δs})
i=1	δ
(7)
≤ ςH=IP(IIfk (Si ,ai) - f *(si, ai)||2 > 2↑l 2dsσk (si, ai)log^s )
Thus, with probability at least 1 - 2Hδ, we have ∑H=ι∣∣fk(Si, ai) - f*(Si,ai)∣∣2	≤
∑H=ι2 J2dsσ2 (Si, ai)log 室.Let δ0 = 2Hδ, we have that with probability 1 - δ, ∑H=ι∣∣fk (Si, ai) -
f *(Si,ai )||2 ≤ £也12{ 2dSσlSi,aiYθ 4HPs ≤ 2H q 2dsσk (Skmax , akmax )log 4^^s , Wherethe
index kmax = arg maxi σk(Si, ai), i = 1, ..., H in episode k. Here, since the posterior distribution is
only updated every H steps, we have to use data points with the max variance in each episode to bound
the result. Similarly, using the union bound for [T ] episodes, and let C = '∏σ2, We have that with
probability at least 1 - δ, ∑k=]1[∆∆k(f )|Hk] ≤ ∑k=1∑H=ι2CHRmaχ∣∣fk(SiM- f *(Si,ai)I∣2 ≤
∑k=ι4CH2Rmaxq2dsσ2(Skma'χ, akmax )log吟.
In each episode k, let σk02 (S, a) denote the posterior variance given only a subset of data points
{(S1max, a1max), ..., (Sk-1max, ak-1max)}, where each element has the max variance in the corre-
sponding episode. By Eq.(6) in Williams & Vivarelli (2000), we know that the posterior variance
reduces as the number of data points grows. Hence ∀(S, a), σk2 (S, a) ≤ σk02 (S, a). By Theorem
5 in Srinivas et al. (2012) which provides a bound on the information gain, and Lemma 2 in
Russo & Van Roy (2014) that bounds the sum of variances by the information gain, we have that
∑k^⅛σk2(Skmax,akmax) = O((ds + da)log[T]) for linear kernels with bounded variances. Note
that the bounded variance property for linear kernels only requires the range of all state-action pairs
actually encountered in M * not to expand to infinity as T grows, which holds in general episodic
MDPs.
Thus with probability 1 - δ, and let δ = T,
夕k=1[入 k (f )lHk ] ≤ ^=14CH 2Rmax ∖J 2dsσ2 (Skmax , akmax )lθg -δ^
≤ ς[=1 8CH2Rmax JdSσk2 (Skmax , akmax )lθg(2TdS)
≤ 8CH2 Rmax ∖/2M'Qk (SkmaX , akmax )
[H [ H ] P dslog(2Tds)
(8)
=8CH2 Rmax√TPdSIOg(2Tds) *
where O ignores logarithmic factors.
I___	~~~~T^	——3 =
O O((ds + da)log[ H]) = O((ds + da)H 2 VT)
Therefore, E[∑k=1∆k(f)∣Ηk] ≤ (1 - T)O((ds + Sa)H3T) + T2HRm,ax * [T] = O(H3d√T),
where 2HRmax is the upper bound on the difference of value functions, and d = ds + da . By
[T ]
similar derivation, E[∑k=1∆k(r)∣Ηk] = O(YdHT). Finally, through the tower property we have
BayeSRegret(T, πps,M*) = O(H3d√T).

6
Under review as a conference paper at ICLR 2021
Algorithm 1 MPC-PSRL
Initialize data D with random actions for one episode
repeat
Sample a transition model and a cost model at the beginning of each episode
for i = 1 to H steps do
Obtain action using MPC with planning horizon τ: a% ∈ arg maXai：i+T Pit+=τi E[r(st, at)]
D = D∪ {(si, ai, ri, si+1)}
end for
Train cost and dynamics representations φr and φf using data in D
Update φr(s, a), φf (s, a) for all (s, a) collected
Perform posterior update of wr and wf in cost and dynamics models using updated representa-
tions φr(s, a), φf (s, a) for all (s, a) collected
until convergence
4.2 Nonlinear case via feature representation
We can slightly modify the previous proof to derive the bound in settings that use feature representa-
tions. We can transform the state-action pair (s, a) to φf (s, a) ∈ Rdφ as the input of the transition
model , and transform the newly transitioned state s0 to ψf (s0) ∈ Rdψ as the target, then the transition
model can be established with respect to this feature embedding. We further assume dψ = O(dφ) as
Assumption 1 in Yang & Wang (2019). Besides, we assume dφ0 = O(dφ) in the feature representation
φr(s, a) ∈ Rdφ0 , then the reward model can also be established with respect to the feature embedding.
Following similar steps, We can derive a Bayesian regret of O(H3/2dg√T).
5	Algorithm Description
In this section, we elaborate our proposed algorithm, MPC-PSRL, as shown in Algorithm 1.
5.1	Predictive model
When model the rewards and transitions, we use features extracted from the penultimate layer of fitted
neural networks, and perform Bayesian linear regression on the feature vectors to update posterior
distributions.
Feature representation: we first fit neural networks for transitions and rewards, using the same
network architecture as Chua et al. (2018). Let xi denote the state-action pair (si, ai) and yi denote
the target value. Specifically, we use reward ri as yi to fit rewards, and we take the difference
between two consecutive states si+1 - si as yi to fit transitions. The penultimate layer of fitted
neural networks is extracted as the feature representation, denoted as φf and φr for transitions and
rewards, respectively. Note that in the transition feature embedding, we only use one neural network
to extract features of state-action pairs from the penultimate layer to serve as φ, and leave the target
states without further feature representation (the general setting is discussed in Section 4.2 where
feature representations are used for both inputs and outputs), so the dimension of the target in the
transition model d(ψ) equals to d』Thus we have a modified regret bound of O(H3/2 y∕ddψT). We
do not find the necessity to further extract feature representations in the target space, as it might
introduce additional computational overhead. Although higher dimensionality of the hidden layers
might imply better representation, we find that only modifying the width of the penultimate layer to
dφ = ds + sa suffices in our experiments for both reward and transition models. Note that how to
optimize the dimension of the penultimate layer for more efficient feature representation deserves
further exploration.
Bayesian update and posterior sampling: here we describe the Bayesian update of transition and
reward models using extracted features. Recall that Gaussian process with linear kernels is equivalent
to Bayesian linear regression. By extracting the penultimate layer as feature representation φ, the
target value y and the representation φ(x) could be seen as linearly related: y = w>φ(x) + , where
is a zero-mean Gaussian noise with variance σ2 (which is σf2 for the transition model and σr2 for the
reward model as defined in Section 3.1). We choose the prior distribution of weights w as zero-mean
7
Under review as a conference paper at ICLR 2021
Carφole without oracle rewards
Figure 1: Training curves of MPC-PSRL (shown in red), and other baseline algorithms in different
tasks. Solid curves are the mean of five trials, shaded areas correspond to the standard deviation
among trials, and the doted line shows the rewards at convergence.
Gaussian with covariance matrix Σp , then the posterior distribution of w is also multivariate Gaussian
(Rasmussen (2003)):
p(w∣D)〜N (σ-2A-1ΦY, AT)
where A = σ-2ΦΦ> + Σp-1, Φ ∈ Rd×N is the concatenation of feature representations {φ(xi)}iN=1,
and Y ∈ RN is the concatenation of target values. At the beginning of each episode, we sample w
from the posterior distribution to build the model, collect new data during the whole episode, and
update the posterior distribution of w at the end of the episode using all the data collected.
Besides the posterior distribution of w, the feature representation φ is also updated in each episode
with new data collected. We adopt a similar dual-update procedure as Riquelme et al. (2018):
after representations for rewards and transitions are updated, feature vectors of all state-action
pairs collected are re-computed. Then we apply Bayesian update on these feature vectors. See the
description of Algorithm 1 for details.
5.2	planning
During interaction with the environment, we use a MPC controller (Camacho & Alba (2013))
for planning. At each time step i, the controller takes state Si and an action sequence ai：i+T =
{ai ,ai+ι,…,ai+τ } as the input, where T is the planning horizon. We use transition and reward mod-
els to produce the first action oi of the sequence of optimized actions arg maXai：i+T Pt=T E[r(st, at)],
where the expected return of a series of actions can be approximated using the mean return of several
particles propagated with noises of our sampled reward and transition models. To compute the optimal
action sequence, we use CEM (Botev et al. (2013)), which samples actions from a distribution closer
to previous action samples with high rewards.
6	Experiments
We compare our method with the following state-of-the art model-based and model-free algorithms
on benchmark control tasks.
Model-free: Soft Actor Critic (SAC) from Haarnoja et al. (2018) is an off-policy deep actor-critic
algorithm that utilizes entropy maximization to guide exploration. Deep Deterministic Policy Gradient
(DDPG) from Barth-Maron et al. (2018) is an off-policy algorithm that concurrently learns a Q-
function and a policy, with a discount factor to guide exploration.
8
Under review as a conference paper at ICLR 2021
Model-based: Probabilistic Ensembles with Trajectory Sampling (PETS) from Chua et al. (2018)
models the dynamics via an ensemble of probabilistic neural networks to capture epistemic uncertainty
for exploration, and uses MPC for action selection, with a requirement to have access to oracle rewards
for planning. Model-Based Policy Optimization (MBPO) from Janner et al. (2019) uses the same
bootstrap ensemble techniques as PETS in modeling, but differs from PETS in policy optimization
with a large amount of short model-generated rollouts, and can cope with environments with no
oracle rewards provided. We do not compare with Gal et al. (2016), which adopts a single Bayesian
neural network (BNN) with moment matching, as it is outperformed by PETS that uses an ensemble
of BNNs with trajectory sampling. And we don’t compare with GP-based trajectory optimization
methods with real rewards provided (Deisenroth & Rasmussen, 2011; Kamthe & Deisenroth, 2018),
which are not only outperformed by PETS, but also computationally expensive and thus are limited
to very small state-action spaces.
We use environments with various complexity and dimensionality for evaluation. Low-dimensional
environments: continuous Cartpole (ds = 4, da = 1, H = 200, with a continuous action space
compared to the classic Cartpole, which makes it harder to learn) and Pendulum Swing Up (ds = 3,
da = 1, H = 200, a modified version of Pendulum where we limit the start state to make it harder
for exploration). Trajectory optimization with oracle rewards in these two environments is easy
and there is almost no difference in the performances for all model-based algorithms we compare,
so we omit showing these learning curves. Higher dimensional environments: 7-DOF Reacher
(ds = 17, da = 7, H = 150) and 7-DOF pusher (ds = 20, da = 7, H = 150) are two more
challenging tasks as provided in Chua et al. (2018), where we conduct experiments both with and
without true rewards, to compare with all baseline algorithms mentioned.
The learning curves of these algorithms are showed in Figure 1. When the oracle rewards are provided
in Pusher and Reacher, our method outperforms PETS and MBPO: it converges more quickly with
similar performance at convergence in Pusher, while in Reacher, not only does it learn faster but
also performs better at convergence. As we use the same planning method (MPC) as PETS, results
indicate that our model better captures uncertainty, which is beneficial to improving sample efficiency.
When exploring in environments where both rewards and transition are unknown, our method learns
significantly faster than previous model-based and model-free methods which do no require oracle
rewards. Meanwhile, it matches the performance of SAC at convergence. Moreover, the performances
of our algorithm in environments with and without oracle rewards can be similar, or even faster
convergence (see Pusher with and without rewards), indicating that our algorithm excels at exploring
both rewards and transitions.
From experimental results, it can be verified that our algorithm better captures the model uncertainty,
and makes better use of uncertainty using posterior sampling. In our methods, by sampling from a
Bayesian linear regression on a fitted feature space, and optimizing under the same sampled MDP in
the whole episode instead of re-sampling at every step, the performance of our algorithm is guaranteed
from a Bayesian view as analysed in Section 4. While PETS and MBPO use bootstrap ensembles of
models with a limited ensemble size to "simulate" a Bayesian model, in which the convergence of the
uncertainty is not guaranteed and is highly dependent on the training of the neural network. However,
in our method there is a limitation of using MPC, which might fail in even higher-dimensional tasks
shown in Janner et al. (2019). Incorporating policy gradient techniques for action-selection might
further improve the performance and we leave it for future work.
7	Conclusion
In our paper, we derive a novel Bayesian regret for PSRL algorithm in continuous spaces with the
assumption that true rewards and transitions (with or without feature embedding) can be modeled by
GP with linear kernels. While matching the best-known bounds in previous works from a Bayesian
view, PSRL also enjoys computational tractability. Moreover, we propose MPC-PSRL in continuous
environments, and experiments show that our algorithm exceeds existing model-based and model-free
methods with more efficient exploration.
9
Under review as a conference paper at ICLR 2021
References
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. arXiv preprint arXiv:1703.05449, 2017.
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA),
pp. 1-9. IEEE, 2018.
Gabriel Barth-Maron, Matthew W Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva Tb,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. arXiv preprint arXiv:1804.08617, 2018.
Zdravko I Botev, Dirk P Kroese, Reuven Y Rubinstein, and Pierre L’Ecuyer. The cross-entropy
method for optimization. In Handbook of statistics, volume 31, pp. 35-59. Elsevier, 2013.
Eduardo F Camacho and Carlos Bordons Alba. Model predictive control. Springer Science &
Business Media, 2013.
Sayak Ray Chowdhury and Aditya Gopalan. Online learning in kernelized markov decision processes.
In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 3197-3205,
2019.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Advances in Neural
Information Processing Systems, pp. 4754-4765, 2018.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465-472, 2011.
Yarin Gal, Rowan McAllister, and Carl Edward Rasmussen. Improving pilco with bayesian neural
network dynamics models. In Data-Efficient Machine Learning workshop, ICML, volume 4, pp.
34, 2016.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. In Advances in Neural Information Processing Systems, pp. 12498-12509,
2019.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is q-learning provably efficient?
In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement
learning with linear function approximation. In Conference on Learning Theory, pp. 2137-2143,
2020.
Sanket Kamthe and Marc Deisenroth. Data-efficient reinforcement learning with probabilistic
model predictive control. In International Conference on Artificial Intelligence and Statistics, pp.
1701-1710. PMLR, 2018.
Ian Osband and Benjamin Van Roy. Model-based reinforcement learning and the eluder dimension.
In Advances in Neural Information Processing Systems, pp. 1466-1474, 2014.
Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement
learning? In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, pp. 2701-2710, International Convention Centre, Sydney,
Australia, 2017. PMLR.
10
Under review as a conference paper at ICLR 2021
Ian Osband, Van Roy Benjamin, and Russo Daniel. (More) efficient reinforcement learning via
posterior sampling. In Proceedings of the 26th International Conference on Neural Information
Processing Systems - Volume 2, NIPS'13,pp. 3003-3011, USA, 2013. Curran Associates Inc.
Ian Osband, Benjamin Van Roy, Daniel J Russo, and Zheng Wen. Deep exploration via randomized
value functions. Journal of Machine Learning Research, 20(124):1-62, 2019.
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer School on Machine
Learning, pp. 63-71. Springer, 2003.
Phillippe Rigollet and Jan-Christian Hutter. High dimensional statistics. Lecture notes for course
18S997, 2015.
Carlos Riquelme, George Tucker, and Jasper Snoek. Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127,
2018.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221-1243, 2014.
Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias W Seeger. Information-theoretic
regret bounds for gaussian process optimization in the bandit setting. IEEE Transactions on
Information Theory, 58(5):3250-3265, 2012.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Christopher KI Williams and Francesco Vivarelli. Upper and lower bounds on the learning curve for
gaussian processes. Machine Learning, 40(1):77-102, 2000.
Lin F Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels, and
regret bound. arXiv preprint arXiv:1905.10389, 2019.
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill. Learning near
optimal policies with low inherent bellman error. arXiv preprint arXiv:2003.00153, 2020.
A Appendix
A.1 Proof of Lemma 1
Here we provide a proof of Lemma 1.
We first prove the results in Rd with d = 1: pι(x)〜N(μ,σ2),p2(x)〜N(μ0, σ2), without loss of
generality, assume μ0 ≥ μ. The probabilistic distribution is symmetric with regard to μ+μ~. Note that
Pι(χ) = p2 (x) at X = μ+μ~. Thus the integration of absolute difference between pdf of pi and p
can be simplified as twice the integration of one side:
2	∞	-(x-μ)	-(x-μ)
/	e 2σ2	— e	2σ2	dx
√2πσ2 √μ+2μ0
|p2(x) -p1(x)|dx
-∞
let zi = x — μ,z2 = X — μ0, we have:

(9)
(10)
lμ0 — μL
11
Under review as a conference paper at ICLR 2021
Now We extend the result to Rd(d ≥ 2): pι(x)〜N(μ,σ2I),p2(x)〜N(μ0,σ2I). We can
rotate the coordinate system recursively to align the last axis with vector μ - μ0, such that the
coordinates of μ and μ0 can be written as (0,0,…,0, μ), and (0,0, ∙∙∙ , 0, μ0) respectively, with
∣μ0 - μ∣ = ∣∣μ - μ0k2. Without loss of generality, let μ ≥ ^.
Clearly, all points with equal distance to μ0 and μ define a hyperplane P : Xd = μ+μ where
p1(x) = p2(x), ∀x ∈ P. More specifically, the probabilistic distribution is symmetric with regard to
P. Similar to the analysis in R1:
∞
广 r ∙∙√,
-∞	-∞	-
-∞
2
P (2π)dσ2d
2
p (2π)dσ2d
∣Pι(x) — P2(x)∣dx1dx2 …dxd
Z∞∞
∞ -∞
∞
Jμ+μ
-χ1
e 2σ2 ..
-xd-ι -(Xd-μ)
• e 2σ2 e 2σ2	dx1dx2 …dxd
Z∞∞
∞	-∞
∞
J”
-X2	-xd-ι -(Xd-μ0)2
e 2σ2 ... e 2σ2 e 2σ2	dxjdx?…dxd
2
P (2π)dσ2d
Z∞
∞
e 2σ2 dxι
Z∞
∞
-2 ,	f∞
e 2σ2 dX2 ∙∙•
-∞
-χd-ι	∞
e	2σ2	dxd-1
∕^+^0
J -^r-
-(xd-μ)
e 2σ2 dXd
(11)
2
—
∞
∞	-x1	∞	-x2
e 2σ2 dxi	e 2σ2
dσ2d -∞	J-∞
∞ -χd-ι	∞
dx2 ... / e 2σ2 dxd-ι
J-∞	Jμ+μ
一(Xd-μ∕)2
e	2σ2
dxd
μ+μ0
2
-(χd-^)2 7	∞
e	2σ2	dXd —
J ^+^0
一(Xd-μ')
2σ2
dxd)
—
e
let zi = Xd — μ,z2 = Xd — μ0, we have:
∞
‰0e
2
dxd
-Z∞
Jμ+μ
一(Xd-μ0)2
e	2σ2	dxd

∞
Jμ一
-z2
e ^2σ2 dzi
∞
Jμ-μ
2
-z2
e ^2σ2 dz2
—
0
尸
Jμ一
2
-z2
e ^2σ2 dz
(12)
0
L
2
0
lμ - μ0l
-z2- 7
e 2σ dz
0
r
≤2
0
1dz
Thus R-∞∞ R-∞∞ . . . R-∞∞ |pi(x) - p2 (x)|dxidx2 . . .dxd
A.2 Experimental details
Here we provide hyperparameters for MBPO:
12
Under review as a conference paper at ICLR 2021
env	cartpole	pendulum	pusher	reacher
env steps per episode	200	200	150	150
model rollouts per env step	400			
ensemble size	5	-			
network architecture	MLP with 2 hidden layers of size 200	MLP with 2 hidden layers of size 200	MLP with 4 hidden layers of size 200	MLP with 4 hidden layers of size 200
policy updates per env step	40			
model horizon	1->15 from episode 1->30	1->15 from episode 1->30	1	1->15 from episode 1->30
Table 1: Hyperparamters for MBPO
And we provide hyperparamters for MPC and Neural Networks in PETS:
env	pusher	reacher
env steps per episode	150	150
popsize	500	400
number of elites	50	50
network architecture	MLP with 4 hidden layers of size 200	
planning horizon	30	30
max iter	5	
ensemble size	5	
Table 2: Hyperparamters for PETS
Here are hyperparameters of our algorithm, which is similar with PETS, except for ensemble
size(since we do not use ensembled models):
env	cartpole	pendulum	pusher	reacher
env steps per episode	200	200	150	150
popsize	500	—	100	—	500	—	400	—
number of elites	50	5	50	50
network architecture	MLP with 2 hidden layers of size 200	MLP With 2 hidden layers of size 200	MLP With 4 hidden layers of size 200	MLP With 4 hidden layers of size 200
planning horizon	30	20	30	30
max iter	5				
Table 3: Hyperparamters for our method
For SAC and DDPG, we use the open source code ( https://github.com/dongminlee94/
deep_rl) for implementation without changing their hyperparameters. We appreciate the authors
for sharing the code!
13