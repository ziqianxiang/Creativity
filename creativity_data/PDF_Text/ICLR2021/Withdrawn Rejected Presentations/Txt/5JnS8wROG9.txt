Under review as a conference paper at ICLR 2021
On the Inductive Bias of a CNN for Distribu-
tions with Orthogonal Patterns
Anonymous authors
Paper under double-blind review
Ab stract
Training overparameterized convolutional neural networks with gradient based
optimization is the most successful learning method for image classification.
However, their generalization properties are far from understood. In this work,
we consider a simplified image classification task where images contain orthog-
onal patches and are learned with a 3-layer overparameterized convolutional net-
work and stochastic gradient descent (SGD). We empirically identify a novel phe-
nomenon of SGD in our setting, where the dot-product between the learned pattern
detectors and their detected patterns are governed by the pattern statistics in the
training set. We call this phenomenon Pattern Statistics Inductive Bias (PSI) and
empirically verify it in a large number of instances. We prove that in our setting,
if a learning algorithm satisfies PSI then its sample complexity is O(d2 log(d))
where d is the filter dimension. In contrast, we show a VC dimension lower bound
which is exponential in d. We perform experiments with overparameterized CNNs
on a variant of MNIST with non-orthogonal patches, and show that the empirical
observations are in line with our analysis.
1	Introduction
Convolutional neural networks (CNNs) have achieved remarkable performance in various computer
vision tasks (Krizhevsky et al., 2012; Xu et al., 2015; Taigman et al., 2014). In practice, these net-
works typically have more parameters than needed to achieve zero train error (i.e., are overparame-
terized). Despite non-convexity and the potential problem of overfitting, training these models with
gradient based methods leads to solutions with low test error. It is still largely unknown why such
simple optimization algorithms have outstanding test performance for learning overparameterized
convolutional networks.
Recently, there have been major efforts to provide generalization guarantees for overparameterized
CNNs. However, current generalization guarantees either depend on the number of channels of the
network (Long & Sedghi, 2020) or hold under specific constraints on the weights (Li et al., 2018).
Clearly, the generalization of overparameterized CNNs depends on both the learning algorithm
(gradient-based methods) and unique properties of the data. Providing generalization guarantees
while incorporating these factors is a major challenge. Indeed, this requires analyzing non-convex
optimization methods and mathematically defining properties of the data, which is extremely diffi-
cult for real-world problems. Therefore, it is necessary to first understand simple settings which are
amenable to theoretical and empirical analysis and share salient features with real-world problems.
Towards this goal, we analyze a simplified pattern recognition task where all patterns in the im-
ages are orthogonal and the classification is binary. The architecture is a 3-layer overparameterized
convolutional neural network and it is learned using stochastic gradient descent (SGD). We take a
unique approach that combines novel empirical observations with theoretical guarantees to provide
a novel generalization bound which is independent of the number of channels and is a low-degree
polynomial of the filter dimension, which is usually low in practice.
Empirically, we identify a novel property of the solutions found by SGD. We observe that the statis-
tics of patterns in the training data govern the magnitude of the dot-product between learned pattern
detectors and their detected patterns. Specifically, patterns that appear almost exclusively in one
of the classes will have a large dot-product with the channels that detect them. On the other hand,
1
Under review as a conference paper at ICLR 2021
patterns that appear roughly equally in both classes, will have a low dot-product with their detecting
channels. We formally define this as the “Pattern Statistics Inductive Bias” condition (PSI) and pro-
vide empirical evidence that PSI holds across a large number of instances. We also prove that SGD
indeed satisfies PSI in a simple setup of two points in the training set.
Under the assumption that PSI holds, we analyze the sample complexity and prove that it is at most
O(d2 log d), where d is the filter dimension. In contrast, we show that the VC dimension of the
class of functions we consider is exponential in d, and thus there exist other learning algorithms (not
SGD) that will have exponential sample complexity. Together, these results provide firm evidence
that even though SGD can in principle overfit, it is nonetheless biased towards solutions which
are determined by the statistics of the patterns in the training set and consequently it has good
generalization performance.
We perform experiments with overparamterized CNNs on a variant of MNIST that has non-
orthogonal patterns. We use our analysis to better understand why SGD has low sample complexity
in this setting. We empirically show that the inductive bias of SGD is similar to PSI. This sug-
gests that the idea of PSI is not unique to the orthogonal case and can be useful for understanding
overparameterized CNNs in other challenging settings.
2	Related Work
Several recent works have studied the generalization properties of overparameterized CNNs. Some
of these propose generalization bounds that depend on the number of channels (Long & Sedghi,
2020; Jiang et al., 2019). Others provide guarantees for CNNs with constraints on the weights (Zhou
& Feng, 2018; Li et al., 2018). Convergence of gradient descent to KKT points of the max-margin
problem is shown in Lyu & Li (2020) and Nacson et al. (2019) for homogeneous models. However,
their results do not provide generalization guarantees in our setting. Gunasekar et al. (2018) study
the inductive bias of linear CNNs.
Yu et al. (2019) study a pattern classification problem similar to ours. However, their analysis holds
for an unbounded hinge loss which is not used in practice. Furthermore, their sample complexity de-
pends on the network size, and thus does not explain why large networks do not overfit. Other works
have studied learning under certain ground truth distributions. For example, Brutzkus & Globerson
(2019) study a simple extension of the XOR problem, showing that overparameterized CNNs gener-
alize better than smaller CNNs. Single-channel CNNs are analyzed in (Du et al., 2018b;a; Brutzkus
& Globerson, 2017; Du et al., 2018c).
Other works study the inductive bias of gradient descent on fully connected linear or non-linear
networks (Ji & Telgarsky, 2019; Arora et al., 2019a; Wei et al., 2019; Brutzkus et al., 2018; Dziugaite
& Roy, 2017; Allen-Zhu et al., 2019; Chizat & Bach, 2020). Fully connected networks were also
analyzed via the NTK approximation (Du et al., 2019; 2018d; Arora et al., 2019b; Fiat et al., 2019).
Kushilevitz & Roth (1996); Shvaytser (1990) study the learnability of visual patterns distribution.
However, our focus is on learnability using a specific algorithm and architecture: SGD trained on
overparameterized CNNs.
3	The Orthogonal Patterns Problem
Data Generating Distribution: We consider a learning problem that captures a key property of
visual classification. Many visual classes are characterized by the existence of certain patterns. For
example an 8 will typically contain an x like pattern somewhere in the image. Here we consider
an abstraction of this behavior where images consist of a set of patterns. Furthermore, each class is
characterized by a pattern that appears exclusively in it. We define this formally below.
Let P be a set of orthogonal vectors in Rd, where ∣P∣ ≤ d. For simplicity, We assume that ∣∣p∣b =
1 for all p ∈ P . We consider input vectors x with n patterns of dimension d. Formally, x =
(x[1], ..., x[n]) ∈ Rnd where x[i] ∈ P is the ith pattern of x and n < d. We denote p ∈ x if x
contains the pattern p ∈ P.1 Let P(x) = {p ∈ x ∣ p ∈ P} denote the set of all patterns in x.
1We say that x contains p if there exists j such that x[j] = p.
2
Under review as a conference paper at ICLR 2021
Next, we define how labeled points are generated. Consider three non-overlapping sets of patterns:
P- , P+ , Ps ⊂ P whose disjoint union is P. P+ is the set of positive patterns, P- the set of negative
patterns and Ps is the set of spurious patterns. For simplicity, in this work we consider the case where
∣P+∣ = ∣P-∣ = 1. We denote, P = {p1,p2, ...,p∣P∣}, P+ = {p1} and P- = {p2}. For convenience, we
also refer to a set of patterns A as a set of the indices of the patterns, .e.g., we denote i ∈ A ifpi ∈ A.
We consider distributions D over (x, y) ∈ Rnd × {±1} with the following properties: (1) P (y = 1) =
P (y = -1) = 1. (2) Given y = 1, a vector X is sampled as follows. Choose the positive pattern p1
and randomly choose a set ofn - 1 patterns from Ps. Denote this set ofn chosen patterns by A. Let
x be some x' such that P(x') = A, i.e., the location of each pattern in X is chosen arbitrarily.2 For
example, ifn = 3 and the n- 1 patterns arep3,p7 this can result in samples such as ([p1,p7,p3], 1)
or ([p3,p1,p7], 1). (3) Similarly for y = -1, only choose p2 ∈ P- instead of p1.
We will consider several distributions that satisfy the above and have different sampling schemes
for the spurious patterns (see Sec. 7). Fig. 4 in the supplementary shows an example of samples
generated using the above procedure. Note that any distribution which satisfies the above is linearly
separable and each vector X can be classified solely based on whether p1 ∈ X or p2 ∈ X.
Neural Architecture: For learning the above pattern detection problems, a natural model in this
context is a 3-layer network with a convolutional layer, followed by ReLU, max pooling and a fully-
connected layer. Each channel in the first layer can be thought of as a detector for a given pattern.
We say that a detector detects pattern p ∈ P, ifp has the largest dot product with the detector among
all patterns in P+ ∪Ps orP- ∪Ps and this dot product is positive.3 For simplicity we fix the weights
on the last linear layer to values ±1.4
Let 2k denote the number of channels. We partition the channels into two sets: w(1) , . . . , w(k)
and u(1), . . . , u(k). These will have weights of +1 and -1 in the output respectively. Finally, let
W ∈ R2k×2 be the weight matrix whose rows are w(i) followed by u(i).
For an input X = (X[1], ..., X[n]) ∈ Rnd where X[i] ∈ Rd, the output of the network is:
k
NW(x) = ∑ [max{σ (w(i) ∙ x[j])} - max{σ (u(i) ∙ x[j])}]
(1)
where σ(x) = max{0, x} is the ReLU activation. Let H denote the class of all networks NW in
Eq. 1, with k > 0. Finally, we note that H can perfectly fit the distribution D above, by setting k = 1,
w(1) = p1 and u(1) = p2. Therefore, for k > 1 the network is overparameterized.
Training Algorithm: Let S be a training set with m IID samples from D. We consider minimizing
the hinge loss: '(W) = ml ∑^xiyi)∈s max{1 - yiNw(xi), 0}. For optimization, we use SGD with
constant learning rate η. The parameters W are initialized as IID Gaussians with zero mean and
(i)	(i)
standard deviation σg. Let Wt be the weight matrix at iteration t of SGD. Similarly let wt , ut be
the corresponding vectors at iteration t.
Detection Ratios: We now define the notion of detection ratios. The detection ratios are a property
of the model and will be key to our analysis. We first define the set of neurons that are maximally
activated by pattern pi among all patterns in Ps ∪ P+ (i.e., all detectors of pattern pi):5
W+(i)
U+(i)
{j ∣ arg max w(j) ∙ Pl = i, w(j) ∙ Pi > 0 }
l∈Ps ∪P+
{j ∣ arg max u(j) ∙ pl = i, u(j) ∙ Pi > 0 }
l∈Ps ∪P+
(2)
Next we define Mw(i) = ∑j∈w+ (i) WIjj ∙Pi and Mu(i) = ∑j∈u+(i) u(j) ∙Pi. The quantity Mw (i) is
the sum of the dot products between a pattern Pi and its detectors w(j). Mw+ (i) can be interpreted
2The order of the patterns will not matter, because the convolutional network is invariant to it.
3The reason we consider these two sets of patterns will be clear when we discuss detection ratios.
4 Note that this does not affect the expressive power of the network.
5Given a tie between sets, we assume the weight is assigned arbitrarily to one of them.
3
Under review as a conference paper at ICLR 2021
as the overall response of w(j) detectors of pattern pi. Similarly, we define Mu+(i), which is like
Mw+ (i), only with detectors u(j).
For all Pi ∈ P+ ∪ Ps We refer to Mu(I) as positive detection ratios. The detection ratio can be
interpreted as the ratio between the uwndesired response of pattern detectors of pi and the desired
response of discriminative pattern detectors (detectors ofp1). Therefore, We Would like this ratio to
be small. Indeed, for any positive point x+ We have that:
NW (x+)≥ M+ ⑴-	∑	M+ (i) = M+ (1)∣ 1 -	∑	Mu(i) I	⑶
w	uw	+
Pi∈Ps∪P+	∖	Pi∈Ps∪P+ Mw(1) /
Where the inequality folloWs since positive points have only patterns in P+ ∪ Ps , by Eq. 1 and the
definitions of Mw+ (i) and Mu+(i). Notice that if all positive detection ratios are small, then the
positive point is classified correctly. We Will empirically shoW that for SGD, the magnitude of the
detection ratios are governed by the statistics of the patterns in the training set, Which Will imply our
generalization result.
Similarly, We define W-(i),U-(i) and Mw- (i), Mu-(i), Where the only difference is using P- in-
stead of P+. Furthermore, for allPi ∈ P-∪Ps we say that Mwi(i) are negative detection ratios. Then,
as in Eq. 3 we have for all negative points X- that: -NW (x-) ≥ Mu(2)(1 - ∑p,∈ps∪p- Mw(；)).
This shows that if negative detection ratios are small, then all negative points are classified corurectly.
In the rest of the paper, we refer to both positive and negative detection ratios as detection ratios.
Empirical Pattern Bias: In a given training set, patterns will appear in both positive and negative
examples. The following measure captures how well-balanced are the patterns between the labels.
For any pattern Pi ∈ P, define the following statistic of the training set:
si
1m
-∑ y 1{Pi ∈ Xj}
(4)
The detection ratios define quantities of the learned model. On the other hand, Eq. 4 is a quantity of
the sampled training set. In the next section, we define the PSI property, which specifies how these
two measures should be related to guarantee good generalization for the learned model.
4 Pattern Statistics Inductive Bias
The inductive bias of a learning algorithm refers to how the algorithm chooses among all models that
fit the data equally well. For example, an SVM algorithm has an inductive bias towards low norm.
Understanding the success of deep learning requires understanding the inductive bias of learning
algorithms used to learn networks, and in particular SGD (Zhang et al., 2017).
In what follows, we define a certain inductive bias of an algorithm in our setting, which we refer
to as the Patterns Statistics Inductive Bias (PSI) property. The PSI property states a simple relation
between the relative frequency of patterns si (see Eq. 4) and the detection ratios. We begin by
providing the formal definition of PSI, and then provide further intuition. For the definition, we let
A be any learning algorithm which given a training set S returns a network A(S) as in Eq. 1.
Definition 4.1. We say that a learning algorithm A satisfies the Patterns Statistics Inductive Bias
condition with constants b, c, δ > 0 ((b,c,δ)-PSI) if the following holds. For any m ≥ 1, 6 with
probability at least 1 - δ over the randomization of A and training set S of size m, A(S) satisfies
the following conditions:
∀i ∈ Ps ∪ P+	:	Mu(i)	≤ bmax (—S, 0) +	~c=	(5)
Mw+ (1)	s1	m
∀i ∈ Ps ∪ P-：	Mw(；)	≤ b max (—S, θ) +	√=	(6)
_________________________________	Mu (2)	∖ S2	)	√m
6We state m ≥ 1 for simplicity. Alternatively, one can assume m ≥ C for a constant C .
4
Under review as a conference paper at ICLR 2021
We next provide some informal intuition as to why SGD updates may lead to PSI (in Sec. 7 we
provide a proof of this for a restricted setting).
We will consider updates made by gradient descent (full batch SGD). Define Wt+ (i) to be the set
W+(i) with weight vectors wt(j) instead of w(j). Similarly, define Ut+(i), Wt-(i) and Ut-(i).
Throughout the discussion below, we assume that these sets have roughly the same size.7 8 We will
show that in certain cases, a high value of - Si implies that the detection ratio Mu(I) has a high
value. Furthermore, a low value of - Si implies a low value of Mu(I). This motivates the bound in
the PSI definition. As we will show, this follows since the statisticws of the patterns in the training set
si , govern the magnitude of the dot-product between a detector and its detected pattern.
By our distribution assumption we should have si ≈ 11. First assume that Si ≈ -ɪ for Pi ∈ Ps, i.e.,
-S1 ≈ 1. Now lets see what the detection ratio Mu(I) should be by the gradient update. Note that
the gradient is a sum of updates, one for each point in the training set. Assume that j ∈ Wt+(1),
i.e., w(j) detects Pi. Then by the gradient update, the value mPi is added to Wyj for all positive
points that have non-zero hinge loss. The value -mPi is also added for a few Pi ∈ P- ∪ PS and
a subset of the negative points (i depends on the specific negative point). In the next iteration, it
holds that j ∈ Wt+ι(1) and the updates continue similarly. Overall, we see that Wyj ∙Pi, which is
the dot-product between the detector and its detected pattern, increases in each iteration and should
be large after a few iterations. Therefore, Mw+ (1) should be large. By exactly the same argument,
we should expect that for j ∈ U+(i), u(jj ∙ Pi increases in each iteration and now MU(i) should be
large. Under the assumption that ∣U+(i)∣ ≈ ∣W+(1)∣, we should have Mu(I) ≈ 1 as well. Therefore,
if - si ≈ 1 then we should expect that Mu (i) ≈ 1.
Si	Mw (i)
On the other hand, if Pi appears in roughly an equal number of positive and negative points, i.e.,
si ≈ 0, then we should expect Mu+(i) to be low. To see this, consider a filter j ∈ Ut+(i). In this
case, positive points that contain Pi and with non-zero loss add - mPi to Uy), while negative points
that contain Pi and have non-zero loss add mPi. Thus, Uy) ∙ Pi should not increase signficantly.
Therefore, both MU(I) and -SSi should be small in this case.
Given the intuition above, one possible conjecture is that the detection ratio Mu(I) is bounded by an
affine function of max (-Si, 0), which leads to the PSI condition in Definition 4.1.8 The bias term
in the affine function takes into account that our intuition above is not exact. Finally, we can make a
similar argument for - Si for motivating Eq. 6.
5 VC Dimension B ound and Relation to PSI
Here we show that the architecture in Section 3 is highly expressive, and can thus potentially overfit
and generalize poorly. Moreover, we show examples of networks that overfit and do not satisfy PSI.
First, a simple argument shows that V C(H) ≤ (nd) in our setting. The proof is given in Section A.
The lower bound below is more challenging, and reveals interesting connections to the PSI property.
Theorem 5.1. Assume that d = 2n and n ≥ 2, then VC(H) ≥ 22-1.
The full proof is given in Section B. Here We give a sketch. We construct a set B of size 2n-1 = 2 2-1
that can be shattered. For a given I ∈ {0, 1}n-1 let I[j] be its jth entry. For any such I, define a
point xI such that for any 1 ≤ j ≤ n - 1, xI[j] = I[j]p2j+1 + (1 - I[j])p2j+2. Furthermore,
arbitrarily choose xI [n] = p1 or xI [n] = p2 and define B = {xI ∣ I ∈ {0, 1}n-1}. Assume that each
point xI ∈ B has label yI. We define a network with filters w(I) = max {αI, 0} ∑1≤j≤n-1 xI[j] and
7This holds with high probability at initialization for a sufficiently large network. Furthermore, in Section
7, we show that it holds during training in the case of two training points.
8We consider max (-Si, 0)in the PSI definition because the detection ratios are non-negative.
5
Under review as a conference paper at ICLR 2021
S ①uu ①-nɔɔo EnU
(a)
(b)
(c)
Figure 1: Empirical analysis of c*. (a) Empirical calculation of c* for Du and Dajc. Values are in
log scale. (b) C as a function of the network size k (c) Positive correlation between Mu(I)) and
max (-SSi, 0). The depicted line is the best PSI bound with b = 2 (lowest c).
u(I) = max {-αI, 0} ∑1≤j≤n-1xI [j] for each I ∈ {0, 1}n-1 and constants αI. Then, we prove that
there exists constants αI such that N (xI) = yI for all I by solving a linear system.
Relation to PSI: Theorem 5.1 shows that there are exponentially large training sets that can be
exactly fit with H. This fact can be used to show a lower bound on sample complexity that is
exponential in d for general ERM algorithms (Anthony & Bartlett, 2009). The networks that fit
these datasets are those defined by w(I), u(I). It is easy to see that these networks do not satisfy the
PSI property. To see this, note that Mw+ (1) = Mu-(2) = 0, which implies that the left-hand sides of
parts 1 and 2 in the Definition 4.1 are infinite. Therefore, PSI is not satisfied for these networks.
These networks classify points based on the spurious patterns Ps , and not on the patterns which
determine the class. Networks that satisfy PSI are essentially the opposite: they classify a point
mostly based on detectors for the patterns p1 and p2 and thus generalize well, as we show next.
6	PSI Implies Good Generalization
In the previous section we showed that a general ERM algorithm for the class H may need exponen-
tially many training samples to get low test error. Here we show that any algorithm satisfying the PSI
condition (see Definition 4.1) will have low-degree polynomial sample complexity, when patterns
in Ps are unbiased (i.e., E [y l{pγ ∈ x}] = 0 for Pi ∈ Ps). Specifically, in the following theorem We
show that such an algorithm will have zero test error w.h.p., given only O(∣P∣2 log(∣P ∣)) training
samples. Note that this also implies a sample complexity of O(d2 log(d)) since ∣P∣ ≤ d.
Theorem 6.1. Assume that D satisfies the conditions in Section 3 and E [yl{Pi ∈ x}] = 0 for
all Pi ∈ Ps. Let A be a learning algorithm which satisfies (b,c,δ)-PSI with b, c ≥ 1. Then, if
m > 300b2c2 ∣P∣2 log(∣P∣), with probability at least 1 - δ - ∣p4∣3,9 A(S) has 0 test error.
We defer the proof to the supplementary but here we sketch the main argument. By the assumption
E [yl{pi ∈ x}] = 0 and standard concentration of measure, ∣s∕ should be small and therefore the
detection ratios should be small by PSI. Then, by the key observation that small detection ratios
imply perfect classification (e.g., Eq. 3), the algorithm achieves zero test error with respect to D.
7	Empirical and Theoretical Evidence that SGD satisfies PSI
Empirical Analysis: Thus far We have established that the PSI property implies good general-
ization. Here We provide empirical evidence that SGD indeed learns such models with overparam-
eterized CNNs. We also provide a qualitative analysis that further confirms that the statistics of the
9We note that the ∣p4∣3 may be improved to an arbitrary Y > 0 if we scale m by log 1.
6
Under review as a conference paper at ICLR 2021
patterns in the training set correlate with the detection ratios. Full details of the experiments are
provided in the supplementary.
We perform experiments with two distributions denoted by Du and Dvc that satisfy the properties
defined in Section 3 and such that E [y	∈ x}] = 0 for all Pi ∈ Ps. Thus, given Theorem 6.1, if
PSI holds, good generalization will be implied. See Section E.2 for details on the distributions.
Next, we show that PSI holds with small constants b and c which do not change the order of magni-
tude of the bound in Theorem 6.1, i.e., b2c2 < 10.10 We trained a neural network in our setting with
SGD as described in Section 3. We performed more than 1000 experiments with different parameter
values for n, d, k and m (see Section E.3 for details) and performed 10 experiments for each set of
values for n, d, k and m. For each experiment, we set b = 2 and empirically calculated the lowest
constant C which satisfies the PSI definition, which we denote by c*. The formal definition of c* is
given in Eq. 10 in the supplementary. Figure 1a shows that across all experiments, the value of c*
is less than 1, i.e., b2 (c*)2 < 10. We further checked how c* varies with k for D = Du, d = 50 and
n = 20. Figure 1b shows that c* is at most slightly correlated with k and has low value for large k.
The intuition we described in Section 4 suggests that there is a positive correlation between Mu(I)
and max (-Ii, 0). To test this, we experimented with a distribution which can vary the probability
of a spurious pattern to be selected and thus can control max (-Si, 0). Figure 1c clearly shows a
positive correlation between these quantities, strongly suggesting that the statistics of the patterns in
the training set govern the magnitude of the detection ratios. See Section E.5 for further details.
Theoretical Analysis in a Simplified Setup: Here we show that PSI holds for a setup of two
training points, S = {(x+, 1), (x-, -1)}. We further assume that x+ and X- have exactly the same
patterns in Ps. We analyze gradient descent with a constant learning rate η = cη. The following
theorem shows that PSI holds with constants b = 1 and C = /18%. The proof analyzes the trajectory
of gradient descent and is provided in Section F.
Theorem 7.1. For a sufficiently small G σg, Cn such that σg << η and k ≥ Poly (log d, ɪ), with
probability at least 1 -
9
d7
- 8e-8, gradient descent converges to a global minimum after T ≤ O
(cη)
iterations and the PSI condition is satisfied with b = 1 and C = ∖∕18cn.
The theorem holds for overparameterized networks, which coincides with our empirical findings
in Section 7. The theorem holds for sufficiently small initialization, and thus it is not in the same
regime of NTK analysis where initialization is large (Woodworth et al., 2019; Chizat et al., 2019).
8	Experiment on MNIST
In this section we report experiments on a variant of MNIST (LeCun, 1998) and show that we can
use our analysis to better understand the performance of overparameterized CNNs in this setting.
Full details of the experiments are given in Section G.
Our PSI results thus far can be summarized informally as follows. In a pattern detection problem,
an algorithm has PSI bias if the dot product between a discriminative pattern and its detector is
large, and the dot product between a spurious pattern and its detector is low. Furthermore, the gap
between these dot products increases with the train size and a sufficiently large gap implies perfect
accuracy. While our analysis required the patterns to be orthogonal, the above idea can work beyond
the orthogonal case, as the experiment below shows.
We consider data generated as follows. Each data point consists of 9 randomly sampled MNIST
images, where if y = 1 one of the 9 digits is randomly chosen to be of color blue and the rest 8
10To empirically validate PSI and show that it implies good generalization, we could in principle show that
the conditions of Theorem 6.1 hold empirically, i.e., there exist b, c and m such that m > 300b2 c2 d2 log(d)
and PSI holds with constants b, c and high probability 1 - δ. However, as with most generalization results, the
numerical value (including constants) results in large m which cannot be empirically tested. Instead, we show
that b and c do not change the order of magnitude of the bound.
7
Under review as a conference paper at ICLR 2021
(a)	(b)
(c)
Figure 2: Experiments on a variant of MNIST. (a) Examples of data points. (b) Filters of CNN with
k = 1 that perfectly classifies the data (c) Examples of learned filters of overparameterized CNNs
for different training set sizes. Figures of all learned filters are given in Section G.
digits are colored green. For y = -1 a similar sampling procedure is performed but with the color
red instead of blue. Figure 2a shows examples of data points (note that in our notation n = 9 and
d = 28 * 28 * 3 = 2352). Thus in this setting, red and blue digits are discriminative whereas green
are spurious.
We use the network in Eq. 1 and SGD to learn a classifier for this data. The data can be perfectly
classified with a network with k = 1 (see Figure 2b). We train an overparameterized network with
k = 20 for different training set sizes. Figure 2c shows subsets of the learned filters (see Section
G for all filters), for different training set sizes. The figures show in color the positive weight filter
entries. The pattern that appears is the pattern that maximally activates them, i.e., the pattern they
detect.
First, we can see that the filters come in three colors (blue, red, green) corresponding to the three
pattern types they detect (positive, negative, spurious respectively). This fact is not trivial and is
similar to what we obtained in the proof of Theorem 7.1 and explained in Section 4.
As noted above, the PSI prediction is that the dot product between detectors and detected patterns
would be large for discriminative patterns (i.e., red and blue) and low for spurious (i.e., green).
Furthermore, this difference should increase with the data size m. Indeed, Figure 2c shows precisely
this behavior. Namely, as we increase the training set size, the green pattern detectors become
darker and thus have low dot product with detected pattern. In contrast, the red and blue maintain
a bright color and thus have large dot products with detected patterns. Finally, the test accuracy for
m = 6, 20, 1000 is 88%, 100%, 100%, respectively. This is in line with Theorem 6.1 that shows PSI
can lead to perfect test accuracy when the gap between dot products is sufficiently large.
9	Conclusions
Understanding the inductive bias of gradient methods for deep learning is an important challenge.
In this paper, we study the inductive bias of overparameterized CNNs in a novel setup and provide
8
Under review as a conference paper at ICLR 2021
theoretical and empirical support that SGD exhibits good generalization performance. Our results
on MNIST suggest that the PSI phenomenon goes beyond orthogonal patterns.
We use a unique approach of combining novel empirical observations with theoretical guarantees to
make headway in a challenging setting of overparameterized CNNs. We believe that our work can
pave the way for studying inductive bias of neural networks in other challenging settings.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in neural information processing
systems,pp. 6155-6166, 2019.
Martin Anthony and Peter L Bartlett. Neural network learning: Theoretical foundations. cambridge
university press, 2009.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems, pp. 7411-7422, 2019a.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019b.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. In International Conference on Machine Learning, pp. 605-614, 2017.
Alon Brutzkus and Amir Globerson. Why do larger models generalize better? a theoretical per-
spective via the xor problem. In International Conference on Machine Learning, pp. 822-830,
2019.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. International Con-
ference on Learning Representations, 2018.
Lenaic Chizat and Francis Bach. Implicit bias of gradient descent for wide two-layer neural networks
trained with the logistic loss. arXiv preprint arXiv:2002.04486, 2020.
Lenaic Chizat, Edouard Oyallon, and Francis Bach. On lazy training in differentiable programming.
In Advances in Neural Information Processing Systems, pp. 2933-2943, 2019.
Simon Du, Jason Lee, Yuandong Tian, Aarti Singh, and Barnabas Poczos. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. In International Conference on
Machine Learning, pp. 1339-1348, 2018a.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Simon S Du, Jason D Lee, and Yuandong Tian. When is a convolutional filter easy to learn? ICLR,
2018b.
Simon S Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Russ R Salakhutdinov, and Aarti
Singh. How many samples are needed to estimate a convolutional neural network? In Advances
in Neural Information Processing Systems, pp. 373-383, 2018c.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. International Conference on Learning Representations,
2018d.
Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization bounds for
deep (stochastic) neural networks with many more parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
9
Under review as a conference paper at ICLR 2021
Jonathan Fiat, Eran Malach, and Shai Shalev-Shwartz. Decoupling gating from linearity. arXiv
preprint arXiv:1906.05032, 2019.
Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent
on linear convolutional networks. In Advances in Neural Information Processing Systems, pp.
9461-9471,2018.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. ICLR,
2019.
Yiding Jiang, Behnam Neyshabur, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Fantas-
tic generalization measures and where to find them. In International Conference on Learning
Representations, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Eyal Kushilevitz and Dan Roth. On learning visual concepts and dnf formulae. Machine Learning,
24(1):65-85, 1996.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Xingguo Li, Junwei Lu, Zhaoran Wang, Jarvis Haupt, and Tuo Zhao. On tighter generalization
bound for deep neural networks: Cnns, resnets, and beyond. arXiv preprint arXiv:1806.05159,
2018.
Philip M Long and Hanie Sedghi. Generalization bounds for deep convolutional neural networks.
ICLR, 2020.
Kaifeng Lyu and Jian Li. Gradient descent maximizes the margin of homogeneous neural networks.
ICLR, 2020.
Mor Shpigel Nacson, Suriya Gunasekar, Jason Lee, Nathan Srebro, and Daniel Soudry. Lexico-
graphic and depth-sensitive margins in homogeneous and non-homogeneous deep models. In
International Conference on Machine Learning, pp. 4683-4692, 2019.
Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change
in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124-127, 1950.
Haim Shvaytser. Learnable and nonlearnable visual concepts. IEEE Transactions on Pattern Anal-
ysis and Machine Intelligence, 12(5):459-466, 1990.
Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to
human-level performance in face verification. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 1701-1708, 2014.
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. Regularization matters: Generalization and
optimization of neural nets vs their induced kernel. In Advances in Neural Information Processing
Systems, pp. 9709-9721, 2019.
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Kernel and
deep regimes in overparametrized models. arXiv preprint arXiv:1906.05827, 2019.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048-2057, 2015.
Bing Yu, Junzhao Zhang, and Zhanxing Zhu. On the learning dynamics of two-layer nonlinear
convolutional neural networks. arXiv preprint arXiv:1905.10157, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. ICLR, 2017.
Pan Zhou and Jiashi Feng. Understanding generalization and optimization performance of deep
cnns. In International Conference on Machine Learning, pp. 5960-5969, 2018.
10
Under review as a conference paper at ICLR 2021
A VC Dimension Upper B ound
Without considering the order of the patterns in the images, there are at most (nd) input points in
D. Since the network in Eq. 1 is invariant to the order of the patterns in an image, this implies:
V C(H) ≤ (nd). Note that for the definition ofVC dimension, we assume that the domain of possible
inputs is the domain of images of the distribution. This gives a tighter upper bound for our problem.
B Proof of Theorem 5.1
We will construct a set B of size 2n-1 = 22-1 that can be shattered. For a given I ∈ {0,1}n-1 let
I[j] be its jth entry. For any such I, define a point xI such that for any 1 ≤ j ≤ n - 1, xI[j] =
I[j]p2j+1 + (1 - I[j])p2j+2. Furthermore, arbitrarily choose xI [n] = p1 or xI [n] = p2 and define
B={xI ∣ I ∈ {0, 1}n-1}.
Now, assume that each point xI ∈ B has label yI . We will show that there is a network N ∈ H
such that N(xI) = yI for all I. For each I ∈ {0, 1}n-1, define w(I) = max {αI, 0} ∑1≤j≤n-1 xI[j]
and u(I) = max {-αI, 0} ∑1≤j≤n-1 xI[j], where {αI} is the unique solution of the following linear
system with 2n-1 equations. For each I ∈ {0, 1}n-1 the system has the following equation:
∑	αι‛ = yιc	⑺
I'∈{0,1}n-1∖{I}
where for any I ∈ {0, 1}n-1, Ic ∈ {0, 1}n-1 is defined such that Ic[j] = 1 -I[j] for all 1 ≤ j ≤ n - 1.
There is a unique solution because the corresponding matrix of the linear system is the difference
between an all 1’s matrix and the identity matrix. By the Sherman-Morrison formula (Sherman &
Morrison, 1950), this matrix is invertible, where in the formula the outer product rank-1 matrix is
the all 1’s matrix and the invertible matrix is minus the identity matrix. Then for N with the above
weights and any xI :
N(xI) =	∑	[ max {σ (w(I ) . x[j])} - max {σ (U(I ) ∙ x[j])}]
I ′ ∈{0,1}n-1	j	j
∑ αι'max{σ (	∑ Xι‛[i] ∙ Xi[j]
I ′ ∈{0,1}n-1	j	1≤i≤n-1
∑ αι' = yι
I'∈{0,1}n-1∖{Ic}
by the definition of N , the orthogonality of the patterns, and Eq. 7. We have shown that any labeling
yI can be achieved, and hence the set is shattered, completing the proof.
C	Proof of Theorem 6.1
WLOG we prove the theorem for ∣P∣ = d. By the assumption, for pi ∈ Ps, si is an average ofm IID
binary variables y7∙l{Pi ∈ Xj} with zero expected value. Thus, by Hoeffding's inequality We have
for all pi ∈ Ps that:
P (b ∣Si∣ ≤ 4b√log(d)) ≤ 2-
卜	Vm J	d4
Therefore, by a union bound over all patterns Pi ∈ Ps, with probability at least 1 一 京,for all Pi ∈ Ps:
b∣Si∣≤ 4b√0g(d) ≤ ɪ	⑻
m	6cd
Next we consider P1 (the positive pattern), for which E [s1] = 0.5 (because it only appears in the
positive examples, and the prior over y is 0.5). Hoeffding’s bound and the definition ofm	imply that
∣sι∣ ≥ 1 with probability at least 1 一 ʤ.11
11In fact we can have exponential dependence here, but we use d3 to simplify later expressions.
11
Under review as a conference paper at ICLR 2021
m
Figure 3: c* as a function of the training set size m.
We can now do a union bound over all patterns and PSI condition to obtain that with probability at
least 1 - δ - ʤ We have by the PSI property and Eq. 8, for all Pi ∈ Ps:
Mu(i ≤ bIsiI + 二 ≤ ɪ + 二 < 1
Mw (1) - ∣s1∣ √m 2 2cd √m d
(9)
From PSI we have Mu(I) ≤ √cm < d. Therefore, for any positive point (x+, 1) Eq. 3 implies:
NW (x+) > Mw+ (1) (ι — 彳)
>0
Thus, x+ is classified correctly. By the symmetry of the problem and part 2 in Definition 4.1, any
negative point will be classified correctly as well.
D Further Experiments for Validation of PSI
To further validate the PSI condition, we tested whether the conditions in the proof of Theorem 6.1
empirically hold. Specifically, in the proof we showed that Mu(I) < d for all Pi ∈ Ps (in Eq. 9). We
checked this for all settings of (n, d) and largest possible k and m, k = 10000 and m = 40000. In
all of our experiments, SGD converged to a solution with 0 test error such that Eq. 9 holds for all
Pi ∈ Ps .
Finally, we checked how c* varies with m. Figure 3 show that c* is at most slightly cor-
related with m and has low value for large m. In the same setup of Section E.3, we
performed experiments with distribution Du , n = 20, d = 50, k = 2500 and m ∈
{100, 200, 500, 1000, 2000, 5000, 20000, 40000, 80000, 120000}.
E	Experimental Details of Section 7
Here we provide details of the experiments performed in Section 7. All experiments were run on
NVidia Titan Xp GPUs with 12GB of memory. Training algorithms were implemented in Tensor-
Flow. All of the empirical results can be replicated in approximately 150 hours on a single Nvidia
Titan Xp GPU.
E.1 VALUE OF c*
We use the following formula to compute c* in the experiments:
c* =	√mmax { max	MuV?	- 2max (-si, 0), max	Mrwy)	- 2max (-si, 0), θ}	(10)
i∈Ps∪P+	Mw (1)	si,	, i∈Ps∪P-	Mu(2)	S2,	,
E.2 Distributions in Experiments
We perform experiments with two types of distributions that satisfy the properties defined in Section
3. They differ in the random sampling procedure of spurious patterns described in Section 3. In both
12
Under review as a conference paper at ICLR 2021
Figure 4: Example of points in an orthogonal patterns distribution. Here there are 25 possible
orthogonal patterns (∣P∣ = 25) and each pattern is a 10 × 10 image patch and thus d = 100. The
number of patterns in each image is n = 16. The image consists of 4 rows of 4 patches each. The
positive examples contain the pattern in P+ . The negative examples contain the pattern in P-. In
the two leftmost images of each class, the corresponding pattern is shown. All other patterns in an
image are from the set of spurious patterns Ps .
distributions P is the set of all one-hot vectors in Rd . In the first distribution Du, the n - 1 spurious
patterns are selected uniformly at random without replacement from Ps . In the second distribution
Dvc, for each 1 ≤ j ≤ n - 1 one of the patterns from {p2j+1, p2j+2} is selected uniformly at random.
Importantly, both Du and Dvc satisfy E [y1{pg ∈ x}] = 0 for all Pi ∈ Ps. Thus, given Theorem 6.1,
if PSI holds, good generalization will be implied.
Remark E.1. The support of Dvc is the shattered set B in the proof of Theorem 5.1. The proof
implies that for any sampled training and test sets which are subsets of B, there exists a network
with 0 training error and arbitrarily high test error. Therefore, by optimizing the training error,
SGD can converge to these solutions. However, as we show empirically, SGD does not converge
to these solutions, but rather it satisfies PSI and converges to solutions with good generalization
performance.
E.3 Figure 1a Experiment
We performed more than 1000 experiments with the network in Eq. 1 and SGD. We experimented
with parameter values k ∈ {1000, 10000}, m ∈ {100, 500, 1000, 2000, 5000, 20000, 40000}
, (n, d) ∈ {(10, 20), (10, 80), (20, 50), (40, 60)} for D = Du and (n, d) ∈
{(10, 20), (40, 80), (25, 50), (30, 60)} for D = Dvc. For each distribution Du or Dvc, we
performed 10 experiments for each set of values for n, d, k and m. For each set of values we
plot the mean of the 10 experiments and standard deviation error bars in shaded regions. In each
one of the 10 experiments we randomly sampled the training and test sets according to the given
distribution Du or Dvc and randomly sampled the initialization of the network. We used a test
set of size 1000. All orthogonal patterns were one-hot vectors. We trained only the weights of
the first convolutional layer. We used a batch size of 20 if k = 10000 and batch size of 100 for
k = 1000. The learning rate was set to max{0001,0.0000001} and σg to 0.000001. The solution
SGD returned was either after 50000 epochs or if there was an epoch where the training loss was
less than 0.00001. For each experiment, We set b = 2 and empirically calculated c*.
13
Under review as a conference paper at ICLR 2021
E.4 Figure 1b Experiment
In the same setup of Section E.3 (i.e., batch size, stopping criteria, learning rate etc.), we
performed experiments with distribution Du , n = 20, d = 50, m = 2000 and k ∈
{50, 100, 1000, 2500, 5000, 7500, 10000}.
E.5 Figure 1c Experiment
We experimented with a distribution Dp which can vary the probability of a spurious pattern to be
selected and thus can control max (-Si, 0). Given y = 1 it selects p3 with probability P or p4
with probability 1 - p. Then it selects the remaining n - 2 patterns from Ps ∖ {p3, p4} uniformly
at random without replacement. Similarly, given y = -1 it selects p3 with probability 1 - p or p4
with probability p. The remaining n - 2 patterns are selected uniformly without replacement from
Ps ∖ {p3, p4}. We experimented with various P and plotted for each solution of SGD, Mu(I) and
max (-Si, 0)for all Pi ∈ PS ∪ P+.
In the setup of Section E.3 we experimented with distributions Dp forP values in
{0.0, 0.01, 0.03, 0.05, 0.07, 0.1, 0.12, 0.2, 0.21, 0.28, 0.3, 0.4,
0.44, 0.5, 0.51, 0.59, 0.6, 0.68, 0.7, 0.78, 0.8, 0.9, 0.91, 0.94
0.95, 0.98, 0.99, 1.0}
We experimented with values n = 40, d = 60, m = 1000 and k = 2500. The solution SGD returned
was either after 2000 epochs or if there was an epoch where the training loss was less than 0.00001.
F Proof of Theorem 7.1
F.1 Notations
Here we define additional notations that will be useful for the proof of the theorem.
Let PT be the set of all patterns that appear in either x+ or x- . Similarly to Eq. 2 define:
W+(i) = ∖j ∣ argmaxWj) ` Pi = i, WYj -Pi > 0 ∙
l∈Pτ ∖{2}
U+(i) = ∙ j ∣ argmaxUj) `Pi = i, U(j - Pi > 0 ∙	(11)
l∈Pτ ∖{2}
and
W-(i) = ∙ j ∣ arg max Wj) ` Pi = i, Wyj - Pi > 0 ∙
l∈Pτ ∖{1}
U-(i) = ∙ j ∣ argmaxUj) ` Pi = i, Uyj - Pi > 0 ∙	(12)
l∈Pτ ∖{1}
Define:
Aw =	U	W+ (i)
i∈PT ∖{2}
Au =	U U0- (i)
i∈PT ∖{1}
Finally we define Poly(x) to be any polynomial function ofx.
14
Under review as a conference paper at ICLR 2021
F.2 Auxiliary Lemmas
We now prove several technical lemmas. In Section F.3 we use the lemmas to prove the theorem. In
the next 3 lemmas we provide high probability bounds on sizes of certain sets that are functions of
the sets in Eq. 11 and Eq. 12.
Lemma F.1. For any 0 < E < 4, with probability at least 1 - 4e-8 for any k > poly( ɪ):
∣Au∣ - (1 - 2-n) ≤ E
k
and
Proof. It suffices to show that for any k:
	∣∣Aw ∣ - (1 - 2-n) k∣ ≤ 2Vk
and	∣Au∣ - (1 - 2-n) k∣ ≤ 2√k
For each 1 ≤ j ≤ k it holds that j ∈	Aw with probability 1 - 2-n . Therefore by Hoeffding’s inequality,
with probability at least 1 - 2e-8 ,
∣∣Aw∣ - (1 - 2-n) k∣ ≤ 2√k.
The same argument applies for ∣Au∣, a union bound and setting k > 表 concludes the proof. □
Lemma F.2. For any E > 0, with probability at least 1 -奈-4e-8, for k > poly (log d, ɪ) and for
all i ∈ PT ∖ {2}:	∣W+(i)∣	≤1 + E k(1 -2-n +E) n
and	W+(i)∣	≥ 1 - E k (1 - 2-n - e) — n
Similarly, for all i ∈ PT ∖ {1}：	∣U-(i)∣	≤1 + E k(1 -2-n +E)	n
and	∣u-(i)∣	≥ 1 - E k (1 - 2-n - e) — n
Proof. Without loss of generality, consider ∣W0+(i)∣. We first condition on the random variable
∣Aw ∣ and given that the event k(1 - 2-n - E) ≤ ∣Aw ∣ ≤ k(1 - 2-n + E) holds. By symmetry, we
have E [ ∣W0 (i)∣ ] = 1 where the expectation is with respect to the initialization. Thus, we get by
∣Aw∣	n
Hoeffding’s inequality:
W+(i)∣ - 11 ≤ 2√op∖
Aw∣	nI ≤ √AW∣ )
,D-2∣Aw ∣( 2√Aogd )2	2
≤ 2e ∖ WAwI)=—
d8
By the law of total probability, applying Lemma F.1 and a union bound over i ∈ PT twice (for both
W+ (i) and U- (i)), We get the desired result.	□
15
Under review as a conference paper at ICLR 2021
Lemma F.3. For any E > 0, with probability at least 1 -奈一4e-8, for k > poly (log d, ɪ) and for
all i ∈ PT ∖ {1, 2} the following holds:
∣W+(i) ∩ W-(2)∣ ≤	1	+ e
k(2 - 2-n-1 + e) n n(n + 1)
∣W+(i) ∩ W-(2)∣	1
--7-;----------；- ≥ --：-— — €
k (1 - 2-n-1 - €) n(n + 1)
U+⑴ ∩u-(i)∣ ≤	1	+ €
k (2 - 2-n-1 + €)— n(n + 1)
and
U+⑴ ∩ u-(i)∣	1
--7-;----------T ≥ ---：----— — €
k (2 - 2-n-1 - €)	n(n + 1)
Proof. The proof is similar to the proofs of Lemma F.1 and Lemma F.2. The difference is
that We use the equalities E [∣Aw ∩ W-(2)∣] = E [∣Au ∩ U+(1)∣] = (2 - 2-n-1) k instead of
E [∣Aw∣] = E [∣Au∣] = (1 - 2-n) k as in Lemma F.1. Furthermore, we use E ∣W0 ⑶∩W0甲 =
∣Aw∩W0 (2)∣
E [∣UAU1∩UU017 ] = n⅛ for fixed Aw ∩ W-(2)∣ and Au ∩ U+(1)∣ instead of E [ W^ =
E IUA(i)∣ = n for fixed ∣Aw ∣ and ∣Au∣ as in Lemma F.2.	□
Lemma F.4. For any M > 0 and δ > 0, there exists a sufficiently small σg > 0, such that with
probability at least 1 - δ,forall 1 ≤ i ≤ k Jw0i)j ≤ M and Qu0i) 卜 ≤ M.
Proof. The proof is immediate.	□
We now proceed to analyze the dynamics of gradient descent in the next two lemmas. Define M
such that for all 1 ≤ i ≤ k,Q w0i) 卜 ≤ M and Qu0i) 卜 ≤ M. Let E be the set of all t such that for all
X ∈ S, it holds that NWt (x) < 1. Let t* = arg mint {t - 1 ∈ E, t ∉ E}. We assume that η and σg are
sufficiently small such that t* ≥ 2.
Lemma F.5. For a sufficiently small €, M, cη such that M << η, the following holds for any
1 ≤ t ≤ t*:
1. If j ∉ Aw, then WY)= w0j) - α2p2 where α ∈ {0,1}.
2∙ If j ∈ W+(1), then Wyj = w0j) - 2 ∑i€pτ∖{i} αipi + 竽pi,	where α	∈	{0,1}.
3.	Ifi ∈ PT ∩ Ps andj ∈ W0+(i) ∩W0-(i) then wt(j) = w(0j).
4.	If i ∈ PT ∩ Ps and j ∈ W+(i) ∩ W-(2), then Wyj= WOj)-	2p2	+ 2Pi
5.	Ifj ∉ Au, then ut(j) = u(0j) - αp1 where α ∈ {0, 1}.
6.	If j ∈ U-(2), then Uy)= UOj)- 2 ∑i∈pτ∖{2} αiPi + 竽p2, where a ∈ {0,1}.
7.	Ifi	∈ PT ∩ Ps	andj	∈ U0-(i) ∩U0+(1) then ut(y) = u(0y).
8.	If i	∈ PT ∩ Ps	and j	∈ U-(i) ∩ U+(1), then Uy)= UOj)- 2Pi	+ 2Pi
Proof. 1. If j ∉ W0- (2), then for t = 1 the gradient of the loss with respect to W(i) is 0,
because every pattern in PT has a negative dot product with W(Oi). Therefore, W(1i) = W(Oi).
By the same argument w(i) = WOi) for all t ≥ 1. If j ∈ W- (2) then 2p2 will subtracted in
the first iteration, and Wt(i) will not change in later iterations.
16
Under review as a conference paper at ICLR 2021
2.	The proof follows directly by the gradient update. In each iteration, 2pi is added and a
pattern in PT ∖ {1} is subtracted unless all such patterns already have a negative dot product
(i)
with wt . Note that we used here the fact that M << η.
3.	For t = 1 we have by the gradient update:
IWCj) — IWCj) , n _n — Cj)	门3、
wι = wo + 2 pi 2 pi = w0	(13)
By induction, w(i) = WOi) for all 1 ≤ t ≤ t*.
4.	The proof follows by the gradient update as in previous proofs. For t = 1, the term 2p2 is
subtracted by the update of x-, since j ∈ W- (2). The term 2p is added due to the update
of x+. Now j ∈ W1+(i) ∩ W1-(i) and thus wtCj) will not change in subsequent iterations, as
in the proof of part 3. This concludes the proof.
By symmetry, the proofs of 5-8 are identical to the proofs of parts 1-4.	□
Define
(1 -2-n)
Y =--------
(14)
n
then we have the following:
Lemma F.6. For a sufficiently small G M, cη such that M << n and k ≥ Poly (log d, ɪ) with
probability at least 1 - d - 8e-8, gradient descent converges to a global minimum after 击 ≤ T ≤
-3- iterations.
Proof. Throughout the proof we use Lemma F.4 to choose a sufficiently small M such that ∣wC0i) ∣ ≤
M and ∣u0i)∣ ≤ M with probablity at least 1 -亲. We further apply Lemma F.1, Lemma F.2 and
Lemma F.3 which together with Lemma F.4 hold with probability at least 1 - d - 8e-8. Define the
sets of weights Bi such that i corresponds to the set of weights in part i of Lemma F.5. For example,
B2 = W+(1) and B8 = Ui∈Pτ∩p, U-(i) ∩ U+(1).
Define the following:
NWC1t)(x)	∑ max {σ (WCi) ∙ i∈B2 j	xj+)}
NWC2t) (x) =	-∑max {σ (uCi) i∈B6 j	∙xj)}
NWC3t)(x)	=∑max {σ (WCi) ∙ i∈B4 j	xj)}
	-∑max {σ (uCi) ∙ i∈B8 j	xj)}
and
NWC4t)(x)=NWt(x)-NWC1t)(x)-NWC2t)(x)-NWC3t)(x)
=∑	max{σ (W⑴∙ xj)}
i∈B1∪B3 j
-	∑ max {σ (uCi) ∙xj)}
i∈B5∪B7 j
We would like to analyze the dynamics of NWt (x+). To do so, we will address each NWCi) (x+) in
turn for 1 ≤ t ≤ t*.
17
Under review as a conference paper at ICLR 2021
Bounding NWI) (x+): By Lemma F.5 part 1, it follows that NWI) (x+) = ∑j^2 w0j + Cn2∣B21 for
all 1 ≤ t ≤ t*. Recall the definition of Y in Eq. 14. Then, by Lemma F.2, for sufficiently small E and
k > poly (log d, ɪ) it holds that:
B21〜
ɪ-Y
≤E
By Lemma F.4, we have:
∑ w (0j) ≤ ∣B2∣M ≤ (Yk + Ek)M
j∈B2
Therefore, after 1 ≤ t ≤ t* iterations, we have:
NWt)(x+)-警∣ ≤ (Yk + ek)M + 竽
By choosing M and E to be sufficiently small (given an upper bound on t that does not depend on
M and e, which we show Iater), ∣NWW) (x+) - c⅛tγ ∣ is sufficiently small.
Calculating NW(2) (x+): Notice that after n - 1 iterations, we have by Lemma F.5 part 6 that
NW(2)(x+) = 0. By taking cη to be sufficiently small, we can ensure that n - 1 < t*.
Bounding NW(3) (x+): By Lemma F.5 parts 4 and 8 and given that M << cη we have for 1 ≤ t ≤ t*:
∣W0+(i)∩W0-(2)∣- ∑ ∣U0-(i)∩U0+(1)∣
i∈PT ∩Ps
By Lemma F.3, for sufficiently small E, for any i ∈ PT ∩ Ps, the difference
1 (∣w+(i) ∩ W-(2)∣ -∣u-(i) ∩ U+(1)∣)
k
is sufficiently small. We conclude that NW(3)(x+) is sufficiently small for small E.
Bounding NW(4) (x+): By Lemma F.5, parts 1,3,5,7 it follows that
N(4)(x+) ≤ kM
Wt
and thus can be made sufficiently small for small M .
Finishing the proof: By combining the previous arguments We have
Nwt (x+) = cη2γ + β+
for 1 ≤ t ≤ t* , where β+ is sufficiently small.
By symmetry, we have:
-NWt(X-) = cηtγ + β-
18
Under review as a conference paper at ICLR 2021
for 1 ≤ t ≤ t*, where β- is sufficiently small.
Our goal is to show that gradient converges to a global minimum at T = t*. Let
and
t+ = arg min
t≤t*
+ β+
>1}
t- = arg min
t≤t*
+ β-
>1}
where the minimum is over integral times t. Notice that t+ ! = t- can only occur when there exists
an integer r such that
1 - cηrγ ≤ 2max{β+ 户}
(15)
Choose Cn to be a small number which is not an integral multiple of 2 (e.g., choose irrational Cn).
Then, max{β+, β-} can be made sufficiently small such that Eq. 15 does not hold. 12 In this case,
after -^- ≤ t+ = t- = t* ≤ -3- iterations, gradient descent converges to a global minimum. □
F.3 Finishing the Proof of Theorem 7.1
We are now ready to prove the theorem. Gradient descent converges to a global minimum after
αc~ ≤ T ≤ OC- iterations by Lemma F.6. Furthermore, by the proof of Lemma F.6, WT (1)=
W+ (1). For each j ∈ Wq (1), the norm of WTj is at least 2T ≥ 击.Therefore, for a sufficiently
small by Lemma F.2:
∣W+(1)∣ ≥ 1
2γk — 3
Mw+ (1) ≥
Now, by Lemma F.6, for all j ∉ W+(1), it holds that ∣wTjj) ∣ ≤ IwQj)∣ + 2 ≤ η = cη. Therefore, for
all i ∈ P ∖ {2}, Mw- (i) ≤ Cn. By symmetry, it follows that the PSI property holds with b = 1 and
c = ∖∕18cn.
G	Experimental Details of Section 8
Here we provide details of the experiments performed in Section 8. All experiments were run on
NVidia Titan Xp GPUs with 12GB of memory. Training algorithms were implemented in PyTorch.
All of the empirical results can be replicated in approximately one hour on a single Nvidia Titan Xp
GPU.
We now describe how we created train and test sets for our setting. For train we sampled digits from
the original MNIST training set and for test we sampled digits from the original MNIST test set. To
sample a data point, we randomly sampled a label y ∈ {±1}. Then, if y = 1 we randomly sampled
9 MNIST digits (either from the MNIST train or test set). Then randomly chose 8 of them to be the
color green and one of them to be the color blue. If y = -1, we do the same procedure with blue
replaced by red.
For training we implemented the setting in Section 3. Specifically, here we have n = 9 and d =
28 * 28 * 3 = 2352. We trained the network in Eq. 1 with k = 20 for training set sizes m = 6,
m = 20 and m = 1000. For each training set size we performed 10 different experiments with
different sampled training set and initialization. We ran SGD with batch size min{10, m}, learning
rate 0.0001 and for 200 epochs. We report the test accuracy and train accuracy in the final epoch
(200). In all runs SGD gets 100% train accuracy. For m = 6 the mean test accuracy is 88.09% with
standard deviation 12.7, for m = 20 the mean test accuracy is 99.84% with standard deviation 0.35
and for m = 1000 the mean test accuracy is 100% with standard deviation 0.
Figure 5, Figure 6 and Figure 7 show the set of all filters in the experiments reported in Figure 2c,
for m = 6, m = 20 and m = 1000, respectively. To plot the figures, for each entry of the filter x
12Note that max{β+ , β- } does not depend on cη.
19
Under review as a conference paper at ICLR 2021
we calculated max{0, x}. We scaled the weights of the network to be with values between 0 to 255
by dividing all entries by the maximum entry across all parameters of the network (after performing
max{0, x}) and multiplying by 255.
20
Under review as a conference paper at ICLR 2021
21