Under review as a conference paper at ICLR 2021
A Distributional Perspective on
Actor-Critic Framework
Anonymous authors
Paper under double-blind review
Ab stract
Recent distributional reinforcement learning methods, despite their successes, still
contain fundamental problems that can lead to inaccurate representations of value
distributions, such as distributional instability, action type restriction, and confla-
tion between samples and statistics. In this paper, we present a novel distributional
actor-critic framework, GMAC, to address such problems. Adopting a stochastic
policy removes the first two problems, and the conflation in the approximation
is alleviated by minimizing the Cramer distance between the value distribution
and its Bellman target distribution. In addition, GMAC improves data efficiency
by generating the Bellman target distribution through the Sample-Replacement
algorithm, denoted by SR(λ), which provides a distributional generalization of
multi-step policy evaluation algorithms. We empirically show that our method
captures the multimodality of value distributions and improves the performance
of a conventional actor-critic method with low computational cost in both discrete
and continuous action spaces, using Arcade Learning Environment (ALE) and
PyBullet environment.
1	Introduction
The ability to learn complex representations via neural networks has enjoyed success in various ap-
plications of reinforcement learning (RL), such as pixel-based video gameplays (Mnih et al., 2015),
the game ofGo (Silver et al., 2016), robotics (Levine et al., 2016), and high dimensional controls like
humanoid robots (Lillicrap et al., 2016; Schulman et al., 2015). Starting from the seminal work of
Deep Q-Network (DQN) (Mnih et al., 2015), the advance in value prediction network, in particular,
has been one of the main driving forces for the breakthrough.
Among the milestones of the advances in value function approximation, distributional reinforce-
ment learning (DRL) further develops the scalar value function to a distributional representation.
The distributional perspective offers various benefits by providing more information on the charac-
teristics and the behavior of the value. One such benefit is the preservation of multimodality in value
distributions, which leads to more stable learning of the value function (Bellemare et al., 2017a).
Despite the development, several issues remain, hindering DRL from becoming a robust framework.
First, a theoretical instability exists in the control setting of value-based DRL methods (Bellemare
et al., 2017a). Second, previous DRL algorithms are limited to a single type of action space, either
discrete (Bellemare et al., 2017a; Dabney et al., 2018b;a) or continuous (Barth-Maron et al., 2018;
Singh et al., 2020). Third, a common choice of loss function is Huber quantile regression loss which
is vulnerable to conflation between samples and statistics without an imputation strategy (Rowland
et al., 2019).
The instability issue is not present if a trainable policy is introduced, i.e., the evaluation setting
of the Bellman operator is used, as shown by the convergence of distributional Bellman operator
(Bellemare et al., 2017a). In addition, the general form of the stochastic policy gradient method
does not assume a specific type of action space, e.g. discrete or continuous (Williams, 1988; 1992;
Sutton et al., 1999). Because Wasserstein distance has biased sample gradients (Bellemare et al.,
2017b), in practice, directly minimizing the Wasserstein distance is often not preferred as a loss
function of a neural network and thus some of the exemplary works (Dabney et al., 2018b;a) of deep
DRL minimizes the Huber quantile regression loss (Huber, 1964) instead. To this end, we treat the
methods which minimize the Huber quantile loss as our baseline. However, as proven in Rowland
1
Under review as a conference paper at ICLR 2021
(a) State of interest
(b) The evaluated value distributions
Figure 1: Modality of value distribution during the learning process of Breakout-v4. (a) An arrow is
added in the inset to indicate the ball’s direction of travel. The episode reaches a terminal state if the
paddle misses the ball. (b) Probability density functions of the value distributions learned by each
actor-critic when {0.2, 0.4, 1.2}M frames are collected. As the policy improves, the probability of
losing a turn (V = 0) decreases, and the probability of earning scores (V > 0) increases. Note that
the modality transition from V = 0 is clearly captured by the GMM + Cramer method.
et al. (2019), representing a distribution using the Huber quantiles instead of samples can lead to
conflation between statistics and samples, and thus an imputation strategy is required to learn a more
accurate representation of a distribution. Despite its theoretical soundness, the imputation strategy
can introduce computational overheads depending on the statistics (e.g. quantiles, expectiles). To
avoid this issue, we formulate the DRL problem through samples and parameters instead of statistics,
and minimize the Cramer distance between the value distributions. Combining these solutions, We
arrive at a distributional actor-critic with Cramer distance as the value distribution loss.
On the other hand, many actor-critic methods suffer from data inefficiency and often include multi-
step algorithms like the λ-return algorithm (Watkins, 1989) and off-policy updates, e.g. importance
sampling. Here we address this problem by adapting multi-step off-policy updates to the distri-
butional perspective, by defining a generalized form of the multi-step distributional Bellman tar-
gets. Furthermore, we introduce a novel value-distribution learning method which we call Sample-
Replacement, denoted by SR(λ). We show that the expectation of the target distribution from SR(λ)
is equivalent to the scalar λ-return. Additionally, we propose to parameterize the value distribution
as a Gaussian mixture model (GMM). When combining GMM with the Cramer distance, we can
derive an analytic solution and obtain unbiased sample gradients at a much lower computational
cost compared to the method using Huber quantile loss. Altogether, we call our framework GMAC
(Gaussian mixture actor-critic).
We present experimental results to demonstrate that this framework successfully outperforms the
baseline algorithm with scalar value function in discrete action space, and can be expanded to con-
tinuous action spaces without any architectural or algorithmic modification. Furthermore, we show
that more accurate representation of value distributions is learned, with a less computational cost.
2	Related Works
Bellemare et al. (2017a) has shown that the distributional Bellman operator derived from the dis-
tributional Bellman equation is a contractor in a maximal form of the Wasserstein distance. Based
on this point, Bellemare et al. (2017a) proposed a categorical distributional model, C51, which is
later discussed to be minimizing the Cramer distance in the projected distributional space (Row-
land et al., 2018; Bellemare et al., 2019). Dabney et al. (2018b) proposed quantile regression-based
models, QR-DQN, which parameterizes the distribution with a uniform mixture of Diracs and uses
sample-based Huber quantile loss (Huber, 1964). Dabney et al. (2018a) later expand it further so
that a full continuous quantile function can be learned through the implicit quantile network (IQN).
Yang et al. (2019) then further improved the approximation of the distribution by adjusting the set
of quantiles. Rowland et al. (2019) proposed expectile regression in place of quantile regression for
learning categorical distribution to address the error in the Bellman target approximation. Choi et al.
(2019) suggested parameterizing the value distribution using Gaussian mixture and minimizing the
Tsallis-Jenson divergence as the loss function on a value-based method. Outside of RL, Bellemare
2
Under review as a conference paper at ICLR 2021
et al. (2017b) proposed to use Cramer distance in place of Wasserstein distance used in WGAN due
to its unbiasedness in sample gradients (Arjovsky et al., 2017).
There have been many applications of the distributional perspective, which exploit the additional
information from value distribution. Dearden et al. (1998) modeled parametric uncertainty and
Morimura et al. (2010a;b) designed a risk-sensitive algorithm using a distributional perspective,
which can be seen as the earliest concept of distributional RL. Mavrin et al. (2019) utilized the
idea of the uncertainty captured from the variance of value distribution. Nikolov et al. (2019) has
also utilized the distributional representation of the value function by using information-directed-
sampling for better exploration of the value-based method. While multi-step Bellman target was
considered (Hessel et al., 2018), the sample-efficiency was directly addressed by combining multi-
step off-policy algorithms like Retrace(λ) (Gruslys et al., 2017).
Just as C51 has been expanded deep RL to disitributional perspective, Barth-Maron et al. (2018)
studied a distributional perspective on DDPG (Lillicrap et al., 2016), an actor-critic method, by pa-
rameterizing a distributional critic as categorical distribution and Gaussian mixture model. Singh
et al. (2020) has further expanded the work by using an implicit quantile network for the critic.
Several works (Duan et al., 2020; Kuznetsov et al., 2020; Ma et al., 2020) have proposed a distri-
butional version of the soft-actor-critic (SAC) framework to address the error from over-estimating
the value. However, previous works concentrated on extending a specific actor-critic framework to
the distributional setting. Therefore, we aim to suggest methods which may be easily adopted in the
process of expanding a scalar value methods to a distributional perspective, along with an attempt to
address the previously mentioned issues present in the value-based ditributional algorithms.
3	Distributional Reinforcement Learning
We consider a conventional RL setting, where an agent’s interaction with its environment is de-
scribed by a Markov Decision Process (MDP) (X , A, R, P, γ), where X and A are state and action
spaces, R(x, a) is the stochastic reward function for a pair of state x and action a, P (x0 |x, a) is the
transition probability of observing x0 given the pair (x, a), and γ ∈ (0,1) is a time discount factor. A
policy ∏(∙∣χ) maps a state X ∈ X to a probability distribution over actions a ∈ A.
The objective of RL is to maximize the expected return, E[Gt] where Gt = Pt∞=0 γtR(xt, at) is the
sum of discounted rewards from state xt given a policy π at time t. Then for any state xt , the value
V and state-action value Q under the given policy π can be defined as
V (xt)	= E[Gt	|X=xt],	Q(xt, at)	=	E[Gt	|X=xt,A=at].	(1)
A recursive relationship in the value in terms of the reward and the random transition is described
by the Bellman equation (Bellman, 1957) given by
Q(x, a) = E[R(x, a)] + γEa0〜∏,x0〜P [Q(x0, a0)],	(2)
where the first expectation is calculated over a given state-action pair (x, a) and the second expecta-
tion is taken over the next possible states χ0 〜P(∙∣χ, a) and actions a0 〜∏(∙∣χ).
DRL extends the Bellman equation to an analogous recursive equation, termed the distributional
Bellman equation (Morimura et al., 2010a;b; Bellemare et al., 2017a), using a distribution of the
possible sum of discounted rewards Z(x, a):
Z(x, a) =D R(x, a) + γZ(x0, a0),	(3)
where =D denotes having equal distributions and Q(x, a) = E[Z(x, a)]. Then Z is learned through
distributional Bellman operator Tπ defined as
T πZ(x, a) :=D R(x, a) + γPπZ(x, a)	(4)
where Pπ : Z → Z is a state transition operator under policy π, PπZ(x, a) :D= Z(x0, a0). Analo-
gously, the distributional Bellman optimality operator T can be defined as
T Z (x, a) := R(x, a) + YZ (x0, arg max Ex，〜P [Z (x0, a0)]).	(5)
a0
3
Under review as a conference paper at ICLR 2021
Algorithm 1: SR(λ)
Input: Trajectory of states and values {(x1, Z1), . . . , (xT, ZT)} for a given length T, discount
factor γ , weight parameter λ
Output: Set of λ-returns {Z1(λ) , . . . , ZT(λ-)1}
X J Collect m samples {Xι,..., Xm,} from ZT
for t = T - 1 to 1 do
X J rt + γX // Bellman operator
Zt(λ) J Pim=1 δXi // empirical distribution using m Diracs
X0 J Collect m samples {X10 , . . . , Xm0 } from Zt(λ)
for i = 1 to m do
I Xi J Xi with probability 1 一 λ
end
end
The distributional Bellman operator has been proven to be a γ-contraction in a maximal form of
Wasserstein distance (Bellemare et al., 2017a), which has a practical definition given by
dp(U, V) = (∕1∣F-1(ω) - F-1(ω)∣pdω) ”,	(6)
where U, V are random variables and FU ,FV are their cumulative distribution functions (cdf).
However, unlike the distributional Bellman operator, the distributional Bellman optimality operator
is not a contractor in any metric (Bellemare et al., 2017a). Thus, the distance dp(TZ1, TZ2) between
some random variables Z1, Z2 may not converge to a unique solution. This issue has been discussed
in Bellemare et al. (2017a), with an example of oscillating value distribution caused by a specific
tie-breaker design of the argmax operator. One way to remove this issue from consideration is by
learning the value distributions via expected Bellman operator with a trainable stochastic policy and
finding an optimal policy under principles of conservative policy iteration by Kakade & Langford
(2002). See Appendix A for more discussion.
4	Algorithm
In this section, we incrementally develop the building blocks of our proposed method. First, we
present an efficient distributional version of λ-return algorithm called Sample-Replacement, denoted
by SR(λ). Then, we show that minimizing the energy distance, which is equivalent to a specific
form of the Cramer distance, between Gaussian mixtures can be a better solution than using quantile
regressions when working with distributional actor-critic and SR(λ).
4.1	SR(λ): SAMPLE-REPLACEMENT FOR λ-RETURN DISTRIBUTION
The actor-critic method is a temporal-difference (TD) learning method in which the value function,
the critic, is learned through the TD error defined by the difference between the TD target given
by n-step return, G(tn) = Pin=1 γi-1rt+i + γnV(xt+n), and the current value estimate V(xt). A
special case of TD method, called TD(λ) (Sutton, 1988), generates a weighted average of n-step
returns for the TD target, also known as the λ-return,
∞
G(λ) = (1- λ) X λn-1G(n), λ ∈ [0,1],	⑺
n=1
to mitigate the variance and bias trade-off between Monte Carlo and the TD(0) return to enhance
data efficiency. From an alternative perspective, equation 7 can be thought of as finding a TD target
via taking the expectation of a random variable G whose sample space is the set of all n-step returns,
{G(0),..., G(∞)}. Then the probability distribution of G is given by
Pr[G = Gtn)] = (1 - λ)λn-1.	(8)
4
Under review as a conference paper at ICLR 2021
Similar to Gt(n), we define n-step approximation of the value distribution as
n-1
Z(n) :D X YiR(Xt+i,at+i)+ γnEa0~∏[Z(xt+n,a0)],	(9)
i=0
where E[Zt(n)] = G(tn). Then, a distributional analogy of equation 8 can be written as
Pr[Z = Z(n)] = (1 - λ)λn-1	(10)
where Z is a random variable whose sample space is a set of all n-step approximations,
{Z(0),..., Z(∞)}. Unlike G, We cannot directly calculate an expectation over a set of random
variables, Z. Instead, We redefine equation 10 in terms of cdfs:
Pr[F = FZg] = (1 - λ)λn-1.	(11)
Zt
FZ(n)denotes the Cdf of the n-step return Z(n), and F = {FZ(o),..., FZ(∞)} is a random vari-
able over the set of F (n) . Then, for any z, We can successfully reWrite equation 11 as a linear
Zt
combination of F (n)
Zt
∞
E[F] = (1 - λ) X λn-1Fz(n).	(12)
n=1
Let us define a random variable Z(λ) that has E[F] as its cdf. Then the expectation of Z(λ) and the
expectation of Zt(n) have an analogous relationship to equation 7 (see Appendix B), meaning that
its behavior in expectation is equal to that of the λ-return. Therefore, We treat the resulting random
variable Zt(λ) as a distributional analogy of the λ-return. We note that, in practice, collecting infinite
horizon trajectory is infeasible and thus We use a truncated sum (Cichosz, 1995; van Seijen et al.,
2011):
N
FZ(λ) = (1 - λ) X λn-1FZ(n) +λNFZ(N).	(13)
n=1
Given a trajectory of length N, naively speaking, finding Z(λ) for each time step requires finding
N different Zt(n) . As a result, We need to find total of O(N2) different distributions to find Zt(λ)
for all states in the given trajectory. The number of distributions to find can be reduced to O(N), in
practice, by approximating the distribution of Zt(n) With a mixture of diracs, as described in Dabney
et al. (2018b), because the Bellman operations can be applied to the same set of samples to find Zt(n)
for different t’s. Then, the approximation takes the form of
1m
Z(n) ≈ Zθ (Xt) := m X δθi(χt)	(14)
m i=1
Where θ : X → Rm is some parametric model. To obtain the target samples for Zθ (Xt), We start
With m samples of the last value distribution in the sampled trajectory. Traversing through the
sampled trajectory in a reversed order, We replace each sample With a neW sample from the next
state With a probability of 1 - λ. Then the obtained set Xm is equivalent to a set of samples from
the approximated distribution of the λ-returns, Zt(λ). A more detailed description of the algorithm
can be found in Algorithm 1.
For sample-based methods like the implicit quantile netWork, one can directly use this set Xm as
the target samples. HoWever, RoWland et al. (2019) has shoWn that the quantiles predicted from
the Huber quantile loss cannot be interpreted as samples, thus an imputation strategy is required to
generate a distribution from the statistics. We propose to minimize the Cramer distance instead in
Which the predicted parameters can be samples or the parameters of the distribution itself.
5
Under review as a conference paper at ICLR 2021
4.2	CRAMER Distance
Let P and Q be probability distributions over R. If we define the cdf of P, Q as FP , FQ respectively,
the lp family of divergence between P and Q is
lp(P, Q) := Z ∞ |FP (x) - FQ(x)|pdx1/p.	(15)
When P = 2, it is termed the Cramer distance. The distributional Bellman operator in the evaluation
setting is a ∣γ|1/p-contraction mapping in the Cramer metric space (Rowland et al., 2019), whose
worked out proof can be found in Appendix C.
A notable characteristic of the Cramer distance is the unbiasedness of the sample gradient,
E Vθl2(Pm, Qθ) = Vθl2(P, Qθ)	(16)
X〜Q
where Xm := {Xι,..., Xm} are samples drawn from P, Pml := m Pm=I δχi is the empirical
distribution, and Qθ is a parametric approximation of a distribution. The unbiased sample gradient
makes it suitable to use Cramer distance with stochastic gradient descent method and empirical
distributions for updating the value distribution.
SzekeIy (2002) showed that, in the univariate case, the squared Cramer distance is equivalent to one
half of energy distance (l22 (P, Q) = ɪE(P, Q)) defined as
E (P, Q) :=E (U, V)=2EkU-Vk2-EkU-U0k2 -EkV -V0k2,	(17)
where U, U0 and V, V 0 are random variables that follow P, Q, respectively.
4.3	En ergy Distance between Gaussian Mixtur e Mod els
So far, we have described the components required to formulate a general setting of the suggested
distributional extension to actor-critic methods. Here, we take one step further to enhance the ap-
proximation accuracy and computational efficiency by considering the parameterized model of the
value distribution as a Gaussian mixture model (Choi et al., 2019; Barth-Maron et al., 2018). Fol-
lowing the same assumption used for equation 14 the approximation using Gaussian mixture is given
using parametric models μ,σ,w : X → RK
KK
Zθ(xt) ：= Ewi(Xt) N(z; μi(xt),σi(xt)2), where Ewi(Xt) = 1.	(18)
i=1	i=1
If random variables X, Y follow the distributions P, Q parameterized as GMM, the energy distance
has the following closed-form
E (X, Y)=2δ(X, Y)-δ(X, X0) - δ(Y, Y0),
where δ(U,V) = Ewuiwvj E [∣N(z; μwι - μvj,σU + σ2)∣] .	(19)
i,j
Here, μχi refers to the ith component for random variable X and same applies for σ and w. With
Gaussian mixtures, the closed-form solution of the energy distance defined in equation 19 has a
computational advantage over sample-based approximations like the Huber quantile loss. When
using GMM, the analytic approximation of equation 14 can be derived as
∞K
Zt(λ) ≈X(1-λ)λn-1Xwnk N (z; μnk ,σn k) ≈
n=1	k=1
1m
—EN (Z； μnk ,σn k ),
mn
i=1
(20)
where μnk refers to kth component of μ2(n) for simplicity of notation, n is sampled from Geo(1-λ)
and the index k ∈ {1, . . ., K} is sampled proportional to the mixture weights {w1, . . ., wK}. This
is equivalent to having a mixture of m Gaussians, thus we can simply perform sample replacement
on the parameters (μ, σ2), instead of realizations of the random variables as in equation 14. Then,
the loss function described in equation 19 can easily be applied.
When bringing all the components together, we have a distributional actor-critic framework with
SR(λ) that minimizes the Cramer distance between Gaussian mixture value distributions. We call
this method GMAC. A brief sketch of the algorithm is shown in Appendix E.
6
Under review as a conference paper at ICLR 2021
T
r5 〜N(0,0.12)
S5
S4
Si
r4 〜U{-1,1}
(b)
MontezumaRevenge-v4
(c)
(a)
γ
γ
Figure 2: (a) An environment with two states and stochastic rewards with expected value of zero. (b)
The probability density function (above) and the Cramer distance between the ground truth and the
estimated distributions for IQ, IQE, and GM (below). (c) Learning curve of Montezuma’s Revenge
using modality information as intrinsic reward.
5	Experiments
In this section, we present experimental results for three different distributional versions of Proximal
Policy Optimization (PPO) with SR(λ): IQAC (IQN + Huber quantile), IQAC-E (IQN + energy
distance), and GMAC (GMM + energy distance), in the order of the progression of our suggested
approach. The performance of the scalar version of PPO with value clipping (Schulman et al., 2016)
is used as the baseline for comparison. Details about the loss function of each method can be found
in Appendix D. For a fair comparison, we keep all common hyperparameters consistent across the
algorithms except for the value heads and their respective hyperparameters (see Appendix F).
The results demonstrate three contributions of our proposed DRL framework: 1) the ability to cor-
rectly capture the multimodality of value distributions, 2) generalization to both discrete and contin-
uous action spaces, and 3) significantly reduced computational cost.
Representing Multimodality As discussed throughout Section 4, we expect minimizing the
Cramer distance to produce a more appropriate depiction of a distribution compared to minimiz-
ing the Huber quantile loss. First, we demonstrate this with a simple value regression problem for
an MDP of five sequential states, as shown in Figure 2 (a). The reward function ri of last two state
Si is stochastic, with r4 from a uniform discrete distribution and r5 from a normal distribution.
Then the value distribution of S1 should be bimodal with expectation of zero (Figure 2 (b)). In this
example, minimizing the Huber quantile loss (labeled as IQ-naive) of a implicit quantile network
underestimates the variance of S1 due to conflation and does not capture the locations of the modes.
Applying an imputation strategy as suggested in Rowland et al. (2019), improvement on the under-
estimation of variance can be seen. On the other hand, minimizing the Cramer distance converges
to correct mode locations using both implicit quantile network and Gaussian mixture model, labeled
as IQE and GM respectively in the figure. More details about the experimental setup and further
results can be found in Appendix G. The comparison can be extended to more complex tasks such
as the Atari games, of which a sample result is shown in Figure 1, and additional visualizations of
the value distribution during the learning process from different games can be found in Appendix G.
So what can be achieved with correct modality? By capturing the correct modes of the distribution,
an additional degree of freedom on top of the expected value can be accurately obtained, from which
richer information can be derived to distinguish states by their value distributions. In particular, the
extra information may be utilized as an intrinsic motivation in sparse-reward exploration tasks. To
demonstrate, We compare using Cramer distance between value distributions as intrinsic reward
to using TD error between scalar value estimates in a sparse reward environment of Montezuma’s
Revenge in Figure 2 (c), which shows a clear improvement in the performance.
Discrete and Continuous Action Spaces Experimental results from the ALE (Bellemare et al.,
2013) and the PyBullet environments (Coumans & Bai, 2016-2020) show that our algorithm can be
7
Under review as a conference paper at ICLR 2021
Figure 3: Learning curves for Atari games from ALE and 3 continuous control tasks from PyBullet.
applied to both discrete and continuous action spaces. While GMAC shows overall improvement in
61 Atari games (Appendix G), we present the full learning curves of 3 Atari games in Figure 3 in
which previous distributional methods have shown improvements over the scalar version. One can
observe that the results from GMAC show the most significant improvement from the baseline.
All experiments are run over 5 random seeds with consistent hyperparameters. In Figure 3, a solid
line represents an average of mean scores over 100 recent episodes and the shaded area represents
the standard deviation over the seeds. For visual clarity, the plots are smoothed over 5M frames and
1.25M frames in Atari and PyBullet, respectively. Non-smoothed learning curves on more tasks and
the final scores over the 61 ALE tasks can be found in Appendix G.
Computational Cost Table 1 shows the number of parameters and the number of floating-point
operations (FLOPs) required for a single inference and update step of each agent. We emphasize
three points here. First, the implicit quantile network requires more parameters due to the intermedi-
ate embeddings of random quantiles. Second, the difference between the FLOPs for a single update
in IQAC and IQAC-E indicates that the proposed energy distance requires less computation than
the Huber quantile regression. Last, the results for GMAC show that using Gaussian Mixtures can
greatly reduce the cost even to match the numbers of PPO while having improved performance.
Table 1: FLOP measurement results for a single process in Breakout-v4
Algorithm	Params (M)	FLOPs (G)	
		Inference	Update
PPO	0.44	1.73	5.19
一一IQAC 一	0.52	--2.98^ 一	一^12.98- -
IQAC-E	0.52	2.98	8.98
GMAC	0.44	1.73	5.27
8
Under review as a conference paper at ICLR 2021
6	Conclusion
In this paper, we have developed the distributional perspective of the actor-critic framework which
integrates the SR(λ) method, Cramer distance, and Gaussian mixture models for improved Perfor-
mance in both discrete and continuous action spaces at a lower computational cost. Furthermore,
we show that our proposed method can capture the correct modality in the value distribution, while
the extension of the conventional method with the stochastic policy fails to do so.
Capturing the correct modality of value distributions can improve the performance of various policy-
based RL applications that exploit statistics from the value distribution. Such applications may
include training risk-sensitive policies and learning control tasks with sparse rewards that require
heavy exploration, where transient information from the value distribution can give benefit to the
learning process. We leave further development of these ideas as future works.
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. In ProceedingS of the 34th
International Conference on Machine Learning (ICML), 2017.
Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney, Dan Horgan, Dhruva TB,
Alistair Muldal, Nicolas Heess, and Timothy Lillicrap. Distributed distributional deterministic
policy gradients. In International COnference on Learning RePreSentatiOnS (ICLR), 2018.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. JOurnal of Artificial Intelligence ReSearch, 47:253-279,
2013.
Marc G. Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In PrOceedingS of the 34th InternatiOnal COnference on Machine Learning (ICML),
2017a.
Marc G. Bellemare, Ivo Danihelka, Will Dabney, Shakir Mohamed, Balaji Lakshminarayanan,
Stephan Hoyer, and Remi Munos. The cramer distance as a solution to biased wasserstein gradi-
ents, 2017b.
Marc G. Bellemare, Nicolas Le Roux, Pablo Samuel Castro, and Subhodeep Moitra. Distribu-
tional reinforcement learning with linear function approximation. In ArtificiaI Intelligence and
StatiStics, volume 89 of PrOceedingS OfMachine Learning ReSearch, pp. 2203-2211, 2019.
Richard Bellman. Dynamic Programming. Dover Publications, 1957.
Dimitri P. Bertsekas and John N. Tsitsiklis. NeUrO-Dynamic Programming. Athena Scientific, 1st
edition, 1996.
Y. Choi, K. Lee, and S. Oh. Distributional deep reinforcement learning with a mixture of gaussians.
In 2019 InternatiOnal COnference on Robotics and AUtOmatiOn (ICRA), pp. 9791-9797, 2019.
P. Cichosz. Truncating temporal differences: On the efficient implementation of TD(λ) for rein-
forcement learning. JOUrnal on Artificial Intelligence, 2:287-318, 1995.
Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
robotics and machine learning. http://pybullet.org, 2016-2020.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos. Implicit quantile networks for
distributional reinforcement learning. In PrOceedingS of the 35th InternatiOnal COnference on
Machine Learning (ICML), 2018a.
Will Dabney, Mark Rowland, Marc G. Bellemare, and Remi Munos. Distributional reinforcement
learning with quantile regression. In AAAI, pp. 2892-2901, 2018b.
Richard Dearden, Nir Friedman, and Stuart Russell. Bayesian q-learning. PrOceedingS of the
fifteenth national/tenth conference on ArtificiaI intelligence/Innovative applications of artificial
intelligence, pp. 761 - 768, 1998.
9
Under review as a conference paper at ICLR 2021
Jingliang Duan, Yang Guan, Shengbo Eben Li, Yangang Ren, and Bo Cheng. Distributional soft
actor-critic: Off-policy reinforcement learning for addressing value estimation errors. arXiv
Preprint arXiv:2001.02811, 2020.
Audrunas Gruslys, Will Dabney, Mohammad Gheshlaghi Azar, Bilal Piot, Marc Bellemare, and
Remi Munos. The reactor: A fast and sample-efficient actor-critic agent for reinforcement learn-
ing. In International ConferenCe on Learning RepreSentation (ICLR), 2017.
Matteo Hessel, Joseph Modayil, H. V. Hasselt, T. Schaul, Georg Ostrovski, W. Dabney, Dan Horgan,
B. Piot, Mohammad Gheshlaghi Azar, and D. Silver. Rainbow: Combining improvements in deep
reinforcement learning. In AAAI, 2018.
Peter J. Huber. Robust estimation of a location parameter. The AnnaIS OfMathematiCaI StatiStics,
35(1):73-101, 1964.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
PrOCeedingS of the 19th International Conference on MaChine Learning (ICML), 2002.
Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry P. Vetrov. Controlling overes-
timation bias with truncated mixture of continuous distributional quantile critics. arXiv preprint
arXiv:2005.04269, 2020.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. JOUrnaI OfMaChine Learning ReSearch, 17:39:1-39:40, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In
PrOCeedingS of the 33rd International COnferenCe on Learning RePreSentatiOnS (ICML), 2016.
Xiaoteng Ma, Qiyuan Zhang, Li Xia, Zhengyuan Zhou, Jun Yang, and Qianchuan Zhao. Distribu-
tional soft actor critic for risk sensitive learning. arXiv PrePrint arXiv:2004.14547, 2020.
Borislav Mavrin, Hengshuai Yao, Linglong Kong, Kaiwen Wu, and Yaoliang Yu. Distributional rein-
forcement learning for efficient exploration. In PrOCeedingS of the 36th International COnferenCe
on MaChine Learning (ICML), 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.
Parametric return density estimation for reinforcement learning. In UAI, pp. 368-375. AUAI
Press, 2010a.
Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya, and Toshiyuki Tanaka.
Nonparametric return distribution approximation for reinforcement learning. In PrOCeedingS of
the 27th International COnference on MaChine Learning (ICML), 2010b.
Nikolay Nikolov, Johannes Kirschner, Felix Berkenkamp, and Andreas Krause. Information-
directed exploration for deep reinforcement learning. In International COnferenCe on Learning
RePreSentatiOnS (ICLR), 2019.
Martin L. Puterman. MarkOV DeCiSiOn Processes: DiSCrete StOChaStiC DynamiC Programming. John
Wiley & Sons, Inc., 1st edition, 1994.
Mark Rowland, Marc G. Bellemare, Will Dabney, Remi Munos, and Yee Whye Teh. An analysis of
categorical distributional reinforcement learning. In Amos J. Storkey and Fernando Perez-CrUz
(eds.), ArtifiCiaI Intelligence and StatiStiCS (AISIATS), volume 84 of Proceedings of MaChine
Learning ReSearch, pp. 29-37. PMLR, 2018.
10
Under review as a conference paper at ICLR 2021
Mark Rowland, Robert Dadashi, Saurabh Kumar, Remi Munos, Marc G. Bellemare, and Will Dab-
ney. Statistics and samples in distributional reinforcement learning. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning (ICML), 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Proceedings of the 32nd International Conference on Machine Learning
(ICML), 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael I. Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. In International
COnferenCe on Learning RepreSentatiOnS (ICLR), 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv Preprint arXiv:1707.06347, 2017.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Rahul Singh, Keuntaek Lee, and Yongxin Chen. Sample-based distributional policy gradient. arXiv
PrePrint arXiv:2001.02652, 2020.
Richard S. Sutton. Learning to predict by the methods of temporal differences. MaChine Learning,
3(1):9-44, aug 1988. ISSN0885-6125. doi: 10.1023/A:1022633531479.
Richard S. Sutton and Andrew G. Barto. IntrOdUCtiOn to ReinfOrCement Learning. MIT Press,
Cambridge, MA,USA, 1stedition, 1998. ISBN0262193981.
Richard S. Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. In PrOCeedingS of the 12th International
COnferenCe on NeUral InfOrmatiOn PrOCeSSing SyStemS (NeUrIPS), Cambridge, MA, USA, 1999.
GaborSzekely. E-statistics: The energy of statistical samples. 10 2002. doi: 10.13140/RG.2.1.
5063.9761.
Harm van Seijen, Shimon Whiteson, Hado van Hasselt, and Marco Wiering. Exploiting best-match
equations for efficient reinforcement learning. JOUmal of MaChine Learning ReSearch, 12:2045-
2094, 2011.
C. J. C. H. Watkins. Learning from Delayed Rewards. PhD thesis, King,s College, Oxford, 1989.
R. J. Williams. Toward a theory of reinforcement-learning connectionist systems. Technical Report
NU-CCS-88-3, College of Comp. Sci., Northeastern University, Boston, MA, 1988.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. MaChine Learning, 8:229-256, 1992.
Derek Yang, Li Zhao, Zichuan Lin, Tao Qin, Jiang Bian, and Tie-Yan Liu. Fully parameterized
quantile function for distributional reinforcement learning. In 33rd AnnUal COnferenCe on NeUral
InfOrmatiOn PrOCeSSing SyStemS (NeUrIPS), pp. 6190-6199, 2019.
11
Under review as a conference paper at ICLR 2021
Appendices
A Discussion on the choice of Proximal Policy Optimization as a
BASELINE
A general learning process of RL can be described using policy iteration, which consists of two
iterative phases: policy evaluation and policy improvement (Sutton & Barto, 1998). In policy it-
eration, the value function is assumed to be exact, meaning that given policy, the value function is
learned until convergence for the entire state space, which results in a strong bound on the rate of
convergence to the optimal value and policy (Puterman, 1994).
But the exact value method is often infeasible from resource limitation since it requires multiple
sweeps over the entire state space. Therefore, in practice, the value function is approximated, i.e.
it is not trained until convergence nor across the entire state space on each iteration. The approxi-
mate version of the exact value function method, also known as asynchronous value iteration, still
converges to the unique optimal solution of the Bellman optimality operator. However, the Bellman
optimality only describes the limit convergence, and thus the best we can practically consider is to
measure the improvement on each update step.
Bertsekas & Tsitsiklis (1996) have shown that, when we approximate the value function Vπ of some
policy π with V , the lower bound of a greedy policy π0 is given by
V∏0(X) ≥ VΠ(X)- ^τ~^~一,	QI)
1-γ
where ε = maxχ∣V(x) - Vπ(x)| is the L∞ error of value approximation V. This means a greedy
policy from an approximate value function guarantees that its exact value function will not degrade
more than 12--Y. However, there is no guarantee on the improvement, i.e. V∏o (x) > Vn (x) (Kakade
& Langford, 2002).
As a solution to this issue, Kakade & Langford (2002) have proposed a policy updating scheme
named conservative policy iteration,
∏new(a|x) = (1 - α)∏oid(a∣x) + απ0(a∣x),	(22)
which has an explicit lower bound on the improvement
η(πnew )
≥ Lπold (πnew ) -
2eγ
(1-Y2
α2 ,
(23)
where C = maxχ ∣E∏o [A∏(x,a)]∣, A∏(x, a) = Q(x,a) - V(x) is the advantage function, η(π)
denotes the expected sum of reward under the policy π,
∞
η(π) = E XγtR(Xt,at)
t=0
(24)
and Lπold is the local approximation ofη with the state visitation frequency under the old policy.
From the definition of distributional Bellman optimality operator in equation 5, one can see that the
lower bound in equation 23 also holds when π 0 is greedy with respect to the expectation of the value
distribution, i.e., Ex，〜P[Z(x0, a0)]. Thus the improvement of the distributional Bellman update is
guaranteed in expectation under conservative policy iteration, and the value functions are guaranteed
to converge in distribution to a fixed point by γ-contraction.
Schulman et al. (2015) takes this further, suggesting an algorithm called trust region policy optimiza-
tion (TRPO), which extends conservative policy iteration to a general stochastic policy by replacing
α with Kullback-Leibler (KL) divergence between two policies,
DmLx(∏,∏) = maxDKL (∏(∙∣x)k∏(∙∣x)).	(25)
x
Then, the newly formed objective is to maximize the following, which is a form of constraint opti-
mization with penalty:
Et [t⅛)At-βDKL(π(M),WB))] = Et hrt(π)At-βDKL(π(∙lxt),π(M))i . (26)
12
Under review as a conference paper at ICLR 2021
where r(∏) refers to the ratio r(∏) = 累；|：；). However, in practice, choosing a fixed penalty
coefficient β is difficult and thus Schulman et al. (2015) uses hard constraint instead of the penalty.
maxEt [rt(∏)^l∕∣	(27)
s.t. DκL(∏(∙∣xt),∏(∙∣xt)) ≤ δ	(28)
Schulman et al. (2017) simplifies the loss function even further in proximal policy optimization
(PPO) by replacing KL divergence with ratio clipping between the old and the new policy with the
following:
LCLIP = Et [min }t(∏)At, clip(rt(∏), 1 - e, 1 + c)At)] .	(29)
Thus, by using PPO as the baseline, we aim to optimize the value function via unique point con-
vergence of distributional Bellman operator for a policy being approximately updated under the
principle of conservative policy.
B EXPECTATION VALUE OF Zt(λ)
Continuing from equation 12, let us define a random variable that has a cumulative distribution
function of E[Fz] as Z(λ). Then, its cumulative distribution function is given by
∞
FZ(λ) = (1 -λ)Xλn-1FZ(n).
n=1
Ifwe assume that the support of Zt(λ) is defined in the extended real line [-∞, ∞],
E[Zt(λ)] =Z ∞ 1- FZ(λ)dz-Z 0 FZ(λ
Z0∞
∞
1 - (1 - λ) X λn-1 FZ(n)
) dz
!0∞
dz -	(1 - λ)	λn-1FZt(n)dz
-∞	n=1	t
1 - FZ(n)dz-	0 FZ(n)dz
n=1
∞
0
∞
(1-λ)Xλn
n=1
(30)
(31)
(32)
(33)
(34)
Thus we can arrive at the desired expression of E[Zt(λ)] = G(tλ).
C Distributional BELLMAN operator AS A contraction in CRAMER
METRIC SPACE
The Cramer distance possesses the following characteristics (detailed derivation of each can be
found in (Bellemare et al., 2017b)):
lp(A+X,A+Y) ≤lp(X,Y),
lp(cX, cY) ≤ |c|1/plp(X, Y).	(35)
Using the above characteristics, the Bellman operator in lp divergence is
lp (TπZ1 (x, a), TπZ2(x, a)) = lp(R(x, a) + γPπZ1 (x, a), R(x, a) + γPπZ2(x, a))
≤ |Y|1/plp(PπZι(x,a),PπZ(x,a))
≤ ∣γ∣1∕p sup lp(Zι(x0, a0), Z2(x0, a0)).
x0,a0
(36)
13
Under review as a conference paper at ICLR 2021
Substituting the result into the definition of the maximal form of the Cramer distance yields
Zp(TnZι,Tπ Z2)=sup lp(TπZι(x,a),Tπ Z2(x,a))
x,a
≤ Iγ|1/p sup lp(Z1(x0, a0), Z2(x0, a0))	(37)
x0,a0
=∣γ ∣1∕% (Z1,Z2).
Thus the distributional Bellman operator isa | γ∣ 1/p-contraction mapping in the Cramer metric space,
which was also proven in Rowland et al. (2019).
Similar characteristics as in equation 35 can be derived for the energy distance
E(A+X,A+Y) ≤ E(X,Y),	E(cX,cY) = cE(X,Y),	(38)
showing that the distributional Bellman operator is a γ-contractor in energy distance
E(TπZ1,TπZ2) ≤γE(Z1,Z2).	(39)
D Loss Functions
As in other policy gradient methods, our value distribution approximator models the distribution of
the value, V (xt), not the state-action value Q(xt, at), and denote it as Zθ(xt) parametrized with θ,
whose cumulative distribution function is defined as
FZθ(xt) =	π(a, xt)FZ(xt,a).	(40)
a∈A
Below, we provide the complete loss function of value distribution approximation for each of the
cases used in experiments (Section 5).
D.1 Implicit Quantile + Huber quantile (IQAC)
For the value loss of IQAC, we follow the general flow of Huber quantile loss described in Dabney
et al.(2018b). For two random samples τ,τ0 〜U([0,1]),
δτ,τ = Z(λ)(xt,at; T0) - Zθ(xt； T)	(41)
where Zt(λ) is generated via SR(λ) and Z(x; τ) = FZ-1(τ) is realization of Z(X) given X = x and
T. Then, the full loss function of value distribution is given by
N N0	0
LZθ =而 XX PKi (δτi,τj)	(42)
i=1 j=1
where N andN 0 are number of samples of T, T0, respectively, and ρ is the Huber quantile loss
ρT(δij) = |t - I{δj < 0}|L(δj, with
κ
L (δ..) = ∫ 2δ2j,	ifMij l≤ K
K j [κ(∣δij| - 1 κ), otherwise.
(43)
(44)
D.2 Implicit Quantile + Energy Distance (IQAC-E)
Here, we replace the Huber quantile loss in equation 42 with sample-based approximation of energy
distance defined in equation 19.
LZθ
W ,τi0∣- N x x δTj00,τj
j=1 j0=1
N0
N0
(45)
14
Under review as a conference paper at ICLR 2021
D.3 Gaussian Mixture + Energy Distance (GMAC)
Unlike the two previous losses, which use samples at τ generated by the implicit quantile network
Zθ(xt; τ), here we discuss a case in which the distribution is k-component Gaussian mixture param-
eterized with g, σk ,wk).
Using the expectation of a folded normal distribution, we define δ between two Gaussian distribu-
tions as
δ(μi,σ2,μj ,σ2)
((μi - μj)2
I- 2(σ2 + σ2)
+ (μi - μj)
Let Zθ(x) and Z(λ) be Gaussian mixtures parameterized with (μθi, σ& wθi), (μλj,σ2j, wλj), re-
spectively. Then, the loss function for the value head is given by
2 N N0
LZe = NN0 ΣΣwθiwλj δ(μθi, σθi, μλj ,σ2j)
NN i=1 j =1
NN
-wɪɪWθiWθi0 δ(μθi, σ2, μθi, Q^)	(47)
i=1 i0=1
N0 N0
-N ∑∑wλj wλj0 δ(μλj , σλj, μλj0, σλj0 ) .
N j=1 j0=1
15
Under review as a conference paper at ICLR 2021
E Pseudocode of GMAC
Algorithm 2: GMAC
Input: Initial policy parameters θo, initial value function parameters φo, length of trajectory T,
number of environments E, clipping factor , discount factor γ, weight parameter λ
for k = 0, 1, 2, . . . do
for e = 1, . . . , E do
Collect samples of discounted sum of rewards {Z1, . . . , ZT} by running policy
πk = π(θk ) in the environment
Compute the parameters (μi,σi,wi) for each of the λ-returns {Z(λ),..., zTλ-1} by
SR(λ) (Algorithm 1)
Compute advantage estimates At using GAE (Schulman et al., 2016), based on the
current value function Vφk
end
Gather the data from E environments
Update policy using the clipped surrogate loss:
θk+ι = arg max E min (五"，Ist) At, g(g At))
θ π	∖θkθa (at|st)	力
via stochastic gradient ascent.
Update value function using the energy distance between Gaussian mixtures (Equation 19):
φk+1 = arg min E hE Vφ(st), Zt(λ)i
via stochastic gradient descent.
end
The clipping function g(, A) shown in the algorithm is defined as follows:
(A)=(1+)A ifA≥0
g(, A) = (1-)A ifA< 0
Note that expectation of each loss is taken over each the collection of trajectories and environments.
16
Under review as a conference paper at ICLR 2021
F Implementation Details
For producing a categorical distribution, a softmax layer was added to the output of the network. For
producing a Gaussian mixture distribution, the mean of each Gaussian is simply the output of the
network, the variance is kept positive by running the output through a softplus layer, and the weights
of each Gaussian is produced through the softmax layer. Since our proposed method takes an archi-
Table 2: Network architecture for GMAC on atari
Layer Type	Specifications	Filter size, stride
Input	84x84x4		
Conv1	20 x 20 x 32		8 x 8 x 32, 4
Conv2	9 x 9 x 64		4x4x64, 2
Conv3	7 x 7 x 32		3 x 3 x 32, 1
FC1		512	
Heads	Policy	Value	
一 一（FC厂	action dim	# of modes (= 5)	
tecture which only changes the value head of the original PPO network, we base our hyperparameter
settings from the original paper (Schulman et al., 2017). We performed a hyperparameter search on
a subset of variables: optimizers={Adam, RMSprop}, learning rate={2.5e-4, 1.0e-4}, number of
epochs={4, 10}, batch size={256, 512}, and number of environments={16, 32, 64, 128} over 3
atari tasks of Breakout, Gravitar, and Seaquest, for which there was no degrade in the performance
of PPO.
Table 3: Parameter settings for training atari games
Parameter	PPO	IQAC IQAC-E	GMAC
Learning rate		2.5e-4	
Optimizer		Adam	
Total frames		2e8	
Rollout steps		128	
Skip frame		4	
Environments		64	
Minibatch size		512	
Epoch		4	
γ		0.99	
λ		0.95	
Dirac samples	-	64	-
Mixtures	-	--	5
Table 4: Parameter settings for training PyBullet games
Parameter	PPO	IQAC IQAC-E	GMAC
Learning rate		1e-4	
Optimizer		Adam	
Total env steps		5e-7	
Rollout steps		512	
Skip fram		1	
Environments		64	
Minibatch size		2048	
Epoch		10	
γ		0.99	
λ		0.95	
Dirac samples	-	64	-
Mixtures	-	--	5
17
Under review as a conference paper at ICLR 2021
G More Experimental Results
G.1 Five-state MDP
(a) Quantile Regression
(b) Expectile Regression
Figure 4: In addition to 2, quantile and expectile regressions are also evaluated in the 5-state MDP
with imputation using neural networks architectures of implicit quantile network against IQE and
Gaussian mixture with their respective loss functions.
(d) Huber Quantile Regression
(e) Quantile Regression
(f) Expectile Regression
Figure 5: Evaluation of the five-state MDP under tabular setting on a symmetric reward distribution
(top row) and on assymetric reward distribution (bottom row). Huber quantile(κ = 1), quantile,
and expectile regressions are compared to the energy distance minimization between samples and
Gaussian mixtures.
Here we provide more details on the five-state MDP presented in Figure 2. For each cases in the
figure, 15 diracs are used for quantile based methods and 5 mixtures are used for GMM to balance
the total number of parameters required to represent a distribution. For the cases with the label
“nalve”，the network outputs (quantiles, expectiles, etc.) are used to create the plot. On the other
hand, the cases with ”imputation” labels apply appropriate imputation strategy to the statistics to
produce samples which are then used to plot the distribution. In all cases, the network is trained on
full batches of all states for 8 different empirical samples with Adam optimizer and learning rate
of 1e-3. Sample based energy-distance was used to calculate the distance from the true distribution
for all cases. The tabular setting of five-state MDP in Figure 5 uses same exact experimental set-up
as the one with neural network. The only difference is that the statistics are from evenly spaced
quantile/expectile targets τ ∈ [0, 1].
18
Under review as a conference paper at ICLR 2021
G.2 Value distribution learning in Atari games
(a) Breakout-v4
(c) Seaquest-v4
(b) Breakout-v4
Figure 6: More value distributions of different tasks. All states are chosen such that the agent is
in place of near-death or near positive score. Thus, when the policy is not fully trained, such as
in a very early stage, the value distribution should include a notion of death indicated by a mode
positioned at zero. In all games, IQN + Huber quantile (IQAC) fails to correctly capture a mode
positioned at zero while the other two methods, IQN + energy distance (IQAC-E) and GMM +
energy distance (GMAC) captures the mode in the early stage of policy improvement. Again, the
visual representation is maxpool of the 4 frame stacks in the given state.
19
Under review as a conference paper at ICLR 2021
G.3 Learning curves
(a) BeamRider
(b) Breakout
(c) Enduro
(d) Gopher
(e) Gravitar
(f) Montezuma’s Revenge
Figure 7: Raw learning curves for 8 selected atari games.
20
Under review as a conference paper at ICLR 2021
(d) Ant
(e) Humanoid
Figure 8: Raw learning curves for 5 selected PyBullet continuous control tasks.
21
Under review as a conference paper at ICLR 2021
MsPacman
StarGunner
Figure 9: Full learning curves of 61 atari games from ALE
Jamesbonc
IQAC-E
GMAC
CarniV a∖	
	
	
EIevatorAction	
f	
Gravitar	
Z	
Sτiii	
	
L	

BeamRider

Centipede

Enduro
片
Hero
_l'
KungFuMaster

Pong

Robotank

Tennis
*
Ul WizardofWor
22
Under review as a conference paper at ICLR 2021
Table 5: Average score over last 100 episodes in 200M frame collected for training 61 atari games.
The algorithms are trained using same single seed and hyperparameters. Random and Human scores
are taken from Wang et al.
GAMES	RANDOM	HUMAN	PPO	IQAC-E	GMAC
Adventure	NA	NA	0.00	0.00	0.00
AirRaid	NA	NA	10,205.75	7,589.50	62,328.75
Alien	227.80	7,127.70	2,918.60	2,704.20	3,687.10
Amidar	5.80	1,719.50	1,244.12	932.11	1,363.72
Assault	222.40	742.00	7,508.03	8,589.55	10,281.73
Asterix	210.00	8,503.30	13,367.00	15,426.00	22,650.00
Asteroids	719.10	47,388.70	2,088.10	2,332.00	2,597.50
Atlantis	12,850.00	29,028.10	3,073,796.00	3,373,635.00	3,141,534.00
BankHeist	14.20	753.10	1,263.80	1,286.60	1,274.30
BattleZone	2,360.00	37,187.50	18,540.00	21,310.00	32,490.00
BeamRider	363.90	16,926.50	5,913.84	6,507.68	8,718.72
Berzerk	123.70	2,630.40	1,748.10	887.50	3,081.20
Bowling	23.10	160.70	33.54	30.00	19.39
Boxing	0.10	12.10	96.79	97.10	99.89
Breakout	1.70	30.50	384.29	445.64	462.68
Carnival	NA	NA	5,079.20	4,401.00	6,344.20
Centipede	2,090.90	12,017.00	5,205.25	4,864.69	4,303.10
ChopperCommand	811.00	7,387.90	872.00	1,314.00	1,795.00
CrazyClimber	10,780.50	35,829.40	112,640.00	121,550.00	125,143.00
DemonAttack	152.10	1,971.00	50,590.65	236,839.85	411,118.85
DoubleDunk	-18.60	-16.40	-3.26	-8.28	-2.72
ElevatorAction	NA	NA	10,449.00	8,516.00	14,254.00
Enduro	0.00	860.50	1,588.68	1,612.17	2,092.65
FishingDerby	-91.70	-38.70	37.01	33.13	37.52
Freeway	0.00	29.60	32.53	33.68	32.84
Frostbite	62.50	4,334.70	3,571.50	307.10	3,392.40
Gopher	257.60	2,412.50	8,199.80	16,934.60	25,266.80
Gravitar	173.00	3,351.40	1,151.50	2,178.50	2,401.00
Hero	1,027.00	30,826.40	37,725.55	43,065.95	41,509.05
IceHockey	-11.20	0.90	-1.90	2.13	0.34
Jamesbond	29.00	302.80	642.50	961.00	1,512.00
JourneyEscape	NA	NA	-607.00	-840.00	-680.00
Kangaroo	52.00	3,035.00	1,742.00	12,208.00	12,909.00
Krull	1,598.00	2,665.50	9,605.51	9,514.03	9,127.63
KungFuMaster	258.50	22,736.50	26,846.00	33,378.00	31,025.00
MontezumaRevenge	0.00	4,753.30	0.00	0.00	0.00
MsPacman	307.30	6,951.60	3,674.20	4,699.00	3,884.40
NameThisGame	2,292.30	8,049.00	13,229.10	13,454.00	14,031.30
Phoenix	761.40	7,242.60	37,263.70	26,154.00	42,664.00
Pitfall	-229.40	6,463.70	0.00	-18.86	-3.36
Pong	-20.70	14.60	20.87	20.88	20.97
Pooyan	NA	NA	4,018.95	3,674.70	4,178.65
PrivateEye	24.90	69,571.30	100.00	196.30	100.00
Qbert	163.90	13,455.00	25,519.25	21,599.50	23,176.25
Riverraid	1,338.50	17,118.00	15,983.00	18,073.40	19,761.30
RoadRunner	11.50	7,845.00	56,321.00	56,121.00	68,272.00
Robotank	2.20	11.90	23.45	36.69	45.82
Seaquest	68.40	42,054.70	1,832.00	1,814.60	1,838.40
Skiing	-17,098.10	-4,336.90	-7,958.81	-29,971.02	-29,975.52
Solaris	1,236.30	12,326.70	2,452.80	2,204.80	2,579.20
SpaceInvaders	148.00	1,668.70	2,544.10	2,410.90	2,228.30
StarGunner	664.00	10,250.00	74,848.00	97,450.00	104,188.00
Tennis	-23.80	-8.30	-8.16	-7.54	-5.90
TimePilot	3,568.00	5,229.20	12,157.00	11,704.00	13,227.00
Tutankham	11,40	167.60	206.32	208.72	209.82
UpNDown	533.40	11,693.20	158,629.50	161,328.40	129,243.70
Venture	0.00	1,187.50	0.00	1,339.00	1,181.00
VideoPinball	16,256.90	17,667.90	279,504.81	59,988.90	55,272.82
WizardOfWar	563.50	4,756.50	8,749.00	9,165.00	11,388.00
YarsRevenge	3,092.90	54,576.90	92,709.94	100,082.55	103,895.05
Zaxxon	32.50	9,173.00	13,336.00	14,882.00	18,436.00
23