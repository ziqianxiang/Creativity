Under review as a conference paper at ICLR 2021
Understanding How Over-Parametrization
Leads to Acceleration: A case of learning a
SINGLE TEACHER NEURON
Anonymous authors
Paper under double-blind review
Ab stract
Over-parametrization has become a popular technique in deep learning. It is ob-
served that by over-parametrization, a larger neural network needs a fewer training
iterations than a smaller one to achieve a certain level of performance — namely,
over-parametrization leads to acceleration in optimization. However, despite that
over-parametrization is widely used nowadays, little theory is available to explain
the acceleration due to over-parametrization. In this paper, we propose under-
standing it by studying a simple problem first. Specifically, we consider the set-
ting that there is a single teacher neuron with quadratic activation, where over-
parametrization is realized by having multiple student neurons learn the data gen-
erated from the teacher neuron. We provably show that over-parametrization helps
the iterate generated by gradient descent to enter the neighborhood of a global op-
timal solution that achieves zero testing error faster. On the other hand, we also
point out an issue regarding the necessity of over-parametrization and study how
the scaling of the output neurons affects the convergence time.
1	Introduction
Over-parametrization has become a popular technique in deep learning, as it is now widely observed
larger neural nets can achieve better performance. Furthermore, a larger network can be trained to
achieve a certain level of prediction performance with fewer iterations than that of a smaller net. This
observation, to our knowledge, can be dated back as early as the work of Livni et al. (2014), who try
different levels of over-parametrization and report that SGD converges much faster and finds a better
solution when it is used to train a larger network. However, the reason why over-parametrization
can lead to an acceleration still remains a mystery, and very little theory has helped explain the ob-
servation, with perhaps the notable exception of (Arora et al., 2018). Arora et al. (2018) consider
over-parametrizing a single-output linear regression with lp loss for P > 2-the square loss corre-
sponds to p = 2-and they study the linear regression problem by replacing the model W ∈ Rd by
another model w1 ∈ Rd times a scalar w2 ∈ R. They show that the dynamics of gradient descent on
the new over-parametrized model are equivalent to the dynamics of gradient descent on the original
objective function with an adaptive learning rate plus some momentum terms. However, in practice,
people actually use the techniques of over-parametrization, adaptive learning rate, and momentum
simultaneously in deep learning (see e.g. (Hoffer et al., 2017; Kingma & Ba, 2015; Loshchilov &
Hutter, 2019; Lucas et al., 2019; Sutskever et al., 2013)), as each technique appears to contribute
to performance and they may, to some extent, be complementary. It has been suggested that over-
parameterizing a model leads implicitly to an adaptive learning rate or momentum, but this does not
appear to fully explain the performance improvement.
To understand the benefits of overparameterization, let us begin by studying a simple canonical
problem: a single teacher neuron w* ∈ Rd with quadratic activation function. Specifically, the label
yi and the design vector Xi 〜N(0, Zd) of sample i satisfies yi = (χ> w* )2. Therefore, the standard
objective function for learning the teacher neuron w* is
minw∈Rdf (W) := 4n Pn=I ((X>w)2 — yi)2
(1)
where n denotes the number of samples. Problem (1) is called phase retrieval in signal processing
literature, which has real applications in physical science such as astronomy and microscopy (see
1
Under review as a conference paper at ICLR 2021
(a) Obj. (2) on training data. (b) Obj. (2) on testing data. (C) dist(W#K, w*) vs. t.	(d) Quantity (9) over t.
Figure 1: In the experiment, we set the dimension d = 10 and the number of training samples n = 200.
Additional 200 samples are sampled and served as testing data. We let w* = e1 with e1 being the unit
veCtor. EaCh neuron w(k) ∈ Rd (k ∈ [K]) of the student network is initialized by sampling from an isotropiC
distribution and is close to the origin (i.e. WOk)〜0.01 ∙ N(0, Id/d)). We apply gradient descent with the same
step size η = 0.001 to train different sizes of neural networks. EaCh Curve represents the progress of gradient
descent for different K. Subfigure (a) and (b): Objective value (2) vs. iteration t on training data (testing data,
respectively). For subfigure (c) and (d), please see Section 3 for the precise definition and details.
e.g. (CandeS et al., 2013), (Fannjiang & Strohmer, 2020), (Shechtman et al., 2015)). There are also
some specialized algorithms designed for achieving a better sample complexity or computational
complexity to recover w* modulo the unrecoverable sign (e.g. (CandeS & Li, 2014; CandeS et al.,
2015; 2013; Chen & Candes, 2017; Ma et al., 2017)). We choose this problem as a starting point
of understanding acceleration due to over-parametrization. Specifically, we consider the following
way to over-parametrize the original objective (1),
minW∈Rd×κ f (W) := 4n Pn=I ((X>W⑴产 + (X>W⑵尸 + ... + (X>W(K))2 - yi)2,⑵
where w(j) denote the jth column of the weight matrix W ∈ Rd×K . Optimizing the objective func-
tion (2) can be viewed as training a student network with K student neurons. While a d dimensional
model exists which perfectly predicts the labels generated by the teacher neuron (i.e. ±W*), one
can consider training a much larger model instead (i.e. d × K number of parameters). Note that if
K = 1, objective (2) reduces to the original objective (1). On the other hand, a larger k > 1 means
a higher degree of over-parametrization.
Let us now establish, empirically, the clear advantage of over-parametrization for accelerating the
learning and optimization process. We tried K = {1, 3, 10} number of student neurons in (2),
which represent different degrees of over-parametrization, and we applied gradient descent to train
the networks. Figure 1 shows the results. The empirical findings displayed on the figure are quite
stark. First, we see that over-parametrization not only helps to decrease the training error faster but
also decrease the testing error faster (c.f. subfigure (a) and (b)). Furthermore, the generalization
error is very small since both training error and testing error approach zero. Second, regardless
of the size K, a common pattern is that the dynamics of gradient descent can be divided into two
stages. In the first stage, gradient descent makes little progress on decreasing the function value;
while in the second stage, the iterate generated by gradient descent exhibits a linear convergence to
a global solution. Specifically, by over-parametrization, gradient descent spends fewer iterations in
the first stage and enters the linear convergence regime that makes the fast progress more quickly.
We provide more results in Appendix F. Specifically, we also tried different values of the step size η
and we observed similar patterns as Figure 1 shows. Even when gradient descent uses the best step
size for each model with different neurons K , we still observe that gradient descent enters the linear
convergence regime faster for a larger model. Thus, the acceleration due to over-parametrization
cannot be simply explained by that gradient descent uses a larger effective step size, as the effect
due to parameters η and K is complementary in the experiment.
In the later sections, we will answer why an over-parametrized network trained by gradient de-
scent can still generalize well and why over-parametrization helps gradient descent to enter the
linear convergence regime faster. We will also point out an issue regrading the necessity of over-
parametrization in the end.
2
Under review as a conference paper at ICLR 2021
2	Related works
Over-parametrization: Though our work focuses on understanding why over-parametrization
leads to acceleration in optimization (i.e. improving the convergence time), we also want to ac-
knowledge some related works of understanding over-parametrization in different aspects (e.g.
(Arora et al., 2019; Brutzkus & Globerson, 2019; Emschwiller et al., 2020; Goldt et al., 2019; Tian,
2020) and have a brief review in Appendix A. There is also a trend of works studying how over-
parametrization changes the optimization landscape of empirical risk minimization for neural nets
with quadratic activation. (e.g. (Du & Lee, 2018; Gamarnik et al., 2019; Ge et al., 2019; Kazemipour
et al., 2019; Soltanolkotabi et al., 2018; Nguyen & Hein, 2017; Venturi et al., 2019; Mannelia et al.,
2020)). The goals of these works are different from ours. We provide more details in Appendix A.
Quadratic activation and matrix sensing: The optimization landscape of problem (1) (i.e. phase
retrieval) and its variants has been studied by (Davis et al., 2018; Soltanolkotabi, 2014; Sun et al.,
2016; White et al., 2016), which shows that as long as the number of samples is sufficiently large,
it has no spurious local optima and all the local optima are globally optimal. Chen et al. (2019)
provably show that applying gradient descent with an isotropic random initialization for solving (1)
leads to an optimal solution that recovers the teacher neuron w* modulo the unrecoverable sign. In
this work we show that an over-parametrized student network trained by gradient descent takes even
fewer iterations to recover w* . We also note that the optimization problem (2) can be rewritten as the
form of matrix sensing (see e.g. Gunasekar et al. (2017); Li et al. (2019; 2018); Gidel et al. (2019)).
In Appendix A, we provide a brief review of matrix sensing.
Learning a single neuron: Studying learning a single neuron in non-convex optimization is not
new (e.g. Goel et al. (2019); Yehudai & Shamir (2020); Kakade et al. (2011); Goel et al. (2017);
Kalan et al. (2019); Soltanolkotabi (2017); Mei et al. (2018); Frei et al. (2020)). However, those
works are not for showing faster convergence by over-parametrization. The goals are different.
3	Preliminaries
Notations and assumptions: We use the notation W#K := [w#(1K) , . . . , w#(KK)] ∈ Rd×K to represent
the weights of a student network with K number of neurons. Each column k of the matrix W#K
is the weight vector w#(kK) ∈ Rd that corresponds to the neuron k of the student network. Thus,
W#1 := [w#(11)] ∈ Rd is the network consists of a single neuron; while for K > 1, the notation
represents the weights of an over-parametrized network. In this paper, without loss of generality, we
assume that w* = kw* ke1 ∈ Rd with e1 being the unit vector whose first element is 1. We define
the parallel component w#kKkt and the perpendicular component w#k"： in iteration t as follows,
w
w
(k),k
#K,t :
(k),⊥
#K,t :
hw#(kK),t,w*i = kw*kw#(kK),t[1]
[w#K,t [2], . . . , w#K,t [d]]> .
(3)
Namely, the parallel component w#(kK),,kt is the projection of a student neuron k learned in itera-
tion t on the teacher neuron w*; while the perpendicular component w#k"： is the d-1 dimensional
(k)
sub-vector of w#K,t excluding the first element. We assume that each neuron k of each network,
w#(kK) ,0 ∈ Rd, is initialized i.i.d. randomly from an isotropic distribution (e.g. gaussian distribution).
Metric of the progress in optimization: The first challenge to show that over-parametrization leads
to acceleration in optimization is the design of the metric for the comparison. Since different K’s
corresponds to different optimization problems, the notion of acceleration here is non-standard in
optimization literature. In the optimization literature, acceleration usually means that an algorithm
takes a fewer iterations than other algorithms for the same optimization problem. Fortunately, by
exploiting the problem structure, we can have a natural metric of the progress as follows. For any
size of the student network W ∈ Rd×K , we consider
dist(W,w*) := minq∈REkq∣∣2≤ι kW - w*q>k∙
(4)
This is due to the observation that for any K, the global optimal solutions of (2) that achieve zero
testing error are w*q> ∈ Rd×K for any q ∈ RK such that kqk2 = 1. To see this, substitute
3
Under review as a conference paper at ICLR 2021
W = w*q> ∈ Rd×K into (2). We have that for any Xi ∈ Rd it holds that (x>w(1))2 + (x>w(2))2 +
...+ (x>w(K))2 - y = ∣∣x> W kF - (x>w*)2 = tr((x>w*q>)>(x>w*q>)) - (x>w*)2 = 0.
Therefore, the metric dist(W,w*) as be viewed as a surrogate of the testing error. In particular,
dist( Wt, w*) represents the distance of the current iterate Wt and its closest global optimal solution
to the over-parametrized objective (2) that achieves zero testing error. Note that the argmin of (55)
is q* ：= kW>W**k2 = argminq∈REkq∣∣2≤ι ∣W - w*q>∣∣. On sub-figure (c) of Figure 1, We plot
the distance of the iterates generated by gradient descent and its closet global optimal solution for
different sizes K of neural nets. We see that over-parametrization enables shrinking the distance
dist(Wt#K, w*) faster.
Gradient descent dynamics: For the notation brevity, we will suppress the symbol #K when it is
clear in the context. The gradient ofa student neuron w(k) for the over-parametrized problem (2) is
Vw(k)f (W) := n Pi=ι ((Xirw⑴产 + (XirW⑵产 +----------+ (Xirw(K))2 - y,(Xirw(W))xi.	⑸
Its expectation, which is the population gradient of a student neuron w(k) (i.e. gradient when the
number of samples n is infinite), is
E	[Vw(k)f(W)] =(3∣w(k)k2 -kw*k2)w(k) - 2(w>w(k))w*
X〜N (0,Id)
+ PjK6=k 2((w(j))rw(k))w(j) + kw(j) k2w(k),	(6)
where we use the fact that for any vector u,v ∈ Rd, Ex〜N(o,id)[(X>u)3X] = 3∣∣u∣2u and
Ex〜N(0,id)[(x>u)2(x>v)x] = 2(u>v)u + ∣∣u∣2v. For K = 1, (6) becomes E[Vw(i)f (W#1)]=
3kw(1) k2 - kw* k2 w(1) - 2(w*rw(1))w*. If gradient descent uses the population gradient for the
update (i.e. w#(11),t+1 = w#(11),t - ηE[Vw(1) f (Wt#1)]), then the dynamics of the student network
consists of a single neuron (i.e. K = 1) evolves as follows,
w(+)1k = w(1y(1 + η(3kw*k2 - 3kw(1)k2))	⑺
w(+)1⊥ = w(1),⊥(1 + η(kw*k2 - 3kw(1)k2)).
On the other hand, if a student network has K > 1 neurons, the dynamics of each neuron k of the
student network evolves as follows,
w
(k),k
t+1
wt(k),k
K
(1 + 3η(kw*k2 - kw(k)
component A
K
+ ηwt(k),k X ∣wt(j)∣2)
j6=k
/
component B
KK
w(+)1⊥ = w(k),⊥(1 + η(kw*k2 - 3kw(k)k2))-(2ηXhwj),w(k)iw(j),⊥ + ηw(k),⊥Xkw(j)k2)
'	-{z	}	j = k	j = k
component C	、	 {z 	/
component D
(8)
where both the component B of wt(+k)1,k and the component D of wt(+k)1,⊥ can be viewed as the terms
due to the interaction of student neuron k and the other student neurons.
More observations: On Subfigure (d) of Figure 1, we plot a quantity over iterations, which is
vec(w*qtr -Wt#K)rV2f(Wt#K)vec(w*qtr -Wt#K),	(9)
where V2f(Wt#K) ∈ RdK×dK is the Hessian and w*qtr is the closet global optimal solution to
W#K and the notation vec(∙) represents the vectorization operation of its matrix argument. The
quantity can be viewed as a measure of the strong convexity. Specifically, if the quantity is larger
than 0, then it suggests that the current optimization landscape is strongly convex with respect to
w* qtr . Hence, the observation suggests that gradient descent enters a benign region faster for a
larger student network. In the following section, We will answer why over-parametrization helps
gradient descent to enter a region that has the benign optimization landscape faster.
4
g
=(*e"e)ps!p mqι 乂 > IieIIleqIPUe/C > ∣∣∣*σι∣∣ - ∣[χ]^∣∣ jτ jBψ əjo^ '{L > ∣∣^σι∣∣ p∏B /C
> IIl*σι∣∣ — ∣[χ]^σι∣∣ : 3}uμu =: ljj əugəQ ,Sutmo∏oj əqj ut [# uo∏bjou əqj ssəjddns əM ςX∏ΛQjq
joj -uojnəu əɪgms e Seq Xjuo 平OM]əu juəpnjs əqj ]叫]əseɔ əqj əzAeUe əM ςuoτpgsqns sτqj Ul
6NOH∩HN HlDNIS
V JO SJJSlSNθɔ XXoAVLHN JJNHCmJJS HHl HOJ XXoM JJNHDSHcl 1NHIGVHD SHOG MOH CT
,STSXpUB JBUIJOJ
B əpɪʌojd ɪɪɪʌk əm ςsuo∏θQsqns jəjbj əqj Ul 'ɪəʌəɪ urejjəɔ əAəppe oj	RC uonɔə[ojd
əje缸函e joj *σι uo uonɔə[ojd jəɪpms e spəəu Xjuo (邛严 gɔeə F suojnəu juəpnjs əjoui ψτM
]eq] SMoqS əmgg ə^ɪ Z əm/H uo juəɔsəp ]uə[PeJg Xq pəupjj SXJoM]əu juəpnjs juəjəjjɪp Jo [jʃ] ə 斗
gɔeə joj K*σι"露@ I = | [濡皿 səHHUenb əqj joɪd əM 'uəum击e sτqj UoddnS oɪ ∙(χ = X) uojnəu
juəpnjs əɪgms e Seq Xjuo əuo uəqm əseɔ əm oj pəieduɪoɔ ' 1b UnoJSUe用 e oj dn *σι uojnəu jəqobəj
oq] Oj əsoɪɔ ^puəpgjns oq 血 QAeq oj jəpjo uτ *aι Jo uopɔəjɪp əm uo juəuodmoɔ jəɪpms e QAeq oj
spəəu Xjuo JfXP过 3 ½j jo 0过 3(邛严 uojnəu gɔeə ^ɪpɔtjpəds ,χ < jʃ suojnəu juəpnjs Jo JQqmnu
jə击引 B SuτΛBq Xq pəgsHeS Xjtsbq Qq 叫8μιι PUe juəɔsəp ]uə[PeB Jo əɔuə加əAUoɔ jbqu∏ ^ɪpɔoɪ Qqj
səjnsuə uo∏τpuoo ə^ɪ ∙poys∏BS st i∕ι pəjmbəj ə甲 ψτM ^∣∣ *σι∣∣∕? > (*σι ζ½l)isτp ςuo∏τpuoo ə甲 Se Suoj
sb 'sjəu jbjπqu jo y əzts ^uv joj sppq euraɪə[ ]eq] ə]θN '9 xτpuoddy uτ qjqbjtbab st joojd ə^ɪ
∙(*e 'R)ps!p(([〃乙—〃/[—乙)〃 —ɪ) > (*〃 τ+tn)ps!p Suι^fsμvs。?之?{3} saιmaμ səiməuəs
ɔ > La21SdalSalIlI1以MIUa3sapιua[pm3 P叫工,∣∣ 1b*0τ — C)RIl τ>εllδlhx^D⅛ι∏ιSjre
=：°力 pun 'o < C〃乙-〃灯 -3 saιfsμns 0 < 〃 ∖χp过 ≡ 0t41 以㈣M ∣∣*σι∣∣∕7 > |[?。*出
—0^φ∣∣∣ =: (*σι'0%()]S!p '。？ əuiij,卯 卯甲 as oddTIS ( ∂ju∂Sλ∂λuoj avəuii AmDOD ,【Biuιuθq
,əjbj əɔuəgjəAUoɔ jbqu∏ e Seq juəɔsəp juətpbj^ ςuo∏njos pm∏do pqojS e Jo
PooqJoqqg[əu	uτ st əjbjəjt əqj jəaəuə^m ]eq] SMoqS euraɪə[ ə^ɪ tuiuiqj XəX e əɔnpojjuɪ jsjy ə丛
6noiðhh
NDINHH V HHlNH JJNHDSHcl 1NHIGVHD AH GH1VHHNHD H1VHH1I HHl SHOG NHHAV W
ftuτpjoooB [(½l)∕δ∆]⅛
=：(√H)^δΔ PUe [(½1)∕∆] ⅛ =： (√H)^Δ əlouəp əM PUe əjepdn əm joj (9) j∏QτpBjS uo∏Bjndod
səsn juəɔsəp ]uə[PeB ]呻 əuɪnsse əM ςsτsXp∏B Jo əseə əm joj ,jəjsbj uopnɪos jBm∏do pqojS e
jo PooqJoqqg[əu əqj jəjuə oj əjbjəjt əqj sdɪəq UoHeZ!耳əuɪejed-jəAo XqM jəmsub uəqj əM -ɪpms st
(*σιζ	uoqM uopnɪos pm∏do Ieqo13 e oj əɔuəgjəAUoɔ jbqu∏ sjτqτqxo (QD) juəɔsəp iuə[PeJg
]叩 MoqS JSJij ə丛 ,uo∏bjqjqoob əqj oj sp^əɪ UoHeZ!耳əuɪejed-jəAo XqM jəmsub əm 'uopɔəs sτψ uɪ
SlS 入ZVNY	↑7
, soj∏Syqns oqι UOl=XXqpθ[oqe[ /I' *叫I JoJ əAJnɔ oqι :oɪd osɪŋ əM ζuosμ^duιoo joj ,^∣∣ jf^√H∣∣
SlUəsəɪdəj “tuns,, əʌjnɔ OqJJ sςjf ]uəjə现P joj γ UOJnəu ηɔŋə JO ∕∣’]力四∣ UnoU OJenbS OqI :(p) PUe (ɔ) OJn即
-q∏S ,soj∏Syqns OUreS OqI UOl=XXq pθ[oqe[ ∣ ∖∖^cn∖ JoJ OAJnɔ OqI :oɪd osɪŋ əm 'uospreduɪoɔ joj •网 ə *σι
UOJngU jəηɔŋə: OqI UoVX 网 ≡ Λi 斗1。丛1。U 铝。PmS ŋ Jo uopɔəfojd。]脸腐於 OqI Se p0M0TΛ Oq ueɔ qoτqM

SlUgSgJdgJ “unou ^ɪ,, OAJnɔ OqJJ ^uəɔsəp 豫讥Pe卤 Xq pəurej: {(H W} = X əzɪs q：TM sψθM
-pu 1U。PrUS joj ? UoHeJ。具,sλ q UOJngU qɔŋə Jo | 喘σι∣ 3uəuoduɪoɔ ɪəɪɪŋjŋd :(q) PLre (ŋ) oj∏Syq∏s :0 əjngɪj
∙(oτ = X) sF⅛ll (P) .(£ = X) sF⅛ll (ɔ) ∙(oτ = x) I ∣∣⅛l (q) 立=x) If常| ⑻
[COC HlDI 在 iəded əɔuəjəjuoɔ e Se mətaəj jgpu∩
Under review as a conference paper at ICLR 2021
(a) Component A vs. t.	(b) Component B vs. t.	(c) Component C vs. t.
Figure 3: Subfigure (a) shows w(k),k(1 + η(3∣∣w*k2 - 3||w(k)∣∣2)) (i.e. component A of w#kK]) versus
iteration t for each neuron k. Subfigure (b) plots 2η PjK6=k((wt(j))>wt(k))wt(j),k + ηwt(k),k PjK6=k kwt(j) k2 (i.e.
component B of w#(kK),,kt) versus iteration t for each neuron k. Subfigure (c) plots the norm of component C
of w(kκK⊥, while subfigure (d) plots the norm of component D of w(kκK⊥ for each neuron k. The empirical
findings show that the components due to interaction of the other neurons (i.e. component B and D) are small
(notice that the scale of the vertical axis of (a) and (b), (c) and (d) are different) compared to their counterparts
(i.e. component A and C respectively), which suggests that θ,少 U 10-4 on (10) empirically. Similar patterns
exhibit in training under different K’s (Appendix F).
(d) Component D vs. t.
∣∣wt[1]∣ — ∣∣w*k∣2 + ∣∣w⊥∣∣2 ≤ 2γ2. Let us assume that (C1) ∣∣w*k > 1.1γ (strong signal) and
(C2) γ ≥ 10kw0k (small initialization). Note that to invoke Lemma 1 for showing locally linear
convergence after the iterate gets into a benign region, We will set 2γ2 = V21 w* ∣2 with V satisfying
2 — 14ν — 2ν2 > 0 (i.e. ν ≤ 0.141); consequently, (C1) is trivially satisfied.
Theorem 1. Suppose that the conditions (C1-C2) hold. Assume that the step size satisfies η ≤
C/∣w*∣∣2 for some sufficiently small constant c > 0. Then gradient descent for problem (1) (i.e.
log( kw* k-γ )
K = 1) has TY ≤ log(I+/ , where △ ：= 6γ(∣w*∣ — Y) > 0. Furthermore, for 0 ≤ t ≤ TY, we
have that |wk | ≥ (1 + η∆)t∣w01 and ∣∣w⊥ ∣∣ ≤ γ.
Theorem 1 states that to achieve dist2(wt, w*) ≤ 2γ2, gradient descent only needs at most
log( kw*[-γ)/log(1 + η△) number of iterations. Furthermore, on the signal direction, ∣wt[1]∣
(and hence wtk ) grows exponentially at a rate at least 1 + η∆ before reaching at ∣w* ∣ — γ. On
the other hand, by a close-to-zero initialization (C2), the perpendicular component ∣w⊥ ∣ remains
small. Consequently, we have that dist2 (Wt#1, w*) ≤ max(γ2, (|w* | —∣w0 [1]∣(1+η ∆)t) )+ Y2, for
0 ≤ t ≤ TY before gradient descent enters the linear convergence regime. The proof of Theorem 1
is available in Appendix C. A similar result as Theorem 1 was shown in (Chen et al., 2019).
4.3	How does over-parametrization help entering a benign region
Let us begin by providing a more detailed observation regarding the dynamics of gradient descent.
Figure 3 plots each component of w#KI and w(kK⊥ in the gradient descent dynamics (8) for
K = 10. The empirical findings show that the component due to the interaction (component B,
or component D respectively) is negligible compared to the component that is without the depen-
dency on the other neurons (component A, or component C respectively). Based on the observation,
we can re-write the population dynamics (8) in the early stage (i.e. before gradient descent enters
the linear convergence regime) as follows,
∣w½it+ιI ≥ (1 — θ)∣w&tl(ι + η(3∣w*k2 - 3kw#K,tk2))
∣∣w(kK⊥+ι∣∣ ≤ (IS)kw(kK⊥k(1 + η(kw*k2 - 3kw#K,tk2)).
(10)
for some small numbers θ, ”《1 (empirically U 10-4 as Figure 3 shows). That is, in the early stage,
the approximated dynamics (10) of each student neuron of an over-parametrized network does not
deviate too much from the original dynamics without over-parametrization (7). The approximated
dynamics (10) will be only used for analyzing the stage before the iterate enters the benign region,
i.e. used only before entering a linear convergence regime, though the emprical findings (Figure 3)
suggest that the approximated dynamics hold all the time.
6
Under review as a conference paper at ICLR 2021
On the other hand, by comparing subfigure (a) and subfigure (b) of Figure 3, we see that component
A and B of each neuron k are with the same sign during the execution. This observation together
with the dynamics (8) tend to implies that |w#k"kt+il ≤ |w#k"ktl(1 + η(3∣∣w*k2 - 3kw(kκK tk2))
when K > 1. On the other hand, the dynamics (7) has 喏*1| = |w#1i：tl|(1 + η(3kw*k2 -
3kw#(11),tk2). Also, as the case of single neuron, the perpendicular component of each neuron k of
K (i.e. kw(kκK⊥ k) remains small (figures in Appendix F). Consequently, We could write
| (1),l | & | (k),l | nd k (1) k & k (k) k	(11)
|w#1,t | & |w#K,t| and kw#1,tk & kw#K,tk,	(11)
where the approximation & accounts for the fact that the size of initial points due to the random
initialization may be different and that the small interaction components are present in the dynamics
for K > 1. Figure 2 confirms that the relation (11) generally holds on average empirically.
Lemma 2. Suppose that the approximated dynamics (10) and (11) hold from iteration 0 to iteration
t. Then, the network with a single neuron and an over-parametrized network trained by GD with the
same ste^π siτe rn has ʌ /P PK lw(k)'k |2 & p (1 ~——2tθ)√Klw(ɪ),k I
same step sze η as k=1 |w#K,t | &	( -	)	|w#1,t |.
Lemma 2 states that if the single neuron of the non-overparametrized network has a certain projec-
tion on w* at time t, then the over-parametrized network with K neurons will have approximately
√K times larger projection on w*, modulo the √1 - 2tθ factor which is close to 1 if the prod-
uct tθis small (as Figure 3 shows). This demonstrates the advantage of over-parametrization —
over-parametrization helps to make more progress on growing the model’s projection on w*.
Lemma 3. Suppose that η ≤ 3|/*口2. By following the conditions as Lemma 2, we have that
(k),k	(k),⊥	(1),k	(k),⊥
∣∣w(k),⊥k . |w#K,tlkw#K,0k ɪ . |仅#1尸 |kw#K,0 k ɪ where ψ := (1 -θ-?9 —θ?外(1 + nkw k?)
kw#K,t k .	κ√k),k । ψt .	∣7√k),k । ψt，where ψ :=11 θ U θ刃 (1+ ηkw*k ).
|w#K,0|	|w#K,0 |
Lemma 3 states that the ratio of the perpendicular component kw#k"： ∣∣ to the parallel component
∣w(kκKkt∣ of each neuron decays exponentially if ψ > 1 (which holds if η∣∣w*∣∣2 & [-+匕).By
combining Lemma 2 and 3, we have the following theorem, which characterizes the difference of
the distances to w* at iteration t.
Theorem 2. (Snapshot at t) Suppose that the approximated dynamics (10) and (11) hold from 0 to
t and that at iteration t, the student network with a single neuron trained by GD with the step size
η has |w#(11),,tk | = c1,t ∣w* ∣2 for some number c1,t satisfying 1 > c1,t > 0. Denote c2,t a number
c kw(k),⊥ k
that satisfies	#K,0 士 ≤ 。21 for each k ∈ [K]. Suppose that the step size η also satisfies
|w#(kK) ,0 [1]| ψt	2,t
η ≤ 3kw1 k2 and makes ψ ：= (1 — θ — U — θU)(1 + η∣∣w*∣∣2) > 1. Then, an over-parametrized
network Wt#K trained by GD with the same η has
dist2(Wf1,w*) — dist2(W#K,w*) & ∣w*∣2(2cι,t(√(1 — 2tθ)√K — 1) — K(c2,t + c2,t) + c2 j
Recall that in Theorem 1, we upper-bound dist2(Wt#1, w*). Simply combining Theorem 1 and 2
leads to a distance upper-bound of dist2(Wt#K , w*) at certain iteration t. The lower bound of the
difference of the distances in Theorem 2 shows a strict improvement due to over-parametrization
when it is positive, which answers why over-parametrization helps gradient descent to enter the
linear convergence regime faster — gradient descent for an over-parametrized network shrinks
the distance to w* faster in the early stage. Note that the lower bound is a quadratic func-
tion of √K and is increasing for 1 ≤ √K ≤ c1,tv2%2tθ, which means that up to a cer-
c1,t+c2,t
tain threshold of K, more over-parametrization could lead to more improvements. Moreover, if
cι,t and c2,t further satisfy (*) 2%t(√2,(1 — 2tθ) 一 1)一 c21 一 2c21 > 0, then the lower
bound of the difference for K = 2 neurons is strictly positive and keeps being positive up to
cι,t√1-2tθ+√c1 t(1-2tθ)-(c2 t + c2 t)(2cι,t-c1
√K ≤ b --------------V 1,t c2 +eg 1,t__2,t ：——1t~C, which gives an upper limit of the degree of
over-parametrization that allows acceleration. The condition (*) is easily satisfied when (1) tθ《1
so that，(1 — 2tθ) U 1 and (2) c2,t《 1, which happens when the approximated dynamics (8)
holds for a small θ and that the ratio of the perpendicular component to the parallel component of
neuron k decays sufficiently fast (Lemma 3).
7
Under review as a conference paper at ICLR 2021
4.4	Is over-parametrization necessary for acceleration
In this subsection, let us consider a new objective,
minw∈Rdf (W) := 4n Pn=ι (C(X>W)2 - yi)2,	(12)
where C ≥ 1 is an user-defined parameter. This modification can be viewed as setting the weight of
the second layer to C instead of 1. The population dynamics due to gradient descent becomes
w(+)1k	=	W(I),k (1	+ η(3C|lw*||2 -	3C2IlW(I)∣∣2))	and	w(+)ι⊥ =	W(I),⊥(1	+ η(Ckw*∣∣2 -
3C2kWt(1) k2)). Let us also consider an over-parametrized version of (12),
minW∈Rd×κ f (W) := 4n Pn=I (C(X>W(I))2 + C(X>W⑵)2 + ... + C(X>W(K))2 - yi)2. (13)
On Figure 4, we report gradient descent with the same step size
η for solving (13) under different C’s and K’s. The result is in-
teresting; it shows that simply scaling the weight of the single
neuron (i.e. C > 1) without over-parametrization (i.e. K = 1)
can achieve a similar acceleration and that the effect seems to
be more significant. The observation questions the necessity of
over-parametrization, as the figure suggests that one can avoid
the computational overhead due to over-parametrization by sim-
ply scaling up the weight of the output node while enjoys accel-
eration.
Let us provide an analysis of the acceleration observed here.
/	Figure 4： Gradient descent for (13)
Since W = ±w*/√C are the global solutions of (12), We define Under different K and C
Tγ,c := miη{t : ∣∣Wt[1]∣ - ∣∣w*∣∣∕√C| ≤ γ and ∣∣w⊥∣∣ ≤ Y}.
For γ being sufficiently small, We can shoW that gradient descent enters the linear convergence
regime at Tγ,C (see Subsection E.1 in Appendix E, Where We provide a counterpart of Lemma 1
for general C). Therefore, the question is hoW the scaling of the output Weight helps to reduce
the number of iterations spent in the early stage. We have the folloWing theorem. The proof is in
Appendix E.
Theorem 3. Suppose that ∣w* ∣ > 1.1 VCγ and Y ≥ 10∣w0 ∣∣. Assume that the step size satisfies
η ≤ c∕∣W* ∣2 for some sufficiently small constant C > 0. Then gradient descent for problem (12) has
log( kw* l"√C-Y )
Tγ,c ≤ ∣og(1+η∆C) , where ∆c := 6γC(√C∣∣w*∣∣ — CY) > 0. Furthermore, for 0 ≤ t ≤ Tγ,c,
we have that ∣Wk | ≥ (1 + η∆c)t|w0 | and ∣∣w⊥ ∣∣ ≤ γ.
As Theorem 1, the condition ∣∣w*∣∣ > 1.1 ∖TC∖ (strong signal) in the theorem is trivially satisfied
when we set 2γ2 = V2∣W*∣∣2 and makes ν > 0 a small number (see Appendix E.1 for details).
NoW let us conclude by making tWo important remarks regarding the acceleration effect due to
scaling up the output weight. First, the single neuron only needs to be w* / VZC for achieving zero
testing error. Since we have a close-to-zero random initialization here, it means that |W0(1) | only
needs to grow from a close-to-zero number to ∣∣w*∣∕λ∕C instead of ∣w*∣. This implies that the
modification shortens the effective distance to a global optimal solution. Second, up to a certain
threshold C, the growth rate of the parallel component, (1 + η∆c), is larger for a larger C ≥ 1.
To see this, fix a ∣W* ∣ and Y, one can show that the derivative of ∆c w.r.t. C is non-negative as
long as 3∣w*∣∕(4γ) ≥ VzC. The means that the speed to get into the neighborhood of a global
optimal solution is faster as C increases. These two mechanisms explain why scaling up the weight
of the output node helps reducing the number of iterations Tγ,c for gradient descent entering the
linear rate regime. Our findings thus show that the scaling of the output layer has a great impact on
the speed of convergence. Nevertheless, we notice that some works in the literature suggest leaving
the output layer (the last layer) untrained (e.g. (Hoffer et al., 2018)) and that most of the theory
works regarding convergence results of GD in deep learning assume that the output layer is fixed
(e.g. (Allen-Zhu et al., 2019; Du et al., 2019)). Hence, our results here might raise another important
issue in practice.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, , and Zhao Song. A convergence theory for deep learning via
overparameterization. ICML, 2019.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. ICML, 2018.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. NerurIPS, 2019.
Alon Brutzkus and Amir Globerson. Why do larger models generalize better? a theoretical perspec-
tive via the xor problem. ICML, 2019.
Emmanuel J. Candes and Xiaodong Li. Solving quadratic equations via phaselift when there are
about as many equations as unknowns. Foundations of Computational Mathematics, 2014.
Emmanuel J. Candes, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable
signal recovery from magnitude measurements via convex programming. Communications on
Pure and Applied Mathematics, 2013.
Emmanuel J. Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via wirtinger flow:
Theory and algorithms. IEEE Transactions on Information Theory, 2015.
Yuxin Chen and Emmanuel J. Candes. Solving random quadratic systems of equations is nearly as
easy as solving linear systems. Communications on Pure and Applied Mathematics, 2017.
Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, and Yuling Yan. Gradient descent with random ini-
tialization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming,
2019.
Damek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette. The nonsmooth landscape of phase
retrieval. IMA Journal on Numerical Analysis, 2018.
Simon Du and Jason Lee. On the power of over-parametrization in neural networks with quadratic
activation. ICML, 2018.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. ICLR, 2019.
Matt Emschwiller, David Gamarnik, Eren C. Kizildag, and Ilias Zadik. Neural networks and poly-
nomial regression. demystifying the overparametrization phenomena. arXiv:2003.10523, 2020.
Albert Fannjiang and Thomas Strohmer. The numerics of phase retrieval. Acta Numerica, 2020.
Spencer Frei, Yuan Cao, and Quanquan Gu. Agnostic learning of a single neuron with gradient
descent. arXiv:2005.14426, 2020.
David Gamarnik, Eren C. Kizildag, and Ilias Zadik. Stationary points of shallow neural networks
with quadratic activation function. arXiv:1912.01599, 2019.
Rong Ge, Runzhe Wang, and Haoyu Zhao. Mildly overparametrized neural nets can memorize
training data efficiently. arXiv:1909.11837, 2019.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. NeurIPS, 2019.
Surbhi Goel, Varun Kanade, Adam Klivans, and Justin Thaler. Reliably learning the relu in polyno-
mial time. COLT, 2017.
Surbhi Goel, Adam Klivans, and Raghu Meka. Time/accuracy tradeoffs for learning a relu with
respect to gaussian marginals. NeurIPS, 2019.
Sebastian Goldt, Madhu Advani, Andrew M. Saxe, Florent Krzakala, and Lenka Zdeborova. Dy-
namics of stochastic gradient descent for two-layer neural networks in the teacher-student setup.
NeurIPS, 2019.
9
Under review as a conference paper at ICLR 2021
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Sre-
bro. Implicit regularization in matrix factorization. NIPS, 2017.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the general-
ization gap in large batch training of neural networks. NIPS, 2017.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier: the marginal value of training the
last weight layer. ICLR, 2018.
Sham M. Kakade, Varun Kanade, Ohad Shamir, and Adam Kalai. Efficient learning of generalized
linear and single index models with isotonic regression? NeurIPS, 2011.
Seyed Mohammadreza Mousavi Kalan, Mahdi Soltanolkotabi, and A. Salman Avestimehr. Fitting
relus via sgd and quantized sgd. ISIT, 2019.
Abbas Kazemipour, Brett Larsen, and Shaul Druckmann. No spurious local minima in deep
quadratic networks. arXiv:2001.00098, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.
Yuanxin Li, Cong Ma, Yuxin Chen, and Yuejie Chi. Nonconvex matrix factorization from rank-one
measurements. AISTATS, 2019.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with relu activation.
NeurIPS, 2017.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. COLT, 2018.
Roi Livni, Shai Shalev-Shwartz, and Ohad Shamir. On the computational efficiency of training
neural networks. NeurIPS, 2014.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. ICLR, 2019.
James Lucas, Shengyang Sun, Richard Zemel, and Roger Grosse. Aggregated momentum: Stability
through passive damping. ICLR, 2019.
Cong Ma, Kaizheng Wang, Yuejie Chi, and Yuxin Chen. Implicit regularization in nonconvex statis-
tical estimation: Gradient descent converges linearly for phase retrieval, matrix completion, and
blind deconvolution. Foundations of Computational Mathematics, 2017.
Stefano Sarao Mannelia, Eric Vanden-Eijnden, and Lenka Zdeborova. Optimization and generaliza-
tion of shallow neural networks with quadratic activation functions. arXiv:2006.15459, 2020.
Stefano Sarao Mannellia, Giulio Birolib, Chiara Cammarotac, Florent Krzakalab, Pierfrancesco Ur-
bania, and Lenka Zdeborova. Complex dynamics in simple neural networks: Understanding
gradient flow in phase retrieval. arXiv:2006.06997, 2020.
Song Mei, Yu Bai, and Andrea Montanari. The landscape of empirical risk for non-convex losses.
PNAS, 2018.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. ICML, 2017.
Itay Safran, Gilad Yehudai, and Ohad Shamir. The effects of mild over-parameterization on the
optimization landscape of shallow relu neural networks. arXiv:2006.01005, 2020.
Yoav Shechtman, Yonina C. Eldar, Oren Cohen, Henry Nicholas Chapman, Jianwei Miao, and
Mordechai Segev. Phase retrieval with application to optical imaging: a contemporary overview.
IEEE signal processing magazine, 2015.
Mahdi Soltanolkotabi. Algorithms and theory for clustering and nonconvex quadratic programming.
Stanford University Ph. D. Dissertation, 2014.
Mahdi Soltanolkotabi. Learning relus via gradient descent. NeurIPS, 2017.
10
Under review as a conference paper at ICLR 2021
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. IEEE Transactions on Information
Theory, 2018.
Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. IEEE ISIT, 2016.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initializa-
tion and momentum in deep learning. ICML, 2013.
Yan Shuo Tan and Roman Vershynin. Phase retrieval via randomized kaczmarz: Theoretical guar-
antees. Information and Inference, 2018.
Yuandong Tian. Student specialization in deep rectified networks with finite width and input dimen-
sion. ICML, 2020.
Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural
network optimization landscapes. JMLR, 2019.
Chris D. White, Sujay Sanghavi, and Rachel Ward. The local convexity of solving systems of
quadratic equations. Results in Mathematics, 2016.
Gilad Yehudai and Ohad Shamir. Learning a single neuron with gradient methods.
arXiv:2001.05205, 2020.
Chong You, Zhihui Zhu, Qing Qu, and Yi Ma. Robust recovery via implicit bias of discrepant
learning rates for double over-parameterization. arXiv:2006.08857, 2020.
A Related works
Over-parametrization: Though our work focus on understanding why over-parametrization leads
to acceleration in optimization (i.e. improving the convergence time), we also want to acknowledge
some related works of understanding over-parametrization in different aspects (e.g. (Arora et al.,
2019; Goldt et al., 2019; Brutzkus & Globerson, 2019; Emschwiller et al., 2020; Tian, 2020; Safran
et al., 2020). Arora et al. (2019) study implicit regularization of gradient descent in deep linear
neural network for matrix factorization. They consider increasing the depth of the linear network
and analyze how the singular values of the learned solution evolves during gradient flow due to
increasing the depth. Goldt et al. (2019) use ordinary different equation as a tools for analyzing the
asymptotic generalization error of SGD when the dimension of input data goes to infinity, for the
case that the number of hidden nodes of a student network is larger than that of a teacher network.
Brutzkus & Globerson (2019) prove that for an over-parameterized convolutional networks, gradient
descent converges to a global solution with better generalization performance compared to global
minima of smaller networks. Emschwiller et al. (2020) show that for polynomial activation function,
if a trained student network interpolates sufficient number of training data, then the generalization
error can be arbitrarily small, regardless of the size of the student network. Tian (2020) identify
conditions of which each neuron of a teacher network can be explained by some neurons of a larger
student network. Safran et al. (2020) study the effects of over-parameterization on the optimization
landscape of a network with ReLU activation and show that there is a significant set of points around
a global solution that satisfies a notion of one-point strong convexity for an over-parametrized model.
On the other hand, there is a trend of works studying how over-parametrization changes the opti-
mization landscape of empirical risk minimization for neural nets with quadratic activation. (e.g.
(Du & Lee, 2018; Gamarnik et al., 2019; Ge et al., 2019; Kazemipour et al., 2019; Soltanolkotabi
et al., 2018; Nguyen & Hein, 2017; Venturi et al., 2019; Mannelia et al., 2020)). Du &Lee (2018)
show that when the number of neurons is sufficiently larger than the squared root of the number of
samples, then there is no spurious minima. Mannelia et al. (2020) study how the number of samples
and the width of the teacher network affect the optimization landscape of empirical risk minimiza-
tion. They also identify some conditions so that gradient flow can have small generalization error.
Quadratic activation: The optimization landscape of problem (1) (i.e. phase retrieval) and its
variants has been studied by (Davis et al., 2018; Soltanolkotabi, 2014; Sun et al., 2016; White et al.,
2016), which shows that as long as the number of samples is sufficiently large, it has no spurious
11
Under review as a conference paper at ICLR 2021
local optimum and all the local optima are globally optimal. In addition to the landscape analysis,
there are some convergence results of GD in the literature (e.g. (Chen et al., 2019; Ge et al., 2019)).
Ge et al. (2019) show that gradient descent can train a quadratic network to memorize training
data perfectly in a mildly over-parametrized regime. Chen et al. (2019) provably show that applying
gradient descent with an isotropic random initialization to solving (1) leads to a solution that recovers
the teacher neuron w* modulo the unrecoverable sign. Other relate works include (Tan & Vershynin,
2018) and (Mannellia et al., 2020). Tan & Vershynin (2018) study online gradient descent while
Mannellia et al. (2020) study gradient flow for solving phase retrieval. In this work, we show that
gradient descent on a over-parametrized student network (namely, (2)) takes even fewer iterations to
recover w* . Furthermore, we show that up to a certain threshold, the larger the number of student
neuron K, the faster the global convergence.
Matrix Sensing: In matrix sensing, the setup is that there is an unknown rank-r PSD matrix
W* ∈ Rd×d such that yi = hAi, W*i = tr(Ai>W*), where Ai ∈ Rd×d is a symmetric measurement
matrix. Li et al. (2018) shows that gradient descent on minu∈Rd×d * PNi (yi - hAi, UU>))2
with a close-to-zero random initialization recovers W*, when the matrices {Ai} satisfy restricted
isometry property (RIP). Their work shows an implicit regularization of gradient descent under over-
parametrization, since the result indicates that gradient descent recovers W* without knowing the
matrix rank beforehand. Li et al. (2019), another group of authors, consider the case that Ai = xixi>,
Xi 〜 N(0,Id), and W* is an unknown rank-r PSD matrix. They show that with a special-
ized initialization called spectral initialization, gradient descent solves minu∈Rd×r * PZi Bi -
hAi, UU>i2 and recovers W* up to some orthonormal transform. Note that one can rewrite (2) in
the form of matrix sensing, namely, minw∈Rd×κ * PNi (yi - (x%x>, WW>>) , W* = w*w>,
and yi = hxixi>, W*i = (w*>x)2. Our work does not assume RIP property nor require the spec-
tral initialization. Furthermore, the goal of our work is different from these works; we focus on
understanding the acceleration due to over-parametrization. Lastly, we note that matrix sensing
has been a subject of understanding implicit regularization of gradient descent (see e.g. Gunasekar
et al. (2017); Gidel et al. (2019); You et al. (2020)), where the goal is to understand which solution
gradient descent will converge to.
Two-phase dynamics of gradient descent: The behavior of gradient descent in our problem can
be divided into two phases. We note that the multi-phase of dynamics of gradient descent is also
present for solving other problems (see e.g. (Li & Yuan, 2017)).
B Proof of Lemma 1
Recall the optimization problem is
minW∈Rd×κ f (W) := 4n Pi=i ((X>w(I))2 + (X>w⑵产 + ... + (X>w(K))2 - yi)2,
and the notation that VF(W):= Eχ[Vf (W)] and V2F(W):= Eχ[V2f (W)]. We first introduce
some key lemmas. Lemma 4 below says that for any W ∈ Rd×K, if the iterate is sufficiently close
to a global solution w*q>, then the landscape is essentially strongly convex. Lemma 5, on the other
hand, shows the smoothness of the landscape.
Lemma 4. (locally strong convexity) Assume that dist(Wt0, w*) := kWt0 - w*qt> kF ≤ νkw* k2,
where Wt° ∈ Rd×K, ν > 0, qt。：= argminq∈RK"∣q∣∣2≤ι ∣∣Wt0 — w*q>∣∣F, and w* ∈ Rd being the
teacher neuron. Then
vec(V)>V2F(Wt0)vec(V) ≥ 2tr(qt0 w*> V qt0 w*> V ) + (2 - 14ν - 2ν2)kw*k22kV k2F
for any V ∈ Rd×K, where tr(∙) denotes the matrix trace.
12
Under review as a conference paper at ICLR 2021
Proof. The proof is a modification of the proof ofLemma 14 in Li et al. (2019). For notation brevity,
we will suppress script to in the following (i.e. W J Wt0, q J qt0). We have that
VeC(V )τV2/(W )vec(V)
1 m
=一 X vec(V )τ[((IlxTW ∣∣2 - yi)Iκ + 2W TgxTW) 0 gx>]vec(V)
m i=1
1	二,..丁 ..C	、	ɪ	ɪ	1 二.一	.	丁	丁丁.
=一 X (IlxTW∣∣2 - yi)vec(V)τvec(xix>V) +-X VeC(V)τvec(2xix>VWTxixTW).
m	m
i=1	i=1
1 m
=m X [(kxTW I2 -kxTw*qTk2 )kxTVk2 + 2(xTWV Txi)2].
i=1
By taking the expectation above over xi 〜N(0, Id) and using Lemma 6, we have that
E[vec(V)tV2∕(W)vec(V)] = IlWIIFIlVkF + 2∣VtWIlF - (∣∣w*qT∣FIlVkF + 2∣VTw*qTkF)
+ 2(tr(W τV )2 + tr(W τVW τV) + ∣∣WV TkF )∙
(14)
Now let US set W = w*qτ + θH with a H satisfying ∣∣HkF = 1 and θ :=v∣ w* k for some number
ν > 0.
kW kF = kw*qTkF + θ2kH kF + 2θhw*qτ, H〉
≥	kw*k2- 2θ∣∣w*qTkFkHkF
≥	kw*k2- 2θkw*kkHkF
kV τW kF = kV τw*qTkF + θ2kV τH kF + 2θtr(V τw*qτH τV)
≥	kVTw*qTkF - 2θ∣∣w*qTkkHkFkVkF
kVWτ kF = IVqwTkF + θ2kVH τkF + 2θtr(Vw>qHVτ)
≥	IVqwTkF - 2θkw>qkkHkFkVkF
=	IVqwTkF - 2θkw*kkHkFkVkF
tr(W τVW τV) = tr(qw> VqWT V) + 2θtr(H τVqw> V) + θ2tr(H τVH τV)
=tr(qw>Vqw>V) - 2θkw*qTkkHkFIWlF - θ2kH∣FkVkF.	(15)
Combining (14) and (15), together with the bilinear property of the expectation so that
vec(V)τV2F(w)vec(V) = E[vec(V)τV2f (W)vec(V)], we have that
vec(V)tV2F(W)vec(V) ≥2tr(qw>VqWTV) + 2∣∣Vqw>kF
-14θkw*kkH∣fkVkF - 2θ2kHkFkVkF
=)2tr(qw>VqwTV ) + 2∣∣w*k2kV kF
-14θkw*kkH∣fkVkF - 2θ2kHkFkVkF
=2tr(qw>VqwTV) + (2 - 14ν - 2ν2)kw*k2kVkF,	(16)
where (a) is due to that ∣∣ABkF ≥ σ2(A)∣∣BkF with r being the rank of A, which in our case
A := w*qτ is a rank one matrix and B := VT; consequently σ2(A) = ∣w*qTkF = ∣∣w*∣2.	□
Lemma 5. (smoothness) Assume that dist(Wt0 ,w*) := ∣Wt0 — w*q> ∣∣f ≤ ν∣w*∣2 where Wt° ∈
Rd×K, ν > 0, qto := argmi∏q∈RKRq∣∣2<1 k W — w*qτ ∣f, and w* ∈ Rd being the teacher neuron.
Then, ∣V2F(W)∣∣2 ≤ (15+16ν2)∣w*∣2.
13
Under review as a conference paper at ICLR 2021
Proof. For notation brevity, we will suppress script to in the following (i.e. W J Wt0, q J qt0).
We have that
IlV2F(W)∣∣2 = ∣∣E[[(∣∣x>Wk2 - kx>w*q>k∣)∕κ + 2WτxxτW]乳 xxτ]心
≤ ∣E[[∣∣xτWk∣ -IlXTw*q>k∣∣Iκ + 2∣xτW∣∣Iκ]乳xxτ]∣∣
(	a)
≤	kE[∣ IlXTWk∣ -∣∣xτw*qτk∣ ∣ xxτ] k∣ + 2∣∣E[∣∣XTW∣∣∣xxτ] ∣∣∣
=	) kE[(kXTWk∣ -∣∣xτw*qτk∣)xxτ] k∣ + 2∣∣∣∣WIlFId + 2WWTkI
=k(kWkF -∣∣w*qτ∣∣F)Id + 2(WWT - w*wT)k∣ + 2∣∣∣∣WkFId + 2WWTkI
≤	7kWkF + kw*qTkF + 2kWWT - w*wTk∣
≤	(15 + 16ν∣)kw*k∣,
(17)
where (a) is due to ∣∣I 0 Ak∣ ≤ l∣I k∣kAk∣ = k Ak∣, (b,c) is due to Lemma 6, and the last inequality
is by setting W = w*qτ + θH with a H satisfying ∣∣H∣f = 1 and θ := ν∣∣w*k∙
□
Lemma 4 and Lemma 5 together implies that when the the iterate is in a neighborhood of a global
optimal solution, then gradient descent has a linear convergence rate.
Lemma 1: (locally linear COnVergenCe) Suppose that at time to, dist(Wt0 ,w*) := ∣∣Wt0 —
w*qT k ≤ ν∣∣w*k where W% ∈ Rd×K, ν > 0 satisfies 2 — 14ν — 2ν∣ > 0, and q% :=
arg minq∈RK "∣q∣∣2≤ι kW加 — w* qτ∣∣∙ Then, gradient descent with the step size η ≤ (i5+-6Vν)2∣∣W* 产
generates iterates {Wt}t≥to satisfying
dist∣(Wt+ι, w*) ≤ (1 - η(2 - 14ν - 2ν∣))dist∣(Wt, w*).
Proof. We have that
dist∣(Wt+1) ：= kWt+1 - w*qt+1kF
≤k Wt+i- w* qJllF
=kWt - ηVF(Wt) - w*qjkF
=kwt - w* - ηvec(VF(Wt) - VF(w*q1)) kF
=kwt - w* - η( / V∣F(Wt(τ))dτ)(wt - w*)kF
1
=(wt - w*)T (IdK - η / V∣F(Wt(τ))dτ『(wt 一 w*)
o
≤ kwt - w*k∣ - 2η(wt - w*)τ(/ V∣F(Wt(τ))dτ)(wt - w*)
+ η∣k [1 V∣F(Wt(τ))dτk∣kwt - w*k∣.
o
(18)
where (a) we use the notations that wt := VeC(Wt) and w* := vec(w*qj) and that VF(w*qj) = 0
and (b) we denote Wt(T) := w*qj + τ(Wt - w*qj).
Notice that dist(Wt(τ),w*) ≤ kWt(τ) - w*qJk = T∣∣Wt - w*qJk ≤ νkw*k. So we can invoke
Lemma 4 to obtain that
≥ 2tr(qtwT(Wt - w*qj)qtwτ(Wt- w*qJ)) + (2 - 14ν - 2ν∣)kw*k∣kWt - w*qJ∣∣F
≥(2 - 14ν - 2ν∣) ∣∣w* ∣∣∣k Wt- w*qTkF.	(19)
14
Under review as a conference paper at ICLR 2021
where the last inequality is due to that qt w> (Wt — w* q>) is a symmetric matrix since qt = 口2>；*口
so that tr(qtw>(Wt — w*q>)qtw>(Wt — w*q>)) = ∣∣qtw>(Wt — w*q>)kF ≥ 0. Furthermore, by
Lemma 5, we have that
kV2F(W)k2 ≤ (15 + 16ν2)kw*k2.
So
η2k / V2F(Wt(T))dτk2kwt- W*k2 ≤ η2(15 + 16ν2)2kw*k4kwt- W*∣∣2∙
0
Combining (18), (19), and (21), we get
dist2(Wt+ι,w*) ≤kwt — W*k2 — 2η(wt — w*)>(Z V2F(Wt(T))dτ)(wt — w*)
+ η2k [ 1 V2F(Wt(τ))dτk2kwt — w*k2
0
≤kWt — w*qt> k2F — 2η(2 — 14ν — 2ν 2)kw* k2 kWt — w* qt> k2F
+ η2(15 + 16ν2)2kw*k4kWt — w*qt>k2F
≤(1 — η(2 — 14ν — 2ν2))dist2 (Wt, w*),
(20)
(21)
(22)
where the inequality we use that η ≤
2-14ν-2ν2
(15 + 16ν2)2kw*k4 .
□
Lemma 6. (Lemma 12 in Li et al. (2019)) Suppose X 〜N(0, Id). Thenfor any fixed matrices
W, V ∈ Rd×r. We have that
E[kx>Vk22kx>Wk22] = kVk2FkWk2F+2kV>Wk2F
E[(x>WV > x)2] = (tr (W >V ))2 + tr (W > VW >V) + ∣∣WV >kF.
C Proof of Theorem 1
Theorem: 1 Suppose that the conditions (C1-C2) hold. Assume that the step size satisfies η ≤
C/∣w*k2 for some sufficiently small constant c > 0. Then gradient descent for problem (1) (i.e.
l ( kw* k-γ)
K = 1)	has TY	≤	iog(1w°η∆)	, where △ ：=	6γ(∣w*k — Y) >	0.	Furthermore, for 0 ≤ t ≤	TY,	we
have that |wk | ≥ (1 + η∆)t∣w01 and ∣∣w⊥∣ ≤ γ.
Proof. Since it is clear about the number of the student neuron (which is 1), in the following, we
suppress the subscript #1 and the superscript (1) for the brevity of notations. Recall the dynamics,
wt+1 = wk(1 + η(3kw*k2 - 3kwtk2))	(23)
w⊥+1 = w⊥(1 + η(kw*k2 - 3kwtk2)).
and note that wtk = wt[1]∣w* ∣ so we have that
wt+1[1] = wt[1](1 + η(3kw*k2 - 3kwtk2))	(24)
For a number Y > 0, define TY := min{t : ∣∣wt[1]∣  ∣∣w*∣∣ ≤ Y and ∣∣w⊥∣∣ ≤ γ}. We can
decompose the square of the distance term as follows,
dist2(W#1,w*) = |*[1]| 一 ]血|||2 + ∣w⊥k2.
In the latter part of this proof, we will show that ∣wt⊥ ∣ ≤ Y for all t ≤ TY . Let us upper-bound the
norm of ∣wt∣2 for t ≤ TY as follows.
∣wt∣2 = wt[1]2 + ∣wt⊥∣2 ≤ (∣w*∣ — Y)2 +Y2
= ∣w*∣2 — 2Y∣w*∣ + 2Y2,	(25)
15
Under review as a conference paper at ICLR 2021
where the inequality is because wt[1]2 ≤ (∣∣w* Il- Y)2 for t ≤ TY by the definition and that kw⊥ k ≤
γ for all t ≤ Tγ proved in the latter part. Hence, we have that
∣wt+ι[1]∣ = ∣wt[1]∣(l+ η(3kw*k2 - 3kwtk2)) ≥ ∣wt[1]∣(l+ η(3kw*k2 - 3(kw*k2 - 2γkw*k + 2γ2)))
：=∣wt[i]∣(ι + η∆) = ∣wo[i]∣(ι + η∆)t+1,
(26)
with
∆ := 6γ(∣∣w*k- γ) > 0.
Based on (26), it takes at most number of iterations
loσ∙( kw*k-Y)
T ≤ g( ∣W0[1]I)
Y - iog(i + η∆)
for wt[1] to rises above ∣∣w*k — Y if wo[1] > 0. Similarly, if w0[1] < 0, it takes TY iterations for
wo[1] to satisfy w∕1] ≤ -∣∣w*∣∣ + Y.
Now we switch to show that for 0 ≤ t ≤ TY, Y ≥ Iwt⊥ I. We are going show that the perpen-
dicular component Iwt⊥ I start decaying before it could have increased above Y. From the dynam-
ics Ilwt+ιk = ∣∣w⊥k(1 + η(∣∣w*k2 — 3∣∣wt∣∣2)), we see that once Ilwtk2 ≥ 3∣∣w*∣∣2, the size of
the perpendicular component Iwt⊥ I starts decaying. On the other hand, Iwt⊥ I is increasing when
IlwtII2 ≤ 1 kw*k2, which leads to the following before the perpendicular component starts decaying,
llwtll2 ≥ w2[1] = iɪ-ɪiɪɪ|wk|2 = iɪ-ɪiɪɪlwLl2(1 + n(3Hw* 112 - 31*-1『))2
llw* i	llw* Il
≥ ii-ɪiii |俐-1|2(1 + 功忖*『)2
IlwMI2
≥ u-ɪuilw0l2(1+ 功忖*『)2' =心0[1]|2(1 + 加忖*『)2t,
llw* i
(27)
which means that that the size of Ilwtk2 grows at the rate at least(1 + 2切脑|[2)2 before the per-
pendicular component Iwt⊥ I starts decaying.
The inequality (27) also implies that the number of iterations such that ∣∣wt ∣∣2 ≤ 31 IIwJ2 is at most
kw*
k2
t*< 1	l°g( EF)
—2 log ((1 + 2η∣∣w* I2)2).
(28)
^iiF
After t*, We have that ∣∣w⊥ ∣∣ is decaying. So We only have to show that for 0 < t < t* the
perpendicular component never rise above Y. It suffices to show that an upper bound of Iwt⊥ I for
0 < t < t* is not greater than Y,
nw:n = nwt-iK1+n(Iw* ιι2 - 31*-1『))
< Ilwt-iK1 + n(Iw*Il2 - 3w2-1[1]))
<Iw⊥I∙ ∏S=01(1+ 〃(忖*『-3w2[1]))
≤) Iw⊥I∙∏S=01(1 + 〃(忖*『-3w0[i](i + η∆)2s))
?
< Y,	(29)
?
where (a) we use (26). By taking logarithm on the both sides of < , it suffices to show that
t*-1	?
logllw⊥I + X log (1 + 〃(忖*俨一3w0[i](i + η∆)2s)) < logY	(30)
s=0
16
Ll
(H)
Il iz ⅛∣∣
π^≠<
Il i, ⅛∣∣
-∣x¾-((z∣k>ll - z∣l*mll)(z∣k)mll - d*叫叽〃一$*叫阿+【)(的一伊一。一【)<
小⑼叫
llτ.(^ll (⅛网—扑叫)〃+【)("【)_ llτJ(>ll
小⑸河((/%冲卜/M肥)〃+【)s-【)<⅛T
JBqj 5Λ∏q əm ζ(QT)
soτuiBuXp p5jBuπxojddB Qqj Ag juəmom əgi JOJ y# Jduosqns əgi ssəjddns sn jəɪ ζλjτΛQjq JOH 'Jooλj
,	I O4Jf#m|
.(耕*叫〃+【)(曲一伊—e —1)=：≠	.
"『(％	⑴I
,	I O4 Jf I
V 也	I IIa) I
› τ ∣Γ^^^III ∣∣i^^ι
11 τi(⅛) 111 Il i(⅛) 1
iυψ ∂M)i[ am Z υιuui∂rι sυ suoιιιpuoo a甲 §umouof Xg ∙^L⅛ε 心 〃 IDIμ əsoddn^
›∣∣ζ⅛∣∣
£ :kuiuiət
□
UOTjnqujSTp ubtssπbS Qqj UlaIJ p5zτjπμτuτ p rτ st
əpouqoB5 jπqj qons Uotjbztjbtjtut Qqj Aq st (p) PUB t(χ 'θ)∖H 三/PUB ɪ- < xjojx^ + χ < d(x + ɪ)
Xq sɪ O)∖((jlls,ς*mllε- zW*m∖∖^ +【)号;u < <(∕R瓢怆一怆)〃 + I)MIl W卬 OS
z∖∖s ^mW < e∣Γ'(*m∣∣ JW 8uτsn Aq st (q) ζuoτsuBdxa əʌɪsjnɔəj aqj PuB (次)oj ənp st (b) QjaqM
(ɛɛ)
肥产回
1="
e((zlls,(*mllε — d*叫怕)〃 +【)号;U ∙ zlf(*ml
-,((e∣∣s⅛∣∣ε 一别*叫用〃 +【)号;U ∙ ∕∏⅜
⅛⅛…工
—之"用叫
:叫％("i)qa⑼kj濡叫qa
tQJOJQJQqi
(次)
.（（$7瓢怡一$*叫恰）” + DMlΓ埠湍矶S —【）N
（（/L‘皆叫归-$*叫恰）〃 +【）厂］濡叫s —【）N巾濡叫
JBqj 5Λ∏q əM t(oχ) soτureuAp Qqj UlaIH 'foθΛj
■| j霜叫xZe龙一【)八 < 瑞：濡叫qaj sυι∣ U Θ2is d∂is əuivs
叫 1 妆M QD &q p∂uwM ^oaməu p∂2μi∂uιmnd-Λ∂^o uυ pun uoməu Θ]Suιs n 妆M 斗iom归U 叫「'乜叫上 q
uojiməij Oi Q uo↑ιm∂μ UIatfPloi(q[) SDiiuvuip pdimuixoiddn ∂ψ ιυψ əsoddn^ Kiuuiaq
Z WHHOHH1 ClNV '£ VWWHl 忆 VWWHl Ho HooXd Q
□
"O < ɔ jubjsuoə ɪɪŋms Xpu5τo^jns
əmos JOj ζ∣∣ *m∣∣∕0 = U Ajstjbs əzts dəjs Qqj 5Λ∏q oj səɔg^jns jτ oς ∙∣∣-pm∣∣Qj < L jπqj əsn əm QjQqM
3	书+；=小眇〃"N叫/
jτ PQQjUBiBnS əg ubə qoτqM
A	O=S
.曰-T > H + τ)[τ]^ε_ d*叫)〃Z
JBqj Moqs oj səɔgjns jτ tχ- < x joj x > (x + ɪ)^oɪ jπqj PUB ɪ - ɪ < x^oɪ jπqj iɔBJ əgi Suτs∩
【COZ mɔi JB jədŋd əɔuəjəjuoɔ B sb mətaəj jəpun
Under review as a conference paper at ICLR 2021
where the second inequality We use Hb ≥ 1 + a - b - ab for b ≥ -1 and the last inequality is
because (1 - θ - H - θ")(1 + 2η∣∣w*k2 - η2(3∣∣w*k2 -∣∣w(k)k2)(∣∣w*∣∣2 -∣∣w(k)∣∣2)) & (1 - θ -
6 -M)(1 + ηkw*k2) := ψ, as kw#K,tk2 . kw(1),tk2 < kw*k2 andthatη ≤ 3k£p.
|w(k),k |	|w(k),k |
「cnaeciieTltI▽ ∣ t____L 二> Qht ∣ 0__∖  NTQmeIv
Consequently,	(k),⊥ & ψ Il (k),⊥∣ ∣. Namely,
kwt	k	kw0	k
Il (k),⊥∣∣ < |w#KktIkwkK⊥k 1
kw#K，tk. 噂掰铲.
Moreover, by the condition that |w#(11),,tk | & |w#(kK),,kt|, we have that
k (k),⊥k . |W#KktIkw*0k 1 . |w(1)；k|kw#K0k 1
k #K,tk. iw#Kkoi 铲. 噂焉 不
(35)
(36)
□
Theorem: 2 (Snapshot at t) Suppose that the approximated dynamics (10) and (11) hold from 0 to
t and that at iteration t, the student network with a single neuron trained by GD with the step size
η has Iw#1,tI = cι,tkw*k2 for some number cι,t satisfying 1 > cι,t > 0. Denote c2,t a number
c kw(k),⊥ k
that satisfies \\k)#K 1t ≤ c21 for each k ∈ [K]. Suppose that the step size η also satisfies
|w#K,0[1]| ψ	,
η ≤ 3kW* ∣∣2 and makes ψ := (1 — θ — 6 — θ6)(1 + ηkw*k2) > 1. Then, an over-parametrized
network Wt#K trained by GD with the same η has
dist2(Wf1,w,) - dist2(Wt#Kw*) & kw*k2(2cι,t(P(1- 2tθ) √K - 1) - K (c2,t + c2,t) + c2J
Proof. Let us compute dist(Wt#1,w*) anddist(W#k,w*). For dis^W#1,w*)2, We have that
dist(Wt#1,w*)2 =	min IkWt#1 - w*q>k2 = kw#1) tk2 - 2|w(1)，k| + kw*k2
q∈{-1,+1}
=∣∣w(1),tk2+kw*k2 - 2qkw(1),kk2.
On the other hand, for dist2 (W#K ,w*), We have that
dist2(Wt#KwJ=	min	kW#K - w*q>kF
q∈RK "∣qk2≤l
=	min	tr((W#K - w*q>)>(W#K - w*q>))
q∈RK"∣q∣∣2≤1	'' t	t	7
=kW#KkF + kw*k2-2 max「tr((W#K)>w*q>)
q∈RK "∣q"2≤1
uK
kWt#K kF + kw*k2- 2tX Iw#Kkt∣2,
Where the last inequality is due to that
tr(vq>) =	max …tr(q>v) =) kvk.
q∈RK "0∣2≤1	'	)
max	tr((W#K )>w*q>) =)	max
q∈Rκ "∣qk2≤1	q∈RK "∣q∣∣2 ≤1
(37)
(38)
(39)
where (a) we denote V := [w(1K^,w(2K∖∙∙∙, w(K),k]> ∈ Rk and (b) is because q = v/kvk.
Combining (37) and (38), We have that
dist2(Wt#1,w.) - dist2(Wt#Kw*)
k	_______
2(t X Iw#KktI2 - √kwg,t k2) - (kWt#KkF -kWt#1kF).	(40)
k=1
18
61
.赤叫+/西 T叶础=：(而'"M逑P
tSMOJJOj sb unəj əəubjstp Qqj JO əɪenbs Qqj əsodmoɔəp
UBo əʌv •｛儿 > 后叫I PUe 儿 >	- ∣[τ]jm∣∣ : ⅛uɪuɪ =:。人工 əuɪjəp '0 < 儿 Jaqmnu B joj
(917)	,((zll,mllz>9ε- zll*mll>9ε)⅛ + τ)[τ]j∞ = [τ]τ+,∞
JBqj 5Λ∏q əm os ∣∣*m∣∣[χ]^
= M JBqj əioN Suotjbjou jo Ajtaqjq Qqj joj (ɪ) jdnɔsjədns Qqj PUB [# jdμosqns Qqj ssəjddns
∏tm əm 4Sutmo∏oj Qqj uτ ζ(χ st qoτqM) uojnəu juəpnjs Qqj JO jəgmnu Qqj InoqB ɪeəɪɔ st jt əɔuɪs
(⑴	《别(俨HQS—别*叫》〃 +【)「(俨=甫衿
((/屈叫Q£ - /1*叫戊)〃 +【)『(伊=I荒衿
st juəɔsəp juətpbj^ jo soτureuAp Qqj ζju5τpBj^ UODBlndOd qi[M
(m)	'S'λ~ z(∞ɪʃ)ɔ)% a 空=：(m)/ p≡mwuι
st əʌpɔəfgo Qqj ɪɪŋɔə^ 'Jooλj
∙∕C > ∣∣^m∣∣ Pm) ▽〃 + ɪ) < I ιυψ 孤叫
OLl > ? > QMpaMuuamMJ ∙0 < WC) - ∖∖*m∖∖Dj^')Df-9 =' oV 3j31Im「:瑞It吗 > d'l1
U-^Λ∕∣∣⅛∣∣ JsoI
svι∣(1[) Uidiqoidiofiuəəsəp iuəipm§ uθi[jj ∙q < ɔ iumsuoə unιus ^uəpiffns əiuos ιof ζ 11 11 /ɔ > U
Sdifsiins ∂2is dəis aιμ ιυψ ∂iunssy ∙∣∣0m∣∣Qj ‹ Lpun /tɔ/vɪ-ɪ < ∣∣*m∣∣ ιυψ əsoddn^ £ uɪəjoə^ɪ
£ WHHOHH1 Ho HOOHd H
□
(剑 ,	,
(与 + C'∣2 + ?R)X —(1 — XMe较—1)八)「DZ)/*叫 < (*鼠YjM逑P — (*葭1#‘M"S!P
'(W【步加 Suτuτqmoo Aq RQpaqjj (【【)səsn AjTjnnbQUT jsŋɪ Qqj QjQqM
(>)
1C> + j>)JIM⅛>
g∣∣*m∣∣	+ j⅛0 IS旧 ∣⅜⅛¾∣z∣∣*m∣∣ + ll*mll∣[τ]0⅞ml ʌʌ V4
小潟叫S，WK' 儿匕濡叫")令>
(串和 +M 濡硼 Z=.∣∣j⅛∣∣ Z=1∣∣^∣∣
L	X	X
JBqj5Λ∏qəmoς ∙∣∣*m∣∣^⅛□ >
ιr^ιι∣[τ]0⅞ι =仲	E*阳
「皆和M阳厂「肾和M阳I
> q∣[嚼俏H 印'42 jo uotjtu^əp Qqj PUB ζ∣∣*m∣∣^⅛ = ∣ jπqj PUB ɛ BUnnə[ Aq tQjounQqjjnj
, pl∙τ⅞l = eKτ),'(*m∣ < J11 τ ≠½111 Ji?qj	əʌ^ ζsxιπou aqj JO əɔuəjəjjɪp aqj joj tpuBq jaψo aqj uo
(M)⑴
1="
∙∕*叫E'u(1一x八S亿-i)/ʌ) = M：瘠叫(LXZʌ(e亿-i)/ʌ) < z∣∣f(*m∣∣Λ-z∣ mm∣d
jo punoq jəddn əgi PUB ζ∣∣ OI
JBqj 5Λ∏q əm k BUnIloqKHwIl [j½l∣∣ — i∖∖^M∖∖
濡闯	jo punoq jəmoj əgi pəəu əM 'ənupuoɔ oɪ
【COZ mɔi JB jədŋd əɔuəjəjuoɔ B sb mətaəj jəpun
Under review as a conference paper at ICLR 2021
In the latter part of this proof, we will show that kwt⊥ k ≤ γ for all t ≤ Tγ,C . Let us upper-bound
the norm of kwt k2 for t ≤ Tγ as follows.
kwtk2 = wt[1]2 + kw⊥k2 ≤ (k√Ck- γ)2 + Y2
I∣w*k2	l∣w*k 9
=⅛^- 2γ" + 2γ2,	(47)
where the inequality is because wt[1]2 ≤ (k√W∣k - Yy2 for t ≤ Tγ,c by the definition and that
kwt⊥ k ≤ γ for all t ≤ Tγ,C proved in the latter part. Hence, we have that
∣wt+ι[1]∣ = ∣wt[1]∣(l + η(3C∣∣w*k2- 3C2kwtk2))
≥ ∣Wt[1]∣(l + η(3Ckw*k2 - 3C2(MCJ - 2γkwck + 2γ2)))
：=∣wt[i]∣(ι + η∆c) = ∣wo[i]∣(ι + η∆c )t,	(48)
with
∆c := 6γC(√C∣∣w*k- CY)
Note that ∆c > 0 when ∣∣w* k > √Cγ.
Based on (48), it takes at most number of iterations
loo-( kw*V√C-Y)
T ≤ log( Iwo U )
γ,C — iog(i + η∆c)
for wt[1] to rises above kwck - Y if wo[1] > 0. Similarly, if wo[1] < 0, it takes Tγ iterations for
wo[1] satisfies wo[1] ≤ -kwck + γ.
Now we switch to show that 0 ≤ t ≤ Tγ,C, Y ≥ kwt⊥ k. We are going show that the perpendicular
component kwt⊥ k starts decaying before it could have increased above Y. From the dynamics of
∣∣wt+ιk = ∣∣w⊥k(l+η(CkwJ2-3C2 k wt k2)) ,wesee that once Ilwtk2 ≥ 3C ∣∣w*k2,the size ofthe
perpendicular component kwt⊥ k starts decaying. On the other hand, it is increasing when kwtk2 ≤
3Ck∣w* k2, which leads to the following before the perpendicular component starts decaying,
kwtk2 ≥ w2[1] = L|wk|2 = L∣wk-i∣2(1+ η(3C∣∣w*k2 - 3C2kwt-ik2))2
kw*k	kw*k
≥ 春∣wk-ι∣2(ι + 2ηC∣w*k2)2
≥ 」|w0∣2(l + 2ηCkw*k2)2t = ∣wo[1]∣2(l + 2ηCkw*k2)2t
∣∣w*k2
(49)
which means that that the size of ∣∣wt∣∣2 grows at the rate at least (1 + 2ηC∣∣w*∣∣2)2 before the
perpendicular component kwt⊥ k starts decaying.
The inequality (27) also implies that the number of iterations such that ∣wt∣2 ≤ 3C ∣∣w*∣2 is at most
t*
tC
“ 1	log( 3C∣赢[1]∣2 )
≤-------7----------------TΓ-
—2log ((1 + 2ηCkw*k2)2)
(50)
After t*C, we have that ∣wt⊥ ∣ is decaying. So we only have to show that for 0 ≤ t ≤ t*C the
perpendicular component never rise above Y. It suffices to show that an upper bound of ∣wt⊥ ∣ for
20
Under review as a conference paper at ICLR 2021
0 ≤ t ≤ tC is not greater than γ,
kw⊥k = kw⊥-ik(l+ η(Ckw*k2- 3C2kwt-ik2))
≤ kw⊥-ik(l+ η(Ckw*k2- 3C2w2-i[1]))
≤kw⊥k∙ ∏S=-1(1 + η(Ckw*k2- 3C2w2[1]))
≤) kw⊥k∙∏s=-1(1 + η(Ckw*k2 - 3C2w2[1](1 + η∆c)2s))
≤ γ,
(51)
where (a) we use (26). By taking logarithm on the both sides of ≤ , it suffices to show that
tC-1	?
log l∣w⊥k + X log (1 + η(C∣∣w*k2- 3C2w2[1](1 + η∆c)2s)) ≤ logγ. (52)
s=0
Using the fact that log X ≥ 1 一 ɪ and that log(1 + x) ≤ X for x > -1,it suffices to show that
t* -ι	..,..
X η(Ckw*k2 - 3C2W0[1](1 + η∆c)2s) ≤ 1 一 —.
s=0	γ
which can be guaranteed if
ηtCCkw*k2 ≤ 3C2ηw2[i](⅛∆Clt⅛ + ⅛，	⑴)
(1 + η∆C)2 一 1	10
where we use that Y ≥ 10∣w⊥∣. So it suffices to have the step size satisfy η = c∕∣∣w*k2 for some
sufficiently small constant c > 0.
□
E.1 Optimization landscape
In this subsection, we analyze the optimization landscape of the following,
minw∈Rd×κ f(W):=去 Pn=ι (C(x>w⑴)2 + C(x>W⑵)2 + …+ C(x>w(K))2 - y,2. (54)
We define
dist(W, √) ：= minq∈RK：|团"1 ∣W - √Cq>k∙	(55)
This is due to the observation that the set of global optimal solutions of (2) that achieves zero testing
error is √w*q> ∈ Rd×K for any q ∈ RK such that ∣∣q∣2 = 1. We denote VF(W):= Ex[Vf (W)]
and V2 F (W ):= Ex [V2f (W)], where f (∙) corresponds to (54).
Lemma 7. (locally strong convexity) Assume that dist(Wt0,襄):=∣∣Wto — √w= q> ∣∣f ≤ ν∣w*∣2,
where Wt° ∈ Rd×K, ν > 0, qt0 := arg minq∈RK "∣q∣∣2≤ι ∣∣Wt0 — √wc q>∣∣F, and w* ∈ Rd being the
teacher neuron. Then
vec(V)>V2F(Wt0)Vec(V) ≥ 2Ctr(qt°w>Vqtοw>V) + (2C — 14C3/2V — 2C2ν2)∣w*k2kVkF,
for any V ∈ Rd×K, where tr(∙) denotes the matrix trace.
Proof. For notation brevity, We will suppress script to in the following (i.e. W — Wt0, q - qt°).
We have that
vec(V)>V2f(W)vec(V)
m
=—X vec(V)> [((C∣∣x>W∣2 — yi)CIκ + 2C2W>x,x>W) 0 x,x>]vec(V)
m i=1
1 m	C2 m
=一 ^X C(C∣∣x>Wk2 — yi)vec(V)>vec(χiX>V) +----^X vec(V)>vec(2xix>VW>XiX>W).
m i=1	m i=1
m
=mm X [C(C∣X>Wk2 -kx>w*q>k2)kx>vk2 + 2c2(χ>wv>0)2].
m i=1
21
Under review as a conference paper at ICLR 2021
By taking the expectation above over Xi 〜N(0, Id) and using Lemma 6, we have that
E[vec(V)τV2/(W)vec(V)] = C2∣∣WIIFIWIF + 2C2∣∣VTWIIF - C(∣∣w*q>∣∣FIlVIIF + 2∣∣V>w*q>∣∣F)
+ 2C 2(tr(W TV )2 + tr(W TVW TV) + ∣∣WV TkF).
(56)
Now let US set W =觉qT + θH with a H satisfying ∣∣H∣∣f = 1 and θ := ν∣∣w* ∣∣ for some number
ν > 0.
IW IF = I√w= qTIF + θ2∣∣H∣∣F + 2θh√C qT,H〉
≥I√W= I2- 2θ∣登qT||F∣∣H∣∣f
≥I√wc I2- 2θ∣ 登 IIHIf
IV tW IF = IVT √wC qTIF + θ2IV THkF + 2θtr(VT 宗 QTH t V)
≥IVT√wCqτIF - 2θI√CQtIIHIfIVif
IVW τ∣F = ∣Vq( √⅛ )τ∣F + θ2∣VH τ∣F + 2θtr(V (j)τqHVτ)
CC
≥ ∣Vq(√C)τ∣F - 2θ∣(√w= )τq∣∣H∣F∣VIF
=∣Vq(√C)τ∣F - 2θ∣(√w= )∣∣HIfIVIF
, -I- -T	I^-I- -T	w	W几.~T	-τ
tr(W τVW τV) = -tr(qw> VqwT V) + 2θtr(H τVq( T )τV) + θ2tr(H τVH τV)
CC
=Ctr(qw>VqwTV) - 2θ∣(爰)qτ∣∣H∣fIVIF - θ2∣HIF∣V∣F.	(57)
Combining (56) and (57), together with the bilinear property of the expectation so that
vec(V)τV2F(w)vec(V) = E[vec(V)τV2f (W)vec(V)], we have that
vec(V)tV2F(W)vec(V) ≥2Ctr(qw>VqwTV) + 2C∣Vqw>∣F
-14C3∕2θ∣w*∣∣H∣F∣VIF - 2θ2c2∣h∣F∣v∣F
=2C tr(qwT VqwT V) + 2C∣w* ∣2∣ V∣F
-14C34∣w*∣∣H∣F∣VIF - 2θ2c2∣h∣F∣v∣F
=2Ctr(qwτVqwTV) + (2C - 14C3/2V - 2C2ν2)∣w*∣2∣V∣F,
(58)
where (a) is due to that ∣AB∣F ≥ σr2(A)∣B∣F with r being the rank of A, which in our case
A := w*qτ is a rank one matrix and B := V; consequently σ2(A) = ∣w*qτ∣F = ∣w*∣2∙	□
Lemma 8. (smoothness) Assume that dist(Wt0,登):=∣ W —登q> ∣f ≤ ν∣w*∣2 where Wt0 ∈
Rd×K, v > 0, qto := argmi∏q∈RK Rq∣∣2<1 ∣W 一登 qτ∣F, and w* ∈ Rd being the teacher neuron.
Then, ∣V2F(W)∣2 ≤ (15C +16C2v2)∣w*∣2.
22
Under review as a conference paper at ICLR 2021
Proof. For notation brevity, We will suppress script to in the following (i.e. W J Wt°, q J qt°).
kV2F(W)k2 = kE[[C(Ckx>Wk2 - ∣∣x>w*q>k2)lκ + 2C2W>xx>W]乳 xx>] ∣∣2
≤ kE[[C∣Ckx>Wk2 - kx>w*q>k2IIK + 2C2kx>Wk2lκ]乳 xx>] k2
(a)
≤ kE[C∣Ckx>Wk2 -kx>w*q>k2∣xx>] k2 + 2C2kE[kx>Wk2xx>] k2
=) C kE[(C kx>W k2 -kx>w*q>k2)xx>] k2 + 2C 2kkW kF Id + 2WW >∣∣2
=Ck(CkW kF -kw*q>kF )Id + 2(CWW > - w*w>)k2 + 2C 2kkW kF Id + 2WW >k2
≤ 7C 2k W kF + Ckw*q>kF + 2CkCWW > - w*w>k2
≤ (15C +16C2ν2)kw*k2,
(59)
where (a) is due to ∣∣I 0 A∣∣2 ≤ ∣∣I k2kAk2 = k Ak2, (b,c) is due to Lemma 6, and the last inequality
is by setting W =登q> + θH with a H satisfying j∣H∣∣f = 1 and θ := V∣∣w*k∙
□
Lemma 7 and Lemma 8 together implies that when the the iterate is in the neighborhood of a global
optimal solution, then gradient descent has a linear convergence rate. The proof essentially follows
the same lines as the proof of Lemma 1. Hence, we omit its proof.
Lemma 9. (locally linear convergence) Suppose that at time to, dist(Wt0,w*) := ∣∣Wt0 一
√w⅛q> k ≤ v∣∣w*k where Wt0 ∈ Rd×K, ν > 0 satisfies (2C — 14C3/2V — 2C2ν2) > 0,
and qto := argminq∈RKRq∣∣2≤1 ∣∣Wt0 一 慧q>k. Then, gradient descent with the Step size
η ≤ (IC-I^CC/V-2ICgenerates the iterates {Wt}t≥t° satisfying
dist2(Wt+1,w*) ≤ (1 — η(2C — 14C3/2V — 2C2ν2))dist2(Wt,w*).
Remark: We will need V in Lemma 9 to satisfy (2C - 14C3/2V - 2C2V2 ) > 0 in order to show
that gradient descent enters the linear convergence regime. For a fixed C, the condition translates
to v ≤ 0.14∕√C. Now we also see that the condition ∣∣w*k > 1.1 √Cγ in Theorem 3 is trivially
satisfied when we set 2γ2 = V2∣∣w*k2, since 1 > 1.1 √C ∙ 0.14/√2∕√C. Note that the reason why
we set 2γ2 = V2∣∣w*∣∣2 is because dist2(wt,登)=∣∣wt[1]∣ - k葭k∣2 + kw⊥k2 ≤ 2γ2 = v2∣∣w*k2
implies the iterate wt is at the benign region that allows linear convergence when t = Tγ,C.
23
Under review as a conference paper at ICLR 2021
F More empirical results
(a) Component A vs. t.
(b) Component B vs. t.
(c) Component C vs. t.
(d) Component D vs. t.
(e) Component A vs. t.
(f) Component B vs. t.
(h) Component D vs. t.
(g) Component C vs. t.
Figure 5: Subfigure (a) shows w(k),k (1 + η(3∣∣w*k2 - 3||w(k)∣∣2)) (i.e. component A of
w#(kK),,kt) versus iteration t for each neuron k. Subfigure (b) plots 2η PjK6=k((wt(j))>wt(k))wt(j),k +
ηwt(k),k PjK6=k ∣wt(j) ∣2 (i.e. component B of w#(kK),,kt) versus iteration t for each neuron k. Subfigure
(c) plots the norm of component C of w(kK⊥, while subfigure (d) plots the norm of component D of
(k),⊥
w#K,,t for each neuron k. Our empirical findings show that the components due to interaction of the
other neurons (i.e. component B and D) are small (notice that the scale of the vertical axis of (a) and
(b), (c) and (d) are different) compared to their counterparts (i.e. component A and C respectively),
which suggests that θ," U 10-4 on (10) empirically. Top row: number of neurons K = 10. Bottom
row: number of neurons K = 3.
(a) K = 1.
(b) K = 3.
(c) K = 10.
Figure 6: The perpendicular component ∣w(kK⊥ ∣∣ of each k over iterations t. We see that the
perpendicular component remains small.
24
Under review as a conference paper at ICLR 2021
F.1 Different step sizes
Following the simulation as Figure 1, we tried different values of step sizes η =
{1.0, 0.5, 0.4, 0.3, 0.2, 0.1, 0.05, 0.01, 0.005, 0.001} for each model with different number of neu-
rons K = {1, 3, 10}. We report the quantity (9) over iteration t,
vec(w*q> - WaK)>v2f (Wt#K)vec(w*q> - WaK),
where V2f (Wt#K) ∈ RdK×dK is the Hessian and w*qt> is the closet global optimal solution to
WaK and the notation vec(∙) represents the vectorization operation of its matrix argument. Recall
that the quantity can be viewed as a measure of the strong convexity as mentioned in the main text.
Specifically, if the quantity is larger than 0, then it suggests that the current optimization landscape
is strongly convex with respect to w*qt> .
We found out that gradient descent with step size η = {1.0, 0.5, 0.4, 0.3} either diverges or cannot
converge towards zero testing error for all K = {1, 3, 10}, which means that η = 0.2 is basically
the best step size of gradient descent for each model K . So the result suggests that even under op-
timal tuning of the step size η for each model K , gradient descent for an over-parametrized model
converges faster than the case for a smaller model. We show some results of using different η on Fig-
ure 7, Figure 8, and Figure 9. Based on the empirical results, we conclude that the acceleration due
to over-parametrization cannot be simply explained by that gradient descent uses a larger effective
step size, as the impacts due to parameters η and K seem to be complementary in the experiment.
0.5
0-
-0.5-
-1.5-
IOnPRd JO-δφ>IUEωsθH J20θ>
-2.5 H------------r
0	20
40	60	80	100
iterations
(a) (η = 0.2) quantity (9) over t.
-2.5
0-
-0.5-
-1.5-
-5npRd J01Qθ>IUEωSΘH J2sφ>
50
100
iterations
150	200
(b) (η = 0.05) quantity (9) over t.
0.5
JOnPRd Joj oθ>IUEωSΘH J20φ>
100	200	300	400	500
iterations
-2.5--
0
K=1 neuron
K=3 neurons
K=10 neurons
(c) (η = 0.01) quantity (9) over t.
o-
-0.5-
-1.5-
-2 -
Jonpo,Id -IoJOΦ>IUEωSΘH j208>
0	200	400	600	800	1000
iterations
(d) (η = 0.005) quantity (9) over t.
Figure 7: Gradient descent with different values of the step size. Note that the scales of the horizontal
axes are different. Both the step size η and the degree of over-parametrization affect the time that
gradient descent enters the linear convergence regime. A larger step size η and a larger number of
neurons K help gradient descent to make progress faster.
-2.5
25
Under review as a conference paper at ICLR 2021
(a) (η = 0.5) training error over t.
(b) (η = 0.5) quantity (9) over t.
Figure 8: Gradient descent with η = 0.5. We see that gradient descent cannot converge to zero
training (and testing) error. That is, the step size is too large to converge to a global optimal solution.
Interestingly, we still observe that gradient descent requires fewer iterations to get closer to a global
optimal point for a larger model, though it does not converge to a global optimal point using the
large step size.
Figure 9: We plot the number of iterations required for the metric vec(w* q> -
W#K)>V2f (Wt#K)vec(w*q> — WaK) to be positive (the y-axis) under different values of the
step size η (the x-axis).
26