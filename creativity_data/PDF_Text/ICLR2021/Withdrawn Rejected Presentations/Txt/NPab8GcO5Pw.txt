Under review as a conference paper at ICLR 2021
On the Landscape of Sparse Linear Networks
Anonymous authors
Paper under double-blind review
Ab stract
Network pruning, or sparse network has a long history and practical significance
in modern applications. Although the loss functions of neural networks may yield
bad landscape due to non-convexity, we focus on linear activation which already
owes benign landscape. With no unrealistic assumption, we conclude the fol-
lowing statements for the squared loss objective of general sparse linear neural
networks: 1) every local minimum is a global minimum for scalar output with any
sparse structure, or non-intersected sparse first layer and dense other layers with
orthogonal training data; 2) sparse linear networks have sub-optimal local-min for
only sparse first layer due to low rank constraint, or output larger than three di-
mensions due to the global minimum of a sub-network. Overall, sparsity breaks
the normal structure, cutting out the decreasing path in original fully-connected
networks.
1	Introduction
Deep neural networks (DNNs) have achieved remarkable empirical successes in the domains of
computer vision, speech recognition, and natural language processing, sparking great interests in
the theory behind their architectures and training. However, DNNs are often found to be highly
overparameterized, making them computationally expensive with large amounts of memory and
computational power. For example, it may take up to weeks on a modern multi-GPU server for
large datasets such as ImageNet (Deng et al., 2009). Hence, DNNs are often unsuitable for smaller
devices like embedded electronics, and there is a pressing demand for techniques to optimize models
with reduced model size, faster inference and lower power consumption.
Sparse networks, that is, neural networks in which a large subset of the model parameters are zero,
have emerged as one of the leading approaches for reducing model parameter count. It has been
shown empirically that deep neural networks can achieve state-of-the-art results under high levels
of sparsity (Han et al., 2015b; Gale et al., 2019; Louizos et al., 2017a). Modern sparse networks
are mainly obtained from network pruning (Zhu & Gupta, 2017; Lee et al., 2018; Liu et al., 2018;
Frankle & Carbin, 2018), which has been the subject of a great deal of work in recent years. How-
ever, training a sparse network with fixed sparsity patterns is difficult (Evci et al., 2019) and few
theoretical understanding of general sparse networks has been provided.
Previous work has already analyze deep neural networks, showing that the non-convexity of the
associated loss functions may cause complicated and strange optimization landscapes. However, the
property of general sparse networks is poorly understood. Saxe et al. (2013) empirically showed
that the optimization of deep linear models exhibits similar properties as deep nonlinear models,
and for theoretical development, it is natural to begin with linear models before studying nonlinear
models (Baldi & Lu, 2012). In addition, several works (Sun et al., 2020) have show bad minimum
exists with nonlinear activation. Hence, it is natural to begin with linear activation to understand the
impact of sparsity.
In this article, we go further to consider the global landscape of general sparse linear neural net-
works. We need to emphasize that dense deep linear networks already satisfy that every local min-
imum is a global minimum under mild conditions (Kawaguchi, 2016; Lu & Kawaguchi, 2017), but
findings are different and complicated for sparse linear network. The goal of this paper is to study
the relation between sparsity and local minima with the following contributions:
•	First, we point out that every local minimum is a global minimum in scalar target case
with any depths, any widths and any sparse structure. Besides, we also briefly show that
1
Under review as a conference paper at ICLR 2021
similar results hold for non-overlapping filters and orthogonal data feature when sparsity
only occurs in the first layer.
•	Second, we find out that sparse connections would already give sub-optimal local minima
in general non-scalar case through analytic and numerical examples built on convergence
analyze. The local-min may be produced from two situations: a sub-sparse linear network
which owes its minimum as a local-min of the original sparse network; a rank-deficient
solution between different data features due to sparse connections, while both cases verify
the fact that sparsity cuts out the decreasing path in original fully-connected networks.
Overall, we hope our work contributes to a better understanding of the landscape of sparsity network
on simple neural networks, and provide insights for future research.
The remainder of our paper is organized as follows. In Section 2, we derive the positive findings of
shallow sparse linear networks, providing similar landscape as dense linear networks. In Section 3,
we give several examples to show the existence of bad local-min for non-scalar case. In section 4,
we briefly generalize the results from shallow to deep sparse linear networks. Some proofs are in
Appendix.
1.1 Related Work
There is a rapidly increasing literature on analyzing the loss surface of neural network objectives,
surveying all of which is well outside our scope. Thus, we only briefly survey the works most related
to ours.
Local minima is Global. The landscape of a linear network date back to Baldi & Hornik (1989),
proving that shallow linear neural networks do not suffer from bad local minima. Kawaguchi (2016)
generalized same results to deep linear neural networks, and subsequent several works (Arora et al.,
2018; Du & Hu, 2019; Eftekhari, 2020) give direct algorithm-type convergence based on this benign
property, though algorithm analysis is beyond the scope of this paper. However, situations are
quite complicated with nonlinear activations. Multiple works (Ge et al., 2017; Safran & Shamir,
2018; Yun et al., 2018) show that spurious local minima can happen even in two-layer network with
population or empirical loss, some are specific to two-layer and difficult to generalize to general
multilayer cases. Another line of works (Arora et al., 2018; Allen-Zhu et al., 2018; Du & Hu, 2019;
Du et al., 2018; Li et al., 2018; Mei et al., 2018) understands the landscape of neural network in
an overparameterized setting, discovering benign landscape with or without gradient method. Since
modern sparse networks reserve few parameters compared to overparameterization, we still seek a
fundamental view of sparsity in contrast. Our standpoint is that spurious local minima can happen
when applied with specific sparsity even in linear networks.
Sparse networks. Sparse networks (Han et al., 2015b;a; Zhu & Gupta, 2017; Frankle & Carbin,
2018; Liu et al., 2018) have a long history, but appears heavily on the experiments, and mainly
related to network pruning, which has practical importance for reducing model parameter count
and deploying diverse devices. However, training sparse networks (from scratch) suffers great dif-
ficulty. Frankle & Carbin (2018) recommend reusing the sparsity pattern found through pruning
and train a sparse network from the same initialization as the original training (‘lottery’) to obtain
comparable performance and avoid bad solution. Besides, for fixed sparsity patterns, Evci et al.
(2019) attempt to find a decreasing objective path from ‘bad’ solutions to the ‘good’ ones in the
sparse subspace but fail, showing bad local minima can be produced by pruning, while we give
more direct view of simple examples to verify this. Moreover, several recent works also give abun-
dant methods (Molchanov et al., 2017; Louizos et al., 2017b; Lee et al., 2018; Carreira-Perpinan &
Idelbayev, 2018) for choosing weights or sparse network structure while achieving similar perfor-
mance. In theoretical view, Malach et al. (2020) prove that a sufficiently over-parameterized neural
network with random weights contains a subnetwork with roughly the same accuracy as the tar-
get network, providing guarantee for ‘good’ sparse networks. Some works analyze convolutional
network (Shalev-Shwartz et al., 2017; Du et al., 2018) as a specific sparse structure. Brutzkus &
Globerson (2017) analyze non-overlapping and overlapping structure as we do, but with weight
sharing to simulate CNN-type structure, and under teacher-student setting with population risk. We
do not follow CNN-type network but in general sparse networks, though still linear, to conclude
straightforward results.
2
Under review as a conference paper at ICLR 2021
2 Landscape of Shallow Sparse Linear Networks
2.1	Preliminaries and Notation
We use bold-faced letters (e.g., w and a) to denote vectors, capital letters (e.g., W = [wij] and
A = [aij]) for matrices. Let PX be the orthogonal projection to the column space of the matrix X,
and λi(H) is the i-th smallest eigenvalue of a real symmetric matrix H.
We consider the training samples and their outputs as {(xi, yi)}in=1 ⊂ Rdx × Rdy , which may
come from unknown distribution D. We form the data matrices X = [x1, . . . , xn]T ∈ Rn×dx and
Y = [y1, . . . , yn]T ∈ Rn×dy, respectively. In our analysis in Sections 2 and 3, we consider a
two-layer (sparse) linear neural network with squared loss:
min L(W,A) = 2∣∣Y - XWAkF,	⑴
where the first layer weight matrix W = [w1, . . . , wd] ∈ Rdx×d , and the second layer weight
matrix A = [a1, . . . , ad]T ∈ Rd×dy. After weights pruning or sparsity constraint, many weights
parameters become zero and would not be updated during retraining. We adopt Sj := {k : wkj = 0}
as pruned dimensions in thej-th column of W, and -Sj := Sjc = [dx]\Sj, where [d] := {1, . . . , d}.
In addition, wj,S denotes the sub-vector of wj choosing the positions in S, XS the sub-matrix ofX
choosing the column indices in S.
We let pj = dx - |Sj |, where |S | is the cardinality of the set S. Then wj,-Sj ∈ Rpj is the
remaining j-th column in first layer weight which leaves out pruned dimension set Sj . Similarly,
X-Sj ∈ Rn×pj means the remaining data matrix connected to j -th node in the first layer.
Finally, for simplicity, We denote X-j = X-Sj, w-j = Nj-Sj, and (∙) as the pruned layer weight
with several zero elements not updated all along, if no ambiguity.
Before we begin, a small note on the sparse structure we concern: there may have unnecessary
connections and nodes, such as a node with zero out-degree which can be retrieved and excluded
from the final layer to the first layer, and other cases are showing in Appendix C. Thus we do not
consider them in the subsequent proof and assume each data dimension has valid output connection,
i.e., ∩d=ιSj = 0.
2.2	Scalar Case
In the scalar case, assume dy = 1. We then simplify A = (a1, . . . , ad)T. When pruning any weight
ai in the second layer, the output of the i-th node in the first layer contribute zero to final output.
Hence wi can also be pruned. Without loss of generality, we assume second layer parameters are
not pruned. After pruning several parameters, the original problem becomes
1
min L(f ,A) := - Y 一 (X-ι,
w-i ,ai	2
a1w-1	1 2
X-i, ... , X-d)	. I
adw-d	F
(2)
Theorem 1 For a two-layer linear neural network with scalar output and any sparse structure,
every local minimum is a global minimum.
Proof: From Eq. (2), if a local minimizer satisfies ai = 0 for some 1 ≤ i ≤ d, then based on the
second order condition for a local minima, we have
/	∂2L
∂a2
∂L
∖∂ w-i∂ai
∂2L ∖
∂αi∂wTi
∂L
∂w-i∂ W-J
0,
(3)
3
Under review as a conference paper at ICLR 2021
Input Hidden Output
layer layer 1 layer
Input Hidden Output
layer layer 1 layer
Figure 1: Sparse network without (left) / with (right) overlapping filters in the first layer.
which implies that
(	WLX-iX-iW-i	-(Y - Pd=ιX-iw-iai	X-i
[一XTi (Y - Pd=I χ-iw-iai)	0
Then X-Ti Y-Pid=1X-iw-iai = 0, which is the global minimizer condition of w-iai.
Otherwise, ai 6= 0, then from the first-order condition for a local minima,
∂L
∂w-i
aiX-Ti Y - X X-iw-iai
0,
showing that X-Ti Y - Pid=1 X-iw-iai = 0, which also gives the global minimizer condition
of w-iai. Hence every local minimum is a global minimum.
2.3 Non-scalar Case with Dense Second Layer
Now we discuss the case of non-scalar outputs. By the intractable and various sparse structure, we
first consider pruning only the first layer while retaining the dense second layer. Then the remaining
problem is formulated as follows:
2
min L(Wf, A)
w-i ,ai
d
Y - X X-iw-iaiT
i=1
(5)
1
2
F
Intuitively, if we can separate the weight parameters into d parts, based on linear network results,
we can still guarantee no bad local-min. We show that non-overlapping first layer weight or disjoint
feature extractor, as the left graph of Figure 1 depicts, and orthogonal data feature meet requirements.
Theorem 2 For a two-layer sparse linear neural network with dense second layer, assume that X
is full column rank, and∀ i 6= j, X-T i X-j = 0. Then every local minimum is global.
Proof: Since ∀ i 6= j, X-TiX-j = 0 and X is full column rank, X-i and X-j share no same
columns. Additionally, from our assumption ∩dd=ιSj = 0, We have ∩dd=ιSj = [dχ], meaning that
(X-1, . . . , X-d) is X with different arrangement of columns. Hence,
d
Y=PXY+(I-PX)Y=X(XTX)-1XTY+(I-PX)Y=XX-iZi+(I-PX)Y, (6)
i=1
Where Zi = (XTX)-1XTY -S is the sub-matrix choosing the roW indices in -Si. Then We
only need to consider the objective:
2
1
min Lf, A)=
w-i ,ai	2
d
X X-i (Zi - w-ia-)
i=1
d
2 X ∣∣X-i (Zi- w-i aT) ∣∣F.
F i=1
(7)
4
Under review as a conference paper at ICLR 2021
Input	Hidden	Output
layer	layer 1 layer
Input Hidden Hidden Hidden Output
layer layer 1 layer 2 layer 3 layer
Figure 2: Left: spurious local minimum exists in sparse linear network with dy = 3 shown in above,
that is the global minimum of a sub-network. Right: one simple weight assignment for obtaining
the global minimum in deep sparse linear network with scalar output.
We will see that the objective has already been separated into d parts, while each part is a two-layer
dense linear network with full column rank data matrix. Based on Theorem 2.2 in Lu & Kawaguchi
(2017) or Eckart-Young-Mirsky theorem (Eckart & Young, 1936; Mirsky, 1960), we obtain that
every local minimum is a global minimum.
Additionally, we need to explain the assumption that non-overlapping filters in the first layer involves
convolution networks (Brutzkus & Globerson, 2017) if weight sharing is used. Otherwise, we will
show a bad local minima exists when the first layer is overlapped or training data is not orthogonal
in Section 3.
2.4	GENERAL CASE WITH dy = 2
Previous findings imply positive results with one-dimension outputs, or specific sparse structure
and data assumption. In this subsection, we discuss an arbitrary sparse structure with outputs of
dimension dy = 2. We first prove that some connections still owe common benign landscape which
can be removed.
Theorem 3 For a sparse two-layer network, a node with full out-degree and one in-degree can be
removed if we consider the remaining structure with objective under some projected data, having no
influence for spurious local minima.
Previous result simplifies the sparse structure including such a hidden node with one connection to
input and full connection to output. Next, we will provide another type reduction with only one
connection to output when sparsity is applied to both layers with dy = 2.
We mention the final layer output as Node 1 and Node 2, and the hidden nodes which have only
one connection to the output layer as R1 and R2 while the remaining full connection set as R. Set
T1 = ∩j∈R1Sj, T2 = ∩j∈R2Sj. We define U(T) = {j : wij 6= 0, i ∈ T} as the hidden node set
connected with data feature in T. When the hidden node sets connected to T1 and T2 satisfy the
condition below, we are able to simply the sparse structure into the dense layer case.
Theorem 4 For a sparse two-layer linear network with dy = 2, if UTI ∩ T2) ∩ U (Ti \ T2) = 0
and U (Ti ∩ T2) ∩ U (T \ T1) = 0, then there is a sub-network with dense Second layer optimized
with some projected training data, sharing the same local minimum for the remaining parameters.
The formal proof of the theorem can be found in Appendix E. Additionally, from the proof of
Theorem 4, the objective is converted into two objectives with weight sharing in the first layer
even the assumption does not meet. Weight sharing structure has been shown in some related work
(Shalev-Shwartz et al., 2017; Brutzkus & Globerson, 2017), so we do not give detailed description
and leave it as future work.
5
Under review as a conference paper at ICLR 2021
Now for a sparse two-layer linear network with dy = 2, we focus on the case which has dense
second layer. If one hidden node only has one in-degree, then based on Theorem 3, we can remove
such node and consider the objective optimized with some projected data. Therefore, each hidden
node should have at least two in-degree. Because one hidden node obviously leads to no bad local
minima, the least sparse structure has two hidden nodes with totally eight connections (e.g., two
constructions in Figure 1). We will show the existence of bad local-minima in Section 3.
2.5	GENERAL CASE WITH dy ≥ 3
We finish this section by discovering that a sub-network with its global minima might yield a spuri-
ous local minima of the original sparse network when dy ≥ 3.
Theorem 5 There exists a spurious local minima that is a global minimum of sub-network in two-
layer sparse network when output dimension dy ≥ 3.
Proof: We consider the sparse structure in Figure 2 with only eight connections. The objective is
min L(w1, . . . , w8)
wi
w1	0
Y - X w2	w3
0 w4
w5	w6
0 w7
1
2
1
2
w1w5	w1w6
w2w5	w2w6 + w3w7
0	w4w7
w082F
0
w3w8
w4w8
2
F
Let X = I3 and Y = 2
0
20
10 0 . Clearly, X and Y have full column rank that is the common
04
w1	0	1
assumption in previous work. Then w2 w3	= 2
0	w4	0
w6
w7
w08	=	10
20
30
satisfy VL = 0, and L(wι,..., W8)= 8. In addition, for any small disturbances δi,i = 1..., 8,
2L(w1 + δ1, . . . , w8 + δ8) = (δ1 + δ5 + δ1δ5)2 + (2δ1 + δ6 + δ1δ6)2 + (δ2 + 2δ5 + δ2δ5)2
+ (2δ2 + 2δ6 + δ2 δ6 + 3δ3 + 2δ7 + δ3 δ7 )2
+ (2 + δ3)2δ82 + (3 + δ7)2 δ42 + (δ4δ8 - 4)2
≥ (2 + δ3)2 δ82 + (3 + δ7)2δ42 + (δ4δ8 - 4)2
≥ 2[(2 + δ3)(3 + δ7) — 4] ∣δ4δ81 + 16.
Since the perturbations δi are small, we have (2 + δ3) (3 + δ7) -4 > 0. Hence, L(w1+δ1, . . . , w8+
δ8 ) ≥ 8, verifying the local minimizer.
∕wι	0 ∖	∕√10∕5
However, when w2 w3	=	√10
0	w4	0
L(w1, . . . , w8) = 0.18 < 8. Hence, a bad local minimum exists.
We underline that the bad local minimum is produced from the sub-network when w4=w8 =0. Since
we encounter no bad local minimum in a dense linear network, sparse connections indeed destroy
the benign landscape because sparsity obstructs the decreasing path as Evci et al. (2019) mentioned
from experiments.
3	Bad Local Minimum with Sparse First Layer
Now we turn back to the dense second layer case in Section 2.3 with dy=2, and assume X has full
column rank. We give an algorithm to check the existence of spurious local minima when ∃ i 6= j ,
s.t.,X-TiX-j 6= 0.
6
Under review as a conference paper at ICLR 2021
Algorithm 1 Sparse-2-Opt (Z1, Z2, D): Obtain the solution of two-layer sparse linear neural net-
Work With two hidden neurons._______________________________________________________________
1:	Input: Target matrix Z1,Z2 and covariance diagonal matrix D.
2:	Initialize w2 , d2, a2 ;
3:	while not converge do
4:	w1, d1, a1 = SV D(Z1 + D(Z2 - d2w2a2T));
5:	w2, d2, a2 = SV D(Z2 + DT (Z1 - d1w1a1T));
6:	end while
7:	w1 = d1w1, w2 = d2w2.
8:	if λ1(V2L),λ2(V2L) ≈ 0, λ3(V2L) > 0 then
9:	Return the solution w1, a1, w2, a2.
10:	else
11:	Try again from another initialization.
12:	end if
13:	Output: w1, a1, w2, a2.
Notice that We have no rank constraint for the Zi in Eq. (5). Suppose singular value decomposition
of X-i as X-i = UiDiViT With Ui ∈ Rn×pi, Di ∈ Rpi ×pi, Vi ∈ Rpi×pi. Since Di has full rank,
We take DiViZi and Di Viwi as neW targets and variables. With a slight abuse of notation, then the
problem becomes
1
min -
Wf,A 2
EUi (Zi - WiaT)
i=1	F
(8)
In the folloWing, We shoW d = 2 is enough to give counter examples. Similarly, using the singular
value decomposition of UTU as UTU = UDvT with a rectangle diagonal matrix D ∈ Rp1 ×p2.
Notice that U1 , U2 are column orthogonal matrices, thus Dii ≤ 1, and |{i : Dii = 1}| equals to the
overlapping columns between X-1 and X-2 . Finally, the objective becomes
L(wι, W2, aι, a2) = 1∣∣Zι-WiaTIlF + 1 ∣∣Z2-w2aτ∣∣F + tr[(Zι-wιaT)TD(Z2-w2aT)].
Ifwe fix w2 anda2, we can see w1 and a1 are the best rank-1 approximation of Z1+D(Z2 -w2a2T),
since w1 and a1 are the solution of
arg min ∣Z1 + D(Z2 - w2a2T) - w1a1T∣2F.
w1 ,a1
Similarly, w2 and a2 are the best rank-1 approximation of Z2 + DT (Z1 - w1a1T). Empirically, we
use alternating update method to find the solution in Algorithm 1 for two blocks, where SVD(∙) is
a classical method getting the largest singular value and the corresponding singular vectors.
Since each update does not increase the loss, this makes the convergence of sequence
w1, a1, w2, a2. Once the algorithm converges, the first-order condition is satisfied and two eigen-
vectors with zero eigenvalue of the Hessian matrix are decided. Moreover, we can also prove that
the convergent solution is indeed a local minima (detail see Appendix B). Otherwise, we examine a
local minimum using gradient descent or other optimization method started with noise, if necessary.
Based on Algorithm 1, we find several cases with bad local minima including the overlapping case
(∃i, Dii = 1). The results are shown in Table 1. We observe distinct gaps between the local minima
because our choice of elements in the Zi is small. In the non-overlapping setting, the algorithm
reaches the local min quickly and shows several different examples. As for the overlapping setting,
a simple construction is leaving out the repeated feature away with zero items in the Zi , though we
also show bad-min applied with the overlapping data feature in Row 3 in Table 1.
It is interesting to note that for d = 2, only at most two local minimum are found, and we can
easily broaden the alternating update method into general d case in Appendix D, that will also verify
similar observation: at most d local minimum produced by a sparse-first-layer network with hidden
d nodes, which leaves as future work. Overall, sparsity breaks the original matrix structure, leading
to additional low rank constraint in this case, and still cuts out the decreasing path in the original
fully-connected network.
7
Under review as a conference paper at ICLR 2021
Table 1: Examples found by algorithm with spurious local minimum. All experiments run 600
iterations, except last one with 1000 iterations. λi := λi(V2L).
Z1		D	λ3	λ1, λ2	kVLk2	Objective L
11 01		-^05~oʌ 0 0	0.9 )	2.1 ∙ 10-1 1.4 ∙ 10-1	0 〜10-14 0 〜10-14	< 10-14 < 10-14	0.5143043518476 0.6781647585271
-02	-01	(⅛1)	-<0.8~oʌ 0 O	0.1 )	3.4 ∙ 10-1 1.7 ∙ 10-1	0 〜10-14 0 〜10-14	< 10-14 < 10-14	0.5373672988360 0.6805528480352
--111	001!	(-02 0)	/1	0 o∖ 0	0.6	0 ∖0	0	0.8/	1.7 ∙ 10-1 2.5 ∙ 10-1	0 〜10-14 0 〜10-14	< 10-14 < 10-14	0.8980944246693 0.4712847600704
Additionally, a descent algorithm still will diverge to infinity. For instance, the example in Appendix
A shows that there is a sequence diverging to infinity while the function values are decreasing and
convergent.
4 Landscape of Deep Sparse Linear Networks
In this section, we briefly extend Theorems 1 and 2 to deep sparse linear networks and leave the
proof in Appendix F. The intuition is that deep linear networks have similar landscape property as
the shallow case (Lu & Kawaguchi, 2017; Eftekhari, 2020). However, understanding the landscape
of an arbitrary deep sparse linear network is still complicated.
Theorem 6 For a deep sparse linear neural network with scalar output (dy = 1) and any sparse
structure, every local minimum is a global minimum.
The proof intuition can be described by induction based on shallow linear networks. The above
theorem shows that sparsity introduces no bad-min when applied with scalar target. In addition, we
give a simple choice for obtaining a global minimizer below.
How to obtain a global minimizer in scalar case: One way is to set the first-layer weights as the
global minimizer in the two-layer case with ai = 1, then the remaining layers uniformly distribute
the output of each node to the next layer. For example, if one node has k output connections, then
each connection assigns weight 1/k. Hence, the sum of each layer output remains the best solution
to approximate target Y (see the right graph of Figure 2 for example).
Theorem 7 For a deep sparse linear neural network with a sparse first layer and dense other layers,
assume that X, Y have full column rank, and ∀ i 6= j, X-T i X-j = 0. If di ≥ min{d1, dy}, ∀i ≥ 1,
where di is the hidden width in the i-th layer, then every local minimum is a global minimum.
Note that under our assumption di ≥ min{d1, dy}, ∀i ≥ 1, the deep linear network we study has the
same solution as the shallow linear network when the first-layer weight fixed. Hence, the optimal
value for our objective function is equal to the optimal value of the shallow network problem.
5 Discussion
We have discussed the landscape of sparse linear networks with several arguments. On the positive
side, spurious local minimum does not exist when the objective applied with scalar target, or with
separated first layer and orthogonal training data. On the negative side, we have discovered the bad
local minimum when the previous conditions are violated in a general sparse two-layer linear net-
work, that is, one is generated from low rank constraint, another is produced from sub-sparse struc-
ture. Both the cases show that sparsity cuts out the decreasing path in the original fully-connected
network. Since dense linear networks possess benign landscape, we have concluded that sparsity or
network pruning destroys the favourable solutions. However, some heuristic algorithms combining
training and pruning still work well in practice, leading to mystery of modern network pruning meth-
ods and sparse network design. Other interesting questions for future research include understanding
the gap between global minimum and spurious local minimum, or showing a similar performance
of bad-min, particularly, combining with pruning algorithms.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. arXiv preprint arXiv:1810.02281, 2018.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, l989.
Pierre Baldi and Zhiqin Lu. Complex-valued autoencoders. Neural Networks, 33:136-147, 2012.
Alon Brutzkus and Amir Globerson. Globally optimal gradient descent for a convnet with gaussian
inputs. arXiv preprint arXiv:1702.07966, 2017.
Miguel A Carreira-Perpinan and Yerlan Idelbayev. “learning-compression” algorithms for neural net
pruning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 8532-8541, 2018.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Simon S Du and Wei Hu. Width provably matters in optimization for deep linear neural networks.
arXiv preprint arXiv:1901.08572, 2019.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. arXiv preprint arXiv:1811.03804, 2018.
Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychome-
trika, 1(3):211-218, 1936.
Armin Eftekhari. Training linear neural networks: Non-local convergence and complexity results.
arXiv preprint arXiv:2002.09852, 2020.
Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen. The difficulty of training sparse
neural networks. arXiv preprint arXiv:1906.10732, 2019.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574, 2019.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks
with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015a.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. In Advances in neural information processing systems, pp. 1135-1143,
2015b.
Kenji Kawaguchi. Deep learning without poor local minima. In Advances in neural information
processing systems, pp. 586-594, 2016.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr. Snip: Single-shot network pruning
based on connection sensitivity. arXiv preprint arXiv:1810.02340, 2018.
Dawei Li, Tian Ding, and Ruoyu Sun. On the benefit of width for neural networks: Disappearance
of bad basins. arXiv, pp. arXiv-1812, 2018.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. arXiv preprint arXiv:1810.05270, 2018.
9
Under review as a conference paper at ICLR 2021
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In
Advances in neural information processing Systems, pp. 3288-3298, 2017a.
Christos Louizos, Max Welling, and Diederik P Kingma. Learning sparse neural networks through
l_0 regularization. arXiv preprint arXiv:1712.01312, 2017b.
Haihao Lu and Kenji Kawaguchi. Depth creates no bad local minima. arXiv preprint
arXiv:1702.08580, 2017.
Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir. Proving the lottery ticket
hypothesis: Pruning is all you need. arXiv preprint arXiv:2002.00585, 2020.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018.
Leon Mirsky. Symmetric gauge functions and unitarily invariant norms. The quarterly journal of
mathematics, 11(1):50-59, 1960.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural
networks. arXiv preprint arXiv:1701.05369, 2017.
Itay Safran and Ohad Shamir. Spurious local minima are common in two-layer relu neural networks.
In International Conference on Machine Learning, pp. 4433-4441. PMLR, 2018.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Shai Shalev-Shwartz, Ohad Shamir, and Shaked Shammah. Weight sharing is crucial to succesful
optimization. arXiv preprint arXiv:1706.00687, 2017.
Ruoyu Sun, Dawei Li, Shiyu Liang, Tian Ding, and Rayadurgam Srikant. The global landscape of
neural networks: An overview. IEEE Signal Processing Magazine, 37(5):95-108, 2020.
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie. Small nonlinearities in activation functions create bad
local minima in neural networks. arXiv preprint arXiv:1802.03487, 2018.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for
model compression. arXiv preprint arXiv:1710.01878, 2017.
10
Under review as a conference paper at ICLR 2021
A Decreasing Path of Sparse Linear network with Sparse First
Layer
In addition, there still exists decreasing path to infinity:
min L(w1 , . . . , w8 ) :
wi
w1
Y - X w2
0
ww043!ww57
w1w5
Y - X w2w5 + w3w7
w4w7
ww682F
w1 w6
w2w6 + w3w8
w4w8
(9)
1
2
1
2
2
F
X=I3,Y =	01
oʌ
1 I, Choose (wι, W2,W3,W4, w5, w6, w7, W8)= (-√, -√k, 1,1, √, 0,1,1),
with k ∈ N+. then 2L(wι,..., W8)= (ɪ + 1)2 > 1 decreases when k increases. Since
minwi L(w1, . . . , w8) = 0, hence we get a decreasing path to infinity, but not a global minimum.
B Algorithm Analysis
We built algorithm guarantee in the following:
First, since each update step, the objective doesn’t increase, then the algorithm will converge.
Second, We verify that the convergent solution (wj, a；, wg, a2) satisfy zero gradient. Recall the
first-order condition:
—
∂L
∂w1
∂L
■ ——
∂ aι
∂L
∂w2
∂L
■ ——
∂ a2
Z1	- w1a1T	+ D(Z2	-	w2a2T)	a1 = Z1 + D(Z2	-	w2a2T)	a1 -	a1Ta1w1,
—
(Zι - WIaT + D(Z2 -w2aT))T wι = (Zι + D(Z2 - w2aT))T wι - WTw1a1,
—
Z2	-	w2a2T	+ DT (Z1	- w1a1T)	a2 =	Z2	+	DT (Z1	- w1a1T)	a2	-	a2Ta2w2,
—
(Z2 - w2aT + DT(Zι - WIaT))T w2 = (Z2 + DT(Zι - WIaT))T w2 - WTw2a2.
(10)
Notice that w1；, a1； is the best rank-1 approximation of Z1 + D(Z2 - w2；a2；T), and w2；, a2； are
the best rank-1 approximation of Z2 + DT (Z1 - w1；a1；T). Then we have already got a solution
(w1；, a；1, w2；, a2；) with zero gradient.
Third, we verify that the convergent solution is a local minimizer through the conditions we checked.
Set r1 = Z1 + D(Z2 - w2a2T), r2 = Z2 + DT (Z1 - w1a1T). Then
V2L(wι, aι, w2, a2)
/	aT a1Ip1
(-rι + 2wιaT)
aT a2DT
a1 w2T DT
-r1 + 2w1 a1T
w1Tw1Idy
DTw1 a2T
w1T Dw2Idy
a1Ta2D
a2 w1T D
a2T a2 Ip2
-r2 + 2w2a2T T
Set H； := V2L(wJ, a；, w2, a2). Observe that
(w；T,-a；T, 0t, 0t) H； = 0,
(0T, 0T, w2T, -a2T) H*= 0,
Dw2aT	\
WT Dw2Idy
-r2 + 2w2a2T
w2Tw2Idy
(11)
showing that H； has zero eigenvalue with at least two eigenvectors vι = (w；T, -a；T, OT, OT)T
and v2 = (Ot, Ot, w2T, -a；T)T.
Suppose the third smallest eigenvalue is λ3 ≥	> 0, then for any direction v with kvk2 = 1, we
have V = α1V1 + α2v2 + α3V3 with v3⊥v1, v3⊥v2, P3=1 α2 = 1, and W := w∕kw∣∣2.
11
Under review as a conference paper at ICLR 2021
If α3 = 0, then VHV = α23V3HV3 ≥ α3λ3 ≥ 02e ≥ 0. Otherwise, We set V = δ1V1 + δ2V2
with small δι, δ2 as perturbation, and the perturbed parameters are notated as W1, a 1, W2, a2 =
(1 - δι)wf, (1 - δ2)w2, (1 + δι)a^, (1 + δ2)a2, which yields
L(W1, a 1, W2, a2) = kX1 (Zι - (1 - δ2)wιaT) + X?依-(1 - δ2)w2aT) IlF
=kX1 (Z1 - WIaT) + X2 (Z2 - W2aT) IlF + kδ2W1aT + δ2W2aTIlF
+ 2δ2 tr[(WIaT)T (仃-WIaT)] + 2δ2 tr[(W2aT)T 值-W2aT)]
=∣∣X1 (Z1 - WIaT) + X2 (Z2 - W2aT) kF + kδ2W1aT + δ2W2aTIlF
= L(W1, a1, W2, a2) + Iδ2α21W1a1T + δ2α12W2a2T I2F ≥ L(W1, a1, W2, a2).
(12)
Third equality holds for the rank-1 approximation of the solution. Hence, the convergent solution is
a local minimizer.
Fourth, due to numerical error, we can not obtain exact convergent solution, but we are able
to obtain approximate solution (W1t , at1, W2t , at2) after t iterations with L (Wt1, at1, Wt2, at2) -
L (W12, a21, W22, a22) ≤ 2, and then use Weyl’s inequality (Safran & Shamir, 2018, Theorem 2),
∣λi(V2L(Wt, at1, w2, a2)) - λi(V2L(WR a↑, W2, a2))∣ < O(e),
where λi (H) is the i-th smallest eigenvalue of the real symmetric matrix H. Therefore, if the
approximate solution is approximate positive semi-definite with a large third smallest eigenvalue,
we conclude the convergent solution is a local minimizer.
C Useless Connections and Nodes in Sparse Network
In this section, we explain several kinds of unnecessary connections suffered from sparsity or net-
work pruning.
Input	Hidden	Hidden
layer	layer 1	layer 2
Output
layer
Figure 3: An example of sparse network with no bias. Lines are connections of original sparse
network, dotted lines are useless connections that can be removed, and solid lines are effective
connections.
1. Zero out-degree I: if a node have zero out-degree, such as h(12) 3 in Figure 3, we can eliminate
the input connections.
2. Zero out-degree II: if a node have zero out-degree when removed output connections in
latter layers, such as h(11) in Figure 3. Though it owes one output connection, the connected
node h(12) is zero out-degree, hence the connection can be removed, leading to zero out-
degree. We can eliminate the input connections of h(11) as well.
3. Zero in-degree I: if a node have zero in-degree, such as h(42) and h(41)in Figure 3, we can
eliminate the output connections, but notice that when the node has a bias term, then we
can not remove output connections since the bias constant will still propagate to subsequent
layers.
12
Under review as a conference paper at ICLR 2021
4. Zero in-degree II: if a node have zero in-degree when removed input connections in former
layers, such as h(32) in Figure 3. Though it owes one input connection, the connected node
h(41) is zero in-degree, hence the connection can be removed, leading to zero in-degree. We
can eliminate the output connections of h(32) as well.
D GENERAL d BLOCKS ALGORITHM
Algorithm 2 Sparse-d-Opt (X1, . . . , Xd, Z1, . . . , Zd): Obtain the solution of two-layer sparse lin-
ear neural network With d hidden neurons.____________________________________________________
1:	Input: Input matrix Xi,..., Xd. Target matrix Zι,..., Zd;
2:	Initialize wi , di , ai , i = 2, . . . , d;
3:	while not converge do
4:	for i = 1, . . . , d do
5:	wi, di, ai = SVD(Zi + Pj6=i XiT Xj (Zj - djwjajT));
6:	end for
7:	end while
8:	wi = diwi, i = 1, . . . , d.
9:	if λι(V2L),..., λd(V2L) ≈ 0, λd+ι(V2L) > 0 then
10:	Return the solution wi , ai , i = 1, . . . , d.
11:	else
12:	Try again from another initialization.
13:	end if
14:	Output: wi, ai, i = 1, . . . , d.
The analysis that the convergent solution is a local minimizer is similar to d = 2, so we are not
going to repeat the details. We list some examples searched for d > 2 below.
	0	0	0		0		
d = 3: Target: Z1 =	00	02 ,Z2	=22	, Z3 =		.	
		(			-2	-2		∖
			0.0	-0.088	-0.599	-0.234	
		2.0 0.0			0.429	-0.529	0.278 0.431
Training data: XTX	=	-0.088	2.0	-0.263	-0.0	0.558	0.193
		-0.599	-0.263 0.429	2.0 -0.0		-0.357	-0.244
		-0.234	-0.529	0.558	2.0 -0.357		-0.0
		0.78	0.43	0.93	-0.244	2.0 -0.0	.0
Local minimum found:
Table 2: Examples found by algorithm with spurious local minimum when dy = 3. All experiments
run 2000 iterations. λi := λi(V2L).
λd+i	λ1 , .	. . , λd	kVL∣∣2	Objective L
1.9 ∙ 10-1	0〜	10-14	< 10-i4	0.357481957
9.7 ∙ 10-2	0〜	10-14	< 10T	0.521705675
4.9 ∙ 10-2	0〜	10-14	< 10-i4	0.53973038厂
E	Missing Proofs for Section 2
E.1 Proof of Theorem 3
Proof: Suppose the j-th node has full out-degree and one in-degree, so that w-j ∈ R. We treat
objective with fixed other weights and only consider optimizing w-j , aj .
Ye - X-j wj ajT 2 , Ye := Y - X X-iw-iaiT.	(13)
F	i6=j
Wmiaj `(Wj，a ) = 2∣
13
Under review as a conference paper at ICLR 2021
Based on the proof of scalar case, a local minimizer (Wj, aj) of '(叼，％∙) must satisfy the condition
X-j (y - X-jwjajT) = 0, showing that '(wj, αj) = ɪ∣∣(I - Pχ-j) YIIF. Therefore, the
objective with remaining weights becomes:
2
…∙	1
min 一
w-i,ai,i=j 2
(In - PX_j) Y -	(In - PX-j) X-iW-iaT
i=j
(14)
We define ((In - Pχj XSj, (In - Pχj Y) as new training dataset which is the projection into
the orthogonal complement of X-j, and remove some elements in w- corresponding to the column
in X-j. Moreover, if X has full column rank, then projected data (In - Pχj XSj still has full
column rank. Hence, removing the above connections doesn,t affect the spurious local minima since
these connections preserve certain solution.	口
E.2 Proof of Theorem 4
Proof: The original loss function can be formulated as below,
2
2
2L(W,A)
YLEX-iW-iθ∏- £ X-j w-j αj1
i∈Rι
K-EX-iW-iai2- £ X-j w-j aj2
i∈R-2	j∈R
j∈R
+
F
F
Under similar analysis as scalar case,
Vi ∈ R1, X-i Y1 - X XTWTaiA- X X-w-a”	= 0.
∖	i∈Rι	j∈R	)
Vi ∈ R2, X-i I Y2 - X X-iw-iai2 - X X-jw-jaj2 j = 0.
∖	i∈R2	j∈R	/
Hence,
/ w-i1 ai1ι ∖
.
.
w-ij aij 1	= (X-i「∙ ∙
.
.
∖w-i∣R1∣ ai∖R1∖1/
,X-ij，…，X-i|R1| )	Y1 - £X-jw-jaj1	, ij ∈ R1.
j ∖ j∈R	)
/ w-i1 ai12	∖
w-ij aij2	= (X-i1, ∙∙∙,X-ij, ∙ ∙ ∙ , X-i∣R2∣)(匕-X X-j w-jaj2j , ij ∈ R2.
.
\w-i∣R2∣ ai∣R1∣ 1
Then the objective becomes:
(In- PX-TI)卜1 - X X-jw-jaj) ∣
+	(In- PX-T2 )(匕-X X-jw-jaj2 j ∣ ∣
We can see the objective is separated into two parts with shared sparse first-layer weights. Notice
that if i ∈ Tl, then Xi ∈ X%, hence (In - PX-Tl) Xi = 0. Therefore, we simplify the problem as
/	∖ ∣∣2
1
min Lf, A) = χ
f ,A	2
(In- PX-TI) 1 Y1 - EXi £ wij aj1 I ∣ ∣
∖ i∈T1 j:i/Sj	) ∣ ∣ 2
2	2 ∣ ∣ 2
(15)
1
+ 2
(In- PX-τ2 ) I Y2 - EXi E wijaj2 I ∣ ∣
∖	i∈T2	j"∈Sj	∣ ∣ ∣ 2
14
Under review as a conference paper at ICLR 2021
Use previous analysis again on T1 \ T2 in first output dimension and T2 \ T1 in second output
dimension since no overlap in parameters by the condition U(TI ∩ T2) ∩ U(Tl \ T2) = 0 and
U (T ∩ T2) ∩U (T2 \ T1) = 0. Therefore, we simplify the problem again as
2
min Lf,Ae) =1 (-
Wf,Ae	2
In-PIn-PX
XT1 \T2
(In-PX-Ti ) I Y1-	EXiE Wij aj1
∖	i∈T1 ∩T2	j：i/Sj
1
+2
In-PIn-PX
XT2\T1
(In-PX-T2) 1 Y2-	E	XiEwijaj2
∖	i∈T1 ∩T2	j：i/Sj
2
2
2
Using the fact that (In - PW1) (In - PW2) = In - P(W1,W2) if W1T W2 = 0. Hence the remaining
problem is same as
min L(Wf, Ae)
Wf,Ae
12
1 X
2乙
k=1
(In- PX-TinT2) I Yk - X Xi X Wijajk )1
∖	i∈Ti∩T2	jιi∈Sj	) l∣2
Therefore, the remaining network structure has dense second layer.
F Missing Proofs for Section 4
The objective of a deep linear network with squared loss is
W (ImuW (L) 2 kY-XW(I)…履 L)kF,	(16)
where the i-th layer weight matrix W (i) ∈ Rdi-i ×di, d0 = dx, dL = dy, Data matrix X ∈
Rn×dx , Y ∈ Rn×dy . We adopt Sj(i) = {k : Wk(ij) = 0} as pruned dimensions in j-th column of
W (i). Besides, Wj(,i-) Sas the remaining j -th column in i-th layer weight which leaves out pruned
dimension set S.	For simplification, we denote w-(i)j	=	w(i)	(i) ∈ R	di-i-|Sj	| ,	Wj(ik)	= Wj(ki),
and Wf(i) as the pruned weight matrix with several zero elements as before.
F.1 Proof of Theorem 6
Proof: Using induction. Base on Theorem 1, we have already proof two layer case. If the result holds
for (L - 1)-layer sparse linear network, we consider L layer case. We denote Xnew := XW (1) as
new training set, and ' := Y-Xf(I)… f (L). Then based on inductive assumption, 'tXnew = 0,
showing that
'tX-iw-i = 0, ∀1 ≤ i ≤ di.	(17)
Combined with first-order condition:
∂L
∂w-i)
-'τX-i(f (2)…f(L))i = 0.
If (f (2)…f(L))i = 0, then 'tX-i = 0, which satisfies the global minimizer condition. Other-
wise, any value of w-(1i) doesn’t change the loss since the forward path already contribute zero to the
final output. Hence, arbitrary choice of w(-1i) owes same objective value. Thus, from Eq. (17), we
still obtain 'tX— = 0. Thus any local minimum is a global minimum for the pruned sparse model.
15
Under review as a conference paper at ICLR 2021
F.2 Proof of Theorem 7
Proof: Since ∀ i 6= j, X-i, X-j share no same columns and XTX = Id, then ∀ i 6= j, X-TiX-j =
0. Besides, from our assumption ∩jm=ιSj = 0, then ∩jm=M = {1,...,d}, meaning that
(X-1, . . . , X-d) is X with different arrangement of columns. Hence
d
Y =PXY +(I-PX)Y = X(XTX)-1XTY +(I-PX)Y ,XX-iZi+(I-PX)Y, (18)
i=1
Set W(2) = [a1, . . . , ad1]T, then the objective becomes
d1
1 X kZi-w-iaTW⑶…W(L) kF.
i=1
We set Xe = Xwf(1)
same local minimizer condition for w-1.
(X-1w-1, . . . , X-d1 w-d1). Now we show the following problems have
2
(P)	L(f ⑴,w⑵,...,w(L)) = 1 ∣∣y-X (w⑵ …w(L))Il
F
1
(P1)	L (W,A) = 2
d1
Y - X X-iw-iaiT
i=1
2
(19)
F
If there is a local minimizer w—1, . . . , w—d1 6= 0, for problem (P), since di ≥ min{d1, dy}, ∀i ≥ 1
and X, Y have full column rank, then based on Theorem 2.3 in Lu & Kawaguchi (2017), a local
minimizer of L(Wf(1), W(2), . . . , W(L)) is obtained when
w ⑵ …w(L) = (X T X)T X T Y.
Notice that XTX = diag(wT-1w-1, . . . , w-Td w-d1). Then the objective is simplified as
2
2
2L(Wf(1)) = Y -Xe
XeTXe—1XeTY
(X-iw-i)(X-iw-i)TY
T
w-Tiw-i
d1
X
i=1
d1
X
i=1
For problem (P1),
(X-j w-j)T Y - P
objective as
X-i Zi -
(X-iw-i)(X-iw-i)TX-iZi
w-Tiw
(20)
X-iwT-iw-iZi
X—iZi 一	不
wT-iw-i
similarly, a local
id=1 1 X-iw-iaiT
0.
2L2(Wf) = Y -
d1
i=1
-i
2
minimizer
Then ajT
of
r Z÷77 4X .	...	.
L2 (w, A) is obtained when
IX-Tw-). Y, showing same loss
-j -j
d1
X
i=1
(X—iW-i)(X—i w—i)T Y
wTiw-i
(X—iw—i)(X—iw—i)TX—iZi
X—iZi 一	斤
wT—iw—i
2
= 2L(Wf(1)).
F
(21)
-—■
-—■

F
F
2
F
F
2
F
Finally, based on Theorem 2, every local minimum of (P1) is a global minimum. Hence every local
minimum of (P) is a global minimum.
If there exists i0, such that w-i0 = 0, we show that Zi0 = 0 below. Without loss of generality, we
assume i0 = 1, then the value of a1 does not affect the objective, we take a1 = 0 as well. In order
16
Under review as a conference paper at ICLR 2021
to show the result, we only perturb w-1, a1, W(3), . . . , W(L) into w-1 + ∆w, a1 + ∆a, W(3) +
∆3, . . . , W(L) + ∆L and analyze the difference of loss as ∆L. We set
L	LL
∆W , Y W(i) +∆i -YW(i), Wo, YW(i).	(22)
i=3	i=3	i=3
Then the perturbation leads to
2∆L(∆w, ∆a, ∆W) = kZ1 - ∆w∆aT (Wo + ∆W) k2F - kZ1k2F
+ X kZi - w-iaiT (Wo + ∆W) k2F - kZi - w-iaiTWok2F
i6=1
(23)
= -2tr[Z1T ∆w∆aT (Wo +∆W)] + k∆w∆aT (Wo + ∆W) k2F
-2Xtr[∆WTaiw- (Zi- w—aTWo)] + ∣∣W-iaT∆W∣∣F.
i6=1
Applying the first case to the remaining parameters excluding w-1 and a1 (If there are several w-i s
are zero, we can leave them all out), we have
aiTWo
(X-iW-i)TY _ (w-i)TZi
T	= T
w-Tiw-i	wT-iw-i
which agrees with
WTi (Zi- W-iaTWo) = 0,i = 1.
Hence the second term in the final row of Eq. (23) is zero. Besides, let us note the first-order term
of ∆w, showing that tr[Z1T ∆w∆aT (Wo + ∆W)] = 0. Otherwise, given w-1 = Θ(t-1), a-1 =
Θ(t-1), ∆W = Θ(t-3), ast → ∞, the sign in the final expansions of Eq. (23) depends on the fist
term that is indefinite.
Therefore, ∆a (Wo + ∆W)Z1T = 0, then (Wo + ∆W)Z1T = 0, leading to WoZ1T = 0 and
∆W Z1T = 0.
In view of expression ∆W, it holds that
d1
∆WZT = X (W ⑶…W(iT)∆iW(i+1)…W (L)) ZT +...
i=3
L-2
= Xft(∆3,...,∆L)Z1T,
t=1
(24)
where ft(∆3, . . . , ∆L) is the sum of the product in W(3), . . . , W(L), ∆3, . . . , ∆L that con-
tains exactly t different ∆i s. Then from small-order terms to high-order terms, we obtain
ft(∆3,..., Δl)Zi = 0. It follows from ∕l-2 = ∆3 ∙∙∙ Δl, d% ≥ min{dι, "l}, and the arbitrary
of ∆3 …Δl, We get Zi = 0. Finally, when Zi = 0, It is evident that wι = 0 already satisfies the
global minimizer condition since the objective is separated as Pd= 1 ∣∣Zi-w-iaTW⑶ … W(L) ∣F.
This completes the proof.
17