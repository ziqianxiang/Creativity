Under review as a conference paper at ICLR 2021
Revealing the Structure of Deep Neural Net-
works via Convex Duality
Anonymous authors
Paper under double-blind review
Ab stract
We study regularized deep neural networks (DNNs) and introduce a convex ana-
lytic framework to characterize the structure of the hidden layers. We show that a
set of optimal hidden layer weights for a norm regularized DNN training problem
can be explicitly found as the extreme points of a convex set. For the special case
of deep linear networks with K outputs, we prove that each optimal weight ma-
trix is rank-K and aligns with the previous layers via duality. More importantly,
we apply the same characterization to deep ReLU networks with whitened data
and prove the same weight alignment holds. As a corollary, we prove that norm
regularized deep ReLU networks yield spline interpolation for one-dimensional
datasets which was previously known only for two-layer networks. Furthermore,
we provide closed-form solutions for the optimal layer weights when data is rank-
one or whitened. We then verify our theory via numerical experiments.
1	Introduction
Deep neural networks (DNNs) have become extremely popular due to their success in machine
learning applications. Even though DNNs are highly over-parameterized and non-convex, simple
first-order algorithms, e.g., Stochastic Gradient Descent (SGD), can be used to successfully train
them. Moreover, recent work has shown that highly over-parameterized networks trained with SGD
obtain simple solutions that generalize well (Savarese et al., 2019; Parhi & Nowak, 2019; Ergen &
Pilanci, 2020a;b), where two-layer ReLU networks with the minimum Euclidean norm solution and
zero training error are proven to fit a linear spline model in 1D regression. Therefore, regularizing the
solution towards smaller norm weights might be the key to understand the generalization properties
of DNNs. However, analyzing DNNs is still theoretically elusive even in the absence of nonlinear
activations. Therefore, we study norm regularized DNNs and develop a framework based on convex
duality such that a set of optimal solutions to the training problem can be analytically characterized.
Deep linear networks have been the subject of extensive theoretical analysis due to their tractability.
A line of research (Saxe et al., 2013; Arora et al., 2018a; Laurent & Brecht, 2018; Du & Hu, 2019;
Shamir, 2018) focused on GD training dynamics, however, they lack the analysis of generalization
properties of deep networks. Another line of research (Gunasekar et al., 2017; Arora et al., 2019;
Bhojanapalli et al., 2016) studied the generalization properties via matrix factorization and showed
that linear networks trained with GD converge to minimum nuclear norm solutions. Later on, Arora
et al. (2018b); Du et al. (2018) showed that gradient flow enforces the layer weights to align. Ji &
Telgarsky (2019) further proved that each layer weight matrix is asymptotically rank-one. These
results provide insights to characterize the structure of the optimal layer weights, however, they re-
quire multiple strong assumptions, e.g., linearly separable training data and strictly decreasing loss
function, which makes the results impractical. Furthermore, Zhang et al. (2019) provided some char-
acterizations for nonstandard networks, which are valid for hinge loss and specific regularizations
where the data matrix is included. Unlike these studies, we introduce a complete characterization
for the regularized deep network training problem without requiring such assumptions.
Our contributions: 1) We introduce a convex analytic framework that characterizes a set of optimal
solutions to regularized training problems as the extreme points of a convex set, which is valid
for vector outputs and popular loss functions including squared, cross entropy and hinge loss1; 2)
For deep linear networks with K outputs, we prove that each optimal layer weight matrix aligns
1Extensions to other loss functions, e.g., cross entropy and hinge loss, are presented Appendix A.1
1
Under review as a conference paper at ICLR 2021
	Width (m)	Depth (L)	Vector outputs (K)
Savarese et al. (2019)	∞	2	X (K = 1)
Parhi & Nowak (2019)	∞	2	X (K = 1)
Ergen & Pilanci (2020a;b)	finite	2	X (K = 1)
Our work	finite	L ≥ 2	/ (K ≥ 1)一
Figure 1 & Table 1: One dimensional interpolation using L-layer ReLU networks with 20 neurons in each
hidden layer. As predicted by Corollary 4.2, the optimal solution is given by piecewise linear splines for any
L ≥ 2. Additionally, we provide a comparison with previous studies about this characterization.
with the previous layers and becomes rank-K via convex duality; 3) For deep ReLU networks, we
obtain the same weight alignment result for whitened or rank-one data matrices. As a corollary, we
achieve closed-form solutions for the optimal hidden layer weights when data is whitened or
rank-one (see Theorem 4.1 and 4.3). As another corollary, we prove that the optimal networks
are linear spline interpolators for one-dimensional, i.e., rank-one, data which generalizes the
two-layer results for one-dimensional data in Savarese et al. (2019); Parhi & Nowak (2019);
Ergen & Pilanci (2020a;b) to arbitrary depth. We note that the analysis of ReLU networks for
the one dimensional data considered in these works is non-trivial, which is a special case of our
rank-one/whitened data assumption.
Notation: We denote matrices/vectors as uppercase/lowercase bold letters. We use 0k (or 1k) and
Ik to denote a vector of zeros (or ones) and the identity matrix of size k, respectively. We denote
the set of integers from 1 to n as [n]. To denote Frobenius, operator, and nuclear norms, we use
k ∙ kF, k ∙ ∣∣2, and ∣∣ ∙ ∣*, respectively. Furthermore, σmax(∙) and σmin(∙) represent the maximum
and minimum singular values, respectively and B2 is defined as B2 := {u∈ Rd | ∣u∣2 ≤ 1}.
1.1	Overview of our results
We consider an L-layer network with layer weights Wl ∈ Rml-1 ×ml, ∀l ∈ [L], where m0 =
d and mL = 1, respectively. Then, given a data matrix X ∈ Rn×d, the output is fθ,L (X) =
AL-IWL, Al = g(Al-1 Wl) ∀l ∈ [L — 1], where A0 = X and g(∙) is the activation function.
Given a label vector y ∈ Rn , training problem can be formulated as follows
min L(fθ,L(X), y) + βR(θ),	(1)
{θl}lL=1
where L(∙,∙) is an arbitrary loss function, R(θ) is regularization for the layer weights, β > 0
is a regularization parameter, θl = {Wl , ml}, and θ := {θl}lL=1. In the paper, for the sake of
presentation simplicity, We illustrate the conventional training setup with squared loss and '2-norm
regularization, i.e., L(fθ,L (X), y) = ∣fθ,L (X) — y ∣22 and R(θ) = PlL=1 ∣Wl ∣2F. However, our
analysis is valid for arbitrary loss functions and different regularization terms as proven in Appendix.
Thus, we consider the following optimization problem
L
P * = min L(fθ,L (X), y) + β X IWl kF.	⑵
{θl }lL=1	l=1
Next, we show that the minimum `22 norm is equivalent to minimum `1 norm after a rescaling.
Lemma 1.1. The following problems are equivalent :
L	min L(fθ,L(X),y) + 2βkwLk1 + β(L — 2)t2
miLn L(fθ,L(X), y) + β	kWlk2F =	{θl}lL=1,t	,
{θl}l=1	l=1	s.t. wL-1,j ∈ B2, kWlkF ≤ t, ∀l ∈ [L — 2]
where wL-1,j denotes the jth column of WL-1.
Using Lemma 1.12, we first take the dual with respect to the output layer weights wL and then
change the order of min-max to achieve the following dual deep network training problem, which
provides a lower bound 3
P * ≥D* = min max min	—L* (λ) + β (L — 2)t2 s.t. ∣ AT-1λ∣∞ ≤ 2β .
t λ	wL-1,j ∈B2,∀j	-
kWlkF≤t, ∀l∈[L-2]
2The proof is presented in Appendix A.3.
3For the definitions and details see Appendix A.1.
2
Under review as a conference paper at ICLR 2021
To the best of our knowledge, the above dual deep network characterization is novel. Using this
result, we first characterize a set of weights that minimize the objective via the optimality conditions
and active constraints in the dual objective. We then prove the optimality of these weights by proving
strong duality, i.e., P * = D*, for deep networks. We then show that, for deep linear networks with
K outputs, optimal weight matrices are rank-K and align with the previous layers.
More importantly, the same analysis and conclusions also apply to deep ReLU networks with K
outputs when the input is whitened and/or rank-one. To the best of our knowledge, this is the first
work providing a complete characterization for deep ReLU networks via convex duality. Based on
this analysis, we even obtain closed-form solutions for the optimal layer weights. As a corollary, we
show that deep ReLU networks fit a linear spline interpolation when the input is a one-dimensional
dataset. We also provide an experiment in Figure 1 to verify this claim. We emphasize that this
result was previously known only for two-layer networks (Savarese et al., 2019; Parhi & Nowak,
2019; Ergen & Pilanci, 2020a;b) and here we extend it to arbitrary depth L (see Table 1 for details).
2 Warmup: Two-layer linear networks
To illustrate an application of the convex dual D*, we consider the simple case of two-layer lin-
ear networks with the output fθ,2(X) = XW1w2 and define the parameter space as θ ∈ Θ =
{(W1, w2, m) | W1 ∈ Rd×m, w2 ∈ Rm, m ∈ Z+}. Motivated by recent results (Neyshabur et al.,
2014; Chizat & Bach, 2018; Savarese et al., 2019; Parhi & Nowak, 2019; Ergen & Pilanci, 2020a;b),
we first focus on a minimum norm4 variant of equation 1 when L(fθ,L (X), y) = kfθ,L(X) - yk22
and then extend it to equation 1. The minimum norm primal training problem can be written as
mθ∈iΘn kW1k2F + kw2k22 s.t. fθ,2(X) =y.
Using Lemma A.15, we equivalently have
min kw2k1 s.t. fθ,2(X) = y, w1,j ∈ B2, ∀j,
θ∈Θ
which has the following dual form.
Theorem 2.1. The dual of the problem in equation 4 is given by
P* ≥ D* = max λTy s.t. max λT Xw1 ≤ 1 .
λ∈Rn	w1∈B2	1
(3)
(4)
(5)
For finite width networks, there exists a finite m such that strong duality holds, i.e., P* = D*, and an
optimal W1 for equation 4 satisfies k(XW1*)Tλ* k∞ = 1 , where λ* is the dual optimal parameter.
Using Theorem 2.1, we now characterize the optimal neurons as the extreme points ofa convex set.
Corollary 2.1. Theorem 2.1 implies that the optimal neurons are extreme points which solve the
following problem arg maXw1 ∈b2 ∣λ*T Xwi ].
Definition 1. We call the maximizers of the constraint in Corollary 2.1 extreme points.
From Theorem 2.1, we have the following dual problem
max λτy s.t. max ∣λτXwi | ≤ 1.	(6)
λ	w1 ∈B2
Let X = UxΣxVxT be the singular value decomposition (SVD) of X6. If we assume that there
exists w* such that Xw* = y due to Proposition 2.1, then equation 6 is equivalent to
maxλTΣχW* s.t. k∑Tλ∣∣2 ≤ 1,	(7)
λ
where λ = UTλ and W* = VTw*. Notice that in equation 7, We use an alternative formulation
for the constraint, i.e., ∣XTλ∣2 ≤ 1 instead of ∣λTXwi| ≤ 1, ∀wι ∈ B2 since the extreme point
is achieved when wi = XTλ∕∣XTλ∣2. Given rank(X) = r ≤ min{n, d}, We have
λTΣχw* = λTΣχ [ Ir	00r×d-r ] W* ≤ k∑Tλ∣2∣wr∣2 ≤ ∣wr∣2,	(8)
0d-r×r 0d-r×d-r
、-----------{----------}
Wr
4This corresponds to weak regularization, i.e., β → 0 in equation 1 (see e.g. Wei et al. (2018).).
5All the equivalence lemmas and proofs are presented in Appendix A.3.
6In this paper, we use full SVD unless otherwise stated.
3
Under review as a conference paper at ICLR 2021
which shows that the maximum objective value is achieved when ΣTλ = cιw；. Thus, We have
* = Vχ∑T λ = VxW r = PXT(W*)
W1 — kVχ∑Tλk2 — kwrk2 — kpχτ(w*)k2 ,
where PXT (∙) projects its input onto the range of XT. In the following results, We show that one
can consider a planted model without loss of generality and prove strong duality for equation 4.
Proposition 2.1. [Du & Hu (2019)] Given w* = arg minw kXw - yk2, we have
arg min kXW1w2 - Xw* k22 = arg min kXW1w2 - yk22 .
W1,w2	W1,w2
Theorem 2.2. Let {X, y} be feasible for equation 4, then strong duality holds for finite width
networks.
2.1 Regularized training problem
In this section, we define the regularized version of equation 4 as
min1 kfθ,2(X)- yk2+ βkw2 l∣1 s.t. w1,j ∈B2, ∀j	⑼
θ∈Θ 2
which has the following dual form
maχ- 1 kλ-yk2 + 1 l∣yk2 s∙t∙ max lλTXwi| ≤ β
λ 2	2	w1 ∈B2
Then, an optimal neuron needs to satisfy the condition
xτPχ,β(y)
W 1
1	kXT Pχ,β (y)k2
where Pχ,β(∙) projects its argument to {u ∈ Rn IkXTu∣∣2 ≤ β}. We now prove strong duality for
equation 9.
Theorem 2.3.	Strong duality holds for equation 9 with finite width networks.
2.2 Training problem with vector outputs
Here, the model is fθ,2(X) = XW1W2 to estimate Y ∈ Rn×K, which can be optimized as follows
minlW1l2F+lW2l2F s.t. fθ,2(X) =Y.	(10)
θ∈Θ
Using Lemma A.2, we reformulate equation 10 as
m
min X lw2,j l2 s.t. fθ,2 (X) = Y, w1,j ∈ B2, ∀j.	(11)
θ∈Θ j=1
which has the following dual with respect to W2
maχ trace(ΛT Y) s.t. lΛT Xw1 l2 ≤ 1, ∀w1 ∈ B2 .	(12)
Λ
Since we can assume Y = XW* due to Proposition 2.1, where W* ∈ Rd×K, we have
trace(ΛTY) = trace(ΛTXW*) = trace(ΛUxΣxW*) ≤ σmax(ΛTUxΣx) IW；( ≤ ∣W*∣*
*	(13)
where σmax(ΛTX) ≤ 1 due to equation 12 and W* = CIr	n0r×d-r	VTW*. Given the
r	0d-r×r 0d-r×d-r	x
SVD of W*, i.e., Uw ∑w Vw, choosing
ΛTUxΣx = Vw
Ir
rw
0K-rw ×rw
0rw ×d-rw
0K-rw ×d-rw
UTw
achieves the upper-bound above, where rw = rank(Wr*). Thus, optimal neurons are a subset of the
first rw right singular vectors of ΛX. Moreover, the next result shows that strong duality holds.
Theorem 2.4.	Let {X, Y} be feasible for equation 11, then strong duality holds for finite width
networks.
4
Under review as a conference paper at ICLR 2021
2.2.1 Regularized case
Here, we define the regularized version of equation 11 as follows
m
min 5kfθ,2(X)- YkF + β X kw2,j l∣2 s.t. w1,j ∈ B2, ∀j.
θ∈Θ 2
j=1
which has the following dual with respect to W2
max -- ∣∣λ - YkF + kk kYkF s.t. σmax(ATX) ≤ β
Λ2	2
Then, the optimal neurons are a subset of the maximal right singular vectors of PX,β (Y)TX, where
Pχ,β(∙) projects its input to the set {U ∈ Rn×K | σmaχ(UTX) ≤ β}.
Remark 2.1. Note that the optimal neurons are the right singular vectors of PX,β (Y)T X that
achieve the upper-bound of the set, i.e., ∣Pχ,β (Y)T Xwj k2 = β, where ||w； k2 = 1. This implies
that the optimal neurons satisfy k YTXWik 2 ≥ β. Therefore, the number of optimal neurons and
the rank of the optimal weight matrix, i.e., W1j, are determined by β.
Remark 2.2. There might exist optimal solutions other than the right singular vectors of
Pχ,β(Y)TX. As an example, consider u1 and u2 as the optimal right singular vectors. Then,
any u = α1u1 + α2u2 with α12 + α22 = 1 also achieves the upper-bound, therefore, optimal.
3 Deep linear networks7
We now consider an L-layer linear network with fθ,L(X) = XW1 . . . wL, and the training problem
L
Pi = min XkWlk2F s.t. fθ,L(X) = y.	(14)
{θl }lL=1 l=1
Proposition 3.1. First L - 2 hidden layer weight matrices in equation 14 have the same operator
and Frobenius norms, i.e., t1 = t2 = . . . = tL-2, where tl = kWl kF = kWlk2, ∀l ∈ [L - 2].
Theorem 3.1.	Optimal layer weights for equation 14 satisfy the following relation
{t* Vxw: CT if 1 — 1
t kwrk2ρ1 ifl = 1
tiρl-1ρlT if1 < l ≤ L-2	,
ρL-2 ifl =L- 1
where ∣Pι∣2 = 1, ∀l ∈ [L 一 2] and Wj follows the definition in equation 8.
This result clearly shows that the intra-layer weights need to satisfy an aligment condition. The next
theorem shows that strong duality holds in this case.
Theorem 3.2.	Let {X, y} be feasible for equation 14, then strong duality holds for finite width
networks.
Corollary 3.1. Theorem 3.1 implies that deep linear networks can obtain a scaled version of y
using only the first layer, i.e., XW1ρ1 = cy, where c > 0. Therefore, the remaining layers do not
contribute to the expressive power.
3.1 Training problem with vector outputs
Here, we consider vector output, i.e., mL = K, deep networks with the output fθ,L(X) =
XW1 . . . WL . In this case, we have the following training problem
L
min XkWlk2F s.t. fθ,L(X) =Y.	(15)
{θl}lL=1 l=1
With the same approach, the optimal layer weights for equation 15 can be characterized as follows.
7Since the derivations are similar, we present the details in Appendix A.4 and A.6.
5
Under review as a conference paper at ICLR 2021
Theorem 3.3.	Optimal layer weight for equation 15 can be formulated as follows
ft* Pj=I VwjPTj ifl = 1
Wl = ( t* Pj=I ρl-1,jPTj if1 < l ≤ L - 2
IPK=I PL-2,j ifL = L - 1
where Vwj is the jth maximal right singular vector of ΛtX and we may pick a Set of unit norm
vectors {Pl,j }lL=-12 such that PlT,j Pl,k = 0, ∀j 6= k.
The next theorem formally proves that strong duality holds for the primal problem in equation 15.
Theorem 3.4.	Let {X, y} be feasible for equation 15, then strong duality holds for finite width
networks.
4	Deep ReLU networks
Here, we consider an L-layer ReLU network with fθ,L (X) = AL-1wL, where Al =
(Al-1Wl)+, ∀l ∈ [L - 1], A0 = X, and (x)+ = max{0, x}. Below, we first state the train-
ing problem and then present our results
L
min X kWlk2F s.t. fθ,L(X) = y,	(16)
{θl }lL=1 l=1
Theorem 4.1. Let X be a rank-one data matrix such that X = ca0T, where c ∈ Rn+ and a0 ∈ Rd,
then strong duality holds and the optimal weights for each layer can be formulated as follows
Wi = 1τφ⅛ΦT, ∀l ∈ [L - 2], WL-I = τrφL-⅛,
kφl-1k2	kφL-2k2
where φ0 = a0 and {φl}lL=-12 is a set of vectors such that φl ∈ R+ml and kφl k2 = t*, ∀l ∈ [L - 2].
Our derivations can also be extended to cases with bias term. Below, we first examine a two-layer
ReLU network training problem with bias term and then extend this result to a multi-layer network.
Theorem 4.2. Let X be a data matrix such that X = ca0T, where c ∈ Rn and a0 ∈ Rd. Then, a set
of optimal solutions to equation 16 satisfies {(wi, bi)}m=ι, where Wi = Si1高％, b = -Sicika0k2
with si = ±1, ∀i ∈ [m].
Corollary 4.1. As a result of Theorem 4.2, when we have one dimensional data, i.e., x ∈ Rn, an
optimal solution to equation 16 can be formulated as {(wi, bi)}im=1, where wi = Si, bi = -Sixi
with Si = ±1, ∀i ∈ [m]. Therefore, the optimal network output has kinks only at the input data
points, i.e., the output function is in the following form: fθ,2(X) = Ei (X — Xi)+. Therefore, the
network output becomes linear spline interpolation for one dimensional datasets.
We now extend the results in Theorem 4.2 and Corollary 4.1 for multi-layer ReLU networks.
Proposition 4.1. Theorem 4.1 still holds when we add a bias term to the last hidden layer, i.e., the
output becomes AL-2WL-1 + 1nbT +wL = y, where Al = (Al-1 Wl)+, ∀l ∈ [L - 2].
Corollary 4.2. As a result of Theorem 4.2 and Proposition 4.1, when we have one dimensional
data, i.e., x ∈ Rn, the optimal network output has kinks only at the input data points, i.e., the output
function is in thefolloWingform: ∕θ,l(X) = Ei (X — Xi) +. Therefore, the network output becomes
linear spline interpolation for one dimensional datasets.
Remark 4.1. Note that in Corollary 4.1 and 4.2, we prove that the optimal output function for
multi-layer networks are linear spline interpolators for rank-one data, which generalizes the two-
layer results for one-dimensional data in Savarese et al. (2019); Parhi & Nowak (2019); Ergen &
Pilanci (2020a;b) to arbitrary depth. We also remark that the analysis of ReLU networks for the one
dimensional data considered in these works is non-trivial, which is a special case of our rank-one
data assumption.
6
Under review as a conference paper at ICLR 2021
The analysis in Theorem 4.1 also holds for vector output multi-layer ReLU networks as shown in
the next result.
Proposition 4.2. Strong duality also holds for deep ReLU networks with vector outputs and the
optimal layer weights can be formulated as in Theorem 4.1.
Now, we extend our characterization to arbitrary rank whitened data matrices and fully characterize
the optimal layer weights of a deep ReLU network with K outputs.
Theorem 4.3.	Let {X, Y} be a dataset such that XXT = In8 andY has orthogonal columns, then
the optimal weight matrices for each layer can be formulated as follows
1	2K	φ	1
Wl =	^=	X U φ l-1,r	ΦT,	∀l ∈ [L - 2],	Wl-1	= ^= ”然-2,1	... ”然-2,2K ,
l	√2K	W kΦl-1,r k2 l,r,	L j, LI √2K	[kφL-2,lk2	kφL-2,2κ k2J,
where (Φ0,2j-1, Φ0,2j) = (XT(yj) + , XT( — Nj)+)，∀j ∈ [K] and {Φl,r}L=ι2 is a set of vectors
such that Φl,r ∈ Rml, kΦl,r k2 = t*, and ΦTiΦl,j = 0, ∀i = j.
Remark 4.2. In one hot encoded labeling, which is the conventional labeling for classification
tasks, the label matrix Y ∈ Rn×K has nonoverlapping, therefore orthogonal, columns. Hence,
classification tasks with one hot encoded labels directly satisfy the assumption in Theorem 4.3.
Remark 4.3. We note that the whitening assumption XXT = In necessitates that n ≤ d, which
might appear to be restrictive. However, this case is common in few-shot classification problems
with limited labels (Chen et al., 2018). Moreover, it is challenging to obtain reliable labels in prob-
lems involving high dimensional data such as in medical imaging (Hyun et al., 2020) and genetics
(Singh & Yamada, 2020), where n ≤ d is typical. More importantly, SGD employed in deep learn-
ing frameworks, e.g., PyTorch and Tensorflow, operate in minibatches rather than the full dataset.
Therefore, even when n > d, each gradient descent update can only be evaluated on small batches,
where the batch size nb satisfies nb d. Hence, the n ≤ d case implicitly occur during the training
phase.
We note that these results also hold for regularized ReLU networks as in the previous sections and
we can obtain closed-form solutions for all the layers weights as proven in the next result.
Theorem 4.4.	Let {X, Y} be a dataset such that XTX = In and Y has orthogonal columns, then
a set of optimal layer weight matrices for the following regularized training problem
1	βL
minxkfθ,L(X)- YkF + β X kWlkF	(17)
θ∈Θ 2	2
l=1
can be formulated as follows
wl = (PrKI ⅛⅛ ΦTr,	if 1 ≤ l ≤L -1
[PrK1 (kΦ0,rk2 -β) Φl-1,r^T	if l = L	,
where ^2j-1 = ^2j = ej, ∀j ∈ [K], ej is the jth ordinary basis vector, and the other definitions
followsfrom Theorem 4.3 except t* = 1.
Remark 4.4. Theorem 4.4 proves that when the data matrix is whitened and the label matrix sat-
isfies certain conditions, all the layer weights can be obtained as closed-form analytical formulas.
We further note the conditions in this theorem are common in some generic regression/classification
frameworks. As an example, for image classification tasks, it has been shown that whitening sig-
nificantly improves the classification accuracy of the state-of-the-art architectures, e.g., ResNets, on
benchmark datasets such as CIFAR-100 and ImageNet (Huang et al., 2018). Furthermore, since the
label matrix is one hot encoded in image classification tasks, it directly satisfies the condition in the
theorem. Therefore, in such cases, there is no need to train a deep ReLU network in an end-to-end
manner. Instead one can directly use the closed-form formulas in Theorem 4.4.
8This can be achieved by applying batch whitening, which often improves accuracy (Huang et al., 2018).
7
Under review as a conference paper at ICLR 2021
(a)	(b)
Figure 2: Verification of Remark 2.1. (a) Rank of the hidden layer weight matrix as a function of β and (b)
rank of the hidden layer weights for different regularization parameters, i.e., β1 < β2 < β3 < β4.
Iteration
(a)
Figure 3: Verification of Proposition 3.1 and 4.1. (a) Evolution of the operator and Frobenius norms for the
layer weights of a linear network and (b) Rank of the layer weights of a ReLU network with K = 1.
5	Numerical experiments
Here, we present numerical results to verify our theoretical analysis. We first use synthetic datasets
generated from a random data matrix with zero mean and identity covariance and the corresponding
output vector is obtained via a randomly initialized teacher network9. We first consider a two-layer
linear network with W1 ∈ R20×50 and W2 ∈ R50×5. To prove our claim in Remark 2.1, we train
the network using GD with different β . In Figure 2a, we plot the rank of W1 as a function of β , as
well as the location of the singular values of W* Σx VT using vertical red lines. This shows that the
rank of the layer changes when β is equal to one of the singular values, which verifies Remark 2.1.
We also consider a four-layer linear network with W1 ∈ R5×50 , W2 ∈ R50×30 , W3 ∈ R30×40 ,
and W4 ∈ R40×5 . We then select different regularization parameters as β1 < β2 < β3 < β4 .
As illustrated in Figure 2b, β determines the rank of each weight matrix and the rank is same for
all the layers, which matches with our results. Moreover, to verify Proposition 3.1, we choose β
such that the weights are rank-two. In Figure 3a, we numerically show that all the hidden layer
weight matrices have the same operator and Frobenius norms. We also perform an experiment for a
five-layer ReLU network with W1 ∈ R10×50, W2 ∈ R50×40, W3 ∈ R40×30, W4 ∈ R30×20, and
w5 ∈ R20×1. Here, we use data such that X = ca0T, where c ∈ Rn+ and a0 ∈ Rd. In Figure 3b, we
plot the rank of each weight matrix, which converges to one as claimed Proposition 4.1.
We also verify our theory on two real benchmark datasets, i.e., MNIST (LeCun) and CIFAR10
(Krizhevsky et al., 2014). We first randomly undersample and whitened these datasets. Further-
more, we convert the labels into one hot encoded form. Then, we consider ten class classifica-
tion/regression task using three multi-layer ReLU network architecture with L = 3, 4, 5. For each
architecture, we use SGD with momentum for training and compare the training/test performance
with the corresponding network constructed via the closed-form solutions (without any sort of train-
ing) in Theorem 4.3, i.e., denoted as “Theory”. In Figure 4, we observe that Theory achieves the
optimal training objective, which also yields smaller error and higher accuracy in the test phase.
Hence, these experiments numerically verify our claims in Theorem 4.3.
6	Concluding remarks
We studied regularized DNN training problems and developed an analytic framework to characterize
a set of optimal solutions. We showed that optimal layer weights can be explicitly formulated
as the extreme points of a convex set via the dual problem. We then proved that strong duality
9Additional numerical results can be found in Appendix A.2.
8
Under review as a conference paper at ICLR 2021
(a) MNIST-Training objective
(b) MNIST-Test error
Epoch
(d) CIFAR10-Training objective
(e) CIFAR10-Test error
(c) MNIST-Test accuracy
Epoch
(f) CIFAR10-Test accuracy
Figure 4: Training and test performance on whitened and sampled datasets, where (n, d) = (60, 90), K = 10,
L = 3, 4, 5 with 50 neurons per layer and we use squared loss with one hot encoding. For Theory, we use the
layer weights in Theorem 4.3, which achieves the optimal performance as guaranteed by Theorem 4.3.
holds for both deep linear and ReLU networks and provided a set of optimal solutions. We also
extended our derivations to the vector outputs and many other loss functions. More importantly, our
analysis shows that when the input data is whitened or rank-one, instead of training an L-layer deep
ReLU network in an end-to-end manner, one can directly use the closed-form solutions provided in
Theorem 4.1, 4.3, and 4.4. As another corollary, we proved that the kinks of ReLU activations occur
exactly at the input data points so that the optimized network outputs linear spline interpolations
for one-dimensional datasets, which was previously known only for two-layer networks (Savarese
et al., 2019; Parhi & Nowak, 2019; Ergen & Pilanci, 2020a;b). We conjecture that our extreme
points characterization can also be extended to reveal the structure behind cases with arbitrary data.
Therefore, one can explain the extraordinary generalization properties of DNNs.
References
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A convergence analysis of gradient
descent for deep linear neural networks. CoRR, abs/1810.02281, 2018a. URL http://arxiv.
org/abs/1810.02281.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. arXiv preprint arXiv:1802.06509, 2018b.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. arXiv preprint arXiv:1905.13655, 2019.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research,18(1):629-681, 2017.
Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Global optimality of local search for
low rank matrix recovery. In Advances in Neural Information Processing Systems, pp. 3873-3881,
2016.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer
look at few-shot classification. In International Conference on Learning Representations, 2018.
Lenaic Chizat and Francis Bach. A note on lazy training in supervised differentiable programming.
arXiv preprint arXiv:1812.07956, 2018.
9
Under review as a conference paper at ICLR 2021
Simon Du and Wei Hu. Width provably matters in optimization for deep linear neural networks. In
International Conference on Machine Learning, pp. 1655-1664, 20l9.
Simon S Du, Wei Hu, and Jason D Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Sys-
tems, pp. 384-395, 2018.
Tolga Ergen and Mert Pilanci. Convex geometry of two-layer relu networks: Implicit autoencoding
and interpretable models. In Silvia Chiappa and Roberto Calandra (eds.), Proceedings of the
Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of
Proceedings of Machine Learning Research, pp. 4024-4033, Online, 26-28 Aug 2020a. PMLR.
URL http://proceedings.mlr.press/v108/ergen20a.html.
Tolga Ergen and Mert Pilanci. Convex geometry and duality of over-parameterized neural networks.
arXiv preprint arXiv:2002.11219, 2020b.
Miguel Angel Goberna and Marco Lopez-Cerda. Linearsemi-infinite optimization. 01 1998. doi:
10.1007/978-1-4899-8044-L3.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems, pp. 6151-6159, 2017.
Lei Huang, Dawei Yang, Bo Lang, and Jia Deng. Decorrelated batch normalization. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 791-800, 2018.
Chang Min Hyun, Kang Cheol Kim, Hyun Cheol Cho, Jae Kyu Choi, and Jin Keun Seo. Framelet
pooling aided deep learning network: the method to process high dimensional medical data. Ma-
chine Learning: Science and Technology, 1(1):015009, 2020.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=HJflg30qKX.
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. The CIFAR-10 dataset. http://www.cs.
toronto.edu/kriz/cifar.html, 2014.
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In International Conference on Machine Learning, pp. 2902-2907, 2018.
Yann LeCun. The MNIST database of handwritten digits. http://yann.lecun.com/exdb/
mnist/.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning. arXiv preprint arXiv:1412.6614, 2014.
Rahul Parhi and Robert D Nowak. Minimum “norm” neural networks are splines. arXiv preprint
arXiv:1910.02333, 2019.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. L1 regularization in infinite di-
mensional feature spaces. In International Conference on Computational Learning Theory, pp.
544-558. Springer, 2007.
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded
norm networks look in function space? CoRR, abs/1902.05040, 2019. URL http://arxiv.
org/abs/1902.05040.
Andrew M Saxe, James L McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynam-
ics of learning in deep linear neural networks. arXiv preprint arXiv:1312.6120, 2013.
Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear
neural networks. arXiv preprint arXiv:1809.08587, 2018.
Dinesh Singh and Makoto Yamada. Fsnet: Feature selection network on high-dimensional biological
data. arXiv preprint arXiv:2001.08322, 2020.
10
Under review as a conference paper at ICLR 2021
Colin Wei, Jason D Lee, Qiang Liu, and Tengyu Ma. On the margin theory of feedforward neural
networks. arXiv preprint arXiv:1810.05369, 2018.
Hongyang Zhang, Junru Shao, and Ruslan Salakhutdinov. Deep neural networks with multi-branch
architectures are intrinsically less non-convex. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp.1099-1109, 2019.
11
Under review as a conference paper at ICLR 2021
Appendix
Table of Contents
A Appendix	12
A.1	General loss functions ............................................... 12
A.2	Additional numerical results ......................................... 12
A.3	Equivalence (Rescaling) lemmas for the non-convex objectives ......... 13
A.4	Regularized extensions ............................................... 14
A.5	Proofs for the two-layer networks .................................... 15
A.6	Proofs for the deep linear networks .................................. 18
A.7	Proofs for the deep ReLU networks .................................... 23
A	Appendix
Here, we present additional materials and proofs of the main results that are not included in the
main paper due to the page limit. We also restate each result before the corresponding proof for the
convenience of the reader.
A.1 General loss functions
In this section, we show that our extreme point characterization holds for arbitrary convex loss
functions including cross entropy and hinge loss.
min L(fθ,2 (X), y) + βkw2k1 s.t. w1,j ∈ B2,∀j,
θ∈Θ
(18)
where L(∙, y) is a convex loss function.
Theorem A.1. The dual of equation 18 is given by
max-L*(λ) s.t. kXTλ∣∣2 ≤ β,
where L is the Fenchel conjugate function defined as
L (λ) = max ZTλ — L(z, y).
z
Theorem A.1 proves that our extreme point characterization in Corollary 2.1 applies to arbitrary
loss function. Therefore, optimal parameters for equation 3 and equation 9 are a subset of the same
extreme point set, i.e., determined by the input data matrix X, independent of loss function.
Remark A.1. Since our characterization is generic in the sense that it holds for vector output, deep
linear and deep ReLU networks (see the main paper for details), Theorem A.1 is valid for all of our
derivations.
A.2 Additional numerical results
Here, we present numerical results that are not included in the main paper due to the page limit.
In Figure 5a, we perform an experiment to check whether the hidden neurons of a two-layer linear
network align with the proposed right singular vectors. For this experiment, we select a certain β
such that W1 becomes rank-two. After training, we first normalize each neuron to have unit norm,
i.e., kw1,j k2 = 1, ∀j, and then compute the sum of the projections of each neuron onto each right
singular vector, i.e., denoted as vi. Since we choose β such that W1 is a rank-two matrix, most of
the neurons align with the first two right singular vectors as expected. Therefore, this experiment
verifies our analysis and claims in Remark 2.1. Furthermore, as an alternative to Figure 2a, we plot
the singular values of W1 with respect to the regularization parameter β in Figure 5b.
12
Under review as a conference paper at ICLR 2021

0
0
4
3
2
1
6
x104
2	4
Iteration
(a)	(b)
Figure 5: (a) Projection of the hidden neurons to the right singular vectors claimed in Remark 2.1 and (b)
singular values of W1 with respect to β .
A.3 Equivalence (Rescaling) lemmas for the non-convex objectives
In this section, we present all the equivalence (scaling transformation) lemmas we used in the main
paper and the the proofs are presented in Appendix A.5, A.6, and A.7, two-layer, deep linear, and
deep ReLU networks, respectively.
Lemma 1.1. The following problems are equivalent :
L	min L(fθ,L(X), y) + 2βkwLk1 +β(L - 2)t2
miLn L(fθ,L(X), y) + β	kWlk2F =	{θl}lL=1,t	,
{θl}l=1	l=1	s.t. wL-1,j ∈ B2, kWlkF ≤ t, ∀l ∈ [L - 2]
where wL-1,j denotes the jth column of WL-1.
ProofofLemma 1.1. For any θ ∈ Θ, we can rescale the parameters as WL-ι,j = αWL-ι,j and
WLj = WLj/αj, for any aj > 0. Then, the network output becomes
fθ,L(X) = ((XW1)+ ... W L-l) + W L = ((XW1)+ ... Wl-i) + Wl,
which proves ∕θ,l(X) = f8L(X). In addition to this, we have the following basic inequality
L	L-2	m
XkWlk2F ≥ XkWlk2F+2X|WL,j| kWL-1,jk2,
l=1	l=1	j =1
where the equality is achieved with the scaling choice aj = (口]；：：[2)2 is used. Since the scaling
operation does not change the right-hand side of the inequality, we can set kWL-1,j k2 = 1, ∀j.
Therefore, the right-hand side becomes kWL k1.
Now, let us consider a modified version of the problem, where the unit norm equality constraint is
relaxed as kWL-1,j k2 ≤ 1. Let us also assume that for a certain index j, we obtain kWL-1,j k2 <
1 with WL,j 6= 0 as an optimal solution. This shows that the unit norm inequality constraint is
not active for WL-1,j, and hence removing the constraint for WL-1,j will not change the optimal
solution. However, when we remove the constraint, kWL-1,j k2 → ∞ reduces the objective value
since it yields WL,j = 0. Therefore, we have a contradiction, which proves that all the constraints
that correspond to a nonzero WL,j must be active for an optimal solution. This also shows that
replacing kWL-1,j k2 = 1 with kWL-1,j k2 ≤ 1 does not change the solution to the problem.
Then, we use the epigraph form for the norm of the first L - 2 to achieve the equivalence. □
Lemma A.1. [Neyshabur et al. (2014); Savarese et al. (2019); Ergen & Pilanci (2020a;b)] The
following two problems are equivalent:
minkW1k2F + kW2k22
θ∈Θ
s.t. fθ,2(X) = y
min kW2k1
θ∈Θ
s.t. fθ,2 (X) = y, W1,j ∈ B2
13
Under review as a conference paper at ICLR 2021
Lemma A.2. The following problems are equivalent:
minkW1k2F + kW2k2F
θ∈Θ
s.t. fθ,2(X) = Y
m
mθ∈iΘn X kw2,jk2
j=1
s.t. fθ,2(X) = Y, w1,j ∈B2,∀j
Lemma A.3. The following problems are equivalent:
L
θmiLn X kWlk2F
{θl}l=1 l=1
s.t. fθ,L (X) = y
L-2
min kwLk1 + Xtl2
{θl }lL=1 ,{tl }lL=-12	l=1
s.t. fθ,L (X) = y, wL-1,j ∈ B2, kWlkF ≤ tl, ∀l ∈ [L - 2]
Lemma A.4. The following problems are equivalent:
L
min X kW
{θl }lL=1 l=1
s.t. fθ,L (X) =
		mL-1	L-2
l I2F	min =	{θl}lL=1,{tl}lL=-12	X IWL,j I j=i	2+Xtl2 l=i
Y	s.t. fθ,L(X) =	Y, WL-i,j	∈ B2,IWlIF ≤tl, ∀l ∈ [L-2]
A.4 Regularized extensions
In this section, we present the regularized versions of the training problems presented in the main
paper and the proofs are presented in Appendix A.5, A.6, and A.7, two-layer, deep linear, and deep
ReLU networks, respectively.
A.4. 1 Regularized training problem for deep linear networks with scalar
OUTPUTS
Using Lemma A.3 and Proposition 3.1, we have the following dual for the regularized version of
equation 14
max - 1 kλ - y∣∣2 s.t. k(XWι... WL-Y λ∣∣2 ≤ β, ∀θl ∈ Θl-i, ∀l.
λ2
Then, the weight matrices that maximize the value of the constraint can be described as
f t* IlXWXel(y) PT if l = 1
kXT PX,β (y)k2 ρ1
Wi = t ∙t*ρι-ιp if 1 <l≤ L - 2	.
iPL-2 if l = L - 1
where Pχ,β(∙) projects its input to {u ∈ Rn | ∣∣XTu∣∣2 ≤ βγ-1}.
Corollary A.1. The analysis above and Theorem 3.2 also show that strong duality holds for the
regularized deep linear network training problem.
A.4.2 Regularized training problem for deep linear networks with vector
OUTPUT
Using Lemma A.4 and Proposition 3.1, we have the following dual for the regularized version of
equation 15
max - 1 kΛ - YkF s.t. σmaχ(ΛT XWi... Wl-2)≤ β, ∀θl ∈ Θl-i,
Λ2
where we define Θl-i = {θι,...,θL-ι∣∣∣WL-ι,j∣∣2 ≤ 1,∀j ∈ [mL-i], IIWIIIF ≤ t*, ∀l ∈
[L - 2]}. Then, as in equation 32, a set of optimal layer weights is
ft* Pj=ι VxjPTj if l = 1
w；= <t* pj=i PI-IjPT if 1 <l ≤ L - 2	(19)
IPK=I PL-2,j if l = L - 1
where Vxj is a maximal right singular vector of Pχ,β(Y)TX and Pχ,β(∙) projects its input to the
set {U ∈ Rn×k | σmax(UTX) ≤ βγ-i}. Additionally, Pl,j’s is an orthonormal set. Therefore, the
rank of each hidden layer is determined by β as in Remark 2.1.
14
Under review as a conference paper at ICLR 2021
A.5 Proofs for the two-layer networks
Lemma A.1. [Neyshabur et al. (2014); Savarese et al. (2019); Ergen & Pilanci (2020a;b)] The
following two problems are equivalent:
minkW1k2F + kw2k22
θ∈Θ
s.t. fθ,2 (X) = y
min kw2k1
θ∈Θ
s.t. fθ,2 (X) = y, w1,j ∈ B2
ProofofLemma A.1. For any θ ∈ Θ, we can rescale the parameters as Wι,j = αjwι,j and W2,j
w2,j /αj, for any αj > 0. Then, the network output becomes
mm	m
fθ,2(X) = X W2,jXW 1,j = X -2jαjxw1,j = X w2,jXw1,j,
α
j=1	j=1	j=1
which proves fθ,2(X) = f仇2(X). In addition to this, we have the following basic inequality
mm
2 X(W2,j + kw1,j k2) ≥ X(Iw2,j | kw1,j l∣2),
j=1	j=1
where the equality is achieved with the scaling choice aj = (kwwj )1 is used. Since the scal-
ing operation does not change the right-hand side of the inequality, we can set kw1,j k2 = 1, ∀j.
Therefore, the right-hand side becomes kw2 k1.
Now, let us consider a modified version of the problem, where the unit norm equality constraint is
relaxed as kw1,j k2 ≤ 1. Let us also assume that for a certain index j, we obtain kw1,j k2 < 1
with w2,j 6= 0 as an optimal solution. This shows that the unit norm inequality constraint is not
active for w1,j, and hence removing the constraint for w1,j will not change the optimal solution.
However, when we remove the constraint, kw1,j k2 → ∞ reduces the objective value since it yields
w2,j = 0. Therefore, we have a contradiction, which proves that all the constraints that correspond
to a nonzero w2,j must be active for an optimal solution. This also shows that replacing kw1,j k2 = 1
with kw1,j k2 ≤ 1 does not change the solution to the problem.	□
Theorem 2.1.	The dual of the problem in equation 4 is given by
P * ≥ D* = max λτ y s.t.
λ∈Rn
max
w1 ∈B2
λT Xw1
≤1.
(5)
For finite width networks, there exists a finite m such that strong duality holds, i.e., P* = D*, and an
optimal W1 for equation 4 satisfies k(XW1*)Tλ* k∞ = 1 , where λ* is the dual optimal parameter.
Corollary 2.1. Theorem 2.1 implies that the optimal neurons are extreme points which solve the
following problem argmaXw1∈B2 ∣λ*T Xw 11.
Proof of Theorem 2.1 and Corollary 2.1. We first note that the dual of equation 4 with respect to
w2 is
min maxλTy s.t. k(XW1)T+λk∞ ≤ 1, kw1,j k2 ≤ 1, ∀j.
Θ∈Θ∖{W2} λ
Then, we can reformulate the problem as follows
P* =	min	maxλTy +I(k(XW1)T+λk∞ ≤ 1), s.t. kw1,j k2 ≤ 1, ∀j.
Θ∈Θ∖{W2} λ
where I(k(XW1)Tλk∞ ≤ 1) is the characteristic function of the set k(XW1)Tλk∞ ≤ 1, which
is defined as
I(k(XW1)Tλk∞ ≤1)=0	ifk(XW1)Tλk∞ ≤1 .
∞	-∞ otherwise
Since the set k(XW1)Tλk∞ ≤ 1 is closed, the function Φ(λ, W1) = λTy +I(k(XW1)Tλk∞ ≤
1) is the sum of a linear function and an upper-semicontinuous indicator function and therefore
upper-semicontinuous. The constraint on W1 is convex and compact. We use P* to denote the
15
Under review as a conference paper at ICLR 2021
value of the above min-max program. Exchanging the order of min-max we obtain the dual problem
given in equation 5, which establishes a lower bound D* for the above problem:
P* ≥ D* =max	min	λτy +1(k(XWι)Tλ∣∣∞ ≤ 1), s.t. kw1j∣2 ≤ 1,∀j,
λ θ∈θ∖{w2}
= max λT y, s.t. k(XW1)Tλk∞ ≤ 1 ∀w1,j : kw1,j k2 ≤ 1, ∀j,
= max λT y, s.t. k(Xw1)Tλk∞ ≤ 1 ∀w1 : kw1k2 ≤ 1,
We now show that strong duality holds for infinite size NNs. The dual of the semi-infinite program
in equation 5 is given by (see Section 2.2 of Goberna & LoPez-Cerda (1998) and also Bach (2017))
min ∣∣μkτv
s.t.
/
w1 ∈B2
Xwιdμ(wι)
y,
where TV is the total variation norm of the Radon measure μ. This expression coincides with the
infinite-size NN as given in Bach (2017), and therefore strong duality holds. We also note that
although the above formulation involves an infinite dimensional integral form, by Caratheodory’s
theorem, the integral can be represented as a finite summation ofat most n + 1 Dirac delta functions
(Rosset et al., 2007). Next we invoke the semi-infinite optimality conditions for the dual problem in
equation 5, in particular We apply Theorem 7.2 of Goberna & LOPez-Cerda (1998). We first define
the set
sXw1
1
, w1 ∈ B2, s ∈ {-1, +1};	-n1
Note that K is the union of finitely many convex closed sets, since the function Xw1 can be ex-
pressed as the union of finitely many convex closed sets. Therefore the set K is closed. By Theorem
5.3 Goberna & LOPez-Cerda (1998), this implies that the set of constraints in equation 5 forms a
Farkas-Minkowski system. By Theorem 8.4 of Goberna & LOPez-Cerda (1998), primal and dual
values are equal, given that the system is consistent. Moreover, the system is discretizable, i.e., there
exists a sequence of problems with finitely many constraints whose optimal values approach to the
optimal value of equation 5. The optimality conditions in Theorem 7.2 Goberna & LOPez-Cerda
(1998) implies that y = XW1*w2* for some vector w2*. Since the primal and dual values are equal,
We have λ*Ty = λ*TXW*w2 = ∣∣w2∣∣ι, which shows that the primal-dual pair ({w2, W*}, λ*)
is optimal. Thus, the optimal neuron weights W* satisfy ∣∣(XW*)tλ*∣∞ = 1.	□
Proposition 2.1. [Du & Hu (2019)] Given w* = arg minw ∣Xw - y∣2, we have
arg min ∣XW1w2 - Xw* ∣22 = arg min ∣XW1w2 - y∣22 .
W1,w2	W1,w2
Proof of Proposition 2.1. Let us first define a variable w* that minimizes the following problem
w* = min ∣Xw - y∣22 .
w
Thus, the following relation holds
XT (Xw* - y) = 0d.
Then, for any w ∈ Rd, we have
f(w) = ∣Xw - Xw* + Xw* - y∣22
= ∣Xw - Xw* ∣22 + 2(w - w*)T XT (Xw* - y) +∣Xw* - y∣22
X------------------------------------V-----}
=0d
= ∣Xw - Xw* ∣22 + ∣Xw* - y∣22.
Notice that ∣Xw* -y∣22 does not depend on w, thus, the relation above proves that minimizing f (w)
is equivalent to minimizing ∣Xw - Xw* ∣22, where w* is the planted model parameter. Therefore,
the planted model assumption does not change solution to the linear network training problem in
equation 4.	□
16
Under review as a conference paper at ICLR 2021
Theorem 2.2.	Let {X, y} be feasible for equation 4, then strong duality holds for finite width
networks.
Proof of Theorem 2.2. Since there exists a single extreme point, we can construct a weight vector
we ∈ Rd that is the extreme point. Then, the dual of equation 4 with W1 = we is
D* = max λτy s.t. k(Xwe)Tλ∣∣∞ ≤ 1.	(20)
Then, we have
P * =	min	max λτ y	≥ max min	λτ y
θ∈θ∖{w2} λ	λ θ∈θ∖{w2}
s.t k(XW1)T λk∞ ≤ 1, kw1,jk2 ≤ 1,∀j	s.t. k(XW1)Tλk∞ ≤ 1, kw1,jk2 ≤ 1,∀j
= max λTy
s.t. k(Xwe)Tλk∞ ≤ 1
= De* = D*	(21)
where the first inequality follows from changing order of min-max to obtain a lower bound and the
equality in the second line follows from Corollary 2.1.
From the fact that an infinite width NN can always find a solution with the objective value lower
than or equal to the objective value of a finite width NN, we have
P* = min	|w2|	≥ P* =min kw2k1	(22)
θ∈Θ∖{Wι,m}	一	θ∈Θ
s.t. Xwew2 = y	s.t. XW1w2 = y, kw1,j k2 ≤ 1, ∀j,
where P * is the optimal value of the original problem with infinitely many neurons. Now, notice
that the optimization problem on the left hand side of equation 22 is convex since it is an '1 -norm
minimization problem with linear equality constraints. Therefore, strong duality holds for this prob-
lem, i.e., Pe* = De*. Using this result along with equation 21, we prove that strong duality holds for
a finite width NN, i.e., Pe* = P* = D* = De*.
□
Theorem 2.3. Strong duality holds for equation 9 with finite width networks.
Proof of Theorem 2.3. Since there exists a single extreme point, we can construct a weight vector
we ∈ Rd that is the extreme point. Then, the dual of equation 9 with W1 = we
De =max- 1kλ - yk2 + Ikyk2 s.t. lλTXWeI ≤ β
λ2	2
Then the rest of the proof directly follows Proof of Theorem 2.2.	□
Theorem A.1. The dual of equation 18 is given by
max-L*(λ) s.t. kXT λk2 ≤ β,
where L* is the Fenchel conjugate function defined as
L*(λ) = max zTλ - L(z, y) .
Proof of Theorem A.1. The proof follows from classical Fenchel duality (Boyd & Vandenberghe,
2004). We first describe equation 18 in an equivalent form as follows
min L(z, y) +βkw2k1 s.t. z = XW1w2, kw1,j k2 ≤ 1, ∀j.
z,θ∈Θ
Then the dual function is
g(λ) = min L(z, y) - λTz + λTXW1w2 +βkw2k1 s.t. kw1,j k2 ≤ 1, ∀j.
z,θ∈Θ
Therefore, using the classical Fenchel duality (Boyd & Vandenberghe, 2004) yields the claimed dual
form.	□
17
Under review as a conference paper at ICLR 2021
Lemma A.2. The following problems are equivalent:
minkW1k2F + kW2k2F
θ∈Θ
s.t. fθ,2(X) = Y
m
mθ∈iΘn X kw2,jk2
j=1
s.t. fθ,2(X) = Y, w1,j ∈B2,∀j
Proof of Lemma A.2. The proof directly follows from Proof of Lemma A.1.
□
Theorem 2.4. Let {X, Y} be feasible for equation 11, then strong duality holds for finite width
networks.
Proof of Theorem 2.4. Since there exist rw possible extreme points, we can construct a weight
matrix We ∈ Rd×rw that consists of all the possible extreme points. Then, the dual of equation 11
with W1 = We
D* =maxtrace(ΛTY) s.t. ∣∣ΛTXWej∣∣2 ≤ 1,∀j ∈ 卜W].
eΛ
Then the rest of the proof directly follows Proof of Theorem 2.2.
□
A.6 Proofs for the deep linear networks
Lemma A.3. The following problems are equivalent:
L
{θm}iLn X∣Wl∣2F
{θl}l=1 l=1
s.t. fθ,L (X) = y
L-2
min ∣WL ∣1 + X tl2
{θl }lL=1 ,{tl }lL=-12	l=1
s.t. fθ,L(X) =y, WL-1,j ∈B2,∣Wl∣F ≤tl, ∀l ∈ [L-2]
Proof of Lemma A.3. Applying the scaling trick in Lemma A.1 to the last two layers of the L-layer
network in equation 14 gives
L-2
min	∣WL ∣1 + X ∣Wl ∣2F
{θl}lL=1,{tl}lL=-12	l=1
.
s.t. ∣WL-1,j ∣2 ≤ 1, ∀j ∈ [mL-1]
XW1 . . . WL-1WL = y
Then, We use the epigraph form for the norm of the first L 一 2 to achieve the equivalence. □
Proposition 3.1. First L - 2 hidden layer weight matrices in equation 14 have the same operator
and Frobenius norms, i.e., t1 = t2 = . . . = tL-2, where tl = ∣Wl ∣F = ∣Wl ∣2 , ∀l ∈ [L 一 2].
Proof of Proposition 3.1. Let us first denote the sum of the norms for the first L 一 2 layer as t, i.e.,
t = PlL=-12 tl, Where tl = ∣Wl∣2 = ∣Wl∣F since the upper-bound is achieved When the matrices
are rank-one (seeequation 28). Then, to find the extreme points, We need to solve the folloWing
problem
max ∣Wl-2∣2 ... ∣Wι∣2∣Vχ W"∣2
{θl }lL=-12	.
We can equivalently reWrite this problem using the variables {tl }lL=-12 as folloWs
L-2
max	tl
{tl }lL=-12 l=1
L-2
s.t. t = X tl, tl ≥ 0
l=1
max
L-3	L-3
t -	X %，	Yti
l=1	j=1
L-3
s.t. X tl ≤ t, tl ≥ 0
l=1
18
Under review as a conference paper at ICLR 2021
If we take the derivative of the objective function of the latter problem, i.e., denoted as
f(t1, . . . , tL-3), with respect to tk, we obtain the following
∂f(tι, . ..,tL-3)
∂tk
L-3	L-3	L-3	L-3
t Y tl - 2 Y tl - X tl Y tl.
l=1	l=1	l=1	j=1
l6=k	l6=k	l6=k
Then, equating the derivative to zero yields the following relation
L-3
tk = t - X t
l=1
where t^ denotes the optimal operator norm for the kth layer's weight matrix. We also note that
these solutions satisfy the constraints in the optimization problem above. Since by definition t -
Pl=1 tl = tL-2, we have tl = t2 = ... = tL-2.	口
Theorem 3.1. Optimal layer weights for equation 14 satisfy the following relation
(产 Vxw: CT if l - 1
t kwrk2p1 ifl = 1
t*ρι-1ρf if 1 <l≤ L — 2	,
ρL-2 ifl = L - 1
where kpιk2 = 1, ∀l ∈ [L 一 2] and Wr follows the definition in equation 8.
Proof of Theorem 3.1. Using Lemma A.3 and Proposition 3.1, we have the following dual problem
for equation 14
P* = min max λτy + (L - 2)t2 s.t. ∣(XWι... wl-1 j)Tλ∣ ≤ 1, wl-1 j ∈ B2	(23)
{θl}lL=-11,t λ
kWlkF ≤t, ∀l ∈ [L-2].
Now, let us assume that the optimal Frobenius norm for each layer l is t* 10. Then, if we define
ΘL-1 = {θ1, . . . , θL-1 |kwL-1,j k2 ≤ 1,∀j ∈ [mL-1], kWlkF ≤ t*, ∀l ∈ [L - 2]}, equation 23
reduces to the following problem
P* ≥ D* =mɑx λτ y s.t. ∣(XW1... WL-I)T λ∣ ≤ 1, ∀θι ∈ Θl-i, ∀l,	(24)
where we change the order of min-max to obtain a lower bound for equation 23. The dual of the
semi-infinite problem in equation 24 is given by
min ∣∣μkτv s.t. /	XWi ... WL-1dμ(θ1,.. .,Θl-i) = y,	(25)
{θl}lL=-11∈ΘL-1
where μ is a signed Radon measure and k ∙ ∣∣τv is the total variation norm. We emphasize that
equation 25 has infinite width in each layer, however, an application of Caratheodory’s theorem
shows that the measure μ in the integral can be represented by finitely many (at most n + 1) Dirac
delta functions (Rosset et al., 2007). Such selection of μ yields the following problem
mL-1
Pm* =	min	∣WL ∣i s.t. X XWij... WjL-iwL,j =y,	θιj	∈	ΘL-i,	∀l	(26)
{θl}lL=1	j=i
We first note that since the model in equation 26 has multiple weight matrices for each layer, it
has more expressive power than a regular network. Thus, we have P* ≥ Pm* . Since the dual of
equation 14 and equation 26 are the same, we also have Dm* = D*, where Dm* is the optimal dual
value for equation 26.
We now apply the variable change in equation 7 to equation 24 as follows
max λT ΣχWr s.t. ∣WT-2... WT Vχ∑T λ ∣2 ≤ 1, ∀θι ∈ Θl-i, ∀l	(27)
10With this assumption, (L - 2)t2 becomes constant so we ignore this term for the rest of our derivations.
19
Under review as a conference paper at ICLR 2021
which shows that the maximum objective value is achieved when ΣTλ = ci W；. Thus, the optimal
layer weights can be found as the maximizers of the constraint when ΣT λ = ciw；. To find the
formulations explicitly, we first find an upper-bound for the constraint in equation 27 as follows
∣WT-2... WT Vχ∑T λ k2 = cι∣WT-2... WT Vx W"∣2 ≤ ci∣Wl-2 k2 …kVχW"∣2 ≤ c1γkVχWrk2,
where the last inequality follows from the constraint on each layer weight,s norm and Y = t*L 2.
This upper-bound can be achieved when the layer weights are
ft* VxW: CT if l _ 1
t kwrk2ρ1 ifl = 1
t*ρl-1ρlT if1 <l ≤L-2	,	(28)
ρL-2 ifl=L-1
where kρl k2 = 1, ∀l ∈ [L - 2]. This shows that the weight matrices are rank-one and align with
each other. Therefore, an arbitrary set of unit norm vectors, i.e., {ρl }lL=-12 can be chosen to achieve
the maximum dual objective.
We note that the layer weights in equation 28 are optimal for the relaxed problem in equation 26.
However, since there exists a single possible choice for the left singular vector of W1 and we can
select an arbitrary set for {ρl}lL=-12, we achieve Dm* = D* using the same layer weights. Therefore,
the set of weights in equation 28 are also optimal for equation 14.	□
Theorem 3.3. Optimal layer weight for equation 15 can be formulated as follows
ft* Pj=ι VwjPTj ifl = 1
Wl* = t* PjK=1ρl-1,jρlT,j if1 < l ≤ L - 2	,
[pK=i PL-2,j ifl = L - 1
where Vwj is the jth maximal right singular vector of ΛtX and we may pick a Set of unit norm
vectors {ρl,j }lL=-12 such that ρlT,j ρl,k = 0, ∀j 6= k.
Proof of Theorem 3.3. Using Proposition 3.1 and Lemma A.4, we obtain the following dual problem
max trace(ΛT Y) s.t. σmax (ΛTXW1 . . . WL-2) ≤ 1, ∀θl ∈ ΘL-1 .	(29)
Λ	max
It is straightforward to show that the optimal layer weights are the extreme points of the constraint
in equation 29, which achieves the following upper-bound
max	σmax (ΛTXW1 . . . WL-2) ≤ σmax (ΛTX)γ.
{θl}lL=-12∈ΘL-1
This upper-bound is achieved when the first L - 2 layer weights are rank-one with the singular value
t* by Proposition 3.1. Additionally, the left singular vector of W1 needs to align with one of the
maximum right singular vectors of ΛT X. Since the upper-bound for the objective is achievable for
any Λ, we can maximize the objective value, as in equation 13, by choosing a matrix Λ such that
ΛTUxΣx = Vw
0k-rw ×rx
0rx ×d-rw
0k-rw ×d-rw
UTw
where Wr* = UwΣwVwT. Thus, a set of optimal layer weights can be formulated as follows
ft*vw,jρT,j ifl = 1
Wlj =	t*ρl-1,jρlT,j if1 < l ≤L-2	,	(30)
lpL-2,j if l = L - 1
where Vwj is the jth maximal right singular vector of ΛtX. However, notice that the layer weights
in equation 30 are the optimal weights for the relaxed problem, i.e.,
mL-1	mL-1
miLn X kWL,jk2 s.t. X XW1j . ..WLj-1WLT,j =Y, ∀θlj ∈ ΘL-1.	(31)
{θl }l=1 j=1	j=1
20
Under review as a conference paper at ICLR 2021
Using the optimal layer weights in equation 30, we have the following network output for the relaxed
model
mL-1	mL-1
X XW1j . . . wjL-1wLT,j = γ X qw,jwLT,j.
j=1	j=1
Since we know that the objective value for equation 31 is a lower bound for equation 15, the layer
weights that achieve the output above for the original problem in equation 15 is optimal. Thus, a set
of optimal solutions to equation 15 can be formulated as follows
ft* Pm=TI VwjPTj if 1 = 1
Wi= <t* Pj=JPl-IjPT if 1 <1 ≤ L - 2	(32)
IPmITPL-2,j if 1 = L - 1
where We select a set of unit norm vectors {ρι,j }L-2 such that PTjpι,k = 0, ∀j = k.	□
Theorem 3.2. Let {X, y} be feasible for equation 14, then strong duality holds for finite width
networks.
Proof of Theorem 3.2. We first select a set of unit norm vectors, i.e., {Pι}ιL=-12, to construct weight
matrices {Wιe}ιL=-11 that satisfies equation 28. Then, the dual of equation 14 can be written as
Di = max λTy
eλ
.
s.t. ∣(XWe ... WL-I)T λ∣ ≤ 1
Then, we have
Pi = min	maxλTy	≥	maxλTy	(33)
{θl}lL=-11∈ΘL-1 λ	λ
s.t. ∣(XWι... WL-I)Tλ∣ ≤ 1	s.t. ∣(XWι... WL-I)Tλ∣ ≤ 1, ∀θι ∈ Θl-i
=	max λTy
s.t. ∣(XWe ... WL-I)T λ∣ ≤ 1
= Di = Di = Di
em
where the first inequality follows from changing the order of min-max to obtain a lower bound and
the first equality follows from the fact that {Wιe}ιL=-11 maximizes the dual problem. Furthermore,
we have the following relation between the primal problems
Pei = min kWL k1	≥
wL
s.t.W1e...WLe-1WL =y
Pi = min kWL k1	(34)
{θl}lL=1∈ΘL-1
s.t. W1 . . . WL-1WL = y,
where the inequality follows from the fact that the original problem has infinite width in each layer.
Now, notice that the optimization problem on the left hand side of equation 34 is convex since it is
an `1 -norm minimization problem with linear equality constraints. Therefore, strong duality holds
for this problem, i.e., Pei = Dei and we have Pei ≥ Pi ≥ Pmi ≥ Dei = Di = Dmi . Using this result
along with equation 33, we prove that strong duality holds, i.e., Pei = Pi = Pmi = Dei = Di =
Di
m.
□
Corollary 3.1. Theorem 3.1 implies that deep linear networks can obtain a scaled version of y
using only the first layer, i.e., XW1P1 = cy, where c > 0. Therefore, the remaining layers do not
contribute to the expressive power.
Proof of Corollary 3.1. The proof directly follows from equation 28.	□
Corollary A.1. The analysis above and Theorem 3.2 also show that strong duality holds for the
regularized deep linear network training problem.
21
Under review as a conference paper at ICLR 2021
Proof of Corollary A.1. The proof directly follows from the analysis in this section and Theorem
3.2.	□
Lemma A.4. The following problems are equivalent:
L
θmiLn X kWlk2F
{θl}l=1 l=1
s.t. fθ,L (X) = Y
mL-1	L-2
{θ}Lmi{nt}L-2 X kwL,jk2+Xtl2
{θl}l=1,{tl}l=1 j=1	l=1
s.t. fθ,L(X) =Y, wL-1,j ∈B2,kWlkF ≤tl, ∀l ∈ [L-2]
Proof of Lemma A.4. Applying the scaling trick in Lemma A.1 to the last two layer of the L-layer
network in equation 15 gives
mL-1	L-2
min X kwL,jk2+XkWlk2F
{θl}lL=1,{tl}lL=-12 j=1	l=1
.
s.t. kwL-1,j k2 ≤ 1,∀j ∈ [mL-1]
XW1 . . . WL-1WL = Y
Then, We use the epigraph form for the norm of the first L - 2 to achieve the equivalence. □
Theorem 3.4. Let {X, y} be feasible for equation 15, then strong duality holds for finite width
networks.
Proof of Theorem 3.4. We first select a set of unit norm vectors, i.e., {ρl,j}lL=-12, to construct Weight
matrices {Wle,j}lL=-11 that satisfies equation 30. Then, the dual of equation 15 can be Written as
D* = maxtrace(ΛT Y)
eΛ
.
s.t. σmax(ΛT XW1e,j... WLe,-j 2) ≤ 1, ∀j
Then, We have
P * = min	max trace(ΛT Y)
{θl}lL=-11∈ΘL-1 Λ
s.t. σmax(ΛTXW1...WL-2) ≤ 1
≥ max trace(ΛT Y)
(35)
s.t. σmax(ΛT XW1... WL-2) ≤ 1, ∀θl ∈ ΘL-1
max trace(ΛT Y)
s.t. σmax(ΛT XW1e,j... WeL,-j 2) ≤ 1, ∀j
De* = D* = Dm*
Where the first inequality folloWs from changing the order of min-max to obtain a loWer bound and
the first equality folloWs from the fact that {Wle,j }lL=-11 maximizes the dual problem. Furthermore,
We have the folloWing relation betWeen the primal problems
mL-1
Pe* = min X kwL,jk2
WL j=1
mL-1
s.t. X W1e,j... WeL,-j 1wLT,j =Y
j=1
mL-1
P* = min X kwL,j k2	(36)
{θl}lL=1∈ΘL-1 j=1
s.t. W1 . . . WL-1WL = Y,
≥
Where the inequality folloWs from the fact that the original problem has infinite Width in each layer.
NoW, notice that the optimization problem on the left hand side of equation 36 is convex since it is
an '2-norm minimization problem with linear equality constraints. Therefore, strong duality holds
for this problem, i.e., Pe* = De* and We have Pe* ≥ P* ≥ Pm* ≥ De* = D* = Dm* . Using this result
along with equation 35, we prove that strong duality holds, i.e., Pe* = P* = Pm* = De* = D * =
D*
m.
□
22
Under review as a conference paper at ICLR 2021
A.7 Proofs for the deep ReLU networks
Theorem 4.1. Let X be a rank-one data matrix such that X = ca0T, where c ∈ Rn+ and a0 ∈ Rd,
then strong duality holds and the optimal weights for each layer can be formulated as follows
Wl = τφ⅛ΦT, ∀l ∈ [L - 2], WLT = TEφL-⅛,
kφl-1 k2	kφL-2 k2
where φo = a° and {φl}L-2 is a set ofvectors such that φl ∈ Rml and ∣∣φl∣∣2 = t*, ∀l ∈ [L 一 2].
Proposition 1. First L - 2 hidden layer weight matrices in equation 16 have the same operator and
Frobenius norms.
Proof of Proposition 1. Let us first denote the sum of the norms for the first L - 2 layer as t, i.e.,
t = PlL=-12 tl, where tl = kWlk2 = kWlkF since the upper-bound is achieved when the matrices
are rank-one. Then, to find the extreme points (see the details in Proof of Theorem 4.1), we need to
solve the following problem
arg max ∣λ*T c∣∣∣aL-2∣∣2 = arg max	∣λ*T c| k(aLτ-3WL-2)+k2
{θl }lL=-12	{θl}lL=-12∈ΘL-1
where we use	aTL-2	=	(aTL-3WL-2)+.	Since	kWL-2 kF =	tL-2	= t -	PlL=-13,	the objective
value above becomes ∣λ*Tc∣∣∣(aL-3∣∣2 (t — PL=3). Applying this step to all the remaining layer
weights gives the following problem
L-3 L-3	L-3
arg max ∣λ*	c∣∣∣aok2	t-X	Ytl s.t.	s.t. X tl	≤ t, tl	≥ 0.
{tl }lL=-13∈ΘL-1	l=1	j=1 l=1
Then, the proof directly follows from Proof of Proposition 3.1.	□
Proof of Theorem 4.1. Using Lemma A.3 and Proposition 1, this problem can be equivalently
stated as
min	kWLk1 s.t. Al = (Al-1 Wl)+, ∀l ∈ [L - 1]
{θl}lL=1∈ΘL-1
AL-1 WL = y
which also has the following dual form
P * = min	max λτ y
{θl}lL=-11∈ΘL-1 λ
.
s.t. kATL-1 λk∞ ≤ 1
Notice that we remove the recursive constraint in equation 38 for notational simplicity, however,
AL-1 is still a function of all the layer weights except WL . Changing the order of min-max in
equation 38 gives
P* ≥ D* =maxλTy s.t. kATL-1 λk∞ ≤ 1, ∀θl ∈ ΘL-1 , ∀l ∈ [L - 1].	(39)
The dual of the semi-infinite problem in equation 39 is given by
min kμkτv
s.t.
where μ is a signed Radon measure and k ∙ kτv is the total variation norm. We emphasize that
equation 40 has infinite width in each layer, however, an application of Caratheodory’s theorem
shows that the measure μ in the integral can be represented by finitely many (at most n +1) Dirac
delta functions (Rosset et al., 2007). Thus, we choose
mL-1
μ = E δ(Wι- W1,..., Wl-1 - wj-1)WL,j,
j=1
(37)
(38)
(40)
(Al-2Wl-1)+dμ(θ1,..., Θl-1) = y,
{
-1
23
Under review as a conference paper at ICLR 2021
where δ(∙) is the Dirac delta function and the superscript indicates a particular choice for the corre-
sponding layer weight. This selection of μ yields the following problem
Pm = min kwLkι
mL-1
s.t. X (AL-2wL-i)+wL,j = y, θj ∈ θL-1, ∀l ∈ [L - 1]
j=1
(41)
Here, we first note that even though the model in equation 41 has the same layer widths with regular
deep ReLU networks, it has more expressive power since it allows us to choose multiple weight
matrices for each layer. Based on this observation, We have P * ≥ Pm.
As a consequence of equation 39, we can characterize the optimal layer weights for equation 41 as
the extreme points that solve
arg max ∣λ*T (Al-2Wl-i)+∣	(42)
{θl}lL=-11∈ΘL-1
where λ* is the optimal dual parameter. Since we assume that X = ca0T with c ∈ Rn+, we have
AL-2 = caTL-2, where alT = (alT-1Wl)+, al ∈ R+ml and ∀l ∈ [L - 1]. Based on this observation,
we have WL-1 = aL-2/kaL-2 k2, which reduces equation 42 to the following
arg max	∣λ*T c∣∣∣aL-2∣∣2	(43)
{θl}lL=-12∈ΘL-1
We then apply the same approach to all the remaining layer weights. However, notice that each
neuron for the first L - 2 layers must have bounded Frobenius norms due to the norm constraint. If
we denote the optimal `2 norms vector for the neuron in the lth layer as φl ∈ R+ml, then we have the
following formulation for the layer weights that solve equation 42
Wl = IIf I φT, ∀l ∈ [L - 2], WLT = ∖∖7L 2∣∣ ,	(44)
kφl-1k2	kφL-2k2
where φ0 = a0, {φl}lL=-12 is a set of nonnegative vectors satisfying kφl k2 = t*, ∀l ∈ [L - 2].
We note that the layer weights in equation 44 are optimal for the relaxed problem in equation 41.
However, since there exists a single possible choice for the left singular vector of W1 and we can
select an arbitrary set for {φl}lL=-12, the dual problems coincide for equation 16 and equation 41, i.e.,
we achieve Dm* = D* using the same layer weights, where Dm* is the optimal dual objective value
for equation 41. Therefore, the set of weights in equation 44 are also optimal for equation 16. 口
Theorem 4.2.	Let X be a data matrix such that X = ca0T, where c ∈ Rn and a0 ∈ Rd. Then, a set
of optimal solutions to equation 16 satisfies {(wi, bi)}m=ι, where Wi = Si 口羡％, b = -Sicika0k2
with si = ±1, ∀i ∈ [m].
Proof of Theorem 4.2. Given X = ca0T, all possible extreme points can be characterized as follows
arg max ∣λτ(Xw + bl) +| = arg max ∣λτ(caTW + bl) *|
b,w:kwk2 =1	b,w:kwk2 =1
n
arg max I	λi (ciaTW + b) +
b,w:kwk2 =1 i=1
which can be equivalently stated as
arg max	λi cia0TW +	λi b s.t.
b,w:kwk2=1 i∈S	i∈S
cia0TW + b ≥ 0, ∀i ∈ S
cj a0TW + b ≤ 0, ∀j ∈ Sc
which shows that W must be either positively or negatively aligned with a°, i.e., W = S 口羡%, where
S = ±1. Thus, b must be in the range of [maxi∈S(-Scika0k2), minj∈Sc(-Scj ka0k2)] Using these
observations, extreme points can be formulated as follows
k⅛ if Ρi∈S λici ≥ 0 and 队=∫minj ∈Sc (-sλcj ka0k2) if Pi∈s λ ≥ 0
-02 otherwise	λ	ImaXi∈s (-Sλci∣∣a0k2)	otherwise ,
where Sλ = Sign(Pi∈s λici).	口
(
Wλ
24
Under review as a conference paper at ICLR 2021
Proposition 4.1. Theorem 4.1 still holds when we add a bias term to the last hidden layer, i.e., the
output becomes AL-2WL-1 + 1nbT +wL = y, where Al = (Al-1Wl)+, ∀l ∈ [L - 2].
Proof of Proposition 4.1. Here, we add biases to the neurons in the last hidden layer of equation 16.
For this case, all the equations in equation 37-equation 39 hold except notational changes due to the
bias term. Thus, equation 42 changes as
arg max	∣λ*T (Al-2Wl-i + b1n)+∣ = arg max	∣λ*T (caT-2 wl-i + b1n)+ |
{θl}lL=-11∈ΘL-1,b	{θl}lL=-11∈ΘL-1,b
n
arg max I λi (ciaT-1wL-1 + b)+ I
{θl}lL=-12∈ΘL-1,b i=1
(45)
which can also be written as
ʌ * T	. ʌ *7 + C	CiaT-?wl-i + b ≥ 0,∀i ∈ S
argmax	XλiciaL-2wL-1 + Xλib s.t. ↑ r TT W JbV ns 二 S C ,
{θl}lL=-11∈ΘL-1,bi∈S	i∈S	cjaL-2wL-1+b≤0,∀j ∈S
where S and Sc are the indices for which ReLU is active and inactive, respectively. This shows that
wl-1 must be wl-i = ±11蔗/ and b ∈ [maxi∈s(-Ci∣∣aL-2∣∣2), minj∈sc(-Cj∣∣aL-21∣2)].
Then, we obtain the following
w*	= (kaL-⅛	if Pi∈s λ*ci	≥ 0 and b*	= (minj∈sc(-sλ-CjllaL-2k2)	if Pi∈s λ*	≥ 0
L-I ɪ ∣-αL-∣∣2	otherwise	Imaxi∈s(-sλ*CiIlaL-2k2)	otherwise
(46)
where sλ* = sign( i∈S λi*Ci). This result reduces equation 45 to the following problem
arg max	|C (λ*, c)∣∣∣aL-2∣∣2,
{θl}1L-∣ ∈ΘL-1
where C(λ* , c) is constant scalar independent of {Wl }lL=-12 . Hence, this problem and its solutions
are the same with equation 43 and equation 44, respectively.
□
Corollary 4.1. As a result of Theorem 4.2, when we have one dimensional data, i.e., x ∈ Rn, an
optimal solution to equation 16 can be formulated as {(wi, bi)}im=1, where wi = si, bi = -sixi
with si = ±1, ∀i ∈ [m]. Therefore, the optimal network output has kinks only at the input data
points, i.e., the output function is in the following form: fθ,2(X) = Ei (X — Xi)+. Therefore, the
network output becomes linear spline interpolation for one dimensional datasets.
Corollary 4.2. As a result of Theorem 4.2 and Proposition 4.1, when we have one dimensional
data, i.e., x ∈ Rn, the optimal network output has kinks only at the input data points, i.e., the output
function is in thefollowingform: fθ,L(X) = Ei (X — Xi) +. Therefore, the network output becomes
linear spline interpolation for one dimensional datasets.
Proof of Corollary 4.1 and 4.2. Let us particularly consider the input sample a0 . Then, the activa-
tions of the network defined by equation 44 and equation 46 are
aT = (aT Wι)+ = (aT 号 ΦT)	= ka0k2ΦT
la0 l2	+
aT =(HTW2)+ = (aT ii£ɪφT )	= kaOgkOTl∣2φT
la1 l2	+
aT-2 = (aT-3WL-2)+ = (aT-3 H L 3 φT-2) = llaolSMTk2 . . . kφT-3k2φT-2
-	-	- laL-3 l2	-	+	-	-
aL-1 = (aTL-2wL-1 + b)+ = (laL-2 l2 - laL-2 l2)+ = 0.
Thus, if we feed Cia0 to the network, we get aL-1 = (Ci laL-2 l2 - Ci laL-2 l2)+ = 0, where we
use the fact that optimal biases are in the form of b = -Ci laL-2 l2 as proved in equation 46. This
analysis proves that the kink of each ReLU activation occurs exactly at one of the data points. □
25
Under review as a conference paper at ICLR 2021
Proposition 4.2. Strong duality also holds for deep ReLU networks with vector outputs and the
optimal layer weights can be formulated as in Theorem 4.1.
Proof of Proposition 4.2. For vector outputs, we have the following training problem
L
min X kWlk2F s.t. fθ,L (X) = Y.
{θl }lL=1 l=1
After a suitable rescaling as in the previous case, the above problem has the following dual
P* ≥ D* =maxtrace(ΛTY) s.t. ∣∣ΛT(Al-2Wl-i) + ∣∣2 ≤ 1, ∀θι ∈ Θl-i, ∀l ∈ [L - 1].(47)
Using equation 47, we can characterize the optimal layer weights as the extreme points that solve
arg max	kΛ*T (AL-2WL-1)+ k2,	(48)
{θl}lL=-11∈ΘL-1
where Λ* is the optimal dual parameter. Since we assume that X = ca0T with c ∈ Rn+, we have
AL-2 = caTL-2, where alT = (alT-1Wl)+, al ∈ R+ml and ∀l ∈ [L - 1]. Based on this observation,
we have WL-1 = aL-2/kaL-2 k2, which reduces equation 48 to the following
arg max	kΛ*T ck2 kaL-2k2.
{θl}lL=-12∈ΘL-1
Then, the rest of steps directly follow Theorem 4.1 yielding the following weight matrices
Wl =名十ΦT, ∀l ∈ [L - 2], WL-1 = 7TφL-V,
kφl-1k2	kφL-2k2
where φ0 = a0, {φl}lL=-12 is a set of nonnegative vectors satisfying kφl k2 = t*, ∀l ∈ [L - 2].
Moreover, as a direct consequence of Theorem 3.4, strong duality holds for deep ReLU networks.
□
Theorem 4.3.	Let {X, Y} be a dataset such that XXT = In 11 and Y has orthogonal columns,
then the optimal weight matrices for each layer can be formulated as follows
1	2K	φ	1
Wl =	X	φl-1,r	φTr, ∀l ∈	[L - 2], Wl-1	=	∣∣zφL-2,1∣∣	...	UfL-2,2K	,
√2K W kφl-1,rk2 ,	√2K [kφL-2,lk2	kφL-2,2Kk2J,
where (Φ0,2j-1, Φ0,2j) = (XT也)十，XT( - 丫/十)，Nj ∈ K] and {Φl,r}L-L2 is a set of vectors
such that φl,r ∈ R+ml， kφl,r k2 = t*， and φlT,iφl,j = 0, ∀i 6= j.
Proof of Theorem 4.3. For vector outputs, we have the following training problem
L
min XkWlk2F s.t. fθ,L (X) = Y.
{θl }lL=1 l=1
After a suitable rescaling as in the previous case, the above problem has the following dual
P* ≥ D* =maxtrace(ΛtY) s.t. ∣∣ΛT(AL-2WL-ι) + k2 ≤ 1, ∀θl ∈ Θl-l, ∀l ∈ [L - 1].(49)
Using equation 49, we can characterize the optimal layer weights as the extreme points that solve
arg max	kΛ*T (AL-2WL-1)+ k2,	(50)
{θl}lL=-11∈ΘL-1
where Λ* is the optimal dual parameter. We first note that since X is whitened such that XXT = In ,
equation 50 implies σmax(Λ*) ≤ t*L-2 . Then, the objective is trivially maximized by Λ* =
11This can be achieved by applying batch whitening, which often improves accuracy (Huang et al., 2018).
26
Under review as a conference paper at ICLR 2021
t*L 2 Y∕σmaχ(Y), which is also a feasible solution. Therefore, equation 50 can be equivalently
written as
arg max	kYT (AL-2wL-1)+k2.	(51)
{θl}lL=-11∈ΘL-1
We now note that since Y has orthogonal columns, equation 51 can be decomposed into k maxi-
mization problems each of which can be maximized independently to find a set of extreme points.
In particular, the j th problem can be formulated as follows
argmaχ IyT(AL-2WL-I)+ | ≤ maχ {k(yj) + k2, k(-yj) + k2}.
{θl}lL=-11∈ΘL-1
Then, noting the whitened data assumption, the rest of steps directly follow Theorem 4.1 yielding
the following weight matrices
2
W — 1 X Φl-1,r 小T U/ U「丁	9] w — 1 l^ φL-2,1	φL-2,2 ]
Wl =	√2∑. kφl-1,r k2 φl,r,	∀l	∈	[L	- 2], WL-I =	√2	[kΦL-2,lk2	kΦL-2,2k/	,
where Φ0,r = XT ± yj + and {Φl,r }lL=-12 is a set of nonnegative vectors satisfying kΦl,r k2 =
t* and φT1Φl,2 = 0, ∀l. Thus, combining the extreme points for each j yields
1	2K	φ	1
Wl = -^= X U φ	,r	ΦT,	∀l	∈	[L —	2],	Wl-1 =	UfL-2,1	...	UfL-2,2K	,
√2Kr=ι kΦl-1,rk2 ,	√2K LkφL-2,ιk2	kφL-2,2Kk2J,
where (Φ0,2j-1, Φ0,2j) = (XT (丫力十,XT ( — 丫力十)，∀j ∈ [K ] and {φl,r }L-2 is a set of nonneg-
ative vectors satisfying ∣∣Φl,r∣∣2 = t* and ΦT,iΦl,j = 0 ∀i = j. Moreover, as a direct consequence
of Theorem 3.4, strong duality holds for deep ReLU networks.	□
Theorem 4.4. Let {X, Y} be a dataset such that XTX = In and Y has orthogonal columns, then
a set of optimal layer weight matrices for the following regularized training problem
min1 kfθ,L(χ)—YkF + β X kWlkF	(17)
θ∈Θ 2	2
l=1
can be formulated as follows
Wl = (PrKI ≡-⅛ΦTr ,	if 1 ≤ l ≤ L - 1
IPrKI (kΦ0,rk2 — β) Φl-1,r^T	if l = L	,
where ^2j-ι = ^2j = ej, ∀j ∈ [K], ej is the jth ordinary basis vector, and the other definitions
follows from Theorem 4.3 except t* = 1.
Proof of Theorem 4.4. From Section A.4.2 and the proof of Theorem 4.3, the dual problem has a
closed-form solution as follows
λ* = (βWB	ifβ ≤kyjk2 , ∀j ∈ [K].	(52)
yj	otherwise
and the corresponding extreme points of the constraint in
Wl = nPrKι 襦』ΦT,	if 1 ≤ l ≤ L - 1 ,	(53)
where the definitions follows from Theorem 4.3.
We now note that given the hidden layer weight in equation 53, the primal problem in equation 17 is
convex and differentiable with respect to the output layer weight WL. Thus, we can find the optimal
output layer weights by simply taking derivative and equating it to zero. Applying these steps yields
the following output layer weights
2K
WL = X (kΦ0,rk2 — β) ΦL-1,reT,
r=1
where e2j-1 = ^2j = ej, ∀j ∈ [K] and ej is the jth ordinary basis vector.	□
27