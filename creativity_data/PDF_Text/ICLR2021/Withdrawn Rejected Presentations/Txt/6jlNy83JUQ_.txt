Under review as a conference paper at ICLR 2021
Low Complexity Approximate Bayesian Logis-
tic Regression for Sparse Online Learning
Anonymous authors
Paper under double-blind review
Ab stract
Theoretical results show that Bayesian methods can achieve lower bounds on re-
gret for online logistic regression. In practice, however, such techniques may not
be feasible especially for very large feature sets. Various approximations that, for
huge sparse feature sets, diminish the theoretical advantages, must be used. Of-
ten, they apply stochastic gradient methods with hyper-parameters that must be
tuned on some surrogate loss, defeating theoretical advantages of Bayesian meth-
ods. The surrogate loss, defined to approximate the mixture, requires techniques
as Monte Carlo sampling, increasing computations per example. We propose low
complexity analytical approximations for sparse online logistic and probit regres-
sions. Unlike variational inference and other methods, our methods use analytical
closed forms, substantially lowering computations. Unlike dense solutions, as
Gaussian Mixtures, our methods allow for sparse problems with huge feature sets
without increasing complexity. With the analytical closed forms, there is also no
need for applying stochastic gradient methods on surrogate losses, and for tuning
and balancing learning and regularization hyper-parameters. Empirical results top
the performance of the more computationally involved methods. Like such meth-
ods, our methods still reveal per feature and per example uncertainty measures.
1	Introduction
We consider online (Bottou, 1998; Shalev-Shwartz et al., 2011) binary logistic regression over a
series of rounds t ∈ {1, 2, . . . , T}. At round t, a sparse feature vector xt ∈ [-1, 1]d with dt d
nonzero values, is revealed, and a prediction for the label yt ∈ {-1, 1} must be generated. The
dimension d can be huge (billions), but dt is usually tens or hundreds. Logistic regression is used
in a huge portion of existing learning problems. It can be used to predict medical risk factors, to
predict world phenomena, stock market movements, or click-through-rate in online advertising. The
online sparse setup is also very common to these application areas, particularly, if predictions need
to be streamed in real time as the model keeps updating from newly seen examples.
A prediction algorithm attempts to maximize probabilities of the observed labels. Online meth-
ods sequentially learn parameters for the d features. With stochastic gradient methods (Bottou,
2010; DUchi et al., 2011), these are weights wi,t associated with feature i ∈ {1,…，d} at round
t. Bayesian methods keep track of some distribution over the parameters, and assign an expected
mixture probability to the generated prediction (Hoffman et al., 2010; Opper & Winther, 1998). The
overall objective is to maximize a sequence likelihood probability, or to minimize its negative log-
arithm. A benchmark measure of an algorithm’s performance is its regret, the excess loss it attains
over an algorithm that uses some fixed comparator values of W = (wι, w2,..., Wd)T (T denoting
transpose). A comparator W that minimizes the cumulative loss can be picked to measure the regret
relative to the best possible comparator in some space of parameter values.
Kakade & Ng (2005); Foster et al. (2018); Shamir (2020) demonstrated that, in theory, Bayesian
methods are capable to achieve regret, logarithmic with the horizon T and linear with d, that even
matches regret lower bounds for d = o(T ). Classical stochastic gradient methods are usually im-
plemented as proper learning algorithms, that determine Wt prior to observing xt, and are inferior
in the worst-case (Hazan et al., 2014), although, in many cases depending on the data, they can still
achieve logarithmic regret (Bach, 2010; 2014; Bach & Moulines, 2013). Recent work (JezeqUeI
et al., 2020) demonstrated non-Bayesian improper gradient based algorithms with better regret.
1
Under review as a conference paper at ICLR 2021
Unfortunately, superiority of Bayesian methods diminishes by their intractability. A theoretically
optimal prior has a diagonal covariance matrix, with each component either uniformly or Normally
distributed with large variance. Effects of such a prior cannot be maintained in practical online
problems with a large sparse feature set, as the posterior of such a prior no longer has the properties
of the prior, but must be maintained as a subsequent prior. Gaussian approximations that rely on
diagonalization of the covariance must be used. Neither normal nor the diagonal assumptions are
true for the real posterior (even with diagonal prior). They thus lead to performance degradations.
Diagonalization is similar to linearization in convex optimization, specifically for stochastic gradient
descent (SGD) (Zinkevich, 2003). It allows handling features independently, but limits performance.
Bayesian learning literature focused on applying such methods to predict posterior probabilities, and
provide model (epistemic) uncertainty measurements (Bishop, 2006; Dempster, 1968; Huelsenbeck
& Ronquist, 2001; Knill & Richards, 1996). However, uncertainty of a feature is, in fact, mostly
a function of the number of examples in which it was present; a measure that can be tracked, not
estimated. Methods, such as Variational Bayesian (VB) Inference (Bishop, 2006; Blei et al., 2017;
Drugowitsch, 2019; Drugowitsch et al., 2019; Ranganath et al., 2014), track such measurements by
matching the posterior. However, as demonstrated in Rissanen (1984) seminal work, minimizing re-
gret is identical to uncertainty reduction, as regret is merely a different representation of uncertainty.
Regret can be universally minimized over the possible parameter space through a good choice of a
prior. Hence, to minimize uncertainty, the role of an approximation is to preserve the effect of such
a prior at least in the region of the distribution that dominates the ultimate posterior at the hori-
zon T . This is a simpler problem than matching the posterior, and opens possibilities for simpler
approximations that can lead to results identical to those of heavy methods as VB.
VB methods are typically used offline to match a tractable posterior to the true one by upper bound-
ing overall loss. They are computationally involved, requiring either iterative techniques (Bishop,
2006; Blei et al., 2017; Murphy, 2012) like Expectation Maximization (EM) (Dempster et al., 1977;
Moon, 1996); or Monte Carlo (MC) sampling, replacing analytical expectation by an empirical one
over a randomly drawn set. To converge, MC can be combined with gradient descent, either requir-
ing heavy computations, or adapting stochastic gradient methods to update posteriors (Broderick
et al., 2013; Knowles, 2015; Nguyen et al., 2017a;b). For online problems, the posterior of an exam-
ple is the prior of the subsequent one. To minimize losses, online VB must converge to the optimal
approximation at every example. Otherwise, additional losses may be incurred, as the algorithm
may not converge at each example, while the target posterior keeps moving with subsequent exam-
ples. Moreover, combining methods that need to tune hyper-parameters defeats the parameter free
nature (Abdellaoui, 2000; Mcmahan & Streeter, 2012; Orabona & Pal, 2016) OfBayesian methods.
Most Bayesian literature addressed the dense problem, where xt consists of mostly nonzero entries
for every t, and the dimension d of the feature space is relatively small. Techniques, like Gaussian
Mixtures (Herbrich et al., 2003; Montuelle et al., 2013; 2014; Rasmussen, 2000; Reynolds et al.,
2000; Sung, 2004), that may use VB, usually apply matrix computations quadratic in d on the
covariance matrix. In many practical problems, however, a very small feature subset is present in
each example. For categorical features, only one of the features in the vector is present at any
example. Techniques, useful for the low dimensional dense problem, may thus not be practical.
Paper Contributions: We provide a simple analytical Bayesian method for online sparse logistic
and probit regressions with closed form updates. We generalize the method also for dense multi-
dimensional updates, if the problem is not completely sparse. Our results are first to study regret
for Bayesian methods that are simple enough to be applied in practice. They provide an example to
the connection between uncertainty and regret, and more broadly the Minimum Description Length
(MDL) principle (Grunwald, 2007; Rissanen, 1978a;b; 1984; 1986; Shamir, 2015; 2020). Empirical
results demonstrate the advantages of our method over computationally involved methods and over
other simpler approximations, both by achieving better regret and better loss on real data. As part of
the algorithm, uncertainty measures are provided with no added complexity. We specifically demon-
strate that it is sufficient to have an approximation focus on the location of the peak of the posterior
and its curvature or value, which are most likely to dominate regret, instead of approximating the
full posterior, which brings unnecessary complexity missing the real goal of preserving the effects
of a good prior. In fact, approximating the full posterior may eventually lead to poor generalization
and overfitting by focusing too much on the tails of the posterior. Our approach directly approxi-
mates the posterior, unlike VB methods that approximate by minimizing an upper bound on the loss.
Finally, our approach leverages sparsity to solve a sparse problem.
2
Under review as a conference paper at ICLR 2021
Related Work: The simplest single dimensional online logistic regression problem (d = 1 and
χι,t = 1,∀t) was widely studied. Jefferys' prior, ρ(θ) 4 1/ (∏pθ(1 - θ)), is asymptotically
optimal (Clarke & Barron, 1994; Xie & Barron, 1997; 2000; Drmota & Szpankowski., 2004)). It
can be expressed in terms of log-odds weights W as P(W) = ew/2 / [π (1 + ew)]. Applying a mixture
leads to the Krichevsky & Trofimov (1981) (KT) add-1/2 estimator Q (yt∣yt-1) = [nt-ι (yt) +
0.5]/t, where nt-1(yt) counts occurrences of yt. We use yt to express a sequence from 1 to t.
Applying this prior in a Follow The Regularized Leader (FTRL) setting (McMahan, 2011) also
leads to the KT estimator. This raised the question whether regret optimality generalizes to large
dimensions (McMahan & Streeter, 2012). Hazan et al. (2014) showed that this was not the case for
proper methods. The bounds, however, theoretically generalize for Bayesian methods (Kakade &
Ng, 2005; Foster et al., 2018; Shamir, 2020), with large variance Gaussian or uniform prior with
diagonal covariance. Peaked priors fail, as for each feature in an example, other features provide a
self excluding log-odds prior, that shifts the relation between the overall distribution and the feature
weight. While wide priors are good theoretically, because of the intractability of the Bayesian
mixture integrals, diagonal approximations that are used unfortunately degrade their effect.
Bayesian methods have been studied extensively for estimating posteriors and uncertainty (Bishop,
2006; Makowski et al., 2002; Sen & Stoffa, 1996). There is ample literature researching such tech-
niques in deep networks (see, e.g., Blundell et al. (2015); Hwang et al. (2020); Kendall & Gal
(2017); Lakshminarayanan et al. (2017); Malinin & Gales (2018); Wilson (2020)). Most of the
work focuses on the ultimate posterior after the full training dataset has been visited. One attempts
to leverage the uncertainty measurements to aid in inference on unseen examples. Techniques like
expectation propagation (EP) (Minka, 2001; 2013) (see also Bishop (2006); Chu et al. (2011); Cun-
ningham et al. (2011); Graepel et al. (2010)) and VB are used to generate estimates of the posterior.
In a dense setup, where there is a relatively small number features (or units in a deep network),
Gaussian Mixture models can also be learned, where a jointly Gaussian posterior is learned, usually
with some kernel that is used to reduce the dimensionality of the parameters that are actually being
trained. Such methods, however, do not fit the sparse online setup.
Variational methods are derived utilizing Jensen’s inequality to upper bound loss of expectation
by expectation of the negative logarithm of the product of the prior ρ(w) and data likelihood
P(yT|xT,w). Normalizing this joint by the expected label sequence probability gives the posterior
P(WIXT,yτ). Then, a posterior Q(∙) with a desired form is matched by minimizing the KL diver-
gence KL(Q∣∣P), which decomposes into expectation w.r.t. Qo over the loss on yτ and KL(Q∖∖ρ)
between the approximated posterior and the true prior. The first term may require techniques like the
iterative mean field approximation EM (Bishop, 2006; Jaakkola & Jordan, 1998), or MC sampling
to be approximated. Gradient methods can also minimize KL(Q∖∖P). In the sparse setup, it is stan-
dard to assume a diagonal Q(∙). In an online setting, the process can be iterated over the examples
(or mini-batches), where the posterior att is the prior att + 1. Computing the approximate posterior
may be very expensive if done for every example. SGD can be used with MC sampling, but that
would incur additional losses, as the posterior changes between successive examples. Like VB, EP
minimizes the opposite divergence KL(P∖∖Q) between the posterior and its approximate.
2	Preliminaries
Let ρt(W) be the prior on the weights at round t, where we start by initializing some ρ1(W). We will
assume that ρ(∙) is approximated by a diagonal covariance Gaussian, with means μ%,t and variances
σi2 *,t for component i at time t. Leveraging results in Kakade & Ng (2005); Foster et al. (2018);
Shamir (2020), if we restrict Wi ∈ [-B, B], a uniform prior over this interval or a normal prior
with standard deviation proportional to B can be picked. (To approximate a Dirichlet-1/2, 0-mean
normal prior with variance 2π can be used.) Observing sparse Xt, the prediction for yt is given by
pt =4 P(yt ∖Xt) =	p(yt∖Xt, W)ρt(W)dW =4	pt(yt, W∖Xt)dW,
(1)
where for binary logistic regression, the probability of the label given the example and weights is
given by the Sigmoid of the label weighted dot product of the example and weights
p(yt∖xt, w) 4 τ------1-----4 Sigma (ytxTW).
1 + exp -ytXtTW
(2)
3
Under review as a conference paper at ICLR 2021
The expected prediction Pt in (1) marginalizes out the weights W according to the prior ρt(∙) from
the joint probability of w and yt . The prediction pt is a function also of all prior pairs sequence
{xt-1,yt-1} through the prior ρt(∙). After observing yt, We try to match a (diagonal) posterior
Q(∙) to the weights that will equal the next round,s prior
ρt+ι(w) 4 Qt(w) ≈ p(w∣χt,yt) = p(yt|xt，W)Pt(W) = P(yt|Xt，W)Pt(W).	⑶
P (yt |xt)	pt
Using ST 4 {xτ,yτ}, the logarithmic loss incurred by approximation Q(∙) on the sequence of
predictions is L(ST,Q) 4 一 PT=IlogPt. Let w* be some fixed comparator in the parameter
values, space. Then, the regret of approximation Q(∙) relative to comparator w* is given by
T
R(ST,Q, w*) = L(ST,Q) - L(ST, W*) = 一 X [logPt + log(1 + exp(-ytXTw*)] .	(4)
t=1
The regret can measure the excess loss relative to the best possible w* comparator, if it is chosen.
3	Marginalized Bayesian Gaus s ian Approximation
In this section, we describe the proposed method. First, the Sigmoid is approximated by a normal
Cumulative Distribution Function (CDF). A prediction for the label of the current example is gen-
erated shrinking the cumulative mean score as function of the cumulative variance over all features.
The main idea for updating feature distributions is marginalizing away all other covariates for each
feature in an example at a given round, such that the mean and variance of the feature can be updated
to match the location of the peak and either its curvature or value for the true marginalized poste-
rior. In Appendix B, we demonstrate the same approach for Probit Regression. It follows the same
steps, except that it does not require the initial approximation. Finally, Appendix D.1 shows how
similar approximation methodology can be used to apply simple multi-dimensional updates instead
of marginalized one, the can be performed when sparsity is limited.
Gaussian Approximation of a Sigmoid: The relation between the logistic distribution and the
Normal one was well studied in the statistics literature (see, e.g., Bishop (2006); Murphy (2012)).
The Sigmoid function in (2) can be viewed as a CDF, which can be approximated by a normal CDF
Φ(z) (The inverse of Φ(∙) is the Probit function.) The derivative of the Sigmoid function is the
0-mean Logistic Probability Density Function (PDF). Matching the PDFs, we have ew /(1 + ew)2 ≈
1 /√2πσ2 exp { - w2 /2σ2}. This yields that the Sigmoid function can be approximated by a 0-mean
Gaussian CDF with variance 8/n. Using the standard 0-mean normal Φ(∙) function, the argument
is scaled by the inverse of the standard deviation，n/8, giving
Sigma(w) =4
1
1 + e-w
(5)
More details about this approximation are in Appendix A.
Approximation approach and some notation: With the diagonal and Gaussian assumptions, for
each sparse example (with only dt	d nonzero entries in Xt), we can assume that we have a
single normal random variable, whose mean is the Xt weighted mean, and whose variance is the
quadratically weighted sum of variances. Denote the example total weight, mean, and variance by
dd	d
wt	= Exi,twi,t,	μt	= Exi,t ∙	μi,t,	σt	= Eχ2,t	∙	σ2,t	(6)
i=1	i=1	i=1
(where the diagonalization assumption is important for the simplicity of the approximation ofσt2).
Since we consider a sparse problem, there is benefit to breaking the dependencies between features
present in a given example and updating each independently. We can achieve that by marginaliz-
ing the prior at t over all other features. Because we assume all features are jointly independent
Gaussians, we can break the joint prior into a product of two components; one, the marginal of the
feature, and the other the marginal of all other features together, i.e., the self excluding prior. To
4
Under review as a conference paper at ICLR 2021
match the posterior, we then marginalize on the latter, and match a single dimensional posterior for
each feature. We define the self excluding prior for feature i at time t, its mean and variance as
d
X4	2	42	22
xj,twj,t -xi,twi,t = / v xj,twj,t;	μ-i,t = μt-xi,tμi,t; σ-i,t = σt -xi,tσi,t∙ (7)
j=1	j6=i
Prediction: With the probit approximation in (5) and the single dimensional variable Wt , we can
compute pt in (1), replacing p(yt|xt, w) in (2) by a normal CDF. Approximating this integral (see,
e.g. Murphy (2012), Section 8.4.4.2, and Bishop (2006)) gives
pt ≈ Sigma
ytμt
√1 + 8 σ2
(8)
This result demonstrates how the prediction variance shrinks the prediction towards probability 0.5.
Marginalization: Given the diagonalization assumption, the prior at t can be expressed as ρt (w) =
Pi,t(wi) ∙ ρ-i,t(w-i), where ρ-i,t(∙) is the prior on the self excluding prior of w%. Hence,
P(yt, w|xt) = P(yt|xt, w)ρi,t(Wi)ρ-i,t(W-i).
Marginalizing on W-i gives
(9)
P(yt,Wi|xt) = ρi,t(Wi)	P(yt|xt, w)ρ-i,t (W-i)dW-i =4 ρi,t(Wi)Iw-i,t.
-∞
(10)
The inner integral, which marginalizes over W-i with its prior ρ-i,t(W-i), can be approximated by
Iw-i,t
Z∞
∞
(≈a)
Z∞
∞
1
q∕2πσ-i,t
1
q∕2πσ-i,t
exp -
(W-2σ2μ-i,t) ) ∙ Sigma [yt(xi,tWi + W-i)] dW-i
exp -
(W-2σ2μ-i,t) ) ∙ Φ [∖lπyt(Xi,tWi + W-i) dW-i
(b)
(c)
/	φ(z) ∙ Φ ]πy(yt(xi,tWi + μ-i,t + σ-i,
Φ P P8yt(μ-i,t + xi,tWi) ʌ
∖	∕1 + 8σ-i,t	)
tz) dz
(≈d)
Sigma
yt(μ-i,t + χi,twi)
√1 + 8 σ-i,t
(11)
Step (a) follows, again, from the approximation in (5). For (b), we apply the change of variables z =
(w-i -μ-i,t)∕σ-i,t, where φ(∙) is the standard Gaussian PDF. The integral in (b) gives Φ (√+p),
with a = ʌ/ɪyt(μ-i,t + Xi,tWi) and b2 = 8σ-讨 to lead to (c). Finally, the approximation in (5) is
used to go back from a Normal CDF to a Sigmoid in (d).
Posterior: The posterior on Wi is given by plugging (11) into (10) normalizing byPt given in (1).
Pi,t+ι(wi) = Qi,t(wi) ≈ P(Wi∣xt,yt) = P- ∙ Pi,t(wi) ∙ Sigma
Iyt (μ-i,t + χi,t Wi)
√1 + 8 σ-i,t
(12)
The approximation on the right implies matching the current true posterior with the ith component
of the approximate posterior Q(∙). It can be simplified to
「xp -
σi,t+1
(Wi - μi,t+ι) ʌ 1
--D 2	≈--------exp
2σi2,t+1	Ptσi,t
(Wi - μi,t)2∖
F- ∖.Sigma
yt(μ-i,t + χi,t Wi)
√1 + 8 σ-i,t
—
(13)
5
Under review as a conference paper at ICLR 2021
Approximations: Because the functional form of the posterior is not Gaussian, there are multiple
ways to fit a Gaussian. We review alternatives in Appendix D. However, we want to ensure that the
regions of the true posterior we are most likely to converge to at the horizon are not scaled down too
much, as this will incur additional loss. It is thus desirable to match the peak of the true posterior
with the peak of the approximation. One method is to match both the location and hight of the peak.
The other, Laplace approximation (Bishop, 2006), matches the location and curvature at the peak.
Both methods give the same approximate for μi,t+ι, but a somewhat different one for σ2,t+1.
To give μi,t+ι, We find Wi that maximizes the r.h.s. of (13), or minimizes its negative logarithm. Let
1
Pi t+ 4 Sigma	yt (μ-i,t + xi,tμi,t+)
，	∖ J + 8 σ-i,t
1 + exp -
y (μ-i,t + χi,tμi,t+ι)
q1 + 8 σ-i,t
(14)
be almost Pt in (8), except that μi,t+ι replaces μi,t and σ-讨 replaces σ2. Thus pi,t+ is the prob-
ability predicted for yt if we update μi,t and shrink as function of σ-^. The minimization gives
μi,t+1 = μi,t +
2
ytxi,tσ2,t
qi + 8 σ-i,t
• (1 -Pi,t+).
(15)
Eq. (15) can be solved iteratively, where the Newton’s method can be used, as described in Ap-
pendix C. The solution for μi,t+ι can also be expressed in terms of the r generalized Lambert W
function (Corless et al., 1996; Mezo & Baricz, 2015).
Alternatively, to avoid multiple iterations per update when using Newton’s method, we can use a
Taylor series approximation of 1 - Pi,t+ around 1 - Pi,t, where
Pi,t 4 Sigma y yt (μ-i,t + x"tμ3 ∖ = Sigma (产μ ∖ .	1⑹
∖	√1 + 8 σ-i,t )	W1 + 8 σ-i,t)
LikePi,t+, Pi,t is not Pt. Instead, it is the probability ofyt as projected by the means of the weights
att, shrunk as function of σ-2 i,t instead ofσt2. More importantly, it depens only on parameters before
the update at t + 1 is applied, giving a closed form solution. Applying first order approximation we
have
ytxi,tσi2,t (1 - Pi,t)
μi,t+1 = μi,t +	/	------「	'	T
V1 + 8σ-i,t [1 + ι+8σ-i,t y2x2,tσ2,t(1 - Pi,t) Pi,t]
(17)
If may be simpler to store the precision \/媚t in which case, (17) may be easier to compute by
normalizing both numerator and denominator by σi2,t, applying this normalization on the right term
of the denominator. Second or higher orders approximations can also be applied, but may not be
necessary, as the first order one already gives identical performance to the iterative method.
After updating μi,t+ι, we can apply (13) to update σ3t+ι. Plugging (15) in (13), we solve for σ3t+ι,
ɪ + ⅛i⅛- • Pi,t+ • (1 - Pi,t+)
σi,t	1 + 8 σ-i,t
2
σi,t+1
ptσi,t	J (μi,t+1 - μi,t) [	ptσi,t	y	yt xi,tσi,t	∕1	∖2 [ ∕iq∖
σi,t+ι =——-• exp < -Hri^^— = =——^ • exp ↑ e In 2 、• (1 - PiQ 〉. (18)
Pi,t+	2	2哈	J	Pi,t+	[2(1 + 8σ-i,t)	' J
Alternatively to (18), Laplace approximation can be used by finding the second derivative of the
negative logarithm of the posterior, giving
-1
.	(19)
The procedures described are summarized in Algorithm 1. In Appendix D, we describe several
different methods and approximations that can be used, including one that applies the same approxi-
mation steps we applied in this section without the marginalization. As empirical results, in the next
section, show, however, there is no performance advantage to applying any of the more involved
methods.
6
Under review as a conference paper at ICLR 2021
Algorithm 1 Marginalized Bayesian Gaussian Approximation
1: procedure Marginalized BAYESIAN GAUSSIAN APPROXIMATION(Parameters: μ0, σ2)
2：	∀i ∈ 1,..., d; μi,ι 一 μo, σ2 1 — σ2.
3:	for t=1,2,...,T do
4:	Get xt.
5:	Compute μt, σ2 with (6).
6:	Generate pt for yt ∈ {-1, 1} with (8).
7:	Observe yt .
8:	for i : xi,t 6= 0 do
9:	Compute pi,t Pi,t+ With (16) and (14), respectively, using μi,t+1 = μi,t for (14).
10:	Iterate on (15) and (14) with Newton's method, or use (17) to update μi,t+1.
11:	Update σi2,t+1 with either (18) or (19)
12:	end for
13:	end for
14:	end procedure
Figure 1: Rt/ logt vs. round t for different methods for randomly drawn d binary features, expected
dt/d features per example, and standard deviation of true log-odds noted in each graph.
4	Numerical Results
To benchmark regret of an algorithm, one needs the ground truth of real or loss minimizing param-
eters. On real data, the loss minimizing parameters are unknown. Furthermore, true benchmark
datasets can consist of non-stationary data, and the feature sets selected by a model one trains may
misspecify the “true” features of such a model. Thus real data may not give clean evaluation of
the proposed methods. We present results on a benchmark dataset at the end of this section, but to
measure regret performance of different algorithms, we present results on synthetic data.
Synthetic Data: We simulated data by setting d features with true log-odds weights that were drawn
randomly with some prior (or with multiple priors, each governing a subset of the features). At t,
dt d features were selected in random from the set of d features, where different rules were used
for different simulations to draw the dt features. In the fully random case, we set a random fraction
α = dt/d parameter, and each feature was activated with probability α. We either used binary
features xi,t ∈ {0, 1}, or also drew xi,t randomly in [0, 1]. For categorical features, the d features
were partitioned into categories, and for every example, one or a preset number of features from
each category were randomly selected, with different types of randomness, including fast decaying
long tail distributions. Let θ be the vector of true parameters, then, Pr(Yt = 1) was computed as
Sigma(xtT θ). The probability was used to randomly draw yt. We used Algorithm 1, as well as other
algorithms, described in Appendix D, to sequentially predict yt and update. For gradient methods,
we used Stochastic Gradient Descent (SGD) with AdaGrad scheduling (Duchi et al., 2011).
We ran grids of algorithm hyper-parameters for all algorithms to find optimal ones, and we show
results for these optimal hyper-parameters for all algorithms. Since we know the true weights, we
7
Under review as a conference paper at ICLR 2021
Table 1: Runtime and regret coefficients rT = RT/ logT for different algorithms on synthetic 200
features models with true log-odds std of 1 and algorithm parameters as in Fig 1.
Alg.	Per Example Mean: 20				Per Example Mean: 40			
	1M Examples		10M Examples		1M Examples		10M Examples	
	Time	rT	Time	rτ	Time	rT	Time	rT
SGD	-8s	117.6	58s	135.9	10s	246.3	1:18m	219.88
ADF	6s	190.3	45s	540.3	7s	178.6	53s	388.51
DimGauss	9s	78.02	1:01m	82.54	14s	97.52	1:42m	109.16
Gauss	9s	77.66	1:05m	93.85	14s	91.5	1:32m	144.28
ApproxG	9s	77.66	1:05m	93.85	12s	91.5	1:32m	144.28
VB-100	3:47m	144.22	35:25m	649.57	7:21m	202.9	1:10:25h	1125.3
VB-1000	35:16m	85.07			1:11:01h	104.31		
use them for a comparator baseline w*. Curves show progressive validation regret. At round t We
measure the cumulative regret up to t given by Rt 4 - PT=ι [log IpT + log(1 +exp(-yτ XT w*)].
Instead of showing Rt, we plot Rt/ log t. If an algorithm has logarithmic regret, the normalized
curve of the algorithm will converge to the constant. This methodology thus allows us to observe
whether an algorithm has logarithmic regret or not. Results are shown for Algorithm 1 with its
different variations (labeled by Gauss for updates with (15), and by Gauss Approx with (17)). Labels
also designate the prior used (σ0 and μo), and which variance approximation was used (G for (18)
and L for (19)). Reference results are shown for SGD, multi-dimensional Gaussian approximation
update (DimGauss) described in Appendix D.1, EP; using Assumed Density Filtering (ADF), and
marginalized VB (VBApprox), described in Appendix D.3.
Fig. 1 shows normalized regret for two different true data configurations described at the top of each
graph with random binary features. More detailed results on multiple configurations are shown in
Appendix E. Unfortunately, unlike theoretical results (Shamir, 2020), the prior has to fit the data
for all methods of Gaussian approximation for good regret. This is true for any known practical
Bayesian method, as well as for the learning parameters of SGD. However, ifwe choose a prior that
matches the true prior, regret rates logarithmic in T, usually close to the lower bounds of 0.5 logT
per parameter, are achieved on all experiments with Algorithm 1. The same results are achieved
whether we use the iterative version in (15) or its simpler approximation (17), and whether the
variance is updated by (18) or (19). Unlike Algorithm 1, both, DimGauss and ADF appear to be
optimized for priors that are different from the true one, and that depend on the fraction or number
of features that occur in each example. DimGauss requires larger prior with more features. As the
sparsity is reduced, DimGauss with its optimal prior seems to improve relative to Algorithm 1. This
is expected, as the sparsity assumption becomes less valid. Algorithm 1 outperforms both ADF and
SGD in all cases. We can find SGD hyper-parameters that seem to still exhibit logarithmic regret for
each configuration, but are inferior to Algorithm 1, with increasing gaps with more active features.
Table 1 shows both execution runtimes and regret coefficients rT =4 RT/ log T for the full simu-
lations with the algorithms and configurations in Fig 1. Simulations were run on a single Ubuntu
machine and included synthetic data generation and similar outputs for all algorithms compared.
DimGauss, VB, and ADF may have a slight advantage, as they were implemented with the Eigen
package, which is highly optimized for matrix operations. We show benchmarks for 106 and 107
examples. We observe rather equal runtimes for SGD, ADF, DimGauss and Algorithm 1, with slight
advantage to ADF, which may be due to the highly optimized matrix operations. Good regret is
obtained for both methods of Algorithm 1, but DimGauss slowly improves over Algorithm 1. This
is true because selecting 20 or 40 features out of 200 results in repetitions of co-occurrences as more
data is observed, and DimGauss utilizes these co-occurrences. We show below and also observe
from the results in Appendix E, however, that with higher degrees of sparsity, the DimGauss al-
gorithm degrades, while Algorithm 1 retains its advantages. Interestingly, the Newton method in
the Gauss algorithm does not require more time than its Taylor approximation, and both regret and
runtime are matched between the two. This is attributed to the fact that the Newton method requires
very few iterations to converge. Boldfaced in the table are the poor runtime results of the VB meth-
8
Under review as a conference paper at ICLR 2021
Figure 2: Left: Rt / log t vs. t for synthetic data with millions of long tail features. Right: Loss
relative to the best SGD on the Criteo benchmark dataset for multiple algorithms.
ods (shown with 100 and 1000 samples), whose worst-case complexity is O(dtNJT ), where N is
the number of samples and J is the maximal number of iterations. The table illustrates how runtime
increases substantially relative to all other algorithms whose complexity is O(dtT). Regret only
approaches that of Algorithm 1 with 1000 samples, and is far inferior with 100 samples.
Fig. 2 (left) shows normalized regret of the different methods for a synthetic categorical model,
with 22 categories. Category j, j = 1, 2, . . . consists of 2j features (a total of over 8M). For each
example, a single feature is randomly drawn from each category with Zipf distribution c/(n + 1)1.75,
where c is a normalizer and n is the feature index in its category. The true weights are randomly
drawn as before, with standard deviation 2. This gives a long tail distribution over features, where
for each category small indices are drawn more often, but many features from the long tail do occur
in examples. This models a realistic sparse dataset. Algorithm 1 outperforms other methods, and
the best values of the DimGauss algorithm are inferior due to the sparsity.
Criteo display advertising challenge benchmark Dataset: The right graph in Fig. 2 gives relative
percent aggregate loss performance of the Bayesian algorithms relative to the best configuration
we found for an AdaGrad SGD on the Criteo dataset1. We trained all algorithms on the over 45M
examples in this dataset, which consists of 13 integer valued features, and 26 categorial features with
different category counts. For each example, we generated a prediction, computed its log loss on the
label, and applied update. The aggregate log loss is a sum of data uncertainty and regret. The first
term is equal for all algorithms and linear in the size of the data, where the second is the regret, which
is sub-linear in the data size for a good algorithm. Thus even small noticeable percent improvements
imply possible substantial improvements of regret. Algorithm 1 achieved with only linear models
0.465 progressive validation log loss, which is better, for example, than results reported also using
deep networks in (Cheng et al., 2016). We observe advantages to the Bayesian methods over SGD,
where Algorithm 1 was superior to all methods. The DimGauss method slowly degrades relative to
the other methods due to the sparsity of some of the categorical features.
5	Conclusions
We introduced a simple Bayesian mixture diagonal Gaussian approximation method based on
marginalization for sparse online logistic regression and probit regression, that attempts to retain
the affects of a good prior around the optimal values of the weights. The method does not require
the complexities of standard Bayesian methods, as VB, but was empirically shown to achieve regret
rates as good and even better. With proper priors, empirical results were close to regret lower bounds,
and superior to other Bayesian methods also measured with their best choices of priors. With the
strong relation between regret and uncertainty, this approach gives good uncertainty estimates. The
methodology was proposed for logistic regression, and extended for probit regression, but can be
further extended to other settings. We also demonstrated a matching approach that performs high
dimensional updates, and can be used for dense problems.
1 https://www.kaggle.com/c/criteo- display- ad- challenge
9
Under review as a conference paper at ICLR 2021
References
Mohammed Abdellaoui. Parameter-free elicitation of utility and probability weighting functions.
Management science, 46(11):1497-1512, 2000.
F.	Bach. Self-concordant analysis for logistic regression. Electronic Journal ofStatistics, 4:384414,
2010.
F.	Bach. Adaptivity of averaged stochastic gradient descent to local strong convexity for logistic
regression. Journal of Machine Learning Research, 15(1):595-627, 2014.
F.	Bach and E. Moulines. Non-strongly-convex smooth stochastic approximation with convergence
rate o(1/n). In Advances in neural information processing systems, pp. 773-781, 2013.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer, New York, 2006.
David M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-
cians. Journal of the American statistical Association, 112(518):859-877, 2017.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
L.	Bottou. Large-scale machine learning with stochastic gradient descent. In COMPSTAT, 2010.
Leon Bottou. Online learning and stochastic approximations. On-line learning in neural networks,
17(9):142, 1998.
Tamara Broderick, Nicholas Boyd, Andre Wibisono, Ashia C Wilson, and Michael I Jordan. Stream-
ing variational bayes. In Advances in neural information processing systems, pp. 1727-1735,
2013.
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,
Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for recom-
mender systems. In Proceedings of the 1st workshop on deep learning for recommender systems,
pp. 7-10, 2016.
Wei Chu, Martin Zinkevich, Lihong Li, Achint Thomas, and Belle Tseng. Unbiased online active
learning in data streams. In Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining, pp. 195-203, 2011.
Bertrand S Clarke and Andrew R Barron. Jeffreys’ prior is asymptotically least favorable under
entropy risk. Journal of Statistical planning and Inference, 41(1):37-60, 1994.
R. Corless, G. Gonnet, D. Hare, D. Jeffrey, and D. Knuth. On the lambert w function. Adv. Compu-
tational Mathematics, 5:329-359, 1996.
John P Cunningham, Philipp Hennig, and Simon Lacoste-Julien. Gaussian probabilities and expec-
tation propagation. arXiv preprint arXiv:1111.6832, 2011.
Arthur P Dempster. A generalization of bayesian inference. Journal of the Royal Statistical Society:
Series B (Methodological), 30(2):205-232, 1968.
Arthur P Dempster, Nan M Laird, and Donald B Rubin. Maximum likelihood from incomplete data
via the em algorithm. Journal of the Royal Statistical Society: Series B (Methodological), 39(1):
1-22, 1977.
M.	Drmota and W. Szpankowski. Precise minimax redundancy and regrets. IEEE Trans. Inf. Theory,
IT-50:2686-2707, 2004.
J.	Drugowitsch. Variational bayesian inference for linear and logistic regression.
arXiv:1310.5438v4, 2019.
Jan Drugowitsch, Andre G Mendonca, Zachary F Mainen, and Alexandre Pouget. Learning optimal
decisions with confidence. Proceedings of the National Academy of Sciences, 116(49):24872-
24880, 2019.
10
Under review as a conference paper at ICLR 2021
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12:2121-2159, Feb. 2011.
Dylan J Foster, Satyen Kale, Haipeng Luo andMehryar Mohri, and Karthik Sridharan. Logistic
regression: The importance of being improper. In COLT - Conference on Learning Theory, 2018.
Thore Graepel, Joaquin Quinonero Candela, Thomas Borchert, and Ralf Herbrich. Web-scale
bayesian click-through rate prediction for sponsored search advertising in microsoft’s bing search
engine. Omnipress, 2010.
P. D. Grunwald. The Minimum Description Length Principle. MIT Press, 2007.
E. Hazan, T. Koren, and K. Y. Levy. Logistic regression: Tight bounds for stochastic and online
optimization. In The 27th Conference on Learning Theory, COLT 2014, pp. 197-209. MIT press,
2014.
Ralf Herbrich, Neil D Lawrence, and Matthias Seeger. Fast sparse gaussian process methods: The
informative vector machine. In Advances in neural information processing systems, pp. 625-632,
2003.
Matthew Hoffman, Francis R. Bach, and David M. Blei. Online learning for latent dirichlet alloca-
tion. In Advances in Neural Information Processing Systems 23, pp. 856-864. Curran Associates,
Inc., 2010.
John P Huelsenbeck and Fredrik Ronquist. Mrbayes: Bayesian inference of phylogenetic trees.
Bioinformatics, 17(8):754-755, 2001.
Seong Jae Hwang, Ronak R Mehta, Hyunwoo J Kim, Sterling C Johnson, and Vikas Singh.
Sampling-free uncertainty estimation in gated recurrent units with applications to normative mod-
eling in neuroimaging. In Uncertainty in Artificial Intelligence, pp. 809-819. PMLR, 2020.
Tommi S Jaakkola and Michael I Jordan. Improving the mean field approximation via the use of
mixture distributions. In Learning in graphical models, pp. 163-173. Springer, 1998.
Remi JezeqUeL Pierre Gaillard, and Alessandro Rudi. Efficient improper learning for online logistic
regression. In COLT - Conference on Learning Theory, volume 125 of Proceedings of Machine
Learning Research, pp. 2085-2108. PMLR, 09-12 Jul 2020.
S. M. Kakade and A. Y. Ng. Online bounds for bayesian algorithms. In NIPS, pp. 641-648, 2005.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision? In Advances in neural information processing systems, pp. 5574-5584, 2017.
David C Knill and Whitman Richards. Perception as Bayesian inference. Cambridge University
Press, 1996.
David A Knowles. Stochastic gradient variational bayes for gamma approximating distributions.
arXiv preprint arXiv:1509.01631, 2015.
R. E. Krichevsky and V. K. Trofimov. he performance of universal encoding. IEEE Trans. Inform.
Theory, IT-27(2):199?207, Mar. 1981.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems, pp. 6402-6413, 2017.
David Makowski, Daniel Wallach, and Marie Tremblay. Using a bayesian approach to parameter
estimation; comparison of the glue and mcmc methods. Agronomie, 22(2):191-203, 2002.
Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In Advances
in Neural Information Processing Systems, pp. 7047-7058, 2018.
Brendan Mcmahan and Matthew Streeter. No-regret algorithms for unconstrained online convex
optimization. In Advances in neural information processing systems, pp. 2402-2410, 2012.
11
Under review as a conference paper at ICLR 2021
H. B. McMahan. Follow-the-regularized-leader and mirror descent: Equivalence theorems and L1
regularization. In AISTATS, 2011.
H.	B. McMahan and M. J. Streeter. Open problem: Better bounds for online logistic regression. In
Journal of Machine Learning Research-Proceedings Track, 23, 2012.
I.	Mezo and A. Baricz. On the generalization of the lambert w function. arXiv:1408.3999v2, 2015.
Thomas P Minka. Expectation propagation for approximate bayesian inference. arXiv preprint
arXiv:1301.2294, 2013.
Thomas Peter Minka. A family of algorithms for approximate Bayesian inference. PhD thesis,
Massachusetts Institute of Technology, 2001.
Lucie Montuelle, Erwan Le Pennec, and Serge Cohen. Gaussian mixture regression model with
logistic weights, a penalized maximum likelihood approach. arXiv preprint arXiv:1304.2696,
2013.
Lucie Montuelle, Erwan Le Pennec, et al. Mixture of gaussian regressions model with logistic
weights, a penalized maximum likelihood approach. Electronic Journal of Statistics, 8(1):1661-
1695, 2014.
Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6):
47-60, 1996.
K. P. Murphy. Machine Learning A Probabilistic Perspective. MIT Press, Cambridge, Mas-
sachusetts, 2012.
Cuong V Nguyen, Thang D Bui, Yingzhen Li, and Richard E Turner. Online variational bayesian
inference: Algorithms for sparse gaussian processes and theoretical bounds. 2017a.
Cuong V Nguyen, Yingzhen Li, Thang D Bui, and Richard E Turner. Variational continual learning.
arXiv preprint arXiv:1710.10628, 2017b.
Manfred Opper and Ole Winther. A bayesian approach to on-line learning. On-line learning in
neural networks, pp. 363-378, 1998.
Francesco Orabona and David Pal. Coin betting and parameter-free online learning. In Advances in
Neural Information Processing Systems, pp. 577-585, 2016.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black Box Variational Inference. volume 33 of
Proceedings of Machine Learning Research, pp. 814-822. PMLR, 2014.
Carl Edward Rasmussen. The infinite gaussian mixture model. In Advances in neural information
processing systems, pp. 554-560, 2000.
Douglas A Reynolds, Thomas F Quatieri, and Robert B Dunn. Speaker verification using adapted
gaussian mixture models. Digital signal processing, 10(1-3):19-41, 2000.
J. Rissanen. Modeling by the shortest data description. Automatica, 14:465-471, 1978a.
J. Rissanen. Minimax codes for finite alphabets. EEE Trans. Inform. Theory, IT-24(3):389-392,
1978b.
J. Rissanen. Universal coding, information, prediction, and estimation. IEEE Trans. Inform. Theory,
IT-30(4):629-636, Jul. 1984.
J.	Rissanen. Stochastic complexity and modeling. The Annals of Statistics, 14:1080-1100, 1986.
Mrinal K Sen and Paul L Stoffa. Bayesian inference, gibbs’ sampler and uncertainty estimation in
geophysical inversion 1. Geophysical Prospecting, 44(2):313-350, 1996.
Shai Shalev-Shwartz et al. Online learning and online convex optimization. Foundations and trends
in Machine Learning, 4(2):107-194, 2011.
12
Under review as a conference paper at ICLR 2021
Gil I. Shamir. Minimum description length (MDL) regularization for online learning. In Proceedings
of the 1st International Workshop on Feature Extraction: Modern Questions and Challenges at
NIPS 2015,pp. 260-276, 2015.
Gil I Shamir. Logistic regression regret: What’s the catch? In COLT - Conference on Learning
Theory, volume 125 of Proceedings of Machine Learning Research, pp. 3296-3319, 2020.
Jack Sherman and Winifred J Morrison. Adjustment of an inverse matrix corresponding to a change
in one element of a given matrix. The Annals of Mathematical Statistics, 21(1):124-127, 1950.
Hsi Guang Sung. Gaussian mixture regression and classification. PhD thesis, 2004.
Andrew Gordon Wilson. The case for bayesian deep learning. arXiv preprint arXiv:2001.10995,
2020.
Q. Xie and A. Barron. Minimax redundancy for the class of memoryless sources. IEEE Trans.
Information Theory, pp. 647-657, 1997.
Q. Xie and A. Barron. Asymptotic minimax regret for data compression, gambling, and prediction.
IEEE Trans. Information Theory, 46:431-445, 2000.
M. Zinkevich. Online convex programming and generalized infinitesimal gradient ascent. In ICML,
pp. 928-936, 2003.
A Relation Between Gaus sian and Sigmoid
The Sigmoid function, which converts log-odds to probability is very close in shape to the Gaussian
Cumulative Distribution Function (CDF) Φ(z), as well established in the statistics literature (see,
e.g., Bishop (2006); Murphy (2012)). The derivative of the Sigmoid function is given by
dSigma(w)	ew
dw	(1 + ew )2
(20)
and equals the PDF of a 0-mean Logistic distribution. We can approximate the logistic PDF by a
Gaussian by matching the PDFs,
ew	1	w2
Lwy ≈√2∏σexp u 枳σ
Matching the distributions at W = 0 yields σ =，8/n,
ew	1 f πw2 ]	∏	1 f w2 ]
(Γ+^wP≈4expu 1^JV8∙√2πexpI ∏8卜
(21)
(22)
Thus, We can approximate the Sigmoid with a 0-mean Gaussian CDF with variance 8∕π, giving (5).
It remains to demonstrate that the PDFs (and CDFs) are close to each other not only at the peak.
Figure 3 demonstrates the approximation of the Logistic distribution (left) and the Sigmoid function
(middle) by a normal PDF and a normal CDF both with variance 8∕π, respectively. The green curve
shows the differences between the logistic/Sigmoid and the normal, which are also plotted on the
right plot at larger scale. The magnitude of the difference between the Sigmoid and the normal
CDF is bounded by 0.02 over the whole region. The differences appear asymmetric around the
origin, and are substantially small at 1/3 standard deviation from the origin, or less. While they
can accumulate over multiple examples, it appears that the most probable scenario is that positive
and negative differences over multiple examples cancel each other. Furthermore, the motivation of
a Bayesian method is to converge toward a peaked point mass, at the loss minimizing value of the
parameter. As the variance is narrowed closer to such a point mass, the approximation tends to exist
in the flat region around the origin, where the difference between the Sigmoid and the normal CDF
is very small.
13
Under review as a conference paper at ICLR 2021
Figure 3: Left: Logistic and Normal N(0, 8∕π) distributions and their differences. Middle: Sigmoid
and Normal CDF N(0, 8∕π) and their differences. Right: Differences between logistic and normal
(PDFs and CDFs).
B Probit Regression
In this appendix, we show the derivation of the method proposed in this paper for Probit Regression,
where, in a similar manner to (2), the predicted label probability with weight vector w, label yt , and
covariates xt is given by the normal CDF
p(yt|xt,w)
4 ytxtTw 1
=	eχ= exp
-∞ λ∕2π
φ(α)dα 4 Φ (ytXTW)
(23)
where, as We recall, φ(∙) and Φ(∙) are the standard Gaussian (normal) PDF and CDF, respectively.
While for logistic regression, we used a Gaussian approximation to obtain analytical expressions for
the prediction in (8) and the marginalization integral in (11), for probit regression, these are no longer
approximations. For the posterior, we will still apply a Gaussian and a diagonal approximations, as
in the derivations based on (13).
Prediction: The approach for probit regression is similar to the one described in Section 3 for
logistic regression. For each feature we track the mean μi,t and the variance σ2,t for the ith feature.
For example t, we use (6) to compute the total weight Wt, its mean μt and variance σ2. Eq. (7) gives
the self excluding weights, their means, and their variances. Similarly to (8), using the approximate
normal prior at t, we can derive the label prediction for yt ,
pt = P(yt|Xt)
(=a)
∞1
/	• exp
-∞ 2πσt
∞1
• exp
-∞ 2πσt2
(Wt - μt)2
2σ2
(Wt - μt)2
2σ2
• Φ (ytXTW) ∙ dwt
ytwt 1
•	-7^exP
-∞
(=b)
(=c)
Z ∞ ι	V 2	yt(σt v+μt) ι
∞ √2π •exp l-万 Γ ]-∞	√2πexp
I	φ(v) . Φ [yt (σtv + μt)] dv
-∞
• dz • dwt
• dz • dv
(=d)
yt μt
(24)
Φ
—
—
For (a), we use the definition of wt in (6). Step (b) follows from substituting V = (wt — μt)∕σt.
Step (c) identified the integrands as a product of the standardized N(0, 1) normal PDF multiplied
by a standardized normal CDF at yt(σtV + μt). This integral gives a normal CDF Φ ( √+b2) for
a = ytμt and b2 = ytσ2 = σ2 leading to (d).
Marginalization: Following the marginalization steps in Section 3, we can express the joint proba-
bility of weight Wi and label yt conditioned on the covariates Xt and marginalized over all the other
14
Under review as a conference paper at ICLR 2021
nonzero covariates at example t as in (10) by
p(yt,wi∣xt) = ρi,t(wi) ∙ Φ
yt(μ-i,t + χi,tWi)
q1+σ-i,t
(25)
where we use the steps of (11), excluding the approximations, to derive (25).
Posterior: The posterior on wi is given as in (12), normalizing p(yt, wi|xt) by pt from (24).
Pi,t+ι(wi) ≈ p(wi∣χt,yt) = — ∙ pi,t(wi) ∙ Φ I yt(μ-i,t + XItwi) J .	(26)
Pt	q	q^+^)
This posterior can now be matched by a normal posterior Qi,t(wi) as in (13).
Approximation: Here, we follow the Laplace approximation applied in Section 3. All other meth-
ods mentioned in the paper are also possible. To find μi,t+ι that minimizes the negative logarithm
of the r.h.s. of (26), define, similarly to (14),
zi,t+
4 yt(μ-i,t + Xi,tμi,t+ι)
q1+σ-i,t
(27)
as the probit score, which serves as the argument of the normal CDF, where the ith mean has been
updated, but all other means have not. We can now express the update of the ith mean by
ytxi,tσi2,t	φ(zi,t+ )
μi,t+1 = μi,t + qz-； ∙ E.
(28)
Eq. (28) is a similar update for probit regression to that of (15) for logistic regression, where the ratio
φ(zi,t+)∕Φ(zi,t+) replaces 1 - pit+ (and the scaling of the self excluding variance is unnecessary).
As (15), (28) must be solved iteratively because the term φ(zi,t+)∕Φ(zi,t+) is a function of μi,t+ι
through the definition of zi,t+ . As in Section 3, we can use a first order Taylor approximation of
φ(zi,t+)∕Φ(zi,t+) around its value for μi,t. Similarly to (16), we define
zi,t
4 yt(μ-i,t + Xi,tμi,t)
q1+σ-i,t
(29)
which is the score before update of all means μi,t, but unlike the one used to compute pt, normal-
ized by the ith self excluding variance σ-2 i,t instead of σt2. With some algebra, this gives a single
operation update, similar to that in (17), given by
ytxi,tσi2,tφ(zi,t)∕Φ(zi,t)
μi,t+1 =	μi,t	+	/	1	'	ʌ	r	ʌ -∣ `ɪ	.	(30)
∕ι	I rr2	1 1	I 1	%2 rr2	φ(zi,t) r I φ(zi,t)	1
V1 + σ-i,t t1 + i+σ--~tyt xi,tσi,t ∙ ΦZ^ ∙ [zi,t + Φ^∖ ʃ
The term zi,t + φ(zi,t)∕Φ(zi,t) in the denominator replaces pi,t in the logistic regression update
equation.
Taking the second derivative of the negative logarithm of the posterior and approximating 1∕σi2,t+1
by it, gives a single operation update of the variance, similarly to (19),
2	_ ( 1	,	ytxi,t	φ(ZiH) Γ , φ(zi,t+) ])	πn
σi,t+1 = iσi2; + τ+¾∙Φ⅛+)∙[zi,t+ + Φ⅛+)]∫	.	(31)
C Mean and Variance Updates
Eq. (15) gives an update for the mean μi,t+1 that cannot be solved in closed form. This is beacuse
pi,t+ is a function ofμi,t+1. However, the update is easily solvable with a few iterations of Newton’s
15
Under review as a conference paper at ICLR 2021
method. We start by plugging 啬=0] = μi,t for iteration ' = 0. We follow by computing pi0+ with
(14). The solution for μi,t+ι is the value of Wi that minimizes the negative logarithm of the r.h.s.
of (12) where ρi,t(wi) is Gaussian with mean μ%,t and variance。2t At iteration ', We can compute
p('-1) with (14), using μi'-11). The gradient of the negative logarithm of the posterior w.r.t. Wi at
Wi = μ('t+1ι) is given by
μi,t+ι - μi,t _	ytxi,t
-σ2t	√1 + 8 σ1i,t
1 - Ps+1)
The second derivative w.r.t. Wi is given by
h = 1 I y2x2,t	n('-i) (1 _ J'-1)
h' = σξ + 1!¥--； ∙ PiH ∙(1 - PiH
Then, μi,t+ι is updated by
(')—('-i)	g`
μi,t+i = μi,t+i - h
(32)
(33)
(34)
The process terminates when the difference ∣μ('j)+1 - μ('-11)I is smaller than some threshold, or a
specified number of iterations had already executed. Note that the second derivative in (33) (the
Newton Hessian step) gives the same expression as the update to the precision (inverse variance) in
(19).
Eq. (18) gives an update to the variance. It is interesting to interpret some observations from this
update. For Xi,t ≥ 0 , Pi,t+ ≥ pt. This leads to ytXi,t(μi,t+ι - μi,t) > 0 because for every yt,
μi,t+ι has to move away from μ%,t with the sign of yt. This implies that we add a positive term from
the log-odds converted to Pt to those converted into Pi,t+ and also apply less shrinkage using σ-2 i,t
instead ofσt2, yielding this claim. Hence, the coefficient of σi,t outside the exponential term in (18)
is upper bounded by 1. The argument of the exponential term is always nonnegative, which implies
that the exponential term is lower bounded by 1. However, if the self-excluding variance σ-2 i,t is
large, it makes the argument small. Similarly, if Pi,t+ is large relative to Pt it makes the whole
expression smaller. These two observations imply that if the self excluding prior is less certain, or
if the self excluding prior has a different belief about the label from the one observed overall, the
uncertainty of the current feature reduces more, because it is deemed responsible for the observation
yt. On the other hand, if the opposite holds, i.e., either the uncertainty of the self excluding prior is
low, or the self excluding prior agrees more with the observed label yt, then, feature i matters less
for the observation, and therefore, its uncertainty is not reduced as much.
D	Other Methods
In this appendix, we describe updates we perform with other Gaussian approximation methods. We
can update the mean vector and covariance of dt features that occur at t, without marginalization,
discarding off-diagonal covariance terms. This approach, that otherwise uses similar approximations
to the ones we used in Section 3, is described in Section D.1, where some of the approximation steps
are novel to this paper.
Alternatively, one can choose any two points on the true posterior and match Q(∙) on these. Least
squares can be used with several points that represent a region ofWi, for which the posterior at T is
likely to have most mass. EP and VB can be applied, discussed in Sections D.2 and D.3, respectively.
For the latter, specifically, single dimensional VB on the marginalized posterior can be applied to
minimize KL(Qi,t(Wi)||P(Wi|xt, yt)), where P(Wi|xt, yt) is the true posterior on the r.h.s. of (12)
given by P(yt, Wi|xt)/Pt, where P(yt, Wi|xt) is in (10). This still requires expensive Monte Carlo
estimation of expectations with an iterative Newton method. Without marginalization, however, a
similar VB approach would require even more expensive Monte Carlo sampling or iterative mean
field approximation EM to converge on all components.
16
Under review as a conference paper at ICLR 2021
D. 1 Multi-Dimensional Gaussian Approximation
Instead of marginalizing on all other features to update wi for which xi,t 6= 0, we can apply multi-
dimensional update on all features for which xi,t 6= 0 at round t. Such updates will enhance cor-
relation between these features, and may be a better fit to problems in which such correlation is
expected. For this update, we assume that the true posterior consists of a product between a prior
with a diagonal covariance matrix and a Sigmoid, and we apply Lapace approximation to obtain new
mean vector and covariance. With some abuse of notation, let all values at t consist only of the dt
nonzero components of xt. Let Σt be the diagonal covariance matrix, with diagonal elements σi2,t .
Let ut be the estimated mean vector at t. Then, the true posterior at t is given by
p(w|xt, yt)
1
Pt
exp {- 1 (W - Ut)T∑-1(w - u)}
P(2∏)dt l∑t∣
1
1 + exp {-ytxTw}
(35)
Similarly to (14), define
Pt+ 4 Sigma (ytxTut+ι)	(36)
as the probability of yt computed with weights after they had been updated (and this time with no
shrinkage), where ut+1 is the updated vector of means. Then, with Laplace approximation, taking
the value of the mean vector that maximizes the posterior, the mean can be updated as in (15) by
ut+1 = ut + Σtytxt (1 - Pt+) .
(37)
This is, again, an equation that must be solved either numerically, or using methods such as Newton’s
method. Again, we can assign u(t0+)1 = ut, and apply (36) on it to obtain Pt(+0). Then, at iteration `,
g` = Σ-1(u(+-υ - ut) - ytxt ∙ (1 - pt+-1))	(38)
and
H(') = ς-1 + y2χtχT ∙p(+-1) ∙(1 -pt+-1)).	(39)
Then, ut+1 is updated by
u(+ι = ut+l1) - H-1g'.	(40)
Termination is either when the update on all components of ut+1 is less than some threshold, or
after a set number of iterations. Inverting the Hessian H also gives the updated covariance Σt+1,
whose diagonal elements can be now used to update σi2,t+1, if we apply the algorithm for a sparse
problem, where it is infeasible to store all covariances.
Instead of updating h`, We can keep track of its inverse H-；, and there is no need to invert the
covariance matrix Σt. With the diagonal form of Σt, all operations can be implemented with linear
complexity in dt using the Sherman & Morrison (1950) formula, Which simplifies matrix inversions
for special matrices. For our specific need here, if A is some matrix, α some constant, and x some
vector, then, the Sherman-Morrison formula is
(A + axxT) 1 = A-1 -
A-1axxτ A-1
1 + αxτ A-1x
(41)
Substituting A = Σ-1, X = xt, and α = y2p(+-1)(1 - p(+-1)), we update H-；, inverting (39).
As in the marginalization method described in Section 3, We can avoid the iterative NeWton method
with a first order Taylor approximation of 1 - pt+ around 1 - pt, where Pt is defined in an analogy
to (16) as
Pt 4 Sigma (yxTUt)	(42)
as the un-shrunk prediction of yt at round t (which is different from pt, which is shrunk by the
variance). The approximation leads to the following set of equations to update both Ut+1 and Σt+1.
For simplification, define
vt = Σtxt .	(43)
Then, temporarily update Σt, using Sherman-Morrison formula, to
∑	= ς _ ytpt(I -Pt)£tXtXTςI = ς _	ytpt(I -Pt)VtVT
t+1	t 1 + y2Pt(1 - Pt )xt ∑txt	t 1 + y22Pt(1 - Pt)XT V
(44)
17
Under review as a conference paper at ICLR 2021
Since Σt is diagonal, the transpose on the last term of the numerator in the first equality is unneces-
sary. The second equality gives vector multiplications, showing that the complexity is linear in the
dimension of the vectors dt . (This is true also to the computation of vt when Σt is diagonal.) Next,
ut+1 can be updated
.	∕r~∖8	∕al∖
ut+ι = Ut + yt(I - ιpt)∑t+1χt.	(45)
Now, we can update pt+ in (36), using ut+1, and use it to update Σt+1 using Sherman-Morrison,
Σt+1 = Σt -
ytpt+(I - PtQvv
1 + y2Pt+(1 - Pt+)xT Vt
(46)
In the sparse case, we can now take the terms of the diagonal of Σt+1 to update σi2,t+1 of the nonzero
covariates at round t.
Finally, it may be simpler to update the precision matrix Ht+1 = Σt-+11 instead of the covariance
Σt+1. Specifically, if multiple updates are performed in a mini batch, the update applied to the
covariance cannot be applied additively. However, additive updates on the precision are valid. Thus
the updates in (44) and (46) can be replaced by
Ht+ι = Ht + y2pt(1 - Pt)XtXT
(47)
and
Ht+1 = Ht + yt2Pt+ (1 - Pt+)XtXtT	(48)
respectively. To update ut+1, we still need to invert Ht+1. We can use (44) if an update was applied
to a single round only. If a mini-batch update additively applied multiple updates at once in (47),
the updated Ht+1 must be inverted to obtain Σt+1.
The multi-dimensional approach described in this section can be applied to sparse problems, but
also to dense problems. In the dense case, the operation in (43) is no longer linear in dt , as the
covariance matrix is not necessarily diagonal. The use of Sherman-Morrison formula, however, to
invert the precision and covariance, still applies and lowers the complexity of the approach. In the
sparse problem, however, this approach may try to force correlations that are not there, that are
then ignored. As empirical results suggest, it may not be as good as the marginalization approach
because of that. Furthermore, unlike the marginalization approach in Section 3, which achieves
best performance if the true prior matches the one used to initialize the algorithm, empirical results
demonstrate that the best performances are obtained with priors that are different from the true one
with the multi-dimensional method when applied on sparse problems.
D.2 Expectation Propagation - Assumed Density Filtering
Instead of minimizing the divergence between the approximate Q and the true posterior, we can use
the expectation propagation approach, as proposed in Minka (2001), which essentially minimizes
the opposite KL divergence, and attempts to match the first two moments. More details can be found
in Minka (2001).
D.3 Marginalized Variational Bayes
Instead of using Laplace approximation or matching the location of the peak of the true posterior and
the estimated one together with either its curvature or its value, we can apply full VB, by matching
the approximate posterior Q with the true one through minimizing the KL divergence K L(Q||P) be-
tween Q, the approximate posterior, and the true posterior. This requires either iterative approaches,
such as mean field approximation EM, or Monte Carlo sampling in order to approximate expectation
over a yet unknown Q. One can apply this approach in dt dimensions as the Laplace approximation
in Subsection D.1. However, due to the inability to separate the covariates (in the Sigmoid), we
would require a power set of samples. If we use N samples per dimension, this approach would
use Ndt samples. This can be infeasible and complex if there are a large number dt of nonzero
covariates. Instead, we can use VB only in the ith dimension for each feature separately together
with the marginalization proposed in Section 3. This can be done by matching the approximate
posterior Qi,t(wi) with the posterior P(wi |Xt, yt) we obtained on the r.h.s. of (12) on feature i after
we marginalized on all other features.
18
Under review as a conference paper at ICLR 2021
The KL divergence can be decomposed into three terms; the KL divergence between Q and the prior
ρi,t, the contribution of conditioning the posterior on the probability pt predicted for yt , and the log
loss (negative log likelihood) term, emerging from the Sigmoid.
KL(Qi,t(Wi)||p(Wi|xt,yt))
=KL(Qi,tllPi,t) + EQi,t logPt + EQi,t
log [l+exp (-yt Tit+ % Wi) ∖∖
,I	∖	q1 + 8 σ-i,t H
EQi,t
l	σi,tpt	(Wi- μi,t+1)2	(Wi- μi,t产
og σ^*	2σ2,t+ι	+	2σ2,t
logll+exp(-yt (“it+ % Wi) ] ∖
i	∖	q1+8σ-i,t一
log σi,tPt - 2 + 2σ2^ [σ2,t+ι + μ2,t+ι + μ2,t - 2μi,tμi,t+ι] +
EQ i{1+eχp(- W≡ >
(49)
All expectations are w.r.t. Qi,t. The KL term can be computed in closed form, giving the second and
third equalities.
The last term on the r.h.s. of (49) cannot be analytically computed without knowledge of the posterior
Qi,t at t. Instead, We use Monte Carlo, by drawing N samples Sj 〜N(0,1), letting Wi,j =
μi,t+ι + Sjθi,t+∖. With known μi,t+ι and σ3t+ι, we can now approximate the expectation term in
(49) as
1 XXι	J	ι + eχ J -yt	M-i,t	+ χi,Mμi,t+ι + Sjσi,t+1))
N M	I [	qι+∏σ--;
where Sj is the jth randomly drawn sample. Unfortunately, μi,t+ι and σft+ι must be updated in this
step, and are not known. This requires, again, an iterative update using Newton’s method. Similarly
to (14) in Section 3, we need to define a prediction pi,j,t+ for which the prior (time t) means and
variances are used for all covariates except the ith one, and the updated μi,t+ι and σi2t+ι are used
for the ith mean and variance, respectively. This time, however, this prediction is defined N times,
uniquely for each of the jth samples
Pij t+ = Sigma yt (μ-i,t + Xi^+11 sjσi,t+I))
''	∖	√1 + 8 σ-i,t
(50)
Then, the updated mean satisfies
2
ytxi,tσi2,t
μi,t+1 = μi,t H /	=
√1 + 8 σ-i,t
1N
• N Σ2(I - pi,j,t+).
j=1
(51)
However, in order to compute both μi,t+ι and σ3t+ι,we need to apply Newton,s method, optimizing
μi,t+ι and σi,t+ι together. We start with μ(0+ι = μi,t and σ(ct+ι = σ*t. For simplicity, we omit
the iteration number (`) from the notation. The following should be read as updates at iteration `
which use P(e-t1+ for the updates. For simplicity, let at 4 ytx%,t/ Jl + (π∕8)σ-i t. Then, the joint
gradient w.r.t. μ*t and σ*t is given by
μ,+-^ - N pj=ι(1 -Pi,j,t+)
-σ⅛τ + σσ2+1 - Nt Pj=I Sj(1 -pi,j,t+)
(52)
g
19
Under review as a conference paper at ICLR 2021
The joint Hessian is given by
σ2,t+ αNt PN=I pi,j,t+(1 - pi,j,t+)
αNt PN=I sjpi,j,t+ (I - pi,j,t+)
Finally, at iteration `,
a PN=I Sjpij,t+(1 - PijH)
σ21+1 + σ2t + N PN=I Sjpij,t+(1 - PijH)
,	,	(53)
(`)
涉1
i,t+1
”('T)
μi,t+1
σ('T)
σi,t+1
- H-1g.
(54)
H
As before, the update terminates if the differences between the values of two iterations are less than
some threshold, or after a set number of iterations.
As observed, this method requires O(dtN J) operations for a single update (and O(dtNJT) oper-
ations overall), where J is the set number of Newton iterations. Empirical results demonstrate that
even with N as large as 1000, results are not as good as those with the marginalization with Laplace
approximation, presented in Section 3. We note that one can use atwo dimensional first order Taylor
approximation on pi,j,t+ around pi,j,t, which is defined similarly to pi,j,t+, with the exception of
using μi,t and σ*t instead of μi,t+ι and σi,t+ι, to obtain an approximation for updating μi,t+ι and
σi,t+1, as in (17). The approximation should be made for every j. It will, however, not require the
O(J) operations of the Newton method. The complexity is still of O(N) factor greater than that of
the method in Section 3.
E Additional Empirical Results
In this appendix, we bring a collection of simulation results demonstrating the performance of Algo-
rithm 1 and the other methods in different settings. In all the simulations we performed we observed
that Algorithm 1, with a proper prior for the setting, consistently gives regret close to the lower
bound (e.g., 0.5 logT cost for each unknown parameter). The other methods, while in some cases
exhibit performance close to that of Algorithm 1, fail to do so consistently in all conditions. The
marginalized VB approach requires very large complexity as shown in Table 1 and Fig. 1 to approach
the performance of Algorithm 1.
Fig. 4	gives a two dimensional grid of varying dt and varying true feature weights. In all these
simulations, Algorithm 1 with prior matched to the true one, gives minimal regret curves. While
SGD seems, with the right choices of parameters, to approach logarithmic regret, its regret is larger,
and increases with the true parameters’ variance and number of active features dt. Multi-dimensional
Gaussian approximation appears to be mis-calibrated on the prior, and depending on the feature
density dt tends to become better only with much larger priors. EP ADF gives larger regrets and
while appearing reasonable with higher feature density, seems to be inferior with lower feature
densities. Similar results are obtained when replacing the data generation model with models with
uniform priors on the feature weights with various ranges.
Fig. 5	shows curves for simulations with different models. On the left, features are nonbinary, and
on the right, the model consists of order of magnitude more features, where in average an order of
magnitude more features occur in each example. In both cases, Algorithm 1 persists with similar
regret rates, whereas other algorithms exhibit larger regrets. With a large number of features, both
the multi dimensional Gaussian approximation and SGD have larger regrets, although with even
lower prior / learning rates slightly better regret may be possible.
Fig. 6	demonstrates curves of the various algorithms for categorical features, where in each example
a fixed set of features are selected from each category. On the right, the selection gives exponentially
decaying distribution over the features, so that some features are selected very often while others
rarely occur. Again, regret rates behave similarly for Algorithm 1 and SGD. The multi-dimensional
Gaussian approximation completely breaks in this setting. This is because it hypothesizes correla-
tions in its updates with features that rarely reoccur.
20
Under review as a conference paper at ICLR 2021
Figure 4: Normalized Rt / log t vs. round t for various methods with randomly drawn binary fea-
tures, with d, expected dt /d, and standard deviation of true log-odds noted in each graph. Graphs
shown for d = 200, E[dt] ∈ {5, 20, 40} and true log-odds std in {1, 2, 3}.
Figure 5: Normalized Rt/ logt vs. round t for different algorithms and different data generation
models. On the left, a model with d = 200 features, out of which in average dt = 40 occur in an
example, and weight standard deviation is 2, with nonbinary feature values uniform in [0, 1]. Right:
models with d = 2000, average dt = 200, data generation standard deviation of 2, with binary
features.
21
Under review as a conference paper at ICLR 2021
Figure 6: Normalized by Rt/ logt vs. round t for categorical models and different algorithms. Left:
2000 features in 10 categories of 200 features each, where in each example 5 features from each
category are present, with true weight standard deviation of 2. On the right: A similar setting, except
that in each category features are selected with a long tail exponential distribution (prioritizing a few
features and rarely selecting others).
22