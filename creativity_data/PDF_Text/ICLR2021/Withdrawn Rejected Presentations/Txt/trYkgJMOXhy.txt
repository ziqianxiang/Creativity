Under review as a conference paper at ICLR 2021
Generative Fairness Teaching
Anonymous authors
Paper under double-blind review
Ab stract
Increasing evidences has shown that data biases towards sensitive features such as
gender or race are often inherited or even amplified by machine learning models.
Recent advancements in fairness mitigate such biases by adjusting the predictions
across sensitive groups during the training. Such a correction, however, can only
take advantage of samples in a fixed dataset, which usually has limited amount
of samples for the minority groups. We propose a generative fairness teaching
framework that provides a model with not only real samples but also synthesized
samples to compensate the data biases during training. We employ such a teaching
strategy by implementing a Generative Fairness Teacher (GFT) that dynamically
adjust the proportion of training data for a biased student model. Experimental
results indicated that our teacher model is capable of guiding a wide range of
biased models by improving the fairness and performance trade-offs significantly.
1	Introduction
Automated learning systems are ubiquitous across a wide variety of sectors. Such systems can be
used in many sensitive environments to make important and even life-changing decisions. Tra-
ditionally, decisions are made primary by human and the basis are usually highly regulated. For
example in the Equal Credit Opportunity ACts (ECOA), incorporating attributes such as race, color,
or sex into credit lending decisions are illegal in United States (Mehrabi et al., 2019). As more and
more of this process nowadays is implemented by automated learning systems instead, algorithmic
fairness becomes a topic of paramount importance. Lending (Hardt et al., 2016), hiring (Alder &
Gilbert, 2006), and educational rights (Kusner et al., 2017) are examples where gender or race bi-
ased decisions from automatic systems can have serious consequences. Even for more mechanical
tasks such as image classification (Buolamwini & Gebru, 2018), image captioning (Hendricks et al.,
2018), word embedding learning (Garg et al., 2018; Bolukbasi et al., 2016), and named co-reference
resolution (Zhao et al., 2018), algorithmic discrimination can be a major concern. As the society
relies more and more on such automated systems, algorithmic fairness becomes a pressing issue. Al-
though much of the focus of developing automated learning systems has been on the performance,
it is important to take fairness into consideration while designing and deploying the systems.
Unfortunately, state-of-the-art automated systems are usually data driven, which makes it more
likely to inherit or even amplify the biases rooted in a dataset. This is an especially serious is-
sue for deep learning and gradient based models, which can easily fit itself into the biased patterns
of the dataset. For example, in a dataset with very few female candidates being labeled as hired in
a job candidate prediction task, models might choose to give unfavorable predictions to qualified
female candidates due to their under-representations in the training data. If deployed, such a biased
predictor will deprive minority groups from acquiring the same opportunities as the others.
Much of the work in the domain of machine learning fairness has been focusing exclusively on
leveraging knowledge from samples in a dataset. One straightforward way is to adjust the distribu-
tions of the training data through pre-processing. In the job candidate prediction example above, this
means that we can either down-sample the majority class or up-sample the minority ones (Kamiran
& Calders, 2012). Another family of fairness methods aims at matching the model performance on
the majority class to that of the minority ones during training by using one of the fairness criteria
(Gajane & Pechenizkiy, 2017). Some examples of such methods includes adding regularizations
(Kamishima et al., 2012) or applying adversarial learning (Madras et al., 2018a). One issue with
these approaches is that in many cases minority groups might be heavily under-represented in the
dataset. Model training with fairness constraints will typically give up much of the performance ad-
1
Under review as a conference paper at ICLR 2021
vantages (e.g., prediction accuracies) in favor of the fairness metrics. Methods concentrate on solely
on a dataset will often find themselves difficult to maintain a good performance - fairness trade-off.
One way to make models learn beyond the dataset is to take advantage of causal reasoning (Pearl
et al., 2009), which borrows knowledge from external structures often formulated as a causal graph.
Counterfactual Fairness (Kusner et al., 2017) and Causal Fairness (Kilbertus et al., 2017) are exam-
ples of such approaches. One unique characteristic of causal fairness is the fact that they need to be
built based on a causal graph. And because those metrics are usually optimized and evaluated their
own objective, which involves a causal graph, it’s not clear how that added knowledge can be used
to benefit other more commonly used fairness criteria such as Demographic Parity and Equalized
Odds. Although it is possible to create causal structures that subsume conditional independencies
in order to benefit DP or EO, we will need those structure information to be known in advance and
we will have to derive one such structure for each metric we find. This is, what we believed, a
significant limitation of the current causal methods which we aim to improve.
In this paper, we propose a generative approach for fairness training that is capable of leveraging
both real data and “counterfactual data” generated from a causal graph. The counterfactual data is
generated in a way that alters the sensitive attribute while keeping other latent factors unchanged.
We formulate such generative model using a novel combination of adversarial training with mutual
information regularization. Next, the two types of data are organized by an architecture called the
teacher, which dynamically determines the proportion of real and counterfactual samples to train
a particular model. Our model - Generative Fairness Teacher (GFT) can be used to improve an
arbitrary fairness criteria based on need. Our experimental results indicate that we are able to take
advantage of the counterfactual generative model and make it able to achieve a significantly better
model fairness on a wide range of datasets across models. we are able to improve upon models with
different levels of biases.
2	Background
We provides a basic overview for the foundations of our method. Here we assume X to be the input
features, while A being the set of sensitive features. We define Y the be favorable outcome and Y to
be the models’ prediction of the favorable outcome given the features. The core idea of Fairness in
machine learning is to distribute those favorable outcomes evenly across each of the sensitive group
A.
2.1	Formal Fairness Criteria
There has been many existing work on fairness focusing on studying criteria to achieve algorithmic
fairness. A straightforward way to define fairness is Demographic Parity Madras et al. (2018a). In
Demographic Parity, the chances of allocating the favorable outcomes Y is the same across sensitive
groups A. Under that definition, the predictive variable Y is independent with A, making predictions
free from discrimination against sensitive groups. Note that even though A takes the form of a binary
variable, we can easily extend the definition into the case of multiple values.
Definition 1 Demographic Parity
P (Y |X = x,A = a) = P (Y |X = x,A = a0)	(1)
Other fairness criteria that are built based on input features includes include Fairness Through Un-
awareness Gajane & Pechenizkiy (2017) , and Individual Fairness Kusner et al. (2017). More re-
cently, Hardt et al. argued that criteria that only takes into account sample features making it difficult
for the algorithms to allocate favorable outcomes to the actual qualified samples in both the minority
and the majority groups. Such an observation leading to a new fairness criteria called Equalized
Odds (and its special case Equal Opportunity) Hardt et al. (2016), where the fairness statement
includes a condition on target variable Y .
Definition 2 Equalized Odds
P (Y = 1|X = x,A = a,Y = y) = P (Y = 1|X = x,A = a0,Y = y)	(2)
2
Under review as a conference paper at ICLR 2021
3	Causal Models and Counterfactual Examples
A causal model Pearl et al. (2000) is defined over a triple (U, V, F) where V is a set of observed
variables and U being a set of latent background variables. F is defined to be a set of equations for
each variable in V , Vi = fi (pai, Upai). Here pai refers to the parent of i in a causal graph. One
importance concept in causal reasoning is intervention, in which case we substitute the variable of
certain equation vi = v.
We define a counterfactual example to be a synthesized sample generated from an existing data X
by manipulating its sensitive feature from a to a0 . Here we assume that both the real sample X and
the CoUnterfactUal sample Xa-a，are generated from a latent code U.
Definition 3 Counterfactual Example
XA 一 a，(U)|X,a	(3)
3.1	Common Techniques for Fairness
Depending on when the fairness criteria are applied, methods for achieving fairness can be catego-
rized as pre-processing, in-processing and post-processing Mehrabi et al. (2019).
In-processing Techniques. In-processing techniqUes apply fairness criteria dUring the training.
Common techniqUes inclUding fairness regUlarizer Kamishima et al. (2012) and adversarial training
Madras et al. (2018a). Other methods fall into this category inclUding the redUction based method
Agarwal et al. (2018) and the more traditional discrimination approach in data miningHajian &
Domingo-Ferrer (2012).
OUr implementation applies in-processing techniqUes althoUgh oUr framework does not deal with
in-processing methods directly.
Pre-processing Techniques. Pre-processing methods applies to the models before the actUal train-
ing happens. Methods fall into this category are almost exclUsively data processing techniqUes that
aims at making the dataset free from biases. Re-sampling and re-weighting are two common tech-
niqUes of pre-processing techniqUes for fairness. Calmon et al. (2017); Kamiran & Calders (2012);
Agarwal et al. (2018). Other techniqUes inclUde that repairs biases in a database Salimi et al. (2019).
OUr method is closely related to the pre-processing techniqUes becaUse from the perspective of the
stUdent model oUr teacher model can be viewed as a data pre-processor.
Post-processing Techniques. When fairness adjUstments are applied after the training is finished,
techniqUes are called post-processing methods. Post-processing methods can be Used to adapt mod-
els with all kinds of biases levels into a fair model Madras et al. (2018b). Other recently proposed in-
clUding the method to model fairness as a score transformation problem Wei et al. (2019) and meth-
ods enforces Independence between sensitive featUres and model oUtcomes throUgh Wasserstein-1
distances Jiang et al. (2020) OUr approach is closely related to the post-processing techniqUe as oUr
teacher model can work with an arbitrarily biased stUdent model.
4	Generative Fairnes s Teaching
In this section, we propose a teaching framework for training a stUdent model that is able to work
with a wide range of fairness criteria. We first present the overview of oUr approach in section 4.1.
Then in section 4.2, we elaborate a novel generative model that can create “coUnterfactUal exam-
ples”. In section 4.4 we will show how to train sUch a teacher policy with the given stUdent model
and coUnterfactUal generative model.
4.1	Fairnes s teaching framework
Given a training dataset Dtrain = {(X = xi, Y = yi, A = ai)}|iD=t1rain|, where X and Y are ob-
served featUres and label, respectively, and A is some sensitive attribUte, we are interested in learn-
ing a predictive modelpθ(Y |X) that is parameterized by θ, sUch that it maximizes the reward on the
validation set:
R(θ) = E(χ,y)〜DvalidlogPθ(y|x) - λfcFC(pθ(Y|X), Dvalid)	(4)
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Generative Teaching Procedure
1
2
3
4
5
6
7
8
9
10
Input: initial student model pθ0 (Y|X), counterfactual generator P(X)
Input: dataset D, Teacher policy πψ
for t J 1 to episode length T do
Sample a minibatch D0 = {di = [xi, yi, ai]}Mι 〜D.
Get counterfactual data D0 = {di = [Xi 〜p(∙∣X = Xi,A = aj, yi, ai]} for each di ∈ D0.
Get current student’s state s = S(D0, pθt-1 (Y|X)) using Eq 11.
Obtain decision at 〜∏ψ(s), and get D = dd∈ ∈ D0∣a(* i) =0}u{di ∈ D0∣a(i) = l}.
Update student: θt = θt-ι + ηNθ=θt-由 P(χ,y)∈D logPθ(y|x)
end for
Return: updated student model pθT (Y|X)
where FC(∙, ∙) stands for the evaluation metric under certain fairness requirement, such as equalized
odds. This objective tries to balance between the generalization error and the fairness constraint,
which is controlled by the hyperparameter λfc .
In our teaching framework, we will teach a student predictive model pθ (Y|X) to minimize Eq 4.
Our teacher model πψ is responsible for providing proper data samples for the student at each step
of optimization to achieve this goal. In most teaching frameworks, the teacher is only responsible
for selecting proper samples from existing dataset. However, due to the potential bias in the dataset,
such assumption is too limited to achieve the fairness requirement.
Recall the definition of counterfactual example in Eq 3. Given a tuple of (U, X = x, A = a),
changing A by A J a0 while keeping U fixed will also change X . Thus the change to the predictive
1 ∙ , ∙1 , ∙	/'i`τ^	/ T T∖ ∖ ∖ t	1	, t ∖ .1	T	Fl	Z-I -γz^∖	1 z-»\
distribution p(YA.。，(U)|x, a) depends on two aspects: 1) the predictive model pθ(Y|X), and 2)
a counterfactual generative model P(X) := p(Xa-a，(U)|X = x, A = a). Suppose we have
the model P(X) ready, then it would be possible to regulate pθ(Y|X) by generate counterfactual
samples during training.
Given the teacher model ∏ψ and the counterfactual generative model P(X), We are ready to present
our iterative teaching approach for learning Pθ (Y|X) in algorithm 1. At each teaching stage, the
teacher will make binary decisions on using 1) the sample selection from the given dataset Dtrain,
or 2) the counterfactual data (X ,Y,A = a0) coming from data sample (X, Y,A = a) ∈ Dtrain and
altered by P(X). The student will then use the selected samples to perform gradient update of θ.
In the following sections, We will present how such counterfactual generative model P(X) are
learned through teaching policy πψ .
4.2 LEARNING COUNTERFACTUAL GENERATOR P(X)
Figure 1: Overview of the counterfactual generative model.
4
Under review as a conference paper at ICLR 2021
To learn the counterfactual data distribution, we first need an understanding of the empirical data
distribution. In next subsection, we first present our latent variable modeling of the data distribution:
4.2.1 Empirical data modeling
We model the empirical data distribution as p(X, A) = U p(A)p(X|U, A)p(U)dU. The design of
this graphical model follows KUsner et al. (2017), where We have the dependency U → X J A.
Here U is assumed to be independent from sensitive attribute A, and U andA will become dependent
when X is observed. The generative process of coUnterfactUal example X depends on both U and
an altered A. To learn sUch latent variable model, we optimize the following lower boUnd:
logp(X,A) ≥ Lib := Eqφq(u∣χ)[logp(A)Pφχ(X∣U,A)]
—	DklE. (UX )I∣P(U)), s.t. I (A； UX )=0
(5)
The mUtUal information constraint indicate that the posterior q(U|X) shoUld not be informative at
predicting data distribUtionp(A|X), and thUs disentangles the sensitive and insensitive latent factors.
We rewrite the mUtUal information in the following way:
Lmu(φq):= I(A；UIX)=E[logP(U(XtX)X)]= EqφqEp(AX)[logP(AIU；X)]+C ⑹
where the constant C is the entropy H (P(AIX)) as P(AIX) is the empirical data distribUtion. ThUs
sUppose we have a predictive model PφA (AIU) that is trained by minimizing:
Latt(OA) = -EX,A〜D,U〜qφq(U|X) [logPφA (AIU)
(7)
then we can address the constraint in Eq 5 Using penalty method, by minimizing Lmu w.r.t φq .
4.2.2 Counterfactual data generative modeling
As introdUced in section 4.1, the coUnterfactUal examples are generated by altering the sensitive
attribUte while keeping the latent factor U Unchanged. However we want to make sUre these samples
are also realistic, in the sense that it is close to the original data distribUtion P(X). To match these
two distribUtions, we leverage the techniqUe of WGAN (Arjovsky et al., 2017) by optimizing the
following objective:
Lwgan(P(X ), D) = mnmx EX~D D(X )] - E(X,A=a)〜D,U 〜qφq(U ∣X),X 〜P(Xa—(U)) [D(X R
(8)
where D is the discriminator in GAN. We also adopt the gradient penalty (GUlrajani et al., 2017)
Lgp with the discriminator to stabilize the training. Note that the coUnterfactUal model shares the
same decoder parameters as PφX (XIU, A) in Eq 5.
We also leverage the attribUte labels as aUxiliary tasks for D. This aUxiliary helps D better distin-
gUish between the realistic images and the generated coUnterfactUal images. Here we create another
linear layer on top of D’s last hidden layer (denoted as DA) and try to minimize:
Lcis := ɪnin —E(χ,A)〜D [logPDA(AX)] — EX〜P(XAfO(U)),ao [logPDA(A = a0|X)]	⑼
4.3	Training generative models
As oUr generative models have mUltiple constraints with entangled dependencies (see Table 1 for
sUmmary), we design the following learning paradigm that can effectively satisfy these objectives:
a)	Train the encoder qφq (UIX), decoder P(φX), and discriminator D, DA in an alternating way,
with La := Lib + Lwgan + Lcis + Lgp + LL2 (φq, φX , D, DA) where the last term is the L2
regUlarization of neUral network parameters;
b)	Train the attribUte classifier PφA with Latt + LL2 (φA);
c)	FinetUne the encoder, decoder and discriminator with Lc := La + Lmu.
IntUitively, the first step gets the generators working reasonably well in generating realistic images.
The second step learns the attribUte classifier from the latent code U, which is also a tractable task. In
5
Under review as a conference paper at ICLR 2021
Table 1: Notation of loss terms.
Notation	Objective
Llb	generative modeling of (X, A) with latent U
L mu	separating the information of A from U
Latt	learning the attribute classifier from U
Lwgan	learning the counterfactual generator
Lcls	auxiliary task for attribute prediction
Lgp, LL2	gradient penalty and L2 regularization
the last step, we address the mutual information constraint using penalty method, by minimizing the
Eq 6 together with other generative model losses. The counterfactual generator would be expected
to learn to adapt to the refined U in the last step. See Figure 1 for the visual demonstration of this
process. In practice, we can also tune the coefficients of each loss term. See the experiment section
for more information.
4.4	LEARNING TEACHER POLICY πψ
Since the student objective defined in Eq 4 is complicated, which involves with arbitrary fairness
metrics, we leverage Policy Gradient to learn the teacher model πψ . Specifically, our objective is
max ET 〜∏ψ
πψ	ψ
r(st, at)
(st,at)∈τ
(10)
where τ is the state-action trajectory sampled from the behavior πψ . Next we define the state, action
and reward in detail. The RL-based teacher act as a data loader in the iterative learning process
between teacher and student. It will feed the real or fake images to student according to current
state. Following the teacher’s instruction, the student model will have a final terminal reward on the
held-out validation set.
• state: It contains the information of student’s model and current training batch. We denote it as
S(D0,Pθ(Y|X)) = [{yi ∈ D0}, {ai ∈ D0} , -{Pθ3位)},	FC(pθ(Y|X),D0)	] (11)
V-----------
labels
{^^^^^^^^
sensitive attributes
X---------{------
cross entropy
}X
group fairness on current training batch
} 、
}
}
•	reward: The reward function defined in Eq 4, which is evaluated on held-out validation set. For
example, if the Equalized Odds is used as fairness metric, then FC(∙, ∙) = -log(∖P(Y = 1|A =
1, Y = y) - P(Y = 1|A = 0,Y = y) |). One can also define the reward for Demographic Parity
or other fairness criteria.
•	action: The teacher needs to make binary decision {am}mM=1, am ∈ {0, 1}M on the minibatch
of instances, where M is the batch size. Here 1 represent using real data, 0 represent using the
corresponding CoUnterfactUaI data sample generated from P(X).
The episode length typically equals to several epochs of the training data. We use the moving average
of the final reward as the baseline to reduce the variance. Note that other advanced RL algorithms
or techniques that handle delayed reward (Arjona-Medina et al., 2019) can also be adapted here to
further boost the performance.
5 Experiments
5.1	Tabular data
Experiment Details. We perform binary classification and fairness metric analysis on tabular data
to show the improvement of performance using Generative Fairness Teacher (GFT). Prediction per-
formance is measured by Testing Error. We choose Equalized Odds defined in Eq.2 tobe our fairness
metrics to illustrate the performance of our model. In practice one can choose to optimize an arbi-
trary metrics based on needs. In each of the experiments we evaluate the gap of Equalized Odds,
defined as
EO = P(Y = 1|X = x,A = a,Y = y) - P(Y = 1|X = x,A = a0,Y = y)	(12)
6
Under review as a conference paper at ICLR 2021
Table 2: Adult Dataset
Method	Error(%)	EO
In-processing Post-processing	16.8 16.9	0.048 0.049
Basel: all real	15.9	0.179
Base2: all fake	-15.8-	0.141
Base3: random	-16.0-	0.180
Base4: balance	-15.8-	0.157
GFT 一	16.1	0.044
Table 3: COMPAS Dataset
Method	Error(%)	EO
In-processing Post-processing	32.9 32.8	0.034 0.039
Base1: all real	32.1	0.224
Base2: all fake	-32.3-	0.231
Base3: random	-32.7-	0.247
Base4: balance	-32.0-	0.229
GFT 一	32.8	0.028
Table 4: CelebA Dataset
Method	Error(%)	EO
Base1: all real	21.0	0.426
Base2: all fake	-40-	0.610
Base3: random	-23.5-	0.417
Base4: balance	-19.6-	0.362
Base5: fix ratio	-207-	0.242
Base6: reverse	-19.2-	0.171
GFT 一	19.1	0.098
We report the maximum among the false positive difference and true positive difference between
protected and unprotected groups. We compared our method with exponentiated-gradient reduction
based in-processing algorithm (Agarwal et al., 2018) and score-based post-processing algorithm
(Hardt et al., 2016). In addition to these two methods, we also compared the GFT with four different
baselines. Base1 denotes the model that trains with all original examples, which is also an uncon-
strained classifiers. Base2 denotes the model that trains using all counterfactual examples. Base3
is the model that trains with a random combination of original and counterfactual examples. Base4
is the model trained with a balance combination of original and counterfactual examples, which
guarantees the proportion of protected and unprotected group in the training set to be the same.
We evaluate our method on two well-known tabular datasets, the ProPublica’s COMPAS recidivism
dataset and the UCI Adult income dataset. The model are trained on randomly selected 75% samples
and evaluated on the rest of 25% testing examples. We follow the same setting as in (Agarwal et al.,
2018), which uses logistic regression in scikit-learn as the classifier.
Adult Income Dataset. The Adult Income dataset contains information about individuals from
the 1994 U.S. census. There are 48,842 instances and 14 attributes, including sensitive attributes
race and sex. From Adult dataset, we select age, education number of years, relationship, race, sex,
capital-gain and hours-per-week to be the decision variables. The binary classification task here
is to predict whether an individual makes more or less than $50k per year. The results in Table
2 show that our GFT can achieve the lowest EO score with the minimum sacrifice on the testing
error compared with other fairness algorithms, achieving the best performance - fairness trade-offs.
Additionally, GFT outperforms the four baselines, indicating that our generative fairness teaching is
indeed more effective than combing the original and the counterfactual data in a mechanical way.
COMPAS Dataset. The ProPublica COMPAS dataset has a total of 7,918 instances, each with 53
features. From COMPAS dataset, we select age, race, sex, count of prior offences, charge for which
the person was arrested and COMPAS risk score to be the decision variables. The binary target
outcome is defined as whether or not the defendant recidivated within two years. The ProPublica
COMPAS dataset has a total of 7,918 data instances, each with 53 features. From COMPAS dataset,
we select age, race, sex, count of prior offences, charge for which the person was arrested and
COMPAS risk score to be the decision variables. Experimental results are illustrated in Table 3,
where one can observe the GFT is consistently better than other methods in terms of Equalized
Odds. Similar to the results in Adult income, we see that GFT achieved the best performance -
fairness trade-offs among all of the methods tested.
5.2	Image data
Experiment Details. We also evaluate our GFT on visual recognition task. In order to generate
high quality counterfactual examples, we leverage an U-Net like connections between encoder and
decoder. The adversarial classifier implemented on the latent representation of the image is a stack
of 9 convolution layers followed by fully connected layers. We follow the same baseline settings
as in section 5.1 to combine original and counterfactual images, the other two fairness algorithms
are not applicable here due to the format of image data. In addition to the four baselines, we add
two different settings. Base5 denotes the baseline model that trains on 90% counterfactual examples
and 10% original examples. Training data in Base6 is obtained by maintaining a part of the original
examples and adding counterfactual examples to reverse the original biased distribution among pro-
tected and unprotected group. The EO score we use here is the sum of the false positive difference
7
Under review as a conference paper at ICLR 2021
and true positive difference between protected and unprotected groups. In the visual recognition
task, the student model is a VGG-16 network trained using momentum SGD optimizer.
CelebA Dataset. CelebA is a commonly used large-scale face attribute dataset. There are 202,599
images, each with 40 binary attributes that reflect appearance (hair color and style, face shape,
makeup, for example), emotional state (smiling), gender, attractiveness and age. For this dataset,
we use ‘Gender’ as the binary sensitive attribute. Among the other 39 attributes, We choose one
of the most correlated attributes to 'Gender'，the 'Arched-Eyebrows' as our classification target to
make this task more challenging. As shown in Table 4, GFT reduced the EO score significantly
comparing to the baselines. We also achieved 〜0.9% improvements on testing error, outperforming
the balanced baseline.
Improving Fairness across Models with Different Bias Level. We perform a post-processing
teaching experiment on four different biased student models. A is the sensitive attribute and Y is
the classification target. We manually select four pre-training image sets according to the certain
ratio shown in Figure 2. After training on these dataset respectively, we obtained four student base
models with different unfair level. The gap of Equalized odds trend in Figure3 shows that the GFT
is capable to alleviate the unfairness of various student base models in a post-processing manner.
student 1
student 2
student 3
student 4
A=0 Y=0	A=0 Y=1
A=1 Y=0	A=1 Y=1
A=0 Y=0	A=0 Y=1
A=1 Y=0	A=1 Y=1
A=0 Y=0	A=0 Y=1
A=1 Y=0	A=1 Y=1
A=0 Y=0	A=0 Y=1
A=1Y=0	A=1Y=1
Numbα∙ oflto,ati<xι
Figure 3: Post-Processing Result
Figure 2: Different Unfair Ratio
5.3	Analysis
Counterfactual Examples. We include a qualitative evaluation of our counterfactual generator
in Fig.4. These visualizations demonstrate the difference of original images and the counterfactual
images by manipulating the binary attributes. We choose the male, young and blonde hair as the
sensitive attributes to show the effects of manipulating a specific property. One can observe that
the target attribute 'Arched-Eyebrows' in our recognition task is not visually altered between the
original example and the counterfactual one. Powered by a generative backbone, our counterfactual
examples are of high quality.
Teacher Model. We analyze the training dynamics and the teaching behavior of our GFT model
in this subsection for the CelebA dataset. We implement a policy gradient based teacher agent as
the data loader to student agent. We shoe negative log reward in Figure 5, the training reward here
is the final Equalized Odds score on the CelebA held-out validation set. After 50 episodes, the
corresponding EO score will be smaller than 0.20, the unfairness is alleviated compared with the
unconstrained baseline 0.67. Figure 6 demonstrates the action adopted by the teacher model. We
also demonstrated the percentage (moving average with sample size 7) of original image in training
the student model. As we defined in section 4.4, the teacher will make binary decision of whether to
feed original or counterfactual image to student on each iteration. Since the teacher model interacts
with student model during the training process, we observe that the percentage of using original
image is also changing. As training progresses, there has been a gradual decline in the use of
original images (and thus an increase in the counterfactual ones).
8
Under review as a conference paper at ICLR 2021
Figure 4: Examples of the CoUnterfactUal images on CelebA from the male, young and blonde_hair
attribute. These result are obtained by our counterfactual generator.
0	20	40	60	80	100	120	140
Number of Episode
Figure 5: Training dynamic
1
0	20	40	60	80
Number of Iteration
Figure 6: Teacher action
6 Conclusions
In this paper, we propose the Generative Fairness Teaching (GFT) framework to achieve algorithmic
fairness for machine learning models. Our method can generate high quality counterfactual exam-
ples, which is a novel approach to compensate the biases in a dataset. Together with a student -
teacher architecture, we dynamically adjust the proportion of counterfactual examples and mix it
with the original ones in order to train a fair model. Experimental results indicated that our method
strongly out-perform baseline methods in both tabular and real image datasets.
9
Under review as a conference paper at ICLR 2021
References
Alekh Agarwal, Alina Beygelzimer, Miroslav Dudik, John Langford, and Hanna Wallach. A reduc-
tions approach to fair classification. arXiv preprint arXiv:1803.02453, 2018.
G Stoney Alder and Joseph Gilbert. Achieving ethics and fairness in hiring: Going beyond the law.
Journal of Business Ethics, 68(4):449-464, 2006.
Jose A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes Brand-
stetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards. In Advances in
Neural Information Processing Systems, pp. 13566-13577, 2019.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is
to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances
in neural information processing systems, pp. 4349-4357, 2016.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commer-
cial gender classification. In Conference on fairness, accountability and transparency, pp. 77-91,
2018.
Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R
Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Infor-
mation Processing Systems, pp. 3992-4001, 2017.
Pratik Gajane and Mykola Pechenizkiy. On formalizing fairness in prediction with machine learning.
arXiv preprint arXiv:1710.03184, 2017.
Nikhil Garg, Londa Schiebinger, Dan Jurafsky, and James Zou. Word embeddings quantify 100
years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115
(16):E3635-E3644, 2018.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in neural information processing systems, pp.
5767-5777, 2017.
Sara Hajian and Josep Domingo-Ferrer. A methodology for direct and indirect discrimination pre-
vention in data mining. IEEE transactions on knowledge and data engineering, 25(7):1445-1459,
2012.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In
Advances in neural information processing systems, pp. 3315-3323, 2016.
Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women
also snowboard: Overcoming bias in captioning models. In European Conference on Computer
Vision, pp. 793-811. Springer, 2018.
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair
classification. In Uncertainty in Artificial Intelligence, pp. 862-872. PMLR, 2020.
Faisal Kamiran and Toon Calders. Data preprocessing techniques for classification without discrim-
ination. Knowledge and Information Systems, 33(1):1-33, 2012.
Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier
with prejudice remover regularizer. In Joint European Conference on Machine Learning and
Knowledge Discovery in Databases, pp. 35-50. Springer, 2012.
Niki Kilbertus, Mateo Rojas Carulla, Giambattista Parascandolo, Moritz Hardt, Dominik Janzing,
and Bernhard Scholkopf. Avoiding discrimination through causal reasoning. In Advances in
Neural Information Processing Systems, pp. 656-666, 2017.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In Advances
in neural information processing systems, pp. 4066-4076, 2017.
10
Under review as a conference paper at ICLR 2021
David Madras, Elliot Creager, Toniann Pitassi, and Richard Zemel. Learning adversarially fair and
transferable representations. In International Conference on Machine Learning, pp. 3384-3393,
2018a.
David Madras, Toni Pitassi, and Richard Zemel. Predict responsibly: improving fairness and accu-
racy by learning to defer. In Advances in Neural Information Processing Systems, pp. 6147-6157,
2018b.
Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram Galstyan. A survey
on bias and fairness in machine learning. arXiv preprint arXiv:1908.09635, 2019.
Judea Pearl et al. Models, reasoning and inference. Cambridge, UK: CambridgeUniversityPress,
2000.
Judea Pearl et al. Causal inference in statistics: An overview. Statistics surveys, 3:96-146, 2009.
Babak Salimi, Luke Rodriguez, Bill Howe, and Dan Suciu. Interventional fairness: Causal database
repair for algorithmic fairness. In Proceedings of the 2019 International Conference on Manage-
ment of Data, pp. 793-810, 2019.
Dennis Wei, Karthikeyan Natesan Ramamurthy, and Flavio du Pin Calmon. Optimized score trans-
formation for fair classification. arXiv preprint arXiv:1906.00066, 2019.
Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in
coreference resolution: Evaluation and debiasing methods. arXiv preprint arXiv:1804.06876,
2018.
11
Under review as a conference paper at ICLR 2021
Appendix
A Experimental Details
Encoder	Decoder	Discriminator	Adv. Classifier
ConvI [4×4, 64, 2]	DeConvI [4×4,1024, 2]	ConvI [4×4, 64, 2]	Conv1.x [3×3,512, 1]×3
Conv2 [4×4,128, 2]	DeConvI 4×4,512, 2]	Conv2 [4×4,128, 2]	Conv2.x [3×3,256,1]×3
Conv3 [4×4, 256, 2]	DeConvI [4×4, 256, 2]	Conv3 [4×4, 256, 2]	-Conv3 [4×4, 256,1]
Conv4 [4 × 4,512, 2]	DeConvI [4×4,128, 2]	Conv4 [4 × 4,512, 2]	Conv4[1× 1,128,1]一
Conv5 [4×4,1024, 2]	DeConvI [4×4,3,2] 一	Conv5 [4×4,1024, 2]	Conv5 [1x1, n_class, 1]
Table 5: Our counterfactual generative model architecture. Conv1.x, Conv2.x and Conv3.x denote
convolution units that may contain multiple convolution layers. E.g., [4×4, 64, 2]×3 denotes 3
cascaded convolution layers with 64 filters of size 4×4 and stride 2.
Generative model settings. The network architectures used in the paper are elaborated in Table 5.
There is an U-Net like connections between Encoder Conv4 layer and Decoder DeConv1 layer. Ex-
cept the Adversarial Classifier, which uses e Stochastic Gradient Descent Optimizer with momentum
0.9. The other module use the Adam Optimizer. We use batch size 64 and start with learning rate
1e - 4. We will first pre-train the Generator and Discriminator for 60 epochs and fix, then train the
Adversarial Classifier solely for 20 epochs, finally we fine-tune Generator and Discriminator again
for another 60 epochs.
Teacher and student model settings. The network architecture used in the teacher model is a three-
layer neural network with layer size d × 15 × n , where d and n represent the dimension of state
and number of action. VGG-16 is used in the face attribute classification task as the student model.
We train the teacher model for 500 episodes, within each episode, the student model is re-initialized
and trained for 20 epochs. Teacher model and student model are optimized by Adam and Stochastic
Gradient Descent Optimizer with momentum 0.9 respectively. We start with learning rate 1e - 3 for
Adam and 0.1 for momentum SGD, divide itby 10 when the performance is saturated. We use batch
size 64. The terminal reward is measured on the held-out validation set after 20 epochs of student
training, the final result is measured on the testing set.
Tabular dataset settings. For the tabular data experiments, we follow the same settings in the
original paper (Agarwal et al., 2018) and the official repositories.1 These settings also include the
standard data pre-processing steps, which convert the data into suitable format for the ML algo-
rithms. Then the data is randomly split into training and testing set in a ratio of 75% and 25%.
The only different setting is the choices of the decision variables. We use age, education number
of years, relationship, race, sex, capital-gain and hours-per-week to be the decision variables in the
Adult dataset. We use age, race, sex, count of prior offences, charge for which the person was
arrested and COMPAS risk score to be the decision variables in the COMPAS dataset.
1https://github.com/fairlearn/fairlearn
12