Under review as a conference paper at ICLR 2021
Identifying Treatment Effects under Unob-
served Confounding by Causal Representa-
tion Learning
Anonymous authors
Paper under double-blind review
Ab stract
As an important problem of causal inference, we discuss the estimation of treat-
ment effects under the existence of unobserved confounding. By representing the
confounder as a latent variable, we propose Counterfactual VAE, a new variant of
variational autoencoder, based on recent advances in identifiability of representa-
tion learning. Combining the identifiability and classical identification results of
causal inference, under mild assumptions on the generative model and with small
noise on the outcome, we theoretically show that the confounder is identifiable up
to an affine transformation and then the treatment effects can be identified. Ex-
periments on synthetic and semi-synthetic datasets demonstrate that our method
matches the state-of-the-art, even under settings violating our formal assumptions.
1	Introduction
Causal inference (Imbens & Rubin, 2015; Pearl, 2009), i.e, estimating causal effects of interven-
tions, is a fundamental problem across many domains. In this work, we focus on the estimation
of treatment effects, e.g., effects of public policies or a new drug, based on a set of observations
consisting of binary labels for treatment / control (non-treated), outcome, and other covariates. The
fundamental difficulty of causal inference is that we never have observations of counterfactual out-
comes, which would have been if we had made another decision (treatment or control). While the
ideal protocol for causal inference is randomized controlled trials (RCTs), they often have ethical
and practical issues, or are prohibitively expensive. Thus, causal inference from observational data
is indispensable, though they introduce other challenges. Perhaps the most crucial one is confound-
ing: there might be variables (called confounders) that causally affect both the treatment and the
outcome, and spurious correlation follows.
Most of works in causal inference rely on the unconfoundedness assumption that appropriate co-
variates are collected so that the confounding can be controlled by conditioning on or adjusting for
those variables. This is still challenging, due to systematic difference of the distributions of the co-
variates between the treatment and control groups. One classical way of dealing with this difference
is re-weighting (Horvitz & Thompson, 1952). There are semi-parametric methods, which have bet-
ter finite sample performance, e.g. TMLE (Van der Laan & Rose, 2011), and also non-parametric,
tree-based, methods, e.g. Causal Forests (CF) (Wager & Athey, 2018). Notably, there is a recent rise
of interest in representation learning for causal inference starting from Johansson et al. (2016).
There are a few lines of works that challenge the difficult but important problem of causal inference
under unobserved confounding. Without covariates we can adjust for, many of them assume special
structures among the variables, such as instrumental variables (IVs) (Angrist et al., 1996), proxy
variables (Miao et al., 2018), network structure (Ogburn, 2018), and multiple causes (Wang & Blei,
2019). Among them, instrumental variables and proxy (or surrogate) variables are most commonly
exploited. Instrumental variables are not affected by unobserved confounders, influencing the out-
come only through the treatment. On the other hand, proxy variables are causally connected to
unobserved confounders, but are not confounding the treatment and outcome by themselves. Other
methods use restrictive parametric models (Allman et al., 2009), or only give interval estimation
(Manski, 2009; Kallus et al., 2019).
1
Under review as a conference paper at ICLR 2021
In this work, we address the problem of estimating treatment effects under unobserved confounding.
We further discuss the individual-level treatment effect, which measures the treatment effect condi-
tioned on the covariate, for example, on a patient’s personal data. To model the problem, we regard
the covariate as a proxy variable and the confounder as a latent variable in representation learning.
Our method particularly exploits the recent advance of identifiability of representation learning for
VAE (Khemakhem et al., 2020). The hallmark of deep neural networks (NNs) might be that they
can learn representations of data. It is desirable that the learned representations are interpretable,
that is, in approximately the same relationship to the latent sources for each down-stream task. A
principled approach to this is identifiability, that is, when optimizing our learning objective w.r.t. the
representation function, only a unique optimum will be returned. Our method builds on this and
further provides the stronger identifiability of representations that is needed in causal inference.
The proposed method is also based firmly on the well-established results in causal inference. In
many works exploiting proxies, it is assumed that the proxies are independent of the outcome given
the confounder (Greenland, 1980; Rothman et al., 2008; Kuroki & Pearl, 2014). This also motivates
our method. Further, our method naturally combines a new VAE architecture with the classical result
of Rosenbaum & Rubin (1983) regarding the sufficient information for identification of treatment
effects, showing identifiability proof of both latent representations and treatment effects.
The main contributions of this paper are as follows: 1) interpretable, causal representation learning
by a new VAE architecture for estimating treatment effects under unobserved confounding; 2) theo-
retical analysis of the identifiability of representation and treatment effect; 3) experimental study on
diverse settings showing performance of state-of-the-art.
2	Related work
Identifiability of representation learning. With recent advances in nonlinear ICA, identifiabil-
ity of representations is proved under a number of settings, e.g., auxiliary task for representation
learning (Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019) and VAE (Khemakhem et al., 2020).
Recently, Roeder et al. (2020) extends the the result to include a wide class of state-of-the-art deep
discriminative models. The results are exploited in bivariate causal discovery (Wu & Fukumizu,
2020) and structure learning (Yang et al., 2020). To the best of our knowledge, this work is the first
to explore this new possibility in causal inference.
Representation learning for causal inference. Recently, researchers start to design representation
learning methods for causal inference, but mostly limited to unconfounded settings. Some methods
focus on learning a balanced covariate representation, e.g., BLR/BNN (Johansson et al., 2016),
and TARnet/CFR (Shalit et al., 2017). Adding to this, Yao et al. (2018) also exploits the local
similarity of between data points. Shi et al. (2019) uses similar architecture to TARnet, considering
the importance of treatment probability. There are also methods using GAN (Yoon et al., 2018,
GANITE) and Gaussian process (Alaa & van der Schaar, 2017). Our method adds to these by also
tackling the harder problem of unobserved confounding.
Causal inference with auxiliary structures. Both our method and CEVAE (Louizos et al., 2017)
are motivated by exploiting proxies and use VAE as a learning method. However, CEVAE assumes
a specific causal graph where the covariates should be independent of the treatment given the con-
founder. Further, CEVAE relies on the assumption that VAE can recover the true latent distribution.
Kallus et al. (2018) uses matrix factorization to infer the confounders from proxy variables, and
gives consistent ATE estimator and its error bound. Miao et al. (2018) established conditions for
identification using more general proxies, but without practical estimation method. Note that, two
active lines of works in machine learning exist in their own right, exploiting IV (Hartford et al.,
2017) and network structure (Veitch et al., 2019).
3	Setup and preliminaries
3.1	Treatment effects and confounders
Following Imbens & Rubin (2015), we begin by introducing potential outcomes (or counterfactual
outcomes) y(t), t = 0, 1. y(t) is the outcome we would observe, if we applied treatment value t.
2
Under review as a conference paper at ICLR 2021
Note that, for a unit under research, we can observe only one of y(0) or y(1), corresponding to
which factual treatment we have applied. This is the fundamental problem of causal inference.
We write expected potential outcomes, conditioned on Covariate(S) X = X as μt(x) = E(y(t)∣x =
x). The estimands in this work are the causal effects, which are Conditional Average Treatment
Effect (CATE) and Average Treatment Effect (ATE) defined by
T(x) = μι(x) - μo(x),	ATE = E(T(X))	(1)
CATE can be understood as an individual-level treatment effect, if conditioned on high dimensional
and highly diverse covariates.
In general, we need three assumptions for identification (Rubin, 2005). There should exist variable
z ∈ Rn satisfies ignorability (y(0), y(1) t|z) and positivity (∀z, t : p(t = t|z = z) > 0), and also
given the consistency of counterfactuals (y = y(t) if t = t) (See Appendix for explanations). Then,
treatment effects can be identified by:
μt(x) = E(E(y(t)∣z, X = x)) = E(E(y∣z, X = x,t = t)) = R(Rp(y∣z, x,t)ydy)p(z∣x)dz (2)
The second equality uses the three conditions. We say that strong ignorability holds when we have
both ignorability and positivity. In this work, we consider unobserved confounding, that is, we
assume the existence of confounder(s) z, satisfying the three conditions, but it is (partially)1 unob-
served.
The following theorem adapted from Rosenbaum & Rubin (1983) is central to causal inference and
we will use it for motivating and justifying our method. Such function b(z) is called a balancing
score (of z). Obviously, the propensity score e(z) := p(t = 1|z), the propensity of assigning the
treatment given z, is a balancing score (with f be the identity function).
Theorem 1 (Balancing score). Let b(z) be a function of random variable z. Then t z|b(z) if
and only if f (b(z)) = p(t = 1|z) := e(z) for some function f (or more formally, e(z) is b(z)-
measurable). Assume further that z satisfies strong ignorability, then so does b(z).
3.2	Variational autoencoders
Variational autoencoders (VAEs) (Kingma et al., 2019) are a class of latent variable models with
latent variable z, and observed variable y is generated by the decoder pθ (y|z). The variational
lower bound of the log-likelihood is written as:
logp(y) ≥ logp(y) - DκL(q(z卜)kp(z∣y))= Ez~qlogpθ(y|z) - DκL(qφ(z∣y)kp(z)), (3)
`--------------------------------------------------------------{z-----------------}
LVAE (y;6,。)
where the encoder qφ(z∣y) is introduced to approximate the true posterior P(Z卜)and DKL denotes
KL divergence. The decoder pθ and encoder qφ are usually parametrized by NNs. We will omit
the parameters θ, φ in notations when appropriate. Using the reparameterization trick (Kingma
& Welling, 2014) and optimizing the evidence lower bound (ELBO) Ey~D(L(y)) with data D,
we train the VAE efficiently. Conditional VAE (CVAE) adds a conditioning variable c to (3) (See
Appendix for details).
As mentioned, identifiable VAE (iVAE) (Khemakhem et al., 2020) provides the first identifiability
result for VAE, using auxiliary variable u. It assumes y u|z, that is, p(y|z, u) = p(y|z). The
variational lower bound is
log p(y|u)
≥ Ez~q logPf (y|z) - DκL(q(z∣y,u)kpτ,λ(ZIu))	(4)
、---------------------{----------------------}
LiV AE (y,u)
where y = f(z) + , is additive noise and z has exponential family distribution with sufficient
statistics T and parameter λ(u). Note that, unlike CVAE, the decoder does not depend on u due to
the independence assumption.
Here identifiability means that the functional parameters (f , T , λ) can be identified (learned) up to
a simple transformation.
4	VAE architecture for causal representation learning
1This allows the existence of observed confounders in x. As we will see, since z is the latent variable(s) for
VAE and is learned from covariates x by the VAE, it can contain all confounders in principle. Our method will
extract the confounding part of x into z.
3
Under review as a conference paper at ICLR 2021
The probabilistic model of our VAE follows naturally from the right
most side of (2), where the involved distribution is p(y, z|x, t) =
p(y|z, x, t)p(z|x, t). If we treat covariate x as a proxy and assume
further the conditional independence y x|z, t as in most work ex-
ploiting proxies, we have
p(y, z|x, t) = p(y|z, t)p(z|x, t)	(5)
A natural next step is to design a VAE for this joint distribution,
which learns to recover a causal representation of z. By “causal”,
we mean the representation can be used to identify or estimate treat-
ment effects. Recovering the true confounder z would be great, but
this is not required, as shown in Theorem 1, which says, for iden-
tification of treatment effects, we only need to have b(z), a causal
representation of z, which contains the information of propensity
score e(z), the part of z that is relevant to treatment assignment.
We are a step away from the VAE architecture now. Note that
(5) has similar factorization with iVAE: p(y, z|u) = p(y|z)p(z|u)
from y u|z, meaning we can use our covariate x as auxiliary vari-
Figure 1: Graphical models of
the decoders. From top: CVAE,
iVAE, and CFVAE. The encoders
for the VAEs are similar: they
take all observed variables and
build approximate posteriors.
able u in iVAE from the independence assumption of proxy variable. Further from the conditioning
on t in (5), we design a VAE architecture as a combination of CVAE and iVAE, with treatment t and
covariate x as conditioning and auxiliary variable, respectively. The ELBO can be derived as
log p(y|x, t) ≥ log p(y|x, t) - DKL(q(z|x, y, t)kp(z|x, y, t))	(6)
=Ez〜q logp(y∣z,t) - DKL(q(z∣x,y, t)kp(z∣x, t)) = LCFVAE(x, y,t).
As in iVAE, the decoder drops the dependence on x. We name this architecture the Counterfactual
VAE (CFVAE). Figure 1 depicts the relationship of CVAE, iVAE, and CFVAE.
We detail parameterization of CFVAE. The decoderpf,g(y|z, t), conditional prior ph,k(z|x, t), and
encoder qr,s(z|x, y, t) are factorized Gaussians, i.e., a product of 1-dimensional Gaussian distribu-
tions. This is not restrictive if the mean and variance are given by arbitrary nonlinear functions.
y∣z,t- Qd=I N (yj ； fj ,gj),	z∣x,t- Qn=I N(zi； hi,ki),	z|x, y,t- Qn=I N (zi； ri,Si).
(7)
θ = (f , g , h, k) and φ = (r, s) are functional parameters given by NNs which take the respective
conditional variables as inputs (e.g. h := (hi(x, t))T).
5 Identifying representation and treatment effects
In the following, we will show that CFVAE can identify the latent variable up to an affine transfor-
mation (Sec. 5.1), it can learn a balancing score as a causal representation, and its decoder is a valid
estimator for potential outcomes (Sec. 5.2).
5.1	Identifiability of representation
In this subsection, we show that CFVAE can identify latent variable z up to an element-wise affine
transformation when the noise on the outcome is small. Based on this result, we can gain insight on
how to make CFVAE learn a balancing score.
Our starting point is the following theorem showing the identifiability of our learning model, adapted
from Theorem 1 in Khemakhem et al. (2020), by adding conditioning on t.
Theorem 2. Given the family pθ (y, z|x, t) specified by (5) and (7)2, for t = 0, 1, assume
1)	ft(z) := (fi(z,t))T is injective;
2)	gt(z) = σy,t is constant (i.e. gi(z,t) = σyi,t);
3)	λt(x) := (h(x, t), k(x, t))T, which is seen as a random variable, is not degenerate.
2We specified factorized Gaussians in (7) and they show good performance in our experiments. But Corol-
lary 1 and Theorem 3 can be extended to more general exponential families, see Khemakhem et al. (2020).
4
Under review as a conference paper at ICLR 2021
Then, given t = t, the family is identifiable up to an equivalence class. That is, for t = 0, 1, if
pθt (y|x, t = t) = pθ0 (y|x, t = t)3, we have the relation between parameters
ft-1 (yt) = Atft0-1 (yt) + bt := At(ft0-1 (yt))	(8)
where p(yt) := p(y|t), At is an invertible n-square matrix and bt is a n-vector.
Similarly to Sorrenson et al. (2019), we can further show that At = diag(at) is a diagonal matrix.
By a slight abuse of symbol, we will overload | to make a shorthand for equations like (8), e.g.,
(8) can be written as f -1(y) = A(f 0-1(y))|t. Note that, by definition of inverse, we also have
f0 = f ◦ A|t.
The importance of model identifiability can be seen more clearly in the limit of small noise on
y. Corollary 1 can be easily understood by noting that after learning with small noise on y,
the encoder and decoder both degenerate to deterministic functions: in (7) g0 = s0 = 0, and
∀x, zt0 = rt0 (x, y) = ft0-1(y). Note that, we only assume the VAE learns observational distribu-
tions pθ0 (y|x, t = t) the same as the truth, but this leaves room for latent distributions different to
the truth.
Corollary 1 (Identifiability of representation). For t = 0, 1, assume 1) σy,t → 0 and 2) CFVAE
can learn a distribution pθ0 = pθt, then the latent variable z and the mean parameter ft of y can
be identified up to an element-wise affine transformation: z = A(z0)|t and f0 = f ◦ A|t.
This is a strong result for learning interpretable representation, but it is not enough for causal infer-
ence. To see this in a principled way, we recall the concept of balancing score. The recovered latent
z0 in Corollary 1 is not a balancing score, due to the different At for t = 0, 1. If z0 were a balancing
score, we would have t z|z0. However, given z0 = z0, z = diag(at)z0 + bt is a deterministic
function of t, contradicting with t z|z0. (A more concrete analysis can be found in Appendix.)
This example also suggests that we will have a balancing score if we can get rid of the dependence
on t = t. The next subsection discusses some assumptions on x to remove the “|t” in Corollary 1.
5.2	Identification of treatment effects
The following definition will be used in Theorem 3. The importance of this definition is immediate
from Theorem 1, that is, if a balancing covariate is also a function of z, then it is a balancing score.
Definition 1 (Balancing covariate). Random variable x is a balancing covariate of random variable
z ift z|x. We also simply say x is balancing (or non-balancing ifit does not satisfy this definition).
Given that a balancing score of the true confounder is sufficient for strong ignorability, a natural
and interesting question is that, does a balancing covariate of the true confounder also satisfies
strong ignorability? The answer is no. To see why, and also to better understand the significance
of Theorem 3, we give Proposition 1 indicating that a balancing covariate of the true confounder
might not even satisfy ignorability. We also refer readers to Appendix 8.5 where we examine two
important special cases of balancing covariate, one of those is noiseless proxy, which might not
satisfy positivity.
Proposition 1. Let x be a balancing covariate ofz. If z satisfies ignorability and y(0), y(1) x|z, t,
then x satisfies ignorability.
Given this proposition, we know our assumptions are weaker than ignorability, and our method
can work under unobserved confounding (x might not satisfy ignorability). Note the independence
y(0), y(1) x|z, t in this proposition implies y x|z, t assumed by CFVAE. From the decomposi-
tion rule of conditional independence, we have y(0), y(1) x|z, t =⇒ ∀t(y(t) x|z, t) =⇒
∀t, f(y(t)ɪx|z, t = t), 4 independence in total. Using only two of them, and with the consistency
of counterfactual, we have ∀t(y(t) x|z, t = t) =⇒ ∀t(y x|z, t = t) =⇒ y x|z, t. In
general, the other two independence y(0) x|z, t = 1 and y(1) x|z, t = 0 do not hold, and thus
y x|z, t =6⇒ y(0), y(1) x|z, t.
As shown in the following theorem, for any balancing covariates, CFVAE can identify treatment
effects, by learning a balancing score of the true confounder as latent representation. Note that
3θt0 = (ft0 , h0t , kt0 ) is another set of parameters giving the same distribution, which is learned by VAE. In
this paper, symbol “0” (prime) always indicates parameters (variables, etc.) learned/recovered by VAE.
5
Under review as a conference paper at ICLR 2021
assumption 3) is nontrivial, it requires that the true data generating distribution is in the learning
model. And thus identification of treatment effects follows from the identifiability of our model.
Theorem 3 (Identification with balancing covariate). Assume
1)	the same as Theorem 2 and Corollary 1;
2)	h, k in (7) depend on x but not t (i.e. hi(x, t) = hi(x) and same for k);
3)	The data generating process can be parametrized by family pθ (y, z|x, t) specified above;
4)	z satisfies strong ignorability and x is a balancing covariate of z.
Then, for both t = 0, 1, we have z = diag(a)z0 + b := A(z0), and z0 satisfies strong ignorability. We
identify the potential outcomes by μt(x) = E(E(y∣z0, X = x, t = F))= E(f^(r0 (x, yj)∣x = x).
The result may be more easily understood as following. Now with the same A for both treatment
groups t, given observation yt, the counterfactual prediction given by CFVAE is the same as truth:
y10 -t = f10-t(zt0) = f1-t ◦ A(A-1(zt)) = f1-t(zt) = y(1 - t) (Also compare this to Appendix
8.4). We can identify potential outcomes, using rt0 (x, yt) = zt0 in the last equality, by
μt(χ) = E(y ⑴|x = X) = E(yHχ = X) = E(fXr0 (χ, yt))|x = x)	⑼
Note that counterfactual assignment t = tF may or may not be the same as factual t. The algorithm
for estimation CATE and ATE is as following. After training CFVAE, we feed data D = {(x, yt) :=
(x, y, t)} into the encoder, and draw sample from it: q(z0|x = x, y = y, t = t) = δ(z0 - rt0 (x, yt))
(δ denotes delta function). Then, setting t = tF∈ {0, 1} in the decoder, feed posterior sample {zt0 =
r0(x,yt)}, We get CoUnterfactUal prediction p(y0∣z0 = Zt, t = t) = δ(y^ - fg(z0)). Finally, We
estimate ATE by taking average ED(y10 -y00 ), and CATE by E{D|x=x} (y10 -y00 ), adding conditioning
on x.
A caveat is that (9) reqUires post-treatment observation yt. Often, it is desirable that We can also
have pre-treatment prediction for a neW sUbject, With only the observation of its covariate x = x.
To this end, We Use conditional prior p(z0|x) as a pre-treatment predictor for z0: inpUt x and draW
sample from p(z0|x = x) instead of q, and all the others remain the same. Since the ELBO has a
KL term betWeen p(z0 |x) and q, the tWo distribUtions shoUld not be very different, and We Will also
have sensible pre-treatment estimation of treatment effects.
AlthoUgh oUr method Works Under Unobserved confoUnding, it still formally reqUires small oUtcome
noise and balancing covariate. HoWever, experiments shoW oUr method can Work very Well With
large oUtcome noise, and the covariates can be non-balancing and also directly affect the oUtcome,
inclUding general proxies, IVs, and even netWorked data.
6	Experiments
As in previoUs Works (Shalit et al., 2017; LoUizos et al., 2017), We report the absolUte error of ATE
ATE := |ED(y(1) - y(0)) - ED(y10 - y00 )|, and the sqUare root of empirical PEHE (Hill, 2011)
PEHE := ED((y(1) - y(0)) - (y10 - y00 ))2 for individUal-level treatment effects.
Unless otherWise indicated, for each fUnction f, g, h, k, r, s in (7), We Use a mUltilayer perceptron
(MLP) that has 3*200 hidden Units With ReLU activation, and h, k depend only on x. The Adam
optimizer With initial learning rate 10-4 and batch size 100 is employed. More details on hyper-
parameters and experimental settings are given in each experiment, and are explained in Appendix.
All experiments Use early-stopping of training by evalUating the ELBO on a validation set. We
evalUate the post-treatment performance on training and validation set jointly (This is non-trivial.
Recall the fUndamental problem of caUsal inference). The treatment and (factUal) oUtcome shoUld
not be observed for pre-treatment predictions, so We report them on a testing set.
6.1	Synthetic dataset
We generate data following (10) with z, y 1-dimensional and X 3-dimensional. μ% and σ% are
randomly generated in range (-0.2, 0.2) and (0, 0.2), respectively. The fUnctions h, k, l are lin-
ear with random coefficients. The oUtcome model is bUilt for the two treatments separately, i.e.
6
Under review as a conference paper at ICLR 2021
f (z, t) := ft (z), t = 0, 1. We generate two kinds of outcome models, depending on the type of ft:
linear and nonlinear outcome models use random linear functions and NNs with random weights,
respectively.
X 〜Q3=1 N(μi, σi); z|x 〜N(h(x), βk(x));
t|x, z 〜Bern(LogiStiC(l(x, z))); y|z, t 〜N(C—1 f (z, t), α).
(10)
We adjust the outcome and proxy noise level by α, β re-
spectively. The output of ft is normalized by Ct :=
Var{D|t=t}(ft(z)). This means we need to use 0 ≤ α < 1
to have a reasonable level of noise on y (the scales of
mean and variance are comparable). Similar reasoning
applies to z|x; outputs of h, k have approximately the
same range of values since the functions’ coefficients are
generated by the same weight initializer.
We experiment on three different causal settings (indi-
cated in Italic). To introduce x as IV, we generate
another 1-dimensional random source w in the same
way as x, and use W instead of X to generate z|w 〜
N (h(w), βk(w)). Besides taking inputs x, z in l, we con-
sider two special cases: l := l(x) (x fully satisfies ignor-
Figure 2: Plots of recovered (x) - true (y) la-
tent on the nonlinear outcome. Blue: t = 0,
Orange: t = 1. α, β = 0.4. “no.” indicates
index among the 100 random models.
ability) and l := l(z) (unobserved confounder z and non-balancing proxy x of z). Except indicated
above, other aspects of the models are specified by (10). See Appendix for graphical models of these
three cases.
In each causal setting, and with the same kind of outcome models and noise levels (α, β), we eval-
uate CFVAE and CEVAE on 100 random data generating models, with different sets of functions
f, h, k, l in (10). For each model, we sample 1500 data points, and split them into 3 equal sets for
training, validation, and testing. Both the methods use 1-dimensional latent variable in VAE. For
fair comparison, all the hyper-parameters, including type and size of NNs, learning rate, and batch
size, are the same for both the methods.
Figure 3 shows our method significantly outperforms CE-
VAE on all cases. Each method works the best under ig-
norability, as expected. The performances of our method
on IV and proxy settings match that of CEVAE under ig-
norability, showing the effective deconfounding. Figure 2
shows our method learns highly interpretable representa-
tion as an approximate affine transformation of the true
latent value. To our surprise, CEVAE is also possible
to achieve this when both noises are small, though the
quality of recovery is lower than CFVAE. The relation-
ship to the true latent is significantly obscured under IVs,
because the true latent is correlated to IVs only given t,
while we model it by p(z0|x) as required by Theorem 3.
0.0 0.2 0.4 0.6 0.8 0.0 0.2 0.4 0.6 0.8
proxy noise	y noise
T - OUrS conf.	T ■ CEVAE conf.	■ + ■ OUrS inst.
-4- Ours ig.	-4- CEVAE ig.
Figure 3: Pre-treatment √epehe on non-
linear synthetic dataset. Error bar on 100
random models. We adjust one of the noise
levels α, β in each panel, with another fixed
to 0.2. See Appendix for results on lin-
ear outcome. Results for ATE and post-
treatment are similar.
We can see our method and also CEVAE are very robust
w.r.t. both outcome and proxy noise. This may due to the
good probabilistic modeling of the noise by VAE. Still,
we can see in Appendix that the noise level affects how
well we recover the latent variable.
6.2	IHDP benchmark dataset
The IHDP dataset (Hill, 2011) is widely used to evaluate
machine learning based causal inference methods, e.g. Shalit et al. (2017); Shi et al. (2019). Here,
ignorability holds given the covariates. See Appendix for detailed descriptions. Note, however, that
this dataset violates our assumption y x|z, t, since the covariates x directly affect the outcome. To
overcome this, we add two components introduced by Shalit et al. (2017) into our method. First,
we build two outcome functions ft (z), t = 0, 1 in our learning model (7), using two separate NNs.
7
Under review as a conference paper at ICLR 2021
Second, we add to our ELBO (6) a regularization term, which is the Wasserstein distance (Cuturi,
2013) between the learned p(z0|x, t = 0) and p(z0|x, t = 1). We find higher than 1-dimensional
latent variable in CFVAE gives better results, very possibly due to the mismatched latent distribution:
the confounder race is discrete but we use Gaussian latent variable. We report results with 10-
dimensional latent variable.
As shown in Table 1, the proposed CFVAE matches the state-of-the-art methods under model mis-
specification. This robustness of VAE was also observed by Louizos et al. (2017), where they used
5-dimensional Gaussian latent variable to model a binary ground truth. And notably, without the
two additional modifications, our method has the best ATE estimation and is overall the best among
generative models (better than CEVAE and GANITE by a large margin).
Table 1: Errors on IHDP. “A/B” means pre-treatment/post-treatment prediction. The mean and std are calcu-
lated over 1000 random draws of the data generating model. *Results with the two modifications. The results
without the modifications are CATE = .21±.oι∕.17±.oι and √epehe = l.0±.05/.97±.04. Bold indicates
method(s) that are significantly better than all the others. The results of the other methods are taken from Shalit
et al. (2017), except GANITE (Yoon et al., 2018) and CEVAE (Louizos et al., 2017).
Method	TMLE	BNN	CFR	CF	CEVAE	GANITE	Ours*
ATE	NA/.30±.01	.42±.03/.37±.03	.27±.01/.25±.01	.40±.03/.l8±.01	.46±.02/.34±.01	.49±.05/.43±.05	.31±.01/.30±.01
√ePEHE	NA/5.0±.2	2.1±.1/2.2±.1	.76±.02 /.71±.02	3.8±.2/3.8±.2	2.6±.1/2.7±.1	2.4±.4/1.9±.4	.77±.02/.69±.02
6.3	Pokec social network dataset
Pokec (Leskovec & Krevl, 2014) is a real world social network dataset. We experiment on a semi-
synthetic dataset based on Pokec, which was introduced in Veitch et al. (2019), and use exactly
the same pre-processing and generating procedure. The pre-processed network has about 79,000
vertexes (users) connected by 1.3 ×106 undirected edges. The subset of users used here are restricted
to three living districts that are within the same region. The network structure is expressed by binary
adjacency matrix G. Following Veitch et al. (2019), we split the users into 10 folds, test on each fold
and report the mean and std of pre-treatment ATE predictions. We further separate the rest of users
(in the other 9 folds) by 6 : 3, for training and validation. Table 2 shows the results. Our method is
the best compared with the methods specialized for networked data. We report pre-treatment PEHE
of our method in the Appendix, while Veitch et al. (2019) does not give individual-level prediction.
Table 2: Pre-treatment ATE on Pokec. Ground truth is 1. “Unadjusted” estimates ATE by ED (y1) - ED (y0).
“Parametric” is a stochastic block model for networked data (Gopalan & Blei, 2013). “Embed-” denotes the
best alternatives given by Veitch et al. (2019). Bold indicates method(s) that are significantly better than all
the others. 20-dimensional latent variable in CFVAE works better, and its result is reported. The results of the
other methods are taken from Veitch et al. (2019).
	Unadjusted	Parametric	Embed-Reg.	Embed-IPW	Ours
Age	4.34 ± 0.05	4.06 ± 0.01	2.77 ± 0.35	3.12 ± 0.06	2.08 ± 0.32
District	4.51 ± 0.05	3.22 ± 0.01	1.75 ± 0.20	1.66 ± 0.07	1.68 ± 0.10
Join Date	4.03 ± 0.06	3.73 ± 0.01	2.41 ± 0.45	3.10 ± 0.07	1.70 ± 0.13
Each user has 12 attributes, among which district, age, or join date is used as a confounder
z to build 3 different datasets, with remaining 11 attributes used as covariate x. Treatment t and
outcome y are synthesised as following:
t 〜Bern(g(z)); y = t + 10(g(z) — 0.5) + e, e 〜N(0,1)	(11)
Note that district is of 3 categories; age and join date are also discretized into three bins.
g(z) maps these three categories and values to {0.15, 0.5, 0.85}.
Some assumptions to justify our method may not hold in this dataset. The important challenges
are 1) x obviously does not satisfy ignorability, and 2) large outcome noise exists. On the other
hand, given the huge network structure, most users can practically be identified by their attributes
and neighborhood structure, which means z can be roughly seen as a deterministic function of G, x.
Then, G, x can be, as defined by us, noiseless proxies of z (see Appendix 8.5). CFVAE is then ex-
pected to control for the confounding to a large extent and able to learn a balancing score based on
8
Under review as a conference paper at ICLR 2021
Theorem 3, if we can exploit the network structure effectively. This idea is comparable to Assump-
tions 2 and 4 in Veitch et al. (2019), which postulate directly that a balancing score can be learned
in the limit of infinite large network.
To extract information from the network structure, we use Graph Convolutional Network (GCN)
(Kipf & Welling, 2017) in conditional prior and encoder of CFVAE. A difficulty is that, the network
G and covariates X of all users are always needed by GCN, regardless of whether it is in training,
validation, or testing phase. However, the separation can still make sense if we take care that the
treatment and outcome are used only in the respective phase, e.g., (ym , tm) of a testing user m is
only used in testing. See Appendix for details.
7	Discussion
In this work, we proposed a new VAE architecture for estimating causal effects under unobserved
confounding, with theoretical analysis and state-of-the-art performance. To the best of our knowl-
edge, this is the first generative learning method that provably identifies treatment effects, without
directly assuming that the true latent variable can be recovered. It is achieved by, on the one hand,
noticing we only need the part of latent information that is correlated to treatment assignment, and,
on the other hand, exploiting the recent advances that the latent variable can be recovered up to
trivial transformations in a broad class of generative models.
Despite the formal requirement, the experiments show our method is robust to large outcome noise.
Theoretical analysis of this phenomenon is an interesting direction for future work. A related the-
oretical issue is that, while Khemakhem et al. (2020) assumes fixed distribution of noise on y, we
observed that, in most cases, allowing the noise distribution to depend on z, t improves performance.
Extending identifiability to conditional noise models is also an interesting direction.
When the latent model is misspecified (Sec. 6.2 and 6.3), our method still matches the state-of-the-
art, though we cannot see apparent relationship between recovered latent variable and the true one.
It would be nice to see the learned representation indeed preserves causal properties under model
misspecification, for example, by some causally-specialized metrics, e.g. Suter et al. (2019). Given
the fact that all nonlinear ICA based identifiability requires an injective mapping between the latent
and observed variables, theoretical extensions to discrete latent variable would be challenging.
References
Ahmed M Alaa and Mihaela van der Schaar. Bayesian inference of individualized treatment effects
using multi-task gaussian processes. In Advances in Neural Information Processing Systems, pp.
3424-3432, 2017.
Elizabeth S Allman, Catherine Matias, John A Rhodes, et al. Identifiability of parameters in latent
structure models with many observed variables. The Annals of Statistics, 37(6A):3099-3132,
2009.
Joshua D Angrist, Guido W Imbens, and Donald B Rubin. Identification of causal effects using
instrumental variables. Journal of the American statistical Association, 91(434):444-455, 1996.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in
neural information processing systems, pp. 2292-2300, 2013.
Prem K Gopalan and David M Blei. Efficient discovery of overlapping communities in massive
networks. Proceedings of the National Academy of Sciences, 110(36):14534-14539, 2013.
Sander Greenland. The effect of misclassification in the presence of covariates. American journal
of epidemiology, 112(4):564-569, 1980.
Jason Hartford, Greg Lewis, Kevin Leyton-Brown, and Matt Taddy. Deep iv: A flexible approach
for counterfactual prediction. In International Conference on Machine Learning, pp. 1414-1423,
2017.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1):217-240, 2011.
9
Under review as a conference paper at ICLR 2021
Daniel G Horvitz and Donovan J Thompson. A generalization of sampling without replacement
from a finite universe. Journal of the American statistical Association, 47(260):663-685, 1952.
AaPo Hyvarinen and Hiroshi Morioka. UnsUPervised feature extraction by time-contrastive learning
and nonlinear ICA. In Advances in Neural Information Processing Systems, pp. 3765-3773, 2016.
Aapo Hyvarinen, Hiroaki Sasaki, and Richard Turner. Nonlinear ica using auxiliary variables and
generalized contrastive learning. In The 22nd International Conference on Artificial Intelligence
and Statistics, PP. 859-868, 2019.
Guido W Imbens and Donald B Rubin. Causal inference in statistics, social, and biomedical sci-
ences. Cambridge University Press, 2015.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning rePresentations for counterfactual infer-
ence. In International conference on machine learning, PP. 3020-3029, 2016.
Nathan Kallus, Xiaojie Mao, and Madeleine Udell. Causal inference with noisy and missing covari-
ates via matrix factorization. In Advances in neural information processing systems, PP. 6921-
6932, 2018.
Nathan Kallus, Xiaojie Mao, and Angela Zhou. Interval estimation of individual-level causal effects
under unobserved confounding. In The 22nd International Conference on Artificial Intelligence
and Statistics, PP. 2281-2290, 2019.
Ilyes Khemakhem, Diederik Kingma, Ricardo Monti, and AaPo Hyvarinen. Variational autoen-
coders and nonlinear ica: A unifying framework. In International Conference on Artificial Intel-
ligence and Statistics, PP. 2207-2217, 2020.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann
LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/
abs/1312.6114.
Diederik P Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations
and TrendsR in Machine Learning, 12(4):307-392, 2019.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-suPervised
learning with deeP generative models. In Advances in neural information processing systems, PP.
3581-3589, 2014.
Thomas N. KiPf and Max Welling. Semi-suPervised classification with graPh convolutional net-
works. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,
France, April 24-26, 2017, Conference Track Proceedings. OPenReview.net, 2017. URL https:
//openreview.net/forum?id=SJU4ayYgl.
Manabu Kuroki and Judea Pearl. Measurement bias and effect restoration in causal inference.
Biometrika, 101(2):423-437, 2014.
Jure Leskovec and Andrej Krevl. SnaP datasets: Stanford large network dataset collection, 2014.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling.
Causal effect inference with deeP latent-variable models. In Advances in Neural Information
Processing Systems, PP. 6446-6456, 2017.
Charles F Manski. Identification for prediction and decision. Harvard University Press, 2009.
Wang Miao, Zhi Geng, and Eric J Tchetgen Tchetgen. Identifying causal effects with Proxy variables
ofan unmeasured confounder. Biometrika, 105(4):987-993, 2018.
Elizabeth L Ogburn. Challenges to estimating contagion effects from observational data. In Complex
Spreading Phenomena in Social Systems, PP. 47-64. SPringer, 2018.
Judea Pearl. Causality: models, reasoning and inference. Cambridge University Press, 2009.
10
Under review as a conference paper at ICLR 2021
Geoffrey Roeder, Luke Metz, and Diederik P Kingma. On linear identifiability of learned represen-
tations. arXiv preprint arXiv:2007.00810, 2020.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika,70(1):41-55,1983.
KJ Rothman, S Greenland, and TL Lash. Bias analysis. Modern epidemiology, 3, 2008.
Donald B Rubin. Causal inference using potential outcomes: Design, modeling, decisions. Journal
of the American Statistical Association, 100(469):322-331, 2005.
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: gener-
alization bounds and algorithms. In International Conference on Machine Learning, pp. 3076-
3085. PMLR, 2017.
Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment
effects. In Advances in Neural Information Processing Systems, pp. 2507-2517, 2019.
Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using
deep conditional generative models. In Advances in neural information processing systems, pp.
3483-3491, 2015.
Peter Sorrenson, Carsten Rother, and Ullrich KOthe. Disentanglement by nonlinear ica with general
incompressible-flow networks (gin). In International Conference on Learning Representations,
2019.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Raphael Suter, Djordje Miladinovic, Bernhard Scholkopf, and Stefan Bauer. Robustly disentangled
causal mechanisms: Validating deep representations for interventional robustness. In Interna-
tional Conference on Machine Learning, pp. 6056-6065. PMLR, 2019.
Mark J Van der Laan and Sherri Rose. Targeted learning: causal inference for observational and
experimental data. Springer Science & Business Media, 2011.
Victor Veitch, Yixin Wang, and David Blei. Using embeddings to correct for unobserved confound-
ing in networks. In Advances in Neural Information Processing Systems, pp. 13792-13802, 2019.
Stefan Wager and Susan Athey. Estimation and inference of heterogeneous treatment effects using
random forests. Journal of the American Statistical Association, 113(523):1228-1242, 2018.
Yixin Wang and David M Blei. The blessings of multiple causes. Journal of the American Statistical
Association, 114(528):1574-1596, 2019.
Pengzhou Wu and Kenji Fukumizu. Causal mosaic: Cause-effect inference via nonlinear ica and en-
semble method. volume 108 of Proceedings of Machine Learning Research, pp. 1157-1167, On-
line, 26-28 Aug 2020. PMLR. URL http://proceedings.mlr.press/v108/wu20b.
html.
Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae:
Structured causal disentanglement in variational autoencoder. arXiv preprint arXiv:2004.08697,
2020.
Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. Representation learn-
ing for treatment effect estimation from observational data. In Advances in Neural Information
Processing Systems, pp. 2633-2643, 2018.
Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GANITE: Estimation of individualized
treatment effects using generative adversarial nets. In International Conference on Learning Rep-
resentations, 2018. URL https://openreview.net/forum?id=ByKWUeWA-.
11
Under review as a conference paper at ICLR 2021
8	Appendix
8.1	Proofs
Proof of Corollary 1. We need the consistency4 of our VAE to learn a observational distribution
equaling to the true one in the limit of infinite data, so that the learned parameters θt0 is in the
equivalence class of θt defined by (8). This can be proved (Khemakhem et al., 2020, Theorem 4) by
assuming: 1) our VAE is flexible enough to ensure the ELBO is tight (equals to the log likelihood
of our model) for some parameters; 2) the optimization algorithm can achieve the global maximum
of ELBO (again equals to the log likelihood).
In this proof, all equations and variables should condition on t, and we omit the conditioning in
notation for convenience. In the limit of σy → 0, the decoder degenerates to a delta function:
p(y|z) = δ(y - f (z)), we have y = f(z) and y0 = f0(z0). From the consistency of VAE, y should
have the same support as y0 . For all y in the support, there exist a unique z and a unique z0 satisfy
y = f(z) = f0(z0) (use injectivity). Substitute y = f(z) into the l.h.s of (8), and y = f0(z0) into
the r.h.s, we have z = diag(a)z0 + b. The relation is one-to-one for all z, so we get z = A(z0).
Similar result for f follows.	口
Proposition 2 (Properties of conditional independence). For random variables w, x, y, z. We have
(Pearl, 2009, 1.1.55):
x	y|z	∧	x	w|y, z	=⇒	x	w, y|z (Contraction).
x	w, y|z	=⇒	x	y|w, z (Weak union).
x	w, y|z	=⇒	x	y|z (Decomposition).
Proof of Proposition 1. This proof will use the above three properties of conditional independence.
We first write our assumptions in conditional independence, as A1. t z|x (balancing covariate),
A2. w t|z (ignorability given z), and A3. w x|z, t, where w := (y(0), y(1)).
Now, from A2 and A3, using contraction, we have w x, t|z, then using weak union, we have
w t|x, z. From this last independence and A1, using contraction, we have t z, w|x. Then t w|x
follows by decomposition.	口
Proof of Theorem 2. From the proof of Theorem 1 in Khemakhem et al. (2020), we know At , bt
depend on t only through h, k. But we assume h, k do not depend on t. So we have z = A(z0) for
both t = 0, 1. From Theorem 1, z0 is a balancing score of z, and satisfies strong ignorability. Here,
we proceed a bit different from (2), we have
μt(χ) = E(E(y(t)∣z0, X = χ)) = J E(y(t)∣z0 = z0, x = x)p(z0∣x = x)dz0
=J E(y∣z0 = z0, x = x, t = t)p(z0∣x = x)dz0 = J(J p(y∣z0, x, t)ydy)p(z0∣x)dz0
(12)
Compare the rightmost side to (2), note that, there is no conditioning on t in p(z0|x), because we
use the strong ignorability given z0 (and consistency of counterfactuals) in the third equality, after
expanding the outer expectation.
From the consistency of VAE, p(y∣z0, x,t) = δ(y - ft0(z0)), and q(z0∣x,y,t) = δ(z0 - r0(x,yt))
p(z0|x, y, t) = δ(z0 - ft0-1(yt)) where (x, yt) := (x, y, t) is a data point. And p(z0|x)
Pt Ry p(z0|x, y, t)p(y, t|x)dy = Pt Ry δ(z0 - ft0-1(yt))p(y,t|x)dy. We have
μt(χ) = L (ft(z'') X Lδ(ZO
- ft0-1(yt))p(y, t|x)dy)dz0
XL ft(f'tΓ1(yt)')p(y,t∖χ)dy = EfXrt(X,yJ)IX = X)
(13)
4This is the statistical consistency of an estimator. Do not confuse with the consistency of counterfactuals.
12
Under review as a conference paper at ICLR 2021
We should note that p(z0|x, y, t) := pθ0 (z0|x = x, y = y, t = t) = pθ0 (y = y, z0|x = x, t =
t)/ pθ0 (y = y, z0|x = x, t = t)dz0 might not be equal to the truth p(z|x, y, t) := pθt (z|x =
x, y = y, t = t) (in particular it is possible that ft0 6= ft), but they are in the same equivalence class
in the sense that θt0, θt should satisfy (8).
Also note that the learning of inverse mapping ft0-1 in the encoder q is enforced by consistency
(q(z0|x, y, t) = p(z0|x, y, t)), we can just use an MLP for r0 in the encoder, and we will have rt0 =
ft0-1 if the MLP is flexible enough to contain ft0-1. Similar situations prevail when identifiability
is achieved by nonlinear ICA (Hyvarinen & Morioka, 2016; Hyvarinen et al., 2019; Khemakhem
etal.,2020).	□
8.2	On the three identification conditions
Ignorability given z means there is no correlation between factual assignment of treatment and
counterfactual outcomes given z, just as it is the case in RCT. Thus, it can be understood as un-
confoundedness given z, and z can be seen as the confounder(s) we want to control for. Positivity
says the supports of p(t = t|x = x), t = 0, 1 should be overlapped, and this ensures there are no
impossible events in the conditions after adding t = t, and the expectations can thus be estimated
from observational data. Finally, consistent counterfactuals are well defined: given assignment of
treatment t = t, the observational outcome y should take the same value as the potential outcome
y(t).
8.3	Conditional VAE
By adding a conditioning variable c (usually a class label), Conditional VAE (CVAE) (Sohn et al.,
2015; Kingma et al., 2014) can give better reconstruction of observation of each class. The varia-
tional lower bound is
logp(y∣c) ≥ Ez〜q logp(y∣z, c) - DκL(q(z∣y, c)kp(z∣c)) := LCVAE(y, c)	(14)
The conditioning on c in the prior is usually omitted, since the dependence between c and the latent
representation is also involved in the encoder q.
8.4	Identifiability of representation in sec. 5.1 is not enough
Consider how the recovered z0 would be used. For a control group (t = 0) data point (x, y, 0), the
real challenge is to predict the counterfactual outcome y(1). Taking the observation, the encoder will
output a posterior sample point z00 = f00-1 (y) = A0-1(z0) (with zero outcome noise, the encoder
degenerates to a delta function: q(z|x, y, 0) = δ(z - f00-1(y))). Then, we should do counterfactual
inference, using decoder with counterfactual assignment t = 1: y10 = f10 (z00 ) = f1 ◦ A1(A0-1(z0)).
This prediction can be arbitrary far from the truth y(1) = f1(z0), due to the difference between A1
and A0 . More concretely, this is because when learning the decoder, only the posterior sample of
the treatment group (t = 1) is fed to f10, and the posterior sample is different to the true value by the
affine transformation A1, while it is A0 for z00 .
8.5	Two special cases of balancing covariate
Definition 2 (Noiseless proxy). Random variable x is a noiseless proxy of random variable z if z is
a function of x (z = ω(x)).
Noiseless proxy is a special case of balancing covariate because if x = x is given, we know z =
ω(x) and ω is a deterministic function, then p(z|x = x) = p(z|x = x, t) = δ(z - ω(x)). Also
note that, a noiseless proxy always has higher dimensionality than z, or at least the same.
Intuitively, if the value of x is given, there is no further uncertainty about z, so the observation of x
may work equally well to adjust for confounding. But, as we will see soon, a noiseless proxy of the
true confounder does not satisfy positivity.
Definition 3 (Injective proxy). Random variable x is an injective proxy of random variable z if x is
an injective function of z (x = χ(z), χ is injective).
13
Under review as a conference paper at ICLR 2021
Injective proxy is again a special case of noiseless proxy, since, by injectivity, z = χ-1(x), i.e. z is
also a function of x.
Under this very special case, that is, if x is an injective proxy of the true confounder z, we finally
have x is a balancing score and satisfies strong ignorability, since x is a balancing covariate and
a function of z. To see this in another way, let f = e ◦ χ-1 and b = χ in Theorem 1, then
f (x) = f (b(z)) = e(z). By strong ignorability of x, (2) has a simpler counterpart μt(x) =
E(y(t)|x = x) = E(y|x = x, t = t). Thus, a regression of y on (x, t) will give a valid estimator of
CATE and ATE.
However, a noiseless but non-injective proxy is not a balancing score, in particular, positivity might
not hold. Here, a simple regression will not do. This is exactly because ω is non-injective, hence
multiple values of x that cause non-overlapped supports of p(t = t|x = x), t = 0, 1 might be
mapped to the same value of z. An extreme example would be t = I(x > 0), z = |x|. We can see
p(t = t|x) are totally non-overlapped, but ∀t, z 6= 0 : p(t = t|z = z) = 1/2.
8.6	Details and additional results for experiments
8.6.1	Synthetic data
Figure 4: Graphical models for generating synthetic datasets. From left: IV, ignorability given x, and non-
balancing proxy x. Note that in the latter two cases, reversing the arrow between x, z does not change any
independence relationships, and causal interpretations of the graphs remain the same.
Interestingly, linear outcome models seem harder for both
methods, maybe because the two true linear outcome
models for t = 0, 1 are more similar, and it is harder to
distinguish and learning outcome models. Note that after
generating the outcomes and before the data is used, we
normalize the distribution of ATE of the 100 generating
models, so the errors on linear and nonlinear settings are
basically comparable.
You can find more plots for latent recovery at the end of
the paper.
8.6.2	IHDP
IHDP is based on an RCT where each data point repre-
sents a child with 25 features about their birth and moth-
Linear	Linear
—Ours ig. "⅛ ■ CEVAE conf. ■ O ■ Ours inst.
+ Ours conf. T- CEVAE ig.
Figure 5: √6pehe on linear synthetic
dataset. Error bar on 100 random models.
We adjust one of α, β at a time. Results for
ATE and post-treatment are similar.
ers. Race is introduced as a confounder by artificially removing all treated children with nonwhite
mothers. There are 747 subjects left in the dataset. The outcome is synthesized by taking the co-
variates (features excluding race) as input, hence ignorability holds given the covariates. Following
previous work, we split the dataset by 63:27:10 for training, validation, and testing.
8.6.3	Pokec
GCN takes the network matrix G and the whole covariates matrix X := (x1T, . . . , xTM)T, where M
is user number, and outputs a representation matrix R, again for all users. During training, we select
the rows in R that correspond to users in training set. Then, treat this training representation matrix
as if it is the covariates matrix for a non-networked dataset, that is, the downstream networks in
conditional prior and encoder are the same as in the above two experiments, but take (Rm,:)T where
xm was expected as input. And we have respective selection operations for validation and testing.
14
Under review as a conference paper at ICLR 2021
We can still train CFVAE including GCN by Adam, simply setting the gradients of non-seleted rows
of R to 0.
Note that GCN cannot be trained using mini-batch, instead, we perform batch gradient decent using
full dataset for each iteration, with initial learning rate 10-2. We use dropout (Srivastava et al.,
2014) with rate 0.1 to prevent overfitting.
The pre-treatment √∈pehe for Age, District, and Join date ConfoUnders are 1.085, 0.686,
and 0.699 respectively, practically the same as the ATE errors.
8.7	Additional plots on synthetic datasets
See next pages.
15
Under review as a conference paper at ICLR 2021
Figure 6: Plots of recovered-true latent under unobserved confounding. Rows: first 10 nonlinear random
models, columns: proxy noise level.
16
Under review as a conference paper at ICLR 2021
Figure 7: Plots of recovered-true latent under unobserved confounding. Rows: first 10 nonlinear random
models, columns: outcome noise level.
17
Under review as a conference paper at ICLR 2021
Figure 8: Plots of recovered-true latent when ignorability holds. Rows: first 10 nonlinear random models,
columns: proxy noise level.
18
Under review as a conference paper at ICLR 2021
Figure 9: Plots of recovered-true latent when ignorability holds. Rows: first 10 nonlinear random models,
columns: outcome noise level.
19
Under review as a conference paper at ICLR 2021
Figure 10: Plots of recovered-true latent when ignorability holds. Conditional prior depends on t. Rows: first
10 nonlinear random models, columns: outcome noise level. Compare to the previous figure, we can see the
transformations for t = 0, 1 are not the same, confirming our Theorem 3.
20
Under review as a conference paper at ICLR 2021
Figure 11: Plots of recovered-true latent on IVs. Rows: first 10 nonlinear random models, columns: outcome
noise level.
21