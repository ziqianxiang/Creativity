Under review as a conference paper at ICLR 2021
Online Learning under Adversarial Corrup-
TIONS
Anonymous authors
Paper under double-blind review
Ab stract
We study the design of efficient online learning algorithms tolerant to adversarially
corrupted rewards. In particular, we study settings where an online algorithm
makes a prediction at each time step, and receives a stochastic reward from the
environment that can be arbitrarily corrupted with probability E ∈ [0,1). Here
is the noise rate the characterizes the strength of the adversary. As is standard
in online learning, we study the design of algorithms with small regret over a
period of time steps. However, while the algorithm observes corrupted rewards,
we require its regret to be small with respect to the true uncorrupted reward
distribution. We build upon recent advances in robust estimation for unsupervised
learning problems to design robust online algorithms with near optimal regret in
three different scenarios: stochastic multi-armed bandits, linear contextual bandits,
and Markov Decision Processes (MDPs) with stochastic rewards and transitions.
Finally, we provide empirical evidence regarding the robustness of our proposed
algorithms on synthetic and real datasets.
1 Introduction
The study of online learning algorithms has a rich and extensive history (Slivkins, 2019). An online
learning algorithm makes a sequence of predictions, one per time step and receives reward. The
predictions could involve picking an expert from a given set of experts, or picking an action from a set
of available actions as in reinforcement learning settings. The goal of the algorithm is to maximize
the long term reward resulting from the sequence of predictions made. The performance of such
an algorithm is measured in terms of the regret, i.e., the difference in the total reward accumulated
by the algorithm and the total reward accumulated by the best expert/action/policy in hindsight.
Various online learning models have been studied in the literature depending on whether the rewards
are generated i.i.d. from some distribution (Gittins, 1979; Thompson, 1933) or are arbitrary (Auer
et al., 2001), and whether the reward for all actions is observed at each time step (full information
setting) (Littlestone & Warmuth, 1994) vs. observing only the reward of the chosen action (bandit
setting) (Auer et al., 2002; 2001).
In this work, we initiate the study of online learning algorithms with adversarial reward corruptions.
Specifically, we focus on the case where the generated rewards are infrequently masked by an
adversary and replaced with potentially unbounded corruptions. In doing so, we develop safeguards
that minimize the impact of such corruptions on control algorithms operating in an online setting. For
example, consider a reinforcement learning agent that interacts with the real world environment to
learn a near optimal policy mapping states to actions. For a given state-action pair (s, a) while the true
reward distribution may be stochastic, the observed reward associated with (s, a) will have inherent
errors due to real world constraints. We would still like to have online learning algorithms that are
robust to these errors and have small regret when compared to the true reward distribution. These
considerations are important in many applications such as routing, dynamic pricing, autonomous
driving and algorithmic trading. For example, a classic problem in routing involves choosing the best
route in the presence of noise in the ETA estimation for any given route. Similarly, dynamic pricing
algorithms need to be robust to adversarial spikes in demand that may lead to unwanted price surges.
To formally study the above scenarios, we consider an online algorithm that proceeds sequentially,
where in each step it makes a prediction and receives a reward. With probability E the observed
reward is adversarially corrupted. More specifically, we take inspiration from Huber’s contamination
1
Under review as a conference paper at ICLR 2021
model that has been successfully applied to study various robust estimation problems in unsupervised
learning (Huber, 2011; Tukey, 1975; Chen et al., 2018; Lai et al., 2016; Diakonikolas et al., 2019a).
Assuming that P is the true distribution of the rewards, in our model the reward at each step is
generated from (1 - E)P + EQ where Q is an arbitrary distribution. Here e < 2 is the noise rate.
Under this model we design online algorithms with near optimal regret, scaling with , for three
important cases: 1) multi-armed stochastic bandits, 2) linear contextual bandits, and 3) learning in
finite state MDPs with stochastic rewards.
Overview of results. We first consider the setting of stochastic multi-armed bandits. In this scenario
there are k arms numbered 1, 2, . . . k. In the standard multi-armed bandit model, at each time step,
the algorithm can pull arm i and get a real valued reward ri generated from a normal distribution with
mean μi and variance σ2.1 We let i* represents the best arm, that is, μi* ≥ μi, ∀i. In the E-corrupted
model we assume that the reward for pulling arm i at time t is rt 〜(1 — E)N(μi, σ2) + eQt, where
Qt is an arbitrary distribution chosen by an adversary. We assume that the adversary has complete
knowledge of the sequence of predictions and rewards up to time t - 1 as well as the true mean
rewards and any internal state of the algorithm. Over T time steps, the pseudo-regret of an algorithm
A that pulls arms (i1, i2 , . . . , iT ) is defined as
T
RegA = μi*∙T - E[Xri」.	(I)
t=1
Notice that while the adversary masks the true rewards with the corrupted ones, we still measure
the overall performance with respect to the true reward distribution. While this setting has been
studied in recent works (Lykouris et al., 2018; Gupta et al., 2019; Kapoor et al., 2019) they either
assume corruptions of bounded magnitude, or provide sub-optimal performance guarantees (see the
discussion in Section A). In particular, we provide the following near-optimal regret guarantee based
on a robust implementation of the UCB algorithm (Auer et al., 2002).
Theorem 1 (Informal Theorem). For the E adversarially corrupted stochastic multi armed bandit
problem, there is an efficient robust online algorithm that achieves a pseudo regret bounded by
O)(σ√kT) + O(σeT).
The first term in the above bound is the optimal worst case regret bound achievable for the standard
stochastic multi armed bandit setting (Auer et al., 2002). The second term denotes the additional
regret incurred due to the corruptions. Furthermore, the work of Kapoor et al. (2019) showed that
additional σET penalty is unavoidable in the worst case, thereby making the above guarantee optimal
up to a constant factor. Furthermore, as in the case of stochastic bandits with no corruptions, we can
also obtain instance wise guarantees where the first term above depends on logarithmically inT and
on an instance dependent quantity that captures how fare off are the arms as compared to the best one.
See Appendix B for details.
Next we consider the case of contextual stochastic bandits. We study adversarially corrupted linear
contextual bandits. In the standard setting of linear contextual bandits (Li et al., 2010) there are k
arms with k associated (unknown) mean vectors μ1,..., μk ∈ Rd. At time t, the online algorithm
sees k context vectors xt1, . . . xtk ∈ Rd, one per arm. If the algorithm pulls arm i then the reward is
generated from the distribution N(μ↑ ∙ xit,σ2). In the corrupted setting, we allow at certain time
steps, the rewards to be corrupted by an adversary. In particular, we assume that the context vectors
are drawn i.i.d. from N(0, I). Given the true context xit, as in the stochastic bandits setting we let
the observed reward be generated from rt 〜(1 - E)N(μ* ∙ xit,σ2) + EQt where Qt is an arbitrary
distribution. Given a sequence of arm pulls (i1, . . . , iT) we define the pseudo-regret of an algorithm
as
TT
RegA = XEmaXμ,i ∙ χt] - EXrd.	⑵
t=1	t=1
In the above definition, the expectation is again taken over the distribution of contexts, the stochastic
rewards and the internal randomness of the algorithm. For this case we provide the following near
optimal regret guarantee
1For simplicity we assume that the variance for each arm distribution is the same. Our results can also be
easily extended to handle different variance and also handle the more standard setting where the true rewards are
bounded in [0, 1].
2
Under review as a conference paper at ICLR 2021
Theorem 2	(Informal Theorem). For the adversarially corrupted linear contextual multi armed
bandit problem, there is an efficient robust online algorithm that achieves a pseudo regret bounded by
O(σd√dk log(T)√T) + O(σe√dlog(1∕e)T).
As in the case of stochastic bandits, the first term is simply the regret bound for the standard linear
contextual bandits problem achieved by the LinUCB algorithm (Chu et al., 2011). The second term is
the additional penalty due to the corruptions and is off from the lower bound of e√dT by a log(1∕e)
factor. Theorem 2 is proved in Appendix C
Finally, we consider the most general setting of learning in Markov Decision Processes (MDPs)
under corruptions. We consider a Markov Decision Process (MDP) with state space S, action space
A and transition probabilities specified by P. If an action a ∈ A is taken from state s ∈ S, then the
next state distribution is specified as p(s0|s, a). Moreover a stochastic reward rs,a is received where
rs,a 〜N(μs,a, σ2). The parameters μs,a and the transition probabilities are unknown to the learning
agent. We assume that the agent start in a fixed state s1 ∈ S and given a policy π : S → A, follows
the trajectory (s1, π(s1)), (s2, π(s2)), . . . , (sT, π(sT)). The total reward accumulated over T time
steps equals
T
Rπ =	rst,π(st).	(3)
t=1
Let ∏* be the optimal policy defined as ∏* = arg max∏ E[R∏]. Here the expectation is taken over
the randomness in the state transitions, the stochastic rewards and the randomness in the policy π
itself. Given any other policy π we define the pseudo-regret of π to be
Regn = E[R∏*] - E[R∏].	(4)
In the above setting, the UCRL2 algorithm (Auer et al., 2009) achieves a regret of O(D |S | p|A|T)
where D is the diameter of the MDp This is almost tight as there is a matching lower bound of
Ω(PD∣S∣∣A∣T).
We extend the above basic model with adversarial reward corruptions. In particular, we assume that
at each time step t, given a state action pair (st, at), the reward ",&七 observed by the agent is drawn
from (1 - ON陋…,σ2) + eQt. Here E is the noise rate and Qt is an arbitrary distribution chosen
by an adversary. Furthermore, we will assume that the adversary can choose Qt using complete
knowledge of the full history of the learning algorithm up to time t. Furthermore, by taking action
a from state s at time t, the observed transition is generated from the corrupted transition matrix
described as (1 - E)p(s0|s, a) + Eqt0(s0|s, a), where again qt0 is an arbitrary distribution chosen by an
adversary. Our goal in this setting would be to aim for a regret guarantee of O(D|S|，|A|T)+O(e∙T).
As in the previous two applications, we are interested in designing policies with low regret with
respect to the observations from true MDP. For this case we provide the following near optimal
guarantee.
Theorem 3	(Informal Theorem). For the E-adversarially corrupted MDP model as described above,
there is an there is an efficient robust online algorithm A that achieves a pseudo regret bounded by
RegA = O(σD∣S∣P∣A∣T log(∣S∣∣A∣T) + O(σT).
The first term corresponds to the regret achieved by the UCRL2 algorithm (Auer et al., 2009) for
the standard MDP setting. The second term is the additional penalty due to corruptions and is again
unavoidable in the worst case.
Techniques. Our work combines classical no-regret learning algorithms with recent advances
in designing robust algorithms for problems in unsupervised learning. For the case of stochastic
multi-armed bandits we modify the standard UCB algorithm (Auer et al., 2002). The UCB algorithm
works by maintaining optimistic estimates for the true mean reward of each arm. These optimistic
estimates are obtained by using confidence intervals build around the average observed rewards.
However, in the presence of adversarial corruptions, these estimates could be arbitrarily bad as we
demonstrate in Lemma 1. Instead, we build confidence intervals around the median that is known
to be robust to corruptions in Huber’s model (Lai et al., 2016; Diakonikolas et al., 2018a). For the
case of linear contextual bandits we modify the popular LinUCB algorithm. The LinUCB algorithm
3
Under review as a conference paper at ICLR 2021
works by maintaining uncertainty estimates around the true mean vectors and picking arms according
to these estimates. These estimates are built by solving a least squares problem at each time step.
Under adversarial corruptions one would like to build these estimates in a robust manner. However,
a straightforward extension of the stochastic bandits case leads to a suboptimal additive penalty
of O(dT). Instead, we build upon the recent work of Diakonikolas et al. (2019b) for robust high
dimensional linear regression to build better uncertainty estimates resulting in the near optimal penalty
of e√d log( ɪ )T.
For the case of learning in MDPs, we first consider MDPs with deterministic transition and adver-
sarially corrupted. Here we show that an extension of a UCB style exploration scheme achieves an
optimal penalty of O(T) by maintaining robust optimistic estimates of rewards at each state-action
pair. We then extend this to the more general case where we modify the UCRL2 algorithm (Auer
et al., 2009) by maintaining robust estimates of the estimated rewards and transition probabilities.
2 Stochastic Bandits
In this section we consider the setting of stochastic multi armed bandits. Here one has k arms. In
each time step, a learning algorithm can pull arm i and observe a reward ri distributed as N(μi,σ2).
Let i* be the arm with the highest expected reward. Then over T time steps, the pseudo-regret of an
algorithm A that pulls arms (i1 , i2, . . . , iT) is defined as
T
RegA = μi* ∙ T - E[X rit].
t=1
(5)
There are many algorithms the near-optimal regret of O(σ√kT) in this setting. The most popular
among them is the UCB algorithm (Auer et al., 2002; Slivkins, 2019). When the rewards are in [0, 1],
the UCB algorithm works by maintaining optimistic estimates of the average reward seen for each
arm. Specifically, the estimate γi,t for arm i at time t is defined as
γi,t=…力 IogkT
(6)
Here μ^i,t is the average reward observed for arm i till time step t, and ni,t is the number of times
arm i has been pulled up to and including time step t. The UCB algorithm starts by pulling each arm
once, and then at each time pulling the arm with the best current optimistic estimate as defined in (6).
UCB can be arbitrarily bad under adversarial corruptions. We now consider the adversarial
model as defined in Section 1 where the E-COrrUPted rewards r% are observed each time. In this case it
is easy to see that the regret of the UCB algorithm can be arbitrarily bad. This is formalized in the
lemma below.
Lemma 1. For any c > 0 and e ∈ (击,1), there exists a StochaStic multi armed bandit setting and
an adversary such that the pseudo-regret ofthe UCB algorithm is at least C ∙ T.
We next show a simple modification of the UCB algorithm that will achieve a near optimal regret of
O(σ√kT) + O(σe ∙ T). The algorithm maintains the following optimistic estimates
Yi,t = μi,t +4σ 产野^.	⑺
Here, μmax is an upper bound on the mean rewards, and μi,t is defined to be the median reward
obtained for arm i so far. The robust algorithm is sketched in Figure 1. For the robust UCB algorithm
we have the following guarantee.
Theorem 4. Under the adversarially corrupted stochastic bandits model the algorithm in Figure 1
achieves a pseudo-regret of O(σ√kT) + O(σeT).
Proof. Is it known that the median is a more robust estimate than the mean. In particular, the result
of Lai et al. (2016) implies that with probability at least 1 - * 1T4, for each arm i, and each time
4
Under review as a conference paper at ICLR 2021
Input: The k arms, reward variance σ2 .
1.	Play each arm once and update the estimates as in (7).
2.	For each subsequent time step t, pick the arm it with the highest estimate as defined in (7).
Play arm it and update the estimates.
Figure 1:	A robust UCB algorithm.
step t ≤ T , it will hold that
lμi,t-μil≤ O(σ ∙ C)+ 2σ Jog(kTtmax).
(8)
Conditioned on the above good event we have that for each time step t and each arm i
μi - O(σ ∙ E) ≤ Yi,t ≤ μi + O(σ ∙ c) + 6σ
∕log(kTμmax)
V	ni,t
(9)
Next, consider an arm i that is pulled ni,ti times in total, where ti is the last time step when it is
pulled. Then at time t it must hold that
,log 1 , l	∕log(kTμmaX)、	、
μi + O(σ ∙ E) + 6σ∖ ------------- ≥ γi,ti ≥ γi* ,ti
ni,ti
=μi* — O(σ ∙ E)
log(kTμmax) ʌ、
------------≥ μi* - μi.
ni,ti
(10)
(11)
(12)
⇒ O(σ ∙ E)
Hence, conditioned on the good event, the total regret accumulated by playing arm i is
ni,ti (μi* 一 μi) ≤ O(σ ∙ E)ni,ti + O
(σ,log(kTμ
max^)ni,t).
Using the fact that i ni,ti ≤ T with Jensen’s inequality and that the good event happens with proba-
bility at least 1 一 * 1T4, We get that the total pseudo-regret is bounded by O(σ ^kT log(kTμmax)) +
O(σ ∙ e)T.	max	□
To contrast the above theorem With the performance of UCB We shoW the folloWing stonger version
of the loWer bound in Lemma 1.
Lemma 2. For any c > 0 and E ∈ (,方),there exists a stochastic multi armed bandit setting
and an adversary such that the pseudo-regret of the UCB algorithm is at least C ∙ T whereas the
algorithm in Figure 1 achieves a vanishing regret of O(√kT).
Proof. We consider a set of k arms with means μι,μ2,... ,μk ∈ (1,4c) such that the difference
betWeen the best arm and all the other arms is at least 2c. Furthermore, We set the variance σ2 in the
true Gaussian reward distribution to be 1. It is easy to see that the robust algorithm will achieve a
regret of O(√kT) from the guarantee of Theorem 4.
When running UCB, since E = Θ(1∕√T), with high probability the adversary will get to perturb the
rewards within the first O(√T) time steps. When this happens, the adversary will choose a reward
of -L for the best arm and L for all the other arms with L》T. Hence, no matter which arm the
UCB algorithm chooses at that time step, the best arm will always be a suboptimal choice for the
algorithm going forward since a large value of L will highly skew the mean reward estimates in the
wrong direction for the best arm. As a result, the pseudo regret of UCB will be at least C ∙ T.	□
5
Under review as a conference paper at ICLR 2021
Input: The state space S, action space A, reward variance σ2 .
1.	Play each (s, a) ∈ S × A once and update the estimates as in (20).
2.	For episodes h = 1, 2, . . . do:
•	Set start time of episode h to be the current time t.
•	For each (s, a) set vh (s, a) = 0.
•	For each (s, a) compute the previous count Nh(s, a) = Pττ<it I(ST = a,aτ = a).
•	For each s, s0, a computePh(S0∣s, a) using estimates UP to time t.
•	For each s, a compute rh(s, a) robustly using estimates UP to time t.
•	Let Mh be the set of all MDPS that whose reward distribution r, and transition proba-
bilities P satisfy
log、—n	/log(|S||A|T〃max)
|r(s, a) — rh(s, a)| ≤ 20σ√----Nh(S a)------- (13)
kP(.∣s,a) - Ph(.∣s,a)kι ≤ 20σ∣ /|S| lOg(AIT：ma".	(14)
Nh (S, a)
•	Find the best policy πh that lies in Mh .
•	While vh(St, πh(at )) < Nh(St, πh(at ))
-	choose action at according to ∏h and update the corresponding Vh values. Set
t=t+ 1.
Figure 2:	A robust algorithm for general MDPs.
3	Learning in MDPs
In this section we study the most general application of our model in learning MDPs under adversarial
corruptions. Recall from Section 1 that we consider a Markov Decision Process (MDP) with state
space S, action space A and transition probabilities specified by P . If an action a ∈ A is taken from
state S ∈ S, then the next state distribution is specified asP(S0|S, a). Moreover a stochastic reward rs,a
is received where r§,a 〜N(μs,a,σ2). We will consider a scenario where at each time step, both the
reward distribution and the transition matrix is corrupted with an probability. We study two settings,
one concerning MDPs with deterministic transitions (hence only corrupted rewards) followed by the
case of more general MDPs. Due to space constraints we discuss the case of deterministic MDPs in
Appendix D.
Handling General MDPs To design efficient algorithms for general MDPs. as before we will
maintain robust estimates of the rewards and the transition probabilities and use these to guide the
search for a near optimal policy. Our proposed algorithm is reminiscent of the UCRL2 algorithm (Auer
et al., 2009) and is sketched in the algorithm in Figure 2. For the general case we have the following
guarantee. The proof can be found in Appendix E.
Theorem 5. The algorithm from Figure 2 achieves a pseudo regret bounded by
RegA = O(σD∣S∣P∣A∣T log(∣S∣∣A∣Tμmaχ) + O(σeT).
Here μmaχ is the maximum mean reward of any state action pair in the MDP
4	Experiments
We empirically validate the robustness of our algorithms to adversarial corruptions. In Section 4.1,
we use a real world routing task to benchmark the performance of the robust UCB in Figure 1 when
compared to the standard UCB algorithm. Similarly, for the MDP setting, in Section 4.2 we consider
routing on randomly generated graphs to compare the performance of the robust UCRL2 in Figure
2 with that of UCRL2. By varying the levels of corruption in the reward structure, we find in both
6
Under review as a conference paper at ICLR 2021
(a) Regret vs step for weak
adversary ( = 0.3, δ =
104).
Epsilon
Epsilon
(d) Regret vs for strong
adversary (δ = 104, step=
105).
(b) Regret vs step for (c) Regret vs for weak ad-
strong adversary ( = 0.1, versary (δ = 104, step=
δ= 104).	105).
Figure 3: Comparison of vanilla and robust versions of the UCB algorithm.
these settings that the learned policies and the regret incurred are far more resilient under our robust
algorithms.
In our experiments, we consider two modes of adversarial corruptions:
•	weak adversary that corrupts rewards with U[0, δ] for all actions, and
•	strong adversary that corrupts rewards with U [-δ, 0] for the optimal actions and with U[0, δ]
for others.
Here U[0, δ] denotes the uniform distribution in [0, δ]. Note that a weak adversary shrinks the mean
rewards for all actions towards δ∕2; this makes the learning harder but otherwise preserves the ranking
of actions. A strong adversary on the other hand enhances bad actions and minimizes good ones,
hence obfuscating the ranking of actions. For each of these modes, we vary the adversary’s strength
via probability and magnitude of the corruptions and δ, respectively.
4.1	Road Traffic Routing
We illustrate the robustness of algorithms introduced in Section 2. We consider a routing application,
where an agent needs to select one of many alternative routes between two locations. We use the
road network and link travel times from the New York City Taxi dataset (Donovan & Work, 2017),
which contains hourly average travel times on road segments across New York City. We focus on
Manhattan for which dense data is available. We first sample N origin-destination pairs and then K
alternative routes for each pair. Note that here routes correspond to arms. The competing routes here
are computed using a standard algorithm that utilizes a bidirectional Dijkstra search and filters paths
for diversity and near-optimality. The distribution of costs for each action is given by observing the
costs of the corresponding path in the historical data on every weekday at 9am.
In our experiments, for N = 200 origin-destination pairs we considered K ∈ [4, 6] alternative routes.
For each source-destination pair, we form a stochastic bandit problem with the corresponding routes
as available arms. We report the average performance across all bandit problems involving multiple
source-destination pairs. We study for each adversary mode the effect of corruption probability
= 0, 0.1, . . . , 0.4 and magnitude δ ∈ [10, 10000] on the regret of UCB and its robust counterpart.
Figure 3a shows a representative curve of the per step regret as a function of the step count for a weak
adversary. Observe the significant improvement offered by robust UCB at each time step. Under a
strong adversary, the performance deteriorates for both algorithms, but the robust variant is more
resilient. For example, Figure 3b shows one such setting where UCB fails to learn while the robust
version learns and the regret asymptotes. Finally, as expected, this pattern continues to hold for both
adversary modes for a range of and δ; see Figures 3c and 3d.
4.2	Learning Shortest Paths on Graphs
We next illustrate the robustness of algorithms introduced in Section 3 here. We consider the problem
of learning shortest paths on a road network. We cast this problem as an MDP whose reward and
transition structure must be learned while minimizing regret.
7
Under review as a conference paper at ICLR 2021
The road network is modeled as a graph G = (V, E ) whose
nodes V represent locations and the edges E the links con-
necting them. The edge costs correspond to the link commute
times. Given a destination t ∈ V while standing at a location
s ∈ V , an agent wishes to use that link e = (s, s0) ∈ E which
minimizes its overall commute time from s to t. That is, e
must lie on the shortest path from s to t. We cast this as an
MDP with the state (s, t) ∈ S = V × V and the action space
A = {1, . . . , A}, where A = maxv∈V Outdegree(v). In state
(s, t), the first Outdegree(s) actions correspond to taking an
edge (s, s0), which changes the state to (s0, t); the remaining
actions are invalid and preserve the state. Upon reaching the
destination in state (t, t), we choose the next destination t0 by
cycling through the nodes in a deterministic fashion and ran-
domly sampling a start node s0, leading to state (s0, t0). This
Figure 4: The Erdos-Renyi graph
for MDP experiments. The edge
thickness indicates its cost.
ensures that the states in S are connected and any trajectory
is infinitely long. Finally, the reward is structured as follows: N(μG, σ2) for optimal actions (e.g.
staying on the shortest path), N(μp, σ2) for suboptimal but valid actions, and N(μH, σ2) for invalid
actions, where μG > μB > μH. Observe that this aligns the agent's objective of maximizing the
cumulative reward with finding the shortest path on the original graph G.
In our experiments, We use a 20-node Erdos-Renyi graph (Erdos & Renyi, 1960) with edge costs
sampled from {1, 2, 3} and a maximum outdegree of 10, as shown in Figure 4. Thus our MDP has
|S| = 400 states and A = 10 actions in each state, yielding 4000 state-action pairs that need to be
assessed. For the random rewards, We set μG = 0,μB = -1,μH = -2 and σ = 1. Under each
adversary mode - weak and strong - the rewards are corrupted with probability = 0, 0.1, . . . , 0.4
and magnitudes δ ∈ [10, 10000]. For each setting, we employ two learning algorithms: the UCRL2
algorithm (Auer et al., 2009) and our robust adaptation in Figure 2. For a rigorous comparison, both
implementations share all code except that for computing the empirical reward estimates. For this, in
the robust version, the streaming median is estimated using a pool of 10,000 samples updated via
reservoir sampling (Vitter, 1985).
Our results indicate that the our algorithm is significantly more resilient to reward corruptions across
a wide range of corruption probabilities and magnitudes δ . Under a weak adversary, as Figure 5a
shows, increasing the frequency of corruption significantly deteriorates UCRL2 performance relative
to our robust counterpart. Even so, both algorithms learn near-optimal policies as indicated by the
regret values (per step regret near or greater than 1 = -μp indicates that learning failed). Increasing
the magnitude of corruption, however, completely breaks down the learning for vanilla UCRL2, while
the robust version is unaffected; see Figure 5b. Under a strong adversary, the performance of both
the algorithms deteriorates but the aforementioned trends continue to hold. UCRL2 fails to learn at
= 0.1, while our robust version has near-optimal performance (Figure 5c). As before though, our
robust version continues to learn well under high corruption magnitudes (Figure 5d). In general, we
see that that the regret of robust UCRL2 is better at nearly all time steps (Figure 5e). Further, the
number of states in which the prescribed action differs from the optimal one is fewer and drops faster
(Figure 5f).
5 Conclusions
In this work we initiated the study of robust algorithms for online learning settings. Several open
directions come out of our work. It would be interesting to design robust algorithms for linear
contextual bandits under more general distribution of context vectors. This would require new
algorithms for performing robust regression under more general co-variate distributions.
An important distinction between our proposed robust algorithms and the classical no-regret coun-
terparts is the amount of space usage. We need to store all the rewards (and contexts) observed up
to time t to compute good uncertainty estimates. It is an interesting open question to reduce this
gap. Finally, it would be interesting to study other scenarios in online learning under adversarial
corruptions.
8
Under review as a conference paper at ICLR 2021
0.1	0.2	0.3
Epsilon
0.00
(b) Regret vs δ for weak adversary
( = 0.1, step= 107).
(c) Regret vs for strong adver-
sary (δ = 10, step= 107).
(a) Regret vs for weak adversary
(δ = 10, step= 107).
(d) Regret vs δ for strong adver-
sary ( = 0.1, step= 107).
(e) Regret vs step for weak adver-
sary ( = 0.4, δ = 10).
O 200000 400000 600000 800000 ιoaoooo
Time Steps
(f) Delta from Optimal Policy vs
step for weak adversary ( =
0.4, δ = 10).
Figure 5: Comparison of vanilla and robust versions of the UCRL2 algorithm.
References
Peter Auer, Nicolo Cesa-Bianchi, and Yoav Freund Robert E Schapire. The non-stochastic multi-
armed bandit problem. 2001.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235-256, 2002.
Peter Auer, Thomas Jaksch, and Ronald Ortner. Near-optimal regret bounds for reinforcement
learning. In Advances in neural information processing systems, pp. 89-96, 2009.
Sivaraman Balakrishnan, Simon S Du, Jerry Li, and Aarti Singh. Computationally efficient robust
sparse estimation in high dimensions. In Conference on Learning Theory, pp. 169-212, 2017.
Moses Charikar, Jacob Steinhardt, and Gregory Valiant. Learning from untrusted data. In Proceedings
of the 49th Annual ACM SIGACT Symposium on Theory of Computing, pp. 47-60. ACM, 2017.
Mengjie Chen, Chao Gao, Zhao Ren, et al. Robust covariance and scatter matrix estimation under
huber’s contamination model. The Annals of Statistics, 46(5):1932-1960, 2018.
Yu Cheng, Ilias Diakonikolas, and Rong Ge. High-dimensional robust mean estimation in nearly-
linear time. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete Algorithms,
pp. 2755-2771. SIAM, 2019a.
Yu Cheng, Ilias Diakonikolas, Rong Ge, and David Woodruff. Faster algorithms for high-dimensional
robust covariance estimation. arXiv preprint arXiv:1906.04661, 2019b.
Wei Chu, Lihong Li, Lev Reyzin, and Robert Schapire. Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics, pp. 208-214, 2011.
Ilias Diakonikolas, Daniel M Kane, and Alistair Stewart. Statistical query lower bounds for robust
estimation of high-dimensional gaussians and gaussian mixtures. In 2017 IEEE 58th Annual
Symposium on Foundations of Computer Science (FOCS), pp. 73-84. IEEE, 2017.
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robustly learning a gaussian: Getting optimal error, efficiently. In Proceedings of the Twenty-Ninth
Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 2683-2702. Society for Industrial and
Applied Mathematics, 2018a.
9
Under review as a conference paper at ICLR 2021
Ilias Diakonikolas, Gautam Kamath, Daniel M Kane, Jerry Li, Jacob Steinhardt, and Alistair Stewart.
Sever: A robust meta-algorithm for stochastic optimization. arXiv preprint arXiv:1803.02815,
2018b.
Ilias Diakonikolas, Gautam Kamath, Daniel Kane, Jerry Li, Ankur Moitra, and Alistair Stewart.
Robust estimators in high-dimensions without the computational intractability. SIAM Journal on
Computing, 48(2):742-864, 2019a.
Ilias Diakonikolas, Weihao Kong, and Alistair Stewart. Efficient algorithms and lower bounds for
robust linear regression. In Proceedings of the Thirtieth Annual ACM-SIAM Symposium on Discrete
Algorithms, pp. 2745-2754. SIAM, 2019b.
B. Donovan and D. B. Work. Empirically quantifying city-scale transportation system resilience to
extreme events. Transportation Research Part C: Emerging Technologies, 79:333-346, 2017.
Paul Erdos and Alfred Renyi. On the evolution of random graphs. Publ. Math. Inst. Hung. Acad. Sci,
5(1):17-60, 1960.
John C Gittins. Bandit processes and dynamic allocation indices. Journal of the Royal Statistical
Society: Series B (Methodological), 41(2):148-164, 1979.
Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with
adversarial corruptions. arXiv preprint arXiv:1902.08647, 2019.
Samuel B Hopkins and Jerry Li. How hard is robust mean estimation? arXiv preprint
arXiv:1903.07870, 2019.
Peter J Huber. Robust statistics. Springer, 2011.
Sayash Kapoor, Kumar Kshitij Patel, and Purushottam Kar. Corruption-tolerant bandit learning.
Machine Learning, 108(4):687-715, 2019.
Adam Klivans, Pravesh K Kothari, and Raghu Meka. Efficient algorithms for outlier-robust regression.
arXiv preprint arXiv:1803.03241, 2018.
Pravesh K Kothari and David Steurer. Outlier-robust moment-estimation via sum-of-squares. Pro-
ceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing (STOC), 2018.
Kevin A Lai, Anup B Rao, and Santosh Vempala. Agnostic estimation of mean and covariance. In
2016 IEEE 57th Annual Symposium on Foundations of Computer Science (FOCS), pp. 665-674.
IEEE, 2016.
Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
World wide web, pp. 661-670, 2010.
Nick Littlestone and Manfred K Warmuth. The weighted majority algorithm. Information and
computation, 108(2):212-261, 1994.
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial
corruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,
pp. 114-122, 2018.
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Corruption robust explo-
ration in episodic reinforcement learning. arXiv preprint arXiv:1911.08689, 2019.
Adarsh Prasad, Arun Sai Suggala, Sivaraman Balakrishnan, and Pradeep Ravikumar. Robust estima-
tion via robust gradient estimation. arXiv preprint arXiv:1802.06485, 2018.
Aleksandrs Slivkins. Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272, 2019.
Jacob Steinhardt, Moses Charikar, and Gregory Valiant. Resilience: A criterion for learning in the
presence of arbitrary outliers. arXiv preprint arXiv:1703.04940, 2017.
10
Under review as a conference paper at ICLR 2021
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
John W Tukey. Mathematics and the picturing of data. In Proceedings of the International Congress
of Mathematicians, Vancouver, 1975, volume 2, pp. 523-531, 1975.
Jeffrey S Vitter. Random sampling with a reservoir. ACM Transactions on Mathematical Software
(TOMS), 11(1):37-57, 1985.
11