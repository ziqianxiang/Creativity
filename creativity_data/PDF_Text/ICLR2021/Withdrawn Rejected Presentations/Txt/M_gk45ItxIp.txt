Under review as a conference paper at ICLR 2021
Interpretable Reinforcement Learning with
Neural Symbolic Logic
Anonymous authors
Paper under double-blind review
Ab stract
Recent progress in deep reinforcement learning (DRL) can be largely attributed
to the use of neural networks. However, this black-box approach fails to explain
the learned policy in a human understandable way. To address this challenge and
improve the transparency, we introduce symbolic logic into DRL and propose a
Neural Symbolic Reinforcement Learning framework, in which states and actions
are represented in an interpretable way using first-order logic. This framework
features a relational reasoning module, which performs on task-level in Hierar-
chical Reinforcement Learning, enabling end-to-end learning with prior symbolic
knowledge. Moreover, interpretability is enabled by extracting the logical rules
learned by the reasoning module in a symbolic rule space, providing explainability
on task level. Experimental results demonstrate better interpretability of subtasks,
along with competing performance compared with existing approaches.
1	Introduction
In recent years, deep reinforcement learning (DRL) has achieved great success in sequential
decision-making problems like playing Atari Games (Mnih & et al, 2015) or the game of Go (Sil-
ver et al., 2017). However, it is hard to apply DRL to practical problems due notably to its lack
of interpretability. The ability to explain decisions is important in earning people’s trust and devel-
oping a robust and responsible system, especially in the area like autonomous driving. Moreover,
an interpretable system makes problems traceable and debugging of systems easier. Therefore, in-
terpretability has attracted increasingly attention in the DRL community recently. Interpretability
can be either intrinsic or post-hoc, depending on how it is obtained. In the latter case, the black-
box model is explained after training by visualizing for instance t-SNE and saliency maps (Zahavy
et al.) or attention masks (Shi et al.). In the former case, interpretability is entailed by the inherent
transparent property of the model. Our current work falls in that category.
To improve the interpretability of DRL policies, we investigate an approach that represents states
and actions using first-order logic (FOL) and makes decision via neural-logic reasoning. In this
setting, interpretability is enabled by inspecting the FOL rules used in the action selection, which
can easily be understood and examined by a human. A number of algorithms (Jiang & Luo, 2019;
Dong et al., 2019; Payani & Fekri, 2020) involving FOL take advantage of neural networks to induce
a policy that performs the action selection via approximate reasoning on existing symbolic states and
possibly additional prior knowledge. In this context, an action is selected if after performing some
reasoning steps, an action predicate becomes true. The rules used in a policy can be learned using
a differentiable version of inductive logic programming (ILP) whose goal is to learn FOL rules to
explain observed data. When a neural network implements the policy, it can be trained to learn
the rules and perform reasoning over those rules by having forward chaining implemented in the
neural network architecture. The main issues with those approaches are their potential high-memory
requirements and their computational costs, which limit their applicability. Alternatively, Lyu et al.
(2019) propose a hierarchical reinforcement learning (HRL) approach where a high-level (task level)
policy selects tasks to be solved by low-level (action level) policies. The latter policies interacts
directly with the environment through potential high-dimensional inputs, while the high-level policy
makes decisions via classical planning. While this approach can scale to larger problems, it requires
the specification by an expert of the planning problem of the high-level policy.
1
Under review as a conference paper at ICLR 2021
To alleviate the problems discussed above, in this paper, we propose a novel framework named
Neural Symbolic Reinforcement Learning (NSRL). Similarly to (Lyu et al., 2019), it is an HRL
framework. However, in NSRL, the high-level policy makes decisions via neuro-logic reasoning.
Thus, in contrast to (Lyu et al., 2019), NSRL does not need the definition of any oracle rules or
transition model in advance. In contrast to differentiable ILP methods, NSRL performs reasoning
by selecting relational paths over the knowledge graph induced by the known predicates. To the best
of our knowledge, this is the first work introducing reasoning into RL that can can succeed in com-
plex domains while remaining interpretable. More specifically, this framework features a reasoning
module based on neural attention network, which performs relational reasoning on symbolic states
and induces the RL policy. We further embed this module on the task level in HRL and evaluate
the framework on Montezuma’s Revenge. Leveraging the power of symbolic representation, the
results demonstrate competing performance with existing RL approaches while providing improved
interpretability by extracting the most relevant relational paths.
2	Related Work
2.1	Inductive Logic Programming
Traditional inductive logic programming (ILP) approaches require the search in a discrete space of
rules and are not robust to noise (Evans & Grefenstette, 2018). To address those issues, many re-
cent works have proposed various differentiable versions of ILP (Evans & Grefenstette, 2018; Dong
et al., 2019; Payani & Fekri, 2020). They are all based on simulating forward chaining and suffer
from some form of scalability issues (Yang & Song, 2020). In contrast, multi-hop reasoning meth-
ods (Gardner & Mitchell, 2015; Das et al., 2017; Lao & Cohen, 2010; Yang & Song, 2020) allow
answering queries involving two entities over a knowledge graph (KG) by searching a relational
path between them. In the ILP context, such paths can be interpreted as grounded first order rules.
Interestingly, they can be computed via matrix multiplication (Yang et al., 2017). Compared to dif-
ferentiable ILP, multi-hop reasoning methods have demonstrated better scalability. Our work can be
seen as the extension of the work by Yang & Song (2020) to the RL setting.
2.2	Interpretable Reinforcement Learning
Recent work on interpretable DRL can be classified into two types of approaches, focusing either
on (i) intrinsic interpretability or (ii) post-hoc explanation. Intrinsic interpretability requires the
learned model to be self-understandable by nature, which is achieved by using a transparent class
of models, whereas post-hoc explanation entails learning a second model to explain an already-
trained black-box model. In approach (i), a (more) interpretable policy can be learned directly online
by considering a specific class of interpretable policies (e.g., (Lyu et al., 2019)), or by enforcing
interpretability via architectural inductive bias (e.g., (Zambaldi et al., 2018), (Jiang & Luo, 2019;
Dong et al., 2019)). Alternatively, an interpretable policy can be obtained from a trained one via
imitation learning (e.g., (Bastani et al., 2018; Vasic et al., 2019), or (Verma et al., 2018; Verma
et al.)). In approach (ii), various techniques have been proposed to explain the decision-making of
DRL agents (e.g., (Zahavy et al.; Greydanus et al., 2018; Gupta et al., 2020), (Shi et al.), (Sequeira &
Gervasio), (Juozapaitis et al.), (Madumal et al., 2020), or (Topin & Veloso, 2019)). More related to
interpretable policies, some work in approach (ii) also tries to obtain a more understandable policy
(e.g., (Coppens et al.)) in order to explain a trained RL agent.
Both our framework and that of Lyu et al. (2019) are formulated in a hierarchical reinforcement
learning (HRL) setting. However, in their work, the meta-controller (i.e., high-level policy) is a
symbolic planner, while in ours, its action selection is realized by neural logic reasoning. Thus, their
proposition requires an expert to describe a task as a symbolic planning problem, whereas our work
only needs the definition of a high-level reward function, which is arguably easier to provide. In
contrast to the extensions of differentiable ILP to RL (Jiang & Luo, 2019; Dong et al., 2019), our
framework is formulated in an HRL setting and only the meta-controller is based on neural-logic
reasoning, which is moreover realized via multi-hop reasoning. We believe that our architectural
choices explain why our method can scale to a complex problem such as Montezuma’s revenge,
while preserving some level of interpretability.
2
Under review as a conference paper at ICLR 2021
3	Preliminary
In this section, we give a brief introduction to the background knowledge necessary for the proposed
framework. Interpretable rules described by First-Order Logic are first introduced, then the basics
about Hierarchical Reinforcement Learning are briefly described.
3.1	First Order Logic
A typical First-Order Logic(FOL) system consists of three components: Entity, Predicate, For-
mula. Entities are constants (e.g., objects) while a predicate can be seen as a relation between enti-
ties. An atom α=p(t1, t2, .., tn) is composed with a n-nary predicate p and n terms {t1, t2, ..., tn},
where a term can be a constant or variable. An atom is grounded if all terms in this atom are con-
stants. A formula is an expression formed with atoms, logical connectives, and possibly existential
and universal quantifiers. In the context of ILP, one is interested to learn formulas of restricted forms
called rules. A rule is called a clause in the form of α J aι ∧ α2,..., ∧ɑn where α is called head
atom and α1, α2, ..., αn are called body atoms. A clause is grounded with all the associated atoms
grounded. The head atom is believed to be true only if all the body atoms are true. For exam-
ple, Connected(X, Z) J Edge(X, Y ) ∧ Edge(Y, Z) is a clause where X, Y, Z are variables and
Connected, Edge are predicates. If we substitute X, Y, Z with constants a, b, c, then a and c are
considered connected if Edge(a, b) and Edge(b, c) hold. Embedded with prior knowledge, clauses
described by FOL is highly understandable and interpretable.
3.2	Hierarchical Reinforcement Learning
Consider a Markov Decision Process defined by a tuple (S, A, Psas0 , rsa, γ) where S and A denote
the state space and action space respectively, Psas0 provides the transition probability of moving from
state s ∈ S to state s0 ∈ S after taking action a ∈ A, rsa is the immediate reward obtained after
performing action a in state s and γ ∈ [0, 1] is a discount factor. The objective of an RL algorithm
is to find a policy ∏ : S → A that maximizes the expected return Vn(S) = En[P∞=° Ytr1+t∣s0 =
s] where rt is the reward at time step t received by following π from state s0. In Hierarchical
Reinforcement Learning (HRL), the agent learns the high-level policy (meta-controller) and low-
level policy (controller) jointly when interacting with the environment. Sutton et al. (1999) formalize
the idea of option as temporally extended actions. In a two-level structure, an option chosen by the
high-level policy is achieved by the lower-level policy. Markov property exists at different levels.
Thus, we can utilize standard RL algorithms to learn policy separately for each level.
4	Proposed Method
In this section, we firstly illustrate the whole structure of Neural Symbolic Reinforcement Learning
(NSRL) and how it performs on the task level in Hierarchical Reinforcement Learning and demon-
strate the cross-fertilization of three components of NSRL. Then, we separately introduce these three
components : reasoning module, attention module and the policy module. In the end of this section,
we present the training process of NSRL.
4.1	Neural Symbolic Reinforcement Learning
Hierarchical Reinforcement Learning (HRL) is an effective method to solve sparse and delayed
reward by integrating temporal abstraction and intrinsic motivation. However, it still suffers from
the lack of interpretability of each abstracted level. In this section, we abstract the Markov Decision
Making Process into two levels: task-level and action level. Then, we put forward the framework of
Neural Symbolic Logic based HRL, providing a high level insight on the decision-making process.
For convenience, We reuse the notation of standard RL. We use (S, A, P^^o ,ra,^) to denote the
higher level where S represents the symbolic state. (S, A, Psas0, rsa, γ) denotes the lower level where
S represents the high dimensional state.
As shown in Figure 1, the left part illustrates the whole training procedure of the algorithms. Firstly,
a detector of objects and relations extracts objects from high-dimensional state s and outputs a
symbolic state S to the meta controller, which allocates tasks and provides intrinsic rewards to the
3
Under review as a conference paper at ICLR 2021
ɪ state
Environment
Object and
Relation Detector
Figure 1: Architecture illustration
controller. Then, the controller interacts with the environment iteratively until achieving the task
or reaching maximum steps. At the same time, the controller collects extrinsic reward to the meta
controller for training. Instead, intrinsic reward is used to train the controller.
The right part depicts the architecture of the meta controller or NSRL. There are three major com-
ponents: reasoning module, attention module and the policy module, which are detailly described in
Section 4.2, Section 4.3, Section 4.4 separately. The symbolic state S would be firstly transformed to
relational matrix, which is sent to two attention modules to generate the attention weights on predi-
cate and length of relational paths. Afterwards, the relational matrix, as well as the attention weights
would be sent into reasoning module, performing reasoning on existing symbolic knowledge. The
reasoning module can take consideration of all predicates at one reasoning step and possible rela-
tional paths of varying length. After that, the policy module would receive the reasoning output and
compute the Q value of action atoms, thus induce the DRL policy.
4.2	Reasoning Module
Consider a knowledge graph, where objects are represented as nodes and relations are edges. Multi-
hop reasoning on such a graph mainly focuses on searching chain-like logical rules of the following
form,
query(X, x) J Rι(X, zι) ∧ …∧ Rn(zn, x)	(1)
The task of multi-hop reasoning for a given query corresponds to finding find a relational path from
X to x0 with multi-steps X -→ •--R→ x0 with given query. Based on Yang et al. (2017), the
inference of this logical path could be seen as a process of matrix multiplication. Every predicate or
relation Pk is represented as a binary matrix Mk in {0,1}lXl×lXl, whose entry (i,j) of Mk is 1 if
Pk(xi, xj) is in the knowledge graph with entity xi and xj are connected by edge Pk. X is the total
numbers of objects. Let vx be the one-hot encoding of object x. Then, the t-th hop of the reasoning
along the path can be computed as
v(0) = vx	(2)
v(t) = M(t)v(t-1)	(3)
where M(t) is the matrix used in t-th hop and v(t-1) is the path features vector. After T steps
reasoning, the score of the query is computed as
4
Under review as a conference paper at ICLR 2021
T
Score(x, x0) = Vχl Y M(t) ∙ Vχo	(4)
t=1
Considering all the predicate matrices at each step and relational paths of different lengths, the final
score could be rewritten with soft attention as below:
T	t0 K
κ(sψ 鸟)=X sψt0) YX S 以 Mk	(5)
t0=1	t=1 k=1
Score(x, x0) = Vχlκ(sψ, Sφ)Vχo	(6)
where T is the maximum reasoning steps, stψ0 corresponds to attention weights on all the relational
paths of length t0 and Sgk to another attention weights on predicate matrix Mk used in the t-th step,
and K denotes the total number of predefined predicates.
4.3	Attention Module
In this section, we introduce the architecture of attention network, a hierarchical stack of transform-
ers, to generate the dynamic attention weights. Consider a basic multi-head dot-product attention
module (MHDPA) in transformer (Vaswani et al., 2017), the input of MHDPA is the query, key and
value representations: Q, K, V . MHDPA firstly computes the similarity or attention weights S
between the query and the key, and then calculates the weighted value as output V 0 :
and V 0 = SV
(7)
where d is the dimension of K. We utilize this module to generate the attention weights sψ and sφ,k.
In fact, the symbolic states can be represented as a 3-dimensional tensor M ∈ [0,1]lXl×lXl×N,
where X denotes the numbers of extracted objects and N represents the numbers of predefined
predicates.
We transform tensor M into a matrix Mf ∈ [0,1]N×2lXl at each time step. Each row of matrix
Mf represents a part of the symbolic state, which can be seen as an embedding of predicate varying
with time. In this way, the attention module can generate weights on predicates at different time
steps, taking consideration of the symbolic information of current RL state. We firstly generate
the query, key and value representation by multi-perception layers with Mf as input initially. For
convenience, we introduce Vφ = Mf . Then, we repeatedly use the output value from last step to
generate the attention weights. For predicate attention module.
QgT),KtT),吗tT)= FeedForWard(Vφ)	(8)
S化吗 = MHDPA(QaT),KT),V(T)), Vφ = V()	(9)
Here, S(t) represents the attention weights on predicates in the t-th hop reasoning.
For path attention module, we reuse the output value of each time step in predicate attention module.
During the iterative processing, the output value at each step embeds the information of paths of
different lengths. We simply use another transformer to generate the path attention weights Sψ . Let
% = [vf),V(1),…,vħl∙
Q中,Kφ, Vφ = FeedForWard(VW)	(10)
Sψ ,Vψ = MHDPA(QpVW)	(11)
5
Under review as a conference paper at ICLR 2021
4.4	Policy Module
In this section, we put forward the policy module to induce DRL policy. Suppose there exists an
entity set Se and a predicate set Sp consisting of predefined predicates Pb and auxiliary predicates
Pa similar to (Evans & Grefenstette, 2018). With the predicate and entity set, we can obtain sym-
bolic state at each time step. The symbolic state can be represented by a 3-dimensional tensor M as
discussed in Section 4.3. Since a policy is a mapping from states to actions, the predicate at the last
hop needs to be constrained to be an action predicate. Instead of introducing a predicate matrix, we
can instead introduce multi-perceptron layers to the output of reasoning to induce a policy.
Qa(x,x0) = (vlMLPa(κ(sψ ,Sφ))Vχ0 )
(12)
Algorithm 1: Neural Symbolic Reinforcement Learning
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Initialize Replay Buffer Dm, Ds1, ..., Dsj;
Initialize Attention Network Lm and Controller Ls1, Ls2, ..., Lsj respectively;
for i = 1 …num_episodes do
extract objects and obtain symbolic state S;
while S is not Terminal and maximal step is not reached do
Perfrom reasoning on symbolic state S based on (12);
choose task g — EPSGreedy(Qm, S);
obtain current state S;
while S is not Terminal and maximal step is not reached do
a J EPSGreedy(Qg, S) where Qg is the output of Controller Lg;
obtrain current state S0 and reward r;
store (S, a, S0, r) in Dg;
S J S0;
if g is reached then
L break
obtain current symbolic state ^0 and reward r;
store (S,g, ^0,r) in Dm；
ʌ	ʌ!
^ ^ J S0;
Update Qj for j = 1, 2,..., num_tasks;
Update Meta Controller Qm
5 Experiments
We conduct experiments on Montezuma’s Revenge, an ATARI Game with sparse and delayed re-
ward. The player is required to navigate through several rooms while collecting treasures. Our
experiments are based on the first room. In this room, the player needs to first fetch the key, re-
ceiving a reward(+100) after taking a long sequence of actions without any reward information.
Then, the player needs to navigate to the door (left or right) and pass through it, resulting in another
reward(+300).
5.1	Symbolic Representation
Similar to HDQN, every task is associated with an extracted object. Logical representation is based
on 8 predefined locations: middle laddder, top-right door, top-left door, bottom-left ladder, bottom-
right ladder, conveyer, rope and the key and 2 mobile objects : man and skull. We introduce pred-
icates like OnSpot, WithObject, SmallerX, SameX, SmallerY, SameY, Near, PathExist, Con-
ditional. The atom Conditional(x,y) means we need to fetch object x firstly and then reach object
y. Firstly, we use logic primitive statements not on these predicates to generate other predicates
like LargerX, LargerY, Away. Since some logical rules may combine the negation of relation of
entities. It’s easy to accomplish negation operation by fuzzy logic, which enables to extend the rule
search space as described in Yang & Song (2020). We introduce one auxiliary and binary action
predicate MoveTo(agent, x), which means the agent needs to reach the object x.
6
Under review as a conference paper at ICLR 2021
5.2	Experimental Results
In this section, we analyze the experimental results. Fig2 illustrates a sample gameplay by our agent
on Montezuma’s Revenge. Fig 3 depicts the performance of algorithms as training proceeds. Finally,
several relational paths learned by the agent are visualized to validate the interpretablity embedded
in NSRL.
(a) Optimal Policy
(b) Choose Key
(d) Choose Right Door
(c) Choose Right Lad-
der
Figure 2:	Sample gameplay on Montezuma’s Revenge : (a) illustrates the optimal policy learned
by the agent. It firstly chooses the key as a task, then turn to reach the bottom-right ladder after
obtaining the key. Finally, the agent chooses top-right door and takes a set of low-level actions to
reach the door. (b-d) presents the process of navigating through the first room. It is easy to see the
key, bottom-right ladder and the top-right door are the key tasks to choose.
(a) Learning Curve
(b) Choosen ratio of different (c) Success ratio of different tasks
tasks over time
over time
Figure 3:	Training performance on Montezuma’s Revenge: (a) illustrates the training phase of NSRL
and HDQN, of which the results are collected and averaged by 8 runs. (b) depicts the distribution
of tasks chosen by the meta controller at different time steps. (c) presents the success ratio of tasks
over time.
We compare our approach with HDQN (Kulkarni et al., 2016a), SDRL (Lyu et al., 2019) as base-
lines. Both HDQN and NSRL coverage to achieve +400 reward but SDRL fails. To learn the optimal
policy, it nearly takes NSRL and HDQN another 4M steps. Different from HDQN, at the beginning
of first 3M steps, NSRL fails to learn the way back to the door (+300 reward). Besides of this,
the performance of NSRL is still competing compared to HDQN since they nearly start to cover-
age almost at the same time and the variance of NSRL is smaller than that of HDQN. Moreover,
with embedded reasoning module, NSRL can provide more interpretability as discussed below. As
shown in Fig.3b, all tasks are nearly preferred at the beginning of training expect the key. As train-
ing proceeds, the agent gradually learns to select the key tasks, the probabilities of which gradually
increase. Specially, at 8M steps, the agent only chooses the key, bottom-right ladder and top-right
door. We also visualize the relational paths to explain the important relations when choosing pivotal
tasks as below. At each reasoning step, we consider the predicates with highest attention weights at
each reasoning step and the connective constraints to formalize a specific path since we use softmax
operation to consider all paths of different lengths. For convenience, we use RightDoor to represent
top-right door, LeftDoor to represent top-left door, RightLadder to represent bottom-right ladder,
LeftLadder to represent bottom-left ladder and Man to agent.
7
Under review as a conference paper at ICLR 2021
Relational Paths
1	MoveTo(Man, Key) — WithOutObjeCt(Man, Key)
2	MoveTo(Man, Key) — WithOutObjeCt(Man, Key) ∧ SmallerY(Key, Man) ∧ LargerY(Man, Key)
3	MoveTo(Man, Key) — WithOutObjeCt(Man, Key) ∧ SmallerY(Key, LeftDoor)
∧ LargerY(LeftDoor,Key)
4	MoveTo(Man, Key) — WithOutObjeCt(Man, Key) ∧ SmallerY(Key, RightDoor)
∧ LargerY(RightDoor,Key)
5	MoveTo(Man, Key) — WithOutObjeCt(Man, Key) ∧ SmallerY(Key, RightDoor)
5	∧ LargerY(RightDoor,Man) ∧ LargerY(Man, Key)
Table 1:	Choose Key
Relational Paths
1	MoveTo(Man, RightLadder) — WithObject(Man, Key) ∧ Conditional(Key, LeftDoor)
∧ LargerY(LeftDoor, Man) ∧ LargerY(Man, Conveyer) ∧ SmallerX(Conveyer, RightLadder)
2	MoveTo(Man, RightLadder) — WithObject(Man, Key) ∧ Conditional(Key, LeftDoor)
∧ LargerY(LeftDoor, Man) ∧ LargerY(Man, LeftLadder) ∧ SmallerX(LeftLadder, RightLadder)
3	MoveTo(Man, RightLadder) — WithObject(Man, Key) ∧ Conditional(Key, RightDoor)
∧ LargerY(RightDoor, Man) ∧ LargerY(Man, Conveyer) ∧ SmallerX(Conveyer, RightLadder)
Table 2:	Choose RightLadder
Relational Paths
1	MoveTo(Man, RightDoor) — Near(Man, RightLadder) ∧ Away(RightLadder, RightDoor)
MoveTo(Man, RightDoor) — Near(Man, RightLadder) ∧ Away(RightLadder, RightDoor)
2	∧ SameX(RightDoor, RightLadder) ∧ LargerX(RightLadder, Man) ∧ LargerX(Man, MiddleLadder)
∧ Near(MiddleLadder, Conveyer) ∧ Away(Conveyer, RightDoor)
MoveTo(Man, RightDoor) — Near(Man, RightLadder) ∧ Away(RightLadder, RightDoor)
3	∧ SameX(RightDoor, RightLadder) ∧ LargerX(RightLadder, Rope) ∧ LargerX(Rope, Conveyer)
∧ Near(Conveyer, MiddleLadder) ∧ Away(MiddleLadder, RightDoor)
Table 3:	Choose RightDoor
Tables 1-3 present the relational paths the agent puts most attention on when Choosing different
tasks. NSRL realizes the faCt that the agent does not possess the key by putting highest attention
on prediCate WithOutObject. As shown in Table 2, we Can see that NSRL pays Close attention on
prediCate WithObject and Conditional at ConseCutive steps when Choosing bottom-right ladder as
a task, indiCating that NSRL gradually learns the importanCe of possessing the key to the door. After
that, the agent Considers the relevant loCation information to Choose bottom-right ladder. In Table 3,
the agent Considers more distanCe information when Choosing RightDoor. It first reCognizes the Cur-
rent loCation by atom Near(Man, RightLadder) and then Considers the reaChbility from RightLadder
to RightDoor. All of these relational paths are a form of logiCal rules, providing interpretability on
Choosing tasks.
6	Conclusion
In this paper, we propose a novel framework performing neural-logiC reasoning to enable inter-
pretability by visualizing the relational paths to tasks. Exploiting multi-hop reasoning and hierarChi-
Cal reinforCement learning, our approaCh Can solve large-sized Complex problem like Montezuma’s
Revenge, in Contrast to other reCent neuro-symboliC approaChes. Compared to other blaCk-box style
methods, the learning proCess of our approaCh naturally integrates with symboliC knowledge while
aChieving Comparable performanCe and preserving interpretability. The introduCed symboliC predi-
Cates Could easily be reused in other domains.
8
Under review as a conference paper at ICLR 2021
7	Acknowledgement
References
Osbert Bastani, Yewen Pu, and Armando Solarlezama. Verifiable reinforcement learning via policy
extraction. Neural Information Processing Systems,pp. 2494-2504, 2018.
Youri Coppens, Kyriakos Efthymiadis, Tom Lenaerts, and Ann Now. Distilling deep reinforcement
learning policies in soft decision trees. In Proceedings of the IJCAI 2019 Workshop on Explain-
able Artificial Intelligence, pp. 1-6. URL https://cris.vub.be/files/46718934/
IJCAI_2019_XAI_WS_paper.pdf. 00000.
Rajarshi Das, Arvind Neelakantan, David Belanger, and Andrew Mccallum. Chains of reasoning
over entities, relations, and text using recurrent neural networks. Conference of The European
Chapter of The Association for Computational Linguistics, 1:132-141, 2017.
Honghua Dong, Jiayuan Mao, Tian Lin, Chong Wang, Lihong Li, and Dengyong Zhou. Neural logic
machines. International Conference on Learning Representations, 2019.
R Evans and E Grefenstette. Learning explanatory rules from noisy data. Journal of Artificial
Intelligence Research, 61(1):1-64, 2018.
Matt Gardner and Tom M Mitchell. Efficient and expressive knowledge base completion using
subgraph feature extraction. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 1488-1498, 2015.
Sam Greydanus, Anurag Koul, Jonathan Dodge, and Alan Fern. Visualizing and understanding atari
agents. International Conference On Machine Learning, pp. 1787-1796, 2018.
Piyush Gupta, Nikaash Puri, Sukriti Verma, Dhruv Kayastha, Shripad Deshmukh, Balaji Krish-
namurthy, and Sameer Singh. Explain your move: Understanding agent actions using focused
feature saliency. International Conference on Learning Representations, 2020.
Zhengyao Jiang and Shan Luo. Neural logic reinforcement learning. International Conference on
Machine Learning, pp. 3110-3119, 2019.
Zoe Juozapaitis, Anurag Koul, Alan Fern, Martin Erwig, and Finale Doshi-Velez. Explainable
reinforcement learning via reward decomposition. In IJCAI/ECAI Workshop on Explainable Ar-
tificial Intelligence. URL https://web.engr.oregonstate.edu/~afern/Papers/
reward_decomposition__workshop_final.pdf. 00000.
Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Joshua B Tenenbaum. Hierarchical
deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation. Neural
Information Processing Systems, 2016a.
Tejas D Kulkarni, Karthik Narasimhan, Ardavan Saeedi, and Joshua B Tenenbaum. Hierarchical
deep reinforcement learning: integrating temporal abstraction and intrinsic motivation. Neural
Information Processing System, pp. 3682-3690, 2016b.
Ni Lao and William W Cohen. Relational retrieval using a combination of path-constrained random
walks. Machine Learning, 81(1), 2010.
Daoming Lyu, Fangkai Yang, Bo Liu, and Steven Gustafson. Sdrl: interpretable and data-efficient
deep reinforcement learning leveraging symbolic planning. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volume 33, pp. 2970-2977, 2019.
Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. Explainable reinforcement learn-
ing through a causal lens. Proceedings of the AAAI Conference on Artificial Intelligence, 2020.
Silver David Mnih, Kavukcuoglu and Rusu et al. Human-level control through deep reinforcement
learning. Nature, 518(740):529-533, 2015.
9
Under review as a conference paper at ICLR 2021
Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo Jimenez Rezende.
Towards interpretable reinforcement learning using attention augmented agents. arXiv: Learning,
2019.
Ali Payani and Faramarz Fekri. Incorporating relational background knowledge into reinforcement
learning via differentiable inductive logic programming, 2020.
Pedro Sequeira and Melinda Gervasio. Interestingness elements for explainable reinforcement learn-
ing: Understanding agents’ capabilities and limitations. 288:103367. ISSN 00043702. doi:
10.1016/j.artint.2020.103367. URL http://arxiv.org/abs/1912.09007.
Wenjie Shi, Shiji Song, Zhuoyuan Wang, and Gao Huang. Self-supervised discovering of causal fea-
tures: Towards interpretable reinforcement learning. URL http://arxiv.org/abs/2003.
07069. 00000.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354-359, 2017.
Richard S Sutton, Doina Precup, and Satinder Singh. Between mdps and semi-mdps: a framework
for temporal abstraction in reinforcement learning. Artificial Intelligence, 112(1):181-211, 1999.
Nicholay Topin and Manuela Veloso. Generation of policy-level explanations for reinforcement
learning. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):2514-2521, 2019.
Marko Vasic, Andrija Petrovic, Kaiyuan Wang, Mladen Nikolic, Rishabh Singh, and Sarfraz Khur-
shid. Moet: Interpretable and verifiable reinforcement learning via mixture of expert trees. arXiv:
Learning, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Neural Information Processing
Systems, pp. 5998-6008, 2017.
Abhinav Verma, Hoang M. Le, Yisong Yue, and Swarat Chaudhuri. Imitation-projected
programmatic reinforcement learning. URL https://papers.nips.cc/paper/
9705-imitation-projected-programmatic-reinforcement-learning.
pdf. 00000.
Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri.
Programmatically interpretable reinforcement learning. International Conference on machine
Learning, 2018.
Fan Yang, Zhilin Yang, and William W Cohen. Differentiable learning of logical rules for knowledge
base reasoning. In Advances in Neural Information Processing Systems, pp. 2316-2325, 2017.
Yuan Yang and Le Song. Learn to explain efficiently via neural logic inductive learning. Interna-
tional Conference on Learning Representation, 2020.
Tom Zahavy, Nir Ben-Zrihem, and Shie Mannor. Graying the black box: Understanding DQNs.
In Maria Florina Balcan and Kilian Q Weinberger (eds.), Proceedings of The 33rd International
Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp.
1899-1908. PMLR. URL http://proceedings.mlr.press/v48/zahavy16.html.
Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl
Tuyls, David P Reichert, Timothy Lillicrap, Edward Lockhart, et al. Relational deep reinforce-
ment learning. arXiv: Learning, 2018.
10
Under review as a conference paper at ICLR 2021
A Reward Engineering
To facilitate the learning process in HRL, we use the reward engineering as below. For action level,
(φ	achieve tasks
ψ	terminal
c	step cost
(13)
The controller of lower level receives cost c at every time step and achieves a large reward φ when
finishing tasks or negative reward ψ when failing to complete the tasks within maximum number of
steps . For task level, we further modify the extrinsic reward based on Hierarchical Deep Q-Learning
Framework
r(sh,gh) =	f(sh,g-hφ)	ββ==01
(14)
whereβ is a flag indicating whether the agent achieved the assigned task (β = 1) or not (β = 0). φ
is a large number for punishing the agent for not finishing the tasks and f (sh, gh) is the cumulative
reward from the environment by following the task gh under state sh . To this end, the agent is
encouraged to learn and select the right order of these tasks.
B	Experimental Setup
For the controller architecture, we follow the one used in Kulkarni et al. (2016b). There are 8
controllers in total, each associated with a predefined location. In terms of the meta controller,
the maximum length of relational path and the numbers of head of transformer are set to 8 and 4
respectively. The length of one episode is set to be 500 and the agent is restricted to finish the game
with one life. Learning rates are all set to be 1e-4. The sizes of replay buffers for meta controller
and low level controller are set to be 5e4 and 6e4 respectively. For reward engineering, following
(14) the agent receives reward φ = 10 when achieving tasks and reward ψ = -5 when losing life.
Cost at each step is set to be r = 0. Step cost r is set to be 0. Following (15) φ = 150. The training
procedure is divided into two phases. (1) In the first phase, the epsilon rate of meta controller is set
to 1 so that all the controllers are pretrained sufficiently to solve a subset of tasks. (2) In the second
phase,the meta controller is trained from scratch with epsilon rate annealing from 1 to 0.05 in 1e6
steps and the lower level controllers are fixed with epsilon rate set to be 0.05. Since we only perform
reasoning on task-level, it’s fair to compare the performance of algorithms with low level controllers
fixed.
11