Under review as a conference paper at ICLR 2021
Learning Robust Models by Countering
Spurious Correlations
Anonymous authors
Paper under double-blind review
Ab stract
Machine learning has demonstrated remarkable prediction accuracy over i.i.d data,
but the accuracy often drops when tested with data from another distribution. One
reason behind this accuracy drop is the reliance of models on the features that are
only associated with the label in the training distribution, but not the test distribu-
tion. This problem is usually known as spurious correlation, confounding factors,
or dataset bias. In this paper, we formally study the generalization error bound
for this setup with the knowledge of how the spurious features are associated with
the label. We also compare our analysis to the widely-accepted domain adaptation
error bound and show that our bound can be tighter, with more assumptions that
we consider realistic. Further, our analysis naturally offers a set of solutions for
this problem, linked to established solutions in various topics about robustness in
general, and these solutions all require some understandings of how the spurious
features are associated with the label. Finally, we also briefly discuss a method
that does not require such an understanding.
1	Introduction
Machine learning, especially deep neural networks, has demonstrated remarkable empirical suc-
cesses over various benchmarks. One promising next step is to extend such empirical achievements
beyond i.i.d benchmarks. If we train a model with data from one distribution (i.e., the source dis-
tribution), how can we guarantee the error to be small over other unseen, but related distributions
(i.e., target distributions). Quantifying the generalization error over two arbitrary distributions is not
useful, thus, we require the distributions of study similar but different: being similar in the sense
that there exists a common function that can achieve zero error over both distributions, while being
different in the sense that there exists another different function that can only achieve zero error over
the training distribution, but not the test distribution.
This problem may not be trivial because the empirical risk minimizer (ERM) may lead the model to
learn this second function, a topic studied under different terminologies such as spurious correlations
(Vigen, 2015), confounding factors (McDonald, 2014) or dataset bias (Torralba & Efros, 2011). As
a result, small empirical error may not mean the model learns what we expect (Geirhos et al., 2019;
Wang et al., 2020), thus the model may not be able to perform consistently over other related data.
In particular, our view of the challenges in this topic is illustrated with a toy example in Figure 1
where the model is trained on the source domain data to classify triangle vs. circle and tested on the
target domain data. However, the color coincides with the shape on the source domain, so the model
may learn either the desired function (relying on shape) or the spurious function (relying on color).
The spurious function will not classify the target domain data correctly while the desired function
can, but the ERM cannot differentiate them. As one may expect, whether shape or color is considered
as desired or spurious is subjective dependent on the task or the data, and in general irrelevant to
the statistical nature of the problem. Therefore, our error bound will require the knowledge of the
spurious function. While this is a toy example, this scenario surly exists in real world tasks (e.g., Jo
& Bengio, 2017; Geirhos et al., 2019; Wang et al., 2020). The contributions of this paper are:
•	We analyze the cross-distribution generalization error bound of a model when the model is
trained with a distribution with spuriously correlated features, which is formalized as the
main theorem of this paper.
1
Under review as a conference paper at ICLR 2021
Figure 1: A toy example of the main problem focused in this paper.
•	We compare our bound to the widely-accepted domain adaptation one (Ben-David et al.,
2010) and show that our bound can be tighter under assumptions that we consider realistic.
•	Our main theorem naturally offers principled solutions of this problem, and the solutions
are linked to many previous established methods for robustness in a broader context.
•	As the principled solutions all require some knowledge of the task or the data, our main
theorem also leads to a new heuristic absent of the knowledge. This new method may be
on a par with the principled solutions, and can outperform the vanilla training empirically.
2	Related Work
There is a rich history of learning robust models. We first discuss works in three topics, all centering
around the concept of invariance, where invariance intuitively means the model’s prediction pre-
serves under certain shift of the data. We then highlight works related to our theoretical discussion.
Cross-domain Generalization This line of works probably originates from domain adaptation
(Ben-David et al., 2007), which studies the problem of training a model over one distribution and test
it over another one. Since (Ganin et al., 2016), recent advances along this topic mainly center around
the concept of invariance: most techniques leverage different regularizers to learn representations
that are invariant to the marginals of these two distributions (e.g., Ghifary et al., 2016; Rozantsev
et al., 2018). Further, the community aims beyond the situation that a trained model from domain
adaptation may only be applicable to one distribution, and focuses on domain generalization (Muan-
det et al., 2013), which studies the problem of training a model over a collection of distributions and
test it with distributions unseen during training. Similarly, most recent methods aim to learn repre-
sentations invariant to the marginals of the training distributions (e.g., Motiian et al., 2017; Li et al.,
2018; Carlucci et al., 2018). Recently, the community extends the study to domain generalization
without domain IDs to address the real-world situations that domain IDs are unavailable (Wang et al.,
2019b), which again focuses on learning representations invariant to specifically designed functions.
Adversarially Robust Models The study of robustness against adversarial examples was popular-
ized by the empirical observations that small perturbations on image data can significantly alter the
model’s prediction (Szegedy et al., 2013; Goodfellow et al., 2015). This observation initiated a line
of works building models invariant to such small perturbations (the rigorous definitions of “small
perturbations” will not be discussed in details here) (e.g., Lee et al., 2017; Akhtar et al., 2018) and
adversarial training (Madry et al., 2018) is currently the most widely-accepted method in terms of
empirical defense. On the other hand, the community also aims to develop methods that are prov-
ably robust to predefined perturbations (e.g., Wong & Kolter, 2018; Croce & Hein, 2020), which
links back to the works of distributional robust models (e.g., Abadeh et al., 2015; Sagawa* et al.,
2020), whose central goal is to train models invariant to a predefined shift of distributions. Recent
evidence shows key challenges of learning adversarially robust models are spuriously correlated
features (Ilyas et al., 2019; Wang et al., 2020), connecting adversarial robustness to the next topic.
Countering Spurious Correlation Works along this line usually connects the robustness ofa model
to its ability of ignoring the spurious correlation in the data, which was also studied under the
terminologies of confounding factors, or dataset bias. With different concrete definitions of the
spurious correlation, methods have been developed for various applications, such as image/video
classification (e.g., Goyal et al., 2017; Wang et al., 2019a;b; Bahng et al., 2019; Shi et al., 2020),
text classification (e.g., He et al., 2019; Clark et al., 2019; Bras et al., 2020; Zhou & Bansal, 2020;
2
Under review as a conference paper at ICLR 2021
Ko et al., 2020), medical diagnosis (e.g., Zech et al., 2018; Chaibub Neto et al., 2019; Larrazabal
et al., 2020) etc. The key concept is, as expected, to be invariant to the spurious correlated features.
Related Theoretical Discussion Out of a rich collection of theoretical discussions in learning robust
models, we only focus on the ones for unsupervised domain adaptation, as they will be related to our
discussions in the sequel. Popularized by (Ben-David et al., 2007; 2010), these analyses, although
in various forms (Mansour et al., 2009; Germain et al., 2016; Zhang et al., 2019; Dhouib et al.,
2020), mostly involve two additional terms than standard machine learning generalization bound:
one term describes the “learnable” nature of the problem and one term quantifies the differences of
the two distributions. This second term probably inspired most of the empirical methods forcing
invariant representations from distributions. However, the value of invariance is recently challenged
(Wu et al., 2019; Zhao et al., 2019). For example, Zhao et al. (2019) argued that “invariance is
not sufficient” by showing counter examples violating the “learnable” nature of the problem and
formalized the understanding as that the two distributions have possibly different labeling functions.
Key Difference: However, we find the argument of disparity in labeling functions less intuitive,
because human will nonetheless be able to agree on the label of an object whichever distribution
the object lies in: in the context of this paper, we argue a shared labeling function always exists (in
any task reasonable to human), but the ERM model may not have the incentive to learn this function
and learns a spurious one instead. As in Figure 1, we formalize the problem as learning against
spurious functions, and argue that the central problem is still invariance, but instead of invariance to
marginals, we urge for invariance to the spurious function. Our discussion also applies to more than
unsupervised domain adaptation and relates to most of the topics discussed in this section.
3	Generalization Understanding with Spurious Correlation
3.1	Notations & Background
We consider a binary classification problem from feature space X ∈ Rp to label space Y ∈ {0, 1}.
The distribution over X is denoted as P. A labeling function f : X → Y is a function that maps
feature x to its label y. A hypothesis or model θ : X → Y is also a function that maps feature to the
label. The difference in naming is only because we want to differentiate whether the function is a
natural property of the space or distribution (thus called a labeling function) or a function to estimate
(thus called a hypothesis or model). The hypothesis space is denoted as Θ. This work concerns with
the generalization error across two distributions, namely source and target distribution, denoted as
Ps and Pt respectively. As stated previously, we are only interested when these two distributions
are similar but different: being similar means there exists a desired labeling function, fd , that maps
any x ∈ X to its label (thus the label y := fd(x)); being different means there exists a spurious
labeling function, fp, that for any X 〜 P§, fp(x) = fd(x). This “similar but different" property
will be reiterated as an assumption (A2) later. We use (x, y) to denote a sample, and use (X, Y)P
to denote a finite dataset if the features are drawn from P. We use P (θ) to denote the expected risk
of θ over distribution P, and use ∙ to denote estimated term ∙ (e.g., the empirical risk is ep(θ)). We
use l(∙, ∙) to denote a generic loss function. For a dataset (X, Y)p, if We train a model
θb = arg min	l(θ(X), y),	(1)
θ∈Θ (x,y)∈(X,Y)P
previous generalization study suggests that We can expect the error rate to be bounded as
ep(θ) ≤ bp(θ) + Φ(∣θ∣,n,δ),	⑵
Where P (θ) and bP(θ) respectively are
1
ep(θ) = Ex〜P∣θ(x)- y| = Ex〜P∣b(x)- fd(x)∣ and ①(θ) = n E	lb(x) - y|,
n (x,y)∈(X,Y)P
and φ(∣θ∣, n, δ) is a function of hypothesis space ∣Θ∣, number of samples n, and δ accounts for the
probability When the bound holds. This paper only concerns With this generic form that can subsume
many discussions, each With its oWn assumptions. We refer to these assumptions as A1.
A1: basic assumptions needed to derived (2), formalized With tWo examples in appendix A.1.
3
Under review as a conference paper at ICLR 2021
3.2 Generalization Understanding with Spurious Correlation
Our interest lies in more than the expected performance over samples from the same distribution,
but over a different distribution that shares the same labeling function. As we argue previously, the
key difficulty in learning a robust model is the existence of the extra labeling function for features
sampled from Ps. More formally, we have our first assumption describing this problem
A2: Existence of Spurious Correlation: For any x ∈ X, y := fd(x). We also have a fp that
is different from fd, and for X 〜 P§, fd(x) = fp(x).
Thus, θ that learns either fd or fp will lead to small source error, but only θ that learns fd will lead
to small target error. Note that fp may not exist for an arbitrary Ps . In other words, A2 can be
interpreted as a regulation to Ps so that fp, while being different from fd, exists for any X 〜Ps.
In this problem, fp and fd are not the same despite fp(x) = fd(x) for any X 〜Ps, and We consider
the differences lie in the features they use. To describe this difference, we introduce the notation
A(∙, ∙), which denotes a set parametrized by the labeling function and the sample, to describe the
active set of features that are used by the labeling function. By active set, we refer to the minimum
set of features that a labeling function requires to map a sample to its label. Formally, we define
A(f, x) =	arg min	∣αχ (z)|,	(3)
z∈X,f (z)=f (x)
where αχ(z) = {i∣Zi = Xi} is the set of indices by which Z and X are the same, and | ∙ | measures
the cardinality. Although fp(X) = fd(X), A(fp, X) and A(fd, X) can be different.
Further, we define a new function difference given a sample as
dχ(θ,f) =	m max	∣θ(z) - f(z)l,	(4)
z∈X:zA(f,x)=xA(f,x)
where XA(f,x) denotes the features ofX indexed by A(f, X). In other words, the distance describes:
given a sample X, what is the maximum disagreement of the two functions θ and f for all the other
data z ∈ X with a constraint that the features indexed by A(f, X) are the same as those ofX. Notice
that this difference is not symmetric, as the active set is determined by the second function. By
definition, we have dχ(θ, f) ≥ ∣θ(x) - f (x)|.
We introduce two more assumptions:
A3: Separable Labeling Functions: For any X ∈ X, A(fd, x) ∩ A(fp, x) = 0
A4: Realized Hypothesis: Given a large enough hypothesis space Θ, for any sample (X, y),
for any θ ∈ Θ, which is not a constant mapping, if θ(X) = y, then dx(θ, fd)dx(θ, fp) = 0
Intuitively, A3 assumes the active sets of the two labeling functions do not overlap. A4 assumes a θ
at least learns one labeling function for the sample X if θ can map the X correctly.
Finally, we use the term
r(θ, A(f, x)) = maχ	∣Θ(x) - y|	(5)
xA(f,x) ∈XA(f,x)
to describe how θ depends on the active set of f. Notice that r(θ, A(f, X)) = 1 alone does not
mean θ depends on the active set of f, it only means so when we also have θ(X) = y (see formal
discussion in Lemma B.1). With all above, we formalize a new generalization bound as follows:
Theorem 3.1 (The Curse of Universal Approximation). With Assumptions A1-A4, with probability
as least 1 - δ, we have
俎⑻ ≤ bPs ⑻ + c(θ) + φ(Iθl,n,δ)	⑹
where C⑻=1 P(x,y)∈(X,Y)ps I[θ(X) = y]r(θ, A(Zp, X))∙
IH is a function that returns 1 if the condition ∙ holds true, and 0 otherwise. As θ may learn fp,
bPs (θ) can no longer alone indicate Pt (θ), thus we introduce c(θ) to account for the discrepancy.
Intuitively, c(θ) quantifies the samples that are correctly predicted, but only because the θ learns fp
for that sample. c(θ) critically depends on the knowledge of fp.
4
Under review as a conference paper at ICLR 2021
We name Theorem 3.1 the curse of universal approximation to highlight the fact that the existence
of fp is not always obvious, but the models can usually learn it nonetheless. For example, Ilyas et al.
(2019) suggest the root to the performance drop over adversarial examples are spurious features,
and Wang et al. (2020) demonstrate the existence of human-imperceptible high-frequency spurious
signals in image datasets, which may explain several generalization issues of the models. In other
words, even in a well-curated dataset that does not seemingly have spurious correlated features,
modern machine learning models may still use some spurious features not understood by human,
leading to non-robust behaviors when tested over other datasets that human consider similar. This
argument may also align with recent discussions suggesting that reducing the model complexity can
improve cross-domain generalization (Chuang et al., 2020).
3.3 In Comparison to the View of Domain Adaptation
We further compare Theorem 3.1 with established understandings of domain adaptation. We sum-
marize the several domain adaptation understandings in the following form (see details in §2):
EPt (θ) ≤ bps (θ) + Dθ(Ps, Pt) + λ + φ0(∣θ∣, n, δ)	⑺
where DΘ(Ps, Pt) quantifies the differences of the two distributions, and λ describes the nature of
the problem, and usually involves non-estimable terms about the problem or the distributions.
For example, Ben-David et al. (2010) formalize the difference as H-divergence, and describe the
corresponding empirical term as (Θ∆Θ is the set of disagreement between two hypotheses in Θ):
Dee,Pt) = ι- min(1 χ i[χ∈(X,Y" + 1 X i[χ∈(χ,Y"),⑻
θ∈Θ∆Θ n	n
x：e(x)=o	x：e(x)=i
where m denotes the number of unlabelled samples in Ps and Pt each. λ = bPt (θ?) + bPs (θ?),
where θ? = arg minθ∈Θ bPt (θ) + bPs (θ),
In our formalization, as we assume the fd applies to any χ ∈ X (according to A2), λ = 0 as long
as the hypothesis space is large enough. Therefore, the difference mainly lies in the comparison
between c(θ) and DΘ(Ps, Pt). To compare them, we need an extra assumption:
A5: Sufficiency of Training Samples for the two finite datasets in the study, i.e., (X, Y)Ps
and (X, Y)Pt, for any χ ∈ (X, Y)Pt, there exists one or many z ∈ (X, Y)Ps such that
X ∈ {χ0lχ0 ∈ X and xA(fd,z) = zA(fd,z)}	⑼
A5 intuitively means the finite training dataset needs to be diverse enough to describe the concept
that needs to be learned. For example, imagine building a classifier to classify mammals vs. fishes
from the distribution of photos to the distribution of sketches, we cannot expect the classifier to do
anything good on dolphins if dolphins only appear in the test sketch dataset. A5 intuitively regulates
that if dolphins will appear in the test sketch dataset, they must also appear in the training dataset.
Now, in comparison to (Ben-David et al., 2010), we have
Theorem 3.2. With Assumptions A2-A5, and if 1 - fd ∈ Θ, we have
c(θ) ≤ Dθ(Ps, Pt) + n X	I[θ(χ) = y]r(θ, A(fp, x))	(10)
n (x,y)∈(X,Y)Pt
where c(θ) = ɪ P(X y)∈(χ Y)P I[θ(χ) = y]r(θ, A(fp, χ)) and De(Ps, Pt) is defined as in (8).
The comparison involves an extra term, q(θ) := n P(χy)∈(χ γ)p I[θ(χ) = y]r(θ, A(fp, χ)),
which intuitively means that if θ learns fp , how many samples θ can coincidentally predict correctly
over the finite target set used to estimate DΘ (Ps, Pt). For sanity check, if we replace (X, Y)Pt
with (X, Y)Ps, DΘ(Ps, Pt) will be evaluated at 0 as it cannot differentiate two identical datasets,
and q(θ) will be the same as c(θ). On the other hand, if no samples from (X, Y)Pt can be mapped
correctly with fp (coincidentally), q(θ) = 0 and c(θ) will be a lower bound of DΘ(Ps, Pt).
The value of Theorem 3.2 lies in the fact that for an arbitrary target dataset (X, Y)Pt, no samples out
of which can be predicted correctly by learning fp (a situation likely to occur for arbitrary datasets),
c(θ) will always be a lower bound of DΘ (Ps , Pt).
5
Under review as a conference paper at ICLR 2021
Further, when Assumption A5 does not hold, we are unable to derive a clear relationship between
c(θ) and DΘ(Ps, Pt). The difference is mainly raised as a matter of fact that, intuitively, we are
only interested in the problems that are “solvable” (A5, i.e., hypothesis that used to reduce the test
error in target distribution can be learned from the finite training samples) but “hard to solve” (A2,
i.e., another labeling function, namely fp, exists for features sampled from the source distribution
only), while DΘ (Ps , Pt) estimates the divergence of two arbitrary distributions.
Estimation of c(θ). Finally, due to the limitation of space, we discuss the estimation of c(θ) in
appendix A.2. In short, in practice, while we do not know fp, we usually know A(fp , x) through
intuition or common sense, such as texture or background of images. Thus, the estimation is to test
the whether the model switches its correct prediction when these features are perturbed over the
possible space. Also, the search can be terminated once r(θ, A(fp, x)) is evaluated as 1. As one
may be aware of, this process is widely known as adversarial attack (e.g., Goodfellow et al., 2015).
4 Methods to Learn Robust Models
In this section, we will take advantage of our analysis to introduce methods that can be used to learn
robust models by countering the spurious correlation. First, we discuss the principled solutions that
can lead to the error bound in Theorem 3.1 to be smaller. These methods are interestingly linked to
many previously established methods in robustness in general. Second, as all above methods will
require some knowledge of fp or A(fp, x), we also explore a new method that does not require so.
4.1 Principled Solutions of Learning Robust Models
According to Theorem 3.1, a key to learning robust model is to reduce c(θ). However, for the
convenience during training, we can consider its upper bound
1
c(θ) ≤ 1 X	r(θ, A(fp, x))
n (x,y)∈(X,Y)
n
E	max	∣θ(x) - y|,	(11)
(x,y)∈(X,Y) xA(fp,x) ∈XA(fp,x)
which intuitively means that instead of c(θ) that concerns with the correct prediction based on only
fp , now we study any prediction based on only fp .
Also, as We introduced in “estimation of c(θ)"： although We barely know fp in practice, We usually
directly know A(fp, x) through the intuition or common sense of the data or the task.
Adversarially robust models (worst-case data augmentation) The most direct approach of learn-
ing robust models Will be optimizing to reduce (11) in addition to the generic loss (i.e., l(θ(x), y))
of a model. Further, as ∣θ(x) - y| ≤ maXxA(fp,x)exA(fp,x)∣θ(x) - y|, we can drop the generic loss
term, and directly train a model With
min —	max	l(θ(x), y),
θ n (x,y)∈X,Y) xA(fP,χ)∈xA(fP,χ)
(12)
which is one of the most widely used methods in adversarial robust literature: the adversarial training
(Madry et al., 2018), as well as worst-case data augmentation (Fawzi et al., 2016).
Data augmentation Alternatively, we can assume
ExA(f X)〜p%,	Jθ(X) - y| = max lθ(X) - y|,
A(fp,x)	A(fP,x)	xA(fp,x)∈XA(fp,x)
(13)
where Pall denotes the distribution that can help to remove the correlation between fp related fea-
tures and y (e.g., a uniform distribution over X is sufficient, but Ps will not be good enough). The
main strategy is to train with the samples whose fp related features are randomized, so that the
model is expected to learn to ignore the pattern. As x can usually be sampled by Pall, one can drop
the generic loss and train a model with
mθin Id ……A 3ι(θ(x),y).
(14)
6
Under review as a conference paper at ICLR 2021
Regularizing hypothesis space We can also consider to find a Θregularized so that
∣θ(x) - y| = maχ	∣θ(x) - y| for θ ∈ Θregularized,	(15)
xA(fp,x) ∈XA(fp,x)
which intuitively means that for any θ ∈ Θregularized, θ ignores the information from A(fp, x). There
is a proliferation of recent developments of this thread, either through intuitive understanding of the
task or the data (e.g., Wang et al., 2019a;b; Bahng et al., 2019) or through theoretical understanding
for rigorously defined perturbations (e.g., Abadeh et al., 2015; Cranko et al., 2020).
4.2 Learning Robust Models with Minimum Supervision
While our analysis suggests that we cannot have a robust model without the knowledge of fb, we
continue to ask that what the best we can do without such knowledge. If we use F to denote the set
{fd , fp } and use i to index its element, we can have the following upper bound to optimize
c	(θ) ≤ 1 X	r(θ, A(fp, x)) ≤ 1 X	Xr(θ, A(Fi, x)).	(16)
n (x,y)∈(X,Y)	n (x,y)∈(X,Y) i
By optimizing the RHS of (16), we aim to discourage the learning towards functions that only rely
on one labelling function for each sample for any labelling functions. Intuitively, the method is to
encourage the model’s usage in all possible features (either associated with fd or fp), thus the model
may be more robust to the changes of features when dealing with perturbations of the data.
A model using all the features is not expected to be better than a method that only uses the fd
associated features. However, as all methods in §4.1 require specific knowledge of fp, we argue this
is a better practice than vanilla training (1) when there are no side information about either fd or fp .
In practice, as we do not have the knowledge of F, we can use the estimated θ from previous
iteration as a substitute, also as the searching for A(f, x) can be computationally expensive, we use
the gradient information to guide the selection of the features. In summary, this new method, which
we name minimum supervision (MS), has the following three major steps at iteration t:
•	Use θ(t-1) as a substitute of either fd or fp.
•	Identify A(θ(t-1), x) with top ρ fraction of features according to the gradient.
•	Sample xA(θ(t-1),x) over XA(θ(t-1),x) and continue to train the model.
Thus, the new method has a hyperparameter ρ. Due to the limitation of space, we discuss the detailed
algorithm and other practical aspects of the method in appendix A.3.
5	Experiments
The experiments fall into two scenarios: we first use the two binary classification experiments to
support Theorem 3.1 and 3.2 (§5.1); we then test how the new method we introduced in comparison
to previously developed methods explicitly using the knowledge of fp (§5.2).
5.1	Theory Supporting Experiment
Synthetic Data with Spurious Correlation We also Figure 1 to data with p features, where p/2
features are related to fd, p/4 features are related to fp, and the rest p/4 features are independent
on labels. Also, fd is a non-linear function, and fp is simpler. We test across multiple choices.
Overall, the results suggest (1) minimum supervision works better than the vanilla method; (2) c(θ)
is a tighter estimation of the test error than DΘ(Ps, Pt). Details are in appendix C.1.
Binary Digit Classification over Transferable Adversarial Examples We also verify the theoret-
ical discussion through a binary digit classification experiment, where the train and validation set
are digits 0 and 1 from MNIST train and validation dataset. To create the test set, we first estimate
a model, and perform adversarial attacks over this model for the test samples with five adversarial
attack methods (C&W (Carlini & Wagner, 2017), DeepFool (Moosavi-Dezfooli et al., 2016), FGSM
7
Under review as a conference paper at ICLR 2021
■ ⅛,(β)	- ep,(β) ■ ep,(β)+c(β)	- ep,(β) +De
C&W	DeepFooI	FGSM	∣ ∣ SalJ&Pepper ∣ ∣ ∣ ∣ SiPglePilel
R：j ⅛ ⅛ Ii )∣:: Iirr⅛⅞ Ii
„	„	„	0*s
«■«
：：	：；	：；	o»
Illi Illi I
00 . , 0∙0^ .... 0∙0^ .... 000
Vanllla MS AT Aug Vanllla MS AT Aug Vanllla MS AT Aug Vanllla MS AT Aug Vanllla MS AT Aug
Figure 2: Binary MNIST classification error and estimated bounds. Each panel represents one out-
of-domain data generated through an attack method. Four methods are reported in each panel.
For each method, four bars are plotted: from left to right, bPs (θ), bPt (θ), bPs (θ) + c(θ), and
bPs (θ) + DΘ. Some bars are not visible because the values are small.
	Vanilla?	StylisedIN	LearnedMixin	RUBi	ReBias	MS?
Standard Accuracy	90.8	88.4	67.9	90.5	91.9	92.1
Weighted Accuracy	88.8	86.6	65.9	88.6	90.5	91.3
ImageNet-A	24.9	24.6	18.8	27.7	29.6	29.3
ImageNet-Sketch	41.1	40.5	36.8	42.3	41.8	42.9
Table 1: Results comparison on 9 super-class ImageNet classification. ? Only Vanilla and MS do not
leverage any knowledge of the spurious features.
(Goodfellow et al., 2015), Salt&Pepper (Rauber et al., 2017), and SinglePixel (Rauber et al., 2017)).
These adversarially generated examples are considered as the test set from another distribution.
An advantage of this setup is that we can have fp well defined as 1 - fadv , where the fadv is the
function each adversarial attack relies on. Thus, we can assess our analysis on image classification.
We train the models with vanilla method, minimum supervision method (MS), adversarial training
(AT), and data augmentation (Aug). In addition to the training error (i.e., bPs (θ)) and test error (i.e.,
bPt (θ)), we also report bPs (θ) + c(θ) and bPs (θ) + DΘ. Our results in Figure 2 again align with our
analysis: (1) minimum supervision outperforms the vanilla method, but may be inferior to methods
using fp explicitly; (2) c(θ) is often a tighter estimation of the test error than DΘ(Ps, Pt).
5.2	Real Image Classification
Finally, we conduct a real-image classification to compare whether our minimum supervision
method can be compared to other advanced methods in a more challenging and realistic setting.
We follow the setup in (Bahng et al., 2019) and compare the models for a 9 super-class ImageNet
classification (Ilyas et al., 2019) with class balanced strategies. Also, we follow (Bahng et al., 2019)
to report standard accuracy, weighted accuracy, a scenario where samples with unusual texture are
weighted more, and accuracy over ImageNet-A (Hendrycks et al., 2019), a collection of failure cases
for most ImageNet trained models. Additionally, we also report the performance over ImageNet-
Sketch (Wang et al., 2019a), an independently collected ImageNet test set with only sketch images.
We report the results in Table 1. Our method (MS) outperforms other methods in most situations,
which we consider impressive since only MS and vanilla methods are not using any knowledge of fp
or fd . More details of the experiment setup and competing methods are discussed in appendix C.2.
6	Conclusion
In this paper, we formalized the generalization error when the models can use some spurious features
in the training set that are not shared in the test set, a problem widely studied under the terminologies
of spurious correlation, confounding factors, or dataset bias. We formalized a new generalization
error bound, and compared our bound to the well-established domain adaptation one. More impor-
tantly, our theorem naturally offers a set of principled solutions for this problem. These principled
solutions are linked to many previous methods for robustness in a broader context. Since all these
principled solutions require some knowledge of the spurious correlated features, we also leveraged
our theorem to develop a new method that does not require such knowledge.
8
Under review as a conference paper at ICLR 2021
References
Soroosh Shafieezadeh Abadeh, Peyman Mohajerin Mohajerin Esfahani, and Daniel Kuhn. Distri-
butionally robust logistic regression. In Advances in Neural Information Processing Systems, pp.
1576-1584, 2015.
Naveed Akhtar, Jian Liu, and Ajmal Mian. Defense against universal adversarial perturbations. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3389-
3398, 2018.
Hyojin Bahng, Sanghyuk Chun, Sangdoo Yun, Jaegul Choo, and Seong Joon Oh. Learning de-biased
representations with biased representations. arXiv preprint arXiv:1910.02806, 2019.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
Olivier Bousquet, StePhane Boucheron, and Gabor Lugosi. Introduction to statistical learning the-
ory. In Summer School on Machine Learning, pp. 169-207. Springer, 2003.
Ronan Le Bras, Swabha SwayamdiPta, Chandra Bhagavatula, Rowan Zellers, Matthew E Pe-
ters, Ashish Sabharwal, and Yejin Choi. Adversarial filters of dataset biases. arXiv preprint
arXiv:2002.04108, 2020.
Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal
biases for visual question answering. In Advances in neural information processing systems, PP.
841-852, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), PP. 39-57. IEEE, 2017.
Fabio M Carlucci, Paolo Russo, Tatiana Tommasi, and Barbara CaPuto. Agnostic domain general-
ization. arXiv preprint arXiv:1808.01102, 2018.
Elias Chaibub Neto, Abhishek PrataP, Thanneer M Perumal, Meghasyam Tummalacherla, Brian M
Bot, Lara Mangravite, and Larsson Omberg. A Permutation aPProach to assess confounding
in machine learning aPPlications for digital health. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, PP. 54-64, 2019.
Ching-Yao Chuang, Antonio Torralba, and Stefanie Jegelka. Estimating generalization under distri-
bution shifts via domain-invariant rePresentations. arXiv preprint arXiv:2007.03511, 2020.
ChristoPher Clark, Mark Yatskar, and Luke Zettlemoyer. Don’t take the easy way out: Ensemble
based methods for avoiding known dataset biases. arXiv preprint arXiv:1909.03683, 2019.
Zac Cranko, Zhan Shi, Xinhua Zhang, Richard Nock, and Simon Kornblith. Generalised liPschitz
regularisation equals distributional robustness. arXiv preprint arXiv:2002.04197, 2020.
Francesco Croce and Matthias Hein. Provable robustness against all adversarial lp-Perturbations for
p ≥ 1. In International Conference on Learning Representations, 2020.
Sofien Dhouib, Ievgen Redko, and Carole Lartizien. Margin-aware adversarial domain adaPtation
with oPtimal transPort. In Thirty-seventh International Conference on Machine Learning, 2020.
Alhussein Fawzi, Horst Samulowitz, DeePak S. Turaga, and Pascal Frossard. AdaPtive data aug-
mentation for image classification. In 2016 IEEE International Conference on Image Processing,
ICIP 2016, Phoenix, AZ, USA, September 25-28, 2016, PP. 3688-3692. IEEE, 2016.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor S. LemPitsky. Domain-adversarial training of neural
networks. J. Mach. Learn. Res., 17:59:1-59:35, 2016.
9
Under review as a conference paper at ICLR 2021
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias im-
proves accuracy and robustness. In International Conference on Learning Representations, 2019.
Pascal Germain, Amaury Habrard, Francois Laviolette, and Emilie Morvant. A new pac-bayesian
perspective on domain adaptation. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.),
Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York
City, NY, USA, June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings,
pp. 859-868.JMLR.org, 2016.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep
reconstruction-classification networks for unsupervised domain adaptation. In European Con-
ference on Computer Vision, pp. 597-613. Springer, 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples (2014). In International Conference on Learning Representations, 2015.
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6904-6913, 2017.
He He, Sheng Zha, and Haohan Wang. Unlearn dataset bias in natural language inference by fitting
the residual. arXiv preprint arXiv:1908.10763, 2019.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. arXiv preprint arXiv:1907.07174, 2019.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp. 125-136, 2019.
Jason Jo and Yoshua Bengio. Measuring the tendency of cnns to learn surface statistical regularities.
CoRR, abs/1711.11561, 2017. URL http://arxiv.org/abs/1711.11561.
Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first
sentence: Position bias in question answering. arXiv preprint arXiv:2004.14602, 2020.
Agostina J Larrazabal, Nicolas Nieto, Victoria Peterson, Diego H Milone, and Enzo Ferrante. Gen-
der imbalance in medical imaging datasets produces biased classifiers for computer-aided diag-
nosis. Proceedings of the National Academy of Sciences, 117(23):12592-12594, 2020.
Hyeungill Lee, Sungyeob Han, and Jungwoo Lee. Generative adversarial trainer: Defense to adver-
sarial perturbations with gan. arXiv preprint arXiv:1705.03387, 2017.
Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial
feature learning. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit.(CVPR), 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. In 6th International Conference on
Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Confer-
ence Track Proceedings. OpenReview.net, 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. In COLT 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec,
Canada, June 18-21, 2009, 2009.
JH McDonald. Confounding variables. Handbook of biological statistics, 3rd edn. Sparky House
Publishing, Baltimore, pp. 24-28, 2014.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and
accurate method to fool deep neural networks. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2574-2582, 2016.
10
Under review as a conference paper at ICLR 2021
Saeid Motiian, Marco Piccirilli, Donald A Adjeroh, and Gianfranco Doretto. Unified deep super-
vised domain adaptation and generalization. In The IEEE International Conference on Computer
Vision (ICCV), volume 2, pp. 3, 2017.
Krikamol Muandet, David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pp. 10-18, 2013.
Jonas Rauber, Wieland Brendel, and Matthias Bethge. Foolbox: A python toolbox to benchmark the
robustness of machine learning models, 2017.
Artem Rozantsev, Mathieu Salzmann, and Pascal Fua. Beyond sharing weights for deep domain
adaptation. IEEE transactions on pattern analysis and machine intelligence, 41(4):801-814,
2018.
Shiori Sagawa*, Pang Wei Koh*, Tatsunori B. Hashimoto, and Percy Liang. Distributionally robust
neural networks. In International Conference on Learning Representations, 2020.
Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, and Jingdong Wang. Infor-
mative dropout for robust representation learning: A shape-bias perspective. arXiv preprint
arXiv:2008.04254, 2020.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pp. 1521-1528.
IEEE, 2011.
Tyler Vigen. Spurious correlations. Hachette books, 2015.
Haohan Wang, Songwei Ge, Zachary C. Lipton, and Eric P. Xing. Learning robust global repre-
sentations by penalizing local predictive power. In Hanna M. Wallach, Hugo Larochelle, Alina
Beygelzimer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neu-
ral Information Processing Systems 32: Annual Conference on Neural Information Processing
Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 10506-10518,
2019a.
Haohan Wang, Zexue He, Zachary C. Lipton, and Eric P. Xing. Learning robust representations by
projecting superficial statistics out. In 7th International Conference on Learning Representations,
ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019b.
Haohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing. High-frequency component helps explain
the generalization of convolutional neural networks. In Proceedings ofthe IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 8684-8694, 2020.
Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer
adversarial polytope. In International Conference on Machine Learning, pp. 5286-5295. PMLR,
2018.
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. In International Conference on Machine Learn-
ing, pp. 6872-6881, 2019.
John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric K
Oermann. Confounding variables can degrade generalization performance of radiological deep
learning models. arXiv preprint arXiv:1807.00431, 2018.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I. Jordan. Bridging theory and algorithm
for domain adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long
Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 7404-
7413. PMLR, 2019.
11
Under review as a conference paper at ICLR 2021
Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J. Gordon. On learning invariant
representations for domain adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June
2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research,
pp. 7523-7532. PMLR, 2019.
Xiang Zhou and Mohit Bansal. Towards robustifying nli models against lexical dataset biases. arXiv
preprint arXiv:2005.04732, 2020.
12
Under review as a conference paper at ICLR 2021
A Supporting Discussions
A.1 Concrete Examples of Generic Generalization Bound
•	when A1 is “Θ is finite, l(∙, ∙) is a zero-one loss, samples are i.i.d”, φ(∣θ∣, n, δ) =
p(log(∣θ∣)+log(1∕δ))∕2n
•	when A1 is “samples are i.i.d”, φ(∣Θ∣,n,δ) = 2R(L) + P(log 1∕δ)∕2n, where R(L)
stands for Rademacher complexity and L = {lθ | θ ∈ Θ}, where lθ is the loss function
corresponding to θ .
For more information or more concrete examples of the generic term, one can refer to relevant
textbooks such as (Bousquet et al., 2003).
A.2 ESTIMATION OF c(θ)
The estimation of c(θ) mainly involves two difficulties: the knowledge of fp and the computational
cost of the search over the entire space X. The first difficulty is usually resolved with intuition
or common sense of the data or the task: in practice, we usually directly have the knowledge of
A(fp , x), i.e., the spuriously correlated features that fp relies on, such as texture of images. There-
fore, the estimation becomes a process to test the whether the model will switch its correct prediction
when these features are perturbed over the possible space. The second difficulty can be alleviated
due to the fact that the search can be terminated once r(θ, A(f, x)) is evaluated as 1. As one may be
aware of, this process of searching the entire space with perturbations allowed in a predefined scope
to test the model’s worst possible prediction for a sample x is widely known as adversarial attack
(Goodfellow et al., 2015). These techniques also usually leverage the knowledge of the model’s
gradient to accelerate the searching process.
While adversarial attack can offer a fairly accurate estimation of c(θ), it usually requires heavy
computational efforts. As an alternative strategy, many other literature have tested the models with
some fixed perturbations of the x, or in other words, taking advantage of the fact that
lθ(χ0)- y| ≤ max	lθ(χ)- y| = r(θ,A(f,x)),	where χAfx) ∈XA(f,χ).	(17)
xA(f,x) inXA(f,x)
to test a lower bound of c(θ). There are many works in this thread, and we only list a handful
of examples: Jo & Bengio (2017) leveraged Fourier transform to show that models can capture a
significant amount of texture information, later Geirhos et al. (2019) showed that CNNs trained with
ImageNet are also biased towards texture. With a more concrete definition of the texture, Wang
et al. (2020) demonstrated the models can capture high-frequency signals from images, which also
links the discussion of learning through bias signals to the adversarial vulnerability issue of models
(Ilyas et al., 2019). Similarly, these works mostly depend on a subjective choice ofA(fp, χ), usually
given by the knowledge of the data or the task. Although these works did not directly assess c(θ), θ
usually switched the prediction for sufficient samples to raise an alarm.
A.3 Learning Robust Models with Minimum Supervision in Practice
In practice, as we do not have the knowledge of either fd or fp (F), the strategy we use is to estimate
the model first and consider our estimated model θ as a substitute of the labeling function (either fd
or fp). Therefore, at each iteration t, we will use the θ at the previous iteration to identify the active
set for the optimization of (16) (in main manuscript).
Further, another question is that when we have θbt-1, how to identify A(θbt-1, χ), as searching for
A(θt-1, χ) by the definition can be computationally expensive. Our practical strategy is to use the
t1
gradient of θt-1 to guide the selection of the features. Intuitively, we argue that the the features with
larger absolute values of ∂l(θt-1, χ, y)∕∂θt-1 are the features θbt-1 relies on.
13
Under review as a conference paper at ICLR 2021
Finally, we consider the features with values greater than a threshold τ (ρ, g)} are the features that
are in A(θbt-1, x). The threshold hold is set as the ρth quantile of all the calculated gradients for this
sample. The algorithm is shown in Algorithm 1
Algorithm 1: Learning Robust Models with Minimum Supervision
Result: θτ
Input: T, ρ, (X, Y);
initialize θ0, t = 1, η;
while t ≤ T do
for sample (x, y) do
calculate the gradient g = ∂l(θt-1, x, y)∕∂θt-1;
set the threshold τ(ρ, g) to be the ρth quantile of |g|;
set A(θt-1, x) = {i||gi| ≥ τ (ρ, g)};
sample x0 where x0A(θt-1,x) ∈ XA(θt-1,x);
calculate the gradient g0 = ∂l(θt-1, x0, y)∕∂θt-1;
update the model θt = θt-1 - ηg0
end
end
14
Under review as a conference paper at ICLR 2021
B Proofs of Theoretical Discussions
B.1	Lemma B.1 and Proof
Lemma B.1. With sample (x, y) and two labeling functions f1(x) = f2(x) = y, for an estimated
θ ∈ Θ, if θ(x) = y, then with A3 and A4, we have
dχ(θ,fι) = 1 O r(θ, A(f2, x)) = 1	(18)
xA(f,x) ∈ XA(f,x) denotes that the features of x indexed by A(f, x) are searched in the entire
space.
Proof. If θ(x) = y and dx(θ, f1) = 1, according to A4, we have dx(θ, f2) = 0.
First, we consider one direction dx(θ, f1) = 1 =⇒ r(θ, A(f2, x)) = 1 and we prove this by
contradiction.
If the conclusion does not hold, r(θ, A(f2, x)) = 0, which means
maχ	∣θ(x) - y| =0	(19)
xA(f2 ,x) ∈XA(f2 ,x)
Together with dx(θ, f2 ) = 0, which means
” max	∣θ(z) - y| = 0,	(20)
z∈X :zA(f2，x)=xA(f2，x)
we will have
maχ ∣θ(x) - y| = O,	(21)
x∈X
which is θ(x) = y for any x ∈ P.
This contradicts with the premises in A4 (θ is not a constant function).
Second, we consider the other direction r(θ, A(f2, x)) = 1 =⇒ dx(θ, f1) = 1 and we prove
this by showing its contrapositive proposition holds. (Its contrapositive proposition is dx(θ, f1) =
0 =⇒ r(θ, A(f2, x)) = 0, because, by definitions, r and d can only be evaluated as 0 or 1).
Because of A3 (A(fι, x) ∩ A(f2, x) = 0), We have dχ(θ, fι) ≥ r(θ, A(f2, x)),thus the contrapos-
itive proposition can be shown trivially.	□
B.2	Theorem 3.1 and Proof
Theorem. With Assumptions A1-A4, with probability as least 1 - δ, we have
ePt (θ) ≤ bPs (θ) + c(θ) + Φ(∣θ∣,n,δ)	(22)
where C⑻=：P(x,y)∈(X,Y)ps I[θ(X) = y]r(θ, A(fp, X))∙
Proof.
bPs (θ)=1	X	∣θ(x) - f (x)l	(23)
n
(x,y)∈(X,Y)Ps
=1 - n	X	(I[θ(x)	=	f(x)])	(24)
(x,y)∈(X,Y)Ps
=1 - n	X	(I[θ(x)	=	f (x)]I[dχ(θ,fd)=0]	+ I[θ(x)	= f(x)]I[dχ(θ,fd)	= 1])
(x,y)∈(X,Y)Ps
(25)
=1 - n X	(I[θ(x) = f (x)]I[dχ(θ,fd)=0]) - n X	I[θ(x) = f(x)]I[dχ(θ,fd) = 1]
n (x,y)∈(X,Y)Ps	n (x,y)∈(X,Y)Ps
(26)
=bd(θ) - n X	I[θ(x) = f (x)]r(θ, A(fp, x)),	(27)
n (x,y)∈(X,Y)Ps
15
Under review as a conference paper at ICLR 2021
where the last line used Lemma B.1.
Thus, we have
bd(θ)= b(θ) + 1 X	I[θ(x) = f(x)]r(θ, A(fp, x))	(28)
(x,y)∈(X,Y)Ps
where
bd(θ) = 1- 1 X	(I[θ(x) = f (x)]I[dχ(θ,fd)=0]),	(29)
(x,y)∈(X,Y)Ps
which describes the correctly predicted terms that θ functions the same as fd and all the wrongly pre-
dicted terms. Therefore, conventional generalization analysis through uniform convergence applies,
and we have
EPt (θ) ≤ bd(θ)+ Φ(∣θ∣,n,δ)	(30)
Thus, we have:
EPt (θ) ≤ bPs (θ) + 1 X	I[θ(x) = y]r(θ, A(fp, x))+ Φ(∣θ∣,n,δ)	(31)
n (x,y)∈(X,Y)Ps
□
B.3 Theorem 3.2 and Proof
Theorem. With Assumptions A2-A5, and if1 - fd ∈ Θ, we have
c(θ) ≤ Dθ(Ps, Pt) + n X	I[θ(x) = y]r(θ, A(fp,x))	(32)
n (x,y)∈(X,Y)Pt
where c(θ) = ɪ P(X y)∈(χ Y)P I[θ(x) = y]r(θ, A(fp, x)) and Dθ(Ps, Pt) is defined as in (8).
Proof. By definition, g(x) ∈ Θ∆Θ ^⇒ g(x) = θ(x)㊉ θ0(x) for some θ, θ0 ∈ Θ, together with
Lemma 2 and Lemma 3 of (Ben-David et al., 2010), we have
Dθ(ps, Pt) =1 maxi X	lθ(x)-θ0(X)I-	X	lθ(x)-θ0(X)Il (33)
n θ,θ0∈Θ
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Pt
≥ 1∣	X	Iθ(x) - fz(x)∣-	X	Iθ(x) - fz(x)I∣	(34)
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Pt
=1∣	X	I[θ(x) = y] - X	I[θ(x) = y]∣	(35)
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Pt
=1∣	X	I[θ(x) = y]I[r(θ, A(fp, x)) = 1] - X	I[θ(x) = y]I[r(θ, A(fp, x)) = 1]
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Pt
(36)
+ X	I[θ(x) = y]I[r(θ, A(fp, x)) = 0] - X	I[θ(x) = y]I[r(θ, A(fp, x)) = 0]∣∣
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Pt
(37)
=1∣	X	I[θ(x) = y]r(θ, A(fp, X))- X	I[θ(x) = y]r(θ, A(fp, x))∣
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Pt
(38)
≥c(θ) -	X	I[θ(x) = y]r(θ, A(fp, x))	(39)
(x,y)∈(X,Y)Pt
16
Under review as a conference paper at ICLR 2021
First line: see Lemma 2 and Lemma 3 of (Ben-David et al., 2010).
Second line: if 1 - fd ∈ Θ, and we use fz to denote 1 - fd.
Fifth line is a result of using that fact that
X	I[θ(x) = y]I[r(θ, A(fp, x)) = 0] = X	I[θ(x) = y]I[r(θ, A(fp, x)) = 0]
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Pt
(40)
as a result of our assumptions. Now we present the details of this argument:
According to A4, if θ(x) = y, dx(θ, fd)dx(θ, fp) = 0. Since r(θ, A(fp, x)) = 0, dx(θ, fp) cannot
be 0 unless θ is a constant mapping that maps every sample to 0 (which will contradicts A4). Thus,
we have dx (θ, fd) = 0.
Therefore, we can rewrite the left-hand term following
X	I[θ(x) = y]I[r(θ, A(fp, x)) = 0] = X	I[θ(x) = y]I[dx(θ, fd) = 0]
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Ps
(41)
and similarly
X	I[θ(x) = y]I[r(θ, A(fp, x)) = 0] = X	I[θ(x) = y]I[dx(θ, fd) = 0] (42)
(x,y)∈(X,Y)Pt	(x,y)∈(X,Y)Pt
We recap the definition of dχ(∙, ∙), thus dχ(θ, fd) = 0 means
dχ(θ,fd)=	" max	∣θ(z) - fd(z)l =。	(43)
z∈X:zA(f,x) =xA(fd,x)
Therefore dx (θ, fd) = 0 implies I(θ(x) = y), and
lθ(Z)- fd(Z)I =0 ∀ zA(fd,χ) = xA(fd,x)	(44)
Therefore, we can continue to rewrite the left-hand term following
X	I[θ(x) = y]I[dx(θ, fd) = 0] = X	I[θ(z) - fd(z)] = X	I[θ(x) -fd(x)]
(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Ps	(x,y)∈(X,Y)Ps
(45)
and similarly
X	I[θ(x) = y]I[dx(θ, fd) =0] = X	I[θ(z) -fd(z)]	(46)
(x,y)∈(X,Y)Pt	(x,y)∈(X,Y)Pt
where z denotes any z ∈ X and zA(fd,x) = xA(fd,x).
Further, because of A5, we have
X	I[θ(z)-fd(z)] = X	I[θ(x) - fd(x)].	(47)
(x,y)∈(X,Y)Pt	(x,y)∈(X,Y)Ps
Thus, we showed the (40) holds and conclude our proof.
□
17
Under review as a conference paper at ICLR 2021
C Additional Experiments
C.1 Theoretical Supporting Experiments
Synthetic Data with Spurious Correlation We extend the setup in Figure 1 to generate the syn-
thetic dataset to test our methods. We study a binary classification problem over the data with n
samples and p features, denoted as X ∈ Rn×p . For every training and validation sample i, we
generate feature j as following:
(N(0,1)	if 1 ≤ j ≤ 3p/4
χ(i)〜J N(1,1)	if 3p/4 < j ≤ P, and y⑻=1,	WP P
j	((N (-1, 1) if 3p/4 < j ≤ p, and y(i) = 0, w.p. ρ ,
(N(0,1)	if 3p/4 < j ≤ p, w.p. 1 一 P
In contrast, testing data are simply sampled with Xji)〜N(0,1).
To generate the label for training, validation, and test data, we sample two effect size vectors β1 ∈
Rp/4 and β2 ∈ Rp/4 whose each coefficient is sampled from a Normal distribution. We then
generate two intermediate variables:
CIi)= χ1i,2,…,p∕4β and c2i) = χ1i2,...,p∕4β2
Then we transform these continuous intermediate variables into binary intermediate variables via
Bernoulli sampling with the outcome of the inverse logit function (g-1(∙)) over current responses,
i.e.,
r(1i) = Ber(g-1 (c(1i))) and r(2i) = Ber(g-1(c(2i)))
Finally, the label for sample i is determined as y(i) = I(r(1i) = r(2i)), where I is the function that
returns 1 if the condition holds and 0 otherwise.
Intuitively, we create a dataset of p features, half of the features are generalizable across train,
validation and test datasets through a non-linear decision boundary, one-forth of the features are
independent of the label, and the remaining features are spuriously correlated features: these features
are correlated with the labels in train and validation set, but independent with the label in test dataset.
There are about Cn train and validation samples have the correlated features.
P= 100, c=0.95
140
a.7s
i«a
«.75
g，
025
Vvdia MSSat) MSt=IMe MS⅛=aee) Orade
P= 200, c=0.95




Varig MSsQl) MSq=IMS) MS⅛=aeβ Orade
P= 400, c=0.95
i«a

«.75
2
ɪ- I
OSo
025
Varig MSsQl) MSq=IMS) MS⅛=aeβ Orade



OM
V= 100, c=0.97
«.75
oɪs
l«l
«.75 -
OJO-
oɪs-
OM
l«l
«.75 -
OJO-
oɪs-
⅛.W	■ ⅞,W	■ ⅛,(β)+c(β)	■ ⅛,(tf)+Dβ
LM-
«.75-
aκ-
025-
Vanin ms⅛=aw Ms⅛=araj ms⅛=aw)
P= 200, c=0.97
Vanin ms⅛=aw Ms⅛=araj ms⅛=aw)
P= 400, c=0.97
vanilla MS⅛=o,g) Ms⅛=o.τs) MS⅛=aeβ Oracle

P= 100, c=0.99
P= 100, e=1.0
a.oa
1砌
«.7S
«30
«25
04a
140
«.7S
«30
«25
04a
V√ill> MS⅛=o,g) MStf=ILn) MStf=ILte) Orade
g∙a MS⅛=IU) MS⅛=0.π) MSSLIMe cnde
Figure 3: Results of Synthetic Data with Spurious Correlation. Each panel represents one setting.
Five methods are reported in each panel. For each method, four bars are plotted: from left to right,
bPs(θ),bPt(θ),bPs(θ)+c(θ),andbPs(θ)+DΘ.
Results are reported in Figure 3, where each setup we ran 3 random seeds and report the mean
and standard deviation. We train a vanilla method, minimum supervision method with different
hyperparamter P, and an oracle method that uses data augmentation to randomized the previously
known spurious features. The results show the advantage of the new method consistently, although
still not compared to the method with prior knowledge. We also calculate the c(θ) as we perform
adversarial attacks over the spuriously correlated feature space, we also calculate DΘ as defined in
(8). We compared bPs (θ) + c(θ) and bPs (θ) + DΘ and the results suggest that clearly c(θ) offers a
more accurate assessment of the target error than DΘ.
18
Under review as a conference paper at ICLR 2021
C.2 Real Image Classification: More Details
The main experiment setup follows the setup of (Bahng et al., 2019), and the setup can be con-
veniently replicated by the GitHub repo associated with the paper (Bahng et al., 2019). Although
results of ImageNet-C are also reporeted by (Bahng et al., 2019), their github repo does not pro-
vide the corresponding replication scripts, so we also skip the information. Additionally, we report
another ImageNet level test set that is independently collected, and has only sketch images.
We rename the “bias” and “unbiased” in (Bahng et al., 2019) to “standard accuracy” and “weighted
accuracy” to align the terms we use in this paper and also help to explain the results. Intuitively,
“weighted accuracy” refers to the evaluation mechanism that the test samples with unusual texture
will have more weights.
Again, following the setup in (Bahng et al., 2019), the base network is ResNet, and we compare
with the vanilla network, and several methods that are designed to reduce the texture bias: including
StylisedIN (Geirhos et al., 2019), LearnedMixin (Clark et al., 2019), RUBi (Cadene et al., 2019) and
ReBias (Bahng et al., 2019).
Finally, to get the reported performance, our MS method uses an extra heuristic, such as we only
optimize (16) for half of the batch, and optimize the other batch with the vanilla training (1). Despite
this heuristic used, the main message remains: MS method, as a method that does not use the
knowledge of the spurious correlated features, can compete with the methods that use the knowledge
explicitly.
19