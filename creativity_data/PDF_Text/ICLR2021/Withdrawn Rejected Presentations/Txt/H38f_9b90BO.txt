Under review as a conference paper at ICLR 2021
Towards Robust Graph Neural Networks
against Label Noise
Anonymous authors
Paper under double-blind review
Ab stract
Massive labeled data have been used in training deep neural networks, thus label
noise has become an important issue therein. Although learning with noisy labels
has made great progress on image datasets in recent years, it has not yet been
studied in connection with utilizing GNNs to classify graph nodes. In this paper,
we propose a method, named LPM, to address the problem using Label Propaga-
tion (LP) and Meta learning. Different from previous methods designed for image
datasets, our method is based on a special attribute (label smoothness) of graph-
structured data, i.e., neighboring nodes in a graph tend to have the same label. A
pseudo label is computed from the neighboring labels for each node in the training
set using LP; meta learning is utilized to learn a proper aggregation of the original
and pseudo label as the final label. Experimental results demonstrate that LPM
outperforms state-of-the-art methods in graph node classification task with both
synthetic and real-world label noise. Source code to reproduce all results will be
released.
1	Introduction
Deep Neural Networks (DNNs) have achieved great success in various domains, but the necessity of
collecting large amount of samples with high-quality labels is both expensive and time-consuming.
To address this problem, cheaper alternatives have emerged. For example, the onerous labeling pro-
cess can be completed on some crowdsourced system like Amazon Mechanical Turk 1. Besides,
we can collect labeled samples from web with search engines and social media. However, all these
methods are prone to produce noisy labels of low quality. As is shown in recent research (Zhang
et al., 2016b), an intractable problem is that DNNs can easily overfit to noisy labels, which dramati-
cally degrades the generalization performance. Therefore, it is necessary and urgent to design some
valid methods for solving this problem.
Graph Neural Networks (GNNs) have aroused keen research interest in recent years, which resulted
in rapid progress in graph-structured data analysis (Kipf & Welling, 2016; Velickovic et al., 2017;
Xu et al., 2018; Hou et al., 2019; Wang & Leskovec, 2020). Graph node classification is the most-
common issue in GNNs. However, almost all the previous works about label noise focus on image
classification problem and handling noisy labels in the task of graph node classification with GNNs
has not been studied yet. Fortunately, most edges in the graph-structured datasets are intra-class
edges (Wang & Leskovec, 2020), indicating that a node’s label can be estimated by its neighbor
nodes’ labels. In this paper, we utilize this special attribute of graph data to alleviate the damages
caused by noisy labels. Moreover, meta learning paradigm serves as a useful tool for us to learn a
proper aggregation between origin labels and pseudo labels as the final labels.
The key contributions of this paper are as follows:
•	To the best of our knowledge, we are the first to focus on the label noise existing in utilizing
GNNs to classify graph nodes, which may serve as a beginning for future research towards
robust GNNs against label noise.
•	We utilize meta-learning to learn how to aggregate origin labels and pseudo labels properly
to get more credible supervision instead of learning to re-weight different samples.
1https://www.mturk.com/
1
Under review as a conference paper at ICLR 2021
We experimentally show that our LPM outperforms state-of-the-art algorithms in utilizing GNNs to
classify graph nodes with both synthetic and real-world label noise.
2	Related Work
2.1	Graph Neural Networks
To start, we use G = (V, E, X) to denote a graph whose nodes set is V and edges set is E, and
X ∈ Rn×d is the input feature matrix, where n denotes the number of nodes in the graph and dis the
dimension of the input feature vector of each node. We use eu,v ∈ E to denote the edge that connects
node u and v. For each node v ∈ V, its neighbor nodes set can be donated as Nv = {u : eu,v ∈ E}.
For node classification task, the goal of GNNs is to learn optimal mapping function f (∙) to predict
the class label yv for node v. Generally speaking, GNNs follows a framework including aggregation
and combination in each layer. Different GNNs have proposed different ways of aggregation and
combination. In general, the k-th layer of a GNN reads
a(vk) = Aggregate(k) ({h(uk-1) : u ∈ N (v)}), h(vk) = C ombine(k) (h(vk-1) , a(vk)),	(1)
where h(vk) is the output for k-th layer of node v, h(v0) is the input vector of node v.
2.2	Label Propagation
In Label Propagation (LP), node labels are propagated and aggregated along the edges in the graph
(Zhou et al., 2004; Zhu et al., 2005; Wang & Zhang, 2007; Karasuyama & Mamitsuka, 2013).
There are some works which were designed to improve the performance of label propagation. For
example, Gong et al. (2016) proposed a novel iterative label propagation algorithm which explicitly
optimizes the propagation quality by manipulating the propagation sequence to move from simple
to difficult examples; Zhang et al. (2020) introduces a triple matrix recovery mechanism to remove
noise from the estimated soft labels during propagation. Label propagation has been applied in
semi-supervised image classification task. For example, Gong et al. (2017) used a weighted K-
nearest neighborhood graph to bridge the datapoints so that the label information can be propagated
from the scarce labeled examples to unlabeled examples along the graph edges. Park et al. (2020)
proposed a novel framwork to propagate the label information of the sampled data (reliable) to
adjacent data along a similarity based graph. Compared to these methods, we utilize the intrinsic
graph structure instead of handcrafted graph to propagate clean labels information, which is more
reliable for graph-structured data. Besides, GNNs are utilized by us to extract features and classify
nodes for graph-structured data.
2.3	Meta-learning based Methods against Noisy Labels
Meta-learning aims to learn not only neural networks’ weights, but also itself, such as hand-designed
parameters, optimizer and so on (Andrychowicz et al., 2016; Finn et al., 2017). Several works have
utilized meta-learning paradigm to deal with label noise. For example, Li et al. (2019) has proposed
to find noise-tolerant model parameters by keeping the consistency between the output of teacher
and student networks, and Li et al. (2017b) trains the teacher networks with samples with clean
labels and then transfer the knowledge to student networks so that the student can learn correctly
even if the existence of mislabeled data. Besides, Ren et al. (2018); Jenni & Favaro (2018); Shu
et al. (2019) utilize meta-learning paradigm to re-weight samples, i.e., weight samples with clean
labels more and weight mislabeled samples less. The weighting factors are optimized by gradient
decent or generated by a network to minimizes the loss on a small amount of samples with correct
labels. In contrast, meta-learning paradigm is utilized in this paper to learn how to aggregate origin
labels and pseudo labels properly. We can get more credible supervision by combining the original
label information with the label information provided by LP properly.
2
Under review as a conference paper at ICLR 2021
Figure 1: Illustration of label propagation in our method. The two types of nodes are distinguished
by two colours (blue and green). The nodes surrounded by dotted line are training nodes Dtrain
whose label may be incorrect and those surrounded by solid line are clean sets Dclean In Fig-
ure.1(b), one half of every training node is pseudo label predicted by LP and the other half is original
label. Some nodes, (node 5,7) pseudo labels are the same with their original labels, we select them
Dselect to train GNNs and inject them to clean sets for better label propagation. We can get proper
labels for the left nodes Dleft (node 6,8,9,10) based on meta learning.
3	Methods
3.1	Preliminaries
Given a graph data with n nodes and their labels D = {(χo, yo), (χι,yι),…，(χn-ι,yn-ι)},
where Xj is the j-th node and y7- ∈ {0,1}c is the label over C classes. Dtrain =
{(x0,y0), (xι,yι),…，(xs-1,ys-1)} are training nodes with noisy labels. Our goal is to en-
able the GNNs f (xj; W) trained with noisy sets Dtrain can also generalize well on test nodes.
W is the learnable parameters of GNNs. In our method, m nodes with true labels Dclean =
{(xs, ys), (xs+ι, ys+ι),…,(xs+m-ι, ys+m-ι)} in the graph are provided as the initial clean sets
(m《s). GCN (KiPf & Welling, 2016) and GAT (Velickovic et al., 2017) are utilized in our experi-
ments to extract features and classify nodes. Our method includes two main parts: label propagation
and label aggregation. We will go into details about these two parts in the following section 3.2 and
section 3.3.
3.2	Label Propagation
Label Propagation is based on the label smoothness that two connected nodes tend to have the same
label. Therefore, the weighted average of neighbor nodes, label of a node is similar to this node's
true label. An illustration of LP part in our method can be found in Figure. 1. The first step of LP is
to construct an appropriate neighborhood graph. A common choice is k-nearest graph (Iscen et al.,
2019; Liu et al., 2018) but there is an intrinsic graph structure (adjacency matrix A) in graph data,
so our similarities matrix W with zero diagonal can be constructed with A, whose elements Wij
are pairwise similarities between node i and node j:
Wi,j
Aij
d(hi,hj) + ε
(2)
where hi, hj are the feature vectors extracted by GNNs for node i and node j. d(∙, ∙) is a distance
measure (e.g.,Euclidean distance). ε is an infinitesimal. Note that we can get W with time complex-
ity O(| E |) instead of O(n2) because A is a sparse matrix whose edge lists are given. Then we can
normalize the similarities matrix W:
S = DT/2WDT/2,
(3)
where D is a diagonal matrix with (i,i)-value to be the sum of the i-th row of W. Let Y(k) =
[v!k'), ...,ynk∖T ∈ Rn×c be the soft label matrix in LP iteration k and the i-th row ∕k is the
predicted label distribution for node i. When k = 0, the initial label matrix Y(0) = [y(O),…,y?O)]t
3
Under review as a conference paper at ICLR 2021
consists of one-hot label vectors for i = s,s + 1,...,s + m - 1(i.e., initial clean sets) or zero vectors
otherwise. The LP (ZhU et al., 2005) in iteration k can be formulated as:
γ(k+I) = sγ (k),
y(k+1) = y(0),∀i ∈ [s, s + m - 1]
(4)
(5)
In Eq. (4), every node,s label in the (k + 1)-th iteration equals the weighted average of its neighbor
nodes, labels in k-th iteration. In this way, the clean sets propagate labels to the noisy training nodes
according to normalized edge weights. And then in Eq. (5), the labels of clean sets nodes are reset
to their initial values. The reason is that we can take full advantage of the tiny minority of clean
nodes and in case that the effect of clean sets fade away.
Co-teaching (Han et al., 2018) and Co-teaching plus (Yu et al., 2019) have been proposed to train
DNNs robustly against label noise. There are two DNNS which select samples with small loss from
noisy training sets to train each other. Our method is similar to theirs to some extent because LP is
utilized by us to select true-labeled samples from Dtrain for training. However, instead of taking
the nodes with small loss as true-labeled nodes, We select the nodes DseleCt whose original labels
are same with pseudo labels for training. Original labels of DseleCt are credible and we also inject
them to initial clean sets DClean for better LP in next epoch. This is why our method can achieve
better performance even if few true-labeled nodes are provided.
3.3	Meta-learning based label aggregation
2.Aggregation net forward
Training loss
7∙Aggregation net backward
6.Backwacon
backward
5.gnn backward
PJeAIyPEqNN9.
ueəp PJeMJoJ NNq寸
Gradients descent
Figure 2: Computation graph of meta-learning based label aggregation.
In section 3.2, the selected training nodes (node 5,7 in Figure.1) have been utilized for training and
LP but the left training nodes Dleft (node 6,8,9,10 in Figure.1) with abundant information haven’t
been fully exploited. In this section, we mine the abundant and precious information from Dlef t via
meta learning. The computation process of label aggregation is shown in Figure. 2.
For ∀(xj , yj ) ∈ Dleft, we can get two loss values:
lι = loss(yj ,yj),	⑹
l2 = IOss(yj,yj),	G)
where yj is the label predicted by GNNs for training node j and yj is the pseudo label predicted by
LP for node j. We can also get final label yj for node j by aggregating original label yj and pseudo
label yj:
yj = λjyj + (I- λj)yj, λj ∈ [0,1]	(8)
4
Under review as a conference paper at ICLR 2021
where λ is the aggregation coefficient. Some previous methods designed a weighting function map-
ping training loss to sample weights for noisy label problems (Kumar et al., 2010; Ren et al., 2018;
Shu et al., 2019). Instead, we utilize a 3-layer multi-layer perceptron (MLP) as the aggregation
network g(∙; ∙) to map loss values to aggregation coefficient λj:
λj = g(l1 k l2; θ) = λj (θ; w),	(9)
Where l1 k l2 is a 2-dimensional vector which is the concatenation of l1 and l2 and θ is the weights
of aggregation network g. The rationality lies on a consensus that samples’ loss values are affiliated
with the credibility of samples’ original labels (Kumar et al., 2010; Shu et al., 2019; Yu et al., 2019).
The MLP or aggregation networks’ input layer are 2 neurons and its output layer is one neuron,
which can be an approximator to almost any continuous functions. The activation function of the
last layer is sigmoid function to ensure that output λj ∈ [0, 1]. We can get the training loss Ltjr for
node j :
Ljj (w,θ) = loss(yj (w),yj (θ)),	(IO)
Then we can backward on the GNNs:
α
wt(θt) = Wt-PD	। E	VwLj(W,θt)∣wt,
left (xj,yj)∈Dleft
where α is the learning rate of GNNs. Then we can get the loss Lc on clean sets Dclean :
Lc(Wt(θt)) = ∣D1——।	X	Iossf(Xi； Wt(Ot)),yj,
clean (xi,yi)∈Dclean
(11)
(12)
Where f (xi； Wt(θt)) is the output of GNNs. Then We can utilize Lc to update the weights of aggre-
gation network:
θt+ι = θt - βVθLc(W(θ))∣θt,	(13)
where β is the learning rate of aggregation network. Finally, GNNs’ weights can be updated:
α
wt+ι = wt ― ∣~d	∣	〉,	VwLj(W, θt+I)Iwt.	(14)
(xj ,yj )∈Dlef t
To some extent, this part is similar to re-weight based methods (Ren et al., 2018; Shu et al., 2019).
However, LPM has two significant advantages. Firstly, re-weight based methods can not remove
the damages caused by incorrect labels because they assign every noisy training sample a positive
weight while LPM potentially has the ability to take full advantage of noisy samples positively.
Secondly, LPM can generate comparatively credible labels for other usages while re-weight or some
other methods can not. Algorithm. 1 shows all the steps of our algorithm.
3.4 Convergence of LPM
Here we show theoretically that the loss functions will converge to critical points under some mild
conditions. The detailed proof of the following theorems will be provided in Appendix C.
Theorem 1 Suppose the loss function loss is L-LiPschitz smooth, and λ(∙) is differential with a δ -
bounded gradient, twice differential with its Hessian bounded by B with respect to θ. Let the learning
rate at = min{1, T}, for some k > 0, such that T < 1 and learning rate βt a monotone descent
sequence, βt = min{L, √t} for some c > 0, such that L ≤ √t and P∞=ι βt ≤ ∞, P∞=ι β2 ≤
∞. Then the clean loss of Aggregation Net can achieve ∣∣Vθ Lc(W(θt))∣∣2 ≤ e in O(1∕e2) steps.
More specifically,
min
0≤t≤T
kVθLc(W(θt))k2 ≤O
(15)
Theorem 2 Under the conditions of Theorem 1, with the gradient of loss bounded by ρ, then
lim ∣VwtLtr(Wt,θt+1)∣22 =0.	(16)
t→∞
5
Under review as a conference paper at ICLR 2021
Algorithm 1: LPM. Line 2-12: label propagation; Line 13-22: label aggregation.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
Data: D,Dtrain,Dclean, max epochs T , LP iterations K in every epoch,A,feature matrix
X,GNNs feature extractor f, Aggregation Network g, expanding clean set for LP Dc
Result: Robust GNNs parameters wT
Dc = Dclean
fort = 0, 1,2, ...,T - 1 do
for ∀v ∈ D do hv = f (xv ; wt);
for (i, j) ∈ {1, 2, ..., n}2 do Wi,j =
for k = 0, 1, 2, ..., K - 1 do
I γ (k+1) = D-1/2 WD-1∣2Y (k)
end
Aij	;
d(hi,hj )+ε ,
yj(k+1) = yj(0)(∀ node j ∈ Dc)
DseleCt = Dleft =。；
for ∀ node i ∈ Dtrain do
if onehot(y∖K)) = y do DseIeCt = node {i} ∪ DseleCt；
else do Dleft = node {i} ∪ Dieft；
end
DC = DC ∪ DseleCt
Wt J one-step optimization of Wt with the selected nodes DseleCt ；
for ∀ node j ∈ Dlef t do
yj = f(xj； Wt)；
lι = loss(yj, y )； l2 = loss(yj, y )；
λj = g(II Il l2； θ/
yj = λjyj + (1 - λj)yj,λj ∈ [0,1];Lj(w,θ) = loss(yj(W),y(θ4
end
Wt[θ) = wt - DaftI ∑(χj,yj)∈Dieft VlwLj(w,θJ∣wt；
LC(Wt B)) = ∣D⅛n∣ P(Xi,yi)∈Dclean /以5% α(4)0)；
Θt+1 = θt - βVθLc(W(Θ))∣θ,；
Wt+1 = Wt - IDfT P(Xj,yj)∈Dieft VwLj(W; θt+1)lwt.
end
4	Experiments
4.1	Datasets and Implementation Details
We validate our method on six benchmark datasets, namely citation networks (Sen et al., 2008)
including Cora, Citeseer and Pubmed. Coauthor-Phy dataset (Shchur et al., 2018) is also utilized
in our experiments, but the results are shown in Appendix A due to the limited space. Summary
of the graph datasets mentioned above are shown in Table. 1. The Clothing1M (Xiao et al., 2015)
and Webvision (Li et al., 2017a) dataset are utilized to validate the effectiveness of our method in
real-world label noise settings. We take a kNN graph (k = 5) as the graph structure so that GNNs
can be applied in these two datasets, which follows previous work (Franceschi et al., 2019). More
details about our preprocessing on Clothing1M and Webvision datasets can be seen in Appendix B.
The experiments are conducted with two types of label noise: unif or m noise and flip noise
following previous works (Zhang et al., 2016a； Shu et al., 2019). The former means that the label of
each sample is independently changed to a random class with probability p, and the latter means that
the label is independently flipped to a similar class with total probability p. The ratio of training,
validation, and test nodes are set as 4:4:2. Only nearly 25 nodes with clean labels in the validation
set are provided as the clean set in each dataset and we ensure that each class has the same number of
samples. For example, we use 8 clean samples per label class for Pubmed. GCN (Kipf & Welling,
2016) serves as the base classification network model in our experiments and it is trained using
Adam (Kingma & Ba, 2014) with an initial learning rate 0.01 and a weight decay 5 × 10-4, except
that the weight decay equals to 0 in Clothing1M and Coauthor-Phy datasets.
We compare LPM with multiple baselines using the same network architecture. These baselines are
typical and some of them achieve state-of-the-arts performance on image datasets, which include:
6
Under review as a conference paper at ICLR 2021
Table 1: Dataset statistics after removing self-loops and duplicate edges (Wang & Leskovec, 2020)
	Cora	Citeseer	Pubmed	Coauthor-Phy
#nodes	2708	3327	19717	34493
#edges	5278	4552	44324	247962
#features	1433	3703	500	8415
#classes	7	6	3	5
#Intra-class edge rate	81.0%	73.6%	80.2%	93.1%
Table 2: Comparison with baselines in test accuracy (%) on Cora and Citeseer with uniform noise
ranging from 0% to 80%. Mean accuracy (std) over 5 repetitions are reported. The best and the
second best results are highlighted in bold and italic bold respectively.
Datasets	Cora	Citeseer
Method/noise rate	0.0	0.2	0.4	0.6	0.8	0.0	0.2	0.4	0.6	0.8
Basemodel	87.84(0.04) 85.92 (0.10) 82.42 (0.13) 75.77(0.18) 56.32 (0.19)	77.67 (0.13) 76.06 (0.15) 72.97 (0.09) 67.98 (0.12) 55.26 (0.22)
GCN+FT	88.05(0.06) 86.07 (0.13) 82.48 (0.14) 75.88 (0.15) 58.81 (0.22)	77.86 (0.15) 76.24 (0.07) 73.42 (0.21) 68.13(0.19) 56.12 (0.28)
L2RW	88.84 (0.19) 85.10 (0.21) 80.67 (0.22) 73.43 (0.42) 50.09 (0.37)	76.73 (0.20) 73.68 (0.14) 69.93 (0.29) 62.31 (0.32) 46.55 (0.49)
Co-teaching plus + FT	86.76 (0.14) 83.03 (0.19) 71.68 (0.21) 50.05 (0.31) 36.39 (0.44)	76.28 (0.19) 75.49 (0.24) 72.71 (0.13) 66.63 (0.41) 56.27 (0.36)
MW-Nets	88.33 (0.16) 85.93 (0.22) 82.61 (0.45) 75.60 (0.41) 56.37 (0.51)	78.27 (0.12) 76.62 (0.14) 74.25 (0.21) 68.06 (0.25) 56.53 (0.45)
GCEloss+FT	87.87 (0.13) 85.10 (0.09) 82.89 (0.07) 76.16 (0.15)	60.43(0.21)	78.01 (0.12) 76.54 (0.09) 74.06 (0.18) 69.18 (0.24) 58.48 (0.31)
APL+FT	87.68 (0.08) 86.26 (0.05) 82.01 (0.13) 74.49 (0.19) 58.72 (0.25)	76.54 (0.08) 74.32(0.17) 71.77(0.15) 66.78 (0.22) 56.08 (0.34)
Ours	88.75(0.07) 87.46 (0.11) 83.95(0.15) 79.66(0.22) 63.38(0.27)	78.12 (0.13) 77.07 (0.06) 75.19(0.15) 70.05(0.11) 61.71 (0.22)
Base model, referring to the GCN that directly trained on noisy training nodes; Meta-learning based
methods L2RW (Ren et al., 2018), MW-Nets (Shu et al., 2019); Typical and effective method
Co-teaching plus (Yu et al., 2019); Robust loss function against label noise GCE loss (Zhang &
Sabuncu, 2018) and APL (Ma et al., 2020); The most recent method based on co-training JoCoR
(Wei et al., 2020). For those baselines that don’t need clean sets (Base model, Co-teaching plus,
GCE loss, JoCoR and APL), we finetune (denoted by FT in this paper) them on the initial clean
sets after the model was trained on training sets for a fair comparison. More experimental details
about LPM and all baselines are available in the Appendix B.
4.2	Results
Table. 2 shows the results on Cora and Citeseer with different levels of uniform noise ranging from
0% to 80%. Every experiment are repeated 5 times with different random seeds. Finally, we report
the best test accuracy across all epochs averaged over 5 repetitions for each experiment. As can
be seen in Table. 2, our method gets the best performance across all the datasets and all noise
rates, except the second for 0% uniform noise rate. Our method performs even better when the
labels are corrupted at high rate. Table. 3 shows the performance on Cora, Citeseer and Pubmed
with different levels of flip noise ranging from 0% to 40%. It can be seen that our method also
outperforms state-of-the-arts methods under flip noise across different noise rate, except that the
second for 0% flip noise rate. Our method outperforms the corresponding second best method by a
large margin when the noise rate is 0.4. As can be seen in Table. 4, our method can also perform
better than other baselines in datasets with real-world label noise. We also experiment with Graph
Attention Networks (Velickovic et al., 2017) as the feature extractor and classifier, the results shown
in Appendix A demonstrate that our method can also perform well with other GNNs.
Table 3: Comparison with baselines in test accuracy (%) on Cora , Citeseer and Pubmed with flip
noise ranging from 0% to 40%. Mean accuracy (std) over 5 repetitions are reported. The best and
the second best results are highlighted in bold and italic bold respectively.
Datasets	Cora	Citeseer	Pubmed
Method/noise rate	0	0.2	0.4	0	0.2	0.4	0	0.2	0.4
Basemodel	87.84(0.04) 81.64(0.11) 61.12(0.24)	77.67 (0.13) 75.91 (0.14) 52.67 (0.35)	86.18 (0.08) 85.30(0.21) 74.21 (0.29)
GCN+FT	88.05 (0.06) 82.89(0.14) 67.39(0.42)	77.86 (0.15) 75.08 (0.22) 61.41 (0.23)	86.21 (0.09) 85.55(0.24) 80.88 (0.32)
L2RW	88.84 (0.19) 80.90(0.21) 59.00 (0.34)	76.73 (0.20) 71.85 (0.25) 50.04 (0.44)	86.34 (0.14) 84.54(0.19) 76.97 (0.31)
Co-teaching plus+FT	86.76 (0.14) 81.37 (0.21) 53.00(0.51)	76.28 (0.19) 74.66 (0.21) 60.59 (0.33)	85.59 (0.09) 84.61 (0.22) 73.99 (0.33)
MW-Nets	88.33(0.16) 85.33 (0.23) 67.71 (0.43)	78.27 (0.12) 76.84 (0.19) 61.97 (0.33)	86.02 (0.07) 84.74(0.17) 78.59 (0.28)
GCEloss+FT	87.87 (0.13) 83.21 (0.13) 67.80(0.37)	78.01 (0.12) 76.36 (0.20) 63.66 (0.46)	86.15 (0.11) 85.47 (0.06) 80.03 (0.42)
APL+FT	87.68(0.08) 81.09(0.14) 70.07 (0.19)	76.54 (0.08) 73.38 (0.13) 60.81 (0.52)	86.16 (0.05) 85.52(0.06) 70.08 (0.16)
Ours	88.75(0.07) 86.95 (0.12) 78.97 (0.33)	78.12 (0.13) 76.39(0.14) 69.71 (0.39)	86.48 (0.05) 85.58 (0.13) 83.15 (0.36)
7
Under review as a conference paper at ICLR 2021
Table 4: Comparison with baselines in test accuracy (%) on Clothing1M and Webvision. Mean
accuracy (± std) over 5 repetitions are reported. The best is highlighted in bold.
Methods	Basemodel	GCN+FT	L2RW	MW-Nets	GCEloss+FT	JoCoR+FT	Ours
Clothing1M	35.83±0.03	38.05±0.13	53.5±0.08	54.15±0.23	56.9±0.08	56.3±0.12	57.35±0.11
Webvision	32.43±0.05	34.58±0.08	50.12±0.16	52.42±0.25	53.45±0.13	54.12±0.22	55.43±0.17
Figure 3: Comparsion of the true-labeled samples rate in Dtrain
and Dselect in various datasets.
Table 5: The performance of LPM without label aggregation and LPM with random λ in Citeseer.
Noise type	∣	Uniform noise	∣ Flip noise
Method/noise rate	0	0.2	0.4	0.6	0.8	0.2	0.4
Ours w/o label aggregation	72.07	68.36	65.69	62.16	54.39	69.26	63.14
Ours with random λ	76.88	75.08	72.07	68.28	57.40	74.89	68.30
Ours with tuned λ	77.22	76.31	73.55	69.17	58.54	75.11	68.72
Ours	78.12	77.07	75.19	70.05	61.71	76.39	69.71
4.3	Analysis of the necessity and effectiveness of different parts
We design five experiments to validate the neces-
sity and effectiveness of different components of our
algorithm. Firstly, we compare the ratio of true-
labeled nodes in Dselect with Dtrain in the last
epoch to validate the effectiveness of LP. Figure. 3
shows the ratio of true-labeled nodes in Dselect in
the last epoch and Dtrain under uniform noise on
various datasets. It can be found that nearly all the
nodes selected by LP are true-labeled even if most
training nodes are mislabeled, which demonstrates
the great ability of LP to select true-labeled nodes
from noisy training nodes. Secondly, we remove
the label aggregation in LPM to validate its neces-
sity and the result shows that the performance of our
method become much worse without label aggrega-
tion. It is necessary to mine the potential information
from the left noisy training nodes after LP selection.
Besides, we validate the effectiveness by replacing
the learned aggregation coefficients λ with random
Figure 4: ∆λ varies during the training stage on
Cora with various uniform noise rate.
numbers between 0 and 1. It is obvious that the aggregation coefficients λ optimized by meta learn-
ing outperform random λ. Also, we assign the percentage of clean nodes of each label class as λ
(tuned) for comparison. These validate the effectiveness of the meta-learning based label aggrega-
tion. The results of above two experiments are shown in Table. 5. We denote the average of λ of
clean nodes and noisy nodes in Dleft as λclean and λnoise respectively, ∆λ = λclean - λnoise . We
plot the variation of ∆λ during training stage in Figure. 4. It can be observed that λclean > λnoise
across the training stage and the margin between λclean and λnoise grows larger with the training
process, which suggests that λ optimized by our method is valid.
8
Under review as a conference paper at ICLR 2021
(a) Cora
Figure 5: Test accuracy on Cora and Citeseer across various flip noise rate.
(b) Citeseer
4.4	Impact of Finetuning and noise rate
We would like to investigate how our baselines can perform without finetuning. As can be seen in
Figure. 5, the performance of the baselines will degenerate relatively significantly without finetuning
across different noise rate. This illustrates that some baselines (without finetuning) that are designed
for image datasets may perform relatively poor on graph-structured data and this motivates our
work which trains GNNs robustly utilizing the structure information of graph data. Besides, We
can also observe that our method only drops nearly 9% when the flip noise rate increased from 0%
to 40%, whereas the baseline has dropped nearly 20% - 30%, which illustrates that our method is
more robust, especially at high noise rate. At 0% noise, our method only slightly underperforms re-
weights besed methods. This is reasonable because the original labels are all correct but our method
will inevitably perturb a few clean labels while the re-weights based methods will not.
4.5	Size of the clean set
We try to strike a balance and understand when finetuning will be effective. As can be seen in
Figure. 6, our method can also perform better even if the size of clean set is extremely small. The
overall test accuracy does not grow much when the size of clean set is large enough. Besides,
the test accuracy of baselines with fintuning will increase significantly when the size of clean set
grows larger. This suggests that finetuning will be valid when the size of clean set grows larger
because GNNs can achieve good performance with relatively less samples (Kipf & Welling, 2016;
Velickovic et al., 2017). From this perspective, our method can also serve as complementary for
finetuning based methods when the size of clean set is large enough.
(a) Cora	(b) Citeseer
Figure 6: Test accuracy on Cora and Citeseer across various size of clean set.
5	Conclusion and future work
In this work, we proposed a robust framwork for GNNs against label noise. This is the first method
that specially designed for label noise problem existing in utilizing GNNs to classify graph nodes and
it outperforms state-of-the-arts methods in graph-structured data, which may serve as the beginning
for future research towards robust GNNs against label noise. As a future work, we may design an
inductive robust method. Besides, better methods that don’t need clean sets are also the goals of us.
9
Under review as a conference paper at ICLR 2021
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981-3989, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. arXiv preprint arXiv:1703.03400, 2017.
Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures
for graph neural networks. arXiv preprint arXiv:1903.11960, 2019.
Chen Gong, Dacheng Tao, Wei Liu, Liu Liu, and Jie Yang. Label propagation via teaching-to-
learn and learning-to-teach. IEEE transactions on neural networks and learning systems, 28(6):
1452-1465, 2016.
Chen Gong, Hengmin Zhang, Jian Yang, and Dacheng Tao. Learning with inadequate and incorrect
supervision. In 2017 IEEE International Conference on Data Mining (ICDM), pp. 889-894.
IEEE, 2017.
Edward Grefenstette, Brandon Amos, Denis Yarats, Phu Mon Htut, Artem Molchanov, Franziska
Meier, Douwe Kiela, Kyunghyun Cho, and Soumith Chintala. Generalized inner loop meta-
learning. arXiv preprint arXiv:1910.01727, 2019.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Yifan Hou, Jian Zhang, James Cheng, Kaili Ma, Richard TB Ma, Hongzhi Chen, and Ming-Chang
Yang. Measuring and improving the use of graph information in graph neural networks. In
International Conference on Learning Representations, 2019.
Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-
supervised learning. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 5070-5079, 2019.
Simon Jenni and Paolo Favaro. Deep bilevel learning. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 618-633, 2018.
Masayuki Karasuyama and Hiroshi Mamitsuka. Manifold-based similarity adaptation for label prop-
agation. In Advances in neural information processing systems, pp. 1547-1555, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
M Pawan Kumar, Benjamin Packer, and Daphne Koller. Self-paced learning for latent variable
models. In Advances in neural information processing systems, pp. 1189-1197, 2010.
Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy la-
beled data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 5051-5059, 2019.
Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual
learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017a.
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels with distillation. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1910-1918, 2017b.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang.
Learning to propagate labels: Transductive propagation network for few-shot learning. arXiv
preprint arXiv:1805.10002, 2018.
10
Under review as a conference paper at ICLR 2021
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Nor-
malized loss functions for deep learning with noisy labels. arXiv preprint arXiv:2006.13554,
2020.
Hyoungseob Park, Minki Jeong, Youngeun Kim, and Changick Kim. Self-training of graph neural
networks using similarity reference for robust training with noisy labels. In 2020 IEEE Interna-
tional Conference on Image Processing (ICIP), pp. 1951-1955. IEEE, 2020.
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for
robust deep learning. arXiv preprint arXiv:1803.09050, 2018.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, SanPing Zhou, Zongben Xu, and Deyu Meng. Meta-weight-
net: Learning an exPlicit maPPing for samPle weighting. In Advances in Neural Information
Processing Systems, PP. 1919-1930, 2019.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. GraPh attention networks. arXiv: Machine Learning, 2017.
Petar VeliCkovic, Guillem CucurulL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. GraPh attention networks. arXiv preprint arXiv:1710.10903, 2017.
Fei Wang and Changshui Zhang. Label ProPagation through linear neighborhoods. IEEE Transac-
tions on Knowledge and Data Engineering, 20(1):55-67, 2007.
Hongwei Wang and Jure Leskovec. Unifying graPh convolutional neural networks and label ProPa-
gation. arXiv preprint arXiv:2002.06755, 2020.
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A
joint training method with co-regularization. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. PP. 2691-2699, 2015.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are graPh neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does
disagreement helP generalization against label corruPtion? arXiv preprint arXiv:1901.04215,
2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deeP learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016a.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deeP learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016b.
Huan Zhang, Zhao Zhang, Mingbo Zhao, Qiaolin Ye, Min Zhang, and Meng Wang. Robust triPle-
matrix-recovery-based auto-weighted label ProPagation for classification. IEEE Transactions on
Neural Networks and Learning Systems, 2020.
Zhilu Zhang and Mert Sabuncu. Generalized cross entroPy loss for training deeP neural networks
with noisy labels. In Advances in neural information processing systems, PP. 8778-8788, 2018.
Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Scholkopf. Learn-
ing with local and global consistency. In Advances in neural information processing systems, PP.
321-328, 2004.
Xiaojin Zhu, John Lafferty, and Ronald Rosenfeld. Semi-supervised learning with graphs. PhD
thesis, Carnegie Mellon University, language technologies institute, school of . . . , 2005.
11
Under review as a conference paper at ICLR 2021
A Appendix : Additional Experiment Results
Table A.6: Comparison with baselines in test accuracy (%) on Cora and Pubmed with flip noise
ranging from 0% to 40% and Graph Attention Networks. The best result are highlighted in bold.
Datasets	∣		Cora		PUbmed		
Methods/Noise rate	0	0.2	0.4	0	0.2	0.4
GAT	89.85	84.13	67.10	85.55	84.57	74.11
GAT+FT	89.85	84.50	73.12	85.55	84.57	80.55
MW-Nets	87.52	84.26	69.99	85.64	84.5	75.82
Co-teaching plus+FT	88.56	85.42	74.94	85.56	84.48	82.40
GCEloss+FT	89.98	84.38	74.23	85.45	84.54	80.65
JoCoR+FT	90.16	85.00	73.74	85.47	84.58	80.95
Ours	89.92	87.20	75.65	85.72	84.62	83.00
Table A.7: Comparison with baselines in test accuracy (%) on Coauthor-Phy with flip noise ranging
from 0% to 40%. The best result are highlighted in bold.
Method/Noise rate	0.0	0.1	0.2	0.3	0.4
Basemodel	96.92	96.32	95.57	94.91	86.25
GCN+FT	96.96	96.41	95.54	94.46	92.25
Co-teaching plUs+FT	96.45	96.39	96.10	95.27	92.79
MW-Nets	96.56	96.24	95.62	95.56	89.25
GCEloss+FT	96.99	96.58	95.96	94.77	93.64
JoCoR+FT	96.83	96.59	96.07	94.95	94.11
Ours	96.75	96.71	96.49	96.14	95.14
We also take Graph Attention Networks (GAT) as the feature extractor and classifier and the results
shown in Table. A.6 validate that our method can also perform well with various GNNs. Besides,
LPM can also perform better than other baselines in larger graph dataset Coauthor-Phy, the results
can be seen in Table. A.7. We also demonstrate confusion matrices of Basemodel and LPM in
Figure. A.4, which visually show that our method can improve the robustness against label noise of
GNNs by a large margin.
B Appendix : Additional details of our experiments
Original Clothing1M and Webvision datasets are all large-scale datasets with real-world label noise.
We randomly choose 5000 images in 10 classes from original datasets and every image serves as a
node in the graph, a kNN graph (k=5) is treated as the graph structure so that GNNs can be applied
in Clothing1M datasets. This setting is similar to some previous works which also aim to apply
GNNs in datasets without graph structure. ResNet-50 with ImageNet pretrained weights is utilized
by us to extract feature vectors for all the images.
Table. A.8 shows the different hyper-parameters in LPM experiments for different datasets. In all
the experiments, 25 true-labeled nodes are utilized as the initial clean sets or as the samples for
Table A.8: The hyper-parameters of LPM in different datasets.
	Cora	Citeseer	PUbmed	CoaUthor-Phy	Clothing1M
Aggregation Net’s learning rate	1 × 10-4	1 × 10-4	1 × 10-3	1 × 10-3	1 × 10-3
Aggregation Net’s mid-dimension	64	100	100	64	50
Aggregation Net’s weight decay	1 × 10-4	1 × 10-4	1 × 10-4	1 × 10-4	1 × 10-4
LPA iterations	50	50	50	50	50
12
Under review as a conference paper at ICLR 2021
(b)
(d)
(e)
Figure A.4: Confusion matrices of Basemodel and LPM on various datasets under 40% flip noise.
Figure. 4(a)-4(c) are the results of Basemodel. Figure. 4(d)-4(f) are the results of LPM.
finetuning and the total epoch of all the experiments is 300. In Co-teaching plus experiment, the
initial epoch is 270, the forget rate is 0.1 and 5 epochs for linear drop rate ,the exponent of the
forget rate is 1. For MW-Nets, the dimension of the meta net’s middle layer is 100 and the learning
rate is 5 × 10-3. q for GCEloss is 0.1. The combination of Normalized Focal Loss and Mean
Absolute Error is utilized in APL experiments, the weight of Normalized Focal Loss is 0.1 and the
weight of Mean Absolute Error is 10. For JoCoR experiments, the epochs for linear drop rate is 5
and the exponent of the forget rate is 2. The balance coefficient between conventional supervised
learning loss and contrastive loss is 0.01. The learning rate and weight decay of Graph Attention
Networks are 0.01 and 5 × 10-4. The dimension of hidden layer of GAT is 16 and the number
of head attentions is 8. The alpha of the leaky relu is 0.2 and the dropout rate is 0.5. Throughout
this work we implemented gradient based meta-learning algorithms in PyTorch using the Higher
library (Grefenstette et al., 2019).
C Appendix : Convergence of LPM
Our proof of the convergence of LPM mainly follow some previous works (Ren et al., 2018; Shu
et al., 2019) that utilize meta-learning to reweight noisy training samples. As is illustrated in some
previous works (Zhou et al., 2004; Zhu et al., 2005), LPA will converge to a fixed point. Namely,
Dselect and Dlef t will converge to fixed sets. In our proof, the final | Dlef t | and final | Dclean | are
denoted with n and m for easier illustration. Loss function loss is denoted by l in this proof. Here
we first rewrite the forward and backward equations as follows:
yj = f (Xj; Wt) = yj (W)IWt	(17)
λj = g(l(yj,yj) k l(yj,yj);θt) = λj(θ;Wt)lθt	(18)
1n
Ltr(Wt; θt) = — El(λjyj + (1 - λj)yj,yj)	(19)
n j=1
Wt(θt) = Wt- αVwLc(w; θt)|wt	(20)
13
Under review as a conference paper at ICLR 2021
y = f(χi; Wt) = yi(W; χi)∣Wt	(21)
mm	m
Lc(W)Iwt = — X Lc(W)Iwt = — X Lc (w⅛(θ))lθt = — X i(yi,yi)	(22)
tm	tm	tm
i=1	i=1	i=1
Θt+1 = θt- βVθLc(W(θ))∣θt	(23)
wt+1 = Wt- aV w Ltr (W; θt+I)Iwt	(24)
(xj , yj ) is node from the final left training set Dleft ;
(xi , yi ) is node from the final clean set Dclean ;
f is the GCN for classification with its weights W;
g is the Aggregation Net whose input are the nodes from clean set with its weights θ;
Lc is the loss on clean sets. Ltr is the final training loss.
l(y, y) is the loss (such as Cross Entropy) which satisfies linearity given by
l(λyι +(I- λ)y2,y) = λl(yι ,y) + (I- λ)l(y2,y).
Derivation of the equation of updating the weights in Aggregation Net
1 XV7 jτc∕ ʌ/nʌʌ I 1 X dLc(W)I X ^^ɔt(θ) 1 dλj (θ; Wt)I	peʌ
m T vθ Li ⑻⑻)麻=mX F-血 T F7 lθt ∂θ	lθt.	(25)
i=1	i=1	j=1	j
According to Equation (20)
1n
Wt(e)|et = Wt- αvwtn £l(%yj +(1 - λj)yj,yj)
n j=1
dWt(θ)、 __ αv7 dl(λjyj + (1 - λj)yj,yj) 1
Fr lθt = - n vwt	∂λj	lθt
dWt(θ) [	αv 讥λji(yj,yj) +(1 - λj)l(yj,yj)]∣
Fr lθt = - n vwt	∂λj	lθt
源⑻lθt = -αvwt(l(yj,yj) -l(yj,yj))lθt
∂λj	n
dWt(61 _ _α d(I(y,yj) — Kyj,yj)) ∣
∂λj lθt = -n	∂Wt	|wt
Therefore, Equation (25) can be written as
m
—X vθ Lc(W(θ))∣θt
mt
i=1
α X dLc(W) I ʌ X d(l(yj∙,yj) — l(yj,yj)) ∣ dλ(θ; Wt)
mn 乙' ∂W wt 乙'	∂Wt	wt	∂θ
t
nm
-α Xd X
nm
n j=1 m i=1
∂Lc(W) τ ∂(l(yj,yj) - l(yj-,yj-))
|wt
∂W
∂Wt
I	∖∂λj (θ; Wt)I
|wt) ∂θ lθt
α 1 1 ^mC ∖ dλj (θ; Wt)
-n j=1( m i=1 Gij)
Iθt,
where Gij
dLc(W) ∣t d(Myj,yj)τ(yj,yj» ∣
∂w	|wt	∂wt	|wt.
Lemma 1. Suppose the loss function l is L-LiPSchitz smooth, and λ(∙) is differential With a
δ-bounded gradient, twice differential with its Hessian bounded by B with respcet to θ, and the loss
function l(∙, ∙) have ρ-bounded gradients with respect to the parameter w. Then the gradient of W
with respect to Lc(W) is Lipschitz continuous.
14
Under review as a conference paper at ICLR 2021
Proof. The supposition is equivalent to the following inequalities,
∣∣vι^Lc(W)Iwi-VwLc(W)∣w2k ≤ LIlwI- W2k,	(26)
for any w1 , w2 ;
IVθλ(θ; wt)I ≤ ρ;
IVθ22 λ(θ; wt)I ≤B;
∣∣Vwl(yi,0i((wt(w);xi)))∣ ≤ δ.
(27)
(28)
(29)
The gradient of θ with respect to loss on clean set reads
VθLc(w(θ))∣θt
n
-a X
n
j=1
n
dLc(W)∣t d(Uyj,yj) -l(yj,yj))∣	dλj(θ;wt)
|Wt	五二	|wt
∂w
∂wt
∂θ
lθt
-n X Gij
j=1
∂λj(θ; Wt)
∂θ	lθt
Taking the gradient of θ in both sides of the equation, we have
Vθ2 Lc(W(θ))∣θt=- α X (dG⅛ ∖ Iθt + (Gij)
n ∂θ ∂θ
j=1
∂2λj (θ; wt)
∂θ2
Iθt).
For the first term in summation,
k ∂Gij∣ ∂λj(θ; Wt) ∣ k
k ∂θ lθt ∂θ lθt k
‹δ∣∣ d ∕LC(w)∣T)∣τ
≤δk ∂^(	lθt)lwt
d(l(yj,yj) - l(yj,y))
∂wt
|wtI
n
=δ∣ ɪ (-a X
k∂w( n乙
k=1
∂Lc(w) τ ∂(l(yk,yk) - l(yk,yk)) l ∂λj(θ; wt)
|Wt	五二	|wt
∂w
∂wt
∂θ
∣θt)lTt
d(l(yj,yj) - l(yj,y))
∂wt
|wtI
n
=δk(-α X
n
k=1
∂2Lc(w)...
≤δαk -∂W^2lwtkk
≤4αLρ2 δ2 .
∂2LC(W) ∣T ∂(l(yk,^) - l(yk,^))	∂λk(θ; wt)
∂w2 lwt	五二	| wt
∂wt
∂θ
Iθt)lTt
d(Myj,y) - l(yj,y))
d(l(yk,yk) - l(yk,yk))
∂wt
Iwtkkdλk (θ;Wt) Iθt)∣wtkk
∂θ
∂wt
d(I(yj,yj) - l(yj,yj))
|wtI
∂wt
|wtI
And for the second term,
∣(Gij)
∂2λj (θ; wt)
∂θ2
lθt k = Il
dLc(w) it d(ι(y,yj) - l(yj,yj)) | d2λj(θ; Wt)
|wt	Λ.,,	|
∂w
∂wt
Iwt —∂θ2
Iθtk ≤2Bρ2.
Therefore,
kV22Lc(w(θ))∣θtk ≤ 4α2Lρ2δ2 + 2αρ2B.
(30)
Let Lv = 4α2 Lρ2 δ2 + 2αρ2B ,Based on Lagrange mean value theorem, we have
∣VθLc(wt(θι)) - VθLc(wt(θ2))k ≤ Lv∣∣θι - θ2k,
for all θ1, θ2 .
15
Under review as a conference paper at ICLR 2021
Theorem 1. Suppose the loss function l is L-LiPschitz smooth, and λ(∙) is differential with a δ-
bounded gradient, twice differential with its Hessian bounded by B with respect to θ. Let the learning
rate at = min{1, T}, for some k > 0, such that T < 1 and learning rate βt a monotone descent
sequence, βt = min{ L,焉} for some c > 0, such that L≤ √ and P∞=1 βt ≤∞,P∞=ι β2≤
∞. Then the loss of Aggregation Net can achieve ∣∣VθLC(W(θt))k2 ≤ E in O(1∕e2)steps. More
specifically,
(31)
0m≤TkVθLC(W(θt))k2 ≤O
Proof. The iteration for updating the parameter θ reads
Θt+1 = θt- βVθLc(Wt(θ))∣θt∙
In two successive iteration, observe that
LC(Wt+l(θt+1))- LC(Wt(θt))
= [Lc(wt+l(θt+1))- LC(Wt(θt+1))] + [Lc(w^t(θt+1)) - LC(Wt(°t))].	(32)
For the first term, given that loss function on clean set is Lipschitz smooth, we have
LC(Wt+l(θt+l)) - LC(Wt(θt+ι))
≤ < VLC(Wt (θt+l)), Wt+1(%+1)- q^t(θt+1) > +2 IlWt+l(θt+l) - q^t(θt+1)k2.
According to Equation (20) and (23),
n
Wt+iR+i) - Wt(%+1)= -T X[λjVwL(Uj, yj) + (I- λj)vwL(Oj, yj)]|wt+i,
n j=1
and thus,
IlLC(Wt+ι(θt+ι)) - LC(Wt(θt+ι))k ≤ atρ2 + 2α2ρ2,
since the first gradient of loss function is bounded by ρ.
By the Lipschitz continuity of Lc(Wt (θ)) according to Lemma 1., it can be obtained that
LC(Wt(θt+ι)) — LC(Wt(θt))
≤ hvθtLC(Wt0)), θt+ι - θti + 2kθt+ι - θtk2
Lβ2
=hVθtLc(Wt(θt)), -βtVθtLc(Wt(θt))i + -ββt-kVθtLc(Wt(θt))k2
=-(βt - Let2)kVθtLC(Wt(θt))k2∙
Therefore, the Equation (32) satisfies
L	Lβ2
LC (Wt+l(θt+1))- LC (Wt(θt)) ≤ αtρ + 2 at P - (βt----2-)kvθt Lc (Wt(θt))∣∣2
Lβ2	L
(βt-----2-)k2vθt Lc (Wt(θt ))k2 ≤ atρ + 2 at P - LC (Wt+l(θt+1)) + LC (Wt (θt))∙
Summing up above inequalities from 1 to T , we have
T Lβ2	T	L
X(βt-----2-)kvθt L (Wt(θt))∣∣2 ≤Lc (WI(°I)) + XgtP + 2 at P )
t=1	t=1
T	Lβ2	T	L
X(βt —厂)叫n kvθt Lc (Wt(θt))∣∣2 ≤Lc (WI(&))+XgtP + 5atP )∙
2 t	2
t=1
t=1
16
Under review as a conference paper at ICLR 2021
Furthermore,
Lc(W1(θ1)) + P=∖(αtρ2 + 2 a2p2)
mtin kVθtLc(Wtp))k2 ≤
≤ 2Lc(WI(。I)) + PT=ι(2atρ + LatP2
P	P=ι(2βt- Lβ2)
≤ 2Lc(WI(&)) + P=IC2atρ + Lɑ2ρ2)
—	P=ι(βt)
<2Lc(Wι(θι))+ aιρ2T (2 + L)
≤	Te
2Lc(Wι(θι) 1	αιp2(2 + L)
=-T —瓦 +	β
2Lc(Wι(θι)	L √Tη	, k、 T τ √Tη 2, r、
≤---T——-max{L, ɪ} + min{1, T} max{L, ɪ}ρ2(2 + L)
2Lc(Wι(θι)	kρ2(2 + L) _	1
≤	c√T	+	c√T	= O( T).
It holds for PtT=1(βt) ≤ PtT=1(2βt - Lβt2). In conclusion, it proves that the algorithm can always
achieve mino≤t≤τ ∣∣VθLc(W(θt))k2 ≤ O(√T) in T steps.
Lemma 2. Let (an)1≤n, (bn)1≤n be two non-negative real sequences such that the series Pi∞ an
diverges, the series Pi∞ anbn converges, and there exists K > 0 such that ∣bn+1 - bn ∣ ≤ Kan .
Then the seqences (bn)1≤n converges to 0.
Proof. See the proof of Lemma A.5 in [Stochastic majorization-minimization algorithms for ].
Theorem 2. Suppose the loss function l is L-Lipschitz smooth and have P-bounded gradients with
respect to training data and clean set, and λ(∙) is differential with a δ-bounded gradient twice differ-
ential with its Hessian bounded by B with respect to θ. Let the learning rate a = min{1, T}, for
some k > 0, such that T < 1 and learning rate βt a monotone descent sequence, βt = min{ +, √^ }
for some c > 0, such that L ≤ √T and P=I βt ≤ ∞, P∞==1 β2 ≤ ∞. Then
lim ∣VwtLtr(wt; θt+ι)kt =0.
t→∞
Proof. It is obvious that at satisfy Pt∞=0 at = ∞, Pt∞=0 at ≤ ∞. In Eq. 18, 19, 20, and the linearity
of L, we rewrite the update of W as
Wt+1 = Wt - atVLtr(Wt; θt+1)
n
=Wt-子 £%(θt+ι; Wt)Vwtl(yj,yj(Wt)) + (I- λj(θt+ι; Wt))Vwtl(yj,yj(Wt))∙
n j=1
First, we have the difference of the loss function on training set between two iterations,
Ltr (Wt+1; θt+2) - Ltr (Wt; θt+1)
=[Ltr (Wt+1; θt+2) - Ltr (Wt+1; θt+1)] + [Ltr (Wt+1; θt+1) - Ltr (Wt; θt+1)].	(33)
17
Under review as a conference paper at ICLR 2021
For the first term in Eq.33, by the L-Lipschitz-smooth and ρ-bounded gradients of λ with respect
to training and clean set,
Ltr (wt+1; θt+2) - Ltr (wt+1; θt+1)
1n
n ∑(λj (θt+2； wt+1) - λj (θt+i; wt+1))l(yj ,y(Wt+1)) + (λj (θt+i; wt+1) - λj (θt+2; wt+1 ))l(yj ,yj (Wt+1))
n j=1
≤^" X(V -'' ∂θ t+1) l,t+ι ,%+2 — θt+j + 2 kθt+2 - θt+1 k2)(l(yj, y(Wt+1)) + l(yj, y(Wt+1)))
n	∂θ	2
j=1
1	/ dλj (θ; wt+1) I	ay τ( - (∩ ∖∖∖
n χ(ξ-------∂θ----lθt+1, -βtVθtL(wt(θt)),
δβ2
+ -2- IUtL(Wt(%))k2)(I(yj) y(wt+1)) + l(yj, y(wt+1))).
For the second term in Eq. 33,
Ltr (Wt+1; θt+1) - Ltr (Wt; θt+1)
≤ ^VwtLtr (Wt； θt+1), Wt+1 - Wt)+ 2 ||Wt+1 - Wtk2
=-(αt -")kVwtLtr(Wt； θt+1)k2.
Therefore, we have
Ltr (Wt+1； θt+2) - Ltr (Wt； θt+1)
1	∖-^f dλj (θ; Wt+1)	τc(
≤n / (\--------∂θ-----lθt+1, -8NθtL (lt,t(θt))y
δβ2
+^-2 kvθt L(Wt (%))|| 2)(I(Jj ,ry(Wt+1)) + l(yj ,ry(Wt+1)))
-(at------2t )kvwt Ltr (Wt； θt+1 )k2.
Summing up the inequalities in both sides from t = 1 to ∞, we have
lim kLtr(Wt+1； θt+2) - Ltr (W1； θ2)k
t→∞
≤ X - β X [kdλj 嚷t+1) Iθt+1 k2kVθt Lc(Wt(θt))k2(ki(yj ,y(Wt+1))k2 + ki(y∙ ,y(Wt+1 ))k2)
t=1	j=1
∞ δβ2 n
+ ∑ -βt- EkVθt Lc(Wt(θt))k2](ki(yj ,y(Wt+1))k2 + ki(yj ,y(Wt+1))k2)
t=1	j=1
∞	Lα2
-X(αt	厂)kVwtL T(Wt； θt+1)k2 .
t=1
18
Under review as a conference paper at ICLR 2021
Rearrange the terms of the inequality, we obtain
∞
X atkVwtLtr (wt； θt+1)k2
t=1
+ X β X k dλjMt+1) Iθt+1k2kVθtLc(wt(θt))k2(kl(yj,y(wt+1))k2 + kl(yj,y(wt+1))k2)
t=1 j=1
≤Lα2 kVwtLtr(wt；θt+1)k2
∞ δβ2 n
+ ∑ * EkVθtLc(wt(θt))k2](kl(yj,y(wt+1))∣∣2 + kl(yj,y(wt+1))∣∣2)
t=1 j=1
一lim ∣∣Ltr(wt+1； θt+2)k2 + ∣∣Ltr(w1； Θ2)k2
ti∞
∞
≤X
t=1
≤∞.
Lα	∞ δβ2
-ɪP + kL r(w1； θ2)k2 + X ~2(2MP ) - tliιn kL r(wt+1 ； θt+2)k2
t=1
The inequality next to last holds since our loss function is bounded by M, and the last one holds for
Pt∞=1 αt2 and Pt∞=1 βt2 are finite.
In addition, since
X βt X k dλj (仇 wt+1)
乙方乙k ∂θ
t=1	j=1
∞
≤2MPδXβt ≤ ∞,
we can obtain that
Iθt+1 k2kVθt Lc(Wt(θt))k2(ki(yj ,y(wt+1))k2 + ki(yj ,y(wt+1))k2)
∞
X atkVwtLtr(wt； θt+1)∣∣2 ≤ ∞.
t=1
In the other hand, based on the inequality:
(kak+kbk)(kak-kbk)≤ka+bkka-bk,
(34)
we have
IkVLtr(wt+1； θt+2)k2 -kVLtr(wt; Θt+ι)k2∣
=(∣∣VLtr(wt+1； θt+2)k2 + kVLtr(wt； θt+1)k2)(kVLtr(wt+1； θt+2)k2 -kVLtr(wt；%十娟切
≤kVLtr(wt+1； θt+2) + VLtr(wt； θt+1)k2kk2kVLtr(wt+1； θt+2)-VLtr(wt %+1)∣∣2
≤(kVLtr(wt+1； θt+2)k2 + kVLtr(wt； θt+1)k2)kVLtr(wt+1； θt+2) -VLtr(wt； θt+1)k2)
≤2Lρk(wt+1, θt+2) - (wt, θt+1)k2
≤2Lραtβtk(VLtr(wt,θt+1),VLc(wt,θt+1))k2
≤2√2Lρ2β1αt
=Cαt.
For Eq. 34 which reads
∞
X atkVwtLtr(wt； θt+1)k2 ≤ ∞,
t=1
since Pt∞=0 αt = ∞, and there exists K = C > 0, such that |kVLtr(wt+1； θt+2)k22 -
kVLtr(wt； θt+1)k22 | ≤ Cαt, by Lemma 2., we can conclude that
lim kVwtLtr(wt； θt+1)k2 =0,
ti∞
which indicates that the gradient of loss on training set of our algorithm will finally achieve to zero,
and thus the iteration of w enables training loss to converge.
19