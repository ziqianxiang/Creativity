Under review as a conference paper at ICLR 2021
Stochastic Normalized Gradient	Descent
with Momentum for Large Batch Training
Anonymous authors
Paper under double-blind review
Ab stract
Stochastic gradient descent (SGD) and its variants have been the dominating op-
timization methods in machine learning. Compared with small batch training,
SGD with large batch training can better utilize the computational power of cur-
rent multi-core systems like GPUs and can reduce the number of communication
rounds in distributed training. Hence, SGD with large batch training has attracted
more and more attention. However, existing empirical results show that large
batch training typically leads to a drop of generalization accuracy. As a result,
large batch training has also become a challenging topic. In this paper, we pro-
pose a novel method, called stochastic normalized gradient descent with momen-
tum (SNGM), for large batch training. We theoretically prove that compared to
momentum SGD (MSGD) which is one of the most widely used variants of SGD,
SNGM can adopt a larger batch size to converge to the -stationary point with the
same computation complexity (total number of gradient computation). Empiri-
cal results on deep learning also show that SNGM can achieve the state-of-the-art
accuracy with a large batch size.
1	Introduction
In machine learning, we often need to solve the following empirical risk minimization problem:
min F (w)
w∈Rd
1n
n X fi(W),
i=1
(1)
where W ∈ Rd denotes the model parameter, n denotes the number of training samples, fi (W)
denotes the loss on the ith training sample. The problem in (1) can be used to formulate a broad
family of machine learning models, such as logistic regression and deep learning models.
Stochastic gradient descent (SGD) Robbins & Monro (1951) and its variants have been the domi-
nating optimization methods for solving (1). SGD and its variants are iterative methods. In the tth
iteration, these methods randomly choose a subset (also called mini-batch) It ⊂ {1, 2, . . . , n} and
compute the stochastic mini-batch gradient 1/B Pii∈ιt Nfi (Wt) for updating the model parameter,
where B = |It| is the batch size. Existing works Li et al. (2014b); Yu et al. (2019a) have proved
that with the batch size of B, SGD and its momentum variant, called momentum SGD (MSGD),
achieve a OQNrTB) convergence rate for smooth non-convex problems, where T is total number
of model parameter updates.
With the population of multi-core systems and the easy implementation for data parallelism, many
distributed variants of SGD have been proposed, including parallel SGD Li et al. (2014a), decentral-
ized SGD Lian et al. (2017), local SGD Yu et al. (2019b); Lin et al. (2020), local momentum SGD Yu
et al. (2019a) and so on. Theoretical results show that all these methods can achieve a O(1∕√TKb)
convergence rate for smooth non-convex problems. Here, b is the batch size on each worker and K
is the number of workers. By setting K b = B, we can observe that the convergence rate of these
distributed methods is consistent with that of sequential methods. In distributed settings, a small
number of model parameter updates T implies a small synchronize cost and communication cost.
Hence, a small T can further speed up the training process. Based on the O(1∕√TKb) convergence
rate, we can find that if we adopt a larger b, the T will be smaller. Hence, large batch training can
reduce the number of communication rounds in distributed training. Another benefit of adopting
1
Under review as a conference paper at ICLR 2021
Figure 1: The training loss and test accuracy for training a non-convex model (a network with two
convolutional layers) on CIFAR10. The optimization method is MSGD with the poly power learning
rate strategy.
large batch training is to better utilize the computational power of current multi-core systems like
GPUs You et al. (2017). Hence, large batch training has recently attracted more and more attention
in machine learning.
Unfortunately, empirical results LeCun et al. (2012); Keskar et al. (2017) show that existing SGD
methods with a large batch size will lead to a drop of generalization accuracy on deep learning
models. Figure 1 shows the comparison of training loss and test accuracy between MSGD with
a small batch size and MSGD with a large batch size. We can find that large batch training does
degrade both training loss and test accuracy. Many works try to explain this phenomenon Keskar
et al. (2017); Hoffer et al. (2017). They observe that SGD with a small batch size typically makes
the model parameter converge to a flatten minimum while SGD with a large batch size typically
makes the model parameter fall into the region of a sharp minimum. And usually, a flatten minimum
can achieve better generalization ability than a sharp minimum. Hence, large batch training has also
become a challenging topic.
Recently, many methods have been proposed for improving the performance of SGD with a large
batch size. The work in Goyal et al. (2017); You et al. (2020) proposes many tricks like warm-
up, momentum correction and linearly scaling the learning rate, for large batch training. The work
in You et al. (2017) observes that the norms of gradient at different layers of deep neural networks
are widely different and the authors propose the layer-wise adaptive rate scaling method (LARS).
The work in Ginsburg et al. (2019) also proposes a similar method that updates the model parameter
in a layer-wise way. Most of these methods lack theoretical evidence to explain why they can adopt
a large batch size. Although the work in You et al. (2020) proposes some theoretical explanations for
LARS, the implementation is still not consistent with its theorems in which both of the momentum
coefficient and weight decay are set as zeros.
In this paper, we propose a novel method, called stochastic normalized gradient descent with mo-
mentum (SNGM), for large batch training. SNGM combines normalized gradient Nesterov (2004);
Hazan et al. (2015); Wilson et al. (2019) and Polyak’s momentum technique Polyak (1964) together.
The main contributions of this paper are outlined as follows:
•	We theoretically prove that compared to MSGD which is one of the most widely used
variants of SGD, SNGM can adopt a larger batch size to converge to the -stationary point
with the same computation complexity (total number of gradient computation). That is
to say, SNGM needs a smaller number of parameter update, and hence has faster training
speed than MSGD.
•	For a relaxed smooth objective function (see Definition 2), we theoretically show that
SNGM can achieve an -stationary point with a computation complexity of O(1/4). To
the best of our knowledge, this is the first work that analyzes the computation complexity
of stochastic optimization methods for a relaxed smooth objective function.
•	Empirical results on deep learning also show that SNGM can achieve the state-of-the-art
accuracy with a large batch size.
2
Under review as a conference paper at ICLR 2021
2	Preliminaries
In this paper, We use ∣∣∙k to denote the Euclidean norm, use W to denote one of the optimal solutions
of (1), i.e., w* ∈ argminw F(w). We call W an E-StatiOnary point of F(W) if ∣VF(w)∣ ≤ e. The
computation complexity of an algorithm is the total number of its gradient computation. We also
give the folloWing assumption and definitions:
Assumption 1 (σ-bounded variance) For any w, E∣Vfi(w) - VF (w)∣* 2 ≤ σ2 (σ > 0).
Definition 1 (Smoothness)Afunction φ(∙) is L-smooth (L > 0) if for any u, W,
φ(u) ≤ φ(w) + Vφ(w)>(u — w) + g∣∣u — w∣∣2∙
L is called smoothness constant in this paper.
Definition 2 (Relaxed smoothness Zhang et al. (2020)) A function φ(∙) is (L, λ)-smooth (L ≥ 0,
λ ≥ 0) if φ(∙) is twice differentiable andfor any W,
∣V2φ(w)∣ ≤ L+ λ∣Vφ(w)∣,
where V2 φ(w) denotes the Hessian matrix of φ(w).
From the above definition, We can observe that if a function φ(w) is (L, 0)-smooth, then it is a
classical L-smooth function Nesterov (2004). For a (L, λ)-smooth function, We have the folloWing
property Zhang et al. (2020):
Lemma 1 If φ(∙) is (L, λ)-smooth, thenforany u, w, α such that ∣∣u 一 Wk ≤ α, we have
∣Vφ(u)∣ ≤ (Lα + ∣Vφ(w)∣)eλα.
All the proofs of lemmas and corollaries of this paper are put in the supplementary.
3	Relationship between Smoothness Constant and Batch Size
In this section, We deeply analyze the convergence property of MSGD to find the relationship be-
tWeen smoothness constant and batch size, Which provides insightful hint for designing our neW
method SNGM.
MSGD can be Written as folloWs:
vt+1 = βvt + gt,
wt+1 = wt — ηvt+1,
(2)
(3)
Where gt = 1/B i∈I Vfi (wt) is a stochastic mini-batch gradient With a batch size of B, and
vt+1 is the Polyak’s momentum Polyak (1964).
We aim to find hoW large the batch size can be Without loss of performance. The convergence rate
of MSGD With the batch size B for L-smooth functions can be derived from the Work in Yu et al.
(2019a). That is to say, when η ≤ (1 一 β)2∕((1 + β)L), we obtain
T-1
X
t=0
1
T
EkVF"C ≤2(1一"(WO)- F(W*)]
ηT
Lησ2	4L2η2σ2
(1 一 β)2B + (1 — β)2,
+
+ O( η) + O(η2),
B
(4)
where C = TB denotes the computation complexity (total number of gradient computation). Ac-
cording to Corollary 1 in Yu et al. (2019a), we set η = BB/√T = B/√C and obtain that
1 T-1
T EEkVF(wt)k ≤
T t=0
3 √C) + O( B2).
(5)
3
Under review as a conference paper at ICLR 2021
Algorithm 1 SNGM
Initialization: η > 0, β ∈ [0,1), B > 0, T > 0, uo = 0, w0;
for t = 0, 1, . . . , T - 1 do
Randomly choose B function indices, denoted as It ;
Compute a mini-batch gradient gt = -^ Pii∈ιt Nfi (wt);
ut+1 = βut + kgtk;
wt+1 = wt - ηut+1;
end for
Since η ≤ (1 一 β)2 *∕((1 + β)L) is necessary for(4), We firstly obtain that B ≤ O(√C/L). Further-
more, according to the right term of (5), we have to set B such that B2/C ≤ 1/√C, i.e., B ≤ C1/4 * * * *,
for O(1/4) computation complexity guarantee. Hence in MSGD, We have to set the batch size
satisfying
B ≤O(min{孚,C")).	(6)
L
We can observe that a larger L leads to a smaller batch size in MSGD. If B does not satisfy (6),
MSGD will get higher computation complexity.
In fact, to the best of our knowledge, among all the existing convergence analysis of SGD and its
variants on both convex and non-convex problems, we can observe three necessary conditions for
the O(1/4) computation complexity guarantee Li et al. (2014b;a); Lian et al. (2017); Yu et al.
(2019b;a): (a) the objective function is L-smooth; (b) the learning rate η is less than O(1/L); (c)
the batch size B is proportional to the learning rate η. One direct corollary is that the batch size is
limited by the smooth constant L, i.e., B ≤ O(1/L). Hence, we can not increase the batch size
casually in these SGD based methods. Otherwise, it may slow down the convergence rate and we
need to compute more gradients, which is consistent with the observations in Hoffer et al. (2017).
4 Stochastic Normalized Gradient Descent with Momentum
In this section, we propose our novel methods, called stochastic normalized gradient descent with
momentum (SNGM), which is presented in Algorithm 1. In the t-th iteration, SNGM runs the
following update:
gt
ut+1 = β ut + 两,
wt+1 = wt 一 ηut+1,
(7)
(8)
where gt = 1/B i∈I Nfi(wt) is a stochastic mini-batch gradient with a batch size of B. When
β = 0, SNGM will degenerate to stochastic normalized gradient descent (SNGD) Hazan et al.
(2015). The ut is a variant of Polyak’s momentum. But different from Polyak’s MSGD which
adopts gt directly for updating ut+1, SNGM adopts the normalized gradient gt/kgtk for updating
ut+1. In MSGD, we can observe that if gt is large, then ut may be large as well and this may lead
to a bad model parameter. Hence, we have to control the learning rate in MSGD, i.e., η ≤ (1/L),
for a L-smooth objective function. The following lemma shows that kutk in SNGM can be well
controlled whatever gt is large or small.
Lemma 2 Let {ut} be the sequence produced by (7), then we have ∀t ≥ 0,
M k≤ i⅛.
4.1 Smooth Objective Function
For a smooth objective function, we have the following convergence result of SNGM:
4
Under review as a conference paper at ICLR 2021
Table 1: Comparison between MSGD and SNGM for a L-smooth objective function. C denotes the
computation complexity (total number of gradient computation).
	T PT=01 EkVF(Wt)Il	learning rate	batch size
MSGD	qO( √ )+O(管	B √C	min{ √, C1/4}
SNGM	O( C1/4 )	√B √	√C
Theorem 1 Let F (w) be a L-smooth function (L > 0). The sequence {wt} is produced by Algo-
rithm 1. Then for any η > 0, B > 0, we have
TX1 EkVF(wt)k ≤ 2(1-β)[F* FM] +Lκη + '％	⑼
T t=0	ηT	B
where K = (I-^2.
Proof 1 See the supplementary.
We can observe that different from (4) which needs η ≤ O(1/L), (9) is true for any positive learning
rate. According to Theorem 1, we obtain the following computation complexity of SNGM:
Corollary 1 Let F(w) be a L-smooth function (L > 0). The sequence {wt} is produced by Algo-
rithm 1. Given any total number of gradient computation C > 0, let T = dC/Be,

B
C(1 — β)σ2
2L(1 + β )(F (w0) — F(w*))
and
=S 2(1 — β )3 (F (wo) — F(w*))B
η =V	(1+ β)LC	.
Then we have
TXIEkVF(Wt)k≤ 2√2s8L(1+ MWe))- F(WW = O(C1L).
Hence, the computation complexity for achieving an -stationary point is O(1/4).
It is easy to verify that the η and B in Corollary 1 make the right term of (9) minimal. However, the
η and B rely on the L and F(w*) which are usually unknown in practice. The following corollary
shows the computation complexity of SNGM with simple settings about learning rate and batch size.
Corollary 2 Let F(w) be a L-smooth function (L > 0). The sequence {wt} is produced by Algo-
rithm 1. Given any total number of gradient computation C > 0, let T = dC/B], B = √C and
η = B/C/C. Then we have
1 T-1
T EEkVF(wt)k ≤
T t=0
2(1 — β)[F (wo) — F (w*)]	L(1 + β)	2σ
CV4	H (1 — β)2C1/4 + C1/4
O( C1/4).
Hence, the computation complexity for achieving an -stationary point is O(1/4).
According to Corollary 2, the batch size of SNGM can be set as O(√C), which does not rely on
the smooth constant L, and the O(1/4) computation complexity is still guaranteed (see Table 1).
Hence, SNGM can adopt a larger batch size than MSGD, especially when L is large.
5
Under review as a conference paper at ICLR 2021
4.2 Relaxed Smooth Objective Function
Recently, the authors in Zhang et al. (2020) observe the relaxed smooth property in deep neural
networks. According to Definition 2, the relaxed smooth property is more general than L-smooth
property. For a relaxed smooth objective function, we have the following convergence result of
SNGM:
Theorem 2 Let F(w) be a (L, λ)-smooth function (L ≥ 0, λ > 0). The sequence {wt} is produced
by Algorithm 1 with the learning rate η and batch size B. Then we have
T1
1 X EkVF (Wt)k≤ 2 - β)[ (W0)- (W )] +8Lκη + √σ,	(10)
T t=0	ηT	BB
where K =(1-^2 and η ≤ 1∕(8κλ).
Proof 2 The proof is similar to that of Theorem 1. See the supplementary.
According to Theorem 2, we obtain the computation complexity of SNGM:
Corollary 3 Let F (W) be a (L, λ)-smooth function (L ≥ 0, λ ≥ 0). The sequence {Wt} is produced
by Algorithm 1. Given any total number of gradient computation C > 0, let T = 「C/B~∖, B = √C
and η = 4/ 1/C ≤ 1∕(8κλ). Then we have
T1
T X EkVF(wt)k ≤ 2(1-β)[FCW4)- F(w*)] + ≡⅛⅛ + C⅛ = O(C⅛4).
-
Hence, the computation complexity for achieving an -stationary point is O(1/4).
According to Corollary 3, SNGM with a batch size of B = √C can still guarantee a O(1∕e4)
computation complexity for a relaxed smooth objective function.
5 Experiments
All experiments are conducted with the platform of PyTorch, on a server with eight NVIDIA Tesla
V100 (32G) GPU cards. The datasets for evaluation include CIFAR10 and ImageNet.
5.1	ON CIFAR10
First, we evaluate SNGM by training ResNet20 and ResNet56 on CIFAR10. CIFAR10 contains
50k training samples and 10k test samples. We compare SNGM with MSGD and an existing large
batch training method LARS You et al. (2017). We implement LARS by using the open source
code 1. The standard strategy He et al. (2016) for training the two models on CIFAR10 is using
MSGD with a weight decay of 0.0001, a batch size of 128, an initial learning rate of 0.1, and
dividing the learning rate at the 80th and 120th epochs. We also adopt this strategy for MSGD
in this experiment. For SNGM and LARS, we set a large batch size of 4096 and also a weight
decay of 0.0001. Following You et al. (2017), we adopt the poly power learning rate strategy and
adopt the gradient accumulation Ott et al. (2018) with a batch size of 128 for the two large batch
training methods. The momentum coefficient is 0.9 for all methods. Different from existing heuristic
methods for large batch training, we do not adopt the warm-up strategy for SNGM.
The results are presented in Figure 2. As can be seen, SNGM achieves better convergence rate on
training loss than LARS. The detailed information about the final convergence results is presented
in Table 2. We can observe that MSGD with a batch size of 4096 leads to a significant drop of test
accuracy. SNGM with a batch size of 4096 achieves almost the same test accuracy as MSGD with
a batch size of 128. But the other large batch training method LARS achieves worse test accuracy
than MSGD with a batch size of 128. These results successfully verify the effectiveness of SNGM.
1https://github.com/noahgolmant/pytorch-lars
6
Under review as a conference paper at ICLR 2021
ssol gniniart
—MSGD, B=128
— LARS, B=4096
-SNGM, B=4096
50	100	150
#grad/n
ssol gniniart
#grad/n
(b) ResNet56
(a) ResNet20
(c) ResNet20
(d) ResNet56
O

Figure 2:	Learning curves on CIFAR10.
Table 2: Experimental results on CIFAR10. In LARS with warm-up, we adopt the gradual warm-up
strategy and a power of 2, which is the same setting as that in You et al. (2017). In the warm-
up stage (5 epochs), the learning rate increases from 0.1 to the target (2.4 in ResNet20 and 2.8 in
ResNet56) gradually.
		warm-up	initial learning rate	power	batch size	test accuracy
	MSGD	-	01	-	128	-91.63%-
	MSGD	-	0.4	-	4096	89.25%
ResNet20	LARS	No	0.8	1.1	4096	90.66%
	LARS	Yes	2.4	2	4096	90.80%
	SNGM	No	1.6	1.1	4096	91.42%
	MSGD	-	01	-	128	-93.11%-
	MSGD	-	0.3	-	4096	88.55%
ResNet56	LARS	No	0.64	1.1	4096	92.46%
	LARS	Yes	2.8	2	4096	92.98%
	SNGM	No		13		1.1	4096	93.12%
5.2 On ImageNet
We also compare SNGM with MSGD by training ResNet18 and ResNet50 on ImageNet. The stan-
dard strategy He et al. (2016) for training the two models on ImageNet is using MSGD with a weight
decay of 0.0001, a batch size of 256, an initial learning rate of 0.1, and dividing the learning rate at
the 30th and 60th epochs. We also adopt this strategy for MSGD in this experiment. For SNGM, we
set a larger batch size of 8192 and a weight decay of 0.0001. We still adopt the poly power learning
rate and the gradient accumulation with a batch size of 128 for SNGM. We do not adopt the warm-
up strategy for SNGM either. The momentum coefficient is 0.9 in the two methods. The results are
7
Under review as a conference paper at ICLR 2021
presented in Figure 3 and Table 3. As can be seen, SNGM with a larger batch size achieves almost
the same test accuracy as MSGD with a small batch size.
(a) ResNet18
(b) ResNet50
Figure 3:	Learning curves on ImageNet.
Table 3: Experimental results on ImageNet.
		initial learning rate	Power	batch size	test accuracy
ResNet18	MSGD	0.1	-	-256-	-69.71%-
	SNGM	0.8	2	8192	69.65%
ResNet50	MSGD	0.1	-	-256-	-75.70%-
	SNGM		0.8		2	8192	75.42%
6 Conclusion
In this paper, we propose a novel method called stochastic normalized gradient descent with mo-
mentum (SNGM), for large batch training. We theoretically prove that compared to MSGD which
is one of the most widely used variants of SGD, SNGM can adopt a larger batch size to converge
to the -stationary point with the same computation complexity. Empirical results on deep learning
also show that SNGM can achieve the state-of-the-art accuracy with a large batch size.
References
Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan
Leary, Jason Li, Huyen Nguyen, and Jonathan M. Cohen. Stochastic gradient methods with layer-
wise adaptive moments for training of deep networks. CoRR, abs/1905.11286, 2019.
Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ima-
genet in 1 hour. CoRR, abs/1706.02677, 2017.
Elad Hazan, Kfir Y. Levy, and Shai Shalev-Shwartz. Beyond convexity: Stochastic quasi-convex
oPtimization. In Advances in Neural Information Processing Systems, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In Proceedings of Conference on Computer Vision and Pattern Recognition, 2016.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gaP in large batch training of neural networks. In Advances in Neural Information Processing
Systems, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deeP learning: Generalization gaP and sharP minima. In
Proceedings of the International Conference on Learning Representations, 2017.
8
Under review as a conference paper at ICLR 2021
Yann A. LeCun, Leon Bottou, Genevieve B. Orr, and KlaUs-Robert Muller. Efficient BackProp, pp.
9-48. Springer, 2012.
Mu Li, David G. Andersen, Alexander J. Smola, and Kai Yu. Communication efficient distributed
machine learning with the parameter server. 2014a.
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J. Smola. Efficient mini-batch training for
stochastic optimization. In Proceedings of the ACM Conference on Knowledge Discovery and
Data Mining, 2014b.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? A case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, 2017.
Tao Lin, Sebastian U. Stich, Kumar Kshitij Patel, and Martin Jaggi. Don’t use large mini-batches,
use local SGD. In Proceedings of the International Conference on Learning Representations,
2020.
Yurii E. Nesterov. Introductory Lectures on Convex Optimization - A Basic Course, volume 87 of
Applied Optimization. Springer, 2004.
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine translation.
In Proceedings of the Conference on Machine Translation, 2018.
Boris Polyak. Some methods of speeding up the convergence of iteration methods. Ussr Computa-
tional Mathematics and Mathematical Physics, 4:1-17, 12 1964.
Herbert Robbins and Sutton Monro. A stochastic approximation method. The Annals of Mathemat-
ical Statistics, 22(3):400-407, 1951.
Ashia C. Wilson, Lester Mackey, and Andre Wibisono. Accelerating rescaled gradient descent: Fast
optimization of smooth functions. In Advances in Neural Information Processing Systems, 2019.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training.
CoRR, abs/1708.03888, 2017.
Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training BERT in 76 minutes. In Proceedings of the International Conference on
Learning Representations, 2020.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient mo-
mentum SGD for distributed non-convex optimization. In Proceedings of the 36th International
Conference on Machine Learning, 2019a.
Hao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less
communication: Demystifying why model averaging works for deep learning. In Proceedings of
the AAAI Conference on Artificial Intelligence, 2019b.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In Proceedings of the International Conference
on Learning Representations, 2020.
9
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Proof of Lemma 1
The proof follows Zhang et al. (2020). We put it here for completeness. For any u, w, let r(x)
x(u 一 w)+ w,p(x) = ∣∣Vφ(r(x))k, X ∈ [0,1]. Then We have
p(x) =∣Vφ(r(x))∣ = ∣	Hφ(r(y))r0(y)dy + Vφ(r(0))∣
0
=∣	Hφ(r(y))(u 一 w)dy + Vφ(w)∣
0
≤∣u 一 w∣	∣Hφ(r(y))∣dy + ∣Vφ(w)∣
0
≤α	(L + λ∣Vφ(r(y))∣)dy + ∣Vφ(w)∣
0
=Lα + ∣Vφ(w)∣ + λα	p(y)dy.
0
According to GronWall’s Inequality, We obtain
p(x) ≤ (Lα + ∣Vφ(w)∣)eλα
A.2 Proof of Lemma 2
According to (7), We have
∣ut+1∣ ≤β∣ut∣ +1
≤β2∣ut-1∣ + β + 1
≤βt+1kuok + β t + β t-1 + ∙∙∙ + 1
1
≤ L.
A.3 Proof of Theorem 1
Let Zt = wt + ι-ββ(wt — wt-ι), then we have wt+ι = Wt — ηkgtk + β(wt — wt-ι) and
1	β
zt+1 =T-βwt+1 一 T-βwt
=_ η	gt
Zt 1-β kgtk.
Using the smooth property, we obtain
F(zt+1) ≤F(Zt)- T¾VF(Zt)T而 +2UL⅛
=F(Zt) - T¾ kgtk + 2(1 一 β)
一 1一β[(VF(Zt)- VF(Wt))Tkgtk + (VF(Wt)- gt)Tkgtk]
≤F(Zt) — 1 η k kgtk + n 、2 + 1 "得[LkZt 一 Wtk + IlVF(Wt) — gtk] (II)
1 - β 2(1 - β)	1 - β
Since Wt+ι - Wt = β(wt - Wt-ι) - ηgt/kgtk, Weobtain
η
kWt+1 - Wtk ≤ βkWt - Wt-Ik + η ≤ 1-J.
10
Under review as a conference paper at ICLR 2021
Hence, ∣wt - wt-1 ∣ ≤ η∕(1 - β) and
β	βη
kZt - Wtk = 1-β kwt - Wt-Ik ≤ (1 - β)2 .
(12)
Combining the above equations, we obtain
kgtk≤(I-”(Zt)- F(Zt+°] +
Lη
Lβη
2(1 - β) + (1 - β)
2 + ∣VF(Wt) - gt∣.
η
Since ∣∣VF (wt)k ≤ ∣∣VF (Wt)- gtk + ∣∣gtk, we obtain
∣VF(Wt)∣≤ (I-β"(Zt)- F(Zt+0〕+
Lη
Lβη
2(1 - β) + (1 - β)
2 + 2∣VF(Wt) - gt∣.
η
Using the fact that EkVF(Wt) - gt∣ ≤ σ∕√B and summing UP the above inequality from t = 0 to
T - 1, we obtain
1 T-1
T EEkVF(wt)k ≤
T t=0
2(1 - β)[F(wo)- F(w*)]
ηT
2σ
+ Lκη + 适
A.4 Proof of Theorem 2
Let Zt = Wt + 1-ββ(wt - wt-ι), then we have wt+ι = Wt - ηkg^ + β(wt - wt-ι) and
zt+1
1
口
1
1-β
1
Wt+1-占Wt
gt
[wt - η∏~~H-
∣gt ∣
β
β
+ β(wt - Wt-l)] - 1-βWt
η	gt
=1-βwt - 1-β WtT- 1-β 两
=_ η gt
=Zt - 口 两.
Using the Taylor theorem, there exists ξt such that
F(zt+1) ≤F(zt) -
=F(zt) -
η V7PS ∖τ gt IkHF(ξt)kη2
KVF(Zt)两 + 2(1-β)2
η k k + kHF(ξt)llη2
1 - β kgtk + 2(1 - β)2
士[(VF(Zt)- VF(Wt))Tɪ + (VF(Wt)- gt)Tɪ〕.
(13)
Let ψt(w) = (VF(w) 一 VF(Wt))Tkgtk. Using the Taylor theorem, there exists Zt such that
∣Ψt(Zt)| =∣ψt(wt) + Vψt(Zt)(Zt - Wt)| = ∣Vψ(Zt)(Zt - Wt)|
≤kHF(ζt)kkZt -Wtk.
Combining (13) and (14), we obtain
(14)
∣gt ∣ ≤
(1- β)[F(Zt)- F(Zt+ι)] + ∣Hf(ξt)kη
+ 2(1 - β)
+ (∣HF (ζt)∣∣Zt - Wt∣ + ∣VF (Wt) - gt∣).
Since wt+ι — Wt = β(wt — wt-ι) 一 ηgt∕∣∣gt∣∣,Weobtain
(15)
η
kwt+1 - Wtk ≤ βkwt - Wt-lk + η ≤ 1-J.
—
η
11
Under review as a conference paper at ICLR 2021
Hence, kwt - Wt-Ik ≤ η∕(1 - β) and
kzt - Wtk = ———IlWt - Wt-IIl ≤ 7~Bn B
II Z t∣∣	1 - β∣∣ t t 1∣∣ -(1 - β)2
Combining (15) and (16), we obtain
(16)
kgtk≤(1-e)[F(Zt)- F(Zt+1)] +
kHF(ξt)kn + IHf (Zt)kβn
2(1 - β) +	(1 - β)2
η
+ INF(Wt)- gtk.
Since ∣∣VF (wz)∣ ≤ ∣∣VF(Wt)- gtk + ∣∣gt∣, we obtain
kVF(Wt)k ≤(1-e)[F(Zt)- F(zt+ι)l +
端-历 kHF (ξt)k + (Γ-⅛ kHF (Zt)k
η
+ 2IVF (Wt) - gtI.
Next, We bound the two Hessian matrices. For convenience, We denote K = .1+^. Since ∣∣zt -
Wt I ≤ βη∕(1 - β)2 and
∣Zt+1 - Wt∣ ≤∣Zt+1 - Zt∣ + ∣Zt - Wt∣
1β
≤n(I - β + (1 - β)2)
≤κη
1
≤ λ,
we obtain
∣HF(ζt)∣ ≤ L + (L +λ∣VF(Wt)∣)e,
∣HF(ξt)∣ ≤ L + (L +λ∣VF(Wt)∣)e.
Then we obtain
kVF(Wt)k ≤ (I- β)[F(Zt)- F(Zt+1)] + [ —ɪ- + -^n-][L +(L + λ∣VF(Wt)∣)e]
η	2(1 - β)	(1 - β)
+ 2kVF(Wt) - gtk
≤(I - β)[F(Zt)- F(Zt+l)] + 4κn[L + λ∣VF(Wt)k]
η
+ 2kVF(Wt) -gtk.
Since 4λκη ≤ 1∕2, we obtain
kVF(Wt)k ≤2(1 - β)[F(Zt)- F(Zt+l)] + RLal + 4∣∣VF(Wt) - gtk.
η
Summing up the above inequality from t = 0 to T - 1, we obtain
T ∑ EkVF (Wt)k ≤ 2(1-β)[F (WO)- F (W*)] +8Lκn + %
T t=0	ηT	B
where η ≤ 8λK and we use the fact that EkVF (Wt) - gt k ≤ σ∕√B.
A.5 Proof of Corollary 1
Let X = 2(1 - β)[F(w0) - F(w*)], y = Lκ, Z = 2σ. Then we have
胃 + yn + √ ≥ 2^B + √B ≥ 2s2zrf = 2√2存.
The equal sign works if and only if n = {Bx∕Cy B = 'Cz2/(4xy). Then we obtain
ɪ ∑ EkVF (wt)k ≤ 2√2 SS8L(1+ β)[F(WT)- F (W*W.
T t=0	(1 - β)C
12
Under review as a conference paper at ICLR 2021
A.6 Proof of Corollary 2
By plugging T =「C/B], B = √C and η = P1/C into (9), We obtain the result.
A.7 Proof of Corollary 3
By plugging T =「C/B~∖, B = √C and η = P1/C into (10), We obtain the result.
13