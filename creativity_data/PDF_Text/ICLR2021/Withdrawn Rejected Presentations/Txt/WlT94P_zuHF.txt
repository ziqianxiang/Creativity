Under review as a conference paper at ICLR 2021
Transformer-QL: A Step Towards Making
Transformer Network Quadratically Large
Anonymous authors
Paper under double-blind review
Ab stract
Transformer networks have shown outstanding performance on many natural lan-
guage processing tasks. However the context length (the number of previous to-
kens on which the output states depend) of a Transformer network grows at best
linearly with the memory and computational power used. This limitation prevents
a transformer network to have very long context in a resource limited application.
In this work, we propose a class of transformer networks, namely Transformer-QL
(Quadratically Large), in which, the context length can grow at best quadratically
with the memory and computational power used. We have empirically evaluated
a Transformer-QL model in three long range language modeling datasets. The re-
sults show that Transformer-QL can provide significant improvements over other
state of the art networks.
1	Introduction
Since its introduction in Vaswani et al. (2017), Transformer networks have overtaken its predecessor
Recurrent Neural Networks (RNN) in almost every natural language processing task. However, one
limitation of Transformer network is its high requirement of memory and computational power. In a
vanilla Transformer network, the memory and computational requirement grows quadratically with
the sequence length, and thus with context length.
In an effort to overcome the above limitation, Transformer-XL (Dai et al., 2019) and Compressive
Transformer (Rae et al., 2020) have been recently proposed. However, in both the network, the
context length can grow at best linearly with the memory and computation usage. An alternative
strategy have been explored in Li et al. (2019); Ye et al. (2019); Child et al. (2019); Zaheer et al.
(2020); Beltagy et al. (2020); Wang et al. (2020b); Kitaev et al. (2020); Katharopoulos et al. (2020);
Choromanski et al. (2020); Wang et al. (2020a). All these works have proposed to replace the vanilla
self-attention network by a different one with linear or log-linear memory and computation complex-
ity leading to novel transformer architectures with overall linear or log-linear cost. Although, they
provide an improvement over the quadratic cost of the vanilla transformer network, the achieved
cost is still, at best, linear. Besides, since those techniques are based on either sparsification or
compression of the self attention mechanism, they struggle to accumulate long distance information
(Gupta & Berant, 2020).
Several works such as Burtsev & Sapunov (2020); Ainslie et al. (2020); Gupta & Berant (2020)
have proposed to increase the context length by introducing a global attention which attains to every
input token, thus capable of capturing long distance dependency. However, capturing long distance
dependency using those approaches involves extreme compression of state space by the global at-
tention mechanism. Moreover, even though, they perform well on several tasks, their performance
on language modeling task have not been tested. Another line of work (Zhang et al., 2019; Pappa-
gari et al., 2019) have suggested to use hierarchical arrangement of transformer network to capture
document-wide dependency. However, applicability of those networks requires hierarchical struc-
ture in the data itself. Moreover, those techniques have been proposed for document compression
rather than language modeling.
In this paper, we propose a class of transformer architectures, namely Transformer-QL
(Quadratically Large), to alleviate the problem of capturing long distance dependency. Similar to
multi-scale transformer networks (Donahue et al., 2019; Subramanian et al., 2020; Zhao et al., 2020;
1
Under review as a conference paper at ICLR 2021
Dai et al., 2020), Transformer-QL captures the contextual information in multiple temporal scales -
finer scales to capture recent past information and coarser scales to capture distance past information.
Additionally like Transformer-XL, it keeps the hidden states of a past segment in memory and use it
to process future segments causing the context length to grow beyond the current segment. Overall,
the context length in Transformer-QL can grow up to quadratically with the memory/computational
usage. The contributions of the work are as follows:
•	We have proposed a novel class of transformer architectures, namely Transformer-QL, in
which, the context length can be made to grow linearly with memory and computation cost.
Further, employing a linear cost self attention layer like Wang et al. (2020b); Katharopoulos
et al. (2020), the context length of Transformer-QL can be made to grow quadratically in
both memory and computational cost.
•	We have empirically evaluated a Transformer-QL model on three long range language
modeling datasets. The results show significant improvement in perplexity score over
Transformer-XL and Compressive Transformer.
The organization of the paper is as follows. In section 2, the proposed Transformer-QL architec-
ture along with its background has been introduced. Section 3 provides empirical evaluation of
Transformer-QL. The section also studies the sensitivity of Transformer-QL to several hyperparam-
eters. Finally, in Section 4, the conclusion has been drawn and future directions of the work have
been suggested.
2	Method
2.1	Terminology and Notations
In a transformer network, the input sequence are partitioned into smaller segments of fixed length.
Each segment is processed independently of other segments. We refer to the number of tokens in
each segment as segment length. In a transformer network with recurrent memory like Transformer-
XL, the hidden states of the recent past segments are preserved in a fixed length memory. We refer
to the number of tokens in each layer of the memory unit as memory length. For an output state
(i.e. the output states of the last layer), we use the term context length to refer to the number of past
tokens on which the output state depends. In transformer network, different output states might have
different context length. We respectively refer the minimum and maximum of the context lengths
of all the output states in a network as minimum context length and maximum context length of the
network. We refer the sum of segment length and the memory length using the term window length.
We denote the segment length, memory length, window length and model dimension by ns, nm, nw
and dm respectively. Thus, we have nw = ns + nm . We also use the notations slt and mlt to denote
the output and memory of l-th layer at time step t for l = 1, 2,…，L where L is the total number
of Layers. The output and memory of embedding layer at time step t have been denoted by st0 and
mt0 respectively. The number of heads in the self attention layers has been denoted by H .
2.2	Background
Transformer A transformer network consists of stacked collection of multiple transformer layers.
Each transformer layer contains a multi-head self-attention layer followed by a position-wise feed
forward layer. Though the memory and computational cost of position-wise feed forward layer is
linear in the length of input sequence, the multi-head self attention layer has a quadratic cost. The
transformer network tackles the quadratic memory and computational cost by dividing the input se-
quence into smaller segments and applying the transformer network on each segment independently.
However, such method limits the context lengths to the segment length. Dai et al. (2019) has named
this problem as context fragmentation problem.
Transformer-XL Dai et al. (2019) has proposed Transformer-XL to solve the context fragmenta-
tion problem. In Transformer-XL, instead of discarding the hidden states after the computation of a
segment, they are saved in memory (please refer to Figure 3). During the computation of the follow-
ing segments, the self attention is applied over the hidden states of both the current segment and the
2
Under review as a conference paper at ICLR 2021
memory, thus has an increased context length without quadratic increase in the memory and compu-
tational cost. In fact, the memory/computational cost of the self-attention layer of Transformer-XL
grows only quadratically only with the segment size ns and linearly with the memory length nm .
On the other hand, the context lengths get increased by a length of nm per layer. By keeping ns
small and making nm large enough, the memory and computational cost of Transformer-XL can be
made close to linear with respect to the context lengths. Rae et al. (2020) has proposed to improved
the memory/computational cost of Transformer-XL further by keeping the part of the memory states
in a compressed form. However, even with this improvement, the memory and computational cost
can be at best linear in the context length.
2.3	The Model
- Transformer-XL Layer -
- Transformer-XL Layer -
........... 一
,、\、 Compression 一
⅛⅛□□□□--
- Transformer-XLLayer -
∖∖∖∖ 1 一
∖-∖∖ Compression -
Causal Accumulation Layer


Scale 1	Scale 2	Scale 3	Output Layers
Figure 1:	High Level Visualization of Proposed Model. The model processes the input tokens in
multiple temporal scales. Each scale has several transformer layers with recurrent memory. The
output of the last layer of one scale is compressed to form the input of the next scale. As the
segment length gets reduced because of compression, the memory length is increased to make the
total length (i.e. segment length + memory length) of all the layers same. In the figure, the blue
boxes represent hidden states of the current time step where as the red boxes represent the memory
states.
Overview In this paper, we explore to increase the context length by compressing the hidden
states hierarchically. The high level view of our architecture is shown in Figure 1. As shown in the
figure, the model processes the input sequence in several scales of temporal granularity. Each scale
consists of several transformer layers with recurrent memory. The output of the last layer of one
scale is compressed causing the temporal granularity as well as the segment length to reduce. As the
segment length reduces, we also simultaneously increase the memory length to keep the total length
(i.e. segment length + memory length) of the layer constant. Then the new segment and memory is
fed as the input to the first layer of the next scale. The resulting architecture is similar to the multi-
scale transformer architectures (Donahue et al., 2019; Subramanian et al., 2020; Zhao et al., 2020).
Additionally, Transformer-QL keeps recurrent memory to store hidden states of previous segments.
Therefore, in Transformer-QL, the layers belonging to a finer scale process contextual information
in fine-grained manner, but have a smaller context length. On the other hand, a layer belonging to a
coarser scale process information in coarse-grained manner, but have a longer context length (please
refer to Figure 5 for a detailed illustration of the context lengths of Transformer-QL layers). To get
the final output of the network, we causally combine the (possibly over-sampled) outputs of the last
layer of each scale and pass those through several transformer layers (following Subramanian et al.
(2020); Zhao et al. (2020)) to learn deep representation of the output.
3
Under review as a conference paper at ICLR 2021
1
2
3
4
5
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
Function Compute(s, m, l)	Function Shift(s, m)	
begin	1 begin	
S0 — MultiHeadSAttnl (s , m)	2	nm — len(m)
S00 — LayerNOrm(S + s0)	3	m0 - concat (m, s)
s000 — LayerNorm(s00 + POSWiSeFFl(S00))	4	m00 J m0[-nm,:]
return s000	5	return stop_gradient(m00)
(b) One Transformer-XL layer over hidden states s and
memory m
(d) Shift the current hidden state s into memory
m
At the beginning
begin
m0,…,mL 1 . 0
t — 1
// initialize all the memory to zero
// initialize time step to 1
while there is more data to process do
s0 — XtWemb	// embed input segment
mt0 —Shift(S0, m0-1)	// shift hidden states into memory
Shift」en — n0	// initialize shift length to n0
acc_inputs =[]	// list to hold all the inputs of accumulation layer
lg — 1	// initialize global layer index to 1
for i — 1 to no_ofscales do
Li — no of layers at scale i	// set Li to the number of layers at scale i
for l - 1 to Li do
Stg — ComPute(Stg-1, mt--1 ,lg)	// run one Transformer-XL layer
mtg — Shift(Stg[: Shift」en], m；-1)	// shift hidden states into memory
lg — lg + 1	// increase the global layer index
accdnputs.append([s；g 1, m；g 1])	// put output of scale i in acc_inputs
if i < no _ofscales then
// not the last scale, add a compression layer
Stg — fc(concat(mt- 1 1, Stg 1))[-nSg :]	// compress the states
Shift-Ien — Shift」en/c	// reduce the Shift-Ien for the new scale
mtg — Shift(Stg[: Shift」en], mt—1)	// shift hidden states into memory
lg — lg + 1	// increase the global layer index
Shift-Ien — n0	// set the shift length for the output layers
Stg — ACCumuIate(acc_inputS)	// combine the states in acc_inputs
mtg — Shift(Stg[: shift」en], mt—1)	// shift hidden states into memory
lg — lg + 1	// increase the global layer index
Lo — no of output layers	// set Lo to the number of output layers
for l - 1 to Lo do
Stg — ComPute(Stg-1, mg-11,lg)	// one Transformer-XL layer
mtg — Shift(Stg[: shift」en], mt—1)	// shift hidden states into memory
lg — lg + 1	// increase the global layer index
t — t + 1	// increase the time step
(e) Forward paSS of TranSformer-QL. Slt and mlt repreSentS the hidden StateS and memory StateS of layer l at
time Step t. nls and nlm repreSent the Segment length and memory length of l-th layer.
Figure 2:	TranSformer-QL Algorithm.
The Compression Function For CompreSSion, we uSe one of average pooling and max pooling
with pool Size and Stride both equal to c where c iS the rate by whiCh we CompreSS the StateS while
tranSitioning from one SCale to the next. Let slt and mlt be the output and memory StateS of l-th layer
with length nls and nlm reSpeCtively. We apply the CompreSSion funCtion on the ConCatenation of mlt
and slt to get the output slt+1 of length nls+1 = (nls + nlm)/c (for SimpliCity aSSume that nls + nlm
iS diviSible by c). If nls+1 > nls, we take the laSt nls elementS of slt+1 to form the output of the
4
Under review as a conference paper at ICLR 2021
compression layer. Finally, we keep a recurrent memory mlt+1
l l	l+1	l+1
ns + nm = ns + nm o o.
of length nls + nlm - nls+1 making
The Memory Updates In Transformer-XL with segment length ns and memory length nm , the
segment of length ns is get shifted into the memory. In other words, the memory for the next time
step is computed as mlt+1 = concat(mlt, slt)[-nm :] for all layer l. However, in Transformer-QL,
the granularity of layers belonging to different scales are different. More precisely, a segment of
length ns0 belonging to scale 1 is compressed into a segment of length ns0/ci-1 at a layer belonging
to scale i. Thus, in Transformer-QL, we update the memory of a layer l belonging to scale i as
mlt+1 = concat(mlt, slt[: nih])[-nim :] where nih = ns0/ci-1 and nim are the shift length and the
memory length at scale i for i = 0,1,•…respectively. The complete algorithm of Transformer-QL
is shown in Figure 2.
Droppath Regularization Since the output of the last layer of every scale is summed in the ac-
cumulation layer, the path through a higher scale forms to a deeper network while the path through
a lower scale forms to a shallower network. Consequently, layers in the higher scales might remain
under-fitted due to lack of gradient flow through the deeper network while the layers in the lower
scales get over-fitted. To alleviate this problem, we have introduced droppath regularization. In
1l
the accumulation layer, let each output be computed as So = 卞 Ei=I S where S represents the
(possibly over-sampled) output of scale i and l is the total number of scales. In droppath regular-
ization with droppath probability p, we drop the output of all the scales below j with a probability
p/(l - 1) for j = 2, 3,…，l from the accumulated output. More precisely, We generate a random
number U from uniform probability distribution and compute the output as So = ι-j+ι P；=j Si if
U ∈ [ (j—2)p, (j[?p]. For u ≥ p, no droppath is applied.
2.4 The Complexity
The memory/computational complexity of a Transformer-XL network (Dai et al., 2019) with seg-
ment length n§, memory length nm, and L layer is Θ((ɑ(nm/,ns) + n§)L) where α(∙, ∙) is the
complexity of self-attention layer. The context length nc of the network is Θ(nmL). Since
α(nm, ns) = Ω(nm + n§) (Li et al., 2019; Ye et al., 2019; Child et al., 2019; Zaheer et al., 2020;
Beltagy et al., 2020; Wang et al., 2020b; Kitaev et al., 2020; Katharopoulos et al., 2020; Choroman-
ski et al., 2020), the memory and computational complexity of Transformer-XL in terms of context
length is Ω(n). Similarly, the memory and computational complexity of Compressive Transformer
(Rae et al., 2020) in term of context length is Ω(njc) where C is the compression rate. Therefore, the
memory/computational complexity of both Transformer-XL network and Compressive Transformer
network in term of the context length is at least linear. Consequently, increasing the context length in
both the networks requires at least linear increase in the amount of both memory and computational
requirements.
On the other hand, a Transformer-QL network with L Transformer-XL layers and i compression
layers, the context length nc becomes Θ(ci(ns + nm)) = O(clogcns (ns + nm)) = O(ns (ns +nm))
where ns = ns0 and nm = n0m are the segment and memory length in scale 1 of the network.
Note that, since at most i = logcns compression layer can be used in Transformer-QL, we have
ci = O(clogcns) = O(ns). If we set nm = O(ns), we have nc = O((ns)2). However, the
time and memory complexity of a Transformer-QL network is Θ(α(ns, nm)L + (ns + nm)i)) =
Θ(α(ns,nm)(L + i)). Since a(n§,nm) = C(n§ + nm) and we set nm, = O(n§), the mem-
ory/computational complexity of Transformer-QL becomes Ω(ns(L + i)). Therefore, the mem-
ory/computational complexity of Transformer-QL in terms of context length is Ω(√nC(L + i))=
Ω (√nC(L + log°ns)). Thus, the complexity of Transformer-QL can be at best sub-linear. More-
over, if we set the compression rate c to ns, the memory and computational complexity can be at
best Θ(√nC) or, in other words, the context length can be at best quadratic in the memory and com-
putational cost. In Appendix B, we provide an algorithm to compute a tight estimation of the context
length of a Transformer-QL network. In the appendix, we have also provided a detailed illustration
of the dependency structure of the hidden states of a Transformer-QL network on the past tokens.
5
Under review as a conference paper at ICLR 2021
Dataset	Model	Test ns/nm /ncm	nw	Average test nc	Test Perplexity
	Transformer-XL	04/12/-^^	16	98	20.15
	ComP-Transformer	04/06/06	16	146	19.67
SimPleBooks-2	Transformer-QL	04/12/-	16	138	18.78
	Transformer-XL	08/24/-^^	32	-196	19.61
	Comp-Transformer	08/12/12	32	292	19.15
	Transformer-QL	08/24/-	32	276	18.56
	Transformer-XL	04/12/-^^	16	98	12.93
	Comp-Transformer	04/06/06	16	146	12.49
SimPleBooks-92	Transformer-QL	04/12/-	16	138	12.17
	Transformer-XL	08/24/-^^	32	-196	12.35
	Comp-Transformer	08/12/12	32	292	12.01
	Transformer-QL	08/24/-	32	276	11.88
	Transformer-XL	04/12/-^^	16	98	31.19
	Comp-Transformer	04/06/06	16	146	31.91
WikiText-103	Transformer-QL	04/12/-	16	138	29.13
	Transformer-XL	08/24/-^^	32	-196	27.52
	Comp-Transformer	08/12/12	32	292	27.19
	Transformer-QL	08/24/-	32	276	26.63
Table 1: Perplexity scores (lower is better) of the three networks: Transformer-QL, Transformer-
XL and Compressive Transformer (Comp-Transformer). The third column shows the segment
length (ns), memory length (nm) and compressed memory length (ncm) of the test model. The
forth column shows the window length nw of the test model. Note that, for Transformer-XL and
Transformer-QL, the window length is ns + nm and for Compressive Transformer the window
length is ns + nm + ncm . The average test context length nc has been shown in the fifth column.
The average test context length has been computed by taking the average of the minimum and max-
imum context length of the test model where the minimum and maximum context length have been
calculated using the algorithm of Appendix B.
3	Empirical Evaluation
In this section, we empirically evaluate the efficacy of Transformer-QL for long range language mod-
eling task. Towards that goal, we compare the results of Transformer-QL with that of Transformer-
XL (Dai et al., 2019) and Compressive Transformer (Rae et al., 2020). Then we evaluate the sensi-
tivity of Transformer-QL to several hyper-parameters.
3.1	Comparison with State of the Art Methods
State of the Art Methods We compare Transformer-QL network with the following two networks:
Transformer-XL (Dai et al., 2019) Transformer-XL is similar to vanilla Transformer with two
modifications. It uses recurrent memory to store and access the hidden states of the past
time steps. The recurrent memory enables to increase the minimum context length up to
nmL where nm is the memory length and L is the number of layers. It also uses relative
positional embedding of token instead of absolute positional embedding.
Compressive Transformer (Rae et al., 2020) Like Transformer-XL, Compressive Transformer
also uses recurrent memory. However, Compressive Transformer keeps part of the recurrent
memory in compressed format, thus has an increased context length over Transformer-XL.
Datasets We compare Transformer-QL against the above two networks on three long range lan-
guage modeling datasets: SimpleBooks-2 (Nguyen, 2019), SimpleBooks-92 (Nguyen, 2019) and
WikiText-103 (Merity et al., 2017). SimpleBooks-2 and SimpleBooks-92 are created from Guten-
berg book corpus (www.gutenberg.org) while WikiText-103 are created from Wikipedia articles. All
6
Under review as a conference paper at ICLR 2021
Model dimension	Train ns /nm	Test ns /nm	Trans-QL	Trans-XL	Improvement	Relative Improvement
512	16/16	16/16	-33.32-	-3268-	-0.64	--1.96%-
1024	16/16	16/16	27.70	28.42	0.72	+2.53%
1536	16/16	16/16	27.00	28.22	1.22	+4.32%
Table 2: Improvement in test perplexity score (lower is better) of Transformer-QL over Transformer-
XL for three different model dimensions. The forth and fifth columns show the test perplexity
obtained by Transformer-QL and Transformer-XL respectively.
the three datasets preserve paragraph and section structures of their sources making those suitable
for long range language modeling task. The statistics of the three datasets are shown in Table 4 of
Appendix C.
Experimental Details For the experiments of Transformer-XL and Compressive Transformer, we
have used an 8-layer network. And for the experiments of Transformer-QL, we have used a network
with 3 layers in scale 1, 3 layers in scale 2 and 2 layers in the output block. Thus, the Transformer-
QL has a total of eight layers as in Transformer-XL and Compressive Transformer. We set the
compression rate of Compressive Transformer to 2. In Transformer-QL, we have used max-pooling
layer with pool size 2 and stride 2 as the compression layer. Thus, both the Transformer-QL and
Compressive Transformer have a compression rate of 2. For the experiments on the SimpleBooks-
92 and WikiText-103, we have set the model dimension to 1536 and used an initial learning rate
of 1 × 10-4. On the other hand, for the experiments on SimpleBooks-2, we have set the model
dimension to 256 and learning rate to 2.5 × 10-4. All the models have been trained using Adam
optimizer. We set the droppath probability of Transformer-QL to 0.3. The details of other hyper-
parameters can be found in Appendix E .
Results The results of the comparison are shown in Table 1. The results are grouped by the
window length nw of the test model as the lower bound of memory and computation requirement
directly depends on it. In all the datasets and settings, Transformer-XL performs worst among
all three. Worst performance of Transformer-XL is not surprising as it has smallest average con-
text length (shown in the fifth column) for a given nw . However, Compressive Transformer has
a slightly larger average context length than Transformer-QL. Yet, Transformer-QL has performed
similarly or significantly better than Compressive Transformer in all the setting which indicates that
Transformer-QL can exploits the contextual information more effectively than Compressive Trans-
former.
3.2	Effect of Model Dimension
In this section, we investigate the effect of model dimension on the performance of Transformer-
QL. Towards that goal, we have performed experiments on WikiText-103 dataset with varying
model dimension. For each experiment, we compared the test perplexity of Transformer-QL with
that of Transformer-XL. The results are shown in Table 2. The improvement in test perplexity
of Transformer-QL over Transformer-XL has been computed by subtracting the test perplexity of
Transformer-QL from that of Transformer-XL. The relative improvement is computed by
Improvement × 100
Relative improvement = ------7Z；--------7-----TTT
Test perplexity of Transformer-XL
As shown in the table, Transformer-QL performs relatively worse for small model dimension like
512 and relative improvement increases as the model dimension increases. We speculate that the
relatively worse performance of Transformer-QL for smaller model dimension is caused by the
difficulty in compressing hidden states during switching from one scale to the next. To alleviate
the problem, Donahue et al. (2019) have proposed to increase the model dimension as the model
transits from a lower scale to a higher one. On the other hand, Dai et al. (2020) have suggested a
novel query-only-pooling to solve the problem. We take it as a future work to try those approaches
in Transformer-QL.
7
Under review as a conference paper at ICLR 2021
3.3	Effect of Context Length
In this section, we study the relative improvement in perplexity scores obtained by Transformer-QL
over Transformer-XL for varying context length. The results are shown in Table 3. From the table,
it can be noticed that relative improvement obtained by Transformer-QL is more when the context
length of the Transformer-XL networks are smaller in the first place. For example, for test ns = 08
and nm = 08, the relative improvement is as high as 8.80%. On the other hand, for the test ns = 02
and nm = 30, the relative improvement is only 2.76%. This can be explained by the fact that for
the segment and memory length 02 and 30, the average context length of Transformer-XL is already
large enough (241) to provide good result. By extending the average context length from 241 to
332, Transformer-QL provides only a small improvement following the law of diminishing return
(Hestness et al., 2017).
Model Dimension	Test ns/nm	Avg. test nc		Perplexity	
		Trans-XL	Trans-QL	Trans-XL	Trans-QL Rel Imprv
	08/08	68	100	-33.08^^	30.17	+8.80%
1536	16/16	136	200	28.22	27.00	+4.32%
	02/30	241	332	27.19	26.44	+2.76%
Table 3: Relative improvements in perplexity scores (lower is better) obtained by Transformer-QL
over Transformer-XL on WikiText-103 dataset. The second column shows the test segment (ns) and
memory (nm) length. The third and forth column respectively show the average test context length
(nc) of Transformer-XL and Transformer-QL network.
4	Conclusion and Future Work
In the work, we have proposed a class of transformer networks namely Transformer-QL in which
the context length can grow quadratically in memory and computational usage. Our empirical eval-
uation shows that Transformer-QL can perform significantly better than other long range language
modeling networks like Transformer-XL and Multi-scale Transformer by exploiting longer context
length. Further more, it can perform significantly better than Compressive Transformer by exploiting
the contextual information more effectively.
In our empirical evaluation, we have evaluated a Transformer-QL network with only one compres-
sion layer. In future, we want to evaluate a network with more then one compression layers. Also,
we have empirically found that the performance of Transformer-QL network can be worse than
that of Transformer-XL when the model dimension is small. As our future work, we want explore
different methods for removing this limitation.
References
Joshua Ainslie, Santiago Ontanon, Chris Alberti, Philip Pham, Anirudh RavUla, and SUmit Sanghai.
ETC: encoding long and structured data in transformers. CoRR, abs/2004.08483, 2020.
Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.
CoRR, abs/2004.05150, 2020.
Mikhail S. Burtsev and Grigory V. Sapunov. Memory transformer. CoRR, abs/2006.11527, 2020.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. CoRR, abs/1904.10509, 2019.
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, XingyoU Song, Jared Davis, Tamas
Sarlos, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for pro-
teins via linearly scalable long-context transformers. CoRR, abs/2006.03555, 2020.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdi-
nov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings
ofthe 57th Conference of the Association for Computational Linguistics, ACL 2019, pp. 2978—
2988. Association for Computational Linguistics, 2019.
8
Under review as a conference paper at ICLR 2021
Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V. Le. Funnel-Transformer: Filtering out sequen-
tial redundancy for efficient language processing. CoRR, abs/2006.03236, 2020.
David Donahue, Vladislav Lialin, and Anna Rumshisky. Injecting hierarchy with U-Net transform-
ers. CoRR, abs/1910.10488, 2019.
Ankit Gupta and Jonathan Berant. GMAT: global memory augmentation for transformers. CoRR,
abs/2006.03274, 2020. URL https://arxiv.org/abs/2006.03274.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianine-
jad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. CoRR, abs/1712.00409, 2017.
Angelos Katharopoulos, APoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are
RNNs: Fast autoregressive transformers with linear attention. CoRR, abs/2006.16236, 2020.
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th
International Conference on Learning Representations, ICLR 2020,April 26-30, 2020. OpenRe-
view.net, 2020.
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng
Yan. Enhancing the locality and breaking the memory bottleneck of transformer on time series
forecasting. In Advances in Neural Information Processing Systems 32: NeurIPS 2019, pp. 5244—
5254, 2019.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In 5th International Conference on Learning Representations, ICLR 2017, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017.
Huyen Nguyen. SimpleBooks: Long-term dependency book dataset with simplified English vocab-
ulary for word-level language modeling. CoRR, abs/1911.12391, 2019.
Raghavendra Pappagari, Piotr Zelasko, Jesus Villalba, Yishay Carmiel, and Najim Dehak. Hierar-
chical transformers for long document classification. In IEEE Automatic Speech Recognition and
Understanding Workshop, ASRU 2019, Singapore, December 14-18, 2019, pp. 838-844. IEEE,
2019.
Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap.
Compressive transformers for long-range sequence modelling. In 8th International Conference
on Learning Representations, ICLR 2020. OpenReview.net, 2020.
Sandeep Subramanian, Ronan Collobert, Marc’Aurelio Ranzato, and Y-Lan Boureau. Multi-scale
transformer language models. CoRR, abs/2005.00581, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems 30, pp. 5998-6008, 2017.
Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and
Jingjing Liu. Cluster-former: Clustering-based sparse transformer for long-range dependency
encoding. CoRR, abs/2009.06097, 2020a.
Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention
with linear complexity. CoRR, abs/2006.04768, 2020b.
Zihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and Zheng Zhang. BP-Transformer: Modelling
long-range context via binary partitioning. CoRR, abs/1911.04070, 2019.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers
for longer sequences. CoRR, abs/2007.14062, 2020.
9
Under review as a conference paper at ICLR 2021
Xingxing Zhang, Furu Wei, and Ming Zhou. HIBERT: Document level pre-training of hierarchical
bidirectional transformers for document summarization. In Proceedings of the 57th Conference
of the Association for Computational Linguistics, ACL 2019,Volume 1: Long Papers, pp. 5059-
5069. Association for Computational Linguistics, 2019.
Yucheng Zhao, Chong Luo, Zheng-Jun Zha, and Wenjun Zeng. Multi-scale group transformer for
long sequence modeling in speech separation. In Proceedings of the Twenty-Ninth International
Joint Conference on Artificial Intelligence, IJCAI 2020, pp. 3251-3257. ijcai.org, 2020.
A Transformer-XL Algorithm
The algorithm for Transformer-XL is shown in Figure 3.
1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
At the beginning
begin
m0,…，mL 1 . 0
_ t — 1
while there is more data to process do
S0 — XtWemb
m0 — Shift(s0, m0-ι)
for l - 1 to L do
St — Compute (st-1, m；-1, l)
_ mt - Shift(st, mt-ι)
t — t + 1
// initialize memory to zero
// initialize time step to 1
// embed input segment
// shift hidden states into memory
// run one Transformer-XL layer
// shift hidden states into memory
// increase the time step
Figure 3:	Forward pass of Transformer-XL. The functions Compute and Shift are given in Figure 2b
and 2d respectively.
cur_slen — n
cur_mlen — nm
min_clen — 0
cur」ayer_Clen — nm
for i — 1 to no _ofscales do
Li — no of layers at scale i
for l - 1 to Li do
// set current segment length to ns
// set current memory length to nm
// initialize minimum context length to 0
// set current layer context length to nm
// set Li to the number of layers at scale i
L min_clen — min_clen + cur」ayer_clen	// increase min_clen by cur_layer_clen
if i < no_ofscales then
// there is a compression layer
// update min_clen, cur_mlen, cur_slen and cur_layer_clen
// appropriately
min_clen — min_clen + min(cur_mlen, n§)/CuJslen
cur_mlen — (nm + n§) - min((nm, + ns)/2,n$)
cur-slen — cur-slen/c
cur」ayer_clen — cur_mlen × n /cur.slen
cur」ayer_clen — nm	// reset cur_layer_clen for output layers
Lo — no of output layers	// set Lo to the number of output layers
for l - 1 to Lo do
L min_clen — min_clen + cur」ayer_clen	// increase min_clen by cur_layer_clen
return min_clen
Figure 4:	Computation of minimum context length of a Transformer-QL model. The ns, nm respec-
tively represent the segment and memory length of the scale 1 of the network.
10
Under review as a conference paper at ICLR 2021
B Context Length of Transformer-QL
A tight estimate of the minimum context length of a Transformer-QL network can be computed using
algorithm of Figure 4. For simplicity, we have assumed that all the division operations result into
integer output. We have also assumed that there is at least one layer in every scale. The maximum
context length can be obtained by adding ns to the minimum context length.
Autoregressive Prediction
TransformerXLLayer
Q6 St-47:t-7		c6 St-47:t-6		S6-47:t-5	|S6-47:t-4		S6-43:t-3		6 St-43:t-2		6 St-43:t-1		6 St-43:t
TransformerXLLayer
Figure 5: Dependency of hidden states to the past tokens in a Transformer-QL network.
Additionally, in Figure 5, we have shown the detailed computation of minimum/maximum context
length with an example. In the figure, the notation Stl1:t2 used to denote a hidden states of l-th layer
and the state depends on the t1-th to t2-th tokens of the input sequence. In the example of the figure,
each output state depends on at least 44 previous tokens. In other words, minimum context length
of the network is 44. On the other hand, in a Transformer-XL network of same segment length,
memory length and number of layers, the minimum context length would have been 4 × 4 = 16.
C Statistics of Datasets
The statistics of the datasets are shown in Table 4.
11
Under review as a conference paper at ICLR 2021
Dataset	Number of Tokens	Vocabulary Size	Average Frequency
SimpleBooks-2	2.2M~~	11, 492	195.43
SimpleBooks-92	91.5M	98, 304	931.40
WikiText-103	103M	267, 735	385.56
Table 4: Statistics of the datasets used in the experiments.
D Comparison with Multi-scale Transformer
In this section, we empirically compare Transformer-QL with Multi-scale Transformer (Subrama-
nian et al., 2020). Our implementation of Multi-scale Transformer is same as Transformer-QL
without any recurrent memory. The resultant Multi-scale Transformer is similar to the button-up
model of Subramanian et al. (2020). We have used hyperparameter settings same as Transformer-
QL to train the Multi-scale Transformer. The result is shown in Table 5. From the table, we can see
that Multi-scale Transformer has been widely bitten by Transformer-QL even when the Multi-scale
Transformer has been trained and tested with a larger window length.
Dataset	Model	Train		ns	Test nm	nw	Average test nc	Test Perplexity
		ns	nm					
SimpleBooks-2	MS-Transformer Transformer-QL	16 08	- 08	~1^~ 08	- 08	16 16	8 100	25.13 18.92
	MS-Transformer Transformer-QL	16 08	- 08	~4Γ~ 16	- 16	64 32	32 200	-21.89- 18.57
SimpleBooks-92	MS-Transformer Transformer-QL	64 08	- 08	~1^~ 08	- 08	16 16	8 100	28.37 12.42
	MS-Transformer Transformer-QL	64 08	- 08	~4Γ~ 16	- 16	64 32	32 200	-14.30- 11.92
WikiText-103	MS-Transformer Transformer-QL	128 16	- 16	-32^^ 08	- 08	32 16	16 100	44.05 30.17
	MS-Transformer Transformer-QL	128 16	- 16	T28- 16	- 16	128 32	64 200	-29.15- 27.00
Table 5: Comparison of Transformer-QL with Multi-scale Transformer (MS-Transformer). The
third and forth column respectively show the segment length (ns) and the memory length (nm) used
during training. The fifth, sixth and seventh columns respectively show the segment, memory and
the window (nw = ns + nm) length used to compute the text perplexities.The eighth column shows
the average context length (nc) of the test models.
E Hyperparameter Setting
We used the following values for hyperparameter for the experiments on SimpleBooks-2 datasets:
12
Under review as a conference paper at ICLR 2021
Hyperparameter	Transformer-XL	Compressive Transformer	Transformer-QL	Multi-scale Transformer
d_model	256	256	256	256
d_embed	256	256	256	256
div_val	1	1	1	1
untie」	False	False	False	False
proj_same_dim	True	True	True	True
n_head	4	4	4	4
d_head	64	64	64	64
d_inner	1024	1024	1024	1024
train_batch_size	128×8	128×8	128×8	128×8
	ns	ns	ns	ns
train ns	08	04	08	08
train nm	08	06	08	-
train ncm	-	06	-	-
pre_lnorm	True	True	True	True
WarmUP_steps	0	0	0	0
train_steps	60, 000	60, 000	60, 000	60, 000
learning_rate	0.00025	0.00025	0.00025	0.00025
min」r_ratio	0.004	0.004	0.004	0.004
clip	0.25	0.25	0.25	0.25
dropout	0.1	0.1	0.1	0.1
dropatt	0.1	0.1	0.1	0.1
droppath	-	-	0.3	0.3
init_std	0.02	0.02	0.02	0.02
proj Jnit-Std	0.01	0.01	0.01	0.01
recons_loss_weight	-	0.01	-	-
For the SimpleBooks-92 datasets and model dimension 1536, the following values of hyperparame-
ters are used:
Hyperparameter	Transformer-XL	Compressive Transformer	Transformer-QL	Multi-scale Transformer
d_model	1536	1536	1536	1536
d_embed	1536	1536	1536	1536
div_val	4	4	4	4
Untie-T	False	False	False	False
proj_same_dim	True	True	True	True
n_head	16	16	16	16
d_head	96	96	96	96
dinner	6144	6144	6144	6144
train_batch_size	512×8	512×8	512×8	512×8
	ns	ns	ns	ns
train ns	08	04	08	08
train nm	08	06	08	08
train ncm	-	06	-	-
pre_lnorm	True	True	True	True
WarmUP_steps	0	0	0	0
train_steps	250,000	250, 000	250,000	250, 000
Iearning_rate	0.0001	0.0001	0.0001	0.0001
min」r_ratio	0.004	0.004	0.004	0.004
clip	0.1	0.1	0.1	0.1
dropout	0.15	0.15	0.15	0.15
dropatt	0.15	0.15	0.15	0.15
droppath	-	0.3	0.3	0.3
init_std	0.02	0.02	0.02	0.02
Proj _init_std	0.01	0.01	0.01	0.01
recons_loss_weight	-	0.01	-	-
13
Under review as a conference paper at ICLR 2021
For the WikiText-103 datasets and model dimension 1536, the following values of hyperparameters
are used:
Hyperparameter	Transformer-XL	Compressive Transformer	Transformer-QL	Multi-scale Transformer
d_model	1536	1536	1536	1536
d_embed	1536	1536	1536	1536
div_val	4	4	4	4
untie」	False	False	False	False
proj_same_dim	True	True	True	True
n_head	16	16	16	16
d_head	96	96	96	96
dinner	6144	6144	6144	6144
train_batch_size	512×16	512×16	512×16	512×16
	ns	ns	ns	ns
train ns	16	08	16	16
train nm	16	12	16	-
train ncm	-	12	-	-
pre_lnorm	True	True	True	True
WarmUP_steps	0	0	0	0
train_steps	350,000	350, 000	350,000	350, 000
learning_rate	0.0001	0.0001	0.0001	0.0001
min」r_ratio	0.004	0.004	0.004	0.004
clip	0.1	0.1	0.1	0.1
dropout	0.15	0.15	0.15	0.15
dropatt	0.15	0.15	0.15	0.15
droppath	-	-	0.3	0.3
init_std	0.02	0.02	0.02	0.02
proj Jnit-Std	0.01	0.01	0.01	0.01
compression rate	-	2	2	2
recons_loss_weight	-	0.01	-	-
For training models of model dimensions 512 and 1024, we have used initial learning rate of 0.0005
and 0.00025 respectively keeping the rest of the hyper-parameters same.
14