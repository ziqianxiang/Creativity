Under review as a conference paper at ICLR 2021
Diverse Exploration via InfoMax Options
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study the problem of autonomously discovering temporally ab-
stracted actions, or options, for exploration in reinforcement learning. For learning
diverse options suitable for exploration, we introduce the infomax termination ob-
jective defined as the mutual information between options and their corresponding
state transitions. We derive a scalable optimization scheme for maximizing this
objective via the termination condition of options, yielding the InfoMax Option
Critic (IMOC) algorithm. Through illustrative experiments, we empirically show
that IMOC learns diverse options and utilizes them for exploration. Moreover, we
show that IMOC scales well to continuous control tasks.
1	Introduction
Abstracting a course of action as a higher-level action, or an option (Sutton et al., 1999), is akey ability
for reinforcement learning (RL) agents in several aspects, including exploration. In RL problems, an
agent learns to approximate an optimal policy only from experience, given no prior knowledge. This
leads to the necessity of exploration: an agent needs to explore the poorly known states for collecting
environmental information, sometimes sacrificing immediate rewards. For statistical efficiency, it
is important to explore the state space in a deep and directed manner, rather than taking uniformly
random actions (Osband et al., 2019). Options can represent such directed behaviors by capturing
long state jumps from their starting regions to terminating regions. It has been shown that well-defined
options can facilitate exploration by exploiting an environmental structure (Barto et al., 2013) or,
more generally, by reducing decision steps (Fruit and Lazaric, 2017).
A key requirement for such explorative options is diversity. If all options have the same terminating
region, they will never encourage exploration. Instead, options should lead to a variety of regions for
encouraging exploration. However, automatically discovering diverse options in a scalable, online
manner is challenging due to two difficulties: generalization and data limitation. Generalization with
function approximation (Sutton, 1995) is important for scaling up RL methods to large or continuous
domains. However, many existing option discovery methods for exploration are graph-based (e.g.,
Machado et al. (2017)) and incompatible with function approximation, except for that by Jinnai et al.
(2020). Discovering options online in parallel with polices requires us to work with limited data
sampled from the environment and train the model for evaluating the diversity in a data-efficient
manner.
To address these difficulties, we introduce the infomax termination objective defined as the mutual
information (MI) between options and their corresponding state transitions. This formulation reflects
a simple inductive bias: for encouraging exploration, options should terminate in a variety of regions
per starting regions. Thanks to the information-theoretical formulation, this objective is compatible
with function approximation and scales up to continuous domains. A key technical contribution of
this paper is the optimization scheme for maximizing this objective. Specifically, we employ a simple
classification model over options as a critic for termination conditions, which makes our method
data-efficient and tractable in many domains.
The paper is organized as follows. After introducing background and notations, we present the
infomax termination objective and derive a practical optimization scheme using the termination
gradient theorem (Harutyunyan et al., 2019). We then implement the infomax objective on the
option-critic architecture (OC) (Bacon et al., 2017) with algorithmic modifications, yielding the
InfoMax Option Critic (IMOC) algorithm. Empirically, we show that (i) IMOC improves exploration
in structured environments, (ii) IMOC improves exporation in lifelong learning, (iii) IMOC is scalable
1
Under review as a conference paper at ICLR 2021
to MuJoCo continuous control tasks, and (iv) the options learned by IMOC are diverse and meaningful.
We then relate our method to other option-learning methods and the empowerment concept (Klyubin
et al., 2005), and finally give concluding remarks.
2	Background and Notation
We assume the standard RL setting in the Markov decision process (MDP), following Sutton and
Barto (2018). An MDP M consists of a tuple (X, A,p, r, γ), where X is the set of states, A is the
set of actions, p : X × A × X → [0, 1] is the state transition function, r : X × A → [rmin, rmax] is
the reward function, and 0 ≤ γ ≤ 1 is the discount factor. A policy is a probability distribution over
actions conditioned on a state x, π : X × A → [0, 1]. For simplicity, we consider the episodic setting
where each episode ends when a terminal state xT is reached. In this setting, the goal of an RL agent
is to approximate a policy that maximizes the expected discounted cumulative reward per episode:
T-1
JRL(π) = Eπ,x0 X γtRt ,	(1)
t=0
where Rt = r(xt, at) is the reward received at time t, and x0 is the initial state of the episode.
Relatedly, we define the action-value function Qπ (xt, at) d=ef Ext ,at,π PtT0=-t1 γt0-tRt0 and the
state-value function Vπ(xt) = Eχ5∏ Pa ∏(a∣xt)Qπ(xt, a).
Assuming that π is differentiable by the policy parameters θπ, a simple way to maximize the objective
(1) is the policy gradient method (Williams, 1992) that estimates the gradient by:
Vθ∏ JRL(∏)= E∏,χt Vθ∏ logπ(at∣xt)A(xt,at) ,	(2)
where A(xt, at) is the estimation of the advantage function An(xt, at) = Qn (xt, a/ — Vπ (xt). A
common choice of A(xt, at) is N-step TD error PlN=O γiRt+i + YNV(xt+N) 一 V(xt), where N is
a fixed rollout length (Mnih et al., 2016).
2.1	Options Framework
Options (Sutton et al., 1999) provide a framework for representating temporally abstracted actions
in RL. An option o ∈ O consists of a tuple (Io, βo, πo), where Io ⊆ X is the initiation set,
βo : X → [0, 1] is a termination function with βo(x) denoting the probability that option o terminates
in state x, and πo is intra-option policy. Following related studies (Bacon et al., 2017; Harutyunyan
et al., 2019), we assume that Io = X and learn only βo and πo. Letting xs denote an option-starting
state and xf denote an option-terminating state, we can write the option transition function as:
P o(χf |xs) = βo(χf )Iχf=χs + (1- βo(χs)) X pπ° (x|Xs)Po(Xf |x),	(3)
x
where I is the indicator function and pπo is the policy-induced transition function pπo (X0|X) d=ef
Pa∈A ∏o(a∣χ)p(χ0∣χ, a). We assume that all options eventually terminate so that Po is a valid
probability distribution over Xf, following Harutyunyan et al. (2019).
To present option-learning methods, we define two option-value functions: QO and UO, where
QO is the option-value function denoting the value of selecting an option o at a state Xt defined
by Qo(xt,o) = E∏,β,μ [PT-I Yt'-tRt]. Analogously to Qn and Vπ, We let VO denote the
def
marginalized option-value function VO(x) = Eo μ(o∣x)QO(x, o), where μ(o∣xs) : X ×O → [0,1]
is the policy over options. Function UO(X, o) d=ef (1 一 βo(X))QO(X, o) + βo(X)VO(X) is called the
option-value function upon arrival (Sutton et al., 1999) and denotes the value of reaching a state Xt
with o and not having selected the new option.
2.2	Option Critic Architecture
OC (Bacon et al., 2017) provides an end-to-end algorithm for learning πo and βo in parallel. To
optimize πo, OC uses the intra-option policy gradient method that is the option-conditional version
2
Under review as a conference paper at ICLR 2021
of the gradient estimator (2), Vθπo JRL(πo) = E[Vθπo logπo(at∣xt)Ao(xt,at)], where Ao is an
estimation of the option-conditional advantage Aπo .
For optimizing βo, OC directly maximizes QO using the estimated gradient:
Vθβo QO (x, o) = γE h-Vθβo βo(x)QO(x, o) - VO(x)i .	(4)
Intuitively, this decreases the termination probability βo(x) when holding an o is advantageous, i.e.,
QO (x) - VO (x) is positive, and vice versa. Our method basically follows OC but has a different
objective for learning βo .
2.3	Termination Critic
Recently proposed termination critic (TC) (Harutyunyan et al., 2019) optimizes βo by maximizing
the information-theoretic objective called predictability:
JTC(Po) = -H(Xf |o),	(5)
where H denotes entropy and Xf is the random variable denoting the option-terminating states.
Maximizing -H (Xf |o) makes the terminating region of an option smaller and more predictable. In
other words, we can compress terminating regions by optimizing the objective (5). To derivate this
objective by the beta parameters θβo , Harutyunyan et al. (2019) introduced the termination gradient
theorem:
Theorem 1. Let βo be parameterized with a sigmoid function and 射。denote the logit of βo. We
have
VθβPo(Xf |xs) = XPo(x∣Xs)Vθβ'βο(x)(Iχf =x - Po(Xf |x)),	(6)
x
Leveraging the theorem 1, TC performs gradient ascent using the estimated gradient:
vθβο JTC(Po) = -Eχs,χ,χf [vθβ'βο(x)βo(x) ((logPo(X)- logPμo(xf)) + (1 — P0(Xf)POPOXX)))
where Po(X) is the marginalized distribution of option-terminating states.
Contrary to the termination objective of OC (4), this objective does not depend on state values, making
learned options robust against the reward structure of the environment. Our method is inspired by TC
and optimizes a similar information-theoretic objective, not for predictability but for diversity. Also,
our infomax objective requires an estimation of p(o∣Xs,Xf) instead of the option transition model
Po(Xf|Xs), which makes our method tractable in more environments.
3	InfoMax Option Critic
We now present the key idea behind the InfoMax Option Critic (IMOC) algorithm. We first formulate
the infomax termination objective based on the MI maximization, then derive a practical gradient
estimation for maximizing this objective on βo, utilizing the termination gradient theorem 1.
To evaluate the diversity of options, we use the MI between options and option-terminating states
conditioned by option-starting states:
JIMOC=I(Xf;O|Xs) =H(Xf|Xs) - H(Xf |Xs, O),	(7)
where I denotes conditional MI I(A; B|Z) = H(A|Z) - H(A|B, Z), Xs is the random variable
denoting an option-starting state, and O is the random variable denoting an option. We call this
objective the infomax termination objective. Let us interpret Xf |Xs as the random variable denoting
a state transition induced by an option. Then maximizing the MI (7) (i) diversifies a state transition
Xf |Xs and (ii) makes an option-conditional state transition Xf|Xs, o more deterministic. Note that
the marginalized MI I(Xf; O) also makes sense in that it prevents the terminating region of each
option from being too broad, as predictability (5) does. However, in this study, we focus on the
conditional objective since it is easier to optimize.
3
Under review as a conference paper at ICLR 2021
Figure 1: Two insances of InfoMax options in the four state deterministic chain. Left: Options are
diverse but all state transitions per option are one-step. Right: Options enable relatively long state
transitions but option-policies are the same at some states.
To illustrate the limitation of infomax options, we conducted an analysis in a toy four-state determin-
istic chain environment, which has four states and two deterministic actions (go left and go right) per
each state. Since deriving the exact solution is computationally difficult, we searched options that
maximize H(Xf|Xs) from deterministic options that has deterministic option-policies and termina-
tion functions (thus has the minimum H(Xf |Xs, O)). Among multiple solutions, Figure 1 shows
two interesting instances of deterministic infomax options when |O| = 2. The left options enable
diverse behaviors per state, although they fail to capture long-term behaviors generally favorable in
the literature (e.g., Mann et al. (2015)). On the other hand, the right options enable relatively long,
two step state transitions, but they are the same and the rightmost state and the next one. Furthermore,
an agent can be caught in a small loop that consists of the leftmost state and the next one. This
example shows that (i) we can obtain short and diverse options with only a few options, (ii) to obtain
long and diverse options, we need sufficiently many options, and (iii) an agent can be caught in a
small loop with only a few options, failing to visit diverse states. As we show in Appendix A, this
’small loop’ problem cannot happen with four options. Thus, the number of options is important
when we are to maximize the MI (7) and a limitation of this method. However, in experiments, we
show that we can practically learn diverse options with relatively small number of options.
For maximizing the MI by gradient ascent, we now derive the gradient of the infomax termination
objective (7). First, we estimate the gradient of the objective using the option transition model Po
and marginalized option-transition model P(Xf |x§) = Po μ(o∣Xs)Po(xf |x§).
Proposition 1. Let βo be parameterized with a sigmoid function. Given a trajectory τ =
Xs,...,x,...,Xf sampled by πo and βo, we can obtain unbiased estimations of Vθe H(Xf |X§) and
VθβH(Xf Xs,O) by
VθβH(Xf |Xs) = Eχs,χ,χf,o [-Vθβ'βo(χ)βo(χ)(logP(χ∣χs)- logP(Xf E))]	(8)
VθβH(Xf |Xs,O) = Eχs,x,xf ,o [-Vθβ'βo(x)βo(x) (logPo(x∣Xs) - logPo(Xf ∣Xs))]	(9)
where '§。(x) denotes the logit of βo(x).
Note that the additional term βo is necessary because X is not actually a terminating state. The proof
follows section 4 in Harutyunyan et al. (2019) and is given in Appendix B.1.
The estimated gradient of the infomax termination objective (7) can now be written as:
VθβI(Xf; O|Xs) = VθβH(Xf |Xs) - VθβH(Xf ∣Xs,O)
=Eχs,χ,χf ,o [-Vθβ'βο(x)βo(x) (logP(XIXs)- logP(Xf ∣Xs) - (logPo(XIXs)- logPo(Xf ∣Xs)))],
(10)
which means that we can optimize this objective by estimating Po and P . However, estimating the
probability over the state space can be difficult, especially when the state space is large, as common
in the deep RL setting. Hence, we reformulate the gradient using Bayes’ rule in a similar way as
Gregor et al. (2017). The resulting term consists of the reverse option transition p(oIXs, Xf) that
denotes the probability of having an o given a state transition Xs, Xf.
4
Under review as a conference paper at ICLR 2021
Proposition 2. We now have
Vθβ I (Xf; O|Xs )=Vθβ H(Xf ∣Xs)-Vθβ H (Xf ∣Xs,O)
= EXs,X,Xf,O [^63'βo(x)β°(x)(logp(o∣Xs,x) - logp(o∣Xs,Xf))]	(11)
The proof is given in Appendix B.2. In the following sections, we estimate the gradient (11) by
learning a classification model over options p(o∣Xs,Xf) from sampled option transitions.
4 Algorithm
In this section, we introduce modifications for adjusting the OC (Bacon et al., 2017) to our infomax
termination objective. Specifically, we implement IMOC on top of Advantage-Option Critic (AOC),
a synchronous variant of A2OC (Harb et al., 2018), yielding the Advantage-Actor InfoMax Option
Critic (A2IMOC) algorithm. To stably estimates p(o∣Xs,Xf) for updating θg, We sample recent
option state transitions o, xs , xf from We follow AOC for optimizing option-policies except the
folloWing modifications and give a full description of A2IMOC in Appendix C.1. In continuous
control experiments, We also used Proximal Policy InfoMax Option Critic (PPIMOC) that is an
implementation of IMOC based on PPO (Schulman et al., 2017). We give details of PPIMOC in
Appendix C.2.
Upgoing Option-Advantage Estimation Previous studies (e.g., Harb et al. (2018)) estimated the
advantage Aot (xt) ignoring the future reWards after the current option ot terminates. Since longer
rollout length often helps speed up learning (Sutton and Barto, 2018), it is preferable to extend
this estimation to use all available future reWards. HoWever, future reWards after option termination
heavily depends on the selected option, often leading to underestimation of Ao . Thus, to effectively
use future reWards, We introduce an upgoing option-advantage estimation (UOAE). Let t + k denote
the time step Where the current option ot terminates in a sampled trajectory. Then, UOAE estimates
the advantage by:
o
AUOAE = -QO (xt, ot) +
Pk=O γiRt+i + max (χγjRt+j, γkVO(Xt+k))	(k < N)
'----------------V------------------}
upgoing estimation
.PN=0 YiRt+i + Y N UO (xt+N ,ot)
(otherwise)
(12)
Similar to upgoing policy update (Vinyals et al., 2019), the idea is to be optimistic about the future
reWards after option termination by taking the maximum With VO .
Policy Regularization Based on Mutual Information To perform MI maximization not only on
termination functions but also on option-policies, We introduce a policy regularization based on the
maximization of the conditional MI, I(A; O|Xs), Where A is the random variable denoting an action.
This MI can be interpreted as a local approximation of the infomax objective (7), assuming that
each action leads to different terminating regions. Although optimizing the infomax termination
objective diversifies option-policies implicitly, We found that this regularization helps learn diverse
option-policies reliably. Letting ∏μ denote the marginalized policy ∏μ(a∣x) = Po μ(o∣x)πo(a∣x),
We Write I(A; O|Xs ) as:
I (A； O|Xs) = H(A∣Xs) - H(A∣O, Xs) = Ex, [H(∏μ(xs))] - Eχs,o [H (∏o(xs))].
We use this regularization With the entropy bonus (maximization of H (πo)) common in policy
gradient methods (Williams and Peng, 1991; Mnih et al., 2016) and Write the overall regularization
term as
CHμ H(∏μ(x)) + CH H(πo(x)),	(13)
where ch* and CH are weights of each regularization term. Note that we add this regularization term
on not only option-starting states but all sampled states. This introduces some bias, Which We did
5
Under review as a conference paper at ICLR 2021
sp∙JeΛΛ3l±3>4e-nlunu
Method
——A2IMOC
AOC
OurAOC
-A2C
O IOOOOO 200000 300000 400000 500000
Z X zɔ ∙ 1	∙	,	ɪotal Environment Steps
(a) Gridworld environment.
Blue grid is the start and green grids are goals.	(b) Performance progression.
Figure 2: Single Task learning in Gridworld Four Rooms.
not find to be harmful when c/ is reasonably small. To approximate H(∏μ), we employ μ that is
the empirical estimation of μ. Using μ, H(∏μ) is computed by ∏μ(a∣x) ≈ Po ^(o∣x)πo(a∣x) for
descrete action spaces and approximated by Monte Carlo method for continuous action spaces. We
show the details in Appendix C.1.
5	Experiments
We conducted a series of experiments to show two use cases of IMOC: exploration in structured
environments and exploration for lifelong learning (Brunskill and Li, 2014). In this section, we used
four options for all option-learning methods and compared the number of options in Appendix D.7.
5.1	Single Task Learning in Structured Environments
We consider two ’Four Rooms’ domains, where diverse options are beneficial for utilizing environ-
mental structures.
Gridworld Four Rooms with Suboptimal Goals First, we tested IMOC in a variant of the classi-
cal Four Rooms Gridworld (Sutton et al., 1999) with suboptimal goals. An agent is initially placed
at the upper left room and receives a positive reward only at goal states: two closer goals with +1
reward and the farthest goal with +2 reward, as shown in Figure 2a. The episode ends when an
agent reaches one of the goals. The optimal policy is aiming the farthest goal in the lower right room
without converging to suboptimal goals. Thus, an agent is required to learn multimodal behaviors
leading to multiple goals, which options can help. In this environment, we compared A2IMOC with
A2C (Mnih et al., 2016; Wu et al., 2017), AOC, and our tuned version of AOC (our AOC) with all
enhancements presented in section 4 to highlight the effectiveness of the termination objective among
all of our improvements.1 We show the progress of average cumulative rewards over ten trials in
Figure 2b. A2IMOC performed the best and found the optimal goal in most trials. AOC and our
AOC also occasionally found the optimal goal, while A2C overfitted to either of the suboptimal goals
through all trials.
Figure 3 illustrates learned option-polices and termination functions of each compared method.2
Terminating regions learned with A2IMOC is diverse. For example, option 0 mainly terminates in the
right rooms while option 3 terminates in the left rooms. We see that termination regions learned by
A2IMOC are diverse and clearly separated per each option. Although all option policies converged
to the same near optimal one, we show that A2IMOC diversifies option-policies at the beginning of
learning in Appendix D.4. On the other hand, terminating regions learned with AOC overlap each
other, and notably, option 3 has no terminating region. We assume this is because the loss function (4)
decreases the terminating probability when the advantage is positive. We can see the same tendency
in our AOC, although we cannot see the vanishment of the terminating regions.
1Note that we did not include ACTC (Harutyunyan et al., 2019) for comparison since we failed to reliably
reproduce the reported results with our implementation of ACTC.
2Note that we choose the best model from multiple trials for visualization throughout the paper.
6
Under review as a conference paper at ICLR 2021
A2IMOC
Option 0
Option 1
Option 2
1.0
0.8
AOC
Option 0
Option 2
Option 3
-0.6
β
0.4
0.2
l-l0.0
1.0
0.8
0.6
β
0.4
- 0.0
Option 2
Option 3
Figure 3: Learned option-policies (πo) and termination probabilities (βo) for each option in Gridworld
Four Rooms after 6 × 105 steps. Arrows show the probablities of each action and heatmaps show
probabilities of each βo . First row: A2IMOC. Terminating regions are clearly different each other
and there are a few overlapped regions. Second row: AOC. Almost all states has high termination
probablity with option 0, 1, 3 and option 2 has no terminating region. Third row: Our AOC.
Termination regions are not clearly separated per each option.
(b) Performance progression.
Figure 4: Single Task learning in MuJoCo Point Four Rooms.
(a) MuJoCo Point Four Rooms. The agent is an orange
ball and there are three goals: green goals are suboptimal
(+0.5) and the red one in optimal (+1.0).
MuJoCo Point Four Rooms To show the scalability of IMOC in continuous domains, we con-
ducted experiments in a similar four rooms environment, based on the MuJoCo (Todorov et al., 2012)
physics simulator and “PointMaze” environment in rllab (Duan et al., 2016). This environment
follows the Gridworld Four Rooms and has three goals as shown in Figure 4a: two green goals with
+0.5 reward, and a red one with +1.0. An agent controls the rotation and velocity of the orange
ball and receives a positive reward only at the goals. In this environment, we compared PPIMOC
with PPOC (Klissarov et al., 2017), PPO, and our tuned version of PPOC (our PPOC) that is the
same as PPIMOC except for the termination objective. An important difference is that PPOC uses a
parameterized μ trained by policy gradient, while PPIMOC and our PPOC use e-greedy for option
selection. Figure 4b show the progress of average cumulative rewards over five trials. PPIMOC found
the optimal goal four times in five trials and performed slightly bettern than PPO. In a qualitative
7
Under review as a conference paper at ICLR 2021
SPJeAΛ3l±ω>-⅛5ET-υ
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Total Environment Steps	le6
(a) Performance progression in Mountain Car.
Figure 5: Single Task learning in MuJoCo Point Four Rooms.
0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00
Total Environment Steps	le6
(b) Performance progression in Cartpole swing up.
1.0
S 0.8
W
fŋ
i 0.6
CC
a
品0.4
ro
io.2
O
0.0
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
Total Environment Steps	le6
(b) Performance progression.
(a) MuJoCo Point Billiard. Four goals periodically move clockwise.
Figure 6: Lifelong learning in MuJoCo Point Billiard.
analysis, we observed the same tendency in learned options as the Gridworld experiment, where the
details are given in Appendix D.5.
5.2	Single Task Learning in Classical Continuous Control
Additionally, we test PPIMOC on two classical, hard-exploration control problems: Mountain
Car (Barto et al., 1983) and Cartpole swingup (Moore, 1990). In Mountain Car, PPIMOC and our
PPOC successfully learns to reache the goal, while PPO and PPOC converged to run around the start
posisition. In Cartpole swing up, PPIMOC and our PPOC performed better than PPO and PPOC, but
still failed to learn stable behaviors.
5.3	Exploration for Lifelong Learning
As another interesting application of IMOC, we consider the lifelong learning setting. Specifically, we
tested IMOC in ’Point Billiard’ environment. In this environment, an agent receives a positive reward
only when the blue objective ball reaches the goal, pushed by the agent (orange ball). Figure 6a
shows all four configurations of Point Billiard that we used. There are four goals: green goals with
+0.5 reward and a red one with +1.0 reward. The positions of four goals move clockwise after 1M
environmental steps and agents need to adapt to the new positions of goals. We compared PPIMOC
with PPO, PPOC, and our PPOC in this environment. Figure 6b shows the progress of average
cumulative rewards over five trials. Both PPIMOC and our PPOC performed the best and adapted to
all reward transitions. On the other hand, PPO and PPOC struggle to adapt to the second transition,
where the optimal goal moves behind the agent. The ablation study given in Appendix D.6 shows
that UOAE (12) works effectively in this task. However, without UOAE, PPIMOC still outperformed
PPO. Thus, we argue that having diverse terminating regions itself is beneficial for adapting to new
reward functions in environments with subgoals.
8
Under review as a conference paper at ICLR 2021
6	Related Work
Options for Exploration Options (Sutton et al., 1999) in RL are widely studied for many applica-
tions, including speeding up planning (Mann and Mannor, 2014) and transferring skills (Konidaris and
Barto, 2007; Castro and Precup, 2010). However, as discussed by Barto et al. (2013), their benefits
for exploration are less well recognized. Many existing methods focused on discovering subgoals that
effectively decompose the problem then use such subgoals for encouraging exploration. Subgoals
are discovered based on various properties, including graphical features of state transitions (Simsek
and Barto, 2008; Machado et al., 2017; Jinnai et al., 2019) and causality (Jonsson and Barto, 2006;
Vigorito and Barto, 2010). In contrast, our method directly optimizes termination functions instead of
discovering subgoals, capturing environmental structures implicitly. From a theoretical perspective,
Fruit and Lazaric (2017) analyzed that good options can improve the exploration combined with
state-action visitation bonuses. Using infomax options with visitation bonuses would be an interesting
future direction.
End-to-end learning of Options While many studies attempted to learn options and option-policies
separately, Bacon et al. (2017) proposed OC to train option-policies and termination functions in
parallel. OC has been extended with various types of inductive biases, including deliberation
cost (Harb et al., 2018), interest (Khetarpal et al., 2020), and safety (Jain et al., 2018). Our study is
directly inspired by an information-theoretic approach presented by Harutyunyan et al. (2019), as we
noted in section 2.
Mutual Information and Skill Learning MI often appears in the literature of intrinsically moti-
vated (Singh et al., 2004) reinforcement learning, as a driver of goal-directed behaviors. A well-known
example is the empowerment (Klyubin et al., 2005; Salge et al., 2013), which is obtained by maxi-
mizing the MI between sequential k actions and the resulting state I(at, ..., at+k; xt+k|xt). Some
works (Mohamed and Rezende, 2015; Zhao et al., 2020) implemented lower bound maximization
of empowerment as intrinsic rewards for RL agents, encouraging goal-directed behaviors in the
absence of extrinsic rewards We can interpret our objective I(Xf; O|Xs) as empowerment between
limited action sequences and states corresponding to options. Gregor et al. (2017) employed this
interpretation and introduced a method for maximizing the variational lower bound of this MI via
option-policies, using the same model as our p^, while We aim to maximize the MI via termination
functions. MI is also used for intrinsically motivated discovery of skills, assuming that diversity is
important to acquire useful skills. Eysenbach et al. (2019) proposed to maximize MI between skills
and states I(O; X), extended to the conditional one I(O; X0|X) by Sharma et al. (2020). Although
our study shares the same motivation for using MI as these methods, i.e. diversifying sub-policies, the
process of MI maximization is significantly different: our method optimizes termination functions,
while their methods optimize conditional policies by using MI as intrinsic rewards.
7	Conclusion
We presented a novel end-to-end option learning algorithm InfoMax Option Critic (IMOC) that
uses the infomax termination objective to diversify options. Empirically, we showed that IMOC
improves exploration in structured environments and for lifelong learning, even in continuous control
tasks. We also quantitatively showed the diversity of learned options. An interesting future direction
would be combining our method for learning termination conditions with other methods for learning
option-policies, e.g., by using MI as intrinsic rewards. A limitation of the infomax objective presented
in this study is that it requires on-policy data for training. Hence, another interesting line of future
work is extending IMOC to use for off-policy option discovery.
References
P. Bacon, J. Harb, and D. Precup. The option-critic architecture. In Proceedings of the Thirty-
First AAAI Conference on Artificial Intelligence, February 4-9, 2017, San Francisco, California,
USA, pages 1726-1734, 2017. URL http://aaai.org/ocs/index.php/AAAI/AAAI17/
paper/view/14858.
9
Under review as a conference paper at ICLR 2021
A.	G. Barto, R. S. Sutton, and C. W. Anderson. Neuronlike adaptive elements that can solve
difficult learning control problems. IEEE Trans. Syst Man Cybern., 13(5):834-846, 1983. doi:
10.1109/TSMC.1983.6313077. URL https://doi.org/10.1109/TSMC.1983.6313077.
A. G. Barto, G. D. Konidaris, and C. M. Vigorito. Behavioral hierarchy: Exploration and representa-
tion. In G. Baldassarre and M. Mirolli, editors, Computational and Robotic Models of the Hierarchi-
cal Organization of Behavior, pages 13-46. Springer, 2013. doi: 10.1007/978-3-642-39875-9\_2.
URL https://doi.org/10.1007/978-3-642-39875-9_2.
E. Brunskill and L. Li. Pac-inspired option discovery in lifelong reinforcement learning. In Pro-
ceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China,
21-26 June 2014, volume 32 of JMLR Workshop and Conference Proceedings, pages 316-324.
JMLR.org, 2014. URL http://proceedings.mlr.press/v32/brunskill14.html.
P. S. Castro and D. Precup. Using bisimulation for policy transfer in mdps. In M. Fox and D. Poole,
editors, Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010,
Atlanta, Georgia, USA, July 11-15, 2010. AAAI Press, 2010. URL http://www.aaai.org/
ocs/index.php/AAAI/AAAI10/paper/view/1907.
Y. Duan, X. Chen, R. Houthooft, J. Schulman, and P. Abbeel. Benchmarking deep reinforcement
learning for continuous control. In M. Balcan and K. Q. Weinberger, editors, Proceedings of
the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA,
June 19-24, 2016, volume 48 of JMLR Workshop and Conference Proceedings, pages 1329-1338.
JMLR.org, 2016. URL http://proceedings.mlr.press/v48/duan16.html.
B.	Eysenbach, A. Gupta, J. Ibarz, and S. Levine. Diversity is all you need: Learning skills without a
reward function. In 7th International Conference on Learning Representations, ICLR 2019, New
Orleans, LA, USA, May 6-9, 2019, 2019. URL https://openreview.net/forum?id=
SJx63jRqFm.
R.	Fruit and A. Lazaric. Exploration-exploitation in mdps with options. In Proceedings of the 20th
International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017,
Fort Lauderdale, FL, USA, pages 576-584, 2017. URL http://proceedings.mlr.press/
v54/fruit17a.html.
K. Gregor, D. J. Rezende, and D. Wierstra. Variational intrinsic control. In 5th International
Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Workshop
Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=
Skc-Fo4Yg.
J. Harb, P. Bacon, M. Klissarov, and D. Precup. When waiting is not an option: Learning options
with a deliberation cost. In Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18),
and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18),
New Orleans, Louisiana, USA, February 2-7, 2018, pages 3165-3172, 2018. URL https:
//www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17421.
A. Harutyunyan, W. Dabney, D. Borsa, N. Heess, R. Munos, and D. Precup. The termination
critic. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS
2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 2231-2240, 2019. URL http://
proceedings.mlr.press/v89/harutyunyan19a.html.
M. Jaderberg, V. Mnih, W. M. Czarnecki, T. Schaul, J. Z. Leibo, D. Silver, and K. Kavukcuoglu. Rein-
forcement learning with unsupervised auxiliary tasks. In 5th International Conference on Learning
Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
OpenReview.net, 2017. URL https://openreview.net/forum?id=SJ6yPD5xg.
A. Jain, K. Khetarpal, and D. Precup. Safe option-critic: Learning safety in the option-critic
architecture. CoRR, abs/1807.08060, 2018. URL http://arxiv.org/abs/1807.08060.
Y. Jinnai, J. W. Park, D. Abel, and G. D. Konidaris. Discovering options for exploration by minimizing
cover time. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International
10
Under review as a conference paper at ICLR 2021
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings ofMachine Learning Research, pages 3130-3139. PMLR, 2019. URL
http://proceedings.mlr.press/v97/jinnai19b.html.
Y. Jinnai, J. W. Park, M. C. Machado, and G. D. Konidaris. Exploration in reinforcement learning
with deep covering options. In 8th International Conference on Learning Representations, ICLR
2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://
openreview.net/forum?id=SkeIyaVtwB.
A. Jonsson and A. G. Barto. Causal graph based decomposition of factored mdps. J. Mach. Learn.
Res., 7:2259-2301, 2006. URL http://jmlr.org/papers/v7/jonsson06a.html.
K. Khetarpal, M. Klissarov, M. Chevalier-Boisvert, P. Bacon, and D. Precup. Options of interest:
Temporal abstraction with interest functions. In The Thirty-Fourth AAAI Conference on Artificial
Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Con-
ference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 4444-4451. AAAI Press, 2020. URL
https://aaai.org/ojs/index.php/AAAI/article/view/5871.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Y. Bengio and Y. LeCun,
editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,
USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/
1412.6980.
M. Klissarov, P. Bacon, J. Harb, and D. Precup. Learnings options end-to-end for continuous action
tasks. CoRR, abs/1712.00004, 2017. URL http://arxiv.org/abs/1712.00004.
A. S. Klyubin, D. Polani, and C. L. Nehaniv. All else being equal be empowered. In M. S.
Capcarrere, A. A. Freitas, P. J. Bentley, C. G. Johnson, and J. Timmis, editors, Advances in
Artificial Life, 8th European Conference, ECAL 2005, Canterbury, UK, September 5-9, 2005,
Proceedings, volume 3630 of Lecture Notes in Computer Science, pages 744-753. Springer, 2005.
doi: 10.1007/11553090\_75. URL https://doi.org/10.1007/11553090_75.
G. D. Konidaris and A. G. Barto. Building portable options: Skill transfer in reinforcement learning.
In M. M. Veloso, editor, IJCAI 2007, Proceedings of the 20th International Joint Conference
on Artificial Intelligence, Hyderabad, India, January 6-12, 2007, pages 895-900, 2007. URL
http://ijcai.org/Proceedings/07/Papers/144.pdf.
M. C. Machado, M. G. Bellemare, and M. H. Bowling. A laplacian framework for option discovery
in reinforcement learning. In Proceedings of the 34th International Conference on Machine
Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, pages 2295-2304, 2017. URL
http://proceedings.mlr.press/v70/machado17a.html.
T. A. Mann and S. Mannor. Scaling up approximate value iteration with options: Better policies
with fewer iterations. In Proceedings of the 31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, volume 32 of JMLR Workshop and Conference
Proceedings, pages 127-135. JMLR.org, 2014. URL http://proceedings.mlr.press/
v32/mann14.html.
T. A. Mann, S. Mannor, and D. Precup. Approximate value iteration with temporally extended actions.
J. Artif. Intell. Res., 53:375-438, 2015. doi: 10.1613/jair.4676. URL https://doi.org/
10.1613/jair.4676.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In Proceedings of the 33nd Interna-
tional Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24,
2016, pages 1928-1937, 2016. URL http://jmlr.org/proceedings/papers/v48/
mniha16.html.
S.	Mohamed and D. J. Rezende. Variational information maximisation for intrinsically motivated rein-
forcement learning. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information
11
Under review as a conference paper at ICLR 2021
Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada, pages 2125-2133,
2015. URL http://papers.nips.cc/paper/5668- variational- information-
maximisation-for-intrinsically-motivated-reinforcement-learning.
A. W. Moore. Efficient memory-based learning for robot control. Technical Report UCAM-
CL-TR-209, University of Cambridge, Computer Laboratory, Nov. 1990. URL https://
www.cl.cam.ac.uk/techreports/UCAM-CL-TR-209.pdf.
I. Osband, B. V. Roy, D. J. Russo, and Z. Wen. Deep exploration via randomized value functions.
J. Mach. Learn. Res., 20:124:1-124:62, 2019. URL http://jmlr.org/papers/v20/18-
339.html.
A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,
L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chil-
amkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala. Pytorch: An imperative style,
high-performance deep learning library. In H. M. Wallach, H. Larochelle, A. Beygelz-
imer, F. d'Alch6-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Informa-
tion Processing Systems 32: Annual Conference on Neural Information Processing Sys-
tems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages 8024-8035,
2019. URL http://papers.nips.cc/paper/9015-pytorch-an-imperative-
style-high-performance-deep-learning-library.
C. Salge, C. Glackin, and D. Polani. Approximation of empowerment in the continuous domain.
Advances in Complex Systems, 16(2-3), 2013. doi: 10.1142/S0219525912500798. URL https:
//doi.org/10.1142/S0219525912500798.
A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to the nonlinear dynamics of learning
in deep linear neural networks. In Y. Bengio and Y. LeCun, editors, 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
Proceedings, 2014. URL http://arxiv.org/abs/1312.6120.
J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In
F. R. Bach and D. M. Blei, editors, Proceedings of the 32nd International Conference on Machine
Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop and Conference
Proceedings, pages 1889-1897. JMLR.org, 2015a. URL http://proceedings.mlr.press/
v37/schulman15.html.
J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous
control using generalized advantage estimation. CoRR, abs/1506.02438, 2015b. URL http:
//arxiv.org/abs/1506.02438.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/1707.06347.
A. Sharma, S. Gu, S. Levine, V. Kumar, and K. Hausman. Dynamics-aware unsupervised discovery
of skills. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/
forum?id=HJgLZR4KvH.
O. Simsek and A. G. Barto. Skill characterization based on betweenness. In D. Koller, D. SchU-
urmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Sys-
tems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Process-
ing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008, pages 1497-1504.
CUrran Associates, Inc., 2008. URL http://papers.nips.cc/paper/3411-skill-
characterization- based- on- betweenness.
S.	P. Singh, A. G. Barto, and N. Chentanez. Intrinsically motivated reinforcement learning. In
Advances in Neural Information Processing Systems 17 [Neural Information Processing Systems,
NIPS 2004, December 13-18, 2004, Vancouver, British Columbia, Canada], pages 1281-1288,
2004. URL http://papers.nips.cc/paper/2552- intrinsically-motivated-
reinforcement-learning.
R. SUtton and A. Barto. Reinforcement Learning: An Introduction. MIT Press, 2018.
12
Under review as a conference paper at ICLR 2021
R. S. Sutton. Generalization in reinforcement learning: Successful examples using sparse coarse cod-
ing. In D. S. Touretzky, M. Mozer, and M. E. Hasselmo, editors, Advances in Neural Information
Processing Systems 8, NIPS, Denver, CO, USA, November 27-30,1995, pages 1038-1044. MIT
Press, 1995. URL http://papers.nips.cc/paper/1109-generalization-in-
reinforcement-learning- successful- examples- using- sparse- coarse-
coding.
R. S. Sutton, D. Precup, and S. P. Singh. Between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artif. Intell., 112(1-2):181-211, 1999. doi: 10.1016/S0004-
3702(99)00052-1. URLhttps://doi.org/10.1016/S0004-3702(99)00052-1.
T.	Tieleman and G. Hinton. Lecture 6.5—RmsProp: Divide the gradient by a running average of its
recent magnitude. COURSERA: Neural Networks for Machine Learning, 2012.
E.	Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In
2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, Vil-
amoura, Algarve, Portugal, October 7-12, 2012, pages 5026-5033. IEEE, 2012. doi: 10.1109/
IROS.2012.6386109. URL https://doi.org/10.1109/IROS.2012.6386109.
C. M. Vigorito and A. G. Barto. Intrinsically motivated hierarchical skill learning in structured environ-
ments. IEEE Trans. Auton. Ment. Dev., 2(2):132-143, 2010. doi: 10.1109/TAMD.2010.2050205.
URL https://doi.org/10.1109/TAMD.2010.2050205.
O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell,
T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai,
J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Budden,
Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yogatama,
D. Wunsch, K. McKinney, O. Smith, T. SchaUL T. Lillicrap, K. KavUkcUoglu, D. Hassabis,
C. Apps, and D. Silver. Grandmaster level in StarCraft II using multi-agent reinforcement
learning. Nature, 575(7782):350-354, 2019. doi: 10.1038/s41586-019-1724-z. URL https:
//doi.org/10.1038/s41586-019-1724-z.
R. Williams and J. Peng. FUnction optimization Using connectionist reinforcement learning algorithms.
Connection Science, 3:241-, 09 1991. doi: 10.1080/09540099108946587.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Mach. Learn., 8:229-256, 1992. doi: 10.1007/BF00992696. URL https://doi.org/
10.1007/BF00992696.
Y. WU, E. Mansimov, R. B. Grosse, S. Liao, and J. Ba. Scalable trUst-region method for deep
reinforcement learning Using kronecker-factored approximation. In I. GUyon, U. von LUxbUrg,
S. Bengio, H. M. Wallach, R. FergUs, S. V. N. Vishwanathan, and R. Garnett, editors, Advances
in Neural Information Processing Systems 30: Annual Conference on Neural Information
Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 5279-5288,
2017. URL http://papers.nips.cc/paper/7112- scalable- trust- region-
method-for-deep-reinforcement-learning-using-kronecker-factored-
approximation.
R. Zhao, P. Abbeel, and S. Tiomkin. Efficient online estimation of empowerment for reinforcement
learning. CoRR, abs/2007.07356, 2020. URL https://arxiv.org/abs/2007.07356.
13
Under review as a conference paper at ICLR 2021
Figure 7: InfoMax options in the four state deterministic chain. Left: With three options. Right:
With four options.
A More Analysis on Deterministic Chain Example
Figure 7 shows infomax options with three options and four options in the four state deterministic
chain example. Among multiple solutions, we selected options with an absorbing state per option
(i.e., βo (x) = 1.0 for only one x), which are partially the same as the right options in Figure 1. With
four options, Pr(xf |xs) = 0.25 for all xf and xs, thus H(Xf|Xs) is the maximum. This example
shows that we need sufficiently many options for maximizing the MI, otherwise an agent can be
caught in a small loop as we described in Section 3.
B	Omitted Proofs
B.1 Proof of Proposition 1
First, we repeat the assumption 2 in Harutyunyan et al. (2019).
Assumption 1. The distribution dμ(∙∣o) over the starting States of an option o under policy μ is
independent of its termination condition βo.
Note that this assumption does not strictly hold since -Greedy option selection depends on βo via
QO . However, since this dependency is not so strong, we found that βo reliably converged in our
experiments.
Lemma 1. Assume that the distribution dμ(∙∣o) over the starting state ofan option o under policy μ
is independent with βo. Then the following equations hold.
VθβH(Xf Xs) = - X d"(xs,o) XPo(x∣Xs)Vθβ'βo(x) hlogP(x∣Xs) + 1 - XPo(Xf ∣x)(logP(Xf |xs) + 川
xs,o	x	xf
(14)
Vθβ H (Xf Xs,O) = - X d”(χs, o) X P o(χ∣χs)Vθβ 'βo(χ) h log Po(X|Xs) +1 -XPo(Xf|X)logPo(Xf|Xs)+ 1i,
xs ,o	x	xf
(15)
Sampling xs,x,xf, o from dμ and Po,
VθβH(Xf |Xs)= Eχs,χ,χf,o h-Vθβ'βo(x)βo(x)(logP(x|xs) - logP(Xf |xs))]	(16)
VθβH(Xf ∣Xs,O) = Eχs,x,xf,o [-Vθβ'βo(x)βo(x)(logPo(XIXs)- logPo(Xf |xs))] .	(17)
14
Under review as a conference paper at ICLR 2021
Proof of Lemma 1
Proof. First, We prove Equation (14). Let dμ(xs) denote the probability distribution over Xs under
the policy μ, or the marginal distribution of dμ(xs∣o), and dμ(xs,o) denote the joint distribution of
xs and o. Then, We have:
Vθβ H (Xf Xs) = -Vθβ fd"(xs)f P(Xf ∣xs)log P(Xf |xs)
xs	xf
- Xd"(xs) X (Vθβ
P(Xf |Xs) log P(Xf |Xs) + P(Xf |Xs)
xs
xf
vθβ P (Xf |xs) λ
P (xf |xs)	'
-	Xdμ(χs) X VθβP(Xf ∣χs)(logP(Xf |xs) + 1)
xs	xf
—	Xd”(Xs) XXμ(o∣Xs) VθβPo(Xf ∣Xs) (logP(Xf ∣Xs) + 1
Xs	Xf o	'	{z	^
Apply theorem (6)
-	Xd"(Xs) XXμ(o∣Xs) XP o(X∣Xs)Vθβ 'βo (X)(IXf=X — Po(Xf ∣X))(log P(Xf ∣Xs) + 1)
—	Xdμ(Xs) Xμ(o∣Xs) XPo(X∣Xs)Vθβ'βo(x) X(IXf=X — Po(Xf ∣X))(logP(Xf ∣Xs) + 1)
Xs	o	X	Xf
—	X d"(Xs,o) XPo(X∣Xs) Vθβ'βo(x) × [logP(X|Xs) +1 —XPo(Xf|X) logP(Xf|Xs)+ 1 .
Xs ,o	X
1------7--------------{-------'
sample sample
Xf
'------{------}
sample
Sampling xs, x, xf, o, We get (16).
Then We prove Equation (15).
VθβH(Xf Xs,O) = —Ve。]Tdμ(xs,o)∑Po(xf ∣xs)logPo(xf |xs)
Xs ,o
Xf
—	X dμ(xs,o) X (^VθβPo(xf ∣xs)log Po(xf |xs) + Po(xf |xs)
Xs ,o
Xf
Nee P O(Xf IXs)
P o(χf ∣χs)
—	X dμ(xs,o) X VθβPo(xf |xs) (logPo(xf ∣xs) + l)
{Z
Xs ,o
X f -	- {z 一 一
Apply theorem (6)
-	E dμ(xs,o) £ EPo(x∣xs)Vθβ'βο (x)(IXf =X — Po(Xf ∣x))(log Po(Xf |xs) + 1
Xs ,o
Xf X
—	J^dμ(xs,ο) EPo(x∣xs)Vθβ'βο(x) E(IXf =x — Po(xf ∣x))(logPo(xf |xs) + 1
Xs ,o
Xf
—	X dμ(xs,o) XPo(x∣xs) Vθβ'βo(x) × [logPo(x∣xs) + 1 — XPo(xf |x) (logPo(Xf |xs) + 1)]
Xs ,o
^^^{^^^
sample
^z"∖ιf^^~
sample
Xf
1.
^^^{^^^
sample
X
X
1―
}
}
□
Sampling xs, x, xf, o, We get (17).
15
Under review as a conference paper at ICLR 2021
B.2 Proof of Proposition 2
Proof. First, we have that:
logPo(xf|xs) - log P (xf |xs) = log
Po (Xf ∣χs)
P(xf |xs)
log
Pr(Xf ∣Xs ,o)
P(xf |xs)
log
Pr(xs,xf, o) Pr(Xs)
Pr(xs,xf) Pr(xs,o)
l	Pr(xs,Xf,o)
°g Pr(xs,Xf) Pr(o∣xs)
log
Pr(o∣xs,xf )Pr(xs,Xf)
Pr(xs,xf) Pr(o∣xs)
p(o|Xs, Xf)
log Ksr
Using this equation, we can rewrite the equation (10) as:
VθβI(Xf ； O|Xs) = VθβH(Xf |Xs) - VθβH(Xf ∣Xs,O)
=Eχs,x,xf,o [-Vθβ'βo(x)βo(x)(logP(x∣xs) - logP(Xf |xs) - logPo(x∣xs) + logPo(xf |xs)
=Eχs,x,Xf ,o ∣Vθβ 'βo (X)((Iog P o(x∣xs) - log P (XIXs)) — (log Po(Xf ∣Xs)-log P (Xf IXs)))
=Eχs,χ,χf ,o [Vθβ'βo(X)(log p⅛^) - log p⅛≠)]
f L	μ	μ(o∣Xs)	μ(o∣Xs)，」
=Eχs,x,xf ,o Vθβ 'βo (x) (log p(o∣Xs,X) - log p(o∣Xs,Xf))
□
C Implementation Details
C.1 The whole algorithm of A2IMOC
Algorithm 1 shows a full description of A2IMOC. It follows the architecture of A2C (Mnih et al.,
2016; Wu et al., 2017) and has multiple synchronous actors and a single learner. At each optimization
step, We update no, QO,and βo from online trajectories collected by actors. We update p(o∣Xs ,Xf)
for estimating the gradient (11) and μ(o∣Xs) for entropy regularization (13). To learn P and μ stably,
We maintain a replay buffer BO that stores option-transitions, implemented by a LIFO queue. Note
that using older ο,Xs,Xf sampled from the buffer can introduce some bias in the learnedP and μ,
since they depend on the current πo and βo . HoWever, We found that this is not harmful When the
capacity of the replay buffer is reasonably small.
We also add maximization of the entropy of βo to the loss function for preventing the termination
probability saturating on zero or one. Then the full objective of βo is Written as:
logP(o∣Xs,X) - logP(o∣Xs,Xf) + chβH(βo(x)),
Where cHβ is a Weight of entropy bonus.
C.2 Implementation of PPIMOC
For continuous control tasks, We introduce PPIMOC on top of PPO, With the folloWing modifications
to A2IMOC.
16
Under review as a conference paper at ICLR 2021
Algorithm 1 Advantage-Actor InfoMax Option Critic (A2IMOC)
1	: Given: Initial option-value QO , option-policy πo, and termination function βo .
2	: Let BO be a replay buffer for storing option-transitions.
3	: for k = 1, ... do
4	:	for i = 1, 2, ..., N do	. Collect experiences from environment
5	:	Sample termination variable bi from βoi (xi)
6	:	if bi = 1 then
7	:	Store option transition xs , xf, oi to BO
8	:	end if
9	:	Choose next option oi+1 by -Greedy
10	Receive reward Ri and state xi+i, taking a% 〜∏oi+ι (Xi)
11	:	end for
12	:	for all xi in the trajectory do . Train option-policy, option-values, and termination function
13	Update ∏o(ai∣Xi) with PG via the UOAE advantage (12) and policy regularization (13)
14	:	Update QO(xi, o) via the optimistic TD error (12)
15	:	if oi has already terminated then
16	:	Update βo(xi) via (11) and the maximization of cHβH(βo(xi))
17	:	end if
18	:	end for
19	Train P and μ by option-transitions sampled from BO
20	: end for
Upgoing Option-Advantage Estimation for GAE To use an upgoing option-advantage estima-
tion (12) with Generalized Advantage Estimator (GAE) (Schulman et al., 2015b) common with
PPO, we introduce an upgoing general option advantage estimation (UGOAE). Letting δ denote the
TD error corresponding to the marginalized option-state-values, δt = Rt + γVO(xt+1) - VO(xt),
We write the GAE for marginalized policy ∏μ as Aμ = PN=o(γλ)iδt+i, where λ is a coefficient.
Supposing that ot terminates at the t + k step and letting δo denote the TD error corresponding to an
option-state value δto = Rt + γQO (xt+1, o) - QO(xt, o), we formulate UGOAE by:
N
Pk=o(γλ)iδO+i + max( X (γλ)iδt+i, 0)	(k < N)
^o _	i=k+1
AUGOAE = ∖	×___________{z_________}
upgoing estimation
、PN-I(Yλ)iδθ+i + (γλ)N (Rt+N + YUO(xt+N +l) - Qθ(xt+N,o)) (otherwise).
(18)
The idea is the same as UOAE (12) and is optimistic about the advantage after option termination.
Clipped βo Loss In our preliminary experiments, we found that performing multiple steps of
optimization on the gradient (11) led to destructively large updates and resulted in the saturation of
βo to zero or one. Hence, to perform PPO-style multiple updates on βo, we introduce a clipped loss
for βo :
Vθβclip('βo(x) - 'βoold(x), -eβ, eβ)β°°ld(x) (logp(o∣Xs,x) - logp(o∣Xs,Xf)),	(19)
where β is a small coefficient, βoold is a βo before the update, and clip(x, -, ) =
max(-, min(, x)). Clipping makes the gradient zero when βo is sufficiently different than βoold
and inhibits too large updates.
D	Experimental Details
D.1 Network Architecture
Figure 8 illustrates the neural network architecture used in our experiments. In Gridworld experiments,
we used the same state encoder for all networks and we found that it is effective for diversifying πo
as an auxiliary loss (Jaderberg et al., 2017). However, in MuJoCo experiments, we found that sharing
17
Under review as a conference paper at ICLR 2021
Figure 8: Neural Network architecture used for the Gridworld experiments (top) and the MuJoCo
tasks (bottom).
the state encoder can hurt the policy update because the magnitude of βo loss is larger even if clipped
loss (19) is used. As a remedy for this, we used two encoders in MuJoCo experiments: one is for πo
and Qo, and the other is for βo, p, and μ.
In Gridworld experiments, we represented a state as an image and encoded it by a convolutional layer
with 16 filters of size 4 × 4 with stride 1, followed by a convolutional layer with 16 filters of size 2 × 2
with stride 1, followed by a fully connected layer with 128 units. In MuJoCo experiments, we encode
the state by two fully connected layers with 64 units. πo is parameterized as a Gaussian distribution
with separated networks for standard derivations per option, similar to Schulman et al. (2015a). We
used ReLU as an activator for all hidden layers and initialized networks by the orthogonal (Saxe et al.,
2014) initialization in all experiments. Unless otherwise noted, we used the default parameters in
PyTorch (Paszke et al., 2019) 1.5.0.
D.2 Hyperparameters
When evaluating agents, we used -Greedy for selecting options with opt and did not use deterministic
evaluation (i.e., an agent samples actions from πo) in all experiments. We show the algorithm-specific
hyperparameters of A2IMOC in Table 1. In Gridworld experiments, we used = 0.1 for AOC.
Our AOC implementation is based on the released code3 and uses truncated N -step advantage.
Other parameters of AOC and A2C are the same as A2IMOC. We also show the hyperparameters
OfPPIMOC in Table 2. For PPOC, We used Cμent = 0.001 for the weight of the entropy H(μ(x)).
Our PPOC implementation is based on the released code4 and uses N -step (not truncated) GAE for
computing advantage. PPOC and PPO shares all other parameters with PPIMOC.
3https://github.com/jeanharb/a2oc_delib
4https://github.com/mklissa/PPOC
18
Under review as a conference paper at ICLR 2021
Description	Value
γ	0.99
Optimizer	RmsProP (Tieleman and Hinton, 2012)
RmsProp Learning Rate	2 X 10-3
opt	0.8 7 0.2
Number of timesteps per rollout	20
Number of actors	12
cHμ	0.04
cH	0.01
cHβH(βo(x))	0.01
Gradient clipping	1.0
Capacity of BO	480
Batch size for training p^ and μ	240	—
Table 1: Hyperparameters of A2IMOC in Gridworld experiments.
DescriPtion	Value
γ	0.99	二
OPtimizer	Adam (Kingma and Ba, 2015)
Adam Learning Rate	3 × 10-4	―
Adam	1 × 10-4	―
oPt	0.4 → 0.1 (Cartpole swingup), 0.2 → 0.1 (Otherwise)
β	0.05 (Lifelong Billiard),0.1 (otherwise)
Number of timestePs Per rollout	256
Number of actors	16
GAE Parameter (λ)	0.95
Number of ePochs for PPO	10
Minibatch size for PPO	1024
CHμ	0.004
cH	0.001
cHβH(βo(x))	0.01
Gradient cliPPing	0.5
Number of monte carlo rollout to estimate H(∏μ)	20
CaPacity of BO	4096
Batch size for training P and μ	2048	—
Table 2: Hyperparameters of PPIMOC in MuJoCo and Classical Control experiments.
19
Under review as a conference paper at ICLR 2021
A2IMOC
Option 0
AOC
Option 0
OurAOC
Option 0
0.2
— 0.0
Option 2
Option 3
1.0
0.8
0.6
β
0.4
Option 2
Option 3
-0.6
β
0.4
0.2
l-l0.0
1.0
0.8
0.6
β
0.4
, 0.0
Option 2
Option 3
Figure 9: Learned option-policies (πo) and termination functions (βo) for each option in Gridworld
Four Rooms after 5 × 104 steps. Arrows show the probablities of each action and heatmaps show
probabilities of each βo . First row: A2IMOC. Option 0 tends to go down, option 1 and 2 tend to go
right, and option 3 tends to go up. Second row: AOC. All options tend to go down. Third row: Our
AOC. Option 0, 1, and 3 tend to go down and option 2 tends to do right.
D.3 Environmental Details
In the Gridworld experiment, an agent can select four actions: go up, go down, go left and go right.
With the probability 0.1, the agent takes a uniformly random action. If the agent reaches one of
goals, it receives +1.0 or +2.0 reward. Otherwise, an action penalty -0.002 is given. The maximum
episode length is 100.
MuJoCo Point environments are implemented based on “PointMaze” in rllab (Duan et al., 2016)
with some modifications, mainly around collision detection. The maximum episode length is 1000.
In Four Rooms task, an agent receives +0.5 or +1.0 reward when it reaches a goal. Otherwise, an
action penalty -0.0001 is given. This reward structure is the same in Billiard Task: an agent receives
a goal reward when the object ball reaches a goal, otherwise it receives penalty.
D.4 Early Option-Policies learned in Gridworld Four Rooms
Figure 9 shows early option-polices and termination probabilities in Gridworld Four Rooms experi-
ment. We can see that A2IMOC learned the most diverse option-policies.
D.5 Qualitative analysis of Point Four Rooms Experiment
Figure 10 shows the visualizations of learned option-polices and termination functions in MuJoCo
Point Four Rooms, averaged over 100 uniformly sampled states per each position. Arrows show the
expected moving directions, computed from rotations and option-policies. Terminating regions and
option-policies learned with PPIMOC are diverse. For example, option 1 tends to go down while
option 2 tends to go right. In the sampled trajectory of PPIMOC, we can see that it mainly used
option 1 but occasionally switched to option 0 and option 2 for reaching the goal, and switched to
option 3 around the goal. Contrary, for reaching the goal PPOC only used option 3 that does not
terminate in any region. Options learned by Our PPOC is almost the same: termination probability is
high around the upper left corner and option-policies direct downward.
20
Under review as a conference paper at ICLR 2021
PPIMOC
Option 0
Option 1
PPOC
Option 0
Option 1
OurPPOC
Option 0
Option 1
Figure 10: Left: Learned option-policies (πo) and termination functions (βo) in MuJoCo Point
Four Rooms experiment. Arrows show the expected moving direction of the agent and heatmaps
show probabilities of each βo . Right: Sampled trajectories of each method. First row: PPIMOC.
Terminattion regions are clearly separated and option-polices are diverse. Second row: PPOC.
Option 0 terminates at almost everywhere, while option2 and 3 does not terminate anywhere. Third
row: Our PPOC. All options are almost the same.
D.6 Ablation Studies
We conducted ablation studies with three variants of A2IMOC/PPIMOC:
•	cHμ = 0: Do not use the policy regularization based on MI (13).
•	N -step Advantange: Use N -step advantage or N -step GAE instead of UOAE (12)
UGOAE (18).
•	Truncated N -step Advantange: Compute advantage ignoring future rewards instead of using
UOAE or UGOAE.
Figure 11 shows all results in three tasks. We can see that UOAE is effective in all tasks, since
both N -step advantage and truncated N -step advantage performed worse than UOAE. The policy
regularization based on MI (13) is effective only in the Point Billiard lifelong learning task.
D.7 Number of Options
Figure 12 shows the performance of IMOC with varying the number of options. Two options
perfomed worse in all experiments and we need four or more options to make use of IMOC. However,
when we increase the number of options to six and eight, we don’t see any peroformance improvement
from four options, despite of our analysis that we need sufficiently many options to cover the state
space Appendix A. This would be an interesting problem for future works.
21
Under review as a conference paper at ICLR 2021
1.0
S 0.8
P
ra
I 0.6
CC
a
20.4
ro
Io.2
O
0.0
Reward Transition ----------- Truncated N-step GAE
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
Total Environment Steps	1≡6
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
Total Environment Steps	1≡6
Figure 11:	Ablation studies. Top: Peformance progression of A2IMOC in Gridworld Four Rooms.
Bottom: Peformance progression of PPIMOC in MuJoCo Point Four Rooms (left) and MuJoCo
Point Billiard (right).
2.0
乜15
ro
≡
OJ
cc 1.0
E05
ð
0.0
0	100000 200000 300000 400000 500000
Total Environment Steps
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
Total Environment Steps	le6
1.0
S 0.8
P
ro
i 0.6
CC
a
品0.4
ro
io.2
O
0.0
Reward Transition ------ 8
0.0	0.5	1.0	1.5	2.0	2.5	3.0	3.5	4.0
Total Environment Steps	le6
Figure 12:	Ablation studies. Top: Peformance progression of A2IMOC in Gridworld Four Rooms.
Bottom: Peformance progression of PPIMOC in MuJoCo Point Four Rooms (left) and MuJoCo
Point Billiard (right).
22