Under review as a conference paper at ICLR 2021
On Dynamic Noise Influence in Differential
Private Learning
Anonymous authors
Paper under double-blind review
Ab stract
Protecting privacy in learning while maintaining the model performance has be-
come increasingly critical in many applications that involve sensitive data. Private
Gradient Descent (PGD) is a commonly used private learning framework, which
noises gradients based on the Differential Privacy protocol. Recent studies show
that dynamic privacy schedules of decreasing noise magnitudes can improve loss
at the final iteration, and yet theoretical understandings of the effectiveness of
such schedules and their connections to optimization algorithms remain limited.
In this paper, we provide comprehensive analysis of noise influence in dynamic
privacy schedules to answer these critical questions. We first present a dynamic
noise schedule minimizing the utility upper bound of PGD, and show how the
noise influence from each optimization step collectively impacts utility of the final
model. Our study also reveals how impacts from dynamic noise influence change
when momentum is used. We empirically show the connection exists for general
non-convex losses, and the influence is greatly impacted by the loss curvature.
1	Introduction
In the era of big data, privacy protection in machine learning systems is becoming a crucial topic
as increasing personal data involved in training models (Dwork et al., 2020) and the presence of
malicious attackers (Shokri et al., 2017; Fredrikson et al., 2015). In response to the growing demand,
differential-private (DP) machine learning (Dwork et al., 2006) provides a computational framework
for privacy protection and has been widely studied in various settings, including both convex and
non-convex optimization (Wang et al., 2017; 2019; Jain et al., 2019).
One widely used procedure for privacy-preserving learning is the (Differentially) Private Gradient
Descent (PGD) (Bassily et al., 2014; Abadi et al., 2016). A typical gradient descent procedure updates
its model by the gradients of losses evaluated on the training data. When the data is sensitive, the
gradients should be privatized to prevent excess privacy leakage. The PGD privatizes a gradient
by adding controlled noise. As such, the models from PGD is expected to have a lower utility as
compared to those from unprotected algorithms. In the cases where strict privacy control is exercised,
or equivalently, a tight privacy budget, accumulating effects from highly-noised gradients may lead
to unacceptable model performance. It is thus critical to design effective privatization procedures for
PGD to maintain a great balance between utility and privacy.
Recent years witnessed a promising privatization direction that studies how to dynamically adjust
the privacy-protecting noise during the learning process, i.e., dynamic privacy schedules, to boost
utility under a specific privacy budget. One example is (Lee & Kifer, 2018), which reduced the
noise magnitude when the loss does not decrease, due to the observation that the gradients become
very small when approaching convergence, and a static noise scale will overwhelm these gradients.
Another example is (Yu et al., 2019), which periodically decreased the magnitude following a
predefined strategy, e.g., exponential decaying or step decaying. Both approaches confirmed the
empirically advantages of decreasing noise magnitudes. Intuitively, the dynamic mechanism may
coordinate with certain properties of the learning task, e.g., training data and loss surface. Yet there
is no theoretical analysis available and two important questions remain unanswered: 1) What is the
form of utility-preferred noise schedules? 2) When and to what extent such schedules improve utility?
To answer these questions, in this paper we develop a principled approach to construct dynamic
schedules and quantify their utility bounds in different learning algorithms. Our contributions
1
Under review as a conference paper at ICLR 2021
Table 1: Comparison of utility upper bound using different privacy schedules. The algorithms are T -iteration
2R-ZCDP under the PL condition (unless marked With *). The O notation in this table drops other ln terms.
Unless otherwise specified, all algorithms terminate at step T = O(ln NDDR). Assume loss functions are
1-smooth and 1-Lipschitz continuous, and all parameters satisfy their numeric assumptions. Key notations:Op 一
bound occurs in probability p; D 一 feature dimension; N 一 sample size; R 一 privacy budget; Ci - constant; other
notations can be found in Section 4. An extended table and explanation are available in Appendix A.
Algorithm	Schedule (σt2)	Utility Upper Bound
GD+MA (Wang et al., 2017)	O(RTδ)	D ln2 N∖ θ (N2R,δ
Adam+MA (Zhou et al., 2020)	O(京)	O ( √D ln(ND∕∕(1-p))) Op I	N%δ	1
GD, Non-Private	0	θ ( NDR )
GD+zCDP, Static Schedule	T R	C (D ln N、 o IN2R
GD+zCDP, Dynamic Schedule	O (γ(t-T)/2)	O ( NDR )
Momentum+zCDP, Static Schedule	T R	o (NDR(C + lnNit>t))
Momentum+zCDP, Dynamic Schedule	O ( cιγτ+t+∕Y(TT)/2 )	O (NDDR(I + NcDRIT>T))
are summarized as follows. 1) For the class of loss functions satisfying the Polyak-Lojasiewicz
condition (Polyak, 1963), we show that a dynamic schedule improving the utility upper bound is
shaped by the influence of per-iteration noise on the final loss. As the influence is tightly connected to
the loss curvature, the advantage of using dynamic schedule depends on the loss function consequently.
2) Beyond gradient descent, our results show the gradient methods with momentum implicitly
introduce a dynamic schedule and result in an improved utility bound. 3) We empirically validate our
results on convex and non-convex (no need to satisfy the PL condition) loss functions. Our results
suggest that the preferred dynamic schedule admits the exponentially decaying form, and works
better when learning with high-curvature loss functions. Moreover, dynamic schedules give more
utility under stricter privacy conditions (e.g., smaller sample size and less privacy budget).
2	Related Work
Differentially Private Learning. Differential privacy (DP) characterizes the chance of an algorithm
output (e.g., a learned model) to leak private information in its training data when the output
distribution is known. Since outputs of many learning algorithms have undetermined distributions, the
probability of their privacy leakages is hard to measure. A common approach to tackle this issue is to
inject randomness with known probability distribution to privatize the learning procedures. Classical
methods include output perturbation (Chaudhuri et al., 2011), objective perturbation (Chaudhuri et al.,
2011) and gradient perturbation (Abadi et al., 2016; Bassily et al., 2014; Wu et al., 2017). Among
these approaches, the Private Gradient Descent (PGD) has attracted extensive attention in recent
years because it can be flexibly integrated with variants of gradient-based iteration methods, e.g.,
stochastic gradient descent, momentum methods (Qian, 1999), and Adam (Kingma & Ba, 2014), for
both convex and non-convex problems.
Dynamic Policies for Privacy Protection. Wang et al. (2017) studied the empirical risk minimization
using dynamic variation reduction of perturbed gradients. They showed that the utility upper bound
can be achieved by gradient methods under uniform noise parameters. Instead of enhancing the
gradients, Yu et al. (2019); Lee & Kifer (2018) showed the benefits of using a dynamic schedule of
privacy parameters or equivalently noise scales. In addition, adaptive sensitivity control (Pichapati
et al., 2019; Thakkar et al., 2019) and dynamic batch sizes (Feldman et al., 2020) are also demonstrated
to improves the convergence.
Utility Upper Bounds. A utility upper bound is a critical metric for privacy schedules that char-
acterizes the maximum utility that a schedule can deliver in theory. Wang et al. (2017) is the first
to prove the utility bound under the PL condition. In this paper, we improve the upper bound by
a more accurate estimation of the dynamic influence of step noise. Remarkably, by introducing a
dynamic schedule, we further boost the sample-efficiency of the upper bound. With a similar intuition,
Feldman et al. (2020) proposed to gradually increase the batch size, which reduces the dependence
2
Under review as a conference paper at ICLR 2021
on sample size accordingly. Recently, Zhou et al. proved the utility bound by using the momentum of
gradients (Polyak, 1964; Kingma & Ba, 2014). Table 1 summarizes the upper bounds of methods
studied in this paper (in the last block of rows) and results from state-of-the-art algorithms based on
private gradients. Our work shows that considering the dynamic influence can lead to a tighter bound.
3	Private Gradient Descent
Notations. We consider a learning task by empirical risk minimization (ERM) f (θ) =
N PnN=I f (θ; Xn) on a private dataset {χn}n^=1 and θ ∈ RD. The gradient methods are defined as
θt+ι = θt - ηtVt, where Nt = Vf (θt) = N Pn Vf (θt; Xn) denotes the non-private gradient at
iteration t, η is the step learning rate. Vtn) = Vf (θt; Xn) denotes the gradient on a sample Xn. Ic
denotes the indicator function that returns 1 if the condition c holds, otherwise 0.
Assumptions. (1) In this paper, we assume f(θ) is continuous and differentiable. Many commonly
used loss functions satisfy this assumption, e.g., the logistic function. (2) For a learning task, only
finite amount of privacy cost is allowed where the maximum cost is called privacy budget and denoted
as R. (3) Generally, we assume that loss functions f(θ; X) (sample-wise loss) are G-Lipschitz
continuous and f(θ) (the empirical loss) is M -smooth.
Definition 3.1 (G-LiPSchitz continuity). A function f (∙) is G-Lipschitz continuous if, for G > 0
and all x, y in the domain of f(∙), f(∙) satisfies ∣∣f (y) - f (x)k ≤ Gky - x∣∣2..
Definition 3.2 (m-strongly convexity). A function f (∙) is m-strongly convex if f (y) ≥ f (x) +
Vf (x)t(y — x) + m∣∣y — x∣2, for some m > 0 and all x, y in the domain of f (∙).
Definition 3.3 (M -smoothness). A function is M -smooth w.r.t. l2 norm if f(y) ≤ f(X) +
Vf (X)T(y - x) + MM ∣∣y - x∣2, for some constant M > 0 and all x, y in the domain of f (∙).
For a private algorithm M(d) which maps a dataset d to some output, the privacy cost is measured
by the bound of the output difference on the adjacent datasets. Adjacent datasets are defined to be
datasets that only differ in one sample. In this paper, we use the zero-Concentrated Differential
Privacy (zCDP, see Definition 3.4) as the privacy measurement, because it provides the simplicity
and possibility of adaptively composing privacy costs at each iteration. Various privacy metrics are
discussed or reviewed in (Desfontaines & Pej6, 2019). A notable example is Moment Accoutant
(MA) (Abadi et al., 2016), which adopts similar principle for composing privacy costs while is less
tight for a smaller privacy budget. We note that alternative metrics can be adapted to our study
without major impacts to the analysis.
Definition 3.4 (ρ-zCDP (Bun & Steinke, 2016)). Let ρ > 0. A randomized algorithm M : Dn → R
satisfies ρ-zCDP if, for all adjacent datasets d, d0 ∈ Dn, Dα(M(d)∣M(d0)) ≤ ρα, ∀α ∈ (1, ∞)
where Da(∙∣∙) denotes the Renyi divergence (Renyi, 1961) of order α.
zCDP provides a linear composition of privacy costs of sub-route algorithms. When the input vector
is privatized by injecting Gaussian noise ofN(0, σt2I) for the t-th iteration, the composed privacy
cost is proportional to Pt Pt where the step cost is Pt = ∙1. For simplicity, we absorb the constant
coefficient into the (residual) privacy budget R. The formal theorems for the privacy cost computation
of composition and Gaussian noising is included in Lemmas B.1 and B.2.
Generally, we define the Private Gradient Descent (PGD) method as iterations for t = 1 . . . T :
Θt+1 = θt - ηtφt = θt - ηt(Vt + σtGνt∕N),	(1)
where φt = gt is the gradient privatized from Vt as shown in Algorithm 1, G/N is the bound of
sensitivity of the gathered gradient excluding one sample gradient, and Vt 〜N(0, I) is a vector
element-wisely subject to Gaussian distribution. We use σt to denote the noise scale at step t and use
σ to collectively represents the schedule (σ1, . . . , σT) if not confusing. When the Lipschitz constant
is unknown, we can control the upper bound by scaling the gradient if it is over some constant. The
scaling operation is often called clipping in literatures since it clips the gradient norm at a threshold.
After the gradient is noised, we apply a modification, φ(∙), to enhance its utility. In this paper, we
consider two types of φ(∙):
φ(mt, gt) = gt (GD), φ(mt, gt) = [β(1 - βt-1)mt + (1 - β)gt]∕(1 - βt) (Momentum)
We now show that the PGD using Algorithm 1 guarantees a privacy cost less than R:
3
Under review as a conference paper at ICLR 2021
Algorithm 1 Privatizing Gradients
Input: Raw gradients [vt1),..., V(n)] (n = N by default), vt,residual privacy budget Rt assuming
the full budget is R and R1 = R.
1： Pt - 1∕σ2, Vt 一 1 Pn=1 V(i)	. Budget request
2:	if ρt < Rt then
3:	Rt+1 - Rt - Pt
4:	gt -Vt + Gσwt∕N, Vt 〜N(0,I)	. Privacy noise
5:	mt+1 J Φ(mt, gt) or gι if t = 1
6:	return ηtmt+1, Rt+1	. Utility projection
7:	else
8:	Terminate
Theorem 3.1. Suppose f(θ; x) is G-Lipschitz continuous and the PGD algorithm with privatized
gradients defined by Algorithm 1, stops at step T. The PGD algorithm outputs θT and satisfies
ρ-ZCDP where P ≤ 2 R.
Note that Theorem 3.1 allows σt to be different throughout iterations. Next we present a principled
approach for deriving dynamic schedules optimized for the final loss f(θT ).
4 Dynamic Policies by Minimizing Utility Upper B ounds
To characterize the utility of the PGD, we adopt the Expected Excess Risk (EER), which notion is
widely used for analyzing the convergence of random algorithms, e.g., (Bassily et al., 2014; Wang
et al., 2017). Due to the presence of the noise and the limitation of learning iterations, optimization
using private gradients is expected to reach a point with a higher loss (i.e., excess risk) as compared
to the optimal solution without private protection. Define θ* = arg min& f (θ), after Algorithm 1 is
iterated for T times in total, the EER gives the expected utility degradation:
EER = Eν[f(θτ +ι)] - f(θ)
Due to the variety of loss function and complexity of recursive iterations, an exact EER with noise is
intractable for most functions. Instead, we study the worst case scenario, i.e., the upper bound of
the EER, and our goal is to minimize the upper bound. For consistency, we call the upper bound of
EER divided by the initial error as ERUB. Since the analytical form of EER is either intractable or
complicated due to the recursive iterations of noise, studying the ERUB is a convenient and tractable
alternative. The upper bound often has convenient functional forms which are (1) sufficiently simple,
such that we can directly minimize it, and (2) closely related to the landscape of the objective
depending on both the training dataset and the loss function. As a consequence, it is also used in
previous PGD literature (Pichapati et al., 2019; Wang et al., 2017) for choosing proper parameters.
Moreover, we let ERUBmin be the achievable optimal upper bound by a specific choice of parameters,
e.g., the σ and T .
In this paper, we consider the class of loss functions satisfying the Polyak-Lojasiewicz (PL) condition
which bounds losses by corresponding gradient norms. It is more general than the m-strongly
convexity. If f is differentiable and M -smooth, then m-strongly convexity implies the PL condition.
Definition 4.1 (Polyak-Lojasiewicz condition (Polyak, 1963)). For f (θ), there exists μ > 0 and for
every θ, ∣∣Vf(θ)∣∣2 ≥ 2μ(f (θ) - f(θ*)).
The PL condition helps us to reveal how the influence of step noise propagates to the final excess
error, i.e., EER. Though the assumption was also used previously in Wang et al. (2017); Zhou et al.
(2020), neither did they discuss the propagated influence of noise. In the following sections, we will
show how the influence can tighten the upper bound in gradient descent and its momentum variant.
4
Under review as a conference paper at ICLR 2021
4.1	Gradient Descent Methods
For the brevity of our discussion, we first define the following constants:
1 △ 2RMN2
α ,	DG2
(f(θι)- f(θ*)),
K , M, and Y , 1-1,
(2)
μ
κ
which satisfy K ≥ 1 and Y ∈ [0,1). Note that K is the condition number of f (∙) if f (∙) is strongly
convex. K tends to be large if the function is sensitive to small differences in inputs, and 1∕a tends to
be large if more samples are provided and with a less strict privacy budget. The convergence of PGD
under the PL condition has been studied for private (Wang et al., 2017) and non-private (Karimi et al.,
2016; Nesterov & Polyak, 2006; Reddi et al., 2016) ERM. Below we extend the bound in (Wang
et al., 2017) by considering dynamic influence of noise and relax σt to be dynamic:
Theorem 4.1. Let a, K and Y be defined in Eq. (2), and η =4.Suppose f (θ; Xi) is G-Lipschitz
and f (θ) is M -smooth satisfying the Polyak-Lojasiewicz condition. For PGD, the following holds:
T
ERUB = YT + R	qtσt2 , where qt , YT-tα.
(3)
In Eq. (3), the step noise magnitude σt2 has an exponential influence, qt, on the EER. The dynamic
characteristic of the influence is the key to prove a tighter bound. Plus, on the presence of the dynamic
influence, it is natural to choose a dynamic σt2. When relaxing qt to a static 1, a static σt2 was studied
by Wang et al. They proved a bound which is nearly optimal except a ln2 N factor. To get the optimal
bound, in the following sections, we look for the σ and T that minimize the upper bound.
4.1.1	Uniform Schedule
The uniform setting of σt has been previously studied in Wang et al. (2017). Here, we show that the
bound can be further tightened by considering the dynamic influence of iterations and a proper T .
Theorem 4.2. Suppose conditions in Theorem 4.1 are satisfied. When σt2 = T∕R, let α, Y and K be
defined in Eq. (2) and let T be:
T
O
K ln
1+
(4)
Meanwhile, if K ≥ ɪ--e > 1, 1∕a > 1∕ɑ0 for some constant C ∈ (0,1) and a0 > 0, the correspond-
ing bound is:
ERUBmfm = θ (" ln (1 + Ka)).	(5)
Sketch of proof. The key of proof is to find a proper T to minimize
TT
ERUB = E = γT + ^X ]YT-taRσ2 = YT + αT —-= YT + qk(1 — YT )T
where we use σt = YT/R. Vanishing its gradient is to solve YT ln Y+qk(1-yt)-gkTyT ln Y = 0,
which however is intractable. In (Wang et al., 2017), T is chosen to be O(ln(1∕α)) and ERUB is
relaxed as YT + αKT2. The approximation results in a less tight bound as O(α(1 + K ln2(1∕α)))
which explodes as K → ∞.
We observe that for a super sharp loss function, i.e., a large K, any minor perturbation may result in
tremendously fluctuating loss values. In this case, not-stepping-forward will be a good choice. Thus,
We choose T =由七))ln(1 + ln(0/Y)) ≤ O (Kln(1 + 熹))which converges to 0 as K → +∞.
The full proof is deferred to the appendix.	□
4.1.2	Dynamic Schedule
A dynamic schedule can improve the upper bound delivered by the uniform schedule. First, we
observe that the excess risk in Eq. (3) is upper bounded by two terms: the first term characterizes the
error due to the finite iterations of gradient descents; the second term, a weighted sum, comes from
error propagated from noise at each iteration. Now we show for any {qt|qt > 0, t = 1, . . . , T} (not
limited to the qt defined in Eq. (3)), there is a unique σt minimizing the weighted sum:
5
Under review as a conference paper at ICLR 2021
Lemma 4.1 (Dynamic schedule). Suppose σt satisfy PtT=1 σ-2 = R. Given a positive sequence
{qt }, the following equation holds:
minRXT=Iqtσ2 = (XL √qt)，whenσ2 = RXT=I rqt.	⑹
Remarkably, the difference between the minimum and T PtT=1 qt (uniform σt ) monotonically in-
creases by the variance of √qt w.r.t. t.
We see that the dynamics in σt come from the non-uniform nature of the weight qt . Since qt presents
the impact of the σt on the final error, we denote it as influence. Given the dynamic schedule in
Eq. (6), it is of our interest to which extent the ERUB can be improved. First, we present Theorem 4.3
to show the optimal T and ERUB.
Theorem 4.3. Suppose conditions in Theorem 4.1 are satisfied. Let α, κ and γ be defined in Eq. (2).
When ηt 二 吉,σt (based on Eqs. (3) and (6)) and the T minimizing ERUB are, i.e.,
1 vWYF -1
R 1 -√γ
2κ ln
1+
(7)
Meanwhile, when K ≥ 1 and 1∕α ≥ 1∕ɑ° for some positive constant ao, the minimal bound is:
ERUBmZmiC = Θ (F⅛
(8)
4.1.3 Discussion
In Theorems 4.2 and 4.3, we present the tightest bounds for functions satisfying the PL condition,
to our best knowledge. We further analyze the advantages of our bounds from two aspects: sample
efficiency and robustness to sharp losses.
Sample efficiency. Since dataset cannot be infinitely large, it is critical to know how accurate the
model can be trained privately with a limited number of samples. Formally, it is of interest to study
when κ is fixed and N is large enough such that α 1. Then we have the upper bound in Eq. (5) as
ERUB 嚅rm ≤ O 1 2α ln(κα)) ≤ O(DM2N(N)),
(9)
where We ignore K and other logarithmic constants with O as done in Wang et al. (2017). As a
result, we get a bound very similar to (Wang et al., 2017), except that R is replaced by RM A =
2∕ ln(1∕δ) using Moment Accountant. In comparison, based on Lemma B.3, R = 2ρ = 2 +
4ln(1∕δ) + 4pin(1∕δ)(e + ln(1∕δ) if θT satisfies P-ZCDP Because ln(1∕δ) > 1, it is easy to see
R = RzCDP > RM A when ≤ 2 ln(1∕δ). As compared to the one reported in (Wang et al.,
2017), our bound saved a factor of ln Nand thus is require less sample to achieve the same accuracy.
Remarkably, the saving is due to the maintaining of the influence terms as shown in the proof of
Theorem 4.2.
Using the dynamic schedule, we have ERUBmynamiC ≤ O(ɑ) = O (MGR), which saved another
ln N factor in comparison to the one using the uniform schedule Eq. (9). As shown in Table 1, such
advantage maintains when comparing with other baselines.
Robustness. Besides sample efficiency, we are also interested in robustness of the convergence under
the presence of privacy noise. Because of the privacy noise, the convergence of private gradient
descent will be unable to reach an ideal spot. Specifically, when the samples are noisy or have noisy
labels, the loss curvature may be sharp. The sharpness also implies lower smoothness, i.e., a small M
or has a very small PL parameter. Thus, gradients may change tremendously at some steps especially
in the presence of privacy noise. As illustrated in the left figure, the highly-curved loss function (the
green curve) results in mean higher final loss (the red dashed line) than the flatten curve (purple and
blue lines). Such changes have more critical impact when only a less number of iterations can be
executed due to the privacy constraint. Assume α is some constant while K 1∕α, we immediately
get:
ERUBmniform = Θ (Kln(1 + Ka)) = Θ (1) ≤ O (MDGR) , ERUBmynnamic = Θ(1).
6
Under review as a conference paper at ICLR 2021
Both are robust, but the dynamic schedule has a smaller factor since 1∕a could be a large number. In
addition, the factor implies that when more samples are used, the dynamic schedule is robuster.
4.2 Gradient Descent Methods with Momentum
Section 4.1 shows that the step noise has an exponentially increas-
ing influence on the final loss, and therefore a decreasing noise
magnitude improves the utility upper bound by a ln N factor. How-
ever, the proper schedule can be hard to find when the curvature
information, e.g., κ, is absent. A parameterized method that less de-
pends on the curvature information is preferred. On the other hand,
long-term iterations will result in forgetting of the initial iterations,
since accumulated noise overwhelmed the propagated information
from the beginning. This effect will reduce the efficiency of the
recursive learning frameworks.
Alternative to GD, the momentum method can mitigate the two is-
sues. It was originally proposed to stabilize the gradient estimation
(Polyak, 1964). In this section, we show that momentum (agnostic
about the curvature) can flatten the dynamic influence and improve
the utility upper bound. Previously, Pichapati et al. used the mo-
mentum as an estimation of gradient mean, without discussions of
Figure 1: Private gradient
descent repeated 100 times
on two differently-curved loss
functions. Solid lines are
optimization trajectories and
dashed horizontal lines are the
averaged final losses.
convergence improvements. Zhou et al. gave a bound for the Adam with DP. However, the derivation
is based on gradient norm, which results in a looser bound (see Table 1).
The momentum method stabilizes gradients by moving average history coordinate values and thus
greatly reduces the variance. The φ(mt, gt) can be rewritten as:
mt+1 = φ(mt, gt) = ivt+⅛, vt+1 = βvt + (1 - β)gt = (I-⑶ Xt=I βt-igt, v1 = 0, (10)
where β ∈ [0, 1]. Note v+1 is a biased estimation of the gradient expectation while m+1 is unbiased.
Theorem 4.4	(Convergence under PL condition). Suppose f(θ; xi) is G-Lipschitz, and f(θ) is M-
smooth and satisfies the PoIyak-Lojasiewicz condition. Assume β = Y and β ∈ (0,1). Let η =翁
and no ≤ 8 (pl + 64βγ (γ — β)-2(1 — β)-3 + 1) .Then thefollowing holds:
EER ≤
(YT + 2Rnoα U3(σT} )(f(θι) — f(θ*)) — Z2M X
noise varinace	'
T
t=1γt- Ekvt+ιk
"{^^^^^^^^^^"
momentum effect
where
γ = 1— n0，Z = 1 — βα⅛yn2 - 4no ≥0，
U3 = XT=IYTT>Xt=I β2f
(11)
(12)
(13)
2
The upper bound includes three parts that influence the bound differently: (1) Convergence. The
convergence term is mainly determined by no and κ. no should be in (0, K) such that the upper bound
can converge. A large η0 will be preferred to speed up convergence if it does not make the rest two
terms worse. (2) Noise Variance. The second term compressed in U3 is the effect of the averaged
noise, Pit=1 β2(t-i)σ2. One difference introduced by the momentum is the factor (1 — β)∕(1 — βt)
which is less than Yt at the beginning and converges to a non-zero constant 1 — β. Therefore, in U3 ,
γT-t(1 — β)/(1 — βt) will be constantly less than γτ meanwhile. Furthermore, when t > T, the
moving average Pit=1 β2(t-i)σi2 smooths the influence of eachσt. In Appendix D, we will see that
the influence dynamics is less steep than that of GD. (3) Momentum Effect. The momentum effect
term can improve the upper bound when no is small. For example, when β = 0.9 and Y = 0.99, then
no ≤ 0.98/M which is a rational value. Following the analysis, when M is large which means the
gradient norms will significantly fluctuate, the momentum term may take the lead. Adjusting the
noise scale in this case may be less useful for improving utility.
To give an insight on the effect of dynamic schedule, we provide the following utility bounds.
7
Under review as a conference paper at ICLR 2021
Theorem 4.5	(Uniform schedule). Suppose the assumptions in Theorem 4.4 are satisfied.
σt2 = T /R, and let:
Let
T=max t s. YtT ≥[，T=[O (η ln (ι+≡))].
Given some positive constant C and a0 > 0 with 1∕α> I∕ɑ0, thefollowing inequality holds:
ERUBmm ≤ O ( κ +ηo∕ɑ hIT ≤T + YTTln (1 + ")IT>T ] ).
Theorem 4.6	(Dynamic schedule). Suppose the assumptions in Theorem 4.4 are satisfied. Let
α = γ(2-0^2), β < Y and T = max t s.t. YtT ≥ l-t. Use thefollowing schedule:
σt2
O (生 ln (1 + 变
η0	κα
where qt = ci YT+卬丁≤T + YT-1C2 YT-t1τ>τ for some positive constants ci and c2. Thefollowing
inequality holds:
T
ERUB ≤ YT+2η0α	t=iRqtσt2, ERUBmin
≤O
κα
κα + η0
κα
1 IT ≤T + it>T
κα + η0 T ≤T	T >T
Discussion. Theoretically, the dynamic schedule is more influential in vanilla gradient descent
methods than the momentum variant. The result is mainly attributed to the averaging operation. The
moving averaging, (1 - β) Pti=i βt-igi∕(1 - βt), increase the influence of the under-presented
initial steps and decrease the one of the over-sensitive last steps. Counterintuitively, the preferred
dynamic schedule should be increasing since qt decreases when t ≤ T.
4.3 Private Stochastic Gradient Descent (NEW SECTION on REBUTTAL)
Though PGD provides a guarantee both for utility and privacy, computing gradients of the whole
dataset is impractical for large-scale problems. For this sake, studying the convergence of Private
Stochastic Gradient Descent (PSGD) is meaningful. The Algorithm 1 can be easily extended to
PSGD by subsampling n gradients where the batch size n N. According to (Yu et al., 2019),
when privacy is measured by zCDP, there are two ways to account for the privacy cost of PSGD
depending on the batch-sampling method: sub-sampling with or without replacement. In this paper,
we focus on the random subsampling with replacement since it is widely used in deep learning in
literature, e.g., (Abadi et al., 2016; Feldman et al., 2020). Accordingly, we replace N in the definition
ofα by n because the term is from the sensitivity of batch data (see Eq. (1)). For clarity, we assume
that T is the number of iterations rather than epochs and that ▽ t is mean stochastic gradient.
When a batch of data are randomly sampled, the privacy cost of one iteration is cp2∕σt where c
is some constant, p = n∕N is the sample rate, and 1∕σt2 is the full-batch privacy cost. Details of
the sub-sampling theorems are referred to the Theorem 3 of (Yu et al., 2019) and their empirical
setting. Threfore, we can replace the privacy constraint Pt p2∕σt2 = R by Pt 1∕σt2 = R0 where
R0 = R∕p2 = N R. Remarkably, we omit the constant C because it will not affect the results
regarding uniform or dynamic schedules. Notice N2R in theα is replaced by n2R0 = N2R. Thus,
the form ofα is not changed which provides convenience for the following derivations.
Now we study the utility bound of PSGD. To quantify the randomness of batch sampling, we define a
random vector ξt with E[ξt] = 0 and E ∣∣ξt∣∣2 ≤ D such that ▽ t ≤ Vt + σgξt∕n for some positive
constant σg . Because ξt has similar property to the privacy noise νt, we can easily extend the PGD
bounds to PSGD bounds by following theories.
Theorem 4.7 (Utility bounds of PSGD). Let a, K and Y be defined in Eq. (2), and ηt =吉.Suppose
f (θ; xi) is G-Lipschitz and f (θ) is M -smooth satisfying the Polyak-Lojasiewicz condition. For
PSGD, when batch size satisfies n = max{N√R, 1}, thefollowing holds:
T
ERUB = YT +αgσg2 + R0	qtσt2, where qt , YT-tα,	1∕σt2 = R0.	(14)
t=i	t
where Qg = 2μN 2R(f Dι)-f (θ*)).
8
Under review as a conference paper at ICLR 2021
Theorem 4.8 (PSGD with momentum). Let ag = 2.n2Rf ⑸)_于(®*)). Suppose assumptions in
Theorem 4.4 holds. When batch size satisfies n = max{N√R, 1} ,the U3(σ, T) has to be replaced
by
U3 = Ug + U3, with αRU3 ≤ αgσg	(15)
when PSGD is used.
As shown above, the utility bound of PSGD differs from the PGD merely by αgσg2 . Note αg =
O( NdDr ) which fits the order of dynamic-schedule bounds. In addition, α and other variables are not
changed. Hence, the conclusions w.r.t. the dynamic/uniform schedules maintain the same.
5	Experiments
We empirically validate the properties of privacy schedules and their connections to learning algo-
rithms. In this section, we briefly review the schedule behavior on quadratic losses under varying
data sensitivity. Details of experimental setups and empirical results are available in Appendix D.
We first show the estimated influence of step noise qt (by retraining the private learning algorithms,
ref. Appendix D) in Fig. 2 Left. We see the trends of influence are approximately in an exponential
form of t. This obvervation motivates the use of exponential decay schedule in practice.
We then show the trends on the variance of influence (dashed lines with the right axis) and relative
final losses (solid lines with the left axis) in the Middle Pane, where uni denotes the uniform schedule
baseline, exp is an exponential schedule, dyn denotes the dynamic schedule minimizing the ERUB.
The influences increases steeply when the data scale is large and therefore have a large variance.
Meanwhile, dynamic schedules show improvements of the final loss when the variance is large. It
reveals the connection between the influence and the dynamic advantage (refer to Lemma 4.1).
We lastly evaluate the impacts from momentum in the Right Pane, using a Deep Neural Network
(DNN) with 2 layers and 100 hidden units. Because of time costs of training deep networks, we do
not estimate the influence by retraining and then compute schedules. Instead, we grid-search for the
schedule hyper-parameters to find the best one. We see that influence modeled by an exponential
function (expinfl) has comparable performance of the influence modeled by linear combination of two
reverse exponential functions (momexpinfl). The latter only shows advantage in the setting that data
scale is 25 and the number of iteration is only 100, which is expected by our analysis Theorems 4.5
and 4.6. The inherent reason is that the dynamic schedule is more effective when T is larger.
sso--eUy ueəu Q>Re-a4
sso--euy ueəu Q>ae-aJ
02-
04-
06-
08-
10-
12-
14-
10	15	20
data scale
⅛He>t
Figure 2: Comparison of dynamic schedule and uniform schedule on different data scale. Left pane
is the influence by iteration estimated by retraining. The rest two are the relative loss by varying
data scale (left axis with solid lines) and the variance of influence (right axis with dashed lines). The
middle is on the MNIST35 dataset consisting of 1000 digit 3 and 5 images using quadratic regression.
The right is the final loss on subsampled MNIST dataset of 1000 training samples and 50, 000 test
samples when using DNN and momentum methods.
6	Conclusion
When a privacy budget is provided for a certain learning task, one has to carefully schedule the
privacy usage through the learning process. Uniformly scheduling the budget has been widely used
in literature whereas increasing evidence suggests that dynamically schedules could empirically
outperform the uniform one. This paper provided a principled analysis on the problem of optimal
budget allocation and connected the advantages of dynamic schedules to both the loss structure and
the learning behavior. We further validated our results through empirical studies.
9
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar,
and Li Zhang. Deep Learning with Differential Privacy. In CCS: Proceedings of the 2016 ACM
SIGSAC Conference on Computer and Communications Security, CCS '16, pp. 308-318, New
York, NY, USA, 2016. ACM.
R. Bassily, A. Smith, and A. Thakurta. Private Empirical Risk Minimization: Efficient Algorithms
and Tight Error Bounds. In 2014 IEEE 55th Annual Symposium on Foundations of Computer
Science, pp. 464-473, October 2014.
Raef Bassily, Vitaly Feldman, Kunal Talwar, and Abhradeep Guha Thakurta. Private Stochastic
Convex Optimization with Optimal Rates. In Advances in Neural Information Processing Systems
32, pp. 11282-11291. Curran Associates, Inc., 2019.
Mark Bun and Thomas Steinke. Concentrated Differential Privacy: Simplifications, Extensions,
and Lower Bounds. In Theory of Cryptography, volume 9985, pp. 635-658. Springer Berlin
Heidelberg, Berlin, Heidelberg, 2016.
Kamalika Chaudhuri, Claire Monteleoni, and Anand D. Sarwate. Differentially Private Empirical
Risk Minimization. Journal of Machine Learning Research, 12(Mar):1069-1109, 2011.
Rachel Cummings, Sara Krehbiel, Kevin A Lai, and Uthaipon Tantipongpipat. Differential Privacy
for Growing Databases. In Advances in Neural Information Processing Systems 31, pp. 8864-8873.
Curran Associates, Inc., 2018.
Damien Desfontaines and Balgzs Pej6. SoK: Differential Privacies. arXiv:1906.01337 [cs], June
2019.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating Noise to Sensitivity
in Private Data Analysis. In Theory of Cryptography, Lecture Notes in Computer Science, pp.
265-284. Springer Berlin Heidelberg, 2006.
Cynthia Dwork, Alan Karr, Kobbi Nissim, and Lars Vilhuber. On Privacy in the Age of COVID-19.
Journal of Privacy and Confidentiality, 10(2), June 2020.
Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: Optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing, STOC 2020, pp. 439-449, New York, NY, USA, June 2020. Association for Computing
Machinery.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model Inversion Attacks That Exploit
Confidence Information and Basic Countermeasures. In CCS: Proceedings of the 22Nd ACM
SIGSAC Conference on Computer and Communications Security, CCS ’15, pp. 1322-1333, New
York, NY, USA, 2015. ACM.
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the Last Iterate of SGD Information
Theoretically Optimal. In Conference on Learning Theory, pp. 1752-1755, June 2019.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear Convergence of Gradient and Proximal-
Gradient Methods Under the Polyak-匕OjaSieWiCz Condition. In Machine Learning and Knowledge
Discovery in Databases, Lecture Notes in Computer Science, pp. 795-811, Cham, 2016. Springer
International Publishing.
Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv:1412.6980
[cs], December 2014.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, November 1998.
Jaewoo Lee and Daniel Kifer. Concentrated Differentially Private Gradient Descent with Adaptive
per-Iteration Privacy Budget. In Proceedings of the 24th ACM SIGKDD International Conference
on Knowledge Discovery & Data Mining, KDD ’18, pp. 1656-1665, New York, NY, USA, 2018.
ACM.
10
Under review as a conference paper at ICLR 2021
Yurii Nesterov and B.T. Polyak. Cubic regularization of Newton method and its global performance.
Mathematical Programming, 108(1):177-205, AUgUst 2006.
Venkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X. Yu, Sashank J. Reddi, and Sanjiv Kumar.
AdaCliP: Adaptive Clipping for Private SGD. arXiv:1908.07643 [cs, stat], October 2019.
B. T. Polyak. Gradient methods for the minimisation of fUnctionals. USSR Computational Mathemat-
ics and Mathematical Physics, 3(4):864-878, JanUary 1963.
B. T. Polyak. Some methods of speeding Up the convergence of iteration methods. USSR Computa-
tional Mathematics and Mathematical Physics, 4(5):1-17, JanUary 1964.
Ning Qian. On the momentUm term in gradient descent learning algorithms. Neural Networks, 12(1):
145-151, JanUary 1999.
Sashank J. Reddi, Ahmed Hefny, SUvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic Variance
RedUction for Nonconvex Optimization. In International Conference on Machine Learning, pp.
314-323, JUne 2016.
Alfred R6nyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics. The Regents of the University of California, 1961.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. Cambridge University Press, May 2014.
Shai Shalev-Shwartz, Nathan Srebro, and Karthik Sridharan. Stochastic Convex Optimization. In
Proceedings of the 22nd Annual Conference on Learning Theory, COLT ’09, pp. 11, 2009.
R. Shokri, M. Stronati, C. Song, and V. Shmatikov. Membership Inference Attacks Against Machine
Learning Models. In 2017 IEEE Symposium on Security and Privacy (SP), pp. 3-18, May 2017.
Om Thakkar, Galen Andrew, and H. Brendan McMahan. Differentially Private Learning with
Adaptive Clipping. arXiv:1905.03871 [cs, stat], May 2019.
Di Wang, Minwei Ye, and JinhUi XU. Differentially Private Empirical Risk Minimization Revisited:
Faster and More General. In Advances in Neural Information Processing Systems 30, pp. 2722-
2731. CUrran Associates, Inc., 2017.
Di Wang, ChangyoU Chen, and JinhUi XU. Differentially Private Empirical Risk Minimization with
Non-convex Loss FUnctions. In International Conference on Machine Learning, pp. 6526-6535,
May 2019.
Xi WU, Fengan Li, ArUn KUmar, Kamalika ChaUdhUri, Somesh Jha, and Jeffrey NaUghton. Bolt-on
Differential Privacy for Scalable Stochastic Gradient Descent-based Analytics. In Proceedings of
the 2017 ACM International Conference on Management of Data, SIGMOD ’17, pp. 1307-1322,
New York, NY, USA, 2017. ACM.
Lei YU, Ling LiU, Calton PU, Mehmet Emre GUrsoy, and Stacey TrUex. Differentially Private Model
PUblishing for Deep Learning. proceedings of 40th IEEE Symposium on Security and Privacy,
April 2019.
YingxUe ZhoU, Xiangyi Chen, Mingyi Hong, Zhiwei Steven WU, and Arindam Banerjee. Private
Stochastic Non-Convex Optimization: Adaptive Algorithms and Tighter Generalization BoUnds.
arXiv:2006.13501 [cs, stat], AUgUst 2020.
11
Under review as a conference paper at ICLR 2021
A Comparison of algorithms
Table 2: Comparison of empirical excess risk bounds. The algorithms are T-iteration 2R-ZCDP or
equivalently (, δ)-DP under the PL condition (unless marked with * for convexity). The O notation
in this table drops other ln terms. All algorithms in the second part terminate at step T = O(ln NDDR).
Assume loss functions are 1-smooth and 1-Lipschitz continuous, and all parameters satisfy their
numeric assumptions. Key notations: Op - bound occurs in probability p; D - feature dimension; N
- sample size; R - privacy budget; Ci - constant.
Algorithm	Schedule (σt2)	Utility Upper Bd.
*GD+Adv (Bassily et al., 2014)	-O( In(Nδδ))-	((D ln3 N ) Ol ^RT)
GD+MA (Wang et al., 2017)	O(怠)	D ln2 N∖ O <N2RZδ
*GD+Adv+BBImp (Cummings et al., 2018)	O (n⅛zδ))	O D D2 ln2 (l/p)ʌ Op I Re,δN1-c
Adam+MA (Zhou et al., 2020)	O(怠)	, O √ √D ln(NDe∕(1-p))∖ Op I	NR0	1
GD, Non-Private	0	O ( NR )
GD+zCDP, Static Schedule	T R	D ln N ∖ O〈 N2R
GD+zCDP, Dynamic Schedule	O (γ(t-T)/2)	O ( NDR )
Momentum+zCDP, Static Schedule	T R	o (NDR(C + ln NIT>T))
Momentum+zCDP, Dynamic Schedule	O ( C1YT + t+∖γ(T-t)/2 )	O (NDR (I + NDRIT>T))
We present Table 2 as an sumpplementary to the Table 1. Asymptotic upper bounds are achieved
when sample size N approaches infinity. Both R and R,δ with R,δ < R are the privacy budgets
of corresponding algorithms. Specifically, Re,δ = e2/ ln(1∕δ) < R when the private algorithm is
(e, δ)-DP with e ≤ 2ln(1∕δ).
PGD+Adv. Adv denotes the Advanced Composition method (Bassily et al., 2014). The method
assumes that loss function is 1-strongly convex which implies the PL condition and optimized variable
is in a convex set of diameter 1 w.r.t. l2 norm.
PGD+MA. MA denotes the Moment Accoutant (Abadi et al., 2016) which improve the composed
privacy bound versus the Advanced Composition. The improvement on privacy bound lead to a
enhanced utility bound, as a result.
PGD+Adv+BBImp. The dynamic method assumes that the loss is 1-strongly convex and data
comes in stream with n ≤ N samples at each round. Their utility upper bound is achieved at some
probability p with any positive c.
Adam+MA. The authors prove a convergence bound for the gradient norms which is extended to loss
bound by using PL condition. They also presents the results for AdaGrad and GD which are basically
of the same upper bound. Out theorems improve their bound by using the recursive derivation based
on the PL condition, while their bound is a simple application of the condition on the gradient norm
bound.
GD, Non-Private. This method does not inject noise into gradients but limit the number of iterations.
With the bound, we can see that our utility bound are optimal with dynamic schedule.
GD+zCDP. We discussed the static and dynamic schedule for the gradient descent method where the
dynamic noise influence is the key to tighten the bound.
Momemtum+zCDP. Different from the GD+zCDP, momentum methods will have two phase of
utility upper bound. When T is small than some positive constant T, the bound is as tight as the
non-private one. Afterwards, the momentum has a bound degraded as the GD bound.
12
Under review as a conference paper at ICLR 2021
Table 3: Comparison of true excess risk bounds. The algorithms are T-iteration 2 R-ZCDP or
equivalently (e, δ)-DP under the μ-strongly-convex condition. The O notation in this table drops
other ln terms. Assume loss functions are 1-smooth and 1-Lipschitz continuous, and all parameters
satisfy their numeric assumptions. * marks the method with convex assumption.
Algorithm	Utility Upper Bd.	T
GD+Adv (Bassily et al., 2014)	O- (√DnN⅛M)	N 2
SVRG+MA (Wang et al., 2017) SSGD+zCDP (Feldman et al., 2020)	O (μN¾)	O(ln ND,δ) O (( √N + √√N ) ln N)	16D/N2+4N
* SGD+MA (Bassily et al., 2019)	O (max {N √De,δ，√N})	min{ {N，N⅛δ}
GD+zCDP, Static Schedule	Oi-P (G ZD MNNRWp) + P))	O(ln NDR)
GD+zCDP, Dynamic Schedule	Oi-P (GN (JDN^ + P))	O(in nDr)
Momentum+zCDP, Static Sch.	O1-p (G (qDInRzp)(C+lnNIτ>τ)+p))	O(InNDR)
Momentum+zCDP, Dynamic Sch.	Oi-p (G (JDlnRZp)(I + NRit>t) + P))	O(InNDR)
GD, Non-Private	O(NR)	OS NR)
GD+zCDP, Static Schedule	O (DNnN)	O(in NDR)
GD+zCDP, Dynamic Schedule Momentum+zCDP, Static Sch.	O (NDR)	O(ln NR) O (NDR(C + ln NIT>T))	O(ln NDR)
Momentum+zCDP, Dynamic Sch.	O (NDR (I + NcDR IT>T))	O(In NDR)
A. 1 Comaprison of generalization bounds
In addition to the empirical risk bounds in Table 2, in this section we study the true risk bounds, or
generalization error bounds. True risk bounds characterize how well the learnt model can generalize
to unseen samples subject to the inherent data distribution. By leveraging the generic learning-theory
tools, we extend our results to the True Excess Risk (TER) for strongly convex functions as follows.
For a model θ, its TER is defined as follows:
___Λ 一	■一 ■ . , . 、/	一	. . ^
TER，Ex〜X[E[f(θ; x)]] - mι□θEx〜X[f(θ; x)],
where the second expectation is over the randomness of generating θ (e.g., the noise and stochastic
batches). Assume a dataset d consist of N samples drawn i.i.d. from the distribution X . Two
approaches could be used to extend the empirical bounds to the true excess risk: One is proposed by
Shalev-Shwartz et al. (2009) where the true excess risk of PGD can be bounded in high probability.
For example, Bassily et al. (2014) achieved a InNN bound with N2 iterations. Alternatively, instead
of relying on the probabilistic bound, Bassily et al. (2019) used the uniform stability to give a tighter
bound. Later, Feldman et al. (2020) improve the efficiency of gradient computation to achieve a
similar bound. Both approaches introduce an additive term to the empirical bounds. In this section,
we adopt both approaches to investigate the two types of resulting true risk bounds.
(1) True Risk in High Probability. First, we consider the high-probability true risk bound. Based
on Section 5.4 from (Shalev-Shwartz et al., 2009) (restated in Theorem A.1), we can relate the EER
to the TER.
Theorem A.1. Let f (θ; x) be G-Lipschitz, and f (θ) be μ-strong convex loss function given any
x ∈ X . With probability at least 1 - p over the randomness of sampling the data set d, the following
inequality holds:
TER(θ) ≤
2G2 PfW-TF) + 与
μN	pμN
(16)
13
Under review as a conference paper at ICLR 2021
where θ* = argmin& f (θ).
To apply the Eq. (16), we need to extend EER, the expectation bound, to a high-probability bound.
Following (Bassily et al., 2014) (Section D), we repeate the PGD with privacy budget R/k for k times.
Note, the output of all repetitions is still of R budget. When k = 1, let the EER of the algorithm be
denoted as F (R). Then the EER of one execution of the k repetitions is F (R/k) where privacy is
accounted by zCDP. When k = log2(1/p) for p ∈ [0, 1], by Markov’s inequality, there exists one
repetition whose EER is F(R/ log2(1/p)) with probability at least 1 - 1/2k = 1 - p. Combined
with Eq. (16), we use the bounds of uniform schedule and dynamic schedules in Section 4.1.3 to
obtain:
■ r	~
TERUmform ≤ o
G2 r /D ln(N) ln(1∕p)
μN ∖∖ NR
+ pH
TERdyi ≤o (, IyDNF+P))
(17)
(18)
where we again ignore the κ and other constants. Similarly, we can extend the momentUm methods.
(2) True Risk by Unfirom Stability. Following Bassily et al. (2019), we Use the Uniform stability
(defined in Definition A.1) to extend the empirical boUnds. We restate the related definition and
theorems as follows.
Definition A.1 (Uniform stability). Let s > 0. A randomized algorithm M : DN → Θ is s-
Uniformly stable w.r.t. the loss fUnction f if for any neighor datasets d and d0, we have:
supx∈x E[f(M(d); X)- f(M(d0); x)] ≤ s,
where the expectation is over the internal randomness of M.
Theorem A.2 (See, e.g., (Shalev-Shwartz & Ben-David, 2014)). Suppose M : DN → Θ is a
s-uniformly stable algorithm w.r.t. the loss function f. Let D be any distribution from over data space
and let d 〜DN. Thefollowing holds true.
Ed〜DN [E[f(M(d); D)- f(M(d); d)]] ≤ s,
where the second expectation is over the internal randomness of M. f (M(d); D) and f (M(d); d)
represent the true loss and the empirical loss, respecitvely.
Theorem A.3 (Uniform stability of PGD from (Bassily et al., 2019)). Suppose η < 2∕M for M
smooth, G-Lipschitz f(θ; x). Then PGD is s-uniformly stable with s = G2 T η∕N.
Combining Theorems A.2 and A.3, we obtain the following:
TER ≤
BecaUse EER in this paper compresses aγT or similar exponential terms, Unlike (Bassily et al., 2019),
we cannot directly minimize the TER Upper boUnd w.r.t. T and η in the presence of a polynomial
form of YT and T. Therefore, We still use T = O(ln NDDR) and η for minimizing EER. Note that
G2 ητ
G2
≤°( Mln
N2R
---)≤ O
D ) ≤
Where We assume N D and use ln N ≤ N. Because the term O G2∕M is constant and
independent from dimension, We folloW (Bassily et al., 2019) to drop the term When comparing the
bounds. After dropping the additive term, it is obvious to see that the advantage of dynamic schedules
still maintains since TER ≤ EER. A similar extension can be derived for (Wang et al., 2017).
We summarize the results and compare them to prior Works in Table 3 Where We include an additional
method: SnoWball Stochastic Gradient Descent (SSGD). SSGD dynamically schedule the batch size
to achieve an optimal convergence rate in linear time.
Discussion. By using uniform stability, We successfully transfer the advantage of our dynamic
schedules from empirical bounds to true risk bounds. The inherent reason is that our bounds only
need ln N iterations to reach the preferred final loss. With uniform stability, the logarithmic T
reduce the gap caused by transferring. Compared to the (Feldman et al., 2020; Bassily et al., 2019),
our method has remarkably improved efficiency in T from N or N2 to ln(N). That implies feWer
iterations are required for converging to the same generalization error.
14
Under review as a conference paper at ICLR 2021
B Preliminaries
B.1	Privacy
Lemma B.1 (Composition & Post-processing). Let two mechanisms be M : Dn → Y and M0 :
Dn ×Y → Z. Suppose M satisfies (ρι, a)-zCDP and M0(∙, y) satisfies (ρ2, a)-zCDPfor ∀y ∈ Y.
Then, mechanism M00 : Dn → Z (defined by M00(x) = M0 (x, M (x))) satisfies (ρ1 +ρ2)-zCDP.
Definition B.1 (Sensitivity). The sensitivity of a gradient query Nt to the dataset {xi}N=ι is
∆2(Vt) = max -1 XN	Vj)--1 XN Vj)	=ImaXl∣v("∣	(19)
n N	j =1,j 6=n	N j =1	2 N n	2
where Vt(n) denotes the gradient of the n-th sample.
Lemma B.2 (Gaussian mechanism (Bun & Steinke, 2016)). Let f : Dn → Z have sensitivity ∆.
Define a randomized algorithm M : Dn → Z by M(x) - f(x) + N(0, ∆2σ21). Then M satisfies
212 -ZCDP
Lemma B.3 ((Bun & Steinke, 2016)). IfM is a mechanism satisfying ρ-zCDP, then M is (ρ +
2pρ ln(1∕δ), δ)-DPfor any δ > 0.
By solving P + 2,ρln(1∕δ) = e, we can get P = e + 2ln(1∕δ) + 2pin(1∕δ)(c + ln(1∕δ).
B.2	Auxiliary lemmas
Lemma B.4. If maxn ∣∣xn ∣∣2 = 1 and N En Xn = 0, then the gradient sensitivity of the squared
loss will be
△2(V) = max = P2f(θ;Xi) ∣Xi∣2 ≤ 1(DM ∣θ∣2 + 1),
iN	2
where ΘM is the set of all possible parameters θt generated by the learning algorithm M.
Proof. According to the definition of sensitivity in Eq. (19), we have
△2(V) = max∣∣V(i)∣∣2 =max1 ∣∣A(i)θ - x』？
where we use i denotes the index of sample in the dataset. Here, we assume it is constant 1. We may
get
∣∣∣A(i)θ -	Xi∣∣∣	= ∣∣Xi(Xi>θ	- 1)∣∣22	=	(Xi>θ - 1)2	∣Xi∣22	=	2f (θ;	Xi)	∣Xi∣22
where f(θ; Xi) = 2(χ>θ — 1)2. Thus,
△2(V) = max Wp2f (θ; Xi) ∣∣Xi∣2
iN
Since ∣∣Xnk2 ≤ 1 and N Pn=I Xn = 0,
1N	1N	1
f(θ) = 2N ∑[(X>θ)2 - 2X>θ + 1] ≤ 2n ∑[(∣Xnkkθk)2 + 1] ≤ 2∙(DM kθk2 + 1)
2N	2N	2
n=1	n=1
□
Lemma B.5. Assume assumptions in Theorem 4.4 are satisfied. Given variables defined in Theo-
rem 4.4, the following inequality holds true:
T
X YTT型广 XtT βt-i IVt-Vik2 ≤
t=1	bt
2M(1 -β)3(γ -β)2
T-1
XγT-i∣vi+1∣2.
i=1
15
Under review as a conference paper at ICLR 2021
Proof. We first handle the inner summation. By smoothness, the inequality ∣∣Vf (x) - Vf (y)k ≤
M kx - yk holds true. Thus,
Xt βt-i∣Vt-Vi∣2≤M2Xt βt-i∣θt-θi∣2
i=1	i=1
=M2Xt-1βk∣θt-θt-k∣2
k=0
=M2 Xk=0 βk ∣Xt=1-k ηiVi+ι∕bi∣2
≤ M2 Xk=0 ββ (Xj=t-k jj (Xt-1-k kVi+1 k2
where the last inequality is by Cauchy-Schwartz inequality.
= = ηo-
ηt	2M,
Because b1 =1-βt ≤ 1⅛ and
Xi=Iβt-i kVt -Vik2 ≤ 4(1⅛Xβ=0ββkXt-I-β kVi+1 k2
= 4(1‰ Xβ=0 βkkX二 M+1『Ni ≥t - k)
= 4(1‰Xt-Ikvi+ιk2Xβ=0ββki(k ≥ …
= 4(1‰ Xt-Ikvi+1k2 Xβ=t-i ββ k	(20)
where I(∙) is the indicating function which output 1 if the condition holds true, otherwise 0.
Denote the left-hand-side of the conclusion as LHS. We plug Eq. (20) into LHS to get
LHS ≤ X YT-iɪ 4” Q Xt-1 kvi+ιk2 X- . ββk
bt 4M(1 - β) i=1	β=t-i
≤	一 X γ T-iXt-ikvi+ιk2 Xt-ι .β"
4M (1 - β)2	i=1	β=t-i
where we relax the upper bound by -1 =匚宗 ≤ ι-β. Using Lemma B.6 can directly lead to the
conclusion:
LHS ≤
η3βY
2M(1 - β)3(γ - β)2
T-1
X γT-i
i=1
kvi+1k2.
□
Lemma B.6. Given variables defined in Theorem 4.4, the following inequality holds true:
T
X γT-t Xti=-11 kvi+1k2 Xtβ-=1t-i kββ ≤
t=1
2βγ
(γ-β)2(1-β)
T-1
XγT-ikvi+1k2.
i=1
16
Under review as a conference paper at ICLR 2021
Proof. We first derive the summation:
UG), Xxzk=X=LXL β
Xt-LX ] β I 叼 ≤ k)
XtT XtT β,
j--*,j=1 4JI=max(t-i,j)
t-1 βmaχ(t-i,j) _ βt
^^j=I	1 - β
τ⅛ Si)βt-i +
占((fβi +
βt-i+1 _ βt	β _ β∖
1 - β 1 _ β )
βt-i+1 _ β A
1 - β )
Now, we substitute U1(t, i) into LHS and replace t - i by j, i.e., t = j + i, to get
T	t-1
LHS = X YT-t X MHI2
t=1	i=1
T-1	T
X U XY
i=1	t=i+1
T-1	T-i
上…t-i+，
T-t 占「+ I
βj+1  β
1 _ β
占(jβj +
∑>i+1k2E YT …
i=1	j=1
Let a = β∕γ, we show
T-i	T-i j
X jaj = XX aj
j=1	j=1o=1
T-i T-i
XX αj
o=1j=o
T-i
X(
o=1
ao  aT-i+1
1_a
)
a - aT-i+1	IE	«-i+1
=(1 - a)2	-(T - i)TZɪ
a
≤ (1 _ a)2 .
17
Under review as a conference paper at ICLR 2021
Thus,
LHS ≤ 1⅛ X1 YT-i kvi+ιk2((1⅛ + i¾ Ea)
≤ 1⅛ X1Y T-i kvi+1k2 ((1⅛ + i⅛ 占)
T-1
≤ 门冬 G X YT-i kvi+1k2
(1-a)2(1- β) i=1
Because γ < 1, β < a = β∕γ and
a β a	2a
(1 — a)2 + 1 — β 1 — a - (1 — a)2 ∙
Therefore,
T-1	T-1
LHS ≤ (1^⅛→) X YT-i kvi+ιk2 = ra⅛ X Y T-i kvi+ιk2
□
Lemma B.7. Suppose Y ∈ (0, 1) and β ∈ (0, 1). Define
1 — β
T = max t s.t. Y 1 ≥ ---—.
1 — βt
If t ≤ T,导 ≤ Yj for t = 1,...,T .If t>T, M < YT-1.
Proof. Define h(t) = Yt-1(1 — βt) whose derivatives are
h (t) = Yt-1(1 — βt)ln Y + Yt-1(—βt)ln β
= Yt-1 lnY — βt(lnY + ln β)
= Yt-1 1 — βt(1 + logγ β) lnY.
Simple calculation shows 1 — βt(1 + logγ β)t=0 = — logγ β < 0 and limt→+∞ 1 — βt(1 +
logγ β) = 1. When t = — logβ (1 + logγ β) denoted as t0, 1 — βt(1 + logγ β) = 0. Be-
cause 1 — βt(1 + logγ β) is monotonically increasing by t and Yt-1 lnY is negative, h0(t) ≥ 0
if t ≤ t0. Otherwise, h0(t) < 0. Therefore, h(t) is a concave function. Because h(1) = 1 — β and
h(Τ) = YT-1(1 — βT) ≥ 1 — β > 0, h(t) ≥ 1 — β for t = 1,...,T. Thus, for all t ∈ [1,T], We
have 1--βt ≤ Yt-1.
^	1 β	1 β 1 β	t-- 1
For t > T , because 1 β"t mιonotonιcaιιy increases by t, We have 1 β"t < "ɪ t^, *≤ Y .
C Proofs
Proof of Theorem 3.1. Because all sample gradient are G-Lipschitz continuous, the sensitivity of the
averaged gradient is upper bounded by G/N. Based on Lemma B.2, the privacy cost of gt is a1.
Here, We make the output of each iteration a tuple of (θt+1, vt=1). For the 1st iteration, because θ1
does not embrace private information by random initialization, the mapping,
v2	= g1
θ2	θ1 — η1g1	,
1For brevity, When We say the privacy cost of some value, e.g., gradient, We actually refer to the cost of
mechanism that output the value.
18
Under review as a conference paper at ICLR 2021
is pι-zCDP where ρι =支.
Suppose the output of the t-th iteration, (θt, vj is ρ⅛-zCDP. At each iteration, we have the following
mapping (θt,vt) → (θt+1,vt+1) defined as
vt+1	=	φ(vt, gt)
θt+1	θt - ηtφ(vt, gt)
Thus, the output tuple (θt+1,vt+1) is (Pt + *)-zCDP by Lemma B.1.
Thus, the recursion implies that (θT+1, vT+1) has privacy cost as
PT+1 = PT +2⅛
T 1	1T 1	1
X 2σ2 =2 XPt ≤ 2(R - RT) ≤ 2R.
t=1	t	t=1
Let ρ = PT +1. Then we can get the conclusion.
□
C.1 Gradient Descents
Proof of Theorem 4.1. With the definition of smoothness in Definition 3.3 and Eq. (1), we have
f (θt+ι) - f (θt) ≤ -ηtV>(Vt + GσtVt∕N) + 1 Mn ∣∣Vt + GσΜ∕N『
=-ηt(1 - 2Mnt) kVtk2 - (1 - Mηt)ηtV>GσtνJN + 2Mn kGσtVt∕N『
≤ -2μnt(i - 2Mnt)(f (θt) - f (θ*)) - (1 - Mnt)ntV>Gσtνt∕N
+ 2Mn2 kGσtνJNk2.
where the last inequality is due to the Polyak-Lojasiewicz condition. Taking expectation on both
sides, we can obtain
E[f (θt+ι)] - E[f (θt)] ≤ -2μnt(1 - Mnt)(E[f (θt)] - f (θ*)) + M(ntGσt∕N)2EkVtk2
which can be reformulated by substacting f (θ*) on both sides and re-arranged as
E[f(θt+ι)] - f(θ*) ≤ (1 - 2μnt(I —2nj) (Ef(Ot)] - f(θ*)) + ~2(ntGσt/N)2D
Recursively using the inequality, we can get
T
E[f (θT+1)] - f (θ*) ≤ Y (1 - 2μnt(1 - Mnt)) (E[f(θι)] - f (θ*))
TT
+—2- X Y (1- 2μni(1 - -2n∙ιj (ntGσt∕N)2.
t=1 i=t+1
Let nt ≡ 1∕M . Then the above inequality can be simplified as
E[f(θT +1)] - f(θ*) ≤ YT(E[f(θι)]-
T
YT(E[f (θι)] - f(θ*)) + RX γT-tασ2(E[f (θι)] - f(θ*))
t=1
(T + R X=I qtσj (f(θι) - f(θ*)
□
19
Under review as a conference paper at ICLR 2021
Proof of Theorem 4.2. The minimizer of the upper bound of Eq. (3) can be written as
T* = arg min YT + ακ(1 一 YT)T	(21)
T
where we substitute σ2 = T/R in the second line. To find the convex minimization problem, we
need to vanishing its gradient which involves an equation like TYT = c for some real constant c.
However, the solution is Wk(c) for some integer k where W is Lambert W function which does not
have a simple analytical form. Instead, because YT > 0, we can minimize a surrogate upper bound as
following
T* = arg min YT + ɑκT =[(])ln (In(I∕γ)) , if Ka + ln γ < 0	(22)
where We use the surrogate upper bound in the second line and utilize Y = 1 一 1. However, the
minimizer of the surrogate objective is not optimal for the original objective. When κ is large, the
term, 一ακγτT, cannot be neglected as we expect. On the other hands, T suffers from explosion if
κ → ∞ and meanwhile 1/Y →+ 1. The tendency is counterintuitive since a small T should be taken
for sharp losses. To fix the issue, we change the form of T* as
T * = —1— ln(l+ ln(1/Y)
ln(1∕γ)	+ α
(23)
which gradually converges to 0 as κ → ∞.
Now we substitute Eq. (23) into the original objective function, Eq. (21), to get
ERUBuniform
1
1 + ln(1∕Y )∕a
1 + κ ln 1 +
In(LY)
α
(24)
Notice that
ln(1∕Y) = ln(κ∕(κ - 1)) = ln(1 + 1∕(κ 一 1)) ≤ -ɪ- ≤ —
κ 一 1 cκ
because K ≥ 占 > 1 for some constant C ∈ (0,1). In addition,
ln(1∕Y) = 一 ln(1 一 1∕κ) ≥ 1∕κ.
Now, we can get the upper bound of Eq. (24) as
ERUBuniform ≤
K
K+ 1∕α
1 + κ ln I 1+———
cKα
K
≤ Ci ------K
1K + 1∕α
2
K
≤ c1c2	, 1 I
K + 1∕α
ln(1 + 工)+ ln(1))
Kα c
ln f 1 + ɪ
Kα
for some constants ci, c2 and large enough 1. Also, we can get the lower bound
ERUBuniform ≥ —cκ- [1 + κ ln(1 +	1 ≥ C K , ln f 1 + ɪ).
CK + 1∕α	Kα K + 1∕α	Kα
where we use the condition C ∈ (0,1). Thus, ERUBUnifOrm = Θ Q+1∕α ln(1 + Ka
□
Proof of Lemma 4.1. By PtT=1 σ-2 = R and Cauchy-Schwarz inequality, we can derive the achiev-
able lower bound as
R X qtσ2 = X σ12 X qtσ2 ≥ (X 小
20
Under review as a conference paper at ICLR 2021
where the inequality becomes equality if and only if s∕σ2 = qtσ2, i.e., σt = (s/qt)1/4, for some
positive constant s. The equality PT=I σ-2 = R immediately suggests √s = R PT=I √qt. Thus,
we get the σt.
Notice
T XX qt - (∑τ=ι √qt) 2 = T2 T XX (√t - T £ √q) = T2 Var[qt]
where the variance is w.r.t. t.
(25)
□
ProofoJ Theorem 4.3. The upper bound of Eq. (3) can be written as
+k
ERUBdyn = YT
T
+ Q
t=1
2
+ Q I
I1 - √γ
where we make use of Lemma 4.1. Then, the minimizer of the ERUB is
arg min YT + α (-----
T	1 — √7
α \
=2 logγ
α + (I - √γ)2
(26)
We can substitute Eq. (26) into ERUBdyn to get
ERUB 鬻
Q + (I - √γ)2√
(Q(I-C2
Q(I-G2 + 1
q(1-√Y )-2
Q(I- √γ)-2 +1
2
Notice that (1 — √-)	= κ2 + κ2 — K + 2κpK(K — 1) = κ(2κ — 1 + 2pK(K — 1)) and it is
bounded by
κ(2κ — 1 + 2∖Jκ(κ - 1)) ≤ 4κ2,
κ(2κ — 1 + 2pκ(κ — 1)) ≥ κ(2κ — (3κ — 2) + 2p(K — 1)(κ — 1)) = κ(-κ + 2 + 2κ — 2) = κ2.
Therefore, K ≤ (1 - √γ) 1 ≤ 2κ, with which we can derive
ERUBmnn
ERUBmnn
K2Q
≤ 4 -	,
K2Q + 1
〉	K2Q	〉1 K2Q
—4k2q + 1 — 4 k2q + 1
ThUs, ERUB鬻=θ (κ⅛).
T
Y
ʌ T
Y
2
α
□
21
Under review as a conference paper at ICLR 2021
C.2 Gradient Descents with Momentum
Proof of Theorem 4.4. Without loss of generality, we absorb the Cσt /N into the variance of νt such
that Vt 〜N (0, Cσ I) and gt JNt + νt. Define b = 1 - βt.
By smoothness and Eq. (1), we have
1
f (θt+ι) - f (θt) ≤ Nt (θt+ι - θt) + 2M kθt+ι - θtk
=-段btV>vt+ι + 1 Mn kvt+ιk2
(IlbtNt - vt+"∣2 -∣IbtNt『一kvt+ι∣∣2) + 2Mn llvt+ιk2
IIbtVt --vt+ιk2^-η l∣Vtk2 - ηt(I- 2Mnt) llvt+ιk2,	(27)
UIIt)
2一斤2一斤
==
where only the U1(t) is non-negative. Specifically, U1(t) describes the difference between current
gradient and the average. We can expand vt+1 to get an upper bound:
U1(t) = IbtVt-vt+1I2
= (1 - β) Xit=1 βt-iVt - (1 - β) Xit=1 βt-igi2
=(1-β)2Xit=1βt-i(Vt-gi)2
=(1-β)2Xit=1βt-i(Vt-Vi)+Xit=1βt-i(Vi-gi)2
≤2(1-β)2	Xit=1 βt-i(Vt - Vi)2 + Xti=1βt-i(Vi-gi)2
≤ 2(1 - β) bt Xi=I βt-i IVt - Vik2 +(1 - β) IXt=I βt-iVi『
|~ɪ-----------------}	|--------------/
U2 (t) (gradient variance)	noise variance
where we use Ix + yI2 ≤ (IxI + IyI)2 ≤ 2(IxI2 + IyI2). The last inequality can be proved by
Cauchy-Schwartz inequality for each coordinate.
We plug the U1 (t) into Eq. (27) and use the PL condition to get
f(θt+ι) - f(θt) ≤ ntU∖(t)-nt IVtk2 - nt(1 - 2Mnt) Ul2
≤ -n IlVt『+ nt 2(1—e)jt U2 ⑴ + (1- β) ∣∣Xi=ι βt-i ViH
-	n(I - 2 Mnt) k%+i『
≤ -2μnt(f (θt) - f(θ*)) + 2(1 -J)nt U2(t) + 2(1 一))2n ∣∣Xt=1 βt-iVi∣∣2
-	n(I - 2 Mnt) k%+i『.
22
- Z=I+la⅛Jvi — 品 T*MI -ss 邕N(GXl)ZTMLM+
S
‘ > ’
((左 T (IBS(苫工¾7M二Mκlz + A H
a Il 2 H
Z = I+la⅛ɪI&t^m1s£¥7li)ztmlm+
2 1=2
τ⅛⅛(1¾JM二」：N+ ((邑 T U6SJVl£7 . U+M62出
^tc Z 2GlJG I
sω=duη AMnbωESjnɔəj OqITye IIU∖oIIU} əjəlp^
Z=I+la⅛Λ—&I .SS邕 N(GXI)Z +
I⅛ 1 ∖M^d⅛ + r i 1巴：
Z=I+la⅛Λ—品 I 一SS 邕N(GyI)Z +
J t WU产” I)Z + ((*B)J — 一 (φ≤出YVlST 」 (I+ 芭 J邕
MoqSuloωdXəuppl PUE SUnəiUEXmωX
IZOZ xu0IaJOdEd əɔuəjəjuoɔ E SE m-aəj Jopun
□
- Z = I+£出TJɪ 啃 I ((*B)J — (IBs((bb≡≡lz + ZVl(*B)Jl -(I+MB2a
I
§0M CG T)mJo uuyωp əip A
∞
1工
I——N
£9
+
}∞

vl0 Il
.0λij°AEqUEɔ°M
CAwoods C.绊 20U- qoβnou5 ɪɪBulS E UOqAV
(0 — I)~0 — £
oJoqΛ∖
vlss
I——N
.IΛI七Xq .SnqI
(！d)JVZVlLr .
g∞s≡三二WgH
OAEq əmlxmEUIuIəu酉
∙⅛ H ⅛ PUE (*6)』”16)』x^°^H d əzəln əM əjəlp^
s
Under review as a conference paper at ICLR 2021
ProofoJ Theorem 4.5. Since σt is static, by definition of U3 in Theorem 4.4,
t=1
YT-t I Xi=I β2(I)σ2
Because
1-β 1+βt
。工
t=1
Tr-t
σ2∑-
t=1
YT-t
。工
t=1
YT-t
(1 -⑶2 Xt	β2(t-i)
(1 — βt)2 N^i=I
(1 - β)2 1 - β2t
(1 - βt)2 1 - β2
1 - β 1 + βt
1 - βt 1 + β .
≤ 1, the U3 will be smaller than the corresponding summation in GD with
U3 =£；
T
T
T
T
1-βt 1+β
uniform schedule.
By Lemma B.7, when T > T,we can rewrite U3 as
U3 ≤ σ2k
σW
't=1
T
≤ σW
't=1
T
t=1
Y T-t
1-β
1 - βt
1 - β + σ2 X
1 - βt + M
YT-tYt-1 + 0 工
σ2YT-1T + σ2YT-1 X
T-T
t=1
ʌ
t=T+1
t=T+ι
Y T-tγTT
一 ^
Y T-TT
σ2YT-1T +
σ YTT-YT T
1 - Y
T-T - 1∖
1 - Y )
where we use σ2 = T/R in the last line. Without assuming T > T,we can generally write the upper
bound as
t t	YT-T -1 Λ
U3 ≤ TRY I mιn{T, T} + max{，1- Y 7 0}卜
By Theorem 4.4, because Z ≥ 0, we have
ERUB ≤ YT + 2Rη0αU3
&	YT-T - 1	∖
=YT(1 + —T (min{T, T} + max{ 7 ] - Y , 0}))
where a = 2η0 ɑ.
First, we consider T ≤ t Use T = ιn⅛γ) In (1 + 悬)=[O (京ln (1 + 悬
ERUB ≤
-ɪ )(1 + α0Y-1(	2^ In (1 + 也
Q + ηo∕κ∕ ∖	ln(1∕Y)	KQ
α ∖ Λ	8κ2 Q 1 2 (
-Ξ ~T ) ( 1 + ln (1 +
Q + ηo∕κ∕ 1	η0Y
( K	8κ2Q , 2 /
(-v τ~ (1 + ln (1 +
IK + ηo∕α	η0 Y
(K+⅛∕α (T))
(κ2	)
∖K + no/ɑ/
))]to get
2
T
Y T-t
T
YT-t I
T
?t+T
≤
≤
O
O
O
24
Under review as a conference paper at ICLR 2021
where we used ln(1∕γ) ≥ ηo∕κ and ln(1 + x) ≤ √x for any x > 0.
Second, when T > T,
4
erub ≤ Yt (1+Q0t t+「)
≤O (YT + 2α0 TK(YT - yt )).
Make use of T = ∣^ɪnʊ1^ ln (1 + 悬)]to show
ERUB ≤ O
K
K + ηo∕α
2
K
K + ηo∕α
4κ2α / 小
+F(Y
YT-I ln (1 +
―^ʃ )ln (1 + 也
K + ηo∕α ∖ KQ
—
≤ O
□
ProofoJ Theorem 4.6. By Lemma B.7, we can rewrite U3 as
t=1
t=1
yt T — Xt=1 炉(I) σ2
t
YT-tY2(tT) Xi=1 β2(t-i)σ2 + X
t=T+ι
YT-tY2(TT) Xt	β2(t-i)σ2
t=1
{Z
Vi
We derive Vi and V2 separately.
HI Xt=1 β2(t-i)σ2 +Y：
--_ 一	/
,2(T-I) X
乙、
t=T+ι
Yt-t Xi=1 β2(t-i)σ2
一___ 〃
U3 = T
T

T
T
≤ YT-TX
、 ，
T
T
{z
½
For Vι, we can obtain the upper bound by
%=k
t=1
YT-tY2(T) Xt 1 β2(t-i)σ2
i=1
T
For V2, we can derive
T
T
ʌ
YTp-2
t=1
Yt X∖β2(I)σ2
i=1

*
YTp-2
Φ	rΓ	.
Xi=i β-2iσ2 Xt=i")t
ʌ
YT-2
T
Xi=1 β-2iσ2
^
(γβ2)τ+1
Y2T-3X
'i=1
y2t-3X
'i=1
≤ γ2(i - γβ2)
≤ Y(Y-俨)
Yi-T-I - ∕2(T+1-i)
1 - Yβ2
- ^
1 - (γβ2)T+1-i
1 - γβ2
^
Xi=IYi 蟾
Σ..
'i=1
σ2
〜i—T-1丁2
Y	σi
T
YT
T
YT
—
1 - γβ2
T
T
T
γiσ2
V2 = T
t=T+ι
Yt-t Xt=1 β2(t-i)σ2
't=1
T
yt-tXi=1 ―蟾-X
、T
t=1
YT-t Xt=1 β 2(F
t=1
YT-tXi=i β
-A
4t-i)σ2 - yt-TE
t=1
YT-t Xt 1β2(t-i)σ2.
i=1
Σ
T

T
25
Under review as a conference paper at ICLR 2021
We first consider the first term
__τ _	__t ,
X」-EI β2(I)蟾
__T _____T
X. ισ2 X QT-tβ2(I)
iyi=1	tyt=^
__T _	__T
Xi=IYT Lσ XfLe
T 〜T β-2ifτ2 (β2∕γ)i - (β2/γ)τ +1
P	i —1 - (β2∕γ)一
∖ ∖T YT +1-i - p2(T +1-i)
2^i=1	Y — β2
Similarly, we have
YT-T X
t=1
YT-Xt=1 β,
,2(t-i) σ2
Thus,
ʌ __
YT-T X,
'i=1
ʌ _ , ʌ .
YT+1— i - β2(T+1- i)
Y - β2
σ2
一 一 ʌ _ , ʌ .
XT yt +1-i - yt-tp2(t+1-i) 2
A^i=I	Y - β2	σi
T
T
一 __________ . 一 一 ʌ , ʌ .
LT Yτ +1-i - β2(T +1-i) 2 LT Yτ +1-i - Yτ-t β2(τ+1-i) 2
V2 = ^i=1	Y-β2	σi - ^i=1	Y-β2	σi
, . ʌ ʌ .
=XT	YT +1-i-β2(T +1-i) σ2 + XT YT T -β2(T T)β2(T+1-i)σ2
=^i=T+1	Y - β2	σi + 2^i=1	Y - β2 β	σi
^
T—T
Y
T
≤ Ei=T+1
YT +1—i - β2(T +1—i) 2	「T
Y--I2	σi + ‰=1
Y - β2
β2(τ+1-i)σ2
Substitute H and V2 into U3 to get
U3 ≤ YT： XTIYE + Y2T-2 XT=τ+1
YT +1- i - β 2(T +1- i)
σ2
Y - β2
+
T
i=1
一 ʌ
YT+T-2
Y - β2
β2(τ+1-i)σ2
≤ (Y(⅛ Xτ=1(Yi + Yτ-1β2(T+1—i))σ2 + Y2τ-2 E
T
'i=T+1
Y - β2
YT +1-i - β2(T +1 -i)
≤
(2YT XT ,σ2 + 2T-2 XT YT +1-i- β2(T +1-i) σ2
(Y(Y - β2) Ti=IY i Y ki=τ+1	Y - β2	i
XT=1 qtσ2
where
2	ʌ 、yT +1-i _ β2(T +1-i)
==__________勺T+tι ʌ + 勺2(T-1) 2_________E_______勺T-tι
qt= γ(γ - β2)Y IT ≤τ + Y	Y - β2	Y I
≤ C1YT+tIτ≤T + YTTC2YT-tIτ>T
C	2T
where C1 = γ(γ-0) and C2 = γβ2 .
26
Under review as a conference paper at ICLR 2021
When T > T, by Lemma 4.1, the lower bound of R PL q⅛σ2 is
T
Y
≤O C2
2
I _ ≠r-T-1)/2、
+ √2	√γ -1
+ XT=T+1 ,7t j2γ-t)
1 _ YT 2	Y(T-T-1)/2 _ 1∖ 2
+ √2 J~尸一
1	- √γ	1 - √γ	J
Yt(TT)/2 _ γT∕21∖
I 1 -√γ	ʃ)
which is achieved when
By Theorem 4.4, because Z ≥ 0, we have
ERUB ≤ γτ + 2Rη0QU3
=γτ + 2η0Q χT=ι R务 σ2∙
And the minimum of the upper bound is
ERUBmin = Tt + Q0O ({ T(T-11-2√TTT"))
where q0 = 2η0c2 q. Let T
ln(l∕7) ln (1 + 悬).Then,
ERUBmin = O ((E)2 +
Q0
T(TT)/2(1 — τ (TT)/2)kq12
(1 _ √t)2
KQ + ηo
(
≤O ((")2 +
KQ + η0
2η0C2Q ( T(TT)/2 )
(I 一 √γ)2 IKQ + η0 ʃ
KQ
(KQ + ηo)2
KQ
(KQ + ηo)2
KQ
.l 2η0 c2∣k (T-I)
kq +(T_7T)2 T
(KQ +
_	∖kq + η0
where eɜ is some constant.
When T ≤ T,
U3 ≤ Tt-Tk
、 ，
t=1
Tt-tT2(I) X∖β2(I)σ2
/⅛=1
_ - -
{z
Vi
≤ i¾工
with which we obtain
'i=1
Yiσ2
ERUB ≤ TT + 2Rη0QU3
≤ TT + 2ηoα 二. XT=I R务σ2∙
O
O
≤ O
T
T
27
Under review as a conference paper at ICLR 2021
where we let qt = YT+t. By Lemma 4.1,
Thus,
YT T
ERUBmin ≤ YT 十2〃。。「
1 + 2ηoγcιa
T
Y
Let T = l ln(l∕7) ln (1 + 悬f| . Then,
ERUBmin ≤
Ka Y (ι + 2ηoγcιa( 1	)2
Ka + ηo	(1 - √γ)2 Ka + 1
≤ ( K ) (1 + O(^T)
∖κa + ηo J ∖ Ka + 1
≤ O ( Ka ).
IKa + ηo J
In summary,
ERUBmin ≤ O (^Ka + ηo
Ra
I	+ it>t
Ka + ηo t;t
□
C.3 Stochastic Gradient Descents
Proofof Theorem 4.7. Let ▽ t be the stochastic gradient of the step t. By the smoothness, we have
f (θt+ι) - f (Ot) ≤ -ηtV>Nt + GQtVtln) + 2Mη2 (十t + Gσtνt∕n∣∣
=-ηtV>(Vt + σgξt∕n + GQtUt/n + 1 Mηt ∣∣Vt + σgξt∕n + Gσt%∕M∣2 .
Note that E(σgξt∕n + GσtVt∕n) = 0 and E(σgξt∕n + GσtVt∕n)2 = σg + (Gσt∕n)2. Without loss
of generality, we can write σgξt + Gσtνt as σtζt where σt，Q<j+ + (GσJ2 and Zt is a random
vector with EZt = 0 and EIIZt『≤ D. Therefore,
f(θt+ι) - f (Ot) ≤ -ηtV>(Vt + σtζt∕n) + ∣Mη2 ∣Vt + 力却加『
=-ηt(I- 2Mηt) kVt『-(1 - Mηt)ηtV>σtZt/n + 2Mη1 ∣∣σt金/«■『
≤ -2μηt(1 - 2Mηt)(f(Ot) - f(O*)) - (1 - Mηt)ηtV;σtζt∕n
+ 1Mη ∣σtζt∕n∣∣2.
28
Under review as a conference paper at ICLR 2021
Then following the same proof of Theorem 4.1, we can get
E[f(θτ +ι)] — f(θ*) ≤ YT(E[f(θι)] — f(。*)) + R0 XX YT-tαG12σ2(E[f (θι)] - f(θ*))
t=1
T1
=YT + Rf YT-tα(GG2σg+ 蜡)(E[f(θι)] — f(θ*))
t=1
11T	T
=YT + RaGσgT-Yr + R0 X YT-tασ2 (E[f(θι)] — f(θ*))
Y	t=1
≤ YT + R⅛aσg+R0XXYT-taσ1 (E[f(θι)] — f(θ*)).
G2
t=1
where RG2ɑ = 2”(f(θιD-f(θ* )) n2 = 2”(f(θιD-f(θ*)) min{ N2R, 1} ≤ 2μ(f(θιD-f(θ*)) N2R .
Proofof Theorem 4.8. Without loss of generality, We can write σgξt + Gσtνt as σtZt where σt，
σg2 + (Gσt)2 and ζt is a random vector with Eζt = 0 and E kζtk2 ≤ D. Therefore, we replace νt
by Zt and σ2 by σ2∕G2 = σg/G2 + σ2. Now, we only need to update U3(σ, T) as
=ɪ XT yt-t(I 二β2 Xt β2(t-i)σ2
G2 Lt=J (1 — βt)2 乙i=ιβ i
=XT T--t (I 二 β) Xt	β2(t-i)
=乙t=1 y	(1 — βt)2 乙i=1 β
= U3g + U3
where we define
Ug，ɪσ2 XT T--t (I - e)2 Xt β2(t-i)
3 , G2gLt=J	(1 — βt)2 乙 i=ι β
We can upper bound U3g by
1
G2
1
σg2
2 22
一 GGGI-G
=≤≤
σg2
σg2
T
t=1
T
t=1
T
t=1
YT-t
YT-t
(1 — β)2 1 — β2t
(1 — βt)2 1 — β2
1 — β 1 + βt
1 — βt 1 + β
YT-t
σ2	1
g 1 — Y
=G呜
Combine with the factors of U3 in the PGD bounds:
αR0
aRU3 ≤ aR行
αR0 2
不κσg
Dσg
Dσg
2μn2(f(θι) — f(θ*)) ≤ 2μN2RW1F TW .
□
D Experiments
Dataset. (1) Synthetic data. We generate a 100-dimensional dataset including linearly separable
data points using sckit-learn package. The data points are distributed in two points in the
hyper-cubic with Euclidean distance of 10. In total, 1000 samples are generated for training with the
29
Under review as a conference paper at ICLR 2021
logistic loss. (2) Real data. We create a subset of the MNIST dataset (Lecun et al., 1998) including
1000 handwritten images of digit 3 and 5 (MNIST35). Compared to the original dataset (70, 000
samples), the small set will be more vulnerable to attack and the private learning will require larger
noise (see the 1/N factor in Eq. (1)). Following the preprocessing in (Abadi et al., 2016), we project
the vectorized images into a 60-dimensional subspace extracted by PCA.
Setup. The samples are first normalized so that PnN=1 xn = 0 and the standard deviation is 1. Then
the sample norms are scaled such that maxn kxn k = 10 (i.e., data scales). We fix the learning rate
to 0.1 based on the corresponding experiments of non-private training (same setting without noise).
The total privacy budget is 0.1963-zCDP (equal to (4, 10-8)-DP) which implies R = 0.3927. To
control the sensitivity of the gradients, we clip gradients by a clipping norm fixed at 4. Formally, we
scale down the sample gradients to length 4 if its norm is larger than 4. Because the schedule highly
depends on the iteration number T, we grid search the best T for compared methods. Therefore, we
ignore the privacy cost of such tuning in our experiments which protocol is also used in previous
work (Abadi et al., 2016; Wu et al., 2017). All the experiments are repeated 100 times and metrics
are averaged afterwards.
0.30
0.25
0.20
0.15
0.10
0.05
--t-10
一t-60
--t-90
0
25	50	75	1∞ U5 150 175 200
2.0
1.5
1.0
0.5
0.0
data scale-1
data scale-5
data scale-10
data scale-15
O 20	40	60 BO IOO
UO
data scale-1
18	scale-5
data scale-10
—data scale-15
BO
8 «
40
20
O -------------------
0	5	10	15	20
data scale
ss。- -euu UeWE
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
data scale
ie-3	le4
-1.6
-1.4
-1.2
-10I
-0∙β §
-0.6 J
-0.4
-0.2
---t-10
---t-60
---t-90
O 20	40	60 BO U»
0.28
0.26
0.24
■teta scale-5
O 20	40	60 BO IOO UO 140
t
2.5	5.0	7.5	10.0 L2.5 15.0 17.5 20.0
data scale
ss。- -eu≈UeWE 3>qJ53J
15‰
_ _
Figure 3: Experiments with Logistic loss on synthetic data and squared, Logistic loss and DNN on
MNIST35 by rows. (left) The final loss is fitted by a quadratic function formulated as c2σt2 + c0 .
(middle) The influence values are estimated using the Hessian eigenvalues (squared loss) and by
retraining (logistic loss). The larger is the data scale, the larger the influence variance. (right) The
relative final training losses versus the data scale where uniform schedule (uni), dynamic schedule
(dyn) and exponential-decaying schedule (exp) are compared. The relative loss is computed w.r.t. the
losses of the uniform schedule. For example, if the dynamic loss is e1 and the uniform loss is e0, then
the relative loss is (e1 - e0)/e0. The dashed lines are with the right axis to present influence-related
term.
30
Under review as a conference paper at ICLR 2021
Estimate of influences by retraining. In the left pane of Fig. 3, we estimate the influence of σt by
retraining multiple times. When keeping σi for i 6= t fixed (as the fine-tuend uniform schedule σ),
the value of σt is varied from 20 to 200. Then we fit a quadratic function of σt where the coefficient
is treated as estimation of qt. Repeating the estimation for all t in range 1 to 100 could provide us the
trend of influence in the middle pane.
Results. In the left pane of Fig. 3, the squared final loss (i.e., f(θT+1)) is approximately a
quadratic function of the σt with small relative variance (i.e., the variance divided by the mean
value, 0.14, 0.032, 0.016 for t = 10, 60, 90, respectively). When σt increases, the frequency of
clipping increases as well which leads to more variance in the final losses. We use the least square
method to fit the relation shown as the solid lines. The final logistic loss is more sensitive to the noise
because of the additional uncertainty from the changing Hessian. We still fit a quadratic function
on σt . It turns out the relative variances of the quadratic coefficient (approximately the influence) is
small which are 0.025, 0.024, 0.027 for t = 10, 60, 90, respectively.
The middle pane shows the relationship between the estimated qt and learning steps. The qt of
squared loss is computed by analytical solution using the Hessian eigen values. The qt of logistic
loss is computed by retraining based on uniform schedule. Both loss functions show an increased
influence as learning continues, which indicates that the dynamic schedule should be decreasing
accordingly. The squared loss has a rather steep trend while the logistic has a relative flatten one.
The reason is that the logistic loss has a larger variance in the gradient norm and therefore clipping
happens more frequently (approximately more than 80% gradients are clipped). As a result, the
variance of influence will be relative small for logistic losses. Moreover, we vary the data scale (scale
all samples uniformly such that all sample norms are less than a specific value), changing the variance
of influence, as seen in the figures.
The last pane compares uniform, dynamic and exponential decay schedules Yu et al. (2019) using
final losses relative to the uniform schedule. We set the exponential decay schedule to approximate
the dynamic schedule by fitting σt = σ0 exp(-kt) using the least squares method. The final losses
are picked by grid searching the best T ∈ [1, 100]. We see that the advantage of the dynamic
schedule over the uniform one increases when data scales less than 15. But we also notice some loss
gaps decrease, suggesting that the data scale is not the inherent reason for the dynamic advantage.
According to Lemma 4.1, the advantage should be proportional to T2 Var(qt) which is shown to be
decreasing when the data scale is larger than 15. When the data scale continues to increase, gradient
clipping will change the curvature of the loss function. Therefore, a increasing Var(Tqt) is witnessed.
Meanwhile, the loss gap decreases.
In the last row of Fig. 3, the DNN is experimented with the same setting. Though the influence
increases by t, the variance is small and less dependent on the data scale in comparison to shallow
models. Though the dynamic schedule estimated by retraining influences does not performs stably,
the exp method still decreasingly depends on the variance of influence as expectation.
31