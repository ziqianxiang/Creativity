Under review as a conference paper at ICLR 2021
Conditioning Trick for Training Stable GANs
Anonymous authors
Paper under double-blind review
Ab stract
In this paper we propose a conditioning trick, called difference departure from
normality, applied on the generator network in response to instability issues dur-
ing GAN training. We force the generator to get closer to the departure from
normality function of real samples computed in the spectral domain of Schur de-
composition. This binding makes the generator amenable to truncation and does
not limit exploring all the possible modes. We slightly modify the BigGAN archi-
tecture incorporating residual network for synthesizing 2D representations of au-
dio signals which enables reconstructing high quality sounds with some preserved
phase information. Additionally, the proposed conditional training scenario makes
a trade-off between fidelity and variety for the generated spectrograms. The exper-
imental results on UrbanSound8k and ESC-50 environmental sound datasets and
the Mozilla common voice dataset have shown that the proposed GAN configu-
ration with the conditioning trick remarkably outperforms baseline architectures,
according to three objective metrics: inception score, Frechet inception distance,
and signal-to-noise ratio.
1	Introduction
Generative models have been widely used in several audio and speech processing tasks such as
speaker verification (Reynolds et al., 2000), enhancement (Chehrehsa & Moir, 2016), synthesis
(Raitio et al., 2010), etc. In the last few years, many generative adversarial network (GAN) archi-
tectures have been introduced (Bollepalli et al., 2019; Sriram et al., 2018; Hosseini-Asl et al., 2018;
2019) and they have contributed for tackling such challenging tasks. Furthermore, GANs have been
employed in high-level data augmentation for supervised, semi-supervised, and unsupervised au-
dio and speech classification (Hu et al., 2018; Donahue et al., 2018). In augmentation with paired
transformations, cycle-consistent GANs have been developed for environmental sound classification
(Esmaeilpour et al., 2020b) as well as for more sophisticated tasks such as voice conversion (Fang
et al., 2018). A typical GAN runs a minimax game between generator and discriminator networks,
where the latter should distinguish the real sample from the generated example. Following such
a baseline GAN, many other variants in terms of similarity metric, loss function, and architectures
have been introduced such as the Wasserstein GAN (Arjovsky et al., 2017), GANs with least squares
loss (Mao et al., 2017), attention-based architectures for adversarially training (Zhang et al., 2019),
etc. However, training and tuning such advanced GAN models have always been a mounting con-
cern and a difficult challenge due to their instability in training (Salimans et al., 2016; Arjovsky &
Bottou, 2017; Yang et al., 2017).
Our main contribution in this paper is in response to this issue. We propose to condition the gen-
erator for minimizing dissimilarity among generated and real samples using a constraining metric,
called difference departure from normality (DDFN), computed in the spectral domain of Schur de-
composition (Van Loan & Golub, 1983). This binding improves stability of the generator and does
not limitate exploring all the possible modes during training. Without loss of generality, our focus in
this paper is evaluating the performance of the DDFN metric in comparison with other regulariza-
tion approaches on yielding more stable GANs for synthesizing high quality audio/speech signals.
However, we also report some results on image generation in Appendix A.
As a common practice in audio/speech processing domain, which often uses 2D representations
(spectrograms) over 1D signals (e.g., DeepSpeech (Hannun et al., 2014) and Kaldi (Povey et al.,
2011) systems), our synthesis approach is also based on spectrograms. Technically, spectrograms
are generated from the magnitude of a time-domain signal, while its phase information is discarded
1
Under review as a conference paper at ICLR 2021
(Lang & Forinash, 1998). Spectrogram either from short-time Fourier transformation (STFT) and
its variants such as Mel-frequency cepstral coefficients (Hossan et al., 2010) or discrete wavelet
transform (DWT) is a standard and common signal representation in signal processing due to its
lower dimensionality which densely encodes local frequency information (Mallat, 1999; Rioul &
Vetterli, 1991). Therefore, algorithms require much less training parameters on spectrograms and
advanced architectures developed in the computer vision domain can be easier generalized to them
compared to 1D signals. Signal synthesis with spectrograms also enquires phase matrices. Since our
main goal in this paper is synthesizing high quality signals with less audible noises, we do not use
any phase vector approximation algorithm. Instead of that, we use original phase vectors derived
from real samples available in the datasets. Therefore, for synthesizing high quality signals, we need
to synthesize high quality spectrograms.
For objectively measuring the impact of our conditioning trick using the DDFN metric on the base-
line models and our slightly modified BigGAN architecture (Brock et al., 2019), we compute both
the inception score (IS) (Salimans et al., 2θ16) and Frechet inception distance (FID) (HeUseI et al.,
2017). Then, we reconstruct audio signals with some preserved original phase information and mea-
sUre the signal-to-noise ratio (SNR). OUr experimental resUlts on two environmental soUnd datasets
and on a sUbset of Mozilla common voice dataset have shown considerable improvement in gener-
ating diverse signals with high fidelity. This paper is organized as follows. In Section 2, we review
some related works and in Section 3, we explain theories for constraining the generator. We provide
oUr experimental resUlts and associated discUssions in Section 4.
2	Background
The generator network G(z; θg) commonly learns to map from a random probability distribUtion
Pz 〜N(0, I) or U[-1,1] to the generator probability domain Pg for Z ∈ Rdz and the discriminator
network D(x; θd) should maximize EZ〜Pz(Z) [log (1 - D(G(Z)))] against the generator as defined
in (1) (Goodfellow et al., 2014).
minmaxEx〜pr(x) [log D(x)] + EZ〜Pz(Z) [log(1 - D(G(Z)))]	(1)
where θg and θd denote the training parameters of the generator and discriminator networks, respec-
tively. These networks are often modeled by convolutional neural networks with different archi-
tectures (Radford et al., 2015). The optimization problem of (1) not only requires carefully-tuned
hyperparameters, but also is very unstable and often collapses during training (Salimans et al., 2016;
Thanh-Tung et al., 2019). Generally, there are two approaches to address this issue. First, changing
the optimization functions mainly for the generator (Fedus et al., 2017; Zhang et al., 2019; Karras
et al., 2018; Dumoulin et al., 2016; Nowozin et al., 2016; Salimans et al., 2016; Chen et al., 2016;
S0nderby et al., 2016; Odena et al., 2017). Likewise, changing the similarity metric from Jensen-
Shannon divergence (JSD) to Wasserstein loss (Arjovsky et al., 2017) and Pearson χ2 divergence in
least-square GAN (Mao et al., 2017) have also been proposed. Second, constraining the discrim-
inator network to provide meaningful gradients everywhere to G(Z; θg) (Mescheder et al., 2018;
Miyato et al., 2018; Gulrajani et al., 2017; Kodali et al., 2017).
There are two approaches that are relevant to our work for improving stability in GAN training.
The first one is focused on bijective mapping between Pg and real sample distribution Pr using
another network such as an autoencoder (Donahue et al., 2016). This ensures a correlation between
generated and real samples through a regularization function similar to min kZ - Rec(G(Z))k2 +
H(Z, Rec(x)) where Rec denotes the reconstructor network (autoencoder) and H is the entropy loss
(Srivastava et al., 2017). Besides, some variational autoencoder schemes have been also proposed for
avoiding instability (Kingma & Welling, 2014; Rezende et al., 2014). The effectiveness of explicit
regularization of loss functions with autoencoder loss has been studied by Che et al. (2016). They
have introduced several costly metrics for estimating modes and enhancing quality of the generated
samples. A similar metric for encoding random samples from Pr to Pg has been developed by
Larsen et al. (2015). Both of these approaches incorporate L [x, G(Rec(x))] which is a pixel-wise
loss followed by another regularization term. The second approach is spectral normalization (Miyato
et al., 2018) which conditions the discriminator to support Lipschitz continuity using singular value
decomposition. This approach regularizes θd (the weight matrix of the discriminator) towards the
direction of the top rank (first) singular value. Inspired by Zhang et al. (2019), it has been shown
2
Under review as a conference paper at ICLR 2021
that this regularization can be more effectively implemented for G(z; θg) as (Brock et al., 2019):
θg = θg - max(0, σ0 - σclamp)v0u0	(2)
where θg denotes the weight matrix of the generator and v0σ0u0> forms the first basis matrix of
θg after decomposition. The threshold σclamp can be set to a predefined value (which implies an
additional hyperparameter) or σclamp = sg(σ1); where sg stands for the stop-gradient operation.
This normalization can be generalized to other subsequent singular values, efficiently computed by
Alrnoldi method (Golub & Van der Vorst, 2000).
While instability is a common problem when training GANs with any dataset, it is critical for 2D
representations of audio and speech, mainly due to the properties of Fourier or wavelet transforms
(Esmaeilpour et al., 2020b). In the next section, we explain how to control the generator for produc-
ing high fidelity spectrograms through correctly conditioning it.
3	Conditioning the Generator
In light of correlating pg to pr and spectral normalization in response to the instability issue in
adversarial training, we condition G(z; θg) in another spectral domain to enhance the stability and
also in order to provide better gradients everywhere to the discriminator. Our conditioning trick
is fundamentally different from employing an autoencoder or regularizing either θg or θd through
normalizing their singular value(s). Instead of normalizing the top rank eigenvalues of θg in spectral
normalization (Zhang et al., 2019), we asymptotically correlate pg to pr in such a way that it forces
the generator to follow Schur spectral distribution pr. In fact, we force the generator to get closer to
the departure from normality function computed for the original spectrograms in pr defined in the
spectral domain of the generalized Schur decomposition (Van Loan & Golub, 1983). This binding
makes the generator amenable to the truncation trick (Brock et al., 2019) and the discriminator might
converge in fewer iterations. Additionally, it neither limits spanning the entire possible modes nor
loses sample variety.
Lemma. For an input sample (spectrogram) xi from a given distribution, there exists a unitary
representation Q ∈ Cn×n in such a way that (Golub & Van Loan, 2012):
QHxiQ = V + S	(3)
where QH denotes the conjugate transpose of Q in vector space of Schur decomposition, S =
{si | i = 0 : n - 1} ∈ Cn×n is an upper triangular matrix, and V = diag(λ0, λι, ∙∙∙, λn-ι) Con-
tains eigenvalues of xi (λi denotes an independent eigenvalue for xi). In this unitary vector space,
Which might also yield a quasi-upper triangular representation for S, Q = [qο | qι | q2 | …| qn-ι]
provides the pencil of -→qi - λi q--i+→1 for i ≤ n - 2 which is also known as basis vector for xi . Ac-
cording to the perpendicularity of the achieved pencils and the support matrix S, We Write (Golub &
Van Loan, 2012):
n-1
xiqk ≈ λkqk +	sikqi, k = 0 : n - 1.	(4)
i=0
Therefore With the general assumption of quasi-upper triangular subspaces With the normal span of
{qο, qι,…，qk} for k = 0 : n - 1, we can conclude that the choice of S should be independent of
Q (Golub & Van Loan, 2012). Accordingly, We can compute its Frobenius norm using λi as:
n-1
kskF = kXikF - X ∣λi∣2≡ ∆2(Xi)	⑸
i=0
where ∆2 is known as departure from normality (DFN (Golub & Van Loan, 2012)). For ensur-
ing the correlation between Xr and Xg randomly drawn from pr and pg in their designated vector
spaces (span of qis), we should expect ∆2 (Xj ) - ∆2 (Xi) < for a small enough . Such a
condition which we refer to as difference departure from normality (DDFN) also ensures the con-
sistency of corresponding pencils and contributes to the generalized form of Schur decomposition
with QkH Xr Zk = Rk(Vk + Sk) and QkH Xg Zk = Rk(Vk + Sk), where Rk = QkH (Xr Xg-k1 Qk) and
Zk is also unitary and supports for limi→∞(Qki, Zki) = (Q, Z) (Golub & Van Loan, 2012). The
intuition behind exploiting these basis vectors is providing pencils of X-→ri - λiX-→gi for learning the
3
Under review as a conference paper at ICLR 2021
original distribution pr by the generator. The derivable pencils are not necessarily normal in the
span of their associated subspaces, however, their linear combination imparts pr to pg (in the closed
form). Furthermore, diagonal values in V constitute coefficient of basis vectors (sub-pencils in their
manifolds) and represent local properties of the given input sample.
Proposition. The DDFN metric in the form ∆2 (xg) - ∆2 (xr) < ensures the correlation of
Xg 〜Pg and Xr 〜Pr in the spectral domain with a measurable error term e.
Proof. Since ∆2(∙) is differentiable in its designated subspaces, we can find an upper bound for
. For all xi := max(diag(Si)) we assume gr (xi) = ∆2 (Xi) ∈ Cn+1 with degree n + 1 and
gg(xi) = ∆2(G(zi)) ∈ Cn with degree n are differentiable over the interval [$inf, $sup], therefore
we can approximate the error function as (Phillips, 2003):
g g g g	g g	grn+ι)(ξ) YYz x	/c
e(x) = 9r(x) - 9g,n(x) = (n +1)! ∣∣(χ - Xi)	(6)
where ξ ∈ (Binf ,$Sup) With the marginal condition using the second derivative ®(x)| < % for
0 ≤ % << 1 and n = 1 we write:
「、	(	、/	、gr(ξ)	「、	/	、
gr(χ)-gg,ι(χ) = (X - Xi)(χ - Xi+ι)	⇒ gf (x) = 2χ-(χi+χi+ι) = 0 ⇒
'------------------} 2!
Xi + Xi+1
X =	2
{z^^^
gf (x)
xi+1P)2⇒∣e(x)l≤ 增(Xi+1-Xi)2 口 ⑺
28
Xi + Xi+
gfl
Herein, ∣e(χ)∣ measures how the dynamic ∆g(∙) is close to the static △") and equivalently mea-
sures the correlation between Pg and Pr . We experimentally demonstrate that minimizing the gener-
ator G(z), subject to:
IEz~pz(z)∆2(G(Z))- Eχ~pr(χ)∆2(x)∣ < e	(8)
yields high quality spectrograms and improves stability.
3.1	Fast DFN Approximation
Computing Pn=1 ∣λ∕2 as defined in (5) can be computationally prohibitive since it involves re-
cursive multiplication of pencils. In response to this issue, fV (λi) can be defined as a polynomial
function originating from the absolute product of eigenvalues (Edelman, 1993; Van Loan & Golub,
1983):
n-1	n-1 n-1	n-1
X ∣λi∣2 = X(∏ fv (λi)∏ ∣λi- λj | |_P>2/} )	(9)
i=0	i=0 \i=0	i=j	Normal distribution/
where it imparts a normal distribution and closely relates to the joint density function of the eigen-
values fora random symmetric matrix fk×k := (% + %>)/2. We derive % by downsampling a given
input spectrogram (i.e., Xn×n) by a factor of two. Every single element of the derived Toeplitz ma-
trix (f) should be also distributed according to a normal distribution (Van Loan & Golub, 1983).
The probability distribution of f which is also known as Gaussian orthogonal ensemble (Alt et al.,
1995; Wu et al., 1990) for the eigenvalues of λi has the form of (Edelman, 1993):
2_k/2
Qk=ιν (i/2)
k_1
∏ ∣λi- λj | e-0∙5 P λ2
i6=j
(10)
where V(∙) denotes Minkowski function (Panti, 2008; Thompson & Thompson, 1996). Accordingly,
the summation of (9) for fast approximation (5) is the expectation over the determinants of f as
(Edelman, 1993):
n_1 n_1	n_1	k_1
1 X∣∏ fv(λi)	∏	∣λi	- λj	| e- P λ2∕2	) ≈	2k∕2	∏ V (i/2)	Ef det	(	∣{^	)	(11)
i=0 i=0	i6=j	i=0	fV (λi)
Although there are some analytical approaches for closely approximating fV (λi), we simply use
f since it is normally distributed over X and λ% → 0 for i > 0.5 ∙ k. Therefore det(f) gives an
acceptable approximation.
4
Under review as a conference paper at ICLR 2021
3.2	Intuition for the DDFN Metric
The DDFN metric as shown in (8) regularizes the generator network to craft spectrograms within the
manifolds of real samples which consequently binds its learning behavior and results to better sta-
bility. The DFN metric (5) computes a single real value for a spectrogram relative to its eigenvalues
in the subspace of Schur decomposition. According to (5), the DFN measures the depth between the
Frobenius norm of a spectrogram (mean energy (Golub & Van Loan, 2012)) and the summation of
all its possible eigenvalues (which encode structural components (Golub & Van Loan, 2012)). This
depth is in fact a single-value descriptor for a spectrogram representing its geometrical position in a
manifold. Therefore, the more similar two spectrograms become, the closer their DFN values (de-
scriptors) are expected so that lie in the same manifold. In other words, minimizing over the DDFN
metric, forces the generator to produce spectrograms within the manifold of real spectrograms.
Constraining the generator using the DDFN metric has some advantages. First, unlike the spectral
normalization, it does not manipulate weight vectors of the model. Therefore, it does not negatively
affect the computational complexity of the model during backpropagation in each epoch. Second,
we compute this metric for all the training samples and this helps to span all possible varieties
in the dataset. Third, the value for in (8) is justifiable according to the required quality for the
spectrogram. This helps to make a better trade-off between more samples variety (by increasing
) and higher quality (by decreasing ). In Section 4, we experimentally prove the effectiveness of
employing DDFN metric on GAN architectures for improving stability.
4	Experimental Results
In this section, we provide details of our experiments on two benchmarking environmental sound
datasets: UrbanSound8K (Salamon et al., 2014) and ESC-50 (Piczak, 2015). We have also con-
ducted a brief ablation study for Mozilla common voice (MCV)1. The first two datasets include 8,732
and 2,000 short environmental audio signals (≤5 sec) organized in 10 and 50 different classes, re-
spectively. MCV consists of 4,257 recorded hours of multi-language speeches (≤7 sec) and the cor-
responding text transcriptions. For generating DWT spectrograms, we use complex Morlet mother
function with static sampling frequency of 16 kHz and frame length of 50 ms with 50% overlap-
ping. We represented each spectrogram using three different visualizations of linear, logarithmic,
and logarithmic-real magnitude scales (see Appendix D)(Esmaeilpour et al., 2020a). For STFT
spectrograms, we set the number of filters to 2,048 with the hop length of 1,024 and reflect padding
in overlapping audio chunks of 50 ms (with ratio 0.5) (Esmaeilpour et al., 2020a). Meanwhile, we
apply pitch shifting (Salamon & Bello, 2017) with scales 0.75, 0.9, 1.15, and 1.5 for audio signals
with length ≤ 2 seconds for data augmentation (Esmaeilpour et al., 2020).
Our baseline model is the SA-GAN architecture (Zhang et al., 2019) using the hinge loss objective
for smooth training (Lim & Ye, 2017). Since we employ class-conditional (CC) learning similar to
Mirza & Osindero (2014), the generator receives CC batch-norm as suggested by De Vries et al.
(2017). For the discriminator, we run the identical procedure with projection (Miyato & Koyama,
2018) following the same optimization procedure in the baseline SA-GAN, however with some
modifications in the number of channels and applying spectral regularization for the generator net-
work. We have evaluated different options for initializing both networks, such as Glorot (Glorot &
Bengio, 2010) and variants ofN(0, I) (e.g., N(0, 0.02I)) (Radford et al., 2016). Since this choice
noticeably affects the training performance (Brock et al., 2019) and requires to be evaluated indi-
vidually according to properties of the dataset, we eventually used orthogonal regularization (Saxe
et al., 2014).
Figure 1 depicts the performance of four GAN architectures trained on logarithmic DWT spec-
trograms. All these models undergo collapse at different iterations which forces to early stop the
training. All the subsequent quality measurements on the performance of these models will be con-
ducted on the checkpoints prior to collapse. We compute the ∆2 (xg) - ∆2 (xr) for each model
to demonstrate collapse without imposing the DDFN condition on their generators (see (8)). These
plots corroborate the positive impact of spectral normalization (Zhang et al., 2019) on delaying col-
lapse at further iterations. The best performance of this normalization is when it is applied only on
1https://voice.mozilla.org/en/datasets
5
Under review as a conference paper at ICLR 2021
D(x; θd) (see Figures 1(c) and 1(d)), though incorporating it into both G(z; θg) and D(x; θd) still
outperforms the baseline SA-GAN.
20 40 60 80 100 120
IterationxlOOO
20 40 60 80 IOU
Iterationx 1000
20 40 60 80 100
IterationxlOOO
(a)	(b)	(c)	(d)
Figure 1: The typical plot of the DDFN measures for a random non-convolution layer in G(z; θg)
trained on STFT and logarithmic DWT representations of UrbanSound8K dataset. (a) The base-
line SA-GAN (Zhang et al., 2019), (b) the BigGAN architecture (Brock et al., 2019) with spectral
normalization in both G(z; θg) and D(x; θd), (c) the BigGAN configuration with spectral normal-
ization only in D(x; θd), and (d) the scaled up (by a factor of 2) configuration for (c). Arrows in
these sub-figures refer to collapse onsets.
Furthermore, we have evaluated the impact of increasing the batch size on the performance of GANs
during training while monitoring the DDFN measure along iterations. We scaled up the batch size
to 256 and 512 aiming at covering more modes and providing better gradients to both G(z; θg) and
D(x; θd) (Brock et al., 2019). Figure 1(d) depicts this effect for the BigGAN with batch size of 512
and regularized θd using spectral normalization. According to this graph although this GAN keeps
its stability until about 100k iterations, it undergoes complete collapse afterward. Moreover, the
DDFN measure is considerably reduced compared to other three GANs which indicates convergence
in fewer iterations. We additionally scaled up the number of channels (width) in each layer of the
networks by a factor of 2 at the cost of doubling the number of required parameters. However this
did not rectify nor delay the collapse at higher iterations.
We slightly modify the BigGAN architecture for class conditional (CC) training (see Appendix C).
Three major settings in CC training are required to adapt our benchmarking datasets. Firstly setting
the conditional vector (c-embedding) and secondly, the skip connection (direct skip-z) from the
given noise vector according to the designated probability distribution. While using separate layers
for c-embeddings has been proposed by Miyato et al. (2018), we also found shared embeddings
(Perez et al., 2018; Brock et al., 2019) outperforms the latter. For the skip-z connection probe, we
firstly evaluated splitting Z 〜N(0,I) and Z 〜N(0,0.02I) vectors into smaller chunks (We set
to 20) and concatenating them into c-embeddings in each level of resolution as suggested by Brock
et al. (2019), then explored a couple of other variants (Denton et al., 2015; GoodfelloW et al., 2014).
We ended up to concatenate the vector Z in its entire dimension to c-embeddings (Brock et al., 2019).
Finally, the third setting is the number of units in each hidden layer of the netWorks (the channel
multiplier). Increasing this hyperparameter relatively affects the IS and FID scores for the model.
Our objective evaluations are summarized in Table 1 and the number of iterations Without collapse
indicate considerable stability improvement compared to the baseline GANs depicted in Figure 1.
This table also shoWs that for the majority of the cases, the DDFN outperforms the orthogonal reg-
ularization (Brock et al., 2017) in delaying potential collapse. Additionally, it compares the effect
of batch size and number of channels on the performance of our slightly modified BigGAN archi-
tecture using spectral normalization and our introduced DDFN measure. According to this table,
doubling the batch size considerably improves both the IS and FID. Among different configurations
for the GAN, models trained on three visualizations of DWT spectrograms dominantly outperform
the STFT. We conjecture that this is due to the complexity of Morlet mother function (Young, 2012)
compared to sinusoidal transform in STFT.
4.1	Orthogonal Regularization vs. DDFN
The orthogonal regularization (Brock et al., 2017) has been proposed for smooth training, Which to
some extent, makes a better trade-off betWeen sample variety and fidelity by constraining over θd
6
Under review as a conference paper at ICLR 2021
Dataset	Orthogonal Regularization		Iteration ×1000		IS	FID
	False		132, 136		43.12 ± 1.46	44.57 ± 2.93
	(batch =	256)	(ch.	64)	Q 3.50, ↑ 1.14,-)	(↑ 1.67, J 2.95, ↑ 22.23)
	True		138, 144		46.89 ± 0.12	29.06 ± 1.18
	(batch =	256)	(ch.	96)	Q 2.64, ↑ 1.54,-)	(↑ 5.28, J 6.71, J 5.35)
	True		106,108		38.55 ± 1.09	49.91 ± 1.52
US8K	(batch =	二 256)	(ch.二	二 96)	(-,J 1.18,↑ 0.53)	(J 2.36, ↑ 1.82,-)
	False		134, 147		49.61 ± 0.12	21.75 ± 0.31
	(batch =	512)	(ch.	64)	(J 2.41, ↑ 1.78,-)	(J 2.09, J 3.14, -)
	True		133, 151		52.36 ± 1.19	21.93 ± 1.11
	(batch =	512)	(ch.	96)	(↑ 1.94, ↑ 2.72, J 27.19)	(J 1.40,-, ↑ 14.18)
	True		107,110		40.76 ± 0.69	33.56 ± 1.25
	(batch =	二 512)	(ch.二	二 96)	(↑ 1.03, J 0.93,-)	(J 1.33, J 2.39, ↑ 1.05)
	False		148, 152		68.76 ± 3.14	34.61 ± 2.23
	(batch =	256)	(ch.	64)	(↑ 0.64, ↑ 5.63,-)	(J 2.16, ↑ 1.77, -)
	True		143, 156		71.94 ± 1.42	29.81 ± 3.15
	(batch =	256)	(ch.	96)	(-,↑ 7.91, J 19.68)	(↑ 3.62, J 2.13, ↑ 2.73)
	True		107,115		68.79 ± 1.37	34.09 ± 0.85
ESC-50	(batch =	二 256)	(ch.二	二 96)	(J 1.22, ↑ 2.27, J 4.66)	(↑ 3.21, J 2.76, J 1.79)
	False		139, 155		71.46 ± 0.26	29.03 ± 1.05
	(batch =	512)	(ch.	64)	(J 1.47, J 2.38,-)	(J 6.91, J 4.63, -)
	True		145, 161		73.69 ± 1.47	23.03 ± 2.47
	(batch =	512)	(ch.	96)	(↑ 3.55, ↑ 1.95,-)	(↑ 2.79, ↑ 1.31, ↑ 21.84)
	True		109,116		69.81 ± 2.45	36.07 ± 2.68
	(batch =	二 512)	(ch.二	二 96)	(↑ 1.19, J 1.87, J 7.45)	(J 1.29,-, ↑ 1.32)
Table 1: Comparison of the inception score (higher is better) and Frechet inception distance (lower
is better) for our proposed modification to BigGAN architecture trained on logarithmic DWT spec-
trograms (resolution 128 × 128). Unhighlighted rows refer to the models with running spectral nor-
malization and DDFN on D(x; θd) and G(z; θg), respectively. Highlighted rows denote the models
with orthogonal regularization and spectral normalization without DDFN. Scores are averaged over
5k generated spectrograms over 10 different runs for each dataset. The left values in the iteration
column are associated with the highest achieved IS/FID before initial collapse, and the right values
refer to the extreme collapse onsets.Values inside parenthesis in the last two columns correspond
to linear-DWT, logarithmic-real-DWT, and STFT representations. Accordingly, up-/down-ward ar-
rows denote increase/decrease in scores relative to the logarithmic DWT. The best results are shown
in boldface.
as:
Rβ =βθd>θd-I2F	(12)
where β is a small hyperparameter. This regularization prohibitively affects the generator and results
to early collapse at about 25k iterations while trained on logarithmic DWT spectrograms. Recently
an improved version of this regularization has been introduced (Brock et al., 2019):
Rβ =βθd>θd(1-I)2F	(13)
where 1 refers to a matrix with constant values of 1. This regularization term binds to minimize
local similarity among filters. In our experiment we found out β ∈ (10-4, 10-5] is the best range
for STFT and DWT spectrograms except for the logarithmic-real. Upon running several exploratory
experiments, we set β = 10-4 for it. Unlike orthogonal regularization, the DDFN does not involve
weight matrix manipulations at each layer. This considerably reduces the computational cost for
training and makes the discriminator converge in fewer iterations and remarkably delays the collapse
at higher iterations. Table 1 shows the positive effect of orthogonal regularization while used with
minimized DDFN measure (see 8).
4.2	Spectrogram Fidelity and variety
Similar to Brock et al. (2019), our modification to the BigGAN architecture makes the model
amenable to truncation for Pr 〜N(0, αI) where the threshold α ∈ [0,1). Smaller values for ɑ
7
Under review as a conference paper at ICLR 2021
negatively affects the total number of spectrogram variety however, considerably boosts the qual-
ity of the generated samples. This trade-off (see a relevant study in Marchesi (2017)) also affects
the actual value of in computing the DDFN measure (8). Averaged over different experiments
on logarithmic DWT representations, for α ≤ 0.5 we achieve ≤ 37.16. This indicates higher
quality of the generated spectrogram at the cost of losing sample variety. Likewise, increasing
expands the sample variety but with degraded sample quality (see Figure 2). Our GAN architecture
for α ≤ 0.2 generates oversmoothed spectrograms as a result of a poorly conditioned model and
increases (explodes) e toward higher values.
α = 2,e = 78.145fc = 4
Oi = I, e = 56.37, fc = 4
a = 0.5, e = 47.05, ⅛ = 2 CL = 0.25, e = 34.53, k = 1
Figure 2: Generated logarithmic DWT spectrograms using our modified BigGAN model with spec-
tral normalization and the DDFN measure. From left to right: decreasing e and α makes G(z; θg) to
generate spectrograms with higher qualities and lower varieties. In this typical example, k denotes
the total number of spectrogram variety in each batch.
The truncation trick directly increases the IS and specifically for α = 0.25 we noticed 12.3%
improvement in generating high quality logarithmic DWT spectrograms compared to α = 2 or
U [-1, 1]. Moreover, a slight reduction in α positively impacts the FID by reducing the score 10%
on average, however, when α → 0 the FID sharply increases. Whereas FID, the IS does not penalize
sample variety, however, it rewards precision and this turns out to be a biased objective evaluation
on the quality of generated spectrograms (Brock et al., 2019). Moreover, interpretation and quality
analysis of the generated spectrograms could be very difficult to human eyes. To address this issue,
we reconstruct the audio signals from them and measure the SNR. This requires access to the phase
information for each spectrogram (Koerich et al., 2020) and unfortunately we could not successfully
train our model with phase vectors. Although, there are some approaches for phase approximation
(Leeb & Henk, 1989; Mulgrew, 2013) or reconstructing signal almost without phase information
(Kumar et al., 2019) using GAN, we noticed they often introduce ambient and background noises to
the reconstructed signal (see Appendix D). We eventually opted to utilize subjective phase vectors
from original known samples. Figure 3 shows that spectrograms with lower values of α and e better
reconstruct audio signals in terms of quality (see Appendix D for additional information).
Figure 3: Comparison of original and reconstructed sounds from the generated logarithmic DWT
spectrograms with identical phase matrix randomly drawn from an original speech signal from MCV.
For better visualization we have shown the first 500 ms from the entire three second-length signal.
8
Under review as a conference paper at ICLR 2021
Dataset	Model	Res.	IS	FID	SNRdb
	SA-GAN (baseline)	128	-	59.14 ± 2.24 (( 4.54, ( 1.46, -)	41.91
	BigGAN	128	-	54.95 ± 1.04 (↑ 2.14, ( 3.48, -)	46.93
MCV	BigGAN (+DDFN)	128	35.57 ± 2.92 (↑ 1.02, ↑ 3.46,-)	51.01 ± 2.18 (( 3.52, ( 2.11, -)	53.72
	Proposed (+DDFN)	128 256	50.79 ± 0.14 ((3.30, ； 2.48,-) 56.20 ± 1.73 ((4.03, J 2.95,-)	43.37 ± 1.65 ((6.88, ( 4.99,-) 36.31 ± 3.63 (( 7.59, ( 4.36, ↑ 23.48)	56.91 62.89
	SA-GAN (baseline)	256	34.72 ± 0.23 (↑ 1.17, ↑ 2.39,-)	-	34.12
US8K	BigGAN	256	39.16 ± 5.07 ((4.56, ( 3.94,-)	37.34 ± 3.53 (( 5.21, ( 2.75, ( 16.18)	48.14
	BigGAN (+DDFN)	256	48.62 ± 0.23 (-,↑ 2.39,-)	25.63 ± 2.42 (↑ 2.17, ↑ 0.94, -)	55.17
	ProPoSed (+DDFN)	256	57.91 ± 2.19 ((7.48, ( 4.88, ( 15.34)	16.33 ± 1.87 (( 3.42, ( 4.88, -)	63.24
Table 2: Comparison of average IS, FID and SNR for different models trained on logarithmic DWT
spectrograms. Outperforming values are in bold. Spectral normalization applied only on D(x; θd)
for all the BigGANs and its modified variant (the proposed architecture). Res. stands for resolution.
For supplementary results see Appendix B.
4.3	Summary and Ablation Study
In this section we compare the best performance of our proposed model with other GANs consid-
ering 256×256 and 128×128 input resolutions, as shown in Table 2. We use the ablated model
for MCV dataset on 100k randomly selected recordings shorter than five seconds. Moreover, we
objectively compare the quality of the reconstructed signals using the SNR2 (Kereliuk et al., 2015)
A high SNR means the reconstructed signal has low noise. Table 2 summarizes the results on four
spectrogram representations averaged over 5k generated spectrogram at 10 different runs for each
dataset. Doubling the resolution to 256×256 improves both the IS and the FID, however roughly
doubles the number of training parameters. Adding this note to the truncation trade-off, we can
conclude that for synthesizing high quality sounds, we need to tune three major hyperparameters:
α, large batch sizes, and high resolutions. Table 2 also shows the results of an ablation study on
UrbanSound8K and the conclusions are similar to those on MCV.
5 Conclusion
In this paper, we have proposed a conditioning trick for the generator network based on the DDFN
metric in the spectral domain of Schur decomposition. We have experimentally demonstrated its
positive impact in improving model stability at larger iterations for both baseline self-attention and
on our slightly modified BigGAN architecture for class-conditional learning. We have also shown
that our training scenario makes the model amenable to truncation and this helps to make a trade-off
between spectrogram quality and variety. According to three objective metrics of IS, FID, and SNR,
models conditioned with the DDFN outperform baselines in generating high quality spectrograms
and less noisy reconstructed audio and speech signals. Our proposed conditioning trick which has
been essentially developed for audio/speech signal synthesis purposes, is generalizable to GANs on
different modalities. We demonstrate the effectiveness of this approach on CIFAR-10 dataset for
natural image generation as discussed in Appendix A.
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
2 SNRdb (xr, xg) = 20 log10 P w(x)/P w(x) where P w(.) denotes the power of the signal.
9
Under review as a conference paper at ICLR 2021
scale machine learning. In 12th {USENIX} symposium on operating systems design and imple-
mentation ({OSDI} 16),pp. 265-283, 2016.
J.	I. Agbinya. Discrete wavelet transform techniques in speech processing. In Proceedings of Digital
Processing Applications (TENCON ’96), volume 2, pp. 514-519 vol.2, 1996.
H Alt, H-D Graf, HL Harney, R Hofferbert, H Lengeler, A Richter, P Schardt, and HA WeidenmUller.
Gaussian orthogonal ensemble statistics in a microwave stadium billiard with chaotic dynamics:
Porter-thomas distribution and algebraic decay of time correlations. Physical review letters, 74
(1):62, 1995.
Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial
networks. arxiv e-prints, art. arXiv preprint arXiv:1701.04862, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Mohammed Bahoura and Jean Rouat. Wavelet speech enhancement based on the teager energy
operator. IEEE signal processing letters, 8(1):10-12, 2001.
Bajibabu Bollepalli, Lauri Juvela, and Paavo Alku. Generative adversarial network-based glottal
waveform model for statistical parametric speech synthesis. arXiv preprint arXiv:1903.05955,
2019.
Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Cneural photo editing with
introspective adversarial networks. In International conference on machine learning, 2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=B1xsqj09Fm.
Tong Che, Yanran Li, Athul Paul Jacob, Yoshua Bengio, and Wenjie Li. Mode regularized generative
adversarial networks. arXiv preprint arXiv:1612.02136, 2016.
Sarang Chehrehsa and Tom James Moir. Speech enhancement using maximum a-posteriori and
gaussian mixture models for speech and noise periodogram estimation. Computer Speech &
Language, 36:58-71, 2016.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Harm De Vries, Florian Strub, Jeremie Mary, Hugo Larochelle, Olivier Pietquin, and Aaron C
Courville. Modulating early visual processing by language. In Advances in Neural Information
Processing Systems, pp. 6594-6604, 2017.
Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a
laplacian pyramid of adversarial networks. In Advances in neural information processing systems,
pp. 1486-1494, 2015.
Chris Donahue, Bo Li, and Rohit Prabhavalkar. Exploring speech enhancement with generative
adversarial networks for robust speech recognition. In IEEE Intl Conf on Acoustics, Speech and
Signal Processing (ICASSP), pp. 5024-5028, 2018.
Jeff Donahue, Philipp Krahenbuhl, and Trevor Darrell. Adversarial feature learning. arXiv preprint
arXiv:1605.09782, 2016.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Ar-
jovsky, and Aaron Courville. Adversarially learned inference. arXiv preprint arXiv:1606.00704,
2016.
Alan Edelman. The circular law and the probability that a random matrix has k real eigenvalues.
preprint, 1993.
10
Under review as a conference paper at ICLR 2021
M. Esmaeilpour, P. Cardinal, and A. L. Koerich. A robust approach for securing audio classification
against adversarial attacks. IEEE Transactions on Information Forensics and Security, 15:2147-
2159, 2020.
Mohammad Esmaeilpour, Patrick Cardinal, and Alessandro Lameiras Koerich. From sound repre-
sentation to model robustness. arXiv preprint arXiv:2007.13703, pp. 1-12, 2020a.
Mohammad Esmaeilpour, Patrick Cardinal, and Alessandro Lameiras Koerich. Unsupervised fea-
ture learning for environmental sound classification using weighted cycle-consistent generative
adversarial network. Applied Soft Computing, 86:105912, 2020b.
Fuming Fang, Junichi Yamagishi, Isao Echizen, and Jaime Lorenzo-Trueba. High-quality non-
parallel voice conversion based on cycle-consistent adversarial network. In IEEE Intl Conf on
Acoustics, Speech and Signal Processing (ICASSP), pp. 5279-5283, 2018.
William Fedus, Mihaela Rosca, Balaji Lakshminarayanan, Andrew M Dai, Shakir Mohamed, and
Ian Goodfellow. Many paths to equilibrium: Gans do not need to decrease a divergence at every
step. arXiv preprint arXiv:1710.08446, 2017.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Gene H Golub and Henk A Van der Vorst. Eigenvalue computation in the 20th century. Journal of
Computational and Applied Mathematics, 123(1-2):35-65, 2000.
Gene H Golub and Charles F Van Loan. Matrix computations, volume 3. JHU press, 2012.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger,
Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. Deep speech: Scaling up end-to-end
speech recognition. arXiv preprint arXiv:1412.5567, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proc. IEEE conference on computer vision and pattern recognition, pp. 770-778, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626-6637, 2017.
M. A. Hossan, S. Memon, and M. A. Gregory. A novel approach for mfcc feature extraction. In
2010 4th International Conference on Signal Processing and Communication Systems, pp. 1-5,
2010.
Ehsan Hosseini-Asl, Yingbo Zhou, Caiming Xiong, and Richard Socher. A multi-
discriminator cyclegan for unsupervised non-parallel speech domain adaptation. arXiv preprint
arXiv:1804.00522, 2018.
Ehsan Hosseini-Asl, Yingbo Zhou, Caiming Xiong, and Richard Socher. Augmented cyclic ad-
versarial learning for low resource domain adaptation. In International Conference on Learning
Representations, 2019. URL https://openreview.net/forum?id=B1G9doA9F7.
Hu Hu, Tian Tan, and Yanmin Qian. Generative adversarial networks based data augmentation for
noise robust speech recognition. In IEEE Intl Conf on Acoustics, Speech and Signal Processing
(ICASSP), pp. 5044-5048, 2018.
11
Under review as a conference paper at ICLR 2021
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for im-
proved quality, stability, and variation. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=Hk99zCeAb.
Corey Kereliuk, Bob L Sturm, and Jan Larsen. Deep learning and music adversaries. IEEE Trans-
actions OnMultimedia,17(11):2059-2071, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Stochastic gradient vb and the variational auto-encoder. In
2nd Intl Conf on Learning Representations, ICLR, volume 19, 2014.
Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira. On convergence and stability of gans.
arXiv preprint arXiv:1705.07215, 2017.
K.	M. Koerich, M. Esmaeilpour, S. Abdoli, A. S. Britto Jr., and A. L. Koerich. Cross-representation
transferability of adversarial attacks: From spectrograms to audio waveforms. In IEEE Intl J Conf
on Neural Networks, pp. 1-8, 2020.
Martin Krawczyk and Timo Gerkmann. Stft phase reconstruction in voiced speech for an improved
single-channel speech enhancement. IEEE/ACM Transactions on Audio, Speech, and Language
Processing, 22(12):1931-1940, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo,
Alexandre de Brebisson, Yoshua Bengio, and Aaron C Courville. Melgan: Generative adversarial
networks for conditional waveform synthesis. In Advances in Neural Information Processing
Systems, pp. 14910-14921, 2019.
W Christopher Lang and Kyle Forinash. Time-frequency analysis with the continuous wavelet trans-
form. American journal of physics, 66(9):794-797, 1998.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. Autoen-
coding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015.
F Leeb and T Henk. Simultaneous amplitude and phase approximation for fir filters. International
journal of circuit theory and applications, 17(3):363-374, 1989.
Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017.
StePhane Mallat. A wavelet tour ofsignalprocessing. Elsevier, 1999.
Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley.
Least squares generative adversarial networks. In IEEE Intl Conf onComputer Vision (ICCV), PP.
2813-2821, 2017.
Marco Marchesi. MegaPixel size image creation using generative adversarial networks. arXiv
preprint arXiv:1706.00082, 2017.
Yoshiki Masuyama, Kohei Yatabe, Yuma Koizumi, Yasuhiro Oikawa, and Noboru Harada. DeeP
griffin-lim iteration. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech
and Signal Processing (ICASSP), PP. 61-65. IEEE, 2019.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do
actually converge? arXiv preprint arXiv:1801.04406, 2018.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Takeru Miyato and Masanori Koyama. cGANs with Projection discriminator. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?
id=ByS1VpgRZ.
12
Under review as a conference paper at ICLR 2021
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=B1QRgziT-.
Bernard Mulgrew. The stationary phase approximation, time-frequency decomposition and auditory
processing. IEEE transactions on signal processing, 62(1):56-68, 2013.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In Advances in neural information processing systems,
pp. 271-279, 2016.
Augustus Odena, Christopher Olah, and Jonathon Shlens. Conditional image synthesis with auxil-
iary classifier gans. In International conference on machine learning, pp. 2642-2651, 2017.
Giovanni Panti. Multidimensional continued fractions and a minkowski function. Monatshefte fur
Mathematik, 154(3):247-264, 2008.
Ethan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron C. Courville. Film: Visual
reasoning with a general conditioning layer. In Sheila A. McIlraith and Kilian Q. Weinberger
(eds.), Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18),
the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium
on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA,
February 2-7, 2018, pp. 3942-3951. AAAI Press, 2018. URL https://www.aaai.org/
ocs/index.php/AAAI/AAAI18/paper/view/16528.
George M Phillips. Interpolation and approximation by polynomials, volume 14. Springer Science
& Business Media, 2003.
Karol J Piczak. Esc: Dataset for environmental sound classification. In Proc. 23rd ACM interna-
tional conference on Multimedia, pp. 1015-1018. ACM, 2015.
Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel,
Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer,
and Karel Vesely. The kaldi speech recognition toolkit. In IEEE 2011 Workshop on Automatic
Speech Recognition and Understanding. IEEE Signal Processing Society, December 2011. IEEE
Catalog No.: CFP11SRW-USB.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In Yoshua Bengio and Yann LeCun (eds.), 4th
International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May
2-4, 2016, Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.
06434.
Tuomo Raitio, Antti Suni, Junichi Yamagishi, Hannu Pulakka, Jani Nurminen, Martti Vainio, and
Paavo Alku. Hmm-based speech synthesis utilizing glottal inverse filtering. IEEE transactions
on audio, speech, and language processing, 19(1):153-165, 2010.
Douglas A Reynolds, Thomas F Quatieri, and Robert B Dunn. Speaker verification using adapted
gaussian mixture models. Digital signal processing, 10(1-3):19-41, 2000.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Olivier Rioul and Martin Vetterli. Wavelets and signal processing. IEEE signal processing magazine,
8(4):14-38, 1991.
J. Salamon, C. Jacoby, and J. P. Bello. A dataset and taxonomy for urban sound research. In 22nd
ACM Intl Conf on Multimedia, Orlando, FL, USA, Nov. 2014.
Justin Salamon and Juan Pablo Bello. Deep convolutional neural networks and data augmentation
for environmental sound classification. IEEE Signal Processing Letters, 24(3):279-283, 2017.
13
Under review as a conference paper at ICLR 2021
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training gans. In Advances in neural information processing systems,
pp. 2234-2242, 2016.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks. In Yoshua Bengio and Yann LeCun (eds.), 2nd
International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April
14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/abs/1312.
6120.
Casper Kaae S0nderby, Jose Caballero, Lucas Theis, Wenzhe Shi, and Ferenc Huszar. Amortised
map inference for image super-resolution. arXiv preprint arXiv:1610.04490, 2016.
Anuroop Sriram, Heewoo Jun, Yashesh Gaur, and Sanjeev Satheesh. Robust speech recognition
using generative adversarial networks. In IEEE Intl Conf on Acoustics, Speech and Signal Pro-
cessing (ICASSP), pp. 5639-5643, 2018.
Akash Srivastava, Lazar Valkov, Chris Russell, Michael U Gutmann, and Charles Sutton. Veegan:
Reducing mode collapse in gans using implicit variational learning. In Advances in Neural Infor-
mation Processing Systems, pp. 3308-3318, 2017.
Hoang Thanh-Tung, Truyen Tran, and Svetha Venkatesh. Improving generalization and stability of
generative adversarial networks. arXiv preprint arXiv:1902.03984, 2019.
Anthony C Thompson and Anthony C Thompson. Minkowski geometry. Cambridge University
Press, 1996.
C. F. Van Loan and G. H. Golub. Matrix computations. Johns Hopkins University Press, 1983.
HUa Wu, Michel Vallieres, Donald WL Sprung, et al. GaUSSian-orthogonal-ensemble level statistics
in a one-dimensional system. Physical Review A, 42(3):1027, 1990.
Shan Yang, Lei Xie, Xiao Chen, Xiaoyan Lou, Xuan Zhu, Dongyan Huang, and Haizhou Li. Statisti-
cal parametric speech synthesis using generative adversarial networks under a multi-task learning
framework. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pp.
685-691, 2017.
Randy K Young. Wavelet theory and its applications, volume 189. Springer Science & Business
Media, 2012.
Han Zhang, Ian Goodfellow, Dimitris Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. In International Conference on Machine Learning, pp. 7354-7363. PMLR,
2019.
A	Appendix
Without loss of generality, the proposed conditioning trick is also applicable to GANs for image
generation. Therefore, we run an ablation study on CIFAR-10 (Krizhevsky et al., 2009) dataset,
which includes 6k samples organized into 10 classes. This dataset has been one of the common
benchmarking datasets for GAN analysis.
Similar to Table 2, we measure the effectiveness of the DDFN metric in training three GAN configu-
rations namely, SA-GAN, BigGAN, and our slightly modified BigGAN architecture (see Appendix
C) without investigation for the amenability to truncation. Results are summarized in Table 3.
As shown in this table, the DDFN metric not only improves both IS and FID scores of the GAN
architectures, but also it considerably delays collapse onsets for the models. The rest of the settings
for these models is as explained in Appendix C.
Since images in this dataset have low resolution, it is very difficult to visually compare quality of
the generated images with the ground-truth, however we have included some images in the supple-
mentary material.
14
Under review as a conference paper at ICLR 2021
Model	IS	FID	Iteration ×1000
SA-GAN	06.32 ± 0.4F=	27.36 ± 1.49==	58.36
SA-GAN+DDFN	07.53 ± 0.08	23.19 ± 0.12	64:73
BigGAN	08.87 ± 0.03	18.91 ± 1.14	66:23
BigGAN+DDFN	09.14 ± 0.16	15.05 ± 0.51	71:04
Proposed	08.98 ± 0.04	14.98 ± 0.71	71:07
Proposed + DDFN	09.31 ± 0.17~	14.79 ± 0.06~	79.38
Table 3: Comparison of the inception score and Frechet inception distance for three state-of-the-art
GAN configurations on CIFAR-10 dataset. In all these experiments the batch size is set to 512.
Moreover, the spectral normalization has been applied on all the discriminator networks. Score are
averaged over 5k generated sample in 10 different runs. Better values are shown in boldface.
B Appendix
Table 4 is an extension of Table 2, which includes results for various resolutions. Both IS and
FID scores for the resolution 128×128 are very competitive to 256×256. Since Table 1 reports the
results of our proposed GAN architecture with the DDFN metric on the resolution 128×128, we
have excluded this record in Table 4.
Dataset	Model	Res.	IS	FID	SNRdb
	SA-GAN (baseline)	128 256	- -	59.14 ± 2.24 ((4.54, ( 1.46,-) 60.68 ± 2.17 (( 2.39, ↑ 0.73, ↑ 1.03)	41.91 39.11
	BigGAN	128	-	54.95 ± 1.04 (↑ 2.14, ( 3.48,-)	46.93
MCV		256	-	55.57 ± 2.78 (( 1.71, ( 2.15, ↑ 1.56)	46.29
	BigGAN (+DDFN)	128	35.57 ± 2.92 (↑ 1.02, ↑ 3.46, -) 34.43 ± 1.19	51.01 ± 2.18 ((3.52, ( 2.11,-) 53.46 ± 1.21	53.72
		256	(-,↑ 2.04, ； 0.97)	(↑ 0.86, ( 2.55, -)	50.18
	Proposed (+DDFN)	128	50.79 ± 0.14 ((3.30, ； 2.48,-) 56.20 ± 1.73	43.37 ± 1.65 ((6.88, ( 4.99,-) 36.31 ± 3.63	56.91
		256	((4.03, J 2.95,-)	(( 7.59, ( 4.36, ↑ 23.48)	62.89
	SA-GAN (baseline)	128 256	32.44 ± 1.76 (( 0.43, ↑ 1.35, -) 34.72 ± 0.23	-	33.95 34.12
			(↑ 1.17, ↑ 2.39,-)	-	
		128	36.58 ± 1.88	39.20 ± 2.76	41.02
	BigGAN		(( 1.37, ↑ 0.77, ( 1.83)	(-,(1.59, ( 10.63)	
US8K		256	39.16 ± 5.07 ((4.56, ( 3.94,-)	37.34 ± 3.53 (( 5.21, ( 2.75, ↑ 16.18)	48.14
	BigGAN (+DDFN)	128	40.99 ± 3.56 (( 2.16, ↑ 0.71, -) 48.62 ± 0.23	31.74 ± 1.17 (↑ 1.95, ( 2.33, ( 8.77) 25.63 ± 2.42	49.39
		256	(-,↑ 2.39,-)	(↑ 2.17, ↑ 0.94, -)	55.17
	Proposed (+DDFN)	256	57.91 ± 2.19 ((7.48, ( 4.88, ( 15.34)	16.33 ± 1.87 (( 3.42, ( 4.88, -)	63.24
Table 4: Extended comparison of average IS, FID and SNR for different models trained on logarith-
mic DWT spectrograms. Outperforming values are in bold. Spectral normalization applied only on
D(x; θd) for all the BigGANs and its modified variant (the proposed architecture). Res. stands for
resolution.
15
Under review as a conference paper at ICLR 2021
C Appendix
Figure 4 depicts the schematic of our slightly modified version of BigGAN (Brock et al., 2019).
This architecture uses the ResNet (He et al., 2016) with different channel multipliers and shared
class embeddings in the generator. Unlike the BigGAN architecture, we constantly use 3×3 padded
convolution with stride of 2. For the skip-z connection, we use static chunks of 20-D in accordance
with each residual block. Details of the networks are shown in Table 5 and Table 6.
Table 5:	Our slightly modified BigGAN architecture for the resolution 128×128 spectrograms.
Channel stands for the width multiplier in both networks. The rest of the settings such as standing
statistics for batch normalization (at the test time) are the same as (Brock et al., 2019).
Generator
Z ∈ R120 〜N(0,I)
Linear (20+ 128)(→ 4 X 4 X 16 Channe)
Residual Block (16 channel → 4 channel)
Residual Block (4 channel → 1 channel)^^
Non-IocaI Block (128 X 128)	一
Batch Normalization, ReLU, Convolution
tanh
Discriminator
RGB SPeCtrogram Xg ∈ r128×128×3
Residual Block (channel → 2 channel)
Non-local Block (64 X 64)
Residual Block (channel 2 → 8 channel)
Residual Block (channel 8 → 16 channel)
ReLU, Global Sum Pooling
Linear → 1
Table 6:	The modified BigGAN architecture for 256 X 256 sPectrograms. This architecture has one
additional residual network comPared to smaller resolution 128.
Generator
Z ∈ R140 〜N(0,I)
Linear (20 + 128)(→ 4 × 4 × 16 channel)
Residual Block (16 channel → 4 channel)
Residual Block (4 channel → 4 channel)^^
Residual Block (4 channel → 1 channel)
Non-localBlock (128 × 128)
Batch Normalization, ReLU, Convolution
tanh
Discriminator
RGB SPeCtrOgram Xg ∈ r256×256×3
Residual Block (channel → 2 channel)
Non-local Block (64 X 64)
Residual Block (2 channel → 4 channel)
Residual Block (4 channel → 8 channel)
Residual Block (8 channel → 16 channel)
ReLU, Global Sum Pooling
Linear → 1
As suggested (Brock et al., 2019) for the CC conditioning in each residual block, the linear Projection
has been used where the bias and gain Projections are centered at 0 and 1, resPectively. We use
Orthogonal initialization (Saxe et al., 2014) for both generator and discriminator networks. For the
choice of the oPtimier, Adam (Kingma & Ba, 2014) is utilized with β1 = 0.0 and β2 = 0.9. The
learning rate is set to 2 ∙ 10-4 and 3 ∙ 10-5 for discriminator and generator at both resolutions. The
rest of settings for baselines are the same as (Zhang et al., 2019) imPlemented in TensorFlow (Abadi
et al., 2016).
C.1 Steps for Training
The first steP in training a GAN with DDFN is comPuting DFN for real and generated samPles.
However, we have exPlained how to fastly aPProximate this metric in section 3.1. For real samPles
once we comPute the DFN, but this oPeration is needed for all the generated samPles. The second
steP is initializing the differentiable interval [$inf , $sup] for (6). We emPirically set $inf = -1
and $sup = 1. However, this is tunable according to the Performance of the model. The third steP
is running (1) with the constraint (8).
D	Appendix
We highly recommend reviewing these sources (Agbinya, 1996; Bahoura & Rouat, 2001) about
DWT rePresentations for audio and sPeech signals. Then we draw attentions to the clarification
of three visualizations for DWT sPectrograms. Briefly, the following code sniPPet exPlains linear,
logarithmic, and logarithmic real visualizations.
16
Under review as a conference paper at ICLR 2021
■Cl..............j
I ；卜-A Split --ALinear--A RB --A RB --> NL
卜lass}-A c]......................-i
Spectrogram
Non-Local NL
C Concat
Upsampling
Residual Block
Conv.
3x3 Convolution
Batch Normalization
Average Pooling
Figure 4: The proposed architecture, which is the slightly modified version of BigGAN (Brock
et al., 2019). (Top): The generator architecture. (Middle): A residual block in generator. (Bottom):
a residual block in discriminator.
17
Under review as a conference paper at ICLR 2021
static float linearDWTSpectrogram(float* data)
{
/* Generating linear visualization for DWT spectrogram
Real part: data[0], Imaginary part: data[1]
*/
return sqrt(data[0]*data[0]+data[1]*data[1]);
}
static float logDWTSpectrogram(float* data)
{
/* Generating logarithmic visualization
for DWT spectrogram */
return log(sqrt(data[0]*data[0]+data[1]*data[1]));
}
static float logRealDWTSpectrogram(float* data)
{
/* Generating logarithmic-real visualization
for DWT spectrogram */
if (data[0] == 0.0) {
return 0.0;
}
return log(abs(data[0]));
}
We have carefully trained (early stopped at checkpoints before overtraining or potential collapse)
our proposed GAN architecture (the modified version of BigGAN with DFN) separately on the
generated visualizations of DWT spectrograms. Afterwards, we illustrate some generated spectro-
grams. Additionally, we reconstruct each audio signals (one channel) from its spectrogram with a
predefined original phase matrix to measure relative SNR (see Figures 5-10).
We encourage readers to listen to some reconstructed speech signals from the synthesized loga-
rithmic DWT spectrograms using the proposed GAN architecture3 which are included in folder
”Synthesized-Speech” of the supplementary material. This folder includes two subfolders (set 1 and
set 2) associated with two different resources. Each subfolder contains eight WAV signals (with their
associated spectrograms under the same names as signals) which their details are listed herein:
1.	Original speech: This is a random original file from MCV dataset.
2.	Synthesized speech 1: The reconstructed speech using the identical phase information
from the aforementioned original speech. The synthesized spectrogram is generated by the
model with = 34.53 and α = 0.25.
3.	Synthesized speech 2: The reconstructed speech using the identical phase information
from the aforementioned original speech. The synthesized spectrogram is generated by the
model with = 47.05 and α = 0.50.
4.	Synthesized speech 3: The reconstructed speech using the identical phase information
from the aforementioned original speech. The synthesized spectrogram is generated by the
model with = 56.37 and α = 1.
5.	Synthesized speech 4: The reconstructed speech using the identical phase information
from the aforementioned original speech. The synthesized spectrogram is generated by the
model with = 78.14 and α = 2.
6.	Approximated approach 1: The reconstructed speech using the approximated phase in-
formation as proposed in (Krawczyk & Gerkmann, 2014). The synthesized spectrogram is
generated by the model with = 34.53 and α = 0.25.
7.	Approximated approach 2: The reconstructed speech using the Griffin-Lim approach
(Masuyama et al., 2019).
8.	Approximated approach 3: The reconstructed speech using MelGAN approach (Kumar
et al., 2019).
3Res. = 256, batch = 512, ch. = 96 with spectral normalization and the DDFN on discriminator and
generator, respectively.
18
Under review as a conference paper at ICLR 2021
There is (relatively) no sensible background or ambient noise in the reconstructed speech signals
with synthesized spectrograms and original phase information (synthesized speech 1-4). Although,
reconstructing signals with approximated phase vectors (i.e., approximated approach 1-3) add no-
ticeable noises to the signals, still the speech is understandable. This denotes the critical role of high
quality spectrogram in reconstruction.
19
Under review as a conference paper at ICLR 2021
(b) Original Spectrogram
(a) Original phase
(c) Synthesized spectrogram
(d) Synthesized spectrogram
(e) Synthesized spectrogram
(f) Synthesized spectrogram
Figure 5: Generated logarithmic DWT spectrograms with our modified BigGAN and the DDFN for
a random signal sig1. Synthesized signals are reconstructed with the the original phase.
(a) Original phase
(b) Original spectrogram
(c) Synthesized spectrogram
Figure 6: Generated logarithmic DWT spectrograms with our modified BigGAN and the DDFN for
another random signal sig2 . Synthesized signals are reconstructed with the the original phase.
(d) Synthesized spectrogram
20
Under review as a conference paper at ICLR 2021
(a) Original Phase	(b) Original spectrogram
(c) Synthesized spectrogram	(d) Synthesized spectrogram
(e) Synthesized spectrogram	(f) Synthesized spectrogram
Figure 7: Generated linear DWT spectrograms with our modified BigGAN and the DDFN for a
random signal sig1. Synthesized signals are reconstructed with the the original phase.
(a) Original phase	(b) Original spectrogram
(c) Synthesized spectrogram
(d) Synthesized spectrogram
Figure 8: Generated linear DWT spectrograms with our modified BigGAN and the DDFN for an-
other random signal sig2 . Synthesized signals are reconstructed with the the original phase.
21
Under review as a conference paper at ICLR 2021
(a) Original phase
(b) Original spectrogram
(c) Synthesized spectrogram
(d) Synthesized spectrogram
(e) Synthesized spectrogram
(f) Synthesized spectrogram
Figure 9: Generated logarithmic-real DWT spectrograms with our modified BigGAN and the DDFN
for a random signal sig1. Synthesized signals are reconstructed with the the original phase.
(a) Original phase
(b) Original spectrogram
(c) Synthesized spectrogram	(d) Synthesized spectrogram
Figure 10: Generated logarithmic-real DWT spectrograms with our modified BigGAN and the
DDFN for another random signal sig2 . Synthesized signals are reconstructed with the the origi-
nal phase.
22