Under review as a conference paper at ICLR 2021
Neural Nonnegative CP Decomposition for
Hierarchical Tensor Analysis
Anonymous authors
Paper under double-blind review
Ab stract
There is a significant demand for topic modeling on large-scale data with complex
multi-modal structure in applications such as multi-layer network analysis, tempo-
ral document classification, and video data analysis; frequently this multi-modal
data has latent hierarchical structure. We propose a new hierarchical nonnegative
CANDECOMP/PARAFAC (CP) decomposition (hierarchical NCPD) model and
a training method, Neural NCPD, for performing hierarchical topic modeling on
multi-modal tensor data. Neural NCPD utilizes a neural network architecture and
backpropagation to mitigate error propagation through hierarchical NCPD.
1 Introduction
The recent explosion in the collection and availability of data has led to an unprecedented demand for
scalable data analysis techniques. Furthermore, data that has a multi-modal tensor format has become
ubiquitous across numerous fields (Cichocki et al., 2009). The need to reduce redundant dimensions
(across modes) and to identify meaningful latent trends within data has rightly become an integral
focus of research within signal processing and computer science. An important application of these
dimension-reduction techniques is topic modeling, the task of identifying latent topics and themes of
a dataset in an unsupervised or partially supervised approach. A popular topic modeling approach for
matrix data is the dimension-reduction technique nonnegative matrix factorization (NMF) (Lee &
Seung, 1999), which is generalized to multi-modal tensor data by the nonnegative CP decomposition
(NCPD) (Carroll & Chang, 1970; Harshman et al., 1970). These models identify r latent topics
within the data; here the rank r is a user-defined parameter that can be challenging to select without a
priori knowledge or a heuristic selection procedure.
In topic modeling applications, one often additionally wishes to understand the hierarchical topic
structure (i.e., how the topics are naturally related and combine into supertopics). For matrices
(tensors), a naive approach is to apply NMF (NCPD) first with rank r and then again with rank j < r,
and simply identify the j supertopics as linear (multilinear) combinations of the original r subtopics.
However, due to the nonconvexity of the NMF (NCPD) objective function, the supertopics identified
in this way need not be linearly (multi-linearly) related to the subtopics. For this reason, hierarchical
models which enforce these relationships between subtopics and supertopics have become a popular
direction of research. A challenge of these models is that the nonconvexity of the model at each level
of hierarchy can yield cascading error through the layers of models; several works have proposed
techniques for mitigating this cascade of error (Flenner & Hunter, 2018; Trigeorgis et al., 2016;
Le Roux et al., 2015; Sun et al., 2017; Gao et al., 2019).
In this work, we propose a hierarchical NCPD model and Neural NCPD, an algorithm for training this
model which exploits backpropagation techniques to mitigate the effects of error introduced at earlier
(subtopic) layers of hierarchy propagating downstream to later (supertopic) layers. This approach
allows us to (1) explore the topics learned at different ranks simultaneously, and (2) illustrate the
hierarchical relationship of topics learned at different tensor decomposition ranks.
Notation. We follow the notational conventions of Goodfellow et al. (2016); e.g., tensor X, matrix X,
vector x, and (integer or real) scalar x. In all models, we use variable r (with superscripts denoting
layer of hierarchical models) to denote model rank and use j when indexing through rank-one
1
Under review as a conference paper at ICLR 2021
components. In all tensor decomposition models, we use k to denote the order (number of modes) of
the tensor and use i when indexing through modes of the tensor. In all hierarchical models, we use L
to denote the number of layers in the model and use ' to index layers. We let 0 denote the vector
outer product and adopt the CP decomposition notation
r
[Xi, X2,…，XkJ ≡ X xj1) 0 xj2) 0 …0 xjk),	(1)
j=1
(i)
where xj is the jth column of the ith factor matrix Xi (Kolda & Bader, 2009).
Contributions. Our main contributions are two-fold. First, we propose a novel hierarchical non-
negative tensor decomposition model that we denote hierarchical NCPD (HNCPD). Our model
treats all tensor modes alike and the output is not affected by the order of the modes in the tensor
representation; this is a property not shared by other hierarchical tensor decomposition models such
as that of Cichocki et al. (2007a). Second, we propose an effective neural network-inspired training
method that we call Neural NCPD. This method builds upon the Neural NMF method proposed
in Gao et al. (2019), but is not a direct extension; Neural NCPD consists of a branch of Neural NMF
for each tensor mode, but the backpropagation scheme must be adapted for factorization information
flow between branches.
Organization. In the remainder of Section 1, we present related work on tensor decompositions and
training methods. In Section 2, we present our main contributions, hierarchical NCPD and the Neural
NCPD method. In Section 3, we test Neural NCPD on real and synthetic data, and offer some brief
conclusions in Section 4. We include justification of several computational details of our method and
further experimental results in Appendix A.
1.1	Related Work
In this section, we introduce NMF, hierarchical NMF, the Neural NMF method, and NCPD, and then
summarize some relevant work.
Nonnegative Matrix Factorization (NMF). Given a nonnegative matrix X ∈ Rn≥10×n2, and a
desired dimension r ∈ N, NMF seeks to decompose X into a product of two low-dimensional
nonnegative matrices; dictionary matrix A ∈ Rn≥10×r and representation matrix S ∈ Rr≥×0n2 so that
r
X ≈ AS = X aj 0 sj ,	(2)
j=1
where aj is a column (topic) of A and sj is a row of S. Typically, r is chosen such that r <
min{n1, n2} to reduce the dimension of the original data matrix or reveal latent themes in the data.
Each column of S provides the approximation of the respective column in X in the lower-dimensional
space spanned by the columns of A. The nonnegativity of the NMF factor matrices yields clear
interpretability; thus, NMF has found application in document clustering (Xu et al., 2003; Gaussier
& Goutte, 2005; Shahnaz et al., 2006), and image processing and computer vision (Lee & Seung,
1999; Guillamet & Vitria, 2002; Hoyer, 2002), amongst others. Popular training methods include
multiplicative updates (Lee & Seung, 1999; 2001; Lee et al., 2009), projected gradient descent (Lin,
2007), and alternating least-squares (Kim et al., 2008; Kim & Park, 2008).
Hierarchical NMF (HNMF). HNMF seeks to illuminate hierarchical structure by recursively factor-
izing the NMF S matrices; see e.g., (Cichocki et al., 2009). We first apply NMF with rank r(0) and
then apply NMF with rank r(1) to the S matrix, collecting the r(0) subtopics into r(1) supertopics.
HNMF with L layers approximately factors the data matrix as
X ≈ A(O)S⑼ ≈ A(O)A(I)S⑴ ≈∙∙∙≈ A(O)A⑴…A(LT)S(L-1).	(3)
Here the A(i) matrix represents how the subtopics at layer i collect into the supertopics at layer i + 1.
Note that as L increases, the error ∣∣X 一 A(O)A(I) •…A(L-1)S(L-1)kF necessarily increases as
error propagates with each step. As a result, significant error is introduced when L is large. Choosing
r(0),r(1),... , r(L-1) in practice proves difficult as the number of possibilities grow combinatorially.
2
Under review as a conference paper at ICLR 2021
Neural NMF (NNMF). In the previous work of Gao et al. (2019), the authors developed an iterative
algorithm for training HNMF that uses backpropagation techniques to mitigate cascading error
through the layers. To form this hierarchical factorization, the Neural NMF algorithm uses a neural
net architecture. Each layer ' of the network has weight matrix A('). In the forward propagation step,
the network accepts a matrix S('-1), calculates the nonnegative least-squares solution
S(') = q(A('), S('-1)) ≡ argmin ∣∣S('-1) - A(')S∣∣f,	(4)
S≥0
and sends the matrix S⑻ to the next layer. In the backpropagation step, the algorithm calculates
gradients and updates the weights of the network, which in this case are the A matrices.
Nonnegative CP Decomposition (NCPD). The NCPD generalizes NMF to higher-order tensors;
specifically, given an order-k tensor X ∈ R≥0×n2 ×∙∙∙×nk and a fixed integer r,the approximate NCPD
of X seeks Xi ∈ R≥0×r, X2 ∈ R≥0×r, •二，Xk ∈ R≥k×r so that
X ≈ [Xi, X2,…，Xk]].	(5)
The Xi matrices will be referred to as the NCPD factor matrices. A nonnegative approximation
with fixed r is obtained by approximately minimizing the reconstruction error between X and the
NCPD reconstruction. This decomposition has found numerous applications in the area of dynamic
topic modeling where one seeks to discover topic emergence and evolution (Cichocki et al., 2007b;
Traore et al., 2018; Saha & Sindhwani, 2012). Methods for training NMF models can often be
generalized to NCPD; for example, multiplicative updates (Welling & Weber, 2001) and alternating
least-squares (Kim et al., 2014).
Other Related Work. Other works have sought to mitigate error propagation in HNMF models
with techniques inspired by neural networks (Trigeorgis et al., 2016; Le Roux et al., 2015; Sun et al.,
2017; Flenner & Hunter, 2018). Additionally, previous works have developed hierarchical tensor
decomposition models and methods (Vasilescu & Kim, 2019; Song et al., 2013; Grasedyck, 2010).
The model most similar to ours is that of Cichocki et al. (2007a), which we refer to as hierarchical
nonnegative tensor factorization (HNTF). This model consists of a sequence of NCPDs, where a
factor matrix for one mode is held constant, the remaining factor matrices produce the tensor which
is decomposed at the second layer, and this decomposition is combined with the fixed matrix from
the previous layer. We note that HNTF is dependent upon the ordering of the modes, and specifically
which data mode appears first in the representation of the tensor. We refer to ‘HNTF-i‘ as HNTF
applied to the representation of the tensor where the modes are reordered with mode i first.
2	Our Contributions
In this section, we present our two main contributions. We first describe the proposed hierarchical
NCPD (HNCPD) model, and then propose a training method, Neural NCPD, for the model.
2.1	HIERARCHICAL NCPD (HNCPD)
Given an order-k tensor X ∈ Rn1 ×...×nk, HNCPD consists ofan initial rank-r NCPD layer with factor
matrices Xi, X2,..., Xk, each with r columns, and an HNMF with ranks r(0),r(1), •…,r(L-2) for
each of these factors matrices; that is, for each Xi at layer `, we factorize Xi as
Xi ≈ Xi ≡ A(O)Ai1)...A('-2)S('-2)	(6)
where A(') has r(') columns; see Figure 1 for a visualization. Thus, HNCPD consists of tensor
approximations
X ≈ [Al0)…Al'-2)s('-2),…,AkO)…Ak'-2)sk'-2)]].	⑺
To access hierarchical structure between tensor topics at each layer, we need to utilize information in
the S(') matrices for all modes. To simplify this hierarchical structure, we develop an approximation
scheme such that the hierarchical topic structure for all modes is given by a single matrix.
3
Under review as a conference paper at ICLR 2021
For simplicity, we first consider the two layer case. We note that
[fl, f2,…，Xk I= X	αjι,j2,...jk ((Al0))：,ji ③(a20) )：,j2 ③...③(Ak0))"jJ
1≤j1 ,j2 ,...jk ≤r(0)
(8)
where αj1 ,j2,...jk = Prp
=1(S1(0))j1,p(S2(0))j2,p... (Sk(0))jk,p; we justify this statement in Appendix
A. We refer to decomposition summands in (8)where jι = j2 =…=jk as vector outer products of
same-index factor matrix topics, and all other summands as vector outer products of different-index
factor matrix topics. To identify clear hierarchy, we avoid these different-index column outer products.
The approximation scheme computes ma-
(0)
trices Ai whose columns visualize the
desired r(0) NCPD topics along each
mode while avoiding different-index col-
umn outer products in the decomposition.
We approximate the summation (8) by re-
placing all summands that include column
p2 of A(k0) with a single rank-one vector
outer product, (√e10))：g ③(""):g ③
...X(Ak-i)：,P2 ③(A.k0))62. To minimize
error introduced by this approximation, we
transform factor matrices Ai(0) for i 6= k to
Aei(0) by collecting into (Aei(0)):,p2 the ap-
X1
LZLL
Figure 1: A visualization of a two-layer HNCPD model.
The colored edges of the order-three tensor, X, represent
the three modes.

proximate contribution of all columns of Ai(0) in vector outer products with (A(k0)):,p2 in (8). That is,
for 1 ≤ p1, p2 ≤ r(0) and 1 ≤ i < k, let Wi ∈ Rr(0) ×r(0) be a matrix with
(Wi)p1,p2 =	X	αj1,j2,...,jk	and	Aei(0) =Ai(0)Wi,	(9)
ji=p1,jk=p2,1≤j1,j2,...jk≤r(0)
Furthermore, we can identify the topic hierarchy from the Sk(0) matrix. We can generalize
this process to later layers ` by noting that we can group the Ai matrices together, so Xi ≈
(A(O)A(I)... A('))S('). Thus,we can treat this approximation as above, replacing Ai(0) with the
product A(O)A(D ... A(').
Like in HNMF, errors in earlier layers can propagate through to later layers and produce highly
suboptimal approximations. Challenges encountered during computation of HNMF are exacerbated
in an HNCPD model. For this reason, we exploit approaches developed for HNMF in Gao et al.
(2019) in our training method Neural NCPD. Furthermore, the computation of HNMF factor matrices
for Xi are independent from Xj if the factorizations are applied sequentially; Neural NCPD allows
factor matrices in (6) for all other modes to influence the factorization of a given mode.
2.2	Neural NCPD
Our iterative method consists of two subroutines, a forward-propagation and a backpropagation. In
Algorithms 1 and 2, we display the pseudocode for our proposed method. Following this learning
process for the factor matrices in (6), we apply the approximation scheme described in Section 2.1 to
the learned factor matrices to visualize the hierarchical structure of the computed HNCPD model.
Forward Propagation. The forward-propagation treats Af) matrices as neural network weights
and uses A(') and previous layer output to compute
S(') = q(A('), S('-1)),	(10)
where q is as defined in equation 4 and Si(-1) = Xi , producing the matrices Si(O), . . . , Si(L-2)
for 1 ≤ i ≤ k. The function q(A('), S('-1)), as a nonnegative least-squares problem, can be
4
Under review as a conference paper at ICLR 2021
calculated via any convex optimization solver; we utilize an implementation of the Hanson-Lawson
algorithm (Lawson & Hanson, 1995). Finally, We pass the A(' and S(') matrices and X into a loss
function, which we differentiate and backpropagate.
Backpropagation. Our goal is to dif-
ferentiate our cost function C with
respect to the weights in each layer,
the A(') matrices and backpropa-
gate. This algorithm accepts any first-
order optimization method, denoted
optimizer (e.g., SGD (Robbins &
Monro, 1951), Adam (Kingma & Ba,
2014)), but projects the updated weight
matrix into the positive orthant to main-
tain nonnegativity.
For the NCPD task, the most natural
loss function is the reconstruction loss,
__ ~~^ -~~^	-~~^
C = kX - [X1, ff2,…，Xk]∣F.
(11)
In order to encourage optimal fit at
each layer, we also introduce a loss
function that we refer to as energy loss.
First we denote the approximation of
X at layer ` of our network as
Algorithm 1 Forward Propagation
procedure FORWARDPROP({Xi}k=0, {Af)}：=屋0)
for i = 1,…，k do
for ' = 0, ∙∙∙ , L- 2 do
S(') - q(A('), S('-1))	. see equation 4
Algorithm 2 Neural NCPD
Input: Tensor X ∈ Rn1 ×n2 ×...×nk, cost C
Xi, X2,…，Xk — NCPD(X), initialize {A(')}k⅛0
for iterations = 1, . . . , T do
ForwardPropGXi^oNAf)}：=；'=O) . Alg. 1
for i = 1,…，k, ' = 0,…，L - 2 do
A(' J (optimizer(A('), ∂A¾)))+
. any first-order method
X' = [A10)A11) …Ai'-2)S 尸), A20)A21) …A2'-2)s2'-2),..., Ak0)Ak1) …Ak'-2)sk'-2".
(12)
Then, we calculate energy loss as
L-2
E = kX - [Xi, X2,…，XklkF + X kX - X'kF.	(13)
The derivatives of q(A, X) with respect to A and X are derived and exploited to differentiate a
generic cost function for the hierarchical NMF model in Gao et al. (2019); here we summarize these
derivatives and illustrate how to combine them with simple multilinear algebra for HNCPD.
Gao et al. (2019) show that, if (^dCy) is the derivative of C with respect to A(') holding the S
matrices constant, then
∂C
∂A('1)
+
'1≤'2≤L-2
i≤j≤r
(`1 ,`2 ),j
Ui
(14)
where U('1,'2),j relates C to A('1) through S('2) and S('1), is defined column-wise (j), and depends
upon (8姬氏2)) , the derivative of C with respect to S('2) holding S('2+i), ... , S(L-2) constant.
The definition of U('1,'2),j is given in Gao et al. (2019) and utilizes, via the chain-rule, the partial
derivative of q(A', S'-1) for all' ∈ ['1,'2].
Example. The derivative of the previously defined, or other differentiable cost functions can be
calculated using these results of Gao et al. (2019) and some simple multi-linear algebra. As an
example, we directly compute the backpropagation step for the reconstruction loss function C given
in equation 11. Let X(i) be the mode-i matricized version of X, and define
Hi = Xfk	. . .	Xfi+i	Xfi-i	. . .	Xfi ,	(15)
5
Under review as a conference paper at ICLR 2021
∂C
∂A('j)
where denotes the Khatri-Rao product (see e.g., (Kolda & Bader, 2009)). Then we have that
S
=2 (A(O)A(I)…A('jτ"(X(i)- XiHnHi(A”)…A(L-为尸)),，
(16)
*
=2 (AiO)A(I)…A('j)),(X(i)- XiHnHi.	(17)
These derivatives are justified in Appendix A. With equation 14, these derivatives are sufficient to
calculate the partial derivative of C with respect to any A matrix.
. ∂C
3	Experimental Results
We test Neural NCPD on three datasets: one synthetic, one video, and one collected from Twitter.
The synthetic dataset is constructed as a simple block tensor with hierarchical structure. The Twitter
dataset consists of tweets from political candidates during the 2016 United States presidential
election (Littman et al., 2016). We pull the video, a time-lapse of a forest over the span of one
year, from (Solheim). We also compare Neural NCPD to Standard NCPD, in which we perform
an independent NCPD decomposition at each rank, and to Standard HNCPD, in which we perform
NCPD first on the full dataset, and apply HNMF to the fixed factor matrices; here we sequentially
apply NMF to the factor matrices using multiplicative updates and do not update previous layer
factorizations as in Neural NCPD. In all experiments, we use Tensorly (Kossaifi et al., 2018) for
Standard NCPD calculations and to initialize the NCPD layer of our hierarchical NCPD, and in
Neural NCPD we do not backpropagate to this layer as the initialization has usually found a stationary
point. We use Energy Loss (Eq. 12) for all experiments to encourage fit at every layer. Because we
do not backpropagage to the initial factor matrices, the first term in (Eq. 12) is fixed. For the Twitter
and video experiments we use the approximation scheme of Section 2.1 to recover the relationship
between the columns of the A(') matrices and visualize the A(') matrices.
3.1	Experiment on Synthetic Data
We test the Neural NCPD algorithm first using a synthetic dataset. This dataset is a rank seven tensor
of size 40 × 40 × 40 with positive noise added to each entry; we generate noise as n = |g| where
g 〜N(0, σ2). To generate this dataset, we begin with the all-zeros tensor and create three large
nonoverlapping blocks with value 1, and then overlay each block with either two or three additional
blocks with value 3. We display this tensor with two levels of noise at the left of Figure 2; here we
plot projections of all tensors (and all approximations) along the third mode; that is, we construct a
matrix with entries equal to the largest entries of the mode-three fibers (see e.g., (Kolda & Bader,
2009) for relevant definitions). The projections on the remaining two modes are included in the
Appendix A, and are all similar to the third mode.
Table 1: Relative reconstruction loss, Crel, on a synthetic dataset for Neural NCPD, Standard HNCPD,
and HNTF with two different levels of noise. We list the loss of the approximation r(1) = 3. The
results of HNTF are similar for all orderings of the modes, so we list only one.
Method	r = 7	σ2 = 0.05 r(O) = 5	r(1) = 3	r = 7	σ2 = 0.5 r(O) = 5	r(1) = 3
Neural NCPD	0.091	0.229^^	0.467	0.390	0.438	0.490
Standard HNCPD	0.091	0.525	0.674	0.390	0.441	0.643
HNTF	0.091	0.234	0.539	0.390	0.450	0.578
We run Neural NCPD, Standard HNCPD, and Chichocki et al. on this synthetic dataset at two
different levels of noise with three layers of ranks 7, 5, and 3, and display the results in Figure 2
and Table 1; we present the relative reconstruction loss Crel = ∣∣X 一 [Xi, X2,…，Xk]∣f/∣∣X∣∣f.
For each level of noise, we display the rank 7 approximation shared by all methods, and the rank 5
6
Under review as a conference paper at ICLR 2021
and rank 3 approximations produced by Neural NCPD and Standard HNCPD. We also display the
transposed Neural NCPD S30) and S(I) matrices, which show how rank 7 topics collect into ranks 5
and 3 topics. From Table 1, we see that the loss for Neural NCPD is at or below that of Standard
HNCPD and HNTF at each rank and level of noise.
3.2	Temporal Document Analysis
We next apply Neural NCPD to a dataset of tweets from four Republican [R] and four Democratic
[D] 2016 presidential primary candidates, (1) Hillary Clinton [D], (2) Tim Kaine [D], (3) Martin
o’Malley [D], (4) Bernie Sanders [D], (5) Ted Cruz [R], (6) John Kasich [R], (7) Marco Rubio [R],
and (8) Donald Trump [R]; this is constructed from a subset of the dataset of Littman et al. (2016).
We use a bag-of-words (12,721 words in corpus) representation of all tweets made by a candidate
within bins of 30 days (from February to December 2016), and cap each of these groups at 100 tweets
to avoid oversampling from any candidate; resulting in a tensor of size 8 X 10 X 12721.
Rank 8 Topics
Rank 4 Topics
Rank 2 Topics
Topic 1	Topic 2	Topic 3	Topic 4	Topic 1	Topic 2	Topic 1
trump	senate	martinomalley	bemiesanders	marcorubio	trump	trump
hillary	florida	hillaryclinton	people	teammarco	hillary	hillary
donald	zika	realdonaldtrump	bernie	vote	donald	vote
president	Venezuela	campaigning	must	flsen	people	people
timkaine	nicolasmaduro	maryland	change	click	vote	donald
Topic 5	Topic 6	Topic 7	Topic 8	Topic 3	Topic 4	Topic 2
tedcruz	j ohnkasich	marcorubio	crooked	tedcruz	senate	tedcruz
cruz	kasich	teammarco	hillary	cruz	florida	cruz
ted	ohio	vote	thank	ted	zika	ted
internet	jθhn	fl sen	great	j ohnkasich	Venezuela	j ohnkasich
choosecruz	gov	click	clinton	kasich	nicolasmaduro	kasich
Figure 3: A three-layer Neural NCPD on the Twitter dataset at ranks r = 8, r(0) = 4 and r(0) = 2.
At each rank, we display the top keywords and topic heatmaps for candidate and temporal modes.
In Table 2, we display the relative reconstruction loss on the Twitter political dataset for all models.
We see that Neural NCPD significantly outperforms Standard HNCPD, slightly outperforms Standard
NCPD while offering a hierarchical topic structure, and outperforms all HNTF-i, for which loss
varies significantly based on the arrangement of the tensor. In Figure 3, we show the topic keywords
and factor matrices of a rank 8, 4, and 2 hierarchical NCPD approximation computed by Neural
NCPD. Note that in the rank 8 candidates mode factor and keywords we see that nearly every
topic is identified with a single candidate. Topic two of the rank 8 approximation aligns with
7
Under review as a conference paper at ICLR 2021
Table 2: Relative reconstruction loss, Crel, on the Twitter
political dataset for Neural NCPD, Standard NCPD,
Standard HNCPD, and HNTF at ranks r = 8, r(0) = 4,
and r(1) = 8. For HNTF we display the loss given the
three possible arrangements of the tensor.
Method	r = 8	r (0) = 4	r⑴=2
Neural NCPD	0.834	0.883	0.918
Standard NCPD	0.834	0.889	0.919
Standard HNCPD	0.834	0.931	0.950
HNTF-1	0.834	0.890	0.927
HNTF-2	0.834	0.909	0.956
HNTF-3	0.834	0.895	0.942
SOIdOI N au
SOIdOI 寸 4u
Figure 4: The S3(00 (top) and S(I)
(bottom) matrices produced by Neu-
ral NCPD on the Twitter dataset.
political issues (the Zika virus and the Venezuelan government) rather than a single candidate, and
is temporally most present in May to July 2016 (during the Zika outbreak and the Venezualan state
of emergency). Topics one and eight, corresponding to candidates Clinton and Trump, are most
present in the months immediately leading up to the election.
At rank 4, we see that topics one and four are inherited from
the rank 8 approximation, topic two combines the rank 8 topics
of candidates Trump and Clinton (final candidates), and topic
3 combined the topics of candidates Cruz and Kasich (Republi-
cans). Meanwhile, the rank 2 NCPD topics are nearly identical
to rank 4 NCPD topics two and three. We display HNTF for
each ordering of the tensor modes in Appendix A.
In Figure 4, we display the S30) (top) and S31) (bottom) matri-
ces produced by Neural NCPD on the Twitter dataset, which
illustrate how topics collect at each rank. We see topics 5 and
6 from the rank 8 factorization combine to form topic 3 at rank
4 and topic 2 at rank 2. This is expected because both topics in-
clude keywords from Cruz and Kasich, who had high presence
in topics 5 and 6 respectively in the rank 8 factorization.
Rank 4 Topics
Topic 1	Topic 2
trump	bemiesanders
hillary	people
j Ohnkasich	bernie
ohio	mustt
kasich	vote
Topic 3	Topic 4
crooked	tedcruz
hillary	cruz
thank	ted
marcorubio	internet
great	choosecruz
Rank 2 Topics
Topic 1
trump
hillary
vote
people
donald
Topic 2
tedcruz
cruz
ted
j ohnkasich
kasich
Clinton
Kaine
O'Malley
Sanders
Figure 5: Standard ranks 4 and 2
NCPD of the Twitter dataset. At
each rank, we display the top five
keywords and candidate and tempo-
ral mode heatmaps.
Clinton
Kaine
O'Malley
Sanders
In Figure 5, we display the results of performing separate NCPD
decomposition of ranks 4 and 2 on the Twitter dataset. We
see that the results are similar to those of Neural NCPD, but
these independent decompositions lack the clear hierarchical
structure provided by Neural NCPD. Note that while the topics
corresponding to Kasich and Clinton combine in the rank 4
NCPD, these candidates are present in different topics in the
rank 2 NCPD; Neural NCPD prevents this breach of hierarchy.
3.3	Video Data Analysis
We next apply Neural NCPD to video data constructed from a year-long time-lapse video of a forest;
see Figure 6 for a selection of frames and Figure 11 in Appendix A for more details. We extract
37 frames and flatten each frame (RGB image) into a single matrix, to form a tensor X of size
37 × 3 × 57600; here the first mode represents frames (temporal mode), the second colors (chromatic
mode), and the third pixels (spatial mode).
Figure 6: We display seven of 37 extracted frames from a year-long time-lapse video of a forest.
8
Under review as a conference paper at ICLR 2021
In Figure 7, we show the three-layer Neural NCPD decompositions of the video tensor with ranks 8,
6, and 3. For each rank, we plot the topics in the spatial (left), temporal (top right), and chromatic
(bottom right) modes. We note that many of the identified topics represent visual seasonal changes.
Topic six of the rank 8 decomposition represents the green and leafy late-summer to early fall. Topic
one of the rank 6 decomposition represents the winter sky and leafless trees. Topic three of the rank 3
decomposition represents the summer and fall sky and tree leaves.
Rank 6 Topics
Figure 7: A three-layer Neural NCPD of the time-lapse video at ranks r = 8, r0 = 6, and ri = 3.
We display topics at each rank for spatial (left), temporal (top right), and chromatic (bottom right)
modes. Relative reconstruction loss is 0.105, 0.109, and 0.122 respectively at each layer.
Rank 6 Topics
Rank 3 Topics
IbPiC 1____
Rank3 Topics
We additionally apply NMF to the slices of the tensor along a single mode. Slicing along the temporal
or spatial modes would make interpretation of the resulting topics challenging, so We choose to
slice along the chromatic mode, producing three matrices. In Figure 8, we visualize a rank 3 NMF
on each of the three chromatic slices of
the video tensor. The chromatic factoriza-
tions are nearly identical, illustrating lit-
tle salient dynamic information. While
similar to the rank 3 Neural NCPD layer,
the chromatic NMFs obscure much of the
chromatic interaction evidenced by Neu-
ral NCPD. In particular, Neural NCPD il-
lustrates the spatial and temporal dynam-
ics of multi-colored features and their co-
occurrence hierarchy, while NMF provides
only single-colored features and requires
far more work to glean multi-colored fea-
ture co-occurrence information.
Figure 8: A decomposition of the time-lapse video by
rank 3 NMF on slices of the tensor along the chromatic
mode. For each color, we display the three topics in
spatial (left) and temporal (right) modes. Relative re-
construction loss is 0.101.
4	Conclusions
In this paper, we introduced the hierarchi-
cal NCPD model and presented a novel algorithm, Neural NCPD, to train this decomposition. We
empirically demonstrate the promise of this method on both real and synthetic datasets; in particular,
this model reveals the hierarchy of topics learned at different NCPD ranks, which is not available to
standard NCPD or NMF-based approaches.
9
Under review as a conference paper at ICLR 2021
References
Ali Anaissi, Basem Suleiman, and Seid Miad Zandavi. NeCPD: An online tensor decomposition
with optimal stochastic gradient descent. arXiv preprint arXiv:2003.08844, 2020.
J. Douglas Carroll and Jih-Jie Chang. Analysis of individual differences in multidimensional scaling
via an N-Way generalization of “Eckart-Young" decomposition. PSychometrika, 35(3):283-319,
1970.
Andrzej Cichocki, Rafal Zdunek, and Shun-ichi Amari. Hierarchical als algorithms for nonnegative
matrix and 3d tensor factorization. In International Conference on Independent Component
AnalySiS and Signal Separation, pp. 169-176. Springer, 2007a.
Andrzej Cichocki, Rafal Zdunek, and Shun-ichi Amari. Nonnegative matrix and tensor factorization
[lecture notes]. IEEE Signal Proc. Mag., 25(1):142-145, 2007b.
Andrzej Cichocki, Rafal Zdunek, Anh Huy Phan, and Shun-ichi Amari. Nonnegative matrix and
tenSor factorizationS: applicationS to exploratory multi-way data analySiS and blind Source
Separation. John Wiley & Sons, 2009.
Jennifer Flenner and Blake Hunter. A deep non-negative matrix factorization neural netWork, 2018.
Unpublished.
Mengdi Gao, Jamie Haddock, Denali Molitor, Deanna Needell, Eli Sadovnik, Tyler Will, and Runyu
Zhang. Neural nonnegative matrix factorization for hierarchical multilayer topic modeling. In
Proc. Interational WorkShop on Computational AdvanceS in Multi-SenSor Adaptive ProceSSing,
2019.
Eric Gaussier and Cyril Goutte. Relation betWeen PLSA and NMF and implications. In Proc. 28th
ACM SIGIR Conference on ReSearch and Development in Information Retrieval, pp. 601-602,
2005.
Ian GoodfelloW, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep learning, volume 1.
MIT Press, 2016.
Lars Grasedyck. Hierarchical singular value decomposition of tensors. SIAM J. Matrix Anal. A., 31
(4):2029-2054, 2010.
David Guillamet and Jordi Vitria. Non-negative matrix factorization for face recognition. In Proc.
Catalonian Conference on Artificial Intelligence, pp. 336-344. Springer, 2002.
Richard A. Harshman et al. Foundations of the PARAFAC procedure: Models and conditions for an
“explanatory” multimodal factor analysis. 1970.
Patrik O. Hoyer. Non-negative sparse coding. In Proc. 12th IEEE WorkShop on Neural NetworkS for
Signal ProceSSing, pp. 557-565. IEEE, 2002.
Dongmin Kim, Suvrit Sra, and Inderjit S. Dhillon. Fast projection-based methods for the least squares
nonnegative matrix approximation problem. Stat. Anal. Data Min., 1(1):38-51, 2008.
Hyunsoo Kim and Haesun Park. Nonnegative matrix factorization based on alternating nonnegativity
constrained least squares and active set method. SIAM J. Matrix Anal. A., 30(2):713-730, 2008.
Jingu Kim, Yunlong He, and Haesun Park. Algorithms for nonnegative matrix and tensor factor-
izations: A unified vieW based on block coordinate descent frameWork. J. Global Optim., 58(2):
285-319, 2014.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Tamara G. Kolda and Brett W. Bader. Tensor decompositions and applications. SIAM Rev., 51(3):
455-500, 2009.
10
Under review as a conference paper at ICLR 2021
Tamara G. Kolda and David Hong. Stochastic gradients for large-scale tensor decomposition. arXiv
preprint arXiv:1906.01687, 2019.
Jean Kossaifi, Yannis Panagakis, Anima Anandkumar, and Maja Pantic. TensorLy: Tensor learning
in Python. CoRR, abs/1610.09555, 2018.
Charles L. Lawson and Richard J. Hanson. Solving least squares problems. SIAM, 1995.
Jonathan Le Roux, John R. Hershey, and Felix Weninger. Deep NMF for speech separation. In Proc.
Int. Conf. Acoust. Spee.,pp. 66-70. IEEE, 2015.
Daniel D. Lee and H. Sebastian Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401:788-791, 1999.
Daniel D. Lee and H. Sebastian Seung. Algorithms for non-negative matrix factorization. In Proc.
Adv. Neur. In., pp. 556-562, 2001.
Hyekyoung Lee, Jiho Yoo, and Seungjin Choi. Semi-supervised nonnegative matrix factorization.
IEEE Signal Proc. Let., 17(1):4-7, 2009.
Chih-Jen Lin. Projected gradient methods for nonnegative matrix factorization. Neural Comput., 19
(10):2756-2779, 2007.
Justin Littman, Laura Wrubel, and Daniel Kerchner. 2016 United States Presidential Election Tweet
Ids, 2016. URL https://doi.org/10.7910/DVN/PDI7IN.
Herbert Robbins and Sutton Monro. A stochastic approximation method. Ann. Math. Stat., pp.
400-407, 1951.
Ankan Saha and Vikas Sindhwani. Learning evolving and emerging topics in social media: a dynamic
NMF approach with temporal regularization. In Proc. ACM International Conference on Web
Search and Data Mining, pp. 693-702, 2012.
Farial Shahnaz, Michael W Berry, V Paul Pauca, and Robert J Plemmons. Document clustering using
nonnegative matrix factorization. Information Processing & Management, 42(2):373-386, 2006.
Eirik Solheim. One year in 40 seconds. URL https://www.youtube.com/watch?v=
lmIFXIXQQ_E&ab_channel=EirikSolheim.
Le Song, Mariya Ishteva, Ankur Parikh, Eric Xing, and Haesun Park. Hierarchical tensor decomposi-
tion of latent tree graphical models. In Proc. Int. Conf. Mach. Learn., pp. 334-342, 2013.
Xiaoxia Sun, Nasser M. Nasrabadi, and Trac D. Tran. Supervised multilayer sparse coding networks
for image classification. CoRR, abs/1701.08349, 2017. URL http://arxiv.org/abs/
1701.08349.
Abraham Traore, Maxime Berar, and Alain Rakotomamonjy. Non-negative tensor dictionary learning.
2018.
George Trigeorgis, Konstantinos Bousmalis, Stefanos Zafeiriou, and Bjorn W. Schuller. A deep
matrix factorization method for learning attribute representations. IEEE T. Pattern Anal., 39(3):
417-429, 2016.
M Alex O Vasilescu and Eric Kim. Compositional hierarchical tensor factorization: Representing
hierarchical intrinsic and extrinsic causal factors. arXiv preprint arXiv:1911.04180, 2019.
Max Welling and Markus Weber. Positive tensor factorization. Pattern Recogn. Lett., 22(12):
1255-1261, 2001.
Wei Xu, Xin Liu, and Yihong Gong. Document clustering based on non-negative matrix factorization.
In Proc. ACM SIGIR Conference on Research and Development in Informaion Retrieval, pp.
267-273, 2003.
11
Under review as a conference paper at ICLR 2021
A Appendix
In this supplementary material, we provide the details of the example derivative computations from
Section 2.2, give a justification of the NCPD expansion formula exploited in Section 2.1, and provide
further experimental results that we were not able to include in Section 3.
Example derivations
Here, we justify the derivations provided in the example in Section 2.2. We note that Anaissi et al.
(2020); Kolda & Hong (2019) provide similar derivations for the CP tensor decomposition, but their
decompositions do not attempt to further decompose the CP factor matrices, and thus, their results
are not sufficient for providing derivatives with respect to the A and S matrices. Consider the full
reconstruction loss function for the order-k tensor X,
C= kX- [[X1,X2,...,Xk]]k2F,
∂C
where for some fixed 1 ≤ i ≤ k, Xi = ABC and consider the gradient ^B. Let
Hi = Xk . . . Xi+1 Xi-1	. . . X1.
Now, we let X = [[X1, X2, . . . , Xk]]. Then, if X(i) is the mode-i matricization of X (see e.g., (Kolda
& Bader, 2009)), we have
Xf(i) = Xi(Xk	. . .	Xi+1	Xi-1	. . .	X1)>] = XiHi>.
Thus, ifwe let X(i) be the mode-i matricization ofX, we have that
C= kX(i)	-Xf(i)k2F	= kX(i)	-	(ABC)(Xk...Xi+1Xi-1	...X1)>k2F.
Now, we compute the desired gradient through a series of applications of the chain rule. We then see
that
∂C ∂C ∂Xi Hi> ∂ABC
...=----------：-----：---
∂B ∂XiH ∂ABC ∂B
=a-γ( ∂C	∂XiH> AC >
Id XiHi ∂Xi C
= 2A>(X(i)- XiHnHiC>.
∂C
Now, using the calculations above we can proceed In calculating-Q. Gao et al. (2019) show that
if (	Cj)) denotes derivative of C with respect to A('j), holding the S matrices constant, then
we have
∂C
∂Ai'j)
S
+ X u ('1,'2),j
'1≤'2≤L-2
1≤j≤r
where u('1,'2),j relates C to A('1) through S('2) and S('1), is defined column-wise (j), and de-
pends upon
*
,the derivative of C with respect to S('2) holding S('2+1)
.. , Si(L-2)
constant. Thus,
gradient (⅛Γ
dC .) 1 and ( dC,)1 are sufficient to calculate	dC .). We calculate the
∂Ai j	∂Si j	∂Ai j
S
where
C= kX- [[Xf1,Xf2,...,Xfk]]k2F
12
Under review as a conference paper at ICLR 2021
∂C
∂Afj)
and Xi = Ai0)A(1)...AiL-2)S(L-2). Since We can assume that A('1) is independent of all other
A’s and S’s, we have that
S
=2 (A(O)AiI)…A('jT)] (X(i)- XiH)Hi (A('j"…A(L-2)S(L-2)].
(18)
Now, we calculate ( ^dCTy )
S’s, We have that
(`j)
Since we can assume that Si j is independent of all other A’s and
(X(i)- XiH>)Hi.
(19)
Thus, we have the required derivatives to evaluate dC .)
∂Ai j
HNCPD expansion
We now provide brief justification of the expansion of the NCPD in terms of later factorizations used
in Section 2.1; that is,
[Xi, f2,…，Xk J= X	αjι,j2,...jk ((Al0))：,ji 0(a20) )：,j2 0...0(Ak0))"jk)
1≤j1 ,j2 ,...jk ≤r(0)
where αj1,j2,...jk = Prp=1(S1())j1,p(S2())j2,p . . . (Sk())jk,p.
We have that by definition,
r
[Xi, X2,…，Xk F = E ((Xi):,p 氧(X2):,p 氧...乳(Xk):,p).
p=1
We also have that Xfi = Ai(0)Si(0) for 1 ≤ i ≤ k, so we have that for each column p, 1 ≤ p ≤ r of
-~~■
Xfi ,
r(0)
(Xfi):,p =X(Si(0))j,p(Ai(0)):,j.
j=i
Thus, by the linearity of the outer product we have that
((XI):,p % (X2):,p % ... (Xk1,p) =	〉：	αp,j1,j2,…jk ((AI )1,ji % (A2 )1,j2 % ... % (Ak )},jk)
i≤j1,j2,...jk≤r(0)
whereαp,j1,j2,...jk = (Si(0))j1,p(S2(0))j2,p.. . (Sk(0))jk,p. Now, by noting that
αj1,j2,...,jk
r
αp,j1,j2,...,jk
p=i
we arrive at the original statement.
Synthetic Experiment
In this section, we provide the additional views of the synthetic tensor and computed approximations
from Section 3.1. In Figure 2 in the main text, for visualization we displayed the projection of each
tensor onto the third mode. In Figure 9, we display the projections of these tensors onto all three
modes. We see that due to the simple block structure used to produce the synthetic data tensor, the
three modes all tell a similar story; that is, Neural NCPD is able to recover meaningful structure along
all three modes.
13
Under review as a conference paper at ICLR 2021
Rank 7
Original
High
Low
Figure 9: Here We display the projections onto all three modes for the original data tensor X and
approximations of X at ranks r = 7, r(0) = 5, and r(1) = 3 produced by Neural NCPD, Standard
HNCPD, and HNTF at tWo levels of noise.
14
Under review as a conference paper at ICLR 2021
Temporal Document Analysis Experiment
HNTF-I
Topic 1	Topic 2
hillary	tedcruz
tmmp	cruz
realdonaldtrump	ted
crooked	internet
hillaryclinton	choosecruz
Topic 3	Topic 4
marcorubio berniesanders
teammarco	people
vote	bemie
flsen	must
click	change
Clinton
Kaine
O'Malley
Sanders
Cruz
Kasich
Rubio
Trump
1 2 3 4	1 2 3 4
Topic 1
trump
hillary
tedcruz
people
berniesanders
Topic 2
marcorubio
teammarco
vote
flsen
click
Clinton
Kaine
O'Malley
Sanders
Cruz
Kasich
Rubio
frump
HNTF-2
Topic 1	Topic 2
j ohnkasich	marcorubio
kasich	teammarco
ohio	vote
john	florida
gov	flsen
Topic 3	Topic 4
tedcruz	berniesanders
cruz	people
ted	bernie
internet	must
choosecruz	change
Topic 1
marcorubio
teammarco
vote
florida
flsen
Topic 2
berniesanders
people
bernie
must
change
Clinton
Kaine
O'Malley
Sanders
Cruz
Kasich
Rubio
Trump
1 2	1 2
HNTF-3
Topic 1	Topic 2
trump	marcorubio
hillary	teammarco
donald	vote
president	flsen
timkaine	click
Topic 3	Topic 4
berniesanders	tedcruz
people	cruz
bernie	ted
must	crooked
country	thank
Topic 1
tedcruz
cruz
ted
hillary
crooked
Topic 2
berniesanders
people
bernie
must
vote
Clinton
Kaine
O'Malley
Sanders
Cruz
Kasich
Rubio
Trump
Figure 10: Here We display a three-layer HNTF on the TWitter dataset from Section 3.2 at ranks
r = 8, r(0) = 4, and r(1) = 2, run separately for each of the possible ordering on the data tensor. We
display the top keywords and heatmaps of topics in the candidate and temporal modes at ranks 4 (left)
and 2 (right). We note that the rank 8 factorization is identical to that of Neural NCPD, so We do not
re-display it here (see section 3.2).
in Figure 10, We display the results from running HNTF on the TWitter dataset in section 3.2,
excluding the topics at rank 8 because they are identical to those learned by Neural NCPD (see
section 3.2). We see that While the factorization for the first possible ordering is similar to that
of Neural NCPD and contains significant meaningful topic modeling information, the other tWo
orderings lose significant information by the last layer and, and have topic presence and from only 2
or 3 of the eight candidates.
15
Under review as a conference paper at ICLR 2021
Video Data Experiment
Frame 1
Frame 2
Frame 3
Frame 7
Frame 8
rrame 9
Frame 10
Frame 11
f∙rame 12
Frame 13
Frame 14
Frame 15
Frame 16
Frame 17
Frame 18
Frame 19
Frame 20
Frame 21
Frame 22
Frame 23
Frame 24
Frame 25
Frame 26
Frame 27
Frame 28
Frame 29
Frame 30
Frame 31
Frame 32
Frame 33
Frame 34
Frame 35
Frame 36
Figure 11: Here we display the first 36 of 37 frames of the time lapse video dataset from Section 3.3
(The 37th frames is included in Figure 6
In Figure 11, we display the first 36 of 37 frames of the time lapse video dataset from Section 3.3
(the 37th frame is included in Figure 6) in order to make it clear how seasons progress throughout
the frames. We see that the video begins in the white winter months, transitions to spring at around
frame 16, and stays green until it transitions to fall around frame 28.
In Figure 12, we display the S3(0) matrix (top) and S3(1) matrix (bottom) produced by Neural NCPD
on the time-lapse video tensor described in Section 3.3. By examining the S matrices from our
Neural NCPD algorithm, we are also able to see the hierarchical relationship between the topics
from different ranks. In the S3(0) matrix, we see the hierarchical relationship between the rank 6 and
rank 8 topics. In the S3(1) matrix, we see the hierarchical relationship between the rank 3 and rank 8
topics. We note that the S3(0) matrix (top) illustrates that topic one of rank 6 NCPD is closely related
to topic eight of rank 8 NCPD, and S3(1) (bottom) similarly illustrates that topic two of rank 3 NCPD
is closely related to topic eight in rank 8 NCPD; these relationships are unsurprising because, as seen
in Figure 7 in the main text, these topics are present temporally during winter and fall and spatially in
the sky behind the trees.
16
Under review as a conference paper at ICLR 2021
Rank 8 Topics
Figure 12: The S3(00 matrix (top) and S3I) matrix (bottom) produced by Neural NCPD on the
time-lapse video tensor described in Section 3.3.
17