Under review as a conference paper at ICLR 2021
The Lipschitz Constant of Self-Attention
Anonymous authors
Paper under double-blind review
Ab stract
Lipschitz constants of neural networks have been explored in various contexts
in deep learning, such as provable adversarial robustness, estimating Wasserstein
distance, stabilising training of GANs, and formulating invertible neural networks.
Such works have focused on bounding the Lipschitz constant of fully connected or
convolutional networks, composed of linear maps and pointwise non-linearities.
In this paper, we investigate the Lipschitz constant of self-attention, a non-linear
neural network module widely used in sequence modelling. We prove that the
standard dot-product self-attention is not Lipschitz, and propose an alternative L2
self-attention that is Lipschitz. We derive an upper bound on the Lipschitz constant
of L2 self-attention and provide empirical evidence for its asymptotic tightness. To
demonstrate the practical relevance of our theoretical work, we formulate invertible
self-attention and use it in a Transformer-based architecture for a character-level
language modelling task.
1 Introduction
Lipschitz continuity is a strong form of continuity for functions. Loosely speaking, a function is
Lipschitz continuous if changing its input by a certain amount cannot change its output by more
than K times that amount. The constant K is a hard constraint on how rapidly the function’s output
can vary, and the smallest such K is known as the function's LiPschitz Constant. For example,
fι(χ) = a∕∣X∣^ and f2 (x) = exp(χ) for X ∈ R are not Lipschitz continuous, because their output can
change arbitrarily fast as x approaches 0 and +∞ respectively. On the other hand, g1(x) = tanh(x)
and g2 (x) = αx are Lipschitz continuous, because their rate of change (derivative) is bounded.
In deep learning, we often use Lipschitz continuity as a constraint for neural networks, to control
how much a network’s output can change relative to its input. Such Lipschitz constraints are useful
in several contexts. For example, Lipschitz constraints can endow models with provable robustness
against adversarial pertubations (Cisse et al., 2017; Tsuzuku et al., 2018; Anil et al., 2019), and
guaranteed generalisation bounds (Sokolic et al., 2017). Moreover, the dual form of the Wasserstein
distance is defined as a supremum over Lipschitz functions with a given Lipschitz constant, hence
Lipschitz-constrained networks are used for estimating Wasserstein distances (Peyre & Cuturi, 2019).
Further, Lipschitz-constrained networks can stabilise training for GANs, an example being spectral
normalisation (Miyato et al., 2018). Finally, Lipschitz-constrained networks are also used to construct
invertible models and normalising flows. For example, Lipschitz-constrained networks can be used as
a building block for invertible residual networks and hence flow-based generative models (Behrmann
et al., 2019; Chen et al., 2019). Additionally, Neural ODEs (Chen et al., 2018; Grathwohl et al.,
2019) are typically defined using vector fields parameterized via Lipschitz networks, so that the flow
generated by the vector field is guaranteed to exist for all times.
Nonetheless, designing Lipschitz-continuous neural networks and computing (or even upper-
bounding) their Lipschitz constant is a hard problem. Previous work mostly focused on fully-
connected and convolutional networks, not only because they are common in deep learning, but also
because they are relatively simple to analyze, as compositions of linear maps and pointwise non-
linearities. Even in this case however, exact evaluation of the Lipschitz constant of fully-connected
and convolutional networks is NP-hard (Virmaux & Scaman, 2018) and obtaining a tight upper bound
remains a challenging task (Virmaux & Scaman, 2018; Fazlyab et al., 2019; Latorre et al., 2020).
Fully-connected and convolutional networks are not the only neural networks worthy of interest.
Recently, self-attention (Vaswani et al., 2017) has become a popular alternative to recurrent neural
1
Under review as a conference paper at ICLR 2021
networks. Self-attention is a key component of the Transformer (Vaswani et al., 2017), that has
found success as a building block in models of various data modalities, starting with natural-language
processing (Vaswani et al., 2017; Devlin et al., 2019; Brown et al., 2020) and extending to computer
vision (Zhang et al., 2019; Parmar et al., 2019), audio generation (Huang et al., 2019), and reinforce-
ment learning (Parisotto et al., 2020). However, so far no previous work has analyzed the Lipschitz
properties of self-attention, and thus it has been unclear whether self-attention is a viable option in
applications that require Lipschitz constraints. In this work, we address this gap in the theory of
self-attention by providing a thorough analysis of its Lipschitz properties. In particular, we make the
following contributions:
•	We prove that the widely used dot-product self-attention is not Lipschitz, and therefore not suitable
to use in applications requiring Lipschitz constraints.
•	We formulate L2 self-attention as an alternative, and show that it is Lipschitz.
•	We derive a theoretical upper bound on the Lipschitz constant of L2 self-attention, and provide
empirical evidence of the asymptotic tightness of the bound.
•	As a practical demonstration of the theory, we use this bound to formulate invertible self-attention,
and explore its use in a Transformer architecture for character-level language modelling.
2 Lipschitz Constant of Fully-Connected/Convolutional Layers
We first define the notion of Lipschitz continuity, and proceed to define the Lipschitz constant.
Definition 2.1. Given two metric spaces (X , dX) and (Y, dY), a function f : X → Y is called
Lipschitz continuous (or K-Lipschitz) if there exists a constant K ≥ 0 such that
dY (f (x), f(x0)) ≤ KdX (x, x0) for all x, x0 ∈ X.	(1)
The smallest such K is the Lipschitz constant of f, denoted Lip(f).
In this paper, we focus on the common case where X = Rn, Y = Rm, and dX, dY are induced by
a p-norm kxkp := (Pi |xi|p)1/p. We will primarily consider the cases p = 2 and p = ∞, where
kxk∞ := maxi |xi|. To emphasise the dependence of the Lipschitz constant on the choice of p-norm,
we will often denote it by Lipp(f). In this case, it follows directly from Definition 2.1 that the
Lipschitz constant is given by
Lipp(f) =	sup
x6=x0 ∈Rn
kf(x)-f(x0)kp
kx - x0kp
(2)
Next, we outline some basic results that are useful for estimating Lipschitz constants, also covered
in related works (Virmaux & Scaman, 2018; Behrmann et al., 2019). We describe how these
results are used to provide bounds on the Lipschitz constant of fully-connected networks (FCN) and
convolutional neural networks (CNN), using the fact that both are compositions of linear maps and
pointwise non-linearities. To begin with, the following theorem suggests a way to bound Lipp(f) for
a differentiable Lipschitz function f :
Theorem 2.1 (Federer, 1969). Let f : Rn → Rm be differentiable and Lipschitz continuous
under a choice of P-norm k ∙ kp. Let Jf (x) denote its total derivative (Jacobian) at X. Then
Lipp(f) = supx∈Rn kJf (x)kp where kJf (x)kp is the induced operator norm on Jf (x).
Hence if f is a linear map represented by a matrix W then
Lipp(f) =	kWkp := sup	kWxkp	= σmax(PW),	ifp=2	(3)
p	kxkp=1	maxi j |Wij|	ifp = ∞
where kW kp is the operator norm on matrices induced by the vector p-norm, and σmax (W) is the
largest singular value of W. Under this choice of norm, many common non-linearities (including
relu, sigmoid, tanh, elu) are 1-Lipschitz. kW k2 = σmax(W) is usually estimated via power
iteration; we provide details on how this is done in Appendix B.
Since we now know the Lipschitz constants of the components of both FCN and CNN, we can bound
their Lipschitz constants by applying the following lemma:
Lemma 2.1 (Federer, 1969). Let g, h be two composable Lipschitz functions. Then g ◦ h is also
Lipschitz with Lip(g ◦ h) ≤ Lip(g) Lip(h).
2
Under review as a conference paper at ICLR 2021
Corollary 2.1. For a fully-connected network (FCN) or a convolutional neural network (CNN)
f = WK ◦ ρK-1 ◦ WK-1 ◦ . . . ◦ ρ1 ◦ W1, we have Lipp (f) ≤ Qk kWk kp under a choice of p-norm
with 1-Lipschitz non-linearities ρk .
The above bound is not necessarily tight; there are various works that compute tighter bounds for
FCN and CNN (e.g. Virmaux & Scaman, 2018; Fazlyab et al., 2019; Latorre et al., 2020).
3 Lipschitz Constant of Self-Attention
3.1 DOT-PRODUCT SELF-ATTENTION IS not LIPSCHITZ
Moving on, we investigate whether self-attention is Lipschitz. We first consider the widely used
(scaled) dot-product multihead self-attention as formulated by Vaswani et al. (2017). Let x1, . . . , xN
be a sequence of N elements, where xi ∈ RD for i = 1, . . . , N. We represent this sequence as a
matrix X ∈ RN×D such that the ith row of X is the ith element of the sequence, i.e. Xi: = xi> .
Dot-product multihead self-attention (DP-MHA) is a map from RN×D to RN×D consisting of H
’heads'，where H is chosen to divide D. Each head is amap from RN×D to RNxD/H defined by
DP(X) := SoftmaX (XWQ(XWKFNDlH) XWV,	(4)
where W Q,W K ,W V ∈ RD×D/H are learnable parameters specific to each head. The input to the
softmax is an N × N matrix of dot products (hence dot-product self-attention), and the softmax is
applied to each row of this matrix. Finally, the outputs of all heads are concatenated into an N × D
matrix and are right multiplied by WO ∈ RD×D, thus DP-MHA is defined by
MHA(X) := DP1(X),...,DPH(X) WO.	(5)
In what follows, we will prove that MHA as defined above is not Lipschitz, assuming that the MHA
map is non-trivial, i.e. WQ, WK, WV, WO 6= 0. It is sufficient to show that a single head DP is not
Lipschitz, since MHA is a linear combination of the outputs of each head. Let us write Equation (4)
as DP(X) = PXWV, where P ∈ RN×N is the output of the softmax (we suppress the dependence
of P on X to reduce clutter below). P is a stochastic matrix, i.e. its entries are non-negative and its
rows sum to 1. Since the rows of X are the xi’s, a linear transformation of each xi by some matrix A
is equivalent to right multiplication of X by A>. So right multiplication of X by WV is a linear map
and thus Lipschitz. Therefore, we are interested in the mapping f(X) = PX; this is not a linear
mapping because P itself is a non-linear function of X . In fact, we show that f is not Lipschitz, thus
proving the first main result of the paper:
Theorem 3.1. DP-MHA is not Lipschitzfor any vector P-norm ∣∣ ∙ ∣∣p with P ∈ [1, ∞].
Summary of Proof. We use Theorem 2.1, noting that if the supremum of the norm of the Jacobian is
infinite, then the mapping is not Lipschitz. In particular, we show that when xi = 0 for some i, some
elements of the Jacobian of f grow proportionally to the sample variance of x6=i , which is unbounded.
Proof. We show the proof for the case D = 1 (i.e. X ∈ RN×1, a column vector) for readability. See
Appendix C for the general case, which follows the same logic.
The mapping f can be written as	r f (x )> -
f(X) = PX =SoftmaX (aXXT) X =	.	∈ RN×1,	(6)
fN(X)>
where a = WKWQ ∈ R (we assume a 6= 0 such that self-attention is non-trivial) and fi (X) =
PjN=1 Pij xj with Pi>: = SoftmaX (axiX). Hence f can be interpreted as a map of each xi to a point
in the convex hull ofx1, ..., xN. Since f is a map from RN×1 to RN×1, its Jacobian is
J11	. . . J1N
Jf =	.	...	.	∈ RN×N
JN 1	. . .	JNN
(7)
where Jij = dfiχ ∈ R.	By taking
aX >P(i) [ejiX + δij X] + PijI where eij
partial derivatives we can show that Jij =
∈ RN ×N is a binary matrix with zeros everywhere
3
Under review as a conference paper at ICLR 2021
except the (i, j)th entry, δij is the Kronecker delta, and P(i) := diag(Pi:) - Pi>: Pi:. So for i = j:
Jii = aX>P(i)eiiX + aX>P(i)X + Pii	(8)
Let us investigate the scalar X>P (i)X. We observe that it is in fact a variance ofa discrete distribution.
Specifically:
X>P(i)X = Pk Pikx2k - (Pk Pikxk)2 = Var(X),	(9)
where X is a discrete distribution with support at the inputs {x1, . . . , xN} and probability mass
function given by their softmax probabilities P(X = xj) = Pij. A consequence of this interpretation
is that P(i) is positive semi-definite (PSD) since X>P(i)X = Var(X) ≥ 0, with equality if and only
if the xj are all equal.
We use this observation to show that Jii is unbounded, and so kJf kp is unbounded, hence DP-MHA
is not Lipschitz. Consider the case Xi = 0. Then P> = Softmax (XAxi) = N1, i.e. We have
uniform attention regardless of x6=i. The first term of Jii in Equation (8) disappears since eiiX =
[0,...,xi,..., 0] = 0, and the last term becomes NI. Now consider the second term aX> P(i) X =
aVar(Xl). Note X is uniformly distributed, since P(X = xj) = Pij = 1/N. Hence the second term
is equal to a times the sample variance of xι,...,xN, which can be arbitrarily large.	□
High-level intuition for proof. At xi = 0, fi(X) = N Pk xk, the mean of the inputs. The rate of
change of fi is governed by how fast the softmax saturates when xi is perturbed, which is determined
by how spread out the x6=i are. The more spread out they are (the higher the sample variance), the
greater the rate of saturation of the softmax, and the faster the rate of change of fi . Since the sample
variance of x6=i can be arbitrarily large, the rate of change of fi can also be arbitrarily large, i.e. the
entries of the Jacobian (and hence its p-norm) can become arbitrarily large. In Appendix D, we show
that adding bias terms to xi> WQ and xj>WK does not resolve the issue.
The implications of this result are the following. (1) There can be undesirable behaviour (e.g. training
instabilities) for the Transformer when some inputs are close to zero. (2) Dot-product self-attention
(and hence the standard Transformer) is not a suitable choice when we require a Lipschitz neural
network, such as for formulating invertible residual networks (Behrmann et al., 2019). Therefore, to
use self-attention and Transformers in such applications, a Lipschitz formulation of self-attention is
required, together with an explicit (ideally tight) upper bound to its Lipschitz constant, to quantify
how much the output can change with respect to changes in the input.
One method to make dot-product self-attention Lipschitz is by ensuring its inputs are bounded.
Indeed, if the input space is compact, e.g. [0, 1]N×D, any continuously differentiable function is
Lipschitz, including dot-product self-attention. However, as we further discuss in Section 6, such an
approach has its own challenges, since it makes the Lipschitz constant depend on the input range.
Instead, in the next section we formulate a version of self-attention that is provably Lipschitz on all
ofRN×D, allowing us to derive an upper bound that holds for any subset ofRN×D.
3.2 L2 self-attention: a Lipschitz formulation of self-attention
The pathology in dot-product self-attention arises because the softmax probabilities Pi: are constant
with respect to x6=i when xi = 0. This behaviour can be undesirable as we want Pij to vary according
to xj , regardless of whether xi is zero or not. Hence we propose an alternative form of self-attention
based on L2 distance:
Pij <x exp(Lij) := exp (-∣∣x>W Q - x> W K∣∣2 IpDIH) ,	(10)
with the normalisation constant ensuring that Pj Pij = 1. We will refer to it as L2 self-attention. It is
reminiscent of the standard squared-exponential kernel, but with softmax normalisation that ensures
that each row of the kernel matrix sums to 1. Normalisation is usually necessary to deal with inputs
of varying length N (Wang et al., 2018), hence we keep the softmax for L2 self-attention. Similarly
to dot-product self-attention, L2 self-attention can be computed efficiently with matrix operations;
see Appendix E for details, with a comparison of wall-clock runtimes between different choices of
attention.
We first state the mathematical formulation ofL2 multihead self-attention (L2-MHA) before proving
the main result ——the upper bound of its Lipschitz constant with respect to ∣∣ ∙ kp for P = 2, ∞. The
4
Under review as a conference paper at ICLR 2021
full L2-MHA map F : RN×D → RN×D is defined as
F(X) := f 1(X)W V,1, . . . ,fH(X)WV,H WO where fh(X) := PhXAh.
In the above, W V,h ∈ Rdxd/h, W O ∈ RD×D, Ph is defined as in Equation (10) with W Q,h =
WK,h ∈ Rd×"h, and Ah := WQ,hWQ,h>/pD/H ∈ RD×D. There are two changes from the
usual form of multihead self-attention:
(1)	We require WQ,h = WK,h for each head fh (X) to be Lipschitz. In Lemma F.1 of Appendix F
we show that L2-MHA is not Lipschitz for arbitrary WQ,h, WK,h, and that tying WQ,h = WK,h
is sufficient for L2-MHA to be Lipschitz, with intuition for why tying is sufficient.
(2)	In each head of the self-attention fh(X), right multiplication by Ah has been included for the
theorem below to hold (details are in the proof). In practice, there is little harm done by this
extra linear transformation, since when the heads are combined together in F, each fh (X) is
additionally transformed by WV,h, a free parameter.
The second main result of the paper is the following:
Theorem 3.2. L2-MHA is Lipschitz, with the following bound on Lip∞(F):
Lip∞(F) ≤ (4φ-l(N - 1) + PDH) max IWQ,hk∞kWQ，h> k∞ max ∣∣WV，h> k∞ ∣∣WO> k∞
and the following bound on Lip2 (F):
Lip2(F) ≤ PDNh (4φ-1(N - 1) + 1) QP kWQ，hk2 kWV,hk2) ∣WOk2
where φ(x) := x exp(x + 1) is an invertible univariate function on x > 0, andN is the input
sequence length.
Specifically, φ-1(N — 1) = W0( N) where W° is the Lambert W -function, which grows Sub-
logarithmically as O(log N - log logN ) (Corless et al., 1996). Hence the above bounds can be
Simplified to Ο(log N) for P = ∞ and O(√Nlog N) for P = 2.
Proof. See Appendix F, which uses the key observation that X>P(i)X is a covariance matrix (c.f.
Equation (9)) to bound kJF kp , the norm of the Jacobian of F. Appendix G shows how the argument
can be modified to prove the analogous result for the case with masking in the self-attention. □
These bounds are complemented by the concurrent work of Vuckovic et al. (2020), which provides a
Ο(√D log N) bound on LiPI(F) using measure-theoretic tools.
4	Application: Invertible S elf-Attention
4.1	Invertible residual network
Consider the residual function g(x) := x + f (x). Behrmann et al. (2019) give the following sufficient
condition for its invertibility: if f is a contraction with respect to some metric, i.e. if LiP(f) < 1,
and the metric space on which f is defined is complete, then g is invertible. (A Euclidean space with
a metric induced by a p-norm ∣∣ ∙ ∣∣p for P ∈ [1, ∞] is always complete.) Specifically, the inverse
g-1 (y) is the unique fixed point of the recursion xi+1 := y - f(xi), since by the definition of the
inverse we have y = g-1 (y) + f(g-1 (y)). Because f is a contraction, Banach’s Fixed Point Theorem
guarantees that this fixed point exists and is unique for all y , and that the recursion converges for all
initial values x0 (often set to y in practice) exponentially fast. Hence the inverse can be computed to
arbitrary accuracy (up to numerical precision in practice) by the above fixed-point iteration.
Note that a composition of such invertible residual blocks is also invertible. Behrmann et al. (2019)
use this observation to design invertible ResNets: they take f to be a CNN normalised by an upper
bound on LiP(f) given by Corollary 2.1, making the resulting function contractive. For the 2-norm
k ∙ ∣2, a hyperparameter c < 1 is chosen and each linear map (convolution) W in the CNN is
multiplied by c/k W ∣∣2 if c < ∣∣ W ∣∣2 where ∣∣W ∣∣2 is estimated by power iteration (c.f. Appendix B).
This multiplicative factor determines the scale of the Lipschitz constant of the normalised function.
5
Under review as a conference paper at ICLR 2021
4.2	Invertible self-attention
The standard use case of self-attention is with a skip connection inside the
Transformer. A Transformer block is composed of residual blocks of multihead
self-attention (MHA) and fully-connected (FCN) layers (Figure 1). Hence
similarly to invertible ResNets, we can normalise L2-MHA by the upper bounds
given in Theorem 3.2 to obtain Contractive-L2-MHA f, with which we
can obtain invertible self-attention g(x) = x + f (x).
In the next section, we investigate the properties of invertible self-attention
and how it compares with the standard dot-product self-attention; we replace
DP-MHA in the Transformer with Contractive-L2-MHA, hence replacing
the residual self-attention module with invertible self-attention. We are not
interested in the modified Transformer per se, but rather in comparing the
properties of invertible self-attention to standard self-attention — we only use
the Transformer as a testbed for this purpose, since self-attention is commonly
used in a Transformer. Given the theoretical focus of the paper, we believe that
a more challenging application of invertible self-attention, such as normalising
flow-based modelling, would be more suitable as a separate paper focused
on that particular application. In Appendix H, we show that Dropout in the
residual branch is also contractive.
5	Experimental Results
5.1	ASYMPTOTIC TIGHTNESS OF THE UPPER BOUND ON Lip∞ (F)
Figure 1: A Trans-
former block.
A tight bound on the Lipschitz constant of self-
attention is desirable for all listed applications
in Section 1; it leads to tighter generalisation
bounds, lighter constraints for provable robust-
ness, and better expressiveness in residual flow
models. Hence we investigate the tightness of
our bound on the Lipschitz constant of L2-MHA.
The Lipschitz constant is a supremum over the
space of inputs X ∈ RN ×D (c.f. Equation
(2)) and approximating it requires solving an
intractable optimisation problem. Hence it is
infeasible to estimate accurately in general, es-
pecially when X is high-dimensional. However,
we may compute a lower bound on the Lipschitz
N
Figure 2: Lower and upper bound on Lip∞ (f) for
L2-MHA f, with H = D = 1 and varying N.
constant by maximising the norm of the Jacobian kJf (X)k with respect to X until convergence. This
local optimum will form a lower bound by Theorem 2.1, and we can expect this lower bound to be
fairly tight for the low-dimensional case, provided the optimisation is thorough.
We use this observation to provide empirical evidence for the asymptotic tightness of the upper
bound on Lip∞ (f) in Theorem 3.2. In Figure 2, we show the upper bound as well as the lower
bound on Lip∞(f) obtained by optimising kJf (X)k∞ with respect to X for L2-MHA f with 50
different random initialisations of X, with H = D = 1 and N varying between 100 and 1000. See
Appendix I for further details. Note that we use a log-scale for the x-axis, and recall that the upper
bound is O(log N - log log N), dominated by the O(log N) term for large N. Hence the plot for the
upper bound shows a linear trend. We also observe that the slope of the lower bound is very similar,
providing empirical evidence that the O(log N - log log N) upper bound is asymptotically tight.
There are at least two possible explanations for the gap between the upper and lower bounds. (1) The
lower bound is only a local optimum — the true Lipschitz constant is a global optimum across inputs,
which can be difficult to attain especially for high values of N . (2) The multiplicative constant of the
upper bound may be loose. Assuming asymptotic tightness, it remains an open question whether the
multiplicative constant can be tightened. We show the analogous plot for Lip2 (F) and discuss the
results in Appendix K. Additionally in Appendix L, we show that optimising kJf (X)k∞ w.r.t. X
for DP-MHA f causes the norm to diverge, providing empirical verification of Theorem 3.1, that
DP-MHA is indeed not Lipschitz.
6
Under review as a conference paper at ICLR 2021
5.2	Numerical invertibility of MHA residual map
Recall from Section 4.1 that g(x) = x + f (x)
is invertible if f is contractive. Hence if f is
Contractive-L2-MHA, g is necessarily in-
vertible. However, technically we do not dis-
prove the invertibility of DP-MHA, since the
converse does not hold in general i.e. if f is
DP-MHA, which we have shown is not Lipschitz
hence not contractive, it may still be the case
that g is invertible. To verify that DP-MHA (with
the skip connection) is not invertible in practice,
we compare the numerical invertibility of the
residual map g(x) = x + cf (x) between the
cases where f is L2-MHA and DP-MHA in Fig-
ure 3. For each, we take MHA with 8 heads and
Figure 3: Invertibility of g(x) = x + cf (x) where
f is L2-MHA (left) and DP-MHA (right).
randomly initialised weights, and quantify the
maximum reconstruction error across a batch of 128 inputs whose outputs are inverted via the
fixed-point iteration described in Section 4.1. We use N = 64, D = 64, and c ∈ {0.5, 0.7, 0.9}
(see Appendix J for analogous results for a wider range of N and D and for DP-MHA with trained
weights). To highlight the difference between the two types of self-attention, recall in the proof of
Theorem 3.1 (showing that DP-MHA is not Lipschitz) that when one of the inputs xi is 0, some terms
of the Jacobian grow with the sample variance of x6=i . Hence we check numerical invertibility at
a set of N inputs where xi = 0 and x6=i are chosen uniformly at random. In Figure 3, we see that
DP-MHA is not invertible whereas L2-MHA is invertible for sufficiently small c. This shows how
not having the theoretical guarantee of f being contractive can cost us invertibility in practice. We
note that the figure shows local invertibility at the sampled inputs, as opposed to global invertibility
across the whole input space, yet this clearly highlights the difference between the two choices of
self-attention. Experiments with the globally invertible self-attention obtained by normalising with
the Lipschitz upper bound are provided in the next section.
5.3	Expressiveness of L2-MHA and invertible self-attention
A natural question to ask is: how does the expressiveness of L2-MHA and Contractive-L2-MHA
(that leads to invertible self-attention with the skip connection) compare with the original DP-MHA?
We expect that the Lipschitz constraint will limit the expressiveness of the Transformer, and would
like to find out by how much. We investigate this by comparing the performance of the original
Transformer and the Transformer with invertible self-attention (c.f. Figure 1) at character-level
language modelling on the Penn Treebank dataset (Marcus et al., 1993). We compare the test
negative log-likelihood (NLL) of a baseline LSTM, the original Transformer (DP-MHA), and a
series of models between the original Transformer and the Transformer with invertible self-attention
(Contractive-L2-MHA), making one change at a time and tuning the hyperparameters on a
validation set. For Contractive-L2-MHA, we normalise L2-MHA by the bound on Lip∞ (F) as
it is tighter than the bound on Lip2(F). See Appendix I for experimental details.
20 40 60 80 100
epoch
Original Transformer (DP)
20 40 60 80 100
epoch
Figure 4: Test NLL curves during training for various LSTM/Transformer models.
Transformer (Contractive-L2),
7
Under review as a conference paper at ICLR 2021
The results are shown in Figure 4. The first plot shows the best performing LSTM reaching a test
NLL of around 1.0, and the second plot shows the best performing Transformer reaching a slightly
improved performance for 3-5 layers of Transformer blocks. We observe instabilities in training for a
higher number of layers, requiring careful tuning of the learning rate schedule for stability at the cost
of performance, a commonly observed phenomenon in the literature of deep Transformer architectures
(Bapna et al., 2018; Parisotto et al., 2020). The third plot shows results for the Transformer with
DP-MHA replaced with L2-MHA but without tying WQ and WK, and we observe a very similar
test performance. The fourth plot shows the change when we further tie the query and key weights
(making WQ = WK); we see that there is a small degradation in performance. Here the number of
trainable parameters has been reduced, but in Appendix M we show that matching parameter count
does not help performance, suggesting that the reduction in performance when tying queries and keys
is not solely due to having fewer parameters. We note that performance saturates at around 5 layers
for each Transformer model so far. On the rightmost plot we show results when further dividing
self-attention in each block by the upper bound on Lip∞ (F), to obtain invertible self-attention. This
does give reduced performance for the same number of layers, but we can attain similar performance
with more layers, no longer saturating at 5 layers.
Thus we conclude the following. (1) Replacing the dot-product with the L2 distance incurs hardly
any loss in expressiveness. (2) Tying the query and key weights to obtain Lipschitz self-attention
incurs a small loss in expressiveness. (3) Dividing by the upper bound on Lip∞ (F) to obtain
invertible self-attention incurs a noticeable loss in expressiveness, but also has a stabilising effect
on the optimisation of the Transformer, thus allowing one to compensate for the apparent loss in
expressiveness by increasing the number of layers. We show further experimental results that compare
the training stability of DP-MHA and (Contractive)-L2-MHA in Appendix N.
6	Conclusion and Discussion
We have shown that the widely used dot-product self-attention is not Lipschitz, and that the proposed
L2 self-attention is Lipschitz, by deriving an O(log N-log log N) Lipschitz bound forp = ∞ and an
O(√N(log N-log log N)) bound forP = 2, where N is the input sequence length. We also provided
empirical evidence of the asymptotic tightness of the bound for p = ∞. Finally we demonstrated
that Lipschitz-constrained self-attention can be used to formulate invertible self-attention, which we
experimentally evaluated on a character-level language modelling task.
Our approach to Lipschitz self-attention has been to replace the dot-product kernel with an L2 kernel.
An alternative would be to constrain the inputs of self-attention to be bounded; if the input space is
compact, e.g. [0, 1]N×D, any continuously differentiable function is Lipschitz, including dot-product
self-attention. However, while being simple to implement, this solution has its own difficulties.
First, it makes the Lipschitz constant depend on the range of the input, and thus obtaining a tight
bound would require non-trivial mathematical work. We stress that a guarantee that the function is
Lipschitz does not tell us anything about its Lipschitz constant; without a tight Lipschitz bound, the
true Lipschitz constant can be very large, at which point it is unhelpful that the function is Lipschitz.
Second, since self-attention is typically applied at multiple layers within a model (e.g. Transformer),
the input to each self-attention will live in a different compact set that depends on the parameters
of the previous layers, complicating the analysis for subsequent layers. A solution is to constrain
the inputs of each layer to be in the same compact set, e.g. by passing them through a sigmoid
non-linearity. This however can have undesirable side effects such as vanishing gradients when the
sigmoids are saturated (Hochreiter, 1998). Despite these difficulties, this could be a worthwhile
alternative route for obtaining Lipschitz self-attention to explore in the future.
Having a provably Lipschitz self-attention module at our disposal makes it possible to use Transformer-
based architectures in applications requiring Lipschitz constraints, while enjoying theoretical guaran-
tees. A natural application of Lipschitz self-attention is for residual flows (Behrmann et al., 2019),
and for parameterising Neural ODEs (Chen et al., 2018) where a Lipschitz vector field guarantees
the existence of a unique solution to the ODE for all times. These models can be used for density
estimation and generative modelling of sets. Another interesting direction for future work would
be to analyse different variants of self-attention based on kernels other than dot-product and L2, as
Tsai et al. (2019) do from an experimental perspective, for which we believe the mathematical tools
developed in this paper may aid the analysis.
8
Under review as a conference paper at ICLR 2021
References
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A.,
Dean, J., Devin, M., et al. Tensorflow: Large-scale machine learning on heterogeneous distributed
systems. arXiv preprint arXiv:1603.04467, 2016.
Anil, C., Lucas, J., and Grosse, R. Sorting out Lipschitz function approximation. In International
Conference on Machine Learning, pp. 291-301, 2019.
Bapna, A., Chen, M. X., Firat, O., Cao, Y., and Wu, Y. Training deeper neural machine translation
models with transparent attention. In Proceedings of the 2018 Conference on Empirical Methods
in Natural Language Processing, pp. 3028-3033, 2018.
Behrmann, J., Grathwohl, W., Chen, R. T. Q., Duvenaud, D., and Jacobsen, J.-H. Invertible residual
networks. In Proceedings of the 36th International Conference on Machine Learning, 2019.
Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A.,
Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. arXiv preprint
arXiv:2005.14165, 2020.
Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural ordinary differential
equations. In Advances in Neural Information Processing Systems, pp. 6571-6583, 2018.
Chen, R. T. Q., Behrmann, J., Duvenaud, D., and Jacobsen, J.-H. Residual flows for invertible
generative modeling. In Advances in Neural Information Processing Systems, 2019.
Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and Usunier, N. Parseval networks: Improving
robustness to adversarial examples. In Proceedings of the 34th International Conference on
Machine Learning, pp. 854-863, 2017.
Corless, R. M., Gonnet, G. H., Hare, D. E., Jeffrey, D. J., and Knuth, D. E. On the Lambert W
function. Advances in Computational mathematics, 5(1):329-359, 1996.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT: Pre-training of deep bidirectional
transformers for language understanding. In Proceedings of the 2019 Conference of the North
American Chapter of the Association for Computational Linguistics, pp. 4171-4186, 2019.
Fazlyab, M., Robey, A., Hassani, H., Morari, M., and Pappas, G. Efficient and accurate estimation
of Lipschitz constants for deep neural networks. In Advances in Neural Information Processing
Systems, pp. 11423-11434, 2019.
Federer, H. Geometric Measure Theory. Classics in Mathematics. Springer Berlin Heidelberg, 1969.
ISBN 9783642620102.
Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks.
In Proceedings of the thirteenth international conference on artificial intelligence and statistics,
pp. 249-256, 2010.
Grathwohl, W., Chen, R. T. Q., Betterncourt, J., Sutskever, I., and Duvenaud, D. FFJORD: Free-form
continuous dynamics for scalable reversible generative models. In International Conference on
Learning Representations, 2019.
Hochreiter, S. The vanishing gradient problem during learning recurrent neural nets and problem
solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):
107-116, 1998.
Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Simon, I., Hawthorne, C., Shazeer, N., Dai, A. M.,
Hoffman, M. D., Dinculescu, M., and Eck, D. Music Transformer. In International Conference on
Learning Representations, 2019.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In 3rd International
Conference on Learning Representations, 2015.
Latorre, F., Rolland, P., and Cevher, V. Lipschitz constant estimation of neural networks via sparse
polynomial optimization. In International Conference on Learning Representations, 2020.
9
Under review as a conference paper at ICLR 2021
Marcus, M. P., Marcinkiewicz, M. A., and Santorini, B. Building a large annotated corpus of English:
ThePennTreebank. Computational Linguistics,19(2):313-330, 1993.
Mises, R. and Pollaczek-Geiringer, H. Praktische verfahren der gleichungsauflosung. ZAMM-Journal
ofApplied Mathematics and Mechanics/ZeitSchriftfUrAngewandte Mathematik und Mechanik, 9
(2):152-164, 1929.
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y. Spectral normalization for Generative
Adversarial Networks. In International Conference on Learning Representations, 2018.
Parisotto, E., Song, H. F., Rae, J. W., Pascanu, R., Gulcehre, C., Jayakumar, S. M., Jaderberg, M.,
Kaufman, R. L., Clark, A., Noury, S., Botvinick, M. M., Heess, N., and Hadsell, R. Stabilizing
Transformers for reinforcement learning. In International Conference on Machine Learning, 2020.
Parmar, N., Ramachandran, P., Vaswani, A., Bello, I., Levskaya, A., and Shlens, J. Stand-alone
self-attention in vision models. In Advances in Neural Information Processing Systems, pp. 68-80,
2019.
Peyra G. and Cuturi, M. Computational optimal transport. Foundations and Trends in Machine
Learning, 11(5-6):355-607, 2019.
Sokolic, J., Giryes, R., Sapiro, G., and Rodrigues, M. R. Robust large margin deep neural networks.
IEEE Transactions on Signal Processing, 65(16):4265-4280, 2017.
Tsai, Y.-H. H., Bai, S., Yamada, M., Morency, L.-P., and Salakhutdinov, R. Transformer dissection:
An unified understanding for Transformer’s attention via the lens of kernel. In Proceedings of the
2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing, pp. 4344-4353, 2019.
Tsuzuku, Y., Sato, I., and Sugiyama, M. Lipschitz-margin training: Scalable certification of perturba-
tion invariance for deep neural networks. In Advances in Neural Information Processing Systems,
pp. 6541-6550, 2018.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, E., and
Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems,
pp. 5998-6008, 2017.
Virmaux, A. and Scaman, K. Lipschitz regularity of deep neural networks: analysis and efficient
estimation. In Advances in Neural Information Processing Systems, pp. 3835-3844, 2018.
Vuckovic, J., Baratin, A., and Tachet des Combes, R. A mathematical theory of attention. arXiv
preprint arXiv:2007.02876, 2020.
Wang, X., Girshick, R., Gupta, A., and He, K. Non-local neural networks. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 7794-7803, 2018.
Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A. Self-attention Generative Adversarial
Networks. In Proceedings of the 36th International Conference on Machine Learning, pp. 7354-
7363, 2019.
10
Under review as a conference paper at ICLR 2021
A Chain Rule for vector valued Functions
In this section, we list some useful identities for deriving the Jacobians of the expressions in the
paper.
Suppose λ is a scalar, u, v, x are column vectors, and f (u) is a vector valued function. We use the
standard convention that for a ∈ Rm, b ∈ Rn, we have ∂a ∈ Rm×n. Then We have the following
chain rule identities:
•	# [λu] = λ ∂u + U ∂λ
∂x	∂x ∂x
•	df (U) = df (U) ∂u
∂x 一 ∂u ∂x
•	等[u>v] = u> ∂v + v> ∂u
∂x	∂x	∂x
NOte * isa row VeCtor, so U * isa matrix.
B Power Iteration
Although k W∣∣∞ can be computed efficiently in O(nm) time for W ∈ Rm×n, naively computing
IlW∣∣2 = σmax(W) ：= vzλmax(W>W) requires O(n3) operations. (By λmaχ(A) we denote the
greatest eigenvalue of a symmetric matrix A.) We can however obtain an underestimate σ(W) via
power iteration:
八 _ W >Wbk	~/ m、_ KW >Wbk	nlλ
bk+1= kWτWbkk2, σk(W) = V	b>bk	,	()
with each iteration taking O(n2) time. Then using K《n iterations gives us an underestimate σκ in
O(Kn2) time. Since this is an underestimate, the resulting approximation to the Lipschitz constant of
the linear map will not be an upper bound. However the number of power iterations is usually chosen
so that σ is accurate enough - K = 5 is shown to be sufficient in the context of fully connected
networks or convolutions considered by Behrmann et al. (2019).
The iteration will converge if WτW has an eigenvalue that is strictly greater in magnitude than
its other eigenvalues, and the starting vector b0 has a nonzero component in the direction of an
eigenvector associated with the dominant eigenvalue. This happens with probability 1 if b0 is chosen
at random, and the convergence is geometric with ratio | λ? /λmaχ | where λ? is the eigenvalue with
second largest magnitude (Mises & Pollaczek-Geiringer, 1929).
C PROOF OF THEOREM 3.1 FOR GENERAL D
Theorem 3.1. DP-MHA is not Lipschitz for any vector p-norm ∣∣ ∙ ∣∣p withP ∈ [1, ∞].
Proof. The mapping f can be written as
f (X) = PX = SoftmaX (XATXT) X
f1(X)τ
.
.
.
fN(X)T
∈ RN×D,
(12)
where A = WKWQ>/pD/H ∈ Rd×d and fi(X) = PjN=I PijXj with PT = SoftmaX (XAxi).
Hence f can be interpreted as a map of each xi to a point in the convex hull of x1, ..., xN . Since f is
a map from RN ×D to RN ×D , its Jacobian is
J11 . . .	J1N
Jf =	.	...	.	∈ RNDxND,	(13)
JN 1 . . .	JN N
where Jij ="(X) ∈ Rd×d.	By taking partial derivatives we can show that Jij =
∂xj
XTP(i) ejiXAT + X Aδij + PijI where eij ∈ RN×N is a binary matrix with zeros everywhere
11
Under review as a conference paper at ICLR 2021
except the (i, j)th entry, δij is the Kronecker delta, and P(i) := diag(Pi:) - Pi>: Pi:. So for i = j:
Jii = X>P(i)eiiXA> +X>P(i)XA+PiiI
= Pii (xi -PkPikxk)xi>A> + X>P(i)XA + PiiI.	(14)
For the last equality, note eiiX has all rows equal to zero except for the ith row given by xi>. We can
then verify that X>P(i)eiiX simplifies to Pii(xi - Pk Pikxk)xi>.
For vector p-norms, kJf kp is bounded if and only if its entries are bounded, by definition of the
operator norm. The entries ofX>P(i)XA are bounded for arbitrary A only if the entries ofX>P(i)X
are bounded. So let us investigate the entries of this D × D matrix. Writing out each term of the
matrix, we observe that it is in fact a covariance matrix of a discrete distribution. Specifically:
[X>P(i)X]lm = Pk Pikxklxkm - (Pk Pikxkl) (Pk Pikxkm) = Cov(Xl, Xm),	(15)
where X is a discrete distribution with support at the inputs {x1, . . . , xN} and probability mass
function given by their softmax probabilities P(X = xj) = Pij. A consequence of this interpretation
is that P(i) is positive semi-definite (PSD) since for D = 1, Equation (15) becomes X>P(i)X =
Var(X) ≥ 0, with equality if and only if the xj are all equal.
We use this observation to show that the terms of Jii are unbounded, and so DP-MHA is not Lipschitz.
Consider the case Xi = 0. Then p> = Softmax (XAxi)= 得 1, i.e. We have uniform attention
regardless ofx6=i. The first term of Jii in Equation (14) disappears since xi = 0, and the last term
becomes -NI. For the second term, the entries [X>P(i)X]〃 = Var(Xl) are unbounded since the
latter is equal to the sample variance of xιl,... ,xNl, which can be arbitrarily large.	□
D Bias term in DP Self-Attention
A natural question to ask is whether we can add bias terms bQ to xi>WQ and bK to xj>WK to resolve
the issue of attention weights Pi: becoming uniform when xi = 0. The answer is no in general. It
can again be shown that Jii is unbounded when xi is chosen such that xi>WQ + bQ = 0 (such a
choice is possible assuming WQ is full rank, a dense set in Rdxd/h). Then Pi> =得 1 again, and
the diagonal entries of X>P(i)X are unbounded.
E Efficient Computation of L2 Self-Attention
Dot-product self-attention only requires a few matrix multiplications to compute the logits (i.e. the
inputs to the softmax) between all pairs of inputs, without having to loop over pairs, hence it can
be computed efficiently. Similarly, we can show that L2 self-attention can also be computed in an
efficient manner. Using the identity ka - bk22 = kak22 - 2a>b + kbk22 we can compute the logits of
L2 attention between all pairs via matrix multiplications and computation of row-wise L2 norms,
with negligible overhead compared to dot-product self-attention. Specifically, for L2 self-attention
we can show that
P	k kxwQk2owi> - 2xwQ(XWK)> + ikxwKk2oW
P — SoftmaX I	,---
∖	PD/H
(16)
where kAkr2ow applies the squared L2 norm to each row of A, so if A ∈ Rm×n then kAkr2ow ∈ Rm .
In Table 1 we show the wall-clock training times for the Transformer models with different attention
functions and a varying number of layers. It is evident that the differences between the models are
rather small.
12
Under review as a conference paper at ICLR 2021
	1 Layer	2 Layers	3 Layers	4 Layers	5 Layers
Transformer (DP)	37	56	77	92	110
Transformer (L2)	-35-	56-	-73	-99	-1Γ5-
Transformer, W Q = W K (L2)	-39-	58	-79	-91	-108
TranSformer, (Contractive-L2)	37	60	81	102	127
Table 1: Wall clock training times for one epoch of training (seconds)
F Proof of Theorem 3.2
Recall the formulation of L2-MHA:
F : RN×D →RN×D
F(X) = f1(X)WV,1,...,fH(X)WV,H WO
fh(X) = PhXAh
Pj X exp(Lj):= exp
∣∣x>WQ,h - x>WK,hk2
pD/H
Pihj=1
j
where We have that WQ,h,WK,h,WV,h ∈ RD×D/H, WO ∈ RD×D, Ph ∈ RN×N and Ah :=
WQ,hWQ,h / YDlH ∈ RD×D, and the SoftmaX is applied to each row of the input matrix. Recall
Equation (16):
Ph = softmax
—
∣XWQ,h ∣r2ow1> - 2XWQ,h (XWK,h)> + 1∣X W K,h ∣r2o>w
√D∕H
F.1 L2 SELF-ATTENTION IS not LIPSCHITZ FOR GENERAL WQ, WK
Let us first look at the case of H = 1 and suppress the index h to reduce clutter. Consider the map
f(X) := PX, so f (X) = f(X)A. We need f to be Lipschitz for f and hence F to be Lipschitz.
Note that P is defined as:
Pij X exp(Lij ) := exp
∣∣x>w Q - x>w Kk2
pD/H
—
and the normalisation constant satisfies Pj Pij = 1, for P ∈ RN×N, X ∈ RN×D.
For L2 self-attention, we may take partial derivatives and use the chain rule to show that the Jacobian
〜
offis:				
		r τ	τ η J11	. . .	J1N		
	Jf =	.	..	. .	..	∈ RND×ND	(17)
		JN 1	. . .	JNN		
with				
	Jij=	X>p(i) 1 + PjI ∈ rd×d ∂xj		(18)
where				
and
dLi：
dxj
2
pD/H
[(XWK - 1x>WQ) WQ>δj + (ejiXWQ - ejjXWK) WK]
(19)
P(i) := diag(Pi:) - Pi>: Pi: =
ΓPii(1 - Pii)
-Pi2 Pi1
-Pi1Pi2	...
Pi2 (1 - Pi2 ) . . .
-Pi1PiN
-Pi2 PiN
-PiN Pi1	-PiN Pi2	. . . PiN (1 - PiN)
13
Under review as a conference paper at ICLR 2021
P = exp(-∣∣x>WQ -x>WKk2)
ij = Pk exp (-kx>WQ -x>WKk2).
Recall that eji ∈ RN ×N is a binary matrix with zeros everywhere except the (j, i)th entry. Hence
ejiX has all rows equal to zero except for the jth row given by xi> . We can then verify:
X>P(i)ejiX=Pij(xj -XPikxk)xi>.	(20)
k
Also note P(i) is symmetric, and each row/colum sums to 0, i.e. P(i) 1 = 1>P(i) = 0. Hence we
may simplify the Jacobian terms as follows:
Jii
2
pD/H
X>P(i) (XWK - 1xiTWQ)WQ> + X>P(i)eiiX(WQ - WK)WK>	+PiiI
〜
2
pD/H
X>P(i) (XWK - 1xiTWQ)WQ> + Pii (xi - XPikxk)xi>(WQ - WK)WK>
k
+ Pii I
=^= X>P⑺XWKWQ + Pii(Xi- XPikXk)x>(WQ - WK)WK	+ PiiI,
D/H	k
(21)
and for i 6= j :
-pDZHX TP (i) (eijXW Q - ejj XW K)W K> + Pij I
-ɪ= Pij (Xj- X PikXk )(x>WQ - XT WK)WK> + PijI.	(22)
D/H	k
We are now ready to show that f is not LiPschitz for general WQ, WK:
Lemma F.1. If WK ∈ Rdxd/h is full rank (i.e. full column rank), and WK = W Q, then Jij has
terms that are unbounded for i 6= j, hence f is not Lipschitz.
Proof. Let US investigate the expression KKij := PijWk>(Xj- Pk PikXk)(XTWQ 一 XTWK) ∈
DD
RH × H for i = j, which is related to Jij as follows by Equation (22):
W k> Jij=(PDHKij+ Pij i)w k> .
It suffices to show that Kj is unbounded to show that Jij is unbounded, since WK is full rank and
Pij ∈ [0, 1].
Let yjT = XiTWQ - XjTWK. Then we have:
yj -XPikyk =WQ>Xi -WK>Xj -XPik(WQ>Xi -WK>Xk)
kk
=WQ>Xi -WK>Xj -(WQ>Xi -XPikWK>Xk)
k
= -WK> (Xj - XPikXk).
k
Hence Kij = -Pij (yj 一 Pk Pikyk)y>. Note yi can take an arbitrary value in RD/H, since
WK 6= WQ and WK is full-rank.
For all j 6= i, let us choose Xj such that yj = -yi. This is possible for any value of yi since
WK is full-rank. Note yj = -yi and not yi. We then have that kyj k22 is equal for all j, hence
14
Under review as a conference paper at ICLR 2021
Pij := PeXp(P(-jy2)2) = N for all j. Then for i = j, Kij simplifies to
Kij = -N (-yi - N(N - 2)(-yi)) (-yi)> =----------------N— yiy>
whose entries are unbounded since yi can be any vector in RD/H (note we assume N ≥ 2 for
self-attention to be well-defined, hence 2N - 2 = 0).	□
The intuition for this result is as follows: a reason for DP-MHA not being Lipschitz is that for xi = 0„
the attention weights Pij become uniform regardless of the values of xj for j 6= i. A similar issue
arises for L2-MHA with WQ 6= WK and full-rank WK, as shown above: given any xi, we can
choose xj such that the Pij become uniform.
F.2 L2 SELF-ATTENTION IS LIPSCHITZ FOR WQ = WK
Hence we impose the restriction that WK = WQ. With this assumption we have
Pij H exp (-∣∣(xi - Xj)>√A∣∣2)	(23)
where A = WQWQ>/'D/H ∈ Rd×d and √A is chosen such that A = √A√A>, in particular
√A := WQ/(D/H) 1. The terms in the Jacobian of f simplify to:
Jii = 2X>P(i)XA + PiiI (note P(i)1 = 0),	(24)
Jij = 2Pij(Xj- EPikXk)(xi - Xj)>A + PijI for i = j.	(25)
k
Let the Jacobian of f(X ) be:
J11 . . .	J1N
Jf =	.	...	.	∈ Rnd×nd.	(26)
JN 1 . . .	JNN
Since f (X) = f(X )A, and by the chain rule dd- [fi(X )A] = A> fX = A fX (by symmetry
xj	xj	xj
of A), we have that Jij = AJij . Hence
Jii = 2AX>P(i)XA + PiiA (noteP(i)1 = 0),	(27)
Jij = 2Pij A(Xj - X Pik Xk )(Xi - Xj )>A + PijA for i 6= j.	(28)
k
Noting Lipp(f) = supX kJf(X)kp, we would like to upper bound kJf kp.
F.2.1 UPPER BOUND ON Lip∞ (F) FOR L2-MHA
Consider the choice p = ∞, where kJf k∞ is the maximum absolute row sum of Jf. A key
observation is that if we can bound the ∞-norm of the Jacobian of fi, a single output of f (i.e. a
single block row k[Ji1, ..., JiN]k∞ of Jf), then this is also a bound on kJf k∞ due to permutation
equivariance of self-attention; all block rows have the same maximal ∣∣ ∙ ∣∣∞ when each is optimised
over the input X. Using this, we can prove that kJf k∞ admits an upper bound that is O(log N -
log log N). Below we state and prove lemmas that lead to the proof of this upper bound.
First we analyse the term √A>X> P(i) X√A, that appears in the first term of Jn. Note that for
Y := X √A, so that the rows of Y are y> := χ> √A, we have
√A>X >P (i)X √A = Y >P⑷Y = Cov(Y)	(29)
where P(Y = yj∙) = Pij = exp(-∣∣yj∙ - yi∣∣2)/ Pk exp(-∣yk - yi∣2). The last equality uses the
observation in Equation (9).
The central inequality used throughout the proof of the main theorem is the following:
15
Under review as a conference paper at ICLR 2021
Lemma F.2. Tr(Cov(Y)) = Pj Pij kyj -Pk Pikykk22 ≤ Pj Pij kyj -yik22 ≤ φ-1(N-1) where
φ(c) = c exp(c + 1) is a one-dimensional invertible function on R≥0.
Proof. The first equality holds since Tr(Cov(Y)) = j Cov(Y)jj = j Var(Yj) = j E[(Yj -
E[Yj])2]. The next inequality holds since Var(Yj) = Var(Yj) = E[Y2] - E[Yj]2 ≤ E[Y2] where
Y = Y - yi . The final inequality can be proved as follows.
We would like to bound
2	Pj kyj - yik22exp(-kyj -yik22)	Pj zj2 exp(-zj2)
j Pijkyj- yik2 = -Pk exp(-kyk-yik2)- = Pk exp(-zk)
where zj := kyj - yi k2 (hence zi = 0). Define:
g(z)∙= PjZ exp(-z2) = Pj= z2 exp(-z2)
'	PkeXp(-z2)	1 + pk=i eχp(-z2).
(30)
(31)
First note that as zj → ∞, exp(-zj2) → 0 exponentially fast, causing the product zj2 exp(-zj2) → 0.
Hence we expect the above quantity to be bounded and attain its maximum.
Let h(zj) := exp(-zj2) for notational conciseness, and note h(zj) > 0. By taking partial derivatives
with the chain rule, we have that for j 6= i
∂g(z)	2yj h(zj)
--7-——-Z=-----:--. . C
dzj	(Pk h(zk ))2
(1 - zj2)	h(zk) +	h(zk)zk2
kk
(32)
Hence the derivative is 0 if and only if zj = 0 or (1 - zj2) Pk h(zk) + Pk h(zk)zk2 = 0, the latter
being equivalent to Zj = 1+ Pk hhZZ)Zk = 1 + g(z). Hence at the maximum, the non-zero values
among {zj}jN=1 must be equal to one another. It is clear now that the maximum value c is attained
when zj2 = 1 + c for j 6= i (and recall zi = 0). So h(zj ) = exp(-1 - c) for j 6= i. Substituting
this into g(z), and rearranging, we obtain c exp(c + 1) = N - 1. Note φ(x) := x exp(x + 1) is
increasing for x > 0 hence C = φ-1(N — 1).	口
Note φ(log N) = (log N) exp(log N+ 1) ≥ Nlog N ≥ N- 1 for N ≥ 3. Since φ is increasing, we
have φ-1(N- 1) ≤ log(N) for N ≥ 3. In fact, itis known that φ-1(N- 1) = O(log N-log log N)
(Corless et al., 1996).
Note the A term in f (X) = f(X)A allows us to use the above inequality, since Y>P(i)Y = Cov(Y)
now appears in the terms of Jf :
Jii = 2√A[Y >P (i)Y ]√A> + PiiA,	(33)
Jij, = 2√APij(yj- X Pikyk)(yi - yj)>√A> + PijA for i = j.	(34)
k
Using the inequalities kBCk ≤ kBkkCk, kB+Ck ≤ kBk + kCk and k[A1, . . . ,AN]k ≤ Pi kAik,
we have:
k[Ji1, . . . , JiN]k∞
≤kJii k∞ + X kJij k∞
j6=i
≤2k√Ak∞kY >P (i)Y k∞k√A>k∞ + PiikAk∞
+ 2 X k√Ak∞kPij (yj- X Pikyk )(yi-yj )>k∞k√A>k∞ + PijkAk∞
j 6=i	k
=2k√Ak∞k√A>k∞ (kY>P(i)Yk∞ + X kPij (yj- X Pik yk Nyi - y )>k∞) + ∣∣A∣∣∞
j 6=i	k
=2kWQk∞WQ>k∞ (kY>P(i)Yk∞ + XkPij(yj - XPikyk乂…)>k∞) + ≡⅛.
D/H	j	k	D/H
16
Ll
oMλM(X~)6 = o½l [h^λM(x')hΓ" 'ivv41(x)M JX ： d
:st (x)“ SpBQq əgi səuɪgmoɔ IBql dπm uotjuəjjb pB5qτηnm UnJ Qqi
"g < ʌ[ joj spjoq Ajτjπnb5uτ jsŋɪ əgi
(2
00IlxfeTllrllfeTllll >
00IlxfeTllrlIfeTllII >
列审Ml + (L	∞Frll
5Λ∏q əm ζ(mns moj əjnɪosgŋ mnmτxBm Qqj st 001∣ ∙ ∣∣ PUB juBUBΛτnb5 uoμ
-ŋjnunəd st / əɔms) oo∣∣ʃ∕s∣∣ JO ooubtjbλut UogBInUnəd Aq OOh[(χ)N^ ∙ ∙ ∙ ^χ)[汩 HXdnS
= OOll(X) ʃf Il XdnS uoτjBΛJθsqo ə卬 ψτM ζjoψoSojjB sŋmməɪ əAOqB ə甲 ⅛uμjnj
□	∙(τ -ΛT)t-≠∣7Λ
> z∣∣P∣∣z∣∣3∣∣fyA > z∣∣⅛∣∣z∣∣w∣∣ əɔuən ZHeUraKnUKMJOsp? (ɪ — n)Le > |||% -加产Cr'X
=例PIlPUe 忆HeuraKnUroJJ (1—%)Le > ((A)AOQ)JII = ∣∣∣⅜¾fcf ¾^-⅞∣∣-⅛	=外团IMoN
'z∣∣P∣∣fyA > z∣∣⅛∣∣ əɔuən ∙π∕σa ə ʃ JOJ (ɪ PUB [∖H∕ax∖^-∙ 1∣τχ∣] Uo Xj∏Bnbauτ
zjjBMq3s-Aq3nB□ aqj Aq -g ə) z∣∣x∣∣^y^ > ^∣∣x∣∣ əɔms ^∣∣⅞ -加产(Jzʌ先α=：-p^J^ > f⅛ osɪv
∙^∣∣□∣∣ > ^∣∣υ∣∣ əɔuəH ʃ jojɔəʌ joj ^∣∣χ∣∣ > oo∣∣χ∣∣ əɔms 制寸6寸甘寸Z- ⅞∣∣-¾∕v =: ⅛ > ⅛* əioN
∙τ∣∣-⅜ - -^∣∣-⅛∕v =切0|| 优寸- -f^∣∣-¾∕v = ⅛* QnqM
lzll⅛llzllwll >⅛xw =
T∣∣⅜ -网产H优学HM —町∣∙6⅛M = OOh("-次)(优寸甘M — ⅜)-⅛∣∣ M
əɔuəH *q 勿 sjojɔəʌ jbəj joj1|忖||°OImll = OOhq例| OIoN 'f00^d
∙b√o√λ(t - n∖-Φ > OOh(% -为)(优寸分纪-¾)∙⅛ll -fZ r,jeuɪuɪəi
□	ZH buiuiəi
UlaIJ st AjTJBnbouT jsŋɪ ə甲 PUB ’("尺)乙。IUZzʌ > (瓜” PUB (ɪ PUB [(ʃʃ/ɑʌ)r*t ∙ ∙ ∙ "(ta)i>]
uo AjTJBnbouT ZjJBMqos-AqonBQ ə甲 Suτsn -g-ə Xq) ("r)严"芯八基A > ("尺)。"Z əɔɑɪs
七-N)l*A >
Ul
(U)Λθθ)ΛI-∣^ =	>
Ul	Ul
(Uix)∙oR(⅛)∙OXwEm 产[(a)a。。]IRXwE = ooII(a)λ00∣I
∞∞H '(mA)-0(⅛-° > R(A)Ao0]
U叫IL ∙mA J° UOTJBTAap prepuujs。甲 əjouəp (Ui尺)。Wl ∙(a)λoO = A^d1A W甲【吃双'f00-ld
■(Z-J VUiUisq UJ SV pəuifəp φ) h∕qjΛ(T - jV)τ-≠ > °o∖∖A^dςA∖∖ VJ Buiuiaq
:sŋunnəɪ
Sutmo∏oj ə甲 Aq popunoq əjb siərəBJq ə甲 uτ sunəj ə甲 JO qoB∏ 0 = % -流 unəj ə卬 əsnŋɔəg 0 st
z = C joj puBuιmns aqι iBψ əjou tAι∏B∏b9 puoɔəs aqι joj "ɪ = -¾ ’芯 PqI əlɔu tAi∏Bnba ιsjy aqι joj
【COZ mɔi JB jədŋd əɔuəjəjuoɔ B sb mətaəj jəpun
Under review as a conference paper at ICLR 2021
where g : X 7→ [f1(X),...,fH(X)], WO ∈ RD×D and
-W V,1 ...	0 -
W V =	.	..	.	∈ RDH×D
.	..
0	...	WV,H
Note the Jacobian Jg is a block matrix whose rows are Jfh , hence kJg k∞ = maxh kJfh k∞ , and
similarly kWV > k∞ = maxh kWV,h> k∞. Hence we have
Lip∞ (F) ≤ max kJfhk∞ max kW V,h>k∞kW O>k∞.
Combining this with Inequality (35), we have:
Lip∞(F) ≤ (4φ-l(N - 1) + Pd1∕h) max kWQ,hk∞kWQ,h> k∞ max ∣∣WV,h> k∞ ∣∣WO> k∞.
F.2.2 UPPER BOUND ON Lip2 (F) FOR L2-MHA
For p = 2, we use the following lemma:
Lemma F.5. LetA be a block matrix with block rows Ai,..., An. Then k A∣∣2 ≤ ，£i ∣∣Ai∣∣2, and
equality holds if and only if the first right singular vectors of the Ai align.
Proof.
kAk22
A1
.
.
.
AN
= sup
kxk2=1
2
A1
.
.
.
AN
= kxsku2p=1 i kAixk22 ≤ i kxsku2p=1kAixk22= i kAik22.
2
2
2
x
Note that equality holds if and only if the first right singular vectors of the Ai align.	□
Hence a bound on the spectral norm of each block row of Jf can give us an O(√N) bound on ∣∣ Jf ∣∣2,
which may be loose, and it remains an open question as to whether this bound can be tightened.
To bound the ∣∣ ∙ ∣2 norm of each row of Jf, we use the following lemmas:
Lemma F.6. ∣Y >P(i)Y ∣2 ≤ φ-1(N - 1)
Proof. ∣Y>P(i)Y∣2 = ∣Cov(Y)∣2 = λmax(Cov(Y)) ≤ Tr(Cov(Y)) ≤ φ-1(N - 1), where the
first equality holds by symmetry of Cov(Y) and the next holds by Cov(Y) being positive semi-
definite, so all its eigenvalues are non-negative, and hence the maximal eigenvalue is bounded by the
sum of the eigenvalues, equal to its trace. The final inequality is from Lemma F.2.	□
Lemma F.7.	Pj	∣Pij(yj	-	Pk Pikyk)(yi	- yj)>∣2	≤ φ-1(N	- 1)
Proof. Directly use Cauchy-Schwartz on C and d in the proof of Lemma F.4.	□
Again using the inequalities ∣BC∣ ≤ ∣B∣∣C∣, ∣B + C∣ ≤ ∣B∣ + ∣C∣ and ∣[A1, . . . ,AN]∣ ≤
Pi ∣Ai ∣, with the additional equality ∣B> ∣2 = ∣B ∣2, we have the bound:
∣[Ji1 , . . . , JiN]∣2
≤ 2kWQ黑Q>k2 (kY>P(i)Yk2 + XkPij(yj - XPikykH)>k) + ¾>
≤ …-1) Pa +、
≤ ∣WQk2
_ PD/H
4φ-1(N - 1)+ 1
18
Under review as a conference paper at ICLR 2021
Using Lemma F.5, we have that
kJfk2≤
工(4Φ-1(N - 1) + 1
D/H
(36)
VN kw Qk2
PD/H
( logN + 1).
≤
To obtain the final result for the full multihead self-attention F , we need a final lemma:
Lemma F.8. LetA be a block matrix with block columns Ai,..., An. Then ∣∣ A∣∣2 ≤ vz∑2i Mik2∙
Proof.
kAk2 = k[A1, . . . ,AN]k2 = sup
Pi kxik22=1
[A1, . . . , AN]
k	Aixik2
22 =1	i
≤ sup	kAixi k2 = sup	λi kAiei k2 = sup	λi kAi k2
Pi kxik22=1 i	keik2=1,Pi λi2=1 i	Pi λi2=1 i
≤ WkAik2,
where We are using the substitution Xi = λiei, and the last inequality holds by e.g. Cauchy-SchWartz
inequality on [λι,...,λN ] and [∣∣A1∣∣2,..., IMn ∣∣2].	□
Recall that
F : X 7→ f1(X)W V,1,. ..,fH(X)WV,H WO.
Since kfh(X)WV,hk2 ≤ kJfh k2 kWV,hk2, by Lemma F.8 We have that
and hence
[f1(X)WV,1,...,fH(X)WV,H]2
≤	Xh
kJfhk22kWV,hk22
Lip2 (F) ≤
(sX
kJfhk22kWV,hk22
kWOk2.
(37)
Combining this With Inequality (36), We have:
LiP2(F) ≤ PDNH (4φ-1(N -1) + 1) HP ∣wQhk2 ∣WV,hk2) ∣w0k2.
G The Case with Masking
Since self-attention is often used With masking, a natural question is hoW masking affects the derived
bounds. In self-attention (for any choice of attention function), masking is implemented as folloWs:
given a set of mask indices M ⊂ {1, . . . , N} × {1, . . . , N}, the logits (i.e. the inputs to the softmax)
are set to -∞ at the mask indices. That is,
I.. = [Lij if (i,j) ∈ M
ij -∞ if (i, j ) ∈ M
Where Lij is the original logit (e.g. for L2 self-attention, Lij = -(xi - xj)>A(xi - xj)).
Masking implies fi(X) is not a function of xj for (i, j) ∈ M, hence Jij = 0 for (i,j) ∈ M. Thus
fi (X) is equal to the ith output for self-attention With inputs restricted to {xj : (i, j) ∈/ M}, the
unmasked inputs With respect to the ith output. Hence Jij Will no longer contribute to the bound
on k[Ji1, . . . , JiN]k, and hence the bound for the unmasked case Will continue to hold as long as
(i, i) ∈ M i.e. xi attends to itself (this is necessary for the proof of Lemma F.2 to hold). The bound
can in fact be tightened by replacing N With |{xj : (i, j) ∈/ M}|, the number of unmasked inputs
With respect to the ith output.
19
Under review as a conference paper at ICLR 2021
H Dropout is Contractive
At test time, Dropout multiplies inputs by the dropout keep probability p < 1, so it is a contraction
with Lipschitz constant p at evaluation time. At training time, Dropout amounts to setting some
inputs to zero, while keeping other inputs constant. This can be expressed as right multiplication by a
diagonal binary matrix M, and for such matrices we can verify kMkp := supkxkp=1 kM xkp ≤ 1.
I Experimental Details
For the experiment in Section 5.1, showing the asymptotic tightness of the upper bound on Lip∞ (F)
where F is L2-MHA, we fix all free parameters of F (namely WQ, WV) to be the identity, and only
optimise the input X. We use 50 random initialisations of X for each N, where Xij 〜U[-c, c] for
C 〜U[0,10] (We observed that having C itself be random improves optimisation). We display the top
5 results for each value of N after optimising each random initialisation till convergence using Adam
(Kingma & Ba, 2015) with a learning rate of 0.1.
For the experiments in Section 5.3, we comparing the performance of the original Transformer and
the Transformer with Lipschitz/invertible self-attention at character-level language modelling on the
Penn Treebank dataset (Marcus et al., 1993).1 Each training example is a sentence represented as
a variable-length sequence of characters, and examples are batched according to length such that
padding is minimised, with the maximum sequence length set to 288. All models are autoregressive,
outputting the logits for the categorical likelihood predicting the next character, and are trained
using maximum likelihood (cross-entropy loss) with a batch size of 64. The LSTM models have
the dimensionality of the hidden state equal to the dimensionality D of the cell state (the usual
default implementation). The Transformer models are trained with a varying number of blocks
(number of layers) with H = 8 heads and D = 512, tuning hyperparameters for dropout rate in
{0, 0.1, 0.2} and base learning rate γ ∈ {0.2, 0.4, 0.6, 0.8, 1.0, 1.5, 2.0} with number of warmup
iterations w ∈ {1000, 2000, 4000, 8000} for the standard custom learning rate schedule in Vaswani
et al. (2017):
γ
√D
min(t-1/2, tw-3/2),
where t is the learning rate at training iteration t. Hence the learning rate linearly increases from
0 to (Dw)-1/2 over w iterations, then decays proportionally to t-1/2. We use Glorot Uniform
initialisation (Glorot & Bengio, 2010) for all weights (U
[-√d-13ou;, Jd⅛;]),except for
weights in L2-MHA that are initialised from U
[-√Sβ, √Sβ], and S is a hyperparameter. For D = 512,
we used S =击.All experiments were done in Tensorflow 1.14 (Abadi et al., 2016). The code will
be released upon de-anonymisation.
In Table 2 we show the best Test NLL across training of Transformer models in Figure 4.
Number of Layers	1	2	3	4	5	6	8	10	12	14
(DP)=	1.164	0.989	0.954	0.943	0.940	-	-	-	-	-
八（L2）,一	1.134	0.987	0.958	0.942	0.946	-	-	-	-	-
WCQ = W K 7L2)~	1.237	1.055	1.001	0.994	1.023	-	-	-	-	-
(Contractive-L2)	1.558	1.453	1.343	1.191	1.205	1.119	1.086	1.0495	1.015	1.005
Table 2: Test NLL for Transformer models on PTB character level language modelling
J Numerical Invertibility of MHA Residual Map
Following Section 5.2, Figure 5 confirms that numerical invertibility does not hold for trained weights
for dot-product multihead self-attention (DP-MHA) (obtained from one-layer Transformer (DP)
1We use the standard training-validation-test split, and the dataset can be found at e.g. https://github.
com/harvardnlp/TextFlow/tree/master/data/ptb.
20
Under review as a conference paper at ICLR 2021
model used for Figure 4), similar to the randomly initialised weight case. Figure 6 shows additional
results for different values of N and D.
Inverting trained DP-MHA
Residual Map (D=512rH=8)
J。」」① uotru⅛uouaj XraE
jo.uə u.9ErLIj-jsuouə,j XBlU
寸9hn
Figure 5: Invertibility of g(x) = x + cf (x) for trained DP-MHA f.
」0L① Uo-tiMSUoUa」XBuJ
8zihn
5
2
O
2
fixed point iterations
O
1
5
O
fixed point iterations fixed point iterations
Figure 6: Numerical invertibility of g(x) = x + cf (x) where f is L2-MHA(left) or DP-MHA (right),
for different values of N and D.
fixed point iterations
21
Under review as a conference paper at ICLR 2021
K B EHAVIOUR OF LOWER B OUND ON Lip2(F )
N
Figure 7: Lower bound on Lip2 (F) where F is L2-MHA, with D = 1 and varying N, obtained by
optimising kJF (X)k2 with respect to X, with 50 random initialisations of X for each N.
In Figure 7, we show the lower bound on Lip2 (F) obtained by optimising kJF (X)k2 using the same
optimisation procedure as for Figure 2 of Section 5.1. Here the optimisation is more difficult, evident
in the variance of the top 5 values, and the trend is less clear, but it appears that Lip2(f) grows at a
rate of O(log N). The message is less clear here, and there are at least two possibilities:
(1)	The optimisation is difficult even for small values of N , hence Figure 7 shows a loose lower
bound.
(2)	If the lower bound is tight, this suggests that the O(√Nlog N) bound in Theorem 3.2 is not
asymptotically tight, and could be improved to O(log N) (or O(log N - loglogN) as for
p = ∞).
L Optimising the norm of the Jacobian of DP-MHA
In Figure 8, we show how the norm of the Jacobian kJf (X)k∞ for DP-MHA f keeps increasing
when being optimised with respect to X . This is a useful sanity check validating our theoretical result
of Theorem 3.1, that DP-MHA is not Lipshchitz. The oscillations are likely due to momentum term
of Adam optimizer that was used to optimise the norm.
Optimise IiMX川8 wrt Xfor
f = DP-MHA (trained, D=512)
Figure 8: Optimise kJf (X)k∞ w.r.t. X for trained DP-MHA f.
22
Under review as a conference paper at ICLR 2021
M Experiment tying keys and queries of L2-MHA but preserving
PARAMETER COUNT
In Figure 4 of Section 5.3, we have shown that there is a clear reduction in performance when tying
the keys and queries. To test whether this can be attributed to the reduction in parameter count, we
tried doubling the number of columns of WQ when the keys and queries are shared (i.e. from D/H to
2D/H) so that the shared model has the same number of parameters as the unshared model. In Figure
9, the third column shows results for shared L2-MHA, but with the same number of parameters as the
unshared L2-MHA i.e. without tying the keys and queries. The performance is similar to the second
column (tying with a reduced number of parameters), suggesting that there is an inherent limitation
in expressiveness to tying the keys and queries, and that the reduction in number of parameters is an
insufficient explanation this phenomenon.
Transformer (L2)
1.8
1.2
1.0
1.6
i1.4
∏5
20	40	60	80 100
epoch
Num Layers
——1
2
3
——4
——5
Transformer (L2), W0 = Wκ
20	40	60	80 100
epoch
Num Layers
——1
2
3
——4
——5
Figure 9: Experiment tying keys/queries but preserving parameter count.
epoch
N Stability Experiments
In Figure 10 below, we compare the output variance of trained L2-MHA against trained DP-MHA, with
weights from the one-layer Transformer (L2), WQ = WK model and (DP) model used for Figure 4
respectively. We take the same distribution of inputs as used for the numerical invertibility experiment
in Section 5.2, and show the histogram of inputs and outputs after flattening the input/output tensors.
We see that the range of outputs remains similar to the range of inputs for Lipschitz L2-MHA, whereas
for DP-MHA the outputs have a much wider range, because the Jacobian norm is large for DP-MHA
at these inputs.
In practice, this leads to instabilities while training for DP-MHA, hence requiring careful tuning
of the learning rate schedule for training deeper Transformer models: linear warmup and square
root decay, as detailed in Appendix I. In Figure 11, we show how training instabilities arise for
DP-MHA with deeper Transformer models if we use a fixed learning rate. We compare the training
curves of DP-MHA, L2-MHA (WQ = WK) and Contractive-L2-MHA for a varying number
of layers for the first 20 epochs of training with a fixed learning rate. We see that DP-MHA fails
to train beyond 10 layers, whereas both L2-MHA (WQ = WK) (i.e. Lipschitz L2-MHA but not
contractive) and Contractive-L2-MHA shows stable training for up to 18 layers. This was the
deepest model we could fit on a single GPU, and we expect to be able to train even deeper models
with L2-MHA (WQ = WK) and Contractive-L2-MHA. In Table 2 we show the best Test NLL
across training of Transformer models with fixed learning rate. Note that for DP-MHA training
becomes unstable beyond 10 layers, so we are only able to provide results up to 10 layers. The
generalisation performance of the best model for each setting is similar.
23
Under review as a conference paper at ICLR 2021
Figure 10: Histogram showing distribution of inputs/outputs of L2-MHA and DP-MHA
Transformer (DP) Transformer (L2, Wκ = Wq) Transformer (Contractive-L2)
TlN U'<5-1
Num Layers
2
4
6
---8
——10
——12
——14
16
18
0	5	10	15	20 0	5	10	15	20 0	5	10	15	20
epoch	epoch	epoch
Figure 11: Train NLL for Transformer (DP), Transformer (L2) and Transformer (Contractive-L2)
Number of Layers	2	4	6	8	10	12	14	16	18
(DP)一	1.061	1.032	1.021	1.017	1.025	-	-	-	-
W Q = w K(L2)	1.168	1.040	1.023	1.024	1.019	1.008	1.018	1.027	1.034
(ContractiVe-L2)	1.246	1.135	1.103	1.079	1.072	1.060	1.039	1.029	1.031
Table 3: Test NLL for Transformer models trained with fixed learning rate on PTB character level
language modelling
24