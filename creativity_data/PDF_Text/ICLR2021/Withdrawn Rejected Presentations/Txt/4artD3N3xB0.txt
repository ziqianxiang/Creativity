Under review as a conference paper at ICLR 2021
Bayesian Learning to Optimize:
Quantifying the Optimizer Uncertainty
Anonymous authors
Paper under double-blind review
Ab stract
Optimizing an objective function with uncertainty awareness is well-known to im-
prove the accuracy and confidence of optimization solutions. Meanwhile, another
relevant but very different question remains yet open: how to model and quan-
tify the uncertainty of an optimization algorithm itself? To close such a gap, the
prerequisite is to consider the optimizers as sampled from a distribution, rather
than a few pre-defined and fixed update rules. We first take the novel angle to
consider the algorithmic space of optimizers, each being parameterized by a neu-
ral network. We then propose a Boltzmann-shaped posterior over this optimizer
space, and approximate the posterior locally as Gaussian distributions through
variational inference. Our novel model, Bayesian learning to optimize (BL2O) is
the first study to recognize and quantify the uncertainty of the optimization algo-
rithm. Our experiments on optimizing test functions, energy functions in protein-
protein interactions and loss functions in image classification and data privacy
attack demonstrate that, compared to state-of-the-art methods, BL2O improves
optimization and uncertainty quantification (UQ) in aforementioned problems as
well as calibration and out-of-domain detection in image classification.
1	Introduction
Computational models of many real-world applications involve optimizing non-convex objective
functions. As the non-convex optimization problem is NP-hard, no optimization algorithm (or opti-
mizer) could guarantee the global optima in general, and instead, their solutions’ usefulness (some-
times based on their proximity to the optima), when the optima are unknown, can be very uncertain.
Being able to quantify such uncertainty is important to not only assessing the solution uncertainty
after optimization but also enhancing the search efficiency during optimization. For instance, re-
liable and trustworthy machine learning models demand uncertainty awareness and quantification
during training (optimizing) such models, whereas in reality deep neural networks without proper
modeling of uncertainty suffer from overconfidence and miscalibration (Guo et al., 2017). In another
application example of protein docking, although there exists epistemic uncertainty of the objective
function and the aleatoric uncertainty of the protein structure data (Cao & Shen, 2020), state-of-
the-art methods only predict several single solutions (Porter et al., 2019) without any associated
uncertainty, which makes those predictions hard to be interpreted by the end users.
Various optimization methods have been proposed in response to the need of uncertainty aware-
ness. Stochastic optimization methods like random search (Zhigljavsky, 2012), simulated annealing
(Kirkpatrick et al., 1983), genetic algorithms (Goldenberg, 1989) and particle swarm optimization
(Kennedy & Eberhart, 1995) injected the randomness into the algorithms in order to reduce uncer-
tainties. However, these methods do not provide the uncertainty quantification (UQ) of solutions.
Recently, there have been growing interests in applying inference-based methods to optimization
problems (Brochu et al., 2010; Shapiro, 2000; Pelikan et al., 1999). Generally, they transfer the un-
certainties within the data and model into the final solution by modelling the posterior distribution
over the global optima. For instance, Bijl et al. (2016) uses sequential Monte Carlo to approximate
the distribution over the optima with Thompson sampling as the search strategy. Hernandez-Lobato
et al. (2014) uses kernel approximation for modelling the posterior over the optimum under Gaus-
sian process. Ortega et al. (2012); Cao & Shen (2020) directly model the posterior over the optimum
as a Boltzmann distribution. They not only surpass the previous methods in accuracy and efficiency,
but also provide easy-to-interpret uncertainty quantification.
1
Under review as a conference paper at ICLR 2021
Despite progress in optimization with uncertainty-awareness, significant open questions remain.
Existing methods consider uncertainty either within the data or the model (including objective func-
tions) (Kendall & Gal, 2017; Ortega et al., 2012; Cao & Shen, 2020). However, no attention was
ever paid to the uncertainty arising from the optimizer that is directly responsible for deriving the
end solutions with given data and models. The optimizer is usually pre-defined and fixed in the
optimization algorithm space. For instance, there are several popular update rules in Bayesian opti-
mization, such as expected improvement Vazquez & Bect (2010) or upper confidence bound Srinivas
et al. (2009), that are chosen and fixed for the entire process. For Bayesian neural networks training,
the update rule is usually chosen off-the-shelf, such as Adam, SGD, or RMSDrop. The uncertainty
in the optimizer is intrinsically defined over the optimizer space and important to the optimization
and UQ solutions. However, such uncertainty is unwittingly ignored when the optimizer is treated
as a fixed sample in the space.
To fill the aforementioned gap, the core intellectual value of this work is to recognize and quantify
a new form of uncertainty, that lies in the optimization algorithm (optimizer), besides the classical
data- or model- based uncertainties (also known as epistemic and aleatoric uncertainties). The un-
derlying innovation is to treat an optimizer as a random sample from the algorithmic space, rather
than one of a few hand-crafted update rules. The key enabling technique is to consider the algorith-
mic space being parameterized by a neural network. We then leverage a Boltzmann-shaped posterior
over the optimizers, and approximate the posterior locally as Gaussian distributions through varia-
tional inference. Our approach, Bayesian learning to optimize (BL2O), for the first time addresses
the modeling of the optimizer-based uncertainty. Extensive experiments on optimizing test func-
tions, energy functions in a bioinformatics application, and loss functions in the image classification
and data privacy attack demonstrate that compared to the start-of-art methods, BL2O substantially
improves the performance of optimization and uncertainty quantification, as well as calibration and
out-of-domain detection in classification.
In the following sections, we first review related methods in details and reveal the remaining gap.
We then formally define the problem of optimization with uncertainty quantification and point out
the optimizer as a source of uncertainty. After formally defining the optimizer space, the optimal
optimizer as a random vector in the space, and the optimizer uncertainty, we propose our novel
model, BL2O. And lastly, we compare our BL2O with both Bayesian and non-Bayesian competing
methods on extensive test functions and real-world applications.
2	Related work
Many works (Wang & Jegelka, 2017; Hennig & Schuler, 2012) studied optimization with uncer-
tainty quantification under the framework of Bayesian optimization (Shahriari et al., 2016; Brochu
et al., 2010). In these studies, multiple objectives are sampled from the posterior over the objec-
tives (p(f |D)), where D is the observed data. Each sampled objective is optimized for obtaining
samples of the global optima: w* so that the empirical distribution over w* can be built. APProx-
imation is much needed since those approaches need optimization for every sample. For instance,
Henrandez-Lobato et al. (2014) uses kernel approximation to approximate the posterior distirbution.
Another line of work uses various sampling schemes for estimating the density of posterior distribu-
tions. For instance, Bijl et al. (2016) uses sequential Monte Carlo sampling. De Bonet et al. (1997)
designs a randomized optimization algorithm that directly samples global optima. These methods
are much more efficient, but their performance heavily depends on the objective landscapes. More-
over, a few studies (Ahmed et al., 2016; Lizotte, 2008; Osborne et al., 2009; Wu et al., 2017) in
Bayesian optimization utilize first-order information to boost the performance of optimization. For
instance, Osborne et al. (2009) uses gradient information to improve the covariance matrix in Gaus-
sian process. Wu et al. (2017) embeds the derivative knowledge into the acquisition function which
is optimized in every iteration.
Finally, there are approaches (Ortega et al., 2012; Cao & Shen, 2020) that directly model the shape
of posterior as the Boltzmann distributions: p(w*∣D) 8 eχp(-αf (w*)), where α is the sched-
uled temperature constant. They automatically adjust α during the search in order to balance the
exploration-exploitation tradeoff. They beat previous work in terms of both efficiency and accuracy.
However, as revealed earlier in the Introduction, none of the methods above consider the uncertainty
within the optimizer.
2
Under review as a conference paper at ICLR 2021
3	Methods
Notation. We use a bold-faced uppercase letter to denote a matrix (e.g. W), a bold-faced lowercase
letter to denote a vector (e.g. w), and a normal lowercase letter to denote a scalar (e.g. w).
3.1	Problem Statement
The goal of optimization is to find the global optimum for an objective function f (w) w.r.t. w:
w* = argmin f (w).	(1)
w
w* is assumed unknown and treated as a random vector in this study. Once a optimizer obtains W, its
estimate of w*, it is important to assess the quality and the uncertainty of the solution. Considering
that many real-world objective functions are nonconvex and noisy in W, solution quality is often
measured by ||W - w*||, the proximity to the global optimum rather than that to the optimal function
value. Examples include energy functions as the objective and RMSDs as the proximity measure in
protein docking (Lensink et al., 2007). Therefore, the goal of uncertainty quantification (UQ) here
is the following:
P(||W - w*|| 6 r |D) = σ	(2)
where rσ is the upper bound of ||W - w*|| at σ confidence level, and D denotes samples during
optimization. Such UQ results additionally provide confidence in the solution W and improve model
reliability for end users.
To calculate the probability defined in Eq 2 and perform UQ, a direct albeit challenging way is to
model the posterior over W* (p(W* |D)) and then sample from the posterior. When the optimizer g
is regarded fixed in existing literature, the posterior is actually p(W* |D, g). A central contribution
of ours is to further consider the optimizer as a source of uncertainty, model it as a random vector in
an optimizer space, and perform posterior estimation of p(W* |D).
3.2	Optimizer Uncertainty: A Framework
An optimizer is directly responsible for optimization and thus naturally a source of solution uncer-
tainty. To address this often-neglected uncertainty source, we first define the space of optimizers and
then model an optimizer as a point in this space. Considering that many widely-used optimizers are
iterative and using first-order derivatives, we restrict the optimizer space as follows:
Definition 3.1 ((First-order Iterative) Optimizer Space) We define a first-order, iterative algo-
rithmic space G, where each point g ∈ G is an iterative optimizer, that has the following mapping:
g({Vf (WT)}T=ι) = δwt, where Vf (W)T and δwt are the gradient and the update vector at Tth
and tth iteration, respectively.
Here We use g(∙) to denote a pre-defined update rule and the resulting optimizer. For instance, in
gradient descent, g({Vf(WT)}tT=1) = -αVf (Wt), where α is the step size. Now that the optimizer
space is defined, we next define the (unknown) optimal optimizer and its uncertainty.
Definition 3.2 (Optimal Optimizer) We define the optimal optimizer g* ∈ G as the optimizer that
can obtain the lowest function value with a fixed budget T :
T
g* = arg min(E f(Wg))	(3)
g∈G	t=1
where Wgt = Wgt-1 + g({Vf(WgT)}tT-=11) is the parameter value at tth iteration updated through the
optimizer g.
In practice the optimal optimizer g* is unknown so we treat g* as a random vector and formally
define the optimizer uncertainty as follows:
Definition 3.3 (Optimizer Uncertainty) Let G be the algorithmic space, where each point g ∈ G
is an optimizer. We assume there is a prior distribution over the optimal optimizer g* as p(g*). We
also assume a likelihood distribution as p(D|g*), where D are the observed data (sample trajectory)
given g*. Then we define the optimizer uncertainty throughp(g*∣D) H p(D∣g*)p(g*).
3
Under review as a conference paper at ICLR 2021
To inject the optimizer uncertainty into p(w*∣D), it is straightforward to have the following integra-
tion for posterior estimation:
p(w* |D)= /
p(g*∣D)p(w*∣D,g")dg
(4)
3.3	Parameterizing the Optimizer Space
The optimizer uncertainty p(g*∣D) as defined in Def. 3.3 can be intractable when there is no proper
parameterization of the optimizer space G. Therefore, we next introduce possible ways of parame-
terizing G as defined in Def. 3.1.
Parameterization through Hyperparameters of Specific Optimizers. A simple way to param-
eterize the optimizer space for classical optimizers (e.g. Gradient Descent, Adam) is based on their
hyperparameters: G = H, where H is the hyperparameter space. For instance, for gradient descent,
we have H = (α), where α is the learning rate. For Adam, we have H = (α, β1, β2), where β1 and
β2 are the coefficients used for computing running averages of gradient and its square.
However, such parameterization has significant drawbacks. The resulting algorithmic space G is
very limited and heavily depends on the specific optimizer. The G (a 1D space) parameterized by
the hyperparameters of gradient descent is different from that (a 3D space) parameterized by the
hyperparameters of Adam. In fact, each is a rather restricted region of the actual G . The intrinsic
flexibility (uncertainty) that lies in an iterative optimizer’s update rule is not explored at all in this
parameterization. These drawbacks are empirically demonstrated in Sec. 4.
Parameterization through Neural Networks. In order to reasonably and accurately model the
intrinsic uncertainty within the update rule, we need to find a much more flexible way for modelling
g. We thus consider to parameterize the optimizer space as a neural network: G = Θ, where each
θ ∈ Θ are the parameters in the neural network. Overcoming drawbacks of the optimizer space H by
hyperparameters, Θ by neural network parameters generalizes update rules through neural networks
that can represent a wide variety of functions. We note that this is also the space of meta-optimizers
that learn to optimize (L2O) iterative update rules from data on a given task (Andrychowicz et al.,
2016; Chen et al., 2017; Lv et al., 2017; Cao et al., 2019a). However, there has been no notion of
uncertainty let alone the task of UQ for the learned optimizer in these L2O methods, which is to be
addressed in our Bayesian L2O (BL2O).
3.4	Modeling an optimizer as a random vector
Now that we have the optimizer space G properly defined and parameterized, we proceed to model
an optimizer g as a random vector in the space.
Boltzmann-shaped Posterior. Since we have G = Θ, we can rewrite each g ∈ G as gθ with
θ ∈ Θ and the optimal optimizer g* as gθ*. Therefore, p(g* ∣D) becomes p(θ*∣D). We consider
a Gaussian prior over the parameters of the neural network : p(θ*) 8 exp(-λ∣∣θ* ||2), where λ
is a constant controlling the variance. We use the chain rule to decompose the likelihood function
p(D∣θ*) atafixedbudge T:
T
T
p(D∣θ*) = ∏p(f(wθ*), wθ*∣θ*,{f(wθ*), wθ*}T=0) = ∏p(f (wθ*), |
t=1
t=1
wθ *, θ*, {f (wθ *), wθ JT=O)
(5)
The second equality is due to that wθ* is fixed given θ* and past data points. For the single sample
likelihood, we apply the results from Ortega et al. (2012); Cao & Shen (2020) and obtain
P(f(wθ*), ∣wθ*, θ", {f(wθ*), wθ*}T=1) H exp(-f(wθ*))	(6)
We multiply the likelihood functions of all samples together and obtain the Boltzmann-shaped like-
lihood function as p(D∣θ*) h exp(- PT=I f (wθ*)). We finally multiply the conjugate prior to the
likelihood and obtain the Boltzmann-shaped posterior as:
T
p(θ* |D) (X exp(- X f(Wθ*)) ∙ exp(-λ∣∣θ* ||2) = exp(-F(θ*))	⑺
t=1
4
Under review as a conference paper at ICLR 2021
where F(θ*) = PT=I f (wθ*) + λ∣∣θ*∣∣2, which actually contains the objective in Eq 3 plus a L2
regularization constant.
Local Approximation and Bayesian Loss. However, the above posterior distribution involves
an integral in the normalization constant which is computationally intractable. Moreover, the ar-
chitecture of F(θ*) is so complicated that it is impossible to directly sample from the posterior
distribution. In order to overcome the aforementioned challenges, we would like to learn a distribu-
tion function q(θ*∣φ) that has the analytic form and is easy to be sampled, where φ is the parameter
vector in q(θ*∣φ), to approximate the real posteriorp(θ*∣D).
Furthermore, due to the high dimensions of θ* and the complicated landscape of the posterior,
it is impossible to approximate p(θ*∣D) at every position in the θ* space. We then consider to
approximate it locally around θc, an optimum of interest for F(θ*).
We denote the local region as Θc, a neighborhood around θc, and re-normalization constant C =
Jθ*三θc p(θ*∣D)dθ*. Then the local posterior will be a conditioned (re-scaled) version of p(θ*[D):
p0(θ*∣D)	=	p(θ*∣D)∕C,	θ*	∈	Θc.	In order to make	q(θ*∣φ)	≈	p0(θ*∣D),	We calculate the
KL-divergence between these two:
"")""◎)= Z…qsφ)l°g PO) 附=/*—("°限 p(⅛w⅛dθ
=L∈ΘC qw*lφ)logeχP((θFφθ*)) dθ*+Zθ*∈θc q("0)log(ZC)dθ*,
(8)
where Z = ʃ exp(-F(θ*))dθ* is the normalization constant. The second term in the above equa-
tion equals to log(ZC), a constant w.r.t. φ, thus could be ignored during optimization.
We then propose our Bayesian loss as:
Fb(Φ) = [	q(θ*∣φ)log q(θ*∣φ)dθ* + [
θ* ∈Θc	θ* ∈Θ
=-H (q(θ*∣φ)) + Eq(θ*∣φ)[F (θ*)],
q(θ*∣φ)F(θ*)dθ*
c
(9)
where the first term of FB measures the negative entropy of our approximated posterior, and the
second term is the expectation of the loss function over of posterior.
Gaussian Posterior. For local approximation, We consider φ = (μ, Σ) and q(θ*∣φ) = N(μ, Σ),
where μ is the mean vector and Σ is the covariance matrix of a normal distribution. For simplic-
ity, we consider Σ to be a diagonal matrix: Σ = diag(σ12, σ22, σ32, ...). The second term in Eq (9)
involves the integral over F(θ*), which is intractable. Therefore, we use Monte Carlo sampling
through q(θ*∣φ) to replace the integral there. However, the direct sampling of the posterior pa-
rameters makes it difficult for the optimization as it is inaccessible to get the gradient w.r.t. μ and
Σ. Moreover, the standard deviation σ1, σ2, ... must be non-negative, making the optimization
constrained.
To overcome those two challenges, we use the trick introduced in (Blundell et al., 2015) to shift
sampling from q(θ*∣φ) to sampling from a standard normal distribution N (0, I). And we reparam-
eterize standard deviation σi to Pi as σi = log(1 + exp(ρi)). Then for any E sampled from N(μ, I),
we could calculate θ* as θ* = U + log(1 + exp(ρ)) Θ e, where Θ means element-wise product and
ρ = (ρ1 , ρ2 , ...).
3.5	Bayesian Averaging
We recall our goal to build the posterior over the global optimum: p(w*∣D) through Eq 4. We
consider using Monte Carlo sampling to approximate the integral as:
p(w*∣D) = /
p(g(∙)∣D)p(w*∣D,g)dg
/
θ* ∈Θ
N
q(θ*∣φ)p(w*∣gθ*(∙),D)dθ* ≈ Xp(w** (∙),D)
i=1
(10)
where θ* is sampled from q(θ*∣φ). Since q(θ*∣φ) follows a multivariate Gaussian distribution
where individual dimensions are independent of each other, we estimate the summation above in

5
Under review as a conference paper at ICLR 2021
each dimension using independent MC samplings. In practice, N being 10, 000, 100, 000, and
500, 000 led to negligible differences in the 1D estimations, and N was thus fixed at 10, 000.
3.6	Meta-training Set
In order to boost the robustness and generalizability of our optimizer posterior p(g*∣D), We consider
using an ensemble of objective functions F = {fi}iN=1. Specifically, we replace the objective
function in Eq 3 with 焉 PN=I PT=I fi(wθ* i) and rewrite F(θ*) as:
1NT
F (θ*) = NN XX fi(wθ *,i) + λ∣∣θ*∣∣2	(11)
i=1 t=1
where wθt * ,i is the solution at tth iteration for objective fi optimized by gθ* . Such replacement
can let our posterior generalize to novel objective functions. We regard the functional dataset F as
the meta-training set. During the experiments, we create different meta-training sets for different
problems, which will be described in details in Sec 4. We note that Eq 11 is also the objective or part
of the objective in many meta-optimizers (Andrychowicz et al., 2016; Chen et al., 2017; Lv et al.,
2017; Cao et al., 2019a). However, those methods are focusing on training a deterministic optimizer
without uncertainty-awareness.
3.7	Two-stage Training for empowering the local posterior
As mentioned before, due to the extreme large optimizer space, we focus on modelling the posterior
locally around θc, an optimum of interest. Ifwe directly train our model through the Bayesian loss
in Eq 9, we simply regard that our posterior is locally around the random initialized point. In order
to obtain an real optimum of interest θc, we first train our model in a non-Bayesian way through
minimizing the loss in Eq 11. We then use θc as the warm start for μ, and start the second Bayesian
training stage through the loss in Eq 9. Both training stages are critical for empowering our local
posterior. Such statement can be demonstrated through the ablation study in Appendix F.
3.8	Model Architecture, Implementation and Computational Complexity
The model is implemented in Tensorflow 1.13 (Abadi et al., 2016) and optimized by Adam (Kingma
& Ba, 2014). For the optimizer architecture, we use the coordinate-wise LSTM from Andrychowicz
et al. (2016). We also validate this design choice in Appendix F. Due to the coordinate-wise nature,
our BL2O model only contains 10,282 free parameters. For all experiments, the length of LSTM is
set to be 20. Both training stages include 5,000 training epochs.
The time complexity for BL2O is O(KBNe +KNeH2), where K is the number of sampling trajec-
tories, B is the minibatch size, Ne is the number of objective parameters, and H is the hidden size
of LSTM (H = 20 in the study). As the batch size increases, the computational cost is close to the
traditional Bayesian neural networks trained through SGD. Due to the coordinate-wise LSTM, the
space coomplexity (memory cost) of BL2O is only O(H 2), which remains the same as the number
of objective parameters varies. Both the time and the space complexity of BL2O are the same as
DM_LSTM (Abadi et al., 2016), while those of Adam are O(KBNe) and O(Ne), respectively.
4 Experiments
We test our BL2O model extensively on optimizing: non-convex test functions, energy functions in
protein-protein interactions, loss functions in image classification and loss functions in data privacy
attack. We compare BL2O to three non-Bayesian methods: Adam, Particle Swarm Optimization
(PSO) (Kennedy & Eberhart, 1995), DM-LSTM (Andrychowicz et al., 2016) and a recently PUb-
lished Bayesian method, BAL (Cao & Shen, 2020). All algorithms are running for 10,000 times with
random initializing points to obtain the empirical posterior distributions. During each run, the hy-
perparameters in Adam and PSO are sampled from Table 4 in Appendix A. Out of 10,000 solutions
we choose the one with the lowest function value to be the final solution (W).
Generally, for optimization performance, we assess the distance between the final solution and the
global optima: ||W - w*||. The lower the distance is, the better the solution quality is. For un-
certainty quantification, we assess the upper bound rσ and the real confidence σ given a fixed
6
Under review as a conference paper at ICLR 2021
confidence level σ . The real confidence σ is defined by the fraction of 10,000 solutions that actu-
ally fall in the bounded region. The lower the rσ is, the tighter the confidence interval is. And the
closer of σ to σ is, the more accurate the confidence estimate is.
Comparison in optimizing test functions. We first test the performance on test functions in the
global optimization benchmark set (Jamil & Yang, 2013). We choose three extremely rugged, non-
convex functions: Rastrigin, Ackley and Griewank in 5 dimensions: 6D, 12D, 18D, 24D, 30D. For
each function, we create a diverse, broad family of similar functions fj (w) as the meta-training set
used for training DM_LSTM and BL2O. The analytical forms and the meta-training sets of those
functions are shown in Table 5 in Appendix B.
We compare BL2O with all 4 competing methods. The optimization and UQ performances are
shown in Fig. 1. In all three cases and 5 dimensions, BL2O has led to the best solution quality. In
terms of UQ, BL2O has shown the most accurate confidence estimation (σ ≈ σ) when σ = 0.9 and
σ = 0.8, while BAL was the second best. And BL2O has shown much tighter confidence intervals
rσ against BAL. In some cases, although DM_LSTM has lower r。than BL2O, it has much lower
confidence level, indicating that this tight upper bound in DM_LSTM is miscalibrated. As a result,
BL2O has shown the best performance in both optimization and UQ.
Rastrigin
-⅜- Adam
-⅜- PSO
-⅜- BAL
-⅜- DM-LSTM
-⅜- BL云)
RaStngIn
663
0987
■ ■ ■ ■
Iooo
RaStrigin
ι.0 co
0.9 6
0.83
0.7
14-
12-
10
8
6
4
2-
6	1L, 18 . 24 30
Dimension
Ackley
10
8 6 4 2 0
=*∕IΛI 哈--
6	1L, 18 . 24 30
Dimension
GrieWank
6
6 5 4 3 2 1 0
-W=
12	18	24	30
Dimension
14
010
J 6
2-
4 0 6 2
1 1
∞0u
6	12.	18 .24
Dimension
6	121	18 .24	30
Dimension
GrieWank
68
0987
■ ■ ■ ■
Iooo
6	12.	18 .24
Dimension
≈ιo	一.一■
E 6 ∙---→一二:一.一
2 .二二二章三三聿二三二；…二才
6	1?. 18 1 3。
Dimension
6$
2
6	12	18	24	30
Dimension
GrieWank
Figure 1: The optimization performance (left) and the UQ performance (rσ and σ) of different
methods on three test functions.
Comparison in optimizing energy functions for protein docking. We then apply BL2O to a
bioinformatics application: predicting the 3D structures of protein-complexes (Smith & Sternberg,
2002), called protein docking. Ab initio protein docking can be recast as optimizing a noisy and
expensive energy function in a high-dimensional conformational space (Cao & Shen, 2020): x* =
arg minx f (x). While solving such optimization problems still remains difficult, quantifying the
uncertainty of resulting optima (docking solutions) is even more challenging. In this section, we
apply our BL2O to optimization and uncertainty quantification in protein docking and compare with
a state-of-the-art method BAL (Cao & Shen, 2020).
We describe the detailed settings of BL2O on protein docking in Appendix C. From BL2O, we obtain
a posterior distribution p(w* D) over the native structure w* and the lowest energy structure, W. In
protein docking, the quality of a predicted structure is based on the distance to the native structure
(the global OPtimUm): ||W - w* ||. For UQ, We assess the two-sided confidence interval at σ = 0.9
as P(l0.9 6 ||W - w*|| 6 r0.9) = 0.9.
7
Under review as a conference paper at ICLR 2021
In Table 1, we assess ||W - w*||, r0.9 -10.9 and whether ||W - w*|| is within the confidence interval.
For optimization, BL2O clearly outperforms BAL in two medium cases while performing slightly
worse in the other cases. Yet for UQ, BL2O shows clearly superior performance over BAL in all
cases, with accurate or/and tight confidence intervals. We also visulize the posterior distributions
over ||W - w*|| for protein 1JMO_4. As shown in Fig 2, We can see compared to that of BAL,
BL2O's distribution has real ||W - w*|| within the 90% C.I. and smaller variance. More posterior
distributions are shown in Appendix D.
Table 1: Performances in optimization and uncertainty quantification on 5 docking cases.
	||w - w」		r0.9 — lθ.9 (A)		||W — w*|| ∈ [l0.9,r0.9]?	
Target (docking diffficulty)	BAL	BL2O	BAL	BL2O	BAL	BL2O
1AHW_3 (easy)	1.89	2.07	2.20	1.98	No	No
1AK4_7 (easy)	2.45	2.70	1.93	1.66	Yes	Yes
3CPH7 (medium)	3.89	3.21	1.70	2.20	No	Yes
1HE8_3 (medium)	3.05	2.32	2.24	1.61	Yes	Yes
UMO_4 (difficult)	1.45	1.55	2.90	1.26	No	YeS	
Figure 2: Visualizations of estimated posterior distributions and confidence intervals.
Comparison in optimizing loss functions in image classification. We then test the performance
of optimizing the loss function in image classification on the MNIST dataset. We apply a 2-
layers MLP network as the classifier. The competing methods include Adam, DM-LSTM and two
Bayesian neural network methods: variational inference (VI) (Blundell et al., 2015) and Learnable
Bernoulli Dropout (LBO) (Boluki et al., 2020). Moreover, for DM_LSTM and BL2O, we apply a
trick during the optimizer training called curriculum learning (CL) and introduce it in detail in Ap-
pendix E for training over long-term iterations. We call DM-LSTM with CL as DM-LSTM-C and
BL2O with CL as BL2O_C.
The assessment of the optimization and UQ for this machine learning task is different from that
for optimization before. In terms of optimization, we assess the classification accuracy on the test
set. In terms of UQ, we measure two metrics that assess the robustness and trustworthiness of the
classifier: the in-domain calibration error and the out-of-domain detection rate.
We first compare the accuracy on the testing set among different methods. As shown in Table 2,
Adam, DM-LSTM-C and BL2O_C have almost the same best performance. The significant im-
provement from DM_LSTM to DM_LSTM_C, and from BL2O to BL2O_C shows the big advantage
of curriculum learning in learning to optimize. In conclusion, BL2O_C had on par accuracy with
Adam and DM_LSTM_C on the MNIST dataset.
However, classification models must not only be accurate, but also indicate when they are likely to
be incorrect. Confidence calibration, the probability that estimates the true likelihood of each pre-
diction is also important for classification models. In the ideal case, the maximum output probability
(MaxConfidence) for each test sample should be equal to the prediction accuracy for that sample.
To assess the calibration of each methods, we split the test set into 20 equal-sized bins and assess the
calibration error as the average discrepancy between accuracy and MaxConfidence in each bin. As
seen in Table 2, among all methods compared, BL2O_C and BL2O had the least calibration error.
The figure of Acc. vs MaxConf. is also shown in Fig. 4 in Appendix E.
We also inspect the out-of-domain detection of BL2O, BL2O_C and competing methods. We train
all models on the data belonging to the first 5 classes in the MNIST training dataset (the last layer
of the optimizee is modified to have 5 rather than 10 neurons) and test them on the remaining
8
Under review as a conference paper at ICLR 2021
Table 2: Performance of classification on the MNIST test set.
Models	In-Domain Accuracy (%)	In-Domain Calibration Error	Out-of-Domain Detection	
			q.4(%)	q.5(%)
Adam	93.2	=	5.0E-4	^07	~∑8
DM_LSTM	81.0	4.2E-3	0.4	2.0
DM-LSTM-C	93.4	9.9E-4	4.6	10.6
VI	87.8	4.4E-3	4.8	10.6
LBO	91.2	9.6E-4	5.2	11.3
BL2O	901	4.9E-4	^99	-31.8
BL2O工	93.5	4.3E-4	12.4	20.9	
samples from the other 5 classes. An ideal model would predict a uniform distribution over the 5
wrong classes. Therefore, we define the out-of-domain detection rate at threshold t, qt, as the
percentage of test samples with max class confidence below t. The larger the qt , the better out-of-
domain detection is. As shown in Table 2, BL2O and BL2O_C shows superior performance with all
competing methods. Notably, BL2O without curriculum learning had much better out-of-domain
detection rates compared to BL2O with curriculum learning.
Comparison in optimizing loss functions for data privacy attack. We finally apply our model
to an application that critically needs UQ. As many machine learning models are deployed publicly,
it is important to avoid leaking private sensitive information, such as financial data, health data and
so on. Data privacy attack (Nasr et al., 2018) studies this problem by playing the role of hackers and
attacking the machine-learning models to quantify the risk of privacy leakage. Better attacks would
help models to be better prepared for privacy defense.
We use the model and dataset in (Cao et al., 2019b), where each input has 9 features involving
patient genetic information and the output p is the probability of the clinical significance (having
cancer or not) for a patient. We study the following model inversion attack (Fredrikson et al., 2015):
by giving 5 features w0 ∈ [0, 1]5 out of 9 and the label p of each patient, we want to recover the
rest 4 features w* ∈ [0,1]4 (potentially sensitive patient information). Therefore, for each patient,
the objective is w* = argminw∈[o,i]4(m(w0, W) - p)2, where w* is the ground-truth of W and
m is the trained predictive model. The closeness between the predicted and the real input features
can quantify the risk of information leakage and the quality of the attack. We compare BL2O with
Adam, PSO, BAL and DM_LSTM on optimization and UQ on all test cases in (Cao et al., 2019b).
The meta-training objectives for BL2O and DM_LSTM are the training set in (Cao et al., 2019b).
As shown in Table 3, BL2O has shown the best performance in both optimization and UQ com-
pared to all competing methods. It is noteworthy that learned optimizers (DM-LSTM and BL2O)
had much better optimization performance than pre-defined optimizers. And the Bayesian meth-
ods (BAL and BL2O) had significantly better UQ performance than non-Bayesian methods. BL2O
possessed the advantages of both learned and Bayesian optimizers to achieve the best performance.
Table 3: The optimization and UQ performance of different methods on data privacy attack.
	||w - w*||	ro.9	kθ.9 - 0.9|	ro.8	kθ.8 - 0.8|
Adam	0.45	0.74	00	0.63	020
PSO	0.32	0.82	0.10	0.72	0.20
BAL	0.34	0.53	0.06	0.41	0.08
DM-LSTM	0.20	0.52	0.10	0.47	0.19
BL2O	0.17	0.43	0.05	0.32	0.04
5 Conclusion
Current optimization algorithms, even with uncertainty-awareness, do not address the uncertainty
arising within the optimizer itself. To close this gap, we parameterize the update rule as a neural net-
work and build a Boltzmann-shaped posterior over the algorithmic space. We apply our Bayesian
Learning-to-Optimize (BL2O) framework to optimize test functions, energy functions in protein
docking, loss functions in image classification and loss functions in data privacy attack. The empiri-
cal results demonstrate that BL2O outperforms the state-of-the-art methods in both optimization and
uncertainty quantification, as well as the calibration and out-of-domain detection in classification.
9
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSD116),pp. 265-283, 2016.
Mohamed Osama Ahmed, Bobak Shahriari, and Mark Schmidt. Do we need “harmless” bayesian
optimization and “first-order” bayesian optimization. NIPS BayesOpt, 2016.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in Neural Information Processing Systems, 2016.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings of the 26th annual international conference on machine learning, pp. 41-48, 2009.
Hildo Bijl, Thomas B Schon, Jan-Willem van Wingerden, and Michel Verhaegen. A sequen-
tial monte carlo approach to thompson sampling for bayesian optimization. arXiv preprint
arXiv:1604.00169, 2016.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in
neural networks. arXiv preprint arXiv:1505.05424, 2015.
Shahin Boluki, Randy Ardywibowo, Siamak Zamani Dadaneh, Mingyuan Zhou, and Xiaoning Qian.
Learnable bernoulli dropout for bayesian deep learning. arXiv preprint arXiv:2002.05155, 2020.
Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.
Yue Cao and Yang Shen. Bayesian active learning for optimization and uncertainty quantification in
protein docking. Journal of chemical theory and computation, 16(8):5334-5347, 2020.
Yue Cao, Tianlong Chen, Zhangyang Wang, and Yang Shen. Learning to optimize in swarms. In
Advances in Neural Information Processing Systems, pp. 15018-15028, 2019a.
Yue Cao, Yuanfei Sun, Mostafa Karimi, Haoran Chen, Oluwaseyi Moronfoye, and Yang Shen. Pre-
dicting pathogenicity of missense variants with weakly supervised regression. Human mutation,
40(9):1579-1592, 2019b.
Yutian Chen, Matthew W Hoffman, Sergio Gomez Colmenarejo, Misha Denil, Timothy P Lillicrap,
Matt Botvinick, and Nando de Freitas. Learning to learn without gradient descent by gradient
descent. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 748-756. JMLR. org, 2017.
Jeremy S De Bonet, Charles Lee Isbell Jr, and Paul A Viola. Mimic: Finding optima by estimating
probability densities. In Advances in neural information processing systems, pp. 424-430, 1997.
Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks that exploit confi-
dence information and basic countermeasures. In Proceedings of the 22nd ACM SIGSAC Confer-
ence on Computer and Communications Security, pp. 1322-1333, 2015.
David E Goldenberg. Genetic algorithms in search, optimization and machine learning, 1989.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 1321-1330. JMLR. org, 2017.
Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global optimiza-
tion. The Journal of Machine Learning Research, 13(1):1809-1837, 2012.
10
Under review as a conference paper at ICLR 2021
Jose MigUel Henrandez-Lobato, Matthew W. Hoffman, and ZoUbin Ghahramani. Predictive Entropy
Search for Efficient Global Optimization of Black-box Functions. In Proceedings of the 27th In-
ternational Conference onNeural Information Processing Systems - Volume 1, NIPS’14, pp. 918—
926, Cambridge, MA, USA, 2014. MIT Press. URL http://dl.acm.org/citation.
cfm?id=2968826.2968929.
Jose Miguel Hernandez-Lobato, Matthew W Hoffman, and Zoubin Ghahramani. Predictive entropy
search for efficient global optimization of black-box fUnctions. In Advances in neural information
processing systems, pp. 918-926, 2014.
Howook Hwang, Thom Vreven, Joel Janin, and Zhiping Weng. Protein-Protein Docking Benchmark
Version 4.0. Proteins, 78(15):3111-3114, November 2010. ISSN 0887-3585. doi: 10.1002/prot.
22830. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2958056/.
Momin Jamil and Xin-She Yang. A literature survey of benchmark functions for global optimisation
problems. International Journal of Mathematical Modelling and Numerical Optimisation, 4(2):
150-194, 2013.
Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer
vision?, 2017.
J Kennedy and R Eberhart. Particle swarm optimization, proceedings of ieee international confer-
ence on neural networks (icnn’95) in, 1995.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Scott Kirkpatrick, C Daniel Gelatt, and Mario P Vecchi. Optimization by simulated annealing.
science, 220(4598):671-680, 1983.
Marc F. Lensink, RaUl Mendez, and Shoshana J. Wodak. Docking and scoring protein complexes:
CAPRI 3rd Edition. Proteins: Structure, Function, and Bioinformatics, 69(4):704-718, Decem-
ber 2007. ISSN 1097-0134. doi: 10.1002/prot.21804. URL https://onlinelibrary.
wiley.com/doi/abs/10.1002/prot.21804.
Daniel James Lizotte. Practical bayesian optimization. University of Alberta, 2008.
Kaifeng Lv, Shunhua Jiang, and Jian Li. Learning gradient descent: Better generalization and longer
horizons. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 2247-2255. JMLR. org, 2017.
Iain H. Moal and Paul A. Bates. SwarmDock and the Use of Normal Modes in Protein-Protein Dock-
ing. International Journal of Molecular Sciences, 11(10):3623-3648, September 2010. ISSN
1422-0067. doi: 10.3390/ijms11103623. URL https://www.ncbi.nlm.nih.gov/pmc/
articles/PMC2996808/.
Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis of deep learning:
Stand-alone and federated learning under passive and active white-box inference attacks. arXiv
preprint arXiv:1812.00910, 2018.
Pedro Ortega, Jordi Grau-Moya, Tim Genewein, David Balduzzi, and Daniel Braun. A nonparamet-
ric conjugate prior distribution for the maximizing argument of a noisy function. In Advances in
Neural Information Processing Systems, pp. 3005-3013, 2012.
Michael A Osborne, Roman Garnett, and Stephen J Roberts. Gaussian processes for global opti-
mization. In 3rd international conference on learning and intelligent optimization (LION3), pp.
1-15, 2009.
Martin Pelikan, David E Goldberg, Erick CantU-Paz, et al. Boa: The bayesian optimization al-
gorithm. In Proceedings of the genetic and evolutionary computation conference GECCO-99,
volUme 1, pp. 525-532. Citeseer, 1999.
11
Under review as a conference paper at ICLR 2021
Brian G. Pierce, Kevin Wiehe, Howook Hwang, Bong-Hyun Kim, Thom Vreven, and Zhip-
ing Weng. ZDOCK server: interactive docking prediction of protein-protein complexes and
symmetric multimers. Bioinformatics, 30(12):1771-1773, 02 2014. ISSN 1367-4803. doi:
10.1093/bioinformatics/btu097. URL https://doi.org/10.1093/bioinformatics/
btu097.
K. A. Porter, I. Desta, D. Kozakov, and S. Vajda. What method to use for protein-protein docking?
Curr. Opin. Struct. Biol., 55:1-7, 04 2019.
B. Shahriari, K. Swersky, Z. Wang, R. P. Adams, and N. de Freitas. Taking the Human Out of the
Loop: A Review of Bayesian Optimization. Proceedings of the IEEE, 104(1):148-175, January
2016. ISSN 0018-9219. doi: 10.1109/JPROC.2015.2494218.
Alexander Shapiro. Probabilistic constrained optimization: Methodology and applications. Statisti-
cal inference of stochastic optimization problems, pp. 282-304, 2000.
Graham R Smith and Michael JE Sternberg. Prediction of protein-protein interactions by docking
methods. Current opinion in structural biology, 12(1):28-35, 2002. Niranjan Srinivas, Andreas Krause, Sham M Kakade, and Matthias Seeger. cess optimization in the bandit setting: No regret and experimental design.	Gaussian pro- arXiv preprint
arXiv:0912.3995, 2009.
Emmanuel Vazquez and Julien Bect. Convergence properties of the expected improvement algo-
rithm with fixed mean and covariance functions. Journal of Statistical Planning and inference,
140(11):3088-3095, 2010.
Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization. arXiv
preprint arXiv:1703.01968, 2017.
Jian Wu, Matthias Poloczek, Andrew G Wilson, and Peter Frazier. Bayesian optimization with
gradients. In Advances in Neural Information Processing Systems, pp. 5267-5278, 2017.
Anatoly A Zhigljavsky. Theory of global random search, volume 65. Springer Science & Business
Media, 2012.
12
Under review as a conference paper at ICLR 2021
A Optimizer distribution settings for Adam and PSO
Methods
Adam
PSO
Optimizer Distribution Settings
Iogio(Ir)〜U[-2,-1], βι 〜U[0.9,1.0], B2〜U[0.999,1.0]
W 〜U[0.5,1.5], C1 〜U[1.5, 2.5], C2 〜U[1.5, 2.5]
Table 4: The optimizer distributions over hyperparameters in Adam and PSO.
13
Under review as a conference paper at ICLR 2021
B Analytic forms and Meta-training sets of Test functions
Function Name	Analytic form
Rastrigin	fj(w) = ∣∣w∣∣2 - Pn=I 10cos(2πwi) + 10n
Ackley	fj(W) = —20 exp(-0.2p0.5∣∣w∣∣2) — Pn=I exp(cos(2πwi)∕n) + e + 20
Griewank	fj (W) = 1 + 40100 P2ι ||w||2 - Qi=ι COs(Wi)
	Meta-training set
Rastrigin	fj(w) = || Ajw — bj ||2 — 10cj cos(2πw) + 10n
Ackley	fj (w) = —20exp(-0.2p0.5∣∣AjW — bj||2) — exp(cj cos(2πw)∕n)
Griewank	fj (W) = 1 + 40100 Pn=I ||AjW - bj ||2 - Qn=I(Cos(Wi) + Cji - I)
Table 5: The analytic forms of test functions and the meta-training sets used for training DM_LSTM
and BL2O, where n is the dimension and Aj ∈ Rn×n, bj ∈ Rn×1 and cj ∈ Rn×1 are parameters
whose elements are sampled from i.i.d. normal distributions. It is obvious that the test functions are
the special cases in the training sets, respectively.
14
Under review as a conference paper at ICLR 2021
C Settings for Protein Docking Experiments
We calculate the energy function (objective function f (x)) in a CHARMM 19 force field as in (Moal
& Bates, 2010). 25 protein-protein complexes are chosen from the protein docking benchmark set
4.0 (Hwang et al., 2010) as the training set, which is shown in Table 6. For each target, we choose 5
starting points (top-5 models from ZDOCK (Pierce et al., 2014)). In total, our training set includes
125 samples. Moreover, we parameterize the search space as R12 as in BAL (Cao & Shen, 2020).
The resulting f(x) is fully differentiable in the search space. We only consider 100 interface atoms
due to the computational concern. The number of iterations for one training epoch is 600 and in total
we have 5000 training epochs. Both BL2O and BAL have 600 iterations during the testing stage.
For fair comparison, after optimization, we rescore the BL2O samples for UQ using the scoring
function (random forest) in BAL.
Table 6: 4-letter ID of proteins used in the training set.
Difficulty level	Protein Data Bank (PDB) code
-ɪigid	1N8O ,7CEI,1DFJ ,1AVX ,1BVN ,1IQD ,1CGI,1MAH ,1EZU , 1JPS , 1PPE ,1R0R , 2I25,2B42,1EAW , 2JEL ,1BJ1,1KXQ , 1EWY
Medium	1XQS ,1M10,1JK,1GRN
Flexible	1IBR ,1ATN
15
Under review as a conference paper at ICLR 2021
D Posterior Distributions in Protein Docking
BAL IAHW 3
>⅛ω⊂ΦQ
——Real ∣∣w*,w∣∣
5% tails
90% C.l.
BAL 1AK4 7
>⅛ω⊂ΦQ
6
BL2O IAHW 3
>⅛ω⊂ΦQ
B2L0 1AK4 7
-----Real ∣∣w ,w∣∣
5% tails
90% c.ι.
A 4'5iu ①。
>^-ω⊂ΦQ
A 4-SU ①。
Figure 3: Visualizations of estimated posterior distributions and confidence intervals for more dock-
ing cases.
2
3
5
6
16
Under review as a conference paper at ICLR 2021
E	Curriculum Learning and Calibration Figure in Image
CLASSIFICATION
A common issue in learning to optimize for neural network training is that: the optimizer training
usually takes hundreds of iterations, while training a neural network usually costs thousands or tens
of thousand iterations. For instance, for a MNIST training dataset consisting of 50,000 images,
training a neural network for 100 epochs corresponds to almost 20,000 iterations with a batch size
of 128.
Hundreds of iterations are good for the first few epochs during optimizer training, since we would
like to only focus on decreasing the loss in the first hundreds of iterations. However, as training goes
on, we would like to see that the later iterations could also decrease the training loss. During this
stage, only hundreds of iterations are clearly not enough.
In order to overcome this issue, we bring the idea from (Bengio et al., 2009). Specifically, We set a
list for the number of iterations: [100, 200, 500, 1000, 1500, 2000, 2500, 3000] and we gradually
increase the number of iterations following the list for every 100 epochs in optimizer training if the
optimizee loss is decreasing. Once the number reaches 3000, it will not change any more until the
training ends.
0.0
1.0
0.8
0.6
0.4
0.2
>υs⊃8<
0.0	0.2
0.4	0.6	0.8	1.0
MaxConfidence
Figure 4: The calibration figure in image classification.
F Ablation Study.
In order to validate various design choices, we perform the ablation study as follows:
•	B1: We use the coordinate-wise gated recurrent unit (GRU) network as the optimizer ar-
chitecture. We train our model directly on the Bayesian loss: Eq 9 without non-Bayesian
training.
•	B2: We replace the GRU network with the LSTM network.
•	BL2O: We add the non-Bayesian training stage to find a local optimum of interest first
before training on the Bayesian loss.
17
Under review as a conference paper at ICLR 2021
--*∕IΛ— 哈=
Rastngin
ι.o «?
0.9 o
0.83
0.7
14-
sɪɔ
J 6
2-
6	1?. 18 1 3。
Dimension
Figure 5: Ablation study: the optimization performance (left) and the UQ performance (rσ and σ)
of different models on the Rastrigin function.
We test these three models on the Rastrigin test function. As shown in Fig 5, B2(LSTM) has slightly
better performance in both optimization and UQ compared to GRU. But BL2O has shown superior
performance compared to both B1 and B2 in optimization and UQ. These results clearly demonstrate
that the two training stages must be coupled together to empower the local posterior.
18