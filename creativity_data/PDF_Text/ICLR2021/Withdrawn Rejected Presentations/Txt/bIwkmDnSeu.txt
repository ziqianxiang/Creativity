Under review as a conference paper at ICLR 2021
Unbiased Learning with State-Conditioned
Rewards in Adversarial Imitation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial imitation learning has emerged as a general and scalable framework
for automatic reward acquisition. However, we point out that previous methods
commonly exploited occupancy-dependent reward learning formulation—which
hinders the reconstruction of optimal decision as an energy-based model. Despite
the theoretical justification, the occupancy measures tend to cause issues in prac-
tice because of high variance and low vulnerability to domain shifts. Another
reported problem is termination biases induced by provided rewarding and regu-
larization schemes around terminal states. In order to deal with these issues, this
work presents a novel algorithm called causal adversarial inverse reinforcement
learning. Our formulation draws a strong connection between adversarial learn-
ing and energy-based reinforcement learning; thus, the architecture is capable of
recovering a reward function that induces a multi-modal policy. In experiments,
we demonstrate that our approach outperforms prior methods in challenging con-
tinuous control tasks, even under significant variation in the environments.
1	Introduction
Inverse reinforcement learning (IRL) is an algorithm of recovering the ground truth reward function
from observed behavior (Ng & Russell, 2000). IRL algorithms—followed by appropriate reinforce-
ment learning (RL) algorithms—can optimize policy through farsighted cumulative value measures
in the given system (Sutton & Barto, 2018); hence it can usually achieve more satisfying results
than mere supervision. While a few studies have investigated recovering reward functions to con-
tinuous spaces (Babes et al., 2011; Levine & Koltun, 2012), IRL algorithms often fail to find the
ground-truth reward function in high-dimensional complex domains (Finn et al., 2016b).
The notion of the ground-truth reward requires elaboration since IRL is an ill-posed problem; there
can be numerous solutions to the reward function inducing the same optimal policy (Ng et al., 1999;
Ng & Russell, 2000). Recently, adversarial imitation learning (AIL) as a reward acquisition method
has shown promising results (Ho & Ermon, 2016). One of the distinctive strengths of AIL is the
scalability through parameterized non-linear functions such as neural networks.
The maximum causal entropy principles are widely regarded as the solution when the optimal con-
trol problem is modeled as probabilistic inference (Ziebart et al., 2010; Haarnoja et al., 2017). In
particular, probabilistic modeling using a continuous energy function forms a representation called
an energy-based model (EBM). We highlight the following advantages of the energy-based IRL:
•	It provides a unified framework for stochastic policies to the learning; most probabilistic models
can be viewed as special types of EBMs (LeCun et al., 2006).
•	It rationalizes the stochasticity of behavior; this provides robustness in the face of uncertain dy-
namics (Ziebart et al., 2010) and a natural way of modeling complex multi-modal distribution.
AIL reward functions seem to be exceptions to these arguments—the AIL framework produces
distinct types of rewards that are ever-changing and are intended for discriminating joint densities.
We argue that these characteristics hinder proper information projection to the optimal decision.
This work points out that there remain two kinds of biases in AIL. The established AIL algorithms
are typically formalized by the cumulative densities called occupancy measure. We claim that the
accumulated measures contain biases that are not related to modeling purposeful behavior, and the
formulation is vulnerable to distributional shifts of an MDP. Empirically, they work as dominant
1
Under review as a conference paper at ICLR 2021
noises in training because of the formulation’s innate high variance. The other bias is implicit
survival or early termination bias caused by reward formulation, which lacks consideration for the
terminal states in finite episodes. These unnormalized rewards often provokes sub-optimal behaviors
where the agent learns to maliciously make use of temporal-aware strategies.
This paper proposes an adversarial IRL method called causal adversarial inverse reinforcement
learning (CAIRL). We primarily associate the reward acquisition method with approaches for
energy-based RL and IRL algorithms; the CAIRL reward function can induce complex probabilistic
behaviors with multiple modalities. We then show that learning with a dual discriminator architec-
ture provides stepwise, state-conditioned rewards. For handling biases induced by finite-horizon, the
model postulates the reward function satisfies a Bellman equation, including “self-looping” terminal
states. As a result, it learns the reward function satisfying the property of EBMs.
Noteworthy contributions of this work are 1) a model-free, energy-based IRL algorithm that is ef-
fective in high-dimensional environments, 2) a dual discriminator architecture for recovering a ro-
bust state-conditioned reward function, 3) an effective approach for handling terminal states, and 4)
meaningful experiments and comparison studies with state-of-the-art algorithms in various topics.
2	Related Works
Imitation learning is a fundamental approach for modeling intellectual behavior from an expert at
specific tasks (Pomerleau, 1991; Zhang et al., 2018). For the standard framework called Behavioral
Cloning, learning from demonstrations is treated as supervised learning for a trajectory dataset. On
the other hand, IRL aims to study the reward function of the underlying system, which characterizes
the expert. In this perspective, training a policy with an IRL reward function is a branch of imitation
learning, specialized in dealing with sequential decision-making problems by recovering the concise
representation of a task (Ng & Russell, 2000; Abbeel & Ng, 2004).
For modeling stochastic expert policies, Boltzmann distributions appeared in early IRL research,
such as Bayesian IRL, natural gradient IRL, and maximum likelihood IRL (Ramachandran & Amir,
2007; NeU & Szepesvari, 2012; Babes et al., 2011). Notably, maximum entropy IRL (Ziebart et al.,
2008) is explicitly formulated based on the principle of maximum entropy. The framework has also
been derived from causal entropy—the derived algorithm can model the purposeful distribution of
optimal policy into a reward function (Ziebart et al., 2010). Our work draws significant inspirations
from these prior works and aims to redeem the perspective of probabilistic causality.
Recently, AIL methods (Ho & Ermon, 2016; Fu et al., 2017; Ghasemipour et al., 2020) have shown
great success on continuous control benchmarks. Each of the them provides a unique divergence
minimization scheme by its architecture. In particular, our work shares major components with
AIRL. It has been argued that the algorithm does not recover the energy of the expert policy (Liu
et al., 2020). We stress that our work introduces essential concepts to draw an energy-based repre-
sentation of the expert policy correctly. The discriminator design is based on the rich energy-based
interpretation of GANs (Zhao et al., 2016; Azadi et al., 2018; Che et al., 2020) and numerous studies
with multiple discriminators (Chongxuan et al., 2017; Gan et al., 2017; Choi et al., 2018).
The issues of finite-horizon tasks were initially raised in RL during the discussion of time limits in
MDP benchmarks (Pardo et al., 2017; Tucker et al., 2018). It turned out that the time limits, or even
the existence of terminal states, would significantly affect the value learning procedure of RL com-
pared to that generated in infinite horizon MDPs. IRL suffers from the identical problem that reward
learning of finite episodes is not really stable for tasks outside of appropriate benchmarks. Kostrikov
et al. (2018) suggested explicitly adding a self-repeating absorbing state (Sutton & Barto, 2018) after
the terminal state; consequently, AIL discriminators can evaluate the termination frequencies.
3	Background
Markov Decision Process (MDP). We define an MDP as a tuple M = (S, A, P, r, p0, γ) where S
and A denote the state and action spaces, and γ is the discount factor. The transition distribution
P, the deterministic state-action reward function r, and the initial state distribution p0 are unknown.
Let τπ and τE be sequences of finite states and actions (s0, a0, . . . , aT-1, sT) obtained by a policy
π and the expert policy πE, respectively. The term ρπ denotes the occupancy measures derived by
2
Under review as a conference paper at ICLR 2021
Table 1: The objectives for AIL algorithms in a form as the minimization of statistical divergences.
Method
Behavioral Cloning
GAIL (Ho & Ermon, 2016)
AIRL (fu et al., 2017)
FAIRL (ghasemipour et al., 2020)
CAIRL (this work)
Optimized Objective (Minimization)
E∏e[Dkl(∏e (a∣s)k∏(a∣s))] = -EnE [log π(a∣s)] + const
En [Djs (ρ∏(s, a),PE(s,a)) - H(∏(∙∣s))]
Eπ DKL ρπ (s, a)ρE (s, a) = -Eπ log ρE (s, a) + H(ρπ)
Eπ DKL ρE (s, a)ρπ(s, a) = -EπE logρπ(s, a) + H(ρE)
En [Dkl (∏(a∣s)∣∣∏E(a|s))] = -E∏[r(s,a) + H(∏(∙∣s))] + const
π, and is defined as Pn (s, a) = π(a∣s) P∞=ο Yt Pr(St = s∣∏). With a slight abuse of notation, We
refer to the occupancy measures of states as ρE(s) and ρn(s). The expectation of π for an arbitrary
function C denotes an expected return for infinite-horizon: En [c(s, a)]，E[P∞=o γtc(s, a) ∣∏].
Maximum Entropy IRL (MaxEnt IRL). Ziebart (2010) and Haarnoja et al. (2017) defined the
optimality of stochastic policy With an entropy-regularized RL objective as folloWs:
∏? = arg max Et 园⑶如)〜ρ∏ [r(st, at) + αH(at∣st)]
n∈Π
Where H denotes the causal entropy function.* 1 IfπE is the MaxEnt RL policy, the softmax Bellman
optimality equations can be defined by the folloWing recursive logic:
Q* (St, at) = r(st, at) + YEst+1 〜P(∙∣st,at) [V ? (st+1)]
V?(st) = Eat〜∏E(∙∣st) [Q*(st,at) - log∏E(at∣St)]
(1)
MaxEnt IRL algorithms (Ziebart et al., 2008; 2010) are energy-based interpretations of IRL Which
aim to find behavior abiding the MaxEnt principle. Such algorithms, hoWever, are difficult to be
computed When the given spaces are continuous or dynamics are unknoWn (Finn et al., 2016a).
Adversarial Imitation Learning. Ho & Ermon (2016) considered adversarial learning as a model-
free, sampling-based approximation to MaxEnt IRL. Instead of exhaustively solving the problem,
GAIL performs imitation learning by minimizing the divergence betWeen the state-action occupancy
measures from expert and learner through the folloWing logistic objective:
min max E∏e [log D(s, a)] + En [log(l — D(s, a))] — H(π)
n∈Π D
(2)
Where D ∈ (0, 1)|S||A| indicates a binary classifier trained to distinguish betWeen τn and τE. The
AIRL discriminator tries to disentangle a reWard function that is invariant to dynamics. It takes a
particular form: D§(s, a) = exp(fθ,ψ(s, a))∕(exp(fθ,ψ(s, a)) + ∏φ(a∣s)). Learning with the AIRL
can be considered as the reverse KL divergence betWeen occupancy measures. Ghasemipour et al.
(2020) proposed the FAIRL algorithm as an adversarial method for the forward KL divergence.
4 Unbiased Reinforcement S ignals for Energy-Based Models
Our aim in this section is to investigate unbiased probabilistic modeling of the causality of decisions
using the MaxEnt framework. In Sec. 4.1, we discuss the energy-based reward function. Sec. 4.2
and Sec. 4.3 introduce the modeling method of the particular reward function.
4.1	Energy-based Reward Representation
To accurately manage the MaxEnt framework, the EBM of expert is set to ∏e (at∣st) 8
exp{-E(St, at) = exp{Q(St, at)}, such that the likelihood of distribution is proportional to the
soft Q-function. The MaxEnt RL processes can be interpreted as minimizing the expected KL-
divergence using the information projection (Haarnoja et al., 2017; 2018):
J (∏) = Es 〜PnhDKLk (∙∣s)∣∣ expZgs," )i
1For the rest of the paper, we occasionally omit the parameter with α = 1 for simplicity of derivation.
3
Under review as a conference paper at ICLR 2021
Apparently in IRL, since the ground-truth reference will be the expert policy, the general objective
of imitation learning: π? = argmin∏ En[Dkl(∏(∙∣s)∣∣∏e(∙∣s))], is indeed the special type of the
MaxEnt RL objectives. We describe a state-conditioned, energy-based reward as a representation:
•	If Y ≈ 0, it formulates the “myopic” 1-step conditional KL divergence: Dkl[∏(∙∣s)∣∣∏e(∙∣s)].
•	If γ ≈ 1, assuming the dynamics are identical, it leads to the “far-sighted” cumulative densities:
E[P∞=0 DκL[∏(∙∣st)k∏E(∙∣st)]∣∏] = DKL [Pr(ao, sι,... ∣so = s,∏)k Pr(ao, sι,... ∣so = s,∏e)].
By the discount factor γ, we can control how much subsequent steps we want to model. Therefore,
the energy-based rewards generalize learning probabilistic inference of conditional decisions.
Note that AIL reward functions, curated in Table 1, usually do not retain these properties. For exam-
ple, the AIRL reward function, namely: f (s, a) = log(ρE(s, a)∕ρ∏ (s)), recovers the expert policy,
only in the trivial case Y = 0, as ∏e(a|s) = P "；(；)@0). For γ > 0, the projection generally is not
πE, and also it is difficult to be analyzed. We alaso highlight that the standard MaxEnt RL with AIL
rewards may not precisely recover the EBM. The more discussions are addressed in Appendix B.2.
4.2	State-Conditioned Rewards on the Principle of Maximum Entropy
We clarify a valid candidate set of reward functions when entropy-regularization is applied. One
trivial solution of such function is the log-likelihood of expert r?(s, a) = log ∏e(a|s), in the con-
dition: V?(s) = V ?(s0) = 0. Then, 1-step expectation of r? draws a conditional KL divergence:
Ea〜∏(∙∣s)[r*(s,a) + H(∏(∙∣s))] = -DKLln(∙∣s)∣∣∏e(∙∣s)] ≤ 0	⑶
By the property ofKL divergence, the above expression is less than or equal to 0 where the equality
holds if and only if π(a∣s) = ∏e(a|s), ∀a ∈ A. Since the optimal value function outputs zero, the
reward shaping (Ng et al., 1999) of r? is not required. As a result, the optimality of the action is
instantaneously determined without consideration of future rewards. It leaves the following remark.
Remark 4.1. For arbitrary discounted rates, r?(s, a) = log πE (a|s) is the optimally shaped reward
function for learning efficiency in the Shannon entropy-regularization.
From this insight, the log-likelihood of expert policy can be considered as a desirable state-
conditioned reward function for the MaxEnt RL. However, oftentimes directly modeling the log
density (such as BC methods) is not practical due to the limited number of samples. We can alter-
natively relaxes likelihood estimation by using a state potential function Ψ (Ng et al., 1999):
R = {r∣r(s, a) + γΨ(s0) 一 Ψ(s) = log∏e(a|s), Ψ : S → R, ∀s,a,s0 ∈ S ×A×S}	(4)
Akin to a deterministic case, we formalize a point that Eq. (4) does not change the learning objective.
Proposition 4.1. Let r be a function satisfying Eq. (4). Then the expected cumulative reward of π
hasfollowingproperty: E∏ [r(s,a) + H(∏(∙∣s))] = 一E∏ [Dkl(∏(∙∣s)∣∣∏e(∙∣s))] + ES〜p0 [Ψ(s)].
Note that the term ES〜p0 [Ψ(s)] is independent of n. Compared to the AIRL, the function r ∈ R
can be applied for training arbitrary policies since the overlapping state densities log ρE (s)∕ρπ (s) are
detached. The subsequent learning of r provides an projection, which is the closest estimation of
Pr(a0, . . . |s0 = s, πE) for the current state.
4.3	Handling Finite-Horizon Biases via KL Regularization Assumption
In order to mitigate the terminiation biases depicted in Fig. 1 (a), Kostrikov et al. (2018) highlighted
the concept of absorbing states and suggested adding synthetic transitions into the trajectories (see
Fig. 1 (b)), which explicitly promotes learning for termination frequencies. In contrast, we focus on
another intriguing point that the absorbing state makes all episodes virtually have the properties of
infinite horizon. Consider that the expert represents the MaxEnt behavior even when a self-looping
state is encountered. Since the action selection can change neither reward nor transition state, the
“expert” would represent complete random behavior, which is identical to the uniform distribution.
In the sense that they do not require the explicit manipulation of trajectories, Fig. 1 (c) and
(d) demonstrate two possible simpler examples to turn finite-length episodes into infinite-horizon
episodes by assuming self-looping terminal states. The two instances are alike for handling the bi-
ases as Ht = KLt + const, but the KL regularization has a slight advantage because a terminal
4
Under review as a conference paper at ICLR 2021
HT-2
HT-1
Survive
HT-2
(c)	Entropy bonuses with a looping terminal state.
(a) Entropy bonuses in a finite horizon MDP.
(b) Entropy bonuses including an absorbing state.
sT-2
KLT -2
(d)	KL penalties with a looping terminal state.
Figure 1: Visualization of entropy regularization methods and biases around terminal states. The
red vertices and edges represent absorbing states and maximum entropy action selection. Each
regularization is defined as Ht = H(∏(∙∣st)) and KLt = -DκL∣∏(∙∣st)∣∣Punif(.)], respectively.
state value is always zero instead of Pk∞=0 γkHmax. In other words, only switching the regulation
scheme from Fig. 1 (a) has eliminated the termination bias. Consequently, the KL penalties can be
seamlessly integrated into finite-Horizon MDPs without any pre-processing around terminal states.
With the same reasoning as in Sec. 4.2, the term log πe(a∖s)∕u can be understood as the optimally
shaped reward under the KL regularization. The constant u denotes the likelihood of punif (i.e.
u , punif (a), ∀a ∈ A). To this end, we set the learning objective of the shaped reward function:
fθ,ψ(s,a, s0) = rθ(s, a) + γhψ(s0) - hψ(S) = logπESIs)/u	(5)
where r and hψ denote reward and potential networks. hψ denotes a target potential network with
the identical parameter of ψ, yet the gradient computation is disconnected while training, which is
analogous to constructing target value networks in the deep RL domain (Mnih et al., 2015). For
every terminal state, we fixed the output to the optimal solution: rθ(sT , a) = hψ (sT ) = 0, ∀a ∈ A.
5 Causal Adversarial Inverse Reinforcement Learning
5.1	A Dual Discriminator Architecture
The methodology is grounded on the findings that an AIL discriminator also contains an EBM, and
the property that an EBM on joint densities can be decomposed into multiple EBMs (Zhao et al.,
2016; Che et al., 2020; Azadi et al., 2018). Suppose that the GAIL discriminator is nearly optimal:
1
D(s, a) =
ρE(s, a)
1 + exp(-d(s,a)) ≈ PE(s,α) + ρ∏(s,a)
where d(s, a) denotes the logit of D(s, a). We disentangle the logit function to the following form:
ρE (s, a)	ρE(s)	πE (a|s)
d(s, a) ≈ log —7—÷ = log —HL + log / I √	(6)
Pn (s,a)	Pn (s)	π(a∣s)
and then we have two log-ratios of state occupancy measures and policy distributions.
The model substitutes the log-ratio of state occupancy measures using a state-only discriminator
DW with a nearly optimal logit score of dψ(s) ≈ log(ρE(S)/ρ∏(s)). The role of DW is to nullify the
difference between state densities by pre-applying it. We propose an architecture of discriminator:
Dθ,ψ(s, a, s0)
_____________expfθ,ψ(s,a, S0)]
expfθ,ψ(s,a, s0)] +exp[-dψ(s) + log nφ(a∖s)]
(7)
where dψ(s) and ∏φ(a∣s) are pre-computed. Using Dψ and ∏φ as scaffolds, the shaped reward
function fθ,ψ converges to Eq. (5) and becomes specialized in evaluating conditional decisions,
where learning with the reward function correctly projects to the optimal policy.
The discriminators Dψ and Dθ,ψ are trained for maximizing the following objectives, respectively:
J (Dψ) = EnE [log Dψ(S)] +Enφ[log(1-Dψ(S))],
J (Dθ,ψ) = EnE [log Dθ,ψ (S, a, S0)] +Enφ [log(1 - Dθ,ψ (S, a, S0))] - λEnφ,nE [kχψ (S, S0)k22],
5
Under review as a conference paper at ICLR 2021
where χψ (s,s0) = γhψ(s0) - hψ (S) denotes the shaping function. To make the r to be close to the
case log πE (a|s)/u, we regularize the function by minimizing squared L2-norm with λ ∈ R+. The
regularization on the shaping function eventually makes it converge to zero, but it achieves relatively
stable results than regularizing ∣∣hψ(s)k2. The algorithm provides an IRL reward as a ∙ r (s, a);
ideally, the performance is invariant to α, if all the processes share the same temperature. In some
benchmarks, the reward function is constrained by using the softmax activation function. As the
terminal states get the highest entropy bonuses, this constraint does not exploit awareness of terminal
states. In some benchmarks it has the practical advantage of preventing overly pessimistic rewards
when IRL is not sufficiently trained. We defer additional implementation details to Appendix D.
5.2	Analyses on the Causal AIRL Algorithm
Entropy-regularized IRL. We draw a connection between entropy-regularized policy gradient al-
gorithms and our method as an adversarial training method for the policy network.
Proposition 5.1. If Eq. (5) is satisfied, the following equality holds.
VφE∏φ [Dkl(∏φ(∙∣s)∣∣∏e(∙∣s))] = -E∏φ [Qπφ(s, a)Vφlog∏φ(a∣s) + VφH(∙∣s)],	(8)
where Qπ(s, a)，r (s, a) + E[P∞=ι Yt(rθ(st, at) - log∏φ(at∣st))∣so = s,ao = a,∏].
The proposition again shows the strong relationship between entropy-regularized RL and AIL. Un-
like deterministic RL, the policy of entropy-regularized RL is proved to be converged to a unique
fixed point in a regularized condition (Geist et al., 2019; Yang et al., 2019). Thus, we deduce that
the learning scheme of CAIRL leads the policy to be converged to the fixed point of the expert.
Application to Transfer Learning. Suppose that the expert resides in another MDP: ME =
(S, A, PE, r, po, γ). Extending our formulation, We can induce that the function fθ,ψ (s, a, s0) would
converge to log( PE(SsIssa)) ∙ πEUαls)), and the expectation of the function with the KL penalty gives
Ea~∏(∙∣s),s0~P(∙∣s,a) [fθ,ψ(s, a, s0) — Dkl(∏(∙∣s)∣∣Punif(•))] = -DKL [p∏φ(a, s0∣s)∣∣Pe(a, s0∣s)],
(9)
where pπφ (a, s0|s) and pE (a, s0|s) denote the agent’s and the expert’s conditional probability dis-
tributions of action and transition state, and learning with rθ (s, a) also promotes the identical
effect. If the distributions po and PE are different, the optimal behavior has to adapt to the
gap between domains. The energy-based reward function rθ(s, a) is robust to dynamics mis-
alignment as it learns the conditional joint distribution of actions and transition states, namely
DKL Pr(a0, s1, . . . |s0 = s,π)∣ Pr(a0, s1, . . . |s0 = s,πE) .
Objective of Potential Networks. The reward shaping can be viewed as a value iteration scheme.
We relate Eq. (5) to an entropy-regularized operator called mellowmax defined as mmα (X) =
log(§ Pn=ι exp(axi))/α, and show that r and hψ satisfy the mellowmax Bellman optimality.
Proposition 5.2. If fθ,ψ (s, a, s0) = log πE (a|s)/u for all states, actions, and transition states, then
hψ(s) = log
Thus hψ corresponds to the mellowmax optimal value function of rθ with α = 1.
According to analyses of Asadi & Littman (2017), the standard softmax operator may lead the value
learning to multiple fixed-points when γ ≈ 1. As the mellowmax is a non-expansion operator, the
use of the KL penalties achieves relatively stable potential function optimization.
6	Experimental Results
Our experiments aim to understand CAIRL and verify the effectiveness of the algorithm in the sense
of our claims. We evaluate our approach on three topics, whose settings are motivated by the previ-
ous works (Haarnoja et al., 2017; 2018; Fu et al., 2017). For the RL algorithms, we implemented an
algorithm based on OpenAI-PPO2 (Schulman et al., 2017) and use the KL regularization in order
to eliminate the survival biases in MaxEnt RL as addressed in Sec. 4.3.
U ∙ / exp{rθ(s,a) + YEso∣s,α[hψ(s0)]} da
A
6
Under review as a conference paper at ICLR 2021
CAIRL
AIRL
Figure 3: Ant robot trajectories.
Figure 2: IRL trajectories of 2D Multi-goal environments.
Ant-v3, 4 expert trajectories
Walker2d-v3, 4 expert trajectories
Figure 4: Training curves of stochastic policies on imitation learning benchmarks.
Multi-Modal Behavior. The first experiment setting is a multi-goal environment. From the initial
agent position, four goals are located at the four cardinal directions. While a deterministic policy
commits to a single goal at the earliest attempt, we hypothesize that the optimal MaxEnt policy
distribution represents a multi-modal behavior, which is reaching all the four goals at the same rate.
We evaluated algorithms on two settings. In the 2D setting, the agent is a point mass. The ground-
truth reward function is defined as the difference between Gaussian mixture model values of points:
GMM(xt+1) - GMM(xt) where xt is a 2D representation of state. In the 3D setting, the agent is a
simulated robot where its state is defined by the position and its joint values. The detailed setting
and expert of each of the environments are provided in Appendix C.1.
Fig. 2 visualizes trajectories obtained by the IRL al-
gorithms. The symmetric multi-goal environment ex-
permient (left) demonstrates that CAIRL is capable of
recovering a multi-modal policy resulting to reach all
goals. For evaluating survival bias handling, we imple-
mented a similar task, called an asymmetric multi-goal
environment, where the right side goal is located sub-
stantially further than others (right). Table 2 displays
an ablation study that shows episode time steps and ra-
tios that an IRL agent reach the goal in the right-side.
The result shows that CAIRL can induce more uniform
multi-modal distribution than AIRL variants and that the
algorithm is robust to the survival bias. Especially, the
CAIRL algorithm the both activation functions achieved
Table 2: Statistics of CAIRL and AIRL in
the asymmetric multi-goal environment.
	Ep. Timestep	Right-side
Expert	42.61 ±18.51	0.25
AIRL	70.11 ±80.35	0.24
AIRL+absorb	87.22 ±108.0	0.33
AIRL+softplus	52.33 ±52.08	0.31
CAIRL+no KL	40.9 ±19.3	0.23
CAIRL+linear	41.07 ±17.47	0.24
CAIRL+softplus	42.05 ± 19.46	0.25
high performance; the difference between two cases was not significant. Fig. 3 shows a result that
CAIRL outperforms AIRL in the quality of generated trajectories. CAIRL searched all the four
goals with the robot, which prominently shows that our algorithm can reconstruct rewards from
multi-modal policies in the complex control tasks.
Imitation Learning. The second experiment is the imitation learning tasks of continuous domains
from the Gym benchmark suite (Brockman et al., 2016). We evaluated our algorithm on five chal-
lenging tasks, including Humanoid benchmark with 21 action dimensions.
7
Under review as a conference paper at ICLR 2021
Source Env. Target Env.
Figure 5: Illustrations of transfer learning and total distance traveled by transfer learning agents.
In Fig. 4, training curves are visualized with the results of five individual runs where the scores are
rescaled with respect to the expert score, and the shaded regions represent the minimum and max-
imum scores. Since expert policies are uni-modal Gaussian policies, we restricted the number of
trajectories to 4 except the Humanoid task. In the figure, CAIRL algorithms (blue lines) shows the
fast learning speed in the early phase of training and at least the second-best results for all provided
experiments. CAIRL clearly outperforms previous methods in the Hopper-v3, Walker2d-v3, and
the Humanoid-v3 tasks. These tasks are considered to be challenging due to the harsh termination
condition. We highlight that CAIRL with linear activation (purple lines) also shows competitive per-
formance in the conducted experiments. In our experiments, FAIRL showed the lowest performance.
We suspect that the FAIRL reward formulation r(s, a) = exp(d(s, a)) ∙ (-d(s, a)) sometimes give
too much penalty for a single step reward, even when the proposed clipping method is applied.
Transfer Learning. The third experiment is the transfer learning tasks. The setup is inspired by Fu
et al. (2017), but the settings are quite different. We trained each IRL network in the target (test)
environment. To simply put, our experiment aims to measure the flexibility of the reward learning
process. The formulation was designed to show that knowledge transfer requires adaptation. We
wanted to find out whether each algorithm provides helpful rewards without pretraining. Also, it is
natural to think that only a chunk of trajectories is available in a realistic problem. We implemented
three transfer learning tasks called SlopeHopper, SlopeWalker2d, and CrippledAnt. In the Slope-
Hopper and SlopeWalker tasks, the agent has the same configuration with the original 3D models
but the ground is tilted by certain degree ranges to [1, 15]. In the CrippledAnt task, the robot has
noticeably shorter forelegs as colored red in Fig. 5. We additionally restricted the joint angles of
forelegs in the range of {.01,.25,.50,.75,1.0} compared to the original model.
Given the expert trajectories from the source environments, the results of transfer learning tasks
are shown in Fig. 5. For each task, we repeated all runs 5 times and report the results which are
averaged over scores from the last 0.5 million training steps. In transfer learning setting, CAIRL
outperformed other algorithms considerably that it achieved the highest performance for every ex-
periment. The results imply that the algorithm extracted informative rewards for new environments
with different dynamics, such that our reward acquisition methods are robust with the variation be-
tween tasks. Other algorithms failed considerably; it empirically validate one of our hypothesis that
the cumulative sum of state occupancy measures hiders robust learning when domain shift happens.
The experiments have verified that with the proper consideration of temporal dependencies, the AIL
algorithms could be extended towards transfer learning problems.
7	Conclusion
In this paper, we have proposed a causal AIRL algorithm that recovers a robust reward function.
We have provided theoretical analyses, including reward shaping in entropy-regularized MDPs, and
the connection between adversarial learning and energy-based RL. We have proposed a novel dual
discriminator architecture, which learns a reward and a value function of regularized Bellman opti-
mality equations. Our model can efficiently disentangle biases originated from state occupancy and
terminal states. We have verified that the proposed IRL method has clear advantages over AIRL
for learning multi-modal behaviors and handling termination biases. The proposed method recovers
state-conditioned rewards, which has advantages over AIL algorithms in terms of the robustness
of imitation performance in challenging continuous domains. Furthermore, the proposed method
outperformed other methods in domain adaptation in the transfer learning experiments.
8
Under review as a conference paper at ICLR 2021
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1. ACM, 2004.
Kavosh Asadi and Michael L Littman. An alternative softmax operator for reinforcement learning.
In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 243-
252. JMLR. org, 2017.
Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus Odena. Discrim-
inator rejection sampling. arXiv preprint arXiv:1810.06758, 2018.
Monica Babes, Vukosi Marivate, Kaushik Subramanian, and Michael L Littman. Apprenticeship
learning about multiple intentions. In Proceedings of the 28th International Conference on Ma-
chine Learning (ICML-11), pp. 897-904, 2011.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and
Yoshua Bengio. Your gan is secretly an energy-based model and you should use discriminator
driven latent sampling. arXiv preprint arXiv:2003.06060, 2020.
Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Star-
gan: Unified generative adversarial networks for multi-domain image-to-image translation. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 8789-8797,
2018.
LI Chongxuan, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial nets. In Advances
in neural information processing systems, pp. 4088-4098, 2017.
Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. A connection between generative
adversarial networks, inverse reinforcement learning, and energy-based models. arXiv preprint
arXiv:1611.03852, 2016a.
Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control
via policy optimization. In International Conference on Machine Learning, pp. 49-58, 2016b.
Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adversarial inverse rein-
forcement learning. arXiv preprint arXiv:1710.11248, 2017.
Zhe Gan, Liqun Chen, Weiyao Wang, Yuchen Pu, Yizhe Zhang, Hao Liu, Chunyuan Li, and
Lawrence Carin. Triangle generative adversarial networks. In Advances in neural information
processing systems, pp. 5247-5256, 2017.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized Markov decision
processes. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning
Research, pp. 2160-2169, Long Beach, California, USA, 09-15 Jun 2019. PMLR.
Seyed Kamyar Seyed Ghasemipour, Richard Zemel, and Shixiang Gu. A divergence minimization
perspective on imitation learning methods. In Conference on Robot Learning, pp. 1259-1277.
PMLR, 2020.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1352-1361. JMLR. org, 2017.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1856-1865, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in NIPS,
pp. 4565-4573, 2016.
9
Under review as a conference paper at ICLR 2021
Ilya Kostrikov, Kumar Krishna Agrawal, Debidatta Dwibedi, Sergey Levine, and Jonathan Tomp-
son. Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial
imitation learning. arXiv preprint arXiv:1809.02925, 2018.
Gerhard Kramer. Directed information for channels with feedback. PhD thesis, ETH Zurich, 1998.
Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based
learning. 2006.
Sergey Levine and Vladlen Koltun. Continuous inverse optimal control with locally optimal exam-
ples. In Proceedings of the 29th International Coference on International Conference on Machine
Learning ,ICML'12,pp. 475-482, Madison, WL USA, 2012. OmniPress.ISBN 9781450312851.
Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. arXiv
preprint arXiv:2004.09395, 2020.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Gergely Neu and Csaba Szepesvari. Apprenticeship learning using inverse reinforcement learning
and gradient methods. arXiv preprint arXiv:1206.5264, 2012.
Andrew YNg and Stuart J Russell. Algorithms for inverse reinforcement learning. In Proceedings of
the Seventeenth International Conference on Machine Learning, pp. 663-670. Morgan Kaufmann
Publishers Inc., 2000.
Andrew Y Ng, Daishi Harada, and Stuart Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In ICML, volume 99, pp. 278-287, 1999.
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement
learning. arXiv preprint arXiv:1712.00378, 2017.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neu-
ral computation, 3(1):88-97, 1991.
Deepak Ramachandran and Eyal Amir. Bayesian inverse reinforcement learning. In Proceedings of
IJCAI, pp. 2586-2591, 2007.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Wenjie Shi, Shiji Song, and Cheng Wu. Soft policy gradient method for maximum entropy deep
reinforcement learning. arXiv preprint arXiv:1909.03198, 2019.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,
second edition, 2018.
Umar Syed, Michael Bowling, and Robert E Schapire. Apprenticeship learning using linear pro-
gramming. In Proceedings of the 25th international conference on Machine learning, pp. 1032-
1039. ACM, 2008.
George Tucker, Surya Bhupatiraju, Shixiang Gu, Richard E Turner, Zoubin Ghahramani, and Sergey
Levine. The mirage of action-dependent baselines in reinforcement learning. arXiv preprint
arXiv:1802.10031, 2018.
Wenhao Yang, Xiang Li, and Zhihua Zhang. A regularized approach to sparse optimal policy in
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5938-5948,
2019.
Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee, Xi Chen, Ken Goldberg, and Pieter Abbeel.
Deep imitation learning for complex manipulation tasks from virtual reality teleoperation. In
2018 IEEE International Conference on Robotics and Automation (ICRA), pp. 1-8. IEEE, 2018.
10
Under review as a conference paper at ICLR 2021
Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial network.
arXiv preprint arXiv:1609.03126, 2016.
Zhiming Zhou, Jiadong Liang, Yuxuan Song, Lantao Yu, Hongwei Wang, Weinan Zhang, Yong Yu,
and Zhihua Zhang. Lipschitz generative adversarial nets. In International Conference on Machine
Learning,pp. 7584-7593, 2019.
Brian D Ziebart. Modeling Purposeful Adaptive Behavior with the Principle of Maximum Causal
Entropy. PhD thesis, Carnegie Mellon University, 2010.
Brian D Ziebart, Andrew Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse
reinforcement learning. 3:1433-1438, 2008.
Brian D Ziebart, J Andrew Bagnell, and Anind K Dey. Modeling interaction via the principle of
maximum causal entropy. In Proceedings of the 27th International Conference on International
Conference on Machine Learning, pp. 1255-1262. Omnipress, 2010.
11
Under review as a conference paper at ICLR 2021
A Proofs
Proposition 4.1. Let r be a function satisfying Eq. (4). Then the expected cumulative reward of π
hasfollowingproperty: Eπ [r(s,α) + H(π(∙∣s))] = -E∏ [Dkl(∏(∙∣s)∣∣∏e(∙∣s))] + Es~p0 [Ψ(s)].
Proof. ρ∏ (S) denotes the state-only occuany measure of state s.
E∏ [r(s,α) + H(∏(∙∣s))] = E
Yt(r(st,at) - logπ(at∣st))
E y'Yt(log∏e(at∣st) - logπ(αt∣st) - YΨ(st+1) +Ψ(st))
∞
∞
Y t(log
Y t(log
EXYt (IOg
∏e (αt∣st)
π(at∣st)
∏E (αt∣st)
π(at∣st)
∏E (αt∣st)
π(at∣st)
)-∑ Y t+1Ψ(st+1) + ∑ γtΨ(st)
t=0
∞
t=1
) + Ψ(s0)
t=0
∞
t=0
Z∕4Pπ (s,a) [log
(Pn(S) ∕ι"α∣S)
∏E (α∣s)
π(a∣s)
]da ds + J p0(s)Ψ(s) ds
∏E (α∣s)
π(a∣s)
]da ds + Es~p0 [Ψ(s)]
E
E
-	J Pπ(S)(DKl(∏(∙∣s)∣∣∏e(∙∣s))) ds + Es~p0[Ψ(s)]
-	J Pπ(s) / π(α∣s)(DκL(∏(∙∣s)∣∣∏E(∙∣s)))dα ds + Es* [Ψ(s)]
-	E∏[Dkl(∏(∙∣s)∣∣∏e(∙∣s))] + Es~p0 [Ψ(s)]
Therefore, the reward function r(s, a) also provides the same global objective as the likelihood
log ∏e (α∣s).	口
Proposition 5.1. Ifall critic functions are optimal, thefollowing equality holds.
VφEπψ [Dkl(∏φ(∙∣s) I I ∏e(∙∣s))] = -E∏φ[Qπφ(s,a)V0log∏φ(a∣s) + V0H(∙∣s)],
where Qπ(s, a)，r§(s, a) + E[P∞=1 Yt(rj(st, at) - log ∏φ(at∣st))∣s0 = s,a0 = a,π].
Proof. By the policy gradient theorem Sutton & Barto (2018), we can derive the gradient of φ when
initial state is s.
-VφE∏φ ∣Dkl[∏φ(∙∣s)∣∣∏e(∙∣s)]卜0 = s] = Vφ(E∏φ[rθ(s,a) - logπφ(a∣s)∣s0 = s] + const.)
VφE
I- ∞
t=0
st,at) - log∏φ(at∣st)
s0
s
VφVπφ (s)
where Vπ(s)，E[P∞=° Yt(r0(st, at) - log πφ(at∣st))∣s0 = s,π]. By the product rule, we get
VφVπφ(s) = / Vφ∏φ(a∣s)^Qπ(s,a) - log∏φ(a∣s)) + πφ(a∣s)V≠^Qπ(s,a) - log∏φ(a∣s)) da
=V Vφ∏φ(a∣s)Qπ(s,a) + ∏φ(a∣s)VφH(∙∣s) + ∏φ(a∣s) / γP(s0∣s,a)VφVπφ (sz) ds da
JA	JS
12
Under review as a conference paper at ICLR 2021
By repeatedly unrolling VφVπ(St), We can derive the following form:
VφVπ(s)=	X∞ γk Pr(s →x,k,∏φ) J (Vφ∏φ(a∣x)Qπ(x, a) + ∏φ(a∣x)VφH(∙∣x)) da dx
where Pr(s → x, k, π) is the probability of transitioning from state s to state x in k steps under
policy π. Thus, using the log derivative trick we can derive the rest of equations as follows
VφE∏φ [Dkl[∏φ(∙∣s)∣∣∏e(∙∣s)]] = / po(s)VφE∏φ ∣Dkl[∏φ(∙∣s)∣∣∏e(∙∣s)]卜0 = Si ds
∞
=— J X YtPr(st = SgJ (Vφ∏φ(a∣s)Qπ(s,a) + ∏φ(a∣s)VφH(∙∣s)) da ds
=—/ ρ∏φ (S) / (Vφ∏φ(a∣s)Qπ(s,a) + ∏φ(a∣s)VφH(∙∣s)) da ds
=—/ρ∏φ(s) / ∏φ(a∣s)((Qπ(s,a)Vφ log∏φ(a∣s) + VφH(∙∣s)) da ds
=-E∏φ [Qπ(s, a)Vφ log∏φ(a∣s) + VφH(∙∣s)]
□
Proposition 5.2. If fθ,ψ (s, a, s0) = log πeSIs)/u for all states, actions, and transition states, then
hψ(s) = log
U ∙ / exp{r (s,a) + YEso∣s,a [hψ(s0)] } da
A
Thus hψ corresponds to the optimal value function of the mellowmax regularizer with α = 1.
Proof.
rθ(s, a) + γhψ(s0) — hψ(s) = log∏e(a|s) — logU ∀s0 ∈ S
=⇒ rθ(s, a) + γEso∣s,a[hψ(s0)] — hψ(s) = log∏e(a|s) — logU
=⇒ Uexp(rθ(s, a) + γEso∣s,a[hψ(s0)] — hψ(s)) = ∏e(a|s)
exp(rθ (s,a) + γEso∣s,a [hψ (s0)])
exp(hψ (s))
πE (a|s)
(10)
exp(rθ (s,a) + γEso∣s,a[hψ (s0)])
exp(hψ (s))
da
πE (a|s) da
hψ (s) = log
u ∙
rθ (s,a) + γEso 〜P (∙∣s,a)
[hψ(s0)]
da
A
□
B	Discussions with MaxEnt IRL Methods
In this section, we address similarities between frameworks of the maximum causal entropy IRL
and the causal AIRL.
B.1	Maximum Causal Entropy Framework
Following the notation of Kramer (1998), the causal entropy can be defined as
T
H(AT ||ST) ,EA,S[—logP(AT||ST)] =XH(At|S1:t,A1:t-1)
t=1
13
Under review as a conference paper at ICLR 2021
The objective of maximum causal entropy IRL can be formulated by the following optimization
Ziebart et al. (2010):
arg maxH(AT ||ST)
P(At|St)
such that： Ep(s,A)[F(S,A)] = Ep(s,A)[F(S,A)]
∀st	X P(At|S1:t, A1:t-1) = 1
at
where the overall dynamics P(ST ||AT-1) , QtT=1 P(St|St-1, At-1) are given.
According to Ziebart et al. (2010), the distribution satisfying the constrained optimization problem
can be defined as a recurrence relation:
β(st, at)
pθ (atlst) = MT
logβ(st, at) = θTFst,at +	p(st+1|st, at) logβ(st+1)
st+1
log β(st) = log	β(st,at) = softmax log β(st,at)
at	t
With the perspective of energy-based RL, we draw the following implications from the solution:
•	pθ (at | s t) can be understood as the unique fixed point of MaxEnt framework, which can be
achieved by the optimization via soft RL algorithms.
•	rθ(st, at) = θTFst,at can be understood as a reward function which is subject to the linear
constraints on feature function.
•	log β(st, at) and log β(st) can be understood as the (optimal) soft values satisfying the
Bellman equation.
B.2	Vanilla AIRL Reward Function Does Not Formulate an EBM
Adversarial Inverse Reinforcement Learning (AIRL) (Fu et al., 2017) is a well-known IRL method
that applies an adversarial architecture to solve the IRL problem. Formally, AIRL constructs the
discriminator as
D(	) =	eχpf (S,a)}
(,) = exp{f(s,a)} + π(a∣s)
(11)
This is highly motivated by the former GAN-GCL work (Finn et al., 2016b), which proposes that
one can apply GAN to train the discriminator as
D(T )=	1 exp(C(T))
ZZ eχp(c(τ)) + π(τ)
(12)
where c(T) denotes the cost function for trajectories. From the AIL formulation, AIRL uses a
surrogate reward as
r(s, a) = log D(s, a) — log(1 — D(s, a)) = f(s, a) — logπ(a∣s),	(13)
where f (s, a) = log(ρE(s,a)∕ρ∏(S)) and the overall reward can be seen as an entropy-regularized
reward function. If γ = 0, it is evident that AIRL is identical to the standard adversarial learning
without temporal sequences. As a result, it makes sense to directly optimize the policy by taking the
energy model as the target policy instead of the reward function, which leads to the optimal solution
as:
?( 1 ) =	exp(Qπ(s,a))	=	exp(f(s,a))	=	P∏e (s,a)
P P P	∑αo exP(Qπ(s,a0))	∑0o exP(f(s,a0))	∑0o P∏e (s,a0)
γ = 0.
By the fundamental property of occupancy measure (Theorem 2 of Syed et al. 2008). However,
for the general case, the projection of Q-value function cannot be close to πE like the standard
probabilitic inference using energy-based models.
14
Under review as a conference paper at ICLR 2021
B.3	Causal Adversarial Inverse Reinforcement Learning Formulation
As a direct interpretation of the maximum causal entropy framework, the goal of CAIRL can be
seen as training a parameterized distribution over the expert trajectories as the following maximum-
likelihood objective:
max J (θ) = max EnE [log pθ (a|s)],	(12)
θθ
where the conditional distributionpθ(at∣st) is parameterized as pθ(at∣st) α exp (Qθ(st, at)). We
can compute the gradient with respect to θ as follows:
∂∂
∂θ J ⑻=E∏E [∂θlog Pp(a|s)]
EnEh ∂θQθ (s, a) - ∂θ log Zp (S)i
EnEh ∂θQp(S，a)i - EnEhEa0 〜Pθ (Ts) [ ∂θQθ (S, aO)]],
(13)
where Zp(S) is the partition function normalizes the distribution pp. Since sampling with πE and
pp is difficult, we can think of substituting the formulation with the following adversarial learning
form:
E∏E [∂θrθ (S, a)] - En [∂θrθ(S，a)] .	(14)
Nevertheless the formulation in Eq. (14) is similar with a standard adversarial framework with i.i.d.
data, the approximation is nota safe choice because of unbounded divergences among π,pp, andπE.
Therefore, appropriate adversarial learning in sequential decision problems is essentially different
to GANs for joint distribution matching.
In CAIRL, we replace the reward learning objective with training a logistic discriminator:
D (	)=	eχpfθ(s,a)]
θ( , ) = exp[fθ(s,a)] + κ(s,a),
where κ(s, a) = PPn'苣au. The objective of the discriminator is to maximize the generalized Jensen-
Shannon divergence between of the generated samples:
J(Dp) =EnE[logDp(S,a)]+En[log(1- Dp(S, a))]
For the discriminators, the objective can be written as follows:
J(Dp) =EnE[logDp(S,a)]+En[log(1- Dp(S, a))]
=F	h∣	exp[fθ(s,a)]	i F h∣ __________κ(s,a)______i
="Elogexpfp (s,a)] + κ(s,a)[+ n[ogexpfθ (s,a)] + κ(s,a) _l
=EnEfθ(s, a)] + En [κ(s,a)] — 2 ∙ En [log {exp[fp(s,a)] + κ(s, a)}],
where the operator En denote the expectation using an occupancy measure Pn (s, a)
PE(s,a)+Pn(s,a). Taking the derivative with respect to θ,
d7⑺、—党	∣^dr 寸 9 κ Γ	exPfp(S,a)]	d 一 寸
∂θ J (Dp) = EnE [∂θfθ (s, a)] - 2 ∙ En LXpfp (S,a)] + κ(s,a) ∂θfθ(s, 初
=EnEhɪ/θ(s,a)i - En h - PEZp,：「ɪ/p(s,a)i
Ldθ 」	l2PE(S)Pp(a|s) + 1 Pn(s,a) dθ	」
=EnEh ∂θfp (S, a)i - En E h X Pp (a0|S) ∙ δ(S, a0) ∙ ∂θfp (S, a0)i ,
a0∈A
where
Pp(a∣S) = U ∙ exρ(fp(S,a)) and δ(S,a) =	PE]:,2 + FS')、.
PE (S)Pp (a∣S) + Pn (S,a)
The optimal θ can be easily found by considering 另 J(Dp) = 0, in condition of Pp (a∣S) = ∏e (a∣S)
with δ(S, a) = 1 for all states and actions. Moreover, for the right hand side of the expression, we
can draw the following properties of δ :
15
Under review as a conference paper at ICLR 2021
•	If ρπ (s, a)	ρE (s, a), meaning the supports are disjoint, δ(s, a) ≈ 1.
•	If ρ∏(s, a) ≈ pE(s, a), where pθ(a|s) is normally closer to ∏e than π, δ(s, a) ≈ 1.
Therefore, the expression matches the Eq. (13). CAIRL approximates the maximum causal entropy
framework without exhaustively computing the recursive equation, and also recovers a plausible
reward function on both supports of ρπ and ρE .
C Experimental Details
C.1 Mutl-Goal Environments
•	2D Point Environment: Let the 2D coordinate denote the position of a point mass on the
environment. The agent is generated according to the normal distribution N(0, (0.1)2I).
The four goals are located at (6, 0), (-6, 0), (0, 6), and (0, -6), where the agent can move
maximum 1 unit per timestep for each coordinate. The ground-truth reward is given by
the difference between successive values of a Gaussian mixture depicted as Fig. 6. The
assymetric multi-goal environment has similar settings, except the scale is five times bigger
goal of the east side is further located at (60, 0).
•	3D Ant Environment: Let the 2D coordinate representation denotes the orthogonal projec-
tion of the position of Ant robot torso. The simulated robot is spawned near the origin. The
four goals are located at (30, 0), (-30, 0), (0, 30), and (0, -30), where the agent has to
control four legs to reach one of the goals. It requires approximately 150 timesteps for an
expert to reach one of the goals from the initial position. A vector of the current position
and the robot’s joint values represent the state. Since it is hard to train a single expert model
to represent the desired multi-modal behavior precisely, we evenly merged 2,000 trajectory
samples from 8 uni-modal policies specialized in moving to one of the fixed positions.
Figure 6: Illustrations of multi-goal environments. From the initial agent position, four goals are
symmetrically located at the four cardinal directions. The ground-truth energy function of the 2D
environment and the expert trajectory samples for each environment are displayed.
C.2 Transfer Learning Environments
The experiment setting was designed to show that imitation learning for realistic tasks regardless of
domain shifts. We trained each IRL network in the target (test) environment. To simply put, our
experiment aims to measure the flexibility of reward learning process. We used 1,000 trajectories
from the source task as transfer learning data.
•	SlopeHopper & SlopeWalker2d: We used the same 3D model from the Hopper-v3 and
Walker2d-v3 Mujoco benchmarks. For each task, the ground is tilted with a certain de-
gree in [1, 15]. For computing a state vector, the models’ height is adjusted to the vertical
distance from the slope.
•	CrippledAnt: Compared to the original Ant model, we shortened the length of two
forelegs to half. We additionally restricted the joint angles of forward legs in the range
of {.01,.25,.50,.75,1.0}. While the objective of the agent is for running to the right side,
the reward can be calculated by the velocity of robot moving along the x-axis.
16
Under review as a conference paper at ICLR 2021
D Implementation Details
D.1 Algorithms
Algorithm 1 summarizes the overall IRL procedures. The term “REG” in Line 7 of the algorithm
refers to the shaping regularization minimizing squared L2-norm kγhψ(s0) - h(s)k2 With the reg-
ularization parameter λ ∈ R+ in order to make the overall algorithm well-defined. For stochastic
action distributions, the temperature parameter α has to be multiplied for exact matching betWeen
conditional distributions. For ensuring Lipschitz continuity, every critic netWork is regularized by a
gradient penalty (Zhou et al., 2019).
Algorithm 1 Causal adversarial inverse reinforcement learning.
1:	Input: Expert trajectory dataset {τ*) }N=1, temperature α.
2:	Initialize policy ∏φ and critic functions DQ and Dθ,ψ.
3:	for step i in {1, . . . , N} do
4:	Collect τπ by executing πφ .
5:	{(st, at, ∙, St)}T=I = τ∏, {(St, αt, ∙, st)}T=ι = TE* i * * * V
6:	Update Dψ using T PT=1 [log(1 - Dr(St)) + logDφ(st)] with GP
7:	Update Dθ,ψ using T PT=1[log(1 -Dθ,ψ(st, at, st))+logDθ,ψ(«t,«t,st)] with REG+GP.
8:	Update ∏φ with a ∙ r (s, a) using a maximum entropy policy optimization method.
9:	return πφ, rθ
For a policy optimization in the experiments we used PPO implementation train policy by a clipped
surrogate objective function as follows:
LnLlIP(O) = Enold[min( πφ(als)Aaold(S,a),clipf πφ*),1- ε, 1+ε)Aα°ld(S,a" (15)
L	∖∏oid(a∣s)	∖∏oid(a∣s)	a	力
where ε is the clipping range of PPO algorithm, ensuring that the updated policy does not diverge
too far from the previous distribution. One thing to be careful in the implementation is that the KL
penalty terms are dependent on the policy distribution. Therefore for a PPO algorithm with the KL
regularization, the term Anαold (S, a) can be computed as follows:
Vnold(s) = e[y^γt(rθ(st,at) - αlognφ(atlst)∕u) ∣s0 = s,∏oid],	(16)
∞
Qnold(s,a) = r (s,a) + E[£ γt(rθ(st,at) - αlognφ(atlst)∕u)∣S0 = S, a0 = a,πold],	(17)
t=1
Anold (S, a) = Qnold (S, a) - V nold (S),	(Advantage estimate)	(18)
Anαold (S, a) = Anold (S, a) - α.	(19)
We refer the work of Shi et al. (2019) for the detailed derivation of entropy-regularized policy gra-
dient algorithms.
D.2 Network Architectures and Hyperparameters
For all policy and discriminator networks, we use networks with 2-layer MLP with 256-dim layers,
and the activation function is ReLU. For input layer, we normalize inputs (state and action vectors)
by calculating exponential moving average and variance.
Details for Policy Networks For multi-goal environments, the policy function is represented by a
Gaussian mixture with four modes. In contrast, a single diagonal Gaussian function is used for other
unimodal tasks. For enforcing action bounds, we apply an invertible squashing function (tanh) to the
raw action samples and compute the likelihoods of the bounded actions such as tanh(Normal (μ, σ))
(Haarnoja et al., 2018). In our implementation, we separated the distribution network into mean
network and standard deviation network. The gradient penalty regularization is applied to the value
network.
17
Under review as a conference paper at ICLR 2021
Details for CAIRL Networks For CAIRL networks, the reward network consists softplus activation
at the final output layer while the potential network has linear activation. Gradient penalty is also
used in the both networks. We do not explicitly construct the target potential networks, instead
we use the same potential network, but the target potential network does not involved any gradient
computation. Also, to discard the learning of terminal state values, hψ is outputs as follows:
ψ( (S) = {no-grad(hψ (S))
ifs is a terminal state not caused by time limits,
otherwise
Table 3 shows the hyperparameters of conducted experiments.
Table 3: Hyperparameters.
(a) Shared parameters		(b) Environment specific parameters		
Parameter	Value	Environment	# of threads	Temp. (α)
Optimizer	Adam(0, 0.99)	Multi-goal (Point)	1	1
Learning rate (policy)	1∙10-4	Multi-goal (Ant)	16	10-2
Learning rate (discriminator)	4∙10-4	Humanoid-v3	8	10-3
Discount factor (γ)	0.99	Walker2d-v3	3	10-3
Clipping Range (ε)	0.2	Others	1	10-2
Gradient penalty (η)	10-4		—	
Shaping regularization (λ)	10-4			
Batch size per rollout	2048			
Policy rollouts	3			
Discriminator rollouts	3			
Gradient steps per iteration	15			
E Additional Experimental Results
E.1 Additional Benchmark Results
Table 4: The evaluation on imitation learning benchmark control tasks presented over 5 different
random seeds with 4 and 100 expert trajectory data.
Method	Hopper-v3	HalfCheetah-v3	Environment		Humanoid-v3
			Walker2d-v3	Ant-v3	
Expert	3688.9±358	3580.3±135	5297.0±696 一	4030.8±839	9049.0±3114
GAIL	3227.6±403	3431.9±251	2369.8±507	4090.8±252	8816.5±504
AIRL	3229.8±341	3127.6±296	3326.7±1231	3956.5±302	538.8±580
FAIRL	374.1±300	1147.4±330	281.4±116	1501.3±371	351.9±30
CAIRL	3565.4±233	3163.5±297	5089.6±339	3990.9±254	9644.6±385
GAIL	3513.1±273	3493.9±279	4392.4±39i	4093.2±258	9199.4±443
AIRL	3349.0±318	3482.6±202	5165.7±205	4027.0±282	8532.5±720
FAIRL	236.2±10	744.2±249	281.4±116	2106.0±372	375.2±22
CAIRL	3571.6±309	3246.4±244	5268.1±194	3979.6±296	9469.5±374
E.2 Visualization of Trajectories Generated by Imitation Learning Agents
Fig. 7 shows the trajectories from the trained models in the 3D environment with different seeds.
Firstly, the result suggest that CAIRL recovers multi-modal policies which is equivalent to the prop-
erty MaxEnt IRL. At the same time, CAIRL is fundamentally advanced algorithm compared to
MaxEnt IRL algorithms because it can be applied various large domain without knowing dynamics
by by enlarging number of parameters in neural networks.
18
Under review as a conference paper at ICLR 2021
CAIRL #0	CAIRL #2	CAIRL #2
Figure 7: Multi-goal Ant trajectories with different random seeds
E.3 Ablation Study on Hyperparameters
We provide experiments on ablation study on three controllable hyperparameters in Fig. 8.
Figure 8: Experiments on hyperparameters of CAIRL. (a) If the shaping regularization λ is ex-
cessively high, learning of the potential-based shaping is disabled, and the reward function shows
unstable behavior. (b) The gradient penalty is applied for ensuring convergence of discriminators.
The results suggest that sufficiently low gradient penalty is preferred for achieving desired perfor-
mance. (c) Compared to the other hyperparameters, CAIRL is robust with the temperature α.
E.4 Visualization of CrippledAnt Locomotion
We provide the visualization of generated samples from a trained agent trained by CAIRL algorithm
in Fig. 9. Even though dynamics of the environment is considerably misaligned, the algorithm still
can teach agent appropriate algorithm to teach an agent to go desired direction. We believe that this
work also provided effective transfer learning algorithm for sequential decision problems.
Figure 9: Visualization of trained agent on CrippledAnt environment. The model maximizes the
movement speed toward the right side, without showing biased locomotion such as moving to the
forward or to the backward. Also, the orientation of the torso is preserved throughout the episode,
implying that the algorithm is capable of imitating experts even under variation in the dynamics.
19
Under review as a conference paper at ICLR 2021
E.5 Visualization of AIL Reward functions
We provide the visualization of CAIRL and AIRL rewards in the 2D multi-goal environment. To
effectively represent the two functions that takes the argument (s, a) ∈ S × A into the space S, we
calculated a two-dimensional vector representation of reward for each state by averaging all possible
action in A,i.e., Vi =告 Pa∈A [r(s, a)∙ai],i ∈ {0,1}. As a results, we plotted contour maps by the
relative value of rewards and corresponding vector fields of reward function for a state grid, which
is shown in Figs. 10 and 11. Apparently, it can be observed that the CAIRL reward function is much
more analogous to the MaxEnt likelihood depicted by Haarnoja et al. (2017). More importantly,
CAIRL is prominently showing that our reward modeling provides more informative reward that
approximately advise the best direction to reach one of the goals for each state.
Figure 10: Illustration of the CAIRL reward function in the 2D multi-goal environment. Left:
recovered reward function which is averaged over action. The goals are located in the four cardinal
direction (6,0), (-6,0), (0,-6), (0,6). Right: visualization of each vector field of reward function and
the corresponding local contour map.
Figure 11: Illustration of the AIRL reward function.
20