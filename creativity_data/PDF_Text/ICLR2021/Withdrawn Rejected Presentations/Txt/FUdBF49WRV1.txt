Under review as a conference paper at ICLR 2021
Directional Graph Networks
Anonymous authors
Paper under double-blind review
Ab stract
In order to overcome the expressive limitations of graph neural networks (GNNs),
we propose the first method that exploits vector flows over graphs to develop glob-
ally consistent directional and asymmetric aggregation functions. We show that
our directional graph networks (DGNs) generalize convolutional neural networks
(CNNs) when applied on a grid. Whereas recent theoretical works focus on un-
derstanding local neighbourhoods, local structures and local isomorphism with no
global information flow, our novel theoretical framework allows directional con-
volutional kernels in any graph. First, by defining a vector field in the graph, we
develop a method of applying directional derivatives and smoothing by projecting
node-specific messages into the field. Then we propose the use of the Lapla-
cian eigenvectors as such vector field, and we show that the method generalizes
CNNs on an n-dimensional grid, and is provably more discriminative than stan-
dard GNNs regarding the Weisfeiler-Lehman 1-WL test. Finally, we bring the
power of CNN data augmentation to graphs by providing a means of doing reflec-
tion, rotation and distortion on the underlying directional field. We evaluate our
method on different standard benchmarks and see a relative error reduction of 8%
on the CIFAR10 graph dataset and 11% to 32% on the molecular ZINC dataset.
An important outcome of this work is that it enables to translate any physical or
biological problems with intrinsic directional axes into a graph network formalism
with an embedded directional field.
1	Introduction
One of the most important distinctions between convolutional neural networks (CNNs) and graph
neural networks (GNNs) is that CNNs allow for any convolutional kernel, while most GNN methods
are limited to symmetric kernels (also called isotropic kernels in the literature) (Kipf & Welling,
2016; Xu et al., 2018a; Gilmer et al., 2017). There are some implementation of asymmetric kernels
using gated mechanisms (Bresson & Laurent, 2017; VeliCkovic et al., 2017), motif attention (Peng
et al., 2019), edge features (Gilmer et al., 2017) or by using the 3D structure of molecules for
message passing (Klicpera et al., 2019).
However, to the best of our knowledge, there are currently no methods that allow asymmetric graph
kernels that are dependent on the full graph structure or directional flows. They either depend on
local structures or local features. This is in opposition to images which exhibit canonical directions:
the horizontal and vertical axes. The absence of an analogous concept in graphs makes it difficult
to define directional message passing and to produce an analogue of the directional frequency filters
(or Gabor filters) widely present in image processing (Olah et al., 2020).
We propose a novel idea for GNNs: use vector fields in the graph to define directions for the prop-
agation of information, with an overview of the paper presented in 1. Hence, the aggregation or
message passing will be projected onto these directions so that the contribution of each neighbour-
ing node nv will be weighted by its alignment with the vector fields at the receiving node nu . This
enables our method to propagate information via directional derivatives or smoothing of the features.
We also explore using the gradients of the low-frequency eigenvectors of the Laplacian of the graph
φk, since they exhibit interesting properties (Bronstein et al., 2017; Chung et al., 1997). In particular,
they can be used to define optimal partitions of the nodes in a graph, to give a natural ordering (Levy,
2006), and to find the dominant directions of the graph diffusion process (Chung & Yau, 2000).
Further, we show that they generalize the horizontal and vertical directional flows in a grid (see
1
Under review as a conference paper at ICLR 2021
Input graph
Compute first k non-
trivial eigenvectors
Pre-computed steps O(kE')
Compute the gradient
Create the aggregation
matrices B
Graph neural network steps O(kE + kN)
Input graph
Aggregation of
neighbouring features
MLP
The a-directional
adjacency matrix A
is given as an input.
We then compute
the Laplacian
matrix L
The eigenvectors OOfZ are
computed and sorted such
that φι has the lowest non-
zero eigenvalue and m has
the k-hh IOVteSt.
Both Andd L ere Of
size NXMhthere
N is thenumbetef
nodes. The
matrices are often
sparse with Eeeing
the number of
edges.
This step is the most
expensive computationally.
There are methods to
compute k-firs teigenvectors
with a complexity of O(kE)
The gradient of ⅛Si s a
function ofthe edges (a
matrix) such that
Vφlj =机一机 if the
nodes i,j etc COneeCteCo Or
Vφlj = 0 Ohherwise.
If the graph has a known
direction, it can be encoded
as field F(n nnnd-SymmeteiC
matrix) instead of using Vφ.
Each row (i,:) Ofthe fiel d FiS
normalized by it’s L1norm to create the
^ngregation matrices.
P = K”
'" = I∣F,JI - + e
•	Bav is the directional smoothing matrix.
BaV = IPl
•	Bd* is the directional derivative matrix.
(Bifa)"
&： - dag(1R,j)
A graph with the
node features is
given. X⑼ is the
feature matrix of the
graph at the0-th
GNN layer.
X(0)has Mows s(the
number of nodes)
and n0 columns (the
number of input
features).
The aggregation matrices
B：'v* are Used to aggregate
the features X<0). For BiX we
take the absolute value due
to the sign ambiguity of φ.
B st Simigt oo^veeighted
adjacency matrix (with
possible negative weights),
the aggregation is simply the
matrix product with the
feature vector.
This is the only step with
learned parameters.
Based on the GCN method,
each aggregation is followed
by a multi I即er PeeCePrtOn
(MLP) on all the features.
In our case, the MLP is
applied on the columns of
Y,0), thus we have a
complexity of O(JiN^).
• X〈0)hasno columns
• X<1)hasnι columns
I	X(1)= MLP (r(0〉)
∖Bixχ^∖
Field matrix colormap
max
Other non-directional
aggregators are used, such as
the mean SngregatiOn
D-1AXC0).
Node colormap
-max
• κ0) has (2k + 1)n0
columns
The aggregation
matrices BiV嚷 are
taken from the pre -
computed steps.
B3
f' = q©
"“
Next GNN
layer
Let the number of features
at the t-th Iyet in X«〉be
nt. Then:
Figure 1: Overview of the steps required to aggregate messages in the direction of the eigenvectors.
「：.-
4	:	:
The resulting matrix Y<0)is
the column-concatenation of
all the aggregations. The
complexity is O(kE), wthhE
being the number of edges,
or O(Ee if th agggr^^ions
parallelized.
DTAA町
IBE
Xm L
t → t +1
XG) → χ/1)
χ(0) → Xa)
r〈0)→ r〈1)
figure 2), allowing them to guide the aggregation and mimic the asymmetric and directional kernels
present in computer vision. In fact, we demonstrate mathematically that our work generalizes CNNs
by reproducing all convolutional kernels of radius R in an n-dimensional grid, while also bringing
the powerful data augmentation capabilities of reflection, rotation or distortion of the directions.
We further show that our directional graph network (DGN) model theoretically and empirically
allows for efficient message passing across distant communities, which reduces the well-known
problem of over-smoothing, and aligns well with the need of independent aggregation rules (Corso
et al., 2020). Alternative methods reduce the impact of over-smoothing by using skip connections
(Luan et al., 2019), global pooling (Alon & Yahav, 2020), or randomly dropping edges during
training time (Rong et al., 2020), but without solving the underlying problem. In fact, we also prove
that DGN is more discriminative than standard GNNs in regards to the Weisfeiler-Lehman 1-WL
test, showing that the reduction of over-smoothing is accompanied by an increase of expressiveness.
Our method distinguishes itself from other spectral GNNs since the literature usually uses the low
frequencies to estimate local Fourier transforms in the graph (Levie et al., 2018; Xu et al., 2019).
Instead, we do not try to approximate the Fourier transform, but only to define a directional flow at
each node and guide the aggregation.
2	Theoretical development
2.1	Intuitive overview
One of the biggest limitations of current GNN methods compared to CNNs is the inability to do
message passing in a specific direction such as the horizontal one in a grid graph. In fact, it is
difficult to define directions or coordinates based solely on the shape of the graph.
The lack of directions strongly limits the discriminative abilities of GNNs to understand local struc-
tures and simple feature transformations. Most GNNs are invariant to the permutation of the neigh-
bours’ features, so the nodes’ received signal is not influenced by swapping the features of 2 neigh-
bours. Therefore, several layers in a deep network will be employed to understand these simple
changes instead of being used for higher level features, thus over-squashing the message sent be-
tween 2 distant nodes (Alon & Yahav, 2020).
In this work, one of the main contributions is the realisation that low-frequency eigenvectors of the
Laplacian can overcome this limitation by providing a variety of intuitive directional flows. As a first
example, taking a grid-shaped graph of size N X M with 与 < M < N, we find that the eigenvector
2
Under review as a conference paper at ICLR 2021
associated to the smallest non-zero eigenvalue increases in the direction of the width N and the
second one increases in the direction of the height M . This property generalizes to n-dimensional
grids and motivated the use of gradients of eigenvectors as preferred directions for general graphs.
We validated this intuition by looking at the flow of the gradient of the eigenvectors for a vari-
ety of graphs, as shown in figure 2. For example, in the Minnesota map, the first 3 non-constant
eigenvectors produce logical directions, namely South/North, suburb/city, and West/East.
Another important contribution also noted in figure 2 is the ability to define any kind of direction
based on prior knowledge of the problem. Hence, instead of relying on eigenvectors to find direc-
tions in a map, we can simply use the cardinal directions or the rush-hour traffic flow.
Direction in/out of every city
(This mimics traffic in the
morning/afternoon, and the
expansion of suburbs population)
Field propagation due to the
presence of a charge or
defect in the crystal
(In an hexagonal lattice, it is
possible to add edges between
diagonal neighbours)
Figure 2: Possible directional flows in different types of graphs. The node coloring is a potential
map and the edges represent the gradient of the potential with the arrows in the direction of the
flow. The first 3 columns present the arcosine of the normalized eigenvectors (acos φ) as node
coloring, and their gradients represented as edge intensity. The last column presents examples of
inductive bias introduced in the choice of direction. (a) The eigenvectors 1 and 2 are the horizontal
and vertical flows of the grid. (b) The eigenvectors 1 and 2 are the flow in the longest and second-
longest directions. (c) The eigenvectors 1, 2 and 3 flow respectively in the South-North, suburbs to
the city center and West-East directions. We ignore φ0 since it is constant and has no direction.
Non-diagonal
grid graph
Direction out of the global
centroid
(We can also use local centroids,
J∙ local polarity, or 3D structure to
define the fields)
2.2	Vector fields in a graph
Based on a recent review from Bronstein et al. (2017), this section presents the ideas of differential
geometry applied to graphs, with the goal of finding proper definitions of scalar products, gradients
and directional derivatives.
Let G = (V, E) be a graph with V the set of vertices and E ⊂ V × V the set of edges. The graph
is undirected meaning that (i,j) ∈ E iff (j, i) ∈ E. Define the vector spaces L2(V ) and L2(E) as
the set of maps V → R and E → R with x, y ∈ L2(V ) and F, H ∈ L2(E) and scalar products
hx,yiL2(V) :=	xiyi	, hF,HiL2(E) :=	F(i,j)H(i,j)	(1)
i∈V	(i,j)∈E
Think of E as the “tangent space” to V and of L2(E) as the set of “vector fields” on the space V
with each row Fi,: representing a vector at the i-th node. Define the pointwise scalar product as the
map L2(E) × L2(E) → L2(V ) taking 2 vector fields and returning their inner product at each point
of V , at the node i is defined by equation 2.
hF,Hii := X Fi,jHi,j	(2)
j :(i,j )∈E
In equation 3, We define the gradient V as a mapping L2 (V) → L2 (E) and the divergence div as a
mapping L2 (E) → L2(V), thus leading to an analogue of the directional derivative in equation 4.
(Vx)(i,j) := x(j) - x(i)	,	(div F)i := X	F(i,j)	(3)
j :(i,j )∈E
3
Under review as a conference paper at ICLR 2021
Definition 1. The directional derivative of the function x on the graph G in the direction of the
vector field F where each vector is of unit-norm is
DFx(i) ：= hVx, Fii= E (x(j) - x(i))Fi,j	(4)
j1i,j)∈E
|F | will denote the absolute value of F and ||Fi,: ||Lp the Lp-norm of the i-th row of F . We also
define the forward/backward directions as the positive/negative parts of the field F±.
2.3 Directional smoothing and derivatives
Next, we show how the vector field F is used to guide the graph aggregation by projecting the
incoming messages. Specifically, we define the weighted aggregation matrices Bav and Bdx that
allow to compute the directional smoothing and directional derivative of the node features.
The directional average matrix Bav is the weighted aggregation matrix such that all weights are
positives and all rows have an L1-norm equal to 1, as shown in equation 5 and theorem 2.1, with a
proof in the appendix C.1.
Bav(F )i,： = ||"FL1| + e
(5)
The variable is an arbitrarily small positive number used to avoid floating-point errors. The L1-
norm denominator is a local row-wise normalization. The aggregator works by assigning a large
weight to the elements in the forward or backward direction of the field, while assigning a small
weight to the other elements, with a total weight of 1.
Theorem 2.1	(Directional smoothing). The operation y = Bavx is the directional average ofx, in
the sense that yu is the mean ofxv, weighted by the direction and amplitude ofF.
The directional derivative matrix Bdx is defined in (6) and theorem 2.2, with the proof in ap-
pendix C.2. Again, the denominator is a local row-wise normalization but can be replaced by a
global normalization. diag(a) is a square, diagonal matrix with diagonal entries given by a. The
aggregator works by subtracting the projected forward message by the backward message (similar
to a center derivative), with an additional diagonal term to balance both directions.
Bdx(F L= FiL diag(X FJ,：	Fi,二 (w‰
(6)
Theorem 2.2	(Directional derivative). Suppose F have rows of unit L1 norm. The operation y =
Bdx (F)x is the centered directional derivative ofx in the direction ofF, in the sense of equation
4, i.e.
y = DFX = (F - diag(XF,j))x
j
These aggregators are directional, interpretable and complementary, making them ideal choices for
GNNs. We discuss the choice of aggregators in more details in appendix A, while also provid-
ing alternative aggregation matrices such as the center-balanced smoothing, the forward-copy, the
phantom zero-padding, and the hardening of the aggregators using softmax/argmax on the field. We
further provide a visual interpretation of the Bav and Bdx aggregators in figure 3. Interestingly, we
also note in appendix A.1 that Bav and Bdx yield respectively the mean and Laplacian aggregations
when F is a vector field such that all entries are constant Fij = ±C.
Graph features focused on the neighbourhood of nv
rV.U2
V,U3
Directional smoothing aggregation BaV(F)X
I -v,“ι∣"t⅛ + W,%¾J⅞⅛ + ∣Fv,u3∣χn3
v: Node receiving the message
u 1,2,3: Neighbouring node
xu: Feature at node u
F■,£: DireCtional vector field between the node Uadd u
Absolute weighted sum
Sum of the absolute weights
Directional derivative aggregation BdX (F)%
— &) + /t>,U2 (MV _ 32)+ 6,¾⅛(*t> _
IFiMtII + ∣F%ι⅛l + IFV,ι⅛∣
Weighted forward Weightedbackward Weightedbackward
derivative with u 1 + derivative with u2 + derivative with u3
Sum of the absolute weights
Figure	3: Illustration of how the directional aggregation works at a node nv , with the arrows repre-
senting the direction and intensity of the field F.
4
Under review as a conference paper at ICLR 2021
2.4	Gradient of the eigenvectors as interpretable vector fields
In this section we give theoretical support for the choice of gradients of the eigenfunctions of the
Laplacian as sensible vectors along which to do directional message passing since they are inter-
pretable and allow to reduce the over-smoothing.
As usual the combinatorial, degree-normalized and symmetric normalized Laplacian are defined as
L = D — A ,	Lnorm = DTL ,	Lsym = D-2 LD 1	⑺
The problems of over-smoothing and over-squashing are critical issues in GNNs (Alon & Yahav,
2020; Hamilton, 2020). In most GNN models, node representations become over-smoothed after
several rounds of message passing (i.e., convolutions), as the representations tend to reach a mean-
field equilibrium equivalent to the stationary distribution of a random walk (Hamilton, 2020). Over-
smoothing is also related to the problem of over-squashing, which reflects the inability for GNNs to
propagate informative signals between distant nodes (Alon & Yahav, 2020) and is a major bottleneck
to training deep GNN models (Xu et al., 2019). Both problems are related to the fact that the
influence of one node’s input on the final representation of another node in a GNN is determined by
the likelihood of the two nodes co-occurring on a truncated random walk (Xu et al., 2018b).
We show in theorem 2.3 (proved in appendix C.3) that by passing information in the direction of φ1,
the eigenvector associated to the lowest non-trivial frequency of Lnorm, DGNs can efficiently share
information between the farthest nodes of the graph, when using the K-walk distance to measure
the difficulty of passing information. Thus, DGNs provide a natural way to address both the over-
smoothing and over-squashing problems: they can efficiently propagate messages between distant
nodes and in a direction that counteracts over-smoothing.
Definition 2 (K-walk distance). The K-walk distance dK (vi , vj ) on a graph is the average number
of times vi is hit in a K step random walk starting from vj .
Theorem 2.3 (K-Gradient of the low-frequency eigenvectors). Let λi and φi be the eigen-
values and eigenvectors of the normalized Laplacian of a connected graph Lnorm and let
a, b = arg max1≤i,j≤n{dK (vi, vj)} be the nodes that have highest K-walk distance. Let m =
arg min1≤i≤n(φ1)i and M = arg max1≤i≤n(φ1)i, then dK(vm, vM) - dK(va, vb) has order
O(1 - λ2).
As another point of view into the problem of oversmoothing, consider the hitting time Q(x, y)
defined as the expected number of steps in a random walk starting from node x ending in node
y with the probability transition P(x, y)=六.In appendix C.4 We give an informal argument
supporting the following conjecture.
Definition 3 (Gradient step). Suppose the two neighboring nodes x andz are such that φ(z) - φ(x)
is maximal among the neighbors of x, then we will say z is obtained from x by taking a step in the
direction ofthe gradient Vφ.
Conjecture 2.4 (Gradient steps reduce expected hitting time). Suppose that x, y are uniformly dis-
tributed random nodes such that φi (x) < φi (y). Let z be the node obtained from x by taking one
step in the direction ofVφi, then the expected hitting time is decreased proportionally to λi-1 and
Ex,y[Q(z, y)] ≤ Ex,y[Q(x, y)]
The next two corollaries follow from theorem 2.3 (and also conjecture 2.4 ifit is true).
Corollary 2.5 (Reduces over-squashing). Following the direction of Vφ1 is an efficient way of
passing information between the farthest nodes of the graph (in terms of the K-walk distance).
Corollary 2.6 (Reduces over-smoothing). Following the direction of Vφ1 allows the influence dis-
tribution between node representations to be decorrelated from random-walk hitting times (assuming
the definition of influence introduced in Xu et al. (2018b)).
Our method also aligns perfectly with a recent proof that multiple independent aggregators are
needed to distinguish neighbourhoods of nodes with continuous features (Corso et al., 2020).
When using eigenvectors of the Laplacian φi to define directions in a graph, we need to keep in
mind that there is never a single eigenvector associated to an eigenvalue, but a whole eigenspace.
5
Under review as a conference paper at ICLR 2021
For instance, a pair of eigenvalues can have a multiplicity of 2 meaning that they can be generated by
different pairs of orthogonal eigenvectors. For an eigenvalue of multiplicity 1, there are always two
unit norm eigenvectors of opposite sign, which poses a problem during the directional aggregation.
We can make a choice of sign and later take the absolute value (i.e. Bav in equation 5). An alterna-
tive is to take a sample of orthonormal basis of the eigenspace and use each choice to augment the
training (see section 2.8). Although multiplicities higher than one do happen for low-frequencies
(square grids have a multiplicity 2 for λ1) this is not common in “real-world graphs”; we found no
λ1 multiplicity greater than 1 on the ZINC and PATTERN datasets (see appendix B.4). Further,
although all φ are orthogonal, their gradients, used to define directions, are not always locally or-
thogonal (e.g. there are many horizontal flows in the grid). This last concern is left to be addressed
in future work.
2.5	Generalization of the convolution on a grid
In this section we show that our method generalizes CNNs by allowing to define any radius-R
convolutional kernels in grid-shaped graphs. The radius-R kernel at node u is a convolutional kernel
that takes the weighted sum of all nodes v at a distance d(u, v) ≤ R.
Consider the lattice graph Γ of size N1 × N2 × ... × Nn where each vertices are connected to
their direct non-diagonal neighbour. We know from Lemma C.1 that, for each dimension, there is
an eigenvector that is only a function of this specific dimension. For example, the lowest frequency
eigenvector φ1 always flows in the direction of the longest length. Hence, the Laplacian eigenvectors
of the grid can play a role analogous to the axes in Euclidean space, as shown in figure 2.
With this knowledge, we show in theorem 2.7 (proven in C.7), that we can generalize all convolu-
tional kernels in an n-dimensional grid. This is a strong result since it demonstrates that our DGN
framework generalizes CNNs when applied on a grid, thus closing the gap between GNNs and the
highly successful CNNs on image tasks.
Theorem 2.7 (Generalization radius-R convolutional kernel in a lattice). For an n-dimensional
lattice, any convolutional kernel of radius R can be realized by a linear combination of directional
aggregation matrices and their compositions.
As an example, figure 4 shows how a linear combination of the first and m-th aggregators
B(Vφι,m) realize a kernel on an N X M grid, where m = \N/M] and N > M.
Graph aggregation	y = 28Μ X	y = 2B戈	X y =	28北 X	y =	2Br^x X，=(吗E:教	#
∖ +ZW4i∕αv 十	dx /
/	卬4	\
equivalent°n HRIrKNlI
image 1'×M, With	1y = {Ix* 1 ∣1	⅛= 4--i 11 Iy = ∖1^*	h =(左*	/» = (/** -2W,% +2W3
N > M ; %%M ≠ 0	∖	)	\	1	∖ H /	∖ BJ 1	%43/
—W5
Figure 4: Realization of a radius-1 convolution using the proposed aggregators. Ix is the input
feature map, * the convolutional operator, Iy the convolution result, and Bi = B(Vφi).
2.6	Extending the radius of the aggregation kernel
Having aggregation kernels for neighbours of distance 2 or 3 is important to improve the expres-
siveness of GNNs, their ability to understand patterns, and to reduce the number of layers required.
However, the lack of directions in GNNs strongly limits the radius of the kernels since, given a graph
of regular degree d, a mean/sum aggregation at a radius-R will result in a heavy over-squashing of
dR messages. Using the directional fields, we can enumerate different paths, thus assigning a differ-
ent weight for different R-distant neighbours. This method, proposed in appendix A.7, avoids the
over-squashing, but empirical results are left for future work.
2.7	Comparison with Weisfeiler-Lehman (WL) test
We also compare the expressiveness of the Directional Graph Networks with the classical WL graph
isomorphism test which is often used to classify the expressivity of graph neural networks (Xu et al.,
2018a). In theorem 2.8 (proven in appendix C.8) we prove that DGNs are capable of distinguishing
pairs of graphs that the 1-WL test (and so ordinary GNNs) cannot differentiate.
6
Under review as a conference paper at ICLR 2021
Theorem 2.8 (Comparison with 1-WL test). DGNs using the mean aggregator, any directional
aggregator of the first eigenvector and injective degree-scalers are strictly more powerful than the
1-WL test.
2.8 Data augmentation
Another important result is that the directions in the graph allow to replicate some of the most
common data augmentation techniques used in computer vision, namely reflection, rotation and
distortion. The main difference is that, instead of modifying the image (such as a 5° rotation), the
proposed transformation is applied on the vector field defining the aggregation kernel (thus rotating
the kernel by -5° without changing the image). This offers the advantage of avoiding to pre-process
the data since the augmentation is done directly on the kernel at each iteration of the training.
The simplest augmentation is the vector field flipping, which is done changing the sign of the field
F , as stated in definition 4. This changes the sign of Bdx , but leaves Bav unchanged.
Definition 4 (Reflection of the vector field). For a vector field F, the reflected field is -F.
Let F1 , F2 be vector fields in a
i ∙ . 1 i-τ	ι i-τ F ♦
graph, with F1 and F2 being
the field normalized such that each row
has a unitary L2-norm. Define the angle vector a by〈(Fj,：, (A)i,：i = cos(α∕. The vector field
i—τ I ∙ . )	ι ∙ ι	. i` τ-↑	ι ∙ ι . i-τ EI	. ∙ ι ι	ι t`	τ-↑ I EI
F2⊥ is the normalized component of F2 perpendicular to F1 . The equation below defines F2⊥ . The
next equation defines the angle
(F⊥ )i,：=
,ʌ	. ʌ	ʌ	.	ʌ	.
(F2 -〈Fl,F2iF1)i,:
...ʌ	. ʌ	ʌ	.	ʌ	.	..
∣∣(F2 -〈Fi,F2iF1)"
Notice that we then have the decomposition (F2)i,: = cos(αi)(F1)i,: + sin(αi)(F2⊥)i,:.
Definition 5 (Rotation of the vector fields). For F1 and F2 non-colinear vector fields with each
vector of unitary length, their rotation by the angle θ in the plane formed by {F1, F2} is
FI = FIdiag(Cosθ) + F⊥diag(smθ) ,	F2 = FIdiag(cos(θ + α)) + F2⊥diag(sin(θ + α)) (8)
Finally, the following augmentation has a similar effect to a wave distortion applied on images.
Definition 6 (Random distortion of the vector field). For vector field F and anti-symmetric random
noise matrix R, its randomly distorted field is F0 = F + R ◦ A.
3 Implementation
We implemented the models using the DGL and PyTorch libraries and we provide the code at the
address https://anonymous.4open.science/r/a752e2b1-22e3-40ce-851c-a564073e1fca/. We test our
method on standard benchmarks from Dwivedi et al. (2020) and Hu et al. (2020), namely ZINC,
CIFAR10, PATTERN and MolHIV with more details on the datasets and how we enforce a fair
comparison in appendix B.1.
For the empirical experiments we inserted our proposed aggregation method in two dif-
ferent type of message passing architecture used in the literature: a simple one simi-
lar to the one present in GCN (equation 9a) (Kipf & Welling, 2016) and a more com-
plex and general one typical of MPNN (9b) (Gilmer et al., 2017) with or without edge
features eji. Hence, the time complexity O(Em) is identical to the PNA (Corso et al.,
2020), where E is the number of edges and m the number of aggregators, with an ad-
ditional O(Ek) to pre-compute the k-first eigenvectors, as explained in the appendix B.2.
Xi(t+1) =U M Xj(t)	(9a)	Xi(t+1) =U Xi(t), M MXi(t),Xj(t), eji	(9b)
(j,i)∈E	(j,i)∈E	o|pt{iozn}al
where is an operator which concatenates the results of multiple aggregators, X is the node fea-
tures, M is a linear transformation and U a multiple layer perceptron.
7
Under review as a conference paper at ICLR 2021
We tested the directional aggregators across the datasets using the gradient of the first k eigenvectors
Vφι,...,k as the underlying vector fields. Here, k is a hyperparameter, usually 1 or 2, but could be
bigger for high-dimensional graphs. To deal with the arbitrary sign of the eigenvectors, we take the
absolute value of the result of equation 6, making it invariant to a reflection of the field. In case of a
disconnected graph, φi is the i-th eigenvector of each connected component. Despite the numerous
aggregators proposed in appendix A, only Bdx and Bav are tested empirically.
4 Results and discussion
Directional aggregation Using the benchmarks introduced in section 3, we present in figure 5 a
fair comparison of various aggregation strategies using the same parameter budget and hyperparam-
eters. We see a consistent boost in the performance for simple, complex and complex with edges
models using directional aggregators compared to the mean-aggregator baseline.
	ZINC			PATTERN		CIFAR10		MolHIV I
AggregatOrs	Simple	Complex	Complex-E	Simple	Complex	Simple	Complex	Simple
	MAE	MAE	MAE	% acc	% acc	% acc	% acc	% ROC-AUC
mean	0.316	0.353	0.262	80.77	83.34	55.9	62.8	75.1
mean Posl	0.349	0.332	0.297	80.76	83.74			75.8
mean Posl po%	0.344	0.330	0.284	84.51	81.25			76.1
mean dx1	0.296	0.233	0.191	84.22	83.44			78.0
mean dx】dx2	0.337	0.271	0.205	81.61	86.62	52.9	69.8	76.5
mean av1	0.317	0.332	0.276	84.54	83.21			78.4
mean av】av2	0.367	0.332	0.260	85.12	85.38	60.6	65.1	77.1
mean dx】av1	0.290	0.245	0.192	85.17	86.68			79.0
Figure 5: Test set results using a parameter budget of 〜100k, with the same hyperparameters as
Corso et al. (2020). The low-frequency Laplacian eigenvectors are used to define the directions,
except for CIFAR10 that uses the coordinates of the image. For brevity, we denote dxi and avi as
the directional derivative Bdix and smoothing Baiv aggregators of the i-th direction. We also denote
posi as the i-th eigenvector used as positional encoding for the mean aggregator.
In particular, we see a significant improvement in ZINC and MolHIV using the directional aggrega-
tors. We believe this is due to the capacity to move efficiently messages across opposite parts of the
molecule and to better understand the role of atom pairs. Further, the thesis that DGNs can bridge
the gap between CNNs and GNNs is supported by the clear improvements on CIFAR10 over the
baselines. This contrasts with the positional encoding which showed no clear improvement.
With our theoretical analysis in mind, we expected to perform well on PATTERN since the flow of
the first eigenvectors are meaningful directions in a stochastic block model and passing messages
using those directions allows the network to efficiently detect the two communities. The results
match our expectations, outperforming all the previous models.
Comparison to the literature In order to compare our model with the literature, we fine-tuned it
on the various datasets and we report its performance in figure 6. We observe that DGN provides
significant improvement across all benchmarks, highlighting the importance of anisotropic kernels.
In the work by Dwivedi et al. (2020), they proposed the use of positional encoding of the eigen-
vectors in node features, but these bring significant improvement when many eigenvectors and high
network depths are used. Our results outperform them with fewer parameters, less depth, and only
1-2 eigenvectors, further motivating their use as directional flows instead of positional encoding.
Data augmentation To evaluate the effectiveness of the proposed augmentation, we trained the
models on a reduced version of the CIFAR10 dataset. The results in figure 7 show clearly a higher
expressive power of the dx aggregator, enabling it to fit well the training data. For a small dataset,
this comes at the cost of overfitting and a reduced test-set performance, but we observe that randomly
rotating or distorting the kernels counteracts the overfitting and improves the generalization.
As expected, the performance decreases when the rotation or distortion is too high since the aug-
mented graph changes too much. In computer vision images similar to CIFAR10 are usually rotated
by less than 30° (Shorten & Khoshgoftaar; O’Gara & McGuinness, 2019). Further, due to the Con-
stant number of parameters across models, less parameters are attributed to the mean aggregation in
8
Under review as a conference paper at ICLR 2021
	ZINC		PATTERN	CIFAR10		MolHIV I
Model	No edge features MAE	Edge features MAE	No edge features % acc	No edge features % acc	Edge features % acc	No edge features % ROC-AUC
GCN	0.469±0.002		65.880±0∙074	54.46±0.10		76.06±0.97 *
GIN	0.4O8±0.008		85.590±0.0iι	53.28±3.70 一		75.58±1∙40 *
GraphSage	0.410±0.005		50.516±0∙001	66.08±0.24		
GAT	0.463±0.002		75.824±i∙823	65.48±0.33		
MoNet	0.407±0.007		85.482±0.037	53.42±0.43		
GatedGCN	0.422±0.006	0.363±0.009	84.480±0.122	69.19±0.28	69.37±0.48	
PNA	0.320±0.032	0.188±0.004	86.567±0∙075	70.46±0.44	70.47±0.72	79.05±i∙32 *
DGN	0.219±0.010	0.168±0.003	86.680±0.034	72.70±0∙54	72.84±0∙42	79.70±0∙97
Figure 6: Fine-tuned results of the DGN model against models from Dwivedi et al. (2020) and Hu
et al. (2020): GCN (Kipf & Welling, 2016), GraphSage (Hamilton et al., 2017), GIN (Xu et al.,
2018a), GAT (VelickoVic et al., 2017), MoNet (Monti et al., 2017), GatedGCN (Bresson & Laurent,
2017) and PNA (Corso et al., 2020). All the models use 〜100k parameters, except those with *
who use 300k to 1.9M. In ZINC the DGN aggregators are {mean, dx1, max, min}, in PATTERN
{mean, dx1, av1}, in CIFAR10 {mean, dx1, dx2, max}, in MolHIV {mean, dx1, av1, max, min}.
the directional models, thus it cannot fit well the data when the rotation/distortion is too strong since
the directions are less informatiVe. We expect large models to perform better at high angles.
Rotation angle
Rotation angle
Percentage distortion
Percentage distortion
Figure 7: Accuracy of the Various models using data augmentation with a complex architecture
of 〜 100k parameters and trained on 10% of the CIFAR10 training set (4.5k images). An angle
of X corresponds to a rotation of the kernel by a random angle sampled uniformly in (-x°,x°)
using definition 5 with F1,2 being the gradient of the horizontal/Vertical coordinates. A noise of
100x% corresponds to a distortion of each eigenVector with a random noise uniformly sampled in
(-χ ∙ m,χ ∙ m) where m is the average absolute value of the eigenvector,s components. The mean
baseline model is not affected by the augmentation since it does not use the underlining Vector field.
5 Conclusion
The proposed DGN method allows to solve many problems of GNNs, including the lack of
anisotropy, the low expressiveness, the over-smoothing and over-squashing. For the first time in
graph networks, we generalize the directional properties of CNNs and their data augmentation ca-
pabilities. Based on an intuitive idea and backed by a set of strong theoretical and empirical results,
we believe this work will give rise to a new family of directional GNNs. Future work can focus on
the implementation of radius-R kernels and improving the choice of multiple orthogonal directions.
Broader Impact This work will extend the usability of graph networks to all problems with phys-
ically defined directions, thus making GNN a new laboratory for physics, material science and biol-
ogy. In fact, the anisotropy present in a wide variety of systems could be expressed as vector fields
(spinor, tensor) compatible with the DGN framework, without the need of eigenvectors. One exam-
ple is magnetic anisotropicity in metals, alloys and also in molecules such as benzene ring, alkene,
carbonyl, alkyne that are easier or harder to magnetise depending on the directions or which way
the object is rotated. Other examples are the response of materials to high electromagnetic fields
(e.g. to study material responses at terahertz frequency); all kind of field propagation in crystals
lattices (vibrations, heat, shear and frictional force, young modulus, light refraction, birefringence);
multi-body or liquid motion; traffic modelling; and design of novel materials and constrained struc-
tures. This also enables GNNs to be used for virtual prototyping systems since the added directional
constraints could improve the analysis of a product’s functionality, manufacturing and behavior.
9
Under review as a conference paper at ICLR 2021
Author Contributions
Anonymous
Acknowledgments
Anonymous
References
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
2020. URL http://arxiv.org/abs/2006.05205.
Xavier Bresson and Thomas Laurent. Residual gated graph convnets. arXiv preprint
arXiv:1711.07553, 2017.
Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-
IicdeePlearning: going beyond euclidean data. 34(4):18-42,2017. ISSN 1053-5888,1558-0792.
doi: 10.1109/MSP.2017.2693418. URL http://arxiv.org/abs/1611.08097.
Fan Chung and S.T.Yau. Discrete green’s functions, 2000.
Fan Chung and S. T. Yau. Discrete green’s functions. 91(1):191-214, 2000. ISSN 0097-
3165. doi: 10.1006/jcta.2000.3094. URL http://www.sciencedirect.com/science/
article/pii/S0097316500930942.
F.R.K. Chung, F.C. Graham, CBMS Conference on Recent Advances in SPectral GraPh Theory,
National Science Foundation (U.S.), American Mathematical Society, and Conference Board
of the Mathematical Sciences. Spectral Graph Theory. CBMS Regional Conference Se-
ries. Conference Board of the mathematical sciences, 1997. ISBN 9780821803158. URL
https://books.google.ca/books?id=4IK8DgAAQBAJ.
Gabriele Corso, Luca Cavalleri, DominiqUe Beaini, Pietro Lio, and Petar Velickovic. Principal
neighbourhood aggregation for graPh nets. arXiv preprint arXiv:2004.05718, 2020.
Vishwaraj Doshi and Do Young Eun. Fiedler vector approximation via interacting random walks.
2000. doi: 10.1145/3379487. URL http://arxiv.org/abs/2002.00283.
Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.
Miroslav Fiedler. Algebraic connectivity of graphs. Czechoslovak Mathematical Journal, 23:298-
305, 01 1973. doi: 10.21136/CMJ.1973.101168.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In Proceedings of the 34th International Conference on
Machine Learning-Volume 70, pp. 1263-1272. JMLR. org, 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024-1034, 2017.
William L. Hamilton. Graph Representation Learning. Morgan and Claypool, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
10
Under review as a conference paper at ICLR 2021
Md Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convolutional
neural networks encode? 2020. URL http://arxiv.org/abs/2001.08248.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for
molecular graph generation. 2018. URL http://arxiv.org/abs/1802.04364.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Johannes Klicpera, Janek Groβ, and StePhan Gunnemann. Directional message passing for molec-
ular graphs. 2019. URL https://openreview.net/forum?id=B1eWbxStPH.
Boris Knyazev, Graham W Taylor, and Mohamed Amer. Understanding attention and generalization
in graph neural networks. In Advances inNeural Information Processing Systems, pp. 4204-4214,
2019.
Risi Kondor, Hy Truong Son, Horace Pan, Brandon Anderson, and Shubhendu Trivedi. Covariant
compositional networks for learning graphs. arXiv preprint arXiv:1801.02144, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images, 2009.
Cornelius Lanczos. An iteration method for the solution of the eigenvalue problem of linear differ-
ential and integral operators. United States Governm. Press Office Los Angeles, CA, 1950.
Ron Levie, Federico Monti, Xavier Bresson, and Michael M. Bronstein. CayleyNets: Graph
convolutional neural networks with complex rational spectral filters. 2018. URL http:
//arxiv.org/abs/1705.07664.
B. Levy. Laplace-beltrami eigenfunctions towards an algorithm that ”understands” geometry. In
IEEE International Conference on Shape Modeling and Applications 2006 (SMI’06), pp. 13-13,
2006. doi: 10.1109/SMI.2006.21.
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multi-
scale deep graph convolutional networks. In Advances in Neural Information Processing Systems,
pp. 10943-10953, 2019.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. arXiv preprint arXiv:1812.09902, 2018.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-
5124, 2017.
Sarah O’Gara and Kevin McGuinness. Comparing data augmentation strategies for deep image clas-
sification. 2019. doi: http://doi.org10.21427/148b-ar75. URL https://arrow.tudublin.
ie/impstwo/7.
Chris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan
Carter. An overview of early vision in InceptionV1. 5(4):e00024.002, 2020. ISSN 2476-
0757. doi: 10.23915/distill.00024.002. URL https://distill.pub/2020/circuits/
early-vision.
Hao Peng, Jianxin Li, Qiran Gong, Senzhang Wang, Yuanxing Ning, and Philip S. Yu. Graph
convolutional neural networks via motif-based attention. 2019. URL http://arxiv.org/
abs/1811.08270.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. DropEdge: Towards deep graph
convolutional networks on node classification. pp. 17, 2020.
Connor Shorten and Taghi M. Khoshgoftaar. A survey on image data augmentation for deep learn-
ing. 6(1):60. ISSN 2196-1115. doi: 10.1186/s40537-019-0197-0. URL https://doi.org/
10.1186/s40537-019-0197-0.
11
Under review as a conference paper at ICLR 2021
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Bingbing XU, HUawei Shen, Qi Cao, YUnqi QiU, and XUeqi Cheng. Graph wavelet neUral network.
2019. URL http://arxiv.org/abs/1904.07785.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? arXiv preprint arXiv:1810.00826, 2018a.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks. In International
Conference on Machine Learning, pp. 5453-5462, 2018b.
A	Appendix - Choices of directional aggregators
This appendix helps Understand the choice of Bav and Bdx in section 2.3 and presents different
directional aggregators that can be Used as an alternative to the ones proposed.
A simple alternative to the directional smoothing and directional derivative operator is to simply
take the forward/backward valUes according to the Underlying positive/negative parts of the field
F , since it can effectively replicate them. However, there are many advantage of Using Bav,dx .
First, one can decide to Use either of them and still have an interpretable aggregation with half
the parameters. Then, we also notice that Bav,dx regUlarize the parameter by forcing the network
to take both forward and backward neighboUrs into accoUnt at each time, and avoids one of the
neighboUrs becoming too important. Lastly, they are robUst to a change of sign of the eigenvectors
since Bav is sign invariant and Bdx will only change the sign of the resUlts, which is not the case
for forward/backward aggregations.
A. 1 Retrieving the mean and Laplacian aggregations
It is interesting to note that we can recover simple aggregators from the aggregation matrices
Bav(F) and Bdx(F). Let F be a vector field sUch that all edges are eqUally weighted Fij = ±C
for all edges (i, j). Then, the aggregator Bav is eqUivalent to a mean aggregation:
Bav (F)x = D-1Ax
Under the condition Fij = C, the differential aggregator is eqUivalent to a Laplacian operator L
normalized Using the degree D
Bdx(CA)x =D-1(A-D)x = -D-1Lx
A.2 Global field normalization
The proposed aggregators are defined with a row-wise normalized field
F. = -FL-
HFiJLP
meaning that all the vectors are of Unit-norm and the aggregation/message passing is done only
according to the direction of the vectors, not their amplitUde. However, it is also possible to do a
global normalization of the field F by taking a matrix-norm instead of a vector-norm. Doing so will
modUlate the aggregation by the amplitUde of the field at each node. One needs to be carefUl since a
global normalization might be very sensitive to the nUmber of nodes in the graph.
A.3 Center-balanced aggregators
A problem arises in the aggregators Bdx and Bav proposed in eqUations 5 and 6 when there is
an imbalance between the positive and negative terms of F± . In that case, one of the directions
overtakes the other in terms of associated weights.
12
Under review as a conference paper at ICLR 2021
An alternative is also to normalize the forward and backward directions separately, to avoid having
either the backward or forward direction dominating the message.
Bav-center
(F)i,:
Fi+ + Fi-
IlFj + Fj IIli
f0± =	∣F± I
i,:	IIF± IIli + e
(10)
The same idea can be applied to the derivative aggregator equation 11 where the positive and nega-
tive parts of the field F± are normalized separately to allow to project both the forward and back-
ward messages into a vector field of unit-norm. F+ is the out-going field at each node and is used
for the forward direction, while F- is the in-going field used for the backward direction. By aver-
aging the forward and backward derivatives, the proposed matrix Bdx-center represents the centered
derivative matrix.
Bdx-center (F)i,: = Fi0,: - diag
( ∖
F+	F-
IIF+ IIli + e+ IIF- IIli + e
、	，—y_ J 、	' 一〜一 J
forward field	backward field
(11)
(X Fj, F
1
2
A.4 Hardening the aggregators
The aggregation matrices that we proposed, mainly Bdx and Bav depend on a smooth vector field
F. At any given node, the aggregation will take a weighted sum of the neighbours in relation to the
direction of F. Hence, if the field Fv at a node v is diagonal in the sense that it gives a non-zero
weight to many neighbours, then the aggregator will compute a weighted average of the neighbours.
Although there are clearly good reasons to have this weighted-average behaviour, itis not necessarily
desired in every problem. For example, if we want to move a single node across the graph, this
behaviour will smooth the node at every step. Instead, we propose below to soften and harden the
aggregations by forcing the field into making a decision on the direction it takes.
Soft hardening the aggregation is possible by using a softmax with a temperature T on each row
to obtain the field Fsofthard.
(Fsofthard)i,: = sign(Fi,:)softmax(T IFi,: I)	(12)
Hardening the aggregation is possible by using an infinite temperature, which changes the soft-
max functions into argmax. In this specific case, the node with the highest component of the field
will be copied, while all other nodes will be ignored.
(Fhard)i,: = sign(Fi,:)argmax(IFi,: I)
(13)
An alternative to the aggregators above is to take the softmin/argmin of the negative part and the
softmax/argmax of the positive part.
A.5 Forward and backward copy
The aggregation matrices Bav and Bdx have the nice property that if the field is flipped (change of
sign), the aggregation gives the same result, except for the sign of Bdx . However, there are cases
where we want to propagate information in the forward direction of the field, without smoothing it
with the backward direction. In this case, we can define the strictly forward and strictly backward
fields below, and use them directly with the aggregation matrices.
Fforward = F+
Fbackward = F
(14)
Further, we can use the hardened fields in order to define a forward copy and backward copy, which
will simply copy the node in the direction of the highest field component.
Fforward copy = Fhard	,	Fbackward copy = Fhard
(15)
13
Under review as a conference paper at ICLR 2021
A.6 Phantom zero-padding
Some recent work in computer vision has shown the importance of zero-padding to improve CNNs
by allowing the network to understand it’s position relative to the border (Islam et al., 2020). In
contrast, using boundary conditions or reflection padding makes the network completely blind to
positional information. In this section, we show that we can mimic the zero-padding in the direction
of the field F for both aggregation matrices Bav and Bdx .
Starting with the Bav matrix, in the case of a missing neighbour in the forward/backward direction,
the matrix will compensate by adding more weights to the other direction, due to the denominator
which performs a normalization. Instead, we would need the matrix to consider both directions
separately so that a missing direction would result in zero padding. Hence, we define Bav,0pad
below, where either the F + or F - will be 0 on a boundary with strictly in-going/out-going field.
(Bav,0pad)i,
ι ( IF+1	F-1
2 IF+ IIli + e + IIF-IIli + e
(16)
Following the same argument, we define Bdx,0pad below, where either the forward or backward
term is ignored. The diagonal term is also removed at the boundary so that the result is a center
derivative equal to the subtraction of the forward term with the 0-term on the back (or vice-versa),
instead of a forward derivative.
H+
Bdx-Opad(F)i,: = < Fi7
〔1(F0,+ + F0,- - diag (Pj F0+ + F0-)i,:
F 0+ =	FT	F 0- =	Fi,：
i，： _ IIF+ IIli + e i,: _ IIF-IIli + e
if Pj Fi0,-j =0
if Pj Fi0,+j =0
otherwise	(17)
A.7 Extending the radius of the aggregation kernel
We aim at providing a general radius-R kernel BR that assigns different weights to different subsets
of nodes nu at a distance R from the center node nv .
First, we decompose the matrix B(F) into positive and negative parts B± (F) representing the
forward and backward steps aggregation in the field F.
B(F) = B+(F) - B- (F)	(18)
Thus, defining Bfb(F)i,： = E⅛p , we can find different aggregation matrices by using different
combinations of walks of radius R. First demonstrated for a grid in theorem 2.7, we generalize it in
equation 19 for any graph G.
Definition 7 (General radius R n-directional kernel). Let Sn be the group of permutations over n
elements with a set of directional fields Fi.
BR:=
V={v1,v2,...,vn}∈Nn	σ∈Sn
l∣V∣∣Li ≤R,	-R≤Vi≤R	|{z}
、	_、z _	/ optional
Any^ choice of walk V with at most R steps permutations
using all combinations of v1 , v2 , ..., vn
N
X av Y(Bsgn(Vσ(j"(Fσ(j)))lvσ(j)1
j=1
|-----------------V----------------}
Aggregator following the steps V, permuted by Sn
(19)
In this equation, n is the number of directional fields and R is the desired radius. V represents
all the choices of walk {v1, v2, ..., vn} in the direction of the fields {F1, F2, ..., Fn}. For example,
V = {3, 1, 0, -2} has a radius R = 6, with 3 steps forward of F1, 1 step forward of F2, and 2 steps
backward of F4. The sign of each Bf±b is dependant to the sign of vσ(j), and the power Ivσ(j) I is the
14
Under review as a conference paper at ICLR 2021
number of aggregation steps in the directional field Fσ(j). The full equation is thus the combination
of all possible choices of paths across the set of fields Fi , with all possible permutations. Note that
we are restricting the sum to vi having only a possible sign; although matrices don’t commute, we
avoid choosing different signs since it will likely self-intersect a lower radius walk. The permutations
σ are required since, for example, the path up → left is different (in a general graph) than the path
left → up.
This matrix BR has a total of PR=o(2n)r = (2*]]-1 parameters, with a high redundancy
since some permutations might be very similar, e.g. for a grid graph we have that up → left is
identical to left → up. Hence, we can replace the permutation Snby a reverse ordering, mean-
ing that QjN Bj = BN ...B2 B1 . Doing so does not perfectly generalize the radius-R kernel for
all graphs, but it generalizes it on a grid and significantly reduces the number of parameters to
PR	Pmin(n,r)
2r (n )(r-1)∙
B Appendix - Implementation details
B.1	Benchmarks and datasets
We use a variety of benchmarks proposed by Dwivedi et al. (2020) and Hu et al. (2020) to test the
empirical performance of our proposed methods. In particular, to have a wide variety of graphs and
tasks we chose:
1.	ZINC, a graph regression dataset from molecular chemistry. The task is to predict a score
that is a subtraction of computed properties logP - SA, with logP being the computed
octanol-water partition coefficient, and SA being the synthetic accessibility score (Jin et al.,
2018).
2.	CIFAR10, a graph classification dataset from computer vision (Krizhevsky, 2009). The
task is to classify the images into 10 different classes, with a total of 5000 training image
per class and 1000 test image per class. Each image has 32 × 32 pixels, but the pixels have
been clustered into a graph of 〜 100 super-pixels. Each super-pixel becomes a node in
an almost grid-shaped graph, with 8 edges per node. The clustering uses the code from
Knyazev et al. (2019), and results in a different number of super-pixels per graph.
3.	PATTERN, a node classification synthetic benchmark generated with Stochastic Block
Models, which are widely used to model communities in social networks. The task is
to classify the nodes into 2 communities and it tests the fundamental ability of recognizing
specific predetermined subgraphs.
4.	MolHIV, a graph classification benchmark from molecular chemistry. The task is to predict
whether a molecule inhibits HIV virus replication or not. The molecules in the training, val-
idation and test sets are divided using a scaffold splitting procedure that splits the molecules
based on their two-dimensional structural frameworks.
Our goal is to provide a fair comparison to demonstrate the capacity of our proposed aggregators.
Therefore, we compare the various methods on both types of architectures using the same hyper-
parameters tuned in previous works (Corso et al., 2020) for similar networks. The models vary
exclusively in the aggregation method and the width of the architectures to keep a set parameter
budget.
In CIFAR10 it is impossible to numerically compute a deterministic vector field with eigenvectors
due to the multiplicity of λ1 being greater than 1. This is caused by the symmetry of the square
image, and is extremely rare in real-world graphs. Therefore, we used as underlying vector field
the gradient of the coordinates of the image. Note that these directions are provided in the nodes’
features in the dataset and available to all models, that they are co-linear to the eigenvectors of the
grid as per lemma C.1, and that they mimic the inductive bias in CNNs.
B.2	Implementation and computational complexity
Unlike several more expressive graph networks (Kondor et al., 2018; Maron et al., 2018), our method
does not require a computational complexity superlinear with the size of the graph. The calculation
15
Under review as a conference paper at ICLR 2021
of the first k eigenvectors during pretraining, done using Lanczos method (Lanczos, 1950) and the
sparse module of Scipy, has a time complexity of O(Ek) where E is the number of edges. During
training the complexity is equivalent to a m-aggregator GNN O(Em) (Corso et al., 2020) for the
aggregation and O(N m) for the MLP.
To all the architectures we added residual connections (He et al., 2016), batch normalization (Ioffe
& Szegedy, 2015) and graph size normalization (Dwivedi et al., 2020).
For all the datasets with non-regular graphs, we combine the various aggregators with logarithmic
degree-scalers as in Corso et al. (2020).
An important thing to note is that, for dynamic graphs, the eigenvectors need to be re-computed
dynamically with the changing edges. Fortunately, there are random walk based algorithms that can
estimate φ1 quickly, especially for small changes to the graph (Doshi & Eun, 2000). In the current
empirical results, we do not work with dynamic graphs.
B.3	Running time
The precomputation of the first four eigenvectors for all the graphs in the datasets takes 38s for
ZINC, 96s for PATTERN and 120s for MolHIV on CPU. Table 1 shows the average running time
on GPU for all the various model from figure 5. On average, the epoch running time is 16% slower
for the DGN compared to the mean aggregation, but a faster convergence for DGN means that the
total training time is on average 8% faster for DGN.
Table 1: Average running time for the non-fine tuned models from figure 5. Each entry represents
average time per epoch / average total training time. Each of these models has a parameter budget
〜100k and was run on a Tesla T4 (15GB GPU). The avg increase row is the average of the relative
running time of all rows compared to the mean row, with a negative value meaning a faster running
time.
		ZINC				PATTERN		
Aggregators	Simple	Complex	Complex-E	Simple	Complex
mean	3.29s∕1505s	3.58s/1584s	3.56s∕1654s	153.1s/10154s	117.8s/9031s
mean dx1	3.86s∕1122s	3.77s/1278s	4.22s∕1371s	144.9s/8109s	127.2s/8417s
mean dx1 dx2	4.23s∕1360s	4.55s/1560s	4.63s∕1680s	153.3s/8057s	167.9s/9326s
mean av1	3.68s∕1297s	3.84s/1398s	3.92s∕1272s	128.0s/8680s	88.1s/7456s
mean av1 av2	3.95s∕1432s	4.03s/1596s	4.07s∕1721s	134.2s/8115s	170.4s/11114s
mean dx1 av1	3.89s∕1079s	4.09s/1242s	4.58s∕1510s	118.6s/6221s	144.2s/9112s
avg increase	+ 19%/-16%—	+13%/-11%	+20%/-9% -	-11%/-23%	+18%/+1%
		CIFAR10			MolHIV
Aggregators	Simple	Complex	Simple
mean	83.6s∕10526s	78.7s/10900s	11.4s∕2189s
mean dx1			12.6s∕2348s
mean dx1 dx2	98.4s∕8405s	100.9s/5191s	14.1s∕2345s
mean av1			12.2s∕2177s
mean av1 av2	117.1s∕12834s	89.5s/14481s	13.9s∕2150s
mean dx1 av1			14.0s∕2070s
avg increase	+29%/+1%~~	+21%/-10%	+17%/+1% ―
B.4	Eigenvector multiplicity
The possibility to define equivariant directions using the low-frequency Laplacian eigenvectors is
subject to the uniqueness of those vectors. When the dimension of the eigenspaces associated with
the lowest eigenvalues is 1, the eigenvectors are defined up to a constant factor. In section 2.4, we
propose the use of unit vector normalization and an absolute value to eliminate the scale and sign
ambiguity. When the dimension of those eigenspaces is greater than 1, it is not possible to define
equivariant directions using the eigenvectors.
16
Under review as a conference paper at ICLR 2021
Fortunately, it is very rare for the Laplacian matrix to have repeated eigenvalues in real-world
datasets. We validate this claim by looking at ZINC and PATTERN datasets where we found no
graphs with repeated Fiedler vector and only one graph out of 26k with multiplicity of the second
eigenvector greater than 1.
When facing a graph that presents repeated Laplacian eigenvalues, we propose to randomly shuffle,
during training time, different eigenvectors randomly sampled in the eigenspace. This technique
will act as a data augmentation of the graph during training time allowing the network to train with
multiple directions at the same time.
C Appendix - Mathematical proofs
C.1 Proof for Theorem 2.1 (Directional smoothing)
The operation y = Bavx is the directional average of x, in the sense that yu is the mean of xv ,
weighted by the direction and amplitude of F .
Proof. This should be a simple proof, that if we want a weighted average of our neighbours, we
simply need to multiply the weights by each neighbour, and divide by the sum of the weights. Of
course, the weights should be positive.
□
C.2 Proof for Theorem 2.2 (Directional derivative)
Suppose F have rows of unit L1 norm. The operation y = Bdx (F)x is the centered directional
derivative of x in the direction of F , in the sense of equation 4, i.e.
y = DFX = (F - diag(XF,j))x
j
Proof. Since F rows have unit L1 norm, F = F. The i-th coordinate of the vector
(F -diag(PjF:,jxis
(Fx - diag (E F J xj = EFij XCj)-(E FijJx ⑶
= X (x(j ) - x(i))Fi,j
j∙(i,j)∈E
= DF xCi)
□
C.3 Proof for Theorem 2.3 (K-Gradient of the low-frequency eigenvectors)
Let λi and φi be the eigenvalues and eigenvectors of the normalized Laplacian of a connected graph
Lnorm and let a, b = arg max1≤i,j≤n{dK Cvi, vj)} be the nodes that have highest K-walk distance.
Let m = argmin1≤i≤nCφ1)i and M = argmax1≤i≤nCφ1)i, then dKCvm,vM) - dKCva,vb) has
order OC1 - λ2).
Proof. For this theorem, we use the indices i = 0, ..., CN - 1), sorted such that λi ≤ λi+1. Hence,
λ0 = 0 and λ1 is the first non-trivial eigenvalue.
First we need the following proposition:
17
Under review as a conference paper at ICLR 2021
Proposition 1 (K-walk distance matrix). The K-walk distance matrix P associated with a graph is
the matrix such that (P)i,j = dK(vi, vj) can be written as PpK=1 Wp, where W = D-1A is the
random walk matrix.
Let’s define W = D-1A the random walk matrix of the graph.
First, we are going to show that W is jointly diagonalizable with Lnorm and we are going to relate
its eigenvectors φ0i and its eigenvalues λ0i with the ones of W .
Indeed, Lsym is a symmetric real matrix which is semi-positive definite diagonalizable by the spectral
theorem. Since the matrix LnOrm is similar to D 1 LnOrmD-1 = D-1LD-2 = LSym and the matrix
of similarity is D 2, a positive definite matrix, Lnorm is diagonalizable and semi-positive definite.
By
Lnorm = D-1L = D-1(L + D -D) = I + D-1(L -D) = I - D-1A = I - W
the random walk matrix is jointly diagonalizable with the random walk Laplacian. Also their eigen-
values and eigenvectors are related to each other by φi = φ0n-1-i and λ0i = 1 - λn-1-i
Moreover, the constant eigenvector associated with eigenvalue 0 of the Random walk Laplacian,
is the eigenvector associated with the highest eigenvalue of the Random walk matrix and by the
formula obtained, λ0n-1 = 1 - λ0 = 1
Now, we are going to approximate the K-walk distance matrix P using the 2 eigenvectors of the
Random walk matrix associated with the highest eigenvalues.
By Proposition 1 we have that P = PpK=1 Wp, which can be written as
K n-1	K n-1
X(X φ0iφ0iT (λ0i))p = XXφ0iφ0iT(λ0i)p
by eigen-decomposition.
Since λn-1-i = 1 - λ0i and λ2	λ1, we have that λ0n-2	λ0n-3, hence we can approximate
K n-1	K	n-1
P = X(X φ0iφ0i(λ0i)p) ≈ X( X φ0iφ0iT (λ0i)p) + O(λ0n-3) =
p=1 i=0	p=1 i=n-2
K1	K
= X(X φi φiT (1 - λi)p) + O(1 - λ2) = X(φ0φ0T + φ1φ1T (1 - λ1)p) + O(1 - λ2)
p=1 i=0	p=1
= Kφ0φ0T + κφ1φ1T + O(1 - λ2)
where κ = PpK=1(1 - λ1)p is a positive constant.
Now we are going to show that the farthest nodes with respect to the K-walk distance are the ones
associated with the highest and lowest value of φ1.
Indeed if we want to choose i, j to be at the farthest distance we need to minimise
(P )ij = (K ΦoΦτ + κφιφT )ij = K + κφι(i)φι(j')
which is minimum when φ1 (i)φ1 (j) is minimum.
We are going to show that exist p, q such that φ1(p) < 0, φ1(q) > 0. Since the eigenvector is non-
zero, without loss of generality assume φ1(0) 6= 0. Since φ0 and φ1 are eigenvectors associated
with different eigenvalues of a real symmetric matrix, they are orthogonal:
n-1
X φ0(i) ∙ φ1⑶=0
i=0
18
Under review as a conference paper at ICLR 2021
and since φ0 is constant the previous equation leads to
n-1	n-1
X φ1(i) = 0 ^⇒ φ1 (O) = — X φ1(i)
i=0	i=1
If such p,q didn,t exist then we would get that ∀i,j φι(i) ∙ φι(j) ≥ 0, hence multiplying both sides
of the previous equation by φ1 (0) we get
n-1
φι(0)2 = - X φl(i) ∙ φl(0) ⇒ φl(0)2 ≤ 0
i=1
Which is a contradiction since by assumption φ1(0) > 0; hence exist p, q such that φ1(p) <
0, φ1(q) > 0.
Since φ1 attains both positive and negative values, the quantity φ1(i)φ1(j) is minimised when it
has negative sign and highest absolute value, hence when i, j are associated with the negative and
positive values with the highest absolute value: the lowest and the highest value of φ1. Hence,
dK(vM, vm) - dK(va, vb) = O(1 - λ2)
□
C.4 Informal argument in support of Conjecture 2.4 (Gradient steps reduce
expected hitting time)
Suppose that x, y are uniformly distributed random nodes such that φi(x) < φi(y). Let z be the
node obtained from X by taking one step in the direction of Vφi, then the expected hitting time is
decreased proportionally to λi-1 and
Ex,y[Q(z, y)] ≤ Ex,y[Q(x, y)]
As a reminder, the definition of a gradient step is given in the definition 3, copied below.
Suppose the two neighboring nodes x and z are such that φ(z) - φ(x) is maximal among the
neighbors ofx, then we will say z is obtained from x by taking a step in the direction of the gradient
Vφ.
In (Chung & S.T.Yau, 2000), it is shown the hitting time Q(x, y) is given by the equation
G G	/G(y,y)	G(χ,y)∖
Q(χ, y) = vol ---------------ʒ—
dy	dx
With λk and φk being the k-th eigenvalues and eigenvectors of the symmetric normalized Laplacian
Lsym, vol the sum of the degrees of all nodes, dx the degree of node x andG Green’s function for
the graph
G(χ,y) = d2dy2 X JΦk (χ)Φk (y)
k>0 k
Since the sign of the eigenvector is not deterministic, the choice φi(x) < φi(y) is used to simplify
the argument without having to consider the change in sign.
Supposing λ1 λ2, the first term of the sum ofG	has much more weight than the following terms.
With z obtained from x by taking a step in the direction of the gradient of φ1 we have
φ1 (z) - φ1(x) > 0
We want to show that the following inequality holds
Ex,y (Q(z, y)) < Ex,y (Q(x, y))
this is equivalent to the following inequality
Ex,y[G(z, y)] > Ex,y[G(x,y)]
19
Under review as a conference paper at ICLR 2021
By the hypothesis λ1	λ2, we can approximate G(x, y)
inequality is equivalent to
1	— 1
〜dx2 dy2 λγΦ1(x)Φ1(y) so the last
"ι ι 1
Eχ,y dz dy2 —φ1(z)φ1(y)
λ1
Γ ι ι 1
> Eχ,y dχ2dy2 ■—φι(x)φι(y)
λ1
Removing all equal terms from both sides, the inequality is equivalent to
Eχ,y hd1 Φ1(z)i
> Eχ,y [dX2 Φ1(x)i
But showing this last inequality is not easy. We know that φ1(z) > φ1(x) and from the choice of z
being a step in the direction of Vφ1, We know it is less likely to be on the border of the graph so We
believe E(dz) ≥ E(dx ). Thus we also believe that the conjecture should hold in general.
We believe this should be true even without the assumption on λ1 and λ2 and for more eigenvectors
than φ1.
C.5 Proof for Lemma C.1 (Cosine eigenvectors)
Consider the lattice graph Γ of size N1 × N2 × ... × Nn, that has vertices Qi=1,...,n{1, ..., Ni} and
the vertices (xi)i=1,...,n and (yi)i=1,...,n are connected by an edge iff |xi - yi| = 1 for one index i
and 0 for all other indices. Note that there are no diagonal edges in the lattice. The eigenvector of
the Laplacian of the grid L(Γ) are given by φj.
Lemma C.1 (Cosine eigenvectors). The Laplacian of Γ has an eigenvalue 2 — 2 cos (N^) With the
associated eigenvector φj that depends only the variable in the i-th dimension and is constant in all
others, with φj = 1n 0 1n 总…总 xi,n 总…0 lNn, and xi,n (j) = cos (∏j — 合)
Proof. First, recall the well known result that the path graph on N vertices PN has eigenvalues
with associated eigenvector xk with i-th coordinate
πki πk
Xk ⑺= cos( — + 2nJ
The Cartesian product of two graphs G = (VG, EG) and H = (VH, EH) is defined as G × H =
(VG×H, EG×H) with VG×H = VG × VH and ((u1,u2 ), ((v1,v2 )) ∈ EG×H iff either u1 = v1 and
(u2,v2) ∈ EH or (uι,vι) ∈ VG and u = v2. It is shown in (Fiedler, 1973) that if (μi)i=ι,…,m
and (λj)j=1,...,n are the eigenvalues of G and H respectively, then the eigenvalues of the Cartesian
product graph G X H are μ% + λj for all possible eigenvalues μ% and λj. Also, the eigenvectors asso-
ciated to the eigenvalue μi + λj are Ui 0 Vj with Ui an eigenvector of the Laplacian of G associated
to the eigenvalue μ% and Vj an eigenvector of the Laplacian of H associated to the eigenvalue λj.
Finally, noticing that a lattice of shape N1 × N2 × ... × Nn is really the Cartesian product of path
graphs of length N1 up to Nn , we conclude that there are eigenvalues 2 — 2cos (Ni). Denoting
by 1Nj the vector in RNj with only ones as coordinates, then the eigenvector associated to the
eigenvalue 2 — 2 cos(N") is
1N1 0 1N2 0 ... 0 x1,Ni 0 ... 0 1Nn
where x1,Ni is the eigenvector of the Laplacian of PNi associated to its first non-zero eigenvalue.
2 - 2cos (N ).	□
20
Under review as a conference paper at ICLR 2021
C.6 Radius 1 convolution kernels in a grid
In this section we show any radius 1 convolution kernel can be obtained as a linear combination of
the Bdχ(Vφi) and Bav(Vφi) matrices for the right choice of Laplacian eigenvectors φi. First We
show this can be done for 1-d convolution kernels.
Theorem C.2. On a path graph, any 1D convolution kernel of size 3 k is a linear combination of
the aggregators Bav, Bdx and the identity I.
Proof. Recall from the previous proof that the first non zero eigenvalue of the path graph PN has
associated eigenvector φι(i) = cos( Ni — 矗).Since this is a monotone decreasing function in i,
the i-th roW of Vφ1 Will be
(0,...,0,si-1,0,-si+1,0,...,0)
With si-1 and si+1 > 0. We are trying to solve
(aBav + bBdx + cId)i,: = (0,...,0,x,y,z,0,...,0)
With x, y, z, in positions i - 1, i and i + 1. This simplifies to solving
1
akSkLi
|s| + bi∏ι一
kskL2
s+ c(0, 1,0) = (x, y,z)
with S = (Si-1,0, -si+ι), which always has a solution because si-ι, Si+ι > 0.	□
Theorem C.3 (Generalization radius-1 convolutional kernel in a grid). Let Γ be the n-dimensional
lattice as above and let φj be the eigenvectors of the Laplacian of the lattice as in theorem C.1.
Then any radius 1 kernel k on Γ is a linear combination of the aggregators Bav (φi), Bdx (φi) and
I.
Proof. This is a direct consequence of C.2 obtained by adding n 1-dimensional kernels, with each
kernel being in a different axis of the grid as per Lemma C.1. See figure 4 for a visual example in
2D.
□
C.7 PROOF FOR THEOREM 2.7 (GENERALIZATION RADIUS-R CONVOLUTIONAL KERNEL IN
a lattice)
For an n-dimensional lattice, any convolutional kernel of radius R can be realized by a linear com-
bination of directional aggregation matrices and their compositions.
Proof. For clarity, we first do the 2 dimensional case for a radius 2, then extended to the general
case. Let k be the radius 2 kernel on a grid represented by the matrix
	0 0	0 a-i,-i	a-2,0 a-i,0	0 a-i,i	0 0
a5×5 =	a0,-2	a0,-i	a0,0	a0,i	a0,2
	0	ai,-i	ai,0	ai,i	0
	0	0	a2,0	0	0
since we supposed the N1 × N2 grid was such that N1 > N2, by theorem C.1, we have that φ1 is
depending only in the first variable x1 and is monotone in x1 . Recall from C.1 that
φ1(i)
The vector N∏1 V arccos(φι) will be denoted by Fi in the rest. Notice all entries of Fi are 0 or ±1.
Denote by F2 the gradient vector N2 V arccos(φk) where φk is the eigenvector given by theorem
C.1 that is depending only in the second variable x2 and is monotone in xi and recall
φk (i)
21
Under review as a conference paper at ICLR 2021
For a matrix B, let B± the positive/negative parts of B, ie matrices with positive entries such that
B = B+ - B - . Let Br1 be a matrix representing the radius 1 kernel with weights
0	a-1,0	0
a3×3	=	a0,-1	a0,0	a0,1
0	a1,0	0
The matrix Br1 can be obtained by theorem C.3. Then the radius 2 kernel k is defined by all the
possible combinations of 2 positive/negative steps, plus the initial radius-1 kernel.
Br2
X	ai,j(F1sgn(i))|i|(F2sgn(j))|j|
-2≤i,j≤2 |
|i|+|j|=2
_____________ - /
Any combination of 2 steps
Br1
|z}
all possible single-steps
+
with sgn the sign function sgn(i) = + if i ≥ 0 and - if i < 0. The matrix Br2 then realises the
kernel a5×5 .
We can further extend the above construction to N dimension grids and radius R kernels k
N
X	aV	Y (Fjsgn(vj))|vj|
V ={v1 ,v2 ,...,vN }∈Nn
I∣v∣∣li ≤R
-R≤vi≤R
'-------{^----------}
Any choice of walk V with at most R-steps
j=1
Aggregator following the steps defined in V
With Fj = N∏j V arccos φj ,φj the eigenvector with lowest eigenvalue only dependent on the j-th
variable and given in theorem C.1 and Q is the matrix multiplication. V represents all the choices of
walk {v1, v2, ..., vn} in the direction of the fields {F1, F2, ..., Fn}. For example, V = {3, 1, 0, -2}
has a radius R = 6, with 3 steps forward of F1, 1 step forward of F2, and 2 steps backward of F4.
□
C.8 Proof for Theorem 2.8 (Comparison with 1-WL test)
DGNs using the mean aggregator, any directional aggregator of the first eigenvector and injective
degree-scalers are strictly more powerful than the 1-WL test.
Proof. We will show that (1) DGNs are at least as powerful as the 1-WL test and (2) there is a pair
of graphs which are not distinguishable by the 1-WL test which DGNs can discriminate.
Since the DGNs include the mean aggregator combined with at least an injective degree-scaler,
Corso et al. (2020) show that the resulting architecture is at least as powerful as the 1-WL test.
22
Under review as a conference paper at ICLR 2021
Graph 1	Graph 2
Aggregation matrix	Graph 1	Graph 2
1a +	1b →	b	1a	+	1b → b
1a +	2b →	a	1a	+	2b → a
∣1α —	1b∣ → b	∣1a	—	1b∣ → b
0	→ a	10.44b	—	0.44a∣ → a
1a → b	1a → b
1b → a	0.44b + 0.56a → a
Figure 8: Illustration of an example pair of graphs which the 1-WL test cannot distinguish but
DGNs can. The table shows the node feature updates done at every layer. MPNN with mean/sum
aggregators and the 1-WL test only use the updates in the first row and therefore cannot distinguish
between the nodes in the two graphs. DGNs also use directional aggregators that, with the vector
field given by the first eigenvector of the Laplacian matrix, provides different updates to the nodes
in the two graphs.
Then, to show that the DGNs are strictly more powerful than the 1-WL test it suffices to provide an
example of a pair of graphs that DGNs can differentiate and 1-WL cannot. Such a pair of graphs is
illustrated in figure 8.
The 1-WL test (as any MPNN with, for example, sum aggregator) will always have the same features
for all the nodes labelled with a and for all the nodes labelled with b and, therefore, will classify
the graphs as isomorphic. DGNs, via the directional smoothing or directional derivative aggregators
based on the first eigenvector of the Laplacian matrix, will update the features of the a nodes dif-
ferently in the two graphs (figure 8 presents also the aggregation functions) and will, therefore, be
capable of distinguishing them.
□
23