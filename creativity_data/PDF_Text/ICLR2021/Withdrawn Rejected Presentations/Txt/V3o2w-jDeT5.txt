Under review as a conference paper at ICLR 2021
Multi-Source Unsupervised
Hyperparameter Optimization
Anonymous authors
Paper under double-blind review
Ab stract
How can we conduct efficient hyperparameter optimization fora completely new
task? In this work, we consider a novel setting, where we search for the optimal
hyperparameters for a target task of interest using only unlabeled target task and
‘somewhat relevant’ source task datasets. In this setting, it is essential to estimate
the ground-truth target task objective using only the available information. We
propose estimators to unbiasedly approximate the ground-truth with a desirable
variance property. Building on these estimators, we provide a general and tractable
hyperparameter optimization procedure for our setting. The experimental eval-
uations demonstrate that the proposed framework broadens the applications of
automated hyperparameter optimization.
1	Introduction
Hyperparameter optimization (HPO) has been a pivotal part of machine learning (ML) and contributed
to achieving a good performance in a wide range of tasks (Feurer & Hutter, 2019). It is widely
acknowledged that the performance of deep neural networks depends greatly on the configuration
of the hyperparameters (Dacrema et al., 2019; Henderson et al., 2018; Lucic et al., 2018). HPO is
formulated as a special case of a black-box function optimization problem, where the input is a set of
hyperparameters, and the output is a validation score. Among the black-box optimization methods,
adaptive algorithms, such as Bayesian optimization (BO) (Brochu et al., 2010; Shahriari et al., 2015;
Frazier, 2018) have shown superior empirical performance compared with traditional algorithms,
such as grid search or random search (Bergstra & Bengio, 2012).
One critical assumption in HPO is the availability of an accurate validation score. However, in
reality, there are many cases where we cannot access the ground-truth of the task of interest (referred
to as target task hereinafter). For example, in display advertising, predicting the effectiveness of each
advertisement, i.e., click-through rates (CTR), is important for showing relevant advertisements (ads)
to users. Therefore, it is necessary to conduct HPO before a new ad campaign starts. However, for
new ads that have not yet been displayed to users, one cannot use labeled data to conduct HPO. In
this case, the standard HPO procedure is infeasible, as one cannot utilize the labeled target task data
and the true validation score of the ML model under consideration.
In this work, we address the infeasibility issue of HPO when the labels of the target task are
unavailable. To formulate this situation, we introduce a novel HPO setting called multi-source
unsupervised hyperparameter optimization (MSU-HPO). In MSU-HPO, it is assumed that we do
not have the labeled data for a target task. However, we do have the data for some source tasks with
a different distribution from the target task. It is natural to assume that we have access to multiple
source tasks in most practical settings. In the display advertising example, several labeled datasets of
old ads that have already been deployed are often available, which we can use as labeled source task
datasets. To the best of our knowledge, no HPO approach that can address a situation without labeled
target task data exists despite its significance and possibility for applications.
A problem with MSU-HPO is that the ground-truth is inaccessible, and one cannot directly apply
the standard HPO procedure. Thus, it is essential to accurately approximate it using only available
data. For this purpose, we propose two estimators, enabling the evaluation of the ML models without
the labeled target task data. Our estimators are general and can be used in combination with any
common black-box optimization methods, such as Gaussian process-based BO (Srinivas et al., 2010;
Snoek et al., 2012; Hennig & Schuler, 2012; Contal et al., 2014; Herngndez-Lobato et al., 2014;
1
Under review as a conference paper at ICLR 2021
Wang & Jegelka, 2017) and the tree-structured Parzen estimator (Bergstra et al., 2011; 2013). In
addition, we show that the proposed estimators can unbiasedly approximate the target task objective,
one of which achieves a desirable variance property by selecting useful source tasks based on a task
divergence measure. We also present a general and computationally inexpensive HPO procedure for
MSU-HPO building on our estimators. Finally, we demonstrate that our estimators work properly
through numerical experiments with synthetic and real-world datasets.
Related Work. A typical HPO setting is to find a better set of hyperparameters using a labeled
target task of interest. As faster convergence is an essential performance metric of the HPO methods,
the research community is moving on to the multi-source or transfer settings for which there are
some previously solved related source tasks. By combining the additional source task information
and the labeled target task dataset, it has been shown that one can improve the hyperparameter search
efficiency, and thus reach a better solution with fewer evaluations (Bonilla et al., 2008; Bardenet et al.,
2013; Swersky et al., 2013; Yogatama & Mann, 2014; Ramachandran et al., 2018; Springenberg
et al., 2016; Poloczek et al., 2017; Wistuba et al., 2018; Feurer et al., 2018; Perrone et al., 2018;
2019; Salinas et al., 2019). A critical difference between the multi-source HPOs and our MSU-HPO
settings is the existence of labels for the target task. Previous studies usually assume that analysts
can utilize labeled target data. However, as discussed above, this is often unavailable, and thus, most
of these methods are infeasible.
One possible solution to address the unavailablity of labeled target data is to use warm starting
methods (Vanschoren, 2019), which aims to find good initial hyperparameters for the target task.
Learning Initialization (LI) finds promising hyperparameters by minimizing a sum of a loss function
surrogated by a Gaussian process on each source task (Wistuba et al., 2015). While LI is effective
when the source and target tasks are quite similar, it is hard to achieve a reasonable performance
otherwise. In contrast, DistBO learns the similarity between the source and target tasks with a joint
Gaussian process model on hyperparameters and data representations (Law et al., 2019). However,
many transfer methods including DistBO need abundant hyperparameter evaluations for the source
tasks to surrogate objective function for each task well, which will be confirmed in our experiments.
Another related field is model evaluation in covariate shift, whose objective is to evaluate the
performance of the ML models of the target task using only a relevant single source dataset (Sugiyama
et al., 2007; You et al., 2019; Zhong et al., 2010). These studies build on the importance sampling
(IS) method (Elvira et al., 2015; Sugiyama et al., 2007) to obtain an unbiased estimate of ground-truth
model performances. While our proposed methods are also based on IS, a major difference is that
we assume that there are multiple source datasets with different distributions. We demonstrate that
with the multi-source setting, the previous IS method can fail, and propose an estimator satisfying
the optimal variance property. Moreover, as these methods are specific to model evaluation, the
connection between the IS-based estimation techniques and the automated HPO methods has not yet
been explored despite their possible, broad applications. Consequently, we are the first to empirically
evaluate the possible combination of the IS-based unbiased estimation and adaptive HPO.
Contributions. The contributions of this work can be summarized as follows: (i): We formulate a
novel and highly practical HPO setting, MSU-HPO. (ii): We propose two unbiased estimators for the
ground-truth validation score calculable with the available data. Additionally, we demonstrate that one
of them achieves optimal finite variance among a reasonable class of unbiased estimators. (iii): We
describe a flexible and computationally tractable HPO procedure building on the proposed estimators.
(iv): We empirically demonstrate that the proposed procedure works favorably in MSU-HPO setting.
Furthermore, our empirical results suggest a new possible connection between the adaptive HPO and
IS-based unbiased estimation techniques.
2	Problem Setting
In this section, we formulate MSU-HPO. Let X ⊆ Rd be the d-dimensional input space and Y ⊆ R
be the real-valued output space. We use pT (x, y) to denote the joint probability density function of
the input and output variables X ∈ X and Y ∈ Y of the target task. The objective of this work is to
find the best set of hyperparameters θ with respect to the target distribution:
θopt = arg min fT (θ)	(1)
θ∈Θ
2
Under review as a conference paper at ICLR 2021
where Θ is a pre-defined hyperparameter search space and fT (θ) is the target task objective, which is
defined as the generalization error over the target distribution:
fτ(θ)= E(X,Y )~Pτ[L(hθ (X ),Y)]	(2)
where L : Y × Y → R≥0 is a bounded loss function such as the zero-one loss. hθ : X → Y is an
arbitrary machine learning model that predicts the output values using the input vectors with a set of
hyperparameters θ ∈ Θ.
In a standard hyperparameter optimization setting (Bergstra et al., 2011; Feurer & Hutter, 2019;
Snoek et al., 2012), labeled i.i.d. validation samples {xi, yi}rnTι 〜PT are available, and one can
easily estimate the target objective in Eq. (2) by the following empirical mean:
nT
fτ (θ; DTabeled) = — X L(hθ (xi),yi)	⑶
T	nT i=1
where DTlabeled is any size nT of the i.i.d. labeled samples from the target task distribution. Then, a
hyperparameter optimization is conducted directly using the estimated target function in Eq. (3) as a
reasonable replacement for the ground-truth target objective fT(θ) in Eq. (2).
In contrast, under the MSU-HPO setting, labels of the target task are assumed to be unobservable; we
can use only unlabeled target validation samples denoted as DT = {xi}in=T1 hereinafter. Instead, we
assume the availability of the multiple source task datasets which is denoted as {DSj }jN=S1 where j is
a source task index and NS denotes the number of source tasks. Each source task data is defined as
the i.i.d. labeled samples: DSj = {xj ,yj }nSj 〜PSj where PSj (x, y) is ajoint probability density
function that characterizes the source task j . Note here that marginal input distributions of the target
and source tasks are different, i.e., PT (x) 6= PSj (x), ∀j ∈ {1, . . . , NS}.
Regarding the target and source distributions, we make the following assumptions.
Assumption 1. Source tasks have support for the target task, i.e., PT (x) > 0 ⇒ PSj (x) > 0, ∀x ∈
X,∀j ∈ {1,...,NS}.
Assumption 2. Conditional output distributions remain the same between the target and all of the
source tasks, i.e., PT (y|x) = PSj (y|x), ∀j ∈ {1, . . . , NS}.
The above assumptions are common in the covariate shift literature Shimodaira (2000) and suggest
that the input-output relation is the same, but the input distributions are different for the target and
source task distributions. 1
One critical difficulty of the MSU-HPO setting is that the simple approximation using the empirical
mean is infeasible, as the labeled target dataset is unavailable. It is thus essential to accurately
estimate the target task objective using only an unlabeled target dataset and labeled multiple source
datasets.
3	Method
In this section, we propose estimators to approximate the target task objective by applying an
importance weighting technique.
3.1	Unbiased Objective Estimator
A natural first candidate method to approximate the target task objective function is to use importance
weighting (Shimodaira, 2000). To define our estimator, we first introduce the density ratio between
the target task distribution and the source task distribution below.
Definiton 1. (Density Ratio) For any (x, y) ∈ X × Y with a positive source density PSj (x, y) > 0,
the density ratio between the target and a source task distributions is
0 ≤ WSj(X, y) = PT(x,y)) = PT，) = WSj(X) ≤ C	(4)
PSj (x, y)	PSj (x)
where C is a positive constant. The equalities are derived from Assumption 2.
1These assumptions seem to be strict, but in fact, they are relatively reasonable given that the general HPO
literature implicitly assume that the train-test distributions are the same.
3
Under review as a conference paper at ICLR 2021
Using the density ratio, we define an estimator for the target task objective function.
Definiton 2. (Unbiased Estimator) For a given set of hyperparameter θ ∈ Θ, the unbiased estimator
for the target task objective function is defined as
1 NS nSj
fUB (θ; {DSj }N=Sl) = n∑SΣwSj (xi ) ∙ L(hθ (xi ),yj)	⑸
nj=1 i=1
where UB stands for unbiased, n = PjN=S1 nSj is the total sample size of the source tasks, DSj is any
sample size ns of the i.i.d. samples from the distribution of source task j, and WSj (∙) is the true
density ratio function.
The estimator in Eq. (5) is an application of the importance weighted cross-validation (Sugiyama
et al., 2007) to the multiple-source task setting and can easily be shown to be statistically unbiased for
the ground-truth target task objective function, i.e., for any given θ, E[fuB ^θ; {Dsj }N=Sι^ ] = fτ(θ).
We also characterize the variance of the unbiased estimator.
1 NS
V fuB (θXDsj}N⅛)) = n X nsj(E(X,Y)〜p,j [wS j (X) ∙ L2(hθ (X ),Y)] — (fτ(θ))2)
j=1	(6)
As stated above, the unbiased estimator is a valid approach for approximating a target task objective
because of its unbiasedness. The problem is that its variance depends on the square value of the
density ratio function, which can be huge when there is a source task with a distribution that is
dissimilar to that of the target task.
To illustrate this variance problem, we use a toy example where {x1, x2} ⊆ X, {y1, y2} ⊆
Y, p(y1|x1) = p(y2 |x2) = 1, p(y2 |x1) = p(y1|x2) = 0. The loss values for possible tuples,
and the probability densities of the target and two source tasks are presented in Table 1. It shows that
the target task T is similar to the source task S2, but its distribution is significantly different from that
of S1. For simplicity and without loss of generality, suppose there are two source task datasets such
as D1 = {(x11, y11)} and D2 = {(x12, y12)}. Then from Eq. (6), the variance of the unbiased estimator
is about 64.27. Intuitively, this large variance is a result of the large variance samples from S1. In
fact, by dropping the samples of S1 reduces the variance to 4.27. From this example, we know that
the unbiased estimator fails to make the most of the source tasks, and there is room to improve its
variance by down-weighting the source tasks dissimilar to the target task.
Table 1: Dropping data samples from S1 significantly lowers the variance of the unbiased estimator
__________________________________________________________(x1,y1)	(x2,y2)
loss function: L(hθ(x), y)	10	1
target task (T) distribution: pT (x, y)	0.8	0.2
source task (S1) distribution: pS1 (x, y)	0.2	0.8
source task (S2) distribution: pS2 (x, y)	0.9	0.1
3.2 Variance Reduced Objective Estimator
As illustrated with the toy example, an unbiased estimator can be unstable when there are some
source tasks with a distribution significantly different from that of the target task. To address this
variance issue, we define a divergence measure between the two tasks below.
Definiton 3. (Task Divergence Measure) The divergence between a source task distribution pSj
where j ∈ {1, . . . NS} and the target task distribution pT is defined as
Div(T || S j = E(χ,γ 卜 pSj [wS j (X) ∙ L2(hθ (X ),Y)] -(fτ(θ))2	⑺
This task divergence measure is large when the corresponding source distribution deviates significantly
from the target task distribution. Building on this measure, we define the following estimator for the
target task objective.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Hyperparameter optimization procedure under the MSU-HPO setting
Input: unlabeled target task dataset DT = {xi}in=T1 ; labeled source task datasets {DSj =
{xij, yij}in=S1j }jN=S1; hyperparameter search space Θ; a machine learning model hθ; a target task
objective estimator f, a hyperparameter optimization algorithm OPT
1:	for j ∈ {1, . . . , NS} do
2:	Split DSj into three folds DSdejnsity , DStrjain, and DSvaj l
3:	Estimate density ratio WSj (∙) by ULSIF with DT and DSnslItty
4:	end for
5:	Optimize the hyperparameter θ ∈ Θ of hθ with OPT by setting f(θ; {DSl}N=ι) as its objective
6:	(the model parameter of hθ is obtained by optimizing f(θ; {DtS'jain}j=ι))
7:	return hθ? (where θ? is the output of OPT)
Definiton 4. (Variance Reduced Estimator) For a given set of hyperparameters θ ∈ Θ, the variance
reduced estimator for the target task objective function is defined as
NS	nSj
fvR (θ; {Dsj}%ι) = Xλ? X W(Xj) ∙ L(hθ(Xj),yj)	⑻
j=1	i=1
where VR stands for variance reduced, DSj is any sample size nSj of the i.i.d. samples from
the distribution of source task j, and WSj (∙) is the true density ratio function. λ? is a weight for
source task j, which is defined as λ? = (Div(T || Sj) PNSI DivnSjI Sj))	. Note that, for all
j ∈ {1, . . . NS}, λj? ≥ 0 and PjN=S1 λj? nSj = 1.
The variance reduced estimator in Eq. (8) is also statistically unbiased for the ground-truth target task
objective in Eq.(2), i.e., for any given θ, E[fvR (θ; {Dsj }NSι)] = fτ(θ)
Then, we demonstrate that the variance reduced estimator in Eq. (8) is optimal in the sense that any
other convex combination of a set of weights λ = {λ1, . . . λNS} that satisfies the unbiasedness for
the target task objective function does not provide a smaller variance.
Theorem 1. (Variance Optimality; Extension of Theorem 6.4 of (Agarwal et al., 2017)) For any given
set of weights λ = {λ1, . . . λNS} that satisfies λj ≥ 0 and PjN=S1 λj nSj = 1 for allj ∈ {1, . . . NS},
the following inequality holds
V (fVR (θ; {DSj}Nι)) = (X Div n‰j) j	≤ V (fλ (θ; *sj}Nι))
where fλ(θ; {DSj }NNSι) = PN=I λj PnSj W(Xj) ∙ L(hθ(Xj),yj). SeeAppendixA for the proof.
Theorem 1 suggests that the variance reduced estimator achieves a desirable finite sample variance
property by weighting each source task based on its divergence to the target task.
Let us now return to the toy example in Table 1. The values of the divergence measure for S 1 and S2
are 252.81 and 4.27, respectively. This leads to the weights of λ? ≈ 0.017 and λ? ≈ 0.983. Then,
the variance of the variance reduced estimator is equal to 4.21 < 4.27 (variance when S1 is dropped.).
It is obvious that the variance reduced estimator performs better than the unbiased estimator does by
optimally weighting all available source tasks.
3.3 Hyperparameter Optimization Procedure
We describe several detailed components of the HPO procedure in the MSU-HPO setting.
Density Ratio Estimation: In general, density ratio functions between the target and source tasks
are unavailable and thus should be estimated beforehand. To estimate this parameter, we employ the
5
Under review as a conference paper at ICLR 2021
unconstrained Least-Squares Importance Fitting (uLSIF) procedure Kanamori et al. (2009); Yamada
et al. (2011), which suggests directly minimizing the following squared error for the true density ratio
function:
S = arg min EpSjh(W(X) - s(X))2] = arg min 1 EpSJS2(X)] - EpT [s(X)]	⑼
s∈S	s∈S 2
where S is a class of measurable functions. It should be noted that the empirical version of Eq. (9) is
calculable with unlabeled target and source task datasets.
Task Divergence Estimation: To utilize the variance reduced estimator, the task divergence
measure Div T || Sj in Eq. (7) needs to be estimated from the available data. This can be done
using the following empirical mean. 2
How to train hθ?: To evaluate the validation score of θ ∈ Θ, the model parameters of hθ should
be optimized by the supervised learning procedure. However, in the MSU-HPO setting, the labeled
target task dataset is unavailable, and direct training of hθ is infeasible. Therefore, we suggest
splitting the labeled source task datasets {DSj } into the training {DStrjain} and validation {DSvajl}
sets. Then, We can train hθ using the training set by hθ = arg minhθ∈h f(θ∙,{Dttr黑n}N=1). where
f is an estimator for the target task objective function such as the unbiased and variance reduced
estimators, and Hθ is a hypothesis space defined by a set of hyperparameters θ ∈ Θ. This training
procedure enables us to obtain the model parameters of hθ as if it were trained on the labeled target
task dataset. In addition, it is sufficient to train hθ only once to evaluate θ ∈ Θ; the proposed
procedure is computationally inexpensive.
Overall Procedure: Building on the above details, Algorithm 1 summarizes the high-level hyper-
parameter optimization procedure under the MSU-HPO setting3. We also provide the regret bound of
our HPO procedure in the MSU-HPO setting in Appendix C.
4 Experiments
We investigate the behavior of our proposed HPO procedure in MSU-HPO using a synthetic problem
in Section 4.1 and real-world datasets in Section 4.2. We compare the following methods as possible
baselines 4: (i) Learning Initialization (LI) (Wistuba et al., 2015), (ii) DistBO (Law et al., 2019),
(iii): Naive method, which uses the performance on the concatenation of source tasks as a validation
score, (iv) Oracle method, which uses the labeled target task for HPO. Thus, the oracle method is
infeasible in MSU-HPO, and we regard the performance of the oracle method as an upper bound to
which other methods can reach. In all the experiments, we set the number of evaluations B = 50 and
use GP-UCB (Srinivas et al., 2010) as a hyperparameter optimization algorithm5.
4.1	Toy Problem
We consider a 1-dimensional regression problem with the MSU-HPO setting. The generative process
of the toy dataset is as follows:
μi 〜U(-Ci, Ci), {xi}n=ι I μi i" N(μi, 1), {蜡}却 | {喇① i" {N(0.7xi + 0.3,1)}∏=ι,
where U is the uniform distribution, N denotes the normal distribution, and ci ∈ R is a prior
parameter that characterizes the marginal input distribution (p(x)) of task i. The objective function f
is given by:
1n
f(θ;Di) = - EL(θ,yι), L(θ,yι) = (θ - yι)2∕2.
n l=1
(10)
2Ddv(T||SD = n1-PnSj(W(Xj) ∙ l&(χj),yj))2 - (n1-PnSj W(Xj) ∙ l&(χj),yj))2
3We describe the specific hyperparameter optimization procedure when BO is used as OPT in Appendix B.
4We describe the details of the baseline methods in Appendix D.
5We describe detailed settings in our experiments in Appendix E.
6
Under review as a conference paper at ICLR 2021
2.0
2.0	3.0	4.0	5.0
source C bound
(a) Comparing all methods
pəɔnpə- 90ueze> 一 P9s-qun
1.0	2.0	3.0	4.0	5.0
source C bound
(b) Comapring unbiased and variance reduced
Figure 1: Results of the experiment on synthetic toy problems over 30 runs.
Note: The horizontal axis represents the prior parameters of the source tasks CS ∈ {1.0, •一,5.0}, (i ∈
{1,… ,Ns}). (a) The vertical axis represents the mean and standard error of the evaluated loss for each
estimator. (b) The vertical axis represents the ratio of the mean loss of the unbiased estimator to the variance
reduced estimator.
Similar to the toy experiment in (Law et al., 2019), θ ∈ [-8, 8] is a hypothetical ‘hyperparameter’ we
would like to optimize. The optimal solution for this experiment is thus θ = n-1 Pln=1 yl.
We described in Section 3 that when p(x) of the source task and the target task differs significantly,
the performance of the variance reduced estimator is better than that of the unbiased estimator. To
demonstrate this, We set Ci separately for the source (CS ∈ {1.0,2.0, ∙∙∙ , 5.0}, i ∈ {1, ∙∙∙ , NS})
and the target tasks (cT = 1.0). That is, the source and target distributions are similar when
CiS = 1.0(= CT); in contrast, the source and target distributions are quite different When CiS = 5.0.
Finally, We set NS = 2 and n = 1000.
Figure 1 shoWs the results of the experiment on the toy problem over 30 runs With different random
seeds. First, Figure 1 (a) indicates that the proposed unbiased and variance reduced estimators
significantly outperform the naive method and LI in all settings. This is because our estimators
can unbiasedly approximate the target task objective by considering the distributional shift, While
the naive method and LI cannot. Moreover, this figure shoWs the advantage of unbiasedness is
highlighted When the distributions of the target and source tasks diverge largely (i.e., When CiS is
large.). DistBO also shoWs relatively good performance despite the lack of unbiasedness. Next, We
compare the performance of the unbiased and variance reduced estimator in Figure 1 (b). This reports
the performance of the unbiased estimator relative to the variance reduced one With varying values of
C. The result indicates that the advantages of using the variance reduced estimator over the unbiased
one are further strengthened When there is a large divergence betWeen the target and source task
distributions, Which is consistent With our theoretical analysis. Finally, as shoWn in Figure 1 (a), the
variance reduced estimator achieves almost the same performance as the upper bound Without using
the labels of the target task, suggesting its poWerful HPO performance on an unlabeled target task.
4.2	Hyperparameter Optimization on Real-World Datasets
Datasets: We use Parkinson’s telemonitoring (Parkinson) (Tsanas et al., 2009) and Graft-versus-
host disease (GvHD) datasets (Brinkman et al., 2007) to evaluate our methods on real-World problems.
Parkinson data consists of voice measurements of 42 patients With the early-stage Parkinson disease
collected by using a telemonitoring device in remote symptom progression monitoring. Each patient
has about 150 recordings characterized by a feature vector With 17 dimensions. The goal is to predict
the Parkinson disease symptom score for each recording from the recordings.
GvHD is an important medical problem in the allogeneic blood transplantation field (Brinkman
et al., 2007). The issue occurs in allogeneic hematopoietic stem cell transplant recipients When
donor-immune cells in the graft recognize the recipient as foreign and initiate an attack on several
tissues. The GvHD dataset contains Weekly peripheral blood samples obtained from 31 patients
characterized by a feature vector With 7 dimensions. FolloWing (Muandet et al., 2013), We omit one
patient Who has insufficient data, and subsample data of each patient to have 1000 data points each.
7
Under review as a conference paper at ICLR 2021
The goal is to classify CD3+CD4+CD8+ cells, which have a high correlation with the development
of the disease (Brinkman et al., 2007).
Experimental Procedure: To create the MSU-HPO setting, for both datasets, we treat each
patient as a task. We select one patient as a target task and regard the remaining patients as multiple
source tasks. Then, we use the following experimental procedure: (1) Tune hyperparameters of an
ML model by an HPO method using the unlabeled target task and labeled source tasks, (2) Split the
original target task data into 70% training set and 30% test set, (3) Train an ML model tuned by an
MSU-HPO method using the training set of the target task, (4) Predict target variables (symptom
scores for Parkinson and CD3+CD4+CD8+ cells for GvHD) on the test set of the target patient, (5)
Calculate target task objective of the prediction and regard it as the performance of the MSU-HPO
method under consideration, (6) Repeat the above steps 10 times with different seeds and report the
mean and standard error over the simulations.
As for an ML model and a target task objective, we use support vector machine (SVM) implemented
in scikit-learn (Pedregosa et al., 2011) and mean absolute error (MAE) for Parkinson. In contrast,
we use LightGBM (Ke et al., 2017) as an ML model and binary cross-entropy (BCE) as a target task
objective for GvHD.
Table 2: Comparing different MSU-HPO methods (Mean ±StdErr)
Estimators	Parkinson (MAE)	GvHD (BCE)
LI	0.41507 ±0.1669	0.19695 ±0.0468
DistBO	1.54202 ±0.1006	0.33015 ±0.0600
Naive	1.10334 ±0.0908	0.02121 ±0.0052
Unbiased (ours)	1.08283 ±0.1981	0.02141 ±0.0052
Variance reduced (ours)	0.40455 ±0.1755	0.01791 ±0.0039
Oracle (reference)	0.06862 ±0.0011	0.01584 ±0.0043
Note: The red fonts represent the best performance among estimators using only the unlabeled target task and
labeled source task datasets. The mean and standard error (StdErr) are induced by running 10 simulations with
different random seeds.
Results: Table 2 presents the results of the experiments over 10 runs with different random seeds.
In contrast to the results in Section 4.1, the performance of DistBO has deteriorated significantly.
While DistBO requires a reasonable number of hyperparameter evaluations per source task, our setting
allows only a very small number of evaluations per source task, which may lead to learning inaccurate
surrogate models6. The unbiased estimator performs almost the same with naive in Parkinson given
their standard errors. Moreover, it slightly underperforms the naive in GvHD, although the unbiased
estimator satisfies the unbiasedness. This is because the number of data for each task is small, and
the variance issue of the unbiased estimator is highlighted in these data. Therefore, pursuing only
unbiasedness in the approximation of the target task objective is not sufficient in MSU-HPO. On the
other hand, the variance reduced estimator alleviates the instability issue of the unbiased estimator
and performs best in both datasets. The results also suggest that the variance reduced estimator works
well on both regression (Parkinson) and classification (GvHD) tasks. Therefore, we conclude from
its variance optimality and empirical performance that using the variance reduced estimator is the
best choice for MSU-HPO.
5 Conclusion
We studied a novel problem setting, MSU-HPO, with the goal of enabling effective HPO with only
an unlabeled target task and multiple labeled source task datasets. We proposed two estimators to
approximate the target task objective from available data. Empirical evaluations demonstrated that
the proposed HPO procedure helps to determine useful hyperparameters without the labels of the
target task.
6A more detailed discussion is provided in Appendix D.3.
8
Under review as a conference paper at ICLR 2021
References
Aman Agarwal, Soumya Basu, Tobias Schnabel, and Thorsten Joachims. Effective evaluation
using logged bandit feedback from multiple loggers. In Proceedings of the 23rd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 687-696, 2017.
Remi Bardenet, Mgtygs Brendel, Baldzs KegL and Michele Sebag. Collaborative hyperparameter
tuning. In International conference on machine learning, pp. 199-207, 2013.
James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of
machine learning research, 13(Feb):281-305, 2012.
James Bergstra, Daniel Yamins, and David Daniel Cox. Making a science of model search: Hyperpa-
rameter optimization in hundreds of dimensions for vision architectures. 2013.
James S Bergstra, Remi Bardenet, Yoshua Bengio, and Balgzs Kegl. Algorithms for Hyper-Parameter
Optimization. In Advances in neural information processing systems, pp. 2546-2554, 2011.
Ilija Bogunovic, Jonathan Scarlett, Stefanie Jegelka, and Volkan Cevher. Adversarially robust
optimization with gaussian processes. In Advances in neural information processing systems, pp.
5760-5770, 2018.
Edwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task Gaussian Process Prediction.
In Advances in neural information processing systems, pp. 153-160, 2008.
Ryan Remy Brinkman, Maura Gasparetto, Shang-Jung Jessica Lee, Albert J Ribickas, Janelle Perkins,
William Janssen, Renee Smiley, and Clay Smith. High-content flow cytometry and temporal data
analysis for defining a cellular signature of graft-versus-host disease. Biology of Blood and Marrow
Transplantation, 13(6):691-700, 2007.
Eric Brochu, Vlad M Cora, and Nando De Freitas. A tutorial on bayesian optimization of expensive
cost functions, with application to active user modeling and hierarchical reinforcement learning.
arXiv preprint arXiv:1012.2599, 2010.
Emile Contal, Vianney Perchet, and Nicolas Vayatis. Gaussian process optimization with mutual
information. In International Conference on Machine Learning, pp. 253-261, 2014.
Maurizio Ferrari Dacrema, Paolo Cremonesi, and Dietmar Jannach. Are we really making much
progress? a worrying analysis of recent neural recommendation approaches. In Proceedings of the
13th ACM Conference on Recommender Systems, pp. 101-109, 2019.
Victor Elvira, Luca Martino, David Luengo, and M6nica F Bugallo. Efficient multiple importance
sampling estimators. IEEE Signal Processing Letters, 22(10):1757-1761, 2015.
Matthias Feurer and Frank Hutter. Hyperparameter Optimization. In Automated Machine Learning,
pp. 3-33. 2019.
Matthias Feurer, Benjamin Letham, and Eytan Bakshy. Scalable Meta-Learning for Bayesian
Optimization using Ranking-Weighted Gaussian Process Ensembles. In AutoML Workshop at
ICML, 2018.
Peter I Frazier. A tutorial on bayesian optimization. arXiv preprint arXiv:1807.02811, 2018.
Javier Gonzglez, Zhenwen Dai, Philipp Hennig, and Neil Lawrence. Batch bayesian optimization via
local penalization. In Artificial intelligence and statistics, pp. 648-657, 2016.
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger.
Deep Reinforcement Learning that Matters. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018.
Philipp Hennig and Christian J Schuler. Entropy search for information-efficient global optimization.
Journal of Machine Learning Research, 13(Jun):1809-1837, 2012.
9
Under review as a conference paper at ICLR 2021
Jose MigUel Herndndez-Lobato, Matthew W Hoffman, and ZoUbin Ghahramani. Predictive entropy
search for efficient global optimization of black-box functions. In Advances in neural information
processing Systems,pp. 918-926, 2014.
TakafUmi Kanamori, Shohei Hido, and Masashi SUgiyama. A least-sqUares approach to direct
importance estimation. Journal of Machine Learning Research, 10(JUl):1391-1445, 2009.
GUolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei Ye, and Tie-
Yan LiU. Lightgbm: A highly efficient gradient boosting decision tree. In Advances in neural
information processing systems, pp. 3146-3154, 2017.
Ho ChUng Law, Peilin Zhao, LeUng Sing Chan, JUnzhoU HUang, and Dino Sejdinovic. Hyperparameter
learning via distribUtional transfer. In Advances in Neural Information Processing Systems, pp.
6801-6812, 2019.
Mario LUcic, Karol KUrach, Marcin Michalski, Sylvain Gelly, and Olivier BoUsqUet. Are Gans
Created EqUal? A Large-Scale StUdy. In Advances in neural information processing systems, pp.
700-709, 2018.
Krikamol MuandeL David Balduzzi, and Bernhard Scholkopf. Domain generalization via invariant
featUre representation. In International Conference on Machine Learning, pp. 10-18, 2013.
Vu Nguyen, Santu Rana, Sunil K Gupta, Cheng Li, and Svetha Venkatesh. Budgeted batch bayesian
optimization. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pp. 1107-
1112. IEEE, 2016.
Fabian Pedregosa, Gael Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machine learning in python. the Journal of machine Learning research, 12:2825-2830, 2011.
Valerio Perrone, Rodolphe Jenatton, Matthias W Seeger, and Cedric Archambeau. Scalable Hyperpa-
rameter Transfer Learning. In Advances in Neural Information Processing Systems, pp. 6845-6855,
2018.
Valerio Perrone, Huibin Shen, Matthias W Seeger, Cedric Archambeau, and Rodolphe Jenatton.
Learning search spaces for bayesian optimization: Another view of hyperparameter transfer
learning. In Advances in Neural Information Processing Systems, pp. 12751-12761, 2019.
Matthias Poloczek, Jialei Wang, and Peter Frazier. Multi-information source optimization. In
Advances in Neural Information Processing Systems, pp. 4288-4298, 2017.
Anil Ramachandran, Sunil Gupta, Santu Rana, and Svetha Venkatesh. Information-theoretic Transfer
Learning framework for Bayesian Optimisation. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pp. 827-842, 2018.
David Salinas, Huibin Shen, and Valerio Perrone. A copula approach for hyperparameter transfer
learning. arXiv preprint arXiv:1909.13595, 2019.
Bobak Shahriari, Kevin Swersky, Ziyu Wang, Ryan P Adams, and Nando De Freitas. Taking the
human out of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):
148-175, 2015.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization of machine
learning algorithms. In Advances in neural information processing systems, pp. 2951-2959, 2012.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian Optimization
with Robust Bayesian Neural Networks. In Advances in Neural Information Processing Systems,
pp. 4134-4142, 2016.
Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process opti-
mization in the bandit setting: no regret and experimental design. In Proceedings of the 27th
International Conference on International Conference on Machine Learning, pp. 1015-1022, 2010.
10
Under review as a conference paper at ICLR 2021
Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert MAzller. Covariate shift adaptation by
importance weighted cross validation. Journal ofMachine Learning Research, 8(May):985-1005,
2007.
Kevin Swersky, Jasper Snoek, and Ryan P Adams. Multi-Task Bayesian Optimization. In Advances
in neural information processing systems, pp. 2004-2012, 2013.
Athanasios Tsanas, Max A Little, Patrick E McSharry, and Lorraine O Ramig. Accurate telemon-
itoring of parkinson’s disease progression by noninvasive speech tests. IEEE transactions on
Biomedical Engineering, 57(4):884-893, 2009.
Joaquin Vanschoren. Meta-Learning. In Automated Machine Learning, pp. 35-61. 2019.
Zi Wang and Stefanie Jegelka. Max-value entropy search for efficient bayesian optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3627-3635.
JMLR. org, 2017.
Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Learning hyperparameter optimization
initializations. In 2015 IEEE international conference on data science and advanced analytics
(DSAA), pp. 1-10. IEEE, 2015.
Martin Wistuba, Nicolas Schilling, and Lars Schmidt-Thieme. Scalable gaussian process-based
transfer surrogates for hyperparameter optimization. Machine Learning, 107(1):43-78, 2018.
Makoto Yamada, Taiji Suzuki, Takafumi Kanamori, Hirotaka Hachiya, and Masashi Sugiyama.
Relative density-ratio estimation for robust distribution comparison. In Advances in neural
information processing systems, pp. 594-602, 2011.
Dani Yogatama and Gideon Mann. Efficient transfer learning method for automatic hyperparameter
tuning. In Artificial intelligence and statistics, pp. 1077-1085, 2014.
Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selection
in deep unsupervised domain adaptation. In International Conference on Machine Learning, pp.
7124-7133, 2019.
Erheng Zhong, Wei Fan, Qiang Yang, Olivier Verscheure, and Jiangtao Ren. Cross validation
framework to choose amongst models and datasets for transfer learning. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases, pp. 547-562. Springer,
2010.
11
Under review as a conference paper at ICLR 2021
A	Omited Proofs
A. 1 Derivation of Unbiasedness
We first define a general class of unbiased estimators called λ-unbiased estimator that includes the
unbiased and variance reduced estimators as special cases.
Definiton 5. (λ-unbiased Estimator) When a set of weights λ = {λ1, . . . λNS} that satisfies λj ≥ 0
and PjN=S1 λj nSj = 1 for allj ∈ {1, . . . NS} is given, the λ-unbiased estimator for the target task
objective function is
NS	nSj
fλ (θ; {DsjjSι) = £% EwSj (xi) ∙ L(hθ(xi),yj).	1 (II) * * V
j=1	i=1
When λi = nSj /N, it is the unbiased estimator in Eq. (5). In contrast, it is the variance reduced
estimator in Eq. (8) when λi = λi?
Then we show that the λ-unbiased estimator is statistically unbiased for the target task function.
Proof. By the linearity of the expectation operator,
NS	nSj
E fλ (θ; {Dsj }N≥ι) ] = X λj X E(χ,γ)〜pSj [wsj (X) ∙L(hθ (X ),Y)]
i =1	i=1
NS	nSj
=X λi X E(X,Y卜。Sj ；T((XY)) ∙ L(hθ (X),Y)
i =1	i=1	S ,
NS	nSj
=X λj X E(X,Y)〜PT [L(hθ(X),Y)]
i =1	i=1
NS	nSj
= X λi X fT(θ)
i =1	i=1
=(X λins) ∙ fτ(θ)
= fT(θ )
Thus, the unbiased estimator in Eq. (5) and the variance reduced estimator in Eq. (8) are both
statistically unbiased for the ground truth target task objective function in Eq. (2).	□
A.2 Derivation of Eq. (6)
Proof. The variance can be represented as follows because samples are independent
1 NS nSj
V fuB (θXDsj%)) =n2空V(wSj(X) ∙ L(hθ(X),Y))
1 NS
=/ X nsj ∙ V (wsj (X) ∙ L(hθ(X),Y))
n i=1
V (wsj (X) ∙ L(hθ(X), Y)) is decomposed as
V (wSj (X) ∙ L(hθ (X ),Y ))= E(X,Y)〜PSj [ws j (X) ∙ L2(hθ (X ),Y)] -(E(x,Y)〜ps, [ws。(X) ∙ L(h (X ),Y )])2
From the unbiasedness property, E(χ,γ)〜PS j [wsj (X) ∙ L(hθ (X), Y)] = fτ (θ). Then, we now have
V (wSj (X) ∙ L(hθ (X ),Y)) = E(X,Y)〜PSj [wS j (X) ∙ L2(hθ(X ),Y)] - (fτ(θ))2
□
12
Under review as a conference paper at ICLR 2021
A.3 Proof of Theorem 1
By following the same logic flow as in Section A.2, the variance of the λ-unbiased estimator in Eq.
(11) is
NS
V (fλ (θ; {Dsj }%ι)) = X λjnsj (E(X,Y)〜pSj 解 , (X) ∙ L2(hθ (X ),Y)] - (fτ(θ))2
j=1
NS
=X λ2nSj ∙ Div(T 11 Sj)
j=1
(12)
Thus, by replacing λj for (Div(T || Sj) PN=I DivnSI Sj)) ，We have
NS	NS
V (fλ (θ; {DSj 哨))=X (X D-jj) 1 nSj ∙ Div(T || S j
X	nsj Div(T|| S j______
⅛ (Div (T|| Sj))2(PN1 Div(TSjI Sj) )2
nSj
Div (T || Sj)
NS
X
j=1
nSj
Div(T||Sj)
NS
X
j=1
nSj
Div (T|| Sj)
-1
Moreover, for any set of Weights λ ={λ1 , . . . λNS }, We obtain the folloWing variance optimality
using the Cauchy-SchWarz inequality.
(NS	∖	(NS	∖	/ NS	∖ 2
X 净 Sj Div (T∣∣S j))	(X Dij))	≥ (X λj nSj)	=1
(NS	∖	/ Ns	∖ -1
X J Div(TIIS j )) ≥ (XX D⅛)
=⇒ V (fλ (θ; {DSj}N1)) ≥ V (fνR (θ; {DSj }N=Sι))
B Bayesian Optimization Under the MSU-HPO Setting
In Algorithm 1, We described the abstracted hyperparameter optimization procedure Which alloWs
any black-box optimization method to be used. Here, in Algorithm 2, We describe the hyperparameter
optimization procedure under the MSU-HPO setting With the popular Bayesian optimization method.
C Regret Analysis
In this section, We analyze the regret bound under the MSU-HPO setting. We define a regret as
ʌ .. ..
rnB = f(θB) — f(θ*),
Where f : Θ → R is the ground-truth target task objective, n = PjN=S1 nSj is the total sample
size among source tasks, B is the total number of evaluations, θ* = arg mmj∈θ f (θ), and θB =
arg mmθ∈{θι ∙. ,Θb} fn(θ) where fn : Θ → R is a estimated target task objective by any estimator
13
Under review as a conference paper at ICLR 2021
Algorithm 2 Bayesian Optimization under the MSU-HPO setting
Input: unlabeled target task dataset DT = {xi}in=T1 ; labeled source task datasets {DSj =
{xij, yij}in=S1j }jN=S1; hyperparameter search space Θ; a machine learning model hθ; a target task
objective estiamtor f, number of evaluations B, acquisition function α(∙)
Output: the optimized set of hyperparameters θ? ∈ Θ
1:	Set Ao - 0
2:	for j ∈ {1, . . . , NS} do
3:	Split DSj into three folds DSdejnsity , DStrjain, and DSvaj l
4:	Estimate density ratio WSj (∙) by ULSIF with DT and DSnslItty
5:	end for
6:	for t = 1, 2, . . . , B do
7:	Select θt by optimizing α(θ | At-1)
8:	Train hθt by optimizing f(θ; {DtSrjain}j=ι) and obtain a trained model h：
9:	Evaluate h： and obtain a validation score Zt = f(θ; {DSjjl}j=ι)
10:	At — At-1 ∪ {(θt, Zt)}
11:	end for
12:	t? = arg mint {Z1, . . . ZB}
13:	return hθ? (where θ? = θt?)
(e.g., the unbiased estimator and the variance reduced estimator). Note that each of {θι,…，Θb} is
the hyperparameter selected in B evaluations in the optimization.
To bound the regret above, we first decompose it into the following terms:
ʌ .. ..
rB = f(θB) — f (θ*)
ʌ . . ʌ ʌ . . . ʌ ʌ . . , ʌ ʌ . . , ... ʌ ʌ ..
=(fWB ) - fn(θB )) + fn(θB ) + (fn(θ*) — f (θ*)) — fn(θ*)
,ʌ ʌ . . ʌ ʌ . . . ʌ . . ʌ ʌ . . . , ʌ ʌ .. ....
=(fn(θB ) — fn(θ:)) + (f(θB ) — fn(θB )) + (fn(θ* ) — f (θ*)),	(13)
--------V---------} V------V--------} V-------V-------}
(A)	(B)	(C)
where θ: = arg mι□θ∈θ fn(θ).
The term (A) represents the regret obtained by optimizing the estimated target task objective fn. The
term (B) represents the difference of a function value between the true objective f and the estimated
objective fn at θB, which is the solution obtained by the optimization for the estimated objective.
The term (C) represents the difference between the minimum value for the estimated objective f
and that of the true objective f .
We first show the following two lemmas which is used to bound the regret.
Lemma 2. The following inequality holds with a probability of at least 1 — δ, δ ∈ (0, 1)
(f(θB ) — fn(θB )) ≤ √V(fn(θB ))∕δ.
Proof. By Chebyshev’s inequality, we have
- ʌ . . ʌ ʌ. . - -. ʌ. . ʌ ʌ ... -
P{f(θB) — fn(θB) ≥ c}≤ P{∣f(θB) — fn(θB)| ≥ c}
≤ V(fn(θB ))∕c2.
Putting the RHS as δ and solving it for c completes the proof.
□
Lemma 3. The following inequality holds with a probability of at least 1 — δ, δ ∈ (0, 1)
ʌ ʌ . . . . / , , ʌ . . , ʌ ʌ ....
fn(θ*) — f(θ*) ≤ √(V(fn(θ*)) + V(fn(θ*)))∕δ.
14
Under review as a conference paper at ICLR 2021
Proof. By Chebyshev’s inequality, we have
,ʌ ʌ .. .. -
P{fn(θ*)- f(θ*) ≥ c}
,.ʌ ʌ .. ... -
≤ P{∣fn(θ*)- f(θ* )| ≥ c}
,.ʌ . . . . ʌ ʌ . .	ʌ ... -
≤ P{∣fn(θ*) - f(θ* )| ≥ C ∪ lfn(θ*) — f(θ* )| ≥ c}
,.ʌ . . . - , . ʌ ʌ . . ʌ ... -
≤ P{∣fn(θ*)- f(。* )| ≥ C} + P{∣fn(θ*)- f(θ*)l ≥ C}
ʌ ʌ ʌ
≤ F (V(fn(θ*))+ V(fn(θ*)))∙
c2
Putting the RHS as δ and solving it for C completes the proof.	□
Theorem 4. (Regret Bound on the MSU-HPO setting) When the λ-unbiased estimator with an
arbitrary set of weights λ is used as f(θ,; {Dga "N=SJ, the following regret bound holds with a
probability of at least 1 - δ, δ ∈ (0, 1),
Trn ≤ Rn + √2V(fn(θB ))∕δ + √2(V(fn(θ*)) + V(fn (θ* )))∕δ,	(14)
_	O , O . .	O , O ..
where Rn = fn(θB) - fn(θ*).
Proof. Putting Lemma 2 to the term (B) in Eq. (13) and Lemma 3 to the term (C) in Eq. (13)
complete the proof.	□
When an estimator is the proposed unbiased estimator or variance reduced esitmator, the variance
V(fn(∙)) is o(n); the second term and third term in Eq. (14) is to be no-regret (Srinivas et al., 2010)
with respect to n, i.e., hmn→∞(，2V(fn(θB))∕δ + yz2(V(fn(θ*)) + V(fn(θ*)))∕δ)∕n = 0. This
means that, if the optimization method is no-regret with respect to the number of evaluations B, the
overall regret approaches 0 as n and B are increased.
D Baselines
In this section, we first give a brief description of Learning Initialization (LI) (Wistuba et al., 2015)
and DistBO (Law et al., 2019) used as baseline methods in the experiments. Then, we discuss the
application of these methods to MSU-HPO.
D. 1 Learning Initialization
Learning Initialization (LI) by Wistuba et al. (2015) suggests promising hyperparameters by minimiz-
ing a meta-loss function. The meta-loss function is defined by the sum of a surrogated loss function
on each source task. Intuitively, by minimizing the meta-loss function, LI can obtain hyperparameters
that show good performance on average for source tasks.
D.2 DistBO
DistBO transfers knowledge across tasks using learnt representations of training datasets (Law et al.,
2019). To measure similarity between these tasks, DistBO uses a distributional kernel, which learns
the relationship between source and target tasks by joint Gaussian process model on hyperparameters
and data representation. At the first iteration of optimization for the target task, DistBO uses LCB as
the acquisition function to quickly select good hyperparameters7.
DistBO models a joint distribution p(x, y) of the training data for each task. However, in the MSU-
HPO setting, modeling p(x, y) for the target task is not possible because the labels on the target task
is not available. Therefore, in our experiments, DistBO models the marginal distribution p(x), not
the joint distribution p(x, y), for each task. This setting is also used in the original paper of DistBO
(Section 5.1 in (Law et al., 2019)). If the covariate shift assumption holds, then from the optimization
point of view, it is sufficient to model p(x), not p(x, y).
7Note that while our goal is to minimize the objective function, DistBO aims to maximize the objective
function.
15
Under review as a conference paper at ICLR 2021
D.3 Discussion on application to MSU-HPO
The major difference from the proposed method lies in the method of evaluating hyperparameters.
While the proposed method evaluates one hyperparameter using all source tasks, LI and DistBO
consider the situation where hyperparameters are evaluated for any one source task. The effect
originated by this difference will become significant when the number of source tasks is large. For
example, let us consider a scenario of optimization by DistBO where the available evaluation budget
is B = 100. If the number of source tasks is 2, we can optimize each source task with evaluation
budget B = 50 to assign a uniform evaluation budget to each source task. On the other hand, if the
number of source tasks is 50, the evaluation budget for each source task is only B = 2, which makes
optimization very difficult. Actually, in the Appendix C.5 in (Law et al., 2019), the existence of
1230 hyperparameter evaluations on source tasks is assumed on Parkinson’s dataset. In contrast, our
method works properly even if there is a limited evaluation budget for source tasks. In our experiment
on Parkinson’s dataset (in Section 4.2), we assume that only 50 hyperparameter evaluations on source
tasks are available.
E Experimental Setting
E.1	Settings
For fair comparison, we used Gaussian Process Upper Confidence Bound (GP-UCB) (Srinivas et al.,
2010) as a hyperparameter optimization algorithm for source tasks in all the optimization methods.
For example of the proposed method, this corresponds to OPT in Algorithm 1. In our experiments,
the confidence parameter in GP-UCB is fixed to 2 (following the setting used in (Nguyen et al., 2016;
Gonzdlez et al., 2016; BogUnovic et al., 2018)).
We set the number of evaluations (i.e., B in Algorithm 1 and 2) to 50. At the beginning of optimization,
We sample 5 initial points uniform randomly. A Matem 5/2 kernel is used in the implementation
of GP-UCB. Note that oUr stUdy is formUlated as a minimization, so we actUally Use LCB (Lower
Confidence Bound) instead of UCB as an acquisition function. We used densratio_py8 to estimate
the density ratio by uLSIF (Kanamori et al., 2009). All the experiments Were conducted on Google
Cloud Platform (n1-standard-4) or MacBook Pro (2.2 GHz Intel Core i7, 16 GB).
For LI, Which utilizes the gradient descent algorithm to optimize a meta-loss function defined by
source tasks, We need to set a learning rate η and a number of epochs E. FolloWing (Wistuba et al.,
2015), We set η = 10-3 and E = 103. Note that We need one hyperparameter to be evaluated
for the target task in MSU-HPO, so the number of hyperparameters We obtain by LI is one. To
obtain the results of DistBO in our experiments, We use the authors’ implementation9. We use a
Matern 5/2 kernel for a fair comparison With other methods, Whereas the default kernel in the code
implementation is a Matern 3/2 kernel.
E.2 Hyperparameter Optimization
Table 3 and 4 shoW the hyperparameter search spaces (Θ) of SVM on the Parkinson’s telemonitoring
dataset and LightGBM on the GvHD dataset. We treat integer-valued hyperparameters as a continuous
variable and rounded off before evaluations. For SVM, We used Radial Basis Function kernel in
scikit-learn (Pedregosa et al., 2011). We used microsoft/LightGBM10 to implement LightGBM. For
the details of these parameters, please refer to the scikit-learn documentation11 and the LightGBM
documentation12.
We normalized the feature vectors of the GvHD dataset as a preprocessing procedure. In contrast, in
the Parkinson dataset, We did not apply standardization or normalization, because We have confirmed
that these operations cause performance degradation. In the Parkinson dataset, We selected a task
(patient) With the maximum number of data as the target task. In contrast, the tasks in the GvHD
8https://github.com/hoxo-m/densratio_py
9https://github.com/hcllaW/distBO
10https://github.com/microsoft/LightGBM
11https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html
12https://lightgbm.readthedocs.io/en/latest/Parameters.html
16
Under review as a conference paper at ICLR 2021
dataset all have the same number of data, thus we selected the task (patient) with the first task index as
the target task. For statistical correctness, in the proposed unbiased and variance reduced estimators,
we used different sources of data for the density ratio estimation and for the training of ML models.
Specifically, we used 30% of the training set for the density ratio estimation and the remaining 70%
for the learning of the ML models.
Table 3: Details of hyperparameters of SVM on Parkinson,s telemonitoring dataset.
Hyperparameters	Type	Scale	Search Space
Kernel Coefficient	float	log	[5.0 × 10-5,5.0 × 103]
L2 Regularization Parameter	float	log	[5.0 × 10-5,5.0 × 103]
Table 4: Details of hyperparameters of LightGBM on GvHD dataset.			
Hyperparameters	Type	Scale	Search Space
Max Depth for Tree	int	linear	[2, 6]
Feature Fraction	float	linear	[0.1, 1.0]
Learning Rate	float	log	[1.0 × 10-3, 1.0 × 10-1]
L2 Regularization Parameter	float	log	[5.0 × 10-5,5.0 × 103]
17