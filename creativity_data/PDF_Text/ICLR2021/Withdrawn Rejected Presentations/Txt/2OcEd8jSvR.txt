Under review as a conference paper at ICLR 2021
An Euler-based GAN for time series
Anonymous authors
Paper under double-blind review
Ab stract
We introduce a new model of generative adversarial network (GAN) for time se-
ries generation based on Euler embedding together with Wasserstein metric in-
cluding Sinkhorn divergence. We observe that the Euler embedding improves the
stability of learning, provides meaningful learning parameters such as drift and
volatility while allowing the representation of a large class of underlying time
series. We demonstrate the capacity of the Euler GAN simulator to replicate clas-
sical Monte Carlo simulations, even in a possibly large multi-dimensional setting.
The generator can retrieve correlation structure or independence property. Com-
bining the proposed methodology with transfer learning methods, we verify that
our generator can learn efficiently from one single time serie trajectory. The ap-
proach is illustrated on S&P500 trajectories.
1	Introduction
Monte Carlo simulations of time series are widely used for industrial applications (stress testing,
investment decisions, stochastic control, weather forecast, anomaly predictions...). They are in par-
ticular extensively used in the financial sector, for market stress testing (Sorge (2004)), risk control
and deep hedging (Longstaff & SchWartz (2001); BUehler et al. (2019); FecamP et al.(2019)) or for
the measurement of common risk indicators such as Value at Risks (Jorion (2000)) among others.
Providing accUrate Monte Carlo simUlations of time series is a challenging task, Which reqUires Un-
derlying modelling assUmptions on the time dependency of the variables. The design of a realistic
and tractable model remains a tedioUs and mainly manUal task. Monte Carlo simUlators are reqUired
to select one type of models, sUch as AR(p), ARM A, Black-Scholes or Heston model, Which are
very common references in the indUstry. Hereby, it is not straightforWard to Update these models
When data of a neW kind is observed (e.g. apparition of negative interest rates or negative prices
in electricity markets, an economic crisis modifying the market strUctUre or neW meteorological
conditions). Updating the model in these sitUations is long and costly. This natUrally calls for the
development of reliable model free data driven generators for time series.
Generative methods sUch as Variational AUto Encoders (VAE) and Generative Adversarial NetWorks
(GAN) shoW impressive resUlts for text, images or video data. Developing similar generative meth-
ods for time series application is very appealing as it directly learns a reliable diffUsion model from
the data, WithoUt Underlying modelling assUmption. HoWever, dUe to the complex and possibly
non stationary Underlying time strUctUre of the initial time series, sUch generative methods are very
difficUlt to apply as sUch. FigUre 1 illUstrates one of the difficUlty: the generator is able to learn
properly the marginal distribUtion of the time series at each date, Whereas it is not able to captUre
the Underlying dynamics in time of the process.
The bootstrap method proposed by (Efron (1982)) is one of the first data-driven model free attempt
to generate time series data directly from historical dataset. Random samples of the data are simply
taken randomly With replacement. The scope of this techniqUe is limited as it does not generate
additional synthetic data bUt only historical ones. A first attempt toWards GAN application to time
series has been recently proposed in Yoon et al. (2019), folloWed by YU et al. (2017) or LUo et al.
(2018). More recently, some literatUre focUsed on the embedding of time series and stochastic pro-
cesses Using signatUre Fermanian (2019) or FoUrier Wasserstein distance Steinerberger (2018). A
specific application if GANs to commodity markets is detailed in Schreiber et al. (2019), as Well
as in FU et al. (2019). Several papers combine transport optimal theory With generative methods in
order to improve distribUtion learning Arjovsky et al. (2017), Genevay et al. (2018), While XU et al.
(2020) focUs on seqUence generation. In these contribUtions, the reqUired volUme of training data is
1
Under review as a conference paper at ICLR 2021
Figure 1: Left: A generated time series which marginal at each date is equal to the target time series
of the Middle. Right: Distribution of the time-flattened output vectors for the generator and of the
real sample
huge, and stationarity properties of the time series are also usually required.
In this paper, we build generators based on a Euler embedding of the time series, focusing hereby on
the general temporal structure of Ito processes. This formulation enables a rigorous mathematical
formulation of the problem as well as considering relevant constraints on the generator. Such embed-
ding can also be generalized by adding jumps or autocorrelation structures. We develop three Euler
based GANs for time series, relying on several distances between marginal distributions, including
1-Wasserstein, and Sinkhorn distance. We verify that the GANs can learn to replicate a Monte Carlo
simulator of one or multi-dimensional classical stochastic processes. It recovers the underlying cor-
relation or independence structure and can scale up to dimension 20 in our experiments. We then
perform a transfer learning approach on the S&P 500 time series, allowing to train our method on
a simulator and then to fine tune the model on the real data points. Mixing properly real data and
synthetically generated Monte Carlo simulations, we observe that the GAN can properly learn time
series from a single trajectory observation and provide relevant risk indicators such as Value at Risk
in such framework.
Main Contributions:
•	Combining an Euler embedding with aGAN structure, we exhibit a reliable time series gen-
erator, learning the underlying trend and volatility structures. The process space spanned
by the Euler embedding is large enough for most applications, and can easily be extended
to include auto regressive time series or series with jumps.
•	Using different distances between distributions (Jensen-Shannon, Wasserstein, Sinkhorn),
we are able to take into account the geometry of multivariate sequence space, allowing to
capture non-linear temporal dynamics.
•	We propose an innovative way to combine usual Monte Carlo together with GANs for
risk management purpose, improving liability and risk criterion for financial applications
(financial derivative pricing and Value at Risk computation).
•	We develop a reliable transfer learning approach when few data points are available, by
first learning on synthetic Monte Carlo based data points.
•	Our generators capture correlation and independence between time series and scale up to
dimension 20 in our experiments.
2	Problem description and Euler GAN design
We are given samples of a time series X ∈ Rd starting from a same point X0 and observed on the
time grid π = t0 < t1 < ... < tN . X may be considered as a random vector defined on RN × Rd.
We assume we do not know the distribution P of X . Our objective is to find a generator gθ that
generates a random vector X ∈ RN X Rd having distribution Pθ the closest possible to P. The time
series X considered here is not supposed to be stationary, in comparison to the current literature on
the topic. For example, the log return of a Black-Scholes diffusion provides stationary gaussian iid
samples, and considerably ease our problem.
2
Under review as a conference paper at ICLR 2021
2.1	SDE representation of the generator
.	* TL ɪ	.1	Z ∖ ∙ ∙	1	C	1	C/),,	1 .	Fl C
In our GAN setting, the generator (gθ) is in charge of producing Xθ through two measurable func-
tions bθ, σθ representing respectively the drift and the volatility, as well as a covariance matrix Γθ,
output of a neural network parameterized by θ. The generated continuous time process Xθ is by
construction following the stochastic differential equation:
ʌ λ	Zl ,	ʌ λ .	Zl ,	ʌ λ .
dXtθ	= bθ(t,Xtθ)dt+ σθ(t, Xtθ)dBt.
(1)
With B a d-dimensional BroWnian motion on some probability space (Ω, F, P) where (Ft)t0≤t≤tN
is a filtration satisfying the usual conditions. This representation gives a temporal structure to Xθ
and differs from the usual applications of GANs and VAE to time series. For given bθ, σθ, generated
samples of Xθ are given by discretizing Equation (1) with an EUler embedding:
X0	= Xo
Xθi+1	= Xti + bθ (ti, XXti )(ti+1 - ti) + σθ (ti, Xti)(ti+1 - ti )Z.	(2)
where Z 〜N(0, Γθ). During the training phase, gt calibrates two functions bθ and σθ and builds
a process trajectory Xθ following Euler scheme (2) from a noise vector Z, then a discriminator dφ
(parameterized by 夕 ∈ θ) tries to distinguish between fake times series generated by the Euler GAN
and real time series. A control of drift and volatility is possible with generator last layers activation
function, for example with a tanh to force drift in [-1, 1] interval. For a continuous distribution Q
and samples x drawn from Q we denote by Πd(x) the empirical distribution obtained from x.
In order to emphasize the robustness of our approach, we investigate three different GAN represen-
tations of for time series:
• Jensen-Shannon divergence: As defined in Goodfellow et al. (2014), we intend to solve
infsupEX〜p[log(dw(X))]+ Eχθ〜pθ [log(1 - dφ(X^θ)].	(3)
Given samples X and Xθ drawn respectively from P and Pθ, the term in the inf SuP may be
estimated by:
ljs(θ,ψ,x,xθ) = E[log(dφ(x))] + E[log(1 - dφ(Xθ)].	(4)
This setting, referred to as Euler GAN (eGAN) in this paper and its algorithm is described
in Annex 2. Such approach is recognized to fail providing meaningful representations of
disjoint distributions, see Arjovsky et al. (2017), and we investigate several alternatives.
• 1-Wasserstein distance: One way to improve the training of vanilla GANs is to use a
different distance between distributions. First, we replace the Jensen-Shannon divergence
by the 1-Wasserstein metric, due to its nicer regularity properties. Wasserstein distance is
for example intensively used in Optimal Transport field (see Villani (2008)). For a cost
function c : RN × RN → R, the p-Wasserstein distance between two distributions P and
Pθ (defined on M) is given by:
WCp(P,pθ) = (∏∈∏nP,pθ) M ×Mc(χ,y)pdπ(χ,y))p	⑸
As pointed out in Arjovsky et al. (2017), under the assumption that dψ is K-Lipshitz and
for p = 1, c(x, y ) = kx - yk1 , Kantorovich-Rubinstein duality applies and Equation (5) is
equal to:
ɪinf SuP	EX〜P [dψ(X)] - Eχθ〜pθ [dψ(Xθ)]	⑹
K θ Ψ,∣∣d"L≤κ
Given samples X and Xθ respectively drawn from P and Pθ the term inside the inf sup in
(equation 6) may be approximated by:
lW1 (θ, ψ, x, Xθ) = E [dψ(x)] - EXθ〜pθ [dψ(Xθ)] .	(7)
This setting is referred to as eWGAN, the algorithm is described in Annex 3.
3
Under review as a conference paper at ICLR 2021
• Regularized 2-Wasserstein distance: It defines for λ ≥ 0 by
Wλ,c(P,Pθ)p = π∈Πin(Pf,Pθ) M1×M2c(x,y)pdπ(x,y)
+ λZm-m lOg(£(xP；y)) dπ(x,y).
(8)
As mentioned in Cuturi et al. (2019) the regularization term ensures that Wλ,c(P, Pθ ) is
θ-differentiable as well as computationally tractable using using the Sinkhorn algorithm.
Let cφ(χ, y) := ∣∣dφ(χ) - dφ(y)k2. In order to reduce the bias in measuring the distance
between the two distributions without reducing the regularization parameter λ, Genevay
et al. (2018) proposes to use the following Sinkorn divergence:
SK(P, Pθ) = inf sup 2Wλ,% (P, Pθ )p - Wι,“(P, P)p - Wι,“(Pθ, Pθ )p	(9)
For two samples χ, χθ, let Wλ,%(χ,χθ) = Wλ,%(∏d(χ),∏d(χθ)). The term inside the
inf sup may be approximated by:
ISK(θ, ψ, χ, χθ) = 2Wλ,c式χ, χθ)p - Wλ,c(χ, χ)p - Wλ,c(^θ, χθ)p	(10)
This setting is referred to as eSGAN and its algorithm is described in Algorithm 1.
In the two first configurations, the generator minimizes only the expectation of generated data (the
second term in Equation 3 and 6) while the discriminator maximizes the whole loss. However, in
Sinkhorn GAN, the generator minimizes the whole loss 'sk .
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Input: θo,夕0 randomly chosen, α, β learning rates, K number of iterations,
M batch size, nc critic iterations, c clipping value, X(i) i=1M real data
Output: θ,夕
θ 一 θo,2一夕 o；
for k = 1..K do
for j = 1..ncritic do
X — M samples with X⑴=(X(i),..., X(N))i=ι..M；
Z J M samples iid gaussian noise;
χθ J M generation from Euler scheme and gθ(z);
夕 J 2 + αAdam RA'SK(dφ(χ), dφ(χθ),α);
夕 J clip(夕，-c, c);
end
χ J M samples withX(i) = (Xt(1i), . . . ,Xt(Ni))i=1..M ;
z J M samples iid gaussian noise;
χθ J M generation from Euler scheme and gθ(z);
θ J θ - βAdam (Vθ('SKK(dφ(χ), dφ(χθ));
end
Algorithm 1: Algorithm for Sinkhorn GAN.
3 Numerical Experiments
We compare the three Euler GAN variants (eGAN , eWGAN and eSGAN) on generated known
stochastic processes (namely the Black&Scholes model). Test are done in dimension one in Section
3.1 and scaling study on dimensions up to 20 is done in Section 3.2. In Section 3.3, we propose to
use transfer learning technique to enrich a limited historical data set (less than 1000 observations).
Our eGan method is trained on Monte Carlo simulations during the first stochastic gradient descent
iterations and on real data during the last iteraions. By fine-tuning our model on the real world data
set, we show that predictive and risk factor scores based on generated trajectories overcome classical
the initial Monte Carlo simulator.
All neural network architectures and hyper parameters such as batch size, learning rate are described
in Annex A.3.
4
Under review as a conference paper at ICLR 2021
3.1	Experiments for a one-dimensional simulated process
In this section, in order to obtain all the useful statistics, we test our approach on a well-known and
easy to simulate Black-Scholes model defined by:
dXt = rXtdt + σXtdWt	(11)
where r = 0.8, σ = 0.3, X0 = 0.2. Simulations are done on 90 dates and maturity is 0.25 (1
simulation per day during 3 months). Table 3.1 compares statistics obtained with the reference MC
simulator with those obtained with our eGAN simulators. One can notice a nice adequation for the
moments, the 5% and 95% percentile.
Moments	MC	eGAN	eWGAN	eSGAN
min	1.54e-01	Black-Scholes 1.61e-01	1.40e-01		1.33e-01
5%q	1.87e-01	1.89e-01	1.90e-01	1.76e-01
mean	2.22e-01	2.22e-01	2.24e-01	2.20e-01
95%q	2.60e-01	2.61e-01	2.57e-01	2.61e-01
max	3.23e-01	3.27e-01	3.01e-01	2.96e-01
var	5.90e-04	5.58e-04	4.55e-04	7.69e-04
Table 1: Moment comparison of real and generated process, where m(x) = 1 PT=I f (x(i)) with f
is min, 5%q,... over batch of size M = 1000
The Figure 2 shows a good match between the histograms of the flattened vectors in time of the real
and generated time series. These nice results are upgraded by realistic looking trajectories (same
Figure). Finally, still in Figure 2, the moments and the envelopes are also confirming the reliablity
of our approach.
Thanks to the chosen Euler embedding, drift and volatility estimations can be easily visualized
and analyzed. Table 3.1 shows that the (rXt) seems well represented, while the estimation of the
volatility is slightly less accurate.
The temporal dependence between marginals is characterized with AutoCorrelation Function (ACF)
in Figure 3. QQPlots and distribution of the log returns show represented in Figure 4.
Time dependence properties compared with real process simulations is evaluate with ACF plots in
Figure 3. Mean squared error between auto correlation of the lagged time series measures and real
one is satisfying.
Another way to illustrate if we capture previous dates marginal distributions dependencies is to
focus on log return which in the Black-Scholes are gaussian. This property seems to be respected
by generated time series as illustrated in Figure A.2.
Figure A.2 in the Annex presents percentile of discriminant scores for each real and generated data.
The discriminant seeks to maximize the gap between real and fake data, as the generator learns to
minimize it.
In Figure 4, QQ-plots are given to compare the Black-Scholes simulated distribution and generated
ones. We note here that Sinkhorn Euler GAN presents some weakness compared to other presented
GAN, the distribution tails scatter from x-axis. But, all Box-Cox plot demonstrates a good perfor-
mance of time dependence extrapolation by passing the normality test.
Real	MC	eGAN	eWGAN	eSGAN
Black-Scholes
E [bθ(t,Xt)∕Xt]	0.8	0.803	0.838	0.770	0.777
E [σθ(t, Xt)∕Xt]	0.4	0.400	0.290	0.400	0.410
Table 2: Comparison of E[bθ(t,Xθ)∕Xθ], E[σθ(t,Xθ"Xθ] obtained with eGAN, eWGAN, and
eSGAN with the real theoretical value.
5
Under review as a conference paper at ICLR 2021
Figure 2: Up: Histogram and KDE of real processes (in blue) and GAN generations (in orange)
for Black-Scholes. Middle :Some fake simulations obtained with the three Euler Gans Bottom:
Envelope and percentile per Time Step / Comparison with the Monte Carlo reference simulator
Figure 3: Auto correlation ACF of real and Euler GAN generations
Autocorrelation is particularly accurate, and mean squared error between real BS (first row) and
eGAN (resp. eWGAN, eSGAN) is 4.48.10-4 (resp. 1.88.10-4, 3.59.10-4).
6
Under review as a conference paper at ICLR 2021
Figure 4: First row, QQplot theorical quantile from real gaussian distribution according to empirical
quantiles for log returns of reps. Monte Carlo Black-Scholes simulations, eGAN, eWGAN and
eSGAN. The Second row shows distributions of log return, which should be gaussian. Last row
indicates BoxCox normality graph.
In Table3 we use our eGANs simulator to price european call and put options. These Monte Carlo
pricings are consistent with prices obtained with the theoretical Black&Scholes formula.
Strike	0.1	0.2	0.3	0.4
Call Black-Scholes	0.118	0.039	0.003	0.000
Call eWGAN	0.119	0.040	0.002	0.000
Call eSGan	0.118	0.039	0.001	0.000
Call eGan	0.117	0.036	0.000	0.000
Put Black Scholes	0.000	0.003	0.049	0.128
Put WGAN	0.000	0.003	0.046	0.126
Put eSGan	0.000	0.003	0.047	0.128
Put eGan	0.000	0.001	0.047	0.128
Table 3: Pricing comparison between Call and Put options obtained with the Black-Scholes formula
and with discounted average of 1000 simulations obtained from our eGANs (x0 = 0.2, r = 0.8, σ =
0.4, T = 0.25)
Euler-based generators seems to be good candidates for time series generation. The question we
address in the next Section is whether eGAN can scale to higher dimensions.
3.2	Scaling the dimension and capturing the correlation structure
In this section, we use the very same methodology described in section 3.1 but with multivariate
time series. We focus on eWGAN and we show that 2-dimensional correlation factor. With a
ρ = 0.6 two correlated processes are simulated and given for training to the eulerGAN generator.
The generator is able to identify the covariance matrix Γθ as illustrated in figure 3.2. If the processes
are independent, the χ2 test accepts independence with α = 0.05 in dimension 2 and 3. In Figure
A.2, a training a 20-dimensional Black-Scholes process with independent increments is done and
show an envelope for the 20 process well respected.
7
Under review as a conference paper at ICLR 2021
Figure 5: Correlation matrix of 2D Black Scholes. Left column is correlation matrix of generated
data from eWGAN and right from Monte Carlo simulations. The first row indicates correlation
factors at ending date T, the second is the mean correlation values across time.
3.3 Transfer learning - Monte Carlo to historical tuning
Our Euler GANs may need more data than available to be trained. If one has already designed a
reasonable model, we can rely on the use of transfer learning techniques. This transfer learning tech-
nique is applied in this section on the S&P500: the networks are firstly trained on a Black-Scholes
process calibrated on the S&P500 and secondly on samples of monthly data from 1957 to 1999. We
compare percentiles on a testing period (2000-2016) obtained with a Monte Carlo simulator cali-
brated on the 1957-1999 period and Euler GANs trajectories trained on the MC simulator and on
the 1957-1999 dataset. As shown in Table 4 the percentiles obtained with our eGaN is closer to the
historical 2000-2016 percentile than the ones obtained with the initial Monte-Carlo simulator. The
Value-At-Risk (VaR) (Percentile(XT - Xt0 where t0 = 01/12/2016 and t1 = 31/12/2016)) is also
compared in 4 and acknowledge the accuracy of our approach.
Moments	Real data	Monte Carlo	eGAN	eWGAN	eSGAN
VaR	-5.93e-02	-8.26e-02 =	-1.80e-02	-7.16e-02	-5.58e-02
min	-1.39e-01	-1.82e-01	-3.12e-01	-1.68e-01	-1.58e-01
5%q	-7.65e-02	-8.88e-02	-7.73e-02	-8.41e-02	-7.37e-02
mean	4.70e-03	6.08e-03	3.33e-03	5.95e-03	6.44e-03
95%q	7.13e-02	1.01e-01	8.85e-02	9.61e-02	8.68e-02
max	9.57e-02	1.92e-01	3.09e-01	1.83e-01	1.64e-01
variance	2.01e-03	3.32e-03	2.94e-03	3.01e-03	2.41e-03
Table 4: Average time-flatten moments and Value-at-Risk of real S&P500 data (on testing set),
Monte Carlo simulation and Euler GAN generations.
8
Under review as a conference paper at ICLR 2021
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Hans Buehler, Lukas Gonon, Josef Teichmann, and Ben Wood. Deep hedging. Quantitative Finance,
19(8):1271-1291,2019.
Marco Cuturi, Olivier Teboul, and Jean-Philippe Vert. Differentiable ranking and sorting using
optimal transport. In Advances in Neural Information Processing Systems, pp. 6861-6871, 2019.
Bradley Efron. The jackknife, the bootstrap and other resampling plans. SIAM, 1982.
Simon Fecamp, Joseph Mikael, and Xavier Warin. Risk management with machine-learning-based
algorithms. arXiv preprint arXiv:1902.05287, 2019.
Adeline Fermanian. Embedding and learning with signatures. arXiv preprint arXiv:1911.13211,
2019.
Rao Fu, Jie Chen, Shutian Zeng, Yiping Zhuang, and Agus Sudjianto. Time series simulation by
conditional generative adversarial net. arXiv preprint arXiv:1904.11419, 2019.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with sinkhorn di-
vergences. In International Conference on Artificial Intelligence and Statistics, pp. 1608-1617,
2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Philippe Jorion. Value at risk. McGraw-Hill Professional Publishing, 2000.
Francis A Longstaff and Eduardo S Schwartz. Valuing american options by simulation: a simple
least-squares approach. The review of financial studies, 14(1):113-147, 2001.
Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et al. Multivariate time series imputation with
generative adversarial networks. In Advances in Neural Information Processing Systems, pp.
1596-1607, 2018.
Jens Schreiber, Maik Jessulat, and Bernhard Sick. Generative adversarial networks for operational
scenario planning of renewable energy farms: A study on wind and photovoltaic. In International
Conference on Artificial Neural Networks, pp. 550-564. Springer, 2019.
Marco Sorge. Stress-testing financial systems: an overview of current methodologies. 2004.
Stefan Steinerberger. Wasserstein distance, fourier series and applications. arXiv preprint
arXiv:1803.08011, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Tianlin Xu, Li K Wenliang, Michael Munn, and Beatrice Acciaio. Cot-gan: Generating sequential
data via causal optimal transport. arXiv preprint arXiv:2006.08571, 2020.
Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar. Time-series generative ad-
versarial networks. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alche-Buc,
E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp.
5508-5518. Curran Associates, Inc., 2019. URL http://papers.nips.cc/paper/
8789-time-series-generative-adversarial-networks.pdf.
Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets
with policy gradient. In Thirty-first AAAI conference on artificial intelligence, 2017.
9
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Algorithm details
1
2
3
4
5
6
7
8
9
10
11
12
13
14
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Input: θo,夕° randomly chosen, α, β learning rates, K number of iterations,
M batch size, nc critic iterations, c clipping value, X(i) i=1 M real data
Output: θ,夕
θ 一 θo,2一2 0；
for k = 1..K do
for j = 1..ncritic do
X — M samples With X(i) = (X(i),..., X(N))i=ι..M；
Z J M samples iid gaussien noise;
Xθ J M generation from Euler scheme and gθ (z);
φ J φ + αAdam (Vφ(E[dφ(x)] 一 E[dφ(Xθ)]), α);
夕 J clip(夕，-c, c);
end
x J M samples WithX(i) = (Xt(1i), . . . ,Xt(Ni))i=1..M;
z J M samples iid gaussian noise;
Xθ J M generation from Euler scheme and gθ (z);
θ J θ 一 βAdam (Vθ(E[dφ(Xθ)]), β);
end
Algorithm 2: Algorithm of Euler GAN
Input: θ0,夕0 randomly chosen, α, β learning rates, K number of iterations,
M batch size, nc critic iterations, c clipping value, X (i) i=1 M real data
Output: θ,夕
θ J θθ,夕 J 20;
for k = 1..K do
for j = 1..ncritic do
x J M samples WithX(i) = (Xt(1i), . . . ,Xt(Ni))i=1..M;
z J M samples iid gaussien noise;
Xθ J M generation from Euler scheme and gθ (z);
φ J φ + αAdam (Vφ(E[log(dφ(x))] — E[d,φ(log(1 — Xθ))]),α);
夕 J clip(夕，—c, c);
end
x J M samples WithX(i) = (Xt(1i), . . . ,Xt(Ni))i=1..M;
z J M samples iid gaussian noise;
Xθ J M generation from Euler scheme and gθ(z);
θ J θ — βAdam (Vθ(E[dφ(log(Xθ))]), β);
end
Algorithm 3: Algorithm for adversarial learning for Euler 1-Wasserstein GAN.
A.2 Additional figures
10
Under review as a conference paper at ICLR 2021
Figure 6: Quantiles of discriminant score distribution. Blue quantile is the real data discriminant
score and orange fake ones.
Figure 7: Envelope of a 20-dimensional Black-Scholes (Blue: generated/Orange: Real
)
A.3 Models and hyper-parameters
Settings of neural networks	
Training data lenght (ndates)	90
Random noise vector dimension	(90×ndim)
Random noise distribution	white noise
Generator structure	{Dense(128),Dense(64),Dense(32),Dense(2×ndim)}
Discriminator structure	{Dense(128),Dense(64),Dense(1×ndim)}
Optimizer	Adam
Learning rates	1.10-4
Batch size	300	
Table 5: Neural networks parameters
11