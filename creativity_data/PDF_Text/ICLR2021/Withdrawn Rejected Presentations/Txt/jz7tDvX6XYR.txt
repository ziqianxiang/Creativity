Under review as a conference paper at ICLR 2021
Speeding up Deep Learning Training by Shar-
ing Weights and Then Unsharing
Anonymous authors
Paper under double-blind review
Ab stract
It has been widely observed that increasing deep learning model sizes often leads
to significant performance improvements on a variety of natural language process-
ing and computer vision tasks. In the meantime, however, computational costs and
training time would dramatically increase when models get larger. In this paper,
we propose a simple approach to speed up training for a particular kind of deep
networks which contain repeated structures, such as the transformer module. In
our method, we first train such a deep network with the weights shared across
all the repeated layers till some point. We then stop weight sharing and con-
tinue training until convergence. The untying point is automatically determined
by monitoring gradient statistics. Our adaptive untying criterion is obtained from
a theoretic analysis over deep linear networks. Empirical results show that our
method is able to reduce the training time of BERT by 50%.
1	Introduction
It has been widely observed that increasing model size often leads to significantly better perfor-
mance on various real tasks, especially natural language processing and computer vision applica-
tions (Amodei et al., 2016; He et al., 2016a; Wu et al., 2016; Vaswani et al., 2017; Devlin et al., 2019;
Brock et al., 2019; Raffel et al., 2020; Brown et al., 2020). However, as models getting larger, the
training can become extremely resource intensive and time consuming. As a consequence, there has
been a growing interest in developing systems and algorithms for efficient distributed large-batch
training (Goyal et al., 2017; Shazeer et al., 2018; Lepikhin et al., 2020; You et al., 2020).
In this paper, we seek for speeding up deep learning training by exploiting unique network architec-
tures rather than by distributed training. In particular, we are interested in speeding up the training
of a special kind of deep networks which are constructed by repeatedly stacking the same layer, for
example, the transformer module (Vaswani et al., 2017). We propose a simple method for efficiently
training such kind of networks. In our approach, we first force the weights to be shared across all
the repeated layers and train the network, and then, at some point, we stop weight sharing and con-
tinue training until convergence. The point for stopping weight sharing can be either predefined or
automatically chosen by monitoring gradient statistics during training. Empirical studies show that
our method can reduce the training time of BERT (Devlin et al., 2019) by 50%.
Our method is motivated by the successes of weight sharing models, in particular, ALBERT (Lan
et al., 2020). Itis a variant of BERT in which the weights across all the transformer layers are shared.
As long as its architecture is sufficiently large, ALBERT can be comparable with or even outper-
form the original BERT on various downstream natural language processing benchmarks. However,
when its architecture being the same as the original BERT, ALBERT performs significantly worse.
Since the weights in the original BERT are not shared at all, it is natural to expect that ALBERT’s
performance will be improved if we stop its weight sharing at some point of training.
To make this idea work, however, we need to know when to untie the shared weights. A randomly
chosen untying point will not work. We can see this from the two extreme cases: ALBERT which
shares weights all the time, and BERT which has no weight sharing at all. To find an effective
solution for automatic weight untying, we turn to theoretic analysis over deep linear networks (Hardt
& Ma, 2017; Laurent & Brecht, 2018; Wu et al., 2019). A deep linear model is constructed by
stacking a series of matrix multiplications. In its forward pass, a deep linear model is trivially
equivalent to a single matrix. However, when being trained with backpropagation, its behavior is
1
Under review as a conference paper at ICLR 2021
analogous to the deep models with non-linearity but much easier to understand. Our theoretical
analysis shows that, when learning a positive definite matrix (which admits an optimal solution
with all layers having the same weights), training with weight sharing can bring significantly faster
convergence. More importantly, our theoretical analysis leads to the adaptive weight untying rule
that we need to construct our algorithm (see Algorithm 2). Empirical studies on real tasks show
that our adaptive untying method can be at least as effective as using the best untying point which is
obtained by running multiple experiments of which each has a different point to untie weights and
then choosing the best result.
The rest of this paper is organized as follows. We present our weight sharing algorithm in Section
2. It actually contains three versions, depending on how to stop weight sharing during training. In
Section 3, we present our theoretical results for positive definite deep linear models. All the proofs
are deferred to the Appendix. In Section 4, we discuss related work. In Section 5, we show detailed
experimental setups and results. We also provide various ablation studies on different choices in
implementing our algorithm. Finally, we conclude this paper with discussions in Section 6.
2	Algorithm: Sharing Weights and Then Unsharing
Assume we have a deep network which is obtained by repeatedly stacking the same neural module
n times, such as the transformer module in transformer models (Vaswani et al., 2017). Denote by
w1 , . . . , wn the weights of these n layers. In our method, we first train the deep network with all the
weights tied. Then, after a certain number of training steps, we untie the weights and further train
the network until convergence. In what follows, we first present a simple version of our algorithm
in which the weight untying point is predefined. Then, we move to its adaptive version in which
the layers are automatically gradually untied according to gradient statistics. Finally, we discuss a
simplified variant of this adaptive method which unties the layers all the once.
Untying weights at a fixed point. This is the simplest version of our method (Algorithm 1). We
first train the deep network with all the weights tied for a fixed number of steps, and then untie the
weights and continue training until convergence.
Algorithm 1 SHARING WEIGHTS AND THEN UNTYING AT AT A FIXED POINT
1:
2:
3:
4:
5:
6:
7:
Input: total number of training steps T, untying point τ, learning rates {α(t), t = 1, . . . , T}
Randomly and equally initialize weights w1(0) , . . . , wn(0)
for t = 1 to T do
if t < τ then
(t)
wi
else
(t)
wi
(t-1)
wi
(t-1)
wi
- α(t) × mean grad loss, wk(t-1) , k = 1, . . . , n , i = 1, . . . , n
- α(t) × grad loss, wi(t-1) , i = 1, . . . , n
Note that, from line 1 to 5, we initialize all the weights equally, and then update them using the
mean of their gradients. It is easy to see that such an update is equivalent to weight sharing or tying.
For the sake of simplicity, in lines 5 and 7, we only show how to update the weights using the plain
(stochastic) gradient descent rule. One can replace this plain update rule with any of their favorite
optimization methods, for example, the Adam optimization algorithm (Kingma & Ba, 2015).
While the repeated layers being the most natural units for weight sharing, that is not the only choice.
We may view several layers together as the weight sharing unit, and share the weights across those
units. The layers within the same unit can have different weights. For example, for a 24-layer
transformer model, we may combine every four layers as a weight sharing unit. Thus, there will
be six such units for weight sharing. Such flexibility of choosing weight sharing units allows for a
balance between “full weight sharing” and “no weight sharing” at all.
Adaptive weight untying. The theoretical analysis in Section 3 motives us to adaptively and gradu-
ally untie weights based on the gradient correlation of adjacent layers (Algorithm 2). To implement
this idea, all layers are put in the same group during initialization. Then, at any time step of the train-
ing, suppose we have groups G = {G1, G2, ..., Gk}. For each group Gi, we compute the correlation
2
Under review as a conference paper at ICLR 2021
between the gradients of any two adjacent layers by
It.」),娟〉
correlation；」=--1~τj~
j kgji)kkgj+ιk
gj* (i), gj(i+)1 denote the gradients of two adjacent layers in Gi. (1)
(i)
Once correlationj falls below ρ (0.5 as default) consecutively for a certain number of times (3 as
default), we split the group Gi into two subgroups, by splitting at j-th layer in Gi . The layers in the
same group are updated using the gradient mean of this group as in Algorithm 1.
Algorithm 2 ADAPTIVE WEIGHT UNTYING
1:	Input: total number of training steps T, threshold ρ, learning rates {α(t), t = 1, . . . , T}
2:	All layers are put in the same group, and randomly initialized with the same weights
3:	for t = 1 to T do
4:	for adjacent layers i, i + 1 in the same group do
5:	Calculate the gradient correlation as Equation 1 //Motivated by Section 3
6:	If the correlation falls below ρ, break the group between layer i and i + 1
7:	Update the weights in each group using the average of gradients within each group
Simplified adaptive untying. To simplify the multi-step adaptive untying, instead of gradually
splitting the groups, we may untie the weights of all layers at once. For every certain number of
iteration steps, we monitor the gradient correlations as described in multi-step adaptive untying. If
more than half of these layer correlations are less than the predefined threshold ρ (0.5 as default)
consecutively for a certain number of times (3 as default), we stop sharing weights for all layers.
3 Theoretic Motivation
In this section, we show how our method including the adaptive untying criterion can be motivated
by analyzing the dynamics of training a deep linear network by gradient descent. At the first glance,
deep learning models may look trivial since a deep linear model is just equivalent to a single matrix.
However, when being trained with backpropagation, its behavior is analogous to generic deep mod-
els. Thus, analyzing deep linear models has attracted increasing interest in the theoretic research
community (Hardt & Ma, 2017; Laurent & Brecht, 2018; Wu et al., 2019).
A deep linear network is a series of matrix multiplication
f(x;W1,...,WL) = WLWL-1...W1x,	Wl ∈ Rd×d, `= 1,...,L.
The task is to train the deep linear network to learn a target matrix Φ ∈ Rd×d . To focus on the
training dynamics, we adopt the simplified objective function
R(Wi,...Wl) = 2 IIWlWl-I …W2W1 - ΦkF.
Denote VlR to be the gradient of R with respect to Wl. We have
∂R	T	T
VlR = ∂W = WL：l^+1(WL：1 - 剌WlT：1，
where Wl2:l1 = Wl2Wl2-1...Wl1+1Wl1 . The standard gradient update is given by
Wl(t+1) = Wl(t) -ηVlR(t),	l= 1,..., L.
To train with weights shared, all the layers need to have the same initialization. And the update is
L
W1 (t + 1)= Wι(t)-η X ViR(t),	l = 1,...,L.
(2)
i=1
Since the initialization and updates are the same for all layers, the parameters W1(t), ..., WL(t) are
equal for all t. For simplicity, we denote the weight at time t to be W0(t). Notice that the gradients
are averaged, the norm of update to each layer doesn’t scale with L.
Suppose that the target matrix Φ is a positive definite matrix. It is immediate that Φ^n is a solution
to the deep linear network. Before looking into the detailed convergence analysis, it worth first
showing a Lemma that reveals the updates in the weight sharing training.
3
Under review as a conference paper at ICLR 2021
Lemma 1. With a positive definite target matrix Φ and initializing with W0 (0) = I, update the
parameters according to Equation 2, we have
Wι(t +1)- Wι(t) = -ηWL-1 (t)(wL(t)- Φ), l = 1,...,L, ∀t ≥ 0.
Intuitively, the Lemma 1 shows that weight sharing allows all the layers to be trained “equally well”,
the layers that are far away from the output layer won’t suffer from gradient vanishing or exploding.
In the following subsections, we first study the convergence result with continuous-time gradient
descent, which demonstrates the benefit of training with weight sharing when learning a positive
definite matrix Φ. We then extend the results to the discrete-time gradient descent. We draw a
comparison with training with zero-asymmetric (ZAS) initialization (Wu et al., 2019). To the best
of our knowledge, ZAS gives the state-of-the-art convergence rate. It is actually the only work
showing the global convergence of deep linear network trained by gradient descent for an arbitrary
target matrix.
3.1	Continuous-time gradient descent
With continuous-time gradient descent (i.e. η → 0), training with gradient descent and ZAS, the
loss decays as R(t) ≤ exp(-2t)R(0). For training with weight sharing, the loss becomes R(t) ≤
exp(-2L min(1, λmin(Φ))t)R(0), when the target matrix Φ is positive definite. The extra L in the
exponent demonstrates the acceleration of training with weight sharing.
With η → 0, the training dynamics of continuous-time gradient descent can be described as
dWt(⅛ = Wι(t) = -VιR(t),	l =1,…，L, t ≥ 0.
The ZAS initializes the weights Wi = W2 = •…=Wl-i = I and WL = 0. It helps avoiding
saddle points and has the following convergence result.
Theorem 1.	[Continuous-time gradient descent without weight sharing (Wu et al., 2019)] For the
deep linear network f(x; W1, ..., WL) = WLWL-1...W1x, the continuous time gradient descent
with the zero-asymmetric initialization satisfies R(t) ≤ exp (-2t) R(0).
Theorem 1 shows that with the zero-asymmetric initialization, the continuous gradient descent lin-
early converges to the global optimal solution for general target matrix Φ.
Next, considering the special case where the goal is to learn a positive definite matrix Φ. Based on
Lemma 1, we have the following convergence result for training with weight sharing.
Theorem 2.	[Continuous-time gradient descent with weight sharing] For the deep linear network
f(x; W1, ..., WL) = WLWL-1...W1x, initialize all Wl(0) with identity matrix I and update ac-
cording to Equation 2. With a positive definite target matrix Φ, the continuous-time gradient descent
satisfies R(t) ≤ exp (—2Lmin(1, *nin(Φ))t) R(0).
Remark 1. The difference between convergence rates in Theorem 1 and Theorem 2 is not an artifact
of analysis. For example, when the target matrix is simply Φ = αI , α > 1. It can be explicitly
shown that with the initialization in Theorem 1, we have R(0) = -2R(0) while training with
weight sharing (Theorem 2), we have R(0) = -2LR(0). This implies that the convergence results
in Theorem 1 and Theorem 2 cannot be improved in general.
The extra L in the exponent leads to faster convergence. The key to show the acceleration is
竽=XXtr (v>R(t)Wι(t)) ≤ —2Lλmin(Wo(t)LT)R(t),
l=1
where we see the L comes from the summation. This sheds light on two important factors:
1.	All layers need to have sufficiently large update (i.e. Wl(t) is large for all l).
2.	Each layer’s update needs to well correlate with its gradient (i.e. VlR(t) correlates with Wl(t)).
4
Under review as a conference paper at ICLR 2021
Initializing the weights to be the same and using the average of gradients guarantees that all layers
are sufficiently trained. The high correlation of VιR(t) and Wl (t) relies on Φ being positive definite.
Suppose the gradients of different layers do not correlate well (e.g. tr (ViRgVjR(t)) ≈ 0, i = j)
and the weights are still forced to be shared via the updates according to Equation 2. Recall that
Wι(t) = L PL=I ViR(t),wethenhave PL=1 tr (v>R(t)Wι(t)) ≈ 一L PL=1 kVR(t)kF, which
loses the extra L acceleration in the convergence due to the 1/L leading factor.
When dealing with real deep learning models, there is no guarantee that all the gradients at different
layers highly correlate. Thus, we may monitor gradient correlations during training: sharing weights
when gradients well correlate, and break the ties when gradient correlations fall below a certain
threshold. This matches the adaptive untying rule we proposed in Section 2.
3.2 Discrete-time gradient descent
Here we extend the previous result to the discrete-time gradient descent with a positive constant step
size η. It can be shown that with zero-asymmetric initialization, training with the gradient descent
will achieve R(t) ≤ within O(L3 log(1/)) steps; initializing and training with weights sharing,
the deep linear network will learn a positive definite matrix Φ to R(t) ≤ within O(L log(1/))
steps, which reduces the required iterations by a factor of L2 .
To make easy comparisons, we first repeat without proving the discrete-time gradient descent con-
vergence result of ZAS.
Theorem 3.	[Continuous-time gradient descent without weight sharing (Wu et al., 2019)] For
deep linear network f(x; W1, ..., WL) = WLWL-1...W1x with zero-asymmetric initialization and
discrete-time gradient descent, if the learning rate satisfies η ≤ min {(4L3ξ6) 1, (l44L2ξ4) 1},
where ξ = max {2|悭|旧，3L-1/2, 1}, then we have linear convergence R(t) ≤(1 一 2)t R(0).
Since the learning rate is η = O(L-3), Theorem 3 indicates that the gradient descent can achieve
R(t) ≤ within O(L3 log(1/)) steps.
In the special case of learning a positive definite matrix Φ, initialize all weights Wl to be the same
and train with weights sharing, we have the following convergence result.
Theorem 4.	[Discrete-time gradient descent with weight sharing] For the deep linear network
f(x; W1, ..., WL) = WLWL-1...W1x, initialize all Wl(0) with identity matrix I and update ac-
cording to Equation 2. With a positive definite target matrix Φ, and setting η ≤ ^√^m2nλmn黑)；))i)，
we have linear convergence R(t) ≤ exp [—(2L — 2) min Gmin(Φ), 1)ηt∖ R(0).
Take λmin(Φ)∕λmax(Φ), d as constants and focus on the scaling with L, e, We have η = O(L-2).
Because of the extra L in the exponent, we know that when learning a positive definite matrix Φ,
training with weight sharing can achieve R(t) ≤ within O (L log (1/)) steps. The dependency on
L reduces from previous L3 to linear, which shows the acceleration of training by weight sharing.
4 Related Work
Lan et al. (2020) propose ALBERT with the weights being shared across all its transformer layers.
Large ALBERT models can achieve good performance on several natural language understanding
benchmarks. Bai et al. (2019b) propose trellis networks which are temporal convolution networks
with shared weights and obtain good results for language modeling. This line of work is then ex-
tended to deep equilibrium models (Bai et al., 2019a) which are equivalent to infinite-depth weight-
tied feedforward networks. Dabre & Fujita (2019) show that the translation quality of a model that
recurrently stacks a single layer is comparable to having the same number of separate layers. Zhang
et al. (2020) also demonstrate the application of weight-sharing in neural architecture search.
Deep linear models have been widely studied for its simplicity and similarity to deep learning mod-
els. Baldi & Hornik (1989) show that all local minima are also global minima for two-layer linear
networks. Laurent & Brecht (2018) extend the same result to deep linear networks. Hardt & Ma
5
Under review as a conference paper at ICLR 2021
(2017) show the PL condition is satisfied within the neighbour of a global optimum. Shamir (2019)
show that, for one-dimensional deep linear networks, with the Xavier or near-identity initialization,
it requires at least exp(Ω(L)) steps to converge, where L is the depth. WU et al. (2019) show that
this result can be improved to O(L3 log 1/) with a special zero-asymmetric initialization.
5	Experiments
In this section, we present the experimental setup and results for training the BERT Large model with
the standard training procedure as in the literature as well as our Sharing WEights (SWE) method.
In what follows, without explicit clarification, BERT always means the BERT Large model.
5.1	Experimental Setup
We use the TensorFlow official implementation of BERT (team & contributors). We first show
experimental results with English Wikipedia and BookCorpus for pre-training as in the original
BERT paper (Devlin et al., 2019). We then move to the XLNet enlarged pretraining dataset (Yang
et al., 2019). We preprocess all datasets with WordPiece tokenization (Schuster & Nakajima, 2012).
We mask 15% tokens in each sequence. For experiments on English Wikipedia and BookCorpus,
we randomly choose tokens to mask. For experiments on the XLNet dataset, we do whole word
masking - in case that a word is broken into multiple tokens, either all tokens are masked or not
masked. For all experiments, we set both the batch size and sequence length to 512.
We use the AdamW optimizer (Loshchilov & Hutter, 2019) with the weight decay rate being 0.01,
β1 = 0.9, and β2 = 0.999. For English Wikipedia and BookCorpus, we use Pre-LN (He et al.,
2016b; Wang et al., 2019b) instead of the original BERT’s Post-LN. Note that the correct implemen-
tation of Pre-LN contains a final layer-norm right before the final classification/masked language
modeling layer. Unlike the claim made by Xiong et al. (2020), we notice that using Pre-LN with
learning rate warmup leads to better performance. In our implementation, the learning rate starts
from 0.0, linearly increases to the peak value 3 × 10-4, the learning rate used by Xiong et al. (2020),
at the 10k-th iteration, and then linearly decays to 0.0. For the XLNet dataset, we apply the same
Pre-LN setup except the peak learning chosen to be 2 × 10-4. The peak learning rate of 3 × 10-4
makes training unstable here and yields worse performance than 2 × 10-4 .
After pre-training, we fine-tune the models for the Stanford Question Answering Dataset (SQuAD
v1.1 and SQuAD v2.0) (Rajpurkar et al., 2016) and the GLUE benchmark (Wang et al., 2019a). For
all fine-tuning tasks, we follow the setting as in the literature: the model is fine-tuned for 3 epochs;
the learning rate warms up linearly from 0.0 to peak in the first 10% of the training iterations,
then linearly decay to 0.0. We select the best peak learning rate based on the validation set from
{1×10-5,1.5×10-5,2×10-5,3×10-5,4×10-5,5×10-5,7.5×10-5,10×10-5,12×10-5}.
For the SQuAD datasets, we fine-tune each model 5 times and report the average. For the GLUE
benchmark, for each training method, we simply train one BERT model and submit the model’s
predictions over the test sets to the GLUE benchmark website to obtain test results. We observed
that when using Pre-LN, the GLUE finetuning process is stable and no model diverged.
Training methods. The training procedure in the TensorFlow official implementation of BERT
serves as our baseline, where the baseline training takes 1 million steps (both on English Wikipedia
plus BookCorpus and on the enlarged XLNet dataset). For our Sharing WEight (SWE) method,
only half of the number of iterations is taken. For a complete comparison, we also report the results
from the baseline method with half of the number of iterations. Three versions of our method with
hyperparameter settings are listed below.
SWE-F Fixed point untying. It serves as a baseline of its adaptive version. We ran experiments
with a set of different untying points, and identified the best untying point τ = 50k. The
full results from different τ values are presented in Section 5.3.1.
SWE-A Adaptive untying. We check the gradient correlations for every 1000 iterations. If the
gradient correlation of adjacent layers is below a threshold ρ = 0.5 for three consecutive
times, we break the tie. The effect of different ρ values is studied in Section 5.3.1.
6
Under review as a conference paper at ICLR 2021
SWE-S Simplified adaptive untying. We use the same setup as in SWE-A, except we break the tie
all at once, when the majority of gradient correlations are below the threshold ρ for three
consecutive times.
5.2	Experiment Results
For English Wikipedia and BookCorpus, both pretraining and finetuning results of our method vs.
the baseline method are shown in Table 1. From the results, we see that our method with 500k
training iterations matches the performance of the baseline method with 1 million training iterations,
and significantly outperforms the baseline method with 500k training iterations. The results for the
XLNet dataset are shown in Table 2. We observe similar advantages of our approach over the
baseline.
Table 1: Training BERT on English Wikipedia and BookCorpus. Our method with half of a million
iterations matches the baseline performance with one million iteration steps, and outperforms the
baseline with half of a million iterations.
	Baselines		Our method, 0.5m iter.		
	1m iter.	0.5m iter.	SWE-F	SWE-A	SWE-S
Pretrain MLM (acc.%)	74.98	-^73.66-	73.92	74.03	74.13
SQuAD v1.1 (F-1%)	92.58	-^9154-	92.54	92.31	92.54
SQuAD v2.0 (F-1%)	85.06	82.99	84.14	84.16	84.79
GLUE/AX (corr%)	-423^^	390-	-^0∏^^	42.4	41.8
GLUE/MNLI-m (acc.%)	86.9	85.9	87.2	87.4	87.2
GLUE/MNLI-mm (acc.%)	86.1	85.4	86.7	86.8	86.7
GLUE/QNLI (acc.%)	93.5	92.3	93.4	93.2	92.9
GLUE/QQP (F-1%)	72.2	72.1	71.7	71.8	71.5
GLUE/SST-2 (acc.%)	94.8	94.3	94.8	95.6	94.2
Table 2: Training BERT on the XLNet dataset. Our method with half of a million iterations matches
the baseline performance with one million iteration steps, and outperforms the baseline performance
with half of a million iterations.
	Baselines		Our method, 0.5m iter.		
	1m iter.	0.5m iter.	SWE-F	SWE-A	SWE-S
Pretrain MLM (acc.%)	71.75	-^70:06-	70.18	70.92	70.60
SQuAD v1.1 (F-1%)	93.37	-^92:41-	92.82	92.65	92.99
SQuAD v2.0 (F-1%)	86.49	85.28	85.57	85.41	86.12
GLUE/AX (corr%)	-442^^	-^432-	43.7^^	44.1	43.3
GLUE/MNLI-m (acc.%)	88.6	87.3	88.4	87.7	88.2
GLUE/MNLI-mm (acc.%)	87.9	87.4	87.4	87.4	87.6
GLUE/QNLI (acc.%)	92.3	91.9	92.8	92.8	93.2
GLUE/QQP (F-1%)	72.3	71.7	72.1	72.2	72.0
GLUE/SST-2 (acc.%)	95.9	95.7	95.4	95.9	96.0
5.3	Ablation studies
In this section, we study the effects of different choices in implementing our method.
5.3.1	When to Stop Weight Sharing
In this section, we study the effects of using different untying points (Algorithm 1) and thresholds
(Algorithm 2). If weights are shared throughout the entire pretraining process, the final performance
7
Under review as a conference paper at ICLR 2021
will be much worse than without any form of weight sharing (Lan et al., 2020). On the other hand,
without weight sharing at all yields slower convergence.
Results of using different untying point τ and threshold ρ values are summarized in Table. 3. Models
are trained for 500k iterations on English Wikipedia and BookCorpus. From the results, we see that
for the SWE-F method, a smaller τ value performs better than a larger τ . This means that the weight
sharing stage should not be too long. We also see that the performance of the SWE-A method is not
sensitive to the threshold ρ.
Table 3: Results from different untying points τ and thresholds ρ. Models are trained for 500k
iterations on English Wikipedia and BookCorpus.
	T=25k	SWE-F τ =50k	τ=200k	SWE-A				ρ=0.9
				ρ=0.1	ρ=0.3	ρ=0.5	ρ=0.7	
Pretrain MLM (acc.%)	71.48	73.92	72.42	73.76	74.11	74.03	74.12	73.71
SQuAD v1.1 (F-1%)	90.51	92.54	91.65	92.29	92.15	92.31	92.16	92.17
SQuAD v2.0 (F-1%)	81.11	84.14	83.35	84.69	84.30	84.16	83.38	84.21
GLUE/AX (corr%)	37.0	40.1	42.6	40.0	41.5	42.4	40.2	40.3
GLUE/MNLI-m (acc.%)	86.1	87.2	85.5	86.9	87.4	87.4	87.0	86.5
GLUE/MNLI-mm (acc.%)	83.9	86.7	85.4	86.1	86.4	86.8	86.9	86.6
GLUE/QNLI (acc.%)	92.8	93.4	91.9	93.3	93.3	93.2	93.3	92.7
GLUE/QQP (F-1%)	70.3	71.7	70.8	71.4	71.6	71.8	71.9	72.0
GLUE/SST-2 (acc.%)	93.4	94.8	93.5	93.4	94.7	95.6	95.2	94.9
5.3.2	How to choose weight sharing units
Note that it is not necessary to be restricted to share weights only across the original layers. We can
group several consecutive layers as a weight sharing unit. We denote A × B as grouping A layers as
a weight sharing unit which is being shared with B times. Since BERT has 24 layers, the baseline
method without weight sharing can be viewed as “24x1”, and our method shown in Table 1 can be
viewed as “1x24”. We present results from more different choices of weight sharing units in Table
4. We can see that, in order to achieve good results, the size of the chosen weight sharing unit should
not be larger than 6 layers. This means that the weights of a layer must be shared for at least 4 times.
Table 4: We group several consecutive layers as a weight sharing unit instead of sharing weights
only across original layers. A × B means grouping A layers as a unit which is being shared with B
times. Models are trained for 500k iterations on English Wikipedia and BookCorpus.
	Baseline 24x1	12x2	6x4	SWE-F 4x6	2x12	1x24
Pretrain MLM (acc.%)	73.66	73.82	73.99	73.90	74.16	73.92
SQuAD v1.1 (F-1%)	91.54	92.18	92.62	92.52	92.44	92.54
SQuAD v2.0 (F-1%)	82.99	83.74	84.56	85.82	85.06	84.14
GLUE/AX (corr%)	-390-	40.7	40.3	40.2	43.0	40.1
GLUE/MNLI-m (acc.%)	85.9	86.9	87.9	87.0	87.1	87.2
GLUE/MNLI-mm (acc.%)	85.4	85.8	87.0	86.4	86.4	86.7
GLUE/QNLI (acc.%)	92.3	93.1	93.2	92.9	93.8	93.4
GLUE/QQP (F-1%)	72.1	72.0	72.0	71.6	71.8	71.7
GLUE/SST-2 (acc.%)	94.3	94.7	94.3	94.8	94.7	94.8
6	Conclusion
We proposed a simple weight sharing method to speed up the training of deep networks with repeated
layers and showed promising empirical results on BERT training. Our method is motivated by the
successes of weight sharing models in the literature as well as our theoretic analysis on deep linear
8
Under review as a conference paper at ICLR 2021
models. For future work, we will extend our empirical studies to other deep learning models and
tasks, and analyze under which conditions our method will be helpful.
References
Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric Battenberg, Carl
Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-
end speech recognition in english and mandarin. In International conference on machine learning,
pp.173-182, 2016.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural
Information Processing Systems, pp. 690-701, 2019a.
Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In Inter-
national Conference on Learning Representations, 2019b.
Pierre Baldi and Kurt Hornik. Neural networks and principal component analysis: Learning from
examples without local minima. Neural networks, 2(1):53-58, 1989.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019.
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Raj Dabre and Atsushi Fujita. Recurrent stacking of layers for compact neural machine translation
models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 6292-
6299, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pp. 4171-
4186, 2019.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Moritz Hardt and Tengyu Ma. Identity matters in deep learning. In International Conference for
Learning Representations, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016b.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference for Learning Representations, 2015.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Sori-
cut. ALBERT: A lite BERT for self-supervised learning of language representations. In Interna-
tional Conference on Learning Representations, 2020.
Thomas Laurent and James Brecht. Deep linear networks with arbitrary loss: All local minima are
global. In International conference on machine learning, pp. 2902-2907. PMLR, 2018.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional
computation and automatic sharding. arXiv preprint arXiv:2006.16668, 2020.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2019.
9
Under review as a conference paper at ICLR 2021
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. Journal ofMachine Learning Research, 21(140):1-67, 2020.
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100, 000+ questions
for machine comprehension of text. In EMNLP, 2016.
Mike Schuster and Kaisuke Nakajima. Japanese and Korean voice search. In IEEE International
Conference on Acoustics, Speech and Signal Processing, 2012.
Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear
neural networks. In Conference on Learning Theory, pp. 2691-2713. PMLR, 2019.
Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool,
Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. Mesh-TensorFlow: Deep
learning for supercomputers. In Advances in Neural Information Processing Systems, pp. 10414-
10423, 2018.
TensorFlow team and contributors. TensorFlow model garden NLP. github.com/
tensorflow/models/tree/master/official/nlp.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
International Conference on Learning Representations, 2019a.
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao.
Learning deep transformer models for machine translation. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pp. 1810-1822, 2019b.
Lei Wu, Qingcan Wang, and Chao Ma. Global convergence of gradient descent for deep linear
residual networks. In Advances in Neural Information Processing Systems, pp. 13389-13398,
2019.
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,
Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-
lation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,
Yanyan Lan, Liwei Wang, and Tie-Yan Liu. On layer normalization in the transformer architec-
ture. In International Conference on Machine Learning, 2020.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
XLNet: Generalized autoregressive pretraining for language understanding. In Advances in neu-
ral information processing systems, pp. 5753-5763, 2019.
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training bert in 76 minutes. In International Conference on Learning Representations,
2020.
Yuge Zhang, Zejun Lin, Junyang Jiang, Quanlu Zhang, Yujing Wang, Hui Xue, Chen Zhang, and
Yaming Yang. Deeper insights into weight sharing in neural architecture search. arXiv preprint
arXiv:2001.01431, 2020.
10
Under review as a conference paper at ICLR 2021
A	Proofs of Section 3
A.1 Proof of Lemma 1
Proof. By definition, we have
WR(t) = Wo(t)LT(WL(t) - Φ) W0-1(t).
To see the result in Lemma 1, it is sufficient to show that W0(t) has the same eigenvectors as Φ.
First, it is not hard to see that by initializing W0 = I, the requirement trivially holds for W0(0).
Now, suppose W0(t0) has the same eigenvectors as Φ, then from the definition of update
V1R(t0) = W0L-l(to) (W0L(to) - Φ) W0-1(to).
We thus know that V1R(t0) has the same eigenvectors as Φ for all layer l. Therefore the update for
W0 (t0), which is given by PiL=1 ViR(t0)/L also has the same eigenvectors as Φ. Inductively, it
indicates that from time step t0, all subsequent weights W0 (t) has the same eigenvectors as Φ.
We can thus exchange the order of matrix multiplication and for all t ≥ 0, we have
VιR(t) = Wo(t)LT(WL(t) - Φ) W0-1(t) = W0L-1(t) (W0L(t) — Φ),
which directly implies that
L
Wι(t + 1) - Wl(t) = -η X ViR(t) = -ηwL-1(t) (wL(t) — Φ).
L i=1
A.2 Proof of Theorem 2
Proof. From Lemma 1, we have
L
Wι(t) = - X VlR(t)∕L = -WLT (t) (W0L(t) - Φ).
l=1
For the loss function R(t), we have
LL
R(t) = Xtr (V>R(t)Wι(t)) = - X kVlR(t)kF
L
=-X kWo(t)L-1 (Wo (t)L- Φ) kF
l=1
≤ - 2Lλ2min(W0(t)L-1)R(t).
By continuous gradient descent with W0(0) = I, it is easy to see that
λmin(W0L-1(t)) ≥ min(1, λmin(Φ)).
Therefore we have
R(t) ≤-2Lmin(1,λmin(Φ))R(t) =⇒ R(t) ≤ e-2Lmin(I,λmλφDtR(0).
A.3 Technical Lemma
Lemma 2. Initializing W0 = I and training with weight sharing update (Equation 2), by setting
η ≤ L max(λl(Φ),1)，wehave
λ min (Wθ(t)) ≥ min(λ min (Φ)1a, 1),	λ tnax (W0(t)) ≤ max(λ max ®)1/L, 1).
11
Under review as a conference paper at ICLR 2021
Proof. In the proof of Lemma 1, we know that W0 (t) has the same eigenvectors as Φ. Take any
eigenvector, denote λ(t) and λL to be the corresponding eigenvalue of W0(t) and Φ. By Lemma 1,
we have
bλ(t + 1) = λb(t) - ηλb(t)L-1 bλ(t)L - λL .
For λ > 1, we would like to show λ(t) ∈ [1, λ],∀t ≥ 0 by setting η ≤ L12L. Since λ(0) = 1, We
know this claim holds trivially at t = 0. Then suppose we have the claim holds for t = t0, then
L-1
bλ(t0 + 1) = bλ(t0) - η bλ(t0)L-1 bλ(t0) - λ X bλ(t0)iλL-1-i
i=0
E	1个/,	.	/ 1	ι 1
To make λ(to + 1) ∈ [1, λ], we set η ≤ ^^ and have
λb(tO + 1) ≤ λb(tO ) -	b(to)LT PL=O1 b(to)iλLT-i Lλ2L	(λb(tO)	- λ)
≤ λb(tO) -	b(to)LT PL=OI b(to)iλLT-i λLT p31 λLT	(λb(tO)	- λ) ≤ λ
A 1	、 C	J .个/ ,	, T、、个 /,、、 T C .	1 J	.	、一 T	,	个 /,、_
And η ≥ 0 guarantees that λ(t0 + 1) ≥ λ(t0) ≥ 1. By induction, when λ > 1, we haveλ(t) ∈
[1, λ], ∀t ≥0.
1
Similarly, for λ < 1, we would like to show λ(t) ∈ [λ, 1] by setting η ≤ L. Note again the claim
1 11 . ∙ ∙ 11	1	>	CC	个/ ,、一 「'ri	1
holds trivially when t = 0. Suppose λ(t0) ∈ [λ, 1], we have
b(t0 + 1) ≥ b(t0) - b(tO)LT PP⅞0"LT-i (b(t0) - λ) ≥ λ.
1 i=0 1
And η ≥ 0 guarantees that λ(t0 + 1) ≤ λ(t0) ≥ 1. By induction, when λ < 1, we haveλ(t) ∈
[λ, 1], ∀t ≥0.
Note that the two claims hold for all λ, it then directly implies that by setting η ≤ L max^2 ㈤ 1),
we have
λmin (Wθ(t)) ≥ min(λmin(Φ)1a, 1),	λmax (W0(t)) ≤ maX(》max®)1/L, 1),
which completes the proof.
A.4 Proof of Theorem 4
Proof. Denote φ = max(λmax(Φ), 1). From the proof of Lemma 1, we have
Vl R(t) = wL-1(t)(wL(t)- Φ),
By Lemma 2, setting η ≤ L max(λmax(Φ),1), We have
λmin (Wθ(t)) ≥ min(λmin(Φ)“L, 1),	λmax (Wθ(t)) ≤ max(λmax(Φ)14, 1).
Denote φ = max(λmax(Φ), 1), we immediately have
kVlR(t)kF ≤ φ(Lτ”Lp2R(t),	kVlR(t)kF ≥ min(λmin(Φ), 1)(LT"L,2R(t).
With one step of gradient update, we have
R(t + 1) - R(t)=2 U(WO- ηvoR(t))L — φkF -IlWO- φkFi
=((Wo - ηVoR(t))L - WL) Θ (WL - Φ) + 1 k (Wo - ηV0R(t))L - WLkF,
where the denotes the element-wise multiplication. Let
LL
I1 = ηA1 θ (WL - φ) ,	I2 = X ηkAk θ (WL - φ) ,	I3 = 2 k X ηkAk IlF,
k=2	k=1
12
Under review as a conference paper at ICLR 2021
where the matrix Ak comes from
(Wo - ηV0R(t))L = Ao + ηAι + …+ #Al.
We have
R(t + 1) - R(t) ≤ I1 + I2 + I3 .
Note that
kW0k2 ≤ O1/L,	kVoR(t)kF ≤ φ(LT"LP2R(t).
Using the fact that for 0 ≤ y ≤ x/L2,
(x + y)L ≤ xL + 2LxL-1y,	(x + y)L ≤ xL + LxL-1y + L2xL-2y2.
Take η ≤
1
L2φ√2R(t)
, we have
L
k X ηk AkkF ≤ (φ1∕L + ηφL- p2R(t)
k=1
L
k X ηk AkkF ≤ (φ1∕L + ηφL- p2R(t)
k=2
Thus we have
L	C /_____
—φ ≤ 2ηLφ /2R(t)
L — φ — ηLφ2(L-I)∕L,2R(t) ≤ 2η2L2φ3R(t).
L
I2 ≤ k XηkAkkFkWo — ΦkF ≤ 2√2η2L2φ2R(B/
k=2
I3 ≤ 2k XηkAkkF ≤ 4η2L2φ4R(t).
k=1
For I1 , we directly have
Ii = —ηLVoR(t) Θ WLT (WL — Φ) = —ηL∣∣VoR(t)kF.
Since We have ∣∣V0R(t)kF ≥ min (λmin (Φ), 1) p2R(t). Therefore by setting
≤ min(	1______________1	mincmMφbI) mincmMφbI) ʌ
/一 LL max (λmax (Φ),1), √2L2φR(t)” , 2√2L2 02R(t)1/2 ,	4L2φ4	J
We have
R(t + 1) — R(t) ≤ (2 — 2L)ηmin(λ2min(Φ), 1)R(t).
By directly setting
λ “	minCmin阐,I)
λ ≤ ---------------------,
—4√dL2 max(λmaχ(Φ), 1)
We can satisfy all the requirements above, Which Will give us
R(t) ≤ [1 — (2L — 2) min (入/口画, 1)η]t R(0) ≤ exp [—(2L — 2) min 加式①), 1)ηt] R(t).
13