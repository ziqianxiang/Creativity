Under review as a conference paper at ICLR 2021
Mutual Calibration between Explicit and Im-
plicit Deep Generative Models
Anonymous authors
Paper under double-blind review
Ab stract
Deep generative models are generally categorized into explicit models and im-
plicit models. The former defines an explicit density form that allows likelihood
inference; while the latter targets a flexible transformation from random noise to
generated samples. To take full advantages of both models, we propose Stein
Bridging, a novel joint training framework that connects an explicit (unnormal-
ized) density estimator and an implicit sample generator via Stein discrepancy.
We show that the Stein bridge 1) induces novel mutual regularization via kernel
Sobolev norm penalization and Moreau-Yosida regularization, and 2) stabilizes
the training dynamics. Empirically, we demonstrate that Stein Bridging can facil-
itate the density estimator to accurately identify data modes and guide the sample
generator to output more high-quality samples especially when the training sam-
ples are contaminated or limited.
1	Introduction
Deep generative model, as a powerful unsupervised framework for learning the distribution of high-
dimensional multi-modal data, has been extensively studied in recent literature. Typically, there are
two types of generative models: explicit and implicit (Goodfellow et al., 2014). Explicit models
define a density function of the distribution, while implicit models learn a mapping that generates
samples by transforming an easy-to-sample random variable.
Both models have their own power and limitations. The density form in explicit models endows them
with convenience to characterize data distribution and infer the sample likelihood. However, the
unknown normalizing constant often causes computational intractability. On the other hand, implicit
models including generative adversarial networks (GANs) can directly generate vivid samples in
various application domains including images, natural languages, graphs, etc. (Goodfellow et al.,
2014; Radford et al., 2016; Arjovsky et al., 2017; Brock et al., 2019). Nevertheless, one important
challenge is to design a training algorithm that do not suffer from instability and mode collapse. In
view of this, it is natural to build a unified framework that takes full advantages of the two models
and encourages them to compensate for each other.
Intuitively, an explicit density estimator and a flexible implicit sampler could help each other’s train-
ing given effective information sharing. On the one hand, the density estimation given by explicit
models can be a good metric that measures quality of samples (Dai et al., 2017), and thus can be
used for scoring generated samples given by implicit model or detecting outliers as well as noises
in input true samples (Zhai et al., 2016). On the other hand, the generated samples from implicit
models could augment the dataset and help to alleviate mode collapse especially when true samples
are insufficient that would possibly make explicit model fail to capture an accurate distribution. We
refer to Appendix A for a more comprehensive literature review.
Motivated by the discussions above, in this paper, we propose a joint learning framework that enables
mutual calibration between explicit and implicit generative models. In our framework, an explicit
model is used to estimate the unnormalized density; in the meantime, an implicit generator model is
exploited to minimize certain statistical distance (such as the Wasserstein metric or Jensen-Shannon
divergence) between the distributions of the true and the generated samples. On top of these two
models, a Stein discrepancy, acting as a bridge between generated samples and estimated densities,
is introduced to push the two models to achieve a consensus. Unlike flow-based models (Nguyen
et al., 2017; Kingma & Dhariwal, 2018; Papamakarios et al., 2017), our formulation does not impose
1
Under review as a conference paper at ICLR 2021
invertibility constraints on the generative models and thus is flexible in utilizing general neural
network architectures. Our main contribution are as follows.
•	Theoretically, we prove that our method allows the two generative models to impose novel mutual
regularization on each other. Specifically, our formulation penalizes large kernel Sobolev norm
of the critic in the implicit (WGAN) model, which ensures the critic not to change suddenly
on the high-density regions and thus preventing the critic of the implicit model being to strong
during training. In the mean time, our formulation also smooths the function given by the Stein
discrepancy through Moreau-Yosida regularization, which encourages the explicit model to seek
more modes in the data distribution and thus alleviates mode collapse.
•	In addition, we also show that the joint training helps to stabilize the training dynamics. Compared
with other common regularization approaches for GAN models that may shift original optimum,
our method can facilitate convergence to unbiased model distribution.
•	Extensive experiments on synthetic and image datasets justify our theoretical findings and demon-
strate that joint training can help two models achieve better performance. On the one hand,
the energy model can detect complicated modes in data more accurately and distinguish out-of-
distribution samples. On the other hand, the implicit model can generate higher-quality samples,
especially when the training samples are contaminated or limited.
2	Background
We briefly provide some technical background related to our model.
Energy Model. The energy model assigns each data x ∈ Rd with a scalar energy value Eφ(x),
where Eφ(∙) is called energy function and is parameterized by φ. The model is expected to assign
low energy to true samples according to a Gibbs distribution pφ(x) = exp{-Eφ(x)}∕Zφ, where Zφ
is a normalizing constant dependent of φ. The normalizing term Zφ is often hard to compute, making
the training intractable, and various methods are proposed to detour such term (see Appendix A).
Stein Discrepancy. Stein discrepancy (Gorham & Mackey, 2015; Liu et al., 2016; Chwialkowski
et al., 2016; Oates et al., 2017; Grathwohl et al., 2020) is a measure of closeness between two
probability distributions and does not require knowledge for the normalizing constant of one of the
compared distributions. Let P and Q be two probability distributions on X ⊂ Rd, and assume Q has
a (unnormalized) density q. The Stein discrepancy S(P, Q) is defined as
S(P,Q):= sup Ex〜p[AQf(x)]:= sup {Γ(Eχ〜p[Vχlogq(x)f(x)> + Vχf(x)])},	(1)
f∈F	f∈F
where F is often chosen to be a Stein class (see, e.g., Definition 2.1 in (Liu et al., 2016)),
f : Rd → Rd0 is a vector-valued function called Stein critic and Γ is an operator that transforms a
d × d0 matrix into a scalar value. One common choice of Γ is trace operator when d0 = d. One can
also use other forms for Γ, like matrix norm when d0 6= d (Liu et al., 2016). If F is a unit ball in
some reproducing kernel Hilbert space (RKHS) with a positive definite kernel k, it induces Kernel
Stein Discrepancy (KSD). More details are provided in Appendix B.
Wasserstein Metric. Wasserstein metric is suitable for measuring distances between two distri-
butions with non-overlapping supports (Arjovsky et al., 2017). The Wasserstein-1 metric between
distributions P and Q is defined as
W (P, Q) := mγin E(x,y)〜Y [kx - yk],
where the minimization with respect to γ is over all joint distributions with marginals P and Q. By
Kantorovich-Rubinstein duality, W(P, Q) has a dual representation
W (P, Q) ：= max {Ex 〜p[D(x)] - Ey 〜Q[D(y)]} ,	(2)
where the maximization is over all 1-Lipschitz continuous functions.
Sobolev space and Sobolev dual norm. Let L2 (P) be the Hilbert space on Rd equipped with
an inner product hu, viL2(P) := Rd uvdP(x). The (weighted) Sobolev space H1 is defined as
the closure of C0∞, a set of smooth functions on Rd with compact support, with respect to norm
kU∣Hι ：= (RRd (u2 + kVu∣∣2)dP(x))1/2, where P has a density. For V ∈ L2, its Sobolev dual norm
2
Under review as a conference paper at ICLR 2021
kvkH-1 is defined by (Evans, 2010)
kv∣∣H-ι := sup Ihv,u〉L2 : / ∣∣Vuk2 dP(x) ≤ 1, / u(x)dP(x) = 0 >.
u∈H1	Rd	Rd
The constraint Rd u(x)dx = 0 is necessary to guarantee the finiteness of the supremum, and the
supermum can be equivalently taken over C0∞ .
3 Proposed Model： STEIN Bridging
In this section, We formulate our model
Stein Bridging. A scheme of our frame-
work is illustrated in Figure 1. Denote by
Prealthe underlying real distribution from
which the data {x} are sampled. The for-
mulation simultaneously learns two gener-
ative models - one explicit and one im-
plicit 一 that represent estimates of Preal.
The explicit generative model has a dis-
tribution PE on X with explicit probabil-
ity density proportional to exp(-E(x)),
X ∈ X, where E is referred to as an en-
ergy function. We focus on energy-based
explicit model in model formulation as it
Implicit Model PG	Explicit Model PE
-----D Data Flow With BP - - U Update & Impact
Dl = W(Prefl7iD1G) 。2 = (S(Prcub¾) P3 = 5(Pf7,Ps)
Figure 1: Model framework for Stein Bridging.
does not enforce any constraints or assume specific density forms. For specifications, one can also
consider other explicit models, like autoregressive models or directly using some density forms such
as Gaussian distribution with given domain knowledge. The implicit model transforms an easy-to-
sample random noise z with distribution P0 via a generator G to a sample xe = G(z) with distribution
PG. Note that for distribution PE, we have its explicit density without normalizing term, while for
PG and Preal, we have samples from two distributions. Hence, we can use the Stein discrepancy
(that does not require the normalizing constant) as a measure of closeness between the explicit dis-
tribution PE and the real distribution Preal, and use the Wasserstein metric (that only requires only
samples from two distributions) as a measure of closeness between the implicit distribution PG and
the real data distribution Preal .
To jointly learn the two generative models PG and PE, arguably the most straightforward way is to
minimize the sum of the Stein discrepancy and the Wasserstein metric:
min W(Preal,PG)+λS(Preal,PE),
E,G
where λ ≥ 0. However, this approach appears no different than learning the two generative mod-
els separately. To achieve information sharing between two models, we incorporate another term
S (Pg, PE) — called the Stein bridge - that measures the closeness between the explicit distribution
PE and the implicit distribution PG:
min W(Preal,PG)+λ1S(Preal,PE)+λ2S(PG,PE),	(3)
E,G
where λ1 , λ2 ≥ 0. The Stein bridge term in (3) pushes the two models to achieve a consensus.
Remark 1. our formulation is flexible in choosing both the implicit and explicit models. In (3), we
can choose statistical distances other than the Wasserstein metric W(Preal, PG) to measure closeness
between Preal and PG, such as Jensen-Shannon divergence, as long as its computation requires only
samples from the involved two distributions. Hence, one can use GAN architectures other than
WGAN to parametrize the implicit model. In addition, one can replace the first Stein discrepancy
term S(Preal, PE) in (3) by other statistical distances as long as its computation is efficient and hence
other explicit models can be used. For instance, if the normalizing constant of PE is known or easy
to calculate, one can use Kullback-Leibler (KL) divergence.
Remark 2. The choice of the Stein discrepancy for the bridging term S(PG, PE) is crucial and
cannot be replaced by other statistical distances such as KL divergence, since the data-generating
distribution does not have an explicit density form (not even up to a normalizing constant). This
3
Under review as a conference paper at ICLR 2021
is exactly one important reason why Stein bridging was proposed, which requires only samples
from the data distribution and only the log-density of the explicit model without the knowledge of
normalizing constant as estimated in MCMC or other methods.
In our implementation, we parametrize the generator in implicit model and the density estimator in
explicit model as Gθ(z) and pφ(x), respectively. The Wasserstein term in (3) is implemented using
its equivalent dual representation in (2) with a parametrized critic Dψ (x). The two Stein terms in
(3) can be implemented using (1) with either a Stein critic (parametrized as a neural network, i.e.,
fw(x)), or the non-parametric Kernel Stein Discrepancy. Our implementation iteratively updates the
explicit and implicit models. Details for model specifications and optimization are in Appendix E.2.
We also compare with some related works that attempt to combine both of the worlds (such as
energy-based GAN, contrastive learning and cooperative learning) in Appendix A.3.
4 Theoretical Analysis
In this section, we theoretically show that the Stein bridge allows the two models to facilitate each
other’s training by imposing certain regularizations on both the implicit and the explicit models, as
well as stabilizing the training dynamics.
4.1	Regularization via Stein Bridge
We first show the regularization effect of the Stein bridge on the Wasserstein critic. Define the kernel
Sobolev dual norm as
l|DkH-i(p;k):= sup {(D,u>L2(P) : Eχ,χ0〜p[Vu(x)>k(x, x0)Vu(x0)] ≤ 1, Ep[u] =0}.
u∈C0∞
which can be viewed as a kernel generalization of the Sobolev dual norm defined in Section 2, which
reduces to the Sobolev dual norm when k(x, x0) = I(x = x0) and P is the Lebesgue measure.
Theorem 1.	Assume that {PG}G exhausts all continuous probability distributions and S is chosen
as kernel Stein discrepancy. Then problem (3) is equivalent to
mEnmDax {Ey〜PE [D(y)] - Ex〜5[D(x)]-土 ∣D∣H-i(Pe；k)+ λιS(Preal,Pe)}.
The kernel Sobolev norm regularization penalizes large variation of the Wasserstein critic D. Par-
ticularly, observe that (Villani, 2008) if k(x, x0) = I(x = x0) and EPE [D] = 0, and then
lDlH-1(PE;k)
lim W2((1 + ©PE, PE)
→0
where W2 denotes the 2-Wasserstein metric. Hence, the Sobolev dual norm regularization ensures
D not to change suddenly on high-density region of PE , and thus reinforces the learning of the
Wasserstein critic. Stein bridge penalizes large variation of the Wasserstein critic, in the same spirit
but of different form comparing to gradient-based penalty (e.g., (Gulrajani et al., 2017; Roth et al.,
2017)). It prevents Wasserstein critic from being too strong during training and thus encourages
mode exploration of sample generator. To illustrate this, we conduct a case study where we train
a generator over the data sampled from a mixture of Gaussian (μι = [-1, -1], μ2 = [1,1] and
Σ = 0.2I). In Fig. 2(a) we compare gradient norms of the Wasserstein critic when training the
generator with and without the Stein bridge. As we can see, Stein bridge can help to reduce gradient
norms through training, with a similar effect as WGAN-GP.
Moreover, the Stein bridge also plays a part in smoothing the output from Stein discrepancy and we
show the result in the following theorem.
Theorem 2.	Assume {PG }G exhausts all continuous probability distributions, and the Stein class
defining the Stein discrepancy is compact (in some linear topological space). Then problem (3) is
equivalent to
min λ1S(P
real
,PE) + λ2 max Ex〜Preal
[(APEf)λ2(x)] ,
where (APE f)λ2(∙) denotes the (generalized) Moreau-Yosida regularization of the function APE f
with parameter λ2, i.e., (APE f)λ2 (x) = miny∈χ {Ape f (y) + = ||x — y||}.
4
Under review as a conference paper at ICLR 2021
(b)	(c)	(d)
Figure 2: (a) The gradient norm of Wasserstein critic with (blue) and without (red) the Stein bridge when data
are sampled from a mixture of Gaussian. (b) Contour of an energy model with one mode and empirical data
from a distribution with a different mode (blue dots); (c) & (d) Contours of the Stein critics between the two
distributions in (b) learned with and without the Stein bridge, respectively.
Note that (APEf)λ2 is LiPsChitz continuous With constant 1∕λ2. Hence, the Stein bridge, together
with the Wasserstein metric W(Preal, PG), plays as a Lipschitz regularization on the output of the
Stein oPerator APEf via Moreau-Yosida regularization. This suggests a novel regularization scheme
for Stein-based GAN. By smoothing the Stein critic, the Stein bridge encourages the energy model
to seek more modes in data instead of focusing on some dominated modes, thus alleviating mode-
collaPse issue. To illustrate this, We consider a case Where We have an energy model initialized With
one mode center and data samPled from distribution of another mode, as dePicted in Fig. 2(b). Fig.
2(c) and 2(d) comPare the Stein critics When using Stein bridge and not, resPectively. The Stein
bridge helPs to smooth the Stein critic, as indicated by a less raPidly changing contour in Fig. 2(c)
comPared to Fig. 2(d), learned from the data and model distributions Plotted in Fig. 2(b).
4.2	Training Stability
Figure 3: Numerical SGD uPdates of Stein
Bridging, WGAN and its variants With dif-
ferent regularizations.
In this subsection, We further shoW that Stein Bridging
could helP stabilize adversarial training betWeen genera-
tor and Wasserstein critic With a local convergence guar-
antee. As is knoWn, the training for minimax game in
GAN is difficult. When using traditional gradient meth-
ods, the training Would suffer from some oscillatory be-
haviors (GoodfelloW, 2017; Liang & Stokes, 2019; Zhang
& Yu, 2020). In order to better understand the oPti-
mization behaviors, We first comPare the behaviors of
WGAN, likelihood- and entroPy-regularized WGAN, and
our Stein Bridging under SGD via an easy to comPre-
hend toy examPle in one-dimensional case. Fig. 3 shoWs
numerical results that comPare the oPtimization behav-
iors of above methods. As We can see, Stein Bridging
achieves good convergence to the oPtimum Point, While
WGAN suffers from an oscillation instead of converging. EntroPy regularization (ER) can encour-
age the generator to seek more modes but Would make the model diverge in this case. By contrast,
likelihood regularization (LR) can helP for training stability but it changes the converging Point to
a biased distribution. A recently ProPosed variational annealing strategy (VA) (Tao et al., 2019)
for regularized GAN introduces a trade-off betWeen convergence and unbiased result. The detailed
discussions and Proofs are Presented in APPendix D.1. We also generalize the convergence results
to multi-dimensional bilinear system F (ψ, θ) = θ>Aψ - b>θ - c>ψ in APPendix D.2. Our
theoretical results indicate that Stein Bridging could stabilize the minimax training of GAN Without
changing its oPtimum. In the exPeriments, We Will emPirically validate our analysis.
5 Experiments
In this section, We conduct exPeriments1 to verify the effectiveness of ProPosed method from mul-
tifaceted vieWs. We consider tWo synthetic datasets With mixtures of Gaussian distributions: TWo-
Circle and TWo-SPiral. The first one is comPosed of 24 Gaussian mixtures that lie in tWo circles.
Such dataset is extended from the 8-Gaussian-mixture scenario Widely used in Previous PaPers, so
1The exPeriment codes Will be released.
5
Under review as a conference paper at ICLR 2021
(a) True (b) GAN (c) WGAN (d) DGM (e) Joint-JS (f) Joint-W
Figure 4:	Comparison for
density estimation. (a) True
densities of real distribution
and (b)〜(f) estimated densi-
ties given by the estimators
of different methods on Two-
Circle (upper line) and Two-
Spiral (bottom line) datasets.
Figure 5:	Comparison for
generated sample quality. (a)
True samples from real dis-
tribution and (b)〜(f) gener-
ated samples produced by the
generators of different meth-
ods on Two-Circle (upper line)
and Two-Spiral (bottom line)
datasets.
that we can use it to test the quality of generated samples and mode coverage of learned energy.
The second dataset consists of 100 Gaussian mixtures whose centers are densely arranged on two
centrally symmetrical spiral-shaped curves. This dataset can be used to examine the power of gen-
erative model on complicated data distributions. The ground-truth distributions and samples are
shown in Fig. 4(a) and Fig. 5(a). Furthermore, we also apply the method to MNIST and CIFAR
datasets which require the model to deal with high-dimensional image data. In each dataset, we use
observed samples as input of the model and leverage them to train our model. The details for each
dataset are reported in Appendix E.1.
We term the model Joint-W if using Wasserstein metric in (3) and Joint-JS if using JS divergence in
this section. We consider several competitors. For implicit generative models, we basically consider
the counterparts without joint training with energy model, which are equivalently valina GAN and
WGAN with gradient penalty (Gulrajani et al., 2017), for ablation study. Also, as comparison to the
new regularization effects by Stein Bridging, we consider a recently proposed variational annealing
regularization (Tao et al., 2019) for GANs (short as GAN+VA/WGAN+VA). We employ denois-
ing auto-encoder to estimate the gradient for regularization penalty, which is proposed by (Alain &
Bengio, 2014). For explicit models, we also consider the counterparts without joint training with
generator model, i.e., directly training Deep Energy Model (DEM) using Stein discrepancy (Grath-
wohl et al., 2020). Besides we compare with energy calibrated GAN (EGAN) (Dai et al., 2017) and
Deep Directed Generative (DGM) Model (Kim & Bengio, 2017) which adopt contrastive divergence
to train a sample generator with an energy estimator. See Appendix A for brief introduction of these
methods and Appendix E.3 for implementation details.
5.1	Density Estimation of Explicit Model
Mode Coverage for Complicated Distributions. One advantage of joint learning is that the gener-
ator could help the density estimator to capture more accurate distribution. As shown in Two-Circle
case in Fig 5, both Joint-JS and Joint-W manage to capture all Gaussian components while other
methods miss some of modes. In Two-Spiral case in Fig 4, Joint-JS and Joint-W exactly fit the
ground-truth distribution. Nevertheless, DEM misses one spiral while EGAN degrades to a uniform-
like distribution. DGM manages to fit two spirals but allocate high densities to regions that have low
densities in the groung-truth distribution. As quantitative comparison, we study three evaluation
metrics: KL & JS divergence and Area Under the Curve (AUC). The detailed information and re-
sults are given in Appendix E.4 and Table 5 respectively. The values show that Joint-W and Joint-JS
provide better density estimation than all competitors over a large margin .
Density Rankings for High-Dimensional Digits. We also rank generated digits (and true digits) on
MNIST w.r.t densities given by the energy model in Fig. 11, Fig. 12 and Fig. 13. As depicted in the
figures, the digits with high densities (or low densities) given by Joint-JS possess enough diversity
(the thickness, the inclination angles as well as the shapes of digits diverses). By constrast, all the
digits with high densities given by DGM tend to be thin and digits with low densities are very thick.
6
Under review as a conference paper at ICLR 2021
Also, as for EGAN, digits with high (or low) densities appear to have the same inclination angle (for
high densities, ‘1’ keeps straight and ‘9’ ’leans’ to the left while for low densities, just the opposite),
which indicates that DGM and EGAN tend to allocate high (or low) densities to data with certain
modes and miss some modes that possess high densities in ground-truth distributions. By contrast,
our method manages to capture these complicated features in data distributions.
Detection for Out-of-distribution Samples. We further study model performance on detection
for out-of-distribution samples. We consider CIFAR-10 images as positive samples and construct
negative samples by (I) flip images, (II) add random noise, (III) overlay two images and (IV) use
images from LSUN dataset, respectively. A good density models trained on CIFAR-10 are expected
to give high densities to positive samples and low densities to negative samples, with exception for
case (I) (flipping images are not exactly negative samples and the model should give high densities).
We use the density values rank samples and calculate AUC of false positive rate v.s. true positive
rate, reported in Table 2. Our model Joint-W manages to distinguish samples for (II), (III), (IV) and
is not fooled by flipping images, while DEM and EGAN fail to detect out-of-distribution samples
and DGM recognizes flipping images as negative samples.
5.2	Sample Quality of Implicit Model
Generated Samples over Synthetic Datasets.
Calibrating explicit (unnormalized) density
model with implicit generator is expected to
improve the quality of generated samples. In
Fig. 5 we show the results of different genera-
tors in Two-Circle and Two-Spiral datasets. In
Two-Circle, there are a large number of gener-
ated samples given by GAN, WGAN-GP and
DGM locating between two Gaussian compo-
nents, and the boundary for each component
is not distinguishable. Since the ground-truth
densities of regions between two components
Table 1: Inception Scores (IS) and Frechet Incep-
tion Distance (FID) on CIFAR-10.
Method	IS	FID
WGAN-GP	6.74±0.041	42.2±0.572
Energy GAN	6.89±0.081	45.6±0.375
WGAN+VA	6.90±0.058	45.3±0.307
DGM	6.51±0.041	48.8±0.492
Joint-W(ours)	7.12±0.101	41.0±0.546
very low, such generated samples possess low-
quality, which depicts that these models capture the combinations of two dominated features (i.e.,
modes) in data but such combination makes no sense in practice. By contrast, Joint-JS and Joint-W
could alleviate such issue, reduce the low-quality samples and produce more distinguishable bound-
aries. In Two-Spiral, similarly, the generated samples given by GAN and WGAN-GP form a circle
instead of two spirals while the samples of DGM ‘link’ two spirals. Joint-JS manages to focus more
on true high densities compared to GAN and Joint-W provides the best results. To quantitatively
measure the sample quality, we adopt Maximum Mean Discrepancy (MMD) and High-quality Sam-
ple Rate (HSR). The details are in Appendix E.4 and we report results in Table 5 where our models
significantly outperform the competitors over a large margin.
Sample Quality for Generated Images. We calculate the Inception Score (IS) and Frechet Incep-
tion Distance (FID) to measure the sample quality on CIFAR-10. As shown in Table 1, Joint-W
outperforms other competitors by 0.2 and achieves 5.6% improvement over WGAN-GP w.r.t IS.
As for FID, Joint-W slightly outperforms WGAN-GP and beats energy-based GAN and variational
annealing regularized WGAN over a large margin. One possible reason is that these methods both
consider entropy regularization which encourages diversity of generated samples but will have a
negative effect on sample quality. Stein Bridging can overcome this issue via joint training with
explicit model. The performance of DGM tends to be much worse than others. In practice, DGM is
hard for convergence and suffers from severe instability in training.
Model Performance in Contaminated or Limited Data. As further discussions, we highlight
that Stein Bridging has promising power in some extreme cases where the training sample are con-
taminated or limited. We consider noised data scenario and randomly add n noise points sam-
pled from Gaussian distribution N (0, σ0I) where σ0 = 2 to the original true samples in Two-
Circle dataset. The results on noised dataset are presented in Fig. 8(a) where we set noise ratio
n = [40, 100, 160, 300, 400, 600, 800, 1000] and report the HSRs of Joint-W and WGAN-GP. The
noise ratio in data impacts the performance of WGAN-GP and Joint-W, but comparatively, the per-
formance decline of Joint-W is less insignificant than WGAN-GP, which indicates better robustness
of joint training w.r.t. noised data.
7
Under review as a conference paper at ICLR 2021
Table 2: AUCS for out-of-distribution
sample detection on CIFAR-10. We use
negative samples from (I) flip images, (II)
add random noise, (III) overlay two im-
ages and (IV) use images from LSUN
dataset.
AUC	I	II	III	IV
JOint-W	0.50	0.92	0.95	0.85
DEM	0.50	0.52	0.51	0.56
DGM	1.00	1.00	1.00	0.82
EGAN	0.50	0.42	0.30	0.52
Epoch 410
Epoch 420
Epoch 430
Epoch 440
Epoch 450
WGAN-GP
OZiw-
om -
CUO-
«.125
ClM
OMS
9.QSQ
Q.01S
0.000-
o
0,124
oux)
Og
ouw
O ɪw
Q.17S
20000	40000 βoo∞
Iteration
XWoO WOOOO
(a) Two-Circle
aww

t
□εε
7<xxx> mm 9θ∞o
z(xx>o
•0000 WWOO
Iteration
WooO WOOOO
(b) Two-Spiral
Figure 6: Learning curves of Joint-W (resp. Joint-JS) compared
with WGAN (resp. GAN) and its regularization-based variants on
Two-Circle and Two-Spiral datasets.
ʃʃʃʒrʃ
77777
Ooooo
V y W17
3 2 232
i I 4 I I
K 4 4 4 S
? ? 7 7 7
份夕。夕c⅜
F 7 夕 S R
5 6 5 δ 5
/» /V H √L
3 3 2 2 Z
+ +≠+≠
035-
β.eα-
35-
0.70-
«.es-
Noise Ratio (%)
(a)
0.75-
0.70-
OM-
055-
58 WOO ISOO
True Sample Size
(b)
Figure 8: Impact of (a) noise in data and (b)
insufficient data on model performance.
WGAN+VA	Ours
FigUre 7: Generated digits given by the same noise Z in adja-
cent training epochs on MNIST.
To study the impact of insufficient data, in Fig. 8(b), We consider sample size N as
[100,200,300, 500,700,1000, 2000] in Two-Spiral dataset and report the AUC of Joint-W and
DEM. When sample size decreases from 2000 to 100, the AUC value of DEM declines dramat-
ically, showing its dependency on sufficient training samples. By contrast, the AUC of Joint-W
exhibits a small decline when the sample size is more than 500 and suffers from an obvious decline
when it is less than 300. Such phenomenon demonstrates its lower sensitivity to data size.
5.3 Enhancing the Stability of GAN
Joint training also helps to stabilize training dynamics. In Fig. 6 we present the learning curves
of Joint-W (resp. Joint-JS) compared with WGAN (resp. GAN) and likelihood- and entropy-
regularized WGAN (resp. GAN). The curves depict that joint training could reduce the variance
of metric values especially during the second half of training. Furthermore, we visualize gener-
ated digits given by the same noise z in adjacent epochs in Fig. 7. The results show that Joint-W
gives more stable generation in adjacent epochs while generated samples given by WGAN-GP and
WGAN+VA exhibit an obvious variation. Especially, some digits generated by WGAN-GP and
WGAN+VA change from one class to another, which is quite similar to the oscillation without con-
vergence discussed in Section 3.2. To quantify the evaluation of bias in model distributions, we
calculate distances between the means of 50000 generated digits (resp. images) and 50000 true dig-
its (resp. images) in MNIST (reps. CIFAR-10). The results are reported in Table 4. We can see that
the model distributions of other competitors are more biased from true data distribution, compared
with Joint-W.
6 Conclusions
In this paper, we aim at uniting the training for implicit generative model (represented by GAN or
WGAN) and explicit generative model (represented by a deep energy-based model) via an bridging
term of Stein discrepancy between the generator and the energy-based density estimator. Theoret-
ically, we show that joint training could i) enforce dual regularization effects on both models and
thus encourage mode exploration, and ii) help to facilitate the convergence of minimax training dy-
namics. We also conduct extensive experiments on different tasks and applications to verify our
theoretical findings as well as demonstrate the superiority of our method compared with training
generator models or energy-based models alone. Our formulation is flexible in handling various
implicit or explicit models. As such, for future works, one can try other generative models such as
VAE or flowed-based model as replacement for our GAN and energy-based models. It would also
be interesting to exploit our formulation in the context of few-shot learning in generative models.
8
Under review as a conference paper at ICLR 2021
References
Guillaume Alain and Yoshua Bengio. What regularized auto-encoders learn from the data-generating
distribution. J. Mach. Learn. Res.,15(1):3563-3593, 2014.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In ICML, pp. 214-223, 2017.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In ICLR, 2019.
Kacper Chwialkowski, Heiko Strathmann, and Arthur Gretton. A kernel test of goodness of fit. In
ICML, pp. 2606-2615, 2016.
Zihang Dai, Amjad Almahairi, Philip Bachman, Eduard H. Hovy, and Aaron C. Courville. Calibrat-
ing energy-based generative adversarial networks. In ICLR, 2017.
Chao Du, Kun Xu, Chongxuan Li, Jun Zhu, and Bo Zhang. Learning implicit generative models by
teaching explicit ones. CoRR, abs/1807.03870, 2018.
Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. CoRR,
abs/1903.08689, 2019.
L.C. Evans. Partial Differential Equations. Graduate studies in mathematics. American Mathemat-
ical Society, 2010. ISBN 9781470411442. URL https://books.google.com/books?
id=UL9WtAEACAAJ.
Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions, and the bayesian
restoration of images. IEEE Trans. Pattern Anal. Mach. Intell., 6(6):721-741, 1984.
Ian Gemp and Sridhar Mahadevan. Global convergence to the equilibrium of gans using variational
inequalities. CoRR, abs/1808.01531, 2018.
Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A varia-
tional inequality perspective on generative adversarial networks. In ICLR, 2019.
Ian J. Goodfellow. NIPS 2016 tutorial: Generative adversarial networks. CoRR, abs/1701.00160,
2017.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, pp. 2672-2680,
2014.
Jackson Gorham and Lester Mackey. Measuring sample quality with stein’s method. In Advances
in Neural Information Processing Systems, pp. 226-234, 2015.
Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, and Richard S. ZemeL
Cutting out the middle-man: Training and evaluating energy-based models without sampling.
CoRR, abs/2002.05616, 2020.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Im-
proved training of wasserstein gans. In NIPS, pp. 5767-5777, 2017.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In ICML, pp. 1352-1361, 2017.
Geoffrey E. Hinton. Product of experts. In ICANN’99 Artificial Neural Networks, 1999.
Geoffrey E. Hinton, Simon Osindero, and Yee Whye Teh. A fast learning algorithm for deep belief
nets. Neural Computation, 18(7):1527-1554, 2006.
Tianyang Hu, Zixiang Chen, Hanxi Sun, Jincheng Bai, Mao Ye, and Guang Cheng. Stein neural
sampler. CoRR, abs/1810.03545, 2018.
Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. J. Mach.
Learn. Res., 6:695-709, 2005.
9
Under review as a conference paper at ICLR 2021
Taesup Kim and Yoshua Bengio. Deep directed generative models with energy-based probability
estimation. In ICLR, 2017.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in neural information processing Systems, pp. 10215-10224, 2018.
Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu Jie Huang. A tutorial on
energy-based learning. Predicting Structured Data, MIT Press, 2006.
Yingzhen Li and Richard E. Turner. Gradient estimators for implicit models. In 6th International
Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3,
2018, Conference Track Proceedings, 2018.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. In AISTATS, pp. 907-915, 2019.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. In NIPS, pp. 2370-2378, 2016.
Qiang Liu and Dilin Wang. Learning deep energy models: Contrastive divergence vs. amortized
MLE. CoRR, abs/1707.00797, 2017.
Qiang Liu, Jason D. Lee, and Michael I. Jordan. A kernelized stein discrepancy for goodness-of-fit
tests. In ICML, pp. 276-284, 2016.
Vaishnavh Nagarajan and J. Zico Kolter. Gradient descent GAN optimization is locally stable. In
NIPS, pp. 5585-5595, 2017.
Radford M. Neal. Stochastic relaxation, gibbs distributions, and the bayesian restoration of images.
Handbook of Markov Chain Monte Carlo, 2, 2011.
Jiquan Ngiam, Zhenghao Chen, Pang Wei Koh, and Andrew Y. Ng. Learning deep energy models.
In ICML, pp. 1105-1112, 2011.
Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy, and Jason Yosinski. Plug & play
generative networks: Conditional iterative generation of images in latent space. In CVPR, pp.
3510-3520, 2017.
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Yingnian Wu. On learning non-convergent non-
persistent short-run mcmc toward energy-based model. CoRR, abs/1904.09770, 2019.
Chris J Oates, Mark Girolami, and Nicolas Chopin. Control functionals for monte carlo integration.
Journal of the Royal Statistical Society, Series B, 2017.
George Papamakarios, Theo Pavlakou, and Iain Murray. Masked autoregressive flow for density
estimation. In Advances in Neural Information Processing Systems, pp. 2338-2347, 2017.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. In ICLR, 2016.
Kevin Roth, Aurelien Lucchi, Sebastian Nowozin, and Thomas Hofmann. Stabilizing training of
generative adversarial networks through regularization. In Advances in neural information pro-
cessing systems, pp. 2018-2028, 2017.
Ruslan Salakhutdinov and Geoffrey E. Hinton. Deep boltzmann machines. In AISTATS, pp. 448-
455, 2009.
Alexander Shapiro, Darinka Dentcheva, and Andrzej RUszczynski. Lectures on Stochastic Program-
ming: modeling and theory. SIAM, 2009.
Chenyang Tao, ShUyang Dai, LiqUn Chen, Ke Bai, JUnya Chen, Chang LiU, RUiyi Zhang, Georgiy V.
Bobashev, and Lawrence Carin. Variational annealing of gans: A langevin perspective. In ICML,
pp. 6176-6185, 2019.
10
Under review as a conference paper at ICLR 2021
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
David Warde-Farley and Yoshua Bengio. Improving generative adversarial networks with denoising
feature matching. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings, 2017.
Nik Weaver. Lipschitz algebras. World Scientific, 1999.
Ying Nian Wu, Song Chun Zhu, and Xiuwen Liu. Equivalence of julesz ensembles and FRAME
models. International Journal ofComputer Vision, 38(3):247-265, 2000.
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. Cooperative training of descriptor and
generator networks. CoRR, abs/1609.09408, 2016a.
Jianwen Xie, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. A theory of generative convnet. In
ICML, pp. 2635-2644, 2016b.
Jianwen Xie, Yang Lu, Ruiqi Gao, and Ying Nian Wu. Cooperative learning of energy-based model
and latent variable model via MCMC teaching. In AAAI, pp. 4292-4301, 2018.
Shuangfei Zhai, Yu Cheng, Weining Lu, and Zhongfei Zhang. Deep structured energy based models
for anomaly detection. In ICML, pp. 1100-1109, 2016.
Guojun Zhang and Yaoliang Yu. Convergence of gradient methods on bilinear zero-sum games. In
ICLR, 2020.
JUnbo Jake Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial net-
works. In ICLR, 2017.
Song Chun Zhu, Ying Nian Wu, and David Mumford. Minimax entropy principle and its application
to texture modeling. Neural Computation, 9(8):1627-1660, 1997.
11
Under review as a conference paper at ICLR 2021
A	Literature Reviews
We discuss some of related literature and shed lights on the relationship between our work with
others.
A. 1 Explicit Generative Models
Explicit generative models are interested in fitting each instance with a scalar (unnormalized) density
expected to explicitly capture the distribution behind data. Such densities are often up to a constant
and called as energy functions which are common in undirected graphical models (LeCun et al.,
2006). Hence, explicit generative models are also termed as energy-based models. An early version
of energy-based models is the FRAME (Filters, Random field, And Maximum Entropy) model (Zhu
et al., 1997; Wu et al., 2000). Later on, some works leverage deep neural networks to model the en-
ergy function (Ngiam et al., 2011; Xie et al., 2016b) and pave the way for researches on deep energy
model (DEM) (e.g., (Liu & Wang, 2017; Kim & Bengio, 2017; Zhai et al., 2016; Haarnoja et al.,
2017; Du & Mordatch, 2019; Nijkamp et al., 2019)). Apart from DEM, there are also some other
forms of deep explicit models based on restricted Boltzmann machines like deep belief networks
(Hinton et al., 2006) and deep Boltzmann machines (Salakhutdinov & Hinton, 2009).
The normalized constant under the energy function requires an intractable integral over all possi-
ble instances, which makes the model hard to learn via Maximum Likelihood Estimation (MLE).
To solve this issue, some works propose to approximate the constant by MCMC methods (Geman
& Geman, 1984; Neal, 2011). However, MCMC requires an inner-loop samples in each training,
which induces high computational costs. Another solution is to optimize an alternate surrogate loss
function. For example, contrastive divergence (CD) (Liu & Wang, 2017) is proposed to measure
how much KL divergence can be improved by running a small numbers of Markov chain steps to-
wards the intractable likelihood, while score matching (SM) (Hyvarinen, 2005) detours the constant
by minimizing the distance for gradients of log-likelihoods. A recent study (Grathwohl et al., 2020)
uses Stein discrepancy to train unnormalized model. The Stein discrepancy does not require the nor-
malizing constant and makes the training tractable. Moreover, the intractable normalized constant
makes it hard to sample from. To obtain an accurate samples from unnormalized densities, many
studies propose to approximate the generation by diffusion-based processes, like generative flow
(Nguyen et al., 2017) and variational gradient descent ((Liu & Wang, 2016)). Also, a recent work
(Hu et al., 2018) leverages Stein discrepancy to design a neural sampler from unnormalized densi-
ties. The fundamental disadvantage of explicit model is that the energy-based learning is difficult to
accurately capture the distribution of true samples due to the low manifold of real-world instances
(Liu & Wang, 2017).
A.2 Implicit Generative Models
Implicit generative models focus on a generation mapping from random noises to generated sam-
ples. Such mapping function is often called as generator and possesses better flexibility compared
with explicit models. Two typical implicit models are Variational Auto-Encoder (VAE) (Kingma
& Welling, 2014) and Generative Adversarial Networks (GAN) (Goodfellow et al., 2014). VAE
introduces a latent variable and attempts to maximize the variational lower bound for likelihood
of joint distribution of latent variable and observable variable, while GAN targets an adversarial
game between the generator and a discriminator (or critic in WGAN) that aims at discriminating
the generated and true samples. In this paper, we focus on GAN and its variants (e.g., WGAN (Ar-
jovsky et al., 2017), WGAN-GP (Gulrajani et al., 2017), DCGAN (Radford et al., 2016), etc.) as the
implicit generative model and we leave the discussions on VAE as future work.
Two important issues concerning GAN and its variants are instability of training and local optima.
The typical local optima for GAN can be divided into two categories: mode-collapse (the model
fails to capture all the modes in data) and mode-redundance (the model generates modes that do
not exist in data). Recently there are many attempts to solve these issues from various perspectives.
One perspective is from regularization. Two typical regularization methods are likelihood-based and
entropy-based regularization with the prominent examples (Warde-Farley & Bengio, 2017) and (Li
& Turner, 2018) that respectively leverage denoising feature matching and implicit gradient approx-
imation to enforce the regularization constraints. The likelihood and entropy regularizations could
12
Under review as a conference paper at ICLR 2021
respectively help the generator to focus on data distribution and encourage more diverse samples,
and a recent work (Tao et al., 2019) uses Langevin dynamics to indicate that i) the entropy and like-
lihood regularizations are equivalent and share an opposite relationship in mathematics, and ii) both
regularizations would make the model converge to a surrogate point with a bias from original data
distribution. Then (Tao et al., 2019) proposes a variational annealing strategy to empirically unite
two regularizations and tackle the biased distributions.
To deal with the instability issue, there are also some recent literatures from optimization perspec-
tives and proposes different algorithms to address the non-convergence of minimax game optimiza-
tion (for instance, (Gemp & Mahadevan, 2018; Liang & Stokes, 2019; Gidel et al., 2019)). Moreover,
the disadvantage of implicit models is the lack of explicit densities over instances, which disables
the black-box generator to characterize the distributions behind data.
A.3 Attempts to Combine Both of the Worlds
Recently, there are several studies that attempt to combine explicit and implicit generative models
from different ways. For instance, (Zhao et al., 2017) proposes energy-based GAN that leverages
energy model as discriminator to distinguish the generated and true samples. The similar idea is
also used by (Kim & Bengio, 2017) and (Dai et al., 2017) which let the discriminator estimate
a scaler energy value for each sample. Such discriminator is optimized to give high energy to
generated samples and low energy to true samples while the generator aims at generating samples
with low energy. The fundamental difference is that (Zhao et al., 2017) and (Dai et al., 2017) both
aim at minimizing the discrepancy between distributions of generated and true samples while the
motivation of (Kim & Bengio, 2017) is to minimize the KL divergence between estimated densities
and true samples. (Kim & Bengio, 2017) adopts contrastive divergence (CD) to link MLE for energy
model over true data with the adversarial training of energy-based GAN. However, both CD-based
method and energy-based GAN have limited power for both generator and discriminator. Firstly,
if the generated samples resemble true samples, then the gradients for discriminator given by true
and generated samples are just the opposite and will counteract each other, and the training will
stop before the discriminitor captures accurate data distribution. Second, since the objective boils
down to minimizing the KL divergence (for (Kim & Bengio, 2017)) or Wasserstein distance (for
(Dai et al., 2017)) between model and true distributions, the issues concerning GAN (or WGAN)
like training instability and mode-collapse would also bother these methods.
Another way for combination is by cooperative training. (Xie et al., 2016a) (and its improved version
(Xie et al., 2018)) leverages the samples of generator as the MCMC initialization for energy-based
model. The synthesized samples produced from finite-step MCMC are closer to the energy model
and the generator is optimized to make the finite-step MCMC revise its initial samples. Also, a recent
work (Du et al., 2018) proposes to regard the explicit model as a teacher net who guides the training
of implicit generator as a student net to produce samples that could overcome the mode-collapse
issue. The main drawback of cooperative training is that they indirectly optimize the discrepancy
between the generator and data distribution via the energy model as a ‘mediator’, which leads to
a fact that once the energy model gets stuck in a local optimum (e.g., mode-collapse or mode-
redundance) the training for the generator would be affected. In other words, the training for two
models would constrain rather than exactly compensate each other. Different from existing methods,
our model considers three discrepancies simultaneously as a triangle to jointly train the generator
and the estimator, enabling them to compensate and reinforce each other.
B	Background for Stein Discrepancy
Assume q(x) to be a continuously differentiable density supported on X ⊂ Rd and f : Rd → Rd0 a
smooth vector function. Define Aq[f (x)] = Vχ log q(x)f (x)> + Vχf (x) as a Stein operator. If f is
a Stein class (satisfying some mild boundary conditions) then we have the following Stein identity
property:
Ex 〜q [Aq [f (x)]] = Ex 〜q [Vχ log q(x)f (x)> + Vχf (x)] = 0.
Such property induces Stein discrepancy between distributions P : p(x) and Q : q(x), x ∈ X:
S (Q,P) = sup{Ex 〜q [Ap [f (x)]] = sup{Γ(Eχ 〜q [Vx log p(x)f (x)> + Vxf (x)])},	(4)
f∈F	f∈F
13
Under review as a conference paper at ICLR 2021
where f is what we call Stein critic that exploits over function space F and if F is large enough then
S(Q, P) = 0 if and only ifQ = P. Note that in (1), we do not need the normalized constant for p(x)
which enables Stein discrepancy to deal with unnormalized density.
IfF is a unit ball in a Reproducing Kernel Hilbert Space (RKHS) with a positive definite kernel func-
tion k(∙, ∙), then the supremum in (1) would have a close form (See (LiU et al., 2016; Chwialkowski
et al., 2016; Oates et al., 2017) for more details):
SK(Q,P) = Eχ,χθ〜q[up(x, x0)],	(5)
where Up(x, x0)	=	Rx logp(x)>k(x, x0Nx logp(x0) + Rx logp(x)>Vχk(x, x0) +
▽xk(x, x0)>Vx logp(x0) + tr(Vx xok(x, x0)). The (5) gives the Kernel Stein Discrepancy
(KSD).
C Proofs of Results in Section 4.1
C.1 Proof of Theorem 1
Proof. Applying Kantorovich’s duality on W(PG, Pr) and using the exhaustiveness assumption on
the generator, we rewrite the problem as
min max{EP[D] - EPreal[D] + λ1S(Preal, PE) + λ2S(P, PE)},	(6)
E,P D
where the minimization with respect to E is over all energy functions, the minimization with respect
to P is over all probability distributions with continuous density, and the maximization with respect
to D is over all 1-Lipschitz continuous functions. Recall the definition of kernel Stein discrepancy
S (P, Pe ) = Eχ,xo 〜p[(Vχ log dP/dPE (x))>k(x, x0)Vχ log dP/dPE (x0)],
where dP/dPE is the Radon-Nikodym derivative. Observe that S(P, PE) is infinite ifP is not abso-
lutely continuous with respect to PE. Hence, to minimize the objective of (6), it suffices to consider
those P’s that are absolutely continuous with respect to PE . Introducing a variable replacement
h(x) := dP/dPE (x) - 1, then problem (6) becomes
min max EPE [(1 + h)D] - EPreal[D] + λ1S(Preal, PE)
E,h D
(7)
+ λι2 ∙ Eχ,x0〜P[Vx log(1 + h(x))>k(x, x0)Vx log(1 + h(x0))] ,
where the minimization with respect to h is over all L1(PE) functions with PE -expectation zero.
Fixing E, we claim that we can swap minh and maxD. Indeed, without loss of generality, we can
restrict D to be such that D(x0) = 0 for some element x0, as a constant shift does not change the
value of EPE [(1 + h)D] - EPreal [D]. The set of Lipschitz functions that vanish at x0 is a Banach
space, and the set of 1-Lipschitz functions is compact (Weaver, 1999). Moreover, L1 (PE) is also a
Banach space and the objective function is linear in both h and D. The above verifies the condition
of Sion’s minimax theorem, and thus the claim is proved.
Swapping minh and maxD in (7) and fixing E and D, we consider
min	{Epe [hD] + λ ∙ Ex,x，〜p[Vχ log(1 + h(x))>k(x, x0)Vχ log(1 + h(x0))]}
h:EPE [h]=0
= min	(EPE [hD] +	λ?	∙ Ex,x0〜P	Vxh(X)、k(x, x0) IVxh(X 0)	1
h:EPE [h]=0 E	1 + h(x)	1 + h(x0)
= min	{Epe [hD] + λ? ∙ Ex,xo〜PE [Vxh(x)>k(x, x0)Vxh(X0)]},
h:EPE [h]=0
where the first equality follows from the chain rule of the derivative, and the second equality follows
from a change of measure dP = (1 + h)dPE. Introducing an auxiliary variable r so that r2 is an
14
Under review as a conference paper at ICLR 2021
upper bound of Eχ,χo〜PE [Vχh(x)>k(x, x0)Vχh(x0)], we have that
min	{Epe [hD] + λ? ∙ Eχχ〜PE [Vχh(x)>k(x, x0) Vxh(X0)]}
h:EPE [h]=0
=min min	EPE[hD]+λ2r2 :
Ex,X0-Pe Vxh(X)>k(X, X0)Vxh(X0) ≤ r2}
r≥0 h:EPE [h]=0
=min min rEPE [hD] + λ2r2 : Ex,x0-PE Vxh(X)>k(X, X0)Vxh(X0) ≤ 1}
r≥0 h:EPE [h]=0
= mr≥in0 λ2r2 - r kDkH-1(PE;k)
=-4λ2 kDkH-ι(PE ；k),
where the first equality holds because Ex,x0-PE Vx(rh)(X)>k(X, X0)Vx(rh)(X0)	=
r2Ex,x0-PE Vxh(X)>k(X, X0)Vxh(X0) for all r ≥ 0 and by introducing an auxiliary vari-
able r2 = Ex,x0-PE Vxh(X)>k(X, X0)Vxh(X0) ; the second equality follows from a change of
variable from h to rh; and the third equality follows from the definition of the kernel Sobolev dual
norm. Plugging back in (7) yields the ideal result.	□
C.2 Proof for Theorem 2
Proof. Applying the definition of Stein discrepancy on S(PE, PG) and under the exhaustiveness
assumption of G, we rewrite the problem as
min max{λ1S(Preal, PE) + λ2Ey-P[APE f (y)] + W(Preal, P)},
where the minimization with respect to E is over the set of all engergy functions; the minimization
with respect to P is over all continuous distributions; and the maximization with respect to f is
over the Stein class for PE . Let us fix E. Using a similar argument as in the proof of Theorem
1, it suffices to restrict P on the set of distributions that are absolutely continuous with respect to
PE, which can be identified as the set of L1(PE) functions with PE -mean zero and is thus Banach.
Together with the compactness assumption of the Stein class, using Sion’s minimax theorem, we
can swap the minimization over P and the maximization over f. Now, fixing further f, consider
min {λ2Ey-P[APE f (y)] + W(Preal, P)}.	(8)
Recall the definition of Wasserstein metric
W(Preal,P) = mγin E(x,y)-γ [kX - yk],
where the minimization is over all joint distributions of (X, y) with X-marginal Preal and y-marginal
P. We rewrite problem (8) as
min{E(x,y)-γ [λ2 APE f (y) + ||X - y||]},
P,γ
where γ has marginals Preal and P. Since P is unconstrained, the above problem is further equivalent
to
mγin{E(x,y)-γ [λ2APE f (y)] + ||X - y||]},
where the minimization is over all joint distributions of (X, y) with X-marginal being Preal. Using
the law of total expectation, the problem above is equivalent to
min	Ex-PrealEy-γx [λ2APEf(y) + ||X -y|| | X]
{γx}x∈suppPreal
= Ex-Preal mγin Ey-γx [λ2APE f (y) + ||X - y|| | X]
Ex-Preal
min{λ2APE f (y) + ||X - y||}
y∈X
where the minimization in the first line of the equation is over γx , the set of all conditional distri-
butions of y given X where X is over the support supp Preal of Preal ; the exchanging of min and E
15
Under review as a conference paper at ICLR 2021
in the first equality follows from the interchangebability principle (Shapiro et al., 2009); the second
equality holds because the infimum can be restricted to the set of point masses. Finally, the original
problem is equivalent to
min max (λιS(Preal,Pe) + Ex〜Preal min{λ2ApE f (y) + ||x - y||} ∖ .
E f	real y∈X	E
Therefore, the proof is completed using the definition of Moreau-Yosida regularization.	□
D	Details and Proofs in Section 4.2
D.1 Discussions on One-Dimensional Case
The training for minimax game in GAN is difficult. When using traditional gradient methods, the
training would suffer from some oscillatory behaviors (Goodfellow, 2017; Liang & Stokes, 2019). In
order to better understand the optimization behaviors, we first study a one-dimension linear system
that provides some insights on this problem. Such toy example (or a similar one) is also utilized by
(Gidel et al., 2019; Nagarajan & Kolter, 2017) to shed lights on the instability of WGAN training2.
Consider a linear critic Dψ (x) = ψx and generator Gθ (z) = θz. Then the Wasserstein GAN
objective can be written as a constrained bilinear problem: min& max∣ψ∣≤ι ψE[x] - ψθE[z], which
could be further simplified as an unconstrained version (the behaviors can be generalized to multi-
dimensional cases (Gidel et al., 2019)):
min max ψ  ψ ∙ θ.	(9)
θψ
Unfortunately, such simple objective cannot guarantee convergence by traditional gradient methods
like SGD with alternate updating3: θk+1 = θk+ηψk,, ψk+1 = ψk +η(1 - θk+1). Such optimization
would suffer from an oscillatory behavior, i.e., the updated parameters go around the optimum point
([ψ*,θ*] = [0,1]) forming a circle without converging to the centrality, which is shown in Fig. 3(a).
A recent study in (Liang & Stokes, 2019) theoretically show that such oscillation is due to the
interaction term in (9).
One solution to the instability of GAN training is to add (likelihood) regularization, which has been
widely studied by recent literatures (Warde-Farley & Bengio, 2017; Li & Turner, 2018). With regu-
larization term, the objective changes into minθ max∣ψ∣≤ι ψE[x] - ψθE[z] - λE[logμ(θz)], where
μ(∙) denotes the likelihood function and λ is a hyperparameter. A recent study (Tao et al., 2019)
proves that when λ < 0 (likelihood-regularization), the extra term is equivalent to maximizing
sample evidence, helping to stabilize GAN training; when λ > 0 (entropy-regularization), the ex-
tra term maximizes sample entropy, which encourages diversity of generator. Here we consider a
Gaussian likelihood function for generated sample x0, μ(x0) = exp(-2(x0 - b)2) which is up to
a constant. Its parameter can be estimated by b = E[x]. Then for generated sample x0 = θz, we
have E(logμ(θz)) = - 11 E[z2]θ2 + E[z]E[x]θ - 2E[x]2. Like the case in WgAn, we consider
E[x] = E[z] = 1. Assume Var[z] = 1 and we have E[z2] = 1 + E[z]. Hence, for the analysis on
likelihood- (and entropy-) regularized WGAN, we can study the following system:
min max ψ - ψ ∙ θ - λ(θ2 - θ).	(10)
θψ
When λ = 1, the above objective degrades to (9); when λ < 0 (likelihood-regularization), the
the gradient of regularization term pushes θ to shrink, which helps for convergence; when λ > 0
(entropy-regularization), the added term forms an amplifiying strength on θ and leads to divergence.
Another issue of likelihood-regularization is that the extra term changes the optimum point and
makes the model converge to a biased distribution, as proved by (Tao et al., 2019). In this case,
one can verify that the optimum point becomes [ψ*,θ*] = [-λ, 1], resulting in a bias. To avoid
this issue, (Tao et al., 2019) proposes to temporally decrease ∣λ∣ through training. However, such
method would also be stuck in oscillation when ∣λ∣ gets close to zero as is shown in Fig. 3(a).
Finally, consider our proposed model. We also simplify the density estimator as a basic en-
ergy model pφ(x) = exp(- 11 x2 - φx) whose score function Rx logpφ(x) = -x - φ. Then
2Our theoretical discussions focus on WGAN, and we also compare with original GAN in the experiments.
3Here, we adopt the most widely used alternate updating strategy. The simultaneous updating, i.e., θk+1 =
θk + ηψk and ψk+1 = ψk + η(1 - θk), would diverge in this case.
16
Under review as a conference paper at ICLR 2021
if we specify the two Stein discrepancies in (3) as KSD with kernel k(x1, x2) = I(x1 =
x2), then S (Preal , PE)
= Ex1,x2[(Vχι logPφ(χι) - Vxi logμ(χ1))k(χ1,χ2)(Vχ2 logPφ(χ2)-
Vχ2 logμ(x2))] = Eχ[(Vχ logpφ(x) — Vχ logμ(x))2] = (φ + E[x])2. Similarly, one can obtain
S(PG, PE) = (φ + θE[z])2. Therefore we arrive at the objective in (11)
min max min ψ — ψ ∙ θ + — (1 + φ)2 + — (θ + φ)2.	(11)
θψφ	2	2
Interestingly, for ∀λ1,λ2, the optimum remains the same [ψ*,θ*,φ*] = [0,1, -1]. Then We show
that the optimization guarantees convergence to [ψ*,θ*,φ*].
Proposition 1. Using alternate SGDfor (11) geometrically decreases the square norm Nt = ∣ψt∣2 +
∣θ — 1|2 + ∣φ + 1|2, for any 0 < η < 1 with λι = λ2 = 1,
Nt+1 = (1 - η2 (1 - η)2)Nt.	(12)
Proof. Instead of directly studying the optimization for (11), we first prove the following problem
will converge to the unique optimum,
min max min θψ + θφ + ɪθ2 + φ2.	(13)
Applying alternate SGD we have the following iterations:
ψt+ι = ψt + η * θt,
Φt+ι = Φt - η * (θt + 2φt) = (1 - 2η)Φt - ηθt,
θt+1 = θt - η(ψt+1 + φt+1 + θt) = -η(1 - 2η)φt + (1 - η)θt - ηψt.
Then we obtain the relationship between adjacent iterations:
ψt+1
φt+1
θt+1
1
0
-η
0
1 - 2η
-η(1 - 2η)
η
-η
1-η
ψt
φt
θt
ψt
M ∙ φt
θt
We further calculate the eigenvalues for matrix M and have the following equations (assume the
eigenvalue as λ):
(λ-1)3+3η(λ-1)2+2η2(1+η)(λ-1)+2η3=0.
One can verify that the solutions to the above equation satisfy ∣λ∣ <，(1 一 η + η2)(1 + η 一 η2).
Then we have the following relationship
ψt+1	2
φt+1	= [ψt φt
θt+1	2
θt] ∙ M>M ∙ "Φt
θt
2
≤ λm ∙
2
ψt
φt
θt
2
2
where λm denotes the eigenvalue with the maximum absolute value of matrix M . Hence, we have
ψt2+1 + φt2+1 + θt2+1 ≤ (1 - η + η2)(1 + η - η2)[ψt2 + φt2 + θt2].
We proceed to replace ψ, φ and θ in (13) by ψ0, φ0 and θ0 respectively and conduct a change of
variable: let θ0 = 1 - θ and φ0 = -1 - φ. Then we get the conclusion in the proposition.
□
As shown in Fig. 3(a), Stein Bridging achieves a good convergence to the right optimum. Compared
with (9), the objective (11) adds a new bilinear term φ ∙ θ, which acts like a connection between the
generator and estimator, and two other quadratic terms, which help to penalize the increasing of val-
ues through training. The added terms and original terms in (11) cooperate to guarantee convergence
to a unique optimum. In fact, the added terms λ21 (1 + φ)2 + λ2 (θ + φ)2 in (11) and the original terms
ψ - ψ ∙ θ in WGAN play both necessary roles to guarantee the convergence to the unique optimum
points [ψ*, θ*, φ*] = [0,1, -1]. If we remove the critic and optimize θ and φ with the remaining loss
terms, we would find that the training would converge but not necessarily to [ψ*,θ*] = [0,1] (since
the optimum points are not unique in this case). On the other hand, if we remove the estimator, the
system degrades to (9) and would not converge to the unique optimum point [ψ*,θ*] = [0,1]. If
we consider both of the world and optimize three terms together, the training would converge to a
unique global optimum [ψ*,θ*, φ*] = [0,1, -1].
17
Under review as a conference paper at ICLR 2021
D.2 Generalization to Bilinear Systems
Our analysis in the one-dimension case inspires us that we can add affiliated variable to modify
the objective and stabilize the training for general bilinear system. The bilinear system is of wide
interest for researchers focusing on stability of GAN training ((Goodfellow, 2017; Liang & Stokes,
2019; Gidel et al., 2019; Gemp & Mahadevan, 2018; Zhang & Yu, 2020)). The general bilinear
function can be written as
F(ψ,θ) = θ>Aψ - b>θ - c>ψ,	(14)
where ψ, θ are both r-dimensional vectors and the objective is min max F (ψ, θ) which can be
θψ
seen as a basic form of various GAN objectives. Unfortunately, if we directly use simultaneous
(resp. alternate) SGD to optimize such objectives, one can obtain divergence (resp. fluctuation).
To solve the issue, some recent papers propose several optimization algorithms, like extrapolation
from the past ((Gidel et al., 2019)), crossing the curl ((Gemp & Mahadevan, 2018)) and consensus
optimization ((Liang & Stokes, 2019)). Also, (Liang & Stokes, 2019) shows that it is the interaction
term which generates non-zero values for Vθψ F and VψθF that leads to such instability of training.
Different from previous works that focused on algorithmic perspective, we propose to add new
affiliated variables which modify the objective function and allow the SGD algorithm to achieve
convergence without changing the optimum points.
Based on the minimax objective of (14) we add affiliated r-dimensional variable φ (corresponding
to the estimator in our model) the original system and tackle the following problem:
minmaxminF(ψ, θ) + αH (φ, θ),	(15)
θψφ
where H(φ, θ) = 2(θ + φ)>B(θ + φ), B = (AA>)2 and α is a non-negative constant.
Theoretically, the new problem keeps the optimum points of (14) unchanged. Let L(ψ, φ, θ) =
F(ψ, θ) + αG(φ, θ).
Proposition 2. Assume the optimum point of min max F(ψ, θ) are [ψ*, θ*], then the optimum
θψ
points of (15) would be [ψ*, θ*, φ*] where φ* = -θ*.
Proof. The condition tells us that VθF(ψ*, θ) = 0 and VψF(ψ, θ*) = 0. Then We derive the
gradients for L(ψ, φ, θ),
Vψ L(ψ*, φ, θ) = Vθ F(ψ*, θ) = 0,	(16)
VθL(ψ, φ, θ*) = VθF(ψ, θ*) + VθH(φ, θ*) = 2(B + B>)(θ* + φ),	(17)
VφL(ψ, φ, θ) = VφH(φ, θ) = 1(B + B>)(φ + θ),	(18)
Combining (17) and (18) we get φ* = -θ*. Hence, the optimum point of(15) is [ψ*, θ*, φ*] where
φ* = -θ*.	□
The advantage of the new problem is that it can be solved by SGD algorithm and guarantees conver-
gence theoretically. We formulate the results in the following theorem.
Theorem 3. For problem min max min L(ψ, φ, θ) using alternate SGD algorithm, i.e.,
θψφ
ψt+1 = ψt + ηVψL(θt, ψt, φt),
φt+1 = φt - ηVφL(θt, ψt+1, φt),	(19)
θt+1 = θt - ηVθL(θt, ψt+1, φt+1),
we can achieve convergence to [ψ*, θ*, φ*] where φ* = —θ* with at least linear rate of (1 一 ηι +
η22)(1 + η2 - η12) where η1 = ησmin, η2 = ησmax and σmin (resp. σmax) denotes the maximum
(resp. minimum) singular value of matrix A.
To prove Theorem 3, we can prove a more general argument.
18
Under review as a conference paper at ICLR 2021
Lemma 1. If we consider any first-order optimization method on (15), i.e.,
Ψt+ι ∈ Ψo + Span(L(ψo, φ, θ),…，F (ψt, φ, θ)),∀t ∈ N,
φt+ι ∈ Ψo + Span(L(ψ, φo, θ),…，L(ψ, φt, θ)),∀t ∈ N,
θt+ι ∈ Ψo + Span(L(ψ, φ, θo),…，L(ψ, φ, θt)),∀t ∈ N,
Then we have
ψt = V>(ψt-ψ*),	Φt = U>(φt- φ*), θt = U>(θt- θ*),
where U and V are the singular vectors decomposed by matrix A using SVD decomposition, i.e.,
A = UDV> and the triple ([ψt]i, [φt]i, [θt]i)1≤i≤r follows the update rule with step size σiη as
the same optimization method on a unidimensional problem
12	12
min max min θψ + θφ + -θ2 + -φ2,	(20)
with step size η, where σi denotes the i-th singular value on the diagonal of D.
Proof. The proof is extended from the proof of Lemma 3 in (Gidel et al., 2019). The general class
of first-order optimization methods derive the following updations:
t+1	t+1
ψt+1 = ψ0 + X Pst(A>θs - C)= ψ0 + X PstA>(θs - θ*),
s=0	s=0
1 t+1
φt+1 = φ0 + 2 X δst(B + B>)(θs + φS),
t+1	1
θt+ι = θo + ^Xμst[A(ψs - ψ*) + 2(B + B>)(θs + φS)],
where ρst, δst, μst ∈ R depend on specific optimization method (for example, in SGD, Ptt = δtt
μtt remain as a non-zero constant for ∀t and other coefficients are zero).
Using SVD A = UDV> and the fact θ* = -φ*, B = (UDD>U>) = D, We have
t+1
V>(ψt+ι - ψ*) = V>(ψo - ψ*) + £ρstD>U>(θs - θ*)
s=0
t+1
U>(φt+ι - φ*) = U>(φo - φ*) + XδstU>D(θs - θ*) + U>D(φs - φ*),
t+1
U>(θt+ι -θ*) = U>(θo -θ*) + Xρst[DV>(ψs -ψ*) + U>D(θs -θ*) + U>D(φs -φ*)],
s=0
and equivalently,
t+1
ψet+1 = ψe0 +	PstD>θet,
s=0
φt
t+1
~	x_*> 一 _ ，丁 ~ 、
φe0 +	δstD(θet + φet),
s=0
t+1
θt+1 = θ0 +	PstD(ψt + θt + φt).
s=0
Note that D is a rectangular matrix with non-zero elements on a diagonal block of size r. Hence,
the above r-dimensional problem can be reduced to r unidimensional problems:
t+1	t+1
r T i 「71 . Y^~'`	ΓΛ 1	「71	「71 . Y^~'` Γ / ΓΛ 1 . r T ι ∖
[ψt+1]i = [ψ0]i +	Pstσi[θt]i,	[φt]i = [φ0]i +	δstσi([θt]i + [φt]i),
s=0	s=0
t+1
r≈	r~ I	/r ≈ I	r≈ I	rT、、
[θt+1]i = [θ0]i +	Pstσi([ψt]i + [θt]i + [φt]i).
s=0
The above iterations can be conducted independently in each dimension where the optimization in
i-th dimension follows the same updating rule with step size σiη as problem in (20).	口
19
Under review as a conference paper at ICLR 2021
Furthermore, since problem (20) can achieve convergence with a linear rate of (1-η+η2)(1+η-η2)
using alternate SGD (the proof is similar to that of (13)), the multi-dimensional problem in (15) can
achieve convergence by SGD with at least a rate of (1 - η1 + η22)(1 + η2 - η12) where η1 = ησmax,
η2 = ησmin and σmax (resp. σmin) denotes the maximum (resp. minimum) singular value of matrix
A. We conclude the proof for Theorem 4.
Theorem 3 suggests that the added term H(φ, θ) with affiliated variables φ could help the SGD
algorithm achieve convergence to the the same optimum points as directly optimizing F (ψ, θ).
Our method is related to consensus optimization algorithm ((Liang & Stokes, 2019)) which adds
a regularization term ∣∣VθF(ψ, θ)k + ∣∣VψF(ψ, θ)k to (14) resulting extra quadratic terms for θ
and ψ. The disadvantage of such method is the requirement of Hessian matrix of F(ψ, θ) which
is computational expensive for high-dimensional data. By contrast, our solution only requires the
first-order derivatives.
E Details for Implementations
E.1 Synthetic Datasets
We provide the details for two synthetic datasets. The Two-Circle dataset consists of 24 Gaussian
mixtures where 8 of them are located in an inner circle with radius r1 = 4 and 16 of them lie
in an outer circle with radius r2 = 8. For each Gaussian component, the covariance matrix is
0.2
0
00.2
σιI and the mean value is [ri cos t, ri Sin t], where t = 2π^k, k = 1,…，8, for the
inner circle, and [r2 Cos t, r sin t], where t = 2πjk-, k = 1, ∙∙∙ , 16 for the outer circle. We sample
Ni = 2000 points as true observed samples for model training.
The Two-Spiral dataset contains 100 Gaussian mixtures whose centers locate on two spiral-shaped
curves. For each Gaussian component, the covariance matrix is
0.5
0
00.5
σ2I and the mean
value is [—ci cos c1,c1 sin ci], where ci = 2ππ + Linspaceg, 0.5, 50) ∙ 2π, for one spiral, and
[c2 cos c2, —c2 sin c2], where c2 = 要 + linspace(0,0.5,50) ∙ 2π for another spiral. We sample
N2 = 5000 points as true observed samples.
E.2 Model Specifications and Training Algorithm
In different tasks, we consider different model specifications in order to meet the demand of capacify
as well as test the effectiveness under various settings. Our proposed framework (3) adopts Wasser-
stein distance for the first term and two Stein discrepancies for the second and the third terms. We
can write (3) as a more general form
minDi(Preal,PG)+λiD2(Preal,PE)+λ2D3(PG,PE),	(21)
θ,φ
where Di , D2 , D3 denote three general discrepancy measures for distributions. As stated in our
remark, Di can be specified as arbitrary discrepancy measures for implicit generative models. Here
we also use JS divergence, the objective for valina GAN. To well distinguish them, we call the model
using Wasserstein distance (resp. JS divergence) as Joint-W (resp. Joint-JS) in our experiments. On
the other hand, the two Stein discrepancies in (3) can be specified by KSD (as defined by S- in (5)) or
general Stein discrepancy with an extra critic (as defined by S in (1)). Hence, the two specifications
for Di and the two for D2 (D3) compose four different combinations in total, and we organize the
objectives in each case in Table 3.
In our experiments, we use KSD with RBF kernels for D2 and D3 in Joint-W and Joint-JS on two
synthetic datasets. For MNIST with conditional training (given the digit class as model input), we
also use KSD with RBF kernels. For MNIST and CIFAR with unconditional training (the class is
not given as known information), we find that KSD cannot provide desirable results so we adopt
general Stein discrepancy for higher model capacity.
The objectives in Table 3 appear to be comutationally expensive. In the worst case (using general
Stein discrepancy), there are two minimax operations where one is from GAN or WGAN and one
is from Stein discrepancy estimation. To guarantee training efficiency, we alternatively update the
20
Under review as a conference paper at ICLR 2021
Table 3: Objectives for different specifications of D1 (Preal, PG), D2(Preal, PE) and D3(PG, PE).
We specify D1 as Wasserstein distance or JS divergence in our paper and for D2 and D3 we consider
the general Stein discrepancy or kernel Stein discrepancy. Here we use W, JS to denote Wasser-
stein distance and JS divergence respectively, and S, Sk to represent general Stein discrepancy and
kernel Stein discrepancy respectively. We omit the gradient penalty term for Wasserstein distance
here but use it in experiments.
D1	D2	D3	Objective
W	S	S	mine min。maxψ max∏ Ex〜pdata [dψ (x)] - EZ〜p0 [dψ (Gg (z))] +λιEχ 〜Pdata [Apφ [f∏ (x)]] + λ2Ez~0 [Apφ [f∏ (Gθ (z))]]
W	Sk	Sk	mine min。maxψ Ex〜Pdata[dψ (x)] - EZ〜p0[dψ (Gg (z))] + XlEx,x0~Pdata [upφ (x, X )] + λ2Ez,Z0 ~P0 [upφ (G6 (Z) , G6 (z'))]
JS	S	S	mine min。 maxψ maxπ Ex 〜Pr[log(dψ (x))] + EZ〜P0[log(1-dψ(Ge(z)))] + λ1Ex 〜Pdata [Apφ f∏ (X)]] + λ2Ez 〜I。[Apφ f∏(Gθ (Z))]]
JS	Sk	Sk	mine min。maxψ Ex〜Pr[log(dψ(x))] + EZ〜p。[log(1 - dψ(Ge(z)))] +XlEx,x0〜Pdata[uPφ (X, x0)] + λ2Ez,z0〜p。[up6 (Gθ (Z), Gθ (ZO))]	
generator, estimator, Wasserstein critic and Stein critic over the parameters θ, φ, ψ and π respec-
tively. Specifically, in one iteration, we optimize the generator over θ and the estimator over φ with
one step respectively, and then optimize the Wasserstein critic over ψ with nd steps and the Stein
critic over π with nc steps. Such training approach guarantees the same time complexity order of
proposed method as that of GAN or WGAN, and the training time for our model can be bounded
within constant times the time for training GAN model. In our experiment, we set nd = nc = 5 and
empirically find that our model Stein Bridging would be two times slower than WGAN on average.
We present the training algorithm for Stein Bridging in Algorithm 1.
E.3 Implementation Details
We give the information of network architectures and hyper-parameter settings for our model as well
as each competitor in our experiments.
The energy function is often parametrized as a sum of multiple experts ((Hinton, 1999)) and each
expert can have various function forms depending on the distributions. If using sigmoid distribution,
the energy function becomes (see section 2.1 in (Kim & Bengio, 2017) for details)
Eφ(x) = X log(1 + e-(Win(x)+bi)),	(22)
i
where n(x) maps input x to a feature vector and could be specified as a deep neural network, which
corresponds to deep energy model ((Ngiam et al., 2011))
When not using KSD, the implementation for Stein critic f and operation function φ in (1) has
still remained an open problem. Some existing studies like (Hu et al., 2018) set d0 = 1 in which
situation f reduces to a scalar-function from d-dimension input to one-dimension scalar value. Such
setting can reduce computational cost since large d0 could lead to heavy computation for training.
Empirically, in our experiments on image dataset, we find that setting d0 = 1 can provide similar
performance to d0 = 10 or d0 = 100. Hence, we set d0 = 1 in our experiment in order for efficiency.
Besides, to further reduce computational cost, we let the two Stein critics share the parameters,
which empirically provide better performance than two different Stein critics.
Another tricky point is how to design a proper Γ given d0 6= d where the trace operation is not ap-
plicable. One simple way is to set Γ as some matrix norms. However, the issue is that using matrix
norm would make it hard for SGD learning. The reason is that the Γ and the expectation in (1)
cannot exchange the order, in which case there is no unbiased estimation by mini-batch samples for
the gradient. Here, we specify Γ as max-pooling over different dimensions of Apφ [fπ (x)], i.e. the
gradient would back-propagate through the dimension with largest absolute value at one time. The-
oretically, such setting can guarantee the value in each dimension reduces to zero through training
and we find it works well in practice.
21
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
Algorithm 1: Training Algorithm for Stein Bridging
REQUIRE: observed training samples {x}〜Preai.
REQUIRE: θ0, φ0, ψ0, π0, initial parameters for generator, estimator, Wasserstein critic and
Stein critic models respectively. αE = 0.0002, β1E = 0.9, β2E = 0.999, Adam
hyper-parameters for explicit models. α* I * * * * * * = 0.0002, β1I = 0.5, β2I = 0.999, Adam
hyper-parameters for implicit models. λ1 = 1, λ2, weights for D2 and D3 (we suggest
increasing λ2 from 0 to 1 through training). nd = 5, nc = 5 number of iterations for
Wasserstein critic and Stein critic, respectively, before one iteration for generator and
estimator. B = 100, batch size.
while not converged do
for n = 1,…，nd do
Sample B true samples {xi}iB=1 from {x};
Sample B random noise {zi}B=ι 〜 P0 and obtain generated samples ei = Gθ(Zi);
Ldis = B1 PB=I dψ (Xi)- dψ (ei) - λ(kVχi dψ (Xi)k - 1)2 // the last term is for
gradient penalty in WGAN-GP where Xi = CiXi + (1 — ∈i)Xi, Ei 〜U(0,1)；
ψk+ι J Adam(-Ldis,ψk,αI, βI, βI)// update the Wasserstein critic;
for n = 1, ∙∙∙ , n do
Sample B true samples {Xi}iB=1 from {X};
Sample B random noise {zi}B=ι 〜 Po and obtain generated samples ei = Gθ(Zi);
Lcritic = B1 PB=I λ1Apφ [fπ(x)] + λ2Apφ [fπ(xi)];
∏k+ι J Adam(-Lcritic, ∏k, αE, βE, βE)// update the Stein critic;
Sample B random noise {zi}B=ι 〜 Po and obtain generated samples ei = Gθ(Zi);
Lest = B1 PB=I λ1APφf∏(X)] + λ2APφ [fπ(ei)];
φk+1 J Adam(Lest, φk, αE, β1E, β2E)// update the density estimator;
Lgen = ⅛ Pi=1 -dψ(Xei) + λ2Apφ[fπ(Xei)];
θk+ι J Adam(Lgen θk, αI, β∣, β∣)// update the sample generator;
OUTPUT: trained sample generator Gθ(Z) and density estimator pφ (X).
For synthetic datasets, we set the noise dimension as 4. All the generators are specified as a three-
layer fully-connected (FC) neural network with neuron size 4-128-128-2, and all the Wasserstein
critics (or the discriminators in JS-divergence-based GAN) are also a three-layer FC network with
neuron size 2 - 128 - 128 - 1. For the estimators, we set the expert number as 4 and the feature
function n(X) is a FC network with neuron size 2 - 128 - 128 - 4. Then in the last layer we sum
the outputs from each expert as the energy value E(X). The activation units are searched within
[LeakyReLU, tanh, sigmoid, sof tplus]. The learning rate [1e - 6, 1e - 5, 1e - 4, 1e - 3, 1e -
2] and the batch size [50, 100, 150, 200]. The gradient penalty weight for WGAN is searched in
[0, 0.1, 1, 10, 100].
For MNIST dataset, we set the noise dimension as 100. All the critics/discriminators are im-
plemented as a four-layer network where the first two layers adopt convolution operations with
filter size 5 and stride [2, 2] and the last two layers are FC layers. The size for each layer is
1 - 64 - 128 - 256 - 1. All the generators are implemented as a four-layer networks where
the first two layers are FC and the last two adopt deconvolution operations with filter size 5 and
stride [2, 2]. The size for each layer is 100 - 256 - 128 - 64 - 1. For the estimators, we con-
sider the expert number as 128 and the feature function is the same as the Wasserstein critic ex-
cept that the size of last layer is 128. Then we sum the outputs from each expert as the energy
value. The activation units are searched within [ReLU, LeakyReLU, tanh]. The learning rate
[2e - 5, 2e - 4, 2e - 3, 2e - 2] and the batch size [32, 64, 100, 128]. The gradient penalty weight for
WGAN is searched in [1, 10, 100, 1000].
For CIFAR dataset, we adopt the same architecture as DCGAN for critics and generators. As for the
estimator, the architecture of feature function is the same as the critics except the last year where we
set the expert number as 128 and sum each output as the output energy value. The architectures for
Stein critic are the same as Wasserstein critic for both MNIST and CIFAR datasets. In other words,
22
Under review as a conference paper at ICLR 2021
Table 4: Distances between means of generated digits (resp. images) and true digits (resp. images)
on MNIST (resp. CIFAR-10).
	MNIST Il	CIFAR			
Method	lι Dis	12 Dis	1ι Dis	l2 Dis
WGAN-GP	13.80	0.93	80.98	1.72
WGAN+LR	12.91	0.86	82.96	1.81
WGAN+ER	12.26	0.77	72.28	1.59
WGAN+VA	12.38	0.78	69.01	1.53
DGM	12.12	0.79	179.30	3.95
Joint-W	11.82	0.73	64.23	1.41
Table 5: Quantitative results including MMD (lower is better), HSR (higher is better) as the metrics
for quality of generated samples and KLD (lower is better), JSD (lower is better), AUC (higher is
better) as the metrics for accuracy of estimated densities on Two-Circle and Two-Spiral datasets.
TWo-CirlCe	∣∣	TWo-SPiral
Method	MMD	HSR	KLD	JSD	AUC Il MMD		HSR	KLD	JSD	AUC
GAN	0.0033	0.772	-	-	-	0.0082	0.583	-	-	-
GAN+VA	0.0118	0.295	-	-	-	0.0085	0.761	-	-	-
WGAN-GP	0.0010	0.841	-	-	-	0.0090	0.697	-	-	-
WGAN+VA	0.0016	0.835	-	-	-	0.0159	0.618	-	-	-
DEM	-	-	2.036	0.431	0.683	-	-	1.206	0.315	0.640
EGAN	-	-	3.350	0.474	0.616	-	-	1.916	0.445	0.499
DGM	0.0040	0.774	2.272	0.445	0.600	0.0019	0.833	1.725	0.414	0.589
Joint-JS	0.0037	0.883	1.104	0.297	0.962	0.0031	0.717	0.655	0.193	0.808
Joint-W	0.0007	0.844	1.030	0.281	0.961	0.0003	0.909	0.364	0.110	0.810
We consider d0 = 1 in (1) and further simply φ as an average of each dimension of Ex〜p[AqF(x)].
EmPiriCally We found this setting Can Provide effiCient ComPutation and deCent PerformanCe.
E.4 Evaluation Metrics
We adopt some quantitative metrics to evaluate the performance of each method on different tasks.
In section 4.1, We use tWo metrics to test the sample quality: Maximum Mean Discrepancy (MMD)
and High-quality Sample Rate (HSR). MMD measures the discrepancy betWeen tWo distributions X
and Y, MMD(X,Y) = k1 Pn=1 Φ(χi) - m1 Pj=I Φ(yi)k where Xi and yj denote samples from
X and Y respectively and Φ maps each sample to a RKHS. Here We use RBF kernel and calculate
MMD betWeen generated samples and true samples. HSR statistics the rate of high-quality samples
over all generated samples. For TWo-Cirlce dataset, We define the generated points Whose distance
from the nearest Gaussian component is less than σ1 as high-quality samples. We generate 2000
points in total and statistic HSR. For TWo-Spiral dataset, We set the distance threshold as 5σ2 and
generate 5000 points to calculate HSR. For CIFAR, We use the Inception V3 NetWork in TensorfloW
as pre-trained classifier to calculate inception score.
In section 4.2, We use three metrics to characterize the performance for density estimation: KL
divergence, JS divergence and AUC. We divide the map into a 300 meshgrid, calculate the unnor-
malized density values of each point given by the estimators and compute the KL and JS divergences
betWeen estimated density and ground-truth density. Besides, We select the centers of each Gaussian
components as positive examples (expected to have high densities) and randomly sample 10 points
Within a circle around each center as negative examples (expected to have relatively loW densities)
and rank them according to the densities given by the model. Then We obtain the area under the
curve (AUC) for false-positive rate v.s. true-positive rate.
23
Under review as a conference paper at ICLR 2021
38c3<∕y97qqγ
} 3 56 9ro 2 q6/ v* 4
3VS 3 000∖ZX9 夕 I
U£ i*5∖3zg3γ8s
X 夕 g3F6「706Qio
Ul 7 6 O 7$5/16 /
SbO67507%。。Sg
0∕∖35q 彳 6?夕 g√7
763Q∙∕1or435
o⅛⅛8q∕7√GG>3∕g
52009。43，。46“
夕/，72夕Γ7 5 g¾G
L)<fcO/ /f // H 4/ ^zoλ√»(
3J537r3G234q7
366 IZb 3 6 91 LΩ∕Z
LrlC 7362∕∕3oa
I∕536∙τGg02 7,
0 7e∖3 qQyt)÷l7g
3 夕。夕 5I∕ΣΓQO34/
44，夕/ 8，32。^5,
S376 -6 ? 4 5x1 / /
Y2夕J"gv& a『83
<07b O，0//〃/?
/3。//0001840
(a) Randomly sampled over all digits	(b) Randomly sampled over digits with top 50% densi-
ties
Figure 9: Generated digits given by Joint-W on MNIST.
(a) Randomly sampled over all images (b) Randomly sampled over images with top 50% densi-
ties
Figure 10: Generated images given by Joint-W on CIFAR.
24
Under review as a conference paper at ICLR 2021
J ∖ f ∖ k ∖ ∖ ∖ / ∖ / ∖ ∖ ∖ / ∖ ∖ ∖ ∖. {
1∣1M!I1M∕∕11∕(I1∣∕
55 ^55^5 5 S55S>55⅛5^55 5
。白3Qq 9。93彳3?995?3
(a)	Generated digits with highest densities
(b)	Generated digits with lowest densities
6S,G5"S5》分£6:556£$5
夕,q(7 ? 91、q 夕夕 gqq 夕，
IJ∕∕∕f∕∕Hi(l∕∕lI∕ll
S"5∈S⅛5 5'65⅞55∙55^¾55^J-
，？，，qqq夕qq夕9弓99§>933
(c)	Real digits with highest densities
(d)	Real digits with lowest densities
Figure 11:	The generated digits (and real digits) with the highest densities and the lowest densities
given by Joint-W.
1 \ / 1 ) i s* λ 3 * e 八) \ 5 M n £ V
5bS565G 岁 c55C5G5 5∙5o556
q q q s、q <⅞ q g自守夕。令 /向 q u< u∣守
(b) Generated digits with lowest densities
(a) Generated digits with highest densities
∖∕1∕∕∕∕1∕∕∕∕∕∕∕∕1∕71
&5SB//G 4/65号m=5方万/5万万万
r4QQo1q/RqC⅝qjqdqqqqj?
S夕£5,个5555£5J£S5555与
夕qqyq，q夕，夕夕夕夕夕g夕夕夕夕g
(c)	Real digits with highest densities
(d)	Real digits with lowest densities
Figure 12:	The generated digits (and real digits) with the highest densities and the lowest densities
given by DGM.
，「5/5 EMS^F, =6S5f5-Sr■广，
9，9夕夕夕夕夕。夕夕夕夕夕2夕，夕夕夕
J4///////////////Z//
SSS555S55S65S5S65554S
q°ιo∣RRQqcιc)qqqq)气 qqg∖c⅝3
(a)	Generated digits with highest densities
(b)	Generated digits with lowest densities
_llllll/MI / Illlll I 1112////1///1/7////////
$「'5「5右3k「引「5$S9广二广/155b$5$3&555$5555£令55
夕 αo 夕9,男广 SS 夕夕。2 2 3 夕 9。夕.Rqgqgqqqqc)今\。qqqc⅝qqq
(c) Real digits with highest densities	(d) Real digits with lowest densities
Figure 13:	The generated digits (and real digits) with the highest densities and the lowest densities
given by EGAN.
25