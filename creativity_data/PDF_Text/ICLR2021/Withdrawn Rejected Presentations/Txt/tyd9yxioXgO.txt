Under review as a conference paper at ICLR 2021
Compositional Video Synthesis with Action
Graphs
Anonymous authors
Paper under double-blind review
Figure 1: We focus on video synthesis from actions and propose a new task called Action Graph to Video. To
represent input actions, we use a graph structure called Action Graph, and together with the first frame and
first scene layout, our goal is to synthesize a video that matches the input actions. For illustration, we include
above a (partial) example. Our model outperforms various baselines and can generalize to previously unseen
compositions of actions.
Abstract
Videos of actions are complex signals, containing rich compositional structure.
Current video generation models are limited in their ability to generate such
videos. To address this challenge, we introduce a generative model (AG2Vid)
that can be conditioned on an Action Graph, a structure that naturally represents
the dynamics of actions and interactions between objects. Our AG2Vid model
disentangles appearance and position features, allowing for more accurate genera-
tion. AG2Vid is evaluated on the CATER and Something-Something datasets and
outperforms other baselines. Finally, we show how Action Graphs can be used for
generating novel compositions of actions.
1	Introduction
Learning to generate visual content is a fundamental task in computer vision, with numerous appli-
cations from sim-to-real training of autonomous agents, to creating visuals for games and movies.
While the quality of generating still images has leaped forward recently (Karras et al., 2020; Brock
et al., 2019), generating videos is much harder. Generating actions and interactions is perhaps the
most challenging aspect of conditional video generation. Actions create long-range spatio-temporal
dependencies between people and the objects they interact with. For example, when a player passes
a ball, the entire movement sequence of all entities (thrower, ball, receiver) must be coordinated
and carefully timed. The current paper focuses on this difficult obstacle, the task of generating
coordinated and timed actions, as an important step towards generating videos of complex scenes.
Current approaches for conditional video generation are not well suited to condition the generation
on actions. First, future video prediction (Ye et al., 2019; Watters et al., 2017), generates future
frames based on an initial input frame, but a first frame cannot be used to predict coordinated actions.
Second, in video-to-video , the goal is to translate a sequence of semantic masks into an output video.
1
Under review as a conference paper at ICLR 2021
However, segmentation maps contain only class information, and thus do not explicitly capture
the action information. As Wang et al. (2018a) notes, this is a limitation that leads to systematic
mistakes, such as in the case of car turns. Finally, text-to-video (Li et al., 2018; Gupta et al., 2018) is
potentially useful for generating videos of actions because language can describe complex actions.
However, in applications that require a precise description of the scene, language is not ideal due
to ambiguities (MacDonald et al., 1994) or subjectivity of the user (Wiebe et al., 2004). Hence, we
address this problem with a more structural approach.
To provide a better way to condition on actions, we introduce a formalism we call an “Action Graph”
(AG), propose a new task of “Action Graph to Video” (AG2Vid), and present a model for this task.
An AG is a graph structure aimed at representing coordinated and timed actions. Its nodes represent
objects, and edges represent actions annotated with their start and end time (Fig. 1). We argue
that AGs are an intuitive representation for describing timed actions and would be a natural way to
provide precise inputs to generative models. A key advantage of AGs is their ability to describe the
dynamics of object actions precisely in a scene.
In our AG2Vid task, the input is the initial frame of the video and an AG. Instead of generating
the pixels directly, our AG2Vid model uses three levels of abstraction. First, we propose an ac-
tion scheduling mechanism we call “Clocked edges” that tracks the progress of actions in different
timesteps. Second, based on this, a graph neural network (Kipf & Welling, 2016) operates on the
AGs and predicts a sequence of scene layouts, and finally, pixels are generated conditioned on the
predicted layouts. We apply this AG2Vid model to the CATER (Girdhar & Ramanan, 2020) and
Something-Something (Goyal et al., 2017) datasets and show that this approach results in realistic
videos that are semantically compliant with the input AG. To further demonstrate the expressiveness
of AG representation and the effectiveness of the AG2Vid model, we test how it generalizes to pre-
viously unseen compositions of the learned actions. Human raters then confirm the correctness of
the generated actions.1
Our contributions are as follows: 1) Introducing the formalism of Action Graphs (AG) and proposing
a new video synthesis task. 2) Presenting a novel action-graph-to-video (AG2Vid) model for this
task. 3) Using the AG and AG2Vid model, we show this approach generalizes to the generation of
novel compositions of the learned actions.
2	Related work
Video generation is challenging because videos contain long range dependencies. Recent generation
approaches (Vondrick et al., 2016; Kumar et al., 2020; Denton & Fergus, 2018; Lee et al., 2018;
Babaeizadeh et al., 2018; Villegas et al., 2019) extended the framework of unconditional image
generation to video, based on a latent representation. For example, MoCoGAN (Tulyakov et al.,
2018) disentangles the latent space representations of motion and content to generate a sequence of
frames using RNNs; TGAN (Saito et al., 2017) generates each frame in a video separately while
also having a temporal generator to model dynamics across the frames. Here, we tackle a different
problem by aiming to generate videos that comply with AGs.
Conditional video generation has attracted considerable interest recently, with focus on two main
tasks: video prediction (Mathieu et al., 2015; Battaglia et al., 2016; Walker et al., 2016; Watters
et al., 2017; Kipf et al., 2018; Ye et al., 2019) and video-to-video translation (Wang et al., 2019;
Chan et al., 2019; Siarohin et al., 2019; Kim et al., 2019; Mallya et al., 2020). In prediction, the goal
is to generate future video frames conditioned on few initial frames. For example, it was proposed
to train predictors with GANs (Goodfellow et al., 2014) to predict future pixels (Mathieu et al.,
2015). However, directly predicting pixels is challenging (Walker et al., 2016). Instead of pixels,
researchers explored object-centric graphs and perform prediction on these (Battaglia et al., 2016;
Luc et al., 2018; Ye et al., 2019). While inspired by object-centric representations, our method is
different from these works as our generation is goal-oriented, guided by an AG. The video-to-video
translation task was proposed by Wang et al. (2018a), where a natural video was generated from
frame-wise semantic segmentation annotations. However, densely labeling pixels for each frame is
expensive, and might not even be necessary. Motivated by this, researchers have sought to perform
generation conditioned on more accessible signals including audio or text (Song et al., 2018; Fried
1Our code and models will be released upon acceptance.
2
Under review as a conference paper at ICLR 2021
Action Graph
Clocked Graph
Pick place.
Pick place.
Grey, Large,
Metal,
Cylinder
Contain,
Contain,:
Contain, 0%
Rotate, 0%
Rotate, 3-5
Rotate, 100‰
Red,
Small,
Rubber
Cone
Grey, Large,
Metal,
Cylinder
Grey, Large,
Metal,
Cylinder
Grey, Large,
Metal,
Cylinder
Contain, 100%
Green,
-__________ Large,
,	Metal
Cone
(Contain,
^-/eein.tairɪ, 0%
Red,
Small, ___________
Rubber
^BCOne P
Figure 2: Example of a partial Action Graph execution schedule in different time-steps.
et al., 2019; Ginosar et al., 2019). Here, we propose to synthesize videos conditioned on a novel AG,
which is easy to obtain compared to semantic segmentation and is a more structured representation
compared to natural audio and text.
A recent method, HOI-GAN (HG) (Nawhal et al., 2020b), was proposed for the generation task of
a single action and object. Specifically, this work addresses the zero-shot setting, and the model
is tested on action and object compositions which are first presented at test time. Our focus is on
generation of multiple simultaneous actions over time, performed by multiple objects. Our approach
directly addresses this challenge via the AG representation and the notion of clocked edges.
Various methods have been proposed to generate videos based on input text (Marwah et al., 2017;
Pan et al., 2017; Li et al., 2018). Most recent methods typically used very short captions which
do not contain complex descriptions of actions. For example, (Li et al., 2018) used video-caption
pairs from YouTube, where typical captions are ”playing hockey” or ”flying a kite”. Gupta et al.
(2018) proposed the Flinstones animated dataset and introduced the CRAFT model for text-to-video
generation. While the CRAFT model relies on text-to-video retrieval, our approach works in an
end-to-end manner and aims to accurately synthesize the given input actions.
Scene Graphs (SG) (Johnson et al., 2015; 2018) are a structured representation that models scenes,
where objects are nodes and relations are edges. SGs have been widely used in various tasks in-
cluding image retrieval (Johnson et al., 2015; Schuster et al., 2015), relationship modeling (Krishna
et al., 2018; Schroeder et al., 2019; Raboh et al., 2020), SG prediction (Xu et al., 2017; Newell &
Deng, 2017; Zellers et al., 2018; Herzig et al., 2018), and image captioning (Xu et al., 2019). Re-
cently, SGs have been applied to image generation (Johnson et al., 2018; Deng et al., 2018; Herzig
et al., 2020), where the goal is to generate a natural image corresponding to the input SG. More
generally, spatio-temporal graphs have been explored in the field of action recognition (Jain et al.,
2016; Sun et al., 2018; Wang & Gupta, 2018; Yan et al., 2018; Girdhar et al., 2019; Herzig et al.,
2019; Materzynska et al., 2020). For example, a space-time region graph is proposed by (Wang &
Gupta, 2018) where object regions are taken as nodes and a GCN (Kipf & Welling, 2016) is applied
to perform reasoning across objects for classifying actions. Recently, it was also shown by (Ji et al.,
2019; Yi et al., 2019; Girdhar & Ramanan, 2020) that a key obstacle in action recognition is the
ability to capture the long-range dependencies and compositionality of actions. While inspired by
these approaches, we focus on generating videos which is a very different challenge.
Recently, Ji et al. (2019) presented Action Genome, a new video dataset annotated by SGs. This
dataset includes spatio-temporal SG annotations, where for each video, few individual frames were
chosen and spatially annotated by SGs. Here, we use the Something-Something V2 (Goyal et al.,
2017) dataset that is larger (200K vs. 10K videos) and more diverse since it includes basic hu-
man activities created by a large number of crowd workers. Finally, we propose the Action Graph
representation, which we view as a temporal extension of SGs.
3	Action Graphs
Our goal in this work is to build a model for synthesizing videos that contain a specified set of
actions. A key component in this effort is developing a semantic representation to describe the
actions performed by different objects in the scene. Towards this end, we introduce a formalism we
call Action Graph (AG). In an AG, nodes correspond to objects, and edges correspond to actions that
3
Under review as a conference paper at ICLR 2021
Figure 3: Our AG2Vid Model. The AG At describes the execution stage of each action at time t. Together
with the previous layout `t-1, it is used to generate the next layout `t which has object representations that are
enriched with At actions information. Then, `t, `t-1, vt-1 are used to generate the next frame.
these objects participate in. Objects and actions are annotated by semantic categories, and actions
are also annotated by their start and end times.
More formally, an AG is a tuple (C, A, O, E) described as follows:
•	An alphabet of object categories C. Categories can be compounded and include attributes. For
example “Blue Cylinder” or “Large Box”.
•	An alphabet of action categories A. For Example “Slide” and “Rotate”. Similarly, actions can
contain attributes (e.g., rotation speed).
•	Object nodes O: A set O ∈ Cn of n objects.
•	Action edges E: Actions are represented as labeled directed edges between object nodes. Each
edge is annotated with an action category and with the time period during which the action is
performed. Formally, each edge is of the form (i, a, j, ts, te) where i, j ∈ {1, ..., n} are object
instances, a ∈ Ais an action and ts, te ∈ N are action start and end time. Thus, this edge implies
that object i (which has category oi) performs an action a over objectj, and that this action takes
place between times ts and te . We note that an AG edge can directly model actions over a single
object and a pair of objects. For example, “Swap the positions of objects i and j between time 0
and 9” is an action over two objects corresponding to edge (i, swap, j, 0, 9). Some actions, such
as “Rotate”, involve only one object and will therefore be specified as self-loops.
4 Action Graph to Video via Clocked Edges
We now turn to the key challenge of this paper: transforming an AG into a video. Naturally, this
transformation will be learned from data. The generation problem is defined as follows: we wish to
build a generator G that takes as input an AG and outputs a video. We will also allow conditioning
on the first video frame and layout, so we can preserve the visual attributes of the given objects.2
There are multiple unique challenges in generating a video from an AG that cannot be addressed
using current generation methods. First, each action in the graph unfolds over time, so the model
needs to “keep track” of the progress of actions rather than just condition on previous frames as
commonly done. Second, AGs may contain multiple concurrent actions and the generation process
needs to combine them in a realistic way. Third, one has to design a training loss that captures the
spatio-temporal video structure to ensure that the semantics of the AG is accurately captured.
Clocked Edges. As discussed above, we need a mechanism for monitoring the progress of action
execution during the video. A natural approach is to keep a “clock” for each action, for keeping
track of action progress as the video progresses. See Fig. 2 for an illustration. Formally, we keep
a clocked version of the graph A where each edge is augmented with a temporal state. Let e =
2Using the first frame and layout can be avoided by using a SG2Image model (Johnson et al., 2018; Ashual
& Wolf, 2019; Herzig et al., 2020) for generating the first frame.
4
Under review as a conference paper at ICLR 2021
(i, a,j, ts,te) ∈ E be an edge in the AG A. We define the progress of e at time t to be r = 1-t ,
and clip r to [0, 1]. Thus, if r = 0 the action has not started yet, if r ∈ (0, 1) it is currently being
executed, and if r = 1 it has completed. We then create an augmented version of the edge e at time
t given by et= (i, a, j, ts, te, r). We define At= {et|e ∈ A} to be the AG at time t. To summarize
the above, we take the original graph A and turn it into a sequence of AGs A0, . . . , AT , where T
is the last time-step. Each action edge in the graph now has its unique clock for its execution. This
facilitates both a timely execution of actions and coordination between actions.
4.1	THE AG2VID MODEL
Next, we describe our proposed AG-to-video model (AG2Vid). Fig. 3 provides a high-level illustra-
tion of our model architecture. The rationale of our generation process is that first the AG is used
to produce intermediate layouts, and then these layouts are used to produce frame pixels We let
`t= (xt, yt, wt, ht, zt) denote the set of predicted layouts for all the n objects in the video at time
t. The values xt, yt, wt, ht ∈ [0, 1]n are the bounding box coordinates for all objects, and ztis a
descriptor vector for the object (later used for frame generation). Let vt denote the generated frame
at time t, and p(v2 ,...,vτ, '2 ,...'t ∣A,v1,'1) denote the generating distribution of the frames and
layouts given the AG and the first frame v1 and scene layout l1 .
We assume that the generation of the frame and layout directly depends only on recent
generated frames and layouts.3 Specifically, this corresponds to the following form for p:
p(v2, ...,vT,'2, ..., 'T|A, v1, l1) = QtT=2p('t|At,'t-1)p(vt|vt-1,'t,'t-1). We refer to the distri-
bution p(lt∣∙) as The Layout Generating Function (LGF) and to p(vt∣∙) as The Frame Generating
Function (FGF). Next, we describe how we model these distributions as functions.
The Layout Generating Function (LGF). At time t we want to use the previous layout 't-1 and
current AG Atto predict the current layout 't. The rationale is that At captures the current state of
the actions and can thus “propagate” 't-1 to the next layout. This prediction requires integrating
information from different objects as well as the progress of the actions given by the edges of At.
Thus, a natural architecture for this task is a Graph Convolutional Network (GCN) that operates on
the graph At whose nodes are “enriched” with the layouts 't. Formally, we construct a new graph of
the same structure as At, with new features on nodes and edges. At the graph node corresponding
to object i the features are comprised of the previous object location defined in 'it-1 and object class
embedding. The features on the edges are the embedding of action a and the progress of the action
r, taken from (i, a, j, r) from At. Then, node and edge features are repeatedly re-estimated for K
steps using a GCN. The resulting activations of the ith object at timestep t are zti ∈ RD which we
use as the new object descriptor. An MLP is then applied to it to produce the new box coordinates,
which together form the predicted layout 't. For more details refer to Sec.1 in the Suppl.
The Frame Generating Function (FGF). After obtaining the layout 't which contains updated
objects representations zt, we wish to use it along with vt-1 and 't-1 to predict the next frame.
The idea is that 't, 't-1 characterize how objects should move, zt, zt-1 should capture the object-
actions dynamics, and vt-1 shows their last physical appearance. Combining these information
sources we should be able to generate the next frame accurately. As a first step, we construct a mask
mt-1, mt∈ RH×W×D using the embedding and layout pairs. Then, we estimate the optical flow
at time t, denoted by ft. We let ft= F(vt-1, mt-1, mt). The idea is that given the previous frame
and two consecutive layouts, we should be able to predict in which direction pixels in the image
will move, namely predict the flow. The flow prediction network F is similar to (Ilg et al., 2017),
and it is trained using an auxiliary loss and does not require additional supervision (see section 4.2).
Given the flow ft and previous frame vt-1 a natural estimate of the next frame is to use a warping
function (Zhou et al., 2016) wt= W(ft, vt-1). Finally we fine-tune wt via a network S(mt, wt)
that provides an additive correction resulting in the final frame prediction: vt = wt+ S(mt, wt),
where the S network a SPADE generator (Park et al., 2019).
3 Note that layout contain descriptors that can encode all generation history.
5
Under review as a conference paper at ICLR 2021
Actions in CATER
Figure 4: Qualitative examples of generation on CATER and Something Something. AG2Vid generated videos
of four and eight standard actions on CATER and Something Something, respectively. For CATER we also
used AGs with multiple simultaneous actions, and the generated actions indeed correspond to those (verified
manually). For more examples please refer to Figure 1 and 2 in the Supp. Click the image to play the video clip
in a browser.
Actions in Something Something
Push Left	MoveDown	UnCober PKSh
PKSh Right Mobe UP Cover
Take
4.2	Losses and Training
We use ground truth frames VGT and layouts '(GT for training,4 and use the following losses:
Layout Prediction Loss l`. Defined as l` = ∣∣'t - '(GT∣∣1, the L1 loss between ground-truth
bounding boxes 'GT and predicted boxes 't. Here We ignore the object descriptor part of 't.
Pixel Action Discriminator Loss LA. For the generated pixels vt we employ a GAN loss that
uses a discriminator between generated frames vt and GT frames vtGT conditioned on At and lt.
Formally, let DA be a discriminator with output in (0, 1). First, a GCN is applied over At to ob-
tain objects representations, which are then using together with the GT layout boxes to construct a
scene layout. The layout and frames are then concatenated and fed into a multi-scale PatchGAN
discriminator (Wang et al., 2018b). The loss is then the GAN loss (e.g., see Isola et al. (2017)):
LA = maxEGT [log DA(At,v(GT,'GT)] + Ep [log(1 - DA(At,vt,'GT))]	⑴
DA
where GT corresponds to sampling frames from the ground truth videos, and p corresponds to sam-
pling from the generated videos. Optimization of this loss is done in the standard way of alternating
gradient ascent on DA parameters and descent on generator parameters.
Flow Loss Lf. The flow loss measures the error between the warps of the previous frame and the
ground truth of the next frame VGT: Lf = T--ɪ PT-LIIlWt — Vt ∣∣1, where wt = W(ft,vt-1) as
defined in Section 4.1. This loss was proposed previously by (Zhou et al., 2016; Wang et al., 2018a).
Perceptual and Feature Matching Loss LP . We use these losses as proposed in pix2pixHD (Wang
et al., 2018b; Larsen et al., 2016) and other previous works.
The overall optimization problem is to minimize the weighted sum of the losses.
5	Experiments and Results
We evaluate our AG2Vid model on two datasets: CATER and Something Something V2 (Smth). For
each dataset, we learn an AG2Vid model with a given set of actions. We then evaluate the visual
quality of the generated videos and measure how they semantically comply with the input actions.
Last, we estimate the generalization of the AG2Vid model to novel composed actions.
Datasets. We use two datasets: (1) CATER (Girdhar & Ramanan, 2020) is a synthetic video
dataset originally created for action recognition and reasoning. Each video contains multiple objects
performing actions. The dataset contains bounding-box annotations for all objects, as well as labels
of the actions. These include: “Rotate”, “Cover”, “Pick Place” and “Slide”. See Figure 7 for
4GT layouts can be obtained automatically from videos using object tracking as a pre-processing step.
6
Under review as a conference paper at ICLR 2021
Hand
Coin
Move Right,
l-10z
Move Up,
1-10
Move Left,
Move Down, Contain,
Pick-
Place,
1-10,
Loc(P-Sphere)
l!wθ, CI^here
Loc(G-Cone)
GT
CVP
V2V
OurS
Figure 5: Comparison of baselines methods. The
top row are based on CATER videos, while the
bottom row are based on Something Something.
Click the images to play the video clips in a browser.
I ⅜J
Huddle
Swap
Figure 6: Composing unseen actions in Something-
Something and CATER. For example, the “swap” ac-
tion is composed by combining the “Pick-Place” and
“Slide” actions on frames 1 - 10 and their locations.
Right Up Left Down
Table 1: Human evaluation of action generation with respect to Semantic Accuracy and Visual Quality. For
each metric, raters selected the better of two generation methods. In the results XX/Y Y means that AG2Vid
was selected as better for X% of the presented pairs. Image resolution is 256 × 256.
Methods	I Semantic Accuracy	Visual Quality
AG2Vid/Baseline		CATER	Smth	CATER	Smth
AG2Vid/CVP( etal.,201 )	85.7/14.3	90.6/9.4	76.2/23.8	93.8/6.2
AG2Vid / HG ( awhal et al., 2020 )	-/-	84.6/15.4	-/-	88.5/11.5
AG2Vid / V2V ( ng et al., 2018a)	68.8/31.2	84.4/15.6	68.8/31.2	96.9/3.1
AG2Vid/RNN	56.0/44.0	80.6/19.4	52.0/48.0	77.8/22.2
AG2Vid/AG2Vid, GTL		48.6/51.4	46.2/53.8	42.9/57.1	50.0/50.0
qualitative examples. For “Pick Place” and “Slide” we include the action destination coordinates.
We use these actions to create action graphs for training and evaluation.
We employ the standard CATER training partition (3849 videos) and split the validation into 30%
val (495 videos) and use the rest for testing (1156 videos). (2) Something Something V2 (Goyal
et al., 2017) contains real world videos of humans performing basic actions. Here we use the
eight most frequent actions (e.g., “Putting [something] on a surface” and “Covering [something]
with [something]”). All videos contain up to three different objects, including the hand which is
performing the action. We use the box annotations of the objects from Materzynska et al. (2020).
See Sec. 3 in Suppl for the full list of actions.
Implementation details. The GCN model uses K = 3 hidden layers and an embedding layer of
128 units for each object and action. For optimization we use ADAM Kingma & Ba (2014) with
lr = 1e - 4 and (β1, β2) = (0.5, 0.99). Models were trained on an NVIDIA V100 GPU. For loss
weights (see section 4.2) we use λB = λF = λP = 10 and λA = 1. For training we use a batch size
of 2. We use videos of 8 FPS and 6 FPS for CATER and Smth and evaluate on videos consisting of
16 frames which correspond to spans of 2.7 and 2 seconds accordingly.
Performance metrics. The AG2Vid outputs can be quantitatively evaluated as follows. a) Visual
Quality: It is common in video generation to evaluate the visual quality of videos, regardless of
the semantic content. To evaluate visual quality, we use the Learned Perceptual Image Patch Sim-
ilarity (LPIPS) (Zhang et al., 2018) (lower is better) over predicted and GT videos. For the Smth
dataset, since videos contain single actions (meaning the AG contains a single triplet), we can report
the Inception Score (IS) Salimans et al. (2016) (higher is better) and Frechet Inception Distance
(FID) (Heusel et al., 2017) (lower is better) using a TSM (Lin et al., 2019) model, pretrained on
Smth. We note that we cannot report FID and IS on CATER since it provides multiple activities
simultaneously, and hence does not support a pretrained video classifier. Finally, we also evaluate
relative visual quality of two models by asking human annotators to select the video with higher
quality. b) Semantic Accuracy: The key goal of AG2Vid is to generate videos which contain spec-
ified actions. To evaluate this, we ask human annotators to select which of two video generation
7
Under review as a conference paper at ICLR 2021
Table 2: Visual quality metrics of conditional video-generation methods in CATER and Smth. All methods use
resolution 256 × 256 except for HG, which only supports 64 × 64.
In	I~.	I	Γ7^7~ι	I Inception ↑ I FID ] I	LPIPS ∖
Resolution 1	Methods
		Smth	Smth	CATER	Smth
	Real Videos	3.9 ± 0.12	^^0.0 ± 0.0^^	0.0 ± 0.0	0.0 ± 0.0
64x64	HG( awhaletal.,2020 )	1.66 ± 0.03	35.18 ± 3.6	-	0.33 ± 0.08
	AG2Vid (Ours)	2.51 ± 0.08	26.05 ± 0.73	0.04 ± 0.01	0.13 ± 0.01
	Real Videos	7.58 ± 0.2	^^0.0 ± 0.0^^	0.0 ± 0.0	0.0 ± 0.0
	-CVP(YeetaL,201 )-	1.92 ± 0.03	67.77 ± 1.43	0.24 ± 0.04	0.55 ± 0.08
	RNN	1.99 ± 0.05	74.17 ± 1.54	0.14 ± 0.05	0.26 ± 0.08
256x256	V2V( g etal., 2018 )	2.22 ± 0.07	67.51 ± 1.42	0.11 ± 0.02	0.29 ± 0.09
	AG2Vid (OurS)	3.02 ± 0.11	66.7 ± 1.29	0.07 ± 0.02	0.25 ± 0.08
	AG2Vid, GTL (OurS)	3.52 ± 0.14	65.04 ± 1.25	0.06 ± 0.02	0.22 ± 0.09
Table 3: Ablation experiment for components of the frame generation. Losses are added one by one.
Loss	Inception ↑	FID ；	LPIPS 1	
	Smth	Smth	CATER	Smth
Flow	1.59 ± 0.02	107.26 ± 1.46	.14 ±.01	.70 ± .06
+ Perceptual	2.21 ± 0.07	71.70 ± 1.46	.08 ±.03	.29 ± .07
+ Action Disc.	3.02 ± 0.11	66.7 ± 1.29	.07 ± .02	.25 ± .08
models provides a better depiction of actions in the real video. The protocol is similar to the visual
quality evaluation above. We also evaluated action timing, see below.
Compared methods. Generating videos based on action-graphs is a new task. There are no off-the-
shelf models that can be used for direct evaluation with our approach, since no existing models take
as input an action graph and output a video. To provide fair evaluation, we compare with two types
of baselines. (1) First, existing baseline models that share some functionality with AG2Vid. (2)
Second, variants of the AG2Vid model that shed light on its design choices. Each baseline serves to
evaluate specific aspects of the model, as described next. Baselines: (1) HOI-GAN (HG) (Nawhal
et al., 2020b) generates videos given a single action-object pair, an initial frame and a layout. It can
be viewed as operating on a two-node action graph without timing information. we compare HG to
AG2Vid on the Smth dataset because it contains exactly such action graphs. HG is not applicable
to CATER data. (2) CVP (Ye et al., 2019) uses as input the first image and layout for future frame
prediction without access to action information. CVP allows us to asses the visual quality of AG2Vid
videos. However, it is not expected that CVP captures the semantics of the action-graph, unless
the first frame and action are highly correlated (e.g., a hand at the top-left corner always moves
downwards). (3) V2V (Wang et al., 2018a): This baseline uses a state-of-the-art Vid2Vid model
based on (Wang et al., 2018a) to generate videos from ground-truth layout. Since it uses ground-
truth layout it provides an upper bound on Vid2Vid performance for this task. We note that Vid2Vid
cannot use the action graph, and thus it is not provided as input. AG2Vid variants: (4) RNN: This
AG2Vid variant replaces the layout generation GCN with an RNN that processes the action graphs.
The frame generation part is the same as AG2Vid. More details are provided in the Supp Sec.4.1.
(5) AG2Vid, GTL: An AG2Vid model that uses ground truth layout at inference time. It allows us
to test if using the GT layout for all frames improves overall AG2Vid video quality and semantics.
Layout Generation Ablations. We experiment with an RNN architecture as an alternative to
the GCN implementation of the LGF. The motivation behind the RNN experiment is to compare
the design choice of the GNN to a model that processes edges sequentially (RNN). This RNN
has access to the same input and supervision to the GCN, namely, lt-1 and At, and the results
from Table 4 confirm the advantage of GCN processing. For more details, see Sec. 4.1 in the
Supplementary.
Loss Ablations. Table 3 reports ablations over the losses of FGF, confirming that the perceptual
loss and actions discriminator losses improve the overall visual quality on CATER and Smth.
Semantic and Visual Quality. Fig. 4 shows sample videos generated by AG2Vid, and Fig. 5 shows
comparison to generation by baselines. Table 1 shows the results of human evaluations for semantic
and visual quality. It can be seen that AG2Vid is more semantically accurate and has better visual
8
Under review as a conference paper at ICLR 2021
quality than the baselines, and it is comparable to AG2Vid,GTL. See Sec 4.4 in Suppl for additional
evaluations of AG2Vid correctness of the generated actions. Table 2 evaluates visual quality using
several metrics, with similar takeaways as in Table 1.
Action Timings. To evaluate the extent to which the AG2Vid model can control the timing of
actions, we generated AGs of actions at different times and asked annotators to choose in which
video the action is executed first. In 89.45% of the cases, the annotators confirmed the intended
result. For more information see Sec. 4.3 in Suppl.
Composing New Actions. To evaluate the extent to which the AG2Vid model can generalize at test
time to unseen actions, we manually defined four compositions of learned actions. As seen in Fig. 6,
we are using learned atomic actions to generate new action combinations that did not appear in
the training data (either by having the same object perform multiple objects at the same time, or
multiple objects performing coordinated actions). For example, in CATER, we created the actions
“swap” based on “pick-place” and “slide” and “huddle” based on “contain”. For Smth we composed
the “push-left” and “move-down” to form the “left-down” action. For each generated video, raters
were asked to choose the correct action class from a list of possible actions. The avg. class re-
call for CATER and Smth is 96.65 and 87.5 respectively, see the Supplementary for results by action.
6	Discussion
We present a video-synthesis approach with anew Action Graph formalism, that describes how mul-
tiple objects interact in a scene over time. By using this formalism, we can synthesize complicated
compositional videos and construct novel actions and action combinations. Although our approach
outperforms previous methods, our model still fails in several situations. First, our model depends
on the initial frame and layout. This could be potentially addressed by using an off-the-shelf im-
age generation model. The formal AG representation is designed for describing complex semantic
information in an easy-to-grasp way. The formalism could be further extended to handle other ac-
tions or their properties that were not present in today’s datasets. For instance, it may be desired
to capture features of actions described by adverbs. This can be achieved by adding attributes over
actions, which we leave for future work. Finally, in this work we present an hierarchical and mod-
ular pipeline of video synthesis: first actions are scheduled for a specific timestep, then the scene
layout is predicted, and finally the future flow is predicted and refined. While this pipeline is fairly
general, we believe these representations can be further adopted to different datasets. For example,
pose representation can be added for videos of people.
7	B roader Impact
The paper proposes a new framework for video generation, which focuses on coordinated multiple
simple actions, operating on simple daily objects. We believe it has potential for improving the qual-
ity and versatility of video generation. Video synthesis technology has many practical implications,
such as generating simulated data for training robots and improving content search in video. These
clearly have positive societal impact. The current work does not focus on generating faces or human
movement, and as a result, we estimate that the potential for negative societal and ethic aspects is
low.
References
Oron Ashual and Lior Wolf. Specifying object attributes and relations in interactive scene genera-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 4561-4569,
2019.
Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell, and Sergey Levine.
Stochastic variational video prediction. In International Conference on Learning Representations,
2018.
Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks
for learning about objects, relations and physics. In Advances in neural information processing
systems, pp. 4502-4510, 2016.
9
Under review as a conference paper at ICLR 2021
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity
natural image synthesis. In International Conference on Learning Representations, 2019.
Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In Pro-
Ceedings of the IEEE International Conference on Computer Vision,pp. 5933-5942, 2019.
Zhiwei Deng, Jiacheng Chen, Yifang Fu, and Greg Mori. Probabilistic neural programmed networks
for scene generation. In Advances in Neural Information Processing Systems, pp. 4028-4038,
2018.
Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. In ICML, pp.
1174-1183, 2018.
Ohad Fried, AyUsh Tewari, Michael Zollhofer, Adam Finkelstein, Eli Shechtman, Dan B Gold-
man, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh Agrawala. Text-based editing of
talking-head video. ACM Transactions on Graphics (TOG), 38(4):1-14, 2019.
Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learn-
ing individUal styles of conversational gestUre. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 3497-3506, 2019.
Rohit Girdhar and Deva Ramanan. CATER: A diagnostic dataset for Compositional Actions and
TEmporal Reasoning. In ICLR, 2020.
Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video action transformer net-
work. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
244-253, 2019.
Ian Goodfellow, Jean PoUget-Abadie, Mehdi Mirza, Bing XU, David Warde-Farley, Sherjil Ozair,
Aaron CoUrville, and YoshUa Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Raghav Goyal, Samira Ebrahimi KahoU, Vincent Michalski, Joanna Materzynska, SUsanne West-
phal, HeUna Kim, Valentin Haenel, Ingo FrUend, Peter Yianilos, Moritz MUeller-Freitag, et al.
The” something something” video database for learning and evalUating visUal common sense. In
ICCV, pp. 5, 2017.
Tanmay GUpta, DUstin Schwenk, Ali Farhadi, Derek Hoiem, and AnirUddha Kembhavi. Imagine
this! scripts to compositions to videos. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 598-613, 2018.
Roei Herzig, Moshiko Raboh, Gal Chechik, Jonathan Berant, and Amir Globerson. Mapping im-
ages to scene graphs with permUtation-invariant strUctUred prediction. In Advances in Neural
Information Processing Systems (NIPS), 2018.
Roei Herzig, Elad Levi, HUijUan XU, Hang Gao, Eli Brosh, Xiaolong Wang, Amir Globerson, and
Trevor Darrell. Spatio-temporal action graph networks. In Proceedings of the IEEE International
Conference on Computer Vision Workshops, pp. 0-0, 2019.
Roei Herzig, Amir Bar, HUijUan XU, Gal Chechik, Trevor Darrell, and Amir Globerson. Learn-
ing canonical representations for scene graph to image generation. In European Conference on
Computer Vision, 2020.
Martin HeUsel, HUbert RamsaUer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale Update rUle converge to a local nash eqUilibriUm. In I. GUyon,
U. V. LUxbUrg, S. Bengio, H. Wallach, R. FergUs, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems 30, pp. 6626-6637. CUrran Associates, Inc.,
2017.
E. Ilg, N. Mayer, T. Saikia, M. KeUper, A. Dosovitskiy, and T. Brox. Flownet 2.0: EvolUtion of
optical flow estimation with deep networks. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), JUl 2017. URL http://lmb.informatik.uni-freiburg.de/
/Publications/2017/IMKDB17.
10
Under review as a conference paper at ICLR 2021
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. CVPR, 2017.
Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Structural-rnn: Deep learning
on spatio-temporal graphs. In Proceedings of the ieee conference on computer vision and pattern
recognition, pp. 5308-5317, 2016.
Jingwei Ji, Ranjay Krishna, Li Fei-Fei, and Juan Carlos Niebles. Action genome: Actions as com-
position of spatio-temporal scene graphs. arXiv preprint arXiv:1912.06992, 2019.
Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David Shamma, Michael Bernstein, and
Li Fei-Fei. Image retrieval using scene graphs. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pp. 3668-3678, 2015.
Justin Johnson, Agrim Gupta, and Li Fei-Fei. Image generation from scene graphs. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 1219-1228, 2018.
Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-
ing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), June 2020.
Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Deep video inpainting. In Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5792-5801,
2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational
inference for interacting systems. arXiv preprint arXiv:1802.04687, 2018.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Ranjay Krishna, Ines Chami, Michael S. Bernstein, and Li Fei-Fei. Referring relationships. ECCV,
2018.
Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan, Chelsea Finn, Sergey Levine, Laurent
Dinh, and Durk Kingma. Videoflow: A conditional flow-based model for stochastic video gener-
ation. In International Conference on Learning Representations, 2020.
Anders Boesen Lindbo Larsen, S0ren Kaae S0nderby, Hugo Larochelle, and Ole Winther. AUtoen-
coding beyond pixels using a learned similarity metric. In Proceedings of The 33rd International
Conference on Machine Learning, pp. 1558-1566, 2016.
Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine.
Stochastic adversarial video prediction. arXiv preprint arXiv:1804.01523, 2018.
Yitong Li, Martin Renqiang Min, Dinghan Shen, David E Carlson, and Lawrence Carin. Video
generation from text. In AAAI, pp. 5, 2018.
Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understanding.
In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October
2019.
Pauline Luc, Camille Couprie, Yann Lecun, and Jakob Verbeek. Predicting Future Instance Seg-
mentation by Forecasting Convolutional Features. In ECCV, pp. 593-608, 2018.
Maryellen C MacDonald, Neal J Pearlmutter, and Mark S Seidenberg. The lexical nature of syntactic
ambiguity resolution. Psychological review, 101(4):676, 1994.
Arun Mallya, Ting-Chun Wang, Karan Sapra, and Ming-Yu Liu. World-consistent video-to-video
synthesis. In European Conference on Computer Vision (ECCV), 2020.
11
Under review as a conference paper at ICLR 2021
Tanya Marwah, Gaurav Mittal, and Vineeth N Balasubramanian. Attentive semantic video genera-
tion using captions. In Proceedings of the IEEE International Conference on Computer Vision,
pp.1426-1434, 2017.
Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, and Trevor Darrell.
Something-else: Compositional action recognition with spatial-temporal interaction networks. In
proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.
Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond
mean square error. arXiv preprint arXiv:1511.05440, 2015.
Megha Nawhal, Mengyao Zhai, Andreas Lehrmann, Leonid Sigal, and Greg Mori. Generating
videos of zero-shot compositions of actions and objects. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), 2020a.
Megha Nawhal, Mengyao Zhai, Andreas Lehrmann, Leonid Sigal, and Greg Mori. Generating
videos of zero-shot compositions of actions and objects. In European Conference on Computer
Vision (ECCV), 2020b.
Alejandro Newell and Jia Deng. Pixels to graphs by associative embedding. In Advances in Neural
Information Processing Systems 30 (to appear), pp. 1172-1180. Curran Associates, Inc., 2017.
Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generat-
ing videos from captions. In Proceedings of the 25th ACM international conference on Multime-
dia, pp. 1789-1798, 2017.
Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with
spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 2337-2346, 2019.
Moshiko Raboh, Roei Herzig, Gal Chechik, Jonathan Berant, and Amir Globerson. Differentiable
scene graphs. In Winter Conf. on App. of Comput. Vision, 2020.
Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with sin-
gular value clipping. In ICCV, 2017.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and
Xi Chen. Improved techniques for training gans. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
2234-2242. Curran Associates, Inc., 2016.
Brigit Schroeder, Subarna Tripathi, and Hanlin Tang. Triplet-aware scene graph embeddings. In
The IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2019.
Sebastian Schuster, Ranjay Krishna, Angel Chang, Li Fei-Fei, and Christopher D Manning. Gen-
erating semantically precise scene graphs from textual descriptions for improved image retrieval.
In Proceedings of the fourth workshop on vision and language, pp. 70-80, 2015.
Aliaksandr Siarohin, StePhane Lathuiliere, Sergey Tulyakov, Elisa Ricci, and NicU Sebe. Animating
arbitrary objects via deep motion transfer. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2377-2386, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Yang Song, Jingwen Zhu, Dawei Li, Xiaolong Wang, and Hairong Qi. Talking face generation by
conditional recurrent adversarial network. arXiv preprint arXiv:1804.04786, 2018.
Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy, Rahul Sukthankar, and Cordelia
Schmid. Actor-centric relation network. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 318-334, 2018.
Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. MoCoGAN: Decomposing motion
and content for video generation. In IEEE Conference on Computer Vision and Pattern Recogni-
tion (CVPR), pp. 1526-1535, 2018.
12
Under review as a conference paper at ICLR 2021
Ruben Villegas, Arkanath Pathak, Harini Kannan, Dumitru Erhan, Quoc V Le, and Honglak Lee.
High fidelity video prediction with large stochastic recurrent neural networks. In Advances in
Neural Information Processing Systems 32, pp. 81-91. Curran Associates, Inc., 2019.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
In Advances in neural information processing systems (NIPS), 2016.
Jacob Walker, Carl Doersch, Abhinav Gupta, and Martial Hebert. An uncertain future: Forecasting
from static images using variational autoencoders. In European Conference on Computer Vision,
pp. 835-851. Springer, 2016.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan
Catanzaro. Video-to-video synthesis. In Advances in Neural Information Processing Systems
(NeurIPS), 2018a.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 8798-8807, 2018b.
Ting-Chun Wang, Ming-Yu Liu, Andrew Tao, Guilin Liu, Jan Kautz, and Bryan Catanzaro. Few-
shot video-to-video synthesis. In Advances in Neural Information Processing Systems (NeurIPS),
2019.
Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In ECCV, 2018.
Nicholas Watters, Daniel Zoran, Theophane Weber, Peter Battaglia, Razvan Pascanu, and Andrea
Tacchetti. Visual interaction networks: Learning a physics simulator from video. In Advances in
neural information processing systems, pp. 4539-4547, 2017.
Janyce Wiebe, Theresa Wilson, Rebecca Bruce, Matthew Bell, and Melanie Martin. Learning sub-
jective language. Computational linguistics, 30(3):277-308, 2004.
Danfei Xu, Yuke Zhu, Christopher B. Choy, and Li Fei-Fei. Scene Graph Generation by Iterative
Message Passing. In Proc. Conf. Comput. Vision Pattern Recognition, pp. 3097-3106, 2017.
Ning Xu, An-An Liu, Jing Liu, Weizhi Nie, and Yuting Su. Scene graph captioner: Image cap-
tioning based on structural visual representation. Journal of Visual Communication and Image
Representation, pp. 477-485, 2019.
Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for
skeleton-based action recognition. In Thirty-second AAAI conference on artificial intelligence,
2018.
Yufei Ye, Maneesh Singh, Abhinav Gupta, and Shubham Tulsiani. Compositional video prediction.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 10353-10362,
2019.
Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B
Tenenbaum. Clevrer: Collision events for video representation and reasoning. arXiv preprint
arXiv:1910.01442, 2019.
Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. Neural motifs: Scene graph parsing
with global context. In Conference on Computer Vision and Pattern Recognition, 2018.
Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectiveness of deep features as a perceptual metric. In CVPR, 2018.
Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros. View synthesis
by appearance flow. In ECCV, 2016.
13
Under review as a conference paper at ICLR 2021
In this supplementary file we provide additional information about our model, training losses, ex-
perimental results, and qualitative examples.
1	Graph Convolution Network
As explained in the main paper, we used a Graph Convolution Network (GCN) (Kipf & Welling,
2016) to predict the layout `t at time step t. The GCN uses the structure of the action graph, and
propagates information along this graph (in K iterations) to obtain a set of layout coordinates per
object.
Each object category c ∈ C is assigned a learned embedding φc ∈ RD and each action a ∈ R is
assigned a learned embedding ψa ∈ RD. We next explain how to obtain the layouts `t using a GCN.
Consider the action graph At at time t with the corresponding clocked edges (i, a, j, r). Denote the
layout for node i at time t - 1 by 't-ι,i. The GCN iteratively calculates a representation for each
object and each action in the graph. Let zi,k ∈ Rd be the representation of the ith object in the
kth layer of the GCN. Similarly, for each edge in At given by e = (i, a, j, r) let ue,k ∈ Rd be the
representation of this edge in the kth layer. These representations are calculated as follows. At the
GCN input, We set the representation for node i to be: Zi,o = 血⑷,'1,4. And, for each edge
e = (i, a,j, r) set Ue,o = [ψa, r, 't-ι,i, 't-ι,j]. All representations at time 0 are transformed to
D dimensional vectors using an MLP. Next, We use three functions (MLPs) Fs, Fa , Fo, each from
RD × RD × RD to RD. These can be thought ofas processing three vectors on an edge (the subject,
action and object representations) and returning three neW representations. Given these functions,
the updated object representation is the average of all edges incident on i:5
zi,k+1 =	Fs (zi,k , ue,k , zj,k ) +	Fo (zj,k , ue,k , zi,k )	(2)
e=(i,a,j,r)	e=(j,a,i,r)
Similarly, the representation for edge e is updated via: ue,k+1 = Fa(zi,k+1, ue,k, zj,k+1).
Finally, We transform the GCN representations above at each time-step t to a layout `t as folloWs.
Let K denote the number of GCN updates. The layout coordinates of `t,i are the output of an MLP
applied to zit,K, Which are simply the set of the predicted normalized bounding box coordinates.
The object descriptor is zit,K .
2	Losses and Training
We elaborate on the FloW and Perceptual losses from Section 4.2.
Optical flow loss LF. The floW loss LF is the Warping loss Which measures the error betWeen the
Warps of the previous frame and the ground truth of the next frame vtGT .
1 T-1
Lf =占 X kwt-vGTkI
(3)
t=1
Where wt = W(ft, vt-1) as defined in Section 4.1. This floW loss proposed previously in Wang
et al. (2018a); Zhou et al. (2016).
Perceptual loss LP. This is the standard perceptual loss as in pix2pixHD (Wang et al., 2018b).
In particular, We use the VGG netWork (Simonyan & Zisserman, 2014) as a feature extractor and
minimize the error betWeen the extracted features from the generated and ground truth images from
L layers.
L1
LP = X PPIIΦ(I)(Vt) - Φ(I)(VGT)I∣1
l Pl
(4)
5 Note that a box can appear both as a “subject” and an “object” thus tWo different sums in the denominator.
14
Under review as a conference paper at ICLR 2021
Figure 7: Qualitative examples for the generation of actions on the CATER dataset. We use the AG2Vid model
to generate videos of four standard actions and two composed unseen actions (“Swap” and “Huddle”). The
objects involved in actions are highlighted. Click the image to play the video clip in a browser.
where φ(l) denotes the l-th layer with Pl elements of the VGG network. We sum the above over all
frames in the videos.
The overall optimization problem is to minimize the weighted sum of the losses:
minmaxLA(DA) + 入'L' + λfLf + λpLP ,	(5)
where θ are all the trainable parameters of the generative model, L' is the Layout loss, and LA is the
pixel action discriminator loss from Section 4.2. In addition to the loss terms in Equation 5, we use
a feature matching loss (Larsen et al., 2016; Wang et al., 2018b) to match the statistics of features
extracted by the GAN discriminators.
3	Actions
For the Something Something dataset (Goyal et al., 2017), we use the eight most frequent actions.
These include: “Putting [something] on a surface”, “Moving [something] up”, “Pushing [something]
from left to right”, “Moving [something] down”, “Pushing [something] from right to left”, “Covering
[something] with [something]”, “Uncovering [something]”, “Taking [one of many similar things on
the table]” . See Figure 8 for qualitative examples. The box annotations of the objects from the
videos are taken from Materzynska et al. (2020).
Model	mIOU ↑		R@0.3 ↑		R@0.5 ↑	
	CATER	Smth	CATER	Smth	CATER	Smth
Random	5.05	13.55	5.94	16.50	01.86	4.81
RNN	75.71	41.28	80.67	61.70	78.91	39.23
AG2Vid	93.09	51.32	99.55	74.50	98.04	53.85
Table 4: Layout generation evaluation.
15
Under review as a conference paper at ICLR 2021
Push Right Push Left Move Up Move Down Take
Figure 8: Qualitative examples for the generation of actions on the Something Something dataset. We use our
AG2Vid model to generate videos of eight standard actions and two composed unseen actions (“Right Up” and
“Down Left”). Click the image to play the video clip in a browser.
4 Experiments and Results
4.1	RNN baseline
We experiment with an RNN architecture as an alternative to the GCN implementation of the LGF.
This RNN has access to the same input and supervision to the GCN, namely, to lt-1 and At. We
next explain how to obtain the layouts `t using the RNN.
Each object category c ∈ C is assigned a learned embedding φc ∈ RD and each action a ∈ R
is assigned a learned embedding ψa ∈ RD + 1. Consider the action graph At at time t with the
corresponding clocked edges (i, a, j, r).
let Ui ∈ RlEl×4D+1 denote the matrix, where every row that corresponds to edge is comprised of
the object, action, subject embeddings, the embedding of the ith object, and the target the progress
feature. We apply the RNN over Ui and denote zi,t as the last hidden state of the result. The new
object descriptor of lt,i is then zi,t, and to obtain a new bounding box location, an MLP is applied
over zi,t. lt is the new updated bboxes and object descriptors. The RNN model has 3 layers and 512
hidden layer size.
16
Under review as a conference paper at ICLR 2021
Composed Actions
SWaP HUddle RU DL
^92∏~98.6~75.0 100.
Slide	Standard Actions		Rotate	Composed Actions Swap Huddle
	Contain	Pick Place		
~96T^	100.0	90.0	56.7	93.3	100.0~~
Table 6: HUman evalUation of timing in generated videos (see sec-
tion 4.3). The table rePorts accUracy of hUman annotator ansWer
With resPect to the trUe ansWer.
Table 5: HUman evalUation of
the semantic accuracy of the ac-
tions in the generated videos.
4.2	Layout Accuracy
AG2Vid ProdUces boUnding boxes of objects as a fUnction of time. Since oUr datasets contain
groUnd-trUth boxes, We can comPare oUr Predictions to these. We evalUate the intersection over
Union (IOU) over the Predicted and groUnd trUth boxes. We rePort the mean intersection over Union
(mIOU) Which is the mean over the entire set of boxes. Additionally, We measUre the recall over
the by considering an object to be a correct detected if the IOU betWeen the GT and Predicted box
is higher than 0.3 (R@0.3) or 0.5 (R@0.5). ResUlts are rePorted in Table 4. It can be seen that the
RNN UnderPerforms, sUPPorting the choice of GCN for layoUt generation in the AG2Vid model.
The RNN is likely to Under-Perform as it assUmes order over the AG list of edges, Which is not a
natUral Way to Process a graPh.
4.3	Human Evaluation of Action Timing in Generated Videos
As described in Section 5.1, We evalUated to Which extent the action graPhs (AGs) can control the
timing execUtion of actions on the CATER dataset. ThUs, We generated 90 Pairs of action graPhs
Where the only difference betWeen the tWo graPhs is the timing of one action. We then asked the an-
notators to select the video Where the action is execUted first. The fUll resUlts are dePicted in Table 6,
and visUal examPles are shoWn in FigUre 9. The resUlts for all actions bUt “Rotate” are consistent
With the exPected behavior, indicating that the model correctly execUtes actions in a timely fashion.
The “Rotate” action is esPecially challenging to generate since it occUrs Within the intermediate
layoUt. It is also easier to miss as it involves a relatively sUbtle change in the video.
Slide	Contain	Pick Place
Rotate
Swap
Huddle
FigUre 9: Timing exPeriment examPles in CATER. We shoW the clock edges can maniPUlate the timing of
a video by controlling When the action is Performed to achieve goal-oriented video synthesis. The objects
involved in “rotate” are highlighted. Click the image to play the video clip in a browser.
4.4	Human Evaluation of Semantic Quality in Generated Videos
To test the degree to Which the generated videos match their corresPonding actions, We generated
tWenty videos Per action for the Something-Something dataset and asked three different hUman
annotators to evalUate each video. Each annotator Was asked to Pick the action that best describes
17
Under review as a conference paper at ICLR 2021
the video out of the list of possible actions. We provide the results in Fig. 7. Each cell in the table
corresponds to the class recall of a specific action. To determine if a video correctly matches its
corresponding action, we used the majority voting over the answers of all annotators.
It turns out that humans do not perform perfectly in the above task. We quantified this effect in the
following experiments on the Something-Something dataset. We used the above annotation process
for ground-truth videos (see “Real” row in Table 7). Interestingly, it can be seen from the reported
accuracy in Table 7 that our generated action videos of “Move Down” and “Take” are more easily
recognizable by humans than the ground truth videos. For the CATER dataset, we did not perform
such human evaluation of predicted actions, since CATER videos contain multiple activities.
To evaluate the extent to which the AG2Vid model can generalize at test time to unseen actions, we
manually defined four compositions of learned actions. In Table 5, we show the semantic accuracy
of the human evaluation we did for the new unseen actions: “huddle”, “Swap”, “Right Up” and
“Down Left”.
5」	^ I	Standard Actions
Video Source ______________________________________________________________
	Right	Up	Down	Left	Put	Take	Uncover	Cover
Generated	100.	50.	100.	75.	95.	80.	25.	55.
Real	100.	100.	90.	100.	100.	65.	100.	85.
Table 7: The semantic quality evaluated by humans of the generated and real action videos. We asked raters
to select the action described in the video for each synthesized video with a given action. The table reports
the accuracy of the human annotators with respect to the true action underlying the video. Actions above
correspond to: ’Pushing [something] from left to right’, ’Moving [something] up’, ’Moving [something] down’,
’Pushing [something] from right to left’, ’Putting [something] on a surface’, ’Taking [one of many similar things
on the table]’, ’Uncovering [something]’, ’Covering [something] with [something]’ .
4.5	Comparing AG2Vid to Scene-Graph Based Generation
Scene graphs are an expressive formalism for describing image content. Both datasets we use have
frame-level scene graph annotation. Thus, we wanted to compare generation from these scene graphs
with generation from action graphs. Towards this end, we used a scene-graph-to-image model (John-
son et al., 2018) trained to generate the images in the videos from their corresponding scene graphs.
This model does not condition the action or initial frame and serves only for comparison in terms
of realistic generation. It can be seen in Figure 10 that the temporal coherency of AG2Vid is more
consistent and coherent than the sequence of scene graphs.
18
Under review as a conference paper at ICLR 2021
Figure 10: Comparing Sg2Im and Ag2Vid results in CATER. Each column is a different sample. Click the
image to play the video clip in a browser.
19