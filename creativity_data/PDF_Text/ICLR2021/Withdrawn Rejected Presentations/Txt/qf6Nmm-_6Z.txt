Under review as a conference paper at ICLR 2021
VECoDeR - Variational Embeddings for Com-
munity Detection and Node Representation
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we study how to simultaneously learn two highly correlated tasks
of graph analysis, i.e., community detection and node representation learning.
We propose an efficient generative model called VECoDeR for jointly learn-
ing Variational Embeddings for Community Detection and node Representation.
VECoDeR assumes that every node can be a member of one or more commu-
nities. The node embeddings are learned in such a way that connected nodes are
not only “closer” to each other but also share similar community assignments. A
joint learning framework leverages community-aware node embeddings for better
community detection. We demonstrate on several graph datasets that VECoDeR
effectively outperforms many competitive baselines on all three tasks i.e. node
classification, overlapping community detection and non-overlapping community
detection. We also show that VECoDeR is computationally efficient and has
quite robust performance with varying hyperparameters.
1	Introduction
Graphs are flexible data structures that model complex relationships among entities, i.e. data
points as nodes and the relations between nodes via edges. One important task in graph analysis
is community detection, where the objective is to cluster nodes into multiple groups (communi-
ties). Each community is a set of densely connected nodes. The communities can be overlapping
or non-overlapping, depending on whether they share some nodes or not. Several algorithmic (Ahn
et al., 2010; Derenyi et al., 2005) and probabilistic approaches (GoPalan & Blei, 2013; Leskovec &
Mcauley, 2012; Wang et al., 2017; Yang et al., 2013) to community detection have been proposed.
Another fundamental task in graph analysis is learning the node embeddings. These embeddings
can then be used for downstream tasks like graph visualization (Tang et al., 2016; Wang et al., 2016;
Gao et al., 2011; Wang et al., 2017) and classification (Cao et al., 2015; Tang et al., 2015).
In the literature, these tasks are usually treated separately. Although the standard graph embed-
ding methods capture the basic connectivity, the learning of the node embeddings is independent
of community detection. For instance, a simple approach can be to get the node embeddings via
DeepWalk (Perozzi et al., 2014) and get community assignments for each node by using k-means
or Gaussian mixture model. Looking from the other perspective, methods like Bigclam (Yang &
Leskovec (2013)), that focus on finding the community structure in the dataset, perform poorly for
node-representation tasks e.g. node classification. This motivates us to study the approaches that
jointly learn community-aware node embeddings.
Recently several approaches, like CNRL (Tu et al., 2018), ComE (Cavallari et al., 2017), vGraph
(Sun et al. (2019)) etc, have been proposed to learn the node embeddings and detect communities
simultaneously in a unified framework. Several studies have shown that community detection is
improved by incorporating the node representation in the learning process (Cao et al., 2015; Kozdoba
& Mannor, 2015). The intuition is that the global structure of graphs learned during community
detection can provide useful context for node embeddings and vice versa.
The joint learning methods (CNRL, ComE and vGraph) learn two embeddings for each node. One
node embedding is used for the node representation task. The second node embedding is the “con-
text” embedding of the node which aids in community detection. As CNRL and ComE are based on
Skip-Gram (Mikolov et al., 2013) and DeepWalk (Perozzi et al., 2014), they inherit “context” em-
bedding from it for learning the neighbourhood information of the node. vGraph also requires two
1
Under review as a conference paper at ICLR 2021
node embeddings for parameterizing two different distributions. In contrast, we propose learning a
single community-aware node representation which is directly used for both tasks. In this way, we
not only get rid of an extraneous node embedding but also reduce the computational cost.
In this paper, we propose an efficient generative model called VECoDeR for jointly learning
both community detection and node representation. The underlying intuition behind VECoDeR
is that every node can be a member of one or more communities. However, the node embeddings
should be learned in such a way that connected nodes are “closer” to each other than unconnected
nodes. Moreover, connected nodes should have similar community assignments. Formally, we
assume that for i-th node, the node embeddings zi are generated from a prior distribution p(z).
Given zi, the community assignments ci are sampled from p(ci|zi), which is parameterized by
node and community embeddings. In order to generate an edge (i, j), we sample another node
embedding zj from p(z) and respective community assignment cj from p(cj |zj). Afterwards, the
node embeddings and the respective community assignments of node pairs are fed to a decoder. The
decoder ensures that embeddings of both the nodes and the communities of connected nodes share
high similarity. This enables learning such node embeddings that are useful for both community
detection and node representation tasks.
We validate the effectiveness of our approach on several real-world graph datasets. In Sec. 4, we
show empirically that VECoDeR is able to outperform the baseline methods including the direct
competitors on all three tasks i.e. node classification, overlapping community detection and non-
overlapping community detection. Furthermore, we compare the computational cost of training
different algorithms. VECoDeR is up to 40x more time-efficient than its competitors. We also
conduct hyperparameter sensitivity analysis which demonstrates the robustness of our approach.
Our main contributions are summarized below:
•	We propose an efficient generative model called VECoDeR for joint community detec-
tion and node representation learning.
•	We adopt a novel approach and argue that a single node embedding is sufficient for learning
both the representation of the node itself and its context.
•	Training VECODER is extremely time-efficient in comparison to its competitors.
2	Related Work
Community Detection. Early community detection algorithms are inspired from clustering algo-
rithms (Xie et al., 2013). For instance, spectral clustering (Tang & Liu, 2011) is applied to the graph
Laplacian matrix for extracting the communities. Similarly, several matrix factorization based meth-
ods have been proposed to tackle the community detection problem. For example, Bigclam (Yang
& Leskovec (2013)) treats the problem as a non-negative matrix factorization (NMF) task. It aims
to recover the node-community affiliation matrix and learns the latent factors which represent com-
munity affiliations of nodes. Another method CESNA(Yang et al. (2013)) extends Bigclam by mod-
elling the interaction between the network structure and the node attributes. The performance of
matrix factorization methods is limited due to the capacity of the bi-linear models. Some generative
models, like vGraph (Sun et al., 2019), Circles (Leskovec & Mcauley (2012)) etc, have also been
proposed to detect communities in a graph.
Node Representation Learning. Many successful algorithms which learn node representation in
an unsupervised way are based on random walk objectives (Perozzi et al., 2014; Tang et al., 2015;
Grover & Leskovec, 2016; Hamilton et al., 2017). Some known issues with random-walk based
methods (e.g. DeepWalk, node2vec etc) are: (1) They sacrifice the structural information of the
graph by putting over-emphasis on the proximity information (Ribeiro et al., 2017) and (2) great
dependence of the performance on hyperparameters (walk-length, number of hops etc) (Perozzi
et al., 2014; Grover & Leskovec, 2016). Recently, Gilmer et al. (2017) recently showed that graph
convolutions encoder models greatly reduce the need for using the random-walk based training
objectives. This is because the graph convolutions enforce that the neighboring nodes have similar
representations. Some interesting GCN based approaches include graph autoencoders e.g. GAE and
VGAE(Kipf & Welling (2016b)) and DGI(Velickovic et al., 2019).
Joint community detection and node representation learning. In the literature, several attempts
have been made to tackle both these tasks in a single framework. Most of these methods propose
2
Under review as a conference paper at ICLR 2021
an alternate optimization process, i.e. learn node embeddings and improve community assignments
with them and vice versa (Cavallari et al., 2017; Tu et al., 2018). Some approaches, like CNRL (Tu
et al., 2018) and ComE (Cavallari et al., 2017), are inspired from random walk, thus inheriting the
shortcomings of random walk. Others, like GEMSEC (Rozemberczki et al. (2019), are limited to the
detection of non-overlapping communities. There also exist some generative models like Commu-
nityGAN (Jia et al. (2019)) and vGraph (Sun et al. (2019)) that jointly learn community assignments
and node embeddings. Some methods have high computational complexity, i.e. quadratic to the
number of nodes in a graph, e.g. M-NMF (Wang et al. (2017)) and DNR (Yang et al., 2016a). CNRL,
ComE and vGraph require learning two embeddings for each node for simultaneously tackling the
two tasks. Unlike them, VECoDeR learns a single community-aware node representation which is
directly used for both tasks.
It is pertinent to highlight that although both vGraph and VECoDeR adopt a variational approach
but the underlying models are quite different. vGraph assumes that each node can be represented as
a mixture of multiple communities and is described by a multinomial distribution over communities,
whereas VECoDeR models the node embedding by a single distribution. For a given node, vGraph,
first draws a community assignment and then a connected neighbor node is generated based on
the assignment. Whereas, VECoDeR draws the node embedding from prior distribution and then
community assignment is conditioned on a single node only. In simple terms, vGraph also needs
edge information in the generative process whereas VECoDeR does not require it. VECoDeR
relies on the decoder to ensure that embeddings of the connected nodes and their communities share
high similarity with each other.
3	Methodology
3.1	Problem Formulation
Suppose an undirected graph G = (V, E) with the adjacency matrix A ∈ RN×N and a matrix
X ∈ RN×F of F -dimensional node features, N being the number of nodes. Given K as the num-
ber of communities, we aim to jointly learn the node embeddings and the community embeddings
following a variational approach such that: (1) One or more communities can be assigned to every
node and (2) the node embeddings can be used for both community detection and node classification.
3.2	Variational Model
Generative Model: Let us denote the latent node embedding and community assignment for i-th
node by the random variables zi ∈ Rd and ci respectively. The generative model is given by:
p(A) =	Xp(Z,
c, A)dZ,
(1)
where C = [c1,c2, •…，CN] and the matrix Z = [zι, z2,…,ZN] stacks the node embeddings. The
joint distribution in (1) is mathematically expressed as
P(Z, c, A) = P(Z) Pθ(CIZ) Pθ(AIGZ),	⑵
where θ denotes the model parameters. Let us denote elements of Aby aij. Following existing
approaches (Kipf & Welling, 2016b; Khan et al., 2020), we consider zi to be i.i.d random vari-
ables. Furthermore, assuming ciIzi to be i.i.d random variables, the joint distributions in (2) can be
factorized as
N
P(Z) = YP(zi)
i=1
N
(3)	Pθ(CIZ) = YPθ(ciIzi)	(4)
i=1
Pθ(AIC, Z) = Pθ(aijIci, cj, zi, zj),
i,j
(5)
where Eq. (5) assumes that the edge decoderPθ(aijIci, cj, zi, zj) depends only on ci, cj, zi and zj.
3
Under review as a conference paper at ICLR 2021
Inference Model: We aim to learn the model parameters θ such that log(pθ (A)) is maximized. In
order to ensure computational tractability, we introduce the approximate posterior
qφ (Z, c∣i) = ∏qφ(zi,Ci∣i) = ∏qφ(zi |I )qφ(ci∣zi, I),	(6)
ii
where I = (A, X ) if node features are available, otherwise I = A. We maximize the correspond-
ing ELBO bound (for derivation, refer to the supplementary material), given by
N	NM
LELBO ≈ - XDκL(qφ(zi∣I) || P(Zi))-X M X DκL(qφ(ci∣zim'),I) ||pθ(ci∣z(m)))
i=1	i=1	m=1
+ X ： E(zi,Zj ,Ci,Cj )^qφ (zi,Zj ,Ci,Cj |I) ∣ log (pθ (aij |ci，cj，zi，Zj )) )，	⑺
(i,j)∈E
where DKL(.||.) represents the KL-divergence between two distributions. The distribution
qφ(zi, zj, ci, cj |I) in the third term of Eq. (7) is factorized into two conditionally independent dis-
tributions i.e.
qφ(zi, zj, ci, cj |I) = qφ(zi, ci |I )qφ(zj, cj |I).	(8)
3.3	Design Choices
In Eq. (3), p(zi) is chosen to be the standard gaussian distribution for all i. The corresponding
approximate posterior qφ(zi∖I) in Eq. (6), used as node embeddings encoder, is given by
qφ(zi∣I) = N (〃i(I), diag(σ2i(I))).	(9)
The parameters of qφ(zi |I) can be learnt by any encoder network e.g. graph convolutional network
(KiPf & Welling (2016a)), graph attention network (Velickovic et al. (2017)), GraphSAGE (Hamil-
ton et al. (2017)) or even two matrices to learn μi(I) and diag(σ2i(I)). Samples are then generated
using reparametrization trick (Doersch (2016)).
For parameterizing pθ(c/zi) in Eq. (4), we introduce community embeddings {gι,…，gκ}; gk ∈
Rd. The distribution pθ (ci∣zi) is then modelled as the softmax of dot products of zi with gk, i.e.
exp(< zi, gk >)
pθ (Ci - k|zi) = ~K	.	(10)
P exp(< zi, g` >)
'=1
The corresponding approximate posterior qφ(ci = k|zi,I) in Eq. (6) is affected by the node embed-
ding zi as well as the neighborhood. To design this, our intuition is to consider the similarity ofgk
with the embedding zi as well as with the embeddings of the neighbors of the i-th node. The overall
similarity with neighbors is mathematically formulated as the average of the dot products of their
embeddings. Afterwards a hyperparameter α is introduced to control the bias between the effect of
zi and the set Ni of the neighbors of the i-th node. Finally, a softmax is applied as follows
qφ (ci = k|zi , G)
exp (α< zi, gk > +(1 - α) UNJ P < Nj, gk >
|Ni| j∈Ni
K
EexP α < zi,g` > +(1 - α)-∏Nj E < zj,g` >
'=ι '	1N i1 j∈Ni
(11)
Hence, Eq. (11) ensures that graph structure information is employed to learn community assign-
ments instead of relying on an extraneous node embedding as done in (Sun et al., 2019; Cavallari
et al., 2017). Finally, the choice of edge decoder in Eq. (5) is motivated by the intuition that the
nodes connected by edges have a high probability of belonging to the same community and vice
versa. Therefore we model the edge decoder as:
Pθ (a"i = ',cj=m, zi, zj) = σ(< zi, gm >) + σ(< zj, g' >).	(12)
4
Under review as a conference paper at ICLR 2021
For better reconstructing the edges, Eq. (12) makes use of the community embeddings, node em-
beddings and community assignment information simultaneously. This helps in learning better node
representations by leveraging the global information about the graph structure via community detec-
tion. On the other hand, this also forces the community assignment information to exploit the local
graph structure via node embeddings and edge information.
3.4	Practical Aspects
The third term in Eq. (7) is estimated in practice using the samples generated by the approximate pos-
terior. This term is equivalent to the negative of binary cross-entropy (BCE) loss between observed
edges and reconstructed edges. Since community assignment follows a categorical distribution, we
use Gumbel-softmax (Jang et al. (2016)) for backpropagation of the gradients. As for the second
term of Eq. (7), it is also enough to set M = 1, i.e. use only one sample per input node.
For inference, non-overlapping community assignment can be obtained for i-th node as
Ci = arg max qφ(ci = k|zi,I).
k∈{1,∙∙∙ ,K}
(13)
To get overlapping community assignments for i-th node, we can threshold its weighted probability
vector at , a hyperparameter, as follows
Ci = nk
ma；，：==：% ≥ e}“
(14)
3.5	Complexity
Computation of dot products for all combinations of node and community embeddings takes
O(NKd) time. Solving Eq. (11) further requires calculation of mean of dot products over
the neighborhood for every node, which takes O(|E|K) computations overall as we traverse
every edge for every community. Finally, we need softmax over all communities for every
node in Eq. (10) and Eq. (11) which takes O(NK) time. Eq. (12) takes O(|E|) time for
all edges as we have already calculated the dot products. As a result, the overall complex-
ity becomes O(|E|K + NKd). This complexity is quite low compared to other algorithms de-
signed to achieve similar goals (Cavallari et al., 2017; Wang et al., 2017; Yang et al., 2016a).
4 Experiments
4.1	Datasets
We have selected 18 different datasets rang-
ing from 270 to 126,842 edges. For non-
overlapping community detection and node
classification, we use 5 the citation datasets
(BCjchevski & Gunnemann (2017); Yang et al.
(2016b)). The remaining datasets (Leskovec
& Mcauley (2012); Yang & LeskCvec (2015)),
used fCr Cverlapping cCmmunity detectiCn, are
taken frCm SNAP repCsitCry (LeskCvec &
Krevl (2014)). FCllCwing (Sun et al., 2019),
we take 5 biggest grCund truth cCmmunities fCr
yCutube, amazCn and dblp. MCreCver, we alsC
analyse the case Cf large number Cf cCmmuni-
ties. FCr this purpCse, we prepare twC subsets
Dataset	|V|	|E|	K	|F|	Overlap
CiteSeer	3327	9104	-6-	3703	N-
CiteSeer-full	4230	10674	6	602	N
Cora	2708	10556	7	1433	N
Cora-ML	2995	16316	7	2879	N
Cora-full	19793	126842	70	8710	N
fb0	333	2519	24	N/A	Y
fb107	1034	26749	9	N/A	Y
fb1684	786	14024	17	N/A	Y
fb1912	747	30025	46	N/A	Y
fb3437	534	4813	32	N/A	Y
fb348	224	3192	14	N/A	Y
fb414	150	1693	7	N/A	Y
fb698	61	270	13	N/A	Y
youtube	5346	24121	5	N/A	Y
amazon	794	2109	5	N/A	Y
amazon500	1113	3496	500	N/A	Y
amazon1000	1540	4488	1000	N/A	Y
dblp	24493	89063	5	N/A	Y
Table 1: Every dataset has |V|nodes, |E|edges, K
communities and |F| features. |F| = N/A means
that either the features were missing or not used.
Cf amazCn dataset by randCmly selecting 500 and 1000 cCmmunities frCm 2000 smallest cCmmuni-
ties in the amazCn dataset.
5
Under review as a conference paper at ICLR 2021
4.2	Baselines
For overlapping community detection, we compare with the following competitive baselines:
MNMF(Wang et al., 2017) learns community membership distribution by using joint non-negative
matrix factorization with modularity based regularization. BIGCLAM(Yang & Leskovec (2013))
also formulates community detection as a non-negative matrix factorization (NMF) task. It simul-
taneously optimizes the model likelihood of observed links and learns the latent factors which rep-
resent community affiliations of nodes. CESNA (Yang et al. (2013)) extends BIGCLAM by sta-
tistically modelling the interaction between the network structure and the node attributes. Cir-
cles (Leskovec & Mcauley (2012)) introduces a generative model for community detection in
ego-networks by learning node similarity metrics for every community. SVI (Gopalan & Blei
(2013)) formulates membership of nodes in multiple communities by a Bayesian model of networks.
vGraph (Sun et al. (2019)) simultaneously learns node embeddings and community assignments by
modelling the nodes as being generated from a mixture of communities. vGraph+, a variant further
incorporates regularization to weigh local connectivity. ComE (Cavallari et al. (2017)) jointly learns
community and node embeddings by using gaussian mixture model formulation. CNRL(Tu et al.,
2018) enhances the random walk sequences (generated by DeepWalk, node2vec etc) to jointly learn
community and node embeddings. CommunityGAN (ComGAN)is a generative adversarial model
for learning node embeddings such that the entries of the embedding vector of each node refer to the
membership strength of the node to different communities. Lastly, we compare the results with the
communities obtained by applying k-means to the learned embeddings of DGI (Velickovic et al.,
2019).
For non-overlapping community detection and node classification, in addition to MNMF, DGI,
CNRL, CommunityGAN, vGraph and ComE, we compare VECoDeR with the following base-
lines: DeepWalk (Perozzi et al. (2014)) makes use of SkipGram (Mikolov et al. (2013)) and trun-
cated random walks on network to learn node embeddings. LINE (Tang et al. (2015)) learns node
embeddings while attempting to preserve first and second order proximities of nodes. Node2Vec
(Grover & Leskovec (2016)) learns the embeddings using biased random walk while aiming to
preserve network neighborhoods of nodes. Graph Autoencoder (GAE)Kipf & Welling (2016b)
extends the idea of autoencoders to graph datasets. We also include its variational counterpart i.e.
VGAE. GEMSEC is a sequence sampling-based learning model which aims to jointly learn the
node embeddings and clustering assignments.
4.3	Settings
For overlapping community detection, we learn mean and log-variance matrices of 16-
dimensional node embeddings. We set α = 0.9 and = 0.3 in all our experiments. Following
Kipf & Welling (2016b), we first pre-train a variational graph autoencoder. We perform gradient
descent with Adam optimizer (Kingma & Ba (2014)) and learning rate = 0.01. Community assign-
ments are obtained using Eq. (14). For the baselines, we employ the results reported by Sun et al.
(2019). For evaluating the performance, we use F1-score and Jaccard similarity.
For non-overlapping community detection, since the default implementations of most the base-
lines use 128 dimensional embeddings, for we use d = 128 for fair comparison. Eq. (13) is used
for community assignments. For vGraph, we use the code provided by the authors. We employ
normalized mutual information (NMI) and adjusted random index (ARI) as evaluation metrics.
For node classification, we follow the training split used in various previous works (Yang et al.,
2016b; Kipf & Welling, 2016a; Velickovic et al., 2019), i.e. 20 nodes per class for training. We
train logistic regression using LIBLINEAR (Fan et al. (2008)) solver as our classifier and report the
evaluation results on rest of the nodes. For the algorithms that do not use node features, we train the
classifier by appending the raw node features with the learnt embeddings. For evaluation, we use
F1-macro and F1-micro scores.
All the reported results are the average over five runs. Further implementation details can be found
in the code: https://anonymous.4open.science/r/1d95bf8f-8ce3-4870-a454-07db463b419f.
4.4	Discussion of results
In the following, we discuss the results to gain some important insights into the problem.
6
Under review as a conference paper at ICLR 2021
F1 Score (%)
Dataset	MNMF	Bigclam	CESNA	Circles	SVI	VGraph	VGraph+	ComE	CNRL	ComGan	DGI	VECoDeR
fb0	-14.4-	-29.5	-28.1	28.6-	28.1	-24.4-	26.1	31.1	11.5	35.0	27.35	347
fb107	12.6	39.3	37.3	24.7	26.9	28.2	31.8	39.7	20.2	47.5	35.78	59.7
fb1684	12.2	50.4	51.2	28.9	35.9	42.3	43.8	52.9	38.5	47.6	42.85	56.4
fb1912	14.9	34.9	34.7	26.2	28.0	25.8	37.5	28.7	8.0	35.6	32.6	45.8
fb3437	13.7	19.9	20.1	10.1	15.4	20.9	22.7	21.3	3.9	39.3	19.66	50.2
fb348	20.0	49.6	53.8	51.8	46.1	55.4	53.1	46.2	34.1	55.8	54.68	58.2
fb414	22.1	58.9	60.1	48.4	38.9	64.7	66.9	55.3	25.3	43.9	56.93	69.6
fb698	26.6	54.2	58.7	35.2	40.3	54.0	59.5	45.8	16.4	58.2	52.19	64.0
Youtube	59.9	43.7	38.4	36.0	41.4	50.7	52.2	65.5	51.4	43.6	47.8	67.3
Amazon	38.2	46.4	46.8	53.3	47.3	53.3	53.2	50.1	53.5	51.4	44.7	58.1
Amazon500	30.1	52.2	57.3	46.2	41.9	61.2	60.4	59.8	38.4	59.3	33.8	67.6
Amazon1000	19.3	28.6	30.8	25.9	21.6	54.3	47.3	50.3	27.1	52.7	37.7	60.5
Dblp	21.8	23.6	35.9	36.2	33.7	39.3	39.9	47.1	46.8	34.9	44	53.9
Table 2: F1 scores for overlapping communities. Best and second best values are bold and blue respectively.
jaccard (%)												
Dataset	MNMF	Bigclam	CESNA	Circles	SVI	vGraph	vGraph+	ComE	CNRL	ComGan	DGI	VECoDeR
^fb0	8.0	-18.5	-17.3-	18.6-	17.6	-14.6-	15.9	19.5	-68-	24.1	16.8	247
fb107	6.9	27.5	27.0	15.5	17.2	18.3	21.7	28.7	11.9	38.5	25.29	46.8
fb1684	6.6	38.0	38.7	18.7	24.7	29.2	32.7	40.3	25.8	37.9	38.85	42.5
fb1912	8.4	24.1	23.9	16.7	20.1	18.6	28.0	18.5	4.6	13.5	22.48	37.3
fb3437	7.7	11.5	11.7	5.5	9.0	12.0	13.3	12.5	2.0	33.4	11.55	36.2
fb348	11.3	35.9	40.0	39.3	33.6	41.0	40.5	34.4	21.7	23.2	41.77	43.5
fb414	12.8	47.1	47.3	34.2	29.3	51.8	55.9	42.2	15.4	53.6	46.36	58.4
fb698	16.0	41.9	45.9	22.6	30.0	43.6	47.7	33.8	9.6	46.9	42.1	50.4
Youtube	46.7	29.3	24.2	22.1	28.7	34.3	34.8	52.5	35.5	44.0	32.72	53.3
Amazon	25.2	35.1	35.0	36.7	36.4	36.9	36.9	34.6	38.7	38.0	29.09	41.9
AmaZon500	20.8	51.2	53.8	47.2	45.0	59.1	59.6	58.4	41.1	57.3	23.28	64.9
AmaZon1000	20.3	26.8	28.9	24.9	23.6	54.3	49.7	52.0	26.9	54.1	23.25	57.1
Dblp		20.9	13.8	22.3	23.3	20.9	25.0	25.1	27.9	32.8	25.0	29.15	37.3
Table 3: Jaccard scores for overlapping communities. Best and second best values are bold and blue respec-
tively.
NMI(%)						ARI(%)				
Algorithm	CiteSeer	CiteSeer-full	Cora	Cora-ML	Cora-full	CiteSeer	CiteSeer-full	Cora	Cora-ML	Cora-full
MNMF	14.1	94	19.7	378	420	2.6	0.4	2.9	24.1	61
DeePWalk	8.8	15.4	39.7	43.2	48.5	9.5	16.4	31.2	33.9	22.5
LINE	8.7	13.0	32.8	42.3	40.3	3.3	3.7	14.9	32.7	11.7
Node2Vec	14.9	22.3	39.7	39.6	48.1	8.1	10.5	25.8	27.9	18.8
GAE	17.4	55.1	39.7	48.3	48.3	14.1	50.6	29.3	41.8	18.3
VGAE	16.3	48.4	40.8	48.3	47.0	10.1	40.6	34.7	42.5	17.9
DGI	37.8	56.7	50.1	46.2	39.9	38.1	50.8	44.7	42.1	12.1
GEMSEC	11.8	11.1	27.4	18.1	10.0	0.6	1.0	4.8	1.0	0.2
CNRL	13.6	23.3	39.4	42.9	47.7	12.8	20.2	31.9	32.5	22.9
ComGAN	3.2	16.2	5.7	11.5	15.0	1.2	4.9	3.2	6.7	0.6
vGraph	9.0	7.6	26.4	29.8	41.7	5.1	4.2	12.7	21.6	14.9
ComE	18.8	32.8	39.6	47.6	51.2	13.8	20.9	34.2	37.2	19.7
VECODER	38.5	59.0	52.7	56.3	55.2	35.2	60.3	45.1	49.8	28.8
Table 4: Non-overlapping community detection results. Best and second best values are bold and blue respec-
tively.
	F1-macro(%)					F1-micro(%)				
Algorithm	CiteSeer	CiteSeer-full	Cora	Cora-ML	Cora-full	CiteSeer	CiteSeer-full	Cora	Cora-ML	Cora-full
MNMF	57.4	68.6	60.9	64.2	30.4	60.8	68.1	62.7	64.2	32.9
DeepWalk	49.0	56.6	69.7	75.8	41.7	52.0	57.3	70.2	75.6	48.3
LINE	55.0	60.2	68.0	75.3	39.4	57.7	60.0	68.3	74.6	42.1
Node2Vec	55.2	61.0	71.3	78.4	42.3	57.8	61.5	71.4	78.6	48.1
GAE	57.9	79.9	71.2	76.5	36.6	61.6	79.6	73.5	77.6	41.8
VGAE	59.1	74.4	70.4	75.2	32.4	62.2	74.4	72.0	76.4	37.7
DGI	62.6	82.1	71.1	72.6	16.5	67.9	81.8	73.3	75.4	21.1
GEMSEC	37.5	53.3	60.3	70.6	35.8	39.4	53.5	59.4	72.5	38.9
CNRL	50.0	58.0	70.4	77.8	41.3	53.2	57.9	70.4	78.4	45.9
ComGAN	55.9	65.7	56.6	62.5	27.7	59.1	64.9	58.5	62.8	29.4
vGraph	30.8	28.5	44.7	59.8	33.4	32.1	28.5	44.6	62.3	37.6
ComE	59.6	69.9	71.6	78.5	42.2	63.1	70.2	74.2	79.5	47.8
VECODER	64.8	76.8	73.1	80.2	43.1	68.2	77.0	75.6	82.0	49.6
Table 5: Node classification results. Best and second best values are bold and blue respectively.
7
Under review as a conference paper at ICLR 2021
Tables 2 and 3 summarize the results of the performance comparison for the overlapping community
detection task.
First, we note that our proposed method VECoDeR outperforms the competitive methods on all
datasets in terms of Jaccard Similarity. VECoDeR also outperforms its competitors on 12 out of 13
datasets in terms of F1-score. It is the second best method on the 13th dataset (fb0). These results
demonstrate the capability of VECoDeR to learn multiple community assignments quite well and
hence reinforces our intuition behind the design of Eq. (11).
Second, we observe that there is no consistent performing algorithm among the competitive meth-
ods. That is, excluding VECoDeR , the best performance is achieved by vGraph/vGraph+ on 5,
ComGAN on 4 and ComE on 3 out of 13 datasets in terms of F1-score. A a similar trend can be seen
in Jaccard Similarity. Third, it is noted that all the methods which achieve second best performance
are solving the task of community detection and node representation learning jointly. This supports
our claim that treating the two tasks jointly results in better performance.
Fourth, we observe that vGraph+ results are generally better than vGraph. This is because vGraph+
incorporates a regularization term in the loss function which is based on Jaccard coefficients of
connected nodes as edge weights. However, it should be noted that this prepossessing step is com-
putationally expensive for densely connected graphs.
Tab. 4 shows the results on non-overlapping community detection. First, we observe that MNMF,
DeepWalk, LINE and Node2Vec provide a good baseline for the task. However, these methods are
not able to achieve comparable performance on any dataset relative to the frameworks that treat the
two tasks jointly. Second, VECoDeR consistently outperforms all the competitors in NMI and ARI
metrics, except for CiteSeer where it achieves second best ARI. Third, we observe that GCN based
models i.e. GAE, VGAE and DGI show competitive performance. That is, they achieve second best
performance in all the datasets except CiteSeer. In particular, DGI achieves second best NMI results
in 3 out of 5 datasets and 2 out of 5 datsets in terms of ARI. Nonetheless, DGI results are not very
competitive in Tab. 2 and Tab. 3, showing that while DGI can be a good choice for learning node
embeddings for attributed graphs with non-overlapping communities, it is not the best option for
non-attributed graphs or overlapping communities.
The results for node classification are presented in Tab. 5. VECoDeR achieves best F1-micro and
F1-macro scores on 4 out of 5 datasets. We also observe that GCN based models i.e. GAE, VGAE
and DGI show competitive performance, following the trend in results of Tab. 4. Furthermore, we
note that the node classification results of CommunityGan (ComGAN) are quite poor. We think a
potential reason behind it is that the node embeddings are constrained to have same dimensions as
the number of communities. Hence, different components of the learned node embeddings simply
represent the membership strengths of nodes for different communities. The linear classifiers may
find it difficult to separate such vectors.
4.5	Hyperparameter sensitivity
We study the dependence of VECODER on and α by evaluating on four datasets of different
sizes: fb698(N = 61), fb1912(N = 747), amazon1000(N=1540) and youtube(N = 5346). We
sweep for E = {0.1,0.2, ∙∙∙ , 0.9}. For demonstrating effect of a, we fix E = 0.3 and sweep for
α = {0.1,0.2, ∙∙∙ , 0.9}. The average results of five runs for E and α are given in Fig. IaandFig. 1b
respectively. Overall VECODER is quite robust to the change in the values of E and α. In case
of E, we see a general trend of decrease in performance when the threshold E is set quite high e.g.
E > 0.7. This is because the datasets contain overlapping communities and a very high E will cause
the algorithm to give only the most probable community assignment instead of potentially providing
multiple communities per node. However, for a large part of sweep space, the results are almost
consistent. When E is fixed and α is changed, the results are mostly consistent except when α is set
to a low value. Eq. (11) shows that in such a case the node itself is almost neglected and VECoDeR
tends to assign communities based upon neighborhood only, which may cause a decrease in the
performance. This effect is most visible in amazon1000 dataset because it has only 1.54 points on
average per community i.e. there is a good chance for neighbours of a point of being in different
communities. Therefore, only depending upon the neighbors will most likely result in poor results.
8
Under review as a conference paper at ICLR 2021
E	α
(a) Effect of e. Overall a slight decrease in scores can (b) Effect of a. The scores generally tend to decrease
be observed after = 0.7 mark.	for small values of α.
Figure 1: Effect of hyperparameters on the performance. F1 and Jaccard scores are in solid and
dashed lines respectively.
■ Node2Vec ■ LINE ■ DeePWalk ■ VGraPh ■ ComE ■ CNRL ■ VECoDER
log(seconds)
Figure 2: Comparison of running times of different algorithms.
4.6 Training Time
Now we compare the training times of different algorithms in Fig. 2. As some of the baselines are
more resource intensive than others, we select aws instance type g4dn.4xlarge for fair com-
parison of training times. For vGraph, we train for 1000 iterations and for VECoDeR for 1500
iterations. For all other algorithms we use the default parameters as used in section 4.3. We ob-
serve that the methods that simply output the node embeddings take relatively less time compared to
the algorithms that jointly learn node representations and community assignments e.g VECoDeR ,
vGraph and CNRL. Among these algorithms VECoDeR is the most time efficient. It consistently
trains in less time compared to its direct competitors. For instance, it is about 12 times faster than
ComE for CiteSeer-full and about 40 times faster compared to vGraph for Cora-full dataset. This
provides evidence for lower computational complexity of VECoDeR in Section 3.5.
5 Conclusion
We propose a scalable generative method VECoDeR to simultaneously perform community detec-
tion and node representation learning. Our novel approach learns a single community-aware node
embedding for both the representation of the node and its context. VECoDeR is scalable due to
its low complexity, i.e. O(|E |K + NKd). The experiments on several graph datasets show that
VECoDeR consistently outperforms all the competitive baselines on node classification, overlap-
ping community detection and non-overlapping community detection tasks. Moreover, training the
VECoDeR is highly time-efficient than its competitors.
9
Under review as a conference paper at ICLR 2021
References
Yong-Yeol Ahn, James P Bagrow, and Sune Lehmann. Link communities reveal multiscale com-
Plexity in networks. nature, 466(7307):761-764, 2010.
Aleksandar BCjchevski and Stephan Gunnemann. Deep gaussian embedding of graphs: UnsUPer-
vised inductive learning via ranking. arXiv preprint arXiv:1707.03815, 2017.
Shaosheng Cao, Wei Lu, and Qiongkai Xu. Grarep: Learning graph representations with global
structural information. In Proceedings of the 24th ACM International on Conference on Informa-
tion and Knowledge Management, CIKM ’15, pp. 891900, New York, NY, USA, 2015. Associ-
ation for Computing Machinery. ISBN 9781450337946. doi: 10.1145/2806416.2806512. URL
https://doi.org/10.1145/2806416.2806512.
Sandro Cavallari, Vincent W Zheng, Hongyun Cai, Kevin Chen-Chuan Chang, and Erik Cambria.
Learning community embedding with community detection and node embedding on graphs. In
Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pp.
377-386, 2017.
Imre Derenyi, Gergely Palla, and Tamas Vicsek. Clique percolation in random networks. Physical
review letters, 94(16):160202, 2005.
Carl Doersch. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A
library for large linear classification. Journal of machine learning research, 9(Aug):1871-1874,
2008.
Sheng Gao, Ludovic Denoyer, and Patrick Gallinari. Temporal link prediction by integrating content
and structure information. In Proceedings of the 20th ACM international conference on Informa-
tion and knowledge management, pp. 1169-1174, 2011.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Prem K Gopalan and David M Blei. Efficient discovery of overlapping communities in massive
networks. Proceedings of the National Academy of Sciences, 110(36):14534-14539, 2013.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864, 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024-1034, 2017.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. arXiv
preprint arXiv:1611.01144, 2016.
Yuting Jia, Qinqin Zhang, Weinan Zhang, and Xinbing Wang. Communitygan: Community detec-
tion with generative adversarial nets. In The World Wide Web Conference, pp. 784-794, 2019.
Rayyan Ahmad Khan, Muhammad Umer Anwaar, and Martin Kleinsteuber. Epitomic variational
graph autoencoder, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016a.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint
arXiv:1611.07308, 2016b.
10
Under review as a conference paper at ICLR 2021
Mark Kozdoba and Shie Mannor. Community detection via measure space embedding. In Proceed-
ings of the 28th International Conference on Neural Information Processing Systems - Volume 2,
NIPS’15, pp. 28902898, Cambridge, MA, USA, 2015. MIT Press.
Jure Leskovec and Andrej Krevl. SNAP Datasets: Stanford large network dataset collection. http:
//snap.stanford.edu/data, June 2014.
Jure Leskovec and Julian J Mcauley. Learning to discover social circles in ego networks. In Advances
in neural information processing systems, pp. 539-547, 2012.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-
tations in vector space. arXiv preprint arXiv:1301.3781, 2013.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710, 2014.
Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node
representations from structural identity. In Proceedings of the 23rd ACM SIGKDD international
conference on knowledge discovery and data mining, pp. 385-394, 2017.
Benedek Rozemberczki, Ryan Davies, Rik Sarkar, and Charles Sutton. Gemsec: Graph embedding
with self clustering. In Proceedings of the 2019 IEEE/ACM international conference on advances
in social networks analysis and mining, pp. 65-72, 2019.
Fan-Yun Sun, Meng Qu, Jordan Hoffmann, Chin-Wei Huang, and Jian Tang. vgraph: A generative
model for joint community detection and node representation learning. In Advances in Neural
Information Processing Systems, pp. 514-524, 2019.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th international conference on world
wide web, pp. 1067-1077, 2015.
Jiliang Tang, Charu Aggarwal, and Huan Liu. Node classification in signed social networks. In
Proceedings of the 2016 SIAM international conference on data mining, pp. 54-62. SIAM, 2016.
Lei Tang and Huan Liu. Leveraging social media networks for classification. Data Mining and
Knowledge Discovery, 23(3):447-478, 2011.
Cunchao Tu, Xiangkai Zeng, Hao Wang, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun, Bo Zhang,
and Leyu Lin. A unified framework for community detection and network representation learning.
IEEE Transactions on Knowledge and Data Engineering, 31(6):1051-1065, 2018.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. arXiv preprint arXiv:1710.10903, 2017.
Petar Velickovic, William Fedus, William L Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep graph infomax. 2019.
Daixin Wang, Peng CUi, and WenwU ZhU. StrUctUral deep network embedding. In Proceedings of
the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.
1225-1234, 2016.
Xiao Wang, Peng CUi, Jing Wang, Jian Pei, WenwU ZhU, and Shiqiang Yang. CommUnity preserving
network embedding. In AAAI, volUme 17, pp. 203-209, 2017.
JierUi Xie, Stephen Kelley, and Boleslaw K Szymanski. Overlapping commUnity detection in net-
works: The state-of-the-art and comparative stUdy. Acm computing surveys (csur), 45(4):1-35,
2013.
Jaewon Yang and JUre Leskovec. Overlapping commUnity detection at scale: a nonnegative matrix
factorization approach. In Proceedings of the sixth ACM international conference on Web search
and data mining, pp. 587-596, 2013.
11
Under review as a conference paper at ICLR 2021
Jaewon Yang and Jure Leskovec. Defining and evaluating network communities based on ground-
truth. Knowledge and Information Systems, 42(1):181-213, 2015.
Jaewon Yang, Julian McAuley, and Jure Leskovec. Community detection in networks with node
attributes. In 2013 IEEE 13th International Conference on Data Mining, pp. 1151-1156. IEEE,
2013.
Liang Yang, Xiaochun Cao, Dongxiao He, Chuan Wang, Xiao Wang, and Weixiong Zhang. Mod-
ularity based community detection with deep learning. In IJCAI, volume 16, pp. 2252-2258,
2016a.
Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
graph embeddings. In International conference on machine learning, pp. 40-48. PMLR, 2016b.
12