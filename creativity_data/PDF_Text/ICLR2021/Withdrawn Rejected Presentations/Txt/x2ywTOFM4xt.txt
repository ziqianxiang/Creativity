Under review as a conference paper at ICLR 2021
Variational saliency maps
for explaining model’ s behavior
Anonymous authors
Paper under double-blind review
Ab stract
Saliency maps have been widely used to explain the behavior of an image classi-
fier. We introduce a new interpretability method which considers a saliency map as
a random variable and aims to calculate the posterior distribution over the saliency
map. The likelihood function is designed to measure the distance between the
classifier’s predictive probability of an image and that of locally perturbed image.
For the prior distribution, we make attributions of adjacent pixels have a positive
correlation. We use a variational approximation, and show that the approximate
posterior is effective in explaining the classifier’s behavior. It also has benefits of
providing uncertainty over the explanation, giving auxiliary information to experts
on how much the explanation is trustworthy.
1	Introduction
Since the advent of deep learning brought significant improvement in general machine learning
tasks (Krizhevsky et al., 2012), explaining deep networks have become an important issue (Ribeiro
et al. (2016)). Problems inherent in training a deep neural network, such as fairness (Arrieta et al.,
2020) or the model classifying based on unintended features (Ribeiro et al., 2016), can be mitigated
when the model is finely explained. Therefore, the models that have gained users’ trust through
explanation are preferred in practical applications.
Saliency maps, also called attribution maps or relevance maps, have been widely used for inter-
pretability methods in classification tasks, typically in an image domain (Simonyan et al., 2013).
A saliency map represents the importance of each feature of given data that influences the model’s
decision. There have been several approaches for obtaining the saliency map, which are back-
propagation based methods (Ancona et al., 2017; Bach et al., 2015; Lundberg & Lee, 2017; Mon-
tavon et al., 2017; Selvaraju et al., 2017; Shrikumar et al., 2017; Simonyan et al., 2013; Smilkov
et al., 2017; Srinivas & Fleuret, 2019; Sundararajan et al., 2017) and perturbation based methods
(Chang et al., 2019; Chen et al., 2018; Dabkowski & Gal, 2017; Fong et al., 2019; Fong & Vedaldi,
2017; Schulz et al., 2020; Zeiler & Fergus, 2014; Zintgraf et al., 2017). Regardless of the ap-
proaches, the common implicit assumption shared by most of the previous interpretability methods
is that a saliency map exists in a deterministic manner when a model and an input data are given:
one attribution map is provided to explain the model’s decision for each data point.
Instead of the implicit assumption, we propose a stochastic approach called Variational Saliency
maps (VarSal) where it is assumed that the interpretation has inherent randomness. The intuition
stems from the stochastic effect that makes interpretation methods more explainable. For instance,
FIDO (Chang et al., 2019) expands the search space of the mask by drawing it from Bernoulli
distribution. This approach prevents the mask to be searched in the local space when it is directly
optimized (Fong & Vedaldi, 2017). The example informs us that the stochastic property draws better
interpretation.
We define the posterior distribution as the probability of the saliency map when the training data
and the classifier are given. To make the posterior behave as the distribution of explanation, it is
essential to carefully design the likelihood function and the prior distribution. We follow the idea
of perturbation based methods to form the likelihood where the input that only contains features
which correspond to high attribution in a saliency map is likely to describe the classifier’s behavior.
For modeling the prior, we propose a new covariance matrix of Gaussian distribution that implies
the property of having a positive correlation among attributions of adjacent pixels. As this property
1
Under review as a conference paper at ICLR 2021
mimics total variation (TV) regularization, we name the prior as soft-TV Gaussian prior. After mod-
eling the likelihood and the prior, the Variational Bayesian method (Hoffman et al., 2013; Kingma
& Welling, 2013) is used since the posterior is intractable.
After the optimization, unlike most of perturbation based methods, VarSal produces a real-time
saliency map since only a single forward pass is required for generating it. Also, the VarSal method
provides high quality in the visual inspection where sophisticated borderlines exist with object-
oriented attention. We compare VarSal with baseline methods on the perturbation benchmark test to
show the effectiveness of our approach. At the end, we examine the benefit of employing a posterior
distribution, which is uncertainty over the explanation.
2	Related work
In this section, we take a look at perturbation based interpretability methods. Fong & Vedaldi (2017)
optimize the cost function with respect to the mask which indicates the most important features in an
image for the classifier’s prediction. This approach is further developed by Fong et al. (2019) where
they introduce a new method for making a perturbed image which helps to reduce hyper-parameters
and produces better qualitative results. Both methods should optimize the mask every time they
receive input, which is computationally expensive. Dabkowski & Gal (2017) relax the problem of
time complexity by using a trained network of which the output is a saliency mask. However, all
three methods have a limitation for producing importance ranking among features of a given image
since their objective is to produce a binary mask.
PDA (Zintgraf et al., 2017) produces a saliency map from a different perspective. It computes the
importance of each pixel by regarding it as an unobserved pixel and marginalizes it out to get the
predictive probability output of the classifier. The same idea is used in FIDO (Chang et al., 2019) to
generate a perturbed image that is regarded as a sample from training data distribution. It optimizes
the parameters of a Bernoulli dropout distribution for making a saliency mask. It helps exploring
the search space of binary mask rather than being limited to local search since the mask is sampled
from the distribution for each training iteration. Our method is similar to FIDO in that VarSal also
explores the search space by sampling the saliency map from the encoder in the training phase.
There is an information theoretic approach for explaining the classifier’s prediction. Schulz et al.
(2020) adopt an information bottleneck for restricting the flow of information in an intermediate
layer by adding noise. They find the importance of each feature by calculating the information flow.
Chen et al. (2018) also adopt mutual information concept and optimize its variational bound for
training a network that maps an input image to a saliency map. VarSal is similar in that we also train
the encoder network by optimizing the evidence lower bound (ELBO). However, our method differs
in that we regard the saliency map as a random variable and aim to calculate the posterior over the
saliency map.
3	Variational saliency maps
In this section, We introduce details of the VarSal method which	A
provides stochastic saliency maps. Let Us define a pre-trained	0--^r÷f S ɔ	M
classifier that we aim to interpret as M : Rc×h×w → Y where /^sʌ	/
X ∈ Rc×h×w is an input with c, h, and w to be channel, height, and 『	∖	/
width of the input image, respectively, and Y = {1,2,..., K} is ∖	∖ /
a set of classes. The classifier M provides categorical probability ''Λ4Λ	r^∖
PM(∙) = y ∈ 4K-1 where 4K-1 is a K -1 simplex. Since the	~ʌɪj
purpose of a saliency map S ∈ Rh×w is to describe the behavior V	J
of the classifier’s prediction, our goal is to calculate the posterior Fi	1 GhifI	dfq
Figure 1: Graphical model.
distribution of the saliency map, p(s∖x,y) (solid lines in Figure 1).
By Bayes’ rule, the posterior is stated as:
p(s∖χ,y) = p(y∖χ,s) p(s∖χ) / Z,
(1)
where Z is the marginal likelihood. To calculate the posterior, we should model two terms: the
likelihoodp(y)∖x,s) and the prior p(s∖x).
2
Under review as a conference paper at ICLR 2021
3.1	Modeling likelihood
The likelihood should be well-designed to make the posterior over the saliency map explain the
behavior of the classifier. We focus on the property that the importance of each feature in an image
is determined by observing the response of the classifier’s output when the feature is perturbed
(Zeiler & Fergus, 2014; Fong & Vedaldi, 2017; Zintgraf et al., 2017). More specifically, important
features are enough to correctly classify the input as target class with high confidence. This concept
is first introduced by Dabkowski & Gal (2017), and called smallest sufficient region (SSR). The
difference between SSR and our approach is that we do not consider the smallest sufficient region,
but rather rank the features (therefore, s ∈ Rh×w, not s ∈ {0, 1}h×w). Moreover, we consider
not the target class but the categorical probability to interpret the model itself. The likelihood is
designed such that the aforementioned properties satisfy
-logp(y∣x,s,k) = Dkl[PM(x) k PM(X Θ T(k)(s))] + const,	⑵
p(y∣x,s) = Ep(k)[p(y∣x,s,k)],	(3)
where DKL is a Kullback-Leibler (KL) divergence, τ(k) is a top-k operation, and is a perturb
operation that makes local perturbation of input x. The top-k operation applied to the saliency map,
τ(k) (s) ∈ {0, 1}h×w, acts as a mask where [τ(k) (s)]i,j = 1 when si,j corresponds to the biggest
k attributions in s. This way, the top-k operation makes the conditional likelihood in equation 2
to consider only the selected features in the input. By varying k, the amount of selected features
is controlled, and we set p(k) as uniform distribution. To make the local perturbation of input x
using perturb operation , we follow the method proposed by Fong & Vedaldi (2017), x m =
x ◦ m + X ◦ (1 一 m), where X is a baseline input, and ◦ is a pointwise multiplication. We bring
three baseline settings in our experiment: blurred baseline1, noise baseline2, and mean baseline3.
The equation 2 states that the categorical probability y is more likely when the distance between the
classifier’s predictive probability of input X and that of perturbed input is close for given k. This
makes sense since better saliency map that explains the classifier’s behavior would approximate the
model’s prediction closer with the selected features of top-k attributions. Also, we do not consider
the ground-truth class or the top-1 predicted class, but rather whole classes with predictive probabil-
ity in order to examine the classifier’s behavior itself. To consider various values of k, we also take
the expectation in equation 3.
3.2	Soft-tv gaussian prior
The easiest way to model the prior distribution p(s|X) is to consider it as independent of X, p(s|X) =
p(s), and design it as standard Gaussian distribution N (vec(s); 0, I) where vec(s) ∈ Rhw is a
vectorized version of s. However, the standard Gaussian prior does not consider the belief over
the saliency map that the attribution of neighbor pixels might have correlation (Fong et al., 2019).
Therefore, we propose a new prior distribution that expresses the belief. While Dabkowski & Gal
(2017); Fong & Vedaldi (2017) proposed total variation (TV) regularization to prevent the saliency
mask from being occurred adversarial artifacts, we mimic the TV method in building the prior
distribution to provide the positive correlation between neighbor attributions. To be more specific,
we design the prior distribution as zero mean Gaussian distribution, N (vec(s); 0, Σ), and infuse the
TV knowledge to the covarianace matrix by setting Σi,j > 0 when pixel i and pixel j are identical
or adjacent. We have
Σi,j
if i=j
if i-j ∈ {-w, -1, 1, w} and j ∈ Adji
if i - j ∈ {-w - 1, -w + 1, w - 1, w + 1} and j ∈ Adji
otherwise,
(4)
1 We define “blurred baseline” as an input image blurred with Gaussian kernel.
2The “noise baseline” is defined as Gaussian noise.
3We term “mean baseline” when the baseline is set to be the per channel mean of an original image and added
by Gaussian noise.
3
Under review as a conference paper at ICLR 2021
where α > 0 and Adji is the adjacent index set of pixel i (Figure 9 in Appendix B). This way, we
grant the TV knowledge to the Gaussian prior, and call it soft-TV Gaussian prior.
3.3	Variational inference on saliency maps
Modeling the likelihood as equation 3 and the prior as N (vec(s); 0, Σ) makes the posterior of
equation 1 intractable. Therefore, We approximate it with the distribution qθ (s|x) parameterized by
θ (dotted lines in Figure 1) where the objective is to minimize the KL divergence between qθ(s|x)
and p(s∣x,y):
argmin Dkl[qθ(s|x) k p(s∣x,y)]
θ
=argmin Eq[ — logp(y∣x,s)] + Dkl[qθ(s|x) k p(s∣x)] .	(5)
θ S-----------{z-----} S------------{z---------}
(*)	(**)
We apply a mean-field approximation with univariate Gaussian for each factorized term of approxi-
mate posterior, qθ (s|x) = N (vec(s); μθ (x), diag(νθ (x))), where μθ (∙) ∈ Rhw is the mean of the
distribution and diag(νθ(∙)) ∈ Rhw×hw is the diagonal covariance matrix with the main diagonal
to be vθ(∙) ∈ Rhw. μθ and νg are collectively called encoder network parameterized by θ. The
encoder network is optimized with the training dataset used for training the classifier M, and the
reparameterization trick (Kingma & Welling, 2013) is applied during optimization. The schematic
description is shown in Appendix A.
There are two problems in optimizing the equation 5: non-differentiable top-k operation in equation
(*) and computationally expensiveness in equation (**). In case of (*), the top-k operation T(k) is
non-differentiable where the gradient cannot flow backward. To overcome this issue, we approxi-
mate it with a differentiable SOFT operator proposed by Xie et al. (2020). This allows flowing the
gradient from the classifier M to the encoder parameters θ. Note that the classifier M is a pre-trained
classifier that we aim to interpret, and thus should be fixed.
Since the size of covariance Σ is large, which is hw×hw, it is computationally expensive to calculate
(**) when it is naively used (in case of Imagenet dataset (Russakovsky et al., 2015), the size of Σ is
2244 !). We solve the problem by decomposing Σ with a Kronecker product:
∑ = Kh 0 κw ,	(6)
where 0 is the Kronecker product, and Kh ∈ Rh×h and Kw ∈ Rw×w are tridiagonal matrices with 1
for main diagonal and α for first diagonal below and above the main diagonal (Figure 9 in Appendix
B). After all, (**) can be analytically derived as:
Dkl [ qθ (s|x) k p(s∣x)] = diag (KwI)T ∙rsh(νj) ∙ diag (K-I) — sum (log V)
+ sum(rsh(μθ) Θ (Kw1 ∙rsh(μθ) ∙ (k-1)T)) + const,	(7)
where sum(∙) is the summation of elements. For a vector b ∈ Rhw, we denote rsh(b) ∈ Rw×h
as reshaping the vector b to the matrix where [rsh(b)]i,j = bi+wj. Also, for a square matrix B,
diag(B) is the vector where the ith entry is Bi,i. The derivation of equation 7 is provided in
Appendix D. The equation 7 shows that the computation is easily done in general deep learning
frameworks such as Pytorch (Paszke et al., 2017).
4	Experiment
4.1	Implementation detail
There are two terms in the objective function equation 5: the reconstruction term (*) and the reg-
ularization term (**). The components in the regularization term have high dimension, which is
hw. This is usually too large that the regularization term becomes dominant in the loss function.
4
Under review as a conference paper at ICLR 2021
Image Input Gradient PDA	Real-Time	EP	FIDO VarSal (Ours)
Figure 2: Qualitative results. Compared with previous methods, VarSal visually captures the most
sophisticated borderlines and object-oriented saliency map.
We avoid this phenomenon by introducing a hyper-parameter β to the regularization term (Higgins
et al., 2017). We set the default value of β to be 1/(10hw).
We test our method on the ImageNet dataset (Russakovsky et al., 2015). We use a pre-trained
VGG16 (Simonyan & Zisserman, 2014) for the classifier M, and 16 convolutional layers for the
encoder μθ and vθ . As for the variable k used in the top-k operator, We sample k from the uniform
distribution, k 〜 U(hw/10,9hw∕10), for each training iteration. The baseline input X is also ran-
domly selected among three baselines (blurred, noise, and mean baseline) for each training iteration.
As for α in the soft-TV Gaussian prior, We set α = 0.4. Finally, if not mentioned, the mean of ap-
proximate posterior is used When performing qualitative and quantitative experiments. More details
are given in Appendix C.
4.2	Qualitative results
We perform visual inspection by comparing VarSal With previous interpretability methods. For
fair comparison, With the data that are correctly classified, previous interpretability methods are
performed based on the ground-truth target, While VarSal explains the predictive probability. In
Figure 2, the first heatmap (Simonyan et al., 2013) are generated by the gradients of input. They
visually highlights the object, but there exists sparsity. PDA (Zintgraf et al., 2017) usually provides
unnecessary highlights since it only considers spatially local parts in the optimization process. Real-
time saliency (DabkoWski & Gal, 2017) and EP (Fong et al., 2019) shoW a boolean mask for the
saliency map With smoothed borderlines. This is because it optimizes the mask With the size smaller
than the input, folloWed by performing upsampling for the final saliency mask. FIDO (Chang et al.,
5
Under review as a conference paper at ICLR 2021
2019) provides the shape of the object to some extent, but it still has sparsity. Compared to the
previous approaches, VarSal does not contain upsampling process, but rather directly provides the
saliency map with the same size of input image. As the last heatmap shows, the VarSal highlights
the object with more sophisticated borderlines. More results are provided in Appendix E.
To investigate the importance of modeling the prior distribution,
we perform visual inspection between the VarSal method trained
with standard Gaussian prior and that of soft-TV Gaussian prior.
As Figure 3 shows, VarSal trained with standard Gaussian prior
provides saliency map with high frequency noise inside the ob-
ject boundary. This is because the standard Gaussian prior does
not constrain the adjacent attributions to have correlation. On the
other side, VarSal optimized using the soft-TV Gaussian prior
shows smaller variation of attribution between adjacent pixels.
Moreover, the noise on the background has been reduced when
the soft-TV prior is used for modeling the prior distribution.
4.3	Sanity check
JOR joFd
Jnd目 ρ⅛∏8JS Λ1-⅛OS
Figure 3: Prior selection.
The prerequisite for becoming an interpretability method
is to pass the sanity check (Adebayo et al., 2018). This is
to identify whether the interpretability method provides
a saliency map dependent of a classifier or a data, and
is tested by randomizing the classifier’s parameters. The
difference between saliency maps obtained by the orig-
inal classifier M and the parameter-randomized classi-
fier is measured by structural similarity index (SSIM) and
Spearman rank correlation. Itis known that Guided Back-
original 3 layers 6 layers 9 layers 12 layers all layers
(0 layer)
VGG16 initialized layers (top to bottom layers)
Figure 4: Sanity check.
Prop (Springenberg et al., 2014) and Guided GradCAM (Selvaraju et al., 2017) do not pass the sanity
check since the SSIM and the Spearman metric is close to 1. Instead, Figure 4 shows that our method
has lower values for each metric and different randomization of the classifier, indicating that VarSal
has passed the sanity check.
4.4	Quantitative results
3.5
3.0
<O 2.5
β
U) 2.0
恭
亳L5
4 10
0.5
0.0
Low-k pixels perturbed
Inference time complexity
3 2 10 1
Oooo-
Illl10
PUoogS
Figure 5: (a) Quantitative results for pixel perturbation benchmark The results verify the usefulness
of our method. (b) Time complexity of each interpretability methods. SG-sq. refers to SmoothGrad-
squared.
Determining the state-of-the-art interpretability method is challenging since there is no evaluation
benchmark that exactly reflects the method,s performance (Hooker et al., 2019). Commonly used
benchmarks can split the superiority and inferiority of each method to some extent, but cannot ex-
actly rank them based on the quantitative evaluation indicator. We use pixel perturbation benchmark
to verify the usefulness of our method.
6
Under review as a conference paper at ICLR 2021
For the pixel perturbation metric, image pixels are erased that correspond to the largest k% saliency
values (Ancona et al., 2017; Samek et al., 2016) or the smallest k% saliency values (Srinivas &
Fleuret, 2019), and observe the response to the change of classifier’s output. In our experiment, we
erase pixels with the latter procedure as the former is more prone to create unnecessary artifacts that
lead to misunderstanding of the reason for the score drop (Srinivas & Fleuret, 2019). We change
pixel values of the input image to zero that correspond to the least k% values in the saliency map,
and observe the KL divergence between the classifier’s predictive probability of original input and
that of perturbed input. The interpretability method is thought to be better when the distance is
smaller.
It takes expensive computational time for PDA and FIDO method, about 25 minute and 1 minute per
one image (Figure 5(b)). Therefore, for evaluating PDA and FIDO, we randomly sample 100 data
in the validation dataset to perform the top-k perturbation benchmark, with repeating the process 5
times. Other methods use entire validation dataset for the evaluation. For fair comparison, saliency
map is generated from the predicted class for each interprebility methods except VarSal. Also,
perturbing randomly drawn k pixels is suggested as a control experiment. We omit drawing error
range for this control experiment since the error (standard deviation) is less than 1e-2. The results
are shown in Figure 5(a). It is observed that Input-Gradient (Simonyan et al., 2013), Integrated-
Gradient (Sundararajan et al., 2017), and PDA gets close to the control setting as k gets larger. For
others such as SmoothGrad-squared (Smilkov et al., 2017; Hooker et al., 2019), FIDO, and VarSal,
they have similar values throughout the change of k values. Moreover, VarSal performs better when
using the soft-TV Gaussian prior than using the standard Gaussian prior.
An interesting point is that the saliency map obtained by sam-
pling from the approximate posterior instead of the mean values
results in low-quality (red dotted line in Figure 5(a)). We spec-
ulate this is because the sampling method produces artifacts in
a perturbed image that cause score drop (Kurakin et al., 2016).
As shown in Figure 6, even though both the sampled saliency
map and the mean saliency map captures objects with high val-
ues, the sampled one has high-frequency noise. When the image
pixels corresponding to low k% of the noisy saliency map are
perturbed, the perturbed image has the artifact of sharp color
contrast between adjacent pixels that might cause score degrada-
tion.
4.5	Uncertainty over explanation
sdtq Saqduies
Figure 6: Sampled saliency map.
-ndɑɪ IIB 省 PjBP且 S
(a) Ongm
-ndɑɪ IIB 省 PjBP且 S
(b) Bnghtness
UOA»IA。P
-ndɑɪ IIB 省 PjBP且 S
(c) Motion_blur
Figure 7: Uncertainty over the explanation. The posterior distribution of saliency map gives two
summaries, which are the explanation (mean) and the uncertainty over the explanation (standard
deviation). They are compared qualitatively on the original dataset and two different shifted dataset.
One advantage of having posterior distribution over the saliency map is that it gives us the uncer-
tainty of the explanation. This is done by summarizing the posterior with its covariance matrix.
Since the VarSal approximates the posterior with factorized Gaussian, we can observe the variance
7
Under review as a conference paper at ICLR 2021
of each attribution where the examples are shown in Figure 7(a). While the explanation (second
row) has higher attribution at the object, the uncertainty over the explanation (third row) presents at
the borderline of the object.
We also qualitatively compare the posterior results on the shifted samples. Recently Hendrycks &
Dietterich (2018) have established Imagenet-C dataset that is created by visually corrupting the data
in the Imagenet dataset. The first row in Figure 7(b) and (c) respectively shows the example of
images corrupted by ”Brightness” and ”Motion blur” with level 5 of severity. While the explanation
of corrupted images is similar to that of original images in that they all capture the object to some
extent, the uncertainty of the explanation shows different appearance where the heatmaps of standard
deviation are noisy in the corrupted images.
5	Conclusion and future work
In this paper, we presented a new perspective on a saliency map where it is assumed to be a random
variable. After designing the likelihood function and the prior distribution that makes the posterior
distribution over the saliency map explain the behavior of the classifier’s prediction, the approxi-
mate posterior is optimized by maximizing ELBO. The experimental results were performed with
the mean of the approximate posterior, and showed that our method has visually sharp borderlines
with object-oriented saliency map. For quantitative results, the pixel perturbation benchmark is used
to prove the effectiveness of our method. We verified that using the proposed soft-TV Gaussian
distribution rather than the standard Gaussian distribution for modeling the prior has better perfor-
mance in both qualitative and quantitative comparison. It was also shown that the proposed VarSal
method has a strong advantage over other methods in terms of inference computation complexity.
Finally, we showed that our method provides not only the explanation but also the uncertainty over
the explanation.
There remain future works for producing better quality of the posterior distribution over a saliency
map. In modeling the likelihood, the problem of data distribution shift could be mitigated by gener-
ating a perturbed image that is expected to be sampled from training data distribution (Chang et al.,
2019). It would also be interesting to consider axiomatic prior knowledge (Sundararajan et al., 2017;
Srinivas & Fleuret, 2019) in modeling the prior distribution and the likelihood. Finally, the uncer-
tainty consideration over explaination is believed to be critical since it can tells us how much the
explanation given by an interpretability method can be trusted, which needs to be studied further.
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. In Advances in Neural Information Processing Systems, pp.
9505-9515, 2018.
Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding of
gradient-based attribution methods for deep neural networks. arXiv preprint arXiv:1711.06104,
2017.
Alejandro Barredo Arrieta, Natalia Diaz-Rodrlguez, Javier Del Ser, Adrien Bennetot, Siham Tabik,
Alberto Barbado, Salvador Garcia, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, et al.
Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges to-
ward responsible ai. Information Fusion, 58:82-115, 2020.
Sebastian Bach, Alexander Binder, GregOire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PloS one, 10(7), 2015.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image clas-
sifiers by counterfactual generation. In International Conference on Learning Representations,
2019. URL https://openreview.net/forum?id=B1MXz20cYQ.
8
Under review as a conference paper at ICLR 2021
Jianbo Chen, Le Song, Martin Wainwright, and Michael Jordan. Learning to explain: An
information-theoretic perspective on model interpretation. In International Conference on Ma-
chine Learning, pp. 883-892, 2018.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Advances in
Neural Information Processing Systems, pp. 6967-6976, 2017.
Ruth Fong, Mandela Patrick, and Andrea Vedaldi. Understanding deep networks via extremal per-
turbations and smooth masks. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 2950-2958, 2019.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proceedings of the IEEE International Conference on Computer Vision, pp. 3429-3437,
2017.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In International Conference on Learning Representations, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. Iclr, 2(5):6, 2017.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-
ence. The Journal of Machine Learning Research, 14(1):1303-1347, 2013.
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretabil-
ity methods in deep neural networks. In Advances in Neural Information Processing Systems, pp.
9734-9745, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Advances
in neural information processing systems, pp. 4765-4774, 2017.
Gregoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert
Muller. Explaining nonlinear classification decisions with deep taylor decomposition. Pattern
Recognition, 65:211-222, 2017.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. 2017.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. ” why should i trust you?” explaining the
predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference
on knowledge discovery and data mining, pp. 1135-1144, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International journal of computer vision, 115(3):211-252, 2015.
Wojciech Samek, Alexander Binder, GregOire Montavon, Sebastian Lapuschkin, and Klaus-Robert
Muller. Evaluating the visualization of what a deep neural network has learned. IEEE transactions
on neural networks and learning systems, 28(11):2660-2673, 2016.
9
Under review as a conference paper at ICLR 2021
Karl Schulz, Leon Sixt, Federico Tombari, and Tim Landgraf. Restricting the flow: Information
bottlenecks for attribution. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=S1xWh1rYwB.
Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based local-
ization. In Proceedings of the IEEE international conference on computer vision, pp. 618-626,
2017.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In Proceedings of the 34th International Conference on Ma-
chine Learning-Volume 70, pp. 3145-3153. JMLR. org, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks: Vi-
sualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.
Daniel Smilkov, Nikhil ThoraL Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Suraj Srinivas and Francois Fleuret. Full-gradient representation for neural network visualization.
In Advances in Neural Information Processing Systems, pp. 4126-4135, 2019.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319-
3328. JMLR. org, 2017.
Yujia Xie, Hanjun Dai, Minshuo Chen, Bo Dai, Tuo Zhao, Hongyuan Zha, Wei Wei, and Tomas
Pfister. Differentiable top-k operator with optimal transport. arXiv preprint arXiv:2002.06504,
2020.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
European conference on computer vision, pp. 818-833. Springer, 2014.
Luisa M Zintgraf, Taco S Cohen, Tameem Adel, and Max Welling. Visualizing deep neural network
decisions: Prediction difference analysis. arXiv preprint arXiv:1702.04595, 2017.
Appendix
A S chematic description
Figure 8: Schematic description.
10
Under review as a conference paper at ICLR 2021
An input image is fed into the encoder network that gives the mean and the variance of Guassian
distribution (which is the approximate posterior). A saliency map is sampled from it, followed
by passing a differentiable top-k operator to provide a binary mask. With this mask, the input
image is perturbed. The perturbed image is passes a classifier M that gives categorical probability
l^perturb. The loss function is composed of two terms: the reconstruction term between yperturb
and the categorical probability obtained from the original image if origin, and the regularization term
between the approximate posterior q(s∣x) = N(s; μ(x), diag(ν(x))) and the prior distribution
N(s; 0, Σ). As the classifier M is the model that we aim to interpret, it is fixed so as the parameter
not to be updated while training the encoder network.
B S oft-TV Gaussian prior
(a)
h
Figure 9: Soft-TV Gaussian prior.
■ = 1	□= α □= α2	□= 0
(b)
We consider the prior knowledge that adjacent pixels in a saliency map have positive correlation
(Figure 9(a)). After modeling the prior as Gaussian distribution N(s; 0, Σ), the covariance matrix
Σ is designed to infuse this prior knowledge into the prior distribution (Figure 9(b)). The covari-
ance matrix is then decomposed by Kronecker product to better calculate the KL divergence of the
regularization loss.
C Implementation details
Encoder architecture We use 17 convolution layers for the encoder network. To make the spatial
size of the encoder’s input and output to be same, we do not use a pooling layer. Every convolution
layer is comprised ofa convolution with kernel size 3 × 3, stride 1, and padding 1, followed by batch
normalization and a rectified linear unit. The number of output channels for each convolution is as
follows: [64, 64, 64, 64, 32, 32, 32, 32, 32, 16, 16, 16, 16, 16, 16, 2]. The encoder network provides
μ ∈ Rh×w and η ∈ Rh×w for each channel of the output where μ is the mean of the Gaussian
distribution and η = log ν with ν ∈ Rh×w the variance of the Gaussian distribution.
hyper-parameters We use Adam (Kingma & Ba, 2014) optimizer with learning rate to be 0.0001,
weight decay to be 0.0005, and betas to be (0.9, 0.99). We use batch size of 128 while training the
encoder network. We run 10 epochs for the Imagenet dataset, and save the network that has the
lowest loss.
D	Proof of regularization loss equation
Let us first define the notation. 0 is the Kronecker product, and Θ is the element-wise multiplication.
sum(∙) is the summation of elements. For a vector b ∈ Rhw, we denote rsh(b) ∈ Rw ×h as reshaping
11
Under review as a conference paper at ICLR 2021
the vector b to the matrix where the (i,j)-th entry is b[i + wj], and diag(b) as the diagonal matrix
where the diagonal is b. Also, for a square matrix B, diag(B) is the vector where the i-th entry is
B[i, i]. For a matrix B, vec(B) denotes the vectorization by stacking the columns of the matrix B
to a single column vector.
Recall that the approximate posterior is q(s∣x) = N(μ, ∑o) where ∑o = diag(ν) with μ, V ∈
Rhw, and the prior distribution is p(s∣x) = N(0, ∑ι) where ∑ι = Kh 0 Kw with Kh ∈ Rh×h and
κw ∈ Rw×w . KL divergence between two Gaussian distribution is:
Dkl[q(s∣x) k p(s∣x)] = Dkl[N(μ, ∑o) k N(0,∑ι)]
=2 ^tr (ς-1ςo) + μTE-1μ - hw + log ∣∑∣) .	(8)
We compute each term in RHS of equation 8 to make it computationally efficient.
tr(∑-1∑o) = tr ((κh 0 κw厂1∙diag(ν))
=tr((κ-1 0 κ-1) ∙ diag (ν))
= diag Kh-1 0 Kw-1	diag (ν)
=diag (κw1)τ ∙ rsh (ν) ∙ diag (κ-1) .	(9)
μTς-1" = μτ ∙ (K-10 KwI) ∙ μ
=μτ ∙ (k-1 0 KwI) ∙ VeC (rsh (μ))
=μτ ∙ vec (K-1 ∙ rsh (μ) ∙ KwIT)
=SUm ksh(μ) Θ ^Kw1 ∙ rsh (μ) ∙ (K-I)T)) .	(10)
l∑ιl	.	...
log-^r- = log ∣Kh 0 Kw | - log ∣diag(ν)∣
|20|
hw
= log |Kh|w|Kw|h - logYνi
i=1
hw
=W ∙ log ∣Kh∣ + h ∙ log ∣Kw | — log ɪɪ Vi
i=1
= -sUm (log Vi ) + const .	(11)
Therefore, the equation 8 is derived as:
Dkl[q(s∣x) k p(s∣x)] = diag (Kw1 )T ∙ rsh(ν) ∙ diag (K-I)
+ sum (rsh (μ) Θ (Kw1 ∙ rsh (μ) ∙ (K-I)T))	(12)
- sUm (log Vi ) + const .
12
Under review as a conference paper at ICLR 2021
PWnq
E
Qualitative results

uμpndeo
8fqoπ4
q*gαoH pτmσqjiQM q*μj le^lləəuɪ
q*5PI0g
Mosl
a≡3su!BPtaBTl砂 I。∏B0∏9V
13