Under review as a conference paper at ICLR 2021
FERMI: Fair Empirical Risk Minimization Via
Exponential RENYI Mutual Information
Anonymous authors
Paper under double-blind review
Ab stract
Several notions of fairness, such as demographic parity and equal opportunity, are
defined based on statistical independence between a predicted target and a sen-
sitive attribute. In machine learning applications, however, the data distribution
is unknown to the learner and statistical independence is not verifiable. Hence,
the learner could only resort to empirical evaluation of the degree of fairness vi-
olation. Many fairness violation notions are defined as a divergence/distance be-
tween the joint distribution of the target and sensitive attributes and the Kronecker
product of their marginals, such as Renyi correlation, mutual information, L∞
distance, to name a few. In this paper, we propose another notion of fairness vi-
olation, called Exponential Renyi Mutual Information (ERMI) between sensitive
attributes and the predicted target. We show that ERMI is a strong fairness vio-
lation notion in the sense that it provides an upper bound guarantee on all of the
aforementioned notions of fairness violation. We also propose the Fair Empirical
Risk Minimization via ERMI regularization framework, called FERMI. Whereas
existing in-processing fairness algorithms are deterministic, we provide a stochas-
tic optimization method for solving FERMI that is amenable to large-scale prob-
lems. In addition, we provide a batch (deterministic) method to solve FERMI.
Both of our proposed algorithms come with theoretical convergence guarantees.
our experiments show that FERMI achieves the most favorable tradeoffs between
fairness violation and accuracy on test data across different problem setups, even
when fairness violation is measured in notions other than ERMI.
1	Introduction
Ensuring that decisions made using machine learning algorithms are fair to different subgroups is
of utmost importance. Without any mitigation strategy, machine learning algorithms may result in
discrimination against certain subgroups based on sensitive attributes, such as gender or race, even if
such discrimination is absent in the training data (Datta et al., 2015; Sweeney, 2013; Bolukbasi et al.,
2016; Angwin et al., 2016; du Pin Calmon et al., 2017b; Feldman et al., 2015; Hardt et al., 2016;
Fish et al., 2016; Woodworth et al., 2017; Zafar et al., 2017; Bechavod & Ligett, 2017; Kearns et al.,
2018). To remedy such discrimination issues, several notions for imposing algorithmic fairness have
been proposed in the literature.
A learning machine satisfies the demographic parity notion, if the predicted target is independent
of the sensitive attributes (Dwork et al., 2012). Promoting demographic parity can lead to poor
performance, especially if the true outcome is not independent of the sensitive attributes. To remedy
this, (Hardt et al., 2016) proposed equalized odds to ensure that the predicted target is conditionally
independent of the sensitive attributes given the true label. A further relaxed version of this notion
is equal opportunity which is satisfied if predicted target is conditionally independent of sensitive
attributes given that the true label is in an advantaged class (Hardt et al., 2016). Note that the
inherent assumption in such conditional notions is that the true labels are unbiased. These notions
suffer from a potential amplification of the inherent biases that may exist in the targets/labels in the
training data (e.g., data collection bias). Tackling such bias is beyond the scope of this work.
In practice, the learner cannot empirically verify independence of random variables, and hence can-
not verify demographic parity, equalized odds, or equal opportunity. This has led the machine
learning community to define several notions of fairness violation that quantify the degree of inde-
pendence between random variables, e.g., demographic parity/equalized odds, L∞ distance (Dwork
et al., 2012; Hardt et al., 2016), mutual information (Kamishima et al., 2011; Rezaei et al., 2020;
1
Under review as a conference paper at ICLR 2021
Steinberg et al., 2020; Zhang et al., 2018; Cho et al., 2020), Pearson correlation (Zafar et al., 2017),
false positive/negative rates (Bechavod & Ligett, 2017), Hilbert Schmidt independence criterion
(HSIC)(Perez-SUay et al., 2017), and Renyi correlation (Baharlouei et al., 2020; Grari et al., 2020;
2019), to name a few. In this paper, we define yet another notion of fairness violation, called ex-
PonentiaI Renyi mutual information (ERMI). We show that ERMI is easy to compute empirically
and prove that ERMI provides an upper bound on the existing notions of fairness violation such as
demographic parity, equalized odds, and equal opportunity.
Given a notion of fairness violation, it is still not straightforward to train an algorithm that satisfies
a fairness violation constraint (Cotter et al., 2019). The fairness-promoting machine learning algo-
rithms can be categorized in three main classes: pre-processing, post-processing, and in-processing
methods. Pre-processing algorithms (Feldman et al., 2015; Zemel et al., 2013; du Pin Calmon et al.,
2017b) transform the biased data features to a new space in which the labels and sensitive attributes
are statistically independent. This transform is oblivious to the training procedure. Post-processing
approaches (Hardt et al., 2016; Pleiss et al., 2017) mitigate the discrimination of the classifier by
altering the the final decision, e.g., by changing the thresholds on soft labels, or reassigning the
labels to impose notions of fairness. In-processing approaches focus on the training procedure and
impose the notions of fairness as constraints or regularization terms in the optimization procedure.
Several regularization-based methods are proposed in the literature to impose measures of fairness to
decision-trees (Kamiran et al., 2010; Raff et al., 2018; Aghaei et al., 2019), support vector machines
(Donini et al., 2018), neural networks (Grari et al., 2020), or (logistic) regression models (Zafar
et al., 2017; Berk et al., 2017; Taskesen et al., 2020; Chzhen & Schreuder, 2020; Baharlouei et al.,
2020; Jiang et al., 2020; Grari et al., 2019). To the best of our knowledge, existing in-processing
methods are all deterministic, making them impractical for large-scale problems. Furthermore, most
in-processing methods (with the exception of (Baharlouei et al., 2020)) are designed for problems
in which the sensitive attribute and/or the target is binary. In this paper, we introduce a new fair
empirical risk minimization framework via ERMI regularization, and call it FERMI. We provide
novel batch and stochastic gradient-based methods with guarantees for solving FERMI and demon-
strate their effectiveness on multiple numerical experiments, which include a large-scale problem
and problem with both non-binary sensitive attributes and targets. We show that FERMI can be used
to achieve the most favorable tradeoffs between performance and fairness, even if fairness violation
is measured in notions other than ERMI.
2	(Z, Z)-FAIRNESS: A GENERAL NOTION OF FAIRNESS
We consider a learner who trains a model to predict a
target, Yb , e.g.,
whether or not to extend a loan,
supported on Y which can be discrete or continuous. The prediction is made using a set of features,
X, e.g., financial history features, length of credit, and amount of debt. We also assume that there
is a set of discrete sensitive attributes, S, e.g., race and sex, supported on S, associated with each
sample. Further, let A ⊆ Y denote an advantaged outcome class, e.g., the outcome where a loan is
extended. Next, we will present the main fairness notion considered in this paper, which generalizes
several existing ones.
Definition 1 ((Z, Z)-fairness). Given a random variable Z, let Z be a subset of values that Z can
take. We say that a learning machine satisfies (Z, Z)-fairness if for every z ∈ Z, Yb is conditionally
independent of S given Z = z. More precisely,
pYb,S|Z(yb,s|z) =pYb|Z(yb|z)pS|Z(s|z) ∀yb∈ Y,s ∈ S,z ∈ Z.	(1)
Notice that (Z, Z)-fairness recovers several important existing notions of fairness as special cases:
1.
2.
(Z, Z)-fairness recovers demographic parity (Dwork et al., 2012) if Z = 0 and Z = {0}.
In this case, conditioning on Z has no effect, and hence (0, {0}) fairness is equivalent to the
independence between Yb and S, i.e
., demographic parity (see Definition 7, Appendix A).
3.
(Z, Z)-fairness recovers equalized odds (Hardt et al., 2016) if Z = Y and Z = Y. In
the case, Z ∈ Z is trivially satisfied and could be dropped. Hence, conditioning on Z is
equivalent to conditioning on Y, which recovers the equalized odds notion of fairness, i.e.,
conditional independence of Y and S given Y (see Definition 8, Appendix A).
(Z, Z)-fairness recovers equal opportunity (Hardt et al., 2016) if Z = Y and Z = A. This
is also similar to the previous case with Y replaced with A (see Definition 9, Appendix A).
2
Under review as a conference paper at ICLR 2021
Demographic parity amounts to requiring equality of outcomes across sensitive groups. However,
it can result in poor performance of a learned model, particularly if the true outcome Y is not inde-
pendent of S. Equalized odds and equal opportunity remedy this issue by relaxing the independece
constraint (Hardt et al., 2016). In the binary classification setting with binary sensitive attributes
(e.g., male or female), equalized odds ensures equality of false negatives and false positives of a
classifier across sensitive groups. Equal opportunity is a further relaxation of the equalized odds cri-
terion, which, in the binary setting, requires just equality of false negatives across sensitive groups.
For example, this could be used to enforce that a face recognition software does not falsely classify
people of one race as criminals more often than people of other races.
Note that verifying (Z, Z)-fairness requires having access to the joint distribution of random vari-
ables (Z, Y , S). This joint distribution is unavailable to the learner in the context of machine learn-
ing, and hence the learner would resort to empirical estimation of the amount of violation of inde-
Pendence. In the next section, We propose exponential Renyi mutual information as a notion of the
violation of (Z, Z)-fairness, and show that it is a stronger notion compared to several existing fair-
ness violation notions, including demographic parity L∞ distance (Kearns et al., 2018), equalized
odds L∞ distance (Hardt et al., 2016), and equal opportunity L∞ distance (Hardt et al., 2016).
3	Exponential Renyi mutual information
In this section, We define ERMI and shoW that several existing fairness violation notions (Which
are mostly instances of f -divergences or distance metrics betWeen the joint probability distribution
and the Kronecker product of the marginals) are also upper bounded by ERMi, implying that ERMi
is a stronger notion of fairness violation. in particular, this means that if ERMi is small, then We
automatically obtain guarantees that all other notions of fairness violation must be small as Well.
We present all definitions and results for general (Z, Z) fairness notion, Which requires careful
extension of several existing notions to the conditional case. These definitions and results Will
simplify significantly When Z = 0 and Z = {0}, Which Will eliminate all conditional expectations.
Definition 2 (ERMI — exponential Renyi mutual information). We define the exponential Renyi
mutual information between Yb and S given Z ∈ Z as
DR(Yb;S|Z ∈ Z) := EZ,Yb,S
PY ,s∣z(Y，SIZ)
pYb|Z(Yb|Z)pS|Z(S|Z)
Z∈Z -1.
(2)
^
in Appendix B, We unravel the definition for the special cases of interest corresponding to the exist-
ing notions of fairness. We also discuss that ERMi is the χ2-divergence (Which is an f -divergence)
between the joint distribution, pγ s∣z, and the Kronecker product of marginals, PYg 0 ps∣z. In
particular, ERMi is non-negative, and zero if and only if (Z, Z)-fairness is satisfied. Hence, ERMi
is a valid notion of fairness violation.
Definition 3 (Renyi mutual information (Renyi, 1961)). Let the Renyi mutual information of order
α > 1 between random variables Yb and S given Z ∈ Z be defined as:
/	( /	, ^	, 1 χ α-1	∖ ∖
Ia(Y; SIZ ∈Z ):=占 log (EZ,Y ,S ∖ (PYpYYZ)PS∣z∣(S)∣Z)
which generalizes Shannon mutual information
Z∈Z
(3)
Iι(Y ； SIZ ∈Z ):= EZY S (log p	PY ,SlZ(Y,SlZ)	!∣Z ∈z) ,	(4)
",SI	<PY∣z(Y IZ )Ps∣z (S∣Z )J∖	ʃ
and recovers it as limα→1+ Iα(Y; SIZ ∈ Z) = I1 (Y; SIZ ∈ Z).
TL ɪ . .ι . i ∕∙f> r>l Γ7 _ r∖ 、 C Ji	ι∙ . ∙ /` t t ∙ i' / Γ7 r∖ r∙ ∙	♦	. ∙ r` t
Note that Iα (Y; SIZ ∈ Z) ≥ 0 with equality if and only if (Z, Z)-fairness is satisfied.
Theorem 1	(ERMi is stronger than Shannon mutual information). We have
0 ≤ I1 (Y; S|Z ∈ Z) ≤ I2(Y; S|Z ∈ Z) ≤ eI2(Y;S\Z∈Z) - 1 = DR(Y； S∣Z ∈ Z).	(5)
All proofs are relegated to the appendix. Theorem 1 establishes that ERMi is a stronger notion of
fairness in the sense that driving it to zero would also bound the Shannon mutual information. it
also shows that ERMI is exponentially related to the Renyi mutual information of order 2.
3
Under review as a conference paper at ICLR 2021
Definition 4 (Renyi correlation (Hirschfeld, 1935; Gebelein, 1941; Renyi, 1959)). Let F and G
be the set of measurable functions such that for random variables Y and S, EYb {f(Y ; z)} =
ES {g(S； z)} = 0, EY{f(Yb; z)1 2} = ES {g(S； z)2} = 1, for all Z ∈ Z. Renyicorrelation is:
PR(Yb ,S |Z ∈Z) := sup	EZY S n f (Yb; Z)g(S; Z)I Z ∈Z }.
f∈F,g∈G	, ,
Renyi correlation generalizes Pearson correlation coefficient
EYb S{YbS|Z}
ρ(Y, S|Z ∈ Z) := EZ ( l y,s ==
[√Eγ{Yb2∣Z}ES{S2∣Z}
(6)
(7)
Z∈Z
to capture nonlinear dependencies between the random variables by finding functions of random
variables that maximize the Pearson correlation coefficient between the random variables. In fact,
it is true that ρR(Y , S|Z ∈ Z) ≥ 0 with equality if and only if (Z, Z)-fairness is satisfied. Due to
these favorable properties, Renyi correlation has gained popularity as a measure of fairness viola-
tion (Baharlouei et al., 2020; Grari et al., 2020).
Theorem 2	(ERMI is stronger than Renyi correlation.). We have
, ^ . . , ^ , ^
0 ≤ ∣ρ(Yb, S|Z ∈ Z )| ≤ PR (Y ,S|Z ∈ Z) ≤ Dr(Y; S|Z ∈ Z),	(8)
, ^	, ^
andif|S| = 2, DR(Yb;S|Z ∈ Z) = PR(Yb,S|Z ∈ Z).
Next, we turn to another popular notion of fairness violation and establish similar relationships.
Definition 5 (Lq fairness violation). We define the Lq fairness violation for q ≥ 1 by:
Lq(Y,S|Z ∈ Z) := EZ { (MYO Ps∈S0 ∣pγ,S∣z(b,s∣Z) - Pγ∣z(y∖Z)Ps∣z(s|Z)∣qdy) 1 Z ∈ z}.
(9)
Note that Lq(Y, S∖Z ∈ Z) = 0 if and only if (Z, Z)-fairness is satisfied. In particular, L∞ fairness
violation recovers the demographic parity violation (Kearns et al., 2018, Definition 2.1) if we let
Z = {0} and Z = 0. It also recovers equal opportunity violation (Hardt et al., 2016) if we let
Z = A and Z = Y. Lq fairness violation generalizes this notion by considering the Lq norm
of the difference between the joint distribution pYb ,S and the Kronecker product of the marginal
distributions pγ 0 PS.
Theorem 3	(ERMI is stronger than L∞ fairness violation). Let Yb be a discrete or continuous
random variable, and S be a discrete random variable supported on a finite set. Then for any
q ≥ 1,1	____________________________________________________
0 ≤ Lq(Yb,S∖Z∈ Z) ≤	DR(Yb,S∖Z∈ Z).	(10)
The above theorem says that if a method controls ERMI value for imposing fairness, L∞ demo-
graphic parity violation (Kearns et al., 2018), L∞ equal opportunity violation (Hardt et al., 2016),
or L∞ equalized odds violation (Hardt et al., 2016) is also guaranteed to be bounded.
4 Fair Risk Minimization Via ERMI
Our goal is to train a model that balances fairness and accuracy objectives. To this end, we introduce
fair risk minimization through exponential Renyi mutual information framework defined below:2
Definition 6 (FRMI 一 fair risk minimization through exponential Renyi mutual information). To
balance fairness and accuracy, we consider the learning objective function:
min	Ex,y,s {'(X,Y； θ)} + XDr(Y^(X); S),	(11)
1 Note that a similar relationship with TV norm could be established as well.
2In this section, we present all results in the context of Z = 0 and Z = {0}, leaving off all conditional
expectations for clarity of presentation. The results could be generalized for general (Z, Z), as we have used
the resulting algorithms for empirical experiments.
4
Under review as a conference paper at ICLR 2021
where ` denotes the loss function, such as L2 loss or cross entropy loss; λ > 0 is a scalar balancing
the accuracy versus fairness objectives; DR Yθ (X); S is the notion of ERMI given in Eq. (17);
and Yθ (X) is the output of the learned model (e.g., the output of a classification or a regression
task, or the cluster number in a clustering task).
5τi ∙ι -Cr m ♦	1	.ι t	t	rτ∙ 1 八 ♦ ,ι	/` .< ∙	J	1	.1
While Yθ (X) inherently depends on X and θ, in the rest of this paper, we sometimes leave the
dependence of Yb
on X and/or θ implicit for brevity of notation. Notice that we have also left the
dependence of the loss on the predicted outcome Y implicit. FRMI is the objective we should solve
if demographic parity is the desired fairness notion; if instead we are interested in equalized odds
or equal opportunity, then DR(Y , S) should be replaced by DR(Y , S|Z ∈ Z) for an appropriate
(Z, Z) pair, per the discussion in Section 3. Since the theory is very similar in both cases, we stick
with FRMI as defined in Definition 6. In practice, the true joint distribution of (X, S, Y, Yb) is un-
known and we only have N samples at our disposal, making it impossible to solve FRMI. Hence,
We turn into fair empirical risk minimization via exponential Renyi mutual information (FERMI)
approach. While it is natural to estimate EX,Y,S ` X, Y ; θ through the empirical risk, the esti-
mation of DR(Y , S) in the objective function in Eq. (11) is not as straightforWard. In What folloWs,
We propose tWo approaches for estimating DR(Y , S). These tWo approaches result in tWo different
algorithms for balancing fairness and accuracy, Where We discuss the benefits and shortcomings of
each.
4.1	FERMI via empirical estimation of the probability distributions
Let {xi , si , yi , ybi}i∈[N] denote the features, sensitive attributes, targets, and the predictions of the
model parameterized by θ for samples i ∈ [N]. A natural approach to estimate the objective function
in Eq. (11) and learning the parameter θ is through solving the problem
min I NN X '(xi,yi； θ) + λDbR(Yθ; S)} ,	(12)
[N i∈[N]
Here D)R(Yθ; S):= Ps∈S Rb∈Y PY标募(：)dy - 1 is an empirical estimate of DR(Yθ, S) where
pbS (s), pbYb (yb), and pbYb ,S (yb, s) are the empirical estimation of the corresponding probability func-
tions. To make the objective function differentiable with respect to θ, we make the following as-
sumption:
Assumption 1. Assume the sensitive attribute is a deterministic function of the features, i.e., S =
fs (X). This trivially holds if the sensitive attribute is available as part of the features. Further,
assume the following soft conditional density:
e	e-τ'(x,b^)	]
PYθ,S(bθ, S) = Ex {l{fs(X) = s}Ry∕τ'(XMθ)dyj ,
(13)
where τ > 0 controls the softness of the decision andτ → ∞ would recover the hard decision made
by choosing y that minimizes the loss function.
Assumption 1 is a generalization of the assumption in RFI (Baharlouei et al., 2020), and is natural
in problem instances where the decisions made by the learning algorithm are soft decisions. In
particular, logistic regression or neural networks with soft-max layer and cross entropy loss satisfy
Assumption 1. We can further findpYb|S, pYb , and pS from Eq. (13).
Under this assumption, we will have the following empirical estimate of the joint distribution of the
predicted target and the sensitive attribute:
1	e-τ'(χi,b5θ)
bYθ,S(bθ,s) := NT 1{Si = s} R ∖,e-τ'(χiMθ)dy,
i∈[N]	y∈Y
(14)
which could be marginalized to define PS(s) andPY(∙) as well. These empirical probabilities make
DR (Yθ; S) a differentiable function of θ. Notice that these empirical quantities converge to the
5
Under review as a conference paper at ICLR 2021
true distributions for finite S and Y , however, the sample complexity required for their estimation
scales linearly with |S | and |Y |, which implies an exponential scaling with the number of sensitive
attributes. Our stochastic algorithm that will be presented in the next section will aim to remedy this
potential problem.
To solve Eq. (12), one can apply the gradient descent algorithm and use the dynamics
θt+1 = θt-ηVθ J N X '(xi,yi； θt) + λDDR(Yb9t; S)卜	(15)
I	i∈[N ]
where η > 0 is the learning rate/step-size. Note that when the sensitive attribute is binary, our
algorithm is the same as the one in (Baharlouei et al., 2020) since then DR Yθt ; S = ρR(Y, S),
the Renyi correlation, by Theorem 8. However, in the non-binary case, our algorithm is different
from (Baharlouei et al., 2020) in general, and we show in Sec. 5 that it achieves a more favorable
fairness-accuracy tradeoff curve. Under standard assumptions on the loss function and the learning
rate, one can show that the dynamics in Eq. (15) find an -stationary point, (i.e., a point with the
norm of gradient being smaller than E) in O( *) iterations (Nesterov, 2013).
Theorem 4.	(Informal statement) Gradient descent (i.e. Eq. (15)) converges to the set of -first
Orderstationary points ofthe FERMI objective (Eq. (12)) in O( *) iterations (gradient evaluations).
While this algorithm achieves the optimal rate of first-order methods for general smooth non-convex
optimization problems, the empirical ERMI term in the objective in Eq. (12) is a biased estimator
of ERMI in Eq. (11). This bias makes the optimization problem in Eq. (12) not suitable for using
stochastic methods. For example, in the extreme case, where only one sample is available to the
learner for updating the objective at each turn, Dr(∙; ∙) can be severely biased due to the nonlin-
earities in how it is defined. In the next subsection, we propose another approach for estimation of
Dr (•; ∙) that results in an unbiased estimator, which is amenable to stochastic optimization.
Algorithm 1 Two-Time Scale SGDA for FERMI
1:	Input: θ0 ∈ Rdθ, W0 ∈ W ⊂ Rk×m, step-sizes (ηθ, ηw), mini-batch size M, fairness param-
eter λ ≥ 0, iteration number T.
2:	for t = 0, 1, . . . , T do
3:	Draw a batch B of data points {(xi, yi)}i∈B
4:	Set θt+1 ― θt - ηθ hlB Pi∈B Vθ'(Xi, yi, θ) - 2λVθ vec(%(θ)bi(θ)T)T Vec(WTW) +
2λVθ vec(bi(θ)sT)T vec(WTP-1/2)]
5:	Set Wt+1 一 ∏w (Wt + ηw Pi∈B h - 2λWbi(θ)bi(θ)T + 2λP-1/24仇⑹])
6:	end for
7:	Pick £ uniformly at random from {1,..., T}
8:	Return: θ*.
4.2 Stochastic FERMI
In order to solve the population level objective in Eq. (11) using stochastic methods (such as stochas-
tic gradient descent), one needs to obtain an unbiased estimate of the objective in equation 11, i.e.,
Ex,y,s {'(X, Y; θ)} + XDr(YΘ(X); S). Clearly, the empirical average 吉 pi∈β 'g, y； θ) is
an unbiased estimator of EX,Y,S ` X, Y; θ , where B ⊆ [N] is a batch of data points. Thus, to
develop a stochastic algorithm, we need to have an unbiased estimator of DR Yθ (X); S given a
batch of data points B . The following Theorem will help us obtain such an estimator.
Theorem 5.	For discrete random variables Y and S where Y ∈ [m], S ∈ [k], we have
DR(Yb;S) = max	- Tr(W PybW T) + 2 Tr(W Pyb,sPs-1/2) -1 ,	(16)
W ∈Rk×m	s
6
Under review as a conference paper at ICLR 2021
a) Adult Dataset
Test Error
Uo+30o>sppo POZ=Pnb 山
UO+3B-O>Awed uzdB⅛OEΘα
Test Error
0.20	0.22	0.24	0.26	0.28	0.30
Test Error
Figure 1: Tradeoff of fairness violation vs test error for different baselines on German Credit and Adult
datasets. The desired operation point is the lower left corner where both fairness violation and test error are
small. FERMI achieves the best fairness vs performance tradeoff across all baselines.
(PY ⑴	0 ʌ
where Pb = I	∙∙.	I ,
0	pYb (m)
(PY ,S (1,1)	...	PY ,S (1,k) ʌ	(PS ⑴	0 ʌ
Pyb,s = I ...	...	... I , Ps = I	. . .	I .
PYb ,S (m, 1)	. . .	PYb,S (m, k)	0	PS (k)
ʌ
Let Yb ∈ {0, 1}m and Sb ∈ {0, 1}k be the
the above theorem implies that Eq. (11) can be re-written as
one hot encoded version of Y and S, respectively. Then,
min max E
θ W ∈Rk×m
{'(X, Y; θ) - Tr(W Y Y T W T) + 2 Tr(W Y ST P-1/2) - 1}.
Hence, given a batch of data points B , we can obtain an unbi-
ased estimator of the above objective function by the empirical average
|BBJ Pi∈B {'(Xi,yi； θ) — Tr(WbibTWT) + 2 Tr(W%STP-1/2) τ} ∙ This observation
leads to the stochastic algorithm presented in Algorithm 1. Notice that this algorithm is based on
the assumption that Ps is known. This assumption is practical since the distribution of sensitive
attributes (such as male vs female) is known in many applications (or it can be estimated accurately
using the training data). The convergence rate of Algorithm 1 is analyzed in Theorem 6.
Theorem 6.	(Informal statement) Algorithm 1 converges to the set of -first order stationary points
of the FERMI objective (c.f. Eq. 12) in O( -14) iterations (stochastic gradient evaluations).
The formal statement of this theorem can be found in Theorem 11 in Appendix D. Notice that
while this algorithm has a slower rate of convergence than the batch algorithm, it is stochastic (each
iteration is computationally cheap) and amenable to large-scale problems. Note also that a faster
convergence rate of O(-⅛) could be obtained by using the (more complicated) SREDA method of
(Luo et al., 2020) instead of SGDA to solve FERMI objective. We omit the details here. In the next
section, we numerically evaluate the performance of the algorithms described in this section.
5	Numerical Experiments
5.1	B inary fair classification with a binary sensitive attribute
We start the experimental setup for binary classification problem with a binary sensitive attribute.
This is a common setup among most existing baseline methods. Per Theorem 2, in this binary
7
Under review as a conference paper at ICLR 2021
Figure 2: Comparison between FERMI and RFI (Baharlouei et al., 2020). FERMI achieves a better fairness vs
performance tradeoff. Moreover, due to computationally expensive operations like performing singular value
decomposition (SVD), RFI has poor scalability with the cardinality of sensitive features and target classes.
classification case ERMI is equivalent to Renyi correlation (BaharloUei et al., 2020), and Per the
discussion in Sec. 4.1, our batch algorithm is exactly the same as RFI (Baharlouei et al., 2020).
While we solve FERMI to imPose an ERMI regularizer, we still measure fairness violation via PoP-
ular fairness violation notions, such as conditional demograPhic Parity L∞ violation (Definition 10),
conditional equal oPPortunity L∞ violation (Definition 11), and conditional equalized odds viola-
tion. In Fig. 1, we rePort the fairness violation vs error for German Credit and Adult datasets. As
can be seen, for all three PoPular notions of fairness, FERMI achieves the best trade-off between
fairness and error Probability on the test data. This could be Partly due to smoothness of FERMI
oPtimization Problem, and Partly due to the fact that ERMI uPPer bounds other fairness notions as
discussed in Sec. 3. We will have a more detailed discussion on this in Section 6.
5.2	Non-binary fair classification with a non-binary sensitive attribute
Next, we consider a general classification Problem with |S | > 2. In this case, we consider the
Communities and Crime dataset, which has 18 binary sensitive attributes in total, and we Pick
{7, 10, 14, 18} sensitive attributes out of those for different exPeriments, which corresPonds to
|S | ∈ {27 , 210 , 214 , 218 }. We discretize the target into three classes {high, medium, low} (ternary
classification). The only baseline that we are aware of that can handle non-binary classification with
non-binary is sensitive attributes is RFI (Baharlouei et al., 2020). The results are Presented in Fig. 2,
where we use conditional demograPhic Parity L∞ violation (Definition 10) and conditional equal
oPPortunity L∞ violation (Definition 11) as the fairness violation notion. As can be seen, FERMI
achieves a better tradeoff curve as comPared with RFI. It is noteworthy that the Per-iteration com-
Plexity of FERMI is far less than that of RFI, which requires solving a singular value decomPosition
at each iteration. Finally, the convergence rate for FERMI given in Theorem 4 guarantees an O(十)
convergence vs O(^⅛) for RFI (Baharlouei et al., 2020, Theorem 4.1). Empirically, We observed that
FERMI converges 〜10x faster on this problem instance. Finally, we also show that conditional de-
mographic parity L∞ violation (Definition 10) and square root of ERMI are approximately linearly
related, which further justifies the use of ERMI regularizer in the FERMI framework.
5.3	Large-scale classification with FERMI
In this experiment, we consider the color MNIST dataset (Li &
Vasconcelos, 2019) where MNIST digits are colored with dif-
ferent colors drawn from a Gaussian distribution with variance
σ around a certain average color. It is shown in (Li & Vas-
concelos, 2019) that as σ → 0, a convolutional network model
overfits significantly to the color on this dataset, and hence
will not be able to generalize on a regular black and white test
set. Our goal in this experiment is to show that FERMI can
promote independence between the predicted target and color
(which we use as the sensitive attribute within FERMI) to improve generalization in this setup.
This also allows us to examine the scaling of stochastic FERMI when used in convolutional neural
networks. We consider σ = 0, where the test performance is the lowest due to overfitting.
The result of the experiment is presented in Fig. 4. As expected, FERMI results in learning rep-
resentations that have less dependence on the color, hence leading to better generalization. It is
8
Under review as a conference paper at ICLR 2021
Figure 4: The application of FERMI to Color MNIST dataset (Li & Vasconcelos, 2019). As expected, FERMI
achieves a tradeoff between demographic parity L∞ violation and train error on the colored training samples.
On test data, however, reducing the demographic parity violation translates to less dependence on the color,
which avoids overfitting and decreases the test error. Finally, as λ increases we see that the gap between
train error and test error significantly decreases which shows that FERMI can avoid overfitting. For stochastic
FERMI, we use a mini-batch of size 512 and achieve a speedup of 100x per iteration. Along with scalability, we
also observe that the stochastic variant (Algorithm 1) has better generalization performance (lower test error).
noteworthy that the test error achieved by FERMI when σ = 0 is 22.6%, as compared to 23.3%
obtained using REPAIR (Li & Vasconcelos, 2019) for σ = 0.1. Further decreasing σ ≤ 0.05 the
test error using REPAIR sharply goes above 50%. We were unable to run REPAIR for σ = 0.
6	Discussion & Concluding Remarks
In this paper, We proposed a new notion of fairness, called exponential Renyi mutual information
(ERMI). We showed that ERMI is a strong notion of fairness violation providing guarantees on
several other popular notions, namely Pearson correlation, Renyi correlation, Shannon mutual infor-
mation, Renyi mutual information, and Lq distance violation. We proposed a Fair Empirical Risk
Minimization framework with an ERMI regularizer to balance performance and fairness, and called
it FERMI. Additionally, we showed that FERMI could be efficiently solved for non-binary sensitive
attributes and non-binary target variables. We proposed batch and stochastic algorithms for solving
FERMI with convergence guarantees for smooth losses. In particular, Algorithm 1 is unique among
existing fair algorithms as it is stochastic, making it much more practical for large-scale problems
(as demonstrated in Sec. 5.3). This is made possible by Theorem 5, which leads to an unbiased es-
timator of the gradient of ERMI. It is not at all clear if replacing ERMI by another regularizer, such
as Renyi correlation or Shannon mutual information, in Eq. (12) would be amenable to stochastic
optimization.
From an experimental perspective, we showed that FERMI leads to better fairness-accuracy tradeoffs
than the existing baselines. There are several possible explanations for the superior empirical per-
formance of FERMI compared to existing methods. One possible reason is that the objective func-
tion Eq. (12) is easier to optimize than the objectives of competing in-processing methods: ERMI
is smooth; and in the discrete case, is equal to the trace of a matrix (see Theorem 8), which is easy
to compute. Contrast this with the larger computational overhead of Renyi correlation, for example,
which requires finding the second singular value of a matrix. Furthermore, the sample complexity of
estimating Renyi mutual information of order 2 (and consequently that ofERMI) scales as Θ( a∕∣S∣)
as compared to Shannon mutual information which scales as Θ(∣S∣/ log |S|) (Acharya et al., 2014).
Another possible explanation is that ERMI is a stronger notion of fairness than all of the most widely
used fairness notions, as shown in Sec. 3, which might lead to better generalization. Together, these
facts suggest that ERMI serves as an efficient and easily optimizable proxy for these other notions,
leading to better practical performance regardless of which fairness measure is used. We leave it
as future work to rigorously understand which of these (or other) factors are responsible for the
favorable performance tradeoffs observed from FERMI. Finally, on the Color MNIST experiment
with neural network function approximation, we observed that stochastic FERMI outperforms batch
FERMI. In this case, we suspect that the randomness in stochastic FERMI supposedly contributes
to its convergence to a local minimum with superior generalization performance compared to batch
FERMI (see (Kleinberg et al., 2018) and the references therein).
9
Under review as a conference paper at ICLR 2021
References
Jayadev Acharya, Alon Orlitsky, Ananda Theertha Suresh, and Himanshu Tyagi. The complexity of
estimating Renyi entropy. arXiv:1408.1000v1, 2014.
Sina Aghaei, Mohammad Javad Azizi, and Phebe Vayanos. Learning optimal and fair decision trees
for non-discriminative decision-making. In Proceedings of the AAAI Conference on Artificial
Intelligence, volume 33, pp. 1418-1426, 2019.
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner. Machine bias. ProPublica, 2016.
Sina Baharlouei, Maher Nouiehed, Ahmad Beirami, and Meisam Razaviyayn. Renyi fair inference.
In ICLR, 2020.
Yahav Bechavod and Katrina Ligett. Penalizing unfairness in binary classification. arXiv preprint
arXiv:1707.00044, 2017.
Richard Berk, Hoda Heidari, Shahin Jabbari, Matthew Joseph, Michael Kearns, Jamie Morgen-
stern, Seth Neel, and Aaron Roth. A convex framework for fair regression. arXiv preprint
arXiv:1706.02409, 2017.
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. Man is
to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances
in neural information processing systems, pp. 4349-4357, 2016.
Jaewoong Cho, Gyeongjo Hwang, and Changho Suh. A fair classifier using mutual information. In
2020 IEEE International Symposium on Information Theory (ISIT), pp. 2521-2526. IEEE, 2020.
Evgenii Chzhen and Nicolas Schreuder. A minimax framework for quantifying risk-fairness trade-
off in regression. arXiv preprint arXiv:2007.14265, 2020.
Andrew Cotter, Heinrich Jiang, Maya R Gupta, Serena Wang, Taman Narayan, Seungil You, and
Karthik Sridharan. Optimization with non-differentiable constraints with applications to fairness,
recall, churn, and other goals. Journal of Machine Learning Research, 20(172):1-59, 2019.
Thomas M Cover and Joy A Thomas. Information theory and statistics. Elements of Information
Theory, 1(1):279-335, 1991.
Imre Csiszar and Paul C Shields. Information theory and statistics: A tutorial. Now Publishers Inc,
2004.
Amit Datta, Michael Carl Tschantz, and Anupam Datta. Automated experiments on ad privacy
settings. Proceedings on privacy enhancing technologies, 2015(1):92-112, 2015.
A. Dembo and O. Zeitouni. Large deviations techniques and applications. Springer Science &
Business Media, 2009.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil. Em-
pirical risk minimization under fairness constraints. In Advances in Neural Information Process-
ing Systems, pp. 2791-2801, 2018.
Flavio du Pin Calmon, Ali Makhdoumi, Muriel Medard, Mayank Varia, Mark Christiansen, and
Ken R Duffy. Principal inertia components and applications. IEEE Transactions on Information
Theory, 63(8):5011-5038, 2017a.
Flavio du Pin Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and
Kush R Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural
Information Processing Systems, pp. 3992-4001, 2017b.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In Proceedings of the 3rd innovations in theoretical computer science confer-
ence, pp. 214-226, 2012.
10
Under review as a conference paper at ICLR 2021
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD
international conference on knowledge discovery and data mining, pp. 259-268, 2015.
Benjamin Fish, Jeremy Kun, and Adam D Lelkes. A confidence-based approach for balancing fair-
ness and accuracy. In Proceedings of the 2016 SIAM International Conference on Data Mining,
pp. 144-152. SIAM, 2016.
Hans Gebelein. Das statistische problem der korrelation als variations-und eigenwertproblem und
sein zusammenhang mit der ausgleichsrechnung. ZAMM-Journal of Applied Mathematics and
Mechanics/Zeitschrift fur Angewandte Mathematik UndMechanik, 21(6):364-379, 1941.
Vincent Grari, Boris Ruf, Sylvain Lamprier, and Marcin Detyniecki. Fairness-aware neural Reyni
minimization for continuous features. arXiv preprint arXiv:1911.04929, 2019.
Vincent Grari, Oualid El Hajouji, Sylvain Lamprier, and Marcin Detyniecki. Learning unbiased
representations via Renyi minimization. arXiv preprint arXiv:2009.03183, 2020.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In
Advances in neural information processing systems, pp. 3315-3323, 2016.
Hermann O Hirschfeld. A connection between correlation and contingency. In Proceedings of the
Cambridge Philosophical Society, volume 31, pp. 520-524, 1935.
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa. Wasserstein fair
classification. In Uncertainty in Artificial Intelligence, pp. 862-872. PMLR, 2020.
Faisal Kamiran, Toon Calders, and Mykola Pechenizkiy. Discrimination aware decision tree learn-
ing. In 2010 IEEE International Conference on Data Mining, pp. 869-874. IEEE, 2010.
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regular-
ization approach. In 2011 IEEE 11th International Conference on Data Mining Workshops, pp.
643-650. IEEE, 2011.
Michael Kearns, Seth Neel, Aaron Roth, and Zhiwei Steven Wu. Preventing fairness gerryman-
dering: Auditing and learning for subgroup fairness. In International Conference on Machine
Learning, pp. 2564-2572, 2018.
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan. An alternative view: When does sgd escape local
minima? In International Conference on Machine Learning, pp. 2698-2707, 2018.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998. doi: 10.1109/5.726791.
Yi Li and Nuno Vasconcelos. Repair: Removing representation bias by dataset resampling. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 9572-
9581, 2019.
Tianyi Lin, Chi Jin, and Michael I. Jordan. On gradient descent ascent for nonconvex-concave
minimax problems. arXiv: 1906.00331v6, 2020.
Luo Luo, Haishan Ye, and Tony Zhang. Stochastic recursive gradient descent ascent for stochastic
nonconvex-strongly-concave minimax problems. arXiv: 2001.03724, 2020.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Adrian Perez-Suay, Valero Laparra, Gonzalo Mateo-Garcla, Jordi Munoz-Marl, Luis Gomez-Chova,
and Gustau Camps-Valls. Fair kernel learning. In Joint European Conference on Machine Learn-
ing and Knowledge Discovery in Databases, pp. 339-355. Springer, 2017.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness
and calibration. In Advances in Neural Information Processing Systems, pp. 5680-5689, 2017.
11
Under review as a conference paper at ICLR 2021
Edward Raff, Jared Sylvester, and Steven Mills. Fair forests: Regularized tree induction to minimize
model bias. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.
243-250, 2018.
Alfred Renyi. On measures of dependence. Acta Mathematica Academiae Scientiarum Hungarica,
10(3-4):441-451, 1959.
Alfred Renyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley
Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of
Statistics. The Regents of the University of California, 1961.
Ashkan Rezaei, Rizal Fathony, Omid Memarrast, and Brian D Ziebart. Fairness for robust log loss
classification. In AAAI, pp. 5511-5518, 2020.
Daniel Steinberg, Alistair Reid, Simon O’Callaghan, Finnian Lattimore, Lachlan McCalman, and
Tiberio Caetano. Fast fair regression via efficient approximations of mutual information. arXiv
preprint arXiv:2002.06200, 2020.
Latanya Sweeney. Discrimination in online ad delivery. arXiv preprint arXiv:1301.6822, 2013.
Bahar Taskesen, Viet Anh Nguyen, Daniel Kuhn, and Jose Blanchet. A distributionally robust
approach to fair classification. arXiv preprint arXiv:2007.09530, 2020.
Hans S Witsenhausen. On sequences of pairs of dependent random variables. SIAM Journal on
Applied Mathematics, 28(1):100-113, 1975.
Blake Woodworth, Suriya Gunasekar, Mesrob I Ohannessian, and Nathan Srebro. Learning non-
discriminatory predictors. arXiv preprint arXiv:1702.06081, 2017.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rogriguez, and Krishna P Gummadi. Fair-
ness constraints: Mechanisms for fair classification. In Artificial Intelligence and Statistics, pp.
962-970. PMLR, 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International Conference on Machine Learning, pp. 325-333, 2013.
Brian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-
sarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.
335-340, 2018.
12
Under review as a conference paper at ICLR 2021
A Existing notions of fairnes s
T . r-Cr Λ f^1∖ t	. .1	.	.	.	t∙ . t .	. . 1	1	1	1	1 . 1
Let (Y, Y , A, S) denote the true target, predicted target, the advantaged outcome class, and the
sensitive attribute, respectively. We review three major notions of fairness.
Definition 7 (demographic parity (Dwork et al., 2012)). We say that a learning machine satisfies
demographic parity if Y is independent of S.
Definition 8 (equalized odds (Hardt et al., 2016)). We say that a learning machine satisfies equalized
odds, if Y is conditionally independent of S given Y .
Definition 9 (equal opportunity (Hardt et al., 2016)). We say that a learning machine satisfies equal
opportunity with respect to A, if Y is conditionally independent of S given Y = y for all y ∈ A.
Notice that the equal opportunity as defined here generalizes the definition in (Hardt et al., 2016).
It recovers equalized odds if A = Y, and it recovers equal opportunity of (Hardt et al., 2016) for
A = {1} in binary classification.
B Properties and special cases of ERMI
Notice that ERMI is in fact the χ2-divergence between the conditional joint distribution, pYb ,S, and
the Kronecker product of conditional marginals, PY 0 PS, where the conditioning is on Z ∈ Z.
Further, χ2-divergence is an f-divergence with f (t) = (t 一 1)2. See (Csiszar & Shields, 2004,
Section 4) for a discussion. As an immediate result of this observation and well-known properties
of f -divergences, we can state the following property of ERMI:
Remark 7. DR(Y ; S|Z ∈ Z) ≥ 0 with equality if and only if for all z ∈ Z, Y and S are
conditionally independent given Z = z.
To further clarify the definition of ERMI, especially as it relates to demographic parity, equalized
odds, and equal opportunity, we will unravel the definition explicitly in a few special cases.
First, let Z = 0 and Z = {0}. In this case, Z ∈ Z trivially holds, and conditioning on Z has no
effect, resulting in:


DR(F S) = DR(F SIZ ∈z Uz=0,z={o}
=EY s ( pY,S(Y，S)) 一 1
y,s IPY(Y)PS(S) ∫
PYb,S (yb, s) 一 PYb (yb)PS (s)
=S∈S JL ―PY‰(s-PY S (y, s)dy.
(17)
TΛ /仆 C∖	.<	J	C llʌ » r-r .t . 1	111	1 1	.1	1 • 1	J	CC+	1
DR (Y; S) is the notion of ERMI that should be used when the desired notion of fairness is de-
mographic parity. In particular, DR(Yb; S) = 0 implies that χ2 divergence between PYb ,S, and the
Kronecker product of marginals, PYb 0 PS is zero. This in turn implies that Yb and S are independent,
which is the definition of demographic parity. We note that when Yb and S are discrete, this special
case (Z = 0 and Z = {0}) of ERMI is referred to as χ2-information in (du Pin Calmon et al.,
2017a).
13
Under review as a conference paper at ICLR 2021
Next, we consider Z = Y and Z = Y . In this case, Z ∈ Z is trivially satisfied, and hence,
Dr(Y; S|Y) := Dr(Y; S|Z ∈ Z)1
Z=Y,Z=Y
，	.■—■	、
=EYY s (	Ppb ,S∣Y (Y,")	) - 1
y,y,s Ipy∣y (Y|Y )PS∣Y (S|Y )∫
v- p p	PY,s∣y(b s|y)- PY∣y(b|y)psiY(s|y)	ʌ (
S∈S Jy∈γ Jy∈γ	PY∣y(b|y)Ps|Y(SIy)	pY,Y,s y,y,s y y
=XZ Z	PYr|y)2)PY(y)dbdy-1.	'
s∈S y∈Y	yb∈Y PYb|Y(yb|y)Ps|Y(S|y)
C ∕∙f> Cm 1	111	1	1	.1 t ∙ t	i' i~ ∙	♦	1∙ t t t ɪ	t
DR (Y; S|Y) should be used when the desired notion of fairness is equalized odds. In particular,
DR (Y; S|Y) = 0 directly implies the conditional independence of Y and S given Y.
Finally, we consider Z = Y and Z = A. In this case, we have
DA(Y;SIY) := Dr(Υ;S∣Z∈Z)
Z =Y,Z =A
,^ _____
EY,Yb,s
PY ,S|Y (Y,S∣Y)
,^ . : : : -
PYb |Y (Yb IY)Ps|Y (SIY)
Y∈A -1
PYb,s|Y(yb,SIy) - PYb |Y (ybIy)Ps|Y (SIy) A
S∈S Jy∈A Jy∈Y	PY|Y(b|y)PS|Y(SIy)	PY y y y
XZ Z	PYiSIY(y,s∣y)∣)PY S∣Y(b,s|y)PA(y)dbdy - 1,
s∈S y∈A yb∈Y PYb|Y(ybIy)Ps|Y(SIy)	,
(19)
where
PY (y)
Ryo∈A Py (y0)dy0
(20)
This notion is what should be used when the desired notion of fairness is equal opportunity. This
can be further simplified when the advantaged class is a singleton (which is the case in binary
classification). If Z = Y and Z = {y}, then
DR(Yb; SIY = y) := DR{y}(Yb; SIY)
v- p	py,s∣y(b,s|y)-PY∣γ(b|y)PsiY(SIy)
=⅛y∈γy	pγ∣γ(y∖y)ps∣γ(s|y)	pγ，S|Y(y,sly) y
=Xf	PY ,s∣y (b,s∖y)2
S∈Sy∈γy Pγ∣γ(yb∖y)Ps∣γ(s∖y) y .
(21)
τ-,∙	11	.1	.1	.	. ∙	1-∖	∕C^ Cm 1 1-∖	∕C^ CI，广	∖ . -I	∙ .	.	∙.)
Finally, we note that we use the notation DR (Y; S∖Y) and DR(Y; S∖Y = y) to be consistent with
the definition of conditional mutual information in (Cover & Thomas, 1991).
C Relations between ERMI and other fairnes s violation notions
Proof of Theorem 1. We proceed to prove all the (in)equalities one by one:
C / T /C Γ-^∖ Γ7 _ r∖ EI ♦ ♦	11 1	1 .1	C	FCl ♦	♦ C	J
•	0 ≤ Is (Y ; S ∖Z ∈ Z ). This is well known and the proof can be found in any information
theory textbook (Cover & Thomas, 1991).
•	Iι(Y; S∖Z ∈ Z) ≤ I2(Y； S∖Z ∈ Z). ThisisaknoWnProPertyofRenyimUtUalmforma-
tion, but we provide a proof for completeness in Lemma 1.
14
Under review as a conference paper at ICLR 2021
•	I2(Yb; S|Z ∈ Z) ≤ eI2(YSZ∈Z) 一 1. This follows from the fact that X ≤ ex 一 1.
•	eI2(Y;S)IZ∈Z - 1 = DR(Yb; S|Z ∈ Z). This follows from simple algebraic manipulation.
Lemma 1. Let Yb , S, Z be discrete
or continuous random variables. Then:
□
_	-	r _ ，C	_ ，C	_
(a)	For any α, β ∈ [1, ∞], Iβ(Yb; S|Z ∈ Z) ≥ Iα(Yb; S|Z ∈ Z) if β > α.
(b)	limα→ι+ Iα(Yb ； S|Z ∈ Z) = Iι(Yb ； S) ：= EZ { DKL(PY ,s∣z ∣∣Pγ∣z 乳 Ps∣z )∣Z ∈Z},
where Iι(∙; ∙) denotes the Shannon mutual information and DKL is Kullback-Leibler di-
vergence (relative entropy).
(c)	For all α ∈ [1, ∞], Iα(Y ; S|Z ∈ Z) ≥ 0 with equality if and only if for all z ∈ Z, Y and
S are conditionally independent given z.
Proof. (a) First assume 0 < α < β < ∞ and that α, β 6= 1. Define a = α 一 1, andb = β 一 1. Then
the function φ(t) = tb/a is convex for all t ≥ 0, so by Jensen’s inequality we have:
b log ( E
b
p(Yb,S|Z)
,^..........
p(Yb|Z)p(S|Z)
Z ∈Z I ≥1log e[(gIZ)
厂 b ∖ Up(YbIZ)p(s∣z)
= 1log(E (( P(Yb，S|Z)
a V [<p(Yb∣Z)p(S|Z)
b/a
Z∈Z
Z∈Z .
Now suppose α
_ ,ʌ _
(22)
1. Then by the monotonicity for α 6= 1 proved above, we have
I1(Y ;S)
limα→∞
limα→1- Iα(Y ;S) = supα∈(0,1)Iα(Y ;S) ≤ infα>1Iα(Y ;S). Also, I∞(Y ;S)
Iα(Y ;S) = supα>0 Iα(Y ;S).
^


^

a
a

(b)	This is a standard property of the cumulant generating function (see (Dembo & Zeitouni, 2009)).
(c)	It is straightforward to observe that independence implies that Renyi mutual information van-
ishes. On the other hand, if Renyi mutual information vanishes, then part (a) implies that Shannon
mutual information also vanishes, which implies the desired conditional independence.	□
Proof of Theorem 2. The proof is completed using the following pieces.
, ，c	,	，c _	.
•	0 ≤ Iρ(Yb, SIZ ∈ Z)I ≤ ρR(Yb, SIZ ∈ Z). This is obvious from the definition of
,^
ρR(Yb,SIZ∈ Z).
•	ρR(Yb, SIZ ∈ Z) ≤ DR(Yb; SIZ ∈ Z). This follows from Theorem 8.
•	Notice that if ISI = 2, Theorem 8 implies that DR(Yb; SIZ ∈ Z) = ρR(Yb, SIZ ∈ Z).
□
Theorem 8.	Suppose that S = [k]. Let the k × k matrix P be defined as P = {Pij }i,j∈[k]×[k],
where
P :=	1	/	pPY,s(y,i)pγ,s(y,j)
j	√pSWpSCj) Jy∈Y ∖	PY(y)
Let 1 = σ1 ≥ σ2 ≥ . . . ≥ σk ≥ 0 be the eigenvalues of P. Then,
ρR(Yb, S) = σ2 ,
k
DR(Yb;S) = Tr(P) 一 1 =Xσi.
i=2
(23)
(24)
(25)
15
Under review as a conference paper at ICLR 2021
Proof. Eq. (24) is proved in (Witsenhausen, 1975, Section 3). To prove Eq. (25), notice that
Tr(P) =	Pii
i∈[k]
X ɪ [
i∈[k] pS (i) y∈Y
PY ,s (y,i产
pYb (y)
dy
EY s (( PY M ⑸!)
y,sKpY (Y )ps (S )〃
1 +DR (Yb;S ),
which completes the proof.
□
Proof of Theorem 3. It suffices to prove the inequality for L1 , as Lq is bounded above by L1 for
all q ≥ 1. The proof for the case where Z = 0 and Z = {0} follows from the following set of
inequalities:
L1 (Yb,S|Z ∈ Z )= X	PYb,S (y, s)-PYb (y )PS (s) dy	(26)
Lf /-----------IPYS (y,s)- PY (y)ps (S)I
=£ /	JPY(y)ps(S)-~~:~~/ ( 、 ( 、--dy	(27)
s∈S y∈Y	PYb (y)PS(S)
≤ t(X L 产 M T(X Zy.I	!!
(28)
uu	(PYb,S (y, S) -PYb(y)PS(S))2
≤ tS∈S L∈y∖ —PY≡W— )dy	(29)
=JDR (Y； S),	(30)
where Eq. (28) follows from Cauchy-Schwarz inequality, and Eq. (30) follows from Lemma 2.
The extension to general Z and Z is immediate by observing that ρ(Y, S|Z ∈ Z) =
EZ h ρ(Y,S∣Z)∣Z ∈Z i，pr(Y,S∣Z ∈ Z) = EZ [ pr(Y,S∣Z )∣Z ∈z],and DR (Y,S∣Z ∈
Z) = EZ hDr(Y,S∣Z)∣Z ∈Z].	□
Lemma 2. We have
"S) = X ∕∈y(
(PY,s(y,s) -PY(y)Ps(S))2
PYb (y)PS (S)
dy.
(31)
16
Under review as a conference paper at ICLR 2021
Proof. The proof follows from the following set of identities:
sX∈SZy∈Y
(PY,s(y,s)-PY⑻PS(S))2
pYb (y)pS (s)
(PYb,S (y, S))2
y y^ S∈S y∈γy PY (y)Ps(S) y
- 2X	PYb,S(y,S)dy
+	PYb (y)PS(S)dy
s∈S y∈Y
=E ( PY,S(Y，S)) - 1
PYb(Yb)PS(S)
= DR(Yb;S).
(32)
(33)
(34)
□
Next, we present some alternative fairness definitions and show that they are also upper bounded by
ERMI.
Definition 10 (conditional demographic parity L∞
violation). Given a predictor Yb supported
on
Y and a discrete sensitive attribute S supported on a finite set S, we define the conditional demo-
graphic parity violation by:
dep(Yb|S) := sup max PYb|S(yb|S) - PYb (yb) .
yb∈Y s∈S
(35)
First, we show that dp(Y|S) is a reasonable notion of fairness violation.
Lemma 3. dp(Y|S) = 0 iff (if and only if) Y and S are independent.
Proof. By definition, dep(Yb |S) = 0 iff for all yb ∈ Y, S ∈ S, PYb,S (yb|S) = PYb (yb) iff Yb and S are
independent (since We always assume P(S) > 0 for all S ∈ S).	□
Theorem 9	(ERMI is stronger than conditional demographic parity L∞
violation). Let Yb be a dis-
crete or continuous random variable supported on Y, andS be a discrete random variable supported
on a finite set S. Denote PSmin := mins∈S PS (S) > 0. Then,
1
0 ≤ dp (Y ⑸ ≤ F √Dr(Y ； S).	(36)
PS
Proof. The proof follows from the following set of (in)equalities:
2
dep(Yb |S)	= Sup max
yb∈Y s∈S
1
PYb|S(yb|S) -PYb
≤ -——SUp max
一 (Pmm )2 b∈Y s∈s
PYb,S(yb, S) - PYb (yb)PS (S))
≤ —--77
一 (Pmm )2
_	1
=(Pmin )2
X PYb,S (yb, S) -PYb(yb)PS(S))
DR(Yb;S),
where Eq. (40) follows from Theorem 3.
(37)
(38)
(39)
(40)
□
1
17
Under review as a conference paper at ICLR 2021
Definition 11 (conditional equal opportunity L∞ violation (Hardt et al., 2016)). Let Y, Y take val-
ues in Y and let A ⊆ Y be a compact subset denoting the advantaged outcomes (For example, the
decision “to interview” an individual or classify an individual as a “low risk” for financial pur-
poses). We define the conditional equal opportunity L∞ violation of Y with respect to the sensitive
attribute S and the advantaged outcome A by
eeo(Yb |S, Y ∈ A)
EY
sup ms∈aSx pYb,S|Y (yb|s, Y) - pYb |Y (yb|Y) Y ∈ A .
(41)
Theorem 10	(ERMI is stronger than generalized equal opportunity Kolmogorov violation - alterna-
tive definition). Let Yb , Y, be discrete or continuous random variables supported on Y , and let S
be a discrete random variable supported on a finite set S. Let A ⊆ Y be a compact subset of Y .
Denote pSm|iAn = mins∈S,y∈A pS|Y (s|y). Then,
o ≤ eo(YSY ∈ A) ≤ Wn ∖∣Dr(Y； s∣y ∈ A).
(42)
Proof. Notice that the same proof for Theorem 9 would give that for all y ∈ A:
0 ≤ sup max pYb,S|Y(yb|s,y) -pYb|Y(yb|y) := eeo(Yb |S, Y = y)
≤ P⅛ qDR(Y;SIY =y)
≤ *√Dr(Y; SIY = y).
Hence,
eeo(Yb IS, Y ∈ A) =EY neeo(YbIS,Y)Y ∈ Ao
≤ 志 Eγ {qDR(Y ； SIY)IY ∈A}
≤ 志 rEγn "Sy XY ∈Ao
=志 qDR(Y ； S ∣Y ∈a),
where the last inequality follows from Jensen’s inequality. This completes the proof.
□
D Stochastic FERMI
ProofofTheorem 5. Let W * ∈ argmaxw ∈Rk×m — Tr(WPbWT) + 2Tr(WPb,§P—1/2). WeWill
compute W * and plug it in the RHS of Eq. (16) to show the equality in Eq. (16). Setting the
derivative of the expression on the RHS equal to zero leads to:
—2WPyb+2Ps-1/2PybT,s=0 =⇒ W*=Pyb-1PybT,sPs-1/2.
Plugging this expression for W*, we have
W 臀m- 3WPbW T ) + 2 3WPb41/2)
= —Tr(Ps-1/2PybT,sPyb-1PybPyb-1Ps-1/2)+2Tr(Ps-1/2PybT,sPyb-1PybPyb-1Ps-1/2)
= Tr(Ps-1/2PybT,sPyb-1Pyb,sPs-1/2)
= Tr(Ps-1PybT,sPyb-1Pyb,s).
18
Under review as a conference paper at ICLR 2021
Writing out the matrix multiplication explicitly in the last expression, we have
Ps-1PybT,sPyb-1Pyb,s = UVT,
where Ui,j = Ps(i)TpY,S(j, i) and Vij= PY⑺TpY,S(j, i), for i ∈ [k],j ∈ [m]. Hence
max -Tr(WPybWT) + 2Tr(WPyb,sPs-1/2) = Tr(UVT)
=PY,s(j, i产
.⅛1 ps(i ps ⑶PY Cj)
i∈[k] j∈[m]	Y
= DRCYb; S),
which completes the proof.
□
Next, we move to the statement and proof of the precise version of Theorem 6. We first recall some
basic definitions:
Definition 12. Afunction f is β-smooth iffor all u, u0, we have ∣∣Vf (U) 一 Vf (u)k ≤ β∣∣u - u0∣∣.
Definition 13. A point θ is an E-stationary point of a differentiable function Φ if ∣∣VΦ(θ)k ≤ 匕
Assumption 2.	• ' is twice differentiable, l`-Lipscthiz, and β'-smooth in θ.
•	∣∣Vθ Py∣2 := ∣∣Vθ Vec(Pb)∣2 ≤ Ly and maxι∈[m] ∣∣Vθ ((Py) ι,ι) ∣∣2 ≤ Ly
•	maxl∈[m] ∣Vθ2θ (Pyb)l,l ∣2 ≤ βy .
•	∣VθPybT,s∣2 := ∣Vθvec(PybT,s)∣2 ≤ Lys and maxl∈[m],j∈[k] ∣Vθ ((Pyb,s)l,m) ∣2 ≤ Lys
•	maxl∈[m],j∈[k] ∣Vθθ (Pyb,s)l,j ∣2 ≤ βy,s .
Theorem 11 (Precise version of Theorem 6). Denote
f(θ, W ) = N X '(xi, yi； θ) + λ (- Tr(WPyW T) + 2 Tr(WPy,sP-1/2) - 1).
i∈[N]
Set W := BF(0,2D) ⊂ Rk×m (Frobenius norm ball of radius 2D), D :=…minmk^. Denote
pyy	ps
∆Φ := Φ(θ0) - minθ Φ(θ), where Φ(θ) := maxW ∈W f(θ, W). In Algorithm 1, choose the step-
sizes as η = Θ(1∕κ2β) and ηw = Θ(1∕β) and mini-batch size as M = Θ (max {1, κσ2e-21).
Then under Assumption 2, the iteration complexity of Algorithm 1 to return an E-stationary point of
f is bounded by
O (κ2β∆φ + κβ2D2 )
which gives the total stochastic gradient complexity of
O (κ4 + κβ2D2 max {1,κσ2[2}),
where
β = βι + 8λD2βy + 4λ—	. (√mk3/2Deys) + 2λ + 4λ f DLy +
√pmin ∖	)	∖
μ = 2λ% in,
K = β/μ,
2
σ2 = 2 (l` + 2λLyD2 + 4λPD^√mkLys) +2 (2λD + 2(pmin)-"√mk)2
The theorem follows from (Lin et al., 2020, Theorem 4.5) combined with the following technical
lemmas. We assume Assumption 2 holds for the remainder of the proof of Theorem 11:
19
Under review as a conference paper at ICLR 2021
Lemma 4. Let
f (θ, W) = NN X '(xi, yi； θ) + λ (- Tr(WPbWT) + 2 Tr(WPb,sP-1/2) - 1)
i∈[N]
：=N X g(θ,W,χi,yi).
i∈[N]
Then
1.	f is	β-smooth,	where β =	βι	+	8λD2βy	+ 4λ	√^r (√mk3∕2Deys) +	2λ	+
^V PS
4λ (DLy + √⅛fe).
2.	f(θ, ∙) is 2λpm in -strongly ConcaVefor all θ.
3.	∣∣W*∣∣f ≤ D, where D is as defined in Theorem 11 and W * denotes any maximizer of
f(θ,W).
Proof. By Assumption 2, g is twice continuously differentiable. Hence for part 1, it suffices to
upper bound the spectral norm of the second derivative of g(∙,∙, Z) by β for all Z = (x, y), where
we vectorize and then differentiate with respect to w := vec W and/or θ, so that the resulting first
and second derivatives are always vectors or a matrices (not tensors). Notice that g(θ, w, Z) =
'(z, θ) - λwτ(Py ㊈ I)W + 2λ(vec(W))TPy,sPU/ - λ and
v2g(θ,w,Z)=Gw；g(*Z))	Uθ,,w,,z)).
Further, by the definition of operator norm, we have
kv2g(θ, w, Z)k2 ≤ kvθ2θg(θ, w, Z)k2 + 2kvθ2wg(θ, w, Z)k2 + kv2wwg(θ, w, Z)k2.
Now we vectorize all matrices and then compute derivatives of g with respect to θ and vec(W):
Vθg(θ, W, z) = Vθ'(z, θ) — 2λVθ VeC(Pb)T Vec(WTW) + 2λVθ VeC(Pb,s)T Vec(WTP-1/2)
(43)
=Vθ'(z, θ) - 2λ	X	WilVθ ((Py)ι,ι)
l∈[m],i∈[k]
+ 2λ	X	Wi,j(Vθ (Pbs)j,i)(PsΤ2)i,i ；	(44)
j∈[m],i∈[k]
Vw g(θ,w,z) = -2λWPy + 2λPs 1/2PyTs.	(45)
Differentiating again yields:
Vwwg(θ,w, z) = -2λPy 乳 Ik;
Vwθg(θ, W,z) = ∂θa吗:⑶=-2λ(Im 乳 W)VθPy + 2λ(Im 乳 P-1分)Vθ Vec(PyTs);
V2θg(θ,w, z) = V2'(z, θ) - 2λ	X	Wi2lVθθ ((Pb)l,l)
l∈[m],i∈[k]
+ 2λ X	Wi,j(Vθθ (Pbs)j,i)(Psτ∕2)i,i .
j∈[m],i∈[k]
20
Under review as a conference paper at ICLR 2021
Then to establish part 1, use Assumption 2, Clairaut’s theorem, the definitions of the matrices
and fact that their entries are in [0,1], the relations IlABk2 ≤ IlAIl2∣∣B∣∣2 and IlvecWkι ≤
√mkk vec W∣∣2 = √mk∣W∣∣f, and the fact that ∣∣A 0 B∣∣2 = ∣∣Ak2kBk2 to bound the spectral
norm of each second derivative above.
The strong concavity statement follows by noticing NIwwg(θ, W) 4 -μI iff Py < 2λI iff
mini∈[m] py(i) ≥ 2λ .
Part 3 follows from the expression for W * in the proof of Theorem 5.	□
Lemma 5. Consider f and g as defined above. Then we have
EzNg(θ,W,z)=Nf(θ,W),
EzkVg(θ,W,z)-Vf(θ,W)k2 ≤ 2 (l` + 2λLyD2 + 4λ-D= √mkellS∖
p	√Pmin	)
+ 2 (2λD + 2(pmin)τ∕2√mk)2,
where both expectations are with respect to the empirical distribution on {zi}i∈[N] .
Proof. The first statement is obvious. The second follows from Eq. (44) in the proof of Lemma 4,
since
Ez∣Vg(θ, W, z) - Vf (θ, W)∣22
1N	1 N
=NN EkVg(θ,W,Zi )k2 - N E hVg(θ,W, Zi), Vg(θ,W,Zj )i
i=1	i,j=1
≤ 2 sup ∣Vg(θ, W, zi)∣22
zi
≤ 2sup {∣∣Vθg(θ, W,z)k2 + ∣∣Vwg(θ,W, z)k2}
z
≤ 2sup<∣Vθ'(z, θ) - 2λ	E	Wi∣ιVθ ((Py)出
z	l∈[m],i∈[k]
+ 2λ	X	Wi,j (Vθ (Pys)j,i)(P-1‰
j∈[m],i∈[k]
+ 2∣∣-2λWPy + 2λP-"PTslI2.
2
2
)
Then use Assumption 2 and basic norm inequalities to bound the norm of each term.	□
E Experiment details
E.1 Model Description
For all the experiments, the model’s output is of the form O = softmax(W x + b). The model
outputs are treated as conditional probabilities p(yb = i|x) = Oi which are then used to estimate
the ERMI regularizer. We encode the true class label Y and sensitive attribute S using one-hot
encoding. We define `() as the cross-entropy measure between the one-hot encoded class label Y
and the predicted output vector O.
We perform the experiments in sections 5.1 and 5.2 with a linear model (with softmax activation).
The model parameters are estimated using the algorithm described in section 4.1. In section 5.2,
the data set is cleaned and processed as described in (Kearns et al., 2018). The trade-off curves
for FERMI are generated by sweeping across different values for λ in [0, 100], learning rate η in
[0.0005, 0.01], and number of iterations T in [50, 200].
21
Under review as a conference paper at ICLR 2021
For the experiments in section 5.3, we create the synthetic color MNIST as described in (Li &
Vasconcelos, 2019). We set the value σ = 0. In figure 4, we compare the performance of stochastic
solver (section 4.2) against the GD algorithm (section 4.1). We use a mini-batch of size 512 when
using the stochastic solver. The color MNIST data has 60000 training samples, so using the stochas-
tic solver gives a speedup of around 100x for each iteration, and an overall speedup of around 40x.
We present our results on two neural network architectures; namely, LeNet-5 (Lecun et al., 1998)
and a Multi-layer perceptron (MLP). We set the MLP with two hidden layers (with 300 and 100
nodes) and an output layer with ten nodes. A ReLU activation follows each hidden layer, and a
softmax activation follows the output layer.
Some general advice for tuning λ: Larger value for λ generally translates to better fairness, but one
must be careful to not use a very large value for λ as it could lead to poor generalization performance
of the model. The optimal values for λ, η , and T largely depend on the data and intended application.
We recommend starting with λ ≈ 10. In Appendix E.2, we can observe the effect of changing λ on
the model accuracy and fairness for the COMPAS dataset.
E.2 Effect of Hyper-parameter Tuning on the Accuracy-Fairness Trade-off
We run ERMI algorithm for the binary case to COMPAS dataset to investigate the effect of hyper-
parameter tuning on the accuracy-fairness trade-off of the algorithm. As it can be observed in Fig. 5,
by increasing λ from 0 to 1000, test error (left axis, red curves) is slightly increased. On the other
hand, the fairness violation (right axis, green curves) is decreased as we increase λ to 1000. More-
over, for both notions of fairness (demographic parity with the solid curves and equality of opportu-
nity with the dashed curves) the trade-off between test error and fairness follows the similar pattern.
To measure the fairness violation, we use demographic parity violation and equality of opportunity
violation defined in Section equation 5 for the solid and dashed curves respectively.
Figure 5: Tradeoff of fairness violation vs test error for ERMI algorithm on COMPAS dataset. The solid
and dashed curves correspond to ERMI algorithm under the demographic parity and equality of opportunity
notions accordingly. The left axis demonstrates the effect of changing λ on the test error (red curves), while
the right axis shows how the fairness of the model (measured by equality of opportunity or demographic parity
violations) depends on changing λ.
22
Under review as a conference paper at ICLR 2021
E.3 Datasets Description
All of the following datasets are publicly available at UCI repository.
German Credit Dataset.3 German Credit dataset consists of 20 features (13 categorical and 7
numerical) regarding to social, and economic status of 1000 customers. The assigned task is to
classify customers as good or bad credit risks. Without imposing fairness, the DP violation of the
trained model is larger than 20%. We chose first 800 customers as the training data, and last 200
customers as the test data. The sensitive attributes are gender, and marital-status.
Adult Dataset.4 Adult dataset contains the census information of individuals including education,
gender, and capital gain. The assigned classification task is to predict whether a person earns over
50k annually. The train and test sets are two separated files consisting of 32,000 and 16,000 samples
respectively. We consider gender and race as the sensitive attributes (For the experiments involving
one sensitive attribute, we have chosen gender). Learning a logistic regression model on the training
dataset (without imposing fairness) shows that only 3 features out of 14 have larger weights than
the gender attribute. Note that removing the sensitive attribute (gender), and retraining the model
does not eliminate the bias of the classifier. the optimal logistic regression classifier in this case is
still highly biased. For the clustering task, we have chosen 5 continuous features (Capital-gain, age,
fnlwgt, capital-loss, hours-per-week), and 10,000 samples to cluster. The sensitive attribute of each
individual is gender.
Communities and Crime Dataset.5 The dataset is cleaned and processed as described in (Kearns
et al., 2018). Briefly, each record in this dataset summarizes aggregate socioeconomic information
about both the citizens and police force in a particular U.S. community, and the problem is to predict
whether the community has a high rate of violent crime.
COMPAS Dataset.6 Correctional Offender Management Profiling for Alternative Sanctions (COM-
PAS) is a famous algorithm which is widely used by judges for the estimation of likelihood of re-
offending crimes. It is observed that the algorithm is highly biased against the black defendants.
The dataset contains features used by COMPAS algorithm alongside with the assigned score by the
algorithm within two years of the decision.
3https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)
4https://archive.ics.uci.edu/ml/datasets/adult.
5http://archive.ics.uci.edu/ml/datasets/communities+and+crime
6https://www.kaggle.com/danofer/compass
23