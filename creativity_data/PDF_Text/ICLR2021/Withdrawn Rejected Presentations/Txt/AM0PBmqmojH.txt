Under review as a conference paper at ICLR 2021
Warpspeed Computation of Optimal Transport,
Graph Distances, and Embedding Alignment
Anonymous authors
Paper under double-blind review
Ab stract
Optimal transport (OT) is a cornerstone of many machine learning tasks. The
current best practice for computing OT is via entropy regularization and Sinkhorn
iterations. This algorithm runs in quadratic time and requires calculating the full
pairwise cost matrix, which is prohibitively expensive for large sets of objects. To
alleviate this limitation we propose to instead use a sparse approximation of the cost
matrix based on locality sensitive hashing (LSH). Moreover, we fuse this sparse
approximation with the Nystrom method, resulting in the locally corrected Nystrom
method (LCN). These approximations enable general log-linear time algorithms
for entropy-regularized OT that perform well even in complex, high-dimensional
spaces. We thoroughly demonstrate these advantages via a theoretical analysis and
by evaluating multiple approximations both directly and as a component of two
real-world models. Using approximate Sinkhorn for unsupervised word embedding
alignment enables us to train the model full-batch in a fraction of the time while
improving upon the original on average by 3.1 percentage points without any model
changes. For graph distance regression we propose the graph transport network
(GTN), which combines graph neural networks (GNNs) with enhanced Sinkhorn
and outcompetes previous models by 48 %. LCN-Sinkhorn enables GTN to achieve
this while still scaling log-linearly in the number of nodes.
1 Introduction
Measuring the distance between two distributions or sets of objects is a central problem in machine
learning. One common method of solving this is optimal transport (OT). OT is concerned with
the problem of finding the transport plan for moving a source distribution (e.g. a pile of earth) to a
sink distribution (e.g. a construction pit) with the cheapest cost w.r.t. some pointwise cost function
(e.g. the Euclidean distance). The advantages of this method have been shown numerous times, e.g.
in generative modelling (Arjovsky et al., 2017; Bousquet et al., 2017; Genevay et al., 2018), loss
functions (Frogner et al., 2015), set matching (Wang et al., 2019), or domain adaptation (Courty et al.,
2017). Motivated by this, many different methods for accelerating OT have been proposed in recent
years (Indyk & Thaper, 2003; Papadakis et al., 2014; Backurs et al., 2020). However, most of these
approaches are specialized methods that do not generalize to modern deep learning models, which
rely on dynamically changing high-dimensional embeddings.
In this work we aim to make OT computation for point sets more scalable by proposing two fast
and accurate approximations of entropy-regularized optimal transport: Sparse Sinkhorn and LCN-
Sinkhorn, the latter relying on our newly proposed locally corrected Nystrom (LCN) method. Sparse
Sinkhorn uses a sparse cost matrix to leverage the fact that in entropy-regularized OT (also known as
the Sinkhorn distance) (Cuturi, 2013) often only each point’s nearest neighbors influence the result.
LCN-Sinkhorn extends this approach by leveraging LCN, a general similarity matrix approximation
that fuses local (sparse) and global (low-rank) approximations, allowing us to simultaneously capture
both kinds of behavior. LCN-Sinkhorn thus fuses sparse Sinkhorn and Nystrom-Sinkhorn (Altschuler
et al., 2019). Both sparse Sinkhorn and LCN-Sinkhorn run in log-linear time.
We theoretically analyze these approximations and show that sparse corrections can lead to significant
improvements over the Nystrom approximation. We furthermore validate these approximations by
showing that they are able to reproduce both the Sinkhorn distance and transport plan significantly
better than previous methods across a wide range of regularization parameters and computational
1
Under review as a conference paper at ICLR 2021
MultisCale OT
Nystrom Sinkhorn
Sparse Sinkhorn
LCN-Sinkhorn
Figure 1: The proposed methods (sparse and LCN-Sinkhorn) show a Clear Correlation with the full
Sinkhorn transport plan, as opposed to previous methods. Entries of approximations (y-axis) and full
Sinkhorn (x-axis) for pre-aligned word embeddings (EN-DE). Color denotes sample density.
budgets (as e.g. demonstrated in Fig. 1). We then show the impaCt of these improvements by
employing Sinkhorn approximations end-to-end in two high-impaCt maChine learning tasks. First, we
inCorporate them into Wasserstein ProCrustes for word embedding alignment (Grave et al., 2019).
LCN-Sinkhorn improves upon the original method’s aCCuraCy by 3.1 perCentage points using a third of
the training time without any further model Changes. SeCond, we develop the graph transport network
(GTN), whiCh Combines graph neural networks (GNNs) with optimal transport, and further improve
it via learnable unbalanCed OT and multi-head OT. GTN with LCN-Sinkhorn is the first model that
both overComes the bottleneCk of using a single embedding per graph and sCales log-linearly in the
number of nodes. In summary, our paper’s main Contributions are:
•	LoCally CorreCted Nystrom (LCN), a flexible, log-linear time approximation for similarity matriCes,
leveraging both loCal (sparse) and global (low-rank) approximations.
•	Entropy-regularized optimal transport (a.k.a. Sinkhorn distanCe) with log-linear runtime via sparse
Sinkhorn and LCN-Sinkhorn. These are the first log-linear approximations that are stable enough
to substitute full entropy-regularized OT in models that leverage high-dimensional spaCes.
•	The graph transport network (GTN), whiCh Combines a graph neural network (GNN) with multi-
head unbalanCed LCN-Sinkhorn. GTN both sets the state of the art on graph distanCe regression
and still sCales log-linearly in the number of nodes.
2 Sparse Sinkhorn
Entropy-regularized optimal transport. In this work we foCus on optimal transport between two
disCrete sets of points. We furthermore add entropy regularization, whiCh enables fast Computation
and often performs better than regular OT (Cuturi, 2013). Formally, given two CategoriCal distributions
modelled via the veCtors p ∈ Rn and q ∈ Rm supported on two sets of points Xp = {xp1 , . . . , xpn}
and Xq = {xq1, . . . , xqm} in Rd and the Cost funCtion c : Rd × Rd → R (e.g. the squared L2
distanCe) giving rise to the Cost matrix Cij = c(xpi , xqi) we aim to find the Sinkhorn distanCe dcλ
and the assoCiated optimal transport plan P (Cuturi, 2013)
dcλ=minhP,CiF-λH(P),	s.t.P1m=p, PT1n=q,	(1)
with the Frobenius inner produCt h., .iF and the entropy H(P) = - in=1 jm=1 Pij log Pij. Note
that djλ includes the entropy and can thus be negative, while Cuturi (2013) originally used d1CutUri,c =
hP, CiF. This optimization problem can be solved by finding the vectors s and t that normalize the
Cij
columns and rows of the matrix P = diag(s)K diag(t) with the similarity matrix Kij = e- ɪ, so
that P 1m = P and PT 1n = q. This is usually achieved via the Sinkhorn algorithm, which initializes
the normalization vectors as s(1) = 1n and t(1) = 1m and then updates them alternatingly via
s(i) = p	(Kt(i-1)),	t(i) =q(KTs(i))	(2)
until convergence, where denotes elementwise division.
Sparse Sinkhorn. The Sinkhorn algorithm is faster than non-regularized EMD algorithms, which run
in O(n2mlognlog(nmax(C))) (Tarjan, 1997). However, its computational cost is still quadratic
2
Under review as a conference paper at ICLR 2021
in time, i.e. O(nm), which is prohibitively expensive for large n and m. We propose to overcome
this by observing that the matrix K, and hence also P, is negligibly small everywhere except at
each point’s closest neighbors because of the exponential used in K’s computation. We propose to
leverage this by approximating C via the sparse matrix Csp, where
Cij	if xpi and xqj are “near”,
∞	otherwise.
(3)
Ksp and Psp follow according to the definitions of K and P. In this work We primarily consider
neighbors with distance lower than r1 as “near”. Finding such neighbors can be efficiently solved via
locality sensitive hashing (LSH) on Xp ∪ Xq.
Locality sensitive hashing. LSH tries to filter “near” from “far” data points by putting them into
different hash buckets. Points closer than a certain distance r1 are put into the same bucket with
probability at least pi, while those beyond some distance r = C ∙ ri with c > 1 are put into the
same bucket with probability at most p2 p1 . There is a plethora of LSH methods for different cost
functions (Wang et al., 2014; Shrivastava & Li, 2014), so we do not have to restrict our approach to
a limited set of functions. In this work we focus on cross-polytope LSH (Andoni et al., 2015) and
k-means LSH (PaUIeVe et al., 2010), depending on the cost function (see App. H). Sparse Sinkhorn
with LSH scales log-linearly with the number of points, i.e. O(n log n) for n ≈ m (see App. A and
App. K for details). Unfortunately, LSH can fail when e.g. the cost between pairs is very similar (see
App. B). However, We can alleviate these limitations by fusing Ksp with the Nystrom approximation.
3	Locally Corrected NYSTRGM and LCN-SINKHORN
Nystrom method. The Nystrom method is a popular way of approximating similarity matrices that
provides performance guarantees for many important tasks (Williams & Seeger, 2001; Musco &
Musco, 2017). It approximates a positive semi-definite (PSD) similarity matrix K via its low-rank
decomposition KNys = UA-iV . Since the optimal decomposition via SVD is too expensive to
compute, Nystrom instead chooses a set of l landmarks L = {xli, . . . , xll} and obtains the matrices
via Uij = k(xpi, xlj), Aij = k(xli, xlj), and Vij = k(xli, xqj), where k(xi, x2) is an arbitrary
c(xi ,X2 )
PSD kernel, e.g. k(xi, x2) = e λ for Sinkhorn. Common methods of choosing landmarks
from Xp ∪ Xq are uniform and ridge leverage score (RLS) sampling. We instead focus on k-means
Nystrom and sampling via k-means++, which we found to be significantly faster than recursive RLS
sampling (Zhang et al., 2008) and perform better than both uniform and RLS sampling (see App. H).
Sparse vs. Nystrom. Exponential kernels like the one used for K (e.g. the Gaussian kernel) typically
have a reproducing kernel Hilbert space that is infinitely dimensional. The resulting Gram matrix K
thus always has full rank. A low-rank approximation like the Nystrom method can therefore only
account for its global structure and not the local structure around each point x. As such, it is ill-suited
for any moderately low entropy regularization parameter, where the transport matrix P resembles
a permutation matrix. Sparse Sinkhorn, on the other hand, cannot account for global structure and
instead approximates all non-selected distances as infinity. It will hence fail if more than a handful of
neighbors are required per point. These approximations are thus opposites of each other, and as such
not competing but rather complementary approaches.
Locally corrected Nystrom. Since we know that the entries in our sparse approximation are exact,
fusing this matrix with the Nystrom method is rather straightforward. For all non-zero values in the
sparse approximation Ksp we first calculate the corresponding Nystrom approximations, obtaining
the sparse matrix KNspys. To obtain the locally corrected Nystrom (LCN) approximation we remove
these entries from KNys and replace them with their exact values, i.e.
KLCN = KNys + K∆sp = KNys - KNspys + Ksp .
(4)
LCN-Sinkhorn. To obtain the approximate transport plan PLCN we run the Sinkhorn algorithm
with KLCN instead of K. However, we never fully instantiate KLCN. Instead, we only save the
decomposition and directly use these parts in Eq. (2) via KLCNt = U(A-iV t) + K∆spt, similarly
to Altschuler et al. (2019). As a result we obtain the decomposition of PLCN = RyS + P∆p =
PUPW + Psp 一 PNyS and the approximate distance (using Lemma A from Altschuler et al. (2019))
dλCN,c = λ (STPUPW 1m + ITPUPWt + STP∆p1m + ITP∆pt).
(5)
3
Under review as a conference paper at ICLR 2021
This approximation scales log-linearly with dataset size (see App. A and App. K for details). It allows
Us to smoothly move from Nystrom-Sinkhorn to sparse Sinkhorn by varying the number of neighbors
and landmarks. We can thus freely choose the optimal “operating point” based on the underlying
problem and regularization parameter. We discuss the limitations of LCN-Sinkhorn in App. B.
4	Theoretical Analysis
Approximation error. The main question we aim to answer in our theoretical analysis is what
improvements to expect from adding sparse corrections to Nystrom Sinkhorn. To do so, we first
analyse approximations of K in a uniform and a clustered data model. In these we use Nystrom and
LSH schemes that largely resemble k-means, as used in most of our experiments. Relevant proofs
and notes for this section can be found in App. C to G.
Theorem 1.	Let Xp and Xq have n samples that are uniformly distributed in a d-dimensional
closed, locally Euclidean manifold with unit volume. Let furthermore Cij = kxpi - xqj k2 and
Kij = e-Cij /λ. Let the l landmarks L be arranged optimally and regularly so that the expected L2
distance to the closest landmark is minimized. Denote R = 1 minχ,y∈L,χ=y ∣∣x 一 y∣∣2. Assume that
the sparse correction Kisjp = Kij if and only if xqj is one of the k - 1 nearest neighbors of xpi, and
that the distance to xpi’s k-nearest neighbor δk R. Then the expected maximum error in row i of
the LCN approximation KLCN is
E[∣Ki,: — Klcnz∣∞] = E[e-δk/λ] 一 E[KLCN,i,j],	(6)
with j denoting the index of xpi ’s k-nearest neighbor. Using the upper incomplete Gamma function
Γ(., .) we can furthermore bound the second term by
e-√dR" ≤ E[κ	∙ .] ≤ 2d(r(d) - r(d, 2R")) + o(e-2√3R")	(7)
≤ EKLCN,i,j]一 (2R∕λ)d(1 + e-2R∕λ) + O(e	).	⑺
The error in Eq. (6) is dominated by the first term since δk	R. Note that R only decreases
slowly with the number of landmarks since R ≥ ((d/2)! )1/d2√∏ (Cohn, 2017). Moving from pure
Nystrom to LCN by correcting the nearest neighbors’ entries thus provides significant benefits, even
for uniform data. For example, by just correcting the first neighbor we obtain a 68 % improvement in
the first term (d=32, λ = 0.05, n= 1000). This is even more pronounced in clustered data.
Theorem 2.	Let Xp , Xq ⊆ Rd be distributed inside the same c clusters with cluster centers xc. Let r
be the maximum L2 distance of a point to its cluster center and D the minimum distance between two
points from different clusters, with r D. Let each LSH bucket used for the sparse approximation
Ksp cover at least one cluster. Let KNys use 1 ≤ l ≤ d and KLCN use l = 1 optimally distributed
landmarks per cluster. Then the maximum error is
le-2√r2+⅛1 δ/λ
1 一 max --------------τ-7τ——
∆∈[0,r] 1 + (l - 1)e-△/λ
max ∣K 一 Ksp∣∞ = e-D”,	(9)
max ∣K 一 KLCNk∞ = e-D”(1 - e-2"λ(2 - e-2"λ) + O(e-D")).	(10)
Since We can lower bound Eq. (8) by 1 一 le-2r∕λ - O(e-D∕λ) we can conclude that the error in
KNyS is close to 1 for any reasonably large λr (which is the maximum error possible). The errors in
Ksp and KLCN on the other hand are vanishingly small, since r D.
Moreover, these maximum approximation error improvements directly translate to improvements
in the Sinkhorn approximation. We can show this by slightly adapting the error bounds for an
approximate Sinkhorn transport plan and distance due to Altschuler et al. (2019).
Theorem 3 (Altschuler et al. (2019)). Let Xp , Xq ⊆ Rd have n samples. Denote ρ as the maximum
distance between two samples. Let K be an approximation of the similarity matrix K with Kij =
e-kxpi-χqjk2∕λ and ∣K 一 K∣∣∞ ≤ 三e-ρ∕λ, where ε0 = min(1, 505+：^ λn)). Whenperforming
the Sinkhorn algorithm until ∣JP 1n — Pk1+ ∣∣.PT 1n — q∣ι ≤ ε0∕2, the resulting approximate
transport plan P and distance d： are bounded by
ldλ 一 d： | ≤ ε,	DKL(PllP) ≤ ε"∙	(11)
max ∣K 一 KNys∣
O(e-D∕λ),	(8)
4
Under review as a conference paper at ICLR 2021
Convergence rate. We next show that approximate Sinkhorn converges as fast as regular Sinkhorn
by slightly adapting the convergence bound by Dvurechensky et al. (2018) to account for sparsity.
Theorem 4 (DvUrechensky et al. (2018)). Given the matrix K ∈ Rn×n and p, q the Sinkhorn
algorithm gives a transport plan satisfying kP1N - pk1 + kPT1N - qk1 ≤ ε in iterations
k ≤ 2+ -4ln(minij{Kj |Kj > 0} mmij{pi, qj D
ε
(12)
Backpropagation. Efficient gradient compUtation is almost as important for modern deep learning
models as the algorithm itself. These models UsUally aim at learning the embeddings in Xp and Xq
and therefore need gradients w.r.t. the cost matrix C. We can estimate these either via aUtomatic
differentiation of the Unrolled Sinkhorn iterations or via the analytic solUtion that assUmes exact
convergence. Depending on the problem at hand, either the aUtomatic or the analytic estimator
will lead to faster overall convergence (Ablin et al., 2020). LCN-Sinkhorn works flawlessly with
automatic backpropagation since it only relies on basic linear algebra (except for choosing Nystrom
landmarks and LSH neighbors, for which we Use a simple straight-throUgh estimator (Bengio et al.,
2013)). To enable fast analytic backpropagation we provide analytic gradients in Proposition 1. Note
that both backpropagation methods have runtime linear in the number of points n and m.
Proposition 1. The derivatives of the distances dcλ and dLλCN,c (Eqs. (1) and (5)) and the optimal
transport plan P ∈ Rn×m w.rt. the (decomposed) cost matrix C ∈ Rn×m in entropy-regularized
OT and LCN-Sinkhorn are
ddλCN,c	∂dcλ 	= ∂C =-λs(W球, 骼=	∂P∙. =P.	dpj : ,	∂Ckl 二-λ(sτU)ttτ,	=-1 Pij δik δji, λ ddλCN,c = fp sp	ddλCN,c _	(13) =-λPN3s,
∂U	二			∂ log Ksp	，	d log KNyS	
					(14)
∂Pu,ij _ X X "∂ukT = δik δjl si, dPwj = δik δ,l ti, ∂Wki	ik jl j,	dPW,ij _ p t c p	dPU,ij —力 /力 t ∂Ukl = U,ik k W,j,	∂Wki = U,ik t W,j, ∂Ppsp	∂Ppsp	(15) 	j- = P spδ»δ∙7		NyS,ij- = PF	δ”δ∙7 ∂logKkP	Pj δikδjt,	∂logKNyS,kt	PNyS,jδikδjt,
with δj denoting the Kronecker delta and * the Moore-Penrose pseudoinverse. Using these decompo-
sitions we can backpropagate through LCN-Sinkhorn in time O((n + m)l2 + l3).
5	Graph Transport Network
Graph distance learning. The ability to predict similarities or distances between graph-structured
objects is useful across a wide range of applications. It can e.g. be used to predict the reaction rate
between molecules (Houston et al., 2019), search for similar images (Johnson et al., 2015), similar
molecules for drug discovery (Birchall et al., 2006), or similar code for vulnerability detection (Li
et al., 2019). We propose the graph transport network (GTN) to evaluate approximate Sinkhorn and
advance the state of the art on this task.
Graph transport network. GTN first uses a Siamese graph neural network (GNN) to embed
two graphs independently as sets of node embeddings. These embeddings are then matched using
enhanced entropy-regularized optimal transport. Given an undirected graph G = (V , E ), with node
set V and edge set E, node attributes xi ∈ RHx and (optional) edge attributes ei,j ∈ RHe , with
i, j ∈ V , we update the node embeddings in each GNN layer via
hs(ell)f,i = σ(Wn(old)ehi(l-1) + b(l)),	(16)
hi( ) = hs(e)lf,i + X ηi(,j)hs(e)lf,jWedgeei,j,	(17)
j∈Ni
with Ni denoting the neighborhood of node i, hi(0) = xi, hi(l) ∈ RHN for l ≥ 1, the bilinear layer
Wedge ∈ RHNXHN ×He, and the degree normalization η(1j = 1 and η(lj = 1 / Pdegi degj for l > 1.
5
Under review as a conference paper at ICLR 2021
This choice of ηi,j allows our model to handle highly skewed degree distributions while still being
able to represent node degrees. We found the choice of non-linearity σ not to be critical and chose a
LeakyReLU. We do not use the bilinear layer Wedgeei,j if there are no edge attributes. We aggregate
each layer’s node embeddings to obtain the final embedding of node i
hifinal = [hs(e1l)f,i k hi(1) k hi(2) k ... khi(L)].	(18)
Having obtained the embedding sets H1final and H2final of both graphs we use the L2 distance as a cost
function and then calculate the Sinkhorn distance, which is symmetric and permutation invariant
w.r.t. the sets H1final and H2final. We obtain the embeddings for matching via h(i0) = MLP(hifinal) and
obtain the final prediction via d = dcλwout + bout, with learnable wout and bout. All weights in GTN are
trained end-to-end via backpropagation. For small graphs we use the full Sinkhorn distance and scale
to large graphs by leveraging LCN-Sinkhorn. GTN is more expressive than models that aggegrate
node embeddings to a single fixed-size embedding for the entire graph but still scales log-linearly
in the number of nodes, as opposed to previous approaches that scale quadratically. Note that GTN
inherently performs graph matching and can therefore also be applied to this task.
Learnable unbalanced OT. Since GTN regularly encounters graphs with disagreeing numbers of
nodes it needs to be able to handle cases where kpk1 6= kqk1 or where not all nodes in one graph
have a corresponding node in the other and thus P1m < p or PT1n < q. Unbalanced OT allows us
to handle both of these cases (Peyre & Cuturi, 2019). Previous methods did so by swapping these
requirements with a uniform divergence loss term on p and q (Frogner et al., 2015; Chizat et al.,
2018). However, these approaches uniformly penalize deviations from balanced OT and therefore
cannot adapt to only ignore parts of the distribution. We propose to alleviate this limitation by
swapping the cost matrix C with the bipartite matching (BP) matrix (Riesen & Bunke, 2009)
C	C	C (p,ε) C (p,ε) ci,ε	i	= j C (ε,q) cε,j	i	= j C (ε,ε)	0
CBP=	C(ε,q)	C(ε,ε)	, Cij	=	∞	i	6=j	, Cij	=	∞	i	6=j	, Cij	=0,
(19)
and adaptively computing the costs ci,ε , cε,j and cε,ε based on the input sets Xp and Xq. Using the
BP matrix adds minor computational overhead since we only need to save the diagonals cp,ε and cε,q
of Cp,ε and Cε,q. We can then include the additional parts of CBP in the Sinkhorn algorithm (Eq. (2))
via
KBPt
Kt + Cp,ε Θ t
Cε,q Θ t + 1Tt
KBTP s =
KTs + Cε,q Θ S
cp,ε θ 5 + ImS
(20)
where t denotes the upper and t the lower part of the vector t. To calculate dC we can decompose the
transport plan PBP in the same way as CBP, with a single scalar for Pε,ε . For GTN we obtain the
deletion cost via ci,ε = kα Θ xpik2, with a learnable vector α ∈ Rd.
Multi-head OT. Inspired by attention models (Vaswani et al., 2017) we further improve GTN by
using multiple OT heads. Using K heads means that we calculate OT in parallel for K separate sets of
embeddings representing the same pair of objects and obtain a set of distances dcλ ∈ RK. We can then
transform these distances to a final distance prediction using a set of linear layers h(ik) = W (k) hifinal
for head k and obtain the final prediction via d = MLP(dcλ). Note that both learnable unbalanced
OT and multi-head OT might be of independent interest.
6	Related Work
Log-linear optimal transport. For an overview of optimal transport and its foundations see Peyre
& Cuturi (2019). On low-dimensional grids and surfaces OT can be solved using dynamical OT (Pa-
padakis et al., 2014; Solomon et al., 2014), convolutions (Solomon et al., 2015), or embedding/hashing
schemes (Indyk & Thaper, 2003; Andoni et al., 2008). In higher dimensions we can use tree-based
algorithms (Backurs et al., 2020) or hashing schemes (Charikar, 2002), which are however limited to
a previously fixed set of points Xp, Xq, on which only the distributions p and q change. For sets that
change dynamically (e.g. during training) one common method of achieving log-linear runtime is a
multiscale approximation of entropy-regularized OT (Schmitzer, 2019; Gerber & Maggioni, 2017).
Tenetov et al. (2018) recently proposed using a low-rank approximation of the Sinkhorn similarity
6
Under review as a conference paper at ICLR 2021
Table 1: Mean and standard deviation (w.r.t. last digits, in parentheses) of relative Sinkhorn distance
error, IoU of top 0.1 % and correlation coefficient (PCC) of OT plan entries across 5 runs. Sparse
Sinkhorn and LCN-Sinkhorn consistently achieve the best approximation in all 3 measures.
EN-DE	EN-ES	EN-FR	EN-RU
ReL err. dλ	PCC IoUReL err. dλ~~PCC	IoUReL err. dλ~~PCC	IoURel. err. dλ~PCC	IoU
Factored OT	0.318(1)~~0.044(1)^^0.019(2)^^0.332(1) 0.037(2) 0.026(5)^^0.326(2) 0.038(1) 0.034(5)~~0.281(2) 0.055(1) 0.025(2)
Multiscale OT	0.634(11)	0.308(14)	0.123(5)	0.645(14)	0.321(6)	0.125(12)	0.660(17)	0.330(9)	0.121(7)	0.667(16)	0.281(19)	0.125(9)
Nystrom Skh.	1.183(5)	0.077(1)	0.045(5)	1.175(18)	0.068(“	0.048(6)	1.172(13)	0.070(3)	0.052(4)	1.228(18)	0.091(2)	0.047$)
Sparse Skh.	0.233(2)	0.552(4)	0.102(1)	0.217(1)	0.623(4)	0.102(1)	0.220(1)	0.608(5)	0.104(2)	0.272(2)	0.446(8)	0.090(1)
LCN-Sinkhorn	0.406(15)	0.673(12)	0.197(7)	0.368(12)	0.736(3)	0.201(3)	0.342(5)	0.725(4)	0.209(3)	0.465(10)	0.623(5)	0.210(4)
matrix obtained via a semidiscrete approximation of the Euclidean distance. Altschuler et al. (2019)
improved upon this approach by using the Nystrom method for the approximation. These approaches
still struggle with high-dimensional real-world problems, as we will show in Sec. 7.
Sliced Wasserstein distance. Another approach to reduce the computational complexity of optimal
transport (without entropy regularization) are sliced Wasserstein distances (Rabin et al., 2011).
However, they require the L2 distance as a cost function and are either unstable in convergence or
prohibitively expensive for high-dimensional problems (O(nd3)) (Meng et al., 2019).
Fast Sinkhorn. Another line of work has been pursuing accelerating entropy-regularized OT without
changing its computational complexity w.r.t. the number of points. Original Sinkhorn requires
O(1∕ε2) iterations (Dvurechensky et al., 2018) and Jambulapati et al. (2019) recently proposed an
algorithm that reduces them to O (1 /ε). Alaya et al. (2019) proposed to reduce the size of the Sinkhorn
problem by screening out neglectable components, which allows for approximation guarantees.
Genevay et al. (2016) proposed using a stochastic optimization scheme instead of Sinkhorn iterations.
Essid & Solomon (2018) and Blondel et al. (2018) proposed alternative regularizations to obtain OT
problems with similar runtimes as the Sinkhorn algorithm. This work is largely orthogonal to ours.
Embedding alignment. For an overview of cross-lingual word embedding models see Ruder et al.
(2019). Unsupervised word embedding alignment was proposed by Conneau et al. (2018), with
subsequent advances by Alvarez-Melis & Jaakkola (2018); Grave et al. (2019); Joulin et al. (2018).
Graph matching and distance learning. Most recent approaches for graph matching and graph
distance learning either rely on a single fixed-dimensional graph embedding (Bai et al., 2019; Li
et al., 2019), or only use attention or some other strongly simplified variant of optimal transport
(Bai et al., 2019; Riba et al., 2018; Li et al., 2019). Others break permutation invariance and are
thus ill-suited for this task (Ktena et al., 2017; Bai et al., 2018). So far only approaches using a
single graph embedding allow faster than quadratic scaling in the number of nodes. Compared to the
Sinkhorn-based image model concurrently proposed by Wang et al. (2019) GTN uses no CNN or
cross-graph attention, but an enhanced GNN and embedding aggregation scheme. OT has recently
been proposed for graph kernels (Maretic et al., 2019; Vayer et al., 2019), which can (to a limited
extent) be used for graph matching, but not for distance learning.
7	Experiments
Approximating Sinkhorn. We start by directly investigating different Sinkhorn approximations.
To do so we compute entropy-regularized OT on pairs of 10 000 word embeddings from Conneau
et al. (2018), which we preprocess with Wasserstein Procrustes alignment in order to obtain both
close and distant neighbors. We let every method use the same total number of 40 neighbors and
landmarks (LCN uses 20 each) and set λ = 0.05 (as in Grave et al. (2019)). We measure transport
plan approximation quality by (a) calculating the Pearson correlation coefficient (PCC) between all
entries in the approximated卫lan and the true P and (b) comparing the sets of 0.1 % largest entries in
the approximated and true P using the Jaccard similarity (intersection over union, IoU). In all figures
the error bars denote standard deviation across 5 runs, which is often too small to be visible.
Table 1 shows that both sparse Sinkhorn, LCN-Sinkhorn and factored OT (Forrow et al., 2019)
obtain distances that are significantly closer to the true dcλ than Multiscale OT and Nystrom-Sinkhorn.
Furthermore, the transport plan computed by sparse Sinkhorn and LCN-Sinkhorn show both a PCC
and IoU that are around twice as high as Multiscale OT, while Nystrom-Sinkhorn and factored OT
exhibit almost no correlation. LCN-Sinkhorn performs especially well in this regard. This is also
7
Under review as a conference paper at ICLR 2021
Runtime (ms)
Figure 2: Tradeoff between OT Figure 3: Tradeoff between OT Figure 4: OT plan approximation
plan approximation (via PCC) plan approximation and number quality for varying entropy reg-
and runtime. Sparse Sinkhorn of- of neighbors/landmarks. LCN- ularization λ. Sparse Sinkhorn
fers the best tradeoff, with LCN- Sinkhorn achieves the best ap- performs best for low and LCN-
Sinkhorn trailing closely behind. proximation for low and sparse Sinkhorn for moderate and fac-
The arrow indicates factored OT Sinkhorn for high budgets.	tored OT for very high λ.
results far outside the range.
Table 2: Accuracy and standard deviation (w.r.t. last digits, in parentheses) across 5 runs for unsuper-
vised word embedding alignment with Wasserstein Procrustes. LCN-Sinkhorn improves upon the
original by 3.1 pp. before and 2.0 pp. after iterative CSLS refinement. *Migrated and re-run on GPU via PyTorch
	Time (s)	EN-ES	ES-EN	EN-FR	FR-EN	EN-DE	DE-EN	EN-RU	RU-EN	Avg.
Original*	268	79.2(2)	78.8(2.8)	81.0⑶	79.4(9)	71.7(2)	65.7(3.4)	36.3(1.1)	51.1(1.1)	67.9
Full Sinkhorn	402	81.1()	82.0()	81.2()	81.3()	74.1()	70.7()	37.3()	53.5()	70.1
Multiscale OT	88.2	23.6(31.4) 74.7(3.3) 26.9(31.7) 6.3(4.4) 35.8(10.4) 47.0(20.5)						0.0()	0.2(1)	26.8
Nystrom Skh.	102	64.4(1.0)	59.3(1.2)	64.1(1.6) 56.8(4.0)		54.1(6)	47.1(3.5)	14.1(1.2)	22.5(2.4)	47.8
Sparse Skh.	49.2	80.2(2)	81.7(4)	80.9(3)	80.1(2)	72.1(6)	65.1(1.7)	35.5(6)	51.5(4)	68.4
LCN-Sinkhorn	86.8	81.8(2)	81.3(1.8)	82.0(4)	82.1(3)	73.6(2)	71.3(9)	41.0(8)	55.1(1.4) 71.0	
Original* + ref.	268+81	83.0(3)	82.0(2.5)	83.8(1)	83.0(4)	77.3(3)	69.7(4.3)	46.2(1.0)	54.0(1.1)	72.4
LCN-Skh. + ref. 86.8+81		83.5(2)	83.1(1.3)	83.8(2)	83.6(1)	77.2(3)	72.8(7)	51.8(2.6) 59.2(1.9) 74.4		
evident in Fig. 1, which shows how the 104 × 104 approximated OT plan entries compared to the
true Sinkhorn values.
Fig. 2 shows that sparse Sinkhorn offers the best trade-off between runtime and OT plan qual-
ity. Factored OT exhibits a runtime 2 to 10 times longer than the competition due to its iterative
refinement scheme. LCN-Sinkhorn performs best for use cases with constrained memory (few neigh-
bors/landmarks), as shown in Fig. 3. The number of neighbors and landmarks directly determines
memory usage and is linearly proportional to the runtime (see App. K). Fig. 9 shows that sparse
Sinkhorn performs best for low regularizations, where LCN-Sinkhorn fails due to the NyStrom part
going out of bounds. Nystrom Sinkhorn performs best at high values and LCN-Sinkhorn always
performs better than both (as long as it can be calculated). Interestingly, all approximations except
factored OT seem to fail at high λ. We defer analogously discussing the distance approximation to
App. L. All approximations scale linearly both in the number of neighbors/landmarks and dataset
size, as shown in App. K. Overall, we see that sparse Sinkhorn and LCN-Sinkhorn yield significant
improvements over previous approximations. However, do these improvements also translate to better
performance on downstream tasks?
Embedding alignment. Embedding alignment is the task of finding the orthogonal matrix R ∈ Rd×d
that best aligns the vectors from two different embedding spaces, which is e.g. useful for unsupervised
word translation. We use the experimental setup established by Conneau et al. (2018) by migrating
Grave et al. (2019)’s implementation to PyTorch. The only change we make is using the full set
of 20 000 word embeddings and training for 300 steps, while reducing the learning rate by half
every 100 steps. We do not change any other hyperparameters and do not use unbalanced OT. After
training we match pairs via cross-domain similarity local scaling (CSLS) (Conneau et al., 2018). We
use 10 Sinkhorn iterations, 40 neighbors for sparse Sinkhorn, and 20 neighbors and landmarks for
LCN-Sinkhorn (for details see App. H). We allow both multiscale OT and Nystrom Sinkhorn to use
as many landmarks and neighbors as can fit into GPU memory and finetune both methods.
Table 2 shows that using full Sinkhorn yields a significant improvement in accuracy on this task
8
Under review as a conference paper at ICLR 2021
Table 3: RMSE for GED regression across
3 runs and the targets’ standard deviation σ.
GTN outperforms previous models by 48 %.
Table 4: RMSE for graph distance regression across
3 runs. Using LCN-Sinkhorn with GTN increases
the error by only 10 % and allows log-linear scaling.
	Linux	AIDS30	Pref. att.	GED			PM [10-2]
σ	0.184^^	16.2	48.3		AIDS30	Pref. att.	Pref. att. 200
SiamMPNN	0.090(7)	13.8(3)	12.1(6)	σ	~^16.2^^	48.3	10.2
SimGNN	0.039	4.5(3)	8.3(1.4)	Full Sinkhorn	3.7(1)	4.5(3)	1.27(6)
GMN	0.015()	10.3(6)	7.8(3)	NystrOm Skh.	3.6(3)	6.2(6)	2.43(7)
GTN, 1head 0.022(1)		3.7(1)	4.5(3)	Multiscale OT	11.2(3)	27.4(5.4)	6.71(44)
8 OT heads	0.012(1)	3.2(1)	3.6(2)	Sparse Skh.	44.0(30.4) 40.7(8.1)		7.57(1.09)
Balanced OT	0.034(1)	15.3(1)	27.4(9)	LCN-Skh.	4.0(1)	5.1(4)	1.41(15)
compared to the original approach of performing Sinkhorn on randomly sampled subsets of embed-
dings (Grave et al., 2019). LCN-Sinkhorn even outperforms the full version in most cases, which is
likely due to regularization effects from the approximation. It also runs 4.6x faster than full Sinkhorn
and 3.1x faster than the original scheme. Sparse Sinkhorn runs 1.8x faster than LCN-Sinkhorn but
cannot match its accuracy. LCN-Sinkhorn still outcompetes the original method after refining the
embeddings with iterative local CSLS (ConneaU et al., 2018). Both multiscale OT and NystrOm
Sinkhorn fail at this task, despite their larger computational budget. This shows that the improvements
achieved by sparse Sinkhorn and LCN-Sinkhorn have an even larger impact in practice.
Graph distance regression. The graph edit distance (GED) is useful for various tasks, such as image
retrieval (Xiao et al., 2008) or fingerprint matching (Neuhaus & Bunke, 2004), but its computation
is NP-complete (Bunke & Shearer, 1998). Therefore, to use it on larger graphs we need to learn
an approximation. We use the Linux dataset by Bai et al. (2019) and generate 2 new datasets by
computing the exact GED using the method by Lerouge et al. (2017) on small graphs (≤ 30 nodes)
from the AIDS dataset (Riesen & Bunke, 2008) and a set of preferential attachment graphs. We
compare GTN to 3 state-of-the-art baselines: SiameseMPNN (Riba et al., 2018), SimGNN (Bai et al.,
2019), and the Graph Matching Network (GMN) (Li et al., 2019). We tune the hyperparameters of all
baselines and GTN on the validation set via a grid search. For more details see App. H to J.
We first test both GTN and the proposed OT enhancements. Table 3 shows that GTN improves upon
competing models by 20 % with a single head and by 48 % with 8 OT heads. These improvements
break down when using regular balanced OT, showing the importance of learnable unbalanced OT.
Having established GTN as a state-of-the-art model we next ask whether we can sustain its per-
formance when using approximate OT. To test this we additionally generate a set of larger graphs
with around 200 nodes and use the Pyramid matching (PM) kernel (Nikolentzos et al., 2017) as the
prediction target, since these graphs are too large to compute the GED. See App. J for hyperparameter
details. Table 4 shows that both sparse Sinkhorn and the multiscale method using 4 (expected) neigh-
bors fail at this task, demonstrating that the low-rank approximation in LCN has a crucial stabilizing
effect during training. NystrOm Sinkhorn with 4 landmarks performs surprisingly well on the AIDS30
dataset, suggesting an overall low-rank structure with NystrOm acting as regularization. However,
it does not perform as well on the other two datasets. Using LCN-Sinkhorn with 2 neighbors and
landmarks works well on all three datasets, with an RMSE increased by only 10 % compared to full
GTN. App. K furthermore shows that GTN with LCN-Sinkhorn indeed scales linearly in the number
of nodes across multiple orders of magnitude. This model thus allows to perform graph matching and
distance learning on graphs that are considered large even for simple node-level tasks (20 000 nodes).
8 Conclusion
Locality sensitive hashing (LSH) and the novel locally corrected NystrOm (LCN) method enable fast
and accurate approximations of entropy-regularized OT with log-linear runtime: Sparse Sinkhorn
and LCN-Sinkhorn. The graph transport network (GTN) is one example for such a model, which can
be substantially improved with learnable unbalanced OT and multi-head OT. It sets the new state of
the art for graph distance learning while still scaling log-linearly with graph size. These contributions
enable new applications and models that are both faster and more accurate, since they can sidestep
workarounds such as pooling.
9
Under review as a conference paper at ICLR 2021
References
Pierre Ablin, Gabriel Peyra and Thomas Moreau. SUPer-efficiency of automatic differentiation for
functions defined as a minimum. In ICML, 2020.
Mokhtar Z. Alaya, Maxime Berar, Gilles Gasso, and Alain Rakotomamonjy. Screening Sinkhorn
Algorithm for Regularized OPtimal TransPort. In NeurIPS, 2019.
Jason Altschuler, Francis Bach, Alessandro Rudi, and Jonathan Niles-Weed. Massively scalable
Sinkhorn distances via the Nystrom method. In NeurIPS, 2019.
David Alvarez-Melis and Tommi S. Jaakkola. Gromov-Wasserstein Alignment of Word Embedding
SPaces. In EMNLP, 2018.
Alexandr Andoni, Piotr Indyk, and Robert Krauthgamer. Earth mover distance over high-dimensional
sPaces. In ACM-SIAM symposium on Discrete algorithms (SODA), 2008.
Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya P. Razenshteyn, and Ludwig Schmidt. Practical
and OPtimal LSH for Angular Distance. In NeurIPS, 2015.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein Generative Adversarial Networks.
In ICML, 2017.
Arturs Backurs, Yihe Dong, Piotr Indyk, Ilya Razenshteyn, and Tal Wagner. Scalable Nearest
Neighbor Search for OPtimal TransPort. In ICML, 2020.
Yunsheng Bai, Hao Ding, Yizhou Sun, and Wei Wang. Convolutional Set Matching for GraPh
Similarity. In NeurIPS-W, 2018.
Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang. SimGNN: A Neural
Network APProach to Fast GraPh Similarity ComPutation. In WSDM, 2019.
Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or ProPagating Gradients
Through Stochastic Neurons for Conditional ComPutation. CoRR, 1308.3432, 2013.
Christian Berg, Jens Peter Reus Christensen, and Paul Ressel. Harmonic Analysis on Semigroups.
Number 100 in Graduate Texts in Mathematics. 1984.
Kristian Birchall, Valerie J. Gillet, Gavin HarPer, and StePhen D. Pickett. Training Similarity
Measures for SPecific Activities: APPlication to Reduced GraPhs. Journal of Chemical Information
andModeling, 46(2):577-586, 2006.
Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and SParse OPtimal TransPort. In
AISTATS, 2018.
Olivier Bousquet, Sylvain Gelly, Ilya Tolstikhin, Carl-Johann Simon-Gabriel, and Bernhard
SchoelkoPf. From oPtimal transPort to generative modeling: the VEGAN cookbook. arXiv,
1705.07642, 2017.
Horst Bunke and Kim Shearer. A graPh distance metric based on the maximal common subgraPh.
Pattern Recognition Letters, 19(3):255-259, 1998.
Moses Charikar. Similarity estimation techniques from rounding algorithms. In ACM symposium on
Theory of computing (STOC), 2002.
LenaiC Chizat, Gabriel Peyre, Bernhard Schmitzer, and FrangOiS-XaVier Vialard. Scaling algorithms
for unbalanced oPtimal transPort Problems. Mathematics of Computation, 87(314):2563-2609,
2018.
Henry Cohn. A ConcePtual Breakthrough in SPhere Packing. Notices of the American Mathematical
Society, 64(02):102-115, 2017.
Alexis Conneau, Guillaume LamPle, Marc’Aurelio Ranzato, Ludovic Denoyer, and Herve Jegou.
Word translation without Parallel data. In ICLR, 2018.
10
Under review as a conference paper at ICLR 2021
Nicolas Courty, Remi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution
optimal transportation for domain adaptation. In NeurIPS, 2017.
Marco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In NeurIPS,
2013.
Pavel E. Dvurechensky, Alexander Gasnikov, and Alexey Kroshnin. Computational Optimal Trans-
port: Complexity by Accelerated Gradient Descent Is Better Than by Sinkhorn’s Algorithm. In
ICML, 2018.
Montacer Essid and Justin Solomon. Quadratically Regularized Optimal Transport on Graphs. SIAM
Journal on Scientific Computing, 40(4):A1961-A1986, 2018.
Matthias Fey and Jan E. Lenssen. Fast Graph Representation Learning with PyTorch Geometric. In
ICLR-W, 2019.
Aden Forrow, Jan-Christian Hutter, Mor Nitzan, Philippe Rigollet, Geoffrey Schiebinger, and Jonathan
Weed. Statistical Optimal Transport via Factored Couplings. In AISTATS, 2019.
Charlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio Araya-Polo, and Tomaso A. Poggio.
Learning with a Wasserstein Loss. In NeurIPS, 2015.
Aude Genevay, Marco Cuturi, Gabriel Peyre, and Francis R. Bach. Stochastic Optimization for
Large-scale Optimal Transport. In NeurIPS, 2016.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning Generative Models with Sinkhorn
Divergences. In AISTATS, 2018.
Samuel Gerber and Mauro Maggioni. Multiscale Strategies for Computing Optimal Transport. J.
Mach. Learn. Res.,18:72:1-72:32, 2017.
Edouard Grave, Armand Joulin, and Quentin Berthet. Unsupervised Alignment of Embeddings with
Wasserstein Procrustes. In AISTATS, 2019.
Paul L. Houston, Apurba Nandi, and Joel M. Bowman. A Machine Learning Approach for Prediction
of Rate Constants. The Journal of Physical Chemistry Letters, 10(17):5250-5258, 2019.
Piotr Indyk and Nitin Thaper. Fast image retrieval via embeddings. In ICCV-W, 2003.
Arun Jambulapati, Aaron Sidford, and Kevin Tian. A Direct tilde{O}(1/epsilon) Iteration Parallel
Algorithm for Optimal Transport. In NeurIPS, 2019.
Justin Johnson, Ranjay Krishna, Michael Stark, Li-Jia Li, David A. Shamma, Michael S. Bernstein,
and Fei-Fei Li. Image retrieval using scene graphs. In CVPR, 2015.
Armand Joulin, Piotr Bojanowski, Tomas Mikolov, Herve Jegou, and Edouard Grave. Loss in
Translation: Learning Bilingual Word Mapping with a Retrieval Criterion. In EMNLP, 2018.
Sofia Ira Ktena, Sarah Parisot, Enzo Ferrante, Martin Rajchl, Matthew Lee, Ben Glocker, and
Daniel Rueckert. Distance Metric Learning Using Graph Convolutional Networks: Application
to Functional Brain Networks. In MICCAI, 2017.
Julien Lerouge, Zeina Abu-Aisheh, Romain Raveaux, Pierre Heroux, and Sebastien Adam. New
binary linear programming formulation to compute the graph edit distance. Pattern Recognit., 72:
254-265, 2017.
Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. Graph Matching Networks
for Learning the Similarity of Graph Structured Objects. In ICML, 2019.
Hermina Petric Maretic, Mireille El Gheche, Giovanni Chierchia, and Pascal Frossard. GOT: An
Optimal Transport framework for Graph comparison. In NeurIPS, 2019.
Cheng Meng, Yuan Ke, Jingyi Zhang, Mengrui Zhang, Wenxuan Zhong, and Ping Ma. Large-scale
optimal transport map estimation using projection pursuit. In NeurIPS, 2019.
11
Under review as a conference paper at ICLR 2021
Cameron Musco and Christopher Musco. Recursive Sampling for the Nystrom Method. In NeurIPS,
2017.
Michel Neuhaus and Horst Bunke. An Error-Tolerant Approximate Matching Algorithm for Attributed
Planar Graphs and Its Application to Fingerprint Classification. In Structural, Syntactic, and
Statistical Pattern Recognition, 2004.
Giannis Nikolentzos, Polykarpos Meladianos, and Michalis Vazirgiannis. Matching Node Embed-
dings for Graph Similarity. In AAAI, 2017.
David Nister and Henrik SteWenius. Scalable Recognition with a Vocabulary Tree. In CVPR, 2006.
Nicolas Papadakis, Gabriel Peyre, and Edouard Oudet. Optimal Transport with Proximal Splitting.
SIAMJ. Imaging Sciences,7(1):212-238, 2014.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An Imperative Style, High-Performance
Deep Learning Library. In NeurIPS, 2019.
Loic Pauleve, Herve J6gou, and Laurent Amsaleg. Locality sensitive hashing: A comparison of hash
function types and querying mechanisms. Pattern Recognit. Lett., 31(11):1348-1358, 2010.
Allon G Percus and Olivier C Martin. Scaling Universalities ofkth-Nearest Neighbor Distances on
Closed Manifolds. Advances in Applied Mathematics, 21(3):424-436, 1998.
Gabriel Peyre and Marco Cuturi. Computational Optimal Transport. Foundations and Trends in
Machine Learning, 11(5-6):355-607, 2019.
Julien Rabin, Gabriel Peyre, Julie Delon, and Marc Bernot. Wasserstein Barycenter and Its Application
to Texture Mixing. In Scale Space and Variational Methods in Computer Vision (SSVM), 2011.
Pau Riba, Andreas Fischer, Josep Llad6s, and Alicia Fornes. Learning Graph Distances with Message
Passing Neural Networks. In ICPR, 2018.
Kaspar Riesen and Horst Bunke. IAM Graph Database Repository for Graph Based Pattern Recogni-
tion and Machine Learning. In Structural, Syntactic, and Statistical Pattern Recognition, 2008.
Kaspar Riesen and Horst Bunke. Approximate graph edit distance computation by means of bipartite
graph matching. Image Vis. Comput., 27(7):950-959, 2009.
Sebastian Ruder, Ivan Vulic, and Anders S0gaard. A Survey of Cross-lingual Word Embedding
Models. J. Artif. Intell. Res., 65:569-631, 2019.
Bernhard Schmitzer. Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport
Problems. SIAM Journal on Scientific Computing, 41(3):A1443-A1481, 2019.
Anshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for Sublinear Time Maximum Inner
Product Search (MIPS). In NeurIPS, 2014.
Justin Solomon, Raif M. Rustamov, Leonidas J. Guibas, and Adrian Butscher. Earth mover’s distances
on discrete surfaces. ACM Trans. Graph., 33(4):67:1-67:12, 2014.
Justin Solomon, Fernando de Goes, Gabriel Peyre, Marco Cuturi, Adrian Butscher, Andy Nguyen, Tao
Du, and Leonidas J. Guibas. Convolutional wasserstein distances: efficient optimal transportation
on geometric domains. ACM Trans. Graph., 34(4):66:1-66:11, 2015.
Robert E. Tarjan. Dynamic trees as search trees via euler tours, applied to the network simplex
algorithm. Mathematical Programming, 78(2):169-177, 1997.
Evgeny Tenetov, Gershon Wolansky, and Ron Kimmel. Fast Entropic Regularized Optimal Transport
Using Semidiscrete Cost Approximation. SIAM J. Sci. Comput., 40(5):A3400-A3422, 2018.
12
Under review as a conference paper at ICLR 2021
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is All you Need. In NeurIPS, 2017.
TitoUan Vayer, Nicolas Courty, Romain Tavenard, Laetitia Chapel, and Remi Flamary. Optimal
Transport for structured data with application on graphs. In ICML, 2019.
Jingdong Wang, Heng Tao Shen, Jingkuan Song, and Jianqiu Ji. Hashing for Similarity Search: A
Survey. CoRR, 1408.2927, 2014.
Runzhong Wang, Junchi Yan, and Xiaokang Yang. Learning Combinatorial Embedding Networks
for Deep Graph Matching. In ICCV, 2019.
Christopher K. I. Williams and Matthias Seeger. Using the Nystrom Method to Speed UP Kernel
Machines. In NeurIPS, 2001.
Bing Xiao, Xinbo Gao, Dacheng Tao, and Xuelong Li. HMM-based graph edit distance for image
indexing. Int. J. Imaging Systems and Technology, 18(2-3):209-218, 2008.
Kai Zhang, Ivor W. Tsang, and James T. Kwok. Improved Nystrom low-rank approximation and
error analysis. In ICML, 2008.
13
Under review as a conference paper at ICLR 2021
A Complexity Analysis
Sparse Sinkhorn. A common way of achieving a high p1 and low p2 in LSH is via the AND-OR
construction. In this scheme We calculate B ∙ r hash functions, divided into B sets (hash bands) of r
hash functions each. A pair of points is considered as neighbors if any hash band matches completely.
Calculating the hash buckets for all points With b hash buckets per function scales as O((n+m)dBbr)
for the hash functions We consider. As expected, for the tasks and hash functions We investigated
We obtain approximately m/br and n/br neighbors, With br hash buckets per band. Using this We
can fix the number of neighbors to a small, constant β in expectation with br = min(n, m)∕β. We
thus obtain a sparse cost matrix Csp With O(max(n, m)β) non-infinite values and can calculate s
and t in linear time O(NSink max(n, m)β), where NSink ≤ 2 +---j-ij ε~~ɪ ij{pi,qj})
(see Theorem 4) denotes the number of Sinkhorn iterations. Calculating the hash buckets with
r = log min(ngm)-logβ takes O((n + m)dBb(logmin(n, m) 一 log β)/ log b). Since B, b, and β are
small, we obtain roughly log-linear scaling with the number of points overall, i.e. O(n log n) for
n ≈ m.
LCN-Sinkhorn. Both choosing landmarks via k-means++ sampling and via k-means with a fixed
number of iterations have the same runtime complexity of O((n + m)ld). Precomputing W can be
done in time O(nl2 + l3). The low-rank part of updating the vectors s and t can be computed in
O(nl + l2 + lm), with l chosen constant, i.e. independently of n and m. Since sparse Sinkhorn with
LSH has a log-linear runtime we again obtain log-linear overall runtime for LCN-Sinkhorn.
B	Limitations
Sparse Sinkhorn. Using a sparse approximation for K works well in the common case when the
regularization parameter λ is low and the cost function varies enough between data pairs, such that
the transport plan P resembles a sparse matrix. However, it can fail if the cost between pairs is very
similar or the regularization is very high, if the dataset contains many hubs, i.e. points with a large
number of neighbors, or if the distributions p or q are spread very unevenly. Furthermore, sparse
Sinkhorn can be too unstable to train a model from scratch, since randomly initialized embeddings
often have no close neighbors (see Sec. 7). LCN-Sinkhorn largely alleviates these limitations.
LCN-Sinkhorn. Since we cannot calculate the full cost matrix, LCN-Sinkhorn cannot provide
accuracy guarantees in general. Highly concentrated distributions p and q might have adverse effects
on LCN-Sinkhorn. However, we can compensate for these by sampling landmarks or neighbors
proportional to each point’s probability mass. We therefore do not expect LCN-Sinkhorn to break
down in this scenario. If the regularization parameter is low or the cost function varies greatly, we
sometimes observed stability issues (over- and underflows) with the Nystrom approximation because
of the inverse A-1, which cannot be calculated in log-space. Due to its linearity the Nystrom method
furthermore sometimes approximates similarities as negative values, which leads to a failure if the
result of the matrix product in Eq. (2) becomes negative. In these extreme cases we also observed
catastrophic elimination caused by the correction K∆sp. Since this essentially means that optimal
transport will be very local, we recommend using sparse Sinkhorn in these scenarios. This again
demonstrates the complementarity of the sparse approximation and Nystrom: In cases where one
fails we can often resort to the other.
C Proof of Theorem 1
We first prove a lemma that will be useful later on.
Lemma A. Let K be the Nystrom approximation ofthe similarity matrix Kij = e-kxi-xj k2/', Let
xi and xj be data points with equal L2 distance ri and rj to all l landmarks, which have the same
distance ∆ to each other. Then
l	le-(ri+rj "λ
Kii = 1 + (l- 1)e-∆∕λ	QI)
Proof. The inter-landmark distance matrix is
A = e-∆∕λ1l×l + (1 一 e-∆∕λ)Il,	(22)
14
Under review as a conference paper at ICLR 2021
where 1l×l denotes the constant 1 matrix. Using the identity
(b1n×n +	(a	- b)In)-1	=	(a	- b)(a+(n -	1)b)	1n×n	+ 占In	(23)
we compute
Kij = Ui-.A-1V,j
/ e-rj/λ∖
=(e-r,∕λ ef∕λ ..)(________________-e-△/“____________1l×l + ___1____Il)	e-rj/λ
(	(∖(1 - e-aλ)(1 + (l - 1)e-aλ)	1 - e-aλ J .
..
e-(ri +rj)∕λ	-l2e-∆∕λ	e-(ri +rj)∕λ	l - le-∆∕λ
1 - e-&λ (l + (l - 1)e-aλ + 7	1 - c-δ6 1 + (l - 1)e-aλ
le-(ri +rj)∕λ
1 + (l — 1)c-δ”
(24)
□
Now consider the error kKi,: - KLCN,i,: k∞. The k - 1 nearest neighbors are covered by the sparse
correction and therefore the next nearest neighbor has distance δk . The expected distance from the
closest landmark is greater than the expected distance inside the surrounding d-ball of radius R, i.e.
E[r] ≥ EV(R)[r] = -^+^R. Because furthermore δk《R, the error is dominated by the first term
and the maximum error in row i is given by the k-nearest neighbor of i, denoted by j . Thus
E[kKi,: - Klcn"∣∞]= E[Ki,j- KLCN,i,j] = E[Ki,j] - E[KLCN,i,j] = E[e-δk∕λ] - E[KL
CN,i,j]
(25)
Note that we can lower bound the first term using Jensen’s inequality. However, we were unable to
find a reasonably tight upper bound and the resulting integral (ignoring exponentially small boundary
effects, see Percus & Martin (1998))
n!
(n - k)!(k - 1)!
((d/2)!)1/d
e-"λV(r)k-1(1 - V(r))n-kdV(r) dr,
(26)
0
with the volume of the d-ball
V(r)
πd∕2rd
TW
(27)
does not have an analytical solution. We thus have to resort to calculating this expectation numerically.
We lower bound the second term by (1) ignoring every landmark except the closest one, since
additional landmarks can only increase the estimate KLCN,i,j . We then (2) upper bound the L2
distance to the closest landmark r by √dR∕2, since this would be the furthest distance to the closest
point in a d-dimensional grid. Any optimal arrangement minimizing E[miny∈L kx - yk2 | x ∈ Xp]
would be at least as good as a grid and thus have furthest distances as small or smaller than those in a
grid. Thus,
r	[ (1) O /ʌ (2)	√1-D I∖
E[KLCN,i,j] ≥ e-2r∕λ ≥ e-VdR/A.	(28)
We upper bound this expectation by considering that any point outside the inscribed sphere of the
space closest to a landmark (which has radius R) would be further away from the landmarks and thus
have a lower value e-d∕λ. We can therefore reduce the space over which the expectation is taken to
the ball with radius R, i.e.
E[KLCN,i,j] ≤ EV (R) [KLCN,i,j]	(29)
Next we (1) ignore the contributions of all landmarks except for the closest 2, since a third landmark
must be further away from the data point than √3R, adding an error of O(e-2v^R/A). We then (2)
15
Under review as a conference paper at ICLR 2021
lower bound the distances of both points to both landmarks by the closest distance to a landmark
r = min{kxi - xl1 k2, kxi - xl2k2, kxj - xl1 k2, kxj - xl2 k2} and use Lemma A to obtain
EV(R)KLCN,i,j] = EV(R) [KLCN, 2 landmarks,i,j] + O(e-2√3R∕,
(2)
≤ EV (R)
2e-2r∕λ
1 + e-2R∕λ
+ O(e-2√3R")
(30)
= 2EVr⅛RrP + O(e-2√3R∕λ).
1 + e-2R∕λ
Assuming Euclideanness in V (R) we obtain
EV(Ri"] = V(R) Z0 L"竽 dr = R	"
= T2R‰ (Γ(d) - Γ(d, 2R∕λ))
(2R/A)
(31)
□
D Proof of Theorem 2
Note that this theorem does not use probabilistic arguments but rather geometrically analyzes the
maximum possible error. Ksp is correct for all pairs inside a cluster and 0 otherwise. We therefore
obtain the maximum error by considering the closest possible pair between clusters. By definition,
this pair has distance D and thus
max IIK — Ksp∣∣∞ = e-D∕λ
(32)
LCN is also correct for all pairs inside a cluster, so we again consider the closest possible pair xi , xj
between clusters. We furthermore only consider the landmarks of the two concerned clusters, adding
an error of O(e-D∕λ). Hence,
1	e-(2r+D)∕λ
KLCN, 2 landmarks,ij = (e r" e (r+°”') ( -(2r+D)∕λ	]
1 - e—(4r+2D)∕λ
1
-e-(2r+D)∕λ
e-(r+D)∕λ - e
1 - e. —(4r+2D)∕λ
1
e-(r+D)∕λ∖
e-r∕λ )
-e-(2r+D)∕λ
-(3r+D)∕λ)
e-r∕λ - e-(3r+2D)∕λ
e-(r+D)∕λ
e-r∕λ
1 _ e-(4r+2D)∕λ
e-(2r+D)∕λ
1 _ e-(4r+2D)∕λ
(e-(2r+D)∕λ _ e-(4r+D)∕λ + e-(2r+D)∕λ _ e-(4r+3D)∕λ)
(2 - e-2r∕λ - e-(2r+2D)∕λ)
1
= e-D∕λe-2r∕λ(2 - e-2r∕λ) - O(e-2D∕λ)
and thus
max IK _ KLCNI∞ = e-D∕λ(1 _ e-2r∕λ(2 _ e-2r∕λ) + O(e-D∕λ)).
(33)
(34)
For pure Nystrom We need to consider the distances inside a cluster. In the worst case two points
overlap, i.e. Kij = 1, and lie at the boundary of the cluster. Since r D we again only consider
the landmarks in the concerned cluster, adding an error of O(e-D∕λ). Because of symmetry we can
optimize the worst-case distance from all landmarks by putting them on a (l _ 1)-simplex centered on
the cluster center. Since there are at most d landmarks in each cluster there is always one direction in
which the worst-case points are r away from all landmarks. The circumradius of an (l _ 1)-simplex
with side length ∆ is Jl-1 ∆. Thus, the maximum distance to all landmarks is Jr2 + l-1 ∆2.
Using Lemma A we therefore obtain the Nystrom approximation
l -2 √r2 + ⅛1 ∆ ∕λ
F= lιe+(i _ i)e-∆∕λ +OD
(35)
□
16
Under review as a conference paper at ICLR 2021
E	Note on Theorem 3
Lemmas C-F and and thus Theorem 1 by Altschuler et al. (2019) are also valid for Q outside the
simplex so long as kQk1 = n and it only has non-negative entries. Any P returned by Sinkhorn
fulfills these conditions. Therefore the rounding procedure given by their Algorithm 4 is not necessary
for this result.
Furthermore, to be more consistent with Theorems 1 and 2 we use the L2 distance instead of L22 in
this theorem, which only changes the dependence on ρ.
F Notes on Theorem 4
To adapt Theorem 1 by Dvurechensky et al. (2018) to sparse matrices (i.e. matrices with some
Kij = 0) we need to redefine
ν := min{Kij |Kij > 0},
i,j
(36)
i.e. take the minimum only w.r.t. non-zero elements in their Lemma 1.
G Proof of Proposition 1
Theorem A (Danskin’s theorem). Consider a continuous function φ : Rk × Z → R, with the
compact set Z ⊂ Rj. If φ(x, z) is convex in x for every z ∈ Z and φ(x, z) has a unique maximizer
Z, the derivative of
f(x) = mz∈aZx φ(x, z)
(37)
is given by the derivative at the maximizer, i.e.
∂f = ∂φ(x, N)
∂x	∂x
(38)
We start by deriving the derivatives of the distances. To show that the Sinkhorn distance fulfills the
conditions for Danskin’s theorem we first identify x = C, z = P, and φ(C, P ) = -hP , CiF +
λH(P). We next observe that the restrictions P 1m = p and PT1n = q define a compact, convex
set for P . Furthermore, φ is a continuous function and linear in C, i.e. both convex and concave
for any finite P. Finally, φ(C, P) is concave in P since hP, CiF is linear and λH(P) is concave.
Therefore the maximizer PN is unique and Danskin’s theorem applies to the Sinkhorn distance. Using
d CNys,ij ∂Uk	二	=∂Ukl bλ Iog(XX UiaWaj )) = -λδik P WWaj = -λδik KW- ,	(39)
d CNys,ij ∂Wki	二 PN PNys,ij 二 KNys,ij	二 ∂W1 (-λ Iog(X UiaWaj )! = -λδ-l PuW- = -λδ-l KB-，(40) kl	a	a ia aj	Nys,ij 二 Eb PU,ibPW,bj = Sitj £b UibWbj = S N £b UibWbj = S t	(4ι) -Pa UiaWaj = Pa UiaWaj = St Pa UiaWaj = St	()
17
Under review as a conference paper at ICLR 2021
and the chain rule we can calculate the derivative w.r.t. the cost matrix as
∂dcλ
∂C
∂
-公(-hP,CiF + λH(P)) = P,
∂C
ddλCN,c
∂Uki
dCNys,ij ddλCN,c
乙 ∂Uki ∂CNys,ij
i,j
-λ	δikWlj
i,j
At
PNys,ij
KNys,ij
ddλCN,c
∂Wki
-λfδik Wij Stj = -λSkf Wljtj = (-λ3(W t)T )kl,
i,j	j
ΣdCNys,ij ddLCN,c _ ʌ X ʌ P P	PNys,ij
i,j	∂Wkl ∂CNys,ij = -	i,j jl ik KNys,ij
-λXδjlUikSiij = -λ (XSiUik 卜l = (-λ(sτU)Tt)kl,
i,j	i
(42)
(43)
(44)
□nr! ""LCN,c qnrl ""LCN,c 八彳FTUle∖x∕ fττΛin ddc
and ∂ log KsP and ∂ log Kp directly follow JTom ∂c .
Theorem B (Implicit function theorem). Let f : Rn0 × Rm0 → Rm0 be a continuously differentiable
function with f (a, b) = 0. Ifits Jacobian matrix Jfi(x,y),yj = ∂fi(a, b) is invertible, then there
exists an open set a ∈ U ⊂ Rn0 on which there exists a unique continuously differentiable function
g : U → Rm0 with g(a) = b and ∀x ∈ U : f(x, g(x)) = 0. Moreover,
一 (Jf(X,y),y(x,g(x)))i,:
∂X f(x,g(X))) :,j
(45)
Next we derive the transport plan derivatives. To apply the implicit function theorem we identify
x = C and y = P as flattened matrices. We will index these flat matrices via index pairs to simplify
interpretation. We furthermore identify
∂
f(C,P) = ∂PP (hP, CiF- λH(P)) = C + λ(logP +1).	(46)
The minimizer P cannot lie on the boundary of P's valid region since limp→o ∂pP log P =
limp→0 logp + 1 = -∞ and therefore PAij > 0. Hence, f(C, PA) = 0 with PA(C) =
arg minP hP, CiF - λH(P) and we find that g(x) = PA(C). We furthermore obtain
∂
Jfij(c,P),Pab(C,P(C)) = E (Cij + λ(logPij + 1)) = δiaδjbλ∕P>ij,	(47)
∂∂
kfab(GP(C)) = k (Cab + λ(lθg Pab + 1)) = Sakδbl.	(48)
∂Ckl	∂Ckl
Jfij(C,P),pab(C, P(C)) is hence a diagonal matrix and invertible since λ∕Pj > 0. We can thus
use the implicit function theorem and obtain
∂Pij
∂Ckl
- X(Jfij (C,P),Pab (C, P(C ))) 1
a,b
∂
F fab(C, P(C ))
- ^X δiaδjb λPij δak δbl = - ʌ Pijδikδjl ∙
a,b
To extend this result to LCN-OT we use
∂P⅛ = ∂p^ X Pys,ik PW,kj = δiaPW,bj,
IP⅛=∂P~; XPU ,ik Pys,kj=院PUi
(49)
(50)
(51)
18
Under review as a conference paper at ICLR 2021
and the chain rule to obtain
∂PU,ij
∂Ukl
∑∂CNys,cd ∂PNys,ab ∂PU,j
abcd	dUki	dCNys,cd ∂RyS,ab
X (-λδck Kn idcd ) (-λRyS,abδacδbd) (δiaPW,bj
a,b,c,d	Nys,cd
(52)
∂PW,ij
∂Uki
dPU,ij
∂Wkl
∂PW,ij
∂ Wki
X Wib Ry2
b	KNys,ib
δikδjiSi,
PW,bjδik = δikSiWlbtb-PW,bj = δiksi	^^w,lb^^w,bj
Cιχ-γ	Cl τS	Cl ŋ
∑d CNyS,cd∂PNys,ab ∂ PW,ij
a,b,c,d	∂Uki	∂CNys,cd ∂PNys,ab
XdSCk KWdd) (-1 Pys,ab
a,b,c,d
(δjbPU,ia)
(53)
Wij 屋y，'k ' PUJik = WijtjPU,iksk = PU,iksk PW,ij,
KNys,kj ,	,	,
∑∂CNys,cd ∂PNys,ab ∂PU,j
a,b,c,d ∂ Wki ∂CNys,cd ∂PNys,ab
X (-λδdi JCk ) (-γ-PNys,abδac
a,b,c,d	KNys,cd λ
Uik Kys" PWM = SiUik tiPW,ij = PU,ik ti PWM,
KNys,ii
一_ 一 — — _ 一
(54)
Σ
a,b,c,d
d CNys,cd ∂pys,ab ∂ p^w,ij
∂Wkl ∂CNys,cd
X S C
X Uak Kysj PU,iaδjl
a	KNys,aj
δik δjitj .
∂ Pys,ab
(一λ PNys,ab
(δjbPU,ia)
(55)
δjl	SaUaktjPU,ia = δjltj JS PU,iaPU,ak
b
b
a
a
∂PSP
∂ log KsP
一	∂PSF	一 一 一一^ g∙f*
and ∂ log KSP directly follow from ∂∂C.
We can calculate the pseudoinverses PU = (PTPU)-1Pu and PW = PW(PWPW)-1 in time
O((n + m)l2 + l3) since PU ∈ Rn×i and PW ∈ Ri×m. We do not fully instantiate the matrices
required for backProPagation but instead save their decomPositions, similar to the transPort Plan
RyS = PUPW. We can then compute backpropagation in time O((n + m)l2) by applying the
sums over i and j in the right order. We thus obtain O((n + m)l2 + l3) overall runtime for
backpropagation.
□
H	Choosing LSH Neighbors and NYSTRGM Landmarks
We focus on two LSH methods for obtaining near neighbors. Cross-polytope LSH (Andoni et al.,
2015) uses a random projection matrix R ∈ Rd×b/2 with the number of hash buckets b, and then
decides on the hash bucket via h(x) = arg max([xT R k -xT R]), where k denotes concatenation.
K-means LSH computes k-means and uses the clusters as hash buckets.
We further improve the sampling probabilities of cross-polytope LSH via the AND-oR construction.
In this scheme we calculate B ∙ r hash functions, divided into B sets (hash bands) of r hash functions
19
Under review as a conference paper at ICLR 2021
Table 5: Graph dataset statistics.
Distance (test Set) Graphs Avg. nodes Avg. edges Node Edge
	Graph type	Distance	Mean	Std. dev.	train/val/test	per graph	per graph	types	types
AIDS30	Molecules	GED	50.5	-162-	144/48/48	20.6	44.6	53	4
Linux	Program dependence	GED	0.567	0.181	600/200/200	7.6	6.9	7	-
Pref. att.	Initial attractiveness	GED	106.7	48.3	144/48/48	20.6	75.4	6	4
Pref. att. 200	Initial attractiveness	PM	0.400	0.102	144/48/48	199.3	938.8	6	-
Pref. att. 2k	Initial attractiveness	PM	0.359	0.163	144/48/48	2045.6	11330	6	-
Pref. att. 20k	Initial attractiveness	PM	0.363	0.151	144/48/48	20441	90412	6	-
each. A pair of points is considered as neighbors if any hash band matches completely. K-means
LSH does not work well with the AND-OR construction since its samples are highly correlated. For
large datasets We use hierarchical k-means instead (PaUleVe et al., 2010; NiSter & SteWenius, 2006).
Since the graph transport network (GTN) uses the L2 distance between embeddings as a cost
function we use (hierarchical) k-means LSH and k-means Nystrom in both sparse OT and LCN-
OT. For embedding alignment We use cross-polytope LSH for sparse OT since similarities are
measured via the dot product. For LCN-OT We found that using k-means LSH Works better With
Nystrom using k-means++ sampling than cross-polytope LSH. This is most likely due to a better
alignment betWeen LSH samples and Nystrom. We convert the cosine similarity to a distance via
X	XT X
dcos = ∖ 1 - ∣∣χpkp ∣∣Xq∣∣2 (Berg et al., 1984) to use k-means with dot product similarity. Note that
this is actually based on cosine similarity, not the dot product. Due to the balanced nature of OT We
found this more sensible than maximum inner product search (MIPS). For both experiments we also
experimented with uniform and recursive RLS sampling but found that the above mentioned methods
work better.
I	Implementational Details
Our implementation runs in batches on a GPU via PyTorch (Paszke et al., 2019) and PyTorch Scatter
(Fey & Lenssen, 2019). To avoid over- and underflows we use log-stabilization throughout, i.e. we
save all values in log-space and compute all matrix-vector products and additions via the log-sum-exp
trick log Pi exi = maxj xj + log(Pi exi-maxj xj ). Since the matrix A is small we compute its
inverse using double precision to improve stability. Surprisingly, we did not observe any benefit
from using the Cholesky decomposition or not calculating A-1 and instead solving the equation
B = AX for X. We furthermore precompute W = A-1V to avoid unnecessary operations.
We use 3 layers and an embedding size HN = 32 for GTN. The MLPs use a single hidden layer,
biases and LeakyReLU non-linearities. The single-head MLP uses an output size of HN, match = HN
and a hidden embedding size of 4HN, i.e. the same as the concatenated node embedding, and the
multi-head MLP uses a hidden embedding size of HN. To stabilize initial training we scale the node
embeddings by _ / d directly before calculating OT. d denotes the average graph distance in the
n ʌ/ HN, match
training set, n the average number of nodes per graph, and HN match the matching embedding size, i.e.
32 for single-head and 128 for multi-head OT.
J Graph Dataset Generation and Experimental Details
The dataset statistics are summarized in Table 5. Each dataset contains the distances between all
graph pairs in each split, i.e. 10 296 and 1128 distances for preferential attachment. The AIDS dataset
was generated by randomly sampling graphs with at most 30 nodes from the original AIDS dataset
(Riesen & Bunke, 2008). Since not all node types are present in the training set and our choice of
GED is permutation-invariant w.r.t. types, we permuted the node types so that there are no previously
unseen types in the validation and test sets. For the preferential attachment datasets we first generated
12, 4, and 4 undirected “seed” graphs (for train, val, and test) via the initial attractiveness model with
randomly chosen parameters: 1to5 initial nodes, initial attractiveness of0 to4 and 1/2n and 3/2n
total nodes, where n is the average number of nodes (20, 200, 2000, and 20000). We then randomly
label every node (and edge) in these graphs uniformly. To obtain the remaining graphs we edit the
“seed” graphs between n/40 and n/20 times by randomly adding, type editing, or removing nodes
20
Under review as a conference paper at ICLR 2021
Table 6: Hyperparameters for the Linux dataset.
	lr	batchsize	layers	emb. size	L2 reg.	λbase
SiamMPNN	1 × 10-4	256	~~3^^	32	5 × 10-4	-
GMN	1 × 10-4	20	3	64	0	-
GTN, 1 head	0.01	1000	~~3^^	32	1 × 10-6	1.0
8 OT heads	0.01	1000	3	32	1 × 10-6	1.0
Balanced OT	0.01	1000	3	32	1 × 10-6	2.0
	Table 7: Hyperparameters for the AIDS dataset.					
	lr	batchsize	layers	emb. size	L2 reg.	λbase
SiamMPNN	1 × 10-4	256	-^3~~	32	5 × 10-4	-
SimGNN	1 × 10-3	1	3	32	0.01	-
GMN	1 × 10-2	128	3	32	0	-
GTN, 1 head	0.01	100	-^3~~	32	5 × 10-3	0.1
8 OT heads	0.01	100	3	32	5 × 10-3	0.075
Balanced OT	0.01	100	3	32	5 × 10-3	0.1
NyStrom	0.015	100	-^3~~	32	5 × 10-3	0.2
Multiscale	0.015	100	3	32	5 × 10-3	0.2
Sparse OT	0.015	100	3	32	5 × 10-3	0.2
LCN-OT	0.015	100	3	32	5 × 10-3	0.2
and edges. Editing nodes and edges is 4x and adding/deleting edges 3x as likely as adding/deleting
nodes. Most of these numbers were chosen arbitrarily, aiming to achieve a somewhat reasonable
dataset and process. We found that the process of first generating seed graphs and subsequently
editing these is crucial for obtaining meaningfully structured data to learn from. For the GED we
choose an edit cost of 1 for changing a node or edge type and 2 for adding or deleting a node or an
edge.
We represent node and edge types as one-hot vectors. We train all models except SiamMPNN (which
uses SGD) and GTN on Linux with the Adam optimizer and mean squared error (MSE) loss for up to
300 epochs and reduce the learning rate by a factor of 10 every 100 steps. On Linux we train for up
to 1000 epochs and reduce the learning rate by a factor of 2 every 100 steps. We use the parameters
from the best epoch based on the validation set. We choose hyperparameters for all models using
multiple steps of grid search on the validation set, see Tables 6 to 8 for the final values. We use the
originally published result of SimGNN on Linux and thus don’t provide its hyperparameters. GTN
uses 500 Sinkhorn iterations. We obtain the final entropy regularization parameter from λbase via
λ = λbase n login, where d denotes the average graph distance and n the average number of nodes
per graph in the training set. The factor d/n serves to estimate the embedding distance scale and
1/ logn counteracts the entropy scaling with nlogn. Note that the entropy regularization parameter
was small, but always far from 0, which shows that entropy regularization actually has a positive
Table 8: Hyperparameters for the preferential attachment GED dataset.
	lr	batchsize	layers	emb. size	L2	reg.	λbase
SiamMPNN	1 × 10-4	256	-^3^^	64	1×	10-3	-
SimGNN	1 × 10-3	4	3	32		0	-
GMN	1 × 10-4	20	3	64		0	-
GTN, 1 head	0.01	100	-^3^^	32	5×	10-4	0.2
8 OT heads	0.01	100	3	32	5×	10-3	0.075
Balanced OT	0.01	100	3	32	5×	10-4	0.2
Nystrom	0.02	100	-^3^^	32	5×	10-5	0.2
Multiscale	0.02	100	3	32	5×	10-5	0.2
Sparse OT	0.02	100	3	32	5×	10-5	0.2
LCN-OT	0.02	100	3	32	5×	10-5	0.2
21
Under review as a conference paper at ICLR 2021
Table 9: Runtimes (ms) of Sinkhorn approximations for EN-DE embeddings at different dataset sizes.
Full Sinkhorn scales quadratically, while all approximationes scale at most linearly with the size.
Sparse approximations are 2-4x faster than low-rank approximations, and factored OT is multiple
times slower due to its iterative refinement scheme. Note that similarity matrix computation time (K)
primarily depends on the LSH/Nystrom method, not the OT approximation.
	N=	10000	N	二 20000	N=	50000
	K	-^OT^	K^	^^Or-	K	
Full Sinkhorn	8	2950	29	11 760	OOM	OOM
Factored OT	29	809	32	1016	55	3673
Multiscale OT	90	48	193	61	521	126
Nystrom Skh.	29	135	41	281	79	683
Sparse Skh.	42	46	84	68	220	137
LCN-Sinkhorn	101	116	242	205	642	624
(S) qood°J°d°InLL
Avg. graph size
Figure 6: Log-log runtime per epoch for GTN
with full Sinkhorn and LCN-Sinkhorn. LCN-
Sinkhorn scales almost linearly with graph size
while sustaining similar accuracy.
300
S
二200
≡
I100
∙¼Multsc. OT ɪ ⅝T	 Ql-L	
ɪ Nys. Skh.	
・"Sparse Skh.	■，二二”
LCN-Skh.	
0	100	200
Neighbors + landmarks
0
Figure 5: Runtime scales linearly with the number
of neighbors/landmarks for all relevant Sinkhorn
approximation methods.
effect on learning. On the pref. att. 200 dataset we use no L2 regularization, λbase = 0.5, and a batch
size of 200. For pref. att. 2k we use λbase = 2 and a batch size of 20 for full Sinkhorn and 100 for
LCN-OT. For pref. att. 20k we use λbase = 50 and a batch size of 4. λbase scales with graph size due
to normalization of the PM kernel.
For LCN-OT we use roughly 10 neighbors for LSH (20 k-means clusters) and 10 k-means landmarks
for Nystrom on pref. att. 200. We double these numbers for pure NystrUm Sinkhorn, sparse OT, and
multiscale OT. For pref. att. 2k We use around 15 neighbors (10 ∙ 20 hierarchical clusters) and 15
landmarks and for pref. att. 20k we use roughly 30 neighbors (10 ∙ 10 ∙ 10 hierarchical clusters) and
20 landmarks. The number of neighbors for the 20k dataset is higher and strongly varies per iteration
due to the unbalanced nature of hierarchical k-means. This increase in neighbors and landmarks and
PyTorch’s missing support for ragged tensors largely explains LCN-OT’s deviation from perfectly
linear runtime scaling.
We perform all runtime measurements on a compute node using one Nvidia GeForce GTX 1080 Ti,
two Intel Xeon E5-2630 v4, and 256GB RAM.
K	Runtimes
Table 9 compares the runtime of the full Sinkhorn distance with different approximation methods
using 40 neighbors/landmarks. We separate the computation of approximate K from the optimal
transport computation (Sinkhorn iterations), since the former primarily depends on the LSH and
Nystrom methods we choose. We observe a 2-4x speed difference between sparse (multiscale OT
and sparse Sinkhorn) and low-rank approximations (Nystrom Sinkhorn and LCN-Sinkhorn), while
factored OT is multiple times slower due to its iterative refinement scheme. In Fig. 5 we observe that
this runtime gap stays constant independent of the number of neighbors/landmarks, i.e. the relative
22
Under review as a conference paper at ICLR 2021
Figure 7: Sinkhorn distance ap- Figure 8: Sinkhorn distance ap- Figure 9: Sinkhorn distance
proximation at different runtimes proximation for varying compu- approximation for varying en-
(varied via the number of neigh- tational budget. The dashed tropy regularization λ at constant
bors/landmarks). The dashed line denotes the true Sinkhorn computational budget. Sparse
line denotes the true Sinkhorn distance. Sparse Sinkhorn Sinkhorn performs best for low
distance. Sparse Sinkhorn con- mostly performs best, with LCN- λ, LCN-Sinkhorn for moderate
sistently performs best. The ar- Sinkhorn coming in second. Fac- and high λ and factored OT for
row indicates factored OT results tored OT performs well with very high λ.
far outside the depicted range. few landmarks.
λ
difference decreases as we increase the number of neighbors/landmarks. This gap could either be due
to details in low-level CUDA implementations and hardware or the fact that low-rank approximations
require 2x as many multiplications for the same number of neighbors/landmarks. In either case, both
Table 9 and Fig. 5 show that the runtimes of all approximations scale linearly both in the dataset size
and the number of neighbors and landmarks, while full Sinkhorn scales quadratically.
We furthermore investigate whether GTN with approximate Sinkhorn indeed scales log-linearly
with the graph size by generating preferential attachment graphs with 200, 2000, and 20 000 nodes
(±50 %). We use the Pyramid matching (PM) kernel (Nikolentzos et al., 2017) as prediction target.
Fig. 6 shows that the runtime of LCN-Sinkhorn scales almost linearly (dashed line) and regular full
Sinkhorn quadraticly (dash-dotted line) with the number of nodes, despite both achieving similar
accuracy and LCN using slightly more neighbors and landmarks on larger graphs to sustain good
accuracy. Full Sinkhorn went out of memory for the largest graphs.
L Distance Approximation
Figs. 7 and 8 show that for the chosen λ = 0.05 sparse Sinkhorn offers the best trade-off between
computational budget and distance approximation, with LCN-Sinkhorn and multiscale OT coming in
second. Factored OT is again multiple times slower than the other methods and thus not included in
Fig. 7. Note that dcλ can be negative due to the entropy offset. This picture changes as we increase the
regularization. For higher regularizations LCN-Sinkhorn is the most precise at constant computational
budget (number of neighbors/landmarks), as shown in Fig. 9. Note that the crossover points in this
figure roughly coincide with those in Fig. 4. Keep in mind that in most cases the OT plan is more
important than the raw distance approximation, since it determines the training gradient and tasks
like embedding alignment don’t use the distance at all. This becomes evident in the fact that sparse
Sinkhorn achieves a better distance approximation than LCN-Sinkhorn but performs worse in both
downstream tasks investigated in Sec. 7.
23