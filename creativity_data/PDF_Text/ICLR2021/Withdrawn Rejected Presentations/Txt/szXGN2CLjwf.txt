Under review as a conference paper at ICLR 2021
Adam+: A Stochastic Method with Adaptive
Variance Reduction
Anonymous authors
Paper under double-blind review
Ab stract
Adam is a widely used stochastic optimization method for deep learning applica-
tions. While practitioners prefer Adam because it requires less parameter tuning,
its use is problematic from a theoretical point of view since it may not converge.
Variants of Adam have been proposed with provable convergence guarantee, but
they tend not be competitive with Adam on the practical performance. In this
paper, we propose a new method named Adam+ (pronounced as Adam-plus).
Adam+ retains some of the key components of Adam but it also has several no-
ticeable differences: (i) it does not maintain the moving average of second mo-
ment estimate but instead computes the moving average of first moment estimate
at extrapolated data points; (ii) its adaptive step size is formed not by dividing
the square root of second moment estimate but instead by dividing the root of
the norm of first moment estimate. As a result, Adam+ requires few parameter
tuning, as Adam, but it enjoys a provable convergence guarantee. Our analysis
further shows that Adam+ enjoys adaptive variance reduction, i.e., the variance of
the stochastic gradient estimator reduces as the algorithm converges, hence enjoy-
ing an adaptive convergence. We also propose a more general variant of Adam+
with different adaptive step sizes and establish their fast convergence rate. Our
empirical studies on various deep learning tasks, including image classification,
language modeling, and automatic speech recognition, demonstrate that Adam+
significantly outperforms Adam and achieves comparable performance with best-
tuned SGD and momentum SGD.
1	Introduction
Adaptive gradient methods (Duchi et al., 2011; McMahan & Streeter, 2010; Tieleman & Hinton,
2012; Kingma & Ba, 2014; Reddi et al., 2019) are one of the most important variants of Stochastic
Gradient Descent (SGD) in modern machine learning applications. Contrary to SGD, adaptive gra-
dient methods typically require little parameter tuning still retaining the computational efficiency of
SGD. One of the most used adaptive methods is Adam (Kingma & Ba, 2014), which is considered
by practitioners as the de-facto default optimizer for deep learning frameworks. Adam computes the
update for every dimension of the model parameter through a moment estimation, i.e., the estimates
of the first and second moments of the gradients. The estimates for first and second moments are
updated using exponential moving averages with two different control parameters. These moving
averages are the key difference between Adam and previous adaptive gradient methods, such as
Adagrad (Duchi et al., 2011).
Although Adam exhibits great empirical performance, there still remain many mysteries about
its convergence. First, it has been shown that Adam may not converge for some objective func-
tions (Reddi et al., 2019; Chen et al., 2018b). Second, it is unclear what is the benefit that the
moving average brings from theoretical point of view, especially its effect on the convergence rate.
Third, it has been empirically observed that adaptive gradient methods can have worse generaliza-
tion performance than its non-adaptive counterpart (e.g., SGD) on various deep learning tasks due
to the coordinate-wise learning rates (Wilson et al., 2017).
The above issues motivate us to design a new algorithm which achieves the best of both worlds,
i.e., provable convergence with benefits from the moving average and enjoying good generalization
performance in deep learning. Specifically, we focus on the following optimization problem:
min F (w),
w∈Rd
1
Under review as a conference paper at ICLR 2021
Table 1: Summary of different algorithms with different assumptions and complexity results for find-
ing an e-stationary point. “Individual Smooth" means assuming that F(W) = Eξ〜D [f (w; ξ)] and
that every component function f (w; ξ) is L-Smooth. “Hessian Lipschitz” means that kV2F(x)-
V2F(y)k ≤ LH ∣∣x - yk holds for x, y and LH ≥ 0. “Type I” means that the complexity depends
on E[PT=ι |gi：T,ik], where gi：T,i stands for the i-th row of the matrix [gι,..., gτ] with gt being
the stochastic gradient at t-th iteration and T being the number of iterations. “Type II” means that
complexity depends on E[PtT=1 ∣zt ∣], where zt is the variance-reduced gradient estimator at t-th
iteration.
Algorithm	Individual Smooth	Hessian Lipschitz	WOrSt-CaSe Complexity betterthan O(e-4)?	Data-dependent Complexity
Generalized Adam (Chen et al., 2018b) PAdam (Chen et al., 2018a) Stagewise Adagrad (Chen et al., 2019) SGD with Adaptive Stepsize (Li & Orabona, 2019) Adagrad-Norm (Ward et al., 2019)	No	No	No	Type I
SPIDER (Fang et al., 2018) STORM (Cutkosky & Orabona, 2019) SNVRG (Zhou et al., 2018) Prox-SARAH (Pham et al., 2020)	Yes	No	Yes	N/A
SGD (Fang et al., 2019) Normalized momentum SGD (Cutkosky & Mehta, 2020)	No	Yes	Yes	N/A
Adam+ (this work)	No	Yes	Yes	Type II
where we only have access to stochastic gradients of F. Note that F could possibly be nonconvex
in w. Due to the non-convexity, our goal is to design a stochastic first-order algorithm to find the
e-stationary point, i.e., finding w such that E [∣VF (w)∣] ≤ e, with low iteration complexity.
Our key contribution is the design and analysis of a new stochastic method named Adam+ . Adam+
retains some of the key components of Adam but it also has several noticeable differences: (i) it
does not maintain the moving average of second moment estimate but instead computes the moving
average of first moment estimate at extrapolated points; (ii) its adaptive step size is formed not
by dividing the square root of coordinate-wise second moment estimate but instead by dividing
the root of the norm of first moment estimate. These features allow us to establish the adaptive
convergence of Adam+ . Different from existing adaptive methods where the adaptive convergence
depends on the growth rate of stochastic gradients (Duchi et al., 2011; McMahan & Streeter, 2010;
Kingma & Ba, 2014; Luo et al., 2019; Reddi et al., 2019; Chen et al., 2018a;b; Ward et al., 2019;
Li & Orabona, 2019; Chen et al., 2019), our adaptive convergence is due to the adaptive variance
reduction property of our first order moment estimate. In existing literature, the variance reduction
is usually achieved by large mini-batch (Goyal et al., 2017) or recursive variance reduction (Fang
et al., 2018; Zhou et al., 2018; Pham et al., 2020; Cutkosky & Orabona, 2019). In contrast, we do not
necessarily require large minibatch or computing stochastic gradients at two points per-iteration to
achieve the variance reduction. In addition, we also establish a fast rate that matches the state-of-the-
art complexity under the same conditions of a variant of Adam+. Table 1 provides an overview of our
results and a summary of existing results. We refer readers to Section F for a comprehensive survey
of other related work. We further corroborate our theoretical results with an extensive empirical
study on various deep learning tasks.
Our contributions are summarized below.
•	We propose a new algorithm with adaptive step size, namely Adam+, for general noncon-
vex optimization. We show that it enjoys a new type of data-dependent adaptive conver-
gence that depends on the variance reduction property of first moment estimate. Notably,
this data-dependent complexity does not require the presence of sparsity in stochastic gra-
dients to guarantee fast convergence as in previous works (Duchi et al., 2011; Kingma &
Ba, 2014; Reddi et al., 2019; Chen et al., 2019; 2018a). To the best of our knowledge, this
is the first work establishing such new type of data-dependent complexity.
•	We show that a general variant of our algorithm can achieve O(e-3.5) worst-case complex-
ity, which matches the state-of-the-art complexity guarantee under the Hessian Lipschitz
assumption (Cutkosky & Mehta, 2020).
•	We demonstrate the effectiveness of our algorithms on image classification, language mod-
eling, and automatic speech recognition. Our empirical results show that our proposed
algorithm consistently outperforms Adam on all tasks, and it achieves comparable perfor-
mance with the best-tuned SGD and momentum SGD.
2
Under review as a conference paper at ICLR 2021
Algorithm 1 Adam+ : Good default settings for the tested machine learning problems are α
0.1, a = 1, β = 0.1, e0 = 10-8.
1:	Require: α, a ≥ 1: stepsize parameters
2:	Require: β ∈ (0, 1): Exponential decay rates for the moment estimate
3:	Require: gt(w): unbiased stochastic gradient with parameters w at iteration t
4:	Require: w0: Initial parameter vector
5:	z0 = g0(w0)
6:	for t = 0, . . . , T do
7:	Setηt=
max(kztk1∕2,eo)
8:	wt+1 = wt - ηtzt
9：	Wt+ι = (1 - 1∕β)wt + 1∕β^∙ wt+ι
10:	zt+1 = (1 - β)zt + βgt+1(wb t+1)
11:	end for
2 Algorithm and Theoretical Analysis
In this section, we introduce our algorithm Adam+ (presented in Algorithm 1) and establish its
convergence guarantees. Adam+ resembles Adam in several aspects but also has noticeable differ-
ences. Similar to Adam, Adam+ also maintains an exponential moving average of first moment (i.e.,
stochastic gradient), which is denoted by zt, and uses it for updating the solution in line 8. However,
the difference is that the stochastic gradient is evaluated on an extrapolated data point wb t+1, which
is an extrapolation of two previous updates wt and wt+1. Similar to Adam, Adam+ also uses an
adaptive step size that is proportional to 1/|%||1/2. Nonetheless, the difference lies at its adaptive
step size is directly computed from the square root of the norm of first moment estimate zt . In
contrast, Adam uses an adaptive step size that is proportional to 1/√vt, where Vt is an exponential
moving average of second moment estimate. These two key components of Adam+, i.e., extrap-
olation and adaptive step size from the root norm of the first moment estimate, make it enjoy two
noticeable benefits: variance reduction of first moment estimate and adaptive convergence. We shall
explain these two benefits later.
Before moving to the theoretical analysis, we would like to make some remarks. First, it is worth
mentioning that the moving average estimate with extrapolation is inspired by the literature of
stochastic compositional optimization (Wang et al., 2017). Wang et al. (2017) showed that the ex-
trapolation helps balancing the noise in the gradients, reducing the bias in the estimates and giving
a faster convergence rate. Here, our focus and analysis techniques are quite different. In fact, Wang
et al. (2017) focuses on the compositional optimization while we consider a general nonconvex op-
timization setting. Moreover, the analysis in (Wang et al., 2017) mainly deals with the error of the
gradient estimator caused by the compositional nature of the problem, while our analysis focuses
on carefully designing adaptive normalization to obtain an adaptive and fast convergence rates. A
similar extrapolation scheme has been also employed in the algorithm NIGT by Cutkosky & Mehta
(2020). In later sections, we will also provide a more general variant of Adam+ which subsumes
NIGT as a special case.
Another important remark is that the update of Adam+ is very different from the famous Nesterov’s
momentum method. In Nesterov’s momentum method, the update of wt+1 uses the stochastic gradi-
ent at an extrapolated point wb t+1 = wt+1 + γ(wt+1 - wt) with a momentum parameter γ ∈ (0, 1).
In contrast, in Adam+ the update of wt+1 is using the moving average estimate at an extrapolated
point Wt+ι = wt+ι + (1∕β - 1)(wt+ι - wt). Finally, Adam+ does not employ coordinate-wise
learning rates as in Adam, and hence it is expected to have better generalization performance ac-
cording to Wilson et al. (2017).
2.1	Adaptive Variance Reduction and Adaptive Convergence
In this subsection, we analyze Adam+ by showing its variance reduction property and adaptive
convergence. To this end, we make the following assumptions.
Assumption 1. There exists positive constants L, ∆, LH , σ and an initial solution w0 such that
(i)	F is L-smooth, i.e., kVF(x) — VF(y)k ≤ L ∣∣x — yk, ∀x, y ∈ Rd.
3
Under review as a conference paper at ICLR 2021
(ii)	For ∀x ∈ Rd, we have access to a first-order stochastic oracle at time t gt (x) such that
E [gt(x)] = NF(x), Ekgt(X)- NF(x)k2 ≤ σ2.
(iii)	NF is a LH-smooth mapping, i.e., kV2F(x) — N2F(y)k ≤ LH∣∣x 一 yk,∀x, y ∈ Rd.
(iv)	F (wo) — F ≤ ∆ < ∞, where F = inf w∈Rd F (W).
Remark: Assumption 1 (i) and (ii), (iv) are standard assumptions made in literature of stochastic
non-convex optimization (Ghadimi & Lan, 2013). Assumption (iii) is the assumption that deviates
from typical analysis of stochastic methods. We leverage this assumption to explore the benefit of
moving average, extrapolation and adaptive normalization. It is also used in some previous works
for establishing fast rate of stochastic first-order methods for nonconvex optimization (Fang et al.,
2019; Cutkosky & Mehta, 2020) and this assumption is essential to get fast rate due to the hardness
result in (Arjevani et al., 2019). Itis also the key assumption for finding a local minimum in previous
works (Carmon et al., 2018; Agarwal et al., 2017; Jin et al., 2017).
We might also assume that the stochastic gradient estimator in Algorithm 1 satisfies the following
variance property.
Assumption 2. Assume that E[kg0(w0) — NF(w0)k2] ≤ σ02 and E[kgt(wt) — NF(wt)k2] ≤
σm2 , t ≥ 1.
Remark: When g0 (resp. gt) is implemented by a mini-batch stochastic gradient with mini-batch
size S, then σ0 (resp. σ21) can be set as σ2/S by Assumption 1 (ii). We differentiate the initial
variance and intermediate variance because they contribute differently to the convergence.
We first introduce a lemma to characterize the variance of the moving average gradient estimator zt .
Lemma 1. Suppose Assumption 1 and Assumption 2 hold and a ≥ 1. Then, there exists a sequence
of random variables δt satisfying kzt — NF(wt)k ≤ δt for ∀t ≥ 0,
E [δ2+ι] ≤ (l 一 2) E [δ2] + 2β 2σm + E [ CCLH 忡才一wtk4 一
≤(1 - 2) E [δ2] + 2β2σm + E [CLHɑ4β4-3|。『],
where C= 1944.
Remark: Note that δt is an upper bound of kzt 一 NF(wt)k, the above lemma can be used to
illustrate the variance reduction effect for the gradient estimator zt . To this end, we can bound
Ilztll2 ≤ 2δt + 2∣VF(Wt)II2, then the term CL2α4β4a-3δ2 can be canceled with —β∕4δ2 with
small enough a. Hence, We have Eδ2+ι ≤ (1 — β∕4)E[δ2] + 2β2σt2-b + cE[∣VF(wt)∣2] with a
small constant c. As the algorithm converges with E[INF(wt)I2] and β decreases to zero, the
variance of zt will also decrease. Indeed, the above recursion of zt’s variance resembles that of the
recursive variance reduced gradient estimators (e.g., SPIDER (Fang et al., 2018), STORM (Cutkosky
& Orabona, 2019)). The benefit of using Adam+ is that we do not need to compute stochastic
gradient twice at each iteration.
We can now state our convergence rates for Algorithm 1.
Theorem 1.	Suppose Assumption 1 and Assumption 2 hold. Suppose INF (w)I ≤ G for any
w ∈ Rd. By choosing the parameters such that α4 ≤ 37j2 , a ≤ ±, a = 1 and e0 = βa, we have
-XXX E ∣VF (wt)k2 ≤ GE hPt=1kztki + ʌ+ 18£o +30βσ2 .	(1)
t	m.
t=1	α
In addition, suppose the initial batch size is T0 and the intermediate batch size is m, and choose
β = T-b with 0 ≤ b ≤ 1, we have
1T
T EEkVF(Wt)『≤
t=1
E WPT=I kzt∣	=	18σ2	30σ2
τ	+ αT + T 1-bτ0 + mτb.
(2)
4
Under review as a conference paper at ICLR 2021
Theorem 2.	Suppose Assumption 1 and Assumption 2 hold. By choosing parameters such that
640α3L%2 ≤ 1/120, a = 1, e0 = 0,β = 1/TS with S = 2/3 then it takes T = O (e-4∙5) number
of iterations to ensure that
T
T XE hkVF(wt)k3/2i ≤ e3/2，TE
t=1
T
X δ3/	≤ e3/2 .
t=1
Remarks:
From Theorem 1, we can observe that the convergence rate of Adam+ crucially depends on
the growth rate of E
PtT=1
E PtT=1 kztk
kzt k , which gives
a data-dependent adaptive complexity. If
≤ Tα with α < 1, then the algorithm converges. Smaller α implies faster
convergence. Our goal is to ensure that T PT=I EkVF(Wt)II2 ≤ e2. Choosing b = 1 - α,
m = O⑴ and To = T1-α = O(e-2), and We end UP with T = O (e- 1-2α) complexity.
• Theorem 2 shows that in the ergodic sense, the Algorithm Adam+ always converges, and
the variance gets smaller when the number of iteration gets larger. Theorem 2 rules out
the case that the magnitude of zt converges to a constant and the bound (2) in Theorem 1
becomes vacuous.
To compare with Adam-style algorithms (e.g., Adam, AdaGrad), these algorithms’ con-
vergence depend on the growth rate of stochastic gradient, i.e., Pid=1 kg1:T,ik/T, where
g1:T,i = [g1,i, . . . , gT,i] denotes the i-th coordinate of all historical stochastic gradients.
Hence, the data determines the growth rate of stochastic gradient. If the stochastic gradi-
ents are not sparse, then its growth rate may not be slow and these Adam-style algorithms
may suffer from slow convergence. In contrast, for Adam+ the convergence can be ac-
celerated by the variance reduction property.
Note that we have E PtT=1 kztk /T ≤
E PtT=1(δt + kVF(Wt)k) /T.
reduction property of zt .
Hence, Adam+ ’s convergence depends on the variance
2.2 A General Variant of Adam+ : Fast Convergence with Large Mini-batch
Next, we introduce a more general variant of Adam+ by making a simple change. In particu-
lar, we keep all steps the same as in Algorithm 1 except the adaptive step size is now set as
ηt = maχ(αβ∣∣p e0), Where P ∈ [1/2,1) is parameter. We refer to this general variant of Adam+
as power normalized Adam+ (Nadam+). This generalization allows us to compare with some ex-
isting methods and to establish fast convergence rate. First, We notice that When setting p = 1 and
a = 5/4 and β = 1/T 4/7, Nadam+ is almost the same as the stochastic method NIGT (Cutkosky
& Mehta, 2020) With only some minor differences. HoWever, We observed that normalizing by kzt k
leads to sloW convergence in practice, so We are instead interested in p < 1. BeloW, We Will shoW
that NAdam+ With p < 1 can achieve a fast rate of 1/e3.5, Which is the same as NIGT.
Theorem 3. Under the same assumption as in Theorem 1, further assume σ2 = σ1 /To and σ21 =
σ2/m. By using the step size η = max(；； ；/3 . ) in Algorithm 1 With CL2 α4 ≤ 1/14, e0 = 2β4/3,
in order to have E [kVF (Wτ)k] ≤ efora randomly selected solution Wτ from {W1, . . . , WT}, it suf-
fice to set β = O(e1/2), T = O(e-2), the initial batch size To = 1/e=O(e-1/2), the intermediate
batch size as m = 1/e3 = O(e-3/2), which ends UP with the total complexity O(e-3∙5).
Remark: Note that the above theorem establishes the fast convergence rate for Nadam+ With p =
2/3. Indeed, We can also establish a fast rate of Adam+ (Where p = 1/2) in the order of O(1/e3.625)
With details provided in the Appendix E.
3	Experiments
In this section, We conduct empirical studies to verify the effectiveness of the proposed algorithm
on three different tasks: image classification on CIFAR10 and CIFAR100 dataset (Krizhevsky et al.,
5
Under review as a conference paper at ICLR 2021
Figure 1: Comparison of optimization methods for ResNet18 Training on CIFAR10.
2009), language modeling on Wiki-Text2 dataset (Merity, 2016) and automatic speech recognition
on SWB-300 dataset (Saon et al., 2017). We choose tasks from different domains to demonstrate
the applicability for the real-world deep learning tasks in a broad sense. The detailed description
is presented in Table 2. We compare our algorithm Adam+ with SGD, momentum SGD, Adagrad,
NIGT and Adam. We choose the same random initialization for each algorithm, and run a fixed
number of epochs for every task. For Adam we choose the default setting β1 = 0.9 and β2 = 0.999
as in the original Adam paper.
Table 2: Summary of setups in the experiments.
Domain	Task	Architecture	Dataset
Computer Vision	Image Classification	ResNet18	CIFAR10
Computer Vision	Image Classification	VGG19	CIFAR100
Natural Language Processing	Language Modeling	Two-layer LSTM	Wiki-Text2
Automatic Speech Recognition	Speech Recognition	Six-layer BiLSTM	SWB-300
3.1	Image Classification
CIFAR10 and CIFAR100 In the first experiment, we consider training ResNet18 (He et al., 2016)
and VGG19 (Simonyan & Zisserman, 2014) to do image classification task on CIFAR10 and CI-
FAR100 dataset respectively. For every optimizer, we use batch size 128 and run 350 epochs. For
SGD and momentum SGD, we set the initial learning rate to be 0.1 for the first 150 epochs, and the
learning rate is decreased by a factor of 10 for every 100 epochs. For Adagrad and Adam, the initial
learning rate is tuned from {0.1, 0.01, 0.001} and we choose the one with the best performance. The
best initial learning rates for Adagrad and Adam are 0.01 and 0.001 respectively. For NIGT, we
tune the their momentum parameter from {0.01, 0.1, 0.9} (the best momentum parameter we found
is 0.9) and the learning rate is chosen the same as in SGD. For Adam+, the learning rate is set ac-
cording to Algorithm 1, in which we choose β = 0.1 and the value of α is the same as the learning
rate used in SGD. We report training and test accuracy versus the number of epochs in Figure 1 for
CIFAR10 and Figure 2 for CIFAR100. We observe that our algorithm consistently outperforms all
other algorithms on both CIFAR10 and CIFAR100, in terms of both training and testing accuracy.
Notably, we have some interesting observations for the training of VGG19 on CIFAR100. First, both
Adam+ and NIGT significantly outperform SGD, momentum SGD, Adagrad and Adam. Second,
Adam+ achieves almost the same final accuracy as NIGT, and Adam+ converges much faster in the
early stage of the training.
3.2	Language Modeling
Wiki-text2 In the second experiment, we consider the language modeling task on WikiText-2
dataset. We use a 2-layer LSTM (Hochreiter & Schmidhuber, 1997). The size of word embeddings
is 650 and the number of hidden units per layer is 650. We run every algorithm for 40 epochs, with
batch size 20 and dropout ratio 0.5. For SGD and momentum SGD, we tune the initial learning rate
from {0.1, 0.2, 0.5, 5, 10, 20} and decrease the learning rate by factor of4 when the validation error
6
Under review as a conference paper at ICLR 2021
AOeJnOOV 6u_u_e」J.
Number of Epochs
Figure 2: Comparison of optimization methods for VGG19 training on CIFAR100.
Number of Epochs
Figure 3: Comparison of optimization methods for two-layers LSTM training on WikiText-2.
saturates. For Adagrad and Adam, we tune the initial learning rate from {0.001, 0.01, 0.1, 1.0}. We
report the best performance for these methods across the range of learning rate. The best initial
learning rates for Adagrad and Adam are 0.01 and 0.001 respectively. For NIGT, we tune the
initial value of learning rate from the same range as in SGD, and tune the momentum parameter β
from {0.01, 0.1, 0.9}, and the best parameter choice is β = 0.9. The learning rate and β are both
decreased by a factor of 4 when the validation error saturates. For Adam+, we follow the same
tuning strategy as NIGT.
We report both training and test perplexity versus the number of epochs in Figure 3. From the Fig-
ure, we have the following observations: First, in terms of training perplexity, our algorithm achieves
comparable performance with SGD and momentum SGD and outperforms Adagrad and NIGT, and
it is worse than Adam. Second, in terms of test perplexity, our algorithm outperforms Adam, Ada-
grad, NIGT and momentum SGD, and it is comparable to SGD. An interesting observation is that
Adam does not generalize well even if it has fast convergence in terms of training error, which is
consistent with the observations in (Wilson et al., 2017).
3.3	Automatic Speech Recognition
SWB-300 In the third experiment, we consider the automatic speech recognition task on SWB-300
dataset (Saon et al., 2017). SWB-300 contains roughly 300 hours of training data of over 4 million
samples (30GB) and roughly 6 hours of held-out data of over 0.08 million samples (0.6GB). Each
training sample is a fusion of FMLLR (40-dim), i-Vector (100-dim), and logmel with its delta and
double delta. The acoustic model is a long short-term memory (LSTM) model with 6 bi-directional
layers. Each layer contains 1,024 cells (512 cells in each direction). On top of the LSTM layers,
there is a linear projection layer with 256 hidden units, followed by a softmax output layer with
32,000 (i.e., 32,000 classes) units corresponding to context-dependent HMM states. The LSTM
is unrolled with 21 frames and trained with non-overlapping feature sub-sequences of that length.
This model contains over 43 million parameters and is about 165MB large. The training takes
about 20 hours on 1 V100 GPU. To compare, we adopt the well-tuned Momentum SGD strategy
as described in (Zhang et al., 2019) for this task as the baseline: batch size is 256, learning rate is
7
Under review as a conference paper at ICLR 2021
=xτu
Figure 4: Comparison of optimization methods for six-layers LSTM training on SWB-300.
10
x104 ReSNet18 training on CIFAR10
12
2
0	2	4	6	8	10	12	14
NUmber Of Iterations (t)	X104
x104VGG19 training on CIFAR100
10
2
0
0	5	10	15
NUmber of Iterations (t) X104
=iTM
0
Figure 5: The growth of quantity Pit=1 kzi k in Adam+
0.1 for the first 10 epochs and then annealed by V0.5 for another 10 epochs, with momentum 0.9.
We grid search the learning rate of Adam and Adagrad from {0.1, 0.01, 0.001}, and report the best
configuration we have found (Adam with learning rate 0.001 and Adagrad with learning rate 0.01).
For NIGT, we also follow the same learning rate setup (including annealing) as in Momentum SGD
baseline. In addition, we fine tuned β in NIGT by exploring β in {0.01, 0.1, 0.9} and reported the
best configuration (β=0.9). For Adam+, we follow the same learning rate and annealing strategy as
in the Momentum SGD and tuned β in the same way as in NGIT, reporting the best configuration
(β=0.01). From Figure 4, Adam+ achieves the indistinguishable training loss and held-out loss w.r.t.
well-tuned Momentum SGD baseline and significantly outperforms the other optimizers.
3.4	GROWTH RATE OF Pit=1 kzi k
In this subsection, we consider the growth rate of Pit=1 kzi k, since they crucially affect the conver-
gence rate as shown in Theorem 1. We report the results of both ResNet18 training on CIFAR10
dataset and VGG19 training on CIFAR100 dataset. From Figure 5, we can observe that it quickly
reaches a plateau and then grows at a very slow rate with respect to the number of iterations. This
phenomenon verifies the variance reduction effect and also explains the reason why Adam+ enjoys
a fast convergence speed in practice.
4	Conclusion
In this paper, we design a new algorithm named Adam+ to train deep neural networks efficiently.
Different from Adam, Adam+ updates the solution using moving average of stochastic gradients
calculated at the extrapolated points and adaptive normalization on only first-order statistics of
stochastic gradients. We establish data-dependent adaptive complexity results for Adam+ from
the perspective of adaptive variance reduction, and also show that a variant of Adam+ achieves
state-of-the-art complexity. Extensive empirical studies on several tasks verify the effectiveness of
the proposed algorithm. We also empirically show that the slow growth rate of the new gradient
estimator, providing the reason why Adam+ enjoys fast convergence in practice.
8
Under review as a conference paper at ICLR 2021
References
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approxi-
mate local minima faster than gradient descent. In Proceedings of the 49th Annual ACM SIGACT
Symposium on Theory of Computing, pp.1195-1199, 2017.
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International conference on machine learning, pp. 699-707, 2016.
Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Woodworth.
Lower bounds for non-convex stochastic optimization. arXiv preprint arXiv:1912.02365, 2019.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for nonconvex
optimization. SIAM Journal on Optimization, 28(2):1751-1772, 2018.
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, and Quanquan Gu. Closing the gener-
alization gap of adaptive gradient methods in training deep neural networks. arXiv preprint
arXiv:1806.06763, 2018a.
Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence ofa class of Adam-type
algorithms for non-convex optimization. arXiv preprint arXiv:1808.02941, 2018b.
Zaiyi Chen, Zhuoning Yuan, Jinfeng Yi, Bowen Zhou, Enhong Chen, and Tianbao Yang. Universal
stagewise learning for non-convex problems with convergence on averaged solutions. In Interna-
tional Conference on Learning Representations, 2019.
Ashok Cutkosky and Harsh Mehta. Momentum improves normalized SGD. arXiv preprint
arXiv:2002.03305, 2020.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex SGD.
In Advances in Neural Information Processing Systems, pp. 15236-15245, 2019.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex op-
timization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689-699, 2018.
Cong Fang, Zhouchen Lin, and Tong Zhang. Sharp analysis for nonconvex SGD escaping from
saddle points. arXiv preprint arXiv:1902.00247, 2019.
Saeed Ghadimi and Guanghui Lan. Stochastic first-and zeroth-order methods for nonconvex stochas-
tic programming. SIAM Journal on Optimization, 23(4):2341-2368, 2013.
Priya Goyal, Piotr Doll狂 Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape
saddle points efficiently. arXiv preprint arXiv:1703.00887, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
9
Under review as a conference paper at ICLR 2021
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. CIFAR-10 and CIFAR-100 datasets. URl:
https://www. cs. toronto. edu/kriz/cifar. html, 6:1, 2009.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
SCSG methods. In Advances in Neural Information Processing Systems, pp. 2348-2358, 2017.
Kfir Levy. Online to offline conversions, universality and adaptive minibatch sizes. In Advances in
Neural Information Processing Systems, pp. 1613-1622, 2017.
Xiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive
stepsizes. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 983-
992, 2019.
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei
Han. On the variance of the adaptive learning rate and beyond. arXiv preprint arXiv:1908.03265,
2019.
Liangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic
bound of learning rate. arXiv preprint arXiv:1902.09843, 2019.
H Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex opti-
mization. arXiv preprint arXiv:1002.4908, 2010.
Stephen Merity. The WikiText long term dependency language modeling dataset. Salesforce Meta-
mind, 9, 2016.
Nhan H Pham, Lam M Nguyen, Dzung T Phan, and Quoc Tran-Dinh. ProxSARAH: An efficient
algorithmic framework for stochastic composite nonconvex optimization. Journal of Machine
Learning Research, 21(110):1-48, 2020.
Sashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola. Stochastic variance
reduction for nonconvex optimization. In International conference on machine learning, pp. 314-
323, 2016.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of Adam and beyond. arXiv
preprint arXiv:1904.09237, 2019.
George Saon, Gakuto Kurata, Tom Sercu, Kartik Audhkhasi, Samuel Thomas, Dimitrios Dimitri-
adis, Xiaodong Cui, Bhuvana Ramabhadran, Michael Picheny, Lynn-Li Lim, Bergul Roomi, and
Phil Hall. English conversational telephone speech recognition by humans and machines. In
Interspeech, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop, coursera: Neural networks for machine
learning. University of Toronto, Technical Report, 2012.
Mengdi Wang, Ethan X Fang, and Han Liu. Stochastic compositional gradient descent: algorithms
for minimizing compositions of expected-value functions. Mathematical Programming, 161(1-2):
419-449, 2017.
Zhe Wang, Kaiyi Ji, Yi Zhou, Yingbin Liang, and Vahid Tarokh. SpiderBoost and momentum:
Faster variance reduction algorithms. In Advances in Neural Information Processing Systems, pp.
2406-2416, 2019.
Rachel Ward, Xiaoxia Wu, and Leon Bottou. AdaGrad stepsizes: Sharp convergence over noncon-
vex landscapes. In International Conference on Machine Learning, pp. 6677-6686, 2019.
Ashia C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin Recht. The marginal
value of adaptive gradient methods in machine learning. In Advances in Neural Information
Processing Systems, pp. 4148-4158, 2017.
Yang You, Igor Gitman, and Boris Ginsburg. Scaling SGD batch size to 32k for imagenet training.
arXiv preprint arXiv:1708.03888, 6, 2017.
10
Under review as a conference paper at ICLR 2021
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan
Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch optimization for deep
learning: Training BERT in 76 minutes. arXiv preprint arXiv:1904.00962, 2019.
Jingzhao Zhang, Hongzhou Lin, Suvrit Sra, and Ali Jadbabaie. On complexity of finding stationary
points of nonsmooth nonconvex functions. arXiv preprint arXiv:2002.04130, 2020.
Wei Zhang, Xiaodong Cui, Ulrich Finkler, Brian Kingsbury, George Saon, David Kung, and Michael
Picheny. Distributed deep learning strategies for automatic speech recognition. In ICASSP’2019,
May 2019.
Dongruo Zhou, Pan Xu, and Quanquan Gu. Stochastic nested variance reduction for nonconvex
optimization. In Advances in Neural Information Processing Systems, pp. 3921-3932, 2018.
11
Under review as a conference paper at ICLR 2021
A Proof of Lemma 1
Proof. The proof is similar to that of Lemma 12 in (Wang et al., 2017). Define
β(1 - β)t-k	ift ≥ k > 0
(1 - β)t-k	ift ≥ k = 0
(3)
By the definition of ζt(k) and the update of Algorithm 1, we have
tt	t
ζ(t+1) = (1-β)ζ(t), X ζ(t) = 1, Wt = X ζ(t)w t+ι, Zt+1 = X Zk)Wf(W t+i； ξt+ι).
k=0	k=0	k=0
Define mt+1 = Pk=O ζ(t)kwt+1 — Wk + ik2, nt+1 = Pk=O Zk [Vf(Wk+1； ξk+ι) — F(Wk + 1儿
where Wf (Wb k+1； ξk+1) is an unbiased stochastic first-order oracle for F (Wb k+1) with bounded vari-
ance σm2 . Note that VF is a LH -smooth mapping (according to Assumption 1 (iii)), then by Lemma
10 of (Wang et al., 2017), we have
kZt - VF(Wt)k2 ≤ (LH mt + kntk)2 ≤ 2L2H mt2 + 2kntk2.
Define qt+i = Ptk=O Zk(t) kWt+i - Wb k+i k. According to Lemma 11 (a) and (b) of (Wang et al.,
2017), we have
β	18
mt+i + 4q2+ι ≤ (1 一 ,J (mt + 4q2) + ~jkWt+i — Wtk2.
Taking squares on both sides of the inequality and using the fact that (a+b)2 ≤ (1 + β)a2 + (1 + β)b2
for β > 0, we have
(mt+1 +4q2+l)2 ≤ (1 + 2) (1 - 2) (mt +4q2)2 + (1 + β)号 kWt+1 - Wtk4
≤ji- 2) (mt + 4q2)2 + 9β72 kWt+i-Wt k4,
where the last inequality holds since 1∕β ≥ 1.
Define δt2 = 2L2H (mt + 4qt2)2 + 2kntk2, then we have kZt - VF(Wt)k2 ≤ δt2 for all t. Denote
Ft+i by the σ-algebra generated by ξi, . . . , ξt+i. Taking the summation of (4) and according to the
bound of nt derived in Lemma 11 (c) of (Wang et al., 2017), we have
E [δ2+ι∣Ft+ι] ≤ (1 - 2 )δt + 2β2σm + 1944LHkWJ-Wtk4,
Taking expectation on both sides yields
E [δ2+ι] ≤ (1 - 2) E [δ2] + 2β2σm + E [1944LH力24;
NOtethat Di = maχ(kαβaι∕2,eo)，Wehave
E [δ2+ι] ≤ (1 - 2) E [δ2] + 2β2σm + E [CL2α4β4a-3kZtk2] .	□
B Proof of Theorem 1
Proof. By Lemma 1 and the update rule of Algorithm 1, we have
E
[δt2] +2β2σm2 +E
[δt2] +2β2σm2 +E
.CLH α4β4akztk4	^
.β3(max(kZtk1∕2,eo))L
2CLH α4β 4(kδtk2 + kVF (Wt)k2)-
β3	_
(5)
12
Under review as a conference paper at ICLR 2021
where the second inequality holds since (max(kzt k1/2 , 0))4 ≥ kztk2 and kztk2 ≤ 2kδtk2 +
2 kVF(Wt)『.
Note that 2C L2H α4 ≤ 1/18. Plugging it in (5), we have
8βE [δ2] ≤ E[δ2- δ2+ι] + 2β2σm + E [ɪ∣∣VF(wt)k21 .	(6)
18	18
Summing over t = 1, . . . , T on both sides of (6) and with some simple algebra, we have
TT
+ X5βσm + XE -kVF(wt)k2 .	(7)
t=1	t=1
TT
XE[δt2]≤XE
3 (δ2 - δ2+l)
β
By Assumption 1 (i) and by the property of L-smooth function, we know that
F(wt+ι) ≤ F(Wt) + V>F(Wt)(Wt+ι — Wt) + L ∣∣Wt+ι — Wt『
η2 L
=F(Wt)- ηtV>F(Wt)zt + ηtr IIztIl2
≤	F(Wt) — ηtV>F(Wt)(zt — VF(Wt) + VF(Wt)) + ηt2L kzt — VF(Wt)k2 + kVF(Wt)k2
=	F(Wt) — (ηt — ηt2L)IVF(Wt)I2 — ηtV>F(Wt)(zt — VF(Wt)) + ηt2L Izt — F(Wt)I2
≤	F(Wt)—瑞—η2L) kVF(Wt)k2 + 居 + η2L) kzt — F(Wt)k2.
Noting that η = maχ(∣∣Z[∣ι∕2 e )，ɑ ≤ -/4L and e0 = βa, We know that ηtL ≤ 1/4. Hence We
have
kVF(Wt)k2 ≤ 4(F(Wt)- F(Wt+l)) + 3∣zt — VF(Wt)k2.
ηt
Taking summation over t = 1, . . . , T and taking expectation yield
T
XEkVF(Wt)k2 ≤E
t=1
≤E
T
X
t=1
T
X
t=1
4(F(Wt) — F(Wt+1))
ηt
4(F(Wt) — F(Wt+1))
ηt
T
+ 3XEkzt — VF(Wt)k2
t=T1	(8)
+ 3 X E [δt2] .
t=1
Combining (7) and (8) yields
T
EkVF(Wt)k
t=1
2≤E
t=1
By some simple algebra, we have
4(F(Wt) — F(Wt+1))
t=1
T
+ X 15βσm2
ηt
T
+XE
t=1
T
+XE
t=1
3
CkVF(Wt)k2 .
8
X E ∣vf (Wt)k2 ≤E 归8(F(Wt) —tF(Wt+1)) j+X E
T
+ X 30βσm2 .
t=1
Then we have
1 5tλ	2	Γ5tλ 8max (kztk1/2,eo) (F(Wt) — F(Wt+ι))"∣ 18σ2	Q
T EEkVF(Wt)k2 ≤ E E——W tk , αβ(aT	~(-t+-n- +-βT0+30βσm. (9)
Noting that |F(Wt) — F(Wt+1)| ≤ Gηtkztk, we have
T
XEkVF(Wt)k2
t=1
8GE PtT=1 kztk	∆	18σ02	2
—lT	+ OT + 苛 + 30βσm.
(10)
□
T
X
β
1
T
≤
13
Under review as a conference paper at ICLR 2021
C Proof of Theorem 2
Before introducing the proof, we first introduce several lemmas which are useful for our analysis.
Lemma 2. Adam+ with ηt
_______ααβ________
max(∣∣zt∣∣1∕2,e0)
and €0 = 0 satisfies
F(wt+ι) - F(Wt) ≤ αβ (-"VF(W"B2 + 9∣% - VF(w/。3/2) + 64α4^L .
Proof. By the L-smoothness and the update of the algorithm, we have
F(wt+ι) - F(Wt) ≤ VτF(Wt)(Wt+1 - Wt) + LkWt+12- Wtk
{VF (Wjzt〉	α2β2Lkztk2
≤ —Qe ∙ --------------- + ------------------
max (kzt∣∣1/2, €0)	(max (∣∣ztk1/2, 6o))2
(11)
Define ∆t = Zt - VF(w。. IfkVF(Wt)k ≥ 2∣∣∆t∣∣, we have
〈zt, VF(wt)i	kVF(wt)k2 + (∆t, VF(Wt))
---:------=	:--------------
max (kztk1∕2,eo)	max (∣∣VF(Wt) + ∆t∣∣1∕2,eo)
V	kVF (wt)k2	V	kVF (wt)k3/2
≤≤
≤	2kVF(wt)+∆tk1/2 ≤	3
≤-≡w≡2 +8k∆tk3∕2.
(12)
IfkVF(wt)k ≤ 2k∆tk, we have
〈zt, VF(wt)) _	kVF(wt)k2 + h∆t, VF(wt))
max (kztk1∕2,e0)	max (∣∣VF(Wt) +∆tk1∕2,eo)
≤ 6k∆tk2
- IAk1/2
6k∆tk3∕2 ≤-≡w≡! +8k∆tk3∕2.
(13)
By (12) and (13), we have
〈zt, VF (Wt))
max (kztk1/2,e0)
≤-T止 + 8k∆t k3∕2.
(14)
By(11)and(14), we have
F(wt+ι) - F(wt) ≤ αβ ( k VF(W斓尸2 + 8k∆tk3/2) + α2β2Lkztk
-Qe f-≡w^ + 8k∆tk3∕2) + ∕2lm1n (P + J)
∖	3	χ>0 ∖ 3x	3 J
≤ Qe (-≡w^ + 8k∆t k"?) + Q2β2L (I^ +「)
∖	3	3(8QeL)	3 J
≤ Qe Pvf⅞^ + 9k∆tk3∕2) + 64^,
where the last inequality holds because |%||3/2 ≤ 2||VF(wt)k3/2 + 2∣∣∆t∣∣3/2.
Lemma 3. For Adam+ with ηt
,there exist random variables & such that
E
[吊/2] + 2e 3∕2σ3/2 + E
320L普2∣∣wt+1 - Wtk3
e2
□
14
Under review as a conference paper at ICLR 2021
Proof. The proof shares the similar spirit of Lemma 12 in (Wang et al., 2017), but we adapt the
proof for our purpose. Define
(t) = β(1 - β)t-k
k = (1 - β)t-k
if t ≥ k > 0
if t ≥ k = 0
(15)
By the definition of ζt(k) and the update of Algorithm 1, we have
tt	t
ζ(t+1) = (1 - β)ζ(t),	X ζ(t) = 1, Wt = X ζ(t)wt+1, zt+ι = X ZktNf(Wt+1； ξt+ι).
k=0	k=0	k=0
Define mt+i = Pik=O Z(t)kwt+1 - Wk+ιk2, nt+1 = Pik=O Z(t) [Vf (Wk+i； ξk+ι) - F(Wk+ι)],
where Nf (Wb k+1; ξk+1) is an unbiased stochastic first-order oracle for F(Wb k+1) with bounded vari-
ance σ2. Note that VF is a LH -smooth mapping (according to Assumption 1), then by Lemma 10
of (Wang et al., 2017), we have
kzt - VF (Wt)k3/2 ≤ (LHmt+kntk)3/2 ≤ 2L3H/2mt3/2 + 2kntk3/2.
Define qt+1 = Ptk=O ζk(t) kWt+1 - Wb k+1k. According to Lemma 11 (a) and (b) of (Wang et al.,
2017), we have
β	18
mt+i + 4q2+ι ≤ (i - ,J (mt + 4q2) + ~jkWt+i - Wtk2.
Taking the power 3/2 on both sides of the inequality and using the fact that (a + b)3/2 ≤
1+-2- βa3/2 + {1 + 2b3/2 for β > 0, we have
(mt+1 +4q2+i)3/2
≤ (1 + 2)	(1 - 2)	(mt + 4q2)3∕2 + (1 + 2)	β802kWt+i - Wtk3	(16)
≤ (1 - 2) (mt + 4q2)3∕2 + -^ kWt+i - Wtk3 ,
where the last inequality holds since 1∕β ≥ 1.
By the definition of nt, we have nt+i = (1 - β)nt + β (Vf (Wb t+i) - F(Wb t+i)). Denote Ft+i by
the σ-algebra generated by ξi, . . . , ξt+i. Noting that
E hknt+i k3/2|Ft+ii ≤ (E [knt+ιk2∣Ft+ι])3/4 ≤ (1 - e/2)3/2 ||司户/2 + β3∕2σ3/2,	(17)
where the last inequality holds by invoking Lemma 11(c) of (Wang et al., 2017). Define δt3∕2 =
2L%2(mt + 4q2)3/2 + 2∣∣ntk3∕2, then We have ∣∣zt 一 VF(Wt)『/2 ≤ δ3/ for all t According
to (16) and (17), we have
E [δ3+1∣Ft+ι] ≤ (1 - 2) kδtk3/2 +2β3∕2σ3∕2 + 320L3/2kW+1-Wtk3 .
Taking expectation on both sides yields
E 阵1] ≤ (1 - β) E 忖/1 + 2β3∕2σ3∕2 + E "/"y- Wtk3 .	□
Lemma 4. Adam+ with learning rate η = ιnaχ(∣晨[1/2 e ) and 640α3L3/2 ≤ 1/120 satisfies
T X E h∣vF (Wt)k3/2i ≤ 黑 + 2727ET" i +4545β1∕2σ3∕2 + 3^.
t=i
To ensure that T PT=IE [∣VF(Wt)∣3/2] ≤ e3/2, we can choose β = e3, T = O(e-9/2).
15
Under review as a conference paper at ICLR 2021
Proof. By Lemma 3 and noting that ηt
max(kZtβ“⑹,wehave
E hδ3+1i ≤(1 - 2) E 苗2] + 2β3∕2σ3/2 + E 320L3/273-/3/2
≤ (1 - 2) E [s3/2] + 2β3∕2σ3/2 + E h640LH∕2α3β (/VF(wt)//2 + ||瓦/3/2)].
Note that 640α3L%2 ≤ 1/120. Plugging it into (18), We have
590E [δ3∕2i ≤ E [δ3∕2 - δ3+li + 2β3∕2σ3∕2 + E 磊/VF(wt)k3/2 .
Summing over t = 1, . . . , T on both sides of (19) and With some simple algebra, We have
XXXE [铲]≤ XXE "3('3/2-δ3+1)# + XX5β1∕2σ3∕2 + XXe]59/vf(wt)k2/3
t=1	t=1	t=1	t=1
By Lemma 2, taking expectation on both sides, We have
E [F(wt+ι) - F(wt)] ≤ αβ (-E "^F，物尸2] + 9E ['3/2" + 的吸”3 .
(18)
(19)
(20)
Summing (20) over t = 1, . . . , T yields
504αβ XXE [/VF(wt)k3/2i ≤ F(WI)-F*+αβ f	i + XX 45β1∕2σ3/2]+64αβ4L3τ.
504 t=1	β	t=1	3
Hence, We have
T
T XE [/VF(wt)/3/2] ≤
t=1
≤
野+ 2727Ej3/2] +4545β ι∕2σ3∕2 + 215503β3L3
αβT	β
101∆	2727E [δ3/2]
αβT + βT
+ 4545β 1/2。3/2 +
3β3L3/2
100
□
Lemma 5. Under the same setting of Lemma 4, we know that to ensure that
3/2, we need T = O(-9/2) iterations.
T PT=1 e [δ3/2] ≤
Proof. From (19) and Lemma 4, We have
TT
X 59β E [δ3∕2i ≤ E [δ3∕2i +2β3∕2σ3∕2T + X E 1β0/VF (Wt)/3/2
t=1	t=1
Noting that β = T-b With 0 < b < 1, then We knoW that there exists a universal constant C > 0
such that
T XX120E hδ3/2] ≤ ETU+T/2+T XXE [ 1⅛/vf (Wt )/3/21.	(21)
-Lt ] J. ^-ι∖J L	-I	JL	JL '	_L	ɪ	J- ^-ι∖J
Take b = 3. From Lemma 4, We know that it takes T = O(e-9/2) iterations to ensure that
TT PT=1 E [/VF(Wt)Il3/2] ≤ e3/2. In addition, From (21), we know that it takes T
iterations to ensure that
T PT=1 e [δ3/2]
≤ e3/2.
O(e-9/2 )
□
We can easily prove Theorem 2 by incorporating the results in Lemma 4 and Lemma 5. It is also
evident to see that if β = 1/Ts with 0 < s < 1, then it takes T = O (poly(1/e)) number of
iterations to ensure that T PT=IE [δ3/2] ≤ e3/2 and T1 PT=1 E [/VF(wt)/3/2] ≤ e3/2 hold
simultaneously.
16
Under review as a conference paper at ICLR 2021
D Proof of Theorem 3
Proof. Define Yt = min (7；产/3,答)with e° = 2βa. Then We know that η = αγt and Yt ≤ 1.
Note that α ≤ L, so we have η ≤ 芸.By the L-Smoothness of F, we have
F(wt+ι) ≤ F(Wt) + V>F(Wt)(Wt+ι - wt) + 2 ∣∣wt+ι - Wt∣∣2
≤ F(Wt) - ηtv>F(Wt)Zt + (* + 2γL) kztk2 - 2LYtkztk2
=F(Wt) - ηtV>F(Wt) (Zt - VF(Wt) + VF(Wt)) + (斗 + 2L) ∣∣Zt∣∣2 — —11*12
≤ F(Wt)- ηtV>F(Wt)(zt - VF(Wt) + VF(Wt)) + (η2L + Yt) (IIZt- VF(Wt)k2 + ∣∣VF(Wt)『)-2LYt∣∣Zt∣∣2
≤) F(Wt) - η2t kVF(Wt)k2 + η2kzt - VF(Wt)k2 + (IIL + L) (∣IZt- VF(Wt)Il2 + IlVF(Wt)Il2)-义包区『
=F(Wt) - (1 - η2L - L) IIvF(Wt)112 + (η2L + L + .) Ilzt- vf(Wt)Il2 - 2LγtIIztIl2
≤ F(Wt) -彳YtkztI12 + 丁 Ilzt - VF(Wt)『,
2L	L
(22)
where (a) holds since IztI2 ≤ 2Izt - VF(Wt)I2 +2IVF(Wt)I2, (b) holds since -V> F(Wt)zt ≤
2 (IlVF(Wt)I2 + Izt - Vf(Wt)I2), (C) holds due to η2t - η22L - L ≥ 0 (since η ≤ 2L, we have
-2t — η2 L ≥ 2L and note that γt ≤ ^L).
By the definition of Yt, we have
Ytkztk2 ≥ 僧1%『/3 min (『,⅛/3)=^^1 min (,")
β β e0	β	2β	(23)
S Mlztl ( l⅛T - 1) = β。口产-：,
β2	2
where (a) holds since X ≥ X - 2, x2 ≥ X - 2 hold for any X and let X =1吗［/
Combining (22) and (23), we have
βaIztI4/3 ≤ Ytkztk2 + 4?"3 ≤ 2L (F(Wt) - F(Wt+1)) + β叫丁?/3 + 2Izt - VF(Wt)∣∣2
βa
=2L (F(Wt) - F(Wt+ι)) + βakztk4/3 ∙ 2^23 + 2kzt - VF(Wt)k2.
If 2战2/3 ≤ 1, we have βakztk4/3 ≤ 4L (F (Wt)- F (Wt+ι))+4∣∣zt-VF (Wt)k2 .If 劣在八 >
1, then βa > Iztk2/3, and hencewe have βa Iztk4/3 ≤ β3a. As a result, we have
βaIztI4/3 ≤ 4L (F(Wt) - F(Wt+1)) + 4kzt - VF(Wt)I2 + β3a.	(24)
Taking summation on both sides of (24) over t = 1, . . . , T yields
TT	T
X Iztk4/3 ≤ 4L X F(Wt)-βF(Wt+1) + X βa Izt - VF (Wt )k2 + β2aT.	(25)
t=1	t=1	β	t=1 β
Define ∆t = zt - VF(Wt), then we have
IVF(Wt)I4/3 ≤ 2kztk4/3 + 2k∆tk4/3.	(26)
17
Under review as a conference paper at ICLR 2021
Hence,
T
X kztk4/3 + kVF(wt)k4/3
t=1
TT
≤) 2X k∆tk4/3 + 12L X
t=1
t=1
F(wt) - F (wt+1)
βa
T
T 12
+ ∑ βa kzt-VF (wt)k2 +3β 2aT
t=1 β
(≤) X 4 (中 + β2a )+12L X
F(wt) - F (wt+1)
βa
T
T 12
+ ∑ βakzt-VF(wt)k2 +3β2aT
t=1 β
T
≤ 12LX
t=1
F(wt) - F (wt+1)
βa
T
+ X βakzt-VF(wt)k2 +4β2aT,
t=1 β
where (a) holds due to (25) and (26), (b) holds because minχ>0 c2 + X2 = 3c4/3
By Lemma 1, we know that
(27)
+ 2β2σ2 + E
+ 2β2σ2 + E
-CL2η4kztk4 一
_	β3	_
Γ CLɑ4β4akztk4 ]
max((ztk8/3,e0)e3 _
+ 2β2σ2 + E [cL2α4β4a-3kztk4/3]
Note that CL2α4 ≤ 1/14, we have
2 E[δ2] ≤ E[δ2- δ2+j +2β 2σ2 + E
β4a-3kztW3 一
14
(28)
Taking summation on both sides of (28) over t = 1, . . . , T, we have
T	E δ12	T
∑E [δ2] ≤ -ɪɪɪ + 2βσ2T + ɪj E
E δ12	T
=-ɪɪɪ + 2βσ2T + ]E
β4a-4∣∣ztk4∕3 -
14
βakztk4∕3 一
14-
(29)
where the last equality holds since a = 4/3.
Taking expectation on both sides of (27) and combining (29), we have
T
XE hkztk4/3 + kVF(wt)k4/3i ≤ -β-
As a resUlt, we have
1T
T XE [kVF(wt)k4/3] ≤
t=1
14E [δ2]+ 28βσ2T
β1+a
βa
T
+XE kztk4/3 +4β2aT.
t=1
12L∆
βaT +
14E δ12
β1+aT
手 +4β2a
+
+
Suppose initial batch size is T0, the intermediate batch size is m, and a = 4∕3, then we have
T XE hkVF (Wt)k4/3i ≤ β2LT+β⅛+恐+4β8/3.
(30)
Wecanchoose β = O(e1/2), T = O (e- 2) ,the initial batch size T0 = 1∕β = O(e-1/2), the interme-
diate batch size as m = 1∕β3 = O(e-3/2), which ends UP with the total complexity O(e-3∙5). □
18
Under review as a conference paper at ICLR 2021
E A New Variant of Adam+
Theorem 4. Assume that ∣∣Vf (w; ξ)k ≤ G almost surely for every W ∈ Rd. Choose η
max(∣I；[" W ) With a = 4/3, and we have
T X E h∣VF (Wt)∣3∕2i ≤ ' + * + 28βσ2 +4β3a
Denote the initial batch size and the intermediate batch size are T0 andm respectively, then we have
T
T XE [∣VF(Wt)k3∕2] ≤
t=1
12L∆+	14σ2	+28σL +4/ 3。
βaT + ToTβ1+a + βa-1m + β
To ensure that 1 PT=IE [∣VF(wt)∣3/2] ≤ e3/2, we choose β = e3/8, T = O(1∕e2), the initial
batch size is To = 1/e3/8 and m = 1/e1.625, then the total computational complexity is O(1/e3.625).
Proof. Define Yt = min .；：产/2,答) with e° = 2βα. Then We know that η = αγt and Yt ≤ 11.
Note that α ≤ L, so we have η ≤ 1L. By the L-Smoothness of F, we have
F(wt+ι) ≤ F(Wt) + V>F(Wt)(Wt+ι — Wt) + & ∣∣Wt+ι — Wt∣∣2
≤ F(Wt) - 2v>f (Wt)Zt + (η∣- + 2YγL) kztk2 - 2Lγtkztk2
=F(Wt) - ηtv>F(Wt)(Zt - vf(Wt) + vf(Wt)) + (ηt2— ■+ 2YγL) Ilztk2 - 2LYtkztk2
≤ F(Wt)- ηtv>F(Wt)(Zt- VF(Wt) + vf(Wt)) + (η2L + Yt) (kzt - VF(Wt)k2 + kVF(Wt)II2) - 2LYtkztk2
≤	F(Wt) -	η2t	kVF(Wt)k2 +	η2tkzt - VF(Wt)k2 +	(η2L + L)	(∣Izt- VF(Wt)Il2 + kVF(Wt)Il2) -	2LγtkZtk2
=F(Wt)	-	(ηt	-	η2L -	Yt) l∣vF(Wt)『+	(η2L +	Yt	+	ηt)	Ilzt - vf(Wt)∣∣2 -	2Lγt 1忆”2
≤ F(Wt) - 彳Ytkztk2 + T Ilzt - VF(Wt)『,
2L	L
(31)
where (a) holds since IztI2 ≤ 2Izt - VF(Wt)I2 +2IVF(Wt)I2, (b) holds since -V> F(Wt)zt ≤
2 (kVF (Wt) k2 + kzt - VF (Wt) k2), (C) holds due to η2t - η2L - L ≥ 0 (since ηt ≤ 2L, we have
^2t - η2L ≥ 2L and note that Yjt ≤ 2L).
By the definition of Yt, we have
Ytkztk2 ≥ β2αkztk min (k"。/ , kztk) = β2akztk min ( kz& , , k∣t2
βa	βaeo	βa	2β2
S β 1akztk (T - 1) = βakztk3/2 - /
(32)
where (a) holds since X ≥ X - 2, x22 ≥ X - 2 hold for any X and let X = kzk—
Combining (31) and (32), we have
βakztk3∕2 ≤ Ytkztk2 + β2a2zk ≤ 2L (F(Wt) - F(Wt+ι)) + β2a2zk + 2kzt - VF(Wt)k2
βa	C
=2L (F(Wt) - F(Wt+ι)) + βakztk3∕2 ∙ 2∣^p2 + 2kzt - VF(Wt)k2.
19
Under review as a conference paper at ICLR 2021
If
βa
2kztk1/2
1, then βa
≤ 1, we have βa∣∣ztk"3 ≤ 4L(F(wt) — F (wt+ι))+4∣% - VF(Wt) k2 .If
> ∣∣zt k1/2, and hence we have βakztk3/2 ≤ β4a. As a result, we have
βa
2kztk1/2
>
βakztk3/2 ≤ 4L (F(Wt)- F(wt+ι)) + 4∣zt — VF(wt)∣2 + β4a.
Taking summation on both sides of (33) over t = 1, . . . , T yields
(33)
t=1
kztk3/2 ≤ 4L
t=1
F(wt) - F(wt+1)
βa
T4
+ X βa kzt-VF (Wt )k2 + β3aT.
t=1 β
(34)
T
T
Define ∆t = zt - VF(wt), then we have
kVF(wt)k3∕2 ≤ 2kztk3∕2 + 2k∆tk3∕2.
(35)
Hence, we have
T
X kztk3/2 + kVF(wt)k3/2
t=1
(a) T	T
≤ 2X ∣∆tk3/2 + 12L X
t=1
t=1
F(wt) - F (wt+1)
βa
T
12
+ ∑ βa kzt-VF (wt)k2 +3β 3aT
t=1 β
≤) X 2 (彗 + β3a )+12L X
t=1	t=1
F(wt) - F (wt+1)
βa
T
12
+ ∑ βakzt - VF(wt)k2 +3β3aT
t=1 β
T
≤ 12LX
t=1
F(wt) - F (wt+1)
βa
T
+ X β∣kzt-VF(wt)k2 +4β3aT,
t=1 β
where (a) holds due to (34) and (35), (b) holds because minx>0 C + x3 = 4c3/2
By Lemma 1, we know that
(36)
E
Note that CL2Hα4
+2β2σ2+E
+2β2σ2+E
CL2Hηt4kztk4
β3
CL2Hα4β4akztk4
max(∣zt k2,e4)β 3_
+ 2β2σ2 + E [CL2H α4β4a-3kztk2 .
2E [δ2] ≤ E[δ2- δ2+ι]+2β2σ2 + E
β4a-3kztk2 -
14G1/2
(37)
Taking summation on both sides of (37) over t = 1, . . . , T, we have
T	E [δ12	T
EE 尾]≤ -ɪɪɪ + 2βσ2T + 谷E
E [δ12	T
=+1 + 2βσ2T + IE
E [δ12	T
≤ -ɪɪɪ + 2βσ2T +《E
β4a-4kzt k2 一
14
βakztk2 一
14G1∕2
βaι∣ztk3/2 一
14
(38)
where the equality holds since a = 4/3 and last inequality holds since kzt k ≤ G.
20
Under review as a conference paper at ICLR 2021
Taking expectation on both sides of (36) and combining (38), we have
T
XE UZtk3/2 + kVF(wt)k3/2]
t=1
≤ 12L(FT F*) + T + 注T + X E hkztk3∕2i + 4e3。/
βa	β 1+a	βa
As a result, we have
T X E hkVF (wt)k3∕2i ≤ 12"- F) + W + 28βσ2 + 4β%	□
t=1	β	β	β
F Related Work
Adaptive Gradient Methods Adaptive gradient methods were first proposed in the framework of
online convex optimization (Duchi et al., 2011; McMahan & Streeter, 2010), which dynamically
incorporate knowledge of the geometry of the data to perform more informative gradient-based
learning. This type of algorithm was proved to have fast convergence if stochastic gradients are
sparse (Duchi et al., 2011). Based on this idea, several other adaptive algorithms were proposed to
train deep neural networks, including Adam (Kingma & Ba, 2014), Amsgrad (Reddi et al., 2019),
RMSprop (Tieleman & Hinton, 2012). There are many work trying to analyze variants of adaptive
gradient methods in both convex and nonconvex case (Chen et al., 2018a; 2019; 2018b; Luo et al.,
2019; Chen et al., 2018a;b; Ward et al., 2019; Li & Orabona, 2019; Chen et al., 2019). Notably,
all of these works are able to establish faster convergence rate than SGD, based on the assumption
that stochastic gradients are sparse. However, this assumption may not hold in deep learning. In
contrast, our algorithm can have faster convergence than SGD even if stochastic gradients are not
sparse, since our algorithm’s new data-dependent adaptive complexity does not rely on the sparsity
of stochastic gradients.
Variance Reduction Methods Variance reduction is a technique to achieve fast rates for finite
sum and stochastic optimization problems. It was first proposed for finite-sum convex optimiza-
tion (Johnson & Zhang, 2013) and then it was extended in finite-sum nonconvex (Allen-Zhu &
Hazan, 2016; Reddi et al., 2016; Zhou et al., 2018) and stochastic nonconvex (Lei et al., 2017; Fang
et al., 2018; Wang et al., 2019; Pham et al., 2020; Cutkosky & Orabona, 2019) optimization. To
prove faster convergence rate than SGD, all these works make the assumption that the objective
function is an average of individual functions and each one of them is smooth. In contrast, our
analysis does not require such an assumption and to achieve a faster-than-SGD rate.
Other Related Work Arjevani et al. (2019) show that SGD is optimal for stochastic nonconvex
smooth optimization, if one does not assume that every component function is smooth. There are
recent work trying to establish faster rate than SGD, when the Hessian of the objective function is
Lipschitz (Fang et al., 2019; Cutkosky & Mehta, 2020). There are several empirical papers, includ-
ing LARS (You et al., 2017) and LAMB (You et al., 2019)), which utilize both moving average
and normalization for training of deep neural networks with large-batch sizes. Zhang et al. (2020)
consider an algorithm for finding stationary point for nonconvex nonsmooth problems. Levy (2017)
considers convex optimization setting and design algorithms which adapts to the smoothness pa-
rameter. Liu et al. (2019) introduced Rectified Adam to alleviate large variance at the early stage.
However, none of them establish data-dependent adaptive complexity as in our paper.
G	Adam+ with Fixed Learning Rate
We report the results on image classification with CIFAR10 on ResNet18. We further considered
the Adam+ with fixed learning rate 0.1 and do not employ any learning rate annealing scheme.
We report our result on Figure 6, in which "Adam+ Fixed Stepsize" uses the default setting of
Algorithm 1. As we can see from the Figure, Adam+ with fixed stepsize still outperforms Adam,
21
Under review as a conference paper at ICLR 2021
Figure 6: Comparison of Adam+ and Adam+ with Fixed Stepsize
Figure 7: log(Pit=1 kzik) versus log(t)
Adagrad and momentum SGD. If we use the same learning rate scheduler of SGD for Adam+, then
Adam+ performs the best.
H GROWTH RATE ANALYSIS OF Pit=1 kzik
We provide the log-log plot (log(Pit=1 kzik) versus log(t)) for the ResNet18 training on CIFAR10
experiment, as illustrated in Figure 7. We can see that the slope is around 0.8. Although the slope is
not small, we can see from Figure 5 that the slope becomes almost zero after the iteration 6 × 104
(which corresponds to the epoch number 154). At this particular epoch, the training and test accuracy
are not the best so we need to keep the training until epoch 350. Then our algorithm Adam+ is able
22
Under review as a conference paper at ICLR 2021
to take advantage of the slow growth rate of Pit=1 kzi k for large t and enjoys faster convergence,
which is consistent with Figure 1.
23