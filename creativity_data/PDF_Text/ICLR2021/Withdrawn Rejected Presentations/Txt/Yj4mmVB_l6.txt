Under review as a conference paper at ICLR 2021
Two steps at a time — taking GAN training in
stride with Tseng’s method
Anonymous authors
Paper under double-blind review
Ab stract
Motivated by the training of Generative Adversarial Networks (GANs), we study
methods for solving minimax problems with additional nonsmooth regularizers.
We do so by employing monotone operator theory, in particular the Forward-
Backward-Forward (FBF) method, which avoids the known issue of limit cycling
by correcting each update by a second gradient evaluation. Furthermore, we
propose a seemingly new scheme which recycles old gradients to mitigate the
additional computational cost. In doing so we rediscover a known method, related
to Optimistic Gradient Descent Ascent (OGDA). For both schemes we prove novel
convergence rates for convex-concave minimax problems via a unifying approach.
The derived error bounds are in terms of the gap function for the ergodic iterates.
For the deterministic and the stochastic problem we show a convergence rate of
O(1∕k) and O(1∕√k), respectively. We complement our theoretical results with
empirical improvements in the training of Wasserstein GANs on the CIFAR10
dataset.
1	Introduction
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have proven to be a powerful
class of generative models, producing for example unseen realistic images. Two neural networks,
called generator and discriminator, compete against each other in a game. In the special case of a
zero sum game this task can be formulated as a minimax (aka saddle point) problem.
Conventionally, GANs are trained using variants of (stochastic) Gradient Descent Ascent (GDA)
which are known to exhibit oscillatory behavior and thus fail to converge even for simple bilinear
saddle point problems, see Goodfellow (2016). We therefore propose the use of methods with
provable convergence guarantees for (stochastic) convex-concave minimax problems, even though
GANs are well known to not warrant these properties. Along similar considerations an adaptation of
the Extragradient method (EG) (Korpelevich, 1976) for the training of GANs was suggested in Gidel
et al. (2019), whereas Daskalakis et al. (2018); Daskalakis & Panageas (2018); Liang & Stokes (2019)
studied Optimistic Gradient Descent Ascent (OGDA) based on optimistic mirror descent (Rakhlin &
Sridharan, 2013a;b). We however investigate the Forward-Backward-Forward (FBF) method (Tseng,
1991) from monotone operator theory, which uses two gradient evaluations per update, similar to EG,
in order to circumvent the aforementioned issues.
Instead of trying to improve GAN performance via new architectures, loss functions, etc., we
contribute to the theoretical foundation of their training from the point of view of optimization.
Contribution. Establishing the connection between GAN training and monotone inclusions moti-
vates to use the FBF method, originally designed to solve this type of problems. This approach allows
to naturally extend the constrained setting to a regularized one making use of the proximal operator.
We also propose a variant of FBF reusing previous gradients to reduce the computational cost per
iteration, which turns out to be a known method, related to OGDA. By developing a unifying scheme
that captures FBF and a generalization of OGDA, we reveal a hitherto unknown connection. Using
this approach we prove novel non asymptotic convergence statements in terms of the minimax gap
for both methods in the context of saddle point problems. In the deterministic and stochastic setting
We obtain rates of O(1∕k) and O(1∕√k), respectively. Concluding, We highlight the relevance of
1
Under review as a conference paper at ICLR 2021
our proposed method as well as the role of regularizers by showing empirical improvements in the
training of Wasserstein GANs on the CIFAR10 dataset.
Organization. This paper is structured as follows. In Section 2 we highlight the connection of
GAN training and monotone inclusions and give an extensive review of methods with convergence
guarantees for the latter. The main results as well as a precise definition of the measure of optimality
are discussed in Section 3. Concluding, Section 4 illustrates the empirical performance in the training
of GANs as well as solving bilinear problems.
2	GAN training as monotone inclusion
The GAN objective was originally cast as a two-player zero-sum game between the discriminator Dy
and the generator Gx (Goodfellow et al., 2014) given by
minmax EP〜q[log(Dy(ρ))]+ EZ〜p[log(1 - Dy(Gx(Z)))],
xy
exhibiting the aforementioned minimax structure. Due to problems with vanishing gradients in the
training of such models, a successful alternative formulation called Wasserstein GAN (WGAN) (Ar-
jovsky et al., 2017) has been proposed. In this case the minimization tries to reduce the Wasserstein
distance between the true distribution q and the one learned by the generator. Reformulating this
distance via the Kantorovich Rubinstein duality leads to an inner maximization over 1-Lipschitz
functions which are approximated via neural networks, yielding the saddle point problem
min ll max 一 EP~qDy(P)] - EZ~p[Dy(Gx(Z))].
x y：kDy l∣Lip≤1
2.1	Convex-concave minimax problems
Due to the observations made in the previous paragraph we study the following abstract minimax
problem
min max Ψ(x,	y)	：=	f(x)	+ Eξ〜Q	[Φ(x,	y； ξ)]	-	h(y),	(1)
x∈Rd y∈Rn
where the convex-concave coupling function Φ(x, y) := Eξ〜Q [Φ(x, y; ξ)], which hides the Stochas-
ticity for ease of notation, is differentiable with L-Lipschitz continuous gradient. The proper, convex
and lower semicontinuous functions f : Rd → R ∪ {+∞} and h : Rn → R ∪ {+∞} act as
regularizers. A solution of (1) is given by a so-called saddle point (x*,y*) fulfilling for all X and y
Ψ(χ*,y) ≤ Ψ(χ*,y*) ≤ Ψ(χ,y*)∙
In the context of two-player games this corresponds to a pair of strategies, where no player can be
better off by changing just their own strategy.
For the purpose of this motivating section, we will restrict ourselves for now to the special case of the
deterministic constrained version of (1), given by
min max Φ(x, y),
x∈X y∈Y
where f and h are given by indicator functions of closed convex sets X and Y , respectively. The
indicator function δC ofa set C is defined as δC(z) = 0 for z ∈ C and δC(z) = +∞ otherwise.
2.2	Minimax problems as monotone inclusions
If the coupling function Φ is convex-concave and differentiable then solving (1) is equivalent to
solving the first order optimality conditions which can be written as a so-called monotone inclusion
with w = (x, y) ∈ Rm and m = d + n, given by
0 ∈ F(w) + Nω(w).	(2)
The entities involved are
F(x, y) := (VχΦ(x, y), -VyΦ(x, y)),	(3)
2
Under review as a conference paper at ICLR 2021
and the normal cone Nω of the convex set Ω := X X Y. The normal cone mapping is given by
Nω(w) = {v ∈ Rm : hv, w0 — Wi ≤ 0 ∀w0 ∈ Ω},
for w ∈ Ω and Nω(w) = 0 for w / Ω. Here, the operators F and Nω satisfy well known properties
from convex analysis (Bauschke & Combettes, 2011), in particular the first one is monotone (and
Lipschitz if VΦ is so) whereas the latter one is maximal monotone. We call a, possibly set-valued,
operator A from Rm to itself monotone if
hu — u0, z — z0i ≥ 0 ∀u ∈ A(z), u0 ∈ A(z0).
We say A is maximal monotone, if there exists no monotone operator A0 such that the graph of A is
properly contained in the graph of A0 .
Problems of type (2) have been studied thoroughly in convex optimization, with the most established
solution methods being Extragradient (Korpelevich, 1976) and Forward-Backward-Forward (Tseng,
1991). Both methods are known to generate sequences of iterates converging to a solution of (2).
Note that in the unconstrained setting (i.e. if Ω is the entire space) both of these algorithms even
produce the same iterates.
2.3	S olving monotone inclusions
The connection between monotone inclusions and saddle point problems is of course not new. The
application of Extragradient (EG) to minimax problems has been studied in the seminal paper Ne-
mirovski (2004) under the name of Mirror Prox and a convergence rate of O(1/k) in terms of the
function values has been proven. Even a stochastic version of the Mirror Prox algorithm has been
studied in Juditsky et al. (2011) with a convergence rate of O(1∕√k). Applied to problem (2), with
Pω being the projection onto Ω, it iterates
EG: wk = PΩ [zk - αkF(Zk )]
, Zk+1 = Po[zk — ak F (wk)].
The Forward-Backward-Forward (FBF) method, introduced in Tseng (1991), has not been studied
rigorously for minimax problems in terms of function values yet, despite promising applications
in Bot et al. (2020) and its advantage of it only requiring one projection, whereas EG needs two. It is
given by
FBF:	Wk = PΩ[zk — αk F (Zk)]
zk+1 = wk + αk (F(zk) — F(wk)).
(4)
Both, EG and FBF, have the “disadvantage” of needing two gradient evaluations per iteration. A
possible remedy — suggested in Gidel et al. (2019) for EG under the name of extrapolation from the
past — is to recycle previous gradients. In a similar fashion we consider
FBFn: wk = PΩ[zk - αkF(Wk-I)]
p Zk+1 = wk + αk(F (wk-1) — F(wk)),
(5)
where we replaced F(Zk) by F (wk-1) twice in (4). As a matter of fact, the above method can be
written exclusively in terms of the first variable wk by incrementing the index k in the first update
and then substituting in the second line. This results in
wk+1 = Pω IWk — ɑk+ιF(Wk) + αk(F(Wk-I) 一 F(Wk))].
(6)
This way we rediscover a known method which was studied in Malitsky & Tam (2020) for general
monotone inclusions under the name of forward-reflected-backward. It reduces to optimistic mirror
descent (Rakhlin & Sridharan, 2013a;b) in the unconstrained case with constant step size αk = α,
giving
wk+1 = wk — α(2F (wk) — F (wk-1))
(7)
which has been proposed for the training of GANs under the name of Optimistic Gradient Descent
Ascent (OGDA), see Daskalakis et al. (2018); Daskalakis & Panageas (2018); Liang & Stokes (2019).
All of the above methods and extensions rely solely on the monotone operator formulation of the
saddle point problem where the two components x and y play a symmetric role. Taking the special
minimax structure into consideration, Hamedani & Aybat (2018) showed convergence of a method
3
Under review as a conference paper at ICLR 2021
that uses an optimistic step (7) in one component and a regular gradient step in the other, thus
requiring less storing of past gradients in comparison to (6).
On the downside, however, by reducing the number of required gradient evaluations per iteration, the
largest possible step size is reduced from 1/L (see Korpelevich (1976) or Section 3) to 1/2L (see Gidel
et al. (2019); Malitsky & Tam (2020); Malitsky (2015) or Section 3). To summarize, the number of
required gradient evaluations is halved, but so is the step size, resulting in no clear net gain.
2.4	Regularizers
The role of regularizers is well studied in many fields such as statistics (Tibshirani, 1996), signal
processing (Palomar & Eldar, 2010) or inverse problems (Rudin et al., 1992). They serve different
purposes such as inducing sparsity in the solution or conditioning of the problem. In the context of
deep learning this has been explored from different perspectives, e.g. in incremental convex neural
networks where neurons with zero weights are removed from the network and new ones are inserted
according to different policies, see Bach (2017); Bengio et al. (2006); Rosset et al. (2007); Pieper
& Petrosyan (2020). Other examples include the box-constraints for WGANs with weight clipping
(see Arjovsky et al. (2017)) or spectral normalization (see Miyato et al. (2018)) which has so far
rather been considered as part of the architecture, but can at the same time seen as a regularization
term of the function values.
In the framework of monotone operator theory the optimality condition of the regularized minimax
problem (1) can be written as
0 ∈ F(w) + ∂r(w),
(8)
where r is given by (x, y) 7→ f (x) + h(y). The possibly set-valued operator ∂r denotes the
subdifferential of r and is given by
∂ r(w) :=	{v	∈	Rm	:	hv, w0	- wi +	r(w)	≤	r(w0)	∀w0 ∈	Rm}.
The monotone inclusion (8) generalizes (2) in a natural way, since Nω = ∂δo. Similarly, the
projection constitutes a special case of the so-called proximal mapping which for the function r and
λ > 0 is given by
proxλr (W) := arg min ( r(w0) + ɪ kw0 — w∣∣2 0.
w0 ∈Rm	2λ
In particular, the proximal mapping of the indicator 6。yields the projection onto the set Ω, i.e.
ProxλδΩ = PΩ.
3 Main results
Motivated by the considerations above we study the inclusion problem
0 ∈ F (w) + ∂r(w),	(9)
where F : Rm → Rm is a monotone and Lipschitz operator and r : Rm → R ∪ {+∞} is a proper
convex lower semicontinuous function.
3.1	Measure of optimality
There are two common quantities measuring the quality of a point with respect to the monotone
inclusion (8). The most natural one is the distance to the solution set for which typically only
asymptotic convergence can be proved. If F arises from a saddle point problem (1) meaning that
F has the form (3), we want to use a more problem specific measure, the minimax gap, which for a
point w = (u, v) ∈ Rd × Rn is given by
sup Ψ(u, y) - inf Ψ(x, v) = sup	Ψ(u, y) - Ψ(x, v) .	(10)
y∈Rn	x∈Rd	x∈Rd,y∈Rn
This minimax gap can be interpreted from a game theoretic standpoint as the sum of the maximal
payoffs achievable by the two players by playing their respective best responses, given the current
4
Under review as a conference paper at ICLR 2021
strategy of the opponent. In the more general monotone inclusion setting where no function values
are available, an appropriate generalization of (10) is given for any w ∈ Rm by
sup hF (z), w - zi + r(w) - r(z).
z∈Rm
If r is the indicator 6。of the compact and convex set Ω it is clear that the supremum is only taken
over Z ∈ Ω and will thus be finite.
The restricted gap. Since the problem (9) is in general unconstrained and the supremum can be
infinite we consider instead, as done for example in Nesterov (2007), the restricted gap where the
above supremum is taken over an auxiliary compact set B ⊂ Rm instead of the entire space. Note
that the restricted gap is in general only a reasonable measure of optimality for elements of B . It
is nonnegative on B and zero for points of B which solve (9). Additionally we want to be able to
conclude that if a point w* has zero gap it solves (9). This is for example the case if w* is in the
interior of B, which can always be ensured if B is chosen large enough.
In order to capture both at the same time we define the following unifying gap
GB (w) :
sup(x,y)∈B Ψ(u, y) -Ψ(x,v)
supz∈B hF(z),w - zi + r(w) - r(z)
if F and r come from (1)
otherwise.
(11)
3.2	Methods
We now present a novel unifying scheme for solving problem (9), which generalizes FBF (4) and
in addition recovers the method motivated in (5) as FBFp. Let us point out again that the latter
algorithm was already introduced in Malitsky & Tam (2020) and corresponds to OGDA (Rakhlin
& Sridharan, 2013a; Daskalakis et al., 2018; Daskalakis & Panageas, 2018) if F stems from the
minimax setting (3).
Algorithm 3.1 (generalized FBF). For a starting point z0 ∈ Rm and step sizes αk > 0 we consider
for all k ≥ 0
wk = proxαkr (zk - αkF (♦k))
zk+1 = wk + αk (F (♦k) - F (wk)).
For ♦k = zk this reduces to the well known FBF method, whereas ♦k = wk-1, with the additional
initial condition w-1 = z0, recycles previous gradients (FBFp).
Consider the scenario where F is given as an expectation Eξ[F(∙; ξ)], e.g. coming from (1), and only
a stochastic estimator F(∙; ξ) is accessible instead of F itself. In this case we adapt Algorithm 3.1 in
the following way.
Algorithm 3.2 (generalized stochastic FBF). For a starting point z0 ∈ Rm and step sizes αk > 0
we consider for all k ≥ 0
ξk 〜Q (optionally ηk 〜Q)
Wk = ProXakr (Zk - akF(♦k； 4k))
zk+1 = wk + αk (F (♦k ; 4k ) - F(wk ; ξk )).
For ♦k = Zk and 4k = ηk this results in a stochastic version of FBF, whereas ♦k = wk-1 and
4k = ξk-1 recycles previous gradients (stochastic FBFp) with the additional initial condition
w-1 = Z0 and ξ-1 = η0.
Even though both methods encompassed by the unifying scheme Algorithm 3.1 have been studied
in the deterministic setting before, the stated convergence results are new. Note that while the rate
for FBF is completely new our result for FBFp provides only a generalization of the known rate for
OGDA, see Mokhtari et al. (2019). Similarly, the stochastic version of FBF has been considered
before in Bot et al. (2019) and rates have been obtained, but only in terms of the fixed point residual
and not the function values. However, we want to point out that the stochastic version of FBFp has
not been considered prior to this work.
5
Under review as a conference paper at ICLR 2021
3.3	Convergence
Let in the following B ⊂ Rm be the compact set of the restricted (unifying) gap function (11) with
D := supw,z∈B kz - wk denoting its diameter. For convenience in the estimation we assume that
the starting point z0 of the discussed methods is in B.
Theorem 3.1 (deterministic). Let (wk)k≥0 be the sequence generated by Algorithm 3.1. If
(i)	FBF, i.e. ♦k = zk, with step size αk = α ≤ 1/L, or
(ii)	FBFp, i.e. ♦k = wk-1, with step size αk = α ≤ 1/2L
is chosen, thenfor all K ≥ 1 the averaged iterates WK :=春 PK-01 Wk fulfill
D2
GB(WK) ≤ 2αK,
where GB is the restricted gap defined in (11).
In order to derive similar convergence statements for the stochastic algorithm we need to assume
(standard) properties of the gradient estimator F(∙; ξ).
Assumption 1. Unbiasedness: Eξ [F (W; ξ)] = F(W) ∀W ∈ Rm.
Assumption 2. Bounded variance: Eξ [kF (W; ξ) - F (W)k2] ≤ σ2 ∀W ∈ Rm.
In particular we actually only need the above assumption to hold for all iterates Wk . Such an
hypothesis is in practice difficult to check, but could be exploited in special cases where additional
properties of the variance and boundedness of the iterates are known a priori.
Assumption 3. The samples ξk are independent of the iterates Wk, for all k ≥ 0.
Equipped with these assumptions we are now able to prove the statement.
Theorem 3.2 (stochastic). Let Assumption 1, 2 and3 hold and let (Wk)k≥0 be the sequence generated
by Algorithm 3.2. If
(i)	stochastic FBF i.e. ♦k = Zk and 4k = ηk, With SteP size ɑ^ ≤ α ≤ 1∕√2l, or
(ii)	stochastic FBFp, i.e. ♦k = Wk-1 and 4k = ξk-1, with step size αk ≤α ≤ 1/3L
PK-1 α w
is chosen, thenforall K ≥ 1 the averaged iterates WK := 2pK-1 ;仅k fulfill
E[GB (WK)] ≤ D2 + PK-PK0 α,
k=0 αk
where GB is the restricted gaP defined in (11).
The above theorem exhibits a classical step size dependence (Robbins & Monro, 1951), yielding
convergence for sequences (αk)k≥0 that are square summable Pk∞=0 α2k < +∞ but not summable
Pk∞=0 αk = +∞. Additionally, if in the setting of Theorem 3.2 the step size is chosen to be
ɑk = α∕√k + 1, a convergence rate can be obtained and is given by
E [GB (WK)]
(12)
If the step size does not go to zero, the gap can usually not be expected to vanish either. However, we
can still show decrease in the gap up to a residual stemming from the variance. In particular, for a
constant step size αk = α we have
D2
E[GB(WK)] ≤ OK + 24σ α∙
(13)
Additionally, if the number of iterations K is fixed beforehand, a conclusion similar to (12) can be
obtained by choosing α = 1∕√k in (13).
6
Under review as a conference paper at ICLR 2021
4	Experiments
The aim of this section is to show how the use of methods with convergence guarantees, albeit only in
the monotone setting, can yield better training performance for different architectures and objectives.
In particular, we demonstrate that FBF can perform at least as good as EG although requiring less
evaluations of the regularizers.
4.1	2D toy example
Following Goodfellow (2016); Mescheder et al. (2018) and others we consider the canonical example
minx maxy xy, illustrating the cycling behavior of (even bilinear) minimax problems. We augment
this approach by adding a nonsmooth L1-regularizer for one player, with κ > 0, resulting in
min max κ∣x∣ + xy.	(14)
x∈R y∈[-1,1]
The aforementioned issue of GDA (and its proximal extension PGDA) cycling around the solution is
highlighted in Figure 1. The other methods, for which we display the averaged iterates, however do
converge to a solution and show a decrease in the restricted gap according to theory. EVen though the
proximal steps provide improvement towards the solution (0,0) and FBF only uses half the amount
of evaluations compared to EG, it outperforms the competing algorithms.
(a) Trajectories converging to solution.
Figure 1: A comparison of the methods presented in Section 2.3 applied to problem (14) with
κ = 0.01. PGDA denotes (alternating) gradient descent ascent with proximal steps. As mentioned
in the introduction it fails to converge. EGp denotes the method presented in Gidel et al. (2019) as
extrapolation from the past. For the restricted gap we use B1 = B2 = [-1, 1].
(b) Restricted gap function.
4.2	WGAN TRAINED ON CIFAR 1 0
In this section we apply the above proposed techniques from monotone inclusions to the training
of Wasserstein GANs employing DCGAN (Radford et al., 2015) and ResNet (He et al., 2016)
architectures. All models are trained on the CIFAR10 dataset (Krizhevsky et al., 2009) which consists
of 60,000 images in 10 different classes (with 50,000 training images and 10,000 test images) using
an NVIDIA RTX 2080Ti GPU.
For the DCGAN experiments we work with the original WGAN formulation including weight clip-
ping, since it includes regularizers innately (the indicator ofabox for the weights of the discriminator).
In addition we propose a modification of the WGAN formulation which replaces the box constraint
on the discriminator’s weights with an L1-regularization, under the name of WGAN-L1. This results
in a soft-thresholding operation instead of the “harsh” clipping.
For the experiments on ResNet we use the WGAN-GP formulation (Gulrajani et al., 2017) which
penalizes the norm of the gradient of the discriminator to enforce the Lipschitz constraint, together
with spectral normalization of the weight matrices (Miyato et al., 2018) which can be seen as a
projection.
7
Under review as a conference paper at ICLR 2021
Table 1: The best Inception Score (IS) and Frechet Inception Distance (FID). The column denoted
by WGAN, WGAN-L1 and WGAN-GP refers to the standard formulation with weight clipping, our
regularized implementation using the 1-norm and the formulation with gradient penalty and spectral
normalization, respectively.
WGAN	WGAN-L1	WGAN-GP
Method	IS	FID	IS	FID	IS	FID
AltAdam1	4.12±.06	56.44±.62	4.43±.03	50.86±2.17	6.01±.31	28.11±3.65
Extra Adam	4.07±.05	56.67±.61	4.67±.11	47.24±1.21	6.58±.08	21.40±.58
FBF Adam	4.54±.04	45.85±.35	4.68±.16	46.60±.76	6.57±.10	21.22±1.29
Opt. Adam	4.35±.06	50.41±.46	4.63±.13	47.98±1.49	6.42±.10	23.01±.95
Given the ubiquity and dominance of Adam (Kingma & Ba, 2014) as an optimizer for many deep
learning related training tasks, instead of using vanilla SGD we opt for Adam updates. This results
in a method we call FBF Adam. Analogous approaches have been applied in Gidel et al. (2019)
and Daskalakis et al. (2018) resulting in Extra Adam and Optimistic Adam, respectively. We compare
the aforementioned methods with the status-quo in GAN training, namely alternating one Adam step
for each network: AltAdam1.
Our hyperparameter search was limited to the step sizes when using the WGAN-L1 and WGAN-GP
formulation, while all other parameters were kept the same as in Gidel et al. (2019); Bot et al. (2020).
It seems noteworthy that in the case of soft-thresholding bigger step sizes performed better with the
only exception of AltAdam1.
Figure 2:	Left: Average and best/worst IS on the WGAN objective with weight clipping. Right:
Average and best/worst IS on the WGAN-L1 objective using the proximal operator; The WGAN-
L1 objective improves the IS in comparison to weight clipping and stabilizes the behavior of all
considered methods during the training procedure.
The two evaluation metrics used are the Inception Score (IS, higher is better) (Salimans et al., 2016)
and the Frechet inception distance (FID, lower is better) (HeuSel et al., 2017), both computed on
50,000 samples. In the case of the IS we use the updated and corrected implementation from Barratt
& Sharma (2018). All results are averaged over 5 runs for each method.
ω) əjoos ⊂o⅛φo⊂-
Figure 3:	Average and best/worst results regarding IS (left) and FID (right) using ResNet architecture
on the WGAN-GP objective including spectral normalization. Middle: Samples from the generator
trained with FBF Adam.
In Table 1 the best IS and FID for each method are reported. FBF Adam performs at least as good as
all considered competitors with respect to both evaluation metrics. One can also see that WGAN-L1
8
Under review as a conference paper at ICLR 2021
using the proximal operator improves the performance of all considered methods. Figure 2 shows the
training progress regarding IS for each method and both problem formulations. The graphs suggest
that making use of WGAN-L1 objective has a stabilizing effect during training, leading to a smoother
and more consistent learning curve — a property that only FBF Adam seems to exhibit for weight
clipping. Figure 3 as well as Table 1 show that for the WGAN-GP formulation FBF Adam maintains
the improved performance of EG compared to GDA, while only requiring half the amount of spectral
normalizations, resulting in time savings ofup to 10% as reported in Miyato et al. (2018).
5 Conclusion
By highlighting the connection between GAN objectives and monotone inclusions, we are able to
tackle their training via the Forward-Backward-Forward method which is known to converge to a
solution for convex-concave minimax problems. We deepened this theoretical understanding by
proving novel convergence rates in terms of the function values. We complement these rigorous
considerations by promising practical results, indicating that application of FBF can lead to improved
performance and saved computation time (compared to EG).
References
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein GAN. arXiv:1701.07875, 2017.
Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal of
Machine Learning Research,18(1):629-681, 2017.
Shane Barratt and Rishi Sharma. A note on the inception score. arXiv:1801.01973, 2018.
Heinz H Bauschke and Patrick L Combettes. Convex Analysis and Monotone Operator Theory in
Hilbert Spaces, volume 408. Springer, 2011.
Yoshua Bengio, Nicolas L Roux, Pascal Vincent, Olivier Delalleau, and Patrice Marcotte. Convex
neural networks. In Advances in Neural Information Processing Systems, pp. 123-130, 2006.
Radu Ioan Bot, Panayotis Mertikopoulos, Mathias Staudigl, and Phan Tu Vuong. Forward-backward-
forward methods with variance reduction for stochastic variational inequalities. arXiv preprint
arXiv:1902.03355, 2019.
Radu Ioan Bot, Michael Sedlmayer, and Phan TU Vuong. A relaxed inertial forward-baCkWard-
forward algorithm for solving monotone inclusions with application to GANs. arXiv:2003.07886,
2020.
Constantinos Daskalakis and Ioannis Panageas. The limit points of (optimistic) gradient descent in
min-max optimization. In Advances in Neural Information Processing Systems, pp. 9236-9246,
2018.
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng. Training GANs
with optimism. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=SJJySbbAZ.
Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A
variational inequality perspective on generative adversarial networks. In International Confer-
ence on Learning Representations, 2019. URL https://openreview.net/forum?id=
r1laEnA5Ym.
Ian Goodfellow. Nips 2016 tutorial: Generative adversarial networks. arXiv:1701.00160, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville.
Improved training of Wasserstein GANs. In Advances in Neural Information Processing Systems,
pp. 5767-5777, 2017.
9
Under review as a conference paper at ICLR 2021
Erfan Yazdandoost Hamedani and Necdet Serhat Aybat. A primal-dual algorithm for general convex-
concave saddle point problems. arXiv:1803.01401, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in Neural Information Processing Systems, pp. 6626-6637, 2017.
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel. Solving variational inequalities with
stochastic mirror-prox algorithm. Stochastic Systems, 1(1):17-58, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980,
2014.
GM Korpelevich. The extragradient method for finding saddle points and other problems. Matecon,
12:747-756, 1976.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Tengyuan Liang and James Stokes. Interaction matters: A note on non-asymptotic local convergence
of generative adversarial networks. In Kamalika Chaudhuri and Masashi Sugiyama (eds.), The
22nd International Conference on Artificial Intelligence and Statistics, volume 89 of Proceedings
of Machine Learning Research, pp. 907-915. PMLR, 2019.
Yura Malitsky. Projected reflected gradient methods for monotone variational inequalities. SIAM
Journal on Optimization, 25(1):502-520, 2015.
Yura Malitsky and Matthew K Tam. A forward-backward splitting method for monotone inclusions
without cocoercivity. SIAM Journal on Optimization, 30(2):1451-1472, 2020.
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do
actually converge? In International Conference on Machine Learning, 2018.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In International Conference on Learning Representations, 2018.
URL https://openreview.net/forum?id=B1QRgziT-.
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil. Convergence rate of O(1/k) for optimistic
gradient and extra-gradient methods in smooth convex-concave saddle point problems. arXiv
preprint arXiv:1906.01115, 2019.
Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with
Lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
SIAM Journal on Optimization, 15(1):229-251, 2004.
Yurii Nesterov. Dual extrapolation and its applications to solving variational inequalities and related
problems. Mathematical Programming, 109(2-3):319-344, 2007.
Daniel P Palomar and Yonina C Eldar. Convex optimization in signal processing and communications.
Cambridge university press, 2010.
Konstantin Pieper and Armenak Petrosyan. Nonconvex penalization for sparse neural networks.
arXiv:2004.11515, 2020.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv:1511.06434, 2015.
Alexander Rakhlin and Karthik Sridharan. Online learning with predictable sequences. In Proceedings
of the 26th Annual Conference on Learning Theory, pp. 993-1019, 2013a.
Sasha Rakhlin and Karthik Sridharan. Optimization, learning, and games with predictable sequences.
In Advances in Neural Information Processing Systems, pp. 3066-3074, 2013b.
10
Under review as a conference paper at ICLR 2021
Herbert Robbins and Sutton Monro. A stochastic approximation method. The annals of mathematical
statistics, pp. 400-407, 1951.
Saharon Rosset, Grzegorz Swirszcz, Nathan Srebro, and Ji Zhu. ` 1 regularization in infinite
dimensional feature spaces. In International Conference on Computational Learning Theory, pp.
544-558. Springer, 2007.
Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal
algorithms. Physica D: nonlinear phenomena, 60(1-4):259-268, 1992.
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improved techniques for training GANs. In Advances in Neural Information Processing Systems,
pp. 2234-2242, 2016.
Robert Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267-288, 1996.
Paul Tseng. Applications of a splitting algorithm to decomposition in convex programming and
variational inequalities. SIAM Journal on Control and Optimization, 29(1):119-138, 1991.
A Definitions
In Section 2.4 we require the regularizers to be proper, convex and lower semicontinuous which are
common properties in convex analysis. We call a function r : Rm → R ∪ {+∞} proper if it is not
constant +∞, which means that it takes a finite value for at least a single point. In addition, we say
that r is lower semicontinuous if for all z0 ∈ Rm
lim inf r(z) ≥ r(z0).
z→z0
It is easy to see that if C ⊂ Rm is nonempty, closed and convex, then the indicator δC of this set,
given by
δC(z) =	0+∞
if z ∈ C
otherwise
fulfills the assumptions of being proper, convex and lower semicontinuous.
B Ab out the gap function
Typically in monotone inclusions, the distance to the set of solutions is used as a measure of quality
of a given point due to the lack of more specific structure in general. Asymptotic convergence of
the iterates has been established for FBF and FBFp in Bauschke & Combettes (2011, Proposition
27.13) and Malitsky & Tam (2020), respectively. Furthermore, no convergence rates can be expected
without stronger monotonicity assumptions. We want to take into account the special structure of the
monotone inclusion coming from the minimax problem (1). For this reason we use the following
(restricted) minimax gap, common for saddle point problems, which for a point (u, v) is given by
GB (u, v) = sup Ψ(u, y) - Ψ(x, v).	(15)
(x,y)∈B
For the general case, i.e. F being an arbitrary monotone and Lipschitz operator this is connected to
the other measure of optimality we use in (11), for w ∈ Rm given by
GB (w) = sup hF(z), w - zi + r(w) - r(z),	(16)
z∈B
where we interpret the possible occurrence of ∞ - ∞ as +∞. It stems from the field of Variational
Inequalities where such a function is also known as merit function (Nesterov, 2007). The relevance
of the above two quantities will be made clear by the following statements.
Theorem B.1. Let Φ : Rd × Rn → R be continuously differentiable and f : Rd → R ∪ {+∞},
h : Rn → R ∪ {+∞} be proper, convex and lower semicontinuous and B ⊂ Rd × Rn. A point
(x*,y*) in the interior of B solves the saddle point problem (1) if and only if its minimax gap (15) is
zero, GB (x*, y*) = 0. For all other elements of B the gap is nonnegative.
11
Under review as a conference paper at ICLR 2021
Proof. A saddle point (χ*,y*) clearly fulfills that suP(χ,y)∈Rd×Rn Ψ(χ*,y) - Ψ(χ, y*) = 0. On the
other hand let GB (x*,y*) = 0. For an arbitrary point (x, y) We can choose α ∈ (0,1) large enough
such that (u, V) := α(x*,y*) + (1 — α)(x, y) is in the interior of B. Therefore,
Ψ(x*, v) — Ψ(u,y*) = Ψ(x*, αy* + (1 — α)y) — Ψ(αx* + (1 — α)x, y*) ≤ 0.
Using the convex-concave structure of Ψ We deduce that
αΨ(x*,y*) + (1 — α)Ψ(x*,y) — αΨ(x*,y*) — (1 — α)Ψ(x, y*) ≤ 0,
which implies that Ψ(x*,y) ≤ Ψ(x,y*). Since (x,y) was chosen arbitrary (x*,y*) is a saddle
point.	□
Similarly, an analogous statement can be shown for (16). The proof, however is split up into multiple
lemmas to highlight the connection to Variational Inequalities.
Theorem B.2. Let F : Rm → Rm be monotone and continuous, r : Rm → R ∪ {+∞} proper,
convex and Iowersemicontinuous and B ⊂ Rm. A point w* in the interior of B solves the monotone
inclusion
0 ∈ F(w) + ∂ r(w)	(17)
if and only if its restricted gap (16) is zero, GB (w*) = 0. For all other elements of B the gap is
nonnegative.
Let the assumptions of Theorem B.2 hold true for the following lemmas as we break up the proof
into separate statements. We do so by making use of the associated Variational inequality (VI)
find w such that hF (w), z — wi + r(z) — r(w) ≥ 0 ∀z ∈ Rm.	(18)
Lemma B.3. The monotone inclusion (17) is equivalent to the VI (18).
Proof. The equivalence of (17) and (18) follows immediately from the definition of the subdifferential
of r.	□
The formulation (18) is typically referred to as the strong form of the VI, whereas
find w such that	hF(z), z —	wi	+ r(z)	—	r(w)	≥ 0	∀z	∈	Rm,	(19)
is known as the weak formulation.
Lemma B.4. Under the given assumptions the notion of weak and strong VI are equivalent.
Proof. For the monotone operator F it is clear that if w* is a solution to the strong formulation (18),
it is also a solution to the weak formulation (19). In fact, if F is continuous the reverse implication
also holds true. To see this, let w* be a solution to the weak VI (19) and z = αw* + (1 — α)u for an
arbitrary u ∈ Rm and α ∈ (0, 1), then
hF (αw* + (1 — α)u), (1 — α)(u — w*)i + r(αw* + (1 — α)u) — r(w*) ≥ 0.
This implies by the convexity of r that
(1 — α)hF (αw* + (1 — α)u), (u — w*)i + (1 — α)(r(u) — r(w*)) ≥ 0.
By dividing by (1 — α) and then taking the limit α → 1 we obtain that w* is a solution of the strong
form (18).	□
With the notion of VIs in mind, the above defined gap (16) becomes natural as it measures how much
the statement of (19) is violated.
Lemma B.5. GB is nonnegative on B and zero for solutions of the weak VI.
Proof. It is clear that GB (w) ≥ 0 for w ∈ B as z = w can be chosen in the supremum. On the other
hand if w* ∈ B is a solution to the weak VI (19) then GB(w*) = 0. This follows from the fact that
for a solution of (19) for all z ∈ B
hF (z), w* — zi + r(w*) — r(z) ≤ 0.
Therefore the supremum over the above expression in z is also less than zero, but clearly zero is
obtained for Z = w*.	□
12
Under review as a conference paper at ICLR 2021
For the reverse implication to hold true, we may not use points on the boundary of B .
Lemma B.6. If a point w* in the interior of B exhibits zero gap GB (w*) = 0, then it is a solution
to the weak VI (19).
Proof. Since w* is in the interior of B we can, for an arbitrary w ∈ Rm, choose α ∈ (0, 1) large
enough such that z := αw* + (1 - α)w ∈ B. Using this z in the supremum of the gap we deduce
that
hF (αw* + (1 - α)w), w* - αw* - (1 - α)wi + r(w*) - r(αw* + (1 - α)w) ≤ 0.
This implies that
(1 - α)hF (αw* + (1 - α)w), w - w*i + (1 - α)(r(w) - r(w*)) ≥ 0.
By dividing by (1 - α) and then taking the limit α → 1 we deduce that w* solves the strong form of
theVI(18).	□
Now, we can turn to proving the theorem.
ProofofTheorem B.2. Combine Lemma B.3, B.4, B.5 and B.6.	□
C Refined theorems
Recall that restricted (unifying) gap function GB defined in (11) is computed with respect to a
set B ⊂ Rm where D := supw,z∈B kz - wk denotes its diameter and it is assumed that z0 ∈ B.
Furthermore, the averaged iterates WK for K ≥ 1 are given by
WK
K-1
Tk=o aWk
K^K—i
k=0 αk
C.1 Deterministic statements
The convergence statement of Theorem 3.1 actually holds true not just for a constant step size as
presented in Section 3, but for variable step sizes as well.
Theorem C.1. Let (Wk)k≥0 be the sequence generated by Algorithm 3.1. If
(i)	FBF, i.e. ♦k = zk, with step size 0 < αk ≤ α≤ 1/L, or
(ii)	FBFp, i.e. ♦k = Wk—1, with step size 0 < αk ≤ α≤ 1/2L
is chosen, then for all K ≥ 1
D2
GB (WK ) ≤ o「K -1—.
2	k=0 αk
C.2 Stochastic statements
We actually prove a slightly more general version of Theorem 3.2. In particular the step size can be
chosen larger than initially claimed, however, at the cost of a worse constant.
Theorem C.2. Let Assumption 1, 2 and 3 hold and let (Wk)k≥0 be the sequence generated by FBF,
i	.e. Algorithm 3.2 with ♦k = Zk and 4k = ηk. Let the step size αk ≤ α < L, then
E[G (而 )] ≤ D2 +4(I-。”2)-%2 PK-Iak
EiGB (WK)] ≤	CLK-I	，
2	k=0 αk
for all K ≥ 1.
Theorem 3.2 (i) can be deduced from the above statement by using α = 1∕√2L which yields that
(1 - α2L2 )-1 = 2.
13
Under review as a conference paper at ICLR 2021
Theorem C.3. Let Assumption 1, 2 and 3 hold and let (wk)k≥0 be the sequence generated by FBFp,
i.e. Algorithm 3.2 with ♦ = wk-ι and 4k = ξk-ι. Let the step size αk ≤ α < 2√l, then
λl D2+6(l + 1≡‰ ”2 PK:01αk
E[GB (WK )] ≤ ---------LK-1 --------------,
k=0 αk
for all K ≥ 1.
Theorem 3.2 (ii) is obtained from the above theorem by using the particular step size bound of
α = 1∕3L, which yields that
4α2L2
---------=4
1 — 8α2 L2
Although, the step size in the refined statements Theorem C.2 and C.3 can be chosen arbitrarily close
to 1∕l and 1∕(2√2L) for stochastic FBF and stochastic FBFp, respectively. This does not mean it
should be — since the constant in the convergence rate deteriorates when the step size is close to its
allowed upper bound.
D Proofs
D. 1 Preparations
We introduce the notation connected to the strong formulation of the VI (18) associated to the
monotone inclusion (9), given by
g(w, z) := hF (w), w — zi + r(w) — r(z),
for g : Rm × Rm → R ∪ {+∞}. Next we will establish the fact that this function can be used to
bound the (restricted) unifying gap function, which we remind, is defined as
GB (w)
sup(x,y)∈B Ψ(u,y) - Ψ(x, v)
if F is (3)
supz∈B hF (z), w - zi + r(w) - r(z) otherwise,
where in the first case (u, v) ∈ Rd × Rn is identified with w ∈ Rm. In particular the dimensions
fulfill d + n = m, and r(w) is given by f(u) + h(v).
Lemma D.1. It holds that for all K ≥ 1
1 K-1
sup ∖	K-1- ɪ? αkg(wk ,z) > ≥ GB (WK).
z∈B	k=0 αk k=0
Proof. First we will prove the case if F is derived from a saddle point problem. Note that from the
convex-concave structure of Φ we get that
Φ(u, y) ≤ Φ(u, v) + hVyΦ(u, v), y — vi
and
Φ(u, V) + hVχΦ(u, v), X — u〉≤ Φ(x, v).
By summing the two up we obtain
Φ(u,y) — Φ(x,v) ≤ - —vxφ(u,V), x - u ∖
,	,	Vy Φ(u, v),	y — v
We can reformulate the above inequality in terms of g to see that for z = (x, y ) ∈ Rd × Rn
hF (w), w — zi ≥ Φ(u, y) — Φ(x, v).
The statement of the first case is obtained by adding r(w) — r(z) on both sides and using the fact that
Ψ is convex-concave.
If F is a general monotone operator, then we use its monotonicity to deduce that
hF (w), w — zi ≥ hF(z),w — zi.
The desired result follows from using the linearity of the inner product.
□
14
Under review as a conference paper at ICLR 2021
Notation. We denote the error of the stochastic estimator via
Zk := F (♦k; 4k ) - F (♦k) and Wk := F (wk; ξk ) - F (wk)	QO)
Furthermore, We will denote via E[ ∙ | U], the conditional expectation with respect to the random
variable U.
We will also need the following lemma.
Lemma D.2. Let (pk)k≥0 ∈ Rd be a given sequence and (vk)k≥0 recursively defined for all k ≥ 0
by vk+1 := vk - pk for some v0 ∈ Rd, then
K-1	K-1
X hpk, vk - ui ≤ 2 kv0 - uk2 + 2 X kpk k2.
k=0	k=0
Proof. From the three point identity it follows immediately that
hPk, vk - ui = hvk - vk+1, vk - ui = £ (∣∣vk - uk2 - kvk+1 - uk2 + kvk+1 - Vk ∣∣2)
from which the statement of the lemma follows.	口
D.2 A unified decrease result
We will start with a unifying proposition which covers the common parts of all convergence proofs.
Proposition D.3. For a γ > 0 we have that for all k ≥ 0 and z ∈ Rm
αk g(wk , z) + £ kzk+1 - zk2 ≤ £ kzk - zk2 - £ kzk - wk 112 + £(1+ γ)akL2kQk - wk 112
+ αk hWk, z - wki + (1 + γ-1)αk2 (kWk k2 + kZk k2).
(21)
Proof. Let k ≥ 0 and z ∈ Rm be arbitrary. Using the decomposition (20) it follows that
hαkF(wk; ξk), wk - zi = αkhWk, wk - zi + αkhF(wk), wk - zi.	(22)
Since proxα r = (Id + αk∂r)-1 we deduce that
hz 一 wk,wk - Zk + αkF(♦k; 4k)i ≥ αk(r(wk) 一 r(z)).	(23)
Adding (22) and (23) gives that
Gk(F(wk; ξk) — F(♦k; 4k)) + Zk — wk,wk — zi≥ αk hWk, wk — Zi + αkg(wk,z),
which, using the definition of zk+1, is equivalent to
hZ — wk, Zk+1 — Zki ≥ αkhWk,wk — Zi + αkg(wk, Z).	(24)
We estimate the inner product on the left side of the inequality by inserting and subtracting Zk and
using the three point identity twice to deduce
hZ — wk, Zk+1 — Zki = hZ — Zk + Zk — wk, Zk+1 — Zki
=£ (∣∣z — zk k2 一 kzk+1 一 zk2 + kzk+1 一 wk k2 一 kzk 一 wk∣∣2) .
The first two summands are fine as they will telescope, so we are left with estimating kZk+1 — wk k2.
By the definition of Zk+1 we have that
kZk+1 — wk k2 =a kF (♦k ; 4k) — F (wk; ξk )k2
=αk kF (♦k ) 一 F (wk ) + Zk 一 Wk ∣∣2
≤ (1 + γ)αk kF (♦k ) — F (wk )k2 + (1 + γ-1)αk kZk - Wk k2
(26)
≤ (1+ Y)αkL2k♦k — wkk2 +z(l + γT)αk(∣∣Zkk2 + kWkk2),
where we inserted and subtracted F (♦k) and F (wk) and applied Young,s inequality to deduce the
result. Adding (26), (25) and (24) we conclude that
αk g(wk, z) + £ kzk+1 一 zk2 ≤ £ kzk 一 zk2 一 £ kzk - wk k2 + £(1+ γ)αkL2k♦k - wk k2
+ αk hWk, Z 一 wki + (1 + γ-1)αk2 (kWk k2 + kZk k2).
□
15
Under review as a conference paper at ICLR 2021
D.3 Forward-Backward-Forward
Proof for deterministic FBF, Theorem C.1 (i). We start off by plugging ♦k = zk into (21). Since
Wk = Zk = 0 we can use γ → 0 to deduce that for all k ≥ 0
αkg(Wk, Z) + 2 ∣∣zk+1 - zk2 ≤ 2 Ilzk - zk2
-2(I - αkL2)kzk - wk k2.
From this it is clear that the step size is constrained by α ≤ 1/L as stated in the theorem. By summing
up from k = 0 to K - 1 and dividing by PkK=-01 αk we obtain
1 X C X/ IIzO - zk
LK-I	αg(wk, Z) ≤ LK-I	.
k=0 αk k=0	2 k=0 αk
The claimed statement is then derived by taking the supremum in z over B and applying Lemma D.1.
□
Proof for stochastic FBF, Theorem C.2. Plugging ♦k = zk and 4k = ηk into (21) gives for all
k≥0
αkg(wk,z) + 2 kzk+1 — zk2
≤ 2Ilzk — zk2 — 2(1 — (1 + Y)akL2)kzk — Wkk2 + αk hWk, z — vki
+ αk hWk, vk — wki + (1 + γ -1)α2k (IWk I2 + IZk I2).
By summing this inequality UP and applying Lemma D.2 With vo = zo, Pk = —αk Wk and vk+1 ：=
vk — pk we deduce that
K-1	K-1
X h-αkWk JVk-Zz ≤ 2kzo - zk2 + 2 X akkWk∣∣2,	(27)
k=0	k=0
and therefore
K-1	K
X αk g(Wk, z) ≤ Iz0 — zI2 + X αk hWk, vk — Wki + 2(1 + γ -1)α2k (IWk I2 + IZk I2).
k=0	k=0
By choosing Y such that α = (√1 + YL)-1 we deduce that 1 + Y-1 = 1/(1 — α2L2). Next, we
take the supremum over z ∈ B and the expectation to obtain
E sup
z∈B
K-1	K-1
αkg(Wk, z)	≤ D2 + 4(1 — α2L2) σ2	αk2 ,
k=0	k=0
where we used that
E[hWk,vk — Wki] = E EhWk,vk — Wki W[k], ξ[k-1]
= E EWk W[k], ξ[k-1], vk — Wk = 0,
with ξ[k-1] = (ξ0, . . . , ξk-1) and W[k] = (W0, . . . , Wk). The final statement follows by dividing by
PK=01 ak and applying Lemma D.1.	口
D.4 Forward-Backward-Forward-past
Proof for deterministic FBFp, Theorem C.1 (ii). We start off by plugging ♦k = zk into (21). Since
Wk = Zk = 0 we can use Y → 0 to conclude that for all k ≥ 0
αkg(wk,Z) + 2kzk+1 — zk2 ≤ 2kzk — zk2 — 2kzk — wkk2 + 2αkL2kwk-1 — wkk2.	(28)
Now we need to bound the term IWk-1 — Wk I2 by Izk — Wk I2 . Since
2Izk — Wk I2 + 2Izk — Wk-1 I2 ≥ IWk — Wk-1 I2	(29)
16
Under review as a conference paper at ICLR 2021
we have for all k ≥ 1
Ilzk - wkk2 ≥ -Ilzk - wk-1k2 + 2 Ilwk-I - wkk2
≥ -αk-ιL kwk-1 - wk-2k2 + 2 Ilwk-I - wk k2
whereas for k = 0, since w-1 = z0, we have that
Iz0 - w0 I2 = Iw-1 - w0 I2 .
Plugging (31) into (28) for k = 0 we get that
a0g(w0, Z) + 2 kz1 - zk2 + 2(I - α0L2)kw0 - w-1k2 ≤ 2 kz0 - zk2.
Plugging (30) into (28) we get that for all k ≥ 1
akg(Wk, Z) + 2 kzk + 1 - zk2 + 2 (2 - αkL2) Ilwk - wk-1k2
≤2Ilzk - zH2 + 2αk-ιL2∣∣wk-ι - wk-oll2.
In order to be able to telescope we need to ensure that for all k ≥ 0
(30)
(31)
(32)
(33)
(2 - akL2) ≥ αkL2.
This is equivalent to the condition αk ≤ 1/2L which was required in the statement of the theorem.
Now we sum up (33) from k = 1 to K - 1 which yields
K-1	1	1 1
αk g(wk, z) + QkzK - zk2 + 5 ( - - αK-1 L2 ) kwK-1 - wK-2k2
k=1	2	2 2	(34)
≤ 2Ilzi - zll2 + 2a2L2∣∣wo - w-1∣∣2.
Adding (34) and (32) and dividing by PkK=-01 αk to deduce
1	/ Ilzo - zll
LK-I	Qkg(wk, z) ≤ DLK-I	,
k=0 αk k=0	2 k=0 αk
where we used that 1 - α02L2 ≥ α02L2 to get rid of Iw0 - w-1 I2. The final statement follows by
taking the supremum in z over B and applying Lemma D.1.	□
Proof for stochastic FBFp, Theorem C.3. By using ♦k = wk-1 we deduce from (21) for all k ≥ 0
that
αk g(wk, z) + 2 llzk+1 - zll2 ≤ 2 Ilzk - zll2 - 2 Ilzk - wk l∣2 + 2(1+ Y)QkL2 ∣∣wk-1 - wk ∣∣2
+ αk hWk, z - wki + 2(1 + γ-1)α2k (IWk I2 + IZk I2).
As in (27) we can split hQkWk, z -wki into hQkWk, z -vki + hQkWk, vk -wki and use Lemma D.2
to deduce
K-1	K-1 1	1
^X αkg(wk, z) ≤ Ilzo - zll2 - ^X (2 Ilzk - wk ∣∣2 + 2(1+ Y)QkL2 Ilwk-1 - wk Il2
k=0	k=0
+ hQkWk, vk - wki + 3(1 + Y -1)Q2k (IWk I2 + IZk I2).
Taking now the supremum over z ∈ B and then the expectation we conclude that the inequality
K-1	1 K-1
E sup < E Qkg(wk,z)} ≤ D2 — 2 £ (Ilzk - wkIl2 - (1 + Y)QkL2∣∣wk-1 — wkIl2)
z∈B	k=o	2 k=o
K-1
+ 3(1 + Y-1)σ2	XQ2k
k=o
(35)
17
Under review as a conference paper at ICLR 2021
holds. Let from now on k ≥ 1 as we will treat the case k = 0 separately. Using (29) we deduce that
Ilzk - wk ∣∣2 ≥ -Ilzk - wk-1k2 + 5∣∣wk-l - wk ∣∣2
2	1	(36)
≥ -α2-1kF (Wk-I; ξk-1) - F (wk-2; ξk-2) k2 + q Ilwk-I - wk ∣∣2.
Now we bound the difference of the two estimators by inserting ±F (wk-1), ±F(wk-2) and applying
the inequality Ia + b + cI2 ≤ 3(IaI2 + IbI2 + IcI2) which yields
IF (wk-1; ξk-1) - F (wk-2; ξk-2)I2 ≤ 3IWk-1I2 + 3IWk-2I2 + 3IF (wk-2) - F (wk-1)I2.
We conclude that
EIF (wk-1; ξk-1) - F (wk-2; ξk-2)I2 ≤ 6σ2 + 3L2EI
wk-1 - wk-2I2.	(37)
Using (37) in (36) we deduce that
Ellzk - wk 112 ≥ -αk-ι(6σ2 + 3L2Ekwk-l - wk-2 |2) + 2Ekwk-I- wk ∣∣2,	(38)
whereas for k = 0 we have (31). Now we plug (38) into (35) to conclude that
E sup
z∈B
X αkg(wk, z)
1 K-1	1
≤ D2 - 2 X ( -3αk-1L2Ekwk-I- wk-2∣∣2 + (2 - (1+ Y )α2L2) kwk-1 - wk k2
k=1
1	K-1
+ X((1 + γ)α0L2 - 1)kw-1 - w0∣∣2 +6(1+ Y 1)σ2 E αk
k=0
From this we conclude that in order to be able to telescope we need to enforce
(39)
which is equivalent to
,1、≥ αk L2.
2(4 + γ) ≥ k
Since αk ≤ α, we can ensure this by choosing Y such that
1
2(4 + Y)
α2L2.
(40)
With (40) in place conclude from (39) that the inequality
sup
z∈B
K-1
X
k=0
αkg(wk, z)
))
1	K-1
≤ D2 + 5((4 + Y)α0L2 - 1)kw-ι - wo『+ 6(1 + Y 1)σ2 E αk
k=0
Using the fact that 3α02L2 ≤ 1 - (1 + Y)α02L2 from (40) to discard the kw0 - w-1k2 term, yields
sup	αkg(wk, z)	≤
z∈B k=0
K-1
D2 + 6(1 + Y-1)σ2 X αk2
k=0
(41)
Through (40), we can estimate
1	2α2L2
—=--------——.
Y 1 — 8a2L2
(42)
Plugging (42) into (41), dividing by PkK=-01 αk and applying Lemma D.1, deduces the final statement.
一 □
E
E
18
Under review as a conference paper at ICLR 2021
E Architecture
E.1 DCGAN
Table 2:	DCGAN architecture used for the CIFAR10 experiments.
Generator
Input: Z ∈ R128 〜N(0, I)
Linear 128 → 512 × 4 × 4
Batch Normalization
ReLU
transposed conv. (kernel: 4 × 4, 512 → 256, stride: 2, pad: 1)
Batch Normalization
ReLU
transposed conv. (kernel: 4 × 4, 256 → 128, stride: 2, pad: 1)
Batch Normalization
ReLU
transposed conv. (kernel: 4 × 4, 128 → 3, stride: 2, pad: 1)
Tanh(∙)
Discriminator
Input: x ∈ R3×32×32
conv. (kernel: 4 × 4, 1 → 64, stride: 2, pad: 1)
LeakyReLU (negative slope: 0.2)
conv. (kernel: 4 × 4, 64 → 128, stride: 2, pad: 1)
Batch Normalization
LeakyReLU (negative slope: 0.2)
conv. (kernel: 4 × 4, 128 → 256, stride: 2, pad: 1)
Batch Normalization
LeakyReLU (negative slope: 0.2)
Linear 128 × 4 × 4 × 4 → 1
E.2 ResNet
Table 3:	ResNet architecture used for the CIFAR10 experiments.
Generator
Input: Z ∈ R128 〜N(0, I)
Linear 128 → 128 × 4 × 4
ResBlock 128 → 128
ResBlock 128 → 128
ResBlock 128 → 128
ReLU
transposed conv. (kernel: 3×3, 128 → 3, stride: 1, pad: 1)
Tanh(∙)
Discriminator
Input: x ∈ R3 × 32 × 32
ResBlock 3 → 128
ResBlock 128 → 128
ResBlock 128 → 128
ResBlock 128 → 128
Linear 128 → 1
19
Under review as a conference paper at ICLR 2021
F Hyperparameters
For the WGAN formulation with weight clipping, see Table 4, we used the extensively tuned
hyperparameters from Gidel et al. (2019) for ExtraAdam, Adam1 and OptimisticAdam. Note that our
values of the Inception Score (IS) differ from the ones reported in Gidel et al. (2019) as we use the
newer implementation of the IS proposed in Barratt & Sharma (2018). For FBF-Adam we tuned the
step size and kept all other hyperparameters equal.
Table 4: Hyperparameters used for the WGAN formulation (with weight clipping).
(DCGAN) WGAN Hyperparameters		
Batch size	=64
Number of generator updates	= 500, 000
Adam β1	= 0.5
Adam β2	= 0.9
Weight clipping for the discriminator	= 0.01
Learning rate for discriminator	= 5 × 10-4 (Extra Adam)
	= 2 × 10-4 (AltAdam1, FBF Adam, Optim. Adam)
Learning rate for generator	= 5 × 10-5 (Extra Adam)
	= 2 × 10-5 (AltAdam1, FBF Adam, Optim. Adam)
For our newly proposed WGAN-L1 formulation using 1-Norm regularization, see Table 5, we limited
the hyperparameter search to the step sizes, with the values in Table 4 as initial guesses. We chose
the value performing the best in terms of IS and FID for a sample seed. All other parameters were
kept the same as in Gidel et al. (2019); Bot et al. (2020).
Table 5: Hyperparameters used for the WGAN-L1 formulation (with soft thresholding).
(DCGAN) WGAN-L1 Hyperparameters
Batch size	= 64
Number of generator updates	= 500, 000
Adam β1	= 0.5
Adam β2	= 0.9
L1 regularization for the discriminator	=1 × 10-4
Learning rate for discriminator	= 1 × 10-3 (FBF Adam, Extra Adam)
	= 5 × 10-4 (Optim. Adam)
	= 2 × 10-4 (AltAdam1)
Learning rate for generator	= 1 × 10-4 (FBF Adam, Extra Adam)
	= 5 × 10-5 (Optim. Adam)
	= 2 × 10-5 (AltAdam1)
For the experiments based on the WGAN-GP formulation including spectral normalization we limited
the hyperparameter search to the step sizes, with the values recommended in Gidel et al. (2019) as
initial guesses. We used a single power iteration for the spectral normalization as suggested in Miyato
et al. (2018) and reduced the number of generator updates by a factor of two to ease the computational
burden.
20
Under review as a conference paper at ICLR 2021
Table 6: Hyperparameters used for the WGAN-GP formulation (with spectral normalization).
(ResNet) WGAN-GP Hyperparameters
Batch size	= 64
Number of generator updates	= 250, 000
Adam β1	= 0.5
Adam β2	= 0.9
Gradient penalty	=10
Power iterations for spectral normalization	=1
Learning rate for discriminator	= 5 × 10-4 (FBF Adam, Extra Adam, Optim. Adam)
	= 2 × 10-5 (AltAdam1)
Learning rate for generator	= 5 × 10-4 (FBF Adam, Extra Adam, Optim. Adam)
	= 2 × 10-5 (AltAdam1)
21