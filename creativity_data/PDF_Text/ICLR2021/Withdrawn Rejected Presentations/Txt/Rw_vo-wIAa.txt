Under review as a conference paper at ICLR 2021
Multi-agent Policy Optimization with Approx-
imatively Synchronous Advantage Estimation
Anonymous authors
Paper under double-blind review
Ab stract
Cooperative multi-agent tasks require agents to deduce their own contributions
with shared global rewards, known as the challenge of credit assignment. General
methods for policy based multi-agent reinforcement learning to solve the chal-
lenge introduce differentiate value functions or advantage functions for individual
agents. In multi-agent system, polices of different agents need to be evaluated
jointly. In order to update polices synchronously, such value functions or advan-
tage functions also need synchronous evaluation. However, in current methods,
value functions or advantage functions use counter-factual joint actions which are
evaluated asynchronously, thus suffer from natural estimation bias. In this work,
we propose the approximatively synchronous advantage estimation. We first de-
rive the marginal advantage function, an expansion from single-agent advantage
function to multi-agent system. Further more, we introduce a policy approxima-
tion for synchronous advantage estimation, and break down the multi-agent policy
optimization problem into multiple sub-problems of single-agent policy optimiza-
tion. Our method is compared with baseline algorithms on StarCraft multi-agent
challenges, and shows the best performance on most of the tasks.
1	Introduction
Reinforcement learning(RL) algorithms have shown amazing performance on many single-
agent(SA) environment tasks (Mnih et al., 2013)(Jaderberg et al., 2016)(Oh et al., 2018). However,
for many real-world problems, the environment is much more complex where RL agents often need
to cooperate with other agents. For example, taxi scheduling(Nguyen et al., 2018) and network
control(Chu et al., 2019).
In cooperative multi-agent tasks, each agent is treated as an independent decision-maker, but can
be trained together to learn cooperation. The common goal is to maximize the global return in the
perspective of a team of agents. To deal with such tasks, the architecture of centralized training and
decentralized executions(CTDE) is proposed(Oliehoek & Vlassis, 2007)(Jorge et al., 2016). The
basic idea of CTDE is to construct a centralized policy evaluator, which only works during training
and is accessable to global information. At the same time, each agent is assigned with a local
policy for decentralized execution. The role of the evaluator is to evaluate agents’ local policies
differentially from the global perspective.
A challenge in construction of centralized evaluator is multi-agent credit assignment(Chang et al.,
2004): in cooperative settings, joint actions typically generate only global rewards, making it diffi-
cult for each agent to deduce its own contribution to the team’s success. Credit assignment requires
differentiate evaluation for agents’ local policies, but designing individual reward function for each
agent is often complicated and lacks of generalization(Grzes, 2017)(Mannion et al., 2018). Cur-
rent policy based MARL methods generally realize credit assignment by introducing differentiate
value functions or advantage functions(Foerster et al., 2018)(Lowe et al., 2017). However, these
value functions or advantage functions are estimated asynchronously but decentralized policies are
updated synchronously, as shown in figure 1(b), which results in natural estimation bias.
In this paper, we propose a novel policy based MARL method called multi-agent policy optimiza-
tion with approximatively synchronous advantage estimation(ASAE). In our work, we first define
the counter-factual scenes, in which MA advantage estimation can be converted to SA advantage
estimation. For certain agent, each counter-factual scene is assigned with a SA advantage. Then
1
Under review as a conference paper at ICLR 2021
asynchronous update &
asynchronous estimation
zι x synchronous update &
(' asynchronous estimation
synchronous update &
synchronous estimation
Figure 1: Comparison among three different manners of advantage estimation & update. πa,t repre-
sents the policy of agent a at iteration t and there are n agents in total. Lines with arrow represent
policy updates and ep denotes the update epoch. In single iteration, synchronous update takes only
one epoch while asynchronous update takes n epochs. In advantage estimation, policies need to
be estimated jointly, and the dashed boxes contain joint polices used for advantage estimation in
corresponding update. Particularly, synchronous estimation requires other agents’ future polices.
the marginal advantage function is defined as the expectation of SA advantages on distribution of
counter-factual scenes, and credit assignment is realized by constructing different scenes’ distri-
bution for different agents. Moreover, in order to achieve synchronous advantage estimation, an
approximation of other agents’ joint future policy is introduced. To ensure the approximation is
reliable, a restriction is applied to the original multi-agent policy optimization(MAPO) problem.
The approximate optimization problem is simplified and broken down into multiple sub-problems,
which has a similar form to trust region policy optimization(TRPO) problem. And the sub-problems
are finally solved by proximal policy optimization(PPO) method.
We have two contributions in this work: (1) A novel advantage estimation method called marginal
advantage estimation, which realizes credit assignment for MARL is proposed. More importantly,
this method provides a channel for various SA advantage functions expanding to multi-agent system.
(2) A simple yet effective method for approximatively synchronous advantage estimation is firstly
proposed.
2	Related Work
A common challenge in cooperative multi-agent tasks is credit assignment. RL algorithms designed
for single-agent tasks, ignore credit assignment and take other agents as part of partial observable
environment. Such algorithms perform poorly in complex cooperative tasks which require high
coordination(Lowe et al., 2017). To deal with the challenge, some value based MARL methods
estimate a local Q value for each agent, and the shared global Q value is then constructed through
these local Q values. Value decomposition network(VDN) constructs the global Q value by simply
adding all local Q values together(Sunehag et al., 2018). And in QMIX algorithm(Rashid et al.,
2018), the global Q value is obtained by mixing local Q values with a neural network. In mean field
multi-agent methods, local Q values are defined on agent pairs. The mapping from local Q values
to the global Q value is established by measuring the influence of each agent pair’s joint action to
the global return(Yang et al., 2018).
Similarly, for policy based MARL methods, credit assignment is generally realized through differ-
entiated evaluation with CTED structure. Some naive policy based methods estimate local Q values
for individual agents with a centralized critic(Lowe et al., 2017), resulting in large variance. Some
other methods try to introduce advantage function in MARL. Counter-factual multi-agent policy
gradient(COMA) method(Foerster et al., 2018) is inspired by the idea of difference reward(Wolpert
& Tumer, 2002) and provides a naive yet effective approach for differentiated advantage estimation
in cooperative MARL. In COMA, a centralized critic is used to predict the joint Q value function
Qπ (s, u) of joint action u under state s. And the advantage for agent a is defined as
Aa (s, U) = Q(s, U)- X πa(Ua ∣τa)Q(s, (u-a,u0a))	⑴
u0a
where τ and π represent trajectory and policy respectively. a and -a denote current agent and the
set of other agents respectively. COMA introduces a counter-factual baseline, which assumes that
2
Under review as a conference paper at ICLR 2021
other agents take fixed actions, as shown in figure 1(b). COMA performs synchronous updates
with asynchronous estimation, which leads to lagging and biased advantage estimation. In contrast,
asynchronous estimation & asynchronous updating is more reliable yet more complicated. An ideal
approach is synchronous estimation & synchronous updating. However, it requires prediction of
other agents’ future policies.
3 Background
We consider a most general setting of partially observable, full cooperative multi-agent tasks, which
can be described as a stochastic game defined by a tuple G =< S, U, P, r, Z, O, n, γ >. The true
state of environment s ∈ S is unavailable to all agents. At each time step, n agents identified by
a ∈ A (A = {1, 2, ∙∙∙ ,n}) receive their local observations Za ∈ Z, and take actions Ua ∈ U
simultaneously. The joint observation Z = Zn is acquired by the observation function O(s, a) :
S × A → Z. The next state is determined by joint action u ∈ U (U = Un) and the transition
function P(s0|s, u) : S × U × S → [0, 1]. The reward function r(s, u) : S × U → R is shared by all
agents, so as the discounted return Gt = Pt∞+i γtrt+i. γ ∈ [0, 1) is a discount factor.
In policy based MARL with CTED architecture, each agent has a local trajectory τ a consists of
historical observation and action {(zα,〃§),(〃？，zf),… }. And an independent policy ∏a(ua∣τa)
is constructed for each agent on their local trajectory. Action-state value function Qπ(s, u) and
state value function V π (s) are used to evaluate joint policy. The advantage function is Aπ (s, u) =
Qπ(s, u) - V π(s). For clarity, symbols in bold are used to denote the joint variable of group agents.
In single-agent policy optimization problems(Schulman et al., 2015a), the objective is to maximize
the expected action state value function Eπθ [Qπθ]. Similarly, for MAPO with CTDE structure, each
agent optimize its local policy individually with estimated Q values from centralized critic. Under
this circumstance, the overall objective is
for agent a
mθax E(πθa ,π-a
1 to n :
) hQ(πθa,π-a)i
(2)
Where Q values can be substituted by advantages to reduce the variance.
4 Approximatively Synchronous Advantage Estimation in
Multi-agent System
In this section, we first introduce marginal advantage estimation which expands advantage functions
of SARL to MARL as well to realize credit assignment. And then, we describe how to realize
approximatively synchronous advantage estimation based on the marginal advantage function in
MAPO problem.
4.1	Marginal Advantage Estimation
In this subsection, we are going to solve the challenge of credit assignment through the proposed
marginal advantage estimation. We first consider an counter-factual way where advantages are esti-
mated asynchronously but policies are updated synchronously, as shown in figure 1(b). In this case,
a counter-factual scene can be defined as: at certain state, for agent a, other agent always take fixed
actions. In partially observable, full cooperative multi-agent settings, the counter-factual advantage
of agent a’s action ua under state s is derived based on the joint action’s value(or joint Q value)
function Q(s, u)
Aa (s, u) = Aa(s, (ua, u-a))
=Q(s,U) — / Q(s,u-a,ua) dπα(uα∖τa)
ua
(3)
From the view of agent a, the counter-factual advantage depends on other agents’ joint action u-a,
which is a random variable and u-a 〜∏-a. In order to remove the dependency, the marginal Q
value function of agent a is defined as
Qa(s,Ua) = Eu-a~∏-a
Q(s, (ua, u-a))
(4)
3
Under review as a conference paper at ICLR 2021
Notice that in CTED structure, policy πa(ua∣τa) and π-a(u-a∣τ-a) are independent. By replacing
joint Q value function with marginal Q value function, the marginal advantage function is derived
Aa(s,ua) = Qa(s,ua) - / Qa(s,ua) d∏a(ua∣τa)
ua
Q Q(s,ua,u-a)dπ-a(u-a∣τ-a)-
u-a
Q(s, ua
u-a) dπ-a(u-a ∣τ-a)dπa(ua∣τa)
I
u
Z
u
-a
Q(s, ua, u-a) - Q(s, ua, u-a
ua
)dπa(ua∣τa) dπ-a(u-a∣τ-a)
Aa(s,U) dπ-a(u-a∣τ-a)
-a
(5)
Such replacement will not change the result of advantage estimation because the substitution of joint
Q value is its expectation. Form equation(5), for different agent, the value of marginal advantage is
different, which realizes credit assignment. It can be easily proved that if counter-factual advantage
Aa(s, u) is an unbiased estimation of joint Q value Q(s, u), then marginal advantage is also an
unbiased estimation of marginal Q value(Appendix I).
In a counter-factual scene, from the view of agent a, other agents and their fix joint actions u-a can
be regarded as part of the environment. Let (s, u-a) = sctf and counter-factual advantage function
can be written as
Aa(s,u)=Aa(sctf,ua)
=Q(sctf ,ua) — / Q(sctf ,ua) d∏a(ua∣τa)
ua
(6)
In counter-factual scenes, counter-factual advantage function is identical to advantage function in
SARL, which means the counter-factual advantage in equation(5) can be replaced by any form of
advantage function used in SARL. For example, considering using TD residual δta = r(st, uta) +
γV (st+1) - V (st) as an estimation of joint advantage Aa(st, ut), the marginal advantages could be
written as
∞
Aa(st, ut) : = Eu-a〜∏-a	X γ'δ7+ι
l=0
(7)
Aa(st,ut):= Eu-a 〜∏-a W]
The former is unbiased estimation, but has high variance. The latter is biased estimation for any
V 6= V π , but has much lower variance. These two methods can be combined for compromise
between bias and variance(Schulman et al., 2015b).
As agents’ policies are independent, the expectation in equation(5) can be split into a (n - 1)-layer
integration, which is complicated. For simplicity and efficiency, the Monte-Carlo(MC) sampling
can be applied as a substitution.
Aa(st,ut)
1m
J	Aa(st, Ut )d∏-a ≈ m£Aa (st, Ut)
(8)
Where m is the number of other agents’ joint action samples. The principle of one step process to
calculate marginal advantage with TD residual is shown in figure 2. Firstly, based on the last true
state st, m joint action samples are sampled. These samples are then reorganized. Take agent 1
as example, action u1,t from Sa1 is combined with other agents’ action samples from Sa2 to Sam
respectively. As a result, m reorganized new samples are acquired. Based on these new samples,
one step simulations are executed and m counter-factual rewards and states are acquired, which are
used to calculate the estimation of marginal advantage. At last, the next true state is selected form
counter-factual states.
Both methods in equation(7) use V value predictor and require interactive simulation. Agent needs
to interact with environment to get extra samples. In this work, we consider using centralized critic
to predict joint Q values, and the marginal advantages can be directly calculated with these Q values,
which avoids interactive simulation.
4
Under review as a conference paper at ICLR 2021
Figure 2: One step process of marginal advantage Aa(St, Sa1(U1,t))’s estimation with TD residual.
St represents true state of time step t. Sam represents mth joint action sample and Un,t denotes
the action of agent n in corresponding sample. Blue solid lines connect the reorganized joint action
samples. Based on these samples, one step simulations are executed and St+1,m, rt+1,m represent
counter-factual state and reward acquired in simulation respectively. Finally, the next true state is
selected from these counter-factual states.
4.2	Approximatively Synchronous Advantage Estimation
In marginal advantage estimation, actions are sampled from the agents’ past policies. The estimation
is still asynchronous because it assumes the invariance of others’ policies. However, synchronous
advantage estimation requires the prediction of other agents’ future action, as shown in figure 1(c).
In marginal advantage estimation, problem of action prediction becomes policy prediction.
Direct prediction of others’ future policies is very difficult. In iterative training, only others’ policies
of next iteration are needed. Assume others' joint policy of iteration i is π-a(u-a∣τ-a). Syn-
chronous marginal advantage is given by
Aa,syn (s,ua )= Eu-a Fa [Aa(s,ua L )]
(9)
To calculate the synchronous marginal advantage, we first introduce an approximation that
Aia(s, u) ≈ Aia-1 (s, u). The reliability of this approximation is ensured by a restriction
KL [∏a, ∏a-1] < δSchulman et al. (2015a). For simplicity, We use πa to represent πa(Ua∣τ). In
marginal advantage estimation, we have introduced Monte Carlo(MC) sampling and samples form
others’ joint policy πi-a are needed. HoWever, only polices before iteration i are available. So the
second approximation is introduced as πi--a1 ≈ πi-a . Similarly, in order to ensure the approximation
is reliable, a KL divergence restriction betWeen πi-a and πi--a1 is applied as KL πia , πia-1 < δ. The
objective of policy optimization problem With synchronous advantage estimation for agent a is
max Eua 〜πa
πia	i-1
[Aa-1,syn (S,ua) ∙ ∏:iΓ-
πi-1
= max EuFJAa-1(S, U) ∙表
(10)
subject to : KL πi-a, πi--a1 < δ1
KL πia,πia-1 < δ2
5
Under review as a conference paper at ICLR 2021
The first restriction involves other agents’ polices, which requires joint optimization of all agents’
policies. The integral objective of multi-agent policy optimization with n agents is
n	a	πia
max〉,Eu〜∏i-ι Ai-I(S,U) ∙ —a—
πia a	i-1	πia-1
n
subject to :[ KLπi-a,πi--a1 <δ1	(11)
a
n
[KLπia,πia-1 <δ2
a
It can be proved that K L πi-a , πi--a1 < Po-a KL πio, πio-1 (Appendix II). For simplification, a
tighter form of the restriction KL πi-a , πi--a1 < δ1 can be written as
-a
KL [πo,πo-ι] < -17 = δ1, for o in U	(12)
n-1
By replacing the restriction KL πi-a , πi--a1 < δ1 with the tighter form, the first restriction in
equation(11) is simplified:
n -a
[ [{KL [πio, πio-1] < δ10 }a	(13)
ao
Notice that there are n - 1 duplicate restrictions for each KL πia , πia-1 < δ0, remove redundant
duplicates and the first restrictions in equation(11) finally equals to
n
[KL[πia,πia-1] <δ10	(14)
a
Set δ1 = (n - 1)δ10 = (n - 1)δ2 and the two restrictions in equation(11) can be combined into
SanKL [πia,πia-1] < δ2.
The integral problem of MAPO in equation(11) consists of n individual policy optimization prob-
lems with n sub-restrictions. In CTED structure, policies of different agents are updated indepen-
dently. For agent a, only the sub-restriction KL πia , πia-1 < δ2 is effective. Thus, for further
simplification, the integral objective can be split into n sub-objectives:
for a in 1, 2,…,n :
πia
max Eu〜∏i-ι Ai-I(S,U) ∙ —a —	(15)
πi	πi-1
Subject to : K L [πia , πia-1 ] < δ2
The sub-objectives above are similar to the objective in trust region policy optimization prob-
lemSchulman et al. (2015a). It’s proved that the KL divergence restriction can be effectively re-
placed by a clip operation(Schulman et al., 2017). The sub-objectives of MAPO with ASAE is
finally acquired
for a in 1, 2, ∙∙∙ , n :
m	πa
mαxE Aa-i(s,U) ∙ clip(
πi	1	πi-1
(16)
1 - , 1 + )
5	Experiments
In this section, we use COMA advantage as counter-factual advantage to estimate the approxima-
tively synchronous advantage. And we compare our method with baseline algorithms on the bench-
mark StarCraft multi-agent challenge(SMAC).
6
Under review as a conference paper at ICLR 2021
5.1	experiment settings
StarCraft II is a Real Time Strategy(RTS) game. And SMAC is a popular benchmark for cooperative
MARL algorithms which provides an interface for RL agents to interact with StarCraft II, getting
rewards, observations and sending actions. In our experiments, we consider different types of tasks
of battle games involving both mixed and single type of agents. Specifically, our experiments are
carried out on 8 tasks of different difficulty level, as shown in table 1. In, homogeneous tasks, agents
are of the same type. In symmetric battle scenarios, each army are composed of the same units,
agents need to learn to focus fire without overkill. The asymmetric scenarios are more challenging
because the enemy army always outnumbers allied army by one or more units. In micro-trick tasks,
agents’ are required a higher-level of cooperation and a specific micromanagement trick to defeat
the enemy, which is the most difficult.
Table 1: Information of SMAC tasks in experiments
name	Ally Units	Enemy Units	Type
3m	3 Marines	3 Marines	homogeneous&symmetric
8m	8 Marines 3 Stalkers	8 Marines 3 Stalkers	homogeneous&symmetric
3s5z	5 Zealots 1 Colossi	5 Zealots 1 Colossi	heterogeneous&symmetric
1c3s5z	3 Stalkers 5 Zealots	3 Stalkers 5 Zealots	heterogeneous&symmetric
2m_vs_1z	2 Marines	1 Zealot	micro-trick
10m_vs_11m	10 Marines	11 Marines	homogeneous&asymmetric
2c_vs_64zg	2 Colossi 1 Medivac	64 Zerglings 1 Medivac	micro-trick
MMM2	2 Marauders 7 Marines	3 Marauders 8 Marines	heterogeneous&asymmetric
In our experiment settings, only ally units are considered to be MARL agents. The environment is set
to be partially observable and each agent has a sight range, which is set to be a circular area around
the agent. Only specific attributes of units in sight range are observable. These attributes include
distance, relative x coordinate, relative y coordinate, health and shield. Agents can also observe the
terrain features surrounding them. And in addition, the last actions of ally units are accessable for
all agents. The global state including information about all units on the map and it’s only available
in centralized training. The action space is discrete and consists of 4 moving directions, k attack
actions where k is the number of the enemy units in map, stop and none-operation. The SMAC
environment provides both sparse reward and shaped reward settings, we only consider the shaped
reward situation where reward are much denser.
In ASAE, we use COMA advantage as counter-factual advantage to calculate the approximatively
synchronous advantage. We adopted the CTED architecture and the structure of actors and critic
in our methods is the same to other policy based MARL methods, such as COMA. The centralized
critic is used to predict the joint Q value function of reorganized samples. The state value is calcu-
lated with the average Q values of these reorganized samples, which avoids interactive simulations.
The number of sample m is set to be 50 and the clip range is 0.1.
We compare our method with baseline algorithms including IQL(Tan, 1993), VDN, QMIX and
COMA. In these baselines, only COMA is policy based and most similar to our method. For all
algorithms, good samples are added to replay buffer in initializing stage for early policy training.
And all algorithms are trained for 3 million time-steps. The training batch-size is set to be 32 and
discount factor is 0.99. The learning rates for critic and actors are both 0.0005.
5.2	experiment results
The test wining rates during training are shown in figure 3. Compared to other baseline algorithms,
our algorithms converges fastest and perform best in most of the tasks. Particularly, compared to
7
Under review as a conference paper at ICLR 2021
Figure 3: Test wining rate vs. training steps of various methods on SMAC benchmarks
the other policy based MARL algorithm COMA, our algorithm shows considerable improvement
in 7 tasks of 8. According to the results, in homogeneous & symmetric tasks such as 3m and 8m,
our algorithm converges after 1 million steps of training and reach approximate 100 percent test
wining rate. For homogeneous & asymmetric tasks(10m_vs_11m) and simple heterogeneous &
symmetric tasks such as 3s5z and 1c3s5z, our algorithms converges slower, and the wining rate
fluctuates slightly during training. However, our algorithm also reaches approximate 100 percent
test win rate after 3 million steps of training. For different micro-trick tasks, the performance and
convergence speed of our algorithm varies greatly. While in harder tasks as 10m_vs_11m, MMM2
and 2m_vs_1z, COMA algorithm shows no performance. The wining rate after training is tested
and shown in table 2. Our algorithm also shows the best performance in most of the tasks.
An interesting phenomenon is that, the wining rate curve shows less fluctuation in tasks with ho-
mogeneous ally units, such as 3m, 2m_vs_64Zg, 2m_vs_1z, etc. It,s inferred that, in such tasks,
different agents are functionally replaceable, which provides a higher fault tolerance rate for indi-
vidual agent’s action. As a result, the performance fluctuation of certain agent during training has
less influence on group’s joint policy.
In order to analyse the cooperative strategies learned by agents, we render the battle process between
default AI and agents trained by our algorithms. Some key frames are showed in figure 4. Agents
in red are ally units. In The first task 2s3z, cooperative agents learn to focus fire after training.
Table 2: Test wining rates
ENV	Algorithms				
	ours	COMA	QMIX	IQL	VDN
3m	1.0	0.81	1.0	1.0	1.0
8m	0.97	0.97	1.0	0.91	1.0
3s5z	0.95	0.2	0.84	0	0.72
1c3s5z	1.0	0.83	0.97	0.72	0.94
2m_vs_1Z	1.0	0	1.0	1.0	1.0
10m_vs_11m	0.98	0	0.97	0.31	0.97
2c_vs_64zg	0.97	0.25	0.88	0.78	0.91
MMM2	0.78	0	0.72	0.03	0
8
Under review as a conference paper at ICLR 2021
Focus
Fire
(2s3z)
Adjust
Formation
(MMM2)
Utilize
Terrian
Features
(2m_vs_64zg)
Figure 4: Display of learned cooperative strategies.
While the enemy agents tend to attack the units nearby. After few rounds of crossfire, enemy group
quickly lose the first unit. In the second task MMM2, besides focus fire, cooperative agents also
learn to adjust formation and use skill to avoid being destroyed. Particularly, in micro-trick tasks,
cooperative agents learn to take advantage of map features and units’ differences. As shown in the
third sub-graph, in task 2m_vs_64Zg, only ally units are able to move across terrain. Take advantage
of this, ally units can attack enemy and move across the terrain when enemy approaching thus avoid
being attacked.
6	Conclusion
In this work, we propose a novel method of advantage estimation which address credit assignment
and synchronous estimation in cooperative multi-agent systems. By introducing marginal Q value
function, we derived the marginal advantage function and it’s relationship with counter-factual ad-
vantage function. Then, we define the counter-factual scene where counter-factual advantage can
be replaced by single agent advantage. So we acquire a method for single agent advantage function
expanding to multi-agent systems. Based on marginal advantage, we propose the approximatively
synchronous advantage estimation. Through policy approximation and constrain simplification, the
problem of MAPO is decomposed into multiple sub-problems of SA policy optimization and finally
solved by PPO method. Our algorithms are evaluated on battle tasks of SMAC benchmark. Com-
pared to baselines, our algorithms perform best in both training and testing. Moreover, visualized
battle processes show that our agents acquire heuristic cooperation strategies after training.
For future work, we are interested in applying our algorithm to cooperative robot tasks. For example,
two arms’ cooperation tasks where two arms are treated as individual agents and need to cooperate
to accomplish complicate work. Moreover, because our method provides a channel for SA advan-
tage function expanding to multi-agent system, it’s also interesting to investigate the application of
various policy based RL algorithms on multi-agent scenarios.
References
Yu-Han Chang, Tracey Ho, and Leslie P Kaelbling. All learning is local: Multi-agent learning in
global reward games. In Advances in neural information processing systems, pp. 807-814, 2004.
Tianshu Chu, Sandeep Chinchali, and Sachin Katti. Multi-agent reinforcement learning for net-
worked system control. In International Conference on Learning Representations, 2019.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-
son. Counterfactual multi-agent policy gradients. In Thirty-second AAAI conference on artificial
intelligence, 2018.
9
Under review as a conference paper at ICLR 2021
Yasuhiro Fujita and Shin-ichi Maeda. Clipped action policy gradient. In International Conference
on Machine Learning,pp. 1597-1606, 2018.
Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient
estimates in reinforcement learning. Journal of Machine Learning Research, 5(Nov):1471-1530,
2004.
Marek Grzes. Reward shaping in episodic reinforcement learning. 2017.
Max Jaderberg, Volodymyr Mnih, Wojciech M. Czarnecki, Schaul Tom, Leibo Joel Z., Silver David,
and Kavukcuoglu Koray. Reinforcement learning with unsupervised auxiliary tasks. arXiv
preprint arXiv:1611.05397, 2016.
Emilio Jorge, Mikael Kageback, Fredrik D Johansson, and Emil Gustavsson. Learning to Play guess
who? and inventing a grounded language as a consequence. arXiv preprint arXiv:1611.03218,
2016.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Ryan Lowe, Yi I Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
agent actor-critic for mixed cooperative-competitive environments. In Advances in neural infor-
mation processing systems, pp. 6379-6390, 2017.
Patrick Mannion, Sam Devlin, Jim Duggan, and Enda Howley. Reward shaping for knowledge-
based multi-objective multi-agent reinforcement learning. The Knowledge Engineering Review,
33, 2018.
Volodymyr Mnih, Kavukcuoglu Koray, Silver David, Graves Alex, Antonoglou Ioannis, Wierstra
Daan, and Riedmiller Martin. Playing atari with deep reinforcement learning. arXiv preprint
arXiv:1312.5602, 2013.
Duc T. Nguyen, Kumar Akshat, and Hoong Chuin L. Credit assignment for collective multiagent rl
with global rewards. volume 1806.05635, pp. 8102-8113, 2018.
Junhyuk Oh, Guo Yijie, Singh Satinder, and Lee Honglak. Self-imitation learning. volume
1806.05635, pp. 3878-3887, 2018.
Frans A Oliehoek and Nikos Vlassis. Q-value functions for decentralized pomdps. In Proceedings
of the 6th international joint conference on Autonomous agents and multiagent systems, pp. 1-8,
2007.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent
reinforcement learning. arXiv preprint arXiv:1803.11485, 2018.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-
dimensional continuous control using generalized advantage estimation. arXiv preprint
arXiv:1506.02438, 2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Peter Sunehag, Guy Lever, Audrunas Gruslys, WCjciech Marian Czarnecki, VinIciUS Flores Zam-
baldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z Leibo, Karl Tuyls, et al. Value-
decomposition networks for cooperative multi-agent learning based on team reward. In AAMAS,
pp. 2085-2087, 2018.
Ming Tan. Multi-agent reinforcement learning: Independent vs. cooperative agents. In Proceedings
of the tenth international conference on machine learning, pp. 330-337, 1993.
10
Under review as a conference paper at ICLR 2021
PaWeI WaWrzynski. Real-time reinforcement learning by sequential actor-critics and experience
replay. NeuraINetworks, 22(10):1484-1497, 2009.
David H Wolpert and Kagan Tumer. Optimal payoff functions for members of collectives. In
Modeling complexity in economic and social systems, pp. 355-369. World Scientific, 2002.
Yaodong Yang, Rui Luo, Minne Li, Ming Zhou, Weinan Zhang, and Jun Wang. Mean field multi-
agent reinforcement learning. In International Conference on Machine Learning, pp. 5571-5580,
2018.
A Appendix I
Assume that joint advantage Aa(s, u) is a unbiased estimation of joint Q value function Q(s, u).
Then
Aa(s,u)=Q(s,u)-bs
where : Vθb§ ≡ 0
(17)
Then
Aa(s,u) = Eπ-a [Aa(s, u)]
= Eπ-a [Q(s, u) - bs]
= Eπ-a [Q(s, u)] - Eπ-a [bs]
Eπ-a [Q(s, u)] is exact the marginal Q value function and VθEπ-a [bs] = Eπ-a [Vθbs] ≡ 0
B Appendix II
Consider tWo agents, Whose policies of episode i are represented by πi1 and πi2 respectively.
KL [π1π2,π1-1 π2-1] = Z π1π2 log M2	du
πi-1πi-1
πi1	πi2
log -1----+ log -2— ) du
πi1-1	πi2-1
<	∏1 log πi du + ∏ π2 log ； du
πi1-1	πi2-1
=KL [πi1,πi1-1 + KL [πi2,πi2-1
The relation can be expanded to joint distribution of other agents’ policies
KL [∏-a, ∏-a1] = / Y∏0 log F-a ∏o du
o	o πio-1
-a
< XKL [πio,πio-1
o
(18)
(19)
(20)
11