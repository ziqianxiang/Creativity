Under review as a conference paper at ICLR 2021
Provab le More Data Hurt in High Dimensional
Least Squares Estimator
Anonymous authors
Paper under double-blind review
Ab stract
This paper investigates the finite-sample prediction risk of the high-dimensional
least squares estimator. We derive the central limit theorem for the prediction
risk when both the sample size and the number of features tend to infinity. Fur-
thermore, the finite-sample distribution and the confidence interval of the predic-
tion risk are provided. Our theoretical results demonstrate the sample-wise non-
monotonicity of the prediction risk and confirm “more data hurt” phenomenon.
1 Introduction
More data hurt refers to the phenomenon that training on more data can hurt the prediction per-
formance of the learned model, especially for some deep learning tasks. Loog et al. (2019) shows
that various standard learners can lead to sample-wise non-monotonicity. Nakkiran et al. (2019)
experimentally confirms the sample-wise non-monotonicity of the test accuracy on deep neural net-
works. This challenges the conventional understanding in large sample properties: if an estimator is
consistent, more data makes the estimator more stable and improves its finite-sample performance.
Nakkiran (2019) considers adding one single data point to a linear regression task and analyzes its
marginal effect to the test risk. Derezinski et al. (2019) gives an exact non-aSymPtotic risk of the
high-dimensional least squares estimator and observes the sample-wise non-monotonicity on mean
square error. For adversarially robust models, Min et al. (2020) Proves that more data may increase
the gaP between the generalization error of adversarially-trained models and standard models. Chen
et al. (2020) shows that more training data causes the generalization error to increase in the strong
adversary regime. In this work, we derive the finite-samPle distribution of the Prediction risk under
linear models and Prove the “more data hurt” Phenomenon from an asymPtotic Point of view.
Intuitively, the “more data hurt” stems from the “double descent” risk curve: as the model com-
Plexity increases, the Prediction risk of the learned model first decreases and then increases, and
then decreases again. The double descent Phenomenon can be Precisely quantified for certain sim-
Ple models (Hastie et al. (2019); Mei & Montanari (2019); Ba et al. (2019); Belkin et al. (2019);
Bartlett et al. (2020); Xing et al. (2019)). Among these works, Hastie et al. (2019) and Mei & Mon-
tanari (2019) use the tools from random matrix theory and exPlicitly Prove the double descent curve
of the asymPtotic risk of linear regression and random features regression in high dimensional setuP.
Ba et al. (2019) gives the asymPtotic risk of two-layer neural networks when either the first or the
second layer is trained using a gradient flow.
The second decline of the Prediction risk in the double descent curve is highly related to the more
data hurt Phenomenon. In the over-Parameterized regime when the model comPlexity is fixed while
the samPle size increases, the degree of over-Parameterization decreases and becomes close to the
interPolation boundary (for examPle p/n = 1 in Hastie et al. (2019)), in which a high Prediction
risk is achieved. However, the existing asymPtotic results, which focus on the first-order limit of the
Prediction risk, cannot fully describe the more data hurt Phenomenon. In fact, the “double descent”
curve is a function of the limiting ratio lim p/n, which may not be able to characterize the emPirical
Prediction risk in finite samPle situations. There will be a non-negligible discrePancy between the
emPirical Prediction risk and its limit, esPecially when the samPle size or dimension is small. Fine-
grained second-order results are thus needed to fully characterize such discrePancy and further, a
confidence band for the Prediction risk can be constructed to evaluate its finite samPle Performance.
We take Figure 1 as an examPle to illustrate this. According to the first-order limit, given a fixed
dimension p = 100, the Prediction risks at samPle size n = 90 and n = 98 are about 10.20 and
1
Under review as a conference paper at ICLR 2021
49.02. More data hurt seems true. However, the 95% confidence interval of the prediction risks with
sample size 98 is [4.91, 142.12], which contains the risk for n = 90. Then more data hurt is not
statistically significant. Hence, in this work, we characterize the second-order fluctuations of the
prediction risk and make attempts to fill this gap. We employ the linear regression task in Hastie
et al. (2019) and Nakkiran (2019), and introduce new tools from the random matrix theory, e.g. the
central limit theorems for linear spectral statistics in Bai & Silverstein (2004); Bai et al. (2007), to
derive the central limit theorem of the prediction risk.
Consider a linear regression task with n data points and p features, the setup of the more data hurt
is similar to that in the classical asymptotic analysis in Van der Vaart (2000). According to the
classical asymptotic analysis With P fixed and n → ∞, the least square estimator is unbiased and
√n-consistent to the ground truth. This implies that the more data will not hurt and even improve
the prediction performance. HoWever, the story is very different in the over-parameterized regime.
The prediction risk doesn’t decrease monotonously with n when p > n. More data does hurt in
the over-parameterized case. In the following, we will justify this phenomenon by developing the
second-order asymptotic results as both n and p tend to infinity. We assume p/n → c, and denote
0 < n1 < n2 < +∞, c1 = p/n1 and c2 = p/n2. Then the direct comparison of the prediction risk
between sample sizes n1 and n2 can be decomposed into three parts: (i) the gap between the finite-
sample risk under n = n1 and the asymptotic risk with c = c1; (ii) the gap between the finite-sample
risk under n = n2 and the asymptotic risk with c = c2; (iii) the comparison between two asymptotic
risks under c = c1 and c = c2. Theorem 1 and 2 of Hastie et al. (2019) give answers to the task (iii).
For (i) and (ii), we develop in this paper the convergence rate and the limiting distribution of the
prediction risk as n, p → +∞, p/n → c. Furthermore, the confidence interval of the finite-sample
risk can be obtained as well.
50	100	150
Sample Size
Density
H
0.0	1
0	25	50	75	100	125	150	175	200
Sample Size
Confidence Interval
Figure 1: Sample-wise double descent. We take p = 100 and 1 ≤ n ≤ 200. Left: The conditional
density of the prediction risk when sample size varies from 1 to 200. According to the conditional
distribution of the prediction risk, we can observe the sample-wise double descent phenomenon.
Right: The 95%-confidence band (point-wise) of the prediction risk. In the over-parameterized
regime 1 ≤ n < 100, there exists some pairs (n1, n2), 1 ≤ n1 < n2 < 100 such that the upper
boundary of the confidence interval at n1 is smaller than the lower boundary of the confidence
interval at n2 . This confirms the more data hurt phenomenon.
The main goal of this paper is to study the second order asymptotic behavior of two different types
of conditional prediction risk in the linear regression model. One is RX,β (β, β) given both the
training data and regression coefficient while the other is RX (β, β) given the training data only.
We summarize our main results as follows: (1) The regression coefficient is set to be either random
or nonrandom to cover more cases. Different convergence rates and limiting distributions of both
prediction risk are derived under various scenarios. (2) In particular, the finite-sample distribution of
the conditional prediction risk given both the training data and regression coefficient is derived and
the sample-wise double descent is characterized in Theorem 4.2 and Theorem 4.5 (see Figure 1).
Under certain assumptions, the more data hurt phenomenon can be confirmed by comparing the
confidence intervals built via the central limit theorems. (3) Our results incorporate non-Gaussian
observations. For Gaussian data, the limiting mean and variance in the central limit theorems have
simpler forms, see Section 4.2 and 4.3 for more details.
The rest of this paper is organized as follows. Section 3 introduces the model settings and two
different prediction risk. Section 4 presents the main results on CLTs for the two types of risk with
2
Under review as a conference paper at ICLR 2021
discussion. Section 5 conducts simulation experiments to verify the main results. All the technical
proofs and lemmas are relegated to the appendix in the supplementary file.
2	Related work
Double Descent The double descent curve describes how generalization ability changes as model
capacity increases. It subsumes the classical bias-variance trade-off, a U-shape curve, and further
show that the test error exhibits a second drop when the model capacity exceeds the interpolation
threshold (Belkin et al. (2018); Geiger et al. (2019); Spigler et al. (2019); Advani & Saxe (2017)).
The double descent phenomenon has been quantified for certain models, including two-layer neural
networks via non-asymptotic bounds or asymptotic risk (Belkin et al. (2019); Muthukumar et al.
(2020); Hastie et al. (2019); Mei & Montanari (2019); Ba et al. (2019)). As our results are based on
the linear regression, we mainly focus on the literature of linear models. Muthukumar et al. (2020)
and Bartlett et al. (2020) derive the generalization bounds for over-parametrized linear models and
show the benefits of the interpolation. Hastie et al. (2019) gives the first-order limit of the gen-
eralization error for linear regressions as n,p → +∞. Derezinski et al. (2019) provides an exact
non-asymptotic expression for double descent of the high-dimensional least square estimator. Wu
& Xu (2020) extends the first-order limit of the prediction error of the generalized weighted ridge
estimator to more general case with anisotropic features and signals. Montanari et al. (2019), Deng
et al. (2019) and Kini & Thrampoulidis (2020) investigate the sharp asymptotic of binary classifica-
tion tasks with the max-margin solution and the maximum likelihood solution. Emami et al. (2020)
and Gerbelot et al. (2020a) consider the double descent in generalized linear models. Furthermore,
the double descent phenomenon is also observed on linear tasks with various problems and assump-
tions, e.g. LeJeune et al. (2020); Gerbelot et al. (2020b); Javanmard et al. (2020); Dar & Baraniuk
(2020); Xu & Hsu (2019); Dar et al. (2020). Xing et al. (2019) sharply quantifies the benefit of in-
terpolation in the nearest neighbors algorithm. Mei & Montanari (2019) derives the limiting risk on
the random features model and shows that minimum generalization error is achieved by highly over-
parametrized interpolators. Ba et al. (2019) gives the limiting risk of the regression problem under
two-layer neural networks. However, the existing asymptotic results focus on the first-order limit of
the prediction risk and do not indicate the convergence rate. There are very few second-order results
in the literature, Shen & Bellec (2020) establishes the asymptotic normality for the derivatives of
2-layers neural network, but not the exact limiting distribution of the risk. In this work, we are the
first to develop results on second-order fluctuations of the prediction risk in linear regressions and
provide its corresponding confidence intervals. The more data hurt phenomenon is further justified
from the asymptotic point of view.
Random Matrix Theory The primary tool for analyzing the second-order fluctuations of prediction
risk comes from random matrix theory. In particular, Bai & Silverstein (2004) refines the central
limit theorem for linear spectral statistics of large dimensional sample covariance matrix with gen-
eral population and the population is not necessary to be Gaussian. Similar central limit theorems
are also developed for other random matrix ensembles, see Sinai & Soshnikov (1998); Bai & Yao
(2005); Zheng (2012). Other than the central limit theorem for linear spectral statistics, Bai et al.
(2007) and Pan & Zhou (2008) study the asymptotic fluctuation of eigenvectors of sample covari-
ance matrices. Bai & Yao (2008) considers the fluctuation of quadratic forms. All these technical
tools and results are adopted and fully utilized in this paper, especially those related to Stieltjes
transform, which are closely connected to the prediction risk studied in this paper.
3	Preliminaries
3.1	Problem, data and estimator
Suppose that the training data {(xi, yi) ∈ Rp × R, i = 1, 2, . . . , n} is generated independently from
the model (ground truth or teacher model):
Vi = β Xi + J, and (Xi, J) 〜(PX, Pe), i = 1, 2, . . . ,n.	(I)
Here, Px is a distribution on Rp such that E(xi) = 0, Cov(xi) = Σ, and P is a distribution
on R such that E(i) = 0, Var(i) = σ2. In particular, the coordinates of Xi are not necessarilV
3
Under review as a conference paper at ICLR 2021
independent, that is, Σ is not restricted to be diagonal. To proceed further, we denote
Xn×p = (x1,x2,. . . ,xn)T, y = (y1,y2, . . . , yn)T.
The minimum `2 norm (min-norm) least squares estimator, of y on X, is defined by
β = argmin ky - Xβk2 = (XTX)+XTy,	(2)
β
where (XTX)+ denotes the Moore-Penrose pseudoinverse of XTX.
3.2	Bias, variance and risk
Similar to Hastie et al. (2019), we define two different types of out-of-sample prediction risk. The
first one is given by
Rχ(β, β) = E[(xTβ - xTβ)2∣X] = E[kβ - βk∑∣X],	⑶
where xo 〜Px is a test point and is independent of the training data, and the notation kβk∑ stands
for β T Σβ. Here β is assumed to be a random vector independent of x0 . In this definition, the
expectation E stands for the conditional expectation for x0, β and β when X is given. According to
the bias-variance decomposition, we have Rχ(β , β) := Bχ (β , β ) + Vχ(β, β), where
Bχ(β, β)= E{kE(β∣X)- βk∑∣X} and Vχ(β, β) = Tr{Cov(β∣X)∑}.	(4)
Plugging the model (1) into the min-norm estimator (2), the bias and variance terms can be rewritten
as
2
Bχ(β, β)= E{βτ∏∑∏β∣X} and Vχ(β, β) = -- Tr(Σ +Σ),
where Σ = XTX/nis the (uncentered) sample covariance matrix of X, and Π = Ip - Σ+Σ is the
projection onto the null space of X.
The second type of out-of-sample prediction risk is defined as
Rχ,β (β, β) = E[(xTβ - xTβ)2∣X, β] = E[kβ - βkΣ∣X, β],	(5)
where
2
-
Bχ,β(β, β) = βτ∏∑∏β and Vχ,β(β, β) = Vχ(^, β)=反 Tr(Σ+Σ).
In this definition, the parameter β is assumed to be given. The expectation E is the conditional
expectation for x0 and β when X and β are given. This is consistent with the commonly-used
testing procedure, in which a trained model is evaluated by the average loss on unseen testing data.
4 Main Results
Before stating our main results, we briefly highlight the challenges we faced in proving the more
data hurt phenomenon. First, the finite-sample behaviors of the prediction risk is required. Hastie
, 1 ∕r-∖Γ∖-i ZΛ∖	. 1 C, F 1 ∙	l' 1 , 1 TΛ /片 C∖ FC /片 C∖	,	F
et al. (2019) gives the first-order limits of both Rχ,β(β, β) and Rχ(β , β) as n, p → +∞ and
p/n → c ∈ (0, +∞). However, to prove the more data hurt phenomenon, we should fix p and
investigate the finite-sample risk with sample size n varies. This implies that only knowing the first-
order limit is not enough, the convergence rate is also needed. To solve this problem, we have derived
the central limit theorems for both Rχ,β(β, β) and Rχ(β, β), respectively, which characterize the
second-order fluctuations of the risk. Then we can figure out the finite-sample behavior of the risk
by computing the gap between the risk and its limit. The confidence intervals of the risk can be
further obtained. Second, the parameter β also contributes randomness to the finite-sample risk,
which further influences the convergence rate. To analyze the contribution of β, we need to make
use of the technical tools and asymptotic results for eigenvectors and quadratic forms developed in
Bai et al. (2007) and Bai & Yao (2008). Another interesting finding is that, in the over-parameterized
regime such that p > n, the two types of out-of-sample prediction risk Rχ,β (β , β ) and Rχ (β , β )
enjoy different convergence rates.
4
Under review as a conference paper at ICLR 2021
4.1	Assumptions and more notations
As follows are some notations used in this paper. Thep × p identity matrix is denoted by Ip. For a
symmetric matrix A ∈ Rp×p , we define its empirical spectral distribution as
1p
FA(X) = P N 1{"(A) ≤ x}
where 1{∙} is the indicator function and λi(A), i = 1, 2,...p are the eigenvalues of A. The notation
→ stands for the convergence in distribution. Za∕2 is the a/2 upper quantile of the standard normal
distribution, λmax(A) and λmin(A) denote the largest and smallest eigenvalues of A, respectively.
Here we list all the assumptions for X and β needed under different scenarios:
(A)
(B1)
(B2)
Xj 〜 Px is of the form Xj = ∑1∕2zj, Where Zj is a p-length random vector with i.i.d.
entries that have zero mean, unit variance, and a finite 4-th order moment E(zi4j ) = ν4 ,
i = 1, …，P, j = 1,…，n.
Σ is a deterministic positive definite matrix, such that 0 < c0 ≤ λmin (Σ) ≤ λmax(Σ) ≤
c1 , for all n, p and some constants c0 , c1 . As p → ∞, we assume that the empirical spectral
distribution FΣ converges weakly to a probability measure H.
Σ is an identity matrix, Σ = Ip .
(C1) β is a nonrandom constant vector, and kβk22 = βTβ = r2 .
2
(C2) β 〜 Pe is independent of X and follows multivariate Gaussian distribution Np(0, TpIp).
Throughout this paper, we consider the limiting distributions and the convergence rates of the out-
of- sample prediction risk when n,p → ∞ such that p/n = cn → c ∈ (0, ∞). If c > 1, the sample
size n is smaller than the number of parameters p, we call this case “over-parametrized”. Otherwise
when c < 1, we call it “under-parameterized”.
4.2	Under-parametrized asymptotics
In this section, we focus on the risk of the min-norm estimator (2) in the under-parametrized regime.
&	i∙	, El	t t' TT J . i zz>∕a -t r∖∖ i ,ι τ-ι /片 zα∖ ι τ-ι /片 zα∖	,
According to Theorem 1 of Hastie et al. (2019), both BX,β(β, β) and BX(β, β) converge to
σ2c∕(1 - C) almost surely. The following Theorem 4.1 and 4.2 show that both Bχ(β, β) and
Bχ,β(β, β) converge to σ2c∕(1 — c) at the rate 1 /p. Furthermore, the limiting distributions are de-
rived by making use of the CLT for linear spectral statistics of large-dimensional sample covariance
matrices.
Theorem 4.1.	Suppose that the training data is generated from the model (1), and the assumptions
(A) and (B1) hold. Then the first type of out-of-sample prediction risk RX(β, β) of the min-norm
estimator (2) satisfies that, as n, p → ∞ such that p/n = cn → c < 1,
where
μc
Conclusively,
2
p(Rχ(β, β) - -n-) → N(μc,σ2),
1 - cn	c
c2σ2	σ2c2 (ν4 - 3)	2	2c3σ4
7----τv7 +----;------- and σc =	+
(c-1)2	1-c	c	(c-1)4
一 ，一	„	z ^	一	、
P(La,c ≤ RX(β, β) ≤ Uα,c) → 1 - α,
c3 σ4 (ν4 — 3)
(1-c)2
where 1 - α is the confidence level and
(6)
(7)
cnσ	1	cnσ	1
La,c = 1--C----+ P (Pc - Za∕2σc ),	Ua,c = ɪ--C---+ P (Pc + Za∕2σc)∙
Under the assumptions of Theorem 4.1, we know that Π = IP - Σ+ Σ = 0 and
σ
Bχ(β, β) = Bχ,β(β, β) = O,	VX(β, β) = Vχ,β(β, β)=瓦 Tr(∑ +∑).
El τ~ι /片 zα∖	ι , ^γλ /片 c∖ ι,ι	∙ ι ι .ι	, , ∙ ι ∙
Thus Rχ(β, β) equals to Rχ,β(β, β) and the two risk share the same asymptotic limit.
5
Under review as a conference paper at ICLR 2021
Theorem 4.2.	Under the assumptions of Theorem 4.1, the second type of out-of-sample prediction
risk RX,β (β, β) of the min-norm estimator (2) satisfies that, as n,p → ∞ such that p/n = cn →
c< 1,
2
p(RX,β(β, β) -君-------)→ N(μc, σ2),
1 - cn
and
一 ，一	一	z ^	一	、
P(La,c ≤ RX,β (β, β) ≤ Uα,c) → 1 - α,
where μc, σ2, La,c and Ua,c are the Same as those in Theorem 4.1.
4.3 Over-parametrized asymptotics
In this section, we consider the min-norm estimator (2) in the over-parametrized case c > 1. The bias
term, either BX(β, β) or BX,β(β, β), is generally nonzero. According to Lemma 2 in Hastie et al.
(2019), both BX(β, β) and BX,β(β, β) converge to r2(1 - 1/c) as n,p → +∞ and p/n → c > 1.
This implies that the bias term can influence the asymptotic behavior of the prediction risk, including
the convergence rate. Hence to derive the CLT of the out-of-sample prediction risk, we need to
consider both the bias and variance terms in (4).
In the following, we investigate the asymptotic properties of the two prediction risk RX(β, β) and
RX,β(β, β) under various combinations of the assumptions (A1), (B2) for X and scenarios (C1),
(C2) for both random and nonrandom β. We start with the case when β is a constant vector.
Theorem 4.3.	Suppose that the training data is generated from the model (1), and the assumptions
(A), (B2) and (C1) hold. Then the first type of out-of-sample prediction risk RX(β, β) of the min-
norm estimator (2) satisfies that, as n,p → ∞ such that p/n = cn → c > 1,
√pnRχ^,β) -(I —)r2 —^o →N (μc,ι,σc,ι),	⑻
cn	cn - 1
where μc,ι = 0 and σ2,ι = 2(c-1) r4. A more practical version is to replace μc,ι and σ2,ι with
1	cσ2	σ2(ν4 - 3)	2	2(c - 1) 4	1	2c3σ4	cσ4(ν4 - 3)
μc,1 = √pl (1 - c)2 + C - 1 J and σc,1 = -C2- r + p∖ (1 - c)4 +	(C -1)2 J.
Conclusively,
_ , _ _ ^ 一 .
P(La,c ≤ RX(β, β) ≤ Ua,c) → 1 - α,	⑼
where 1 - α is the confidence level and
La,c	=	(1-----)r2	+----+ +-----丁印c,1	- Zɑ∕2σc,1),
Cn	Cn - 1 p
Ua,c	=	(I-----)r2	+----+ +-----尸 (μc,1	+ Za∕2σc,I).
Cn	Cn 一 1	√P
TTB	1 Λ Λ TT 1	,♦	∕CT∖1-1/ 片 C∖	T-I	/片 C∖	TC /片 C∖	T~l	/片 C∖
Remark 4.1. Under assumption (C1), BX(β, β) = BX,β (β, β) and RX(β, β) = RX,β (β, β).
Thus Theorem 4.3 still holds if we replace RX(β, β) with RX,β (β, β).
Remark 4.2. Under Assumption (B2), the eigenvector of Σ is asymptotically Haar distributed.
Therefore, the bias term BX(β, β) is only related to the length ofβ. However, in the anisotropic
settings with general Σ, the eigenvector of the Σ is no longer asymptotically Haar distributed. The
limiting behavior of BX(β, β) heavily relies on the interaction between β and the eigenvectors of
Σ. Therefore, we conjecture that there is no universal convergence rate for the bias term BX (β, β)
that can cover arbitrary non-random β and anisotropic Σ in the over-parametrized case, not to
mention the prediction risk RX(β, β). A small simulation experiment is conducted in Appendix G
to confirm our conjecture on this point.
Next we consider the case when β is a random vector that follows assumption (C2).
6
Under review as a conference paper at ICLR 2021
Theorem 4.4.	Suppose that the training data is generated from the model (1), and the assumptions
(A), (B2) and (C2) hold. Then as n, p → ∞ such that p/n = cn → c > 1, the first type of
out-of-sample prediction risk RX(β, β) of the min-norm estimator (2) satisfies,
PnRX(e, β) - (I-------)r2-------70 → N(μc,2, σ2,2),
cn	cn - 1
where
cσ2	σ2(ν4 - 3)	2	2c3σ4	cσ4(ν4 - 3)
μc,2 = (T-τρ + C-1 and σc,2 = (T-∑f + (C-1)2
Hence we have
一 ，一	„	z ^	一	、
P(La,c ≤ Rχ(∣β, β) ≤ Uα,c) → 1 - α,
where
La，c = C^^-ɪ + (I - C )r2 + p (μc,2 - Za∕2σc,2),
Uα,c = C--------1 + (1 — - )r2 + p (μc,2 + Za∕2σc,2).
As for RX,β(β, β), we have the following theorem.
Theorem 4.5.	Suppose that the training data is generated from the model (1), and the assumptions
(A), (B2) and (C2) hold. Then, as n,p → ∞ such that p/n = Cn → C > 1, the second type of
out-of-sample prediction risk RX,β (β, β) of the min-norm estimator (2) satisfies,
√pnRX,βB β) - (1 - - )r2 - C - 1 O → N(μc,3,σ2,3),	(10)
where μc,3 = 0 and σ2,3 = 2(1 一 1 )r4. A more practical version is to replace μc,3 and σ2,3 with
The corresponding (1 - α)-confidence interval is given by
2	1 4	1	2C3σ4	Cσ4(ν4 - 3)
and σc,3 = 2(1 - C)r + P {(T-^F +	(C - i)2
_ , _ _ ^ 一 .
P(La,c ≤ RX,β (β, β) ≤ Ua,c) → 1 一 α,	(II)
with
Lα,c	=	----+ + (1--------)r2	+---=Lcpc- -	Za∕2σc,3),
Cn - 1	Cn	P
Ua,c	=	----+ + (1--------)r2	+-^(μc,3 +	Za∕2σc,3).
Cn ― 1	Cn	PP	/
Remark 4.3. Note that besides the leading constants in (μc,3,σc,3), the version (pc,3, σc,3) also
contains smaller order terms, including terms oforder O(1∕√p) in pc,3 and terms oforder O(1∕p)
in σc,3. These smaller order terms will vanish when P and n grow very large, but for finite sam-
ple situations, these smaller order terms will provide a finer approximation for the finite sample
distribution ofRX,β(β , β ). As shown in the following experiments, these terms have indeed made
non-negligible contributions to fitting the empirical distribution of RX,β (β, β ), which sheds new
lights for practitioners.
Remark 4.4. Ifwe compare the results in Theorem 4.3 and 4.5, we will find out that RX (β, β ) with
constant β and RX,β (β , β ) with random β share the same first-order limit and second-order error
rate O(P-1/2). This is quite intuitive because both risk treat β as a constant. Their differences are
reflected in their limiting variances. Nevertheless, it’s very interesting to observe from Theorem 4.4
that, RX(β , β) with random β under the over-parametrized case has a smaller second-order error
rate O(P-1). It enjoys the same rate as the under-parametrized case in Theorem 4.1. A possible
explanation would be that averaging over the randomness in β can partially offset the curse of
dimensionality so that RX(β , β) achieves the same error rate for all P, n combinations.
Remark 4.5. It’s worth mentioning that the only assumption regarding data distribution is assump-
tion (A), where only finite fourth order moment is required. Non-Gaussianity allows our theoretical
results more widely applied.
7
Under review as a conference paper at ICLR 2021
4.4 Discussion
In this section, we first make a short conclusion of what we have done theoretically in this paper and
further discuss some possible directions of extension.
We have systematically investigated the second-order fluctuations of two types of prediction risk,
RX (β, β) and RX,β (β, β), for the high dimensional least square estimators β. Theorem 4.1 and
4.4 are for RX(β, β) while Theorem 4.2 and 4.5 are for RX,β(β, β). Both fixed effect and ran-
dom effect of the regression coefficients β are discussed following the settings in Hastie et al.
(2019). RX(β, β) and RX,β(β, β) are the same when β is nonrandom as established in Theo-
rem 4.3. Asymptotic results are categorized into the under-parametrized case (p < n) and the
over-parametrized case (p > n).
Although the first-order limits of the prediction risk in high dimensional linear models have already
been well studied in recent years, including general extensions to anisotropic features and signals in
Wu & Xu (2020), the “double descent” risk curve is just a function of the limiting ratio lim p/n.
There is still a non-negligible discrepancy between the finite-sample prediction risk and its first-order
limit on the “double descent” curve. How large is this discrepancy? How fast does the risk converge
to its limit? Our CLTs provide answers to such questions and give a fine-grained characterization of
the second-order fluctuations of the prediction risks. Not only explicit forms of the leading constants
in the limiting means and variances are shown in the main theorems, smaller order terms are also
derived to improve the empirical performance for practitioners.
It is also important to recognize the limitations of our results. First, the present paper only concerns
linear regression tasks since the linear regression task is simple but important as well. Some recent
works linearize neural networks at the initialization and employ Neural Tangent Kernels (Jacot et al.
(2018)) to approximate the training procedure of a strongly over-parametrized neural network by
solving a linear regression task, e.g. Du et al. (2018); Arora et al. (2019); Lee et al. (2019). Though
the setting considered in this paper is simple and limited, the problem has not however been fully
understood so far in the literature. Therefore, we are among the first to take the task and develop the
second-order fluctuation results for the prediction risk. Second, we assume general covariance Σ and
non-Gaussianity for the under-parametrized case, which fits the most updated and realistic settings in
the literature, however we only investigate the isotropic settings while still allow for non-Gaussianity
under over-parametrization. We haven’t extended itto the more general anisotropic settings yet. The
reasons are two-fold. On the one hand, according to Wu & Xu (2020), the first-order limits depend
on the Stieltjes transforms of the unknown spectral distribution of Σ. Since Σ is unknown, we
cannot obtain any explicit characterization of the first-order limits, not to mention the second-order
fluctuations. The CLTs would only be written as certain complicated implicit functions of Σ and
would be too abstract to evaluate practically. More restrictions would be imposed on Σ to guarantee
the second-order convergence. On the other hand, from the technical perspective, the techniques
required for anisotropic over-parametrized cases are very different from the isotropic cases due to
difference in the bias-variance decomposition in (4). The tools in random matrix theory have not
been fully developed yet for anisotropic cases. Since we have considered various scenarios in this
paper, including random and nonrandom signals β for both conditional and unconditional risks, it
will take great efforts and continuous work to extend all of them to the most general settings, which
would lead to many subsequent works in the field of machine learning and random matrix theory
literature.
5	Experiments
In this section, we carry out simulation experiments to examine the central limit theorems and the
corresponding confidence intervals in Theorem 4.2 and Theorem 4.5. We generate data points from
the linear model (1) and directly compute the prediction risk via the bias-variance decomposition
in (4). The generative distribution Px is taken to be the standard normal distribution. The noise
distribution P is taken to be N(0, 1). In the following, we present the gap between the finite-sample
distribution of the prediction risk and the corresponding limiting distribution to check the central
limit theorems and use the cover rate to measure the effectiveness of the confidence intervals. More
simulation results, including cases with non-Gaussian distributions for Px and P are relegated to
the Appendix due to space limitations.
8
Under review as a conference paper at ICLR 2021
Example 1.	This example examines the results in Theorem 4.2. We define a statistic
According to Theorem 4.2, Tn weakly converges to the standard normal distribution as n, p → ∞.
In this example, C = 1/2 andp= 50, 100, 200. The finite-sample distribution of Tn is presented by
the histogram of Tn in Figure 2 with 1000 repetitions, where the solid blue curve stands for standard
normal density function. It can be seen that the finite-sample distribution of Tn is very consistent
with the standard normal distribution, especially when n, p become larger. When α = 0.05, the
empirical cover rates of the 95%-confidence interval are 94.2%, 93.5% and 95.3% forp= 50, 100
and 200, respectively. All these experiments verify the correctness of our theoretical results.
Normal
Normal
.54.32
Alωu ① Cl
Normal
/Gωus°
Figure 2: The histogram of Tn. The solid line is the density of the standard normal distribution.
Example 2.	This example verifies the results in Theorem 4.5. Here we define two statistics:
σp{Rx,β(β, β)-(1- c1)r2 -
2
σ2 o - μc,3
Cn - 1 ʃ	σc,3
σp {Rχ,β(β, β)-(1- c1)r2 -
2
σ2 o - μc,3
Cn - 1 ʃ	σc,3
According to Theorem 4.5, both Tn,0 and Tn,1 weakly converge to the standard normal distribution
as n,p→ +∞. Compared to Tn,0, Tn,1 provides a better approximation for the finite sample distri-
bution of RX,β(β, β) because it contains smaller order terms in the asymptotic mean and variance.
We take C = 3/2 andp= 150, 300, 450. Similarly the finite-sample distributions of Tn,0 and Tn,1
are presented by the histogram of Tn,0 and Tn,1 with 1000 repetitions. The comparison between
these two statistics is shown in Figure 3. It can also be seen that the finite sample distributions of
Tn,0 and Tn,1 both match the standard normal distribution quite well, especially Tn,1 with more
precise characterization. The empirical cover rates of the 95%-confidence interval (11) are 93.8%,
94.7% and 94.4% for p = 150, 300 and 600 respectively, which further shows the validity of our
theoretical results.
Figure 3: The histograms of Tn,0 and Tn,1 . The solid line is the density of the standard normal
distribution.
9
Under review as a conference paper at ICLR 2021
References
Madhu S Advani and Andrew M Saxe. High-dimensional dynamics of generalization error in neural
networks. arXiv preprint arXiv:1710.03667, 2017.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019.
Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Denny Wu, and Tianzong Zhang. Generalization of two-
layer neural networks: An asymptotic viewpoint. In International Conference on Learning Rep-
resentations, 2019.
Zhidong Bai and Jack W. Silverstein. Clt for linear spectral statistics of large-dimensional sample
covariance matrices. The Annals of Probability, 32(1A):553-605, 2004.
Zhidong Bai and Jianfeng Yao. On the convergence of the spectral empirical process of wigner
matrices. Bernoulli, 11(6):1059-1092, 2005.
Zhidong Bai and Jianfeng Yao. Central limit theorems for eigenvalues in a spiked population model.
Annales de l'IHP ProbabiIites et Statistiques, 44(3):447474, 2008.
Zhidong Bai and YongQua Yin. Limit of the smallest eigenvalue of a large dimensional sample
covariance matrix. In Advances In Statistics, pp. 108-127. World Scientific, 2008.
Zhidong Bai, Baiqi Miao, and Guangming Pan. On asymptotics of eigenvectors of large sample
covariance matrix. The Annals of Probability, 35(4):1532-1572, 2007.
Peter L Bartlett, Philip M Long, Gabor Lugosi, and Alexander Tsigler. Benign ovefitting in linear
regression. Proceedings of the National Academy of Sciences, 2020.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine learning
and the bias-variance trade-off. stat, 1050:28, 2018.
Mikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv
preprint arXiv:1903.07571, 2019.
Lin Chen, Yifei Min, Mingrui Zhang, and Amin Karbasi. More data can expand the generalization
gap between adversarially robust and standard models. arXiv preprint arXiv:2002.04725, 2020.
Yehuda Dar and Richard G Baraniuk. Double double descent: On generalization errors in transfer
learning between linear regression tasks. arXiv preprint arXiv:2006.07002, 2020.
Yehuda Dar, Paul Mayer, Lorenzo Luzi, and Richard G Baraniuk. Subspace fitting meets regression:
The effects of supervision and orthonormality constraints on double descent of generalization
errors. arXiv preprint arXiv:2002.10614, 2020.
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis. A model of double descent for high-
dimensional binary linear classification. arXiv preprint arXiv:1911.05822, 2019.
MichaI Derezinski, Feynman Liang, and Michael W Mahoney. Exact expressions for double descent
and implicit regularization via surrogate random design. arXiv preprint arXiv:1912.04533, 2019.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.
Melikasadat Emami, Mojtaba Sahraee-Ardakan, Parthe Pandit, Sundeep Rangan, and Alyson K
Fletcher. Generalization error of generalized linear models in high dimensions. arXiv preprint
arXiv:2005.00180, 2020.
Mario Geiger, Stefano Spigler, StePhane d,Ascoli, Levent Sagun, Marco Baity-Jesi, Giulio Biroli,
and Matthieu Wyart. Jamming transition as a paradigm to understand the loss landscape of deep
neural networks. Physical Review E, 100(1):012115, 2019.
10
Under review as a conference paper at ICLR 2021
Cedric Gerbelot, Alia Abbara, and Florent Krzakala. Asymptotic errors for teacher-student con-
vex generalized linear models (or: How to prove kabashima’s replica formula). arXiv preprint
arXiv:2006.06581, 2020a.
Cedric Gerbelot, Alia Abbara, and Florent Krzakala. Asymptotic errors for convex penalized linear
regression beyond gaussian matrices. arXiv preprint arXiv:2002.04372, 2020b.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensional ridgeless least squares interpolation. arXiv preprint arXiv:1903.08560, 2019.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in neural information processing systems, pp. 8571—
8580, 2018.
Adel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani. Precise tradeoffs in adversarial training
for linear regression. arXiv preprint arXiv:2002.10477, 2020.
Ganesh Kini and Christos Thrampoulidis. Analytic study of double descent in binary classification:
The impact of loss. arXiv preprint arXiv:2001.11572, 2020.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
under gradient descent. In Advances in neural information processing SyStemS, pp. 8572-8583,
2019.
Daniel LeJeune, Hamid Javadi, and Richard Baraniuk. The implicit regularization of ordinary least
squares ensembles. In International Conference on Artificial Intelligence and Statistics, pp. 3525-
3535, 2020.
Marco Loog, Tom Viering, and Alexander Mey. Minimizers of the empirical risk and risk mono-
tonicity. In Advances in Neural Information Processing Systems, pp. 7478-7487, 2019.
Song Mei and Andrea Montanari. The generalization error of random features regression: Precise
asymptotics and double descent curve. arXiv preprint arXiv:1908.05355, 2019.
Yifei Min, Lin Chen, and Amin Karbasi. The curious case of adversarially robust models: More
data can help, double descend, or hurt generalization. arXiv preprint arXiv:2002.11080, 2020.
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan. The generalization error of max-
margin linear classifiers: High-dimensional asymptotics in the overparametrized regime. arXiv
preprint arXiv:1911.01544, 2019.
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai. Harmless inter-
polation of noisy data in regression. IEEE Journal on Selected Areas in Information Theory,
2020.
Preetum Nakkiran. More data can hurt for linear regression: Sample-wise double descent. arXiv
preprint arXiv:1912.07242, 2019.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learn-
ing Representations, 2019.
G. M. Pan and W. Zhou. Central limit theorem for signal-to-interference ratio of reduced rank
linear receiver. Ann. Appl. Probab., 18(3):1232-1270, 06 2008. doi: 10.1214/07-AAP477. URL
https://doi.org/10.1214/07-AAP477.
Yiwei Shen and Pierre C Bellec. Asymptotic normality and confidence intervals for derivatives of
2-layers neural network in the random features model. In Neural Information Processing Systems,
2020.
Ya Sinai and Alexander Soshnikov. Central limit theorem for traces of large random symmetric
matrices with independent matrix elements. Boletim da Sociedade Brasileira de MatematiCa-
Bulletin/Brazilian Mathematical Society, 29(1):1-24, 1998.
11
Under review as a conference paper at ICLR 2021
S Spigler, M Geiger, S d’Ascoli, L Sagun, G Biroli, andM Wyart. A jamming transition from under-
to over-parametrization affects generalization in deep learning. Journal of Physics A: Mathemat-
ical and Theoretical, 52(47):474001, 2019.
Aad W Van der Vaart. Asymptotic statistics, volume 3. Cambridge university press, 2000.
Denny Wu and Ji Xu. On the optimal weighted l2 regularization in overparametrized linear regres-
sion. arXiv preprint arXiv:2006.05800, 2020.
Yue Xing, Qifan Song, and Guang Cheng. Benefit of interpolation in nearest neighbor algorithms.
arXiv preprint arXiv:1909.11720, 2019.
Ji Xu and Daniel J Hsu. On the number of variables to use in principal component regression. In
Advances in Neural Information Processing Systems, pp. 5094-5103, 2019.
Shurong Zheng. Central limit theorems for linear spectral statistics of large dimensional f-matrices.
Annales de l'IHP Probabilites et StatiStiqueS, 48(2):444U76, 2012.
Shurong Zheng, Zhidong Bai, and Jianfeng Yao. Substitution principle for clt of linear spectral
statistics of high-dimensional sample covariance matrices with applications to hypothesis testing.
The Annals of Statistics, 43(2):546-591, 2015.
A Proof of theorem 4.1 and theorem 4.2
Let X = ZΣ1/2. According to the Bai-Yin theorem (Bai & Yin (2008)), the smallest eigenvalue of
ZTZ/n is almost surely larger than (1 - √c)2∕2 for sufficiently large n. Thus
λmin(1 XTX) ≥ C0λmin(1 ZTZ) ≥	(1 - √C)2,
n	n2
which implies that the matrix XTX/n is almost surely invertible for large n. By Section 3.2, Π = 0,
ʌ
ʌ
ʌ
ʌ
ʌ
Bχ(β, β) = Bχ,β(β, β) = 0 and Vχ(β, β) = Vχ,β(β, β). Thus the CLT of Rχ(β, β) is same to
ʌ
ʌ
that of Rχ,β(β, β). For simplicity, we focus on Rχ(β, β) in the following. Notice that
2
σ
Vχ(^, β) = — Tr(∑-1∑)
n
=匕 Tr (∑T2 (—)-1∑T2∑)
nn
σ2 XX 1	σ2p Γ1
=——=	=--- -dFz(s)
n si n s
i=1
1 FFc(S) = σ2 -c-
S	1-c
pc(x)
where Fz is the spectral measure of ZTZ/n. According to Theorem 1 of Hastie et al. (2019), as
n,p → ∞ such that p/n = cn → c ∈ (0, ∞), Fz(x) weakly converges to the standard Marcenko-
Pastur lawFc(x) and
Vχ(^, β) → σ2c
Here the standard Marcenko-Pastur lawFc(x) has a density function
2∏cx ʌ/(b - x)(x - a),	if a ≤ X ≤ b,
0,	o.w.,
where a =(1 - √C)2, b = (1 + √C)2 andpc(x) has a point mass 1 - C at the origin if c > 1. Hence
RX(β, β) - σ2 ~—n— = —"[-FZz (S) - σ2cn / —dFcn (S)
1	- cn	n s	s n
=σ2cn /s (FFZ (S)- FFcn (S)).
12
Under review as a conference paper at ICLR 2021
According to Theorem 1.1 of Bai & Silverstein (2004),
P(RX(e，β)- σ21⅛) → N (μc,σ2),	(12)
where
μc
1	cm(z)3(1 + m(z))-3	"
Z {1 — Cm(Z)2(1 + m(z))-2}2
(13)
σ2c(ν4 — 3)
2πi
1 1 Cm(Z)3(1 + m(z))-3
Y z 1 — cm(z)2(1 + m(z))-2 Zz
σ4C2	1	1	d	d
2π2 幺I Jcz Z1Z2 (m(zι) — m(z2))2 dzι- 1 dz2— 2	1 2
(14)
σ4c3(ν4 — 3) £ 1	1	1	，，、，，、
4∏^ —幺 £ Z^ (1+ m(z1))2(1+ m(z2))2 dm(Z1)dm(Z2).
Here the contours in (13) and (14) are closed and taken in the positive direction in the complex plane,
enclosing the support of FZ, i.e. [(1 — √c)2, (1 + √c)2]. The Stieltjes transform m(z) satisfies the
equation
Z =」+
m
C
1 + m
To further simplify the integrations in μc and σc, let Z = 1 + √c(rξ + rξ) + C and perform change
of variables, then we have
1	1	ʌ/er
m(Z) = - 1+√rξ, dz = ^(r —忑)dξ,	dm =(1 + √rξ)2 dξ
and when ξ moves along the unit circle ∣ξ∣ = 1 on the complex plane, Z will orbit around the center
point 1 + C along an ellipse which enclosing the support of FZ . Thus
μc
1	cm(z)3(1 + m(z))-3
--------------------------dz
z (1 — cm(z)2(1 + m(z))-2)2
σ2c(ν4 — 3) / 1 cm(z)3(1 + m(z))-3
2πi	/ z 1 — cm(z)2(1 + m(z))-2 Z
σ2c ʃ	1
2πi 兔=1 r(√c + rξ)(1 + √crξ)(ξ — 1 )(ξ + 1)
σ2c(ν4 — 3) ʃ	1
+	2πi	T∣ξ∣=ι rξ2(√c + rξ)(1 + √crξ)
σ2c2	σ2c2 (ν4 — 3)
(C — 1)2 + —T-^ 一.
As for σc2 , note that
2∏i 力I zι(mι — m2)2 dm1
ɪ 次 ______________1_____________________√crι__________dξ1
2∏i ∕∣ξ1∣=ι 1 + √C(r1ξ1 + r⅛) + C (m2 + 1+√⅛1ξ1 )2(1 + √Cr1ξ1 )2 '1
1 £	_______________«以&___________________dξi
2πi %1∣=1 (ξι + √)(r1ξ1√C +1) {(r1ξ1√C +1)m2 + 1}2	1
C
(C — 1) {(c — 1)m2 - 1}2 ,
13
Under review as a conference paper at ICLR 2021
therefore
σ4c2 — 2∏r _	2σ4c2 =	2πi , _	2σ4c2 2πi e	dm —7  	FdmIdm JT Z1Z2(m1 - m2)2 一 一 ψ		C	2 dm2 ∣ξ∖ξ= = Z z2(c - 1){(C - 1)m2 - 1} —— √		√cr2ξ2	dξ = 2c3σ4 Tiξ2∖=1 (C- 1)(1 + √Cr2ξ2)(√C + r2ξ2)3 ξ2 = (C - 1)4 .
Meanwhile,	2πi 力I Z1 (1 + m(zι))2 dm(ZI) 2πi lξ∖= = L √cξ(1 + √Crξ)(√C + rξ)"d = C - 1,
hence
σ4c3(ν4 - 3)	1	1	σ4c3(ν4 - 3)
—-4∏l 幺 2 Z1Z2 (1+ m(zι))2(i+ m(z2))2 "dm ⑶ )=	…
and	2	2c3σ4	σ4 c3(ν4 — 3) σc = (C - 1)4 +	(1 - c)2	.
Let	Tn = - (Rχ(β, β) - σ2	- μ). σc	1 - Cn	p
According to (12), we have
_ , _	_	, Λ	__、	_ ,	_	_	_	、
P(La,c ≤ RX,β(β, β) ≤ Uα,c) = Pl-Za/2 ≤ Tn ≤ Zα∕2) → 1 - α,
where	La,c	=	σ2 1~nC	+ P (μc	-	Za∕2σc), Ua,c	=	σ2 ；			1	(2c	+	Zɑ∕2σc). 1 - Cn	p

B Proof of theorem 4.3
Notice that
Bχ(β,β) = βτ(Ip- Σ +Σ)β
=lim βτ(Ip - (∑ + ZIp)-1Σ)β
z→0+
= lim zβT(∑ + zip)-1β.
z→0+
Since β is a constant vector, we can make use of the results in Theorem 3 in Bai et al. (2007) and
Theorem 1.3 in Pan & Zhou (2008) regarding eigenvectors. Their works investigate the sample co-
variance matrix Ap = Tp1/2XpTXpTp1/2/n, where Tp is an p × p nonnegative definite Hermitian
matrix with a square root Tp1/2 and Xp is an n × p matrix with i.i.d. entries (xij)n×p. Let UpΛpUpT
denote the spectral decomposition of Ap where Λp = diag(λ1, •…，λp) and Up is a unitary ma-
trix consisting of the orthonormal eigenvectors of Ap . Assume that xp is an arbitrary nonrandom
Unit vector and y = (y1,y2,…，yp)T = Upxp, two empirical distribution functions based on
eigenvectors and eigenvalues are defined as
p	1p
FAp(x) = Σ ∣yi∣21(λi ≤ x),	FAp(x) = p = 1(λi ≤ x).
14
Under review as a conference paper at ICLR 2021
Then for a bounded continuous function g(x), we have
p
X ∣yjl2g(λj)-
j=1
1 X g(λj) = ʃ g(x)dFAp (X)- ∕g(x)dF Ap (x).
The results in Bai et al. (2007) and Pan & Zhou (2008) show that
Lemma B.1. (Theorem 3 Bai et al. (2007) and Theorem 1.3 Pan & Zhou (2008)) Suppose that
(1)	xij ’s are i.i.d. satisfying E(xij) = 0, E(|xij |2) = 1 and E(|xij |4) < ∞;
(2)	xp ∈ Cp, kxp k = 1, limn,p→∞ p/n = c ∈ (0, ∞);
(3)	Tp is nonrandom Hermitian non-negative definite with with its spectral norm bounded in p,
with Hp = FTp -→d H a proper distribution function and xpT(Tp - zIp)-1xp → mFH (z),
where mFH (z) denotes the Stieltjes transform of H (t);
(4)	gι, ∙∙∙ ,gk are analytic functions on an open region of the ComPlex plain which contains
the real interval
hliminf 入仃加(Tp)1(o」)(c)(1 - √c)2, limsupλmaχ(Tp)1(o,1)(c)(1 + √≡)2];
(5)	as n, p → ∞,
sup √n∣∣xT (mFCn,Hp (Z)Tp - Ip)Txp
1+ tmFcn,Hp (Z) dHn(t)|l
→ 0.
—
Define Gp (x) = √n(FAp (x) — F Ap(X)) ,then the random vectors
(/gι(x)dGp(x),…，/gk(x)dGp(x))
forms a tight SeqUenCe and converges weakly to a Gaussian vector Xgi, ∙∙∙ , Xgk with mean zero and
covariance function
Cov(Xg1,xg2) = -2∏2 JJC2 gl(Zl)g2(Z2)c2zιz2z2m--Iz(m2)-mι)dzldz2.
The contours Ci, C? are disjoint, both contained in the analytic regionfor thefunctions (gι,…，gk)
and enclose the support ofFcn,Hp for all large p.
(6)	IfH(X) satisfies
dH(t)	_ d	dH(t)	d	dH(t)
(1 + tm(zι))(1 + tm(z2))	1 1 + tm(zi) J 1 + tm(z2)
then the covariance function can be further simplified to
CoV(Xg1, Xg?) = C(/ g1(x)g2(x)dFc,H(x) - ∕gι(x)dFJH(χ) l^ g?(x)dFJH(χ)).
Recall that Bχ(β, β) = limz→0+ zβτ(Σ + ZIP)Tβ. Let g(x) = 1/(x + z) and xp = β∕r. Then
we have
/ g(x)dGn(x) = √n( r2 βτ(∑ + ZIP)Te - / g(x)dFcn (x)),
where Fcn (X) is the standard Marcenko-Pastur law. It is not difficult to check that under Assump-
tions (A1), (B1) and (C1), all the conditions (1)-(6) in Lemma B.1 are satisfied.
To proceed further, denote a = (1 - √c)2, b = (1 + √c)2. If C is replaced by cn, a and b are denoted
by an and bn respectively. By some algebraic calculations, we have
g(X)dFcn (X)
(1----) ∙---+ /  ：  ∙ Z-------p(bn - X)(X - an)dx
cn	Z	an X + Z 2πcn X
1	-1 + Cn + Z -，cn + 2cn (Z - 1) + (1 + Z)2
Z	2cn Z
15
Under review as a conference paper at ICLR 2021
and
Var(xg)
{g(x)}2dFc(x) - { / g(x)dFc(x)}]
2 ((1-1)二 +
c c	z2
-2 ((1 - 1) ∙ 1 +
c cz
b1
(x + z)2 2πcx
x + z 2πcx
Therefore,
lim Z / g(x)dFcn (x) = 1 - ɪ
z→0+	cn
Furthermore, as n, p → ∞, p/n = cn → c > 1,
1
√n(Bx(β, β) - (1---
cn
This can be rewritten as
√p(Bχ(β, β)-(1-
_ _ , ʌ
1
cn
p(b — x)(x — a) dx)
and lim z2Var(xg) = 2(C - 0
z→0+	c3
→ N(0, 2(CC-1) r4
→ N(0, 2(CC-1) r4
Next we deal with the variance term VX (β, β). According to the Assumption (B1), the variance
term is
σ2
2n
i=1
1
Si
a
b
1
1
a
ʌ
ʌ
1
—
where si, i = 1, . . . , n are the nonzero eigenvalues of XTX/n. Let {ti, i = 1, . . . n} denote the
non-zero eigenvalues of XXT /p, then we have
2n
σ1
Vx(β, β) = — X —
p i=1 ti
t dFxxT/p(t) → ^--
By interchanging the role of p and n, from the result in Theorem 4.1, as n,p → ∞, p/n = cn →
c > 1, we have,
n
X 1 - A → N
i=1t 1 - cn
ι c0(ν4 — 3)	2c0	ι c0(ν4 — 3)
ψ +	1 - C0 , (c0 - 1)4 + (1 - C0)2
where c0n = n/p = 1/cn, c0 = 1/c. This result can be rewritten as
X 1 - -ɪ → N ( C +(
匕 t	Cn - 1 → N 1(1 -C)2 +
Hence the CLT of VX(β, β) is given by
22
p(Vχ(β,β)- J) → N(二 +
cn - 1	(1 - c)2
一 ~ /i , i	一，△一、∖ 一
(ν4 - 3)	2c3	c(ν4 - 3)
C - 1 , (1 - c)4 + (C - 1)2
σ2(ν4 — 3)	2c3σ4	cσ4(ν4 — 3)
—C-I —, (1 - c)4 +	(C - 1)2
-‰ τ	. 1 . ∕^r I I > ∕cc∖ TT^∕zic∖∖	r-∖ *	t •	. . 1	∙	.	t . 1 1 • • , •
Notice that Cov BX (β, β), VX(β, β) = 0. According to the consistency rate and the limiting
ʌ
ʌ
ʌ
distribution of Bx(β, β) and Vx(β, β), we know that the bias Bx(β, β) is the leading term of
一 ,△一、 一 .. 一. 一
Rχ(β, β). This implies that
√p{Rχ(β,β) - (1 - g)kβk2 - c-σ--ɪO → N(0,σ2,ι),
where σc2,1 = 2(C - 1)r4/C2. A practical version of this CLT is given by
Rx(β, β) - (1---)kβk2 -
cn
2
Cn - 1 O → N(μc,1,σ2j),
where
μc,1
2
σc,1
1 n	cσ2	+ σ2(ν4 - 3) o
√p I (1 - c)2 +一C-I — r
2(c — 1) 4	1 n 2c3σ4 cσ4(ν4 — 3) o
c2 r +pj (1 — c)4 + (c — 1)2 J .
16
Under review as a conference paper at ICLR 2021
C Proof of theorem 4.4
τ-,∙	∙ 1	.1	1 ∙	.	I >	/ A Zl∖ 1 ʌ A	. ∙	Z * ■« ∖ /ɪʌ Λ ∖	1 ∕z-xr∖∖
First we consider the bias term Bχ(β, β). By Assumption (A1), (B1), and (C2),
,ʌ m . m
Bχ (β, β) = E[βτΠΣ∏β∣X] = E[βτ∏β∣X]
=Tr {(Ip — Σ +Σ)E(ββτ∣X)}
r2
= 一Tr{Ip — ∑+∑} = r2(1 — n/p).
p
Alternatively, we can rewrite the bias as
,ʌ .
Bχ(β, β)
lim E[βT(Ip — (∑ + zIp)-1∑)β∣X]
z→0+
lim E[zβτ(∑ + ZIp)Tβ∣X]
z→0+
r2	1
lim z- Tr(Σ + ZIp).
z→0+ p
Define that fn(z) = Zr2 Tr(∑ + ZIp)-1. Notice that ∣fn(z)∣ and |fn(z)| are bounded above. By the
Arzela-Ascoli theorem, we deduce that fn(z) converges uniformly to its limit. Under Assumption
(C2), by the Moore-Osgood theorem, almost surely,
r2	1
lim Bχ(β, β) = lim lim Z一 Tr(Σ + ZIp)
n,p→∞	z→0+ n,p→∞ p
= lim lim zr2 Tr f1XXT + ZInI	,
z→0+ n,p→∞ p n
In fact,
lim Bχ(β, β) = r2 lim lim zmn(—z),
n,p→∞	z→0+ n,p→∞
where mn(z) is the Stieltjes transform of empirical spectral distribution of Σ = XTX/n. Accord-
ing to Theorem 2.1 in Zheng et al. (2015) and Lemma 1.1 in Bai & Silverstein (2004), the truncated
version of p(mn(z) — m(z)) converges weakly to a two-dimensional Gaussian process M(∙) satis-
fying
E[M(z)]
cm3(1 + m)	c(ν4 — 3)m3
{(1 + m)2 — cm2}2 + (1 + m) {(1 + m)2 — cm2} ,
and
m0(zι )m0(z2)
1
2
Cov M(z1),M(z2)
—
(m(zi) — m(z2))2	(Z1 — Z2)2
}
c(ν4 — 3)m0(zι )m0(z2)
+ (1 + m(z1))2(i + m(z2))2,
where m = m(z) represents the Stieltjes transform of limiting spectral distribution of companion
matrix XXT /n satisfying the equation
1c
Z =- m + ι+m，m(z)
1 - c
------+ Cm(z).
z
—
When p > n,we can actually solve m(z) equation and obtain that
m(z)
m(z)
—1 + c — z + ——4z +(1 — c + z)2
2z
1 — c — Z + P—4z + (1 — C + z)2
.
2Cz
17
Under review as a conference paper at ICLR 2021
Therefore, by some algebraic calculations, we have
zm(-z) + z(1 - ɪ)ɪ
cZ
lim Bχ(β, β) = lim r2 lim zmn(-z) = r2 lim
n,p→∞	n,p→∞	z→0+	z→0+
= lim r2 lim Znmn(Z)= r2- lim zm(-z)
n,p→∞	z→0+ P -	C z→0+ 一
=r2(I -1).
c
Moreover,
Var M(Z)
lim Cov M(Z1), M(Z2)
z1→z2=z
2m0(z)m000 (z) — 3(m00(z))2	c(ν4 — 3)(m0(z))2
6(m0(z))2	+	(1 + m(z))4
By substituting of the explicit form of m(z), We can easily derive that
lim ZE[M (—Z)] = 0, lim Z2Var(M(—Z)) = 0,
z→0+	z→0+
Which means that the second-order limit of Bχ(β, β) is still r2 (1 — 1/c). All in all, Bχ(β, β) is
identical With a constant r2(1 — 1/c) in distribution.
On the other hand, by Assumption (B1),
σ2
1
2n1
n
i=1 si
Where si, i = 1, . . . , nare the nonzero eigenvalues of XT X/n. Similar to the proof of Theorem 4.3,
the CLT of VX(β, β) is given by
PWXB β)—二)→ N (πc⅛ + σ⅛)
cn — 1	(1 — c)2 c — 1
2c3σ4	cσ4(ν4 — 3))
(1 — c)4 +	(c — 1)2 J'
Combining the results of BX(β, β) and VX(β, β), We have
pn Rχ(∣β, β)—r2 (I—) —7o →N (μc,2, σ2,2),
cn	cn — 1
Where
cσ2 σ2(ν4 —	3)	2	2c3σ4 cσ4(ν4 — 3)
μc,2 =	(1 — c)2	+ c — 1	, σc,2	= (1 — c)4	+	(C — 1)2
D Proof of theorem 4.5
Note that under Assumption (B1) and (C2), BXe (β, β) = βτ∏β = β (Ip — Σ +Σ)β. If we
directly consider βrT (Ip — Σ +Σ)β, we can make use of the asymptotic results for quadratic forms
Theorem 7.2 in Bai & Yao (2008) stated as follows.
Lemma D.1. (Theorem 7.2 in Bai & Yao (2008)) Let {An = [aij (n)]} be a sequence of n × n
real symmetric matrices, {xi}i∈N be a sequence of i.i.d. K dimensional real random vectors, with
E(xi) = 0, E(xixiT) = (γij)K ×K and E[kxik4] < ∞. Denote
Xi = (X'i)K×1,	X(I) = (X'1,…，x'n)T, ' = 1,…，K, i = 1,…，n,
assume the following limits exist
1n	1
ω = lim 一 a22~( α2i(n),	θ = lim	Tr An.
n→∞ n	n→∞ n
i=1
Then the K -dimensional random vectors
Zn = (zn,')κ×1,	zn,' = √n (X(')TAnX(') — Y'' τr{An}),	1 ≤ ' ≤ K,
converge weakly to a zero-mean Gaussian vector with covariance matrix D = D1 + D2 where
[Dι]''0 = ω {E(x2ιX2oι) — Y''Y'0'0} , [D2]''0 = (θ — ω)(γ''0γ'0' + γ2'0), 1 ≤ ',' ≤ K.
18
Under review as a conference paper at ICLR 2021
i	1∙	.	.1	1 .	∙ T	1 ʌ <	1	,	4	I I	T	<∖-l-	.1	1
According to the results in Lemma D.1, let An = Π = Ip - Σ+Σ, then we have, as P → ∞,
√pnβτΠβ - r2 Tr(∏)} → N(0, d2 = d2 + d22),
where
1p
ω = lim - T∏2i,
p→∞ p	ii
i=1
θ = lim 1Tr(Π2) = 1 - 1,
p→∞ p
c
and
2
d2	=ω {E(X21x2I)-Y2e}=ω( r EGeI)- nr4,
d2	= (θ - ω)(γ2e + γ'e) = 2(θ - ω)r4.
Since in the proof of Theorem 4.4, We have already shoWn that
r2 Tr(Π) = r2(1 - n).
pp
2
In particular, if β follows multivariate Gaussian distribution, i.e. β 〜 Np(0, rpIp), then as P → ∞,
ʌ
Moreover, VX,β(jβ, β) = Vχ(β, β), We have already proved in Theorem 4.4 that
σ2
P(VX,β(β, β)---------7
cn - 1
一 ~ , — , ʌ
d n( cσ2	σ2(ν4 一 3)	2c3σ4	cσ4(ν4 一 3)
) →	((1 - c)2 + c - 1	, (1 - c)4 +	(C - 1)2
一 , 一 , " _ . _______ , " _ ,,
Note that CoV(BX,β(β, β), Vχ,β(β, β))
一一 ,ʌ 一 一 一 一. —
0. According to the consistency rate of BX,β(β, β)
ʌ . 一. 一 — , ʌ 一 . . 一.
and VX,β(β, β), we know that the bias Bχ(β, β) is the leading term of Rχ,β(β, β). This implies
that
√pnRX,β(β, β) - r2(1	)	70 → N(0,σ2,3),
Cn	Cn - 1
where σc2,3 = 2r4(1 - 1/C). A practical version of this CLT is given by
√pnRX,β(β, β) - r2(1-----)--------70 → N(μc,3, σ2,3),
Cn	Cn - 1
ʌ
where
μc,3
cσ2	σ2
(1f + 一
(ν4 - 3)
c-1
市2	2(1	1)r4+ 1 n 2c3σ4
σc,3 = 2(I- C)r + Pi (Γ-∑f
,
cσ4(ν4 — 3) o
(C - 1)2 Γ
+
E More experiments
E.1 More results of Example 1
This example checks Theorem 4.2. We define a statistic
Tn
σpc (Rχ(β, β)-
2	Cn
σ -------
1 - Cn
)-也
σc
According to Theorem 4.2, Tn weakly converges to the standard normal distribution as n,P → ∞. In
this example, C = 1/2 andP = 50, 100, 200. To make sure the assumption (A) holds, the generative
distribution Px is taken to be the standard normal distribution, the centered gamma with shape 4.0
and scale 0.5, and the normalized Student-t distribution with 6.0 degree of freedom. The finite-
sample distribution of Tn is estimated by the histogram of Tn under 1000 repetitions. The results
are presented in Figure 4. One can find that the finite-sample distribution ofTn tends to the standard
normal distribution as n,P → +∞. When α = 0.05, the empirical cover rates of the 95%-confidence
interval are reported in Figure 5.
19
Under review as a conference paper at ICLR 2021
Figure 4: The histogram of Tn . The solid line is the density of the
Normal
t-distribution
standard normal distribution.
Normal	0.97. Gamma
Figure 5: The cover rate of the confidence interval (7) as p creases.
The confidence level is 95%.
20
Under review as a conference paper at ICLR 2021
E.2 More results of Example 2
The Example 2 checks Theorem 4.5. Here we consider the standardized statistics:
Tn,0	=
Tn,1	=
pnR nRx(aβ) - (1---)r2—
σc,3	cn
2
σ o - μ23
cn - 1	σc,3 ,
2
σ2 o - μc,3
Cn - 1 J	σc,3
RXe ⑶ -(I-Cn )r2
—
According to the central limit theorem (10) and its practical version, both Tn,0 and Tn,1 weakly
converge to the standard normal distribution as n, p→ +∞. We take C = 2 and p= 100, 200, 400.
The finite-sample distributions of Tn,0 and Tn,1 are estimated by the histogram of Tn,0 and Tn,1
under 1000 repetitions. The results are presented in Figure 6 and Figure 7. When α = 0.05, the
empirical cover rates of the 95%-confidence interval (11) are reported in Figure 8.
Figure 6: The histogram of Tn,0 . The solid line is the density of the standard normal distribution.
21
Under review as a conference paper at ICLR 2021
Al-SU ① Cl Al-SU ① Cl Al-SU ① Cl
Normal
Al-SU ① Cl
Figure 7: The histogram of Tn,1. The solid line is the density of the
Al-SU ① Cl
Al-SU ① Cl
standard normal distribution.
Figure 8: The cover rate of the confidence interval (11) as p creases.
t-distribution
The confidence level is 95%.
22
Under review as a conference paper at ICLR 2021
E.3 Example 3
This example checks Theorem 4.3. To proceed further, we denote two statistics:
Tn,2	=
Tn,3	=
pnR nRx(aβ) - (1---)r2—
σc,1	cn
2
σ o - μ,ι
cn - 1	σc,1 ,
2
σ2 o - μc,ι
Cn - 1 J σc,1
RXe ⑶ -(I-Cn )r2
—
According to the central limit theorem (8) and its practical version, both Tn,2 and Tn,3 weakly
converge to the standard normal distribution as n, p→ +∞. We take C = 2 and p= 100, 200, 400.
The finite-sample distributions of Tn,2 and Tn,3 are estimated by the histogram of Tn,2 and Tn,3
under 1000 repetitions. The results are presented at Figure 9 and Figure 10. One can see that the
finite-sample distributions of Tn,2 and Tn,3 are close to the standard normal distribution, and the
finite-sample performance of Tn,3 is better than that of Tn,2 . When α = 0.05, the empirical cover
rates of the 95%-confidence interval (9) are reported in Figure 11.
Figure 9: The histogram of Tn,2. The solid line is the density of the standard normal distribution.
23
Under review as a conference paper at ICLR 2021
Normal
Al-Suoa AI-SUəɑɪ Al-Suoa
Al-SUəɑɪ
Al-SUəɑɪ
Normal
0.4
Al-SUəɑɪ
Tn.3
Gamma
t-distribution
Figure 10: The histogram of Tn,3. The solid line is the density ofth
e standard normal distribution.
Normal	0.97. Gamma
Figure 11: The coverage of confidence interval (9) as p increases.
The confidence level is 95%.
24
Under review as a conference paper at ICLR 2021
F An example from Figure 1
Figure 12: According to the first-order limit, given a fixed dimension p = 100, the prediction risks at
sample size n = 90 and n = 98 are about 10.20 and 49.02. More data hurt seems true. However, the
95% confidence interval of the prediction risks with sample size 98 is [4.91, 142.12], which contains
the risk for n = 90. Then more data hurt is not statistically significant.
G	An anis otropic example for Remark 4.2
In the over-parameterized case, the bias term BX(β, β) = βTΠΣΠβ is non-zero while the variance
term VX(β, β) remains the same as under-parameterized case. Therefore in this section, we conduct
a small simulation to examine the fluctuation of the bias BX for both isotropic and anisotropic Σ
in the over-parameterized case with non-random β satisfying Assumption (C1). In particular, in the
following we set r = 1.
We consider both localized and delocalized β such that
1.	Localized case: βι = (1,0,…，0);
2.	Delocalized case: β2 = √1p (1,…,1);
and both the isotropic and anisotropic Σ
3.	Identity case: Σ1 = Ip ;
4.	Compound symmetric case: Σ2 = 0.5Ip + 0.51p1pT.
Then we fix p/n = 2 and let p vary from 10 to 300, we present in Figure 13 the empirical variance
of √p * BX and p * BX under various combinations of Σ and β with 1000 replications.
From the plot on the top left panel in Figure 13, we can see that the variance of √p * BX for both
βι and β2 remain constant as P grows, which indicates that the convergence rate of BX is 1/√p
under the isotropic case regardless of localized or delocalized β. As for the anisotropic case on the
top right corner, the variance of √p* BX stabilizes for βι, while decays for β2, which indicates that
convergence rate of BX under (Σ2, β2) and (Σ2, β1) are different.
This simulation result further confirms our conjecture that in the over-parameterized case, there is
no universal CLT for the prediction risk RX(β, β) under the anisotropic setting for non-random β.
25
Under review as a conference paper at ICLR 2021
Figure 13: The upper panel is the empirical variance of √p * Bχ, the lower panel is for P * Bχ.
26