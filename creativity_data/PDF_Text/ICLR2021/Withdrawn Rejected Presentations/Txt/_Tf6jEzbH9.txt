Under review as a conference paper at ICLR 2021
Context-Agnostic Learning
Using Synthetic Data
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel setting for learning, where the input domain is the image of
a map defined on the product of two sets, one of which completely determines
the labels. Given the ability to sample from each set independently, we present
an algorithm that learns a classifier over the input domain more efficiently than
sampling from the input domain directly. We apply this setting to visual classi-
fication tasks, where our approach enables us to train classifiers on datasets that
consist entirely of a single example of each class. On several standard benchmarks
for real-world image classification, our approach achieves performance compet-
itive with state-of-the-art results from the few-shot learning and domain transfer
literature, while using significantly less data.
1	Introduction
Despite recent advances in deep learning, one central challenge is the large amount of labelled train-
ing data required to achieve state-of-the-art performance. Procuring such volumes of high quality,
reliably annotated data can be costly or even close to impossible (e.g., obtaining data to train an
autonomous navigation system for a lunar probe). Additional hurdles include hidden biases in large
datasets (Tommasi et al., 2017) and maliciously perturbed training data (Biggio et al., 2012).
Synthetically generated data has seen growing adoption in response to these problems, since the
marginal cost of producing new training data is generally very low, and one has full control over
the generation process. This is particularly true for applications with a physical component, such as
autonomous navigation (Gaidon et al., 2016) or robotics (Todorov et al., 2012). However, training
with purely synthetic data suffers from the so-called “reality gap”, whereby good performance on
synthetic data does not necessarily yield good performance in the real world (Jakobi et al., 1995).
In particular, the difficulty of generating realistic training images scales not just with the objects of
interest, but also the real-world contexts in which the learned model is expected to operate.
This work begins with the simple observation that, for many classification tasks, the label ofan input
is determined entirely by the object; however, this additional structure is discarded by current syn-
thetic data pipelines. Our goal is to leverage this decomposition to develop more efficient methods
for the related problems of generating training data and learning from a synthetic domain.
Our contributions are two-fold: first, we formally introduce the setting of context-agnostic learning,
where the input space is decomposed into object and context spaces, and the labels are independent
of contexts when conditioned on the objects. Second, we propose an algorithm to efficiently train
a classifier in the context-agnostic setting, which relies on the ability to sample from the object and
context spaces independently. We apply our methods to train deep neural networks for real-world
image classification using only a single synthetic example of each class, obtaining performance com-
parable to existing methods for domain adaptation and few-shot learning while using substantially
less data. Our results show that it is possible to train classifiers in the absence of any contextual
training data that nonetheless generalize to real world domains.
2	Related work
Domain shift refers to the problem that occurs when the training set (source domain) and test set
(target domain) are drawn from different distributions. In this setting, a classifier which performs
1
Under review as a conference paper at ICLR 2021
well on the source domain may not generalize well in the target domain. A standard method for
addressing this challenge is domain adaptation, which leverages a small amount of data from the
target domain to adapt a function that is learned over the source domain (Blitzer et al., 2006).
In the context of learning from synthetic data, the domain shift that occurs between synthetic and real
world data is known as the reality gap (Jakobi et al., 1995). State-of-the-art rendering engines, such
as those used for video games, can help narrow this gap by generating photorealistic data for training
(Dosovitskiy et al., 2017; Johnson-Roberson et al., 2016; Qiu and Yuille, 2016). Another technique
is using domain randomization to generate the source domain with more variability than is expected
in the target domain (e.g., extreme lighting conditions and camera angles), so as to make real images
appear as just another variant (Tobin et al., 2017; Tremblay et al., 2018); in particular, Torres et al.
(2019) apply domain randomization to traffic sign detection and find that arbitrary natural images
suffice for the task. Another body of work exploits generative adversarial networks (Goodfellow
et al., 2014a) to generate synthetic domains (Hoffman et al., 2017; Liu et al., 2017; Shrivastava
et al., 2016; Taigman et al., 2016; Tzeng et al., 2017). Finally, several works have explored using
synthetic data for natural image text recognition (Gupta et al., 2016; Jaderberg et al., 2014). These
works use an approach that is roughly analogous to our baseline models, and test their techniques
on the target domain of street signs rather than handwritten characters (as we do).
A different paradigm for the low-data regime is few-shot learning. In contrast to domain adapta-
tion, few-shot learning operates under the assumption that the target and source distributions are the
same, but the ability to sample certain classes is limited in the source domain. Early approaches
emphasized capturing knowledge in a Bayesian framework (Fe-Fei et al., 2003), which was later
formulated as Bayesian program learning (Lake et al., 2015). Another approach based on metric
learning is to find a nonlinear embedding for objects where closeness in the geometry of the embed-
ding generalizes to unseen classes (Koch, 2015; Snell et al., 2017; Sung et al., 2018; Vinyals et al.,
2016). Meta-learning approaches aim to extract higher level concepts which can be applied to learn
new classes from a few examples (Finn et al., 2017; Munkhdalai and Yu, 2017; Nichol et al., 2018;
Ravi and Larochelle, 2016). A conceptually-related method that leverages synthetic training data is
learning how to generate new data from a few examples of unseen classes; in contrast to our work,
however, these methods still require a large number of samples to learn the synthesizer (Schwartz
et al., 2018; Zhang et al., 2019). Finally, some works combine domain adaptation with few-shot
learning to learn under domain shift and limited samples (Motiian et al. (2017)).
The main characteristic that differentiates our work from these approaches is that we are interested in
learning classifiers that are context-agnostic, i.e., do not rely on background signals. As such, while
we find our approach is applicable to many of the same tasks as the aforementioned works, our
theoretical setting and objectives differ significantly. From a practical perspective, we demonstrate
our techniques when the entire training set consists solely of a single synthetic image of each class,
though our techniques can certainly be applied when more data is available; however we do not
expect the reverse to hold for domain adaptation or few-shot learning in our setting. Indeed, we
consider this work to be complementary in that we are concerned with exploiting the additional
structure that is inherent in certain source domains, while the goal of domain adaptation and few-
shot learning is to achieve good performance under various downstream domain shift assumptions.
3	Setting
The standard supervised learning setting consists of an input space X, an output space Y, and a
hypothesis space H of functions mapping X to Y . A domain PD is a probability distribution over
(X, Y). Given a target domain PT and a loss function `, the goal is to learn a classifier h ∈ H that
minimizes the risk, i.e., the expected loss RPT (h) := EPT ['(h(x), y)]. The training procedure Con-
sists of n samples (x1, y1), ..., (xn, yn) from a source domain PS. A standard approach is empirical
risk minimization, which takes the classifier that minimizes Remp(h) = 1 Pi '(h(xi),yi); if PS is
close to PT, then with enough samples, such a classifier also achieves low risk in the target domain.
3.1	Context-agnostic learning
In general, we can frame the goal of classification as learning to extract reliable signals for the label y
from points x ∈ X. This task is often complicated by the presence of noise or other spurious signals.
2
Under review as a conference paper at ICLR 2021
However, for input spaces generated by physical processes, such signals are generally produced by
distinct physical entities and can thus be thought of as independent signals that become mixed via
the observation process. We aim to capture this additional structure in our setting.
Concretely, we have an object space O, a context space C, and an observation function γ on O × C.
The input space X is defined as the image of γ : O × C → X. We will assume that points in O are
associated with a unique label in Y, and require that γ preserves this property when passing to X .
Note that this setting can be easily generalized to a case when the image of γ is a subdomain of X .
In this work, we will consider the special case when X ⊆ C. Conceptually, the context space is an
“ambient space” containing not only valid inputs, but also random noise or irrelevant classes; the
input space is a subset of the context space for which there exists a well-defined label. For example,
in our experiments we explore such a decomposition for the task of traffic sign recognition, where
the object space O consists of traffic signs viewed from different angles, the context space C is
unconstrained pixel space, and the input space X is the set of images that contain a traffic sign.
Recall that the standard objective of learning is to find a good classifier for an unknown subdomain
XPT ⊆ X . We consider instead the task of learning a classifier on the entire input space X. To
sample from X we are given oracle access to the observation function and draw (labelled) samples
from O and C independently. Clearly, if this problem is realizable, i.e., there exists h ∈ H for
which RX(h*) = 0, then We do not even need to know the target domain PT, since
XPT ⊆ X =⇒ [Rχ(h*) =0 v RPT(h*) = 0]
Assuming access to X through γ, we can learn h simply by taking the number of samples to infinity.
Unfortunately, learning a classifier on X generally requires many more samples than learning a
classifier on Xp『.Thus we aim to learn h using as few samples as possible.
Our new goal will be to learn a classifier over X which depends only on signals from O; more
precisely, we have the following definitions:
Definition 3.1. A function f on X is context-agnostic if
Pr[f ◦ γ(o, c) = x] = Pr[f ◦ γ(o, c0) = x]	∀c, c0 ∈ C, o ∈ O, x ∈ Im(f)
Definition 3.2. Given a context-agnostic labelfunction y*, the objective of context-agnostic learn-
ing is to find h ∈ H such that h achieves the lowest risk of all context-agnostic classifiers.
The hope is that, since y* is context-agnostic, we can learn y* through the lower dimensional struc-
ture of O using fewer samples. Note, however, that while we only need max(|O|, |C|) samples to
observe every object and context once, we need |O| * |C| samples to observe every object in every
context. Hence the main challenge when the number of samples is low will be avoiding spurious
signals, i.e., statistical correlations between context and objects (and by extension, labels) which are
artifacts of the sampling process and do not generalize outside the training set.
We conclude with some high-level remarks about this setting. First, note that if the problem is
realizeable, then the lowest risk classifier is also context-agnostic. Second, we recover the standard
supervised setting for the trivial context space C = 0. Conversely, classification remains well-
defined even in the trivial object space O = {yi }, the set of classes; however, this pushes all the
complexity to the observation function γ, which may be hard to define or intractable to compute.
Finally, we do not preclude the existence of useful signals originating from the context for certain
domains. For instance, a great deal of information can often be gleaned from the backgrounds of
photos, e.g., stop signs are more often found in cities than on highways. Our theoretical setting
avoids this issue by assuming realizability and uniqueness of labels; more practically, we argue that
a “good” classifier should nonetheless recognize stop signs on the highway, and our experimental
results provide evidence that over-reliance on such background signals leads to brittle classifiers.
3.2	Efficient sampling for object-context decomposed input spaces
In this section, we present an algorithm for context-agnostic learning. We first develop a formal
notion of contextual bias for this setting. We assume a binary classifier h and slightly abuse notation,
writing h for h ◦ γ, i.e., h : O × C → {-1, 1}. For an object o, denote the correct label o*, the
expected classification o := Ec〜C [h(o, c)], and the object error o := |o* 一 同.
3
Under review as a conference paper at ICLR 2021
Definition 3.3. The context bias B(h, c) of a classifier h on the context c is defined as
sgn(B(h, c)):= sgn(Eo〜o[h(o,c)-司)
||B(h, c)|| ：= Eo〜。['(h(o, c),θ)]
where ' is the hinge loss '(i, j) ：= max(0,1 — i * j).
Intuitively, sign of the bias corresponds to the label toward which the classifier is biased by a given
context; the magnitude measures the strength of this bias. Clearly, the classifier is context-agnostic
exactly when the bias is zero. We are now ready to state our main theoretical result, which gives an
upper bound on the risk in terms of the context bias on C and object error over O.
Theorem 3.1. Let h be a classifier with average bias K and object error for all objects bounded
from above by α < 1. Then the risk is bounded from above by K/(2 - α). Furthermore, equality
holds if and only if all object errors equal α.
We give a proof in Appendix A. The assumption α < 1 is fairly weak, being equivalent to the
classifier performing better than random guessing. Note that the error bound α and bias bound K
are not independent; in particular, α = 0 if and only if K = 0 and α < 1. Observe also that when
C = 0, K = 0 holds trivially, but α < 1 for all objects means the classifier is correct on all inputs.
The central idea behind Theorem 3.1 is leveraging the fact that labels depend only on objects to
factor the risk into separate terms for object error and context bias. This factorization enables us to
exploit our ability to sample independently from the object and context spaces. More specifically, we
can use samples from O to minimize the object error, and samples from C to minimize the context
bias. Since we only need α < 1, we continue to draw objects randomly; however given an object
o, we aim to observe it with the context for which the classifier has the strongest opposing bias.
Intuitively, this allows the classifier to “correct” its bias and unlearn the spurious signals, thereby
minimizing the bias and also the risk.
Adopting this approach without modification requires computing the bias of every context in C . In
most cases, however, even estimating a single bias may be prohibitively expensive. Thus, rather than
solve for the maximum bias explicitly, we instead propose a heuristic for identifying contexts with
large biases. Note that since X ⊆ C, a reasonable assumption is that the classifier learns a strong
bias on recent training inputs when taken as contexts. This suggests a simple greedy approach for
correcting biases by repurposing recent training inputs as contexts; we call this algorithm Greedy
Bias Correction and present a description in Algorithm 1.
4	Learning visual tasks using context-agnostic synthetic data
We introduce an instantiation of Greedy Bias Correction for learning visual tasks using synthetic
data. We are given a function which takes a label y and outputs a rendering of the corresponding class
in a random pose without any background. The context is the background of the image, on which
we place no restrictions. The observation function γ superimposes an object over a background.
Local refinement via robustness training We note that our observation function γ is fairly re-
strictive; for instance, we do not support occlusions. Because our ultimate goal will be to perform
on data taken from a real-world context, we aim to capture this discrepancy using robustness train-
ing.1 In particular, we assume that the image of γ is an -covering of X, where a set A is said
to be an -covering of another set B iff for all points b ∈ B, there exists a point a ∈ A such
that ||a - b|| ≤ . Then for a given sample, we will instead add the point in the -neighborhood
of x which maximizes the training loss, i.e., for a classifier h and a sample x = γ(o, c), we use
x0 = argmaXχo∈N(χ) '(h(x0), y). This formulation is often used to train models which are robust
against local perturbations. An empirically effective method for finding approximations to x0 is
known as Projected Gradient Descent (PGD) (Goodfellow et al., 2014b; Madry et al., 2017). The
1Robustness training is more commonly referred to as adversarial training in the adversarial robustness com-
munity whence we borrow this technique. We use the nonstandard term to avoid confusion with the unrelated
(generative) adversarial methods found in the few-shot learning literature.
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Greedy Bias Correction
Input: Object space O, context space C, observation function γ, number of rounds R,
resample probability p, classifier update subroutine Fit, binary classifier h
Output: Trained classifier h
// initialize random context and label
C〜C;
y 〜{-1,1};
for r — 1 to R do
o 〜O(y); //sample object
X J γ(o, c); //observe object and context
h J Fit(h, x,y); //perform classifier update
// update context and label
p0 J Uniform(0, 1);
if p0 < p then
// resample random context and label
c〜C;
y 〜{-1,1};
else
I c J x; //previous image becomes new context
I y J----y; //flip label
end
end
(1)
(2a)
(5)
(4)
(2b)
(3)
Figure 1: A graphical representation of the generative loop in Algorithm 2 using real training data.
(1) Sample from object space. (2) Observe object and context. (3) Perform local refinement. (4)
Add to training set. (5) Previous image becomes next context (resample from C with probability p).
algorithm can be summarized as
x0 J x + δ
Xi J ∏x+e(Xi-1 + η ∙ sgn(Vχ'(h(xi-i),y))),	i = 1,…,n
where δ is a small amount of random noise, Π is the projection back onto to the -ball, η is the step
size, and n is the number of iterations. As is standard for robustness training, we use the '∞ norm
defined as || (x 1,…,Xn) || ∞ = maxi Xi. Our choice of e will depend on the task at hand, and We also
use different for the portions of the image corresponding to the object and context.
Additionally, since we are no longer in a binary context, we sample a random permutation on labels
instead of flipping the label deterministically. The full algorithm is presented as Algorithm 2 in
Appendix B; Figure 1 provides a visualization of the key generative process, with images taken
from a real step of training a deep neural network to perform classification of traffic signs.
From a practical standpoint, this algorithm makes concrete several benefits of our approach. First,
rendering object classes, i.e. sampling from O, is often relatively easy. In the case of two-
dimensional rigid body objects, this can be captured using standard data augmentation such as
rotations, flips, and perspective distortions. Indeed, in this setting, our work can be viewed as a
form of minimal one-shot learning, where the training data consists solely of a single unobstructed
straight-on shot for each object class. Second, there is no requirement to perform realistic rendering
of contexts C , avoiding an additional layer of complexity.
5
Under review as a conference paper at ICLR 2021
Approach	Picto → GTSRB	Digit → MNIST	Omnifont → Omniglot
baseline	72.0	81.9	71.9
+ random-context	72.1	88.3	69.8
+ refinement-only	86.4	89.7	90.8
+ bias-correction	87.3	89.2	80.5
+ full	95.9	90.2	92.2
Finally, because our approach is context agnostic, our functions are learned without any reference
to target domains. In the formal setting, we assumed that the target domain was contained in the
image of the observation function; however, synthetic images will always be subject to the reality
gap. Our experiments suggest that our approach overcomes this barrier and successfully generalizes
to natural images while training on synthetic data only.
5	Experiments
We evaluate our approach to learning visual tasks using synthetic data on three benchmarks for im-
age recognition. Our training sets consist of a single synthetic image for each object class with no
additional information about the target domain; Figure 2 shows examples of the training and test
images from two of the datasets. On all three benchmarks, our models perform comparably with
previous state-of-the-art results from related settings using few-shot learning and domain adaptation.
Table 1 provides a summary of our results; comprehensive results and comparisons are compiled in
Appendix D. Appendix C provides the full experimental setup and training details. Sample images
from all datasets referenced below, including examples of rendered training data from the experi-
ments and ablation studies, are shown in Appendix E.
5.1	GTSRB
The German Traffic Sign Recognition Benchmark (GTSRB) (Stallkamp et al., 2012) contains
39,209 training and 12,630 test images of 43 classes of German traffic signs taken from the real
world. Our training set consists of a single, canonical pictogram of each class taken from the visual-
ization software accompanying the dataset, which we refer to as Picto. We achieve 95.9% accuracy
on the GTSRB test set training only on Picto, against a human baseline of 98.8%. A comprehensive
comparison with existing approaches can be found in Appendix D, Table 2.
SynSign (Moiseev et al., 2013) is a synthetic dataset designed to provide realistic training data for
traffic sign recognition. The dataset comprises 100,000 synthetically generated images of signs
from Sweden, Germany, and Belgium in a variety of poses, rendered against domain-appropriate
real-world backgrounds (e.g. trees, roads, sky). The dataset contains a superset of the GTSRB
classes; as a result, Saito et al. (2017) report 79.2% accuracy by training directly on SynSign.
For domain adaptation, all approaches train on the full 100,000 images in SynSign plus part of
the GTSRB training set. ATT (Saito et al., 2017) is the only method with better performance than
ours, achieving 0.3% higher accuracy; however they use 31,367 unlabelled images from the GTSRB
training set (in addition to SynSign). Methods using few-shot learning train on roughly half of the
data (22 classes) from the GTSRB training set. The leading few-shot learning approach, VPE (Kim
et al., 2019), adds a pictographic dataset similar to Picto, but achieves only 83.79% accuracy. In
comparison, our training set consists of only 43 images, none of which are from GTSRB.
6
Under review as a conference paper at ICLR 2021
5.2	Handwritten character recognition
MNIST (LeCun) consists of 60,000 training and 10,000 test images of handwritten Arabic numerals
in grayscale against a blank background. Our training set, Digit, consists of a single example of each
digit taken from a standard digital font. Omniglot (Lake et al., 2015) consists of 1623 hand-written
characters from 50 different alphabets, with 20 samples each. The samples were sourced online from
20 workers on Amazon’s Mechanical Turk, who were asked to copy each character from a single
font-based example using digital input (e.g., a mouse). We obtained the original representations for
our dataset, OmniFont. On MNIST, we achieve 90.2% accuracy training only on Digit, compared
to human accuracy of 98%; on Omniglot, we achieve 92.2% 20-way accuracy training only on
Omnifont, compared to human accuracy of 95.5%. Tables 3 and 4 in Appendix D compare these
results with approaches using few-shot learning and domain adaptation.
Handwritten characters and GTSRB present conceptually opposed challenges for learning: in
GTSRB, the objects are rigid two-dimensional objects and backgrounds are complex settings in
the natural world; in Omniglot and MNIST, backgrounds are uniform, but classes no longer have
a strict specification and individual examples exhibit high variability. Thus, the main challenge of
these tasks is learning how to generalize over the object class. Despite the inherent variation, a
baseline model trained on Digit with plain data augmentation was able to achieve 81.9% accuracy
on MNIST, exceeding many domain adaptation approaches and all the one-shot learning results;
Omniglot is more difficult, with an Omnifont plus data augmentation baseline accuracy of 71.9%.
On MNIST, every approach using domain adaptation uses the full Street View House Numbers
(SVHN) training set of 73,257 images of house numbers obtained from Google Street View (Netzer
et al., 2011), plus varying amounts of data from MNIST. The domain transfer problem faces a
similar challenge as Digit, namely, handwriting exhibits different characteristics than house numbers
fonts. Nevertheless, we note that SVHN contains far more examples of each digit. The only non-
baseline approach to exceed our performance is CyCADA (Hoffman et al., 2017), which achieves
0.2% better performance by performing domain adaptation using 60,000 unlabelled images from
the MNIST training set (in addition to training on SVHN). All approaches using few-shot learning
(except FADA) train on 32,460 images from Omniglot and use as few as one image per class from
MNIST; the best result achieves accuracy 3% below ours using 70 images from MNIST. In contrast,
we use only 10 images, none of which are from MNIST.
Omniglot is often described as an MNIST-transpose, where the goal is learn handwriting rather than
specific symbols, and is widely used as a benchmark for few-shot learning. We reproduce the most
common split given in Lake et al. (2015), which uses a predefined set of 30 alphabets, with 19,280
images for training. Test performance is reported as an average over random subsets of n = 5, 20
unseen classes for the n-way task (given one labelled example). In comparison, for each test run, we
retrain a model using only the corresponding n images from OmniFont. As expected, our method
finds 5-way classification easier than 20-way classification (95.8% vs 92.2%). In both cases, our
performance lags behind the state-of-the-art for few-shot learning (>99%), though we emphasize
that our experimental setup differs significantly in both the type and amount of training data used.
Finally, several approaches apply few-shot learning from Omniglot to MNIST, with the idea of trans-
ferring extracted features from human handwriting. However the one-shot experiments all perform
worse than even our baseline approach. We hypothesize that in comparison to Omniglot, where all
the samples come from the same 20 subjects, MNIST may be particularly difficult for transfer one-
shot learning, since any two examples will likely exhibit high “variance”; conversely, our approach
benefits from using a canonical form which might be closer to the “mean” representation.
5.3	Ablation Studies
We conduct two sets of ablation studies to better understand our approach to context-agnostic learn-
ing. The first study tests the individual components of our algorithm for their contributions to gener-
alization over the real world dataset. All strategies employ the same data augmentation and use the
following sampling procedures: baseline picks a fresh random background for each training point,
and measures the performance of training on our synthetic dataset with plain data augmentation;
random-context reuses random backgrounds as contexts; bias-correction reuses previous training
images as contexts; refinement-only is the same as random-context with the addition of PGD-based
refinement; full is the full algorithm as described in Algorithm 2. The results are in Table 1.
7
Under review as a conference paper at ICLR 2021
a) Random initialization (/255)
Figure 3: Context-agnostic performance on Picto using a PGD adversary on the background.
0	4	16	64	255
b) Bias heuristic (/255)
In all cases, we observe that both bias correction and local refinement contribute individually and
jointly to the performance of our models. For GTSRB, a particularly interesting comparison is
training on SynSign, a dataset designed to provide synthetic training data with realistic backgrounds
for GTSRB, which yields 79.2% accuracy (Saito et al., 2017). Though this is an improvement over
our baseline of using random backgrounds at 72.0% accuracy, refinement-only and bias-correction
achieve higher accuracy at 86.4% and 87.3%, respectively. Both methods leverage the background
of training images to combat spurious signals, generating completely unrealistic backgrounds; this
suggests that learning context-agnostic features is more effective than using realistic backgrounds.
The second study measures classification performance in a context-agnostic setting on the synthetic
Picto dataset. By definition, the performance of a context-agnostic classifier should not degrade
under perturbations of the background. We thus run an adaptive attack using a PGD adversary
which fixes the foreground pixels, and ranges from fixed to unbounded on the background pixels,
effectively searching the context space for a background that causes a misclassification on the given
object. We also consider two initialization strategies for the PGD adversary: a standard random
initialization, and initializing to the previous image, inspired by our bias heuristic. We test the same
set of strategies as before, plus a classifier trained directly on the GTSRB training set achieving 98%
performance on the GTSRB test set (real2sim). Appendix E.2 contains samples of the generated
images, and the results are plotted in Figure 3.
Across all experiments, the models have worse (or very close) performance when using our bias
heuristic for initialization. We believe this supports our usage of the bias heuristic for context-
agnostic learning. Additionally, in the last column of Figure 3b, only our full method maintains
passable accuracy, which suggests the gap between models is larger than performance on GTSRB
indicates. We also note that real2sim seems to suffer from a “synthetic gap” even at = 0/255,
which is not entirely unexpected. However, in both settings, performance degrades very quickly
as increases: the effect is most pronounced when the bias heuristic is used to initialize the PGD
adversary, though in both cases the accuracy eventually drops to 0. We emphasize that all of the ex-
periments leave the foreground objects completely unperturbed (and easily human-identifiable); our
results thus suggest that classifiers trained on natural images can become over-reliant on contextual
signals, leading to surprisingly brittle behavior even given unambiguous foregrounds.
6	Conclusion
We introduce the task of context-agnostic learning, a theoretical setting for learning models whose
predictions are independent of background signals. Leveraging the ability to sample objects and con-
texts independently, we propose an approach to context-agnostic learning by minimizing a formally
defined notion of context bias. Our algorithm has a natural interpretation for training classifiers on
vision-based tasks using synthetic data, with the distinct advantage that we do not need to model the
background. We evaluate our methods on several real-world domains; our results suggest that our
approach succeeds in learning context-agnostic classifiers that generalize to natural images using
8
Under review as a conference paper at ICLR 2021
only a single synthetic image of each class, while training with natural images can lead to brittleness
in the context-agnostic setting. Our performance is competitive with existing methods for learning
when data is limited, while using significantly less data. More broadly, the ability to learn from
single synthetic examples of each class also affords fine-grained control over the data used to train
our models, allowing us to sidestep issues of data provenance and integrity entirely.
9
Under review as a conference paper at ICLR 2021
References
Antreas Antoniou, Harrison Edwards, and Amos J. Storkey. How to train your MAML. CoRR,
abs/1810.09502, 2018. URL http://arxiv.org/abs/1810.09502.
Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against support vector ma-
chines, 2012.
John Blitzer, Ryan McDonald, and Fernando Pereira. Domain adaptation with structural correspon-
dence learning. In Proceedings of the 2006 conference on empirical methods in natural language
processing, pages 120-128, 20θ6.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Advances in neural information processing systems, pages 343-
351, 2016.
G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools, 2000.
Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An
open urban driving simulator. arXiv preprint arXiv:1711.03938, 2017.
Li Fe-Fei et al. A bayesian approach to unsupervised one-shot learning of object categories. In
Proceedings Ninth IEEE International Conference on Computer Vision, pages 1134-1141. IEEE,
2003.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 1126-1135. JMLR. org, 2017.
Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-
object tracking analysis. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 4340-4349, 2016.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Muhammad Ghifary, W. Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep
reconstruction-classification networks for unsupervised domain adaptation, 2016.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pages 2672-2680, 2014a.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014b.
Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in
natural images, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition, 2015.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprint
arXiv:1711.03213, 2017.
Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and
artificial neural networks for natural scene text recognition, 2014.
Nick Jakobi, Phil Husbands, and Inman Harvey. Noise and the reality gap: The use of simulation in
evolutionary robotics. In European Conference on Artificial Life, pages 704-720. Springer, 1995.
10
Under review as a conference paper at ICLR 2021
Matthew Johnson-Roberson, Charles Barto, Rounak Mehta, Sharath Nittur Sridhar, Karl Rosaen,
and Ram Vasudevan. Driving in the matrix: Can virtual worlds replace human-generated annota-
tions for real world tasks? arXiv preprint arXiv:1610.01983, 2016.
Junsik Kim, Seokju Lee, Tae-Hyun Oh, and In So Kweon. Co-domain embedding using deep
quadruplet networks for unseen traffic sign recognition. In Thirty-Second AAAI Conference on
Artificial Intelligence, 2018.
Junsik Kim, Tae-Hyun Oh, Seokju Lee, Fei Pan, and In So Kweon. Variational prototyping-encoder:
One-shot learning with prototypical images. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 9462-9470, 2019.
Gregory Koch. Siamese neural networks for one-shot image recognition. 2015.
Brenden M Lake, Ruslan Salakhutdinov, and Joshua B Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/.
Aoxue Li, Tiange Luo, Tao Xiang, Weiran Huang, and Liwei Wang. Few-shot learning with global
class representations. In Proceedings of the IEEE International Conference on Computer Vision,
pages 9715-9724, 2019.
Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks.
In Advances in neural information processing systems, pages 700-708, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Boris Moiseev, Artem Konev, Alexander Chigorin, and Anton Konushin. Evaluation of traffic sign
recognition methods trained on synthetically generated data. In International Conference on
Advanced Concepts for Intelligent Vision Systems, pages 576-583. Springer, 2013.
Saeid Motiian, Quinn Jones, Seyed Iranmanesh, and Gianfranco Doretto. Few-shot adversarial
domain adaptation. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,
pages 6670-6680. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7244-few-shot-adversarial-domain-adaptation.pdf.
Tsendsuren Munkhdalai and Hong Yu. Meta networks. In Proceedings of the 34th International
Conference on Machine Learning-Volume 70, pages 2554-2563. JMLR. org, 2017.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,
Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance
deep learning library. In Advances in Neural Information Processing Systems 32, pages
8024-8035. Curran Associates, Inc., 2019. URL http://papers.neurips.cc/paper/
9015-pytorch-an-imperative-style-high-performance-deep-learning-library.
pdf.
Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adap-
tation. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.
Weichao Qiu and Alan Yuille. Unrealcv: Connecting computer vision to unreal engine. In European
Conference on Computer Vision, pages 909-916. Springer, 2016.
11
Under review as a conference paper at ICLR 2021
Tiago Ramalho and Marta Garnelo. Adaptive posterior learning: few-shot learning with a surprise-
based memory module, 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Kuniaki Saito, Yoshitaka Ushiku, and Tatsuya Harada. Asymmetric tri-training for unsupervised
domain adaptation. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pages 2988-2997. JMLR. org, 2017.
Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-
learning with memory-augmented neural networks. In International conference on machine learn-
ing, pages 1842-1850, 2016.
Alice Schoenauer-Sebag, Louise Heinrich, Marc Schoenauer, Michele Sebag, Lani F. Wu, and
Steve J. Altschuler. Multi-domain adversarial learning, 2019.
Eli Schwartz, Leonid Karlinsky, Joseph Shtok, Sivan Harary, Mattias Marder, Rogerio Feris, Ab-
hishek Kumar, Raja Giryes, and Alex M. Bronstein. Delta-encoder: an effective sample synthesis
method for few-shot object recognition, 2018.
Ashish Shrivastava, Tomas Pfister, Oncel Tuzel, Josh Susskind, Wenda Wang, and Russ Webb.
Learning from simulated and unsupervised images through adversarial training, 2016.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in neural information processing systems, pages 4077-4087, 2017.
J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine
learning algorithms for traffic sign recognition. Neural Networks, (0):-, 2012. ISSN 0893-6080.
doi: 10.1016/j.neunet.2012.02.016. URL http://www.sciencedirect.com/science/
article/pii/S0893608012000457.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages 1199-1208, 2018.
Yaniv Taigman, Adam Polyak, and Lior Wolf. Unsupervised cross-domain image generation. arXiv
preprint arXiv:1611.02200, 2016.
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and Pieter Abbeel. Do-
main randomization for transferring deep neural networks from simulation to the real world. In
2017 IEEE/RSJ international conference on intelligent robots and systems (IROS), pages 23-30.
IEEE, 2017.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 5026-5033.
IEEE, 2012.
Tatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne Tuytelaars. A deeper look at dataset
bias. In Domain adaptation in computer vision applications, pages 37-55. Springer, 2017.
Lucas Tabelini Torres, Thiago M. Paixao, Rodrigo F. BerrieL Alberto F. De Souza, ClaUdine Badue,
Nicu Sebe, and Thiago Oliveira-Santos. Effortless deep training for traffic sign detection using
templates and arbitrary natural images, 2019.
Jonathan Tremblay, Aayush Prakash, David Acuna, Mark Brophy, Varun Jampani, Cem Anil, Thang
To, Eric Cameracci, Shaad Boochoon, and Stan Birchfield. Training deep networks with synthetic
data: Bridging the reality gap by domain randomization. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition Workshops, pages 969-977, 2018.
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 7167-7176, 2017.
12
Under review as a conference paper at ICLR 2021
Stefan van der Walt, Johannes L. Schonberger, Juan Nunez-Iglesias, Francois Boulogne, Joshua D.
Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. scikit-
image: image processing in Python. PeerJ, 2:e453, 6 2014. ISSN 2167-8359. doi: 10.7717/peerj.
453. URL https://doi.org/10.7717/peerj.453.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing Systems, pages 3630-3638, 2016.
Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-
adaptive projection for few-shot learning, 2019.
Jian Zhang, Chenglong Zhao, Bingbing Ni, Minghao Xu, and Xiaokang Yang. Variational few-shot
learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV),
October 2019.
13
Under review as a conference paper at ICLR 2021
A Proofs
ProofofTheorem 3.1. By the assumption that α < 1, we have that for all o, the signs of the expected
classification o and correct classification o* match, so that α ≥ S= |o* -同=1 - |o|. Then for all
o,
'(θ, o*) = 1 — o * o*
=1 T司
1 + 1司门∣-∩
= TTM (IiI)
1 — o * o
=1 +同
='(o,。)
—1 +同
< ,伍,。)
― 2 - α
Now to bound the risk, we can write,
R(h) := E。〜o,sθ['(h(o,c),。*)]
=i⅛∣ XX 仲(。,。,。*)
' ''	' c o
=高 X ⅛ X(1-h©c)*。*)
=高 X(I-。*。*)
1	\、1 —。* o
≤ 同 N 2-α
= (2-⅛M X⅛X(1-hMc)*。
=(2-，|。| X X …@
=曰画£"")"
_ K
2	— a
as desired. It also follows that equality holds if and only if α = ^ for all o.	□
14
Under review as a conference paper at ICLR 2021
B Greedy Bias Correction for visual tasks
Algorithm 2: Visual Learning Using Context-Agnostic Synthetic Data
Input: Object space O, context space C, random permutations Π, observation function γ,
number of rounds R, batch size B, number of classes N , resample probability p,
classifier update subroutine Fit, projected gradient descent subroutine PGD,
classifier h
Output: Trained classifier h
for r — 1 to R do
// initialize empty training batch and random contexts
X i
for n — 1 to N do
I Cn 〜C;
end
for b — 1 to B do
// sample random permutation
π 〜Π(N);
// generate new training data
for n — 1 to N do
o 〜 O(n); // sample object for class
X J γ(o, c∏(n)); //observe object and random (permuted) context
x0 J PGD(h,x); //perform local refinement
X J X ∪ {(x0, y)}; // add to training set
cn J x0 ; // previous sample becomes next context
end
// resample contexts
for n J 1 to N do
p0 J Uniform(0, 1);
if p0 < p then
I Cn 〜C;
end
end
end
// perform classifier update
h J Fit(h, X);
end
15
Under review as a conference paper at ICLR 2021
C Experimental setup
We used PyTorch 1.5.0 (Paszke et al., 2019), OpenCV 4.2.0 (Bradski, 2000), and scikit-image 0.17.2
(van der Walt et al., 2014) for all experiments. In setting the number of epochs, we did not observe
any significant degradation or improvements in performance when training for longer. We use fewer
epochs in the case of Omniglot due to computational constraints, as the model is retrained for each
test split.
For GTSRB, we use a 5-layer convolutional neural network adapted from the official PyTorch
tutorials. To train with Picto, the data augmentation consists of PyTorch transforms Rando-
mAffine(5, translate=(.15, .15), scale=(0.65, 1.05), shear=5), RandomPerspective(0.5, p=1); Col-
orJitter(brightness=.8, contrast=.8, saturation=.8, hue=.05); OpenCV box blur with a random kernel
size between 1 and 6 in both dimensions (independently sampled, so not necessarily square); and a
random exposure adjustment by adjusting all pixels by the same random amount between -30% and
50%. For refinement, we used step sizes of α = 2/255 with 8 steps and an epsilon of = 4/255 for
the foreground only. For the observation function, we superimpose the segmented foreground of the
transformed pictographic sign over the context. We train for 300 epochs using the Adam optimizer
(learning rate 1e-4, weight decay 1e-4), with 5 examples of each class per batch and 20 batches
per epoch. We report results for the model that achieves the best performance on the training set,
checking every 5 epochs.
For MNIST, we use the two-layer convolutional neural network from the official PyTorch exam-
ples for MNIST, with Dropout regularization replaced with pre-activation BatchNorm. To train with
Digit, the data augmentation consists of PyTorch transforms RandomAffine(15, translate=(.15, .15),
scale=(0.75, 1.05), shear=40), RandomPerspective(0.5, p=1); OpenCV box blur with a random ker-
nel size between 1 and 6 in both dimensions (independently sampled, so not necessarily square);
then set the foreground to all pixels with value greater than 0.2. For refinement, we used step sizes
of α = 1.6/255 with 8 iterations and no projection ( = ∞). For the observation function, we blend
the object with the context at a 2:1 ratio; this ensures that inputs have a well-defined ground truth
label. We train for 300 epochs using the Adam optimizer (learning rate 1e-4, weight decay 1e-4),
with 5 examples of each class per batch and 20 batches per epoch. We report results for the model
that achieves the best performance on the training set, checking every 5 epochs.
For Omniglot, we use the pre-activation variant of ResNet18 (He et al., 2015). To train with Omni-
font, we first preprocess with scikit-learn skeletonize and dilation to standardize stroke widths. Data
augmentation consists of PyTorch transforms RandomAffine(15, translate=(.15, .15), scale=(0.75,
1.1), shear=20), RandomPerspective(0.25, p=1); OpenCV box blur with a random kernel size be-
tween 1 and 3 in both dimensions (independently sampled, so not necessarily square); then resize
the images to 28 by 28. For refinement, we used step sizes of α = 1.6/255 with 8 iterations and no
projection ( = ∞). For the observation function, we blend the object with the context at a 2:1 ratio;
this ensures that inputs have a well-defined ground truth label. For the n-way classification task,
we randomly sample n characters from the Omniglot test set, and use the corresponding characters
from the Omnifont dataset as our training set. We then train a fresh model for 150 epochs using
the Adam optimizer (learning rate 1e-4, weight decay 1e-4), and report performance on the all 20n
images in the Omniglot test set, averaged over 20 runs (10 runs for the ablation studies).
16
Under review as a conference paper at ICLR 2021
D Full experimental results
We compare a model trained using our methods with previous state-of-the-art results from related
settings using few-shot learning and domain adaptation on GTSRB (Table 2), MNIST (Table 3), and
Omniglot (Table 4). When multiple experiments are reported for the same approach, we compare
against both the most accurate result as well as the result using the least amount of target data. We
distinguish between labelled (L) and unlabelled (UL) data; experiments for which the training data
is not known are marked (?).
Table 2: GTSRB results.
Approach	Method	Train Source	ing Data Target	Accuracy (%)
	Source Only (Saito et al. (2017))	SynSign		79.2
Baselines	Human (Stallkamp et al. (2012))			98.8
	Target Only (Ganin et al. (2016))		All L	99.8
	VPE (Kim etal. (2019))§	Picto*	22 classes L	83.8
Few-Shot Learning	MatchNet (Vinyals et al. (2016))§		22 classes L	53.3
	QuadNet (Kim et al.(2018)产		22 classes L	45.3
	DSN (BoUsmalis et al. (2016))	SynSign	1280 UL	93.0
	ML (Schoenauer-Sebag et al. (2019))§	SynSign	22 classes L	89.1
Domain Adaptation	MADA (Pei etal.(2018)产	SynSign	22 classes L	84.8
	DANN (Ganin et al. (2016))	SynSign	31367 UL	88.7
	ATT (Saito et al. (2017))	SynSign	31367 UL	96.2
	baseline	Picto		72.0
	+ random-context	Picto		72.1
Context Agnostic	+ refinement-only	Picto		86.4
	+ bias-correction	Picto		87.3
	+ full	Picto		95.9
§ Test accuracy on remaining 21 unseen classes.
*Kim et al. (2019) use a PiCtograPhiC dataset similar to Picto.
^Reported in Kim et al. (2019).
^Reported in Schoenauer-Sebag et al. (2019).
17
Under review as a conference paper at ICLR 2021
Table 3: MNIST results.
Approach	Method	Trai Source	ning Data Target	Accuracy (%)
T⅛l⅞QP1iτiPQ asenes	Human (Netzer et al. (2011))			98.0
	Target Only (Tzeng et al. (2017))		All L	99.2
	FADA (Motiian et al. (2017))	SVHN	1 L / class	72.8
	+ more data	SVHN	7 L / class	87.2
HHX/-Shcf T PQTTiinπ	SiamNet (Koch (2015))	Omniglot	1 L / class	70.3
ew-ot earnng	MatchNet (Vinyals et al. (2016))	Omniglot	1 L / class	72.0
	APL (Ramalho and Garnelo (2019))	Omniglot	1 L / class	61.0
	+ more data	Omniglot	?*	86.0
	DSN (Bousmalis et al. (2016))	SVHN	1000 UL	82.7
	DRCN (Ghifary et al. (2016))	SVHN	?	81.9
lɔɑmain ʌ(~\∙ΛV∖t∙Λtiγλi^i	DANN (Ganin et al. (2016))	SVHN	?	73.9
oman aptaton	ATT (Saito et al. (2017))	SVHN	? L+ 1000 UL	86.0
	ADDA (Tzeng et al. (2017))	SVHN	60,000 UL	76.0
	CyCADA (Hoffman et al. (2017))	SVHN	60,000 UL	90.4
	baseline	Digit		81.9
	+ random-context	Digit		88.3
Context Agnostic	+ refinement-only	Digit		89.7
	+ bias-correction	Digit		89.2
	+ full	Digit		90.2
*Cumulative accuracy from adapting over the test set.
Table 4: Omniglot results for one-shot classification.*
Approach	Method	Training Data	Accur 5-way	acy (%) 20-way
Baselines	Human (Lake et al. (2015))			95.5
	MANN (Santoro et al. (2016))	Omniglot	82.2	
	SiamNet (Koch (2015))	Omniglot	96.7§	92.0
Pew-ShctT pιsmiτιo	MatchNet (Vinyals et al. (2016))	Omniglot	98.1	93.8
ew-ot earnng	PN (Snell et al. (2017))	Omniglot	98.8	96.0
	BPL (Lake et al. (2015))	Omniglot		96.7
	APL (Ramalho and Garnelo (2019))	Omniglot	97.9	97.2
	RN (Sung et al. (2018))	Omniglot	99.6	97.6
	MAML++ (Antoniou et al. (2018))	Omniglot	99.5	97.7
	TapNet (Yoon et al. (2019))	Omniglot		98.1
	GCR (Li et al. (2019))	Omniglot	99.7	99.6
	baseline	Omnifont		71.9
	+ random-context	Omnifont		69.8
Context Agnostic	+ refinement-only	Omnifont		90.8
	+ bias-correction	Omnifont		80.5
	+ full	Omnifont	95.8	92.2
* The exact set up of the one-shot classification task often varies between authors. We believe the broad
performance numbers are still useful for contextualizing our approach, and refer the reader to the original
works for details.
§ As reported in Vinyals et al. (2016)
18
Under review as a conference paper at ICLR 2021
E Training and test set visualizations
E.1 Datasets
Figure 5: From top to bottom: samples from the Omniglot test set, Omnifot dataset, MNIST test set,
Digit dataset, and SVHN training set.
19
Under review as a conference paper at ICLR 2021
E.2 Ablation studies
Figure 6: Training images from the first ablation study using Picto dataset. From top to bottom:
baseline, random-context, refinement-only, bias-correction, full.
Figure 7: Test images from the second ablation study using the Picto dataset. Examples of test im-
ages generated using a PGD adversary initialized randomly (top) and with the bias heuristic (bottom)
at = 255/255.
Figure 8: Training images from the first ablation study for the Digit dataset. From top to bottom:
baseline, random-context, refinement-only, bias-correction, full.
20