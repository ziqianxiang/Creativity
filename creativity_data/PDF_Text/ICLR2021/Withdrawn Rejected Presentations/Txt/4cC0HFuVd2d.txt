Under review as a conference paper at ICLR 2021
Decoy-enhanced Saliency Maps
Anonymous authors
Paper under double-blind review
Ab stract
Saliency methods can make deep neural network predictions more interpretable
by identifying a set of critical features in an input sample, such as pixels that
contribute most strongly to a prediction made by an image classifier. Unfortunately,
recent evidence suggests that many saliency methods poorly perform, especially in
situations where gradients are saturated, inputs contain adversarial perturbations,
or predictions rely upon inter-feature dependence. To address these issues, we
propose a framework that improves the robustness of saliency methods by following
a two-step procedure. First, we introduce a perturbation mechanism that subtly
varies the input sample without changing its intermediate representations. Using
this approach, we can gather a corpus of perturbed data samples while ensuring
that the perturbed and original input samples follow the same distribution. Second,
we compute saliency maps for the perturbed samples and propose a new method
to aggregate saliency maps. With this design, we offset the gradient saturation
influence upon interpretation. From a theoretical perspective, we show that the
aggregated saliency map not only captures inter-feature dependence but, more
importantly, is robust against previously described adversarial perturbation methods.
Following our theoretical analysis, we present experimental results suggesting that,
both qualitatively and quantitatively, our saliency method outperforms existing
methods, in a variety of applications.
1	Introduction
Deep neural networks (DNNs) deliver remarkable performance in an increasingly wide range of
application domains, but they often do so in an inscrutable fashion, delivering predictions without
accompanying explanations. In a practical setting such as automated analysis of pathology images, if
a patient sample is classified as malignant, then the physician will want to know which parts of the
image contribute to this diagnosis. Thus, in general, a DNN that delivers interpretations alongside its
predictions will enhance the credibility and utility of its predictions for end users (Lipton, 2016).
In this paper, we focus on a popular branch of explanation methods, often referred to as saliency
methods, which aim to find input features (e.g., image pixels or words) that strongly influence the
network predictions (Simonyan et al., 2013; Selvaraju et al., 2016; Binder et al., 2016; Shrikumar
et al., 2017; Smilkov et al., 2017; Sundararajan et al., 2017; Ancona et al., 2018). Saliency methods
typically rely on back-propagation from the network’s output back to its input to assign a saliency
score to individual features so that higher scores indicate higher importance to the output prediction.
Despite attracting increasing attention, saliency methods suffer from several fundamental limitations:
•	Gradient saturation (Sundararajan et al., 2017; Shrikumar et al., 2017; Smilkov et al., 2017) may
lead to the problem that the gradients of important features have small magnitudes, breaking down
the implicit assumption that important features, in general, correspond to large gradients. This
issue can be triggered when the DNN outputs are flattened in the vicinity of important features.
•	Importance isolation (Singla et al., 2019) refers to the problem that gradient-based saliency
methods evaluate the feature importance in an isolated fashion, implicitly assuming that the other
features are fixed.
•	Perturbation sensitivity (Ghorbani et al., 2017; Kindermans et al., 2017; Levine et al., 2019) refers
to the observation that even imperceivable, random perturbations or a simple shift transformation
of the input data may lead to a large change in the resulting saliency scores.
1
Under review as a conference paper at ICLR 2021
In this paper, we tackle these limitations by proposing a decoy-enhanced saliency score. At a high
level, our method generates the saliency score of an input by aggregating the saliency scores of
multiple perturbed copies of this input. Specifically, given an input sample of interest, our method
first generates a population of perturbed samples, referred to as decoys, that perfectly mimic the
neural network’s intermediate representation of the original input. These decoys are used to model
the variation of an input sample originating from either sensor noise or adversarial attacks. The decoy
construction procedure draws inspiration from the knockoffs, proposed recently by Barber & Candes
(2015) in the setting of error-controlled feature selection, where the core idea is to generate knockoff
features that perfectly mimic the empirical dependence structure among the original features.
In brief, the current paper makes three primary contributions. First, we propose a framework to perturb
input samples to produce corresponding decoys that preserve the input distribution, in the sense that
the intermediate representations of the original input data and the decoys are indistinguishable. We
formulate decoy generation as an optimization problem, applicable to diverse deep neural network
architectures. Second, we develop a decoy-enhanced saliency score by aggregating the saliency maps
of generated decoys. By design, this score naturally offsets the impact of gradient saturation. From a
theoretical perspective, we show how the proposed score can simultaneously reflect the joint effects of
other dependent features and achieve robustness to adversarial perturbations. Third, we demonstrate
empirically that the decoy-enhanced saliency score outperforms existing saliency methods, both
qualitatively and quantitatively, on three real-world applications. We also quantify our method’s
advantage over existing saliency methods in terms of robustness against various adversarial attacks.
2	Related work
A variety of saliency methods have been proposed in the literature. Some, such as edge detectors and
Guided Backpropagation (Springenberg et al., 2014) are independent from the predictive model (Nie
et al., 2018; Adebayo et al., 2018). 1 Others are designed only for specific architectures (i.e., Grad-
CAM (Selvaraju et al., 2016) for CNNs, DeConvNet for CNNs with ReLU activations (Zeiler &
Fergus, 2014)). In this paper, instead of exhaustively evaluating all saliency methods, we apply our
method to the three saliency methods that do depend on the predictor (i.e., passing the sanity checks
in Adebayo et al. (2018) and Sixt et al. (2020)) and are applicable to diverse DNN architectures:
•	The vanilla gradient method (Simonyan et al., 2013) simply calculates the gradient of the class
score with respect to the input x, which is defined as Egrad (x; Fc) = OxFc(x).
•	The SmoothGrad method (Smilkov et al., 2017) seeks to reduce noise in the saliency map
by averaging over explanations of the noisy copies of an input, defined as Esg (x; Fc) =
N1 PN=1 Egrad(X + gi； FC) with noise vectors gi 〜N(0, σ2).
•	The integrated gradient method 2 (Sundararajan et al., 2017) starts from a baseline input x0 and
sums over the gradient with respect to scaled versions of the input ranging from the baseline to the
observed input, defined as Eig(x; Fc) = (x - x0) × R01OxFc(x0 + α(x - x0))dα.
We do not empirically compare to several other categories of methods. Counterfactual-based methods
work under the same setup as saliency methods, providing explanations for the predictions of a pre-
trained DNN model (Sturmfels et al., 2020). These methods identify the important subregions within
an input image by perturbing the subregions (by adding noise, rescaling (Sundararajan et al., 2017),
blurring (Fong & Vedaldi, 2017), or inpainting (Chang et al., 2019)) and measuring the resulting
changes in the predictions (Ribeiro et al., 2016; Lundberg & Lee, 2017; Chen et al., 2018; Fong &
Vedaldi, 2017; Dabkowski & Gal, 2017; Chang et al., 2019; Yousefzadeh & O’Leary, 2019; Goyal
et al., 2019). Although these methods do identify meaningful subregions in practice, they exhibit
several limitations. First, counterfactual-based methods implicitly assume that regions containing the
object most contribute to the prediction (Fan et al., 2017). However, Moosavi-Dezfooli et al. (2017)
showed that counterfactual-based methods are also vulnerable to adversarial attacks, which force these
methods to output unrelated background rather than the meaningful objects as important subregions.
1Sixt et al. (2020) shows that LRP (Binder et al., 2016) is independent of the parameters of certain layers.
2Ancona et al. (2018) shows that input gradient and DeepLIFT (Shrikumar et al., 2017) are strongly
related to the integrated gradient. As such, we only select the integrated gradient.
2
Under review as a conference paper at ICLR 2021
(A) Patch masks Pretrained network Label-dependent gradients
Original image
Decoy images
Figure 1: The overview of the proposed method. (A) The workflow of creating decoy-enhanced
saliency maps. (B) The operation of swapping image patches between original and decoy images.
Decoy-enhanced
saliency maps
Saliency maps of decoys
(B) original image
Identical
intermediate
representation
Decoy image
Second, the counterfactual images may be potentially far away from the training distribution, causing
ill-defined classifier behavior (Burns et al., 2019; Hendrycks & Dietterich, 2019).
In addition to these limitations, counterfactual-based methods and our decoy-based method are
fundamentally different in three ways. First, the former seeks the minimum set of features to exclude
in order to minimize the prediction score or to include in order to maximize the prediction score (Fong
& Vedaldi, 2017), whereas our approach aims to characterize the influence of each feature on the
prediction score. Second, counterfactual-based methods explicitly consider the decision boundary by
comparing each image to the closest image on the other side of the boundary. In contrast, the proposed
method only considers the decision boundary implicitly by calculating the gradient’s variants. Third,
unlike counterfactual images, which could potentially be out-of-distribution, decoys are plausibly
constructed in the sense that their intermediate representations are indistinguishable from the original
input data by design. Because of these limitations and differences, we do not compare our method
with counterfactual-based methods.
In addition to saliency methods and counterfactual-based methods, several other types of interpretation
methods have been proposed that either aim for a different goal or have a different setup. For
example, recent research (e.g., Ribeiro et al. (2016); Lundberg & Lee (2017); Chen et al. (2018;
2019b)) designed techniques to explain a black-box model, where the model’s internal weights are
inaccessible. Koh & Liang (2017) and some follow-up work (Yeh et al., 2018; Koh et al., 2019)
tried to find the training points that are most influential for a given test sample. Some other efforts
have been made to train a more interpretable DNN classifier (Fan et al., 2017; ZoIna et al., 2019;
Alvarez-Melis & Jaakkola, 2018; Toneva & Wehbe, 2019), synthesize samples that represent the
model predictions (Ghorbani et al., 2019; Chen et al., 2019a)), or identifying noise-tolerant features
(Ikeno & Hara, 2018; Schulz et al., 2020). However, due to the task and setup differences, we do not
consider these methods in this paper.
3	Methods
3.1	Problem setup
Consider a multi-label classification task in which a pre-trained neural network model implements
a function F : Rd 7→ RC that maps from the given input x ∈ Rd to C predicted classes. The score
for each class C ∈ {1, ∙∙∙ ,C} is Fc(x), and the predicted class is the one with maximum score,
i.e., argmaxc∈{i,…，c} Fc(x). A saliency method aims to assign to each feature a saliency score,
encoded in a saliency map E(x; Fc) : Rd 7→ Rd, in which the features with higher scores represent
higher “importance” relative to the final prediction.
Given a pre-trained neural network model F with L layers, an input x, and a saliency method E such
that E(x; F) is a saliency map of the same dimensions as x, the proposed scores can be obtained in
two steps: generating decoys and aggregating the saliency maps of the decoys (See Fig. 1(A) that
illustrates the workflow of creating decoy-enhanced saliency maps).
3
Under review as a conference paper at ICLR 2021
3.2	Decoy definition
Say that f` : Rd → Rd' is the function instantiated by the given network, which maps from an input
X ∈ Rd to its intermediate representation F'(x) ∈ Rd' at layer ' ∈ {1,2,…，L}. A vector X ∈ Rd
is said to be a decoy of x ∈ Rd at a specified layer ` if the following swappable condition is satisfied:
Fxx) = f`(XSWaP(x,κ)), for swappable features K⊂{1, ∙∙∙ ,d} .	(1)
Here, the swap(X, K) operation swaps features between X and X based on the elements in K. In this
work, K represents a small meaningful feature set, which represents a small region/segment in an
image or a group of words (embeddings) in a sentence. Take an image recognition task for example.
Assume K = {10} and X is a zero matrix, then XSWaP(X,K) indicates a new image that is identical to
X except that the tenth pixel is set to zero. An illustrative explanation of swap operator is shown in
Fig. 1(B). Using the swappable condition, we aim to ensure that the original image X and its decoy X
are indistinguishable in terms of the intermediate representation at layer `. Note in particular that the
construction of decoys relies solely on the first' layers of the neural network F1,F2,…，f` and is
independent of the succeeding layers F'+ι, ∙∙∙ , Fl. As such, X is conditionally independent of the
classification task F(x) given the input x; i.e., X ɪ F(x) |x.
3.3	Decoy generation
To identify decoys satisfying the swappable condition, we solve the following optimization problem:
maXimiZe 玄 ∈[xrain,xmx]d
s.t.
II((X - χ) ∙ s)+∣∣1,
{1(F'-xX)- F (-M=0，
(2)
Here, (∙)+ = max(∙,0), and the operators 小% and ∣∣∙k∞ correspond to the Li and L∞ norms,
respectively. M ∈ {0,1}d is a specified binary mask. And the value of each feature in the decoy X
is restricted to lie in a legitimate value range i.e., [Xmin, Xmax] (e.g., the pixel value should lie in [0,
255]). We impose the constraint HFi(X) 一 FXX)∣∞ ≤ e, which ensures that the generated decoy
satisfies the swappable condition described in Eqn. 1. It should be noted that we take X and X to be
indistinguishable except for the swappable features indicated by the mask (i.e., XSwap(X,k)= X).
As is shown later in Section 3.4, our decoy-enhanced saliency score is defined to capture the empirical
range of the decoy saliencies. Here, we first need to estimate the upper/lower ends of the legitimate
decoys. To achieve this, in Eqn. 2, we maximize the deviation between X and X from both the positive
and negative directions, i.e., s = +1 and s = 一1. By using this objective function, for each mask
M, we can compute two decoys—one for the positive deviation (i.e., s = +1) and the other for the
negative one (i.e., s = 一1).
To solve the optimization function in Eqn. 2, we employ three commonly adopted methods - lagrange
multiplier, projected gradient, and change-of-variable (Carlini & Wagner, 2017) - to transform the
original objective function into the following form:
∣	1	∣	∣	1	∣2
minimize* — max((-(tanh(x) + 1) — x) ∙ s, 0)	+ λ ∙ (|F'(-(tanh(x) + 1)) — F⅛(x)∣- τ)+	,
∣	2	∣1	∣	2	∣2
(3)
where λ > 0 is the lagrange multiplier. Xi = arctanh(2Xi 一 1), for all i ∈ {1, 2, ∙∙∙ ,d}. τ > 0 is
introdued to approximate the L∞ norm in Eqn. 2. After obtaining X by solving Eqn. 3, we compute X
and map it back to the original feature value range [Xmin, Xmax]. More details about how to transform
Eqn. 2 into Eqn. 3 can be found in Section A6.
3.4	Decoy-enhanced saliency scores
Given an input sample X and a swappable patch with size P, we can obtain (√d 一 P + stride)2
unique masks by sliding the swappable patch across the input with a certain stride. For computational
efficiency, we aggregate m masks into one decoy sample and optimize these masks jointly by
solving one decoy sample from Eqn. 3. Then, we can generate 2n decoys for that sample. We
denote these decoys as {X1, X2,…,X2n}. Here, n
[(√d 一
P + stride)2/m
. For these decoys,
4
Under review as a conference paper at ICLR 2021
we can then apply a given saliency method E to yield the corresponding decoy saliency maps
{E(X1; F), E(x2; F), .…，E(x2n; F)}. With these decoy Saliency maps in hand, for each feature
xi in x, we can characterize its saliency score variation by using a population of saliency scores
Ei = {E(X1; Fc)i,E(X2; Fc)i,…，E(X2n; Fc)i}. In this work, we define the decoy-enhanced
saliency score Zi for each feature xi as
一 ,` . ,` .
Zi = max(Ei) — mm(E¾) .	(4)
Here, Zi is determined by the empirical range of the decoy saliency scores. Ideally, important features
will have large values and unimportant ones will have small values. Note that the proposed method is
designed specifically for nonlinear models in need of interpretation. As is discussed in Section A7, it
cannot output meaningful saliency maps on linear models. It should also be noted that by sliding the
swappable patch across the input and ensembling the obtained decoy-enhanced saliency maps, we
could capture the saliency of each feature. The motivations of manipulating at a patch level rather
than the entire input are capturing the local dependency structure and enabling batch operations for
better efficiency.
3.5	Theoretical insights
In this section, we analyze the saliency score method in a theoretical fashion. 3 In particular, we
take a convolutional neural network with the ReLU activation function as an example to discuss why
the proposed interpretation method can account for inter-feature dependence while also improving
explanatory robustness. It should be noted that, while we conduct our theoretical analysis in the setting
of CNNs with a specific activation function, the conclusions drawn from the theoretical analysis
can easily be extended to other feed-forward neural architectures and other activation functions
(e.g., sigmoid and tanh). For analysis of other neural architectures, see Section A9.
Consider a CNN with L hidden blocks, with each layer ` containing a convolutional layer with a filter
of size √s' X √S' and a max pooling layer with pooling size √⅞ X √s'. (We set the pooling size
the same as the kernel size in each block for simplicity.) The input to this CNN is x ∈ Rd, unrolled
from a √d X √d matrix. Similarly, we also unroll each convolutional filter into g` ∈ Rs', where
g` is indexed as (g`j for j ∈ j`. Here, j` corresponds to the index shift in matrix form from the
top-left to bottom-right element. For example, a 3 X 3 convolutional filter (i.e., s` = 9) is indexed
by J' = {-√d - 1, -√d, -√d + 1,-1,0,1, √d - 1, √d, √d + l}
The output of the network is
the probability vector p ∈ RC generated by the softmax function, where C is the total number of
classes. Such a network can be represented as
m` = pool(relu(g` * m`-i)) for ' = 1,2, 3,…，L ,
o = WTL+1mL + bL+1,	(5)
p = softmax(o) ,
where relu(∙) and pool(∙) indicate the ReLU and pooling operators, m` ∈ Rd' is the output of the
block ' (mo = x), and (g` * m`-i) ∈ Rd'-1 represents a convolutional operation on that block. We
assume for simplicity that the convolution retains the input shape.
Consider an input X and its decoy x, generated by swapping features in K. For each feature i ∈ K,
we have the following theorem for the decoy-enhanced saliency score Zi :
Theorem 1. In the aforementioned setting, Zi is bounded by
Zi — 1 X (x+ — X-)(Hχ)k,i	≤ Cl .	(6)
k∈K
Here, C1 > 0 is a bounded constant and Hx is the Hessian of Fc(x) on x where (Hx)i,k
∂2F C
∂xi ∂xk
X+ and X- refer to the decoy that maximizes and minimizes E(X; Fc), respectively. See Section A7
for the proof. Theorem 1 implies that the proposed saliency score is determined by the second-
order Hessian ((Hx)i,k) in the same swappable feature set. The score explicitly models the feature
3Note that we developed the theoretical properties by using the vanilla gradient as the base saliency method.
5
Under review as a conference paper at ICLR 2021
dependencies in the swappable feature set via this second-order Hessian, potentially capturing
meaningful patterns such as edges, texture, etc.
In addition to enabling representation of inter-feature dependence, Theorem 1 sheds light on the
robustness of the proposed saliency score against adversarial attack. To illustrate the robustness
improvement of our method, we introduce the following proposition. The proof of this proposition as
well as in-depth analysis can be found in Section A8.
Proposition 1. Given an input X and the corresponding adversarial sample x, ifboth |xi - Xi| ≤
C2δi and JXi - Xi J ≤ C2δi can be obtain where C2 > 0 is a bounded constant and δi =
|E(X, F)i - E(x, F)i|, then the following relation can be guaranteed.
|(Zx)i-(Zχ)i∣ ≤ ∣E(X,F)i- E(x,F)i|.	(7)
Given an adversarial sample X (i.e., the perturbed x), we say a saliency method is not robust
against X if the deviation of the corresponding explanation δi = |E(X, F) - E(x, F)i| (for all
i ∈ {1,2, ∙∙∙ ,d}) is large. According to the proposition above, we can easily discover that the
deviation of our decoy-enhanced saliency score is always no larger than that of other saliency methods
when a certain condition is satisfied. This indicates that, when the condition holds, our saliency
method can guarantee a stronger resistance to the adversarial perturbation. To ensure the satisfication
of conditions ∣Xi - Xi| ≤ C2δi and ∣Xi - XiJ ≤ C2δi, we can further introduce the corresponding
condition as a constraint to Eqn. 2. In the following section, without further clarification, the saliency
scores used in our evaluation are all derived with this constraint imposed.
4	Experiments
To evaluate the effectiveness of our proposed method, we perform extensive experiments on deep
learning models that target three tasks: image classification, sentiment analysis, and network intrusion
detection. The performance of our approach is assessed both qualitatively and quantitatively. The
results show that our proposed method identifies intuitively more coherent saliency maps than the
state-of-the-art saliency methods alone. The method also achieves quantitatively better alignment
to truly important features and demonstrates stronger robustness to adversarial manipulation. The
description of the datasets and experimental setup can be found in Section A10.
4.1	Saliency benchmark
As mentioned in Section 2, we apply our decoy enhancement method to three saliency methods:
vanilla gradient, SmoothGrad, and integrated gradient. Here, we applied the default setup for the
integrated gradient (a zero baseline) and SmoothGrad. Section A14 shows that our method can
also improve the performance of the variants of the integrated gradient/SmoothGrad and GradCAM.
In each case, the decoy-enhanced saliency scores are post-processed in the following way before
qualitative and quantitative evaluations. To rule out the bias introduced by the saliency values and
ensure a fair comparison, we constructed a binary saliency map by retaining only the top-K features
ranked by each method. Specifically, we set the saliency value of the selected features as 1 and
the rest features as 0. In this section, we choose the K as the top 20 percent of all features. Note
that Section A16 shows that subtly varying K does not influence the experiment conclusions. To
demonstrate that all three methods, when enhanced with decoys, still depend on the predictor, we
carry out a sanity check on the ImageNet dataset. The results show that our decoy enhanced-saliency
methods pass the sanity check (see Section A11 for details).
4.2	Performance in various applications
To comprehensively evaluate our proposed approach against the baselines mentioned above, we
focus on two criteria. First, we aim to achieve qualitative coherence of the identified saliency
map. Intuitively, we prefer a saliency method that highlights features that align closely with the
predictions (e.g., highlights the object of interest in an image or the words indicating the sentiment
of the sentence). Second, we aim to quantify the correctness of the saliency maps produced by the
corresponding method. To do it, we use the fidelity metric (Dabkowski & Gal, 2017), defined as:
SF(E(∙; Fc), x) = - log Fc(E(x; FC) ◦ x)	(8)
6
Under review as a conference paper at ICLR 2021
stcejbo dnuorgeroF
Gradient Gradient
Terrier SF 1195 SF 2 44
Scotter SF 14「 SF 782
Bustard SF 305 SF 008
stcejbo dnuorgkca
VOlcano
Seashor
IntGrad IntGrad IntGrad
w/o decoy w/ decoys difference
Gradient
SF 12 21
SF(TOT8^ SF 0 012
SGrad SGrad SGrad
w/o decoy w/ decoys difference
SF 0 064
SF 0 0 6
至布piz
(B)
Decoy variations: ■ Decoys w/ range aggregation ■
■ Without decoy ■ Constant w/ range aggregation ∣
Noise w/ range aggregation
Decoys w/ mean aggregation
Gradient
IntGrad
SGrad
SF 4 19
SF 5 95
SF 8 96 SF 3 38
SF 5.34
SF 4.98
+1.01...........................................I	-1.0
saliency w/ decoys - saliency w/o decoy
0 5 0 5c
‹ɔ)20ZL
C)ɪ ɪ ɪ
( ytilediF
Number of used decoys
Figure 2: Performance evaluation on ImageNet. (A) Visualization of saliency maps on foreground
and background objects. (B) Fidelity comparison of original saliency method (i.e., “Without decoys”),
our method (i.e., “Decoys w/ range aggregation”), and its alternatives: replacing the decoy generation
(Eqn. 2) with constant perturbation (i.e., “Constant w/ range aggregation”) or noise perturbation
(i.e., “Noise w/ range aggregation”); replacing the decoy aggregation (Eqn. 4) with mean aggregation
(i.e., “Decoys w/ mean aggregation”) (See Tab. A4 for more statistics about the performance differ-
ences between our method and the baselines). (C) Performance with regard to variant patch size and
different number of decoys.
where c indicates the predicted class of input x, and E(x; Fc) is the top-K -retained binary saliency
map described above. E(x; Fc) ◦ x performs entry-wise multiplication between E(x; Fc) and x,
encoding the overlap between the object of interest and the concentration of the saliency map. The
rationale behind this metric utilization is as follows. By viewing the saliency score of the feature as
its contribution to the predicted class, a good saliency method will highlight more important features
and thus give rise to higher predicted class scores and lower metric values.
4.2	. 1 Performance on the ImageNet dataset
We applied our decoy-enhanced saliency score to randomly sampled images from the ImageNet
dataset (Russakovsky et al., 2015), with a pretrained VGG16 model (Simonyan & Zisserman, 2014).
See Section A12 for applicability of our method to diverse CNN architectures such as AlexNet
(Krizhevsky et al., 2012) and ResNet (He et al., 2016). The 3 × 3 image patches are treated as
swappable features in generating decoys.
A side-by-side comparison (Fig. 2(A)) suggests that decoys consistently help to reduce noise and
produce more visually coherent saliency maps. For example, the original integrated gradient method
highlights the region of dog head in a scattered format, which is also revealed by the difference plot.
In contrast, the decoy-enhanced integrated gradient method not only highlights the missing body but
also identifies the dog head with more details such as ears, cheek, and nose (See Section A18 for
more visualization examples). The visual coherence is also quantitatively supported by the saliency
fidelity score.
To further evaluate the necessity of the two steps (i.e., decoy generation and aggregation) in our
method, we carried out a control experiment by replacing either step with alternatives. Specifically,
as alternatives to the decoy generation, we used an image in which all pixel values are either replaced
with a single mean pixel value or contaminated with Gaussian white noise. Regarding the decoy
aggregation, we calculated the mean saliency score as the alternative. As shown in Fig. 2(B), our
method, which incorporate both steps, reports the best performance. This validates the effectiveness
of each of our designs.
Recall that the number of decoys n is decided by the patch size (P), stride, and the number of
multiple masks in one decoy (m). Here, we keep stride as 1 and vary n by selecting different P and
7
Under review as a conference paper at ICLR 2021
(A) Gradient w/o decoy No	movement	no	yuks	not	much	of	anything	SF:	1.003
Gradient w/ decoys ∣No	movement	no	yuks	not	much	of	anything	SF:	0.075
IntGrad w/o decoy ∣No	movement	no	yuks	not	much	of	anything	SF:	1.003
IntGrad w/ decoys No	movement	no	yuks	not	much	of	anything	SF:	0.084
SGrad w/o decoy ^∣ movement	^o yuks	not	much	of	anything	sf：	0.111
SGrad w/ decoys No movement 卜。yuks much of anything sf： 0.105
Gradient w/o decoy [most new movies have
Gradient w/ decoys most new movies have
IntGrad w/o decoy [most new movies have
IntGrad w/ decoys most new movies have
SGrad w/o decoy most new movies have
SGrad w/ decoys [most new movies have
IbHH∣^ SF: 0.127
a .ht sh. SF: 0.119
I 1 IB SF: 0.093
a bright sheen SF: 0.093
a bright sheen SF: 0.458
a bright sheen SF: 0.031
Negative sentiment Positive sentiment
(B)
Decoy variations： Decoys w/ range aggregation Noise w/ range aggregation
■ Without decoy ■ Constant w/ range aggregation ■ Decoys w/ mean aggregation
1.0
0.5
0.0
Al=① piz
Gradient
IntGrad	SGrad
Figure 3: Evaluation results obtained from the SST dataset. (A) Visualization of saliency maps
in each word, where the normalized saliency values are shown for better distinction. (B) Fidelity
comparison of the original saliency method, our method, and its alternatives. Here, the alternative
methods represent the practice of replacing the decoy generation (Eqn. 2) with constant perturbation
or noise perturbation as well as the practice of replacing the decoy aggregation (Eqn. 4) with mean
aggregation (See Tab. A5 for more statistics about the performance differences).
M . Fig. 2(C) shows that our method achieves stable fidelity scores across the substantial variations
of decoy numbers. The sensitivity test of other hyper-parameters can be found in Section A16.
4.2.2 Performance on the Stanford Sentiment Treebank (SST) dataset
We also applied our decoy-enhanced saliency score to randomly sampled sentences from the Stanford
Sentiment Treebank (SST) (Russakovsky et al., 2015). We train a two-layer CNN (Kim, 2014) which
takes the pretrained word embeddings as input (Pennington et al., 2014) (see A10 for experimental
details). As suggested by Guan et al. (2019), the average saliency value of all dimensions of a word
embedding is regarded as the word-level saliency value. The embeddings of the words are treated as
swappable features when generating decoys.
As shown in Fig. 3(A), a side-by-side comparison suggests that decoys consistently help to produce
semantically more meaningful saliency maps. For example, in a sentence with negative sentiment,
keywords associated with negation, such as ’no’ and ’not’, are more highlighted by decoy-enhanced
saliency methods. The semantic coherence is also quantitatively supported by the saliency fidelity
(Fig. 3(B)). We also tested the alternatives mentioned above: constant (replacing the decoy generation
with the mean embedding of the whole dictionary) and noise perturbation with range aggregation,
and decoy with mean aggregation. Fig. 3(B) shows that our method outperforms these alternatives.
4.3	Robustness to adversarial attacks
Next we investigate the robustness of our method to adversarial manipulations of images.In particular,
we focus on three popular adversarial attacks (Ghorbani et al., 2017): (1) the top-k attack, which
seeks to decrease the scores of the top k most important features, (2) the target attack, which aims to
increase the importance of a pre-specified region in the input image, and (3) the mass-center attack,
which aims to spatially change the center of mass of the original saliency map. Here, we specify the
bottom-right 4 × 4 region of the original image for the target attack and select k = 5000 in the top-k
attack. We use the sensitivity metric (Alvarez-Melis & Jaakkola, 2018) to quantify the robustness of
a saliency method E to adversarial attack, defined as:
SS(E(∙, Fc), x, x) = k(E(X，FC)- E(x,Fc))k2	(9)
kx - xk2
where X is the perturbed image of x. A small sensitivity value means that similar inputs do not lead
to substantially different saliency maps.
As shown in Fig. 4(A), a side-by-side comparison suggests that decoys consistently yield low
sensitivity scores and help to produce more visually coherent saliency maps, mitigating the impact of
various adversarial attacks. More examples can be found in Section A18. The visual coherence and
robustness to adversarial attacks are also quantitatively supported by Fig. 4(B)〜(D). As is mentioned
above, we also did experiments on a MLP trained with a network intrusion dataset and show the
results in Section A13. The results are consistent with those on CNNs, which confirm our method’s
applicability to the widely-used feed-forward networks.
8
Under review as a conference paper at ICLR 2021
Target
SS: 30.50
SS： 24.10
SS： 16.21
SS： 27.15 SS： 22.70
SS： 24.81
Attack
Figure 4: Robustness to adversarial attacks on images. (A) Visualization of saliency maps under
adversarial attacks. (B)~(D) The decoy-enhanced Saliency score is compared to the original Saliency
score under adversarial attacks, evaluated by sensitivity (See Tab. A6 for more statistics about the
performance differences).
5 Discussion and conclusion
In this work, we propose a method for computing, from a given saliency method, decoy-enhanced
saliency scores that yield more accurate and robust saliency maps. We formulate the decoy generation
as an optimization problem, applicable to diverse deep neural network architecture. We demonstrate
the superior performance of our method relative to three standard saliency methods, both qualitatively
and quantitatively, even in the presence of various adversarial perturbations to the image. From a
theoretical perspective, by deriving a closed-form solution, we show that the proposed score can
provably compensate for the limitations of existing saliency methods by reflecting the joint effects
from other dependent features and maintaining robustness to adversarial perturbations.
Fig. 2(C) shows our method can achieve a decent performance with only a small number of decoys.
Section A15 further shows the runtime of generating one decoy is marginal compared to existing
saliency methods. This indicates that our technique can improve the existing saliency methods
without introducing too much computational overhead. With the parallel computing enabled by
multiple GPUs, our method can be much faster, which further decreases the overhead and esca-
lates our method’s practicability (See Section A15 for a detailed runtime analysis). Our method
mainly introduce three hyperparameters: swappable feature size K, network layer `, and initial
Lagrange multiplier λ. In Section A16, we show that our method is insensitive to the substantial
variation of hyperparameters. We generate decoys by using Eqn. 2. While there are other widely
used perturbation methods (e.g., random noise, blurring, and inpainting), they are not suitable for
generating decoys. First, Section 4 shows that some general pertrbations (i.e., random noise and
constant perturbation) obtain worse fidelity than decoy. Second, without ensuring the swappable
condition in Eqn. 1, they cannot provide a theoretical guarantee for robustness improvement. Third,
methods like blurring and inpainting are not well-defined for applications beyond computer vision.
This work points to several promising directions for future research. First, (E(x; Fc) ◦ x) may
be out-of-distribution and thus fails our fidelity metric. We will investigate more rigorous metrics
and use other benchmark datasets (e.g., BAM (Yang & Kim, 2019)) for evaluation. Second, a
possible extension is to customize our method to recurrent neural networks and to inputs with
categorical/discrete features. Third, recent work (Bansal et al., 2020; Chen et al., 2019c) shows that
adversarial training can improve the interpretability of a DNN model. It is worth exploring whether
our method could further enhance the quality of saliency maps derived from these adversarially
retrained classifiers. A fourth promising direction could be reframing interpretability as hypothesis
testing and using decoys to deliver a set of salient features, subject to false discovery rate control at
some pre-specified level (Burns et al., 2019; Lu et al., 2018).
9
Under review as a conference paper at ICLR 2021
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim. Sanity
checks for saliency maps. In Proc. of NeurIPS, 2018.
David Alvarez-Melis and Tommi S Jaakkola. Towards robust interpretability with self-explaining
neural networks. In Proc. of NeurIPS, 2018.
Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understanding of
gradient-based attribution methods for deep neural networks. In Proc. of ICLR, 2018.
Naman Bansal, Chirag Agarwal, and Anh Nguyen. Sam: The sensitivity of attribution methods to
hyperparameters. arXiv preprint arXiv:2003.08754, 2020.
Rina Foygel Barber and Emmanuel J Candes. Controlling the false discovery rate via knockoffs. The
Annals of Statistics, 2015.
Alexander Binder, GregOire Montavon, Sebastian Lapuschkin, Klaus-Robert Muller, and Wojciech
Samek. Layer-wise relevance propagation for neural networks with local renormalization layers.
In Proc. of ICANN, 2016.
Collin Burns, Jesse Thomason, and Wesley Tansey. Interpreting black box models via hypothesis
testing. arXiv:1904.00045, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In Proc.
of S&P, 2017.
Tsung-Han Chan, Kui Jia, Shenghua Gao, Jiwen Lu, Zinan Zeng, and Yi Ma. PCANet: A simple
deep learning baseline for image classification. IEEE Transactions on Image Processing, 2015.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image
classifiers by counterfactual generation. In Proc. of ICLR, 2019.
Chaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. This looks
like that: deep learning for interpretable image recognition. In Proc. of NeurIPS, 2019a.
Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. Learning to explain: An
information-theoretic perspective on model interpretation. In Proc. of ICML, 2018.
Jianbo Chen, Le Song, Martin J Wainwright, and Michael I Jordan. L-shapley and c-shapley: Efficient
model interpretation for structured data. In Proc. of ICLR, 2019b.
Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha. Robust attribution regular-
ization. In Proc. of NeurIPS, 2019c.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In Proc. of
NeurIPS, 2017.
Lijie Fan, Shengjia Zhao, and Stefano Ermon. Adversarial localization network. In Proc. of NeurIPS
LLD Workshop, 2017.
Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. In Proc. of ICCV, 2017.
Canadian Institute for Cybersecurity. Cse-cic-ids2018 on aws. https://www.unb.ca/cic/
datasets/ids-2018.html, 2018.
Amirata Ghorbani, Abubakar Abid, and James Zou. Interpretation of neural networks is fragile.
arXiv:1710.10547, 2017.
Amirata Ghorbani, James Wexler, James Y Zou, and Been Kim. Towards automatic concept-based
explanations. In Proc. of NeurIPS, 2019.
Yash Goyal, Ziyan Wu, Jan Ernst, Dhruv Batra, Devi Parikh, and Stefan Lee. Counterfactual visual
explanations. Proc. of ICML, 2019.
10
Under review as a conference paper at ICLR 2021
Chaoyu Guan, Xiting Wang, Quanshi Zhang, Runjin Chen, Di He, and Xing Xie. Towards a deep
and unified understanding of deep neural models in nlp. In Proc. of ICML, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proc. of CVPR, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In Proc. of ICLR, 2019.
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretability
methods in deep neural networks. In Proc. of NeurIPS, 2019.
Kouichi Ikeno and Satoshi Hara. Maximizing invariant data perturbation with stochastic optimization.
arXiv preprint arXiv:1807.05077, 2018.
Yoon Kim. Convolutional neural networks for sentence classification. Proc. of EMNLP, 2014.
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schutt, Sven
Dahne, Dumitru Erhan, and Been Kim. The (Un) reliability of saliency methods. arXiv:1711.00867,
2017.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. Proc.
of ICML, 2017.
Pang Wei W Koh, Kai-Siang Ang, Hubert Teo, and Percy S Liang. On the accuracy of influence
functions for measuring group effects. In Proc. of NeurIPS, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Proc. of NeurIPS, 2012.
Alexander Levine, Sahil Singla, and Soheil Feizi. Certifiably robust interpretation in deep learning.
arXiv preprint arXiv:1905.12105, 2019.
Zachary C Lipton. The mythos of model interpretability. arXiv:1606.03490, 2016.
Yang Lu, Yingying Fan, Jinchi Lv, and William Stafford Noble. DeepPINK: reproducible feature
selection in deep neural networks. In Proc. of NeurIPS, 2018.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In Proc. of
NeurIPS, 2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal
adversarial perturbations. In Proc. of CVPR, 2017.
Weili Nie, Yang Zhang, and Ankit Patel. A theoretical explanation for perplexing behaviors of
backpropagation-based visualizations. In Proc. of ICML, 2018.
Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word
representation. In Proc. of EMNLP, 2014.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: Explaining the
predictions of any classifier. In Proc. of KDD, 2016.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International Journal of Computer Vision, 2015.
Andrew M Saxe, Pang Wei Koh, Zhenghao Chen, Maneesh Bhand, Bipin Suresh, and Andrew Y Ng.
On random weights and unsupervised feature learning. In Proc. of ICML, 2011.
Karl Schulz, Leon Sixt, Federico Tombari, and Tim Landgraf. Restricting the flow: Information
bottlenecks for attribution. 2020.
Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh,
and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localiza-
tion. arXiv:1611.07450, 2016.
11
Under review as a conference paper at ICLR 2021
Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghorbani. Toward generating a new intrusion
detection dataset and intrusion traffic characterization. In Prof. of ICISSP, 2018.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. In Proc. of ICML, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv:1409.1556, 2014.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. arXiv:1312.6034, 2013.
Sahil Singla, Eric Wallace, Shi Feng, and Soheil Feizi. Understanding impacts of high-order loss
approximations and features in deep learning interpretation. arXiv:1902.00407, 2019.
Leon Sixt, Maximilian Granz, and Tim Landgraf. When explanations lie: Why many modified bp
attributions fail. In Proc. of ICML, 2020.
Daniel Smilkov, Nikhil ThoraL Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv:1706.03825, 2017.
Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for
simplicity: The all convolutional net. arXiv preprint arXiv:1412.6806, 2014.
Pascal Sturmfels, Scott Lundberg, and Su-In Lee. Visualizing the impact of feature attribution
baselines. Distill, 2020.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In Proc.
of ICML, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv:1312.6199, 2013.
Mariya Toneva and Leila Wehbe. Interpreting and improving natural-language processing (in
machines) with natural language-processing (in the brain). In Proc. of NeurIPS, 2019.
Mengjiao Yang and Been Kim. Benchmarking Attribution Methods with Relative Feature Importance.
CoRR, abs/1907.09701, 2019.
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection
for explaining deep neural networks. In Proc. of NeurIPS, 2018.
Roozbeh Yousefzadeh and Dianne P O’Leary. Interpreting neural networks using flip points. arXiv
preprint arXiv:1903.08789, 2019.
Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In Proc.
of ECCV, 2014.
Konrad Zoina, Krzysztof J Geras, and KyUnghyUn Cho. Classifier-agnostic saliency map extraction.
In Proceedings of AAAI, 2019.
A6	Implementation details
The optimization fUnction proposed to generate decoys is non-differentiable and very difficUlt to
solve; hence, we instead solve an alternate formUlation with the help of the following tricks. First,
we introdUce a Lagrange mUltiplier λ > 0 and aUgment the first constraint in the optimization
fUnction as a penalty in the objective fUnction. This will rUle oUt the hyper-parameter in Eqn. 2.
Second, we Use projected gradient descent dUring the optimization to eliminate the mask constraint
(i.e., (X - x) ◦ (1 - M) = 0). Specifically, after each standard gradient descent step, We enforce
X = X ◦ M + x ◦ (1 - M). Third, we use the change-of-variable trick (Carlini & Wagner, 2017) to
eliminate the feature value constraint (i.e., X ∈ [xmi∩, Xmax]d). Instead of directly optimizing X, we
first normalize it to [0,1] and introduce X satisfying Xi = 2 (tanh(Xi) + 1), for all i ∈ {1, 2,…，d}.
12
Under review as a conference paper at ICLR 2021
Because tanh(Xi) ∈ [-1,1] implies Xi ∈ [0,1], any solution to X is naturally valid. It should be
noted that other transformations for this third step are also possible but were not explored in this
paper. Putting these ideas together, we minimize the following objective function:
minimizeχ - (；(tanh(X) + 1) - x) ∙ s)+	+ λ ∙ FX；(tanh(X) + 1)) - F'(x)	, (10)
1∞
where λ > 0 is initialized small and repeatedly doubled until the optimization succeeds. Because
the L∞ norm is not fully differentiable, we adopt the approximation trick introduced by Carlini &
Wagner (2017) and solve the following formulation:
2
minimize^ 一
max((- (tanh(x) + 1) — x) ∙ s, 0)	+ λ ∙
2 1
(∣F'(1(tanh(x) + 1)) - FHX)I-T) +
2
(11)
where τ > 0. In this paper, we follow the selection strategy proposed in Carlini & Wagner (2017)
and initialize τ = 1. After each iteration, if the second term is zero, then we reduce τ by a factor
of 0.95 and repeat; otherwise, We terminate the optimization. After obtaining X, We compute X and
map it back to the original feature value range [xmin , xmax]. Note that Eqn. 3 can be efficiently solved
by any first-order optimization method Without introducing too much computational overhead. In
practice, the average run time of solving it is 62.3% shorter than the fastest, vanilla gradient method.
A7 Proof of Theorem 1
Before proving Theorem 1, We first state and prove the folloWing lemma.
Lemma 1. Consider an input X and its decoy X, generated by replacing the original features with
Swappablefeatures in K, |K| = K .The partial derivative of F c(X) w,r.t. to Xi for i ∈ K is
(OxFc(X))i - 2 X(Xk - Xk)(Hχ)i,k ≤ C.	(12)
k∈K
Proof. The second-order Taylor expansion of the predicted Fc(X) for target class c around X is as
folloWs:
FC(X) ≈ Fc(X) + OxFC(X)T∆ + ∣∆τHx∆,	(13)
where ∆ = X - X. By definition of the decoys in Section 2.2 (i.e., Fc(x) = Fc(X)), the following
equation holds:
OxF c(X)t ∆ ≈- ∣∆τ Hx∆ .	(14)
From the above equation, we can see that, for a linear model, the linearity zeroes out the gradient of
the decoys, causing our method to output zero saliency scores for all input features. We clarified in
Section 3.4 that our method is mainly defined for non-linear complicated models.
Given a swappable patch of size K X 1 starting from position iι, then ∆ = [0,…，x^ - X”，…，XiK -
XiK, 0,..., 0]. As such, we have
OxFc(X)t∆ = X(OxFc(X))i(Xi - Xi),
i∈K
∆τHx∆ = X(Xi- Xi) X(Hx)i,k(xk - Xk).
i∈K	k∈K
Plugging Eqn. (15) into Eqn. (14), we have
X[(Ox F c(X))i + 1 X (Hx)i,k(χk - Xk)](χi - Xi) = 0.
i∈K	k∈K
Then we can derive
(OxFC(X))i + 1 X(Xk - Xk)(Hx)i,k ≤ C,
k∈K
(OxFC(X))i - 1 X(Xk- Xk)(Hx)i,k ≤ C.
k∈K
(15)
(16)
(17)
13
Under review as a conference paper at ICLR 2021
First, We can derive |Xi - x/ is bounded by 2max(xmaχ, ∣Xmin∣). We also have ∣Xi+k - Xi+k| 0
in that we can always find a small perturbation to each feature in X such that ∣∣FXX)-
F'(x)k∞ ≤ e. In addition, both gradient and Hessian are bounded by some Lipschitz con-
stant (Szegedy et al., 2013). 4 As a result, we can always find a constant C, such that C ≥
-Pk1∈κ"(0 文 F c(X))kι+1 Pk2 ∈k(H )k1,k2 (xk2-Xk2 )](xkι-Xkι)
(Xi-Xi)
For the case K = 1, we have
(OXFC(X))i = 1 (HX)i,i(Xi- Xi).

Now we prove Theorem 1 from Section 3.5.
Consider a CNN with L hidden blocks, with each layer ` containing a convolutional layer with a filter
of size √S X √S' and a max pooling layer with pooling size √S' X √S'. The input to this CNN is
x ∈ Rd, unrolled from a √d X √d matrix. Similarly, we also unroll each convolutional filter into
g` ∈ Rs', where g` is indexed as (g` j for j ∈ j` Here, j` corresponds to the index shift in matrix
form from the top-left to bottom-right element. The output of the network is the probability vector
p ∈ RC generated by the softmax function, where C is the total number of classes. Such a network
can be represented as
m` = pool(relu(g` * m`-i)) for ' = 1, 2,3,…,L,
o = WLT+1mL + bL+1,	(18)
p = softmax(o) ,
where relu(∙) and pool(∙) indicate the ReLU and pooling operators, m` ∈ Rd' is the output of the
block ' (mo = x), and (g` * m`-i) ∈ Rd'-1 represents a convolutional operation on that block.
Consider an input X and its decoy X, generated by swapping features in K. For each feature i ∈ K,
we have the following theorem for the decoy-enhanced saliency score Zi :
Theorem 1. In the aforementioned setting, Zi is bounded by
Zi- 2 X(X+ - X-)(Hχ)k,i ≤ C1.	(19)
k∈K
Proof. The gradient of pc with respect to X can be written as follows, using the denominator layout
notation of the derivative of a vector:
O	YL ∂m`	∂o ∂pc
xP。-U dm'-ι dmL 而,
(20)
where
∂ o _
∂mL = WL+1,
(21)
and
∂ dPc.
d ∂oc0
∂ dPc
I dθc0
Then we can write dpcas follows:
(pc - pc2 )	if c0 = c ,
-pcpc0 otherwise .
(22)
dPc	P
=	= p∙c ,
∂o
where P∙c corresponds to the c-th column of P and P = diag(p) -PPT. We then define b`
as b` ∈ Rd'-1×d'. In the following, we compute b`.
First, we can have
(23)
dm`
dm`-i
d	d(m'j
∂ ∂(relu(g'*m'-ι))n
]	d(m'j
I ∂(relu(g'*m'-ι))n

1 if j - n ∈J', and n = argmaxno∈j+j' (g` * m`-i)n0,
0 otherwise ,
(24)
4Following other works that also utilized Lipschitz continuity to analyze DNNs (Szegedy et al., 2013;
Ghorbani et al., 2017), we assume that f` is locally continuous around x, for ' = 1, 2,…，L.
14
Under review as a conference paper at ICLR 2021
1	ʌ	.	. 1	.	C∙ . 1	1 ∙	.	1	1	/	∖	1 ∙	1	1 .	∙	/	∖ El
where j represents the center of the pooling patch in relu(g` * m`-i), which results in (m`)j. Then
we can compute
( MX- 1))n = (a`)n(g')n-i if n - i ∈J' ,
I d(rel∂⅞*m'-1))n =0 otherwise,
(25)
where (a`)n = 1 {(relu(g' * m`-i)n) ≥ 0}. If we change the activation function to either sigmoid
or tanh, then (a`)n in Eqn. 25 will be replaced with the derivative of either function. For the sigmoid
activation function σ(x), the derivative is σ(x)(1 — σ(x)), with a range of [0, ɪ]. For the tanh
activation function tanh(x), the derivative is 1 - tanh(x)2, with a range of [0, 1]. We conclude that
the derivative of both sigmoid and tanh are bounded by a value no larger than 1.
Combining Eqn. 24 with 25, we have
((B')ij	=	d(mmm-,j)i	=	(a`)n(g')n-i	if n - i ∈	J',	j - n ∈J',	and n = argmax^∈j+j'(g` *	m`-i)nθ	,
∖ (B')ij=d(mm-,j)i=0 otherwise.
(26)
For simplicity, we rewrite the non-zero condition as n ∈ j`. Plugging b`, ' = 1,…，L, into Eqn. 20,
we can obtain the partial derivative Oxpc.
Further, we compute each element in the Hessian matrix Hij as follows:
Hij = Oxi (Oxj pc)
∂(Q3 B')j∙Wl+iP∙c
∂Xi
L
(Y B')j∙WL+ι
'=1
dL	L
X (Y B')jnL (WL+1)
nL=1'=1
nL∙
ʌ更
∂ ∂Xi
(27)
and
„ ʌ ，，. . . _ .
∂PCC = (	(1 - 2pc)θχiPc	if C = c,
∂xi	I PcOxi Pco + Pco Oxi pc	otherwise.
(28)
(29)
(30)
Now we compute (QL=I B'j” as
L	L-1
(Y B')jnL =(Bι)j∙ Y B'(BL)∙nL ,
'=1	'=2
where
(B1)j∙B2 = [0,...,Cn2(a2)n2 E (a1)ngn-1, ..., 0] ,
n1∈J1
and where C,2 = (g2)n2-2 Pnl∈j gn-ι. Here, we redefine JL as the set of indices such that
(B1)jn1 = 0 for nι ∈ Ji. As such, we can compute (Bι)j∙ QL=-I B' as
L-1	L-2
(Bi)j∙ Y B' = [0,..,CnL-1 (aL-i)nL-1 X X (a`)n`,…,0] .	(31)
'=2	'=1 n'∈J'
Plugging Eqn. 31 into Eqn. 29, we have
L	L-1	L-1
(Y B')jnL = (Bl)j∙ Y B'(Bl)∙,l = (CcnL (aL)nLXX
(a')n`.	(32)
'=1	'=2	'=1 n '∈J'
Plugging Eqn. 32 into Eqn. 27, we have
H
Hij
CjX nN'(T M
(33)
where	Cj	is a linear combination of	g1,	...,	gL,	WL+1,	which is bounded.	Hij	equals the
multiplication of two components—the summation of neurons activated by x and a gradient
15
Under review as a conference paper at ICLR 2021
∂∂Xc. Given the total number of neurons in a CNN is a constant (denoted by CT), we have
0 ≤ (Pi=I Pnc∈力(ɑ`)nj ≤ Ct. Then, we have ∣(Hχ)ij∙| ≤ CTICjIPf |. Since the deriva-
tives of both sigmoid and tanh are no larger than 1, this inequality also applies to the network
with these two functions as the activation function. Similarly, for the Hessian (Hχ )j of a decoy
nŋ
x, We also have ∣(Hχ)j ≤ CTICjdx∙c∣. Given the inequality of (Hχ)j and (Hχ)j, we can
.. ^ ^ ^
obtain that ∣(Hχ)j - (Hχ)j∣ ≤ 2Ctmax(∣Cjd∂χ-c∣, ∣CjdXc)∣, where 舒 is given by Eqn. 28.
Recalling that PC is within [0,1], the gradient ∣χc is bounded by some Lipschitz constant (Szegedy
Λ^
et al., 2013), we can obtain that dpc is bounded by some constant. Finally, we can derive that
∣(Hχ )j-(Hχ)j ∣ ≤ CC, where CC represents the upper bound.5
Now, we derive the decoy-enhanced saliency score Zi for x^, given a population of saliency scores
Ei = {E(X1; F)i,E(X2; F)i,…，E(X2n; F)i}. Let X+, X- ∈ {x1, X2,…，X2n} denotes the
decoy which maximizes and minimize E(X; F)i, respectively. According to Lemma 1, the partial
derivative ▽文-PC has the following relationship
(OXFc(X))i - J X(Xk- Xk)(Hχ)i,k ≤ C,	(34)
,k∈K
Then, we can derive
2 X(X + - Xk)(Hχ+ )i,k - C ≤ (Oχ+ Fc(X+))i ≤ 1 X(X + - Xk)(Hχ+ )i,k + C, (35)
k∈K	k∈K
-2 X (X- - Xk )(Hχ- )i,k - C ≤ -(Oχ- FC(X-))i ≤ - 2 X (X- - Xk)(Hχ- )i,k + C , (36)
4k∈K	乙k∈K
Then, we have
Zi = (Oχ+ Fc(X+))i - (Oχ-Fc(X-))i
≤2 X(X+ - Xk)(Hχ+ )i,k - 2 X(X- - Xk)(Hχ- )i,k + 2C
k∈K	k∈K
≤1 X(X+ - Xk)((Hχ)i,k + CC) - 1 X(X- - Xk)((Hi- )i,k - CC) + 2C (37)
4k∈K	乙k∈K
≤ 1 X(X+ - X-)(Hχ)i,k + 1 Cc X(X+ - X-) + 2C,
4	k∈K	k∈K
And
Zi = (Oχ+ Fc(X+))i - (Oχ-Fc(X-))i
≥	2 X (X+ - Xk)(Hχ+ )i,k - 2 X (X- - Xk )(Hχ- )i,k - 2C
k∈K	k∈K
≥	2 X (X+ - Xk)((Hχ)i,k - cC) - 2 X (X- - Xk)((H文-)i,k + cC) + 2C (38)
4k∈K	乙k∈K
≥	2 X (x+ - ^^-)(Hχ)i,k -1CC X (x+ - x-) - 2c,
k∈K	k∈K
Combining Eqn. 37 with Eqn. 38, we have
Zi- 2 X (X+ - X-)(HX)k,i ≤ C1 ∙	(39)
,k∈K
5Note that this inequality cannot be directly obtained by the Lipschitz inequality, because the gradient may
not be continuous.
16
Under review as a conference paper at ICLR 2021
Recall that (X + - X-) is bounded by a upper-bound, We can obtain that there exist a constant Ci,
such that C1 ≥ 2Cc Pk∈κ(X+ -X-) + 2C. Note that this upper bound is data specific, and we
leave the exploration on its tightness as a part of future Works.
A8	Proof of Proposition 1
Proposition 1. Given an input X and its corresponding adversarial sample X, if both
∣Xi 一 Xi | ≤ C2δi and IXi 一 Xil ≤ C2δi can obtain where C2 > 0 is a bounded constant and
δi = ∣E(X, F )i — E(x, F )i∣, then the following relation can be guaranteed.
∣(Zχ)i-(Zχ)i∣ ≤ ∣(E(X,F)i- E(x,F))i∣.	(40)
Proof. Recall the goal of the attack against saliency maps is to subtly perturb an input sample such
that the added perturbation does not change the output of the classifier (Ghorbani et al., 2017) but
force a saliency method to output a less meaningful saliency map (i.e., highlighting features that are
irrelevant to the classifier prediction). To achieve this goal, when generating an adversarial sample X
from the given input x, an attacker needs to impose the following constraint kX - x∣∣∞ ≤ e. Suppose
we have an adversarial sample X satisfies this constraint. Then, we can assume (X - x)i = ^i, where
怕i∣ ≤ e, for i = 1,2,..., d. In addition, we can compute saliency maps E(X, F) and E(x, F) for X
and X by using an existing saliency method. 6 Given both saliency maps, we can further compute the
difference between E(X, F) and E(x, F) as
d
(E(X, F) - E(x, F))i = OxFc(X) - OxFc(x) = (Hχ(X - x))i = X(Hx) jj	(41)
j=1
Based on the Eqn. 2 in Section 3.3, when generating the decoys X, we ensure the classifier's
predictions for those decoys are as same as that of the X. In this work, we achieve this by bounding
the difference between the hidden representations of X and x. As is discussed in Section A7, to
preserve the same prediction C for X and x, one has to ensure ∣Fc(X) - Fc(x)∣ is bounded. This
implies the difference between X and X is bounded within e. Here, Wi represents the maximum
difference between Xi and Xi at the ith dimension. As is mentioned above, the adversarial sample X
does not change the classifier,s prediction. Therefore, we could imply ^i ≤ ei, for i = 1, 2,…,d.
Now, suppose we obtain a set of decoys for X and have their corresponding saliency maps, i.e.,
{E(X1; F )i,E (X2; F )i,∙∙∙ ,E(X2n; F )i)}.Let X+ ∈ {X1, X2,…，Xn} denote the decoys which
maximize E(X; F)i and let X- denote the decoys which minimize E(X; F). Similarly, we can also
have the corresponding decoys X and X for the adversarial sample X as well as their corresponding
saliency maps. With both the decoys and saliency maps for the input sample X and its adversarial
sample X, we can compute the difference between (Zx) and (Zx)i as
(Z^)i - (Zx)i
=(E(X+,F)i - E(X-,F)i) - (E(X+, F)i - E(X-, F)i))
=((Hx(X+ - x))i - (Hx(X- - x))，一((Hx(X+ - x))i - (Hx(X- - x))i)	(42)
d
=X(Hx)ij ((x+ - x-)-(X + - x-)).
j=1
6For simplicity, we use the vanilla gradient method. The conclusion can be generalized to the other saliency
methods considered in this paper
17
Under review as a conference paper at ICLR 2021
To guarantee an improvement in robustness against the adversarial perturbation, we have to ensure
that ∣(Z^)i - (Zχ)i∣ - |(E(x, F) - E(x, F))i| ≤ 0, for i = 1,2…，d. That is,
d	d
X(HX)j ((X + - x-) - (x+ - x-)) - X(HX)j$ ≤ 0,
j=i	j=i
d	d
X(Hx) j ((x + - x-) - (x+ - x-)) ≤ X(Hχ)g ,
j=i	j=i
As is discussed in Section A7, ∣(Hχ) j ∣ ≤ CC. With this, we can have
d
X(HX) j ((x+ - x-)-(x+ - x-))
j=i
(43)
d
≤ X ∣(Hχ)ij ∣∣ (x+ - x-) - (x+ - x-) I	(44)
j=i
d
≤ X 4∣(x+ - x-)-(x+ - x-) I
j=i
By plugging Eqn. 44 into Eqn. 43, we conclude that as long as I(X+ -x-)-(X+ -x-)∣ ≤
CTd I Ed=I(HX)j^j I , our method could guarantee to improve the robustness against the adversarial
perturbations. Let & = ∣E(x, F)i - E(x, F)i∣. If we can ensure that ∣xi - x/ ≤ ZC& and
C
i xi - xi i	≤ 4⅛δi,	we can have	i x+	- x- I	≤	2CCdδi	and	i x+	- x- i	≤	2⅛δi.	ThUs,	the
aforementioned condition can be satisfied, i.e., I(x +- x-)-(x+- x-) i ≤ Cdδi. By setting
C2 = zCkd, we could obtain the robustness conditions in Proposition 1.
□
A9 Corollary 1
Consider a multilayer perceptron with L fully-connected hidden layers and a decoy swappable size
K × 1. The input of this MLP is X ∈ Rd. For each hidden layer, we use the ReLU activation function.
Similar to the CNN mentioned above, the output of this CNN is P ∈ RC. The network can be
represented as:
m` = relu(WTm`-1 + b`), For ' = 1,3,…,L,
O = WT+1mL + bL+1,	(45)
p = softmax(o).
where w` ∈ Rd'-1 ×d', for ' ∈ {1, ∙∙∙ , L + 1} represents the weights of the neural network, and
b` ∈ Rd' represents the biases, where d0 = d and dL+1 = C. m` ∈ Rd' is the output of each hidden
layer, with mo = X and o ∈ Rc is the logits. The entry-wise softmax operator for target class C is
defined as PC = PCeOCeO/,for C ∈ {1, 2,…，C}.
Corollary 1. For the above MLP, Zi is also bounded by:
Zi ≤ 2 ^X (xi+k - xi+k )(Hχ)i+k,i + C2 ∙	(46)
,k∈K
Proof. Based on the proof of Theorem 1, the gradient of PC with respect to X can be written as
follows
L
OχPc = Y b`wL+1P∙c .	(47)
l = 1
18
Under review as a conference paper at ICLR 2021
where b` =用*,B' ∈ Rd'-1×d'. P七 is also defined as P = diag(p) - PPT. In the following,
d^m' — 1
we compute Bl. First, we can compute (B1)ij, in which
(R)	d (ml)j	d(WT X + b1)j	d(m1)j	(W)	(4G
(B1)ij = Fr = —∂Xi— ∂(WTX + bι)j = (WI)ijMj,	(48)
where (aι)j = 1{(WTX + bι)j ≥ 0}. Similar, We can also compute (B')ij, for ' = 2,3,…，L
(B')ij = (We)ij (a`)j,	(49)
where (a')j = 1{(W'T X + b')j ≥ 0}.
Then, we compute the each element in the Hessian matrix Hij . Specifically, based on Eqn. 27, we
have
(dL	L	∖ ∂P^
Hij =	X (YB')jnL(W
L+1 )nL J -∂Xc ,
(50)
^ nŋ
where ∂pχ∙c is the same with Eqn. 28.
Now, we compute (Q'L=1 Bl )jnL as
L	L-1
(Y B')jnL = (Bι)j∙ Y B'(BL)∙nL ,	(51)
'=1	'=2
where (Bι)j∙ = [(Wi)jι(aι)ι, (W1)j2(a1)2,..., (W1)jd1 (aι)dj and
d1	d1
(B1)j∙B2 = [(a2)1 E(C2)in1 (a1)n1,…,(a2)d2 E(C2)d2,n1 (a1)n1 ],
n1 =1	n1 =1
(52)
where (C2)n2,n1 = (W2)n1,n2(W1)j,n1. For simplicity, we can rewrite Pdn11=1(C2)n2,n1(a1)n1
(C2)n2 Pdn11=1(a1)n1. Then, we have
d1	d1
(B1)j∙B2 = [(C2)1(a2)i E(ai)nι ,…,(C2)d2(a2)d2 E(aι)n]	(53)
n1 =1	n1 =1
As such, we can compute (Bι)j∙ QL-21 B' as
L-1	L-2 d`	L-2 d`
(B1)j∙ ∏B' = [(CL-i)1(aL-i)1ΣΣ(a' )n` , ..., (CL-1 )dL-1 (aL-1 )dL-1	(a')n`].
'=2	'=1 n'=1	'=1 n' = 1
(54)
Plugging Eqn. 54 into Eqn. 23, we have
L	L-1	L-1 d`
(	B')jnL = (BI)j∙ H B'(BL)∙nL = (CL)nL (aL)nL E E (aQn' ∙
'=1	'=2	'=1 n'=1
Finally, we can obtain that
(55)
H
Hij
dL	L-1 d`
E (CL)nL (aL)nL E E (a`)n` (WL+1)nL∙
nL = 1	'=1 n' = 1
L	d`
CjXX (a`)n`
.	'=1 n'=1
(56)
where Cj is a linear combination of the elements in (W1)j∙, W2,..., WL+1.
Note that the Hessian derived from the MLP has a similar form with the Hessian derived from the
CNN in Eqn. 33, i.e., the summation of neurons activated by X multiplying the gradient. Here,
the summation of neurons activated by X is again bounded by the total number of neurons in the
19
Under review as a conference paper at ICLR 2021
,	IEl	「	，∂P 一 ♦	1	111	τ ∙	,	Γ..	∙1	1	1	.1	11
network. The gradient ∂pχ∙c is bounded by a LiPschitz constant. Similarly, We also have the following
inequality for (Hx Iij and (Hχ)j, i.e., ∣(Hχ Iij-(HxIij | ≤ CM.
Similar to Theorem 1,let X+, X- ∈ {X1,X2, ∙∙∙ ,X2n} denotes the decoy which maximizes and
minimize E(X; F)i, respectively. Based on Eqn. 34 to Eqn. 39, we have
Zi- 1 X(X+ - X-)(Hχ)k,i ≤ C2.	(57)
k∈K
C2 ≥ 2CM Pk∈κ(X+ - X-) + 2C. Slightly different for CNN, MLP sometimes is used to process
the input that does not have a strong local dependency. In this case, we can set the swappable path
size K = 1. Then, Eqn. 57 can reformulated as IZi - 11 ∣(X + - X-)(Hχ)i,i∣∣ ≤ C2. As we can
observe from this equation, our proposed saliency score is still able to compensate for the gradient
saturation problem.
Table A1: The hyper-parameter choices of the proposed method on different target models.
	`	λ	patch_size (P)	stride	τ
ImageNet AlexNet	6	10000	3	1	1
ImageNet VGG16	3	10000	3	1	1
ImageNet ResNet	2	10000	3	1	1
SST CNN	2	10000	1	1	1
IDS MLP	2	10000	1	1	1
A10 Datasets and experiment setup
In this section, we introduce the datasets used in our experiments and the neural network trained on
each dataset, followed by our choices of hyper-parameters when explaining each model.
ImageNet. We randomly select a subset of samples from the ImageNet validation set, which
can be downloaded from the following link: http://www.image-net.org/. We adopt the
most widely used preprocessing method for the selected images. Specifically, for each image,
we resized it to 227 × 227, converted it to BGR format, and subtract the mean value of each
channel [103.939, 116.779, 123.68] from the image. Rather than training our own networks, we
downloaded a pretrained VGG16 model, AlexNet model, and ResNet_v1_50 model from the fol-
lowing link: https://github.com/tensorflow/models/tree/master/research/
slim and http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/. We applied our
proposed method to explain the predictions of these networks on the selected samples.
SST. We downloaded the Stanford Sentiment Treebank (SST1) from the following link: https:
//github.com/harvardnlp/sent-conv-torch/tree/master/data. The data is
spited into a training set of 76, 961 samples and a testing set of 1, 821 samples. We used a pre-
trained glove embedding to represent each word in the sentences (sample). The embedding of each
word is a vector of 100 dimensions. The pretrained embedding matrix can be downloaded from
the following link: http://nlp.stanford.edu/data/wordvecs/glove.6B.zip. We
trained a two-layer CNN with the embeddings as inputs. The model achieves about 80% accuracy on
the testing set. The preprocessed testing data and the pretrained model can be downloaded from the
following link: https://tinyurl.com/y9noqj6l. We run our explanation method on the
pretrained model with the testing samples.
Network intrusion detection (IDS). We use a subset of CSE-CIC-IDS2018 dataset (Sharafaldin
et al., 2018; for Cybersecurity, 2018), a network intrusion dataset contains the benign network traffic
traces and malicious traces generated by three types of attacks: Denial of Service (DoS)-Hulk, SSH-
BruteForce, and Infiltration. The training set contains 88, 661 samples and the testing set has 22, 165
samples. Each sample is represented as a vector of 83 dimensions, where each feature represents the
statistics of network traffic flows (e.g., Number of packets, Number of bytes, Length of packets, etc).
The features are normalized within [0, 1] by using the scikit-learn MinMaxScaler function.
We trained a two-layer MLP to classify whether an input is a benign traffic or an attack (intrusion).
20
Under review as a conference paper at ICLR 2021
Figure A5: Cascading randomization on VGG16 network. The figure shows the original saliency map
(first column) for the terrier. Progression from left to right corresponds to complete randomization
of the pretrained VGG16 network weights from the top layer to the bottom layer. Note that, here,
we followed the visualization method in Adebayo et al. (2018) to show the saliency maps, i.e., 0-1
normalization. The row labels share the same meanings as the column labels in Fig. 2.
Figure A6: Structural similarity index (SSIM) for Cascading Randomization on VGG16 network.
Note that the legends have the same meaning as the column labels in Fig. 2.
The model reaches 99% accuracy on the testing set. After training the model, we randomly sampled
a subset of 2, 000 testing samples and used our method to derive explanations from the model
predictions of samples in this subset. The dataset, model, and the descriptions of each feature can be
found in https://tinyurl.com/y9noqj6l.
Hyper-parameter choices. The hyper-parameter choices of the proposed method on three datasets
are shown in Table A1. In the table, ` is the index of the layer within the target model that is selected
to generate the decoy images. The Lagrange multiplier λ controls the weight of ∣∣F'(X) - F'(x) k∞.
The patch_size and stride control the size and the stride step of each decoy patch. τ is introduced by
Eqn. 3 in Section A6. Note that we set the swappable patch size of SST and IDS data as 1, because
their features may not have a strong local correlation. It should also be noted that we selected the
swappable patch size of ImageNet data as the widely used convolutional kernel size 3 and stride
size 1. We set the number of patches (masks) in each decoy m as 100 for ImageNet, 1 for SST and
IDS. When generating adversarial attack images, we applied the code released by the corresponding
work (Ghorbani et al., 2017) and followed their default setup in our implementation.
A11 Sanity check for decoy-enhanced saliency maps
As suggested by Adebayo et al. (2018), any valid saliency methods should pass the sanity check in the
sense that the saliency method should be dependent on the learned parameters of the predictive model,
instead of edge or other generic feature detectors. We performed the model parameter randomization
test (Adebayo et al., 2018) on the ImageNet dataset by comparing the output of the proposed saliency
method on a pretrained VGG16 network with the output of the proposed saliency method on a
weight-randomized VGG16 network. If the proposed saliency method indeed depends on the learned
parameters of the model, it is expected that the outputs between the two cases differ substantially.
Following the cascading randomization strategy (Adebayo et al., 2018), the weights of pretrained
VGG16 network are randomized from the top to bottom layers in a cascading fashion. This cascading
randomization procedure is designed to destroy the learned weights successively. As illustrated in
Fig. A5, the cascading randomization destroys the decoy-enhanced saliency maps combined with
three existing saliency methods, qualitatively. The conclusion is also supported by quantitative
comparison measured by the structural similarity index (SSIM), shown in Fig. A6.
21
Under review as a conference paper at ICLR 2021
Gradient Gradient Gradient IntGrad IntGrad IntGrad SGrad SGrad SGrad
w/o decoy w/ decoys difference w/o decoy w/ decoys difference w/o decoy w/ dec
Bustard SF: 4.21 SF: 0.40	SF: 0.007 SF: -0.009	SF: 0.25 SF:
(a) Saliency maps generated on AlexNet.
(b) Saliency maps generated on ResNet.
Figure A7: Visualization of saliency maps under different CNN architectures. Here, the column
labels are as same as those in Fig. 2. The difference figures share the same colorbar as those in Fig. 2.
Table A2: Quantitative comparison of our method and baselines on the network intrusion dataset. We
report the means and standard errors of the fidelity scores.
Salinecy method	Fidelity (SF)				
	Without deocy	Decoys With range	Constant with range	Noise with range	Decoys with mean
Gradient	1.80 ± 0.39-	1.64 ± 0.40	1.68 ± 0.40	-1.78 ± 0.43	2.04 ± 0.40
IntegratedGrad	1.68 ± 0.39-	1.57 ± 0.40	1.68 ± 0.44	-1.79 ± 0.43	2.19 ± 0.39
SmoothGrad	1.59 ± 0.39-	1.57 ± 0.40	1.74 ± 0.44	1.73 ± 0.44~~	1.87 ± 0.45
A12 Applicab ility to other CNN architectures
In addition to the VGG16 model, we generated saliency maps for AlexNet (Krizhevsky et al.,
2012) and ResNet (He et al., 2016) trained from the ImageNet dataset. We visualize their saliency
maps in Fig. A7. We observe that our method consistently outperforms the baseline methods, both
quantitatively and qualitatively. Together with the results in Section 4, these results suggest that we
can apply our decoy-enhanced saliency methods to various feed-forward network architectures and
expect consistent performance.
A13 Performances on the network intrusion dataset.
Rather than visualizing the saliency scores through heatmaps, we apply the following to compare the
saliency scores obtained by different methods qualitatively. We ranked the features based on their
saliency scores and compared the ranking obtained by the existing methods with that obtained by
our decoy-enhanced method. “Minimum size of packet in forward direction”, “Minimum length of
a packet”, “Minimum time between two packets sent in the forward direction” are ranked higher
by our methods than the baselines. These features could capture the differences between benign
and malicious traffics. This is because attackers usually tend to rapidly send small packages to
discover the backdoors in the victim network system, while the benign users may send much larger
packages with a longer interval between two packages. On the contrary, features that are not that
useful for intrusion detection (e.g., timestamp, Download and upload ratio) are wrongly pinpointed
by the existing method. However, our methods correctly assign lower importance to these features.
Table A2 shows the fidelity comparisons of different saliency methods. We can observe that our
decoys-enhanced methods outperform the original saliency methods. These results show that our
method could pinpoint more accurate features and achieve a higher fidelity than baselines. We
also evaluated three alternatives used in Section 4: constant perturbation with range aggregation,
noise perturbation with range aggregation, decoys generation with mean aggregation. The results in
Table A2 are consistant with those in Fig. 2 and Fig. 3, i.e., our method outperforms these baselines.
In summary, the results on this dataset align with those on the other datasets. This confirms our
method’s applicability to multilayer perceptrons.
A14 Decoys on Other Baselines.
In Section 4, we evaluated our methods on three state-of-the-art saliency methods. Recent re-
search (Sturmfels et al., 2020; Hooker et al., 2019) suggests some variants that improve the perfor-
22
Under review as a conference paper at ICLR 2021
Figure A8: Visualization of saliency maps obtained by original saliency methods and our decoy-
enhanced versions. “ExpGrad” refers to Expected Gradient, “SGradRage” stands for Smoothgrad
with range aggregation, and “IntUniform” represents integrated gradient with uniform baseline. The
difference figures share the same colorbar as those in Fig. 2.
⅛⅛⅛⅛⅛
ExpGrad VarGrad SGradRange IntUniform GradCAM
Without decoy
With decoys
Figure A9: Fidelity comparision of saliency maps obtained by original saliency methods and our
decoy-enhanced versions. “ExpGrad” refers to Expected Gradient, “SGradRage” stands for Smooth-
grad with range aggregation, and “IntUniform” represents integrated gradient with uniform baseline
(See Tab. A7 for more statistics about the performance differences).
mance of these baseline methods. Here, by using ImageNet data, we evaluate whether our decoy
method could further improve these variants and another widely used saliency method. Specifically,
we consider two variants of the integrated gradient: integrated gradient with uniform baseline (Sturm-
fels et al., 2020) and Expected Gradient (Sturmfels et al., 2020); two variants of the SmoothGrad:
VarGrad (Hooker et al., 2019) and Smoothgrad with range aggregation; and one existing saliency
method: Grad-CAM (Selvaraju et al., 2016). For the variants of the integrated gradient and Smooth-
Grad, we kept the number of samples the same as the original version and used the default number
suggested by existing works - 25 (See https://github.com/PAIR-code/saliency). We
will investigate whether increasing the sample numbers improve the existing saliency methods’
fidelity and robustness in future work.
Fig. A8 and Fig. A9 shows the qualitatively and quantitatively comparison of each method
with/without decoys. As is depicted in Fig. A8, our method helps knock off the noises and im-
prove the visual quality of the saliency maps. Fig. A9 further demonstrates the advantage of our
method in explanation fidelity. Together with the results in Section 4, they demonstrate the generaliz-
ability of our technique to different saliency methods. Note that our method only imposes a minor
improvement on Grad-CAM both qualitatively and quantitatively. As part of future work, we will
explore how to customize our method for Grad-CAM and investigate the effectiveness of applying
our technique to more saliency methods.
A15 Runtime of Decoy Generations
To evaluate the computational cost of our decoy generations, we carried out the run time comparison
between optimizing one decoy and calculating three types of saliency methods, repeated 500 times
with respect to different patch masks. As illustrated in Fig. A10a, on average, optimizing one decoy
is 62.3% faster than the fastest vanilla gradient-based saliency method. For other methods, the
optimization is even less expensive, in a relative sense.
As is mentioned in Section 3.4, our decoy generation, and the saliency map computation can be run
parallelly in a batch mode. In the optimal case, where we have enough resources to compute each
23
Under review as a conference paper at ICLR 2021
1 SPU83-- 30J'=6wuun≈*
SPUOKS C-φE4
IO1
Grad	IntGrad SGrad
(a)	Run time to optimize one decoy and calcu- (b) Run time to compute saliency maps with and
late saliency map with the existing methods. without optimizing one decoy.
Figure A10: Run time of decoy generation. The comparison is conducted in the same CPU/GPU
to ensure fairness. Note that “Grad”, “IntGrad”, and “SGrad” stands for the vanilla gradient, the
integrated gradient, and the SmoothGrad, respectively.
Λ=10	Λ=100 入=1000 A=10000	人=100000	入=10	入=100	入=1000	Λ=10000 入=100000	入=10	Λ=100	Λ=1000 Λ=1OOO0	λ=100000
口口口口口 Ξ0000 □□□□□
SF: 2.30 SF: 2.36 SF: 2.29 SF：2.44 SF：2.30 SF: 0.37 SF: 0.32 SF: 0.38 SF: 0.34 SF: 0.39 SF: 0.38 SF: 0.35 SF: 0.37 SF: 0.34 SF: 0.39
Gradient w/ decoys	IntegratedGrad w/ decoys	SmoothGrad w/ decoys
Figure A11: Visualization of saliency maps optimized using different initial λ.
decoy Saliency map (i.e., E(xi; F)) Parallelly, the overall runtime of generating one decoy-enhanced
saliency map is the total time of generating one decoy and computing one saliency map using the
existing methods. Fig. A10b shows the comparisons between the optimal decoy-enhanced saliency
map generation time and the original saliency map generation time across three saliency methods.
As we can observe from the figure, our method introduces negligible computational overhead over
the existing methods. Taking a step back, when the users have limited resources for running the
decoy-enhanced saliencies in a fully parallel fashion. As is shown in Fig. 2, our method is not
sensitive to the variations in the number of generated decoys. More specifically, Fig. 2 shows that we
can obtain a decent performance by only solving 16 decoys on the ImageNet dataset. In the worst-case
scenario where a user cannot run decoy generation in parallel, our method’s computational overhead
over the baselines is 24X for the Gradient approach, 16 X for the Integrated gradient method, and 16
X for the SmoothGrad method. In most cases, where users could afford partial parallel computing,
this overhead will be decreased linearly with the available computational resources. For example, if a
user has 4 GPUs, the overhead will drop to 6X for Gradient, 4X for Integrated gradient, and 4X for
SmoothGrad. We argue that for an ensemble method, this overhead is acceptable. Besides, saliency
generation is much lighter weight than training deep neural networks. Even with 4X〜6X overheads,
the time of computing saliency maps is still much less than network training. In addition, our method
can be even faster on more powerful machines, which escalates the practicality of our method.
A16 Hyper-parameter sensitivity
We also conduct experiments on the VGG16 to understand the impact of hyper-parameter choices on
the performance of our optimization-based decoy generation method. Specifically, we focus on the
choice of three hyper-parameters: network layer `, initial Lagrange multiplier λ, and patch size.
Accordingly, we first varied the value of ` for VGG16 and compared the differences of the generated
decoy saliencies from the three aforementioned saliency methods. In particular, we set it to range from
the first convolutional layer to the last pooling layer and demonstrate the generated decoy saliencies
in Fig. A19. Note that according to our design, only the convolutional layers and the pooling layers
can be used to generate decoy images. For each saliency method, Fig. A19 demonstrates that the
decoy saliencies generated from different layers for the same image are of similar qualities. Fig. A19
also shows the mean and standard derivation of the SF scores for each saliency method. These
quantitative results also support the conclusion that our approach is not sensitive to the layer. This
is likely because, as previous research has shown (Chan et al., 2015; Saxe et al., 2011), the final
classification results of a DNN are not highly related to the hidden representations. As a result,
24
Under review as a conference paper at ICLR 2021
P = 3 P = 5 P = 7 P = 9 P = 11 P = 3 P = 5 P = 7 P = 9 P = 11 P = 3 P = 5 P = 7 P = 9 P = 11
SF: 2.44 SF: 1.81 SF: 2.51 SF: 2.35 SF: 2.91 SF: 0.34 SF: 0.30 SF: 0.29 SF: 0.38 SF: 0.32 SF: 0.34 SF: 0.40 SF: 0.38 SF: 0.28 SF: 0.38
Gradient w/ decoys	IntegratedGrad w/ decoys	SmoothGrad w/ decoys
Figure A12: Visualization of saliency maps optimized using different patch size P .
Without decoy
Decoysw/range aggregation
ConstantwZrange aggregation
Noise w/range aggregation
Decoys w/mean aggregation
(a) Fidelity comparison when selecting top 10% features on ImageNet.
20
O
Φ1O
P
⅛⅛i≡i
O
Grad	IntGrad	SGrad
Without decoy
Decoysw/range aggregation
ConstantwZrange aggregation
NoisewZrange aggregation
Decoys w/mean aggregation
(b)	Fidelity comparison when selecting top 40% features on ImageNet.
Figure A13: Fidelity comparison of our methods and baselines under different choices of K (See
Tab. A8 and A9 for more statistics about the performance differences).
generating decoy saliencies for the same sample with the same label from different layers should
yield similar results.
We also varied the initial Lagrange multiplier λ to be 101, 102, 103, 104, 105 and compared the
differences of the generated decoy saliencies. Fig. A11 depicts the quantitative and qualitative
comparison results. As shown in the figure, the different choices of initial λ all produce similar
saliency maps, indicating a negligible influence upon our method.
Then, we fixed m and increased the patch size to be {3, 5, 7, 9, 11} and showed the generated decoy
saliencies in Fig. A12. The results show that varying the patch size within a certain range only
imposes a negligible influence upon our method.
Recall that in Section 3.4, we mention that decoy masks are generated by sliding the swappable
patch across a given input. With a given constant stride 1, the number of sliding windows is equal to
(√d - P + 1)2. In our implementation, to enable batch computing, We introduce m, which controls
ys is 2 [(√d - P + 1)2∕m].
the number of sliding windows in each decoy. Then, the number of deco
Fig. A12 shows the results of fixing m as 100 and varying P . In Fig. 2(C), we substantially varied
both P and m and showed that our method is insensitive to the variations in the number of decoys n.
Note that the box bars with the same color in Fig. 2(C) are drawn by fixing P and varying m. Their
slight difference indicates the robustness of our method in the variations of m.
The results in Fig. 2(C), A19, A11, and A12 indicate we can expect to obtain stable decoy saliencies
when the hyper-parameters are subtly varied. This is a critical characteristic because users do not
need to overly worry about setting very precise hyper-parameters to obtain a desired saliency map.
In addition to the hyper-parameters introduced by our methods, we also test the sensitivity of fidelity
evaluation results to the choice ofK in the topK normalization. Specifically, we varied K to select top
10% and 40% important features and redrawn the fidelity/sensitivity comparison figures in Fig. 2(B)/
Fig. 4 (B)〜(D).The results in Fig. A13, A14, and A15 are aligned with those in Fig 2 and 4.
A17 Object localization
We compare our method and the vanilla gradient on the object localization task (Dabkowski & Gal,
2017; Fong & Vedaldi, 2017), where the model was trained with the class label only without access
to any localization data. We carried out Imagenet ILSVRC’14 localization task (Russakovsky et al.,
25
Under review as a conference paper at ICLR 2021
(a) Top-k attack.
Grad	IntGrad
SGrad
(b) Mass center attack.	(c) Target attack.
Figure A14: Sensitivity comparison when selecting top 10% features on ImageNet (See Tab. A10 for
more statistics about the performance differences).
400
≤∙
>
⅞200
(a) Top-k attack.
Without decoy
S) Withdecoys
Grad
IntGrad
SGrad
IntGrad
SGrad
(b) Mass center attack.	(c) Target attack.
Figure A15: Sensitivity comparison when selecting top 40% features on ImageNet (See Tab. A11 for
more statistics about the performance differences).
Table A3: ImageNet localization accuracy on VGG16 network using different thresholding strategies.
Accuracy	Value thresholding (0.25)	Energy thresholding (0.25)	Mean thresholding (0.25)
Gradient	0.662	OΓ5	0.662
Gradient W/ decoys	0.722	0.723	0.665
2015) which contains 50K ImageNet validation images with annotated bounding boxes as ground
truth. For each image, we first calculated the gradient-based saliency maps with and without using
decoys, based on the pretrained model. Following the preprocessing steps suggested by Dabkowski
& Gal (2017); Fong & Vedaldi (2017), we then obtained a bounding box from each calculated
saliency maps based on certain thresholds. Specifically, we investigated three thresholding strategies
suggested by Fong & Vedaldi (2017): value thresholding, energy thresholding, and mean thresholding.
Following the evaluation protocol of Dabkowski & Gal (2017); Fong & Vedaldi (2017), we then
computed the Intersect over Union (IoU) of the extracted box and the ground truth. If an IoU is
greater than 0.5, the corresponding box is marked as correct. Table A3 shows that decoy-enhanced
saliency maps achieve higher accuracy than those of the vanilla gradient.
A18 Additional experimental results
Fig. A17, Fig. A16, and Fig. A18 provide more results of the fidelity and robustness evaluation.
These results are consistent with those shown in the Section 4.
A19 Statistics of the Performance differences
In section 4, Section A14, and Section A16, we varied the choice of K in the top-K normalizations,
compared our method with each baseline approach, and showed the fidelity/sensitivity of each
approach in the box-plots. To demonstrate the advantage of our method over the baselines, we further
compared the fidelity/sensitivity difference between our method and the corresponding baseline
approach. To be more specific, given two sets of fidelity/sensitivity scores (sour and sbase) obtained
from our method and a baseline approach respectively, we first computed their difference, i.e.,
diff = sour - sbase. Then, we conducted a statistical measure on the values of diff by computing
the mean, the standard error, and the p-value of the paired t-test. For the paired t-test, our null
hypothesis is H0 : E[dif f] ≥ 0. This indicates that, if the value of p is larger than a threshold,
we cannot reject this null hypothesis, and have to conclude that our method cannot outperform the
corresponding baseline approach. As We present in Table A4〜Table A11, the overall experiment
results align with those shown in the box plots, demonstrating the superiority of our method over the
baselines.
26
Under review as a conference paper at ICLR 2021
Gradient w/o decoy this	S one of Bolanski `s	best films
Gradient w/ decoys this	S one of polanski Ie	films
IntGrad w/o decoy Bhis	S one of polanski `s	films
IntGrad w/ decoys this	S one of polanski `s	best films
SGrad w/o decoy this	S one of polanski Ie	bes] films
SGrad w/ decoys this	S one of polanski F	films
SF: 0.457
SF: 0.038
SF: 0.457
SF: 0.038
SF: 0.073
SF: 0.062
Gradient w/o decoy ∣Nο	movement	no	yuks	not	much	of	anything	SF: 1.003
Gradient w/ decoys ∣No	movement	no	yuks	not	much	of	anything	SF: 0.075
IntGrad w/o decoy ∣No	movement	no	yuks	not	much	of	anything	SF: 1.003
IntGrad w/ decoys No	movement	no	yuks	not	much	of	anything	SF: 0.084
SGrad w/o decoy ^B movement	^B	yuks	not	much	of	anything	SF: 0.111
SGrad w/ decoys No movement no yuks much of anything SF: 0.105
Gradient w/o decoy most new movies ■
bright sheen
Gradient w/ decoys most new movies have a brigh∣ sheen
IntGrad w/o decoy ∣most new movies a
IntGrad w/ decoys most new movies have a
SGrad w/o decoy most new movies have
SGrad w/ decoys most new movies have a
bright sheen
bright sheen
bright sheen
sheen
SF: 0.457
SF: 0.049
SF: 0.457
SF: 0.049
SF: 0.069
SF: 0.031
a
Gradient w/o decoy ∣as	a	singular	character	study	it	‘s	perfect	SF: 0.457
Gradient Wl decoys ∣as	a	singular	character	study	it	's	perfect	SF: 0.039
lntGrad w/o decoy as	a	singular	character	study	it	's	perfect	SF: 0.457
IntGrad w/ decoys ∣as	a	singular	character	study	it	's	perfect	SF: 0.039
SGrad w/o decoy as	a	singular	character	study	it	's	perfect	SF: 0.076
SGrad w/ decoys as	a	singular	character	study	it	's	perfect	SF: 0.056
Gradient w/o decoy
Gradient w/ decoys
IntGrad w/o decoy
IntGrad w/ decoys
SGrad w/o decoy
SGrad w/ decoys
a well made and If^n I^H depiction of
a well made and Bften lovely depiction of
a well made and If^n ∣^m depiction
a well made and I often lovely depiction
a well made and often ∣^H depiction
a well made and often ∣^BB depiction
of
of
of
of
the mysteries of friendship
the mysteries of ■
the mysteries of friendship
the mysteries of
the mysteries of
the mysteries of
Hendship
friendship
Hendship
SF: 0.457
SF: 0.013
SF: 0.457
SF: 0.007
SF: 0.006
SF: 0.006
Figure A16: Visualization of saliency maps on the sentences in SST dataset. The row labels and
colorbar are the same with those in Fig. 3(A).
r adent radent ]：-nt nt rad nt rad nt rad
r ad rad
r ad
radent r adent r adent nt rad nt rad nt rad
rad rad
rad
SF 11 SF 2 -
SF : SF 34
SF	SF
SF 4 SF 1
SF 「 SF 34
in"gMa —二 ^ SF
Shield SF 6 SF 34	SF 6 6 SF 5	SF ∙ SF 6∙1
SF 64
:F 5 4
.■■■	3
V ι an
F 1
Cliff S F 1 SF 4	S F 1 3ι SF	SF 34 SF 4
:3 5
SF
S F 14 1 S F
SF 1 SF 1
S F 1	1 S F 6
SF 5 SF 6 4
SF 6 SF 33
SF 5 5 SF .1
SF 6 SF 6 4
:F 6 46
SF 6 1 SF 1
SF 1 5 2 SF 2 5
de de d "e-e 1-	1- d ---	1-	1- d "---

de de d ---	1-	1- d ---	1-	1- d ---
Figure A17: Visualization of saliency maps on the images in ImageNet dataset. The column labels
and colorbar are the same with those in Fig. 2(A).
27
Under review as a conference paper at ICLR 2021
SS 16.21
Target
Attack
SS 30.50
SS 27.15 SS 22.7o
SS: 77.48 SS 56.36
SS 19.50 SS 15.97
SS 197 97 SS 138 14
Target
Attack
SS 20 46 SS 16 50
Top-k
Attack
w/o decoy w/ decoys difference w/o decoy w/ decoys difference w/o decoy w/ decoys difference
Mass
Center
Attack
Target
Attack
Gradient Gradient Gradient IntGrad IntGrad IntGrad
SGrad SGrad SGrad
SS 25.53 SS 21.81
SS 269.87 SS 183.50
SS: 35.97 SS 28.72
SS 16.49 SS 13.82
SS 32.75 SS 29.32
SS 139.94 SS 121.71
SS 24.63
SS 20 68
SS 23.68
Target
Attack
Top-k
Attack
SS： 102 42 SS 77 11
Figure A18: Visualization of saliency maps on the perturbed images generated by using three attacks
in VGG16. The column labels are the same with those in Fig. 2(A).
Bottom layers to top layers
Shield
Gradient
w/ decoys
IntGrad
w/ decoys
SGrad
w/ decoys
(a)	The mean and standard derivation of SF score for gradient, integrated gradient and SmoothGrad are:
(10.23, 0.29), (10.37, 0.84), (9.34, 0.51).
(b)	The mean and standard derivation of SF score for gradient, integrated gradient and SmoothGrad are:
(0.07, 0.02), (0.01, 0.003), (0.06, 0.007).
(c)	The mean and standard derivation of SF score for gradient, integrated gradient and SmoothGrad are:
(2.15, 0.50), (0.97, 0.56), (0.19, 0.06).
Figure A19: Demonstrations of decoy-enhanced saliency maps generated from each convolutional
and pooling layer in VGG16.
28
Under review as a conference paper at ICLR 2021
Table A4: Mean, standard error, and p-value of the difference in Fig. 2(B).
Salinecy method	Without decoy		Constant with range		Noise with range		Decoys with mean	
	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
Gradient	-1.61±3.24	0.014	-1.26±2.29	0.009	-051 ±1.74	0.093	-1.80± 2.15	< 0.001
IntegratedGrad	-1.14±3.82	0.087	-O71±3.41	0.170	-0.06±3.03	0.440	-2.53± 2.25	< 0.001
SmoothGrad	-0.41±1.23	0.068	-0.44±1.27	0.058	-0.79±1.22	0.003	-1.80± 2.65 一	0.002
Table A5: Mean, standard error, and P-value of the difference in Fig. 3(B).
Salinecy method	Without decoy		Constant with range		Noise with range		Decoys with mean	
	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
Gradient	-0.29±0.57	< 0.001	0.003±0.09	0.921	0.003±0.09	0.912	-0.17±0.51	< 0.001
IntegratedGrad	-0.12±0.56	< 0.001	0.001 ±0.07	0.744	-0.20±0.44	< 0.001	-0.09±0.52	< 0.001
SmoothGrad	-0.02±0.52	0.043	-0.02±0.52 一	0.043	-0.02±0.51 一	0.029	0.006±0.15 一	0.959
Table A6: Mean, standard error, and P-value of the difference in Fig. 4(B)~(D).
Attack	Gradient		Integrated gradient		SmoothGrad	
	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
Top-k	-23.52 ± 57.02	0.008	-3.89 ± 2.47	<0.001	-2.32 ± 21.00	。349
Mass Center	-30.43± 25.48	<0.001	-6.06 ± 4.56	<0.001	-2.75 ± 1.85	<0.001
Target	-7.66 ± 3.03~	<0.001	-4.77 ± 1.29~	<0.001	-2.81 ± 2.88~	0.002
Table A7: Mean, standard error, and P-value of the difference in Fig. A9.
ExpGrad		VarGrad		SGradRange		IntUniform		GradCAM	
Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
-2.26 ± 4.11	0.009	-0.95 ± 1.18~	0.001	-0.66 ± 1.51^^	0.026	-2.98 ± 3.18~^	< 0.001	-0.08 ± 0.25~	0.121
Table A8: Mean, standard error, and p-value of the difference in Fig. A13a.
Salinecy method	Without decoy		Constant with range		Noise with range		Decoys with mean	
	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
Gradient	-0.87±2.13	0.034	-0.30±1.21	0.126	-O29±1.01	0.100	-0.91± 1.96	0.021
IntegratedGrad	-1.39±2.29	0.005	-1.31±1.64	O001	-0.79±1.50	O011	-2.02± 1.91	< 0.001
SmoothGrad	-0.79±0.97	< 0.001	-1.16±1.17	< 0.001	-0.58±1.01「	0.007	-1.76± 1.69 一	< 0.001
Table A9: Mean, standard error, and P-value of the difference in Fig. A13b.
Salinecy method	Without decoy		Constant with range		Noise with range		Decoys with mean	
	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
Gradient	-3.26±3.88	< 0.001	-0.37±3.74	0.320	-1.27±2.51	0.014	-3.73± 2.75	< 0.001
IntegratedGrad	-2.31±3.70	0.004	-0.21±2.99	0.374	-1.87±3.08	0.005	-4.33± 3.41	< 0.001
SmoothGrad	-0.94±1.21「	O001	-0.94±1.09	< 0.001	-0.48±0.70	0.002	-2.67± 2.92 一	< 0.001
Table A10: Mean, standard error, and P-value of the difference in Fig. A14.
Attack	Gradient		Integrated gradient		SmoothGrad	
	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
Top-k	-8.04 ± 49.69	0.285	-1.58 ± 1.75	0.003	-1.34 ± 16.30	0.386
Mass Center	-14.48± 15.68	0.003	-2.98 ± 2.41	< 0.001	-1.87 ± 1.26	< 0.001
Target	-3.95 ± 2.42~	<0.001	-2.30 ± 1.06~	< 0.001	-1.81 ± 1.96~	0.003
Table A11: Mean, standard error, and P-value of the difference in Fig. A15.
Attack	Gradient		Integrated gradient		SmoothGrad	
	Mean±Std	P-value	Mean±Std	P-value	Mean±Std	P-value
Top-k	-42.64 ± 76.08	0.032	-8.37 ± 4.26	< 0.001	-2.81 ± 23.61	0.338
Mass Center	-56.54± 38.27	<0.001	-10.28±31.85	0133	-2.51 ± 1.49	< 0.001
Target	-13.09 ± 3.60~	<0.001	-8.29 ±2.08~~	< 0.001	-3.12 ± 3.69~	0.005
29