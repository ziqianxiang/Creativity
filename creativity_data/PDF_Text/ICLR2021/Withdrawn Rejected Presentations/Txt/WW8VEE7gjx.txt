Under review as a conference paper at ICLR 2021
Dimension reduction as an optimization
PROBLEM OVER A SET OF GENERALIZED FUNCTIONS
Anonymous authors
Paper under double-blind review
Ab stract
We reformulate unsupervised dimension reduction problem (UDR) in the lan-
guage of tempered distributions, i.e. as a problem of approximating an empirical
probability density function pemp (x) by another tempered distribution q(x) whose
support is in a k-dimensional subspace. Thus, our problem is reduced to the min-
imization of the distance between q and pemp, D(q, pemp), over a pertinent set of
generalized functions.
This infinite-dimensional formulation allows to establish a connection with an-
other classical problem of data science — the sufficient dimension reduction prob-
lem (SDR). Thus, an algorithm for the first problem induces an algorithm for the
second and vice versa. In order to reduce an optimization problem over distribu-
tions to an optimization problem over ordinary functions we introduce a nonneg-
ative penalty function R(f) that “forces” the support of f to be k-dimensional.
Then we present an algorithm for minimization of I(f) + λR(f), based on the
idea of two-step iterative computation, briefly described as a) an adaptation to real
data and to fake data sampled around a k-dimensional subspace found at a pre-
vious iteration, b) calculation of a new k-dimensional subspace. We demonstrate
the method on 4 examples (3 UDR and 1 SDR) using synthetic data and standard
datasets.
1	Introduction
Linear dimension reduction (LDR) is a family of problems in data science that includes principal
component analysis, factor analysis, linear multidimensional scaling, Fisher’s linear discriminant
analysis, canonical correlations analysis, sufficient dimensionality reduction (SDR), maximum au-
tocorrelation factors, slow feature analysis and more. In unsupervised dimension reduction (UDR)
we are given a finite number of points in Rn (sampled according to some unknown distribution)
and the goal is to find a “low-dimensional” affine (or linear) subspace that approximates “the sup-
port” of the distribution. The study field currently achieved a saturation level at which unifying
frameworks for the problem become of special interest Cunningham & Ghahramani (2015). An
approach that we present in that paper is based on the theory of generalized functions, or tempered
distributions Soboleff (1936); Schwartz (1949). An important generalized function that cannot be
represented as an ordinary function is the Dirac delta function, denoted δ, and δn denotes its n-
dimensional version.
Any dataset {xi}iN=1 ⊆ Rn naturally corresponds to the distribution pemp(X) = N PIN=I δn(X-Xi)
which, with some abuse of terminology, can be called the empirical probability density function.
Based on that, UDR can be understood as a task whose goal is to approximate pemp(X) by q(X),
where q(X) is a distribution whose density is supported in a k-dimensional affine subspace A ⊆ Rn.
Note that a function whose density is supported in some low-dimensional subset of Rn is not an
ordinary function. Exact definitions of such distributions can be found in Section 3. To formulate
an optimization task we additionally need a loss D(pemp, q) that measures the distance between the
ground truth pemp and a distribution q, that we search for. Thus, in our approach, the UDR problem
is defined as:
I (q) = D (pemp, q) → min	(1)
q
under the condition that q(X) has a k-dimensional support.
1
Under review as a conference paper at ICLR 2021
The SDR problem is tightly connected with the UDR problem. In SDR, given supervised data,
the goal is to find the so called effective subspace, defined by its basis vectors {wι,…,Wk} ⊆
Rn, such that the regression function can be searched in the form g(wTx, ∙∙∙ , WTx). In Wang
et al. (2010) it was shown how a method originally developed for SDR can be turned into an UDR
method, i.e. applied to unsupervised data, by simply setting an output to be equal to an input.
The key observation of our analysis, stated in Theorem 2, is that a class of functions of the form
g(wτx,…，WTx) can be characterized as functions whose Fourier transform is supported in the
corresponding effective subspace. In Section 4 we give 3 examples of UDR problems that we cast
as 1 and in the fourth example we formulate SDR as an optimization task with the search space dual
to that of UDR. Thus, all 4 examples can be studied within the same optimization framework.
The structure of the paper is as follows: in Section 3 we formally define the search space in
Problem 1, denoted Gk, and an image of Gk under the Fourier transform, denoted Fk. In-
stead of searching directly in a set of generalized functions, Gk, in Section 5 we describe how
we substitute an ordinary function for a distribution in the optimization task at the expence of
adding a new penalty term to its objective, λR(f). Using a gaussian kernel M(x, y), Theorem 4
characterizes generalized g ∈ Gk as such g for which the matrix of properly defined integrals
Mg = Re [RjRn×Rn Xiyj g(x)*M (x, y)g(y)dxdy] i -l is of rank k. We define R(f) as a squared
i,j=1,n
Frobenius distance from MMf to the closest matrix of rank k. In Section 6 We suggest a method
for solving minφ I(φ) + λR(φ) which we call the alternating scheme. Section 7 is dedicated to
experiments with the alternating scheme on synthetic data and standard datasets.
2	Preliminaries and notations
Throughout this paper we use standard terminology and notation from functional analysis. For
exact definitions one can address the textbook on the theory of distributions Friedlander & Joshi
(1998). The Schwartz space of functions and its dual space are denoted by S(Rn) and S0(Rn)
correspondingly. For a tempered distribution T ∈ S0(Rn) and φ ∈ S(Rn), hT, φi denotes T (φ).
The Fourier and inverse Fourier transforms are denoted by F, F-1 : S0(Rn) → S0(Rn). For brevity,
we denote F[f] by f. If all required conditions are satisfied, an integrable f : Rn → C (or, a Borel
measure μ on Rn) is used as the tempered distribution Tf (or, Tμ) wherehTf, φi = JRn f (x)φ(x)dx
(or, hTμ, φi = JRn φ(x)dμ). For Ω ⊆ S0(Rn), Ω* denotes the sequential closure of Ω with respect to
weak topology of S0(Rn). By L2(Rn) we denote the L2-space with the inner product: hu, viL2 =
J u(x)*v(x)dx. For φ ∈ S(Rn),ψ ∈ S0(Rn), their convolution and multiplication are denoted
by φ * ψ and φψ correspondingly. For	gι	∈	S0(Rk)	and	g2	∈	S0(Rn-k),	gι 0	g?	∈	S0(Rn)
denotes their tensor product. For a square matrix A, Tr(A) denotes its trace and for arbitrary matrix,
||A||f d=f PTr(AT A). Identity matrix of size n is denoted by In
3	Basic function classes
An example of a generalized function, whose density is concentrated in a k-dimensional subspace,
is any distribution that can be represented as g 0 δn-k d= g 0 δ 0 ∙ ∙ ∙ 0 δ where g ∈ S0(Rk).
^^~^^{^^~^^}
n - k times
If g = Tf , where f : Rk → R is an ordinary function, then g 0 δn-k can be understood as a
generalized function whose density is concentrated in a subspace {x ∈ Rn|xi = 0, i > k} and
equals f (x1:k). It can be shown that the distribution acts on φ ∈ S(Rn) in the following way:
hTf 0δn-k,φi
/	f(xi：k)0(xi：k
Rk
, 0n-k)dx1:k
Now to generalize the latter definition to any k-dimensional subspace we have to introduce a change
of variables in tempered distributions.
Let g ∈ S0(Rn) and U ∈ Rn×n be an orthogonal matrix, i.e. UTU = In. Then, gU ∈ S0(Rn) is
defined by the rule: hgU, φi = hg, ψi where ψ(x) = φ(U T x). If g = Tf, the latter definition gives
gU = Tf0 where f0(x) = f(Ux). Now, we define classes of tempered distributions:
Gk = {(f 0 δn-k)u|f ∈ S0(Rk), U ∈ Rn×n, UTU = In}
(2)
2
Under review as a conference paper at ICLR 2021
Gk = {(Tf ㊈ δn-k)u|f ∈ S(Rk), U ∈ Rn×n, UTU = In}	(3)
Fk = {Tr|r(x) = f(Ux), f ∈S(Rk),U ∈Rk×n,rank(U) =k}	(4)
The first two classes are related as:
__	_ _ .	——*
Theorem 1.	Gk = Gk .
The last two classes are isomorphic under the Fourier transform.
Theorem 2.	F[Gk] = Fk andF-1[Fk] = Gk.
For any collection fι,…，力 ∈ S 0(Rn), spanR{fi}1 denotes {pi=ι λif∖λi ∈ R} ⊆ S 0(Rn),
which is a linear space over R. The set Gk0 has the following simple characterization:
Theorem 3.	Forany T ∈ S 0(Rn), T ∈ Gk if and only if dim spanR {xιT, x2T,…，xnT} ≤ k.
Informally, the theorem holds because any linear dependency a1x1T + …+ αnXnT = 0 over R
implies that if α1x1 +-+ anx = 0, then T = 0. This is equivalent to a statement that the support
of T is concentrated on a subspace α1x1 +-+anXn = 0. If dimspanR{x1T,x2T, ∙∙∙ ,xnT} ≤ k,
then one can find n - k such dependencies, which means that the support of T is k-dimensional.
Let B(Rn ) denote the Borel sigma-algebra on Rn and P denote a set of all Borel probability mea-
sures on Rn . Let us now define
Pk = {μ ∈ P∣∃vι,…，Vk ∈ Rn,∀A ∈ B(Rn) : μ(A) = μ(A ∩ SPan(VI,…，Vk))}	(5)
i.e. Pk is a set of probability measures with all probability concentrated in some subspace
SPan(VI,…，Vk) whose dimension is not greater than k. It is easy to see that Tμ ∈ Gk for any
μ ∈Pk.
4 Examples of LDR formulations
UDR: Maximum mean discrepancy (MMD) Let k(x)
∣χ∣2
e 2h2 be the radial gaussian
1
、/(2∏h2)n
kernel on Rn. The kernel k(x) defines the so-called kernel embedding of probability measures
φ Muandet et al. (2017):
μ ∈ P → Φ(μ) = k * μ = Ey〜“k(x - y) = / k(χ - y)dμ(y)
The Maximum Mean Discrepancy (MMD) distance Gretton et al. (2012) is defined as the distance
induced by metrics on L? (Rn), i.e. for two probability measures μ,ν ∈ P:
dMMD(μ, ν) = llφ(μ) - φ(V)||L2(Rn)
Let xι,…,XN ∈ Rn be the dataset of points. This dataset defines the empirical probabilistic
measure μdata that corresponds to the tempered distribution T^ata = NN PN=I δn(x - Xi). We
shall study a method concurrent to PCA that is based on solving the following problem:
I (V) = dMMD (μdata,V) = ∣∣φ(μdata) - φ(ν )∣∣L2(Rn) → mi∏
ν∈Pk
(6)
i.e. We shall attempt to approximate the empirical probabilistic measure μdata with another proba-
bilistic measure V which is supported in some k-dimensional subspace of Rn .
UDR: Distance based on higher moments (HM) It is well-known that maximum mean discrep-
ancy measures the similarity between characteristic functions of two probability distributions in the
O (ɪ) -neighbourhood of the origin. Another approach to measure the similarity of two distributions
is based on the difference between moments:
dHM(μ, ν)2 = X λs	X	(mil…is - niι∙∙∙is )2
s=1	1≤iι,…，is ≤n
where m”…is = EX〜μ [X[iι]…X[is]] and n”…is = EX〜V [X[iι]…X[is]] are corresponding
moments. The positive parameters λ1, λ2, λ3, λ4 are chosen to fix the relative importance of the
mean, the co-variance, the co-skewness and the co-kurtosis.
3
Under review as a conference paper at ICLR 2021
Thus, we will be interested in the following optimization task (analogous to 6):
dHM(μdata, ν) → min
ν∈Pk
(7)
UDR: Wasserstein distance (WD) Another important distance between probability measures that
has the origins in the transport theory is the Wasserstein distance Villani (2008).
Let (Rn, || ∙ ||) be a Banach space. Between any two Borel probability measures μ, V on Rn with
J ∣∣x∣∣dμ < ∞ and J ∣∣x∣∣dν < ∞ the Wasserstein distance is:
W(μ, ν) =	inf	I ||x - y∣∣d∏
π∈π(μ,ν) J
where Π(μ, ν) is a set of all couplings of μ and V. The Wasserstein distance defines another version
of LDR problem:
W(μdata,ν) → min
ν∈Pk
(8)
In the appendix B one can find proofs that in the case of L1 norm ||x|| = i |xi |, the task 8
corresponds to the well-studied robust PCA problem CandeS et al. (2011). If, instead of the L∖-
norm, we use the L2-norm, this leads to another well-studied task, which is known as the outlier
pursuit problem Xu et al. (2010).
Sufficient dimension reduction (SDR) Given a labeled dataset {(xi, yi)}iN=1 where xi ∈ Rn, yi ∈
C (C is a finite set of classes for a classification, or R for a regression problem), the suffi-
cient dimension reduction problem can be informally described as a problem of finding vectors
wι,…，Wk ∈ Rn such thatp(y∣wTx,…，WTx) ≈ p(y∣x) (possibly, under some additional as-
sumptions on the form ofp(y|x)).
We formulate the SDR problem as an optimization task:
inf J(f)
f∈Fk
(9)
The object f : Rn → R is a smooth real-valued function. We assume that f is a candidate for the
regression function and J(f) is a cost function that values how strongly f fits in this role. In practice
for the regression case and for the binary classification case with 0-1 outputs we use the following
cost functions correspondingly:
1N	2
J(f) = N 工 EsN(0,υ2In) |yi - f (Xi + €)|
1 N	ef (xi +)
J(f) = N TECN(0,U2In)H (yi, I + f (Xi + e))
i=1
where H(y, p) = -y logp - (1 - y) log(1 - p) and υ > 0 is a parameter.
By requiring f ∈ Fk, we assume that the regression function f satisfies (for k fixed in advance):
f(x) = g(wTX,…，WTx)
where wι, ∙∙∙ , Wk ∈ Rn. Thus, given an input x, an output of f depends on the projection of
X onto SPan(Wι, ∙∙∙ , wk). The set SPan(W1,…,Wk) is called the effective subspace. Note that
the way we defined the SDR’s objective J(f) for the regression and the classification cases is not
unique. There are definitions that has the same form 9, but deal with the conditional distribution
p(y |x) as an argument, instead of the regression function.
5 Reduction of the optimization problem to ordinary functions
The central problem that our paper addresses is how to minimize an objective function over Gk0 (or
Pk)? In this section we describe an approach based on penalty functions and kernels.
Let us assume for simplicity that M is the gaussian kernel, i.e. M (x, y) = Gσn(x - y) where
1	一 ∣χ∣2
Gn(X) = √^ςn e-0. Besides the gaussian kernel our theory also captures many other kernels,
4
Under review as a conference paper at ICLR 2021
including cases of the Abel Kernel
Poisson kernel: ------cnσ-n+ι.
(σ2 + ∣x-y∣2) 2
σ1ne-修 and the FOUrier tranform of the Abel Kernel, the
For f, g ∈ S(Rn) let Us denote:
hf |M|gi d=f 〃	f(x)*M(x,y)g(y)dxdy ≤ maxM(x,y)||f |匕|回匕 < ∞
Rn ×Rn	x,y
For general f, g ∈ S0(Rn) the expression hf|M|gi is defined if ∃f, g ∈ S(Rn) sUch that Tf =
f * Gn, Tge = g * Gn and hfe∖M∣gei e→0 A. Then, hf |M|g〉d=f A. For example, (δn∣M∣δni =
M(0, 0).
Theorem 3 concludes, from f ∈ Gk, that dimspanR{x1f,x2f, ∙∙∙ ,xnf} ≤ k. Using the kernel
M, one can bUild the Gram matrix from the collection of distribUtions, [hxif∖M∖xjfi]1≤i,j≤n. For
any f ∈ S0(Rn) let us denote a real part of the Gram matrix [Re hxif ∖M ∖xj f i]1≤i,j≤n by Mf (if it
is defined).
Theorem 4.	If f ∈ Gk, then hxif∖M∖xjfi is defined and rank Mf ≤ k.
Definition 1. Let A ∈ Rn×n be a positive Semidefinite matrix with eigenvalues λι ≥ λ2 ≥∙∙∙≥ λn
(with counting multiplicities). Then, the Ky Fan k-anti-norm of A is ∖∖A∖∖k = Pk=I λn+ι-k.
Let R(f) = ∖∖Mf ∖∖n-k. Theorem 4 tells US that that for f ∈ Gk R(f) = 0. For ordinary f,
the Eckart-Young-Mirsky theorem gives US R(f) = mi□A∈Rn×n,rankA≤k ∖∖ JMf 一 A∖∖F. Thus,
by penalizing the value of R(f), we enforce Mf to be close to some matrix of rank k. For I :
Gk0 ∪ S(Rn) → R+, it is natural to reduce the optimization task over tempered distributions
I(f) → min
f∈Gk0
to an optimization task over ordinary functions with a penalty term R:
I(f)+λ∖∖Mf∖∖n-k=I(f)+λR(f)→ f∈iSn(fRn)
(10)
(11)
Details on the conditions, under which this reduction holds, can be found in the appendix D. Let us
now concentrate on the task 11 and describe the alternating scheme for its solution.
6 The alternating scheme
We will concentrate on problem 11. It is known Hiai (2013) that the Ky Fan anti-norm is a concave
function, i.e. R(φ) = ∖∖Mφ∖∖n-k depends on Mφ in a concave way. It can be shown that the
dependence of R(φ) on φ is both non-convex and non-concave, i.e. we deal with a non-convex
optimization task.
The kernel M(x, y) : Rn × Rn → C induces a linear operator from L2 (Rn) to L2 (Rn):
O(M)[f] = Rn M(x, y)f (y)dy. For any operator O between spaces H1 and H2, we denote
its range as R[O] = {O(x)∖x ∈ H1}. Let B(H1 , H2) denote a set of bounded linear oper-
ators between Hilbert spaces H1 and H2 . For O ∈ B(H1 , H2 ) the rank of O is defined as
dim R(O). Let Lr2 (Rn) be the Hilbert space (over R) of real-valued functions from L2 (Rn) and
Lg(Rn) = Lr(Rn) X Lr(Rn). The space Lg(Rn) is equivalent to L2(Rn) treated as a linear space
over R. Below We do not distinguish [φι, φ2] ∈ Lg(Rn) and φ1 + iφ2 ∈ L2(Rn). It is easy to see
that any O ∈ B(L2g (Rn), Rn) can be given by formula:
O[φ]i = Re hOi, ΦiL2(Rn),Oi ∈ L2(Rn),i = in
i	.e. O ∈ B(L2(Rn), Rn) can be identified with a vector of functions O =[。儿=可,Oi ∈ L2(Rn)
and the Hilbert-Schmidt norm on B(Lg (Rn), Rn) (i.e. √Tr OtO) is:
∖∖O∖∖g = t
n
X ∖∖Oi ∖∖2L2 (Rn)
i=1
(12)
5
Under review as a conference paper at ICLR 2021
Recall that for the kernel M,O(M) is positive and self-adjoint. Since O(M) is also bounded, then
the square root "O(M) can be correctly defined RUdin (1991). For any complex-valued function f
let us introduce a linear operator Sf : L22 (Rn) → Rn by the following rule:
Sf [Φ]i = Re hxif(x), PO(M)[φ]iL2(Rn) i.e. (Sf ) = PO(M)[xif (x)],i = 17n
Theorem 5.	If Tr Mf < ∞, then Sf ∈ B(Lg(Rn), Rn) and Sf Sf = Mf. Moreover,
R(f) =	min	||Sf - S||2
S∈B(L*(Rn),Rn ),rank S≤k
and the minimum is attained at S = PfSf where Pf = Pik=1 uiuif and {ui}1k are unit eigenvectors
of Mf corresponding to the k largest eigenvalues (counting multiplicities).
Given the new representation R(f) =	min	||Sf - S|已 it is natural to view the
S∈B(L*(Rn),Rn),rank S≤k	J
Task 11 as a minimization of I(φ) + λ∣∣Sφ - S||2 over two objects: φ and S ∈ B(Lg(Rn),Rn):
rank S ≤ k . The simplest approach to minimize a function over two arguments is to optimize
alternatingly, i.e. first over φ, and then over S : rank S ≤ k, and so on. Theorem 5 gives that the
minimization over S is equivalent to the truncation of SVD(Sφ) at the k-th term. This idea, that we
dub the alternating scheme, is described in Algorithm 1.
Algorithm 1 The alternating scheme for 11
PO V  0, Sφo V  0
for t = 1,…，T do
φt V- argmin I (φ) + λ∣∣Sφ - Pt-ιSφt-∕∣2 (minimizing over φ)
φ-
Calculate Mφt and find {vi}n s.t. Mφt Vi = λ%Yi, λι ≥ ∙∙∙ ≥ λn
Pt V- Pik=1 viviT (Truncated SVD(Sφt ) is PtSφt)
Output： V1,	∙ , Vk
The alternating scheme 1 allows for a reformulation in the dual space. By this we mean that in
Scheme 1 we substitute φt for the original φt. If the primal Scheme 1 deals with operators Sφ, Sφt-1 ,
the dual version deals with vectors of functions \/^7碧,ʌ/GZdd-I. Details of the dual algorithm
can be found in the appendix F.
7	Experiments
The alternating scheme 1 is a general optimization method which needs to be specified for every
optimization task. We designed numerical specifications of the alternating scheme 1 for all 4 opti-
mization tasks: 6, 7, 8 and 9 and made experiments with all of them. Details of the algorithms, i.e.
numerical methods to minimize over φ and calculate Mφt, can be found in the appendix (G, H, I
and J). Note that for Wasserstein distance minimization 8 we exploit the alternating scheme in the
initial form (i.e. 1), and for MMD 6, HM 7 and SDR 9 we use the dual version of the scheme.
Behaviour of MMD for small h. We studied the difference in the behaviour of PCA and a solution
of 6 obtained by the alternating scheme 1 (MMD), for the case when h is small compared to the
standard deviation of features. Experiments show that they are sharply different when data points
are sampled along a low-dimensional manifold M, which is bent globally, goes through the origin
O and has a large curvature at O. Because PCA is a global method and points do not lie on an affine
subspace, interpreting principal directions is not straightforward.
We select a smooth function f : Rn-1 → R, such that f(0) = 0 and generate points in the
following way: points xι, x2,…，XN 〜[-10,10]n-1 are sampled uniformly, after calculation of
yi = f (Xi) we add some noise: Zi = (xi, yi) + 且，Wi 〜N(0,0.01In). Both PCA and MMD are
applied to the dataset (first 3 pictures on Figure 1a). As we see, MMD, unlike PCA, tries to catch
ideal alignments of points rather that searching for a global alignment of points (which can be non-
existent). This property of MMD makes it a promising tool for the calculation of the tangent space
6
(a) ViSUaHZarδ∙n OfOUrPUrS Of the PCA and MMD methods ∙ MMD (green Hne) rends S SeIeCr a SUbCOneCrδ∙n
Of PoinrS that: SharPIy aligns along the main direction” whereas the firsr PrinCiPaI COmPOnenr (red Hne) COUld be
a resuH Of averaging OVer different: directions in rhe dara∙
(b)L⅛rplor二R ——P 一y6 U 005“ > H 20pCaSeL ■ 6 U 0b5>u 2。O case«・ 6 U 0b5>u
IoOPCaSe L ∙ 6 Ug-5) > H IoOo CaSe n“ 6 U 0∙L › H IoOo CaSe L ■ 6 U 0∙L › H IoOo CaSe π∙
Righr Pg=PP = FaSa funcHon Ofln ; MMD“ ■ HM“ WD∙
S a da〔a manifold a given Poinr∙ FoUIth PiCrUre ShOWSrhar When WehaVe 2 equa∏y imporranr
direao∙ns5∙da〔a SUCh Cha 二 he nrs〔 PrinCiPai direcro∙n Of PCA is baween Chem (red line)yand We
saA; U Lrhen MMD (green line) always ChoOSeS One Of〔hose direcro∙ns∙
EXPerimeIlfS Wifh ou≡er daeaioɪl (MMD∙ HM∙ WaSSerSfeill di⅛ance)∙ FOnOW5∙g rhe exPeri—
menr saup Of XUaal∙ (2010)》We ChOOSe ParameSrS N U τz U 400" 5u0∙05(0∙l)3∕cu 1。and
generaCe random matrices 4 ∈ IRN(I——5) X了 B ∈ WhOSe entries are iid as .ʌf(ɔI) ∙ The?
Srhe COlUmnS Of rhe matrix EAT m 用KXN(I——5) (WhOSerank is ≤ k) are COnCarenared Wirh rhe
COlUmnS Of rhe matrix C ∈ IRKXNf χ U CollCa什(R 4J C) ∈ IRKXN∙ The enrries5∙Care ehher
iid as Ne I) (CaSe I) Or No COPieS Of rhe Same VeCSrWhOSe entries are iid as .ʌf(ɔI) (CaSe π)∙
LaXU -XI" ∙ ∙ ∙ 3 XNL i.e∙ COlUmnS Of X are rhe da〔a PoinrS∙ ThU∞N(I ——5) COIUmnS Of EAT
He in a A:—dimensional SUbSPaCe Of IRK and No COlUmnS Of Care Ourliery and SOlUriOnS Of Casks 6》7
Or 8 for Chis darasaare expeaed S be SUPPolted in a COlUmn SPaCe Of EAT.
AfSr every ireraro∙n (SreP t Of rhe alremar5∙g SCheme I)We CaICUlare rhe FrObe≡.us disBnce baween
rhe projecro∙n OPerarOr RofI and rhe projection OPerarOr Frorhe ColUmn SPaCe Of i.e∙
一 一⅛I F=F∙ FOrrhe Cask∞rhe dependence Of=F ——F - - FOnffOr differed ValUeS Of Paramaers
5 and A is ShOWn in FigUre Ib∙ ForraSkS 6》7 rhe behaviour Of rhe alremar5∙g SCheme is SimiIaL 7
israro∙ns are enough S approach rhe OPrimaI SUbSPace∙
BeSideSrheSPeed Of COnVergenCe We WerealSo5∙reresred5∙how =F* I F 一yWhere F* H
HmrJ8⅛is rhe final projection OPerarOr (eg J⅞o5∙pracrice)y depends On rhe paramaer a.
OfrhekemeI MUGq. IriS naCural S expearhe qualify of rhe SoIUriOn F* S degrade as
QJ +∞ (rhis COITeSPOndSrO AI(X3y) J 0)9 andyIeSStTiVianyyas Q，O (rhis COlTeSPOndSrO
AI(X3y) →¾(xly))∙
EXPerimellfS Wifhfhe SUffidellC dimension reducfion∙ We made experimenCs Onrhe Srandard
darasa∞HearL BreaSr CanCen IOnOSPhee Diabae∞BOSrOn house PriCeS and Wine qualiry∙ FirSr
We applied SHCed InVerSe RegreSSiOn algorithm (SlR) Li(1991) Srhe training saand CaICUlaSd rhe
effective SUbSPaCe for /c U 2" 3∙ An PoinrSWere PrOjeaed OmOrhar SPaCe and We Obra5∙ed rwo— Or
rhree—dimensional represemaro∙ns Of5∙pur PoinrS ∙ In rhe IasC SreP We appliedOnearesC neighbours
algorithm (KNN) S PrediaOUrPUrS (based On reduced5∙purs) On rhe SSr Ser (for rhe regression CaSf
rheO—KNN regression WaS USed) ∙ The Same SChemeWaS repeaCed Wirh PCA》KenIeI DimenSiOnaHry
RedUCriOn (KDR) algorithm FUkUmiZUaal∙ (2004) and rhe alsmar5∙g SCheme 1 adapted for SDR∙
Under review as a conference paper at ICLR 2021
We experimented with the dual version of algorithm 1, setting (after the data was standardized) the
kernel’s parameter σ = 0.81 and λ = 10.0. Details of its numerical implementation can be found
in the appendix J. In the table 1 one can see the obtained test set accuracy on the classification tasks
and R2 on the regression tasks. As we see from the table 1, after reducing the dimension of an input
to k = 2, 3, we are still able to obtain good accuracy of prediction on a test set.
^^^^^ethod Dataset^^^^^^^	PCA		SIR		KDR		AS 1	
Dimension k	2	3	2	3	2	3	2	3
Heart (acc)	79.80	79.46	82.49	81.82	86.33	88.77	81.48	83.50
Breast(acc)	93.46	93.65	97.30	96.73	93.13	95.95	97.88	97.69
Ionosphere (acc)	80.29	86.57	89.14	89.43	83.43	86.29	88.29	90.57
Diabetes (R2)	25.34	28.72	43.47	43.61	41.82	44.30	43.07	44.48
Boston (R2)	56.42	67.12	76.03	74.29	77.88	79.97	73.21	77.88
Wine (R2)	93.91	94.12	98.68	99.24	98.30	96.02	97.10	96.93
Table 1: The cross-validated accuracies/R2 of KNN on 2 or 3-dimensional input representations.
The code is available on github to facilitate the reproducibility of our results.
8	Related work
We present an optimization framework in which the search space is Gk0 , or Pk. Another unifying
framework for LDR tasks is suggested by Cunningham & Ghahramani (2015) in which the basic
search space is the Stiefel manifold S(n, k). The main disadvantage of using Gk0 , instead of the
Stiefel manifold, is that its infinite number of dimensions requires a special procedure to turn an op-
timization into a finite-dimensional task. Both an optimization over Gk0 and over S(n, k) is typically
hard: for a final point, at best one can guarantee that it is a local extremum. Promising aspects of
Gk0 are: a) Gk0 allows to formulate naturally a new class of objectives on it, b) local extrema on Gk0
substantially differ from local extrema on S(n, k), because a local search over Gk0 uses more degrees
of freedom.
Using Ky-Fan k-antinorm as a regularizer for the matrix completion problem has been suggested
by Hu et al. (2013) and further developed in Oh et al. (2016); Liu et al. (2016); Hong et al. (2016).
Unlike this chain of works, we formulate an infinite-dimensional task. Also, our regularizer R(f) =
||Mf ||n-k is a sum of smallest n-k squared singular values of the operator Sf where Sf depends on
f linearly. The idea of alternating two basic stages, the convex optimization and SVD, is ubiquitous
in low-rank optimization, see e.g. Mazumder et al. (2010); Hastie et al. (2015).
Zhu & Zeng (2006) applied the Fourier transform for estimating the effective subspace in SDR,
implicitely using an analog of Theorem 2.
9	Conclusions
We develope a new optimization framework for LDR problems. The alternating scheme for the
optimization task demonstrates both the computational efficiency and the applicability to real-world
data. The algorithm performs quite stably when we vary most of the hyperparameters, though it
crucially depends on two parameters, the bandwidth of the “smoothing” kernel M, σ, and the penalty
parameter λ. We believe that the MMD/HM/WD methods for UDR could be used as an alternative
to PCA in study fields in which data demonstrate “heavy-tailed” and “non-gaussian” behaviour,
such as financial applications. Also, our formulation of SDR is free from any assumptions on the
distribution of input-output pairs, which makes it an alternative to other methods of the efficient
subspace estimation. More detailed report on these topics is a subject of future research.
1Since the role of the parameter σ is similar to that of the bandwidth in the kernel density estimation, we
use Silverman’s rule of thumb to set σ = N -1/(n+4).
8
Under review as a conference paper at ICLR 2021
References
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference
on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 214-223,
International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR. URL http://
proceedings.mlr.press/v70/arjovsky17a.html.
A. R. Barron. Universal approximation bounds for superpositions of a sigmoidal function. IEEE
Transactions on Information Theory, 39(3):930-945, May 1993. ISSN 0018-9448. doi: 10.1109/
18.256500.
S.	Bochner. Vorlesungen UberFouriersChe Integrale: von S. BoChner. Mathematik und ihre Anwen-
dungen in Monographien Und Lehrbuchern. Akad. Verl.-Ges., 1932.
Emmanuel J. Candes, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?
J. ACM, 58(3), June 2011. ISSN 0004-5411. doi: 10.1145/1970392.1970395. URL https:
//doi.org/10.1145/1970392.1970395.
John P. Cunningham and Zoubin Ghahramani. Linear dimensionality reduction: Survey, insights,
and generalizations. Journal of Machine Learning Research, 16(89):2859-2900, 2015. URL
http://jmlr.org/papers/v16/cunningham15a.html.
F.G. Friedlander and M.S. Joshi. Introduction to the Theory of Distributions. Cambridge University
Press, 1998. ISBN 9780521649711.
Kenji Fukumizu, Francis R. Bach, and Michael I. Jordan. Dimensionality reduction for supervised
learning with reproducing kernel hilbert spaces. J. Mach. Learn. Res., 5:73-99, December 2004.
ISSN 1532-4435.
A. Gretton, K. Borgwardt, M. Rasch, B. Scholkopf, and A. Smola. A kernel two-sample test. Journal
of Machine Learning Research, 13:723-773, March 2012.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,
S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30,
pp. 5767-5777. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7159-improved-training-of-wasserstein-gans.pdf.
Trevor Hastie, Rahul Mazumder, Jason D. Lee, and Reza Zadeh. Matrix completion and low-rank
svd via fast alternating least squares. Journal of Machine Learning Research, 16(104):3367-3402,
2015. URL http://jmlr.org/papers/v16/hastie15a.html.
Fumio Hiai. Concavity of certain matrix trace and norm functions. Linear Algebra and its
Applications, 439(5):1568 - 1589, 2013. ISSN 0024-3795. doi: https://doi.org/10.1016/j.
laa.2013.04.020. URL http://www.sciencedirect.com/science/article/pii/
S0024379513002826.
Bin Hong, Long Wei, Yao Hu, Deng Cai, and Xiaofei He. Online robust principal component
analysis via truncated nuclear norm regularization. Neurocomputing, 175:216 - 222, 2016.
ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2015.10.052. URL http://www.
sciencedirect.com/science/article/pii/S0925231215015118.
T.	Hsing and R. Eubank. Theoretical Foundations of Functional Data Analysis, with an Intro-
duction to Linear Operators. Wiley Series in Probability and Statistics. Wiley, 2015. ISBN
9780470016916.
Y. Hu, D. Zhang, J. Ye, X. Li, and X. He. Fast and accurate matrix completion via truncated nuclear
norm regularization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(9):
2117-2130, 2013.
Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statisti-
cal Association, 86(414):316-327, 1991. ISSN 01621459. URL http://www.jstor.org/
stable/2290563.
9
Under review as a conference paper at ICLR 2021
Q. Liu, Z. Lai, Z. Zhou, F. Kuang, and Z. Jin. A truncated nuclear norm regularization method based
on weighted residual error for matrix completion. IEEE Transactions on Image Processing, 25
(1):316-330, 2016.
Rahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for learn-
ing large incomplete matrices. Journal of Machine Learning Research, 11(80):2287-2322, 2010.
URL http://jmlr.org/papers/v11/mazumder10a.html.
Krikamol Muandet, Kenji FUkUmizu, Bharath SriPerUmbUdur, and Bernhard SchOlkopf. Kernel
mean embedding of distributions: A review and beyond. Foundations and Trends® in Machine
Learning, 10(1-2):1-141, 2017. ISSN 1935-8237. doi: 10.1561/2200000060. URL http:
//dx.doi.org/10.1561/2200000060.
T. Oh, Y. Tai, J. Bazin, H. Kim, and I. S. Kweon. Partial sUm minimization of singUlar valUes in
robUst pca: Algorithm and applications. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 38(4):744-758, 2016.
Henning Petzka, Asja Fischer, and Denis LUkovnikov. On the regUlarization of wasserstein
GANs. In International Conference on Learning Representations, 2018. URL https://
openreview.net/forum?id=B1hYRMbCW.
Walter RUdin. Functional analysis. International Series in PUre and Applied Mathematics. McGraw-
Hill Inc., New York, second edition, 1991. ISBN 0-07-054236-8.
Laurent Schwartz. Theorie des distributions et transformation de fourier. Analyse Harmonique,
Colloques Internationaux du C.N.R.S., 15:1-8, 1949.
S. Soboleff. Methode nouvelle a resoudre le Probleme de Cauchy pour les equations Iineaires hy-
perboliques normales. Rec. Math. Moscou, n. Ser., 1:39-71, 1936.
C. Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften.
Springer Berlin Heidelberg, 2008. ISBN 9783540710509. URL https://books.google.
kz/books?id=hV8o5R7_5tkC.
Meihong Wang, Fei Sha, and Michael I. Jordan. Unsupervised kernel dimension re-
duction. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and
A. Culotta (eds.), Advances in Neural Information Processing Systems 23, pp. 2379-
2387. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/
4122- unsupervised- kernel- dimension- reduction.pdf.
Xiang Wei, Zixia Liu, Liqiang Wang, and Boqing Gong. Improving the improved training of
wasserstein GANs. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=SJx9GQb0-.
Huan Xu, Constantine Caramanis, and Sujay Sanghavi. Robust pca via outlier pursuit. In J. D. Laf-
ferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta (eds.), Advances in Neural
Information Processing Systems 23, pp. 2496-2504. Curran Associates, Inc., 2010. URL http:
//papers.nips.cc/paper/4005-robust-pca-via-outlier-pursuit.pdf.
Yu Zhu and Peng Zeng. Fourier methods for estimating the central subspace and the central mean
subspace in regression. Journal of the American Statistical Association, 101(476):1638-1651,
2006. doi: 10.1198/016214506000000140.
A Proofs for section 3
A.1 Proof of Theorem 1: given for completeness
Proof The inclusion Gk ⊆ GJ follows from a well-known fact that S(Rk) is dense in S0(Rk). I.e.
for any f ∈ S0(Rk) one can always find a sequence {fi} ⊆ S(Rk) such that Tfi →* f. Therefore,
for any (f 0 δn-k)u ∈ Gk there is a sequence {(Tf 0 δn-k)u} ⊆ Gk such that (Tfi 0 δn-k)u →*
(f 0 δn-k )u . Thus, Gk ⊆G *.	2	2
10
Under review as a conference paper at ICLR 2021
*
Since Gk ⊆ Gk0 , to prove Gk0 = Gk it is enough to show that Gk0 is sequentially closed.
We need a simple fact from a theory of distributions.
Lemma 1. If Ti →* T and φi → φ, then hTi, φii → hT, φi.
ProofofLemma. SchWartz space S(Rn) is a Frechet space, therefore the Banach-SteinhaUs theo-
rem applies to S0(Rn). Since Ti →* T, we have supi ∣hTi,φ}∣ < ∞ for any φ ∈ S(Rn). From the
Banach-SteinhaUs theorem, applied to a set {Ti }1∞, We obtain for any > 0, there is a neighboUr-
hood U of 0 ∈ S(Rn) SUch that |〈储，φ>∣ < e whenever φ ∈ U. Thus, | ⑵，φi - φi∣ < e for a large
enough i. From that we conclude that (Ti, φii →(T, φ).	□
For any T ∈ S0(Rn) and ψ ∈ S(Rn-k),let US define Tψ ∈ S0(Rk) as (Tψ,φi = (T, φ % ψ).
Suppose that {fi}∞ ⊆ S0(Rk), {Ui}∞ are such that (fi 0 δn-k), →* f. We need to prove that
f ∈ Gk0 . Since a set of orthogonal matrices is compact, then one can always find a subsequence
{Uni} such that Uni → U. Since (fni 0 δn-k)Uni →* f and φ(Uni x) → φ(U x) (for any fixed
φ ∈ S(Rn)), using lemma 1 we obtain:
(fni0δn-k,φi = ((fni0δn-k)Uni,φ(Unix)i→ (f, φ(Ux)i = (fUT,φ(x)i
Thus, we have:
fni 0 δn-k →* fUT
From the last we see that fni →* fUψT where ψ is such that ψ(0) = 1. Therefore, fUT = fUψT 0δn-k
and f = (fUτ 0 δn-k)u ∈ Gk.	□
A.2 Proof of Theorem 2
Proof. Let us prove first that ifg = Tf 0 δn-k, then
F [g] = Tr
1	/	∖	/	∖	_	T∏><K)	1 ∙	.1	.	i	.	. i	/ -r-Γ 1 I ∖	Im i ∖ r∙
where r(x) = f (x1:k), x ∈ Rn. For that we have to prove that (F[g], φi = (Tr, φi for any
φ ∈ S(Rn). Indeed,
,	φ(y)e-ixTydyi
Rn
(F [g], φi = (g, F [φ]i = (Tf 0 δn-k
(Tf,
Rn
T
Φ(y)e	LkyLk dyi
/
Rn+k
f(Xi：k )Φ(y)e-ixTk y1:k dydxi：k
f f(yι*)φ(y)dy
Rn
(Tr,φi
Let us calculate the image of Gk under the Fourier transform. It is easy to see that for any g ∈
S0(Rn), φ ∈ S(Rn) and orthogonal U ∈ Rn×n we have:
(F [gU], φ(x)i = (gU, F [φ](x)i = (g, F [φ](U T x)i =
= (g,F[φ(UTx)]i= (F [g], φ(U T x)i = ((F[g])U,φ(x)i
Therefore, F[gU] = (F [g])U. Thus, ifg = Tf 0 δn-k, then
(F [gU ]) = (Tr )U = Tr0
where r0(x) = r(Ux) = f(Ux) and Uk ∈ Rk×n is a matrix consisting of first k rows of U. Thus,
Tr0 ∈ Fk.
Let us show that by varying f ∈ S(Rk) and U in the expression /(Ukx) we can obtain any function
from Fk . For this it is enough to show that Fk is equivalent to the following set of functions:
Q = {g(Ukχ)∣g∈s(Rk),Uk ∈Rk×n,UkUT = Ik}
The fact Q ⊆ Fk is obvious. Let us now prove that Q ⊇ {g(Px)|g ∈ S(Rk), P ∈ Rk×n, rank P =
k} = Fk. Indeed, if f(x) = g(Px), then f(x) = g0(Ukx) where Uk = (PPT)-1/2P and
g0(y) = g((P P T)1/2y). By construction, UkUkT = Ik and g0 ∈ S(Rk). Thus, Q = Fk.
Therefore, F[Gk] = Fk, and from the bijectivity of the Fourier transform we obtain F-1[Fk] =
Gk.	□
11
Under review as a conference paper at ICLR 2021
A.3 Proof of Theorem 3
ProofofTheorem 3 (⇒). Let Us prove that from T = (f 0 δn-k)u, f ∈ S0(Rk),UTU = In it
follows that dim spanR{xιT, x2T,…，XnT} ≤ k.
It is easy to see that Xi[f 0 δn-k] = 0 if i > k. If U = [uι, •…，Un]T, then for i > k We have
0 = (xi[f 0 δn-k])U = uiT x(f 0 δn-k)U = uiTxT.
Thus, we have n 一 k orthogonal vectors, Uk+ι, ∙∙∙ , Un, such that [xιT,…,XnT]ui = 0. Using
standard linear algebra we obtain there are at most k0 distributions x” T,…,Xi^ T, k0 ≤ k that
form a basis of spanR{xiT}?.	□
To prove the second part of theorem we need the following lemma.
Lemma 2. IfT ∈ S0(Rn) is such that yiT = 0for any i > k, then T ∈ Gk0 .
Proofoflemma. Recall from functional analysis, for f ∈ S0(Rn), the tempered distribution f is
defined by the condition h∂df, φ = -hf, ∂dφ). Once the Fourier transform is applied, our lemma's
dual version is equivalent to the following formulation: if∂f = 0,i>k,then f ∈ F；. Let us
prove it in this formulation.
A set of infinitely differentiable functions with a compact support is denoted by Cc∞ (R). Suppose
φ ∈ S(Rn) andp ∈ Cc∞(R) are chosen in such a way that -∞∞ p(yi)dyi = 1, supp p ⊆ [A, B]. Let
us define:
r(x) =	φ(x-i, yi)dyi -	p(yi)dyi	φ(x-i, yi)dyi
-∞	-∞	-∞
It is easy to see that for any α ∈ Nn-1, α0 ∈ N, β ∈ Nn-1, β0 ∈ N we have (at least one derivative
over Xi is present):
0	∂β,1+β0 r
Xa Xa，_______r_
ixi ∂x-i∂x1+β0
xα-iXiα
0 ∂β,β0 [φ(x) -p(Xi) R-∞∞ φ(x-i,yi)dyi]
∂ x-i∂xβ0
Xa xa0 dβ,β0 φ(X)
ixi ∂x-i∂xβ0
0
a0
- Xi
∂β p(Xi) ∞° a ∂β φ(x-i,yi)
FeL J-∞χ-	∂ x-i	dyi
The terms x-ixα :； φ(XO and Xa "；Ppei) are bounded by the definition of S(Rn), C∞(R). The
boundedness of R∞ xɑidpφ(x-i,yi)dyi is a consequence of the inequality (which holds because
-∞ -i	∂xβ-i
φ∈S(Rn)): ∣x-idβφ∂X-i,yi)∣≤ 1⅛.
Analogously (not a single derivative over Xi is present):
0
aa
x-iXi
∂βr
dx-i
X? Zxi x-i dβφ(x-i,yi) dyi - X«0
-∞	∂x-i
Z xi p(yi)dyi Z∞
-∞	-∞
x-i dβφl dyi
∂xβ-i
=Xiα0 (1-ZXi p(yi)dyi) f Xi x-i 演φ(x-i,yi) dyi-Xiα0 [xi p(yi)dyi f ∞ x-i 演φ(x-i,yi) dyi
-∞	-∞	∂x-i	-∞	xi	∂x-i
The second term is 0 when Xi ≤ A. It is also bounded when Xi > A because 氏匕 dφ(x-i,yi) | ≤
-i	∂Xβ-i
(i+yCF+ιand:
Xa「x-i dβφl dyi
xi	∂x-i
0	∞	C0
≤kil L(1+y2)ɑ0+1 dyi
The latter is bounded because limxi→+∞ ∣Xi∣α0 R∞∞ .+^2^0+1 dyi = 0.
12
Under review as a conference paper at ICLR 2021
The first term is 0 when xi ≥ B and it is bounded for xi < B :
a，/Xi	ɑ ∂βφ(X-i,yi),
Xi LXi	dyi
≤lxi'∕∞ (1+ C2)a0+1 dyi
The latter is also bounded, since limxi→-∞ |xi|a R-∞ (1+yC)ɑ0 + 1 dyi = 0.
Thus, Xaddx(X) is bounded and r ∈ S(Rn). Therefore ∂f = 0 implies：
∂r
hf,w- i = 0 ⇒ f [φ]
Z∞
∞
φ(X-i, yi)dyi]
Since this sequence of arguments works for any i > k, we can apply them sequentially to initial φ ∈
S(Rn) w.r.t. xk+1, ..., xn and obtain for any pk+1, ..., pn ∈ Cc(R) such that -∞∞ pi(yi)dyi = 1:
f[φ]
f [Pk+l(xk + l) •…Pn (Xn) /
Rn-k
φ(xLk, xk+Ln)dxk + 1:n]
Moreover, since Cc∞(R) is dense in S(R), we can assume thatpk+1, ..., pn ∈ S(R). For the inverse
Fourier transform T = F-1 [f] the latter condition becomes equivalent to:
hT, φi = hT,pk+1 (Xk+1) •…Ipn(Xn) φ (x1: k, 0k+1:n)i
2
for any p0k+1, ..., p0n ∈ S(R) such that p0i(0) = 1. Let us define p0i(Xi) = e-xi . It is easy to check
that T = g 0 δn-k where g ∈ S0(Rk), <g,ψ> = hT,eTXk+1：n|2ψ(x±k))for ψ ∈ S(Rk). I.e.
T ∈ Gk and lemma is proved.	□
Proof of Theorem 3 (U). If dim spanR {xιT, x2T, ∙∙∙ , XnT} ≤ k, then
dim{v ∈ Rn∣ [xiT,…，XnT]v = 0} ≥ n — k
Thus, there exist at least n — k orthonormal vectors Vk+ι,…,vn, such that [xiT, ∙∙∙ , XnT]vi = 0.
Therefore, [xiT,…，XnT]vi = (VTx)T = 0.
Let US complete Vk+ι,…,Vn to form an orthonormal basis of Rn: vι, ∙∙∙ , vn Let US define a
matrix V = [vι,…,Vn]. It is easy to see that:
((VT X)T )v = (VT V X)TV = XiTV
Since for i > k we have	(ViT x)T =	0,	then	XiTV	=	0.	Using lemma 2 we obtain	TV	∈	Gk0 .
Therefore, (TV)vT = T ∈ Gk. Theorem proved.	□
B S tructure of WD
Recall that (Rn, ∣∣∙∣∣) is a Banach space. Now, let us consider an optimization problem: for a given
X ∈ Rn×N solve
||X — L|| → min
rank(L)≤k
(13)
where ∣∣∙∣∣ is extended to Rn×N by ∣∣[sι,…，SN]|| d=f Pi ||si||.
The following simple theorem shows that the two tasks are connected, so that the solution of one
directly leads to the solution of another.
Theorem 6.	Given data points {xι,…，XN } ,let X = [xι,…，XN ] ∈ Rn×N. Then,
mPk W ("data, V )= N Y ∈Rn×N±k(Y )≤k IX -
Moreover, min^∈Pk W(μdata, V) is attained on V*, where V* is a uniform distribution over {yi}N=ι
and [yι,…,yN] ∈ argminY∈Rn×N,rank(γ)≤k IX - Y||.
13
Under review as a conference paper at ICLR 2021
Proof. Let Us prove first that inf μ∈Pk W (μdata, μ) ≤ N ||X - Y * || where
Y * = [yι,…，yN ] ∈ arg min,、∣∣x - Y Il
Y ∈Rn×N ,rank(Y )≤k
Let π be a uniform distribution over {(xi, yi)}N=1 and μ* be a uniform distribution over {yi}N=1.
Since ∏ ∈ ∏(μdata,μ*), we obtain W(μdata,μ*) ≤ N PN=1 |肉-yi|| = N||X - Y*∣∣.
The support of μ* is k-dimensional, because rank(Y*) ≤ k. Thus, We have μ* ∈ Pk and
infμ∈Pk W(μdata, μ) ≤ W(μdata,μ*) ≤ N||X - Y*||. Now, if We prove the inverse inequal-
ity, i.e. inf μ∈p% W (μdata, μ) ≥ N ||X - Y * ∣∣, this will imply that inf μ∈p% W (μdata,μ)=
N∣∣X - Y*∣∣ and therefore, infμ∈p% W(μdata,μ) = W(μdata,μ*). This will in the end give Us
μ* ∈ arg infμ∈p% W(μdata, μ).
Let {μt}∞ be such that μt ∈ Pk and W(μdata, μt) - infμ∈p% W(μdata, μ) → 0. Let Lt denote a
k-dimensional support of μt and Pt is a projection operator onto Lt.
Let μ* be a uniform distribution over {Ptx1, ∙∙∙ , PtXN}, i.e. μ*(A) = NN PN=1[PtXi ∈ A]. It is
easy to see that W(μ*,μdata) ≤ W(μt,μdata), because μ* and μt share the same k-dimensional
support Lt, but the “transportation of a mass” concentrated in point Xi of the empirical distribution
μemp can be most optimally done by just moving it to PtXi (i.e. to the closest point on Lt). Thus,
we have infμ∈Pk W(μdata,μ) ≤ W(μdata,μ*) ≤ W(μdata,μt), and therefore, W(μdata,μ*)-
infμ∈Pk W (μdata, μ) → 0.
Since a set of projection operators is compact, one can always extract a subsequence {Pts}s∞=1,
such that Pts → P. It is easy to see that μ*s → μ** (i.e. W(μ*s,μ**) → 0) where μ** is
a uniform distribution over {Px1, ∙∙∙ ,PXN}. For that distribution we have W(μdata,μ**) =
lims→∞ W(μdata,μ*s) = infμ∈Pk W(μdata,μ). Thus, the infinum is attained on μ**.
It is easy to see that W(μdata, μ**) = W(μ**,μdata) = NI∣X - PX||. Since rank(PX) ≤ k we
obtain W(μdata,μ**) ≥ N minγ∈Rn×N,rank(γ)≤k ||X - Y∣∣. This completes the proof.	口
Note that the case of L1 norm ||X|| = i |xi| in the task 13 corresponds to the well-studied robust
PCA problem Candes et al. (2011). If, instead of the L1-norm, we use the L2-norm, this leads to
another task:
||X - L||1,2 → ranmk(Lin)≤k
(14)
where ||S||1,2
i si2j, which known as the outlier pursuit problem Xu et al. (2010).
C Proper kernels and proof of Theorem 4
C.1 Proper kernels
In the main part of the paper we assume M to be a gaussian kernel, though the theory can be applied
to a more general case of the so called proper kernels.
Recall that for any operator O between spaces Hi and H2 we denote its range as R[O] = {O(χ) ∣χ ∈
Hi}. For Ω ⊆ S(Rn), Ω denotes the sequential closure of Ω with respect to natural topology of
S(Rn). A set of continuous functions in Rn is denoted by C(Rn). A set of infinitely differentiable
functions with compact support in Rn is denoted as Cc∞ (Rn)
Definition 2. The function M(X, y) : Rn × Rn → C is called the proper kernel if and only if
•	O(M)[f] = RRn M(X, y)f (y)dy is a linear operator from L2(Rn) to L2(Rn),
•	M(y,X) =M(X,y)*,
•	hf,O(M)[f]iL2(Rn) >0,∀f ∈L2(Rn),f 6=0.
•	maxx,y |M (X, y)| < ∞,
•	R[O(M)] ∩ S(Rn) = S(Rn).
14
Under review as a conference paper at ICLR 2021
The gaussian kernel M(x, y) = Gσn(x-y), which is of special interest from an application-oriented
perspective, is captured by the following lemma:
Lemma 3. If ζ, ζ ∈ C(Rn) are bounded, ∀x ζ(x) > 0, then M (x, y) = ζ(x - y) is a proper
kernel.
Proof. Verification of the first four conditions is easy, so we only check the fifth condition. Let us
denote linear operators CZ[f] = Z * f and Og[f](x) = g(x)f (x). Then We have F[C《[L2(Rn)]]=
Oζ[L2(Rn)] ⊇ C∞(Rn). Therefore, R[O(M)] = CZ[L2(Rn)] ⊇ F-1[C∞(Rn)]. Since C∞(Rn)
is dense in S(Rn), then F-1[C∞(Rn)] also has this property. I.e. R[O(M)] ∩S(Rn) = S(Rn).
□
Besides the gaussian kernel the lemma also captures a case of the Abel Kernel ζ(x) = e-|x|. It is
well-known that the Fourier tranform of the Abel Kernel is the PoiSSon kernel: Z(x) =-Cn n+ι
(1 + ∣X∣2) ɪ
(which is also proper).
C.2 Proof of Theorem 4
We will prove a more general statement:
Theorem 7.	Let M(x, y) be a proper kernel and, additionally, a Lipschitz function. If f ∈ Gk, then
hxif|M|xjfi is defined and rank Mf ≤ k.
Proof. Let us first show that hf |M|g〉is defined for all f,g ∈ Gk. Note that for any f =(Ta 0
δn-k)U ∈ Gk we have
Tf = (Ta 0 δn-k)U * Gn = ((Ta *Gk)0TGn-k)U
Let us denote a = a * Gk and b = b * Gk. It is easy to see that
f = (a(x1:k)Gn-k (xk+1:n))U ∈ S(Rn)
From a well-known property of the Weierstrass transform we have:
IifeiiLi = ι∣ad∣Lι ∙ι∣Gn-k 必 回同区
From this we obtain for any f = (Ta 0 δn-k)U, g = (Tb 0 δn-k)V ∈ Gk:
ihfeiMigeii ≤ max iM (x, y)i iifeiiL1iigeiiL1 ≤ max iM (x, y)i iiaiiL1 iibiiL1 < ∞
x,y	x,y
Thus, hfeiMigei is defined and:
hfe∣M Igei = / a*(xi：k )Gn-k (Xk+l：n)M (U T x,V T y)be(yi:k )Gn-k (yk+1：n )dxdy =
Rn ×Rn
=Z a^xi-k )Me(XLk，y1：k )be(yi：k)dxi：k dyi：k
Rk ×Rk
where
Me(x1:k,y1:k) =	Gen-k(xk+1:n)M (U T x, V T y)Gen-k(yk+1:n)dxk+1:ndyk+1:n
Rn-k ×Rn-k
Let Uk , Vk ∈ Rn×n be matrices that comprise the first k rows of U, V correspondingly and n - k
zero rows below. Also, let L denote Lipschitz constant for M such that iM (x, y) - M(x0, y0)i ≤
L(ix - x0i + iy - y0i). For the function Me(x1:k, y1:k) we have:
iMe (x1:k, y1:k) - M (UkT x, VkT y)i =
I / Gn-k(xk+Ln) (M(UTx, VTy) - M(UTx,VTy)) Gn-k(y®+i：n)dxk+i：ndyk+i：n|
R2n-2k
15
Under review as a conference paper at ICLR 2021
≤ Ll / Gn k (xk + 1:n) (∣(U - Uk )T x∣ + I(V - Vk )T y∣) Gn k (y k+1-.n)dxk+1-.ndy k+1:n ∣
R2n-2k
LI /
R2n-2fc
Gi k(Xk+ l:n)(|xk+1:n l + |Yk+1:n|) Gi ( (yk+1:n)dxk + 1:ndyk + 1:n l
2L J |xk + 1:n |G1 ((xk+1:n)dxk + 1:n
Rn-k
2Le / |xk"n|GnT(χk+Ln)dχk+Ln
Rn-k
Thus, there exists bounded Ml(XI•*, yi:k) = M(UTx, VkTy) SUCh that:
J a*(X1:k)M(X1:k,y1:k)b(y1:k)dX1:kdy1:k1
Me(XI:k, yi:k) 1→0 M(X1: k, y 1: k ) in l∞ (R )
Further we assume that e > 0 is small enough, so that Me(XI.*, yi:k) ≤ C = 2 max |M|. Now we
have:
Ihfe|M |ge)-
Rk×Rk
1 ZSg∙k)Me(X1: k，yi：k)My1: k)-a*(X1: k)M(X1:k,yi:k)b(yi:k))dX1:kdyi:kl =
Rk ×Rk
` Z Me(Xi: k,yi:k应(Xi: k)(Myi： k)- b(yi:k))dXi:kdyi：k+
Rk ×Rk
J Me(X1: k,yi:k)b(yi:k)a(X1: k)- α*(X1:k))dX1:kdyi：k+
Rk ×Rk
J a*(X1: k)b(yi:k)(Me(X1: k,yi：k)- M(X1: k,yi:k))dX1:kdyi:kl≤
Rk×Rk
C ||aeHLi ||be - b||Li + C||b||Li ||ae - a||Li + " a" (X1:k ) b(y 1: k )H∑ι ||Me - M "l∞
It is well-known (e.g. see Theorem 2.25) that ||ae 一 a\∣Lp, ||be - b||Lp → 0, ||ae||Li ≤ ||a||Li and
||Me - MHl∞ → 0. Thus, lime→ohfe|M|仇)exists and (f |M|g〉is defined.
Let us now prove that rankMf ≤ k. Since f ∈ Gk, then f = (Tg 0 δn-k)u where U is an
orthogonal matrix and U = [wi, ∙ ∙ ∙ , Wn]. It is easy to see that:
Qif|M|xjf)= h(xf )uT |M(Utx, Uty)|(xjf )uT)
(wTXTg 0 δn-k|M(UTX, UTy)|wTXTg 0 δn-k)
Let us now denote V = [ui, ∙ ∙ ∙ , Un] ∈ Rk×n a submatrix of U in which only first k rows of U are
present. Then, the latter integral is equal to:
Rk×Rk
UT Xi：k yTkUjg(Xi：k)" M (V T Xi：k, V T yi：k )g(yi：k)dXi：k dyi：k
UT BUj
where
B = KxigIMIxjg)]1≤i,j≤k ,M0(Xi：k,yi：k) = M(VTXi：k, VTyi：k)
is the Gram matrix of the collection {gg(Xi：k)}k=i ⊆ S(Rk).
Obviously, rank Mf = rank [Re UT Bu7-] i≤. j≤n = rank V T(Re B)V ≤ rank V = k.	口
D General theory of THE reduction for Section 5
For a sequence {fs}∞=i ⊆ S0(Rn), Lim fs denotes a set of points f ∈ S0(Rn), such that there
s-∞
exists a growing sequence {si} ⊆ N and limi→∞ fSi = f.
16
Under review as a conference paper at ICLR 2021
D.1 Regular solutions and reduction theorems
For I : Gk0 ∪ S(Rn) → R+, it is natural to reduce the optimization task 10 to an optimization task
over ordinary functions with a penalty term 11. To have an equivalence between 10 and 11 we need
to assume that I’s behaviour when approaching f ∈ Gk0 from a set S(Rn) is continuous, i.e. for any
sequence {fi} ⊆ S(Rn) such that Tfi →* f ∈ Gk, We have limi→∞ I(Tfi) = I(f).
Let us introduce the notion of a regular solution both for 10 and 11. Let
Bk = [ {f ∈Gk∣ Tr(Mf) ≤ C}*
C>0
Definition 3. Any f ∈ Arg min I(f)	Bk is called a regular solution of 10.
f∈Gk0
In other words, Bk formalizes a set of distributions from Gk0 , that can be approached through se-
quences {fi} ⊆ Gk, for which Tr(Mfi) does not blow up. Obviously, Gk ⊆ Bk ⊆ Gk0 . In ap-
plications, regular solutions include all Arg min I(f) if we choose the kernel M correctly. This
regularity is important for a reduction to the penalty form 11, because when approaching a non-
regular solution we are unable to guarantee a bounded behaviour of Mf (and of R(f)).
Definition 4. A sequence {fi}1∞ ⊆ S(Rn) is said to solve 11 if:
I(fi)+λiR(fi) ≤	infn I(f) + λiR(f) + i
(15)
where i → +0 and λi → +∞, i → +∞. If, additionally, Tr(Mfi) is bounded, then {fi}1∞ is said
to solve 11 regularly.
Let us define
rsol(I(f),R(f))=	Lim Tfi
i→∞
{fi }1∞ r. solves (11)
Theorem 8.	If M is a proper kernel, then rsol (I(f), R(f)) ⊆ Arg min I(f).
f∈Gk0
Theorem 9.	If M is a proper kernel and rsol (I(f), R(f)) 6= 0, then Arg min I(f) T Bk ⊆
f∈Gk0
rsol (I(f), R(f)).
Theorem 10	(Reduction theorem). If M is a proper kernel, Arg min I(f) ⊆ Bk and
f ∈Gk0
rsol(I(f),R(f)) 6= 0, then rsol(I(f),R(f)) = Arg min I(f).
f∈Gk0
Suppose that we now solve a sequence of problems 11 and find {fs}1∞. According to Theorems 8
and 9, the following are potential scenarios:
(1) Tr(Mfs) blows up and the convergence is not guaranteed. This situation can be avoided by
controlling Tr(Mf ) in an optimization process. In practice, when f has a parameterized form, this
can be done by bounding parameters.
If Tr(Mfs) does not blow up, we still have two subcases:
(2.1)	Lim Tfs = 0. This implies a positive outcome to approach 11 to the optimization problem,
s→∞ s
Problem 10.
(2.2)	Lim Tfs = 0. This exotic situation can happen only if a sequence Tfs leaves any sequentially
compact subset of S0 (Rn). Bounding parameters also tackles this case.
D.2 Proofs of Theorem 8 and 9
For any f =(Tl 0 δn-k)u ∈ Gk and σ > 0, let us define fσ as:
Tfσ = (Tl 0 δn-k)u * Gn = (Tlσ 0 TGn-QU
fσ = (lσ (xi:k )Gn-k (Xk+1：n))u
17
Under review as a conference paper at ICLR 2021
lσ = l * GJk
We have Tfb →* H % δn-kIU as σ → +0.
Lemma 4. For any f ∈ Gk, limσ→+0(xi∕σ|M|叼fσ)= 0, for any (i,j) ∈ {1,...,k}2, and
suPσ∈[0,i] hχifσ|M∣Xj fσ〉< ∞,forany (i,j) ∈ {1,…，k}2.
Proof. W.l.o.g. we can assume that f = Ti 0 δn~k, l ∈ S(Rk). If i > k,j ≤ k we have:
hxifσ∣m∣xjfσi = (2πσ2)n-k HRn×RnxMe
xk + 1:n 1 2	_ |yk + 1:n 1 2
- lσ(xi：k)M(x, y)e	2σ2- lσ(yi：k)dxdy

Ln √2πσ2n-k
|xk + 1:n|2
lσ (xi:k )P (x)dx
xie	2b2

where P(X) = JRn √2π^n-k yjM(χ, y)e-
obtain:
1 yk+1:n 1
—2σ2 — lσ(yi：k)dy. Using the Holder inequality we
Kxifσ ∣M ∣xj fσ i∣ ≤ ∣∣ 1
λ∕2πσ'
|xk+1:n |2
-----xie	2b2
ɪn-k	i
lσ (XI：k )llLι(Rn)HP llL∞(Rn)

ll	n-k xie
V2πσ2
|xk + 1:n|2
∣∣L1 (Rl) lllσ ∣∣Lι(Rk)∣∣P ∣∣L∞(Rn)


Since ∣M(x, y)∣ ≤ Y for some γ, we have:
|yk + 1:n | 2
IP(X)l≤ γll√2⅛
父 yje--2b2
lσ(y1:J)IILI (Rn)

ll 1
γ∣l / n-ke
√2πσ2
Thus,
1 yk + 1:n 1
2σ2	llL1 (Rn-k)llyj lσ (y1：k )llL1(Rk) = Y||yj lσ (y1：k ^L1(Rk)

Kxifσ ∣M∣Xj fσ)| ≤ ∣∣
USing ∣∣lσ ∣∣Lι(Rk)-∣∣l∣∣Lι(Rk)
1
√2πσ'
σ→+0
|xk+1:n|2
----rx’e	2b2
ɪn-k i
llLι(Rn-k)lllσ ∣∣Lι(Rk)γ∣∣yjlσ ∣∣Lι (Rk)
of ∣∣lσ ∣∣L1(Rk) γ ∣∣yj lσ ∣∣L1(Rk) and proceed:
≤ C∣∣----1——rxie
- U i-----nn — k i
√2πσ2
0, l∣yj lσ ∣∣L1(Rk) TIyj l ll L1 (Rk) σ→0 0, We See the boundedness
2
|xk + 1:nI
2b2	||L1(Rn-k)


It is easy to see that ∣∣√? ；n-kxie'

|xk + 1:n 1
-2σ2-∣∣L1(Rn-k)	→	0 as σ →	0, therefore
hxifσ ∣M∣xj fσ)→ 0.
Similarly, we can prove that hxifσ ∣MIxj fσ)→ 0 if i,j > k.
The entries of the main k × k minor [hxifσ ∣MIxj fσi]ι≤i,j≤J are bounded, because:
Tr Mfσ
J ILRn
|xk+1:n|2
Iy k + 1: n I
lσ(xi：k)M(x, y)e	2b2- lσ(yi：k)dxdy ≤
TTi 2∖n-k	(|x1：k ,y1：k |+ |xk + 1：n ,yk + 1：n|)e
(2πσ2)	k √√Rn×Rn
|xk+1:n|2 + |yk + 1:n|2
lσ(xi：k)lσ(yi：k)dxdy ≤
Y〃
J√Rn×Rn
|X1：J ∙ yi：k∣lσ(xi：k)lσ(yi：k)dxi：kdyi：k + γ∣∣lσ∣∣L1(n - k)σ2 ≤
Again, using ∣∣lσ∣∣Lι(Rk)
boundedness of RHS.
k
Y X llyj lσ llL1 (Rk ) + Ylllσ llL1 (n - k)σ2
j = 1
l∣l∣lLI(Rk) σ→00, ∣∣yjlσ∣∣L1(Rk) - ∣∣yjl∣∣L1(Rk)σ→00, we obtain the
□
X ∙ ye	2b2

τσ2
—
18
Under review as a conference paper at ICLR 2021
Corollary 1. For any f ∈ Gk, limσ→0 R(fσ) = 0.
Proof. W.l.o.g. We can assume that f = Tl 0 δn-k, l ∈ S(Rk). By lemma, all entries of Mfσ except
those of the main k × k minor approach 0 as σ → 0. This means that
lim Q(fσ) =0
σ→+0
where Q(fσ) = P2k+ι (Xi fσ|M∣xifσ). Let vι,…,Vn be unit eigenvectors of Mfσ CorresPond-
ing to the eigenvalues λ1 ≥ …≥ λn, P = Pn=k+ι eieT ,then
n	nn
R(fσ)= X λi= min	X λipi ≤XλiTr(PviviTP) =Tr(PMfσP)=Q(fσ)
i=k+1	pi∈[0,1],P1npi=n-k i=1	i=1
Since R(fσ) ≤ Q(fσ), We obtain limσ→o R(fσ) =0.	□
D.2.1 Proof of Theorem 8
Proof. Suppose that a sequence {fi}s∞=1 ⊆ S(Rn) regularly solves (7) and T ∈ Lim fi. W.l.o.g.
i→∞
we can assume that Tfi →* T and Tr(Mfi) is bounded and I(fi) + λiR(fi) ≤
λiR(f) + i, i → 0. Below we use continuity ofI and corollary 1:
inf
f∈S(Rn)
I(f) +
inf	I(f) + λiR(f) ≤ inf inf I(fσ) + λiR(fσ) ≤ inf lim I(fσ)+λiR(fσ) ≤ inf I(f)
f∈S(Rn)	i	f∈Gk σ>0	σ i σ	f∈Gk σ→+0	σ i σ	f∈Gk
from which we conclude that λiR(fi) ≤ inf I(f) + i and, therefore, R(fi) i→→∞ 0.
f∈Gk
For each l, let US define Pl as the projection operator to a subspace spanned by first principal com-
ponents of the matrix y∕Mf, i.e.
k
Pl =XvilvilT
i=1
where v；,…,VIk are orthonormal eigenvectors that correspond to k largest eigenvalues of JMfl.
From the Eckart-Young-Mirsky theorem we see that R(fl) = ||、/Mfl 一 Pl、/Mfl ||F. Since a
set of all projection operators {P ∈ Rn×n |P 2 = P, PT = P} is a compact subset of Rn2, one
can always find a projection operator P = Pik=1 ViViT and a growing subsequence {ls} such that
||Pls 一 P||F → 0 as s → ∞. Thus, for the subsequence {fls} we have:
∣∣qMfis - p qMf∣s ∣∣f=∣∣qMs - Pls qMfιs+Pls qMfιs - P 河S ∣∣f ≤
IiqMflS-PlsqMflSI∣f + l∣Pls -PI∣fIiqMflSI∣f = pR(fS) + “Pls -pIIf√Tr(Mfs)
and using the boundedness of Tr(Mfs) we obtain ∣∣PMflS 一 PPMflS ∣∣f → 0.
Since ∣∣VMs-PVMsIIf → 0, let us complete V1 , ..., Vk to an orthonormal basis V1 , ..., Vn and
make the change of variables yi = ViT x. Let us denote V = [V1 , ..., Vn] and let VT = [w1 , ..., wn].
Then, after that change of variables any function f(x) corresponds to f0(y) = f(V y) and the kernel
M corresponds to M0 (y, y0) = M(V y, Vy0). If we apply that change of variables in the integral
expression ofhxif∣M∣xjfi, we will obtain:
hxif∣M∣xjfi =hwiTyf0∣M0∣wjTyf0i = wiT [hyi0f0∣M0∣yj0f0i]n×nwj ⇒
Rehxif∣M∣xjfi = wiT [Rehyi0f0∣M0∣yj0f0i]n×nwj
I.e. Mf = VMf00VT, or Mf00 = VTMfV . Note that P = VInkVT where Ink is a diagonal matrix
whose main k X k minor is the identity matrix, and all other entries are zeros. Using the fact that
the Frobenius norm of orthogonally similar matrices are equal and the identity VT PMflS V =
,VTMflS V, we obtain:
I∣qMfls - P √MfZ I∣F = I∣VT ,Mfls V - VT P qMfs V I∣F =
19
Under review as a conference paper at ICLR 2021
ii∕vTMfls V - VTVInVTvzMfZV∣∣F = ∣∣qMfτ -InqMf?∣∣f
Thus, the property ||PMflS - PPMflS ||f → 0 implies that:
Rehyifl0s|M0|yjfl0si →0, IFi>k
Moreover, for i = j we have Re hyifl0 |M0|yjfl0 i = hyifl0 |M0|yjfl0 i. It is easy to see that after
the change of variables We still have f[ →* TV. Since f[ ∈ S(Rn), We have yif[ ∈ S(Rn)
and, therefore, yifl0 ∈ L2(Rn). Let us treat now M0 as an operator O(M 0) : L2(Rn) →
L2 (Rn), O(M 0)[f](x) = RRn M0(x, y)f (y)dy. Let us take any function φ ∈ L2 (Rn) such that
ψ = O(M 0)[φ] ∈ S(Rn). Since O(M0) is a strictly positive self-adjoint operator, by the Cauchy-
SchWarz inequality, We obtain:
IhyifOs,O(M0)[φ]il ≤ RhyiflN0∣yif0siPh。。“0升。〕〉
Therefore, for any ψ ∈ R[O(M 0)] ∩ S(Rn) and i > k We have lims→∞ hyifl0 , ψi =
lims→∞ hf∣s ,yiψi = 0. Since f∣s →* TV we obtain hTV,yiψi = hyTV,ψi = 0 for any
ψ ∈ R[O(M 0)] ∩S(Rn). But the denseness ofR[O(M0)] ∩S(Rn) in S(Rn) implies thatyiTV = 0.
Using lemma 2 and (TV)VT = T We obtain T ∈ Gk0 . Thus, We proved that Tfi → T ∈ Gk0 .
Since I(fi) ≤ I(fi) + λiR(fi) ≤ inf I(f) + i and I is continuous, We finally get that I(T) ≤
f∈Gk0
inf I(f), i.e. T ∈ Argmin I(f).	口
f ∈Gk	f ∈Gk
D.2.2 Proof of Theorem 9
Proof. Suppose f* ∈ Arg min I(f) ∩Bfc, i.e. f* ∈ Bk and I(f*) = min I(f). Since f* ∈ Bk,
f∈Gk0	f∈Gk0
then there exists a sequence {si} ⊆ Gk such that Tsi →* f* and Tr Msi < ∞.
Let us define siσ ∈ S(Rn) as
Tsσ = Tsi * Gn
Since limσ→o R(s2) = 0 (lemma 4), there exists σi > 0, such that R(s*) < 1 whenever 0 <
σ ≤ σi. Also, by definition Tr Msi = limσ→0 Tr Msi . Therefore, there exists σi0 > 0, such that
Tr Msi < Tr Msi + 1 whenever 0 < σ ≤ σi0.
If we set σ* = min{σi, σ0,1}, then a sequence {s**} ⊆ S(Rn) satisfies:
lim R(s,σ*) = 0
i→∞	i
Tr M i < ∞
sσi*
and (using lemma 1)
Tsi * →* f *
σ*
Due to the continuity of I we have
lim I(siσ*) =I(f*)
i→∞	i
Now we set f = s2*, λi =	——and we obtain the needed sequence:
σi	R(fi)
lim I(fi) = lim I(fi) + λiR(fi) = I(f*), lim λi = +∞
i→∞	i→∞	i→∞
where Tr Mfi is bounded. It remains to check that our sequence regularly solves (7), i.e.
limi→∞	inf I(f)+λiR(f) =I(f*)(thiswillimplylimi→∞I(fi)+λiR(fi)- inf I(f)+
f∈S(Rn)	f∈S(Rn)
λiR(f) = 0). The inequality in one direction is obvious,
f∈iSn(fRn)I(f)+λiR(f) ≤fi∈nGfkσin>f0I(fσ)+λiR(fσ) ≤ fi∈nGfk σl→im+0 I(fσ) + λiR(fσ) =
20
Under review as a conference paper at ICLR 2021
inf I(f)= I(f*)
f∈Gk
Let us prove the inverse inequality.
Since rsol (I(f), R(f)) = 0, there exists {fi} ⊆ S(Rn) such that:
~. ~ , ~
〜
〜
I(fi) + λiR(f) ≤ inf	I(f) +	λiR(f)	+ ci,	lim	λ%	= +∞,	lim	Ei	= 0, Tr Mf.	‹ ∞
f∈S(Rn)	s→+∞	i→+∞	fi
and a = limi→+∞ Tf . From theorem 5 we obtain a ∈ Arg min I(f).
i	f∈Gk0
One can always find a subset {λdi} ⊆ {λi} such that λdi < λi, λdi → ∞ and obtain:
inf I(f) + λiR(f) ≥ inf I(f) + 晨,R(f) ≥
f∈S(Rn)	i	f∈S(Rn)	di
一，：、 T 一 ,:、	一：、
I (fdi ) + λdi R(fdi ) - edi ≥ I (fdi ) - edi
Therefore,
, ~ , ..
lim inf I (f) + λiR(f) ≥ lim If) - % = I (a) = inf I (f ) = I (f *)
i→∞ f∈S(Rn)	i→∞	f ∈Gk0
This proves that {fi} regularly solves (7) and limi→∞ f = f * i.e. f * ∈ rsol (I(f), R(f)).	□
E Proof of Theorem 5
Again we will prove a more general statement.
Theorem 11. If M is a proper and a real-valued kernel, O(M) is bounded and Tr Mf < ∞, then
Sf ∈ B(L*(Rn), Rn) and Sf S； = Mf. Moreover,
R(f)
min
S∈B(L*(Rn),Rn ),rank S≤k
||Sf-S||2*
and the minimum is attained at S = PfSf where Pf = Pik=1 uiui； and {ui}1k are unit eigenvectors
of Mf corresponding to the k largest eigenvalues (counting multiplicities).
Proof. The boundedness of Sf follows from the Cauchy-Schwarz inequality:
∣Sf[φ]i|2 = |Re hPθ(M)[xif],φi∣2 ≤ h,O(M)Ef], POM)Mf]ihΦ, Φi =
hxi f, O(M)[xif]ihφ, φi
and therefore:
n
||Sf [φ]ll2 = X |Sf [φ]/2 ≤ TrMf llφML2(Rn)
i=1
I.e. we have checked that Sf is bounded.
By definition, Sf； : Rn → Lr2 (Rn) × Lr2(Rn) and hu, Sf[φ1, φ2]i = hSf； [u], [φ1, φ2]i, u ∈
Rn, [φ1, φ2] ∈ Lr2(Rn) × Lr2(Rn). Let us denote f1 = Re f, f2 = Im f. It is easy to see that
the following operator satisfies the latter identity:
O[u] = [PO(M)[fι(x)xτu], Pθ(M)[f2(x)xτu]]
Since the adjoint is unique, then Sf； = O. Let us calculate Sf Sf； :
-hxιfι(x), Pθ(M)[PO(M)[fι(x)xτu]]i + hx1f2(x), PO(M)[PO(M)[f2(x)xτu]]i'
• ∙ ∙
Kxnfι(x), POWpOWfι(X)XT u]]i + hxnf2(x), POWpOWf2(X)XT u]]i.
21
Under review as a conference paper at ICLR 2021
-Pj=IhxIfj (X),O(M)[fj (X)XT u]i^
…	=[Re hxif, M [xjf ]i]1≤i,j≤n U = Mfu
Pj2=1hxnfj (X), O(M)[fj (X)XT u]i
Thus, SfSf = Mf. Since Tr Sf Sff < ∞ and ||S；[u]||2 ≤ {u, MfU)，we obtain Sjis a bounded
operator.
Let uι,…Un be orthonormal eigenvectors of Mf = Sf Sf and λι ≥ … ≥ λn > 0 be corre-
sponding nonzero eigenvalues. For σ% = √λi let Us define Vi = SfUil. Vector Vi corresponds to a
σi
pair of functions:
Vi = L [Pθ(M)[fι(χ)χτUi], POM)[f2(X)XTUi]] ∈ Lr(Rn) × Lr(Rn)
σi
It is easy to see that vι, •…7n is an orthonormal basis in ImSf, and Sf can be expanded in the
following way:
n0
Sff =	σiVi Uif
i=1
and therefore, SVD for Sf is:
0
n
Sf =	σi UiVif
i=1
By the Eckart-Young-Mirsky theorem (see Theorem 4.4.7 from Hsing & Eubank (2015)), an optimal
S in	min	||Sf - S||2 is defined by a truncation of SVD for S f at kth term, i.e.:
S∈B(L*(Rn),Rn),rank S≤k
k
S = X σiUiVif = PfSf	(16)
i=1
where Pf = Pik=1 UiUif is a projection operator to first k principal components of Mf. Moreover,
IISf - PfSf Il2 = PL+1 σ2 = ||Mf||n-k = R(f).
□
F THE ALTERNATING SCHEME IN THE DUAL SPACE FOR M(x, y) = ζ(x - y)
When M(X, y) = ζ(X - y), the alternating scheme 1 allows for a reformulation in the dual space.
By this We mean that in Scheme 1 We substitute φt for the original φt. If the primal Scheme 1 deals
with operators Sφ, S©.、, the dual version deals with vectors of functions ∖∣~Z^dφ, ∖β^"φ- -. The
substitution is based on the folloWing simple fact:
Theorem 12. If M(X, y) = ζ(X -y), ζ, ζ ∈ C(Rn) and ∀X ζ(X) > 0, then there exist constants c1
α^
and C2 such that ∣∣Sφ - Pt-ιSφt-J∣? = ci|| ∣∣祟-Pt-1-11∣2 |员式限九)and hxif ∣MIxjf i =
_ ʌ , ʌ
I ∂f ∂f ∖
c2h∂X^, ∂XjiL2,ζ(Rn)
Proof. Let f : Rn → C be such that IIxifIIL2 (Rn ) < ∞.
O(M)[ψ] = Z * ψ ⇒ F {O(M)[ψ]}〜Zψ ⇒ F {pθ(M)[ψ]}〜ζψl⇒ ⇒
Sf [ψ]i = Re hxif, pO(M)[ψ]i 〜Re hF{xif} , F {√O(M)[ψ]}i 〜
Λ^ I-	!- A^
Re hi∂xi,次ψi = Re hiEWi ,ψi
22
Under review as a conference paper at ICLR 2021
SinceSf[ψ]i = Re h(Sf)i, ψi
~ Re h(Sf )i, ψi, We obtain
I- Cl P
(Sf )i = κqζ∂Xi
Where κ is a constant.
Let Us now introduce a vector of functions Vf = [(Sf )ι, ∙∙∙ , (Sf )n]T ∈ Ln(Rn). Using
obtain [i = κ4i∂f, and therefore:
(17)
17 We
_ ʌ
Vf=Kq^
∂x
Thus, the expression ∣∣Sφ 一 Pt-ιSφt-ι ||? in the alternating scheme can be rewritten as:
ʌ
ʌ
llVΦ 一 Pt-1Vφt-1 llLn(Rn) ~ ZKqZdx 一 PtTKq ∂χ 1 11Ln(Rn) ~
ʌ
ʌ
11 11 ∂χ - Pj d⅛U l|2 llL2,Z(Rn )
The matrix Mf can also be calculated from f using the folloWing identity:


/ ，切，八一/	f、\	/ ∂f ^ ∂f ∂f ∂f
hxif,M[χjf]i = hxif,ζ * (Xjf )i ~ h∂χ^,ζ∂X^i = h西，∂X^iL2,ζ(Rn)
□
Let us introduce a function I such that I(f) = I(f). Then, we see that all steps in Scheme 1 can be
performed with φt rather than with φt, using the algorithm 2.
Informally, the dual algorithm works as follows: at each iteration t we compute a function φt adapt-
♦ ∙ . . i . / .∖ .	τ / ? ∖ ∖ i i . ∙ ∙ .	t ∙ ,r∙ιι, .1	ι ι ι	ι ∙ ,r∙ιι r∙
ing it to data (the term I ()) and adapting its gradient field to the rank reduced gradient field of
.ι	♦	1	i -	i`r` ♦ . i i	m ∙ . ∙ ιι	ι ?	7	El	. ι	ι
the previous t-1. For a sufficiently large T, it will converge and T ≈ T-1. Then, the second
^	、，，，，A4	44一，，，，
term in the last step will be approximately equal to λ∣∣ ∣∣-∂φT 一 PT-ιd∂φT∣∣2 ∣∣L 八的)，enforcing
α^α^^	α^
-∂φT ≈ PT-ι-φT for random X ~ ^Z-. Thus, gradients -φτ lie in a k-dimensional subspace
Ux	Ux	||Z||Ll	Ux
colPT-1. This last property is a characteristic property of functions from Fk.
Algorithm 2 The alternating scheme in the dual space.
PO《—0, φο《—0
for t = 1,	∙ ,T do
φt J argmin I(B) + A|| || -X - Pt-I ~∂-^ ||2 llL2,ζ(Rn)
Calculate Mt= hRe h ∂φt,需 iL2,ζ(Rn)i
Find {vi}n s.t. Mtvi =入”,人\ ≥ …≥ λn
Pt - Pk=1 VivT
Output: vι,	∙ , vk
G A numerical alternating scheme for MMD
G.1 S TRUCTURE OF F[Pk]
From theorems 1 and 2, F[Pk] ⊆ Fj. In fact, a famous theorem of Bochner (1932) gives us that
the Fourier transform of any positive finite Borel measure is a continuous positive definite function.
That is, if f ∈ F[P], then for any distinct yι,…，Ns ∈ Rn the matrix [f (yi 一 Zj)]切=万 is
23
Under review as a conference paper at ICLR 2021
positive Semidefinite. Since μ(Rn) = 1, We additionally have f (0) = 1. Let PDF denote the set of
all continuous positive definite functions on Rn and
Mk = {f ∈ PDF∣∃vι,..., Vk ∈ Rn,g : Rk → C s.t. f(x) = g(vT x,∙∙∙, VT x),f(0) = 1} (18)
Thus, the folloWing characterization of F[Pk] becomes evident.
Theorem 13. F[Pk] = Mk.
G.2 The dual form of MMD
h2|x|2
Let Us define another gaussian kernel Y(X) = e	2- = F[k]. Let Pdata(X) denote the charac-
T
teristic function of the random vector Xdata 〜μdata. By definition, Pdata(X) = E[eixdatax]=
N PPi=I eɪXi X. Thus, Pdata Y F [ [μdata] and μdata Y F[pdata].
Using the isometry property of the Fourier transform for L2 (Rn) and the convolution theorem, We
see that:
dMMD(μ,V) = ||k *μ 一 k * v||L2(Rn)Y ||Y(X)(F[μ](X) — F[v](X))1匕邯九)
Thus, from Theorem 13 We obtain that the task 6 is equivalent to:
||Pdata - q||L2 2 (Rn) → min
2,γ	q∈Mk
(19)
G.3 Algorithms for MMD
Let Πk : Gk → {1, +∞} and Mk : Fk → {1, +∞} be simple penalty functions:
Πk (φ) = 1, if φ ∈ Pk and Πk (φ) = ∞, otherWise
Mk(φ) = 1, if φ ∈ Mk and Mk (φ) = ∞, otherWise
Then, the task 6 is equivalent to:
I(O)= dMMD(μdata, φ)W(O) → jnf
φ∈Gk
From the result of the previous section We see that ifI(O) = I(O), then:
I(O) = ||Pdata- φML2,γ2(Rn)Mk (φ)
Thus, the Algorithm 3 is an adaptation of Algorithm 2 to MMD.
Algorithm 3 The alternating scheme in the dual space for MMD
P0{—0, qo《—0
for t = 1,…，T do
1	qt <- arg min RRn Y(X)2|Pdata(X) — q(X)FdX + λ RRn Z(X)|| ∂X — Pt-I d∂-1 ||2dX
q∈Mk
2	Calculate Mt = hh ∂Xi, ∂xj iL2,^(Rn)]
3	Find {vi}n s.t. MtVi = λ〃i, λι ≥ …≥ λ∏
4	Pt — Pk=ι ViVT
Output: L = SPan(V 1,…,Vk)
If the function Pdata is real-valued, then only real-valued functions can appear in the Algorithm 3.
This assumption can be satisfied by adding reflections of initial points to the dataset (after it Was
centered).
At step 1, We search over q given in the folloWing parameterized form:
nn
qθ(X) =	αicos(ωiTX)	(20)
i=1
24
Under review as a conference paper at ICLR 2021
where a% > 0 and PnnI α% = 1. In our implementation, We set 陞儿=1^ = Softmax([ui]i=mn)
and ui ’s are unconstrained. The number of neurons in a single layer neural network with a cosine
activation function, nn, is a hyperparameter. Let us denote parameters {ωi , ui }in=n1 by θ. It is easy
to see the function qθ is positive definite. Moreover, using Theorem 2 from Barron (1993), it can
be shown that a set of all such functions, i.e. the convex hull of {cos(ωTx)∣ω ∈ Rn}, is dense
in a set of real-valued functions from Mk . Though this parameterization is quite natural, finding
architectures with more expressive power in a space of real-valued positive definite functions is an
open problem.
Now, to minimize
Ψ(θ)= Z γ(χ)2∣Pdata(χ) - qθ (χ)∣2dχ + λ Z Z(x)∣∣ dqθ - Pt-1 dqθt-1 ∣∣2dx
Rn	Rn	∂x	∂x
with stochastic gradient descent methods (in our case, the Adam optimizer) we need to have an
unbiased estimator of
Vθ Ψ(θ) (X Ez~γ2 Vθ ∣Pdata(z) - qθ(z)∣2 + 近,-《V© || M W- R-1 ^-1(Z0)|l2
where z
function
〜 f denotes that the random vector Z is sampled according to the probability density
f(X)
JRn f(χ)dχ
Thus, a natural estimator of the gradient is:
-Xx VθIPdata(Zi) - qθ(Zi)I2 + A xx Vθ||dqθ^ - Pt-1 的θ-("”∣∣2
m	m	∂ χ	∂ χ	2
i=1	i=1
where {zi}m=1 ~iid γ2 and {ξi}m=1 ~iid Z.
The last important issue with the practical numerical algorithm is the calculation of Mt at step 2. It
is easy to see that:
Mt = Eχ~ζ ⅛(X) ∂B(X)T
In practice we sample χι,…,χι ~ Z and estimate Mt as follows:
Mt ≈ I Xx 黑(Xi)黑(Xi )t
The details of the numerical algorithm are given below 4. In all our experiments with MMD we set
Z = γ2.
A ■	♦八	A EI	♦	1	1	.,1	Γ∙ > m « l ʌ T T	.	T 7	7	CC
Algorithm 4 The numerical algorithm for MMD. Hyperparameters: λ, h, σ, m, l, α, β1 , β2 , nn.
P0《 0,θ0《 0
for t = 1,…，T do
while θ has not converged do
Sample {zi}m=I 〜iid γ2
Sample {ξi}m=ι ~iid Z
L ― ml pm=i IPdata (&) - qθ (&)|2 + ʌ P；^ ∣∣ dqθ(≡ - Pt-1 ^taX(G I∣2
θ <— Adam(VθL,θ, α, β1, β2)
θt 一 θ	.
Sample {Xi}i=ι ~iid Z
Calculate Mt = ɪ Pi=I d⅛^d⅛^T
Find {vi}n s.t. Mtvi = λiVi, λι ≥ …≥ λn
Pt - Pk=1 vivT
Output: vι,	∙ , vk
25
Under review as a conference paper at ICLR 2021
H A numerical alternating scheme for HM
H.1 The dual form of HM
Due to a well-known relationship between moments of the probability measure μ and its character-
istic function p, i.e. ismʤ …is = j? p(dX. , the task 7 is equivalent to:
XX λs	X |:Spdatar - Js吗	|2 → min	(21)
W ns 1≤iltis≤n dxil …dxis	dxil …dxis	q∈Mk
Note that the maximum mean discrepancy distance and the distance based on higher moments are
substantially different. Indeed, even if We set h as a large value (which makes h ≈ 0), the MMD
distance, unlike the HM distance, neglects higher order derivatives of the characteristic functions in
the neigbourhood of the origin. Moreover, from the dual form 21 it is clear that dHM(μdata, V) is a
degenerate case of a weighted Sobolev norm between characteristic functions of μdata and V.
H.2 Algorithms for HM
Analogously to the case of MMD we see that the task 7 is equivalent to:
I(φ) = dHM(μdata,Φ)2∏k(Φ) → Ef
φ∈Gk
and
s=1
Σ
1≤iι,…，is≤n
। ∂spdata(0)
∂Xiι ∙ ∙ ∙ ∂Xis
∂sφ(0)
∂xi1 • 一 ∂xis
I2Mk (Φ)
—
Thus, the Algorithm 5 is an adaptation of Algorithm 2 to HM.
Algorithm 5 The alternating scheme in the dual space for HM
for t = 1,…，T do
1	qt — argqmMfc p4=ι n pι≤iι,…,is≤n | ⅛a∂⅛
PtT dq5X-i ||2 dx
2	Calculate Mt = hhIxi,舞iL2,^(Rn)i
3	Find {v，}? s.t. MtVi = λiVi, λι ≥ …≥ Xn
4	Pt — Pk=ι ViVT
Output: L = Span(Vι,…,Vk)
∂sq(0)
∂Xil ∙∙∙∣Xis
|2 + λ RRn Z(X)11 裂 -
Again, as in a numerical algorithm for MMD, at step 1, we search over q given in the form 20. The
objective of step 1 can be represented as:
P04—0, qo4—0
—
4
Φ(θ)= X λ EiIL
s=1
,is 〜iidU(1,n) |
∂s (pd
ata 一 qθ)(0)
dx" •一 ∂Xis
I2 + ⅛√∣∣ ∂θ (z0)- Pt-I ⅛-i
(z0)||22
where U(1,n) is the discrete uniform distribution over {1, ∙∙∙ ,n}. To apply the stochastic gradient
descent methods we need to have an unbiased estimator of ▽6Φ(θ) which is equal to:
XX λsEiι,∙∙∙,is 〜iidU (1,n)Vθ I a* ∂data∙-∂θ )(O) I2 + 双，〜Z^ II ∂θ (z0) - Pt-1 * (z0)I∣2
x Xi -I	UXi	X ^x	X ^x
1s
Thus, a natural estimator of the gradient is:
XX 工 XX V I	∂ S(Pdata - qθ )(O)	[2 + ɪ XX V II ∂qθ (Si))
S= m1 = θ ∂Xa[s,i,1]∂Xa[s,i,2]…∂Xa[s,i,s]	m2	θ ∂X
- Pt-1
∂qθt-1 (ξi))
∂x
II22
where {a[s,i,j]}s=草,i=1,mij=1;* 〜iid U(1,n) and 依/m；〜iid Z. Overall, we obtain the fol-
lowing Algorithm 6.
26
Under review as a conference paper at ICLR 2021
Algorithm 6 The numerical
λ, {λs}s=1,4, m1,m2,l, α, β1,β2, nn.
algorithm for HM.
Hyperparameters:
P04—O, θ04—0
for t = 1,…，T do
while θ has not converged do
Sample {a[s, i,j]}s=彳,i=ι,mι,j=辜 ~iid U(I, n)
Sample 体}陞1 〜iid Z
L	√—	P4	ʌs PmI VJ _________ds (Pdata-qθ )(O)___ |2 + ɪ Pm2 VJI dqθ(Si))
乙 s = 1 mi 乙 i=1	θ 1 ∂Xa[s,i,i]∂Xa[s,i,2]…∂Xa[s,i,s] 1	+ m2 ^i=1 θ 11	∂x
P	dqθt-i (Si)) ||2
t -1	∂x	H 2
θ 一 Adam(VθL, θ, α, βι, β2)
—
θt 一 θ	八
Sample {χi}i=ι Jid Z
Calculate Mt = 1 Pi=I dqθ∂χ^^XIT
Find {vi}n s.t. Mtvi = λivi, λ1 ≥ …≥ λn
Pt - Pk=ι vivτ
Output: vι,	∙ , vk
I A numerical alternating scheme for WD
By Theorem 6, the task 13 is equivalent to min*∈Pk W(μ, μdata), or to the following task:
I(φ) → inf
φ∈Gk
where I(Tμ) = W(μ,μdata) if μ ∈ Pk and I(φ) = ∞, if otherwise. The alternating scheme 1 is
designed to solve the penalty form of the problem, i.e.
I(φ) + λR(φ) →
min
φ∈S (Rn)
which is equivalent to
W(Φ, μdata) + λR(φ) → min
ata	φ∈Sp(Rn)
where Sp (Rn) ⊆ S(Rn) is a set of Schwartz functions that can serve as pdf: φ(x) ≥ 0,
Rn φ(x)dx = 1. A numerical version of the alternating scheme requires additional specifications
on: a) how to minimize over φ at step 1, and b) how to estimate Mφt .
I.1	HOW TO MINIMIZE OVER φ?
In the case of WD, the minimization step of the alternating scheme makes the following:
φt4 arg min	W(φ, μdata) + λ∣∣Sφ - Pt-ISφt-ι∣∣2	(22)
φ∈Sp(Rn)	t-i
where Sf = PO(M)[xf (x)].
For a numerical implementation of that step we need to choose some family of functions that is
dense in Sp(Rn) (or, rich enough to approach the solution μ*). Following the tradition of GAN
research let us assume that the family is given in the following form2:
H = {φθ∣φθ(x) is Pdf of random vector gθ(z),z 〜p(z),θ ∈ Θ}	(23)
where {gθ ∣θ ∈ Θ} is a parameterized family of smooth functions (usually, a neural network) and
p(z) is some fixed distribution (usually, the gaussian distribution). Following Arjovsky et al. (2017),
we make the assumption 1. In a numerical algorithm we need an access to a procedure that samples
according to φθ (x), not the function itself.
Assumption 1. |血，(z0) - gθ(z)∣∣ ≤ L(θ,z)(∣∣θ0 - θ∣∣ + ∣∣z0 - z∣∣) where
EZ〜p(z)L(θ, Z) < +∞
2If H ⊆ S(Rn) is not satisfied, then We can choose He = {φθ * GRθ ∈ Θ} for a very small €.
27
Under review as a conference paper at ICLR 2021
Thus, instead of solving 22 we solve:
φt V-- arg min W(φ, μdata) + λllSφ - Pt-1Sφt-ι ||2
φ∈H
taking into account that φt-1 ∈ H.
The Kantorovich-Rubinstein duality theorem gives us that:
W (Φθ ,μdata)
max
f "lfχ∣l≤ι
Ex 〜μdata[f(x)] — Ez 〜p(z)[f(gθ (Z))]
which turns 22 into the following minimax task:
M J arg mH f fχ≤ιEx~μdata[f (X)I - Ez~p(Z)[f(gθ (Z))]+ λllSφ - Pt-1Sφt-1 |12
(24)
In practice, we choose a family of functions L = {fw|w ∈ W} and internal maximization is made
over w ∈ W with an additional penalty term that penalizes a violation of the Lipschitz condition:
∀X : ||fx || ≤ 1.
A family of minimax algorithms for the minimization of W(φθ,μemp) was developed in a series
of papers Arjovsky et al. (2017); Gulrajani et al. (2017); Wei et al. (2018). The standard minimax
scheme that gained popularity in GAN literature iterates two steps: a) niter times make a gradient
ascent over w ∈ W, b) make a gradient descent over θ. The task 24 can be viewed as a Wasser-
stein GAN with an additional regularization term λT(θ) where T(θ) = ||S@g - Pt-ιSφθ^ɪ ||2. To
adapt these algorithms to the minimization of our function, we only need to have an unbiased esti-
mator of the gradient ∂T. ThiS estimator is needed for the generator to make its gradient descent
step. The discriminator’s part of the algorithm (in which we maximize over Lipschitz functions
fw) can be set in a standard fashion — we choose Petzka et al. (2018)’s version, in which the term
max{0, ||d∂fw(ξx +(1 一 ξ)gθ(z))∣∣ — 1}2 enforces Lipschitz condition (see step (*) of the Algo-
rithm 7).
||x-y||2
Algorithm 7 Numerical algorithm for WD. We use M(x, y) = e n- and default values of
λ = 10, Λ = 100, ncritic = 5,m = 40, l = 10000n, α = 0.00001, βι = 0.5, β2 = 0.9
P0 V— 0, θ0 V— 0
for t = 1,…，T do
Minimax realization of min W(φθ, μemp) + λT(θ) (*):
θ
while θ has not converged do
for s = 1, ..., ncritic do
Discriminator updates w
SamPIe {zi}m=ι, {zi}m=ι 〜P(Z)	-
L V------mm Pm=I fw(gθ(zi)) + λPij f,“,Zj) (Ξ is defined in equation 25)
θ V Adam(VθL,θ, α, β1,β2)
θt V- θ
Realization of step (**):
SamPIe {Zi}i=ι, {zi}i=ι 〜P(Z)
Mt V- Pijgθt(zi)gθt(z0j)TM(gθt(zi),gθt(z0j))
Find {vi}? s.t. MtVi = λiv λι ≥ …≥ λn
Pt V- Pik=1viviT
Output： Vι,	∙ , Vk
I.2	HOW TO ESTIMATE 孺 AND Mφθt ?
Another important aspect of the numerical algorithm is the complexity of estimating the matrix
Mφθt at step (**). The following theorem shows that We only need to sample Z 〜P a sufficient
number of times to estimate ∂∂θ and Mφθt.
Theorem 14. If φθ is pdf of the random vector gθ(z), Z 〜p(z), then
28
Under review as a conference paper at ICLR 2021
∂T _E	∂Ξ(θ,z,z0)
丽=Ez,z0 〜P —∂θ—
Mφθ = Ez,z0 〜pgθ (z)gθ (z0)T M (gθ (z),gθ(z0))
where
Ξ(θ, z, z0) = (gθ(z) ∙ gθ(ZO))M(gθ(z), gθ(z0))-
2(gθ(z) ∙ Pt-Igθt-ι (ZO))M(gθ(z),gθt-ι (z0))
(25)
and RHS is well-defined.
I.2.1	DEFINITION OF H
Specifically, for robust PCA/outlier pursuit applications, we define φθ (x) as a probability density
function of the random vector a + b, where a, b are independent and a is the i-th column of
matrix θι ∈ Rn×N (where i 〜 U(1,N) is sampled uniformly from {1,…，N}), b = gθ2(c),
C 〜N(0,In) and gθ2 : Rn → Rn is a neural network with weights θ2. Thus, θ = (θ1,θ2). It
can be checked that H, defined in this way, satisfies the Assumption 1. We specifically introduce
the random vector a here because, according to Theorem 6, the ultimate solution of the problem
corresponds to θ1 = Y and b = 0. This guarantees that the solution is approachable from set H.
I.3	Proof of theorem 14
We need to following lemma.
Lemma 5. ∣∣Sφ - PSψ||2 = Eχ,y〜φ(x ∙ y)M(x,y) + Eχ,y〜ψ(X ∙ Py)M(x, y) - 2Eχ〜φ,y〜ψ(X ∙
Py)M(x, y)
Proof of lemma.
l∣Sφ - PSψIl2 = ∣pO(M)[xφ(x)] - PPθ(M)[xψ(x)]||2 =
n
=IlPOW[xφ(x) - Pxψ(x)]∣l2 = X ∣∣PO(M)[χiΦ(x) - (Px)iψ(x)]∣∣2 =
i=1
n
Xhxiφ(x)∣O(M)[xiφ(x)]i + h(P x)iψ(x)∣O(M)[(P x)iψ(x)]>-2((P x)iψ(x)∣O(M )[xiφ(x)]i =
i=1
Eχ,y〜φ(x ∙ y)M(x, y) + Eχ,y〜ψ(x ∙ Py)M(x, y) - 2Eχ〜φ,y〜ψ(x ∙ Py)M(x, y)
□
Proof of theorem 14. Using lemma 5 we have:
T(θ) = Eχ,y〜φθ (x ∙ y)M(x, y) + Eχ,y〜φθt-1 (x ∙ Pt-Iy)M(x, y)-
-2Eχ〜φθ,y〜φθt-1 (x ∙ Pt-Iy)M(x, y)=
Ez,z0〜p(gθ(z) ∙ gθ(z0))M(gθ(z),gθ(z0))+
EZN〜p(gθt-ι (Z) ∙ Pt-IgΘt-1 (ZO))M(gθt-ι (Z), gθt-ι (z0))-
2Ez,z，〜p(gθ(z) ∙ Pt-1gθt-1 (z0))M(gθ(z),gθt-ι (z0))
The second term does not depend on θ. Therefore,
∂T — ∂ 党	0、
而=∂θEz,z0〜PξCz, Z )
where
Ξ(θ,z,z0) = (gθ(z) ∙ gθ(z0))M(gθ(z),gθ(z0)) - 2(gθ(z) ∙ Pt-Igθ-ι (z0))M(gθ(z),gθt- (z0))
29
Under review as a conference paper at ICLR 2021
If Ez,z0~pd"∂,Z,z ) is well-defined (the proof of sufficiency of that condition is similar to the proof
of Theorem 3 from Arjovsky et al. (2017)), then, using Leibniz integral rule, we obtain:
∂	0	∂ Ξ(θ, z, z0)
∂θEz，z0~pU(0,z,z ) = Ez,z'~p —∂θ-
The fact that
Mφθ = Ez,z0~pgθ (z)gθ (z0)T M (gθ (z),gθ (z0))
is obvious from the definition Mg® = Eχ,y~φgXyTM(x, y).
□
J A numerical alternating scheme for SDR
For a binary classification case, given a labeled dataset {(Xi, yi)}iN=1, Xi ∈ Rn, yi ∈ C, C = {0, 1}
we formulate the sufficient dimension reduction problem as the minimization task:
J(f) = E(z,c)~μdata,W~N (O,υ2In)Z (°,于(Z + E))	' P^
where L(c, y) = -c log(y) - (1 - c) log(1 - y).
We apply the alternating scheme in the dual space (Algorithm 2) to this task. We set M(X, y) =
ζ(X - y), where ζ is a strictly positive probability density function. A numerical version of the
scheme is given below (Algorithm 8).
At every iteration t = 1,…，T of the Algorithm 2 We solve the task (in our case I = J):
ʌ
ʌ
φt J argminI(B)+ A|| |∂^ - Pt-I ∂χ 1 | |员 Z(Rn)
φ	∂ X	∂ X	2,ζ
In a numerical version of the algorithm we assume that φ is given as a neural network fθ , i.e. our
task becomes:
θ J arg min J (fθ) + λEξ~ζ“ ^∂χθ (ξ)- Pt-I 于∂χ- - (ξ)ll2
Thegradientofthefunction Φ(θ) = J(fθ) + *Eξ~ζ∣∣ f (ξ) - P1 f-(ξ)∣∣2 equals:
∂Φ(θ) _E	∂	鼐 ∂ ∂fθ	P	∂fθt-ι	2
∂θ = E(z,c)~Pdata ,€~N (0,U2In) d^L(c, fθ (Z + E)) + λEξ~Z 加 11 ∂χ (ξ) - Pt-I jχ (ξ)"
That is why VθL (given to Adam optimizer in the gradient descent loop) in the Algorithm 8 is an
unbiased estimator of dφθ. Thus, in the “while loop” we find optimal φt = fθt.
According to Algorithm 2, the next goal is to estimate Mt
see that
IRe h爵,翁＞L2,ζ(Rn)] .Itiseasyto
ʌ
ʌ
M=	E -dφt (Y) dφt	(Y)T	= E f (Y) f (Y)T
Mt	=	Eχ~ζ ∂X(X) ∂X	(χ)	= Ex~ζ ∂x (χ) ∂x	(χ)
1 -	. 1 1	. 1 ..1	. ∙ Tl ʃ	F	. ∙ . 1 1	1 ∙	£	i'Γ∙ ∙	Λ i'
From the last we see that the matrix Mt can be estimated by sampling X ~ Z a sufficient number of
times (the parameter l in our algorithm). All the rest is identical to Algorithm 2.
The regression version of the algorithm can be obtained by setting L(c,c0)=(c-c0)2.Implemen-
tations for different databases can be found at github.
30
Under review as a conference paper at ICLR 2021
A ■	♦八	C EI	∙ 1 1 .	. ∙	1	i` r. iʌ iʌ -»-v τ	r r∖	∖	/ ∖	ι
Algorithm 8 The numerical alternating scheme for SDR. We use υ = 1.0, ζ(x) = G0n 8(x) and
default values of λ = 10,m ≈ 50, m0 = 100, l = 30000, α = 0.0001, βι = 0.5, β2 = 0.9
P04—0, θ04—0
for t = 1, ∙∙∙ ,T do
while θ has not converged do
Sample {(zi, ci)}m=ι 〜Pdata
Sample 汨凄1 〜N(0,υ2In)
0
SamPIe {ξi}m=ι -Z
L — m Pm=ι L(Ci,fθ (Zi + J) + m pm=01 Il dfθ(ξ)) - Pt-1 f∂Xξl ∣F
θ 一 Adam(VθL, θ, α, βι, β2)
θt — θ
Sample {χi}* li=1 - ζ
Calculate Mt = ɪ	d⅛)) d⅛))T
Find {vi}n s.t. Mtvi =入限入、≥ …≥ λn
Pt - Pk=ι vivτ
Output: vι,	∙ , vk
31