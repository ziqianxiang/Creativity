Under review as a conference paper at ICLR 2021
CAFENet: Class-Agnostic Few-Shot
Edge Detection Network
Anonymous authors
Paper under double-blind review
Ab stract
We tackle a novel few-shot learning challenge, few-shot semantic edge detection,
aiming to localize boundaries of novel categories using only a few labeled sam-
ples. Reliable boundary information has been shown to boost the performance of
semantic segmentation and localization, while also playing a key role in its own
right in object reconstruction, image generation and medical imaging. Few-shot
semantic edge detection allows recovery of accurate boundaries with just a few
examples. In this work, we present a Class-Agnostic Few-shot Edge detection
Network (CAFENet) based on meta-learning strategy. CAFENet employs a se-
mantic segmentation module in small-scale to compensate for lack of semantic
information in edge labels. The predicted segmentation mask is used to generate
an attention map to highlight the target object region, and make the decoder mod-
ule concentrate on that region. We also propose anew regularization method based
on multi-split matching. In meta-training, the metric-learning problem with high-
dimensional vectors are divided into smaller subproblems with low-dimensional
sub-vectors. Since there are no existing datasets for few-shot semantic edge de-
tection, we construct two new datasets, FSE-1000 and SBD-5i , and evaluate the
performance of the proposed CAFENet on them. Extensive simulation results
confirm that the proposed CAFENet achieves better performance compared to the
baseline methods using fine-tuning or few-shot segmentation.
1 Introduction
Semantic edge detection aims to identify pixels that belong to boundaries of predefined categories.
Boundary information has been shown to be effective for boosting the performance of semantic
segmentation (Bertasius et al., 2016; Chen et al., 2016) and localization (Yu et al., 2018a; Wang
et al., 2015). It also plays a key role in applications such as object reconstruction (Ferrari et al.,
2007; Zhu et al., 2018), image generation (Isola et al., 2017; Wang et al., 2018) and medical imaging
(Abbass & Mousa, 2017; Mehena, 2019). Early edge detection algorithms interpret the problem as
a low-level grouping problem exploiting hand-crafted features and local information (Canny, 1986;
Sugihara, 1986). Recently, there have been significant improvements on edge detection thanks to the
advances in deep learning. Moreover, beyond previous boundary detection, category-aware semantic
edge detection became possible (Acuna et al., 2019; Hu et al., 2019; Yu et al., 2018b). However, it
is impossible to train deep neural networks without massive amounts of annotated data.
To overcome the data scarcity issue in image classification, few-shot learning has been actively
discussed for recent years (Finn et al., 2017; Lifchitz et al., 2019). Few-shot learning algorithms train
machines to learn previously unseen classification tasks using only a few relevant labeled examples.
More recently, the idea of few-shot learning is applied to computer vision tasks requiring highly
laborious and expensive data labeling such as semantic segmentation (Dong & Xing, 2018; Wang
et al., 2019) and object detection (Fu et al., 2019; Karlinsky et al., 2019). Based on meta-learning
across varying tasks, the machines can adapt to unencountered environments and demonstrate robust
performance in various computer vision problems. In this paper, we consider a novel few-shot
learning challenge, few-shot semantic edge detection, to detect the semantic boundaries using only
a few labeled samples. Through experiments, we show that few-shot semantic edge detection can
not be simply solved by fine-tuning a pretrained semantic edge detector or utilizing a nonparametric
edge detector in a few-shot segmentation setting. To tackle this elusive challenge, we propose a
class-agnostic few-shot edge detector (CAFENet) and present new datasets for evaluating few-shot
semantic edge detection.
1
Under review as a conference paper at ICLR 2021
Support Set
Query set
Feature Extractor	Segmentator	Edge Detector
Multi-scale
side output
Figure 1: Architecture overview of the proposed CAFENet. The feature extractor or encoder extracts feature
from the image, the segmentator generates a segmentation mask based on metric learning, and the edge detector
detects semantic boundaries using the segmentation mask and query features.
Fig. 1 shows the architecture of the proposed CAFENet. Since the edge labels do not contain
enough semantic information due to the sparsity of labels, performance of the edge detector severely
degrades when the training dataset is very small. To overcome this, We adopt the segmentation pro-
cess in advance of detecting edge with downsized feature and segmentation labels generated from
boundaries labels. We utilize a simple metric-based segmentator generating a segmentation mask
through pixel-wise feature matching with class prototypes, which are computed by masked average
pooling of (Zhang et al., 2018). The predicted segmentation mask provides the semantic information
to the edge detector. The multi-scale attention maps are generated from the segmentation mask, and
applied to corresponding multi-scale features. The edge detector predicts the semantic boundaries
using the attended features. Using this attention mechanism, the edge detector can focus on rele-
vant regions while alleviating the noise effect of external details. For meta-training of CAFENet,
we introduce a simple yet powerful regularization method, Multi-Split Matching Regularization
(MSMR), performing metric learning on multiple low-dimensional embedding sub-spaces during
meta-training.
The main contributions of this paper are as follows. First, we introduce a few-shot semantic edge
detection problem for performing semantic edge detection on previously unseen objects using only
a few training examples. Second, we introduce two new datasets of SBD-5i and FSE-1000 for
few-shot edge detection. Third, we propose a few-shot edge detector, CAFENet and validate the
performance of the proposed method through experiments.
2	Related Work
2.1	Few-shot Learning
To tackle the few-shot learning challenge, many methods have been proposed based on meta-
learning. Optimization-based methods (Finn et al., 2017; Ravi & Larochelle, 2016) train the meta-
learner which updates the parameters of the actual learner so that the learner can easily adapt to a new
task within a few labeled samples. Metric-based methods (Vinyals et al., 2016; Snell et al., 2017;
Yoon et al., 2019) train the feature extractor to assemble features from the same class together on the
embedding space while keeping features from different classes far apart. Recent metric-based ap-
proaches propose dense classification (Hou et al., 2019; Kye et al., 2020). Dense classification trains
an instance-wise classifier on pixel-wise classification loss which imposes coherent predictions over
the spatial dimension and prevents overfitting as a result. Our model adopts the metric-based method
for few-shot learning. Inspired by dense classification, we propose multi-split matching regulariza-
tion which divides the feature vector into sub-vector splits and performs split-wise classification for
regularization in meta-learning.
2.2	Few-shot Semantic S egmentation
The goal of few-shot segmentation is to perform semantic segmentation within afew labeled samples
based on meta-learning (Shaban et al., 2017; Dong & Xing, 2018; Wang et al., 2019). OSLSM of
(Shaban et al., 2017) adopts a two-branch structure: conditioning branch generating element-wise
scale and shift factors using the support set and segmentation branch performing segmentation with
a fully convolutional network and task-conditioned features. Co-FCN (Rakelly et al., 2018) also
utilizes a two-branch structure. The globally pooled prediction is generated using support set in
2
Under review as a conference paper at ICLR 2021
conditioning branch, and fused with query features to predict the mask in segmentation branch. SG-
One of (Zhang et al., 2018) proposes a masked average pooling to compute prototypes from pixels
of support features. The cosine similarity scores are computed between the prototypes and pixels
of query feature, and the similarity map guides the segmentation process. CANet of (Zhang et al.,
2019) also adopts masked average pooling to generate the global feature vector, and concatenate it
with every location of the query feature for dense comparison in predicting the segmentation mask.
PANet of (Wang et al., 2019) introduces prototype alignment, predicting the segmentation mask of
support samples using query prediction results as labels of query samples, for regularization. PMM
of Yang et al. (2020) utilizes multiple prototypes generated using Expectation-Maximization (EM)
algorithm to effectively leverage the semantic information from the few labeled samples.
2.3	Semantic Edge Detection
Semantic edge detection aims to find the boundaries of objects from image and classify the objects at
the same time. The history of semantic edge detection (Acuna et al., 2019; Hu et al., 2019) dates back
to the work of (Prasad et al., 2006) which adopts the support vector machine as a semantic classifier
on top of the traditional canny edge detector. Recently, many semantic edge detection algorithms
rely on deep neural network and multi-scale feature fusion. CASENET of (Yu et al., 2017) addresses
the semantic edge detection as a multi-label problem where each boundary pixel is labeled into
categories of adjacent objects. Dynamic Feature Fusion (DFF) of (Hu et al., 2019) proposes a novel
way to leverage multi-scale features. The multi-scale features are fused by weighted summation with
fusion weights generated dynamically for each images and each pixel. Meanwhile, Simultaneous
Edge Alignment and Learning (SEAL) of (Yu et al., 2018b) deals with severe annotation noise of
the existing edge dataset (Hariharan et al., 2011). SEAL treats edge labels as latent variables and
jointly trains them to align noisy misaligned boundary annotations. Semantically Thinned Edge
Alignment Learning (STEAL) of (Acuna et al., 2019) improves the computation efficiency of edge
label alignment through a lightweight level set formulation.
3	Problem Setup
For few-shot semantic edge detection, we use train set Dtrain and test set Dtest consisting of non-
overlapping categories Ctrain and Ctest. The model is trained only using Ctrain, and the test cat-
egories Ctest are never seen during the training phase. For meta-training of the model, we adopt
episodic training as done in many previous few-shot learning works. Each episode is composed of
a support set with a few-labeled samples and a query set. When an episode is given, the model
adapts to the given episode using the support set and detect semantic boundaries of the query set. By
episodic training, the model is learned so that it adapts to the unseen class using only a few labeled
samples and predict semantic edges of query samples.
For Nc-way Ns-shot setting, each training episode is constructed by Nc classes sampled from
Ctrain . When Nc categories are given, Ns support samples and Nq query samples are randomly
chosen from Dtrain for each class. In evaluation, the performance of the model is evaluated using
test episodes. The test episodes are constructed in the same way as the training episodes, except Nc
classes and corresponding support and query samples are sampled from Ctest and Dtest .
In this work, we address Nc-way Ns-shot semantic edge detection. The goal is training the model
to generalize to Nc unseen classes given only Ns images and their edge labels. Based on the few
labeled support samples, the model should produce edge predictions of query images which belong
to Nc unencountered classes.
4	Method
We propose a novel algorithm for few-shot semantic edge detection. Fig. 2 illustrates the network
architecture of the proposed method. The proposed CAFENet adopts the semantic segmentation
module to compensate for the lack of semantic information in edge labels. The predicted segmenta-
tion mask is utilized for attention in skip connection. The final edge detection is done using attentive
multi-scale features.
4.1	Semantic Segmentator
Most previous works on semantic edge detection directly predict edges from the given input im-
age. However, direct edge prediction is a hard task when only a few labeled samples are given. To
overcome this difficulty in few-shot edge detection, we adopt a semantic segmentation module in ad-
vance of edge prediction. With the assistance of the segmentation module, CAFENet can effectively
3
Under review as a conference paper at ICLR 2021
Query sample
320 X 320
E(1)
160 X 160
J
E⑵
80 X 80
.
E(3)
40 X 40
E(4)
20 X 20
Figure 2: Network architecture overview of proposed CAFENet. ResNet-34 encoder E(I)〜E⑷ extracts
multi-level semantic features. The Segmentator module generates a segmentation prediction using query fea-
ture from E(4) and prototypes Pfg, PBG from support set features. Small bottleneck blocks S(O) 〜 S(4)
transform the original image and multi-scale features from encoder blocks to be more suitable for edge detec-
tion. The attention maps generated from segmentation prediction are applied to multi-scale features to localize
the semantically related region. Decoder D(O)〜 D(4) takes attentive multi-scale features to give edge predic-
tion.
PrediCtion
localize the target object and extract semantic features from query samples. For few-shot segmen-
tation, we employ the metric-learning which utilizes prototypes for foreground and background as
done in (Dong & Xing, 2018; Wang et al., 2019). Given the support set S = {xis, yis}iN=s1, the en-
coder E extracts features {E(xis)}iN=s1 from S. Also, for support labels {yis}iN=s1, we generate the
dense segmentation mask {Mis }iN=s1 using a rule-based preprocessor, considering the pixels inside
the boundary as foreground pixels in the segmentation label. Using down-sampled segmentation
labels {mis}iN=s1, the prototype for foreground pixels PFG is computed as
PFG = NH×1W XXEj(xis)mis,j	(1)
s	ij
where j indexes the pixel location, Ej (x) and mis,j denote the jth pixel of feature E(x) and segmen-
tation mask mis. H, W denote height and width of the images. Likewise, the background prototype
PBG is computed as
PBG=N h 1 w xx Ej (Xs)(I - ms,j).	⑵
s	ij
Following the prototypical networks of (Snell et al., 2017), the probability that pixel j belongs to
foreground for the query sample xiq is
p(yq , = FGIxq. E) =___________exp(-τd(Ej(Xq),pfg))______________ (3)
i,j	i,	exp(-τd(Ej (xq ),Pfg)) + exp(-τd(Ej (xq ),Pbg))
where d(∙, ∙) is squared Euclidean distance between two vectors and T is a learnable temperature
parameter. With query samples {xiq }iN=q1 and the down-sampled segmentation labels for query
{miq }iN=q1 , the segmentation loss LSeg is calculated as the mean-squared error (MSE) loss between
predicted probabilities and the down-sized segmentation mask
1	1	Nq H×W
LSeg = NH X W X X {(p(yq,j = FGIxq；E) - ml/}.	(4)
q ×	i=1 j=1
Note that the segmentation mask is generated in down-sized scale so that any pixel near the bound-
aries can be classified into the foreground to some extent, as well as the background. Therefore, we
approach the problem as a regression using MSE loss rather than cross entropy loss.
4.2 Multi-Split Matching Regularization
The metric-based few-shot segmentation method utilizes distance metrics between the high-
dimensional feature vectors and prototypes, as seen in Fig. 3a. However, this approach is prone
4
Under review as a conference paper at ICLR 2021
Background
prototype
Pixel-wise
matching
Foreground
prototype
Query feature
(a) High-dimensional Matching
Foreground
prototype
Split-wise
matching
Background
prototype
(b) Split-wise Matching
Figure 3: Comparison between (a) High-dimensional feature matching used in (Dong & Xing, 2018; Wang
et al., 2019) and (b) split-wise feature matching in MSMR
to overfit due to the massive number of parameters in feature vectors. To get around this issue,
We propose a novel regularization method, multi-split matching regularization (MSMR). In MSMR,
high-dimensional feature vectors are split into several low-dimensional feature vectors, and the met-
ric learning is conducted on each vector split as Fig. 3b.
With the query feature E(Xq) ∈ RC×W×H, where C is channel dimension and H, W are spatial
dimensions, We divide E(Xq) into K sub-vectors {Ek(Xq)}3ι along channel dimension. Each
sub-vector Ek(Xq) is in RC ×W×h. Likewise, the prototypes PFG and PBG are also disassembled
into K sub-vectors {P%g}3i and {Pjko}k=ι along channel dimension where PFkG, PBG ∈ RC
.For the kth sub-vector of query feature Ek(Xq), the probability that the jth pixel belongs to the
foreground class is computed as follows:
Pk % = FG∣χq ； E)
___________exp(τd(Ej(x( ),PFg))_________
exp(-τd(Ej (Xq ),PFg ))+ exp(-τd(Ej (x? ),PBg))
(5)
MSMR divides the original metric learning problem into K small sub-problems composed of a fewer
parameters and acts as regularizer for high-dimensional embeddings. The prediction results of K
sub-problems are reflected on learning by combining the split-wise segmentation losses to original
segmentation loss in Eq. 4. The total segmentation loss is calculated as
1	1 Nq H ×W	K
LSeg = NH×W X X {(Pij- mq,j )2 + X(Pkj- mq,j)2}.	⑹
q	i=1 j=1	i=1
where pi,j = p(yiq,j = FG|Xiq;E) and pik,j = pk(yiq,j = F G|Xiq; E).
4.3 Attentive Edge Detector
As shown in Fig. 2, we adopt the nested encoder structure to extract rich hierarchical features. The
multi-scale side outputs from encoder E⑴ 〜E(4) are post-processed through bottleneck blocks
S⑴ 〜S(4). Since ResNet-34 gives side outputs of down-sized scale, we pass the original image
through bottleneck block S(0) to extract local details in original scale. In front of S(3), we employ
the Atrous Spatial Pyramid Pooling (ASPP) block of (Chen et al., 2017). We have empirically found
that locating ASPP there shows better performance.
In utilizing multi-scale features, we employ the predicted segmentation mask M from the segmen-
tator where the jth pixel of MM is the predicted probability from Eq. 3. Note that we generate M
based on the entire feature vectors and the prototypes instead of utilizing sub-vectors, since the split-
wise metric learning is used only for regularizing the segmentation module. For each layer l, M(l)
denotes the segmentation mask upscaled to the corresponding feature size by bilinear interpolation.
Using segmentation prediction mask M(l), we generate attention map A(l), as follows. First, the
prediction with a value lower than threshold λ is rounded down to zero, to ignore activation in re-
gions with low confidence. Second, we broaden the attention map using morphological dilation of
(Feng et al., 2019) as a second chance, since the segmentation module may not always guarantee
fine results. The final attention map of lth layer A(l) is computed as follows
A(I) = 1(mM(I) > λ)M(I) + Dilation(I(M(I) > λ)M⑴)	(7)
where 1(M(l) > λ)M(l) is the rounded value of prediction mask M(l). The attention maps are
applied to the multi-scale features of corresponding bottleneck blocks S(O)〜S(4). We apply the
5
Under review as a conference paper at ICLR 2021
Input image
Input image
I 3 ,,
J E(I) H S(I) J-►
64
I EZ) p⅜pS⑵ I-►
128
I EA ∣→p^ ⑶ I-►
256
Encoder side-output	Pixel-wise semantic attention	Decoder input
Figure 4: An example of activation map of (Yosinski et al., 2015) before and after pixel-wise semantic attention
(warmer color has higher value). As seen, the attention mechanism makes encoder side-outputs attend to the
regions of the target object (horse in the figure).
residual attention of (Hou et al., 2019), where the initial multi-level side outputs from S(I) are pixel-
wisely weighted by 1 + A(l), to strengthen the activation value of the semantically important region.
We visualize the effect of semantic attention in Fig. 4.
As shown in Fig. 2, the decoder network is composed of five consecutive convolutional blocks.
Each decoder block Da) contains three 3 X 3 convolution layers. The outputs of decoder blocks
D(I)〜 D⑷ are bilinearly upsampled by two and passed to the next block. Similar to (Feng et al.,
2019), the up-sampled decoder outputs are then concatenated to the skip connection features from
bottleneck blocks S(O)〜S(4) and previous decoder blocks. Multi-scale semantic information and
local details are transmitted through skip architectures. The hierarchical decoder network in turn
refines the outputs of the previous decoder blocks and finally produces the edge prediction yf of
query samples xiq .
Following the work of (Deng et al., 2018), we combine cross-entropy loss and Dice loss to produce
crisp boundaries. Given a query set Q = {x，, yf }匕 and prediction mask yf, the cross-entropy loss
is computed as
Nq
LCE = - X{ X log(yq) + X IOg(I- yq)}	(8)
i=1 j∈Y+	j∈Y-
where Y+ and Y- denote the sets of foreground and background pixels.
computed as
Nq
LDice =	{
i=1
Pj (yq)+Pj/J
2 Pj yq,j yq,j
The Dice loss is then
(9)
}
where j denotes the pixels of a label. The final loss for meta-training is given by
Lfinal = LSeg + LCE + LDice.
(10)
5	Experiments
5.1	Datasets
5.1.1	SBD-5i
Based on the SBD dataset of (Hariharan et al., 2011) for semantic edge detection, we propose a
new SBD-5i dataset. With reference to the setting of Pascal-5i, 20 classes of the SBD dataset
are divided into 4 splits. In the experiment with split i, 5 classes in the ith split are used as test
classes Ctest. The remaining 15 classes are utilized as training classes Ctrain. The training set
Dtrain is constructed with all image-annotation pairs whose annotation include at least one pixel
from the classes in Ctrain . For each class, the boundary pixels which do not belong to that class are
considered as background. The test set Dtest is also constructed in the same way as Dtrain , using
Ctest this time. Considering the difficulty of few-shot setting and severe annotation noise of the
SBD dataset, we extract thicker edges. We utilize edges extracted from the segmentation mask as
ground truth instead of original boundary labels of the SBD dataset, and thickness of extracted edge
lies between 3 〜4 pixels on average. We conduct 4 experiments with each split of i = 0 〜3, and
report performance of each split as well as the averaged performance. Note that unlike Pascal-5i,
we do not consider division of training and test samples of the original SBD dataset. As a result, the
images in Dtrain might appear in Dtest with different annotation from class in Ctest .
6
Under review as a conference paper at ICLR 2021
5.1.2	FSE-1000
The datasets used in previous semantic edge detection research such as SBD of (Hariharan et al.,
2011) and Cityscapes of (Cordts et al., 2016) are not suitable for few-shot learning as they have
only 20 and 30 classes, respectively. We propose a new dataset for few-shot edge detection, which
we call FSE-1000, based on FSS-1000 of (Wei et al., 2019). FSS-1000 is a dataset for few-shot
segmentation and composed of 1000 classes and 10 images per class with foreground-background
segmentation annotation. From the images and segmentation masks of FSS-1000, we build FSE-
1000 by extracting boundary labels from segmentation masks. As done in SBD-5i , we extract thick
edges of which thickness is around 2 〜3 pixels on average in the light of difficulty associated
with few-shot setting. For dataset split, we split 1000 classes into 800 training classes and 200 test
classes. We will provide the detailed class configuration in the Supplementary Material.
5.2	Evaluation Settings
We use two evaluation metrics to measure the few-shot semantic edge detection performance of
our approach: the Average Precision (AP) and the maximum F-measure (MF) at optimal dataset
scale (ODS). In evaluation, we compare the unthinned raw prediction results and the ground truths
without Non-Maximum Suppression (NMS) following (Acuna et al., 2019; Yu et al., 2018b). For the
evaluation of edge detection, an important parameter is matching distance tolerance which is an error
threshold between the prediction result and the ground truth. Prior works on edge detection such as
(Acuna et al., 2019; Hariharan et al., 2011; Yu et al., 2017; 2018b) adopt non-zero distance tolerance
to resolve the annotation noise. However, the proposed datasets for few-shot edge detection utilize
thicker boundaries to overcome the annotation noise issue instead of adopting distance tolerance.
Moreover, evaluation with non-zero distance tolerance requires additional heavy computation. This
becomes more problematic under few-shot setting where the performance should be measured on
the same test image multiple times due to the variation in the support set. For these reasons, we
set distance tolerance to be 0 for both FSE-1000 and SBD-5i. In addition, we evaluate the positive
predictions from the area inside an object and zero-padded region as false positives, which is stricter
than the evaluation protocol in prior works of (Hariharan et al., 2011; Yu et al., 2017).
5.3	Implementation Detail
We implement our framework using Pytorch library and adopt Scikit-learn library to construct the
precision-recall curve and compute average precision (AP). For the encoder, ResNet-34 pretrained
on ImageNet is adopted. All parameters except the encoder parameters are learned from scratch.
The entire network is trained using the Adam optimizer of (Kingma & Ba, 2014) with weight decay
regularization of (Loshchilov & Hutter, 2017). In both experiments on FSE-1000 and SBD-5i , we
use a learning rate of 10-4 and an l2 weight decay rate of 10-2. For FSE-1000 experiments, the
model is trained with 40,000 episodes and the learning rate is decayed by 0.1 after training 38,000
episodes. For SBD-5i experiments, 30,000 episodes are used for training, and the learning rate is
decayed by 0.1 after training 28,000 episodes. Higher shot training of (Liu et al., 2019) is employed
in 1-shot experiments for both datasets.
5.3.1	Data preprocessing
During training, we adopt data augmentation with random rotation by multiples of 90 degrees for
both SBD-5i and FSE-1000. We additionally resize SBD-5i data to 320×320, while no such resizing
is performed on FSE-1000. During evaluation, images of SBD-5i are zero-padded to 512×512.
Again, the original image size is used for FSE-1000.
5.4	Experiment Result
Table 1 shows the experiment results on the SBD-5i dataset. To verify the value of the proposed
method, we compare CAFENet with two baselines. The first baseline is a fine-tuned edge detection
model with only a few labeled samples. Meta-learning strategy is not used for the first baseline.
We employ the DFF of (Hu et al., 2019), utilizing the implementation offered by the authors. For
each split of SBD-5i, we pretrain a 15-way semantic edge detector with training classes and fine-
tune the pretrained edge detector with a few labeled samples for new classes in test split. During
pretraining, we follow the training strategies and hyperparameters of (Hu et al., 2019). In fine-
tuning, we randomly initialize some sub-modules that are closely related to final prediction (”side5”,
”side5-w”, and ”ada-learner”) and train them altogether using the support images.
7
Under review as a conference paper at ICLR 2021
The second baseline is constructed by combining a non-parametric edge detectors such as Canny
detector or Sobel detector with a few-shot segmentation algorithm. The semantic edge detection is
occasionally interpreted as a dual task of semantic segmentation, but prior work of (Acuna et al.,
2019) verifies that the semantic edge detector outperforms the segmentator combined with the Sobel
edge detector, and demonstrates the importance of semantic edge detection. In our experiments,
we combine PANet and PMM, the state-of-the-art few-shot segmentation method, with the Sobel
edge detector. To experiment with existing segmentation methods, we utilize the implementation
provided by the authors. For each split of SBD-5i, we meta-train the PANet and PMM on training
classes using the segmentation labels. In evaluation, we obtain the edge predictions of test classes
by applying the Sobel edge detector on the segmentation masks as done in (Acuna et al., 2019), and
compare the predictions with the edge labels of test classes.
We utilize the ResNet-34 backbone for CAFENet and PANet. For PMM, we utilize ResNet-50
backbone. We also employ higher shot training in 1-shot experiments for both baselines as done
in CAFENet experiments. The results in Table 1 show that the proposed CAFENet outperforms
all baselines in both MF and AP scores by significant margin. The experiment results prove that
the few-shot semantic edge detector can not be simply substituted by few-shot segmentator or the
fine-tuned semantic edge detector. In Table 2, the experiment results on the FSE-1000 dataset are
shown. For FSE-1000, we only experiment with the few-shot segmentation baseline since it is hard
to train a semantic edge detector with training set of FSE-1000, due to the large number of training
classes. We can see that the proposed CAFENet outperforms the baseline even when the dataset
contains more diverse classes.
Table 1: Evaluation results of proposed CAFENet on SBD-5i. 1000 randomly sampled test episodes are used
for evaluation. MF and AP scores are measured by %
Metric ∣ Method (5-shot) ∣ SBD-50 ∣ SBD-51 ∣ SBD-52 ∣ SBD-53 11 Mean
MF (ODS)	DFF + Finetune PANet + Sobel PMM + Sobel CAFENet (Ours)	9.25 19.66 31.73 34.71	8.63 23.48 29.99 36.81	8.08 21.10 29.91 32.02	7.83 18.09 26.03 28.37	8.45 20.58 29.42 32.98
	DFF + Finetune	9.17	6.77	7.04	6.14	7.28
AP	PANet + Sobel	11.91	14.13	11.92	9.42	11.85
	PMM + Sobel	23.26	20.76	20.38	17.94	20.59
	CAFENet (Ours)	30.47	32.40	27.01	23.06	28.24
Metric	Method (1-shot)	SBD-50	SBD-51 I SBD-52 I SBD-53 H Mean			
	DFF + Finetune	8.87	5.54	4.91	2.95	5.57
MF	PANet + Sobel	19.33	23.01	20.79	17.62	20.19
(ODS)	PMM + Sobel	31.18	29.23	29.38	25.65	28.86
	CAFENet (Ours)	31.54	34.75	29.47	26.68	30.61
	DFF + Finetune	7.91	3.71	3.31	1.55	4.12
AP	PANet + Sobel	12.32	14.39	12.13	9.76	12.15
	PMM + Sobel	22.77	20.21	19.85	17.56	20.10
	CAFENet (Ours)	26.81	29.08	23.77	20.44	25.03
Table 2: 1-way 1-shot and 1-way 5-shot results of proposed CAFENet on FSE-1000. 1000 randomly sampled
test episodes are used for evaluation. MF and AP scores are measured by %
Metric	Method	1-way 1-shot	1-way 5-shot
MF (ODS)	PANet + Sobel PMM + Sobel CAFENet (Ours)	38.38 36.31 58.47	38.78 36.46 60.63
AP	PANet + Sobel PMM + Sobel CAFENet (Ours)	27.85 31.22 60.54	28.11 31.82 63.92
8
Under review as a conference paper at ICLR 2021
6	Conclusion
In this paper, we establish the few-shot semantic edge detection problem. We proposed the Class-
Agnostic Few-shot Edge detector (CAFENet) based on a skip architecture utilizing multi-scale fea-
tures. To compensate the shortage of semantic information in edge labels, CAFENet employs a
segmentation module in low resolution and utilizes segmentation masks to generate attention maps.
The attention maps are applied to multi-scale skip connection to localize the semantically related
region. We also present the MSMR regularization method splitting the feature vectors and pro-
totypes into several low-dimension sub-vectors and solving multiple metric-learning sub-problems
with the sub-vectors. We built two novel datasets of FSE-1000 and SBD-5i well-suited to few-shot
semantic edge detection. Experimental results demonstrate that the proposed method significantly
outperforms the baseline approaches relying on fine-tuning or few-shot semantic segmentation.
References
Husein Hadi Abbass and Zainab Radhi Mousa. Edge detection of medical images using markov
basis. Applied Mathematical Sciences,11(37):1825-1833, 2017.
David Acuna, Amlan Kar, and Sanja Fidler. Devil is in the edges: Learning semantic boundaries
from noisy annotations. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 11075-11083, 2019.
Gedas Bertasius, Jianbo Shi, and Lorenzo Torresani. Semantic segmentation with boundary neural
fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
3602-3610, 2016.
John Canny. A computational approach to edge detection. IEEE Transactions on pattern analysis
and machine intelligence, (6):679-698, 1986.
Liang-Chieh Chen, Jonathan T Barron, George Papandreou, Kevin Murphy, and Alan L Yuille.
Semantic image segmentation with task-specific edge detection using cnns and a discriminatively
trained domain transform. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4545-4554, 2016.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):
834-848, 2017.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban
scene understanding. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 3213-3223, 2016.
Ruoxi Deng, Chunhua Shen, Shengjun Liu, Huibing Wang, and Xinru Liu. Learning to predict
crisp boundaries. In Proceedings of the European Conference on Computer Vision (ECCV), pp.
562-578, 2018.
Nanqing Dong and Eric Xing. Few-shot semantic segmentation with prototype learning. In BMVC,
volume 3, 2018.
Mengyang Feng, Huchuan Lu, and Errui Ding. Attentive feedback network for boundary-aware
salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1623-1632, 2019.
Vittorio Ferrari, Loic Fevrier, Frederic Jurie, and Cordelia Schmid. Groups of adjacent contour
segments for object detection. IEEE transactions on pattern analysis and machine intelligence,
30(1):36-51, 2007.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126-1135. JMLR. org, 2017.
9
Under review as a conference paper at ICLR 2021
Kun Fu, Tengfei Zhang, Yue Zhang, Menglong Yan, Zhonghan Chang, Zhengyuan Zhang, and Xian
Sun. Meta-ssd: Towards fast adaptation for few-shot object detection with meta-learning. IEEE
ACCeSS,7:77597-77606, 2019.
Bharath Hariharan, Pablo Arbelaez, LUbomir Bourdev, Subhransu Maji, and Jitendra Malik. Se-
mantic contours from inverse detectors. In 2011 International ConferenCe on Computer ViSion,
pp. 991-998. IEEE, 2011.
Ruibing Hou, Hong Chang, MA Bingpeng, Shiguang Shan, and Xilin Chen. Cross attention network
for few-shot classification. In AdvanCeS in Neural Information ProCeSSing SyStemS, pp. 4005-
4016, 2019.
Yuan Hu, Yunpeng Chen, Xiang Li, and Jiashi Feng. Dynamic feature fusion for semantic edge
detection. arXiv preprint arXiv:1902.09104, 2019.
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with
conditional adversarial networks. In ProCeedingS of the IEEE ConferenCe on Computer viSion and
pattern reCognition, pp. 1125-1134, 2017.
Leonid Karlinsky, Joseph Shtok, Sivan Harary, Eli Schwartz, Amit Aides, Rogerio Feris, Raja
Giryes, and Alex M Bronstein. Repmet: Representative-based metric learning for classification
and few-shot object detection. In ProCeedingS of the IEEE ConferenCe on Computer ViSion and
Pattern ReCognition, pp. 5197-5206, 2019.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Seong Min Kye, Hae Beom Lee, Hoirin Kim, and Sung Ju Hwang. Transductive few-shot learning
with meta-learned confidence, 2020.
Yann Lifchitz, Yannis Avrithis, Sylvaine Picard, and Andrei Bursuc. Dense classification and im-
planting for few-shot learning. In ProCeedingS of the IEEE ConferenCe on Computer ViSion and
Pattern ReCognition, pp. 9258-9267, 2019.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sungju Hwang, and Yi Yang.
Learning to propagate labels: Transductive propagation network for few-shot learning. In In-
ternational ConferenCe on Learning RepreSentationS, 2019. URL https://openreview.
net/forum?id=SyVuRiC5K7.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
J Mehena. Medical image edge detection using modified morphological edge detection approach.
2019.
Mukta Prasad, Andrew Zisserman, Andrew Fitzgibbon, M Pawan Kumar, and Philip HS Torr. Learn-
ing class-specific edges for object detection and segmentation. In Computer ViSion, GraphiCS and
Image ProCeSSing, pp. 94-105. Springer, 2006.
Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alyosha Efros, and Sergey Levine. Conditional
networks for few-shot semantic segmentation. 2018.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for
semantic segmentation. arXiv preprint arXiv:1709.03410, 2017.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
AdvanCeS in neural information proCeSSing SyStemS, pp. 4077-4087, 2017.
Kokichi Sugihara. MaChine interpretation of line drawingS. The Massachusetts Institute of Tech-
nology, 1986.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In AdvanCeS in neural information proCeSSing SyStemS, pp. 3630-3638, 2016.
10
Under review as a conference paper at ICLR 2021
Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot im-
age semantic segmentation with prototype alignment. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 9197-9206, 2019.
Shenlong Wang, Sanja Fidler, and Raquel Urtasun. Lost shopping! monocular localization in large
indoor spaces. In Proceedings of the IEEE International Conference on Computer Vision, pp.
2695-2703, 2015.
Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-
resolution image synthesis and semantic manipulation with conditional gans. In Proceedings of
the IEEE conference on computer vision and pattern recognition, pp. 8798-8807, 2018.
Tianhan Wei, Xiang Li, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. Fss-1000: A 1000-class
dataset for few-shot segmentation. arXiv preprint arXiv:1907.12347, 2019.
Boyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye. Prototype mixture models for
few-shot semantic segmentation. arXiv preprint arXiv:2008.03898, 2020.
Sung Whan Yoon, Jun Seo, and Jaekyun Moon. Tapnet: Neural network augmented with task-
adaptive projection for few-shot learning. arXiv preprint arXiv:1905.06549, 2019.
Jason Yosinski, Jeff Clune, Anh Nguyen, Thomas Fuchs, and Hod Lipson. Understanding neural
networks through deep visualization. arXiv preprint arXiv:1506.06579, 2015.
Xin Yu, Sagar Chaturvedi, Chen Feng, Yuichi Taguchi, Teng-Yok Lee, Clinton Fernandes, and
Srikumar Ramalingam. Vlase: Vehicle localization by aggregating semantic edges. In 2018
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pp. 3196-3203.
IEEE, 2018a.
Zhiding Yu, Chen Feng, Ming-Yu Liu, and Srikumar Ramalingam. Casenet: Deep category-aware
semantic edge detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 5964-5973, 2017.
Zhiding Yu, Weiyang Liu, Yang Zou, Chen Feng, Srikumar Ramalingam, BVK Vijaya Kumar, and
Jan Kautz. Simultaneous edge alignment and learning. In Proceedings of the European Confer-
ence on Computer Vision (ECCV), pp. 388-404, 2018b.
Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. Canet: Class-agnostic segmen-
tation networks with iterative refinement and attentive few-shot learning. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 5217-5226, 2019.
Xiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas Huang. Sg-one: Similarity guidance network
for one-shot semantic segmentation. arXiv preprint arXiv:1810.09091, 2018.
Dongchen Zhu, Jiamao Li, Xianshun Wang, Jingquan Peng, Wenjun Shi, and Xiaolin Zhang. Seman-
tic edge based disparity estimation using adaptive dynamic programming for binocular sensors.
Sensors, 18(4):1074, 2018.
A	Additional Experimental Setup
In this section, we provide detailed information about experimental setup. We adopt the ImageNet
pretrained ResNet-34 with 64-128-256-512 channels for each residual block from [pytorch frame-
work] as the encoder. To construct the skip architecture, we employ the bottleneck block of ResNet
as the post-processing blocks S (1) 〜S(4). Each bottleneck block consists of two 1x1 convolutional
layers and one 3x3 convolutional layer with expansion rate of 4. Dropout with the ratio of 0.25 is
applied to the end of each bottleneck block. For the ASPP Module in front of S(3), we adopt the
dilation rate of 1,4,7,11. The segmentation module generates a segmentation prediction with the
rounding threshold value λ of 0.4. For decoder, each decoder block is composed of three consecu-
tive 3x3 convolutional layers, and dropout with the ratio of 0.25 is again located at the end of each
layer.
11
Under review as a conference paper at ICLR 2021
During meta-training of CAFENet, we set the number of query samples in training episodes to be
5 forFSE-1000 and 10 for SBD-5i ,respectively. In evaluation, We employ average.Precision_Score
function of Scikit-learn library to measure the Average Precision (AP) score. We compute the AP
score for each image and average them to measure the overall performance. For Maximum F-
measure (MF) score, We measure true positives (TP), false positives (FP) and false negatives (FN) at
every 0.01 threshold intervals for each image, and accumulate the values for all images in 1000 test
episodes. The MF score is computed using the accumulated TP, FP, and FN values.
B Ablation Studies
In this section, We shot the results of ablation experiments to examine the impact of proposed MSMR
and attentive decoder. Table B.1 shoWs the experiment results on the FSE-1000 dataset. The base-
line method in Table B.1 conducts edge prediction in loW resolution and utilizes the auxiliary loss
from loW resolution together With the loss from the edge prediction in original resolution for meta-
training. The edge prediction is done using a metric-based method utilizing prototypes Which are
computed using doWn-sampled edge labels. The method dubbed as Seg utilizes a segmentation
module Without MSMR or attentive decoding. Seg directly matches high-dimensional query feature
vectors With prototypes in both training and evaluation. In Seg, the segmentation module is utilized
only to provide the segmentation loss that assists for model learning to extract semantic features.
Seg + Att employs the predicted segmentation mask for the additional attention process in skip ar-
chitecture. Seg + MSMR + Att additionally utilizes the MSMR regularization for training. For
fair comparison, all methods use the same netWork architecture and training hyperparameters. For
SBD-5i datasets, the ablation experiments are done With same model variations as FSE-1000. The
results on SBD-5i are shoWn in Table B.2.
Table B.1: Ablation experiment results of proposed CAFENet on FSE-1000. 1000 randomly sampled test
episodes are used for evaluation. MF and AP scores are measured by %
Metric	Method	1-way 1-shot	1-way 5-shot
	baseline	52.71	53.52
MF	Seg	56.89	59.65
(ODS)	Seg + Att	58.00	60.14
	Seg + Att + MSMR	58.47	60.63
	baseline	53.66	54.59
ʌp AP	Seg	58.80	61.87
	Seg + Att	59.81	62.37
	Seg + Att + MSMR	60.54	63.92
Tables B.1 and B.2 demonstrate that the use of the segmentation module in Seg gives significant
performance advantages over baseline for both FSE-1000 and SBD-5i datasets. It is also seen that
the additional use of attentive decoding, Seg + Att, generally improves the performance over Seg.
Finally, adding the effect of MSMR regularization gives substantial extra gains, as seen by the scores
associated With Seg + MSMR + Att. Clearly, When compared With baseline, our overall approach
Seg + MSMR + Att provides large gains. In the main paper, We report the results of Seg + MSMR
+ Att as the results of CAFENet.
B.1	Additional Experiments on Multi-Split Matching Regularization
B.1.1	Feature matching method for segmentation
In Table B.3, We have compared various feature matching methods betWeen prototypes and query
feature vectors for producing segmentation prediction on SBD-5i . The method baseline refers to
the original method generating segmentation prediction using only the similarity metric betWeen
high-dimensional vectors as done in Eq. 3. For the method average, segmentation predictions from
loW-dimensional feature splits (Eq. 5) and original high-dimensional feature vectors (Eq. 5) are
averaged to generate the final prediction mask. The average method can be understood as a method
12
Under review as a conference paper at ICLR 2021
Table B.2: Ablation experiment results of proposed CAFENet on SBD-5i. 1000 randomly sampled test
episodes are used for evaluation. MF and AP scores are measured by %
i=0	i=1	!	i=2	I	I	i=3
aeroplane,bike,bird,boat,bottle	bus,car,cat,chair,cow	table,dog,horse,mbike,person	plant,sheep,sofa,train,tv
Metric ∣ Method (5-shot)	∣SBD-50 ∣ SBD-51 ISBD-52 ∣ SBD-53 11 Mean
MF (ODS)	baseline Seg Seg + Att Seg + Att + MSMR	22.27 30.61 31.75 34.71	19.64 31.62 33.41 36.81	20.41 28.06 28.44 32.02	20.41 24.97 26.03 28.37	20.20 28.82 29.91 32.98
	baseline	18.68	15.57	14.97	14.05	15.82
AP	Seg	26.14	26.78	21.92	18.43	23.32
	Seg + Att	27.61	28.39	22.66	20.11	24.69
	Seg + Att + MSMR	30.47	32.40	27.01	23.06	28.24
Metric	Method (1-shot)	SBD-50 I SBD-51 ∣ SBD-52			SBD-53 11 Mean	
	baseline	21.81	19.49	20.34	18.06	19.93
MF	Seg	29.89	31.64	27.89	24.41	28.46
(ODS)	Seg + Att	30.72	33.03	28.63	25.04	29.36
	Seg + Att + MSMR	31.54	34.75	29.47	26.68	30.61
	baseline	18.11	15.47	14.73	13.89	15.55
AP	Seg	25.16	26.15	21.52	18.52	22.84
	Seg + Att	26.10	27.21	22.47	18.81	23.65
	Seg + Att + MSMR	26.81	29.08	23.77	20.44	25.03
utilizing MSMR not only for regularization, but also for inference. In the weighted sum method,
the above five segmentation masks are combined using a weighted sum with learnable weights.
As we can see in Table B.3, the MSMR method shows the best performance when employed for
regularization.
Table B.3: Comparison of different feature matching method on SBD-5i under 1-way 5-shot setting. MF and
AP scores are averaged over 4 splits
Feature matching method ∣∣ baseline ∣ average ∣ weighted sum
AP	34.61	31.05	31.44
MF(ODS)		29.91	26.20	26.46
B.1.2	Number of vector splits
MSMR divides the high-dimensional feature into multiple splits. Table B.4 shows the performance
of proposed CAFENet with varying numbers of splits K . Comparing the K = 1 case with other
cases, we can see that applying MSMR regularization consistently improves performance. We can
see that K = 4 results in the best AP and MF performance. The performance gain is marginal when
we divide the embedding dimension into too small (K = 16) or too big (K = 2) a pieces.
Table B.4: Comparison of different numbers of vector splits K on SBD-5i under 1-way 5-shot setting. MF and
AP scores are averaged over 4 splits
Number of splits ∣∣ K =1 ∣ K = 2 ∣ K = 4 ∣ K = 8 ∣ K = 16
AP	24.69	26.48	29.91	27.68	23.83
MF(ODS)	29.91	31.62	32.30	31.58	30.77
13
Under review as a conference paper at ICLR 2021
B.1.3	Random projection for vector splits
In MSMR, we divide query feature into K splits along the channel dimension, i.e. we apply deter-
ministic splitting. In Table B.5, we compare the performance of different splitting methods. The
method dubbed as Deterministic refers to the MSMR method that we utilize in CAFENet. The
Random method randomly splits feature vectors into 4 parts in each episode. The Baseline method
does not split features at all. Interestingly, Random’s performance significantly degrades, even
below that of Baseline.
Table B.5: Comparison of different splitting methods on SBD-5i. MF and AP scores are measured by %
Metric	Method (5-shot) ∣ SBD-50 ∣ SBD-51 ∣ SBD-52 ∣ SBD-53 11 Mean					
MF (ODS)	Deterministic	34.71	36.81	32.02	28.37	32.98
	Baseline	31.75	33.41	28.44	26.03	29.91
	Random	23.67	17.22	21.16	20.51	20.64
	Deterministic	30.47	32.40	27.01	23.06	28.24
AP	Baseline	27.61	28.39	22.66	20.11	24.69
	Random	20.37	17.51	18.26	15.76	18.07
Metric	Method (1-shot)	SBD-50 I SBD-51		SBD-52	SBD-53	I Mean
MF (ODS)	Deterministic	31.54	34.75	29.47	26.68	30.61
	Baseline	30.72	33.03	28.63	25.04	29.36
	Random	23.36	17.04	21.43	20.47	20.58
	Deterministic	26.81	29.08	23.77	20.44	25.03
AP	Baseline	26.10	27.21	22.47	18.81	23.65
	Random	20.12	17.35	18.57	15.64	17.92
B.2	Additional Experiments on Semantic Attention
B.2	. 1 Experiments on Applying the Semantic Attention
In the proposed CAFENet, we utilize the semantic attention in the attentive decoder. In Table B.6,
we compare three methods that utilize the attention in different manners. Attentive Decoding is
our proposed CAFENet that applies the semantic attention to the multi-scale features. The second
method, Direct Attention, is a method that directly passes the feature to the edge detector and
applies the semantic attention to the final edge prediction. The last method, No Attention is a
baseline where the edge detector generates prediction without any attention. For both 1-shot and
5-shot settings, regardless of how the attention is applied, semantic attention considerably improves
performance. These results show the effectiveness of semantic attention. The results also show that
the proposed approach of Attentive Decoding yields better results compared to Direct Attention.
B.2	.2 Experiments on Generating the Attention
In the proposed attentive decoding, we can generate an attention map using various methods. In
Table.B.7, we compare two different methods to generate the attention map. Segmentation Atten-
tion is the method adopted in proposed CAFENet. In Segmentation Attention, the output of the
segmentation module S is utilized as the M in equation 7. In Edge Attention, the edge prediction
E(S) is generated from segmentation mask S by equation 11 following (Feng et al. (2019)). The
generated edge prediction E(S) is then used as the M in equation 7. Experiment results show that
utilizing the segmentation mask to generate the attention map performs better than utilizing the edge
prediction.
E(S) = |S- AvgP ool(S)|	(11)
14
Under review as a conference paper at ICLR 2021
Table B.6: Comparison of different methods to apply attention on SBD-5i .
Metric ∣ Method (5-shot) ∣ SBD-50 ∣ SBD-51 ∣ SBD-52 ∣ SBD-53 11 Mean
MF	Attentive Decoding	34.71	36.81	32.02	28.37	32.98
(ODS)	Direct Attention	33.47	36.64	31.85	28.28	32.56
	No Attention	31.75	33.41	28.44	26.03	29.91
	Attentive Decoding	30.47	32.40	27.01	23.06	28.24
AP	Direct Attention	30.31	31.93	26.15	22.59	27.75
	No Attention	27.61	28.39	22.66	20.11	24.69
Metric	Method (1-shot)	SBD-50	SBD-51 I SBD-52 I SBD-53 H Mean			
MF (ODS)	Attentive Decoding	31.54	34.75	29.47	26.68	30.61
	Direct Attention	31.27	34.53	30.01	26.41	30.56
	No Attention	30.72	33.03	28.63	25.04	29.36
	Attentive Decoding	26.81	29.08	23.77	20.44	25.03
AP	Direct Attention	26.46	28.61	23.64	20.23	24.74
	No Attention	26.10	27.21	22.47	18.81	23.65
Table B.7: Comparison of different methods to generate attention on SBD-5i .
Metric	Method (5-shot)	∣ SBD-50 ∣ SBD-51 ∣ SBD-52 ∣ SBD-53 11 Mean					
MF (ODS)	Segmentation Attention Edge Attention	34.71 33.83	36.81 37.01	32.02 31.14	28.37 27.92	32.98 32.48
AP	Segmentation Attention	30.47	32.40	27.01	23.06	28.24
	Edge Attention	29.16	32.35	25.56	22.52	27.40
Metric	Method (1-shot)	SBD-50	SBD-51	SBD-52 I SBD-53 H Mean		
MF	Segmentation Attention	31.54	34.75	29.47	26.68	30.61
(ODS)	Edge Attention	30.25	33.58	30.64	26.16	30.16
AP	Segmentation Attention	26.81	29.08	23.77	20.44	25.03
	Edge Attention	25.21	27.83	24.87	20.34	24.56
C Qualitative Results
In Figures C.1 and C.2, we visualize the qualitative results of CAFENet for FSE-1000 and SBD-
5i , respectively. We can see that the proposed CAFENet successfully detect the edge of the target
class from given images. In Figure C.1, we can compare the qualitative results of edge predictions
from different methods. We can see that the ’DFF + Finetune’ method succeeds in finding the
edges of the objects, but it lacks the ability to learn semantic information and hardly distinguish the
boundary of target class from the other boundaries. For ’PANet + Sobel’ method, on the other hand,
it successfully understands the semantic information and localizes the target object, but it fails to
refine the correct boundary. The proposed CAFENet, however, is capable of localizing the objects
from target class and detecting the correct boundary at the same time.
Figure C.3 visualizes more qualitative results on SBD-5i from ablation experiments. We illustrate
and compare the boundary prediction results of the baseline, Seg, Seg + Att, and Seg + Att +
MSMR methods. For fair comparison, all methods share the same support set. From the results,
we can clearly see that the techniques proposed in CAFENet steadily improve the quality of edge
prediction.
15
Under review as a conference paper at ICLR 2021
CAFENet
image
Figure C.1: Qualitative examples of 5-shot edge detection on SBD-5i dataset.
ground truth DFF + Finetune PANet + Sobel
D Label Generator
D.1 EDGE Label Generator
Algorithm D.1 generates the edge labels from the segmentation labels. The edge label generator
finds the regions where the pixel value of segmentation label drastically changes, and determine
the pixels in the regions as the boundary. Note that the pixels at the border of the image are also
determined as the boundary.
16
Under review as a conference paper at ICLR 2021
image
ground truth
PANet
Figure C.2: Qualitative examples of 5-shot edge detection on FSE-1000 dataset.
PANet + Sobel
D.2 Segmentation Label Generator
Algorithm D.2 is the segmentation label generator which generates the segmentation label from
the edge label. Before the label generation, the pixels are divided into several groups based on
boundary labels. We employ the Breadth-First Search (BFS) algorithm and divide pixels into groups
{G1,G2,…,Gn}. The segmentation label generator of Algorithm D.2 classifies these groups into
foreground and background. First, the algorithm sweeps each column and row to count the number
of pixel value change in edge label. If there are certain numbers of changes, the algorithm again
sweeps the column or row and record the location of foreground pixels and mark the foreground
pixels in the column or row in a matrix T. Based on pixel groups {G1, G2,..., Gn}, the marking
results in T are then divided into pixel value groups {T 1,T2,..., Tn}. The probability that each
17
Under review as a conference paper at ICLR 2021
Figure C.3: Qualitative results comparison of ablation experiments
18
Under review as a conference paper at ICLR 2021
Algorithm D.1 Edge Label Generation
Input: Segmentation label M of an image
Output: Edge label y of an image.
y — 0W XH	. Initialize y as zero matrix having same shape with M
for (h, w) in (1, 1),...,(H,W) do	. H/W is height/width of the image
if Mh,w = 1 then
for (a, b) = (-r,-r),...,(r,r) do	. radius r determines thickness of edge
if Mh+a,w+b = 0 then
yh,w — 1	. 0/1 means non-edge/edge pixel, respectively
break
else if (h + a < 0) or (h + a > H) or (w + b < 0) or (w + b > W) then
yh,w 1 1
break
return y
.Return label annotation
image	segmentation label	edge label	image	segmentation label	edge label
Figure D.1: Result of edge label generator.
group Gi belongs to the foreground is calculated as the mean of pixel values Ti. The groups with
probability above the threshold λ are determined as the foreground groups, and the pixels belongs
to foreground groups are marked as foreground pixels. We set the threshold value λ to be 20/255.
Figure D.2: Result of segmentation label generator.
E Details on Datasets
E.1 FSE-1000
We build FSE-1000 using an existing few-shot segmentation dataset, FSS-1000. We extract the
boundary labels from segmentation annotations using Algorithm D.1. The radius value r in Algo-
rithm D.1 is set to 3 in FSE-1000. 1000 categories in FSE-1000 are split into 800 train classes and
200 test classes. For the detailed class configuration, the reader may refer to our attachment on class
configuration. Figure D.1 visualizes the result of our edge extraction algorithm.
E.2 SBD-5i
SBD-5i is constructed based on the existing semantic edge detection dataset (SBD). Due to the
noise of boundary annotations in original SBD, we utilize the thicker edge as done in FSE-1000. To
extract thicker edge, we generate the segmentation labels from the edge labels using Algorithm D.2
instead of using existing segmentation labels of SBD. Figure D.2 shows the process of generating
the segmentation label from the edge label. From the generated segmentation labels, we extract edge
19
Under review as a conference paper at ICLR 2021
Algorithm D.2 Segmentation Label Generation
Input: Edge label y of an image, pixel groups G1 , G2, ..., Gn
Output: Segmentation label M of an image.
M,T — 0wh	. Initialize M, T as zero matrix having same shape with y
for h = 1,...,H do
cnt, mode - 0
for w = 1,...,W do
if yh,w = mod(mode + 1, 2) then
Cnt - Cnt + 1
mode — mod(mode + 1, 2)
if mod(Cnt, 4) = 0 and Cnt 6= 0 then
cnt0, mode0 - 0
for w0 = 1,...,W do
if yh,w0 = mod(mode0 + 1, 2) then
cnt0 J Cnt + 1
mode0 J mod(mode0 + 1, 2)
if mod(Cnt0, 4) = 2 then
Th,w0 J 1
for w = 1,...,W do
Cnt, mode J 0
for h = 1,...,H do
if yh,w = mod(mode + 1, 2) then
Cnt J Cnt + 1
mode J mod(mode + 1, 2)
if mod(Cnt, 4) = 0 and Cnt 6= 0 then
Cnt0, mode0 J 0
for h0 = 1,...,H do
if yh0,w = mod(mode0 + 1, 2) then
Cnt0 J Cnt0 + 1
mode0 J mod(mode0 + 1, 2)
if mod(Cnt0, 4) = 2 then
Th0,w J 1
. H is height of the image
. W is width of the image
. Accumulate changes of pixel value
. Check if there are FG pixels in the row
. Find location of FG pixels in the row
. Record location of FG pixels in the row
Repeat the same process for every column
for i = 1, ..., n do
Ti J Th,w∖(h,w)∈Gi
if mean(T i ) ≥ λ then
Mh,w∖(h,w)∈Gi J 1
else
Mh,w∖(h,w)∈Gi J 0
return M
. Check the probability that Gi belongs to foreground
. 1 means a foreground pixel
. 0 means a background pixel
. Return segmentation annotation
labels using Algorithm D.1 with a radius value of 4. This process allows us to train the proposed
CAFENet using only the edge labels.
While all images in FSE-1000 have the same size, images in SBD-5i have different size. However,
constructing the training episode as a mini-batch requires images with the same size. Previous
works on semantic edge detection typically apply random cropping to deal with this issue. For the
few-shot setting, however, random cropping severely degrades informativeness of support set and
consequently hinders learning. Alternatively, we utilize the training examples resized to 320 × 320
to maintain the information of images as much as possible. When resizing the edge labels for
training, we first generate segmentation labels in original scale using Algorithm D.2 and resize the
segmentation labels to 320 × 320. Then, we extract edge labels from resized segmentation labels
using Algorithm D.1 with a radius value of 3.
20
Under review as a conference paper at ICLR 2021
F Additional Results with Multi-angle Input Test
In this section, we report the few-shot semantic edge prediction results with multi-angle input test.
In multi-angle input test, the model predicts the edge by averaging the 4 edge prediction results from
4 copies of an input image rotated by multiples of 90 degrees. We have empirically found that multi-
angle input test significantly improves the performance. Table F.1 and F.2 show the evaluation results
with multi-angle input test for FSE-1000 and SBD-5i , respectively. We can verify the effectiveness
of the multi-angle input test from the results.
Table F.1: 1-way 5-shot results with the multi-angle input test on FSE-1000. 1000 randomly sampled test
episodes are used for evaluation. MF and AP scores are measured by %
Metric	Method	1-way 5-shot
	baseline	55.23
MF	Seg	61.17
(ODS)	Seg + Att	61.90
	Seg + Att + MSMR	62.23
	baseline	56.12
AP	Seg	63.32
	Seg + Att	63.83
	Seg + Att + MSMR	65.81
Table F.2: 1-way 5-shot results with the multi-angle input test on SBD-5i. 1000 randomly sampled test episodes
are used for evaluation. MF and AP scores are measured by %
Metric	Method(5-shot)	SBD-50	SBD-51	SBD-52	SBD-53 11 Mean	
	baseline	24.38	23.78	22.81	20.39	22.84
MF	Seg	32.46	33.18	29.63	26.46	30.43
(ODS)	Seg + Att	34.29	35.76	32.25	28.12	32.61
	Seg + Att + MSMR	36.13	37.92	34.18	30.21	34.61
	baseline	21.03	20.39	17.48	16.31	18.80
AP	Seg	27.92	28.00	23.94	20.06	24.98
	Seg + Att	29.94	30.40	24.34	21.38	26.52
	Seg + Att + MSMR	31.92	33.51	29.28	24.91	29.91
21