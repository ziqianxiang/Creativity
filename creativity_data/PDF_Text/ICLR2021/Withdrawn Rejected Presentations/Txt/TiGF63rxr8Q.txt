Under review as a conference paper at ICLR 2021
Efficient Reinforcement Learning in
Resource Allocation Problems Through
Permutation Invariant Multi-task Learning
Anonymous authors
Paper under double-blind review
Ab stract
One of the main challenges in real-world reinforcement learning is to learn success-
fully from limited training samples. We show that in certain settings, the available
data can be dramatically increased through a form of multi-task learning, by ex-
ploiting an invariance property in the tasks. We provide a theoretical performance
bound for the gain in sample efficiency under this setting. This motivates a new
approach to multi-task learning, which involves the design of an appropriate neural
network architecture and a prioritized task-sampling strategy. We demonstrate
empirically the effectiveness of the proposed approach on two real-world sequential
resource allocation tasks where this invariance property occurs: financial portfolio
optimization and meta federated learning.
1	Introduction
Sample efficiency in reinforcement learning (RL) is an elusive goal. Recent attempts at increasing the
sample efficiency of RL implementations have focused to a large extent on incorporating models into
the training process: Xu et al. (2019); Clavera et al. (2018); Zhang et al. (2018); Berkenkamp et al.
(2017); Ke et al. (2019); Yarats et al. (2019); Huang et al. (2019); Chua et al. (2018); Serban et al.
(2018). The models encapsulate knowledge explicitly, complementing the experiences that are gained
by sampling from the RL environment. Another means towards increasing the availability of samples
for a reinforcement learner is by tilting the training towards one that will better transfer to related
tasks: if the training process is sufficiently well adapted to more than one task, then the training of a
particular task should be able to benefit from samples from the other related tasks. This idea was
explored a decade ago in Lazaric & Ghavamzadeh (2010) and has been gaining traction ever since, as
researchers try to increase the reach of deep reinforcement learning from its comfortable footing in
solving games outrageously well to solving other important problems. Yu (2018) discusses a number
of methods for increasing sample efficiency in RL and includes experience transfer as one important
avenue, covering the transfer of samples, as we do here, transfer of representation or skills, and
jumpstarting models which are then ready to be quickly, i.e. with few samples, updated to different
tasks. D’Eramo et al. (2020) address the same idea, noting that multi-task learning can improve the
learning of each individual task, motivated by robotics-type tasks with underlying commonality, such
as balancing a single vs. a double pendulum, or hopping vs. walking.
We are interested in exploiting the ability of multi-task learning to solve the sample efficiency problem
of RL. Our setting does not apply to all problem classes nor does it seek to exploit the kind of physical
similarities found in robotics tasks that form the motivation of Lazaric & Ghavamzadeh (2010);
D’Eramo et al. (2020). Rather, we show that there are a number of reinforcement learning tasks
with a particular fundamental property that makes them ideal candidates for multi-task learning
with the goal of increasing the availability of samples for their training. We refer to this property as
permutation invariance. It is present in very diverse tasks: we illustrate it on a financial portfolio
optimization problem, whereby trades are executed sequentially over a given time horizon, and on
the problem of meta-learning in a federated supervised learning setting.
Permutation invariance in the financial portfolio problem exhibits itself as follows: consider the task
of allocating a portion of wealth to each of a number of financial instruments using a trading policy.
If the trading policy is permutation invariant, one can change the order of the instruments without
1
Under review as a conference paper at ICLR 2021
changing the policy. This allows one to generate multiple portfolio optimization tasks from a given
set of financial instruments. A commonality between applications that have this property is that they
concern sequential resource allocation: at each time step, the resource allocation scores the quality of
each available candidate entity (for example a financial instrument in the above example), then based
on those scores, apportions out the resource (the total wealth to invest, in the above example) among
the entities at that time step, so that over the horizon of interest, the reward is maximized.
Sequential resource allocation problems include applications such as sequential allocation of budget,
sequential allocation of space, e.g. in IT systems, hotels, delivery vehicles, sequential allocation of
people to work slots or appointments, etc. Many such applications possess permutation invariance in
that the ordering of the entities, i.e. where the resources are allocated, can change without changing
the resulting optimal allocation. We show that under this form of permutation invariance, it is possible
to derive a bound on the performance of the policy. The bound is an extension of that of Lazaric
& Ghavamzadeh (2010), and while similar to, provides additional information beyond the bound
of D’Eramo et al. (2020). We use the bound to motivate an algorithm that allows for substantially
improved results as compared with solving each task on its own. The bound and the algorithm are first
analyzed on a synthetic problem that validates the bound in our theorem and confirms the multi-task
gain that the theory predicts. Hessel et al. (2018); Bram et al. (2019) have cautioned against degrading
of the performance on each task when some tasks bias the updates to the detriment of others in
multi-task learning. They claim that some tasks have a greater density or magnitude of in-task rewards
and hence a disproportionate impact on the learning process. In our setting, deleterious effects of
some tasks on others could also arise. The algorithm we propose handles this through a form of
prioritized sampling, where priorities are put on the tasks themselves, and acts like a prioritized
experience replay buffer, applied to a multi-task learning problem. We show empirically that the
priorities thus defined protect the overall learning problem from the deleterious effects that unrelated
or unhelpful tasks could otherwise have on the policy.
The contributions of this work are as follows: (1) we identify the permutation invariance property of
the class of reinforcement learning problems involving sequential resource allocation, (2) we define a
method to increase sample efficiency in these reinforcement learning problems by leveraging this
property of permutation invariance; (3) we provide a theoretical performance bound for the class of
problems; (4) we validate experimentally the utility of permutation variance on sample efficiency
as well as the validity of the bound on a synthetic problem; and (5) we illustrate two real-world RL
resource allocation tasks for which this property holds and demonstrate the benefits of the proposed
method on sample efficiency and thus also on the overall performance of the models.
2	Related work
A notable first stream of work on leveraging multi-task learning for enhancing RL performance on
single tasks can be found in Wilson et al. (2007); Lazaric & Ghavamzadeh (2010) which consider,
as we do, that there is an underlying MDP from which the multiple tasks can be thought to derive.
They use however a Bayesian approach and propose a different algorithmic method than ours. Our
results extend performance bounds by Lazaric et al. (2012) on single-task RL. As noted by Yu (2018),
jumpstarting, or distilling experiences and representations of relevant policies is another means to
increasing sample efficiency in solving a new but related problem. Rusu et al. (2016) uses this idea in
so-called progressive neural networks and Parisotto et al. (2015) leverage multiple experts to guide
the derivation of a general policy. With a similar objective, Teh et al. (2017) define a policy centroid,
that is, a shared distilled policy, that captures the commonalities across the behaviors in the tasks. In
all of these distillation-type methods, the tasks considered are simple or complex games.
Teh et al. (2017) note that their policy centroid method, distral, is likely to be affected by task
interference, in that differences across tasks may degrade the performance of the resulting policy
of any of the constituent tasks. This topic was studied by Hessel et al. (2018); Bram et al. (2019).
Hessel et al. (2018) proposed a solution to this by extending the so-called PopArt normalization van
Hasselt et al. (2016) to re-scale the updates of each task so that the different characteristics of the
task-specific reward do not skew the learning process. Bram et al. (2019) use a different approach
that learns attention weights of the sub-networks of each task and discards those that are not relevant
or helpful. Vuong et al. (2019); D’Eramo et al. (2020) are, like our work, concerned with sharing
of experiences to facilitate a more sample-efficient learning process. Vuong et al. (2019) suggest
2
Under review as a conference paper at ICLR 2021
identifying the shared portions of tasks to allow sharing of samples in those portions. The work of
D’Eramo et al. (2020) is in some ways quite similar to ours: the authors’ goal is the same and they
derive a bound as we do on the performance in this setting. However, their setting is different in that
their tasks have both shared and task-specific components, and their bound becomes tighter only
as the number of tasks increases. In our setting, we do not require a task-specific component, and
we are able to show how the distance between the MDPs of each task, in addition to the number of
tasks, affects the strength of the bound. Recently, permutation invariance has been exploited in deep
multi-agent reinforcement learning (Liu et al., 2019) where the invariance properties arise naturally
in a homogeneous multi-agent setting. Their work employs permutation invariance in learning the
critic whereas in our case the entire learned policy employs permutation invariance.
3	Preliminaries
We begin by defining notation. For a measurable space with domain X, let S(X) denote the set of
probability measures over X, and B(X; L) the space of bounded measurable functions with domain
X and bound 0 < L < ∞. For a measure ρ ∈ S(X) and a measurable function f : X → R, the
l2(ρ)-norm of f is kf kρ, and for a set of n points Xi,…，Xn ∈ X, the empirical norm, kf kn is
kfkρ = Zf(X)2Pmx) and kfkn = n Xf(Xt)2.
Let kfk∞ = supx∈X |f (x)| be the supremum norm of f. Consider a set of MDPs indexed by
t. Each MDP is denoted by a tuple Mt = hX, A, Rt, Pt, γi, where X, a bounded closed subset
of the s-dimensional Euclidean space, is a common state space; A is a common action space,
Rt : X × A → R is a task specific reward function uniformly bounded by Rmax, Pt is a task specific
transition kernel such that Pt(∙∣x, a) is a distribution over X for all x ∈ X and a ∈ A, and Y ∈ (0,1)
is a common discount factor. Deterministic policies are denoted by π : X → A. For a given
policy π, the MDP Mt is reduced to a Markov chain Mtπ = hX, Rtπ , Ptπ , γi with reward function
Rn(x) = Rt(x,π(x)), transition kernel Pt(∙∣x) = Pt(∙∣x,π(x)), and stationary distribution ρ∏.
The value function Vtπ for MDP t is defined as the unique fixed-point of the Bellman operator
Ttπ : B(X; Vmax = Rmax/(1 - γ)) → B(X; Vmax), given by
(TnV)(x) = Rn(x) + γ I Ptn(dy∣x)V(y).
X
k.∙i∙ F	..	J	-,	∙,.	C	J	-,	-,	C	J	Trn C	， ， . F C	F	..
Let ∏t denote the optimal policy for Mt. The optimal value function Vt t for Mt is defined as the
Ll~Π
unique fixed-point of its optimal Bellman operator Tt t which is defined by
..k *	...
(Ttnt V)(x) = max
t	a∈A
Rt(x, a) + γ
Pt(dy|x, a)V(y)
X
X
To approximate the value function V, we use a linear approximation architecture with parameters
α ∈ Rd and basis functions 4i ∈ B(X; L) for i = 1,…，d. Let 夕(∙)=(夕ι(∙),…，夕d(∙))T ∈ Rd
be the feature vector and F the linear function space spanned by basis functions 夕i. Thus, F =
{fα | α ∈ Rd and fα(∙)=以∙)Tα}.
Consider a learning task to dynamically allocate a common resource across entities Ut ⊆ U . Each
t corresponds to a task, but for now take t to be an arbitrary fixed index. At each time step n, the
decision maker observes states xn = (xi,n)i∈Ut of the entities, where xi,n is the state of entity i, and
takes action an = (ai,n)i∈Ut, where ai,n is the share of the resource allocated to entity i. The total
resource capacity is normalized to 1 for convenience. Therefore, allocations satisfy 0 ≤ ai,n ≤ 1 and
Pi∈U ai,n = 1. We consider policy πθ(xn) parameterized by θ. Assume that we have access to the
reward function Rt as well as a simulator that generates a trajectory of length N given any arbitrary
policy πθ. The objective of the learning task is to maximize
N
Jt (θ) = E	γ γ Y	Rt(X n, an)	an+1	=	πθ (xn),	xn+1	〜Pt(,∖xn, an) ,	xi 〜Pt 卜)
n=1
In many settings, N is small and simulators are inaccurate; therefore, trajectories generated by the
simulator are poor representations of the actual transition dynamics. This occurs in batch RL where
trajectories are rollouts from a dataset. In these cases, policies overfit and generalize poorly.
3
Under review as a conference paper at ICLR 2021
4	Theoretical Results
We introduce first a property that we term permutation-invariance for the policy network that can be
shown to help significantly reduce overfitting.
Definition 1 (Permutation Invariant Policy Network) A policy network πθ is permutation invari-
ant if it satisfies πθ (σ (x)) = σ (πθ (x)) for any permutation σ.
Permutation invariant policy networks have significant advantages over completely integrated policy
networks. While the latter are likely to fit correlations between different entities, this is not possible
with permutation invariant policy networks as they are agnostic to identities of entities. Therefore,
permutation invariant policy networks are better able to leverage experience across time and entities,
leading to greater efficiency in data usage. Moreover, observe that if the transition kernels can be
factored into independent and identical transition kernels across entities, then the optimal policy is
indeed permutation invariant.
Our main theoretical contributions start with an extension of results from Lazaric et al. (2012), where
a finite-sample error bound was derived for the least squares policy iteration (LSPI) algorithm on a
single task. Lazaric et al. (2012) provided a high-probability bound on the performance difference
between the final learned policy and the optimal policy, of the form ci + c2∕√N, where ci and
c2 are constants that depend on the task and the chosen feature space, and N is the number of
training examples. We extend their result by showing that, as long as tasks are -close to each other
(with respect to a similarity measure We define later), the error bound of solving each task using
our multi-task approach has the form ci + c2∕√NT+c3e, where T is the number of tasks and c3
is a task-dependent constant. Specifically, our theorem provides a general result and performance
guarantee with respect to using data from a different but similar MDP. Definition 1 provides a basis
for generating many such MDPs. Finally, the benefit of doing so shall be provided by Corollary 2.
Thus, provided is small, a given task can benefit from a much larger set of NT training examples.
In addition to the assumptions of Lazaric et al. (2012), we extend the definition of second-order
discounted-average concentrability, proposed in Antos et al. (2008), and define the notion of first-order
discounted-average concentrability. The latter will be used in our main result, Theorem 1.
Assumption 1 There exists a distribution μ ∈ S (X) such that for any Policy π that is greedy with
respect to afunctιon in the truncated space F, μ ≤ Cρ∏ for all t, where C < ∞ ISa constant. Given
the target distribution σ ∈ S(X) and an arbitrary sequence of policies {πm}m≥i, let
Cσ,μ = SUP
π1,...,πm
d(μPπι ...Pπm)
dσ
We assume that C,07 ,*, Cσ,μ < ∞, and define first and second order discounted-average concentrabil-
ity of future-state distributions as follows:
Cσ ,μ = (I-Y) X γmcσ,μ (m),
m≥0
C/," =(I-Y)2 X mYm-icσ,μ(m).
m≥i
Theorem 1 (Multi-Task Finite-Sample Error Bound) Let M = hX, A, R, P, Yi be an MDP with
reward function R and transition kernel P. Assume A finite. Denote its Bellman operator by
(TπV)(x)
Rπ(x) + Y
Pn (dy∣χ)V (y).
X
X
Given a policy π, define the Bellman difference operator between Mt and M to be DtπV =
TtπV - TπV. Apply the LSPI algorithm to M, by generating, at each iteration k, a path from M
ofsize N, where N satisfies Lemma 4 in Lazaric et al. (2012). Let V-i ∈ F be an arbitrary initial
value function, V0, ∙∙∙ , VK-I (V0, •…,VK-1) be the sequence ofvalue functions (truncated value
functions) generated by the LSPI after K iterations, and πk be the greedy policy w.r.t. the truncated
value function V^k-i. Suppose also that
kD∏Vπkμ ≤ C ∀ π, and IlDnkVk-i∣∣μ ≤ C ∀ k.
4
Under review as a conference paper at ICLR 2021
Then, for constants c1, c2, c3, c4 that are dependent on M, with probability 1 - δ (with respect to the
random samples):
k*	_	1
IlVt t - Vt	∣∣σ ≤ c1 √N + c2E
+ c3
+ c4.
The proof is deferred to the Appendix. Theorem 1 formalizes the trade off between drawing
fewer samples from the exact MDP Mt , versus drawing more samples from a different MDP M.
Importantly, it shows how to benefit from solving a different MDP, M, when: (a) additional samples
can be obtained from M, and (b) M is not too different from Mt . In particular, the distance measure
is simply the distance between the Bellman operators of the MDPs, which can be bounded if the
difference in both the transition and reward functions are bounded.
In recent work, a performance bound for multi-task learning was given in Theorem 2 and 3 of D’Eramo
et al. (2020). However, the authors used a different setup containing both shared and task-specific
representations, and their focus was on showing that the cost of learning the shared representation
decreases with more tasks. They did not show how the similarity or difference across tasks affects
performance. In contrast, our setup does not contain task-specific representations, and our focus is
on how differences across MDPs impact the benefit of having more tasks (and consequently more
samples). We show this in Corollary 1 and Corollary 2.
Remark 1 While our theoretical results are based on LSTD and LSPI and assume finite action space,
our approach is applicable to a wide range of reinforcement learning algorithms, including policy
gradient methods and to MDPs with continuous action spaces. Deriving similar results for a larger
family of models and algorithms remains an interesting, albeit challenging, future work.
Permutation invariant policy networks allow using data from the global set of entities U . Since the
policy network is agnostic to the identities of the entities, one can learn a single policy for all tasks,
where each task t ∈ [T] is a resource allocation problem over a subset of entities Ut. For notational
simplicity, assume that all tasks have the same number of entities, and all trajectories are of equal
length N. Our approach can, however, be readily extended to tasks with different numbers of entities
and different trajectory lengths. Permutation invariance allows a large set of MDPs to leverage the
result of Theorem 1. In the next section we shall provide an algorithm, motivated by the following
corollaries, and a prioritized sampling strategy for this setting that drives significantly greater sample
efficiency for the original task. The sampling strategy also helps to stabilize the learning process,
reducing the risk of deleterious effects of the multi-task setting, as discussed by Teh et al. (2017) and
addressed in works such as Hessel et al. (2018); Bram et al. (2019).
Corollary 1 Let [T] be a set of similar tasks such that their distance from the average MDP, given by
TT
1T	1T
(Tn V )(x) = T ERn (x) + YLT EPn (dy∣χ)V(y),
t=1	X	t=1
is bounded by E as defined in Theorem 1. LetN be the number of samples available in each task.
Let ∏k be the policy obtained at the Kth iteration when applying LSPI to the average MDP. Then,
the SuboPtimality ofthe policy on each task is O(1∕√NT) + O(E) + C for some constant C (where
suboptimality is defined according to Theorem 1).
Recall that each task is formed by selecting a subset Ut of entities from the global set U . We thus have
the following sample gain that can be attributed to the permutation invariance of the policy network.
Corollary 2 (Gain in Sample Efficiency from Permutation Invariance) Let M = |U | and m =
|Ut|. Givenfixed M and m, there are T = (Mm) ≥ (M)7m different tasks. Then, by Cor. 1, assuming
all pairs of tasks are weakly correlated, the potential gain in sample efficiency is exponential in m.
Disregarding correlation between samples from tasks with overlapping entities Corollary 1 and
Corollary 2 together suggest that the (up to) exponential increase in the number of available tasks can
significantly improve sample efficiency as compared to learning each task separately.
5
Under review as a conference paper at ICLR 2021
5	Exploiting Permutation Invariance through Multi-task
Reinforcement Learning
Our approach to exploiting permutation invariance is via multi-task reinforcement learning, where
each “task” corresponds to a particular choice of subset Ut ⊂ U . Furthermore, for each task, we
enforce permutation invariance among the entities i by forcing the neural network to apply the same
sequence of operations to the state input xi of each instrument through parameter sharing.
The proposed method, shown in Algorithm 1, learns a single policy by sampling subsequences of
trajectories from the different MDPs. At each step, we sample a task t according to a distribution
defined by task selection policy p. Then, a minibatch sample Bt is drawn from the replay buffer for
task t, and gradient descent is performed using the sampled transitions Bt (alternatively, samples can
be generated using policy rollouts for the specific task). Separate replay buffers maintained for each
task are updated only when the corresponding task is being used.
In contrast with other active sampling approaches in multi-task learning, our approach maintains an
estimate of the difficulty of each task t as a score, st . After each training step, we update the score
for only the sampled task based on minibatch Bt , avoiding evaluation over all the tasks. The scoring
functions depend on the sampled minibatch; to reduce fluctuations in scores for each task, exponential
smoothing is applied St — Yst + (1 - Y) ∙ Scorer(Bt). We propose a stochastic prioritization method
that interpolates between pure greedy prioritization and uniform random sampling. Our approach
is similar to prioritized experience replay (PER) by Schaul et al. (2016), but while classical PER
prioritizes samples, we prioritize tasks. The probability of sampling task t is pt = stα/ Pt0 stα0, where
the exponent α determines the degree of prioritization, with α = 0 corresponding to the uniform
case. We correct for bias with importance-sampling (IS) weights Wt = 1∕(Tpt)β, that compensate
for non-uniform probabilities if β = 1. We normalize weights by 1/ maxt wt. Tasks on which the
reward variance is high can be interpreted as having more challenging samples, hence reward variance
can be used as a scoring function.
Algorithm 1 Prioritized Multi-Task Reinforcement Learning for Increasing Sample Efficiency
Initialize policy network ∏θ
Initialize replay buffers R1 , . . . , RT
Initialize time steps nι J 1,... ,nτ J 1
loop
Select a task t 〜P to train on
Sample a random minibatch Bt of transitions (xn, an, rn, xn+1) from Rt
Update policy θ using Bt and chosen RL approach (correcting for bias using IS weights w)
Update score st J Yst + (1 - Y) ∙ Scorer(Bt)
Update ALL selection probabilities p and IS weights w
for n = nt, . . . , min{nt + ne, N } do
For task t, select action an according to current policy and exploration noise
Execute action an , and observe reward rn and new state xn+1
Store transition (xn, an, rn, xn+1) in Rt
end for
If n < N, update nt J n + 1, otherwise, update nt J 1
end loop
6	Experiments
6.1	Synthetic data
With the aim of validating the theory presented in Section 4, we define a synthetic example to explore
the efficiency gain afforded by permutation invariance. To do so, we control of the deviation
between any two tasks, thereby empirically validating the main theoretical results.
Consider a resource allocation problem where the observed state xi for each entity i ∈ {1 . . . m}
is a single scalar xi ∈ [0, 1]. The action space is the probability simplex, where each action
6
Under review as a conference paper at ICLR 2021
Figure 1: Performance for = 0.8 (left), = 0 (middle), and at N = 2000 with varying (right).
a = (a1 . . . am) indicates the fraction of resource allocated to each entity. The reward function is
R(x, a) :=	xiai - βiai log ai
i
where βi is a weight parameter for each entity. Note that when βi = β for all i, the reward
function becomes R(x, a) = (Pi xiai) + βH(a) where H is the Shannon entropy. This implies
that maximizing the reward involves a tradeoff between focusing resources on high xi or distributing
them uniformly across all i. Note that the reward function is permutation invariant, but that when we
allow a varying βi over the entities, the function deviates from being perfectly permutation invariant.
We use the range maxi βi - mini βi as a stand-in for . Let m = 10. For each , we run two
experiments. The first examines the performance of policies trained by LSPI using N real examples
drawn i.i.d from the state-action space, for N = 20 . . . 2000. A small Gaussian noise is added to each
reward to make learning harder. The second experiment uses only 20 real examples, but augments
the training set (up to N) through random permutation of the real examples. The first two figures
in Fig. 1 show the results for e = 0.8 and e = 0, respectively. Performance improves with N, as
predicted by the 1∕√N term in our error bound. Note that in the experiment using only 20 real
examples, a performance gain is achieved by using permuted examples; this corresponds precisely to
the multi-task gain predicted by the 1/√NT term. When e is large, there is a significant gap between
the results of the two experiments, as predicted by the e-term in the error bound. The last plot in
Fig. 1 shows this gap at N = 2000 when e varies from 0 to 0.8.
6.2	Real-word data
We consider two real-world resource allocation settings: financial portfolio optimization and meta
federated learning. Financial portfolio optimization is discussed below while meta federated learning
is in the Appendix. Given historical prices for a universe of financial assets, U , the goal of task t is
to allocate investments across a subset of assets Ut ⊆ U . The multiple tasks t thus correspond to
multiple portfolios of instruments. Permutation invariance will be of use in this setting since, from a
given universe of instruments (e.g. the 500 instruments in the S&P 500), an exponential number of
tasks can be generated, each with its own portfolio. Consider now one such task.
At the beginning of time period n, the action ai,n represents the fraction of wealth the decision maker
allocates to asset i. The allocations evolve over the time period due to changes in asset prices. Let
wi,n denote the allocation of asset i at the end of time period n. We model the state of an asset using
its current allocation and a window of its H most recent prices. In particular, let vi,n denote the close
price of asset i over time period n, and let yi,n = vi,n/vi,n-1 denote the ratio of close prices between
adjacent time periods 1. Then, the allocation in asset i at the end of time period n is given by
ai,n yi,n
wi,n = P----------------
i∈Ut ai,nyi,n
and the state of asset i at the beginning of time period n is given by
xi,n = (wi,n-1 , vi,n-H /vi,n-1 , . . . , vi,n-2 /vi,n-1 ).
1Daily high and low prices are also used in the state but omitted here for brevity.
7
Under review as a conference paper at ICLR 2021
Figure 2: Scatter plots of the maximum absolute
deviation from Equal CRP vs. the change in re-
wards every 50 steps (left) and the max. norm of
the gradient for the minibatch (right).
Out-of-Sample Universe
Figure 3: Mean performance gain over Equal CRP
of the learned policies when tested on 10 tasks us-
ing out-of-sample instruments. Error bars denote
the standard deviation over 10 experiments.
The change in portfolio value over period n depends on the asset prices and transaction costs incurred
in rebalancing the portfolio from (wi,n-1)i∈Ut to (ai,n)i∈Ut. The reward over period n is defined as
the log rate of return:
Rt (xn , an) = ln β ((wi,n-1 )i∈Ut , (ai,n)i∈Ut )	ai,nyi,n
i∈Ut
where β can be evaluated using an iterative procedure (see Jiang et al. (2017)). Defining the
reward this way is appealing because maximizing average total reward over consecutive periods is
equivalent to maximizing the total rate of return over the periods. To leverage this, we approximate
β((wi,n-1)i∈Ut, (ai,n)i∈Ut) ≈ c Pi∈Ut |wi,n-1 - ai,n|, where c is a commission rate to obtain a
closed-form expression for Rt(xn, an) (see Jiang et al. (2017)). We optimize using direct policy
gradient on minibatches of consecutive samples
1 nb+B-1
θ J θ + ηVθ B 工 WtRt (Xn ,∏θ (Xn)),
n=nb
where nb is the first time index in the minibatch, B the size of a minibatch, and wt the IS weight
for task t. As in Jiang et al. (2017), we sample nb from a geometric distribution that prioritises
recent samples and implement replay buffers for each task. A benchmark trading strategy is equal
constantly-rebalanced portfolio (CRP) that rebalances to maintain equal weights. As we noted earlier,
ideally one would prefer for the scoring function to depend only on the minibatch Bt . A deviation
from Equal CRP can be viewed as learning to exploit price movements, and is thus here we use this
as the goal of the policy. Prioritised MTL thus prioritises tasks which deviate from Equal CRP. Note
that the policy deviates from CRP only when profitable. Let
scorer(Bt) =	max
n∈{nb,...,nb+B-1}
πθ(xn)-日
∞
be the scoring of tasks in Prioritised MTL using mean absolute deviation of the minibatch allocation
from Equal CRP. Figure 2 (left) shows a scatter plot of the maximum score seen every 50 steps and
the change in episode rewards in a single-task learning experiment, and (right) of the minibatch
score and the maximum gradient norm for the minibatch. Higher scores imply higher variance
in the episode rewards and hence more challenging and useful samples. The correlation between
scores and gradient norms shows that our approach is performing gradient-based prioritisation,
(see Katharopoulos & Fleuret (2018); Loshchilov & Hutter (2015); Alain et al. (2015)) but in a
computationally efficient manner. The details of the dataset and parameter settings can be found in
the Appendix. Figure 3 shows the performance of the learned policies tested on 10 tasks drawn from
out-of-sample instruments. The policy network with weights initialized close to zero behaves like an
Equal CRP policy. As noted, any profitable deviation from Equal CRP implies learning useful trading
strategies. The plots show that the MTL policies perform well on instruments never seen during
training, offering a remarkable benefit for using RL in the design of trading policies. Fig. 4 shows the
performance of prioritised multi-task learning (MTL) versus single-task learning (STL) (i.e. learning
a policy for each task independently on the instruments in the task). We also show results for MTL
without prioritised sampling, i.e., with α = 0. We consider 5 tasks and 30 tasks. The plots show that
prioritised MTL performs significantly better than STL in both convergence time and final achieved
8
Under review as a conference paper at ICLR 2021
Figure 4: The two left plots show mean annualized return in the testing period over 10 experiments
(different instruments) each with 5 and 30 tasks. X-axes are scaled to make the curves comparable:
each epoch has 1500 (5-tasks) and 9000 steps (30-tasks) and an evaluation. Shaded regions denote
the interquartile range. The rightmost figure shows, for each fraction of tasks, the gain over Single
Task Learning (STL). A curve further to the right shows higher gain over STL. From 30% on the
y-axis, the P-MTL gain is higher (more towards the right) than the MTL gain. As expected, when few
tasks are used, prioritizing tasks doesn’t help much (y-axis from 0 to 0.2).
Figure 5: Comparison of a multi-task policy vs. a single-task policy on the testing period for a specific
task. The leftmost plot shows the percentage gain in portfolio value over time for both policies against
that from the baseline Equal CRP policy. The right two plots show the asset allocations.
performance. The performance with 30 tasks is significantly better than the performance with 5 tasks,
showing that our approach leverages the samples of the additional tasks.
Fig. 5 illustrates the typical behavior of a multi-task learning (MTL) and a single-task learning
(STL) policy on the test period for tasks where multi-task policy performed significantly better. The
single-task policy kept constant equal allocations while the multi-task policy was able to learn more
complex allocations. In financial data, strongly trending prices do not occur often and are inherently
noisy. Multi-task learning with permutation invariance helps with both challenges, allowing the
algorithm to learn more complex patterns in a given training period.
7 Conclusions
We introduce an approach for increasing the sample efficiency of reinforcement learning in a setting
with widespread applicability within the class of sequential resource allocation problems. This
property is permutation invariance: resources are allocated to entities according to a score, and
the order can change without modifying the optimal allocation. Under this property, we show that
a bound exists on the policy performance. This bound motivates a highly effective algorithm for
improving the policy through a multi-task approach. Using prioritized task-sampling, the method
not only improves the reward of the final policy but also renders it more robust. We illustrate the
property and the method on two important problems: sequential financial portfolio optimization and
meta federated learning, where the latter is provided in the Appendix.
9
Under review as a conference paper at ICLR 2021
References
Guillaume Alain, Alex Lamb, Chinnadhurai Sankar, Aaron Courville, and Yoshua Bengio. Variance
reduction in sgd by distributed importance sampling, 2015.
Andras Antos, Csaba Szepesvari, and Remi Munos. Learning near-optimal policies with bellman-
residual minimization based fitted policy iteration and a single sample path. Machine Learning, 71
(1):89-129, 2008.
Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, and Andreas Krause. Safe model-based
reinforcement learning with stability guarantees. In NIPS, 2017.
Timo Bram, Gino Brunner, Oliver Richter, and Roger Wattenhofer. Attentive multi-task deep
reinforcement learning. In ECML/PKDD, 2019.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Data-efficient model-based
reinforcement learning with deep probabilistic dynamics models. In NIPS 2018, 2018.
Ignasi Clavera, Jonas Rothfuss, John Schulman, Yasuhiro Fujita, Tamim Asfour, and Pieter Abbeel.
Model-based reinforcement learning via meta-policy optimization. ArXiv, abs/1809.05214, 2018.
Carlo D’Eramo, Davide Tateo, Andrea Bonarini, Marcello Restelli, and Jan Peters. Sharing knowledge
in multi-task deep reinforcement learning. In International Conference on Learning Representa-
tions, 2020. URL https://openreview.net/forum?id=rkgpv2VFvr.
Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon Schmitt, and Hado van
Hasselt. Multi-task deep reinforcement learning with popart. In AAAI, 2018.
Wenzhen Huang, Junge Zhang, and Kaiqi Huang. Bootstrap estimated uncertainty of the environment
model for model-based reinforcement learning. In AAAI, 2019.
Zhengyao Jiang, Dixing Xu, and Jinjun Liang. A deep reinforcement learning framework for the
financial portfolio management problem, 2017.
Angelos Katharopoulos and Francois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. In ICML, 2018.
Nan Rosemary Ke, Amanpreet Singh, Ahmed Touati, Anirudh Goyal, Yoshua Bengio, D. Parikh, and
Dhruv Batra. Modeling the long term future in model-based reinforcement learning. In ICLR,
2019.
Alessandro Lazaric and Mohammad Ghavamzadeh. Bayesian multi-task reinforcement learning. In
ICML, 2010.
Alessandro Lazaric, Mohammad Ghavamzadeh, and Remi Munos. Finite-sample analysis of least-
squares policy iteration. Journal of Machine Learning Research, 13(Oct):3041-3074, 2012.
Iou-Jen Liu, Raymond A. Yeh, and Alexander G. Schwing. PIC: permutation invariant critic for
multi-agent deep reinforcement learning. In 3rd Annual Conference on Robot Learning, CoRL
2019, Osaka, Japan, October 30 - November 1, 2019, Proceedings, pp. 590-602, 2019. URL
http://proceedings.mlr.press/v100/liu20a.html.
Ilya Loshchilov and Frank Hutter. Online batch selection for faster training of neural networks, 2015.
Emilio Parisotto, Jimmy Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and transfer
reinforcement learning. CoRR, abs/1511.06342, 2015.
Andrei A. Rusu, Neil C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James Kirkpatrick,
Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell. Progressive neural networks. ArXiv,
abs/1606.04671, 2016.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
International Conference on Learning Representations, 2016.
10
Under review as a conference paper at ICLR 2021
Iulian Serban, Chinnadhurai Sankar, Michael Pieper, Joelle Pineau, and Yoshua Bengio. The bottle-
neck simulator: A model-based deep reinforcement learning approach. ArXiv, abs/1807.04723,
2018.
Yee Whye Teh, Victor Bapst, Wojciech Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell,
Nicolas Manfred Otto Heess, and Razvan Pascanu. Distral: Robust multitask reinforcement
learning. In NIPS, 2017.
Hado van Hasselt, Arthur Guez, Matteo Hessel, Volodymyr Mnih, and David Silver. Learning values
across many orders of magnitude. In NIPS, 2016.
Tung-Long Vuong, Do Van Nguyen, Tai-Long Nguyen, Cong-Minh Bui, Hai-Dang Kieu, Viet-Cuong
Ta, QUoc-Long Tran, and Thanh Ha Le. Sharing experience in multitask reinforcement learning.
In IJCAI, 2019.
Aaron Wilson, Alan Fern, Soumya Ray, and Prasad Tadepalli. Multi-task reinforcement learning: a
hierarchical bayesian approach. In ICML ’07, 2007.
Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu Ma. Algorithmic framework for
model-based reinforcement learning with theoretical guarantees. ArXiv, abs/1807.03858, 2019.
Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob Fergus. Improving
sample efficiency in model-free reinforcement learning from images. ArXiv, abs/1910.01741,
2019.
Yang Yu. Towards sample efficient reinforcement learning. In IJCAI, 2018.
Marvin Zhang, Sharad Vikram, Laura Smith, Pieter Abbeel, Matthew J. Johnson, and Sergey Levine.
Solar: Deep structured latent representations for model-based reinforcement learning. ArXiv,
abs/1808.09105, 2018.
A Appendix
Theorem 1 Let M = hX, A, R, P, γi be an MDP with reward function R and transition kernel P.
Denote its Bellman operator by
(TπV)(x)
Rπ(x) + γ
Pn (dy∣x)V (y).
X
X
Given a policy π, define the Bellman difference operator between Mt and M to be Dtπ V =
TtπV - TπV. Apply the LSPI algorithm to M, by generating, at each iteration k, a path from M
of size N, where n satisfies Lemma 4 in Antos et al. (2008). Let V-1 ∈ F be an arbitrary initial
value function, V0,…，VK-I (Vθ,…，VK-ι) be the sequence of value functions (truncated value
functions) generated by the LSPI after K iterations, and πk be the greedy policy w.r.t. the truncated
value function Vk-1. Suppose also that
kD∏Vπkμ ≤ E ∀ ∏, and IIDnkVk-1∣∣* ≤ C ∀ k.
Then, with probability 1 - δ (with respect to the random samples), we have
kV尸-VtK kσ ≤ (ɪ »+γ)qCC0; [p1= (2√2E0(F)+
2 寸 τ d . ∕8lοg(8dK∕δ) 1、\ 厂
+ L	(V % / J + N ζ∣+ Ei
+ YK- Rmax + 3cq2Cσμ}.
11
Under review as a conference paper at ICLR 2021
Proof: For convenience, we will simply remove the task subscript whenever we refer to variables
associated with M. Define
d = Dn Vπ,
~ ~ ~
dt,k = Dnk Vk-ι,
ek = Vk -Tnk Vk,
Ek = Pnk+1(I - YPnk+1 )-1 - Pn* (I - YPnk)-1
Fk = Pnk+1 (I - YPnk+1 )-1 + Pn* (I - YPnk)-1
From the proof of Lemma 12 in Antos et al. (2008), we get
K-1
Vn* - Vnκ ≤ Y X (YPn* )κ-k-1Ekek + (yPn* )k(Vn* - Vno).
k=0
By applying the above inequality, and taking the absolute value on both sides point-wise, we get
*
IVnt- VnK |
=IVn* - Vn *
| + |Vn* - Vnκ | + |Vnκ - VnKI
≤Y
≤Y
K-1
X (YP
k=0
K-1
X (YP
k=0
L *
(7Γ
(π
)κ-k-1Fk∣ek| + (yPn*)k|Vn* - Vno| + |Vn* - Vn* | + |VnK - VnKI
)K-k-1FkIek i+⅛x YK+|VL-V n*|+1V nK-VtnKI
I	*
where we used the fact that ∣Vn
∣Vn* - Vn* | and |VTK
-Vno | ≤ (2Rmax/(1 - γ))1. Next, we derive upper bounds for
(a) Observe that
L*	*	l*l*	*	*
Vnt - Vn = Tnt Vnt - Tn Vn
L *	L *	*	*
≤ Tnt Vnt - Tnt Vn
* * *
=Tnt Vnt - Tnt Vnt + Tnt (Vnt - Vn )
≤ (i-yPt*)-1dΓ*.
The first inequality follows from the fact that π* is optimal with respect to Vn*. The second
inequality follows from the taylor expansion of the inverse term. By closely following the
same steps, we also get
,* *
_ *	*	_ *	_ *	*
Vnt - Vn = Tnt Vnt - Tn V
_ *
-Tr
≥Tn * Vn*
=Tn * Vn *
-Tπ
-Tπ
,*
,*
V n*
Vn* + τn* (Vn* - Vn*)
*
-VnK |.
*
*
*
By splitting into positive and negative components and applying the above bounds, we get
-* * — * * — * *
IVnt- Vn | = |( Vnt- Vn )+ -(Vnt- Vn )-|
≤
-*	* — * *
|(Vnt- Vn )+| + |(Vnt- Vn )-|
≤I(i-YPn* )-1dn: i + ∣(i-YPn * )-1dn * ∣
≤ (i-ypt* )-1∣dn: ∣ + (i-YPn * )-1∣dn* i
12
Under review as a conference paper at ICLR 2021
(b)
Observe that
VπK - Vπκ ≤ TKK VπK
一 〜 一〜 一 _
+ TnKVk-1 -TrK Vk-1 -TnK VnK
TKK V
∏k
一〜 一〜 _ _ _ _
+ TKKVK-I -TrKVK-I -TnKVKK + TnK(VKK
-VKK)
≤ (I -YPtKK) 1 (-dKK - dt,κ).
The first inequality follows from the fact that ∏κ is optimal with respect to VK-1. The
second inequality follows from the taylor expansion of the inverse term. By closely following
the same steps, we also get
_ _ _ _ 一 ~ 一 ~ _ _
Vkk - VKK ≥ Tkk Vkk - Tkk VK-I + TlK VK-I - TKK VKK
一 _ _ ~ 一 ~ 一 _ _ _ _
TKK VKK -TKK VK-I + TtKK VK-I -T	VKK + TKK (VKK - VKK)
≥ (I -YPtKK )T (-dKK + dt,K).
By splitting into positive and negative components and applying the above bounds, we get
|Vkk - VKKI = |(Vkk - VKK)+ - (Vkk - VKK)-1
≤ |(Vkk - VKK)+∣ + |(Vkk - VKK)-∣
=I(I-YPtKK)-1(-dKK - dt,K)|+ I(I-YPtKK)-1(-dKK + dt,K)|
≤ (I -YPtKK)-1| - dKK - dt,K|+ (I-YPKK)-1| - dKK + dt,K|
≤ 2(I-YPtKK)-1(|dKK| + |dt,K|).
By applying the upper bounds from (a) and (b), we get
*
IVKt- VKK | ≤
2(1 - Yk+2)
(i-y)2
K-1
):αk Ak |ek | + α(Rmax/Y)
_k=0
+ (β∕6)Bκ* ∙ 6|dK* | + (β∕6)Bκ* ∙ 6|dK* |
+ (β∕3)BKK ∙ 6|dKK | + (β∕3)Bκκ ∙ 6|MK|
where we introduced the positive coefficients
(1 - Y)
1 - yk+2 第
(I-Y) Y
1 - yK+2 Y
(1 - Y)
κ-k, for 0 ≤ k<K,
K+1
Qk
Q
β
2(1 - γk+2),
and the operators
Let λκ
2(1-YK +2)
Bκ
P
l-2(Pκ*)K-k-1Fk, for 0 ≤ k<κ,
(1-Y)(I-YPtK )-1∙
.Note that the coefficients Qk, α, and β, sum to 1, and the operators are
Ak
[
TT-YF
]
positive linear operators that satisfy Ak 1 = 1 and Bκ 1 = 1. Therefore, by taking the Pth power on
both sides, applying Jensen,s inequality twice, and then integrating both sides with respect to σ(x),
we get
kVκt - VKKkP,σ = J b(dx)|VKt- VKKIP
K-1
≤ λκ σ X Qk Ak |ek |P + Q(Rmax∕γ)p
_k=0
+ (β∕6)Bκ* (6|dK* |)p + (β∕6)Bκ* (6|dK* |)p
+ (β∕3)Bκκ (6|dKKI)P + (β∕3)Bκκ (6|dt,K I)p
13
Under review as a conference paper at ICLR 2021
From the definition of the coefficients cσ,μ(m), We get
σAk ≤ (1 - Y) E Ymcσ,μ(m + K - k)μ,
m≥0
σBπ ≤ (I-Y) E Ymcσ,μ(m)μ.
m≥0
Therefore, it folloWs that
K-1	K-1
σ X akAk |ek|p ≤ (1 - Y) X ak X γmcσ,μ(m + K - k)μ∣ek |p
k=0	k=0	m≥0 2 K-1 =γ⅛⅛ XX Ym+κ-kτcσ,μ (m + K - k)∣ekkp,μ - Y	k=0 m≥0 ≤ 1 - Yk+2 Cσ,μep
where e = maxo≤k<κ Ilekkp,μ. The terms involving Bn satisfy
σ [Bπ(6∣d∏∣)p] ≤ 6p(1-Y) X Ymcσ,μ(m)μ∣d∏|p ≤ 65；“|固唱,“.
m≥0
Putting all these together, and choosing p = 2, we get
IIVπt	V∏k Il < λ1	Y C00 尸2_1_ (I - Y)Y +	R R Al2	_i_	36(I - Y) C0	2	一
kVt	- Vt kσ ≤ λκ	[ι - YK+2 Cσ,μe	+	1-)仆2	(Rmax/y)	+ 2(]-)仆2) Cσ,μe
≤ (1 - Y)2 lCσ,μe + (I - Y)YK+1(Rmax/y)2 +	([ Y' Cσ/
2	一	C	一」	C	- 1
≤ (1-^)2 [cσ,μe2 + YK+1 (Rmax/y)2 + 18Cσ川2]2
≤ 二y hqCσZe + YK- Rmax + 注产：i .
The desired result can then be obtained by applying the same steps as in the proof of Theorem 8
in Lazaric et al. (2012).

A. 1 Financial Portfolio Optimization: Additional Details
The dataset consists of daily prices for 68 instruments in the technology and communication sectors
from 2009 to 2019. We use 2009-2018 for training and 2019 for testing. To validate that our approach
learns common features across instruments, and thus can transfer, we reserve 18 instruments not
seen during training for further testing. The global asset universe U used for training contains 50
instruments.
We construct tasks by randomly choosing a portfolio of |Ut| = 10 instruments for each task. We
create a permutation invariant policy network by applying the same sequence of operations to every
instrument state. That is, for each instrument, the flattened input prices are passed through a common
RNN with 25 hidden units and tanh activation, this output is concatenated with the latest allocation
fraction of the instrument, and passed through a common dense layer to produce a score. Instrument
scores are passed to a softmax function to produce allocations that sum to one. The smoothing
parameter for the scores Y = 0.2, α = 0.5 for the task prioritisation parameter and β = 1.0 to fully
compensate for the prioritized sampling bias.
A.2 Meta Federated Learning
Suppose we have a universe of federated learning clients U . The goal of task t is to aggregate models
in a federated learning experiment over a subset of clients Ut ⊆ U . At each step n, the action ai,n
14
Under review as a conference paper at ICLR 2021
Figure 6: These plots compare the behavior of a multi-task policy and a single-task policy during
testing. FedAvg denotes the accuracy of federated learning with uniform averaging. The left plot
shows the accuracy of the aggregate model during federated learning. The right two plots show the
weights produced by the policy for different clients. Note that clients 8 and 9 possess 40% of the
unique labels.
represents the weight assigned to the supervised learning model of client i in the averaging procedure.
Let vi,n denote the model of the client (i.e. the tensor of model parameters). We model the state
of the client as some function of its H most recent models xi,n = f (vi,n-H+1 , . . . , vi,n). Assume
that the aggregator has access to a small evaluation dataset that it can use to approximately assess
the quality of models. We define the reward at each step to be the accuracy of the aggregate model,
Rt(xn, an) = L i∈U ai,nvi,n , where L(v) is a function that provides the accuracy of a model v
on the evaluation dataset. Therefore, by maximizing the total return over all time periods, we seek to
maximize both the accuracy at the final time step as well as the time to convergence. We optimize the
policy using Proximal Policy Optimization (PPO).
We use the MNIST digit recognition problem. Each client observes 600 samples from the train dataset
and trains a classifier composed of one 5x5 convolutional layer (with 32 channels and ReLu activation)
and a softmax output layer. We use the same permutation invariant policy network architecture as
before with 10 hidden units in the RNN. We randomly select |Ut| = 10 clients for each task. We
learn using an evaluation dataset comprised of 1000 random samples from the test dataset and test
using all 10000 samples in the test dataset. We fix the number of federated learning iterations to 50.
We explore the benefit of MTL in identifying useful clients in scenarios with skewed data distribution.
We partition the dataset such that 8 of the clients in each task observe random digits between 0 to 5
and the remaining 2 clients observe random digits between 6 to 9. Therefore, for each task, 20% of
the clients possess 40% of the unique labels. The state of each client are the accuracies of its H most
recent models on the evaluation dataset.
Figure 6 shows the potential benefits of multi-task learning when simulators are inaccurate. In
particular, we obtain two aggregation policies, one trained using single-task learning (STL), and
another trained using multi-task learning (MTL), both trained using the same number of steps, and
we observe their behavior during testing. The plots show that multi-task learning is able to learn
non-uniform averaging policies that improve the convergence and performance of federated learning
runs. More importantly, it can perform better than single-task learning even with the same number
of samples. This may be attributed to the wider variety of client configurations (and consequently
experiences) in the multi-task approach.
15