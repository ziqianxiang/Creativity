Under review as a conference paper at ICLR 2021
Meta-learning Transferable Representations
with a Single Target Domain
Anonymous authors
Paper under double-blind review
Ab stract
Recent works found that fine-tuning and joint training—two popular approaches
for transfer learning—do not always improve accuracy on downstream tasks. First,
we aim to understand more about when and why fine-tuning and joint training can
be suboptimal or even harmful for transfer learning. We design semi-synthetic
datasets where the source task can be solved by either source-specific features or
transferable features. We observe that (1) pre-training may not have incentive to
learn transferable features and (2) joint training may simultaneously learn source-
specific features and overfit to the target. Second, to improve over fine-tuning
and joint training, we propose Meta Representation Learning (MeRLin) to learn
transferable features. MeRLin meta-learns representations by ensuring that a head
fit on top of the representations with target training data also performs well on
target validation data. We also prove that MeRLin recovers the target ground-truth
model with a quadratic neural net parameterization and a source distribution that
contains both transferable and source-specific features. On the same distribution,
pre-training and joint training provably fail to learn transferable features. MeRLin
empirically outperforms previous state-of-the-art transfer learning algorithms on
various real-world vision and NLP transfer learning benchmarks.
1 Introduction
Transfer learning—transferring knowledge learned from a large-scale source dataset to a small target
dataset—is an important paradigm in machine learning (Yosinski et al., 2014) with wide applications
in vision (Donahue et al., 2014) and natural language processing (NLP) (Howard & Ruder, 2018;
Devlin et al., 2019). Because the source and target tasks are often related, we expect to be able to
learn features that are transferable to the target task from the source data. These features may help
learn the target task with fewer examples (Long et al., 2015; Tamkin et al., 2020).
Mainstream approaches for transfer learning are fine-tuning and joint training. Fine-tuning initializes
from a model pre-trained on a large-scale source task (e.g., ImageNet) and continues training on
the target task with a potentially different set of labels (e.g., object recognition (Wang et al., 2017;
Yang et al., 2018; Kolesnikov et al., 2019), object detection (Girshick et al., 2014), and segmentation
(Long et al., 2015; He et al., 2017)). Another enormously successful example of fine-tuning is in
NLP: pre-training transformers and fine-tuning on downstream tasks leads to state-of-the-art results
for many NLP tasks (Devlin et al., 2019; Yang et al., 2019). In contrast to the two-stage optimization
process of fine-tuning, joint training optimizes a linear combination of the objectives of the source
and the target tasks (Kokkinos, 2017; Kendall et al., 2017; Liu et al., 2019b).
Despite the pervasiveness of fine-tuning and joint training, recent works uncover that they are not
always panaceas for transfer learning. Geirhos et al. (2019) found that the pre-trained models learn the
texture of ImageNet, which is biased and not transferable to target tasks. ImageNet pre-training does
not necessarily improve accuracy on COCO (He et al., 2018), fine-grained classification (Kornblith
et al., 2019), and medical imaging tasks (Raghu et al., 2019). Wu et al. (2020) observed that large
model capacity and discrepancy between the source and target domain eclipse the effect of joint
training. Nonetheless, we do not yet have a systematic understanding of what makes the successes of
fine-tuning and joint training inconsistent.
The goal of this paper is two-fold: (1) to understand more about when and why fine-tuning and
joint training can be suboptimal or even harmful for transfer learning; (2) to design algorithms that
overcome the drawbacks of fine-tuning and joint training and consistently outperform them.
1
Under review as a conference paper at ICLR 2021
To address the first question, we hypothesize that fine-tuning and joint training do not have incentives
to prefer learning transferable features over source-specific features, and thus their capability of
learning transferable features is rather accidental depending on the property of the datasets. To
empirically analyze the hypothesis, we design a semi-synthetic dataset that contains artificially-
amplified transferable features and source-specific features simultaneously in the source data. Both
the transferable and source-specific features can solve the source task, but only transferable features
are useful for the target. We analyze what features fine-tuning and joint training will learn. See Figure
1 for an illustration of the semi-synthetic experiments. We observed following failure patterns of
fine-tuning and joint training on the semi-synthetic dataset.
•	Pre-training may learn non-transferable features that don’t help the target when both transferable
and source-specific features can solve the source task, since it’s oblivious to the target data. When
the dataset contains source-specific features that are more convenient for neural nets to use, pre-
training learns them; as a result, fine-tuning starting from the source-specific features does not lead
to improvement.
•	Joint training learns source-specific features and overfits on the target. A priori, it may appear
that the joint training should prefer transferable features because the target data is present in the
training loss. However, joint training easily overfits to the target especially when the target dataset
is small. When the source-specific features are the most convenient for the source, joint training
simultaneously learns the source-specific features and memorizes the target dataset.
Toward overcoming the drawbacks of fine-tuning and joint training, we first note that any proposed
algorithm, unlike fine-tuning, should use the source and the target simultaneously to encourage
extracting shared structures. Second and more importantly, we recall that good representations should
enable generalization: we should not only be able to fit a target head with the representations (as joint
training does), but the learned head should also generalize well to a held-out target dataset. With this
intuition, we propose Meta Representation Learning (MeRLin) to encourage learning transferable
and generalizable features: we meta-learn a feature extractor such that the head fit to a target training
set performs well on a target validation set. In contrast to the standard model-agnostic meta-learning
(MAML) (Finn et al., 2017), which aims to learn prediction models that are adaptable to multiple
target tasks from multiple source tasks, our method meta-learns transferable representations with
only one source and one target domain.
Empirically, we first verify that MeRLin learns transferable features on the semi-synthetic dataset. We
then show that MeRLin outperforms state-of-the-art transfer learning baselines in real-world vision
and NLP tasks such as ImageNet to fine-grained classification and language modeling to GLUE.
Theoretically, we analyze the mechanism of the improvement brought by MeRLin. In a simple
two-layer quadratic neural network setting, we prove that MeRLin recovers the target ground truth
with only limited target examples whereas both fine-tuning and joint training fail to learn transferable
features that can perform well on the target.
In summary, our contributions are as follows. (1) Using a semi-synthetic dataset, we analyze and
diagnose when and why fine-tuning and joint training fail to learn transferable representations. (2)
We design a meta representation learning algorithm (MeRLin) which outperforms state-of-the-art
transfer learning baselines. (3) We rigorously analyze the behavior of fine-tuning, joint training, and
MeRLin on a special two-layer neural net setting.
2	Setup and Preliminaries
In this paper, we study supervised transfer learning. Consider an input-label pair (x, y) ∈ Rd × R.
We are provided with a source distributions Ds and a target distribution Dt over Rd × R. The
source dataset Dbs = {xis, yis}in=s1 and the target dataset Dbt = {xit, yit}in=t 1 consist of ns i.i.d. samples
from Ds and nt i.i.d. samples from Dt respectively. Typically ns nt. We view a predictor as a
composition of a feature extractor hφ : Rd → Rm parametrized by φ ∈ Φ, which is often a deep
neural net, and a head classifier gθ : Rm → R parametrized by θ ∈ Θ, which is often linear. That is,
the final prediction is fθ,φ(χ) = gθ(hφ(χ)). Suppose the loss function is '(∙, ∙), such as cross entropy
loss for classification tasks. Our goal is to learn an accurate model on the target domain Dt .
Since the label sets of the source and target tasks can be different, we usually learn two heads for
the source task and the target task separately, denoted by θs and θt , with a shared feature extractor
2
Under review as a conference paper at ICLR 2021
A. Source With B. Source With source-
transferable features
Pre-train
Transferable
features
Fine-tune
Good performance
on the target
specific features
Target
Pre-train
Non-transferable
features
en worse than
target-only
Fine-tune ∣
Figure 1: Comparison of fine-tuning, joint training, and MeRLin on the semi-synthetic dataset. Left:
The semi-synthetic dataset and the qualitative observations on the representations learned by three
algorithms. Right: Quantitative results on the target test accuracy. See more interpretations, analysis,
and results in Section 3.
φ. Let LDb (θ, φ) be the empirical loss of model gθ(hφ(x)) on the empirical distribution Db, that is,
LD (θ, φ) := E(X y)∈D'(gθ (hφ(χ)), y) where (x, y) ∈ D means sampling uniformly from the dataset
Db. Using this notation, the standard supervised loss on the source (with the source head θs) and loss
on the target (with the target head θt) can be written as LDb (θs, φ) and LDb (θt, φ) respectively.
We next review mainstream transfer learning baselines and describe them in our notations.
Target-only is the trivial algorithm that only trains on the target data Dbt with the objective LDb (θt, φ)
starting from random initialization. With insufficient target data, target-only is prone to overfitting.
Pre-training starts with random initialization and pre-trains on the source dataset with objective
function LDb (θs, φ) to obtain the pre-trained feature extractor φpre and head θs.
Fine-tuning initializes the target head θt randomly and initializes the feature extractor φ by φpre
obtained in pre-training, and fine-tunes φ and θt on the target by optimizing LDb (θt, φ) over both θt
and φ. Note that in this paper, fine-tuning refers to fine-tuning all layers by default.
Joint training starts with random initialization, and trains on the source and target dataset jointly
by optimizing a linear combination of their objectives over the heads θs , θt and the shared feature
extractor φ: minθs,θt,φ Ljoint(θs, θt, φ) := (1 - α)LDb (θs, φ) + αLDb (θt, φ). The hyper-parameter
α is used to balance source training and target training. We use cross-validation to select optimal α.
3	Limitations of Fine-tuning and Joint Training: Analysis on
Semi-synthetic Data
Previous works (He et al., 2018; Wu et al., 2020) have observed cases when fine-tuning and joint
training fail to improve over target-only. Our hypothesis is that both pre-training and joint training do
not have incentives to prefer learning transferable features over source-specific features, and thus
the performance of fine-tuning and joint training rely on whether the transferable features happen
to be the best features for predicting the source labels. Validating this hypothesis on real datasets
is challenging, if not intractable—it’s unclear what’s the precise definition or characterization of
transferable features and source-specific features. Instead, we create a semi-synthetic dataset where
transferable features and source-specific features are prominent and well defined.
A semi-synthetic dataset. The target training dataset we use is a uniformly-sampled subset of the
CIFAR-10 training set of size 500. The target test dataset is the original CIFAR-10 test set. The
source dataset of size 49500, denoted by AB, is created as follows. The upper halves of the examples
are the upper halves of the CIFAR-10 images (excluding the 500 example used in target). The lower
halves contain a signature pattern that strongly correlates with the class label: for class c, the pixels
of the lower half are drawn i.i.d. from gaussian distribution N (c/10, 0.22). Therefore, averaging
the pixels in the lower half of the image can reveal the label because the noise will get averaged out.
The benefit of this dataset is that any features related to the top half of the images can be defined as
transferable features, whereas the features related to the bottom half are source-specific. Moreover,
we can easily tell which features are used by a model by testing the performance on images with
masked top or bottom half. For analysis and comparison, we define A to be the dataset that contains
the top half of dataset AB and zeros out the bottom half, and B vice versa. See Figure 1 (left) for an
illustration of the datasets. Further details are deferred to Section A.1
3
Under review as a conference paper at ICLR 2021
(a) T-SNE embeddings of features on the target dataset.
Figure 2: (a) T-SNE visualizations of features on the target train and test set. The representations
of pre-training work poorly on both target train and test set, indicating that transferable features are
not learned. Both joint training and fine-tuning work well on the target train set but poorly on the
test set, indicating overfitting. MeRLin works well on the target test set. (b) Evaluation of different
methods on A and B. Joint-training and pre-training rely heavily on the source-specific feature B
and learn the transferable feature A poorly compared to MeRLin. See more details in Section 3.
(b) Ablation.
In Figure 1 (right), we evaluate various algorithms’ performance on target test data. In Figure 2(a)
(left), we run algorithms with AB being the source dataset and visualize the learned features on
the target training dataset and target test dataset to examine the generalizability of the features. In
Figure 2(a) (right), we evaluate the algorithms on the held-out version of dataset A and B to examine
what features the algorithms learn. ResNet-32 (He et al., 2016) is used for all settings.
Analysis: First of all, target-only has low accuracy (38%) because the target training set is small.
Except when explicitly mentioned, all the discussions below are about algorithms on the source AB.
Fine-tuning fails because pre-training does not prefer to learn transferable features and fine-tuning
overfits. Figure 2(b) (pre-training) shows that the pre-trained model has near-trivial accuracy on held-
out A but near-perfect accuracy on held-out B, indicating that it solely relies on the source-specific
feature (bottom half) and does not learn transferable features. Figure 2(a) (pre-training) shows that
indeed pre-trained features do not have even correlation with target training and test sets. Figure 2(a)
(fine-tuning) shows that fine-tuning improves the features’ correlation with the training target labels
but it does not generalize to the target test because of overfitting. The performance of fine-tuning
(with source =AB) in Figure 1 (right) also corroborates the lack of generalization.
Joint training fails because it simultaneously learns mostly source-specific features and features that
overfit to the target. Figure 2(b) (joint training) shows that the joint training model performs much
better on held-out B (with 92% accuracy) than on the held-out A (with 46% accuracy), indicating it
learns the source-specific feature very well but not the transferable features. The next question is what
features joint training relies on to fit the target training labels. Figure 2(a) shows strong correlation
between joint training model’s features and labels on the target training set, but much less correlation
on the target test set, suggesting that the joint training model’s feature extractor, applied on the target
data (which doesn’t have source-specific features), overfits to the target training set. This corroborates
the poor accuracy of joint training on the target test set (Figure 1), which is similar to target-only’s.1
In Section 5, we rigorously analyze the behavior of these algorithms on a more simplified settings
and show that the phenomena above can theoretically occur.
4	MeRLin: Meta Representation Learning
In this section, we design a meta representation learning algorithm that encourages the discovery of
transferable features. As shown in the semi-synthetic experiments, fine-tuning does not have any
incentive to learn transferable features if they are not the most convenient for predicting the source
labels because it is oblivious to target data. Thus we have to use the source and target together to
1As sanity checks, when the source contains only transferable features (Figure 1, right, source = A), fine-
tuning works well, and when no transferable features (Figure 1, right, source = B), it does not.
4
Under review as a conference paper at ICLR 2021
learn transferable representations. A natural attempt would have been joint training, but it overfits to
the target when the target data is scarce as shown in the t-SNE visualizations in Figure 2(a).
To fix the drawbacks of the joint training, we recall that good representations should not only work
well for the target training set but also generalize to the target distribution. More concretely, a
good representation hφ should enable the generalization of the linear head learned on top of it—a
linear head θ that is learned by fixing the feature hφ (x) as the inputs should generalize well to a
held-out dataset. We design a bi-level optimization objective to learn such features, inspired by meta-
learning for fast adaptation (Finn et al., 2017) and learning-to-learn for automatic hyperparameter
optimization (Maclaurin et al., 2015; Thrun & Pratt, 2012) (more discussions below.)
We first split the target training set Dbt randomly into Dbttr and Dbtval. Given a feature extractor φ, let
θt(φ) be the linear classifier learned by using hφ (x) as the inputs on the dataset Dttr.
θt(φ) = arg min LDbtr (θ, φ)	(1)
θt
Note that θt(φ) depends on the choice by φ (and is almost uniquely decided by it because the objective
is convex in θ.) As alluded before, our final objective involves the generalizability of θt (φ) to the
held-out dataset Dbtval:
Lmeta,t(φ) = LDvaI (θt(φ), φ) = E(χ,y)∈Dbvai'(gbt(φ) (hφ(x)), y)	⑵
The final objective is a linear combination of Lmeta,t(φ) with the source loss
minimize Lmeta(φ,θs) := LD (θs,φ) + P ∙ Lmeta,t(φ)	(3)
φ∈Φ,θs∈Θ	s
To optimize the objective, we can use standard bi-ievei optimization technique as in iearning-to-iearn
approaches. We aiso design a sped-up version of MeRLin by changing the ioss to squared ioss so
that the θt(φ) has an anaiyticai soiution. More detaiis are provided in Section A.3 (Aigorithm 2).
Comparison to other meta-learning work. The key distinction of our approach from MAML (Finn
et ai., 2017) and other meta-iearning aigorithms (e.g., (Nichoi et ai., 2018a; Bertinetto et ai., 2019))
is that we oniy have a singie source task and a singie target task. Recent work (Raghu et ai., 2020)
argues that feature reuse is the dominating factor of the effectiveness of MAML. In our case, the
training target task is exactiy the same as the test task, and thus the oniy possibie contributing factor
is a better-iearned representation instead of fast adaptation. Our aigorithm is in fact cioser to the
work on hyperparameter optimization (Maciaurin et ai., 2015; Zoph & Le, 2016)—if we view the
parameters of the head θt as m hyperparameters and view φ and θs as the ordinary parameters, then
our aigorithm is tuning hyperparameters on the vaiidation set using gradient descent.
4.1	MeRLin Learns Transferable Features on Semi-Synthetic Dataset
We verify that MeRLin iearns transferabie features in the semi-synthetic setting of Section 3 where
fine-tuning and joint training faii. Figure 1 (right) shows that MeRLin outperforms fine-tuning and
joint training by a iarge margin and is ciose to fine-tuning from the source A, which can be aimost
viewed as an upper bound of any aigorithm’s performance with AB as the source. Figure 2(b) shows
that MeRLin (trained with source = AB) performs weii on A, indicating it iearns the transferabie
features. Figure 2(a) (MeRLin, train& test) corroborates the conciusion.
5	Theoretical Analysis with Two-layer Quadratic Neural Nets
The experiments in Section 3 demonstrate the weakness of fine-tuning and joint training. On the
other hand, MeRLin is abie to iearn the transferabie features from the source datasets. In this section,
we instantiate transfer iearning in a quadratic neurai network where the aigorithms can be rigorousiy
studied. For a specific data distribution, we prove that (1) fine-tuning and joint training faii to iearn
transferabie features, and (2) MeRLin recovers target ground truth with iimited target exampies.
Models. Consider a two-iayer neurai network fθ,φ(x) = gθ(hφ(x)) with gθ(z) = θ>z and hφ =
σ(φ>x), where φ = [φι, φ2,∙∙∙ , φm] ∈ Rd×m is the weight of the first layer, θ ∈ Rm is the linear
head, and σ(∙) is element-wise quadratic. We consider squared loss '(fθ,φ(x), y) = (fθ,φ(x) - y)2.
5
Under review as a conference paper at ICLR 2021
Source distribution. Let k ∈ Z+ such that 2 ≤ k ≤ d. We consider the following source distribution
which can be solved by multiple possible feature extractors. Let x[i] denotes the i-th entry of x ∈ Rd .
Let y = 0 happens with prob. 1/3, and conditioned on y = 0, we have x[i] = 0 for i ≤ k, and
x[i]〜{±1,0} uniformly randomly and independently for i > k. With prob. 2∕3 We have y = 1, and
conditioned on y = 1, We have x[i]〜{±1} uniformly randomly and independently for i ≤ k, and
x[i]〜{±1,0} uniformly randomly and independently for i > k.
The design choice here is that x[1] , . . . , x[k] are the useful entries for predicting the source label,
because y = x[2i] for any i ≤ k. In other Words, features σ(e[>i]x) for i ≤ k are useful features to
learn, and any linear mixture of them Works. All other entries ofx are independent With the label y.
Target distribution. The target distribution is exactly k = 1 version of the source distribution.
Therefore, y = x[21], and σ(e[>1]x) is the correct feature extractor for the target. All other x[i] for i > 1
are independent With the label.
Source-specific features and transferable features. As mentioned before, σ(e>R, ∙ , σ(e^x)
are all good features for the source, Whereas only σ(e[>1]x) is transferable to the target.
Since usually the source dataset is much larger than target, We assume access to infinite source data
for simplicity, so Ds = Ds . We assume access to nt target data Dt .
Regularization: Because the limited target data, the optimal solutions With unregularized objective
are often not unique. Therefore, we study '2-regularized version of the baselines and MeRLin. The
regularization strength λ > 0 is selected to achieve optimal LDt. The regularized MeRLin objective
is Lλmeta(θs, φ) := Lmeta(φ, θs) + λ(kθsk2 + kφk2F). The regularized joint training objective is
Ljλoint(θs,θt,φ) := Ljoint(θs,θt, φ)+λ(kθsk2+kθtk2+kφk2F). We also regularize the tWo objectives
in the pre-training and fine-tuning. We pre-train With LλD (θs, φ) := LDs (θs, φ) + λ(kθs k2 + kφk2F),
^
and then only fine-tune the head2 by minimizing the target loss Lʌ,°pre(θt):= LDjθt,φp1∙e) + λ ∣∣θtk2.
The folloWing theorem shoWs that neither joint training nor fine-tuning is capable of recovering the
target ground truth given limited number of target data.
Theorem 1. There exists universal constants c ∈ (0, 1) and > 0, such that so long as nt ≤ cd, for
any λ > 0, the following statements are true:
•	With prob. at least 1 一 4 exp(一Ω(d)) ,the solution (θs,θt, φjoint) ofthe joint training satisfies
LDt (θt, φjoint) ≥ .	(4)
1
•	With prob. at least 1 — 1 (over the randomness of pre-training), the solution (θt,φpre) Ofthe
head-only fine-tuning satisfies
LDt (θt,Φpre) ≥ E∙	(5)
As Will be shoWn in the proof, not surprisingly, fine-tuning fails because it learns a random feature
σ(e[>i]x) (Where i ∈ [k]) for the source during pre-training Which does not transfer to the target When
i 6= 1. Joint training fails because it uses one neuron to learn a generalized linear model to overfit the
target nt training data exactly, and then use another neuron to learn a random feature σ(e[>i]x) (Where
i ∈ [k]) for the source. The proof of Theorem 1 is deferred to Section B.
In contrast, the folloWing theorem shoWs that MeRLin can recover the ground truth of the target task:
Theorem 2. For any λ < λ0 where λ0 is some universal constant and any failure rate ξ > 0, if the
k
target Set Size n > Θ(log ξ), With probability at least 1 一 ξ, the feature extractor φmeta found by
tr
MeRLin and the head θt(φmeta) trained on Dttr recovers the ground truth of the target task:
T ( f∖ ( r^L ∖	?	、 C
LDt (θt (φ
meta),φ
meta ) = 0∙
(6)
Intuitively, MeRLin learns the transferable feature σ(e[>1]x) because it simultaneously fits the source
and enables the generalization of the head on the target. The proof can be found in Section B.
2For theoretical analysis We consider only fine-tuning θt . It is Worth noting that fine-tuning both θt and φ
converges to the same solution as target-only training, Which also has large generalization gap due to overfitting.
6
Under review as a conference paper at ICLR 2021
Table 1: Accuracy (%) on computer vision tasks.
Source	Fashion	SVHN I		ImageNet		Food-101
Backbone	LeNet		ReSNet-18			
Target	USPS (600)		CUB-200	Caltech-256	Stanford Cars	CUB-200
Target-only	91.07 ± 0.45	91.07 ± 0.45	32.05 ± 0.67	45.63 ± 1.26	23.22 ± 1.02	32.13 ± 0.64
Joint training	89.59 ± 0.56	91.54 ± 0.32	55.81 ± 1.36	78.20 ± 0.50	63.25 ± 0.72	42.08 ± 0.59
Fine-tuning	90.80 ± 0.20	92.12 ± 0.39	72.52 ± 0.51	81.12 ± 0.27	81.59 ± 0.49	52.30 ± 0.51
L2-sp	89.74 ± 0.41	91.86 ± 0.27	73.20 ± 0.38	82.31 ± 0.22	81.26 ± 0.27	53.84 ± 0.37
MeRLin 193.34 ± 0.41193.10 ± 0.38∣75.42 ± 0.47182.45 ± 0.261 83.68 ± 0.57 158.68 ± 0.43
Table 2: Accuracy (%) ofBERT-base on GLUE sub-tasks dev set.
Target	MRPC	RTE	QNLI
Fine-tuning	83.74 ± 0.93	68.35 ± 0.86	91.54 ± 0.25
L2-sp	84.31 ± 0.37	67.50 ± 0.62	91.29 ± 0.36
MeRLin-ft	86.03 ± 0.25	70.22 ± 0.86	92.10 ± 0.27
6	Experiments
We evaluate MeRLin on several vision and NLP datasets. We show that (1) MeRLin consistently
improves over baseline transfer learning algorithms including fine-tuning and joint training in both
vision and NLP (Section 6.2), and (2) as indicated by our theory, MeRLin succeeds because it learns
features that are more transferable than fine-tuning and joint training (Section 6.3).
6.1	Setup: Tasks, Models, Baselines, and Our Algorithms
The evaluation metric for all tasks is the top-1 accuracy. We run all tasks for 3 times and report their
means and standard deviations. Further experimental details are deferred to Section A.
Datasets and models. We consider the following four settings. The first three are object recognition
problems (with different label sets). The fourth problem is the prominent NLP benchmark where the
source is a language modeling task and the targets are classification problems.
SVHN or Fashion-MNIST → USPS. We use either SVHN (Netzer et al., 2011) (73K street view
house numbers) or Fashion-MNIST Xiao et al. (2017) (50K clothes) as the source dataset. The target
dataset is a random subset of 600 examples of USPS (Hull, 1994), a hand-written digit dataset. We
down-sampled USPS to simulate the setting where the target dataset is much smaller than the source.
We use LeNet (LeCun et al., 1998), a three-layer ReLU network in this experiment.
ImageNet → CUB-200, Stanford Cars, or Caltech-256. We use ImageNet (Russakovsky et al., 2015)
as the source dataset. The target dataset is Caltech-256, CUB-200 (Wah et al., 2011), or Stanford Cars
(Krause et al., 2013). These datasets have 25468, 5994, 8144 labeled examples respectively, much
smaller than ImageNet with 1.2M labeled examples. Caltech is a general image classification dataset
of 256 classes. Stanford Cars and CUB are fine-grained classification datasets with 196 categories of
cars and 200 categories of birds, respectively. We use ResNet-18 (He et al., 2016).
Food-101 → CUB-200. Food (Bossard et al., 2014) is a fine-grained classification dataset of 101
classes of food. Here we validate MeRLin when the gap between the source and target is large. We
also use ResNet-18.
ImageNet → Stanford Dogs, MIT-indoors, or Aircraft. We further test the proposed method with
ResNet-50 (He et al., 2016) as baselines. We still use ImageNet (Russakovsky et al., 2015) as the
source dataset. The target dataset is Stanford Dogs (Khosla et al., 2011), MIT-indoors (Quattoni &
Torralba, 2009), or Aircraft (Maji et al., 2013) consisting of 12000, 5360, 8000 labeled examples,
respectively.
Language modeling → GLUE. Pre-training on language modeling tasks and fine-tuning on labeled
dataset such as GLUE (Wang et al., 2019) is dominant following the success of BERT (Devlin et al.,
7
Under review as a conference paper at ICLR 2021
Table 3: ACCuraCy (%) of computer vision tasks With ResNet-50 backbone and ImageNet.			
Target	MIT-indoors	Stanford Cars	Aircraft
Fine-tuning	83.21 ± 0.21	84.77 ± 0.32	81.13 ± 0.21
L2-sp	83.98 ± 0.29	86.42 ± 0.20	80.98 ± 0.29
DELTA	83.66 ± 0.20	85.01 ± 0.22	80.44 ± 0.20
BSS	83.73 ± 0.18	86.69 ± 0.19	81.48 ± 0.18
MeRLin	84.50 ± 0.26	87.05 ± 0.33	82.57 ± 0.27
2019). We fine-tune BERT with MeRLin and evaluate it on the three tasks of GLUE with the smallest
number of labeled examples which standard fine-tuning likely overfits.
Baselines. (1) target-only, (2) fine-tuning, and (3) joint-training have been defined in Section 2.
Following standard practice, the initial learning rate of fine-tuning is 0.1× the initial learning rate
of pre-training to avoid overfitting. For joint training, the overall objective can be formulated as:
(1 - α)LDb + αLDb . We tune α to achieve optimal performance. The fourth baseline is (4) L2-sp
(Li et al., 2018), which fine-tunes the models with a regularization penalizing the parameter distance
to the pre-trained feature extractor. (5) DELTA (Li et al., 2019) constrains the layer outputs selected
with attention. (6) BSS (Chen et al., 2019) regularized the singular value of feature matrix. We also
tuned the strength the L2-sp, Delta and BSS regularizer.
MeRLin. We perform standard training with cross entropy loss on the source domain while meta-
learning the representation in the target domain as described in Section 4.
MeRLin-ft. In BERT experiments, training on the source masked language modeling task is
prohibitively time-consuming, so we opt to a light-weight variant instead: start from pre-trained
BERT, and only meta-learn the representation in the target domain.
6.2	Results
Results of digits classification and object recognition are provided in Table 1. MeRLin consistently
outperforms all baselines. Note that the discrepancy between Fashion-MNIST and USPS is very large,
where fine-tuning and joint training perform even worse than target-only. Nonetheless, MeRLin is
still capable of harnessing the knowledge from the source domain. On Food-101→CUB-200,
MeRLin improves over fine-tuning by 6.58%, indicating that MeRLin helps learn transferable
features even when the gap between the source and target tasks is huge. In Table 2, we validate
our method on GLUE tasks. MeRLin-ft outperforms standard BERT fine-tuning and L2-sp. Since
MeRLin-ft only changes the training objective of fine-tuning, it can be easily applied to NLP models.
6.3	Analysis
We empirically analyze the representations and verify that
MeRLin indeed learns more transferable features than fine-
tuning and joint training.
Intra-class to inter-class variance ratio. Suppose the rep-
resentation of the j-th example of the i-th class is φi,j .
μi = N Pj= 1 φi,j， and μ = C Pi=1 μi Then the
intra-class to inter-class variance ratio can be calculated
as σintra = C "P' kφi,j .2k . LoW values of this ratio cor-
σ inner	i-^i μi ”
respond to representations Where classes are Well-separated.
Results on ImageNet → CUB-200 and Stanford Cars task
are shoWn in Figure 3. MeRLin reaches much smaller ratio
than baselines.
Figure 3: Comparison of intra-Class
to inter-class variance ratio. This
quantity is loWest for MeRLin, indicat-
ing that it separates classes best.
7 Additional Related Work
Transfer learning is prevalent in deep learning applications. In computer vision, ImageNet pre-training
is a common practice for nearly all target tasks. Early Works (Oquab et al., 2014; Donahue et al.,
2014) directly apply ImageNet features to target tasks. Fine-tuning from ImageNet pre-trained models
has become dominant (Long et al., 2015; He et al., 2017; Kolesnikov et al., 2019). On the other
8
Under review as a conference paper at ICLR 2021
hand, transfer learning is also crucial to the success of NLP. Pre-training transformers on large-scale
language tasks boosts performance on downstream tasks (Devlin et al., 2019; Yang et al., 2019).
A recent line of literature casts doubt on the consistency of transfer learning’s success (Raghu et al.,
2019; He et al., 2018; Kornblith et al., 2019). Huh et al. (2016) observed that some set of examples in
ImageNet are more transferable than the others. Geirhos et al. (2019) found out that the texture of
ImageNet is not transferable to some target tasks. Training on the source dataset may also need early
stopping to find optimal transferability Liu et al. (2019a); Neyshabur et al. (2020).
Meta-learning, originated from the learning to learn idea (Hochreiter et al., 2001; Vilalta & Drissi,
2002; Maclaurin et al., 2015; Zoph & Le, 2016), learns from multiple training tasks models that can
be swiftly adapted to new tasks (Finn et al., 2017; Rajeswaran et al., 2019; Nichol et al., 2018b).
Raghu et al. (2020); Goldblum et al. (2020) empirically studied the mechanism of MAML’s success.
Computationally, our method uses bi-level optimization techniques similar to meta-learning work.
E.g., Bertinetto et al. (2019) speeds up the implementation of MAML Finn et al. (2017) with closed-
form solution of the inner loop, which is a technique that we also use. However, the key difference
between our paper from the meta-learning approach is that we only learn from a single target task
and evaluate on it. Therefore, conceptually, our algorithm is closer to the learning-to-learn approach
for hyperparameter optimization (Maclaurin et al., 2015; Zoph & Le, 2016), where there is a single
distribution that generates the training and validation dataset.
8 Conclusion
We study the limitations of fine-tuning and joint training. To overcome their drawbacks, we propose
meta representation learning to learn transferable features. Both theoretical and empirical evidence
verify our findings. Results on vision and NLP tasks validate our method on real-world datasets.
Our work raises many intriguing questions for further study. Could we apply meta-learning to
heterogeneous target tasks? What’s more, future work can pay attention to disentangling transferable
features from non-transferable features explicitly for better transfer learning.
References
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In Proceedings
ofthe 36th International Conference on Machine Learning, volume 97, pp. 322-332, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exact computation with an infinitely wide neural net. In Advances in Neural Information Processing
Systems 32, pp. 8141-8150. 2019b.
Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural
Information Processing Systems 32, pp. 690-701. 2019.
Luca Bertinetto, Joao F. Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differentiable
closed-form solvers. In International Conference on Learning Representations, 2019.
Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 - mining discriminative compo-
nents with random forests. In European Conference on Computer Vision, 2014.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and deep
neural networks. In Advances in Neural Information Processing Systems 32, pp. 10836-10846.
2019.
Xinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long, and Jianmin Wang. Catastrophic forgetting
meets negative transfer: Batch spectral shrinkage for safe transfer learning. In Advances in Neural
Information Processing Systems, volume 32, pp. 1908-1918. Curran Associates, Inc., 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, 2019.
9
Under review as a conference paper at ICLR 2021
Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor
Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In
Proceedings of the 31st International Conference on Machine Learning, volume 32, pp. 647-
655, 2014.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning,
volume 70 of Proceedings of Machine Learning Research, pp. 1126-1135. PMLR, 2017.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves
accuracy and robustness. In International Conference on Learning Representations, 2019.
Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate
object detection and semantic segmentation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), June 2014.
Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Goldstein.
Unraveling meta-learning: Understanding feature representations for few-shot tasks. volume 119
of Proceedings of Machine Learning Research, 2020.
G. Griffin, A. Holub, and P. Perona. Caltech-256 object category dataset. Technical report, California
Institute of Technology, 2007.
Kaiming. He, Xiangyu. Zhang, Shaoqing. Ren, and Jian. Sun. Deep residual learning for image
recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the
IEEE International Conference on Computer Vision (ICCV), Oct 2017.
Kaiming He, Ross B. Girshick, and Piotr Dollar. Rethinking imagenet pre-training. arxiv,
abs/1811.08883, 2018.
Sepp Hochreiter, A. Steven Younger, and Peter R. Conwell. Learning to learn using gradient descent.
In International Conference on Artificial Neural Networks, 2001.
Jeremy Howard and Sebastian Ruder. Universal language model fine-tuning for text classification.
2018.
Mi-Young Huh, Pulkit Agrawal, and Alexei A. Efros. What makes imagenet good for transfer
learning? arxiv, abs/1608.08614, 2016.
Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on pattern
analysis and machine intelligence, 16(5):550-554, 1994.
Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In 2018 IEEE/CVF Conference on Computer Vision and Pattern
Recognition, 2017.
Aditya Khosla, Nityananda Jayadevaprakash, BangpengYao, and Li Fei-Fei. Novel dataset for
fine-grained image categorization: Stanford dogs. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2011.
Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-,
and high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,
and Neil Houlsby. Big transfer (bit): General visual representation learning, 2019.
Simon Kornblith, Jonathon Shlens, and Quoc V. Le. Do better imagenet models transfer better? In
The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2661-2671, 2019.
10
Under review as a conference paper at ICLR 2021
Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained
categorization. In 4th International IEEE Workshop on 3D Representation and Recognition
(3dRR-13), Sydney, Australia, 2013.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324,1998.
Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, and Jun Huan. DELTA: deep
learning transfer using feature map with attention for convolutional networks. International
Conference on Learning Representations, 2019.
Xuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning
with convolutional networks. In Proceedings of the 35th International Conference on Machine
Learning, volume 80, pp. 2825-2834, 2018.
Zichuan Lin, Garrett Thomas, Guangwen Yang, and Tengyu Ma. Model-based adversarial meta-
reinforcement learning, 2020.
Hong Liu, Mingsheng Long, Jianmin Wang, and Michael I. Jordan. Towards understanding the
transferability of deep representations, 2019a.
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for
natural language understanding. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pp. 4487-4496, 2019b.
J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In
2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3431-3440,
2015.
M. Long, Y. Cao, J. Wang, and M. I. Jordan. Learning transferable features with deep adaptation
networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pp.
97-105, 2015.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization
through reversible learning. In International Conference on Machine Learning, pp. 2113-2122,
2015.
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained
visual classification of aircraft, 2013.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS workshop on deep learning
and unsupervised feature learning, volume 2011, pp. 5, 2011.
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang. What is being transferred in transfer
learning?, 2020.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv
preprint arXiv:1803.02999, 2018a.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms, 2018b.
Maxime Oquab, Leon Bottou, Ivan Laptev, and Josef Sivic. Learning and transferring mid-level
image representations using convolutional neural networks. In The IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2014.
A. Quattoni and A. Torralba. Recognizing indoor scenes. In IEEE Conference on Computer Vision
and Pattern Recognition, 2009.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature
reuse? towards understanding the effectiveness of maml. In International Conference on Learning
Representations, 2020.
11
Under review as a conference paper at ICLR 2021
Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio. Transfusion: Understanding
transfer learning for medical imaging. In Advances in Neural Information Processing Systems 32,
pp. 3342-3352. 2019.
Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit
gradients. In Advances in Neural Information Processing Systems 32, pp. 113-124. 2019.
Mark Rudelson and Roman Vershynin. Non-asymptotic theory of random matrices: extreme singular
values. In Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4
Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II-IV: Invited Lectures, pp. 1576-1602.
World Scientific, 2010.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):
211-252, 2015.
Alex Tamkin, Trisha Singh, Davide Giovanardi, and Noah Goodman. Investigating transferability in
pretrained language models, 2020.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.
Laurens J.P. van der Maaten and Geoffrey E. Hinton. Visualizing high-dimensional data using t-sne.
Journal of Machine Learning Research, 9(2):2579-2605, 2008.
Ricardo Vilalta and Youssef Drissi. A perspective view and survey of meta-learning. Artificial
Intelligence Review, 18(2):77-95, 2002.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.
GLUE: A multi-task benchmark and analysis platform for natural language understanding. In
International Conference on Learning Representations, 2019.
Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M. Summers.
Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classifi-
cation and localization of common thorax diseases. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2017.
Sen Wu, Hongyang R. Zhang, and Christopher Re. Understanding and improving information transfer
in multi-task learning. In International Conference on Learning Representations, 2020.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms. arxiv, abs/1708.07747, 2017.
Ze Yang, Tiange Luo, Dong Wang, Zhiqiang Hu, and Liwei Wang. Learning to navigate for fine-
grained classification. In Proceedings of the European Conference on Computer Vision (ECCV),
2018.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural
Information Processing Systems 32, pp. 5753-5763. 2019.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in Neural Information Processing Systems 27, pp. 3320-3328.
2014.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprint
arXiv:1611.01578, 2016.
12
Under review as a conference paper at ICLR 2021
A Additional Details of Experiments
A.1 The Semi-synthetic Experiment
The original CIFAR images are of resolution 32 × 32. For the transferable dataset A, we reserve the
upper 16 × 32 and fill the lower half with [0.485, 0.456, 0.406] for the three channels (the mean of
CIFAR-10 images). For the non-transferable dataset B, the lower part 16 × 32 pixels are generated
with i.i.d. gaussian distribution with the upper half filled with [0.485, 0.456, 0.406] similarly. To
make the non-transferable part related to the labels, we set the mean of the gaussian distribution to
0.1 × c, where c is the class index of the image. The variance of the gaussian noise is set to 0.2. We
always clamp the images to [0, 1] to make the generated images valid. For the source dataset, we
use 49500 CIFAR-10 images, while for the target, we use the other 500 to avoid memorizing target
examples.
We use ResNet-32 implementation provided in github.com/akamaster/pytorch_
resnet_cifar10. We set the initial learning rate to 0.1, and decay the learning rate by 0.1
after every 50 epochs. We use t-SNE (van der Maaten & Hinton, 2008) visualizations provided in
sklearn. The perplexity is set to 80.
A.2 Implementation on Real Datasets
We implement all models on PyTorch with 2080Ti GPUs. All models are optimized by SGD with 0.9
momentum. For object recognition tasks, the initial learning rate is set to 0.01, with 5 × 10-4 weight
decay. The batch-size is set to 64. We run each model for 100 epochs. ImageNet pre-trained models
can be found in torchvision. We use a batch size of 128 on the source dataset and 512 on the
target dataset. The initial learning rate is set to 0.1 for training from scratch and 0.01 for ImageNet
initialization. We decay the learning rate by 0.1 every 50 epochs until 150 epochs. The weight decay
is set to 5 × 10-4. For GLUE tasks, we follow the standard practice of Devlin et al. (2019). The
BERT model is provided in github.com/Meelfy/pytorch_pretrained_BERT. For each
model, we set the head (classifier) to the top one linear layer. We use a batch size of 32. The learning
rate is set to 5 × 10-5 with 0.1 warmup proportion. During fine-tuning, the initial learning rate is
10 times smaller than training from scratch following standard practice. The hyper-parameter ρ is
set to 2, and λ is found with cross validation. We randomly split the training set into 80% and 20%.
We train on the 80% part and use the rest 20% as the validation set. We also provide the results of
varying ρ and λ in Section A.7. In object recognition however, we also provides a pre-trained version:
starting from ImageNet pre-trained solution.
A.3 Implementing the Speed-up Version
Practical implementation: speeding up with MSE loss. Training the head gθ in the inner loop of
meta learning can be time-consuming. Even using implicit function theorem or implicit gradients
as proposed in (Bai et al., 2019; Rajeswaran et al., 2019; Lin et al., 2020), we have to approximate
the inverse of Hessian. To solve the optimization issues, we propose to analytically calculate the
prediction of the linear head θt and directly back-prop to the feature extractor hφ . Thus, we only
need to compute the gradient once in a single step. Concretely, suppose we use MSE-loss. Denote by
H ∈ Rnt ×m the feature matrix of the n^ target samples in the target meta-training set Dbtr. Then θt
in equation 1 can be analytically computed as θbt = (HH> + λI)-1y, where λ is a hyper-parameter
for regularization. The objective of the outer loop can be directly computed as
minimize J(φ, θs) = Lb (θs, φ) + ρ
φ∈Φ,θs∈Θ	Ds
HH>+λI)
-1y
(7)
We implement the kernel speed-up version on classification tasks following Arora et al. (2019b).
We treat the classification problems as multi-variate ridge regression. Suppose we have label
C ∈ {1,2,…,10}. Then the target encoding for regression is -0.1 X 1 + e「For example, if the
label is 3, then the encoding will be (-0.1, -0.1,0.9,…，一0.1). Then the parameters of the target
head in the inner loop can be computed as θbt = (HH> + λI)-1Y. We then compute the MSE loss
13
Under review as a conference paper at ICLR 2021
on the target validation set: kHvalθbt - Yval k22 in the outer loop. We summarize the details of the
vanilla version and the speed-up version in Algorithm 1 and Algorithm 2.
Algorithm 1 Meta Representation Learning (MeRLin).
1:
2:
3:
4:
5:
6:
7:
8:
9:
Input: the source dataset Ds and the evolving target dataset Dt .
Output: learned representations φ.
for iter = 0 to MaxIter do
Initialize the target head θt .
Randomly sample target train set Dbttr and target validation set Dbtval from Dbt .
for t = 0 to n do
Train the target head on Dbttr:
θt - θt - η V θt LDbtr (θt, φ).
end for	t
In the outer loop, update the representation φ and the source head θs :
(φ, θs) 一 (φ, θs) - ηv(φ,θs) [LDbs (θs,φ) + PLDFI (θt,φ)].
10: end for
Algorithm 2 Meta Representation Learning (MeRLin): speed-up implementation.
-	，T	一 ，W …	1 .	.一 ，3
Input: the source dataset Ds and the evolving target dataset Dt .
1:
2:
3:
4:
5:
Output: learned representations φ.
for t = 0 to MaxIter do
Randomly sample target train set Dbttr and target validation set Dbtval from Dbt .
Analytically calculate the solution of target head θt(φ) in the inner loop
θbt(φ) = (HH> + λI)-1Y.
6:	In the outer loop, update the representation φ and the source head θs :
(φ, θs) J (φ, θS)- nV (φ,θs) l-LDbs (θs, φ) + Pn IlHvalbbt - YvallI2
7:	end for	t
A.4 Datasets
CUB-200 (Wah et al., 2011) is a fine-grained dataset of 200 bird species. The training dataset consists
of 5994 images and the test set consists of 5794 images. http://www.vision.caltech.edu/
visipedia/CUB- 200-2011.html
Stanford Cars (Krause et al., 2013) dataset contains 16,185 images of 196 classes of cars. The
data is split into 8,144 training images and 8,041 testing images. http://ai.stanford.edu/
~jkrause/cars/car_dataset.html
Food-101 (Bossard et al., 2014) is a fine-grained classification dataset of 101 kinds of food, with
750 training images and 250 test images for each kind. http://www.vision.ee.ethz.ch/
datasets_extra/food-101/
Caltech-256 (Griffin et al., 2007) is an object recognition dataset of 256 categories. In our
experiments, the training set consists of 25468 images, and the test set consists of 5139 im-
ages.http://www.vision.caltech.edu/Image_Datasets/Caltech256/
MIT-indoors (Quattoni & Torralba, 2009) is a scene recognition dataset of 67 classes. The training
set contains 80 examples for each class, and the test set contains 20 examples for each class.
http://web.mit.edu/torralba/www/indoor.html
Stanford-Cars (Khosla et al., 2011) is a fine-grained classification dataset of 120 kinds of dogs.
http://vision.stanford.edu/aditya86/ImageNetDogs/main.html
Aircraft (Maji et al., 2013) is an object recognition dataset of 100 aircraft variants. Each class has
80 images for training and 20 images for testing. http://www.robots.ox.ac.uk/~vgg/
data/fgvc-aircraft/
14
Under review as a conference paper at ICLR 2021
MNIST (LeCun et al., 1998) is a dataset of hand-written digits. It has a training set of 60,000
examples, and a test set of 10,000 examples. We randomly sample 600 samples as the training set for
the target domain. http://yann.lecun.com/exdb/mnist/
SVHN (Netzer et al., 2011) is a real-world image dataset of street view house numbers. It has 73257
digits for training, 26032 digits for testing. In Figure 3 of the main text, we randomly sample 600
digits for training, and all the 26032 digits are used in testing. http://ufldl.stanford.edu/
housenumbers/
USPS (Hull, 1994) is a database for handwritten text recognition. It has 7291 train and 2007 test
images. We randomly sample 600 samples as the training set for the target domain. https:
//www.kaggle.com/bistaumanga/usps-dataset
A.5 Further Ablation Study.
We extend the last column of Table 2 in Table 4. We further compare with two variants of MeRLin as
ablation study:
MeRLin (pre-trained). We first pre-train the model on the source dataset and then optimize the
MeRLin objective starting from the pre-trained solution.
MeRLin-target-only. MeRLin-target-only only meta-learns representations on the target domain
starting from random initialization. We test whether the meta-learning objective itself has regulariza-
tion effect.
Table 4: Accuracy on Food → CUB.
Algorithm ∣ Target-only ∣ Fine-tuning ∣ Joint Training ∣ MeRLin-target-only ∣ MeRLin (pre-trained) ∣ MeRLin
Accuracy ∣32.10 ± 0.64152.30 ± 0.511 42.08 ± 0.59 I 40.17 ± 0.44 I 55.26 ± 0.43	∣58.68 ± 0.43
MeRLin (pre-trained) performs worse than MeRLin, but it still improves over fine-tuning and joint
training. Note-that MeRLin (pre-trained) only need to train on ImageNet for 2 epochs, much shorter
than joint training. MeRLin-target-only improves target-only by 8%, indicating that meta-learning
helps avoid overfitting even without the source dataset.
A.6 Feature-label correlation
7
6
5
4
3
2
uo _4Eaj」」0。- θqElφ-l n4->E3LL,
(a) Comparison of feature-label correlation.
(b) Sensitivity to hyper-parameters.
Figure 4: (a) Analysis of Feature Quality. Comparison of feature-label correlation. A lower
quantity is better, and MeRLin has the lowest value. (b) Sensitivity of the proposed method to
hyper-parameters. We test the accuracy on Food-101→CUB-200 with varying ρ and λ and provide
the visualization.
Suppose the feature matrix is H, and the label vector is y, then the correlation between feature
and label can be defined as y> H>H -1 y. As is shown by Arora et al. (2019a); Cao & Gu
(2019), this term is closely related to the generalization error of neural networks, with a smaller
quantity indicating better generalization. We calculate y> H>H -1 y on ImageNet → CUB-200
and Stanford Cars. As shown in Figure 4(a), the features learned by MeRLin are more closely related
15
Under review as a conference paper at ICLR 2021
to labels than fine-tuning and joint training, indicating MeRLin is indeed learning more transferable
features compared with baselines.
A.7 Sensitivity of the Proposed Method to Hyper-parameters.
We test the model on Food-101→CUB-200 with varying hyper-parameters ρ and λ. Results in Figure
4(b) indicate that the model is not sensitive to varying ρ and λ. Intuitively, larger ρ indicates more
emphasis on the target meta-task. When ρ approaches 0, the performance of MRL is approaching
fine-tuning. λ exerts regularization to the classifier in the inner loop training. It is also note worthy
that λ can avoid the problem that HH> is occasionally invertible. Without λ the model can fail to
converge sometimes.
A.8 B ootstrapped Confidence Interval of Objection Recognition Results
We run 10 seeds of each experiment on L2-sp and MeRLin in Table 1 and calculate 95% bootstrapped
confidence interval of each results in Figure 5.
5 0 5 0 5
9 9 8 8 7
AUEJnUUV
Figure 5: Comparison of L2-sp and MeRLin. The results are averaged over 10 runs, with error
bars representing 95% confidence interval drawn by 1,000 bootstraps.
16
Under review as a conference paper at ICLR 2021
B Missing Details in Section 5
B.1 Proof of Theorem 1
Lemma 1. Suppose 0 ≤ E ≤ 2. For each solution θs,θt, φ satisfying Eχ,y〜Dt ['(fθt,φ(x), y)] ≤ G
the joint training objective Ljλoint(θs, θt, φ) is lower bounded:
Ljλoint(θs, θt, φ) = (1 - α)LDs (θs, φ) + αLDbt(θt, φ) + λ kθsk2 + kθtk2 + kφk2F
≥ min
μ
2
lμl	+ 3(I - α)(μ
Proof of Lemma 1. Define d × d matrix
m
A = X θsiφiφi>.	(8)
i=1
Define X[i：k] and X[k+i：d] be the first k and last d - k dimensions of x. Ak,k, Ak,k and A就k be k X k,
k × (d - k) and (d - k) × (d - k) matrices that correspond to the upper left, upper right and lower
right part of A. For a random vector x where the first k dimensions are uniformly independently from
{±1}, the last d - k dimensions are uniformly indepdently from {0, ±1}, define random variables
Ai = x>.k]Ak,kX[i：k], A2 = X[k+i：d]Afe,feX[k+i：d]. (Notethat X is defined on a different distribution
than Ds .)
We have bound
(1 — α)Eχ,y"s ['(fθs,φ(x),y)]+ λ (|包||2 + ； ||0||F)	⑼
=(1 - α)Ex,y〜Ds ](X>：k]Ak,kx[1:k] +2x>：k]Ak,后x[k+1：d] + x>k+i：d]Ak,kx[k+1：d] - y
+ λ (∣∣θsk2 + 1 kΦkF)	(10)
≥(I- a)Ex,y〜Ds ](x>：k]Ak,kx[i：k] + X>k+i：d]Ak,kx[k + 1：d] - y)	+ λ (kθs∣∣2 + 2 kφ∣∣F
(11)
= (I-	α)	(3Eh(AI-I)2i + 3E [(AI-I)A2] + E [a2]) + λ (kθsk2	+ 2 kφkF)	(12)
= (I-	α)	(3Eh(AI + A2 -I)2i + 3E [a2]) + λ (kθsk2 + 2 kφkF)	(13)
≥3(I- a) (E [A1 + A2] -I)2 + 2^/3 (IE [A1 + A2] I)2/3	(14)
The first inequality is because
Ex,y~Ds [(xLk]Ak,k x[1:k]) (XLk]Ak,工 x>k + Ld]) ] = 0,	(15)
Ex,y〜Ds [(x[k+Ld]A及"x>k + Ld]) (x>：k]AkWx>k+Ld])] =0,	(16)
Ex,y〜DsKX>：k]AkWx[k+Ld]) y] = 0.	(17)
17
Under review as a conference paper at ICLR 2021
The second inequality is because
X ((θsi )2 + 2 kφik2) ≥ 24/3 XX (∣θsiHMk2)2/3
i=1	i=1
(18)
(19)
(20)
where the first inequality is AM-GM inequality, the second inequality is by concavity of (∙)2/3. The
third inequality is because for diagonal matrix D that has 1 at (i, i) if A[i,i] ≥ 0, -1 at (i, i) if
A[i,i] < 0, we have
m	mm
X |A[i,i]| = tr(AD) = X θsiφ>Dφi ≤ X lθs∕∙ llφik2.	QI)
On the other hand, for the target, we define d × d matrix
m
B=Xθtiφiφi>.	(22)
i=1
Define χ[i] and x[2：d] be the first 1 and last d 一 1 dimensions of x. B1,1, B1,1 and Bq be 1 X 1,
1 × (d - 1) and (d - 1) × (d - 1) matrices that correspond to the upper left, upper right and lower
right part of B . For a random vector x where the first dimension is uniformly independently from
{±1}, the last d 一 1 dimensions are uniformly indepdently from {0, ±1}, define random variables
Bi = χ>]B1,1X[i], B2 = x>：d]Bj,jX[2：d]. (Note that X is defined on a different distribution than Dt)
Using similar argument as above, we have
Eχ,y〜Dt ['(fθt,φ(x), y)] ≥ 3(E [Bi + B2] - 1)2.	(23)
However, we know that Eχ,y〜Dt ['(fθ5φ(χ), y)] ≤ 3 so there has to be
E [Bi + B2] ≥ 1 — J33,	(24)
therefore we have
λ (kθtk2+2 kφkF) ≥ 2473 (1 - r∕3f!	.	(25)
Summing up Equation 9 and Equation 25 finishes the proof.
□
Lemma 2. Assume φt is a vector such that hφt, Xti = /；川 for all Xt ∈ Dt, then there exists some
solution (θs , θt, φ) such that
Ljoint(θs, θt, φ) ≤ min 22/3/3 lμl2/3 + 3(1 一 α)(μ -I)2) + 22/3 BII2 .	(26)
ProofofLemma 2. Assume μ* ∈ arg minμ (^l^∣μ∣2/3 + j(1 一 α)(μ 一 1)2), then obviously μ* ∈
[0,1]. Let φι =	(√2μ*)i/3ei,	φ2	=	φt,	φi	= 0 for i >	2,	θs	=	(〃*/2)i/3ei	and
lφtl2
kφ k2/3
θt = " 21% e2. Now we prove that this model satisfies the Equation 26.
18
Under review as a conference paper at ICLR 2021
First of all, we notice that for any xit ∈ Dt, there is
xit > X θti φiφi> xit
=xit>θt2φ2φ2>xti
=hφt,xti2
=yit.
Therefore we have
LDbt (θt, φ) = 0
(27)
(28)
(29)
(30)
(31)
On the other hand, we have
(1 - α)LDs (θs, φ) + λ kθsk2 + kφ1k2	(32)
=3(1 - α)(μ* - 1)2 + 22/3lμ*|2/3	(33)
= mμn (22/3lμl"3 + 3(1 - α)(μ - 1)2) .	(34)
Plugging Equation 31 and Equation 32 into the formula of Ljoint(θs,θt, Φ) finishes the proof. □
Lemma 3. Let X ∈ Rd×n be a random matrix where each entry is uniformly random and inde-
PendentIy sample from {0, ±1}, n < d. Let PX eι be the projection of eι to the column space of
X. Then, there exists absolute constants c0 > 0 and C > 0, such that with probability at least
1 - 4 exp(-C d), there is
n
IIPXeιk2 ≤ codd	(35)
Proof of Lemma 3. Let smin(X) and smax(X) be the minimal and maximal singular values of X
respectively. Then we have
IPXe1I2 = X(X>X)-1X>e12	(36)
≤kχkop∣∣(X >X )-1∣∣op∣∣x >eι∣∣2	(37)
≤ Smax(X )(Smin (X ))-2√n.	(38)
By Theorem 3.3 in Rudelson & Vershynin (2010), there exists constants c1, c2 > 0, such that
P(Smin(X) ≤ cι(√d — √n)) ≤ 2exp(-c2d).	(39)
By Proposition 2.4 in Rudelson & Vershynin (2010), there exists constants c3, c4 > 0, such that
P(Smax(X) ≥ c3(√d + √n)) ≤ 2exp(-c4d).	(40)
Let C =	min{c2, c4}, then with probability at least 1 - 4exp(Cd), there is	
	Smax(X )(Smin (X ))	√n	(41)
	/3 √d + √n L ≤ c2 (√dɪ√ny 5	(42)
	< (2+√2)c3 rn ^(√2 - 1)2c2 V d,	(43)
which completes the proof.
□
19
Under review as a conference paper at ICLR 2021
Proof of Theorem 1. We prove the joint training part of Theorem 1 following this intuition: (1) the
total loss of each solution with target loss Eχ,y〜Dt ['(fθt,φ(x),y)] ≤ E is lower bounded as indicated
by Lemma 1, and (2) there exists a solution with loss smaller than the aforementioned lower bound
as indicated by Lemma 2.
By Lemma 1, for any θs,θt,φ satisfying Eχ,y〜Dt ['(fθt,φ(x),y) ≤ e, the joint training loss
Ljλoint(θs, θt, φ) is lower bounded,
Ljoint(θs, θt,φ) ≥ min (24/3iμi2/3+3(I - α)(μ—I)2)+24/3 (1 - rɪ!	.	(44)
Let PXe1 be the projection of vector e1 to the subspace spanned by the target data. According to
Lemma 2, there exists some solution (θs, θt, φ) such that
Ljoint(θs, θt, φ) ≤ μ 22/3/3 lμl2/3 + 3(1 - α)(μ -I)2) + 22/3 lφJL .	(45)
2	2--ʌ 2/3
Let €o > 0 bea constant such that 21/3 (1 - ʌ/ 3f0)	> 21/3 - 21/3. According to Lemma 3, there
exists absolute constants c ∈ (0, 1), C > 0, such that so long as nt ≤ cd, there is with probability at
least 1 - 4exp(-Cd),
(46)
Now we prove the upper bound in Equation 45 is smaller than the lower bound in Equation 44. This
is because
min (孩lμl2/3 + 2(1 - α)(μ - 1)2) + 2273 llφtH2z	(47)
= min (孩lμl2/3 + 2(1 - α)(μ - 1)2) - 3λ(⅛7-^) + 装 l∣φtll2/ + 3λ(击 - 24/3)
(48)
≤ min (24/3 lμl2/3+3(1 - α)(μ - 1)2)+22⅛llφtll2/ +3λ(2273 - 24/3)	(49)
2/3
≤min (24/3|〃|2/3+3(1 - α)(μ - 1)2) + 24/3 (1 -4ɪj ,	(50)
where the first inequality uses that fact that ∣μ∣ < 1 for the optimal μ, the second inequality is by
Equation 46. This completes the proof for joint training.
ml	.1	1 .	1	. r∙ .	1	1 ∙	. T	Λ	♦♦♦	/ /C ?	∖ ɪ'
Then, we prove the result about fine-tuning. According to Lemma 4, any minimizer (θs , φpre) of
LλD (θs, φ) either satisfies φpre = 0, or only one φi is non-zero but looks like (up to scaling) ej for
j ∈ [k]. When φpre = 0, there is
Ex,y〜Dt ['(f^t,φpre (x),y)i = 3 > 27 .	(51)
When only one φi is non-zero but looks like ej for j ∈ [k], since all the first k dimensions are
equivalent for the source task, with probability 1 - 1, this dimension is j = 1. The target funciton
fine-tuned on this φpre looks like f^ φ^ (x) = Yxj for some Y ∈ R, so there is
Ex,y〜Dt h'(f^t,φpre(x),y)i = Eχ,y〜Dt [(YX2 - X2)2]	(52)
=Y Ex,y〜Dt [xt] - 2γEx,y〜Dt [χt x1] + Ex,y〜Dt [X1]	(53)
2Y2 - 8Y +2 ≥ 竺.
3 Y 9 Y 3 — 27
(54)
Combining these two possibilities finishes the proof for fine-tuning. Finnaly, setting E = min{∈o, 17}
finishes the proof of Theorem 1.	□
20
Under review as a conference paper at ICLR 2021
B.2 Proof of Theorem 2
Lemma 4. Define the source loss as
LλDs(θs, φ) = LDs(θs, φ) + λ kθsk2 + kφk2F .
Then, for any λ > 0, any minimizer of LλD (θs, φ) is one of the following cases:
(i)	θs = 0 and φ = 0.
(ii)	for one i ∈ [m], θsi > 0, φi = ±(√2θsi) ∙ ej for some j ≤ k; for all other i ∈ [m],
lθsil = kφik = 0 ∙
Furthermore, when 0 < λ < 0.1, all the minimizers look like (ii).
Proof of lemma 4. Define d × d matrix
m
A = X θsiφiφi>.
i=1
(55)
Define X[i：k] and X[k+i：d] be the first k and last d - k dimensions of x. Ak,k, Ak,k and A就k be k X k,
k × (d - k) and (d - k) × (d - k) matrices that correspond to the upper left, upper right and lower
right part of A. For a random vector x where the first k dimensions are uniformly independently from
{±1}, the last d - k dimensions are uniformly indepdently from {0, ±1}, define random variables
Ai = x>.k]Ak,kx[i：k], A2 = X[k+i：d]Afe,fex[k+i：d]. (Notethat X is defined on a different distribution
than Ds .)
The loss part of LλD (θs, φ) can be lower bounded by:
Eχ,y"s['(fθs,φ(x),y)]
=Ex,y 〜Ds ](x[Lk]Ak,k x[1:k] + 2x[1：k] Ak,fe x[k+1:d] + x[k+1:d]Ak,k x[k+1:d] - y)
≥Ex,y〜Ds ](x>：k]Ak,kx[i：k] + x[k + i：d]Ak,kx[k+1：d] - y)]
=3E [(Ai - I)2i + 3E [(A1 -I)A2] + E [a2]
=3E [(AI + A2-I)2] + 3E [a2].
The inequality is because
Ex,y 〜Ds [(X[1：k]Ak,k x[1:k]) (X[1：k]Ak,k x[k + 1:d]) ] = 0,
Ex,y〜Ds [(x[k+Ld]Ak,kx[k + 1:d]) (X[1：k]Ak,kx[k+1:d])] = 0,
Ex,y~Ds [(X[1：k]Ak,kx[k+1:d]) y] = 0.
The inequality is equality if and only if Ak,kk = 0.
The regularizer part of LλD (θs, φ) can be lower bounded by:
2/3
|A[i,i]|
(56)
(57)
(58)
(59)
(60)
(61)
(62)
(63)
(64)
(65)
(66)
21
Under review as a conference paper at ICLR 2021
where the first inequality is AM-GM inequality, the second inequality is by concavity of (∙)2/3. The
third inequality is because for diagonal matrix D that has 1 at (i, i) if A[i,i] ≥ 0, -1 at (i, i) if
A[i,i] < 0, we have
m	mm
X |A[i,i]| = tr(AD) = Xθsiφ>Dφi ≤ X lθs∕∙ ∣∣φik2.	(67)
All the inequalities are equality if and only if (θsi)2 = 1 ∣∣φi∣∣2 > 0 for at most one i ∈ [m], and for
all other i ∈ [m] there is ∣θs∕ = ∣∣φi∣ = 0.
Combining the two parts gives a lower bound for LλD (θs , φ):
LDs (θs , φ) ≥ 22/3 (X |A[i,i]|!	+3E h(A1 + A2 - 1)2i + 3E [A2]	(68)
3λ	2	2/3	2
≥22/3 (Itr(Ak,k + 3Ak,k)|J + 3 (E [A1 + A2] - I) ,	(69)
where both inequalitites are equality if and only if A工# = 0 (therefore A2 = 0) and Var [Ai] = 0
(therefore Ak,k is diagonal by Lemma 5).
Notice that E[Ai + A2] = tr(Ak,k + ∣AM工),the above lower bound is further minimized when
E[A1] = μ* where μ* is the minimizer of function L(μ) = 2^ (∣μ∣)2/3 + ∣ (μ 一 1)2.
To see when this lower bound is achieved, we combine all the conditions for the inequalities to be
equality. When μ* = 0, this lower bound is achieved if and only if θs = 0 and φ = 0. When μ* > 0,
*
this lower bound is only achieved when the solution look like this: for one i ∈ [m], θsi = (μ2)1/3,
φi = ±(√2θsi) ∙ ej for some j ≤ k; for all other i ∈ [m],= ∣∣φi∣ = 0.
Obviously, there is either μ* = 0 or μ* > 0. Also, when λ < 0.1, the minimizer μ* of L(μ) is
strictly larger than 0 (since L(1) < L(0)). So this completes the proof.	□
Lemma 5. Let M ∈ Rk×k be a symmetric matrix, x ∈ Rk is a random vector where each dimension
is indepedently uniformly from {±1}. Then, Var[x>Mx] = 0 if and only if M is a diagonal matrix.
Proof of Lemma 5. In one direction, when M is diagonal matrix, obviously Var[x>Mx] = 0. In
the other direction, when Var[x>Mx] = 0, there has to be x>Mx be the same for all x ∈ {±1}k.
For any i 6= j, let x(1) = 1 一 2ei 一 2ej, x(2) = 1, x(∣) = 1 一 2ei, x(4) = 1 一 2ej. Then the (i, j)
element of M is 8 (x(1)>Mx⑴ + x(2)>Mx(2) 一 x(∣)>Mx(∣) 一 x(4)>Mx(4)), which is 0. So M
has to be a diagonal matrix.	□
Proof of Theorem 2. Define the source loss as in Lemma 4, then we have
Lλmeta(θs, φ) = LλDs(θs, φ) + E hLDbval(θbt(φ), φ)i .	(70)
By Lemma 4, the source loss LλD (θs, φ) is minimized by a set of solutions that look like this: for
one i ∈ [m], θsi > 0, φi = ±(√2θsi) ∙ ej for some j ≤ k; for all other i ∈ [m], ∣θs∕ = ∣∣φi∣ = 0.
When j = 1, the only feature in φ is ei. When n ≥ 18log ∣, according to Chernoff bound, with
probability at least 1 一 ∣ there is strictly less than half of the data satisfy xi = 0. Therefore, any
Dbttr contains data with x1 6= 0, and the only target head that fits Dbttr has to recover the ground truth.
LDbval (θbt(φ), φ)
Hence there is E
0.
When j 6= 1, the only feature is ej . This feature can be used to fit the target data if and only if either
xi2[j] = xi2[i] for all target data xi, or xi[i] = 0 for all xi . Since there are at most k 一 1 possible j,
by union bound we know the probability of any of these happens for any j = 1 is at most k(∣)nt.
22
Under review as a conference paper at ICLR 2021
Hence, when n ≥ 3log 2kk, the probability of any ej fits the target data is smaller than 2. Therefore,
with probabiltiy 1 - 2, E [力力对(θt(φ), φ)] > 0 for any j = 1.
So with probability at least 1 - ξ, the only minimizer of Lλmeta(θs, φ) is the subset of minimizers of
LλD (θs, φ) with feature e1, and with this φ and any random Dttr, the only θt(Dttr, φ) that fits the target
recovers the ground truth, i.e., Eχ,y~Dt pθt(Dtr,φ),φ(x,y)] = 0.
□
23