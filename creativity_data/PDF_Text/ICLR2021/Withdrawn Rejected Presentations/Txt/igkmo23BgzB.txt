Under review as a conference paper at ICLR 2021
End-to-end	Quantized	Training via Log-
Barrier Extensions
Anonymous authors
Paper under double-blind review
Ab stract
Quantization of neural network parameters and activations has emerged as a suc-
cessful approach to reducing model size and inference time on hardware that sup-
ports native low-precision arithmetic. Fully quantized training would facilitate
further computational speed-ups as well as enable model training on embedded
devices, a feature that would alleviate privacy concerns resulting from the transfer
of sensitive data and models that is necessitated by off-device training. Existing
approaches to quantization-aware training (QAT) perform “fake” quantization in
the forward pass in order to learn model parameters that will perform well when
quantized, but rely on higher precision variables to avoid overflow in large matrix
multiplications, which is unsuitable for training on fully low-precision (e.g. 8-bit)
hardware. To enable fully end-to-end quantized training, we propose Log Barrier
Tail-bounded Quantization (LogBTQ). LogBTQ introduces a loss term, inspired
by the log-barrier for constrained optimization, that enforces soft constraints on
the range of values that model parameters can take on. By constraining and spar-
sifying model parameters, activations and inputs, our approach eliminates over-
flow in practice, allowing for fully quantized 8-bit training of deep neural network
models. We show that models trained using our approach achieve results compet-
itive with state-of-the-art full-precision networks on the MNIST, CIFAR-10 and
ImageNet classification benchmarks.
1	Introduction
As state-of-the-art deep learning models for vision, language understanding and speech grow in-
creasingly large and computationally burdensome (He et al., 2017; Devlin et al., 2018; Karita et al.,
2019), there is increasing antithetical demand, motivated by latency, security and privacy concerns,
to perform training and inference in these models on smaller devices at the edge rather than in server
farms in the cloud. Model quantization has emerged as a promising approach to enable deployment
of deep learning models on edge devices that reduce energy, latency and storage requirements by
performing floating-point computation in low precision (less than 32 bits).
There are two primary strategies for quantization: Post-training approaches quantize the parameters
of a model trained in full precision post-hoc, and tend to suffer a heavy penalty on accuracy since
their inference graph differs substantially from training (Jacob et al., 2018). Quantization-aware
training (QAT) (Bhuwalka et al., 2020) combats this discrepancy by simulating quantization during
training, so that model parameters are learned that will work well when inference is performed in
low precision. In this work, we focus on the latter setting, suitable for fully quantized training on
low-precision (e.g. 8-bit) devices.
Though QAT results in quantized models that perform largely on par with their non-quantized coun-
terparts, current state-of-the-art QAT methods (Wu et al., 2018; Wang et al., 2018; Bhuwalka et al.,
2020) are not suitable for training on fully low-precision hardware because they employ fake quan-
tization, meaning each operation is executed using 32- or 16-bit floating point arithmetic, and its
output is quantized to lower precision, e.g. int8. This results in two key incompatibilities with
fully low-precision training, and consequently deployment on real low-precision hardware. First,
existing QAT approaches assume perfect sums in inner product operations, which means that the
accumulators used to compute matrix multiplies (the acc row in Table 1) must be higher precision
than the values being multiplied (other bit-precision rows in Table 1). This is to avoid losing res-
1
Under review as a conference paper at ICLR 2021
Figure 1: Left: Visualization of the log barrier constraint applied to network parameters quantized in
range [-2, 2]. See §3.3 for an approximated tail bound on possible overflow. Right: μ-law encoding
vs. FP8(1-5-2) and FP8(1-4-3) for all possible values on interval [-2, 2]. μ-law maintains higher
precision at concentrated small values.
olution in low-precision additions, also known as swamping (Wang et al., 2018)1. Second, QAT
commonly leverages dynamic quantization ranges per-layer, meaning the mapping between high-
and low-precision values varies by layer, carefully tuned as a function of the network architecture,
optimization dynamics and data during training. While this practice results in higher quantized in-
ference accuracy, it is also a challenge to low-precision training, since it is unclear how to tune those
ranges when training on new data in the absence of high-precision arithmetic. These incompati-
bilities present a substantial hurdle to quantized training in practice. For example, an automotive
electronics manufacturer may want to deploy a machine learning model on its 8-bit door lock or
power window controller to adaptively fit the users’ habits. In this scenario, existing approaches for
quantized training would fail (Sakr et al., 2019).
In response, we propose a new approach for fully quantized training of neural network models, in-
spired by the barrier method from convex optimization (Boyd & Vandenberghe, 2004). Log Barrier
Tail-bounded Quantization (LogBTQ) utilizes a log barrier extension loss (Kervadec et al., 2019)
to constrain the output of the network, encouraging all model parameters and activations to stay
within the same predefined range. The log barrier function itself is a smooth approximation of the
indicator function, which is ideal for selecting the weights that are within the range of quantization
(see Figure 1, left). By fixing a single quantization range throughout the network at the beginning
of training, our approach both obviates the need for dynamic ranges, and the limits of the range
are set so as to alleviate overflow2 in matrix multiply accumulations. We combine the log barrier
extension loss with an L1 regularization term (Hoffer et al., 2018) to further reduce the total mag-
nitude of parameters and activations in the model. To allow for gradients, which tend form a peaky
distribution near extremely small values (Zhou et al., 2016; Jain et al., 2020), to be quantized using
the same range as the rest of the network, We also adopt the nonlinear μ-law algorithm from audio
applications (Deng & Doroslovacki, 2006) to construct a new MU8 codebook that better deals with
“swamping” issues compared to the standard IEEE Float Standard. Experiments show that our ap-
proach achieves competitive results compared to state-of-art full-precision models on the MNIST,
CIFAR-10 and ImageNet classification benchmarks, despite our models being trained end-to-end
using only 8 bits of precision.
1Swamping: Accumulation of floating-point numbers, where the small magnitude value is ignored (or trun-
cated) when it is added to the large magnitude sum.
2Overflowing: for the fixed-point accumulation where the accumulated value wraps around to the small
value when it exceeds the largest value representable by the given accumulation precision.
2
Under review as a conference paper at ICLR 2021
Quantized Training Scheme	W	Trai x	ning Precision dW dx acc			Quant. Range	Train/T FP32	est Prec. Low
DoReFa-Net (Zhou et al., 2016)	1	2	32	6	32	Dynamic	55.9	46.1
WAGE (Wu et al., 2018)	2	8	8	8	32	Dynamic	—	51.6
DFP (Das et al., 2018)	16	16	16	16	32	Dynamic	75.70	75.77
MPT (Micikevicius et al., 2018)	16	16	16	16	32	Dynamic	75.92	76.04
Wang et al. (2018)	8	8	8	8	16	Dynamic	72.14	71.72
HFP8 (Sun et al., 2019)	8	8	8	8	8/16	Dynamic	76.44	76.22
LogBTQ (ours)	8	8	8	8	8	Fixed	73.59	71.11
Table 1: Comparison of reduced-precision training for top-1 accuracy (%) using ResNet-50 (Im-
ageNet). For works that did not evaluate on ResNet-50, we include AlexNet results (italicized).
Dynamic indicates that quantization ranges vary by layer and must be learned or tuned; Fixed indi-
cates a single quantization range is fixed globally throughout the network.
2	Background and related work
2.1	Post-training Quantization
There was been a recent surge of interest in quantization research. In 2020 alone, there were a
number of important developments in post-training quantization. Rusci et al. (2020); Jain et al.
(2020); Esser et al. (2020); Uhlich et al. (2020) proposed learning-based approaches for determin-
ing the quantization ranges of activation and weights at low precision. Stock et al. (2020) advo-
cates preserving the quality of the reconstruction of the network outputs rather than its weights.
They all show excellent performance compared to full-precision models after quantization. Sakr &
Shanbhag (2019) presented a detailed analysis of reduced precision training for a feedforward net-
work that accounts for both the forward and backward passes, demonstrating that precision can be
greatly reduced throughout the network computations while largely preserving training quality. Our
work share the same intuition of preferring small predetermined dynamic range (PDR) and small
clipping rate3. However, Sakr & Shanbhag (2019)’s approach requires the network first be trained
to convergence at full 32 bit precision, which is a significant limitation. In this paper, we focus on
training rather than inference on low-precision hardware, therefore, we do not assume access to a
full-precision high-performing model as a starting point.
2.2	Quantization-aware Training
Pioneering works in this domain (Zhou et al., 2016; Courbariaux et al., 2015) looked at quantizing
model weights, activations, gradients to lower precision to accelerate neural network training. The
terminology quantization aware training (QAT) was first introduced by Jacob et al. (2018). QAT
incoporates quantization error as noise during training and as part of the overall loss, which the
optimization algorithm tries to minimize. Hence, the model learns parameters that are more robust
to quantization, but QAT is not meant to be performed entirely in low precision, it aims to learn
parameters that will work well for low-precision inference. More recently, several works further
pursued the goal of enabling fully low-precision training (Wu et al., 2018; Wang et al., 2018; Das
et al., 2018; Sun et al., 2019). As shown in Table 1, most existing work employs fake quantization,
resorting to higher precision values to compensate for the swamping issue, especially during gradient
accumulation. Mixed-precision quantization (Das et al., 2018; Wang et al., 2018; Zhang et al.,
2020a), which quantizes a neural network using multiple bit precisions across layers, still relies on
higher-precision gradients to preserve model accuracy. This means it is difficult, if not impossible,
to implement these approaches on low-bit (e.g. 8-bit) hardware.
Most similar to our work, Sun et al. (2019) claim it is possible to do every step in low precision,
but the quantization range for the layers in their work is very carefully chosen empirically, which
presents great difficulty if we were to train models from scratch on low-precision hardware. Their
method also requires a copy of the quantization error (residual) in FP16(1-6-9) (hence 8/16 in Ta-
3see Appendix B of Sakr & Shanbhag (2019) explaining PDR; refer to their Criterion 2 about clipping rate.
3
Under review as a conference paper at ICLR 2021
Figure 2: Left: Diagram of the forward pass using LogBTQ quantization. Right: Gradient propaga-
tion.
ble 1). In addition to the 9-bit mantissa, the exponent bit in their floating point format would need
to be manually modified to store the residual due to its small value.
In this paper, we propose a new quantization sCheme: log-barrier tail-bounded quantization (Log-
BTQ) that Can perform fully end-to-end low preCision training, suitable for deployment on low-
preCision hardware. Our major Contributions are the following:
1.
2.
3.
We apply a log barrier extension loss to soft-threshold the values of network weights and
activations to constrain all the values to be small. Our quantization scheme also enables
global fixed-range quantization which together significantly alleviates the overflow issue
caused by large numbers and dynamic range.
We add an L1 loss term to encourage sparsity and further reduce overflow.
We propose μ-law quantization (MU8) instead of INT8, FP8(1-4-3) or FP8(1-5-2) to Con-
struct a more accurate codebook that better compensates for the peaky concentration of
network parameters around small values.
3	Log Barrier Tail-bounded Quantization (LogBTQ)
The overall diagram of our quantization sCheme is shown in Figure 2 (left). Figure 2 (right) shows
the baCkward pass, where we quantize everything at eaCh layer and all operations inCluding input x,
weights w , aCtivations a, errors e, and gradients g (inCluding the gradient aCCumulation step). We
denote all these values as the set Z = {x, w, a, e, g}. In this work, different from previous works
(Sakr & Shanbhag, 2019; Zhang et al., 2020b) that used adaptive quantization range, we adopt a
globally fixed quantization range for every element z ∈ Z, and set z ∈ [-2, 2]. We do not need to
adjust the range and preCision during training as in other quantization work that relies on layer-wise
dynamiC ranges. This would greatly reduCe the overhead for implementation on hardware.
3.1	Constrained Formulation
Let D = {I1, ...IN} denote the labeled set of N training images, and f denote the neural network
model, θ here denotes all the parameters of the neural network inCluding weights w . For task, suCh
as image ClassifiCation, we are usually solving suCh an optimization problem: minL(fθ (I)) where
θ
L is the loss funCtion of our neural network training objeCtive. In this work, we use the typiCal
Cross-entropy loss, and sinCe we are interested in Constraining the quantization threshold, we are
effeCtively performing Constrained optimization in suCh a form:
minimize L (fθ (I))
θ
SubjeCt to ∣θn| ≤ u, n = 1,..., N.
(1)
4
Under review as a conference paper at ICLR 2021
With u our desired barrier (perturbation). In practice, we set u = 0.1 to ensure we can represent
as much information as possible within our quantization range (Figure 1, left). This setting is also
explained further in Section 3.3.
3.2	log-barrier Extension Function
Theoretically, problem (1) should be best solved by the log barrier method which is an interior
point method that perfectly handles inequality constraints. (Tibshirani, 2019; Boyd & Vandenberghe,
2004): In phase I, we would perform Lagrangian-dual optimization to find the feasible points:
N
maximize minimize L (x, λ) = L (fθ(I)) + ^X λn(∣θn∣ 一 U))
λ	θ	n=1	(2)
subject to	λ	0, n = 1, . . . , N.
where λ ∈ R1+×N is the Lagrangian multiplier (dual variable). After we find a feasible set of network
parameters, we can use the barrier method to solve equation (1) as an unconstrained problem:
N
minimize L(fθ(I)) + X Ψt(∣θn∣ - U)	⑶
n=1
solving problem (3) is Phase II, where ψt is the standard log-barrier function: ψt = _ 1 log(-z). As
t approaches infinity, the approximation becomes closer to the indicator function. Also, for any value
oft, if any of the constraints is violated, the value of the barrier approaches infinity. However, a huge
limitation in applicability to practical problems such as ours is that the domain of Eq. (3) must be
the set of feasible points. The canonical barrier method above is also prohibitively computationally
expensive given there are millions of parameters in the network, and we need to alternate the training
between primal and dual and do projected gradient ascent for the dual variable.
We are not particularly concerned with the weak duality gap to lower-bound the optimal solution
in this work, instead, we are interested in the property of the barrier method to handle inequality
constraints. Therefore, inspired by Kervadec et al. (2019), we formulate quantization as an uncon-
strained loss to approximate the constrained optimization problem:
N
minimize L(fθ(I)) + X ψt(∣θn∣ - U)
n=1
1	7 ∙ .1 1	F	♦	.	♦	< ∙ < ∙	. ∙	1 .	1∙ /'/'	. ∙ 11
where ψt is the log-barrier extension, which is convex, continuous, and twice-differentiable:
ψt(z) = J-11OK-Z Z ≤ -12
Itz - 1 log(*)+ 1 otherwise
(4)
(5)
in our case, the input z to the log-barrier extension is the same z we defined in the beginning of
Section 3, and t is the scaling parameter. It basically shares the same property with the standard log-
barrier function, when t approaches +∞, our log-barrier extension would approach a hard indicator:
H(z) = 0 if z ≤ 0 and +∞ otherwise. But its domain is not restricted to the feasible points only.
This removes the demanding requirement for explicit Lagrangian optimization.
We are doing approximated Lagrangian optimization with implicit dual variables. Our strict positive
gradient of ψt will get higher when z approaches violation and effectively push back into the feasible
value set. Because our penalty does not serve as a strict barrier of the feasible set, there is possibility
of overflow. However, our goal is to achieve the practical goal of fully-quantized training on low-bit
hardware, as long as the majority of values stay within a high confidence interval, the approach will
work in practice, as we show in experimental results (§4). Recall the scenario in Figure 1(left).
3.3	Tail Bound of Distribution
In this section, we demonstrate that the probability of overflowing the quantization range can be
controlled for a standard ResNet model. The ResNet model has L layers, each layer contains a CNN
operation and ReLU as an activation. Consider at layer l, for each output element O ∈ R in the
5
Under review as a conference paper at ICLR 2021
layer, a CNN operation with the kernel size k and channel size c can be viewed as a rectified dot
product between the input feature I ∈ Rck2 with w ∈ Rck2.
ck2
O = ReLU(XwiIi)
i=1
(6)
Suppose Wi 〜N(0, σ(i)) and。a is dependent on the layer l and the parameter U We choose.
The second moment of any element z(l+1) ∈ R from layer l + 1 can be connected with the second
moment of element z(l) from layer l as folloWs:
(7)
In the He initialization, we could set σw = «系 to cancel the first two terms. In this work, we
Want to control σ(2l) in order to reduce the chance of overfloW. We choose σ(2l) to depend on u Which
was defined in optimization problem (1): σ0) = JcU2. Then, we can simplify Eq. (7):
E[z(l+1)] = uE[z(l)]	(8)
Suppose the input feature to the first layer has second moments E(1) = E[z12], then the second
moments at layer l can be estimated as: E(l) = E[z(2l)] = ul-1E(1). Next, for each element z(l) at
layer l, its tail distribution outside the quantization region can be bounded by
P(∣Z(1)I > 2)= P(Z(1) > 2) ≤ P(∣Z(1) - E[Z(1)]| > |2- E[Z(1)]∣) ≤ ⑫^谓尸
(9)
where the second inequality is established by Chebyshev’s inequality, here, zl is guaranteed to be
nonnegative since it’s the output of the ReLU layer. We can further estimate the worst upper bound
for the right hand side by considering the following optimization problem:
sup
E[z(l)],Var(z(l) )
subject to
Var(Z(i))
(2- EM)])2
(E[z(l)])2 +Var(z(l)) = E(l)
As z(l) is the rectified value, E[z(l)] > 0. One upper bound can be established as follows:
Var(z(l))	E(l)
SUD	<
E[z(l)],Vpr(z(l)) (2 - E[Z(1)])2 < (2- PE^2
(10)
because E[z(i)] < EE{a and Var(z。))< E(). Notice that the last term can be approximated
to E(l) when E(l) is small enough, and we can establish our probability bound of overflowing the
quantization range:
P(M)1> 2)< (^⅛T ‘牛	(II)
Finally, consider the average overflow probability of z ∈ R from any layer:
P(IzI > 2) = 1 XP(Z(I) > 2) ' 4l XUlTE(I) ' 4L(1(-U)	(12)
l=1	l=1
Formula (12) is derived because in deep neural networks, L is usually a large number allowing us
to effectively ignore UL . Therefore, the overflow probability is determined by the input’s second
moments E, the layer size L and the barrier parameter U. In practice, we can adjust both U and L
to control the overflowing tails. In this work, E(1) is set around 1.0 because the input features are
normalized, andL is around 50 in the ResNet-50. By choosing U = 0.1, the number of overflowing
parameters can be controlled under 2.3%.
6
Under review as a conference paper at ICLR 2021
3.4	Sparsity
In order to achieve the goal of practical implementation on 8-bit hardware, we aggressively fix the
range of quantization to z ∈ [-2, 2]. This leaves us with the task of mapping millions of parameters
to this range. Therefore, we desire a sparse solution, and naturally, as is pointed out by Hoffer et al.
(2018), we use L1 penalty |n | to encourage sparsity. Our unconstrained loss then becomes:
NN
minimize Lf (I)) + X ψt (∣θn∣ - u) + Y X 怎|
n=1	n=1
(13)
where γ > 0 and is tuning variable, and ψt is the log-barrier extension as proposed.
3.5	μ-LAW QUANTIZATION CODEBOOK (MU8)
Even after sparsifying the weight, we still are left will many parameters which are non-linear dis-
tributed. Luckily, they should all be small enough by now thanks to our log-barrier constraint.
Uniform quantization (e.g. INT8) would cause huge information loss in this case. Inspired by
μ-law's (Deng & Doroslovacki, 2006) application in audio encoding, We construct a non-linear
codebook accordingly, which could be implemented on 8-bit hardware directly. As is shown in
Figure 1(right), we computed all possible values of FP8(1-4-3) or FP8(1-5-2)4 *used by Sun et al.
(2019), and we can see our MU8 encoding are better at handling very small values e.g. FP8 (1-4-
3)’s smallest possible number is 1.9 × 10-3, FP8 (1-5-2) is 1.5 × 10-5, whereas our MU8 could
handle 5 × 10-6 with the cost of getting sparser in larger numbers, which we have almost eliminated
using log-barrier. Let's denote our the quantization function as Q*8(X) which take X as input and
output quantized value χ0. First, following the range of μ-law encoding, we set the input to Equation
15 x = z/2, since z ∈ [-2, 2] in this paper.
一、	,、ln(1 + μ∣x∣)
F(X) = sgn(x)-r---i	-1 ≤ x ≤ 1	(14)
ln(1 + μ)
where μ is a tuning variable, the larger μ is, the more non-linear it becomes, and Sgn(X) is the
sign function. Then, we get the y ∈ [-1, 1] as the output of F (X), then we perform Stochastic
Rounding step introduced by Gupta et al. (2015). e.g. y × (27 - 1) ∈ [15, 16], ify = 15.5/(27 - 1),
then P(y = 16/(27 - 1)) = P(y = 15/(27 - 1)) = 0.5; if y = 15.1/(27 - 1), then P((y =
15/(27 - 1)) = 0.9, P(y = 16/(27 - 1)) = 0.1, as shown in the following equation.
y J Stochastic Rounding(y, LL, UL) = (LL w.p. 1	(y - LLL	(15)
UL w.p. (y - LL)
where UL indicates upper limit and LL indicates lower limit. At last, we get the quantized value X0
via the inverse function of encoding function (15):
X0 J F-1(y) = sgn(y)1((1 + μ)lyl -1)	(16)
μ
3.6 Other Useful Techniques
Straight-through Estimator (STE): We adopt the same STE as Zhou et al. (2016) which is critical
to the convergence of our models. The gradient of the quantization steps in the quantization model
is based on the straight through estimator.
Chunked Updates: We also use the same gradient accumulation strategy as Sakr et al. (2019);
Wang et al. (2018). During updates, since we have quantized the gradient and weight parame-
ters using Qμ8(X), the gradient will be lower bounded by the smallest possible value of the MU8
encoding(10-6), which is small enough to preserve model accuracy within the 8-bit constraint.
However, directly adding this small gradient to a large number (weight) in our MU8 quantization
would cause this small gradient to be rounded-off and thus lose information due to the growing
4FP8(1-4-3)'s largest value is [-240, 240], and FP8(1-5-2)'s is [-57344, 57344], they are better at handling
long-tails than MU8, but we are not interested in large numbers in the long-tail.
7
Under review as a conference paper at ICLR 2021
Figure 3: Left: LogBTQ ResNet-50 weight distribution with and without log-barrier loss throughout
the training process. Right: Example weight distribution in a conv layer of LogBTQ ResNet-50.
intervals of MU8 encoding at larger values. Performing chunked gradient updates, updating the
weights only after k steps of gradient descent (in this work, we set k = 20), helps to accumulate the
gradients to be large enough to be rounded up, aiding convergence.
4 Experimental Results
Table. 2 shows the performance of models trained with LogBTQ from scratch on the MNIST,
CIFAR-10, and ImageNet datasets. Our performance on CIFAR-10 is even higher than the FP32
baseline, and our results using ResNet-50 trained from scratch on ImageNet are competitive against
both of the works in comparison. This is particularly impressive given that LogBTQ uses strictly
8-bits to store gradients whereas previous work resorted to FP16 or hybrid 8/16. Our accuracy loss
of ResNet-50 on ImageNet is only 2.48% which for many use cases is worth the benefit of keeping
everything in only 8 bits of precision. For practical purposes, 2-5% accuracy loss should be tolerable
if calculations can be kept strictly in 8 bits.
Training Scheme	MNIST ReSNet-18		CIFAR-10 ReSNet-18		ImageNet			
					ReSNet-50		MobileNet	
	FP32	8-bit	I FP32	8-bit I	FP32	8-bit	I FP32	8-bit
Wang et al. (2018)	—	—	92.77	92.21	72.14	71.72	—	—
HFP8 (Sun et al., 2019)	—	—	—	—	76.44	76.22	71.81	71.61
LogBTQ FP8(1-4-3)	99.9	98.1	94.08	92.31	73.59	54.32	71.68	50.19
LogBTQ FP8(1-5-2)	99.9	98.3	94.08	93.21	73.59	58.46	71.68	51.33
LogBTQ MU8(ours)	99.9	99.6	94.08	94.50	73.59	71.11	71.68	68.41
Table 2: Results of LogBTQ training compared with the two most relevant low-precision training
schemes. Note that the previous work listed here is not directly comparable to ours; as is shown
in Table 1. Though these works perform the majority of computation in 8 bits, they still have the
advantage of FP16 or Hybrid FP8/16 when accumulating gradients, and dynamic quantization range.
First 2 rows are numbers reported by Wang et al. (2018) and Sun et al. (2019).
5	Discussion
Hou et al. (2019) also provides a bound for the quantized gradient of QAT models, giving the intu-
ition that quantized gradients would slow down convergence, which we also observe in our training.
For each arithmetic operation, We perform our Qμ8 (X) quantization right after each operation to
ensure the number still falls into 8 bits. For out of range numbers, we clamp. We verify our assump-
8
Under review as a conference paper at ICLR 2021
tions in our experiments: Figure 3 (left) shows a layer’s weight distribution during training, and we
can see that they are nicely bounded by our log-barrier to within [-1.5, 1.5]. Figure 3 (right) shows
the weight distribution is concentrating to smaller values during training.
Since we only have 10-6 precision to handle the gradients, we are inevitably losing some accuracy
compared with higher precision training schemes. FP16 can handle 10-8 precision and FP32 can
handle 10-32 precision. As we can see in Table 2, LogBTQ can perform almost perfectly on easier
tasks such as MNIST and CIFAR-10, but degrades when we are training on the more challenging
ImageNet dataset. Our Mu8 encoding performs better than 1-5-2 and 1-4-3 with our LogBTQ
quantization scheme, showing μ-law encoding can handle small fixed quantization range better than
1-5-2 or 1-4-3. As is pointed out by Sheng et al. (2018), MobileNet architecture has some layers
that are unfriendly for quantization, e.g. An outlier in one channel could cause a huge quantization
loss for the entire model due to an enlarged data range. We employed the same techniques proposed
by Sheng et al. (2018), though MobileNet benchmark is still not as good as ResNet-50 due to the
agressive reduction of parameters.
Overall, it still depends on the use case of the deep learning models to consider the trade-off between
accuracy and precision to choose which quantization scheme to adopt. As far as we are aware,
LogBTQ is the first full 8-bit quantization scheme with competitive performance on a large-scale
dataset such as ImageNet. This opens up the prospect of training fully-quantized models on low-
precision hardware.
6	Conclusion
Motivated by the limitations of fake quantization, we propose Log Barrier Tail-bounded Quantiza-
tion (LogBTQ) which introduces a log-barrier extension loss term that enforces soft constraints on
the range of values that model parameters can take on at every operation. Our approach eliminates
overflow in practice, SParsifying the weights and using μ-law non-uniform quantization, allowing
for fully quantized 8-bit training of deep neural network models. By constraining the neural net-
work parameters driven by theoretical motivations, this work enables the possibility for the first
time fully-quantized training on low-precision hardware.
References
Pulkit Bhuwalka, Alan Chiao, Suharsh Sivakumar, Raziel Alvarez,	Feng Liu,
Lawrence Chan, Skirmantas Kligys, Yunlu Li, Khanh LeViet, Billy Lambert,
Mark Daoust, Tim Davis, Sarah Sirajuddin, and Francois Chollet. Quantiza-
tion aware training with tensorflow model optimization toolkit - performance with
accuracy, April 2020. URL https://blog.tensorflow.org/2020/04/
quantization-aware-training-with-tensorflow-model-optimization-toolkit.
html.
Stephen P Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. In Advances in neural information processing
SyStemS,pp. 3123-3131, 2015.
Dipankar Das, Naveen Mellempudi, Dheevatsa Mudigere, Dhiraj Kalamkar, Sasikanth Avancha,
Kunal Banerjee, Srinivas Sridharan, Karthik Vaidyanathan, Bharat Kaul, Evangelos Georganas,
et al. Mixed precision training of convolutional neural networks using integer operations. In
International Conference on Learning RepreSentationS, 2018.
Hongyang Deng and Milos Doroslovacki. Proportionate adaptive algorithms for network echo can-
cellation. IEEE TranSactionS on Signal ProceSSing, 54(5):1794-1803, 2006.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-
dra S Modha. Learned step size quantization. In International Conference on Learning Repre-
SentationS, 2020.
9
Under review as a conference paper at ICLR 2021
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan. Deep learning with
limited numerical precision. In International Conference on Machine Learning, pp. 1737-1746,
2015.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings ofthe
IEEE international conference on computer vision, pp. 2961-2969, 2017.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks. In Advances in Neural Information Processing Systems,
pp. 2160-2170, 2018.
Lu Hou, Ruiliang Zhang, and James T Kwok. Analysis of quantized models. In International
Conference on Learning Representations, 2019.
Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard,
Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for
efficient integer-arithmetic-only inference. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2704-2713, 2018.
Sambhav Jain, Albert Gural, Michael Wu, and Chris Dick. Trained quantization thresholds for
accurate and efficient fixed-point inference of deep neural networks. In I. Dhillon, D. Pa-
pailiopoulos, and V. Sze (eds.), Proceedings of Machine Learning and Systems, volume 2,
pp. 112-128. 2020. URL https://proceedings.mlsys.org/paper/2020/file/
e2c420d928d4bf8ce0ff2ec19b371514- Paper.pdf.
Shigeki Karita, Nanxin Chen, Tomoki Hayashi, Takaaki Hori, Hirofumi Inaguma, Ziyan Jiang,
Masao Someki, Nelson Enrique Yalta Soplin, Ryuichi Yamamoto, Xiaofei Wang, et al. A com-
parative study on transformer vs rnn in speech applications. In 2019 IEEE Automatic Speech
Recognition and Understanding Workshop (ASRU), pp. 449-456. IEEE, 2019.
Hoel Kervadec, Jose Dolz, Jing Yuan, Christian Desrosiers, Eric Granger, and Ismail Ben Ayed.
Constrained deep networks: Lagrangian optimization via log-barrier extensions, 2019.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision
training. In International Conference on Learning Representations, 2018.
Manuele Rusci, Alessandro Capotondi, and Luca Benini. Memory-driven mixed low
precision quantization for enabling deep network inference on microcontrollers. In
I. Dhillon, D. Papailiopoulos, and V. Sze (eds.), Proceedings of Machine Learning and
Systems, volume 2, pp. 326-335. 2020. URL "https://github.com/mrusci/
training- mixed- precision- quantized- networks".
Charbel Sakr and Naresh R Shanbhag. Per-tensor fixed-point quantization of the back-propagation
algorithm. In 7th International Conference on Learning Representations, ICLR 2019, 2019.
Charbel Sakr, Naigang Wang, Chia-Yu Chen, Jungwook Choi, Ankur Agrawal, Naresh Shanbhag,
and Kailash Gopalakrishnan. Accumulation bit-width scaling for ultra-low precision training of
deep networks. In International Conference on Learning Representations, 2019.
Tao Sheng, Chen Feng, Shaojie Zhuo, Xiaopeng Zhang, Liang Shen, and Mickey Aleksic. A
quantization-friendly separable convolution for mobilenets. In 2018 1st Workshop on Energy
Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2), pp.
14-18. IEEE, 2018.
Pierre Stock, Armand Joulin, Remi Gribonval, Benjamin Graham, and Herve Jegou. And the bit
goes down: Revisiting the quantization of neural networks. In Eighth International Conference
on Learning Representations, 2020.
Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalak-
shmi Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit
floating point (hfp8) training and inference for deep neural networks. In Advances in Neural
Information Processing Systems, pp. 4900-4909, 2019.
10
Under review as a conference paper at ICLR 2021
Ryan Tibshirani. Log barrier method, Sept 2019. URL https://www.stat.cmu.edu/
~ryantibs/Convexopt/scribes/barr-method-Scribed.pdf.
Stefan Uhlich, Lukas Mauch, Fabien Cardinaux, Kazuki Yoshiyama, Javier Alonso Garcia, Stephen
Tiedemann, Thomas Kemp, and Akira Nakamura. Mixed precision dnns: All you need is a good
parametrization. In International Conference on Learning Representations, 2020.
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash Gopalakrishnan. Train-
ing deep neural networks with 8-bit floating point numbers. In Advances in neural information
processing systems,pp. 7675-7684, 2018.
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. Training and inference with integers in deep
neural networks. In International Conference on Learning Representations, 2018.
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping accelerates
training: A theoretical justification for adaptivity. In International Conference on Learning Rep-
resentations, 2020a.
Xishan Zhang, Shaoli Liu, Rui Zhang, Chang Liu, Di Huang, Shiyi Zhou, Jiaming Guo, Qi Guo,
Zidong Du, Tian Zhi, and Yunji Chen. Fixed-point back-propagation training. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020b.
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net: Train-
ing low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint
arXiv:1606.06160, 2016.
11