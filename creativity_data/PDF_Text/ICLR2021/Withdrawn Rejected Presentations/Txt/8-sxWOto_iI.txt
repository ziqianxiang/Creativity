Under review as a conference paper at ICLR 2021
Introducing Sample Robustness
Anonymous authors
Paper under double-blind review
Ab stract
Choosing the right data and model for a pre-defined task is one of the critical
competencies in machine learning. Investigating what features of a dataset and
its underlying distribution a model decodes may enlighten the mysterious ”black
box” and guide us to a deeper and more profound understanding of the ongoing
processes. Furthermore, it will help to improve the quality of models which di-
rectly depend on data or learn from it through training. In this work, we introduce
the dataset-dependent concept of sample robustness, which is based on a point-
wise Lipschitz constant of the label map. For a particular sample, it measures
how small of a perturbation is required to cause a label-change relative to the
magnitude of the label map. We introduce theory to motivate the concept and to
analyse the effects of having similar robustness distributions for the training- and
test data. Afterwards, we conduct various experiments using different datasets and
(non-)deterministic models. In some cases, we can boost performance by choos-
ing specifically tailored training(sub)sets and hyperparameters depending on the
robustness distribution of the test(sub)sets.
1	Introduction:
In the age of automated machine learning, we shift our focus evermore towards regarding meta-
hyperparameters such as model-type or training- and validation budget as variables of a loss function
in the most abstract sense. For training sets, however, the mere number of samples often determines
how well suited it is perceived for a particular task. The motivation of this work is to introduce a
concept allowing for the use of datasets as variables of such a generalised loss function as well.
Imagine, for example, patients who share almost identical medical records, but react differently
to some prescribed treatment. They may pose a challenge to machine learning models similar to
what is known as natural adversarial examples in vision tasks, see Hendrycks et al. (2019). The
relationship between medical features and appropriate personal treatments may be sensitive towards
small input variations, i.e. not robust towards perturbations. In this work, we are to the best of
our knowledge the first to introduce and analyse a model-agnostic measure for the robustness of
data. We show that knowledge about the robustness distribution of a specific test(sub)set can allow
for choosing a more appropriate training(sub)set in terms of performance optimisation. Finally, we
discover that the optimal choice of hyperparameters may also depend on the robustness distributions
of both training- and test data.
Let us motivate the concept of sample robustness first on a high level. When collecting and process-
ing a dataset for a pre-defined task, we identify certain features and expressive samples such that a
model may be able to abstract and generalise from these finite points to the whole space of possible
inputs. Assume we have a certain rectangle-shaped data-distribution in a circle-shaped feature space
and a dataset labelled according to two distinct ground truth maps y* ∈ {×, *} and z* ∈ {◦, ∙}
(comp. Figure 1). Here, one can imagine classifying images of horses and cats (assuming ground
truth y*) and classifying images of animal-human pairs (assuming ground truth z*). Evidently, the
distance between differently labelled samples depends on the ground truth map labelling them.
For every sample in a dataset, the intrinsic information of closeness to a differently labelled sam-
ple can be considered a feature itself. For regression tasks and label distributions which are not
necessarily categorical one may also include the distance of the corresponding labels as additional
information. By taking the quotient of these two and maximising it over the dataset, i.e. calculating
a point-wise Lipschitz constant of the label map, one can measure how sensitive a sample is to label-
1
Under review as a conference paper at ICLR 2021
Figure 1: Images from the CIFAR-10 dataset labelled ”horse” and ”cat”, respectively.
changing perturbations. Precisely this is the non-formal basis of the concept we propose, namely
sample robustness. Our work aims to analyse the connection between both model performance and
aligning the robustness distributions of training- and test sets.
1.1	Outline
After citing and discussing related work concerned with decision boundaries, model robustness and
Lipschitz calculus in section 2, we introduce the mathematical framework and the measure of sample
robustness in section 3. We also motivate the concept theoretically and show some natural relations
to K-Nearest Neighbour models. Section 4 is completely devoted to the evaluation using different
datasets (CIFAR-10, Online News Popularity Dataset) and models (Convolutional Neural Networks,
K-Nearest Neighbour, Random Forest). Section 5 finally concludes the findings and discusses other
research approaches. Letters A - F refer to sections in the appendix.
2	Related Work:
Analysing the data-distribution before training yields a way to investigate (and boost) model per-
formance from an earlier stage as is done by unsupervised pre-training (Erhan et al., 2010)). Many
algorithms stemming from the unsupervised setting (Chigirev & Bialek, 2003; Cayton, 2005) are de-
voted to extract information about the so-called data-manifold (Fefferman et al., 2013; Bernstein &
Kuleshov, 2014). Decoding the latent features which determine the data-distribution (Bengio et al.,
2013) provides valuable insight and helps to understand the decision boundaries which a model
learns throughout the training phase. Furthermore, understanding the data-manifold may provide
a view into the ”black box” transforming inputs to outputs (Fawzi et al., 2016; Liu et al., 2017;
Biggio et al., 2013). In this work, we want to use the intrinsic information of distance between
samples in feature space and relate it to the distance of the corresponding labels to introduce a new
dataset-dependent feature.
The robustness of a sample can be regarded as its susceptibility to label-changing perturbations.
Here, one is immediately reminded of adversarial examples (Szegedy et al., 2013) in the context
of model robustness. The difference to our proposed concept, however, is that we only use the
pre-defined labels instead of model predictions as additional input. The term robustness itself is
one of the most prominent throughout the recent literature in many different contexts, from robust
attacks to robust models/defences (Evtimov et al., 2017; Beggel et al., 2019; Madry et al., 2017;
Weng et al., 2018; Tsuzuku et al., 2018). State-of-the-art machine learning models are susceptible
to noise, especially so when crafted purposefully (Fawzi et al., 2016). It leaves these powerful ma-
chines vulnerable to attacks either at training- (Zhu et al., 2019) or at test stage (Biggio et al., 2013),
independent of the architecture used (Papernot et al., 2016b). We follow the idea that models are ex-
tensions of the label map from the (metric) subspace defined by a dataset to the whole feature space.
Hence, they will inherit critical properties from the data. In this work we analyse the robustness
distribution of datasets and the thereon dependent performances of models, but plan on investigating
the connection to model-robustness in the future.
Lipschitz calculus yields a mathematically well-understood approach to describe and measure model
robustness as in Weng et al. (2018) or Tsuzuku et al. (2018). Framing machine learning theory in
terms of metric spaces (and also building robustness concepts thereon) has been done before in
Wang et al. (2016), however, not explicitly connecting it to Lipschitz calculus. In this work, we
2
Under review as a conference paper at ICLR 2021
build the concept of sample robustness based on a point-wise Lipschitz constant of the label map for
metric- and Banach spaces such that it applies to a wide range of different feature- and label spaces
including an ample variety of metrics.
Finally, investigating the dependence of model-hyperparameters on data has been done previously
(Nakkiran et al., 2020). The authors also showed that more data could sometimes decrease model
performance. In this work, we will see similar results regarding these two aspects with the differ-
ence that we could identify such data even before training using the proposed measure of sample
robustness.
3	Sample Robustness
Now we will introduce the primary concept of this work, namely sample robustness. It measures
how sensitive samples are to label-changing perturbations relative to the magnitude of the label map.
3.1	Definition: (Framework)
Let FS be a feature space, i.e. a metric space with metric d := dFS, let TS be a target- or label
space, i.e. a real Banach space with norm ∣∣ ∙ ∣∣ := ∣∣ ∙ ∣∣ts, let x, t ⊆ FS be finite (data)sets with
#x, #t ≥ 2 and let y : x ∪ t → TS be a map of labels with #y(x) = #y(t) ≥ 2.
3.2	Definition: (Reach)
Let X ∈ FS be a sample with label y(x), ∣y(x)∣ ≤ ∣∣y∣∞, and Γχ(x) := {x ∈ X | y(x) = y(X)}.
The reach of x is defined as the distance of x to Γx(x):
rχ(x) ：= dist(x, Γχ(x)) = min d(x,X)
x∈Γχ(x)
In other words: Γχ(x) is the set of samples X in the dataset X that are labelled differently from x.
One may notice that X ∈ Γχ(x) ⇔ X ∈ Γχ(X). The reach of X ∈ X is exactly the minimal distance
of the point representing the image of a ”cat” to a differently labelled (coloured) point in Figure. 1.
If X is "close" to X, one would expect their labels y(∕) and y(X) to be ”close" as well. Taking the
quotient then gives a measure of this error (comp. 3.3) and normalising the latter with respect to the
magnitude of y (using ∣y∣∞ := max ∣y(X)∣) yields the main concept of sample robustness in 3.4.
x∈x
3.3	Definition: (Point-Wise Lipschitz Constant)
ForX ∈ FS with label y(X), ∣y(X)∣ ≤ ∣y∣∞, andΓx(X) as in 3.2 one defines a point-wise Lipschitz
constant of y as:
Qx(X) := max
X∈Γχ(x)
∣y(X) — y(X)∣
d(X, X)
3.4	Definition and Proposition: (Sample Robustness)
Let X ∈ FS with label y(X), ∣y(X)∣ ≤ ∣y∣∞, and Γx(X) as in 3.2. The robustness of the sample
X in X with respect to d is defined as:
Rx(X) = Q⅛⅛C ∈(0,1)
It is independent of rescaling the label map y (comp. A.2). Coming back to the example in the
introduction, we can see now that the almost identical medical records of patients reacting differently
to the same treatment are considered as less robust samples in the above sense.
3
Under review as a conference paper at ICLR 2021
3.5	Theoretic Motivation and Background
Assume we have datasets x and t where both are labelled using the same label map y with
max ky(x)k = max ky(t)k . For any z ∈ x ∪ t it holds that:
x∈x	t∈t
Qx ∪ t(z) = max{Qx(z), Qt(z)} ⇔ Rx ∪ t(z) = min{Rx (z), Rt (z)}
In other words: the closer Rx(z) is to Rt(z), the closer both values are to the robustness of z in the
union x ∪ t. It follows:
Rx(z) ≈ Rt(z) ⇒ Rx(z) ≈ Rx ∪ t(z) ≈ Rt (z),
where at least one side is an equality. For convenience, we write X ~r t ⇔ Rx(Z) ≈ Rt(z) ∀ Z ∈
x ∪ t. Assume now that F is an extension of the label map y from x to x ∪ t1. For given t one
can downsize the set X to X ⊂ X in order to align both robustness distributions; however, there will
likely be a trade-off between this alignment and the distance of F to y as maps on t, because there
are less points to extend from (therefore allowing for a higher variance).
For such an extension F it holds that F|x ≡ y|x, thus:
Q	(x)	=	max	ky(X)-	y(X)k =	max	kF(X)-	F(X)k
Qx(X) = max .	77	= max . Ξ7
x∈Γχ(x)	d(X,X)	定 ∈Γχ(x)	d(X,X)
∀x∈x
Assuming X ZR t then enables the following conclusion for Z ∈ X ∪ t:
(*)
Qx ∪ t(Z) ≈ Qx(Z)
max
2∈Γχ(z)
ky(z) — y(z)k
d(z, Z)
max k (F(Z) ”?)+ 品 k
2∈Γχ(z)	d(z,z)
where z := y(Z) -F(Z). Notably, the rights side depends at most on one point outside X. Therefore
it includes at most one Ez compared to the naive approach including both Ez and 空：
Qx ∪ t(Z) =	max
z∈Γχ ∪ t(z)
ky(z) — y(z)k
d(z, Z)
max
z∈Γχ ∪ t(z)
k (F(Z)- F(Z)) + (Ez-EZ)k
d(Z, Z)
To summarize: by assuming X ZR t one can find a small γz such that Qx ∪ t(Z) = Qx(Z) + γz and
trade EzZ ∈ TS for γz ∈ R. But whereas the first depends on the extension F, the latter only depends
on the data.
Let now L(F - y) be the Lipschitz constant of the map F - y on X ∪ t. Using (*) and the reverse
triangle inequality one can derive the following (comp. A.4):
L(F - y ) ≥ max max
z∈x ∪ t zZ∈Γx ∪ t(z)
ky(Z) - y(Z)k
d(Z, Z)
max
zZ∈Γx ∪ t (z)
kF(Z)- F(Z)k I
d(Z, Z) I
max max
z∈x ∪ t zZ∈Γx(z)
k (F(Z)- F(Z))+ Ezk
d(Z, Z)
+ γz -	max
zZ∈Γx ∪ t (z)
kF(Z)- F(Z)k I
d(Z, Z) I
Hence, EzZ and γz determine a lower bound on L(F - y) and decreasing it a priori may allow for
finding an extension F that minimizes both kF - yk∞ and L(F - y) at the same time. By aligning
the robustness distributions of X and t this bound will not only depend on the extension F, but on
the data (trading the possibly uncontrollable for the controllable). Finally, the true motive for such
an approach stems from functional analysis: the space of Lipschitz functions from X ∪ t to TS,
i.e. LiP(X ∪ t, TS), is a Banach space with respect to the norm k ∙ ksum := k ∙ k∞ + L(∙) (comp.
CObzaS et al. (2019)). So by regarding extensions F of y that minimise kF 一 yksum we a priori
restrict the size of the hypothesis space from arbitrary maps to Lipschitz maps. The completeness of
LiP(X ∪ t, TS) is of importance as it prevents sequences of extensions (Fn) with kFn -yksum → 0
to exit this smaller space2.
1One may think of a machine learning model F trained on x making predictions on t.
2Training a machine learning model produces exactly such a sequence Fn .
4
Under review as a conference paper at ICLR 2021
3.6	Sample Robustness and KNN
Let F(Z) := PK=ι ωiy(zi) be a K-nearest-neighbour model with reference set x, where Zi is the
i-th nearest neighbour of Z in X with weight ωi. The formula (*) from 3.5 translates to:
ky(z) - y(z)k	Il PK=I ωiy(zi) - y(Z) + Ezk
max -——7^-77— ≈ max , -------------------TT-2-37---------
2∈Γχ ∪ t(z) d(z, Z)	2∈Γχ(z)	d(z, Z)
Hence, by assuming X ~r t We have imposed a constraint on the (deterministic) model. More
precisely, it is forced to base its prediction of Z on those K-samples Zi close to Z for which the
weighted linear combination of their labels suffices the above formula a priori. If Z is among the
most robust samples, we know that a small change in feature space will only cause a small change
in label space. Therefore we expect that a higher K produces a higher accuracy as samples close to
Z will be more reliable predictors. Conversely, this is not the case for less robust samples as can be
seen by comparing prediction and true label:
F(Z) -y(Z) = PK=I %(y(Zi) - y(Z))
If there exists a z* close to Z such that the quotient ky(Z(Z-Z)Z)k is large, then by increasing K We
are more likely to find a zi close or equal to z* causing the difference to grow. Another interesting
fact is that less robust samples can by construction be considered as natural adversarial examples for
a KNN model (Hendrycks et al., 2019). Assume for simplicity a classification task with K=1: if x
and t show similar robustness distributions and x ∈ x is among the less robust samples, then there
likely exists a sample ta ∈ t such that d(x, ta) is small and y(x) 6= y(ta) (the true labels). However,
x and ta being close may cause the prediction y arg min d(ta, Z) to be equal to y(x).
Z∈x
4	Evaluation
4.1	Evaluation Model
We now determine the robustness distribution of the training- (x) and test data (t) for both CIFAR-
10 and the Online News Popularity Dataset (short: ONP) to cover the classification and regression
setup. In each case, we identify subsets of x and t stemming from the extremes of the respective
robustness distribution hoping to amplify any possible effects. Then we use deterministic mod-
els (K-Nearest Neighbour=:KNN) and the non-deterministic models (Convolutional Neural Net-
works=:CNNs, Random Forest=:RF) to analyse performance in terms of the different subsets of x
and t. For this purpose, we measure accuracy and loss that indicate how much and how well a
model can generalise. ”Best” performances are displayed in boldface. The algorithms are noted in
B as they are partially based on additional results in A. We also extended the analysis to include the
MNIST dataset in F.
The CIFAR-10 dataset (Krizhevsky, 2012) consists of 60,000 322-pixel RGB-images evenly split
into the classes airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. Figure 2 provides
visual examples. The ONP dataset consists of metadata of over 39,000 articles published by Mash-
able (Fernandes, 2015) split into 58 predictive attributes (e.g. ”number of words”, ”data channel”,
”polarity of positive/negative words”,...), 2 non-predictive (”URL”, ”timedelta”) and 1 goal field
(”shares”). Although the latter is traditionally regarded as a regression dataset, one can associate
a pseudo-classification task with it, where we classify popular (≥ 1400 shares, comp. Fernandes
(2015)) and unpopular articles based on the prediction of shares. For almost all ONP-subsets, the
binary labels are equally distributed (rate is 1:1).
For CIFAR-10 we used a KNN classifier and two different CNN architectures, one ”small” CNN and
the more complex ResNet-56 model (see C.1 for details). For ONP, we used a KNN regressor as well
as an RF regressor (see C.2 for details). Neither for CIFAR-10 nor for ONP we used cross-validation
due to the high computational effort of computing sample robustness values for each newly formed
training setx and test set t. One could determine Rx∪t(x) for all samples beforehand and then apply
cross-validation to allow for feasibility, but these robustness values would no longer be independent
of each other.
5
Under review as a conference paper at ICLR 2021
For the small CNN we first estimated its overfitting threshold for the different training(sub)sets, for
the ResNet-56 we measured after how many epochs validation loss did not decrease further (using
callbacks). In both cases, we used t as the validation set (instead of held-out data) to emphasise
the impact of the different robustness distributions in an artificial best-performance setup. The RF
regressor we trained using different numbers of trees ∈ {1, 2, 3, 4, 5, 6, 7, 8, 9} and both Mean Abso-
lute Error (MAE) and Mean Squared Error (MSE) as loss criteria. The KNN classifier and regressor
we constructed for K ∈ {1, 2, 3, . . . , 15} and both uniform weights (”uni”) and weights defined by
Euclidean distance ∣∣ ∙ ∣∣2 ("dist"). The non-deterministic models We trained 25 times and discarded
the ten highest and lowest loss- and accuracy values. Finally, we averaged the remaining five to
avoid focus on individual performances. The details for each model are in C.3.
4.2	Classification Centric Analysis (CIFAR- 1 0)
Consider the feature space FS := [0,1]3∙322 with Euclidean metric ∣∣ ∙ ∣2 and the CIFAR-10 dataset
of 50,000 training images x and 10,000 test images t labelled as k ∈ {airplane, automobile, bird,
cat, deer, dog, frog, horse, ship, truck}. We regard the label maps as functions yk : x → R, where
yk (x) is 1 if the label of x is k and 0 otherwise. The target space R is equipped with its Euclidean
norm making it a Banach space. Using Algorithm 2 (comp. B) we determined the robustness of
all samples in both x and t, respectively. Afterwards we collected subsets of least- and most robust
samples, the distribution of which is shown in Table 1. The subscript defines the subset-size in
thousands (number) and whether they belong to the least- (L) or most (M) robust samples ofx or t.
As an example, xL40 stands for the 40,000 least robust samples of the training set x. The graphs in
D.1 compare the label-wise robustness distributions for x and t.
Table 1: Sample distribution per label of all data(sub)sets.
Class	X	xL40	xM40	xL25	xM25	t	tL5	tM5	tL2	tM2
”airplane”	5000	4332 (87%)	3538 (71%)	3109 (62%)	1891	1000	622 (62%)	378	303 (30%)	136(14%)
”auto”	5000	3171 (63%)	4898 (98%)	982 (20%)	4018	1000	207 (21%)	793	24 (2%)	358 (36%)
”bird”	5000	4540(91%)	3104 (62%)	3476 (70%)	1524	1000	675 (68%)	325	366 (37%)	116(12%)
”cat”	5000	3795 (76%)	4358 (87%)	2153 (43%)	2847	1000	435 (44%)	565	135 (14%)	224 (14%)
”deer”	5000	4704 (94%)	2904 (58%)	3844 (77%)	1156	1000	787 (79%)	213	432 (43%)	54 (5%)
”dog”	5000	3701 (74%)	4403 (88%)	2029 (41%)	2971	1000	398 (40%)	602	123 (12%)	281 (28%)
”frog”	5000	4544 (91%)	3467 (69%)	3453 (69%)	1547	1000	711 (71%)	289	312 (31%)	94 (9%)
”horse”	5000	3789 (76%)	4517 (90%)	2048 (41%)	2952	1000	387 (39%)	613	69 (7%)	245 (25%)
”ship”	5000	4323 (86%)	3927 (79%)	2921 (58%)	2079	1000	567 (57%)	433	207 (21%)	136(14%)
”truck”	5000	3101 (62%)	4884 (98%)	985 (20%)	4015	1000	211 (21%)	789	29 (3%)	356 (36%)
Σ	50000	-40000-	40000	-25000-	25000	10000	-5000-	5000	2000	-2000
The relative label-distributions of both x and t are similar judging by the 50%, 20% and 80%
quantiles (black percentage values are almost equal, blue and green values add up to about 100%).
In Figure. 2 some of the most- and least robust samples in x are displayed together with their
robustness value. Here, the values of the least robust samples of ”airplane” and ”ship” coincide as
one is the closest differently labelled image with respect to the other (and vice versa). The same
holds for the least robust images of ”bird” and ”deer”. In this classification setup, one can see that
a close ”visual distance” causes low sample robustness as the difference of labels is either 1 or 0.
More examples are in D.2.
0.9515	0.9456	0.9461	0.9379	0.7334	0.7334	0.7526	0.7526
Figure 2: Most- (left) and least robust samples in x labelled ”airplane”, ”ship”, ”bird”, ”deer”.
4.2	. 1 CNNS
The results (LOSS \ ACC) for the CNNs are displayed in Table 2 - 4. We noted the number of
epochs it took to reach the lowest average loss on t (comp. C.3) and the corresponding performances
for all training- and test(sub)sets. Table 3 displays the performance matrix of an alternative session
for the small CNN where loss was re-weighted to account for class imbalance in the subsets of t.
D.3 displays the accuracy and loss curves of a small CNN prototype model trained on x for all
test(sub)sets.
6
Under review as a conference paper at ICLR 2021
Table 2:	Small CNN performance matrix and number of epochs until overfitting commences.
Epochs
t
tL5
tM5
tL2
tM2
x
85
0.6804 \ 0.7822
0.6970 \ 0.7765
0.6609 \ 0.7871
0.6538 \ 0.7909
0.6684 \ 0.7852
xL40
80
0.6858 \ 0.7760
0.6819 \ 0.7772
0.6916 \ 0.7764
0.6358 \ 0.7953
0.7122 \ 0.7708
xM40
80
0.6925 \ 0.7766
0.7433 \ 0.7631
0.6417 \ 0.7900
0.7578 \ 0.7643
0.6409 \ 0.7940
xL25
75
0.8123 \ 0.7428
0.7517 \ 0.7570
0.8808 \ 0.7276
0.6553 \ 0.7839
0.9742 \ 0.7166
xM25
75
0.7973 \ 0.7336
0.8884 \ 0.7006
0.7039 \ 0.7676
0.9327 \ 0.6877
0.6882 \ 0.7793
Table 3:	Small CNN performance matrix with re-weighted LOSS (same epochs as in Table.2).
Epochs
t
tL5
tM5
tL2
tM2
x
85
0.6807 \ 0.7833
0.6974 \ 0.7774
0.7436 \ 0.7880
0.7559 \ 0.7927
0.7943 \ 0.7896
xL40
80
0.6912 \ 0.7771
0.6956 \ 0.7788
0.7658 \ 0.7761
0.7544 \ 0.7950
0.8634 \ 0.7688
xM40
80
0.6972 \ 0.7763
0.7362 \ 0.7626
0.7228 \ 0.7900
0.8335 \ 0.7623
0.7608 \ 0.7958
xL25
75
0.8071 \ 0.7419
0.7651 \ 0.7563
0.9546 \ 0.7293
0.8102 \ 0.7816
1.1018 \ 0.7171
xM25
75
0.7979 \ 0.7349
0.8571 \ 0.7045
0.8030 \ 0.7660
0.8432 \ 0.6902
0.8475 \ 0.7781
Table 4:	ResNet-56 performance matrix where loss was lowest on t for the amount of epochs noted.
Epochs
t
tL5
tM5
tL2
tM2
x
20+1 (15)
0.7568 \ 0.8535
0.7735 \ 0.8493
0.7397 \ 0.8585
0.7236 \ 0.8570
0.7542 \ 0.8561
xL40
20+1 (13)
0.8466 \ 0.8316
0.8404 \ 0.8332
0.8520 \ 0.8290
0.7769 \ 0.8453
0.8842 \ 0.8209
xM40
15+1(12)
0.8419 \ 0.8308
0.8811 \ 0.8186
0.8045 \ 0.8417
0.8691 \ 0.8168
0.7971 \ 0.8419
xL25
15+1 (11)
1.0567 \ 0.7663
1.0099 \ 0.7821
1.1019 \ 0.7519
0.9177 \ 0.8034
1.1698 \ 0.7390
xM25
15+1 (11)
1.0567 \ 0.7656
1.1431 \ 0.7351
0.9715 \ 0.7938
1.1785 \ 0.7193
0.9576 \ 0.8010
Observations (CNNs): performance on t and the number of epochs necessary to reach the least
loss thereon decreased both by downsizing x. Training on xL40 (i.e. 80% of the original training
set) causes a decrease in performance on tM5 and tM2, but increases performance on tL5 and
tL2 . It shows the effect of training and testing on subsets which are more aligned in terms of their
robustness distribution discussed in 3.5. Similar results hold for xM40 . When removing half the
samples from x, we can see the trade-off between creating more similar robustness distributions and
the overall ability of the model to generalise from its training data. Table 3 reveals furthermore that
training on the more robust half of x causes less of a bias: loss on the test(sub)sets is more evenly
distributed compared to that of models trained on xL25 . This effect is likely caused by the model
learning to connect small differences in feature space with large differences in label space. Hence,
it upscales predictions of more robust samples that are further away from the less robust data it was
trained on. For the ResNet-56, we can see the same relation between training and performing on
subsets of different robustness as for the small CNN. However, there is no (partial) improvement
over the baseline. We attribute this to the different learning structure of a residual neural network
for which the trade-off mentioned above may behave differently.
4.2.2 KNN
For KNN, the highest accuracy in every case was observed for K=13. Table 5 shows the performance
matrix for all data(sub)sets (note that for K = 1 both weights coincide). D.4 displays the accuracy
curves for K ∈ {1, . . . , 15} and both weights using x, xL25 and xM25 as reference sets.
Table 5:	1NN accuracy matrix.
t
tL5
tM5
tL2
tM2
x
0.3539
0.3700
0.3378
0.4095
0.3575
xL40
0.3434
0.3702
0.3166
0.4095
0.3155
xM40
0.3863
0.3948
0.3778
0.3950
0.3875
xL25
0.3135
0.3656
0.2614
0.4095
0.2410
xM25
0.3806
0.3356
0.4256
0.3055
0.4320
Observations (KNN): the highest accuracy on the most- and least robust test(sub)sets is achieved
using the reference sets which are the most similar in terms of their robustness distribution, comp.
3We attribute this to the following fact: a large ”closest” distance to differently labelled samples does not
ensure that there exist close samples of the same label after all. This effect is amplified for image data, where
changing the background does not necessarily change the label ofa displayed object, but will increase Euclidean
distance significantly.
7
Under review as a conference paper at ICLR 2021
3.5	. Whereas accuracy on tL2 did not decrease by removing more robust samples, it increased on
tM2 when removing less robust samples. This effect and the increased accuracy on t by using xL40
we attribute to the removal of natural adversarial examples as discussed in 3.6.
4.3	Regression Centric Analysis (Online News Popularity Dataset)
The Online News Popularity dataset (Fernandes, 2015) was split randomly into training set x and
test set t consisting of 32,000 and 7,644 samples, respectively. Each sample x ∈ x ∪ t is an element
in FS := [0,1]58, where We use the Euclidean metric ∣∣ ∙ k2 again. The label map y : X → R (where
R is equipped with its Euclidean norm as well) assigns to each sample a normalised number of shares
in [0, 1]. This time we used algorithm 1 (comp. B) to calculate the robustness for each element in
x and t (again independently). We then defined different subsets of both x and t using the same
notation as for the CIFAR-10 set. The sets tL and tM consist of the least- and most robust half of
t; the sets tl and tm consist of the 500 least- and most robust samples of tL and tM, respectively.
Figure 3 displays the relative distribution of shares for all data(sub)sets. The black dashed line
divides samples into those below 10,000 shares on the left side and those above 10,000 shares on
the right (cumulated at 110). The values in the middle display the relative amount of samples with
shares above this threshold. For most of the data(sub)sets about 5% of the samples have more than
10,000 shares. For both xM2 and tm the relative amount is about 9.4%. Conversely, while 7.6%
of samples in xL2 have more than 10.000 shares, this holds for only 0.34% of tl (i.e. less than half
the relative amount). Indeed, this implicates that the label-distribution of the ≈ 6%4 least robust
samples in x is quite different from that in t. E.1 compares the overall robustness distribution of x
and t.
Figure 3: Relative distribution of shares for all training- and test(sub)sets.
4.3.1	KNN
Table 6 shows the performance matrix (LOSS \ ACC) for uniform (”uni”)- and distance (”dist”)
weights, as well as the particular choice of K where ACC was highest on t. Table 7 shows the
performances and the specific K (in brackets) for which ACC was the highest on each test(sub)set,
respectively. E.2 displays the performance graphs.
Table 6: ”uni” (left) / ”dist” (right) weights where ACC was highest on t.
x
3
-0.1456 \ 0.5900
-0.0869 \ 0.5900
-0.3863 \ 0.5900
0.0087 \ 0.5900
-0.1255 \ 0.6320
xL24	xM24		X	xL24	xM24
4	4	~~	3	4	4
-0.1377 \ 0.5959	-0.3945 \ 0.5907	t	-0.1328 \ 0.5899	-0.1303 \ 0.5942	-0.4141 \ 0.5917
-0.0940 \ 0.6049	-0.4290 \ 0.5942	tL	-0.0843 \ 0.5911	-0.0899 \ 0.6023	-0.4527 \ 0.5931
-0.3169 \ 0.5869	-0.2531 \ 0.5871	tM	-0.3315 \ 0.5887	-0.2962 \ 0.5861	-0.2558 \ 0.5903
-0.0046 \ 0.6140	-0.1366 \ 0.6020	tl	-0.0073 \ 0.5900	-0.0046 \ 0.6120	-0.1500 \ 0.6000
-0.2217 \ 0.6480	-0.0544 \ 0.6500	tm	-0.1222 \ 0.6280	-0.2224 \ 0.6460	-0.0511 \ 0.6620
Observations (KNN): as for the KNN classifier on CIFAR-10 we can see an improvement on the
most robust subsets oft by choosing the corresponding subset xM24. Conversely, this time a similar
result holds for the less robust data as well. For both weights, ACC on tm is highest and drops off
significantly for tl . This may be explained by the different distributions of the least robust samples
4 #xL2
#x
2000
32000
0.0625 and 蒙
500
7644
0.0654
8
Under review as a conference paper at ICLR 2021
Table 7:	”uni” (left) / ”dist” (right) weights where ACC was highest for each test(sub)set.
x
-0.1456 \ 0.5900 (3)
-0.0867 \ 0.5947 (4)
-0.3863 \ 0.5900 (3)
0.0045 \ 0.6100 (4)
-0.0278 \ 0.6800 (14)
xL24_________
-0.1377 \ 0.5959 (4)
-0.0940 \ 0.6049 (4)
-0.2925 \ 0.5926 (5)
-0.0046 \ 0.6140 (4)
-0.1309 \ 0.6720 (13)
xM24
-0.3945 \ 0.5907 (4)
-0.4290 \ 0.5942 (4)
-0.3664 \ 0.5882 (3)
-0.1366 \ 0.6020 (4)
-0.0165 \ 0.6820 (12)
t
tL
tM
tl
tm
x
-0.1328 \ 0.5899 (3)
-0.0840 \ 0.5945 (4)
-0.3315 \ 0.5887 (3)
0.0040 \ 0.6100 (4)
-0.0239 \ 0.6800 (13)
xL24_________
-0.1303 \ 0.5942 (4)
-0.0899 \ 0.6023 (4)
-0.2645 \ 0.5939 (5)
-0.0046 \ 0.6120 (4)
-0.1314 \ 0.6740 (14)
xM24
-0.4141 \ 0.5917 (4)
-0.4527 \ 0.5931 (4)
-0.2558 \ 0.5903 (4)
-0.1500 \ 0.6000 (4)
-0.0185 \ 0.6840 (12)
in x and t (comp. xL2 and tl in Figure 3). The optimal K for each test(sub)set varies between 3
and 4, except for tm where it is nearly 3-4 times as high. Moreover (and in contrast to KNN on
CIFAR-10), this causes an increase in accuracy of about 2% -5% in accordance with our theoretic
explanation in 3.6.
4.3.2 Random Forest
Table 8 shows the performances matrix (LOSS \ ACC) and the number of trees where ACC on t
was the highest. Table 9 shows the performances and numbers of trees (in brackets) for which ACC
was the highest on each test(sub)set, respectively. E.3 displays the performance graphs.
Table 8:	MAE (left) / MSE (right) as loss-criteria where ACC was highest on t.
x
5
-0.1867 \ 0.5951
-0.1176 \ 0.5968
-0.4461 \ 0.5922
0.0147 \ 0.6256
-0.3125 \ 0.6564
xL24
4
-0.2884 \ 0.5907
-0.1583 \ 0.5964
-0.6904 \ 0.5848
-0.0144 \ 0.6232
-0.4946 \ 0.6376
xM24		X	xL24
3	Trees	7	6
-0.1760 \ 0.5920	t	-0.1148 \ 0.5953	-0.2796 \ 0.5909
-0.1006 \ 0.5974	tL	-0.0590 \ 0.5998	-0.1116 \ 0.5959
-0.4743 \ 0.5864	tM	-0.3814 \ 0.5905	-0.9062 \ 0.5873
0.0236 \ 0.6232	tl	0.0194 \ 0.6296	-0.0013 \ 0.6276
-0.3448 \ 0.6408	tm	-0.3278 \ 0.6616	-0.7771 \ 0.6456
xM24
4
-0.1305 \ 0.5928
-0.0616 \ 0.5968
-0.3704 \ 0.5884
0.0138 \ 0.6256
-0.3306 \ 0.6452
Table 9:	MAE (left) / MSE (right) as loss-criteria where ACC on each test(sub)set is highest.
x
-0.1867 \ 0.5951 (5)
-0.2098 \ 0.5973 (3)
-0.4461 \ 0.5922 (5)
0.0065 \ 0.6316 (3)
-0.2920 \ 0.6620 (7)
xL24
-0.2884 \ 0.5907 (4)
-0.1583 \ 0.5964 (4)
-0.8000 \ 0.5867 (3)
-0.0198 \ 0.6288 (3)
-0.2708 \ 0.6512 (9)
xM24
-0.1760 \ 0.5920 (3)
-0.1006 \ 0.5974 (3)
-0.3172 \ 0.5899 (4)
0.0199 \ 0.6244 (5)
-0.2054 \ 0.6620 (8)
t
tL
tM
tl
tm
x
-0.1148 \ 0.5953 (7)
-0.0348 \ 0.6006 (9)
-0.5619 \ 0.5925 (4)
-0.0003 \ 0.6352 (6)
-0.3217 \ 0.6692 (8)
xL24
-0.2796 \ 0.5909 (6)
-0.1617 \ 0.5963 (4)
-0.9062 \ 0.5873 (6)
-0.0034 \ 0.6356 (7)
-0.6496 \ 0.6520 (9)
xM24
-0.1305 \ 0.5928 (4)
-0.0616 \ 0.5968 (4)
-0.2197 \ 0.5895 (7)
0.0172 \ 0.6288 (5)
-0.2261 \ 0.6672 (8)
Observations (RF): using 75% of the original training data correlates with a a slight decrease in
performance (independent of the metric) and a smaller optimal number of trees for xM24 than for
xL24 . When training on xL24 , LOSS on tM and tm increases significantly. This is likely caused
by the model learning to connect small changes in feature space with large changes in label space.
Hence, it tends to overshoot predictions for more robust data, which is also in accordance with our
observations in Table 3. As for KNN, we can see two things: (i) a large difference in ACC on tl
and tm and (ii) the optimal hyperparameter being significantly higher for tm . The first we attribute
again to the different distributions of xL2 and tl ; for the second we expect a similar explanation as
for KNN (comp. 3.6).
5 Conclusion and Future Work
We introduced the concept of sample robustness for measuring how sensitive elements of a dataset
are towards label-changing perturbations. We provided a theoretical motivation and analysed the
robustness distribution of different datasets, as well as the connection to model performance. In
concordance with our theoretical analysis, we found that it is possible to boost performance on spe-
cific test(sub)sets by choosing training(sub)sets exhibiting similar robustness distributions. Empiri-
cal results, however, indicate that there is a model-dependent trade-off between discarding samples
to align these distributions and the general ability of a model to generalise from its training- or ref-
erence data. Finally, we found that optimal hyperparameters may also depend on the robustness of
both the training- and test set. Possible future research directions are: (i) expanding experiments (us-
ing cross-validation, more datasets and models, different metrics, etc.); (ii) analyse optimal model-
hyperparameters as functions h(xAB, tAB) in terms of training- (xAB) and test(sub)sets (tAB) of
different parts of the robustness spectrum; (iii) explore the relation between training on more- or less
robust data and model-robustness, e.g. susceptibility to adversarial examples (Szegedy et al., 2013).
9
Under review as a conference paper at ICLR 2021
References
Laura Beggel, Michael Pfeiffer, and Bernd Bischl. Robust anomaly detection in images using ad-
versarial autoencoders, 01 2019.
Y. Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new per-
SPectives. IEEE transactions on pattern analysis and machine intelligence, 35:1798-1828, 08
2013. doi: 10.1109/TPAMI.2013.50.
Alexander Bernstein and Alexander Kuleshov. Low-dimensional data rePresentation in data analy-
sis. pp. 47-58, 10 2014. ISBN 978-3-319-11655-6. doi: 10.1007/978-3-319-11656-3_5.
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic, Pavel Laskov, Gior-
gio Giacinto, and Fabio Roli. Evasion attacks against machine learning at test time. ArXiv,
abs/1708.06131, 2013.
Lawrence Cayton. Algorithms for manifold learning. 07 2005.
Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey.
CoRR, abs/1901.03407, 2019. URL http://arxiv.org/abs/1901.03407.
Denis Chigirev and William Bialek. Optimal manifold representation of data: An information theo-
retic approach. 01 2003.
S. Cobzas, R. Miculescu, and A. Nicolae. Lipschitz Functions. Lecture Notes in Mathematics.
Springer International Publishing, 2019. ISBN 9783030164898. URL https://books.
google.de/books?id=r9yZDwAAQBAJ.
Dumitru Erhan, Aaron C. Courville, Yoshua Bengio, and Pascal Vincent. Why does unsupervised
pre-training help deep learning? J. Mach. Learn. Res., 11:625-660, 2010.
Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Xiaodong Song. Robust physical-world attacks on deep learning models.
2017.
Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard. Robustness of classifiers:
from adversarial to random noise. In NIPS, 2016.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.
Journal of the American Mathematical Society, 29, 10 2013. doi: 10.1090/jams/852.
Kelwin Fernandes. A proactive intelligent decision support system for predicting the popularity of
online news. 08 2015. doi: 10.1007/978-3-319-23485-4.53.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, 2016.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples, 07 2019.
Alex Krizhevsky. Learning multiple layers of features from tiny images. University of Toronto, 05
2012.
Yann Lecun, Leon Bottou, Y. Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86:2278 - 2324, 12 1998. doi: 10.1109/5.
726791.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Xiaodong Song. Delving into transferable adver-
sarial examples and black-box attacks. ArXiv, abs/1611.02770, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. ArXiv, abs/1706.06083, 2017.
Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya Sutskever. Deep
double descent: Where bigger models and more data hurt. In International Conference on Learn-
ing Representations, 2020. URL https://openreview.net/forum?id=B1g5sA4twr.
10
Under review as a conference paper at ICLR 2021
Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z. Berkay Celik, and Ananthram
Swami. The limitations of deep learning in adversarial settings. pp. 372-387, 03 2016a. doi:
10.1109/EuroSP.2016.36.
Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. Transferability in machine learning:
from phenomena to black-box attacks using adversarial samples. ArXiv, abs/1605.07277, 2016b.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfel-
low, and Rob Fergus. Intriguing properties of neural networks. CoRR, abs/1312.6199, 2013.
Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certifica-
tion of perturbation invariance for deep neural networks. ArXiv, abs/1802.04034, 2018.
Beilun Wang, Ji Gao, and Yanjun Qi. A theoretical framework for robustness of (deep) classifiers
against adversarial examples, 2016.
Tsui-Wei Weng, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Dong Su, Yupeng Gao, Cho-Jui Hsieh, and
Luca Daniel. Evaluating the robustness of neural networks: An extreme value theory approach.
ArXiv, abs/1801.10578, 2018.
Chen Zhu, W. Huang, Ali Shafahi, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein.
Transferable clean-label poisoning attacks on deep neural nets, 05 2019.
11
Under review as a conference paper at ICLR 2021
A Additional Theory and Proofs:
A.1 Proposition:
Let L(y) be the Lipschitz constant of the label map y : x → TS. The following equality holds:
max Qx (X) = L(y)
x∈x
Proof. One has L(y) ≥ max Qx (X) by construction. Furthermore, as x is compact:
x∈x
L(y) = max
x=x
ky(x)-y(X)k _ ky(xo) - y(Xo)k
d(x, X)
d(x0,X0)
for some xo = Xo such that y(χo) = y(Xo). Thus:
「「xo) ≤ m望 Qx(X)
A.2 Proposition:
Rx(X) is independent of rescaling the label map y.
Proof. Suppose We have a label map y := ay for a ∈ R - {0}, then:
二,.
TR X (x):
kyk∞
ma： ky(χ) - y(χ)k +
x∈Γχ(x)	d(x, x)
|a| kyk∞
*臀X)imX⅛F + ∣a∣kyk∞
Rx(X)
A.3 Proposition:
Let x ∈ x With reach r(x). If y(x) ∈ {0, e} for all x ∈ x, Where e lies on the unit sphere in TS (i.e.
y is a binary label map), then:
Rx (x)
rχ(x)
rχ(x)+ 1
□
□
Proof. It holds that:
Rχ(x) =	max
Vx∈Γχ(x)
叱-y(x)k +1)T= (- max、J+ 1)T
d(x, x)	X	∖x∈Γχ(x) d(x, x)	)
^⅛^ + ι)T = ( r⅛ + ι)T = ( ⅛⅛F r
x∈Γχ(x)
□
12
Under review as a conference paper at ICLR 2021
A.4 Lemma:
With the notions of 3.5 it holds:
L(F - y) ≥ max max
z∈x ∪ t∣2∈Γχ ∪ t(z)
ky(Z) - y(Z)k
d(z, Z)
max
2∈Γχ ∪ t(z)
kF(Z)- F(Z)k I
d(z, Z) I
Proof. Using the reverse triangle inequality twice:
max
2∈Γχ ∪ t(z)
ky(Z) - y(Z)k
d(Z, Z)
max
2∈Γχ ∪ t(z)
kF(Z)- F(Z)k ∣
d(Z, Z) I
≤ max ∣ ky(Z)-y(Z)k _ kF(Z)- F(Z)k
一2∈Γχ ∪ t(z)∣	d(Z,Z)	d(Z,Z)
max
2∈Γχ ∪ t(z)
≤
ky(Z) - y(Z) - F(Z) + F(Z)Il
d(Z, Z)
To see the first inequality, let Xz := Γχ ∪ £z), Vz(Z) := ky(Z(-Z(Z)k, Wz(Z) := kF(Z(-F)(Z)k and
note that V = |V | and W = |W |. Then, by using the uniform norm on Xz defined as
kVk∞,Xz := max |V(Z)|,
z∈Xz
we see that the first inequality is indeed a consequence of the reverse triangle inequality:
IIIkV k∞,Xz - kW k∞,Xz III ≤ kV - W k∞,Xz
Taking the maximum over Z ∈ X ∪ t yields the statement.	□
B Algorithm (Sample Robustness)
Let X ⊂ FS be a dataset in some feature space with metric d and label map y taking values
in some target space TS with norm k ∙ k. For a sample X ∈ FS with label y(χ) such that
ky(χ)k ≤ max ky(X)k one can calculate its robustness using the algorithms below. Note that the
x∈x
second algorithm is a special case of the first.
Algorithm 1 Calculate Rx(x) (General y)
Require: #y(X) ≥ 2, kyk∞ = 1
y — y(χ)
yj J y(xj) for any Xj ∈ X s.t. yj = y
Q j ky - yjk
d(X, Xj )
for Xi ∈ X do
yi J y(Xi)
if y 6= yi then
Q J max{Q,
end if
ky - yik}
d(X, Xi)
end for
print (Q + 1)-1
13
Under review as a conference paper at ICLR 2021
Algorithm 2 Calculate Rx (x) (Binary y)
Require: y(x) = {0, e} with kek = 1
y — y(χ)
r — max
x
for xi ∈ x do
y J 期(Xi)
if y 6= yi then
r J min{r, d(x, xi)}
end if
end for
r
print -----
r+1
C Models
C.1 CIFAR- 1 0
Small CNN
The small CNN architecture was taken initially from the Keras homepage5 and consists of two
convolutional blocks, each built using two convolutional layers with kernel size (3,3) and RELU
activation function, followed by MaxPooling with pool size (2,2) and Dropout ratio of 0.25. After
this, there are two dense layers (512/RELU and 10/Softmax) with a Dropout ratio of 0.5 in between.
It was compiled using the RMSpropo optimizer with learning rate 10-4 and decay of 10-6. We used
the categorical cross-entropy as our loss function and trained the model with a batch size of 32.
ResNet-56
The ResNet-56 architecture (He et al., 2016) was also taken directly from the Keras homepage6. We
used n=9, batch sizes of 32 and the pre-defined learning rates of 10-3 and 10-4.
KNN
For the KNN classifier we used the scikit-learn pre-defined algorithm7 with both uniform weights
(“uni")and weights defined by the Euclidean metric ∣∣ ∙ k2 (“dist").
C.2 Online News Popularity Data Set (ONP)
KNN
For the KNN regressor we used the scikit-learn pre-defined algorithm8 with both uniform weights
(“uni")and weights defined by the Euclidean metric ∣∣ ∙ ∣2 (“dist"). The loss function is defined as
P ( true pred)2
LOSS := 1------J(yi------y一)------2 ∈ (-∞, 1],
Pi yitrue - MEAN(ytrue) 2
where 1 is the optimum. Accuracy is based on the pseudo-classification task (see 4.1).
5It was recently removed.
6https://keras.io/zh/examples/cifar10_resnet/
7https://scikit- learn.org/stable/modules/generated/sklearn.neighbors.
KNeighborsClassifier.html
8https://scikit- learn.org/stable/modules/generated/sklearn.neighbors.
KNeighborsRegressor.html
14
Under review as a conference paper at ICLR 2021
Random Forest
To construct a random forest regressor we also used the scikit-learn pre-defined algorithm9. We
measured the same loss as for KNN. Again, accuracy is based on the pseudo classification task (see
4.1).
C.3 Procedure Details for the Non-Determinisitc Models
Small CNN
For the small CNN, we trained multiple models for an increasing amount of epochs e until we could
roughly pinpoint the overfitting-threshold eover on the test set t for every training(sub)set. Then,
in a second session, we trained 25 independent models (always starting with different weights) for
eover epochs and collected their performances on all of the test(sub)sets t, tL5 , tM5 , tL2 , tM2 in
five accuracy- and five loss-lists, respectively. From each list, we discarded the ten highest and the
ten lowest values of the 25 and then averaged the remaining five in order to avoid focus on individual
model performance. After this we repeated the procedure for ek := eover ± 5k, k ∈ Z, epochs until
we found a local loss-minimum on t away from the lowest and highest number of epochs in each
case. However, it is still possible to fall victim to the stochastic behaviour of neural networks, and
in some cases, we repeated second sessions to account for this fact. The optimal number of epochs
may vary in a ±5 vicinity. Also, since double-descent has been discovered (Nakkiran et al., 2020),
one may always question the optimal overfitting threshold.
ResNet-56
The ResNet-56 models were trained 15 times on each of the different training(sub)sets for 80 epochs
using a learning rate of 10-3. Here, we used callbacks to save the best weights and to identify the
particular number of epochs for which loss on t was the lowest. We then picked an individual
epoch-threshold from {5k | k ∈ N} such that at most 3 of the 15 values lay above it (to account for
outliers). Finally, we conducted second sessions similar to those for the small CNN, where we (i)
trained the models for the number of epochs indicated by these upper thresholds, (ii) saved the best
weights, (iii) rebuilt the models with these weights and (iv) topped everything off with an additional
epoch of training using a lower learning rate of 10-4.
The epoch-history throughout training the 15 models per training(sub)set is shown below. The
number in the brackets is the rounded average of the 15 values after we discarded the highest- and
lowest three entries.
•	x: [13, 17, 20, 12, 12, 15, 15, 18, 13, 19, 12, 23, 23, 17,8] (15)
•	xL40: [9, 17, 14, 15, 12, 18, 13, 16, 11, 12, 64, 7, 13, 12,9] (13)
•	xM40: [14, 11, 21, 11, 9, 10, 77, 15, 11, 14, 10, 8, 14, 8, 13] (12)
•	xL25: [12,7,9,14,12,77,9,14,12,21,5,7,14,7,11] (11)
•	xM25: [6, 15, 9, 13, 13, 9, 7, 10, 15, 11, 13, 17, 10, 10,9] (11)
Random Forest
For every number of trees, we simply used the same second session procedure as for the small CNN,
tailored for the ONP training- and test(sub)sets.
9https://scikit- learn.org/stable/modules/generated/sklearn.ensemble.
RandomForestRegressor.html
15
Under review as a conference paper at ICLR 2021
D CIFAR-10: Additional Material
D.1 Label-wise Robustness Distribution
Figure 4: Relative class-wise sample robustness distribution for x and t.
16
Under review as a conference paper at ICLR 2021
D.2 Least- and Most Robust samples in the CIFAR- 1 0 Trainingset
Below is the juxtaposition of the least- and most robust samples in x for each class together with
their respective sample robustness and the average model output probability (in the brackets) of each
image representing its respective class. Here, the average is determined using the small CNN and the
same second session procedure described in C.3. Notably, the background of each animal or object
displayed in the image impacts robustness as differently coloured surroundings will significantly
increase distance w.r.t. ∣∣ ∙ ∣∣2. Comparing the robustness and average model probability for the
”bird”- and ”frog” images, there seems to be no definitive relationship between those values.
0.9515	0.9463	0.9461	0.9461	0.9379
0.9457
0.9428
0.9444	0.9456	0.9449
(0.9968)	(0.9057)	(0.6126)	(0.8088)	(0.7343)
0.7334	0.8480
(0.6791)	(0.1900)
0.7526	0.7876
(0.8775)	(0.0389)
(0.9394)	(0.3835)	(0.8146)	(0.9770)	(0.9966)
0.7581	0.8253
(0.4009)	(0.3291)
0.7334	0.8497
(0.7865)	(0.0219)
0.7526	0.8193
(0.6538)	(0.2691)
Figure 5: Most- (upper row) and least robust samples (lower row) in the CIFAR-10 training set.
D.3 Prototype Model Loss and Accuracy over 85 Epochs
We trained a single small CNN (see C.1) on x for 85 epochs (which approximately showed the
average performance in Table 2) and monitored its performance throughout the process. The graphs
below show the loss and accuracy values for all test(sub)sets. From the beginning, the model did
perform better on the more robust half of the test set tM5 than on the less robust half tL5. Loss
on tL2 is almost always the least of all five, though this does not necessarily cause a high accuracy.
Indeed, the subset on which the model has the highest accuracy changes every few epochs from tL2
to tM2 and vice versa throughout the whole training process.
Figure 6: Epoch-wise learning behaviour of a small CNN trained on the CIFAR-10 training set x
approximately expressing the loss in Table 2.
17
Under review as a conference paper at ICLR 2021
Figure 7: Epoch-wise learning behaviour of a small CNN trained on the CIFAR-10 training set x
approximately expressing the accuracy in Table 2.
D.4 KNN
Figure 8: KNN for CIFAR-10, split for each test(sub)set. The colour of the graph corresponds to the
reference training set.
E ONP: Additional Material
E.1 Relative Robustness Distribution
The minimum- and maximum sample robustness for x and t are:
min R(x) = 0.2607, max R(x) = 0.7616, min R(x) = 0.3792, max R(x) = 0.8195
x∈x	x∈x	x∈t	x∈t
Figure 9 displays the relative sample robustness distributions. Overall, they exhibit very similar
behaviour (the peak for x is slightly higher).
18
Under review as a conference paper at ICLR 2021
Figure 9: Relative robustness distributions for x and t. The green dashed line marks the 65%
threshold. For x, 1272 samples are below this robustness level (≈ 4%); for t, 267 (≈ 3.5%).
E.2 KNN
Figure 10: KNN LOSS and ACC for ONP, split for each test(sub)set. The colour of the graph
corresponds to the reference set.
19
Under review as a conference paper at ICLR 2021
E.3 Random Forest
Figure 11: RF LOSS and ACC for ONP, split for each test(sub)set. The colour of the graph corre-
sponds to the reference training set.
F MNIST
The MNIST data set (Lecun et al., 1998) consists of 60,000 training- (x) and 10,000 test- (t) 282-
pixel greyscale images of handwritten digits (we used the same mathematical notions as for the
CIFAR-10 set). Table 10 shows the label-distributions of different data(sub)sets (again, the notation
is the same as for the CIFAR-10 set). The relative label-distributions of both x and t are similar judg-
ing by the 50%, 20% and 80% quantiles (black percentage values are almost equal, blue and green
values add up to about 100% except for class ”1”). Overall, images displaying the digits ”1”, ”4”, ”7”
and ”9” were very prominent in xL48 , xL30, tL5 , tL2, whereas images in xM48, xM30 , tM5 , tM2
mainly consist of ”0”s, ”2”s and ”6”s.
20
Under review as a conference paper at ICLR 2021
Table 10: Sample distribution per label of all data(sub)sets.
Digit	X	xL48	xM48	xL30	xM30	t	tL5	tM5	tL2	tM2
”0”	5923	2632 (44%)	5853	632 (11%)	5291	980	-94 (10%)	886	2 (0%)	562 (57%)
”1”	6742	6739 (100%)	2760 1%)	6702 (99%)	40	1135	1125 (99%)	10	896 (79%)	1 (0%)
”2”	5958	2931 (49%)	5813 8%)	815 (14%)	5143	1032	176(17%)	856	14 (1%)	471 (46%)
”3”	6131	5040(82%)	5828 5%)	2219 (36%)	3912	1010	393 (39%)	617	34 (3%)	161 (16%)
”4”	5842	5333 (91%)	4310 4%)	3819 (65%)	2023	982	656 (67%)	326	236 (24%)	91 (9%)
”5”	5421	4567 (84%)	5164 5%)	1895 (35%)	3526	892	292 (33%)	600	18 (2%)	150(17%)
”6”	5918	4285 (72%)	5504 3%)	2004 (34%)	3914	958	265 (28%)	693	18 (2%)	331 (35%)
”7”	6265	5862 (94%)	4257 8%)	4566 (73%)	1699	1028	743 (72%)	285	289 (28%)	47 (5%)
”8”	5851	4832 (83%)	5577 5%)	2185 (37%)	3666	974	387 (40%)	587	22 (2%)	155 (16%)
”9”	5949	5779 (97%)	2934 9%)	5163 (87%)	786	1009	869 (86%)	140	471 (47%)	31 (3%)
Σ	60000	48000	―48000―	30000	30000	10000	5000	5000	2000	-2000-
The authors of Papernot et al. (2016a) noticed that when crafting source-target pairs of adversarial
examples from the MNIST test set, ”classes ”0”, ”2” and ”8” are hard to start with, while classes
”1”, ”7” and ”9” are easy to start with”. Interestingly, comparing the compositions of the 2000 least-
and most robust samples we can see that the three most prominent classes in tL2 were ”1”, ”7”
and ”9” (and close thereafter ”4”), while the two most prominent classes in xM2 were ”0” and ”2”
(followed by ”6” and ”3” before ”8”).
Figure 12 displays the most and least robust samples of each class together with their sample ro-
bustness and the average probability of a shallow CNN with an average accuracy of about 98% (25
models were trained independently and accuracy values were collected similarly to the second ses-
sion for the small CNN on CIFAR-10). The boldness of the written digit has a beneficial impact
on its robustness as it will increase Euclidean distance significantly. We can also see that the least
robust image of a ”3” presents itself as a mislabelled ”9”. Indeed, this makes sense as distance in
feature space to another image of a ”9” in x is relatively small.
0.8979	0.9025	0.8973	0.9031
0.9009
0.9086	0.8780	0.9031
(1.0000)	(0.9987)	(1.0000)
0.8951	0.9060
(1.0000)	(1.0000)
(1.0000)
(1.0000)
(1.0000)	(0.9984)	(0.9707)
0.7984	0.7058	0.7877
(0.7363)	(0.9114)	(0.7235)
0.7763	0.7389	0.7591
(0.0053)	(0.9940)	(0.9764)
0.7561	0.7058	0.7635
(0.7781)	(0.7590)	(0.0212)
0.7389
(0.9996)
Figure 12:	Most- (upper row) and least robust (lower row) samples of each label in the MNIST
training set (”0” - ”9” from left to right).
One may be tempted to regard sample robustness as a tool of anomaly detection (Chalapathy &
Chawla, 2019; Beggel et al., 2019) as it rightfully identifies the mislabelled image of a ”9”. To
underline that it is an utterly intrinsic and metric-dependent concept, consider the set of ”handwritten
digits” in Figure 13 and their respective robustness values (note that the image of the ”4” is not
missing but plain white). The relatively high value of the ”3” being a ”4” may be caused by the
missing normalization of size and position (as was done for the MNIST set).
工仓 乙 耳	施 ：：：VII幺耻
0.9028 (0)	0.9139 (1)	0.8923 (2)	0.8071 (3)	0.7812 (4)	0.9441 (5)	0.8984 (6)	0.9139 (7)	0.9152 (8)	0.9319 (9)
Figure 13:	Robust samples of ”handwritten digits” labelled as the number in the brackets.
21