Under review as a conference paper at ICLR 2021
A Stochastic Gradient Langevin Dynamics Al-
gorithm For Noise Intrinsic Federated Learn-
ING
Anonymous authors
Paper under double-blind review
Ab stract
Non-i.i.d data distribution and Differential privacy(DP) protections are two open
problems in Federated Learning(FL). We address these two problems by propos-
ing the first noise intrinsic FL training algorithms. In our proposed algorithm,
we incorporate a stochastic gradient Langevin dynamices(SGLD) oracle in local
node’s parameter update phase. Our introduced SGLD oracle would lower gen-
eralization errors in local node’s parameter learning and provide local node DP
protections. We theoretically analyze our algorithm by formulating a min-max ob-
jective functions and connects its upper bound with global loss function in FL. The
convergence of our algorithm on non-convex function is also given as contraction
and coupling rate of two random process defined by stochastic differential equa-
tions(SDE) We would provide DP analysis for our proposed training algorithm
and provide more experiment results soon.
1	Introduction
Federated Learning (FL) as a marriage on cloud computing and deep learning are gaining popular-
ity on commercial deployment (Li et al., 2020). It follows a distributed protocol to allow multiple
parties to participate on training process on their local side while collaborating and coordinating
on the cloud site (Konecny et al., 2015). As a result of an innovative corporation pattern, the Con-
sumer node would participate their part of training locally without data publishing, while technical
product provider would provide professional service both on the tuning models in training process
and expertise inference solutions from their per-trained model warehouses (Li et al., 2020). Fed-
erated Learning is especially suitable in the area of medical applications (Sheller et al., 2020). In
one way local hospitals maintain and manage the slides of pathology documents such as images
and reports. In another way, they are the consumers of computer aided automatic diagnosis prod-
ucts which comes from training on the patterns of these data and documents. Coexists with these
promising parts, federated learning has its unique characteristics and challenges.
Firstly, the coordination and communication overheads between distributed nodes and centralized
server is significantly higher than that of localized training (Sattler et al., 2019). A direct conse-
quence is that a feasible FL algorithm consists E steps of local SGD updates in parallel (Li et al.,
2019c) among than Federated Avgeraging (FedAvg) (McMahan et al., 2017) is the first perhaps the
most widely used FL algorithm.
Secondly, the distribution of data is statistically heterogeneous on different devices. The general-
ization error in each single device’s local training is huge. As a result, optimization direction would
towards overfit on local data. The shifts in training optimal solution among local devices would cause
the stabling point of FedAvg deviates be the non-optimal solution (Li et al., 2019c). One solution
for non iid problems would be introducing proximal objective (Li et al., 2018) and dual variables
(Zhang et al., 2020; Karimireddy et al., 2019). Xinwei (Zhang et al., 2020) provides an Augmented
Lagrange solution on FL learning with non iid data.
Thirdly, the data privacy concerns is frequently encountered issue in Federated learning. Due to the
vulnerability properties of internet environment. Information leaking is highly possible. Differential
privacy works to incorporating a randomized mechanism such as injecting gradient noise(Dwork
1
Under review as a conference paper at ICLR 2021
et al., 2014; Abadi et al., 2016) and irregular data sampling(Dong et al., 2019) so that the distribution
of perturbed results are insensitive to single record change.
In an attempt to handle these challenges, we would bring a Stochastic Gradient MCMC (SG-MCMC)
solution into FL settings. SG-MCMC methods as a class of scalable Bayesian sampling algorithm
in machine learning has realized significant success recently. We use SG-MCMC in FL settings
for its lower generation error bounds (Smith & Le, 2017; Li et al., 2019b) and differential privacy
preserving properties (Li et al., 2019a) with appropriately chosen step sizes.
Several existing studies on the extention of SG-MCMC algorithms on improving the generalized
performance of parameter learning and preserving differential privacy in Federated learning. Bhard-
waj (Bhardwaj, 2019) showed that an adaptive stepsize of Stochastic Gradient Langevine Dynam-
ics(SGLD) could escape local extremes of high generalization error. Chaudhari et al. (Chaudhari
et al., 2019) propose a two nested SGD algorithm to perform SGLD in their local loop of optimiza-
tion. Li et al. (Li et al., 2019a) proved that a practical stepsize of sampling models is realizable to
preserve differential privacy. Wang et al. (Wang et al., 2019) gave an bound on empirical risk to mea-
sure the error of non-convex local loss under differential privacy. However their works are studying
on the case of local training on a single node case.
Motivated by their works, we propose an SGLD algorithm in FL. In our proposed algorithm, each
node use SGLD samplings as each node’s local gradient update phase. The whole updating follows
the protocol in FedPD algorithm (Zhang et al., 2020) except that we take expectations of SGLD
sampling on the Augmented Lagrange objective. Next, we analyze our propose algorithm by formu-
lating a joint min-max variational objective functions. The whole learning process in our algorithm
would be viewed as a min-max descent in our objective functions. We then prove that our constructed
min-max functions is a variational upper bound on the global loss functions where the introduced
dual variables closes the gap among local gradient zeros. Finally we study the convergence of our
algorithm by using a technique similar in (Eberle et al., 2019) to study the couplings and contraction
in Hamilton Monte-Carlo. We prove that the distributio of two process from independent random
initialization distributions converges in our designed Wasserstein metric. In this paper, our contribu-
tions are two folds.
•	We propose an SGLD implementations of FL algorithm where the data distribution is non
iid on local nodes.
•	We formulate our SGLD implementaion of FL as optimizing on a min-max points of a
joint learning objective function. And then we derive two types of variational upper bounds
of our learning objectives on global loss functions and connects it with optimal primal-
dual conditions in consensus problems. We also study our algorithm’s convergence to the
stabling point.
2	Preliminaries
2.1	Augmented Lagrange for Federated Learning
In the framework of federated learning, N distributed nodes aim to learn a coherent network map-
Ping model V(x, ∙) in Rm → Rn parameterized by X by the loss function l(∙, ∙) in Rn, Rn → R. The
data are distributed i.i.d cross N distributed nodes. We use Di to denote the dataset on i’s distributed
node. We denote Di,q as the qth data in Node i and Yi,q as the label for qth data in Node i. The
learning objective in i’s node is defined as the expected loss from the network prediction on a data
distribution Pi 〜p({Di,q, Yi,q} ∈ Di)
Fi(X) = E{Di,q,Yi,q}∈Pil(ν(X, Di,q), Yi,q)	(1)
For simplicity, we use ξi,q , {Di,q, Yi,q} to denote the combination of qth data and label in i’s
node. The loss on ξi,q is denoted as
Fi(X,ξi,q) ,l(ν(X,Di,q),Yi,q)	(2)
The federated learning is aimed as minimizing the averaged loss across all the distributed nodes
x* = arg min N X Fi (x)	(3)
2
Under review as a conference paper at ICLR 2021
The federated learning process consists of multiple rounds of local distributed training, global ag-
gregation. updating and broadcasts on parameters. In the start of round r, the central node first
broadcast its coordinated value of xr0 to each distributed node. Each distributed node keeps a copy
of xr0 as xr0,r in their local side. Then at local distributed training phase, each local node optimize
their local objective function Li(x0, xr0,i , λir) in their local optimization oracle. The local objective
is an augmented Lagrange Li (x0, xr0,i, λir) defined as
Li(x0, xo, λi)，Fi(X) + hλi, x0 - xoi + 2||x0 - xo∣∣2	(4)
, where λir is defined as the dual variable kept at distributed node i that has the same dimension as the
parameters x. Then each node returns a xir+1 from their local optimization oracle on Li(x0, xr0,i, λir).
Then each distributed node use xir+1 and xr0 to update its dual variable from λir to λir+1. Then each
distributed node use its updated dual λir+1 and parameters xir+1 for a new xr0,+i and send xr0,+i to
centralized coordinate nodes. The centralized nodes aggregates xr0,+i from all distributed node i and
use Fedavg to update for a new xr0+1.
And we define the minibatch loss function Fi(x, ξi,Bi,t ) as
1
两
Fi (x, ξi,q)
l(ν (x, Di,bj), Yi,bj)
(5)
Finally, we define the gradient h(xir,q, ξi,Bi,q ) taken at global round r, local round q and node i is
defined as
h(χr,q ,ξi,Bi,q ) = Vχ0 Li(χr,q, x0,i,λr ,ξi,Bi,q )
=Vx Fixrq ,"q)+Y(XF-Xre+λr
(6)
(7)
2.2	Stochastic Gradient Langevine Dynamics
Langevine Dynamics is a family of Gaussian noise diffusion on Force Field VF (F (X)). Its contin-
uous time Ito diffusion could be written as
dxt = -VxF(X)dt + β-2 dBt	(8)
,where Bt ∈ Rp is a p-dimensional Brownian motion. Function F as F : Rp → R are assumed
to satisfy Lipschitz continuous condition. Stochastic Gradient Langevine dynamics could be a dis-
crete form of Langevine Dynamics as a Euler-Maruyama approximation of the stochastic ordinary
equation(SDE). The discretization has a form of Gaussian Noisy injected Gradient. We write their
dicretization in the following form
Xt+1 = Xt - VxF(X)∆t +N(0, ∆tβ-1I)	(9)
By written Xn as Xt+1, ∆t as ηn, we could write the SGLD in the form of step-wise gradient descent
plus an Gaussian Noise term to perform Bayesian samplings
Xn+1 = Xn - ηnVxF(X) + N (0, ηnβ-1I)	(10)
By seeing the noise injected descending steps as a Markov chain, the stationary distribution would
reduce to the following form
P(X) H e-βF(x)	(11)
3	Problem Formulation
3.1	An joint Min-Max Objective for Federated Stochastic Gradient MCMC
We formulate the problem of our federated stochastic gradient MCMC as optimizing the joint min-
max function
N1	γ
maxminF(x,λi) =	N log J exp[β(-Fi(x )- <λ%, X - X > -2||x - x∣∣2)]dx (12)
3
Under review as a conference paper at ICLR 2021
The gradient of the F(x, λ1, . . . λn) at x0 could be given by
δF
δx
x=x0
N1
N2 N (EPi(x0|x0)x0 + λi - x0)
i=1N
where we denote
δF
δx
x0+,i = xi + λi
xi = EPi(x0|x0)x
So we could rewrite the gradient calculation as
N1
=X N (χ+,i- χo)
x=x0 i=1
The gradient of the F(χ, λ1, ...λn) at χ0, λi could be written as
δF
= x0 - xi
δλi x=x0
where the distribution Pi(x0|x0) could be written as
Pi(XiX0) H exp[-βLi(x0, X0, λi)]
where Li (x0, x0) follows our previous definition as
Li(X0, x0, λi) , Fi(XO)+ < λi, χ0 - x0 > +2 ∣∣χ0 - x0∣∣2
(13)
(14)
(15)
(16)
(17)
(18)
(19)
As a federated learning implementation, the computation of δF∕δx is distributed among local nodes.
In one round of learning, each local node i first use Monte-Carlo estimation of xi from the samples
along SGLD steps on function Li(x0, x0) using mini-batch update from its private data. Then each
local node i updates its private owned dual variable λi by equation by Equation 17. Next, each local
node i computes their contributing part of x0+,i by Eq. 14 and sends it to the server node. Then the
server node averages its aggregated x+i from all local nodes for δF∕δx by Eq. 13 and uses gradient
descent to update parameter x. Finally the server node broadcast its update global parameters x back
to each distributed nodes. The algorithm of dual descent on x , λ1, . . . , λN is shown in Algorithm 1.
Algorithm 1: Our Federated Stochastic Gradient MCMC Algorithm
Input: : x0 , η, p, T
Initialize: x00 = x0,
for r = 0, . . . , T - 1 do
for i = 1, . . . ,N	in parallel do
Local Update:
Li(x0, x0,i,λr) = -Fi(XO)- < λr, x0,i - x0 > - 2 ||x0- x0,i||2)
xir+1 = SGLD-Oraclei(Li(x0,xr0,i,λir))
1	= λr+ηγ (xr+1 -
L x0+ = xr+1 +1 λr+1
Global Communicate:
Aggregate:
Broadcast:
xr0,+i1 = xr0 + η(xr0+1 - xr0),i = 1, . . .,N
3.2	SGLD as Local Oracle Stochastic Gradient MCMC
In the inner-loop of our federated learning algorithm, each local node computes EPi(x0|x0) by taking
SG-MCMC steps in their SGLD-Oracle. In their local SGLD-Oracle, the distribution ofPi(xO|x0) is
approximated by samplings along the markov chains of SGLD on the objective function of its local
augmented Lagarange in Eq. 19 . In our implementation of local SGLD-Oracle, we take several
4
Under review as a conference paper at ICLR 2021
epochs of SGLD without taking sampling in the early burn in period. To have a quick burn in times,
we keep the step-size of SGLD fixed in our burn in period. After burn in, we give two SGLD
sampling algorithms for EPi(x0|x0) with fixed stepsize and decreasing stepsize in a rate of ηT =
O(T -1/3)(Teh et al., 2016; Chen et al., 2019) to obtain a optimal mean square error bound.
Algorithm 2: SGLD-Oracle
Input: :Local Dataset ξ, number of local iterations Q, clip norm length L, base step-size {η}
Initialize: xir+1,0 = xr0,+i1, ηt = ηxir+1 = 0,
for q = 0, . . . , Q do
Sample a mini-batch ξi,Bi,q
Calculate gradients h(xr,q,£出日)=NMLi(Xrq, ∙, ∙,ξi,B%,q)
Clip norm : h(∙) = h(∙)∕max(1, llhL)ll2)
if q > Q0 (Decreasing Steps) then
Lnt = (q - QO)-1∕3η
Noisy Gradient Descent: x^q+1 = xrr,q — nthgq,ξi,Bi,q) + √nteN(0, I)
if q = Q0 then
I γr+ι _ γr,q+1
LXi	= Xi
if q > Q0 then
Lχr+1 = σχr + 1 + (1 - σ)χr,q+1
Return: Xir+1
3.3	Differential Privacy Analysis
In each node’s local optimization oracle, Q rounds of mini-batch gradient descents are taken. In
round q, node i samples a minibatch Bit，{bι, b2,..., b∣Bi t∣ ∣∀j, bj ∈ {1,... |D/}}. The SUbsam-
ple follows Poisson sampling method, which is defined as fo,llows.
Definition 3.1. (PoissonS ample). Given a dataset X, the procedure PoissonSample outputs a subset
of the data {xi | σi = 1, i ∈ [n]} by sampling σi 〜Ber(P) independently for i = 1,... ,n.
Definition 3.2. (Gradient Clipping). The clipping operation is defined as
CL(g; C)，------g——「
mM1
max 1 1, r, i
Hence, kg k≤ C.
4	A study on Min-Max Variational B ound
Theorem 4.1.	F(x, λi) is an upper bound on - N PN=I Fi(X)
Proof. Using the second order Taylor approximation of Fi(X0) around X, we have
Fi(X) ≈ Fi(x) + NFi(x) < x0 - x > +2||x0 - XMHi
where Hi = N2Fi(X) is the Hessian matrix. Using the above equation, we have
log I exp(-Fi(x0)- <λi, X0 - X > -Y||x0 - x∣∣2)dx0
x0	2
≈ log / exp(-Fi(X)- < -λi -NFi(X), X - x0 > -2 ||x0 - X∣∣Hi+γi)dX0
= log/ eχp[-Fi(X) + 21∣-% - NFi(X)ll[Hi+γi]-1
(20)
(21)
(22)
(23)
5
Under review as a conference paper at ICLR 2021
-2 ||x - x0 + [Hi + γI]-1[-λi - VFi(x)]T llΗi+γI]dχ0
=-Fi(X) + 2 ll-λi - VFi(χ)ll[Hi+γi]-1 +-2 log π - $ log detlHi + YI|
+ N N(x0; X - [Hi + YI]-1 [VFi(x) + λi]T, [Hi + γI]-1 )dx0
x
=-Fi(X) + 2 ||-λi - VFi(X) ll[Hi+γI]-1 +ɪ log π - $ log det|Hi + YIl
M1
≥ - Fi(X) + -2-log∏ - 2logdet∣Hi + γI|
= - Fi(X) + const
So we have
F(X,λi)
N1
=E N log/,exp(-Fi(x0)- < λi, X0 - X > - Y ||x0 - x||2 )dx0
N1
≥ -E NFi(X)+ const
i=1
(24)
(25)
(26)
(27)
(28)
(29)
(30)
(31)
□
In the above theorem, we find that our federated learning algorithm’s joint min-max objective
F(X, λ) is an upper bound on the averages of loss functions on all nodes. And the saddle point
X?, λi? satisfies the condition that λi + VFi (X) = 0. This condition is in accordance with the opti-
mal primal-dual conditions in augmented Lagrange where the gap between local gradient zeros and
global gradient zeros are closed by the dual parameters λi .
Theorem 4.2.	If PN=I λi = 0, F(x, λi) is an upper bound on log fχ7 exp(N PN=I -Fi(X0)—
γ||x0 - xll2)dx0)
Proof. From Eq. 26, we have
F(x, λi)
1N	1	M	1
≈N E -Fi(X) + 2 llλi + VFi(X) ll[Ηi+γI]-1 +-2 log π - 2 log detlHi + γIl
i=1
From CaUchy-SchWarz inequality, We have
1N	1N
N E"λi + VFi(X) ll[Ηi+γI]-1 ≥ || N £% + VFi(X)ll[H+γI]-1
(32)
(33)
i=1
where H = NN PN=I Hi
i=1
By substituting inequaliy 33 into Eq. 32, we have
F(x, λi)
≥ N X +Fi(X) + 2 ll N X λi 一 VFi(X) ||[H+γI]-1 +-2 log π 一 2 log detlHi + γIl
N	2N	2	2
(34)
i=1
1N
N x
i=1
i=1
11N
2 llN X
i=1
M
[Η+γI]-1 +^2 log π - det|H + YI|
+ detlH + YIl- detlHi +YIl
1N	Y
≈ log	exP( N E-Fi(X) - 2||x - x||2)dx )
x0	N i=1	2
(35)
(36)
6
Under review as a conference paper at ICLR 2021
1N
-N EdetIHi + γI∣+det∣H + γI∣
i=1
1N	γ
= log	exp(N E-Fi(X) - 2||x - x||2)dX) + const
x0	N i=1	2
(37)
(38)
□
In the above theorem, we find that by introducing dual parameters λi , the averages of our local
FL objectives have an upper bound of the same function that the local loss is replace the global
loss N Pi Fi(X). The upper bound is achieved in either of two conditions. The Hessian matrix Hi
of different nodes have the same value. Or the zeros gradient gap among VFi(x) is closed by the
duality parameters λi .
5 Convergence Analysis
In this section, we study the convergence properties of our algorithm. In our analyse, we first see the
whole SG-MCMC Federated learning process as a homogenization ofa stochastic differential equa-
tions(SDE) in the limit of step size variables → 0. Then we use a technique similar as(Eberle et al.,
2019) to analyze the couplings and contraction of two independent randomly initialized stochastic
process. And we derive a exponential bound of convergence of any two process on time in the metric
of our defined Wasserstein Distance.
5.1	Homogenization of SDE systems
Theorem 5.1. Consider of the SDE system given by
dx0(t)	1N	1 二-[x0 -方(Xi +	λi)]dt	(39) Nγ i=1	γ
dλi(t)	-γ1(x0 - xi)dt	(40)
dxi(t)	二一~[VFi(xi) + Y(Xi — xο) + λi]dt + ↑J~^dBt	(41)
It follows that in the limit of → 0, the dynamics of dx0(t) and dλi(t) converges to
dx0(t) =	1N	1 二-[xο - — J2(	XiPi(dxi, xo(t)) + 一λi)]dt	(42) N i=1 xi	γ
dλi(t) =	-γ1(x0 -	xiPi(dxi, x0(t))dt	(43) xi (44)
Proof. The proof follows Sec. 4.1 in(Chaudhari et al., 2018)	□
In above theorem, we would assume our SG-MCMC Federated learning process as a discretization
and homogenization of a stochastic differential equations(SDE).
5.2	Contraction and Coupling rate of SDE
Let probability measures μ(xi(0), xο(0), λi(0)) and μ0(χi(0), x0(0), λi(0)) be any two probabil-
ity measures on the initial distribution of xi,x0,λi. And We denote μpt as the distribution of
μ(xi(t), xο(t),λi(t)) of the process defined in SDE(40, 41, 39) with its initial distribution as μ.
And we have the following theorem on the exponential rate couplings and contractions of two pro-
cess
7
Under review as a conference paper at ICLR 2021
Theorem 5.2. There exists a contant c, and a metric ρ((x0i, x00, λ0i), (xi, x0, λi)) such that for any
t ≥ 0 and any probability measure
Wρ(μpt,μ0pt) ≤ c-ctWρ(μ,μ0)	(45)
Wρ is a Wassertein distance defined on the metric ρ((x0i , x00, λ0i), (xi, x0, λi))
Proof. The proof appears in our Appendix.7.1
□
6 Experiments
In this section, we run simulations on Federated Learning Benchmark in (Shamir et al., 2014; Li
et al., 2018) to verify our algorithms. The data is heterogeneously distributed among devices. We
test our algorithm by comparing it with baseline in both low noise and high noise case. Our result is
shown in Fig.6. The performance of our methods is shown in red line while the baseline method is
in blue line. The high noise case is shown in the lower section. And the low noise case is shown in
the upper section.
6.0. 1 Synthetic Data
In particular, for each device k, we generate data with a generation distribution of y =
argmax(softmax(Wx + b)). We model Wk 〜N(uk,1),bk 〜N(Uk,1),uk ∈ N(0,α),xk 〜
(vk, ∑),vk 〜(0,β + 1).
6.0.2 Low Noise Case
In this case, we inject a tiny noise of β = 10-4 and compares our algorithm with the baseline
where no noise is injected in local FedPD optimizations. Our algorithms have a significantly lower
training loss error and with a much smoother training curve. Because local nodes run SGLD to infer
parameters with lower generalization error bounds.
6.1 High Noise Case
In this case, we inject a significant amount of noise β = 0.5 and compares our algorithm with the
baseline where the same amount of noise is injected in local update without sampling in SGLD.
8
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and
Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security, pp. 308-318, 2016.
Chandrasekaran Anirudh Bhardwaj. Adaptively preconditioned stochastic gradient langevin dynam-
ics. arXiv preprint arXiv:1906.04324, 2019.
Pratik Chaudhari, Adam Oberman, Stanley Osher, Stefano Soatto, and Guillaume Carlier. Deep
relaxation: partial differential equations for optimizing deep neural networks. Research in the
Mathematical Sciences, 5(3):30, 2018.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient
descent into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):
124018, 2019.
Changyou Chen, Wenlin Wang, Yizhe Zhang, Qinliang Su, and Lawrence Carin. A convergence
analysis for a class of practical variance-reduction stochastic gradient mcmc. Science China
Information Sciences, 62(1):12101, 2019.
Jinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. arXiv preprint
arXiv:1905.02383, 2019.
Cynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations
and Trends in Theoretical Computer Science, 9(3-4):211-407, 2014.
Andreas Eberle, Arnaud Guillin, Raphael Zimmer, et al. Couplings and quantitative contraction
rates for langevin dynamics. The Annals of Probability, 47(4):1982-2010, 2019.
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. arXiv
preprint arXiv:1910.06378, 2019.
Jakub Konecny, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed opti-
mization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015.
Bai Li, Changyou Chen, Hao Liu, and Lawrence Carin. On connecting stochastic gradient mcmc and
differential privacy. In The 22nd International Conference on Artificial Intelligence and Statistics,
pp. 557-566. PMLR, 2019a.
Jian Li, Xuanyuan Luo, and Mingda Qiao. On generalization error bounds of noisy gradient methods
for non-convex learning. arXiv preprint arXiv:1902.00621, 2019b.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges,
methods, and future directions. IEEE Signal Processing Magazine, 37(3):50-60, 2020.
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of
fedavg on non-iid data. arXiv preprint arXiv:1907.02189, 2019c.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial In-
telligence and Statistics, pp. 1273-1282. PMLR, 2017.
Felix Sattler, Simon Wiedemann, Klaus-Robert Muller, and Wojciech Samek. Robust and
communication-efficient federated learning from non-iid data. IEEE transactions on neural net-
works and learning systems, 2019.
Ohad Shamir, Nati Srebro, and Tong Zhang. Communication-efficient distributed optimization using
an approximate newton-type method. In International conference on machine learning, pp. 1000-
1008, 2014.
9
Under review as a conference paper at ICLR 2021
Micah J Sheller, Brandon Edwards, G Anthony Reina, Jason Martin, Sarthak Pati, Aikaterini Kotrot-
sou, Mikhail Milchenko, Weilin Xu, Daniel Marcus, Rivka R Colen, et al. Federated learning in
medicine: facilitating multi-institutional collaborations without sharing patient data. Scientific
reports,10(1):1-12, 2020.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. arXiv preprint arXiv:1710.06451, 2017.
Yee Whye Teh, Alexandre H Thiery, and Sebastian J Vollmer. Consistency and fluctuations for
stochastic gradient langevin dynamics. The Journal of Machine Learning Research, 17(1):193-
225, 2016.
Di Wang, Changyou Chen, and Jinhui Xu. Differentially private empirical risk minimization with
non-convex loss functions. In International Conference on Machine Learning, pp. 6526-6535,
2019.
Xinwei Zhang, Mingyi Hong, Sairaj Dhople, Wotao Yin, and Yang Liu. Fedpd: A federated learning
framework with optimal rates and adaptivity to non-iid data. arXiv preprint arXiv:2005.11418,
2020.
7 Appendix
7.1	An Contraction and Coupling Rate Analysis on Convergence
The Fokker-Plank equation of the SDE. (39,41, 40) could be written as
L = 2^ X4χi - ~ χ[vFi(χi) + Y(Xi- XO) + λi] ∙ Vxi
ii
1N 1
-EYI(XO - Xi) ∙ vλi - [x0 - N 工(Xi + γλi)] ∙ VxO	(46)
We consider the following Lyapunov as
AB	C
V(X0, Xi, λi) =	Fi(Xi) + 3 |Xi| +~2 |Xi + ζλi∣ +五 lX0 I	(47)
i
Following the line of the work(Eberle et al., 2019), we make the following assumptions on functions
Fi(X)
Assumption A1.
Fi (X) ≥ 0	(48)
IVFi(X) -VFi(y)∣ ≤ L∣x - y∣	(49)
X ∙VFi(X)∕2 ≥ K(Fi(X) + z∣x∣2∕4) - F	(50)
∣Fi(X)∣≤ G	(51)
Then we have the following lemma
Lemma 7.1. Ifthe above assumption holds, then LV ≤ ɪ(Me(A + B + L) + DG + (A + B)F 一
κ(A+B+Y)V)
Proof. By applying Fokker-Plank Eq. 46, we have
10
Under review as a conference paper at ICLR 2021
LFi(xi)
β 4χi Fi(Xi)- 1[∣VFi(xi)∣2+γVFi(Xi) ∙ Xi- YVFi(Xi) ∙ Xo + VFi(Xi) ∙ λi]


(52)
L 2 |xi|2
L 2 |xo|2
L2 |Xi + ζλil2
Me
-1[VFi(xi) ∙ Xi + γ∣χ∕2-γXi ∙ χo + Xi ∙ λi]
(53)
21N	1
=-[|xo | - N 工(Xi ∙ xo + γλi ∙ X0)]
=---------[VFi(Xi) ∙ Xi + Y∣Xi∣2-YXi ∙ Xo + Xi ∙ λi
+	ZVFi(Xi) ∙	λi	+ ZYXi ∙ λi - ζYλi ∙	x0 +	Z∣λi∣2]
-YιZ(xo ∙ Xi	-	∣Xi∣2) - YiZ2(xo ∙ λi	- Xi	∙ λi)
(54)
(55)
(56)
As
4χiFi(Xi) ≤ L,	∣Fi(xi)∣≤ G2, Xi ∙ VFi(Xi)/2 ≥ K(Fi(Xi) + z∣Xi∣2∕4) - F	(57)
Then we have
AB	C
L(X Fi(Xi) + 2 |Xi| +2 |Xi + ζλil +22 |X0| ))
i
≤ ∣M((B + L)∕β) + DG +(A + B)F]
(58)
—
X 1 { (A + Y； B)ZK + [(A + B)Y- BYIeZ]|Xi|2+BZlλi|2+N|Xo|2+(1 + D)VFi(Xi)2
i
-[(A + B)Y - BγιZe + C]Xi ∙ Xo - [BZγ + -C - Bγι<2]xo ∙ λi
N	YN
+ [A + B + BZY - ByiZ2e]xi ∙ λi + VFi(Xi) ∙ [(1 + BZ)λi - YXo]}	(59)
By choosing the proper values of A, B, C, D, ζ, z, Y1 , we could let the following equality holds
LV ≤ 1(Mβ(A + B + L) + DG + (A + B)F - κ(A + B + γ)V)
e
Here is one set of A, B, C, D, Z, z, Y1 satisfying the above inequality.
A = B = Y, C = WY 2N,	D = 7κY2,	Z =-
3	4Y
where K, Y, Y1 , zsatisfying the following constraints
(60)
(61)
γ1
3κzY
4
κ
4
Y
3κ
^2^
1	2
KY
6
77
——N
150
2	+ 6κγ + 2 κγ4
82
27 [2Y - 2Yld
25
(62)
≤
≤
≥
—
≥
□
7.1.1 Couplings of two Process
Let Xt = [X1T (t), X2T (t) . . . XTN (t)], λt = [λ1T (t), λ2T (t) . . . λTN (t)]. We consider two coupling pro-
cess Xt, λt, Xo (t) and X0t, λ0t, X0o (t) with different initialization. We compose their brownian mo-
tions in the direction of synchronized drift and reflection drift which we would give conditions.
11
Under review as a conference paper at ICLR 2021
Each is governed by the following SDE
dXt = — 1[VFi(Xt)dt + YXtdt — γx0(t)1T dt + λtdt]
+ √β∕2Mc(Zt, Wt, Ytt)dBrc + √β花Sc(Zt, Wt, WdBsc
dxo(t) = —[xo(t)dt —而 i(Xt +—λt)dt]
Nγ
dλt = — [x0(t)idt — Xtdt]
dXt = — IVFi(Xt)dt + YXtdt — γx0(t)1T dt + λ'tdt]
+ pβ∕2Mc(Zt, Wt, Yt)(I — 2eteT)dB，+ √β∕^sc(Zt, Wt,Yt)dBsc
dx0o(t) = —[x0(t)dt — ∙^i(Xt +	λt)dt]
NY
dλt = —[x0(t)iτ dt — Xt dt]
,where 1 is defined as
1	11	(m — 1)M + 1 ≤ n ≤ mM
1m,n = 0	else
(63)
(64)
(65)
The existence and uniqueness of decomposition holds by Levy’s characterization. Then we write the
differentiation of the two process as Zt = Xt — X0t, Wt = x0 (t) — x00 (t), Yt = λt — λ0t. Moreover,
we define we define rc, Sc : R → [0, 1] are Lipschitz continuous functions such that rc2 + Sc2 = 1
as a function of Zt , Wt and Yt
rc = 0	if	|Wt|=0, |Yt|=0 or ∣Zt∣+α1∣Wt∣+α2∣Yt∣≥ Ri	+ ξ	(66)
rc = 1	if	α1∣Wt∣+α2∣Yt∣≥ ξ and ∣zt∣ + αι∣wt∣+α2∣γt∣≤	RI	(67)
We also define et as an unit length vector in the direction of Zt and et shrinks at	|Zt|= 0
et	=Zt/|Zt| if Zt 6=0 and et =0 if Zt =0	(68)
The process of (Zt, Wt, Yt) could be written as
dZt = —[£ VFi(Xi(t)) — Fi(Xi(t)) + γZtdt — YWtiTdt + Ytdt]
+ √2β/Tc(Zt, Wt,Yt)dBrc	(69)
dWt = — [Wtdt — ±i(Zt + 1 Yt)]	(70)
NY
dYt = —[Wt — ZtiT]	(71)
The derivative of |Wt| and |Yt| could be write in the form of
d|Wt| = IWI TWt- Nni(Zt + 1 W]	(72)
dt |Yt| = ]⅜TWtiT- Zt]	(73)
We set
rt=r((Xt,x0(t),λt),(X0t,x00(t),λ0t))= IZtI+α1IWtI+α2IYtI	(74)
ρt=ρ((Xt,x0(t),λt),(X0t,x00(t),λ0t)) =f(rt)Gt	(75)
Gt = 1+νV(x00,x0i,λ0i)+νV(x00,x0i,λ0i)	(76)
Then we have the following lemmas
Lemma 7.2. There exists a R1 if rt ≥ R1 such that
LV(x0, xi, λi) + LV(xo, xi, λi) ≤ — K(A +B + Y) (V(x0, xi, λi) + V(xo, xi, λi))	(77)
12
Under review as a conference paper at ICLR 2021
,where Ri is give by
RI ≤ [^(A(1 + 02)2 +102^ + ^701 )(Mβ(A + B + L) + DG +(A + B)F)/(K(A + B + Y))]1/2
5 A Z BZ C
(78)
Proof. We have
r = ∣Zt∣+α1∣Wt∣+α2∣匕 |
= IXt- Xt∣+αι∣χo(t) - χ0(t)1∣+α2∣λt - λt∣
≤ ∣χt∣+αι∣xo(t)∣+α2∣λt∣+∣χt∣+αι∣χ0(t)∣+α2∣λt∣
≤ ∣χt∣+αι∣χo(t)∣+α2(Z)(IXt∣+∣χt+ ζλt∣) + ∣χt∣+αι∣χ0⑴∣+α2(Z)(IXt∣+∣χt+ ζλt∣)
≤ (I + -ζ22XxJ + -2Γ |χt +c"1+a^Xo^X + q + -2r-修〃 + ^ζ2|Xt + ζλtH+α11x0⑴1 (79)
In Cauchy-Swartz inequality, we have
∑[ A M2+B 氏 + Od2+C ∣χ0∣][ 2(1 + 与)2 + ⅛ + 2pι ]
2	2	2t	ʃɪ Z	JBZ	C
≥ [(1 + -2Γ )χt1 + -2T |χt + ¢"1+01^0(%)1]2	(80)
So we have
rt ≤ 匕(I + 72)2 + -ξ⅛ +	7τ1 ](V(X0) xi, λi) + V(X0,χi, %))	(81)
t A Z	BZ2	C
So we have
V (χ0, χi,λi) + V (χ0, χi,%))
12	..............
≥ -^(Mβ(A + B + L) + DG + (A + B)F)/(κ(A + B + Y))	(82)
5
And thus
LV(χ0, χi,λi) + LV(χ0, χi,λi) ≤ - K(A + B + Y) (V(X0, χi,λi) + V(χ0, χi,λi))	(83)
O€
□
Lemma 7.3. Let c, V and suppose that f: [0, ∞) → [0, ∞) is continuous, non-decreasting, concave
and C2 except for finitely many points. The we have
ectpt ≤ P0 +
∕t
0
ecsKsds + Mt
(84)
where Mt is a local continuous martingale, and Kt could be written as
Kt = Cf(Tt)Gt + (-L√N----Y + Yia2 +—7= XZJ+K—Y + Yi√N - aiXW/
E	E	√N	E
+ (αi + 1)∣K∣)f- (rt))Gt + βrc(Zt,Wt,Yt)2f "(rt)Gt
YE	E
+ Vf (rt)(-(Mβ(B + L)) + DG +(A + B)F - κ(1 + Y)V - κ(1 + Y)V0)))
EE
+ νβ∕E max(L + A + B, BZ∕α2)rtf-(r∕rc(Zt, Wt, Yty
(85)
Proof. We apply Ito,s formula on the process of ∣Zt∣
∣Zt∣= ∣Z0∣+AZ + MQ
(86)
13
Under review as a conference paper at ICLR 2021
where (AQ) and (MMQ) is absolute continuous process and martingale given by
1t
AQ = - - e ∙ (£(V Fi(Xi)- VFi(Xi)) + YZt- YWt + YO
0 i
MQ = PeZZOt rc(Zt,Wt,Yt)eTdB1°
(87)
(88)
Because δ∣∕∣z∣ |z|= 0, there is no Ito's correlation. By the LiPschitz continuous on , We could have
AQ ≤— Z XLllXi- Xill-YlZt∣+γ∣Yt∣+∣WtIdt
0i
=-[(L√N - Y)lZtl+YlWtl + lYtl
0
And then we write the semimartingale decompostion of rt
rt = lQ0l+α1lWtl+α2lYtl
Similarly, we have the following bound on dlWtl and dlYtl
d lWtl≤τWtl+√N (lZtl+-l Yo
dlYtl ≤ Y1[√NlWtl+lZtl]
(89)
(90)
(91)
(92)
(93)
Since by assumption, f is concave and C2, we can now apply Ito-Tanaka formula to f(rt). Let f0
and f00 denote the left-sided first derivative and almost everywhere defined second order derivative.
We obtain the following semimartingale decompostion bound on ectf(rt)
ectf (rt) = f(ro) + At + Mt
with the martingale part
Mt = PWɪ J： ecsf-(rs)rc(Zt,Wt,Yt)ef dB；c
(94)
(95)
and a continuous finite-variation process (At) is bounded by
dAt ≤ (cf(rt) + (-L√N 一 -Y + Y1a2 + α1= )∣Zt∣+((-γ + γ1α2√N 一 ɑι)∣Wt∣	(96)
N
+ (α1 + 1)l YWf- (r» / dt + βrc(Zt,Wt,Yt)2f0(rt)* dt
(97)
γ

Now we bound on the integration of the process’s evolution on time Gt = - + νV(X0, Xi, λi) +
νV(X00, X0i, λ0i), by applying Ito’s formula we have
dGt = ν(LV)(X0, Xi, λi)dt + ν(LV)(X00, X0i, λ0i)dt
+ν√β∕2l(Vχi V(X0, Xi, λi) - Vχi V(x0, xi, λiXeteTTcZt Wt, YtpdBtc
+νPβ72^(Vχi V(x0, Xi, λi) + Vxi V(x0, Xi, λi))(I - eteTT)rc(Zt, Wt, Yt)2dBtc
+νPβ72^(Vχi V(X0, Xi, λi) + Vχi V(x0, xi, λi))Sc(Zt, Wt, Yt)2dBtc
Hence by Ito’s formula, we obtain the following semi-martingale decomposition
ectρt = ectf(rt)Gt = ρ0 + Mt + At
where (Mt ) is a continuous local martingale, and
~	.J.	,	...,	.	.	.....
dAt = GtdAt + νe tf (rt)((LV)(xo, Xi, λi)dt + (LV)(x0, Xi, λi))dt
+ ectνβ/Ef-(rt)rc(Zt, Wt, Yt)2(V(xo, Xi, λi) - VV(x0, xi, λi))dt
(98)
(99)
(100)
14
Under review as a conference paper at ICLR 2021
Now recall that by Lemma 7.1, we have
LV ≤ 1(βM(A + B + L) + DG +(A + B)F - K(A + B + Y)V)	(101)
Furthermore, ∣Vχθ V(x0, xi, λi)∣ is bounded by
|VxiV(x0, xi, λi) - VVxi(X0, Xi, λi)∣= ∣X(VFi(Xi)- VFi(Xi) + (A + B)(Xi- Xi) + BZ(% - λi)∣
i
≤ (L + A + B)IZtl+Bζ∣Wt∣
≤ max(L + A + B, BZ∕α2)rt	(102)
By combining , we finally obtain dAt ≤ ectKtdt, where
Kt = CfgGt + (1 L√N - 1Y + Y1α2 + α1= )∣Zt∣+((1 γ + γ1α2√N - αι)∣Wt∣
N
+ (αi + 1)∣Yt∣)f- (Ttr)Gt + βrc(Zt, WtMYf0(Tt)Gt
Y
+ ∕f(rt)(2(Me(A + B + L))+ DG +(A + B)F - κ(A + B + Y)V - K(A + B + Y)V0)))
+ νβ∕e max(L + A + B, BZ∕α2)rtf-(rt)rc(Zt, Wt, Yt)2	(103)
□
Lemma 7.4. By choosing the following ν and f (r), the continuous evolving process Kt vanishes
as ξ → 0
f(r) =
0
r∧R1
夕(s)g(s)ds
(104)
小 S) = exp(-C2s2),
(105)
g(r) = 1 - C2 / φ(s)夕(S)-1dr,	with φ(s) = / 夕(x)dx
(106)
Ci = Vmax(L + A + B, BZ∕α2) + max(γ + γιɑ2√Ne —Eαι)∕βαι, (——+ l)/ea2)
Y
(107)
_ 9c€
C2 =方
4c=ν(Mβ(A+B+L)+DG+ (A + B)F)
(108)
(109)
(110)
and we assume that
C4 = -^L√N — LY + γ1α2 + √= < 0
(111)
Proof. To bound Kt, we consider different region to achieve up to an error term which vanishes as
ξ→0
(i) α1IZtI+α2IWtI≥ ξ and rt ≤ R1
Here we have rc(Zt, Wt, Yt) = 1. Therefore, since Gt ≥ 1, IWtI≥ 0, IZtI≥
0 and IYt I≥ 0. We have
β1
Kt ≤ —f (Tt)Gt + (Ve maχ(L + A + B, BZ/a2)
+ maχ(Y + Yiα2√Ne - ∈αi)αi, (— + 1)a2))rtGtf- (rt) + 9Cf(Tt)Gt	(112)
Y-
Then we have
'20(rt)+—(Vemax(L+A+B, BZ∕ɑ2)+max(Y+Y1 α2√Ne-eaι)αι, (---hl)α2))rt2(rt) = 0
Y
15
Under review as a conference paper at ICLR 2021
Hense we have
Kt ≤ 9c( rt 22(s)g(s)Gtds - L
rt
ψ(s)Gt)
(113)
In order to ensure g(r) ≥ 1/2 for r < R1, we have to assume
R1
C ≤ 2β∕(9e /	φ(s)夕(s)-1 ds)
0
(114)
(ii)	α1∖Zt∖+α2∖Wt∖< ξ and r ≤ Ri
With the same Choise of f and g ≥ ɪ, similarly We derive a bound on Kt as
Kt ≤ (— f00(rt)Gt + Vmmax(L + A + B,BZ/α2∖rclZt, Wt, Yt)2
+ (1 L√N - 1Y + γ1α2 + -α1= )rtf0(rt) + 9cf(rt)Gt + C3ξf(rt)Gt
N
Where the constant C3 is given by
(115)
C3 = max(正+1γ-για2-等+ɪ+Ya^-1,正+1 ]一“一筌 + 2+ɪ))
N α1 α1	N γα2	α2
(116)
In order to ensure that the upper bound converges to 0 as ξ → 0, We assume
C ≤ 118( -1 L√N+lγ - γ1a2- √aN) r∈in,Rι] *
(117)
(iii)	rt ≥ R1. Here f-0 (rt) =0.
Let。5 = (M (A + B + L) + DG∕β + (A + B)F∕β)
Hence We have
ν β	C
Kt = ~^[ [2C5 + Ve - (K(A + B + Y)-α/⑶(V(X0, xi, λi) + V(XO, xi, λi))]f (rt)
9	15
≤ [4 C5 - 16(K(A + B + Y)-CE/e)(V (x0, Xi, λi) + V (x0, xi, λi))]f (rt)
≤ 0	(118)
provided We assume
c ≤ βκA + B + Y)
16
(119)
We finally introduce Wassertein distance on our defined metric ρ on the probability space of our tWo
distributions of μ(Xt, xo(t), λt) and μ0(Xt, xO(t), λt)
Definition 7.1. For probability measures μ(Xt, xo(t),λt) and μ0(Xt, xO(t),λt) on R2M, we define
WP(μ, μ0) = inf	ρ((X, Xo, λ), (X0, x0, λ0))Γ(d(X, Xo, λ), d(X0, x0, λ0))	(120)
Γ∈Π(μ,μ0)
,where Γ isa coupling of μ and μ0, our defined Wassertein distance is taking the infimum of P metrics
over all couplings.
Then we conclude on our final theorem
Theorem 7.5. For a positive constant c such that
C ≤ min(2β∕(9e ZRl。⑶以旷也)，工(-1 L√N +1 γ-γg-筌)inf * 皿 + B + Y)
≤	( e/( Jo	)	), 18(E + J Y1 2 √N) r∈(0,Rι ] φ(r) ,	16e
(121)
Moreover, let f : [0, ∞, → [0, ∞) be defined above. Then for any t ≥ 0 and for any probabilty
measure μ, μ0 on R2M,
Wρ(μpt,μ0pt) ≤ e-ctWρ(μ,μ0)	(122)
□
)
16
Under review as a conference paper at ICLR 2021
Proof. Let Γ be a coupling of two probability measures μ and μ0 such that Wρ(μpt, μ0pt) < ∞.
We consider two coupling process (X, x0, λ), (X0, x00, λ0)satisfying the initial optimal couplings
(X, x0 , λ), (X0 , x00 , λ0 ) ∈ Γ. in each of the cases conditions considered above, we obtain Kt ≤
C3ξGt . Therefore we apply lemma7.3 and taking expectations, we have
E[ρt] ≤ e-ctE[ρ0] + C3ξ
Zt
0
ec(s-t)E[Gs]ds
(123)
Note that E[Gs] is finite. So at the limit ofξ → 0, we have
E[ρt] ≤ e-ctE[ρ0]	(124)
Since (Xt, xo(t), λj (X；, x0(t), λt) is a coupling of μpt and μ0pt, we have Wp(μpt, μ0pt) ≤ E[ρt].
As the initial optimal couplings conditions, we have
E[ρo] = / Pdr = Wp(μ,μ0)	(125)
So we conclude
Wp(μpt,μ0pt) ≤ e-ctWp(μ,μ0)	(126)
□
17