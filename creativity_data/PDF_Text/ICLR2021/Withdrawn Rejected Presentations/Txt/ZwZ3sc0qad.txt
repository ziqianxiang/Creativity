Under review as a conference paper at ICLR 2021
On Alignment in Deep Linear Neural Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
We study the properties of alignment, a form of implicit regularization, in linear
neural networks under gradient descent. We define alignment for fully connected
networks with multidimensional outputs and show that it is a natural extension of
alignment in networks with 1-dimensional outputs as defined by Ji and Telgarsky,
2018. While in fully connected networks, there always exists a global minimum
corresponding to an aligned solution, we analyze alignment as it relates to the
training process. Namely, we characterize when alignment is an invariant of
training under gradient descent by providing necessary and sufficient conditions for
this invariant to hold. In such settings, the dynamics of gradient descent simplify,
thereby allowing us to provide an explicit learning rate under which the network
converges linearly to a global minimum. We then analyze networks with layer
constraints such as convolutional networks. In this setting, we prove that gradient
descent is equivalent to projected gradient descent, and that alignment is impossible
with sufficiently large datasets.
1	Introduction
Although overparameterized deep networks can interpolate randomly labeled training data (Du et al.,
2019; Wu et al., 2019), training overparameterized networks with modern optimizers often leads to
solutions that generalize well. This suggests that there is a form of implicit regularization occurring
through training (Zhang et al., 2017).
As an example of implicit regularization, the authors in Ji & Telgarsky (2018) proved that the
layers of linear neural networks used for binary classification on linearly separable datasets become
aligned in the limit of training. That is, for a linear network parameterized by the matrix product
WdWd-1 . . . W1, the top left/right singular vectors ui and vi of layer Wi satisfy |viT+1ui| → 1 as the
number of gradient descent steps goes to infinity.
Alignment of singular vector spaces between adjacent layers allows for the network representation to
be drastically simplified (see Equation 3); namely, the product of all layers becomes a product of
diagonal matrices with the exception of the outermost unitary matrices. If alignment is an invariant
of training, then optimization over the set of weight matrices reduces to optimization over the set of
singular values of weight matrices. Thus, importantly, alignment of singular vector spaces allows
for the gradient descent update rule to be simplified significantly, which was used in Ji & Telgarsky
(2018) to show convergence to a max-margin solution.
In this work, we generalize the definition of alignment to the multidimensional setting. We study
when alignment can occur and moreover, under which conditions it is an invariant of training in linear
neural networks under gradient descent. Prior works (Gidel et al., 2019; Saxe et al., 2014; 2019) have
implicitly relied on invariance of alignment as an assumption on initialization to simplify training
dynamics for 2 layer networks. In this work, we provide necessary and sufficient conditions for when
alignment is an invariant for networks of arbitrary depth. Our main contributions are as follows:
1.	We extend the definition of alignment from the 1-dimensional classification setting to the
multi-dimensional setting (Definition 2) and characterize when alignment is an invariant of
training in linear fully connected networks with multi-dimensional outputs (Theorem 1).
2.	We demonstrate that alignment is an invariant for fully connected networks with multidimen-
sional outputs only in special problem classes including autoencoding, matrix factorization
1
Under review as a conference paper at ICLR 2021
and matrix sensing. This is in contrast to networks with 1-dimensional outputs, where there
exists an initialization such that adjacent layers remain aligned throughout training under
any real-valued loss function and any training dataset.
3.	Alignment largely simplifies the analysis of training linear networks: We provide an explicit
learning rate under which gradient descent converges linearly to a global minimum under
alignment in the squared loss setting (Proposition 1).
4.	We prove that alignment cannot occur, let alone be invariant, in networks with constrained
layer structure (such as convolutional networks), when the amount of training data dominates
the dimension of the layer structure (Theorem 3).
5.	We support our theoretical findings via experiments in Section 6.
As a consequence, our characterization of the invariance properties of alignment provides settings
under which the gradient descent dynamics can be simplified and the implicit regularization prop-
erties can be fully understood, yet also shows that further results are required to explain implicit
regularization in linear neural networks more generally.
2	Related Work
Implicit regularization in overparameterized networks has become a subject of significant interest
(Gunasekar et al., 2018a;b; Martin & Mahoney, 2018; Neyshabur et al., 2014). In order to characterize
the specific form of implicit regularization, several works have focused on analyzing deep linear
networks (Arora et al., 2019b; Gunasekar et al., 2018b; 2017; Soudry et al., 2018). Even though such
networks can only express linear maps, parameter optimization in linear networks is non-convex and
is studied in order to obtain intuition about optimization of deep networks more generally.
One such form of implicit regularization is alignment, identified by Ji & Telgarsky (2018) to analyze
linear fully connected networks with 1-dimensional outputs trained on linearly separable data. They
proved that in the limit of training, each layer, after normalization, approaches a rank 1 matrix, i.e.
Wi(t)	T
lim ——乐—=Uivi
t→∞ kWi(t)kF	i
and that adjacent layers, Wi+1 and Wi become aligned, i.e. |viT+1ui | → 1.
In addition, Ji & Telgarsky (2018) proved that alignment in this setting occurs concurrently with
convergence to the max-margin solution. Follow-up work mainly focused on this convergence
phenomenon and gave explicit convergence rates for overparameterized networks trained with
gradient descent (Arora et al., 2019c; Zou et al., 2018).
Our definition of invariance of alignment extends assumptions on initialization appearing in various
prior works (Gidel et al., 2019; Saxe et al., 2014; 2019). While the connection to alignment was
not mentioned in their work, the authors in Gidel et al. (2019) begin to generalize alignment to
multidimensional outputs by considering two-layer networks initialized so that layers are aligned
with each other and to the data. We generalize this to networks of any depth, showing that our
definition of alignment corresponds to the initialization considered in Gidel et al. (2019). Moreover,
we establish necessary and sufficient conditions for when alignment is an invariant of training in
Theorem 1 instead of assuming these conditions. Furthermore, their result on sequential learning of
components can be derived via our singular value update rule in Corollary 1.
Balancedness is another closely related form of implicit regularization in linear neural networks. It
was introduced in Arora et al. (2018) and defined as the property that if WiT Wi = Wi+1WiT+1 for
all i at initialization, then this property is invariant under gradient flow. Du et al. (2018) present a
more general form, that WiT Wi - Wi+1WiT+1 is constant under gradient flow. In practice, analyses
rely on this quantity being close to or exactly zero. In this exact setting, balancedness indeed implies
alignment of singular vector spaces between consecutive layers. To study gradient descent, slightly
more general notions such as approximate balancedness (Arora et al., 2019a) and -balancedness have
been introduced. Du et al. (2018) also defined balancedness with respect to convolutional networks,
showing that under gradient flow, the difference in the norm of the weights of consecutive layers is an
invariant. Generally, the goal of identifying invariants of training such as balancedness or alignment
is to help understand both the dynamics of training and properties of solutions at the end of training.
2
Under review as a conference paper at ICLR 2021
3	Definition of Alignment in the Multi-dimensional Setting
In this section, we first define alignment for linear neural networks with multi-dimensional outputs.
We then define when alignment is an invariant of training.
We consider linear neural networks. Let f : Rk0 → Rkd denote such a d-layer network, i.e.
f(x) = WdWd-1 . . . W1x,	(1)
where Wi ∈ Rki×ki-1 for i ∈ [d], where we follow the convention that [d] = {1, 2, . . . d}. Let
(X, Y ) ∈ Rk0×n × Rkd ×n denote the set of training data pairs {(x(i), y(i))} for i ∈ [n]. Gradient
descent with learning rate γ is used to find a solution to the following optimization problem:
1n
argmin ʒ- E'(f(xCi)),y(i)),	(2)
f∈F 2n
i=1
where F is the set of linear functions represented by f and ` is a real-valued loss function. When not
stated otherwise, we assume `(f (x(i)), y(i)) = ky(i) - f(x(i))k22, which is the squared loss (MSE).
In addition, we denote by Wi(t) for t ∈ Z≥0 the weight matrix Wi after t steps of gradient descent.
When there are no additional constraints on the matrices Wi , then f is a fully connected network.
We next introduce a generalized form of the singular value decomposition:
Definition 1. An unsorted, signed singular value decomposition (usSVD) of a matrix A ∈ Rm×n
is a triple U ∈ Rm×m , Σ ∈ Rm×n , V ∈ Rn×n such that U, V are orthonormal matrices, Σ is
diagonal, and A = UΣV T.
In contrast to the usual definition of singular value decomposition (SVD) of a matrix, the diagonal
entries of Σ may be in any order and take negative values. Throughout, we will refer to the entries of
Σ in a usSVD as singular values and the vectors in U, V as singular vectors. Using the usSVD, we
now generalize the notion of alignment from Ji & Telgarsky (2018) to the multi-dimensional setting.
Definition 2. Let f = WdWd-1 . . . W1 be a linear network. We say that f is aligned if there exists
a usSVD Wi = Ui Σi ViT with Ui = Vi+1 for all i ∈ [d - 1]. (We also say that a matrix A is aligned
with another matrix B if there exist usSVD’s A = UAΣAVAT, B = UB ΣB VBT such that VA = UB.)
Note that if Wi and Wi+1 are rank 1 matrices in an aligned network f, then the inner product of the
first columns of Vi+1 and Ui is 1 in absolute value. Hence Definition 2 is consistent with alignment
in the 1-dimensional setting from Ji & Telgarsky (2018).
We next define when alignment is an invariant of training for deep linear networks. Again, such
invariants are of interest since they may provide insights into properties of trained networks and
significantly simplify the dynamics of gradient descent.
Definition 3. Alignment is an invariant of training for a linear neural network f if there exists an
initialization {Wj(0)}jd=1 such that W1(∞), W2(∞), . . . , Wd(∞) achieves zero training error 1 and for
all gradient descent steps t ∈ Z≥0
(a)	the network f is aligned;
(b)	Wi(t) = Ui Σi(t)ViT for all i ∈ {2, . . . d - 1}, that is, Ui , Vi are not updated;
(c)	W1(t) = U1Σ(1t)V1(t) and Wd(t) = Ud(t)Σ(dt)VdT, that is, U1 and Vd are not updated.
If additionally, V1 and Ud are not updated for any t ∈ Z≥0, then we say that strong alignment is an
invariant of training.
When alignment is an invariant of training, there are important consequences for training. In particular,
note that when the network f is aligned with usSVDs Wi = Ui Σi ViT for all 1 ≤ i ≤ d, then
f(x) = Wd …Wix = Ud(Y ∑d-) Vf x.	(3)
1The interpolation condition in this definition (i.e., achieving zero training error) is important in ruling out
several architectures where the layers are trivially aligned. For example, if all layers are constrained to be
diagonal matrices throughout training, then the layers are all trivially aligned, but cannot interpolate datasets
where the target is not the product of a diagonal matrix with the input.
3
Under review as a conference paper at ICLR 2021
Hence if alignment is an invariant of training, then the singular vectors of layers 2 through d - 1 are
never updated and the analysis of gradient descent can be limited to the singular values of the layers
and the matrices V1 and Ud .
Remarks. For the remainder of the paper, we assume that the gradient of the loss function at
initialization {Wi(0)}id=1 is non-zero. Otherwise, training with gradient descent would not proceed.
We also only consider datasets (X, Y ) for which there is a linear network that achieves loss zero.
This is consistent with the assumptions in Ji & Telgarsky (2018).
4	Alignment in Fully Connected Networks
In this section, we first characterize when alignment is an invariant of training for fully connected
networks (Theorem 1). In particular, we show that this is not the case in general. We then present
special classes of problems for which alignment is an invariant of training, namely autoencoding,
matrix factorization, and matrix sensing. In contrast, for a linear network with 1-dimensional outputs,
we demonstrate that there exists an initialization for which the layers remain aligned throughout
training given any dataset and any real-valued loss function. Finally, we discuss various consequences
of alignment, including a proof of linear convergence of gradient descent to an interpolating solution.
4.1	Characterization of Alignment with Multi-dimensional Outputs
Theorem 1 is one of our main results and characterizes when alignment is an invariant of training in
a fully connected network with multi-dimensional outputs. To simplify notation, we consider the
case when the layers are square matrices, i.e. ki = kj for all 0 ≤ i, j ≤ d. The general result for
non-square matrices is provided in Appendix D.
Theorem 1. Let f : Rk → Rk be a linear fully connected network with d ≥ 3 square layers of
size k > 1. Alignment is an invariant of training under the squared loss on a dataset (X, Y ) ∈
Rk×n × Rk×n if and only if there exist orthonormal matrices U, V ∈ Rk×k such that UTYXTV
and V TXXT V are diagonal.
The full proof of this result is presented in Appendices A-E; here, we provide a proof sketch.
Proof Sketch. The proof essentially follows by induction. For the base case, we initialize the layers
{Wi }id=1 to satisfy the conditions for alignment given in Definition 3. Assuming that these conditions
hold at gradient descent step t, we prove that they hold at step t + 1.
After substituting the alignment conditions into the gradient descent update equation for the squared
loss at step t+ 1 and cancelling terms, we obtain that alignment is an invariant of training if and only if
Tn
Ud(t) X(y(k) - f (x(k)))x(k)T V1(t)	(4)
k=1
is a diagonal matrix. By considering the update for W1(t) and Wd(t), one sees that alignment implies
strong alignment and so Ud , V1 are also invariant across updates. Thus, let Ud = U and V1 = V . By
expanding f (x(k)) using equation 3, and considering the update across multiple timesteps, we obtain
that the matrix in equation 4 is diagonal if and only if UTYXTV and V TXXTV are diagonal. To
complete the proof, we show in Appendix D that under strong alignment, gradient descent converges
to a solution with zero training error.	□
Theorem 1 implies that invariance of alignment throughout training holds only for special classes of
problems. In particular, the above implies that alignment is an invariant of training when X and Y
have the same right singular vectors, a very special condition on the data. Note that this corresponds
to the = 0 data condition with the initialization considered in Gidel et al. (2019). In Section 6, we
also provide empirical support showing that alignment is not an invariant of training for important
tasks that violate the data condition presented here, such as multi-class classification.
4
Under review as a conference paper at ICLR 2021
4.2	Classes of Problems with Alignment
We next discuss classes of problems for which alignment is an invariant of training.
Autoencoding: In the case when X = Y , it holds that UTY XTV = UTXXTV . Taking
U = V to be the left singular vectors of X satisfies the conditions of Theorem 1.
Matrix Factorization and Inversion: In the case of matrix factorization, we have that
X = I. Hence taking U and V to be the left and right singular vectors of Y respectively satisfies the
conditions of Theorem 1. For matrix inversion, we have that Y = I and we proceed analogously.
Matrix Sensing. Given pairs of observations {(Mi, yiU With Mi ∈ Rk×k and y% = Tr(MTX*)
for some unobserved matrix X* ∈ Rk×k, gradient descent on {Wi}n=ι is used to solve
1n
argmin ʒ-El∣yi -Tr(Mi WdWd-I... WI)k2.
{Wi} 2n
i=1
Implicit regularization in the matrix sensing setting has been analyzed extensively (Arora et al.,
2019b; Du et al., 2018; Gunasekar et al., 2017; Li et al., 2018). Theorem 1 shoWs that alignment is an
invariant of training in this setting if and only if Mi = UΛiV T for all i ∈ [n], and Ud = U, V1 = V .
1-dimensional Outputs. In Appendix F, We shoW that alignment is an invariant of training
for fully connected netWorks With 1-dimensional outputs for any real-valued loss function provided
that gradient descent converges to zero training error.
4.3	Consequences of Alignment
We next discuss various consequences of the invariance of alignment for the analysis of training. Our
explicit characterization of alignment as an invariant is significant as it alloWs us to greatly simplify
the convergence analysis of gradient descent, Which is a main goal of defining an invariant of training.
The folloWing corollary (proof in Appendix B) folloWs from the proof of Theorem 1, and shoWs that
under alignment the gradient descent update rule is simplified significantly.
Corollary 1. Let r = min(k0, k1, . . . , kd) > 1 and let the top left r × r submatrix of U TYXTV be
Λ0 and that of V TXX TV be Λ. Under the invariance of strong alignment (i.e., when Λ0 and Λ are
diagonal), we can express the partial derivative with respect to Wi as follows:
d	i-1	∖
Y ∑jT(UtYXtV1 - Σd …∑ιVTXXtV) Y Σjτ I ViT	(5)
j=i+1	j=1
As a result, gradient descent only updates the first r values of Σi. Let Σ0i(t) be the top left r × r matrix
of Σi(t). The updates are then given by:
dd
Σ0(t+1) = Σi(t) + Y Y ∑0jt)(Λ0 - Y Σ0jt)Λ).	(6)
n j=1	j=1
The other entries of Σi(t+1) are not updated.
We can use this corollary to provide an explicit learning rate under Which gradient descent converges
linearly to a global minimum. The proof of the folloWing proposition is given in Appendix C.
Proposition 1. For k ∈ [r], let σk(Wi) denote the kth entry of Σi in the usSVD of Wi, and let
λk, λ0k denote the kth entries of Λ, Λ0 respectively. Under the conditions of Corollary 1 and
assuming that σk(W(0)) > 0 and Q σk (W(0)) < λλk for all k ∈ [r], if the learning rate satisfies
i=1	k
Y ≤ nd2 ∙ mink σ (Wλ02)λk then gradient descent only updates the top r singular values of the
solution and converges linearly to the global minimum.
5
Under review as a conference paper at ICLR 2021
Remarks. Shamir (2018) shows that for linear neural networks with one-dimensional outputs, the
rate of convergence can be exponentially slow in the depth. Proposition 1 shows that for a fixed depth,
gradient descent converges linearly. Our analysis in Appendix C shows that our upper bound on the
rate of convergence can also grow exponentially in the depth.
Alignment in the Limit of Training. We briefly comment on understanding whether alignment will
occur in the limit of training. The following proposition, which states that for a 2-layer network, an
aligned solution achieves the minimum `2 -norm. The proof is given in Appendix G.
Proposition 2. Let W1, W2 be matrices such that W2W1 = P, for a fixed matrix P. Then, kW1 k2F +
kW2 k2F achieves a minimum at the solution where W1 and W2 are aligned and 0-balanced, i.e. there
exist usSVD’s W1 = WΣVT,W2 = UΣWT.
It has been shown that SGD in the overparameterized setting for a network initialized close to zero
will converge to a solution close in '2-norm to the minimum '2-norm solution (Azizan et al., 2019).
Therefore we expect such networks to converge to a solution which is close to an aligned solution.
5	Alignment Under General Layer Structure
In the previous section, we analyzed fully connected networks, where parameters of each weight
matrix are optimized independently. The most commonly used deep learning models, however,
rely on convolutional layers or layers with other forms of constraints. In this section, we analyze
alignment in the setting of linear networks with layer constraints. In particular, we show that when
the dimension of the subspace induced by the layer constraints is small compared to the number of
training samples, alignment cannot happen, let alone be an invariant of training.
5.1	Linear Neural Networks with Layer Structure
We start by setting up mathematical terminology to describe different layer structures.
Definition 4. Let S ⊂ Rm×n be a linear subspace of matrices and let {Ai}ir=1 be an orthogonal2
basis for S. Layer Wi has layer structure S if Wi ∈ S, i.e., there exist coefficients {cij}jr=1 ⊂ R such
that Wi = jr=1 cijAj, and gradient descent operates on the {cij}ir,j =1.
Definition 4 encompasses layer structures commonly used in practice, such as:
Convolutional layers: Treating a p × p image as a vector in Rp2 , a single s × s convolutional filter
with stride 1 and padding (s - 1)/2 maps the image to another p × p image; this linear transformation
is a matrix in Rp2 ×p2 and the set of all such transformations forms an s2-dimensional subspace. The
parameters of the filter are coefficients of an orthogonal basis of this subspace; see Appendix I.
Layers with Sparse Connections: Consider a fixed connection pattern between layers such that
the jth hidden unit in layer i depends only on a subset of units in layer i - 1. In this case, the
subspace S consists of matrices where particular entries are forced to be zero corresponding to
missing connections between features in consecutive layers.
The following theorem provides, in closed-form, the gradient descent update rules for linear networks
with layer structure. The proof is provided in Appendix H.
Theorem 2. Performing gradient descent on the basis coefficients {cij }jr=1 leads to the following
weight matrix updates:
W"+1) = Wi(t) - η∙% (*!，
where πS denotes the projection operator onto S.
Theorem 2 shows that gradient descent in networks with layer structure is equivalent to projected
gradient descent3. Hence alignment is an invariant of training if and only if it holds throughout the
projected gradient descent updates and leads to an aligned solution with zero training loss.
2Orthogonality is w.r.t the inner product hA, Bi = Tr(AT B), or equivalently the dot product in Rmn
3 πS is a projection in the traditional sense if and only if the Aj form an orthonormal basis; otherwise, πS is
a projection onto S followed by an appropriate scaling in each basis direction.
6
Under review as a conference paper at ICLR 2021
5.2 Necessary Condition for Alignment
Motivated by the above characterization via projected gradient descent, we now show that for layer
structures with constrained dimension, aligned networks generally cannot achieve zero training error
under the squared loss, given sufficient data (Proposition 4). This is the case even when there is a
solution with the desired layer structure that achieves zero training error. Hence, if loss is minimized
to zero, gradient descent must lead to a non-aligned network.
We first show that for an aligned network which interpolates the data, the first and last layer must
align with the pseudoinverse. The proof of this result is presented in Appendix J.
Proposition 3. Let (X, Y ) ∈ Rk0 ×n × Rkd ×n such that n ≥ k0 and X is full-rank (ensuring that
XXT is invertible). If an aligned network f = WdWd-1 . . . W1 achieves zero error under squared
loss (i.e. if Y = f(X)), then WdT aligns with Y XT (XXT)-1, which in turn aligns with W1T.
The following result tells us that when a linear space S of matrices is sufficiently low-dimensional,
the set of matrices that align with an element of S has measure zero. While we are mainly interested
in the setting where n ≥ k, we state it in full generality using m2 = 0, when m < 2.
Proposition 4. Let S be an r-dimensional linear subspace of k × k matrices. If r < k - 1 - k-2n
then the set of matrices of size k × n that can align with an element of S, excluding scalar multiples
of the identity, has Lebesgue measure zero.
The proof of Proposition 4 is provided in Appendix K. Taken together, Propositions 3 and 4 imply
Theorem 3, which states that alignment does not occur in linear networks with constrained layer
structures given enough training samples. We assume k = ko =…=kd and that all layers have the
same structure, S . The statement can trivially be extended to the general setting.
Theorem 3. Let n ≥ k, let X, Y ∈ Rk×n be generic, let S ⊂ Rk×k be a linear subspace of
dimension r < k - 1, and let W1, . . . , Wd ∈ S such that at least one Wi is not a scalar multiple of
the identity4. Ifthe network f = Wd .…Wi satisfies Y = f (X) ,then f is not aligned.
Theorem 3 is in contrast to fully connected networks (i.e., no layer constraints), where we showed that
alignment is possible for particular classes of problems including autoencoders. An explicit example
of a convolutional linear autoencoder, where alignment is ruled out by Theorem 3, is discussed next.
Example. If m ≥ 4, then a generic dataset consisting of n ≥ m2 m × m images cannot be aligned
by any convolutional linear autoencoder with filter size 3, aside from the trivial case where all layers
are scalar multiples of the identity. This follows from letting k = m2, r = 9 in Proposition 4.
6 Empirical S upport
In this section, we provide experimental results to validate our findings5 . We measure two properties:
(1) invariance of alignment from initialization, and (2) alignment between layers. Invariance of
alignment at time t is measured by the average dot product between corresponding columns of Ui(t)
and Ui(0) , as well as Vi(t) and Vi(0) . Alignment is measured by the average dot product between
corresponding columns of Ui(t) and Vi(+t)1. For both, a value of 1 is perfect alignment / invariance.
We begin by demonstrating that alignment is not an invariant of training for fully connected networks
when the data conditions of Theorem 1 are violated. Figure 1a shows an example where alignment
is not an invariant for multi-dimensional regression with random data under squared loss. We used
standard normal inputs X ∈ R9×9 and targets Y ∈ R9×9 , and a 2-hidden layer network initialized so
that alignment holds at the start of training. Since X and Y do not have the same right singular vectors,
the conditions of Theorem 1 are violated, and hence alignment is not an invariant of training. In
Figures 1b and c, we show that alignment is also not an invariant in standard classification settings. We
trained a 2-hidden layer fully connected network to classify a linearly separable subset of 256 MNIST
examples under MSE loss and cross entropy loss. Figure 1b is consistent with the generalization of
4This is not a serious restriction; modulo scalar multiplication, the only case in which such a network could
achieve zero loss is autoencoding, in which case the latent space would be a scalar multiple of the data itself.
5Hyperparameter settings are detailed in Appendix M
7
Under review as a conference paper at ICLR 2021
一 Alignment
(a) Multi-dimensional regression on (b) Multi-class classification on (c) Multi-class classification on
random data with squared loss. MNIST with squared loss. MNIST with cross entropy loss.
Figure 1: Examples of fully connected networks with multi-dimensional outputs where alignment is
not an invariant of training.
Oo95B0B5B0乃J7O65I6O
lαo.ao.o.cio.a
0UU.S」©> usu 31UU,≡>lv
Alignment
Invariance
Loss
(a) Matrix factorization with layers
constrained to be Toeplitz matrices.
(b) Autoencoding a single MNIST example
using a convolutional network.
Iteration
Figure 2: Examples of layer constrained networks, where alignment is not an invariant of training.
Theorem 1 (Appendix D). Interestingly, this result empirically transfers to the case of cross entropy
loss, suggesting that our theoretical results may also be relevant for other loss functions.
In networks with constrained layer structure, Theorem 3 shows that given sufficient data alignment
cannot occur. We now present empirical evidence that alignment is not an invariant of training, even
when the number of training samples is much smaller than the output dimension of the network or the
dimensionality of the layer structure is much larger than that of the output. In the setting of matrix
factorization (Y ∈ Rk×k, X = I), k = n, so Theorem 3 states that alignment is impossible when the
linear structure has dimension r < k - 1. In Figure 2a, we observe that alignment is not invariant
also even when r ≥ k - 1, by training a 2-hidden layer Toeplitz network to factorize a 4 × 4 matrix.
Our network has 4 hidden units per layer and thus r = 7, k = 4, n = 4. Even when n < r < k, we
observe that alignment is not an invariant. In Figure 2b, we show that alignment is not an invariant of
training when autoencoding a single MNIST example using a 2-hidden layer linear convolutional
network (i.e. n = 1, r = 9, k = 784). In Appendix L, we provide empirical validation that alignment
is indeed an invariant of training when the data conditions of Theorem 1 are satisfied.
7	Discussion
We generalized the definition of alignment to linear networks with multi-dimensional outputs. We
then analyzed the invariance properties of alignment, showing that under particular data conditions
alignment is an invariant for fully connected networks, which allows us to significantly simplify the
convergence analysis of gradient descent. We then extended our analysis of alignment to networks
with constrained layer structures, such as convolutions, and proved that alignment cannot be an
invariant of training in such networks when the dimension of the layer structure r is small compared
to the number of training samples n.
While the simplification of gradient descent convergence analysis in the fully connected setting
shows that our alignment definition is useful in understanding such networks, the fact that it does not
generalize as an invariant to the constrained layer structure setting suggests that other approaches may
be necessary to fully understand implicit regularization, such as studying how architecture influences
the function classes that can be represented by deep networks (Savarese et al., 2019; Zhang et al.,
2020; Radhakrishnan et al., 2019).
8
Under review as a conference paper at ICLR 2021
References
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In International Conference on Machine Learning (ICML),
2018.
Sanjeev Arora, Nadav Cohen, Noah Golowich, and Wei Hu. A converge analysis of gradient descent
for deep linear neural networks. In International Conference on Learning Representations (ICLR),
2019a.
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo. Implicit regularization in deep matrix
factorization. In Advances in Neural Information Processing Systems (NeurIPS), 2019b.
Sanjeev Arora, Simon S. Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of
optimization and generalization for overparameterized two-layer neural networks. In International
Conference in Machine Learning (ICML), 2019c.
Navid Azizan, Sahin Lale, and Babak Hassibi. Stochastic mirror descent on overparameterized
nonlinear models: Convergence, implicit regularization, and generalization. arXiv preprint
arXiv:1906.03830, 2019.
Rajendra Bhatia. Matrix Analysis. Springer-Verlag, 1997.
Simon S. Du, Wei Hu, and Jason D. Lee. Algorithmic regularization in learning deep homogeneous
models: Layers are automatically balanced. In Advances in Neural Information Processing Systems
(NeurIPS), 2018.
Simon S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks . In International Conference on Learning Representations
(ICLR), 2019.
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient
dynamics in linear neural networks. In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In Advances in Neural Information Processing
Systems (NeurIPS), 2017.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Characterizing implicit bias in
terms of optimization geometry. In Internation Conference on Machine Learning (ICML), 2018a.
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro. Implicit bias of gradient descent on
linear convolutional networks. In Advances in Neural Information Processing Systems (NeurIPS),
2018b.
Ziwei Ji and Matus Telgarsky. Gradient descent aligns the layers of deep linear networks. In
International Conference on Learning Representations (ICLR), 2018.
Jason D. Lee, Max Simchowitz, Michael I. Jordan, and Benjamin Recht. Gradient descent converges
to minimizers. In Conference on Learning Theory (COLT), 2016.
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized
matrix sensing and neural networks with quadratic activations. In Conference On Learning Theory
(COLT), 2018.
Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks:
Evidence from random matrix theory and implications for learning, 2018. arXiv:1810.01075.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the
role of implicit regularization in deep learning, 2014. arXiv:1412.6614.
Adityanarayanan Radhakrishnan, Mikhail Belkin, and Caroline Uhler. Memorization in overpa-
rameterized autoencoders. In ICML Workshop on Identifying and Understanding Deep Learning
Phenomena, 2019.
9
Under review as a conference paper at ICLR 2021
Pedro Savarese, Itay Evron, Daniel Soudry, and Nathan Srebro. How do infinite width bounded norm
networks look in function space? arXiv preprint arXiv:1902.05040, 2019.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear
dynamics of learning in deep linear neural networks. In International Conference on Learning
Representations (ICLR), 2014.
Andrew M. Saxe, James L. McClelland, and Surya Ganguli. A mathematical theory of semantic
development in deep neural networks. Proceedings of the National Academy of Sciences, 116
(23):11537-11546, 2019. ISSN 0027-8424. doi: 10.1073∕pnas.1820226116. URL https:
//www.pnas.org/content/116/23/11537.
Ohad Shamir. Exponential convergence time of gradient descent for one-dimensional deep linear
neural networks. arXiv preprint arXiv:1809.08587, 2018.
Daniel Soudry, Elad Hoffer, Mor S. Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. Journal of Machine Learning Research (JMLR), 19(1):
2822-2878, 2018.
Xiaoxia Wu, Simon S. Du, and Rachel Ward. Global convergence of adaptive gradient methods for
an over-parameterized neural network? arXiv preprint arXiv:1902.07111, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understand-
ing deep learning requires rethinking generalization. In International Conference on Learning
Representations (ICLR), 2017.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, and Yoram Singer. Identity crisis: Memorization and
generalization under extreme overparameterization. In International Conference on Learning
Representations (ICLR), 2020.
Difan Zou, Yuan Cao Cao, Dongruo Zhou, and Quanquan Gu. Stochastic gradient descent optimizes
over-parameterized deep relu networks. arXiv preprint arXiv:1811.08888, 2018.
10
Under review as a conference paper at ICLR 2021
Appendix
A Outline of Proof for Theorem 1, Corollary 1, and Proposition 1
We now provide an outline of our results and proofs.
1.	In Appendix A, we introduce Lemmas 1, 2, which will be used to prove Theorem 1.
2.	In Appendix B, we provide the proof of Corollary 1 - the simplification of gradient descent
under alignment - which relies on Lemma 2.
3.	In Appendix C, we provide the proof of Proposition 1 - linear convergence under strong
alignment - which relies on Corollary 1.
4.	In Appendix D, we introduce Theorem 4, which is a generalization of Theorem 1 to fully
connected networks with rectangular layers. We use Lemma 2 and Proposition 1 to prove
Theorem 4.
5.	In Appendix E, we finally prove Theorem 1, which follows from Theorem 4.
Here, we present two lemmas that will be used extensively in our proofs.
Clearly strong alignment being an invariant implies that alignment is an invariant. Now we show that
alignment implies strong alignment in the case of networks with square matrix layers.
Lemma 1. Let {Wi}id=1 ⊂ Rk×k, where d ≥ 3. If alignment is an invariant of training under
the squared loss for network f = WdWd-1 . . . W1 on data (X, Y ) ∈ Rk×n × Rk×n, then strong
alignment is also invariant.
Proof. Assume that alignment is an invariant of training. Gradient descent on the objective
1n
argmin ʒ- E ∣∣y(i) - f(χ(i))k2	⑺
f∈F 2n
i=1
proceeds via the following update rule:
n
Wi(t+1) =	Wi(t)	+ Y (Wdt)... Wi+)1)T	X(y(l -	f (X(I)))(Wi-I	…w(t)χ(l))T,	∀i	∈	[d].	(8)
n	l=1
Since alignment is an invariant, the initialization satisfies Wi(t) = Ui Σi(t)ViT for 2 ≤ i ≤ d - 1,
W1(t) = U1Σ(1t)V1(t)T, and Wd(t) = Ud(t)Σ(dt)VdT,whereUi = Vi+1 for i ∈ [d-1].For2 ≤ i ≤ d-1,
substituting into Equation (8) yields
n
W(t+1) = Ui£(t)VT + n(Udt)∑dt) …∑(+ιV+ι)T X(W - f (X(I)))(Uiτ∑2 …∑1tHt) X(I))T
l=1
=Ui 卜t) + Y YY ∑jt)TUdt)T X(y(l) - f (X(I)))X(I)Tv(t) ∏ ∑jt)T] VT
n j=i+1	l=1	j=1
=Ui I ∑(t) + Y ∏ ∑jt)T(Udt)TYXTVy)- ∑dt) …∑1t)v1(t)TXXTVJt)) ∏ ∑jt)T I VT
n j=i+1	j=1
Since alignment is an invariant, the quantity
d	i-1
∏ ∑jt) (Udt) YXTVf)- ∑dt) …∑1t)%(t) XX TVf)) ∏ ∑jt)	(9)
j=i+1	j=1
is a diagonal matrix for all t. Since each of the Σj are square, full rank matrices, the quantity
Udt)TYXTV(t) - ∑dt) ∙∙∙ ∑1t)V(t)TXXTV(t)
11
Under review as a conference paper at ICLR 2021
must be diagonal for all t.
The update rule for W1 is given by
n
w(t+1) = W(t) + Y(Wdt)…w2(t))T X(y(l) - f (X(I)))X(I)T
n	l=1
Uι∑1t+1)V1(t+1)T = Uι∑1t)%(t)T + V2 Y ∑jt)TUdt)T(YXT - Ud∑dt) .一 ∑1t)Vι(t)TXXT)
j=2
=⇒ ∑1t+1)%(t+1)TV(t) = ∑1t) + Y ∑jt)T(Udt)TYXTV(t) - ∑dt) ∙∙∙ ∑1t)%(t)TXXTvf)),
j=2
which is diagonal. Therefore V1(t+1) V1(t) is diagonal, and since this is also an orthogonal matrix we
must have that V1(t+1) = V1(t) .
Similarly, the update rule for Wd is given by:
n
wdt+1) = wdt) + nX(y(l) - f(x(l)))x(l)T(Wd-1 ∙∙∙w(t))T
n l=1
udt+1)∑dt+I)VdT = udt)∑1t)VdT + (YXT - udt)∑dt) ∙∙∙ ∑1t)Vι(t)TXXT)V(t) Y ∑jt)TUd-1T
j=1
=⇒ Udt)Tudt+1)∑dt+1) = ∑dt) + (Udt)TYXTVf) - ∑dt) ∙∙∙ ∑1t)V(t)TXXTV(t)) Y ∑jt)TUd-1T,
j=1
which is diagonal. Therefore Ud(t)TUd(t+1) is also diagonal, implying that Ud(t) = Ud(t+1). Therefore
strong alignment is also an invariant. This means that alignment being an invariant and strong
alignment being an invariant are equivalent in the setting where all the k are equal.	□
Now that we have shown the equivalence of alignment being an invariant and strong alignment being
an invariant in the setting where all the layers are square, we prove the following lemma for the
general case where the ki are not necessarily all equal.
Lemma 2. Let f : Rk0 → Rkd be a linear fully connected network as in Equation equation 1, and
let r = min(k0, . . . , kn). For training under the squared loss on the dataset (X, Y), there exists an
aligned initialization f (x) = W(0') …W(0)x such that wf) = Ui∑(t)VT for all i ∈ [d] (that is,
Ui, Vi are not updated) if and only if there exist orthonormal matrices U ∈ Rkd×kd, V ∈ Rk0×k0
such that
UTYXTV
Λ0	0
0	A1
and V TXXTV
Λ0
0	A2
for diagonal r × r matrices Λ, Λ0 and arbitrary A1 ∈ R(k0-r)×(kd-r), A2 ∈ R(k0 -r)×(k0 -r) .
Proof. Gradient descent on the objective
1n
argmin 2 E ky(i) - f(Xci))k2
f∈F 2n
i=1
proceeds via the following update rule:
n
wi(t+1) = Wi(t) + Y (Wdt)... Wi+)1)T X(y(l) - f (χ(l)))(Wi-)1... w(t)χ(l))T,	∀i ∈ [d],
n	l=1
(10)
12
Under review as a conference paper at ICLR 2021
(11)
(12)
where γ is the learning rate and superscript (t) denotes the gradient descent step. Assume that the
network is initialized to be aligned, that is, there exist orthonormal Ui , Vi and diagonal matrices Σi
such that Wi = UiΣiViT and Ui = Vi+1 for i ∈ [d - 1]. Substituting into Equation (10) yields
n
Wi(t+1) = Ui∑(t)VT + n(UMt …∑i+1V+1)τ X(y(I)- f(x(I)))(Ui-1∑i-1 …ΣIt)VITx(I))T
l=1
d	n	i-1	∖
Σ(t) + n Y ∑jt)TUT X(y(l) - f(x(I)))X(I)TVi Y ∑jt)T I VF
n j=i+1	l=1	j=1
d d	τ	i-1	τ ∖
∑(t) + Y Y ∑jt) (UTYXTVi - ∑dt) ∙∙∙∑1t)vTXXTvi) γ∑jt)	VT.
j=i+1	j=1
Thus strong alignment is an invariant if and only if for all i, the quantity
d	T	i-i T
Y ∑jt) (UT YXT V1 - ∑dt) ∙∙∙ ∑it)VT XXT V1) Y ∑jt)
j=i+i	j=i
is an ki × ki-i diagonal matrix for all t. At initialization each of the Σj have rank at least r.
Considering i = 1 and i = d, the above quantity is diagonal if and only if the matrix
UTYXTV1 - ∑dt) …∑1t)vTXXTV1
has its top r rows and top r columns all diagonal; i.e. we can write this expression as
D0
0A
for an r × r diagonal matrix D and an arbitrary (kd - r) × (k0 - r) matrix A.
For the first direction, assume that strong alignment is an invariant, i.e. that Equation (11) can be
written in the above block diagonal form. Define Σ(0t = ∑dt) •…∑1t) - this is a diagonal matrix
whose only nonzero entries are the first r on the diagonal. We know that
UdTYXTV1 - Σ(tto)tV1TXXTV1
is of the form of Equation (12) for all gradient descent steps t, and thus the quantity
Σt(to)t -Σ(to0t) V1TXXTV1
is of this form as well. Assuming that we’ve not initialized any of the singular values to be their
optimal value (which is satisfied with probability 1), the top r diagonal entries of Σt(to)t - Σt(o0t) are
nonzero, which means that the top left r × r submatrix of V1TXXT V1 is diagonal, and that the top
right submatrix consists of all zeros. But since V1TXXTV1 is symmetric, the bottom left submatrix
must also consist of all zeros, and thus we have
V1TXXT V1 =	D02 A02
for an r × r diagonal matrix D2 and arbitrary (k0 - r) × (k0 - r) matrix A2. Plugging this into
Equation (11) implies that UdTYXTV1 must be of this form as well.
We next show the other direction. Assume that for some orthonormal matrices U and V, it holds that
VTXXTV is diagonal and UTYXTV can be written in the block matrix form given by Equation
(12). Initializing the layers such that Ud = U, V1 = V, and Ui = Vi+1 for i ∈ [d - 1] implies that
Equation (11) is also of this block diagonal form, as desired.	□
B Proof of Corollary 1
Proof. The conditions of strong alignment imply the conditions of Lemma 2, which in turn implies
that there exist orthonormal matrices U, V such that
UTYXTV = Λ00 A0 , and
Λ
0
VTXXTV
0
A2 ,
13
Under review as a conference paper at ICLR 2021
where Λ, Λ0 are r × r diagonal matrices. Furthermore, from the proof of Theorem 1, if the layers are
initialized to be aligned, with Ud = U and V1 = V , then the gradient descent updates are as follows:
Wi(t+1) = uj∑it) + Y Y ∑jt)T(UTYXTV1 - ∑dt) …∑It)VTXXTV) Y ∑jt)T I VT
n j=i+1	j=1
Since the minimum of the ranks of the Σi(t) is r, only the top r singular values of Wi are updated.
Plugging in the expressions for UTY XTV and VTXXT V and restricting to the top r singular
values (which we denote by Σ0i), we obtain the statement of Corollary 1, with the singular values of
each layer being updated as:
d
∑i(t+1) = Σi(t) + Y Y Σj(t)(Λ0 - Y Σj(t)Λ).
This completes the proof.	口
C Proof of Proposition 1
Proof. By Corollary 1, under strong alignment, each singular value is updated independently of
each other. Thus we can focus on how the kth singular value for each layer is updated. Recall that
σk(Wi(t)) denotes the kth diagonal entry of Σi(t). Since we’re focusing on a fixed k, we drop the
subscript k for convenience and let σi(t) equal σk(Wi(t)). The σ are updated by the following update
rule:
d
σ”=σ(t) + Y Yσjt)(λk - λfc Y σ"
n j 6=i	j =1
where λ0k, λk are the kth diagonal elements of Λ0, Λ. We assume that Λ0 and Λ have the same zero
pattern. Therefore λk = 0 if and only if λ0k = 0. If both of these values are zero, then σi is not
updated.
Otherwise, assume λk , λ0k 6= 0. Note that λk > 0, since XXT is positive semidefinite. We can also
negate columns of U to ensure that λk > 0 as well. Let η = γλk, and define S(t) = Q；=i σjt). This
yields
σi(t+1) = σi(t) +
- S(t)).
(13)
Therefore (dropping the superscript to let S = S(t)),
dd
S(t+1) = Y σi(t+1) = Y	σi(t) + ηS
i=1	i=1
- S)
S + E	η∣T∣s∣T∣
T ⊂[d]"T∣≥1
S + E	ηlT 1S|T1+1
T ⊂[d]"T∣≥1
⅜ - S)1T1Y 4) Y σ(t)
k	i∈T σi i6∈T
(λk	- S)lTlY p⅛，
k	i∈T (σi )
and hence
0
λk - s(t+i)
λk
λk - S - x 严lSlTl+1(λkk - S)1T1Y -⅛-
k	T⊂[d]"T∣≥1	k	i∈T (σi )
-S)(1 - X ηlTlSlTl+1(λk - S)lTΤY 工
∖ TU[d]:[T∣≥1	k	i∈T (σi )
(14)
(15)
14
Under review as a conference paper at ICLR 2021
Thus we obtain
0
λk - s(t+i)
λk
(16)
where
rkt) = 1- X	ηlT |S|T 1+1
T ⊂[d]"T∣≥1
-S)Y (⅛.
(17)
We aim to bound rk(t) from both above and below. First, we show that rk(t) is nonnegative in order to
prove the following lemma:
Lemma 3. 0 < Sj) ≤ λk forall j ≥ 0.
Proof. We proceed by induction. By the original assumptions in Proposition 1, 0 < S⑼ ≤ λk. Now
assume that 0 < Sj) ≤ λk for all j ≤ t. By the update rule in Equation (13), σ(j+1) ≥ σ(j). Since
σi(0) > 0, σi(j) > 0, so S(j ) > 0. We also have that
∏ 712≤ ≤ ∏ TT0K2 ≤
i∈T (σi )2 i∈T (σi )2
1
(mini σ(0))2lT|
Next, note that we can bound
S∣τ∣+i(λk - s)∣t∣-i ≤(λk )2∣τ∣
λk	λk
This means that we can upper bound the sum in Equation (17) as
X	n|T|S|T1+1(λk - S)ITITn ɪ ≤ X	ηlτl(miinσ(0))-21T|(λk )21T1
T⊂[d]"T∣≥1	k	i∈T (σi )	T⊂[d]"T∣≥1	2	k
=(1 + η ∙ (minσ(0))-2(λk)2) - 1.
(0) 2	(0) 2	2
Since Y ≤ nd2 ∙——i / ——,We have that η ≤ ln2 ∙——i di，∙会,and thus the right-hand
side of the above expression can be upper bounded by
(1+ η∙ (minσ(0))-2)d - 1 ≤ edη(m>η σ(0))-2 - 1 ≤ eln2 - 1 = 1.
Therefore rkt) ≥ 0. Plugging into Equation (16), since S(t) = S ≤ λk, we get that S(t+1) ≤ λk,
which completes the inductive step.	□
Next, we would like to upper bound rk(t) by a term independent of t in order to obtain linear
convergence. We can lower bound the sum in Equation (17) by the sets with size 1, so
X	n|T|S1T1+1(λk - S)|TTn 7⅛ ≥ XηS2τ⅛ ≥ *2 ∙ d-2/d,
T⊂[d]"T∣≥1	k	i∈T (σi )	i=1	(σi )
where the last inequality is due to AM-GM . Lemma 3 implies that S(j+1) ≥ S(j), which means that
the above sum is at least ηd(S(O))2-2/d, which means that we can upper bound rkt) by
rkt) ≤ 1 - ηd(S(O))2-2/d.
This implies that S(t+1) is closer to λk than S is, and in particular
λk - S(t+1) ≤ (λk - S)(1 - dη(S(O))2-2/d)；
15
Under review as a conference paper at ICLR 2021
hence
λk - S㈤ ≤ (λk - S(O))(1 - dη(s⑼)2-2/d)t.
λk	λk
Since the initialization is fixed, the quantity 1 - dn(S(0))2-2/d is fixed, and thus S(t) converges
linearly to λk. Therefore each of the top k singular values converge linearly to their optimal value
λk, which means that the loss converges linearly as well.
To complete the proof, it suffices to show that this limit solution achieves a training loss of zero. This
is proven in a more general setting at the end of Appendix E.	□
Λ0
0	A2
D Proof of Theorem 4
We can finally state the generalization of Theorem 1 to the non-square setting:
Theorem 4. Let f : Rk0 → Rkd be a linear fully connected network as in Equation equation 1, and
let r = min(k0, . . . , kn). Strong alignment is an invariant of training under the squared loss on the
dataset (X, Y ) if and only if there exist orthonormal matrices U ∈ Rkd ×kd, V ∈ Rk0×k0 such that
UTYXTV =	Λ00 A0	, and VTXXTV
for diagonal r × r matrices Λ, Λ0 and arbitrary A1 ∈ R(k0-r)×(kd-r) , A2 ∈ R(k0 -r)×(k0 -r) .
Proof. By Lemma 2 we know that under strong alignment there exist U and V satisfying the above
conditions. In the other direction, Lemma 2 also tells us that given U and V satisfying the data
conditions, all the conditions of strong alignment hold except for convergence to a global minimum.
To conclude, we must show that regardless of the zero pattern of Λ or Λ0 , under a strongly aligned
initialization the network converges to a solution with a loss of zero.
Using the convenient notation that σi(t) = σk(Wi(t)), we again focus on how the kth singular values
of each layer are updated, for some k ∈ [r]. Recall that the σ's are updated as
d
σ”=σ(t) + n Yσjt)(λk - λk Y σ”
j 6=i	j=1
The rank of X must be at least the rank of Y in order for the data to be linearly interpolated. Therefore
we can choosen U, V (via permuting columns) to ensure that whenever λk = 0, λ0k = 0 as well. This
ensures that σk(Wi(t)) is never updated. If λk, λ0k 6= 0, then we showed in Proposition 1 that S(t)
converges to λk/λk in the limit.
Finally, we consider the case where λk = 0, λk = 0. Assume that σ(t) < 1 and γ < λn-. Then, the
σi,s update as
σ”=σ(t) + n Y σjt) f-ʌk Y σjt' = % (l - η Y(σjt))2
j 6=i	j=1	j 6=i
where η = γλk ∙ We observe that 0 ≤ σ(t+1) ≤ σ(t). Therefore
0 ≤ S(t+1) = S ㈤ Y(1 - η Y(σjt))2∣ ≤ S㈤ exp [-n XX Y(σjt))2∣ ≤ S ㈤ exp "dS …2、.
i=1	j6=i	i=1 j 6=i
Since S(0) is positive, we see that 0 ≤ S(t+1) ≤ S(t), and therefore S(t) must converge to some
constant c. Assume that c 6= 0. For all > 0, there exists some t such that S(T ) < c + . Then,
S(T +1) ≤ SlT) eχp (-ndS(T)2 2/d) < (C + E) exp (-32-2"),
16
Under review as a conference paper at ICLR 2021
where exp (-ηc2-2∕d) is a constant which is less than 1. Hence if We choose E such that
exp (一ηc2-2∕d) < 冷,then S(T +1) < c, a contradiction. Therefore C = 0, and hence
S㈤ → 0 = λ'k/λk. c
In general, we have shown that if λk = 0, then σk(Wι(t)) …σk(Wd(t)) → λk∕λk. This solution is
given by f(x) = UdΛ0Λ-1 V1Tx, which is the solution given by the pseudoinverse which obviously
has a loss of zero.	□
E Completing the Proof of Theorem 1
Proof. In Lemma 1, we showed that in the setting where all layers are square, alignment is equivalent
to strong alignment. Theorem 4 states that in general, strong alignment is an invariant if and only if
there exist U, V satisfying particular data conditions. Since in the square setting r = k, by Theorem 4
we have that strong alignment is an invariant if and only if there exist U, V such that UTY XTV and
VT XXT V are diagonal, as desired.	□
F Alignment for 1 -Dimensional Outputs
Proposition 5. Assuming gradient descent avoids the point where all parameters are zero, alignment
is an invariant of training for any linear fully connected network f : Rk0 → R, any convex, twice
continuously differentiable loss function, and data (X, Y ) ∈ Rk0×n × R1×n for which the network
can achieve zero training error.
Proof. If we initialize the weight matrices to be rank 1 and aligned, then the matrices {Σi(t)}id=1 are
diagonal with a single non-zero entry. Following the proof of Theorem 1, we obtain that alignment is
an invariant if the matrix
d
Y
j=i+1
i-1
x(k)T V1(t)	Y
(x(k),y(k))	j=1
T
is diagonal. When i 6= 1, d, this matrix is clearly of rank 1 and diagonal (and has a single nonzero
entry). This implies that Ui, Vi are invariant for all i 6= 1, d. If i = d, then since kd = 1, the above
quantity is also a rank 1 diagonal matrix, implying that Ud and Vd are invariant. Finally, if i = 1, the
above matrix is rank-1 but not necessarily diagonal. However, all but the top row are zeros, which
after plugging into the gradient descent update rule implies that U1 is invariant as well. Importantly,
layers Wi+1, Wi for i ∈ [d - 1] remain aligned regardless of the loss function used, as the expression
above is always a diagonal matrix with a single nonzero entry when the layers are initialized to be
rank 1. The final step is to show that training leads to zero error according to Definition 3. To do
this, we first characterize the stationary points and then under assumptions, we prove that the loss
converges to zero.
We now characterize the stationary points of the above update. Let v1(t) denote the first column of
V1(t), and let σ1(Wj(t)) denote the top singular value in the usSVD of Wj(t). Then the stationary
points are given by:
1. σ1(Wj(t)) = 0forj ∈ [d].
C (t) X X ∂'
2. v1 ⊥ S ∂f
x(k)T
(x(k) ,y(k))
Ifwe initialize σ1(W1(0)) = 0, then we have that:
σ1(W1(t))v1(t)T = Xn c(kt)x(k)T
k=1
ckt+1) = X (ckt) + Y
k=1	j6=k
x(k)T
(x(k) ,y(k)
17
Under review as a conference paper at ICLR 2021
for c(kt)
{x(k)}
∈ R and ∀t ∈ Z≥0. Hence, updates to v1(t) are in the span of the data, and so assuming that
一 一	(t)	一 一 一3 ∂'
n=ι are linearly independent, Vf) cannot be orthogonal to E —
k=1 f
are all 0, i.e. σ1(W1(t)) = 0 for t > 0.
x(k)T unless the
(x(k) ,y(k) )
Next, ifwe initialize σ1(Wi(0)) = σ1(Wj(0)), then σ1(Wi(t)) = σ1(Wj(t)) for all i,j ∈ {2, . . . d}, t ≥
0 since for all i ∈ {2, . . . d}:
σι(Wi(t+1))= σι(W* + Yσι(W∕) (XX	x(k)Tv(t))
j6=i	k=1 ∂f (x(k),y(k))
This initialization corresponds to layers Wi+1, Wi being balanced for i ∈ {2, . . . d}. Thus, under this
initialization, the only other stationary point is given by σ1(Wi(t)) = 0 for all i ∈ {2, . . . d}.
Hence, if gradient descent avoids the non-strict saddle points given by σ1 (Wi(t)) = 0 for all i ∈
{2, . . . , d} and σ1(Wi(t)) = 0 for all i ∈ [d], then gradient descent converges to a local (and thus
global) minimum of the convex loss. The former stationary point can be avoided by re-parameterizing
the network such that σ1(Wi(t)) = σ1 for all i ∈ {2, . . . d} (i.e. σ1 = 0 now corresponds to a
strict saddle as defined in Lee et al. (2016)), and then taking a random initialization for σ1. This
would correspond to gradient descent on the original parameterization with a scaling factor on the
learning rate for parameters σ1(Wi(t)) for i ∈ {2, . . . d}. The latter stationary point is avoided by the
assumption in the proposition.	□
G Proof of Proposition 2
Proof. For any matrices A, B ∈ Cm×n, We have that 2σi(AB*) ≤ σi(A* A + B*B) (Bhatia,1997).
Thus letting A = W2 , B = W1T, we see that
2σi(W2W1) ≤ σi(W2TW2 +W1W1T)
=⇒2X σi(P) ≤X σi(W2T W2 + W1W1T)
= kW2T W2 + W1W1Tk1
≤ kW2TW2k1+kW1W1Tk1
= kW2k2F+kW1k2F
This loWer bound is in fact achieved for an aligned solution. If the SVD of P is P = UΣV T, setting
Wi = WΣ2UT and W2 = UΣ1VT yields ∣∣W1∣∣F = ∣∣W2∣∣F = Tr(Σ), so ∣∣W1∣∣F + ∣∣W2∣∣F =
2Tr(Σ).	□
H Proof of Theorem 2
Proof. Given an arbitrary loss function, assume that the ith layer is restricted to some structure given
by a subspace S and basis matrices A1, . . . Am, so that at timestep t We have that
m
Wi(t) = X(cij )(t)Aj
j=1
We take the gradient of the loss With respect to the cij . The chain rule yields:
∂l =X ∂l	∂(Wi )pq = XX ∂l Aj
M=Py Eq ∙ Fr=Py Eq ∙Pq
18
Under review as a conference paper at ICLR 2021
The gradient descent update on cij is thus:
(Cj产I) = (Cj)㈤-η ∙ M
∂cij
n
(Cij)(t)-ηX
p,q=1
∂l
∂ (Wi)Pq
The corresponding update on Wi becomes
m
Wi(t+1) = X(Cij)(t+1)Aj
j=1
m	mn
X(Cij)(t)Aj-ηXX
j=1	j=1 p,q=
mn
Wij X pXι d(⅛ ∙ Apq Aj
We calculate the projection operator π of some arbitrary matrix M onto S . We can write
m
π(M) = X
j=1
hM, AjiAj
kAj k2
mm
XX
j=1 p,q=1
Mpq Apq Aj
kAj k2
If we define the operator πS as
m	mm
πS(M) = XhM,AjiAj = X X MpqAjpqAj,
j=1	j=1 p,q=1
then gradient descent on the C gives the following update rule on the Wi:
Wi(t+1) = Wi(t) - η ∙ ∏s
If the Aj all have norm 1, then, π = πS , and this is the same update rule given by projected
gradient descent with respect to the subspace S . Otherwise, πS is simply the projection π followed
by appropriate scaling in each of the basis directions.	□
I Treating a Convolutional Layer as a Linear Subspace
Consider a 3 × 3 image. We map it to a 9-dimensional vector as follows
x1	x2	x3	T
x4	x5 x6 =⇒ [x1 x2	x3	x4 x5	x6	x7 x8	x9] .
x7	x8	x9
Then, the linear transformation given by applying the 3 × 3 convolutional filter												C1 C4 C7	C2 C5 C8	C3 C6 C9	is
given by the matrix		"C5	C4	0	C2	C1	0	0	0	0					
		C6	C5	C4	C3	C2	C1	0	0	0					
		0	C6	C5	0	C3	C2	0	0	0					
		C8	C7	0	C5	C4	0	C2	C1	0					
	W=	C9	C8	C7	C6	C5	C4	C3	C2	C1	.				
		0	C9	C8	0	C6	C5	0	C3	C2					
		0	0	0	C8	C7	0	C5	C4	0					
		0	0	0	C9	C8	C7	C6	C5	C4					
		0	0	0	0	C9	C8	0	C6	C5					
Then S consists of all matrices of the form W. S is a 9-dimensional subspace of R9×9 , with an
orthonormal basis with coefficients being the Ci.
19
Under review as a conference paper at ICLR 2021
J Proof of Proposition 3
Proof. For i ∈ [d], let UiΣiViT be a usSVD of Wi witnessing alignment of f. We can then rewrite
Y = f (X) as Y = Ud Qd=ι ∑iVTX, thus proving the desired statement.	□
K Proof of Proposition 4
Before we can prove Proposition 4, we require the following definition from combinatorics.
Definition 5. A partition of an integer k is a tuple λ = (λ1, . . . , λs) such that λi ≥ λi+1 for all i
and k = λι + •…+ λs. Each λi is called a part of λ. We let s(λ) denote the number ofparts of λ
and we write λ ` k to indicate that λ is a partition of k.
Proof of Proposition 4. Given a k × k matrix A, let λ(A) denote the partition λ of k such that λi is
the multiplicity of the ith greatest singular value of A. Let U(A) denote the set of matrices U such
that UΣV T is a usSVD of A. The dimension of U(A) is
s(λ(A))
X	λ2i.
i=1
To see this, note that any orthonormal basis of the eigenspace of AAT corresponding to the
multiplicity-λi eigenvalue of AAT can be the corresponding columns in an element of U(A) and
that the set of orthonormal bases ofan m-dimensional linear space is m2 .
For any set Q of matrices, Define U(Q) to be the set of all possible sets of left-singular vectors of
elements of S . That is,
U(Q) := [ U(A).
A∈Q
For each partition λ of k, let Tλ denote the set of matrices A such that λ(A) = λ. The dimension of
Tλ ∩ S is at most r and therefore the dimension of U(S ∩ Tλ) is at most
r
s(λ)	λ
+Xλ2i.
i=1
Let O(k, n) denote the set of k × n matrices with orthonormal columns. Assume alignment is possible
over S for a non-measure-zero set of matrices with n columns. Then there exists B ⊆ O(k, n) with
dim(B) = dim(O(k, n)) such that for every U0 ∈ B, U(S) contains a matrix whose first n columns
are U0, Therefore dim(U(S)) ≥ dim(O(k,n)). Since dim(O(k,n)) = (2) — (k-n),the following
must be satisfied for some λ ` k
(18)
This is attained when λ = (k), but in this case Tλ is simply the set of scalar multiples of the identity.
If we forbid λ = (k), then we claim that the maximum value of r + Ps=L) (λi) is attained by
λ = (k — 1, 1). To see this, note that for all p < q,
q—2p +
— p(q — p) <
For p > 0, this is maximized when P = 1. This implies that the maximum value of Ps=1) (3)
will be obtained in as few summands as possible (which in our case is two), and in particular when
λ1 = k — 1 and λ2 = 1. In this case, equation 18 becomes
k—1
r+
r+	2
k—n
2
Taking the logical negation of the above inequality and simplifying gives r <k - I-(k-n).	□
20
Under review as a conference paper at ICLR 2021
L Additional Experiments
We provide the following empirical evidence demonstrating that when the conditions of Theorem 1
are satisfied, invariance of alignment can indeed be observed empirically. We use a 2-hidden layer
fully connected network with 9 hidden units per layer.
(a) Autoencoding (X = Y )	(b) Matrix Factorization (X = I) (c) Matrix Inversion (Y = I)
Figure 3: As proven in our work, alignment is an invariant of training when X, Y satisfy the conditions
of Theorem 1.
M Experimental Setup
We provide network architectures and hyperparameters used for our experiments below. We trained
our networks on an NVIDIA TITAN RTX GPU using the PyTorch library. In all settings, we train
using gradient descent with a learning rate of 10-2 until the loss was below 10-4.
1.	Figure 1a: We use a 2-hidden layer fully connected network with 9 hidden units per layer.
Our data is given by matrices (X, Y ) ∈ R9×9 where each matrix entry is drawn from a
standard normal distribution.
2.	Figure 1b: We use a 2-hidden layer fully connected network with 1024 hidden units in the
first hidden layer and 64 hidden units in the second hidden layer. Our data consists of 256
linearly separable examples from MNIST and is trained using Squared Loss.
3.	Figure 1c: We use a 2-hidden layer fully connected network with 1024 hidden units in the
first hidden layer and 64 hidden units in the second hidden layer. Our data consists of 256
linearly separable examples from MNIST and is trained using Cross Entropy Loss.
4.	Figure 2a: We use a 2-hidden layer network with 4 hidden units per layer, where each layer
is constrained to be a Toeplitz matrix. Our input X is equal to the identity, and our output Y
is a 4 × 4 matrix with each entry sampled from a standard normal distribution.
5.	Figure 2b: We use a 2-hidden layer convolutional network with a single 3 × 3 filter in each
layer, stride of 1, and padding of 1. Our data consists of a single example from MNIST.
Code for the experiments can be found at the following anonymized github link: https://
anonymous.4open.science/r/33277cc0-6074-46c4-8642-7feadd678278/.
21