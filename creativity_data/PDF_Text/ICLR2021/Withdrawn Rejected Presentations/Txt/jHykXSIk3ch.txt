Under review as a conference paper at ICLR 2021
A spherical analysis of Adam
with Batch Normalization
Anonymous authors
Paper under double-blind review
Batch Normalization (BN) is a prominent deep learning technique. In spite of its apparent simplicity,
its implications over optimization are yet to be fully understood. While previous studies mostly
focus on the interaction between BN and stochastic gradient descent (SGD), we develop a geometric
perspective which allows us to precisely characterize the relation between BN and Adam. More
precisely, we leverage the radial invariance of groups of parameters, such as filters for convolutional
neural networks, to translate the optimization steps on the L2 unit hypersphere. This formulation and
the associated geometric interpretation shed new light on the training dynamics. Firstly, we use it to
derive the first effective learning rate expression of Adam. Then we show that, in the presence of
BN layers, performing SGD alone is actually equivalent to a variant of Adam constrained to the unit
hypersphere. Finally, our analysis outlines phenomena that previous variants of Adam act on and we
experimentally validate their importance in the optimization process.
1 Introduction
The optimization process of deep neural net-
works is still poorly understood. Their train-
ing involves minimizing a high-dimensional
non-convex function, which has been proved
to be a NP-hard problem (Blum & Rivest, 1989).
Yet, elementary gradient-based methods show
good results in practice. To improve the qual-
ity of reached minima, numerous methods have
stemmed in the last years and become common
practices. One of the most prominent is Batch
Normalization (BN) (Ioffe & Szegedy, 2015),
which improves significantly both the optimiza-
tion stability and the prediction performance; it
is now used in most deep learning architectures.
However, the interaction of BN with optimiza-
tion and its link to regularization remain open
research topics. Previous studies highlighted
mechanisms of the interaction between BN and
SGD, both empirically (Santurkar et al., 2018)
and theoretically (Arora et al., 2019; Bjorck
et al., 2018; Hoffer et al., 2018b). None of them
Figure 1: Illustration of the spherical perspective for
SGD. The loss function L of a NN w.r.t. the parame-
ters xk ∈ Rd of a neuron followed by a BN is radially
invariant. The neuron update xk → xk+1 in the origi-
nal space, with velocity ηk VL(xk), corresponds to an
update uk → uk+1 of its projection through an expo-
nential map on the unit hypersphere Sd-1 with velocity
ηke kVL(uk)k at order 2 (see details in Section 2.3).
studied the interaction between BN and one of the
most common adaptive schemes for Neural Networks (NN), Adam (Kingma & Ba, 2015), except
van Laarhoven (2017), which tackled it only in the asymptotic regime. In this work, we provide an
extensive analysis of the relation between BN and Adam during the whole training procedure.
One of the key effects of BN is to make NNs invariant to positive scalings of groups of parameters.
The core idea of this paper is precisely to focus on these groups of radially-invariant parameters and
analyze their optimization projected on the L2 unit hypersphere (see Fig. 1), which is topologically
equivalent to the quotient manifold of the parameter space by the scaling action. One could directly
optimize parameters on the hypersphere as Cho & Lee (2017), yet, most optimization methods are still
performed successfully in the original parameter space. Here we propose to study an optimization
scheme for a given group of radially-invariant parameters through its image scheme on the unit
hypersphere. This geometric perspective sheds light on the interaction between normalization layers
and Adam, and also outlines an interesting link between standard SGD and a variant of Adam adapted
and constrained to the unit hypersphere: AdamG (Cho & Lee, 2017). We believe this kind of analysis
1
Under review as a conference paper at ICLR 2021
is an important step towards a better understanding of the effect of BN on NN optimization. Please
note that, although our discussion and experiments focus on BN, our analysis could be applied to any
radially-invariant model.
The paper is organized as follows. In Section 2, we introduce our spherical framework to study
the optimization of radially-invariant models. We also define a generic optimization scheme that
encompasses methods such as SGD with momentum (SGD-M) and Adam. We then derive its image
step on the unit hypersphere, leading to definitions and expressions of effective learning rate and
effective learning direction. This new definition is explicit and has a clear interpretation, whereas the
definition of van Laarhoven (2017) is asymptotic and the definitions of Arora et al. (2019) and of
Hoffer et al. (2018b) are variational. In Section 3, we leverage the tools of our spherical framework
to demonstrate that in presence of BN layers, SGD has an adaptive behaviour. Formally, we show
that SGD is equivalent to AdamG, a variant of Adam adapted and constrained to the hypersphere,
without momentum. In Section 4, we analyze the effective learning direction for Adam. The spherical
framework highlights phenomena that previous variants of Adam (Loshchilov & Hutter, 2017; Cho &
Lee, 2017) act on. We perform an empirical study of these phenomena and show that they play a
significant role in the training of convolutional neural networks (CNNs). In Section 5, these results
are put in perspective with related work.
Our main contributions are the following:
•	A framework to analyze and compare order-1 optimization schemes of radially-invariant models;
•	The first explicit expression of the effective learning rate for Adam;
•	The demonstration that, in the presence of BN layers, standard SGD has an adaptive behaviour;
•	The identification and study of geometrical phenomena that occur with Adam and impact signifi-
cantly the training of CNNs with BN.
2 Spherical framework and effective learning rate
In this section, we provide background on radial invariance and introduce a generic optimization
scheme.
Projecting the scheme update on the unit hypersphere leads to the formal definitions of effective
learning rate and learning direction. This geometric perspective leads to the first explicit expression
of the effective learning rate for Adam. The main notations are summarized in Figure 1.
2.1	Radial invariance
We consider a family of parametric functions φx : Rin → Rout parameterized by a group of radially-
invariant parameters x ∈ Rd r {0}, i.e., ∀ρ > 0, φρx = φx (possible other parameters of φx are
omitted for clarity), a dataset D ⊂ Rin × Rout, a loss function ` : Rout × Rout → R and a training loss
function L : Rd → R defined as:
L(X) =f |D| X '(Φχ(s),t).
(s,t)∈D
(1)
It verifies: ∀ρ > 0, L(ρX) = L(X). In the context of NNs, the group of radially-invariant parameters
X can be the parameters of a single neuron in a linear layer or the parameters of a whole filter in a
convolutional layer, followed by BN (see Appendix A for details, and Appendix B for the application
to other normalization schemes such as InstanceNorm (Ulyanov et al., 2016), LayerNorm (Ba et al.,
2016) or GroupNorm (Wu & He, 2018)).
The quotient of the parameter space by the equivalence relation associated to radial invariance is
topologically equivalent to a sphere. We consider here the L2 sphere Sd-1 = {u ∈ Rd/kuk2 = 1}
whose canonical metric corresponds to angles: dS(u1, u2) = arccos(hu1, u2i). This choice of
metric is relevant to study NNs since filters in CNNs or neurons in MLPs are applied through scalar
product to input data. Besides, normalization in BN layers is also performed using the L2 norm.
Our framework relies on the decomposition of vectors into radial and tangential components. During
optimization, we write the radially-invariant parameters at step k ≥ 0 as Xk = rkuk where rk = kXk k
and uk = Xk/kXk k. For any quantity qk ∈ Rd at step k, we write qk⊥ = qk-hqk, ukiuk its
tangential component relatively to the current direction uk .
2
Under review as a conference paper at ICLR 2021
The following lemma states that the gradient of a radially-invariant loss function is tangential and -1
homogeneous:
Lemma 1 (Gradient of a function with radial invariance). If L : Rd → R is radially invariant and
almost everywhere differentiable, then, for all ρ > 0 and all x ∈ Rd where L is differentiable:
hVL(x), Xi = 0 and VL(x) = P VL(ρx).	(2)
2.2	Generic optimization scheme
There is a large body of literature on optimization schemes (Sutskever et al., 2013; Duchi et al., 2011;
Tieleman & Hinton, 2012; Kingma & Ba, 2015; Loshchilov & Hutter, 2019). We focus here on two
of the most popular ones, namely SGD and Adam (Kingma & Ba, 2015). Yet, to establish general
results that may apply to a variety of other schemes, we introduce here a generic optimization update:
Xk+1 = Xk - ηkak	bk,	(3)
ak = βak-1 + VL(Xk) + λXk,	(4)
where Xk ∈ Rd is the group of radially-invariant parameters at iteration k, L is the group’s loss
estimated on a batch of input data, ak ∈ Rd is a momentum, bk ∈ Rd is a division vector that
can depend on the trajectory (Xi, VL(Xi))i∈J0,kK , ηk ∈ R is the scheduled trajectory-independent
learning rate, denotes the Hadamard element-wise division, β is the momentum parameter, and λ is
the L2-regularization parameter. We show how it encompasses several known optimization schemes.
Stochastic gradient descent (SGD) has proven to be an effective optimization method in deep learning.
It can include L2 regularization (also called weight decay) and momentum. Its updates are:
Xk+1 = Xk - ηkmk,	(5)
mk = βmk-1 + VL(Xk) + λXk,	(6)
where mk is the momentum, β is the momentum parameter, and λ is the L2-regularization parameter.
It corresponds to our generic scheme (Eqs. 3-4) with ak = mk and bk = [1 ∙ ∙ ∙ 1]>.
Adam is likely the most common adaptive scheme for NNs. Its updates are:
—	—	mk	v^^vk	-	C
xk+1 = xk-ηk T-βkk+ ° vɪ-β∣+τ +	7
mk =	β1mk-1 +(1 -	β1)(VL(Xk) +	λXk), vk	=	β2vk-1	+ (1 -	β2)(VL(Xk)	+ λXk)2,	(8)
where mk is the momentum with parameter β1, vk is the second-order moment with parameter β2,
and prevents division by zero. (Here and in the following, the square and the square root of a vector
are to be understood as element-wise.) It corresponds to our generic scheme (Eqs. 3-4) with β=β1
and:
mk	ι 1 — βk+1	/ vk	小、
ak =-------, bk = ------------, -------+^ + €.	(9)
1 — βι,	1 — βι 1— — βk+1 +	()
2.3 Image optimization on the hypersphere
The radial invariance implies that the radial part of the parameter update X does not change the
function φx encoded by the model, nor does it change the loss L(X). The goal of training is to find
the best possible function encodable by the network. Due to radial invariance, the parameter space
projected on the unit hypersphere is topologically closer to the functional space of the network than
the full parameter space. It hints that looking at optimization behaviour on the unit hypersphere might
be interesting. Thus, we need to separate the quantities that can (tangential part) and cannot (radial
part) change the model function. Theorem 2 formulates the spherical decomposition (Eqs. 3-4) in
simple terms. It relates the update of radially-invariant parameters in the parameter space Rd and
their update on Sd-1 through an exponential map.
Theorem 2 (Image step on Sd-1). The update of a group of radially-invariant parameters Xk at
step k corresponds to an update of its projection uk on Sd-1 through an exponential map at uk with
velocity ηke ck⊥, at order 3:
uk+ι = EXPuk (― h1 + O ((ηekc⊥ll)2)i ηec⊥),	(10)
3
Under review as a conference paper at ICLR 2021
where Expuk is the exponential map on Sd-1, and with
c, d=f r, a,	© bk	ηe	d=f ηk	(1	_ ηk hCk，Uki A	1
d-1/2kbkk, k rkd-1/2kbkk	I rkd-1/2kbk 11/
(11)
More precisely:
Uk+1
Uk — ηec⊥
qι + (ηe kc⊥k)2
(12)
The proof is given in Appendix C.1.1 and the theorem is illustrated in the case of SGD in Figure 1.
Note that with typical values in CNN training We have 1- InkR,uki > 0, which is a property
rk2 d- /2 kbk k
needed for the proof. Another hypothesis is that steps on the hypersphere are shorter than π. These
hypotheses are discussed and empirically verified in Appendix C.1.2.
2.4 Effective learning rate for Adam
In Theorem 2, the normalized parameters update in Eq. 10 can be read Uk+1 ≈ Expu -ηkeck⊥ ,
where ηke and ck⊥ can then be respectively interpreted as the learning rate and the direction of an
optimization step constrained to Sd-1 since ak is the momentum and, with Lemma 1, the quantity
rk ak in ck can be seen as a momentum on the hypersphere. Due to the radial invariance, only the
change of parameter on the unit hypersphere corresponds to a change of model function. Hence we
can interpret ηke and ck⊥ as effective learning rate and effective learning direction. In other words,
these quantities correspond to the learning rate and direction on the hypersphere that reproduce the
function update of the optimization step.
Using Theorem 2, we can derive actual effective learning
rates for any optimization scheme that fits our generic
framework. These expressions, summarized in Table 1
are explicit and have a clear interpretation, in contrast
to learning rates in (van Laarhoven, 2017), which are
approximate and asymptotic, and in (Hoffer et al., 2018a;
Arora et al., 2019), which are variational and restricted
to SGD without momentum only.
In particular, we provide the first explicit expression of
the effective learning rate for Adam:
ηke
ηkhck,Uki	-1
Table 1: Effective learning rate and direc-
tion for optimization schemes (k omitted),
with ν = rd-1/2 kbk.
Scheme	ηe		c⊥
SGD		ɪ r2	▽L(U)
SGD + L2		η	▽L(U)
	r	∙2(i-ηλ)	
SGD-M	η (1	ηhcu ∖-ι r2-)	c⊥
Adam	rν a	- ηhcu )-1 rν	c⊥
rνk
(13)
where νk = rkd-1/2kbk k is homogeneous to the norm of a gradient on the hypersphere and can be
related to an second-order moment on the hypersphere (see Appendix.C.1.3 for details). This notation
also simplifies the in-depth analysis in Section 4, allowing a better interpretation of formulas.
The expression of the effective learning rate of Adam, i.e., the amplitude of the step taken on the
hypersphere, reveals a dependence on the dimension d (through ν) of the considered group of radially-
invariant parameters. In the case of an MLP or CNN that stacks layers with neurons or filters of
different dimensions, the learning rate is thus tuned differently from one layer to another.
We can also see that for all schemes the learning rate is tuned by the dynamics of radiuses rk, which
follow:
ηk hck, Uki A q
r2d-1/2 kbk k N
1 + (ηkekck⊥k)2.
(14)
In contrast to previous studies (Arora et al., 2019; van Laarhoven, 2017), this result demonstrates that
for momentum methods, hck, Uki, which involves accumulated gradients terms in the momentum as
well as L2 regularization, tunes the learning rate (cf. Fig.1).
4
Under review as a conference paper at ICLR 2021
3 SGD is a variation of Adam on the hypersphere
We leverage the tools introduced in the spherical framework to find a scheme constrained to the
hypersphere that is equivalent to SGD. It shows that for radially-invariant models, SGD is actually an
adaptive optimization method. Formally SGD is equivalent to a version of AdamG, a variation of
Adam adapted and constrained to the unit hypersphere, without momentum.
3.1	Equivalence between two optimization schemes
Due to the radial invariance, the functional space of the model is encoded by Sd-1. In other words,
two schemes with the same sequence of groups of radially-invariant parameters on the hypersphere
(uk)k≥0 encode the same sequence of model functions. Two optimization schemes S and S are
equivalent iff ∀k ≥ 0, Uk = Uk. By using Eq. 12, We obtain the following lemma, which is useful to
prove the equivalence of two given optimization schemes:
Lemma 3 (Sufficient condition for the equivalence of optimization schemes).
U0 = U0
∀k ≥ 0,ηe = ηe,c⊥ = c⊥
⇒∀k ≥ 0, Uk = Uk.
(15)
3.2	A hypersphere-constrained scheme equivalent to SGD
We now study, within our spherical framework, SGD with L2 regularization, i.e., the update xk+1 =
Xk - ηk (▽£(Xk) - λkXk). From the effective learning rate expression, we know that SGD yields an
adaptive behaviour because it is scheduled by the radius dynamic, which depends on gradients. In
fact, the tools in our framework allow us to find a scheme constrained to the unit hypersphere that is
equivalent to SGD: AdamG (Cho & Lee, 2017). More precisely, it is AdamG with a null momentum
factor β1 = 0, an non-null initial second-order moment v0, an offset of the scalar second-order
moment k + 1 → k and the absence of the bias correction term 1 - β2k+1.Dubbed AdamG* this
scheme reads:
(AdamG*) :
xk+1
Xk+1
vk+1
▽L(Xk)
χk - η ^⅛k2,
xk + 1
llxk+ιk,
βvk + kVL(xk )k2.
Starting from SGD, we first use Lemma 3 to find an equivalence scheme with simpler radius dynamic.
We resolve this radius dynamic with a Taylor expansion at order 2 in (ηk kVL(Uk)k)2/rk2. A
second use of Lemma 3 finally leads to the following scheme equivalence in Theorem (see proof in
Appendix C.1.4). If we call « equivalent at order 2 in the step » a scheme equivalence that holds
when we use for rk an expression that satisfies the radius dynamic with a Taylor expansion at order 2
we have the following theorem:
Theorem 4 (SGD equivalent scheme on the unit hypersphere). For any λ > 0, η > 0, r0 > 0, we
have the following equivalence when using the radius dynamic at order 2 in (ηk kVL(Uk)k)2 /rk2:
(SGD)	((AdamG*)
(SG= r u	l χo=uo
X0	r0U0 is scheme-equivalent at order 2 in step with β = (1 - ηλ)4
λk = λ	l ηk = (2β)-1/2
l V0 = r4(2η2β 1∕2)T.
This result is unexpected because SGD, which is not adaptive by itself, is equivalent to a second
order moment adaptive method The scheduling performed by the radius dynamics actually replicates
the effect of dividing the learning rate by the second-order moment of the gradient norm: vk. First,
the only assumption for this equivalence is to neglect the approximation in the Taylor expansion at
order 2 of the radius which is highly verified in practice (order of magnitude of 1e - 4 isee Appendix
C.1.5). Second, with standard values of the hyper-parameters : learning rate η < 1 and weight decay
λ < 1, we have β ≤ 1 which corresponds to a standard value for a moment factor. Interestingly, the
L2 regularization parameter λ controls the memory of the past gradients norm. If β = 1 (with λ = 0)
there is no attenuation, each gradient norm has the same contribution in the order of two moments. If
λ 6= 0, there is a decay factor (β < 1) on past gradients norm in the order 2 moment.
5
Under review as a conference paper at ICLR 2021
4 Geometric phenomena in Adam
Our framework with its geometrical interpretation reveals intriguing behaviors occurring in Adam.
The unit hypersphere is enough to represent the functional space encoded by the network. From the
perspective of manifold optimization, the optimization direction would only depend on the trajectory
on that manifold. In the case of Adam, the effective direction not only depends on the trajectory on
the hypersphere but also on the deformed gradients and additional radial terms. These terms are thus
likely to play a role in Adam optimization.
In order to understand their role, we describe these geometrical phenomena in Section 4.1. Inter-
estingly, previous variants of Adam, AdamW (Loshchilov & Hutter, 2017) and AdamG (Cho &
Lee, 2017) are related to these phenomena. To study empirically their importance, we consider in
Section 4.2 variants of Adam that first provide a direction intrinsic to the unit hypersphere, without
deformation of the gradients, and then where radial terms are decoupled from the direction. The
empirical study of these variants over a variety of datasets and architectures suggests that these
behaviors do play a significant role in CNNs training with BN.
4.1	Identification of geometrical phenomena in Adam
Here, we perform an in-depth analysis of the effective learning direction of Adam.
(a)	Deformed gradients. Considering the quantities defined for a generic scheme in Eq. 11, bk
has a deformation effect on ak, due to the Hadamard division by —r-bk——
d-1/2kbkk
and a scheduling effect
d-1/2 kbk k on the effective learning rate. In the case where the momentum factor is null β1 = 0, the
direction of the update at step k is NL(Uk) 0广六匕 口 (Eq. 11) and the deformation「肃匕 ∣∣ may
push the direction of the update outside the tangent space of Sd-1 at uk, whereas the gradient itself
lies in the tangent space. This deformation is in fact not isotropic: the displacement of the gradient
from the tangent space depends on the position of uk on the sphere. We illustrate this anisotropy in
Fig. 2(b).
(b)	Additional radial terms. In the momentum on the sphere ck , quantities that are radial (resp.
orthogonal) at a point on the sphere may not be radial (resp. orthogonal) at another point. To clarify
the contribution of ck in the effective learning direction ck⊥, we perform the following decomposition
(cf. Appendix D.1):
Ck = (Cgrad + λrkcL2)0 "篙［ With:	(16)
k-1
cgrad =f VL(Uk) + Xβk-irkVL(Ui)
i=0	ri 2
and
ckL2
def
Uk +
k-1
(17)
1. Contribution of cgkrad . At step k, the contribution of each past gradient corresponds to the
orthogonal part VL(Ui) - hVL(Ui), UkiUk. It impacts the effective learning direction depending
on its orientation relatively to Uk. TWo past points, although equally distant from Uk on the sphere
and With equal gradient amplitude may thus contribute differently in ck⊥ due to their orientation (cf.
Fig. 2(c)).
2. Contribution of ckL2. Naturally, the current point Uk does not contribute to the effective
learning direction c⊥, unlike the history of points in Pi=0 βk~-riUi, which does. This dependency
can be avoided if We decouple the L2 regularization, in Which case We do not accumulate L2 terms in
the momentum. This shows that the decoupling proposed in AdamW (Loshchilov & Hutter, 2019)
actually removes the contribution of L2 regularization in the effective learning direction.
(C) The radius ratio rk present in both Ckrad and cL2 (in inverse proportion) impacts the effective
learning direction ck⊥: it can differ for identical sequences (Ui)i≤k on the sphere but with distinct
radius histories (ri)i≤k. Since the radius is closely related to the effective learning rate, it means that
the effective learning direction ck⊥ is adjusted according to the learning rates history.
Note that AdamG (Cho & Lee, 2017), by constraining the optimization to the unit hypersphere and
thus removing L2 regularization, neutralizes all the above phenomena. However, this method has no
scheduling effect allowed by the radius dynamics (cf. Eq.14) since it is kept constant during training.
6
Under review as a conference paper at ICLR 2021
(a) Radial scheduling
(b) Anisotropy of deformation
(c) Gradient-history contribution
Figure 2: (a) Effect of the radial part of ck on the displacement on Sd-1 ; (b) Example of anisotropy and
sign instability for the deformation ψ(VL(uk)) = VL(uk) 0	ULUk"
d k k^L(uk) k
(where | ∙ | is the element-wise
absolute value) occurring in Adam’s first optimization step; (c) Different contribution in ck⊥ of two past gradients
V1 and V2 of equal norm, depending on their orientation. Illustration of the transport of V1 from uk-1 to uk :
Γuukk-1 (V1) (cf. Appendix D.2 for details)
4.2 Empirical study
To study empirically the importance of the identified geometric phenomena, we perform an ablation
study: we compare the performance (accuracy and training loss speed) of Adam and variants that
neutralize each of them. We recall that AdamW neutralizes (b2) and that AdamG neutralizes all of
above phenomena but loses the scheduling effect identified in Eq. 14. To complete our analysis, we
use geometrical tools to design variations of Adam which neutralizes sequentially each phenomenon
while preserving the natural scheduling effect in Theorem 2. We neutralize (a) by replacing the
element-wise second-order moment, (b1) and (b2) by transporting the momentum from a current
point to the new one, (c) by re-scaling the momentum at step k. The details are in Appendix. D.2.
The final scheme reads:
χk+1=Xk- ηk ι	/r ι -Vkk+1+e,	(18)
mk = β1 r 1 ruk-1 (mk-1 ) + (1 - β1 )(VL(χk) + λχk),	(19)
r2
Vk = β2 j-f Vk-1 + (1 - β2)d-1 kVL(xk) + λxkk2,	(20)
rk
where Γuuk	is the hypersphere canonical transport from uk-1 to uk. Implementation details are in
Appendix D.3.
Protocol. For evaluation, we conduct experiments on two architectures: VGG16 (Simonyan &
Zisserman, 2015) and ResNet (He et al., 2016) - more precisely ReSNet20, a simple variant designed
for small images (He et al., 2016), and ResNet18, a popular variant for image classification. We
consider three datasets: SVHN (Netzer et al., 2011), CIFAR10 and CIFAR100 (Krizhevsky et al.,
2009).
Since our goal is to evaluate the significance of phenomena on radially-invariant parameters, i.e., the
convolution filters followed by BN, we only apply variants of Adam including AdamG and AdamW
on convolution layers. For comparison consistency, we keep standard Adam on the remaining
parameters. We also use a fixed grid hyperparameter search budget and frequency for each method
and each architecture (see Appendix D.3 for details).
Results. In Table 2 we report quantitative results of Adam variants across architectures and datasets.
In addition, we compare the evolution of the training loss in Fig. 3. We observe that each phenomenon
displays a specific trade-off between generalization (accuracy on the test set) and training speed, as
following. Neutralizing (a) has little effect on the speed over Adam, yet achieves better accuracy.
Although it slows down training, neutralizing (ab) leads to minima with the overall best accuracy
on test set. Note that AdamWt neutralizes (b2) with its decoupling and is the fastest method, but
finds minima with overall worst generalization properties. By constraining the optimization to the
hypersphere, AdamGt speeds up training over the other variants. Finally, neutralizing (c) with Adam
7
Under review as a conference paper at ICLR 2021
Figure 3: Training speed comparison with ResNet20 on CIFAR10. Left: Mean training loss over all training
epochs (averaged across 5 seeds) for different Adam variants. Right: Zoom-in on the last epochs. Please refer to
Table 2 for the corresponding accuracies.
w/o (abc) brings a slight acceleration, though reaches lower accuracy than Adam w/o (ab). We can
see that the revealed geometrical phenomena impact substantially training of BN-equipped CNNs.
Table 2: Accuracy of Adam and its variants. The figures in this table are the mean top1 accuracy ± the
standard deviation over 5 seeds on the test set for CIFAR10, CIFAR100 and on the validation set for SVHN.
tindicates that the original method is only used on convolutional filters while Adam is used for other parameters.
Method	ResNet20	CIFAR10 ResNet18	VGG16	CIFAR100		SVHN	
				ResNet18	VGG16	ResNet18	VGG16
Adam	90.98 ± 0.06	93.77 ± 0.20	92.83 ± 0.17	71.30 ± 0.36	68.43 ± 0.16	95.32 ± 0.23	95.57 ± 0.20
AdamWt	90.19 ± 0.24	93.61 ± 0.12	92.53 ± 0.25	67.39 ± 0.27	71.37 ± 0.22	95.13 ± 0.15	94.97 ± 0.08
AdamGt	91.64 ± 0.17	94.67 ± 0.12	93.41 ± 0.17	73.76 ± 0.34	70.17 ± 0.20	95.73 ± 0.05	95.70 ± 0.25
Adam w/o (a)	91.15 ± 0.11	93.95 ± 0.23	92.92 ± 0.11	74.44 ± 0.22	68.73 ± 0.27	95.75 ± 0.09	95.66 ± 0.09
Adam w/o (ab)	91.92 ± 0.18	95.11 ± 0.10	93.89 ± 0.09	76.15 ± 0.25	71.53 ± 0.19	96.05 ± 0.12	96.22 ± 0.09
Adam w/o (abc)	91.81 ± 0.20	94.92 ± 0.05	93.75 ± 0.06	75.28 ± 0.35	71.45 ± 0.13	95.84 ± 0.07	95.82 ± 0.05
5	Related work
Understanding Batch Normalization. Albeit conceptually simple, BN has been shown to have
complex implications over optimization. The argument of Internal Covariate Shift reduction (Ioffe
& Szegedy, 2015) has been challenged and shown to be secondary to smoothing of optimization
landscape (Santurkar et al., 2018; Ghorbani et al., 2019) or its modification by creating a different
objective function (Lian & Liu, 2019), or enabling of high learning rates through improved condition-
ing (Bjorck et al., 2018). Arora et al. (2019) demonstrate that (S)GD with BN is robust to the choice
of the learning rate, with guaranteed asymptotic convergence, while a similar finding for GD with
BN is made by Cai et al. (2019).
Invariances in neural networks. Cho & Lee (2017) propose optimizing over the Grassmann mani-
fold using Riemannian GD. Liu et al. (2017) project weights and activations on the unit hypersphere
and compute a function of the angle between them instead of inner products, and subsequently
generalize these operators by scaling the angle (Liu et al., 2018). In (Li & Arora, 2020) the radial in-
variance is leveraged to prove that weight decay (WD) can be replaced by an exponential learning-rate
scheduling for SGD with or without momentum. Arora et al. (2019) investigate the radial invariance
and show that radius dynamics depends on the past gradients, offering an adaptive behavior to the
learning rate. Here we go further and show that SGD projected on the unit hypersphere corresponds
to Adam constrained to the hypersphere, and we give an accurate definition of this adaptive behavior.
Effective learning rate. Due to its scale invariance, BN can adaptively adjust the learning rate (van
Laarhoven, 2017; Cho & Lee, 2017; Arora et al., 2019; Li & Arora, 2020). van Laarhoven (2017)
shows that in BN-equipped networks, WD increases the effective learning rate by reducing the norm
of the weights. Conversely, without WD, the norm grows unbounded (Soudry et al., 2018), decreasing
the effective learning rate. Zhang et al. (2019) brings additional evidence supporting hypothesis in
van Laarhoven (2017), while Hoffer et al. (2018a) finds an exact formulation of the effective learning
rate for SGD in normalized networks. In contrast with prior work, we find generic definitions of the
effective learning rate with exact expressions for SGD and Adam.
8
Under review as a conference paper at ICLR 2021
6	Conclusion
The spherical framework introduced in this study provides a powerful tool to analyse Adam optimiza-
tion scheme through its projection on the L2 unit hypersphere. It allows us to give a precise definition
and expression of the effective learning rate for Adam, to relate SGD to a variant of Adam, and to
identify geometric phenomena which empirically impact training. The framework also brings light to
existing variations of Adam, such as L2-regularization decoupling. This approach could be extended
to other invariances in CNNs such as as filter permutation.
References
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch
normalization. In International Conference on Learning Representations (ICLR), 2019.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Nils Bjorck, Carla P Gomes, Bart Selman, and Kilian Q Weinberger. Understanding batch nor-
malization. In Advances in Neural Information Processing Systems (NeurIPS), pp. 7694-7705,
2018.
Avrim Blum and Ronald L Rivest. Training a 3-node neural network is np-complete. In Advances in
Neural Information Processing Systems (NeurIPS), pp. 494-501, 1989.
Yongqiang Cai, Qianxiao Li, and Zuowei Shen. A quantitative analysis of the effect of batch
normalization on gradient descent. In 36th International Conference on Machine Learning (ICML),
volume 97, 2019.
Minhyung Cho and Jaehyung Lee. Riemannian approach to batch normalization. In Advances in
Neural Information Processing Systems (NeurIPS), pp. 5225-5235, 2017.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of Machine Learning Research (JMLR), 12(Jul):2121-2159, 2011.
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao. An investigation into neural net optimization
via hessian eigenvalue density. In 36th International Conference on Machine Learning (ICML),
volume 97, pp. 2232-2241, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016.
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry. Norm matters: efficient and accurate
normalization schemes in deep networks. In Advances in Neural Information Processing Systems
(NeurIPS), pp. 2160-2170, 2018a.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Fix your classifier: the marginal value of training the
last weight layer. arXiv preprint arXiv:1801.04540, 2018b.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In 32nd International Conference on Machine Learning (ICML),
volume 37, pp. 448-456, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.
Zhiyuan Li and Sanjeev Arora. An exponential learning rate schedule for deep learning. In Interna-
tional Conference on Learning Representations (ICLR), 2020.
Xiangru Lian and Ji Liu. Revisit batch normalization: New understanding and refinement via
composition optimization. In The 22nd International Conference on Artificial Intelligence and
Statistics (AISTATS), pp. 3254-3263, 2019.
9
Under review as a conference paper at ICLR 2021
Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep
hyperspherical learning. In Advances in Neural Information Processing S ystems (NeurIPS), pp.
3950-3960, 2017.
Weiyang Liu, Zhen Liu, Zhiding Yu, Bo Dai, Rongmei Lin, Yisen Wang, James M Rehg, and Le Song.
Decoupled networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 2771-2779, 2018.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101, 2017.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations (ICLR), 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch normal-
ization help optimization? In Advances in Neural Information Processing Systems (NeurIPS), pp.
2483-2493, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations (ICLR), 2015.
Daniel Soudry, Elad Hoffer, Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro. The implicit
bias of gradient descent on separable data. The Journal of Machine Learning Research (JMLR), 19
(1):2822-2878, 2018.
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization
and momentum in deep learning. In 30th International Conference on Machine Learning (ICML),
volume 28, pp. 1139-1147, Atlanta, Georgia, USA, 17-19 Jun 2013.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-31,
2012.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing
ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Twan van Laarhoven. L2 regularization versus batch and weight normalization, 2017. arXiv preprint
arXiv:1706.05350.
Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on
Computer Vision (ECCV), pp. 3-19, 2018.
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse. Three mechanisms of weight decay
regularization. In International Conference on Learning Representations (ICLR), 2019.
10