Under review as a conference paper at ICLR 2021
Distribution-Based Invariant Deep Networks
for Learning Meta-Features
Anonymous authors
Paper under double-blind review
Ab stract
Recent advances in deep learning from probability distributions successfully
achieve classification or regression from distribution samples, thus invariant under
permutation of the samples. Thefirst contribution of the paper is to extend these
neural architectures to achieve invariance under permutation of the features, too.
The proposed architecture, called Dida, inherits the NN properties of universal
approximation, and its robustness with respect to Lipschitz-bounded transforma-
tions of the input distribution is established. The second contribution is to em-
pirically and comparatively demonstrate the merits of the approach on two tasks
defined at the dataset level. On both tasks, Dida learns meta-features supporting
the characterization of a (labelled) dataset. Thefirst task consists of predicting
whether two dataset patches are extracted from the same initial dataset. The sec-
ond task consists of predicting whether the learning performance achieved by a
hyper-parameter configuration under afixed algorithm (ranging in k-NN, SVM,
logistic regression and linear SGD) dominates that of another configuration, for
a dataset extracted from the OpenML benchmarking suite. On both tasks, Dida
outperforms the state of the art: DSS and Dataset2Vec architectures, as well as
the models based on the hand-crafted meta-features of the literature.
1	Introduction
Deep networks architectures, initially devised for structured data such as images (Krizhevsky et al.,
2012) and speech (Hinton et al., 2012), have been extended to enforce some invariance or equivari-
ance properties (Shawe-Taylor, 1993) for more complex data representations. Typically, the network
output is required to be invariant with respect to permutations of the input points when dealing with
point clouds (Qi et al., 2017), graphs (Henaff et al., 2015) or probability distributions (De Bie et al.,
2019). The merit of invariant or equivariant neural architectures is twofold. On the one hand, they
inherit the universal approximation properties of neural nets (Cybenko, 1989; Leshno et al., 1993).
On the other hand, the fact that these architectures comply with the requirements attached to the data
representation yields more robust and more general models, through constraining the neural weights
and/or reducing their number.
Related works.Invariance or equivariance properties are relevant to a wide range of applications.
In the sequence-to-sequence framework, one might want to relax the sequence order (Vinyals et al.,
2016). When modelling dynamic cell processes, one might want to follow the cell evolution at
a macroscopic level, in terms of distributions as opposed to, a set of individual cell trajectories
(Hashimoto et al., 2016). In computer vision, one might want to handle a set of pixels, as opposed
to a voxellized representation, for the sake of a better scalability in terms of data dimensionality and
computational resources (De Bie et al., 2019).
Neural architectures enforcing invariance or equivariance properties have been pioneered by (Qi
et al., 2017; Zaheer et al., 2017) for learning from point clouds subject to permutation invariance
or equivariance. These have been extended to permutation equivariance across sets (Hartford et al.,
2018). Characterizations of invariance or equivariance under group actions have been proposed in
thefinite (Gens & Domingos, 2014; Cohen & Welling, 2016; Ravanbakhsh et al., 2017) or infinite
case (Wood & Shawe-Taylor, 1996; Kondor & Trivedi, 2018).
1
Under review as a conference paper at ICLR 2021
On the theoretical side, (Maron etal., 2019a; Keriven & Peyre, 2019) have proposed a general char-
acterization of linear layers enforcing invariance or equivariance properties with respect to the whole
permutation group on the feature set. The universal approximation properties of such architectures
have been established in the case of sets (Zaheer et al., 2017), point clouds (Qi et al., 2017), equivari-
ant point clouds (Segol & Lipman, 2019), discrete measures (De Bie et al., 2019), invariant (Maron
et al., 2019b) and equivariant (Keriven & Peyre, 2019) graph neural networks. The approach most
related to our work is that of (Maron et al., 2020), handling point clouds and presenting a neural
architecture invariant w.r.t. the ordering of points and their features. In this paper, the proposed
distribution-based invariant deep architecture(D IDA) extends (Maron et al., 2020) as it handles
(discrete or continuous) probability distributions instead of point clouds. This enables to leverage
the topology of the Wasserstein distance to provide more general approximation results, covering
(Maron et al., 2020) as a special case.
Motivations.A main motivation for D IDA is the ability to characterize datasets throughlearned
meta-features. Meta-features, aimed to represent a dataset as a vector of characteristics, have been
mentioned in the ML literature for over 40 years, in relation with several key ML challenges: (i)
learning a performance model, predictinga priorithe performance of an algorithm (and the hyper-
parameters thereof) on a dataset (Rice, 1976; Wolpert, 1996; Hutter et al., 2018); (ii) learning a
generic model able of quick adaptation to new tasks, e.g. one-shot or few-shot, through the so-called
meta-learning approach (Finn et al., 2018; Yoon et al., 2018); (iii) hyper-parameter transfer learning
(Perrone et al., 2018), aimed to transfer the performance model learned for a task, to another task.
A large number of meta-features have been manually designed along the years (Munoz et al., 2018),
ranging from sufficient statistics to the so-calledlandmarks(Pfahringer et al., 2000), computing the
performance of fast ML algorithms on the considered dataset. Meta-features, expected to describe
the joint distribution underlying the dataset, should also be inexpensive to compute. The learning
of meta-features has beenfirst tackled by (Jomaa et al., 2019) to our best knowledge, defining the
Dataset2Vec representation. Specifically, Dataset2Vec is provided two patches of datasets,
(two subsets of examples, described by two (different) sets of features), and is trained to predict
whether those patches are extracted from the same initial dataset.
Contributions.The proposed D IDA approach extends the state of the art (Maron et al., 2020;
Jomaa et al., 2019) in two ways. Firstly, it is designed to handle discrete or continuous probability
distributions, as opposed to point sets (Section 2). As said, this extension enables to leverage the
more general topology of the Wasserstein distance as opposed to that of the Haussdorf distance
(Section 3). This framework is used to derive theoretical guarantees of stability under bounded
distribution transformations, as well as universal approximation results, extending (Maron et al.,
2020) to the continuous setting. Secondly, the empirical validation of the approach on two tasks
defined at the dataset level demonstrates the merit of the approach compared to the state of the art
(Maron et al., 2020; Jomaa et al., 2019; Mufioz et al., 2018) (Section 4).
Notations.1 ;mdenotes the set of integers{1, . . . m}. Distributions, including discrete distri-
butions (datasets) are noted in bold font. Vectors are noted in italic, withx[k]denoting thek-th
coordinate of vectorx.
2	Distribution-Based Invariant Networks for Meta- Feature
Learning
This section describes the core of the proposed distribution-based invariant neural architectures,
specifically the mechanism of mapping a point distribution onto another one subject to sample and
feature permutation invariance, referred to asinvariant layer. For the sake of readability, this section
focuses on the case of discrete distributions, referring the reader to Appendix A for the general case
of continuous distributions.
2.1	Invariant Functions of Discrete Distributions
Letz={(x i, yi)∈R d, i∈1 ;n}denote a dataset includingnlabelled samples, withx	i ∈R dX
an instance andy i ∈R dY the associated multi-label. Withd X andd Y respectively the dimensions
2
Under review as a conference paper at ICLR 2021
def.
of the instance and label spaces, letd =d X +dY . By construction,zis invariant under permutation
on the sample ordering; it is viewed as an n--sZze dSscreee dSsrτibUiOOn 1 En=I δzi in Rd with δZi the
Dirac function atz i. In the following, Zn(Rd)denotes the space of suchn-size point distributions,
with Z(Rd) d=ef.∪ nZn(Rd)the space of distributions of arbitrary size.
LetG d=ef.S dX ×S dY denote the group of permutations independently operating on the feature
and label spaces. Forσ= (σ X,σY )∈G, the imageσ(z)of a labelled sample is defined as
(σχ(x),σγ(y)), with = = (k][⅛]∈ k ∈ [1; Xx]) and GX(x)吧(x[σχ(k)],k∈[l; Xx力.For
simplicity and by abuse of notations, the operator mapping a distributionz= (z i, i∈1 ;n)to
{d(zi)} d=ef.d zis still denotedd.
Let Z(Ω) denote hie SPaCe of distributions SUPPorted On some domain Ω⊂ R 肽d, with QivaariantUn-
der permutations inG. The goal of the paper is to define and train deep architectures, implementing
functions g On Z(。R 肽d) that are nnvariant Under G, Lessuchthat dσ e Gl g(CrHz)=夕Z)1. By
construction, a multi-laiel dataset is invariant under permutations of the samples, of the features, and
of the multi-laiels. Therefore, any meta-feature, that is, a feature descriiing a multi-laiel dataset, is
required to satisfy the aiove sample and feature permutation invariance properties.
2.2	Distribution- Based Invariant Layers
The iuilding ilock of the proposed architecture, the invariant layer meant to satisfy the feature and
laiel invariance requirements, is defined as follows, taking inspiration from (De Bie et al., 2019).
Definition 1. (Dstributon--baeedEaaiannt Iyyrr)LLetnnnneractonnunCtonalI g：IRLd XRd → Rr
beG-invariant:
Vd σ G, V(N1,Z2)σ 肽d × Rd, HN1,Z2)= Hc((N 1)[d(z2)).
Thy dirtribution-bared invariant layer f φ ir defined ar
fφ z= (Ni)iE[1;n] σ Z(R d) ― //⑵=f — E Hz1,zj ),..., — E 夕(zn, Zj ) j σ Z 便 T).
nn
j=1
j=1
(1)
By construction,于W is G-^naai'intt if is is G-iiianriant. The construction of /宁 is extended to
the general case of possiily continuous proiaiility distriiutions iy essentially replacing sums iy
integrals (Appendix A)
Remark1.(Varying dimensionsd	x andd Y). Both in practice and in theory, it is important that
fW layers (in particular thefirst layer of the neural architecture) handle datasets of ariitrary numier
of featuresd x and numier of multislaielsd Y . The proposed approach, used in the experiments
(Section 4), is to definefas follows. Letting(= (x[ y)and(	= (x[ y)ie two samples in
RdX ×RdY , letuie defined fromR 4 ontoR t, consider the sum ofu(x[]][ x []][ y[][ y [])for]
ranging ink1 ;d x andink1 ;d Y, and apply mappingvfromR t toRr on this sum:
EdXdY
u(x[]][ x[]][ y[][ y [])	(2)
=1
Likewise, iy constructionfisisinvariant, i.e. it is invariant to ioth feature and laiel permutations.
As shown in Section 4, this invariance property is instrumental to a good empirical performance.
Remark2.(Varying sample sizen). By construction,f W is defined on Z(Rd) =∪ nZn(Rd)(indes
pendent ofn), such that it supports inputs of ariitrary cardinalityn.
Remark3.(Discussion w.r.t. (Maron et al., 2020)) The aiove definition off W is iased on the ags
gregation of pairwise termsf(( i[ (j). The motivation for using a pairwisefis twofold. On the one
hand, capturing local sample interactions allows to create more expressive architectures, which is
important to improve the performance on some complex data sets, as illustrated in the experiments
(Section 4). On the other hand, interaction functionals are crucial to design universal architectures
1As opposed to G-equlvαriαniSaucOantthatatecharccteHeddby σ∈ ∈ G,g(σyz) = σ口夕(z)
3
Under review as a conference paper at ICLR 2021
(Appendix C, theorem 2). The proposed theoretical framework relies on the Wasserstein distance
(corresponding to the convergence in law of probability distributions), which enables to compare
distributions with varying number of points or even with continuous densities. In contrast, Maron
et al. (2020) do not use interaction functionals, and establish the universality of their DSS architec-
ture forfixed dimensiondand number of pointsn. Moreover, DSS happens to resort to max pooling
operators, discontinuous w.r.t. the Wasserstein topology (see Remark 6).
Two particular cases are when g OnyyePenndSnn its firsθ OrCeCondPnP:t:
(i)	if 夕(苞 z∖ = ψ(Z/), then /宁 computes a global “moment” descriptor of the input, as
fψ(z) = 1 En=I Mz )∈欢 r
(ii)	if HN, N') = (Z)),then 于早 transports the input distribution via ξ, af 于早(Z) = {((Zi)Ii ∈
[1; m]} Z Z(肽r). This operation is referred to as aPIISh-forward.
Remark 4. (LaCalidedCOmPUtatiOn) IP PCaCtiCe,thequadraticcomplexity nf /宁 w.r.t. the number
n Of SmIPieS can be reduced by Oniy COmPUting (ZMi)Zj) for PaisS ?,Zj sufficiently close to each
other. Layer f ψ thus extracts and aggregates information related to the neighborhood of the samples.
Remark 5. (Link to kernels) The use of an interaction functional g isinsjrn`dd omm kemeiideas,
albeit with significant differences: (i) in fφ(zi), the detail of the pairwise interactions gSNi, Zj) is
lost through averaging; (ii)g SaheSiII(OdCyUelItlahels;(iii) SS ISleenIFFUrthew WkrkWib be d^toted
to investigating the properties of the /『(Zi) marrix.
2.3 Learning Meta-features
The proposed distributional neural architectures defined on point distributions (Dida) are sought as
Z ∈(便d) 一 产Z (z) =n^m ◦ n…。…。源(z) ∈ 欢 dm+1	(3)
whereζare the trainable parameters of the architecture (below). Only the cased Y = 1is considered
in the remainder. Thek-th layer is built on the top of) k, mapping pairs of vectors inR dk ontoR dk+1 ,
withd 1 =d(the dimension of the input samples). Last layer is built on)	m, only depending on
its second argument; it maps the distribution in layerm-1onto a vector, whose coordinates are
referred to as meta-features.
TheG-invariance and dimension-agnosticity of the whole architecture only depend on thefirst
layer f^ satisfying these properties. In the fistt Iyyei)夕ι is sought as 夕ι((x,y),(x',y')) =
V(Ek u(x[k],xz[k],y,y")) (Renae±1),w汕心网,工z[k],y,y') = (P(Au∙(x[k]χz[k])+bu,1 y=y)
inR t × {0,1}, whereρis a non-linear activation function,A	u a(t,2)matrix,(x[k];x	[k])
the 2-dimensional vector concatenatingx[k]andx	[k], andb u at-dimensional vector. With
e= Ek u(x[k],x[k],y,y)), functionvlikewise applies a non-linear activation functionPon an
affine transformation of e: v)e= = PlAv ∙ e + V)), with AV a (, r) maiXiXedd Vv a r-dimenSional
vector.
Note that the subsequent layers need neither be invariant w.r.t. the number of samples, nor handle
a varying number of dimensions. However, maintaining the distributional nature among several
layers is shown to improve performance in practice (Section 4). Every) k, k≥2is defined as
)k = P(Ak ∙ +bk), with PendCtiPatienfedCtien, Ak a (dk, dk+ι) matrix and bk a dk+ι-dimensional
vector. The DIDA neural net thus is parameterized byζ d=ef. (Au, du, Av,dv,{Ak,dk }k), that is
classically learned by stochastic gradient descent from the loss function defined after the task at
hand (Section 4). 3
3	Theoretical Analysis
This section analyzes the properties of invariant-layer based neural architectures, specifically their
robustness w.r.t. bounded transformations of the involved distributions, and their approximation
abilities w.r.t. the convergence in law, which is the natural topology for distributions. As already
said, the discrete distribution case is considered in this section for the sake of readability, referring
the reader to Appendix A for the general case of continuous distributions.
4
Under review as a conference paper at ICLR 2021
3.1 Optimal Transport Comparison of Datasets
Point clouds vs. distributions.Our claim is that datasets should be seen as probability distribu-
tions, rather than point clouds. Typically, including many copies of a point in a dataset amounts to
increasing its importance, which usually makes a difference in a standard machine learning setting.
Accordingly, the topological framework used to define and learn meta-features in the following is
that of the convergence in law, with the distance among two datasets being quantified using the
Wasserstein distance (below). In contrast, the point clouds setting (see for instance (Qi et al., 2017))
relies on the Haussdorff distance among sets to theoretically assess the robustness of these archi-
tectures. While it is standard for 2D and 3D data involved in graphics and vision domains, it faces
some limitations in higher dimensional domains, e.g. due to max-pooling being a non-continuous
operator w.r.t. the convergence in law topology.
Wasserstein distance.Referring the reader to (Santambrogio, 2015; Peyr e´ & Cuturi, 2019) for a
more comprehensive presentation, the standard1-Wasserstein distance between two discrete proba-
bility distributionsz,z ∈Z n(Rd)×Z m(Rd)is defined as:
W∖(z,zz) = max Ly f(z i)——^y f(z j)
`	f∈Lip ι(Rd) nJ i mJ j
i=1	j=1
withLip 1(Rd)the space of1-Lipschitz functionsf:R d →R. To account for the invariance require-
ment (making indistinguishablez= (z 1, . . . , zn)and its permuted image(σ(z 1), . . . ,σ(zn)) d=ef.σ z
underσ∈G), we introduce theG-invariant1-Wasserstein distance: forz∈Z	n(Rd),z ∈Z m(Rd):
Wι(z, zz) = minWι(σ^z, zz)
σ∈G
such that Wι(z, Zz) = 0 if amd Only if z and zz belong to the same equivalence class (Appendix A),
i.e. are equal in the sense of probability distributions up to sample and feature permutations.
Lipschitz property.In this context, a mapfirom Z(R	d)onto Z(R r)is continuous ior the conver-
gence in law (a.k.a. weak convergence on distributions, denoted -)) iff for any SeqUellCe z(k) ɜ z,
then ffz(k)) -X (f)Z)TThW WaSSsrStnid dia^ancemeZ'izeseιe convergence in law, inthesense Zhat
z(k) -zis equivalent toW 1(z(k) ,z)→0. Furthermore, mapfis said to beC-Lipschitz ior the
permutation invariant1-Wasserstein distance iii
∀z,z yz(Rd), Wι(f(z),fZ, ))≤CWι(z,z,).	(4)
TheC-Lipschitz property entails the continuity oifw.r.t. its input: ii two input distributions are
close in the permutation invariant1-Wasserstein sense, the corresponding outputs are close too.
3.2 Regularity of Distribution-Based Invariant Layers
Assuming the interaction iunctional to satisiy the Lipschitz property:
∀z ∈ 肽d, 夕(n,∙) and e(、N) are C— - Lipschitz.	(5)
the robustness oi invariant layers with respect to diiierent variations oi their input is established
(proois in Appendix B). Wefirst show that invariant layers also satisiy Lipschitz property, ensuring
that deep architectures oi the iorm (3) map close inputs onto close outputs.
Proposition 1.Invariant layerf — of type (1) is(2rC —)-Lipschitz in the sense of (4).
A second result regards the case where two datasetszandz are such thatz is the image oiz
through some diiieomorphismτ(z= (d	1, . . . , dn)andz	=τ z= (τ(d 1), . . . ,τ(d n)). Iiτis
close to identity, then the following proposition shows that f -(THz) and ʃ-(z) aee CioSe too. MOre
generally, ii continuous transiormationsτandξrespectively apply on the input and output space oi
d—, and are close to identity, thenξ Hd— (τHz)andd —(z)are also close.
Proposition 2. Let r : dL→ → Rd and ξ : Hr → Rr be two Lipschitz maps with respectively LiPschitz
constantsC τ andC ξ. Then,
∀Z ∈ Z(α), Wι(ξf-ez), d-(z)) ≤ SUP	∣ξ(x)-琲 + 2rLipQ) SUP IlT(x) — 11∣2
x∈fφ(τ(Ω))	x∈Ω
∀z,zf ∈ Z(Ω))i TyTiSeqariartant, Wι(ξf-(τ口z), §口f-(τ0)) ≤ 2rC— CT CξWι(z,zz)
5
Under review as a conference paper at ICLR 2021
3.3 Universality of Invariant Layers
Lastly, the universality of the proposed architecture is established, showing that the composition of
an invariant layer (1) and a fully-connected layer is enough to enjoy the universal approximation
property, over all functions defined onZ(R d)with dimensiondless than someD(Remark 1).
Theorem 1. LeF Z : Z(→)一鼠 bivGanvmaanOmaPOnaCoωPaOt Q,continrohe for the conver-
gence in law. Then ∀ε〉O,thereinints OCCOntinIloinaIaPS 我4 Sitha that
∀z ∈ Z(O)F	I尸ψ) f ψ o /.(z)| < E
where (PiS GttaeirinntnndindpPendeno F于尸.
Proof. TheSeechhffheeroOfiiSaf OolloWSCOomPeeeeroOfiillAPPeddiXC). eetuddenne = = goh
where: (i)his the collection ofd X elementary symmetric polynomials in the features andd Y
elemeitary iymmetric POlyiOmiali ii the labeli, which ii iivariait uiderG; (ii) a diicretizatiOi Of
h(Ω) On a grid is heen ConSidered, achieved hιanks to gthataims atcelleetingintegrals erereach
cell Of the diicretizatiOi; (iii)ψaPPliei fuictiOiFOi thii diicretized meaiure; thii requireihtO
be bijective, aii ii achievei by h, thrOugh a PrOjectiOi Oi the quOtieit iPaceS d/Gaii a reitrictiOi
to Sts image compact Ω’. To sum up, f φ defined as such computes ae expectatioe which collects
iitegrali Over each cell Of the grii tO aPPrOiimate meaiureh zby a iiicrete cOuiterPart hz. Heice
1
ψappliesFto	h-1Zhz). Coitiiuity is obtaiied as follows: (i) proximity ofh zaid hzfollows
from .emma 1 ii (De Bie et al., 2019)) aid gets tighter as the grid discretizatioi step teids to 0;
(ii) Map h 1 ss 1/d-Holder, after Theorem 1.3.1 from (Rahman & Schmeisser, 2002)); therefore
1
Lemma 2 entails that Wι(z, h- 1h^z) can bee UPPe--bouunded; (in) SInCe Si CSCOmPaCt,byBanach-
Alaoglu theorem,	Z(Ω) also	ss.	SinCe F isconUinuous,	h	is	IhUS	fnimrmIyWeykcyCOnuinUoUs:
choosing a discretization step small enough ensures the result.	□
Renark6.(Comparison with (Maron et al., 2020)) The above proof holds for functionals of arbitrary
input sample sizen, as well as continuous distributions, generalizing results in (Maron et al., 2020).
Note that the two types of architectures radically differ (more in Section 4).
Renark7.(Approximation byan invariant NN) After theorem 1, any invariant continuous function
defined on distributions with compact support can be approximated with arbitrary precision byan
invariant neural networS (Appendix C). The proof involves mainly three steps: (i) an invariant layer
fφ can be approximated by an invariant network; (ii) the universal approximation theorem (Cybenko,
1989; Leshno et al., 1993); (iii) uniform continuity is used to obtain uniform bounds.
Renark8.(Extension to different spaces) Theorem 1 also extends to distributions supported on dif-
ferent spaces, via embedding them into a high-dimensional space. Therefore, any invariant function
on distributions with compact support inR d withd≤Dcan be uniformly approximated by an
invariant network (Appendix C).
4 Experimental validation
The experimental validation presented in this section considers two goals of experiments: (i) assess-
ing the ability of Dida to learn accurate meta-features; (ii) assessing the merit ofthe Dida invariant
layer design, building invariant /宁 on the top of an interactional function /(Eq.l). Assaid,hhis
architecture is expected to grasp contrasts among samples, e.g. belonging to different classes; the
proposed experimental setting aims to empirically investigate this conjecture.
Baselines. These goals of experiments are tackled by comparing DIDA to three baselines: DSS
layers (Maron et al., 2020); hand-crafted meta-features (HC) (MUnnz et al., 2018) (Table 5 in Ap-
pendix D); Dataset2Vec (Jomaa et al., 2019). We implemented DSS, the code being not avail-
able.2 In order to cope with varying dataset dimensions (as required by the UCI and OpenML
benchmarks), the original DSS was augmented with an aggregator summing over the features. Three
DSS baselines are considered: linear or non-linear invariant layers, possibly preceded by equivariant
layers. Similarly, the original Dataset2Vec implementation has been augmented to address our
experimental setting. The baselines are detailed in Appendix D.3.
2The code source of Dida and (our implementation of) baselines are available in supplementary materials.
6
Under review as a conference paper at ICLR 2021
Figure 1: Learning meta-features with Dida. Top: the Dida architecture (BN stands for batch
norm; FC for fully connected layer). Bottom left: Learning meta-features for patch identification
using a Siamese architecture (section 4.1). Bottom right: learning meta-features for performance
modelling, specifically to rank two hyper-parameter configurationsθ 1 andθ 2 (section 4.2).
Experimental setting.Two tasks defined at the dataset level are considered: patch identification
(section 4.1) and performance modelling (section 4.2). The dataset preprocessing protocols are
detailed in Appendix D.1. On both tasks, the same Dida architecture is considered (Fig 1), involving
2 invariant layers followed by 3 fully connected (FC) layers. Meta-featuresF ζ (z)consist of the
output of the third FC layer, withζdenoting the trained D ida () parameters. All experiments run on
1 NVIDIA-Tesla-V100-SXM2 GPU with 32GB memory, using Adam optimizer with base learning
rate10 -3 .
4.1	Task 1: Patch Identification
The patch identification task consists of detecting whether two blocks of data are extracted from
the same original dataset (Jomaa et al., 2019). Lettingudenote an-sample,d-dimensional dataset,
ann z , dz patchzis constructed fromuby selectingn	z examples inu(sampled uniformly with
replacement) and retaining their description alongd z features (sampled uniformly with replace-
ment). The sizen z and numberd z of features of the patch are uniformly selected infixed in-
tervals (Table 4, Appendix D). To each pair of patchesz,z	with same number of instances
nz =n z , is associated a binary meta-label(z,z )set to 1 iffzandz’ are extracted from the
same initial datasetu. D IDA parametersζare trained to minimize the cross-entropy loss of model
ʌ
G(z, z') = exp ( 一 |IFZ(Z) 一々(z') |12)，WithFZ(Z) and 々(z') Ihe meaa-feaUures ComIPUted for znnd
z:
Minimize £()) : — E 4(z, Z')log(∕ζ(z, Z')) + (1- ((Z, zz)) log(1 — .(z, Z'))	(6)
Z,Z
DIDA znn zll bzselines zre trzinen using z Sizmese zpprozch (Figure 1, bottom left): the szme
DIDA (or bzseline) zrchitecture is usen to compute metz-fezturesF Z (Z)znnF Z (Z)from pztchesZ
znnZ , znn trzinen to minimize the cross-entropy loss W.r.t.(ZZZ ). The clzssificztion results on
toy nztzsets znn UCI nztzsets (Tzble 1, netzilen in Appennix D) shoW the pertinence of the Dida
metz-feztures, pzrticulzrly so on the UCI nztzsets Where the number of feztures Winely vzries from
one nztzset to znother. The relevznce of the interzctionzl invzriznt lzyer nesign is estzblishen on this
problemzsDida outperforms both Dataset2Vec, DSS zsWell zs the function lezrnen on the top
of the hznn-crzften metz-feztures.
An ablation study is connucten to zssess the impzct of (i) the fezture permutztion invzriznce; (ii)
consinering onevstWo invzriznt lzyers of type (1). The so-czllen N o-FInv-DSS bzseline, netzilen
in Appennix D, is built upon (Zzheer et zl., 2017); it only niffers from the DSS bzseline zsit isnot
fezture permutztion invzriznt. With cz the szme number of pzrzmeters zs DSS, its performznces zre
7
Under review as a conference paper at ICLR 2021
Method	# parameters	TOY	UCI
Hand-crafted	53312	77.05 %±L63	58.36 %±2.64
NO-FINV-DSS (no invariance in features)	1297692	90.49 %±L73	64.69 %±4.89
DATASET2Vec (our implementation)	257088	97.90 %±L87	77.0.5 %±3.49
DSS layers (Linear aggregation)	1338684	89.32 %±L85	76.23 %±1.84
DSS layers (Non-linear aggregation)	1338684	96.24 %±2.44	83.97 %±2.89
DSS layers (Equivariant+invariant)	1338692	96.26 %±L40	82.94 %±3.36
Dida (1 invariant layer)	323028	91.37 %±L39	81.03 %±3.23
Dida (2 invariant layers)	1389089	97.20% ±0.10	89.70 % ± 8.89
Table 1: Patch identification (binary classification accuracy) on 10 runs of Dida and considered
baselines.
Method	SGD	svm	LR	k-NN
Hand-crafted	71.18 %±0.41	75.9% ±±0.29	6.41%±0.419	65	65.4± %± 0.73
Dataset2Vec	74.43 %±=0.90	81.7% ±±1.85	89.18%±0.45	72	72.9± %± 1.13
DSS (Linear aggregation)	73.46 %±=1.44	22.9% ±±0.22	87.93%±0.58	70	70.%± %± 2.82
DSS (Equivariant+Invariant)	73.54 %±=0.66	81.29%±1.65	87.65%±0.03	68	68.%± %± 2.84
DSS (Non-linear aggregation)	74.13 %±L01	83.38%±0.37	87.92%±0.27	73	73.07 %± 0.77
DIDA (1 invariant layer)	77.31 %±0.16	S.!。％ 70.0,71	6%±0.1774.41%±	74.41 %± 0.93
DIDA (2 invariant layers)	78.41 %±0.41	84.14%±0.0289.7	7%±0.50	78.71 %± 0.54
Table 2: Pairwise ranking of configurations, for ML algorithms SGD, SVM, LR and k-NN: perfor-
mance on test set of D ida, hand-crafted, Dataset2Vec and DSS (average and std deviation on 3
runs).
significantly lower (Table 1), showcasing the benefits of enforcing the feature invariance property.
Secondly, we compare the 2-invariant layers Dida, with the 1-invariant layer Dida (1L-Dida and
2L-Dida for short): 1L-Dida yields significantly lower performances, which confirms the advan-
tages of maintaining the distributional nature among several layers, as already noted by (De Bie
et al., 2019). Note that the 1L-Dida still outperforms the non feature-invariant baseline, while
requiring much fewer parameters.
4.2	Task 2: Performance model learning
The performance modelling task aims to assessa priorithe accuracy of the classifier learned from a
given machine learning algorithm with a given configurationθ(vector of hyper-parameters ranging
in a hyper-parameter spaceΘ, Table 6 in Appendix D), on a datasetz(for brevity, the performance
ofθonz) (Rice, 1976).
For each ML algorithm, ranging in Logistic regression (LR), SVM, k-Nearest Neighbours (k-NN),
linear classifier learned with stochastic gradient descent (SGD), a set of meta-features is learned
to predict whether some configurationθ 1 outperforms some configurationθ 2 on datasetz: to each
triplet(z,θ 1,θ2)is associated a binary value(z,θ 1,θ2), set to 1 iffθ 2 yields better performance
ʌ
thanθ 1 onz. D IDA parametersζare trained to build model ζ , minimizing the (weighted version of)
ʌ
cross-entropy loss (6), where G(z, θ 1, &) SS a Zlayer FC network Wth nnput VeCto[ FJZ(z);。1； &],
depending on the considered ML algorithm and its configuration space.
In each epoch, a batch made of triplets(z,θ 1,θ2)is built, withθ 1,θ2 uniformly drawn in the al-
gorithm configuration space (Table 6) andzan-sampled-dimensional patch of a dataset in the
OpenML CC-2018 (Bischl et al., 2019) withnuniformly drawn in[700; 900]anddin[3; 10].	Al-
gorithm 1 summarizes the training procedure.
The quality of the Dida meta-features is assessed from the ranking accuracy (Table 2), showing
their relevance. The performance gap compared to the baselines is higher for the k-NN modelling
8
Under review as a conference paper at ICLR 2021
Algorithm 1Performance Modeling
1:	FZ — meaa-eeaUure extracOor (DIDA, DSS, DATASET2Vec, or Hand-crafted)
2:	MLP — NNψLinaar(64)-ReLU-Lieaar(32)-ReLU-Lieaar(1)]
3:	CLF - maciιieela听ingclasfiiear (SGD,SVM, LRark-]⅛))
4:	error—()CV classificatioi error fuictioi
5:	foriteratioi=1,2, . . .do
(:	Sample(θ 1,θ 2), two hyper)parameters of CLFSearch space: Table (
7:	Sample patchzfrom datasetu	Patch dimeisioi: Table 4
8:	pred—softmax(MLP(F ζ (z),θ 1), MLP(Fζ(z),θ2))
9:	Backpropagate logloss(pred, 0 if error(z, CLF(θ1))<error(z, CLF(θ 2)) else 1)
10:	end for
task; this is explaiied as the sought performaice model oily depeids oi the local geometry of the
examples. Still, good performaices are observed over all coisidered algorithms. Note that the 2L)
Dida yields sigiificaitly better (respectively, similar) performaices thai 1L-Dida oi thek)NN
model (resp. oi all other models).
Meta-feature assessment. A regressioi settiig is thereafter coisidered, aimed to predict the ac)
tual performaice of a coifiguratioiθbased oi the (frozei) meta)featuresF ζ (z). The regressioi
accuracy is illustrated for the coifiguratiois of thek)NN algorithm oi Figure 2, left (results for
other algorithms are preseited ii Appeidix D). The comparisoi with the regressioi models based
oi DSS meta)features or haid)crafted features (Figure 2, middle aid right) shows the merits of
the Dida architecture; a teitative iiterpretatioi for the Dida better performaice is based oi the
iiteractioial iature of Dida architecture, better capturiig local iiteractiois.
Figure 2: k)NN: True performaice vs performaice predicted by regressioi oi top of the meta)
features (i) learied by Dida, (ii) DSS or (iii) Haid)crafted statistics.
5 Conclusion
The theoretical coitributioi of the paper is the Dida architecture, able to leari from discrete aid
coitiiuous distributiois oiR d, iivariait w.r.t. feature orderiig, agiostic w.r.t. the size aid di)
meisioidof the coisidered distributioi sample (withdless thai some upper bouidD). This
architecture eijoys uiiversal approximatioi aid robustiess properties, geieraliziig former results
obtaiied for poiit clouds (Maroi et al., 2020). The merits of Dida are demoistrated oi two tasks
defiied at the dataset level: patch ideitificatioi aid performaice model leariiig, comparatively to
the state of the art (Maron et al., 2020; Jomaa et al., 20(9; Mufioz et al., 20(8). The ability to ac)
curately describe a dataset ii the laidscape defiied by ML algorithms opeis iew perspectives to
compare datasets aid algorithms, e.g. for domaii adaptatioi (Bei)David et al., 2007; 20(0) aid
meta)leariiig (Fiii et al., 20(8; Yooi et al., 20(8), ii light of keriel methods.
9
Under review as a conference paper at ICLR 2021
References
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. d.vlvncιcei inNunrainθormi0innPrceesningSyeteιnι99,pp∖7-ι444,
2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. MaChiineLerrning,7(1)：:51-i775,
2010. ISSN 0885-6125, 1573-0565.
Bernd Bischl, Giuseppe Casalicchio, Matthias Feurer, Frank Hutter, Michel Lang, Rafael G Man-
tovani, Jan N van Rijn, and Joaquin Vanschoren. Openml benchmarking suites.erXnv prιprnit
erXnv:1708.03731, 20i7.
Taco Cohen and Max Welling. Group equivariant convolutional networks.nr0cιιdnine 0f Thι 33rd
Irtnrretiorel Conference or Machine Lnerrirg, 48:2770-2777, 20-22 Jun 2016.
David A. Cox, John Little, and Donal O'Shea. IdealSV Vrieeties,rnd Algrrhem:: AnIntOddCCtOrn
to Corputetiorel Alnebreic Georetry erd Corrutetive Alnebre, 3/e (Urdernreduete Texte ir
Metheeetice). Springer-Verlag, Berlin, Heidelberg, 2007. ISBN 0387376707.
George Cybenko. Approximation by superpositions ofa sigmoidal function.Metheeetice of cortrol,
eigrele erd systems, 2(4):303-314, 1787.
Gwendoline De Bie, Gabriel Peyre, and Marco Cuturi. Stochastic deep networks. PtCnendingSff
the 36th Irterretiorel COAererCe or Mechire Leerrirg, pp. 1776-1767, 2017.
Dheeru Dua and Casey Graff. UCI machine learning repository. 2017. URLhttp://archive.
ics.uci.edu/ml.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning.AnverCes
ir Neurel InfOrmetiOr PrOCeeeirn Systems 31, pp. 7716-7727, 2018.
Robert Gens and Pedro M Domingos. Deep symmetry networks.AnverCes ir neurel Irforeetior
Processirn Systems 27, pp. 2737-2747, 2014.
Jason Hartford, Devon R Graham, Kevin Leyton-Brown, and Siamak Ravanbakhsh. Deep models of
interactions across sets.ProCeenirns of the 35th Irterretiorel CorfererCe or MeChire Leerrirn,
2018.
Tatsunori Hashimoto, David Gifford, and Tommi Jaakkola. Learning population-level diffusions
with generative rnns.ProCeenirns of The 33rn Irterretiorel CorfererCe or MeChire Leerrirn,
48:2417-2426, 20-22 Jun 2016.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data.ArXiv, abs/1706.07163, 2017.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups.IEEE
SinreIprocessirn menezire, 27(6):82-77, 2012.
Frank Hutter, Lars Kotthoff, and Joaquin Vanschoren (eds.).Autometen MeChire Leerrirn: Meth-
ons, Systems, Chellernes. Springer, 2018. In press, available at http://automl.org/book.
Hadi S. Jomaa, Josif Grabocka, and Lars Schmidt-Thieme. Dataset2vec: Learning dataset meta-
features.erXiv, abs/1707.11063, 2017.
Nicolas Keriven and Gabriel Peyre. Universal invariant and equivariant graph neural networks.
Adverces ir Neurel Irformetior Processirn Systems 32, pp. 7070-7077, 2017.
Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups.Proceedirns of The 35th Irterretiorel Corfererce or
Mechire Leerrirn, 2018.
10
Under review as a conference paper at ICLR 2021
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-
Volutional neural networks. AdVnTCeeSnTlTeUKmlnTIoOrmaiinTIPOoeeSinTIgSySeems, pp.0977-1055,
2012.
Moshe Leshno, Vladimir Ya Lin, Allan Pinkus, and Shimon Schocken. Multilayer feedforward net-
works with a nonpolynomial activation function can approximate any function.NeUPal neSwoPke,
6(6):861-867,1993.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks.7Sh InSePnaSional ConfePence on LeaPning RepPeeenSaSione, ICLR 2019, New OPleane,
LA, USA, MaS 6-9, 2019, 2019a.
Haggai Maron, Ethan Fetaya, Nimrod Segol, and Yaron Lipman. On the universality of invariant
networks.PPoceedinge of She 36Sh InSePnaSional ConfePence on Machine LeaPning, ICML 2019,
9-15 JUne 2019, Long Beach, CalifoPnia, USA, pp. 4363-4371, 2019b.
Haggai Maron, Or Litany, Gal Chechik, and Ethan Fetaya. On learning sets of symmetric elements.
InSePnaSional ConfePence on Machine LeaPning (ICML), 2020.
Mario A. Mufioz, Laura Villanova, DavaatSeren Baatar, and Kate Smith-Miles. Instance spaces
for machine learning classification.Machine LeaPning, 107(1):109-147, January 2018. ISSN
0885-6125. doi: 10.1007/s10994-017-5629-5.
Mario A Mufioz, Laura Villanova, Davaatseren Baatar, and Kate Smith-Miles. Instance spaces for
machine learning classification.Machine LeaPning, 107(1):109-147, 2018.
Valerio Perrone, Rodolphe Jenatton, Matthias W Seeger, and Cedric Archambeau. Scalable hyperpa-
rameter transfer learning.Addancee in NeUPal InfoPraSion PPoceeeing SSeSere 31, pp. 6845-6855,
2018.
Gabriel Peyre and Marco Cuturi. Computational optimal transport. Funndsitinysnnd Preny5® in
Machine LeaPning, 11(5-6):355-607, 2019. ISSN 1935-8237. doi: 10.1561/2200000073.
Bernhard Pfahringer, Hilan Bensusan, and Christophe G. Giraud-Carrier. Meta-learning by land-
marking various learning algorithms.PPoceeningy of She SedenSeenSh InSePndSiondl ConfePence
on Mdchine LedPning, pp. 743-750, 2000.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets
for 3d classification and segmentation.PPoc. CorpUSeP Viyion dnn PdSSePn RecogniSion (CVPR),
IEEE, 2017.
Qazi Inamur Rahman and Gerhard Schmeisser. Analytic theory of polynomials.OxfoPn UnidePyiSS
PPeyy, 2002.
Siamak Ravanbakhsh, Jeff Schneider, and BarnabaS PoCzos. Equivariance through parameter-
sharing.PPoceeningy of She 34Sh InSePndSiondl ConfePence on Mdchine LedPning, 70:2892-2901,
2017.
John R. Rice. The algorithm selection problem.Anddncey in CorpUSePy, 15:65-118, 1976.
Filippo Santambrogio. Optimal transport for applied mathematicians. Birkdueer, NY, 2015.
Nimrod Segol and Yaron Lipman. On universal equivariant set networks.8Sh InSePndSiondl ConfeP-
ence on Ledrning RepreyenSdSiony, ICLR 2020, 2019.
J. Shawe-Taylor. Symmetries and discriminability in feedforward network architectures.IEEE
TrdnydcSiony on Neurdl NeSworry, 4(5):816-826, Sep. 1993. ISSN 1941-0093. doi: 10.1109/72.
248459.
Jost Tobias Springenberg, Aaron Klein, Stefan Falkner, and Frank Hutter. Bayesian optimization
with robust bayesian neural networks.Anddncey in Neurdl InforrdSion Proceyying SSySery 29,
pp. 4134-4142, 2016.
11
Under review as a conference paper at ICLR 2021
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.
4th International Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico,
May 2-4, 2016, Conference Track Proceedings, 2016.
David H. Wolpert. The lack of A priori distinctions between learning algorithms.Neural Computa-
tion, 8(7):1341-1390,1996.
Jeffrey Wood and John Shawe-Taylor. Representation theory and invariant neural networks.Discrete
applied mathematics, 69(1-2):33-60, 1996.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning.Advances in Neural Information Processing Systems
31, pp. 7332-7342, 2018.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets.Advances in Neural Information Processing Systems 30, pp.
3391-3401, 2017.
12