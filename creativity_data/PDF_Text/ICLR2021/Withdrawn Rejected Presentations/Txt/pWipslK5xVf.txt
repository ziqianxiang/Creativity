Under review as a conference paper at ICLR 2021
Wasserstein Distributional Normalization :
Nonparametric Stochastic Modeling for Han-
dling Noisy Labels
Anonymous authors
Paper under double-blind review
Ab stract
We propose a novel Wasserstein distributional normalization (WDN) algorithm
to handle noisy labels for accurate classification. In this paper, we split our data
into uncertain and certain samples based on small loss criteria. We investigate the
geometric relationship between these two different types of samples and enhance
this relation to exploit useful information, even from uncertain samples. To this end,
we impose geometric constraints on the uncertain samples by normalizing them into
the Wasserstein ball centered on certain samples. Experimental results demonstrate
that our WDN outperforms other state-of-the-art methods on the Clothing1M and
CIFAR-10/100 datasets, which have diverse noisy labels. The proposed WDN is
highly compatible with existing classification methods, meaning it can be easily
plugged into various methods to improve their accuracy significantly.
1	Introduction
The successful results of deep neural networks (DNNs) on supervised classification tasks heavily rely
on accurate and high-quality label information. However, annotating large-scale datasets is extremely
expensive and a time-consuming task. Because obtaining high-quality datasets is very difficult,
in most conventional works, training data have been obtained alternatively using crowd-sourcing
platforms Yu et al. (2018) to obtain large-scaled datasets, which leads inevitable noisy labels in the
annotated samples.
While there are numerous methods that can deal with noisy labeled data, recent methods actively
adopt the small loss criterion, which enables to construct classification models that are not susceptible
to noise corruption. In this learning scheme, a neural network is trained using easy samples first in the
early stages of training. Harder samples are then gradually selected to train mature models as training
proceeds. Jiang et al. (2018) suggested collaborative learning models, in which a mentor network
delivers the data-driven curriculum loss to a student network. Han et al. (2018); Yu et al. (2019)
proposed dual networks to generate gradient information jointly using easy samples and employed
this information to allow the networks to teach each other. Wei et al. (2020) adopted a disagreement
strategy, which determines the gradient information to update based on disagreement values between
dual networks. Han et al. (2020) implemented accumulated gradients to escape optimization processes
from over-parameterization and to obtain more generalized results. In this paper, we tackle to solve
major issues raised from the aforementioned methods based on the small-loss criterion, as follows.
In comprehensive experiments, the aforementioned methods gain empirical insight regarding network
behavior under noisy labels. However, theoretical and quantitative explanation have not been closely
investigated. In contrast, we give strong theoretical/empirical explanations to understand the network
under noisy labels. In particular, we present an in-depth analysis of small loss criteria in a probabilistic
sense. We exploit the stochastic properties of noisy labeled data and develop probabilistic descriptions
of data under the small loss criteria, as follows. Let P be a probability measure for the pre-softmax
logits of the training samples, l be an objective function for classification, and 1口 be an indicator
function. Then, our central object to deal with is a truncated measure defined as
X 〜μ∣ζ = 1{X;l(X)>z}P γ 〜ξ∣ζ = 1{X;l(YXP	⑴
X μιζ P[1(X) >ζ] , Y ξiζ	P[1(Y) ≤ Z] ,	(1)
1
Under review as a conference paper at ICLR 2021
where X and Y, which are sampled from μ∣Z and ξ∣Z, denote uncertain and certain samples defined
in the pre-softmax feature space1 (i.e., Rd), respectively. In equation 1, μ and ξ denote the probability
measures of uncertain and certain samples, respectively, and ζ is a constant. Most previous works
have focused on the usage of Y and the sampling strategy of ζ, but poor generalization capabilities
based on the abundance of uncertain samples X has not been thoroughly investigated, even though
these samples potentially contain important information. To understand the effect of noisy labels
on the generalized bounds, we provide the concentration inequality of uncertain measure μ, which
renders the probabilistic relation between μ and ξ and learnability of the network under noisy labels.
While most conventional methods Han et al. (2018); Wei et al. (2020); Li et al. (2019a); Yu et al.
(2019) require additional dual networks to guide misinformed noisy samples, the scalability is not
guaranteed due to the existence of dual architectures, which have the same number of parameters
as the base network. To alleviate this problem, we build a statistical machinery, which should be
fully non-parametric, simple to implement, and computationally efficient to reduce the computational
complexity of conventional approaches, while maintaining the concept of small-loss criterion. Based
on the empirical observation of ill-behaved certain/uncertain samples, we propose the gradient flow
in the Wasserstein space, which can be induced by simulating non-parametric stochastic differential
equation (SDE) with respect to the Ornstein-Ulenbeck type to control the ill-behaved dynamics. The
reason for selecting these dynamics will be thoroughly discussed in the following sections.
Thus, key contributions of our work are as follows.
•	We theoretically verified that there exists a strong correlation between model confidence
and statistical distance between X and Y. We empirically investigate that the classification
accuracy worsens when the upper-bound of 2-Wasserstein distance W2(μ,ξ) ≤ ε (i.e.,
distributional distance between certain and uncertain samples) drastically increase. Due
to the empirical nature of upper-bound ε, it can be used as an estimator to determine if a
network suffers from over-parameterization.
•	Based on empirical observations, we develop a simple, non-parametric, and computationally
efficient stochastic model to control the observed ill-behaved sample dynamics. As a
primal object, we propose the stochastic dynamics of gradient flow (i.e.,, Ornstein-Ulenbeck
process) to simulate simple/non-parametric stochastic differential equation. Thus, our
method do not require any additional learning parameters.
•	We provide important theoretical results. First, the controllable upper-bound ε with the
inverse exponential ratio is induced, which indicates that our method can efficiently con-
trol the diverging effect of Wasserstein distance. Second, the concentration inequality of
transported uncertain measure is presented, which clearly renders the probabilistic relation
between μ and ξ.
2	Related Work
Curriculum Learning & Small-loss Criterion. To handle noisy labels, Han et al. (2018); Yu et al.
(2019); Jiang et al. (2018); Wei et al. (2020); Lyu & Tsang (2020a); Han et al. (2020) adopted
curriculum learning or sample selection frameworks. However, these methods only consider a small
number of selected samples, where large portion of samples are excluded at the end of the training.
This inevitably leads to poor generalization capabilities. However, this conflicts with sample selection
methods because a large portion of training samples are gradually eliminated. By contrast, our method
can extract useful information from unselected samples X 〜μ (i.e., uncertain samples) and enhance
these samples (e.g., X0 〜Fμ) for more accurate classification. Chen et al. (2019) iteratively apply
cross-validation to randomly partitioned noisy labeled data to identify most samples that have correct
labels. To generate such partitions, they adopt small-loss criterion for selecting samples.
Loss Correction & Label Correction. Patrini et al. (2017a); Hendrycks et al. (2018); Ren et al.
(2018) either explicitly or implicitly transformed noisy labels into clean labels by correcting classifi-
cation losses. Unlike these methods, our method transforms the holistic information from uncertain
samples into certain samples, which implicitly reduces the effects of potentially noisy labels. While
correction of label noisy by modifying the loss-dynamics do not perform well under extreme noise
environments, Arazo et al. (2019) adopt label augmentation method called MixUp Zhang et al. (2018).
1Due to the technical difficulties, we define our central objects on pre-softmax space rather than label space,
i.e., the space of σ(X), σ(Y ), where σ indicates softmax function. Please refer to Appendix for more details.
2
Under review as a conference paper at ICLR 2021
Distillation. Li et al. (2019b) updated mean teacher parameters by calculating the exponential
moving average of student parameters to mitigate the impact of gradients induced by noisy labels.
Lukasik et al. (2020) deeply investigated the effects of label smearing for noisy labels and linked
label smoothing to loss correction in a distillation framework. Similar to these methods, our method
leverages the useful properties of distillation models. We set ν as a pivot measure, which guides
our normalization functional Fμ for uncertain measures. This is similar to self-distillation because
uncertain training samples are forced to be normalized to those of past states.
Other methods. Lee et al. (2019) induced a robust generative classifier based on pre-trained deep
models. Similar to our method, Damodaran et al. (2019) designed a constraint on the Wasserstein
space and adopted an adversarial framework for classification models of noisy labeled data by
implementing semantic Wasserstein distance. Pleiss et al. (2020) identify noisy labeled samples by
considering AUM statistics which exploits differences in training dynamics of clean and mislabeled
samples. In most recent work, Li et al. (2019a) adopts semi-supervised learning (SSL) methods to
deal with noisy labels where the student network utilizes both labeled/unlabeled samples to perform
semi-supervised learning guided by the other teacher network.
3	Distributional Normalization
Because our main target object is a probability measure (distribution), we first define an objective
function in a distributional sense. Let l be cross entropy and r be a corrupted label random vector
for an unknown label transition matrix from a clean label r which is independent of X , with label
transition matrix Q. Then, a conventional objective function for classification with noisy labels can
be defined as follows:
mμn J[μ] = mmin EX〜μ,r∣Q [l(X； ^] .	(2)
However, due to the significant changes in label information, the conventional objective function
defined in equation 2 cannot be used for accurate classification. Instead of directly using uncertain
samples X 〜μ as in previous works, We normalize μ in the form of a metric ball and present a
holistic constraint. For a clear mathematical description, we first introduce the following definition.
Definition 1. (Wasserstein ambiguity set) Let P2(Rd) = {μ : EμdE(xo, x) < ∞,∀x° ∈ Rd} be a
2-Wasserstein space, where d denotes the number of classes, dE is Euclidean distance defined on Rd.
Then, we define a Wasserstein ambiguity set (i.e., metric ball) in this space as follows:
Bw2(ν,ε) = {μ ∈P2 (Rd) : W2(μ,ν) ≤ ε},	⑶
where W2 denotes the 2-Wasserstein distance and ν is the pivot measure.
Then, we propose a new objective function by imposing geometric constraints on μ as follows:
min J [Fμ] + J [ξ]=minEX〜F*°,r[l(X; r)]+ EX〜ξθ,r[l(Y; r)],	(4)
Fμ∈Bw2 (ν,ε),ξ	θ	''
where F : P2(Rd) → P2(Rd) is a functional for probability measures, which assures the constraint
on Fμ (i.e., Fμ ∈ Bw2 (ν, ε)) and our main objective. The right-hand side of equation equation 4
is equivalent vectorial form of distributional form in left-hand side. While our main objects are
defined on pre-softmax, both probability measures μθ and ξθ is parameterized by neural network
with parameters θ. This newly proposed objective function uses the geometrically enhanced version
of an uncertain measure Fμ with a certain measure ξ. In equation 4, probability measure V is
defined as follows: ν = arg min J [ξk?], where ξk denotes a certain measure at the current k-th
iteration and k? ∈ Ik-ι = {1,…，k - 1}. In other words, our method finds the best probability
measure that represents all certain samples so far at training time, where the uncertain measures
are transported to be lying in the Wasserstein ball centered on ν. In equation 4, the Wasserstein
constraint on Fμ enforces uncertain measures statistically resemble V from a geometric perspective
(i.e., W2 (ν, Fμ) ≤ ε).
Now, an important question naturally stems from the aforementioned analysis: how can we select the
optimal radius ε? Clearly, finding an F that induces a small ε ≈ 0 is suboptimal because Fμ ≈ V and
using objective function J[Fμ ≈ V] can lead to the following critical problem. As the optimization
process proceeds, enhanced uncertain samples X0 〜Fμ contribute less and less, because it is
statistically identical to V, meaning our objective in equation 4 would receive little benefits from
these transported uncertain samples. By contrast, if we adopt a large radius for ε, enhanced uncertain
samples will be statistically and geometrically unrelated to V, which causes the normalized measure
Fμ to yield large losses and violates our objective.
3
Under review as a conference paper at ICLR 2021
(a) Divergence or convergence of μ from V	(b) Classification accuracy
Figure 1: Accuracy begins to drop when the uncertain measure μ begins to diverge from V. In
classification models with vanilla cross entropy losses, the uncertain measure μ can easily diverge
from V in the Wasserstein space (the red dotted line in (a)), which induces an accuracy drop (the red
dotted line in (b)). By contrast, the proposed WDN can prevent such divergence by normalizing μ
onto Wasserstein ambiguity set BW2 (V, ε) (the black dotted line in (a)) and can consistently enhance
accuracy as iteration proceeds (the black line in (b)). Please note that d1 and d2 denote the first and
second terms in equation 5, respectively, and ε = d1 + d2 .
To overcome two problems above and select the radius, we make a detour, i.e., a Gaussian measure,
for cutting the path between V and Fμ (i.e., V → N(mν, ∑ν) → Fμ) rather than directly calculating
the geodesic between V and Fμ (i.e., V → Fμ). Specifically, we decompose the original constraint
in equation 4 into two terms using the triangle inequality of the Wasserstein distance:
W2 (v, Fμ) ≤ ε = W2 (v,N(mν, ∑)) + W2 (N(m“, ∑ν), Fμ).	(5)
、	一V-	/ 、 ' 一 /
d1 : Intrinsic statistics	d2 : Wasserstein Normalization
The first intrinsic statistics term sets a detour point as a Gaussian measure, for which the mean
and covariance are the same as those for V (i.e., mν = EY〜ν[Y] and ∑ = CoVY〜ν[Y]). The
Wasserstein upper bound of this term is only dependent on the statistical structure of V because
(mν , Σν ) is dependent on V . Thus, this term induces a data-dependent, non-zero constant upper
bound whenever V 6= N and can prevent the upper-bound from collapsing to ε → 0, regardless
of F. This gives huge advantage when dealing with ε because the first term can be considered a
fixed constant during the training. The second normalization term represents our central objective.
F facilitates geometric manipulation in the Wasserstein space and prevent uncertain measure μ
from diverging, where μ is normalized onto the Wasserstein ambiguity Bw? (v, ε) in Fig1. The
theoretical/numerical advantages of setting detour measure as Gaussian is well-explained following
section.
3.1	Wasserstein Normalization
In the previous section, we present a novel objective function that imposes a geometric constraint on
μ such that the transformed measure Fμ lies in Bw? (v, ε) for v. Now, we specify F and relate it to
the Gaussian measure (generally Gibbs measure). For simplicity, we denote Nν = N(mν , Σν).
Proposition 1. F : R+ ×P2 → P2 is afunctional on the probability measure such that F [t, μ] = μt,
where dμt = PtdNV, dNν = dqtdx, and μt is a solution to thefollowing continuity equations:
∂tμt = V ∙ (μtVt) ,	(6)
which is read as ∂tp(t, X) = V ∙ (p(t, x)V log q(t, x)) in a distributional sense. Then, a uniquely
defined functional Ft[∙] = F [t, ∙] normalizes μ onto Bw2 (NV, e-tK2 (μ)), where K (μ) > 0 is a
constant that depends on μ.
It is well known that the solution to equation 6 induces a geodesic in the 2-Wasserstein space (Villani
(2008)), which is the shortest path from μ = μt=o to NV. The functional Ft generates a path for μt,
in which the distance is exponentially decayed according to the auxiliary variable t and constant K2,
meaning W2 (NV, Ftμ) ≤ K2e-t. This theoretical results indicates that the Wasserstein distance of
second term in equation 5 can be reduced/controlled with exponential ratio. Thus, by setting a
different t, our method can efficiently control the diverging distance in equation 5. Unfortunately, it
is typically intractable to compute the partial differential equation (PDE) in equation 6.
4
Under review as a conference paper at ICLR 2021
Algorithm 1 Wasserstein Distributional Normalization
Require: α ∈ [0, 0.2], % ∈ [0.1, 0.65],T = 64, ∆t = 10-4,τ = 0.001,
for k = 1 to K (i.e., the total number of training iterations) do
1)	Select uncertain (1 - ρ)N and certain ρN samples from the mini-batch N .
{Ykn }{n≤ρN} ~ ξk, {Xkn} {n≤(1-ρ)N} ^ μk
2)	Update the most certain measure ν.
ifJ [ξk] < J [ν] then
V 一 ξk, mν — E [H], and ∑ν — Cov [K]
end if
3)	Update the moving geodesic average N (mα , Σα).
Solve the Ricatti equation TΣνT = Σξk .
Σα = ((1 - α)Id + αT) Σν ((1 - α)Id + αT) and mα = (1 - α)mν + αmξk
4)	Simulate the discrete SDE for T steps.
for t = 0 to T — 1 do
Xn,t+1 = -Vφ(Xn,t; mα)∆t + √2Γ-τ∑αdWtn s.t. {xn,t=°}〜μk, {xn,t=τ}〜Ftμk
end for
5)	Update the network with the objective function.
J [Fμk] + J [ξk ] = EFT μk [i(Xk,τ; ^)] + Eξk [i(Yk; r)]
end for
To solve this problem, we adopt particle-based stochastic dynamics, which enables tractable com-
putation. There exists a unique iterative form corresponding PDE in equation 6 which is called
as multi-dimensional Ornstein-Ulenbeck process, which can be approximated using particle-based
dynamics. In particular, we draw N (1 - %) uncertain samples from a single batch of N samples
using equation 1 for hyper-parameter 0 ≤ % ≤ 1. We then simulate a discrete stochastic differential
equation (SDE) for each particle using the Euler-Maruyama scheme as follows:
χt+ι = Xn -vφ(xn；m“ )∆t + √2τ-1∆t∑zn,	⑺
where φ (Xt； m”) = 2dE (Xt, m”)，n ∈ {1 ∙∙∙ , N(1 一 %)}, “e is a Euclidean distance, and N is a
single mini-batch size. We selected OU process as our stochastic dynamic due to the following reasons:
First, we want to build computationally efficient, and non-parametric method to estimate/minimize
the second term of equation 5. The SDE in equation 7 corresponding OU process have simple
form with fixed drift and diffusion terms which is invariant over times which makes us to induce
the non-parametric representations of simulation of SDE. While the simulation of equation 7 is
just non-parametric for-loops in implementation algorithm, our method is computationally very
efficient compared to other baseline methods such as Han et al. (2018). Second, when estimating
empirical upper-bound of Wasserstein distance, OU process allows us to use explicit form called
Meheler’s formula which can be efficiently estimated (Please refer to Appendix for more details).
The overall procedure for our method is summarized in Algorithm 1.
3.2	Wasserstein Moving Geodesic Average
In our experiments, we observe that the best measure ν is not updated for a few epochs after the
training begins. This is problematic because ν diverges significantly from the current certain measure
ξk, which is equivalent to the normalized measure Fμk diverging from ξk, meaning XT and Y
become increasingly statistically inconsistent. To alleviate this statistical distortion, we modify
detour measure from N” to other Gaussian measure, which allows us to capture the statistics of
both ξk and ν. Inspired by the moving average of Gaussian parameters in batch normalization Ioffe
& Szegedy (2015), we propose the Wasserstein moving geodesic average. Specifically, we replace
Gaussian parameters {m”, ∑ν} with {mα, Σα} such that mɑ = (1 一 a)m” + αmξk and Σα =
((1 一 α)Id + aT) ∑ν ((1 一 α)Id + ɑT), where T is a solution to the Riccati equation T∑νT =
∑ξk. Therefore our final detour Gaussian measure is set to Na ：= N(m(α), ∑(α)), 0 ≤ α ≤ 12.
4	Theoretical Analysis
In equation 5, we select the detour point as a Gaussian measure because this measure can provide a
statistical structure, which is similar to that of the optimal ν . In addition to this heuristic motivation,
setting a detour point as a Gaussian measure (Gibbs measure) also provides theoretical advantages,
e.g., the theoretical upper bound of the Wasserstein constraint terms. In this section, we investigate
the explicit upper bounds of two terms in equation 5, which are naturally induced by the SDE.
2Please refer to Appendix C.4 for more details.
5
Under review as a conference paper at ICLR 2021
Proposition 2. A scalar 0 < β < ∞ exists and depends on ν, resulting in the following inequality:
W2(ν,Ftμ) ≤ ε = Kι(ν) ∨ [e-tK2(μ)+ K2(ν)] ,	(8)
where λmax (Σν) denotes the maximum eigenvalue of the covariance matrix Σν and for some constant
0 < Ki < ∞, we have Kι(ν) = /dβλmaχ(∑ν) + ∣∣EνY∣∣2 which is only dependent on V.
Intuitively, K2(μ) can be interpreted as an indicator that tells Us how the uncertain measure μ is
diffused, whereas the designed term e-tK2(μ) controls the upper bound of the Wasserstein distance
using a variable t. The other term K2(ν) does not vanish even with a very large t, which assures a
non-collapsing upper-bound ε.
Proposition 3. (Concentration inequality for the normalized uncertain measure). Assume that
there are some constants T ∈ [ 1, ∞), η ≥ 0 such that the following inequality holds:
EFTμ[f2] - [Eftμ[f]]2 ≤ (1 + η)EFτμ[AVfTVf],	f ∈ C∞(Rd),	(9)
for A ∈ Symd+ and D(A, Σν) ≤ aη for some a > 0 with any metric D defined on Symd+. In this
case, there is a δ such that the following probability inequality for an uncertain measure is induced:
/	、	√2δ2
FTμ( ∣σ — EV[σ]∣ ≥ δ) ≤ 6e-KW,	(10)
where σ denotes a soft-max function.
In equation 10, we show that the label information induced by the normalized uncertain measure is
close to that of most certain measure Eν [σ], where the upper bound is exponentially relative to the
initial diffuseness of μ (i.e., K (μ)). Because the upper bound of the probability inequality does not
collapse to zero and FTμ is concentrated around the most certain labels (i.e., EV [σ]), the uncertain
sample XT ~ FTμ helps our method avoid over-parameterization.
4.1	Empirical Understandings
We investigate the theoretical upper bound of the Wasserstein ambiguity (i.e., radius of the Wasser-
stein ball) for Fμ and its corresponding probability inequality. To provide more in-depth insights
into the proposed method, we approximate the upper bound and demonstrate that our Wasserstein
normalization actually makes neural networks more robust to label noise.
As we verified previously, according to Proposition 2, the following inequality holds:
W2(Ftμ, V) ≤ ε = Kι(ν) ∨ (K2(ν) + K2(Ftμ)).	(11)
Because the first term K1(ν) is constant, dependent on ν, and generally small compared to the second
term with t ≤ T, we only examine the behavior of the second term K (ν) + K (Ftμ), which can be
efficiently approximated using a simple form. Because our detour measure is Gaussian, we have the
following inequality for any h ∈ C0∞ (Rd)3:
K2(μ) = IimSEx,z~Ni [h 卜-SX + P - e-2s(∑∣Z + m”)) - h(X)] ≤ K2(μ),	(12)
where this equality holds if h is selected to induce a supremum over the set C0∞ . For approximation,
we simply consider h(X) = ∣X∣2 as a test function. In this case, the following inequality naturally
ʌ ʌ , , ..
holds: ε = K2(ν) + K2(Fμ) ≤ K2(ν) + K2(Fμ) ≤ Kι(ν) ∨ (K2(ν) + K2(Fμ)) = ε. Thus, ε
can be considered as an approximation of the theoretical upper bound ε suggested in Proposition 2.
Subsequently, we investigate the effects of Wasserstein normalization based on K2(μ) In equation 12.
(1)	The proposed WDN ensures that the Wasserstein ambiguity is bounded. We examine the
relation between ε and test accuracy in an experiment using the CIFAR-10 dataset with symmetric
noise at a ratio of 0.5. Fig.2 presents the landscape for the log 10-scaled cumulative average of ε and
test accuracy over epochs. The red dotted lines represent the landscape of the vanilla network with
cross-entropy loss, where ε^k = K2(νk) + K2(Ft=0μk) and k is the epoch index. In this case, the time
constant t is set to zero, because Wasserstein normalization is not employed for the vanilla network.
The black lines indicate the landscape of the proposed method, where ε^k = K2(νk) + K2(Ft=Tμk)
3Please refer to Appendix C.2 for additional details.
6
Under review as a conference paper at ICLR 2021
Figure 2: Relation between the approximated upper bound ε and test accuracy.
in this case. It is noteworthy that the test accuracy of the vanilla network begins to decrease after 13-
epochs (red-dotted vertical lines in the top-right plot), whereas the Wasserstein ambiguity (i.e., upper
bound of the Wasserstein distance) increases quadratically in the top-left plot. These experimental
results verify that the distance between uncertain and most certain measure (i.e., ν) becomes large in
the 2-Wasserstein space without any constraints in vanilla networks. They also indicate a definite
relationship between Wasserstein ambiguity and test accuracy. In the proposed WDN, Wasserstein
ambiguity can be efficiently bounded (i.e., limsupk εk ≈ 2.15) as the test accuracy continues to
increase, even after 13-epochs. For detailed analysis, we compute the deviation of an empirical upper
bound as follows: ∆k = εk - εk-ι. In the gray regions, the deviation for the vanilla network is
grater than 2.5 × 10-2, i.e., ∆k > 2.5 × 10-2. Then, its test accuracy begins to drop, as shown in
Fig.2. In contrast to the vanilla network, the maximum deviation of the proposed WDN is bounded
above by a very small value (SuPk ∆k ≤ 8 X 10-3).
(2)	The proposed WDN helps networks to escape from over-parameterization. To analyze the
behavior of deep neural networks under over-parameterization with and without the proposed WDN,
we design several variants of the WDN, which begin at delayed epochs. The green, orange, and
blue curves in the second row of Fig.2 represent the landscapes, when our WDN is applied after
kd ∈ {10,15,20} epochs, respectively. In this experiment, the upper bound ɛk is defined as
(
K2(νk) + K2(Ft=0μk),	if k<kd,	(13)
K2(νk) + K2(Ft=τμk), else k ≥ kd.
Consider kd = 20, which is represented by the blue dotted vertical lines. Before our WDN is
applied (i.e., k < kd), the network suffers from over-parameterization, which induces a significant
performance drop, as indicated by the blue curve in the bottom-right plot. However, the network
rapidly recovers to normal accuracy following Wasserstein normalization (i.e., k ≥ kd). Please
note that similar behavior can be observed in the green and orange curves. In particular, the orange
curve produces less fluctuations than the blue curve in terms of test accuracy. This indicates that
the proposed WDN can help a network escape from over-parameterization by imposing geometric
constraints on the Wasserstein space with proposed method.
(3)	The proposed WDN can derive data-dependent bounds according to different noise levels.
Another interesting point in Fig.2 is that all curves, excluding the red curve, converge to specific
numbers 2.15 = ε := liminfk ɛk ≤ limsupk ɛk := ε = 2.2. The upper bound ε is neither overly
enlarged nor collapsed to zero, while the lower bound ε is fixed for all curves. We argue that this
behavior stems from the geometric characteristics of the proposed method, where the first term in
equation 5, namely W2 (ν, NV) H K2(ν), IS a non-zero data-dependent term that IS minimized by the
proposed geometric constraint. Therefore, we can derive the following relationship:
一 . . . . ., .. , ^ . . ^ . ,..
[W2(ν, F μ) ≤W2(ν, NV)+ W2(NV, F μ)] U H [K2(ν)+K2 (Fμ) = ε] U.	(14)
This empirical observation verifies that a detour point, which is set as a Gaussian measure, can induce
the data-dependent bound (ε, ε), where our data-dependent bound can vary according to different
7
Under review as a conference paper at ICLR 2021
Table 1: Average test accuracy (%) on the CIFAR-10/100 dataset over the last 10 epochs with various noise corruptions. The symbol ? indicates scores provided by the corresponding authors. WDNcot denotes our WDN combined with a co-teaching network. The best results are boldfaced.			
Methods	Symmetric 20%	Symmetric 50%	Asymmetric 45%
Vanilla	71.91 ± .43/40.44 ± .36	49.54 ± .41/21.34 ± .27	49.06 ± 1.02/31.85 ± .85
MentorNet?	80.76 ± .36/52.13 ± .40	71.10 ± .48/39.00 ± 1.00	58.14 ± .38/31.60 ±.51
GCE	84.68 ± .05/51.86 ± .09	61.80 ± .11/37.60 ± .08	61.09 ± .18/33.13 ± .14
RoG?	84.32	/	58.16	76.67	/	45.42	71.26	/	43.18
JoCoR	85.73 ± .19 /53.01 ± .04	79.41 ± .25 /43.49 ± .46	64.21 ± .12/26.51 ± .32
NPCL?	84.30 ± .07/55.30 ± .09	77.66 ± .09/42.56 ± .06	-
SIGUA?	≤ 84	/	—	≤ 78	/	-	≤ 65	/	-
DivideMix	-	81.13 ±.18 / 49.41 ±.25	68.93 ± .33 / 34.24 ± .63
WDN	87.40 ± .23 /59.18 ± .29	82.89 ± .13/48.45 ± .27	76.12 ± .29/38.23 ±.31
Co-teaching	78.23 ± .27/53.89 ± .09	72.81 ± .20/34.96 ± .50	70.46 ± .58/34.55 ± .12
Co-teaching+	80.64 ± .15/56.15 ± .09	58.43 ± .30/37.88 ± .06	70.78 ± .11/32.88 ± .25
WDNcot	87.12 ± .16/57.27 ± .33	76.06 ± .28/42.38 ±.28	74.11 ± .35/44.41 ± .37
Table 2: Test accuracy on the CIFAR-10 dataset with open-set noisy labels from CIFAR-100.			
Methods ∣ Vanilla GCE Co-teaching	Co-teaching+			JoCoR WDN
Accuracy ∣ 38.12	46.57		35.77	42.57	47.73	51.28
noise levels and efficiently leverage data-dependent statistics. Fig.2 indicates that classification
models with more stable data-dependent bounds also induce more stable convergence in test accuracy.
5	Experiments
5.1	Experiments on the CIFAR-10/100 dataset
We used settings similar to those proposed by Laine & Aila (2016); Han et al. (2018) for our
experiments on the CIFAR10/100 dataset. We used a 9-layered CNN as a baseline architecture with a
batch size of 128. We used the Adam optimizer with (β1, β2) = (0.9, 0.99), where the learning rate
linearly decreased from 10-3 to 10-5.
Synthetic Noise. We injected label noise into clean datasets using a noise transition matrix Qi,j =
Pr(r = j|r = i), where a noisy label r is obtained from a true clean label r. We defined Qi,j by
following the approach discussed by Han et al. (2018). For symmetric noise, we used the polynomial,
% = -1.11r2 + 1.78r + 0.04 for 0.2 ≤ r ≤ 0.65, where r is the noise ratio. For the asymmetric
noise, we set % to 0.35. To select the enhanced detour measure, we set α to 0.2 for the Wasserstein
moving geodesic average in all experiments. We trained our classification model over 500 epochs
because the test accuracy of our method continued increasing, whereas those of the other methods
did not. We compared our method with other state-of-the-art methods, including [MentorNet, Jiang
et al. (2018)], [Co-teaching, Han et al. (2018)], [Co-teaching+, Yu et al. (2019)], [GCE, Zhang &
Sabuncu (2018)], [RoG, Lee et al. (2019)], [JoCoR, Wei et al. (2020)], [NPCL, Lyu & Tsang (2020b)],
[SIGUA, Han et al. (2020)], and [DivideMix, Li et al. (2019a)]. As shown in Table 1, the proposed
WDN significantly outperformed other baseline methods. Please note that our WDN utilizes a simple
Gaussian measure as a target pivot measure. Thus, there are potential risks when handling highly
concentrated and non-smooth types of noise (e.g., asymmetric noise). Nevertheless, the proposed
WDN still produced accurate results, even with asymmetric noise. In this case, a variant of our WDN
(i.e., WDNcot) exhibited the best performance.
Open-set Noise. In this experiment, we considered the open-set noisy scenario suggested by Wang
et al. (2018), where a large number of training images were sampled from other CIFAR-100 dataset;
however, these images were still labeled according to the classes in the CIFAR-10 dataset. We used a
9-layered CNN, which also used in our previous experiment. For hyper-parameters, we set % and α to
0.5 and 0.2, respectively. As shown in Table 2, our method achieved state-of-the-art accuracy.
Collaboration with Other Methods. Because our core methodology is based on small loss criteria,
our method can collaborate with co-teaching methods. In Han et al. (2018), only certain samples
(Y 〜ξ) were used for updating colleague networks, where the number of uncertain samples
gradually decreased until it reached a predetermined portion. To enhance potentially bad statistics for
co-teaching, We taught dual networks by considering a set of samples (Y, XT), where XT ~ Ftμ
are uncertain samples enhanced using equation 7.
8
Under review as a conference paper at ICLR 2021
Figure 3: Test accuracy for the proposed collaboration model with co-teaching.
Table 1 shows the test accuracy results for the proposed collaboration model with a co-teaching
network (WDNcot). This collaboration model achieved the most accurate performance for the CIFAR-
100 dataset with asymmetric noise, which verifies that our WDN can be integrated into existing
methods to improve their performance significantly, particularly when the density of pre-logits is
highly-concentrated. Fig.3 reveals that co-teaching quickly falls into over-parameterization and
induces drastic drop in accuracy after the 15th-epoch. WDNcot also exhibits a slight accuracy drop.
However, it surpassed the baseline co-teaching method by a large margin (+7%) during training.
This demonstrates that our enhanced samples XT can alleviate the over-parameterization issues faced
by conventional co-teaching models, which helps improve their accuracy significantly.
5.2	Experiments on a Real-world dataset
To evaluate our method on real-world datasets, we employed the Clothing1M dataset presented
by Xiao et al. (2015), which consists of 1M noisy, labeled, and large-scale cloth images with 14
classes collected from shopping websites. It contains 50K, 10K, and 14K clean images for training,
testing, and validation, respectively. We only used a noisy set for training; for testing, we used a
clean set. We set α = 0.2 and % = 0.1. For fair comparison, we followed the settings suggested in
previous works. We used a pre-trained ResNet50 for a baseline architecture with a batch size of 48.
For the pre-processing steps, we applied a random center crop, random flipping, and normalization to
224 × 224 pixels. We adopted the Adam optimizer with a learning rate starting at 10-5 that linearly
decayed to 5 × 10-6 at 24K iterations. Regarding the baseline methods, we compared the proposed
method to [GCE, Zhang & Sabuncu (2018)], [D2L, Ma et al. (2018)], [FW, Patrini et al. (2017b)],
[WAR, Damodaran et al. (2019)], [SL, Wang et al. (2019)], [JOFL, Tanaka et al. (2018)], [DMI, Xu
et al. (2019)], [PENCIL, Yi & Wu (2019)], and [MLNT, Li et al. (2019b)]. Table 3 reveals that our
method achieved competitive performance as comparison with other baseline methods.
	Table 3: Test accuracy (mean, %) on the Clothing 1M dataset.___
Methods ∣ GCE_________________________________________________________________D2L	FW	JoCoR WAR SL JOFL DMI	MLNT PENCIL	WDN	DivideMix
Accuracy ∣ 69.0	69.47	69.84	70.30 70.66 71.02 72.23 72.46	73.47	73.49	74.75	74.76
5.3	Computational Cost
Because Co-teaching, JoCoR, and DivideMix use additional networks, the number of network
parameters is twice (8.86M) as many as that of the Vanilla network (4.43M). In Table 4, we compare
the average training time for first 5-epochs over various baseline methods under symmetric noise on
the CIFAR-10 dataset. While non-parametric methods such as GCE and WDN require less than 12%
additional time, other methods that require additional networks spent more time than non-parametric
methods. The averaging time can vary according to different experimental environments. In table 4,
we measure the time using publicly available code provided by authors.
Table 4: Average training time for the 5-epochs (sec) on the CIFAR-10 dataset.
Methods I	Vanilla	GCE WDN	Co-teaching JoCoR	DivideMix
Time	11.43 ± .05 11.53 ± .06 12.72 ± .08 15.88 ± .11 17.88 ± .11 34.41 ± .53
∆	-	+9%	+11.3%	+38.9%	+56.3%	+201%
6	Conclusion
We proposed a novel method called WDN for accurate classification of noisy labels. The proposed
method normalizes uncertain measures to data-dependent Gaussian measures by imposing geometric
constraints in the 2-Wasserstein space. We simulated discrete SDE using the Euler-Maruyama scheme,
which makes our method fast, computationally efficient, and non-parametric. In theoretical analysis,
we derived the explicit upper-bound of the proposed Wasserstein normalization and experimentally
demonstrated a strong relationship between this upper-bound and the over-parameterization. We
conducted experiments both on the CIFAR-10/100 and Clothing1M datasets. The results demonstrated
that the proposed WDN significantly outperforms other state-of-the-art methods.
9
Under review as a conference paper at ICLR 2021
References
LUigi Ambrosio, Nicola Gigli, GiUsePPe Savara et al. Bakry-Cmery CUrvatUre-dimension condition
and riemannian ricci curvature bounds. The Annals ofProbability, 43(1):339-404, 2015.
Eric Arazo, Diego Ortego, PaUl Albert, Noel O’Connor, and Kevin McgUinness. UnsUPervised
label noise modeling and loss correction. In International Conference on Machine Learning, PP.
312-321, 2019.
DominiqUe Bakry, Ivan Gentil, and Michel LedoUx. Analysis and geometry of Markov diffusion
operators, volUme 348. SPringer Science & BUsiness Media, 2013.
FrangoiS Bolley and Ivan Gentil. Phi-entropy inequalities for diffusion semigroups. Journal de
mathematiquespures et appliquees, 93(5):449^73, 2010.
Pengfei Chen, Benben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing
deep neural networks trained with noisy labels. arXiv preprint arXiv:1905.05040, 2019.
Bharath Bhushan Damodaran, Kilian Fatras, Sylvain Lobry, RCmi Flamary, Devis Tuia, and
Nicolas Courty. Wasserstein adversarial regularization (war) on label noise. arXiv preprint
arXiv:1904.03936, 2019.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor W. Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
NeurIPS, 2018.
Bo Han, Gang Niu, Xingrui Yu, Quanming Yao, Miao Xu, Ivor W Tsang, and Masashi Sugiyama.
Sigua: Forgetting may make learning with noisy labels more robust. In ICML, 2020.
D. Hendrycks, M. Mazeika, D. Wilson, and K. Gimpel. Using trusted data to train deep networks on
labels corrupted by severe noise. In NeurIPS, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In ICML, 2015.
L.	Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Mentornet: Regularizing very deep neural
networks on corrupted labels. In ICML, 2018.
Oliver Johnson and Yurii Suhov. Entropy and random vectors. Journal of Statistical Physics, 104:
145-165, 01 2001. doi: 10.1023/A:1010353526846.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint
arXiv:1610.02242, 2016.
Kimin Lee, Sukmin Yun, Kibok Lee, Honglak Lee, Bo Li, and Jinwoo Shin. Robust inference via
generative classifiers for handling noisy labels. In ICML, 2019.
Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-
supervised learning. In International Conference on Learning Representations, 2019a.
Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy
labeled data. In CVPR, 2019b.
Michal Lukasik, Srinadh Bhojanapalli, Aditya Krishna Menon, and Sanjiv Kumar. Does label
smoothing mitigate label noise? arXiv preprint arXiv:2003.02819, 2020.
Yueming Lyu and Ivor W. Tsang. Curriculum loss: Robust learning and generalization against label
corruption. In ICLR, 2020a.
Yueming Lyu and Ivor W. Tsang. Curriculum loss: Robust learning and generalization against label
corruption. In ICLR, 2020b.
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. arXiv preprint
arXiv:1806.02612, 2018.
10
Under review as a conference paper at ICLR 2021
LUigi Malagd, LUigi Montrucchio, and Giovanni Pistone. Wasserstein riemannian geometry of
gaussian densities. Information Geometry, 1(2):137-179, 2018.
Robert J McCann. A convexity principle for interacting gases. Advances in mathematics, 128(1):
153-179, 1997.
Ivan Nourdin, Giovanni Peccati, and Yvik Swan. Entropy and the fourth moment phenomenon.
Journal of Functional Analysis, 266(5):3170-3207, 2014.
Felix Otto and Cedric Villani. Generalization of an inequality by talagrand and links with the
logarithmic sobolev inequality. Journal of Functional Analysis, 173(2):361-400, 2000.
G. Patrini, A. Rozza, A. K. Menon, R. Nock, and L. Qu. Making deep neural networks robust to label
noise: A loss correction approach. In CVPR, 2017a.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017b.
Geoff Pleiss, Tianyi Zhang, Ethan R Elenberg, and Kilian Q Weinberger. Identifying mislabeled data
using the area under the margin ranking. arXiv preprint arXiv:2001.10528, 2020.
M.	Ren, W. Zeng, B. Yang, and R. Urtasun. Learning to reweight examples for robust deep learning.
In ICML, 2018.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework
for learning with noisy labels. In CVPR, 2018.
C.	Villani. Optimal Transport: Old and New. Grundlehren der mathematischen Wissenschaften.
Springer Berlin Heidelberg, 2008. ISBN 9783540710509.
Yisen Wang, Weiyang Liu, Xingjun Ma, James Bailey, Hongyuan Zha, Le Song, and Shu-Tao Xia.
Iterative learning with open-set noisy labels. In CVPR, 2018.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
entropy for robust learning with noisy labels. In ICCV, 2019.
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint
training method with co-regularization. In CVPR, 2020.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In CVPR, 2015.
Yilun Xu, Peng Cao, Yuqing Kong, and Yizhou Wang. L_dmi: A novel information-theoretic loss
function for training deep nets robust to label noise. In NeurIPS, 2019.
Kun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In
CVPR, 2019.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does
disagreement help generalization against label corruption? In ICML, 2019.
Xiyu Yu, Tongliang Liu, Mingming Gong, and Dacheng Tao. Learning with biased complementary
labels. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 68-83, 2018.
Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical
risk minimization. In International Conference on Learning Representations, 2018.
Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In NeurIPS, 2018.
11
Under review as a conference paper at ICLR 2021
A Open-source Dataset
Transition matrix for CIFAR-10/100. For the experiment summarized in Table 1, we implemented
open-source code to generate the noise transition matrix discussed by Han et al. (2018), as well as the
9-layered CNN architecture (https://github.com/bhanML/Co-teaching).
Open-set noise. For the experiment summarized in Table 2, we used the same dataset for
open-set noisy labels presented by Lee et al. (2019) (https://github.com/pokaxpoka/
RoGNoisyLabel).
Clothing1M. For the experiment summarized in Table 3, we used the open-source dataset presented
by Xiao et al. (2015) (https://github.com/Cysu/noisy_label).
B Comparisons to Related Works
Methodology ∣ Parametric Class-dependency Distillation Sample-weight Sample-selection
DivideMix	✓	X	X	X	✓
Co-teaching	✓	X	✓	X	✓
JoCoR	✓	X	✓	X	✓
MLNT	✓	✓	✓	X	X
Ren et al. (2018)	X	X	X	✓	X
NPCL	X	X	X	✓	X
GCE	X	X	X	✓	X
WDN	X	X	X	X	X
Table B indicates that no previous methodologies can conceptually include our method.
Because the solution to the Fokker-plank equation can be explicitly calculated without any additional
parameters, our method is fully non-parametric (in terms of additional parameters beyond those
required by the original neural network). By contrast, co-teaching is parametric because it requires
a clone network with additional parameters that are copies of those in the original network. Simi-
larly, MLNT requires an additional teacher network for training, which also contains a number of
parameters.
Many method based on small loss criteria select certain samples, whereas our method uses the
combination of ρN certain and (1 - ρ)N normalized uncertain samples. Therefore, our method
can fully leverage the batches of training datasets, where (1 - ρ)N + ρN = N. Additionally, our
method does not assume any class-dependent prior knowledge. Rather than considering class-wise
prior knowledge, our method uses holistic information from both certain and uncertain samples (i.e.,
Y and XT ) in the logit space. Other meta-class-based model, such as MLNT, assume class-wise
meta prior knowledge from a teacher network.
In Arazo et al. (2019), they assumed the beta-mixture model as a label distribution on label space. But
due to the non-deterministic type of noisy label distribution, it sometimes fails to train with extremely
non-uniform type of noise. For example, Arazo et al. (2019) reported failure case with Clothing1M
dataset. It seems that fundamental assumption on noise model of mixup will be improved in future
work. Similar to this method, our work have trouble when dealing with synthetic asymmetric noise
with high ratio where relatively large performance drop is observed in Table 1 (despite our method
produces second best performance in the table).
Most recent work Li et al. (2019a), they also adopt Co-train by implementing additional dual network,
but much sophisticated methodology called Co-divide/guessing based on SSL. We predict that the
Wasserstein distance between labeled and unlabeled probability measures is well-controlled in their
method. We think that applying the OT/Markov theory (as in our paper) to their method will broaden
the understanding of LNL problem.
In contrast to sample weight methods such as GCE and NPCL, which require prior knowledge
regarding the cardinality of the training samples to be weighted, our method is free from such
assumptions because our Wasserstein normalization is applied in a batch-wise manner.
12
Under review as a conference paper at ICLR 2021
C	Technical Difficulty for applying general Optimal
Transport/Markov theory to Label Space.
Let X, Y be uncertain and certain samples in pre-softmax feature space. And assume that we consider
the distributional constraint on label-space (the space of σ(X), σ(Y ), where σ denotes the soft-max
function). This space is not proper to define the objective function such as (5). Because, all the
samples in this label space is of the form σ(X) = [aι, a2,…,an] such that Pid=1 a% = 1, thus
label-space is d-dimensional affine-simplex Ud which is subset of Euclidean space Ud ⊂ Rd . In this
case, the definition of Wasserstein space in equation (4) is unacceptable while dE is not true metric
on Ud. The Wasserstein space P2 (Ud) is merely investigated in the mathematical literature which
makes unable to use all the technical details and assumptions, theories developed in the P2 (Rd)
which are theoretical ground of our work. But, if we look this problem slightly different point of
view, for example, consider pre-softmax Rd, P2(Rd) as our base space. In this case, all the technical
issues/problems when we try to use OT tools in P2 (Ud) can be overcome/ignored. while softmax is
non-parametric one-to-one function connecting pre-softmax feature space Rd to Ud, there exists a
unique labels in Ud as a mapped point of the manipulated uncertain samples. Even though our objects
are defined on pre-softmax space, the theoretical analysis in Proposition 3 contains softmax function
to evaluate the concentration inequality of proposed transformation F affecting in label-space Ud .
D	Mathematical Background
In this section, we introduce important definitions, notations, and propositions used in our proofs and
the main paper.
D.	1 Notation
We denote /#仙 as a push-forward of μ through f. C∞ (Rd) denotes the set of ∞-class functions
with compact support in Rd. For the Lp-norm of the function f, We denote ∣∣f kp V = (R |f ∣pdν) 1.
The Hessian matrix of the function f is denoted as Hess[f] = [∂i ∂j f]id,j . Symd+ denotes the space
for semi-definite positive symmetric matrices of size d × d. ∣f ∣Lip denotes the Lipschitz norm of
the function f . For any matrix A ∈ Md, we let ∣A∣op denote the operator norm of A.
D.2 Diffusion-invariance and Hyper-contractivity
Definition 2. The Markov semigroup (Pt)t≥0 in Rd acting on a function f ∈ C0∞ is defined as
follows:
Ptf(x) =	f(x0)pt(x, dx0),
(15)
where pt(x, dx0) is a transition kernel that is the probability measure for all t ≥ 0.
Definition 3. (Diffusion Operator) Given a Markov semi-group Pt at time t, the diffusion operator
(i.e., infinitesimal generator) L of Pt is defined as
1	∂2	∂
Lg⑻=limotm = X斫Bj(y)g(y)-XA⑹酝g(y)，	(16)
where B and A are matrix and vector-valued measurable functions, respectively. Bij denotes the
(i, j)-th function of B and Ai denotes the i-th component function of A.
Definition 4. (Diffusion-invariant Measure) Given the diffusion operator L, the probability measure
μ is considered to be invariant measure to L when Eχ~μ[Lf (X)] = 0 for any f ∈ C∞.
Lemma 1. (Infinitesimal generator for the multivariate Gaussian measure, Bolley & Gentil
(2010).) The Gaussian measure Nν := N (mν, Σν) with a mean mν and covariance Σν is an
invariant measure according to the following diffusion-operator L:
Lf(x) = ∑νHess[f](x) - (x - mν)T Vf (x), ∀f ∈ C∞(Rd),
where Bij(x) := [Σν]ijis a constant function, and Ai(x) := xi - miν.
(17)
13
Under review as a conference paper at ICLR 2021
This generator serves as our main tool for the geometric analysis of the upper bound ε. In Section 4.1
in the main paper, We introduced an approximate upper-bound K (μ) without any general description
of the inequality involved. We now introduce the underlying mathematics for equation 12. Because
our detour measure is Gaussian, there is a unique semi-group Pt h called the multidimensional
Ornstein-Ulenbeck semi-group that is invariant to Nν . Specifically, Pt is defined as follows:
Psh(X)= EZ〜NIhh 卜-SX + P - e-2s(∑Z + m“川，∀h ∈ C∞.	(18)
The invariance property of Pt relative to our detour measure is naturally induced by the following
Proposition:
Proposition 4. We define C : Rd → Rd and C(X) = AX + b such that A ∈ Symd+ , b ∈ Rd, and
select an arbitrary smooth h ∈ C0∞(Rd). We then define the diffusion Markov semi-group Psh as
follows:
Psh(X) = EZ〜N [h 卜-sX + P - e-2sC(Z))].	(19)
Then, N (A2, b) is invariant with respect to Ps, meaning the following equality holds for every h
and s ≥ 0:
[Psh(X)
-h(X)]dN(A2,b)(X) =0.
(20)
Proof. For simplicity, we denote N(A2, b) := NC.
Psh(X)dNC (X) =
h(e-sX + √1 - e-2sC(ZγμNc(X)dN(Z)
/ /h ◦ C(e-sZ0 + √1 - e-2s Z )dN (Z0)dN (Z).
(21)
The second equality holds because C is linear in Rd. Let e-s = cos θ and e-2s = sinθ for any
0 ≤ θ ≤ 2π. Then, We define φ as φ(Z0, Z) = e-sZ0 + √1 — e-2sZ = cos(θ)Z0 + sin(θ)Z, and
π(Z0, Z) = Z. Based on the rotation property of the standard Gaussian measure, one can induce the
folloWing equality.
(NXN) ◦ (C◦ φ)-1 = ((NXN) ◦ φ-1) ◦ CT = N。C-1.	(22)
However, we know that dN[C-1(X)] = dNC(X) = ((2π)d∣A2∣) 2 e-0∙5(X-b)TA 2(X-b). By
combining equation 21 and equation 22, one can derive the folloWing result:
/ h ◦ C(e-sZ0 + √1 — e-2sZ)d[NχN] = / h(X)d [(NχN)。φ-1 ◦ C-1] (X)
=/ h(X)d[N。CT](X) = / h(X)dN[C-1(X)]
=	h(X)dNc(X).
(23)
□
1
Proposition 4 demonstrates the invariance property of the defined semi-group. If we set A = ∑V, b
mν, then we can recover equation 18.
We are now ready to define the approximation of K (μ) in terms of semi-group invariance. Specifi-
cally, for any real-valued smooth h, we define the following inequality:
1
K2(μ) = EX〜μ[Lh(X)] = limEX〜μ -(Psh(X) — h(X))
s→0	s
(24)
IimSEx,z〜NIhh 卜-sX + √1 — e-2s (Σ∣ Z + m“)) — h(X)] ≤ &(〃).
This inequality holds if h is selected to induce a supremum over the set C∞, where SuPh K (μ, h)=
SuPh EX〜μ[Lh(X)] = K2(μ). Although a more sophisticated design for the test function h will
induce a tighter upper bound for K2, we determined that the L2-norm is generally sufficient.
14
Under review as a conference paper at ICLR 2021
Definition 5. (Diffuseness of the probability measure) We define the integral operator K2
W2 (Rd) → R+ as follows:
K2(μ)
j Sup Z|Lf (x)| dμ(x).
(25)
According to Definition 4, we know that Lf(X)dNν(X) = 0 for any f. Based on this observation,
it is intuitive that K2 estimates how the probability measure ν is distorted in terms of diffusion
invariance. While this measure takes a supremum over the function space C0∞, it searches for a
function that enables the estimation of maximal distortion. Because the value of K2 is entirely
dependent on the structure of μ, K can be considered as a constant for the sake of simplicity if the
uncertain measure μ is fixed over one iteration of training.
Definition 6. (Diffusion carre du champ) Let f,g ∈ C∞(Rd). Then, we define a bilinearform Γc
in C0∞(Rd) × C0∞ (Rd) as
Γe(f,g) = 1[LΓe-ι(fg) - Γe-ι(fLg)--(gLf)],	e ≥ 1.	(26)
We also denote Γ(f) ≡ Γ(f, f). The bilinear form Γ can be considered as a generalization of the
integration by the parts formula, where f f Lg + Γ(f )dμ = 0 for the invariant measure μ of L.
Definition 7. (Curvature-Dimension condition, Ambrosio et al. (2015)) We can say that the
infinitesimal generator L induces the CD (ρ, ∞) curvature-dimension condition if it satisfies
Γ1(f) ≤ρΓ2(f)forallf∈C0∞.
Because our diffusion operator generates a semi-group with respect to the Gibbs measure, the
curvature-dimension condition can be calculated explicitly. Through simple calculations, the first-
order (c = 1) diffusion Carre du champ can be induced as follows:
Γι(f) = ([Vf]TΣνVf)2.	(27)
Similarly, the second-order (c = 2) diffusion carre du champ is calculated as follows:
Γ2(f) = 1 [L (Γι(f2)) - 2Γι (f, L(f))]
2	(28)
=Tr ([∑νV2f]2) + ([Vf]TΣνVf)2 = Tr ([∑νV2f]2) +Γι(f),
for an arbitrary f ∈ C0∞(Rd). While Tr [ΣV2f]2 is non-negative, we can infer that Γ1 ≤ Γ2.
In this case, the diffusion operator L defined in Lemma 1 induces the CD(ρ = 1, ∞) curvature-
dimension condition. For the other diffusion operators, please refer to Bolley & Gentil (2010).
Proposition 5. (Decay of Fisher information along a Markov semigroup, Bakry et al. (2013).) If
we assume the curvature-dimension condition CD(ρ, ∞), then I(μt∣NV) ≤ e-2ρtI(μ∣NV).
The exponential decay of the Fisher information in Proposition 5 is a core property of the exponential
decay of the Wasserstein distance, which will be used in the proof of Proposition 2.
D.3 Fokker-Plank equation, SDE
Definition 8. (Over-damped Langevin Dynamics) We have
dXt = -Vφ(Xt; mν)dt + P2τ-1∑νdWt,	(29)
where φ (Xt; m”) = Td2 (Xt, m”), Wt denotes Brownian motion, and d denotes Euclidean dis-
tance. The particle Xt is distributed in Xt 〜Pt. The probability density limt→∞ p(x,t) with
respect to X∞ converges to the Gaussian density X∞ = √∑7(Z + m”)〜 p∞(x) = q(x) H
e-d(x,mν)T Σν-1 d(x,mν).
In classical SDE literature, it is stated that E卜upo≤t≤τ IXt- Xt ∣] ≤ G(N%)-1, where G(T) is
some constant that depends only on T and X denotes the true solution of the SDE in equation 29.
While the number of uncertain samples is greater than N% > 40, our method exhibits acceptable
convergence.
15
Under review as a conference paper at ICLR 2021
D.4 Gaussian Wasserstein Subspaces
It is known that the space of non-degenerate Gaussian measures (i.e., covariance matrices are positive-
definite) forms a subspace in the 2-Wasserstein space denoted as W2,g = Sym+ X Rd. Because
the 2-Wasserstein space can be considered as a Riemannian manifold equipped with Riemannian
metrics Villani (2008), W2,g can be endowed with a Riemannian structure that also induces the
Wasserstein metric (McCann (1997)). In the Riemannian sub-manifold of Gaussian measures, the
geodesic between two points γ(0) = NA and γ(1) = NB is defined as follows Malago et al. (2018):
γ(α) = Nt = N (m(α), Σ(α)),
(30)
where m(α) = (1 - α)mA + αmB and Σ(α) = [(1 - α)I + αT] ΣA [(1 - α)I + αT], where
TΣAT = ΣB. In Section 3.2, we set (mA, ΣA) → (mν, Σν) and (mB, ΣB) → (mξk, Σξk).
Regardless of how ν is updated, the statistical information regarding the current certain measure ξk is
considered in the detour Gaussian measure, which yields a much smoother geometric constraint on μ.
E Proofs
Proposition 6. Let Γ(μ, V) be a set of couplings between μ and V, and assume that the noisy label r
is independent of X. Forfunctional J[μ] = Eμ~χl(X; r), we define D(μ, V) as:
D(μ,ν )=	"f	|J [μ] -J [ν]∣,	(31)
Y∈Γ(μ,ν)
where D : P2 × P2 → R. Then, D is the metric defined on P2, which is weaker than the Wasserstein
metric, where D(μ, V) ≤ αW2(μ, V) for ɑ = c-1r + c-1(1 — r) and some constants co, ci > 0.
Proof.
IJ [v ] — J [μ]∣ = ∣Eμ[i(x ； r)] — EV [1(Z; r)]|
=∣E”θν [r (logσ(x) — logσ(Z)) — (1 — r) (log(1 — σ(X)) — log(1 — σ(Z)))]∣
≤ E∣rEμθν [logσ(X) — logσ(Z)]∣ + E ∣(1 — r)Eμ0ν [log(1 — σ(X)) — log(1 — σ(Z))]∣
≤ ErEμ0ν Ilogσ(X) — logσ(Z)∣ + E(1 — r)Eμ0ν ∣log(1 — σ(X)) — log(1 — σ(Z))∣	(32)
≤ c0 1E(r)E*e>v|X — Z| + C-IE(I — r)Eμ0ν |Z — X |
=E[c-1r + c-1(i — r)]Eμ0ν |X — z I
By taking the infimum of the aforementioned inequality with set of couplings Y(μ, ν), we obtain the
following inequality:
D(ν,μ) = inf ∣J [ν] — J [μ]∣≤ E[c-1Y + c-1(1 — Y)] inf EY ∣X — Z ∣
γ(μ,ν)	γ(μ,ν)
=E[c-1Y + c-1(1 — Y )]Wi(μ,ν)	(33)
≤ E[c-1Y + c-1(1 — Y)]W2(μ, v),
which completes the proof.	口
Proposition 6 follows from the Lipschitzness of the functional J, where D searches for the best
coupling to derive the minimal loss difference between two probability measures. This proposition
indicates that inf ∣J [ν] — J [Fμ]∣ is bounded by the Wasserstein distance, which justifies our ge-
ometric constraint presented in equation 4. It should be noted that the prior assumption regarding
noisy labels is essential for Lipschitzness.
Proposition 7. Let F : R+ × P be afunctional on probability measures such that F [t, μ] = μt,
where dμt = PtdNV, dNV = dqtdx, and let μt be a solution of the continuity equation in the
2-Wasserstein space defined as follows:
∂tμt = V ∙ (μtVΦt),	(34)
which is represented as ∂tp(t, x) = V ∙ (p(t, x)V log q(t,x)) in a distributional sense. Then, the
functional Ft[∙] = F[t, ∙] is defined unique and normalizes μ onto Bw)2 (NV, e-tK2 (μ)), where
K2 (μ) ≤ ∞ is an integral operator in Definition 5 with respect to μ.
16
Under review as a conference paper at ICLR 2021
Proof. We assume that the probability measure μt is absolutely continuous with respect to the detour
Gaussian measure N(m“, ∑ν) = NV, μt《NV. In this case, according to the RadOn-NikOdym
theorem, there is a corresponding unique probability density q(t, x) = qt (x) ∈ C0∞ such that
dμt = qtdNν.
Lemma 2. (WI-inequality, Otto & ViUani (2000)) Ifthe StatiOnary state of μt with respect to Pt
satisfies limt→∞ Eμ[Ptf] = 0 for any f ∈ C∞, then thefollowing inequality holds:
ddt+W2(μ,μt) ≤ PI("tlNν) ∙
(35)
By integrating both sides of the inequality in Lemma 2 with respect tot ∈ (0, ∞), the following
inequality can be obtained:
∞ d	∞ Z_______
W2(μt,NV)=L	应+W2(μt,NV)dt ≤ J0	pI(μtιNν)dt∙
(36)
In the aforementioned inequality, we replace the Fisher information with the diffusion generator L as
follows:
W2(μ, Nv ) ≤
√I (μt∣NV) dt
∞
0
[Ptq]-1Γ(Ptq)dNVdt
L(- logPtq)dμtdt
(37)
The second equality above is derived by leveraging the properties of the bilinear operator Γ (Bakry
et al. (2013); Villani (2008)) with respect to the diffusion operator L, which is defined as follows:
∣,[Ptq]-1Γ(Ptq)dNν = - I L(logPtq)qtdNν = / L(TogPtq)dμt ≥ 0∙	(38)
For simplicity, we denote lg l = g+ for any g ∈ C0∞ . According to Proposition 5, we can relate
Ftμ = μt to its initial term μ = μt=o as follows:
ZO ∞ Z j L(-log Pt q)(X )d[Ftμ](X) dt ≤
ZO ∞ je-2Pt Z L (-log Pt=Oq)(X )dμ(X) dt
≤Z0∞
e-2ρt sup	L+g(Z)qdNV(Z)dt
g∈C0∞
(39)
∞
0
√e-2Ptdt1 sup L L+g(X)dμ(X)
g∈C0∞
PTK2 (μ)∙
The second inequality is naturally induced, because the proposed objective function is defined to
select the maximum elements over the set of functions g ∈ C0∞ and Lg ≤ L+g. If the integral
interval is set to (0, s), then we can induce W2 (μ, Ftμ) ≤ P(1 一 e-s)K2 (μ). Our diffusion-operator
induces P = 1, which completes the proof.	□
Proposition 8. There is a scalar 0 < β < ∞ dependent on ν such that the following inequality
holds:
W2(ν, Ftμ) ≤ [√dβλmaχ(∑ν) + kEνYk2] ∨ [e-tK2(μ)+ K2(ν)] ∙	(40)
As a motivation for setting a detour measure to NV, we mentioned the natural property of the
non-collapsing Wasserstein distance of W2 (ν, NV) 6= 0. However, it is unclear from a geometric
perspective exactly how the upper bound (i.e., W2(ν, NV) ≤ ?) can be induced based on the intrinsic
statistics term (i.e., d1 in Fig.1). Specifically, in the situation where the covariance matrices of ν
and NV are identical, it is difficult to determine a theoretical upper bound without additional tools.
The first part of this proof focuses on resolving this important issue. The second part of the proof
is naturally induced by Proposition 1. Please note that in the following proposition, parameter for
Wasserstein moving average is set to α = 0 for clarity.
17
Under review as a conference paper at ICLR 2021
Proof. Before proceeding with the first part of the proof, we define a constant β as follows:
11	2
β = sup	-EysV2 j(Ys)ds.
1≤j≤d 0 s ,
(41)
If We assume a mild condition such that min§,j inf ι≤j≤d O(vs,j) ≥ O(√s), then the integral term in
β is finite and well-defined. This value will directly yield the upper bound of the Kullback-Leibler
(KL) divergence of ν. First, We introduce the folloWing inequality.
Lemma 3. (de Bruijn’s identity, Johnson & Suhov (2001); Nourdin et al. (2014)) We let Y 〜V,
________________________________________________________________________________________________________ 1
Z 〜N(0, I) denote a standard Gaussian random variable, and let define Ys = λ∕sY + √1 — s∑V2 Z
with the score function defined as Vs(x) = V logPs (x) with respect to the random variable Ys. Then,
the following equality holds:
KL(ν|N(O, ∑ν))
Z 1 Tr
0
2sΣνEp-Ys [vs (K)Vs(Ys)T]) ds.
(42)
From equation 42, we can derive the relations between KL-divergence and the constant β defined
earlier.
/1 *Tr (∑νEχ[vs(Ys)vs(K)T])) ds ≤ /1 ；Tr (∑νEχ[vs,iVs,j%)) ds
11	d
≤	-λmαx(∑ν) ɪ2 E
0 2	j=1
v2,j M)
s
1	1 d	1	(43)
ds ≤ 2 λmαx J β β βds = 2 λmax(∑ν )dd
The second inequality holds based on the following element property of symmetric positive-definite
matrices:
Tr(AB) ≤ kAkop Tr(B) = λmax(A)Tr(B), ∀A,B∈Symd+.	(44)
It should be noted that because the distribution of ν is compactly supported (i.e., supp(q) is
compact), the maximum eigenvalue of the covariance ∑ν is finite. The other relations are induced
by the aforementioned definition. Next, we relate the KL-divergence and 2-Wasserstein distance
naturally.
Definition 9. (Talagrand inequality for Gaussian measures, Otto & Villani (2000)) For any non-
degenerate Gaussian measure N with a mean 0, the following inequality is satisfied:
W2 (ν, N) ≤ P2KL(ν∣N),	∀ν ∈ P2 (Rd).	(45)
By combining Definition 9 and equation 43, we can derive the following expression:
W2(ν,N(0, ∑ν)) ≤ p2KL(ν∣N(0, ∑ν)) ≤ pdβλmaχ(∑ν) < ∞.	(46)
According to the triangle inequality for the 2-Wasserstein distance, we obtain:
W2(ν,N(mν,∑ν)) ≤ W2(ν,N(0,∑ν))+W2(N(mν,∑ν),N(0,∑ν))	(47)
In Appendix C.3, we investigated that the geodesic distance between two Gaussian measures having
the same covariance is equivalent to the Euclidean distance between two means. Therefore, we can
obtain the following equality:
W2(N(mν, ∑ν),N(0, ∑ν)) = W2(∣mν [N(0, ∑ν)],N(0, ∑ν))
= kmν — 0k2 = kEν Y k2 ,
(48)
where ιa(X) = X + a for any vector a ∈ supp(q). Now, by adding the two inequalities defined
earlier, we can obtain
W2(ν,N(m“, ∑ν)) ≤ kEνYk2 + Pdβλmaχ(∑ν),	(49)
where it is easily shown that the upper-bound is only dependent on the statistical structure of V.
Specifically, the term ∣∣E^Y∣∣2 represents the center of mass for a density of V and y∕dβλmaχ(∑^) is
related to the covariance structure of V.
18
Under review as a conference paper at ICLR 2021
By applying Proposition 8 to both Ftμ and V, We can easily recover equation 5 as follows:
W2(ν, Ftμ) ≤ ε = W2(ν, N(mν, Σν))+ W2(N(mν, ∑ν), Ftμ)
≤ (h∣∣EνYk2 + Pd,βλmaχ(∑ν)i ∧ K2(ν)) + e-tK2(μ)
≤ [Pdβλmaχ(∑ν) + kEνYk2i ∨ [e-t&(〃)+ K2(V)].
(50)
The second inequality is easily obtained as (a ∧ b) + c ≤ a ∨ (b + c) for any a, b, c ≥ 0, which
completes the proof.	□
Proposition 9. (Concentration inequality for uncertain measures). Assume that there are some
constants s? ∈ [ 1, ∞), η ≥ 0 such that thefollowing inequality is satisfied:
EFs?μ[f2] - [EFs?μ[f]]2 ≤ (1+ η)EFs?μ[AVfTVf],	(51)
for A ∈ Symd+, D(A, Σν) ≤ aη for some a > 0, and for any metric D defined on Symd+. In this
case, there is a δ such that the following probability inequality for an uncertain measure is induced:
/	、	√2δ 2
Fs?μ( ∣σ — EV[σ]∣ ≥ δ) ≤ 6e-,	(52)
where κ denotes the Lipschitz constant of σ.
Proof. Before proceeding with the main proof, we first prove the existence of s? . The limit of
the interval with respect to η converges to a singleton {∞} as I = limη→o[ 1, ∞). In this case,
equation 51is the same as the POinCare inequality for a Gaussian measure NV, which can be written
as
η→0 EFs?μ[f2] - [EFs?μ[f]]2 ≤ η→0(i + η)EFs?“AVfTVf]
=EFs? μ[∑ν VfT Vf ].
(53)
While the Poincare inequality in equation 53 is uniquely defined, we can find at least one value s?
satisfying equation 51. Let X(t, w) = Xt (w) denote the stochastic process with respect to qt (x)
defined in the proof of Proposition 2. Additionally, let C = EV [σ] 一 Efs? μ [σ]. Then, we can obtain
the following inequality:
C = EV [σ]	一	EFs? μ[σ]	= K	(EVh-]	-	EFs? μ	[ —i) ≤ K	SUP	(EVg - EFs? μg)
s	κ	s κ	g∈Lip1	s
KK ( )	1	(54)
≤ KWI(Fs?μ,ν) ≤ κW2(Fs?μ,ν) ≤ 汽 2 ".
1+η
The first inequality is induced by the assumption regarding the K-Lipschitzness of the function σ
and the second inequality is induced by the Kantorovich-Rubinstein theorem. The third inequality is
natural because Wa(∙, ∙) ≤ Wb(∙, ∙) for any 1 ≤ a ≤ b < ∞. because equation 51is equivalent to the
Poincare inequality for the measure Fs?μ, it satisfies the Bakry-emery curvature-dimension condition
CD(1 + η, ∞). Thus, as shown in the proof of Proposition 2 (i.e., equation 39), the last inequality is
induced. Additionally, based on the concentration inequality of Fs?μ [Proposition 4.4.2 Bakry et al.
(2013)], we can derive the following probability inequality:
δ
Fs?μ [σ(Xs? (W)) ≥ Efs?μ[σ] + δ] ≤ 3e √1+ηκ ,	(55)
where the Poincare constant for Fs?μ is naturally 1 + η and ||。||二分？ = κ. Next, we will derive the
desired form from equation 55. First, we introduce the following inequality.
σ(Xs? ) ≥ EFs?μ[σ] + δ ≥ EV [σ] + δ 一 1+ η K2	(56)
The last inequality is directly induced by equation 54 because -C ≥ 一 1++n K2. While η, κ, and K2
are constants with respect to w, the following set inclusion can be obtained naturally:
S1 = {w : σ(Xs?(W)) ≥ Efs?μ[σ] + δ}⊇{w : σ(Xs?(W)) ≥ EV[σ] + δ - γ-+^K2} = S2.
(57)
19
Under review as a conference paper at ICLR 2021
For the modified version of the original probability inequality, We take probability measure Fs?μ[∙]
for the sets S1, S2, which is defined as
_ δ
3e √1+ηκ ≥ Fs?μ ({w : σ(Xs*(w)) ≥ Efs?μ[σ] + δ})
≥ Fs?μ ({w ： σ(Xs*(w)) ≥ EV[σ] + δ - 1+^K2}).
(58)
The concentration inequality around Eν [σ] is obtained by combining the inequalities induced by σ
and -σ as folloWs:
1	Fs? μ	[	{w ： h(Xs*(w))- Eν[h] ≥± (δ - +&}
2	1+η
h∈{σ,-σ}	+η
(59)
Fs?μ ({w ： ∣σ(Xs? (W)) - EV[σ]∣ ≥ δ - 1 +『2}) ≤ 6e- √1+ηκ
The inequality in equation 59 is the general form containing the relation betWeen the upper bound of
the probability and (η,κ, K2). While this form is quite complicated and highly technical, We choose
not to present all the detailed expressions of equation 59 in the main paper. Rather than that, We
re-write it in a much simplified form for clarity. Specifically, by setting κK2∕(l + η) = 0.5δ and
rescaling δ to 2δ, the aforementioned inequality in equation 59 can be converted into the folloWing
simpler form:
√2δ 2
Fs?μ ({w : ∣σ(Xs? (W)) - EV[l]| ≥ δ) ≤ 6e-^Kr.
(60)
□
Finally, if we set σ = Softmax, then the Lipschitz constant is induced as κ= 1. This proof is
completed by setting s? ：= T .
20