Under review as a conference paper at ICLR 2021
Toward Understanding Supervised Represen-
tation Learning with RKHS and GAN
Anonymous authors
Paper under double-blind review
Ab stract
The success of deep supervised learning depends on its automatic data represen-
tation abilities. A good representation of high-dimensional complex data should
enjoy low-dimensionally and disentanglement while losing as little information as
possible. This work gives a statistical understanding of how a deep representation
goal can be achieved via reproducing kernel Hilbert spaces (RKHS) and genera-
tive adversarial networks (GAN). At the population level, we formulate the ideal
representation learning task as that of finding a nonlinear map that minimizes the
sum of losses characterizing conditional independence (via RKHS) and disentan-
glement (via GAN). We estimate the target map at the sample level with deep
neural networks. We prove the consistency in terms of the population objective
function value. We validate the proposed methods via comprehensive numerical
experiments and real data analysis in the context of regression and classification.
The resulting prediction accuracies are better than state-of-the-art methods.
1	Introduction
Over the past decade, deep learning has achieved remarkable successes in many fields such as com-
puter vision and natural language processing (Krizhevsky et al., 2012; Graves et al., 2013; LeCun
et al., 2015). A key factor for the success of deep learning is its automatic data representation ca-
pabilities (Bengio et al., 2013). For all desired characteristics of an ideal representation for a high-
dimensional complex data, information preservation, low dimensionality and disentanglement are
among the topmost (Achille & Soatto, 2018). A representation with these characteristics not only
makes the model more interpretable but also facilitates the subsequent supervised learning tasks.
First, information preservation requires that the learned features should be sufficient statistics for
both estimation and prediction. This can be quantified via the concept of conditional independence.
Second, low dimensionality means that we should use as few features as possible to represent the
underlying structure of data, and the number of features should be fewer than the ambient dimen-
sion. Third, the learned features in the representation can often be interpreted as corresponding
to the hidden causes of the observed data; thus disentanglement is an essential characteristic that
distinguishes cause from others (Goodfellow et al., 2016).
It is widely believed that deep neural networks trained in supervised learning learn effective data
representation automatically. For example, in the context of classification, the last layer of a deep
neural network is a linear classifier and the preceding layers serve as a feature extractor to the
classifier. However, in the supervised training of deep neural networks, the objective functions do not
explicitly impose any conditions that guarantee the desired characteristics of learned representations.
It appears that there are not any explicit guiding principles for understanding the black-box nature
of deep neural networks for representation learning (Alain & Bengio, 2017).
In this paper, we propose a novel supervised representation learning approach (NSRL) to achieve the
ideal representation, which in turn demystifies the success of deep neural networks. The key idea of
NSRL is that we seek a nonlinear map from the high-dimensional input space to a lower-dimensional
feature space such that the data and its label/response are conditionally independent given the value
of the nonlinear map. Meanwhile, we regularize the nonlinear map by matching its pushforward
distribution to a reference distribution with independent components such as standard Gaussian to
achieve disentanglement in the representation. Therefore, the proposed NSRL is guaranteed to enjoy
the three desired characteristics of an ideal data representation described above. For measuring the
1
Under review as a conference paper at ICLR 2021
conditional independence, we use the conditional covariance operator in reproducing kernel Hilbert
space (RKHS) (Baker, 1973; Fukumizu et al., 2004) that can be estimated easily with samples.
To further match the pushforward distribution under the target nonlinear map with the reference
distribution, we use the GAN loss such as the the f -divergence for f -GAN (Nowozin et al., 2016)
or the 1-Wasserstein distance for the WGAN (Arjovsky et al., 2017). Our main contributions are as
follows:
•	At the population level, we formulate the ideal representation learning task equivalently as
finding a nonlinear map that minimizes the loss characterizing both information preserva-
tion and disentanglement via conditional covariance operator in RKHS and GAN, respec-
tively.
•	We estimate the target nonlinear map at the sample level nonparametrically with deep neu-
ral networks, and propose a novel supervised representation learning (NSRL) algorithm.
•	We validate the proposed NSRL via comprehensive numerical simulations and a number of
real datasets, i.e. Life Expectancy and Pole for regression, and MNIST, Kuzushiji-MNIST
and CIFAR-10 for classification. We use the learned features from our NSRL as inputs for
linear regression and nearest neighbor classification. The resulting prediction accuracies
are better than those state-of-the-art methods, that is, linear dimension reduction models
for regression and deep learning models for classification.
2	Setup and background
2.1	Setup
Suppose we have a sample of n input-response/label observations {(Xi, Yi) ⊆ X × Y ⊆
Rd X R1}n=1 that are i.i.d. copies of (X, Y) with an unknown law μχ,γ. In many applications,
high-dimensional complex data X such as images, texts and natural languages, tend to have low-
dimensional representations (Bengio et al., 2013). Mathematically, we model this feature of high-
dimensional complex data by assuming the existence of a nonlinear map g* : Rd → Rd* with
d*《d such that the information of X can be completely encoded by g* in the sense
XUY |g* (X).	(1)
That is, Y and X are conditionally independent given g* (X). The representation g* (X) has much
lower dimensionality than X and captures all the information about the statistical dependence of Y
on X. Moreover, we would like to have the disentanglement property for the representation g* (X),
that is, the potential hidden causes of the observed data. This can be achieved by transforming the
distribution ofg*(X) into standard Gaussian. To this end, we first recall a result in optimal transport
theory (Brenier, 1991; McCann et al., 1995; Villani, 2008).
Lemma 2.1 Let μ be a probability measures on Rd* with Second order moment and absolutely
continuous with respect to the the Gaussian measure γd* . Then it admits a unique optimal trans-
portation map T* : Rd → Rd such that T*#M = γd* = N(0,Id*). Moreover, T* is injective
μ-a.e.
Thanks to Lemma 2.1, the map T* in Lemma 2.1 transforms the distribution of g* (X) satisfying
equation 1 to the standard normal distribution. Specifically, define
F* = T* ◦ g* : Rd → Rd* .
Then we also have
X 卫 Y|F*(X), F*(X)〜γd* = N(0, Id*),	(2)
that is, F* is a target nonlinear map that preserves both disentanglement and conditional indepen-
dence.
Next we recall some background on conditional covariance operator in RKHS that is used to char-
acterize conditional independence, and GAN losses that are used to measure the discrepancy of two
probability measures.
2
Under review as a conference paper at ICLR 2021
2.2	Conditional covariance operators
Let (HX , kX ) and (HY, kY) be RKHS of functions on X and Y, respectively, with measurable
positive definite kernels kX and kY. The cross-covariance operator of (X, Y ) from HX to HY can
be defined so that
hh, ΣYXgiHY =EXY [(g(X) - EX[g(X)]) (h(Y) - EY [h(Y)])]	(3)
holds for all g ∈ HX and h ∈ HY. Conditional covariance operator ΣYY |X can be defined as
(Baker, 1973; Fukumizu et al., 2004; 2009)
ςYYX
ΣYY - ΣYX Σ-X1X Σ
XY.
(4)
Let F : Rd → Rd* be a map and Z ⊆ Rd* and (HZ, kz) be a RKHS on Z with kernel kz. Define
kχ on X as kχ(x, x) = kz(F(x), F(x)). We denote the RKHS that is related to kχ as HX. We
define a new conditional covariance operator that is related to F as
ςYYX = ςYY - ςYX (ςXX) ςXY,	⑸
where ΣYFX (ΣFXX) is the cross-covariance operator of (X, Y ) defined in (3) with kX being replaced
by kXF . Under some mild regularity conditions (Fukumizu et al., 2004; 2009), we have
ςYy|X ≥ ςyy|X,	⑹
where the inequality refers to the order of self-adjoint operators. Moreover,
∑YY|X = ∑Yy∣x	O YɪX | F(X).	⑺
Define
Fi = arg min∑YY ∣x ,	(8)
where the minimization refers to the minimal operators in the partial order of self-adjoint operators.
Then, It follows from (6)-(7) that the target representation map F * in (2) is the minimizer of (8),
i.e., F * ∈ Fi. The loss in minimization problems (8) can be consistently estimated with samples
{(Xi, Yi) ⊆ X × Y}in=1 (Fukumizu et al., 2009) as follows
Tr hGY (GX + nεnIn)	i ,	⑼
where εn is a regularization parameter, and GFX ∈ Rn×n with the (i, j)th entry is defined as
(GFX)i,j = kz (Zi, Zj ) -
1n
—TkZ (Zi,Zb)-
n
b=1
n	nn
1 x kz (Za, Zj ) + i XX kz (Za , Zb ) ,
a=1	a=1 b=1
where Zi = F(Xi), and GY ∈ Rn×n can be computed similarly with Zi replaced by Yi.
2.3	f-GAN LOSS
Denote X 〜μX. Let Z = F (X) and μz be its law. The f -divergence (Ali & Silvey, 1966) between
μz and γd* with μz《γd* is defined as
Df(MZl∣Yd*) = Z	f (半Z )dYd*,	(IO)
Rd*	dγd*
where f : R+ → R is a twice-differentiable convex function satisfying f(1) = 0. The KL di-
vergence and JS divergence correspond to f (t) = t log t and f (t) = -(t + 1) log 1++t + t log t,
respectively. By Jensen,s inequality, Df(μz∣∣γd*) = 0 implies μz = γd* almost everywhere. De-
note F as the Fenchel conjugate of f (Rockafellar, 1970). Then the f -divergence can be recast as
the following f -GAN loss (Nowozin et al., 2016).
Lemma 2.2
Df(μz∣Yd*) =	max	EZ〜μz[D(Z)] - EW〜γ~* [F(D(W))],	(11)
D:Rd* →dom(F)	Z	d
where the maximum is attained when D(X) = f(dμZ (x)).
3
Under review as a conference paper at ICLR 2021
Let
尸2 = arg minDf (〃f(X)∣∣γd*) = min maxEX〜μχ [D(F(X))] - EW〜γd* [F(D(W))]	(12)
F	FD
By Lemma 2.2, We have F * ∈ F2 .Ifwe have Zi = F (Xi), Wi i.i.d drawn from μz = μF(X)and
Yd, respectively. We can estimate the f -GAN loss (11) as
1n
Df(μF(X) ∣∣Yd*) = max — ]T[D(F(Xi))- F(D(Wi))].	(13)
Dn
i=1
Other GAN loss such as the 1-Wasserstein distance for the WGAN (Arjovsky et al., 2017) can also
be used here.
3 NSRL Algorithm
From the above section, we have the target representation map F* in (2) satisfying F* ∈ F1 ∩ F2 .
And the loss for F1 and F2 can be estimated via (9) and (13), respectively. Thus, we can estimate
F * with a neural network Fθ (denoted as a reducer) that minimizes the following criterion
Tr GY GFXθ +nεnIn-1
1n
+ λ max-]T[Dφ (Fθ (Xi)) — F(Dφ(Wi))],	(14)
Dφ n
i=1
where λ > 0 is a tuning parameter, and Dφ is another neural network (denoted as a discriminator)
to estimate the optimal D in (13). Then, we train Fθ according to the loss in equation 14 via in two
steps iteratively as follows:
(i)	Update the discriminator Dφ: Fix θ and calculate the loss for φ in (14) and ascending this
loss by SGD on φ.
(ii)	Update the reducer Fθ: Compute the loss for θ in (14) with the updated φ in (i) and descend
this loss by SGD on θ.
To visualize the framework, we depict it as a flowchart in Figure 1 and give a detailed algorithm
below with the “Log-D” trick GAN (Goodfellow et al., 2014) as an example.
Figure 1: Flowchart for NSRL
• Novel Supervise Representation Learning (NSRL) Algorithm
• Input {Xi, Yi}in=1 . Tuning parameters: m, λ, ε, d* .
• Outer loop for θ
4
Under review as a conference paper at ICLR 2021
-Inner loop for φ
*	Sample {Wi}n=1 〜Yd*
*	Update φ with stochastic gradient with batch size m
Vφ{Pm=1 m1 (IOg (Dφ (Wi)) + Iog(I- Dφ (Fθ (Wi))))}
- End inner loop
- Update θ with stochastic gradient with batch size m
Vθ{Tr GY (GX + mεUm)T - λ Pm=,表 log(Dφ (Fθ (Wi)))}
• End outer loop
4	Related works
Supervised dimension reduction: The seminal paper of Li (1991) proposed sufficient dimension
reduction via sliced inverse regression, where the aim is to find a minimum subspace (Cook, 1998)
such that the orthogonal projection of the data on to which preserves the dependency of the response
and the predictors. There is an extensive literature on sufficient dimension reduction via a linear map
(Li, 1992; Cook, 1998; Li et al., 2005). Alternative approaches have been developed to estimate the
central space (or its subspace) based on nonparametric estimation of conditional independence (Xia
et al., 2002; Fukumizu et al., 2004; 2009; Suzuki & Sugiyama, 2013; Vepakomma et al., 2018).
See also the review paper (Cook et al., 2007) and monograph (Li, 2018) and the references therein.
The methods mentioned above focus on linear dimension reduction (LDR). However, LDR may
not be effective for high-dimensional complex data such as images and natural languages since the
relationship between the raw data and the underlying features can be highly nonlinear.
Representation learning: Tishby & Zaslavsky (2015); Shwartz-Ziv & Tishby (2017); Saxe et al.
(2019) proposed to study the internal mechanism of supervised deep learning from the perspective
of information theory, where they showed that training a deep neural network that optimizes the
information bottleneck (Tishby & Pereira) is a trade-off between the representation and the pre-
diction at each layer. To make the information bottleneck idea more practical, a deep variational
approximation of information bottleneck (VIB) is considered in Alemi et al. (2017). Numerical ex-
periments suggest that the learned representations obtained via VIB are favored by the subsequent
supervised learning task and robust to adversarial inputs. Information-theoretic objectives describ-
ing conditional independence such as mutual information are utilized as loss functions to train a
representation-learning function, i.e., an encoder in the unsupervised setting (Hjelm et al., 2019;
Oord et al., 2018; Tschannen et al., 2020; Locatello et al., 2020; Srinivas et al., 2020). Unsupervised
models such as VAEs (Kingma & Welling, 2014) and its variants (Kim & Mnih, 2018; Higgins et al.,
2017; Tolstikhin et al., 2018; Makhzani et al., 2017) also learn a representation via its encoder as a
by-product.
5	Experimental results
We evaluate our proposed NSRL using simulated data and real benchmark data in the setting of
regression and classification. Some details on the network structures and hyperparameters used on
our experiments are included in the appendix. Our experiments were conducted on Nvidia DGX
Station workstation using a single Tesla V100 GPU unit. The PyTorch code of NSRL is available at
https://github.com/anonymous/NSRL.
5.1	Toy examples and visualization
Visualization. We visualize the learned manifold of NSRL on two simulated data. We first generate
(1) 5,000 data points from 3-dimensional S curve dataset on regression setting as shown in Figure
2 (a); (2) 5,000 data points for each class from 3-dimensional mixed Uniform and Gaussian data on
classification setting as shown in Figure 2 (d). We next map these data points into the ones from
400-dimensional space by multiplying matrices with entries i.i.d Unifrom([0, 1]). Finally, these
400-dimensional datasets with their responses are trained by NSRL to learn 2-dimensional features.
In detail, a 20-layer dense convolutional network (DenseNet) (Huang et al., 2017; 2019) as Fθ and a
4-layer network with Leaky ReLU activation as Dφ are employed. We set the reference distribution
5
Under review as a conference paper at ICLR 2021
γd* uniformly distributed on the surface of the unit sphere to disentangle the representation. Scatter
plots of the evolving learned 2-dimensional features are shown in Figure 2.
(a) S curve
(c) Epoch = 200
(d) Mixed 3D
-2	-1	0
Feature-one
(e) Epoch = 10
Figure 2: Scatter plots of the evolving learned representation.
Simulation on regression. We generate 5000 data points from the following simulated models:
Model A : Y = e2(X1 +X2) log(X2) + 6;
Model B : Y =(X2 + X2 + X∣)2 log(X1 + X2 + X∣)2 + 6;
Model C : Y = sin
∏(χι+x2+x3 )
eɪX,6 ~N(0, 0.25 ∙ I10)
10
+
For the distribution of the 10-dimensional predictor X, we consider three following scenarios:
•	Scenario I: X - N (0, I10), independent GaUSSian predictors;
•	Scenario II: X 〜3N (-110, I10) + 3Unifrom([-1,1]10) + 1N (110, I10), independent
non-Gaussian predictors;
•	Scenario III: X ~ N(0, 0.4 ∙ I10 + 0.6 ∙ 1101>)), correlated Gaussian predictors.
where, the notation of Scenario II is the mixture distribution ofN (-110, I10), Unifrom([-1, 1]10)
and N (110, I10) with mixing probabilities (3,1, 3). These models and the distributional scenarios
are modified from Lee et al. (2013); Li (2018).
We adopt a 4-layer network for Fθ and a 3-layer network for Dφ with Leaky ReLU activation.
We compare NSRL with two linear dimension reduction methods: sliced inverse regression (SIR)
(Li, 1991), sliced average variance estimation (SAVE) (Shao et al., 2007); two nonlinear dimension
reduction methods: generalized sliced inverse regression (GSIR) (Li, 2018) and generalized sliced
average variance estimation (GSAVE) (Li, 2018); and linear regression with original data (LR).
Finally, we fit a linear regression model between the learned features and the response. As shown
in Table 1, we report the prediction error and conditional Hilbert-Schmidt independence criterion
(cHSIC) (Fukumizu et al., 2008) that measures conditional dependence between the learned features
and the response variable. We can see that NSRL outperforms these traditional linear and nonlinear
sufficient dimension reduction methods in terms of the prediction error and cHSIC. Thus, NSRL not
only excels in prediction but also can obtain central subspaces more accurately.
6
Under review as a conference paper at ICLR 2021
Table 1: Averaged prediction errors (RMSE), conditional Hilbert-Schmidt independence criterion
(cHSIC) and their standard errors (based on 5-fold validation).
Model A				Model B		Model C	
Scenario	Method	RMSE	CHSIC	RMSE	CHSIC	RMSE	CHSIC
	NSRL	3.10 ±.2	60.80 ± 3.7	0.68 ± .1	111.26 ± 24.2	0.44 ±.02	367.89 ± 38.5
	SIR (Li, 1991)	3.13 ±.2	71.85 ± 3.4	1.05 ±.0	208.56 ± 3.6	0.46 ± .02	557.54 ± 2.8
I	SAVE (Shao et al., 2007)	3.13 ±.2	70.90 ± 3.7	1.05 ±.0	177.19 ± 4.5	0.46 ± .02	542.11 ±5.2
	GSIR (Li, 2018)	3.11 ±.2	73.19 ± 3.3	0.89 ± .0	191.56 ± 4.8	0.46 ± .02	575.18 ± 1.5
	GSAVE (Li, 2018)	3.17 ±.2	184.94 ± 13.5	0.90 ± .0	221.89 ± 5.2	0.49 ± .01	608.33 ± 6.4
	LR	3.13 ±.2	190.16 ± .8	1.05 ±.0	192.64 ± 1.4	0.46 ± .02	193.91 ± 1.2
	NSRL	3.91 ± .6	57.41 ± 9.8	0.50 ± .1	93.96 ± 16.2	0.53 ± .02	280.91 ± 54.7
	SIR (Li, 1991)	4.39 ± .3	73.47 ± 3.9	1.62 ±.0	283.72 ± 12.3	0.54 ±.02	483.13 ± 2.4
II	SAVE (Shao et al., 2007)	4.45 ± .3	80.32 ± 6.6	1.62 ±.0	225.54 ± 9.5	0.55 ± .02	464.74 ± 9.7
	GSIR (Li, 2018)	4.36 ± .3	73.9 ± 5.4	1.10 ±.1	99.26 ± .1	0.55 ± .02	419.96 ± 9.4
	GSAVE (Li, 2018)	4.40 ± .3	148.7 ± 9.1	1.11 ±.0	96.58 ± 4.7	0.56 ± .02	454.99 ± 7.9
	LR	4.39 ± .3	161.31 ± 2.6	1.62 ±.0	173.08 ± 1.9	0.54 ±.02	283.84 ± 2.9
	NSRL	2.96 ± .7	62.12 ± 10.1	0.54 ±.2	108.85 ± 27.6	0.44 ±.02	245.85 ± 56.3
	SIR (Li, 1991)	3.87 ± .5	110.4 ± 8.9	1.34 ±.1	407.10 ± 17.6	0.47 ± .02	443.50 ± 12.2
III	SAVE (Shao et al., 2007)	3.87 ± .5	121.04 ± 8.1	1.34 ±.1	221.42 ± 15.5	0.47 ± .02	397.25 ± 10.3
	GSIR (Li, 2018)	3.48 ± .5	68.81 ± 2.6	0.69 ± .0	109.04 ± 6.4	0.45 ± .01	352.96 ± 9.0
	GSAVE (Li, 2018)	3.44 ±.5	73.13 ± 3.3	0.65 ± .0	107.73 ± 5.9	0.46 ± .01	352.28 ± 12.6
	LR	3.88 ± .5	191.57 ± 5.2	1.34 ±.1	207.79 ± .9	0.47 ± .02	267.06 ± 1.2
5.2 Performances on real-world settings
Regression. We consider two datasets: Life Expectancy (https://www.kaggle.com/
kumarajarshi/life-expectancy-who) collected from World Health Organization (WHO)
and Pole (Weiss & Indurkhya, 1995) collected from a large telecommunications application. Life
Expectancy dataset has 2938 observations and 20 covariates to predict life expectancy in age. Pole
dataset consists of 15000 observations with 48 predictors. A 2-layer network and a 3-layer network
with Leaky ReLU activation are used for Dφ and Fθ in both datasets, respectively. To obtain the
performance of predictions, a linear regression is adopted to fit the learned representation against
the response variable. As shown in Figure 3, experimental results are reported in terms of root mean
square error (RMSE) and distance correlation (DC), as measured using 5-fold cross-validation. We
compare NSRL with sliced inverse regression (SIR) (Li, 1991), sliced average variance estimation
(SAVE) (Shao et al., 2007), principal component analysis (PCA), sparse principal component anal-
ysis (SPCA) and linear regression with original data (LR). As a result, NSRL outperforms not only
unsupervised representation methods - PCA and SPCA, but also supervised methods based on suf-
ficient dimension reduction - SIR and SAVE.
Dimension	Dimension
(a) DC for Life Expectancy
(b) DC for Pole
Figure 3: Prediction errors and distance correlation between the representation and the response
variable based on 5-fold cross-validation.
7
Under review as a conference paper at ICLR 2021
Classification. We compare NSRL with a feature extractor based on cross entropy loss (CN) on
MINST (LeCun & Cortes, 2010) and Kuzushiji-MNIST (Clanuwat et al., 2018) for handwritten
digits and Japanese letter recognition. MINST and Kuzushiji-MNIST both contain 60k 28 × 28
grayscale images from 10 classes for training and testing, respectively. To demonstrate that NSRL
is compatible with various GAN frameworks, we utilize the vanilla GAN (Goodfellow et al., 2014)
based on log D trick (Heusel et al., 2017) and Wasserstein GAN (WGAN) (Arjovsky et al., 2017)
on our experiments. We employ the VGG-5 with Spinal FC architecture (Kabir et al., 2020) for Fθ
and 4-layer networks for Dφ.
We apply the transfer learning technique to the combination of NSRL and CN compared with CN on
STL-10 (Coates et al., 2011) and CIFAR-10 (Krizhevsky et al., 2009). The STL-10 dataset consists
of 5k and 8k 96 × 96 color images from 10 classes for training and testing, respectively. CIFAR-10
contains 50k and 10k color images with 32 × 32 pixels for training and testing, respectively. The
pretrained WideResnet-101 model (Zagoruyko & Komodakis, 2016) on the Imagenet dataset with
Spinal FC (Kabir et al., 2020) is adopted for Fθ . The discriminator Dφ is a 4-layer network with
Leaky ReLU activation.
Finally, k-nearest neighbor (k = 5) classifier on the learned features is used to obtain classification
accuracy for all methods. Classification accuracies for MNIST and Kuzushiji-MNIST are reported in
Table 2, and those for STL-10 and CIFAR-10 are reported in Table 3. As Table 2 shows, NSRL with
different GAN frameworks are stable and comparable with CN in terms of classification accuracy.
For both STL-10 and CIFAR-10 that use transfer learning, CN leveraging NSRL is comparable to
CN on STL-10 and outperforms CN on CIFAR-10.
Table 2: Classification accuracy on MINST and Kuzushiji-MNIST.
Datasets		MNIST		Kuzushiji-MNIST		
Model	d=8	d=16	d = 32	d=8	d=16	d = 32
CN	99.62	99.70	99.64	98.60	98.80	98.84
NSRL (log D)	99.67	99.66	99.62	98.61	98.81	98.63
NSRL (WGAN)	99.67	99.70	99.66	98.68	98.66	98.72
Table 3: Classification accuracy on STL-10 and CIFAR-10.
Datasets		STL-10			CIFAR-10	
Model	d = 32	d= 64	d = 128	d= 32	d= 64	d= 128
CN	98.17	98.36	98.45	97.79	97.78	97.74
NSRL+CN	98.34	98.24	98.36	97.99	97.82	97.75
6 Conclusion
In this work, we propose a novel approach to achieving a good data representation for supervised
learning with certain desired characteristics including information preservation, low-dimensionality
and disentanglement. We formulate the ideal representation learning task as that of finding a non-
linear map that minimizes the sum of losses characterizing conditional independence and disentan-
glement via conditional covariance operator in RKHS and GAN. The proposed method is validated
via comprehensive numerical experiments and real data analysis in the context of regression and
classification. For the future work, it would be interesting to consider other measures of conditional
independence and generalize the proposed method to semi-supervised learning problems.
References
Alessandro Achille and Stefano Soatto. Emergence of invariance and disentanglement in deep rep-
resentations. The Journal ofMachine Learning Research, 19(1):1947-1980, 2018.
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier
probes. In ICLR Workshop, 2017.
8
Under review as a conference paper at ICLR 2021
Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information
bottleneck. In ICLR, 2017.
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distri-
bution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):
131-142,1966.
Brandon Amos and J. Zico Kolter. A PyTorch Implementation of DenseNet. https://github.
com/bamos/densenet.pytorch. Accessed: [20 Feb 2020].
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Charles R Baker. Joint measures and cross-covariance operators. Transactions of the American
Mathematical Society, 186:273-289, 1973.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798-1828,
2013.
Yann Brenier. Polar factorization and monotone rearrangement of vector-valued functions. Commu-
nications on pure and applied mathematics, 44(4):375-417, 1991.
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, and David
Ha. Deep learning for classical japanese literature, 2018.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelli-
gence and statistics, pp. 215-223, 2011.
R Dennis Cook. Regression graphics: ideas for studying regressions through graphics, volume 482.
1998.
R Dennis Cook et al. Fisher lecture: Dimension reduction in regression. Statistical Science, 22(1):
1-26, 2007.
Kenji Fukumizu, Francis R Bach, and Michael I Jordan. Dimensionality reduction for supervised
learning with reproducing kernel hilbert spaces. Journal of Machine Learning Research, 5:73-99,
2004.
Kenji FUkUmizu, Arthur Gretton, Xiaohai Sun, and Bernhard SchOlkopf. Kernel measures of Condi-
tional dependence. In Advances in neural information processing systems, pp. 489-496, 2008.
Kenji Fukumizu, Francis R Bach, Michael I Jordan, et al. Kernel dimension reduction in regression.
The Annals of Statistics, 37(4):1871-1905, 2009.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. 2016.
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recur-
rent neural networks. In 2013 IEEE international conference on acoustics, speech and signal
processing, pp. 6645-6649, 2013.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626-6637, 2017.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In ICLR, 2017.
9
Under review as a conference paper at ICLR 2021
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In ICLR, 2019.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In CVPR, 2017.
Gao Huang, Zhuang Liu, Geoff Pleiss, Laurens Van Der Maaten, and Kilian Weinberger. Convo-
lutional networks with dense connectivity. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2019.
HM Kabir, Moloud Abdar, Seyed Mohammad Jafar Jalali, Abbas Khosravi, Amir F Atiya, Saeid
Nahavandi, and Dipti Srinivasan. Spinalnet: Deep neural network with gradual input. arXiv
preprint arXiv:2007.03347, 2020.
Amor Keziou. Dual representation of 夕-divergences and applications. Comptes rendus
mathematique, 336(10):857-862, 2003.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In ICML, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Technical report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In NIPS, 2012.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Kuang-Yao Lee, Bing Li, and Francesca Cuiaromonte. A general theory for non-linear sufficient
dimension reduction: formulation and estimation. The Annals of Statistics, 41(1):221-249, 2013.
Bing Li. Sufficient dimension reduction: Methods and applications with R. 2018.
Bing Li, Hongyuan Zha, Francesca Chiaromonte, et al. Contour regression: a general approach to
dimension reduction. The Annals of Statistics, 33(4):1580-1616, 2005.
Ker-Chau Li. Sliced inverse regression for dimension reduction. Journal of the American Statistical
Association, 86(414):316-327, 1991.
Ker-Chau Li. On principal hessian directions for data visualization and dimension reduction: An-
other application of stein’s lemma. Journal of the American Statistical Association, 87(420):
1025-1039, 1992.
Francesco Locatello, Michael Tschannen, Stefan Bauer, Gunnar Ratsch, Bernhard Scholkopf, and
Olivier Bachem. Disentangling factors of variation using few labels. In ICLR, 2020.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, and Ian Goodfellow. Adversarial autoencoders.
In ICLR, 2017.
Robert J McCann et al. Existence and uniqueness of monotone measure-preserving maps. Duke
Mathematical Journal, 80(2):309-324, 1995.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In NIPS, 2016.
10
Under review as a conference paper at ICLR 2021
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-
tive coding. arXiv preprint arXiv:1807.03748, 2018.
Guido Philippis. Regularity of optimal transport maps and applications, volume 17. Springer
Science & Business Media, 2013.
R Tyrrell Rockafellar. Convex analysis. Number 28. Princeton university press, 1970.
Andrew M Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Brendan D
Tracey, and David D Cox. On the information bottleneck theory of deep learning. Journal of
Statistical Mechanics: Theory and Experiment, 2019(12):124020, 2019.
Yongwu Shao, R Dennis Cook, and Sanford Weisberg. Marginal tests with sliced average variance
estimation. Biometrika, 94(2):285-296, 2007.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the black box of deep neural networks via informa-
tion. arXiv preprint arXiv:1703.00810, 2017.
Aravind Srinivas, Michael Laskin, and Pieter Abbeel. Curl: Contrastive unsupervised representa-
tions for reinforcement learning. arXiv preprint arXiv:2004.04136, 2020.
T Suzuki and M Sugiyama. Sufficient dimension reduction via squared-loss mutual information
estimation. Neural computation, 25(3):725-758, 2013.
N Tishby and F Pereira. The information bottleneck method. In Proceedings of the 37-th Annual
Allerton Conference on Communication, Control and Computing, pp. 368-377.
Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In
2015 IEEE Information Theory Workshop, 2015.
I Tolstikhin, O Bousquet, S Gelly, and B SchOlkopf. Wasserstein auto-encoders. In ICLR, 2018.
Michael Tschannen, Josip Djolonga, Paul K Rubenstein, Sylvain Gelly, and Mario Lucic. On mutual
information maximization for representation learning. In ICLR, 2020.
Praneeth Vepakomma, Chetan Tonde, Ahmed Elgammal, et al. Supervised dimensionality reduction
via distance correlation maximization. Electronic Journal of Statistics, 12(1):960-984, 2018.
Cedric Villani. Optimal transport: old and new, volume 338. 2008.
Sholom M Weiss and Nitin Indurkhya. Rule-based machine learning methods for functional predic-
tion. Journal of Artificial Intelligence Research, 3:383-403, 1995.
Yingcun Xia, Howell Tong, WK Li, and Li-Xing Zhu. An adaptive estimation of dimension reduc-
tion space. Journal of the Royal Statistical Society Series B, 64(3):363-410, 2002.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
A Appendix: Experimental details
In this section, we give the details of the experimental implementations, including hyper-parameters,
network architectures, leaning optimizers, and so on. The values of the hyper-parameters are pre-
sented in Table A1, where d is the dimension of the learned features, m is the mini-batch size during
training, T1 is the number of inner loops for training Dφ to learn the target distribution, T2 is the
number of outer loops for training Fθ .
A. 1 S imulation studies
The detailed architectures of dense convolutional network (DenseNet) (Huang et al., 2017; Amos &
Kolter) deployed for Fθ on S curve and Mixed 3D are shown in Table A2. A multilayer perceptron
(MLP) is adopted for Dφ as shown in A3. As shown in Table A4, MLP is used for the neural network
structures of Dφ and Fθ in the regression setting. For all settings, the Adam (Kingma & Ba, 2014)
optimizer is utilized with an initial learning rate of 0.001 and weight decay of 0.0001.
11
Under review as a conference paper at ICLR 2021
Table A1: Hyper-parameters for all experiments.
Dataset	d	λ	m	T1	T2
S curve	2	103	64	1	200
Mixed 3D	2	104	64	1	200
Simulated regression	2, 3	10-3	256	1	300
Life Expectancy	4, 6, 8, 12	10-3	64	1	200
Pole	5, 10, 20, 30	10-3	64	1	200
MNIST	8, 16, 32	10-4	128	1	200
Kuzushiji-MNIST	8, 16, 32	10-4	128	1	200
STL-10	32, 64, 128	10-4	128	1	50
CIFAR-10	32, 64, 128	10-4	128	1	50
Table A2: 20-layer DenseNet architecture for Fθ for visualization experiments.
Layers	Details	Output size
Convolution	3 × 3 Conv	24 × 20 × 20
	BN, 1 × 1 Conv	
Dense Block 1	BN, 3 × 3 Conv	× 2	48 × 20 × 20
Transition Layer 1 BN, ReLU, 2 × 2 Average Pool,1 × 1 Conv 24 × 10 × 10		
	BN, 1 × 1 Conv	
Dense Block 2	BN, 3 × 3 Conv	× 2	48 × 10 × 10
Transition Layer 2 BN, ReLU, 2 × 2 Average Pool, 1 × 1 Conv 24 × 5 × 5		
	BN, 1 × 1 Conv	
Dense Block 3		× 2	48 × 5 × 5
	BN, 3 × 3 Conv	
Pooling	BN, ReLU, 5 × 5 Average Pool, Reshape	48		
Fully connected	Linear	2
A.2 Real datasets
Regression: In the regression problems, we utilize the MLP architecture for Dφ and Fθ as shown in
A5. We adopt the Adam optimizer with an initial learning rate of 0.001 and weight decay of 0.0001.
Classification: the details of 4-layer MLP architecture for Dφ are shown in Table A3. The VGG-5
with Spinal FC architecture (Kabir et al., 2020) for Fθ is presented in Table A6. For training MNIST
and Kuzushiji-MNIST datasets, Adam optimizer with learning rate of 0.005 is adopted for Fθ . For
Fθ of STL-10 and CIFAR-10 datasets, we use customized SGD optimizer with initial learning rate
of 0.001 and momentum of 0.9 and decay the learning rate by 0.1 every 7 epochs.
B	Appendix: Proofs
B.1	Proof of Lemma 2.1
ProofB.1 By assumption μ and γd* are both absolutely continuous with respect to the Lebesgue
measure. The desired result holds since it is a spacial case of the well known results on the exis-
Table A3: MLP architecture for Dφ of toy visualization examples and classification settings.
Layers	Dφ for visualization		Dφ for classification	
	Details	Output size	Details	Output size
Layer 1	Linear, LeakyReLU	64	Linear, LeakyReLU	32
Layer 2	Linear, LeakyReLU	128	Linear, LeakyReLU	64
Layer 3	Linear, LeakyReLU	64	Linear, LeakyReLU	32
Layer 4	Linear	1	Linear	1
12
Under review as a conference paper at ICLR 2021
Table A4: MLP architectures for Dφ and Fθ for simulated regression.
Layers	Dφ		Fθ	
	Details	Output size	Details	Output size
Layer 1	Linear, LeakyReLU	16	Linear, LeakyReLU	32
Layer 2	Linear, LeakyReLU	8	Linear, LeakyReLU	16
Layer 3	Linear	1	Linear, LeakyReLU	8
Layer 4			Linear	d
Table A5: MLP architectures for Dφ and Fθ for the real regression setting.				
	Dφ			Fθ	
Layers	Details	Output size	Details	Output size
Layer 1	Linear, LeakyReLU	8	Linear, LeakyReLU	16
Layer 2	Linear	1	Linear, LeakyReLU	32
Layer 3			Linear, LeakyReLU	8
Layer 4			Linear	d
tence of optimal transport Brenier (1991); McCann et al. (1995), see, Theorem 1.28 on page 24 of
Philippis (2013) for details.
B.2	Proof of Lemma 2.2
Proof B.2 Our proof follows Keziou (2003). Since f(t) is a convex function, then ∀t ∈ R f(t) =
f **(t), where
f **(t)=sup{st- F(s)}
s∈R
is the Fenchel conjugate of F. By Fermafs rule, the maximizer s* satisfies
t ∈ ∂F(s*),
i.e.,
s* ∈ ∂f (t)
Plugging the above display with t = dμZ (x) into the definition of f -divergence, we obtain (8).
Table A6: Network architecture for Fθ on MNIST and Kuzushiji-MNIST.
Layers
Details
Output size
VGG-5 Block
Fully Connected Spinal Block
Fully connected
Γ 3 3 × 3 Conv、 I)	Γ 3 3 × 3 Conv、	2
BN, ReLU × 2 3	× 2,	BN, ReLU	×3
3 × 3 Max Pool	3 × 3 Max Pool
Dropout, Linear
BN, ReLU × 4
Dropout, Linear
× 2	1 × 28 × 28
512
d
13