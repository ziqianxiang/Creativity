Under review as a conference paper at ICLR 2021
A Half-Space Stochastic Projected Gradient
Method for Group Sparsity Regularization
Anonymous authors
Paper under double-blind review
Ab stract
Optimizing with group sparsity is significant in enhancing model interpretability
in machining learning applications, e.g., feature selection, compressed sensing
and model compression. However, for large-scale stochastic training problems,
effective group sparsity exploration are typically hard to achieve. Particularly, the
state-of-the-art stochastic optimization algorithms usually generate merely dense
solutions. To overcome this shortage, we propose a stochastic method—Half-space
Stochastic Projected Gradient (HSPG) method to search solutions of high group
sparsity while maintain the convergence. Initialized by a simple Prox-SG Step,
the HSPG method relies on a novel Half-Space Step to substantially boost the
sparsity level. Numerically, HSPG demonstrates its superiority in deep neural
networks, e.g., VGG16, ResNet18 and MobileNetV1, by computing solutions of
higher group sparsity, competitive objective values and generalization accuracy.
1 Introduction
In many recent machine learning optimization tasks, researchers not only focus on finding solutions
with small prediction/generalization error but also concentrate on improving the interpretation of
model by filtering out redundant parameters and achieving slimmer model architectures. One
technique to achieve the above goal is by augmenting the sparsity-inducing regularization terms to the
raw objective functions to generate sparse solutions (including numerous zero elements). The popular
`1 -regularization promotes the sparsity of solutions by element-wise penalizing the optimization
variables. However, in many practical applications, there exist additional constraints on variables
such that the zero coefficients are often not randomly distributed but tend to be clustered into varying
more sophisticated sparsity structures, e.g., disjoint and overlapping groups and hierarchy (Yuan &
Lin, 2006; Huang et al., 2010; 2009). As the most important and natural form of structured sparsity,
the disjoint group-sparsity regularization, which assumes the pre-specified disjoint blocks of variables
are selected (non-zero variables) or ignored (zero variables) simultaneously (Bach et al., 2012),
serves as a momentous role in general structured sparsity learning tasks since other instances such as
overlapping group and hierarchical sparsity are typically solved by converting into the equivalent
disjoint group versions via introducing latent variables (Bach et al., 2012), and has found numerous
applications in computer vision (Elhamifar et al., 2012), signal processing (Chen & Selesnick, 2014),
medical imaging (Liu et al., 2018), and deep learning (Scardapane et al., 2017), especially on the
model compression of deep neural networks, where the group sparsity1 is leveraged to remove
redundant entire hidden structures directly.
Problem Setting. We study the disjoint group sparsity regularization problem which can be typically
formulated as the mixed `1 /`p -regularization problem, and pay special attention to the most popular
and widely used instance p as 2 (Bach et al., 2012; Halabi et al., 2018),
minimize ηψ(χ) d=f f (X) + X。(X) = -1 X fi(x) + λX k[x]gk0,	⑴
x∈Rn	N
i=1	g∈G
where λ > 0 is a weighting factor, ∣∣∙k denotes '2-norm, f (x) is the average of numerous N
continuously differentiable instance functions fi : Rn → R, such as the loss functions measuring the
deviation from the observations in various data fitting problems, Ω(x) is the so-called mixed '1∕'2
1Group sparsity is defined as # of zero groups, where a zero group means all its variables are exact zeros.
1
Under review as a conference paper at ICLR 2021
norm, G is a prescribed fixed partition of index set I = {1, 2,…，n}, wherein each component
g ∈ G indexes a group of variables upon the perspective of applications. Theoretically, a larger λ
typically results in a higher group sparsity while sacrifices more on the bias of model estimation,
hence λ needs to be carefully fine-tuned to achieve both low f and high group-sparse solutions.
Literature Review. Problem (1) has been well studied in deterministic optimization with various
algorithms that are capable of returning solutions with both low objective value and high group
sparsity under proper λ (Yuan & Lin, 2006; Roth & Fischer, 2008; Huang et al., 2011; Ndiaye et al.,
2017). Proximal methods are classical approaches to solve the structured non-smooth optimization (1),
including the popular proximal gradient method (Prox-FG) which only uses the first-order derivative
information. When N is huge, stochastic methods become ubiquitous to operate on a small subset
to avoid the costly evaluation over all instances in deterministic methods for large-scale problems.
Proximal stochastic gradient method (Prox-SG) (Duchi & Singer, 2009) is the natural stochastic
extension of Prox-FG. Regularized dual-averaging method (RDA) (Xiao, 2010; Yang et al., 2010) is
proposed by extending the dual averaging scheme in (Nesterov, 2009). To improve the convergence
rate, there exists a set of incremental gradient methods inspired by SAG (Roux et al., 2012) to utilizes
the average of accumulated past gradients. For example, proximal stochastic variance-reduced
gradient method (Prox-SVRG) (Xiao & Zhang, 2014) and proximal spider (Prox-Spider) (Zhang &
Xiao, 2019) are developed to adopt multi-stage schemes based on the well-known variance reduction
technique SVRG proposed in (Johnson & Zhang, 2013) and Spider developed in (Fang et al., 2018)
respectively. SAGA (Defazio et al., 2014) stands as the midpoint between SAG and Prox-SVRG.
Compared to deterministic methods, the studies of mixed '1∕'2 -regularization (1) in stochastic
field become somewhat rare and limited. Prox-SG, RDA, Prox-SVRG, Prox-Spider and SAGA are
valuable state-of-the-art stochastic algorithms for solving problem (1) but with apparent weakness.
Particularly, these existing stochastic algorithms typically meet difficulties to achieve both decent
convergence and effective group sparsity identification simultaneously (e.g., small function values but
merely dense solutions), because of the randomness and the limited sparsity-promotion mechanisms.
In depth, Prox-SG, RDA, Prox-SVRG, Prox-Spider and SAGA derive from proximal gradient method
to utilize the proximal operator to produce group of zero variables. Such operator is generic to
extensive non-smooth problems, consequently perhaps not sufficiently insightful if the target problems
possess certain properties, e.g., the group sparsity structure as problem (1). In fact, in convex setting,
the proximal operator suffers from variance of gradient estimate; and in non-convex setting, especially
deep learning, the discreet step size (learning rate) further deteriorates its effectiveness on the group
sparsity promotion, as will show in Section 2 that the projection region vanishes rapidly except RDA.
RDA has superiority on finding manifold structure to others (Lee & Wright, 2012), but inferiority
on the objective convergence. Besides, the variance reduction techniques are typically required to
measure over a huge mini-batch data points in both theory and practice which is probably prohibitive
for large-scale problems, and have been observed as sometimes noneffective for deep learning
applications (Defazio & Bottou, 2019). On the other hand, to introduce sparsity, there exist heuristic
weight pruning methods (Li et al., 2016; Luo et al., 2017), whereas they commonly do not equip with
theoretical guarantee, so that easily diverge and hurt generalization accuracy.
Our Contributions. Half-Space Stochastic Projected Gradient (HSPG) method overcomes the
limitations of the existing stochastic algorithms on the group sparsity identification, while maintains
comparable convergence characteristics. While the main-stream works on (group) sparsity have
focused on using proximal operators of regularization, our method is unique and fresh in enforcing
group sparsity more effectively by leveraging half-space structure and is well supported by the
theoretical analysis and empirical evaluations. We now summarize our contributions as follows.
•	Algorithmic Design: We propose the HSPG to solve the disjoint group sparsity regularized problem
as (1). Initialized with a Prox-SG Step for seeking a close-enough but perhaps dense solution
estimate, the algorithmic framework relies on a novel Half-Space Step to exploit group sparse
patterns. We delicately design the Half-Space Step with the following main features: (i) it utilizes
previous iterate as the normal direction to construct a reduced space consisting of a set of half-
spaces and the origin; (ii) a new group projection operator maps groups of variables onto zero
if they fall out of the constructed reduced space to identify group sparsity considerably more
effectively than the proximal operator; and (iii) with proper step size, the Half-Space Step enjoys
the sufficient decrease property, and achieves progress to optimum in both theory and practice.
•	Theoretical Guarantee: We provide the convergence guarantees of HSPG. Moreover, we prove
HSPG has looser requirements to identify the sparsity pattern than Prox-SG, revealing its superiority
2
Under review as a conference paper at ICLR 2021
on the group sparsity exploration. Particularly, for the sparsity pattern identification, the required
distance to the optimal solution x* of HSPG is better than the distance required by Prox-SG.
•	Numerical Experiments: Experimentally, HSPG outperforms the state-of-the-art methods in the
aspect of the group sparsity exploration, and achieves competitive objective value convergence
and runtime in both convex and non-convex problems. In the popular deep learning tasks, HSPG
usually computes the solutions with multiple times higher group sparsity and similar generalization
performance on unseen testing data than those generated by the competitors, which may be further
used to construct smaller and more efficient network architectures.
2 The HSPG method
We state the Half-Space Stochastic Projected Gradient (HSPG) method in Algorithm 1. In general,
it contains two stages: Initialization Stage and Group-Sparsity Stage. The first Initialization Stage
employs Prox-SG Step (Algorithm 2) to search for a close-enough but usually non-sparse solution
estimate. Then the second and fundamental stage proceeds Half-Space Step (Algorithm 3) started
with the non-sparse solution estimate to effectively exploit the group sparsity within a sequence of
reduced spaces, and converges to the group-sparse solutions with theoretical convergence property.
Algorithm 1 Outline ofHSPG for solving (1).
1:	Input: xo ∈ Rn, αo ∈ (0,1), e ∈ [0,1),and NP ∈ Z+.
2:	for k = 0, 1, 2, . . . do
3:	if k < NP then
4:	Compute Xk+ι J Prox-SG(xk, ak) by Algorithm 2.
5:	else
6:	Compute xk+1 J Half-Space(xk, αk, e) by Algorithm 3.
7:	Update αk+1.
Algorithm 2 Prox-SG Step.
1:	Input: Current iterate xk, and step size αk.
2:	Compute the stochastic gradient of f on mini-batch Bk
▽fBk (Xk) J |Bjj X Wi(Xk).
k i∈Bk
(2)
3:	Return Xk+1 J PrOxakλΩ(∙) (Xk — αkWfBk(Xk)).
Initialization Stage. The Initialization Stage performs the vanilla proximal stochastic gradient
method (Prox-SG, Algorithm 2) to approach the solution of (1). At kth iteration, a mini-batch Bk is
sampled to generate an unbiased estimator of the full gradient of f (line 2, Algorithm 2) to compute
a trial iterate Xbk+1 := Xk — αkWfBk (Xk), where αk is the step size, and fBk is the average of the
instance functions fi cross Bk. The next iterate Xk+1 is then updated based on the proximal mapping
12
Xk+1 = PrOXakλΩ(∙)(xk+ι) = arg mm ɔ—l∣x — Xk+ιk + λΩ(x),	(3)
x∈Rn	2αk
where the regularization term Ω(x) is defined in (1). Notice that the above subproblem (3) has a
closed-form solution, where for each g ∈ G , we have
[Xk+ι]g = max{0,1 - akλ/ k[bk+ι]g∣∣} ∙ [bk+ι]g∙	(4)
In HSPG, the Initialization Stage proceeds Prox-SG Step NP times as a localization mechanism
to seek an estimation which is close enough to a solution of problem (1), where NP := min{k :
k ∈ Z+, lXk — X* l ≤ R/2} associated with a positive constant R related to the optima, see (23) in
Appendix C. In practice, although the close-enough requirement is perhaps hard to be verified, we
empirically suggest to keep running the Prox-SG Step until observing some stage-switch signal by
testing on the stationarity of objective values, norm of (sub)gradient or validation accuracy similarly
to (Zhang et al., 2020). However, the Initialization Stage alone is insufficient to exploit the group
3
Under review as a conference paper at ICLR 2021
O xk+1	[x]l
(a) Half-Space Projection
□ Prox-SG '-∙, [χ]2"	''∙. a HSPG e = 0
ProX-SVRG '∙∙,	''∙.
ProX-SPider
SAGA
□ RDA
-λ
□ HSPG e ∈ (0,1)
O	ak 入"	[x]i
-αkλ	''∙.	''∙,
£ =0	£ ≈1
(b) Projection Region
Figure 1: Illustration of Half-Space Step with projection in (9), where G = {{1, 2}}.
sparsity structure, i.e., the computed solution estimate is typically dense, due to the randomness and
the moderate truncation mechanism of proximal operator constrained in its projection region, i.e., the
trial iterate [bk+ι]g is projected to zero only if it falls into an '2-ball centered at the origin with radius
αkλ by (4). Our remedy is to incorporate it with the following Half-Space Step, which exhibits an
effective sparsity promotion mechanism while still remains the convergent property.
Algorithm 3 Half-Space Step
1: Input: Current iterate xk, step size αk, and .
2: Compute the stochastic gradient of Ψ on I6=0 (xk) by mini-batch Bk
[VψBk (Xk)]ι=0(χk)
J |B^i X [vψi(xk)]z=0(xk)
(5)
3:	Compute [xk+ιk=。^)J [xk — &^^ (xk)]ι=o(χk)and [Xk+1]10(χk)J 0.
4:	for each group g in I6=0(xk) do
5:	if [Xk+ι]> [xk]g < e k [xk]g ∣∣1 2 then
6:	[xk+1]g J 0.
7:	Return Xk+1 J Xk+1.
Group-Sparsity Stage. The Group-Sparsity Stage is designed to effectively determine the groups
of zero variables and capitalize convergence characteristic, which is in sharp contrast to other heuristic
aggressive weight pruning methods but typically lacking theoretical guarantee (Li et al., 2016; Luo
et al., 2017). The underlying intuition of its atomic Half-Space Step (Algorithm 3) is to project
[xk]g to zero only if —[xk]g serves as a descent step to Ψ(xk), i.e., — [xk]> [VΨ(xk))]g < 0, hence
updating [Xk+1]g J [Xk]g — [Xk]g = 0 still results in some progress to the optimality. Before
introducing that, we first define the following index sets for any X ∈ Rn :
I0 (X) :=	{g	: g ∈ G,	[X]g = 0} andI6=0(X) :=	{g	: g ∈	G, [X]g	6=	0},	(6)
where I0(X) represents the indices of groups of zero variables at X, andI6=0(X) indexes the groups
of nonzero variables at X. To proceed, we further define an artificial set that X lies in:
S(X) := nz∈Rn : [z]g =0ifg∈I0(X),and[z]g>[X]g ≥e∣[X]g∣2 ifg∈I6=0(X)o[{0},
(7)
which consists of half-spaces and the origin. Here the parameter e > 0 controls the grey region
presented in Figure 1b, and the exact way to set e will be discussed in Section 4 and Appendix.
Hence, X inhabits S(Xk ), i.e., X ∈ S(Xk ), only if: (i) [X]g lies in the upper half-space for all
g ∈ I6=0(Xk) for some prescribed e ∈ [0, 1) as shown in Figure 1a; and (ii) [X]g equals to zero for all
g ∈ I0(Xk). The fundamental assumption for Half-Space Step to success is that: the Initialization
Stage has produced a (possibly non-sparse) solution estimate Xk nearby a group sparse solution x*
of problem (1), i.e., the optimal distance ∣∣Xk — x*∣ is sufficiently small. As seen in Appendix, it
further indicates that the group sparse optimal solution X* inhabits Sk := S(Xk), which implies that
4
Under review as a conference paper at ICLR 2021
Sk has already covered the group-support of x*, i.e., I=0(x*) ⊆ I=0(xk). Our goal now becomes
minimizing Ψ(x) over Sk to identify the remaining groups of zero variables, i.e., 10(x*)∕I0(xk),
which is formulated as the following smooth optimization problem:
Xk+ι = arg min Ψ(x) = f (x) + λΩ(x).
x∈Sk
(8)
By the definition of Sk, [x]I0(xk) ≡ 0 are constrained as fixed during Algorithm 3 proceeding, and
only the entries in I6=0(xk) are allowed to move. Hence Ψ(x) is smooth on Sk, and (8) is a reduced
space optimization problem. A standard way to solve problem (8) would be the stochastic gradient
descent equipped with Euclidean projection (Nocedal & Wright, 2006). However, such a projected
method rarely produces zero (group) variables as the dense XE illustrated in Figure 1a. To address it,
we introduce a novel projection operator to effectively conduct group projection as follows.
As stated in Algorithm 3, we first approximate the gradient of Ψ on the free variables in I6=0 (xk) by
[VψBk (Xk)]ι=0(χk) (line 2, Algorithm 3), then employ SGD to compute a trial point xek+1 (line 3,
Algorithm 3) which is passed into a new projection operator ProjSk (∙) defined as
Proj (z)	:=	[z]g if [z]g>[xk]g ≥ k[xk]g k ,	(9)
Sk	g 0	otherwise.
The above projector of form (9) is not the standard Euclidean projection operator in most
cases2, but still satisfies the following two advantages: (i) the actual search direction dk :=
(ProjSk(Xk+ι) - Xk)∕αk performs as a descent direction to Ψβk(Xk) := fBk(Xk) + λΩ(xk),
i.e., [dk]> [VΨBk (xk))]g < 0 as θ < 90° in Figure 1a, then the progress to the optimum is made via
the sufficient decrease property as drawn in Lemma 1; and (ii) effectively project groups of variables
to zero simultaneously if the inner product of corresponding entries is sufficiently small. In contrast,
the Euclidean projection operator is far away effective to promote group sparsity, as the Euclidean
projected point XE = 0 versus Xk+ι = ProjSk(Xk+ι) = 0 shown in Figure 1a.
Lemma 1. Algorithm 3 yields the next iterate Xk+ι as PrOjSk(Xk-α* VΨBk (Xk)), then the search di-
rection dk := (Xk+ι-Xk)∕αk is a descent directionfor 中万七(Xk), i.e., d>VΨBk (Xk) < 0. Moreover,
letting L be the Lipschitzconstantfor VΨBk on thefeasible domain, andGk ：= I=O(Xk) ∩I0(Xk+ι)
and Gk ：= I =O(Xk) T I=0(Xk+ι) be the sets ofgroups which projects or not onto zero, we have
ψBk (Xk +1)	≤ψBk	(Xk)	-	(αk -	-2-)	X	k[VψBk	(Xk)]g ∣∣2 - ( IaJ -	ɪ)	X k [xk]g k2 ∙	(10)
g∈Gk	g∈Gk
We then intuitively illustrate the strength of HSPG on group sparsity exploration. In fact, the half-
space projection (9) is a more effective sparsity promotion mechanism compared to the existing
methods. Particularly, it benefits from a much larger projection region to map a reference point
a^k+ι ：= Xk — αkVfek (Xk) or its variants to zero. As the 2D case described in Figure 1b, the
projection regions of Prox-SG, PrOX-SVRG, Prox-Spider and SAGA are '2-balls with radius as ak λ.
In stochastic learning, especially deep learning tasks, the step size αk is usually selected around 10-3
to 10-4 or even smaller for convergence. Together with the common setting of λ 1, their projection
regions would vanish rapidly, resulting in the difficulties to produce group sparsity. As a sharp contrast,
even though αkλ is near zero, the projection region of HSPG {X : Xk>X < (αkλ + kXk k) kXk k}
(seen in Appendix) is still an open half-space which contains those `2 balls as well as RDA’s if is
large enough. Moreover, the positive control parameter adjusts the level of aggressiveness of group
sparsity promotion (9), i.e., the larger the more aggressive, and meanwhile maintains the progress to
the optimality by Lemma 1. In practice, proper fine tuning is sometimes required to achieve both
group sparsity enhancement and sufficient decrease on objective value as will see in Section 4.
Intuition of Two-Stage Method: To end this section, we discuss the advantage of designing
such two stage schema rather than an adaptive switch back and forth between the Prox-SG Step
and Half-Space Step based on some evaluation switching criteria, as many multi-step deterministic op-
timization algorithms (Chen et al., 2017). In fact, we numerically observed that switching back to the
Prox-SG Step consistently deteriorate the progress of group sparsity exploration by Half-Space Step
while without obvious gain on convergence. Such regression on group sparsity by the Prox-SG Step
2Unless Ω(x) is ∣∣x∣∣ 1 where each g ∈ G is singleton, then Sk becomes an orthant face (Chen et al., 2020).
5
Under review as a conference paper at ICLR 2021
is less attractive in realistic applications, e.g., model compression, where people usually possess
heavy models of high generalization accuracy ahead and want to filter out the redundancy effectively.
Therefore, in term of the ease of application, we end at organizing Prox-SG Step and Half-Space Step
as such a two-stage schema, controlled by a switching hypermeter NP . In theory, we require NP
sufficiently large to let the initial iterate of Half-Space Step be close enough to the local minimizer
as shown in Section 3. In practice, HSPG is sensitive to the choice of NP at early iterations, i.e.,
switching to Half-Space Step too early may result accuracy loss. But such sensitivity vanishes rapidly
if switching to Half-Space Step after some acceptable evaluation switching criteria.
3 Convergence Analysis
In this section, we give the convergence guarantee of our HSPG. Towards that end, we make the
following widely used assumption in optimization literature (Xiao & Zhang, 2014; Yang et al., 2019)
and active set identification analysis of regularization problem (Nutini et al., 2019; Chen et al., 2018).
Assumption 1. Each f : Rn → R ,for i = 1,2,…，N, is differentiable and bounded below. Their
gradients Vfi(X) are Lipschitz continuous, and let L be the shared Lipschitz constant.
Assumption 2. The least and the largest '2-norm of non-zero groups in x* are lower and up-
per bounded by some constants, i.e., 0 < 2δι :二 ming∈ι=o(χ*) k[x*]gk and 0 < 2δ2 :=
maxg∈z=0(x*) k[x*]gk. Moreover, we request a common strict complementarity on any B, i.e.,
0 < 2δ3 := ming∈10(χ*) (λ 一 k[VfB(x*)]g∣∣) for regularization optimization.
Notations: Let x* be a local minimizer of problem (1) with group sparsity property, Ψ* be the
local minimum value corresponding to x*, and {xk}k∞=0 be the iterates generated from Algo-
rithm 1. Denote the gradient mapping of Ψ(x) and its estimator on mini-batch B as ξη (x) :=
1 (x - ProXηλΩ(∙)(x - ηVf(x))) and ξη,B(x) := ɪ (X - ProX办。(.)(X - ηVfB(x))) respectively.
We say X a stationary point of Ψ(x) if ξη(x) = 0. To be simple, let X be a neighbor of x* as
T?	r	I I	Il /	∙ ,1 TΛ	∙ . ∙	.	1,1, C- C-	1 Z	Zz>z'>∖ ♦
X := {X : ∣X - X* ∣ ≤ R} with R as a positive constant related to δ1 , δ2 and (see (23) in
AppendiX C), and M be the supremum of ∣∂Ψ(X)∣ on the compact set X.
Remark: Assumption 1 implies that VfB(X) measured on mini-batch B is Lipschitz continuous on
Rn with the same Lipschitz constant L, while VΨB(X) is not as shown in AppendiX. However, the
Lipschitz continuity of VΨB(X) still holds on X = {X : ∣[X]g∣ ≥ δ1 for each g ∈ G} by eXcluding
a '2-ball centered at the origin with radius δι from Rn. For simplicity, let VΨb(x) share the same
Lipschitz constant L on X with VfB(X), since we can always select the bigger value as their shared
Lipschitz constant. Now, we state the first main theorem of HSPG.
Theorem 1. Suppose f isconvex on X, e ∈ [θ, min {击,2δ1-R }), ∣xk - x*∣ ≤ RR for K ≥ NP.
Set k := K + t, (t ∈ Z+). Then for any τ ∈ (0, 1), there exist step size αk
0, min
2(1-e) ɪ 2δι-R-e(2δ2+R)
L-，L，
M
}), and mini-batch size |Bk| = O(t) ≤ N - 2M, such
that {xk } converges to some stationary point in expectation with probability at least 1 - τ, i.e.,
P(limk→∞ E [kξαk,Bk (xk)k] =0) ≥ 1-τ.
Remark: Theorem 1 only requires local convexity off on a neighborhood Xe of x* while itself can be
non-convex in general. This local convexity assumption appears in many non-convex analysis, such as:
tensor decomposition (Ge et al., 2015) and shallow neural networks (Zhong et al., 2017). Theorem 1
implies that if the Kth iterate locates close enough to x* , the step size αk and mini-batch size |Bk |
is set as above, (it further indicates x* inhabits the {Sk}k≥K of all subsequent iterates updated
by Half-Space Step with high probability in Appendix), then the Half-Space Step in Algorithm 3
guarantees the convergence to the stationary point. The O(t) mini-batch size is commonly used in the
analysis of stochastic algorithms, e.g., Adam and Yogi (Zaheer et al., 2018). Later based on numerical
results in Section 4, we observe that a much weaker increasing or even constant mini-batch size is
sufficient. In fact, experiments show that practically, a reasonably large mini-batch size can work
well if the variance is not large. Although the assumption ∣xK 一 x* ∣ < R/2 is hard to be verified
in practice, setting NP large enough usually performs quite well.
We then reveal the sparsity identification guarantee of HSPG as stated in Theorem 2.
6
Under review as a conference paper at ICLR 2021
Theorem 2. If k ≥ NP and ∣∣Xk 一 x*k ≤ [-20+03L, then HSPGyieIds10(x*) ⊆ 10(xk+1).
Remark: Theorem 2 shows that when Xk is in the '2-ball centered at x* with radius1，0k23 T, HSPG
1-+αk L
identifies the optimal sparsity pattern, i.e., I0(x*) ⊆ I0(xk+1). In contrast, to identify the sparsity
pattern, Prox-SG requires the iterates to fall into the '2-ball centered at x* with radius ɑkδ3 (Nutini
et al., 2019). Since αk ≤ 1/L and e ∈ [0,1), then 1-20+03L ≥ αkδ3 implies that the '2-ball
of HSPG contains the '2-ball of Prox-SG, i.e., HSPG has a stronger performance in sparsity pattern
identification. Therefore, Theorem 2 reveals a better sparsity identification property of HSPG
than Prox-SG, and no similar results exist for other methods to our knowledge.
The Initialization Stage Selection: To satisfy the pre-requirement of convergence of Half-Space
Step as Theorem 1, i.e., initial iterate close enough to x*, there exists several proper candidates
e.g., Prox-SG, Prox-SVRG and SAGA to form as the Initialization Stage. Considering the tradeoff
between computational efficiency and theoretical convergence, our default setting is to select Prox-SG.
Although Prox-SVRG/SAGA may have better theoretical convergence property than Prox-SG, they
require higher time and space complexity to compute or estimate full gradient on a huge mini-batch
or store previous gradient, which may be prohibitive for large-scale training especially when the
memory is often limited. Besides, it is well noticed that SVRG does not work as desired on the
popular non-convex deep learning applications (Defazio & Bottou, 2019; Chen et al., 2020). In
contrast, Prox-SG is efficient and can also achieves the good initialization assumption in Theorem 1,
i.e., ∣xNP 一 x* ∣ ≤ R/2, in the manner of high probability via performing sufficiently many times,
as revealed in Appendix C.4 by leveraging related literature (Rosasco et al., 2019) associated with an
additional strongly convex assumption. However, one should notice that Prox-SG does not guarantee
any group sparsity property of xNP due to the limited projection region and randomness.
Remark: We emphasize that this paper focuses on improving the group sparsity identification, which
is rarely explored and also a key indicator of success for structured sparsity regularization problem.
Meanwhile, we would like to point out improving the convergence rate has been very well explored
in a series of literatures (Reddi et al., 2016; Li & Li, 2018), but out of our main consideration.
4 Numerical Experiments
In this section, we present results of several benchmark numerical experiments in deep neural networks
to illustrate the superiority of HSPG than other related algorithms on group sparsity exploration and
the comparable convergence. Besides, two extensible convex experiments are conducted in Appendix
to empirically demonstrate the validness and superiority of the group sparsity identification by HSPG.
Image Classification: We now consider the popular Deep Convolutional Neural Networks (DC-
NNs) for image classification tasks. Specifically, we select several popular and benchmark
DCNN architectures, i.e., VGG16 (Simonyan & Zisserman, 2014), ResNet18 (He et al., 2016)
and MobileNetV1 (Howard et al., 2017) on two benchmark datasets CIFAR10 (Krizhevsky & Hinton,
2009) and Fashion-MNIST (Xiao et al., 2017). We conduct all experiments for 300 epochs with a
mini-batch size of 128 and λ as 10-3, since it returns competitive testing accuracy to the models
trained without regularization, (see more in Appendix D.3). The step size αk is initialized as 0.1, and
decayed by a factor 0.1 periodically. We set each kernel in the convolution layers as a group variable.
In these experiments, we proceed a test on the objective value stationarity similarly to (Zhang et al.,
2020, Section 2.1) and switch to Half-Space Step roughly on 150 epochs with NP as 150N/|B|.
The control parameter e in the half-space projection (9) controls the aggressiveness level of group
sparsity promotion, which is first set as 0, then fined tuned to be around 0.02 to favor the sparsity
level whereas does not hurt the target objective Ψ; the detailed procedure is in Appendix D.3. We
exclude RDA because of no acceptable results attained during our tests with the step size parameter
γ setting throughout all powers of 10 from 10-3 to 103, and skip Prox-Spider and SAGA since
Prox-SVRG has been a superb representative to the proximal incremental gradient methods.
Table 1 demonstrates the effectiveness and superiority of HSPG, where we mark the best values as
bold, and the group sparsity ratio is defined as the percentage of zero groups. In particular, (i) HSPG
computes remarkably higher group sparsity than other methods on all tests under both e = 0 and fine
tuned e, of which the solutions are typically multiple times sparser in the manner of group than those
of Prox-SG, while Prox-SVRG performs not comparably since the variance reduction techniques may
7
Under review as a conference paper at ICLR 2021
Table 1: Final Ψ∕group sparsity ratio/testing accuracy for tested algorithms on non-convex problems.
Backbone Dataset
Prox-SG
Prox-SVRG
HSPG
e as 0	fine tuned e
VGG16
ResNet18
MobileNetV1
CIEAR10	0.59 /53.95%/90.57%~~0.82/ 14.73%/89.42%~~0.59 /74.60%/91.10%^^0.59 /75.61% /90.92%
^Pashion-MNIST_	0.54/_15.63%/92.99%	_	2.66/0.45%^92.69%	_	_0.54/22.18%/92.98% _	0.53/60.77%/92.87%
一— CIEAR10	^031 /"19.50%/-94.09%-	-	0；36/2：79%^94.17%	~	0.3174158%/94.39% 一	刁31 /62.97% /94.53%
^Eashion-MNIST_	_0.14/0.00%/94.82%_	_	0.19/0.00%^94.64%	_	0.131_ 6.60%/94.93% _	_0J3L63.93% /^94.86%
一— CIEaR10	0.40/57.81%/91.60%	^0.65732.22%790.08%-	P.40Γ65.04% /91.86% ~	0.4Γ/71.66% /91.54%
Fashion-MNIST 0.22 / 65.80% / 94.36% 0.48 / 38.76% / 93.95%	0.23 / 74.52% / 94.43%	0.24 / 83.71% / 94.44%
oitaR ytisrapS puorG
Testing Accuracy

(a) Objective Ψ	(b) Group Sparsity Ratio	(c) Testing Accuracy	(d) HSPG VS Truncation
Figure 2: On ResNet18 with CIFAR10, (a)-(c): Evolution of Ψ, group sparsity ratio and testing accuracy,
(d): HSPG versus Prox-SG* and Prox-SVRG* (Prox-SG and Prox-SVRG with simple truncation mechanism).
not work as desired for deep learning applications (Defazio & Bottou, 2019); (ii) HSPG performs
competitively with respect to the final objective values Ψ and f (see f in Appendix). In addition,
all the methods reach a comparable generalization performance on unseen test data. On the other
hand, sparse regularization methods may yield solutions with entries that are not exactly zero but
are very small. Sometimes all entries below certain threshold (T) are set to zero (Jenatton et al.,
2010; Halabi et al., 2018). However, such simple truncation mechanism is heuristic-rule based, hence
may hurt convergence and accuracy. To illustrate this, we set the groups of the solutions of Prox-SG
and Prox-SVRG to zero if the magnitudes of the group variables are less than some T, and denote the
corresponding solutions as Prox-SG* and Prox-SVRG*. As shown in Figure 2d(i), under the T with
no accuracy regression, Prox-SG* and Prox-SVRG* reach higher group sparsity ratio as 60% and
32% compared to Table 1, but still significantly lower than the 70% of HSPG under = 0.05 without
simple truncation. Under the T to reach the same group sparsity ratio as HSPG, the testing accuracy
of Prox-SG* and Prox-SVRG* regresses drastically to 28% and 17% in Figure 2d(ii) respectively.
Remark here that although further refitting the models from Prox-SG* and Prox-SVRG* on active
(non-zero) groups of weights may recover the accuracy regression, it requires additional engineering
efforts and training cost, which is less attractive and convenient than HSPG (with no need to refit).
Finally, we investigate the group sparsity evolution under different ’s. As shown in Figure 2b, HSPG
produces the highest group-sparse solutions compared with other methods. Notably, at the early NP
iterations, HSPG performs merely the same as Prox-SG. However, after switching to Half-Space Step
at the 150th epoch, HSPG outperforms all the other methods dramatically, and larger results in
higher sparsity level. It is a strong evidence that our half-space based technique is much more
successful than the proximal mechanism and its variants in terms of the group sparsity identification.
Besides, the evolutions of Ψ and testing accuracy confirm the comparability on convergence among
the tested algorithms. Particularly, the objective Ψ generally monotonically decreases for small = 0
to 0.02, and experiences a mild pulse after switch to Half-Space Step for larger , e.g., 0.05, which
matches Lemma 1. As a result, with the similar generalization accuracy, HSPG allows dropping entire
hidden units of networks, which may further achieve automatic dimension reduction and construct
smaller model architectures for efficient inference.
5 Conclusions and Future Work
We proposed a new Half-Space Stochastic Projected Gradient (HSPG) method for disjoint group-
sparsity induced regularized problem, which can be applied to various structured sparsity stochastic
learning problem. HSPG makes use of proximal stochastic gradient method to seek a near-optimal
solution estimate, followed by a novel half-space group projection to effectively exploit the group
sparsity structure. In theory, we provided the convergence guarantee, and showed its better sparsity
identification performance. Experiments on both convex and non-convex problems demonstrated
that HSPG usually achieves solutions with competitive objective values and significantly higher group
sparsity compared with state-of-the-arts stochastic solvers. Further study is needed to investigate the
proper leverage of group sparsity into diverse deep learning applications, e.g., help people design and
understand optimal network architecture by removing redundant hidden structures.
8
Under review as a conference paper at ICLR 2021
References
Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et al. Structured sparsity
through convex optimization. Statistical Science, 27(4):450-468, 2012.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: Data repository. 2011. URL https://www.csie.
ntu.edu.tw/~cjlin/libsvmtools/datasets/.
Po-Yu Chen and Ivan W Selesnick. Group-sparse signal denoising: non-convex regularization, convex
optimization. IEEE Transactions on Signal Processing, 62(13):3464-3478, 2014.
Tianyi Chen. A Fast Reduced-Space Algorithmic Framework for Sparse Optimization. PhD thesis,
Johns Hopkins University, 2018.
Tianyi Chen, Frank E Curtis, and Daniel P Robinson. A reduced-space algorithm for minimizing
'ι-regularized convex functions. SIAM Journal on Optimization, 27(3):1583-1610, 2017.
Tianyi Chen, Frank E Curtis, and Daniel P Robinson. Farsa for `1 -regularized convex optimization:
local convergence and numerical experience. Optimization Methods and Software, 2018.
Tianyi Chen, Tianyu Ding, Bo Ji, Guanyi Wang, Yixin Shi, Sheng Yi, Xiao Tu, and Zhihui Zhu.
Orthant based proximal stochastic gradient method for `1 -regularized optimization. arXiv preprint
arXiv:2004.03639, 2020.
Aaron Defazio and Leon Bottou. On the ineffectiveness of variance reduced optimization for deep
learning. In Advances in Neural Information Processing Systems, 2019.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. Saga: A fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in neural information
processing systems, pp. 1646-1654, 2014.
Dmitriy Drusvyatskiy and Adrian S Lewis. Error bounds, quadratic growth, and linear convergence
of proximal methods. Mathematics of Operations Research, 43(3):919-948, 2018.
John Duchi and Yoram Singer. Efficient online and batch learning using forward backward splitting.
Journal of Machine Learning Research, 10(Dec):2899-2934, 2009.
Ehsan Elhamifar, Guillermo Sapiro, and Rene Vidal. See all by looking at a few: Sparse modeling
for finding representative objects. In 2012 IEEE Conference on Computer Vision and Pattern
Recognition, pp. 1600-1607. IEEE, 2012.
Cong Fang, Chris Junchi Li, Zhouchen Lin, and Tong Zhang. Spider: Near-optimal non-convex opti-
mization via stochastic path-integrated differential estimator. In Advances in Neural Information
Processing Systems, pp. 689-699, 2018.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
Robert M. Gower. Convergence theorems for gradient descent. University of Illinois, Urbana
Champaign, 2018.
Marwa El Halabi, Francis Bach, and Volkan Cevher. Combinatorial penalties: Which structures
are preserved by convex relaxations? In International Conference on Artificial Intelligence and
Statistics, pp. 1551-1560. PMLR, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
2016.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Junzhou Huang, Xiaolei Huang, and Dimitris Metaxas. Learning with dynamic group sparsity. In
2009 IEEE 12th International Conference on Computer Vision, pp. 64-71. IEEE, 2009.
9
Under review as a conference paper at ICLR 2021
Junzhou Huang, Tong Zhang, et al. The benefit of group sparsity. The Annals of Statistics, 38(4):
1978-2004, 2010.
Junzhou Huang, Tong Zhang, and Dimitris Metaxas. Learning with structured sparsity. Journal of
Machine Learning Research, 12(Nov):3371-3412, 2011.
Rodolphe Jenatton, Guillaume Obozinski, and Francis Bach. Structured sparse principal component
analysis. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, pp. 366-373, 2010.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Hamed Karimi, Julie Nutini, and Mark Schmidt. Linear convergence of gradient and proximal-
gradient methods under the Polyak-IojasieWicz condition. In Joint European Conference on
Machine Learning and Knowledge Discovery in Databases. Springer, 2016.
A. Krizhevsky and G. Hinton. Learning multiPle layers of features from tiny images. Master’s thesis,
Department of Computer Science, University of Toronto, 2009.
Sangkyun Lee and StePhen J Wright. Manifold identification in dual averaging for regularized
stochastic online learning. The Journal of Machine Learning Research, 13(1):1705-1744, 2012.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Zhize Li and Jian Li. A simPle Proximal stochastic gradient method for nonsmooth nonconvex
oPtimization. In Advances in neural information processing systems, PP. 5564-5574, 2018.
Shujun Liu, Jianxin Cao, Hongqing Liu, Xichuan Zhou, Kui Zhang, and Zhengzhou Li. Mri
reconstruction via enhanced grouP sParsity and nonconvex regularization. Neurocomputing, 272:
108-121, 2018.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level Pruning method for deeP neural
netWork comPression. In Proceedings of the IEEE international conference on computer vision,
PP. 5058-5066, 2017.
Michael Mitzenmacher. Probability and comPuting-randomized algorithms and Probabilistic analysis.
JOURNAL-OPERATIONAL RESEARCH SOCIETY, 56(12):1454, 2005.
Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, and JosePh Salmon. GaP safe screening rules
for sParsity enforcing Penalties. The Journal of Machine Learning Research, 18(1):4671-4703,
2017.
Yurii Nesterov. Primal-dual subgradient methods for convex Problems. Mathematical programming,
2009.
Jorge Nocedal and StePhen Wright. Numerical optimization. SPringer Science & Business Media,
2006.
Julie Nutini, Mark Schmidt, and Warren Hare. “active-set comPlexity” of Proximal gradient: HoW
long does it take to find the sParsity Pattern? Optimization Letters, 13(4):645-655, 2019.
Sashank J Reddi, Suvrit Sra, Barnabas Poczos, and Alexander J Smola. Proximal stochastic methods
for nonsmooth nonconvex finite-sum oPtimization. In Advances in Neural Information Processing
Systems, PP. 1145-1153, 2016.
Lorenzo Rosasco, Silvia Villa, and Bng Cong VU. Convergence of stochastic proximal gradient
algorithm. Applied Mathematics & Optimization, PP. 1-27, 2019.
Volker Roth and Bernd Fischer. The group-lasso for generalized linear models: uniqueness of
solutions and efficient algorithms. In Proceedings of the 25th international conference on Machine
learning, pp. 848-855, 2008.
10
Under review as a conference paper at ICLR 2021
Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an exponential
convergence _rate for finite training sets. In Advances in neural information processing Systems,
pp. 2663-2671,2012.
Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regular-
ization for deep neural networks. Neurocomputing, 241:81-89, 2017.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machine learning algorithms, 2017.
Lin Xiao. Dual averaging methods for regularized stochastic learning and online optimization.
Journal of Machine Learning Research, 11(Oct):2543-2596, 2010.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduction.
SIAM Journal on Optimization, 24(4):2057-2075, 2014.
Haiqin Yang, Zenglin Xu, Irwin King, and Michael R Lyu. Online learning for group lasso. In
Proceedings of the 27th International Conference on Machine Learning (ICML-10), pp. 1191-1198,
2010.
Minghan Yang, Andre Milzarek, Zaiwen Wen, and Tong Zhang. A stochastic extra-step quasi-newton
method for nonsmooth nonconvex optimization. arXiv preprint arXiv:1910.09373, 2019.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods
for nonconvex optimization. In Advances in neural information processing systems, pp. 9793-9803,
2018.
Junyu Zhang and Lin Xiao. Multi-level composite stochastic optimization via nested variance
reduction. arXiv preprint arXiv:1908.11468, 2019.
Pengchuan Zhang, Hunter Lang, Qiang Liu, and Lin Xiao. Statistical adaptive stochastic gradient
methods. arXiv preprint arXiv:2002.10597, 2020.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural networks. In International Conference on Machine Learning, 2017.
A Projection Region
In this Appendix, we derive the projection region of HSPG, and reveal that is a superset of those
of Prox-SG, Prox-SVRG and Prox-Spider under the same αk and λ.
Proposition 1. The Half-Space Step of HSPG yields next iterate xk+1 based on the trial iterate
Xk+1 = Xk - αk WBk(Xk) asfollowsforeach g ∈ Z =0(xk)
[xk+1]g
I [xk + 1]g - akλ
0
Ixk ]g
k[xk ]gk
if [xk + 1]g Ixk]g > (αk λ + E) Il [xk]g Il
otherwise.
(11)
Consequently, if ∣∣[xk+ι]g∣∣ ≤ akλ, then [xk+ι]g = 0 for any e ≥ 0.
Proof. For g ∈ I6=0(xk) T I6=0(xk+1), by Algorithm 3, it is equivalent to
>
g
xk - αk WfBk (xk ) - αk λ
[xk]g ]
k[xk y」
[xk]g > eI[xk]gI2,
[xk + 1]> [xk]g - akλ k[xk]g k > e k[xk]g『,
[xk+ι]>[xk ]g > (αkλ + e k[xk]g k)k[xk]g k .
(12)
11
Under review as a conference paper at ICLR 2021
Similarly, g ∈ I6=0(xk) T I 0(xk+1) is equivalent to
>
g
Xk ― akVfBk (Xk) ― akλ
[xk]g ]
k[xk ]g k」
[Xk]g ≤	k[Xk]g k ,
[xk + 1]g [xk]g ― αkλ Il [xk]g Il ≤ e k [xk]g k ,
[Xk+ι]>[Xk]g≤ (αkλ + e k[xk]gk) k[xk]gk .
(13)
If k[xk+ι]gk ≤ αkλ, then
[xk+1]g [xk]g ≤ Il [xk+1]g ∣∣∣∣ [xk]g ∣∣ ≤ αkλ ∣∣ [xk]g ∣∣ .
(14)
Hence [xk+1]g = 0 holds for any e ≥ 0 by (13), which implies that the projection region
of Prox-SG and its variance reduction variants, e.g., Prox-SVRG, Prox-Spider and SAGA are the
subsets ofHSPG's.	□
B Non-Lipschitz Continuity of VΨ on Rn
The first-derivative of Ψ(x) at x 6= 0 can be written as
vψ(x)=vf(x)+λ X ≡	(15)
We next show 口怦口 is not Lipschitz continuous on Rn if |g| ≥ 2. Take a example for x ∈ R2, and
select x1 = (t, a1t)>, x2 = (t, a2t)>, a1 6= a2 and t ∈ R. Then suppose there exists a positive
constant L < ∞ such that Lipschitz continuity holds as follows
x1 _ x2
kx1k	Ilx2k
(1, a1)>	(1, a2)>
—	=——	=
pl + a2 pl + a2
≤ Lkx1 -x2k
≤ LlaI - a2| ∙ Itl
(16)
holds for any t ∈ R, and note the left hand side is a positive constant. However, letting t → 0, we
have that L → ∞ which contradicts the L < ∞. Therefore,谭gj is not Lipschitz continuous on
R2 , specifically the region surrounding the origin point.
Although [vΨ(x)]I6=0(x) is not Lipscthiz continuous on Rn, the Lipschitz continuity still holds on by
excluding a fixed size '2-ball centered at the origin for the group of non-zero variables I=0(x) from
Rn. For our paper, we define the region where Lipscthiz continuity of [vΨ(x)]I6=0 (x) still holds as
X = {x : k[x]g k ≥ δ1 for each g ∈ I6=0(x), and [x]g = 0 for each g ∈ I0(x)}.	(17)
C Convergence Analysis Proof
Denote the sets of groups which are projected or not onto zero as
Gk ：= I =0(xk) \10(xk+1), and	(18)
Ck ：= I=0(xk) \I =0(xk+1).	(19)
Denote X ：= {x ： k[x]gk ≥ δ1 for each g ∈ G} where the Lipschitz continuity of vΨB(x) still
holds by excluding a '2-ball centered at the origin with radius δι from Rn. Let M denote one upper
bound of k∂ Ψk and kξk.
Additionally, establishing some convergence results require the below constants to measure the least
and largest magnitude of non-zero group variables in x*,
0 < δl ：= 1 min、k[x*]gk , and	QO)
2 g∈I=0(χ*)
0 <δ2 ：= 1 max、k[x*]g Il .	QI)
2 g∈I6=0(x*)
12
Under review as a conference paper at ICLR 2021
and a subsequent results of strict complementary assumption on any B uniformly,
0 <δ3 :=1 min (λ -∣∣[VfB(x*)]gk)	(22)
2 g∈I0(x*)
And denote the following frequently used constant R describing the size of neighbor around x*.
R := min
-(δι + 2吨)+ √(δι + 242)2 -E + 诉 ,
-------------------------------------------,δ1
Remark: (23) is well defined as 0 < e < δ∣-, and degenerated to δι as e = 0.
> 0.	(23)
C.1 Sufficient Decrease of Prox- S G Step and Half-Space Step
Our convergence analysis relies on the following sufficient decrease properties of Half-Space Step
and Prox-SG Step.
Sufficient Decrease of Half-Space Step: We prove the Lemma 1 as the below.
Proof of Lemma 1:
It follows Algorithm 3 and the definition of Gk and Gk as (19) and (18) that xk+1 = xk + αkdk
where dk is
f-[∂ΨBk(Xk)]g if g ∈ Ck = I=0(Xk) TI =0(Xk+l),
[dk]g = - -[xk ]g∕αk	if g ∈Gk = I=0(xk) T10(xk+ι),	(24)
10	otherwise.
We also notice that for any g ∈ Ck, the following holds
[xk - αk∂ΨBk (xk)]g [xk]g < e k[xk]g k ,	(25)
(1- e) k[xk]gk2 < αk[∂ΨBk(xk)]g>[xk]g.
For simplicity, let Ik6=0 := I6=0(xk). Since [dk]g = 0 for any g ∈ I0(xk), then by (24) and (25), we
have
dk>∂ΨBk (xk) = [dk]I>6=0 [∂ΨBk (xk)]I6=0
kk
=-X k[∂ΨBk (Xk)]g『-X 」[Xk]>[∂ΨBk (Xk )]g
g∈Gk	g∈Gk αk	(26)
≤ - X k[∂ΨBk (Xk)]g『-X ɪ (1 - e) k[xk]g k2 < 0,
g∈Gk	g∈Gk k
holds for any e ∈ [0, 1), which implies that dk is a descent direction for ΨBk (Xk).
Now, we start to prove the suffcient decrease of Half-Space Step. By the descent lemma, Xk ∈ X
and the Lipschitz continuity of [∂ΨBk]I6=0 on X, we have that
ΨBk (xk + αkdk) ≤ ΨBk (xk) + αk [∂ΨBk (xk )]>=o [dk]i=o + — Ok ∣∣[dk]i=o ∣∣ .	(27)
Then it follows (24) that (27) can be rewritten as follows
ΨBk (Xk + αkdk )
≤ψBk (Xk) + αk[dψBk (Xk)]>=0 [dk]I=0 + — αk ∣∣[dk]I=0 ∣∣
=ψBk(Xk)- X k[∂ψBk(Xk)]gk2 ,k - — O) - X {[∂ψBk(Xk)]>[Xk]g — — k[Xk]gk2}
g∈Gk	g∈Gk
(28)
13
Under review as a conference paper at ICLR 2021
Consequently, combining with ∈ [0, 1) and (25), (28) can be further shown as
ΨBk (xk+1) ≤ ΨBk (xk) -
which completes the proof.
(αk - α2L) X k[∂ΨBk(Xk)]gk2 - (1— — 2) X k[xk]gk2 ,
g∈GGk	g∈Gk
(29)
Sufficient Decrease of Prox-SG Step: The second lemma is well known for proximal operator
under our notations. We include this proof for completeness.
Lemma 2. Line 3 of Algorithm 2 yields that xk+1 = xk - αk ξαk,Bk (xk), where
ξαk,Bk (Xk) ∈ VfBk (Xk)+ λ∂Ω(Xk + ι).	(30)
And the objective value ΨBk satisfies
ψBk (Xk +1) ≤ ψBk (Xk)- ( αk	k- ) kξɑk,Bk (Xk)k2 ∙	(31)
Proof. It follows from the line (3) in Algorithm 2 and the definitions of proximal operator that
12
Xk+1 = arg min -- IlX 一 (Xk - ɑk WBk (Xk))k + XΩ(x)
x∈Rn 2αk
>	12
=arg min WfBk (Xk) (X - Xk)+ λΩ(X) + 2-- ∣∣x - Xk ∣∣
By the optimal condition, we have
0 ∈	(Xk+1 — Xk) + VfBk (Xk) + λdC(Xk+1).
-k	k
Since Xk+1 = Xk - -kξαk,Bk(Xk), we have
0 ∈ -ξαk,Bk (Xk) + VfBk (Xk) + λdC(Xk+1),
which implies that
ξαk,Bk (Xk) ∈ VfBk (Xk) + λ∂Ω(Xk + 1).
And thus there exists some V ∈ ∂Ω(Xk+1) such that
ξαk,Bk (Xk ) = VfBk (Xk) + λv.
By LiPSchitz continuity of WfBk and convexity of Ω(∙),we have
fBk (Xk+1 ) = fBk (Xk - -kξαk,Bk (Xk))
≤ fBk (Xk) - -kWfBk (Xk)Tξɑk,Bk (Xk) H	~~ ∣∣ξɑ.,Bk (Xk)∣∣
and
λΩ(Xk+1) = λΩ(Xk - -k Wak ,Bk (Xk ))
≤ λC(Xk) + λv (Xk — —kξak,Bk (Xk ) — Xk)
=λΩ(Xk) - -kλv>ξak,Bk (Xk).
(32)
(33)
(34)
(35)
(36)
(37)
(38)
Hence, by (36), (37) and (38), the objective ΨBk (Xk+1) satisfies
ΨBk (Xk+1) = fBk (Xk+1) + λΩ(Xk+1)
≤fBk (Xk)- -kwfBk (Xk)Tξak,Bk (Xk) H	2- kξak,Bk (Xk ) k + λC(Xk) - -kλv'ξak,Bk (Xk)
=ψBk (Xk)- αk(WfBk (Xk) + λv)Tξak ,Bk (Xk ) H	I- kξak,Bk (Xk) k
=ψBk (Xk)- (αk	|— ) kξak,Bk (Xk)『,
which completes the proof.	□
14
Under review as a conference paper at ICLR 2021
According to Lemma 1 and Lemma 2, the objective value on a mini-batch tends to achieve a sufficient
decrease in both Prox-SG Step and Half-Space Step given αk is small enough. By taking the
expectation on both sides, we obtain the following result characterizing the sufficient decrease from
Ψ(xk) to E [Ψ(xk+1)].
Corollary 1. For iteration k, we have
(i)	if kth iteration conducts Prox-SG Step, then
E [Ψ(xk+1)] ≤ Ψ(xk) - αk -
E kξαk,Bk(xk)k2 .
(39)
(ii)	if kth iteration conducts Half-Space Step, xk ∈ X, then
E[Ψ(xk+ι)] ≤ Ψ(xk)-X (a®- akL) E [∣∣∂ΨBk (Xk)『]-(F - L) X k[xk ]g k2 .
g∈Gk	g∈Gk
(40)
Corollary 1 shows that the bound of Ψ depends on step size ak and norm of search direction. It
further indicates that both Half-Space Step and Prox-SG Step can make some progress to optimality
with proper selection of ak.
C.2 Proof of Theorem 1
Toward that end, We first show that if the optimal distance from Xk to the local minimizer x* is
sufficiently small, then HSPG already covers the supports of x*, i.e., I=0(x*) ⊆ I=O(Xk).
Lemma 3. If kXk - X* k ≤ R, then I6=0(X*) ⊆ I6=0(Xk).
Proof. For any g ∈ I6=0(X*), by the assumption of this lemma and the definition of R as (23) and δ1
as (20), we have that
k[X*]gk-k[Xk]gk ≤k[Xk-X*]gk ≤kXk-X*k ≤R≤δ1	41
k[Xk]gk ≥ k[X*]gk -δ1 ≥ 2δ1-δ1 =δ1 >0	(41)
Hence ||[xk]g∣∣ = 0, i.e., g ∈ I =0(xk). Therefore, I=0(x*) ⊆ I=0(xk).	□
The next lemma shows that if the distance between current iterate Xk and X*, i.e., kXk - X* k is
sufficiently small, then X* inhabits the reduced space Sk := S(Xk).
Lemma 4. Under Assumption 1, if 0 ≤ e < δ1, |x® 一 x*∣ ≤ R, thenfor eachg ∈ I=0(x*),
[Xk]g>[X*]g ≥ |[Xk]g|2	(42)
Consequently, it implies X* ∈ Sk by the definition as (7).
Proof. It follows the assumption of this lemma and the definition of R in (23), δ1 and δ2 in (23), (20)
and (21) that for any g ∈ I6=0(X*),
|[Xk]g| ≤ |[X*]g| +R≤ 2δ2+R,
(43)
and the [-(δ1 +2eδ2) + P (δ1 + 2eδ2)2 — 4e2δ2 + 4eδ2 ] /e in (23) is actually the solution of ez2 +
(4eδ2 + 2δ1)z + 4eδ22 - 4δ12 = 0 regarding z ∈ R+. Then we have that
[Xk]g>[X*]g =[Xk - X* + X*]g>[X*]g
=[Xk-X*]g>[X*]g+|[X*]g|2
≥k[x*]gk2 -k[xk - x*]gk k[x*]gk
=|[X*]g| (|[X*]g| -|[Xk-X*]g|)
≥2δ1(2δ1 -R) ≥ e(2δ2 + R)2
≥e ∣[xk]gk2
(44)
15
Under review as a conference paper at ICLR 2021
holds for any g ∈ Z =0(x*), where the second last inequality holds because that 2δι(2δι - R)
(2δ2 + R)2 as R
-(δ1
+ 2δ2) +	(δ1 + 2δ2)2
-4e2δ2
definition of Sk as (7), we have x* inhabits Sk, which completes the proof.
+ 4δ12 /. Now combing with the
□
Furthermore, if ∣∣xk - x*k is small enough and the step size is selected properly, every recovery of
group sparsity by Half-Space Step can be guaranteed as successful as stated in the following lemma.
Lemma 5. Suppose k ≥ NP, ∣Xk 一 x*∣ ≤ R, 0 ≤ e < [)1R and 0 < ak ≤ 2δl-R-M(2δ2+R),
then for any g ∈ Gk = I=0(xk) T10(xk+ι), g must be in 10(x*), i.e., g ∈ 10 (x*).
Proof. To prove it by contradiction, suppose there exists some g ∈ Gk such that g ∈ I =0(x*). Since
g ∈ Gk = I=O(Xk) T10(xk+ι), then the group projection (9) is trigerred at g such that
[xk+1 ]g [xk]g = Ixk - αVψBk (Xk)]g [xk]g
=k [xk]g k - αk [VψBk (Xk)]g [xk]g < e k [xk]g k .
On the other hand, it follows the assumption of this lemma and g ∈ I6=0(x*) that
k[xk - x*]gk ≤ kxk - x* k ≤ R
Combining the definition of δ1 as (20) and δ2 as (21), we have that
k[xk]gk ≥ k[x*]gk -R≥2δ1-R
k[xk]gk ≤ k[x*]gk+R≤2δ2+R
(45)
(46)
(47)
It then follows 0 < αk ≤
e<
2δι-R
2δ2 + R
, that
2δl-R-fδ2+R), where note 2δι - R - e(2δ2 + R) > 0 as R ≤ δι and
[xk+ι]>[xk]g = k[xk]gk2 - αk[VψBk(xk)]>[xk]g
≥ k[xk ]g k2 - ak k[VΨBk (xk )]g kk[xk ]g k
=k[xk ]gk (k[xk]gk-αk k[VΨBk (xk )]g k)
≥k[xk]gk(k[xk]gk-αkM)
≥ k[xk]g k [(2δ1 - R) - αkM]
≥ k[xk]g k 卜2δι - R) - 2δ1- R -ʃ2 + R) M
≥ k[xk]g k [(2δ1 - R) - 2δ1 + R + e(2δ2 + R)]
≥ek[xk]gk(2δ2+R)
≥ ek[xk]gk2
(48)
which contradicts with (45). Hence, we conclude that any g of variables projected to zero, i.e.,
g ∈ Gk = I=0(xk) T10(xk+ι) are exactly also the zeros on the optimal solution x*, i.e., g ∈
10(x*).
We next present that if the iterate of Half-Space Step is close enough to the optimal solution x*,
then x* inhabits all reduced spaces constructed by the subsequent iterates of Half-Space Step with
high probability. To establish this results, we require the below two lemmas. The first bounds the
accumulated error because of random sampling. Here we introduce the error of gradient estimator on
I 6=0(x) forΨ on mini-batch B as
eB(x) ：= [V⅛B(x) - VΨ(x)]ι=o(χ),	(49)
where by the definition of Ω in problem (1), we have eB(x) also equals to the error of estimation for
Vf,
eB(x) = ∣VΨb(x) - VΨ(x)]ι=0(χ) = [VfB(x) - Vf (x)]ι=0(χ)∙	(50)
16
Under review as a conference paper at ICLR 2021
Lemma 6. Given any θ > 1, K ≥ NP, let k := K+t, t ∈ Z+ S{0}, then there exists αk = O(1/t)
and |Bk| = O(t), such that for any yt ∈ Rn,
∞	3R2
{yt}mt∞=a0x∈X ∞Xt=0αkkeBk(yt)k2≤
8(4R +1)
holds with probability at least 1 一 *.
Proof. Define random variable Yt := αK+t keBK+t (yt)k2 for all t ≥ 0. Since {yt}t∞=0 are arbitrarily
chosen, then the random variables {Yt}t∞=0 are independent. Let Y := Pt∞=0 Yt. Using Chebshev’s
inequality, we obtain
P (Y ≥ E[Y] + θPVar[Y]) ≤ P (|Y - E[Y]| ≥ θPVar[Y]) ≤ 需.	(51)
And based on the Assumption 1, there exists an upper bound σ2 > 0 for the variance of random noise
e(x) generated from the one-point mini-batch, i.e., B = {i}, i = 1, . . . , N. Consequently, for each
22
t ≥ 0, we have E[Yt] ≤ αK+tσ and Var[Yt] ≤ ∣K+t ∣ ,then combining with (51), We have
一，	Lt」一 ,∣Bκ+t∣	L t	|BK+t| ,	,
Y ≤ E[Y] + θ√Var[Y]
∞
≤X
t=0
ακ+tσ
P∣Bk+l
∞
+ θ ∙ X
t=0
αK+tσ2
∣Bκ+t∣
∞
≤X
t=0
αK+tσ
P∣Bk+l
∞
+ θ ∙ X
t=0
αK+tσ
P∣BK+∣
∞
(1+θ) X p≡
(52)
(53)
(54)
holds with probability at least 1 一 击.Here, for the second inequality, We use the property that the
equality E[Pt∞=0 Yi] = Pt∞=0 E[Yi] holds whenever Pt∞=0 E[∣Yi∣] convergences, see Section 2.1 in
Mitzenmacher (2005); and for the third inequality, we use αK+tσ ≤ 1 without loss of generality as
|BK+t |
the common setting of large mini-batch size and small step size.
Given any θ > 1, there exists some αk = O(1/t) and ∣Bk∣ = O(t), the above series converges and
satisfies that
∞
(1+θ)X
t=0
αK+tσ	3R2
pB⅛ ≤ 8(4R +I)
(55)
holds. Notice that the above proof holds for any given sequence {yt}t∞=0 ∈ X∞, thus
∞	3R2
max	αkk αk∣eBλ, (yt)∣∣2 ≤ / C---T
{yt}m=o∈x ∞ = kk Bk ("t)"2 ≤8(4R +1)
holds with probability at least 1 一 表.
□
The second lemma draws if previous iterate of Half-Space Step falls into the neighbor of x*, then
under appropriate step size and mini-batch setting, the current iterate also inhabits the neighbor with
high probability.
Lemma 7. Under the assumptions of Lemma 6, suppose IlxK 一 x*" ≤ R/2; for any ' satisfying
K ≤ ' < K +1, 0 < ɑ` ≤ min{L, 2δl-R-M(2δ2+R)}, ∣B'∣ ≥ N —蔡 and ∣∣X' 一 x*" ≤ R holds,
then
∣xK+t 一 x*∣ ≤ R.	(56)
holds with probability at least 1 一 θ2.
Proof. It follows the assumptions of this lemma, Lemma 5, (18) and (19) that for any ` satisfying
K ≤ '<K +1	-
∣∣[x*]gIl= 0, forany g ∈ G'.	(57)
17
Under review as a conference paper at ICLR 2021
Hence we have that for K ≤ ' < K +1,
"ι -打1 2
=X ∣∣[X' - x* - αgVΨ(xg) - αgβB'(x')]g∣∣2 + X ∣∣[x' - x* - X']g『
g∈Cg	g∈Gk
=X {∣[χ' -x*]gIl2 - 2αe[χe - χ*]J[VΦ(χe) + e¾(χ')]g + a2 ∣[VΦ(χe) + e¾(χ')]g『} + X ∣[χ*]g∣∣2
g∈G'	g∈G'
=X {∣[χ' - x*]gIl2 - 2αe[χe - χ*]>[Vψ(X`)]g - 2Mχ' - χ*]>[e¾(X`)]g + α2 ∣[Vψ(χe) + e¾(χe)]g『}
g∈G'
≤ X ∣[X' - x*]g『 -∣[VΨ(xe)]gI2(2α - 02) - 2αe[xe - x*]>[^^儿 + & 眄(-'儿『
g∈G'
+ 202[Vψ(χ')]J[e¾(χ')]g
≤ X ∣[X' -x*]g『-∣[VΨ(xe)]gI2(2号-02) +2α' M-X*]g|||。皿)]。∣ + 诚 ]。皿加『
g∈G'
+ 2ɑ ∣[VΨ(xe)]g |||[毁3① ∣
≤ X ∣[x'-x*]gI2 -∣[VΨ(x')]gI2(2号-02)+(20e + 2。泊晒一x*]g||。皿)]。∣ + & ]。皿加『
g∈G'
≤ X {∣[xe-x*]gI2 -∣[VΨ(x')]gI2(2号-α2)} + (20e + 2α")忸一多*|二加| + 说底(,')『
g∈Ce
(58)
On the other hand, by the definition of e∏(x) as (49), we have that
6b(x)=[VΨb(x) - VΨ(x)]ι≠0(x) = [Wb(x) - V/(x)]i=0(x)
1 r ―	1 A 一
=|B| EVfj (X)]z=0(x) - N ][Vfi(X)]z=0(x)
N X |B| Vfj (X)]z=0(x) -Vfj (X)]z=0(x)
1 N
-N £[Vi(X)]z=0(x)
i=B
(59)
ɪ X '
N j⅛[
N - |B|
--P Vfj (x)]i=0(x)
1 N
-N E[Vfi(X)]z=0(x)
i=1
i∈B
Thus taking the norm on both side of (59) and using triangle inequality results in the following:
IeB(x)I ≤ N X
N j∈B
N-Ph [Vfj(X)]ι=0(χ)∣∣
1 N
+ N XuVfi(X)IZ=0(χ) U
i=1
i∈B
(60)
1 N - |B|	1
≤ NFIBkIM + R(N TBI)M ≤
2(N - ∣B∣)M
N
18
Under review as a conference paper at ICLR 2021
Since ɑ` ≤ 1, and |B'| ≥ N — 备 hence ɑ` ||eB£(ʃ`)k ≤ 1. Then combining with ɑ` ≤ 1/L, (58)
can be further simplified as
E+i —x*『
≤ X {|3 —x*]g k2T[VΨ(xe)]g『(2 詈一α2)} + (20e + 2α")忸一门底(虫 + 说 IleBe)『
g∈G'
≤ X 1k[xe — x*]g『一ɪ k[VΨ(xe )]g『1 +4α' |同一x*IIkeB/g )| +。2 ∣eβe (xe)∣2
g∈小
≤ ∣∣X' — x*『+ 4αg IlX' — x*∣∣∣∣eBg(x')∣∣ + ɑ` IleBg(X`)k
(61)
Following from the assumption that ∣∣xg — x* | ≤ R, then (61) can be further simplified as
∣∣X'+1 — x*『≤ ∣∣X' — x*『+4&gR ∣∣eBg(x')∣∣ + αk ∣∣eBg(x')∣∣
≤∣X' — x*∣2 + (4R + l)ɑ` ∣∣eBg(x')∣∣
Summing the the both side of (62) from ' = K to ' = K +1 — 1 results in
K+t-1
∣∣xκ+t — x*『≤ IlXK — x*『+ (4R + 1) X ɑ` ∣∣eBg(xg)∣∣
2=K
It follows Lemma 6 that the followng holds with probability at least 1 — *,
二	3 R2
X MeBg(X损 ≤ 4(4R +1) ∙
Thus we have that
K+t-1
∣∣xκ+t — x*『≤ ∣∣xκ — x*『+ (4R + 1) X a` IeBg(Xe)I
'=K
R2
≤ ɪ + (4R + 1)
∞
≤ ∣∣XK — X*『+ (4R + 1) X αe∣eBg(x')∣∣
£=K
3R2 R2	3R2 c2
英E ≤ 7 +丁 ≤ R2,
(62)
(63)
(64)
(65)
holds with probability at least 1 — /, which completes the proof.
□
Based on the above lemmas, the Lemma 8 below shows if initial iterate of Half-Space Step locates
closely enough to x*, step size ak polynomially decreases, and mini-batch size Bk polynomially
increases, then x* inhabits all subsequent reduced space {Sk }∞=k constructed in Half-Space Step
with high probability.
Lemma 8. Suppose ∣∣xk — x*∣∣ ≤ 祭 K ≥ NP, k = K +1, t ∈ Z+, 0 < αk = O(1∕(√Nt)) ≤
min{ 2(1-e), L,以1-"-^2'2+® } and |Bk| = O(t) ≥ N — 备.Thenfor any constant T ∈ (0,1),
IlXk — x*∣∣ ≤ R with probability at least 1 — T for any k ≥ K.
Proof. It follows Lemma 4 and the assumption of this lemma that x* ∈ Sk . Moreover, it follows the
assumptions of this lemma, Lemma 6 and 7, the definition of finite-sum f (x) in (1), and the bound
of error as (60) that
(	1 ∖O(N-K)
P({xk}∞=k ∈{x : IX — x* I≤ R}∞) ≥ (1 -西)	≥ 1 — τ,	(66)
where the last two inequalities comes from that the error vanishing to zero as |Bk | reaches the upper
bound N, and θ is sufficiently large depending on T and O(N — K).	□
19
Under review as a conference paper at ICLR 2021
Corollary 2. Lemma 8further implies x* inhabits all subsequent Sk, i.e., x* ∈ Sk for any k ≥ K.
Next, we establish that after finitely number of iterations, HSPG generates sequences that inhabits in
the feasible domain X where Lipschitz continuity of Ψ holds.
Lemma 9. Suppose the assumptions of Lemma 8 hold, then after finite number of iterations, all
subsequent iterates xk ∈ X with high probability.
Proof. It follows Lemma 8 that all subsequent xk satisfying kxk - x* k ≤ R with high probability.
Combining with Lemma 3, we have that I6=0(x*) ⊆ I6=0(xk) for all k ≥ K with high probability.
Then for any g ∈ I6=0(xk), there are two possbilities, either g ∈ I6=0(x*) or g ∈ I0(x*). For the
first case g ∈ I6=0(x*) TI6=0(xk), it follows the definitions of R as (23) and δ1 as (20) that
k[xk - x*]g k ≤ kxk - x*k ≤ R ≤ δ1
k[x*]gk - k[xk]gk ≤ δ1	(67)
k[xk]gk ≥ k[x*]gk-δ1≥2δ1-δ1=δ1
For any g ∈ I0(x*) TI6=0(xk), by Algorithm 3, its norm is bounded below by
δ1≥k[xk-x*]gk=k[xk]gk≥tk[xK]gk,	(68)
where by the Theorem 2 will shown in Appendix C.3, if ∣∣[xk]g ∣∣ ≤ /注：3L, then [xk+ι]g equals to
zero and will be fixed as zero since Algorithm 3 operates on Sk as (7). Note αk = O(1/t), follow-
ing (Karimi et al., 2016, Theorem 4) and (Drusvyatskiy & Lewis, 2018, Theorem 3.2), E[∣[xk]g∣2] =
O(1/t). If > 0, then after finite number of iterations O(1/2), g ∈ I0(x*) TI6=0(xk) becomes
zero. If = 0, note Bk = O(t) and f is finite-sum, then similar result holds by (Gower, 2018,
Theorem 2.3, Theorem 3.2) (f needs further strongly convexity on X). Hence with high probability,
after finite number of iterations, denoted by T, all subsequent xk, k ≥ K + T inhabits X . Regarding
[xk]g∈ι°(x*)TI=0(χk) for K ≤ k ≤ K + T, note et ∣[xκ]91∣ is also bounded below by constant
eT ∣∣[xκ]gk > 0 given XK, for simplicity, denote the Lipschitz constant of [VΨ(xk)]g as L as
well.
We now prove the first main theorem of HSPG, i.e., Theorem 1.
Proof of Theorem 1:
We know that Algorithm 1 performs an infinite sequence of iterations. It follows Corollary 1 that for
any ` ∈ Z+ ,
`
E[Ψ(xκ)] - E[Ψ(x'+ι)] = X {E[Ψ(xk)] - E[Ψ(xk+ι)]}
k=K
≥ X (αk- α2L)X	E h∣[VΨ(xk)]gk2i	+ X (F	- L)X	∣[xk]gk2	.
K≤k≤' ∖	g g∈Gk	K≤k≤' ∖ k	)g∈Gk
(69)
Combining the assumption that Ψ is bounded below and letting ` → ∞, we obtain
X (αk -	α2L j	X E h∣[VΨ(xk)]gk2i	+ X (F	- L) X	k[xk]gk2	< ∞	(70)
k≥κ∖	2 g	g∈Gk	k≥κ∖αk 2 g∈Gk
By Algorithm 3, variables on I0(xk) are fixed during kth Half-Space Step and n is finite, then the
group projection appears finitely many times, consequently,
Σ
k≥K
1 — e
αk
- 2) X k[xk ]g k2
g g∈Gk
< ∞.
(71)
Thus (70) implies that
X (αk - α2L) X E [∣∣[VΨ(xk)]gk2i	(72)
k≥K、	g g∈Gk
2
=X ak X E [k[VΨ(xk)]gk2] - X L X E [k[VΨ(xk)]g『]< ∞	(73)
k≥K	g∈Gk	k≥K	g∈Gk
20
Under review as a conference paper at ICLR 2021
Since ɑk = 0(1/( √Nt)), then Pk≥κ αk = ∞ and Pk≥.κ 02 ≤∞. Combining with (72) and the
boundness of ∂Ψ, it implies
X α X E [k[VΨ(xk)]gk2i < ∞.	(74)
k≥K	g∈Gk
By Pk≥K αk = ∞ and (74), we have that
¾⅛f X E hk[VΨ(xk)]gk2i=0	(75)
k≥K
g∈Gk
then there exists a subsequence K such that
lim X E hk[VΨ(xk)]gk2i =0	(76)
k∈K
g∈Gk
It follows from the assumptions of this theorem and Lemma 3 to 8 and Corollay 2 that with high
probability at least 1 - T, for each k ≥ K, x* inhabits Sk. Note as |Bk | = O(t) linearly increases,
the error of gradient estimate vanishes. Hence, (76) naturally implies that the sequence {xk}k∈K
converges to some stationary point with high probability. And we can extend K to {k : k ≥ K}
due to the non-decreasing distance to optimal solution as shown in the Lemma 8. By the above, we
conclude that
P(limE[kξαk,Bk(xk)k]=0)≥ 1-τ.
k→∞
(77)
C.3 Proof of Theorem 2
In this Appendix, we compare the group sparsity identification property of HSPG and Prox-SG. We
first show the generic sparsity identification property of Prox-SG for any mixed 'ι /'p regularization
for p ≥ 1.
Lemma 10. If kxk - x* kp0 ≤ min{δ3 /L, αkδ3}, where 1/p + 1/p0 = 1 (p0 = ∞ ifp = 1), then
the Prox-SG yields that for each g ∈ I0(x*), [xk+1]g = 0 holds, i.e., I0(x*) ⊆ I0(xk+1).
Proof. It follows from the reverse triangle inequality, basic norm inequalities, Lipschitz continuity of
Vf (x) and the assumption of this lemma that for any g ∈ G,
k[V∕Bk (Xk )]gkp0 -k[VfBk (x*)]gkp0 ≤ k[VfBk (Xk ) -VfBk (x*)]gkp0
≤ kVfBk(xk)- VfBk(x*)kp0	(78)
δ3
≤ L IlXk - X kp0 ≤ L ∙ L = δ3.
By (78), we have that for any g ∈ I0(X*),
I[VfBk(Xk)]gIp0 ≤ I[VfBk(X*)]gIp0 +δ3	(79)
≤ λ - 2δ3 + δ3 = λ - δ3
Combining (79) and the assumption of this lemma, the following holds for any αk > 0 that
I[Xk-αkVfBk(Xk)]gIp0 ≤ I[Xk]gIp0+I[αkVfBk(Xk)]gIp0
≤ αkδ3 + αk(λ - δ3) = αkλ
(80)
which further implies that the Ecludiean projection yields that
ProjB(IHlpo,αkλ)([xk - αkvfBk (Xk)]g) = [xk - αk VfBk (Xk)]g.	(81)
Combining with (81), the fact that proximal operator is the residual of identity operator subtracted by
Euclidean project operator onto the dual norm ball and [Xk]g = 0 for any g ∈ I0(X*) (Chen, 2018),
we have that
[Xk+ι]g = PrOxakλk∙kp([Xk - αkVBk(Xk)]g)
=h1 - ProjB(IHlpOgi)] [Xk - ak VfBk (Xk)]g	(82)
= [Xk - αk VfBk (Xk )]g - [Xk - αkVfBk (Xk )]g = 0,
consequently I0(x*) ⊆ 10(Xk+ι), which completes the proof.	□
21
Under review as a conference paper at ICLR 2021
Now we establish the group-sparsity identification of HSPG as Theorem 2.
(83)
(84)
(85)
(86)
Proof of Theorem 2:
Suppose ∣∣Xk - x*k ≤ 1-2曹：3L. There is nothing to prove if g ∈ 10(x*) T10(xk). For g ∈
10(x*) TI=0(xk), We compute that
Ixk- ak VψBk (Xk)]g [xk]g - e k [xk]g ∣∣
=k[xk]gk2-αk[VΨBk(xk)]>[xk]g-e ∣[xk]s∣2
=(1 - e) ∣[xk]gk2 - a® ([VfBk(xk)]g + λτ⅛τ)	[xk]g
∣[xk]g∣
=(1-e)∣[xk]g∣2-αk[VfBk(xk)]g>[xk]g-αkλ∣[xk]g∣
≤(1-e)∣[xk]g∣2+αk∣[VfBk(xk)]g∣ ∣[xk]g∣ -αkλ∣[xk]g∣
=∣[xk]g∣ {(1 - e) ∣[xk]g∣ +αk∣[VfBk(xk)]g∣ - αkλ}
By the Lipschitz continuity of Vf, we have that for each g ∈ 10 (x*) T I=0 (xk),
k[VfBk (xk) -VfBk (x*)]g k≤ L k[xk - xl k = L k[xk ]g k
k[VfBk (xk)]g k≤ L k[xk ]g k + k[VfBk (x*)]g ∣∣
Combining with the definition of δ3, which implies that ∣ [VfBk (x*)]gk ≤ λ - 2δ3 that
∣[VfBk (xk)]g ∣ ≤ L ∣[xk]g∣ + λ - 2δ3
Hence combining with ∣ [xk]91∣ ≤ 20+：3Le, (83) can be further written as
[xk - αkVΨBk(xk)]g>[xk]g- e ∣[xk]g∣2
≤∣[xk]g∣ {(1 - e) ∣[xk]g∣ +αk∣[VfBk(xk)]g∣ - αkλ}
≤ ∣[xk]g ∣ {(1 - e) ∣[xk]g ∣ + αkL ∣[xk]g ∣ + αkλ - 2αkδ3 - αkλ}
=∣[xk]g∣ {(1-e+αkL)∣[xk]g∣ -2αkδ3}
≤ k[xk]g k [(1 - e + αkL);	k 3 - - 2akδ3]
1 - e + αkL
= ∣[xk]g ∣ (2αkδ3 - 2αkδ3) = 0.
which shows that [xk - αkVΨBk (xk)]g> [xk]g ≤ e ∣[xk]g∣2. Hence the group projection operator
is trigerred on g to map the variables to zero, then g ∈ I0(xk+1), i.e., [xk+1]g = 0. Therefore, the
group sparsity of x* can be successfully identified by Half-Space Step, i.e., 10(x*) ⊆ 10(xk+1).
C.4 UPPER BOUND OF NP UNDER STRONGLY CONVEXITY
Proposition 2. Suppose the following conditions hold:
•	(A1) E[VfBk (x)] = Vf(x).
•	(A2) there exists a σ > 0 such that EB [∣VfB (x) - Vf (x)∣2] ≤ σ2 for any mini-batch B.
•	(A3) there exists a β ∈ (0,1) such that 0 < ak < 1-Lβ.
•	(A4) f is μ-strongly convex.
Set the step-size ak = 2^, k0 = dmaχ{1, 21^}]. For any T ∈ (0,1), there exists a NP ∈ Z+ such
that NP ≥ [max { 8k0E[kR0τ-x k ], 8σμlo∣(NP7-1) }], such that performing Prox-SG NP times
yields
∣xNP - x* ∣ ≤ R/2	(87)
with probability at least 1 - τ.
22
Under review as a conference paper at ICLR 2021
Proof. By the conditions (A1, A2, A3), Assumption 3.1 and Theorem 3.2 in Rosasco et al. (2019),
we have for any k ≥ 2,
E[kxk — x*k2] ≤ E[kxko — x*k2]	+ ⅛log(kk- 1)
μ β k
(88)
Let E[kxko — x*k2 ] = Sko. For any τ ∈ (0, 1), there exists a NP ∈ Z+ satisfying
NP ≥
8k0Sk0	8σ2 log(NP — 1)
max[ ~iτΓ^ —μ2β 2R2τ一
(89)
we have
R2τ
E[kxNp — x*k2] ≤ 丁.
(90)
Therefore, by Markov inequality, we have that
IIxNP — χ"∣∣2 ≤ ɪ ⇔ IIxNP - χ*k ≤ ɪ	(91)
holds with probability at least 1— τ .
□
D	Additional Numerical Experiments
In this section, we provide additional numerical experiments to (i) demonstrate the validness of
group sparsity identification of HSPG; (ii) provide comprehensive comparison to Prox-SG, RDA and
Prox-SVRG on benchmark convex problems; and (iii) describe more details regarding our non-convex
deep learning experiments shown in the main body.
D.1 Linear Regression on Synthetic Data
We first numerically validate the proposed HSPG on group sparsity identification by linear regression
problems with `1 /`2 regularizations using synthetic data. Consider a data matrix A ∈ RN ×n
consisting of N instances and the target variable y ∈ RN, we are interested in the following problem:
minimize U kAx — yk2 + λ X k[x]g Il.	(92)
x∈Rn 2N
g∈G
Our goal is to empirically show that HSPG is able to identify the ground truth zero groups with
synthetic data. We conduct the experiments as follows: (i) generate the data matrix A whose elements
are uniformly distributed among [—1,1]; (ii) generate a vector x* working as the ground truth
solution, where the elements are uniformly distributed among [—1, 1] and the coordinates are equally
divided into 10 groups (|G| = 10); (iii) randomly set a number of groups of x* to be 0 according to
a pre-specified group sparsity ratio; (iv) compute the target variable y = Ax*; (v) solve the above
problem (92) for x with A and y only, and then evaluate the Intersection over Union (IoU) with
respect to the identities of the zero groups between the computed solution estimate x by HSPG and
the ground truth x*.
We test HSPG on (92) under different problem settings. For a slim matrix A where N ≥ n, we
test with various group sparsity ratios among {0.1, 0.3, 0.5, 0.7, 0.9}, and for a fat matrix A where
N < n, we only test with a certain group sparsity value since a recovery of x* requires that the
number of non-zero elements in x* is bounded by N . Throughout the experiments, we set λ to be
100/N, the mini-batch size |B| to be 64, step size αk to be 0.1 (constant), and fine-tune per problem.
Based on a similar statistical test on objective function stationarity (Zhang et al., 2020), we switch
to Half-Space Step roughly after 30 epoches. Table 2 shows that under each setting, the proposed
HSPG correctly identifies the groups of zeros as indicated by IoU(x, x*) = 1.0, which is a strong
evidence to show the correctness of group sparsity identification of HSPG.
23
Under review as a conference paper at ICLR 2021
Table 2: Linear regression problem settings and IoU of the recovered solutions by HSPG.
	N	n	Group sparsity ratio of x*	IoU(X,x*)
	10000	1000	{0.1,0.3,0.5, 07, 0.9}	1.0
Slim A	10000	2000	{0.1, 0.3, 0.5, 0.7, 0.9}	1.0
	10000	3000	{0.1, 0.3, 0.5, 0.7, 0.9}	1.0
	10000	4000	{o.1,0.3,0.5, 0.7, 0.9}	1.0
	200	1000	09	1.0
Fat A	300	1000	0.8	1.0
	400	1000	0.7	1.0
	500	1000	0.6	1.0
D.2 Logistic Regression
We then focus on the benchmark convex logistic regression problem with the mixed `1 /`2-
regularization given N examples (di, lι),…，(dN, In) where di ∈ Rn and li ∈ {-1,1} with
the form
1N
minimize — X log(1 + 巳—叱小力 + 入 X ”乩 ∣∣ ,
(χm∈Rn+ι N M	g∈G
(93)
for binary classification with a bias b ∈ R. We set the regularization parameter λ as 100/N throughout
the experiments since it yields high sparse solutions and low object value f ’s, equally decompose the
variables into 10 groups to form G, and test problem (93) on 8 standard publicly available large-scale
datasets from LIBSVM repository (Chang & Lin, 2011) as summarized in Table 3. All convex
experiments are conducted on a 64-bit operating system with an Intel(R) Core(TM) i7-7700K CPU @
4.20 GHz and 32 GB random-access memory.
We run the solvers with a maximum number of epochs as 60. The mini-batch size |B| is set to
be min{256, d0.01N e} similarly to (Yang et al., 2019). The step size αk setting follows [Section
4](Xiao & Zhang, 2014). Particularly, we first compute a Lipschitz constant L as maxi kdi k2 /4, then
fine tune and select constant αk ≡ α = 1/L to Prox-SG and Prox-SVRG since it exhibits the best
results. For RDA, the step size parameter γ is fined tuned as the one with the best performance among
all powers of 10. For HSPG, we set αk as the same as Prox-SG and Prox-SVRG in practice. We
set NP as 30N/|B| such that Half-Space Step is triggered after employing Prox-SG Step 30 epochs
similarly to Appendix D.1, and the control parameter in (9) as 0.05. We select two ’s as 0 and 0.05.
The final objective value Ψ and f , and group sparsity in the solutions are reported in Table 4-6, where
we mark the best values as bold to facilitate the comparison. Furthermore, Figure 3 plots the relative
runtime of these solvers for each dataset, scaled by the runtime of the most time-consuming solver.
Table 6 shows that our HSPG is definitely the best solver on exploring the group sparsity of the
solutions. In fact, HSPG under = 0.05 performs all the best except ijcnn1. Prox-SVRG is the
second best solver on group sparsity exploration, which demonstrates that the variance reduction
techniques works well in convex setting to promote sparsity, but not in non-convex settings. HSPG
under = 0 performs much better than Prox-SG which matches the better sparsity recovery property
of HSPG as stated in Theorem 2 even under as 0. Moreover, as shown in Table 4 and 5, we
observe that all solvers perform quite competitively in terms of final objective values (round up to 3
decimals) except RDA, which demonstrates that HSPG reaches comparable convergence as Prox-SG
and Prox-SVRG in practice. Finally, Figure 3 indicates that Prox-SG, RDA and HSPG have similar
computational cost to proceed, except Prox-SVRG due to its periodical full gradient computation.
Table 3: Summary of datasets.
Dataset	N	n	Attribute	Dataset	N	n	Attribute
a9a	32561	123	binary {0, 1}	news20	19996	1355191	unit-length
higgs	11000000	28	real [-3, 41]	real-sim	72309	20958	real [0, 1]
ijcnn1	49990	22	real [-1, 1]	UrLcombined	2396130	3231961	real [-4, 9]
kdda	8407752	20216830	real [-1, 4]	w8a	49749	300	binary {0, 1}
24
Under review as a conference paper at ICLR 2021
Table 4: Final objective values Ψ for tested algorithms on convex problems.
Dataset	Prox-SG	RDA	Prox-SVRG	HSPG	
				as 0	as 0.05
a9a	0.355	0.359	0.355	0.355	0.355
higgs	0.357	0.360	0.365	0.358	0.358
ijcnn1	0.248	0.278	0.248	0.248	0.248
kdda	0.103	0.124	0.103	0.103	0.103
news20	0.538	0.693	0.538	0.538	0.538
real-sim	0.242	0.666	0.244	0.242	0.242
UrLCombined	0.397	0.579	0.391	0.405	0.405
w8a	0.110	0.111	0.112	0.110	0.110
Table 5: Final objective values f for tested algorithms on convex problems.
Dataset	Prox-SG	RDA	Prox-SVRG	HSPG	
				as 0	as 0.05
a9a	0.329	0.338	0.329	0.329	0.329
higgs	0.357	0.360	0.365	0.358	0.358
ijCnn1	0.213	0.270	0.213	0.213	0.214
kdda	0.103	0.124	0.103	0.103	0.103
news20	0.373	0.693	0.381	0.372	0.372
real-sim	0.148	0.665	0.159	0.148	0.148
UrLCombined	0.397	0.579	0.391	0.405	0.405
w8a	0.089	0.098	0.091	0.089	0.089
Table 6: Group sparsity for tested algorithms on convex problems.
Dataset	Prox-SG	RDA	Prox-SVRG	HSPG		
				as 0	e as 0.05
a9a	20%	30%	30%	30%	30%
higgs	0%	10%	0%	0%	30%
ijCnn1	50%	70%	60%	60%	60%
kdda	0%	0%	0%	0%	80%
news20	20%	80%	90%	80%	90%
real-sim	0%	0%	80%	0%	80%
UrLCombined	0%	0%	0%	0%	90%
w8a	0%	0%	0%	0%	0%
1.0
0.5
。一π-un 七0 A--。七
0.0
a9a higgs ijcnn1 kdda news20 real-sim url w8a
Figure 3: Relative runtime.
D.3 Deep Learning Experiments
We conduct all deep learning experiments on one GeForce GTX 1080 Ti GPU, and describe how to
fine-tune the control parameter in (9) in details.
Fine-tune λ: The λ balances the sparsity level of the exact optimal solution and the bias of model
estimation. Larger λ encourages higher sparsity but may hurt the objective function. Therefore, in
order to obtain solution of both high group sparsity and low objective value, we need to fine-tune λ
carefully. In our experiments, we iterate λ through all powers of 10 from 10-5 to 10-1 by proceeding
25
Under review as a conference paper at ICLR 2021
Figure 4: Evolution of f value on ResNet18 with CIFAR10.
Prox-SG, and pick up the largest λ which has the same level of test accuracy to the model trained
without any regularization.
Fine-tune : According to Theorem 2, a larger results in a faster group sparsity identification,
while by Lemma 1 on the other hand too large may cause a significant regression on the target
objective Ψ value, i.e., the Ψ value increases a lot. Hence, in our experiments, from the point of
view of optimization, we search a proper in the following ways: start from = 0.0 and the models
trained by employing NP Prox-SG Steps, incrementally increase by 0.01 and check if the Ψ on the
first Half-Space Step has an obvious increase, then accept the largest without regression on Ψ as our
fine tuned shown in the main body of the paper. Particularly, the fine tuned ’s equal to 0.03, 0.05,
0.02 and 0.02 for VGG16 with CIFAR10, VGG16 with Fashion-MNIST, ResNet18 with CIFAR10
and ResNet18 with Fashion-MNIST respectively. Note from the perspective of different applications,
there are different criterions to fine tune , i.e., for model compression, we may accept based on the
validation accuracy regression to reach higher group sparsity.
Final f comparison: Additionally, we also report the final f comparison in Table 7 and its evolution
on ResNet18 with CIFAR10 in Figure 4, where we can see that all tested algorithms can achieve
competitive f values as they do in convex settings. And the evolution of f is similar to that of Ψ, i.e.,
the raw objective f generally monotonically decreases for small = 0 to 0.02, and experiences a
mild pulse after switch to Half-Space Step for larger , e.g., 0.05, which matches Lemma 1.
Table 7: Final objective values f for tested algorithms on non-convex problems.
Backbone	Dataset	Prox-SG	Prox-SVRG	HSPG		
				e as 0	fine tuned
VGG16	CIFAR10	0.010	0.036	0.010	0.009
	Fashion-MNIST	0.181	0.165	0.181	0.182
ResNet18	CIFAR10	0.001	-	0.002	0.001	一^0.004
	Fashion-MNIST	0.006	0.008	0.005	0.010
MobileNetV1	CIFAR10	丽21	-	0.031	0.021	一^0.031
	Fashion-MNIST	0.074	0.057	0.074	0.088
26