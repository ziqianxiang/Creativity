Under review as a conference paper at ICLR 2021
ADIS-GAN: Affine Disentangled GAN
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes Affine Disentangled GAN (ADIS-GAN), which is a Gener-
ative Adversarial Network that can explicitly disentangle affine transformations
in a self-supervised and rigorous manner. The objective is inspired by InfoGAN,
where an additional affine regularizer acts as the inductive bias. The affine regu-
larizer is rooted in the affine transformation properties of images, changing some
properties of the underlying images, while leaving all other properties invariant.
We derive the affine regularizer by decomposing the affine matrix into separate
transformation matrices and inferring the transformation parameters by maximum
likelihood estimation. Unlike the disentangled representations learned by exist-
ing approaches, the features learned by ADIS-GAN are axis-aligned and scalable,
where transformations such as rotation, horizontal and vertical zoom, horizontal
and vertical skew, horizontal and vertical translation can be explicitly selected
and learned. ADIS-GAN successfully disentangles these features on the MNIST,
CelebA, and dSprites datasets.
1	Introduction
In a disentangled representation, observations are interpreted in terms of a few explanatory factors.
Examples of such factors are the rotation angle(s), scale, or position ofan object in an image. Disen-
tangled variables are generally considered as the abstraction of interpretable semantic information
and reflection of separatable factors of variation in the data. Many studies have explored the ef-
fectiveness of disentangled representations (Bengio et al., 2013; N et al., 2017; LeCun et al., 2015;
Lake et al., 2017; Tschannen et al., 2018). The information presented in observations is encoded
in an interpretable and compact manner, e.g., the texture style and the orientation of the objects
(Bengio et al., 2013; LeCun et al., 2015; Lake et al., 2017; Tschannen et al., 2018). The learned
representation are more generalizable and can be useful for downstream tasks, such as classification
and visualization (Bengio et al., 2013; N et al., 2017; Chen et al., 2016).
The concept of disentangled representation has been defined in several ways in the literature (Lo-
catello et al., 2019; Higgins et al., 2018; Eastwood & Williams, 2018). The necessity of explicit
inductive biases both for learning approaches and the datasets is discussed in Locatello et al. (2019).
Inductive bias refers to the set of assumptions that the learner uses to predict outputs of given inputs
that it has not encountered. For instance, in the dSprites dataset objects are displayed at different
angles and positions; such prior knowledge helps to detect and classify the objects. However, the
inductive biases in existing deep learning models are mostly implicit. The proposed ADIS-GAN
utilizes relative affine transformations (see Section 4) as the explicit inductive bias, leading to axis-
aligned and scalable disentangled representations.
Axis-alignment: The issue of axis-alignment is addressed in Higgins et al. (2018), where each
latent dimension should have a pre-defined unique axis-alignment. Without axis-alignment, the
features learned by disentangled representation need to be identified with expert knowledge after
the training, which could be a cumbersome process when dealing with a large number of features.
The axis-alignment property also helps to discover desired but non-dominant attributes (e.g., roll
angle of face in CelebA dataset).
Scalability: The scalability property allows us to make a trade-off between the compactness and
expressivity of the disentangled representation. For example, the zoom attribute can be decomposed
as horizontal and vertical zoom. A more compact representation encodes the zoom attribute by
one latent dimension, while a more expressive representation decomposes the zoom attribute as
1
Under review as a conference paper at ICLR 2021
Figure 1: Disentangled representation of affine transformations on the MNIST dataset. c1: rotation,
c2 : horizontal zoom, c3 : vertical zoom, c4 : horizontal skew, c5 : vertical skew, c6 : horizontal trans-
lation, c7 : vertical translation. To the best of our knowledge, ADIS-GAN is the first algorithm that
can disentangle an entire affine transformation in a self-supervised manner.
horizontal and vertical zoom, encoded by two latent dimensions. The scalability property provides
a solution to an open question related to disentangled representations: Should we capture a data
generative factor (e.g., zoom) by one or multiple latent dimensions? Many disentanglement metrics
Higgins et al. (2017); Chen et al. (2018); Kim & Mnih (2018); Eastwood & Williams (2018) rely
on a single latent dimension, while others Denton & Birodkar (2017); Ridgeway & Mozer (2018);
Higgins et al. (2018) encode latent factors via multiple latent dimensions.
We motivate the importance of axis-alignment and scalability in particular for affine transformations
(see Figure 1), where disentangling object poses from texture and shape is an attractive property of
an algorithm in the imaging domain (Jaderberg et al., 2015; Bepler et al., 2019; Engstrom et al.,
2019). In supervised learning tasks, Spatial Transformer Network Jaderberg et al. (2015) can ac-
tively spatially transform an image by providing a proper affine transformation matrix. In unsuper-
vised learning tasks, few algorithms have successfully disentangled the affine transformation. In
Bepler et al. (2019), an algorithm is introduced that disentangles rotation and translation but not an
entire affine transformation.
We propose ADIS-GAN, which is a Generative Adversarial Network that utilizes the affine regu-
larizer (see Section 4) as an inductive bias to explicitly disentangle the affine transformation. The
affine regularizer is rooted in the affine transformation properties of the images, that affect certain
properties of the underlying images, while leaving all other properties invariant. We derive the affine
regularizer by decomposing the affine matrix into separate transformations and inferring the trans-
formation parameters by maximum likelihood estimation. Unlike the disentangled representations
learned by existing approaches, the features learned by ADIS-GAN are axis-aligned and scalable,
where transformations such as rotation, horizontal and vertical zoom, horizontal and vertical skew,
horizontal and vertical translation can be explicitly selected and learned (see Figure 1).
In the remainder of the paper, we review related work in Section 2. We then compare the difference
between GAN, InfoGAN and the proposed method in Section 3. We introduce the ADIS-GAN in
Section 4, while in Section 5, we show numerical results showing the axis-aligned and scalable
disentangled representation learned by ADIS-GAN. We offer concluding remarks in Section 6.
Our contributions:
1.	To the best of our knowledge, ADIS-GAN is the first algorithm that can disentangle an
entire affine transformation, including rotation, horizontal and vertical zoom, horizontal
and vertical skew, horizontal and vertical translation in a self-supervised manner.
2.	The disentangled representations obtained by ADIS-GAN are axis-aligned. The advantages
are two-folds: a. The attributes to be learned can be pre-defined, which saves the effort to
identify the attributes after the training. b. Desired but non-dominant attributes can be
learned, in parallel with the dominant attributes.
3.	The disentangled representations obtained by ADIS-GAN are scalable. The scalability
property makes it possible to make a trade-off between the compactness and expressivity
of the learned representation.
2
Under review as a conference paper at ICLR 2021
2	Related literature
Recent approaches learning disentangled representations are largely based on Variational Autoen-
coders (VAEs) Kingma & Welling (2013) and InfoGAN Chen et al. (2016). In a standard VAE,
observation x is encoded to latent representation z from variational distribution Q(z|x) using a deep
neural network. In the generative process, the latent variable z is sampled from a prior distribution
P(z) and uses another deep neural network to reconstruct the observation x from conditional distri-
bution P(x|z). To achieve the disentanglement, a factorized aggregated posterior x Q(z|x)P (x)dx
is encouraged during the training. In a standard GAN Goodfellow et al. (2014), latent representation
z is sampled from a prior distribution P (z). The fake data xfake is generated by z from distribution
P (x|z). A discriminator is introduced to encourage the generated data xfake to be close to the ob-
servation xreal. To achieve disentanglement, InfoGAN Chen et al. (2016) proposes to maximize the
mutual information between a subset c of latent representation z and the generated data xfake.
Recently, much attention has been paid to regularizers that promote disentanglement. The β-VAE
Higgins et al. (2017) encourages the disentanglement by increasing the weight of the KL regular-
izer, thus promoting the factorization of the posterior Q(z|x). Both FactorVAE Kim & Mnih (2018)
and β-TCVAE Chen et al. (2018) penalize the total correlation, while the former relies on adversar-
ial training and the latter directly calculates the total correlation through the decomposition of the
β-VAE objective function. The HFVAE Esmaeili et al. (2019) proposes a two-level hierarchical ob-
jective to control relative degree of statistical independence. In the ChyVAE Ansari & Soh (2019),
an inverse-Wishart (IW) prior on the covariance matrix of the latent code is augmented to promote
statistical independence. The DIP-VAE Kumar et al. (2018) penalizes the difference between the
aggregated posterior and a factorized prior. In the AnnealedVAE Huang et al. (2018), the encoder
can concentrate on learning individual factors and variation by gradually increasing the bottleneck
capacity. The IB-GAN Jeon et al. (2019) is an extension to InfoGAN rooted in information bot-
tleneck theory, which includes a mutual information upper bound and forms a mutual information
bottleneck. The InfoGAN-CR Lin et al. (2019) adds a contrastive regularizer on top on InfoGAN,
that compares the changes between the image and latent space.
Even though the aforementioned methods have achieved better disentanglement performance com-
pared to the baseline established by VAE and InfoGAN, none of them have successfully disentangled
an entire affine transformations in a scalable and axis-aligned way, which is a desirable property in
the imaging domain. Moreover, our affine regularizer is orthogonal to those approaches, thus makes
it possible to integrate our affine regularizer with other methods.
In Gidaris et al. (2018); Chen et al. (2019); Wang et al. (2020); Zhang et al. (2019), self-
supervised regularization is applied, where they compare the difference of images before and after
the affine/Projective transformation. The transformation loss is define as: L = ||M(θ0) - M(θ)∣∣2
and a parameterized matrix M ∈ R3×3 . We make a more specific assumption about the matrix and
further decomPose it to achieve disentanglement (see Section 4).
3	Background: GAN, InfoGAN and ADIS-GAN
In a standard GAN Goodfellow et al. (2014), latent rePresentation z is samPled from a Prior dis-
tribution P(z). The fake data xfake is generated from z from conditional distribution P(x|z). A
discriminator is introduced to encourage the generated data xfake to be close to the observation xreal:
min max V (D, G) = Ladv(D, G).	(1)
To achieve disentanglement, InfoGAN Chen et al. (2016) maximizes the mutual information between
a subset c of latent rePresentation z and the generated data xfake:
min max V (D, G) = Ladv(D, G) - λI(c0; xfake).	(2)
To exPlicitly disentangle affine transformation attributes in a scalable and axis-aligned manner,
ADIS-GAN has one additional term called affine regularizer on toP of InfoGAN:
min max V (D, G) = Ladv(D, G) - λI(c0; xfake) - βLaffine.	(3)
3
Under review as a conference paper at ICLR 2021
4	Affine regularizer
Compared to InfoGAN, the major difference of ADIS-GAN is the affine regularization loss Laffine
(see Figure 2). To calculate Laffine, three modifications to the network have been made. 1: Besides
generating xfake and calculating mutual information loss I(c0 ; xfake) like InfoGAN, the semantic
latent vector c of ADIS-GAN has an additional task: to generate random affine transformation pa-
rameters. We now rename the latent vector c as ctrreaanlsform to describe the state before and after the
affine transformation, where superscript “real” stands for the initial state before the transformation
and subscript “transform” stands for the final state after the transformation, more details regarding
the transformation states will be introduced in Section 4.2. ctrreaanlsform is sampled from a uniform distri-
bution, then converted to an affine matrix Mtrreaanlsform using Flow 1 (see Figure 4). 2: Affine transfor-
mation augmented image xtransform is introduced. xtransform is obtained by multiplying Mtrreaanlsform and
xreal sampled from training data (see Figure 3). Unlike InfoGAN where “xreal” is the positive sample
fed to the discriminator. In ADIS-GAN,“xtransform” is the positive sample fed to the discriminator.
This guarantees the affine transformations are observed by the network. 3: Affine reugularization
loss Laffine is calculated by comparing the ground truth affine transformation latent vector ctrreaanlsform
and the reconstructed affine transformation latent vector ctrreaanl0sform, which is obtained by calculating
the relative difference between xreal and its affine augmented pair xtransform (see Figure 3).
Figure 2: The main framework of ADIS-GAN. D stands for discriminator, E stands for encoder, G
stands for generator. xfake is the generated image, xreal is the image sampled from training dataset,
xtransform is the affine transformed image. The generation process of xtransform is demonstrated in
Figure 3. z is latent noise sampled from normal distribution. ctrreaanlsform is the semantic latent vectors
sampled from uniform distribution. ctrreaanlsform has two usages: 1. It is used to generate xfake and
compute mutual information loss. 2. It is used to generate xtransform together with xreal and compute
affine regularization loss (see Figure 3). The meaning of the superscript “real” refers to the initial
state before the transformation and subscript “transform” refers to the final state after the transfor-
mation. I(ci0nfo; xfake) is the mutual informaion loss. Ladv(D, G) is the GAN loss. Laffine is the affine
regularization loss. Figure 3 illustrates how to calculate the affine regularization loss Laffine.
The working principle of affine regularizer is illustrated in Figure 3. First, the latent vector ctrreaanlsform
is sampled from a uniform distribution. It is converted to an affine matrix Mtrreaanlsform through “Flow
1” (see Section 4.1.1). Next, the real image “xreal” sampled from training data is affine transformed
with Mtrreaanlsform to obtain xtransform. Both xreal and its transformed pair xtransform are encoded to latent
vectors crbeaaslis and ctbraasnissform respectively. The encoded latent vectors crbeaaslis and ctbraasnissform are further con-
verted to affine matrix Mrbeaaslis and Mtbraansissform through “Flow 1” (see Section 4.1.1). Then the relative
transformation matrix Mtrreaanls0form is computed using both Mrbeaaslis and Mtbraansissform (see Section 4.2 Equa-
tion 9). Finally, the reconstructed latent vector ctrreaanl0sform is obtained using Mtrreaanls0form through “Flow
2” (see Section 4.1.2). Here we use rotation (θ), horizontal and vertical zoom (p, q), horizontal and
vertical translation (x, y) as an example (more affine parameter combinations can be found in Ap-
pendix H). The affine regularizer loss is Laffine = Pi ||c(i)trreaanlsform -c(i)trreaanl0sform||22, i ∈ {θ,p,q,x,y}.
You may refer to Algorithm 1 and Figure 3 for more details.
4
Under review as a conference paper at ICLR 2021
Mensform Flοw2
Figure 3: Affine regularizer diagram. Inputs: ctrreaanlsform sampled from Unif(-1,1) and xreal sam-
pled from training data. Output: xtransform and ctrreaanl0sform. The affine regularizer loss is: Laffine =
Pi ||c(i)trreaanlsform - c(i)trreaanl0sform||22,i ∈ {θ,p,q,x,y}. θ: rotation, p: horizontal zoom, q: vertical
zoom, x: horizontal translation, y : vertical translation. E stands for encoder, Flow 1 and Flow 2 are
defined in Section 4.1 and Figure 4. Relative transform is defined in Section 4.2 Equation 9.
Icreal0
ICtransforml
Algorithm 1: Affine Regularizer
Input: Sampled images: x3], latent vectors:。⑴第也皿 〜Unif(-1,1), i ∈ {θ,p, q, x, y}
Output: Laffine
Matrix from sampled latent vector: Mtrreaanlsform = Flow 1(c(i)trreaanlsform), i ∈ {θ, p, q, x, y}
Affine transformation: xtransform = Mtrreaanlsform × xreal
Latent vectors encoded from images: ctbraasnissform = Encoder(xtransform), crbeaaslis = Encoder(xreal)
Matrices from encoded latent vector: M basisf = Flow 1(c(i)basisf ), M baslis =
transform	transform real
Flow 1(c(i)rbeaaslis), i ∈ {θ, p, q, x, y}
Relative transform matrix: Mtrreaanls0form = Mtbraansissform × Mrbeaaslis-1
Reconstructed latent vectors: c(i)trreaanlsform = Flow 2(Mtrreaanlsform)
Affine reuglarizer loss: Laffine = Pi ||c(i)trreaanlsform - c(i)trreaanlsform||22, i ∈ {θ,p,q,x,y}
Figure 4: Conversion between latent vectors and affine matrix. Flow 1: convert the latent vectors c to
affine matrix M. Flow 2: convert the affine matrix Mto latent vectors c. c stands for semantic latent
vectors. C stands for affine parameters such as rotation (θ), zoom (p, q) and translation (x, y). M
stands for affine matrix. “NormLA” and “M Initialization” are described in Section 4.1.1. “NormAL”
and “MLE” are described in Section 4.1.2.
4.1	Conversion between latent vectors and affine matrix
4.1.1	Latent vectors to affine matrix: Flow 1
To conduct affine transformation and calculate the relative difference, we need to convert latent
vector C to an affine matrix M. The “Flow 1” in Figure 4 illustrates the process.
NormLA (Latent to Affine): First, we need to normalize the latent vector C to the affine trans-
formation parameter C. For instance, if we set the affine transformation range as rotation θ ∈
[-∏∕10, ∏∕l0], horizontal and vertical zoom p,q ∈ [0.8,1.2], horizontal and vertical translation
x, y ∈ [-0.2, 0.2], and a 5-d latent vector C is sampled from uniform distribution Unif(-1, 1).
NormLA (Latent to Affine) refers to the following normalization: C(θ) = c(θ) X (n/10), C(p)=
c(p) × 0.2 + 1, C(q) = c(q) × 0.2 + 1, C(x) = c(x) × 0.2, C(y) = c(y) × 0.2.
5
Under review as a conference paper at ICLR 2021
M Initialization: Next, the affine parameter c needs to be properly arranged in the affine matrix M.
By default, the affine matrix M is organized as in Equation 4.
M=
A11	A12 A13
A21	A22 A23
001
cos θ
= sin θ
0
- sin θ 0
cos θ 0
01
p00
0q0
001
10x
01y
001
(4)
4.1.2	affine matrix to latent vectors: flow 2
To obtain the reconstructed affine transformation latent vector “ctrreaanl0sform”, we need to convert the
affine matrix to latent vector. The “Flow 2” in Figure 4 illustrates the process.
MLE (Maximum Likelihood Estimation): First, We need to calculate the affine parameter c using
maximum likelihood estimation (see Appendix A).
'θ
P
C(i) = < q
X
y
arctan
2(AII A21-A12A2 2 )
A11+A22-A22-A21，
A11 cosθ + A21 sinθ,
= -A12 sinθ + A21 sin θ,
— A13 Cos θ+A23 Sin θ
p	p	,
— —A13 sin θ+A23 Cos θ
=	q	,
(5)
Where i ∈ θ, p, q, x,y .
NormAL (Affine to Latent): Next, We need to normalize the affine parameter c to latent vector c.
If We apply the same affine transformation setting as mentioned in Section 4.1.1 FloW 1. NormAL
(Affine to Latent) refers to the following normalization: c(θ) = c(θ) × (10∕π), c(p) = (C(P) -1) × 5,
c(q) = (c(q) — 1) × 5, c(x) = C(x) × 5, c(y) = c(y) × 5.
4.2	Relative affine transformation
Figure 5: Illustration of relative affine transformation. Solid line stands for the affine transformation
betWeen tWo real images. Dashed line stands for the affine transformation betWeen one real image
and the imaginary image basis.
We assume, Without loss of generality, that images can be expressed as the multiplication of the
affine transformation Mrbeaaslis ∈ R3×3 (affine matrix is a 3 × 3 matrix by default) and an image basis
xbasis (see Figure 5):
Mrbeaaslis × xbasis = xreal .	(6)
The superscript “basis” of Mrbeaaslis stands for the initial state and subscript “real” stands for the trans-
formed state of the image x. The image basis xbasis is the average manifold of all images Within the
same category. For instance, the digits “0” , “1”, ... “9” in MNIST are different categories. xbasis
does not refer to a particular image in the dataset and it is purely learned from the data.
6
Under review as a conference paper at ICLR 2021
If we purposely apply a known affine transformation Mtrreaanlsform on a sampled image xreal, we can
obtain an new image xtransform:
Mtransform × xreal = xtransform .	(7)
According to the definition in equation 6, the transformed image can also be expressed as the multi-
plication of the Mtbraansissform and the image base xbasis:
Mtransform × xbasis = xtransform .	(8)
By solving the simultaneous formula from equation 6 to 8:
M real	M basis	× Mbasis-1	(9)
Mtransform = Mtransform × Mreal .	(9)
Mtrreaanlsform is the relative affine transformation between affine matrices Mtbraansissform and Mrbeaaslis.
5	Numerical Results
The goal of the experiments in this section is to investigate, both qualitatively and quantitatively,
the disentangled representations obtained by ADIS-GAN. The axis-alignment of the disentangled
representations is demonstrated on CelebA dataset Liu et al. (2015), while the scalability of the
disentangled representation is illustrated on MNIST dataset. Next quantitative results for ADIS-
GAN are presented and compared to benchmarks on the dSprites Matthey et al. (2017) dataset. The
parameters of the affine transformation are selected as follows: rotation range: [-180, 18°], zoom
range: [0.8, 1.2], and translation range: [-0.2, 0.2]. For all the experiments we use Adam optimizer
with the learning rate of 0.0002 for discriminator and 0.0001 for generator and encoder. The batch
size is 128 for MNIST, 64 for dSprites, and 16 for CelebA. The regularization weight in Equation 3
is 1 for both λ and β . You may refer to Appendix G for network structure details.
5.1	Qualitative Results
The axis-alignment and scalability properties are achieved by selecting different matrix initializa-
tions and their corresponding maximum likelihood estimation. You may refer to Appendix I for
various ways of matrix initializations and their maximum likelihood estimation.
5.1.1	Axis-alignment
In CelebA dataset, typical dominant attributes are azimuth, sunglasses, emotion, etc. Existing meth-
ods Chen et al. (2016); Kim & Mnih (2018); Chen et al. (2018) have successfully disentangled those
attributes (see Appendix E). However, non-dominant attributes such as the roll, width and length
of the face and relative position of face in the frame are rarely tackled. Thanks to the axis-aligned
property, ADIS-GAN can explicitly learn those non-dominant attributes (see Figure 6, 7 and 8). It
is interesting to note that ADIS-GAN does not rigidly perform affine transformations on the given
images but understands the relationship between different facial components. For example, when
we try to shorten the length of the face (vertical zoom and translation, see left images on both Fig-
ure 7 and 8), ADIS-GAN knows how to complete the bottom of the images with a neck to make
the images look realistic. To disentangle facial attributes on the CelebA data, we choose to model
the latent space with one categorical code, CCat 〜Cat(K = 10,p = 0.1) and 5 continuous codes
Ccont 〜Unif(-1,1).
5.1.2	Scalability
Scalability refers to the flexibility to encode one attribute via one latent vector or, alternatively,
to decompose one attribute into sub-attributes and encode them by multiple latent vectors. For
example, both zoom and translation attributes can be encoded by one latent vector, or they can be
decomposed horizontally and vertically and encoded by two latent vectors (see Figure 9).
7
Under review as a conference paper at ICLR 2021
ι∩∏A‰aoa
l∩∩>0∩^∩A
RG0CvqnG
I	a. 一
Figure 6: Roll attribute on CelebA dataset.
Figure 7: Vertical (left) and horizontal (right) zoom attributes on CelebA dataset.
aððnee
qnc:Ge<τ
B1∩lfkrkGe
憎
。R。。Oai
ΠΠΠΠΠH
B阴器就朋a冏的船
EA0中
k qθ I
DilΩΩA 0。G 9 A
r⅛ inn,仅)0。6
Γ> InQAbQflCe___________
Γi MI口■仇)G分8C ]Q∏αCι ROG
Γi ]∏E⅞f⅝C£1Γ? ɑ O IRfXfi O1L Cl∏ ɑ
Figure 9: Illustration of scalability. case 1 (row 1 and 2): use two latent vectors to represent horizon-
tal and vertical direction separately. case 2 (row 3): use one latent vector to represent the horizontal
and vertical as a merged representation. You may refer to Appendix I for the derivation.
5.2	Quantitative Results
To disentangle object style on dSprites, we choose to model the latent space by one categorical code
CCat ~ Cat(K = 3,p = 0.33), and 4 continuous codes ccont ~ UnifGL1) that represent rotation,
zoom, horizontal and vertical translation. To the best of our knowledge, ADIS-GAN is the first
algorithm that can disentangle shape attributes with categorical codes on dSprites datasets. We have
observed that shape disentanglement is easier for larger images. We conducted experiments on the
8
Under review as a conference paper at ICLR 2021
Table 1: Comparison of the axis-alignment and scalability of the disentangled representation learned
by different approaches. ADIS-GAN is the only algorithm that generates disentangled representa-
tions with both useful properties.
Property	β-VAE	Factor -VAE	β-TCVAE	Annealed -VAE	Info -GAN	Info -GAN-CR	ADIS -GAN
Axis -alignment	no	no	no	no	no	no	yes
Scalability	no	no	no	no	no	no	yes
“Four Shapes” dataset (see Appendix D). Compared to dSprites, the major difference of the “Four
Shapes” dataset is that the images are larger. We found that ADIS-GAN can successfully disentan-
gle shape attribute on “Four Shapes” dataset with categorical codes (see Appendix D). The image
enlargement procedures and the disentanglement results of dSprites are described in Appendix C.
Table 2: Disentanglement scores on the dSprites dataset. The reference values are from Lin et al.
(2019) Table 1. Note that we do not apply model centrality here, which is a hyperparameter tuning
method, as our focus is on axis-alignment and scalability. InfoGAN (modified) uses spectrum nor-
malization Miyato et al. (2018) for the discriminator of InfoGAN. For the FactorVAE and BetaVAE
scores, we only use continuous codes. For the other scores, we use both continuous and categorical
codes. Overall, ADIS-GAN is comparable with the state-of-the-art disentanglement algorithms on
the dSprites dataset. We refer to Appendix F for the definations of disentanglement metrics.
	Model	FactorVAE	DCI	Explicitness Modularity		MIG	BetaVAE
	VAE	0.63 ± .06	0.30 ± .10			0.10	
	β-TCVAE	0.62 ± .07	0.29 ± .10			0.45	
	HFVAE	0.63 ± .08	0.39 ±.16				
	β-VAE	0.63 ± .10	0.41 ± .11		0.21		
	ChyVAE	0.77					
VAE	DIP-VAE		0.53				
	FactorVAE	0.82				0.15	
	FactorVAE (1.0)	0.79 ± .01	0.67 ± .03	0.78 ± .01	0.79 ± .01	0.27 ± .03	0.79 ± .02
	FactorVAE (10.0)	0.83 ± .01	0.70 ± .02	0.79 ± .00	0.79 ± .00	0.40 ± .01	0.83 ± .01
	FactorVAE (20.0)	0.83 ± .01	0.72 ± .02	0.79 ± .00	0.79 ± .01	0.40 ± .01	0.85 ± .00
	FactorVAE (40.0)	0.82 ± .01	0.74 ± .01	0.79 ± .00	0.77 ± .01	0.43 ± .01	0.84 ± .01
	InfoGAN	0.59 ± .70	0.41 ± .05		0.05		
	IB-GAN	0.80 ± .07	0.67 ± .07				
GAN	InfoGAN(modified)	0.82 ± .01	0.60 ± .02	0.82 ± .00	0.94 ± .01	0.22 ± .01	0.87 ± .01
	InfoGAN-CR	0.88 ± .01	0.71 ± .01	0.85 ± .00	0.96 ± .00	0.37 ± .01	0.95 ± .01
	ADIS-GAN	0.86 ± .02	0.71 ± .02	0.78 ± .00	0.95 ± .01	0.46 ± .01	0.89 ± .01
6	Conclusion
This paper introduces the Affine Disentangled GAN (ADIS-GAN) that can explicitly learn the affine
transformations via disentangled representations. In contrast to earlier approaches to disentangle-
ment, where inductive biases are not explicit, the disentangled representation obtained by ADIS-
GAN is axis-aligned and scalable. The affine regularizer introduced in this paper can be applied
to other methods such as the VAE family Kingma & Welling (2013); Higgins et al. (2017); Kim &
Mnih (2018); Chen et al. (2018).
Another attractive property of the affine regularizer is that it supports the possibility to utilize expert
knowledge as the inductive bias, as it is model based and exploits maximum likelihood estimation.
For example, we can disentangle the 2D affine transformations with the decomposed relationships.
Task-specific explicit regularizers may provide an alternative pathway for disentanglement com-
pared to existing general implicit regularizers.
9
Under review as a conference paper at ICLR 2021
References
Abdul Fatir Ansari and Harold Soh. Hyperprior induced unsupervised disentanglement of latent
representations. In AAAI Conference on Artificial Intelligence, 2019.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Trans. Pattern Anal. Mach. Intell., 35(8):1798-1828, August 2013. ISSN
0162-8828. doi: 10.1109/TPAMI.2013.50.
Tristan Bepler, Ellen Zhong, Kotaro Kelley, Edward Brignole, and Bonnie Berger. Explicitly
disentangling image content from translation and rotation with spatial-vae. In H. Wallach,
H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 32, pp. 15435-15445. Curran Associates, Inc., 2019.
Ricky T. Q. Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of dis-
entanglement in variational autoencoders. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 2610-2620. Curran Associates, Inc., 2018.
Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via
auxiliary rotation loss. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR
2019, Long Beach, CA, USA, June 16-20, 2019, pp. 12154-12163. Computer Vision Foundation
/ IEEE, 2019. doi: 10.1109/CVPR.2019.01243.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan:
Interpretable representation learning by information maximizing generative adversarial nets. In
D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 29, pp. 2172-2180. Curran Associates, Inc., 2016.
Emily L Denton and vighnesh Birodkar. Unsupervised learning of disentangled representations
from video. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 4414-4423. Curran
Associates, Inc., 2017.
Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Explor-
ing the landscape of spatial robustness. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceed-
ings of Machine Learning Research, pp. 1802-1811, Long Beach, California, USA, 09-15 Jun
2019. PMLR.
Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, N Siddharth, Brooks Paige, Dana H.
Brooks, Jennifer Dy, and Jan-Willem van de Meent. Structured disentangled representations.
volume 89 of Proceedings of Machine Learning Research, pp. 2525-2534. PMLR, 16-18 Apr
2019.
Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by
predicting image rotations. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenRe-
view.net, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling,
C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.), Advances in Neural Information Pro-
cessing Systems 27, pp. 2672-2680. Curran Associates, Inc., 2014.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In ICLR. OpenReview.net, 2017.
10
Under review as a conference paper at ICLR 2021
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Lolc Matthey, Danilo J. Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations.	CoRR,
abs/1812.02230, 2018.
Chin-Wei Huang, Shawn Tan, Alexandre Lacoste, and Aaron C Courville. Improving explorabil-
ity in variational inference with annealed variational objectives. In S. Bengio, H. Wallach,
H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 31, pp. 9701-9711. Curran Associates, Inc., 2018.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, and koray kavukcuoglu. Spatial transformer
networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett (eds.), Advances
in Neural Information Processing Systems 28, pp. 2017-2025. Curran Associates, Inc., 2015.
Insu Jeon, Wonkwang Lee, and Gunhee Kim. IB-GAN: Disentangled representation learning with
information bottleneck GAN, 2019.
Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Jennifer Dy and Andreas Krause
(eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Pro-
CeedingS of Machine Learning Research, pp. 2649-2658, Stockholmsmassan, Stockholm Swe-
den, 10-15 Jul 2018. PMLR.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes, 2013.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. VARIATIONAL INFERENCE OF
DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS. In Interna-
tional Conference on Learning Representations, 2018.
Brenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building
machines that learn and think like people. Behavioral and Brain Sciences, 40:e253, 2017. doi:
10.1017/S0140525X16001837.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015. doi: 10.1038/nature14539.
Zinan Lin, Kiran Koshy Thekumparampil, Giulia C. Fanti, and Sewoong Oh. Infogan-cr: Disen-
tangling generative adversarial networks with contrastive regularizers. CoRR, abs/1906.06034,
2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proceedings of International Conference on Computer Vision (ICCV), December 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Ratsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Frederic Bachem. Challenging common assumptions in the unsuper-
vised learning of disentangled representations. In International Conference on Machine Learning,
2019. Best Paper Award.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Siddharth N, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison, Noah Goodman, Pushmeet
Kohli, Frank Wood, and Philip Torr. Learning disentangled representations with semi-supervised
deep generative models. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
5925-5935. Curran Associates, Inc., 2017.
Karl Ridgeway and Michael C Mozer. Learning deep disentangled embeddings with the f-statistic
loss. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett
(eds.), Advances in Neural Information Processing Systems 31, pp. 185-194. Curran Associates,
Inc., 2018.
11
Under review as a conference paper at ICLR 2021
Michael Tschannen, Olivier Bachem, and Mario Lucic. Recent advances in autoencoder-based
representation learning. CoRR, abs/1812.05069, 2018.
Jiayu Wang, Wengang Zhou, Guo-Jun Qi, Zhongqian Fu, Qi Tian, and Houqiang Li. Transforma-
tion gan for unsupervised image synthesis and representation learning. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
Liheng Zhang, Guo-Jun Qi, Liqiang Wang, and Jiebo Luo. AET vs. AED: unsupervised repre-
sentation learning by auto-encoding transformations rather than data. In IEEE Conference on
Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019.
Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00265.
12