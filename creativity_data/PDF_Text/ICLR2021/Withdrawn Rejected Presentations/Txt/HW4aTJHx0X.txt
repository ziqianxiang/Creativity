Under review as a conference paper at ICLR 2021
What’snew?Summarizing Contributions in
S cientific Literature
Anonymous authors
Paper under double-blind review
Abstract
With thousands of academic articles shared on a daily basis, it has become in-
creasingly difficult to keep up with the latest scientific findings. To overcome this
problem, we introduce a new task of disentangled paper summarization, which
seeks to generate separate summaries for the paper contributions and the context
of the work, making it easier to identify the key findings shared in articles. For
this purpose, we extend the S2ORC corpus of academic articles, which spans a di-
verse set of domains ranging from economics to psychology, by adding disentan-
gled “contribution” and “context” reference labels. Together with the dataset, we
introduce and analyze three baseline approaches: 1) a unified model controlled by
input code prefixes, 2) a model with separate generation heads specialized in gen-
erating the disentangled outputs, and 3) a training strategy that guides the model
using additional supervision coming from inbound and outbound citations. We
also propose a comprehensive automatic evaluation protocol which reports the
relevance, novelty, and disentanglement of generated outputs. Through a human
study involving expert annotators, we show that in 79%, of cases our new task is
considered more helpful than traditional scientific paper summarization.
1	Introduction
With the growing popularity of open-access academic article repositories, such as arXiv or bioRxiv,
disseminating new research findings has become nearly effortless. Through such services, tens of
thousands of scientific papers are shared by the research community every month1. At the same
time, the unreviewed nature of mentioned repositories and the sheer volume of new publications has
made it nearly impossible to identify relevant work and keep up with the latest findings.
Scientific paper summarization, a subtask within automatic text summarization, aims to assist re-
searchers in their work by automatically condensing articles into a short, human-readable form that
contains only the most essential information. In recent years, abstractive summarization, an ap-
proach where models are trained to generate fluent summaries by paraphrasing the source article,
has seen impressive progress. State-of-the-art methods leverage large, pre-trained models (Raffel
et al., 2019; Lewis et al., 2020), define task-specific pre-training strategies (Zhang et al., 2019), and
scale to long input sequences (Zhao et al., 2020; Zaheer et al., 2020). Available large-scale bench-
mark datasets, such as arXiv and PubMed (Cohan et al., 2018), were automatically collected from
online archives and repurpose paper abstracts as reference summaries. However, the current form of
scientific paper summarization where models are trained to generate paper abstracts has two caveats:
1) often, abstracts contain information which is not of primary importance, 2) the vast majority of
scientific articles come with human-written abstracts, making the generated summaries superfluous.
To address these shortcomings, we introduce the task of disentangled paper summarization. The new
task’s goal is to generate two summaries simultaneously, one strictly focused on the summarized ar-
ticle’s novelties and contributions, the other introducing the context of the work and previous efforts.
In this form, the generated summaries can target the needs of diverse audiences: senior researchers
and field-experts who can benefit from reading the summarized contributions, and newcomers who
can quickly get up to speed with the intricacies of the addressed problems by reading the context
summary and get a perspective of the latest findings from the contribution summary.
1https://arxiv.org/stats/monthly_submissions
1
Under review as a conference paper at ICLR 2021
For this task, we introduce a new large-scale dataset by extending the S2ORC (Lo et al., 2020)
corpus of scientific papers, which spans multiple scientific domains and offers rich citation-related
metadata. We organize and process the data, and extend it with automatically generated contribution
and context reference summaries, to enable supervised model training. We also introduce three ab-
stractive baseline approaches: 1) a unified, controllable model manipulated with descriptive control
codes (Fan et al., 2018; Keskar et al., 2019), 2) a one-to-many sequence model with a branched
decoder for multi-head generation (Luong et al., 2016; Guo et al., 2018), and 3) an information-
theoretic training strategy leveraging supervision coming from the citation metadata (Peyrard, 2019).
To benchmark our models, we design a comprehensive automatic evaluation protocol that measures
performance across three axes: relevance, novelty, and disentanglement. We thoroughly evaluate
and analyze the baselines models and investigate the effects of the additional training objective on
the model’s behavior. To motivate the usefulness of the newly introduced task, we conducted a
human study involving human annotators in a hypothetical paper-reviewing setting. The results
find disentangled summaries more helpful in 79% of cases in comparison to abstract-oriented out-
puts. Code, model checkpoints, and data preparation scripts introduced in this work are available
at https://github.com/salesforce/disentangled-sum.
2	Related Work
Recent trends in abstractive text summarization show a shift of focus from designing task-specific
architectures trained from scratch (See et al., 2017; Paulus et al., 2018) to leveraging large-scale
Transformer-based models pre-trained on vast amounts of data (Liu & Lapata, 2019; Lewis et al.,
2020), often in multi-task settings (Raffel et al., 2019). A similar shift can be seen in scientific paper
summarization, where state-of-the-art approaches utilize custom pre-training strategies (Zhang et al.,
2019) and tackle problems of summarizing long documents (Zhao et al., 2020; Zaheer et al., 2020).
Other methods, at a smaller scale, seek to utilize the rich metadata associated with scientific articles
and combine them with graph-based methods (Yasunaga et al., 2019). In this work, we combine
these two lines of work and propose models that benefit from pre-training procedures, but also take
advantage of task-specific metadata.
Popular large-scale benchmark datasets in scientific paper summarization (Cohan et al., 2018) were
automatically collected from open-access paper repositories and consider article abstracts as the
reference summaries. Other forms of supervision have also been investigated for the task, including
author-written highlights (Collins et al., 2017), human annotations and citations (Yasunaga et al.,
2019), and transcripts from conference presentations of the articles (Lev et al., 2019). In contrast,
we introduce a large-scale automatically collected dataset with more fine-grained references than
abstracts, which also offers rich citation-related metadata.
Update summarization (Dang & Owczarzak) defines a setting in a collection of documents with
partially overlapping information is summarized, some of which are considered prior knowledge.
The goal of the task is to focus the generated summaries on the novel information. Work in this line
of research mostly focuses on novelty detection in news articles (Bysani, 2010; Delort & Alfonseca,
2012) and timeline summarization (Martschat & Markert, 2018; Chang et al., 2016) on news and
social media domains. Here, we propose a novel task that is analogous to update summarization in
that it also requires contrasting the source article with the content of other related articles which are
considered pre-existing knowledge.
3	Task
Given a source article D, the goal of disentangled paper summarization is to simultaneously sum-
marize the contribution ycon and context yctx of the source article. Here, contribution refers to the
novelties introduced in the article D, such as new methods, theories, or resources, while context
represents the background of the work D, such as a description of the problem or previous work on
the topic. The task inherently requires a relative comparison of the article with other related papers
to effectively disentangle its novelties from pre-existing knowledge. Therefore, we also consider
two sets of citations: inbound citations CI and outbound citations CO as potential sources of useful
information for contrasting the article D with its broader field. Inbound citations refer to the set of
papers that cite D, i.e. relevant future papers, while outbound citations are the set of papers that
2
Under review as a conference paper at ICLR 2021
Table 1: Token length statistics on the training split of our dataset compared to existing scientific paper sum-
marization datasets. Contribution summaries tend to be shorter than context summaries.
Dataset	#Examples	Avg. #Tokens			
		Paper D	Inbound CI	Outbound CO	Contribution ycon	Context yctx
ArXiv (Train)	203037	4938	-	-	220 (Total summary)
PubMed (Train)	119924	3016	-	-	203 (Total summary)
Ours - Train	805152	6351	925	877	136	236
Valid	36129	6374	922	875	135	236
Test	54242	6350	927	892	136	237
D cites, i.e. relevant previous papers. With its unique set of goals, the task of disentangled paper
summarization poses a novel set of challenges for automatic summarization systems to overcome:
1) identifying salient content of D and related papers from CI and CO, 2) comparing the content
of D with each document from the citations, and 3) summarizing the article along the two axes:
contributions and context.
3.1	Dataset
Current benchmark datasets used for the task of scientific paper summarization, such as arXiv and
PubMed (Cohan & Goharian, 2015), are limited in size, the number of domains, and lack of citation
metadata. Thus, we construct a new dataset based on the S2ORC (Lo et al., 2020) corpus, which
offers a large collection of scientific papers spanning multiple domains along with rich citation-
related metadata, such as citation links between papers and annotated citation spans. Specifically,
we carefully curate the data available in the S2ORC corpus and extend it with new reference labels.
Data Curation Some papers in the S2ORC corpus2 do not contain a complete set of information
required by our summarization task: paper text, abstract, and citation metadata. We remove such
instances and construct a paper summarization dataset in which each example a) has an abstract and
body text, and b) has at least 5 or more inbound and outbound citations, CI and CO respectively. In
cases where a paper has more than K incoming or outgoing citations, we first sample K citations
for each of incoming and outgoing citations and sort them in descending order by the number of
their respective citation from and to the target paper. K is a hyperparameter and we choose K = 20
in this work.
Citation Span Extraction Each article in the set of inbound and outbound citations can be rep-
resented by its full text, abstract, or the span of text associated with the citation. In this study, we
follow Qazvinian & Radev (2008) and Cohan & Goharian (2015) in representing citations with the
sentences in which the citation occurs.3 Thus, an outbound citation is represented by a sentence
from the source paper. Usually, such sentences directly refer to the cited paper and place its content
in relation to the source paper. Analogously, an inbound citation is represented by sentences from
the citing paper and relates its content with the source paper.
Reference Generation Our approach relies on the availability of reference summaries for both
contributions and contexts. However, such annotations are not provided or easily extractable from
the S2ORC corpus, and collecting expert annotations is infeasible due to the associated costs. There-
fore, we apply a data-driven approach to automatically extract contribution and context reference
summaries from the available paper abstracts. First, we manually label 400 abstracts sampled from
the training set. Annotations are done on a sentence-level with binary labels indicating contribution-
and context-related sentences4. This procedure yields 3341 sentences with associated binary labels,
which we refer to as golden standard references. Next, we fine-tune an automatic sentence classifier
using the golden standard data. As our classifier we use SciBERT (Beltagy et al., 2019), which
after fine-tuning achieves 86.3% accuracy and 0.932 for Area under ROC curve in classifying con-
tribution and context sentences on a held-out test set. Finally, we apply the fine-tuned classifier
2Release ID: 20190928.
3
If a publication is cited multiple times within a source article we concatenate all relevant sentences.
4
Sentences not labeled as contribution are considered context, we leave finer-grained labels for future work.
3
Under review as a conference paper at ICLR 2021
ICode Distant supervised relation extraction (RE) has been an
effective way of finding novel relational facts from text without
labeled trainmg data. Typically it can be formalized as a ɪnulti-
instance multilabel problem. In this paper, we introduce ...
Distant supervised relation extraction (RE) has been an effective
way of finding novel relational facts from text without labeled
training data. Typically it can be formalized as a multi-instance
multilabel problem. In this paper, we introduce ...
ICoIrtribUti on: I	ICOIrteXtTl
Shared-parameter Model
Shared Encoder Layers
Shared Decoder Layers
In this paper, we propose an
attention-based memory
network for distant supervised
relation extraction (RE), which
is capable of modeling the
semantic relatedness of a...
Relation extraction (RE) is a
fundamental task in NLH and
a crucial component for
building structured Knowledge
Base from free texts. Previous
methods including...
Contribution Decoder Layer
Ψ
In this paper, we propose an
attention-based memory
network for distant supervised
relation extraction (RE), which
is capable of modeling ⅛e
semantic relatedness of a...
Context Decoder Layer
Ψ
Relation extraction (RE) is a
fundamental task in NLH and
a crucial component for
building structured Knowledge
Base from free texts. Previous
methods including...
Figure 1: Model diagram. Left: CONTROLCODE model, in which inputs are prefixed with a prompt symbol
and passed to a shared model to control the output mode. Right: MultiHead model, which shares all of the
model’s parameters apart from the last decoder layer for different output modes, and chooses the final decoder
layer accordingly to control the output mode.
to generate reference labels for all examples in our dataset, which we refer to as silver standard
references. The statistics of the resulting dataset are shown in Table 1.
4	Models
Our goal is to build an abstractive summarization system which has the ability to generate con-
tribution and context summaries based on the source article. To achieve the necessary level of
controllability, we propose two independent approaches building on encoder-decoder architectures:
ControlCode (CC) A common approach to controlling model-generated text is by condition-
ing the generation procedure on a control code associated with the desired output. Previous work
on controllable generation (Fan et al., 2018; Keskar et al., 2019) showed that prepending a spe-
cial token or descriptive prompt to the model’s input during training and inference is sufficient to
achieve fine-grained control over the generated content. Following this line of work, we modify our
training instances by prepending textual control codes, contribution: or context:, to the
summarized articles. During training, all model parameters are updated for each data instance and
the model is expected to learn to associate the provided prompt with the correct output mode. The
approach does not require changes in the architecture, making it straightforward to combine with
existing large-scale, pre-trained models. The architecture is shown on the left of Figure 1.
MultiHead (MH) An alternative way of controlling generation is by explicitly allocating lay-
ers within the model specifically for the desired control aspects. Prior work investigating multi-task
models (Luong et al., 2016; Guo et al., 2018) showed the benefits of combining shared and task-
specific layers within a single, multi-task architecture. Here, the encoder shares all parameters be-
tween the two generation modes, while the decoder shares all parameters, apart from the final layer,
which splits into two generation branches. During training, each branch is individually updated with
gradients from the associated mode. The model shares the softmax layer weights between the out-
put branches under the assumption that token-level vocabulary distributions are similar in the two
generation modes due to the common domain. This approach is presented on the right of Figure 1.
4.1	Informativeness-guided Training
Peyrard (2019) proposed an information-theoretic perspective on text summarization which decom-
poses the criteria of a good summary into redundancy, relevance, and informativeness. Among these
criteria, informativeness measures the user’s degree of surprise after reading a summary given their
background knowledge, and can be formally defined as:
Inf (D, K) = - X PD(ωi) log PK(ωi),	(1)
i
where ωi is a primitive semantic unit, PK is the probability over the unit under the user’s knowledge,
PD is the probability over the unit with respect to the source document, and i is an index over all
semantic units within a summary.
4
Under review as a conference paper at ICLR 2021
As defined by Peyrard (2019), informativeness is in direct correspondence to contribution summa-
rization. Paper contributions are novel contents introduced to the community, which causes surprisal
given the general knowledge about the state of the field. Therefore, in this work we explore utilizing
this measure as an auxiliary objective that is optimized during training. We define the semantic unit
ωi as the summary itself5 , which enables a simple interpretation of the corresponding probabilities.
We estimate PD as the likelihood of the summary given the paper content, PD(ωi) = p(y | D).
Since each paper is associated with a unique context and background knowledge, we treat the back-
ground knowledge as all relevant papers published before the source paper, i.e., outbound cita-
tions CO . Therefore, PK is estimated as the likelihood of the summary given the previous work,
PK(ωi)=p(y | CO). We formulate the informativeness function as:
Inf(D,K)
-p(ycon | D) log p(ycon | CO)
-p(yctx | D) log p(yctx | CI)
if generating contributions
otherwise
(2)
where the conditioning depends on the generation mode of the model, and aim to maximize it during
the training procedure. The estimation of each term is done by a forward pass on the model with
corresponding input and output pairs: p(ycon | CO) is computed by estimating the probability of
ycon when feeding CO as the encoder input.
Combined with a cross entropy loss LCE, we obtain the final objective which we aim to the minimize
during training:
L=LCE-λInf(D,K),	(3)
where λ is a scaling hyperparameter determined through cross-validation.
5	Experiments and Results
In this section, we describe the experimental environment and report automatic evaluation results.
We consider four model variants:
•	CC, CC+INF: CONTROLCODE model without and with the informativeness objective,
•	MH, MH+INF: MULTIHEAD model without and with the informativeness objective.
^con °,---o SCtX
Vcon IJctx
o—o Relevance
口 Purity
o--o Disentanglement
Figure 2: Diagram illustrating the evaluation
protocol assessing summaries along 3 axes: rel-
evance, purity, and disentanglement.
5.1	Evaluation
We perform automatic evaluation of the system out-
puts (scon,sctx) against the silver standard refer-
ences (ycon, yctx). For this purpose, we have de-
signed a comprehensive evaluation protocol, shown
in Figure 2, based on existing metrics that evaluates
the performance of models across 3 dimensions:
Relevance Generated summaries should closely
correspond with the available reference summaries.
We measure the lexical overlap and semantic sim-
ilarity between (scon,ycon) and (sctx ,yctx) using
ROUGE (R-i) (Lin, 2004) and BERTScore (Zhang et al. 2020; BS), respectively.
Purity Generated contribution summary should closely correspond with its respective reference
summary, but should not overlap with the context reference summary. We measure the lexical
overlap between scon and (ycon, yctx) using NouveauROUGE con (Ncon-i) (Conroy et al., 2011).
The metric reports an aggregate score defined as a linear combination between the two components:
NouveauROUGEcon-i = α0i +αi1ROUGE-i(scon,ycon)+αi2ROUGE-i(scon,yctx),
where weights αij were set by the original authors to favor outputs with maximal and minimal
overlap with related and unrelated references, accordingly. Analogously, we calculate Nctx-i in
5For simplicity in modeling, we chose the entire summary. However, this goes against the requirement set by Peyrard (2019) that ωi is a
primitive semantic unit, because a paragraph’s meaning can be decomposed into higher granular units.
5
Under review as a conference paper at ICLR 2021
reverse direction between sctx and (yctx, ycon). Purity P-i is defined as the average novelty in both
directions:
Purity-i = (Ncon-i + Nctx-i)/2;	(P-i).
Disentanglement Generated contribution and context summaries should have minimal overlap.
We measure the degree of lexical overlap and semantic similarity between (scon,sctx) using
ROUGE and BERTScore, respectively. To maintain consistency across metrics (higher is better)
we report disentanglement scores as complements of the associated metrics:
DisROUGE-i = 100 - ROUGE-i; (D-i),
DisBERTScore = 100 - BERTScore; (DBS).
5.2	Implementation Details
Our models build upon distilBART6 (Sanh et al., 2019; Wolf et al., 2019), a Transformer-
based (Vaswani et al., 2017), pre-trained sequence-to-sequence architecture distilled from
BART (Lewis et al., 2020). Specifically, we used a model with 6 self-attention layers in both the
Encoder and Decoder. Weights were initialized from a model fine-tuned on a news summarization
task. 7 For the MultiHead model, the final layer of the decoder was duplicated and initialized with
identical weights. We fine-tuned on the training set for 80000 gradient steps with a fixed learning
rate of 3.0 × 10-5 and choose the best checkpoints in terms of ROUGE-1 scores on the validation
set. The loss scaling hyparameter λ (Eq. 3) was set to 0.05 and 0.01 for the CONTROLCODE and
MultiHead models, accordingly. Input and output lengths were set to 1024 and 200, respectively.
At inference time, we decoded using beam search with beam size 5. The evaluation was performed
using SummEval toolkit (Fabbri et al., 2020).
5.3	Results
In Table 2 we report results from the automatic evaluation protocol described in Subsection 5.1.
Relevance Across most models and metrics, relevance scores for context generation are higher
than those for contribution summarization. Manual inspection revealed that in some cases generated
context summaries also include article contribution information, while this effect was not observed
in the reverse situation. Considering that silver standard annotations may contain noisy examples
with incorrectly separated references, we suspect that higher ROUGE scores for context summaries
may be caused by noisy predictions coinciding with noisy references. Examples of such summaries
are shown in the Appendix D. We also observe that informativeness-guided models (+INF) perform
on par with their respective base versions, and the additional training objective does not affect the
performance on the relevance metric. This insight corroborates with Peyrard (2019) who defines
informativeness and relevance as orthogonal criteria.
Purity While the informativeness objective was designed to improve the novelty of generated
summaries, results show an opposite effect, where informativeness-guided models slightly under-
perform their base counterparts. The true reason for such behavior is unknown, however, it might
be an indicator that the outbound citations CO are not a good approximation of reference context
summaries yctx, or the relationship between the two is weak. This effect is more evident in the
Medical and Biology domains, which are the two most frequent domains in the dataset.
Disentanglement Results indicate that CONTROLCODE-based models perform better than MUL-
tiHead approaches in terms of generating disentangled outputs. This comes as a surprise given
that the CC models share all parameters between the two generation modes, but might indicate that
the two tasks contain complementary training signals. We also noticed that, both informativeness-
guided models performed better in terms of D-1.
Based on both purity and disentanglement evaluations, we suspect that the informativeness objective
does guide the models to output more disentangled summaries (second term in Eq 2), but the signal
6We did not observe a substantial difference in performance between distilBART and BART.
7Model weights are available at https://huggingface.co/sshleifer/student_cnn_6_6.
6
Under review as a conference paper at ICLR 2021
Table 2: Automatic evaluation results on the test set. For all metrics, higher values indicate better
results. Con and Ctx refer to contribution summary and context summary, respectively. Purity and
Disentanglement are measaured on the pairs of contribution and context summaries.
Model		Relevance				Purity		Disentanglement			
		R-1	R-2	R-L	BS	P-1	P-2	D-1	D-2	D-L	DBS
CC	Con	39.16	15.96	24.65	63.22	2.77	3.69	52.95	72.18	69.12	33.62
	Ctx	41.84	17.24	24.55	63.78						
CC+INF	Con	38.92	15.95	24.65	62.94	2.75	3.68	53.68	71.97	68.46	34.09
	Ctx	41.49	17.03	24.50	63.40						
MH	Con	39.20	15.98	24.72	63.04	2.73	3.68	50.89	69.51	65.97	32.51
	Ctx	41.67	17.23	24.65	63.77						
MH+INF	Con	38.74	15.90	24.59	62.70	2.68	3.60	53.35	71.47	67.20	33.86
	Ctx	40.39	16.31	23.83	62.85						
is not strong enough to focus on generating the appropriate content (first term in Eq 2). It is also
clear that the MultiHead model benefits more from the additional training objective.
6 Analysis
6.1	Qualitative Analysis
To better understand the strengths and shortcomings of our models, we performed a qualitative study
of model outputs. Table 3 shows an example of generated summaries compared with the original
abstract of the summarized article. Our model successfully separates the two generation modes and
outputs coherent and easy to follow summaries. The contribution summary clearly lists the novelties
of the work, while the context summary introduces the task at hand and explains its importance. In
comparison, the original abstract briefly touches on many aspects: the context, methods used, and
contributions, but also offers details that are not of primary importance, such as the detailed about
the simulation environment.
More generally, the described trends hold across summaries generated by our models. The model
outputs are fluent, abstractive, offer good separation between modes, and are on topic. However,
the factual correctness of summaries could not be assessed due to the highly specialized content and
language of the summarized articles. An artifact noticed in a few instances of the inspected out-
puts was leakage of contribution information into context summaries. Other examples of generated
summaries are included in the Appendix D.
6.2	Per-domain Performance
Taking advantage of the rich metadata associ-
ated with the S2ORC corpus, we analyze the
performance of models across the 10 most fre-
quent scientific domains. Table 4 shows the re-
sults of contribution summarization using the
ControlCode8 model. While ROUGE-1
scores oscillate around 40 points for most aca-
demic fields, the results indicate that summa-
rizing documents from the Medical domain is
particularly difficult, with models scoring about
7 points below average. Manual inspection of
instances with low scores (R-1 < 20), exposed
that contribution summaries in the Medical do-
main are highly quantitative (e.g. “Among these
Table 4: Relevance evaluation of contribution
summaries for the top 10 domains generated us-
ing the ControlCode model. Performance on
Medicine domain is paricularly low.
Metric	R-1	R-2	R-L	BS
Biology	40.63	17.01	25.59	64.23
Medicine	33.97	13.08	21.73	61.75
Mathematics	40.13	15.56	24.42	61.58
Computer science	43.54	16.41	25.86	63.43
None	40.31	18.14	26.68	64.00
Psychology	39.51	15.56	24.34	62.95
Physics	40.09	15.85	24.89	62.10
Chemistry	40.44	17.77	26.14	63.93
Economics	39.56	14.25	23.41	60.91
Materials science	42.52	18.96	27.57	65.25
treated ... retinopathy was noted in X%”). While other domains such as Biology also suffer from the
same phenomenon, low-scoring quantitative summaries were 1.9 times more frequent in Medicine
than in Biology. An investigation into the domain distribution in our dataset (Appendix) revealed
8The remaining models exhibit the same pattern.
7
Under review as a conference paper at ICLR 2021
Table 3: Generated samples compared with the original and generated abstracts of the associated paper. The
second rows shows the output decoded from DistilBART fine-tuned on our dataset, the third rows shows the
outputs from ControlCode model. Our model successfully generates disentangled content, thus making it
easier to follow than the abstract.
Original Abstract: Energy optimization in buildings by controlling the Heating Ventilation and Air Conditioning (HVAC) system is being
researched extensively. In this paper, a model-free actor-critic Reinforcement Learning (RL) controller is designed using a variant of artificial
recurrent neural networks called Long-Short-Term Memory (LSTM) networks. Optimization of thermal comfort alongside energy consumption
is the goal in tuning this RL controller. The test platform, our office space, is designed using SketchUp. Using OpenStudio, the HVAC system
is installed in the office. The control schemes (ideal thermal comfort, a traditional control and the RL control) are implemented in MATLAB.
Using the Building Control Virtual Test Bed (BCVTB), the control of the thermostat schedule during each sample time is implemented for the
office in EnergyPlus alongside local weather data. Results from training and validation indicate that the RL controller impoves thermal comfort
by an average of 15% and energy efficiency by an average of 2.5% as compared to other strategies mentioned.
Generated Abstract: Despite the advances in research on HVAC control algorithms, most field equipment is controlled using classical
methods that include hysteresis/on/off and Proportional Integral and Derivative (PID) controllers. These classical methods do not perform
optimally. The high thermal inertia of buildings induces large time delays in the building dynamics, which cannot be handled efficiently by
the simple on/off controllers. However, due to the high non-linearity in building dynamics coupled with uncertainties such as weather, energy
pricing, etc., these PID controllers require extensive retuning or auto-tuning capabilities, which increases the difficulty and complexity of
the control problem. In this work, we introduce novel control algorithms from a branch of machine learning called reinforcement learning.
From a controls perspective, reinforcement learning algorithms can be considered as direct adaptive optimal control. Like optimal control,
reinforcement training algorithms minimize the cumulative sum of costs over a time horizon. Unlike traditional optimization algorithms can
learn optimal control actions
Contribution: In this work, we introduce novel control algorithms from a branch of machine learning called reinforcement learning. In
our current approach, the impetus is thermostat control. Instead of traditional on/off heating and cooling control, reinforcement learning is
utilized to set this schedule to obtain improved Predicted Mean Vote (PMV)-based thermal comfort at an optimal energy expenditure. Hence,
a thermostats schedule is computed using an RL controller. The results show that the Q-learning algorithm can learn to adapt to time-varying
and nonlinear system dynamics without explicit identification of the plant model in both systems and controls.
Context: The Heating, Ventilation and Air Conditioning (HVAC) systems can account for up to 50% of total building energy demand. In
the hopes of moving toward a greener, more energy-efficient future, a significant improvement in energy efficiency is needed to achieve this
goal. Despite the advances in research on HVAC control algorithms, most field equipment is controlled using classical methods that include
hysteresis/on/off and Proportional Integral and Derivative controllers. However, due to the high nonlinearity in building dynamics coupled with
uncertainties such as weather, energy pricing, etc., these PID controllers require extensive retuning or auto-tuning capabilities, which increases
the difficulty and complexity of the control problem. The high thermal inertia of buildings induces large time delays in the building dynamics,
which cannot be handled efficiently by the simple on/off controllers.
that Biology and Medicine are the two best represented fields in the corpus, with Biology having
over twice as many examples. We hypothesize that the poor performance of models stems from the
fact that generating such quantitative summaries requires a deeper, domain-specific understanding
of the source document and the available in-domain training data is insufficient to achieve that goal.
6.3	Human Evaluation of Usefulness
To assess the usefulness of the newly introduced
task to the research community, we conducted a
human study involving expert annotators. The
study aimed to compare disentangled papers sum-
Table 5: Usefulness of disentangled summaries in
percentage, e.g., Annotator 1 (A1) chose the disen-
tangled summaries 82% out of all the samples from
S2ORC.
maries with traditional, abstract-based summaries
in a hypothetical paper reviewing setting. Judges	Dataset	A1	A2	A3	AVG.
were shown both types of summaries side by side	S2ORC	82%	78%	70%	77%
and asked to pick one which would be more help-	CORD	88%	76%	78%	81%
ful for conducting the paper review. We show a
screen shot of the annotation user interface in Fig 3. Abstract-based summaries were generated by
a model with a configuration identical to the models previously introduced in this work, trained to
generate full abstracts using the same training corpus. Annotators that participated in this study hold
graduate degrees in technical fields and are active in the research community, however, they were
not involved or familiar with this work prior to this experiment. The study used 100 examples, out
of which 50 were decoded on the test split of the adapted S2ORC dataset, while the other 50 were
generated in a zero-shot fashion from articles in the CORD dataset (Wang et al., 2020), a recently
introduced collection of papers related to COVID-19. The inter-annotator agreement measured by
Fleiss’ Kappa was 0.41 and 0.33 for the S2ORC and CORD datasets, respectively. Results in Table 5
show the proportion of all examples where the annotators preferred the disentangled summaries over
the generated abstracts. The numbers indicate a strong preference from the judges for disentangled
summaries, in the case of both S2ORC and CORD examples. The values on CORD samples are
slightly higher than those on S2ORC; we suspect this being due to the fact that the annotators were
8
Under review as a conference paper at ICLR 2021
A
B
Contribution
Abstract
In this study, we describe the first herpesvirus sequences from two
insectivorous and two hem us bat species of the
Vespertilionidae (Phyllostoi Chiroptera. We show that they
are closely related to Pteropodid alphaherpesvirus 1 (PtAHV1),
which is one of the most common herpesviruses reported for New
World mammals, but not all other viruses described so far. In
addition, our data suggest that these viruses may be transmitted
between different species within a single geographic region.
in terms of their anatomy and lifestyles, and
:eetivorous, frugivores, nectarivoro us,
carnivorous, piscivorous or hematophagous). They play a major role
in the emergence and transmission of z∞notic viruses such as
Iyssaviruses. Bats are also carriers of other herpesviruses, includ
ing herpesvirus species that are host specific. However, there are
examples of cross-species transmission. The first description of bat
herpesviral sequences dates back to 2007 (Wibbelt et al., 2007).
Over the past decade, dozens of virus sequences have been
described from different bat species on every continent. Most of
them were characterized from apparently healthy anima Is sampled
during trapping campaigns in the frame of random surveillance
programs. To date, two bat alphaherpesvirus 1 (FBAHV1) has been
recognized as species by the International Committee for
Context
BACKGROUND Bat herpesviruses, fruit bat alphaherpesvirus 1
(FBAHV1 and BGHV8) have been recognized as species by the
International Committee for Taxonomy of Viruses (ICTV) according
to the latest master species list (MSL# 34) released on March 8,
2019. Most sequences are from New World bat species, six from
North and Central America and two hematophagous bat species
from South America. METHODOLOGY/PRINCIPAL FINDINGS A total
of 233 bat herpes viruses were identified in a wide range of bat
herpesvirides (PtAHVI) and Vespertilionid gammaherpesviruses
(BghV8). The ICTV official names are Pteropodid alphaherpeviral 1
(PTAV1), Vespertendilionid beta-and-Zamudio (ZAMUDio) or
ZAMUDIO (ZMAD
Which of the assistant outputs would you consider more helpful for your task (reviewing)?
Figure 3: The annotation interface. Summaries indicated with A and B are disentangled summaries
and a generated abstract, respectively.
less familiar with the topics described in Covid-related publications and would require more help to
review such articles.
6.4 Evaluation against Gold Annotations
As discussed in Section 3.1, contribution and context labels are assigned automatically using a data-
driven classifier, which could introduce errors in the process. Therefore, we created a gold standard
evaluation set by manually annotating 100 samples from the test set and report the evaluation results
in Table 6. A sharp drop in ROUGE scores for the context summaries is caused by some examples
receiving zero scores for generating context summaries when the manual annotators judged that
there are not existent. The overall trend of ControlCode model outpeforming MultiHead
model is still observed in the evaluation. More importantly, a reverse tendency is noticeable when
the two models are trained with the informativeness objective. Specifically, the MultiHead model
showed significant improvement in terms of novelty and disentanglement.
Table 6: Automatic evaluation results on 100 samples from the test set manually annotated for
contributions. For all metrics, higher values indicate better results.
Model		Relevance				Purity		Disentanglement			
		R-1	R-2	R-L	BS	P-1	P-2	D-1	D-2	D-L	DBS
CC	Con	39.37	15.86	24.73	63.28	2.30	3.22	52.81	71.52	68.36	33.05
	Ctx	30.59	11.22	19.08	55.76						
	Con	38.38	15.21	23.47	62.59			52.49			
CC+INF	Ctx	30.14	11.10	19.00	55.55	2.17	3.10		69.64	66.60	32.76
MH	Con	38.63	15.53	24.68	62.84	2.21	3.13	49.62	67.45	64.43	31.39
	Ctx	29.82	10.61	18.51	55.24						
MH+INF	Con	39.43	15.75	24.77	63.11	2.26	3.13	51.56	68.57	64.97	32.35
	Ctx	29.14	10.25	18.48	54.92						
7 Conclusions
In this paper, we propose disentangled paper summarization, a new task in scientific paper sum-
marizing where models simultaneously generate contribution and context summaries. With the task
in mind, we introduced a large-scale dataset with fine-grained reference summaries and rich meta-
data. Along with the data, we introduced three abstractive baseline approaches to solving the new
task and thoroughly assessed them using a comprehensive evaluation protocol design for the task at
hand. Through human studies involving expert annotators with motivated the usefulness of the task
in comparison to the current scientific paper summarization setting. Together with this paper, we
release the code, trained model checkpoints, and data preprocessing scripts to support future work in
this direction. We hope this work will positively contribute to creating AI-based tools for assisting
scientists in the research process.
9
Under review as a conference paper at ICLR 2021
References
Iz Beltagy, Kyle Lo, and Arman Cohan. SciBERT: A pretrained language model for scientific text. In
Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and
the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp.
3615-3620, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1371. URL https://www.aclweb.org/anthology/D19- 1371.
Praveen Bysani. Detecting novelty in the context of progressive summarization. In Human Language
Technologies: Conference of the North American Chapter of the Association of Computational
Linguistics, Proceedings, June 2, 2010, Los Angeles, California, USA - Proceedings of the Stu-
dent Research Workshop, pp. 13-18. The Association for Computational Linguistics, 2010. URL
https://www.aclweb.org/anthology/N10-3003/.
Yi Chang, Jiliang Tang, Dawei Yin, Makoto Yamada, and Yan Liu. Timeline summarization from
social media with life cycle models. In IJCAI, pp. 3698-3704, 2016.
Arman Cohan and Nazli Goharian. Scientific article summarization using citation-context and ar-
ticle’s discourse structure. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 390-400. Association for Computational Linguistics, Septem-
ber 2015. doi: 10.18653/v1/D15-1045. URL https://www.aclweb.org/anthology/
D15-1045.
Arman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and
Nazli Goharian. A discourse-aware attention model for abstractive summarization of long docu-
ments. In Proceedings of the 2018 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp.
615-621, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi:
10.18653/v1/N18-2097. URL https://www.aclweb.org/anthology/N18- 2097.
Ed Collins, Isabelle Augenstein, and Sebastian Riedel. A supervised approach to extractive sum-
marisation of scientific papers. In Proceedings of the 21st Conference on Computational Natural
Language Learning (CoNLL 2017), pp. 195-205, Vancouver, Canada, August 2017. Association
for Computational Linguistics. doi: 10.18653/v1/K17-1021. URL https://www.aclweb.
org/anthology/K17-1021.
John M. Conroy, Judith D. Schlesinger, and Dianne P. O’Leary. Nouveau-ROUGE: A novelty metric
for update summarization. Computational Linguistics, 37(1):1-8, 2011. doi: 10.1162/coli_a_
00033. URL https://www.aclweb.org/anthology/J11-1001.
Hoa Trang Dang and Karolina Owczarzak. Overview of the tac 2008 update summarization task.
Jean-Yves Delort and Enrique Alfonseca. Dualsum: a topic-model based approach for update sum-
marization. In Walter Daelemans, Mirella Lapata, and Lluls Marquez (eds.), EACL 2012, 13th
Conference of the European Chapter of the Association for Computational Linguistics, Avignon,
France, April 23-27, 2012, pp. 214-223. The Association for Computer Linguistics, 2012. URL
https://www.aclweb.org/anthology/E12-1022/.
Alexander R Fabbri, Wojciech KryRinski, Bryan McCann, Caiming Xiong, Richard Socher,
and Dragomir Radev. Summeval: Re-evaluating summarization evaluation. arXiv preprint
arXiv:2007.12626, 2020.
Angela Fan, David Grangier, and Michael Auli. Controllable abstractive summarization. In Proceed-
ings of the 2nd Workshop on Neural Machine Translation and Generation, pp. 45-54, Melbourne,
Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-2706.
URL https://www.aclweb.org/anthology/W18- 2706.
Han Guo, Ramakanth Pasunuru, and Mohit Bansal. Soft layer-specific multi-task summarization
with entailment and question generation. In Iryna Gurevych and Yusuke Miyao (eds.), Pro-
ceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL
2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 687-697. As-
sociation for Computational Linguistics, 2018. doi: 10.18653/v1/P18-1064. URL https:
//www.aclweb.org/anthology/P18-1064/.
10
Under review as a conference paper at ICLR 2021
Nitish Shirish Keskar, Bryan McCann, Lav Varshney, Caiming Xiong, and Richard Socher. CTRL
- A Conditional Transformer Language Model for Controllable Generation. arXiv preprint
arXiv:1909.05858, 2019.
Svetlana Kiritchenko and Saif Mohammad. Best-worst scaling more reliable than rating scales: A
case study on sentiment intensity annotation. In Proceedings of the 55th Annual Meeting of the
Association for Computational Linguistics (Volume 2: Short Papers), pp. 465-470, Vancouver,
Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-2074. URL
https://www.aclweb.org/anthology/P17-2074.
Guy Lev, Michal Shmueli-Scheuer, Jonathan Herzig, Achiya Jerbi, and David Konopnicki. Talk-
Summ: A dataset and scalable annotation method for scientific paper summarization based on
conference talks. In Proceedings of the 57th Annual Meeting of the Association for Compu-
tational Linguistics, pp. 2125-2131, Florence, Italy, July 2019. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/P19-1204.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer
Levy, Veselin Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-
training for natural language generation, translation, and comprehension. In Proceedings of
the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871-7880, On-
line, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.703.
URL https://www.aclweb.org/anthology/2020.acl-main.703.
Chin-Yew Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization
Branches Out, pp. 74-81, Barcelona, Spain, July 2004. Association for Computational Linguis-
tics. URL https://www.aclweb.org/anthology/W04- 1013.
Yang Liu and Mirella Lapata. Text summarization with pretrained encoders. In Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th In-
ternational Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 3730-
3740, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:
10.18653/v1/D19-1387. URL https://www.aclweb.org/anthology/D19- 1387.
Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The seman-
tic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Associa-
tion for Computational Linguistics, pp. 4969-4983, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://www.aclweb.org/
anthology/2020.acl-main.447.
Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. In Yoshua Bengio and Yann LeCun (eds.), 4th International
Conference on Learning Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,
Conference Track Proceedings, 2016. URL http://arxiv.org/abs/1511.06114.
Sebastian Martschat and Katja Markert. A temporally sensitive submodularity framework for time-
line summarization. In Anna Korhonen and Ivan Titov (eds.), Proceedings of the 22nd Confer-
ence on Computational Natural Language Learning, CoNLL 2018, Brussels, Belgium, October
31 - November 1, 2018, pp. 230-240. Association for Computational Linguistics, 2018. doi:
10.18653/v1/k18-1023. URL https://doi.org/10.18653/v1/k18-1023.
Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. In 6th International Conference on Learning Representations, ICLR 2018, Van-
couver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net,
2018. URL https://openreview.net/forum?id=HkAClQgA-.
Maxime Peyrard. A simple theoretical model of importance for summarization. In Proceedings of
the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1059-1073, Flo-
rence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1101.
URL https://www.aclweb.org/anthology/P19- 1101.
Vahed Qazvinian and Dragomir R. Radev. Scientific paper summarization using citation summary
networks. In Proceedings of the 22nd International Conference on Computational Linguistics
11
Under review as a conference paper at ICLR 2021
(Coling 2008),pp. 689-696, Manchester, UK, August 2008. Coling 2008 Organizing Committee.
URL https://www.aclweb.org/anthology/C08- 1087.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. CoRR, abs/1910.10683, 2019. URL http://arxiv.org/abs/1910.10683.
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version
of BERT: smaller, faster, cheaper and lighter. CoRR, abs/1910.01108, 2019. URL http://
arxiv.org/abs/1910.01108.
Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with
pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pp. 1073-1083, Vancouver, Canada, July
2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1099. URL https:
//www.aclweb.org/anthology/P17-1099.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.
URL http://arxiv.org/abs/1706.03762.
Lucy Lu Wang, Kyle Lo, Yoganand Chandrasekhar, Russell Reas, Jiangjiang Yang, Darrin Eide,
Kathryn Funk, Rodney Kinney, Ziyang Liu, William Merrill, et al. Cord-19: The covid-19 open
research dataset. arXiv preprint arXiv:2004.10706, 2020.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,
Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick
von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger,
Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Huggingface’s transformers: State-
of-the-art natural language processing. ArXiv, abs/1910.03771, 2019.
Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander Fabbri, Irene Li, Dan Friedman, and
Dragomir Radev. ScisummNet: A large annotated corpus and content-impact models for sci-
entific paper summarization with citation networks. In Proceedings of AAAI 2019, 2019. URL
https://www.aaai.org/ojs/index.php/AAAI/article/view/4727/4605.
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon,
Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed. Big bird: Transformers
for longer sequences, 2020.
Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J. Liu. Pegasus: Pre-training with extracted
gap-sentences for abstractive summarization, 2019.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evalu-
ating text generation with bert. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=SkeHuCVFDr.
Yao Zhao, Mohammad Saleh, and Peter J Liu. Seal: Segment-wise extractive-abstractive long-form
text summarization. arXiv preprint arXiv:2006.10213, 2020.
12