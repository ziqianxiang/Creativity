Under review as a conference paper at ICLR 2021
No Spurious Local Minima:
on the Optimization Landscapes of
Wide and Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
Empirical studies suggest that wide neural networks are comparably easy to op-
timize, but mathematical support for this observation is scarce. In this paper, we
analyze the optimization landscapes of deep learning with wide networks. We
prove especially that constraint and unconstraint empirical-risk minimization over
such networks has no spurious local minima. Hence, our theories substantiate the
common belief that increasing network widths not only improves the expressiveness
of deep-learning pipelines but also facilitates their optimizations.
1	Introduction
Deep learning depends on optimization problems that seem impossible to solve, and yet, deep-
learning pipelines outperform their competitors in many applications. A common suspicion is
that the optimizations are often easier than they appear to be. In particular, while most objective
functions are nonconvex and, therefore, might have spurious local minima, recent findings suggest
that optimizations are not hampered by spurious local minima as long as the neural networks are
sufficiently wide. For example, Dauphin et al. (2014) suggest that saddle points, rather than local
minima, are the main challenges for optimizations over wide networks; Goodfellow et al. (2014) give
empirical evidence for stochastic-gradient descent to converge to a global minimum of the objective
function of wide networks; Livni et al. (2014) show that the optimizations over some classes of wide
networks can be reduced to a convex problem; Soudry & Carmon (2016) suggest that differentiable
local minima of objective functions over wide networks are typically global minima; Nguyen & Hein
(2018) indicate that critical points in wide networks are often global minima; and Allen-Zhu et al.
(2019) and Du et al. (2019) suggest that stochastic-gradient descent typically converges to a global
minimum for large networks.
These findings raise the question of whether common optimization landscapes over wide (but finite)
neural networks have no spurious local minima altogether.
Progress in this direction has recently been made in Venturi et al. (2019) and then Lacotte & Pilanci
(2020). Broadly speaking, we call a local minimum spurious if there is no nonincreasing path to
a global minimum (see Section 2.2 for a formal definition). While the absence of spurious local
minima does not preclude saddle points or suboptimal local minima in general, it means that one
can move from every local minimum to a global minimum without increasing the objective function
at any point—see Figure 1 for an illustration. Venturi et al. (2019) prove that there are no spurious
local minima if the networks are sufficiently wide. Their theory has two main features that had not
been established before: First, it holds for the entire landscapes—rather than for subsets of them.
This feature is crucial: even randomized algorithms typically converge to sets of Lebesgue measure
zero with probability one, that is, statements about “almost all” local minima are not necessarily
meaningful. Second, their theory allows for arbitrary convex loss functions. This feature is important,
for example, in view of the trends toward robust alternatives of the least-squares loss (Belagiannis
et al., 2015; Jiang et al., 2018; Wang et al., 2016). On the other hand, their theory has three major
limitations: it is restricted to polynomial activation, which is convenient mathematically but much
less popular than ReLU activation, it disregards regularizers and constraints, which have become
standard in deep learning and in machine learning at large (Hastie et al., 2015), and it restricts to
1
Under review as a conference paper at ICLR 2021
Figure 1: spurious local minimum of a hypothetical objective function
shallow networks, that is, networks with only one hidden layer, which contrasts the deep architectures
that are used in practice (LeCun et al., 2015).
Lacotte & Pilanci (2020) made progress on two of these limitations: first, their theory caters to ReLU
activation rather than polynomial activation; second, their theory allows for weight decay, which is
a standard way to regularize estimators. However, their work is still restricted to one-hidden-layer
networks. The interesting question is, therefore, whether such results can also be established for
deep networks. And more generally, it would be highly desirable to have a theory for the absence of
spurious local minima in a broad deep-learning framework.
In this paper, we establish such a theory. We prove that the optimization landscapes of empirical-risk
minimizers over wide feedforward networks have no spurious local minima. Our theory combines
the features of the two mentioned works, as it applies to the entire optimization landscapes, allows
for a wide spectrum of loss functions and activation functions, and constraint and unconstraint
estimation. Moreover, it generalizes these works, as it allows for multiple outputs and arbitrary
depths. Additionally, our proof techniques are considerably different from the ones used before and,
therefore, might be of independent interest.
Guide to the paper Sections 2 and 5 are the basic parts of the paper: they contain our main result
and a short discussion of its implications. Readers who are interested in the underpinning principles
should also study Section 3, and readers who want additional insights on the proof techniques are
referred to Section 4. The actual proofs are stated in the Appendix.
2	Deep-Learning Framework and Main Result
In this section, we specify the deep-learning framework and state our main result. The framework
includes a wide range of feedforward neural networks; in particular, it allows for arbitrarily many
outputs and layers, a range of activation and loss functions, and constraint as well as unconstraint
estimation. Our main result guarantees that if the networks are sufficiently wide, the objective
function of the empirical-risk minimizer does not have any spurious local minima.
2.1	Feedforward Neural Networks
We consider input data from a domain Dx ⊂ Rd and output data from a domain Dy ⊂ Rm .
Typical examples are regression data with Dy = Rm and classification data with Dy = {±1}m .
We model the data with layered, feedforward neural networks, that is, We StUdy sets of functions
G =	{gθ	:	Dx	→ Rm : Θ ∈M}⊂G∙=	{gθ	:	Dx	→	Rm	: Θ ∈ M} with
gθ[x]∙=Θlfl [θl-1 ∙∙∙ f1[Θ0x]]	for X ∈Dx	(1)
and	_
M⊂M∙= {Θ = (Θl,..., Θ0) : Θj ∈ Rpj+1×pj}.
The quantities p0 = d and pl+1 = m are the input and output dimensions, respectively, l the depth
of the networks, and W ∙= min{p1,...,pl} the minimal width of the networks. The functions
fj : Rpj → Rpj are called the activation functions. We assume that the activation functions
are elementwise functions in the sense that fj[b] = (fj[bι],..., fj[bpj])> for all b ∈ Rpj, where
fj : R → R is an arbitrary function. This allows for an unlimited variety in the type of activation,
including ReLU fj : b → max{0, b}, leaky ReLU fj : b → max{0, b} + min{0, cb} for a fixed
2
Under review as a conference paper at ICLR 2021
C ∈ (0,1), polynomial P : b → Cbk for fixed C ∈ (0, ∞) and k ∈ [1, ∞), and sigmoid activation
F : b → 1/(1 + e-b) as popular examples, and it allows for different activation functions in each
layer.
We study the most common approaches to parameter estimation in this setting: constraint and
unconstraint empirical-risk minimization. The loss function l : Rm × Rm → R is assumed
convex in its first argument; this includes all standard loss functions, such as the least-squares loss
l : (a, b) 7→ ||a - b||22, the absolut-deviation loss l : (a, b) 7→ ||a - b||1 (both typically used for
regression), the logistic loss l : (a, b) 7→ -(1 + b) log[1 + a] - (1 - b) log[1 - a], the hinge loss
l : (a, b) 7→ max{0, 1 - ab} (both typically used for binary classification Dy = {±1}), and so forth.
The optimization domain is the set
M ∙= {Θ ∈M : r[Θ] ≤ 1}
for a constraint r : M → R. Given data (xι, yι),..., (xn, yn) ∈ Dχ X Dy, the empirical-risk
minimizers are then the networks gΘb
erm
with
Θb
erm
∈ arg min
Θ∈M
l gΘ[xi], yi
)
(2)
It has been shown that constraints can facilitate the optimization as well as improve generalization—
see Krizhevsky et al. (2012) and Livni et al. (2014), among others. For ease of presentation, we limit
ourselves to the following class of constraints:
r[Θ] ∙= maxnar	max	∣∣Θj∣∣∣1,br∣∣Θ0∣∣∣qO	for all Θ ∈ M
j∈{1,...,l}
(3)
for fixed tuning parameters ar, br ∈ [0, ∞), a parameter q ∈ (0, ∞], and || ∙ ||q the usual row-wise
'q“-norm,” that is, ∣∣∣Θj∣∣∣q ∙= max® (pi∣(θj)ki∣q)1/q for q ∈ (0, ∞) and ∣∣∣Θj∣∣∣∞ ：= maxki∣(Θj)ki|.
This class of constraints includes the following four important cases:
• Unconstraint estimation: ar = br = 0.
In other words, M = M. Unconstraint estimation had been the predominant approach in the earlier
days of deep learning and is still used today (Anthony & Bartlett, 1999).
• Connection sparsity: q = 1.
This constraint yields connection-sparse networks, which have received considerable attention
recently (Barron & Klusowski, 2018; 2019; Kim et al., 2016; Taheri et al., 2020).
• Strong sparsity: q < 1.
Nonconvex constraints have been popular in statistics for many years (Fan & Li, 2001; Zhang, 2010),
but our paper is probably the first one that includes such constraints in a theoretical analysis in deep
learning.
• Input constraints: ar = 0.
Some researchers have argued for applying certain constraints, such as node-sparsity, only to the
input level (Feng & Simon, 2017). In general, while our proof techniques also apply to many other
types of constraints, there are two main reasons for using the mentioned sparsity-inducing constraints
to illustrate our results: First, sparsity has become very popular in deep learning, because it can lower
the burden on memory and optimization as well as increase interpretability (Hebiri & Lederer, 2020).
And second, the above examples allow us to demonstrate that the discussed features of wide networks
do not depend on smooth and convex constraints such as weight decay.
Our theory can also be adjusted to the regularized versions of the empirical-risk minimizers, that is,
for the networks indexed by any parameter in the set
arg min
θ∈M
lgΘ[xi], yi + r[Θ]	.
3
Under review as a conference paper at ICLR 2021
The proofs are virtually the same as for the constraint versions; we omit the details for the sake of
brevity.
One line of research develops statistical theories for constraint and unconstraint empirical-risk
minimizers—see Bartlett & Mendelson (2002) and Lederer (2020), among others. As detailed above,
empirical-risk minimizers are the networks whose parameters are global minima of the objective
function
n
Θ → i[gθ] ∙= X i[gθ[χi], yi]	(4)
i=1
over M for fixed data (x1, y1), . . . , (xn, yn). While the function gΘ 7→ l[gΘ] is convex by as-
sumption, the objective function Θ 7→ l[gΘ] is usually nonconvex. It is thus unclear, per se,
whether deep-learning pipelines can be expected to yield global minima of the objective function
and, therefore, whether the statistical theories are valid in practice. Our goal is, broadly speaking, to
establish conditions under which global minimization of (4) can indeed be expected.
2.2	Absence of Spurious Local Minima
We now show that the objective function (4) has no spurious local minima if the networks are
sufficiently wide. Recall that a parameter Θ ∈ M that satisfies
l[gθ] ≤ l[gr]	for all Γ ∈ M with ∣∣∣Θ - Γ∣∣∣ ≤ C
for a constant C ∈ (0, ∞) and a norm ∣∣∣∙ ∣∣∣ on M is called a local minimum of the objective function (4).
If the statement holds for every c ∈ (0, ∞), the parameter Θ is called a global minimum. Objective
functions in deep learning have typically many local and global minima; an important question is
whether there are “bad” local minima, that is, suboptimal local minima that are difficult to escape
from. We formalize this notion as follows:
Definition 1 (Spurious local minima). Let Θ ∈ M be a local minimum of the objective function (4).
If there is no continuous function h : [0, 1] → M that satisfies (i) h[0] = Θ and h[1] = Γ for a
global minimum Γ ∈ M of the objective function (4) and (ii) t 7→ l[gh[t]] is nonincreasing, we call
the parameter Θ a spurious local minimum.
See again Figure 1 for an illustration.
The following theorem is our main result:
Theorem 1 (Absence of spurious local minima). Consider the setup ofSection 2.1. If W ≥ 2m(n+1)l,
the objective function (4) has no spurious local minima.
In other words, empirical-risk minimization over sufficiently wide networks does not involve spurious
local minima. Hence, as long as there are means to circumvent saddle points (Dauphin et al., 2014),
it is reasonable to expect that algorithms can find a global minimum and, therefore, that the known
statistical theories for empirical-risk minimizers apply in practice.
The theorem applies very broadly. First, it includes all local minima rather than “many” or “almost
all” local minima. This feature is important, because even randomized algorithms usually converge
to a few, fixed points with high probability. Second, the framework allows for arbitrary convex
loss functions. This feature caters, for example, to a current trend toward robust alternatives of the
least-squares loss function (Barron, 2019; Lederer, 2020). Third, the framework includes ReLU
activation. ReLU activation is nondifferentiable and, therefore, mathematically more challenging
than, for example, linear and polynomial activation, but it has become the predominant type of
activation in practice. Forth, the framework includes constraint as well as unconstraint estimation.
Constraint estimation is particularly suitable for wide networks, and for overparameterized networks
more generally, because it can avoid overfitting and facilitate optimizations. Fifth, our statement
holds for arbitrary output dimensions and depths. The latter is particularly important in view of the
current trend toward deep architectures. In sum, our result is a sweeping proof of the fact that wide
networks have no spurious local minima, and it sheds light on the optimization landscapes of deep
learning more generally.
The bound on the network widths becomes 2m(n + 1)l = 2(n+ 1) in the case ofa single output (m =
1) and a single hidden layer (l = 1), which coincides with the bounds that have been established for
4
Under review as a conference paper at ICLR 2021
shallow networks with one output and specific activation functions and estimators—see Lacotte &
Pilanci (2020) and references therein. Thus, our theory applies extremly broadly and still gives the
expected results in the simple cases. In fact, our proofs only require one layer to have a width of at
least 2m(n + 1)l, but instead of losing ourselves in technical details about the condition, we focus on
the main message of Theorem 1: optimizations become easier with increasing widths.
3	Underlying Concepts
In this section, we introduce concepts that we use in our proofs and that might also be of interest more
generally. We first formulate the notion of path equivalence, which yields a practical characterization
of spurious local minima. We then formulate specific parameters that can act as mediators between
path-equivalent parameters. The main reason why our proof techniques are quite different from what
can be found in the literature is that we cater to deep networks and a range of activation functions.
3.1	Path Relations
The objective functions for optimizing neural networks are typically continuous but not convex or
differentiable. In the following, we characterize the absence of spurious local minima in a way
that suits these characteristics of neural networks. The key concept is formulated in the following
definition.
Definition 2 (Path relations). Consider two parameters Θ, Γ ∈ M. If there is a continuous function
hΘ,Γ : [0, 1] → M that satisfies hΘ,Γ[0] = Θ, hΘ,Γ[1] = Γ, and t 7→ l[ghΘ,Γ[t]] is constant, we
say that Θ and Γ are path constant and write Θ什Γ.
If there is a continuous function hΘ,Γ : [0, 1] → M that satisfies hΘ,Γ[0] = Θ, hΘ,Γ[1] = Γ, and
t 7→ l[ghΘ,Γ[t]] is convex, we say that Θ and Γ are path convex and write Θ^Γ.
Ifthere are parameters Θ0, Γ0 ∈ M such that (i) Θ什Θ0 and Γ什Γ0 and (ii) Θ0^Γ0, we say that Θ
and Γ are path equivalent and write Θ!Γ.
Path constantness means that two parameters are connected by a continuous path of parameters that
is constant with respect to the loss; path convexity relaxes “constant” to “convex;” path equivalence
allows for additional mediators. The three relations are ordered in the sense that Θ什Γ ⇒ Θ^Γ ⇒
Θ!Γ, and they satisfy a number of other basic properties.
Lemma 1 (Basic properties). It holds for all Θ, Γ, Ψ ∈ M that
1.	Θ什Θ; Θ^Θ; and θ!θ (reflexivity);
2.	Θ»Γ ⇒ Γ什Θ; Θ^Γ ⇒ Γ^Θ; and Θ^Γ ⇒ Γ^Θ (symmetry).
3.	Θ»Γ and Γ什Ψ ⇒ Θ什Ψ (transitivity).
The proof is straightforward and, therefore, omitted. The lemma illustrates that the path relations
equip the parameter space with solid mathematical structures.
We can finally use the above-stated concepts to characterize spurious local minima.
Proposition 1 (Characterization of spurious local minima). Assume that for all Θ ∈ M, there is a
global minimum of the objective function (4), denoted by Γ, such that Θ!Γ. Then, the objective
function (4) has no spurious local minima.
Hence, path equivalence of all parameters to a global minimum is a sufficient condition for the
absence of spurious local minima. This statement is the main result of Section 3.1.
3.2	Block Parameters
The parameterization of neural networks is typically ambiguous: many different parameters yield the
same network. We leverage this ambiguity to make the networks more tractable. The key concept is
formulated in the following definition.
5
Under review as a conference paper at ICLR 2021
Figure 2: Illustration of the parameters of an s-upper block parameter (left) and an s-lower block
parameter (right) for l = 2. The dark areas of the matrices can consist of arbitrary values; the light
areas consist of zeros.
Definition 3 (Block parameters). Consider a number s ∈ {0, 1, . . . } and a parameter Θ ∈ M. If
1.	(Θ0)ji = 0 for all j > s;
2.	(Θv)ij = 0 for all v ∈ {1, . . . , l - 1} and i > s and for all v ∈ {1, . . . , l - 1} andj > s;
3.	(Θl)ij = 0 for all j > s,
we call Θ an s-upper-block parameter of depth l.
Similarly, if
1.	(Θ0)ji	= 0 for all j ≤	p1 - s;
2.	(Θv)ij	= 0 for all v ∈	{1, . . . , l	- 1} and i ≤ pv+1	- s and for all v ∈ {1,	. . . , l - 1} and
j ≤ pv - s;
3.	(Θl)ij = 0 for all j ≤ pl - s,
we call Θ an s-lower-block parameter of depth l. We denote the sets of the s-upper-block and
s-lower-block parameters of depth l by Us,l and Ls,l, respectively.
Trivial examples are the 0-block parameters U0 = L0 = {0 = (0pl+1 ×pl , . . . , 0p1×p0)} and the s-
block parameters Us,l = Ls,l = M for s ≥ max{p1, . . . ,pl}. More generally, the block parameters
consist of block matrices: see Figure 2. We show in the following that block parameters can be
mediators in the sense of path equivalence.
We first show that every parameter is path constant to a block parameter.
Proposition 2 (Path connections to block parameters). For every Θ ∈ M and S ∙= m(n + 1)l, there
are Θ, Θ ∈ M with Θ ∈ Us,ι and Θ ∈ Ls,ι such that Θ6Θ and ΘθΘ.
In particular, every parameter is path connected to both an upper-block parameter and a lower-block
parameter. The interesting cases are wide networks: for fixed S, the wider the network, the more
pronounced the block structure.
We then show that there is a connection between upper-block and lower-block parameters.
Proposition 3 (Path connections among block parameters). Consider two block parameters Θ ∈ Us,ι
and Γ ∈ Ls,ι. If W ≥ 2s, it holds that Θ^Γ.
Hence, every upper-block parameter is path connected to every lower-block parameter—as long as
the minimal width of the networks is sufficiently large.
We finally combine Propositions 2 and 3.
Corollary 1 (All parameters are path equivalent). Consider two arbitrary parameters Θ, Γ ∈ M. If
w ≥ 2m(n + 1)l, it holds that Θ^Γ.
See Figure 3 for an illustration. The corollary ensures that as long as the minimal width is sufficiently
large, all networks are path equivalent. This result, therefore, connects directly to the characterization
of spurious local minima in Proposition 1 of the previous section.
6
Under review as a conference paper at ICLR 2021
l[gΨ]
Θ Θ Θ0	Γ0 Γ Γ
Figure 3: path-equivalence between two parameters Θ and Γ—see Corollary 1 and Definition 2
4	Auxilliary Results
In this section, we state four auxilliary results.
4.1	Two-Layer Networks
Here, we show that two-layer networks can be reparametrized such that they are indexed by block
parameters. We first introduce the notation
r[M,q] ∙= ∣∣∣M∣∣∣q =	max (χ∣Maj∣q!	for all M ∈ Rb×c, q ∈ (0, ∞)
a∈{i,…,b} ∖ j=1
and r[M, ∞] ∙= ∣∣∣M∣∣∣∞ = maXαj∣Maj| for all M ∈ Rb×c. These functions are the building
blocks of the constraint on Page 3. Next, given a permutation p : {1, . . . , c} → {1, . . . , c} and a
matrix M ∈ Rb×c, we define the matrix MP ∈ Rb×c through (Mp)j ：= Mipj]. Similarly, given a
permutation p : {1, . . . , b} → {1, . . . , b} and a matrix M ∈ Rb×c, we define the matrix Mp ∈ Rb×c
through (Mp)ji ∙= Mpj]i. The result is then the following:
Lemma 2 (Two-Layer networks). Consider three matrices A ∈ Ru×v, B ∈ Rv×o, and C ∈ Ro×r,
two constants qA ∈ (0, 1] and qB ∈ (0, ∞], and a function h : R → R. With some abuse of notation,
define h : Rv×r → Rv×r through (h[M])ji ：= h[Mji] for all M ∈ Rv×r. Then, there are matrices
A ∈ Ru×v and B ∈ Rv ×o and a permutation P : {1,...,v} → {1,...,v} such that
•	AhBC ] = Aph[BpC ];
•	r[A,qa] ≤ r[Ap,qa] andr[B,qB] ≤ r[Bp,qB]；
•	Aij = 0 for j > u(r + 1); Bji = 0 for j > u(r + 1) and Bji = (Bp)ji otherwise.
Similarly, there are matrices A ∈ Ru×v and B ∈ Rv ×o and a permutation P : {1,...,v}→
{1, . . . , v} such that
•	Ah[BC ] = Aph[BpC ];
•	r[A,qa] ≤ r[Ap,qa] andr[B,qB] ≤ r[Bp,qB];
•	Aij = 0 for j ≤ V — u(r + 1); Bji = 0 for j ≤ V — u(r + 1) and Bji = (Bp)ji otherwise.
Hence, the parameter matrices of two-layer networks can be brought into the shapes illustrated in
Figure 2. We apply this result repeatedly in the proof of Proposition 2.
4.2	S ymmetry Property of Neural Networks
Next, we point out a symmetry in our setup for the neural networks.
Lemma 3 (Symmetry property). Consider permutations Pj : {1, . . . , pj} → {1, . . . , pj} for
j ∈ {0, . . . , l + 1}. Assume that P0 and Pl+1 are the identity functions: P0[j] = Pl+1[j] = j for
all j. The parameter Θ ∈ M is a spurious local minimum of the objective function (4) if and only if
Γ ∈ M defined through (Pj)UV ∙= (Θj)pj+i[u]pj[v] for all j ∈ {0,...,l}, U ∈ {1,... ,pj+1} ,and
V ∈ {1, . . . ,pj} is a spurious local minimum of the objective function (4).
7
Under review as a conference paper at ICLR 2021
The proof follows readily from our setup in Section 2.1 and, therefore, is omitted. The lemma
illustrates that the parameterizations of neural networks are highly ambiguous. But in this case, the
ambiguity is convenient, because it allows us to permute the rows and columns of the parameters to
bring the parameters in shapes that are easy to manage.
4.3	Property of Convex Functions
We now establish a simple property of convex functions.
Lemma 4 (Property of convex functions). Consider a convex function h : [0,1] → R .If h[0] > h[t]
for a t ∈ (0,1], there ISa C ∈ argmmt∈(oj]{h[t]} such that the Junction h : [0,1] → R defined
through h[t] ∙= h[ct] Jorall t ∈ [0,1] is nonincreasing and h[0] > h[1].
This lemma connects the convexity from Definition 2 with the spurious local minima from Definition 1.
We use this result in the proof of Proposition 1.
4.4	CARATHEODORY-TYPE RESULT
Caratheodory's theorem goes back to Caratheodory (1911); see Boltyanski & Martini (2001); Fenchel
(1929); Hanner & RadStrOm (1951) for related results. The following statement combines the classical
theorem and the much more recent results Bastero et al. (1995, Theorem 1 and Lemma 1).
Lemma 5 (Caratheodory-Type result). Consider a number q ∈ (0,1], vectors zι,..., Zh ⊂ Rr, and
the vectors’ q-convex hull Convq[zι,..., zh] ∙= {Pjh=ι Wjzj : t ∈ [0,1]h,同q = 1}. Then, every
vector v ∈ convq[z1, . . . , zh] can be written as v = Pjh=1 tjzj with t ∈ [0, 1]h, ||t||q ≤ 1, and
#{j ∈ {1, . . . , h} : tj 6= 0} ≤ r + 1.
The cardinality of a set A is denoted by #{A} and the 'q-norm of a vector V ∈ Ra by ||v|q ∙=
(Pja=1 |vj |q)1/q. The lemma follows readily from the mentioned results; therefore, its proof is
omitted. The lemma states that every vector in the q-convex hull is a q-convex combination of at
most r + 1 vectors from the set of vectors that generate the q-convex hull. (Note that for q < 1,
our definition of the q-convex hull is more restrictive than the standard definition—cf. Bastero et al.
(1995, Remark on Page 142)—but it leads to a concise statement and is sufficient for our purposes.)
5	Discussion
Empirical evidence has long suggested that wide networks are comparatively easy to optimize. In
this paper, we underpin these observations with rigorous theory: we prove that the optimization
landscapes of empirical-risk minimization over wide networks have no spurious local minima.
Standard competitors to deep learning are “classical” high-dimensional estimators, such as the
ridge and the lasso (Hoerl & Kennard, 1970; Tibshirani, 1996), that have been studied in statistics
extensively (Zhuang & Lederer, 2018). A common argument for these estimators is that their objective
functions are often convex or equipped with efficient algorithms for optimization (Bien et al., 2018;
Friedman et al., 2010), but our results indicate that global optimization of the objective functions in
deep learning can be feasible as well.
Our framework allows for arbitrary depths, for constraint as well as unconstraint estimation, essentially
arbitrary activation, and for a very wide spectrum of loss functions and input and output data.
This generality demonstrates that the absence of spurious local minima is not a feature of specific
estimators, network functions, or data but, instead, a universal property of wide networks. Our theory,
therefore, supports the use of wide networks in general—possibly together with regularization or
constraints to avoid overfitting.
The main idea of most approaches in the field is to construct basis functions for the networks. In
contrast, we formulate parametrizations that make the networks easy to work with. We could thus
envision testing our new concepts beyond the presented application.
8
Under review as a conference paper at ICLR 2021
References
Z. Allen-Zhu, Y. Li, and Z. Song. A convergence theory for deep learning via over-parameterization.
In Proc. ICML,pp. 242-252, 2019.
M. Anthony and P. Bartlett. Neural network learning: theoretical foundations. Cambridge University
Press, 1999.
A. Barron and J. Klusowski. Approximation and estimation for high-dimensional deep learning
networks. arXiv:1809.03090, 2018.
A. Barron and J. Klusowski. Complexity, statistical risk, and metric entropy of deep nets using total
path variation. arXiv:1902.00800, 2019.
J. Barron. A general and adaptive robust loss function. In Proc. CVPR, pp. 4326-4334, 2019.
P. Bartlett and S. Mendelson. Rademacher and Gaussian complexities: risk bounds and structural
results. J. Mach. Learn. Res., 3:463-482, 2002.
J. Bastero, J. Bernues, and A. Pena. The theorems of Caratheodory and Gluskin for 0 < p < 1. In
Proc. Amer. Math. Soc., pp. 141-144, 1995.
V. Belagiannis, C. Rupprecht, G. Carneiro, and N. Navab. Robust optimization for deep regression.
In Proc. ICCV, pp. 2830-2838, 2015.
J. Bien, I. Gaynanova, J. Lederer, and C. Muller. Non-convex global minimization and false discovery
rate control for the trex. J. Comput. Graph. Statist., 27(1):23-33, 2018.
V. Boltyanski and H. Martini. Caratheodory's theorem and h-convexity. J. Combin. Theory Ser A,
93:292-309, 2001.
C. Caratheodory. Uber den VriabiIitatSbereiCh der Fourier schen Konstanten von POSitiVen harmonis-
chen Funktionen. Rend. Circ. Mat. Palermo, 32(1):193-217, 1911.
Y. Dauphin, R. Pascanu, C. Gulcehre, K. Cho, S. Ganguli, and Y. Bengio. Identifying and attacking the
saddle point problem in high-dimensional non-convex optimization. In Proc. NIPS, pp. 2933-2941,
2014.
S. Du, X. Zhai, B. Poczos, and A. Singh. Gradient descent provably optimizes over-parameterized
neural networks. In Proc. ICLR, 2019.
J. Fan and R. Li. Variable selection via nonconcave penalized likelihood and its oracle properties. J.
Amer. Statist. Assoc., 96(456):1348-1360, 2001.
W. Fenchel. Uber Krummung und WindUng geschlossener Raumkurven. Math. Ann., 101:238-252,
1929.
J. Feng and N. Simon. Sparse-input neural networks for high-dimensional nonparametric regression
and classification. arXiv:1711.07592, 2017.
J. Friedman, T. Hastie, and R. Tibshirani. Regularization paths for generalized linear models via
coordinate descent. J. Stat. Softw., 33(1):1, 2010.
I. Goodfellow, O. Vinyals, and A. Saxe. Qualitatively characterizing neural network optimization
problems. arXiv:1412.6544, 2014.
O. Hanner and H. Radstrom. A generalization of a theorem of Fenchel. In Proc. Amer. Math. Soc.,
volume 2, pp. 589-593, 1951.
T. Hastie, R. Tibshirani, and M. Wainwright. Statistical learning with sparsity: the lasso and
generalizations. CRC press, 2015.
M. Hebiri and J. Lederer. Layer sparsity in neural networks. arXiv:2006.15604, 2020.
A. Hoerl and R. Kennard. Ridge regression: biased estimation for nonorthogonal problems. Techno-
metrics, 12(1):55-67, 1970.
9
Under review as a conference paper at ICLR 2021
L. Jiang, Z. Zhou, T. Leung, L.-J. Li, and L. Fei-Fei. Mentornet: learning data-driven curriculum for
very deep neural networks on corrupted labels. In Proc. ICML,pp. 2304-2313, 2018.
J. Kim, V. Calhoun, E. Shim, and J.-H. Lee. Deep neural network with weight sparsity control and
pre-training extracts hierarchical features and enhances classification performance: Evidence from
whole-brain resting-state functional connectivity patterns of schizophrenia. Neuroimage, 124:
127-146, 2016.
A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classification with deep convolutional neural
networks. In Proc. NIPS, pp. 1097-1105, 2012.
J. Lacotte and M. Pilanci. All local minima are global for two-layer ReLU neural networks: the
hidden convex optimization landscape. arXiv:2006.05900, 2020.
Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521:436-444, 2015.
J. Lederer. Risk bounds for robust deep learning. arXiv:2009.06202, 2020.
R. Livni, S. Shalev-Shwartz, and O. Shamir. On the computational efficiency of training neural
networks. In Proc. NIPS, pp. 855-863, 2014.
Q.	Nguyen and M. Hein. Optimization landscape and expressivity of deep CNNs. In Proc. ICML, pp.
3730-3739, 2018.
D. Soudry and Y. Carmon. No bad local minima: data independent training error guarantees for
multilayer neural networks. arXiv:1605.08361, 2016.
M. Taheri, F. Xie, and J. Lederer. Statistical guarantees for regularized neural networks.
arXiv:2006.00294, 2020.
R.	Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc. Ser. B. Stat. Methodol.,
58(1):267-288, 1996.
L. Venturi, A. Bandeira, and J. Bruna. Spurious valleys in one-hidden-layer neural network optimiza-
tion landscapes. J. Mach. Learn. Res., 20(133):1-34, 2019.
Z. Wang, S. Chang, Y. Yang, D. Liu, and T. Huang. Studying very low resolution recognition using
deep networks. In Proc. CVPR, pp. 4792-4800, 2016.
C.-H. Zhang. Nearly unbiased variable selection under minimax concave penalty. Ann. Statist., 38(2):
894-942, 2010.
R. Zhuang and J. Lederer. Maximum regularized likelihood estimators: a general prediction theory
and applications. Stat, 7(1):e186, 2018.
10
Under review as a conference paper at ICLR 2021
A	Proofs
A.1 Proof of Theorem 1 and an Extension
Proof of Theorem 1. The proof combines the main results of Sections 3.1 and 3.2. Let Θ ∈ M
be an arbitrary parameter and Γ ∈ M a global minimum of the objective function (4). In view of
Proposition 1 in Section 3.1, we need to show that Θ!Γ, and this follows directly from Corollary 1
in Section 3.2.	□
Figure 1 illustrates that a nonconvex objective function can have spurious local minima, but Theorem 1
proves that this is not the case for deep learning with wide networks. Figure 1 also illustrates that a
nonconvex objective function can have “disconnected” global minima, but, in view of Corollary 1 and
our results more generally, we can rule out this case as well. In other words, the set of global minima
is “connected” in the sense that it is a set of path-constant parameters. The connectedness of the
global minima is of minor importance in practice but an interesting topological property nevertheless.
A.2 Proof of Proposition 1
Proof of Proposition 1. The key idea is to exploit the properties of monotone functions and convex
functions. Let Θ ∈ M be an arbitrary parameter and Γ ∈ M a global minimum of the objective
function (4) such that Θ!Γ. Let Θ0, Γ0 ∈ M and hΘ0,Γ0 : [0, 1] → M be as described in
Definition 2. Note first that, since Γ什Γ0 (see Definition 2), the parameter Γ0 is also a global
minimum of the objective function. We then use 1. simple algebra, 2. the assumed convexity of the
function t 7→ l[ghΘ0,Γ0] (see Definition 2 again), 3. the assumed endpoints of the function hΘ0,Γ0
(see Definition 2 once more), 4. the fact that Γ0 is a global minimum of (4), and 5. the fact that
(1 - t) + t = 1 to derive for all t ∈ [0, 1] that
l[ghθ0,ro [t]] = l[ghθ0,ro [(i-t)∙o+t∙i]]
≤ (1 - t)lghΘ0,Γ0[0] +tlghΘ0,Γ0[1]
= (1 - t)lghΘ0,Γ0[0] + tl[gΓ0]
≤ (1 - t)lghΘ0,Γ0[0] +tlghΘ0,Γ0[0]
= l ghΘ0,Γ0[0] .
Assume first that the inequality is strict: l[ghOrO困] < l[gh^^⑼]for a t ∈ [0,1]. Then,
by the assumed convexity of t 7→ l[gh 0 0 [t]] (see Definition 2) and Lemma 4, there is a
C ∈ argmmt∈(0,i]{l[ghθor[t]]} SUCh that h : [0,1] → R defined through h[t] ：= 1^^『[心]
for all t ∈ [0,1] is nonincreasing, h[1] is a global minimum of the objective function (4), and
h[0] = l[ghθo,ro[O]] = l[gθo] > E[1] = l[ghθo,ro[c]]. Hence, the function h : [0,1] → M defined
through h[t] ∙= hθo,ro [ct] for all t ∈ [0,1] is a function that satisfies the conditions of Definition 1 for
the parameter Θ0 and the global minimum ⅛[1]. Combining this result with the assumed relationship
Θ什Θ0 (see Definition 2) yields——see Definition 1——the fact that Θ is not a spurious local minimum
of the objective function (4).
We can thus assume that t → l[gh@0「0罔]is constant, which implies Θ0θΓ0 by Definition 2. The
fact that Θ什Θ0 (see Definition 2 again) and the transitivity of the path constantness (see Property 3
in Lemma 1) then yield the fact that Θ什Γ0. Hence, Θ is a global minimum and, therefore, not a
spurious local minimum——see Definition 1 again.	□
A.3 Proof of Proposition 2
Proof of Proposition 2. Our proof strategy is to apply Lemma 2, which is designed for one individual
layer, layer by layer. We first introduce some convenient notation. We define, with some abuse
of notation, fj : Rpj×n → Rpj×n through (fj[M])uv ∙= fj[Muv] for all j ∈ {1,...,l}, U ∈
{1,...,pj}, V ∈ {1,...,n}, and M ∈ Rpj ×n. We also define the data matrix X ∈ Rd×n through
11
Under review as a conference paper at ICLR 2021
Xji ∙= (xi)j for all j ∈ {1,...,d} and i ∈ {1,..., n}, that is, each column of X consists of one
sample. We finally write
gθ[X ] ∙= (gθ[x1],..., gθ[xn]) = θlfl[θl-1 …f1[θ0X ]] ∈ Rm×n	for all Θ ∈M .
Hence, gΘ[X] summarizes the network’s outputs for the given data.
Given a parameter Θ ∈ M,we establish a corresponding upper-block parameter Θ ∈ Us,l layer by
layer, starting from the outermost layer. We write
gθ[X]=	3 f [	θ{-}汇
; A∈Rpl + 1×pl =：h	;B∈Rpl ×Pl-1
[θl-2 …f1[Θ0X]]].
{^^^^^^≡
= ：C∈Rpl-1×n
Lemma 2 for two-layer networks then gives (by Lemma 3, we can assume without loss of generality
the fact that p is the identity function, that is, Ap = A and Bp = B)
gθ[X ] =Θlfl
θθ1)fl-1[θl-2 …f1[θ0X]]
for a matrix θl ∈ Rpl+1×pl that satisfies r[θl, 1] ≤ r[θl, 1] (recall the definition of r on Page 7)
and meets Condition 3 in the first part of Definition 3 on block parameters as long as s ≥ pl+1(n +
1) = m(n + 1), and for a matrix θl-1 ∈ Rm(n+1)×pl 1 that satisfies r[θl-1,1] ≤ r[θl-1, 1] (or
r[θl 1, q] ≤ r[θl 1 ,q] if l = 1) and consists of the first m(n + 1) rows of the matrix θl 1. (We
implicitly assume here and in the following pj ≥ m(n + 1)l-j+1 for all j ∈ {1, . . . , l}—which is
the generic case in view of Corollary 1—to keep the notation manageable, but extending the proof to
the general case is straightforward.)
Now, define a parameter Γl ∈ M through
Γl ∙= (θl, θl-1,..., θ0)
and a function h®,p : [0,1] → M through
hθ,ri [t] ∙= (1 - t)Θ + tΓl	forall t ∈ [0,1].
The function hΘ,Γl is continuous and satisfies hΘ,Γl [0] = Θ and hΘ,Γl [1] = Γl. Moreover, we can
1. use the definitions of the function hΘ,Γl and the networks, 2. split the network along the outermost
layer, 3. invoke the block shape of θl and the definition of θl-1 as the m(n + 1) first rows of the
matrix θl-1, 4. use the above-stated inequalities for the network gΘ[X], and 5. consolidate the terms
to show for all t ∈ [0, 1] that
ghθ,rι [t] [X ] = ((1- t)θl + tθl)fl [θl-1 …f1 [θ0x ]]
=(1 - t)θlfl [θl-1 …f1 [θ0X]] + tθlfl [θl-1 …f1 [θ0X]]
(1 - t)Θlfl [Θl-1 …f1 [Θ0X]] + tΘlfl
广 01)…f1[θ0χ ]
= (1 - t)gΘ [X] + tgΘ [X]
= gΘ[X] .
Hence, the function t 7→ l[gh l [t]] is constant. Finally, we use 1. the definition of the constraint
in (3), 2. the definition of the function hθ,ri, 3. the convexity of the 'ι-norm, 4. the above stated fact
that r[Θl, 1] ≤ r[Θl, 1], 5. a consolidation, 6. again the definition of the regularizer, and 7. the fact
that Θ ∈ M to show for all t ∈ [0,1] that
r[hΘ,Γl [t]] = max ar
= max ar
≤ max ar
j∈max,4 V (her [tDj %，MI (hθ,rl [t])0U∣q}
max
j∈{1,...,l-1}
max
j∈{1,...,l-1}
l∣θjll∣ι,ar∣ll(1 - t)θl + tθl∣∣ι ,br∣∣∣θ0∣∣q}
l∣θj∣∣1, (1 - t)ar∣∣θll∣1 + tar∣∣θll∣1,br∣l∣θ0∣∣q 0
12
Under review as a conference paper at ICLR 2021
≤ max(ar	max	∣∣θj∣∣ι, (1 - t)ar∣∣θl∣∣ι + tar∣∣θl∣∣ι,br∣∣θ0∣∣q∣
j∈{1,...,l-1}
= maxnar	max	∣∣∣Θj∣∣∣1,ar∣∣∣Θl∣∣∣1,br∣∣∣Θ0∣∣∣qo
j∈{1,...,l-1}
= r[Θ]
≤1.
Hence, hΘ,Γl [t] ∈ M for all t ∈ [0, 1]. In conclusion, we have shown—see Definition 2—that Θ
and Γl are path constant: Θ什Γl.
We then move one layer inward. Lemma 2 ensures that (recall again Lemma 3)
Θel-1
θ{-}…f1[Θ0X]] =Θ lTflτ
z^^^^^r^
A∈Rm(n+1)×pl-1	h	B∈Rpl-1 ×pl-2 C∈Rpl-2×n
θ ；) ∙∙∙f1[θ0χ]
for a matrix Θl-1 ∈ Rm(n+1)×pl 1 that satisfies r[Θl-1,1] ≤ r[Θl-1,1] ≤ r[Θl-1,1] and meets
Condition 3 in the first part of Definition 3 on block parameters as long as s ≥ m(n + 1)2, and for
a matrix Θl-2 ∈ RmS+D2×pl 2 that satisfies r[Θl-2,1] ≤ r[Θl-2,1] (or r[Θl-2,q] ≤ r[Θl-2,q] if
l = 2) and consists of the first m(n + 1)2 rows of the matrix Θl-2.
Next, We define Θl-1 ∈ Rpl×pl 1 through (Θl-1)uv ∙= (Θl-1)uv for U ≤ m(n + 1) and
(Θl-1)uv ∙= 0 otherwise. Combining this definition with the above-derived results yields
gθ[X]=Θlfl Θl-1fl-1
and the matrix Θl-1 satisfies r[Θl-1,1] = r[Θl-1,1] ≤ r[Θl-1,1] and meets Condition 2 in the first
part of Definition 3 on block parameters as long as s ≥ m(n + 1)2.
(θ0-2)…MX],
Similarly as above, define a parameter Γl-1 ∈ M through
Γl-1 ∙= (Θl, Θl-1, Θl-2,..., Θ0)
and a function hri,ri-1 : [0,1] → M through
hri,ri-1 [t] ∙=(1 - t)Γl + tΓl-1	for all t ∈ [0,1]
to show that Γl什Γl-1. In view of Property 3 in Lemma 1, we can conclude that Θ什Γl-1
Finish the proof by induction over the layers, and note that the lower-block parameters can be
established in the same way.	□
A.4 Proof of Proposition 3
Proof of Proposition 3. The key ingredient of the proof is the assumed block structure of the parame-
ters. Consider two block parameters Θ ∈ Us,l and Γ ∈ Ls,l and define the parameters
θ0 ∙= (Θl,Θl-1 + Γl-1,...,θ0 + γ0) ∈M；
γ0 ∙= (Γl,Θl-1 + Γl-1,...,θ0 + γ0) ∈M
and the function
hθo,ro ： [0,1] → M
t → (1 - t)Θ0 + tΓ0 = ((1 - t)Θl + tΓl, Θl-1 +Γl-1,..., Θ0 +Γ0).
By the row-wise structure of the constraint (see Page 3 again), the convexity of the 'ι-norm,
and the block shapes of the parameters (see Figure 2 again), we can find that r[hΘ0,Γ0 [t]] ≤
max{r[Θ], r[Γ]} ≤ 1 for all t ∈ [0, 1], that is, hΘ0,Γ0 [t] ∈ M for all t ∈ [0, 1]. One can also
verify readily the fact that the function hΘ0,Γ0 is continuous, hΘ0,Γ0 [0] = Θ0, and hΘ0,Γ0 [1] = Γ0.
Next, we define
[Θ0, Γ0]c1,c2 ∙= (cιΘl + C2 Γl, Θl-1 + Γ l-1,..., Θ0 +Γ0) ∈M	for all c1,c2 ∈ R,
13
Under review as a conference paper at ICLR 2021
which generalizes hΘ0,Γ0 in the sense that hΘ0,Γ0 [t] = [Θ0, Γ0]1-t,t for all t ∈ [0, 1]. We then
1. invoke the definition of [Θ0, Γ0]c1,c2 and the definition of the networks in (1), 2. split the network
along the outer layer, 3. use the block structures of the parameters and the assumption that fj [b] =
(fj[bι],...,fj[bpj])> for all j ∈ {1,...,l} and b ∈ Rpj, 4. continue in this fashion, and 5. invoke
again the definition of the networks in (1) to find for all c1,c2 ∈ R and X ∈Dχ that
g[Θ0,Γ0]c1,c2 [x]
=(cιΘl + C2Γl)fl (Θl-1 + Γl-1)fl-1 h…f1 [(Θ0 + Γ0)x] i
=c1Θlfl [(Θl-1 + Γl-1)flτ […f1 [(Θ0 + Γ0)x]i
+ c2Γlfl (Θl-1 + Γl-1)fl-1 […f1 [(Θ0 + Γ0)x]i
=c1Θlfl Θl-1flτ […f1 [(Θ0 + Γ0)x]i + c2Γlfl Γl-1flτ […f1 [(Θ0 + Γ0)x]i
Cιθlfl Θl-1fl-1 […f1[θ0x]]	+ c2Γlfl
Γl-1flτ[…f1 [Γ0x]]
= c1gΘ[x] + c2gΓ[x] .
Finally, we use 1. the definition of the function hΘ0,Γ0 and of the parameter [Θ0, Γ0]c1,c2, 2. the above
display with c1 = 1 - (1 - a)t1 - at2 and c2 = (1 - a)t1 + at2, 3. a rearrangement of the terms,
4. the assumed convexity of the loss function l, 5. again the above display, and 6. again the definition
of hΘ0
,Γ0, to find for all a, t1, t2 ∈ [0, 1] that
l ghΘ0,Γ0 [(1-a)t1 +at2]
= l g[Θ0,Γ0]
1-(1-a)t1 -at2 ,(1-a)t1 +at2
=l [(1 — (1 — a)tι — at2)gθ + ((1 — a)tι + at2)gr]
=l[((I - a) - (I - a)tl)gΘ + (a - at2)gΘ + (I - a)t1 gr + at2gr]
=l h(1 - a) ((1 - 11)gΘ + t1gr) + a((1 - t2)gΘ + t2gr)]
≤ (1 - a)l[(1 - t1)gΘ + t1gΓ] + al[(1 - t2)gΘ + t2gΓ]
= (1 - a)l[g[Θ0,Γ0]1-t1,t1 ] + al[g[Θ0,Γ0]1-t2,t2]
= (1 - a)l[ghΘ0,Γ0[t1]] + al[ghΘ0,Γ0[t2]] ,
which means that t 7→ l[gh 0 0 [t]] is convex. We conclude—see Definition 2—that Θ0^Γ0.
In view of Definition 2, it is left to show that Θ什Θ0 and Γ什Γ0. Consider now the function
hθ,θo ： [0,1] → M
t 7→ (Θl,Θl-1 +tΓl-1, . . . ,Θ0 +tΓ0) .
By the row-wise structure of the constraint (see Page 3 once more) and the block shapes of the
parameters (see Figure 2 once more), we can find that r[hΘ,Θ0 [t]] ≤ max{r[Θ], r[tΓ]} ≤ 1 for
all t ∈ [0, 1], that is, hΘ,Θ0 [t] ∈ M for all t ∈ [0, 1]. One can also verify readily the fact that the
function hΘ,Θ0 is continuous, hΘ,Θ0 [0] = Θ, and hΘ,Θ0 [1] = Θ0. Moreover, we can 1. invoke
the definition of hΘ,Θ0 and the definition of the networks in (1), 2. use the block structures of the
parameters and the elementwise structure of the activation function, 3. continue in this fashion, and
4. invoke the definitions of the function hΘ,Θ0 and the networks in (1) again to find for all t ∈ [0, 1]
and x ∈ Dx that
ghθ,θθ[t] [x] = Θlfl (Θl-1 + tΓl-1)flτ […f1 [(Θ0 + tΓ0)x]]
Θlfl
Θl-1flτ [∙∙∙f1[(θ0 + tΓ0)x]]
14
Under review as a conference paper at ICLR 2021
=…= Θlfl Θl-1fl-1 […f1[θ0x]]
= ghΘ,Θ0 [0] [x] ,
which implies that the function t 7→ l[gh	0 [t]] is constant. We conclude that—see Definition 2—
that Θ什Θ0.
We can show in a similar way that Γ什Γ0. Hence, given Definition 2, We find that Θ!Γ, as
desired.	□
A.5 Proof of Lemma 2
Proof of Lemma 2. The proof is essentially a careful reparametrization. We proceed in three steps:
see Figure 4 for an overview.
Step 1: Fix a k ∈ {1, . . . , u}. We first show that there is a matrix A ∈ Ru×v such that
, - - -
1.	Ah[BC] = Ah[BC];
2.	r[A,qA] ≤ r[A,qA];
3.	#{j ∈{1,...,v} : Akj =0}≤ r + 1;
4.	Aaj = Aaj for all a 6= k.
Hence, we replace the matrix A, which contains the “output parameters,” by a matrix whose kth row
has at most r + 1 nonzero entries—see the illustration in Figure 4.
The proof of this step is based on our version of Caratheodory's theorem in Lemma 5. For every
k ∈ {1, . . . , u} and i ∈ {1, . . . , r}, elementary matrix algebra yields that
v
(Ah[BC])ki = X Akj (h[BC])ji∙
j=1
Denoting the row vectors of a matrix M ∈ Ra×b by Mι∙,..., Ma∙ ∈ Rb, we then get
v
(Ah[BC])k∙ = X Akj (h[BC ])j∙.
j=1
The case ∣Ak∙ |廉 =0 is straightforward to deal with: all elements of Ak∙ are then equal to zero, and
we canjust set A ∙= A.
We can thus assume the fact that |川.|廉=0. We then get from the preceding equality that
(Ah[BC])k∙ = X pAkj-∣∣a∙∣M h[BCj.
j=1—'=Z,
=:tj
Since
Eqa (EIAkjIqA
1/qA
IAkJqA =1
Hk∙IlqA =	1
the previous equality means that the vector (Ah [BC])k∙ ∈ Rr is a qA-convex combination of the 2v
vectors zι,..., Zv, -zι,..., 一Zv ∈ Rr, that is, (Ah[BC])k∙ ∈ Convq[zι,..., Zv, -zι,..., —Zv].
15
Under review as a conference paper at ICLR 2021
Starting point
A
Step 1
Step 2
Step 3
A	B
Hence, by Lemma 5, there is a t ∈ [-1, 1]v such that ||t||qA ≤ 1, #{j ∈ {1, . . . , v} : tj 6= 0} ≤
r + 1, and
v
(Ah[BC])k∙ = X tj— (h[BC])j.∙
j=1
Hence, by the definition of the row vectorS,
v
(Ah[BC])ki = XtjIA.IL (h[BC])ji
j=1
for all i ∈ {1, . . . , r}, and, more generally, we find for all a ∈ {1, . . . , u} and i ∈ {1, . . . , r} that
(Ah[BCl)	- (Pv=ιtjIAkJqA(h[BC ])ji	for a = k ;
(Ah[BC])砒=[pv=ι Aaj (h[BC])ji	otherwise.
一 . . ~ 一 : .....................
ThiS motivates US to define A ∈ Ru×v
through
Jtj IlAk∙ IIqA	for a = k ;
Aaj	otherwise .
PropertieS 1, 3, and 4 then follow immediately. Property 2 can be derived by uSing 1. the definition of
the basic regularize] r on Page 7, 2. the definition of A, 3. the linearity of finite sums, 4. the definition
of the 'q-norms on Page 8, 5. the above-derived property |用L ≤ 1,6.a consolidation, and 7. again
the definition of the basic regularizer r on Page 7:
v
(r[A,qA])qA = max	EIAaj IqA
a∈{1,...,u}
j=1
=max	(Pv=I ⑹ MkJqAlqA
= a∈{m1,a..x.,u}	Pjv=1IAajIqA
=max IAk∙∣qA Pv=ι∣tjIqA
= a∈{m1,a..x.,u}	Pjv=1IAajIqA
=max	((Pv=ι∣AkjIqA)ItiqA
= a∈{m1,a..x.,u}	Pjv=1IAajIqA
for a = k
otherwise
for a = k
otherwise
for a = k
otherwise
≤ max	Pjv=1|Akj|qA
≤ a∈{m1,a..x.,u} Pjv=1|Aaj|qA
v
= max X|Aaj|qA
a∈{1,...,u}
j=1
=(r[A, qA])qA ,
aS deSired. ThiS concludeS the proof of the firSt Step.
C, C -c-r τ	1	,1 , , 1	∙	, ∙ ^Λ _ TTh-Jf V-)) 1 . 1 ,
Step 2: We now Show that there iS a matrix A ∈ Ru×v Such that
for a = k
otherwise
1. Aeh[BC] = Ah[BC];
16
Under review as a conference paper at ICLR 2021
r≈	π	r .	r
2.	r[A, qA ≤ r[A, qa];
3.	#{j ∈ {1, . . . , v} : Aaj 6= 0} ≤ r + 1 for all a ∈ {1, . . . , u}.
Hence, we replace the matrix A by a matrix whose every row has at most r + 1 nonzero entries—see
again the illustration in Figure 4.
Since Step 1 changes only the kth row of A (see Property 4 derived in Step 1), we can apply it to one
row after another.
Step 3: We finally prove the first part of the lemma—see again the illustration in Figure 4.
By Property 3 of the previous step, the matrix A has at most u(r + 1) nonzero columns. Verify that
replacing A by Ap and B by Bp for a suitable permutation p leads to an A whose entries outside the
first u(r + 1) columns are equal to zero—while all other properties remain intact. We denote this
version of A by A. We then derive for all j ∈ {1,..., v} and i ∈ {1,...,r} that
o
(h[BpC])ji = h[(BpC)ji] = h X(BDjbCbi
b=1
o
h X(C>)ib(Bp)jb
b=1
h[(C>(Bp)j∙)i] = (h[C>(Bp)j∙])i,
where We define (with some abuse of notation) h : Rr → Rr through (h[b])i ∙= h[bi] for all b ∈ Rr.
Combining this result with the results of Step 2 (with A and B replaced by Ap and Bp, respectively)
yields for all a ∈ {1, . . . , u} and i ∈ {1, . . . , r } that
vv
(Aph[BpC])ai = X(Ap)aj (h[BpC])ji = X(Ap)aj (h[C>(Bp)j∙])i
j=1	j=1
v	min{u(r+1),v}
=X Aaj (h[C >(Bp)j∙])i = X	Aaj (h[C>(B p)j∙])i .
j=1	j=1
We then define B ∈ Rv ×o through
(Bp)ji = Bp[j]i	forj ≤ u(r + 1) ;
0	otherwise .
We then use 1. the above-stated equality, 2. the definition of B, 3. a similar derivation as above, 4. the
block structure of A, and 5. a similar derivation as above to establish for all a ∈ {1,..., u} and
i ∈ {1, . . . , r} the fact that
min{u(r+1),v}
(Aph[BpC])ai=	X	Aaj (h[C>(Bp)j∙])i
j=1
min{u(r+1),v}
=X	Aaj (h[C>Bj∙])i
j=1
min{u(r+1),v}
=X	Aaj SM])ji
j=1
v
=X Aaj (h[BC ])ji
j=1
=(Ah[BC])ai∙
The other properties stated in the lemma follow readily.
The second part of the lemma can be derived in the same way.	□
17
Under review as a conference paper at ICLR 2021
A.6 Proof of Lemma 4
Proof of Lemma 4. The proof is a simple exercise in calculus. An illustration of the quantities
involved in the proof is given in Figure 5. The function h is convex by assumption; hence, it is
continuous. Then, according to the extreme value theorem, there is a number
c ∈ arg min h[t] .
t∈[0,1]
Since h[t] < h[0] forat ∈ (0,1], it holds that C ∈ (0,1]. Now consider t2 ∈ [0,1] and tι ∈ [0,t2),
that is, t2 > t1 . Basic calculus ensures that
6[t2] = h[ct2]
ct2 - ct1
ctι +------------(C — ctι)
c - ct1
h
h [ct1+H(C-CtI)
≤
—
t2 - t1	t2 - t1
丁—“ Jctl + 1=1c
1-
t2 — tι ∖
1 — tι J
h[ct1 ] +
t2 — tι
1 — tι
h[c]
≤(1 — ^-L)h[ctι] + ^-Ih[ctι]
1 — t1	1 — t1
h[ct1]
一一 一
h[t1].
TT	7 .	.	.
Hence, h is nonincreasing.
Moreover,
h[0] = h[c ∙ 0] = h[0] > h问 ≥ h[c] = h[c ∙ 1] = 6[1].
T T	7「Cl _	7 J 1 EI ♦	11	.1	C
Hence, h[0] > h[1]. This concludes the proof.
□
Figure 5: quantities in the proof of Lemma 4
18