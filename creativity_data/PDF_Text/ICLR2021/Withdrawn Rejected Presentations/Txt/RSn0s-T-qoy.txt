Under review as a conference paper at ICLR 2021
Multi-View Disentangled Representation
Anonymous authors
Paper under double-blind review
Ab stract
Learning effective representations for data with multiple views is crucial in machine
learning and pattern recognition. Recent great efforts have focused on learning
unified or latent representations to integrate information from different views for
specific tasks. These approaches generally assume simple or implicit relationships
between different views and as a result are not able to accurately and explicitly
depict the correlations among these views. To address this, we firstly propose the
definition and conditions for unsupervised multi-view disentanglement providing
general instructions for disentangling representations between different views.
Furthermore, a novel objective function is derived to explicitly disentangle the multi-
view data into a shared part across different views and a (private) exclusive part
within each view. The explicit guaranteed disentanglement is of great potential for
downstream tasks. Experiments on a variety of multi-modal datasets demonstrate
that our objective can effectively disentangle information from different views
while satisfying the disentangling conditions.
1	Introduction
Multi-view representation learning (MRL) involves learning representations by effectively leveraging
information from different perspectives. The representations produced by MRL are effective when
correlations across different views are accurately modeled and thus properly exploited for downstream
tasks. One representative algorithm, Canonical Components Analysis (CCA) (Hotelling, 1992),
aims to maximize linear correlations between two views under the assumption that factors from
different views are highly correlated. Under a similar assumption, the extended versions of CCA,
including kernelized CCA (Akaho, 2006) and Deep CCA (Andrew et al., 2013), explore more general
correlations. There are also several methods (Cao et al., 2015; Sublime et al., 2017) that maximize
the independence between different views to enhance the complementarity. Going beyond the simple
assumptions above, the latent representation encodes different views with a degradation process
implicitly exploiting both consistency and complementarity (Zhang et al., 2019).
These existing MRL algorithms are effective, however, the assumed correlations between different
views are usually simple thus cannot accurately model or explicitly disentangle complex real-world
correlations, which hinders the further improvement and interpretability. Although there are a
few heuristic algorithms (Tsai et al., 2019; Hu et al., 2017) that explicitly decompose the multi-
view representation into shared and view-specific parts, they are especially designed for supervised
learning tasks without any disentangled representation guarantee and fall short in formally defining the
relationships between different parts. To address this issue, we propose to unsupervisedly disentangle
the original data from different views into shared representation across different views and exclusive
(private) part within each view, which explicitly depicts the correlations and thus not only enhances
the performance of existing tasks but could also inspire potential applications. Specifically, we firstly
provide a definition for the multi-view disentangled representation by introducing the sufficient and
necessary conditions for guaranteeing the disentanglement of different views. According to these
conditions, an information-theory-based algorithm is proposed to accurately disentangle different
views. To summarize, the main contributions of our work are as follows:
•	To the best of our knowledge, this is the first work to formally study multi-view disentangled
representation with strict conditions, which might serve as the foundations of the future
research on this problem.
•	Based on our definition, we propose a multi-view disentangling model, in which information-
theory-based multi-view disentangling can accurately decompose the information into shared
1
Under review as a conference paper at ICLR 2021
(a) Original data (b) φ+(2+<3+(4	⑹②+③+④	(d)①+③+④	(e)①+②+④	(f)①+②+③
Figure 1: Illustration of multi-view disentangled representation. (a): The red and white graphics
indicate the shared information between different views, and the (private) exclusive information
within each view, respectively. (b): The exact disentangled representation can be achieved - the
shared (gray area) and exclusive (white area) components are separated, when the four conditions in
definition 2.1 are satisfied. (c)(d)(e)(f): The exact disentangled representation cannot be guaranteed
when any condition is violated. Intuitively, the proposed four conditions are necessary and sufficient
conditions since any change of (b) will violate the definition.
representation across different views and exclusive representation within each view. The
explicit decomposition enhances the performance of multi-view analysis tasks and could
also inspire new potential applications.
•	Different from the single-view unsupervised disentangled representation learning (Locatello
et al., 2019), we provide a new paradigm for unsupervised disentangled representation
learning from a fresh perspective - disentangling factors between different views instead of
each single view.
•	Extensive experiments on a range of applications verify that the proposed information-theory-
based multi-view disentangling algorithm can accurately disentangle data from multiple
views into expected shared and exclusive representations.
2	Multi-View Disentangled Representation
Existing multi-view representation learning methods (Wu & Good-
man, 2018; Zhang et al., 2019) can obtain a common representation
for multi-view data, however, the correlations between different
views are not explicitly expressed. The supervised algorithms (Hu
et al., 2017; Tan et al., 2019) can decompose multiple views into a
common part and private parts, but there is no disentangling guaran-
tee. Therefore, we propose a multi-view disentanglement algorithm
that can explicitly separate the shared and exclusive information
in unsupervised settings. Formally, we first propose a definition
of a multi-view disentangled representation by introducing four
criteria, which are considered as sufficient and necessary conditions
of disentangling multiple views. The definition is as follows:
Definition 2.1 (Multi-View Disentangled Representation) Given
a sample with two views, i.e., X = {xi}i2=1, the representation
Sdis = {si , ei}i2=1 is a multi-view disentangled representation if
the following conditions are satisfied:
③
②
P
o -∙∈2H12 ①
E
P
o ∙O-S ①
E
②
W
Figure 2: Illustration of our
model, which corresponds to
the objective in Eq. 1 and the
conditions in definition 2.1. Re-
fer to the text for PoE (Product
of Expert) in 2.1.
④
•	Completeness:① The shared representation si and exclusive representation ei should
jointly contain all information of the original representation xi ;
•	Exclusivity:② There is no shared information between common representation si and
exclusive representation ei, which ensures the exclusivity within each view (intra-View).③
There is no shared information between ei and ej, which ensures the exclusivity between
private information of different views (inter-View).
•	Commonality:④ The common representations Si and Sj should contain the same informa-
tion. Equipped with the exclusivity constraints, the common representations are guaranteed
to not only be the same but also contain maximized common information.
The necessity for each criterion is illustrated in Fig. 1 (satisfaction of all the four conditions produces
exact disentanglement, and violation of any condition may result in an unexpected disentangled
representation). Note that, existing (single-view) unsupervised disentanglement focuses on learning
2
Under review as a conference paper at ICLR 2021
a representation to identify explanatory factors of variation, which has been proved fundamentally
impossible (Locatello et al., 2019). The goal of the proposed multi-view disentanglement is to
disentangle multiple views into the shared and exclusive parts which can be well guaranteed as
illustrated in definition 2.1 and Fig. 1.
Mutual information has been widely used in representation learning (Hjelm et al., 2019; Belghazi
et al., 2018). In probability theory and information theory, the mutual information of two random
variables quantifies the “amount of information” obtained about one random variable when observing
the other one, which is well-suited for measuring the amount of shared information between two
different views. To approach the disentangling goal, according to conditions ①〜④,the general form
of the object function is naturally induced as:
2
max X [I (xi; ei ,si) -1 (ei; si) ] - fl(ei; ej)+ fl (si; sj),	⑴
i,j=1 `―{z—‘飞厂	i=j飞厂i=j二厂
where I(∙; ∙) denotes the mutual information. We provide an implementation in Fig. 2 and, in the
following subsections, we will describe this implementation in detail.
2.1	Condition ①：Information Preservation for the Shared and Exclusive
Representations
• How to maximize I(x; e, s)?
For simplicity, x, s, e and xi , si , ei are denoted with the same meanings and used alternately, where
the former and latter are used for intra-view and inter-view cases, respectively. To preserve the
information from the original data in the shared and exclusive representations, the mutual information
I(x; e, s) should be maximized. There are different ways to implement the maximization ofI(x; e, s)
based on the following assumptions.
Assumption 2.1 The shared representation s and exclusive representation e are simultaneously
independent and conditionally independent:
p(s, e) = p(s)p(e), p(s, e|x) = p(s|x)p(e|x).	(2)
Firstly, we expand I(x; e, s) to obtain the following equation (more details are shown in supple-
ment C)：
I(x; e,
s)=
p(x)p(e, s|x) log
p(e, s|x)
p(e,s)
dedsdx.
Then, under Assumption 2.1, the following equation is derived (more details are shown in supple-
ment C)：
I(x; e, s) = I(x; e) + I(x; s).	(3)
According to the above equation, it seems that we can maximize I(x; e) + I(x; s) to maximize
I(x; e, s), which involves making s and e contain as much information from x as possible (ideally,
it will produce e and s to meet I(x; e) = I(x; s) = H(x), where H(x) is the entropy of x). This
actually leads to a strong correlation between s and e, which is in conflict with the independence
Assumption 2.1 about s and e. in other words, it is difficult to balance the completeness (condition
①)and intra-view exclusivity (condition ②)(see experimental results in supplement B.4).
Fortunately, there is an alternative strategy which avoids the difficulty in balancing the completeness
and intra-view exclusivity. Specifically, we introduce a latent representation r generated by two
independent distributions with respect to s and e under a mild assumption：
Assumption 2.2 (Relationship between s, e and r):
p(s, e, x) = p(r, x).
(4)
in our formulation, we define r = f(s, e), where r is derived from s and e with the underlying function
f (∙) and satisfies p(r, x) = p(s, e, x). Eq. 4 is a mild assumption, for example the invertibility of
mapping r = f(s, e) ensuring a sufficient condition which can be easily verified. Note that r = [s, e]
3
Under review as a conference paper at ICLR 2021
is one special case and will be discussed later. Based on Assumption 2.2, we can get (more details
are shown in supplement C):
p(r) = p(s, e), p(r|x) = p(s, e|x).	(5)
Then, we can induce the following result (more details are shown in supplement C):
I(x; e, s) = I(x; r).
(6)
This result indicates that the maximization of I(x; e, s) can be achieved by maximizing the mutual
information of agency r and x. In this way, the independence of e and s is well preserved and
the previous conflict is dispelled. Next, we will explain how to encode the information of x into
independent representations s and e by introducing the agency r.
• How to obtain independent representations e and s by maximizing I(x; r)
First, we consider encoding the observed data x into a latent representation r by maximizing the
mutual information between x and r. Considering robustness and effectiveness (Alemi et al., 2018),
we can maximize the mutual information between r and x through Variational Autoencoders (VAEs)
(Kingma & Welling, 2014). Accordingly, we have the following objective function:
min Ex〜p(x) h - Er〜qr(r|x) [log(d(XIr))] + Er〜q「(r|x) log p(r) ],
(7)
where d(x|r) (the “decoder”) is a variational approximation to p(x|r), and qr(r|x) (the “encoder”) is a
variational approximation to p(r|x), which converts the observed data x into the latent representation
r.
Second, we consider how to obtain independent representations e and s by modeling qr(r|x). For this
goal, the relationships between s, e and r should be jointly modeled. As shown in Eq. 5, we obtain
p(r|x) = p(s, e|x). Under Assumption 2.1, Eq. 5 can be rewritten as p(r|x) = p(s|x)p(e|x), which
implies that qr(r|x) can be considered as the product of p(s|x) and p(e|x). Furthermore, we introduce
PoE (product-of-experts) (Hinton, 2002; Wu & Goodman, 2018) to model the product of qs(s|x) and
qe(e|x), where the variational networks qs(s|x) and qe(e|x) are designed to approximate p(s|x) and
p(e|x). It is worth noting that the key difference from MVAE (Multimodal Variational Autoencoder)
(Wu & Goodman, 2018) is that our model obtains the latent representation r from two independent
components within each single view, while MVAE achieves the unified representation of all views
by assuming independence of representations of different views. Under the assumption that the true
posteriors for individual factors p(s|x) and p(e|x) are contained in the family of their variational
counterparts qs(s|x) and qe(e|x), we have qr(r|x) = qs(s|x)qe(e|x). With Gaussian distributions,
μ°σ2 + μ=σ2
We can obtain the closed-form solution for the product of two distributions: μr =、2；72 e,
22	s e
σr22 =商矛.Therefore, the independent representations between e and S are well preserved by
modeling qr(r|x).
Accordingly, with the results qr(r|x) = qs(s|x)qe(e|x) and p(r) = p(s)p(e), the objective in Eq. 7
is rewritten as:
min Ex〜p(x) - Er〜qs
qs ,qe ,d
(s|x)qe(e|x) log(d(x|r))
qs(s|x)	qe(e|x)
+ Es〜qs(s∣x) log
q⅛⅛j ■ + Ee〜qe(e|x) log . q⅛⅛2],
where p(s) and p(e) are set to Gaussian distributions, which in turn forces qs(s|x) and qe(e|x) to be
closer to a Gaussian distribution, allowing us to find the product of the two distributions. The above
objective is actually the ELBO (evidence lower bound) (Kingma & Welling, 2014) with the first term
being the reconstruction loss, and the second and third terms being the KL divergence.
The proposed variant of VAE inherits two advantages from VAE and PoE, respectively. The first is
that we can obtain approximate distributions of s and e given x to preserve the independence. The
second is that the proposed model still works even when there is a missing case for e or s in the
testing. This means that we can use only s or e as input to the decoder to reconstruct x (shown in the
experimental section), which is quite different from the concatenation of e and s or other forms that
require e and s simultaneously to obtain r. In addition, the way of concatenating s and e does not
well exploit the independent prior of s and e.
4
Under review as a conference paper at ICLR 2021
2.2	Conditions ②-③：Exclusivity
To fulfill conditions ② and ③，We minimize the mutual information between two variables by
enhancing the independence. There are different strategies to promote the independence between
variables, which are endowed with different properties. specifically, the straightforward way is to
promote the independence by minimizing the linear correlation. Accordingly, we have the following
loss function：
T
eiejT
min----------.
qi,qj keikkej k,
(8)
for condition ②，and a similar objective could be induced for condition ③.Although simple and
effective, the linearity property may not be powerful enough to handle complex real-world complex
correlations. Therefore, we also propose an alternative strategy for general correlation cases in the
supplementary material A.1.
2.3	Condition ④：Alignment OF THE Shared Representation FROM Different Views
For condition ④,we ensure the commonality between si and Sj by maximizing the mutual information
as follows：
I (si； Sj) = / / p(si,sj)log ppS⅛S⅜ dsidsj.
it is difficult to calculate the mutual information, since the true distribution is usually unknown.
Based on the scalable and flexible MiNE (Belghazi et al., 2018), we introduce two different strategies
for maximizing the mutual information between the shared representations si 〜qS(si∣χi) and
Sj 〜 qii (SjIxj) from different perspectives.
MiNE can estimate mutual information of two variables by training a classifier to distinguish whether
the samples come from the joint distribution J or the product of marginals M. MiNE actually aims to
optimize the tractable lower bound to estimate mutual information based on the Donsker-Varadhan
representation (Donsker & Varadhan, 1983) of the KL-divergence, with the following form：
I(si,sj) ≥EJ Tθ(si,sj) - log EM heTθ(si,sj)i,
where Tθ is a discriminator function modeled by a neural network with parameters θ. J and M are
the joint and product of marginals, respectively. We can maximize the mutual information of si and
sj by maximizing the lower bound.
Although the KL-based Mi is effective for some tasks, it tends to overemphasize the similarity
between samples and thus cannot thoroughly explore the underlying similarity between different
distributions. To address this issue, we could replace the KL divergence with Js divergence (Hjelm
et al., 2019), which can focus on the similarity in terms of different distributions instead of samples.
Accordingly, we maximize the mutual information of si and sj by the following form：
max Ej[-sp (-Tθ(Si,sj))] - EM [sp(T(si,s0j))],
qsi ,qsj ,Tθ
where si, sj corresponds to one sample with two views, i.e., the ith and jth views, respectively. s0j
corresponds to another sample from the jth view, and sp(z) = log(1 + ez) is the softplus function.
specifically, the inner product is employed in the classifier, i.e., Tθ (a, b) = aTb. We have discussed
these two methods in the supplementary material A.2.
3 Related Work
The disentanglement of representations aims to depict an object through independent factors in order
to provide a more reliable and interpretable representation (Bengio et al., 2013). Most unsupervised
disentangled representation learning methods (Chen et al., 2018; Esmaeili et al., 2019; Kumar et al.,
2018) are based on Variational Autoencoders (VAEs) (Kingma & Welling, 2014). Existing VAE-
based methods basically increase the independence between different factors of the representation.
on the basis of VAEs, β-VAE (Higgins et al., 2017) implicitly obtains promising disentanglement
5
Under review as a conference paper at ICLR 2021
performance by increasing β in ELBO to constrain the capacity of the latent space. Different from
β-VAE, several methods (Chen et al., 2018; Esmaeili et al., 2019) increase the independence between
different factors by minimizing a total correlation (TC) loss. DIP (Disentangled Inferred Prior)
(Kumar et al., 2018) encourages disentanglement by introducing a disentangled prior to constrain the
disentangled representation. However, there are theoretical problems in unsupervised disentanglement
learning (Locatello et al., 2019). Thus there are also several semi-supervised methods (Kingma et al.,
2014; Narayanaswamy et al., 2017; Bouchacourt et al., 2018) for disentanglement representation
learning which have access to partial real factors of the data. There are also various real-world
applications using disentangled representations (Gonzalez-Garcia et al., 2018; Liu et al., 2018).
Multi-view representation learning aims to jointly utilize information from multiple views for better
performance. To jointly learn a unified representation between multiple views, CCA-based (Hotelling,
1992; Akaho, 2006; Andrew et al., 2013; Wang et al., 2015) algorithms maximize the correlation
between different views to extract shared information. KCCA (Akaho, 2006) and DCCA (Andrew
et al., 2013) extend the traditional CCA using kernel and deep neural networks, respectively. DCCAE
(Wang et al., 2015) jointly considers the reconstruction of each single view and the correlation
across different views. To jointly encode the shared and view-specific information, some latent-
representation-based models (Zhang et al., 2019) have been proposed. There are also models (Wu
et al., 2019; Suzuki et al., 2016; Vedantam et al., 2018) that employ a VAE to learn a unified
multi-modal representation.
4	Experiments
Experimental Settings. We conduct quite comprehensive experiments to evaluate the disentangled
representation. Specifically, we investigate the disentanglement quantitively by conducting clustering
and classification (section 4.1), and provide visualization results to intuitively evaluate the disentan-
glement (section 4.2 and section B.5). Furthermore, we conducted ablation experiments (section B.3)
and present application experiments based on the disentangled representation (section B.6). Due to
the space limitation, some experiments are detailed in the supplementary material.
Datasets: Similar to the work (Gonzalez-Garcia et al., 2018), we construct the dataset MNIST-
CBCD, comprising MNIST-CB (MNIST with Colored Background) and MNIST-CD (MNIST
with Colored Digit) as two views, by randomly modifying the color of digits and background of
digit images from the dataset MNIST. Intuitively, the shape of a digit corresponds to the shared
information, while the colors of background and digit correspond to the private information within
each view. The same strategy is applied to FashionMNIST. We also conduct experiments on the
face image dataset CelebA (Liu et al., 2015), which is a large-scale face-attributes dataset with
more than 200K celebrity images, each of which is with 40 attribute annotations. The image and
attribute domains are considered as two different views. We select the 18 most significant attributes
as the attribute vector (Perarnau et al., 2016). For these two views, the shared information are
attribute-related information (e.g., viewpoint, hair color, w/ or w/o glasses etc), and the exclusive
representation is non-attribute information.
To verify the disentanglement, we compare our algorithm with: (1) Raw-data (Raw), which reshapes
images directly into vectors as representations; (2) Variational autoencoders (VAE (Kingma &
Welling, 2014)), which uses VAE to extract features from the data of each view; (3) CCA-based
methods (CCA (Hotelling, 1992), KCCA (Akaho, 2006), DCCA (Andrew et al., 2013) and DCCAE
(Wang et al., 2015)), which obtain the common representation by maximizing the correlation between
different views. (4) Multimodal variational autoencoders (MVAE (Wu et al., 2019)), which can learn
a common representation of two views. For Raw-data and VAE, we report the clustering results using
the representations obtained by view-1, view-2, and representation by concatenating view-1 and
view-2. In our method, we use s1, s2 and the concatenated representation for clustering/classification.
4.1	Quantitative Analysis
To evaluate the disentanglement of our algorithm, we conduct clustering and classification based
on the representations, respectively. For simplicity, we employ k-means as the clustering algorithm,
since k-means is based on the Euclidean distance, which makes it more objective in measuring the
quality of representations. KNN (K-Nearest Neighbour) and linear SVM are employed to conduct
6
Under review as a conference paper at ICLR 2021
		view1	Raw view2	Concat	view1	VAE view2	Concat	CCA-based				MVAE	Ours view1 view2 Concat
								CCA	KCCA	DCCA DCCAE			
M	ACC	11.7	26.8	12.2	35.4	38.5	56.2	46.9	48.2	40.2	47.4	50.4	62.03 62.12 62.41
	NMI	0.30	18.2	0.50	25.0	30.6	50.2	54.2	52.9	51.3	51.9	41.9	54.83 56.42 57.01
F	ACC	15.1	36.8	28.7	42.5	45.6	52.0	^786^	46.2	42.5	46.4	51.8	56.12 54.93 56.31
	NMI	3.90	32.3	24.5	41.1	45.1	55.1	55.1	56.7	52.8	54.0	54.1	58.23 58.52 59.41
Table 1: Comparison on the clustering task. ‘M’ and ‘F’ indicate MNIST-CBCD and FMNIST-CBCD,
respectively. The top three results are in bold and marked with superscript.
		view1	Raw view2	Concat	view1	VAE view2	Concat	CCA-based				MVAE	view1	Ours view2	Concat
								CCA	KCCA	DCCA	DCCAE				
1	M	77.8	90.62 77.9		78.7	89.3	79.2	83.0	55.5	52.9	59.5	87.2	88.1	89.93 91.21	
	F	70.7	75.2	67.8	74.8	78.71 74.9		64.1	53.3	52.8	56.8	73.2	74.2	78.12	77.53
	M	82.3	90.4	91.53	85.6	91.0	91.1	^^835	61.8	51.6	64.9	92.41	89.2	90.5	92.22
	F	59.7	78.1	79.6	77.9	81.3	81.43	65.1	57.4	47.1	59.0	79.8	79.4	81.91 81.91	
Table 2: Comparison on the classification task. ‘KNN’ and ‘LSVM’ indicate the K-Nearest Neighbor
and linear SVM respectively. ‘M’ and ‘F’ indicate MNIST-CBCD and FMNIST-CBCD, respectively
	MNIST-CBCD	FMNIST-CBCD	MVAE		Ours
View-1 (e1)	69.89±8.47	70.80±5.99	top 1	64.12	95.15
Raw View-1	66.39±10.79	57.65±6.94	top 5	63.24	94.57
View-2 (e2)	54.61±3.30	58.12±4.63	top 10	62.85	94.16
Raw View-2	53.19±5.90	51.92±4.80	top 100	61.53	91.96
Table 3: Clustering with exclusive representation.			Table 4: Cross-modal retrieval.		
classification experiments based on the shared representations. All experiments are run 20 times and
the means are reported in terms of accuracy (refer to the supplement for standard deviations).
From the quantitative results in Tables 1 and 2, the following observations are drawn: (1) directly using
the raw features for clustering/classification is not promising, as the digital and color information
are mixed. Moreover, since the background region is much larger than the area of the digit, the
accuracy of using view-1 is relatively low on MNIST; (2) compared with the raw features, the shared
information extracted by our model is competitive due to the clear semantic information; (3) by
extracting the shared (digit) information explicitly, our model obtains much better results.
Furthermore, we evaluate the exclusive representations on clustering. For the MNIST, the colors
of background (MNIST-CB: MNIST with Colored Background) or digits (MNIST-CD: MNIST
with Colored Digit) are considered as class labels. According to Table 3, our algorithm obtains
more promising clustering performance with the exclusive representation compared with the raw
data, while existing algorithms cannot obtain exclusive representation explicitly. The performance
improvement of view-2 (on MNIST-CBCD) is not so substantial. The possible reason is that exclusive
information (the color of digit) from the images is not so significant due to small area ratio of digits,
which increases the difficulty of disentanglement.
We verify our disentangled representation with cross-modal retrieval on CelebA (Liu et al., 2015).
Specifically, after training the disentangling networks, we can obtain the shared representations
from the image and attribute views, respectively. Therefore, the attribute vector can be used to
retrieve the related face images (attribute-specific cross-modal retrieval). The quantitative results are
reported in Table 4, and examples are in Fig. 9(a) (in the supplement). Given the specific attributes
represented as vector ln, We can obtain attribute vector lnk for the kth most similar retrieved image,
which is associated with D attributes (the value of each one is 0 or 1). Accordingly, for the top K
retrieved images, we have accuracy = Pn=1 Pk=NPKID(Inkd,lnkd), where δ(a,b) = 1 when a = b,
otherWise δ (a, b) = 0. According to the results in Table 4, the performances of our model are much
higher than those of MVAE due to the promising disentanglement.
7
Under review as a conference paper at ICLR 2021
■ 8 I 7
、∖ ■看 O 7
6 夕∙θ[3∙
/Bi
，〃 0 " 2
∖3 3 9∣3
HZHH2 /
7国5 了夕・la，
lslo)¾v
^∖^SS7
M7・“ 4 ʃ 9e
(e) (View-2) Shared
(a) (View-1) Original (b) (View-1) Shared (c) (View-1) Exclusive
(f) (View-2) Original
(g) (View-2) Shared
(h) (View-2) Exclusive
(d) (View-1) S&E
(i) (View-2) S&E
(j) (View-1) Shared


Figure 3: Visualization of reconstruction with shared and exclusive representations. The top and
bottom rows correspond to the reconstruction results from the decoders of view-1 and view-2,
respectively. ‘Shared’, ‘Exclusive’ and ‘S&E’ indicate shared, exclusive, and the combination of
shared and exclusive representations, respectively. ‘View-1’ and ‘View-2’ in the parentheses indicate
the view where these representations come from. Note that, the images in (e) ((j)) are reconstructed
results using decoder of view-1 (view-2) using the share representation from view-2 (view-1).
4.2	Qualitative Analysis
We further intuitively demonstrate that our model can well fulfill the four conditions in definition 2.1
with visualization. On MINIST-CBCD, we train the model using the training set and randomly select
64 images in the test set for visual analysis. The disentangled shared and exclusive representations
are used as input to decoders corresponding to different views to reconstruct the original data with
different combinations. For example, we can input the shared representation extracted from view-1
into the decoder corresponding to view-2 to obtain the reconstructed images from view-2. The
visualization of reconstruction results are shown in Fig 3.
From Fig. 3, we have the following observations, which are consistent with the definition of multi-
view disentanglement: (1) By combining the shared and exclusive information, the original image
can be fully reconstructed ((d) and (i)), satisfying condition ①(completeness); (2) The shared and
exclusive representations contain different information. With the shared representation, we can
reconstruct images ((b) and (g)) with clear digit shapes rather than color information as in the original
images. In contrast, with the exclusive representations, we can reconstruct the color information ((c)
and (h)) of the original images rather than the digit shapes. This verifies that the condition ②(intra-
view exclusivity) is satisfied. (3) The exclusive representations from different views contain different
information. Specifically, the exclusive representation (c) from view-1 contains the information of
background color, while the exclusive representation (h) from view-2 contains information of the
digit color. This verifies that our model satisfies condition ③(inter-view exclusivity). (4) The shared
representations ((b), (g), (e) and (j)) from different views contain (almost) the same information, i.e.,
condition ④(commonality). We verify this by reconstructing digit shapes in view-2 using the shared
representations from view-1 and vice versa. Similar experiments are done on CelebA (section B.5).
5	Conclusion
In this work, we proposed a formal definition for disentangling multi-view data, and based on this
developed a principled algorithm which focuses on automatically disentangling multi-view data into
shared and exclusive representations without supervision. Extensive experiments validate that the
proposed algorithm can promote subsequent analysis tasks (e.g., clustering/classification/retrieval).
We consistently validated that the proposed algorithm can provide promising disentanglement and
thus is quite effective and flexible in analyzing and manipulating multi-view data. We will focus on
the semi-supervised setting to improve the discriminative ability in the future.
8
Under review as a conference paper at ICLR 2021
References
Shotaro Akaho. A kernel method for canonical correlation analysis. arXiv preprint cs/0609071, 2006.
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif A Saurous, and Kevin Murphy. Fixing a
broken elbo. In International Conference on Machine Learning, pp. 159-168, 2018.
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation analysis.
In International Conference on Machine Learning, pp. 1247-1255, 2013.
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Devon
Hjelm, and Aaron Courville. Mutual information neural estimation. In International Conference
on Machine Learning, pp. 530-539, 2018.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798-1828,
2013.
Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin. Multi-level variational autoencoder:
Learning disentangled representations from grouped observations. In Thirty-Second AAAI Confer-
ence on Artificial Intelligence, 2018.
Xiaochun Cao, Changqing Zhang, Huazhu Fu, Si Liu, and Hua Zhang. Diversity-induced multi-view
subspace clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 586-594, 2015.
Tian Qi Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentan-
glement in variational autoencoders. In Advances in Neural Information Processing Systems, pp.
2610-2620, 2018.
Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Star-
gan: Unified generative adversarial networks for multi-domain image-to-image translation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8789-8797,
2018.
Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process
expectations for large time. iv. Communications on Pure and Applied Mathematics, 36(2):183-212,
1983.
Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, N Siddharth, Brooks Paige, Dana H Brooks,
Jennifer Dy, and Jan-Willem Meent. Structured disentangled representations. In The 22nd
International Conference on Artificial Intelligence and Statistics, pp. 2525-2534, 2019.
Abel Gonzalez-Garcia, Joost van de Weijer, and Yoshua Bengio. Image-to-image translation for cross-
domain disentanglement. In Advances in Neural Information Processing Systems, pp. 1287-1298,
2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International Conference on Learning Representations,
2017.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
R Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In International Conference on Learning Representations, 2019.
Harold Hotelling. Relations between two sets of variates. In Breakthroughs in statistics, pp. 162-190.
Springer, 1992.
Junlin Hu, Jiwen Lu, and Yap-Peng Tan. Sharable and individual multi-view metric learning. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 40(9):2281-2288, 2017.
9
Under review as a conference paper at ICLR 2021
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann
LeCun (eds.), International Conference on Learning Representations, 2014.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems, pp.
3581-3589, 2014.
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. Variational inference of disen-
tangled latent concepts from unlabeled observations. In International Conference on Learning
Representations, 2018.
Alexander H Liu, Yen-Cheng Liu, Yu-Ying Yeh, and Yu-Chiang Frank Wang. A unified feature disen-
tangler for multi-domain image translation and manipulation. In Advances in Neural Information
Processing Systems, pp. 2590-2599, 2018.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE International Conference on Computer Vision, pp. 3730-3738, 2015.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of dis-
entangled representations. In International Conference on Machine Learning, pp. 4114-4124,
2019.
Siddharth Narayanaswamy, T Brooks Paige, Jan-Willem Van de Meent, Alban Desmaison, Noah
Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr. Learning disentangled representations
with semi-supervised deep generative models. In Advances in Neural Information Processing
Systems, pp. 5925-5935, 2017.
Guim Perarnau, Joost Van De Weijer, Bogdan Raducanu, and Jose M Alvarez. Invertible conditional
gans for image editing. arXiv preprint arXiv:1611.06355, 2016.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint
arXiv:1710.05941, 2017.
Jeremie Sublime, Basarab Matei, and Pierre-Alexandre Murena. Analysis of the influence of diversity
in collaborative and multi-view clustering. In International Joint Conference on Neural Networks,
pp. 4126-4133. IEEE, 2017.
Masahiro Suzuki, Kotaro Nakayama, and Yutaka Matsuo. Joint multimodal learning with deep
generative models. arXiv preprint arXiv:1611.01891, 2016.
Qiaoyu Tan, Guo-Xian Yu, Jun Wang, Carlotta Domeniconi, and Xiangliang Zhang. Individuality-
and commonality-based multiview multilabel learning. IEEE Transactions on Cybernetics, pp.
1-12, 11 2019.
Y Tsai, P Liang, A Zadeh, L Morency, and R Salakhutdinov. Learning factorized multimodal
representations. In International Conference on Learning Representations, 2019.
Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of
visually grounded imagination. In International Conference on Learning Representations, 2018.
Weiran Wang, Raman Arora, Karen Livescu, and Jeff Bilmes. On deep multi-view representation
learning. In International Conference on Machine Learning, pp. 1083-1092, 2015.
Mike Wu and Noah Goodman. Multimodal generative models for scalable weakly-supervised
learning. In Advances in Neural Information Processing Systems, pp. 5575-5585, 2018.
Xuan Wu, Qing-Guo Chen, Yao Hu, Dengbao Wang, Xiaodong Chang, Xiaobo Wang, and Min-Ling
Zhang. Multi-view multi-label learning with view-specific information extraction. In Proceedings
of the 28th International Joint Conference on Artificial Intelligence, pp. 3884-3890, 2019.
Changqing Zhang, Yeqing Liu, and Huazhu Fu. Ae2-nets: Autoencoder in autoencoder networks. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2577-2585,
2019.
10
Under review as a conference paper at ICLR 2021
Supplemental Materials: Multi-View Disentangled Representation
A Supplemental Material for Methods
A.1 Supplemental Material for Conditions ②-③
Inspired by Choi et al. (2018), we introduce a classifier to distinguish these (independent) representa-
tions generated by the encoders. The loss function of the classification can be defined as:
min ER
q,C
-	p(z) logC(z|R)dz,
(9)
where C is a classifier that distinguishes the representation from different sources (independent
representations), R is from a representation set with different sources, e.g., private representation ei
and ej from different views, and q corresponds to the encoder of different views. z is a label which
indicates the source of R.
Generally, it is difficult to strictly guarantee the independence; however, the two different strategies
can promote the independence between different representations (generated by the encoders) to a
certain extent. We implement both strategies and similar results are observed in practice.
A.2 Discussion of Condition ④
Both KL- and JS-based estimators can maximize the mutual information. However, due to the different
properties of the KL and Js divergence, the two estimators are suitable for different scenarios. since
the Js divergence is bounded, in theory, it prevents the estimator from overemphasizing the similarity
of two representations for the same sample (even if they are exactly the same, it will not obviously
reduce the loss). This prevents the encoder from paying too much attention to generating the exact si
coordination with sj instead of the overall objective function. in contrast, since the estimator based
on the KL divergence is unbounded, si and sj are forced to be as similar as possible. Although this is
not appropriate for most tasks, it helps us to observe whether si and sj intuitively have high mutual
information. For example, we can replace si and sj with each other to see if they can accomplish the
same task (which is demonstrated in the experimental part).
B s upplemental Experiments
B.1	Network Architectures
For MNisT and FashionMNisT, two convolutional layers and two fully connected layers are used
for the encoder, while we employ two fully connected layers and two deconvolution layers for the
decoder. For the face image dataset CelebA, we use four convolutional layers and two fully connected
layers to build the encoder for handling the image view, while the decoder is built using two fully
connected layers and four deconvolutional layers. For the attribute vector view, three fully connected
layers are used to construct both the encoder and decoder. The batch normalization ioffe & szegedy
(2015) and swish activation functions Ramachandran et al. (2017) are used between the convolutional
layers.
B.2	Detailed experimental res ults of quantitative experiments
Due to space limitations, we only report the means of clustering and classification experiments in the
text, and here we add their standard deviations in Table 5 and 6.
B.3	Ablation experiments
To verify the necessity of each criterion in definition 2.1, we conduct experiments on the MNisT-
CBCD dataset. Specifically, We conduct the similar experiments by removing ②，③ and ④ in the
objective function, and the corresponding results are shown in Fig. 4, 5, and 6 respectively.
11
Under review as a conference paper at ICLR 2021
Raw
VAE
view1 view2 ConCat view1 view2 ConCat CCA
CCA-based
KCCA DCCA DCCAE
MVAE
Ours
view1 view2 ConCat
ACC 11.7±0	26.8±2	12.2±0	35.4±2	38.5±4	56.2±3	46.9±3	48.2±2	40.2±3	47.4±4	50.4±7	62.0±3	62.1±3	62.4±2
NMI 0.3±0	18.2±2	0.5±0	25.0±3	30.6±3	50.2±2	54.2±3	52.9±1	51.3±2	51.9±3	41.9±3	54.8±1	56.4±1	57.0±1
ACC15.1±1	36.8±2	28.7±2	42.5±3	45.6±3	52.0±3	48.6±3	46.2±2	42.5±3	46.4±3	51.8±4	56.1±3	54.9±3	56.3 ±2
NMI 3.9±1	32.3±2	24.5±1	41.1±2	45.1±3	55.1±3	55.1±3	56.7±2	52.8±3	54.0±2	54.1±2	58.2±2	58.5±2	59.4±2
Table 5: Comparison between existing multi-modal representation learning methods and ours on the
Clustering task. ‘M’ and ‘F’ indiCate the MNIST-CBCD and FMNIST-CBCD datasets respeCtively.
	Raw			VAE			CCA-based		MVAE		Ours	
view1	view2	Concat	view1	view2	Concat	CCA	KCCA DCCA	DCCAE		view1	view2	Concat
KNN	M	77.8±.4	90.6±.5	77.9±.8	78.7±.9	89.3±.6	79.2±.6	83.0±.4	55.5±.9	52.9±.6	59.5±1.1	87.2±.8	88.1±.3	89.9±.6	91.2±.5
KNN	F	70.7±1.175.2±.3	67.8±1.1	74.8±.8	78.7±.6	74.9±.3	64.1±1.353.3±1.452.8±.6	56.8±1.4	73.2±.9	74.2±.7	78.1±.9	77.5±.9
M^^82.3±.7	90.4±.3	91.5±.3~85.6±.6	91.0±.7	91.1±.4~83.5±.7	61.8±.6	51.6±.8	64.9±.9	92.4±.4~89.2±.5	90.5±.6	92.2±.6
LSVM F	59.7±.2	78.1±1.5 79.6±.2	77.9±.8	81.3±.5	81.4±.6	65.1±.8	57.4±.6	47.1±.3	59.0±1.2	79.8±.2	79.4±1.0 81.9±.8	81.9±.8
Table 6: Comparison between existing multi-modal representation learning methods and ours on the
ClassifiCation task. ‘KNN’ and ‘LSVM’ indiCate the K-Nearest Neighbor and linear SVM respeCtively.
As shown in Fig. 4, due to the removal of the intra-view exclusivity (②),there are shared information
between si and ei , whiCh is Clearly validated by the reConstruCted images (g) and (h). Similarly,
as shown in Fig. 5, after removing the inter-view exclusivity (③)，the performance of disentangle-
ment becomes much worse. As shown in Fig. 6, after removing the condition ④，we can hardly
disentangle the information from different views due to the significant difference between the shared
representation si and sj .
■，I■■4%■
与■■，■》■夕
5 6 三*q GSr
^r√B≠zH^
∙3∙c∙∙∙
(a) (View-1) Original
■中■■/∙2∙
7■■/■■目'
7 Γ∣
■ 2■■9
5 6 1
Mr夕
ɜ Gl
+ 60
S'O /
Λ L /
g I /
q o 4
6千。
q q
J~ι 二
。・¥・84・觑
。/K
■■■・名的・・
■闻H∙∙5倒阚
∙bħ∙∙ħ∙∙
(b) (View-1) Shared
(c) (View-1) Exclusive (d) (View-1) S&E (e) (View-2) Shared
7
9
?
6
q
O
ʒ
7
G
/>∙gο τ∙∙70z
2£7¥grq7
rOyM—52 7
(j) (View-1) Shared
(f) (View-2) Original (g) (View-2) Shared (h) (View-2) Exclusive (i) (View-2) S&E
τyτʃ a—
72
Figure 4:	Visualization of reconstruction with shared and exclusive representations after removing
the condition ②(intra-view exclusivity between si and ei). (Zoom in for best view).
B.4	VERIFICATION: MAXIMIZING I(x; e) + I(x; s) IS CONFLICT WITH MINIMIZING I(e; s)
In Section 2.1, we analyze the reason why we do not maximize I(x; s) and I(x; e) to realize
minimizing I(e; s). We provide qualitatively verification on the MNIST-CBCD dataset by only
changing the way of maximizing I(x; e, s). According to the experimental results in Fig. 7, we
can find that both the shared representation (corresponding to Fig. 7(b) and (e)) and the exclusive
representation (corresponding to Fig. 7(c) and (f)) contain almost the same information from the
original views. This actually leads to poor disentanglement performance and also empirically validates
the conflict discussed in Section 2.1.
B.5	Supplemental visualization results on CelebA
Furthermore, we conduct experiments on the face image-attribute dataset: CelebA Liu et al. (2015).
The results are shown in Fig. 8. The reconstructed images with the shared representations from
12
Under review as a conference paper at ICLR 2021
HH/HBt g 夕
训■■回∙Q□3
0中国61* O 3
。28夕b3得3
_q 夕 (2 i q
“户/幺7
(b) (View-1) Shared
(c) (View-1) Exclusive
EI H
■■■明即1。3
「 2 8 7Q3以3
帆■■倒座■留■
(d) (View-1) S&E
Ee用2N阚q Q
■■■■■■■■
，解5 6 1阴，3
GZ篁夕/3乂3
07囱，g 2,夕
4q — " 3
5 4 9 夕 〃 / / ∙/
(e)	(View-2) Shared
(a) (View-1) Original
s∙g］的θ BC
7∙y■■…
■■■■■■■■
339976』,
0y0o⅛zsd>vo
q326∕ooq
/(r8A + gJ./
6夕，：K94 21
58∕qfb*G --
“z 7q4763
(i) (View-2) S&E
? 8√q ? g 5
¥z 7 q4 76
4(∕∙o45a7
÷
9
4
Z
S
q
3 3 夕 <Γ 7c G
(f)	(View-2) Original (g) (View-2) Shared (h) (View-2) Exclusive
(j) (View-1) Shared
B

Figure 5:	Visualization of reconstruction with shared and exclusive representations after removing
the condition ③(inter-view exclusivity between ei and ej). (Zoom in for best view).
(a) (View-1) Original
5 3∣ 31I 4
・)7
■■■ 7 ⅜ 7
1B
q 3
OU
I 7
? q
(b) (View-1) Shared
(c) (View-1) Exclusive
(f) (View-2) Original (g) (View-2) Shared (h) (View-2) Exclusive
(d) (View-1) S&E
1394/902
OOgZO
Γ (/ G ⅛ ⅛Γ I 7
7 I 3。2 3 q
3 b 夕 \ 2,"夕 7
52311616
q 1 1 7 6 q 3
9 7 > 7，夕 I ”
(e) (View-2) Shared
70- 7 V99
33833388
*f949β∙79a>
947999099
■ 383. .99
4∙3ggqgf∙
9 1 9 0 Λo 9 -7
S009¥007
(i)	(View-2) S&E
(j)	(View-1) Shared
Figure 6:	Visualization of reconstruction with shared and exclusive representations after removing
the condition ④(commonality between si and Sj). (Zoom in for best view).
El
/■■■
■”■■」■■
HB> 8 间∙G 5
q /%。■名■
八川3 7切2
C 5 3	6
夕06 3口4 2|2
μl≡≡TWi
7 1"∕B3S
2』ʒ 留
L白乙：；1/ 2∣最
■■? ? q∙J 5
q /HS
BnBz
(c) (View-1) E
(d) (View-2) O
(e) (View-2) S (f) (View-2) E
8石Zr
夕。J
73 ”
3，4-
(a)	(View-1) O
(b)	(View-1) S

H
B


Figure 7:	Visualization of reconstruction with shared and exclusive representations.‘O’, ‘S’ and ‘E’
indicate original image, shared representation and exclusive representation respectively. ‘View-1’
and ‘View-2’ in the parentheses indicate the view where these representations come from. (Zoom in
for best view).
different views ((b) and (e)) are relatively similar, and the results of (b) and (e) both reflect the same
underlying attributes, including smile, hair color, hairstyle, gender (condition ④:commonality). The
exclusive representation from the image view demonstrates that the reconstructed images correlate
little to the attributes. For example, none of the reconstructed people have their mouths open and
13
Under review as a conference paper at ICLR 2021
their genders cannot be easily identified (condition ②：intra-view exclusivity). By combining the
shared and exclusive representations, the original images can be accurately reconstructed (condition
①：completeness). Our model recovers the most critical information without emphasizing details
because the current task is to obtain good representations for clustering/retrieval. By setting the
goal to improve image reconstruction, we can use additional techniques (e.g., using more deeper
networks - only 4 layers in our implementation, or using adversarial strategy). It is worth noting that
there is a small difference from MNIST-CBCD: the information of view-2 (attributes) is actually
contained in view-1 (images). Therefore, it is rather difficult to reconstruct face images using the
exclusive representation from view-2 - the reconstructed images are almost all the same (condition ③:
inter-view exclusivity). The visualization experiments on CelebA further verify that our disentangled
representation can promisingly satisfy the four conditions in the definition 2.1.
(a) Original
(b) Shared
IIU If
%iι i - V ?- i⅛
ɪ- h Λl应M同“
E而遇♦ PTM展♦
心怪同3间0
W3 *】♦■
E1Λ⅛ .KriMEl
ɪ ET闻那后闻
国B⅛WWHM
eG闾多国于6国
倒倒⑶号.©。用
国©履a e遂忖
国 GqWrSlGmH
(c) Exclusive
Figure 8:	Visualization of image reconstruction with shared and exclusive representations. We use the
decoder of the image view to reconstruct images by inputting the shared and exclusive representations,
where ‘Original’ indicates the original images, ‘Shared’ and ‘Exclusive’ indicates the shared and
exclusive representations, respectively. ‘View-1 S&E’ indicates the combination of shared and
exclusive representations. Similarly, ‘View-2 shared’ and ‘View-2 exclusive’ indicate the shared
and exclusive representations from View-2, respectively, which are used as inputs into the decoder.
(Zoom in for best view).
B.6 S upplemental results for Attribute- Specific Cross-Modal Retrieval and
Editing
In this section, we validate the potential use of our multi-view disentangled representation in two real
applications: attribute-specific retrieval and attribute-specific editing.
First, we verify our disentangled representation on the attribute-specific face retrieval task on the
CelebA Liu et al. (2015) dataset. The details are as described in the text, and here we show some
examples in Fig. 9(a).
Second, we demonstrate the potential use of our model in attribute-specific face editing by manip-
ulating the shared representations. The shared representation from the image view allows us to
14
Under review as a conference paper at ICLR 2021
Figure 9: Example results for face retrieval and face editing.
manipulate the specific properties of an image. The magnitude of the change can be controlled by
interpolating between two shared representations. Specifically, to modify the visual properties of a
person in an image, we can perform the following steps: (1) disentangling the shared (siomage) and
exclusive (eiomage) representations for a given image; (2) modifying the values in the attribute vector
corresponding to the properties to be changed, and extracting the shared representation (samttribute)
from the modified vector; (3) replacing the original shared representation (siomage) with snew, where
snew is the linear interpolation between samttribute and siomage ; (4) using snew and eiomage as the input
to reconstruct the intended image. Representative experimental results are shown in Fig. 9(b).
We provide more results of attribute-specific editing for more attributes. The experimental results are
shown in Fig. 10.
Figure 10: Example results for attribute-specific face editing.
C Proofs
C.1 PROOF of I(x; e,s) = RRRp(x, s, e) log Pp(Seex) dsdedX
In order to obtain Eq. 3 in Section 2.1, first according to the chain rule for mutual information, we
can get
I(x; e, s) = I(x; e) + I(x; s|e),	(10)
where
I(x; e) = [ Pp(x, e) log P(F? dedx
p(x)
p(x, e) log p(x|e)dedx -
p(x,e)logp(x)dedx,
(11)
15
Under review as a conference paper at ICLR 2021
I (x; s|e) =Ill
=ZZZ
p(x, s, e) log P(，:, dsdedx
p(x|e)
p(x, s, e) log p(x|s, e)dsdedx -	p(x,e)logp(x|e)dedx.
(12)
Then I(x; e, s) can be formulated as
I(x; e, s) =	p(x, s, e) log p(x|s, e)dsdedx -	p(x, e) log p(x)dedx
p(x, s, e) log p(x|s, e)dsdedx -	p(x, s, e) log p(x)dsdedx
Z Z Z p(x,s,e) log PxFdsdedx
Z Z Zp(x,s,e)logNs，，1：)dsdedx.
p(s, e)
(13)
C.2 PRO OF OF I(x; e, s) = I(x; e) + I(x; s)
Under Assumption 2.1, we can get p(s, e) = p(s)p(e) and p(s, e|x) = p(s|x)p(e|x). Substituting
this into Eq. 13 yields Eq. 3 in Section 2.1,
I(x; e,s) = / P Pp(x, e, s) log P(?可：)dedsdx
p(e, s)
= [P Pp(x)p(e, s|x) log P(： N：)dedsdx
p(e, s)
p(x)p(e|x)p(s|x) log
p(e|x)p(s|x)
P(C)P(S)
dedsdx
(14)
Z Z Zp(x)p(e∣x)p(s∣x)log P(WX)P(S⑶ dedsdx
P(e)P(s)
/ /p(x)p(e∣x) log 夕(，：)dedx + / /P(X)P(S∣x)log PW) dedx
P(e)	P(s)
= I(x; e) + I(x; s).
C.3 PROOF OF I(x; e, s) = I(x; r)
First, based on Assumption 2.2, we can get
P(r) =
P(r, x)dx
P(s,e,x)dx =P(s,e),
and
P(r, x)	P(s, e, x)
P(r|X) =	= -= P(S,e|X).
P(x)	P(x)
Accordingly, Eq. 6 in Section 2.1 can be derived as follows
I(x; e,s) = / / /P(x,e,s)log P(?囚：)dedsdx
P(e, s)
= [ j jp(x)p(c, s|x) log P(? $I：)dedsdx
P(e, s)
P(r |x)
=	P(x)P(r∣x) log ——rddrdx
P(r)
(15)
(16)
(17)
= I(x; r).
16