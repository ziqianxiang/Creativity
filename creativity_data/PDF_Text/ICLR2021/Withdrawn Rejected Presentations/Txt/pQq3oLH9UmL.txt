Under review as a conference paper at ICLR 2021
Achieving Explainability in a Visual Hard
Attention Model through Content Prediction
Anonymous authors
Paper under double-blind review
Ab stract
A visual hard attention model actively selects and observes a sequence of subre-
gions in an image to make a prediction. Unlike in the deep convolution network,
in hard attention it is explainable which regions of the image contributed to the
prediction. However, the attention policy used by the model to select these re-
gions is not explainable. The majority of hard attention models determine the
attention-worthy regions by first analyzing a complete image. However, it may be
the case that the entire image is not available in the beginning but instead sensed
gradually through a series of partial observations. In this paper, we design an
efficient hard attention model for classifying partially observable scenes. The at-
tention policy used by our model is explainable and non-parametric. The model
estimates expected information gain (EIG) obtained from attending various re-
gions by predicting their content ahead of time. It compares EIG using Bayesian
Optimal Experiment Design and attends to the region with maximum EIG. We
train our model with a differentiable objective, optimized using gradient descent,
and test it on several datasets. The performance of our model is comparable to or
better than the baseline models.
1	Introduction
Though deep convolution networks achieve state of the art performance on the image classification
task, it is difficult to explain which input regions affected the output. A technique called visual hard
attention provides this explanation by design. The hard attention model sequentially attends small
but informative subregions of the input called glimpses to make predictions. While the attention
mechanism explains the task-specific decisions, the attention policies learned by the model remain
unexplainable. For example, one cannot explain the attention policy of a caption generation model
that correctly predicts the word ‘frisbee’ while looking at a region far from an actual frisbee (Xu
et al. (2015)).
The majority of hard attention models first analyze a complete image to locate the task-relevant
subregions and then attend to these locations to make predictions (Ba et al. (2014); Elsayed et al.
(2019)). However, in practice, we often do not have access to the entire scene, and we gradually
attend to the important subregions to collect task-specific information. At each step in the process,
we decide the next attention-worthy location based on the partial observations collected so far. The
explainable attention policies are more desirable under such partial observability.
Pioneering work by Mnih et al. (2014) presents a model that functions under partial observability but
their attention policies are not explainable. They train their model with the REINFORCE algorithm
(Williams (1992)), which is challenging to optimize. Moreover, the model’s performance is affected
adversely if the parameterization of the attention policy is not optimal. For example, an object
classification model with unimodal Gaussian policy learns to attend the background region in the
middle of the two objects (Sermanet et al. (2014)).
This paper develops a hard-attention model with an explainable attention policy for classifying im-
ages through a series of partial observations. We formulate the problem of hard attention as a
Bayesian Optimal Experiment Design (BOED). A recurrent model finds an optimal location that
gains maximum expected information about the class label and attends to this location. To estimate
expected information gain (EIG) under partial observability, the model predicts content of the un-
1
Under review as a conference paper at ICLR 2021
seen regions based on the regions observed so far. Using the knowledge gained by attending various
locations in an image, the model predicts the class label.
To the best of our knowledge, ours is the first hard attention model that is entirely explainable un-
der partial observability. Our main contributions are as follows. First, our attention policies are
explainable by design. One can explain that the model attends a specific location because it expects
the corresponding glimpse to maximize the expected information gain. Second, the model does not
rely on the complete image to predict the attention locations and provides good performance under
partial observability. Third, the training objective is differentiable and can be optimized using stan-
dard gradient backpropagation. We train the model using discriminative and generative objectives to
predict the label and the image content, respectively. Fourth, our attention policy is non-parametric
and can be implicitly multi-modal.
2	Related Works
A hard attention model prioritizes task-relevant regions to extract meaningful features from an input.
Early attempts to model attention employed image saliency as a priority map. High priority regions
were selected using methods such as winner-take-all (Koch & Ullman (1987); Itti et al. (1998); Itti
& Koch (2000)), searching by throwing out all features but the one with minimal activity (Ahmad
(1992)), and dynamic routing of information (Olshausen et al. (1993)).
Few used graphical models to model visual attention. Rimey & Brown (1991) used augmented
hidden Markov models to model attention policy. Larochelle & Hinton (2010) used a Restricted
Boltzmann Machine (RBM) with third-order connections between attention location, glimpse, and
the representation of a scene. Motivated by this, Zheng et al. (2015) proposed an autoregressive
model to compute exact gradients, unlike in an RBM. Tang et al. (2014) used an RBM as a generative
model and searched for informative locations using the Hamiltonian Monte Carlo algorithm.
Many used reinforcement learning to train attention models. Paletta et al. (2005) used Q-learning
with the reward that measures the objectness of the attended region. Denil et al. (2012) estimated
rewards using particle filters and employed a policy based on the Gaussian Process and the upper
confidence bound. Butko & Movellan (2008) modeled attention as a partially observable Markov
decision process and used a policy gradient algorithm for learning. Later, Butko & Movellan (2009)
extended this approach to multiple objects.
Recently, the machine learning community use the REINFORCE policy gradient algorithm to train
hard attention models (Mnih et al. (2014); Ba et al. (2014); Xu et al. (2015); Elsayed et al. (2019)).
Among these, only Elsayed et al. (2019) claims explainability by design. Other works use EM-
style learning procedure (Ranzato (2014)), wake-sleep algorithm (Ba et al. (2015)), a voting based
region selection (Alexe et al. (2012)), and differentiable models (Gregor et al. (2015); Jaderberg
et al. (2015); Eslami et al. (2016)).
Among the recent models, Ba et al. (2014); Ranzato (2014); Ba et al. (2015) look at the low-
resolution gist of an input at the beginning, and Xu et al. (2015); Elsayed et al. (2019); Gregor
et al. (2015); Jaderberg et al. (2015); Eslami et al. (2016) consume the whole image to predict the
locations to attend. In contrast, our model does not look at the entire image at low resolution or oth-
erwise. Moreover, our attention policies are explainable. We can apply our model in a wide range
of scenarios where explainable predictions are desirable for the partially observable images.
3	Model
In this paper, we consider a recurrent attention model that sequentially captures glimpses from an
image x and predicts a label y. The model runs for time t = 0 to T - 1. It uses a recurrent net
to maintain a hidden state ht-1 that summarizes glimpses observed until time t - 1. At time t, it
predicts coordinates lt based on the hidden state ht-1 and captures a square glimpse gt centered at
lt in an image x, i.e. gt = g(x, lt). It uses gt and lt to update the hidden state to ht and predicts the
label y based on the updated state ht .
2
Under review as a conference paper at ICLR 2021
Figure 1: A recurrent attention model sequentially observes glimpses from an image and predicts
a class label. At time t, the model actively observes a glimpse gt and its coordinates lt . Given
gt and lt , the feed-forward module F extracts features ft , and the recurrent module R updates
a hidden state to ht . Using an updated hidden state ht , the linear classifier C predicts the class
distribution p(y |ht). At time t+ 1, the model assesses various candidate locations l before attending
an optimal one. It predicts p(y |g, l, ht) ahead of time and selects the candidate l that maximizes
KL[p(y|g, l, ht)||p(y|ht)]. (a) The model predicts the content of g using a Partial VAE to compute
p(y|g, l, ht) without attending to the glimpse g. The normalizing flow-based encoder S predicts the
approximate posterior q(z∖ht), and the decoder D predicts an image X from a sample Z 〜q(z∖ht).
The model uses glimpses from the predicted image X to evaluate p(y∖g,l, ht). Dashed arrows show
a path to compute the lookahead class distribution p(y∖g, l, ht). (b) Alternatively, the model predicts
f, a feature representation of the image x. In f, the features at location l are the features of the
glimpse at location l. The model uses f(l) to predict the lookahead class distribution p(y∖g, l, ht) ≈
p(y∖f (l), ht). Predicting glimpse content in feature space shortens the lookahead path.
3.1	Architecture
As shown in Figure 1(a), our model comprises the following three building blocks. A recurrent
feature aggregator (F and R) maintains a hidden state ht . A classifier (C) predicts the class prob-
abilities p(y∖ht). A normalizing flow-based variational autoencoder (S and D) predicts a complete
image given the hidden state ht; a flow-based encoder S predicts the posterior of a latent variable z
from ht, and a decoder D predicts a complete image from z. The BOED, as discussed in section 3.2,
uses the predicted image to find an optimal location to attend at the next time-step. To distinguish the
predicted image from the actual image, let us call the former X. Henceforth, We crown any quantity
derived from the predicted image X with a (~). Next, we provide details about the three building
blocks of the model, followed by a discussion of the BOED in the context of hard attention.
3.1.1	A Recurrent feature aggregator
Given a glimpse gt and its location lt, a feed-forward module extracts features ft = F(gt, lt), and
a recurrent network updates a hidden state to ht = R(ht-1, ft). Following Mnih et al. (2014), we
define F (gt, lt) = BN (LeakyReLU (Fg (gt) + Fl (lt))) where Fg and Fl are deep networks, and
R(ht-1, ft) = LN (LeakyReLU (Linear(ht-1) + Linear(ft))). Here, BN is a BatchNorm layer
(Ioffe & Szegedy (2015)) and LN is a LayerNorm layer (Ba et al. (2016)).
3.1.2	A classifier
At each time-step t, a linear classifier predicts the distribution p(y∖ht) = C(ht) from a hidden
state ht . As the goal of the model is to predict a label y for an image X, we learn a distribution
p(y∖ht) by minimizing KL[p(y∖X)∖∖p(y∖ht)]. Optimization of this KL divergence is equivalent to
the minimization of the following cross-entropy loss.
LCE(t) = -p(y∖X) log(p(y∖ht))	(1)
3.1.3	A Partial variational Autoencoder
We adapt a variational autoencoder (VAE) to predict the complete image X from the hidden state
ht . A VAE learns a joint distribution between the image X and the latent variable z given ht,
p(X, z∖ht) = p(X∖z)p(z∖ht). An amortized encoder infers the posterior q(z∖X, ht), which is an
approximation of the true posterior p(z∖X, ht), and a decoder infers the likelihood p(X∖z). The
training of VAE requires optimizing the Evidence Lower Bound (ELBO), which involves calculating
KL[q(z∖X, ht)∖∖p(z∖ht)] (Kingma & Welling (2013)). As the hard attention model does not observe
3
Under review as a conference paper at ICLR 2021
the complete image x, it cannot estimate q(z|x, ht). Hence, we cannot incorporate the standard VAE
directly into a hard attention framework.
At the time t, we separate an image x into two parts, ot — the set of regions observed up to t, and
ut — the set of regions as yet unobserved. Ma et al. (2018) observed that in a VAE, ot and ut are
conditionally independent given z, i.e. p(x|z) = p(ut|z)p(ot|z). They predict ut independently
from the sample Z 〜 q(z∖ot), while learning the approximate posterior q(z∖ot) by optimizing the
ELBO on log(p(ot)). They refer to the resultant VAE as a Partial VAE. Recall that the hidden
state ht calculated by our attention model is a summary of the glimpses observed up to t, which is
equivalent to ot, the set of observed regions. Hence, we can write q(z∖ot) as q(z∖ht) in the ELBO of
the Partial VAE.
LPVAE(t) = Eq(z|ot) log(p(ot ∖z)) - KL[q(z∖ot)∖∖p(z)]
= Eq(z|ht) log(p(ot ∖z)) - KL[q(z∖ht)∖∖p(z)]	(2)
In a Partial VAE, p(x, z∖ht) = p(ut∖z)p(ot∖z)p(z∖ht). We implement a decoder D that predicts the
complete image given the sample Z 〜q(z∖ht). Let mt be a binary mask with value 1 for the pixels
observed by the model up to t and 0 otherwise; hence, ot = mt x, where is an element-wise
multiplication. We write the log-likelihood in equation 2 using the mask mt as follows.
log(p(ot ∖Z)) = -0.5 X ∖mt	D(Z) - mt	x∖2 = -0.5 X mt	∖D(Z) - x∖2	(3)
In equation 2, the prior p(Z ) is a Gaussian distribution with zero mean and unit variance. To obtain
expressive posterior q(Z∖ht), we use normalizing flows(Kingma et al. (2016)). As an explicit inver-
sion of the flows is not required, we use auto-regressive Neural Spline Flows (NSF) (Durkan et al.
(2019)) and efficiently implement them using a single feed-forward network with masked weights
as in De Cao et al. (2019). Between the two flow layers, we flip the input (Dinh et al. (2016)) and
normalize it using ActNorm (Kingma & Dhariwal (2018)). In Figure 1(a), the flow-based encoder
S infers the posterior q(Z∖ht) = S(ht). As mentioned earlier, we refer to the prediction from the
Partial VAE as X. The BOED uses X to find an optimal location to attend. 1
3.2	Bayesian Optimal Experiment Design (BOED)
The BOED evaluates the optimality of a set of experiments by measuring the information gain
in the interest parameter due to the experimental outcome. (Chaloner & Verdinelli (1995)). In the
context of hard attention, an experiment is to attend a location l and observe a corresponding glimpse
g = g(X, l). An experiment of attending a location l is optimal if it gains maximum information
about the class label y . We can evaluate the optimality of attending a specific location by measuring
several metrics such as feature variance (Huang et al. (2018)), uncertainty in the prediction (Melville
et al. (2004)), expected Shannon information (Lindley (1956)). For a sequential model, information
gain KL[p(y∖g, l, ht-1)∖∖p(y∖ht-1)] is an ideal metric. It measures the change in the entropy of
the class distribution from one time-step to the next due to observation of a glimpse g at location l
(Bernardo (1979); Ma et al. (2018)).
The model has to find an optimal location to attend at time t before observing the corresponding
glimpse. Hence, we consider an expectation of information gain over the generating distribution of
g. An expected information gain (EIG) is also a measure of Bayesian surprise (Itti & Baldi (2006);
Schwartenbeck et al. (2013)).
EIG(I) = Ep(g∣ι,ht-ι)KL[p(y∖g, l, ht-ι)∖∖p(y∖ht-ι)]	⑷
Inspired by Harvey et al. (2019), we define the distribution p(g∖l, ht-1) as follows.
p(g∖l, ht-1) = Eq(z∣ht-ι)P(g∖z, I) = Eq(z∣ht-ι)S(g(D(Z), Iy)	(5)
Here, δ(∙) is a delta distribution. As discussed in the section 3.1.3, the flow-based encoder S predicts
the posterior q(z∖ht-ι) and the decoder D predicts the complete image X. g(∙) extracts a glimpse
located at l in the predicted image X = D(z). Combining equation 4 and equation 5 yields,
EIG(I) = Eq(z∣ht-ι卢力加侬历⑺⑶⑺乂也-淄以沙电-。]	⑹
1We also tried an alternate method to predict the image x given ht as discussed in section C.
4
Under review as a conference paper at ICLR 2021
To find an optimal location to attend at time t, the model compares various candidates for lt . It pre-
dicts EIG(l) for each candidate l and selects an optimal candidate as lt, i.e. lt = arg maxl EIG(l).
∙-v	∙-v	∙-v
When the model is considering a candidate l, it predicts f = F(g(X,l),l), h = R(ht-ι, f) and
∙-v	∙-v	∙-v
p(y|ht) = C(ht). It uses the distribution p(y|ht) = p(y|g(D(z), l), l, ht-1) to predict EIG in
∙-v
equation 6. We refer to p(y|ht) as the lookahead class distribution computed by anticipating the
content at the location l ahead of time. In Figure 1(a), the dashed arrows show a lookahead step.
3.2.1 Efficient computation of EIG
Convolutional implementation: To compute EIG for all locations simultaneously, we implement
all modules of our model with convolution layers. The model computes EIG for all locations as a
single activation map in a single forward pass. An optimal location is equal to the coordinates of the
pixel with maximum value in the EI G map.
∙-v
Feature generation: During the lookahead step, the feature extractor F computes a feature map f
from the predicted image X. However, we can use the decoder D to directly predict the feature map
∙-v	∙-v
f. Predicting f instead of X achieves two goals. First, the time to compute EIG is reduced as the
model does not require a feature extractor F during the lookahead step (see Figure 1(b)). Second,
the Partial VAE does not have to produce unnecessary details that may later be thrown away by the
feature extractor F, such as the exact color of a pixel.
∙-v
In most cases, the number of elements in the feature map f is greater than the elements in X. The
∙-v
model requires more memory resources and parameters to predict f. So instead, our model predicts
a subsampled feature map that contains features for every nth glimpse. Consequentially, the model
computes an EIG map at low resolution and finds optimal glimpses from a subset of glimpses
separated with stride equal to n.
∙-v
When the decoder predicts a feature map f instead of an image X, the feature likelihood p(fi：tlz)
replaces the likelihood p(ot|z) in equation 2. f1:t are the features of the glimpses observed by the
model until time t. Similar to equation 3, the corresponding log-likelihood is computed as follows.
log(p(f1:t|z)) = -0.5 X mt	|D(z) - f|2 3 4 5 6 7	(7)
∙-v
Here, f is a map containing the features of all glimpses. As f is a feature map of strided glimpses,
we subsample the mask mt and the feature map f in the above log-likelihood.
4	Training and Testing
The training objective is L = PtT=-01 LPVAE(t) + λLCE(t), where λ is a hyperparameter. The
training procedure is given in section B. We use only one sample Z 〜q(z∖ht) to estimate the EIG
map during the training. We found this to work well in practice. We do not compute gradients and
do not update the statistics of normalization layers during the lookahead step. When the decoder
predicts the feature map instead of the image, we pre-train modules F, R, and C for few epochs
with random glimpses. This provides a reasonable target f for the log-likelihood in equation 7.
We naturally achieve exploration early in the training when the model produces noisy output and
computes noisy EIG maps. As the training progress and the predictions become accurate, the
exploitation begins. The testing procedure is shown in Algorithm 1. Unlike in the training phase,
we use P samples of z to estimate the EIG accurately during the prediction phase.
Algorithm 1 Test procedure for the model shown in Figure 1(b)
1: Randomly sample l0; Capture g0 at l0, compute f0, h0 andp(y∖h0)
2: for t ∈ {1, . . . , T - 1} do	. T is the time budget
3:	Sample Zi 〜q(z∖ht-ι) and predict fi; i ∈ {0,...,P 一 1}	. P is the sample budget
4:	Compute hit, p(y∖ht) and EIG = p1 Pi KL[p(y∖hl)∖∖p(y∖ht-ι)]	. equation 6
5:	lt = arg max{E I G}
6:	Capture gt at lt; Compute ft, ht and p(y∖ht)
7: end for
5
Under review as a conference paper at ICLR 2021
Λ32n84
7∙6
O O
AU2nuu4
0.7
0.6
O
2	3	4	5	6
Time
(c)
⅛1 Ikli
0	1	2	3	4	5	6	0	1	2	3	4	5	6
0.5-
⅛0.4
0.2
0.1
0.0
Time
(d)
Time
(e)
M 0.3
0
3
5
6
Figure 2: The classification accuracy as a function of time. A CNN and a soft attention model
observe the entire image to predict the class label. EXPAC, RAM(Mnih et al. (2014)), and GRAM
sequentially attends glimpses in the image and predict the class label. EXPAC and RAM never
observe a complete image and attend the first glimpse at a random location. GRAM observes a gist
of an input at the beginning and predicts glimpse locations from t = 0. (a) MNIST (b) SVHN (c)
CIFAR-10 (d) CIFAR-100 (e) CINIC-10.
5	Experiments
We refer to our model as ‘EXPAC’ which stands for EXPlainable Attention with Content prediction.
We evaluate EXPAC on MNIST (LeCun et al. (1998)), SVHN (Netzer et al. (2011)), CIFAR-10,
CIFAR-100 (Krizhevsky et al. (2009)), and CINIC-10 (Darlow et al. (2018)) datasets. The MNIST
and the SVHN datasets consist of gray-scale and color images of the 0-9 digits, respectively. The
CINIC-10, CIFAR-10 and CIFAR-100 datasets are composed of color images of real-world objects
in the natural setting, categorized into 10, 10 and 100 classes, respectively.
For the MNIST dataset, We use the model shown in Figure 1(a). This model predicts image X and
EIG at full resolution and finds optimal glimpse from a set of all overlapping glimpses. We use
the model shown in Figure 1(b) for the remaining datasets. This model predicts f and EIG for
a set of glimpses separated with stride equal to n. We do not allow this model to revisit glimpses
attended in the past. All models run for T = 7 time-steps and sense glimpses of size 8 × 8 with
stride n = 4. The hyperparameter λ in the training objective is 500, and the sample budget P is 20
for all experiments. We did not observe any performance gain with P > 20. Refer to section A and
B for the remaining details.
5.1	EXPAC predicts class-labels accurately
We compare EXPAC with five baseline models in Figure 2. The RAM is a state-of-the-art model for
hard attention that classify partially observed images (Mnih et al. (2014)). Our implementation of
RAM has a similar structure for the feature extractor F, the recurrent net R, and the classifier C as
EXPAC. Instead of the Partial VAE, RAM has a controller that learns a Gaussian attention policy.
We extend RAM by feeding it with a low-resolution gist of an input at the beginning. To ensure that
gist is used only to predict glimpse locations and not the class labels, the controller receives input
from a separate recurrent net that runs parallel to the recurrent net R. We predict the initial hidden
state of this additional recurrent net from the gist. We refer to this model as Gist-RAM or GRAM.
We also consider a baseline model that attends glimpses on random locations and predicts class
labels. Note that EXPAC and the three baselines described so far observe the image only partially
6
Under review as a conference paper at ICLR 2021
Figure 3: The classification accuracy as a function oftime. iEXPAC interpolates EIG maps to find
optimal glimpses that may overlap. (a) SVHN (b) CIFAR-10 (c) CIFAR-100 (d) CINIC-10.
through a series of glimpses. Additionally, we train a feed-forward CNN and a soft attention model
that observe the entire image to predict the class label. For the soft attention model, we adapt the
method proposed by Jetley et al. (2018).
The CNN outperforms all other models. The performance of hard attention models surpasses the
soft attention model after the former observes a sufficient number of glimpses. Unlike the hard
attention models, the soft attention model exhausts its capacity on computing features and atten-
tion weights for the uninformative regions. Among the hard attention models, EXPAC shows the
best performance. All hard attention models perform better than the random baseline for the digit
datasets. For natural image datasets, RAM and GRAM fall behind the random baseline. As observed
by Sermanet et al. (2014), RAM does not scale well to natural images, probably due to the unimodal
attention policy. Except for the SVHN dataset, GRAM’s accuracy is less than RAM’s accuracy,
despite observing the input gist. We also observed that GRAM overfits the training data and does
not generalize well to the test set.
We speculate that regularizing the feature space for classification is very important for the hard atten-
tion models. The gradients from the ELBO and the REINFORCE objective regularize this feature
space in EXPAC and RAM, respectively. Moreover, the ELBO appears to be a better regularizer
than the REINFORCE objective. Zheng et al. (2015) also observed that the use of generative loss
regularizes the hard attention model. GRAM uses separate feature spaces for the classifier and the
controller; hence the former is not regularized by the latter. GRAM overfits the train set and does
not perform well on the test set due to a lack of regularization.
5.2	EXPAC performs well with the interpolated EIG
To let EXPAC find and attend overlapping glimpses with unit stride, we assume a smoothness
prior and upsample the EIG map using bicubic interpolation to get EIG values for all overlapping
glimpses. We refer to this model as interpolation-EXPAC or iEXPAC. Figure 3 shows a comparison
between EXPAC and iEXPAC. iEXPAC performs better than or comparable to EXPAC during initial
time-steps. As time progresses, EXPAC observes more input regions than iEXPAC through strided
glimpses and outperforms iEXPAC. However, iEXPAC still performs better than RAM and GRAM,
which also attends the overlapping glimpses with unit stride. Next, we assess the policies used by
these models.
5.3	EXPAC learns a useful policy for classification
We compared the accuracy of the hard attention models as a function of the area covered in the
image. We found that iEXPAC achieves an accuracy similar to the other models by covering less
area in the image. The result suggests that iEXPAC observes few but informative regions (refer
section D.1). However, this analysis provides only a narrow perspective on the policies used by
different models, as each model learns a classifier that adapts to its policy. To better understand the
policies, we cover the glimpses observed by the hard attention models and let the baseline CNN
(from Figure 2) classify the occluded images. Accuracy should drop if the object of interest is
occluded. This analysis is similar to the one performed by Elsayed et al. (2019). The results for
natural image datasets are shown in Figure 4 (refer section D.2 for the results on digit datasets). We
do not observe any generalizable trend for GRAM. However, we observe that the accuracy drops
faster by covering the glimpses attended by the RAM compared to iEXPAC. It appears that the
7
Under review as a conference paper at ICLR 2021
(a)	(b)	(c)
Figure 4: The classification accuracy ofa CNN as a function of the area occluded. A CNN classifies
images with the glimpses attended by various hard attention models occluded. (a) CIFAR-10 (b)
CIFAR-100 (c) CINIC-10.
Figure 5: Visualization of the predictions from the Partial VAE. In each panel, from left to the right:
time step, glimpses observed until time t, eight samples of x. The content in X on the observed
locations is consistent with the glimpses.
RAM attend to the object of interest more frequently than iEXPAC. However, iEXPAC achieves a
better performance. The use of EIG in BOED explains the behavior of iEXPAC. If the attended
glimpse provides evidence in favor of one class and the Partial VAE also generates features in favor
of the same class, then EIG for areas characterizing that class is low. In this case, iEXPAC explores
areas with high EIG, away from the regions approving of the believed class. Consequently, iEXPAC
often attend regions outside the object of interest. Being a measure of Bayesian surprise, EIG
promotes iEXPAC to seek glimpses that seem novel according to the latest belief (Itti & Baldi
(2006); Schwartenbeck et al. (2013)). During the process, iEXPAC may collect evidence favoring
an actual class that may have initially appeared unlikely and change the decision.
5.4	Visualization of the EIG maps and the attention policy used by iEXPAC
We display the generated samples of X for MNIST dataset in Figure 5. For the remaining datasets, We
predict f . Figure 6 shows the EIG maps and the optimal glimpses found by iEXPAC on CIFAR-
10 images. First, notice that the EIG maps are multi-modal. Second, activity in the EIG maps
reduces as the model gathers sufficient glimpses to predict a class label. In Figure 6(a), activity in
EIG map is reduced as the model settles on a class ‘Dog’. Once it discovers left ear of a cat at
t = 3, the activity in the EIG map increases at t = 4. The model changes its decision to ‘Cat’
once it discovers the right ear at t = 5. In Figure 6(b), the model predicts class ‘Airplane’ at time
t = 3 once it discovers body of an airplane. At the next time-step, the model expects reduced
information gain on the areas Where the remaining body of the airplane may lie. In Figure 6(c), the
model predicts class ‘Deer’ once it discovers face of the animal at t = 4. Notice reduced activity in
the EIG maps at t = 5 and 6.
5.5	EXPAC attains higher accuracy with flexible posterior: an ablation study
We investigate the necessity of a flexible distribution for the posterior q(z|ht) and, therefore, the
necessity of normalizing floWs in the encoder S. To this end, We model the posterior With a unimodal
Gaussian distribution and let S output mean and diagonal covariance of a Gaussian. We do not
use floW layers in this case. Figure 7 shoWs the result for natural image datasets. Refer Figure
11 for results on digit datasets. Modeling a complex posterior using normalizing floWs improves
accuracy during early time steps. The gap in the accuracy of the tWo reduces With time. The reason
8
Under review as a conference paper at ICLR 2021
Figure 6: Visualization of the EIG maps and the glimpses observed by iEXPAC on CIFAR-10 im-
ages. The top row shows the entire image and the EI G maps for time t = 1 to 6. The bottom row
shows glimpses attended by iEXPAC. The model observes the first glimpse at a random location.
We display the entire image for reference; iEXPAC never observes the whole image. (a-c) success
cases (d) failure case.
0.80
0.75
0.70
g 0.65
S 0.60
0.55
0.50
0.45
0	1	2	3	4	5	6
Time
(a)	(b)	(C)
Figure 7: The classification accuracy as a function of time. We compare performance of EXPAC
with and without using normalizing flows. (a) CIFAR-10 (b) CIFAR-100 (c) CINIC-10.
being as follows. Ideally, the Partial VAE should predict all possibilities of X (or f) consistent with
the observed region ot. Initially, when EXPAC observes a small region, a complex multi-modal
posterior helps determine multiple plausible images from different classes. A unimodal posterior
fails to cover all possibilities. As time progress, EXPAC observes a large region, and the associated
possibilities decrease. Hence, the performance gap between a complex and a uni-modal posterior
decreases. Eventually, only a single possibility ofa complete image remains. Then, the performance
ofa unimodal posterior meets the performance ofa complex posterior.
6 Conclusion
We presented a hard attention model that uses BOED to find the optimal locations to attend when the
image is observed only partially. To find an optimal location without observing the corresponding
glimpse, the model uses Partial VAE to predict the content of the glimpse. The model predicts
the content in either the image space or the feature representation space. The predicted content
enables the model to evaluate and compare the expected information gain (EIG) of various candidate
locations, from which the model selects a candidate with optimal EIG. The model predicts feature
representations and EIG of a subset of glimpses for computational efficiency. We interpolate EIG
maps to let the model find optimal glimpses that may overlap. The attention policy used by our
model is non-parametric, multimodal, and explainable. We trained our model with a differentiable
objective and tested it on five datasets. We found that our model attends glimpses highly relevant to
the classification task and achieves better performance than the other baseline models.
9
Under review as a conference paper at ICLR 2021
References
Subutai Ahmad. Visit: a neural model of covert visual attention. In Advances in neural information
processing systems,pp. 420-427, 1992.
Bogdan Alexe, Nicolas Heess, Yee W Teh, and Vittorio Ferrari. Searching for objects driven by
context. In Advances in Neural Information Processing Systems, pp. 881-889, 2012.
Jimmy Ba, Volodymyr Mnih, and Koray Kavukcuoglu. Multiple object recognition with visual
attention. arXiv preprint arXiv:1412.7755, 2014.
Jimmy Ba, Russ R Salakhutdinov, Roger B Grosse, and Brendan J Frey. Learning wake-sleep
recurrent attention models. In Advances in Neural Information Processing Systems, pp. 2593-
2601, 2015.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Jose M Bernardo. Expected information as expected utility. the AnnalS of Statistics, pp. 686-690,
1979.
Nicholas J Butko and Javier R Movellan. I-pomdp: An infomax model of eye movement. In 2008
7th IEEE International Conference on Development and Learning, pp. 139-144. IEEE, 2008.
Nicholas J Butko and Javier R Movellan. Optimal scanning for faster object detection. In 2009
IEEE Conference on Computer Vision and Pattern Recognition, pp. 2751-2758. IEEE, 2009.
Kathryn Chaloner and Isabella Verdinelli. Bayesian experimental design: A review. Statistical
Science, pp. 273-304, 1995.
Luke N Darlow, Elliot J Crowley, Antreas Antoniou, and Amos J Storkey. Cinic-10 is not imagenet
or cifar-10. arXiv preprint arXiv:1810.03505, 2018.
Nicola De Cao, Ivan Titov, and Wilker Aziz. Block neural autoregressive flow. arXiv preprint
arXiv:1904.04676, 2019.
Misha Denil, Loris Bazzani, Hugo Larochelle, and Nando de Freitas. Learning where to attend with
deep architectures for image tracking. Neural computation, 24(8):2151-2184, 2012.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. In
Advances in Neural Information Processing Systems, pp. 7511-7522, 2019.
Gamaleldin Elsayed, Simon Kornblith, and Quoc V Le. Saccader: improving accuracy of hard
attention models for vision. In Advances in Neural Information Processing Systems, pp. 702-
714, 2019.
SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hin-
ton, et al. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in
Neural Information Processing Systems, pp. 3225-3233, 2016.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and SM Ali Eslami. Conditional neural processes. In
International Conference on Machine Learning, pp. 1704-1713, 2018a.
Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo J Rezende, SM Eslami,
and Yee Whye Teh. Neural processes. arXiv preprint arXiv:1807.01622, 2018b.
Karol Gregor, Ivo Danihelka, Alex Graves, Danilo Jimenez Rezende, and Daan Wierstra. Draw: A
recurrent neural network for image generation. In ICML, 2015.
William Harvey, Michael Teng, and Frank Wood. Near-optimal glimpse sequences for improved
hard attention neural network training. arXiv preprint arXiv:1906.05462, 2019.
10
Under review as a conference paper at ICLR 2021
Sheng-Jun Huang, Miao Xu, Ming-Kun Xie, Masashi Sugiyama, Gang Niu, and Songcan Chen.
Active feature acquisition with supervised matrix completion. In Proceedings of the 24th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 1571-1579,
2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Laurent Itti and Pierre F Baldi. Bayesian surprise attracts human attention. In Advances in neural
information processing systems, pp. 547-554, 2006.
Laurent Itti and Christof Koch. A saliency-based search mechanism for overt and covert shifts of
visual attention. Vision research, 40(10-12):1489-1506, 2000.
Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid
scene analysis. IEEE Transactions on pattern analysis and machine intelligence, 20(11):1254-
1259, 1998.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Ad-
vances in neural information processing systems, pp. 2017-2025, 2015.
Saumya Jetley, Nicholas A Lord, Namhoon Lee, and Philip HS Torr. Learn to pay attention. In
International Conference on Learning Representations, 2018.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in neural information processing systems, pp. 10215-10224, 2018.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In Advances in neural information
processing systems, pp. 4743-4751, 2016.
Christof Koch and Shimon Ullman. Shifts in selective visual attention: towards the underlying
neural circuitry. In Matters of intelligence, pp. 115-141. Springer, 1987.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
Hugo Larochelle and Geoffrey E Hinton. Learning to combine foveal glimpses with a third-order
boltzmann machine. In Advances in neural information processing systems, pp. 1243-1251, 2010.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Dennis V Lindley. On a measure of the information provided by an experiment. The Annals of
Mathematical Statistics, pp. 986-1005, 1956.
Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Sebastian
Nowozin, and Cheng Zhang. Eddi: Efficient dynamic discovery of high-value information with
partial vae. arXiv preprint arXiv:1809.11142, 2018.
Prem Melville, Maytal Saar-Tsechansky, Foster Provost, and Raymond Mooney. Active feature-
value acquisition for classifier induction. In Fourth IEEE International Conference on Data Min-
ing (ICDM’04), pp. 483-486. IEEE, 2004.
Volodymyr Mnih, Nicolas Heess, Alex Graves, et al. Recurrent models of visual attention. In
Advances in neural information processing systems, pp. 2204-2212, 2014.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
11
Under review as a conference paper at ICLR 2021
Bruno A Olshausen, Charles H Anderson, and David C Van Essen. A neurobiological model of vi-
sual attention and invariant pattern recognition based on dynamic routing of information. Journal
OfNeuroscience ,13(11):4700-4719,1993.
Lucas Paletta, Gerald Fritz, and Christin Seifert. Q-learning of sequential attention for visual object
recognition from informative local descriptors. In Proceedings of the 22nd international confer-
ence on Machine learning, pp. 649-656, 2005.
Marc’Aurelio Ranzato. On learning where to look. arXiv preprint arXiv:1405.5488, 2014.
Raymond D Rimey and Christopher M Brown. Controlling eye movements with hidden markov
models. International Journal of Computer Vision, 7(1):47-65, 1991.
Philipp Schwartenbeck, Thomas FitzGerald, Ray Dolan, and Karl Friston. Exploration, novelty,
surprise, and free energy minimization. Frontiers in psychology, 4:710, 2013.
Pierre Sermanet, Andrea Frome, and Esteban Real. Attention for fine-grained categorization. arXiv
preprint arXiv:1412.7054, 2014.
Charlie Tang, Nitish Srivastava, and Russ R Salakhutdinov. Learning generative models with visual
attention. In Advances in Neural Information Processing Systems, pp. 1808-1816, 2014.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048-2057, 2015.
Yin Zheng, Richard S Zemel, Yu-Jin Zhang, and Hugo Larochelle. A neural autoregressive approach
to attention-based recognition. International Journal of Computer Vision, 113(1):67-79, 2015.
12
Under review as a conference paper at ICLR 2021
A Neural Architecture
We implement EXPAC for MNIST dataset based on Mnih et al. (2014). We replace ReLU with
LeakyReLU and add BN in their architecture. Table 1 shows the neural architecture of EXPAC
for color datasets. We implement linear layers using 1 × 1 convolution layers. Note that all modules
use an indispensable number of layers. Fl is the shallowest network to learn the non-linear repre-
sentation of the input. Fg uses the least possible layers to achieve the effective receptive field equal
to the area of a single glimpse. The decoder D uses the smallest number of ConvT ranspose2d
and Conv2d to generate the feature maps of required spatial dimensions and refine them based on
the global context. We use linear classifiers for SVHN, CIFAR-10 and CINIC-10 datasets and use
a two-layer classifier for CIFAR-100. The dimensionality of f, ht and z for various datasets are
mentioned in the Table 2.
	Module	Architecture
Recurrent feature aggregator	Fl	Conv2d(k = 1) — LeakyReLU — BN — Conv2d(k = 1)
	Fg	3 X {Conv2d(k = 3) — LeakyReLU — BN} — Conv2d(k = 2)
	F (gt,lt)	BN (LeakyReLU (Fg 加 + Fl(It))	一
	R(ht-ιf	LN (LeakyReLU (Conv2d(k = 1)(ht-ι) + Conv2d(k = 1)(ft)))
Classifier	C	Conv2d(k = 1)
Partial VAE	S	4 X {ActNorm — permute — NSF}
	D	3 X {ConvTranspose2d(k = 3) — LeakyReLU — BN}— 4 X {Conv2d(k = 3,p =1) — LeakyReLU — BN}
Table 1: Neural architecture of EXPAC for SVHN, CIFAR-10 and CINIC-10 datasets. k
kernel_size, P = padding.
	f	ht	Z
MNIST	2566~	256	^256^
SVHN	128	512	256
CIFAR-10	128	512	256
CIFAR-100	256	1024	256
CINIC-10	128	512	256
Table 2: The size of f, ht and z.
B	Optimization details
We train the model using Adam optimizer (Kingma & Ba (2014)) with a learning rate of 0.001 and
default β1 and β2, and divide the learning rate by ten on a plateau until convergence. We trained all
models on a single Tesla V100 GPU with 16GB of memory for approximately a day. The training
procedure for the model shown in Figure 1(b) is shown in Algorithm 2.
C Alternate Generative Model
We try an alternate generative model for the image x given ht . At time t, we divide the image x
into two parts, ot and ut, a set of regions observed by the model and a set of regions that are yet
unobserved. The set of observed regions ot is equivalent to the glimpses observed by the model until
time t, i.e ot = {g1:t , l1:t }. Recall that the hidden state ht summarizes these glimpses. Hence, we
write p(x|ht) = p(x|ot) = p(ut|ot)p(ot). Forp(ut|ot) and p(ot), we can learn two latent variable
models which optimize the ELBO on log(p(ut|ot)) and log(p(ot)). These ELBOs are similar to the
ones used to train Neural Processes (Garnelo et al. (2018b;a)) and Partial VAE(Ma et al. (2018)),
13
Under review as a conference paper at ICLR 2021
Algorithm 2 Training procedure for the model shown in Figure 1(b)
1	while not converged do
2	:	L=0
3	
4	:	for t ∈ {0, . . . , T - 1} do	. T is the time budget :	if t = 0 then
5	:	Randomly sample l0
6	:	else with no gradient:
7	Compute ht, p(y∖htt and EIG	. Use f and ht-ι computed at t - 1
8	:	lt = arg max{EIG}	. equation 6
9	:	end if
10	Capture gt at lt, Compute ft, ht, p(y∖htt, q(z∖htt and f
11	:	L = L + LPVAE + λLCE	. equation 2,1
12	:	end for
13	Compute gradients ∂θ and update model parameters θ
14	: end while
respectively.
LNP = Eq(z∣ut,ot) log(p(ut∣z, Ot)) — KL[q(z∣Ut,Ot)∣∣p(Z∣Ot)]
=Eq(z∣χ,ht) Iog(P(Ut区 ht)) - κL[q(zlx, ht)Ilp(Hht)]	⑻
LPVAE = Eq(z|ot) log(p(ot|z)) — KL[q(z|ot)||p(z)]
= Eq(z|ht) log(p(otIz)) — KL[q(zIht)IIp(z)]	(9)
C.1 Implementation
We implement a single VAE that takes image x and hidden state ht as input and reconstructs the
image x. The design of this VAE is an extension of Partial VAE from Figure 1 with a conditional
encoder E and normalizing flows S. We train this VAE with a combination ofELBOs in equation 8
and equation 9. The prior p(z) is a Gaussian distribution with zero mean and unit variance. A
conditional encoder E infers the posterior q(z∣χ, ht). We use conditional normalizing flows to
establish a relation between q(z∖ht) and q(z∣ht), i.e. Z = S(西 ht).
q(zlht) = q(z∖ht)∣det(网∂]tt) I	(10)
As in Figure 1, the flow-based encoder S and the decoder D predicts the approximate posterior
q(z∖htt and the complete image X, respectively. Given the binary mask mt as discussed in section
3.1.3, we compute the likelihoods in equation 8 and equation 9 as follows.
log(p(θt∖z)t = - 2 X mt Θ ∖D(z) — x∖2	(11)
log(p(ut∖z, htt) = -1 X(1 - mt) Θ D(S(况 ht)) - x∖2	(12)
C.2 Performance Analysis of the Alternate Generative Model
Here we compare our model’s accuracy when the generative model is Partial VAE versus a combi-
nation of Partial VAE and Neural Processes. We also train the alternate VAE with just the ELBO of
Neural Processes. Figure 8 shows the result. We do not find a single generative model that performs
well across datasets. We selected Partial VAE for our model as it does not use an encoder E and S,
which require additional parameters.
D Additional Results
D. 1 EXPAC attends to glimpses relevant to Classification
We assess the usefulness of the glimpses attended by EXPAC/iEXPAC, RAM, and GRAM for the
classification task. Figure 9 shows the models’ accuracy as a function of the area observed in an
14
Under review as a conference paper at ICLR 2021


'3 m
Fa)
一
re
, 一 e
MiMMMi12
.9.87.6.54
≡6
S
e
3∙)
2τ K
1
。
.87.6.54
X3enuu<

Figure 8:	Classification accuracy as afunction oftime. We test various generative models to predict
the image content. (a) MNIST (b) SVHN (c) CIFAR-10.
0.5-
0.4
+ EXPAC(Ours)
—H RAM
—I— GRAM
0.1	0.2	0.3	0.4	0.5
Area Covered
(a)
1.0
(b)
0.7
&0.6
E
n
u
M 0.5
0.4
0.1
0.2	0.3
Area Covered
(C)
0.4
0.1	0.2	0.3	0.4
Area Covered
(d)
0.1
0.2	0.3
Area Covered
(e)
0.4
Figure 9:	Classification accuracy of the hard attention models as a function of the area covered. (a)
MNIST (b) SVHN (C) CIFAR-10 (d) CIFAR-100 (e) CINIC-10.
image. Compared to the RAM and GRAM, EXPAC/iEXPAC aChieves high aCCuraCy by observing
less area. The results indiCate that EXPAC/iEXPAC observes regions that are more relevant to the
ClassifiCation task than the other two models. Attending few but task-relevant regions lead to better
performanCe.
D.2 EXPAC learns a useful policy for classification
To get an insight into the attention poliCies used by different models, we oCClude the glimpses
attended by the hard attention models and let a baseline CNN prediCt the Class label from an oCCluded
image. We expeCt a drop in aCCuraCy when the foreground objeCt is oCCluded. The results for natural
image datasets are disCussed in seCtion 5.3. Here, we disCuss results for the digit datasets. Unlike
in the Case of natural images, CNN’s aCCuraCy for EXPAC/iEXPAC drops at a greater or similar
rate as RAM and GRAM. In natural images, iEXPAC explores baCkground regions with high EIG
to find evidenCe for different Classes. In digit datasets, the foreground objeCt is at the Center and
is not related to the surrounding. HenCe, EXPAC learns to prediCt low EIG on the baCkground
and always finds an optimal glimpse on the foreground. The aCCuraCy of CNN drops as EXPAC
attends to informative glimpses on the foreground. NotiCe that the aCCuraCy of CNN drops faster for
GRAM on the SVHN dataset. The trend suggests that GRAM finds the most informative glimpses.
The GRAM also aChieves better performanCe than the RAM for this dataset, as shown in Figure 2.
15
Under review as a conference paper at ICLR 2021
0.1	0.2	0.3	0.4	0.5
Area Occluded
(a)
+ iEXPAC(Ours)
—I— RAM
-H GRAM
<! J Ξ Li? J L !
0.1	0.2	0.3	0.4
Area Occluded
(b)
Figure 10:	The classification accuracy of a CNN as a function of the area occluded. A CNN clas-
sifies images with the glimpses attended by various hard attention models occluded. (a) MNIST (b)
SVHN.
1.0
°" 111111 - 111111
“IM E
"	.∙r	■	■ EXPAC - no flows (Ours)	∣,
■_■__■_■__■_■___■_ ∏ 4 I，， ， I I I ， ， I I I ， ， I I ： ， ，I ，,， ，I ，］，I
0	1"	2"	3	4	5"	6'	0	1	2	3	4	5	6
0
3
5
6
Time
(a)
Time
(b)
Figure 11:	The classification accuracy as a function of time. We compare performance of EXPAC
with and without using normalizing flows. (a) MNIST (b) SVHN.
However, iEXPAC outperforms GRAM. Unlike GRAM, iEXPAC learns a better feature space for
classification due to the regularization provided by the generative objective during the training.
16