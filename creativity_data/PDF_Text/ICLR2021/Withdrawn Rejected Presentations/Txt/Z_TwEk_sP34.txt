Under review as a conference paper at ICLR 2021
Does Adversarial Transferability Indicate
Knowledge Transferability?
Anonymous authors
Paper under double-blind review
Ab stract
Despite the immense success that deep neural networks (DNNs) have achieved,
adversarial examples, which are perturbed inputs that aim to mislead DNNs to
make mistakes, have recently led to great concerns. On the other hand, adversar-
ial examples exhibit interesting phenomena, such as adversarial transferability.
DNNs also exhibit knowledge transfer, which is critical to improving learning
efficiency and learning in domains that lack high-quality training data. To uncover
the fundamental connections between these phenomena, we investigate and give an
affirmative answer to the question: does adversarial transferability indicate knowl-
edge transferability? We theoretically analyze the relationship between adversarial
transferability and knowledge transferability, and outline easily checkable suffi-
cient conditions that identify when adversarial transferability indicates knowledge
transferability. In particular, we show that composition with an affine function
is sufficient to reduce the difference between the two models when they possess
high adversarial transferability. Furthermore, we provide empirical evaluation for
different transfer learning scenarios on diverse datasets, showing a strong positive
correlation between the adversarial transferability and knowledge transferability,
thus illustrating that our theoretical insights are predictive of practice.
1	Introduction
Knowledge transferability and adversarial transferability are two fundamental properties when a
learned model transfers to other domains. Knowledge transferability, also known as learning trans-
ferability, has attracted extensive studies in machine learning. Long before it was formally defined,
the computer vision community has exploited it to perform important visual manipulations (Johnson
et al., 2016), such as style transfer and super-resolution, where pretrained VGG networks (Simonyan
& Zisserman, 2014) are utilized to encode images into semantically meaningful features. After the
release of ImageNet (Russakovsky et al., 2015), pretrained ImageNet models (e.g., on TensorFlow
Hub or PyTorch-Hub) has quickly become the default option for the transfer source, because of its
broad coverage of visual concepts and compatibility with various visual tasks (Huh et al., 2016).
Adversarial transferability, on the other hand, is a phenomenon that adversarial examples can not
only attack the model they are generated against, but also affect other models (Goodfellow et al.,
2014; Papernot et al., 2016). Thus, adversarial transferability is extensively exploited to inspire
black-box attacks (Ilyas et al., 2018; Liu et al., 2016). Many theoretical analyses have been conducted
to establish sufficient conditions of adversarial transferability (Demontis et al., 2019; Ma et al., 2018).
Knowledge transferability and adversarial transferability both reveal some nature of machine learning
models and the corresponding data distributions. Particularly, the relation between these two phenom-
ena interests us the most. We begin by showing that adversarial transferability can indicate knowledge
transferability. This tie can potentially provide a similarity measure between data distributions, an
identifier of important features focused by a complex model, and an affinity map between complicated
tasks. Thus, we believe our results have further implications in model interpretability and verification,
fairness, robust and efficient transfer learning, and etc.
To the best of our knowledge, this is the first work studying the fundamental relationship between
adversarial transferability and knowledge transferability both theoretically and empirically. Our main
contributions are as follows.
1
Under review as a conference paper at ICLR 2021
•	We formally define two quantities, τ1 and τ2, to measure adversarial transferability from
different aspects, which enables in-depth understanding of adversarial transferability from a
geometric point of view in the feature representation space.
•	We derive an upper bound for knowledge transferability with respect to adversarial transfer-
ability. We rigorously depict their underlying relation and show that adversarial transferabil-
ity can indicate knowledge transferability.
•	We conduct thorough controlled experiments for diverse knowledge transfer scenarios (e.g.
knowledge transfer among data distributions, attributes, and tasks) on benchmark datasets
including STL-10, CIFAR-10, CelebA, Taskonomy-data, and four language datasets. Our
empirical results show strong positive correlation between adversarial and knowledge
transferability, which validates our theoretical prediction.
2	Related work
Knowledge transferability has been widely applied in scenarios where the available data for certain
domain is limited, and has achieved great success (Van Opbroek et al., 2014; Wurm et al., 2019; Wang
et al., 2017; Kim & Park, 2017; Maqueda et al., 2018; Devlin et al., 2018). Several studies have been
conducted to understand the factors that affect knowledge transferability (Yosinski et al., 2014; Long
et al., 2015b; Wang et al., 2019; Xu et al., 2019; Shinya et al., 2019). Empirical observations show
that the correlation between learning tasks (Achille et al., 2019; Zamir et al., 2018), the similarity of
model architectures, and data distribution are all correlated with different knowledge transfer effects.
Adversarial Transferability has been observed by several works (Papernot et al., 2016; Goodfellow
et al., 2014; Joon Oh et al., 2017). Since the early work, a lot of studies have been conducted, aiming
to further understand the phenomenon and design more transferable adversarial attacks. Regardless of
the threat model, a lot of attack methods have been proposed to boost adversarial transferability (Zhou
et al., 2018; Demontis et al., 2019; Dong et al., 2019; Xie et al., 2019). Naseer et al. (2019) propose
to produce adversarial examples that transfer cross-domain via a generative adversarial network. In
addition to the efficacy, efficiency (Ilyas et al., 2018) and practicality (Papernot et al., 2017) are
also optimized. Beyond the above empirical studies, there is some work dedicated to analyzing this
phenomenon, showing different conditions that may enhance adversarial transferability (Athalye et al.,
2018; Tramer et al., 2017; Ma et al., 2018; Demontis et al., 2019). Building upon these observations,
it is clear that there exist certain connections between adversarial transferability and other knowledge
transfer scenarios, and here we aim to provide the first theoretic justification to verify it and design
systematic empirical studies to measure such correlation.
3	Adversarial Transferability vs. Knowledge Transferability
In this section, we establish connections between adversarial examples and knowledge transferability
rigorously. We first formally state the problem studied in this section. Then, we move on to
subsection 3.1 to introduce two metrics that encode information about adversarial attacks. Finally, we
present our theoretical results about the relationship between adversarial and knowledge transferability
in subsection 3.2.
Notations. We use blackboard bold to denote sets, e.g., R. We use calligraphy to denote distributions,
e.g., D. The support of a distribution D is denoted as supp(D). We use bold lower case letters to
denote vectors, e.g., X ∈ Rn. We use bold uppercase letter to denote a matrix, e.g., A. We use At to
denote the Moore-Penrose inverse of matrix A. We use ◦ to denote the composition of functions,
i.e., g ◦ f (x) = g(f (x)). We use ∣∣ ∙ ∣∣2 to denote Euclidean norm induced by standard inner product
h∙, •). Given a function f, We use f (x) to denote its evaluated value at x, and We use f to represent
this function in function space. We use(•, ∙)d to denote inner product induced by distribution D,
i.e., hf1,f2iD = Ex〜Dhfι(x), f2(x)i. Accordingly, we use ∣∣ ∙ ∣∣d to denote a norm induced by
inner product h∙, )D, i.e., ∣f ∣d = Phf, f〉d. For a matrix function F : supp(D) → Rd×m, we
define its L2 (D)-norm in accordance With matrix 2-norm as ∣F ∣D,2 = ,Ex-D∣F(x)k2. We define
projection operator proj(∙, r) to project a matrix to a hyperball of spectral norm radius r, i.e.,
A,	if ∣A∣2 ≤ r
rA∕kA∣∣2	if ∣∣A∣∣2 >r.
proj(A, r)
2
Under review as a conference paper at ICLR 2021
Setting. Assume We are given a target problem defined by data distribution X ~ D, where X ∈ Rn,
and y : Rn → Rd represent the ground truth labeling function. As a first try, a reference model
fT : Rn → Rd trained on the target dataset is obtained through optimizing over a function class
fT ∈ FT. Now suppose we have a source model fS : Rn → Rm pretrained on source data, and we
are curious how would fS transfer to the target data D?
Knowledge transferability. Given a trainable function g : Rm → Rd, where g ∈ G is from a small
function class for efficiency purpose, we care about whether fs can achieve low loss L(∙; y, D), e.g.,
mean squared error, after stacking with a trainable function g comparing with fT , i.e.,
min L(g ◦ fS;y,D) compare with L(fT; y, D).
g∈G
Clearly, the solution to this optimization problem depends on the choice of G. Observing that in
practice it is common to stack and fine-tune a linear layer given a pretrained feature extractor, we
consider the class of affine functions. Formally, the problem that is studied in our theory is stated as
follows.
Problem 1. Given a reference model fT trained on target distribution D, and a source model fS
pre-trained on source data. Can we predict the best possible performance of the composite function
g ◦ fs on D, where g isfrom a bounded afinefUnction class, given adversarial transferability between
fS and fT ?
3.1	Adversarial Transferability
We use the '2-norm to characterize the effectiveness of an attack.
Definition 1 (Virtual Adversarial Attack (Miyato et al., 2018)). Given a model f : Rn → Rd, the
attack on point X within -ball is defined as arg maxkδk≤ kf (x) - f(x + δ)k2. As this is intractable
in practice, we consider the use of the tangent function to approximate the difference:
δf,e(x)	=	argmax ∣∣Vf(x)>δk2,
kδk≤
where Vf (x) ∈ Rn ×d is the Jacobian matrix. The e will be dropped in clear context or when it is
irrelevant.
To provide a quantitative view of adversarial transferability, we define two metrics τ1 and τ2 . Both
the metrics are in the range of [0, 1], where higher values indicate more adversarial transferability.
Definition 2 (Adversarial Transferability (Angle)). Given two function f1, f2, we assume they have
the same input dimension, and may have different output dimensions. The Adversarial Transferability
(Angle) of f1 and f2 at point X is defined as the squared cosine value of the angle between the two
attacks, i.e.,
τ1(X)
hδfι (x), δf2(x)i2
kδfι (χ)k2 -I/ (χ)∣2.
We denote its expected value as τι = Eχ~D[τι(x)].
Intuitively, τ1 characterizes the similarity of the two attacks. The higher the cosine similarity, the
better they can be attacked together. Noting that we are suggesting to use the square of their cosine
values, which means that cosine value being either 1 or -1 has the same indication of high knowledge
transferability. This is because fine-tuning the last layer can rectify such difference by changing
the sign of the last linear layer. However, it is not sufficient to fully characterize how good fS will
perform only knowing the angle of two attack directions. For example, it is not difficult to construct
two functions with highest τ1 = 1, but not transferable with affine functions. Moreover, it is also
oberserved in our experiments that only τ1 is not sufficient.
Therefore, in addition to the information of attacks δf captured by τ1 , we also need information about
deviation of a function given attacks. We denote the deviation of a function f, given attack δ(X), as
f(X + δ(X)) - f (X), and we define its approximation as
∆f,δ (X) = Vf (X)>δ(X).	(1)
Accordingly, we define another metric to answer the following question: applying f1 ’s adversarial
attacks on both the models, how much can the deviation of their function value be aligned by affine
transformations?
3
Under review as a conference paper at ICLR 2021
Definition 3 (Adversarial Transferability (Deviation)). Given two functions f1, f2 with the same input
dimensions and potentially different output dimensions, the Adversarial Transferability (Deviation)
of adversarial attacks from f1 to f2 given data distribution D is defined as
f1→f2
τ2
h2∆f2,δf1 - A∆f1,δf1, A∆f1,δf1 iD
k∆f2,δf1 kD
where A is a constant matrix defined as
A = Proo (Ex 〜D PfzAf1 (X)δ∕iM (X)>] (Ex 〜D [∆f1,δf1 (x)∆f1,δf1 (x)>])t
IAzMkD
,IAiMkD
We note that A is the best linear map trying to align the two deviations (∆f2,δf and ∆f1,δf ) in
the function space. It serves as a guess on the best linear map to align f1 and f2, using only the
information from adversarial attacks. To have better sense of τ2 and the relationships with other
quantities, we present an example for visual illustration in Figure 1. Note that high τ2 does not
necessarily require ∆f1 ,δf and ∆fz ,δf to be similar, but they can be well aligned by the constant
linear transformation A. We refer to the proof of Proposition 1 at section B in appendix for detailed
explanation of τ2 .
Proposition 1. Both τ1 and τ2 are in [0, 1].
3.2	Adversarial Transferability Indicates Knowledge Transferability
In this subsection, we will provide our theoret-
ical results. First, to have a better intuition, we
will show a special case where the theorems
are simplified, i.e., where fS and fT are both
Rn → R. Then, we present the general case
where fS and fT are multi-dimensional. Note
that their output dimensions are not necessarily
the same.
When fS and fT are both Rn → R, the τ1 and
τ2 come out in a surprisingly elegant form. Let
Figure 1: Illustration of the key variables.
us show what the two metrics are to have further intuition on what τ1 and τ2 characterize.
First, let us see what the attack is in this case. As function f has one-dimensional output, its gradient
is a vector Vf ∈ Rn, Thus,
δf,(x)
arg max kVf(X)>δk2
kδk≤
6Vf(X)
kVf(x)k2
is simply the gradient with its scale normalized. Then, the τ1 becomes
τ1(X)
hv fS (X), VfT (X) i2
kVfs(x)k2 ∙kVfT(x)k2,
which is the squared cosine (angle) between two gradients. For τ2, the matrix A degenerates to a
scalar constant, which makes τ2 simpler as well, i.e.,
A =。方,δfS, ZS,δfs iD	d	fsTfT =	US,δfS, ZT,δfs iD
=	k∆fS,δfS kD	,	2	= k∆fS,δfS kD ∙k∆fτ,δfS kD.
We can see, in this case τ2 is interestingly in the same form of the first metric τ1. We will simply use
τ2 to denote τ2fS →fT afterwards.
Accordingly, when fS and fT are both Rn → R, the result also comes out in an elegant form. In this
case, adversarial attacks reflect all the information of the gradients of the two models, enabling τ1
and τ2 to encode all the information we need to prove the following theorem.
Theorem 1.	For two functions fS and fT that both are Rn → R, there is an affine function g : R → R,
such that
kVfT -V(g ◦ fS)kD = Ex〜D [(I- TI(X)T2)kVfT(X)k2],
4
Under review as a conference paper at ICLR 2021
where g(x) = Ax + C onst. Moreover, though not necessarily, if assuming that fT is L-Lipschitz
continuous, i.e., kVfτ(χ)∣∣2 ≤ L for ∀χ ∈ SUPP(D), we have a more elegant statement:
∣∣Vfτ -V(g ◦ fS )kD ≤ (I- τιτ2)L2 .
The theorem suggests that, if adversarial transferability is high, there exists an affine transformation
with bounded norm, such that g ◦ fS is close to fT . As an intuition of the proof, the difference
between two gradients can be represented by the angle between them, which can be characterized by
τ1 ; and the norm difference between them, which can be characterized by τ2 .
As for the general case, we consider when the output dimensions of both functions are multi-
dimensional and not necessarily the same. In this scenario, adversarial attacks correspond to the
largest singular value of the Jacobian matrix. Therefore, we need to introduce the following definition
to capture other information that is not revealed by adversarial attacks.
Definition 4 (Singular Value Ratio). For any function f, the Singular Value Ratio for the function
gradient at X is defined as λf (x) = σ2((χ), where σ1(x),σ2(x) are the largest and the second largest
singular value in absolute value of Vf(x), resPectively. In addition, we define the worst-case singular
value ratio as λf = maxx∈suPP(D) λf (x).
Theorem 2.	For two functions fS : Rn → Rm, and fT : Rn → Rd, assuming that fT is L-LiPschitz
continuous, i.e., kVfT (x)k2 ≤ L for ∀x ∈ suPP(D), there is an affine function g : Rm → Rd, such that
∣VfT - V(g ◦ fS)∣2D ≤ (1 - τ1τ2) + (1 - τ1)(1 - τ2)λf2T + (λfT + λfS)2 5L2,
where g is defined as g(z) = Az + C onst.
We note that this theorem also has a statement offering tighter bound where we do not assume
Lipschitz continuous. The full version of this theorem is provided in appendix. Theorem 2 suggests
that big τ1 and τ2 indicate potentially small differences of gradients between the target model and the
transferred model. Based on this, intuitively, given the right constant value shift, minimal difference in
gradients implies minimal difference in function value, which should result in bounded loss. Indeed,
we prove in Theorem 3 that the squared loss of the transferred model g ◦ fS is bounded by the loss of
fT and their gradient difference, by assuming the β-smoothness of both the functions.
Definition 5 (β-smoothness). A function f is β-smooth if for all x, y,
∣Vf (x) - Vf (y)∣2 ≤ β∣x - y∣2.
For the target data distribution D, and its ground truth labeling function y, the mean squared loss of
the transferred model is Ex〜D∣∣g◦ fs(x) 一 y(χ)k2 = Ilg ◦ fs - y∣∣D. Therefore, the following theorem
presents upper bound on the mean squared loss of the transferred model.
Theorem 3.	Without loss of generality we assume ∣x∣2 ≤ 1 for ∀x ∈ suPP(D). Consider functions
fs : Rn → Rm, fT : Rn → Rd, and an affine function g : Rm → Rd, suggested by Theorem 1 or
Theorem 2, with the constant set to let g(fs (0)) = fT (0). If both fT, fs are β-smooth, then
kg ◦ fs 一 y∣D ≤ (kfτ 一 ykD + kvfτ -Vg ◦ fsIId + (1 + ∣∣HkD,2) β).
∣VfS ∣D,2
3.3 Practical Measurement of Adversarial Transferability
Existing studies have shown that similar models share high adversarial transferability (Liu et al., 2016;
Papernot et al., 2016; Tramer et al., 2017). In previous work, it is common to use cross adversarial
loss as an indication of adversarial transferability, e.g., the loss of fT with attacks generated on fS.
It is intuitive to consider that the higher cross adversarial loss, the higher adversarial transferability.
However, it may have a drawback comparing to the τ1 , τ2 defined in this work.
Definition 6 (Cross Adversarial Loss). Given a loss function't (∙,y) on the target domain, where y
is ground truth, the adversarial loss of fT with attack δfS generated against source model fS is
Ladv(fτ, δfs;y,D)	=	Ex〜D 'τ(fτ(x + δfs(x)),y(x)).
5
Under review as a conference paper at ICLR 2021
The cross adversarial loss depends on the choice of loss function, the output dimension, etc. Thus,
it can be incomparable when we want to test adversarial transferability among different fT , unlike
that τ1, τ2 are always between [0, 1]. To investigate the relationship between the adversarial loss
and the adversarial transferability we defined, we show in the following proposition that the cross
adversarial loss is similar to τ1. In the next section, we verify the theoretical predictions through
thorough experiments.
Proposition 2. If't is mean squared loss and fτ achieves zero loss on D, then the adversarial loss
defined in Definition 6 is approximately upper and lower bounded by
Ladv(fτ, δfs； y, D) ≥e2Eχ〜D [τι(x) ∣∣Vfτ(x)∣∣2] + O(e3),
Ladv(fτ, δfs; y, D) ≤ e2Eχ〜D [③了 + (1- f )τι(x)) ∣∣Vfτ(x)k2] + O(e3),
where O(e3) denotes a cubic error term.
4	Experimental Evaluation
The empirical evaluation of the relationship between adversarial transferability and knowledge
transferability is done by four different sets of experiment. First we present a set of synthetic
experiment that verifies our theoretical study, and then we present our empirical study on real-
world datasets with models widely used in practice, described in three knowledge transfer scenarios:
knowledge transfer on data distributions, attributes, and tasks. Details regarding the three scenarios
are elaborated below, and all training details are deferred to the Appendix.
Knowledge-transfer among data distributions is the most common setting of transfer learning. It
transfers the knowledge of a model trained/gained from one data domain to the other data domains.
For instance, Shie et al. (2015) manage to use pre-trained ImageNet representations to achieve
state-of-the-art accuracy for medical data analysis. The relation between adversarial and knowledge
transferability can not only determine the best pretrained models to use, but also detect distribution
shifts, which is crucial in learning agents deployed in continual setting (Diethe et al., 2019).
Knowledge-transfer among attributes is a popular method to handle zero-shot and few-shot learn-
ing (Jayaraman & Grauman, 2014; Romera-Paredes & Torr, 2015). It transfers the knowledge learned
from the attributes of the source problem to a new target problem Russakovsky & Fei-Fei (2010).
The relation between adversarial and knowledge transferability can be used as a probe to deployed
classification models to verify attributes that their decisions are based on. This will have profound
implications on fairness and interpretability.
Knowledge-transfer among tasks is widely applied across various vision tasks, such as super
resolution (Johnson et al., 2016), style transfer (Gatys et al., 2016), semantic and instance segmenta-
tion (Girshick, 2015; He et al., 2017; Long et al., 2015a). It involves transferring the knowledge the
model gains by learning to do one task to another novel task. The relation between adversarial and
knowledge transferability, as many recent works (Achille et al., 2019; Standley et al., 2019; Zamir
et al., 2018), can be used to charting the affinity map between tasks, aiming to guide potential transfer.
4.1	Synthetic Experiment on Radial Basis Functions Regression
In the synthetic experiment, we compute quantities that are otherwise inefficient to compute to verify
our theoretical results. We also try different settings to see how other factors affect the results. Details
follow.
Models. Both the source model fS and the target model fT are one-hidden-layer neural networks
with sigmoid activation.
Overall Steps. First, sample D = {(xi, yi)}iN=1 from a distribution (details later), where x is
n-dimensional, y is d-dimensional, and there are N samples. Then we train a target model fT on
D. Denoting the weights of fT as W, we randomly sample a direction V where each entry of V is
sampled from U (-0.5, 0.5), and choose a scale t ∈ [0, 1]. To derive the source model, we perturb
the target model as W0 := W + tV . Define the source model fS to be a one-hidden-layer neural
network with weights W0. Then, we compute each of the quantities we care about, including τ1, τ2,
cross adversarial loss (Definition 6), the upper bound in theorem 2 on the difference of gradients, etc.
6
Under review as a conference paper at ICLR 2021
Noting that we reported the cross adversarial loss normalized by its own adversarial loss, defined as
α =IM ,δfskD/k∆fτ ,δfT kD ≈ Ladv(fτ, δfs; y, D)ILadv(fτ, δfτ; y, D)When fτ achieves low
error. Note that α ∈ [0, 1]. Finally, we fine-tune the last layer of fS, and get the true transferred loss.
Dataset. Denote a radial basis function as φi(x) = e-kx-μik2/(2σi)2, and we set the target ground
truth function to be the sum of M = 100 basis functions as f = PiM=1 φi, where each entry of
the parameters are sampled once from U (-0.5, 0.5). We set the dimension of x to be 30, and the
dimension of y to be 10. We generate N = 1200 samples of x from a Gaussian mixture formed by
three Gaussian with different centers but the same covariance matrix Σ = I . The centers are sampled
randomly from U (-0.5, 0.5)n. We use the ground truth regressor f to derive the corresponding y
for each x. That is, we want our neural networks to approximate f on the Gaussian mixture.
Results. We present two sets of experiment in Figure 2. The correlations between adversarial
transferabilities (τ1 , τ2, α) and the knowledge transferability (transferred loss) are observed. The
upper bound for the difference of gradients (Theorem 2) basically tracks its true value. Although the
absolute value of the upper bound on the transferred loss (Theorem 3) can be big compared to the
true transferred loss, their trends are similar. We note the big difference in absolute value is due to the
use of β-smoothness, which considers the worst case scenario. It is also observed that τ1 tracks the
normalized adversarial cross loss α, as Proposition 2 suggests.
(a) hidden-layer-width = 50
(b) hidden-layer-width = 100
Figure 2: The x-axis is the t ∈ [0, 1] that controls how much the source model deviates from the
target model. There are in total 7 quantities reported, placed under 4 y-axes. Specifically, τ1 , τ2, and
the normalized cross adversarial loss α are plotted as green curves with green y-axis; the upper bound
in theorem 2 on the transferred gradients difference is shown as blue curves with blue y-axis; the true
transferred gradients difference is shown as red curves with red y-axis; the upper bound in theorem 3
on the transferred loss is shown as magenta curves with magenta y-axis; the true transferred loss is
shown as black curves with black y-axis.
4.2	Adversarial Transferability Indicates Knowledge-transfer among Data
Distributions
Figure 3: (left) correlation between the adversarial transferability and knowledge trransferability in
image domain (All values normalized to [0,1]). (right) adversarial transferability and knowledge
transferability in NLP domain.
Data	Yelp	AG	Fake
τ1	0.49	0.47	0.50
τ2	1.34e-3	8e-4	3e-6
adv loss	0716^^	0.13	0.12
knowledge-trans	0.89^^	0.66	0.52
7
Under review as a conference paper at ICLR 2021
In this experiment, we show that the closer the source data distribution is to the target data distribution,
the more adversarially transferable the source model to the reference model, thus we observe that
the source model is more knowledge transferable to the target dataset. We demonstrate this on both
image and natural language domains.
Dataset. Image: 5 source datasets (5 source models) are constructed based on CIFAR-10 (Hinton
et al., 2012) and a single target dataset (1 reference model) based on STL-10 (Coates et al., 2011).
Each of the source datasets consists of 4 classes from CIFAR10 and the target dataset also consists of
4 classes from STL10. Natural Language: We select 4 diverse natural language datasets: AG’s News
(AG), Fake News Detection (Fake), IMDB, Yelp Polarity (Yelp). Then we pick IMDB as the target
and the rest as sources.
Adversarial Transferability. Image: We take 1000 images (STL-10) from the target dataset and
generate 1000 adversarial examples on each of the five source models. We run 10 step PGD L∞ attack
with = 0.1. Then we measure the effectiveness of the adversarial examples by the cross-entropy loss
on the reference model. Natural Language: We take 100 sample sentences from target dataset(IMDB)
and generate adversarial sentences on each of the source models(AG, Fake, Yelp) with TextFooler(Jin
et al., 2019). The ratio of changed words is constrained to less or equal to 0.1. Then, we measure
their adversarial transferability against the reference model(IMDB).
Knowledge Transferability. To measure the knowledge transferability, we fine-tune a new linear
layer on the target dataset to replace the last layer of each source model to generate the corresponding
transferred models. Then we measure the performance of the transferred models on the target dataset
based on the standard accuracy and cross-entropy loss.
Results From Figure 4.2, it’s clear that if the source models that has highest adversarial transferability,
its corresponding transferred model achieves the highest transferred accuracy. This phenomenon is
prominent in both image and natural language domains. The results in Figure 4.2 (b) could verify the
implication by our theory that only τ1 is not sufficient for indicating knowledge transferability.
Data	Young	Male	Attractive	Eyebrows	Lipstick
τ1	0.0707	0.0679	0.0612	0.0609	0.0678
τ2	0.0759	0.0521	0.0418	0.0529	0.0388
adv loss	17.83	16.21	14.13	13.22	12.54
knowledge-trans	0.593	0.589	0.562	0.551	0.554
Table 1: Top 5 Attributes with the highest adversarial transferability and their corresponding average
accuracy on the validation benchmarks.
4.3	Adversarial Transferability Indicating Knowledge-transfer among
Attributes
In addition to the data distributions, we validate our theory on another dimension, attributes. This
experiment suggests that the more adversarially transferable the source model of certain attributes
is to the reference model, the better the model performs on the target task aiming to learn target
attributes.
Dataset CelebA (Liu et al., 2018) consists of 202,599 face images from 10,177 identities. A reference
facial recognition model is trained on this identities. Each image also comes with 40 binary attributes,
on which we train 40 source models. Our goal is to test whether source models of source attributes,
can transfer to perform facial recognition.
Adversarial Transferability We sample 1000 images from CelebA and perform a virtual adversarial
attack as described in section 3 on each of the 40 attribute classifiers. Then we measure the adversarial
transfer effectiveness of these adversarial examples on the reference facial recognition model.
Knowledge Transferability To fairly assess the knowledge transferability, we test the 40 transferred
models on 7 well-known facial recognition benchmarks, LFW (Huang et al., 2007), CFP-FF, CFP-FP
(S. Sengupta, 2016), AgeDB (Moschoglou et al., 2017), CALFW, CPLFW (Zheng et al., 2017) and
VGG2-FP (Cao et al., 2018). We report the average classification accuracy target datasets.
Result In Table 1, we list the top-5 attribute source models that share the highest adversarial
transferability and the performance of their transferred models on the 7 target facial recognition
benchmarks. We observe that the attribute "Young" has the highest adversarial transferability; as a
result, it also achieves highest classification average performance across the 7 benchmarks.
8
Under review as a conference paper at ICLR 2021
4.4	Adversarial Transferability Indicating Knowledge-transfer among Tasks
O. Autoencoder
1.	Curvature
2.	Edge-2D
3.	Key point-2 D
4.	Segmentation-2D
5.	Edge-3D
6.	Keypoint-3D
7.	ReShade
8.	Depth
9.	EUClidean-DiStanCe
10.	Surface-Normal
11.	Segmentation-BD
12-	Segmentation-Semantic
13.	Object-Classification
14.	See n e-CI ass i f i cat io n
Figure 4: Left: Emprically confirmed taskonomy prediction of task categories (Zamir et al., 2018).
Right: Task category prediction based on adversarial transferability. Different colors represent
different task categories including 2D, 3D, Semantic. Itis obvious that the adversarial transferability is
able to predict similar task categories aligned with the pure knowledge-transfer empirical observation.
In this experiment, we show that adversarial transferability can also indicate the knolwdge trans-
ferability among different machine learning tasks. Zamir et al. (2018) shows that models trained
on different tasks can transfer to other tasks well, especially when the tasks belong to the same
“category". Here we leverage the same dataset, and pick 15 single image tasks from the task pool,
including Autoencoding, 2D Segmentation, 3D Keypoint and etc. Intuitively, these tasks can be
categorized into 3 categories, semantic task, 2D tasks as well as 3D tasks. Leveraging the tasks within
the same category, which would hypothetically have higher adversarial transferability, we evaluate
the corresponding knowledge transferability.
Dataset The Taskonomy-data consists of 4 million images of indoor scenes from about 600 indoor
images, every one of which has annotations for every task listed in the pool. We use a public subset
of these images to validate our theory.
Adversarial Transferability Adversarial Transferability Matrix (ATM) is used here to measure the
adversarial transferability between multiple tasks, modified from the Affinity Matrix (Zamir et al.,
2018). To generate the corresponding “task categories" for comparison, we sample 1000 images
from the public dataset and perform a virtual adversarial attack on each of the 15 source models.
Adversarial perturbation with (L∞ norm) as 0.03,0.06 are used and we run 10 steps PGD-based
attack for efficiency. Detailed settings about adversarial transferability are deferred to the Appendix.
Knowledge Transferability We use the affinity scores provided as a 15 × 15 affinity matrix to
compute the categories of tasks. Then we take columns of this matrix as features for each task and
perform agglomerative clustering to obtain the Task Similarity Tree.
Results Figure 4 compares the predictions of task categories generated based on adversarial trans-
ferability and knowledge transferability in Taskonomy. It is easy to see three intuitive categories
are formed, i.e, 2D, 3D, and Semantic tasks for both adversarial and knowledge transferability. To
provide a quantitative measurement of the similarity, we also compute the average inner category
entropy based on adversarial transferability with the categories in Taskonomy as the ground truth (the
lower entropy indicates higher correlation between adversarial and knowledge transferability). In
figure 5 (Appendix), the adversarial transferability based category prediction shows low entropy when
the number of categories is greater or equal to 3, which indicates that the adversarial tranferability
is faithful with the category prediction in Taskonomy. This result shows strong positive correlation
between the adversarial transferability and knowledge transferability among learning tasks in terms
of predicting the similar task categories.
5	Conclusion
We theoretically analyze the relationship between adversarial transferability and knowledge transfer-
ability, along with thorough experimental justifications in diverse scenarios. Both our theoretical and
empirical results show that adversarial transferability can indicate knowledge transferability, which
reveal important properties of machine learning models. We hope our discovery can inspire and
facilitate further investigations, including model interpretability, fairness, robust and efficient transfer
learning, and etc.
9
Under review as a conference paper at ICLR 2021
References
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In
Proceedings ofthe IEEE International Conference on Computer Vision, pp. 6430-6439, 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. In International Conference on Machine
Learning, pp. 274-283, 2018.
Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zisserman. Vggface2: A dataset for recognising faces
across pose and age. In International Conference on Automatic Face and Gesture Recognition,
2018.
Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised
feature learning. In Proceedings of the fourteenth international conference on artificial intelligence
and statistics, pp. 215-223, 2011.
Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Biggio, Alina Oprea,
Cristina Nita-Rotaru, and Fabio Roli. Why do adversarial attacks transfer? explaining transferability
of evasion and poisoning attacks. In 28th {USENIX} Security Symposium ({USENIX} Security
19), pp. 321-338, 2019.
Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin
loss for deep face recognition. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 4690-4699, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Tom Diethe, Tom Borchert, Eno Thereska, Borja Balle, and Neil Lawrence. Continual learning in
practice. arXiv preprint arXiv:1903.05202, 2019.
Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial
examples by translation-invariant attacks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 4312-4321, 2019.
Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional
neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 2414-2423, 2016.
Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision,
pp. 1440-1448, 2015.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings ofthe
IEEE international conference on computer vision, pp. 2961-2969, 2017.
Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint
arXiv:1207.0580, 2012.
Gary B. Huang, Manu Ramesh, Tamara Berg, and Erik Learned-Miller. Labeled faces in the wild: A
database for studying face recognition in unconstrained environments. Technical Report 07-49,
University of Massachusetts, Amherst, October 2007.
Minyoung Huh, Pulkit Agrawal, and Alexei A Efros. What makes imagenet good for transfer
learning? arXiv preprint arXiv:1608.08614, 2016.
10
Under review as a conference paper at ICLR 2021
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In International Conference on Machine Learning, pp. 2137-2146,
2018.
Dinesh Jayaraman and Kristen Grauman. Zero-shot recognition with unreliable attributes. In
Advances in neural information processing systems, pp. 3464-3472, 2014.
Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language
attack on text classification and entailment. arXiv preprint arXiv:1907.11932, 2, 2019.
Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and
super-resolution. In European conference on computer vision, pp. 694-711. Springer, 2016.
Seong Joon Oh, Mario Fritz, and Bernt Schiele. Adversarial image perturbation for privacy protection-
a game theory perspective. In Proceedings of the IEEE International Conference on Computer
Vision, pp. 1482-1491, 2017.
Jiman Kim and Chanjong Park. End-to-end ego lane estimation based on sequential transfer learning
for self-driving cars. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops, pp. 30-38, 2017.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object
detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988,
2017.
Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples
and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Large-scale celebfaces attributes (celeba)
dataset. Retrieved August, 15:2018, 2018.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 3431-3440, 2015a.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International Conference on Machine Learning, pp. 97-105, 2015b.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Dawn Song, Michael E Houle, and James Bailey. Characterizing adversarial subspaces using local
intrinsic dimensionality. arXiv preprint arXiv:1801.02613, 2018.
Ana I Maqueda, Antonio Loquercio, Guillermo Gallego, Narciso GarCia, and Davide Scaramuzza.
Event-based vision meets deep learning on steering prediction for self-driving cars. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5419-5427, 2018.
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a
regularization method for supervised and semi-supervised learning. IEEE transactions on pattern
analysis and machine intelligence, 41(8):1979-1993, 2018.
Stylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and
Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition Workshop, volume 2, pp. 5,
2017.
Muhammad Muzammal Naseer, Salman H Khan, Muhammad Haris Khan, Fahad Shahbaz Khan, and
Fatih Porikli. Cross-domain transferability of adversarial perturbations. In Advances in Neural
Information Processing Systems, pp. 12885-12895, 2019.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks using adversarial samples. arXiv preprint arXiv:1605.07277,
2016.
11
Under review as a conference paper at ICLR 2021
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on
Asia conference on computer and communications security, pp. 506-519, 2017.
Bernardino Romera-Paredes and Philip Torr. An embarrassingly simple approach to zero-shot
learning. In International Conference on Machine Learning, pp. 2152-2161, 2015.
Olga Russakovsky and Li Fei-Fei. Attribute learning in large-scale datasets. In European Conference
on Computer Vision, pp. 1-14. Springer, 2010.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition
challenge. International journal of computer vision, 115(3):211-252, 2015.
C.D. Castillo V.M. Patel R. Chellappa D.W. Jacobs S. Sengupta, J.C. Cheng. Frontal to profile face
verification in the wild. In IEEE Conference on Applications of Computer Vision, February 2016.
Chuen-Kai Shie, Chung-Hisang Chuang, Chun-Nan Chou, Meng-Hsi Wu, and Edward Y Chang.
Transfer representation learning for medical image analysis. In 2015 37th annual international
conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp. 711-714. IEEE,
2015.
Yosuke Shinya, Edgar Simo-Serra, and Taiji Suzuki. Understanding the effects of pre-training for
object detectors via eigenspectrum. In Proceedings of the IEEE International Conference on
Computer Vision Workshops, pp. 0-0, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Trevor Standley, Amir R Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and Silvio Savarese.
Which tasks should be learned together in multi-task learning? arXiv preprint arXiv:1905.07553,
2019.
Florian Tramer, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. The space of
transferable adversarial examples. arXiv preprint arXiv:1704.03453, 2017.
Annegreet Van Opbroek, M Arfan Ikram, Meike W Vernooij, and Marleen De Bruijne. Transfer
learning improves supervised image segmentation across imaging protocols. IEEE transactions on
medical imaging, 34(5):1018-1030, 2014.
Weiyue Wang, Naiyan Wang, Xiaomin Wu, Suya You, and Ulrich Neumann. Self-paced cross-
modality transfer learning for efficient road segmentation. In 2017 IEEE International Conference
on Robotics and Automation (ICRA), pp. 1394-1401. IEEE, 2017.
Zirui Wang, Zihang Dai, Barnabas P6czos, and Jaime Carbonell. Characterizing and avoiding negative
transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
11293-11302, 2019.
Michael Wurm, Thomas Stark, Xiao Xiang Zhu, Matthias Weigand, and Hannes Taubenbock.
Semantic segmentation of slums in satellite images using transfer learning on fully convolutional
neural networks. ISPRS journal of photogrammetry and remote sensing, 150:59-69, 2019.
Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L Yuille.
Improving transferability of adversarial examples with input diversity. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 2730-2739, 2019.
Ruijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive fea-
ture norm approach for unsupervised domain adaptation. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 1426-1435, 2019.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in neural information processing systems, pp. 3320-3328, 2014.
12
Under review as a conference paper at ICLR 2021
Amir R Zamir, Alexander Sax, William Shen, Leonidas J Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 3712-3722, 2018.
Tianyue Zheng, Weihong Deng, and Jiani Hu. Cross-age LFW: A database for studying cross-age
face recognition in unconstrained environments. CoRR, abs/1708.08197, 2017. URL http:
//arxiv.org/abs/1708.08197.
Wen Zhou, Xin Hou, Yongjun Chen, Mengyun Tang, Xiangqi Huang, Xiang Gan, and Yong Yang.
Transferable adversarial perturbations. In Proceedings of the European Conference on Computer
Vision (ECCV), pp. 452-467, 2018.
13
Under review as a conference paper at ICLR 2021
A	Discussion about Validnes s of the Notations
Before starting proving our theory, it is necessary to show that our mathematical tools are indeed
valid. It is easy to verify that(♦,)D is a valid inner product inherited form standard Euclidean inner
product. Therefore, the norm ∣∣T∣d, induced by the inner product, is also a valid norm.
What does not come directly is the validness of the norm ∣∣ ∙ ∣∣d,2. Particularly, whether it satisfies
the triangle inequality. Recall that, for a function of matrix output F : supp(D) → Rd×m, its
L2 (D)-norm in accordance with matrix 2-norm is defined as
kF l∣D,2 = q/Ex-D kF (x)k2.
For two functions F, G, both are SUPP(D) → Rd×m, We can verify the norm k ∙ ∣∣d,2 satisfies triangle
inequality as shown in the following. Applying the triangle inequality of the spectral norm, and with
some algebra manipulation, it holds that
kF + GkD,2 = //Ex 〜D kF (x) + G(x)k2
≤ /Ex〜D (kF(x)k2 + kG(x)k2)2
=//Ex 〜D kF (x)k2 + Ex 〜D kG(x)k2 + 2Eχ 〜D kF (x)k2kG(x)k2
=/kF kD,2 + kGkD,2 + 2Ex 〜D kF (x)k2kG(x)k2.	(2)
Applying the Cauchy-Schwarz inequality, we can see that
EX 〜D kF (x)k2kG(x)k2 ≤ /Ex-D kF (x)k2 ∙ Ex-D kG(x)k2
=kFkD,2 ∙ kGkD,2 .
Plugging this into (2) would complete the proof, i.e.,
(2) ≤ /kFkD,2 + kGkD,2 +2kFkD,2 ∙kGkD,2
=/(kF kD,2 + kGkD,2)2
= kF kD,2 + kGkD,2.
B Proof of Proposition 1
Proposition 1 (Restated). Both τ1 and τ2 are in [0, 1].
Proof. We are to prove that τ1 and τ2 are both in the range of [0, 1]. As τ1 is squared cosine, it is
trivial that τ1 ∈ [0, 1]. Therefore, we will focus on τ2 in the following.
Recall that the τ2 from f1 to f2 is defined as
f1→f2
τ2
h2∆f2,δf1 - A∆f1,δf1 , A∆f1,δf1 iD
k∆f2,δf1 kD
where A is a constant matrix defined as
A = proj(Ex-D[∆f2,δf1 (x)∆fι,δfι (x)>] (EX-D[∆fι,δfι (x)∆fι,δfι (x)>])t, k；f2M UD).
k∆f1,δf1 kD
For notation convenience, we will simply use τ2 to denote τ2f1→f2 in this proof.
τ2 characterizes how similar are the changes in both the function values of f1 : Rn → Rm and
f2 : Rn → Rd in the sense of linear transformable, given attack generated on f1. That is being said,
it is associated to the function below, i.e,
h(B) =	∆f2,δf1	-B∆f1,δf12D=Ex-D∆f2,δf1(x)-B∆f1,δf1(x)22,
14
Under review as a conference paper at ICLR 2021
where∆f1,δf ∈ Rm, ∆f2,δf ∈ Rd, andB ∈ Rd×m.
As ∆f2,δf (x) - B∆f1,δf (x)22 is convex with respect to B, its expectation, i.e. h(B), is also
convex.
Therefore, h(B) it achieves global minima when 貂 =0.
∂B = Ex 〜D ∂B (QfzM (X) - bδ∕i ,δfι (x)ll2)
=2Ex" [(B∆fι,δfι (X)- ∆f2,δf1 (x)) ∆f1,δf1 (x)>]
=2Ex〜D [B∆fι ,δfι (X)∆f1,δf1 (x)> - ∆f2,δf1 (X)∆f1,δf1 (x)>]
=2BEx~D [δ∕i ,δfι (X)∆f1,δf1 (X),] - 2Ex~D a/2,3%(X)∆f1,δf1 (X)T].
Letting 籍=0, and denoting the solution as B*,we have
B* = Ex〜D [∆f2,δfι (X)∆fι,δfι (X)>] (Ex〜D [∆fι,δfι (X)∆fι,δfι (X)>])t.
Noting that A = Proj(B*, ；：；：：； ；D ) is scaled B*, we denote A = ψB*, where ψ a scaling factor
depending on B* and ||；；：力 ||；. According to the definition of the projection operator, we can see
that 0 < ψ ≤ 1.	1
Replacing B by A we have,
h(A) = ∆f2,δf1 - A∆f1,δf1 2D = h∆f2,δf1 - A∆f1,δf1 ,∆f2,δf1 - A∆f1,δf1 iD
= ∆f2,δf1 2D - h2∆f2,δf1 - A∆f1,δf1,A∆f1,δf1iD
= (1 - τ2) ∆f2,δf1 2D .
It is obvious that h(A) = ∆f2,δf - A∆f1,δf 2D ≥ 0, thus we have τ2 ≤ 1.
As for the lower bound for τ2, we will need to use properties of B. Denoting O as an all-zero matrix,
it holds that
h(B*) = min{h(B)} ≤ h(O).	(3)
B
For A = ψB*, according to the convexity of h(∙) and the fact that ψ ∈ [0,1], we can see the
following, i.e.,
h(A) = h(ψB*) = h(ψB* + (1 - ψ)O) ≤ ψh(B*) + (1 - ψ)h(O).
Applying (3) to the above, we can see that
h(A) ≤ h(O).
Noting that h(A) = (1 - τ2) ∆f2,δf 2D and h(O) = ∆f2,δf 2D, the above inequality suggests
that
(1 - τ2 ) ∆f2 ,δf1 D ≤ ∆f2 ,δf1 D ,
0 ≤ τ2.
Therefore, τ2 is upper bounded by 1 and lower bounded by 0.	□
C Proof of Theorem 1
Before actually proving the theorem, let us have a look at what τ1 and τ2 are in the case where fS
and fT are both Rn → R. In this case, both τ1 and τ2 come out in an elegant form. Let us show what
the two metrics are to have further intuition on what τ1 and τ2 characterize.
15
Under review as a conference paper at ICLR 2021
First, let us see what the attack is in this case. As function f has one-dimensional output, its gradient
is a vector Vf ∈ Rn. Thus,
δf,(x)
arg max kVf (x)> δ k2
kδk≤
eVf(x)
kVf(x)∣∣2
Then, the τ1 becomes
τ1 (x)
hVfs(x), Vfτ(x)i2
IlVfS(x)∣∣2 ∙∣∣Vfτ(x)∣∣2
which is the squared cosine (angle) between two gradients.
For τ2, the matrix A degenerates to a scalar constant
A =。方 f , δ∕s,δfsiD
=-k∆fs f ID-,
and the second metric becomes
fs→fτ _	<δ∕s f，δ∕t f〉D
τ2	= k∆fs ,δfs ID ∙∣∆fτ ,δfs ID
We can see, it is interestingly in the same form of the first metric τ1. We will simply use τ2 to denote
τ2fS→fT afterwards.
Theorem 1	(Restated). For two functions fS and fT that both are Rn → R, there is an affine
function g : R → R, so that
IVfT - V(g ◦ fs)kD = Ex〜D [(1 - τ1(x)τ2)kVfτ(x)k2],
where g is defined as g(x) = Ax + C onst.
Moreover, if assuming that fT is L-Lipschitz continuous, i.e., IVfT (x)I2 ≤ L for ∀x ∈ supp(D),
we can have a more elegant statement:
IVfT - V(g ◦ fS)I2D ≤ (1 - τ1τ2)L2 .
Proof. In the case where g is a one-dimensional affine function, we write is as g(z) = Az + b, where
A is defined in the definition of τ2 (Definition 3). In this case, it enjoys a simple form of
A = QfT f，Zs f〉D
=-∣∆fs f ID-.
Then, we can see that
IVfT - V(g ◦ fS)I2D = IVfT - AVfS I2D
=Ex〜D [IVfτ(x) - AVfs(x)I2] .	(4)
To continue, we split VfT as two terms, i.e., one on the direction on VfS and one orthogonal to
Vfs.
Denoting φ(x) as the angle between VfT (x) and Vfs (x) in Euclidean space, we have
VfT (x) = cos(φ(x))
cos(φ(x))
IVfT(X)I2
IVfs (x)I2
IVfT(X)I2
IVfs (x)I2
Vfs(X)+ VfT(X)-cos(φ(X)) fl Vfs(X)
Vfs(x) + v(x),
(5)
where We denote v(x) = VfT(x) 一 cos(φ(x))将fT(X)k2 VfS(x) for notation convenience.
We can see that v(x) is orthogonal to VfS(x), thus ∣∣v(x)I2 =，1 一 cos2(Φ(x))∣∣V∕T(x)I2.
Recall that actually τι(x) = cos2(φ(x)), it can be written as ∣∣v(x)I2 =，1 - τι(x)IVfT(x)I2.
16
Under review as a conference paper at ICLR 2021
Then, plugging (5) into (4) we have
(4) = Eχ~D
=Ex-D
=Ex-D
=Ex-D
=Ex-D
=Ex~D
k cos(φ(x))
cos(φ(x))
cos(φ(x))
cos(φ(x))
cos(φ(x))
INfτ(x)k2
kVfs (x)k2
NfS (x) + v(x) — AVfS (x)k2
kVfτ(x)k2
kVfs (x)k2
kVfτ(x)k2
kVfs (x)k2
kVfτ(x)k2
kVfs (x)k2
kVfτ(x)k2
kVfs (x)k2
-	A VfS (x) + v(x)
2
-	A Vfs(x)	+ kv(x)k22
2
-	A Vfs (x)2 + (1 - τ1(x))kVfT (x)k22
2
-	A Vfs(x)2 + Ex~D(1 - τ1(x))kVfT (x)k22
cos(φ(x)) kVfT (X)k2 — A) 2 kVfs (x)k[ + Ex~d (1-τι(x))kVfτ (x)k2.
(6)
Now let us deal with the first term by plugging in
Of ,δfS , δ∕s ,δfSiD
-k∆Sfι1-
where ∆fT,δfS (x) = cos(φ(x))kVfT (x)k2 and ∆fS,δfS (x) = kVfs(x)k2, and we have
Eχ~D (cos(Φ(x)) kVfT(X)k2 — A)2 kVfs(x)k2
= Ex~D (cos(φ(x))kVfT (x)k2 —AkVfs(x)k2)2
=WEχ~D (δλ,,δfS (X) — A f ,δfS (X))
=WEχ~D (∆fτ,δfs (X)2 + A2∆fs,δfs (X)2 — 2A∆fτ,δfs (X)∆fs,δfs (X))
=N (gfT ,δfs IID+A ∣∣δ∕s ,δfs ∣∣d—2AhAT ,δfs, `f ,δfs iD)
=ɪ A ∆	I∣2 I hZT,δfs , δ∕s,δfs iD _ 2 hZT,δfs , δ∕s,δfs iD ʌ
=M V fT,δfS 11D + -k∆fs,δfs kD	k∆fs,δfs kD-)
=IIδ∕t,δ⅛llD /1 _	3T,δfs，δ∕s,δfsiD	!
=-N- ∖ -∣∣Δ∕s,δfs kD ∙k∆fτ,δfs kD )
= (1 — τ2)Ex~D cos2(X)kVfT(X)k22
= (1 — τ2)Ex~D τ1(X)kVfT(X)k22 .	(7)
Plugging (7) into (6), we finally have
kVfT — V(g ◦ fs)k2D
=(1 — τ2)Eχ~D [τι(x)kVfT(x)k2] + Eχ~D(1 — τι(x))kVfτ(x)k2
=Eχ~D [(1 — τ2τ1(x))kVfT(x)k2]
≤ (1 — τ1τ2 )L2 ,
which completes the proof.	口
17
Under review as a conference paper at ICLR 2021
IlVfT - V(g ◦ fS)IlD ≤ 5Ex〜D
D	Proof of Theorem 2
Theorem 2	(Restated). For two functions fS : Rn → Rm, and fT : Rn → Rd, there is an affine
function g : Rm → Rd, so that
(((1 - τ1(x)τ2) + (1 - τι(x))(1 - τ2)λfτ(x)2) kVfτ(x)k2
∖ + (λfτ (X) + λfs (X))2 ^VSk2^2 kvfT l∣D,2
where g is defined as g(z) = Az + C onst.
Moreover, if assuming that fτ is L-Lipschitz continuous, i.e., ∣Vfτ (x)k2 ≤ L for ∀x 〜supp (D),
and considering the worst-case singular value ratio λ, we can have a more elegant statement:
lVfT - V(g ◦ fS)l2D ≤ (1 - τ1τ2) + (1 - τ1)(1 - τ2)λf2T + (λfT + λfS)2 5L2.
Proof. Recall that the matrix A is defined in Definition 3, i.e.,
A = PrOj(Ex〜D[∆fτ,δfs (x)∆fs,δfs (X)>] (Ex〜D [∆fs,δfs (x)∆fs隈(x)>]) t
and we can see
k∆fτ,δfs kD
BfS,δfS kD
IVfT - V(g ◦ fS)I2D,2 = IVfT> - V(g ◦ fS)> I2D,2 = IVfT> - AVfS> I2D,2
=Ex〜DkVfτ(x)> - AVfS(x)>k2
=Ex〜D max kVfτ(x)>t - AVfS(x)>tk2,	(8)
ktk2=1
where the last equality is due tO the definitiOn Of matrix sPectral nOrm.
DenOting Vf> as either the JacObian matrix VfT> Or VfS>, Singular Value DecOmPOsitiOn (SVD)
suggests that Vf (X)> = UΣV >, where Σ is a diagOnal matrix cOntaining all singular values
ordered by their absolute values. Let σι, ∙∙∙ ,σn denote ordered singular values. Nothing that the
number Of singular values that are nOn-zerO may be less than n, sO we fill the emPty with zerOs, such
that each of them have corresponding singular vectors, i.e., the column vectors vι,…,Vn in V.
That is being said, ∀i ∈ [n], we have
kVf (x)>Vik2 = ∣σi∣.
Let θi and vi denote the singular values and vectors for VfS (X)> . Noting that {vi}in=1 define a
orthonormal basis for Rn , we can represent
n
t =	θivi ,
i=1
(9)
where Pin=1 θi2 = 1.
As adversarial attack is about the largest eigenvalue of the gradient, plugging (9) into (8), we can
split it into two parts, i.e.,
(8) = Ex〜D max
ktk2=1
VfT (X)>	Xn θivi
- AVfS(X)>
Ex〜D max
ktk2=1
VfT (X)> (θ1v1) - AVfS (X)> (θ1v1)
+ VfT (X)>	Xθivi	- AVfS (X)>
(10)
Denoting u = Pin=2 θivi, we can see this vector is orthogonal to v1. Let us denote v10 as the singular
vector with the biggest absolute singular value of VfT(X)>, parallel with attack δfT. Now we split
18
Under review as a conference paper at ICLR 2021
u = u1 + u2 into two terms, where u1 is parallel to v10 , and u2 is orthogonal to u1. As u1 is in the
orthogonal space to v1 while parallel with v10 , it is bounded by the sine value of the angle between v1
and v0ι, i.e.,，1 - τι(x). Hence, noting that U is part of the unit vector t,
IIu1k2 ≤ √1 - τι(χ)IIuIl2 ≤ √1 - τι(χ).	(11)
Plugging u in (10), we have
(10) = Ex〜D max
ktk2=1
≤ Ex 〜D max
—	ktk2 = 1
NfT(x)> (θ1V1) - AVfs(x)> (θ1V1)
+ VfT(X)>(uι + u2) - AVfS(x)>u 2
/ IlVfT(X)> (θ1v1) - AVfs(x)> (Θ1v1)∣∣2	∖
、----------------{z----------------}
Xi
+ ∣∣Vfτ (x)>u1∣∣2 + ∣∣Vfτ (x)>u2 — AVfs (x)>u∣∣2
'-------V------}	'------------V---------}
X2	X3
(12)
where the inequality is due to triangle inequality.
There are three terms we have to deal with, i.e., X1, X2 and X3. Regarding the first term, v1 in X1
aligns with the attack δfS (X), which we have known through adversarial attack. The second term X2
is trivially bounded by (11). Although adversarial attacks tell us nothing about X3, it can be bounded
by the second largest singular values.
Let us first deal with two easiest, i.e., X2 and X3. Applying (11) on X2 directly, we have
X2 = IVfT(x)>k2 ∙ku1k2 ≤ Pl- τι(X)IVfT(x)>k2.
For X3, noting that u2 is orthogonal to v10 , and u is orthogonal to v1, we can see that u2 has no
components of the largest absolute singular vector of VfT(X)>, and u has no components of the
largest absolute singular vector of VfT (X)> . Therefore,
X3 ≤ ∣∣VfT(X)>u2 ∣∣2 + ∣∣AVfs(X)>u∣∣2
≤ σfT,2 (X) Iu2 I2 + σfS,2(X) IAI2 IuI2
=λfT(X)∣∣VfT(X)>∣∣2Iu2I2+λfS(X)∣∣Vfs(X)>∣∣2IAI2IuI2
≤λfT(X)∣∣VfT(X)>∣∣2+λfS(X)∣∣Vfs(X)>∣∣2IAI2,
where the first inequality is due to triangle inequality, the second inequity is done by the attributes
of singular values, and the definition of matrix 2-norm. The equality is done simply by applying
the definition of singular values ratio (Definition 4), and the third inequality is due to the fact that
Iu2I2 ≤ IuI2 ≤ 1.
Before dealing with X1, let us simplify (12) by relax the square of summed terms to sum of squared
terms, as the following.
(12) = Ex 〜D
=Ex 〜D
≤ Ex 〜D
≤ Ex 〜D
=Ex 〜D
max (X1 + X2 + X3)2
tk2=1	1	2	3
max X12 +X22 +X32 +2X1X2 +2X2X3+2X1X3
max X12 + X22 + X32 + 2 max{X12, X22} + 2 max{X22, X32} + 2 max{X12, X32}
tk2=1
max X12 + X22 + X32 + 2(X12 + X22) + 2(X22 + X32) + 2(X12 + X32 )
tk2=1
mt ax 5(X12 + X22 + X32).	(13)
We note that this relaxation is not necessary, but simply for the simplicity of the final results without
breaking what our theory suggests.
19
Under review as a conference paper at ICLR 2021
Bring what we we have about X2 and X3, and noting that θ1 ≤ 1 depends on t, we can drop the max
operation by
(13)	= Ex〜D max 5(X2 + X2 + X2)
ktk2=1	1	2	3
=Ex〜D max 5(∣IVfT(X)>(θ1v1) - AVfS(x)> (θιvι)∣∣2 + X + X2)
ktk2=1	2	2	3
≤	( IIVfT(X)>vι - AVfS(x)>vι∣∣2 + (1-τι(x))kVfτ(x)k2∖
-x~D [	+ ((λfτ(x)+ λfs (x))∣∣Vfs (x)>∣∣2 kAk2)2.	)
Now, let us deal with the first term. As v1 is a unit vector and is in fact the direction of fS (X)’s
adversarial attack, we can write δfS,(X) = v1. Hence,
Ex〜D ∣∣Vfτ(x)>vι - AVfS(x)>vι∣∣2
12
=Ex〜DR ∣ VfT(X)>δfs,e(X)- AVfS(X)Tδfs,e(X)∣∣2
=Ex〜D∣∣∆fτ,δfs (X)- A∆fs,δfs (X)∣∣2,	(15)
where the last equality is derived by applying the definition of ∆(X), i.e., equation (1). Note that we
omit the in δfS, for notation simplicity.
The matrix A is deigned to minimize (15), as shown in the proof of Proposition 1. Expanding the
term we have
(15)=最Ex〜D ∣∣∆fτ,δfs (X)∣∣2 + ∣∣A∆fs,δfs (X)∣∣2 - 2h∆fτ,δfs (X), A∆fs,δfs (X)i
^2 (卜方,δfs ∣∣D + H Aδ∕s,δfs ∣∣D - 2OfT,δfs , A△%,δfsiD
hfT,δfs Hd (I )
e2	(I	τ2)
(1 - T2)Ex〜D l∣VfT(X)>vι∣∣2 .
(16)
Recall that v1 is a unit vector aligns the direction of δfS, and we have used v10 to denote a unit vector
that aligns the direction of δfT . As τ1 tells us about the angle between the two, let us split v1 into
to orthogonal vectors, i.e., vι = ,ti(x)v1 +，1 - τι (x)vj ⊥, where vj ⊥ is a unit vector that is
orthogonal to v10 .
Plugging this into (16) we have
(16) = (1 — τ2)Ex〜D ∣∣VfT(x)>(Pti(x)v1 +，1 - ti(x)vj,⊥)∣^
=(1 - T2)Ex〜D IlVfT(x)>Pti(x)v1∣12 + l∣VfT(x)>P1 — Tl(x)vj,⊥
=(1 - T2)Ex 〜D [τι(X) ∣∣VfT (x)>∣∣2 + (1 - τ1(X))λfτ(X)2 ∣∣Vfr(X)>∣∣2],
where the second equality is due to the image of vj0 and vj0 ,⊥ after linear transformation VfT (X)>
are orthogonal, which can be easily observed through SVD.
20
Under review as a conference paper at ICLR 2021
Plugging this in (14), and with some regular algebra manipulation, finally we have
^1 - T2)[τι(x)∣∣Vfτ(x)>∣∣2 + (1 -τι(x))λfτ(x)2 ∣∣Vfτ(x)>∣∣2P
(14)	= 5Ex〜D	+(1 一 τι(x)) ∣∣Vfτ(x)k2
∖	+ (λfτ(x) + λfs(x))2 ∣∣Vfs(x)>∣∣2 IIAlI2	)
f (1 - τ1(x)τ2)∣∣Vfτ(x)>∣∣2	、
= 5Ex-D	+(1 — τι(x))(1 - T2)λfτ(x)2 ∣∣Vfτ(x)>∣∣2	.	(17)
∖+(λfτ(x)+ λfs (x))2∣∣Vfs (x)>∣∣2 ∣A∣2 )
Recall that A is from a norm-restricted matrix space, i.e., the A is scaled so that its spectral norm is
k∆fT,δfS kD
no greater than k∆fs,δfs∣∣D, thus
kAk2< ∣δ∕t,δfsID /δ∕t,δfτkD
k k2 ― k∆fs,δfs ID ≤ k∆fs,δfs ID
_ Ex〜D∣δ∕t,δfτ(x)k2 = Ex〜D∣Vf>(x)∣2
=Ex 〜D ∣∆fs ,δfS (x)∣2 = Ex 〜D ∣Vf>(x)∣2
=kVf>kD，2	(18)
∣Vf>kD,2.	(18)
Hence, plugging the above inequality to (17), the first statement of the theorem is proven, i.e.,
(17) ≤ 5Ex-D
t (1 - τ1(x)τ2)∣∣Vfτ(χ)>∣∣2	、
+(1 - τι(x))(1 - T2)λfτ∣Vfτ(x)>∣2
∖+(λfT(X) + NS(X))21∣VfS(X)>∣∣2 f∣2)
(19)
To see the second statement of the theorem, we assume fT is L-Lipschitz continuous, i.e.,
IVfT (X)I2 ≤ L for ∀X ∈ supp(D), and considering the worst-case singular value ratio
λ = maxx∈supp(D) for either fS, fT, we can continue as
(Ex〜D [(1 - τ1(χ)τ2) ∣∣Vfτ(x)>∣∣2]	\
(19)	≤ 5	+Ex-D h(1 - τ1(x))(1 - T2)λfτ ∣∣ VfT(X)> ∣∣2i
∖+Ex 〜D (f+λfs )2∣∣VfS (x)>∣∣2 ||Vf:||D,"
(Ex〜D [(1 - T1(x)τ2) ∣∣VfT(X)>∣g	\
=5	+Ex〜D [(1 - T1(X))(1 - T2)λfτ ∣∣Vfτ(X)>∣∣2i
+ (λfT + λfS )2 |VfT> |2D,2
=5Ex-D(1-τ1(X)τ2)+(1-τ1(X))(1-τ2)λf2T+(λfT+λfS)2 ∣∣VfT(X)>∣∣22
≤ Ex-D (1 - τ1(X)τ2) + (1 - τ1 (X))(1 - τ2)λf2T + (λfT + λfS )	5L2
= (1 - τ1τ2) + (1 - τ1 )(1 - τ2 )λ2fT + (λfT + λfS )2 5L2 ,
where the first inequality is due to the definition of worst-case singular value ratio, the last inequality
is by Lipschitz condition, and the last equality is done be simply applying the definition of τ1.
□
21
Under review as a conference paper at ICLR 2021
E Proof of Theorem 3
The idea for proving Theorem 3 is straight-forward: bounded gradients difference implies bounded
function difference, and then bounded function difference implies bounded loss difference.
To begin with, let us prove the following lemma.
Lemma 1. Without loss of generality we assume kxk2 ≤ 1 for ∀x ∈ supp(D). Consider functions
fS : Rn → Rm, fT : Rn → Rd, and an affine function g : Rm → Rd, suggested by Theorem 1 or
Theorem 2, such that g (fS (0)) = fT (0), if both fT, fS are β-smooth in {x | kxk ≤ 1}, we have
IlfT - g ◦ fSIID ≤ ∣∣Vfτ - V(g ◦ fS)kD,2 + (1+ [：fT[D，2 ) β
k	kvfS l∣D,2∕
Proof. Let Us denote v(x) = fτ(x) — g ◦ fs(x), and We can show the smoothness of v(∙).
As g(∙) is an affine function satisfying g(fs(0)) = fτ(0), it can be denoted as g(z) = A(Z —
fS(0)) + fT (0), where A is a matrix sUggested by Theorem 1 or Theorem 2. Therefore, denoting
B1 = {x | IxI ≤ 1} as a unit ball, for ∀x, y ∈ B1 it holds that
IVv(x) - Vv(y)I2 = Vv(x)> - Vv(y)>2
= VfT(x)>-VfT(y)>-A(VfS(x)> - VfS(y)>)2
≤ VfT (x)> - VfT (y)>2 + A(VfS (x)> -VfS(y)>)2
≤ VfT (x)> - VfT (y)>2 + IAI2 VfS (x)> - VfS (y)>2, (20)
where the last second inequality is due to triangle inequality, and the last inequality is by the property
of spectral norm.
Applying the β-smoothness of fS and fT , and noting that IAI2 ≤ fD,2 asshownin(18),we
can continue as
(20)	≤ β ∣x - y∣2 + ∣A∣2 β ∣x - y∣2 ≤ β ∣x - y∣b + 默||；2β ∣x - y∣2
1+
kvfT∣D,2 ʌ
kVfS l∣D,2 )
βIx-yI2,
which suggests that v(∙) is(1 + 的阳*： ) β-smooth.
We are ready to prove the lemma now. Applying the mean value theorem, for ∀x ∈ B1 , we have
v(x) - v(0) = vv(ξx)>x,
where ξ ∈ (0, 1) is a scalar number. Subtracting vv(x)>x on both sides give
v(x) - v(0) - vv(x)>x = (vv(ξx) - vv(x))>x
v(x) - v(0) - vv(x)>x2 = (vv(ξx) - vv(x))>x2
v(x)-v(0)-vv(x)>x2 ≤ I(vv(ξx) - vv(x))I2 IxI2.
Let US denote βι =(1+ 的钿；2) β for notation convenience, and apply the definition of smooth-
ness:	,
Iv(x) - v(0) - vv(x)>xI2 ≤ β1(1 - ξ)IxI22 ≤ β1.	(21)
Noting that v(0) = 0 and applying the triangle inequality, we have
Iv(x) - v(0) - vv(x)>xI2 ≥ Iv(x)I2 - Ivv(x)>xI2 ≥ Iv(x)I2 - Ivv(x)>I2
Plugging it into (21), we have
Iv(x)I2 ≤ β1 + Ivv(x)> I2
Iv(x)I22 ≤ β12 + Ivv(x)>I22 + 2β1Ivv(x)>I2
Ex〜D∣v(x)k2 ≤ β2 + Ex〜D∣Vv(x)>k2 + 2βιEχ〜DkVv(X)>∣2
Ex〜Dkv(x)k2 ≤ β2 + Ex〜D∣∣Vv(x)k2 + 2βιEx〜D∣∣Vv(x)∣2
IIvkD ≤ β2 + l∣VvkD,2 + 2βιEx〜D∣∣Vv(x)∣2
22
Under review as a conference paper at ICLR 2021
Applying Jensen’s inequality to the last term, we get
kvkD ≤ β2 + kVvkD,2 + 2βιjEχ 〜D ∣Vv(χ)k2
=β2 + kVvkD,2 + 2βιJ∣Nv∣∣D,2 = β2 + ∣~v∣∣D,2 + 2β1∣∣s∣∣D,2
= (IIVv∣∣D,2 + βI)2
Plugging βι =(1+ kk3fTkkD,2) β and V = fτ 一 g ◦ fs into the above inequality completes the
proof.	□
With the above lemma, it is easy to show the mean squared loss on the transferred model is also
bounded.
Theorem 3 (Restated). Without loss of generality we assume IxI2 ≤ 1 for ∀x ∈ supp(D). Consider
functions fS : Rn → Rm, fT : Rn → Rd, and an affine function g : Rm → Rd, suggested by
Theorem 1 or Theorem 2, such that g (fS (0)) = fT (0). If both fT, fS are β-smooth, then
kg ◦ fs 一 ykD ≤ (kfT - y∣D + kVfτ - Vg ◦ fskD,2 + (1+ k^fTkD,2) β)
∖	k	kvfS l∣D,2∕ )
Proof. Let us denote β1 =(1+ |禺；|二'2) β, and according to Lemma 1 We can see
kfT 一 g ◦ fskD ≤ kVfT 一 V(g ◦ fs)kD,2 + β1	(22)
Applying a standard algebra manipulation to the left hand side, and then applying triangle inequality,
We have
kfT 一 g ◦ fs kD = kfT 一 y + y 一 g ◦ fs kD ≥ ky 一 g ◦ fs kD 一 kfT 一 y kD .
Plugging this directly into (22), it holds that
ky 一 g ◦ fskD 一 kfT 一 ykD ≤ kVfT 一 V(g ◦ fs)kD,2 + β1
ky 一 g ◦ fskD ≤ kfT 一 ykD + kVfT 一 V(g ◦ fs)kD,2 + β1
Replacing βι by(1 + |昌；|；,2) β and taking the square, we can see Theorem 3 is proven. □
F	Proof of Proposition 2
Proposition 2 (Restated). If't is mean Squared loss and fτ achieves zero loss on D, then the
adversarial loss defined in Definition 6 is approximately upper and lower bounded by
Ladv(fτ, δfs; y, D) ≥e2Eχ〜D [τι(x) ∣Vfτ(x)∣2] + O(e3),
Ladv(fτ, δfs; y, D) ≤ e2Eχ〜D [⑶，+ (1 - f )τ1 (x)) ∣∣Vfτ(x)k2] +。(9，
where O(3) denotes a cubic error term.
Proof. Recall that the empirical adversarial transferability is defined as a loss
Ladv(fτ, δfsQ y, D)	=	Ex〜D 'τ(fτ(x + δfs,e(x)), y(x)).
As't is mean squared loss, and fτ achieves zero loss, i.e., fτ = y, we have
Ladv(fτ, δfsQ y,D) = Ex〜D kfτ(x + δfs,e(x)) - y(x)∣2
=Ex〜D kfτ(x + δfs,e(x)) - fτ(x)k2 .
Denoting δfS,(x) = δfS,1(x), and define an auxiliary function h as
h(t) = fT(x + tδfS,1(x)) - fT (x),
23
Under review as a conference paper at ICLR 2021
we can see that kfT (x + δfS,(x)) - fT (x)k22 = kh()k22.
We can then apply Taylor expansion to approximate h() with a second order error term O(2), i.e.,
∂h
h(e) = dt∖t=0 + O(e2) = ZfT (x)>δfs ,1 + O(e2).
Therefore, assuming that kVfτ(x)k2 is bounded for X ∈ SUPP(D), We have
kfτ(x + δfs,e(x)) - fτ(x)k2 = kh(e)k2 = e2 ∣∣Vfτ(x)>δfs,ι(x)∣∣2 + O(e3),	(23)
Where We have omit higher order error term, i.e., O(4).
Next, let us deal With the term ∣∣VfT (x)> δfS,1 (x)∣∣22. Same us the technique We use in the Proof
of Theorem 2, We sPlit δfS,1(x) = v1 + v2, Where v1 aligns the direction of δfT,1(x), and v2 is
orthogonal to v1. Noting that τ1(x) is the squared cosine of the angle betWeen δfS,1(x) and δfT,1(x),
We can see that
kv1k22 = τ1(x) kδfS,1(x)k22 = τ1(x),
kv2k22 = (1 - τ1(x)) kδfS,1(x)k22 = (1 -τ1(x)).
Therefore, We can continue as
∣∣VfT(x)>δfS,1(x)∣∣22 = ∣∣VfT (x)>(v1 + v2)∣∣22
= ∣∣VfT(x)>v1∣∣22+∣∣VfT(x)>v2∣∣22
= τ1(x) kVfT (x)k22 + ∣∣VfT (x)>v2∣∣22,	(24)
Where the second equality is because that v1 is corresPonding to the largest singular value of
VfT(x)>, and v2 is orthogonal to v1.
Next, We derive the loWer bound and uPPer bound for (24). The loWer bounded can be derived as
τ1(x)kVfT(x)k22+∣∣VfT(x)>v2∣∣22 ≥ τ1(x) kVfT (x)k22,
and the uPPer bounded can be derived as
τ1(x)kVfT(x)k22+∣∣VfT(x)>v2∣∣22 ≤ τ1(x) kVfT (x)k22 + λfT (x)2 kVfT (x)k22 kv2k22
= τ1(x) kVfT (x)k22 + λfT (x)2 kVfT (x)k22 (1 -τ1(x))
≤ τ1(x) kVfT (x)k22 + λ2fT kVfT (x)k22 (1 - τ1(x))
=(λfτ + (1- λfτ)τι(x)) kVfτ(x)k2,
Where λfT (x) is the singular value ratio of fT at x, and λfT is the maximal singular value of fT.
APPlying the loWer and uPPer bound to (23), We finally have
kfT (x + δfS,(x)) - fT (x)k22 ≥2τ1(x)kVfT(x)k22+O(3),
kfτ(x + δfs,e(x)) - fτ(x)k2 ≤ e2 (λfτ + (1- λfτ)τι(x)) ∣∣Vfτ(x)k2 + O(e3).	(25)
Noting that
Ladv(fτ, δfsQ y,D) = Ex〜D kfτ(x + δfs,e(x)) - fτ(x)k2,
we can see that taking expectation to (25) completes the proof.	□
G Experiment Details
All experiments are conducted on 4 RTX 2080 Ti GPUs and in python3 Ubuntu 16.04 environment.
24
Under review as a conference paper at ICLR 2021
G. 1 Attack Methods
PGD Attack is generated iteratively: denote step size as ξ, the source model as fS, and the loss
function on the source problem. 's(∙, ∙). We initialize xo to be uniformly sampled from the eball
B (x) of radius centered as instance x, and then generate the adversarial instance iteratively: at
step t we compute xt+i = Xt + ξ ∙ sign(Vχt's(fs(χt),fs(χ)))∙ Denoting the adversarial example at
instance x using PGD on source model fS as P GDfS (x), we measure the adversarial loss from fS
to fτ based on the loss't(∙,y) of fτ on target data D given attacks generated on fs, i.e.,
Lτ(fτ ◦ PGDfS ； y, D)	=	Ex 〜D 'τ(fτ(PGDfs(x)),y(x))∙
TextFooler iteratively replaces words in target sentences by looking up similar words in the dictionary.
It pauses when the predicted label is changed or runs out of the attack budget. We modify it such that
it pauses when the percentage of changed words reaches 10%.
G.2 Adversarial Transferability Indicates Knowledge-transfer among Data
Distributions
Details of Dataset construction For the image domain, we divide the classes of the original
datasets into two categories, animals (bird, cat, deer, dog) and transportation vehicles (airplane,
automobile, ship, truck). Each of the source datasets consists of different a percentage of animals and
transportation vehicles, while the target dataset contains only transportation vehicles, which is meant
to control the closeness of the two data distributions.
Details of Model Training Image: we train five source models on the five source datasets from 0%
animals to 100% animals, and one reference models on STL-10 with identical architectures and
hyperparameters. We use SGD optimizer and standard cross-entropy loss with learning rate 0.1,
momentum 0.9, and weight decay 10-4. Each model is trained for 300 epochs.
Natural Language: we fine-tune a Bert on each of the datasets with Adam and learning rate 0.0003 for
100 epochs. For transferred models, we run Adam with a smaller learning rate 0.0001 for 3 epochs.
G.3 Adversarial Transferability Indicating Knowledge-transfer among
Attributes
Details of Model Training We train 40 binary source classifiers on each of the 40 attributes of
CelebA with ResNet18 (He et al., 2016). All the classifiers are trained with optimizer Adadelta
with a learning rate of 1.0 for 14 epochs. We also train a facial recognition model as a reference
model on CelebA with 10,177 identities using ResNet18 as the controlled experiment.The reference
facial recognition model is optimized with SGD and initial learning rate 0.1 on the ArcFace (Deng
et al., 2019) with focal loss (Lin et al., 2017) for 125 epochs. For each source model, we construct
a transferred model by stripping off the last layers and attaching a facial recognition head without
parameters. Then we use the 40 transferred models to evaluate the knowledge transferability on 7
facial recognition benchmarks.
G.4 Adversarial Transferability Indicating Knowledge-transfer among Tasks
Details of Model Training We use 15 pretrained models released in the task bank (Zamir et al.,
2018) as the source models. Each source model consists of two parts, an encoder, and a decoder. The
encoder is a modified ResNet50 without pooling, homogeneous across all tasks, whereas the decoder
is customized to suit the output of each task. When measuring the adversarial transferability, we
will use each source model as a reference model and compute the transferability matrix as described
below.
Adversarial Transferability Matrix (ATM) is used here to measure the adversarial transferability
between multiple tasks, modified from the Affinity Matrix in (Zamir et al., 2018). In the experiment
of determining similarity among tasks, it is hard to compare directly and fairly, since each task is
of different loss functions, which is usually in a very different scale with each other. To solve this
problem, we take the same ordinal normalization approach as Zamir et al. (2018). Suppose we have
N tasks in the pool, a tournament matrix MT for each task T is constructed, where the element of
the matrix mi,j represents what percentages of adversarial examples generated from the ith task
transfers better to task T than the ones of the jth task (untargeted attack success rate is used here).
25
Under review as a conference paper at ICLR 2021
Then we take the principal eigenvectors of the N tournament matrices and stack them together to
build the N × N adversarial transferability matrix. To generate the corresponding “task categories"
for comparison, we sample 1000 images from the public dataset and perform a virtual adversarial
attack on each of the 15 source models. Adversarial perturbation with (L∞ norm) as 0.03,0.06
are used and we run 10 steps PGD-based attack for efficiency. Then we measure these adversarial
examples’ effectiveness on each of the 15 tasks by the corresponding loss functions. After we obtain
the 15×15 ATM, we take columns of this matrix as features for each task and perform agglomerative
clustering to obtain the Task Similarity Tree.
0.1	∖
0.0	'
2	4	6	8	10	12	14
number of categories
Figure 5: We also quantitatively compare our prediction with the Taskonomy (Zamir et al., 2018)
prediction when different number of categories is enforced. We find our prediction is similar with
theirs with n ≥ 3.
26