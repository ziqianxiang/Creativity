Under review as a conference paper at ICLR 2021
Analyzing Attention Mechanisms through
Lens of Sample Complexity and Loss Land-
SCAPE
Anonymous authors
Paper under double-blind review
Ab stract
Attention mechanisms have advanced state-of-the-art deep learning models for
many machine learning tasks. Despite significant empirical gains, there is a lack of
theoretical analyses on their effectiveness. In this paper, we address this problem
by studying the sample complexity and loss landscape of attention-based neural
networks. Our results show that, under mild assumptions, every local minimum of
the attention model has low prediction error, and attention models require lower
sample complexity than models without attention. Besides revealing why popular
self-attention works, our theoretical results also provide guidelines for designing
future attention models. Experiments on various datasets validate our theoretical
findings.
1	Introduction
Significant research in machine learning has focused on designing network architectures for superior
performance, faster convergence and better generalization. Attention mechanisms are one such design
choice that is widely used in many natural language processing and computer vision tasks. Inspired
by human cognition, attention mechanisms advocate focusing on relevant regions of input data to
solve a desired task rather than ingesting the entire input.
Several variants of attention mechanisms have been proposed, and they have advanced the state of the
art in machine translation (Bahdanau et al., 2014; Luong et al., 2015; Vaswani et al., 2017), image
captioning (Xu et al., 2015), video captioning (Pu et al., 2018), visual question answering (Zhou
et al., 2015; Lu et al., 2016), generative modeling (Zhang et al., 2018), etc. In computer vision,
spatial/spatiotemporal attention masks are employed to focus only on relevant regions of images/video
frames for underlying downstream tasks (Mnih et al., 2014). In natural language tasks, where input-
output pairs are sequential data, attention mechanisms focus on the most relevant elements in the
input sequence to predict each symbol of the output sequence. Hidden state representations of a
recurrent neural network are typically used to compute these attention masks.
Substantial empirical evidence on the effectiveness of attention mechanisms motivates us to study
the problem from a theoretical lens. To this end, it is important to understand the loss landscape and
optimization of neural networks with attention. Analyzing the loss landscape of neural networks is an
active ongoing research area, and it can be challenging even for two-layer neural networks (Poggio
& Liao, 2017; Rister & Rubin, 2017; Soudry & Hoffer, 2018; Zhou & Feng, 2017; Mei et al.,
2018b; Soltanolkotabi et al., 2017; Ge et al., 2017; Nguyen & Hein, 2017a; Arora et al., 2018).
Convergence of gradient descent for two-layer neural networks has been studied in Allen-Zhu et al.
(2019); Mei et al. (2018b); Du et al. (2019). Ge et al. (2017) shows that there is no bad local minima
for two-layer neural nets under a specific loss landscape design. These works reveal the importance
of understanding loss landscape of neural networks.
Unfortunately, these results cannot be directly applied for attention mechanisms. In attention models,
the network structure is different, and the attention introduces additional parameters that are jointly
optimized. To the best of our knowledge, there is no existing work analyzing the loss landscape
and optimization of attention models. In this work, we present theoretical analysis of self-attention
models (Vaswani et al., 2017), which uses correlations among elements of input sequence to learn an
attention mask.
1
Under review as a conference paper at ICLR 2021
We summarize our work as follows. We carefully analyze attention mechanisms on the loss landscape
in Sections 3 and 4. In Section 3, we show that, under mild assumptions, every stationary point of
attention models achieves a low generalization error. Section 4 studies other properties of attention
models on the loss landscapes. After the loss landscape analyses, we discuss how our theoretical
results can guide the practitioners to design better attention models in Section 5. Then we validate
our theoretical findings with experiments on various datasets in Section 6. Section 7 includes a few
concluding remarks. Proofs and more technical details are presented in the appendix.
2	Attention models
Attention mechanisms are modules that help neural networks focus only on relevant regions of input
data to make predictions. To compare attention model with non-attention model, we first introduce a
two-layer non-attention model as the baseline model. The network architecture consists of a linear
layer followed by rectified linear units (ReLU) as a non-linear activation function, and a second
linear layer. Denote the weights of the first layer by w(1) ∈ Rp×d, the weights of the second layer by
w(2) ∈ Rd, and the ReLU function by φ(∙). Then the response function for the input X ∈ Rp can be
written as y = w(2)T φ(hw(1), xi). We call the above function “baseline model" since it does not
employ any attention.
To study such mechanisms, we mainly focus on analyzing the most popular self-attention model. In
this paper, we consider two types of self-attention model.
For the first type of self-attention model, we consider attention weights that are determined by a
function f (x): y = W⑵Tφ((w(1), X Θ f (x)i) where f (∙) is a known mapping function from Rp
to Rp, representing the attention weight of each feature with any given x. This model is a prototype
version of transformer model (Vaswani et al., 2017), with a pre-determined function as attention
weights.
Second, we introduce a more practical self-attention setup, which is the transformer model proposed
in Vaswani et al. (2017). To mimic the NLP task, we set the input Xi = (Xi1, . . . , Xip) ∈ Rt×p,
where Xij ∈ Rt , are t-dimensional vectors. Intuitively, each Xi corresponds to independent sentences
for i = 1, . . . , n, and Xij ’s are fixed dimensional vector embedding of each word in sentence Xi .
wQ,wK ∈ Rdq ×t are query and key weight matrices, and wV ∈ Rdv ×t is the value matrix. For
each input Xi, the key is calculated as: Ki = (wKXi)T ∈ Rp×dq; For zth vector in the input,
the query vector is computed as: Qiz = (wQXiz)T ∈ R1×dq for z = 1, . . . , p. The value matrix
V = wV Xi ∈ Rdv ×p. Then the self-attention w.r.t to the zth vector in the input Xi is computed as:
aself(Z)(Xz, WQ, WK) = Softmax( QiX )	⑴
dq
for z = 1, . . . , p. And aiself = (aiself(1), . . . , aiself(p)). This self-attention vector represents the
interaction between different words in each sentence. The value vector for each word in the sentence
self (z)
z	z	dv
Xi can be calculated as Vi = V ai ∈ R v . This value vector is then passed to a 2-layer MLP
parameterized by W(1) ∈ Rpdv ×d and W(2) ∈ Rd×1, resulting in the following general model:
yi = W(2)T φ(hW(1), vec(WV Xiaiself)i) + i
(2)
where vec(∙) represents the vectorization of a matrix, and Ei are i.i.d sub-Gaussian error.
3	Sample complexity analyses
In this section, we focus on analyzing the loss landscape for the the self-attention model as introduced
in Section 2. In Section 3.1, we consider the sample complexity of the model with known attention
weight function f (X). In Section 3.2, we consider transformer self-attention model, in which the
attention weight function is also need to be learnt. Section 3.3 discusses the sample complexity result
for multi-layer self-attention model.
To avoid the non-differentiable point of ReLU φ, we use the softplus activation function φτ0 (x),
i.e., φτo (x) = τ^ log(1 + eτ0x) Note φτ0 converges to ReLU as T → ∞ (Glorot et al., 2011). All
2
Under review as a conference paper at ICLR 2021
theoretical results we derived with φτ0 (x) holds for any arbitrarily large τ0. For ease of notation, we
still use φ(x) to denote the softplus function.
In this paper, we focus on the regression task which minimizes the following loss function:L =
E(χ,y)〜D∣∣h(x) - y∣∣2,where h(x) is the definedbaseline/attention models. Our theory Canbe easily
extended to classification tasks as well.
3.1	Asymptotic property of self-attention model with known weight function
We start with a self-attention model with known attention function f (∙) . The objective function is:
1n
min5- E(W⑵TΦ(hw(1), Xi Θ f (Xi)i) — yi)2	(3)
w 2n
i=1
where ∣f (X)∣2 = 1, as the sum one attention weights.
Before proceeding, we introduce three major assumptions for our analysis: (A1): Model output y
can be specified by a two-layer neural network with attention model structure; (A2): The Attention
weights are mainly contributed by top several masks; (A3): Hidden layers of the network are
sufficiently overparameterized such that they have sufficient expressiveness to predict y.
For space consideration, we provide the detailed justification of these assumptions and explain
why they can be intuitively summarized as (A1)-(A3) in the Appendix (A.1). Specifically, (A1) is
supported by previous works. For (A2) and (A3), we verify them both theoretically and empirically in
Appendix Section A.1 and in our experiments Section 6. In short, (A2) naturally holds since softmax
function in attention model helps us to obtain a concentrated attention weights instead of evenly
spread out weights; (A3) naturally holds for overparameterized neural networks. More details can be
found in Appendix Section A.1. In the following, we present their mathematical presentations.
Let φ(hw (1), X f (X)i) represents the d-dimensional random vector of hidden units with network
weightsw(1). For any vector z1 and z2, we denote var(z1|z2) as the covariance matrix of residual
z1 after taking linear regression on z2. The explicit form can be represented as var(z1 |z2) =
Σz1z1 - Σz2z1 Σz-11z1 Σz1 z2, where Σz1z2 represents the cross-covariance matrix of vector z1 and
z2. The explicit derivation can be found on page 176 of Izenman (2013). In our setting, z1 =
(hw(1)?, X f (X)i, and z2 =φ(hw (1), X f (X)i.
(A1) There exists a set of parameters (w(1)?, w(2)?) such that yi =w	(2)?T φ(hw(1)?, Xi f (X)i) +
i for i = 1, 2, ...n, where i’s follow the sub-Gaussian distribution subG(0, C32) and Xi ⊥ i.
(A2) For any Xi, we order the attention weights f(Xi) in descending order as f(Xi)(1), . . . , f(Xi)(p).
There exist positive integer 0 < s0 < p and 0 < τ < 1, such that largest s0 attention weights
fs0 = {f(Xi)(1), . . . ,f(Xi)s0} satisfies ∣fs0∣2 ≥ 1 -τ.
(A3) For γ > 0, λmaχ(Vɑr(φ(<w⑴？, Xi Θ f (x)))∣φ(<w⑴，X Θ f(x)i)) = o(γ2) as n &
Y log( k )(pd + d) and n → ∞ fora constant k which satisfies k < √s0 + T √p.
Other than these three major assumptions, we also assume the regularity assumption (B1): the model
samples Xi’s are i.i.d. with maximum bound; network weights w(1) and w(2) have bounded `2 norms,
denoted by C1 and C2, respectively; and the output y is centralized. The regularity assumption (B1)
is standard in the literature, which will be justified in the Appendix (A.2). Given these assumptions,
we show that the sample complexity bound of attention model:
Theorem 1. (Sample complexity for attention model) Under (A1)-(A3) and regularity assumption
(B1), for any γ > 0 and s0 s.t. k . √s0 + T√p, given the sample size
n & (√01√w log((√s0 + T√p)η)(pd + d)
γ2	γ
where η = C2C2. with probability converging to 1, any stationary point (W(1), W(2)) ofthe objective
function (3) satisfies that: E(W⑵Tφ(hW⑴，X Θ f (x)i) — E(y∣x))2 . Y2.
3
Under review as a conference paper at ICLR 2021
The proof sketch and complete proof are provided in Appendix Section E. To explicitly compare the
sample complexity bound of our attention model with non-attention model, we provide the following
corollary for sample complexity bound of baseline model. We just need to set s0 = 0 and τ = 1 in
Theorem 1, fixing all attention weights to be equal, then the model is the baseline one as follows.
Corollary 1. (Sample complexity for non-attention model) Under (AI)-(A3) and regularity as-
sumption (B1), for non-attention model with f (Xj) = 1∕√p for all features Xj ∈ X, given the sample
size:
p2η2	pη
n & T- log(-)(pd + d)
γ2 γ
where η = C2 C2. With probability converging to 1, any stationary point (w(1), W(2)) ofthe objective
function (3) satisfies that: E(W⑵Tφ(hW⑴，X Θ f (x)i) — E(y∣x))2 . Y2.
There is an important message from the comparison of Theorem 1 and Corollary 1. The corresponding
sample complexity bound from Corollary is in higher order than the sample complexity in Theorem 1.
This fact can be observed as follows. We can see the effect of concentration in attention models from
Theorem 1. In Theorem 1, the sample complexity bound is proportional to (√S0 + T√p)4. When
the attention is sufficiently concentrated, it means that we have a much lower order ofs0 p and
τ → 0, then our sample complexity will be significantly reduced.
Also from the sample complexity bound of Theorem 1, up to a log term, prediction error γ is
proportional to n-1/2, which is the optimal rate of convergence in regression. In Imaizumi &
Fukumizu (2018) and similar works, they showed the generalization error convergence rate is O(n-t)
where 0 < t < 1/2. These facts all imply that the bound is tight in aspect of sample size compared
with existing works.
3.2	Asymptotic property of self-attention model with unknown weight
FUNCTION
In this section, we extend our analyses to the transformer self-attention model introduced in Section
2.
Theorem 1 implies that if the self-attention mask can be precisely computed with f (X), we can derive
its sample complexity bound. However, the function f (∙) is not necessarily known, and needs to be
learned in real-world applications. Therefore, the transformer self-attention setup is more desired in
real-world settings. It provides a concrete model to learn the parameter of attention weight function.
Denoting W = (W(1), W(2), WQ, WK, WV ), the two-layer self-attention model can be formulated as:
1n
min— y^(w(2)Tφ(hw(1 * *), vec(wVXiaseef )i) - yi)2	(4)
w 2n
i=1
We now introduce necessary assumptions.
(A4) For each column aiself, we order the entries of aiself in descending order as ais(e1l)f, . . . , ais(epl)f.
There exist positive integer 0 < s0 < p and 0 < τ < 1, such that the largest s0 attention weights
ais(ellefading) = {ais(e1l)f,..., ais(esl0f)} satisfies kais(ellefading)k2 ≥ 1 -τ.
(A5) Denote Φ? = φ(hw⑴？，vec(wv?Xiaself?)〉)，and Φ = φ(hw(1), vec(wvXiaseef)), For any
γ > 0 We have λmaχ(Vαr(Φ?∣Φ)) = o(γ2). as n & k2d⅛d log( (k+√pt)) and n → ∞.
where similar to (A1), (W(1)?, W(2)?, WV?, aself?) correspond to the true parameter set such that
yi = W(2)?T φ(hW(1)?, vec(WV ?Xiaiself?)i) + i.	(5)
(A4) and (A5) are parallel to (A2) and (A3), assuming the attention weights are focused on s0
items, and a sufficient expressive power of Φ. Furthermore, sparse transformer Child et al. (2019),
sparsemax attention model Martins & Astudillo (2016) and local attention model Luong et al. (2015)
can be regraded as a special case of the assumption with τ = 0, in which we only consider attention
weights between partial locations instead of all locations. We further verify this assumption in our
4
Under review as a conference paper at ICLR 2021
experiments that s0 p and τ → 0. Other regularity assumptions on feature and parameter space are
similar with the ones in Theorem 1. We denote is as (B2): The `2 norm bound for w(1) , w(2),wK and
wQ are, respectively, C1, C2, C5, and C6, and the `1 column norm of wV is C8. They are presented
in Appendix (A.3). Now we provide sample complexity bound for transformer self-attention models:
Theorem 2. (Sample complexity for transformer self-attention model) Under (A4), (A5) and regu-
larity assumptions (B2), for any γ > 0 and s0 s.t k . s0, given the sample size:
n & (V⅞ + T√p)2 + √pt)2pdvdη I。乳((V⅞ + T√) + √pt)ηaηb)
〜	γ2	Y
where ηa = C12C2C8 and ηb = C5 + C6, with probability tending to 1,
any stationary point (w(1), W(2), WQ, WK) of the objective function (4) satisfies that:
E(W⑵Tφ(hw⑴,Vec(WVXia：eef))) — E(y∣x))2 . γ2.
There are two important messages from Theorem 2. First, theorem 2 shows that, with the help
of self-attention, we can achieve consistent predictions under a more expressive class of models
(equation 5), which is analyzed in Yun et al. (2019). Non-attention model does not have consistency
for this class of models. To train a non-attention model on data with self-attention structure, more
layers and larger network parameter size are required to reduce such bias. Second, we can see the
help of concentrating the attention in designing transformer model. Similar to Theorem 1, we see the
sample complexity bound is proportional to (√0 + T√p)4. Therefore, a properly small s0 and T can
significantly reduce the sample complexity of self-attention models. Later in our experiment, we show
that it is exactly what is happening in the real transformer model. Our theorem also answers why the
self-attention design with softmax function can effectively helps us achieve better prediction results.
What’s more, it also explains the effectiveness of sparse design in attention model. Sparse attention is
one special case of our concentration condition with T = 0. There has been work showing that sparse
attention weights can significantly reduce computational cost and improve the performance, and it
is verified in sparse transformer,local attention model and sparsemax attention models(Child et al.,
2019; Luong et al., 2015; Martins & Astudillo, 2016).
3.3	Multi-layer self-attention models and recurrent attention models
Theorems 1 and 2 can be extended to multi-layer neural nets. Due to page limit, the rigorous definition,
notations, assumptions and statements of Theorem 3 are deferred to Appendix Section A.4. Here we
provide a plain statement of it as follows.
Theorem 3. Given the overparameterized and regularity condition, for any given generalization error
level gamma, with high probability, a multi-layer self-attention model can achieve a generalization
error smaller than γ, given the sufficient large sample size.
Our analyses can be also extended to recurrent attention models, following the recurrent attention
setup in Luong et al. (2015). The analysis on recurrent attention model is deferred to Appendix
Section B due to page limits.
4	Non-linearity, flatnes s of minima and small sample size
In this section, we further investigate several additional properties on how attention mechanisms
improve the landscape of neural networks and keep the nice properties of baseline models in aspects
of reducing unnecessary non-linear regions, sharpness of local minimum and it doesn’t affect the loss
landscape in small sample case.
4.1 On the number of linear regions
We first study how attention mechanisms affect the number of linear regions (Montufar et al., 2014) in
a wide two-layer neural network with attention of known attention weight function, when the number
of hidden units is larger than the number of non-zero weights in f (x). This result shows how the
sparsity/concentration of attention weights effects the non-linearity of loss landscape.
Theorem 4. Assume kf (x)k0 = s0, which is the sparsity of the attention mask matrix, and the
number of units in the hidden layer n1 > s0. Then the maximal number of linear regions of the
function by a two-layer attention model with ReLU activation function, is lower bounded by [nn1 C 4 s0.
5
Under review as a conference paper at ICLR 2021
In general, we see the bound of attention model is smaller than the one of baseline model. The
corresponding plots on bounds can be found in appendix Section C.1. The result implies that, when
appropriate attention mechanism is used, the reduction of number of linear regions leads to a simpler
landscape, yet the approximation error remains small.
4.2 On flatness/sharpness of minima
Many recent works, such as Keskar et al. (2016), argue that flatter local minimum tends to generalize
well. However, in a recent study, Dinh et al. (2017) observes that by scale transformation, the minima
which are observationally equivalent, can be arbitrarily sharp, and the operator norm of a Hessian
matrix can also be arbitrarily large. We show that this fact also holds for the self-attention mechanism,
if no '2 norm bound on parameter (W⑴,W(2)) is imposed. Here We introduce the definition of
-flatness as in Hochreiter & Schmidhuber (1997).
Definition 1. Given > 0, a minimum θ, and loss L, C(L, θ, ) is the largest connected set
containing θ such that ∀θ0 ∈ C(L, θ, ), L(θ0) ≤ L(θ) + , and its volume is called the -flatness.
In the folloWing Theorem, We analyze the flatness of stationary point for self-attention model.
Theorem 5. Consider the two-layer ReLU neural network with self-attention mechanism as
stated in Section 3.2: yi = W(2)?T φ(hW(1)?, vec(WV xiaiself)i), and a minimum θ =
(W⑴,W⑵，wV, wQ, wK) satisfying that Wi = 0 for i = (1), (2),V, Q,K. For any E > 0,
C(L, θ, ) has an infinite volume, and for any M > 0, we can find a stationary point such that the
largest eigenvalue of V2L(θ) is larger than M.
Theorem 5 indicates that property on flatness of minima is maintained When attention mechanism is
applied. Furthermore, `2 norm bound helps remove sharp minima Which are bad in generalization. It
also coincides With our theoretical and empirical result that a flat minimum are expected to generalize
better in general (Keskar et al., 2016). We also dicuss the loss landscape of attention model under
small sample size. The results are deferred to Appendix Section C.3.
5	Guidance on improving the architecture of attention models
In this section, We provide insights into future attention model design through our analyses.
Regularization: Our analyses suggest proper regularization is helpful in training an attention model.
We can see that `2 norm bound C1 , C2 play an important role in sample complexity bound. It implies
that an `1 and `2 regularization on netWork Weights W(1) and W(2) are effective in reducing the
sample complexity. In Theorem 5, We also find that imposing constraints and regularization on
netWork Weights help remove sharp minima and keep flat minima With good generalization.
Concentration on attention: From the discussion of Theorem 1, Theorem 2 and Corollary 1, We
conclude that a proper concentration design With small s0 and τ can significantly reduce sample
complexities. Our analysis shoW that the soft-max design concentrate attention on limited number of
entries, Which help reduce the sample complexity. In different problems, We can further concentrate
the attention by adjusting the temperature of the softmax function. The smaller the temperature, more
concentrated the attention Weights are.
Overparameterization in query/key weights From our analyses, We can see that as dq, the dimen-
sion of key and query matrices, increases, the sample complexity Will not increase significantly. It
indicates that We can obtain high expressive poWer in attention model through overparameterization
in query and key matrices to increase expressiveness, Without hurting sample complexity.
6	Experiments
In this section, We validate our theoretical findings empirically. This section is divided into the
folloWing parts: (1) Verification of concentration assumption (A2) and (A4) on Portuguese to English
translation task. (2) Ablation study on the IMDB revieWs dataset, shoWing the effectiveness of
attention, regularization, and attention-concentration on self-attention and recurrent attention models.
(3) Experiments on a constructed noisy-MNIST dataset using self-attention models.
6
Under review as a conference paper at ICLR 2021
Experiment 1: Concentration of top attentions
To verify that our key assumptions/observations (A2) and (A4) hold in the real-world task, we
investigate the distribution of attention weights in the transformer model on the tasks of translating
Portuguese to English as proposed in Vaswani et al. (2017). Then we randomly select the trained
attention weights vector for 100000 different words in training samples. For each attention weight
vector with sentence length pi, We calculate the '2 norm of largest [√p∏ number of attention weights.
We find the top b√pie weights on average contribute to 90.9% of attention weights, very close to 1.
The histogram is plotted as follows Figure 1. We see that most of sum weights are close to 1. The
result indicates that the largest [√pi] attention weights contribute to most of the attention weights.
It indicates that our assumption (A2) and (A4) hold with reasonably small s0 p and τ , such as
so = b√pe.
Experiment 2: Ablation study on self-attention and recurrent
attention on the IMDB reviews dataset:
We consider the problem of sentiment classification on an IMDB
reviews dataset (Maas et al., 2011). The task is to classify the senti-
ment of a sentence as either a positive or negative one. We zero-pad
all our sentences to make their length equal to the sentence length
130. For every input word, we train their embeddings with random
initialization (of dimension 100) which is then passed to the neural
network. Hence, the dimension of input is 130 × 100. We con-
sider baseline 2-layer MLP, and then consider adding self-attention,
weight regularization and tempered softmax function into the model
to verify our theoretical analysis and corresponding guidance in
Section 5.
Figure 1: Distribution of `2
norm of top [√pi] of attention
weights
Baseline model: To train the baseline model, we first flatten the
input one large vector of dimension 130 × 100 and pass it to a 1-hidden layer MLP with h hidden
units. The model is trained using binary cross entropy loss.
Self-attention model: For self-attention model, the dimensions of query, key and value matrices are
wQ ∈ R100×100, wK ∈ R100×100, wV ∈ R100×130, respectively. We first compute the attention
mask aiself as per equation 1 (aiself ∈ R130×130). Using this attention mask, the attended feature
is then computed as fatt = wV xiaiself. The feature vector fatt ∈ R100×130. We then flatten this
attended feature and then pass it through a 1-hidden layer MLP with h hidden units.
Regularization: We impose a 10-4 `1 regularization on both w1 and w2.
Tempered Softmax: We calculate the attention weights as aiself(z) (xiz, wQ, wK)	=
Softmax( 5:QiKi ), We multiply the inner product by 5(or temperature as 1/5). Thus, the softmax
dq
operator pushes the small attention weights close to zero while retaining all large attention weights. In
this way, it achieves higher level of concentration of attention weights, comparing with the standard
softmax function based attention.
Optimization: All models were initialized randomly with Xavier initialization. Binary cross-entropy
loss was used to train the models. All models were trained using Adam optimizer with a learning rate
10-3.
To test the sample complexity, we vary the number of training samples in each experiment, train
all models and compute the performance on the test set. We varied the fraction of training samples
from 1k to 10k of the training data. Each experiment was repeated for 10 replications, and mean and
standard deviation was reported in Table 1.
Table 1:	Testing accuracy of self-attention ablation study on the IMDB reviews dataset.
Training sample size
Baseline
Baseline+regularization
Self Attention
Self Attention+regularization
Self Attention+tempered
Self Attention+regularization+tempered
1k
0.692 (0.016)
0.737 (0.009)
0.806 (0.007)
0.809 (0.011)
0.798 (0.010)
0.817 (0.010)
3k
0.809 (0.016)
0.823 (0.010)
0.821 (0.012)
0.838 (0.011)
0.830 (0.010)
0.843 (0.010)
5k
0.821 (0.015)
0.837 (0.005)
0.832(0.007)
0.853(0.002)
0.840(0.009)
0.856 (0.013)
10k
0.842 (0.006)
0.841 (0.005)
0.861 (0.004)
0.865 (0.006)
0.864 (0.011)
0.868 (0.007)
7
Under review as a conference paper at ICLR 2021
Recurrent attention experiments: Beyond self-attention, we also provide an ablation study on
recurrent attention model, verifying the superiority of recurrent attention model, as our analysis in
the Appendix B.
(1)	RNN baseline: The baseline here is a bi-directional RNN attention model with LSTM cell size
32, and we put the final output of RNN into a 2-hidden layer MLP with hidden units 64 and 20
separately to return the final prediction. (2) Recurrent attention: For recurrent attention, we design
the structure the same as Luong et al. (2015). (3) Regularization: We impose the `1 regularization
of 10-4 on network weights in the attention layer. Here we reported the test accuracy results in the
following Table 2.
Table 2:	Testing accuracy of recurrent attention ablation study on the IMDB reviews dataset.
Training sample size
RNN Baseline
RNN Baseline+regularization
Recurrent Attention
Recurrent Attention+regularization
1k
0.776(0.023)
0.777 (0.012)
0.789 (0.010)
0.792 (0.010)
3k	5k	10k
0.804 (0.009)	0.823 (0.006)	0.829	(0.015)
0.806 (0.010)	0.814 (0.008)	0.832	(0.007)
0.821 (0.007)	0.820 (0.009)	0.835	(0.008)
0.817 (0.009)	0.834 (0.009)	0.862	(0.002)
From the experiment results, we see significant effect of regularization and softmax temperature in
self-attention model, and we also see regularization also helps in the recurrent attention model. They
all coincide with our theoretical findings.
Experiment 3: Self-attention on Noisy-MNIST dataset
To prove the applicability of our analyses, we further verify the effectiveness of attention on image
classification task. We construct a noisy MNIST dataset based on the original MNIST dataset. For
each original 28 × 28 image, we separate a 56 × 56 image into 16 square grids, and put the whole
digit image randomly into 4 neighboring grids. Finally we generate all other grids with uniform
random variables. Examples of our generated dataset are provided in Figure 2.
We consider three models here. (1) CNN: We consider a standard 2-layer Convolu-
tional neural networks with fillter size 64, kernel size 3 * 3 and max pooling size 2 * 2,
following a hidden fully-connected layer with size 128. (2) Self-attention-CNN
We fit a 2-layer convolutional network with filter size 64, kernel size 3 * 3 with a
max pooling size 2 * 2, obtaining a 1600-dimensional embedding for each of 16
square grids. Then treat these embedding of 16 grids as "16 words embedding"
in a sentence, fitting it into equation 2 as our experiment 2. (3) Equal weight
self-attention-CNN: The model structure is the same as model 2, with fixing the
attention weights between all grids are the same. Model (3) is designed as an alter-
native baseline guaranteeing that the improvement of model 2 is from the attention
block instead of the model structure. The testing accuracy of three models are
reported in Table 3. Model 1 has 5558k number of parameters, and model 2 has
Figure 2:
Noisy MNIST
dataset
only 670k number of parameters. This fact also guarantees that the superiority of our model is not
from training a bigger model.
Table 3:	Testing accuracy of self-attention on the Noisy MNIST dataset
Sample size
CNN
Self-attention-CNN
Equal weight self-attention CNN
n=5k	n=20k	n=60k
0.874(0.001)~0,944(0.002)~0,965(0.002)
0.970(0.003)	0.993(0.000)	0.996(0.000)
0.900(0.001)	0,927(0.041)	0,900(0.029)
From the table, we see that Attention-CNN model achieves almost perfect testing accuracy as
the original MNIST dataset. Although equal-attention CNN has the same expressiveness power,
its performance is much worse than attention-CNN model. Again, It shows the usefulness of
concentrating self-attention weights properly in the task.
7 Conclusions
In this paper, we study the loss landscape of neural networks on attention models, and show that
attention mechanisms help reduce the sample complexity and achieve consistent predictions in the
8
Under review as a conference paper at ICLR 2021
large sample regime. Besides theoretical analyses of loss landscape, empirical studies validate our
theoretical findings. Based on our analyses, we discuss how regularization, concentration on attention,
and overparameterization in attention weight matrices can further improve the attention model.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Advances in neural information processing systems,
pp. 6155-6166, 2019.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A Convergence Theory for Deep Learning via
Over-Parameterization. In Proceedings of the 36th International Conference on Machine Learning,
ICML ’19, 2019.
Sanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit
acceleration by overparameterization. In Proceedings of the 35th International Conference on
Machine Learning, ICML 2018, Stockholmsmdssan, Stockholm, Sweden, pp. 244-253, 2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014.
Andrew R Barron and Jason M Klusowski. Approximation and estimation for high-dimensional deep
learning networks. arXiv preprint arXiv:1809.03090, 2018.
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse
transformers. arXiv preprint arXiv:1904.10509, 2019.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1019-1028, 2017.
Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In Proceedings of the 36th International Conference on Machine
Learning, ICML ’19, 2019.
Rong Ge, Jason D. Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. CoRR, abs/1711.00501, 2017.
Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier neural networks. In
Proceedings of the fourteenth international conference on artificial intelligence and statistics, pp.
315-323, 2011.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effectively.
arXiv preprint arXiv:1802.04474, 2018.
Alan Julian Izenman. Multivariate regression. In Modern Multivariate Statistical Techniques, pp.
159-194. Springer, 2013.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Hierarchical question-image co-attention for
visual question answering. In NIPS, pp. 289-297, 2016.
Thang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based
neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods in
Natural Language Processing, pp. 1412-1421. Association for Computational Linguistics, 2015.
9
Under review as a conference paper at ICLR 2021
Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher
Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting
of the Associationfor Computational Linguistics: Human Language Technologies, pp. 142-150,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http:
//www.aclweb.org/anthology/P11-1015.
Andre Martins and Ramon Astudillo. From softmax to sparsemax: A sparse model of attention
and multi-label classification. In International Conference on Machine Learning, pp. 1614-1623,
2016.
Song Mei, Yu Bai, Andrea Montanari, et al. The landscape of empirical risk for nonconvex losses.
The Annals of Statistics, 46(6A):2747-2774, 2018a.
Song Mei, Andrea Montanari, and Phan-Minh Nguyen. A mean field view of the landscape of two-
layer neural networks. Proceedings of the National Academy of Sciences, 115(33):E7665-E7671,
2018b. ISSN 0027-8424. doi: 10.1073/pnas.1806579115.
Volodymyr Mnih, Nicolas Heess, Alex Graves, and koray kavukcuoglu. Recurrent models of visual
attention. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger (eds.),
Advances in Neural Information Processing Systems 27, pp. 2204-2212. Curran Associates, Inc.,
2014.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in neural information processing systems, pp.
2924-2932, 2014.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. In Proceedings
of the 34th International Conference on Machine Learning (ICML 2017), Sydney, NSW, Australia,
6-11 August 2017, pp. 2603-2612, 2017a.
Quynh Nguyen and Matthias Hein. The loss surface of deep and wide neural networks. arXiv preprint
arXiv:1704.08045, 2017b.
Tomaso A. Poggio and Qianli Liao. Theory II: landscape of the empirical risk in deep learning.
CoRR, abs/1703.09833, 2017.
Yunchen Pu, Martin Renqiang Min, Zhe Gan, and Lawrence Carin. Adaptive feature abstraction
for translating video to text. In Proceedings of the Thirty-Second AAAI Conference on Artificial
Intelligence, (AAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pp. 7284-7291, 2018.
Blaine Rister and Daniel L. Rubin. Piecewise convexity of artificial neural networks. Neural Networks,
94:34-45, 2017.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D. Lee. Theoretical insights into the optimization
landscape of over-parameterized shallow neural networks. CoRR, abs/1707.04926, 2017.
Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees
for multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016.
Daniel Soudry and Elad Hoffer. Exponentially vanishing sub-optimal local minima in multilayer
neural networks, 2018.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,匕Ukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
Processing Systems, pp. 5998-6008, 2017.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International Conference
on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2048-2057,
Lille, France, 07-09 Jul 2015. PMLR.
10
Under review as a conference paper at ICLR 2021
Chulhee Yun, Srinadh Bhojanapalli, Ankit Singh Rawat, Sashank J Reddi, and Sanjiv Kumar.
Are transformers universal approximators of sequence-to-sequence functions? arXiv preprint
arXiv:1912.10077, 2019.
Han Zhang, Ian J. Goodfellow, Dimitris N. Metaxas, and Augustus Odena. Self-attention generative
adversarial networks. CoRR, abs/1805.08318, 2018.
Bolei Zhou, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. Simple baseline
for visual question answering. CoRR, abs/1512.02167, 2015. URL http://arxiv.org/abs/
1512.02167.
Pan Zhou and Jiashi Feng. The landscape of deep learning algorithms. CoRR, abs/1705.07038, 2017.
11
Under review as a conference paper at ICLR 2021
Appendix
In this appendix, Section A presents detailed theorem assumptions and their justifications; Section B
presents the theorem of sample complexity bounds for recurrent attention models; Section C presents
details on results in Section 4 of main paper. Section D presents a key lemma of sample complexity
bound on attention model with fixed attention masks for all samples; Finally, we provide proofs in
section E.
A Theorem assumptions
In this section, we provide more rigorous presentations on the standard assumptions of Theorems 1-3
and further justify these assumptions.
A. 1 Justification of assumptions
Here we justify (A1) to (A3) for Theorem 1, and explain why they can be intuitively summarized as
in the main paper. (A1) assumes the expectation of output y can be specified by a two-layer neural
network with attention model structure. It has been studied that general bounded functions with a
Fourier representation on [-1, 1] can be well approximated by the defined two-layer network (Barron
& Klusowski, 2018). Specific to attention models, Yun et al. (2019) prove the strong expressiveness
of transformer type self-attention model give sufficient large network. Therefore it is very mild to
assume attention model can identify the mean of output y.
(A2) assumes that the attention mask is mostly concentrated on the largest s0 attention weights. This
assumption is naturally satisfied due to the softmax function in the self-attention equation. Softmax
function makes attention weight proportion to the exponential of the inner product of query and key
vectors. This fact significantly enlarges the difference between the inner products, and makes the
attention weights do not evenly spread out over all other entries, but only a few of them. Empirically,
we verify that this assumption is satisfied in real-world transformer translation task in our experiments.
Please check experiment 1 in Section 6 for more details on the distribution of attention weights in
Portuguese to English translation. It shows that this assumption is well satisfied.
(A3) assumes that {φ1...d(hw(1), xi f (x)i)} obtain sufficient expressiveness to predict the output
y when sample size is sufficiently large. In an overparameterized network with large d, we know
{φ1...d(hw(1), xif(x)i)} have upto 2d number of linear regions over n samples. And (A2) assumes
these linear regions space all directions of φ(hw(1)?, xi f (x)i) up to a o(γ) term. It says that given
the 2d linear regions and strong expressiveness of {φ1...d(hw(1), x f (x)i)}, the residual vector is a
o(γ) term with respect to all directions in Rd. This assumption is parallel to the full column rank
condition in (Nguyen & Hein, 2017b), where we essentially assume in overparameterized network,
the linear combination spans all directions in Rp . Allen-Zhu et al. (2019) also shows the fact in their
Lemma B.1 and Corollary B.2 that in overparameterized networks, in every small region of parameter
space, there exists set of parameters with good prediction. And it leads to a good landscape in each
neighborhood. These results all indicate that it is reasonable to assume sufficient expressiveness
of φ(hw(1), x ai) in overparameterized networks as n → ∞. What’s more, with required large
sample size, we can also straightforwardly evaluate this assumption by checking whether these
φ(hw(1), x ai) spread out the whole space and their linear combinations have good estimation on
y. All these results guarantee that (A3) can be achieved in overparameterized networks. We also
empirically validate (A3) by computing the largest eigenvalue of such conditional covariance in (A3).
We generate a two-layer network under self-attention model (4) with n = 500, d = 256, p = 2142,
dv = 100. We choose the set of parameter same with our experiment setup in Section 6, and hidden
layers are sufficient overparameterized with p = 2142. Then we compute the empirical conditional
covariance Tmaχ(Var(φ(hw⑴？，Xi Θ f (x)i)∣φ(hw⑴，X Θ f(x)i)) = 3.13 * 10-8. This result
indicates that the largest eigenvalue term λmaχ(Vαr(φ(hw(1)?, Xi Θ f (x)i)∣φ(hw⑴，X Θ f (x)i))
is small enough to be assumes as o(γ2) as in (A3).
12
Under review as a conference paper at ICLR 2021
A.2 Regularity assumptions of Theorem 1
In section 3, we described the regularity assumptions of Theorem 1 in words. Here we provide
rigorous mathematical representations of these assumptions in the following (A1.1) to (A1.3).
(A1.1) xi are i.i.d and kxik∞ < Cx for any i = 1, . . . , n.
(A1.2) There exist C1, C2, Cλ such that λmax(Σφ) ≤ Cλ, kw(1) kF < C1 and kw(2)k2 <
C2 for any w(1), w(2) ∈ S.
(A1.3) E(w(2)φ(hw(1), x	f (x)i)) = E(y) = 0.
(A1.1) to (A1.3) are standard assumptions for parameter and feature space. (A1.1) and (A1.2) require
upper bounds on the input xi and `2 bound for network weights. It is a standard assumption in
landscape analysis (Mei et al., 2018a;b), and also it is crucial to remove sharp minima which may
not generalize well (Keskar et al., 2016; Dinh et al., 2017).(See discussions after Theorem 4). These
assumptions can be achieved through regularization.
(A1.3) can always be achieved by making the data centered.
A.3 Assumptions of Theorem 2
Similarly, for transformer-type self-attention, we also provide similar assumptions (A2.1) and (A2.2)
as follows:
(A2.1): (A1.2) holds with Σφ = cov(φ(hw(1)?, vec(wV xiaiself(z))). Further there exist
C5,C6 and C7 such that kwQkF ≤ C5, kwK kF ≤ C6, kwV kF ≤ C7, and kwV k1 =
max Pjp=1 wiVj ≤ C8. And kQiz KiT k2 ≥ C9 for i = 1, . . . ,n.
i=1,...,dv
(A2.2) There exists a set of parameters (a? , w(1)? , w(2)?) such that yi =
W⑵?Tφ(hw(1)-λ,vec(wvXiaieef(Z))i) + q, where aseef is calculated by (1); Ei 〜
subG(0,C42) fori = 1, 2, ...n, with xi ⊥ Ei. And the output y is centered as (A6).
These assumptions are all parallel to (A3) to (A6).
A.4 Details about Theorem 3
We consider a D-layer network with self-attention structure. We denote the kth self-attention
layer follows gk(xgk-1) = wk2 φ(hwk1 , wv xgk-1aseef i)), where xgk-1 is the output of (k - 1)th
self-attention layer, with wv ∈ Rdv×t, xgk-1 ∈ Rt×dk-1, aseef ∈ Rdk-1 ×dk-1, wk1 ∈ Rdv ×qk
and wk2 ∈ Rdk ×qk . aseef is calculated in the same way with two-layer self-attention net-
works. Then we have the final output h(x) = wD2 φ(hwD1, vec(wvxgD-1aseefi)), where
XDT = (gD-ι(…gι(x)), and WDI ∈ R1×dv, wD2 ∈ Rdk. In this way, the network calcu-
lates self-attention D times and finally produce the final prediction. It is worth mentioning that, to
obtain a scalar prediction in regression model, we flatten the value matrix of the last layer as in
the two-layer model. We still denote u = WD2 φ(hWD1 , vec(Wv XgD-1aseef i)) - E(y|X). Then the
necessary assumptions parallel to (A2) are as follows:
(A6) There exists integer k and r such that k ∈	{1,	. . . , D} and r	∈	{1, 2},	such that
CovEh(Wkr),u) ≥ cγ2) for some constant c, and such	that	k^h(wkr)k2 ≤	Ck.
This Theorem also requires mild regularity conditions as follows:
•	(A3.1) All weights Wkj for k = 1, . . . , D and j = 1, 2 satisfy kWkj k2 ≤ C10. And we
assume the prediction is centered, i.e. E(u) = 0.
•	(A3.2) There exists a set of parameters (W(1)?, W(2)?) such that yi	=
WD2φ({wD1, vec(wvXDTaSeef〉))+ Ei as defined, where Ei 〜subG(0, C4) for
i = 1, 2, ...n, with Xi ⊥ Ei. And the output y is centered as (A6).
13
Under review as a conference paper at ICLR 2021
The following theorem provides sample complexity bound of multi-layer self-attention model:
Theorem 3 Under (A6) and regularity assumptions (A3.1) and (A3.2), dself is the total number
of parameters in all value, query, key matrices. Then for any γ > 0, given the sample size:
n & log( ck )(dseif + D(dk + dv )qk), where Ck is the Lipschitz constant of Vh(wkr). With probability
tending to 1, any stationary point (W ⑴，W(2), W Q, W K) satisfies that: E(h(x) 一 E(y∣x))2 . Y2.
Because multi-layer self-attention models include a large parameter set with complicated gradients,
the assumptions are not as intuitive as the two-layer model. But the main assumptions are parallel,
such that in an overparameterized network, Vh(W) spans almost all directions, and some of them
are correlated with the bias term u. The results show that mainly the size of network parameters
determines the sample complexity bound. It implies that an efficient architecture design is critical
in reducing the sample complexity. For multi-layer cases, the regular assumptions are stated as
following (A3.1) and (A3.2).
B Recurrent attention model
In this section, we consider analyzing the sample complexity bound for representative recurrent
attention framework in Bahdanau et al. (2014). In the recurrent attention network, we follow the
setting in self-attention model, such that xi = (xi1, . . . , xip) ∈ Rt×p, corresponding to p words with
t-dimensional embedding. And x is the population version of xi . Then the generative model can be
represented as:
p
yi = W(2)T hW(1), Xa(xi)jxiji + i
j=1
Analogous to NLP setting, a(xi) is a unknown function mapping xi to a t-dimensional vector, where
a(xi)j represents the effect of the jth word in the sentence for point i. Then following the RNN
setup in Bahdanau et al. (2014), using data features themselves as their annotations, then for time
stamp k = 1, . . . , T, The recurrent attention model estimates for kth time stamp ak(xi) as follows:
sk = f (sk-1, ck-1); ekj = score(sk-1, xij)
p
ekj	j
αkj = p-^p	; Ck = / αkjXi
j=1 ekj	j=1
yi(k) = W(2)T φ(hW(1), Cki)
where score(∙) is the score function representing how well the inputs around position j and the output
at position i match. It can either be a dot product or a MLP. yi(k) is the prediction in kth time stamp.
And we denote a(xi)j = ατ7-, as the attention mask for the final time stamp. And f (∙) is the function
to update sk. Suppose the parameter set inside these two functions are Wa and Wf with number of
parameters as da and df accordingly. Here we show that when these two functions are expressive
enough, recurrent attention networks also have sample complexity bound parallel to self-attention
models. Here we introduce necessary assumptions.
(A7) When T is sufficiently large, the output y can be predicted by the two-layer network
with an independent sub-Gaussian error with variance σ2 , i.e, there exists a set of pa-
rameters (W(1)?, W(2)?) such that yi = W(2)?T φ(hW(1)?, Pjp=1 a(xi)jxiji) + i, where
Ci〜 subG(0,C42) fori = 1, 2, ...n, with xi ⊥ Ci.
(A8) Suppose (A2) holds when we substitute x	f(x) with Pjp=1 a(xi)j xij.
(A9) We assume kWak2 ≤ C8 and kWf k2 ≤ C9.
(A7) to (A9) are parallel to the assumptions in the self-attention case. They can be justified similar to
them, which is discussed in Section B of Appendix. When T is large, recurrent attention models can
represent a wide class of attentions weights. Thus we assume yi can be expressed by such recurrent
attention models.
Now we can provide the following sample complexity bound.
14
Under review as a conference paper at ICLR 2021
Theorem 6. Under (A7) to (A9), for any γ > 0, suppose
n & c2C2η2 log(ηC8C9)(td + d + da + df)
γ2	γ
where η = C1C2Cx, such that if there exist stationary point(s), then with probability tending to 1,
any stationary point (W(1), W(2), Wf, W a) satisfies the following prediction error bound:
p
E(W⑵φ(hW⑴,X a(xi)jXji) - E(y∣x))2 . Y2
j=1
Theorem 6 provides a sample complexity bound for recurrent attention networks. The bound holds
under expressiveness assumption (A7). It also shows a trade-off of expressiveness and sample
complexity. When the number of parameters is too large, the sample complexity will be too large;
When the number of the parameters is too small, we don’t have enough expressiveness to achieve
consistency. If f (∙) and a(∙) are properly selected, they will be sufficiently expressive to obtain good
stationary points, and also the number of parameters dw and df will not be too large. In this way,
an ideal sample complexity bound to these good stationary points can be achieved as Theorem 7
says. However, with an over-complicated design in these functions, the sample complexity bound
will be large; With an over-simple design, such good stationary points don’t exist. It is parallel to a
trade-off between approximation error and estimation error in learning theory. The theory implies a
good design of the recurrent structure will help achieve an optimal sample complexity in recurrent
attention model.
C Appendix for Section 4
C.1 Discussion of Theorem 4
For illustration, the linear region bounds under different
sparsity levels are plotted in Figure 1. The top red line
is for baseline model with p = 100, and other lines are
bounds for attention model with different sparsity level s0 .
In a sufficiently wide network, bn1 Cs0 is much smaller
than bn1 Cp. Then, (n1 )s0 ≤ (n1 )p holds as long as nɪ ≥
exp(Plogp-s0logs0 ).Given Plogp-sO)Iogs0 ≤ P^ logp,
since P-^ is close to P when s° is relatively small, the
result still holds when n1 is larger than the order of p.
For illustration, the linear region bounds under different
sparsity levels are plotted in Figure 1. The top red line
is for baseline model with p = 100, and other lines are
bounds for attention model with different sparsity level
s0. In general, we see the bound of attention model is
smaller than the one of baseline model. The result implies
that, when appropriate attention mechanism is used, the
reduction of number of linear regions leads to a simpler
landscape, yet the approximation error remains small. We
Figure 3: Number of linear regions in
log scale v.s. sparsity
interpret that attention mechanisms help us reduce unnecessary non-linearity inside the landscape.
C.2 Discussion of Theorem 5
Theorem 5 indicates that property on flatness of minima is maintained when attention mechanism is
applied, and there exist good sharp minima, coinciding with the observation in Dinh et al. (2017).
However, there is no guarantee that all sharp minima are good in generalization. Revisiting our
analysis in Section 3, the restriction on the parameter space helps remove these sharp minima.
Specifically, we provide upper bounds on the `2 norm of (W(1), W(2)). These constraints restrict
the parameter space and remove all sharp minima which we construct in the proof of Theorem 5.
In these constructed sharp minima, α1 or α2 goes to infinity, and `2 norm bound guarantee that
15
Under review as a conference paper at ICLR 2021
it cannot happen. In these constructed sharp minima, `2 norm bound guarantees that it cannot
happen. Practically, `2 norm bounds can be achieved through a proper `2 regularization, which
will be discussed in Section 5. It also coincides with our theoretical and empirical result that a flat
minimum are expected to generalize better in general (Keskar et al., 2016).
C.3 Small sample size
In this section, we study the local minimum of wide neural networks in small sample regime. (Nguyen
& Hein, 2017b) proves that a two-layer neural network model can always achieve perfect empirical
estimation error when the sample size is small. Here, we extend this result for self-attention model.
Theorem 7. For self-attention model in Sec 3.2, if rank(φ((W⑴,vec(wVXiaself )i)i=1,2,..n) = n.
Then every stationary point (W (1), W(2), WV, W Q, W K) of object function (4) is a global minima.
rank(φ((W(1), Xi Θ ai)i=1,2,..n) = n is a mild assumption in a wide network with over-
parameterization. We can see that as long as we choose the number of units d to be larger than n,
the linear dependence of hW(1), Xi Θ aii=1,2,..n holds with measure zero. In other words, almost
surely this matrix has full column rank n. Thus after the nonlinear activation, the full column rank
still holds almost surely. This assumption is similar to the condition in Theorem 3.8 of Nguyen &
Hein (2017b), where they assume the number of units in some layer is larger than the sample size.
When the sample size is smaller than the number of units in the network, this theorem holds for the
network without attention. It has been proved by Nguyen & Hein (2017b) and Soudry & Carmon
(2016) under different conditions.
D	Lemma: Models with a fixed attention mask
In this section, we introduced a attention model with fixed attention mask, as a fundamental building
block of attention mechanisms. In the fixed-mask attention model, we consider a dataset D =
{Xi , yi }iN=1 , Xi ∈ Rp, yi ∈ R, where the output yi depends only on certain regions of input Xi, i.e.,
yi = f?(a? Θ Xi), where a? is an unknown fixed attention mask, and f?(.) is the ground-truth
function that is used to generate the dataset and the vector a? ∈ [0, 1]p. The set of entries {ai?|ai? 6= 0}
corresponds to the relevant region of the input, while the complementary set {ai?|ai? = 0} corresponds
to the irrelevant region.
The following Lemma 1 on the sample complexity of attention model with fixed attention mask, is
not only an important building block of the proof of Theorem 1 but also provides helpful insights
for understanding attention mechanisms and revealing the main idea of our proof. With the sparsity
structure of attention mask a, attention mechanisms constrain parameters in a smaller space, thus
they reduce the variability of the empirical landscape, and also reduce the covering number of
parameter space. These results lead to a lower sample complexity compared with the baseline model
not employing attention. Similar to Corollary 1 result, it is straightforward to calculate the sample
complexity bound for the baseline model(not employing attention). To achieve the same error bound,
we substitute s0 with p in the bound, and this results in a much larger sample complexity bound.
The attention model we use can be written as:
f(X) = W(2)T φ(hW(1), X Θ ai)	(6)
The assumptions for analyzing this model is the same as the assumptions of Theorem 1(A1 to A3 and
A1.1 to A1.3), where we substitute f(X) by a in all assumptions with kak0 ≤ s0. For clarity, we
state them here explicitly:
(A.L.1) There exists a set of parameters (W(1)?, W(2)?) such that yi =
W(2)?T φ(hW(1)?, Xi Θ ai) + i for i = 1, 2, ...n, where i’s follow the sub-Gaussian
distribution subG(0, C32 ) and Xi ⊥ i.
(A.L.2) kak0 ≤ s0 with at most s0 non-zero weights.
(A.L.3) For any γ > 0, λmax(V ar(φ(hW(1)?, Xi Θ a?i)
∣φ(hW(1), x Θ a〉))= o(γ2) as n & Y log(γ)(pd + d) and n → ∞.
(A.L.4) Xi are i.i.d and kXi k∞ < Cx for any i = 1, . . . , n.
16
Under review as a conference paper at ICLR 2021
(A.L.5) There exist C1, C2, Cλ such that λmax(Σφ) ≤ Cλ, kw(1) kF < C1 and kw(2) k2 <
C2 for any w(1), w(2) ∈ S.
(A.L.6) E(w(2)φ(hw(1), x	ai)) = E(y) = 0.
Lemma 1. Under (A.L.1) to (A.L.6), then for any γ > 0 and s0 such that k . O(s0), suppose
s20C12Cx2η2	s0η
n & -----2一- log(--)(pd + P + d)
γ2	γ
where η = C1C2Cχ. Then with probability tending to 1, any Stationary point (a, W(1), W(2)) ofthe
objective function (4) satisfies the following prediction error bound:
E(WQ)Tφ(hW(1), X Θ ai) 一 E(y∣x))2 . Y2	(7)
E Proofs
In this section, we first provide a proof sketch for Theorem 1 for readers to understand the high-lelvel
idea. Then we provide proofs of all our theorems. Lemma 1, Theorem 1, Theorem 2, Theorem 3 and
Theorem 7 are proved in similar manners. For Theorem 1, Theorem 2 and Theorem 3, we omit the
exact same part of the proof as Lemma 1.
E.1	Proof sketch of Theorem 1
Proof Sketch: We target at proving that under our assumptions and required sample size, all local
minimum must have prediction ability as good as global minimum up to γ2 . The proof can be
divided into two major steps. First, we show that for all parameter sets with bad prediction, their
φ(W(1), xi Θ a) term must be correlated with the bias E(y|xi) 一 E(W(2))φ(W(1), xi Θ a). It leads
to a large magnitude of population gradient with respect to W(2). Second, we construct an -cover
of parameter sets (W(1), W(2)) to show that sample gradients converge to population gradient, such
that sample gradient with respect to W(2) is also away from zero. Thus these parameter sets with bad
prediction cannot be local minimum. And we conclude that all local minimum must have prediction
as good as global minimum up to O(γ2). The complete proof is provided in the Appendix Section E.
E.2 Proof of Lemma 1
Proof. As described in the proof intuition, the proof is divided into two steps. First we study the
lower bound of population risk gradient ∣∣Eχ,y (VRn(w(2)))k2; In step 2 We study the convergence
of IlVRn(W⑵)k2 to the population gradient ∣∣Eχ,y(VRn(W(2)))∣∣2. We further separate these two
steps into three lemmas.
Lemma 1.1 proves the first step, we study the landscape of population risk, showing that with
high probability, we know the population risk with respect to x and y IEx,y (VRn(W(2)))I2 is
large; Lemma 1.2 and Lemma 1.3 prove the second step. Specifically, in part (b), we consider the
convergence of population risk only taking expectation on y, Ey(VRn(W(2))). In part (c), finally we
consider the convergence of empirical risk gradient VRn(W(2)) to Ey(VRn(W(2))).
We introduce necessary notations beforehand. To emphasize the role of x and y separately, here
we denote R(W(1), W(2), a) = Ey(Rn(W(1), W(2), a)), which is the expectation of the empirical
loss gradient with respect to y, treating x as random, and VR(W) as corresponding derivatives.
And we denote Ex (VR(W(1), W(2), a)) = Ex,y(VRn(W(1), W(2), a)), which is the expectation of
the empirical loss function with expectation to both x and y. In the proof, we may use o(γ) for
vector/matrix case. In these cases, it means that every element in vector/matrix is o(γ).
Lemma 1.1 Under the assumption of Lemma 1, when equation 7 is violated, with probability going
to 1 that, the population risk gradient with respect to W(2) satisfies that:
IEx,y(VRn(W(2)))I2 ≥ O(γ)
17
Under review as a conference paper at ICLR 2021
Proof: We denote:
u = w(2)φ(hw(1), x ai) - E(y|x)	(8)
and ui as the version with the specified sample index. Then the derivatives of population risk with
the expectation to y can be presented as follows:
1n
VR(w(2)) = - y^Uiφ(hw(1), Xi Θ a))
n i=1
1n	0
VR(W(I)) = — Eui(Xi Θ a)(w⑵ Θ φ (hw(1), Xi Θ a)))T
n i=1
1n	0
VR(a) = — ɪ^ui(Xi Θ (W⑴(W⑵ Θ φ (hw(1), Xi Θ a))))
By (A.L.1), we know that E(yi|Xi) = w(2)?T φ(hw(1)?, Xi Θ a?i). Therefore when
(a, W(1), W(2)) = (a?, W(1)?, W(2)?), all the ui ’s are zero, and all the gradients with expectations
to X and y are zero. Thus for any true set of parameter (a?, W(1)?, W(2)?), they have zero gradient
expectations automatically. And the key of our proof is showing that with high probability, any
parameter (a, W(I), w(2)) cannot be stationary point if E(|W(2)φ(hW⑴，X Θ a)) - E(y∣X)∣2) ≥ γ2,
because their gradients w.r.t to W(2) must be bounded away from zero.
In the following section, we prove Lemma 1.1 by showing that if E(u2 ) ≥ γ2, we must have
kE(VR(W(1)))k2 ≥ O(γ). We prove it by contradiction.
In this proof, we denote r = cov(E(y|X), φ(hW(1), X Θ a))) ∈ Rd, and the covariance matrix for
φ(hW(1), X Θ a)) as:
(Σφ)ij = cov(φi(hW(1), X Θ a)), φj (hW(1), X Θ a)))
If we have kE(VR(W(2)))k2 = o(γ), i.e.
kcov(u, φ(W(1), Xi Θ a))k2 = o(γ),
and plug it into cov((W(2))T φ(hW(1), X Θ a)), φ(hW(1), X Θ a))) = r + o(γ), we have:
kΣφW(2) - rk2 = kcov(W(2))T φ(hW(1), X Θ a)) - E(y|X), φ(hW(1), X Θ a)))k2
= kcov(u, φ(W(1), Xi Θ a))k2 = o(γ)
Since we know the true mean y can be specified by a true set of parameter (W(1)?), W(2)?, a?), and
we denote that the covariance matrix for φ(hW(1)?, X Θ a?)) as:
(Σφ)j = coMφi(hw()*, X Θ a?i), φj (hw(1)?, X Θ a?)))
And we denote the cross-covariance matrix of vector φi(hW(1)?, XΘa?)) and vector φj (hW(1), XΘa))
as:
(Σc)j = cov(φi(hw5, X Θ a?)), φj (hw(1), X Θ a)))
Then with zero expectation on E(y|X) and E(W(2)T φ(W(1), Xi Θ a)) we have:
E(u2) = var(E(y|X) - W(2)T φ(W(1), Xi Θ a))
= var(E(y|X)) + var(W(2)T φ(W(1), Xi Θ a)) - 2cov(E(y|X), W(2)T φ(W(1), Xi Θ a))
= var(W(2)?T φ(hW(1)?, X Θ a?))) + var(W(2)T φ(W(1), Xi Θ a))
- 2cov(W(2)?T φ(hW(1)?, X Θ a?)), W(2)T φ(W(1), Xi Θ a))
=w(2)?T Σφw(2)? — 2w(2)?T £?w(2) + rτ Σφr + cλθ(γ2)
=w(2)?T Σφw(2)? — 2w(2)?T Σc Σ-1 Σcw(2)? + w(2)?T ΣcΣ-1Σcw(2)? + cλo(γ)
=W⑵?T (Σφ — ∑c∑-1∑c)w⑵？ + Cλθ(γ2)
18
Under review as a conference paper at ICLR 2021
where Σφ 一 Σ∑-1∑c is exactly the residual of covariance matrix after taking regression on
{φ1...d(hw(1), x	f (x)i)}, as we defined in (A.L.3).(Page 176 of Izenman (2013))
W⑵?T(∑φ 一 ∑c∑-1∑c)w⑵？ is a quadratic form of that conditional variance matrix. By (A.L.5),
we know its largest eigenvalue λmaχ(Σφ — ∑c∑-1∑c) = o(γ). Finally we obtain that:
E(u2) = ∣∣E(VR(w ⑵))k2 + ci。”)
≤ kw(2)*k2 * λmaχ(Σφ - ∑c∑-1∑c) + Ci。”)
=O(Y2)
By contradiction, we conclude that if E(u2) ≥ γ2, we must have kE(VR(w(2))k2 ≥ O(γ). Here
we finish the proof of Lemma 1.1.
Lemma 1.2 Under the assumption of Lemma 1, when equation 7 is violated, the risk gradient of w(2)
with expectation to y satisfies:
kEy(VRn(w(2)))k2 ≥ O(γ)
proof: In Lemma 1.1 we have shown that in population level, all parameter sets with bad prediction
has a large magnitude of E(VR(w(2))). In Lemma 1.2, we use -cover technique to bound the gap
between VR(w (2))(recall that VR(w(2)) is short notation of Ey(VRn(w(2)))) and E(VR(w(2))).
Our parameters w(1),w(2) and a are inside the `2 balls Bd(0, C1),Bp×d(0, C2) and Bp(0, s20), where
Bp(c, r2) represents a p-dimensional `2 with center c and radius r. By Lemma 5.2 in Vershynin
(2010), the -covering number N1,N2,N3 for these three balls are upper bounded by:
N1 ≤ (3C1/)d, N2 ≤ (3C2/)pd, N3 ≤ (3s02/)p
Then we know the joint 3-covering number for the union of all three parameters N3 satisfies
that N3 ≤ N1 N2 N3. For the ease of notation, we denote θ = (a, w(1), w(2)). Let Θ =
{θι,…，ΘnJ be a corresponding cover with N3e elements. Following the E-COVering we construct
for (w(1), w(2), a) separately, we can always find Θ such that for any feasible θ, there exists
j ∈ [N] such that max(kw((j1)) 一 w(1) k2, kw((j2)) 一 w(2) k2, ka(j) 一 ak2 ) ≤ E. In this proof, we use
parenthesis subscription (j) to represent the jth element in the cover, to distinguish it from other
subscriptions.
By triangle inequality, we have:
kVR(w(2))k2 ≥ kVR(w((j2)))k2 一 kVR(w(2)) 一 VR(w((j2)))k2	(9)
Therefore, we only need to bound both parts on the r.h.s of equation 9. We start with the first term
kVR(w((j2)))k2. To achieve this, we first bound the gradient term vi = uiφ(w(1), xi a). For any
fixed parameter set, we calculate:
kvik22 . (C2kφ(w(1),xi	a)k2)2kφ(w(1), xi	a)k22
Here we denote kw(1) k1,active as the `1 norm of w(1) on active features, i.e. on the feature such that
its attention weight a is not zero, and from the sparsity condition we know that there are at most s0
such nonzero elements in x a. Combining with the `2 bound of w(2), we apply Cauchy-Schwarz
inequality:
kφ(w(1), xi	a)k2 ≤ max{|x a|}kw(1)k
1,active = √s0CxC1
It implies:
kvik22 . (C2kφ(w(1), xi	a)k2)2kφ(w(1), xi	a)k22 = O(s20C14C22Cx4)	(10)
From Lemma 1.1, we know there exists a constant c such that kEx(VR(w(2)))k2 ≥ cγ for some
constant c. We denote ξ2 = s20C14C22Cx4 . Then we apply Hoeffding bound on the `2 norm of
19
Under review as a conference paper at ICLR 2021
Ex(VR(w(2))), with the upper bound of it as O(σ2). Denoting VR(w(2)) as the gradient with
respect to jth parameter set in -cover for j ∈ {1, . . . , N}:
P(kVR(w(2)) — Ex(VR(W(2)))k2 ≥ cγ) . exp(-nc2Y-)
(j)	x	(j)	2	3	ξ2
Then by union bound, over all N elements in Θ :
P(∃j ∈ [Ne], kVR(w(-))k2 ≥ 2cγ) & 1 - N exp(-ncξγ2)
Secondly we analyze kVR(w(2)) - VR(w((j2)))k2 term. Here we use ui to represent the prediction
error for ith instant with respect to parameter (a, w(1), w(2)), and use ui(j) to represent the term
with respect to the parameter from j th element in -cover set. By triangle inequality, we have:
2n
∣∣VR(w(2)) - VR(w(2))k2 ≤ n k f(uiφ(<w ⑴,Xi Θ a))- Ui(j)φ(hwj Xi Θ aj)i))∣∣2
n i=1
—
We choose
CY
3soCχc2C2
, and plug back above results into equation 9, then at least with probability
1 一 O(Ne exp(-ncξγ2)), We have ∣∣VR(w(2))k2 > cY. Therefore We can choose:
)σ2	s0C1C2 CxZd ,「八
n & c2ψ log(——cγ——)(pd + P + d)
such that Ne exp(-ncξγ-) = o(1). Finally we can conclude that with probability 1 一 0n(1), for any
(a, W⑴，w(2)) such that E(W(2)φ(hW⑴，X Θ ai) 一 E(y∣x))2 ≥ γ, we have ∣∣VR(w(2))k2 > Cγ.
Lemma 1.3 Under the assumption of Lemma 1, when equation 7 is violated, with probability going
to 1 that, the empirical risk gradient with respect to W(2) satisfies that:
∣VRn (W(2) )∣2 ≥ O(γ)
Proof: So far in Lemma 1.2, we have shown that for population risk with respect to y, with high
probability, all the parameter sets with poor prediction in expectation, i.e E(u2 ) ≥ O(γ2), their
population risk gradient with expectation to y must be away from zero. Now we move forward to
show that empirical risk ∣VRn(W(2))∣2 converges to ∣VR(W(2))∣2. In aspect of W(2), they can be
represented as:
1n
VRn(W(2)) -VR(W⑵)=-Eeiφ(hw⑴，Xi Θ a))
n i=1
With (AL1), we know that e 〜subG(0, C2), thus n Pn=I e = O(√n) by C.L.T, combining
the bound for φ(hW(1), Xi Θ ai) we have derived in the proof of Lemma 1.2, with sample size
n & cξγ2 log( s0CCC2Cx )(pd + p + d), conclude that with probability 1 一 0p(1):
∣VRn(W(2)) -VR(W(2))∣2 ≤ CY	(11)
6
(12)
Recalling part (a), under the first case that w.h.p ∣∣VR(W(2))k2 ≥ 号 for any parameter
(a, W⑴，w(2)) with ∣∣W(2)φ(hW(1), X Θ a)) 一 E(y∣X)∣2 ≥ γ. Combining this with (11), we
conclude that for any positive constant γ > 0, with required sample size, with high probability that
∣VRn(W(2))∣2 > 0, thus they cannot be stationary solution for our loss function.
20
Under review as a conference paper at ICLR 2021
In other words, under our assumptions, all the stationary points (a, W⑴,W⑵)in our programming
satisfy the prediction error upper bound rate γ w.h.p, when sample size:
22
n & -0■为 iog(—)(pd + P + d)
c2 γ2	cγ
□
E.3 Proof of Theorem 1
Proof. Theorem 1 can be proved following the same manner as Lemma 1, substituting a by f (x),
only changing two bounds. Here we specify these two different bounds with Lemma 1. First
difference is the bound of kφ(hW(1), xi f (x)i)k2 in Lemma 1.2, and the second is the -cover
number in Lemma 1.2.
By assumption (A2), we denote f (x)lead = {f(x)(1), . . . , f (x)(s0)} as the top s0 leading attention
weights such that kf(x)leadk2 ≥ 1 - τ. And kf (x)sub k as the other p - s0 attention weights and
it satisfies kf(x)subk2 ≤ τ as kf (x)k2 = kf (x)lead k2 + kf (x)sub k2 = 1. We denote xlead and
Wl(e1a)d as the features and network weights corresponding to leading attention weights, and xsub,
Ws(1u)b as the features and network weights corresponding to other attention weights.
Parallel to Lemma 1.2, we just need to adjust the bound of φ(hW(1), xi	f (x)i) term as:
kφ(hW(1), xi	f (x)i)k2 = kφ(hWl(e1a)d, xlead	f(x)leadi + hWs(1u)b,xsub	f(x)sub)ik2
≤ √2(M(Cw(I!d, XIead Q f (X)Ieadi)k2 + kφ(Cwsub, xsub Q f (X)Subi)k2
The two terms in the last inequality can be bounded by Cauchy-Schwarz inequality separately:
kφ(CWl(e1a)d, Xlead Q f (X)lead i)k2 ≤ kW(1)k2kXlead Q f (X)lead k2
≤ IlW ⑴ ∣∣2∣∣XleadII2kf(x)leadk2 ≤ Cl * √S0 * Cx * (1 - T) ≤ √S0ClCχ
And
Iφ(CWs(1u)b, Xsub Q f (X)sub i)I2 ≤ IW(1) I2 IXsub Q f (X)sub I2
≤ IlW⑴k2∣∣χsub∣∣2∣∣f(χ)sub∣∣2 ≤ Ci * √P* Cx * τ = τ√pCιCx
Combining both inequalities, we have:
kφ(hw⑴，Xi Q f(x)i)∣∣2 ≤ √2(√S0 + T√p)CiCχ	(13)
Further we have
kvik2 . (C2kφ(hw(1), Xi Q f (X)i)k2)2kφ(hw⑴,Xi Q f (X)i)k2 = (√S0 + T√p)4C4C2Cx
(14)
Second, in this case, since f(X) is not optimized together, but calculated from X instead. We don’t
have to consider the -cover number for a in the maximum operator. Therefore the new -cover
number for W(1) and W(2) are upper bounded as:
N1 ≤ (3C1/)d, N2 ≤ (3C2/)pd
Substituting the new -cover bound to the theorem, we obtain the final sample complexity bound:
n & (√s0TE log(也MPd + d)
γ2	γ
□
21
Under review as a conference paper at ICLR 2021
E.4 Proof of Theorem 2
Proof. Theorem 2 can be proved following the same manner as Lemma 1, substituting a by aself,
only changing several bounds. Here we specify the differences with Lemma 1.
Finally, we obtained a new -covering bound accordingly:
N1 ≤ (3C1/)d,N2 ≤ (3C2/)pdvd, N3 ≤ (3C5/)tdq,N4 ≤ (3C6/)tdq, N5 ≤ (3C7/)tdv,
where N = Πi5=1Ni .
Then, the new vi terms parallel to Lemma 1.2 is:
vi = uiφ(hw(1), vec(wV xiaiself)i)
where ui = (w(2)T φ(hw(1), vec(wV xiaiself)) - E(yi|xi). Under assumptions, using the same
argument as Lemma 1.1 and Theorem 1 under overparameterized network, we have that, there exists
a constant C such that either ∣∣E(VR(w(2))k2 ≥ cγ.
In the case when ∣∣E(VR(w(2))∣2 ≥ cγ. Here We derive the new bound of ∣∣v∣2 with respect to x.
We have:
∣vi∣22 . (C2∣φ(hw(1), vec(wV xiaiself)i)∣2)2∣φ(hw(1), vec(wV xiaiself)i)∣22
Then we derive the bound of ∣φ(hw(1), vec(wV xiaiself)i)∣2. Using our assumption (A4):
∣ais(ellefad) ∣2 ≥ 1 - τ, following the same steps as derived in equation 13, we define alseealfd and
asseulbf as top s0 leading attention weights and other following attention weights. Then we have:
kφ((W(I),vec(WV Xiaself )i)∣2 ≤ kw ⑴ gkkwV ∣∣2kxi Θ asef ∣∣2
≤ √2ClC8(∣hxlead Θ aSeadi∣2 + ∣Xsub Θ 成笈川2)
≤√2(√S0 + T √P)C1C8
Finally we obtain:
加k2.(C2 Mhw ⑴,Vec(WV Xiaself ^)^2110((W ⑴,Vec(WV Xiaself )i)∣2 = 4(√s0 + T √P)4。4。2。4。4
Denoting ξ2 = (√S0 + T√p)4C2CgCχ;, parallel to Theorem 3, applying Hoeffding bound and union
bound:
P(∃j ∈ [Ne], ∣Vj ∣2 ≤ 2Cγ) . N exp(-ncξf)
Second, we bound ∣VR(W(2)) - VR(W(2))(j) ∣2 term. The attention weight gap is bounded as:
∣QizKiT - Qiz(j)KiT(j)∣max = ∣(Xiz)T ((WQ)T Wk)Xi - (Xiz)T(W(Qj)W(kj)Xi∣max
. t(C5 +C6)Cx2
With this bound, we know aiself(z) is Lipschitz function under assumption (A9) such that ∣QizKiT ∣2
has lower bounded. Recall the softmax function for vector β is defined as:
sof tmax(β)
exp(β)
Pp=I exp(βi)
It is Lipschitz continuous on β when Pip=1 exp(βi) has a lower bound, which is satisfied by (A2.1).
Thus the bound can be derived as:
l∣aself(z) — as(jlf(z)∣2 . ∣∣softmax(Q√K-) - softmax(Q认√Kij )∣2
.√Pt(C5 + 06)0Xe
22
Under review as a conference paper at ICLR 2021
With these bounds, we have:
∣∣VR(w⑵)-VR(W⑵ jk2
1n
≤ -k ^ui(j)φ(hw(v) ,vec(wv Xiasef )))- Uij)φ(hw(1), vec(wv Xiaself)))∣∣2
n i=1
1n
.n(k E(Ui-Ui°)欣MZveHwV Xiaself ))k2
i=1
n
+ k X Ui(j)(φ(hw(1), wv Xiaiself) - Ui(j)φ(hw((j1)), vec(w(vj)Xia
i=1
.((√s0 + T√P)2tC2C2C2Cxe + √ptcic2C8(C5 + CG)CxE
.((√s0 + T√P)2 + √Pt)(C1C2C8(C1C2C8 + (C5 + C6))e
is(ejl)f)))k2)
Recall that Ui(j ) and ai(j ) are corresponding to the jth element in the epsilon cover. Denote ξ =
((√s0 + τ√p)2 + √pt)(C1C2C8(C1C2C8 + (C5 + C6)), and We choose e = cγ, and combine the
above results. Then at least with probability 1 一 O(Ne exp(-nc~sγ~)), we have ∣∣VR(w(2))k2 > 3.
Therefore we can choose n & CfY2 log( ((√s0+τ√p) +√3?CIC2CCCCCC8C )(pdvd + d + 2pdq), such
then Ne exp(-ncξγ-) = o(1). Thus with this required sample complexity, we have ∣∣VR(w(2)) 一
E(VR(W(2))∣∣2 ≤ 等.
Finally we conclude that with high probability, any parameter (w(1), w(2), w(v), w(k), w(q)) with
E(u2) ≥ γ, we have ∣∣VR(w(2))∣2 > 3. Then following the same empirical risk convergence
argument as Lemma 1.2, with high probability they cannot be stationary point. We conclude the
sample complexity bound as:
n&
(再 + T √p)2 + √pt)2Pdv dη2
Y2
iog( ((√s0 + T √p)2 + √pt)ηaηb)
γ
□
E.5 Proof of Theorem 3
Proof. Under assumption (A3.1), we know all input features and weights are bounded. Therefore we
know ∣Vf (Wkr)∣2 is a Lipschitz continuous function on all parameters, and we denote its Lipschitz
constant ck. For Wkr, we can derive that:
1n
VR(Wkr ) = — EUiVh(Wkr )
n i=1
Under (A6), if we have E(h(X) - E(y|X))2 & γ2, then:
IlE(VR(Wkr))∣∣2 = IIcov(Vh(Wkr),u)∣∣2 & O(Y)
Then similar with Theorem 1 and 2, we construct an e-cover over all parameters θ :=
(Wk1 , Wk2, Wv, WQ, WK), and we denote it as {θ1, . . . , θN} such that for any feasible parame-
ter, there exist j ∈ [N] such that the maximum `2 distance to θj is smaller than e. By calculating the
number of parameters in all matrices in θ, we have
1k
Iog(Ne) = 2O(dself + ɪ^(dk + dv M )
e	i=1
Denoting VR(W(kjr)) as the gradient with respect to jth parameter set in e-cover for j ∈ {1, . . . , Ne}:
P(∣VR(Wj)) - Ex(VR(Wkj))∣2 ≥ CY) . exp(-nc2γ2)
23
Under review as a conference paper at ICLR 2021
By union bound, we have:
P(∃j ∈ [Ne], kVR(wg))k2 ≥ 23γ) . Ne exp(-ncγ)
Secondly We analyze ∣∣VR(wkr) 一 VR(Wj))∣∣2 term. As We have shown that the gradient is
Lipschitz continuous, thus we have:
IlVR(Wkr)-VR(wj))∣∣2 ≤ CKe
We choose E = 3Cγ7, then at least with probability 1-O(Ne exp(-n c2γ2)), we have ∣∣VR(w(2))∣2 >
cY. Therefore we can choose n & log(ck)(dseif + D(dk + dvqk)), such that Ne exp(-nCcY2)=
o(1). Finally we can conclude that with probability 1 一 on(1), for any (a, W(1), W(2)) such that
E(W⑵φ(hW⑴，X Θ ai) 一 E(y∣x))2 ≥ γ, we have ∣VR(w⑵)∣2 > cγ. Then following the
convergence of empirical risk procedure of Lemma 1.2, we show with probability going to 1 such
that ∣VRn(W(2))∣2 > 0 and all parameters with prediction error O(γ) cannot be stationary point as
long as n & log(ck)(dseif + D(dk + dvqk)). Thus we complete the proof.	□
E.6 Proof of Theorem 4
Proof. First with ∣f (x)∣0 = s0, we know all the inputs xi with corresponding ai = 0, will be
inactive in the network. We can omit all these inactive inputs. Then we split n1 units into s0
group, with [nn1 C number of units in each group, and discard the leftover units. s° different groups
correspond to s0 active inputs with non-zero attention weight.
Inside each group, for example in jth group, denoting q = [nC, we choose the input weights and
biases for i = 1,2, ∙ ∙ ∙ , q as:
h1 (x) = max{0, Wj x},
h2(x) = max{0, 2Wjx 一 1},
hq (x) = max{0, 2Wjx 一 (q 一 1)}
here we assign Wj to be a row vector with j th variable equal to 1 and all other entries to be 0. And in
the second layer, we choose W⑵ =(w3,…，w3), where
w3 = (1, -1,1,…，(-1)q+1), corresponding to hi to hq in each group. Then the designed network
has q linear regions inside each group, giving by the intervals:
(-∞, 0],(0,1],(1,2],…，[q - 1, ∞)
Each of these intervals has a subset that is mapped by W3h(x) onto the interval (0,1).Montufar et al.
(2014) Therefore the total number of linear regions is lower bounded by [ nn1 C Cs.	□
E.7 Proof of Theorem 5
Proof. Here we define an α scale transformation such that:
Tα : (W(1), W(2)) 7→ (αW(1), α-1W(2))
And all the value,query and key matrices remain the same. Then we know the Jacobian determinant
for Tα = α(pdv-1)d. Since pdvd ≥ d, as we assign α → ∞, such that the Jacobian determinant goes
to infinity, and the volume of C(L, θ, E) goes to infinity.
For the Hessian matrix, we still assume a positive diagonal element δ > 0 in W(1). Simi-
larly we have the Frobenius norm ∣V2L(Tα (θ))∣F of
V2L(Tα(θ))
α-1I
0
α0I	V2L(θ)
α-1I
0
0
αI
is lower bounded by α-2δ. When we choose sufficient small α, we have the biggest eigenvalue of
V2L(Tα1,α2 (θ)) is larger than any constant M. Therefore there exists a stationary point such that
the operator norm for Hessian is arbitrary large.
24
Under review as a conference paper at ICLR 2021
E.8 Proof of Theorem 6
Proof. First, we obtained new -covering bound for the parameter set (w(2) , w(1) , wf, wa):
N1 ≤ (3C1/)d,N2 ≤ (3C2/)td, N3 ≤ (3C8/)da,N4 ≤ (3C9/)df,
And N ≤ Πi4=1 Ni Similar to Theorem 1, we denote
p
u = (w(2)T φ(hw(1),	a(xi)xij) - E(y|x))	(15)
j=1
Then the derivatives of population risk with expectation to y can be presented as follows:
1n	p
VR(W ⑵)=nɪ2uiΦ(hw(1),∑S a(xi)xj)
i=1	j=1
And also, the new vi term is: vi = uiφ(hwj(1), Pjp=1 a(xi)xiji). Following (A15) and same
procedure as Lemma 1.1, we have kE(VR(w(2))k2 ≥ cγ. we have new bound of kvk22 with respect
to x is upper bounded by σ2 = t2C14C22C82C92CX4 with normalized attention weight. Same argument
follows for the case when kE(VR(w(1))kk2 ≥ cγ. Then following the same approach as Theorem 1
and 2, we obtain the sample complexity bound:
σ2	tC1C2C8C9Cx
n & c2γ2 log(----C3γ----)(d + td + df + da)
□
E.9 Proof of Theorem 7
Proof. Here we only consider the derivatives with respect to w(1) and w(2), they can be presented as
follows:
1n
VRn(w(2)) = 一 yφuW(^1)wVιe(vec^waXiailʃ)i)
n
i=1
1n	0
VRn(W(I)) = - ɪ2Ui(vec(wvXiaieIf ))(w(2) Θ φ (hw(1), vec(wVxiaself ))T
n i=1
By assumption, rank(φ(hw(1), vec(wVXiaiself)i)i=1,...,n) = n. Solving the linear system, we must
have Ui = 0 for any i = 1, 2,..., n to satisfy that VRn(W(2)) = 0. Thus We know that the loss is
exactly zero inside sample. Thus it must be a global minimum.	□
F	Fixed attention mask experiments on NoisyMNIST
In this additional experiments, another variant of NoisyMNIST dataset is constructed where the
images of digits from the MNIST dataset are embedded in noise, as shown in Figure (a). Then we
compare a two-layer MLP baseline model (in blue) with a global (fixed) attention model proposed in
Supplemental D (in red). Fig (b) shows a plot of epochs v.s. test loss while Fig (c) shows a plot of
epochs v.s. test accuracy. We observe that the global attention model obtains lower loss values and
higher test accuracy with fewer epochs. Again it verifies the superiority of attention weights are from
the effect of concentrating attention weights.
25
Under review as a conference paper at ICLR 2021
26