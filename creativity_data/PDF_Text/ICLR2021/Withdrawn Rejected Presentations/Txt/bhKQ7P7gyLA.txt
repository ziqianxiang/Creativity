Under review as a conference paper at ICLR 2021
Manifold Regularization for
Locally Stable Deep Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
We apply concepts from manifold regularization to develop new regularization
techniques for training locally stable deep neural networks. Our regularizers en-
courage functions which are smooth not only in their predictions but also their
decision boundaries. Empirically, our networks exhibit stability in a diverse set
of perturbation models, including '2, '∞, and Wasserstein-based perturbations; in
particular, against a state-of-the-art PGD adversary, a single model achieves both
'∞ robustness of 40% at E = 8/255 and '2 robustness of 48% at E = 1.0 on
CIFAR-10. We also obtain state-of-the-art verified accuracy of 21% in the same
'∞ setting. Furthermore, our techniques are efficient, incurring overhead on par
with two additional parallel forward passes through the network; in the case of
CIFAR-10, we achieve our results after training for only 3 hours, compared to
more than 70 hours for standard adversarial training.
1	Introduction
Recent results in deep learning highlight the remarkable performance deep neural networks can
achieve on tasks using data from the natural world, such as images, video, and audio. Though such
data inhabits an input space of high dimensionality, the physical processes which generate the data
often manifest significant biases, causing realistic inputs to be sparse in the input space.
One way of capturing this intuition is the manifold assumption, which states that input data is not
drawn uniformly from the input space, but rather supported on some smooth submanifold(s) of much
lower dimension. Starting with the work of Belkin et al. (2006), this formulation has been studied
extensively in the setting of semi-supervised kernel and regression methods, where algorithms ex-
ploit the unlabelled data points to learn functions which are smooth on the input manifold (Geng
et al., 2012; Goldberg et al., 2008; Niyogi, 2013; Sindhwani et al., 2005; Tsang and Kwok, 2007;
Xu et al., 2010). Such techniques have seen less use in the context of deep neural networks, owing
in part to the ability of such models to generalize from relatively sparse data (Zhang et al., 2016).
Contributions We apply concepts from manifold regularization to train locally stable deep neural
networks. In light of recent results showing that neural networks suffer widely from adversarial
inputs (Szegedy et al., 2013), our goal is to learn a function which does not vary much in the neigh-
borhoods of natural inputs, independently of whether the network classifies correctly. We show that
this definition of local stability has a natural interpretation in the context of manifold regularization,
and propose an efficient regularizer based on an approximation of the graph Laplacian when the
data is sparse, i.e., the pairwise distances are large. Crucially, our regularizer exploits the contin-
uous piecewise linear nature of ReLU networks to learn a function which is smooth over the data
manifold in not only its outputs but also its decision boundaries.
We evaluate our approach by training neural networks with our regularizers for the task of image
classification on CIFAR-10 (Krizhevsky et al., 2009). Empirically, our networks exhibit robustness
against a variety of adversarial models implementing '2, '∞, and Wasserstein-based attacks. We
also achieve state-of-the-art verified robust accuracy under '∞ of size E = 8/255. Furthermore, our
regularizers are cheap: we simply evaluate the network at two additional random points for each
training sample, so the total computational cost is on par with three parallel forward passes through
the network. Our techniques thus present a novel, regularization-only approach to learning robust
1
Under review as a conference paper at ICLR 2021
neural networks, which achieves performance comparable to existing defenses while also being an
order of magnitude more efficient.
2	Background
Manifold regularization The manifold assumption states that input data is not drawn uniformly
from the input domain X , also know as the ambient space, but rather is supported on a submanifold
M ⊂ X, called the intrinsic space. There is thus a distinction between regularizing on the ambient
space, where the learned function is smooth with respect to the entire input domain (e.g., Tikhonov
regularization (Phillips, 1962; Tikhonov et al., 2013)), and regularizing over the intrinsic space,
which uses the geometry of the input submanifold to determine the regularization norm.
A common form of manifold regularization assumes the gradient of the learned function PMf (x)
should be small where the probability of drawing a sample is large; we call such functions “smooth”.
Let μ be a probability measure with support M. This leads to the following intrinsic regularizer:
||f||I2 :=
M
||VMf (x)∣∣2dμ(x)
(1)
In general, we cannot compute this integral because M is not known, so Belkin et al. (2006) propose
the following discrete approximation that converges to the integral as the number of samples grows:
1N
∣∣f∣∣2 ≈ nf2 Σ (A”) - f(χj))2Li,j
(2)
i,j=1
Here, the xi, ...,xn are samples drawn, by assumption, from the input manifold M according to μ,
and L is a matrix of weights measuring the similarity between samples. The idea is to approximate
the continuous input manifold using a discrete graph, where the vertices are samples, the edge
weights are distances between points, and the Laplacian matrix L encodes the structure of this
graph. A common choice of weights is a heat kernel: Li,j = L(xi, xj) := exp(-||xi - xj||2/s). To
improve computational costs, weights are often truncated to the k-nearest neighbors or within some
-ball. Note that the Laplacian can also be interpreted as a discrete matrix operator, which converges
under certain conditions to the continuous Laplace operator (Belkin and Niyogi, 2008).
ReLU networks Our development focuses on a standard architecture for deep neural networks:
fully-connected feedforward networks with ReLU activations. In general, we can write the function
represented by such a network with n layers and parameters θ = {Ai, bi}i=1,...,n-1 as
z0 = x	(3)
Zi = Ai ∙ zi-1 + bi	for i = 1,…,n - 1	(4)
Zi = σ(Zi)	for i = 1,...,n — 2	(5)
f(x； θ) = Zn-1	(6)
where the Ai are the weight matrices and the bi are the bias vectors. We call the Zi “hidden activa-
tions”, or more simply, activations, and the Zi “pre-activations”.
In this work, We consider networks in which σ(∙) in (5) is the Rectified Linear Unit (ReLU)
Zi = σ(Zi) := max(0, Zi)	(7)
It is clear from this description that ReLU networks are a family of continuous piecewise linear
functions. We denote the linear function induced by an input X as fχ(∙; θ), i.e., the analytic extension
of the local linear component about x over the input domain.
Adversarial robustness One common measure of robustness for neural networks is against a
norm-bounded adversary. In this model, the adversary is given an input budget E over a norm ∣∣∙∣∣in,
and asked to produce an output perturbation δ over a norm ∣∙∣out. A point χ0 is an e-δ adversarial
example for an input pair (x, y ) if
||x0 - x||in ≤ E	(8)
|f(x0; θ) - y|out ≥ δ	(9)
2
Under review as a conference paper at ICLR 2021
When the specific norm is either unimportant or clear from context, we also write the first condition
as x0 ∈ N(x), where N (x) refers to the -ball or neighborhood about x. If such an adversarial
example does not exist, We say that the network is e-δ robust at x. Standard examples of || ∙ ||in
include the '2 and '∞ “norm”, defined for vectors as ∣∣χ∣∣∞ := maxi |xi|. For classification tasks,
the adversary is successful if it produces an example in the -neighborhood of x which causes the
network to misclassify. In this case, we drop δ and say that the network is -robust at x. Note that if
f (x; θ) is already incorrect, then x suffices as an adversarial example.
3	Related work
Manifold regularization was first introduced by Belkin et al. (2006) in the context of semi-supervised
learning, where the goal was to leverage unlabeled samples to learn a function which behaves well
(e.g., is smooth, or has low complexity) over the data manifold. The use of manifold regulariza-
tion for deep neural networks has been explored in several contexts (Tomar and Rose, 2014; 2016;
Hu et al., 2018; Zhu et al., 2018). In particular, Lee et al. (2015) combine manifold regulariza-
tion with adversarial training and show improvements in standard test accuracy. Our approach is
to use manifold regularization to induce stability separately from accuracy. We note that a similar
decomposition between accuracy and stability forms the basis for the TRADES algorithm (Zhang
et al., 2019), though the training procedure ultimately relies on adversarial training. Hein and An-
driushchenko (2017) propose a conceptually similar regularizer to minimize the difference between
logits and show improved `2 certified robustness. Finally, several prior works explore regulariz-
ing for robustness based on other differential forms (Ross and Doshi-Velez, 2017; Jakubovitz and
Giryes, 2018), though they only report results using the weaker single-step PGD adversary. In par-
ticular, a recent work by Zhai et al. (2020) uses randomized smoothing for `2 certified robustness,
and claim similar computational advantage due to avoiding adversarial training, but still take 61
hours to train, compared to only 3 hours in our approach.
Adversarial examples were introduced by Szegedy et al. (2013), who found that naively trained
neural networks suffer almost complete degradation of performance on natural images under slight
perturbations which are imperceptible to humans. A standard class of defenses is adversarial train-
ing, which is characterized by training on adversarially generated input points (Goodfellow et al.,
2014). In particular, the Projected Gradient Descent (PGD) attack (Kurakin et al., 2016; Madry
et al., 2017) is widely considered to be an empirically sound algorithm for both training and evalu-
ation of robust models. However, such training methods rely on solving an inner optimization via
an iterative method, effectively increasing the number of epochs by a multiplicative factor (e.g., an
overhead of 5-10x for standard PGD). Achieving robustness against multiple adversarial models
has also been explored previously (Schott et al., 2018; Tramer and Boneh, 2019; Maini et al., 2019;
Croce and Hein, 2019b), though in most cases these works use weaker variants of the subset of
standard adversaries we consider (e.g., a smaller or the single-step version of PGD).
Another approach is to train models which are provably robust. One method is to use an exact veri-
fication method, such as an MILP solver, to prove that the network is robust on given inputs (Tjeng
et al., 2017). In particular, Xiao et al. (2019) use a similar loss based on ReLU pre-activations to
learn stable ReLUs for efficient verification, but rely on a PGD adversary to train a robust model.
Certification methods modify models to work directly with neighborhoods instead of points (Dvi-
jotham et al., 2018; Gowal et al., 2018; Mirman et al., 2018; Wong et al., 2018). In practice, the
inference algorithms must overapproximate the neighborhoods to preserve soundness while keeping
the representation compact as it passes through the network. This strategy can be interpreted as solv-
ing a convex relaxation of the exact verification problem. Though certification thus far has produced
better lower bounds, verification as a technique is fully general and can be applied to any model
(given sufficient time); recent work also suggests that methods using layerwise convex relaxations
may face an inherent barrier to tight verification (Salman et al., 2019).
4	Setting
We reframe the goal of learning functions that are robust using a perspective which decouples sta-
bility from accuracy. The key observation is that we would like to train networks that are locally
stable around natural inputs, even if the network output is incorrect. This approach contrasts with
3
Under review as a conference paper at ICLR 2021
adversarial training, which attempts to train the network to classify correctly on worst-case adver-
sarial inputs. In particular, recall that a network is -robust at x if no point in the -neighborhood of
x causes the network to misclassify. We consider the related property of -stability (cf. Zheng et al.
(2016)):
Definition 4.1. A function f is	-δ	stable at an input x if for all x0	∈ N(x),	|f(x)	-	f0(x)|	≤	δ.	A
classifier f is -stable at an input x if for all x0 ∈ N(x), f(x) = f (x0).
As -stability is independent of the correct label for x, we argue that -stability is a property of the
function with respect to the input manifold and can thus be captured using manifold regularization.
For completeness, we state the following connection between robustness and stability:
Proposition 4.1. A function f is -δ robust at an input x iff f is -δ stable at x and f(x) = y. A
classifier f is -robust at an input x iff f is -stable at x and f correctly classifies x.
5	Manifold regularization for deep neural networks
Applying the regularization term in (2) yields, in the limit, a function which is smooth on the data
manifold. Unfortunately, a straightforward approach does not suffice for our goal of learning -stable
deep neural networks. The first problem is that smoothness on the data manifold does not yield
-stability (Stutz et al. (2019)); indeed, we are concerned with the behavior of our function in an -
neighborhood of the manifold, not just on the manifold. The second observation is that convergence
of the discrete approximation requires that the samples be dense over the input manifold; however,
this assumption is almost certainly violated in most practical applications, particularly in the deep
learning regime. The next two sections are dedicated to these challenges.
5.1	Resampling for local smoothness
We write the -neighborhood of a manifold M as M := {x : ∃y ∈ M, ||x - y|| ≤ }. Since
-stability is defined over the -neighborhood of every input point, we might want a function which
is smooth over M instead of just M, e.g., by slightly perturbing every input point. This strategy
does produce samples from the -neighborhood of the data manifold, however note that the induced
distribution is not uniform but rather the convolution of the choice of noise distribution with the
uniform distribution over M. Nevertheless, this procedure exhibits several properties we can exploit.
The first is that for sufficiently small , we get nearly the same operator over M and M, so that
smoothness over M does not sacrifice smoothness on the original data manifold M. A more subtle
property is that we can actually draw as many distinct points as we would like fromM. We leverage
these extra points to build a regularizer which yields good estimations of the local smoothness.
Moreover, taking a discrete approximation of the form in Equation 2 with the resampled points from
M still converges to the original operator. Formally, we can state the following:
Proposition 5.1. Let , s > 0 be given. Letx1, ..., xn be n samples drawn uniformly at random from
a submanifold M ⊂ X. For each xi, pick c new points xi,1, ..., xi,c by sampling iid perturbations
δi,ι,…,δi,c and setting xi,j = Xi + δ%,j, where N%,j, ∣∣δi,j|| < e Given a kernel ks(x,y) =
exp(-||x - y||2/s), let L and Lc be the Laplacian matrices defined by the n original samples and
n ∙ c new samples, respectively. Then if e2 < s, we have that L and Lc converge to the same operator
in the limit as n → ∞.
We prove this result in Appendix A. For our purposes, this result states that the resampled Laplacian
enjoys the same behavior in the limit as the original Laplacian.
5.2	Sparse approximation of resampled Laplacian
The Laplacian matrix is dense, and requires both O(N 2) time and space to compute and store. It is
thus standard to employ heuristics for sparsifying Laplacians to derive efficient algorithms; in this
work, we consider the -neighborhood sparsifier, denoted L, which is defined as the Laplacian of
the graph created by retaining only those edges whose weights are at most (Von Luxburg, 2007).
To motivate this approach, our main observation is that, under certain sparsity assumptions in the
data, the resampled Laplacian will consist of two types of edges: very short edges between points
4
Under review as a conference paper at ICLR 2021
resampled from the same -neighborhood, and very long edges between points sampled from dif-
ferent -neighborhoods. For an appropriate choice of the scale factor s, the exponential form of the
heat kernel causes the weights on the long edges to fall off very quickly compared to the short edges.
The following results gives bounds for this approximation of the regularizer in Equation 2 under a
certain separation assumption in the data:
Proposition 5.2. Consider a function f anda set of points {xi}iN=1 such that the pairwise differences
bounded from above and below by 0 < c1 ≤ ||f (xi) - f (xj)||22 ≤ c2 < ∞ for all i 6= j. Let s be the
parameter of the heat kernel ks = exp(-||xi - xj ||2/s). Assume further that the points {xi} are
separated in the sense that there exist at least O(n) pairs ofpoints (xii,xi2), i ∈ S whose distances
are boundedfrom above by √s, and the remainder ofthe points have distances boundedfrom below
by m√s for some constant m > L Writing R(L) := PNj=ι(f (Xi) — f (Xj ))2Lij, we have that
R(L) - c2N2 exp(-m2) ≤ R(L) ≤ R(L) ≤ (1 + b-1)R(L)	(10)
where b = Θ((c1 /c2)(exp(m2)/N)).
We give a proof in Appendix A. In this work, we only consider bounded functions f, which allows us
to apply this proposition. This result gives two bounds on the dense regularization R(L) in terms of
R(L). The first inequality says that the absolute approximation error is bounded by a term which
vanishes unless the squared sample count N2 grows as the exponential of the squared separation
m2 . The third inequality gives a tighter bound on the relative error in a term that vanishes unless
the linear sample count N grows as the exponential of the squared separation m2, but requires that
we have good control over the ratio c1/c2. Note that the dependence on c1/c2 is unavoidable in the
sense that a function which is very smooth in local neighborhoods of size s will necessarily have
large relative contribution from the longer edges; however, in this case we still have a bound on the
absolute contribution (though we trade the ratio c1 /c2 for a factor of N). Crucially, in both cases
the error bound depends on an exponential factor of the squared separation m2 .
To provide some empirical evidence that the separation m can be made non-trivial in some set-
tings, we offer the following statistics from CIFAR-10. We select a random subset of 10,000 (20%)
training samples. For each of these points, we find the closest point within the remaining 49,999
training samples, and compute the distance. Using the `2 metric, we found a mean distance of
9.597, a median distance of 9.405, and the distance at the 10th percentile to be 5.867. Conversely,
We resample points in an '∞ ball of radius E = 8/255, which is contained in an '2 ball of radius
√3 ∙ 32 ∙ 32 ∙ 8/255 = 1.739. In fact, only 21 training samples, or 0.21% of the random subset, have
a partner closer than twice the perturbation bounds. Thus, after the resampling procedure, setting the
scale parameter to s = E2 and taking the E-neighborhood Laplacian should yield a good approxima-
tion to the full Laplacian. Finally, to generalize this to larger datasets, for fixed S one would expect
that the separation m√S between points grows with the dimension of the ambient space, which
gives an exponential decay in the approximation error. Indeed, this is one of the manifestations of
the oft-cited curse of dimensionality (Hastie et al., 2009).
Due to our resampling procedure, we have at least c2N points whose squared pairwise distances are
less than E2, which motivates setting S ≈ E2; notice that this choice of S satisfies the assumptions of
Proposition 5.1 needed for convergence. However, rather than find the E-neighborhood Laplacian,
we propose to compute the regularizer only using points resampled from the same E-neighorhood,
yielding the following sparse approximation of the intrinsic regularizer:
1Nc
||f||2 ≈ c2NX X (f(xi,j) — f(xi,k))2L(xi,j,xi,k)	(II)
i=1 j,k=1
We emphasize that this is a heuristic, motivated by computational efficiency, whose quality ulti-
mately depends on the sparsity of the dataset and the particular choice of E. In particular, this
approximation should not be used when the data is dense and there are many points that are within E
distance of each other. However, the diagonal form of this approximation permits extremely efficient
computations, particularly on vectorized hardware.
5
Under review as a conference paper at ICLR 2021
Figure 1: Surface and contour plots for Hα (α = 1).
5.3	Hamming regularizers
We additionally leverage the structure of ReLU networks to induce a stronger regularization effect
on the data manifold. The central observation is that not just do we want the function computed by
the neural network to be constant (i.e., smooth with respect to its outputs) on the manifold, we also
want the network to produce its outputs in roughly the same way.
First, We identify every local linear component fχ(∙; θ) With its “activation pattern”，i.e., the Se-
quence of branches taken at each ReLU. We will write fH (x; θ) for the map that takes inputs x
to their activation patterns, Which live on the unit hypercube {0, 1}N (Where N is the number of
ReLUs in the netWork). If We endoW the hypercube With the standard Hamming distance dH , this
induces a pullback (Psuedo-)metric in the input space: dH(x, y) = d〃(fH(x; θ), fH(y; θ)). Notice
that the metric identification is exactly the set oflocal linear components fχ(∙; θ).
Recall that the goal of regularization is to reduce the complexity of the learned function f in the
e-neighborhood of inputs x. We argue that dH provides a concise measure of this complexity.
Specifically, if we consider the number of distinct local linear components in Ne(x), then since dH
is a psuedo-metric, we have that ∀x0 ∈ NW(X), dH(x, x0; θ) = 0 if and only if Ne(X) is a single
linear component. Thus, minimizing dH between X and all x0 ∈ Ne(X) reduces the number of
linear components in the neighborhood of x. In fact, by the triangle inequality, the same holds in the
interior of any convex polytope defined by a set of points {xi}i∈ι where ∀i, i0 ∈ I, dH (xi, Xi0;θ)=
0. This makes minimizing dH very sample efficient.
Treating f〃(∙； θ) as an output of the network, we have the following (sparse) manifold regularizer:
IlfH(∙;Θ)I∣2 :=[
M
IVmJh (x; θ)∣∣2dμ(x)
Nc
≈ C2⅛ΣΣdH (Xij ,Xi,k; θ)2 L(Xij,Xi,k)
i=1 j,k=1
(12)
(13)
which is just Equations 1 and 11 with the outputs f(X) replaced by the activation pattern map.
However, this loss term is not continuous in the inputs, and furthermore, the gradients vanish almost
everywhere, so it does not generate good training signals. We thus use a continuous relaxation:
Hα(Z, y； θ) := abs(tanh(α * z) — tanh(α * y))	(14)
This form is differentiable everywhere except when z = y, and recovers the Hamming distance
when we take α to infinity (after scaling). Qualitatively, sensitive activations (i.e., small ∣Z∣ and
|y|) are permitted so long as they are precise. Figure 1 presents the surface and contour plots of
Hα . Note that this idea can be extended more generally to other activation functions by penalizing
differing pre-activations more when the second derivative of the activation function is large (and so
the first-order Taylor approximation has larger errors).
6
Under review as a conference paper at ICLR 2021
5.4	Training with sparse manifold regularization
For every sample x, we generate a new random maximal perturbation ρ ∈ {±}d and produce
the pair of perturbed inputs x+ := x + p and x- := x - p. We compute the standard manifold
regularization term as ||f (∙, θ)∣∣2 8 PJf(x+) - f (x-)∣∣2∙ For the Hamming regularize. We use
the `2 norm to combine the Hamming distances between pre-activations within a layer and normalize
by tWice the number of elements in each layer. We sum over the layers, then normalize by the total
number of layers. Note that in both cases, the Weights L(xi+, xi-) can be dropped since the distance
betWeen xi+ and xi- is constant. The final optimization objective is thus
1N
θ =	argmin	布 Vfv (f (xi； θ),yi) +	YK ||f (•； θ)"K +	YI||f (•；	θ)"2 +	YH ||fH (∙；	θ)ll2	(15)
θ N i=1
where V is the loss function, ||f (∙, θ) ∣∣K is the ambient regularizer (e.g., '2 regularization), and the
YK, YI, YH are hyperparameters Which control the relative contributions of the different regularizers.
5.5 Discussion
Our development has focused on learning deep neural networks which are smooth when samples are
sparse over the input manifold M. While this property is related to -stability, in general the two
are not equivalent. Roughly, -stability is inherently a property about worst case behavior, whereas
manifold regularization is aimed at learning a function which is smooth on average. Nonetheless,
we note that a function achieving zero loss with the manifold regularizer yields perfect stability (i.e.,
a constant function on the data manifold).
Next, we discuss our choice to focus on the sparse setting. The basic idea is two-fold: first, the
requirement that the learned function be -stable significantly alters the geometry of the input mani-
fold; second, when data is sparse, the amount of information one can glean about the overall geom-
etry of the input manifold is limited, as is evidenced by the vanishing weights in the Laplacian. Our
main hypothesis is that the combination of these two observations allows one to formulate a version
of the regularizer built from resampled data, which maintains the same properties in the limit but
yields more local information when data is sparse.
Conversely, consider the alternative approach, namely, directly learning a smooth function when it
is possible to produce samples, not necessarily labelled, that are dense over the input manifold M.
Then given the central thesis behind this work, one would expect stability to improve. In fact, the top
four results for robust accuracy reported in Croce and Hein (2020) all exploit an additional dataset
of unlabeled images which is orders of magnitude larger than the original training set (Carmon et al.,
2019; Hendrycks et al., 2019; Uesato et al., 2019; Wang et al., 2020).
6	Experimental results
We report results for two models trained on CIFAR-10 image classification using our regularization
techniques: a smaller CNN for exact verification, following Xiao et al. (2019); and a PreActRes-
Net18 model (He et al., 2016) for benchmarking against a variety of adversarial models. We also
include three ablation studies for the larger model, including one which does not use the sparsifica-
tion scheme developed in Section 5.2 (dense regularizer), and two using the individual regularizers
to isolate their individual effects on stability (intrinsic / Hamming regularizer only). Appendix B
reports training details and hyperparameters; full experimental results (including MNIST, which is
not discussed here) are in Appendix C.
6.1	Stability
The main characteristic that differentiates our approach conceptually from existing methods for
adversarial robustness is that we train our models using pure regularization, i.e., without reference
to a particular adversary. In comparison, most methods train and test using inputs produced by the
same adversary. While this procedure yields good performance against the adversary used during
training, performance degrades significantly when evaluated under different perturbation models.
In general, the bounds for various adversarial models are set so that maximal perturbations remain
7
Under review as a conference paper at ICLR 2021
Table 1: CIFAR-10 robust accuracy against various adversaries and perturbation models.
Test Accuracy (%)
Defense	`2 = 0.5	'∞ = 8/255	Wasserstein = .1	Clean
Wong et al. (2019)			76	80.69
baseline ('∞ certified)			61	66.33
Maini et al. (2019)	66.0	49.8*		81.7
baseline ('∞ PGD, Madry et al. (2017))	<5.0	45.8		87.3
Manifold Regularization	57.53§	36.90§	66.02	69.95
§ Computed using the full AutoAttack+ attack (Croce and Hein, 2020), which includes APGD-CE,
APGD-DLR (+targeted), FAB (+targeted) (Croce and Hein, 2019a), and Square Attack (Andriushchenko
et al., 2019). ^With slightly smaller e = 0.03 ≈ 7.65/255 JRePorted in Maini et al. (2019).
somewhat reasonable by human standards; thus, we expect a model that is stable in the general sense
to be robust against a variety of adversaries, particularly those not seen during training.
In line with this observation, we test a single model trained to be stable using manifold regularization
against three norm-bounded perturbation models. The first two constitute the most common settings
for adversarial robustness, namely, CIFAR-10 robustness using '∞ and '2 PGD adversaries bounded
at = 8/255 and 0.5, respectively. We also use a Wasserstein-based PGD adversary at = 0.1,
which is a metric for images more closely aligned with human perceptions. Our results indicates that
a model trained using our manifold regularizer does in fact learn to be more stable on test images
across a variety of different perturbation models (Table 1). We obtained our results after training for
3 hours on one GPU, compared to several days for standard PGD training (Shafahi et al., 2019).
For comparison, we note that standard adversarial training using PGD from Madry et al. (2017) ,
which forms the basis of every state-of-the-art approach for '∞ robustness (Croce and Hein, 2020),
achieves negligible performance when evaluated in the `2 setting. We also report results from Maini
et al. (2019), which was the only other approach in the literature to evaluate their models for both '∞
and `2 robustness at the full levels of using a PGD adversary; their models are trained for 50 epochs
with 50 steps of PGD against multiple adversaries in parallel (≈ 5000 effective epochs). Finally, we
also report results from Wong et al. (2019) for comparison against the Wasserstein adversary. Their
results are obtained against a PGD adversary using a 50-step inner optimization (number of training
epochs is not reported); their baseline model is trained to be certifiably robust against '∞ attacks of
size = 2/255, and achieves lower robustness than ours.
6.2	Ablation studies
We run several ablation studies to better understand the behavior of our methods. The first column
of Table 2 reports the verifiable robustness of our smaller model, which outperforms the previous
state of the art as reported in Xiao et al. (2019). We emphasize that we provide this metric as a base-
line for establishing a provable lower bound on the robustness achieved by our defense, compared
to empirical robustness which is often a brittle measure of true robustness, particularly for newly
proposed defenses (Athalye et al., 2018; Carlini et al., 2019; Tramer et al., 2020).
The second column of Table 2 presents robust accuracy against the standard PGD adversary used in
the literature. The dense regularizer has both lower robust and clean accuracy, which suggests an
over-regularization effect stemming from longer edges that do not yield reliable information about
the intrinsic geometry of the manifold. Furthermore, training with the dense regularizer takes around
21.5 hours, or nearly 8 times longer than the sparse method (due to both the overhead of computing
additional gradients during backpropogation, as well as the smaller batch sizes for offsetting the
increased memory requirements for storing the dense Laplacian). We also report the results from
using intrinsic or Hamming regularizers alone, which indicate that both terms contribute jointly and
individually to our performance. For comparison, we report results for other approaches which
are not variants of PGD (i.e., do not rely on solving a minimax problem at each training loop).
8
Under review as a conference paper at ICLR 2021
Table 2: CIFAR-10 provable and robust accuracies against an '∞ adversary at E = 8/255.
Mechanism	Defense	Test Accuracy (%)		
		Verified	Robust	Clean
Adversarial Training	PGD (Madry et al. (2017))		45.8	87.3
	FGSM (Madry et al. (2017))		0.0	90.3
Verification	Xiao et al. (2019)	20.27	26.78	40.45
	Manifold Regularization (small)	21.04	25.56	36.66
	Pang et al. (2019户		24.8	92.7
	Manifold Regularization (large)		40.54	69.95
Regularization	dense regularizer		35.04	64.08
	intrinsic regularizer only		20.11	34.74
	Hamming regularizer only		24.87	90.24
^ With slightly smaller e = 0.03 ≈ 7.65/255. § Computed from 500 / 10000 images in test set. ^Results
for regularization only; reported robust accuracy is 55.0% when trained with PGD.
To the best of our knowledge, Pang et al. (2019) is the only other result in the literature which
achieves non-trivial '∞ robust accuracy on the standard benchmark of CIFAR-10 at E = 8/255
using a regularization-only approach; their results are better than either of our regularizers used
independently, but significantly worse than when the regularizers are used together, in either the
sparse or the dense setting. For context, the best result in this setting is reported by Wang et al.
(2020) using a PGD variant at 65.04% robust accuracy.
7	Conclusion
We design regularizers based on manifold regularization that encourage piecewise linear neural
networks to learn locally stable functions. We demonstrate this stability by showing that a single
model trained using our regularizers is resilient against '2, '∞, and Wasserstein-based attacks. We
also achieve state-of-the-art verified robustness of 21% against '∞-bounded perturbations of size
E = 8/255 on CIFAR-10. Critically, computing our regularizers relies only on random sampling,
and thus does not require running an inner optimization loop to find strong perturbations at training
time. As such, our techniques exhibit strong scaling, since they increase batch sizes rather than
epochs during training, allowing us to train an order of magnitude faster than standard adversarial
training. This work thus presents the first regularization-only approach to achieve comparable results
to standard adversarial training against a variety of perturbation models.
9
Under review as a conference paper at ICLR 2021
References
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack: a query-
efficient black-box adversarial attack via random search, 2019.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples, 2018.
Mikhail Belkin and Partha Niyogi. Towards a theoretical foundation for laplacian-based manifold methods.
Journal ofComputer and System Sciences, 74(8):1289 - 1308, 2008. ISSN 0022-0000. doi: https://doi.org/
10.1016/j.jcss.2007.08.006. URL http://www.sciencedirect.com/science/article/pii/
S0022000007001274. Learning Theory 2005.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric framework for
learning from labeled and unlabeled examples. Journal of machine learning research, 7(Nov):2399-2434,
2006.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks, 2017.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian
Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial robustness. arXiv preprint
arXiv:1902.06705, 2019.
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John C. Duchi. Unlabeled data improves
adversarial robustness, 2019.
Ronald R. Coifman and StePhane Lafon. Diffusion maps. Applied and Computational Harmonic Analysis, 21
(1):5 -30, 2006. ISSN 1063-5203. doi: https://doi.org/10.1016/j.acha.2006.04.006. URL http://www.
sciencedirect.com/science/article/pii/S1063520306000546. Special Issue: Diffusion
Maps and Wavelets.
Francesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive boundary
attack, 2019a.
Francesco Croce and Matthias Hein. Provable robustness against all adversarial lp-perturbations for p ≥ 1,
2019b.
Francesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks, 2020.
Krishnamurthy Dvijotham, Sven Gowal, Robert Stanforth, Relja Arandjelovic, Brendan O’Donoghue, Jonathan
Uesato, and Pushmeet Kohli. Training verified learners with learned verifiers, 2018.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, and Dimitris Tsipras. Robustness (python library), 2019.
URL https://github.com/MadryLab/robustness.
Bo Geng, Dacheng Tao, Chao Xu, Linjun Yang, and Xian-Sheng Hua. Ensemble manifold regularization. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 34(6):1227-1233, 2012.
Andrew B Goldberg, Ming Li, and Xiaojin Zhu. Online manifold regularization: A new learning setting
and empirical study. In Joint European Conference on Machine Learning and Knowledge Discovery in
Databases, pages 393-407. Springer, 2008.
Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples,
2014.
Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Relja
Arandjelovic, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for
training verifiably robust models, 2018.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining,
inference, and prediction. Springer Science & Business Media, 2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks, 2016.
Matthias Hein and Maksym Andriushchenko. Formal guarantees on the robustness of a classifier against ad-
versarial manipulation. In Advances in Neural Information Processing Systems, pages 2266-2276, 2017.
10
Under review as a conference paper at ICLR 2021
Matthias Hein, Jean-Yves AudiberL and Ulrike Von Luxburg. From graphs to manifolds-weak and strong
pointwise consistency of graph laplacians. In International Conference on Computational Learning Theory,
pages 470-485. Springer, 2005.
Matthias Hein, Jean-Yves Audibert, and Ulrike von Luxburg. Graph laplacians and their convergence on
random neighborhood graphs. Journal of Machine Learning Research, 8(Jun):1325-1368, 2007.
Dan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and
uncertainty, 2019.
Hongwei Hu, Bo Ma, Jianbing Shen, Hanqiu Sun, Ling Shao, and Fatih Porikli. Robust object tracking using
manifold regularized convolutional neural networks. IEEE Transactions on Multimedia, 21(2):510-521,
2018.
Daniel Jakubovitz and Raja Giryes. Improving dnn robustness to adversarial attacks using jacobian regulariza-
tion. In Proceedings of the European Conference on Computer Vision (ECCV), pages 514-529, 2018.
Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world. arXiv preprint
arXiv:1607.02533, 2016.
Taehoon Lee, Minsuk Choi, and Sungroh Yoon. Manifold regularized deep neural networks using adversarial
examples, 2015.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep
learning models resistant to adversarial attacks, 2017.
Pratyush Maini, Eric Wong, and J Zico Kolter. Adversarial robustness against the union of multiple perturbation
models. arXiv preprint arXiv:1909.04068, 2019.
Matthew Mirman, Timon Gehr, and Martin Vechev. Differentiable abstract interpretation for provably robust
neural networks. In International Conference on Machine Learning, pages 3578-3586, 2018.
Partha Niyogi. Manifold regularization and semi-supervised learning: Some theoretical analyses. The Journal
of Machine Learning Research, 14(1):1229-1250, 2013.
Tianyu Pang, Kun Xu, Yinpeng Dong, Chao Du, Ning Chen, and Jun Zhu. Rethinking softmax cross-entropy
loss for adversarial robustness, 2019.
David L Phillips. A technique for the numerical solution of certain integral equations of the first kind. Journal
of the ACM (JACM), 9(1):84-97, 1962.
Andrew Slavin Ross and Finale Doshi-Velez. Improving the adversarial robustness and interpretability of deep
neural networks by regularizing their input gradients. arXiv preprint arXiv:1711.09404, 2017.
Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to
tight robustness verification of neural networks. In Advances in Neural Information Processing Systems 32,
pages 9835-9846. Curran Associates, Inc., 2019.
Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel. Towards the first adversarially robust
neural network model on mnist. arXiv preprint arXiv:1805.09190, 2018.
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph Studer, Larry S. Davis, Gavin
Taylor, and Tom Goldstein. Adversarial training for free!, 2019.
Vikas Sindhwani, Partha Niyogi, Mikhail Belkin, and Sathiya Keerthi. Linear manifold regularization for large
scale semi-supervised learning. In Proc. of the 22nd ICML Workshop on Learning with Partially Classified
Training Data, volume 28, 2005.
David Stutz, Matthias Hein, and Bernt Schiele. Disentangling adversarial robustness and generalization. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6976-6987, 2019.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks, 2013.
Andrei Nikolaevich Tikhonov, AV Goncharsky, VV Stepanov, and Anatoly G Yagola. Numerical methods for
the solution of ill-posed problems, volume 328. Springer Science & Business Media, 2013.
11
Under review as a conference paper at ICLR 2021
Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer
programming. arXiv preprint arXiv:1711.07356, 2017.
Vikrant Singh Tomar and Richard C Rose. Manifold regularized deep neural networks. In Fifteenth Annual
Conference of the International Speech Communication Association, 2014.
Vikrant Singh Tomar and Richard C. Rose. Graph based manifold regularized deep neural networks for auto-
matic speech recognition, 2016.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple pertur-
bations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche-Buc, E. Fox, and
R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages 5866-
5876. Curran Associates,	Inc., 2019.	URL http://papers.nips.cc/paper/
8821- adversarial- training- and- robustness- for- multiple- perturbations.
pdf.
Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial
example defenses, 2020.
Ivor W Tsang and James T Kwok. Large-scale sparsified manifold regularization. In Advances in Neural
Information Processing Systems, pages 1401-1408, 2007.
Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, and Pushmeet
Kohli. Are labels required for improving adversarial robustness?, 2019.
Ulrike Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395-416, 2007.
Ulrike Von Luxburg, Mikhail Belkin, and Olivier Bousquet. Consistency of spectral clustering. The Annals of
Statistics, pages 555-586, 2008.
Xu Wang. Spectral convergence rate of graph laplacian, 2015.
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial
robustness requires revisiting misclassified examples. In International Conference on Learning Representa-
tions, 2020.
Eric Wong, Frank R. Schmidt, Jan Hendrik Metzen, and J. Zico Kolter. Scaling provable adversarial defenses,
2018.
Eric Wong, Frank R. Schmidt, and J. Zico Kolter. Wasserstein adversarial examples via projected sinkhorn
iterations, 2019.
Kai Xiao, Vincent Tjeng, Nur Muhammad Shafiullah, and Aleksander Madry. Training for faster adversarial
robustness verification via inducing relu stability. ICLR, 2019.
Zenglin Xu, Irwin King, Michael Rung-Tsong Lyu, and Rong Jin. Discriminative semi-supervised feature
selection via manifold regularization. IEEE Transactions on Neural networks, 21(7):1033-1047, 2010.
Runtian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Liwei
Wang. Macer: Attack-free and scalable robust training via maximizing certified radius. arXiv preprint
arXiv:2001.02378, 2020.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning
requires rethinking generalization, 2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P. Xing, Laurent El Ghaoui, and Michael I. Jordan. Theoret-
ically principled trade-off between robustness and accuracy, 2019.
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. Improving the robustness of deep neural
networks via stability training. In Proceedings of the ieee conference on computer vision and pattern recog-
nition, pages 4480-4488, 2016.
Wei Zhu, Qiang Qiu, Jiaji Huang, Robert Calderbank, Guillermo Sapiro, and Ingrid Daubechies. Ldmnet: Low
dimensional manifold regularized neural networks. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 2743-2751, 2018.
12
Under review as a conference paper at ICLR 2021
A Proofs
Proof of Proposition 5.1. Given a heat kernel ks(x, y) = exp(-||x - y||2/s) and samples
x1, ..., xN, we define the discrete operator L for an out-of-sample point x as
1N	1N
Lf(X) = Nfks(X,xi)f (X)- NfkslX,x )f (Xi)	(16)
Our claim is that the original Laplacian L and the resampled Laplacian L0c converge pointwise to the
same continuous operator.
The proof proceeds in two parts. First, we show that resampling once still yields convergence of the
Laplacian. Notice that we can model the resampling procedure as a form of noise; our desired result
follows immediately from the following result of Coifman and Lafon (2006):
Proposition A.1 (Criterion 5 in Coifman and Lafon (2006)). Suppose that X is a perturbed version
of M, that is, there exists a perturbation function η : M → X with a small norm (the size of
the perturbation) such that every point in X can be written as X + η(X) for some X ∈ M. Then
the approximation is valid as long as the scale parameter s remains larger than the size of the
perturbation.
Since we have that the size of our perturbations is bounded by 2 < s, this gives us the desired
result.
Next we show pointwise convergence holds for arbitrary resampling c. This is a simple conse-
quence of the linearity of the operator L. We order the samples as Xi,j , where i indexes the original
data points i = 1, ..., N, and j indexes the resample j = 1, ..., c. Furthermore, let Ljc denote the
Laplacian operator defined only on the jth resampled points Xi,j for i = 1, ..., N. Then
1	cN	1 cN
Lcf(X) = cN I2ks(X, Xi)f (x) - cNEks(X,Xi)
1	c= 1 N	= 1 N
=CE Nfks(X,xi,j )f (X)- NEks(X,xij)f (Xij)
1c
=一 ELc f (x)
c j =1
-----→ Lf (X)
n→∞
where the last line follows from the first half of the proof.
(17)
(18)
(19)
(20)
□
Remarks. We have shown that L0 converges pointwise to the discrete operator L. However, for
L to converge to its continuous counterpart in Equation 1 (i.e., the Laplace-Beltrami operator), we
actually need to take the scale of the kernel s to zero (see, e.g. the proofs of convergence in Belkin
and Niyogi (2008); Hein et al. (2005; 2007)). Then Proposition 5.1 implies that we also need to take
to zero, so that in the limit we are just sampling from the unperturbed manifold. In fact, it is possible
to prove convergence without taking s to zero, however the continuous operator that is recovered
no longer refers to the gradient but rather the value of the kernel over the manifold (Von Luxburg
et al. (2008) analyze this case). The spectra of these operators are often used in various clustering
algorithms; in the context of deep neural networks, these operators yield similar regularizers for
“smoothness”, though for a different definition than the one used in this work. Finally, it should also
be noted that the convergence of these operators depends on the intrinsic dimension of the manifold;
as we consider here an -neighborhood of the manifold which has the same dimension as the ambient
space, our operators will converge more slowly (Wang (2015)).
13
Under review as a conference paper at ICLR 2021
Proofof Proposition 5.2. We introduce Le := L - Le, which is just the LaPlacian on the Com-
plement of the neighborhood subgraph, i.e., the subgraph whose edges are at least . Clearly
R(L) = R(Le) + R(Le). Then We have the following bounds:
R(Le)= X (f (xi) - f (xj))2Li,j ≤ C2N2 exp(-m2)	(21)
i,j6∈S
R(Le) = X (f(xi) - f (xj))2 Li,j ≥ ac1N exp(-1)	(22)
i,j∈S
where the a is a constant introduced for the O(N) edges in the graPh of Le.
Take b = (ac1/c2)(exp(m2 - 1)/N). A simPle rearrangement gives c2 = (b-1 ac1)(exp(m2 -
1)/N), thus we have that
R(L) = R(Le) + R(L e)	(23)
≤ R(Le) + c2N2 exp(-m2)	(24)
≤ R(Le) + (b-1ac1)(exp(m2 - 1)/N)N 2 exp(-m2)	(25)
= R(Le) + b-1ac1Nexp(-1)	(26)
≤ R(Le) + b-1R(Le)	(27)
= (1 + b-1)R(Le)	(28)
Then the first inequality follows from (24), the second inequality is trivial, and the third inequality
follows from (23)-(28).	□
14
Under review as a conference paper at ICLR 2021
B	Experimental methods and hyperparameters
We use a PreActResNet18 model (He et al., 2016) for the CIFAR-10 robustness experiments. We
train using SGD with a batch size of 128 and weight decay γK of 5e-4. We follow Maini et al.
(2019) for our learning rate schedule, which is piecewise linear and starts at 0 and goes to 0.1 over
first 40 epochs; 0.005 over the next 40 epochs; and 0 over the final 20 epochs. We increase epsilon
from 2 to 8 over epochs 10 to 35. We start the weight γI of the manifold regularization at 0.8 and
the weight γH of the Hamming regularization at 2,400; these increase linearly up to a factor of 10
from epochs 20 to 80. We set the hyperparameter α = 8. We use this set of hyperparamaters for all
the CIFAR-10 ablation studies (except when setting γI or γH to 0 for the ablation studies involving
the individual regularizers).
For the dense regularizer, we used heat kernel with scale s = 2 as weights for the Laplacian. We
use a smaller batch size of 40 to offset the increased memory requirements of storing and computing
the dense regularizer.
We use a two-layer convolutional neural network for the CIFAR-10 verification experiments, con-
sisting of 2x2 strided convolutions with 16 and 32 filters, then a 128 hidden unit fully connected
layer. This is the same model as used in Wong et al. (2018) and Xiao et al. (2019), except those
works use a 100 hidden unit fully connected layer. We use the same schedule for the learning rate
and as in the PreActResNet18 model. The weight γI starts at 0.4 and the weight γH starts at 9000;
these increase linearly up to a factor of 10 from epochs 20 to 80. We use the same hyperparameter
α as for the PreActResNet18 model.
We use the CNN with four convolutional layers plus three fully-connected layers from Carlini and
Wagner (2017) for the MNIST robustness experiments. We use the same schedule for the learning
rate, , γI, and γH as in the PreActResNet18 model (except that scales to 0.3). We use the same
hyperparameter α as for the PreActResNet18 model.
To set the hyperparameters γI and γH on the PreActResNet18 model, we ran a grid search for
γI = 0.2, ..., 1.0 and γH /γI = 100, 200, ..., 500 and selected the settings which yielded the best
robust accuracy against a 20-step '∞ PGD adversary with 10 restarts for E = 8/255 on the full
training set; the range of γH /γI was set such that the corresponding losses were roughly equal
for a randomly initialized, untrained network. For reporting results, we train five models using the
selected hyperparameters and report results using the one with the median performance on the test
set against a 20-step '∞ PGD adversary at E = 8/255 on CIFAR-10 or 0.3 on MNIST. For the
PreActResNet18 model, robust accuracy over the 5 runs ranged from 40.1% to 41.5% with median
40.5%; clean accuracy ranged from 66.9% to 72.4% with median 70.0%.
For our stability results, we use the full version of AutoAttack+, an ensemble of attacks proposed
by Croce and Hein (2020), for the '2 and '∞ perturbations. We choose the attack because it is
parameter-free, which reduces the possibility of misconfiguration; empirically it is at least as strong
as standard PGD, and has been successful in breaking many proposed defenses. For the Wasserstein
adversary, we use an implementation by Wong et al. (2019). For comparing with standard PGD, we
use a 20-step PGD adversary with 10 random restarts and a step size of 2.5 ∙ e/20 as implemented
by Engstrom et al. (2019). For verification, we adopt the setup of Xiao et al. (2019), using the MIP
verifier of Tjeng et al. (2017), with solves parallelized over 8 CPU cores and the timeout set to 120
seconds.
15
Under review as a conference paper at ICLR 2021
C Additional experiments
We plot robustness curves for both CIFAR and MNIST against a standard 20-step PGD adversary
with 10 restarts using both '2 and '∞ bounds in Figure 2. These results show that, compared
with models trained using standard PGD against '∞ perturbations, our methods perform on par (or
substantially better, in the case of the `2 adversary on CIFAR-10) in a variety of settings, despite
the disadvantage of not using the adversary during training. We also used a stronger AutoAttack+
adversary for the CIFAR-10 results in Table 1, which we plot for reference as “ours (AA+)”.
For the CIFAR-10 results, we use a PreActResNet18 model trained with manifold regularization on
CIFAR-10 at e = 8/255. The curves for PGD are taken from Madry et al. (2017). Our findings
mirror those in Schott et al. (2018), namely, that a model trained using PGD on CIFAR-10 for '∞
performs poorly against '2 attacks. We also ran a 500-step '∞ PGD adversary with 20 restarts at
e = 8/255 against our model, yielding 40.4% robust accuracy, which is plotted as “ours (PGD+)”.
For the MNIST results, we use the CNN architecture from Carlini and Wagner (2017), trained with
manifold regularization on MNIST at e = 0.3. The curves for PGD are taken from Madry et al.
(2017). The difference in performance is much less dramatic in this case, which we attribute to the
lower complexity of the task. We note in particular that both methods experience a sharp drop in
performance in the '∞ case for e larger than the one used during training.
0 50 50
07 5 2
1
)%( ycarucca tsuboR
Ou	ours
■	Pgd
―o- ours (AA+)
・ ours (PGD+)
0	10	20	30
'∞ bound (e/255)
(a) Robust accuracy on CIFAR-10 against a 20-
step '∞ PGD adversary With 10 restarts.
0 50 5
07 5 2
1
)%( ycarucca tsuboR
01234
`2 bound (e)
(b) Robust accuracy on CIFAR-10 against a 20-
step `2 PGD adversary With 10 restarts.
0 50 5
07 5 2
1
0	0.1	0.2	0.3	0.4
'∞ bound (e)
(c)	Robust accuracy on MNIST against a 20-step
'∞ PGD adversary with 10 restarts.
0246
'2 bound (e)
(d)	Robust accuracy on MNIST against a 20-step
`2 PGD adversary with 10 restarts.
Figure 2: Robust accuracy as e increases for a 20-step PGD adversary with 10 restarts.
16