Under review as a conference paper at ICLR 2021
Unifying Graph Convolutional Neural Net-
works and Label Propagation
Anonymous authors
Paper under double-blind review
Ab stract
Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are
both message passing algorithms on graphs. Both solve the task of node classifi-
cation but LPA propagates node label information across the edges of the graph,
while GCN propagates and transforms node feature information. However, while
conceptually similar, it is unclear how LPA and GCN can be combined under a
unified framework to improve node classification. Here we study the relationship
between LPA and GCN in terms of feature/label influence, in which we char-
acterize how much the initial feature/label of one node influences the final fea-
ture/label of another node in GCN/LPA. Based on our theoretical analysis, we
propose an end-to-end model that combines GCN and LPA. In our unified model,
edge weights are learnable, and the LPA serves as regularization to assist the GCN
in learning proper edge weights that lead to improved classification performance.
Our model can also be seen as learning the weights for edges based on node
labels, which is more task-oriented than existing feature-based attention models
and topology-based diffusion models. In a number of experiments on real-world
graphs, our model shows superiority over state-of-the-art graph neural networks
in terms of node classification accuracy.
1	Introduction
Consider the problem of node classification in a graph, where the goal is to learn a mapping M :
V → L from node set V to label set L. Solution to this problem is widely applicable to various
scenarios, e.g., inferring income of users in a social network or classifying scientific articles in a
citation network. Different from a generic machine learning problem where samples are independent
from each other, nodes are connected by edges in the graph, which provide additional information
and require more delicate modeling. To capture the graph information, researchers have mainly
designed models on the assumption that labels/features are correlated over the edges of the graph.
In particular, on the label side L, node labels are propagated and aggregated along edges in the
graph, which is known as Label Propagation Algorithm (LPA) (Zhu et al., 2005; Zhou et al., 2004;
Zhang & Lee, 2007; Wang & Zhang, 2008; Karasuyama & Mamitsuka, 2013; Gong et al., 2017;
Liu et al., 2019a); On the node side V, node features are propagated along edges and transformed
through neural network layers, which is known as Graph Convolutional Neural Networks (GCN)1
(Kipf & Welling, 2017; Hamilton et al., 2017; Li et al., 2018; Xu et al., 2018; Liao et al., 2019; Xu
et al., 2019b; Qu et al., 2019).
GCN and LPA are related in that they propagate features and labels on the two sides of the mapping
M, respectively. Prior work Li et al. (2019) has shown the relationship between GCN and LPA in
terms of low-pass graph filtering. However, it is unclear how the discovered relationship benefits
node classification. Specifically, can GCN and LPA be combined to develop a more accurate model
for node classification in graphs?
Here we study the theoretical relationship between GCN and LPA from the viewpoint of fea-
ture/label influence, where we quantify how much the initial feature/label of node vb influences
1There are methods in statistical relational learning Rossi et al. (2012) also using feature propaga-
tion/diffusion techniques. In this work, we focus on GCN, but the analysis and the proposed model can be
easily generalized to other feature diffusion methods.
1
Under review as a conference paper at ICLR 2021
the output feature/label of node va in GCN/LPA by studying the Jacobian/gradient of node vb with
respect to node va . We also prove the quantitative relationship between feature influence and label
influence, i.e., the label influence of vb on va equals the cumulative discounted feature influence of
vb on va in expectation (Theorem 1).
Based on the theoretical analysis, we propose a unified model GCN-LPA for node classification.
We show that the key to improving the performance of GCN is to enable nodes of the same class
to connect more strongly with each other by making edge weights/strengths trainable. Then we
prove that increasing the strength of edges between the nodes of the same class is equivalent to
increasing the accuracy of LPA’s predictions (Theorem 2). Therefore, we can first learn the optimal
edge weights by minimizing the loss of predictions in LPA, then plug the optimal edge weights into a
GCN to learn node representations. In GCN-LPA, we further combine the above two steps together
and train the whole model in an end-to-end fashion, where the LPA part serves as regularization
to assist the GCN part in learning proper edge weights that benefit the separation of different node
classes. It is worth noticing that GCN-LPA can also be seen as learning the weights for edges based
on node label information, which requires less handcrafting and is more task-oriented than existing
attention models that learn edge weights based on node feature similarity (VelickoVic et al., 2018;
Thekumparampil et al., 2018; Zhang et al., 2018; Liu et al., 2019b) or diffusion models that learn
adjacency matrix based on graph topology (Klicpera et al., 2019a; Xu et al., 2019a; Abu-El-Haija
et al., 2019; Klicpera et al., 2019b).
We conduct extensiVe experiments on fiVe datasets, and the results indicate that our model outper-
forms state-of-the-art graph neural networks in terms of classification accuracy. The experimental
results also show that combining GCN and LPA together is able to learn more informatiVe edge
weights thereby leading to better performance.
2	Our Approach
In this section, we first formulate the node classification problem and briefly introduce LPA and
GCN. We then proVe their relationship from the Viewpoints of feature influence and label influence.
Based on the theoretical finding, we propose a unified model GCN-LPA, and analyze why our model
is theoretically superior to Vanilla GCN.
2.1	Problem Formulation and Preliminaries
Consider a graph G = (V, A, X, Y), where V = {vι,…,vn} is the set of nodes, A ∈ Rn×n is the
adjacency matrix, X is the feature matrix of nodes and Y is labels of nodes. aij (the ij -th entry of
A) is the weight of the edge connecting vi and vj . N (v ) denotes the set of first-order neighbors of
node v in graph G . Each node vi has a feature Vector xi which is the i-th row of X , while only the
firstm nodes (m《n) have labels yι,…，ym from a label set L = {1,…，c}. The goal is to learn
a mapping M : V → L and predict labels of unlabeled nodes.
Label Propagation Algorithm. LPA (Zhu et al., 2005) assumes that two connected nodes are
likely to have the same label, and thus it propagates labels iteratively along the edges. Let Y(k) =
[y(k),…，"户]> ∈ Rn×c be the soft label matrix in iteration k > 0, in which the i-th row y(k)>
denotes the predicted label distribution for node vi in iteration k. When k = 0, the initial label
matrix Y⑼ =[y(O),…，y?O)]> consists of one-hot label indicator vectors y(O) for i = 1,…，m
(i.e., labeled nodes) or zero vectors otherwise (i.e., unlabeled nodes). Then LPA in iteration k is
formulated as the following two steps:
Y(k+1) = A γ(k),
(k+1)	(O)
yi	= yi	, ∀ i ≤ m
(1)
(2)
In the above equations, A is the normalized adjacency matrix, which can be the random walk tran-
sition matrix Arw = D-1A or the symmetric transition matrix Asym = D-2 AD- 11, where D is
the diagonal degree matrix for A with entries dii = Pj aij . Without loss of generosity, we use
A = Arw in this work. In Eq. (1), all nodes propagate labels to their neighbors according to nor-
malized edge weights. Then in Eq. (2), labels of all labeled nodes are reset to their initial values,
2
Under review as a conference paper at ICLR 2021
because LPA wants to persist labels of nodes which are labeled, so that unlabeled nodes do not
overpower the labeled ones as the initial labels would otherwise fade away.
Graph Convolutional Neural Networks. GCN Kipf & Welling (2017) is a multi-layer feedfor-
ward neural network that propagates and transforms node features across the graph. The feature
propagation scheme of GCN in layer k is:
X(k+1) = σ (AX(k)W(k)),	(3)
where W(k) is trainable weight matrix in the k-th layer, σ(∙) is an activation function, and
X(k) = [x1k),…,Xnk)]> arethe k-th layer node representations with X (0) = X . By setting the
dimension of the last layer to the number of classes c, the last layer can be seen as (unnormalized)
label distribution predicted for a given node. The whole model can thus be optimized by minimizing
the discrepancy between predicted node label distributions and ground-truth labels Y .
2.2	Feature Influence and Label Influence
Consider two nodes va and vb in a graph. Inspired by Koh & Liang (2017) and Xu et al. (2018), we
study the relationship between GCN and LPA in terms of influence, i.e., how the output feature/label
of va will change if the initial feature/label of vb is varied slightly. Technically, the feature/label
influence is measured by the Jacobian/gradient of the output feature/label of va with respect to the
(k)
initial feature/label of vb. Denote xa as the k-th layer representation vector of va in GCN, and xb
as the initial feature vector of vb. We quantify the feature influence ofvb on va as follows:
Definition 1 (Feature influence) The feature influence of node vb on node va after k layers of GCN
is the L1-norm ofthe expected Jacobian matrix ∂Xak)/∂xb: If (vα, Vb； k) = ∣∣E[∂Xak)/∂xj ∖. The
normalized feature influence is then defined as If(va, vb; k) = If(va, vb; k)/ v ∈V If(va, vi; k).
We also consider the label influence of node vb on node va in LPA (this implies that va is unlabeled
and Vb is labeled). Since different label dimensions of y(∙ do not interact with each other in LPA,
we assume that all yi and y(∙ are scalars within [0,1] (i.e., this is a binary classification task) for
simplicity. Label influence is defined as follows:
Definition 2 (Label influence) The label influence of labeled node Vb on unlabeled node Va after k
iterations of LPA is the gradient of ya(k) with respect to yb: Il (Va, Vb; k) = ∂ya(k) /∂yb.
The following theorem shows the relationship between feature influence and label influence:
Theorem 1 (Relationship between feature influence and label influence) Assume the activation
function used in GCN is ReLU. Denote Va as an unlabeled node, Vb as a labeled node, and β as the
fraction of unlabeled nodes. Then the label influence ofVb on Va after k iterations of LPA equals, in
expectation, to the cumulative normalized feature influence ofVb on Va after k layers of GCN:
E[Il(va, vb； k)] = Xk=I βjIf (va,vb； j).	⑷
Proof of Theorem 1 is in Appendix A. Intuitively, Theorem 1 shows that if Vb has high label influence
on Va, then the initial feature vector of Vb will also affect the output feature vector of Va greatly.
Theorem 1 provides the theoretical guideline for designing our unified model in the next subsection.
2.3	The Unified Model
Before introducing the proposed model, we rethink the GCN method and see what an ideal set of
node representations should be like. Since we aim to classify nodes, the perfect node representation
would be such that nodes with the same label are embedded closely together, which would give a
large separation between different classes. Intuitively, the key to achieve this goal is to enable nodes
within the same class to connect more strongly with each other, so that they are pushed together by
GCN (more discussion is presented in Section 2.4). We can therefore make edge strengths/weights
3
Under review as a conference paper at ICLR 2021
trainable, then learn to increase the intra-class feature influence: i∈L v ,v :y =i,y =i If (va, vb)
(L is the label set), by adjusting edge weights. However, this requires operating on Jacobian ma-
trices with the size of d(0) × d(K) (d(0) and d(K) are the dimensions of input and output in GCN,
respectively), which is impractical if initial node features are high-dimensional. Fortunately, we
can turn to optimizing the intra-class label influence instead, i.e., i∈L v ,v :y =i,y =i Il (va, vb),
according to Theorem 1. Note that Pi∈L Pva,vb:ya=i,yb=i Il(va, vb) = Pva Pvb:yb=yaIl(va,vb).
We further show, by the following theorem, that the term Pv :y =y Il (va, vb) (the total intra-class
label influence on a given node va) is proportional to the probability that va is classified correctly
by LPA:
Theorem 2 (Relationship between label influence and LPA’s prediction) Consider a given node
va and its label ya. If we treat node va as unlabeled, then the total label influence of nodes with
label ya on node va is proportional to the probability that node va is classified as ya by LPA:
X Il(Va,Vb； k) <x Pr(ylpa = y。)，	(5)
vb:yb=ya
where ylfa is the predicted label of Va using a k-iteration LPA.
Proof of Theorem 2 is in Appendix B. Theorem 2 indicates that, if edge weights {aij } maximize
the probability that Va is correctly classified by LPA, then they also maximize the intra-class label
influence on node v。. We can therefore first learn the optimal edge weights A* by minimizing the
loss of predicted labels by LPA:2
A*
arg min Llpa (A)
A
argAmin m X J(yapa,ya),
Va :agm
(6)
where J is the cross-entropy loss, y俨 and y。are the predicted label distribution of Va using LPA and
the true one-hot label of Va, respectively. a ≤ m means Va is labeled. The optimal A* maximizes
the probability that each node is correctly labeled by LPA, thus also maximizes the intra-class label
influence (according to Theorem 2) and intra-class feature influence (according to Theorem 1). Since
A* increases the connection strength within each class, it is expected to improve the performance
of GCN compared with the original adjacency matrix A. Therefore, we can plug A* into GCN to
predict labels:
X(k+1) = σ(A*X(k)W(k)), k = 0,1,…，K - 1.	(7)
We use yacn, the a-th row of X(K), to denote the predicted label distribution of Va using the GCN
specified in Eq. (7). Then the optimal transformation matrices in the GCN can be learned by
minimizing the loss of predicted labels by GCN:
W*
arg min Lgcn(W, A*)
W
1
arg mm —
Wm
E J(yacn,ya),
Va :a：m
(8)
It is more elegant (and empirically better) to combine the above two steps together into a multi-
objective optimization problem, and train the whole model in an end-to-end fashion:
W*,A* =arg min Lgcn(W, A) + λLlpa(A),	(9)
W,A
where λ is the balancing hyper-parameter. In this way, Llpa(A) serves as a regularization term that
assists the learning of edge weights A, since it is hard for GCN to learn both W andA simultaneously
due to overfitting. The proposed GCN-LPA approach can also be seen as learning the importance of
edges that can be used to reconstruct node labels accurately by LPA, then transferring this knowledge
from label space to feature space for GCN.
It is also worth noticing how the optimal A* is configured. The principle here is that we do not mod-
ify the basic structure of the original graph (i.e., not adding or removing edges) but only adjusting
weights of existing edges. This is equivalent to learning a positive mask matrix M for the adjacency
matrix A and taking the Hadamard product M ◦ A = A* . Each element Mij can be set as either a
free variable or a function of the two nodes, for example, Mij = log exp(xi> Hxj) + 1 where H
is a learnable kernel matrix for measuring feature similarity.
2Here the optimal edge weights A* share the same topology as the original graph G, i.e., We do not add or
remove edges from G but only learning the weights of existing edges. See the end of this subsection for more
discussion.
4
Under review as a conference paper at ICLR 2021
2.4	Analysis of GCN-LPA Model Behavior
In this subsection, we show benefits of our unified model compared with GCN by analyzing proper-
ties of embeddings produced by the two models. We first analyze the update rule of GCN for node
Vi： x(k+1) = σ (PvjeN(Vi) &ijxjk)W(k)), where Aij = aij /dii is the normalized weight of edge
(j, i). This formula can be decomposed into the following two steps： (1) In aggregation step, we cal-
culate the aggregated representation h(k) of all neighborhoods N(vi): h(k) = Pv.∈n(VW)ajXjk).
(2) In transformation step, the aggregated representation hi(k) is mapped to anew space by a transfor-
mation matrix and nonlinear function: x(k+1) = σ(h(k)W(k)). We show by the following theorem
that the aggregation step reduces the overall distance in the embedding space between the nodes that
are connected in the graph:
Theorem 3 (Shrinking property in GCN) Let D(x) = 11 PVav Gij ∣∣Xi -Xjk2 beadistanCemet-
ric over node embeddings x. Then we have D(h(k)) ≤ D(x(k)).
Proof of Theorem 3 is in Appendix C. Theo-
rem 3 indicates that the overall distance among
connected nodes is reduced after taking one ag-
gregation step, which implies that connected
components in the graph “shrink” and nodes
within each connected component get closer
to each other in the embedding space. In an
ideal case where edges only connect nodes with
the same label, the aggregation step will push
nodes within the same class together, which
greatly benefits the transformation step that acts
like using a hyperplane W(k) for classification.
However, two connected nodes may have dif-
ferent labels. These “noisy” edges will impede
the formation of clusters and make the inter-
class boundary less clear.
(a) A graph with two (b) Potential intra-class
classes of nodes	edges (bold links)
Figure 1: A graph with two classes of nodes,
while white nodes are unlabeled (Figure 1a). To
classify nodes, our model will increase the con-
necting strength among nodes within the same
class, thereby increasing their feature/label influ-
ence on each other. In this way, our model is able
to identify potential intra-class edges (bold links
in Figure 1b) and strengthen their weights.
Fortunately, in GCN-LPA, edge weights are
learned by minimizing the difference between
ground-truth labels and labels reconstructed
from local neighbors. This will force the model to increase the weight/bandwidth of possible paths
that connect nodes with the same label, so that labels can “flow” easily along these paths for the
purpose of label reconstruction. In this way, GCN-LPA is able to identify potential intra-class edges
and increase their weights to assist learning clustering structures ( see Figure 1 for an illustrating
example).
To empirically justify our claim, we apply a two-layer untrained GCN with randomly initialized
transformation matrices to the well-known Zachary’s karate club network (Zachary, 1977) as shown
in Figure 2a, which contains 34 nodes of 2 classes and 78 unweighted edges (grey solid lines). We
then increase the weights of intra-class edges by ten times to simulate GCN-LPA. We find that GCN
works well on this network (Figure 2b), but GCN-LPA performs even better than GCN because
the node embeddings are completely linearly separable as shown in Figure 2c. To further justify
our claim, we randomly add 20 “noisy” inter-class edges (grey dotted lines) to the original network,
from which we observe that GCN is misled by noise and mixes nodes of two classes together (Figure
2d), but GCN-LPA still distinguishes the two clusters (Figure 2e) because it is better at “denoising”
undesirable edges based on the supervised signal of labels.
3	Connection to Existing Work
Edge weights play a key role in graph-based machine learning algorithms. In this section, we discuss
three lines of related work that learn edge weights adaptively.
5
Under review as a conference paper at ICLR 2021
(a) Karate club network (b) GCN on the (c) GCN-LPA on the
with noisy edges original network original network
(d) GCN on the
noisy network
(e) GCN-LPA on the
noisy network
Figure 2: Node embeddings of Zachary’s karate club network trained on a node classification task
(red vs. blue). Figure 2a visualizes the graph. Node coordinates in Figure 2b-2e are the embedding
coordinates. Notice that GCN does not produce linearly separable embeddings (Figure 2b vs. Figure
2c), while GCN-LPA performs much better even in the presence of noisy edges (Figure 2d vs. Figure
2e). Additional visualizations are included in Appendix D.
Locally Linear Embedding. Locally linear embedding (LLE) (Roweis & Saul, 2000) and its vari-
ants (Zhang & Wang, 2007; Kong et al., 2012) learn edge weights by constructing a linear de-
pendency between a node and its neighbors, then use the learned edge weights to embed high-
dimensional nodes into a low-dimensional space. Our work is similar to LLE in the aspect of trans-
ferring the knowledge of edge importance from one space to another, but the difference is that LLE
is an unsupervised dimension reduction method that learns the graph structure based on local prox-
imity only, while our work is semi-supervised and explores high-order relationship among nodes.
Label Propagation Algorithm. Classical LPA (Zhu et al., 2005; Zhou et al., 2004) can only make
use of node labels rather than node features. In contrast, adaptive LPA considers node features
by making edge weights learnable. Typical techniques of learning edge weights include adopting
kernel functions (ZhU et al., 2003; LiU et al., 2019a) (e.g., aj = exp(-Pd(Xid - Xjd)2∕σ2)
where d is dimensionality of features), minimizing neighborhood reconstruction error (Wang &
Zhang, 2008; KarasUyama & MamitsUka, 2013), Using leave-one-oUt loss (Zhang & Lee, 2007), or
imposing sparseness on edge weights (Hong et al., 2009). However, in these LPA variants, node
featUres are only Used to assist learning the graph strUctUre rather than explicitly mapped to node
labels, which limits their capability in node classification. Another notable difference is that adaptive
LPA learns edge weights by introdUcing the regUlarizations above, while oUr work takes LPA itself
as regUlarization to learn edge weights.
Attention and Diffusion on Graphs. OUr method is also conceptUally connected to attention mech-
anism on graphs, in which an attention weight αij is learned between node vi and vj . For example,
aj = LeakyReLU(a> [WXi∣∣WXj]) in GAT (VelickoVic et al., 2018), αj∙ = a ∙ cos(WXi, WXj)
in AGNN (ThekUmparampil et al., 2018), αij = (W1xi)>W2xj in GaAN (Zhang et al., 2018),
and αij = a> tanh(W1Xi + W2Xj) in GeniePath (LiU et al., 2019b), where a and W are trainable
Variables. OUr method is also similar to diffUsion-based methods (Klicpera et al., 2019a; XU et al.,
2019a; AbU-El-Haija et al., 2019; Klicpera et al., 2019b; Jiang et al., 2019; Yang et al., 2019). Graph
diffUsion Uses extended neighborhoods for aggregation in GNNs, which can be seen as learning a
new adjacency matrix for a giVen graph. A significant difference between attention/diffUsion mech-
anisms and oUr work is that attention/diffUsion is learned based on featUre similarity/graph topology,
while we propose that edge weights shoUld be consistent with the distribUtion of labels on the graph,
which reqUires less handcrafting of the attention/diffUsion fUnction and is more task-oriented.
4	Experiments
4.1	Experiment Setup
Datasets. We Use the following fiVe datasets in oUr experiments. Cora, Citeseer, and Pubmed
(Sen et al., 2008) are citation networks, where nodes correspond to docUments, edges correspond
to citation links, and each node has a sparse bag-of-words featUre Vector as well as a class label.
We also Use two co-aUthorship networks (ShchUr et al., 2018), Coauthor-CS and Coauthor-Phy,
where nodes are aUthors and an edge indicates that two aUthors co-aUthored a paper. Node featUres
represent paper keywords for each aUthor’s papers, and class labels indicate most actiVe fields of
6
Under review as a conference paper at ICLR 2021
Method	Cora	Citeseer	Pubmed	Coauthor-CS	Coauthor-Phy
LR	57.1 ± 2.3	61.0 ± 2.2	64.1 ± 3.1	86.4 ± 0.9	86.7 ± 1.5
LPA	74.4 ± 2.6	67.8 ± 2.1	70.5 ± 5.3	73.6 ± 3.9	86.6 ± 2.0
GCN	81.4 ± 1.3	71.9 ± 1.9	77.5 ± 2.9	91.1 ± 0.5	92.4 ± 1.0
GAT	80.7 ± 1.3	71.4 ± 1.9	76.7 ± 2.3	90.5 ± 0.6	92.2 ± 0.9
JK-Net	81.3 ± 1.4	70.2 ± 1.3	77.6 ± 0.9	90.3 ± 0.4	91.0 ± 0.7
GIN	74.5 ± 1.5	60.7 ± 1.3	73.4 ± 1.2	84.1 ± 1.9	87.3 ± 1.7
GDC	83.2 ± 0.9	72.2 ± 1.4	77.8 ± 0.8	91.4 ± 1.0	92.0 ± 0.7
GCN+LPA	78.4 ± 0.7	69.8 ± 1.4	74.1 ± 0.9	84.5 ± 1.0	89.7 ± 0.8
GCN-LPA	83.0 ± 1.4 -	72.6 ± 0.9	78.4 ± 1.5 一	91.9 ± 0.9	93.4 ± 1.6
Table 1: Mean and the 95% confidence intervals of test set accuracy for all methods and datasets.
Figure 3: Sensitivity to the
number of LPA iterations on
Citeseer dataset.
Figure 4: Sensitivity to λ
(weight of LPA loss) on Cite-
seer dataset.
m∙∙-1
lira
(IPod88E6u-uu
IOK IOOK IM
# nodes
Figure 5: Training time Per
epoch on random graphs.
study for each author. Statistics of the five datasets are shown in Appendix E. We also calculate
the intra-class edge rate (the fraction of edges that connect two nodes within the same class), which
is significantly higher than inter-class edge rate in all networks. The finding supports our claim in
Section 2.4 that node classification benefits from intra-class edges in a graph.
Baselines. We compare against the following baselines in our experiments. Logistic Regression
(LR) is feature-based methods that do not consider the graph structure. Label Propagation (LPA)
(ZhU et al., 2005), on the other hand, only consider the graph structure and ignore node features.
We also compare with several GNNs: Graph Convolutional Network (GCN)(KiPf & Welling,
2017), Graph Attention Network (GAT), Jumping Knowledge Network (JK-Net)(XU et al.,
2018), Graph Isomorphism Network (GIN) (Xu et al., 2019b), and Graph Diffusion Convolution
(GDC) (Klicpera et al., 2019b) (with GCN as the base model). In addition, we propose another
baseline GCN+LPA, which simply adds predictions of GCN and LPA together.
Experimental Setup. Our experiments focus on the transductive setting where we only know labels
of part of nodes but have access to the entire graph as well as features of all nodes.3 We randomly
sample 20 nodes per class as training set, 50 nodes per class as validation set, and the remaining
nodes as test set. The weight of each edge is treated as a free variable during training. We train
our model for 200 epochs using Adam (Kingma & Ba, 2015) and report the test set accuracy when
validation set accuracy is maximized. Each experiment is repeated five times and we report the
mean and the 95% confidence interval. We initialize weights according to Glorot & Bengio (2010)
and row-normalize input features. During training, we apply L2 regularization to the transformation
matrices and use the dropout technique (Srivastava et al., 2014). The settings of all other hyper-
parameters can be found in Appendix F.
4.2	Results
Comparison with Baselines. The results of node classification are summarized in Table 1. Table
1 indicates that only using node features (LR) or graph structure (LPA) will lead to information
loss and cannot fully exploit datasets. The results demonstrate that our proposed GCN-LPA model
surpasses state-of-the-art GNN baselines. We notice that GDC is a strong baseline on Cora, but it
3Our method can be easily generalized to inductive setting if implemented using minibatch training like
GraphSAGE (Hamilton et al., 2017).
7
Under review as a conference paper at ICLR 2021
Labeled node rate	5%	10%	20%	50%	80%
LPA	67.9 ± 2.1	68.1 ± 1.3	70.5 ± 1.5	72.5 ± 1.2	76.4 ± 1.1
GCN	72.1 ± 1.9	72.5 ± 1.8	74.3 ± 0.9	76.8 ± 0.6	80.2 ± 1.5
GCN-LPA	72.7 ± 1.2	73.2 ± 1.1	75.4 ± 1.5	78.2 ± 1.3	82.3 ± 0.9
Table 2: Accuracy of LPA, GCN, and GCN-LPA on Citeseer with different labeled node rate.
does not perform consistently well on other datasets. In addition, GCN+LPA does not perform well,
since it utilizes the prediction of LPA directly, making its performance limited by LPA.
Efficacy of LPA Regularization. We investigate the influence of the number of LPA iterations and
the training weight of LPA loss term λ on the performance of classification. The results on Citeseer
dataset are plotted in Figures 3 and 4, respectively, where each line corresponds to a given number of
GCN layers in GCN-LPA. From Figure 3 we observe that the performance is boosted at first when
the number of LPA iterations increases, then the accuracy stops increasing and decreases since a
large number of LPA iterations will include more noisy nodes. Figure 4 shows that training without
the LPA loss term (i.e., λ = 0) is more difficult than the case where λ = 1 〜5, which justifies our
aforementioned claim that it is hard for the GCN part to learn both transformation matrices W and
edge weights A simultaneously without the assistance of LPA regularization.
Influence of Labeled Node Rate. To study the influence of labeled node rate on the performance
of our model, we vary the ratio of labeled node rate on Citeseer from 5% to 80% while keeping
the validation and test set fixed, and report the result in Table 2. From Table 2 we observe that
GCN-LPA outperforms GCN and LPA consistently, and the improvement achieved by GCN-LPA
increases when labeled node rate is larger (from 0.6% to 2.1% compared with GCN). This is because
GCN-LPA requires node labels to calculate edge weights. Therefore, a larger labeled node rate will
provide more information for identifying noisy edges.
Visualization of Learned Edge Weights. To intuitively understand
what our model learns about edge weights, we split nodes in Coauthor-
CS dataset into 15 groups according to their labels, and calculate the
average weights of edges connecting every pair of node groups as well
as the average weights of edges within every group. The results are
shown in Figure 6, where darker color indicates higher average weights
of edges. It is clear that values along the diagonal (intra-class edges
weights) are significantly larger than off-diagonal values (inter-class
edge weights) in general, which demonstrates that GCN-LPA is able to
identify the importance of edges and distinguish inter-class and intra-
class edges. The visualization results are similar for other datasets.
Time Complexity. We study the training time of GCN-LPA on random
graphs. We use the one-hot identity vector as feature and 0 as label for
each node. The size of training set and validation set is 100 and 200,
Figure 6: Visualization
of learned edge weights
in Coauthor-CS dataset.
respectively, while the rest is test set. The average number of neighbors for each node is set as 5,
and the number of nodes is varied from one thousand to one million. We run GCN-LPA and GCN
for 100 epochs on a Microsoft Azure virtual machine with 1 NVIDIA Tesla M60 GPU, 12 Intel
Xeon CPUs (E5-2690 v3 @2.60GHz), and 128GB of RAM, using the same hyper-parameter setting
as in Cora. The training time per epoch of GCN-LPA and GCN is presented in Figure 5. Our result
shows that GCN-LPA requires only 9.2% extra training time on average compared to GCN.
5	Conclusion
We studies the theoretical relationship between two types of well-known graph-based algorithms
for node classification, label propagation algorithm and graph convolutional neural networks, from
the perspectives of feature/label influence. We then propose a unified model GCN-LPA, which
learns transformation matrices and edge weights simultaneously in GCN with the assistance of LPA
regularizer. We also analyze why our unified model performs better than traditional GCN in terms
of node classification. Experiments on five datasets demonstrate that our model outperforms state-
of-the-art baselines, and it is also highly time-efficient with respect to the size of a graph.
8
Under review as a conference paper at ICLR 2021
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional
architectures via sparsified neighborhood mixing. In Proceedings of the 36th International Con-
ference on Machine Learning, pp. 21-29, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the 13th International Conference on Artificial Intelligence and
Statistics, 2010.
Chen Gong, Dacheng Tao, Wei Liu, Liu Liu, and Jie Yang. Label propagation via teaching-to-
learn and learning-to-teach. IEEE Transactions on Neural Networks and Learning Lystems, 28
(6), 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in Neural Information Processing Systems, 2017.
Cheng Hong, Zicheng Liu, and Jie Yang. Sparsity induced similarity measure for label propagation.
In Proceedings of the 12th IEEE International Conference on Computer Vision. IEEE, 2009.
Bo Jiang, Ziyan Zhang, Doudou Lin, Jin Tang, and Bin Luo. Semi-supervised learning with graph
learning-convolutional networks. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 11313-11320, 2019.
Masayuki Karasuyama and Hiroshi Mamitsuka. Manifold-based similarity adaptation for label prop-
agation. In Advances in Neural Information Processing Systems, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of
the 3rd International Conference on Learning Representations, 2015.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In Proceedings of the 5th International Conference on Learning Representations, 2017.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. In Proceedings of the 7th International
Conference on Learning Representations, 2019a.
Johannes Klicpera, Stefan WeiBenberger, and Stephan Gunnemann. Diffusion improves graph learn-
ing. In Advances in Neural Information Processing Systems, pp. 13354-13366, 2019b.
Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence functions. In
Proceedings of the 34th International Conference on Machine Learning, 2017.
Deguang Kong, Chris Ding, Heng Huang, and Feiping Nie. An iterative locally linear embedding
algorithm. In Proceedings of the 29th International Coference on International Conference on
Machine Learning. Omnipress, 2012.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In The 32nd AAAI Conference on Artificial Intelligence, 2018.
Qimai Li, Xiao-Ming Wu, Han Liu, Xiaotong Zhang, and Zhichao Guan. Label efficient semi-
supervised learning via graph filtering. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 9582-9591, 2019.
Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S Zemel. Lanczosnet: Multi-scale deep
graph convolutional networks. In Proceedings of the 7th International Conference on Learning
Representations, 2019.
Yanbin Liu, Juho Lee, Minseop Park, Saehoon Kim, Eunho Yang, Sung Ju Hwang, and Yi Yang.
Learning to propagate labels: Transductive propagation network for few-shot learning. In Pro-
ceedings of the 7th International Conference on Learning Representations, 2019a.
9
Under review as a conference paper at ICLR 2021
Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath:
Graph neural networks with adaptive receptive paths. In The 33rd AAAI Conference on Artificial
Intelligence, 2019b.
Meng Qu, Yoshua Bengio, and Jian Tang. Gmnn: Graph markov neural networks. In Proceedings
of the 36th International Conference on Machine Learning, 2019.
Ryan A Rossi, Luke K McDowell, David William Aha, and Jennifer Neville. Transforming graph
data for statistical relational learning. Journal of Artificial Intelligence Research, 45:363—441,
2012.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. science, 290(5500), 2000.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3), 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls
of graph neural network evaluation. In Neural Information Processing Systems Workshop on
Relational Representation Learning, 2018.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1), 2014.
Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural
network for semi-supervised learning. arXiv preprint arXiv:1803.03735, 2018.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In Proceedings of the 6th International Conference on Learn-
ing Representations, 2018.
Fei Wang and Changshui Zhang. Label propagation through linear neighborhoods. IEEE Transac-
tions on Knowledge and Data Engineering, 20(1), 2008.
Bingbing Xu, Huawei Shen, Qi Cao, Keting Cen, and Xueqi Cheng. Graph convolutional networks
using heat kernel for semi-supervised learning. In Proceedings of the 28th International Joint
Conference on Artificial Intelligence ,pp.1928-1934. AAAI Press, 2019a.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jumping knowledge networks. In Proceedings
of the 35th International Conference on Machine Learning, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In Proceedings of the 7th International Conference on Learning Representations,
2019b.
Liang Yang, Zesheng Kang, Xiaochun Cao, Di Jin, Bo Yang, and Yuanfang Guo. Topology opti-
mization based graph convolutional network. In IJCAI, pp. 4054-4061, 2019.
Wayne W Zachary. An information flow model for conflict and fission in small groups. Journal of
anthropological research, 33(4), 1977.
Jiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung. Gaan:
Gated attention networks for learning on large and spatiotemporal graphs. arXiv preprint
arXiv:1803.07294, 2018.
Xinhua Zhang and Wee S Lee. Hyperparameter learning for graph based semi-supervised learning
algorithms. In Advances in Neural Information Processing Systems, 2007.
Zhenyue Zhang and Jing Wang. Mlle: Modified locally linear embedding using multiple weights.
In Advances in Neural Information Processing Systems, 2007.
10
Under review as a conference paper at ICLR 2021
Dengyong Zhou, Olivier Bousquet, Thomas N Lal, Jason Weston, and Bernhard Scholkopf. Learn-
ing with local and global consistency. In Advances in Neural Information Processing Systems,
2004.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International Conference on Machine
Learning, 2003.
Xiaojin Zhu, John Lafferty, and Ronald Rosenfeld. Semi-supervised learning with graphs. PhD
thesis, Carnegie Mellon University, school of language technologies institute, 2005.
11
Under review as a conference paper at ICLR 2021
Appendix
A Proof of Theorem 1
Before proving Theorem 1, we first give two lemmas that demonstrate the exact form of feature
influence and label influence defined in this paper. The relationship between feature influence and
label influence can then be deduced from their exact forms.
Lemma 1 Assume that the nonlinear activation function in GCN is ReLU. Let Pka→b be a path
[v(k), v(k-1), ∙∙∙ , v(0)] of length k from node Va to node Vb, where v(k) = Va, v(0) = Vb, and
VUT) ∈ N(Vs)) for i = k, ∙∙∙ , L Then we have
1
If(Va,Vb ； k)= ∑πav(i-1) ,v(i),	(10)
Pa→b i=k
where 篇(一)〃/)is the normalized weight of edge(V(i),V(iT)).
Proof. See Xu et al. (2018) for the detailed proof.
The product term in Eq. (10) is the probability of a given path Pka→b. Therefore, the right hand side
in Eq. (10) is the sum over probabilities of all possible paths of length k from Va to Vb , which is the
probability that a random walk starting at Va ends at Vb after taking k steps.
Lemma 2 Let Uja→b be a path [V(j),V(j-1),…，v(0)] of length j from node Va to node Vb, where
V(j) = Va, V(O) = Vb, VHT) ∈ N(VW)) for i = j, ∙∙∙ , 1, and all nodes along the path are unlabeled
except V(0). Then we have
k1
Il(Va,Vb； k) =E E ɪɪðv(i-l),v(i),	(II)
j =1 Uja→b i=j
where》■(—)〃/)is the normalized weight of edge (V(i),V(iT)).
To intuitively understand this lemma, note that there are two differences between Lemma 1 and
Lemma 2: (1) In Lemma 1, If (Va, Vb; k) sums over all paths from Va to Vb of length k, but in
Lemma 2, Il(Va, Vb; k) sums over all paths from Va to Vb of length no more than k. The is because in
LPA, Vb’s label is reset to its initial value after each iteration, which means that the label of Vb serves
as a constant signal that begins propagating in the graph again and again after each iteration. (2) In
Lemma 1 we consider all possible paths from Va to Vb, but in Lemma 2, the paths are restricted to
contain unlabeled nodes only. The reason here is the same as above: Since the labels of labeled nodes
are reset to their initial values after each iteration in LPA, the influence of Vb ’s label will be absorbed
in labeled nodes, and the propagation of Vb’s label will be cut off at these nodes. Therefore, Vb’s
label can only flow to Va along the paths with unlabeled nodes only. See Figure 7 for an illustrating
example showing the label propagation in LPA.
Proof. As mentioned above, a significant difference between LPA and GCN is that all labeled
nodes are reset to its original labels after each iteration in LPA. This implies that the initial label
yb of node Vb appears not only as y(0), but also as every y(j) for j = 1,…，k - 1. Therefore, the
influence of yb on y^ is the cumulative influence of y(j) on y^ for j = 0,1,…，k — 1:
Il(Va,Vb; k)
Q = X Q
dyb _ j=为"
(12)
According to the updating rule of LPA, we have
∂y(kk _ d Pvz ∈N(Va) aazyzk 1) _ X 〜∂y(kT)
而=	破	=Vz ∈NVa产▽
(13)
12
Under review as a conference paper at ICLR 2021
(a) Iteration 1
(b) Iteration 2
(c) Iteration 3
(d) Paths from va to
vb
Figure 7: An illustrating example of label propagation in LPA. Suppose labels are propagated for
three iterations, and no self-loop exists. Blue nodes are labeled while white nodes are unlabeled. (a)
va’s label propagates to v1 (yellow arrows). Note that the propagation of va’s label to v3 is cut off
since v3 is labeled thus absorbing va ’s label. (b) va ’s label that propagated to v1 further propagates
to v2 and vb (yellow arrows). Meanwhile, va’s label is reset to its initial value then propagates from
va again (green arrows). (c) Label propagation in iteration 3. Purple arrows denote the propagation
of va ’s label starting from va for the third time. (d) All possible paths of length no more than three
from va to vb containing unlabeled nodes only. Note that there is no path of length one from va to
vb.
In the above equation, the derivative
∂ (k-1)
is decomposed into the weighted average of Oy ⑴
∂yb
where vz traverses all neighbors of va. For those vz’s that are initially labeled, yz(k-1) is reset to
their initial labels in each iteration. Therefore, they are always constant and independent of yb(j),
meaning that their derivatives w.r.t. yb(j) are zero. So we only need to consider the terms where vz is
an unlabeled node:
∂yak) _ X ~ ∂yZk-1)
~j~= =	aaz A (j),
∂yb	vz∈N(va),z>m	∂yb
(14)
where z > m means vz is unlabeled. To intuitively understand Eq. (14), one can imagine that we
perform a random walk starting from node va for one step, where the “transition probability” is the
edge weights a, and all nodes in this random walk are restricted to unlabeled nodes only. Note that
we can further decompose every yz(k-1) in Eq. (14) in the way similar to what we do for ya(k) in
Eq. (13). So the expansion in Eq. (14) can be performed iteratively until the index k decreases to j.
This is equivalent to performing all possible random walks for k - j steps starting from va, where
all nodes but the last in the random walk are restricted to be unlabeled nodes:
Q
∂y(j)
EEl ∏	""I
vz ∈V Uka-→jz	i=k-j
∂yZj)
∂yj)，
(15)
where vz in the first summation term is the end node of a random walk, Uka-→jz in the second summa-
tion term is an unlabeled-nodes-only path from va to vz of length k - j , and the product term is the
∂ (j)	∂ (j)
probability of a given path Ua→z. Consider the last term ^zy in Eq. (15). We know that ≤¾∙ = 0
k-j	∂yb(j)	∂yb(j)
for all z = b and dyj) = 1 for Z = b, which means that only those random-walk paths that end
∂yb
exactly at vb (i.e., the end node vz is exactly vb) count for the computation in Eq. (15). Therefore,
we have
∂y(j)
1
E ∏	av(i-i) ,v(i),
Uka-→jb i=k-j
(16)
where Uka-→jb is a path from va to vb of length k - j containing only unlabeled nodes except vb .
Substituting the right hand term of Eq. (12) with Eq. (16), we obtain that
k-1	1	k	1
Il(va,vb;k)=	av(i-1) ,v(i)=∑∑πdv(i-i),v(i).	(17)
j=0 Uka-→jb i=k-j	j=1 Uja→b i=j

13
Under review as a conference paper at ICLR 2021
Now Theorem 1 can be proved by combining Lemma 1 and Lemma 2:
Proof. Suppose that whether a node is labeled or not is independent of each other for the given
graph. Then we have
k1
EIl (va, vb; k) =EIXXYav(i-1) ,v(i)
j =1 Uja→b i=j
k1
EE E ∏e,v(i)
j=1	Uja→b i=j
k1
EEPr(Prb
j=1 Pa→b
is an Umabeled-nodes-only path) ɪɪ dv(i—i),v(i)
i=j
(18)
k1	k
XXβjY
adv(i-1) ,v(i) = X βjIdf (va , vb ; j).
j=1 Pja→b	i=j	j=1

B Proof of Theorem 2
Proof. Denote the set of labels as L. Since different label dimensions in y(α" do not interact with
each other when running LPA, the value of the yα-th dimension in ya) (denoted by ya) [ya]) comes
only from the nodes with initial label ya . It is clear that
k1
ya(k)[ya]=	X XXYadv(i-1),v(i) ,	(19)
vb:yb=ya j=1 Uja→b i=j
which equals Pv :y =y Il(va, vb; k) according to Lemma 2. Therefore, we have
Pr(ya = ya) = ya [yk)	Y yik) [ya] = X Il(va, vb； k)	QO)
Pi∈Lya(k)[i]	a	vb:yb=ya
C Proof of Theorem 3
In this proof we assume that the dimension of node representations is one, but note that the conclu-
sion can be easily generalized to the case of multi-dimensional representations since the function
D(x) can be decomposed into the sum of one-dimensional cases. In the following of this proof, we
still use bold notations xi(k) and hi(k) to denote node representations, but keep in mind that they are
scalars rather than vectors.
We give two lemmas before proving Theorem 3. The first one is about the gradient of D(x):
Lemma 3 hik) = Xik)- dDff).
i
Proof	x(k)	_ dD(Xg)) = x(k) _	P	a∙ .(x(k)-	x(k)) = P	a. .χ(k)	=	h(k)	口
Prr)Of	xi	—	∂χ(k)	= xi 一	Vj∈N ∈N(vi) aij (Xi -	xj ) = j∈N ∈N(vi)	ajxj	=	hi .	口
Xi
It is interesting to see from Lemma 3 that the aggregation step in GCN is equivalent to running
gradient descent for one step with a step size of one. However, this is not able to guarantee that
D(h(k)) ≤ D(x(k)) because the step size may be too large to reduce the value of D.
The second lemma is about the Hessian of D(x):
Lemma 4 V2D(x) W 2I, or equivalently, 2I — V2D(x) is a positive Semidefinite matrix.
14
Under review as a conference paper at ICLR 2021
Proof. We first calculate the Hessian of D(X) = 2 Pva Vj Gj ∣∣Xi 一 Xj |H：
1 一 々ii -αi2
一a21	1 一 丽2
V2D(x)=	.	.
-a1n
一 a2n
I 一 D-1A.
(21)
一an1	-an2
1 一 Gnn.
Therefore, 2I 一 V2D(x) = I + D-1A. Since D-1A is Markov matrix (i.e., each entry is non-
negative and the sum of each row is one), its eigenvalues are within the range [-1, 1], so the eigen-
values of I + D-1A are within the range [0, 2]. Therefore, I + D-1A is a positive semidefinite
matrix, and we have V2D(x) 2I.
We can now prove Theorem 3:
Proof. Since D is a quadratic function, we perform a second-order Taylor expansion ofD around
x(k) and obtain the following inequality:
D(h(k)) =D(x(k)) + VDBkk)>(h(k) 一 X(k)) + ∣(h(k) 一 x(k))>V2D(x)(h(k) 一 x(k))
=D(X(k)) 一 VDBkk)>VD(x(k)) + 1 VD(x(k))>V2D(x)VD(x(k))
≤D(x(k)) 一 VD(x(k))>VD(x(k)) + VD(x(k))>VD(x(k))
=D(x(kk).
(22)

D More Visualization Results on Karate Club Network
Figure 8 illustrates more visualization of GCN and GCN-LPA on karate club network. In each
subfigure, we vary the number of layers from 1 to 4 to examine how the learned representations
evolve. The initial node features are one-hot identity vectors, and the dimension of hidden layers
and output layer is 2. The transformation matrices are uniformly initialized within range [-1, 1]. We
use sigmoid function as the nonlinear activation function. Comparing the four figures in each row,
we conclude that the aggregation step and transformation step in GCN and GCN-LPA do benefit the
separation of different classes. Comparing Figure 8a and 8c (or Figure 8b and 8d), we conclude that
more inter-class edges will make the separation harder for GCN (or GCN-LPA). Comparing Figure
8a and 8b (or Figure 8c and 8d), we conclude that GCN-LPA is more noise-resistant than GCN,
therefore, GCN-LPA can better differentiate classes and identify clustering substructures.
E Datasets Details
The statistics of all datasets are shown in Table 3.
	Cora	Citeseer	Pubmed	Coauthor-CS	Coauthor-Phy
# nodes	2,708	3,327	^^19,717^^	18,333	34,493
# edges	5,278	4,552	44,324	81,894	247,962
# features	1,433	3,703	500	6,805	8,415
# classes	7	6	3	15	5
Intra-class edge rate	81.0%	73.6%	80.2%	80.8%	93.1%
Labeled node rate	5.2%	3.6%	0.3%	1.6%	0.3%
Table 3: Statistics for all datasets.
F Hyper-parameter Settings
The detailed hyper-parameter settings for all datasets are listed in Table 4. In GCN-LPA, we use
the same dimension for all hidden layers. Note that the number of GCN layers and the number of
LPA iterations can actually be different since GCN and LPA are implemented as two independent
modules. We use grid search to determine hyper-parameters on Cora, and perform fine-tuning on
15
Under review as a conference paper at ICLR 2021
(a) GCN on the original network
2-1 aye r
(b) GCN-LPA on the original network
2-layer	3-layer
(c) GCN on the noisy network
2-layer	3-layer
(d) GCN-LPA on the noisy network
Figure 8: Visualization of GCN and GCN-LPA with 1 〜4 layers on karate ClUb network.
	Cora	Citeseer	Pubmed	Coauthor-CS	Coauthor-Phy
Dimension of hidden layers	32	16	32	32	32
# GCN layers	5	2	2	2	2
# LPA iterations	5	5	1	2	3
L2 weight	1 X 10-4	5 X 10-4	2 X 10-4	1 X 10-4	1 X 10-4
LPA weight (λ)	10	1	1	2	1
DropoUt rate	0.2	0	0	0.2	0.2
Learning rate	0.05	0.2	0.1	0.1	0.05
Table 4: Hyper-parameter settings for all datasets.
other datasets, i.e., varying one hyper-parameter per time to see if the performance can be fUrther
improved. The search spaces for hyper-parameters are as follows:
•	Dimension of hidden layers: {8, 16, 32};
•	# GCN layers: {1,2,3,4,5,6};
•	# LPA iterations: {1, 2, 3, 4, 5, 6, 7, 8, 9};
•	L2 weight: {10-7, 2 × 10-7, 5 × 10-7, 10-6, 2 × 10-6,5 × 10-6,10-5,2 × 10-5,5 ×
10-5,10-4,2 × 10-4,5 × 10-4,10-3};
•	LPA weight (λ): {0, 1, 2, 5, 10, 15, 20};
16
Under review as a conference paper at ICLR 2021
•	Dropout rate: {0, 0.1, 0.2, 0.3, 0.4, 0.5};
•	Learning rate: {0.01, 0.02, 0.05, 0.1, 0.2, 0.5}.
17