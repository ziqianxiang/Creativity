Under review as a conference paper at ICLR 2021
Learning-Augmented Sketches for Hessians
Anonymous authors
Paper under double-blind review
Ab stract
Sketching is a dimensionality reduction technique where one compresses a matrix
by often random linear combinations. A line of work has shown how to sketch the
Hessian to speed up each iteration in a second order method, but such sketches
usually depend only on the matrix at hand, and in a number of cases are even
oblivious to the input matrix. One could instead hope to learn a distribution on
sketching matrices that is optimized for the specific distribution of input matrices.
We show how to design learned sketches for the Hessian in the context of second
order methods, where we learn potentially different sketches for the different iter-
ations of an optimization procedure. We show empirically that learned sketches,
compared with their “non-learned” counterparts, improve the approximation ac-
curacy for important problems, including LASSO, SVM, and matrix estimation
with nuclear norm constraints. Several of our schemes can be proven to perform
no worse than their unlearned counterparts.
1 Introduction
Large-scale optimization problems are abundant and solving them efficiently requires powerful tools
to make the computation practical. This is especially true of second order methods which often are
less practical than first order ones. Although second order methods may have many fewer iterations,
each iteration could involve inverting a large Hessian, which is cubic time; in contrast, first order
methods such as stochastic gradient descent are linear time per iteration.
In order to make second order methods faster in each iteration, a large body of work has looked at
dimensionality reduction techniques, such as sampling, sketching, or approximating the Hessian by
a low rank matrix. See, for example, (Gower et al., 2016; Xu et al., 2016; Pilanci & Wainwright,
2016; 2017; Doikov & Richtarik, 2018; GoWer et al., 2018; Roosta-Khorasani & Mahoney, 2019;
Gower et al., 2019; Kylasa et al., 2019; Xu et al., 2020; Li et al., 2020). Our focus is on sketching
techniques, Which often consist of multiplying the Hessian by a random matrix chosen independently
of the Hessian. Sketching has a long history in theoretical computer science (see, e.g., (Woodruff,
2014) for a survey), and We describe such methods more beloW. A special case of sketching is
sampling, Which in practice is often uniform sampling, and hence oblivious to properties of the
actual matrix. Other times the sampling is non-uniform, and based on squared norms of submatrices
of the Hessian or on the so-called leverage scores of the Hessian.
Our focus is on sketching techniques, and in particular, We folloW the frameWork of (Pilanci &
WainWright, 2016; 2017) Which introduce the iterative Hessian sketch and NeWton sketch, as Well
as the high accuracy refinement given in (van den Brand et al., 2020). If one Were to run NeWton’s
method to find a point Where the gradient is zero, in each iteration one needs to solve an equation
involving the current Hessian and gradient to find the update direction. When the Hessian can be
decomposed as A>A for an n × d matrix A With n d, then sketching is particularly suitable.
The iterative Hessian sketch Was proposed in (Pilanci & WainWright, 2016), Where A is replaced
with S ∙ A, for a random matrix S which could be i.i.d. Gaussian or drawn from a more structured
family of random matrices such as the Subsampled Randomized Hadamard Transforms or Count-
SKETCH matrices; the latter was done in (Cormode & Dickens, 2019). The Newton sketch was
proposed in (Pilanci & Wainwright, 2017), which extended sketching methods beyond constrained
least-squares problems to any twice differentiable function subject to a closed convex constraint set.
Using this sketch inside of interior point updates has led to much faster algorithms for an extensive
body of convex optimization problems Pilanci & Wainwright (2017). By instead using sketching as
1
Under review as a conference paper at ICLR 2021
a preconditioner, an application of the work of (van den Brand et al., 2020) (see Appendix E) was
able to improve the dependence on the accuracy parameter to logarithmic.
In general, the idea behind sketching is the following. One chooses a random matrix S, drawn from a
certain family of random matrices, and computes SA. IfA is tall-and-skinny, then S is short-and-fat,
and thus SA is a small, roughly square matrix. Moreover, SA preserves important properties of A.
One typically desired property is that S is a subspace embedding, meaning that simultaneously for
all x, one has kSAxk2 = (1 ± )kAxk2. An observation exploited in (Cormode & Dickens, 2019),
building off of the COUNT-SKETCH random matrices S introduced in randomized linear algebra
in (Clarkson & Woodruff, 2017), is that if S contains O(1) non-zero entries per column, then SA
can be computed in O(nnz(A)) time, where nnz(A) denotes the number of nonzeros in A. This is
sometimes referred to as input sparsity running time.
Each iteration of a second order method often involves solving an equation of the form A>Ax =
A>b, where A>A is the Hessian and b is the gradient. For a number of problems, one has access to
a matrix A ∈ Rn×d with n d, which is also an assumption made in Pilanci & Wainwright (2017).
Therefore, the solution x is the minimizer to a constrained least squares regression problem:
min1 kAx -bk2,	⑴
x∈C 2
where C is a convex constraint set in Rd. For the unconstrained case (C = Rd), various classical
sketches that attain the subspace embedding property can provably yield high-accuracy approximate
solutions (see, e.g., (Sarlos, 2006; Nelson & Nguyen, 2013; Cohen, 2016; Clarkson & Woodruff,
2017)); for the general constrained case, the Iterative Hessian Sketch (IHS) was proposed by Pilanci
& Wainwright (2016) as an effective approach and Cormode & Dickens (2019) employed sparse
sketches to achieve input-sparsity running time for IHS. All sketches used in these results are data-
oblivious random sketches.
Learned Sketching. In the last few years, an exciting new notion of learned sketching has emerged.
Here the idea is that one often sees independent samples of matrices A from a distribution D, and
can train a model to learn the entries in a sketching matrix S on these samples. When given a future
sample B, also drawn from D, the learned sketching matrix S will be such that S ∙ B is a much
more accurate compression of B than if S had the same number of rows and were instead drawn
without knowledge of D. Moreover, the learned sketch S is often sparse, therefore allowing S ∙ B
to be applied very quickly. For large datasets B this is particularly important, and distinguishes this
approach from other transfer learning approaches, e.g., (Andrychowicz et al., 2016), which can be
considerably slower in this context.
Learned sketches were first used in the data stream context for finding frequent items (Hsu et al.,
2019) and have subsequently been applied to a number of other problems on large data. For exam-
ple, Indyk et al. (2019) showed that learned sketches yield significantly small errors for low rank
approximation. In (Dong et al., 2020), significant improvements to nearest neighbor search were
obtained via learned sketches. More recently, Liu et al. (2020) extended learned sketches to sev-
eral problems in numerical linear algebra, including least-squares and robust regression, as well as
k-means clustering.
Despite the number of problems that learned sketches have been applied to, they have not been
applied to convex optimization in general. Given that such methods often require solving a large
overdetermined least squares problem in each iteration, it is hopeful that one can improve each
iteration using learned sketches. However, a number of natural questions arise: (1) how should we
learn the sketch? (2) should we apply the same learned sketch in each iteration, or learn it in the
next iteration by training on a data set involving previously learned sketches from prior iterations?
Our Contributions. In this work we answer the above questions and derive the first learned sketches
for a wide number of problems in convex optimization.
Namely, we apply learned sketches to constrained least-squares problems, including LASSO, sup-
port vector machines (SVM), and matrix regression with nuclear norm constraints. We show empir-
ically that learned sketches demonstrate superior accuracy over random oblivious sketches for each
of these problems. Specifically, compared with three classical sketches (Gaussian, Count-Sketch
and Sparse Johnson-Lindenstrauss Transforms; see definitions in Section 2), the learned sketches in
each of the first few iterations
2
Under review as a conference paper at ICLR 2021
•	improve the LASSO error f(x) - f (x*) by 80% to 87% in two real-world datasets, where
f (X) = 2 kAx -bk2 + ∣∣χkι;
•	improve the dual SVM error f (x) — f (x*) by 10-30% for a synthetic and a real-world dataset,
as well as by 30%T0% for another real-world dataset, where f(x) = ∣∣Bxk2;
•	improve the matrix estimation error f (X) - f (X*) by at least 30% for a synthetic dataset and
at least 95% for a real-world data set, where f(X) = kAX - Bk1 2F.
Therefore, the learned sketches attain a smaller error within the same number of iterations, and in
fact, within the same limit on the maximum runtime, since our sketches are extremely sparse (see
below).
We also study the general framework of convex optimization in (van den Brand et al., 2020), and
show that also for sketching-based preconditioning, learned sketches demonstrate considerable ad-
vantages. More precisely, by using a learned sketch with the same number of rows as an oblivious
sketch, we are able to obtain a much better preconditioner with the same overall running time.
All of our learned sketches S are extremely sparse, meaning that they contain a single non-zero
entry per column. Following the previous work of (Indyk et al., 2019), we choose the position of
the nonzero entry in each column to be uniformly random, while the value of the nonzero entry is
learned. This already demonstrates a significant advantage over non-learned sketches, and has a fast
training time. Importantly, because of such sparsity, our sketches can be applied in input sparsity
time given a new optimization problem.
We also provide several theoretical results, showing how to algorithmically use learned sketches in
conjunction with random sketches so as to do no worse than random sketches.
2 Preliminaries
Classical Sketches. Below we review several classical sketches that have been used for solving
optimization problems.
•	Gaussian sketch: S = √m G, where G is an m X n Gaussian random matrix.
•	COUNT-SKETCH: Each column of S has only a single non-zero entry. The position of the non-
zero entry is chosen uniformly over the m entries in the column and the value of the entry is
either +1 or -1, each with probability 1/2. Further, the columns are chosen independently.
•	Sparse Johnson-Lindenstrauss Transform (SJLT): S is the vertical concatenation of s indepen-
dent C OUNT- S KETCH matrices, each of dimension m/s × n.
Count-Sketch-type Sketch. A COUNT-SKETCH-type sketch is characterized by a tuple
(m, n, p, v), where m, n are positive integers and p, v are n-dimensional real vectors, defined as
follows. The sketching matrix S has dimensions m × n and Spi ,i = vi for all 1 ≤ i ≤ n while
all the other entries of S are 0. When m and n are clear from context, we may characterize such a
sketching matrix by (p, v) only.
Subspace Embeddings. For a matrix A ∈ Rn×d, we say a matrix S ∈ Rm×n is a (1 ± )-subspace
embedding for the column span of A if (1 - ) kAxk2 ≤ kSAxk2 ≤ (1 + ) kAxk2 for all x ∈ Rd.
The classical sketches above, with appropriate parameters, are all subspace embedding matrices
with at least a constant probability; our focus is on Count-Sketch which can be applied in input
sparsity running time. We summarize the parameters needed for a subspace embedding below:
•	Gaussian sketch: m = O(d/e2). Itisa dense matrix and computing SA costs O(m ∙ nnz(A))=
O(nnz(A)d/2) time.
•	COUNT-SKETCH: m = O(d2/e2) (Clarkson & Woodruff, 2017). Although the number of rows
is quadratic in d/e, the sketch matrix S is sparse and computing SA takes only O(nnz(A)) time.
•	SJLT: m = O(d∕e2) and has S = O(1∕e) non-zeros per column (Nelson & Nguyen, 2013;
Cohen, 2016). Computing SA takes O(s nnz(A)) = O(nnz(A)/e) time.
Iterative Hessian Sketch. The Iterative Hessian Sketching (IHS) method (Pilanci & Wainwright,
2016) solves the constrained least-squares problem (1) by iteratively performing the update
1	2>
xt+1 = arg min - ∣St+ιA(x - xt)∣∣2 — hA (b - Axt), x - xt i,	(2)
x∈C 2
3
Under review as a conference paper at ICLR 2021
where St+1 is a sketching matrix. It is not difficult to see that for the unsketched version (St+1 is
the identity matrix) of the minimization above, the optimal solution xt+1 coincides with the optimal
solution to the constrained least square problem (1). The IHS approximates the Hessian A>A by a
sketched version (St+1A)>(St+1A) to improve runtime, as St+1A typically has very few rows.
Unconstrained Convex Optimization. Consider an unconstrained convex optimization problem
minx f (x), where f is smooth and strongly convex, and its Hessian V1 2 3 4f is LiPschitz continuous.
This problem can be solved by Newton’s method, which iteratively performs the update
xt+1 = xt - arg min (V2f(xt)1/2)>(V2f(xt)1/2)z - Vf (xt)	,
Provided it is given a good initial Point x0 . In each steP, it requires solving a regression Problem of
the form minz A>Az - y2, which, with access to A, can be solved with a fast regression solver
in (van den Brand et al., 2020). The regression solver first comPutes a Preconditioner R via a QR
decomPosition such that SAR has orthonormal columns, where S is a sketching matrix, then solves
zb = arg minz0 (AR)> (AR)z0 - y2 by gradient descent and returns Rzb in the end. Here, the
Point of sketching is that the QR decomPosition of SA can be comPuted much more efficiently than
the QR decomPosition of A since S has only a small number of rows.
Learning a Sketch. We use the same learn-
ing algorithm in (Liu et al., 2020), given in
Algorithm 1. The algorithm aims to min-
imize the mean loss function L(S, A) =
N PN=I L(S,Ai), where S is the learned
sketch, L(S, A) is the loss function ofS aPPlied
to a data matrix A, and A = {A1, . . . , AN} is
a (random) subset of training data.
3 Hessian S ketch
In this section, we consider the minimization
Problem
min11 l∣s 6 7 8 9 10 11 12 13 14 15Axk2 - hA>y,χil,	⑶
x∈C 2
which is used as a subroutine for the IHS
(cf. (2)). We Present an algorithm with the
learned sketch in Algorithm 2. To analyze its
Performance, we let R be the column sPace of
A ∈ Rm×n and define the following quantities
(corresPonding exactly to the unconstrained
case in Pilanci & Wainwright (2016))
Z1(S)
inf
v∈R∩Sn-1
lSv l22 ,
Z2(S) =	sup u, (S>S - In)v
u,v∈R∩Sn-1
where Sn-1 denotes the Euclidean unit sPhere
in Rn .
The following is the estimation guarantee of Z1
and Zb2. The Proof is PostPoned to APPendix A.
Algorithm 1 LEARN-SKETCH: Gradient descent
algorithm for learning the sketch values
Require: Atrain = {Aι,...,AN} (Ai ∈ Rn×d),
learning rate α
1:	Randomly initialize p, v for a Count-Sketch-
tyPe sketch
2:	for t = 0 to steps do
3:	Form S using p, v
4:	SamPle batch Abatch from Atrain
5∙ V — V - αaLgAbatch)
J * V ʌ V	<α	∂v
6: end for
Algorithm 2 Solver for (3)
1: Si J learned sketch, S? J random sketch
2: (Zi,i,Zi,2) J ESTIMATE(Si ,A), i =1,2
3: if Zbi,2/Zbi,i < Zb2,2 /Zb2,i then
4:	xb J solution of (3) with S = Si
5: else
6:	xb J solution of (3) with S = S2
7: end if
8: return b_______________________________
9: function ESTIMATE(S, A)
10:	T J sParse (1 ± η)-subsPace embedding
matrix for d-dimensional subsPaces
11:	(Q, R) J QR(TA)
12:	Zbi J σmin(SAR-i)
13:	Z2 J (1 ± η)-aPProximation to
(SAR-i)>(SAR-i) -Iop
14:	return (Zi , Z2 )
15: end function
Lemma 3.1. Suppose that η ∈ (0, 1) is a small constant, A is of full rank and S has O(d2) rows.
ThefUnction ESTIMATE (S, A) returns in O((nnz(A) log(1∕η)+poly(d∕η)) time Zi, Z? which With
probability at least 0.99 satisfy that Z+S) ≤ Zi ≤ Z-S) and (Z+^2 一 3η ≤ Z2 ≤ (Z-IS2 + 3η.
Note for a matrix A, ∣A∣θp = suPχ=o 崂卜 is its operator norm. Similar to (Pilanci & Wainwright,
2016, ProPosition 1), we have the following guarantee. The Proof is PostPoned to APPendix B.
4
Under review as a conference paper at ICLR 2021
Theorem 3.2. Let η ∈ (0, 1) be a small constant. Suppose that A is offull rank and Si and S? are
both COUNT-SKETCH-type sketches with O(d2) rows. Algorithm 2 returns a solution xb which, with
probability at least 0.98, satisfies that kA(x 一 x*)∣∣2 ≤ (1 + η)4 (min{ Z12, Z22} +4η) ∣∣Ax*∣∣2
in O(nnz(A) log( 1) + poly(d)) time, where x* = argminχ∈c ∣∣Ax — bk? is the least-squares
solution.
Algorithm 3 Fast Regression Solver for (4)
4
5
6
7
8
9
10
11
12
13
Si J learned sketch, S2 J random sketch
(Qi, Ri) J QR(SiA), i=1,2
(σi,σi0) J EIG(ARi-1), i = 1,2
. EIG(B) returns the max and min singular
values of B
if σ1∕σ10 < σ2∕σ20 then
P JR1-1,ηJ 1∕(σ12 + (σ10)2)
else
P JR2-1,ηJ 1∕(σ22 + (σ20)2)
end if
z0 J 0
while A>APzt —y2 ≥ e ∣y∣2 do
zt+1 J zt — η(P >A>AP)(P >A>AP zt—P >y)
end while
return P zt
1
2
3
4 Hessian Regression
In this section, we consider the minimiza-
tion problem
mzin A>Az 一 y2 ,	(4)
which is used as a subroutine for the un-
constrained convex optimization problem
minx f (x) with A>A being the Hessian
matrix V2f (x) (See Section 2). Here A ∈
Rn×d, y ∈ Rd, and we have access to A.
We incorporate a learned sketch into the
fast regression solver in (van den Brand
et al., 2020) and present the algorithm in
Algorithm 3.
Here the subroutine EIG(B) applies a (1 + η)-subspace embedding sketch T to B for some small
constant η and returns the maximum and the minimum singular values of T B. Since B admits the
form of AR, the sketched matrix TB can be calculated as (TA)R and thus can be computed in
O(nnz(A) + poly(d)) time if T is a COUNT- S KETCH matrix of O(d2) rows. The extreme singular
values of T B can be found by SVD or the Lanczos’s algorithm.
Similar to Lemma 4.2 in (van den Brand et al., 2020), we have the following guarantee of Algo-
rithm 3. The proof parallels the proof in (van den Brand et al., 2020) and is postponed to Ap-
pendix C.
Theorem 4.1. Suppose that S1 and S2 are both COUNT- S KETCH-type sketches with O(d2) rows.
Algorithm 3 returns a solution x0 such that ^A>Ax0 — y∣1 ≤ e ∣∣yk2 in O(nnz(A)) + O(nd ∙
(min{σι∕σ1, σ2∕σ2})2 ∙log(κ(A)∕e) +poly(d)) time.
Remark 4.2. In Algorithm 3, S2 can be chosen to be a subspace embedding matrix for d-
dimensional subspaces, in which case, AR2-1 has condition number close to 1 (see, e.g., (Woodruff,
2014, p38)) and the full algorithm would run faster than the trivial O(nd2)-time solver to (4).
Remark 4.3. For the original unconstrained convex optimization problem minx f (x), one can run
the entire optimization procedure with learned sketches versus the entire optimization procedure
with random sketches, compare the objective values at the end, and choose the better of the two. For
least-squares f (x) = 1 ∣∣Ax — b∣2, the value of f (x) can be approximated efficiently by a sparse
subspace embedding matrix in O(nnz(A) + nnz(b) + poly(d)) time.
5 IHS Experiments
Training. We learn the sketching matrix in each iteration (also called a round) separately. Recall that
in the (t+1)-stroundofIHS, the optimization problem we need to solve (2) depends on xt. An issue
is that we do not know xt, which is needed to generate the training data for the (t+ 1)-st round. Our
solution is to use the sketch matrix in the previous round to obtain xt by solving (2), and then use it to
generate the training data in the next round. That is, in the first round, we train the sketch matrix S1 to
solve the problem xi = arg minχ∈c 2 ∣∣SιAxk2 — <A>b, Xi and use xi to generate the training data
for the optimization problem for x2, and so on. The loss function we use is the unsketched objective
function in the (t+1)-stiteration, i.e., L(St+ι, A) = 2 ∣∣A(xt+ι — xt)∣2-hAT (b—Axj xt+ι-x∕,
where xt+1 is the solution to (2) and thus depends on St+1.
5
Under review as a conference paper at ICLR 2021
Comparison. We compare the learned sketch against three classical sketches: Gaussian, COUNT-
Sketch, and SJLT (see Section 2) in all experiments. The quantity we compare is a certain error,
defined individually for each problem, in each round of the iteration of the IHS or as a function of
the runtime of the algorithm. All of our experiments are conducted on a laptop with 1.90GHz CPU
and 16GB RAM.
5.1 LASSO
iteration round
Figure 1: Test error of LASSO on
CO emissions dataset, m = 5d
Figure 2: Test error of LASSO on
CO emissions dataset, m = 3d
iteration round
Figure 3: Test error of LASSO on
greenhouse gas dataset, m = 6d
Figure 4: Test error of LASSO on
greenhouse gas dataset, m = 3.5d
We define an instance of LASSO regression as
x* = arg min ɪ ∣∣Ax - b∣∣2 + λ ∣∣x∣∣1 ,	(5)
x∈Rd 2
where λ is a parameter. We use two real-world datasets:
•	CO emission1: the dataset contains 9 sensor measures ag-
gregated over one hour (by means of average or sum),
which can help to predict the CO emission. We divide
the raw data into 120 (Ai , bi) such that Ai ∈ R300×9 ,
bi ∈ R300×1. The data in each matrix is sorted in chrono-
logical order.|(A,b)train| = 96, |(A,b)test| = 24.
•	Greenhouse gas2: time series of measured greenhouse
gas concentrations in the California atmosphere. Each
(A, b) corresponds to a different measurement location.
Ai ∈ R327×14, bi ∈ R327×1, and |(A,b)train| = 400,
|(A, b)test| = 100. (This dataset was also used in (Liu et al.,
2020).)
We choose λ = 1 in the LASSO regression (5). We choose
m = 5d and m = 3d for the CO emission dataset and m = 6d
and m = 3.5d for the greenhouse gas dataset. We consider the
error (ɪ IMx - b∣2 + ∣∣x∣∣1) - (2 ∣∣Ax* - b∣2 + ∣∣x*∣∣1) and take
an average over five independent trials. We plot in (natural) log-
arithmic scale the mean errors of the two datasets in Figures 1 to
4. We can observe that the learned sketches consistently outper-
form the classical random sketches in all cases. Note that our
sketch size is much smaller than the theoretical bound. For a
smaller sketch size, classical random sketches converge slowly
or do not yield stable results, while the learned sketches can
converge to a small error quickly. For larger sketch sizes, all
three classical sketches have approximately the same order of
error, and the learned sketches reduce the error by a 5/6 to 7/8
factor in all iterations.
5.2 Support Vector Machine
In the context of binary classification, a labeled sample is a pair
(ai, zi), where ai ∈ Rn is a vector representing a collection of features and zi ∈ {-1, +1} is the
associated class label. Given a set of labeled patterns {(ai, zi)}id=1, the support vector machine
(SVM) estimates the weight vector w * by minimizing the function
W* = argmin [ C X g(Zi, hwi,电i) + 1 l∣w∣2],
w∈Rn	2 i=1	2
where C is a parameter. Here we use the squared hinge loss g(zi, hw, aii) := (1 - zi hw, aii)2+.
1https://archive.ics.uci.edu/ml/datasets/Gas+Turbine+CO+and+NOx+Emission+Data+Set
2https://archive.ics.uci.edu/ml/datasets/Greenhouse+Gas+Observing+Network
6
Under review as a conference paper at ICLR 2021
Figure 5: Test error of SVM on
random Gaussian dataset
iteration round
Figure 6: Test error of SVM on
swarm behavior dataset
iteration round
Figure 7: Test error of SVM on
Gisette dataset
The dual of this problem can be written as a constrained minimization problem (see, e.g., (Li et al.,
2009; Pilanci & Wainwright, 2015)),
x* := arg min ∣∣Bx∣∣2 ,
x∈∆d
over the domain ∆d = {x ∈ Rd : x ≥ 0 and kxk1 = 1}, the positive simplex in Rd. Here
B = [(AD)> √1yId]> ∈ R(n+d)×d, where A is an n X d matrix With ai ∈ Rn as its i-th column
and D = diag(z) is a d × d diagonal matrix.
We conduct experiments on the following three datasets:
•	Random Gaussian (synthetic): We follow the same construction in Pilanci & Wainwright
(2016). We generate a two-component Gaussian mixture model, based on the component distri-
butions N(μ0, I) and N(μ1, I), where μ0 and μ1 are uniformly distributed in [-3, 3]. Placing
equal weights on each component, we draw d samples from this mixture distribution.
•	Swarm behavior3: Each instance in the dataset has n = 2400 features and the task is to predict
whether the instance is flocking or not flocking. We use only the first 6000 instances of the
raw data, and divide them into 200 smaller groups of instances. Each group contains d = 30
instances, corresponding to a Bi of size 2430 × 30. The training data consists of 160 groups and
the test data consist of 40 groups.
•	Gisette4: Gisette is a handwritten digit recognition problem which asks to to separate the highly
confusable digits ‘4’ and ‘9’. Each instance has n = 5000 features. The raw data is divided
into 200 smaller groups, where each contains d = 30 instances and corresponds to a Bi of size
5030 × 30. The training data consists of 160 groups and the test data consists of 40 groups.
We choose m = 10d in all experiments and define the error as kBxk22 - kBx* k22. For random
sketches, we take the average error over five independent trials, and for learned sketches, over three
independent trials. For the Gisette dataset, we use the learned sketch in all rounds. We plot in a
(natural) logarithmic scale the mean errors of the three datasets in Figures 5 to 7. For Gisette and
random Gaussian datasets, using the learned sketches reduced the error by 10%-30%, and for the
Swarm Behavior dataset, the learned sketches reduce the error by about 30%-40%.
5.3 Matrix Estimation with Nuclear Norm Constraint
In many applications, for the problem
X* := arg min kAX - Bk2F ,
X∈Rd1×d2
it is reasonable to model the matrix X* as having low rank. Similar to '1 -minimization for com-
pressive sensing, a standard relaxation of the rank constraint is to minimize the nuclear norm of X ,
defined as kXk* := Pjm=in1{d1,d2} σj(X), where σj(X) is the j-th largest singular value ofX.
Hence, the matrix estimation problem we consider here is
X* := arg min kAX - Bk2F such that kXk* ≤ ρ.
X∈Rd1×d2
where ρ > 0 is a user-defined radius as a regularization parameter.
3https://archive.ics.uci.edu/ml/datasets/Swarm+Behaviour
4https:〃www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html#gisette
7
Under review as a conference paper at ICLR 2021
Figure 8: Test error of matrix estimation
on synthetic data, m = 50
We conduct experiments on the following two datasets:
•	Synthetic Dataset: We generate the pair (Ai , Bi) as
Bi = AiXi + Wi, where Ai ∈ Rn×d1 with i.i.d
N(0,1) entries. Xi ∈ Rd1 ×d2 is a matrix With rank
at most r. Wi is noise with i.i.d N(0, σ2) entries.
Here we set n = 500, d1 = d2 = 7, r = 3, ρ = 30.
|(A, B)|train = 270, |(A, B)|test = 30.
•	Tunnel5 : The data set is a time series of gas concen-
trations measured by eight sensors in a wind tunnel.
Each (A, B) corresponds to a different data collection
trial. Ai ∈ R13530×5,Bi ∈ R13530×6, |(4,3)|此益口 =
144, |(A, B)|test = 36. The same dataset and param-
eters were also used in (Liu et al., 2020) for regression
tasks. In our nuclear norm constraint, we set ρ = 10.
Figure 9: Test error of matrix estimation We choose m = 40, 50 for the synthetic dataset and m =
on Tunnel dataset, m = 50	10, 50 for the Tunnel dataset and define the error to be
2(kAX - BkF - l∣AX* - BkF). For each data point,
we take the average error of five independent trials. The mean errors of the two datasets when
m = 50 are plotted in a (natural) logarithmic scale in Figures 8 and 9. We observe that the classical
sketches yield approximately the same order of error, while the learned sketches improve the error
by at least 30% for the synthetic dataset and surprisingly by at least 95% for the Tunnel dataset.
The huge improvement on the Tunnel dataset may be due to the fact that the matrices Ai have many
duplicate rows. We defer the results when m = 10 or 40 to Appendix D, which show that the
learned sketches yield much smaller errors than the random sketches and the random sketches could
converge significantly more slowly with considerably larger errors in the first several rounds.
5.4 Runtime of Learned S ketches
As stated in Section 2, our learned sketch matrices S are all
count-sketch-type matrices (each column contains a single
nonzero entry), the matrix product SA can thus be computed
in O(nnz(A)) time and the overall algorithm is expected to be
fast. To verify this, we plot in an error-versus-runtime plot for
the SVM and matrix estimation with nuclear norm constraint
tasks in Figures 10 and 11 (corresponding to the datasets in
Figures 7 and 9). The runtime consists only of the time for
sketching and solving the optimization problem and does not
include the time for loading the data. We run the same exper-
iment three times. Each time we take an average over all test
data. From the plot we can observe that the learned sketch and
Count-Sketch have the fastest runtimes, which are slightly
faster than that of the SJLT and significantly faster than that of
the Gaussian sketch.
Figure 10: Test error of SVM on
Gisette dataset
runtime
Figure 11: Test error of matrix es-
timation on Tunnel dataset
6 Fast Regression Experiment
We consider the unconstrained least-squares problem, i.e., (5) with λ = 0, using the CO emission,
greenhouse gas datasets, and the following Census dataset:
• Census data6 : this dataset consists of annual salary and related features on people who reported
that they worked 40 or more weeks in the previous year and worked 35 or more hours per week.
We randomly sample 5000 instances to create (Ai, bi), where A ∈ R5000×11 and b ∈ R5000×1,
|(A, B)|train = 160, |(A, B)|test = 40.
5https://archive.ics.uci.edu/ml/datasets/Gas+sensor+array+exposed+to+turbulent+gas+mixtures
6https://github.com/chocjy/randomized-quantile-regression-solvers/tree/master/matlab/data
8
Under review as a conference paper at ICLR 2021
Figure 12: Test error of fast regression for the CO emission dataset, first three calls in solving an
unconstrained least-squares
Training. We optimize the learned sketch S1 by gradient descent (Algorithm 1), where L(S, A) =
κ(AR1-1), where R1 is computed as in Algorithm 3 and κ(M) denotes the condition number of a
matrix M .
Next we discuss how to generate the training data. Since we use Newton’s method to solve an
unconstrained convex optimization problem (see Section 2), in the t-round, we need to solve
a regression problem minz ∣∣(V2f (Xt)1/2)>(V2f (xt)1/2)z — Vf (Xt)∣∣2. Reformulating it as
minz A>Az - y 2 , we see that A and y depend on the previous solution xt . Hence, we take
Xt to be the solution obtained from Algorithm 3 using the learned sketch St , and this generates A
and y for the (t + 1)-st round.
Experiment. For the CO emission dataset, we set m = 70, and for the Census dataset, we set
m = 500. For the η in the Algorithm 3, we set η = 1 for the first round and η = 0.2 for the next
rounds for the CO emission dataset, and η = 1 for all rounds for the Census dataset. We leave the
setting and results on the greenhouse gas dataset to Appendix E.
We examine the accuracy of the subproblem (4) and define the error to be ∣A>ARzt — y∣2 / kyk2.
We run the first three subroutines of solving the subproblem for the CO emission dataset and the
Census dataset. The average error of three independent trials is plotted in Figures 12, 13 and 16.
We observe that for the CO emission dataset, the classical sketches have a similar performance and
the learned sketches lead to a fast convergence in the subroutine with the first-round error at least
80% smaller; for the Census dataset, the learned sketch achieves the smallest error for all three
rounds, where we reduce about 60% error in the first round and about 50% error in the third round.
Note that the learned sketch always considerably outperforms Count-Sketch in all cases.
Figure 13: Test error of fast regression for the Census dataset, first three calls in solving an uncon-
strained least-squares
7 Conclusion
We demonstrated the superiority of using learned sketches, over classical random sketches, in the It-
erative Hessian Sketching method and fast regression solvers for unconstrained least-squares. Com-
pared with random sketches, our learned sketches of the same size can considerably reduce the error
in the loss function (i.e., f (x) — f (x*), where X is the output of the sketched algorithm and x* the
optimal solution to the unsketched problem) for a given threshold of maximum number of iterations
or maximum runtime. Learned sketches also admit a smaller sketch size. When the sketch size
is small, the algorithm with random sketches may fail to converge, or converge slowly, while the
algorithm with learned sketches converges quickly.
9
Under review as a conference paper at ICLR 2021
References
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing Systems, pp. 3981-3989, 2016.
Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, 2004.
Kenneth L. Clarkson and David P. Woodruff. Low-rank approximation and regression in input
sparsity time. J. ACM, 63(6), January 2017. ISSN 0004-5411. doi: 10.1145/3019134. URL
https://doi.org/10.1145/3019134.
Michael B. Cohen. Nearly tight oblivious subspace embeddings by trace inequalities. In Proceedings
of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms, SODA ’16, pp.
278-287, USA, 2016. Society for Industrial and Applied Mathematics. ISBN 9781611974331.
Graham Cormode and Charlie Dickens. Iterative hessian sketch in input sparsity time. CoRR,
abs/1910.14166, 2019. URL http://arxiv.org/abs/1910.14166.
Nikita Doikov and Peter Richtarik. Randomized block cubic NeWton method. In Proceedings ofthe
35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stockholm,
Sweden, July 10-15, 2018, pp. 1289-1297, 2018.
Yihe Dong, Piotr Indyk, Ilya P. Razenshteyn, and Tal Wagner. Learning space partitions for nearest
neighbor search. In 8th International Conference on Learning Representations, ICLR 2020, Addis
Ababa, Ethiopia, April 26-30, 2020, 2020.
Robert M. Gower, Donald Goldfarb, and Peter Richtarik. Stochastic block BFGS: squeezing more
curvature out of data. In Proceedings of the 33nd International Conference on Machine Learning,
ICML 2016, New York City, NY, USA, June 19-24, 2016, pp. 1869-1878, 2016.
Robert M. Gower, Filip Hanzely, Peter Richtarik, and Sebastian U. Stich. Accelerated stochastic
matrix inversion: General theory and speeding up BFGS rules for faster second-order optimiza-
tion. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montreal, Canada,
pp. 1626-1636, 2018.
Robert M. Gower, Dmitry Kovalev, Felix Lieder, and Peter Richtarik. RSN: randomized subspace
Newton. In Advances in Neural Information Processing Systems 32: Annual Conference on
Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver,
BC, Canada, pp. 614-623, 2019.
Chen-Yu Hsu, Piotr Indyk, Dina Katabi, and Ali Vakilian. Learning-based frequency estimation
algorithms. In 7th International Conference on Learning Representations, ICLR 2019, New Or-
leans, LA, USA, May 6-9, 2019, 2019.
Piotr Indyk, Ali Vakilian, and Yang Yuan. Learning-based low-rank approximations. In Advances in
Neural Information Processing Systems 32: Annual Conference on Neural Information Process-
ing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pp. 7400-7410,
2019.
Sudhir B. Kylasa, Fred (Farbod) Roosta, Michael W. Mahoney, and Ananth Grama. GPU accelerated
sub-sampled Newton’s method for convex classification problems. In Proceedings of the 2019
SIAM International Conference on Data Mining, SDM 2019, Calgary, Alberta, Canada, May 2-4,
2019, pp. 702-710, 2019.
Xiang Li, Shusen Wang, and Zhihua Zhang. Do subsampled newton methods work for high-
dimensional data? In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020,
The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The
Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York,
NY, USA, February 7-12, 2020, pp. 4723-4730. AAAI Press, 2020.
10
Under review as a conference paper at ICLR 2021
Yu-Feng Li, Ivor W. Tsang, James Tin-Yau Kwok, and Zhi-Hua Zhou. Tighter and convex maximum
margin clustering. In David A. Van Dyk and Max Welling (eds.), Proceedings of the Twelfth Inter-
national Conference on Artificial Intelligence and Statistics, AISTATS 2009, Clearwater Beach,
Florida, USA, April 16-18, 2009, volume 5 of JMLR Proceedings,pp. 344-351.JMLR.org, 2009.
Simin Liu, Tianrui Liu, Ali Vakilian, Yulin Wan, and David P. Woodruff. On learned sketches for
randomized numerical linear algebra. arXiv:2007.09890 [cs.LG], 2020. URL https://arxiv.org/
abs/2007.09890.
J. Nelson and H. L. Nguyen. Osnap: Faster numerical linear algebra algorithms via sparser subspace
embeddings. In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pp.
117-126, 2013.
Mert Pilanci and Martin J. Wainwright. Randomized sketches of convex programs with sharp guar-
antees. IEEE Trans. Inf. Theory, 61(9):5096-5115, 2015.
Mert Pilanci and Martin J. Wainwright. Iterative Hessian sketch: Fast and accurate solution approx-
imation for constrained least-squares. J. Mach. Learn. Res., 17:53:1-53:38, 2016.
Mert Pilanci and Martin J. Wainwright. Newton sketch: A near linear-time optimization algorithm
with linear-quadratic convergence. SIAM J. Optim., 27(1):205-245, 2017.
Farbod Roosta-Khorasani and Michael W. Mahoney. Sub-sampled Newton methods. Math. Pro-
gram., 174(1-2):293-326, 2019.
T. Sarlos. Improved approximation algorithms for large matrices via random projections. In 2006
47th Annual IEEE Symposium on Foundations of Computer Science (FOCS’06), pp. 143-152,
2006.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (overparametrized)
neural networksin near-linear time. arXiv:2006.11648 [cs.LG], 2020.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices, pp. 210-268.
Cambridge University Press, 2012. doi: 10.1017/CBO9780511794308.006.
David P. Woodruff. Sketching as a tool for numerical linear algebra. 10(1-2):1-157, October 2014.
ISSN 1551-305X. doi: 10.1561/0400000060. URL https://doi.org/10.1561/0400000060.
Peng Xu, Jiyan Yang, Farbod Roosta-Khorasani, Christopher Re, and Michael W. Mahoney. Sub-
sampled Newton methods with non-uniform sampling. In Advances in Neural Information Pro-
cessing Systems 29: Annual Conference on Neural Information Processing Systems 2016, De-
cember 5-10, 2016, Barcelona, Spain, pp. 3000-3008, 2016.
Peng Xu, Fred Roosta, and Michael W. Mahoney. Second-order optimization for non-convex ma-
chine learning: an empirical study. In Proceedings of the 2020 SIAM International Conference
on Data Mining, SDM 2020, Cincinnati, Ohio, USA, May 7-9, 2020, pp. 199-207, 2020.
A Proof of Lemma 3.1
Suppose that AR-1 = UW, where U ∈ Rn×d has orthonormal columns, which form an orthonor-
mal basis of the column space of A. Since T is a subspace embedding of the column space of A
with probability 0.99, it holds for all x ∈ Rd that
1+η UTARTxL ≤ MRTxL ≤ 1-η UTARTxL .
Since
UUTAR-1xUU2 = kQxk2 = kxk2
and
kW xk2 = kUWxk2 = UUAR-1xUU2	(6)
11
Under review as a conference paper at ICLR 2021
we have that
ι+η ∣∣xk2 ≤ kWxk2 ≤ i-η ∣∣xk2, X ∈ Rd.
(7)
It is easy to see that
ZI(S)=χmdnιkSUxk2=min IkUW出
and thus,
my6=in0 (1 - η)
ksuwy∣2
kyk2
≤ Z1 (S) ≤ min(1 + η)
y6=0
ksuwy∣2
kyk2
Recall that SUW = SAR-1. We see that
(1- η)σmin(SAR-1) ≤ Z1(S) ≤ (1+ η)σmin(SAR-1).
By definition,
Z2(S) = UT(S>S-In)Uop.
It follows from (7) that
(1-η)2WTUT(STS-In)UWop≤Z2(S) ≤ (1+η)2WTUT(STS-In)UWop.
and from (7), (6) and (Vershynin, 2012, Lemma 5.36) that
(AR-1)>(AR-1) - Iop ≤ 3η.
Since
WTUT(STS-In)UWop= (AR-1)>(ST S - In)AR-1op
and
(AR-1)>STSAR-1-Iop-(AR-1)>(AR-1)-Iop
≤ (AR-1)>(STS-In)AR-1op
≤ (AR-1)>ST SAR-1 - Iop + (AR-1)>(AR-1) - Iop,
it follows that
(1 - η)2 (SAR-1)>SAR-1 - Iop - 3(1 - η)2η
≤ Z2 (S)
≤ (1 + η)2 (SAR-1)>SAR-1 - Iop + 3(1 + η)2η.
We have so far proved the correctness of the approximation and we shall analyze the runtime below.
Since S and T are sparse, computing SA and TA takes O(nnz(A)) time. The QR decomposition
of TA, which is a matrix of size poly(d∕η) X d, can be computed in poly(d∕η) time. The matrix
SAR-1 can be computed in poly(d) time. Since it has size poly(d) × d, its smallest singular value
can be computed in poly(d) time. To approximate Z2(S), we can use the power method to estimate
∖∖(SAR-1)TSAR-1 - Illop UP to a (1 土 η)-factor in O((nnz(A) + poly(d)) log(1∕η)) time.
B Proof of Theorem 3.2
In Lemma 3.1, we have with probability at least 0.99 that
Z2、(⅛Z2(S) - 3η、1 - η Z2(S)	3η
TT- ≥ ---Z--------- ≥  ---------：  ----：  .
Z1 ≥	ι-nZ1(S)	≥ (1 + η)2 Z1(S)	Zι(S)
When S is random subspace embedding, it holds with probability at least 0.99 that Z1(S) ≥ 3/4
and so, by a union bound, it holds with probability at least 0.98 that
Z2 ≥	2 - 4„
Z1≥(1+ η)4 Zi(S)	η,
12
Under review as a conference paper at ICLR 2021
or,
耦 ≤ (1 + η )4 佟 + 4η).
The correctness of our claim then follows from (Pilanci & Wainwright, 2016, Proposition 1), to-
gether with the fact that S2 is a random subspace embedding. The runtime follows from Lemma 3.1
and (Cormode & Dickens, 2019, Theorem 2.2).
C Proof of Theorem 4.1
The proof follows almost an identical argument as that in (van den Brand et al., 2020, Lemma
B.1). In (van den Brand et al., 2020), it is assumed (in our notation) that 3/4 ≤ σmin(AP) ≤
σmax(AP) ≤ 5/4 and thus one can set η = 1 in Algorithm 3 and achieve a linear convergence.
The only difference is that here we estimate σmin(AP) and σmax(AP) and set the step size η in the
gradient descent algorithm accordingly. By standard bounds for gradient descent (see, e.g., (Boyd
& Vandenberghe, 2004, p468)), with a choice of step size η = 2∕(σi21ax(AP) + σmin(AP)), after
O((σmaχ(AP)∕σmin(AP))2 log(1∕e)) iterations, we can find zt such that
IlP>A>AP(Zt- Z*)∣∣2 ≤ 6 IlP>A>AP(zo - Z*)∣∣2 ,
where Z* = argminz ∣∣P>A>APz - P>y∣∣2 is the optimal least-squares solution. This establishes
Eq. (11) in the proof in (van den Brand et al., 2020), and the rest of the proof follows as in there.
D IHS Experiment: Matrix Estimation with Nuclear Norm
Constraint
As stated in Section 5.3, the mean errors of the two datasets when m = 40 for the synthetic dataset
and m = 10 for the Tunnel dataset are plotted in a (natural) logarithmic scale in Figures 14 and 15.
learned
count-sketch
gaussian
sparse JL
Figure 14: Test error of matrix estimation on
synthetic dataset, m = 40
Figure 15: Test error of matrix estimation on
Tunnel dataset, m = 10
E	Fast Regression Experiment: Greenhouse Gas
We set m = 100, η = 0.2 in the first round and η = 1 in the second round. The mean errors of
the first two calls of the fast regression subroutine are plotted in Figure 16. We can observe that
Gaussian and sparse JL sketches have a significantly better performance than Count-Sketch, and
the learned sketch shows again a significant reduction of more than 50% in the first-round error.
13
Under review as a conference paper at ICLR 2021
Figure 16: Test error of fast regression for the greenhouse gas emission dataset, first two calls in
solving an unconstrained least-squares
14