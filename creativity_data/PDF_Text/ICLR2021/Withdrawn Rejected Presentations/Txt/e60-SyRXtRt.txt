Under review as a conference paper at ICLR 2021
GANMEX:	One-vs-One Attributions using
GAN-based Model Explainability
Anonymous authors
Paper under double-blind review
Ab stract
Attribution methods have been shown as promising approaches for identifying
key features that led to learned model predictions. While most existing attribu-
tion methods rely on a baseline input for performing feature perturbations, lim-
ited research has been conducted to address the baseline selection issues. Poor
choices of baselines limit the ability of one-vs-one explanations for multi-class
classifiers, which means the attribution methods were not able to explain why an
input belongs to its original class but not the other specified target class. Achiev-
ing one-vs-one explanation is crucial when certain classes are more similar than
others, e.g. two bird types among multiple animals, by focusing on key differenti-
ating features rather than shared features across classes. In this paper, we present
GANMEX, a novel approach applying Generative Adversarial Networks (GAN)
by incorporating the to-be-explained classifier as part of the adversarial networks.
Our approach effectively selects the baseline as the closest realistic sample belong
to the target class, which allows attribution methods to provide true one-vs-one
explanations. We showed that GANMEX baselines improved the saliency maps
and led to stronger performance on perturbation-based evaluation metrics over the
existing baselines. Existing attribution results are known for being insensitive to
model randomization, and we demonstrated that GANMEX baselines led to better
outcome under the cascading randomization of the model.
1	Introduction
Modern Deep Neural Network (DNN) designs have been advancing the state-of-the-art performance
of numerous machine learning tasks with the help of increasing model complexities, which at the
same time reduces model transparency. The need for explainable decision is crucial for earning trust
of decision makers, required for regulatory purposes Goodman & Flaxman (2017), and extremely
useful for development and maintainability.
Due to this, various attribution methods were developed to explain the DNNs decisions by attributing
an importance weight to each input feature. In high level, most attribution methods, such as inte-
grated gradient (IG) (Sundararajan et al. (2017)), DeepSHAP (Lundberg & Lee (2017)), DeepLift
(Shrikumar et al. (2017)) and Occlusion (Zeiler & Fergus (2013)), alter the features between the
original values and the values of some baseline instance, and accordingly highlight the features that
impacts the model’s decision. While extensive research has been conducted on the attribution algo-
rithms, research regarding the selection of baselines is rather limited, and it is typically treated as
an afterthought. Most existing methodologies by default apply a uniform-value baseline, which can
dramatically impact the validity of the feature attributions (Sturmfels et al. (2020)), and as a result,
existing attribution methods showed rather unperturbed output even after complete randomization
of the DNN (Adebayo et al. (2018)).
In a multi-class classification setting, existing baseline choices do not allow specifying a target class,
and this has limited the ability for providing a class-targeted or one-vs-one explanation, meaning
explaining why the input belongs to class A and not a specific class B. These explanations are
crucial when certain classes are more similar than others, as often happens for example when the
classes have a hierarchy among them. For example, in a classification task of apples, oranges and
bananas, a model decision for apples vs oranges should be based on their color rather than the shape
1
Under review as a conference paper at ICLR 2021
since both an apple and orange are round. This would intuitively only happen when asking for an
explanation of ‘why apple and not orange’ rather than ‘why apple’.
In this paper, we present GAN-based Model EXplainability (GANMEX), a novel methodology for
generating one-vs-one explanations leveraging GAN. In a nutshell, we use GANs to produce a base-
line image which is a realistic instance from a target class that resembles the original instance. A
naive use of GANs can be problematic because the explanation generated would not be specific to
the to-be-explained DNN. We lay out a well-tuned recipe that avoids these problems by incorporat-
ing the classifier as a static part of the adversarial networks and adding a similarity loss function for
guiding the generator. We showed in the ablation study that both swapping in the DNN and adding
the similarity loss are critical for resulting the correct explanations. To the best of our knowledge,
GANMEX is the first to apply GAN for explaining DNN decisions, and furthermore the first to
provide a realistic baseline image, rather than an ad-hoc null instance.
We showed that GANMEX baselines can be used with a variety of attribution methods, including IG,
DeepLIFT, DeepSHAP and Occlusion, to produce one-vs-one attribution superior compared with
existing approaches. GANMEX outperformed the existing baseline choices on perturbation-based
evaluation metrics and showed more desirable behavior under the sanity checks of randomizing
DNNs. Other than its obvious advantage for one-vs-one explanations, we show that by replacing
only the baselines and without changing the attribution algorithms, GANMEX greatly improves the
saliency maps for binary classifiers, where one-vs-one and one-vs-all are equivalent.
2	Related Works
2.1	Attribution Methods and Saliency Maps
Attribution methods and their visual form, saliency maps, have been commonly used for explaining
DNNs. Given an input x = [x1, ..., xN] ∈ RN and model output S(x) = [S1(x), ..., SC (x)] ∈ RC,
an attribution method for output i assign contribution to each pixel AS,c = [a1, ..., aN]. There are
two major attribution method families: Local attribution methods that are based on infinitesimal fea-
ture perturbations, such as gradient saliency (Simonyan et al. (2014)) and gradient*input (Shrikumar
et al. (2016)), and global attribution methods that are based on feature perturbation with respect to a
baseline input (Ancona et al. (2018)). We focus on global attribution methods since they tackle the
gradient discontinuity issue in local attribution methods, and they are known to be more effective
on explaining the marginal effect of a feature’s existence (Ancona et al. (2018)). In this paper, we
discussed five popular global attribution methods below:
Integrated Gradient (IG) (Sundararajan et al. (2017)) calculates a path integral of the model gra-
dient from a baseline image X to the input image x: IGi = (Xi 一 Xi) Jo1=0 ∂χiS(X + ɑ(x 一 X))da.
The baseline is commonly chosen to be the zero input and the integration path is selected as the
straight path between the baseline and the input.
DeepLIFT (Shrikumar et al. (2017)) addressed the discontinuity issue by performing backpropaga-
tion and assigns a score C∆xi∆t to each neuron in the networks based on the input difference to the
baseline ∆χi = Xi 一 Xi and the difference in the activation to that of the baseline ∆t = t(x) 一 t(X),
that satisfies the summation-to-delta property Pi C∆xi∆t = ∆t.
Occlusion (Zeiler & Fergus (2013); Ancona et al. (2018)) applies full-feature perturbations by re-
moving each feature and calculating the impacts on the DNN output. The feature removal was
performed by replacing its value with zero, meaning an all zero input was implicitly used as the
baseline.
DeepSHAP (Chen et al. (2019); Lundberg & Lee (2017); Shrikumar et al. (2017)) was built upon
the framework of DeepLIFT but connecting the multipliers of attribution rule (rescale rule) to SHAP
values, which are computed by ‘erasing features’. The operation of erasing one or more features
require the notion of a background, which is defined by either a distribution (e.g. uniform distribution
over the training set) or single baseline instance. For practical reasons, it is common to choose a
single baseline instance to avoid having to store the entire training set in memory.
Expected Gradient (Erion et al. (2019)) is a variant of IG that calculates the expected attribution
over a prior distribution of baseline input, usually approximated by the training set XT , meaning
2
Under review as a conference paper at ICLR 2021
EGi = Ex〜XTa〜U(o,i)(x - X)i∂χiS(X + α(x - X)) where U is the uniform distribution. In other
words, the baseline ofIG is replaced with a uniform distribution over the samples in the training set.
A crucial property of the above methods is their need for a baseline, which is either explicitly or
implicitly defined. In what follows we show that these methods are greatly improved by modifying
their baseline to that chosen by GANMEX.
2.2	The Baseline Selection Problem
Limited research has been done on the problem of baseline selection so far. A simple ”most natural
input”, such as zero values of all numerical features is commonly chosen as the baseline. For image
inputs, uniform images with all pixels set to the max/min/medium values are commonly chosen.
The static baselines frequently cause the attribution to only focus on or even overly highlight the
area where the feature values are different from the baseline values, and hide the feature importance
where the input values are close to the baseline values (Sundararajan & Taly (2018); Adebayo et al.
(2018); Kindermans et al. (2017); Sturmfels et al. (2020)).
Several none-static baselines have been proposed in the past, but each of them suffered from its
own downsides (Sturmfels et al. (2020)). Fong & Vedaldi (2017) used blurred images as baselines,
but the results are biased toward highlighting high-frequency information from the input. Bach et al.
(2015) make use of the training samples by finding the training example belonging to the target class
closest to the input in Euclidean distance. Even though the concept of minimum distance is highly
desirable, but in practice, the nearest neighbor selection in high dimensional space can frequently
lead to poor outcome, and most of the nearest neighbors are rather distant from the original input.
Along the same concept, expected gradient simply samples over all training instances instead of
identifying the closest instance (Erion et al. (2019)). Expected gradient benefits from ensembling
in a way similar to that of SmoothGrad, which averages over multiple saliency maps produced by
imposing Gaussian noise on the original image (Smilkov et al. (2017); Hooker et al. (2019)). We
claim however that averaging over the training set does not solve the issue; for example, due to
the foreground being located in different sections of the images, the average image would often
resemble a uniform baseline.
2.3	One-vs-One and One-vs-All Attribution
In multi-class settings, while one-vs-all explanation AS,co (x) was designed to explain why the input
x belong to its original class co and not the others, one-vs-one explanations aim to provide an
attribution AS,co→ct (x) ∈ RN that explains why x belong to co and not the specified target class
ct . Most existing attribution methods were primarily designed for one-vs-all explanation, but was
proposed to extend to one-vs-one by simply calculating the attribution with respect to the difference
of the original class probability to the target class probability Sdiff(x) = Sco (x) - Sct (x) (Bach et al.
(2015); Shrikumar et al. (2017)).
It is easy to think of examples where this somewhat naive formulation will not provide correct
one-vs-one explanation. Taking the example of fruit classification, for both apples and oranges the
explanation could easily be the round shape, and taking the difference between those will result in
an arbitrary attribution. We claim that without a class-targeted baseline, the modified attributions
will still omit the ”vs-one” aspect of the one-vs-one explanation. Take IG for example, AS,diff(x) =
AS,co (x)-AS,ct (x). With zero baseline, the target class score Sct (x) and its gradient will likely stay
close to zero along the straight path from the input to the zero baseline, meaning that AS,ct (x) ≈ 0
because the instance never belongs to the target class. With this in mind the one-vs-one explanation
is not very informative with respect to the target class ct .
Few class-targeted baselines were proposed in the past. The minimum distance training sample
(MDTS) described in Section 2.2 is class-targeted as the sample was selected from the designated
target class. While the original expected gradient was defined for one-vs-all explanation only, we
extended the method to one-vs-one by sampling the baselines only from the target class. However,
as mentioned in Section 2.2, MDTS is frequently hindered by the sparsity of the training set in the
high dimensional space, and expected gradient suffers from undesired effects caused by uncorrelated
training samples. The problem of baseline selection, especially for the one-vs-one explainability, has
3
Under review as a conference paper at ICLR 2021
presented a challenging problem, because the ideal baseline choice can simply be absent from the
training set.
2.4	GAN and Image-to-Image Translation
Image-to-Image Translation is a family of GAN originally introduced by Isola et al. (2017) for
creating mappings between two domains of data. While the corresponding pairs of images are rare
in most real-world dataset, Zhu et al. (2017) has made the idea widely applicable by introducing
a reconstruction loss to tackle the tasks with unpaired training dataset. Since then, more efficient
and better performing approaches have been developed to improve few-shot performance (Liu et al.
(2019)) and output diversity (Choi et al. (2020)). Nevertheless, we found the StarGAN variant
proposed by Choi et al. (2017) specifically applicable to the baseline selection problem because of
its standalone class discriminator in the adversarial networks as well as the deterministic mapping
that preserve the styles of the translated images (Choi et al. (2020)). GANs have not been applied
for explaining DNNs in the past to our best knowledge.
Prior to our work, Chang et al. (2018) proposed the fill-in the dropout region (FIDO) methods and
suggested generators including CA-GAN Yu et al. (2018) for filling in the masked area. However,
the CA-GAN generation was designed for calculating the smallest sufficient region and smallest de-
stroying region Dabkowski & Gal (2017) that only produced 1-vs-all explanations. FIDO is compu-
tationally expensive as an optimization task is required for each attribution map. The fill-in method
requires an unmasked area for reference, hence only works for a small subset of attribution meth-
ods. More importantly, the FIDO is highly dependent on the generator’s capability of recreating the
image based on partially masked features. With pre-trained generators like CA-GAN, we argue that
the resulting saliency map is more associated with the pre-trained generator instead of the classifier
itself.
3	GAN-based Model Explainab ility
It has been previously established that attribution methods are more sensitive to features where the
input values are the same as the baseline values and less sensitive to those where the input values and
the baseline values are different (Adebayo et al. (2018); Sundararajan & Taly (2018)). Therefore,
we expect a well-chosen baseline to differ from the input only on the key features. Good candidates
for achieving this would be the sample in the target class but with minimum distance to the input.
Formally, for a one-vs-one attribution problem AS,co→ct (x), We define the class-targeted baseline
to be the closest point in the input space (not limited to the train set) that belongs to the target class
Bc (x) = arg min ∣∣x — Xk	(1)
X∈Gct
Here, Gct is the set of realistic examples in the target class, and ∣ ∙ ∣ is the Euclidean distance.
By using this baseline we have AS,co→ct (x, Bct (x)) providing the explanations as to why input
x belongs to its original class co and not class ct. Now, since it isn’t realistic to optimize within
the actual set Gct We work with a softer version of Equation 1: Bct(x) = arg minχ∈RN(∣x 一
Xk — logT(X, ct)). where T(X, Ct) represent the probability of X belonging to the target class,
meaning X ∈ Gct. Given a classifier S(X) = [Si(x), ..., SC(x)], we have the estimate Sc(X) to
the probability of a realistic image X to be in class c. In order to make use of this we decompose
T(X, Ct) = R(X)Sct (X) where R(X) indicates the probability of X being a realistic image. We end
up with the following objective for the baseline instance.
Bct (x) = arg min (∣∣x — Xk- log R(X)- log Sct (X))	(2)
x∈RN
3.1	Applying StarGAN to the Class-Targeted Baseline
Here we introduce GAN-based Model EXplainability (GANMEX) that uses GAN to generate the
class-targeted baselines. Given an input X and a target class Ct, GANMEX aims to generate a class-
targeted baseline G(X, Ct) that achieve the three following objectives:
1.	The baseline belongs to the target class (with respect to the classifier).
2.	The baseline is a realistic sample.
4
Under review as a conference paper at ICLR 2021
Class 1 Decision boundry
Class 0
Area representing
realistic samples
■ Area representing
training datasets
Figure 1: Intuition of using GANs for generating class-targeted baselines in SVHN dataset. Without GANs,
a closest target class sample can easily be unrealistic (a), while the GAN helps confine the sample in the
realistic sample space (b). The MDTS baseline and other training samples used in expected gradient can be
very different from the input (c). (d) shows the zero baseline that is the most commonly used.
3.	The baseline is close to the input.
To further explain the need for all 3 objectives, we point the reader to Figure 1. The GANMEX
baseline represents the ”closest and realistic target class baseline”. Without the assistance of GANs,
the selected baseline can easily either fall into the domain of unrealistic image. A naive fix will
choose a realistic image from the training set, but that will not be close to the input. Finally, for
correct one-vs-one explainability we need the baseline to belong to a specific target class. We have
provided more intuitions behind the baseline selection requirements in Appendix G.
We chose StarGAN (Choi et al. (2017)) as the method for computing the above T or rather R
function. Although many Image-to-Image translation methods could be applied to do so, StarGAN
inherently works with multi-class problems, and allows for a natural way of using the already trained
classifier S as a discriminator, rather than having us train a different discriminator.
StarGAN provides a scalable image-to-image translation approach by introducing (1) a single gen-
erator G(x, c) accepting an instance x and a class c, and producing a realistic example x in the target
class c, (2) two separate discriminators: Dsrc (x) for distinguishing between real and fake images,
and Dcls(x, c) for distinguishing whether x belongs to class c. It introduced following loss functions
Ladv	=	Ex [log(Dsrc(x))] + Ex,c [log(1	- Dsrc(G(x,	c)))]	(3)
Lrls	=	Ec0,χ∈c0 [- log(Dcis(Clx))]	(4)
Lcfls	=	Ex,c[-log(Dcls(c|G(x,c)))]	(5)
Lrec	=	Ec,c0,x∈c0[kx - G(G(x, c), c0)k1]	(6)
Here, E. [] defines the average over the variables in the subscript, where x is an example in the
training set, and c, c0 are classes. Ladv is the standard adversarial loss function between the generator
and the discriminators, Lcrls and Lcfls are domain classification loss functions for real images and fake
images, respectively, and Lrec is the reconstruction loss commonly used for unpaired image-to-image
translation to make sure two opposite generation action will lead to the original input. The combined
loss functions for the generator and the discriminator are
LD	= -Ladv +λcrlsLcrls	(7)
LG	= Ladv + λcflsLcfls + λrecLrec	(8)
The optimization procedure for StarGAN alternates between modifying the discriminators Dsrc(x),
Dcls(x, c) to minimize LD, and the generator G to minimize LG.
Equation 8 is almost analogical to equation 2. The term Ladv corresponds to - log R(x) and the
term λflsLfls corresponds to the term log SCt (X). There is a mismatch between the term λrecLreC and
kx — x∣∣. One forces the generator to be invertible, while the other forces the generated image to
be close to the original. We found that the Lrec term is useful to encourage the convergence of the
GAN. However, a similarity term kx - G(x, c)k is also needed in order for the baseline image to
be close to the origin - this allows for better explainability. We show in what follows (Figure 7.B,
5
Under review as a conference paper at ICLR 2021
Baselines Integrated Gradient
InPUt MDTS GANMEX Zero MDTS GANMEX
De 即 LIFT Occlusion	DeePSHAP
Zero MDTS GANMEX Zero MDTS GANMEX Zero MDTS GANMEX
(A)
0->6
8->9
NH>S
8->9
(B)
dog ->
airplane
deer
-> horse
horse
-> deer
car
-> ship
InPUt
0->6
0 6 G
OTH <LL.0
夕
1
Z
Figure 2: (A) Saliency maps for multi-class datasets (MNIST, SVHN, CIFAR10) generated with various base-
lines, including zero baseline (Zero), MDTS and GANMEX, with classes co → ct indicated for each exam-
ple. (B) Mis-classification analysis showing with the (mis-classified class, correct class) pairs. The baseline
columns show the expected images generated by GANMEX for the correct classes, and the saliency maps show
the explanation of ”why not the correct class” produce by IG, DeepLIFT (DL) and occlusion (Occ).
Appendix D) that without this similarty term, the created image can indeed be farther away from the
origin. Other than the added similarity term, for GANMEX We replace the discriminator DclS (c|X)
with the classifier Sc (X), since as mentioned above, this way the generator provides a baseline
adapted to our classifier. Concluding, We optimize the folloWing term for the generator
LG = lθg(1 - DSrC (X))- λds lθg(Sc (x)) + λrec Ilx - G(X, C0 )∣∣1 + λsim Ilx - X∣∣1	(9)
where x is short for G(x, c). Notice that we used L1 distance rather than L2 for the similarity
loss, because L2 distance leads to blurring outputs for image-to-image translation algorithms (Isola
et al. (2017)). Other image-to-image translation approaches can potentially select baselines satisfy-
ing the criteria (2) and (3) above, but they lack the replaceable class discriminator component, that
is crucial for explaining the already trained classifier. We provide several ablation studies in Ap-
pendix D where we show that without incorporating the to-be-explained classifier to the adversarial
networks, the GAN generated baselines will fail the randomization sanity checks. We provide more
implementation details including hyper-parameters in Appendix A.2.
6
Under review as a conference paper at ICLR 2021
—IntGrad
---IntGrad + GANMEX
——Expected Gradient
---DeepLIFT
——DeepLIFT + GANMEX
—Occlusion
---Occlusion + GANMEX
MNIST
① U=OSBqXWIAINVD
əu=①Seq OM
Figure 3: (A-C) Perturbation-based evaluation plots for MNIST and SVHN, respectively. The dashed lines
represent the non-class-targeted baselines and the solid lines represent class-targeted baselines. (D-F) Gini
indices, with the yellow bars represent saliency maps with zero baselines and the green bars represent that of
GANMEX baselines. (G) Sanity checks showing the original saliency maps (Orig) and saliency maps under
cascading randomization over the four layers: output layer (Output), fully connected (FC), and two CNN layers
(CNN1, CNN2).
4	Experiments
In what follows we experiment with the datasets MNIST (LeCun & Cortes (2010)), Street-View
House Numbers (SVHN) (Netzer et al. (2011)), CIFAR10 (Krizhevsky (2009)) and apple2orange
(Zhu et al. (2017)). Further details about the datasets and classifiers are given in Appendix A.1.
Our techniques are designed to improve any global attribution method by providing an improved
baseline. In our experiments we consider four attribution methods - IG, DeepLIFT, Occlusion, and
DeepSHAP. The baselines we consider include the zero baseline (the default baseline in all four
methods), minimum distance training sample (MDTS), and the GANMEX baseline. We also com-
pared our results with a modified version of expected gradient aimed to provide 1-vs-1 explanations,
which runs IG over a randomly chosen target class image from the training set, as opposed to a
random image from the training set.
4.1	One-vs-one attribution for Multi-class Classifiers
We tested the one-vs-one attribution on two multi-class datasets - MNIST, SVHN, and CIFAR10. As
shown in Figure 2.A, the GANMEX baseline successfully identified the closest transformed image
in the target class as the baseline. Take explaining why 0 and not 6 for example, the ideal baseline
would keep the ”C”-shape part unchanged, and only erase the top-right corner and complete the
lower circle, which was achieved by GANMEX. Limited by the training space, MDTS baselines
were generally more different from the input image. Therefore, the explanation made with respect
to GANMEX baselines were more focused on the key features compared to that of the MDTS base-
line and expected gradient. We observed the same trends across more numbers, where GANMEX
helps IG, DeepLIFT, Occlusion and DeepSHAP disregard the common strokes between the original
7
Under review as a conference paper at ICLR 2021
and targeted digits, and focusing only on the key differences. The out-performance of GANMEX
was even more obvious in the SVHN datasets, where the numbers can have any font, color, and
background, and in CIFAR10, which has more complexity and diversity. Notice that both the zero
baseline and training set baseline cause the explanation to have more focus on the background, and
in contrast, the GANMEX example focuses only on the key features that would cause the digit to
change.
Zero baselines, on the other hand, were generally unsuccessful in making one-vs-one explanations.
The attributions on MNIST look similar to the original input and ignores everything in the back-
ground, and the attributions on SVHN were rather noisy. As shown in Figure 8, attributions based
on zero baselines only changed marginally with different target classes. This shows that purposely
designed class-targeted baselines are required for meaningful one-vs-one explanation.
Mis-classification Analysis We next demonstrated how one-vs-one saliency maps can be used
for trouble-shooting mis-classification cases. For an input x that belongs to class co but was mis-
classified as class cm. AS,cm→co (x) = AS,cm→co (x, Bco (x)) provides explanation to why x be-
longs to cm and not co with respect to the trained classifier, and this will help human understand
how the classifier has led to the incorrect decision. We provided examples in Figure 2.B where the
samples were mis-classified. For MNIST and SVHN samples, the mis-classification mostly hap-
pened when the digits were presented in a non-typical way. The GANMEX baseline Bco (x) show
how a more typical digit should have been written according to the trained classifier, and the attri-
bution AS,cm→co(x) highlights the area that led to the mis-classification. CIFAR10 presented more
complex classification challenges, and the classifier can easier confused ship with an airplane be-
cause of the pointy front and the lack of sea horizon, or a dog with a cat because of the shape of the
ears and the nose, and those areas were highlighted in the one-vs-one saliency maps.
Perturbation-based evaluation We followed the perturbation-based evaluation suggested by
Bach et al. (2015) that flips input features starting from the ones with the highest saliency values
and evaluates the cumulative impacts on the score delta Sco - Sct as proposed by Shrikumar et al.
(2017). Flipping a feature means to provide with a value of 1 - x where x is its original value,
assuming all features are normalized to x ∈ [0, 1]. A wanted behavior from the attribution map
is that the score delta will decrease as rapidly as possible as we flip the features one by one. We
provide in Figure 3.A-C the perturbation curves for both MNIST, SVHN and CIFAR10, plotting the
score delta as a function of the number of flipped features. It is painfully clear that that by using a
GANMEX baseline rather than the alternative zero baseline, the descent of the curve is much faster,
meaning that we successfully capture the most important features using GANMEX. This holds true
for all attribution methods. As a side note, notice that in SVHN when we flip all features the score
delta goes back to where it was in the beginning as opposed to going down to zero. This is due to
the fact that once all features are flipped, we are back to having the same digit as before.
Gini-index We calculated the Gini Index representing the sparseness of the saliency maps as pro-
posed by Chalasani et al. (2018), where a larger score means sparser saliency map, which is a desired
property. Figure 3.D-F shows our experiments comparing the different techniques with a zero base-
line vs. GANMEX. Other than IG/SVHN where GANMEX has a visible advantage, the results are
roughly the same; we suspect that the sparseness of zero baseline attribution was benefited from
incorrectly hiding key features, as shown in Figure 2.A and 3.A-C. Expected gradient, on the other
hand, consistently under-performs its counterpart (IG+GANMEX) in both datasets.
Cascading randomization We performed the sanity checks proposed by Adebayo et al. (2018)
that performs cascading randomization from top to bottom layers of the DNN and observe the
changes in the saliency maps. Specifically, layer by layer we replace the model weights with Gaus-
sians random variables scaled to have the same norm. For meaningful model explanations, we would
expect the attributions to be gradually randomized during the cascading randomization process. In
contrast, unperturbed saliency maps during the model randomization would suggest that the attribu-
tions were based on general features of the input and not specifically based on the trained model.
Figure 3.G shows the experiment on MNIST data with the network layers named (input to output)
CNN1, CNN2, FC, Output. It shows that even though the saliency maps generated by the original
IG, DeepLIFT and Occlusion were rather unperturbed (still showing the shape of the digit) after the
8
Under review as a conference paper at ICLR 2021
Figure 4: Saliency maps for the classifier on the apple2orange dataset with four baseline choices: zero baseline
(Zero), maximum value baseline (Max), blurred baseline (Blur), and GANMEX baseline (GANMEX).
model randomization, with the help of GANMEX, both the baselines and the saliency maps were
perturbed over the cascading randomization. Expected gradient, while showing more randomization
compared to the zero baseline saliency maps, still roughly shows the shape of the digit throughout
the sanity check.
4.2	Attribution for B inary Classifiers
In addition to the one-vs-one aspect, GANMEX generally improves the quality of the saliency maps
compared with the existing baselines, and this can be tested on binary datasets where the one-vs-one
explanations and the one-vs-all explanations are equivalent. For apple2orange dataset, conceptually
apples and oranges both have round shapes but have different colors, so we would expect the saliency
maps on a reasonably performing classifier to highlight the colors of the fruit, but not the shapes,
and definitely not the background.
In Figure 4 and Figure 9 we compared the saliency map generated by DeepLIFT and IG with the
zero input, max input, blurred image, with those generated with the GANMEX baselines. In all
non-GANMEX baselines (Zero, Max, Blur) we commonly observe one of two errors in the saliency
map. The first error consists of highlighting the background. The second highlights only the edge
of the apple(s) providing the false indication that the model is basing its decisions on the shape of
the object rather than its color. It is quite clear that neither of these errors occur when using the
GANMEX baselines as the background is never present and the full shape of the apple(s)/orange(s)
is highlighted.
5	Conclusion and future work
We have proposed GAN-based model explainability, a novel approach for generating one-vs-one ex-
planation baselines without being constrained by the training set. We used the GANMEX baselines
in conjunction with IG, DeepLIFT, SHAP, and Occlusion, and to our surprise, the baseline replace-
ment was all it takes to address the common downside of the existing attribution methods (blind to
certain input values and fail to randomize with the model randomization) and significantly improve
the one-vs-one explainability. The out-performance was demonstrated through perturbation-based
evaluation, sparseness measures, and cascading randomization sanity checks. The one-vs-one expla-
nation achieved by GANMEX opens up possibilities for obtaining more insights about how DNNs
differentiate similar classes.
While GANMEX showed promising results on explaining binary classifiers, where one-vs-all and
one-vs-one explanations are directly comparable, open questions remain on how to apply GANMEX
to one-vs-all explainability for multi-class classifiers, and how to best optimize the GAN component
to effective generate baselines for classification tasks with large number of classes.
9
Under review as a conference paper at ICLR 2021
References
Julius Adebayo, Justin Gilmer, Michael Muelly, Ian Goodfellow, Moritz Hardt, and Been Kim.
Sanity checks for saliency maps. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 9505-9515. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
8160-sanity- checks-for-saliency-maps.pdf.
David Alvarez-Melis and Tommi S. Jaakkola. Towards robust interpretability with self-explaining
neural networks, 2018.
Marco Ancona, Enea Ceolini, Cengiz Oztireli, and Markus Gross. Towards better understand-
ing of gradient-based attribution methods for deep neural networks. In International Confer-
ence on Learning Representations, 2018. URL https://openreview.net/forum?id=
Sy21R9JAW.
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PLOS ONE, 10(7):1-46, 07 2015. doi: 10.1371/journal.pone.0130140.
URL https://doi.org/10.1371/journal.pone.0130140.
Prasad Chalasani, Jiefeng Chen, Amrita Roy Chowdhury, Somesh Jha, and Xi Wu. Concise ex-
planations of neural networks using adversarial training. CoRR, abs/1810.06583, 2018. URL
http://arxiv.org/abs/1810.06583.
Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duvenaud. Explaining image
classifiers by adaptive dropout and generative in-filling. CoRR, abs/1807.08024, 2018. URL
http://arxiv.org/abs/1807.08024.
Hugh Chen, Scott Lundberg, and Su-In Lee. Explaining models by propagating shapley values of
local components, 2019.
Yunjey Choi, Min-Je Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo.
Stargan: Unified generative adversarial networks for multi-domain image-to-image translation.
CoRR, abs/1711.09020, 2017. URL http://arxiv.org/abs/1711.09020.
Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis
for multiple domains. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, 2020.
Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In I. Guyon,
U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems, volume 30, pp. 6967-6976. Curran Asso-
ciates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/
0060ef47b12160b9198302ebdb144dcf- Paper.pdf.
Gabriel G. Erion, Joseph D. Janizek, Pascal Sturmfels, Scott Lundberg, and Su-In Lee. Learn-
ing explainable models using attribution priors. CoRR, abs/1906.10670, 2019. URL http:
//arxiv.org/abs/1906.10670.
Ruth Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturba-
tion. CoRR, abs/1704.03296, 2017. URL http://arxiv.org/abs/1704.03296.
Bryce Goodman and Seth Flaxman. European union regulations on algorithmic decision-making
and a “right to explanation”. AI magazine, 38(3):50-57, 2017.
Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim. A benchmark for interpretabil-
ity methods in deep neural networks, 2019.
Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. CoRR, abs/1704.04861, 2017. URL http://arxiv.org/abs/
1704.04861.
10
Under review as a conference paper at ICLR 2021
P. Isola, J. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial
networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
5967-5976, 2017.
Pieter-Jan Kindermans, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T Schutt, Sven
Dahne, Dumitru Erhan, and Been Kim. The (Un) reliability of Saliency methods. arXiv preprint
arXiv:1711.00867, 2017.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.
Yann LeCun and Corinna Cortes. MNIST handwritten digit database. 2010. URL http://yann.
lecun.com/exdb/mnist/.
Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, and Jan Kautz.
Few-shot unsupervised image-to-image translation. CoRR, abs/1905.01723, 2019. URL http:
//arxiv.org/abs/1905.01723.
Scott M Lundberg and Su-In Lee. A unified approach to interpreting model pre-
dictions. pp. 4765-4774, 2017. URL http://papers.nips.cc/paper/
7062- a- unified- approach- to- interpreting- model- predictions.pdf.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
An phi Nguyen and Mar´a Rodriguez Martinez. On quantitative aspects of model interpretability,
2020.
W. Samek, A. Binder, G. Montavon, S. Lapuschkin, and K. Muller. Evaluating the visualization of
what a deep neural network has learned. IEEE Transactions on Neural Networks and Learning
Systems, 28(11):2660-2673, 2017.
Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box:
Learning important features through propagating activation differences. CoRR, abs/1605.01713,
2016. URL http://arxiv.org/abs/1605.01713.
Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through
propagating activation differences. CoRR, abs/1704.02685, 2017. URL http://arxiv.org/
abs/1704.02685.
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional networks:
Visualising image classification models and saliency maps. CoRR, abs/1312.6034, 2014. URL
http://arxiv.org/abs/1312.6034.
Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda B. Viegas, and Martin Wattenberg. Smooth-
grad: removing noise by adding noise. CoRR, abs/1706.03825, 2017. URL http://arxiv.
org/abs/1706.03825.
J.T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all
convolutional net. In ICLR (workshop track), 2015. URL http://lmb.informatik.
uni-freiburg.de/Publications/2015/DB15a.
Pascal Sturmfels, Scott Lundberg, and Su-In Lee. Visualizing the impact of feature attribution
baselines. Distill, 2020. doi: 10.23915/distill.00022. https://distill.pub/2020/attribution-baselines.
Mukund Sundararajan and Ankur Taly. A note about: Local explanation methods for deep neural
networks lack sensitivity to parameter values. CoRR, abs/1806.04205, 2018. URL http://
arxiv.org/abs/1806.04205.
Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. CoRR,
abs/1703.01365, 2017. URL http://arxiv.org/abs/1703.01365.
Richard Tomsett, Dan Harborne, Supriyo Chakraborty, Prudhvi Gurram, and Alun Preece. Sanity
checks for saliency metrics, 2019.
11
Under review as a conference paper at ICLR 2021
Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Generative image
inpainting with contextual attention, 2018.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. CoRR,
abs/1311.2901, 2013. URL http://arxiv.org/abs/1311.2901.
Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image trans-
lation using cycle-consistent adversarial networks. CoRR, abs/1703.10593, 2017. URL http:
//arxiv.org/abs/1703.10593.
12
Under review as a conference paper at ICLR 2021
A Implementation Details
A. 1 Datasets and Classifiers
MNIST (LeCun & Cortes (2010)) The classifier consists of two 6x6 CNN layers with a stride of 2,
followed by a 256-unit fully connected layer, a dropout layer with p = 0.5, and the 10 output neu-
rons. As shown in Springenberg et al. (2015) the stride>1 CNN achieved comparable performance
with pooling layers. The classifier was trained for 50 epochs and achieve a test accuracy of 99.3%.
Street-View House Numbers (SVHN) (Netzer et al. (2011)) We tested our models on the cropped
version of SVHN and used the same model architecture with that of MNIST and achieved a test
accuracy of 90.3% after 50 epochs of training.
CIFAR10 (Krizhevsky (2009)) We trained a classifier consist of 4 repetitive units, with each unit
constructed by two 3x3 CNN layers and a 2x2 average pooling layer, with each CNN layer followed
by a batch normalization layer. The classifer achieved 87.8% test accuracy after 100 epochs of
training.
apple2orange (Zhu et al. (2017)) We trained a classifier taking the original 256x256 image as in-
put. The classifier was constructed by adding a global average pooling layer on top of MobileNet
(Howard et al. (2017)), and then followed by a dense layer of 1024 neurons and a dropout layer
of p = 0.5 before the output neurons. The classifier was trained for 50 epochs and achieve a test
accuracy of 87.7%.
A.2 Baseline Generation with GANMEX
Our baseline generation process is based on StarGAN (Choi et al. (2017)). We used the Tensorflow-
GAN implementation (https://github.com/tensorflow/gan) and made the following two modifications
(Equation 9):
1.	The class discriminator Dcls is replace by the target classifier S to be explained.
2.	A similarity loss Lsim is added to the training objective function.
We train the GANMEX model for 100k steps for the MNIST and apple2orange datasets, 300k steps
for the SVHN dataset, and 400k steps for the CIFAR10 dataset. Only the train split is used for
training, and the attribution results and evaluation were done on the test split of the dataset.
A.3 Attribution Methods
We used DeepExplain (https://github.com/marcoancona/DeepExplain) for generating saliency maps
with IG, DeepLIFT, and Occlusion. We modified the code base to use the score delta (Sco - Sct)
instead of the original class score (Sct) and allowing replacing the zero baseline (see Section 2.1)
by custom baselines from GANMEX and MDTS. Expected gradient was separately implemented
according to the formulation in Erion et al. (2019). We set the number of sampling steps to 200 for
both IG and Expected Gradient, and used Occlusion-1 that only perturb the pixel itself (as supposed
to perturbing the whole neighboring patch of pixels).
The DeepSHAP saliency maps were calculated using SHAP (https://github.com/slundberg/shap).
We made similar modification to replace the original class score by the score delta and feed in the
custom baseline instances.
In all saliency maps shown in the paper, blue color in indicates positive values and red color indicates
negative values. We skipped Occlusion for large images (apple2orange) and also skipped SHAP for
full dataset evaluations due to the computation resource constraints.
B Baseline Distance Analysis
To measure how various baseline selection approaches satisfy the minimum distance requirements in
Equation 1, we calculated D(x,X) = ∣∣x -Xk for(1)GANMEX, (2) MDTS, (3) a randomly selected
sample in the target class as baseline and (4) zero baseline. GANMEX was on-par with MDTS on
the MNIST dataset, but on SVHN and CIFAR10 dataset that have more degrees of freedom (object
13
Under review as a conference paper at ICLR 2021
(A)
(B)
IUn0。
2500
(C) input GAN
Lo
MDTS
3.1
(D)
Input GAN
MDTS
l->9
8->2 I
3->2
2->3
7->9
Lo 2.1
1.0 3.6
1.0 4.3
3.0 8.1
5->8 SIQSI
8.ι
2 >8 EEE
Figure 5: (A) Vertical edge area in an SVHN image. (B) Histogram of sample to baseline distance (Dedge (x, X))
in the vertical edge area. (C-E) Samples comparing GANMEX and MDTS baselines with Dedge (x, x) indicated
on the top of the baseline images. (C) Easy cases for GANMEX (Dedge(x, x) ≈ 1). (D) Difficult cases for
GANMEX (Dedge(x, X) ≈ 3). (E) Difficult cases for GANMEX (Dedge(x, X) ≈ 6).
Table 1: Baseline distance analysis comparing the average intra-class distance (Dintra, sampled) and the average
inter-class distance (Dinter, sampled), with the average distance from the instance to the baseline input generated
by GANMEX (GAN), MDTS, random selection (RAND), and zero inputs (Zero).
	Dimension	Data Size		Avg. Distance		Baseline Distance			
		Train	Test	Dintra	Dinter	GAN	MDTS	Rand	Zero
MNIST	784 (28 x 28 x 1)	60,000	10,000	8.96	10.32	7.17	7.18	10.32	9.28
SVHN	3072(32x32x3)	73,257	26,032	14.42	14.48	3.42	5.94	15.44	26.52
CIFAR10	3072(32x32x3)	50,000	10,000	18.28	19.06	6.89	10.54	19.01	29.04
14
Under review as a conference paper at ICLR 2021
size, color, orientation, background, ...), GANMEX was significantly better in identifying minimum
distance baselines compared to the in-sample search. The high dataset complexity of was supported
by the average intra-class distance, the average distance between any two instances within the same
class, which was higher than that of MNIST. Note that the resulting sample to baseline distance
D(x, XGANMEX) is much higher in MNIST than in SVHN, because there were more boundary values
(0s and 1s) in MNIST.
We further evaluated the similarity distance on the vertical edge area (Dedge (x, X)) of the SVHN im-
ages (Figure5.A). Empirically, we observed that the digits of interest were rarely present in the ver-
tical edge, and therefore, We would expect a closest baseline choice will lead to minimal Dedge (x, X)
under the minimum distance requirements. We provided a histogram in Figure 5.B for comparing
the distribution of Dedge(x, X) for MDTS and GANMEX, and we presented sampled success/failure
cases in Figure 5.C-E. Overall, GANMEX leads to baselines that are closer to the original samples.
C Additional Metrics
Here we evaluated the different baseline choices with additional metrics (Table 2). AOPCL mea-
sures the area over the perturbation curve within the first L perturbation steps (Samek et al. (2017);
Tomsett et al. (2019)). One potential downside of AOPCL is that the metric is only sensitive to
the top L features in the saliency map and not the rest. Therefore, in addition to AOPCL=100, we
calculated AOPCall, the area over the perturbation curve across all feature. The gradient family of
IG and expected gradient generally outperformed DeepLIFT and occlusion on the AOPC metrics,
with IG+GANMEX performed the best overall.
The sparsity (Chalasani et al. (2018)) measured by the Gini index is a desirable property for one-
vs-one attribution. We expect a good one-vs-one explanation to highlight only the differentiating
features. Compared with one-vs-all saliency maps, one-vs-one saliency maps should highlight a
smaller subset of features, especially when the target classes are similar to the original classes.
Therefore, one would expect more sparse one-vs-one saliency maps are more likely to be correct.
Our results showed that saliency maps generated by IG+GANMEX and IG+MDTS have higher Gini
index and therefore are more sparse compared to other methods (Table 2).
We also measured the faithfulness reported by Alvarez-Melis & Jaakkola (2018); Tomsett et al.
(2019) and monotonicity suggested by phi Nguyen & Martlnez (2020). Instead of measuring the
cumulative effect of alternating a set of features, both faithfulness and monotonicity measure the
impacts on alternating single features. We found that expected gradient and Occlusion+zero baseline
are the best performers on those two metrics.
Lastly, we designed an inverse localization metric to measure whether the explanation is localized in
the focus area. We observed that the digits mostly have < 1 aspect ratios, meaning that their widths
are smaller then their heights. As a results, the areas at the two vertical edges are generally not
covered by the primary numbers, and instead, they usually show the background or the neighboring
numbers. Therefore, we can reasonably expect the saliency map sensitivity to be location in the
center area (area excluding the vertical edges), and not the vertical edge area (Figure 5.A).
Based on this observation, we designed the inverse localization metric, L(A(x)) =
(card1Sed8e) Pi∈Sedge 1Ai(X) |)/( Card(Scenter) Pi∈ScenJ Ai(X) 1 )，that calculates the ratio of the average
absolute sensitivity between the vertical edge area and the center area. Scenter and Sedge represent
the feature set in the center area and the edge area, respectively. card(.) measures the cardinality of
feature sets, and X and A(X) are the sample and the corresponding saliency map. A lower L(A(X))
would mean that the saliency map A(X) is more localized in the focus area.
As shown in Table 2, we see a consistent trend of the saliency maps with GANMEX baselines
being more localized (lower inverse localization) compared to MDTS baselines across all attribution
methods, and expected gradient and the zero baselines generally lead to the worst results. The
saliency maps produced by occlusion+GANMEX was the most localized among all the methods
tested.
To summarize, we evaluated multiple attribution methods and baseline combinations with metrics
that assess different properties of the saliency maps. While the results of faithfulness and monotinic-
ity were less consistent, GANMEX has consistently lead to better metrics on inverse localization,
15
Under review as a conference paper at ICLR 2021
Table 2: Additional metrics for attribution methods using the zero baseline (Zero), MDTS, and GANMEX
(GAN).
Integrated Gradient EG	DeepLIFT	Occlusion
Metrics	Dataset	Zero	MDTS	GAN		Zero	MDTS	GAN	Zero	MDTS	GAN
AOPC100	MIST	0.614	1.249	1.421	1.260	0.505	0.639	0.724	0.705	1.050	1.221
	SVHN	0.346	0.861	0.921	0.878	0.377	0.634	0.621	0.317	0.547	0.549
	CIFAR10	0.298	0.485	0.494	0.516	0.323	0.464	0.451	0.441	0.492	0.440
AOPCall	MIST	0.889	1.098	1.263	1.13	0.859	0.933	0.992	0.877	1.019	1.114
	SVHN	0.564	0.844	0.822	0.626	0.649	0.750	0.751	0.528	0.586	0.585
	CIFAR10	0.696	0.788	0.808	0.789	0.726	0.780	0.794	0.628	0.694	0.706
sparsity	MIST	0.909	0.911	0.919	0.827	0.047	0.047	0.046	0.062	0.058	0.058
	SVHN	0.606	0.713	0.783	0.615	0.131	0.133	0.139	0.168	0.139	0.144
	CIFAR10	0.565	0.639	0.626	0.522	0.164	0.171	0.169	0.325	0.260	0.223
faithfulness	MIST	0.182	0.224	0.280	0.407	0.075	0.003	0.031	0.257	0.254	0.291
	SVHN	0.017	0.265	0.270	0.548	-0.041	0.075	0.017	0.007	0.306	0.243
	CIFAR10	0.005	0.028	0.027	0.054	0.003	0.017	0.017	0.288	0.285	0.225
monotonicity	MIST	0.118	0.196	0.264	0.357	0.087	0.150	0.206	0.244	0.239	0.280
	SVHN	0.129	0.212	0.248	0.340	0.095	0.150	0.182	0.057	0.210	0.211
	CIFAR10	0.008	0.050	0.042	0.058	0.004	0.044	0.033	0.175	0.140	0.087
inv. localization	SVHN	0.268	0.128	0.113	0.217	0.268	0.156	0.123	0.268	0.144	0.100
AOPCL , AOPCall and sparsity, especially when compared with the non-class-targeted zero baseline
that were widely used in the field.
D	Ablation Studies
Here we analyzed the possibilities of using other GAN models. Different from StarGAN, most
other image-to-image translation algorithms do not have a stand-alone class discriminator that can
be swapped with a trained classifier. To simulate such restrictions, we trained a similar GAN model
but with the class discriminator trained jointly with the generator from scratch. Figure 7.A shows that
while the stand-alone GAN yields similar baseline with GANMEX, both the baselines and saliency
maps of the stand-alone GAN remains unperturbed under cascading randomization of the model.
This indicates that the class-wise explanations provided by stand-alone GAN were not specific to
the to-be-explained classifier.
The importance of the similarity loss in Equation 9 can be demonstrated on a colored-MNIST
dataset, where we randomly assigned the digits with one of the three colors {red, green, blue}, with
labels of the instances remain unchanged from the original MNIST labels of {0, ..., 9}. The classifier
was trained with the same model architecture and training process as for MNIST.
The dataset demonstrated different modes (colors in this case) that are irrelevant to the labels, and
we would expect the class-targeted baseline for x would be another instance that has the same color
as x. Figure 7.B shows that the similarity loss is the crucial component for ensuring that the baseline
has the same color with the input. Without the similarity loss, the generated baseline instance can
easily have a different color with the original image. The reconstruction loss itself does not provide
the same-mode constraint because a mapping of G(red) → green and G(green) → red does not get
penalized by the reconstruction loss. While the reconstruction loss was not required for GANMEX
and the same-mode constraint, we observed that some degrees of reconstruction loss help GANs
converge faster.
E	Hyper-parameter Analysis
We tested how the generated baselines change with respect to the hyperparemeters in the GANMEX
loss function. The hyper-parameters, λcfls, λrec, and λsim, presented in Equation 9 control the degrees
of the classification loss, reconstruction loss, and similarity loss, respectively. We performed the
hyper-parameter scan on the SVHN dataset as it has enough complex and yet simple enough for
visually assessing the attribution.
16
Under review as a conference paper at ICLR 2021
(A)
Input
Stand-alone GAN
Orig Output FC CNN2 CNNl
0
IntGrad
Occlusion
DeepLIFT
Baseline
IntGrad
Occlusion
DeepLIFT
Figure 6: (A) Cascading randomization on baselines generated by a stand-alone GAN lead to little randomiza-
tion on the saliency maps. (B) Colored-MNIST dataset. GAN baselines generated with both similarity loss and
reconstruction loss (S+R), similarity loss only (S), reconstruction loss only (R), and none of those (NA). Only
S+R and S successfully constrained the baselines in the same modes (colors) with the inputs.
17
Under review as a conference paper at ICLR 2021
Table 3: Run Time Analysis comparing the attribution computation time for IG, expected gradient (EG),
DeepLIFT (DL) and Occlusion (Occ), as well as the baseline generation time for EG, GANMEX (GAN),
and MDTS. The computation was performed on a single Tesla V100 GPU, and the compute time was measure
in seconds on calculation over all samples for the dataset, and the baseline generation time is the additional
compute time on top of the attribution methods. (§) We selected the same sampling number for IG and EG, and
the baseline selection time of EG was estimated by the complexity difference of EG and IG.(f) The attribution
inference time for CIFAR10 was measured in 10 separate batches due to the memory constraint. ⑴ MDTS
search was performed on CPU instead of GPU.
Dataset	Attribution Inference	Baseline Generation	GAN Training Size Dim. IG	EG	DL Occ	EG§ GAN	MDTS*	Steps t (hour)
MNIST SVHN CIFARIOt	10k	784	43.5	64.1	0.6	75.7	20.6	92.5	850.8	100k	5.2 26k	3072	557.3	658.9	1.9	4917.0	101.6	399.8	3674.4	300k	18.2 10k	3072	239.4	294.8	30.0	1532.1	55.3	959.7	1085.8	400k	23.8
Classification Loss (λcfls) Low classification loss tended to make some transformation unsuccessful,
and high classification loss introduced additional noise that make the images unrealistic.
Similarity Loss (λrec) Similarity loss is the key component for minimum distance optimization. As
we have shown in Section D and Figure 7.B, at zero similarity loss, the generator is only constraint
by the reconstruction loss and can lead to incorrect font colors and background. High similarity loss,
on the other hand, makes the baselines to be too similar to the original images.
Reconstruction Loss (λsim) As we have mentioned in Section D and Figure 7.B, reconstruction loss
is not required for GANMEX, but it slightly helps GAN to converged. In contrast, high reconstruc-
tion loss can lead to incorrect outputs.
F Compute Time Analysis
In Table 3, we measure the GANMEX compute time compared with various attribution methods.
While the GAN component takes 5-23 hours to train depending on the datasets, the inference step
only requires one single forward operation, and the compute time (MNIST: 9.3 ms, SVHN: 15.4
ms, CIFAR10: 96.0 ms) is at the same order with IG and Occlusion. More experiment details are
provided in the caption of Table 3.
G Intuitions Behind the Minimum Distance Requirements
Here we present the intuitions behind the baseline selection criteria from Section 3.1 using a simpli-
fied formulation. Assuming a transformation ρ projecting from a set of high-level concept variables
V to a sample x, with x = ρ(V ), and we can separate V into three groups V = {Vdis, Vcon, Virr}.
Here, V dis are the discriminating variables that leads to the model decision, for example, the color
the fruits in our apply/orange dataset; V con are the contingent variables that are independent to the
model decision but correlate with how the discriminating variables are presented (eg. size and loca-
tion of the fruits); V irr are the irrelevant variables that are both independent to the model decision
and uncorrelated to the presentation of the discriminating variables (eg. background colors of the
images). The expected one-vs-all explanation under the formulation would be
Aco(x)=Aco(ρ(V))=A(ρ(Vcdois,Vcon,Virr))=αco(Vcdois,Vcon)	(10)
with α being a transformation from the underlying concept variables to the explanation. Here we
assumed a correct mapping α should be independent of V irr because the variable set has no impact
on the discriminating variables themselves or how discriminating variables are presented.
Now if we apply the concept of one-vs-one, we expect a one-vs-one explanation to produce
Ac0→ct (x, Bct (x)) = αc0→ct(Vcdois,Vcdtis,Vcon)
(11)
18
Under review as a conference paper at ICLR 2021
(A)1
'	/ Input
et
rg
Tar
6
Target
O
4
6
7
8
2	3	4	5	7	8	9
λfc
λfc
λfc
λfc
λfc
λfc
Target
1	2	3	4	6	7
(C)
IS Ifl »3 14
MU 13 14
IHTiali
O 1
Mti 13 M
临所用附
l& Itf 18 Ii
IHnTis
»5 17 18 18
Ii Ifl 13 14 15 泪 18 IS

H Il 13 M IS »718 IS
Il h∏3∣4 ∣⅝ 17 Ifi ∣9
∣ΓΓTπ∏∏∏H9
8	9
Oil IB 13 M
Target
Il IS 13 M
Il n ∣5 M




Iff Iti 13 lð
6 Iff


012345789
8	9
入 Sim=°
入 Sim=5
Aim=1。
入 Sim=20
入 Sim=50
入 Sim=Ioo
0 H rg >3 »4 >5 I? Ifl IS
e »1 »9 13 M »5 17 Ifi l⅜
0 Il l& 邑 MlB 17 坦 18
fl U >g >3 M >5 19 ⅛ >⅜
0 Il Ifl 13 14 15 U ∣6 ∣6
Σ
Itt m » M IS tB NI w
Figure 7: GANMEX baselines generated with various weights for the (A) classification loss, (B) similarity loss,
and (C) reconstruction loss.
19
Under review as a conference paper at ICLR 2021
where Ac0→ct and Bct were defined in Section 3, and Vcdois and Vcdtis are the discriminating variables
for class co and ct . In the apple to orange example, Vcdois would represent red color, and Vcdtis would
represent orange color.
For a baseline gerenative function Bct (x) = Bct (P(Vdis, Vcon, Virr)) = P(Vdis, Vcon, Virr), where
Vdis, Vcon and Virr are the concept variables for the generated baseline input. We can write out
ɪ ■ ■ ~ 1.	~ ~ .
Ac0→ct (x, Bct (X)) = Ac0→ct (ρ(Vdis, Vcon, Virr),ρ(Vdis, Vcon, Virr))
(12)
Although Aco→ct can be designed to be independent of Vcon and Vcon, we still need additional
constraint to make Equation 12 satisfy the form of Equation 11. While one can apply additional
constraints on Ac0→ct to satisfy such requirements, an alternative approach would be requiring Bct
to ensure the following.
Vdis
〜
V Con
〜
~ .
V irr
(13)
(14)
(15)
Equation 13 requires the baseline to belong to the target class ct , and this implies that without
additional constraints to Ac0→ct, a class-targeted one-vs-one baseline is required for correct one-vs-
one explanations.
Equation 14 and Equation 15 combined have led to the closest input requirements in Section 3.1.
Assuming a smooth transformation (P), minimizing the distance of kx - Bct (x)k provides an ef-
fective way of ensuring Equation 14 and Equation 15. Going back to the apple/orange example, a
baseline satisfying Equation 13-15 for an apple image input would be an image with an orange fruit
of the same size, at the same location, with the same background to the original input, and all of the
above can be achieved by the minimum distance sample described in Section 3.1.
H Additional Figures
20
Under review as a conference paper at ICLR 2021
IntGrad
IntGrad+GANMEX
DeepLIFT
DeepLI FT+GANMEX
DeepSHAP
IntGrad+GANMEX
DeepLIFT
DeepSHAP+GANMEX
DeepLI FT+GANMEX
DeepSHAP
DeepSHAP+GANMEX
Figure 8: One-vs-one saliency maps using class-targeted baselines (GANMEX) vs non-class-targeted baselines
(zero baselines). One-vs-one saliency maps generated using zero baselines show almost the same attributions
regardless of the target class, making the one-vs-one saliency maps (columns with target labels) similar to the
one-vs-all saliency maps (the ”Avg” columns that show the averaged saliency maps over all target classes).
GANMEX baselines corrected the behavior for both IG, DeepLIFT and DeepSHAP, and produced different
attributions depending on the target classes.
21
Under review as a conference paper at ICLR 2021
Baselines	Integrated Gradient	DeepLIFT
Figure 9: Additional examples of saliency maps for the classifier on the apple2orange dataset with four baseline
choices: zero baseline (Zero), maximum value baseline (Max), blurred baseline (Blur), and GANMEX baseline
(GANMEX).
22