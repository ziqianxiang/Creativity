Under review as a conference paper at ICLR 2021
Convex Regularization
in Monte-Carlo Tree Search
Anonymous authors
Paper under double-blind review
Ab stract
Monte-Carlo planning and Reinforcement Learning (RL) are essential to sequen-
tial decision making. The recent AlphaGo and AlphaZero algorithms have shown
how to successfully combine these two paradigms to solve large scale sequential
decision problems. These methodologies exploit a variant of the well-known UCT
algorithm to trade off the exploitation of good actions and the exploration of unvis-
ited states, but their empirical success comes at the cost of poor sample-efficiency
and high computation time. In this paper, we overcome these limitations by study-
ing the benefit of convex regularization in Monte-Carlo Tree Search (MCTS) to
drive exploration efficiently and to improve policy updates, as already observed in
RL. First, we introduce a unifying theory on the use of generic convex regularizers
in MCTS, deriving the first regret analysis of regularized MCTS and showing that
it guarantees an exponential convergence rate. Second, we exploit our theoretical
framework to introduce novel regularized backup operators for MCTS, based on
the relative entropy of the policy update and on the Tsallis entropy of the policy.
We provide an intuitive demonstration of the effect of each regularizer empirically
verifying the consequence of our theoretical results on a toy problem. Finally, we
show how our framework can easily be incorporated in AlphaGo and AlphaZero,
and we empirically show the superiority of convex regularization w.r.t. represen-
tative baselines, on well-known RL problems across several Atari games.
1	Introduction
Monte-Carlo Tree Search (MCTS) is a well-known algorithm to solve decision-making problems
through the combination of Monte-Carlo planning with an incremental tree structure (Coulom,
2006). Although standard MCTS is only suitable for problems with discrete state and action spaces,
recent advances have shown how to enable MCTS in continuous problems (Silver et al., 2016; Yee
et al., 2016). Most remarkably, AlphaGo (Silver et al., 2016) and AlphaZero (Silver et al., 2017b;a)
couple MCTS with neural networks trained using Reinforcement Learning (RL) (Sutton & Barto,
1998) methods, e.g., Deep Q-Learning (Mnih et al., 2015), to speed up learning of large scale prob-
lems with continuous state space. In particular, a neural network is used to compute value function
estimates of states as a replacement of time-consuming Monte-Carlo rollouts, and another neural
network is used to estimate policies as a probability prior for the therein introduced PUCT action
selection method, a variant of well-known UCT sampling strategy commonly used in MCTS for
exploration (Kocsis et al., 2006). Despite AlphaGo and AlphaZero achieving state-of-the-art per-
formance in games with high branching factor like Go (Silver et al., 2016) and Chess (Silver et al.,
2017a), both methods suffer from poor sample-efficiency, mostly due to the polynomial conver-
gence rate of PUCT (Xiao et al., 2019). This problem, combined with the high computational time
to evaluate the deep neural networks, significantly hinder the applicability of both methodologies.
In this paper, we provide a unified theory of the use of convex regularization in MCTS, which proved
to be an efficient solution for driving exploration and stabilizing learning in RL (Schulman et al.,
2015; 2017a; Haarnoja et al., 2018; Buesing et al., 2020). In particular, we show how a regularized
objective function in MCTS can be seen as an instance of the Legendre-Fenchel transform, similar to
previous findings on the use of duality in RL (Mensch & Blondel, 2018; Geist et al., 2019; Nachum
& Dai, 2020) and game theory (Shalev-Shwartz & Singer, 2006; Pavel, 2007). Establishing our
theoretical framework, we can derive the first regret analysis of regularized MCTS, and prove that
a generic convex regularizer guarantees an exponential convergence rate to the solution of the reg-
1
Under review as a conference paper at ICLR 2021
ularized objective function, which improves on the polynomial rate of PUCT. These results provide
a theoretical ground for the use of arbitrary entropy-based regularizers in MCTS until now limited
to maximum entropy (Xiao et al., 2019), among which we specifically study the relative entropy of
policy updates, drawing on similarities with trust-region and proximal methods in RL (Schulman
et al., 2015; 2017b), and the Tsallis entropy, used for enforcing the learning of sparse policies (Lee
et al., 2018). Moreover, we provide an empirical analysis of the toy problem introduced in Xiao et al.
(2019) to intuitively evince the practical consequences of our theoretical results for each regularizer.
Finally, we empirically evaluate the proposed operators in AlphaGo and AlphaZero on problems of
increasing complexity, from classic RL problems to an extensive analysis of Atari games, confirm-
ing the benefit of our novel operators compared to maximum entropy and, in general, the superiority
of convex regularization in MCTS w.r.t. classic methods.
2	Preliminaries
2.1	Markov Decision Processes
We consider the classical definition of a finite-horizon Markov Decision Process (MDP) as a 5-
tuple M = hS, A, R, P, γi, where S is the state space, A is the finite discrete action space,
R : S × A × S → R is the reward function, P : S × A → S is the transition kernel, and
γ ∈ [0, 1) is the discount factor. A policy π ∈ Π : S × A → R is a probability distribution of
the event of executing an action a in a state s. A policy π induces a value function corresponding
to the expected cumulative discounted reward collected by the agent when executing action a in
state s, and following the policy π thereafter: Qπ(s, a) , E Pk∞=0 γkri+k+1 |si = s,ai = a,π ,
where ri+1 is the reward obtained after the i-th transition. An MDP is solved finding the op-
timal policy ∏*, which is the policy that maximizes the expected cumulative discounted re-
ward. The optimal policy corresponds to the one satisfying the optimal Bellman equation (Bell-
man, 1954) Q* (s, a)，RS P(s0∣s, a)[R(s, a, s0) + Y max。，Q*(s0,a0)] ds0, and is the fixed point
of the optimal Bellman operator T*Q(s, a)，JS P(s0∣s, a)[R(s, a, s0) + Y max。，Q(s', a0)] dS.
Additionally, we define the Bellman operator under the policy π as TπQ(s, a)	,
JS P (s0∣s,a) [R(s, a, s0) + Y JA π(a0∣s0)Q(s0, a0)da0] ds0, the optimal value function V * (S)，
maXa∈A Q*(s, a), and the value function under the policy π as Vπ(S)，maXa∈A Qn(s, a).
2.2	Monte-Carlo Tree Search and Upper Confidence bounds for Trees
Monte-Carlo Tree Search (MCTS) is a planning strategy based on a combination of Monte-Carlo
sampling and tree search to solve MDPs. MCTS builds a tree where the nodes are the visited states
of the MDP, and the edges are the actions executed in each state. MCTS converges to the optimal
policy (Kocsis et al., 2006; Xiao et al., 2019), iterating over a loop composed of four steps:
1.	Selection: starting from the root node, a tree-policy is executed to navigate the tree until a
node with unvisited children, i.e. expandable node, is reached;
2.	Expansion: the reached node is expanded according to the tree policy;
3.	Simulation: run a rollout, e.g. Monte-Carlo simulation, from the visited child of the cur-
rent node to the end of the episode;
4.	Backup: use the collected reward to update the action-values Q(∙) of the nodes visited in
the trajectory from the root node to the expanded node.
The tree-policy used to select the action to execute in each node needs to balance the use of al-
ready known good actions, and the visitation of unknown states. The Upper Confidence bounds
for Trees (UCT) sampling strategy (Kocsis et al., 2006) extends the use of the well-known UCB1
sampling strategy for multi-armed bandits (Auer et al., 2002), to MCTS. Considering each node
corresponding to a state s ∈ S as a different bandit problem, UCT selects an action a ∈ A applying
an upper bound to the action-value function
UCT(s, a) = Q(s, a) +
Ibg N(S)
N N(s,a)
(1)
2
Under review as a conference paper at ICLR 2021
where N (s, a) is the number of executions of action a in state s, N (s) = a N (s, a), and is a
constant parameter to tune exploration. UCT asymptotically converges to the optimal action-value
function Q*, for all states and actions, with the probability of executing a SUboPtimal action at the
root node approaching 0 with a polynomial rate O( t), fora simulation budget t (KoCsis et al., 2006;
Xiao et al., 2019).
3	Regularized Monte-Carlo Tree Search
The success of RL methods based on entropy regularization comes from their ability to achieve
state-of-the-art performance in decision making and control problems, while enjoying theoretical
guarantees and ease of implementation (Haarnoja et al., 2018; Schulman et al., 2015; Lee et al.,
2018). However, the use of entropy regularization is MCTS is still mostly unexplored, although its
advantageous exploration and value function estimation would be desirable to reduce the detrimen-
tal effect of high-branching factor in AlphaGo and AlphaZero. To the best of our knowledge, the
MENTS algorithm (Xiao et al., 2019) is the first and only method to combine MCTS and entropy
regularization. In particular, MENTS uses a maximum entropy regularizer in AlphaGo, proving
an exponential convergence rate to the solution of the respective softmax objective function and
achieving state-of-the-art performance in some Atari games (Bellemare et al., 2013). In the fol-
lowing, motivated by the success in RL and the promising results of MENTS, we derive a unified
theory of regularization in MCTS based on the Legendre-Fenchel transform (Geist et al., 2019), that
generalizes the use of maximum entropy of MENTS to an arbitrary convex regularizer. Notably, our
theoretical framework enables to rigorously motivate the advantages of using maximum entropy and
other entropy-based regularizers, such as relative entropy or Tsallis entropy, drawing connections
with their RL counterparts TRPO (Schulman et al., 2015) and Sparse DQN (Lee et al., 2018), as
MENTS does with Soft Actor-Critic (SAC) (Haarnoja et al., 2018).
3.1	Legendre-Fenchel transform
Consider an MDP M =(S, A, R, P, γ), as previously defined. Let Ω : Π → R be a strongly
convex function. For a policy ∏ = ∏(∙∣s) and Qs = Q(s, ∙) ∈ RA, the Legendre-Fenchel transform
(or convex conjugate) of Ω is Ω* : RA → R, defined as:
Ω*(Qs)，max TnsQs - TΩ(πs),	⑵
πs ∈Πs	s
where the temperature τ specifies the strength of regularization. Among the several properties of the
Legendre-Fenchel transform, we use the following (Mensch & Blondel, 2018; Geist et al., 2019).
Proposition 1 Let Ω be strongly convex.
•	Unique maximizing argument: VΩ* is Lipschitz and satisfies
VΩ*(Qs) = arg max TnsQs — T Ω(πs).	(3)
πs ∈Πs
•	Boundedness: if there are constants Lω and Uω Such that for all ∏s ∈ Πs, we have Lω ≤
Ω(πs) ≤ Uω, then
maxQs(a) — tUω ≤ Ω*(Qs) ≤ maxQs(a) — tLq∙	(4)
a∈A	a∈A
•	Contraction: for any Q1, Q2 ∈ RS×A
k Ω*(Qι) - Ω*(Q2) k∞≤ Y k Qi — Q2 k∞ .	(5)
Although the Legendre-Fenchel transform Ω* applies to every strongly convex function, for the
purpose of this work we only consider a representative set of entropic regularizers.
3.2	Regularized backup and tree policy
In MCTS, each node of the tree represents a state s ∈ S and contains a visitation count N(s, a).
Given a trajectory, we define n(sT ) as the leaf node corresponding to the reached state sT . Let
3
Under review as a conference paper at ICLR 2021
s0, a0, s1, a1..., sT be the state action trajectory in a simulation, where n(sT) is a leaf node of T.
Whenever a node n(sT ) is expanded, the respective action values (Equation 6) are initialized as
Qω(st, a) = 0, and N(ST, a) = 0 for all a ∈ A. For all nodes in the trajectory, the visitation count
is updated by N(st, at) = N(st, at) + 1, and the action-values by
Qn(st, at)
r(st, at) + γρ
Ir(st, at) + YΩ*(Qω(st+ι)∕τ))
ift=T
ift<T
(6)
where Qn(st+ι) ∈ RA With components Qn(st+ι, a), ∀a ∈ A, and P is an estimate returned from
an evaluation function computed in sT, e.g. a discounted cumulative reward averaged over multiple
rollouts, or the value-function of node n(sT+1) returned by a value-function approximator, e.g. a
neural network pretrained with deep Q-learning (Mnih et al., 2015), as done in (Silver et al., 2016;
Xiao et al., 2019). We revisit the E2W sampling strategy limited to maximum entropy regulariza-
tion (Xiao et al., 2019) and, through the use of the convex conjugate in Equation (6), we derive a
novel sampling strategy that generalizes to any convex regularizer
∏t(at∣st) = (1 — λsJVΩ*(QΩ(st )∕τ )(at) + λA,
|A|
(7)
where 入$七= dA∕iog(Pa N(st,a)+i) with e > 0 as an exploration parameter, and VΩ* depends on
the measure in use (see Table 1 for maximum, relative, and Tsallis entropy). We call this sampling
strategy Extended Empirical Exponential Weight (E3W) to highlight the extension of E2W from
maximum entropy to a generic convex regularizer.
3.3	Convergence rate to regularized objective
We show that the regularized value VΩ can be effectively estimated at the root state S ∈ S, with
the assumption that each node in the tree has a σ2-subgaussian distribution. This result extends the
analysis provided in (Xiao et al., 2019), which is limited to the use of maximum entropy.
Theorem 1 At the root node S where N(S) is the number of visitations, with e > 0, VΩ(s) is the
estimated value, with constant C and C, we have
P(l% (s)-匕(s)I >e) ≤ C exp{-m A N	W },	⑻
Cσ(log(2 + N (S)))2
where VΩ(s) = Ω*(Qs) and W⑸ = Ω*(Q*). From this theorem, we obtain that the convergence
rate of choosing the best action a* at the root node, when using the E3W strategy, is exponential.
Theorem 2 Let at be the action returned by E3W at step t. For large enough t and constants C, C
P(at = a*) ≤ Ct exp{- A /	} ∙	(9)
Cσ(log(t))3
4	Entropy-regularization backup operators
From the introduction of a unified view of generic strongly convex regularizers as backup operators
in MCTS, we narrow the analysis to entropy-based regularizers. For each entropy function, Table 1
shows the Legendre-Fenchel transform and the maximizing argument, which can be respectively
replaced in our backup operation (Equation 6) and sampling strategy E3W (Equation 7). Using
maximum entropy retrieves the maximum entropy MCTS problem introduced in the MENTS algo-
rithm (Xiao et al., 2019). This approach closely resembles the maximum entropy RL framework
used to encourage exploration (Haarnoja et al., 2018; Schulman et al., 2017a). We introduce two
novel MCTS algorithms based on the minimization of relative entropy of the policy update, inspired
by trust-region (Schulman et al., 2015) and proximal optimization methods (Schulman et al., 2017b)
in RL, and on the maximization of Tsallis entropy, which has been more recently introduced in RL
as an effective solution to enforce the learning of sparse policies (Lee et al., 2018). We call these
algorithms RENTS and TENTS. Contrary to maximum and relative entropy, the definition of the
4
Under review as a conference paper at ICLR 2021
Legendre-Fenchel and maximizing argument of Tsallis entropy is non-trivial, being
Ω*(Qt) = T ∙ SPmaX(Qt(s, ∙)∕τ),	(10)
V-max( 4 - ^∈ QM片 - 1 , °) ,	(11)
τ	|K|
where sPmaX is defined for any function f : S × A → R as
△ X f f (s, a)2	(Pa∈K f(s,a) - 1)2!	1	门力
spmaχf (s, ∙)) △ N 1^-------------------------------2|K|2-J+2,	(12)
and K is the set of actions that satisfy 1 + if	(s, ai) > Pij=1f	(s, aj), with ai indicating the action
with the i-th largest value off (s, a) (Lee et al., 2018).
Table 1: List of entropy regularizers with Legendre-Fenchel transforms and maximizing arguments.
Entropy	Regularizer Ω(∏s)	Legendre-Fenchel Ω*(Qs)	Max argument VΩ*(Qs)
Maximum	Pa π⑷ S)IOg π⑷S)	-v	Q(S,a) log Σa e T	Q(S,a) e τ
			~^^Q(s,b) Ebe、
Relative	DKL(∏t(a∣s)∣∣∏t-ι(a∣s))	log Pa ∏t-ι(a∣s)eQ^	Qt (S,a) ∏t-ι(a∣s)e -τ-
			Qt(S,b) Eb nt-i(b|S)e	T
Tsallis	2(k ∏(a∣s) k2 -1)	Equation (10)	Equation (11)
4.1	Regret analysis
At the root node, let each children node i be assigned with a random variable Xi , with mean value
Vi, while the quantities related to the optimal branch are denoted by *, e.g. mean value V*. At each
timestep n, the mean value of variable Xi is Vin . The pseudo-regret (Coquelin & Munos, 2007) at
the root node, at timestep n, is defined as RUCT = nV * 一 Pn=I Vit. Similarly, We define the regret
of E3W at the root node of the tree as
nn	n
Rn = nV*一 X Vit = nV*一 XI(it = i)V, = nV*一 X Vi X∏t(ai∖s),	(13)
t=1	t=1	i t=1
where ∏t(∙) is the policy at time step t, and I(∙) is the indicator function.
Theorem 3 Let Ki = VΩ*(αi∖s) + PJCσ2 log 备∕⅛, and Xi = VΩ*(ai∖s) — LNCσ log 圣∕2n,
where VΩ*(.∖s) is the policy with respect to the mean value vector V(∙) at the root node S. Forany
δ > 0, with probability at least 1 一 δ, ∃ constant L, p, C, C so that the pseudo regret Rn satisfies
nV * 一 n X V(K + L ( τ¾γ^)) ≤ Rn ≤ nV * 一 n X 匕 & 一 IL ( ^—^)).
i	pγ	i	pγ
This theorem provides bounds for the regret ofE3W using a generic convex regularizer Ω; thus, we
can easily retrieve from it the regret bound for each entropy regularizer. Let m = mina VΩ* (a∖s).
Corollary 1 Maximum entropy:
nV * 一 n Pi Vi(Ki + L( τ⅛⅛Al)) ≤ Rn ≤ nV * 一 n Pi Vi(Xi- L( τ⅛A-)).
Corollary 2 Relative entropy:
nV * 一 n Pi Vi(κi + L(T (IOgIA-+)))≤ Rn ≤ nV * 一 n Pi Viki- L(T (IOgIA- *))).
Corollary 3 Tsallis entropy:
nV * 一 n P	V (κ∙	+ L (∖A∖ - 1 -T—)) ≤	R	≤	nV *	一 n P	V(Y -	一 L (0 一 1	T
nV n 乙iV IKi	十 21 2∖A∖ 1一 γl J-	Rn	≤	乙iV IXi 21 2∖A∖ 1一	Y
5
Under review as a conference paper at ICLR 2021
Remarks. The regret bound of UCT and its variance have already been analyzed for non-
regularized MCTS with binary tree (Coquelin & Munos, 2007). On the contrary, our regret bound
analysis in Theorem 3 applies to generic regularized MCTS. From the specialized bounds in the
corollaries, we observe that the maximum and relative entropy share similar results, although the
bounds for relative entropy are slightly smaller due to ml. Remarkably, the bounds for Tsallis en-
tropy become tighter for increasing number of actions, which translates in limited regret in problems
with high branching factor. This result establishes the advantage of Tsallis entropy in complex prob-
lems w.r.t. to other entropy regularizers, as empirically confirmed by the positive results in several
Atari games described in Section 5.
4.2	Error analysis
We analyse the error of the regularized value estimate at the root node n(s) w.r.t. the optimal value:
εΩ = VΩ(s) - V *(S).
Theorem 4 For any δ > 0 and generic convex regularizer Ω, with some constant C, C, with prob-
ability at least 1 一 δ, Eq satisfies
/Cσ2 log 与	T(Uω 一 Lω)
2N 2N(S)	1 一 Y-
≤ Eω ≤
JCσ log 与
2N 2N (s)
(14)
To give a better understanding of the effect of each entropy regularizer in Table 1, we specialize
the bound in Equation 14 to each of them. From (Lee et al., 2018), we know that for maximum
entropy Ω(∏t) = Pa ∏ log∏t, We have 一 log |A| ≤ Ω(∏t) ≤ 0; for relative entropy Ω(∏t)=
KL(∏t∣∣∏t-ι), if we define m = mino ∏t-ι(a∣s), then we can derive 0 ≤ Ω(∏t) ≤ 一 log |A| 十
log m; and for Tsallis entropy Ω(∏t) = ɪ(k ∏t k2 -1), we have 一1AAI ≤ Ω(∏t) ≤ 0. Then,
Corollary 4 maximum entropy error: 一
∣cσ log C
2N 2N (s)
T log |A|
1 一 Y
∣cσ log 与
V 2N (s)
Corollary 5 relative entropy error: 一
/ Cσ2 log C
V 2N (S)
T (Iog|A|- log m1)
1 一 Y
≤ Eω ≤
/ Cσ2 log C
V 2N (S)
—
—
≤ R ≤
/Cσ2 log C	W 1 τ	/Cσ2 log C
Corollary 6 Tsallis entropyerror: η/ IN(S)--------2|AT L ≤ eω ≤v ^Nsr∙
These results show that when the number of actions |A| is large, TENTS enjoys the smallest error;
moreover, we also see that lower bound of RENTS is always smaller than for MENTS.
5	Empirical evaluation
In this section, we empirically evaluate the benefit of the proposed entropy-based MCTS regular-
izers. First, we complement our theoretical analysis with an empirical study of the synthetic tree
toy problem introduced in Xiao et al. (2019), which serves as a simple scenario to give an inter-
pretable demonstration of the effects of our theoretical results in practice. Second, we compare to
AlphaGo and AlphaZero (Silver et al., 2016; 2017a), recently introduced to enable MCTS to solve
large scale problems with high branching factor. Our implementation is a simplified version of the
original algorithms, where we remove various tricks in favor of better interpretability. For the same
reason, we do not compare with the most recent and state-of-the-art variant of AlphaZero known as
MuZero (Schrittwieser et al., 2019), as this is a slightly different solution highly tuned to maximize
performance, and a detailed description of its implementation is not available.
5.1 Synthetic tree
This toy problem is introduced in Xiao et al. (2019) to highlight the improvement of MENTS over
UCT. It consists of a tree with branching factor k and depth d. Each edge of the tree is assigned
6
Under review as a conference paper at ICLR 2021
Λ ΛE	k=16 d=l	_ __ k=4 d=2	k=8 d=3	k=12 d=4	OR1 k=16 d=5
u .un 0 ∩4		U.IU 1	U.ZU ∩ ∩β	CrU	［「	0.5 Ti 0 4 I	U.o CUL
		υυo	0.15 f∖ ΛC - I		03 1	
a 0 03		° °≡ L 	 0.10		θɜ K7	0.4		
0.02		0.04 Lr	—		0 2L—
0.01	一 J	0.02 J	u.un		θɪʌe^—	
0.00		0.00	 0.00		0.0	o.oj	
λ ɔn		Λ ɔe 		Λ Λ		Λ C ɔ	_		7 Λ 	
U. NU		u. N n I	u. 4		0b ʌ	-L .U
0.15			0∙20 k I	0.3			0.8		
匕CrC		0.15 -	Cr		%/	0.6'	■
					
S 0.10		CrC I	°∙2		0.3	
3 0.05	—	u.lu- 0.05 U	01	J一	0.2 I 0.1 x—	0.4
0.00	ILJ 		OQOn - 	 θ.θɪ		o.oJ		0 0 Il一—
〕κnn		QΛΛ 一			
IjUU		500	800		500	
1250 1000		400	/ 600 ,3∩∩,	_ _ _		歌/	600 400	/
oc 750 CΛΛ		3UU	400 200			
jUU 250		20°		吗〃	20 ⅛≡
0	0	5e3 10	0 	ι	γj	0 e3 0	5e3 10e3		1	T		'		'	 ɔ 5e3 10e3	0	5e3 10e3	0	5e3 10e3		
	# Simulations	# Simulations	# Simulations	# Simulations	# Simulations				
——UCT ——MENTS
——RENTS ——TENTS
Figure 1:	For each algorithm, we show the convergence of the value estimate at the root node to the
respective optimal value (top), to the UCT optimal value (middle), and the regret (bottom).
UCT R MENTS
2
4
6
8
10
12
14
16
UCT Ea MENTS
12345	12345
UCT ɛuer MENTS
2
4
6
8
10
12
14
16
12345	12345
RENTS TENTS
(a)
12345	12345
0.4	1 2 3 4 5	1 2 3 4 5
0.3
0.2
2
4
0.7 6
8
0.6 10
12
0 5 14
UQ 16
RENTS
2
0.1 10
12
0.0 14
16
j
12
14
16
TENTS
j
12
14
16
1400
1200
1000
800
600
400
200
12345	12345
(b)	(C)
Figure 2:	For different branching factor k (rows) and depth d (columns), the heatmaps show: the
absolute error of the value estimate at the root node after the last simulation of each algorithm w.r.t.
the respective optimal value (a), and w.r.t. the optimal value of UCT (b); regret at the root node (c).
a random value between 0 and 1. At each leaf, a Gaussian distribution is used as an evaluation
function resembling the return of random rollouts. The mean of the Gaussian distribution is the
sum of the values assigned to the edges connecting the root node to the considered leaf, while the
standard deviation is σ = 0.051. For stability, all the means are normalized between 0 and 1. As
in Xiao et al. (2019), we create 5 trees on which we perform 5 different runs in each, resulting in 25
experiments, for all the combinations of branching factor k = {2, 4, 6, 8, 10, 12, 14, 16} and depth
d = {1, 2, 3, 4, 5}, computing: (i) the value estimation error at the root node w.r.t. the regularized
optimal value: e。= VΩ - V*;(ii) the value estimation error at the root node w.r.t. the unregularized
optimal value: e∏ct = VΩ - V*uct； (iii) the regret R as in Equation (13). For a fair comparison,
we use fixed τ = 0.1 and = 0.1 across all algorithms. Figure 1 and 2 show how UCT and each
regularizer behave for different configurations of the tree. We observe that, while RENTS and
MENTS converge slower for increasing tree sizes, TENTS is robust w.r.t. the size of the tree and
almost always converges faster than all other methods to the respective optimal value. Notably, the
optimal value of TENTS seems to be very close to the one of UcT, i.e. the optimal value of the
1The value of the standard deviation is not provided in Xiao et al. (2019). After trying different values, we
observed that our results match the one in Xiao et al. (2019) when using σ = 0.05.
7
Under review as a conference paper at ICLR 2021
Episode
(a) Cartpole.
(b) Acrobot.
Figure 3: Cumulative rewards of AlphaZero with UCT and entropy-based operators, in CartPole (a)
and Acrobot (b). Results are averaged over 5 and 10 seeds and show 95% confidence intervals.
unregularized objective, and also converges faster than the one estimated by UCT, while MENTS
and RENTS are considerably further from this value. In terms of regret, UCT explores less than
the regularized methods and it is less prone to high regret, at the cost of slower convergence time.
Nevertheless, the regret of TENTS is the smallest between the ones of the other regularizers, which
seem to explore too much. These results show a general superiority of TENTS in this toy problem,
also confirming our theoretical findings about the advantage of TENTS in terms of approximation
error (Corollary 6) and regret (Corollary 3), in problems with many actions.
5.2 Entropy-regularized AlphaZero
In its standard form, AlphaZero (Silver et al., 2017a) uses the PUCT sampling strategy, a variant of
UCT (Kocsis et al., 2006) that samples actions according to the policy
PUCT (s,a) = Q(s,a)+ eP(s, a)Μ，、,	(15)
1 + N(s, a)
where P is a prior probability on action selection, and e is an exploration constant. A value network
and a policy network are used to compute, respectively, the action-value function Q and the prior
policy P. We use a single neural network, with 2 hidden layers composed of 128 ELU units, and
two output layer respectively for the action-value function and the policy. We run 500 AlphaZero
episodes, where each episode is composed of 300 steps. A step consists of running 32 MCTS
simulations from the root node, as defined in Section 2, using the action-value function computed
by the value network instead of using Monte-Carlo rollouts. At the end of each cycle, the average
action-value of the root node is computed and stored, the tree is expanded using the given sampling
strategy, and the root node is updated with the reached node. At the end of the episode, a minibatch
of 32 samples is built from the 300 stored action-values, and the network is trained with one step
of gradient descent using RMSProp with learning rate 0.001. The entropy-regularized variants of
AlphaZero can be simply derived replacing the average backup operator, with the desired entropy
function, and replacing PUCT with E3W using the respective maximizing argument and e = 0.1.
Cartpole and Acrobot. Figure 3 shows the cumulative reward of standard AlphaZero based on
PUCT, and the three entropy-regularized variants, on the Cartpole and Acrobot discrete control
problems (Brockman et al., 2016). While standard AlphaZero clearly lacks good convergence and
stability, the entropy-based variants behave differently according to the problem. First, although not
significantly superior, RENTS exhibits the most stable learning and faster convergence, confirm-
ing the benefit of relative entropy in control problems as already known for trust-region methods
in RL (Schulman et al., 2015). Second, considering the small number of discrete actions in the
problems, TENTS cannot benefit from the learning of sparse policies and shows slightly unstable
learning in Cartpole, even though the overall performance is satisfying in both problems. Last,
MENTS solves the problems slightly slower than RENTS, but reaches the same final performance.
Although the results on these simple problems are not conclusive to assert the superiority of one
method over the other, they definitely confirm the advantage of regularization in MCTS, and hint
at the benefit of the use of relative entropy in control problems. Further analysis on more complex
8
Under review as a conference paper at ICLR 2021
Table 2: Average score in Atari over 100 seeds per game. Bold denotes no statistically significant
difference to the highest mean (t-test, p < 0.05). Bottom row shows #no difference to highest mean.
	UCT	MaxMCTS	MENTS	RENTS	TENTS
					
Alien	1, 486.80	1,461.10	1, 508.60	1, 547.80	1, 568.60
Amidar	115.62	124.92	123.30	125.58	121.84
Asterix	4, 855.00	^^5, 484.50^^	5, 576.00	5, 743.50	5, 647.00
Asteroids	873.40	899.60	1, 414.70	1, 486.40	1, 642.10
Atlantis	35, 182.00	35, 720.00	36, 277.00	35, 314.00	35, 756.00
BankHeist	475.50	458.60	622.30	636.70	631.40
BeamRider	2, 616.72	^^2, 661.30^^	2, 822.18	2, 558.94	2, 804.88
Breakout	303.04	296.14	309.03	300.35	316.68
Centipede	1, 782.18	-^1, 728.69~~	2, 012.86	2, 253.42	2, 258.89
DemonAttack	579.90	640.80	1, 044.50	1, 124.70	1, 113.30
Enduro	129.28	124.20	128.79	134.88	132.05
Frostbite	1, 244.00	-^1, 332.10^^	2, 388.20	2, 369.80	2, 260.60
Gopher	3, 348.40	-^3, 303.00^^	3, 536.40	3, 372.80	3, 447.80
Hero	3, 009.95	-^3, 010.55~~	3, 044.55	3, 077.20	3, 074.00
MsPacman	1, 940.20	-^1, 907.10^^	2, 018.30	2, 190.30	2, 094.40
Phoenix	2, 747.30	~~2, 626.60^^	3, 098.30	2, 582.30	3, 975.30
Qbert	7, 987.25	-^8, 033.50^^	8, 051.25	8, 254.00	8, 437.75
Robotank	11.43	11700	11.59	11.51	11.47
Seaquest	3, 276.40	^^3, 217.20^^	3, 312.40	3, 345.20	3, 324.40
Solaris	895.00	923.20	1, 118.20	1, 115.00	1, 127.60
SpaceInvaders	778.45	835.90	832.55	867.35	822.95
WizardOfWor	685.00	666.00	1, 211.00	1, 241.00	1, 231.00
# Highest mean	6/22	7/22 ―	17/22	16/22	22/22
control problems will be desirable (e.g. MuJoCo (Todorov et al., 2012)), but the need to account for
continuous actions, a non-trivial setting for MCTS, makes it out of the scope of this paper.
5.3 Entropy-regularized AlphaGo
The learning time of AlphaZero can be slow in problems with high branching factor, due to the need
of a large number of MCTS simulations for obtaining good estimates of the randomly initialized
action-values. To overcome this problem, AlphaGo (Silver et al., 2016) initializes the action-values
using the values retrieved from a pretrained network, which is kept fixed during the training.
Atari. Atari 2600 (Bellemare et al., 2013) is a popular benchmark for testing deep RL method-
ologies (Mnih et al., 2015; Van Hasselt et al., 2016; Bellemare et al., 2017) but still relatively
disregarded in MCTS. We use a Deep Q-Network, pretrained using the same experimental set-
ting of Mnih et al. (2015), to initialize the action-value function of each node after expansion as
Qinit(s,a) = (Q(s, a) - V (s)) /τ, for MENTS and TENTS, as done in Xiao et al. (2019). For
RENTS we init Qinit(s, a) = log Pprior(a|s)) + (Q(s, a) - V (s)) /τ, where Pprior is the Boltzmann
distribution induced by action-values Q(s, .) computed from the network. Each experimental run
consists of 512 MCTS simulations. The temperature τ is optimized for each algorithm and game
via grid-search between 0.01 and 1. The discount factor is γ = 0.99, and for PUCT the exploration
constant is c = 0.1. Table 2 shows the performance, in terms of cumulative reward, of standard
AlphaGo with PUCT and our three regularized versions, on 22 Atari games. Moreover, we test also
AlphaGo using the MaxMCTS backup (Khandelwal et al., 2016) for further comparison with clas-
sic baselines. We observe that regularized MCTS dominates other baselines, in particular TENTS
achieves the highest scores in all the 22 games, showing that sparse policies are more effective in
Atari. This can be explained by Corollary 6 which shows that Tsallis entropy can lead to a lower
error at the root node even with a high number of actions compared to relative or maximum entropy.
6 Conclusion
We introduced a theory of convex regularization in Monte-Carlo Tree Search (MCTS) based on
the Legendre-Fenchel transform. Exploiting this theoretical framework, we studied the regret of
MCTS when using a generic strongly convex regularizer, and we proved that it has an exponential
convergence rate. We use these results to motivate the use of entropy regularization in MCTS,
particularly considering maximum, relative, and Tsallis entropy. Finally, we test regularized MCTS
algorithms in discrete control problems and Atari games, showing its advantages over other methods.
9
Under review as a conference paper at ICLR 2021
References
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit
problem. Machine learning, 47(2-3):235-256, 2002.
Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environ-
ment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:
253-279, 2013.
Marc G Bellemare, Will Dabney, and Remi Munos. A distributional perspective on reinforcement
learning. In Proceedings of the 34th International Conference on Machine Learning-Volume 70,
pp. 449-458. JMLR. org, 2017.
Richard Bellman. The theory of dynamic programming. Technical report, Rand corp santa monica
ca, 1954.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Lars Buesing, Nicolas Heess, and Theophane Weber. Approximate inference in discrete distribu-
tions with monte carlo tree search and value functions. In International Conference on Artificial
Intelligence and Statistics, pp. 624-634. PMLR, 2020.
Guillaume Chaslot, Mark Winands, Jaap Van Den Herik, Jos Uiterwijk, and Bruno Bouzy. Progres-
sive strategies for monte-carlo tree search. New Mathematics and Natural Computation, 4(03):
343-357, 2008.
Benjamin E Childs, James H Brodeur, and Levente Kocsis. Transpositions and move groups in
monte carlo tree search. In 2008 IEEE Symposium On Computational Intelligence and Games.
IEEE, 2008.
Pierre-Arnaud Coquelin and Remi Munos. Bandit algorithms for tree search. arXiv preprint
cs/0703062, 2007.
Remi Coulom. Efficient selectivity and backup operators in monte-carlo tree search. In International
conference on computers and games, pp. 72-83. Springer, 2006.
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes. In International Conference on Machine Learning, pp. 2160-2169, 2019.
Sylvain Gelly and David Silver. Combining online and offline knowledge in uct. In Proceedings of
the 24th international conference on Machine learning, pp. 273-280. ACM, 2007.
Sylvain Gelly and Yizao Wang. Exploration exploitation in go: Uct for monte-carlo go. In NIPS:
Neural Information Processing Systems Conference On-line trading of Exploration and Exploita-
tion Workshop, 2006.
Jean-Bastien Grill, Florent Altche, Yunhao Tang, Thomas Hubert, Michal Valko, Ioannis
Antonoglou, and Remi Munos. Monte-carlo tree search as regularized policy optimization. arXiv
preprint arXiv:2007.12509, 2020.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In International Confer-
ence on Machine Learning, pp. 1861-1870, 2018.
David P Helmbold and Aleatha Parker-Wood. All-moves-as-first heuristics in monte-carlo go. In
IC-AI, pp. 605-610, 2009.
Jean-Baptiste Hoock, Chang-Shing Lee, Arpad Rimmel, Fabien Teytaud, Mei-Hui Wang, and Oliver
Teytaud. Intelligent agents for the game of go. IEEE Computational Intelligence Magazine, 2010.
Piyush Khandelwal, Elad Liebman, Scott Niekum, and Peter Stone. On the analysis of complex
backup strategies in monte carlo tree search. In International Conference on Machine Learning,
2016.
10
Under review as a conference paper at ICLR 2021
Levente Kocsis, Csaba Szepesvari, and Jan Willemson. Improved monte-carlo search, 2006.
Tomas Kozelek. Methods of mcts and the game arimaa, 2009.
Kyungjae Lee, Sungjoon Choi, and Songhwai Oh. Sparse markov decision processes with causal
sparse tsallis entropy regularization for reinforcement learning. IEEE Robotics and Automation
Letters, 3(3):1466-1473, 2018.
Richard J Lorentz. Improving monte-carlo tree search in havannah. In International Conference on
Computers and Games, pp. 105-115. Springer, 2010.
Jincheng Mei, Chenjun Xiao, Ruitong Huang, Dale Schuurmans, and Martin Muller. On princi-
pled entropy exploration in policy optimization. In Proceedings of the 28th International Joint
Conference on Artificial Intelligence, pp. 3130-3136. AAAI Press, 2019.
Arthur Mensch and Mathieu Blondel. Differentiable dynamic programming for structured prediction
and attention. In International Conference on Machine Learning, pp. 3462-3471, 2018.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level
control through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. In International conference on machine learning, pp. 1928-1937, 2016.
William H Montgomery and Sergey Levine. Guided policy search via approximate mirror descent.
In Advances in Neural Information Processing Systems, pp. 4008-4016, 2016.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. CoRR,
abs/2001.01866, 2020.
Vlad Niculae and Mathieu Blondel. A regularized framework for sparse and structured neural atten-
tion. In Advances in Neural Information Processing Systems, pp. 3338-3348, 2017.
Lacra Pavel. An extension of duality to a game-theoretic framework. Automatica, 43(2):226 - 237,
2007.
Gavin Adrian Rummery. Problem solving with reinforcement learning. PhD thesis, University of
Cambridge Ph. D. dissertation, 1995.
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon
Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy Lillicrap, and
David Silver. Mastering atari, go, chess and shogi by planning with a learned model, 2019.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning (ICML), pp. 1889-1897,
2015.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440, 2017a.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017b.
Shai Shalev-Shwartz and Yoram Singer. Convex repeated games and fenchel duality. Advances in
neural information processing systems, 19:1265-1272, 2006.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. Mastering chess and shogi
by self-play with a general reinforcement learning algorithm. arXiv preprint arXiv:1712.01815,
2017a.
11
Under review as a conference paper at ICLR 2021
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go
without human knowledge. Nature, 550(7676):354-359, 2017b.
Richard S Sutton and Andrew G Barto. Introduction to reinforcement learning, volume 135. MIT
press Cambridge, 1998.
Gerald Tesauro, VT Rajan, and Richard Segal. Bayesian inference in monte-carlo tree search. arXiv
preprint arXiv:1203.3519, 2012.
Fabien Teytaud and Olivier Teytaud. On the huge benefit of decisive moves in monte-carlo tree
search algorithms. In Proceedings of the 2010 IEEE Conference on Computational Intelligence
and Games, pp. 359-364. IEEE, 2010.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012.
David Tom. Investigating uct and rave: Steps towards a more robust method, 2010.
Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In Thirtieth AAAI conference on artificial intelligence, 2016.
Tom Vodopivec, Spyridon Samothrakis, and Branko Ster. On monte carlo tree search and reinforce-
ment learning. Journal of Artificial Intelligence Research, 60:881-936, 2017.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge University Press, 2019.
ChenjUn Xiao, RUitong Huang, Jincheng Mei, Dale Schuurmans, and Martin Muller. Maximum
entropy monte-carlo planning. In Advances in Neural Information Processing Systems, pp. 9516-
9524, 2019.
Timothy Yee, Viliam Lisy, Michael H Bowling, and S Kambhampati. Monte carlo tree search in
continuous action spaces with execution uncertainty. In IJCAI, pp. 690-697, 2016.
A Related Work
Entropy regularization is a common tool for controlling exploration in Reinforcement Learning (RL)
and has lead to several successful methods (Schulman et al., 2015; Haarnoja et al., 2018; Schulman
et al., 2017a; Mnih et al., 2016). Typically specific forms of entropy are utilized such as maxi-
mum entropy (Haarnoja et al., 2018) or relative entropy (Schulman et al., 2015). This approach
is an instance of the more generic duality framework, commonly used in convex optimization the-
ory. Duality has been extensively studied in game theory (Shalev-Shwartz & Singer, 2006; Pavel,
2007) and more recently in RL, for instance considering mirror descent optimization (Montgomery
& Levine, 2016; Mei et al., 2019), drawing the connection between MCTS and regularized policy
optimization (Grill et al., 2020), or formalizing the RL objective via Legendre-Rockafellar dual-
ity (Nachum & Dai, 2020). Recently (Geist et al., 2019) introduced regularized Markov Decision
Processes, formalizing the RL objective with a generalized form of convex regularization, based
on the Legendre-Fenchel transform. In this paper, we provide a novel study of convex regulariza-
tion in MCTS, and derive relative entropy (KL-divergence) and Tsallis entropy regularized MCTS
algorithms, i.e. RENTS and TENTS respectively. Note that the recent maximum entropy MCTS
algorithm MENTS (Xiao et al., 2019) is a special case of our generalized regularized MCTS. Unlike
MENTS, RENTS can take advantage of any action distribution prior, in the experiments the prior
is derived using Deep Q-learning (Mnih et al., 2015). On the other hand, TENTS allows for sparse
action exploration and thus higher dimensional action spaces compared to MENTS. In experiments,
both RENTS and TENTS outperform MENTS.
Several works focus on modifying classical MCTS to improve exploration. UCB1-tuned (Auer
et al., 2002) modifies the upper confidence bound of UCB1 to account for variance in order to im-
prove exploration. (Tesauro et al., 2012) proposes a Bayesian version of UCT, which obtains better
estimates of node values and uncertainties given limited experience. Many heuristic approaches
12
Under review as a conference paper at ICLR 2021
based on specific domain knowledge have been proposed, such as adding a bonus term to value esti-
mates (Gelly & Wang, 2006; Teytaud & Teytaud, 2010; Childs et al., 2008; Kozelek, 2009; Chaslot
et al., 2008) or prior knowledge collected during policy search (Gelly & Silver, 2007; Helmbold
& Parker-Wood, 2009; Lorentz, 2010; Tom, 2010; Hoock et al., 2010). (Khandelwal et al., 2016)
formalizes and analyzes different on-policy and off-policy complex backup approaches for MCTS
planning based on RL techniques. (Vodopivec et al., 2017) proposes an approach called SARSA-
UCT, which performs the dynamic programming backups using SARSA (Rummery, 1995). Both
(Khandelwal et al., 2016) and (Vodopivec et al., 2017) directly borrow value backup ideas from RL
to estimate the value at each tree node, but they do not provide any proof of convergence.
B Proofs
Let r and r be respectively the average and the the expected reward at the leaf node, and the reward
distribution at the leaf node be σ2-sub-Gaussian.
Lemma 1 For the stochastic bandit problem E3W guarantees that, for t ≥ 4,
P(kr- rtk∞≥ lθg(2⅛
≤ 4|A| exp (- (log(2t+ t))3).
Proof 1 Let us define Nt (a) as the number of times action a have been chosen until time t, and
Nt (a) = PS=ι ∏s(a), where ∏s(a) is the E3Wpolicy at time Step S. By choosing λs = S), it
follows that for all a and t ≥ 4,
t	t 1	t 1	s/(s + 1)
t a 一入 πs a log(1 + S)	log(1 + S) (log(1 + s))2
〉［1+t	1
≥ Ji log(1 + S)
SslS+ 1) ds =	1 +1________L ≥________t____
(log(1 + s))2	log(2 + t)	log2 — 2log(2 +1)
From Theorem 2.19 in Wainwright (2019), we have the following concentration inequality:
2	22
P(INt(a) - Nt(a)| > e) ≤ 2eχp{— ——t	2} ≤ 2eχp{---},
2 s=1 σs2	t
where σs2 ≤ 1/4 is the variance of a Bernoulli distribution with p = πs(k) at time step s. We define
the event
Ee = {∀a ∈ A, ∣Nt(a) — Nt(a)∣ ≤ e},
and consequently
P(INt(a) — Nt(a)∣ ≥ E) ≤ 2|A| exp(—— )∙	(16)
Conditioned on the event Ee, for E = 4iogt2+t), we have Nt (a) ≥ 4bgt2+t). For any action a by
the definition of sub-gaussian,
S8 ( ʌ J 、J 8σ2 log( δ )log(2 +1∖vp(∣('	r∖∣: / 2σ2 log( δ)∖vx
Pl|r(a) ―rt(a)| > y-----1---------I ≤ Pl|r(a) ―rt(a)| > V NMa)	) ≤δ
by choosing a δ satisfying log(2)=(由乙^尸,we have
P(|r(a)- rt(a)| > J 2σNtogF! ≤ 2exp (- dog」+1))3).
13
Under review as a conference paper at ICLR 2021
Therefore, for t ≥ 2
P(k r-rt k∞> ιog(2+)! ≤P(k r-rt k∞> ιog(2+)卜!十P(EC)
≤ X (P(Ma)- rt(a)| > ιogd+)) +p(EC) ≤21A1 eχp (- (iog⅛))3!!
+ 21A1 exp-- (log(2t+1))3 ) = 41A1 exp-- (log(2t+ t))3 ).
Lemma 2 Given two policies π(1) = VΩ*(r(1)) and π(2) = VΩ* (r(2)), ∃L, such that
k π(1) - π(2) kp≤ L k r(1) - r(2) kp .
Proof 2 This comes directly from the fact that π = VΩ*(r) is Lipschitz continuous with 'p-norm.
Note that p has different values according to the choice of regularizer. Refer to Niculae & Blondel
(2017) for a discussion of each norm using Shannon entropy and Tsallis entropy regularizer. Relative
entropy shares the same Properties with Shannon Entropy.
Lemma 3 Consider the E3W policy applied to a tree. At any node s of the tree with depth d, Let
us define N汰s, a) = π*(a∣s).t, and Nt(s, a) = P：=i ∏s(a∣s), where ∏k(a∣s) is the policy at time
SteP k. There exists some C and C such that
P(INt(S,a) - Nt(S,a)| > ιogt) ≤ C|A|t exp{- (log力)3}.
Proof 3 We denote the following event,
Erk = {k r(s0,.) - rk(s0,.) k∞< log(21 k) }.
Thus, conditioned on the event Tti=1 Ert and for t ≥ 4, we bound ∣Nt(s, a) - Nt (s, a)∣ as
tt
∣Nt(s,a) - Nt(s,a)∣ ≤ X ∣∏k(a|s) - ∏t(a∣s)∣ + X λk
k=1	k=1
tt
≤ X k ∏k (.∣s)-∏*(.∣s) k∞ + X λk
k=1	k=1
tt
≤ X k ∏k(.∣s)-∏*(.∣s) kp+ Xλk
k=1	k=1
tt
≤ L X k Qk (s0,.) - Q (s',) kp + X λk (Lemma 2)
k=1	k=1
tt
≤ L|A|P X k Qk(s0,.) - Q(s0,.) k∞ + X λk( Property ofP-norm)
tt
≤ L|A|1Yd X k Irk(s00,.) - r(s00,.) k∞ + X λk(Contraction 3.1)
tt
≤L1A11 γ dX d⅛+X λk
≤ L|A| P γd Zt	2σ	dk + Zt I JA1 7、dk
k=0 log(2 + k)
k=0 log(1 + k)
V Ct
一log t
14
Under review as a conference paper at ICLR 2021
for some constant C depending on |A|, p, d, σ, L, and γ . Finally,
P(∣Nt(s,a)- Nt (s,a)∣ ≥
tt	t
≤ XP(Ect)=X 4lAleχp(-(M2+^F)
i=1	i=1
≤ 4|A|teXp(— (log(2t+ t))3)
=O(t exp(-(iθg⅛y)).
Lemma 4 Consider the E3W policy applied to a tree. At any node s of the tree, Let us define
Ntt(s, a) = πt (a|s).t, and Nt(s, a) as the number of times action a have been chosen until time
step t. There exists some C and C such that
P |Nt(s, a) - Ntt(s, a)| >
t
≤ Ct exp{- (iogτy}.
Proof 4 Based on the result from Lemma 3, we have
≤ P(Nt(S, a) - Nt(s,a)∣ >
≤ Ctexp{-(0⅛}
+ P(INt(S, a) - Nt(S, a)| >
≤ 4|A|t exp{-(iog(2t+1))3 } + 2|A| exp{-(iog(2t+1))2 }(Lemma 3 and (16))
≤ Oexp(-(0⅛)).
Theorem 1 At the root node S ofthe tree, defining N(s) as the number ofvisitations and VΩ* (s) as
the estimated value at node s, for > 0, we have
P(I%(s) - Vt(S)I > e) ≤ Cexp{-	N⑶;	}.
C(log(2 + N (S)))2
Proof 5 We prove this concentration inequality by induction. When the depth of the tree is D = 1,
from Proposition 1, we get
IVΩ(S) — Vt(S)I =k Ω*(Qω(s, .)) - Ω*(QΩ(s, .)) k∞≤ Y k r - r* k∞ (Contraction)
where r is the average rewards and rt is the mean reward. So that
P(∣Vω(s) - Vt(S)I > e) ≤ P(γ k r - r* k∞> e).
From Lemma 1, with E = iog(2+N(S)), we have
p(∣Vω(s) - VΩ(S)I > E) ≤ P(Y k r - r* k∞> E) ≤ 4IAI exp{-孙屋+%(S)))2 }
L ʃ	N (S)E
=C exp{ -	}.
C(log(2 + N (S)))J
Let assume we have the concentration bound at the depth D 一 1, Let US define VΩ(Sa) = Qω(s, a),
where Sa is the state reached taking action a from state S. then at depth D - 1
P(IVΩ(Sa)- Vt(Sa)I > E) ≤ C eXP{-C(W))2	QT)
Now at the depth D, because of the Contraction Property, we have
IVΩ(S) - Vt(S)I ≤ γ k Qω(S,.) - QΩ(S,.) k∞
=YIQC(S,a) - ^(怎叫.
15
Under review as a conference paper at ICLR 2021
So that
P(∣Vω(s) - W(S)I〉c) ≤ P(Y k Qω(s, a) - QΩ(s, a) k> E)
L L ʃ	N(Sa)E	I
≤ Ca exp{ - -7-----------------}
一	PI Ca (lθg(2 + N(Sa )))2 ʃ
L L ʃ	N(Sa)E I
≤ Ca exp{ -	}.
- PI Ca (log(2 + N (S)))2,
From (17), we can have limt→∞ N(Sa) = ∞ because if ∃L, N(Sa) < L, we can find E > 0
for which (17) is not satisfied. From Lemma 4, when N(S) is large enough, we have N(Sa) →
π*(a∣S)N (s) (foreXamPle N (Sa) > 2 π*(a∣S)N (s) ), thatmeans we can find C and C that satisfy
P(I%(s)- W(S)I >e) ≤ Cexp{-
N (s)e
C(log(2 + N (S)))2
}.
Lemma 5 At any node S of the tree, N(S) is the number of visitations. We define the event
Es = {∀ a in A, ∣N(s, a) — N*(s, a)∣ < N ； a) } where N*(s, a) = ∏*(0∣s)N(s),
where e > 0 and VΩ* (s) is the estimated value at node s. We have
P(∣Vω(s) - VΩ(s)∣ > E∣Es) ≤ Cexp{-	N(S):	}.
C(log(2 + N (s)))2
Proof 6 The Proof is the same as in Theorem 2. We Prove the concentration inequality by induction.
When the dePth of the tree is D = 1, from ProPosition 1, we get
∣VΩ(s) - VΩ(s)∣ =k Ω*(Qω(s, .)) - Ω*(QΩ(s, .)) k≤ Y k r - r* k∞ (Contraction Property)
where r is the average rewards and r* is the mean rewards. So that
P(∣VΩ(s) - VΩ(s)I >e) ≤ P(y k r - r* k∞> e).
From Lemma 1, with E = iog(2+N(§))and g^ven Es, we have
P(∣Vω(s) -	VΩ(s)∣ >e)	≤	P(y	k r -	r*	k∞>	e)	≤	4∣A∣ exp{-	N(?E	^2 }
2σY(log(2 + N (S)))2
C r	N(s)e	η
=C exp{ --X-----------------}.
C(log(2 + N(S)))2 '
Let assume we have the concentration bound at the depth D — 1, Let US define VΩ(sa) = Qω(s, a),
where Sa is the state reached taking action a from state S, then at depth D - 1
P(∣VΩ(Sa) - V*(Sa)∣ >E) ≤ C exp{-
N(Sa)E
ʌ , . . 「
C(log(2 + N(Sa)))2
}.
Now at depth D, because of the Contraction Property and given Es, we have
∣VΩ(s) - VΩ(s)I ≤ γ k Qω(s,.) - QΩ(s,.) k∞
=Y∣Qω(s, a) - QΩ(s, a)∣(∃a, satisfied).
So that
P(∣VΩ(s) - V*(s)∣ >e) ≤ P(γ k Qω(s, a) - QΩ(s,a) k> E)
ZC ʃ	N(Sa)E
≤ Ca exp{— -----------------}
—	Ca(lθg(2 + N(Sa)))2 ʃ
ZC ʃ	N(Sa)E
≤ Ca exp{— -----------------}
—	Ca(log(2 + N(S)))2'
≤ Cexp{- ------N(S)E------}(because of Es)
—	PI C(log(2 + N(S)))2 八 J ss
16
Under review as a conference paper at ICLR 2021
Theorem 2 Let at be the action returned by algorithm E3W at iteration t. Then for t large enough,
with some constants C, C,
P(at = a*) ≤ Ct exP{-门 JG}.
Cσ(log(t))3
Proof 7 Let Us define ^vent Es as in Lemma 5. Let a* be the action with largest value estimate at
the root node state s. The probability that E3W selects a sub-optimal arm at s is
P(at = a*) ≤ XP(VΩ(Sa)) > VΩ(sa*)|Es) + P(Ec)
a
=X P((VΩ(Sa) - VΩ(Sa)) - (VΩ(Sa* )- ^(S“* )) ≥ 吟(Sa* )- VΩ(Sa)∣Es) + P(Ec).
a
Let us define ∆ = V*(sa*) 一 VΩ*(sa), thereforefor ∆ > 0, we have
P(at = a*) ≤ XP((VΩ(Sa) - VΩ*(Sa)) - (VΩ(Sa*) - VΩ(Sa*)) ≥ ∆∣Es) + +P(Ec)
a
≤ X P(∣VΩ(Sa) - VΩ(Sa)∣ ≥ α∆∣Es) + P(∣VΩ(sa*)-匕屈* )| ≥ β∆∖Es)+ P(Ec)
a
≤ X Ca exp{- ` N(S)(2 ——} + Ca* eχp{-r_N⑹(——} + P(Ec),
^V a	Ca(log(2 + N(S)))2	Ca*(log(2 + N(S)))2
where α +β = 1, α > 0, β > 0, and N(S) is the number of visitations the root node S. Let us define
^ = min{ (OC^), (C**)}, and C = JAj max{Ca, Ca*} we have
t
P(a 6= a* ) ≤ C exp{-
ʌ , . 「
Cσ(log(2 + t))2
} + P(Esc).
00
From Lemma 4, ∃C , C for which
P(Ec) ≤ C0texp{-，}，
so that
P(a = a*) ≤ O(t exP{- (logtt))3 }).
Theorem 3 Consider an E3Wpolicy applied to the tree. Let Ki = VΩ*(ai∣S) + LqCσ log Cδ/2n,
χi
VΩ*(ai∣S) — LyJCσ2log CC/2n, where VΩ*(.∣s) is the policy with respect to the mean value
vector V(∙) at the root node S. For any δ > 0, with probability at least 1 一 δ, ∃ constant L,p, C, C
so that the pseudo regret Rn satisfies
nV * - n X Vi (κi + P (τ¾γL^)) ≤ Rn ≤ nV * - n X Vi & - IL (τ¾γ^)).
Proof 8 From Lemma 2 given two policies π ⑴=VΩ*(r ⑴)and π(2) = VΩ*(r ⑵)，∃L, such that
k ∏⑴一∏⑵ kp≤ L k r⑴一r⑵ kp≤ L1 k r⑴一r⑵ ∣∣∞ .
p
From (13), we have the regret
n
Rn = nV * - X Vi X ∏t(ai∣S),	(18)
i	t=1
where ∏t(∙) is the policy at time step t, and I(∙) is the indicator function. V * is the optimal branch
at the root node, Vi is the mean value function ofthe branch with respect to action i, V(∙) is the |A|
17
Under review as a conference paper at ICLR 2021
vector of value function at the root node. V (∙) is the ∖A∖ estimation vector of value function at the
root node. π(.∖s) = VΩ*(V (∙)) is the policy with respect to the V (∙) vector at the root node.
Thenfor any δ > 0, with probability at least 1 一 δ, we have
............................. L、、…、3................... 、
∖π(αi∖s) — ∏t3∖s)∖ ≤∣∣ ∏(∙∖s) 一 ∏t(.∖s) ∣∣∞≤ — k V(∙) - VS ∣∣∞ (Lemma 2)	(19)
P
≤ P∖V(∙) - V(∙)∖≤ P (I + S¾F)(Theorem4)
So that
,,、L(T (Uω - Lω )	IC σ2 log 字、 一，、	，L(T (Uω - Lω)	IC σ2 log 字、
Maik) 一 万(1 - δ + V	2N(s)	) ≤	πt(ai∖s)	≤	π(ai∖s)	+ p(	1 - δ + V	2N(s))
so that
Rn = nV*- X V X πt(αi∖s)	≤ nV*- X	V X	(π(ai∖s)	- L	(MUIuG	+ JCσ?g 亨))
i t=1	i t=1	P	Y
Rn ≤ nV *-X Vi X (∏(ai∖s) - L (τ¾⅛) + S C^))
P -	n
i t=ι	P	Y
Rn ≤ nV* - n X ¼(π(ai∖s) - P (T(UI-L) + SCσ 2：g δ ))
(20)
And
Rn ≥ nV * - X Vi X gai ∖s) + L (τ¾δM + S c⅛1 ))
P -	n
i	t=ι	t	Y
Rn ≥ nV * - n X Vi(π(ai∖s) + L ( T(U： - L) + S C^ ))
P 1 - δ	2n
In case of Maximum Entropy and Relative Entropy P = 1, because
Il π⑴-π⑵ ∣∣∞≤ L ∣ r⑴一r⑵ ∣∣∞ .
So that we havefor MENTS
nV * - n X Vi[κi + L( T^gjAI)) ≤ Rn ≤ nV * - n X % (χi - L( T^gJAI)).
T `	ι一γ ,	τ `	ι-γ ,
For RENTS, we have
nV * - n X ¾(κi + L( T (I”川一
T '	1-γ
“R " "*	T(T(l°g ∖A∖ -
≤ Rn ≤ nV - n) Vi (Xi - L(-----ɪ--
T 、	1 - Y
where m = mina π(a∖s).
In case of Tsallis Entropy P = 2 (Niculae & Blondel (2017)), so that
∖~rL ∕∖A∖ - 1
^V (κi + 2 (1 -
T
T
nV * — n
—)≤ Rn ≤ nV * - n
X v⅛- 2 (¾jɪ 1 -
Y
Before derive the next theorem, we state here the Theorem 2 in Geist et al. (2019)
• Boundedness: for two constants LQl and Uω such that for all π ∈ Π, we have Lω ≤ Ω(π) ≤
UQI, then
V*(s) - T(Uω - LG ≤ V*(s) ≤ V*(s).	(21)
1 - Y
18
Under review as a conference paper at ICLR 2021
Where τ is the temperature and γ is the discount constant.
Theorem 4 For any δ > 0, with probability at least 1 一 δ, the eq satisfies
一
Cσ2 log C T(Uω 一 Lω)
---------------------
2N(s)
1 一 Y
≤ ε ≤. !Cσ logCl
≤ ω ≤ y	2N(S).
Proof 9 From Theorem 2, let us define δ = C exp｛一｝, so that E =，C；N：g)二 thenfor any
δ > 0, we have
/ Cσ2 log C
P(∣Vω(s) - VΩ(s)l ≤ N 2N(S) δ ) ≥ 1 - δ.
Then, for any δ > 0, with probability at least 1 一 δ, we have
/ Cσ2 log C
ivω(S)- vω(S)I ≤ y 2N(s)
- S CiNogɪ ≤ vω(S)-吗(S)
- SC2Nlog) δ + 吟(S) ≤ %(S)
≤ 4 ICσσ2 log CT
≤ y	2n(s)
/ Cσ2 log C
≤√ inrl + 琦(s).
From Proposition 1, we have
/Cσ2 log C	τ(Uω 一 Lω)	/Cσ2 log C
- V F⅛l + V*(S)- j⅛-9 ≤ %(S) ≤ V F⅛δ + V*(S).
19