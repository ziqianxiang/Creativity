Under review as a conference paper at ICLR 2021
Factoring out prior knowledge from low-
DIMENSIONAL EMBEDDINGS
Anonymous authors
Paper under double-blind review
Ab stract
Low-dimensional embedding techniques such as tSNE and UMAP allow visual-
izing high-dimensional data and therewith facilitate the discovery of interesting
structure. Although they are widely used, they visualize data as is, rather than
in light of the background knowledge we have about the data. What we already
know, however, strongly determines what is novel and hence interesting. In this
paper we propose two methods for factoring out prior knowledge in the form of
distance matrices from low-dimensional embeddings. To factor out prior knowl-
edge from tSNE embeddings, we propose Jedi that adapts the tSNE objective in
a principled way using Jensen-Shannon divergence. To factor out prior knowledge
from any downstream embedding approach, we propose Confetti, in which we
directly operate on the input distance matrices. Extensive experiments on both
synthetic and real world data show that both methods work well, providing em-
beddings that exhibit meaningful structure that would otherwise remain hidden.
1	Introduction
Embedding high dimensional data into low dimensional spaces, such as with tSNE (van der Maaten
& Hinton, 2008) or UMAP (McInnes et al., 2018), allow us to visually inspect and discover mean-
ingful structure from the data that would otherwise be difficult or impossible to see. These methods
are as popular as they are useful, but, at the same time limited in that they are one-shot only: they
embed the data as is, and that is that. If the resulting embedding reveals novel knowledge, all is
well, but, what if the structure that dominates it is something we already know, something we are no
longer interested in, or, if we want to discover whether the data has meaningful structure other than
what the first result revealed? In word embeddings, for example, we may already know that certain
words are synonyms, while in single cell sequencing we may want to discover structure other than
known cell types, or factor out family relationships. The question at hand is therefore, how can we
obtain low-dimensional embeddings that reveal structure beyond what we already know, i.e. how to
factor out prior knowledge from low-dimensional embeddings?
For conditional embeddings, research so far mostly focused on emphasizing rather than factoring
out prior knowledge (De Ridder et al., 2003; Hanhijarvi et al., 2009; Barshan et al., 2011), with
conditional tSNE as notable exception, which, however, can only factor out label information (Kang
et al., 2019). Here, we propose two techniques for factoring out a more general form of prior
knowledge from low-dimensional embeddings of arbitrary data types. In particular, we consider
background knowledge in the form of pairwise distances between samples. This formulation allows
us to cover a plethora of practical instances including labels, clustering structure, family trees, user-
defined distances, but also, and especially important for unstructured data, kernel matrices.
To factor out prior knowledge from tSNE embeddings, we propose Jedi, in which we adapt the
tSNE objective in a principled way using Jensen-Shannon divergence. It has an intuitively appealing
information theoretic interpretation, and maintains all the strengths and weaknesses of tSNE. One of
these is runtime, which is why UMAP is particularly popular in bioinformatics. To factor out prior
knowledge from embedding approaches in general, including UMAP, we hence propose Confetti,
which directly operates on the input data. An extensive set of experiments shows that both methods
work well in practice and provide embeddings that reveal meaningful structure beyond provided
background knowledge, such as organizing flower images according to shape rather than color, or
organizing single cell gene expression data beyond cell type, revealing batch effects and tissue type.
1
Under review as a conference paper at ICLR 2021
2	Related Work
Embedding high dimensional data into a low dimensional spaces is a research topic of perennial
interest that includes classic methods such as principal component analysis Pearson (1901), multi-
dimensional scaling (Torgerson, 1952), self organizing maps (Kohonen, 1982), and isomap (Tenen-
baum et al., 2000), all of which focus on keeping large distances intact. This is inadequate for data
that lies on a manifold that resembles a Euclidean space only locally, which is the case for high
dimensional data (Silva & Tenenbaum, 2003) and for which we hence need methods such as locally
linear embedding (LLE) (Roweis & Saul, 2000) and stochastic neighbor embedding (SNE) (Hinton
& Roweis, 2003) that focus on keeping local distances intact. The current state of the art methods
are t-distributed SNE (tSNE) by van der Maaten & Hinton (2008) and Uniform Manifold Approx-
imation (UMAP) by McInnes et al. (2018). Both are by now staple methods for data processing,
e.g. in biology (Becht et al., 2019; Kobak & Berens, 2019) and NLP (Coenen et al., 2019). As they
often yield highly similar embeddings (Kobak & Linderman, 2019) it is a matter of taste which one
to use. While tSNE has an intuitive interpretation, despite recent optimizations (van der Maaten,
2014; Linderman et al., 2019) compared to UMAP it suffers from very long runtimes.
Whereas the above consider only the data as is, there also exist proposals that additionally take
user input and/or domain knowledge into account. For specific applications to gene expression,
it was proposed to optimize projections of gene expression to model similarities in corresponding
gene ontology annotations (Peltonen et al., 2010). More recently, attention has been brought to
removing unwanted variation (RUV) from data using negative controls in particular in the light
of gene expression, assuming that the expression can be modeled as a linear function of factors
of variation and a normally distributed variable (Gagnon-Bartsch & Speed, 2012). This approach
has been successfully applied to different tasks and domains of gene expression (Risso et al., 2014;
Buettner et al., 2015; Gerstner et al., 2016; Hung, 2019). Here, we are interested to develop a domain
independent method to obtain low-dimensional embeddings while factoring out prior knowledge.
For that, we neither want to assume a functional relationship between prior and input data, nor do
we want to assume a particular distribution of the input, but keep the original data manifold intact.
Furthermore, we do not want to rely on negative samples that have to be known and present in the
data to be able to factor out the prior.
The general, domain independent methods supervised LLE (De Ridder et al., 2003), guided LLE
(Alipanahi & Ghodsi, 2011), and supervised PCA (Barshan et al., 2011) all aim to emphasize rather
than factor out the structure given as prior knowledge. Like us, Kang et al. (2016; 20l9); Puolamaki
et al. (2018) factor out background knowledge, but are much more limited in the type of prior knowl-
edge. In particular, Puolamaki et al. (2018) requires users to specify clusters in the embedded space,
Kang et al. (2016) requires background knowledge for which a maximum entropy distribution can
be obtained, while Kang et al. (2019) extend tSNE and propose conditional tSNE (ctSNE) which
accepts prior knowledge in the form of class labels. In contract, we consider prior knowledge in the
form of arbitrary distance metrics, which can capture relative relationships which appears naturally
in real world data, such difference in age, geographic location, or level of gene expression. We pro-
pose both, an information theoretic extension to tSNE, and an embedding-algorithm independent
approach to factor out prior knowledge. 3
3 Theory
We present two approaches, with distinct properties, that both solve the problem of embedding high
dimensional data while factoring out prior knowledge. We start with an informal definition of the
problem, after which we introduce vanilla tSNE. We then present our first solution, Jedi, which
extends the tSNE objective to incorporate prior information. We then present Confetti, which
uses an elegant yet powerful idea that allows us to directly factor out prior knowledge from the
distance matrix of the high dimensional data, which allows Confetti to be used in combination
with any embedding algorithm that operates on distance matrices.
3.1 THE PROBLEM — INFORMALLY
Given a set of n samples X from a high dimensional space, e.g. IRd, our goal is to find a low
dimensional representation Y in IR2 that captures the local structure in X while factoring out prior
2
Under review as a conference paper at ICLR 2021
knowledge Z about the samples. Here, we consider both high dimensional data X and prior Z to
be given as distance matrices, thus allowing for data from typical spaces such as Euclidean, but also
images, up to unstructured data such as texts or graphs, for which distance matrices can be specified
using a kernel. When embedding X, our goal is to embed the samples such that the pairwise low
dimensional distances DY resemble high dimensional distances DX locally, but are distinct to the
prior distances DZ . Informally, we can state this goal as finding an embedding Y subject to
DX ≈ DY 6≈ DZ .
We could formally define this as a multi-objective problem composed of a minimization over the
difference between DX and DY and a maximization of the difference between DY and DZ . Be-
sides how to measure these differences, there are two problems that render classic multi-objective
optimization impractical. First, the two functions are highly imbalanced, with the minimization ob-
jective obtaining its optimum at 0 and the maximization at +∞, hence we need to constrain the
optimization. Second, we want to put emphasis on correctly reconstructing local structure, as this
yields superior visualizations (van der Maaten & Hinton, 2008; McInnes et al., 2018).
3.2	THE PROBLEM - INFORMATION THEORETICALLY
The t-distributed Stochastic Neighbor Embedding (tSNE) (van der Maaten & Hinton, 2008) is a
state-of-the-art approach for embedding data into low dimensional spaces that preserves the local
structure of the high dimensional data. In particular, it models the local neighborhood of a point
by casting the pairwise distances into similarity distributions that express for each point i the likeli-
hood of observing point j as neighbor, given by pj|i. For the high dimensional distances DiXj , this
likelihood is approximated by a Gaussian kernel centered at point i
_	eχp(TDX 产性2)
pjli = Pk= exp(-(DX)2∕2σ2).
To account for varying densities of points in the space, the variance σi is dependent on where the
kernel is centered. Given the user specified parameter perplexity, which can be thought of as an
estimate of the neighborhood size, we can solve perplexity = 2H(Pi) for σi for each point i, where
H(Pi) = Pj pj|i logpj|i is the entropy. By symmetrizing the conditional probabilities, the joint
probability of a pair of points is given as Pij = pjli/il j, which yields the desired local similarity
representation of high dimensional points.
The low dimensional point similarities qij are represented by a t-distribution instead of a Gaussian,
which solves the crowding problem1 due to its heavy tails. We thus get low dimensional similarities
=(1 + (DY )2)-1
qij = Pk=ι (1 + (DQ2)τ∙
The goal of tSNE is to model pairs of points exhibiting a high similarity in the high dimensional
space to have a high similarity in the low dimensional space. This is achieved by minimizing the
Kullback-Leibler Divergence (KL), given by DKL(P || Q) = P= Pijlog pj, for the pairwise
probabilities. This information theoretic measure yields the number of excess bits needed if we
would encode P using a code optimal for encoding Q and thus models how well Q approximates
P. Minimizing the KL divergence with respect to Y , we get a non-convex objective that we can
practically optimize using gradient descent. Using a similar notion of neighborhood distributions,
we can now define anew objective that instantiates our objective using tools from information theory.
3.2.1	Factoring out prior information with Jedi
To incorporate prior information into the tSNE objective, we first need to model the neighborhood
distribution P0 of the prior. Similar to the high dimensional data, we use a Gaussian kernel by which
we hence put emphasis on samples that are close according to the prior, defined as
o =	exp(-(DZ )2∕2σ2)
Pj1i = Pk=i exp(-(DZ)2∕2σ2),
1The crowding problem is the phenomenon of assembling all points in the center of the map, due to the
accumulation of many small attractive forces as moderate distances are not accurately modelled.
3
Under review as a conference paper at ICLR 2021
with σi a perplexity parameter which describes the desired neighborhood size in the prior space.
We thus search for a similarity distribution Q of points Y in the low dimensional space, that is
similar to the high dimensional similarities P but different from the prior similarities P 0. Similar to
tSNE, the first term of our objective corresponds to minimizing the KL divergence, thus a natural
extension would be to add a second term that rewards maximizing the KL divergence between the
distances of embedding Q and prior P0. This would be naive, however, as this second term would
dominate the optimization because KL divergence is unbounded. Furthermore, it would not allow
us to exploit the asymmetry of divergence in the one or the other direction.
To mitigate the unboundedness, the skewed KL divergence has been proposed, mixing the two
distributions DKβL(P || Q) = DKL(P || (1 - β)P + βQ) with β ∈ [0, 1] controlling skewness and
thus boundedness (see e.g. Yamano (2019)). To obtain symmetry, the β-Jensen-Shannon divergence
defined as JSe(P || Q) = 1 (DKL(P || Q) + DKL(Q Il P)) Was introduced. Based on these ideas,
we propose a new divergence, which we call parameterized Jensen-Shannon Divergence (pJSD),
Which alloWs to control for both, the level of skeWness as Well as the level of symmetry, and prove
that pJSD is bounded.
Definition 1 (Parameterized Jensen Shannon Divergence). For two probability distributions P0 and
Q we define the parameterized Jensen-Shannon divergence as
JSa(P Il Q) = αDκL(P0 Il βQ + (1 - β)P0) + (1 - α)DκL(Q ∣∣ βP0 + (1 - β)Q),
where 0 ≤ α ≤ 1 determines the level of symmetry and 0 < β < 1 determines the level of skewness.
Theorem 1 (Upper bound on pJSD). For 0 ≤ α ≤ 1 and 0 < β < 1 the parametrized JS divergence
is bounded from above by
JSβα ≤ - log(1 - β).
We provide a proof in App. A.1.2. Putting the pieces together, We can noW formulate our objective
as the minimization of the KL divergence betWeen the similarity distributions of X and Y , and the
maximization of the pJSD betWeen the similarity distributions of Y and Z, Which is
argminDKL(PIIQ)-JSβα(P0 II Q).
While the parameters α, β give the user flexibility on hoW much of the prior distribution should be
factored out, We Will later discuss a good default parameter instantiation based on synthetic data.
This objective, similar to vanilla tSNE and related approaches such as ctSNE, is not convex, and is
optimized using gradient descent. We provide the derivation of the gradients in App. A.1.1. This
provides us With a method that solves our problem of factoring out prior knoWledge on information
theoretic grounds, Which We call JEDI in resemblance of the Jensen Shannon Divergence.
Computational Complexity The computational and memory complexity of JEDI is in O(kn2),
for n samples and k iterations, Which comes from the summation over all pairs of sample in the
divergences in each iteration. Due to the interactions in the gradient of pJSD, standard algorithmic
optimizations of tSNE (van der Maaten, 2014; Linderman et al., 2019) are not directly applicable.
Overall, Jedi is a poWerful, theoretically appealing approach to factor out prior knoWledge. Based
on tSNE, Jedi inherits many of its strengths and Weaknesses. In particular runtime and strong
emphasis on local structure make it hard to successfully apply tSNE, and thereWith Jedi, to datasets
that are either very large and/or contain structure at different scales, i.e. data as typically considered
in bioinformatics. In the next section, We therefore revisit the problem, and propose an embedding
algorithm independent approach that is applicable to such settings.
3.3	THE PROBLEM - ALGORITHM INDEPENDENTLY
UMAP is one of the state-of-the-art competitors to tSNE that alleviates its draWbacks for large data
that not only contain local structure. Rather than presenting a dedicated solution for UMAP, We
here propose a general, embedding-formulation independent approach. To do so, We have to revisit
the original problem formulation, Where the goal is to approximate high dimensional distances With
embedding distances While simultaneously keeping them far from the prior. The key idea here
is that, if We factor out the prior knoWledge from the high dimensional distances directly, We are
4
Under review as a conference paper at ICLR 2021
independent of the actual embedding process, and hence any embedding algorithm can be used.
Informally, We can state this as (DX ㊀ DZ) ≈ DY, where ㊀ describes some yet to be defined way
of factoring out prior knowledge Z from the distances over high dimensional data X. Once we have
this operator, we can use any distance metric based embedding algorithm on its result to obtain high
quality embeddings Y from which Z has been factored out. Clearly, the operator should result in a
proper distance metric, discard any structure that is evident and keep any structure that is not evident
given prior knowledge Z. W.l.o.g., for the remainder of this section we assume that the distances D
are ScaIed to D0 = d1— D, with Dmax the maximum value in D. We define operator ㊀ as
Dmax
(DX θλ DZ )j = IDX - 1 λDZ + λ i= j,
Dij	i = j ,
which is to say, we subtract the information given by the prior distances from the high dimensional
distances in a linear form, with λ controlling how much prior to be removed. Although surprisingly
simple, this elegant definition has very convenient properties that render it very powerful. First, there
is only a single parameter λ, which due to linearity gives the user direct and interpretable control
over how much the prior information should be taken into account. Second, the distance matrix we
obtain by applying the operator maintains metric properties - the proof can be found in App. A.1.3
一 that are required to properly optimize downstream embedding algorithms. Without the guarantee
of symmetry and triangle inequality optimizing over pairwise distances is not possible.
Theorem 2 (Metric). Assuming that DX and DZ are based on valid metrics, for any λ > 0,
(DX θλ DZ) fulfills the metric axioms of non-negativity, symmetry, identity, and triangle inequality.
Furthermore, the operator has the property of maintaining the original structure under an unin-
formative prior. More formally, we define NkD(i) = {j ∈ kNN (i) according to D} to be the
k-neighborhood of sample i according to distances Dij . For simplicity of notation, we will assume
that all distances are distinct, the results and definitions can directly be generalized to the case of
equal distances. Assuming an uninformative prior, which was generated independently of the high
dimensional data DZ ⊥ DX, on expectation the neighborhoods of each point stay the same.
Theorem 3 (Uninformative prior (proof in App. A.1.4)). Assume the prior is uninformative, that
is DZ ⊥ DX. Furthermore, the distances are normalized to DiZj ∈ [0, 1]. For fixed λ > 0 let
(Fλ )ij = DX θλ DZ. On expectation, the neighborhoods in X and in X with factored out prior
are the same, that is ∀i, k. NkFλ (i) =E[.] NkDX (i).
This theorem proves that the embeddings obtained from this distance matrix are expected to be
robust against priors unrelated to the input, and thus no knowledge is lost and no spurious knowlegde
is generated. Both the metricity as well as the robustness against uninformative priors are essential
for generating good embeddings, which more complex formulations can not provide that easily.
Normalizing the distances as discussed above, and applying the θλ to factor out prior knowledge, we
obtain an embedding algorithm independent method to factor out prior knowledge. In reminiscence
of how the plots look, we refer to this method as Confetti and give its pseudocode as Alg. 1.
Complexity CONFETTI runs in time O(n2), which includes the normalization of the distance ma-
trix and computation of the operator, which only counts towards a very small constant. Additionally,
we will need to run an embedding algorithm, which respective runtime is added to O(n2).
4	Experiments
We evaluate on both synthetic and real world data. We make the implementations of Jedi and
Confetti available online.2 Since there does not exist any direct competitor that can factor out
arbitrary distance matrices from an embedding, we compare to two closest competitors. The first is
ctS NE (Kang et al., 2019), which extends the tSNE objective and can factor out prior information
given as cluster labels. The second is supervised LLE (De Ridder et al., 2003), which although
originally designed to emphasize structure in the embedding given as labels, we can modify such
that it instead emphasizes any structure not in the prior. We give the details for this modification,
which we refer to as sLLE-1, in App. B.1. For fair comparison, we optimize all parameters via grid
search on a synthetic data hold out set, and use these throughout all experiments (see App. B.2).
2Available after publication.
5
Under review as a conference paper at ICLR 2021
(a)ctSNE.	(b)SLLE-1.	(c) CONFETTI.	(d) JEDI.
Figure 1: Synthetic data. Shown are conditional embeddings of synthetic data given resp. ground
truth labels for ctSNE and sLLE-1 (a,b), and euclidean distance for Confetti and Jedi (c,d).
Colors correspond to cluster assignment over dimensions 1-8, shape (circle, square, triangle, and
cross) to cluster assignment over dimensions 9-12. In b),c), and d), each of the four clusters consists
of samples of one shape.
4.1	Reliably factoring out distance priors
We first consider synthetic data with known ground truth. In particular, we consider synthetic data
of n = 2 000 samples over 14 dimensions, where dimensions 1-8 and 9-12 both exhibit 4 clusters
of different sizes, while dimensions 13 and 14 are Gaussian noise. We give more details, as well
as a tSNE plot in App. B.3. The cluster structure over the first 8 dimensions dominates the tSNE
embedding. To discover information beyond these clusters, we provide Jedi and Confetti the
euclidean distances over these 8 dimensions. To enable a fair comparison to ctSNE and sLLE-1,
which are limited to label priors, we provide the ground truth cluster assignment as background
knowledge. All methods finished within minutes, and we plot the results in Fig. 1. Although given
the true labelling, ctSNE fails to satisfactorily factor out the prior knowledge, whereas our methods
yield the 4 distinct clusters from dimension 9-12. Notably, when we provide Jedi with the ground
truth label assignment, it yields similarly sharp clusters as sLLE-1 (see App. 13).
To objectively quantify how well prior knowledge is factored out from an embedding, we pro-
pose to measure the similarity over neighborhoods. For two distance matrices D, D0 and
neighborhood size k, we define the neighbourhood overlap score (NOS) as NOS(D, D0 , k) =
n 1 pn=1 ∣{kNN of i in D}∩{kNN of i in D0}|. Correspondingly, for a distance matrix D and label
distribution L, We get NOS(D, L,k) = 11 Pn=I ∣L^ ∖{kNN of i in D} ∩ {Li}|. While this score
lends itself for evaluation, it is hard to directly optimize (see App. B.3). Plotting NOS(DX, DY , )
for all neighborhood sizes k = 1 . . . n alloWs us to asses hoW Well We preserve information of
the original data, Whereas plotting NOS(DX , DZ) alloWs us to asses hoW Well We factor out prior
knoWledge from an embedding. As the id-line corresponds to a random neighbor encounter, We can
measure the area betWeen the NOS curve and id-line as a proxy for hoW Well We preserve informa-
tion, respectively hoW Well We factor out prior knoWledge.
For the synthetic data, the area betWeen the curves and the id-line for ctSNE, sLLE-1, Jedi, and
CONFETTI are .237, .344, .340, .344 When We compute the NOS betWeen embedding and input data
Without prior. For the NOS betWeen embedding and prior labels We have .037, .005, .003, .022, re-
spectively (plots are given in App. Fig. 14). We see that Jedi factors out the prior almost ideally,
as Well as sLLE-1, and that Confetti performs slightly Worse especially in small neighborhoods.
When evaluating the NOS With regard to the euclidean distance prior, Confetti beats all other
methods With an area betWeen curves of only 0.002. Overall, ctSNE shoWs the Worst NOS per-
formance, Which is also evident in the embedding (see Fig. 1). As for information captured in the
embedding from the non-prior dimensions, Jedi, Confetti, and sLLE-1 do equally Well, putting
ctSNE at a distance. Overall, Jedi and Confetti are perform on par With the state of the art given
label priors, but perform at least as Well given continuous priors, for Which ctS NE and sLLE-1 are
not applicable.
6
Under review as a conference paper at ICLR 2021
(a) tSNE embedding.
(b) Jedi embedding.
(c) Confetti + tSNE.
Figure 2: Real world data. Embedded are the sum of chi-square matrices for color, local shape and
texture, boundary shape, and spatial petal distribution of the Oxford flower data. Shown are vanilla
tSNE (left), and Jedi (middle) and Confetti (right) with HSV color distances as prior knowledge.
4.2	Recovering flower geometry
To evaluate on real data, we consider the Oxford flower dataset from Nilsback & Zisserman (2008),
which consists of over 8000 images of flowers of 102 different classes. The data is given as a set
of four pairwise distance matrices which are the Chi-squared distances of the color (HSV), the local
shape and texture, the shape of the boundary, and the spatial distribution of petals of the flower, all
computed on segmented flower images. The available implementations for ctSNE and sLLE-1 are
not applicable to distance matrices as input X , and hence we could not compare to these methods.
We will use the sum of all four matrices as high dimensional input. To keep the results interpretable,
we subsample 40 images from 25 different classes each, by which we have n = 1000 samples.
We are interested whether our algorithms is able to factor out a known prior that dominates an
embedding in a real world setting, which is why we specify the HSV color matrix as background
knowledge. While other priors could be specified, we would not know if that prior would be present
in the input data and thus would not be able to evaluate success. Confetti and Jedi terminate in
seconds, respectively two minutes.
We give the vanilla tSNE embedding in Fig. 2a, which besides a clustering according to colour con-
veys little other information. When we factor out the color information with Jedi and Confetti,
we see that colors mix and new clusters form according to other features. For example, spiky petals
arrange at the one side (Fig. 2b bottom, Fig. 2c bottom right), whereas rounded petals assemble on
the respective other side of the space. Similarly, flowers with few but large petals gather on one side
(Fig. 2b right, Fig. 2c top) and flowers with many thin petals on the respective other side. Apart
from these visual changes, we can evaluate based on the NOS plot, which shows that also for this
metric prior our methods are able to factor out the background knowledge (see App. Fig. 15).
4.3	Batch effects in single cell sequencing
One of the major applications of embeddings such as tSNE and UMAP is in single cell sequencing,
where it is a standard routine to visualize the sequencing data, allowing e.g. to assess the quality
of sequencing, to easily remove outliers from the data, or highlight differences between cohorts.
To test the methods on such data, we use a recent single cell data set of cerebrospinal fluid (CSF)
and whole blood of multiple sclerosis patients and a control group (Schafflick et al., 2020). We
generate a vanilla UMAP embedding using their original jupyter notebooks, and plot it as Fig. 3a. It
shows the typical clustering according to the cell types in blood and CSF, when coloring the samples
according to gene expression of standard marker genes (for more information refer to the original
paper). Here, we are interested whether our methods can reveal information beyond cell type. We
thus provide again the gene expression as input, and additionally take the euclidean distance between
the marker gene expression levels of each sample as prior, corresponding to how the coloring was
obtained. As ctSNE cannot deal with continuous priors, we instead provide it the cluster labels from
an agglomerative clustering of the marker gene expression levels, which yielded the same number
7
Under review as a conference paper at ICLR 2021
(a) UMAP embedding,	(b) CONFETTI +	UMAP	(c) ctSNE with labels of	(d)SLLE-1 with labels of
coloring according to cell	with marker gene	expres-	clustering marker gene ex-	clustering marker gene ex-
type.	sion as prior.	pression as prior.	pression as prior.
Figure 3: Real World Data Visualizations of the embedding of single cell sequencing dataofUMAP,
Confetti + UMAP, ctSNE, and sLLE-1. sLLE-1 failed to produce an embedding for the full
data, results are obtained for a random subset of the data. Coloring in b), c), and d) according to
batch ID, shape according to sample type (case vs control, and blood vs CSF).
of clusters as the original paper (see App. Fig. 16b). sLLE-1 requires a matrix inversion, and
for this data gives an error hinting that the data matrix is too large. Hence, we provide sLLE-1
with only a random subset of ~6000 samples to obtain an embedding. While this is not practical, as
important tasks such as outlier detection or assignment of cell types or states can only be done for
that particular subset, it allows us to compare the obtained embeddings. We provide sLLE-1 with
the same cluster labels as for ctSNE. ctS NE terminated within 23 minutes, sLLE-1 in 32 minutes,
Confetti in 10 minutes, and Jedi in 100h, the resulting embeddings are given in Figure 3, a vanilla
UMAP plot with coloring according to batch ID is given in App. Fig. 17c.
The results of Confetti show a surprising separation of samples from different batches and ac-
cordingly separation of the different tissue (blood and CSF), and of case and control along a man-
ifold. Encouraged by these results, we also apply Confetti with tissue information as prior, and
then observe that the previously separated clusters for actually equal cell types (B1,B2, NK1,NK2,
mDC1,mDC2) are now merged (see App. Fig. 17a), while all other information is kept. For ctSNE,
there are no clusters or manifolds directly visible and the result is a rather homogeneous ball, which
show only a clear separation of CD4 cells, which make up the largest proportion of cells (compare
App. Fig. 16a). Similar to ctSNE, Jedi shows no clear separation of cell types (see App. Fig. 17b),
but there is no bias towards CD4 cells as for ctSNE. sLLE-1 does not show any discernable struc-
ture beyond the provided cell types.
5	Discussion and Conclusion
In the experiments, we show that both Jedi and Confetti correctly factor out prior knowledge and
result embeddings that reveal previously hidden structure. On synthetic data, when provided with
euclidean distances capturing the structure which dominates the vanilla tSNE embedding, both our
methods reveal the clustering of the input data that is independent of the background knowledge.
Our closest competitor ctSNE, is not able to factor out the background knowledge well, and its
embeddings still show structure of the prior even when given the ground truth cluster labels. When
we provide our methods the same information, they do recover the correct clustering, demonstrating
their ability to reliably factor out both distances as well as label priors.
On real world data of flower images, where only distance matrices between attributes of the images
are available, both Jedi and Confetti are able to factor out the property dominating the vanilla
tSNE embedding, and organizing flowers according to number and shape of petals rather than by
color. This shows that both are able applicable to real world settings, and unlike their competitors,
can consider both input data and background knowledge in the form of different distance metrics.
This provides us with the advantage to be applicable in domains where the input is only available as
distance matrices, e.g. kernel-derived distances for unstructured data such as graphs or strings.
8
Under review as a conference paper at ICLR 2021
On the perhaps most widespread application of low dimensional embeddings, single cell gene ex-
pression data, we confirm that tSNE based approaches are a bad fit; both ctSNE and Jedi get
lost in local detail, fail to factor out the prior knowledge, and are slow. When we combine our
algorithm-independent approach, Confetti, with UMAP we do arrive at meaningful embeddings
from which the background knowledge has been factored out. In particular, when we provide it with
prior knowledge of marker gene expression - which is used to determine cell type - We arrive at
an embedding that organizes samples according to batch id and tissue type. Conversely, if provided
with tissue type as prior, we observe that previously separate clusters, that actually contain cells of
the same type, are merged.
In practice, a distance based prior is usually the most natural representation of the background
knowledge that cannot be cast easily into class labels. With binning or clustering using distances,
information is lost for differences within a class, and the relative information is lost between classes.
For example, effects based on geographic locations, age differences, or income that should be fac-
tored out from an embedding should be considered as is rather than cast into labels, which is not
possible with current approaches. Our approaches do take into account these relative relationships
by considering distances between pairs of samples, such as the geodesic distance between locations
of two individuals or their difference in age, which would be lost when encoded by labels.
Overall, both our proposals solve the problem of factoring prior knowledge from low dimensional
embeddings, and by allowing the prior knowledge to be specified as distances they are both applica-
ble to a large number of data types and domains. While Jedi has a theoretically appealing objective,
it inherits all strength and weaknesses of tSNE by which especially its runtime is an open problem.
For tSNE, several methods have been proposed that significantly speed up the optimization, yet none
of them seem directly applicable to Jedi. We leave their adaptation for future work. Furthermore,
although yielding overall good results, we would like to more closely investigate the parameters of
tSNE and Jedi in a more principled way, as for new domains parameter settings for e.g. tSNE are
mostly found by trial and error. Here, we tested our methods on different types distance metrics
that cover different applications, and with single cell gene expressions one of the most important
domains for low dimensional embeddings. In the future, we would also like to investigate how our
methods perform when provided with more exotic types of background knowledge.
9
Under review as a conference paper at ICLR 2021
References
Babak Alipanahi and Ali Ghodsi. Guided Locally Linear Embedding. Pattern recognition letters,
32(7):1029-1035, 2011.
Elnaz Barshan, Ali Ghodsi, Zohreh Azimifar, and Mansoor Zolghadri Jahromi. Supervised Principal
Component Analysis: Visualization, Classification and Regression on Subspaces and Submani-
folds. Pattern Recognition, 44(7):1357-1371, 2011.
Etienne Becht, Leland McInnes, John Healy, Charles-Antoine Dutertre, Immanuel WH Kwok,
Lai Guan Ng, Florent Ginhoux, and Evan W Newell. Dimensionality reduction for visualizing
single-cell data using umap. Nature biotechnology, 37(1):38-44, 2019.
F. Buettner, K. N. Natarajan, F. P. Casale, V. Proserpio, A. Scialdone, F. J. Theis, S. A. Teichmann,
J. C. Marioni, and O. Stegle. Computational analysis of cell-to-cell heterogeneity in single-cell
RNA-sequencing data reveals hidden subpopulations of cells. Nat Biotechnol, 33(2):155-160,
Feb 2015.
Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viegas, and Martin Wat-
tenberg. Visualizing and measuring the geometry of bert. arXiv preprint arXiv:1906.02715, 2019.
Dick De Ridder, Olga Kouropteva, Oleg Okun, Matti Pietikainen, and Robert PW Duin. Super-
vised locally linear embedding. In Artificial Neural Networks and Neural Information Processing
ICANN/ICONIP 2003, pp. 333-341. Springer, 2003.
Johann A Gagnon-Bartsch and Terence P Speed. Using control genes to correct for unwanted vari-
ation in microarray data. Biostatistics, 13(3):539-552, 2012.
J. R. Gerstner, J. N. Koberstein, A. J. Watson, N. Zapero, D. Risso, T. P. Speed, M. G. Frank, and
L. Peixoto. Removal of unwanted variation reveals novel patterns of gene expression linked to
sleep homeostasis in murine cortex. BMC Genomics, 17(Suppl 8):727, 10 2016.
Sami Hanhijarvi, Markus Ojala, Niko Vuokko, Kai Puolamaki, Nikolaj Tatti, and Heikki Mannila.
Tell me something I don’t know: randomization strategies for iterative data mining. In KDD, pp.
379-388. ACM, 2009.
Geoffrey E Hinton and Sam T Roweis. Stochastic neighbor embedding. In Advances in neural
information processing systems, pp. 857-864, 2003.
H. Hung. A robust removing unwanted variation-testing procedure via λ-divergence. Biometrics, 75
(2):650-662, 06 2019.
Bo Kang, Jefrey Lijffijt, RaUl Santos-Rodrlguez, and Tijl De Bie. Subjectively Interesting Com-
ponent Analysis: Data Projections that Contrast with Prior Expectations. In Proceedings of the
22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
1615-1624, 2016.
Bo Kang, Dario Garcia-Garcla, Jefrey Lijffijt, Raul Santos-Rodrlguez, and Tijl De Bie. Condi-
tional t-SNE: Complementary t-SNE embeddings through factoring out prior information. CoRR,
abs/1905.10086, 2019. URL http://arxiv.org/abs/1905.10086.
Dmitry Kobak and Philipp Berens. The art of using t-sne for single-cell transcriptomics. Nature
communications, 10(1):1-14, 2019.
Dmitry Kobak and George C Linderman. Umap does not preserve global structure any better than
t-sne when using the same initialization. bioRxiv, 2019.
Teuvo Kohonen. Self-organized formation of topologically correct feature maps. Biological cyber-
netics, 43(1):59-69, 1982.
George Linderman, Manas Rachh, Jeremy Hoskins, Stefan Steinerberger, and Yuval Kluger. Fast
interpolation-based t-SNE for improved visualization of single-cell RNA-seq data. Nature Meth-
ods, 16:1, 03 2019.
10
Under review as a conference paper at ICLR 2021
Leland McInnes, John Healy, and James Melville. Umap: Uniform Manifold Approximation and
Projection for Dimension Reduction. arXiv preprint arXiv:1802.03426, 2018.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number
of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
pp. 722-729. IEEE, 2008.
Karl Pearson. On lines and planes of closest fit to systems of points in space. The London, Edin-
burgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559-572, 1901.
J. Peltonen, H. Aidos, N. Gehlenborg, A. Brazma, and S. Kaski. An information retrieval perspective
on visualization of gene expression data with ontological annotation. In 2010 IEEE International
Conference on Acoustics, Speech and Signal Processing, pp. 2178-2181, 2010. doi: 10.1109/
ICASSP.2010.5495665.
Kai Puolamaki, Emilia Oikarinen, Bo Kang, Jefrey Lijffijt, and Tijl De Bie. Interactive Visual Data
Exploration with Subjective Feedback: An Information-Theoretic Approach. In 2018 IEEE 34th
International Conference on Data Engineering (ICDE), pp. 1208-1211. IEEE, 2018.
D. Risso, J. Ngai, T. P. Speed, and S. Dudoit. Normalization of RNA-seq data using factor analysis
of control genes or samples. Nat Biotechnol, 32(9):896-902, Sep 2014.
Sam T Roweis and Lawrence K Saul. Nonlinear dimensionality reduction by locally linear embed-
ding. science, 290(5500):2323-2326, 2000.
D. Schafflick, C. A. Xu, M. Hartlehnert, M. Cole, A. Schulte-Mecklenbeck, T. Lautwein, J. Wol-
bert, M. Heming, S. G. Meuth, T. Kuhlmann, C. C. Gross, H. Wiendl, N. Yosef, and G. Meyer
Zu Horste. Integrated single cell analysis of blood and cerebrospinal fluid leukocytes in multiple
sclerosis. Nat Commun, 11(1):247, 01 2020.
Vin D Silva and Joshua B Tenenbaum. Global Versus Local Methods in Nonlinear Dimensionality
Reduction. In Advances in neural information processing systems, pp. 721-728, 2003.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A Global Geometric Framework for
Nonlinear Dimensionality Reduction. science, 290(5500):2319-2323, 2000.
Warren S Torgerson. Multidimensional scaling: I. Theory and method. Psychometrika, 17(4):401-
419, 1952.
Laurens van der Maaten. Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine
Learning Research, 15(1):3221-3245, 2014.
Laurens van der Maaten and Geoffrey Hinton. Viualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579-2605, 11 2008.
Takuya Yamano. Some bounds for skewed α-Jensen-Shannon divergence. Results in Applied Math-
ematics, 3:100064, 2019.
A Method Appendix
A. 1 Theoretical results
A.1.1 Working title: The high ground
In the following, we derive the gradient of the conditional cost function
C(Y) = DKL(P || Q)- JSa(P0II Q).
We split the gradient into two parts
δC δDKL (P II Q) δJSβα(P0 II Q)
-----------2-------------:-------.
δyi	δyi	δyi
11
Under review as a conference paper at ICLR 2021
as the first is given by the tSNE gradient with
δDκL(P ||Q)
δyi
4£(Pij- qtj) (1 + kyi-yjk2)-1 3 - yj).
First, we write out the full parametrized Jensen-Shannon Divergence:
JSa(P0 || Q) = αDκL(P0 || βQ + (1 - β)P0) + (1 - α)DκL(Q || βP0 + (1 - β)Q)
=α XPijlog 万~~J pij Q— +(I - α) X qij log R 0 Jqij Q—
i	βqij + (1 -β)pij	i	βp0ij + (1 -β)qij
i,j	i,j
= α	P0ij log P0ij - P0ij log (βqij + (1 - β)Pij)
i,j
+ (1 - α) E qij log qij - qij log (βpj + (1 - β)qij)
The expected similarities P0ij are symmetrized probabilities
2n
with	P0j|i = P
exp(-(DZ)2∕2(σi )2)
k=i exp(-(Dik )2∕2(σi )2)
and the low-dimensional similarities qij are defined as
qij
(1 + kyi-yjk2)-1
Pk=ι(1 + kyk - yιk2)-1,
where we abbreviate the distances by dij = kyi - yj k and the normalization term by Z =	k6=l(1+
(DkYl)2)-1, leading to the succinct notation qij = (1 + (DiYj)2)-1Z-1.
As yi only affects the distances DiYj and DjYi by
δdij
δyi
δyi
Dj
we will derive the gradient with respect to DiYj
δC = Xl
δyi	j ∖
=2X
j
δC δDγ
δC δDγ
δDY yi
δDY δy
δC yi - yj
δDY	DY	.
δ" - yj k _ y - yj
+
Note that the first term of the Jensen-Shannon cost function is constant with respect to DiYj . Thus,
we split the gradient
δJsa(P0 Il Q)
^DY
-α
k6=l
δpkι log (βqki + (1 - β)Pki)
δDY
+(1-α)
k6=l
δqki log qk
δDY
- (1 - α)
k6=l
δqki log(βpkι + (1 - β)qki)
δDY
12
Under review as a conference paper at ICLR 2021
and compute the three parts separately. The building block are the derivatives of Z and qkl with
respect to DiYj :
δZ = X δ(1 + (DY)2)-1
δDY	k=l	δDY
δ(1 + (Dγ )2)-1
δDY
(-1)(1 + (DiYj)2)-22DiYj
-2DiYj(1+(DiYj)2)-2
-2DiYjqijZ(1 + (DiYj)2)-1
A	δ (1+(DY )2)T
δqkl _ δ Z
---= ----------TT---
δDY	δDY
1
Z2
(δ(1 + (DQ2)-1
I	δDγ
Z-(1+(DkYl)2)
Z2 (δ(1+δD∣)2)τ Z - (1 + (DQ2)T (-2DY(1 + (DY)2)-2)
Z2 (δ(1+δDg)2)TZ + (1 + (DQ2)-12DY(1 + (DY)2)-2
1 δ((1 + (DQ2)-1)
Z	δDY
+ (1 + (DY)2)-12Dγ (I +Z2j)
δ((1 + (DY)2)-1) 1
δDY	Z
+ (1 + (DkYl )2 )-1 2DiYj qi2j
(1 + (DiYj)2)-12DiYjqij(qij - 1) ifkl=ij
(1+(DkYl)2)-12DiYjqi2j	ifkl 6= ij.
13
Under review as a conference paper at ICLR 2021
Next we derive the gradients of the three parts:
Σ
k6=l
δp0kl log (βqkl + (1 - β)p0kl)
p0kl
k6=l
δ log (βqkl + (1 - β)p0kl)
X _______Bpkl_________δqkl
白 βqkl + (I - β)p0kl δdij
k6=l
βpij (I + d2j ) 1 2dij qij + βpkl(1 + dkl ) 12dij qij
(βqij + (I - β)pij )	k=l	βqkl + (I - β)pkl
2d..	(— βpijqij(I +	d2j)-1 +	q2,	X	βpkl(I +	dkl) -1	ʌ
ij [	Bqij +(I- β)pij	ij 七l βqkl+ (1 - β)pkl j
βpij qij(I + d2j) 1 I 2 X	βpkl qklZ
βqij + (1- β)pij	qij⅛ βqkl + (1- β)pkl
2dij qij (1 + di2j)-1
—
βpij________, X	βpklqkl
βqij + (I - β)pij	k=l βqkl + (1 - β)pkl
Xδqkl log qkl	δ(log qkl)	δqkl
-YDf- = Zqkl FY- + 可 log qkl
k6=l	ij	k6=l	ij	ij
X1 δqkl	δqkl
_qkl 煮 YDij + YDij log qkl
=X YDYr (1 +log qkl)
k6=l Dij
= -(1 + (DiYj)2)-1 2DiYj qij (1 + log qij) + X(1 + (DkYl)2)-12DiYjqi2j(1 + logqkl)
k6=l
2DYI -(ι+(DY)2) ιqij(1+logqij)+q2j y?(1+(DYy)-I(I+logqkl)
k6=l
2DiYj qij (1 + (DiYj)2)-1	-(1 + log qij) +	qkl(1 + log qkl)
k6=l
14
Under review as a conference paper at ICLR 2021
Σ
k=l
δqk log (βp[l + (1 - β)qki)
δDY
δ log (βpQ + (1 - βIqkI)
qkl
k=l
δDY
Σ
k=l
(1 - β)qkl	δqkl
X δ¾ (βp⅛τ→‰ + log MZ(I- β)qk"
-(1 + (DYB-"DYqij (βpi:+ U)qj + log (βpj + (1 - β)qij))
+X(1+(DQ2)-12dy j βp∖kb+log (βp'kl+(I …
2DY	-(1 + (DY )2)Tqij
+ 2DY qj£(1 + (DQ2)T
k=l
(I - β)qij
βpij + (1 - β}qij
(1 - β)qkl
+ log (βpij + (1 - β)qij)))
βpkl + (1 - β)qkl
+ log (βpkl + (1 - β)qkl)
2DYqij (1 + (DY)2)-1
βppr‰- WPa)
—
+Xqkl (βPk⅛-τ→‰ + log (βpkl + (1 - β)qkl)
Finally we combine them into the derivative of JSa with respect to DY
(1 + (DY)2) δJSα(P0∣∣ Q) = - (_ βpij + X	βPklqkl	∖
2DYqij	δDY	[ βqij + (1 - β)pij	gl βqkl + (1 - β)pkl J
+ (1 - α)	-(1 + log qij) + EqkI(I+log qkl)
∖	k≠l
-	(1 - α) (-βPij¾Γ⅛i" - log (βpij+ (1 - β)qij))
-	(1 - α) Xqkl (βp⅛τ→‰ + log (βpkl + (1 - β)qk")
αβpij______X	αβPkl qkl
βqij + (I - β)pij	k=l βqkl + (1 - β)pkl
+	(1 - °)( - (1 + log qj)+ βpj(IHβ-β)qj + log (βpj + (1 - β)qij)
+	(1 - °) Xqkl (Iiqkl - βPk7W→‰
-log (βpkl + (1 - β)qkl )
15
Under review as a conference paper at ICLR 2021
and substitute this result in the gradient with respect to yi
δJsα(P0IIQ) _ δy	2X δJSα (P0 || Q) (yi-yj) =j -δDY	Dr =4 X(yi - yj)(1 + (DY)2)Tq∕B	Mq- - X B	弋'kq3 0 M	j	\的切+ (1 - β)Pij	M βqk+ (1 - β)Pki + (1 — α) ( 一 (1 + log qij) + ep0, (1 -β)q^. + log (βpij + (1 — β)qij)) +(1- α) X qkι (1+log qkl- βPk(¾τ-⅛- log (βpkι+(1- β)qkl)!!.
A.1.2 Bounding pJSD
Here, we will provide the proof for the upper bound of the pJSD following the idea of Yamano
(2019).
Theorem (Upper bound on parametrized JS divergence) For 0 ≤ α ≤ 1 and 0 < β < 1 the
parametrized JS divergence is bounded from above by
JSβα ≤ - log(1 - β).
Proof. Let us first define a known bound for general f-divergences Yamano (2019).
Lemma 4. Let f *(x) be the conjugate function of f (x) defined as f *(x) = Xf (1). Thef-
divergence satisfies the upper bound
Df(P|| Q) ≤ lim (f(χ) + f*(χ)).
x→0
We will use the definition of our parametrized Jensen Shannon Divergence
JSβα = αKL(P || βQ+(1 - β)P) +(1 - α)KL(Q || P+ (1 - β)Q)
to bound the individual terms, which are essentially skewed KL divergences, to then obtain the upper
bound for pJSD. The skewed β-KL divergence is given by
KLβ(P || Q) =	p(x) log
x∈X
p(x)
(1 - β)p(χ) + βq(χ).
Choosing fi(x) = X log(-&+◎, We can see that the skewed β-KL divergence belongs to the
family of f-divergences. Similarly, choosing f2(x) = -β log βx+ 1 -β , we get the reverse β-KL
divergence KLβ (Q || P). We thus get
JSβα =αKLβ(P || Q)+(1 -α)KLβ(Q ||P)
≤ α Ximo (fι(x) + f1(x)) + (1- α) Xino (f2(x) + f2(x))
X
α lim x log -----------
χ→0 y	(1 — β)x + β
+ log1-β1+ Xe )
+ (1 — α) lim ( — log (βx + 1 — β) — x log (— + 1 — β)
=α log -ɪ- + (1 — α) — log(1 — β)
1 — β
— log(1 — β),
16
Under review as a conference paper at ICLR 2021
where the first inequality is achieved by bounding the individual skewed β-KL divergences using
Lemma 4, the following equality is obtained by plugging in the definitions of the corresponding
f-divergences f1 and f2 with their respective conjugate, and the remainder is obtained by plugging
in X = 0 and using basic math. This completes the proof.	□
A.1.3 Confetti maintains metric properties
Theorem 5.	Assuming that DX and DZ are based on valid metrics, for any λ > 0, (DX ㊀λ DZ)
fulfills the metric axioms of non-negativity, symmetry, identity, and the triangle inequality.
Proof. We will prove that
fλ(i,j)
-2λDZ + λ
0
i 6= j,
i = j,
fulfills the metric axioms and use the fact that the sum of two metrics is a metric.
1.	Non-negativity: ∀i, j.fλ(i, j) ≥ 0.
Using the definition of fλ(i, j), We get λ ≥ 1 λDZ. Since DZ ∈ (0,1), We know that the
inequality always holds.
2.	Symmetry: ∀i,j.fλ(i,j) = fλ(j,i).
FolloWs directly from the symmetry property of DZ .
3.	Identity: ∀i,j.fλ(i,j)=0 o i = j.
We know that for i= j, fλ(i,j) ∈ [— 2λDZaχ + λ, -1 λDZin + λ]. Using DZ ∈ (0,1),
we get fλ(i,j) ∈ [ 1 λ, λ]. The other direction follows directly from the definition.
4.	Triangle Inequality: ∀i, j, k.fλ(i, j) ≤ fλ(i, k) + fλ(k, j).
Using the insight fλ(i,j) ∈ [ 1 λ, λ] from before, we get fλ(i,j) ≤ maxχ,y(fλ(x, y))=
λ = 2 ∙ 2λ = 2 ∙ minχ,y(fλ(x,y)) ≤ fλ(i,k) + fλ(k,j).
Thus, (DX ㊀λ DZ) implements a metric.	□
A.1.4 Confetti is robust against uninformative priors
Theorem 6.	Assume the prior is uninformative, that is DZ ⊥ DX. Furthermore, the distances are
normalized to DZ ∈ [0,1], which is ensured by our method. ForfiXed λ > 0 let (Fλ )j = DX ㊀λ DZ
be the high dimensional distances with factored out prior. On expectation, the neighbourhoods of
X and X with factored out prior are the same, that is NkFλ (i) =E[.] NkDX (i), for any i and k.
Proof. Let us first look at what on expectation the distance of some nearest point of i is under
Fλ. We now pick the j ∈ NkFλ (i) with the largest distance. Note that the distances D define a
fixed ordering and thus for each j there is a k0 such that j has the largest distance in NkF0λ (i). We
prove by contradiction. Let j 6∈ NkDX (i), hence there are k indices given by L = NkDX (i) with
∀l ∈ L. DiXl < DiXj . Let PX and PZ be the generating distributions of DX and DZ, respectively.
We get
DiXl < DiXj
DX - 2 λEPz [DZ ]+ λ < DX - 2 λEPz [DZl ] + λ
EPz [DX - 2λDZ + λ] < EPz[DX - 2λDZ + λ]
EPZ [(Fλ)il] < EPZ [(Fλ)ij],
where line 1 to 2 is obtained using that DX ⊥ DZ and EPZ [DiZl] ∈ [0, 1], and line 2 to 3 is obtained
by using linearity of expectation and DX ⊥ DZ. In other words, for all l on expectation the adapted
distance to i, (Fλ)il, is smaller than the adapted distance between i and j, (Fλ)ij. This means that
L contains k indices that have smaller distances under Fλ than j, hence j 6∈ NkFλ (i). This is a
contradiction.	□
17
Under review as a conference paper at ICLR 2021
A.1.5 Confetti pseudocode
Algorithm 1: CONFETTI
Data: Data distances DX, prior distances DZ, penalty λ
Result: Adjusted distances DX ㊀λ DZ
DX
max DX
DZ
max DZ
DX —
DZ —
1
2
3
4
5
6
foreach i ∈ [1, n] do
foreach j ∈ [i + 1, n] do
(DX ㊀λDZ)ij — DX - 2λDZ +λ
_	(DX ㊀λ DZ)ji — (DX ㊀λ DZ)ij
7 return (DX ㊀λ DZ)
B Experiment Appendix
B.1	Inverse supervised LLE (sLLE-1)
In SLLE (De Ridder et al., 2003), class labels given as additional input are used to emphasize the
structure within the classes. Thus, SLLE improves the separation between classes by homogenizing
the local neighbourhood of each sample with respect to the class labels. This is done by pushing
away data points with different class labels, increasing the distance of such points by a fraction of the
maximum distance observed in the data. This can be summarized by the following update procedure
for the sample distances, using the same notation as in the main paper
D0 = DX + α max(DX)D, 0 ≤ α ≤ 1,
With D the binary
，♦	♦ .1	7^∖
matrix With Dij
1 if samples xi and xj are from different classes.
In our Work, We are interested in factoring out the prior, Which is essentially the opposite of SLLE.
By manipulating the objective from above, We can hoWever achieve our goal of factoring our prior
knoWledge, by optimizing for
D0 = DX + αmax(DX)(1 - D), 0 ≤ α ≤ 1.
We call this method sLLE-1. This method can still only deal With labels as input, but for that it
Works surprisingly Well in our experiments, yielding better results than the current state of the art
ctSNE.
B.2	Choosing the hyperparameters
All methods involved in the experiment section come With a set of hyperparameters that the
user has to fix. We decided to generate a small synthetic data set, generated differently than
the one used for our experiments, to settle for a good set of parameters for each of the
methods. In particular, We explore for CONFETTI coupled With tSNE the parameter λ ∈
{0.2, 0.4, 0.5, 0.6, 0.8, 1, 1.2, 1.4, 1.5, 1.6, 1.8, 2, 2.5, 3, 4} and perplexity 30, 50, 200, 400, for JEDI
We search for α ∈ {0, 0.2, 0.5, 0.8, 1}, β ∈ {0.8, 0.99, 1}, and perplexity and prior per-
plexity 50, 200, 400 each. For ctSNE We search for β ∈ {1e-2, 1e-3, ..., 1e-7}, and for
SLLE-1 the number of neighbours k ∈ {10, 30, 50, 100, 200, 400, 500, 600} and parameter α ∈
{0.0001, 0.001, 0.1, 0.2, ..., 1}. We note that the authors of ctSNE suggest to use a small value
β = 0.01, but use β = 0.0001 for their experiment, Which led us to the search grid for their method.
We generated a simple 10-dimensional dataset With 1k data points, With samples assigned to one of
four clusters in the first 4 dimensions, and one of tWo clusters in dimension 5 and 6. The clusters
in the first 4 dimensions are centered at the four unit vectors along the four axes of the dimensions.
The clusters in the other dimensions are centered at (3), (0). The remaining four dimensions are
gaussian noise N(0, 1). Each data point is randomly assigned to one cluster of the first four, and
18
Under review as a conference paper at ICLR 2021
Figure 4: Visualization of the synthetic data set for parameter tuning based on tSNE. We see samples
with the same colors clustered together (clustering planted in dimensions A) as well as samples with
the same shape (clustering planted in dimensions B).
19
Under review as a conference paper at ICLR 2021
(a)	Euclidean distances to all other samples computed
over all dimensions.
(b)	Euclidean distances to all other samples computed
only over dimensions d1 to d4. We observe the clus-
tering planted into those dimensions.
(c)	Hierarchical clustering with average linkage
based on dimensions d1 to d4. The four different
clusters are easily identifiable.
(d)	Scaled dendrogram distances from the pink refer-
ence sample to all other samples. It is defined as the
height of their lowest common ancestor node in the
dendrogram given in (c).
Figure 5: Visualization of different distances for one sample from the pink cluster to all other sam-
ples in the synthetic data set for parameter tuning.
20
Under review as a conference paper at ICLR 2021
(d) β = 0.8, α
(e) β = 0.99, α
(f) β = 1, α = 1
Figure 6: Visualizations of the synthetic data set for parameter tuning by Jedi with a Euclidean
distance prior based on the dimensions in A (color clusters). Here we compare the influence of the
smoothing parameters α and β .
(a)β =0.8,α =0
(b)β = 0.99, α = 0
(c) β = 1, α = 0
one cluster in dimension 5 and 6, and takes the coordinates of the cluster centers plus some gaussian
noise N(0, 0.01). A tSNE visualization is given in App. Fig. 4. For each method, we provide
embeddings for a subset of their search grid that gives a good idea how the embedding changes
when varying the parameters in Figures 5, 6, 7, 8, 9, 10, and 11.
Looking at the results, we observe that for CONFETTI, as expected, the gradual increase in λ is
directly reflected in the embedding with how strongly differently colored points merge. Here, we
settle for a perplexity of 50 for the prior and λ = 2, as this allows to take into account much
information of the prior while yielding nicely separated and mixed colored clusters.
For Jedi, we have to investigate a good setting of 4 different parameters in a principled manner.
Fixing the perplexities, we observe that α = 0 and β = 0.99 yield much better mixtures of the
similarly colored points, than any other combination, with JEDI not converging with β = 1 (App.
Fig. 6). Yamano (2019) already pointed out that β = 0.99 worked well as a choice for the divergence
mixing. Fixing β = 0.99 we proceed to analyse the impact of α and the perplexity parameters,
noting that the latter is highly dependent on the data set size, and should be adjusted by the user as
With original tSNE. Here, We settle for α = 0, perplexity of 1 n and prior perplexity 击 n, where n
is the number of samples.
For ctSNE, β = 1e-5 Worked best, Which is also close to What the authors use for their original
experiments. In the case of SLLE-1, We observe that for α ≥ 0.3, independent of k, similarly
colored points are not clustered together anymore. Furthermore, only With k ≥ 200 the tWo desired
clusters are clearly visible. We thus propose to use α = 0.5 and k = 1 n as default parameters,
Where k should be carefully adjusted depending on the data set at hand.
B.3 Synthetic data
The data has 14 dimensions in total, Where each sample belongs to one of 4 clusters (A1-A4) in
dimension 1-8 and one of four clusters in dimension 9-12 (B1-B4). We first draW cluster centers
21
Under review as a conference paper at ICLR 2021
(a) prior perplexity = 50, α = 0
(b) prior perplexity = 200, α =
0
(c) prior perplexity =
0
400, α =
(d) prior perplexity = 50, α = 1
(e)	prior perplexity = 200, α =
1
(f)	prior perplexity = 400, α =
1
Figure 7:	Visualizations of the synthetic data set for parameter tuning by Jedi with a Euclidean
distance prior based on the dimensions in A (color clusters). We compare the effect of an increasing
prior perplexity on both divergences determined by α.
22
Under review as a conference paper at ICLR 2021
(a) perplexity = 50, α = 0
(d) perplexity = 50, α = 0.5
(b) perplexity = 200, α = 0
(e) perplexity = 200, α = 0.5
(c) perplexity = 400, α = 0
(g) perplexity = 50, α = 1
(h) perplexity = 200, α = 1
(f) perplexity = 400, α = 0.5
(i) perplexity = 400, α = 1

Figure 8:	Visualizations of the synthetic data set for parameter tuning by Jedi with a Euclidean dis-
tance prior based on the dimensions in A (color clusters). We see the effect of different perplexities
in combination with different values for the mixture of divergences α.
23
Under review as a conference paper at ICLR 2021
(a) perplexity = 50, λ = 0.4
(b) perplexity = 200, λ = 0.4
(c) perplexity = 50, λ = 0.6
(d) perplexity = 200, λ = 0.6
(f) perplexity = 200, λ = 1
(e) perplexity = 50, λ = 1
(g) perplexity = 50, λ = 2
Figure 9: Visualizations of the synthetic data set for parameter tuning by Confetti with a Eu-
clidean distance prior based on the dimensions in A (color clusters). For two different perplexity
values we increase the maximum penalty λ from 0.4 to 2.
(h) perplexity = 200, λ = 2
24
Under review as a conference paper at ICLR 2021
(b) α = 0.5, k = 50
(a) α = 0.1, k = 50
(c) α = 1, k = 50
(d)α = 0.1, k = 200
(e)α = 0.5, k = 200
(f)α = 1,k = 200
(g) α = 0.1, k = 500
Figure 10: Visualizations by sLLE-1 with labels prior based on the clusters in A (color). The
amount how much the distances are adjusted is determined by α and the number of neighbors to
consider is k.
(h) α = 0.5, k = 500
(i) α = 1, k = 500
(a)β = 1e-3	(b)β = 1e-5	(c)β = 1e-7
Figure 11: Visualizations by ctSNE with a label prior based on the clusters in A (color). We try
different values for β which influences how strong the prior is factored out.
25
Under review as a conference paper at ICLR 2021
from N(0, 2) for each of the eight clusters. Then, the feature values for each sample are generated
in three steps as follows:
1.	Pick a cluster a from A1-A4 with probability 0.1, 0.2, 0.3, or 0.4, respectively. Add Gaus-
sian noise to the cluster center a with standard deviation 0.1, 0.2, 0.3, or 0.4, respectively.
Noise is drawn and added for each dimension independently.
2.	Pick a cluster a from B1-B4 with probability 0.1, 0.2, 0.3, or 0.4, respectively. Add Gaus-
sian noise to the cluster center a with standard deviation 0.1, 0.2, 0.3, or 0.4, respectively.
Noise is drawn and added for each dimension independently.
3.	The remaining 2 dimensions of every sample are Gaussian noise from N(0, 1).
A tSNE visualization of the data is given in App. Fig. 12. For this dataset, we take the euclidean
distances between all samples in the first 8 dimensions as prior for our methods, and the ground truth
label assignment A1-A4 as prior for the other methods. Here, we additionally provide the embed-
dings of Confetti and Jedi with label assignment prior, given in App. Fig. 13, and the NOS plots
for both the input data and the prior in App. Fig. 14. While the NOS plots and corresponding area
between curves give insight in how well an embedding factors out or maintains certain information,
it is hard to optimize. In particular, it unclear how to combine and balance the two scores for how
much prior is removed and how much information of the input is maintained. Even if these issues
are resolved, the main problem will be that the kNNs that define this score are essentially describing
binary relationships between entities, thus optimizing for optimal neighbourhoods does not give any
clue how to arrange the samples in a 2D continuous space.
26
Under review as a conference paper at ICLR 2021
Figure 12: tSNE of synthetic data. Visualized is a tSNE plot for the synthetic data set using perplex-
ity 50. Color corresponds to cluster assignment in dimension 1-8 and shape corresponds to cluster
assignment in dimension 9-12.
27
Under review as a conference paper at ICLR 2021
(a) CONFETTI λ = 1.
(b) JEDI α = 0, β = .99.
Figure 13: JEDI and CONFETTI with label prior. Visualized are the plots generated by our methods
when fed with the ground truth labels corresponding to the cluster assignment in the first 8 dimen-
sions of the synthetic data set. Color corresponds to cluster assignment in dimension 1-8 and shape
corresponds to cluster assignment in dimension 9-12.
(a) NOS plot between prior labels and embedding
(smaller is better).
(b) NOS plot between non-prior dimensions and em-
bedding (larger is better).
Figure 14: NOS score visualizations. Given are the NOS plots for prior labels and embedding (a)
and non-prior dimensions 9-12 and embedding (b) on the synthetic data set. On the x-axis, the
neighbourhood k is varied between 0% and 100% of data points, the y-axis is the NOS score.
28
Under review as a conference paper at ICLR 2021
B.4	Flower data
% £ dpμφ>o pooψoq⅛φu
Figure 15: NOS plot for flowers. Visualized is the NOS score for the Oxford flower data set for
JEDI and CONFETTI against the prior. On the x-axis, the neighbourhood k is varied between 0%
and 100% of data points, the y-axis is the NOS score.
B.5	Single cell data
Here, we provide additional plots for the single cell experiments. In particular, we carried out an
agglomerative clustering based on marker gene expression, which arrives at the same number of
clusters as the original paper (App. Fig. 16b) Additionally, we show how the ctSNE embedding
looks when coloring the visualization according to cell type of the original paper (App. Fig. 16a),
and a new result when we use tissue information as prior knowledge for Confetti (App. Fig. 17a),
as well as the vanilla UMAP plot with coloring according to batch ID.
29
Under review as a conference paper at ICLR 2021
(a) ctSNE embedding on single cell data with color
accordint to cell labels.
Figure 16: Embedding of ctSNE and clustering according to marker genes. a) Visualized is the
embedding obtained from ctSNE for the single cell data with our cluster labels as prior. The coloring
is according to cell type. b) Dendogram of agglomerative clustering with complete linkage on
marker gene expression. A natural cutting point is at 1.8 (blue horizontal line), which retrieves the
same number of clusters as the orginal paper.
(b) Dendogram for complete linkage clustering of
samples according to marker gene expression.
(a) Embedding of Confetti with
tissue prior.
(b) Embedding of Jedi with marker
gene prior.
Figure 17: Additional SC embeddings. Visualized is the embedding of CONFETTI for the single
cell data with the tissue type (blood vs CSF) as prior with coloring according to cell labels (a),
the embedding obtained from Jedi for the single cell data with marker gene expression as prior,
with coloring is according to the cell labels (b), and the original UMAP embedding with coloring
according to batch ID (c).
(c) Embedding of vanilla UMAP
with batch ID coloring.
30