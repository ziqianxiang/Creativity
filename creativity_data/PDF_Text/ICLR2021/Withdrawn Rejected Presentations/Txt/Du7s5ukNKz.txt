Under review as a conference paper at ICLR 2021
Policy Learning Using Weak Supervision
Anonymous authors
Paper under double-blind review
Abstract
Most existing policy learning solutions require the learning agents to receive
high-quality supervision signals, e.g., rewards in reinforcement learning (RL) or
high-quality expert’s demonstrations in behavioral cloning (BC). These quality
supervisions are either infeasible or prohibitively expensive to obtain in practice.
We aim for a unified framework that leverages the weak supervisions to perform
policy learning efficiently. To handle this problem, we treat the “weak supervisions”
as imperfect information coming from a peer agent, and evaluate the learning
agent’s policy based on a “correlated agreement” with the peer agent’s policy
(instead of simple agreements). Our way of leveraging peer agent’s information
offers us a family of solutions that learn effectively from weak supervisions with
theoretical guarantees. Extensive evaluations on tasks including RL with noisy
reward, BC with weak demonstrations and standard policy co-training (RL + BC)
show that the proposed approach leads to substantial improvements, especially
when the complexity or the noise of the learning environments grows.
1	Introduction
Recent breakthrough in policy learning (PL) opens up the possibility to apply these techniques in real-
world applications such as robotics (Mnih et al., 2015; Akkaya et al., 2019) and self-driving (Bojarski
et al., 2016a; Codevilla et al., 2018). Nonetheless, most existing works require agents to receive
high-quality supervision signals, e.g., reward or expert’s demonstrations, which are either infeasible
or prohibitively expensive to obtain in practice. For instance, (1) the reward may be collected through
sensors thus not credible (Everitt et al., 2017; Romoff et al., 2018; Wang et al., 2020); (2) the
demonstrations by an expert in behavioral cloning (BC) are often imperfect due to limited resources
and environment noise (Laskey et al., 2017; Wu et al., 2019; Reddy et al., 2020).
Learning from weak supervision signals such as noisy rewards r (noisy versions of r) (Wang et al.,
2020) or low-quality demonstrations DE (noisy versions of DE) produced by problematic expert
∏e (WU et al., 2019) is one of the outstanding challenges that prevents a wider application of PL.
Although some recent works have explored these topics separately in their specific domains (Guo
et al., 2019; Wang et al., 2020; Lee et al., 2020), there is no unified solution towards performing
robust policy learning pip install arxiv-latex-cleaner under this imperfect supervision. In this work,
we first formulate a meta-framework to study RL/BC with weak supervision signals and call it
weakly supervised policy learning. Then as a response, we propose a theoretically principled solution
concept, PeerPL, to perform efficient policy learning using the available weak supervisions.
Our solution concept is inspired by the literature of peer prediction (Miller et al., 2005; Dasgupta
& Ghosh, 2013; Shnayder et al., 2016), where the question concerns verifying information without
ground truth verification. Instead, a group of agents’ reports (none of which is assumed to be high-
quality nor clean) are used to validate each other’s information. We adopt a similar idea and treat the
“weak supervisions” as information coming from a peer agent, and evaluate the learning agent’s policy
based on a “correlated agreement” (CA) with the peer agent’s. Compared to standard reward/loss
functions that impose simple agreements with the weak supervisions, our approach punishes an
over-agreement to avoid overfitting to the weak supervisions. Our way of leveraging peer agent’s
information offers us a family of solutions that 1) does not require prior knowledge of the weakness
of the supervisions, and 2) learns effectively with strong theoretical guarantees.
We demonstrate how the proposed PeerPL framework adapts in challenging tasks including RL with
noisy rewards and behavioral cloning (BC) from weak demonstrations. Furthermore, we provide
1
Under review as a conference paper at ICLR 2021
intensive analysis of the convergence behavior and the sample complexity for our solutions. These
results jointly demonstrate that our approach enables agents to learn the optimal policy efficiently
under weak supervisions. Evaluations on these tasks show strong evidence that PeerPL brings
significant improvements over state-of-the-art solutions, especially when the complexity or the noise
of the learning environments grows.
To summarize, the contributions in the paper are mainly three-folds: (1) We provide a unified
formulation of weakly supervised policy learning to model the weak supervision in RL/BC problems;
(2) We propose a novel PeerPL solution framework based on calculating a correlated agreement
with weak supervisions, a novel way for policy evaluation introduced to RL/BC tasks; (3) PeerPL is
theoretically guaranteed to recover the optimal policy (as if the supervisions are of high-quality and
clean) and competitive empirical performances are observed in several policy learning tasks.
2	Related Work
Learning with Noisy Supervision Learning from noisy labels is widely explored within the
supervised learning domain. Beginning from the seminal work (Natarajan et al., 2013) that first
proposed an unbiased surrogate loss function to recover the true loss given the knowledge of noise
rates, follow-up works focus on how to estimate the noise rates based on noisy observations (Scott
et al., 2013; Scott, 2015; Sukhbaatar & Fergus, 2014; van Rooyen & Williamson, 2015; Menon et al.,
2015). Recent work (Wang et al., 2020) adapts this idea within RL and proposes a statistics-based
estimation algorithm. However, the estimation is not efficient especially when the state-action space
is huge. Moreover, as a sequential process, the error in estimating the noise rate can accumulate and
amplify when deploying an RL algorithm. In contrast, our solution in this paper does not require a
priori specification of the noise rates thus offloading the burden of estimation.
Behavioral Cloning (BC) Standard BC (Pomerleau, 1991; Ross & Bagnell, 2010) tackles the
sequential decision-making problem by imitating the expert’s actions using supervised learning.
Specifically, it aims to minimize the one-step deviation error over the expert’s trajectory without
reasoning the sequential consequences of actions. Therefore, the agent suffers from compounding
errors when there is a mismatch between demonstrations and real states encountered (Ross & Bagnell,
2010; Ross et al., 2011). Recent works introduce data augmentations (Bojarski et al., 2016b) and value-
based regularization (Reddy et al., 2019) or inverse dynamics models (Torabi et al., 2018; Monteiro
et al., 2020) to encourage learning long-horizon behaviors. While simple and straightforward, BC has
been widely investigated in a wide range of domains (Giusti et al., 2016; Justesen & Risi, 2017) and
often yields competitive performance (Farag & Saleh, 2018; Reddy et al., 2019). Our framework is
complementary to current BC literature by introducing a learning strategy from weak demonstrations
(e.g., noisy or from a poorly-trained agent) and provides theoretical guarantees on how to retrieve
clean policy under mild assumptions (Song et al., 2019).
Correlated Agreement Peer prediction aims to elicit information from self-interested agents
without ground-truth verification (Miller et al., 2005; Dasgupta & Ghosh, 2013; Shnayder et al.,
2016). The only source of information to serve as verification is from the agents’ reports. Particularly,
in Dasgupta & Ghosh (2013); Shnayder et al. (2016), a correlated agreement (CA) type of mechanism
is proposed, which evaluates the correlations between agents’ reports. In addition to encouraging some
agreement between agents, CA mechanism also punishes over-agreement when two agents always
report identically. This property helps reduce the effect of noisy reports by punishing overfitting to
them. Recently, Liu & Guo (2020) adapts a similar idea in learning from noisy labels for supervised
learning. We consider a more challenging weakly supervised policy learning setting and study the
convergence rates in sequential decision-making problems.
3	Policy Learning from Weak Supervision
We begin by introducing a general framework to unify PL with low-quality supervision signals.
Then we provide instantiations of the proposed weakly supervised formulation with two differ-
ent applications: (1) RL with noisy reward and (2) behavioral cloning (BC) using weak expert
demonstrations.
2
Under review as a conference paper at ICLR 2021
3.1	Preliminary of Policy Learning
The goal of policy learning (PL) is to learn a policy π that the agent could follow to perform a series
of actions in a stateful environment. For RL, the interactive environment is characterized as an MDP
M = hS, A, R, P,γi. At each time t, the agent in state st ∈ S takes an action at ∈Aby following
the policy π : S×A→ R, and potentially receives a reward r(st, at) ∈R. Then the agent transfers to
the next state st+1 according to a transition probability function P . We denote the generated trajectory
τ = {(st,at,rt)}tT=0, where T is a finite or infinite horizon. RL algorithms aim to maximize the
expected reward over the trajectory T induced by the policy: J (∏) = E(st,at,rj 〜τ[PT=o Y trt], where
Y ∈ (0,1] is the discount factor.
Another popular policy learning method is through behavioral cloning (BC). The goal of BC is to
mimic the expert policy πE through demonstrations DE = {(si,ai)}iN=1 drawn from distribution
DE (generated according to πE), where (si, ai) is the sampled state-action pair from the expert’s
trajectory. Typically, training a policy with standard BC corresponds to maximizing the following
log-likelihood: J(π)=旧斐⑷〜DE [log∏(a∣s)].
In both RL and BC, the agent receives “supervisions” through either the reward r by interacting with
environments or the expert policy πE as observable demonstrations. Consider a particular policy class
Π, the optimal policy is then defined as π* = argmax∏∈∏ J(π): π* obtains maximum expected
reward over the horizon T in RL and ∏* corresponds to the clean expert policy ∏e in BC. In practice,
one can also leverage both RL and BC approaches to take advantage of both worlds (Brys et al.,
2015; Hester et al., 2018; Guo et al., 2019; Song et al., 2019). Specifically, a recent hybrid framework
called policy co-training (Song et al., 2019) is considered in this paper.
3.2	Meta Framework for Policy Learning with Weak Supervision
To unify, We denote these weak supervision signals using Y that is either the reward r for RL or
the action α performed by an expert policy ∏e for BC. Y denotes a weak version of a high-quality
supervision signal Y. As a consequence, in an abstract manner, a weakly supervised PL problem can
be formulated as learning the optimal policy ∏ with only accessing a weak supervision sequence
denoting as {(si, αi), Yi}iN=1.
To unify the discussion, suppose that we have an evaluation function Evaπ((si,αi), Yi) that evaluates
a taken policy at state (si,αi) with a weak supervision Yi. In the RL setting, this Evaπ is the loss for
different RL algorithms, which is a function of noisy reward r received at (si,a). While for the BC
setting, this Evaπ is the loss used to evaluate the action taken by the agent given the action taken by
the expert. Furthermore, we let J (π) denote the function that evaluates policy π under a set of state
action pairs With weak supervision signals {(si,电),匕}N=ι, i.e., J(∏) = E(s,。)〜"Eva∏((s, a),Y)].
Note that above unified notations are only for better delivery of our framework and we still treat
PL as a sequential decision problem. We focus on the following two instantiations for our weakly
supervised settings:
RL with Noisy Reward Consider a finite MDP Mf = hS, A, R,F,P,Yi with noisy reward
channels (Wang et al., 2020), where R : S X A → R, and the noisy reward r is generated following
a certain function F : R→R. Denote the trajectory a policy πθ generates via interacting with M as
Tθ. Assume the reward is discrete and has ∣R∣ levels, the noise rate can be characterized via a matrix
CRRI × ∣r∣ , where each entry j indicates the flipping probability for generating a perturbed outcome:
CRL = P (rt = Rk ∣rt = Rj). We call r and r the true reward and noisy reward respectively.
BC with Weak Demonstration Instead of observing the expert demonstration generated according
to ∏e, denote the available weak demonstrations by {(si,<⅛)}N=ι, where (⅛ 〜∏e(∙∣si) is the noisy
action and each state-action pair (si,* is drawn from distribution DE. In particular, we assume
the noisy action ai is independent of the state S given the deterministic expert action ∏e(s), i.e.,
P(αi∣∏E(Si)) = P()⅛∣Si, ∏e(si)). Similar to RL, we assume the noise regime can be characterized
by a confusion matrix CBAC× ∣∕∣, where each entry cj,k indicates the flipping probability for the expert
to take an suboptimal action CBC = P(∏e(Sk) = Ak ∣∏e(Sj) = Aj). In this setting, we,d like to
recover ∏* as if we were able to access the quality expert demonstration ∏e instead of ∏e .
3
Under review as a conference paper at ICLR 2021
+
BC with Weak Expert
RL with PertUrbed ReWard
Policy Co-Training
Behavior Cloning
Peer Policy Learning
............................••
Reinforcement Learning
Weak
Demonstrations
(",%), % as (
Agent： {…,(#!，$!),…，G，$")卜.,(##,$#),…}
PeerAgent: {…， 4 ,…， 耳，…，闻,…}
CA:	Eva (("!，%!),%)- Eva (("", %。(#)
.as (
Noisy
Environment
(S⑷
%
Figure 1: Illustration of weakly supervised policy learning and our PeerPL solution with correlated
agreement (CA). We use Y to denote a weak supervision, be It a noisy reward, or a noisy demonstra-
tion. Eva stands for an evaluation function. “Peer Agent” corresponds to weak supervisions.
Remark We emphasize that we do not have nor need the knowledge of the weakness of the signals,
i.e., CRRJI × ∣r∣ nor C∕× 闻.As we show later, our approach successfully avoided the need of this
knowledge. This is also a main challenge for designing our solution.
4 PeerPL: Weakly Supervised PL via Correlated Agreement
To deal with weak supervisions in PL, we propose a unified and theoretically principled framework
PeerPL. We treat the weak supervisions as information coming from a “peer agent”, and then
evaluates the policy using a certain type of “correlated agreement” function between the learning
policy and the peer agent.
4.1 Overview of the Idea: Correlated Agreement with Weak Supervisions
We first present the general idea of our PeerPL framework that uses a concept named “correlated
agreement” (CA). For each weakly supervised state-action pair ((si, ai), Yi), we randomly sample a
state-action pair (sj ,aj ),j 6= i, as well as another supervision signal Yk ,k 6= i, j from a different
state-action pair. Then we evaluate ((si,ai), Yi) according to the following:
CA with Weak Supervision: Evan (⑶，。。，匕)一Evan((Sj, aj), Yk)	(1)
Intuitively, the first term above encourages an “agreement” with the weak supervision (that a policy
agrees with the corresponding supervision), while the second term punishes a “blind” and “over”
agreement that happens when the agent’s policy always matches with the weak supervision even on
randomly paired traces (noise). The randomly paired samples j, k helps us achieve this check. Note
the implementation of our mechanism does not require the knowledge of CRRI ×∣r∣ nor CBAC×⑷,and
offers a prior-knowledge free way to learn effectively with weak supervsions. This above process is
illustrated in Figure 1.
Illustrative toy example Consider a toy BC setting where we learned a policy that outputs a
sequence of actions a1 = a2 = a3 =1,a4 =0, and they have perfectly matched the weak supervision
a01 = a02 = a03 =1,a04 =0(atthe same sequence of states). Supposedly Evan((si,ai),a0i) evaluates
how well a policy matches the expert demonstration (Evan =1for agreeing, 0 for not). Using
only Evan((si,ai),a0i) will return the highest score 1 for agreeing with this noisy/imperfect/low-
quality supervision. However the correlated agreement evaluation returns (for example for i =
1) E[Evan((si,ai),a0i) - Evan((sj,aj),a0k,k 6= j)] = 1 - (0.752 +0.252)=0.375, where
0.752 + 0.252 is the probability of randomly paired a and a0 match each other. The above example
shows that a full agreement with the weak supervision will instead be punished!
In what follows, we solidate our implementations within each of the settings considered and provide
theoretical guarantees under weak supervision.
4.2 PeerRL: Peer Reinforcement Learning
Since the reward signals are no longer credible in the weakly supervised RL setting, we propose the
following objective function that punishes the over-agreement based on CA mechanism
JR* L(∏θ) = EhEVaRL((Si,ai),ri)i - ξ ∙ EhEVaRL((Sj,aj),rfc)i，	(2)
where EVaRL((S,a),r) = —'(∏θ, (s, a, r)).	(3)
4
Under review as a conference paper at ICLR 2021
In (2),the first expectation is taken over (si, ai, ri)〜Tθ and second one is taken over (sj, aj, rj)〜
Tθ, (sk, ak, rk)〜Tθ, where Tθ is the trajectory generated by ∏θ with the noisy reward function
r. The choice of the loss function ' depends on the RL algorithms used (e.g., temporal difference
error (Mnih et al., 2013; Wang et al., 2016) or the policy gradient loss (Sutton et al., 1999)). Also,
the learning sequence is encoded in π, therefore, maximizing the objective J RL(π) is equivalent to
maximizing the accumulated reward. ξ ≥ 0 is a hyperparameter to balance the penalty for blind
agreements induced by CA.
In what follows, we consider the Q-Learning (Watkins & Dayan, 1992) as the underlying learning al-
gorithm where '(∏θ, (s, a, r)) = -r and demonstrate our CA mechanism provides strong guarantees
for Q-Learning with only observing the noisy reward. For clarity, we define peer reward as
Peer RL Reward: rpeer(s, a) = r(s, a) - ξ ∙ r0,
where r0 is a randomly sampled reward over all state-action pair and ξ ≥ 0 is a parameter to balance
the noisy reward and the punishment for blind agreement (with r0). We set ξ = 1 (for binary case) in
the following analysis and treat each (s, a) equally when sampling the r0. In our experiment, "er is
not sensitive to the choice of ξ, and we have kept ξ a constant for each run of the RL experiments.
We show peer reward rpeer offers us an affine transformation of the true reward in expectation, which
is the key to guaranteeing the convergence of our Peer RL algorithm to converge to ∏*. For clarity,
consider the binary reward setting (r+ and r-) and denote the error in r as e+ = P(r = r- |r =
r+),e- = P(r = r+∣r = r-) (a simplification of CRR∣×r∣ in the binary setting).
Lemma 1. Let r ∈ [0,Rmax] be a bounded reward. Assume 1 - e- - e+ > 0, then we have:
E[rpeer] = (1 - e— — £+)∙ E[rpeer] =(1 - e— — £+)∙ E[r] + COnSt,
where rpPeer = rpPeer(s, a) = r(s, a) - ξ ∙ r0 is the peer RL reward when observing the true reward r.
Lemma 1 shows that by subtracting the peer penalty term r0 from noisy reward r, rpeer recovers the
clean and true reward r in expectation.
Remark It,s notable that the expectation of the noisy reward E[r] can also be written as:
E[r] = (1 — e— — e+)E[r] + e-r+ + e+r-.
|---------{---------}
const
r+	,
However, we claim that the constant in peer reward has less effect on the true reward r. That’s
because we have that
E[r] = (1 - e— - e+) ∙(E[r] +  ----e+-----r— +  -----e-------
1	- e— - e+	1 - e— - e+
E[rpeer] = (I - e— - e+) ∙ (E[r] - (1 - PPeer)r- — PPeerr+),
where ppeer ∈ [0, 1] denotes the probability that a sample policy gets a reward r+. Since the magnitude
of noise terms 1—£二：十 and -e二二十 can potentially become much larger than 1 - PPeer and PPeer
in a high-noise regime,一二二二十 r+ + 一二七二十 r- will dilute the informativeness of E[r]. On the
contrary, E[Γpeer] contains a moderate constant noise thus maintaining more useful training signals of
the true reward in practice.
Based on Lemma 1, we further offer the following convergence guarantee:
Theorem 1.	(ConvergenCe) Given a finite MDP with noisy reward, denoting as Mf =
hS, A, R,F,P,γi, the Q-learning algorithm with Peer rewards, given by the uPdate rule,
Qt+ι(st,at) = (1 - αt)Qt(st,at) + α± [小小"1) + Y max Qt(st+ι,α)], ∏t(s) = arg max Qt (s,a)
a0∈A	a∈A
converges w.p.1 to the optimal policy π*(s) as long as Et at = ∞ and Et α2 < ∞.
Theorem 1 states that the agent will converge to the optimal policy w.p.1 with peer rewards without
requiring any knowledge of the corruption in rewards (CRRJI ×∣R∣,as opposed to previous work (Wang
et al., 2020) that requires such knowledge). Moreover, we found that, to guarantee the convergence to
∏*, the number of samples needed for our approach is no more than O(1∕(1 - e- - e+)2) times of
the one needed when the RL agent observes true rewards perfectly (see Appendix A).
5
Under review as a conference paper at ICLR 2021
Remark Even though we only presented analysis for the binary case for Q-Learning, our approach
is rather generic and is ready to be plugged into modern DRL algorithms. Specifically, we pro-
vide the multi-reward extension and implementations with DQN (Mnih et al., 2013) and policy
gradient (Sutton et al., 1999) using peer reward in Appendix A.
4.3 PeerBC: Peer Behavioral Cloning
Similarly, we present our CA solution in the setting of behavioral cloning (PeerBC). In BC, the
supervision is given by the weak expert’ noisy trajectory. The EvaπBC function in BC evaluates
the agent policy ∏θ and the weak expert' trajectory {(si, ai)}N=ι using '(∏θ, (Si, Gi)) where ' is an
arbitrary classification loss. Taking for instance the cross-entropy, the objective of PeerBC is:
JBC(∏θ) = EhEvaBC((si,ai),Gi)] - ξ ∙ EhEvaBC(Es),@川，	(4)
where EvaBC((S,a),α) = —'(∏θ, (s,ð.)) = log∏θ(a|s).	(5)
In (4), the first expectation is taken over (si,Gi)〜DeE,a，i 〜∏(∙∣Si) and the second is taken over
(sj, aj)〜DeE, aj 〜π(∙∣Sj), (sk, αk)〜TeE, ak 〜π(∙∣Sk). Again, the second EvaBC term in Jbc
serves the purpose of punishing over-agreement with the weak demonstration. Similarly, ξ ≥ 0 is a
parameter to balance the penalty for blind agreements. At each iteration, the agent learns under weak
supervision aG, and the training samples are generated from the distribution DE determined by the
weak expert.
We prove that the policy learned by PeerBC converges to the expert policy when observing a sufficient
amount of weak demonstrations. We focus on the binary action setting for the purpose of theoretical
analysis, where the action space is given by A = {A+,A-} and the weakness or noise in the weak
expert ∏e is quantified by e± = P(∏e(S) = A-∣∏e(S) = A+) and e- = P(∏e(S) = A+∣∏e(S)=
A-) (a simplification of cBC∣×∣λ∣ in the binary setting). Let the ∏de be the optimal policy by
maximizing the objective in (4) with imperfect demonstrations DeE (a particular set of with N i.i.d.
imperfect demonstrations). Note '(∙) should be specified as indicator loss: l(∏(S),a) = 1 when
π(s) = a, otherwise 1(π(s), a) = 0. We have the following upper bound on the error rate.
Theorem 2.	Denote by RDe := P(s,a)〜De(∏De(s) = a) the errOrratefOrPeerBC. With probability
at least 1 - δ, it is upper-bounded as: RDe ≤
J2log2∕δ
1+ξ
1 —e— —e+
Theorem 2	states that as long as weak demonstrations are observed sufficiently, i.e., N is sufficiently
large, the policy learned by PeerBC is able to converge to the clean expert policy πE (S) with a
convergence rate of O(1/ √N).
Peer Policy Co-Training Our
discussion of BC allows us to study
a more challenging co-training
task (Song et al., 2019). Given
a finite MDP M, there are two
agents that receive partial observa-
tions and we let πA and πB de-
note the policies for agent A and
B. Moreover, two agents are trained
jointly to learn with rewards and
noisy demonstrations from each
other (e.g., at preliminary training
phase). Symmetrically, we consider
on the case where agent A learns
with the demonstrations from B on
sampled trajectories, and πB effec-
Algorithm 1 Peer policy co-training (PeerCT)
Require: Views A, B, MDPs MA, MB, policies πA,πB, mapping
functions fA→B , fB→A that maps states from one view to the
other view, CA coefficient ξ, step size β for policy update.
1:	repeat
2:	Run πA to generate trajectories τA = {(siA, aiA, riA)}iN=1.
3:	Run πB to generate trajectories τB = {(sjB ,ajB ,rjB)}jM=1.
4:	Agents label the trajectories for each other
T 0A J {(sA, nB (fB^A(SA))0，_ ɪ,
T 'B J {(SB , nA(fA—B (SB ))}11.
5:	Updatepolicies: n{A，B} J π{A,B} + β ∙ VJcτ(∏{A,B})
6:	until convergence
tively serves as a noisy version of expert policy ∏e = arg max∏∈∏ E(s,a)〜DE [log π(a∣S)].
For simplicity of demonstration, we focus on recovering the clean expert policy by only adapt-
ing the BC evaluation term (ignoring the effect of RL rewards, see Eqn. (6)). Denote by
τθA = {(SiA,aiA,riA)}iN=1 the trajectory that πA generated via interacting with the partial world
6
Under review as a conference paper at ICLR 2021
BQQ
O 25	50	75	100
episode
(a) e = 0.1
0	50	100
episode
(b) e = 0.2
0	50	100	150
episode
(d) e = 0.4
Figure 2: Learning curves of DDQN on CartPole with true reward (r) ■, noisy reward (r) ■,
surrogate reward (Wang et al., 2020) (r)	, and peer reward (fpeer, ξ = 0.2) ■.
(c) e =0.3
MA. Then πB substitutes each action aA with its selection a0iB 〜πB(∙∣fA→B(SA)) as the weak
supervisions. Similar to PeerRL/PeerBC, the objective function of peer co-learning (PeerCT) becomes
JCT(∏θ) = EhEVaRL((SA,aA),rA) + EVaBC((SA,aA),aiB)] - ξ ∙ EhEVaBC((SA,aA),α0B)],	(6)
where the first expectation is taken over(sA,aA,rA)〜τA,a0iB 〜πB(∙∖fA→B(SA)), and the second
is taken over (sA,aA, rA)〜TA, (sA,aA, rA)〜TA,a'lB 〜πB(∙∖fA→B(sA)), ' is the loss function
defined in Eqn. (5) to measure the policy difference, and EVaπRL , EVaπBC are defined in Eqn. (3) and
(5) respectively. The full algorithm PeerCT is provided in Algorithm 1. We omit detailed discussions
on the convergence of PeerCT due to it can be viewed as a straight-forward extension of Theorem 2
in the context of co-training.
5	Experiments
We evaluate our solution in three challenging weakly supervised PL problems. Experiments on
control games and Atari show that, without any prior knowledge of the noise in supervisions, our
approach is able to leverage weak supervisions more effectively.
5.1	PeerRL with Noisy Reward
We first evaluate our method in RL with noisy reward setting. Following Wang et al. (2020), we
consider the binary reward {-1, 1} for Cartpole where the symmetric noise is synthesized with
different error rates e = e- = e+. We choose DQN (Mnih et al., 2013) and Dueling DQN
(DDQN) (Wang et al., 2016) algorithms and train the models for 10,000 steps. We repeat each
experiment 10 times with different random seeds and leave the results for DQN in Appendix D.
Figure 2 shows the learning curves for DDQN with different approaches in noisy environments
(ξ = 0.2) 1. Since the number of training steps is fixed, the faster the algorithm converges, the fewer
total episodes the agent will involve thus the learning curve is on the left side. As a consequence,
the proposed peer reward outperforms other baselines significantly even in a high-noise regime (e.g.,
e = 0.4). Moreover, we highlight that peer reward does not require any knowledge of noise rates or
complicated estimation algorithms compared to Wang et al. (2020). Table 1 provides quantitative
results on the average reward Ravg and total episodes Nepi . We find the agents with peer reward
lead to a larger Ravg (less generalization error) and a smaller Nepi (faster convergence) consistently,
which again verifies the effectiveness of our approach.
Analysis of the benefits in PeerRL When the noise rate e is small, we observed that the agents
with peer reward even lead to faster convergence than ones observing true reward perfectly. This
indicates there might be other factors apart from noise reduction that promote the RL learning with
peer reward. We hypothesize this is because (1) peer reward scales the reward signals appropriately,
which potentially reduces the variance and makes it easier to learn from; (2) the peer penalty term
encourages explorations in RL implicitly; (3) the human-specific “true reward” is also imperfect which
leads to a weak supervision scenario. More discussions and analysis are deferred to Appendix C.6.
There might be multiple explanations for the improvement in performance provided by PeerRL
depending on the specific RL tasks. However, we emphasize that the property of recovering from
noise is non-negligible especially in a high-noise regime (e.g., e = 0.4).
1We analysed the sensitivity of ξ and found the algorithm performs reasonable when ξ ∈ (0.1, 0.4). More
insights and experiments with varied ξ is deferred to Appendix D.
7
Under review as a conference paper at ICLR 2021
Table 1: Numerical performance of DDQN on CartPole with true reward (r), noisy reward (f),
surrogate reward r (Wang et al., 2020), and peer reward fpeer (ξ = 0.2). Ravg denotes average
reward per episode after convergence, the higher (↑) the better; Nepi denotes total episodes involved
in 10,000 steps, the lower (J) the better.
e = 0.1	e = 0.2	e = 0.3	e = 0.4
Ravg ↑	Nepi J	Ravg ↑	Nepi J
Ravg ↑
Nepi J	Ravg ↑	Nepi J
DDQN
r	195.6 ± 3.1	101.2 ± 3.2	195.6 ± 3.1	101.2 ± 3.2	195.6 ± 3.1	101.2 ±	3.2	195.2 ± 3.0	101.2 ± 3.3
r	185.2 ± 15.6	114.6 ± 6.0	168.8 ± 13.6	123.9 ± 9.6	177.1 ± 11.2	133.2 ±	9.1	185.5 ± 10.9	163.1 ± 11.0
r	183.9 ± 10.4	110.6 ± 6.7	165.1 ± 18.2	113.9 ± 9.6	192.2 ± 10.9	115.5 ±	4.3	179.2 ± 6.6	125.8 ± 9.6
rpeer	198.5 ±	2.3	86.2 ±	5.0	195.5 ±	9.1	85.3 ±	5.4	174.1 ± 32.5	88.8	± 6.3	191.8 ± 8.5	106.9 ±	9.2
Figure 3: Learning curves of BC on Atari. Standard BC ■, PeerBC (ours) ■, expert ■.
5.2	PeerBC from Weak Demonstrations
In BC setting, we evaluate our
approach on four vision-based
Atari games. For each envi-
ronment, we train an imperfect
RL model with PPO (Schulman
et al., 2017) algorithm. Here,
“imperfect” means the training is
terminated before convergence
when the performance is about
70% 〜90% as good as the fully
Table 2: BC from weak demonstrations. Our approach success-
fully recovers better policies than expert.
Environment	Pong	Boxing	Enduro	Freeway	Lift (↑)
Expert	15.1 ± 6.6	67.5 ± 8.5	150.1 ± 23.0	21.9 ± 1.7	-
Standard BC	14.7 ± 3.2	56.2 ± 7.7	138.9 ± 14.1	22.0 ± 1.3	-6.6%
ξ = 0.2	18.8 ± 0.6	67.2 ± 8.4	177.9 ± 29.3	22.5 ± 0.6	+11.3%
PeerBC ξ = 0.5	16.6 ± 4.0	75.6 ± 5.4	230.9 ± 73.0	22.4 ± 1.3	+19.5%
ξ = 1.0	16.7 ± 4.3	69.7 ± 4.7	230.4 ± 61.6	8.9 ± 4.9	+2.0%
Fully converged PPO	20.9 ± 0.3	89.3 ± 5.4	389.6 ± 216.9	33.3 ± 0.8	+70.6%
converged model. We then collect the imperfect demonstrations using the expert model and generate
100 trajectories for each environment. The results are reported under three random seeds.
Figure 3 presents the comparisons for standard BC and PeerBC, from which we observe that our
approach outperforms standard BC and even the expert it learns from. Note that during the whole
training process, the agent never learns by interacting directly with the environment but only have
access to the expert’s trajectories. Therefore, we owe this performance gain to PeerBC’s strong ability
for learning from weak supervisions. The peer term we add not only provably eliminates the effects of
noise but also extracts useful strategy from the demonstrations. In Table 2, we show the quantitative
results. Our approach consistently outperforms the expert and standard BC. As a reference, we also
compare two other baselines GAIL (Ho & Ermon, 2016) and SQIL (Reddy et al., 2019) and provide
the sensitivity analysis of ξ in Appendix D.
Analysis of benefits in PeerBC Similarly, the performance improvement of PeerBC might be also
coupled with multiple possible factors. (1) The imperfect expert model might be a noisy version of
the fully-converged agent since there are less visited states on which the selected actions of the model
contains noise. (2) The improvements might be brought up by biasing against high-entropy policies
thus PeerBC is useful when the true policy itself is deterministic. We provide more discussions about
the second factor in Appendix C.5.
5.3	PeerCT for Standard Policy Co-training
Finally, we verify the effectiveness of PeerCT algorithm in policy co-training setting (Song et al.,
2019). This setting is more challenging since the states are partially observable and each agent needs
to imitate another agent’s behavior that is highly biased and imperfect. Note that we adopt the exact
same setting as Song et al. (2019) without any synthetic noise included. This implies the potential
of our approach to deal with natural noise in real-world applications. Following Song et al. (2019),
we mask the first two dimensions respectively in the state vector to create two views for co-training
in classic control games (Acrobot and CartPole). Similarly, the agent either removes all even index
8
Under review as a conference paper at ICLR 2021
Figure 4: Policy co-training on control/Atari. Single view H, Song et al. (2019) , PeerCT (ours) ■.
coordinates (view-A) in the state vector or removing all odd index ones (view-B ) on Atari games. As
shown in Table 3 and Figure 4, PeerCT algorithm outperforms training from single view, and CoPiEr
algorithm consistently on both control games (ξ =0.5 in Figure 4a, 4b) and Atari games (ξ =0.2 in
Figure 4c, 4d). In most cases, our approach leads to a faster convergence and lower generalization
error compared to CoPiEr, which again verify that our ways of leveraging information from peer
agent enables recovery of useful knowledge from highly imperfect supervisions.
6	Conclusion
To deal with a series of RL/BC
problems with low-quality super-
vision signals, we formulate a
meta-framework weakly super-
vised policy learning to unify the
problem instances with weak su-
pervision in sequential decision-
making. Inspired by the cor-
related agreement (CA) mecha-
nism, we propose a theoretical
Table 3: Comparison with single view training and CoPiEr (Song
et al., 2019) on standard policy co-training.
Environment ∣ Acrobot CartPole Pong Breakout	Lift (↑)
Single View	A B	-136.6 ± 15.6 172.8 ± 5.5 17.8 ± 0.6 148.0 ± 16.5 34.7% -126.4 ± 8.0 186.7 ± 8.1 17.7 ± 0.5 137.8 ± 12.5 35.5%
CoPiEr	A B	-136.2 ± 5.2 174.1 ± 5.1 16.8 ± 0.5	107.5 ± 5.8 52.9% -131.5 ± 4.5 174.3 ± 5.4 16.5 ± 0.2	82.7 ± 6.9	72.0%
PeerCT	A B	-87.0 ± 3.9 188.8 ± 2.7 20.5 ± 0.4 263.6 ± 36.0	- -87.1 ± 6.3 184.7 ± 3.9 20.4 ± 0.5 268.6 ± 33.6	-
principled framework PeerPL that builds on evaluating a learning policy’s correlated agreements
with the weak supervisions. We demonstrate how our method adapts in RL/BC and the combined
co-training tasks and provide intensive analyses of their convergence behaviors and sample complex-
ity. Experiments on these tasks show our approach leads to substantial improvements over baseline
methods.
References
Ilge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,
Alex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, Jonas Schneider, Nikolas Tezak,
Jerry Tworek, Peter Welinder, Lilian Weng, Qiming Yuan, Wojciech Zaremba, and Lei Zhang.
Solving rubik’s cube with a robot hand. CoRR, abs/1910.07113, 2019.
John Asmuth, Michael L. Littman, and Robert Zinkov. Potential-based shaping in model-based
reinforcement learning. In AAAI, pp. 604-609. AAAI Press, 2008.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning
for self-driving cars. arXiv preprint arXiv:1604.07316, 2016a.
Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon
Goyal, Lawrence D. Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, Xin Zhang, Jake Zhao,
and Karol Zieba. End to end learning for self-driving cars. CoRR, abs/1604.07316, 2016b.
Kiante Brantley, Wen Sun, and Mikael Henaff. Disagreement-regularized behavior cloning. In Eighth
International Conference on Learning Representations (ICLR), April 2020.
Tim Brys, Anna Harutyunyan, Halit Bener Suay, Sonia Chernova, Matthew E. Taylor, and Ann Nowe.
Reinforcement learning from demonstration through shaping. In IJCAI, pp. 3352-3358. AAAI
Press, 2015.
Felipe Codevilla, Matthias Miiller, Antonio Lopez, Vladlen Koltun, and Alexey Dosovitskiy. End-to-
end driving via conditional behavior cloning. In 2018 IEEE International Conference on Robotics
and Automation (ICRA), pp. 1-9. IEEE, 2018.
9
Under review as a conference paper at ICLR 2021
Anirban Dasgupta and Arpita Ghosh. Crowdsourced judgement elicitation with endogenous profi-
Ciency. In Proceedings of the 22nd international conference on World Wide Web, pp. 319-330,
2013.
Tom Everitt, Victoria Krakovna, Laurent Orseau, and Shane Legg. Reinforcement learning with a
corrupted reward channel. In IJCAI, pp. 4705-4713, 2017.
Wael Farag and Zakaria Saleh. Behavior cloning for autonomous driving using convolutional
neural networks. 2018 International Conference on Innovation and Intelligence for Informatics,
Computing, and Technologies, 3ICT 2018, 2018. doi: 10.1109/3ICT.2018.8855753.
Alessandro Giusti, Jerome Guzzi, Dan C. Ciresan, Fang Lin He, Juan P. Rodriguez, Flavio Fontana,
Matthias Faessler, Christian Forster, Jurgen Schmidhuber, Gianni Di Caro, Davide Scaramuzza,
and Luca M. Gambardella. A Machine Learning Approach to Visual Perception of Forest Trails
for Mobile Robots. IEEE Robotics and Automation Letters, 1(2):661-667, 2016. ISSN 23773766.
doi: 10.1109/LRA.2015.2509024.
Xiaoxiao Guo, Shiyu Chang, Mo Yu, Gerald Tesauro, and Murray Campbell. Hybrid reinforcement
learning with expert state sequences. In AAAI, pp. 3739-3746. AAAI Press, 2019.
Todd Hester, Matej Vecerik, Olivier PietqUin, Marc Lanctot, Tom SchaUL Bilal Piot, Dan Horgan,
John Quan, Andrew Sendonaris, Ian Osband, Gabriel Dulac-Arnold, John Agapiou, Joel Z. Leibo,
and AUdrUnas GrUslys. Deep q-learning from demonstrations. In AAAI, pp. 3223-3230. AAAI
Press, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial behavior cloning. In Advances in Neural
Information Processing Systems, pp. 4572-4580, 2016.
Tommi S. Jaakkola, Michael I. Jordan, and Satinder P. Singh. Convergence of stochastic iterative
dynamic programming algorithms. In NIPS, pp. 703-710, 1993.
Niels JUstesen and Sebastian Risi. Learning macromanagement in starcraft from replays Using deep
learning. In 2017 IEEE Conference on Computational Intelligence and Games (CIG), pp. 162-169.
IEEE, 2017.
Sham Machandranath Kakade. On the Sample Complexity of Reinforcement Learning. PhD thesis,
University of London, 2003.
Michael J. Kearns and Satinder P. Singh. Finite-sample convergence rates for q-learning and indirect
algorithms. In NIPS, pp. 996-1002, 1998.
Michael J. Kearns and Satinder P. Singh. Bias-variance error boUnds for temporal difference Updates.
In COLT, pp. 142-147, 2000.
Michael J. Kearns, Yishay MansoUr, and Andrew Y. Ng. A sparse sampling algorithm for near-optimal
planning in large markov decision processes. In IJCAI, pp. 1324-1231, 1999.
Michael Laskey, Jonathan Lee, Roy Fox, Anca D. Dragan, and Ken Goldberg. DART: noise injection
for robUst behavior cloning. In CoRL, volUme 78 of Proceedings of Machine Learning Research,
pp. 143-156. PMLR, 2017.
Lisa Lee, Benjamin Eysenbach, RUslan SalakhUtdinov, Shixiang, GU, and Chelsea Finn. Weakly-
sUpervised reinforcement learning for controllable behavior, 2020.
Timothy P. Lillicrap, Jonathan J. HUnt, Alexander Pritzel, Nicolas Heess, Tom Erez, YUval Tassa,
David Silver, and Daan Wierstra. ContinUoUs control with deep reinforcement learning. CoRR,
abs/1509.02971, 2015.
Yang LiU and Hongyi GUo. Peer loss fUnctions: Learning from noisy labels withoUt knowing noise
rates. ICML, abs/1910.03231, 2020.
Aditya Menon, Brendan Van Rooyen, Cheng Soon Ong, and Bob Williamson. Learning from
corrUpted binary labels via class-probability estimation. In ICML, pp. 125-134, 2015.
10
Under review as a conference paper at ICLR 2021
Nolan Miller, Paul Resnick, and Richard Zeckhauser. Eliciting informative feedback: The peer-
prediction method. Management Science, 51(9):1359 -1373, 2005.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529, 2015.
Juarez Monteiro, Nathan Gavenski, Roger Granada, Felipe Meneguzzi, and Rodrigo Coelho Barros.
Augmented behavioral cloning from observation. CoRR, abs/2004.13529, 2020.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Andrew Y. Ng, Daishi Harada, and Stuart J. Russell. Policy invariance under reward transformations:
Theory and application to reward shaping. In ICML, pp. 278-287. Morgan Kaufmann, 1999.
Dean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural
computation, 3(1):88-97, 1991.
Siddharth Reddy, Anca D. Dragan, and Sergey Levine. SQIL: Behavior Cloning via Reinforcement
Learning with Sparse Rewards. 2019. URL http://arxiv.org/abs/1905.11108.
Siddharth Reddy, Anca D. Dragan, and Sergey Levine. SQIL: behavior cloning via reinforcement
learning with sparse rewards. In ICLR. OpenReview.net, 2020.
Joshua Romoff, Alexandre PiCha Peter Henderson, Vincent Frangois-Lavet, and Joelle Pineau.
Reward estimation for variance reduction in deep reinforcement learning. In ICLR (Workshop).
OpenReview.net, 2018.
StePhane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings ofthe
thirteenth international conference on artificial intelligence and statistics, pp. 661-668, 2010.
Stephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured
prediction to no-regret online learning. In Proceedings of the fourteenth international conference
on artificial intelligence and statistics, pp. 627-635, 2011.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
Clayton Scott. A rate of convergence for mixture proportion estimation, with application to learning
from noisy labels. In AISTATS, 2015.
Clayton Scott, Gilles Blanchard, Gregory Handy, Sara Pozzi, and Marek Flaska. Classification with
asymmetric label noise: Consistency and maximal denoising. In COLT, pp. 489-511, 2013.
Victor Shnayder, Arpit Agarwal, Rafael M. Frongillo, and David C. Parkes. Informed truthfulness in
multi-task peer prediction. In EC, pp. 179-196. ACM, 2016.
Jialin Song, Ravi Lanka, Yisong Yue, and Masahiro Ono. Co-training for policy learning. In UAI, pp.
441. AUAI Press, 2019.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv
preprint arXiv:1406.2080, 2(3):4, 2014.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In NIPS, pp. 1057-1063. The
MIT Press, 1999.
Faraz Torabi, Garrett Warnell, and Peter Stone. Behavioral cloning from observation. In IJCAI, pp.
4950-4957. ijcai.org, 2018.
11
Under review as a conference paper at ICLR 2021
John N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine Learning, 16(3):
185-202,1994.
Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-
learning. In AAAI, pp. 2094-2100, 2016.
Brendan van Rooyen and Robert C Williamson. Learning in the presence of corruption. arXiv
preprint arXiv:1504.00091, 2015.
John Von Neumann and Oskar Morgenstern. Theory of games and economic behavior (commemora-
tive edition). Princeton university press, 2007.
Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed rewards. In AAAI,
2020.
Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas.
Dueling network architectures for deep reinforcement learning. In ICML, volume 48, pp. 1995-
2003, 2016.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. In Machine Learning, pp. 279-292, 1992.
Yueh-Hua Wu, Nontawat Charoenphakdee, Han Bao, Voot Tangkaratt, and Masashi Sugiyama.
Behavior cloning from imperfect demonstration. In ICML, volume 97 of Proceedings of Machine
Learning Research, pp. 6818-6827. PMLR, 2019.
12
Under review as a conference paper at ICLR 2021
A	Analysis of PeerRL
We start this section by providing the proof of the convergence of Q-Learning under peer reward rpeeτ
(Theorem 1). Moreover, we give the sample complexity of phased value iteration (Theorem A1). In
the rest of this section, we show how to extend the proposed method to multi-outcome setting (Sec-
tion A.3) and modern deep reinforcement learning (DRL) algorithms such as policy gradient (Sutton
et al., 1999) and DQN (Mnih et al., 2013; van Hasselt et al., 2016) (Section A.4).
A.1 Convergence
Recall that we consider the binary reward case {r+,r-}, where r+ and r- are two reward levels. The
flipping errors of the reward are defined as e+ = P(rt = r- |rt = r+) and e- = P(rt = r+ |rt = r-).
The peer reward is defined as rpeer(s, a) = r(s, a) - r0, where r0 is randomly sampled reward over
all state-action pair (s, a). Note that we treat each (s, a) equally when sampling the r0 due to lack of
the knowledge of true transition probability P . In practice, the agent could only noisy observation of
peer reward rpeer(s, a) = r(s, a) - r0. We provide the Q-learning with peer reward in AlgOrithmlAI
Algorithm A1 Q-Learning with Peer Reward
〜1
Require: M =(S, A, R, P, γ), learning rate α ∈ (0, 1), initial state distribution β0.
1:	Initialize Q: S×A→R arbitrarily
2:	while Q is not converged do
3:	Start in state S 〜βo
4:	while s is not terminal do
5:	Calculate π according to Q and exploration strategy
6:	a — ∏(s); s0 〜P(∙∣s,a)
7:	Observe noisy reward	r(s, a) and	randomly sample	another r0	from all state-action pairs
8:	Calculate peer reward	rpeer(s, a)	= r(s, a) - r0
9:	Q(s,a) — (1 - α) ∙ Q(s,a) + α ∙ (rpeer(s,a) + Y ∙ max。，Q(s0,a0))
10:	s — s0
11:	end while
12:	end while
13:	return Q(s, a) and π(s)
We then show the proposed peer reward 心©「offers us an affine transformation of true reward in
expectation, which is the key to guaranteeing the convergence for RL algorithms.
Lemma 1. Letr ∈ [0,Rmax] be bounded reward and assume 1 - e- - e+ > 0. Then, if we define
the Peer reward rpeer(s, a) = r(s,a) — r0 ,in which the penalty term r0 is randomly sampled noisy
reward over all state-action pair (s, a), we have
E[rpeer] = (1 一 e- 一 e+)E[rpeer] = (1 — e- - e+)E[r] + const,
where rpeer is the clean version of peer reward when observing the true reward.
Proof. With slight notation abuse, we let rpeer, r, r, r0 also represent the random variables. Let ∏
denotes the RL agent’s policy. Consider the two terms on the RHS of noisy peer reward separately,
E[r]	= P(r = r+∣∏) ∙ Ey [P(r = r- |r = r+) ∙ r- + P(r = r+∣r =	r+)	∙	r+]	(7)
+ P(r = r- ∣π) ∙ Er=r- [P(r = r- ∖r = r-) ∙ r- + P(r = r+ |r	= r-)	∙ r+ ]	(8)
=P(r = r+∖∏) ∙ Er=r+ [e+r- + (1 - e+)r+]	(9)
+ P(r = r- ∖π) ∙ Er=r- [(1 — e-)r- + e-r+]	(10)
= P(r = r+ ∖π) ∙ Er=r+ [(1 — e+ — e-) ∙ r+ + e+r- + e-r+]	(11)
+ P(r = r- ∖π) ∙ Er=r- [(1 — e— — e+) ∙ r- + e-r+ + e+r-)]	(12)
=(1— e+ — e-)E[r] + e-r+ + e+r-.	(13)
Since we are treating the visitation probability of all state-action pair (s, a) equally while sampling
the peer penalty r0, then the probability of true reward r under this sampling policy πsample is a
13
Under review as a conference paper at ICLR 2021
constant, denoting as Ppeer, i.e., PPeer = P(r = r- ∣∏sampie) is a constant. Then We have,
E[r0] = P(r = r- ∣∏sample) ∙ 丫- + P(r = r+ ∣∏sample) ∙/+	(14)
=(e+ppeer + (1 ― e- )(1 ― PPeer)) ∙ r- + ((1 ― e+ )PPeer + e- (1 ― PPeer)) ∙ r+	(15)
=(1 - e- - e+)[(1 - Ppeer) ∙ Ir- + Ppeer ./十]+ £+『一+ £-『十.	(16)
As a consequence, We obtain the expectation of peer reWard satisfies
E[rxpeer]=旧旧一旧[河	(17)
=(1 - e+ - e- )E[r] - (1 - e- - e+)[(1 - Ppeer) ∙ r- + Ppeer ∙ r+]	(18)
=(1- e- - e+)E[r]+const.	(19)
Similarly, it is easy to obtain that E[.peer] = E[r] - [(1 - PPeer) ∙ r- + Ppeer ∙ r+]. Therefore, We
have E[rpeer] = (1 - e- - e+ )E[rpeer] = (1 - e- - e+)E[r] + const.	□
Lemma 1 shows the proposed peer reward rpeer offers US a “noise-free” positive (1 - e- - e+ >
0) linear transformation of true reWard r in expectation, Which is shoWn the key to govern the
convergence. It is widely known in Utility theory and reward shaping literatUre (Ng et al., 1999;
AsmUth et al., 2008; Von NeUmann & Morgenstern, 2007) that any positive linear transformations
leave the optimal policy Unchanged. As a conseqUence, we consider a “transformed MDP” M with
reward r = (1 - e- - e+)r + const, where the const is the same as the constant in Eqn. (19).
In what follows, we provide the formUlation of the concept of “transformed MDP” with the policy
invariance gUarantee.
Lemma A1. Given a finite MDP M =(S, A, R, P, Yi, a transformed MDP M =(S, A, R, P, Yi
with positive linear transformation in reward r := a ∙ r + b, where a, b are constants and a > 0, is
guaranteed consistency in optimal policy.
T->	e El √^ι C∙	. ∙	C∙	C	FTk Λ-T^TΓA * * ^√ Z 1	, ∙	zA∖ ∙	Γ∙ 11
Proof. The Q fUnction for transformed MDP M (denoting as Q) is given as follows:
∞∞
Q(s, a)=X Ytrt= X Yt(α ∙ Tt + b)
t=0	t=0
∞∞
= aXYtrt + XYtb
t=0	t=0
=a ∙ Q(s, a) + B,
where B = P∞=0 Ytb is a constant. Therefore, there is only a postive linear shift (a > 0) in Q(s, a)
thUs resUlting in invariance in optimal policy for transformed MDP:
*	K*
π*(s) = arg max Q*(s, a) = arg max [a ∙ Q(s, a) + B]
a∈A	a∈A
= arg max Q(s, a) = π*(s).
a∈A
□
Lemma A1 states that we only need to analysis the convergence of learned policy π(s) to the optimal
policy ∏* (s) for transformed MDP M, which is equivalent to the optimal policy π(s)* for original
MDP. This resUlt is relevant to potential-based reward shaping (Ng et al., 1999; AsmUth et al., 2008)
where a specific class of state-dependent transformation is adopted to speed up the convergence speed
of Q-Learning meanwhile maintaining the optimal policy invariance. Moreover, a degenerate case
for single-step decisions is studied in utility theory (Von Neumann & Morgenstern, 2007) which also
implies our result.
Finally, we need an auxiliary result (Lemma A2) from stochastic process approximation to analyse
the convergence for Q-Learning.
Lemma A2. The random process {∆t} taking values in Rn and defined as
∆t+1 (x)=(1- αt(x))∆t(x)+αt(x)Ft(x)
converges to zero w.p.1 under the following assumptions:
14
Under review as a conference paper at ICLR 2021
•	0 ≤	αt	≤	1,	t	αt(x)	= ∞ and	t αt (x)2	<	∞;
•	||E [Ft(χ)∣Ft] ||w ≤ γ∣∣∆t∣∣, with Y < 1 ；
•	Var [Ft(x)∣Ft] ≤ C(1 + ∣∆t∣∣W) ,for C> 0∙
Here Ft = {∆t, ∆t-ι, •一,Ft-ι …,ɑt, ∙∙∙} stands for the past at step t, αt(x) is allowed to
depend on the past insofar as the above conditions remain valid. The notation || ∙ ||w refers to some
weighted maximum norm.
ProofofLemma A2. See previous literature (Jaakkola et al., 1993; Tsitsiklis, 1994).	□
Theorem 1. (Convergence) Given a finite MDP with noisy reward, denoting as Mf
hS, A, R,F,P,γi, the Q-learning algorithm with peer rewards, given by the update rule,
Qt+1(st, at) = (1 - αt)Qt(st, at) + αt
,peer (st,at)+γmaxQt(st+1,b)
b∈A
πt(s) = argmaxQt(s, a)
a∈A
(20)
(21)
converges w.p.1 to the optimal policy π*(s) as long as Et at = ∞ and Et α2 < ∞.
Proof. Firstly, We construct a surrogate MDP M with the POSitiVe-Iinearly transformed reward
r = (1 - e- - e+) ∙ r + const, where const = -(1 - e- - e+)((1 - P) ∙ r- + P ∙ r+) is a
constant. From Lemma A1, we know the optimal policy for M is precisely the optimal policy for
M: π*(s) = π*(s).
Let Q denotes the optimal state-action function for this transformed MDP M. For notation brevity,
we abbreviate St, st+ι, r peer (st, st+ι), Qt, Qt+1, and αt, as s, s0, Q, Q0, rpeer and α, respectively.
Subtracting from both sides the quantity Q*(s, a) in Eqn. (21):
Q0(s, a) - Q*(s, a) =(1 - α)(Q(s, a) - Q*(s, a)) + α
0	*
rpeer + Y maxQ(s , b) - Q (s, a)
b∈A
*	0*
Let ∆t(s,a) = Q(s,a) - Q (s,a) and Ft(s,a) = rpeer + Y maXb∈A Q(s ,b) - Q (s, a).
∆t+1 (s0,a)=(1- α)∆t(s, a) + αFt(s, a).
In consequence,
E [Ft(s,a)∣Ft] = E rpeer + Y max Q(s,b) - Q (s,a)
b∈A
=E rpeer + Ymax Q(s, b) — r — Y max Q*(s0, b)
=E [rpeer] — E [r] + YE max Q(s0, b) — max Q*(s0, b)
p	b∈A	b∈A
=YE maxQ(s0, b) — maxQ*(s0, b)
≤ YE	max ∣Q(s0, b) — Q*(s0, b)I
b∈A,s0 ∈S
=Y EhkQ - Q*k∞i = Y ||Q - Q*h = Y |Ah.
15
Under review as a conference paper at ICLR 2021
In above derivations, we utilize the unbiasedness property for peer reward (Lemma 1) and the
inequality maxb∈∕Q(s0, b) - maXb∈A Q*(s0,b) ≤ maXb∈AS∈s ∣Q(s0,b) - Q*(s0,b)∣.
Var [Ft(s,a)∣Ft] = E ( rpeer + YmaxQ(s0,b) - Q*(s,a) - E
b∈A
)
2
E 1,peer + YmaxQ(s0, b) - E
r peer
+ Y maxQ(s0,b)
Var
卜。eer
+ Y max Q(s0, b)
b∈A
Since rpeer is bounded, it can be clearly verified that
Var [Ft(s,a)∣Ft] ≤ C00(1 + ∣∣∆t(s,a)∣∣∞)
for some constant C00 > 0. Then, ∆t converges to zero w.p.1 from Lemma A2, i.e., Q(s, a)
converges to Q*(s, a). As a consequence, we know the policy ∏t(s) converges to the optimal policy
π*(s) = π* (s).
A.2 Sample Complexity
In this section, we establish the sample complexity for Q-Learning with peer reward as discussed in
Sec 4.2. Since the transition probability P in MDP remains unknown in practice, we firstly introduce
a practical sampling model G(M) following previous literature (Kearns & Singh, 1998; 2000; Kearns
et al., 1999). in which the transition can be observed by calling the generative model. Then the
sample complexity is analogous to the number of calls for G(M) to obtain a near optimal policy.
Definition A1. A generative model G(M) for an MDP M is a sampling model which takes a
state-action pair (st, at) as input, and outputs the corresponding reward r(st, at) and the next state
st+1 randomly with the probability of Pa (st,st+ι), i.e., st+ι 〜P(∙∣s, a).
It is known that exact value iteration is not feasible when the agent interacts with generative model
G(M) (Wang et al., 2020; Kakade, 2003). For the convenience of analysing sample complexity, we
introduce a phased value iteration following Wang et al. (2020); Kearns & Singh (1998); Kakade
(2003).
Algorithm A2 Phased Value Iteration
Require: G(M): generative model of M =(S, A, R, P,Y), T: number of iterations.
1:	SetVT(s)=0
2:	for t = T - 1,…，0 do
3:	Calling G(M) m times for each state-action pair.
Pa(…)=#["- 't+1]
m
4:	Set
V (st) = max X P a(st,st+ι) [rt + YV (st+ι)]
a∈A
st+1 ∈S
π(s) = arg max V(st)
a∈A
5:	end for
6:	return V(s) and π(s)
Note that Pa(st, st+ι) is the estimation of transition probability Pa(st, st+ι) by calling G(M) m
times. For the simplicity of notations, the iteration index t decreases from T - 1 to 0.
We could also adopt peer reward in phased value iteration by replacing Line 4 in Algorithm A2 by
V (st) = max Y' Pa(st,st+ι)[rpeer(st,α) + YV (st+ι)].
a∈A
st+1 ∈S
16
Under review as a conference paper at ICLR 2021
Then the sample complexity of one variant (phased value iteration) of Q-Learning is given as follows:
Theorem A1. (Sample Complexity) Let r ∈ [0, Rmax] be bounded reward, for an appropriate choice
of m, the phased value iteration algorithm with peer reward rpeer calls the generative model G(M)
O Q(1SeAT 十)2 log |S||AIT) times in T epochs, and returns a policy such that for all state s ∈ S,
11VK(S) — V*(s)∣ ≤ e, w.p. ≥ 1 — δ, 0 < δ < 1, where η = 1 — e_ — e+ > 0 is a constant.
Proof. Similar to Theorem 1, we firstly construct a transformed MDP M and the optimal policies
for these two MDP are equivalent (Lemma A1). As a result, we could analyse the sample complexity
of phased value iteration under M.
It is easy to obtain that "er ∈ [0, Rmax] and Vπ(s) ∈ [0, R_ax] are also bounded. Using Hoeffd-
ing,s inequality, we have
Pr(∣E [*(st+ι)] — X Pa(St,St+1)V+1(st+1)
I	St 十1∈s
—2me2(1 — Y )2、
T ≤2exp(	Rmaxγ)),
Pr I |E [rpeer(st, a)] —): Pa(St, st+1)rpeer(st, a)
|	St 十1∈s
2me2 ʌ
≥e) ≤2exp(FJ
EI ,1	1 ∙ /'Z'	1	1	1	1 r'	T TrTT / ∖	1	, ∙	1	1 r~	τ∕V * / ∖	1
Then the difference between learned value function Vπ(s)t and optimal value function V*(s)t under
transformed MDP at iteration t is given:
匕"(s) — Vt(S)J =maχE [rt + YVt+ι(st+ι)] 一 max X Pa(st, st+ι) [rpeer(st, a) + γVt⅛ι(st+ι)]
a∈A	a∈A δ—y
st+ι ∈s
≤ max E [rt] — T Pa(st, st+ι)rpeer(st, a)
a∈A	—y
st+ι ∈s
+ Y max
a∈A
E [Vt+1(St+1)] - X Pa(St, St+1)%+1(St+1)
st+ι ∈s
≤ e1 + max lE [rt] 一 E [rpeer]∣ + Ye2 + IEhVt+1 (st+l)] - ElVt+1(St+1)] |
≤ Ymaχl%(S) — v+1 (s)| + e1 + Ye2
Recursing above equation, we get
max〔V*(s) — V(s) ≤ (e1 + γe2) + γ(e1 + γe2) +-----+ YT-1(e1 + γe2)
s∈S I
=(e1 + Ye2)(1 — YT)
1 — Y
Let e1 = e2 = f-?：, then maxs∈s ∣ V * (S) — V (s)∣ ≤ e. In other words, for arbitrarily small e, by
choosing m appropriately, there always exists e1 and e2 such that the value function error is bounded
within e. As a consequence the phased value iteration algorithm can converge to the near optimal
policy within finite steps using peer reward.
Note that there are in total ∣S∣∣A∣T transitions under which these conditions must hold, where ∣ ∙ ∣
represent the number of elements in a specific set. Using a union bound, the probability of failure in
any condition is smaller than
2∣SIIAIT ∙ exp ✓-mI ∙ ¾f ).
17
Under review as a conference paper at ICLR 2021
We set above failure probability less than δ, and m should satisfy that
1	∣S∣∣A∣T∖
m = O ① log - )-
In consequence, after m∣S∣∣A∣T calls, which is, O (lsl[AIT log 1S11A1T), the value function Con-
verges to the optimal value function V*(s) for every S in transformed MDP M, With probability
greater than 1 - δ .
From Lemma A1, we know V*(s) = (1 - e- - e+) ∙ V*(s) + C, where C is a constant. Let
e = (1 - e- - e+) ∙ e0 and V(S) = (1 - e- - e+) ∙ V0(s) + C, we have
| *( )	0( )|	ʌ V*(s) - C	V(s) - C
|V (S) - V (S)| =				 - 			 (I - e- - e+)	(I - e- - e+)
(1-e--e+) IV*(S)- V(S)I ≤ e0
(22)
(23)
This indicates that when the algorithm converges to the optimal value function for transformed
MDP M, it also finds a underlying value function V0(s) = 1V(s) that converges the optimal value
function V* (S) for original MDP M.
As a consequence, we know it needs to call O (jj-eA-e十)2 log 1S11A1T)
in value function for original MDP M, which is no more than O
(1 )
 
to achieve an e0 error
times of the one
needed when the RL agent observes true rewards perfectly. When the noise is in high-regime, the
algorithm suffers from a large 门 一 1一)2 thus less efficient. Moreover, the sample complexity of
(1-e- -e+ )
phased value iteration with peer reward is equivalent to the one with surrogate reward in Wang et al.
(2020) though sampling peer reward is less expensive and does not rely on any knowledge of noise
rates.
□
A.3 Multi-outcome Extension
In this section, we show our peer reward is generalizable to multi-class setting. Recall that in Section
3.2 we suppose the reward is discrete and has |R| levels, and the noise rates are characterized as
CRRI×∣r∣. Here we make further assumptions on the confusion matrix: the reward is misreported to
each level with specific probability, e.g.,
C
RL
IRI×IRI
1 - i6=1 ei
e1,
.
.
.
e1,
e2,
e2,
eIRI
eIRI
.
.
.
1 -	i6=IRI ei
(24)
Following the notations in A.1, we define the peer reward in multi-outcome settings as r(S, a)=
r(s, a) - r0, where r0 is randomly sampled following a specific sample policy ∏sampie over all
state-action pairs. Let Rpeer, R, R, and R denote the random variables corresponding to rpeer, r, r,
18
Under review as a conference paper at ICLR 2021
rz, cij represents the entry of C∣¾∣x∣^∙ Then we have 〜 因	的 je7γ [川=y^p(κ=⅞k) ⅛=1	j=l = jp(R = j¾m (1-J2ed Ri+ ∑ i=l	L ∖	j≠i )	3≠ 因	Γ /	困、	” = EP(R =居忻)1—E“兄十工 ⅛=ι	|_ ∖	j=i /	j= / 困 ∖	I^l =(1 - £ 立西典 + £ %Rj) ∖ J=I )	J=I and 〜 Ia	〜 %sample [川=' P (R = Rl7fSample) i=l I^l 因 =〉:Rj〉[ IP)(R= Ri 忻SamPIe) Cij J=I 』 I^i Γ = y^Rj y^P(H = ¾∣7Γsampie) Gj ÷ P (H = ¾∣7Γs J=I 户 j I^i Γ 的 = y^Rj y^P(H = ¾∣7Γsampie) ej- ÷ P (H = ¾∣πs J=I Ij=I / 因 \ 因 =I 1 - ɪ3 βi I 叱sample [R] +	% Rj . \	』)	J = I Then, the peer reward is formulated as E [¾eer] = Eπ [h] - E [A，] / I^l ∖	/ I^l ∖ =11- ɪj sj % 典—(i —工 / J 也 = (I- ɪ2 eJ Eπ [J?] + const.	d %Rj 、 - d EjRj 1 ample) I ɪ 一): ɛi ∖	/j /	因 ample) I ɪ 一): ɛi ∖』 sample [用
19
Under review as a conference paper at ICLR 2021
A.4 Extension in Modern DRL algorithms
In this section, we give the following deep reinforcement learning algorithms combined with our
peer reward in Algorithm A3 and A4. In Algorithm A3, we give the peer reward aided robust
policy gradient algorithm, where the gradient in Equation 25 corresponds to the loss function
'((s, a),q) = qlog∏θ(a|s), which is classification calibrated (Liu & Guo, 2020). So the expectation
of the gradient in 25 is an unbiased esitmation of the policy gradient in corresponding clean MDP. In
(A4), We present a robust DQN algorithm with peer sampling, in which the origin loss is '((s, a), y),
also classification calibrated. Thus the robustness can be proved via Liu & Guo (2020).
Algorithm A3 Policy Gradient (Sutton et al.,1999) with Peer Reward
〜1
Require: M =(S, A, R, P, γ), learning rate α ∈ (0, 1), initial state distribution β0, weight parameter ξ.
1:	Initialize πθ : S × A→R arbitrarily
2:	for episode =1to M do
3:	Collect trajectory tθ = {(si,ai,ri)}T=o, where so 〜βo, at 〜∏θ(∙∣st), st+ι 〜P(∙∣st,at).
4:	Compute qt = PT= YtTri for all t ∈ {0,1,...,T}
5:	For each index i ∈{0, 1,...,T}, we independently sample another two different indices j, k,
6:	and update policy parameter θ following
θ 一 θ + α [qiVθ log∏θ(a"si) - ξ ∙ qkVθ log∏θ(aj∣Sj)]
(25)
7:	end for
8:	return πθ
Algorithm A4 Deep Q-Network (Mnih et al., 2013) with Peer Reward
			

Require: M =(S, A, R, P, γ), learning rate α ∈ (0, 1), initial state distribution β0, weight parameter ξ.
1:
2:
3:
4:
5:
6:
7:
8:
Initialize replay memory D to capacity N
Initialize action-value function Q with random weights
for episode =1to M do
for t =1to T do
With probability e select a random action at, otherwise select at = maxa Q* (s, a)
Execute action at and observe reward rt and observation st+ι
Store transition (st,at,rt,st+ι) in D
Sample three random minibatches of transitions (si,ai,ri ,Si+ι),	(sj,aj,rj,Sj+ι),
(sk,ak,rk,Sk+ι) from D.
9:
Set yTi = (
for terminalsi
10:
Set ypeer
ri + γmaxaθ Q(si+ι, a0) for non-terminal Si+ι
J Tk	for terminalsi
]亍k + γmaxaθ Q(sj+ι,a0) for non-terminal sj+ι
11:
12:
13:
14:
Perform a gradient descent step on (yi — Q(si,ai))2 — ξ ∙ (ypeer — Q(Sj, aj))2
end for
end for
return Q
20
Under review as a conference paper at ICLR 2021
B Analysis of PeerBC
We prove that the policy learned by PeerBC converges to the expert policy when observing a sufficient
amount of weak demonstrations in Theorem A2.
Theorem A2. With probability at least 1 - δ, the error rate is upper-bounded by
RDE ≤ I —,	(26)
where N is the number of state-action pairs demonstrated by the expert.
Proof. Recall DE denotes the joint distribution of imperfect expert' state-action pair (s, a). Assume
there is a perfect expert and the corresponding state-action pairs (s,a) 〜DE. The indicator
classification loss l(∏(s),a) is specified here for a clean presentation, where l(∏(s), a) = 1 when
π(s) = a, otherwise l(π(s), a) = 0. Let DE := {(si, ai)}N=ι be the set of imperfect demonstrations,
and DE := {(si, <⅛)}N=ι be the set of weak demonstrations. Define:
RDE (n):= E(s,a)〜DE Xn(S),a)] , RDES)= E(s,G)〜DE U(n(S), a)]
RDE(n) := ： X l(π(si),ai), RDEg := ɪ X l(π(si),5i).
i∈[N ]	i∈[N ]
Note we focus on the analyses of loss in this proof. The negative of loss can be seen as a reward.
Denote by πDe and πDe be the optimal policy obtained with minimizing the indicator loss with
dataset DE and distribution DE. We shorten ∏ee as n*, which is the best policy we can learn from
imperfect demonstration with our algorithm. Let n* be the policy for the perfect expert. We would
like to see the performance gap of policy learning between imperfect demonstrations and perfect
demonstrations, i.e. RDE (n*) - RDE (n*). Using Hoeffding,s inequality with probability at least
1 - δ, we have 
IRDE (n)- rDe (n)∣ ≤ (1+	∙
Note we also have
RDE(A *) - RDeE (nDeE )
≤rDe(π*) - RDE(TDE)+(rDe(π*) - RDEb))
+ RDeE (nDeE) - RDeE (nDeE)
≤0 + 2 max I RDEg- RD E (n)|
≤(1 +
Before proceeding, we need to define a constant to show the affect of label noise. When the dimension
of action space is 2, the problem is essentially a binary classification with noisy labels (Liu & Guo,
2020), where the noise rate (a.k.a confusion matrix) is defined as e+ = P(∏e(s) = A-∣n*(s) = A+)
and e- = P(∏e(s) = A+∣n*(s) = A-). Recall the action space is defined as A = {A+,A-}. The
noise constant is denoted by e = e-1 + e+1. Accordingly, when the dimension of action space is
|R| > 2, we can also get similar results under uniform noise where
eu := P(∏e(s) = u∣n*(s) = u0), u0 = u.
(27)
The noise constant e is denoted by e = P|uR=|1 eu . The feature-independent assumption holds thus
the properties of peer loss functions (Liu & Guo, 2020) can be used, i.e.
RDe (n*) - RDe (n*)
=二)(RDE(n*)- RDEmDE))
<1 + ξ 22log2∕δ
≤ 1 - e V N
21
Under review as a conference paper at ICLR 2021
From definition and deterministic assumption for ∏*, We have RDE (∏*)
the k-th iteration is
R	∕*j1+ ξ, /2log2∕δ
RDE (π ) ≤ RDE (π ) + 1-:ʌ/ —N—
=1+ ξ r 2log2∕δ
=1 - e V -N~.
Note RDE (∏*) = RDE by definition.
0. Thus the error rate in
(28)
□
22
Under review as a conference paper at ICLR 2021
C Supplementary Experiments
C.1 Experimental Setup
We set up our experiments within the popular OpenAI stable-baselines2 and keras-rl3
framework. Specifically, three popular RL algorithms including Deep-Q-Network (DQN) (Mnih
et al., 2013; van Hasselt et al., 2016), Dueling-DQN (DDQN) (Wang et al., 2016) and Proximal Policy
Optimization Algorithms (PPO) are evaluated in a varied of OpenAI Gym environments including
classic control games (CartPole, Acrobot) and vision-based Atari-2600 games (Breakout,
Boxing, Enduro, Freeway, Pong).
C.2 Implementation Details
RL with noisy reward Following Wang et al. (2020), we consider the binary reward {-1, 1} for
Cartpole where the symmetric noise is synthesized with different error rates e = e- = e+ .We
adopted a five-layer fully connected network and the Adam optimizer. The model is trained for
10,000 steps with the learning rate of 1e-3 and the Boltzmann exploration strategy. The update rate
of target model and the memory size are 1e-2 and 50,000. The performance is reported under 10
independent trials with different random seeds.
BC with weak expert We train the imperfect expert on the framework stable-baselines
with default network architecture for Atari and hyper-parameters from rl-baselines-zoo4. The
expert model is trained for 1, 400, 000 steps for Pong and 2, 000, 000 steps for Boxing, Enduro and
Freeway. For each of those environment, We use the trained model to generate 100 trajectories, and
behavior cloning is performed on these trajectories. We adopt cross entropy loss for behavior cloning
and add a small constant (1 × 10-8) for each logit after the softmax operation for peer term to avoid
this term become too large. In BC experiments, the batchsize is 128, learning rate is 1 × 10-4 and
the e value for Adam optimizer is 1 X 10-8.
Policy co-training For the experiments on Gym (CartPole and Acrobot), we mask the first co-
ordinate in the state vector for one view and the second for the other, same as Song et al. (2019).
Both policies are trained with PPO(Schulman et al., 2017) + PeerBC. In each iteration, we sample
128 steps from each of the 8 parallel environments. These samples are fed to PPO training with
a batchsize of 256, a learning rate of 2.5 × 10-4 and a clip range of 0.1. Both learning rate and
clip range decay to 0 throughout time. We represent the policy by a fully connected network with 2
hidden layers, each has 128 units.
For the experiments on Atari (Pong and Breakout), the input is raw game images. We adopt the
preprocess introduced in Mnih et al. (2013) and mask the pixels in odd columns for one view and
even columns for the other. The policy we use adopts a default CNN as in stable-baselines.
Batchsize, learning rate, clip range and other hyper-parameters are the same as Gym experiments.
Note that we only add PeerBC after 1000 episodes.
2https://github.com/hill-a/stable-baselines
3 https://github.com/keras- rl/keras- rl
4https://github.com/araffin/rl-baselines-zoo/blob/master/hyperparams/
ppo2.yml#L1
23
Under review as a conference paper at ICLR 2021
3
C.
Supplementary
500050
8pos'Q.8」8d sdφls
NoCI
200500050
φpoωds」8d sdφω
BQQ
O 25	50	75	100
episode
(a) e = 0.1
Results for Figure 2 and Table 1
150
100
50
(b) e =0.2
(c) e =0.3
200
150
100
50
0	50	100
episode
(d) e = 0.4
150
Figure A1: Learning curves on CartPole game with true reward (r) , noisy reward (r) , surrogate
reward (Wang et al., 2020) (r)	, and peer reward (FPeer, ξ = 0.2) ■. Each experiment is repeated
10 times with different random seeds.
Table A1: Numerical performance of DDQN on CartPole with true reward (r), noisy reward (rF),
surrogate reward r (Wang et al., 2020), and peer reward rpeer(ξ = 0.2). Ravg denotes average
reward per episode after convergence, (last five episodes) the higher (↑) the better; Nepi denotes total
episodes involved in 10,000 steps, the lower ⑷ the better.
		e=	0.1	e=	0.2	e=	0.3	e=	0.4
		Ravg ↑	Nepi ]	Ravg ↑	Nepi ]	Ravg ↑	Nepi ]	Ravg ↑	Nepi ]
	r	183.6 ± 7.6	101.3 ± 4.8	184.0 ± 7.3	101.5 ± 4.6	184.0 ± 7.3	101.5 ± 4.6	184.0 ± 7.3	101.5 ± 4.6
DQN	~ r	189.3 ± 12.7	98.2 ± 6.5	189.7 ± 7.9	110.5 ± 7.1	183.2 ± 9.8	130.5 ± 7.7	169.7 ± 18.6	150.2 ± 11.4
	ʌ r	188.3 ± 8.2	101.1 ± 6.2	192.7 ± 9.2	97.9 ± 6.4	185.4 ± 15.9	116.9 ± 11.0	184.8 ± 16.4	123.1 ± 8.6
	r peer	177.2 ± 19.1	91.2 ± 5.9	170.0 ± 24.8	94.6 ± 8.5	190.5 ± 14.3	99.4 ± 5.2	183.1 ± 13.3	118.1 ± 10.7
	r	195.6 ± 3.1	101.2 ± 3.2	195.6 ± 3.1	101.2 ± 3.2	195.6 ± 3.1	101.2 ± 3.2	195.2 ± 3.0	101.2 ± 3.3
DDQN	~ r	185.2 ± 15.6	114.6 ± 6.0	168.8 ± 13.6	123.9 ± 9.6	177.1 ± 11.2	133.2 ± 9.1	185.5 ± 10.9	163.1 ± 11.0
	ʌ r	183.9 ± 10.4	110.6 ± 6.7	165.1 ± 18.2	113.9 ± 9.6	192.2 ± 10.9	115.5 ± 4.3	179.2 ± 6.6	125.8 ± 9.6
	r rpeer	198.5 ± 2.3	86.2 ± 5.0	195.5 ± 9.1	85.3 ± 5.4	174.1 ± 32.5	88.8 ± 6.3	191.8 ± 8.5	106.9 ± 9.2
C.4 Additional Results for PeerRL on Pendulum
To further evaluate the effectiveness of PeerRL, we conduct experiments on a more challenging
continuous control task Pendulum, where the goal is to keep a frictionless pendulum standing up.
Since the rewards in pendulum are continuous: r ∈ (-16.3, 0.0], we discretized it into 17 intervals:
(-17, -16], (-16, -15], ∙∙ ∙ , (-1,0], with its value approximated using its maximum point. We
experiment the DDPG (Lillicrap et al., 2015) and uniform noise in this environment. In Figure A3,
the RL agents with proposed CA objective successfully converge to the optimal policy under different
amounts of noise. On the contrary, the agents with noisy rewards suffer from the biased noise
especially in a high-noise regime.
C.5 Sensitivity Analysis of Peer Penalty ξ
In this section, we analyze the sensitivity of ξ in RL and BC tasks. Note that we did not tune this
hyperparameter extensively in all the experiments presented above since we found our method works
robustly in a wide range of ξ .
RL with noisy reward We repeat the experiment in Figure A1 for DQN but with a varying ξ from
0.1 to 0.4. As shown in Figure A2, our method works reasonably and leads to faster convergence
compared to baselines. However, we found that the late stage of training, a small ξ is necessary
since the agent already gains useful knowledge and make reasonable actions, therefore, an over-large
24
Under review as a conference paper at ICLR 2021
(b) e
(a) e
(c) e
(d) e =0.4
Figure A2: Learning curves of DQN on CartPole game With Peer reward (Fpeer) ■ under different
choices of ξ (from 0.1 to 0.4).
Figure A3: Learning curves of DDPG (Lillicrap et al., 2015) on Pendulum with true reward (r) ■,
noisy reward (r) ■, and peer reward (Fpeer) ■.
25
Under review as a conference paper at ICLR 2021
penalty might avoid the agent achieving simple agreements with the supervision signals, especially in
a low-noise regime (see ξ =0.4,e=0.1). This observation inspires us that a decay schedule of ξ
might be helpful in stabilizing the training of PeerRL algorithms. To verify this hypothesis, we repeat
the above experiments but with a linear decay ξ that decreases from 0.4 to 0.1. In Figure A4, we
found the linear decay schedule is able to stabilize the convergence of PeerRL algorithms compared
to static ξ = 0.4. The theoretical principles and insights of dynamic peer penalty merit further study.
(b) e =0.2
(f) e =0.2
(c) e =0.3
(g) e =0.3
(d) e =0.4
(h) e =0.4
Figure A4: Learning curves of DQN on CartPole game With Peer rewards (FPeer) ■. Here, a linear
decay ξ is applied during training procedure (initial ξ = 0.4). Compared to static ξ = 0.4, the linear
decay Peer Penalty stabilizes the convergence of RL algorithms.
BC from weak demonstrations We conduct exPeriments on Pong with 12 different ξ values,
varying from 0.1 to 1.2. As a reference, two other BC baselines GAIL (Ho & Ermon, 2016) and
SQIL (Reddy et al., 2019) are considered. We do not include GAIL in the figure, since GAIL fails
to Produce meaningful results on vision-based Atari games as also observed (Reddy et al., 2019;
Brantley et al., 2020). From Figure A5, we can see PeerBC outPerforms Pure behavior cloning and
SQIL(Reddy et al., 2019) when ξ is within [0.1, 0.7], revealing our ProPosed PeerBC is a suPerior
behavior cloning aPProach able to better elicit information from imPerfect demonstrations.
C.6 Variance Reduction in Noisy Reward Setting
In Practice, there is a trade-off question between bias and variance that influence the RL training.
Even though variance reduction aPProaches theoretically do not resolve the challenge when bias
Presents in the observed rewards, it might be beneficial to the training Procedure in Practice. To
investigate whether the variance reduction techniques are helPful in the setting with biased noise, we
rePeated the exPeriments in Table 1. We adoPt the variance reduction technique (VRT) ProPosed
in Romoff et al. (2018) and found it brings little benefits to noisy reward and surrogate reward as
shown in Figure A6. However, Peer + VRT Performs worse comPared to Peer reward only. This
demonstrates the benefits of scenarios does not mainly come from the variance reduction in our
setting. The full quantitative results are Presented in Table A2.
C.7 Stochastic Policy for Behavioral Cloning
In this section, we analyze the stochasticity of the imPerfect exPert model and fully-converged PPO
agent (assumed to be the clean exPert), and show that our PeerBC can handle both cases when the
clean exPert is stochastic and when it’s rather deterministic.
26
Under review as a conference paper at ICLR 2021
(a) ξ =0.1
(b) ξ =0.2
(c) ξ =0.3
(e) ξ =0.5
(h) ξ =0.8
(i)ξ=0.9	(j)ξ=1.0	(k)ξ=1.1	(l)ξ=1.2
Figure A5: Sensitivity analysis of ξ for PeerBC on Pong With behavior cloning ■, PeerBC
varies from 0.2 to 0.5 and 1.0), expert ■, and SQIL ■ reported by SQIL (Reddy et al., 2019).
experiment is repeated under 3 different random seeds.
∙(ξ
Each
ZOQQ
25	50	75	100
episode
(a) e = 0.1
50	100
episode
(b) e = 0.2
50	100
episode
(c) e = 0.3
50 IoO
episode
(d) e = 0.4
Figure A6: Learning curves of DDQN on CartPole with true reward (r) ■, noisy reward (r) ■, noisy
reward + VRT ■, surrogate reward (Wang et al., 2020) (r) ■, surrogate reward + VAT ■ and peer
reward (FPeer, ξ = 0.2) ■.
Table A2: Numerical performance of DDQN on CartPole with true reward (r), noisy reward (f),
surrogate reward r (Wang et al., 2020), and peer reward rpeer(ξ = 0.2). + VRT denotes the variance
reduction techniques in Romoff et al. (2018) are adopted.
e =0.1
e =0.2
e =0.3
e =0.4
	Ravg ↑	Nepi J	Ravg ↑	Nepi J	Ravg ↑	Nepi J	Ravg ↑	Nepi J
	r	195.6 ± 3.1	101.2 ± 3.2	195.6 ± 3.1	101.2 ± 3.2	195.6 ± 3.1	101.2 ± 3.2	195.2 ± 3.0	101.2 ± 3.3
DDQN	r	185.2 ± 15.6	114.6 ± 6.0	168.8 ± 13.6	123.9 ± 9.6	177.1 ± 11.2	133.2 ± 9.1	185.5 ± 10.9	163.1 ± 11.0
	+VRT 190.2 ± 14.6	111.8 ± 4.5	183.6 ± 15.7	118.0 ± 9.5	179.6 ± 19.9	129.6 ± 13.9	182.6 ± 13.4	157.2 ± 19.0
	~τ-	183.9 ± 10.4	110.6 ± 6.7	165.1 ± 18.2	113.9 ± 9.6	192.2 ± 10.9	115.5 ± 4.3	179.2 ± 6.6	125.8 ± 9.6
	+ VRT 179.0 ± 13.5	112.1 ± 5.4	163.4 ± 17.0	112.8 ± 9.1	190.1 ± 10.3	112.6 ± 3.9	179.8 ± 15.1	119.7 ± 9.1
	rpeer	198.5 ± 2.3	86.2 ± 5.0	195.5 ± 9.1	85.3 ± 5.4	174.1 ± 32.5	88.8 ± 6.3	191.8 ± 8.5	106.9 ± 9.2
	+ VRT 184.1 ± 8.8	95.2 ± 8.8	193.9 ± 6.0	86.5 ± 3.9	174.2 ± 24.3	93.5 ± 6.2	181.2 ± 15.3	117.7 ± 9.5
We plot the entropy of the PPO agent during training on four environments from the BC task in
Figure A7, and we give the entropy value of the imperfect expert model and the optimal policy in
Table A3. We observe that except for Freeway, the entropy of expert policies is always larger than 1.
We calculate the mean value of the highest action probability over 1000 steps for the full-converged
PPO agents in Table A4, which again verifies that the true expert policy we aim to recover might not
27
Under review as a conference paper at ICLR 2021
Freeway
(a) Pong	(b) Boxing	(c) Enduro	(d) Freeway
Figure A7: The policy entropy of the PPO agent during training. The imperfect expert model is
trained for 0.2 × 107 timesteps as the red line indicates.
be fully deterministic. These results demonstrate the flexibility of our proposed approach in dealing
with both stochastic and deterministic clean expert policies in practice, although a deterministic clean
expert policy is assumed in our theoretical analysis.
Also, from Figure A7 and Table A3, we notice that the entropy of imperfect expert models are higher
than the fully converged PPO agents, implying that the expert models might contain an amount of
noise. That’s because there might be states on which the expert has not seen enough and the selected
actions contain much noise. This is consistent with our claim, that the benefits of PeerBC might come
from two aspects, both noise reduction of the imperfect expert and inducing a more deterministic
policy.
Timesteps (×107)	Pong	Boxing	Enduro	Freeway
0.2 (Imperfect Expert) 1.0 (Fully converged PPO)	1.201 1.250	1.949 1.168	1.637 1.126	0.318 0.171
Table A3: The policy entropy of the PPO agent during training.
Trained timesteps (× 107) ∣ Pong ∣ Boxing ∣ EndUro ∣ Freeway
1.0 (Fully converged PPO) ∣ 0.492 ∣ 0.579 ∣ 0.664 ∣ 0.903
Table A4: The mean valUe of the highest action probability over 1000 steps.
28