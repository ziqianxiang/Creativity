Under review as a conference paper at ICLR 2021
PLM: Partial Label Masking for Imbalanced
Multi-label Classification
Anonymous authors
Paper under double-blind review
Ab stract
Neural networks trained on real-world datasets with long-tailed label distributions
are biased towards frequent classes and perform poorly on infrequent classes.
The imbalance in the ratio of positive and negative samples for each class skews
network output probabilities further from ground-truth distributions. We propose a
method, Partial Label Masking (PLM), which utilizes this ratio during training. By
stochastically masking labels during loss computation, the method balances this
ratio for each class, leading to improved recall on minority classes and improved
precision on frequent classes. The ratio is estimated adaptively based on the
network’s performance by minimizing the KL divergence between predicted and
ground-truth distributions. Whereas most existing approaches addressing data
imbalance are mainly focused on single-label classification and do not generalize
well to the multi-label case, this work proposes a general approach to solve the
data imbalance issue for multi-label classification. PLM is versatile: it can be
applied to most objective functions and it can be used alongside other strategies
for class imbalance. Our method achieves strong performance when compared to
existing methods on both multi-label (MultiMNIST and MSCOCO) and single-
label (imbalanced CIFAR-10 and CIFAR-100) image classification datasets.
1	Introduction
The impressive performance of deep learning methods has led to the creation of many large-scale
datasets (Deng et al., 2009; Gu et al., 2018; Lin et al., 2014; Krasin et al., 2017). Due to the naturally
imbalanced distribution of objects within the world, these datasets contain imbalanced numbers of
samples for different classes. The class labels in these datasets form a long-tailed distribution: several
classes appear frequently (the head classes), while many classes contain few samples (the tail classes).
This imbalance causes classifiers to perform poorly, especially on classes which are infrequent in
training. Finding a solution to this problem is necessary to successfully scale deep networks to larger
real-world datasets which tend to have long-tail data distributions.
Previous works on data imbalance (Buda et al., 2018; Cui et al., 2019; Cao et al., 2019) tend to focus
on single-label classification datasets, but these approaches do not generalize well to the multi-label
case, where a single sample may have multiple class labels. Two common methods for learning
long-tailed distributions, reweighting and resampling, were not designed for data with multiple labels.
Our experiments show that reweighting based on the inverse number of samples performs poorly on
multi-label datasets; also, it is difficult to resample multi-label data due to the co-occurrence of labels
within individual samples. Therefore, in this work, we propose a general solution for long-tailed
imbalance which works for both multi-label and single-label classification.
Classifiers trained on imbalanced multi-label datasets tend to over-predict frequent classes and under-
predict minority classes. This behaviour is displayed in Figure 1. When the class is not present
within the image, the probability output for the frequent classes is skewed towards 1; conversely,
when an infrequent class is present within the sample, the classifier outputs a low probability score.
These output probability distributions differ greatly from the ideal distribution (i.e. the ground-truth
distribution where all positive samples are labeled 1 and all negative samples are labeled 0). We argue,
that this behaviour is caused not only by an imbalance in the number of positive samples between
different classes, but also by the ratio of positive and negative samples for each class.
1
Under review as a conference paper at ICLR 2021
As dataset imbalance increases, the ratio of pos-
itive samples to negative samples increases for
head classes and decreases for tail classes. We
find that the change in this ratio greatly impacts
the classifier’s ability to generalize. If a class
has large ratio of positive samples to negative
samples, the classifier over-predicts the given
class, leading to an increase of false positive
predictions; conversely, a small ratio leads to
under-prediction and an increase of false nega-
tives. Assuming there is an optimal ratio which
can minimize the over/under-predictions, an al-
gorithm can estimate and leverage this ratio to
improve network performance.
We present Partial Label Masking (PLM): a
novel approach for training classifiers on imbal-
anced multi-label datasets which improves net-
work generalization by leveraging this ratio. By
partially masking positive and negative labels
for frequent and infrequent classes respectively;
our method reduces the discrepancy between the
classifiers’ output probability distribution and
the ground-truth distribution (as seen in Figure
Figure 1: The output probability (yc) distributions
of a ResNet-32 classifier trained on artificially im-
balanced CIFAR-10 for positive (left) and negative
samples (right). For frequent classes, the predicted
distribution skews towards 1; for infrequent classes,
it skews towards 0. Classifiers trained using PLM
(bottom) reduces this bias, when compared to clas-
sifers trained with binary cross-entropy (top).
1). Our method performs this masking stochastically for each sample and it continually adapts
the target ratio based on the classifier’s output probabilities. This leads to improved precision on
classes with many samples and improved recall on classes with few samples. Moreover, our method
consistently improves performance on difficult classes, regardless of the number of samples.
Our contributions include: (i) we present a general solution for data imbalance which balances the
ratio between positive and negative samples, (ii) we propose an adaptive strategy to determine the ideal
ratio which minimizes the difference between predicted probability and ground-truth distributions,
(iii) we empirically evaluate our method on both multi-label datasets (imbalanced MultiMNIST and
MSCOCO) and single-label datasets (CIFAR10 and CIFAR100), and (iv) we thoroughly analyse our
method’s ability to improve classifiers’ performance on both difficult and infrequent classes.
2 Method
Notation We denote a dataset with N samples and C class categories as D = { x(i) , y(i) } where
x(i) is the ith input and y(i) = [y1(i) , ..., yC(i)] is its corresponding binary label vector; note that
multiple elements in this label may be non-zero in multi-label classification. The number of positive
samples for a class c is nc+ = |{i : i ∈ {1, ..., N}, where yc(i) = 1}|, and the number of negative
samples is n- = N - n+. The ratio of positive samples to negative samples for a class C is rc = n-.
nc
We denote the “optimal” ratio, which minimizes over/Under-PrediCtiOnS as rc.
2.1 Partial Label Masking
Given a network prediction, y(i), the total loss is sum of the losses over all C classes:
C
L(y(i), y(i) ) = X ` (yji) ,yji)).	(1)
j=1
This is a general form (e.g. ' (y, y) = — [y log (y) + (1 — y) log (1 — y)] for binary cross-entropy).
Partial Label Masking (PLM) masks the loss computed for certain classes. For each image i we
generate a binary mask g(i) = [g1(i), ..., gC(i)] that masks loss calculations for certain classes as follows:
C
L(y(i), y(i), g(i)) = X gj(i ` (yji) ,yji)).	(2)
j=1
2
Under review as a conference paper at ICLR 2021
Note that equation 2 is a generalized form of 1 when all masks equal one, i.e. gc(i) = 1. Therefore,
PLM can be used with any loss which sums over different classes.
Masks Generation With this formulation, one can generate masks so that frequent class samples
are ignored by the loss calculation and an equal number of samples for each class is used to train the
network, which is typically employed in under-sampling. The issue with under-sampling samples,
however, is that frequent and infrequent classes can co-occur within the same sample, so removing it
from training would not meaningfully change the imbalance. The benefit of the PLM formulation is
that individual positive, or negative, labels can be masked to ensure a specific number of labels for
each class is used to train the network regardless of co-occurrence.
Instead of under-sampling based on the number of positive samples, we consider the ratio of positive
to negative samples for each class, r「Assuming there is some ideal ratio,建,which minimizes
over/under-predictions, we can generate the masks, g(i), stochastically as a function of the label:
f 1 [rc] , rc > rc and yCi) = 1
rc
g(i) = f (yCi)) = λ 1 rc , rc < rc and yCi) = 0	⑶
(1,	otherwise
where 1 [p] = 1 with probability p, and 0 with probability 1 - p. The probabilities used,尸c/rc and
rc/尸c, ensure that the ratio between positive and negative samples on which loss is calculated is 屋.
With partial label masking, we are able to train a classifier with a set ratio between positives and
negatives for each class. Now the question arises: How does one determine this ideal ratio?
Ratio Selection The ratio between positives and negatives for each class should be set to minimize
over-predictions and under-predictions for the head and tail classes respectively. Since decreasing
the ratio reduces the number of positive samples used for loss calculation and increasing the ratio
reduces the number of negative samples, the ratio selected for the head classes should be decreased
(i.e. rc < rc). On the other hand, the ratio for the tail classes should be increased (尸C > rj.
One simple approach for setting the ratio would be to use the average ratio of all the classes. This
ensures a decrease in the ratio for head classes and increase for tail classes. Although, this ratio leads
to improved results on some datasets, it is sensitive to different levels of imbalance within the dataset
as well as the relative difficulty of the various classes. Ideally this hyper-parameter would change
based on the performance of the classifier on the dataset on-hand. To this end, we propose a method
to estimate the ratio adaptively based on the output probability distribution of the classifier.
Ratio Adaptation For a class c, the set of output probabilities for the positive and negative samples
are represented by Sc and S-, respectively. The ground-truth sets are Sc and S-, where members
of this set will be 1 for a positive sample and 0 for a negative sample. Formally, these sets can be
defined as:
Scc = {yc(i)|yc(i) = 1, ∀i},
Sc = {y(i)∣y(i) = 1, ∀i},
Sc- = {yc(i)|yc(i) = 0, ∀i},
S- = {y(i)∣y(i)=0, ∀i}.
A discrete distribution is formed using these sets by placing the probabilities in τ bins of width
1∕τ. For each set, we denote these distributions as P+, P+, PC , and PC respectively. When
data imbalance is present, the classifiers output probabilities skew further from the ground-truth
distributions. For head classes, the difference between P- and P- increases as the probability
outputs for negative samples become larger; conversely, for tail classes, probabilities are pushed
closer to 0 for positive samples, so the difference between Pc and P+ increases.
We utilize this difference to change the ratio until the output distributions better resemble the ground-
truth distribution. To compute the difference between the distributions, we use the Kullback-Leibler
divergence:
Dc = DKL (Pc ||Pc) ； and D- = DKL (P) ||P—) .	(5)
3
Under review as a conference paper at ICLR 2021
Algorithm 1 PLM training algorithm. Inputs are a network Fθ and the dataset D = { x(i), y(i) }iN=1.
1	Initialize fc1
2	: for t = 1 to T epochs do
3	:	Generate masks gc(i) = f yc(i) , using rfc,t	. Equation 3
4	:	repeat
5	B — SampleMiniBatch (D)
6	^ J Fθ (B)	. Perform forward pass on mini-batch
7	Fθ J Fθ - αVθL (y, y, g)	. One SGD step with loss in equation 2
8	:	until all data has been sampled
9	Generate P+ ,P; ,P+, P- from all y(i) and y(i)
10	:	Compute Dc = Dc+ - Dc- using equations 5 and 6
11	:	rfc,t+1 J eλDc rfc,t	. Update ratio
We normalize the divergence
D + = D+ - μ+ ∙ and D - = D- - μ
Dc = σ+	； and Dc = σ-

(6)
with their means (μ+ and μ-) and standard deviations (σ+ and σ-). After normalization, D + tends
to be positive when the network under-predicts class c and negative when the network over-predicts.
The inverse holds for Dc-: it is positive when the network over-predicts, and negative otherwise.
The ratio should be adjusted during training until the divergence scores in equation 6 become balanced
(D+ = D-). At each epoch t, the ratio fct becomes
屋,t = eλDc rc,t-i,	(7)
where 尸c,ι is the initial ratio (e.g. the dataset,s ratio, r) DC = D + - D-, and λ is a hyper-parameter
which controls the rate of change of the ratio. The exponential term in equation 7 increases, or
decreases, the current ratio for each class based on the divergence between ground-truth and output
probability distributions: if Dc > 0, which tends to occur for infrequent classes, then the ratio
increases; if Dc < 0, which occurs for frequent classes, then the ratio decreases. If Dc = 0 (i.e.
Dc+ = Dc-), then the ratio remains unchanged as some optimal ratio has been reached. The training
procedure used for partial label masking is described in Algorithm 1.
3	Experimental Evaluation
We evaluate our proposed method on several image classification benchmarks with varying levels of
imbalance. Imbalance, ρ, is denoted as the ratio between the number of samples for the most frequent
class and the least frequent class: ρ = maxi{ni+}/ mini{ni+}.
3.1	Multi-label Classification
Datasets We evaluate our method on two multi-label datasets: MultiMNIST and the large-scale
real-world multi-label dataset MSCOCO (Lin et al., 2014). We construct the imbalanced MultiMNIST
dataset by superimposing two MNIST (LeCun et al., 1998) digits into a single image. We sample the
MNIST training set to obtain a long-tail distribution over the different digits. Due to coocurrence
of the same digit within a sample, the final imbalance of the dataset is ρ = 90.33. The test set
consists of 90000 samples; it is roughly balanced, with all classes being present a similar number of
samples1. MSCOCO is a multi-label image classification benchmark, which contain a large amount
of imbalance (ρ = 352.92). We use the standard train/test split which consists of 82,783 training and
40,504 evaluation images. It contains 80 classes, with an average of 2.9 labels per sample.
Metrics We evaluate classifier performance using multiple standard multi-label classification
metrics: per-class precision, per-class recall, F1-score, and 0-1 exact match accuracy. To evaluate
performance on tail classes, we measure some metrics averaged over the K most infrequent classes.
A description of these metrics is in Section A.1 of the Appendix.
1Additional details of the dataset’s construction and the dataset statistics are in section A.2 of the Appendix.
4
Under review as a conference paper at ICLR 2021
Loss	Und.	Precision Avg.	Avg.	Recall K=5	K=3	Avg.	F1 Score K=5	K=3	0-1 Acc.
^CE	X	-8696-	78.65	63.39	51.50	80.35	73.87	65.73	53.42-
Focal	X	86.75	78.28	62.53	50.44	79.93	73.33	64.88	52.64
BCE+CB	X	85.86	76.55	61.02	49.12	78.05	72.32	63.81	48.51
BCE	✓	87.97	79.00	65.36	54.16	81.39	75.57	67.92	54.46
PLM (BCE)	X	-8741-	81.55	71.91	65.31	83.73	79.17	74.27	56.95-
PLM (Focal)	X	87.50	80.95	70.27	62.31	83.22	78.13	72.61	56.45
PLM (BCE+CB)	X	89.37	75.24	65.31	56.25	80.52	75.64	69.28	49.95
PLM (BCE)	✓	88.54	80.38	71.29	65.86	83.56	79.44	75.10	56.53
Table 1: Results for multi-label image classification on MultiMNIST. “Und.” denotes undersampling
and “+CB” denotes class-based reweighting (Cui et al., 2019).
Loss	Und.	Precision Avg.	Avg.	Recall K=20	K=10	Avg.	F1 Score K=20	K=10	0-1 Acc.
BCE	X	-69.35-	39.89	30.02	16.00	48.74	37.94	21.80	21.76-
Focal	X	68.01	38.62	28.92	15.10	47.38	36.52	20.46	20.88
BCE+CB	X	67.82	37.23	35.25	23.68	46.10	43.64	30.53	19.43
BCE	✓	65.96	37.75	35.18	19.50	46.06	41.59	24.16	18.60
PLM (BCE)	X	-6775-	41.29	39.69	29.91	50.16	44.24	31.26	-20.51-
PLM (Focal)	X	66.89	39.48	37.63	27.67	48.49	42.38	29.56	19.66
PLM (BCE+CB)	X	65.71	37.06	39.06	29.25	46.39	45.68	33.18	17.41
PLM (BCE)	/	63.72	36.23	40.21	31.46	44.85	44.49	32.72	16.71
Table 2: Results for multi-label image classification on MSCOCO. “Und.”, denotes undersampling
and “CB” denotes class-based reweighting (Cui et al., 2019).
Baselines Since recent class imbalance works tend to focus on single-label classification, we
compare our method with those approaches which can be applied to multi-label classification. We
use the following baselines: i) binary cross-entropy, ii) focal loss (Lin et al., 2017), iii) reweighting
by inverse number of classes (CB), and iv) undersampling. For reweighting, we use the formulation:
C
L(y(i),y(i)) = XWj'(yji),yji)),	(8)
j=1
where wj is the inverse number of samples for class j, as calculated in (Cui et al., 2019). Undersam-
pling that removes all imbalance is a non-trivial operation when multiple labels can are present in
each sample. Therefore, for each epoch, we select S samples from each class (without duplication) to
reduce the imbalance seen by the network (S=500 for MultiMNIST and S=1000 for MSCOCO).
Implementation For experiments on MultiMNIST, we train a ResNet-12 (He et al., 2016) model
using stochastic gradient descent (SGD) with momentum of 0.9 for 90 epochs. The initial learning
rate is set to 0.1 which is decayed by a factor of 0.1 at epochs 60 and 80. We employ a linear learning
rate warm-up (Goyal et al., 2017) for the first 5 epochs. The network is model with a batch size
of 128. On MSCOCO, a ResNet-50 model is trained with a similar training procedure; however,
the initial learning rate is set to 0.4 which is decayed epochs 30, 60, and 80, and the batch size is
increased to 200. For both datasets, We initialize the ratio with the dataset's ratio (i.e.屋/=建)and
set λ to 0.01. Since the imbalance in MSCOCO is large, we clip the ratios within the range [0, 1].
Results on MultiMNIST We present results for MultiMNIST in Table 1. Undersampling improves
results on MultiMNIST across all metrics, but the use of reweighting (CB) leads to a decrease in
performance since the technique is formulated for the single-label case. The use of PLM during
training improves results across all metrics. The F1-score improves by 2.34% and the exact match
accuracy improves by 2.49% over the next best baseline. Notably, PLM greatly improves performance
on the minority classes - for the three most infrequent digits, the recall increases by 11.7% and the
F1-score increases by 7.18%.
Results on MSCOCO Results for MSCOCO are presented in Table 2. We find conventional
methods designed for single-label data imbalance (focal loss, class-based reweighting, and under-
sampling) tend to underperform standard binary cross-entropy (BCE) training. Although reweighting
5
Under review as a conference paper at ICLR 2021
Dataset		CIFAR-10						CIFAR-100				
Imbalance	100		10 Overall	K=5	100		10	
	Overall	K=5			Overall	K=5	Overall	K=5
BCE	29.43^^	46.46	12.71 ^^	15.98	59.88^^	79.18	43.35^^	52.12
Focal	29.69	46.88	12.63	16.74	60.04	81.62	43.04	53.74
CB	25.43	-	12.51	-	60.40	-	42.01	-
LDAM-DRW	22.97	-	11.84	-	57.96	-	41.29	-
PLM (BCE)	24.86^^	36.05	12.35^^	14.98	55.98^^	73.14	41.71 ^^	49.78
PLM (Focal)	25.19	36.53	12.20	14.60	56.19	72.39	41.54	48.83
Table 3: Error rates for single-label image classification on imbalanced CIFAR-10 and CIFAR-100.
We measure the average error across all classes (Overall) as well as the error on the K classes with
the fewest number of samples (K=5 and K=50 for CIFAR-10 and CIFAR-100 respectively).
p = 20	p = 100	p = 200
Class	Class	Class
Figure 2: The test accuracy for each class on CIFAR-10 with varying levels of imbalance. PLM leads
to improved generalization on minority classes, especially when there is a large amount of imbalance.
and under-sampling lead to improvements on tail classes, they dramatically reduce performance on
head classes leading to an overall performance decrease. PLM, on the other hand, achieves an overall
improvement on recall (1.4%) and f1-score (1.42%). There is a decrease in precision due to the
drastic improvement of recall on minority classes - this is further discussed in section 3.3. Whereas
PLM leads to a 0-1 accuracy improvement on MultiMNIST, there is a decrease on MSCOCO due to
the imbalance of the MSCOCO test set and the bias of this metric to frequently occurring classes.
Overall, PLM greatly improves performance on tail classes: when compared to the BCE baseline
the 10 most infrequent classes have a 13.91% increase in recall and a 9.46% increase in f1-score.
Moreover, these experiments highlight the versatility of our method: PLM can be used on top of
existing data imbalance methods as well as different losses.
3.2	Single-label Classification
Datasets CIFAR-10 and CIFAR-100 (Krizhevsky et al., 2009) contain 32 × 32 images with 10 and
100 classes respectively. To artificially create imbalance, we reduce the training examples for certain
classes, while keeping the 10,000 validation images unchanged. Following the experimental setup in
(Cui et al., 2019), we evaluate on varying levels of long-tailed imbalance, ranging from 10 to 200.
Baselines We compare our method with the following baselines: i) binary-cross entropy (BCE), ii)
focal loss, iii) reweighting (CB) (Cui et al., 2019), and iv) LDAM-DRW (Cao et al., 2019). The later
two are recent methods for dealing with imbalanced single-label datasets.
Implementation For the CIFAR experiments, we train a ResNet-32 following the training pro-
cedure described in (Cui et al., 2019). Again, we initialize the ratio with the dataset’s ratio. The
hyper-parameter λ is set to 1.0 and 0.01 for the CIFAR-10 and CIFAR-100 experiments respectively.
These values were selected empirically - we analyse the effect of this hyper-parameter in section 3.3.
Results Table 3 contains PLM’s results on CIFAR-10 and CIFAR-100, respecively, for imbalance
factors2 ρ = 10 and ρ = 100. We achieve improvement when compared to the common methods for
data imbalance - focal loss and reweighting based on the inverse effective number of samples (CB).
Furthermore, we achieve comparable accuracy when compared to the recent LDAM-DRW approach.
2We present results on other imbalance factors ρ = 20, ρ = 50, and ρ = 200 in section A.3 of the Appendix.
6
Under review as a conference paper at ICLR 2021
MSCOCO Fl Score
一①一-p」一eq
豆捌0乜
一端IU bu->μed
s」ossus
qsn」q£001
JOP4->0q
ɔuop
① >eΛλo.l!=!UJ
d①①IlS
P-JQOqMOUS
Figure 3: The class-wise F1 score on the MSCOCO dataset. The classes are ordered based on the
number of samples in the training set. The bars represent the results achieved by using PLM, and
the points are the results with BCE. Not only does PLM improves results on the 15 most infrequent
classes, but also it improves scores on difficult classes, like backpack, bench, and cell phone.
3.3	Additional Analysis
Generalization on minority classes To perform well on long-tailed datasets, classifiers must
generalize well on classes which have few training samples. All of our experiments show that PLM
greatly improves performance on minority classes. Figure 2 displays the class-level accuracy on
the CIFAR-10 dataset for various levels of imbalance; as the imbalance factor increases, there is a
corresponding relative improvement on tail classes. Similarly, the MSCOCO experiments show an
improvement on all 15 of the least frequent classes in terms of F1-score, as seen in Figure 3.
Effects on Precision and Recall In imbalanced datasets, classifiers tend to have poor precision on
frequent classes and poor recall on infrequent classes. This is due, in part, to the large difference in the
ratio of positive to negative samples. By minimizing the KL divergence between the ground-truth and
predicted output distributions, PLM effectively balances this ratio, leading to improved performance.
We present per-class precision and recall graphs in section A.3 of the Appendix. In summary, on
MSCOCO, we find that our classifier’s precision over the 5 most frequent classes improves by
12.73% whereas its recall over the 5 least frequent classes improves by 13.04%. In general, PLM
leads to improved recall over most classes, which suggests that our method would be best suited for
applications which prioritize true positives over false positives.
Ratio Adaptation Since our approach attempts to estimate an optimal ratio for training a network,
we investigate how the ratio changes throughout training. We present the effect of ratio’s rate of
change (λ) on the CIFAR-10 and CIFAR-100 datasets in Table 3.3. We find that all tested values lead
to an improvement over no change (i.e. using standard BCE). On CIFAR-10, a higher rate of change
(λ = 1.0) leads to best performance. Meanwhile, a lower rate of change (λ = 0.01) leads to best
results on CIFAR-100 (this is also the case on MultiMNIST and MSCOCO). Figure 4 depicts how the
ratio changes throughout training on the imbalanced MultiMNIST dataset. The ratio of head classes
(digits 0, 1, and 2) decrease, while the ratio of the tail classes (digits 7, 8, and 9) increase. Over
the 90 epochs, the ratio of the most frequent class (digit 0) decreases from 1.82 to 0.24 (decreasing
by a factor of 7.58), and the ratio of the least frequent class (digit 9) increases from 0.007 to 0.047
(increasing by a factor of 6.54). This demonstrates that the original ratios (i.e. the dataset’s ratios) are
not optimal for performance; by adapting the ratio, PLM greatly improves the training process.
Improvements on difficult classes Another benefit of using PLM during training is the improve-
ments observed on difficult classes. This behaviour can be seen in Figure 3, with categories backpack,
bench, and cell phone. These categories contain many samples, but achieve less than 20% F1-score
when trained with BCE; when the network is trained with PLM, each of these categories improve by
an average of 11.16%. This improvement is caused by the increasing of the ratio fc during training,
which leads to masking more negative labels for these classes and an increase in recall.
7
Under review as a conference paper at ICLR 2021
Figure 4: The change of the ratio over time on im-
balanced MultiMNIST. The ratio for tail classes
tend to increase while the ratio for head classes
tend to decrease.
λ	CIFAR-10	CIFAR-100
1.0	24.86	56.92
0.1	25.08	57.67
0.01	25.64	55.98
0.0 (BCE)	29.43	59.88
Figure 5: Error rates on CIFAR-10 and CIFAR-
100 (both with ρ = 100). This ablation measures
the effect of λ. For these experiments, the initial
ratio is the dataset's ratio rc,1 = rc.
4 Related Work
Many works have studied the problem of long-tailed data imbalance. Extensive overviews of the
problem can be found for both classical methods (Japkowicz & Stephen, 2002; He & Garcia, 2009)
and convolutional neural networks (Buda et al., 2018).
Resampling Resampling techniques either over-sample the infrequent classes (Chawla et al.,
2002; Han et al., 2005; He et al., 2008; Mullick et al., 2019), or under-sample the frequent classes
(Drummond et al., 2003; Liu et al., 2008). By resampling, the amount of imbalance seen by the
network during training is greatly reduced. One benefit of PLM over conventional resampling is
that it can be successfully applied to the multilabel case, where the co-occurrence of labels within
individual samples causes issues with traditional resampling approaches. If tail and head classes
co-occur frequently within the same inputs, then resampling would not result in a meaningful change
of the imbalance. However, PLM masks individual labels, which allows for the head class’ label to be
masked (i.e. removed from training) while keeping the tail class, resulting in more balanced training.
Reweighting Reweighting methods apply weights to certain classes, or samples, during the ob-
jective calculation. The standard approach is to apply class weights based on the inverse number
of samples (Huang et al., 2016; Wang et al., 2017; Khan et al., 2019; Huang et al., 2019), or the
square-root of the inverse number of samples (Mikolov et al., 2013). Cui et al. (2019) proposed class
weights based on the effective number of samples, which improved results on more frequent classes.
The focal loss (Lin et al., 2017) is another popular technique, which reweighs the loss based on the
classifiers’ output probabilities. Our method can be viewed as a type of loss reweighting where the
weights are binary and generated for each sample stochastically. Furthermore, PLM is versatile and
can be applied on top of existing reweighting and resampling approaches.
Partial Label Learning Our approach takes inspiration from partial label learning, in which
learning is performed on data with incomplete, or missing, labels (Bucak et al., 2011; Sun et al.,
2010). Recently, (Durand et al., 2019) proposed an effective method for training a convolutional
neural network on partial labels. Whereas the goal of partial label learning is to learn from sparsely
annotated samples, our work deals with learning completely annotated imbalanced datasets; our
method assumes all labels are complete, and we partially mask some labels to reduce the effect of
data imbalance on network training.
5	Conclusion
In this work, we propose a general approach to the long-tail data imbalance problem. Our partial
label masking algorithm leverages the ratio of positive and negative samples for each class to greatly
improve performance on both infrequent and difficult classes. The method can be used alongside
most existing methods for data imbalance and is designed for both single-label and multi-label
classification. We evaluate PLM on multiple datasets and perform extensive analysis to verify its
effectiveness.
8
Under review as a conference paper at ICLR 2021
References
Serhat Selcuk Bucak, Rong Jin, and Anil K Jain. Multi-label learning with incomplete class
assignments. In CVPR 2011 ,pp. 2801-2808. IEEE, 2011.
Mateusz Buda, Atsuto Maki, and Maciej A Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. Neural Networks, 106:249-259, 2018.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems, pp. 1565-1576, 2019.
Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote: synthetic
minority over-sampling technique. Journal of artificial intelligence research, 16:321-357, 2002.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on
effective number of samples. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 9268-9277, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. Ieee, 2009.
Chris Drummond, Robert C Holte, et al. C4. 5, class imbalance, and cost sensitivity: why under-
sampling beats over-sampling. In Workshop on learning from imbalanced datasets II, volume 11,
pp. 1-8. Citeseer, 2003.
Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning a deep convnet for multi-label classifi-
cation with partial labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 647-657, 2019.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkaSz Wesolowski, AaPo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Chunhui Gu, Chen Sun, David A Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra
Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, et al. Ava: A video dataset
of spatio-temporally localized atomic visual actions. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 6047-6056, 2018.
Hui Han, Wen-Yuan Wang, and Bing-Huan Mao. Borderline-smote: a new over-sampling method in
imbalanced data sets learning. In International conference on intelligent computing, pp. 878-887.
Springer, 2005.
Haibo He and Edwardo A Garcia. Learning from imbalanced data. IEEE Transactions on knowledge
and data engineering, 21(9):1263-1284, 2009.
Haibo He, Yang Bai, Edwardo A Garcia, and Shutao Li. Adasyn: Adaptive synthetic sampling
approach for imbalanced learning. In 2008 IEEE international joint conference on neural networks
(IEEE world congress on computational intelligence), pp. 1322-1328. IEEE, 2008.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deep representation for
imbalanced classification. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 5375-5384, 2016.
Chen Huang, Yining Li, Change Loy Chen, and Xiaoou Tang. Deep imbalanced learning for
face recognition and attribute prediction. IEEE transactions on pattern analysis and machine
intelligence, 2019.
9
Under review as a conference paper at ICLR 2021
Nathalie Japkowicz and Shaju Stephen. The class imbalance problem: A systematic study. Intelligent
data analysis, 6(5):429-449, 2002.
Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right
balance with uncertainty. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pp. 103-112, 2019.
Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova,
Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, Serge Belongie, Victor Gomes, Abhinav
Gupta, Chen Sun, Gal Chechik, David Cai, Zheyun Feng, Dhyanesh Narayanan, and Kevin Murphy.
Openimages: A public dataset for large-scale multi-label and multi-class image classification.
Dataset available from https://github.com/openimages, 2017.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Yann LeCun, Corinna Cortes, and Chris Burges. Mnist handwritten digit database, 1998. 1998.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C LaWrence Zitnick. Microsoft coco: Common objects in context. In European
conference on computer vision, pp. 740-755. Springer, 2014.
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object
detection. In Proceedings of the IEEE international conference on computer vision, pp. 2980-2988,
2017.
Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. Exploratory undersampling for class-imbalance
learning. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 39(2):
539-550, 2008.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed representations
of Words and phrases and their compositionality. In Advances in neural information processing
systems, pp. 3111-3119, 2013.
Sankha Subhra Mullick, Shounak Datta, and SWagatam Das. Generative adversarial minority
oversampling. In Proceedings of the IEEE International Conference on Computer Vision, pp.
1695-1704, 2019.
Yu-Yin Sun, Yin Zhang, and Zhi-Hua Zhou. Multi-label learning With Weak label. In Twenty-fourth
AAAI conference on artificial intelligence, 2010.
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In Advances in
Neural Information Processing Systems, pp. 7029-7039, 2017.
A Appendix
In the appendix We include a description of different multi-label evaluation metrics used (section
A.1) and additional details on the imbalanced MultiMNIST dataset (section A.2). Furthermore, We
include additional results on the MSCOCO and CIFAR datasets as Well as an ablation study on the
initialization of the ratio (section A.3).
A.1 Metrics
In this section, We explain the metrics used in our multi-label classification experiments. In an evalua-
tion set, there are N images With corresponding labels {y(1), ..., y(N)} Where y(i) = [y1(i), ..., yC(i)]
is the ith binary label vector; note that multiple elements in this label vector may be non-zero in
multi-label classification. For each sample, the classifier predicts class probabilities to Which a
threshold (0.5 in our experiments) is applied to obtain binary prediction vectors {^(1),..., y(N)}.
10
Under review as a conference paper at ICLR 2021
Per-class Precision and Recall
by
The precision and recall of a classifier on a single class c is given
P (C) P31 M)= y(1
()= PN=IIhy(i) = ιi ,
R (c) =
PN=Il
(9)
PN=IlMi) = 1 ,
respectively, where 1 [...] is the indicator function. To obtain the reported metric, We average the
precision (or recall) over all classes:
Precision
1C
⅛ X P (C),
c=1
1C
Recall = — E R (C).
c=1
(10)
In these metrics, each class is treated equally regardless of the number of samples in the test set.
F1-score The F1-score is the harmonic mean between precision and recall:
F1 (C)
2P (c) R (c)
P (c)+ R (C)
(11)
The final F1-score is averaged over all classes:
1C
FI-SCore = C EFI (c).
c=1
(12)
0-1 Accuracy This metric measures how often the network is able to output predictions which
exactly match the ground-truth labels. It is computed as follows:
1N
0-1 Accuracy = N X 1 卜⑴=y(i)] .	(13)
i=1
Since this metric is averaged over all samples, as opposed to all classes, it tends to be biased towards
more frequent classes. Therefore, this metric tends to be a poor measure of model performance if
there is imbalance present in the test set.
A.2 Imbalanced MultiMNIST Dataset
Dataset We construct the Imbalanced MultiMNIST dataset by superimposing two MNIST LeCun
et al. (1998) digits into a single image. We begin by sampling the MNIST training set similarly to
how Cui et al. (2019) create the Imbalanced CIFAR dataset with an imbalance of ρ = 100. These
images are padded to become 32 × 32 and shifted randomly by -6 to 6 pixels, both vertically and
horizontally. Then, each image is randomly paired with another digit in the training set, which is also
padded and shifted randomly. These two digits are superimposed upon each other to create a 32 × 32
image with two digits. Due to cooccurrence of the same digit, within a sample, the final imbalance of
the dataset becomes 90.33.
For evaluation, we select all samples in the MNIST test set and perform the same padding and shifting
operations. For each digit, we select other random test digits that have different labels, which are
superimposed to make 9 samples with two digits each. This results in 90000 test samples. The test
set is roughly balanced, with all classes having a similar number of samples. The number of samples
for each class can be found in Table 4.
11
Under review as a conference paper at ICLR 2021
Digit	Train	Test
0	9485	17840
1	6167	19080
2	3967	18256
3	2476	18080
4	1508	17856
5	875	17136
6	538	17664
7	331	18224
8	164	17792
9	105	18072
Total	14694	90000
Table 4: The number of samples for each digit in the Imbalanced MultiMNIST dataset.
MSCOCO Precision
MSCOCO Recall
i⅛鲤
他E 6u->μed
1SSUS
」q£001
1Ou
OP
^0^u-E
WS ∙
P-JQOqMOUS
6u->μed
Vos
① >eΛλo-l!=!UJ
də 0IlS
p-Jco0qM0us
Figure 6: The class-wise precision and recall scores on the MSCOCO dataset. The classes are ordered
based on the number of samples in the training set. The bars represent the results achieved by using
the PLM method, and the points are the results when using BCE.
A.3 Additional Results
Additional MSCOCO Results We include more results on the MSCOCO dataset. Figure 6
contains the class-wise precision and recall scores on the MSCOCO dataset. We find that using PLM
leads to an large improvement in recall for the tail classes, and a slight improvement in precision
for the head classes (7.86% improvement on the 10 most frequent classes). This is in line with the
observation that PLM reduces over-prediction on the head classes while reducing under-prediction on
the tail classes (as well as some difficult classes).
Additional CIFAR Results In Tables 5 and 6 we present results on CIFAR-10 and CIFAR-100 for
the imbalance ratios ρ = 20, 50, 200. Once again, PLM leads to an improvement over CB for all
12
Under review as a conference paper at ICLR 2021
Imbalance	200		50		20	
	Overall	K=5	Overall	K=5	Overall	K=5
^CE	34.88	55.84	23.34^^	35.52	16.37^^	22.54
Focal	36.28	58.80	23.38	35.22	16.53	23.00
CB	31.11	-	20.73	-	15.64	-
LDAM-DRW	-	-	-	-	-	-
PLM (BCE)-	29.67	44.65	20.50^^	28.66	15.21 ^^	19.92
PLM (FoCal)	29.79	44.54	20.19	27.94	15.21	19.60
Table 5: Error rates for single-label image classification on imbalanced CIFAR-10. We measure the
average error across all classes (Overall) as well as the error on the 5 classes with the fewest number
of samples (K=5).
Imbalance	200		50		20	
	Overall	K=50	Overall	K=50	Overall	K=50
BCE	63.92^^	86.10	55.25^^	72.84	48.66	59.98
Focal	64.67	87.84	55.71	74.60	48.62	63.86
CB	63.77	-	54.68	-	47.41	-
LDAM-DRW	-	-	-	-	-	-
PLM (BCE)	61.37^^	80.79	52.49^^	67.07	46.15	57.26
PLM (Focal)	61.68	79.43	52.09	65.39	46.11	56.28
Table 6: Error Rates for single-label image classification on imbalanced CIFAR-100. We measure the
average error across all classes (Overall) as well as the error on the 50 classes with the fewest number
of samples (K=50).
Initial Ratio	CIFAR-10	CIFAR-100
min{rc}	-28.82-	-73.46
max{rc}	30.74	56.73
mean{rc}	25.65	56.08
rc	24.86	55.98
Table 7: Ablations on CIFAR-10 and CIFAR-100 (both with ρ = 100) measuring the effect of the
initial ratio. λ = 1 and λ = 0.01 for CIFAR-10 and CIFAR-100 respectively.
levels of imbalance. One interesting result is that focal loss (with and without PLM) tends to perform
best with smaller levels of imbalance (ρ ≤ 50) and underperforms when compared to BCE for larger
levels of imbalance. This is consistent with our multi-label experiments, which are on datasets with
relatively high levels of imbalance.
Ablation on the initialization of the ratio We analyse the importance of the initial ratio, rc1,
for our algorithm. We run ablations on CIFAR-10 and CIFAR-100 (ρ = 100), where we initialize
the ratio with various values. The results for this ablation can be found in Table A.3. We find that
initializing the ratio with the dataset’s ratio (rc) or the mean of the dataset’s ratio (mean{rc}) leads to
the lowest error. We find that initializing with the maximum or minimum ratio of the dataset tends
to lead to poor results. Since the datasets are heavily imbalanced, initializing with either of these
ratios leads to too many masked labels which adversely impacts the network’s training. We find
that initializing the ratio with the dataset’s ratio is necessary to achieve good performance on the
large-scale dataset MSCOCO. This may be the result of the larger amount of imbalance in MSCOCO
or the presence of very difficult classes which have many samples (i.e. a difficult class with many
positive samples may require a larger ratio than an easy class with fewer samples).
13