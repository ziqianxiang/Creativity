Under review as a conference paper at ICLR 2021
One-class classification robust to geometric
TRANSFORMATIONS
Anonymous authors
Paper under double-blind review
Ab stract
Recent studies on one-class classification have achieved a remarkable perfor-
mance, by employing the self-supervised classifier that predicts the geometric
transformation applied to in-class images. However, they cannot identify in-
class images at all when the input images are geometrically-transformed (e.g.,
rotated images), because their classification-based in-class scores assume that in-
put images always have a fixed viewpoint, as similar to the images used for train-
ing. Pointing out that humans can easily recognize such transformed images as
the same class, in this work, we aim to propose a one-class classifier robust to
geometrically-transformed inputs, named as GROC. To this end, we introduce a
conformity score which indicates how strongly an input image agrees with one
of the predefined in-class transformations, then utilize the conformity score with
our proposed agreement measures for one-class classification. Our extensive ex-
periments demonstrate that GROC is able to accurately distinguish in-class im-
ages from out-of-class images regardless of whether the inputs are geometrically-
transformed or not, whereas the existing methods fail.
1	Introduction
One-class classification refers to the problem of identifying whether an input example belongs to a
single target class (in-class) or any of novel classes (out-of-class). The main challenge of this task
is that only in-class examples are available at training time. Thus, by using only positive examples,
a model has to learn the decision boundary that distinguishes in-class examples from out-of-class
examples, whose distribution is assumed to be unknown in practice. Early work on one-class clas-
Sification mainly utilized kernel-based methods (ScholkoPf et al., 2000; Tax & Duin, 2004) to find
a hypersphere (or hyperplane) enclosing all training in-class examples, or density estimation tech-
niques (Parzen, 1962) to measure the likelihood of an inPut examPle.
In the era of deeP learning, numerous literature have tried to emPloy deeP neural networks to ef-
fectively learn the high-dimensional data (e.g., images). Most of them aim to detect out-of-class
examPles based on density estimation, by adoPting the architecture of autoencoders (Ruff et al.,
2018; Zong et al., 2018) or generative adversarial networks (GANs) (Schlegl et al., 2017; Zenati
et al., 2018). Nevertheless, their suPervision is not useful enough to caPture the semantic of high-
dimensional data for a target class, which eventually leads to the limited Performance. Recently,
there have been several attemPts to make use of self-suPervised learning (Golan & El-Yaniv, 2018;
Hendrycks et al., 2019; Bergman & Hoshen, 2020) for more informative suPervision on the target
class, and made a major breakthrough to this Problem. They build a self-labeled image set by aP-
Plying a bunch of geometric transformations to training images, then train a classifier to accurately
Predict the transformation aPPlied to original inPut images. This aPProach achieved the state-of-the-
art Performance for one-class classification even without modeling the latent distribution of in-class
examPles for density estimation.
However, all the aforementioned methods are quite vulnerable to sPatial variances within the im-
ages, because they were develoPed based on the assumPtion that in-class (and out-of-class) images
have a fixed viewPoint. In Particular, the existing self-suPervised methods do not work comPletely
for the inPuts with various viewPoints in that their caPability of Predicting the geometric transfor-
mation relies on the fixed viewPoint. Note that humans usually recognize that the images of a target
object with different viewPoints belong to the same class; in this sense, the one-class classifiers also
1
Under review as a conference paper at ICLR 2021
should be robust to the viewpoint of input images. In other words, we need to make geometrically-
transformed in-class images not to be identified as out-of-class, from the perspective that a geometric
transformation (e.g., rotation & x,y-translation) does not change the semantic (i.e., object class) but
the viewpoint.
The goal of our work is to propose an effective strategy that can circumvent the limitation of view-
point sensitivity, without compromising the performance for the images with the fixed viewpoint.
We first present several evaluation settings for validating the robustness to flexible viewpoints, arti-
ficially introduced by geometric transformations. Then, we describe our proposed solution, termed
as GROC, which measures a conformity score indicating how confidently an input image matches
with one of the predefined (anchor) in-class transformations. In this work, we offer two measures
for the conformity score, which are the inner product similarity and the conditional likelihood, and
show how they can be optimized by the training in-class images. The empirical experiments on the
proposed evaluation scenarios show that GROC considerably outperforms all the other competing
methods in terms of the robustness to geometric transformation.
2	Preliminaries
2.1	Problem Formulation
Let X be a set of all kinds of images, Xin ⊆ X and Xout = X \Xin be the sets of all in-class and
out-of-class images, respectively. Given training in-class data Xitnr ⊆ Xin, we consider the one-class
classification problem which differentiates in-class and out-of-class data. The problem aims to build
a classifier by using only the known in-class data for training. The classifier learns an in-class score
function, Sin(x) : X → R, where a higher score indicates that the input x is more likely to be in
Xin. Based on the score, the classifier determines whether the input belongs to in-class or not.
2.2	Self-supervised Learning Methods for One-class Classification
Recently, the self-supervised learning methods (Golan & El-Yaniv, 2018; Hendrycks et al., 2019;
Bergman & Hoshen, 2020) have achieved the state-of-the-art performance in one-class classification.
For self-supervised learning, they first create a self-labeled dataset and use it to train a multi-class
classifier. Concretely, let T = {T0,…，Ti,…，TK-1} be a set of predefined (anchor) geometric
transformations, where T0 (x) = x is the identity mapping and each transformation Ti is a com-
position of multiple unit transformations (i.e., rotation & x,y-translation). The self-labeled dataset
consists of transformed images and their corresponding labels.
Dself = {(Ti(x),i)∣x ∈ Xitr, 0 ≤ i<K},	(1)
where Ti(∙) is the i-th transformation operator and its label i is the transformation id of Ti(∙).
Using the self-labeled dataset, these methods train a softmax classifier based on a multi-class clas-
sification loss (i.e., cross-entropy) for a discrimination among the transformations. For one-class
classification, they define an in-class score under the assumption that a well-trained classifier would
better predict the transformation for the in-class images than that for the out-of-class images. In the
end, the in-class score for an unseen image x is defined by the sum of softmax probabilities that
its transformed images are correctly classified as their labels (Golan & El-Yaniv, 2018; Bergman &
Hoshen, 2020).
K-1
Sin(x) = X p (y = i|Ti(x)) ,	(2)
i=0
where p (y = i|Ti(x)) is the softmax probability that Ti(x) is classified as the i-th transformation.
The state-of-the-art method based on this self-supervised approach (Hendrycks et al., 2019) sig-
nificantly improves the performance by formulating the classification task in a multi-label manner.
Since each transformation is determined by the combination of unit transformations from three cat-
egories1, (i.e., rotation, (horizontal) x-translation, and (vertical) y-translation), the unit transforma-
tions applied to an input image can be independently predicted for each category. Thus, they adopt a
1They build the set of transformations T by the combination of the following unit transformations: rotation
∈ {0°, 90°, 180°, 270° }, x-translation ∈ { — 8, 0, +8}, and y-translation ∈ { — 8, 0, +8}.
2
Under review as a conference paper at ICLR 2021
Softmax head for each transformation category, then train a classifier to predict the degree of trans-
formations within each category. The final in-class score is also replaced with the one summarizing
all the softmax heads, each of which is for the unit transformation applied to the input.
3	Method
3.1	Motivation
The underlying concept of the self-supervised methods based on transformation classification is to
learn discriminative features of in-class images, in order to classify various viewpoints caused by
the geometric transformations. The precondition for this approach is that the viewpoint of training
images is always the same, otherwise the classifier cannot be trained due to the inconsistent super-
vision. However, at test time, the input images can have different viewpoints from those appearing
in the training images. We remark that the images of the same object with different viewpoints
belong to the same class, as usually recognized by humans. In this sense, it is desired that in-class
images with various viewpoints are identified as in-class, not out-of-class. That is, the robustness to
geometric transformations should be considered for one-class classification.
In this respect, the existing self-supervised
methods totally fail to compute the effective in-
class scores for inputs with various viewpoints.
We observe that they produce an undesirable
in-class score especially when the input image
has the same (or similar) viewpoint represented
by the anchor transformations T∖{T0}. For ex-
ample, suppose a classifier is trained on Dself
with the transformations T of clockwise rota-
tions {0°, 90°, 180°, 270°}. Given two images
of sea lions χ0 and x00, let x0 have the same
viewpoint with the training images and X0 have
the 90° rotated viewpoint, which is equivalent
to Tι(x0). As illustrated in Figure 1, the soft-
max probability of each transformed image has
a high value for the input x0, but the one for X0
has a low value. Consequently, it cannot cor-
rectly identify X0 as in-class, though it comes
from the target class as well. We point out that
setting the target label of each transformed im-
age to the applied transformation is not valid
any longer when an input viewpoint is changed.
0°
τ03)
SOftmaX
Xf	Prob.
元〃
Transform
Softmax
Prob.
90°	180°	270°
W)	72(%‘)	73(%‘)
Si.(χf) = 'p(y = " K3))
Smaff) = ɪp(v = i"G'f))
Figure 1: Atoy example of computing the in-class
scores for two in-class (sea lion) images with dif-
ferent viewpoints. Each row represents how the
in-class score is calculated for a given input.
A straightforward solution for this challenge is augmenting the training dataset so that it can cover
various viewpoints of in-class images. Unfortunately, the data augmentation technique is not ap-
plicable because it results in inconsistent supervision for the task of discriminating the viewpoints,
which is the learning objective of the self-supervised methods. On the other hand, there exist several
one-class classification methods (Ruff et al., 2018; Zong et al., 2018) that can adopt the data aug-
mentation technique. However, they cannot achieve the performance as high as the self-supervised
methods even in the case that all input images have a fixed viewpoint; this will be further discussed
in Section 4. To sum up, we need to consider another strategy to develop a robust one-class classifier
that works well even for the input images having various viewpoints.
3.2	Proposed Setups
We first propose three evaluation setups for testing the robustness to various viewpoints: 1) fixed
viewpoint, 2) anchor viewpoint, and 3) random viewpoint. We artificially introduce the spatial
variance (i.e., the changes of the viewpoint) in test images by using the geometric transformations.
Note that Xte denotes the test data, which contains both in-class and out-of-class images.
Fixed viewpoint setup. In this setup, we consider only the fixed viewpoint that is used for training,
as done in previous work. We do not change the viewpoint of the original test images, Xfv = Xte.
3
Under review as a conference paper at ICLR 2021
(a) Fixed setup, Xftev
(b) Anchor setup, Xatve
(c) Random setup, Xrtve
CIFAR10 CIFAR100 SVHN
(d) For transformed inputs
Figure 2: The one-class classification performance of the self-supervised method (Hendrycks et al.,
2019). (a-c) The in-class score distributions of in-class and out-of-class test images (Dataset:
CIFAR-10, In-class: Horse), (d) AUROC for geometrically/non-geometrically transformed inputs.
Anchor viewpoint setup. This setup is designed for verifying the robustness to the viewpoints
induced by the anchor transformations. We build a test dataset by Xav = {T(x)|T 〜T, X ∈ Xte},
where T is randomly sampled from the set of the anchor transformations T for each image x.
Random viewpoint setup. The random viewpoint setup further considers the geometric transfor-
mations that are not included in the set of anchor transformations. We first define the superset ofT,
denoted by T*, including a number of transformations with continuous degrees. A test dataset for
this setup is built by Xrte = {T(x) |T 〜T*, X ∈ Xte}, where T is sampled for each image x.
As a preliminary result, we plot the in-class score distributions for in-class and out-of-class test im-
ages, computed by the state-of-the-art self-supervised method (Hendrycks et al., 2019). In Figure 2,
we observe that the score distributions of in-class and out-of-class images in Xftev are clearly distin-
guishable, which supports the great performance for one-class classification. On the contrary, the
two score distributions are almost overlapping with each other in cases of Xatve and Xrtve, strongly
indicating that they fail to figure out in-class images due to their various viewpoints.
We additionally investigate the performance drop for geometrically/non-geometrically transformed
inputs. In Figure 2(d), it is obvious that geometric transformations make the self-supervised method
totally malfunction, while non-geometric transformations (e.g., brightness, contrast, sharpness, and
color temperature) hardly degrade the final performance for one-class classification.
3.3	Proposed Strategy
To deal with the viewpoint sensitivity of the self-supervised methods, we note that the in-class
images match better with the in-class transformations than the out-of-class images do, regardless
of their viewpoints. Our proposed strategy, named as GROC, defines the in-class score by the sum
of the conformity scores for K transformed images; Sconf (∙; T) calculates how conformable an
input image is to the given set T. Formally, it is defined by the maximum similarity between the
representation of an input image and that of each anchor transformation:
K-1
Sin(X) = X Sconf (Ti(X); T), where Sconf (X; T) = max [sim (X, Tj)] .	(3)
i=0	Tj ∈T
The foremost condition for GROC is that the representations of the anchor transformations should be
discriminative, so that the similarity measure can effectively capture the viewpoint of input images.
Note that the similarity between an image X and a transformation Tj, denoted by sim(X, Tj ), can
be defined in various ways. In the following subsections, we offer two similarity measures for the
conformity score, respectively modeled by inner product similarity and conditional likelihood, and
present how the representations of input images and anchor transformations are optimized.
3.3.1	Inner Product Similarity for Conformity Score
To model the similarity measure for the conformity score, We use our encoder network f (∙; θ):
X → Rd which outputs the representation of an input image, and the weight matrix W ∈ RK×d
that parameterizes the representations of K anchor transformations. We first present GROC-IP
whose similarity measure is simply computed by the inner product of f(X; θ) and wj:
sim (X,Tj) = f (X; θ)>wj.	(4)
4
Under review as a conference paper at ICLR 2021
Based on the inner product similarity, the encoder network needs to map all in-class images with the
same viewpoint close to their corresponding transformation vector, while keeping the transformation
vectors far from each other. In other words, it has to extract discriminative features for classifying
the input images according to their viewpoint; this has been already achieved by the conventional
softmax classifier in a self-supervised manner.
Therefore, we adopt the optimization strategy provided by the existing self-supervised methods for
one-class classification. After we build a softmax classifier by adding the linear classification layer
of weights W on the top of the encoder network f (∙; θ), We train both the weight matrix and the
network by using the cross-entropy loss with Dself . In case of GROC-IP, the conformity score
becomes equivalent to the maximum logit value computed by the softmax classifier.
3.3.2	Conditional Likelihood for Conformity Score
Our second method, GROC-CL, defines the similarity measure by using the likelihood of an input
image conditioned on each transformation. For simplicity, we assume that the conditional likelihood
has the form of an isotropic Gaussian distribution, whose mean is μj ∈ Rd and standard deviation is
σj ∈ R+ for the condition j (i.e., transformation Tj). In short, GROC-CL models the representation
of Tj as (μj,σj) rather than Wj. Similar to Section 3.3.1, all the Gaussian distributions need to be
separable for a discrimination among different viewpoints. Based on this assumption, the similarity
between x and Tj is defined by the log-likelihood as follows.
Sim (x,Tj) = log N (f(x; θ)∣μj ,σ2I) ≈ - (kf(X; ：；2 μjk2 + log σd) .	(5)
Note that this similarity can be interpreted as the Mahalanobis distance between f (x; θ) and μj∙ with
the covariance matrix σj2I. The challenge here is to optimize the encoder network so that its outputs
follow N(μj,σjI) for all in-class inputs having the viewpoint corresponding to Tj. We train the
parameters for the Gaussian distributions (μ, σ) and the network f (∙; θ) by the following objective.
K-1
max
θ,μ,σ,b
Σ Σ
x∈Xitnr j=0
Sim (Tj(x),Tj) + V log
exp (Sim (Tj(x), Tj) + bj)
PK-IIeXP(Sim (Tj(X),Tk) + bk)
(6)
where bj ∈ R is the bias term for transformation j . As discussed in (Lee et al., 2020), this objective
aims to learn the discriminative conditional likelihoods modeled by K separable Gaussian distribu-
tions. To be precise, the first term is enforcing that f (Tj (x)) follows the j-th Gaussian distribution,
and the second term makes them distinguishable based on Gaussian discriminant analysis.
4	Experiments
4.1	Competing methods
In our experiments, we consider a variety of approaches to one-class classification as the compet-
ing methods. We choose One-class SVM (OCSVM) (Scholkopf et al., 2000) and Deep SVDD
(DSVDD) (Ruff et al., 2018) as the non-self-supervised methods. OCSVM is a classical kernel-
based method for one-class classification, which finds a maximum-margin hyperplane the separates
enclosing most of the training in-class examples. DSVDD, a deep learning variant of OCSVM,
explicitly models the latent space in which training in-class examples gather to a specific center
point.
The main competitors are the self-supervised methods based on transformation classification: Ge-
ometric Transformation (GT) (Golan & El-Yaniv, 2018) and Multi-labeled Geometric Transforma-
tion (MGT) (Hendrycks et al., 2019). The details of the methods are presented in Section 2.2.
For the anchor transformations, GT adopts four transformation categories (i.e., horizontal flipping,
x,y-translation, and rotation), while MGT excludes horizontal flipping from the above categories.
The last competing method is SimCLR (Chen et al., 2020) which learns the transformation-invariant
representations of input images in a self-supervised manner. Since it is optimized to maximize the
agreement among the images differently-transformed from a single image, it is capable of alleviating
5
Under review as a conference paper at ICLR 2021
the viewpoint sensitivity to some degree. Several recent work on representation learning based
on this approach (Chen et al., 2020; Grill et al., 2020; He et al., 2020) showed the remarkable
performance for a wide range of downstream tasks. Note that SimCLR is not originally designed
for one-class classification, thus we tailor it for our task. We define the final in-class score by
K-1
Sin (x) = X
i=1
f(To(x); θ)>f (Ti(X) θ)
Ilf(To(x); θ)k2kf (Ti(x); θ)k2.
(7)
For its optimization, we use the set of the anchor transformations adopted by MGT. More details of
SimCLR are provided in Appendix A.
4.2	Datasets
We validate the effectiveness of the proposed methods using three benchmark image datasets:
CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009), and SVHN (Netzer et al., 2011). We scale the
pixel values of all images to be in [-1, 1] as done in (Golan & El-Yaniv, 2018). Note that CIFAR-
100 has 20 super-classes and we use these super-classes rather than 100 full classes.
4.3	Experimental Settings
Following the experimental setting of the previous studies (Golan & El-Yaniv, 2018; Ruff et al.,
2018; Zenati et al., 2018), we employ the one-vs-all evaluation scheme. For the dataset with C
classes, we generate C one-class classification settings; the images of a target class are regarded as
in-class data, and the other images belonging to C - 1 classes are regarded as out-of-class data. We
use the Area Under the ROC curve (AUROC) as an evaluation metric.
We build the set of the anchor transformations by the combination of the following unit
transformations: x-translation ∈ {-8, 0, +8}, y-translation ∈ {-8, 0, +8}, and rotation ∈
{0°, 90°, 180°, 270°}. As presented in Section 3.2, We evaluate our method by using the three Pro-
posed setups: fixed viewpoint, anchor viewpoint, and random viewpoint. For the random viewpoint
setup, We build the test set by randomly sampling the transformation degrees for the x,y-translation
in the range of [-8, 8], and for the rotation in the range of [0°, 360°]. In case of SVHN, We exclude
the x,y-translation because it may change the semantic (i.e., class label) of the images; the label of
each image is determined by the digit in the middle of the image.
4.4	Experimental Results
4.4	. 1 Comparisons with Self-supervised Methods
The experimental results of the self-supervised methods on the three setups are presented in Table 1.
For each dataset, the original class name is replaced by its class id due to the limited space. In sum-
mary, our methods effectively overcome the limitation of vieWpoint sensitivity, Without compromis-
ing the performance in the fixed vieWpoint setup. We analyze the results from various perspectives.
Fixed viewpoint setup. In this setup, the state-of-the-art competitor MGT consistently shoWs the
best results, and our methods shoW the comparable results to MGT. We also observe that the per-
formance of SimCLR is not as good as those of the classification-based methods. Note that the
classification-based methods aim for a discrimination among the differently-transformed images,
Whereas SimCLR tries to make them indistinguishable. From this observation, We can conclude that
directly learning the transformation-invariant visual features is less effective to identify in-class and
out-of-class images for one-class classification.
Anchor viewpoint setup. In case that the input images have anchor vieWpoints, the classification-
based methods fail to distinguish in-class images from out-of-class images; their performances are
even Worse than a random guess Whose AUROC is 0.5. Because their capability of discriminat-
ing the geometric transformations depends on the fixed vieWpoint, they do not Work at all for the
input images With various vieWpoints, as discussed in Section 3.1. In contrast, our methods con-
siderably outperform all the competing methods While providing outstanding performances robust
to the changes of the vieWpoint. These results shoW that our methods successfully identify the in-
class images irrespective of their vieWpoint, by the help of the conformity score that measures hoW
6
Under review as a conference paper at ICLR 2021
Table 1: The AUROC of self-supervised methods for one-class classification on the three evaluation
setups. The best results are marked in bold face.
Dataset in-class
Fixed Viewpoint
Anchor Viewpoint
Random Viewpoint
SimCLR / GT /MGT/ GROC-IP/ GROC-CL
。'XVH lɔ
0
1
2
3
4
5
6
7
8
9
0.66/0.77/0.78/0.76/0.75
0.62/0.87/0.96/0.95/0.97
0.64/0.83/0.86/0.85/0.84
0.54/0.78/0.80/0.77/0.77
0.61 /0.86/0.91/0.90/0.90
0.54/0.87/0.89/0.88/0.88
0.65/0.89/0.89/0.90/0.93
0.58 / 0.91 /0.96/0.95/0.96
0.52/0.88/0.94/0.92/0.94
0.51 /0.84/0.90/0.90/0.93
0.54/0.44/0.45/0.70/0.68
0.40 / 0.32 / 0.39 / 0.91 / 0.94
0.57/0.42/0.45/0.76/0.73
0.43 /0.45/0.44/0.73/0.69
0.49/0.39/0.42/0.85/0.83
0.43 /0.42/0.42/0.83/0.83
0.42 / 0.44 / 0.42 / 0.81 / 0.83
0.43 /0.32/0.42/0.91/0.89
0.44 / 0.35 / 0.41 / 0.88 / 0.87
0.41 /0.36/0.42/0.86/0.88
0.53/0.44/0.44/0.63/0.64
0.40/0.39/0.40/0.84/0.88
0.56/0.46/0.45/0.64/0.65
0.43/0.47/0.49/0.62/0.61
0.49/0.38/0.42/0.72/0.71
0.44/0.44/0.44/0.72/0.73
0.42/0.44/0.46/0.72/0.76
0.43/0.40/0.40/0.77/0.77
0.41 /0.36/0.39/0.80/0.80
0.40/0.39/0.40/0.79/0.83
avg 0.59/0.85/0.89/0.88/0.89	0.46/0.39/0.42/0.82/0.82 0.45 /0.42/0.43/0.73/0.74
。。'XVH lɔ
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
0.57/0.75/0.77/0.77/0.78
0.64/0.69/0.72/0.74/0.72
0.63 / 0.71 / 0.71 / 0.75 / 0.82
0.58/0.78/0.80/0.80/0.80
0.63 / 0.78 / 0.80 / 0.81 / 0.81
0.59/0.66/0.67/0.67/0.66
0.50/0.86/0.88/0.84/0.85
0.67/0.65/0.66/0.67/0.69
0.50/0.84/0.87/0.85/0.87
0.42 / 0.88 / 0.92 / 0.91 / 0.91
0.60 / 0.85 / 0.87 / 0.85 / 0.85
0.53/0.84/0.86/0.86/0.87
0.54 / 0.82 / 0.84 / 0.83 / 0.84
0.65/0.60/0.62/0.60/0.59
0.52/0.92/0.93/0.90/0.90
0.56 / 0.69 / 0.70 / 0.71 / 0.72
0.55/0.76/0.80/0.80/0.76
0.65/0.90/0.93/0.92/0.92
0.52/0.89/0.90/0.90/0.92
0.53/0.85/0.89/0.87/0.85
0.55 /0.43/0.43/0.71 / 0.72
0.58 /0.46/0.43/0.66/0.62
0.49/0.44/0.48/0.70/0.75
0.55 / 0.40 / 0.41 / 0.72 / 0.75
0.53 /0.42/0.46/0.76/0.76
0.56/0.45/0.49/0.60/0.59
0.48/0.43/0.46/0.77/0.82
0.56/0.46/0.47/0.60/0.62
0.48 /0.43/0.46/0.78/0.80
0.41 / 0.39 / 0.41 / 0.89 / 0.87
0.54/0.39/0.44/0.82/0.81
0.46 / 0.41 /0.43/0.80/0.81
0.47/0.43/0.45/0.77/0.80
0.63 / 0.51 /0.49/0.56/0.55
0.44/0.40/0.42/0.86/0.84
0.55 /0.45/0.46/0.65/0.65
0.52/0.48/0.43/0.73/0.68
0.55 / 0.36 / 0.41 / 0.88 / 0.84
0.44/0.40/0.40/0.87/0.86
0.45 / 0.40 / 0.41 / 0.81 / 0.79
0.55/0.44/0.47/0.62/0.64
0.60/0.48/0.47/0.60/0.62
0.51 /0.48/0.47/0.65/0.70
0.50/0.48/0.47/0.52/0.65
0.50/0.46/0.43/0.70/0.72
0.53/0.48/0.51/0.53/0.55
0.44/0.46/0.46/0.59/0.58
0.57/0.48/0.48/0.59/0.56
0.45/0.47/0.46/0.69/0.75
0.41 /0.38/0.39/0.76/0.77
0.56/0.42/0.43/0.73/0.74
0.48/0.47/0.44/0.70/0.70
0.47/0.45/0.45/0.68/0.72
0.61 / 0.49 / 0.47 / 0.51 / 0.52
0.44/0.42/0.43/0.73/0.70
0.53/0.47/0.49/0.60/0.61
0.52/0.49/0.46/0.66/0.66
0.52/0.38/0.39/0.79/0.76
0.41 /0.47/0.42/0.75/0.76
0.47/0.43/0.39/0.73/0.71
avg 0.57/0.79/0.81/0.80/0.81	0.51 /0.43/0.44/0.75/0.75	0.50 / 0.45 /0.45/0.66/0.67
NHAS
0
1
2
3
4
5
6
7
8
9
0.79 / 0.71 /0.86/0.92/0.92
0.68/0.72/0.80/0.89/0.90
0.75/0.98/0.96/0.94/0.94
0.63 / 0.91 /0.94/0.93/0.92
0.63 / 0.98 / 0.98 / 0.95 / 0.96
0.86 / 0.96 / 0.95 / 0.95 / 0.94
0.69 / 0.96 / 0.96 / 0.91 / 0.91
0.75/0.99/0.98/0.96/0.96
0.67/0.77/0.88/0.92/0.91
0.65/0.97/0.97/0.90/0.89
0.79 / 0.39 / 0.21 / 0.92 / 0.92
0.68 /0.35/0.20/0.89/0.89
0.75 /0.30/0.23/0.94/0.94
0.63 / 0.35 / 0.41 / 0.93 / 0.91
0.63 / 0.33 / 0.21 / 0.95 / 0.96
0.86/0.30/0.32/0.95/0.94
0.69/0.31/0.35/0.91/0.91
0.75 /0.30/0.23/0.96/0.96
0.67/0.36/0.33/0.92/0.91
0.65 / 0.32 / 0.31 / 0.90 / 0.89
0.63/0.43/0.44/0.78/0.80
0.52/0.42/0.43/0.75/0.77
0.53/0.40/0.41/0.75/0.76
0.60/0.42/0.42/0.75/0.73
0.54/0.42/0.42/0.71/0.71
0.69/0.39/0.40/0.83/0.78
0.61 /0.36/0.39/0.80/0.78
0.46/0.39/0.43/0.70/0.74
0.59/0.44/0.46/0.71/0.67
0.60/0.34/0.43/0.79/0.78
avg 0.71 /0.90/0.93/0.93/0.92	0.71 /0.33/0.28/0.93/0.92 0.58/0.40/0.42/0.76/0.75
confidently an input image matches with one of the in-class transformations. Interestingly, the per-
formance of SimCLR is higher than that of the classification-based methods in this setup. This is
because its learning objective that encourages the multiple transformations of an input image to be
similar with each other makes the one-class classifier less affected by the viewpoint.
Random viewpoint setup. In the hardest setting where the input images have random viewpoints,
the classification-based methods cannot beat a random guessing, similarly to the anchor viewpoint
setup. Our methods perform the best for all the datasets, which strongly indicates the robustness to
the changes of the viewpoint. In conclusion, both of the our methods (i.e., GROC-IP and GROC-
CL) are able to correctly classify the images having diverse viewpoints into in-class or out-of-class,
even for the viewpoints that have not been seen during the training.
4.4.2 Comparisons with Non-self-supervised Methods
Figure 3 presents the comparison results against the non-self-supervised methods on the three eval-
uation setups. Due to the limited space, we report the score averaged over C in-class settings for
7
Under review as a conference paper at ICLR 2021

864 2
Oooo
Dod∩4
OCSVM
DSVDD
DSVDD+
GROC-IP
GROC-CL
:h-AOAoAoAolooo
三一三一三「m
,S36 4 2
Oooo
0Hn4
-mu
864 2
Oooo
UoH∩4
≡=
∖
OCSVM
DSVDD
DSVDD+
GROC-IP
GROC-CL
三三二二二一
K∖∖∖∖∖∖
∖
∖
∖
∖
∖
∖
∖
CIFAR10 CIFAR100 SVHN
CIFAR10 CIFAR100 SVHN
CIFAR10	CIFAR100	SVHN
(a)	Fixed Viewpoint
(b)	Anchor Viewpoint
(c)	Random Viewpoint

-------------⅜
-、、一MMMMMM
UMM

Figure 3: Comparisons with non-self-supervised methods in the three evaluation setups
each dataset. We also provide the results of DSVDD that adopts the data augmentation technique,
denoted by DSVDD+; the training in-class images are randomly augmented by T 〜T*. For all
the setups, the competing methods show poorer performances compared to our methods. Specifi-
cally, the data augmentation technique results in the limited performance gains in the anchor/random
setups, whereas it even brings an adverse effect in the fixed setup. This implies that the simple ap-
proach is not sufficient to address the viewpoint sensitivity of the one-class classifiers.
4.4.3 Further Analysis
We also provide in-depth analyses on the performance of the self-supervised methods, for the SVHN
dataset. In the fixed viewpoint setup, we observe that there is a distinct performance improvement
of our GROC over MGT, especially for the cases of in-class 0, 1, and 8. Figure 4 shows the in-class
score distributions obtained by GROC and MGT, where the class 0 is set to in-class. Since the digit
‘0’ has a symmetric shape, it is difficult for MGT to differentiate its transformations between the
rotation 0° and 180° (or, 90° and 270°). For this reason, as illustrated in the rightmost figure, MGT
outputs relatively low scores for the images having a single ‘0’ but high scores for the images having
other digits around ‘0’. On the contrary, GROC produces the similar (and high) in-class scores for
these images (i.e., with or without other digits), which can be separated from the scores of out-of-
class images. This helps GROC to less overlap the in-class scores between in-class and out-of-class
images, and as a result, it leads to higher AUROC compared to MGT.
On the other hand, for the cases of in-class 6 and 9, the performance of GROC slightly degrades
because the 180° rotation of the out-of-class digit ‘9’ is more likely to be conformable to the in-
class digit ‘6’, and vice versa. Nevertheless, under the assumption that the input images have various
viewpoints, it is impossible even for humans to accurately figure out whether the images that look
like ’6’ (or ’9’) belong to in-class or out-of-class.
Figure 4: The in-class score distributions of in-class and out-of-class test images in the fixed view-
point setup (Dataset: SVHN, In-class: 0).
5 Conclusion
This paper proposes a novel one-class classification method robust to geometric transformations,
which effectively addresses the challenge that in-class images cannot be correctly distinguished
from out-of-class images when they have various viewpoints. We first present new evaluation setups
that cover the diverse viewpoints by artificially introducing the spatial variance into test images.
Then, we define the conformity-based in-class score so as to measure how strongly an input im-
age is conformable to one of the anchor transformations, whose representations are optimized to be
discriminative. The extensive experiments demonstrate that the proposed GROC keeps its outstand-
8
Under review as a conference paper at ICLR 2021
ing performance even in the anchor/random viewpoint setups where the input images have various
viewpoints, whereas the state-of-the-art methods perform even worse than a random guessing.
References
Liron Bergman and Yedid Hoshen. Classification-based anomaly detection for general data. arXiv
preprint arXiv:2005.02359, 2020.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.
Izhak Golan and Ran El-Yaniv. Deep anomaly detection using geometric transformations. In Ad-
Vances in Neural Information Processing Systems,pp. 9758-9769, 2018.
Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec, Pierre H Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohammad Gheshlaghi
Azar, et al. Bootstrap your own latent: Anew approach to self-supervised learning. arXiv preprint
arXiv:2006.07733, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and Dawn Song. Using self-supervised learning
can improve model robustness and uncertainty. In Advances in Neural Information Processing
Systems, pp. 15663-15674, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Dongha Lee, Sehun Yu, and Hwanjo Yu. Multi-class data description for out-of-distribution detec-
tion. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discov-
ery & Data Mining, pp. 1362-1370, 2020.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Emanuel Parzen. On estimation of a probability density function and mode. The annals of mathe-
matical statistics, 33(3):1065-1076, 1962.
Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexan-
der Binder, Emmanuel Muller, and Marius Kloft. Deep one-class classification. In International
conference on machine learning, pp. 4393-4402, 2018.
Thomas Schlegl, Philipp Seebock, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg
Langs. Unsupervised anomaly detection with generative adversarial networks to guide marker
discovery. In International conference on information processing in medical imaging, pp. 146-
157. Springer, 2017.
Bernhard Scholkopf, Robert C Williamson, Alex J Smola, John Shawe-Taylor, and John C Platt.
Support vector method for novelty detection. In Advances in neural information processing sys-
tems, pp. 582-588, 2000.
9
Under review as a conference paper at ICLR 2021
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
David MJ Tax and Robert PW Duin. Support vector data description. Machine learning, 54(1):
45-66, 2004.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar.
Adversarially learned anomaly detection. In 2018 IEEE International Conference on Data Mining
(ICDM), pp. 727-736. IEEE, 2018.
Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng
Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In Inter-
national Conference on Learning Representations, 2018.
10
Under review as a conference paper at ICLR 2021
A SIMCLR
In the experiments, we slightly modified SimCLR (Chen et al., 2020) for the one-class classification
task. The main idea of SimCLR is learning representations by maximizing the agreement among the
images differently-transformed from the same image via a contrastive loss in the latent space. For
the optimization of SimCLR, we use the set of the anchor transformations T that is adopted by the
classification-based self-supervised method (i.e., MGT); this set is different from the one used by
the original SimCLR. Let X ∈ Xin and X = T (x) where T is a transformation operator randomly
sampled from the set of the anchor transformations T. Given a batch B = {xι,…，XN }⊂ Xtnr, We
define B = {xι, x2,…，x?n-i, x?n} where x2k_1 and X2k are generated by applying different
transformations to each image Xk in the batch. The loss function for a pair of two differently-
transformed images (X2k-1, X2k) from the input image Xk is defined as follows.
l(~	~ ) = _“________________exp (Sim (f (X2k-1； θ") , f (X2k； θ) /)__________ (8)
l 2k-1, X2k) — - g P= I[i = (2k - 1)]exp (Sim (f (X2k-1; θ), f (X,; θ)) IT))⑻
where N is the number of images in a batch, f (∙) is an encoder network including a projection layer,
and Sim(u, v) = u>vIkuk2 kvk2 is the cosine similarity. It is worth noting that we include the pro-
jection layer in the encoder network in order to obtain the transformation-invariant representations,
unlike the original version of SimCLR that discards the projection layer for their downstream tasks.
In the end, the objective function of SimCLR for a batch B is defined as
1N
LSimCLR = 2N ɪ2 U (X2k-1, X2k ) + l (x 2k, X2k-l)] ∙
k=1
(9)
B	Implementation Details
We choose a 16-4 WideResnet (Zagoruyko & Komodakis, 2016) as the backbone architecture. We
adopt the training strategy for multi-label classification, proposed in (Hendrycks et al., 2019). Dur-
ing the training, we use the cosine annealing for scheduled learning (Loshchilov & Hutter, 2016)
with initial learning rate 0.1 and Nesterov momentum. The dropout rate (Srivastava et al., 2014) is
set to 0.3. All the self-supervised methods based on transformation classification also use the same
backbone architecture and training hyperparameters with ours. For DSVDD, we use LeNet (LeCun
et al., 1998) style network as described in the paper and implementation.2 For SimCLR, we employ
the ResNet18 (He et al., 2016) with the fixed τ value of 0.5. For GROC-CL, the regularization
coefficient ν for optimizing the conditional likelihoods is set to 0.0001.
C	Experimental Results
In Table 2, we report the full comparison results with the non-self-supervised methods for one-class
classification (summarized in Figure 3).
D Theoretical Backgrounds for GROC-CL
GROC-CL basically utilizes the encoder network f that induces the latent space, where the similarity
between the representation of an input image and that of each anchor transformation is modeled
by an isotropic Gaussian distribution (conditioned on the transformation). Formally, the similarity
between X and Tj can be described as sim (x, Tj) = logP (x|T-) = logN (f (x) ∣μj,σjl), where
p (X|Tj) is the class-conditional (or transformation-conditional) probability.
As discussed in Section 3.3, the representation of each anchor transformation should be distinguish-
able from the others’, which is the foremost condition for GROC, in order to effectively calculate
the conformity score and identify in-class/out-of-class images from the score. In this sense, GROC-
CL can be understood from the perspective of Gaussian Discriminant Analysis (GDA). To this end,
2https://github.com/lukasruff/DeeP-SVDD-PyTOrCh
11
Under review as a conference paper at ICLR 2021
we optimize the encoder network by maximizing the posterior probability of a transformed image
Tj (x) having the maximum similarity with the transformation j, which is denoted by p (Tj |Tj (x)).
For simplicity, we assume that the prior probability for each class (or transformation) follows the
Bernoulli distribution, i.e.,p(Tj) = βj/ kβk.
p(Tj|Tj(x)) =
P (Tj)P (Tj(X)T∙)
PKo1 p (Tk)p (Tj(x)∣Tk)
exp (- (2σ2)-1 kf (Tj(X))- μj-k2 - logσd + logβj)
PKOIeXP (- (2σ2)-1 Ilf (Tj(X))- μkIl2 - logσd + logβ)
(10)
exp (Sim (Tj(x),Tj) + bj)
PK01 exp (sim (Tj(x),Tk) + b®)
Note that taking the log of this equation becomes equivalent to the second term in Equation (6).
In addition, we need to force the empirical class-conditional distribution to follow the isotropic
Gaussian distribution and also approximate the empirical class mean to the obtained class mean
μj. Thus, We minimize the KUllback-Leibler (KL) divergence between the j-th empirical class-
conditional distribution Pj and the corresponding Gaussian distribution N (μj∙, σj2I). The empirical
class-conditional distribution for transformation j is defined as follows.
1
而
Pj
δ(z-f(Tj(x))),
x∈Xitnr
(11)
where δ (∙) is the Dirac measure. Finally, the KL divergence is obtained by
KL (PjkN ("j"2l))=-
xXJ(Z-f (Tj (X)))log l(2∏σjrexp
+ J 向 X δ (z-f(Tj (x)))log I 向 X δ (z-f(Tj (X))) I dz
in X∈Xitnr	in X∈Xitnr
—
T) dz
-向 XXnlog
Q ∖d/2 eXP (-
2πσj2
kf(Tj(x)) - μjk2
2σj
1
+log 而
向 XXtr(
in
kf(Tj(x)) - μjk2
2σj
+ log σjd + constant
-ιχτtri X Sim (Tj(X),Tj) + constant.
|Xin | X∈X tr
in
(12)
The final form of the KL divergence is derived by using the definition of the Dirac measure. After the
constant term is excluded, it becomes the same with the first term in Equation (6). Minimizing the
KL term for all the classes (or transformations) matches the empirical class-conditional distributions
with the isotropic Gaussian distributions.
12
Under review as a conference paper at ICLR 2021
Table 2: The AUROC of non-self-supervised methods for one-class classification on the three eval-
uation setups.
Dataset in-class
Fixed Viewpoint	Anchor Viewpoint	Random Viewpoint
OCSVM / DSVDD / DSVDD+ / GROC-IP/ GROC-CL
。'XVH lɔ
0
1
2
3
4
5
6
7
8
9
0.65/0.65/0.80/0.76/0.75
0.41 /0.54/0.45/0.95/0.97
0.65/0.67/0.62/0.85/0.84
0.50/0.52/0.54/0.77/0.77
0.75/0.76/0.76/0.90/0.90
0.51 / 0.51 /0.54/0.88/0.88
0.72/0.75/0.72/0.90/0.93
0.51 /0.52/0.58/0.95/0.96
0.68 / 0.68 / 0.41 / 0.92 / 0.94
0.49/0.52/0.37/0.90/0.93
0.63 /0.64/0.65/0.70/0.68
0.38 /0.48/0.45/0.91 / 0.94
0.65 /0.62/0.63/0.76/0.73
0.49 / 0.52 / 0.54 / 0.73 / 0.69
0.74/0.72/0.75/0.85/0.83
0.50 / 0.48 / 0.49 / 0.83 / 0.83
0.71 /0.72/0.73/0.81 / 0.83
0.50 / 0.51 / 0.53 / 0.91 / 0.89
0.54/0.53/0.50/0.88/0.87
0.38 /0.37/0.39/0.86/0.88
0.60/0.63/0.69/0.63/0.64
0.39/0.46/0.49/0.84/0.88
0.64/0.63/0.64/0.64/0.65
0.49/0.45/0.53/0.62/0.61
0.72/0.73/0.74/0.72/0.71
0.48/0.45/0.45/0.72/0.73
0.70/0.68/0.73/0.72/0.76
0.50/0.43/0.54/0.77/0.77
0.54/0.36/0.61/0.80/0.80
0.38/0.40/0.40/0.79/0.83
avg 0.59 / 0.61 /0.58/0.88/0.89	0.55 /0.56/0.57/0.82/0.82 0.54/0.52/0.58/0.73/0.74
。。'XVH lɔ
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
0.66/0.64/0.58/0.77/0.78
0.52/0.56/0.47/0.74/0.72
0.52/0.56/0.39/0.75/0.82
0.51 /0.58/0.47/0.80/0.80
0.52 / 0.58 / 0.46 / 0.81 / 0.81
0.44/0.50/0.43/0.67/0.66
0.52 / 0.51 /0.50/0.84/0.85
0.59/0.63/0.62/0.67/0.69
0.67/0.67/0.63/0.85/0.87
0.69 / 0.69 / 0.51 / 0.91 / 0.91
0.75/0.82/0.57/0.85/0.85
0.61 / 0.61 /0.53/0.86/0.87
0.68/0.63/0.65/0.83/0.84
0.59/0.65/0.66/0.60/0.59
0.44/0.45/0.46/0.90/0.90
0.60 / 0.65 / 0.59 / 0.71 / 0.72
0.66/0.65/0.60/0.80/0.76
0.71/0.76/0.70/0.92/0.92
0.52/0.50/0.44/0.90/0.92
0.55/0.56/0.45/0.87/0.85
0.65 /0.48/0.58/0.71 / 0.72
0.51 /0.57/0.57/0.66/0.62
0.52/0.52/0.54/0.70/0.75
0.50/0.53/0.57/0.72/0.75
0.51 /0.53/0.56/0.76/0.76
0.44/0.50/0.47/0.60/0.59
0.51 /0.49/0.47/0.77/0.82
0.59/0.58/0.55/0.60/0.62
0.66/0.63/0.69/0.78/0.80
0.50/0.52/0.48/0.89/0.87
0.62 / 0.71 /0.67/0.82/0.81
0.61 /0.60/0.58/0.80/0.81
0.67/0.65/0.64/0.77/0.80
0.59/0.60/0.62/0.56/0.55
0.43 /0.43/0.46/0.86/0.84
0.60/0.57/0.59/0.65/0.65
0.65 /0.58/0.60/0.73 / 0.68
0.61 /0.48/0.53/0.88/0.84
0.45 /0.49/0.43/0.87/0.86
0.49 / 0.43 / 0.44 / 0.81 / 0.79
0.64/0.49/0.58/0.62/0.64
0.49/0.45/0.54/0.60/0.62
0.48/0.55/0.61/0.65/0.70
0.48/0.53/0.44/0.52/0.65
0.46 / 0.31 /0.56/0.70/0.72
0.43/0.55/0.51/0.53/0.55
0.51 /0.42/0.40/0.59/0.58
0.54/0.56/0.58/0.59/0.56
0.67/0.66/0.65/0.69/0.75
0.55/0.42/0.47/0.76/0.77
0.61 /0.58/0.67/0.73/0.74
0.60/0.54/0.52/0.70/0.70
0.66/0.62/0.66/0.68/0.72
0.58 / 0.66 / 0.65 / 0.51 / 0.52
0.44/0.45/0.46/0.73/0.70
0.55/0.58/0.58/0.60/0.61
0.63/0.58/0.59/0.66/0.66
0.57/0.58/0.61/0.79/0.76
0.48/0.48/0.47/0.75/0.76
0.49/0.47/0.45/0.73/0.71
avg 0.59 / 0.61 / 0.53 / 0.80 / 0.81	0.56/0.54/0.55/0.75/0.75	0.54 / 0.53 /0.55/0.66/0.67
NHAS
0
1
2
3
4
5
6
7
8
9
0.54/0.62/0.55/0.92/0.92
0.51 /0.55/0.50/0.89/0.90
0.52/0.55/0.50/0.94/0.94
0.51 /0.53/0.50/0.93/0.92
0.50/0.52/0.50/0.95/0.96
0.52/0.54/0.52/0.95/0.94
0.51 /0.56/0.50/0.91 / 0.91
0.51 /0.56/0.52/0.96/0.96
0.50/0.54/0.49/0.92/0.91
0.52 / 0.52 / 0.51 / 0.90 / 0.89
0.53 / 0.51 /0.52/0.92/0.92
0.50 / 0.51 /0.52/0.89/0.89
0.51 /0.50/0.50/0.94/0.94
0.50 / 0.49 / 0.50 / 0.93 / 0.91
0.49/0.49/0.50/0.95/0.96
0.51 / 0.49 / 0.51 / 0.95 / 0.94
0.50 / 0.49 / 0.50 / 0.91 / 0.91
0.50 / 0.51 /0.52/0.96/0.96
0.49/0.48/0.49/0.92/0.91
0.51 / 0.49 / 0.51 / 0.90 / 0.89
0.52/0.53/0.54/0.78/0.80
0.50/0.53/0.52/0.75/0.77
0.50/0.50/0.51/0.75/0.76
0.50/0.50/0.51/0.75/0.73
0.48 / 0.51 / 0.51 / 0.71 / 0.71
0.51 / 0.51 /0.51/0.83/0.78
0.51 /0.49/0.51/0.80/0.78
0.49 / 0.51 /0.51/0.70/0.74
0.49 / 0.50 / 0.51 / 0.71 / 0.67
0.51 /0.49/0.52/0.79/0.78
avg 0.51 / 0.55 / 0.51 / 0.93 / 0.92	0.50 / 0.50 / 0.51 / 0.93 / 0.92 0.50 / 0.51 /0.52/0.76/0.75
13