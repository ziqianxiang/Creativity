Under review as a conference paper at ICLR 2021
MDP Playground: Controlling Orthogonal
Dimensions of Hardness in Toy Environments
Anonymous authors
Paper under double-blind review
Ab stract
We present MDP Playground, an efficient benchmark for Reinforcement Learning
(RL) algorithms with various dimensions of hardness that can be controlled inde-
pendently to challenge algorithms in different ways and to obtain varying degrees
of hardness in generated environments. We consider and allow control over a
wide variety of key hardness dimensions, including delayed rewards, rewardable
sequences, sparsity of rewards, stochasticity, image representations, irrelevant
features, time unit, and action range. While it is very time consuming to run
RL algorithms on standard benchmarks, we define a parameterised collection of
fast-to-run toy benchmarks in OpenAI Gym by varying these dimensions. Despite
their toy nature and low compute requirements, we show that these benchmarks
present substantial challenges to current RL algorithms. Furthermore, since we can
generate environments with a desired value for each of the dimensions, in addition
to having fine-grained control over the environments’ hardness, we also have the
ground truth available for evaluating algorithms. Finally, we evaluate the kinds
of transfer for these dimensions that may be expected from our benchmarks to
more complex benchmarks. We believe that MDP Playground is a valuable testbed
for researchers designing new, adaptive and intelligent RL algorithms and those
wanting to unit test their algorithms.
1	Introduction
RL has succeeded at many disparate tasks, such as helicopter aerobatics, game-playing and continuous
control (Abbeel et al., 2010; Mnih et al., 2015; Silver et al., 2016; Chua et al., 2018; Fujimoto et al.,
2018; Haarnoja et al., 2018), and yet a lot of the low-level workings of RL algorithms are not well
understood. This is exacerbated by the absence of a unifying benchmark for all of RL. There are
many different types of benchmarks, as many as there are kinds of tasks in RL (e.g. Todorov et al.,
2012; Bellemare et al., 2013; Cobbe et al., 2019; Osband et al., 2019). They specialise in specific
kinds of tasks. And yet the underlying assumptions are nearly always those of a Markov Decision
Process (MDP). We propose a benchmark which distills difficulties for MDPs that can be generalised
across RL problems and allows to control these difficulties for more precise experiments.
RL algorithms face a variety of challenges in environments. For example, when the underlying
environment is an MDP, however the information state, i.e., the state representation used by the
agent is not Markov, we have partial observability, (see, e.g., Mnih et al., 2015; Jaakkola et al.,
1995). There are additional aspects of environments such as having irrelevant features, having
multiple representations for the same state, and the action range, that significantly affect agent
performance. We aim to study what kind of failure modes can occur when an agent is faced with
such environments and to allow other researchers the benefit of the same platform to be able to create
their own experiments and gain high-level insights.
We identify dimensions of MDPs which help characterise environments, and we provide a platform
with different instantiations of these dimensions in order to understand the workings of different RL
algorithms better. To this end, we implemented a Python package, MDP Playground, that gives us
complete control over these dimensions to generate flexible environments.
Furthermore, commonly used environments, that RL algorithms are tested on, tend to take a long
time to run. For example, a DQN run on Atari took us 4 CPU days and 64GB of memory to run. Our
1
Under review as a conference paper at ICLR 2021
platform can be used as a low-cost testbed early in the RL agent development pipeline to develop and
unit test algorithms and gain quick and coarse insights into algorithms.
The main contributions of this paper are:
•	We identify dimensions of MDPs that have a significant effect on agent performance, both
for discrete and continuous environments;
•	We open-source a platform with fine-grained control over the dimensions; baseline RL
algorithms can be run on these in as little as 30 seconds on a single core of a laptop;
•	We study the impact of the dimensions on baseline agents in our environments;
•	We evaluate transfer of some of these dimensions to more complex environments.
2	Dimensions of Hardnes s
We begin by defining basic deterministic version of MDP, followed by a POMDP and then motivate
the dimensions of hardness. We define an MDP as a 7-tuple (S, A, P, R, ρo, γ, T), where S is the set
of states, A is the set of actions, P : S×A×S → S describes the transition dynamics, R : S ×A → R
describes the reward dynamics, ρo : S → R+ is the initial state distribution, γ is the discount factor
and T is the set of terminal states. We define a POMDP as an 9-tuple (S, O, A, P, Ω, R, ρ0, γ, T),
where O represents the set of observations, Ω: S X A X O → R describes the probability of an
observation given a state and action and the rest of the terms have the same meaning as for the MDP
above.
To identify the dimensions of hardness, we went over the components of MDPs and POMDPs and
tried to exhaustively list dimensions that could make an environment harder. This has resulted in
many dimensions and a highly parameterisable platform. To aid in understanding we categorise the
dimensions according to the component of (PO)MDPs they affect.
To clarify terminology, we will use information state to mean the state representation used by the
agent and belief state to be equivalent to the full observation history. If the belief state were to be used
as the information state by an agent, it would be sufficient to compute an optimal policy. However,
since the full observation history isn’t tractable to store for many environments, agents in practice
stack the last few observations to use as their information state which renders it non-Markov.
An implicit assumption for many agents is that we receive immediate reward depending on only the
current information state and action. However, this is not true even for many simple environments. In
many situations, agents receive delayed rewards (see e.g. Arjona-Medina et al., 2019). For example,
shooting at an enemy ship in Space Invaders leads to rewards much later than the action of shooting.
Any action taken after that is inconsequential to obtaining the reward for destroying that enemy ship.
In many environments, we obtain a reward for a sequence of actions taken and not just the information
state and action. A simple example is executing a tennis serve, where we need a sequence of actions
which would result in a point if we served an ace. In contrast to delayed rewards, rewarding a
sequence of actions addresses the actions taken which are consequential to obtaining a reward. Sutton
et al. (1999) present a framework for temporal abstraction in RL to deal with such sequences.
Environments can also be characterised by their reward sparsity (Gaina et al., 2019), i.e., the
supervisory reward signal is 0 throughout the trajectory and then a single non-zero reward is received
at its end. This also holds true for our example of the tennis serve above.
Another characteristic of environments that can significantly impact performance of algorithms is
stochasticity. The environment, i.e., dynamics P and R, may be stochastic or may seem stochastic
to the agent due to partial observability or sensor noise. A robot equipped with a rangefinder, for
example, has to deal with various sources of noise in its sensors (Thrun et al., 2005).
Environments also tend to have a lot of irrelevant features (Rajendran et al., 2018) that one need
not focus on. For example, in certain racing car games, though one can see the whole screen, we
only need to concentrate on the road and would be more memory efficient if we did. This holds for
table-based learners and approximators like Neural Networks (NNs). NNs additionally can even fit
random noise (Zhang et al., 2017) and having irrelevant features is likely to degrade performance.
Another aspect we want to motivate is that of representations. The same underlying state may
have many different external representations/observations, for example, feature space vs pixel space.
2
Under review as a conference paper at ICLR 2021
Mujoco tasks may be learnt in feature space vs directly from pixels, and Atari games can use the
underlying RAM state or images. For images, various image transformations (shift, scale, rotate, flip
and others; Hendrycks & Dietterich, 2019) may manifest as observations of the same underlying state
and can pose a challenge to learning.
Further, we identify several additional dimensions for continuous control problems. For instance, for
the task of reaching a target, we have target radius (see, e.g., Klink et al., 2019), a measure of the
distance from the target within which we consider the target was successfully reached, action range,
the range of actions that may be taken, action loss weight, a weight penalising actions, time unit, the
discretisation of time, and transition dynamics order, the order of P that is considered.
Dimensions of hardness of environments that we identify from the above
discussion are (with the (PO)MDP component they impact in brackets):
•	Reward Delay (R)	• Representations (O)
•	Sequence Length (R)	• Action Range (A)
•	Reward Sparsity (R)	• Time Unit (P)
•	Stochasticity (P and R)	• Target Radius (T)
•	Irrelevant Features (O)	• Dynamics Order (P)
While we include only these for the main paper, Appendix A lists all dimensions that are controllable
in MDP Playground. While many dimensions may seem a curse at first, it is also the nature of
RL that different dimensions tend to be important in different specific applications. For instance,
terminalstatecosts are very important for many environments, however for the continuous environ-
ments we considered here, they were absent and hence unimportant. So, we allow setting terminal
state costs (in addition to goal state rewards to distinguish good and bad terminal states) to be able to
define toy environments where this dimension will be important. Another example is of rewardscale.
The agents we tested here rescale or clip rewards already and the effects of this dimension are not as
important as they would be otherwise. The experiments here are only a glimpse into the power and
flexibility of MDP Playground. Users can even upload custom P s and Rs and custom images for
representations O and our platform will take care of injecting the other difficulties for them (wherever
possible). This allows users to control different dimensions in the same base environment and even
gain insights we haven’t had so far in our extensive experiments.
We now mathematically highlight some of our dimensions of hardness to aid understanding. The
information state of an agent to compute an optimal policy would need to stack the previous n + d
observation and action pairs from the environment where n denotes a sequence length and d denotes
a delay, i.e., a sequence of actions needs to be followed to obtain a reward which may be delayed by
a certain number of steps. Sparsity controls the number of elements in S that are rewardable.
Additionally, the continuous control dimensions can mathematically be described as follows. The
target radius sets T = {s | ks - stk2 < target radius}, where st is the target point. The action range
sets A to be equal to the Cartesian product of the ranges of the action dimensions where A ⊂ Rn .
The action loss weight, λ, shapes rewards to be R(s, a)shaped = R(s, a) 一 λ *∣∣a∣∣2. The time unit,
t, sets P(s, a) = s + t Pcont(s, a) dt where Pcont is the underlying continuous dynamics function.
The transition dynamics order, n, sets P to be in Cn, the set of functions differentiable n times.
3	MDP Playground
MDP Playground generates parameterised OpenAI Gym (Brockman et al., 2016) environments
to facilitate researchers to benchmark algorithms across dimensions of hardness. It is also easily
configurable, so that the environment difficulty can be controlled in a fine-grained manner. We now
briefly describe the discrete and continuous environments followed by implementation details of
selected dimensions.
Discrete Environments In the discrete case, S and A contain categorical elements, and we generate
random instantiations of P and R after the remaining dimensions have been set. The generated P
and R are deterministic and held fixed for the environment. We keep ρo to be uniform over the
non-terminal states, and T is fixed to be a subset of S based on a user-chosen terminal state density.
Continuous Environments In the continuous case, environments correspond to the simplest real
world task we could think of that would be worth solving, moving a rigid body to a target point,
3
Under review as a conference paper at ICLR 2021
similar to (Haarnoja et al., 2017; Klink et al., 2019). P is parameterised so that each action dimension
affects the corresponding space dimension. P is designed such that when the transition dynamics
order is n, the nth derivative of a is set to be equal to the action applied for time unit seconds on a
body with configurable inertia. This is integrated over time to yield the next state. R is designed such
that the reward for the current time step is the distance travelled towards the target since the last step.
Reward Delay We delay the reward for a state-action pair by a non-negative integer number of
timesteps, which we call the delay, d.
Rewardable Sequence Length For discrete environments, we reward only specific sequences of
states of positive integer length n.1 We consider states to be distinct along the sequences allowing for
(ISSljT-：)! sequences. For the continuous environment of moving to a target, n is variable since it
already corresponds to a real world task.
Reward Sparsity For discrete environments, we define the reward density rd of sequences in terms
of the fraction of possible sequences of length n that are actually rewarded by the environment,
given that n is constant. If numr sequences are rewarded, we define the reward density to be
rd = num,r/("[—!^α! and the sparsity as 1 - rd. For continuous environments, We do not define
sparsity, but rather allow having a sparse or dense environment using a make_denser configuration
option.
Stochasticity For discrete environments, We implement a transition noise t_n ∈ [0, 1]. With
probability t_n, an environment transitions uniformly at random to a state that is not the true
next state given by the generated P. We further implement a reward_noise。…∈ R and add a
normal random variable distributed according to N(0, σ2r.n) to the true reward. For continuous
environments both these noises are normally distributed and directly added to the states and reWards.
Irrelevant Features For discrete environments, we introduce new discrete dimensions. Each di-
mension i has its own transition function Pi which is independent of all other transition functions.
However, only one of these is relevant to calculate the reward function. Similarly, in continuous
environments we label existing dimensions of S and A as irrelevant and do not consider them in the
reward calculation.
Representations For discrete environments, when this aspect is enabled, each categorical state
is associated with an image of a regular polygon which becomes the externally visible state s to
the agent. This image can further be transformed by shifting, scaling, rotating or flipping, which
are applied at random to the polygon whenever an observation is generated. The transforms are
only applied within a range so that the polygon is always present in the image. Examples of some
generated states can be seen in Figure 8 in Appendix F. This dimension is currently not implemented
for continuous environments because it would be too expensive.
Very low cost of execution Experiments with MDP Playground are cheap, allowing academics
without special hardware to perform insightful experiments. Wall-clock times depend a lot on the
algorithm, network size and dimensions of hardness. Nevertheless, to give the reader an idea of the
runtimes involved, DQN experiments (with a network with 2 hidden layers of 256 units each) took
on average 35s for a complete run of DQN for 20 000 environment steps. In this setting we restricted
Ray RLLib (Liang et al., 2018) and the underlying Tensorflow (Abadi et al., 2015) to run on one core
of a laptop.2 This equates to roughly 30 minutes for the entire delay experiment shown in Figure
1a which was plotted using 50 runs (10 seeds × 5 settings for delay). Even when using the more
expensive continuous or representation learning environments, training was only about 3-5 times
slower. When parallelised on a cluster, complete experiments can finish in a few minutes.
4	Experiments and Results
To demonstrate the usefulness of our benchmark, we evaluated Rllib implementations (Liang et al.,
2018) of DQN (Mnih et al., 2015), Rainbow DQN (Hessel et al., 2018), A3C (Mnih et al., 2016)
on discrete environments and DDPG (Lillicrap et al., 2016), TD3 (Fujimoto et al., 2018) and SAC
1In general, d and n would not be constants in real world environments, but for our toy environments, we use
fixed n and d.
2core-i7-8850H CPU — The full CPU specifications for a single core can be found in Appendix P.
4
Under review as a conference paper at ICLR 2021
(Haarnoja et al., 2018) on continuous environments over grids of values for the dimensions of hardness
discussed above. We used fully connected networks except for pixel-based representations where we
used Convolutional Neural Networks (CNNs) (LeCun et al., 1989). We first describe the discrete
environment experiments, followed by those on continuous environments. All hyperparameters and
the tuning procedure we used are available in Appendix M.
4.1	Discrete environments
We set |S| and |A| to 8 and the density of terminal states to 0.25 for the experiments. The reward
scale is set to 1.0 whenever a reward is given by the environment. We generated random Ps that
were completely connected. This is done by selecting for each state s, for each action a, a random
successor state s0 , so that from each state there is a transition possible to every state in state space
through one of the available actions. We do this to keep a regular structure for the P . This leaves
the environments in our experiments with a high bias nature versus the high variance nature seen
in RL benchmarks in practice. Having a very complex P or R itself can introduce "noise" into the
evaluation of algorithms and require many iterations of training before we can see if the agent is
learning. We leave this for complex benchmarks to capture as they are closer to real world use cases.
For unit testing, especially, we need quick insights and given that most agents are agnostic to the
choice of P and R, it is beneficial to have what we term high bias environments to test whether
agents are learning. We discuss some more aspects of such a design in Appendix C.
Varying reward delay
Figures 1a-c, depict
the mean and standard
deviation over 10 runs
for various delays. One
run consists of 10 random
seeds for the algorithm
but uses a fixed seed for
the environment. We
plot the Area Under the
Curve (AUC) which takes
the mean over previous
training rewards. As can
be seen from the figure,
all algorithms perform
very well in the vanilla
environment where the
πτrτι
0	12	4	8
delay
(a) DQN delay
SequenceJength
(d) DQN seq len
F 50
M
(υ
CL
0
0	12	4	8
delay
(b) Rainbow delay
⅛ 50
I
K °	1	2	3	4
SequenceJength
(e) Rainbow seq len
delay
(c) A3C delay
P
to 50
SequenceJength
(f) A3C seq len
Figure 1: AUC of episodic reward at the end of training for different
agents for varying delays (top) and sequence lengths (bottom). Error
bars represent 1 standard deviation. Note the reward scales.
MDP is fully observable as the information state of the agent is equal to the MDP’s state. For all
algorithms, performance degrades in environments where the information state is non-Markov.
Performance clearly degrades more as the information state needed to compute the optimal policy
requires more observations to be stacked. It is interesting (and expected) that Rainbow DQN is
somewhat more robust than DQN. The plots also show that DQN variants are more robust to delay as
compared to A3C variants.
Varying rewardable sequence length Results here are qualitatively similar to the ones for delay.
However, we observe in Figures 1d-f that sequence length has a more drastic effect in terms of degra-
dation of performance. The improvements of Rainbow DQN over DQN are also more pronounced
for these harder problems.
The standard deviation in the preceding plots is high in many of the environments with non-Markov
information state. Sometimes, however, the algorithms still managed to perform well, which empha-
sises that algorithms can perform well even when their assumption of having a Markov information
state is violated; this is one of the possible explanatory factors for the fact that tuning seeds can lead
to good results (refer Figure 5, Henderson et al., 2018). This also leads to noisy learning curves
typically associated with RL as can be seen in Appendix J.
Varying representations For representations, we used image representations and applied various
transforms (shift, scale, rotate and flip) one at a time and also all at once. We observed that the more
transforms that are applied to the images, the harder it is for algorithms to learn, as can be seen in
5
Under review as a conference paper at ICLR 2021
PJeM,υκ
none s S f r sSrf
imagej:ransforms
(a) DQN
O C
5
PJeM
IΠ I in
none s S f r sSrf
image transforms
(c) A3C
εeM3H
2	4	8	16
image sh quant
(d) DQN shift
(b) Rainbow
Figure 2: AUC of episodic reward at the end of training for the different algorithms when varying
representation. ’s’ denotes shift (quantisation of 1), ’S’ scale, ’f’ flip and ’r’ rotate in the labels
in the first three subfigures and image_sh_quant represents quantisation of the shifts in the DQN
experiment for this. Error bars represent 1 standard deviation. Note the different reward scales.
Figures 2a-c. This was to be expected since there are many more combinations to generalise over
for the agent. What was unexpected was that the most problematic transform for the algorithms
to deal with was shift. Despite the spatial invariance learned in CNNs (LeCun, 2012), our results
imply that that seems to be the hardest one to adapt to. As these trends were strongest in DQN,
we evaluated further ranges for the individual transforms for DQN. shifting had the most possible
different combinations that could be applied to the images. Therefore, we quantised the shifts to
have fewer possible values. Figure 2d shows that DQN’s performance improved with increasing
quantisation of shift (⇒ fewer possible values). We noticed similar trends for the other transforms as
well, although not as strong as they do not have as many different values as shift (see Appendix G).
This indicates that shift and other types of invariance do not come for free and that one needs to have
sufficient amount of samples for the algorithm to become invariant to the transforms we desire. This
is also a good sanity check that our toy environments allow such insights and other potential insights
we may not yet be aware of.
Results for further dimensions and baselines can be found in the appendix. See Appendices G and I
that show additional interesting results for varying transition noise, reward noise, reward sparsity.
Further, even though our own focus with the benchmark is designing and unit testing deep RL, we
believe it is also a valuable tool for theoreticians and we evaluated tabular baselines Q-learning
(Sutton & Barto, 2018), Double Q-learning (van Hasselt, 2010) and SARSA (Sutton & Barto, 2018)
on the discrete non-image based environments with similar qualitative results to those for deep agents.
These can be found in Appendix H.
4.2	Continuous Environments
We set the state and action space dimensionalities to 2. The state space range for each dimension
was [-10, 10] while the default action space range was [-1, 1]. The task would terminate when an
algorithm would reach the target point, or after at most 100 timesteps. We focus on results for DDPG
as results for TD3 and SAC are qualitatively similar (see Appendix G).
(a) target radius rew.
W+十
0.1 0.2 0.5 1.0 2.0 4.0 8.0
time unit
2	3	4	6	10
state_space_dim
(e) target radius len.
0.1 0.25 0.5 1.0 2.0 4.0 8.0
action space max
(b) action range rew.
(f) action range len.
(c) time unit rew.
(g) time unit len.
3
45。
(υ
∣o
dims. rew.
(h) irr. dims. len.
2	3	4	6	10
stats space dim

Figure 3: AUC of episodic reward (top) and lengths (bottom) for DDPG at the end of training. Error
bars represent 1 standard deviation. Note the different y-axis scales.
Varying action range We observed that the total reward gets worse for action max > 1. Up until
the value of 1, the episode lengths decreased as we would desire (see Figures 3b & 3f). This can be
attributed to the fact that the exploration schedules for the studied agents take the max range available
and explore based on that. But, as can be seen from these results, tuning these ranges or adapting
exploration mechanisms can produce substantially better results.
6
Under review as a conference paper at ICLR 2021
Varying time unit We observed that increasing the time unit helps up to a point, until which the
episode lengths also go down (see Figures 3c & 3g). Thus, tuning this value to learn how long we
must play an action will also impact performance of algorithms significantly.
Varying target radius The target radius is a value which is generally set to a small enough value to
be able to say that the algorithm has reached the target. However, we noticed that, for small values,
all the continuous control agents oscillated around the target to reach it exactly. This can be observed
in Figure 3a and 3e, where we note that even though the task was learnt for different target radii, the
episode lengths were shorter for larger radii as the agents kept oscillating outside the radius. Even for
such a simple task all evaluated algorithms failed to adapt to performing fine-grained control near
the target. We hypothesise that the agents did not learn to slow down close to the goal. Given more
experience close to the goal, we expect the agents to be able to learn this behaviour.
Varying irrelevant features We observed that introducing irrelevant dimensions to the control prob-
lem, while keeping the number of relevant dimensions fixed to 2, decreased an agent’s performance
(see Figures 3d & 3h). We believe this is because irrelevant features interfere with the learning process.
Additional interesting results, including results for action loss weight can be found in Appendix G.
5	Using MDP Playground
We believe MDP Playground offers so much power and flexibility that we can only list a few sample
use cases here and let the user choose to explore further what even we might not have thought of.
5.1	Insights into Existing Agents
Design and Analyse Experiments We allow the user the power to inject dimensions into a base
environment in a fine-grained manner and analyse results using the 1-D plots from before or radar
plots with user defined weights for the plots. Since, different users might be interested in different
dimensions, these are loaded dynamically from the data. For instance, radar plots for the dimensions
we varied in our experiments can be seen as in Figures 4a and 4b.
transltlon-nolse
«
i mag⅛1traπsforms
reward.
State_SPaCe_dlm
——DDPG
——7B3
——SAC
、_W_______s__	——A3C
traπsιtιoπ-πoιse	___ q∩∣∣
— RAlNBOW
60	ʌʌ
qι∙
actlon-space2max------^tlme-unlt
delay
(a) discrete envs.
(b) cont. envs.
X 0.25
4
8 0.5
(c) reward density
(d) action range and time unit
3r
Figure 4: Analysing and Debugging
0.05
time_unit


Varying Multiple Dimensions It is possible to vary multiple dimensions at the same time in the
same base environment. For instance, Figure 4d shows the diagonal relationship between varying
the action range and time unit together. The platform allows this for all of dimensions that can be
controlled together. Even more such experiments can be found in Appendix I, including varying both
P and R noises together in discrete environments and interactions between P noise and the target
radius, and interaction of the order of the transition dynamics with both time unit and action range.
Transfer to complex environments We have designed wrappers for Atari and Mujoco which can be
used to inject some of the dimensions we have for toy environments as well. For instance, for time
unit, there is an optimal value of the time unit with decreasing performances on either side of this
optimum. For all three agents (Figure 5a-5c), we see peak performance for a time unit 0.4 smaller
than that of the tuned vanilla environment. The same holds for SAC on Pusher whereas DDPG and
TD3 failed to solve Pusher (see Appendix K). The three agents also peaked on the toy environment
at similar values (DDPG and TD3 at 0.2 and SAC at 0.5 closely followed by 0.2). All three agents
peaked on Reacher at time unit 1.0. We attribute this to the frame skip of Reacher being 2 while
that of HalfCheetah and Pusher is 5. What does not transfer from the toy environments is that the
underlying optimal time unit is much smaller for the complex environments (0.02s vs 0.2s). This is
7
Under review as a conference paper at ICLR 2021
IOOOO
(a) SAC time unit
° 0.2 0.4 1.0 2.0 4.0
time unit
P
⅝ 5000
ω
H 0
(b) DDPG time unit
0.2 0.4 1.0 2.0 4.0
tlme_unlt
P
≥5000
ω
(c) TD3 time unit
° 0.2 0.4 1.0 2.0 4.0
tlme-unlt
Figure 5: AUC of episodic reward at the end of training on HalfCheetah
varying time unit (bottom). Error bars represent 1 standard deviation.
Note the different y-axis scales.
likely because we require much more fine-grained control for high-dimensional environments. At the
same time, this also indicates time unit should not be infinitesimally small to achieve too fine-grained
control since there is an optimal time unit for which we should repeat the same action (Biedenkapp
et al., 2020). This also has important consequences in environments where acquiring observations is
expensive (Huang et al., 2019).
Here we have described
just one of the dimensions
and environments. How-
ever, we performed exten-
sive experiments for dif-
ferent environments and
dimensions in Appendix
D. They all supported our
argument that the high-
level trends for the dimen-
sions are similar to the
trends on the toy environments. This might seem obvious to some readers after the fact, but the fact
that these insights could have been gained directly on the toy environments without trying them on
complex environments shows the potential insights that may be gained on toy environments.
5.2	Designing New Agents
We hope our benchmark will help identify the inductive biases needed for designing new RL
algorithms without getting bogged down by other sources of "noise" in the evaluation just as MNIST
helped to identify the inductive bias of convolutions. MNIST by no means represents the true
distribution of image data in the real world but it retains the key properties needed to identify
inductive biases needed to perform well on image data. In the same way we believe we retain key
dimensions of the problems which need inductive biases to be identified for them. The fact that CNNs
can classify and learn even random noise (see, e.g., Zhang et al., 2017; Arpit et al., 2017) shows that
even random data should be sufficient to identify inductive biases as long as the key dimension is
present in the data.
5.3	Debugging Existing Agents
Analysing how an agent performs under the effect of various dimensions can reveal unexpected
aspects of an agent. For instance, when using bsuite agents, we noticed that when we varied our
environment’s reward density, the performance of the bsuite Sonnet DQN agent would go up in
proportion to the density (see Figure 4c). This did not occur for other bsuite agents. This seemed to
suggest something different for the DQN agent and when we looked at DQN’s hyperparameters we
realised that it had a fixed schedule while the other agents had decaying schedules. Such insights
can easily go unnoticed if the environments used are too complex. We know of many researchers who
have had frustrating experiences debugging in complex environments, where it’s unclear whether a
hyperparameter was the cause or the difficulty of the environment itself and such toy environments
aid debugging by removing the "noise" of complex, real environments.
6	Discussion and Related Work
To the best of our knowledge, we are the first to perform a principled study of how significant aspects
such as non-Markov information states, irrelevant features, representations and low-level dimensions,
like time discretisation, affect agent performance. MDP Playground facilitates easy study of the
presented dimensions, giving us an abstraction bridging MDPs and the real world.
Our framework is very well suited to designing, developing and unit-testing new algorithms. No
other benchmark that we know of offers fine-grained control over orthogonal dimensions of difficulty
for agents all of which can be applied in the same base environment.
In the case of Rainbow and vanilla DQN we demonstrated how our environments can be used to
compare extensions of algorithms and how one mitigates issues of the other. We show how learning
8
Under review as a conference paper at ICLR 2021
curve graphs with hardness inserted look reminiscent of typical RL curves. For example, for sequence
lengths (and other dimensions in Appendix J) when the information state is non-Markov, we see the
typical noise associated with RL algorithms (see, e.g., Henderson et al., 2018).
The concurrently developed behaviour Suite for RL (bsuite; Osband et al., 2019) is the closest related
work to MDP Playground. Osband et al. (2019) collect known (toy) environments from the literature
and use these to try and characterise agents based on their performance on these environments.
Instead of providing individually controllable dimensions of hardness, bsuite provides environments
that exhibit some of our dimensions of hardness that can not always be controlled by a user. Further,
most environments in bsuite can be seen as an intermediate step between our MDPs and even more
complex environments. One notable distinction between bsuite and MDP Playground is that bsuite
provides no evidence of relevance of the contained environments to more complex problems, whereas
we demonstrate that the identified trends on dimensions of hardness are not only relevant on toy
environments but also transfer to more complex environments. Finally, bsuite offers no continuous
benchmarks, whereas MDP Playground provides both discrete and continuous environments.
Maillard et al. (2014) defines a novel theoretical metric for defining hardness of MDPs. It captures
difficulties within MDPs when the true state of the MDP is known. However, a large part of the
hardness in our MDPs comes from the agent not knowing the optimal information state to use. It’d
be interesting to design a metric which captures this aspect of hardness as well.
Further benchmark environments include Procgen (Cobbe et al., 2019), Obstacle Tower (Juliani et al.,
2019) and Atari (Bellemare et al., 2013). Procgen adds various heterogeneous environments and
tries to quantify generalisation in RL. In a similar vein, Obstacle Tower provides a generalization
challenge for problems in vision, control, and planning. These benchmarks do not capture orthogonal
dimensions of difficulty and as a result, they do not have the same type of fine-grained control over
their environments’ difficulty and neither can each dimension be controlled independently. We view
this as a crucial aspect when benchmarking new algorithms. Dulac-Arnold et al. (2020) has some
overlapping dimensions with our platform but it consists of much more specific environments, and
only those that are continuous. This means, they are further down in the RL development pipeline
than MDP Playground or bsuite which target the toy environment domain and thus complementary to
our approach. More details on related work can be found in Appendix E.
We believe we need a curriculum of benchmarks for RL. In this hierarchy, our benchmark is a low-
level, high bias benchmark that should be the first test for newly implemented agents and bsuite comes
at the secondary level and other more specialised benchmarks that capture more specific environments
like (Cobbe et al., 2019; Cobbe et al., 2019; Juliani et al., 2019) are even more specialised.
7	Conclusion and Future Work
We introduced a low-cost benchmark to test RL algorithms in environments with varying and
controllable dimensions of hardness. We demonstrated their effects on well-known RL algorithms.
The platform allows us to disentangle various factors that make RL benchmarks hard by providing
fine-grained control over various dimensions. We further demonstrated how the performance of the
studied agents is adversely affected by some dimensions of hardness of environments which are under
the control of researchers even in real world environments, such as the time-unit. We will open-source
our code to facilitate better, cheaper, more reproducible, and more directed benchmarking in the RL
community. We also evaluated the transfer of the dimensions to more complex environments and
showed how similar challenges are present there as well. While we tried to exhaustively identify
dimensions of hardness, it is unlikely that we have captured all orthogonal dimensions of hardness in
RL. We welcome more dimensions that readers think will help us encapsulate further challenges in
RL.
We believe providing theoretical and practical researchers such a powerful tool is like providing a
programming language. Not every part of the MDP search space may be interesting to a user, but
they have the power to define their own toy MDPs easily with high-level primitives and not be limited
to the glimpse we have provided here. Future work should try to research a stronger link for toy
MDPs with real environments. We are convinced that MDP Playground can have a great impact on
RL research and is an important ingredient towards an era with more reproducible, generalisable and
better understood RL research.
9
Under review as a conference paper at ICLR 2021
References
M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. Corrado, A. Davis, J. Dean,
M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz,
L. Kaiser, M. Kudlur, J. Levenberg, D. Mane, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster,
J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas,
O. Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. TensorFlow: Large-scale
machine learning on heterogeneous systems, 2015. URL https://www.tensorflow.org/.
P. Abbeel, A. Coates, and A. Y Ng. Autonomous helicopter aerobatics through apprenticeship
learning. The International Journal ofRobotics Research, 29(13):1608-1639, 2010.
J.	A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter, and S. Hochre-
iter. RUDDER: return decomposition for delayed rewards. In H. M. Wallach, H. Larochelle,
A. Beygelzimer, F. d’Alche-Buc, E. B. Fox, and R. Garnett (eds.), Proceedings of the 32nd Inter-
national Conference on Advances in Neural Information Processing Systems (NeurIPS’19), pp.
13544-13555, 2019.
D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer,
A. C. Courville, Y. Bengio, and S. Lacoste-Julien. A closer look at memorization in deep networks.
In D. Precup and Y. W. Teh (eds.), Proceedings of the 34th International Conference on Machine
Learning, (ICML’17), pp. 233-242. PMLR, 2017.
M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An
evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279,
Jun 2013.
A. Biedenkapp, R. Rajan, F. Hutter, and M. Lindauer. Towards TempoRL: Learning when to act. In
Workshop on Inductive Biases, Invariances and Generalization in RL (BIG@ICML’20), July 2020.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. OpenAI
gym. arXiv:1606.01540 [cs.LG], June 2016.
P. Chrabaszcz, I. Loshchilov, and F. Hutter. Back to basics: Benchmarking canonical evolution
strategies for playing atari. In J. Lang (ed.), Proceedings of the Twenty-Seventh International Joint
Conference on Artificial Intelligence, (IJCAI’18), pp. 1419-1426. ijcai.org, 2018.
K. Chua, R. Calandra, R. McAllister, and S. Levine. Deep reinforcement learning in a handful of
trials using probabilistic dynamics models. In Proceedings of the 31st International Conference on
Advances in Neural Information Processing Systems (NeurIPS’18), pp. 4754-4765, 2018.
K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging Procedural Generation to Benchmark
Reinforcement Learning. arXiv:1912.01588 [cs.LG], Dec 2019.
K.	Cobbe, O. Klimov, C. Hesse, T. Kim, and J. Schulman. Quantifying generalization in reinforcement
learning. In K. Chaudhuri and R. Salakhutdinov (eds.), Proceedings of the 36th International
Conference on Machine Learning (ICML’19), pp. 1282-1289. PMLR, June 2019.
Gabriel Dulac-Arnold, Nir Levine, Daniel J. Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,
and Todd Hester. An empirical investigation of the challenges of real-world reinforcement learning.
CoRR, abs/2003.11881, 2020. URL https://arxiv.org/abs/2003.11881.
M. Fortunato, M. G. Azar, B. Piot, J. Menick, M. Hessel, I. Osband, A. Graves, V. Mnih, R. Munos,
D. Hassabis, O. Pietquin, C. Blundell, and S. Legg. Noisy networks for exploration. In Proceedings
of the International Conference on Learning Representations (ICLR’18), 2018. Published online:
iclr.cc.
S.	Fujimoto, H. van Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods. In J. G. Dy and A. Krause (eds.), Proceedings of the 35th International Conference on
Machine Learning (ICML’18), pp. 1582-1591. PMLR, 2018.
R. D. Gaina, S. M. Lucas, and D. Perez-Liebana. Tackling sparse rewards in real-time games
with statistical forward planning methods. In Proceedings of the 33rd Conference on Artificial
Intelligence (AAAI’19), pp. 1691-1698. AAAI Press, 2019.
10
Under review as a conference paper at ICLR 2021
T.	Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based
policies. In D. Precup and Y. W. Teh (eds.), Proceedings of the 34th International Conference on
Machine Learning, (ICML,17), pp. 1352-1361. PMLR, 2017.
T. Haarnoja, A. Zhou, P. Abbeel, and Sergey Levine. Soft Actor-Critic: Off-policy maximum entropy
deep reinforcement learning with a stochastic actor. In J. G. Dy and A. Krause (eds.), Proceedings
of the 35th International Conference on Machine Learning (ICML’18), pp. 1856-1865. PMLR,
2018.
P. Henderson, R. Islam, P. Bachman, J. Pineau, D. Precup, and D. Meger. Deep reinforcement learning
that matters. In S. A. McIlraith and K. Q. Weinberger (eds.), Proceedings of the Conference on
Artificial Intelligence (AAAI’18), pp. 3207-3214. AAAI Press, 2018.
D. Hendrycks and T. G. Dietterich. Benchmarking neural network robustness to common corruptions
and perturbations. In Proceedings of the International Conference on Learning Representations
(ICLR’19), 2019. Published online: iclr.cc.
M. Hessel, J. Modayil, H. van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot,
M. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforcement learning.
In S. A. McIlraith and K. Q. Weinberger (eds.), Proceedings of the Conference on Artificial
Intelligence (AAAI’18), pp. 3215-3222. AAAI Press, 2018.
Y. Huang, V. Kavitha, and Q. Zhu. Continuous-time markov decision processes with controlled
observations. In Proc. of Allerton’19, pp. 32-39. IEEE, 2019.
T. Jaakkola, S. P. Singh, and M. I. Jordan. Reinforcement learning algorithm for partially observable
markov decision problems. In G. Tesauro, D. S. Touretzky, and T. K. Leen (eds.), Proceedings of the
7th International Conference on Advances in Neural Information Processing Systems (NeurIPS’95),
pp. 345-352, 1995.
A. Juliani, A. Khalifa, V.P. Berges, J. Harper, E. Teng, H. Henry, A. Crespi, J. Togelius, and D. Lange.
Obstacle Tower: A Generalization Challenge in Vision, Control, and Planning. In S. Kraus (ed.),
Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence (IJCAI),
pp. 2684-2691. ijcai.org, Feb 2019.
P. Klink, H. Abdulsamad, B. Belousov, and J. Peters. Self-paced contextual reinforcement learning.
In L. P. Kaelbling, D. Kragic, and K. Sugiura (eds.), 3rd Annual Conference on Robot Learning,
(CoRL’19), pp. 513-529. PMLR, 2019.
Y. LeCun. Learning invariant feature hierarchies. In A. Fusiello, V. Murino, and R. Cucchiara (eds.),
Computer Vision - ECCV 2012, pp. 496-505. Springer, 2012.
Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropaga-
tion applied to handwritten zip code recognition. Neural Comput., 1(4):541-551, 1989.
E. Liang, R. Liaw, R. Nishihara, P. Moritz, R. Fox, K. Goldberg, J. E. Gonzalez, M. I. Jordan, and
I. Stoica. RLlib: Abstractions for distributed reinforcement learning. In J. Dy and A. Krause (eds.),
Proceedings of the 35th International Conference on Machine Learning (ICML’18), volume 80,
pp. 3059-3068. Proceedings of Machine Learning Research, 2018.
T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Continuous
control with deep reinforcement learning. In Y. Bengio and Y. LeCun (eds.), Proceedings of
the International Conference on Learning Representations (ICLR’16), 2016. Published online:
iclr.cc.
Odalric-Ambrym Maillard, Timothy A. Mann, and Shie Mannor. How hard is my mdp?" the
distribution-norm to the rescue". In Zoubin Ghahramani, Max Welling, Corinna Cortes, Neil D.
Lawrence, and Kilian Q. Weinberger (eds.), Advances in Neural Information Processing Systems
27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014,
Montreal, Quebec, Canada, pp. 1835-1843, 2014. URL http://papers.nips.cc/paper/
5441-how-hard-is-my-mdp-the-distribution-norm-to-the-rescue.
11
Under review as a conference paper at ICLR 2021
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A.
Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529-533, 2015.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In M. Balcan and K. Weinberger (eds.),
Proceedings of the 33rd International Conference on Machine Learning (ICML’16), volume 48,
pp. 1928-1937, 2016.
D. S. Nau. Pathology on game trees revisited, and an alternative to minimaxing. Artif. Intell., 21(1-2):
221-244, 1983.
I. Osband, Y. Doron, M. Hessel, J. Aslanides, E. Sezener, A. Saraiva, K. McKinney, T. Lattimore,
C. Szepezvari, S. Singh, B. Van Roy, R. Sutton, D. Silver, and H. Van Hasselt. Behaviour
suite for reinforcement learning. In Proceedings of the International Conference on Learning
Representations (ICLR’19), 2019. Published online: iclr.cc.
J. Pearl. Theoretical impediments to machine learning with seven sparks from the causal revolution.
In Y. Chang, C. Zhai, Y. Liu, and Y. Maarek (eds.), Proceedings of the Eleventh ACM International
Conference on Web Search and Data Mining, (WSDM’18), pp. 3. ACM, February 2018.
J. Rajendran, J. Ganhotra, S. Singh, and L. Polymenakos. Learning end-to-end goal-oriented dialog
with multiple answers. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii (eds.), Proceedings
of the Conference on Empirical Methods in Natural Language Processing (EMNLP’19), pp.
3834-3843. Association for Computational Linguistics, 2018.
R. Ramanujan, A. Sabharwal, and B. Selman. On adversarial search spaces and sampling-based
planning. In R. I. Brafman, H. Geffner, J. Hoffmann, and H. A. Kautz (eds.), Proceedings of the
20th International Conference on Automated Planning and Scheduling, (ICAPS’10), pp. 242-245.
AAAI, 2010.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXiv:1707.06347 [cs.LG], 2017.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner,
I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering
the game of go with deep neural networks and tree search. Nature, 529(7587):484-489, 2016.
Elizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89-96,
2007.
R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. The MIT Press, second
edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.html.
R.	S. Sutton, D. Precup, and S. Singh. Between mdps and semi-mdps: A framework for temporal
abstraction in reinforcement learning. Artificial intelligence, 112(1-2):181-211, 1999.
C. Thornton, F. Hutter, H. Hoos, and K. Leyton-Brown. Auto-WEKA: combined selection and
hyperparameter optimization of classification algorithms. In I. Dhillon, Y. Koren, R. Ghani,
T. Senator, P. Bradley, R. Parekh, J. He, R. Grossman, and R. Uthurusamy (eds.), The 19th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD’13), pp.
847-855. ACM Press, 2013.
S.	Thrun, W. Burgard, and D. Fox. Probabilistic robotics. Intelligent robotics and autonomous agents.
MIT Press, 2005.
E. Todorov, T. Erez, and Y. Tassa. MuJoCo: A physics engine for model-based control. In Interna-
tional Conference on Intelligent Robots and Systems (IROS’12), pp. 5026-5033. IEEE, 2012.
H. van Hasselt. Double q-learning. In J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and
A. Culotta (eds.), Proceedings of the 24th International Conference on Advances in Neural
Information Processing Systems (NeurIPS’10), pp. 2613-2621, 2010.
12
Under review as a conference paper at ICLR 2021
T.	Wang, X. Bao, I. Clavera, J. Hoang, Y. Wen, E. Langlois, S. Zhang, G. Zhang, P. Abbeel, and J. Ba.
Benchmarking model-based reinforcement learning. arXiv:1907.02057 [cs.LG], 2019.
C.	Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. In 5th International Conference on Learning Representations, (ICLR’17).
OpenReview.net, 2017.
13
Under review as a conference paper at ICLR 2021
A Dimensions of Hardnes s in MDP Playground
We list here the hardness dimensions for MDP Playground. Details on each meta-feature can be found
in the documentation for the class mdp_playground.envs.RLToyEnv in the accompanying
code.
•	Reward Delay
•	Rewardable Sequence Length
•	Reward Sparsity
•	P Noise
•	R Noise
•	Irrelevant Features
•	Transforms for Representation Learning
•	Reward Shift
•	Reward Scale
•	State space size/dimensionality
•	Action space size/dimensionality
•	Terminal State Density
•	Terminal State Reward
•	Relevant Dimensions (for both state and action spaces)
*	Only for discrete environments:
• Image Representations
* Only for Image Representations:
•	Shift Quantisation
•	Scale Range
•	Rotation Quantisation
* Only for continuous environments:
•	Target Point
•	Target Radius
•	Time Unit
•	Inertia
•	State Space Max
•	Action Space Max
•	Transition Dynamics Order
•	Reward Function
* Currently fixed hardness dimensions:
• Initial State Distribution
A.1 Additional sparsity option for sequences
With regard to sparsity, recall the tennis serve again. The point received by serving an ace would be a
sparse reward. We as humans know to reward ourselves for executing only a part of the sequence
correctly. Rewards in continuous control tasks to reach a target point (e.g. in Mujoco, Todorov
et al., 2012), are usually dense (such as the negative squared distance from the target). This lets the
algorithm obtain a dense signal in space to guide learning, and it is well known (Sutton & Barto,
2018) that it would be much harder for the algorithm to learn if it only received a single reward at the
target point. The environments in MDP Playground have a configuration option, make_denser, to
allow this kind of reward shaping to make the reward denser and observe the effects on algorithms.
To achieve this, when make_denser is T rue, the environment gives a fractional reward if a fraction
of a rewardable sequence is achieved in discrete environments. For continuous environments, for
the move to a target point reward function, this option toggles between giving a dense reward as
described in the main paper and giving a sparse reward when the agent is within the target radius.
14
Under review as a conference paper at ICLR 2021
B Algorithm for generating MDPs
Algorithm 1 Generating MDPs with MDP Playground
1:	Input:
2:	number of states |S |,
3:	number of actions |A|,
4:	reward delay d,
5:	length of rewardable sequences n,
6:	density of rewardable sequences rd,
7:	transition noise t_n or σ&n,
8:	reward noise。『_冗,
9:	reward_scale,
10:	reward_shift,
11:	term_state_reward,
12:	make_denser,
13:	terminal_state_density
14:	relevant_dimensions
15:	Hardness dimensions specific to continuous environments:
16:	target_point
17:	target_radius
18:	transition_dynamics_order
19:	time_unit
20:	inertia
21:
22:	function INIT_TRANSITION_FUNCTION():
23:	if discrete environment then
24:	for each state s do	. For generating a completely connected P
25:	Set possible successor states: S0 = S
26:	for each action a do
27:	Set P(s, a) = s0 sampled uniformly from S0 and remove s0 from S0
28:	if irrelevant features then
29:	Generate dynamics Pirr of irrelevant part of state space as was done for P
30:	else
31:	Do nothing as continuous environments have a fixed parameterisation
32:
33:	function INIT_REWARD_FUNCTION(n):
34:	if discrete environment then
35:	Randomly sample rd *("环#-：)! sequences and store in rewardable_sequences
36:	else
37:	Do nothing as continuous environments have fixed options for the reward function
38:
15
Under review as a conference paper at ICLR 2021
39:	function TRANSITION_FUNCTION(s, a):
40:	if discrete environment then
41:	s0 = P (s, a)
42:	if U(0, 1) < t_n then
43:	s0 = a random state in S \ {P (s, a)}	. Inject noise
44:	if irrelevant features then
45:	Execute dynamics Pirr of irrelevant part of state space and concatenate with s0
46:	if representation learning then
47:	s0 = image of corresponding polygon(s) with applied selected transforms
48:	else
49:	Set	n = transition_dynamics_order
50:	Set	an = a	.	Superscript	n represents nth	derivative
51:	Set	sn = an/inertia . Each	state	dimension	is controlled by	each	action dimension
52:	for	i in reversed(range(n)) do
n-i
53:	Set st+ι = P si+j ∙ 1 ∙ time_Unitj	.t is current time step.
j=0
54：	st+ι+ = N (0,σ2t_n)
55:	s0 = st+1
56:	return s0
57:
58:	function REWARD_FUNCTION(s, a):
59:	r = 0
60:	if irrelevant features then
61:	s = s[relevant_dimensions]	. Sub-select the part of state space relevant to
reward
62:	if discrete environment then
63:	if not make_denser then
64:	if state sequence of n states ending d steps in the past is in
rewardable_sequences then
65:	r = reward_scale
66:	else
67:	for i in range(n) do
68:	if sequence of i states ending d steps in the past is a prefix sub-sequence of a
sequence in rewardable_sequences then
69:	r+ = i/n
70:	else
71:	r = Distance moved towards the target_point
72：	r+ = N(0, σ2i)
73:	r* = reward_Scale
74:	r+ = reward_shift
75:	if reached terminal state then
76:	r+ = term_state_reward
77:	return r
78:
79:	function MAIN():
80:	INIT_TERMINAL_STATES()	. Set T according to terminal_state_density
81:	INIT_INIT_STATE_DIST()	. Set ρo to uniform distribution over non-terminal states
82:	init_transition_function()
83:	init_reward_function()
16
Under review as a conference paper at ICLR 2021
C Design Decisions
C.1 Regular structure of transition function for experiments
It is always possible to design adversarial Ps (Nau, 1983; Ramanujan et al., 2010) which can be
made arbitrarily hard to solve. So, we imposed a regular structure on P. If we have an environment,
say the Earth, where a big reward (e.g. hidden treasure) is placed in an unknown and deliberately
unexpected location. Then evaluating the intelligence of an agent on such an environment clearly
does not give us a true measure of agent intelligence. This is also a problem with many benchmark
environments, e.g., qbert has a bug which allows the agent to achieve a very large number of points
(Chrabaszcz et al., 2018) and breakout has a scenario where, if an agent creates a hole through the
bricks, it can achieve a very large number of points. Even though the latter is a sign of intelligent
behaviour, it skews the distribution of rewards and introduces variance in the evaluation. In keeping
with the low-level and high bias nature of our toy environments, we leave these to more complex
benchmarks with more variant environments and instead impose a regular structure on P . We do
allow completely random generation ofPs which do not have a regular structure and can have much
more variance in their connections, but as we noticed in initial experiments this produces variant
environments and leads to noisy evaluations. We also plan to let users also generate or provide their
own P so that they might play with toy versions of environments they are interested in.
17
Under review as a conference paper at ICLR 2021
D	Effect of dimensions on more complex benchmarks
We tested the trends of the dimensions on more complex Atari and Mujoco tasks. For Atari, we ran
the agents on beam_rider, breakout, qbert and space_invaders when varying the dimensions delay
and transition noise. For Mujoco, we ran the agents on HalfCheetah, Pusher and Reacher using
mujoco-py when varying the dimensions time unit and action range. We evaluated 5 seeds for 500k
steps for Pusher and Reacher, 3M for HalfCheetah and 10M (40M frames) for Atari. The values
shown for action range and time unit are relative to the ones used in Mujoco.
Varying transition noise We observe similar trends for injecting transition noise into Atari envi-
ronments for all three agents as for the toy environments. We also observe that for some of the
environments, transition noise actually helps improve performance. This has also been observed in
prior work (Wang et al., 2019). This happens when the exploration policy was not tuned optimally
since inserting transition noise is almost equivalent to -greedy exploration for low values of noise.
We also observed a similar effect for the toy environments in Figure 13 in Appendix G. However, we
also observe that performance drop is different for different environments. This is to be expected as
there are other dimensions of hardness which we cannot control or measure for these environments.
Varying reward delay We see that performance drops for the delay experiments when more delay is
inserted. For qbert (Figure 6c), these drops are greater on average across the agents. However, for
breakout (Figure 6b), in many instances, we don’t even see performance drops. In beam_rider (Figure
6a) and space_invaders (Figure 6d), the magnitude of these effects are intermediate to breakout and
qbert. This trend becomes clearer when we also look at Figures 82b-l in Appendix K. We believe
this is because large delays from played action to reward are already present in breakout, which
means that inserting more delays does not have as large an effect as in qbert (Figures 6c). Agents
are strongest affected in qbert which, upon looking at gameplay, we believe has the least delays
from rewarding action to reward compared to the other games. The trends for delay however were
noisier. Many considered environments tend to also have repetitive sequences which would dilute
the effect of injecting delays. Many of the learning curves in Appendix L, with delays inserted,
are indistinguishable from normal learning curves. We believe that, in addition to the motivating
examples, this is empirical evidence that delays are already present in these environments and so
inserting them does not cause the curves to look vastly different. In contrast, when we see learning
curves for transition noise, we observe that, as we inject more and more noise, training tends to a
smoother curve as the agent tends towards becoming a completely random agent.
To analyse transfer of dimensions between toy and complex benchmarks, we use the Spearman rank
correlation coefficient between corresponding toy and complex experiments for performance across
different values of the dimension of hardness. The Spearman correlation was >= 0.7 for 19 out of
24 experiments and a positive correlation for four of the remaining five. DQN with delays added on
breakout was the only experiment with correlation 0.
p,JeM9≈
(a) DQN beam_rider
O C
5
2
p,JeM9
rτππ
(c) DQN qbert (d) DQN space_invaders
00C
5
eM9
rrrm
0.0 0.01 0.02 0.1 0.25
transition noise
_________________
(b) DQN breakout
(f) DQN breakout
(g) DQN qbert
O C
2
eM9
(h) DQN space_invaders
(e) DQN beam_rider
Figure 6:	AUC of episodic reward for DQN on various environments at the end of training. Error
bars represent 1 standard deviation. Note the different y-axis scales.
Varying action range We observed similar trends as discussed prior, in that there was an optimal
value of action range with decreasing performances on either side of this optimum. Figure ?? shows
this for all considered agents on HalfCheetah (for SAC and DDPG, runs for action range values
>= 2 and >= 4 crashed and are absent from the plot). Qualitatively similar results on the other
environments are given in Appendix K. This supports the insight gained on our simpler environment
18
Under review as a conference paper at ICLR 2021
that tuning this value may lead to significant gains for an agent. For already tuned environments, such
as the ones in Gym, this dimension is easily overlooked but when faced with new environments setting
it appropriately can lead to substantial gains. In fact, even in the tuned environment setting of Gym, we
found that all three algorithms performed best for an action range 0.25 times the value found in Gym
for Reacher (Figures 83c, 83k, 83g in Appendix K). This observation is representative for the types
of insight our benchmark can yield for RL algorithm design, as ideally an agent would adaptively set
these bounds since these are under its control. Moreover, the learning curves in Appendix L further
show that for increasing action range the training gets more variant. The difference in performances
across the different values of action range is much greater in the complex environments. We believe
this is due to correlations within the multiple degrees of freedom as opposed to a rigid object in the
toy environment. Due to the high bias nature of the toy environments, the high level trend of an
optimal action range transfers to more complex ones.
m 10000-
M
S
O
0.1 0.25 0.5 1.0 2.0
action space max
0.1 0.25 0.5 1.0 2.0 4.0
action space max
0
-10000∣
0.1 0.25 0.5 1.0 2,0 4.0 8.0
action space max

(a) SAC action range (b) DDPG action range (c) TD3 action range
Figure 7:	AUC of episodic reward at the end of training on HalfCheetah
varying action range. Error bars represent 1 standard deviation. Note
the different y-axis scales.
19
Under review as a conference paper at ICLR 2021
E More on Related Work
Our benchmark is also designed with long term AGI in mind. Dimensions like identifying delays
and sequences will be essential to solving AGI. Most current algorithms lack such capability (Pearl,
2018).
Many of the other benchmarks mentioned in the main paper are largely vision-based, which means
that a large part of their problem solving receives benefits from advances in the vision community
while our benchmarks try to tackle pure RL problems in their most toy form. This also means that
our experiments are extremely cheap, making them a good platform to test out new algorithms’
robustness to different challenges in RL.
A parallel and independent work along similar lines as the MDP Playground, which was released
a month before ours on arXiv, is the Behaviour Suite for RL (bsuite, Osband et al. (2019)). In
contrast to our generated benchmarks, that suite collects simple RL benchmarks from the literature
that are representative of various types of problems which occur in RL and tries to characterise RL
algorithms. Unlike their framework, where currently there is no toy environment for Hierarchical
RL (HRL) algorithms, the rewardable sequences that we describe would also fit very well with HRL.
Additionally, we also have a toy continuous environment whereas bsuite currently only has discrete
environments. An important distinction between the two platforms could be summed up by saying
that they try to characterise algorithms while we try to characterise environments with the aim that
new adaptable algorithms can be developed that can tackle environments of desired difficulty. They
also do not generate completely random P and R for their environments like we do, which would
help avoid algorithms overfitting to certain benchmarks.
For some readers, it might feel obvious that injecting many of these dimensions causes difficulties for
agents. But to the best of our knowledge no other work has tried to collect all orthogonal dimensions
in one place and study them comprehensively and what aspects transfer from toy to more complex
environments.
The nature of the toy environments is one of high bias. We believe that the transfer of the hardness
dimensions from toy to complex environments occurs because the algorithms we have tested are
environment agnostic and usually do not take aspects of the environment into account. Q-learning for
instance is based on TD-errors and the Bellman equation. The equation is agnostic to the environment
and while adding deep learning may help agents learn representations better, it does not remove the
problems inherent in deep learning. While it’s nice to have general algorithms that may be applied in
a black box fashion, by studying the dimensions we have listed and their effects on environments, we
will gain deeper insights into what is needed to design better agents.
An additional comment can be made comparing the continuous and discrete complex environments
comparisons to the toy benchmarks. The noise in comparing the discrete environments was higher
and we believe this is due to the discrete environments being much more sparse and having many
more lucky areas that can be exploited as with the qbert bug and breakout strategy mentioned. In
comparison, continuous environments usually employ a dense reward formulation in which case the
value functions are likely to be continuous.
Algorithms like DQN (Mnih et al., 2015) were applied to many varied environments and produce
very variable performance across these. In some simple environments, DQN’s performance exceeds
human performance by large amounts, but in other environments, such as Montezuma’s revenge,
performance is very poor. For some of these environments, e.g. Montezuma’s revenge, we need a
very specific sequence of actions to get a reward. For others, there are different delays in rewards. A
problem with evaluating on these environments is that we have either no control over their difficulty
or little control such as having different difficulty levels. But even these difficulty levels, do not
isolate the confounding factors that are present at the same time and do not allow us to control the
confounding factors individually. We make that possible with our Dimensions.
When designing the platform, we went over the components of an MDP and tried to exhaustively
add as many parameterisable dimensions as possible with the condition that they are all orthogonal
and can be applied independently of each other. In a sense, this is an attempt to capture fundamental
dimensions of hardness in the same way that human cognition is founded, in part, on four different
systems and endow humans with abstract reasoning abilities (Spelke & Kinzler, 2007). We don’t
try to capture, say credit assignment or generalisation as dimensions. These are to be dealt with
20
Under review as a conference paper at ICLR 2021
at a higher level the same way that intelligent behaviour and reasoning arise from the interplay of
different underlying cognitive systems but which process objects or space at a lower sensory level.
E.1	Debugging with MDP Playground
We discuss here further 2 more examples of how the toy environments helped us debug RL algorithms
in practice.
When merging some of our environments into bsuite, we noticed that when we varied sparsity, the
performance of their DQN agent would go down in proportion to the environment’s sparsity. This did
not occur for other agents. This seemed to suggest something different for the DQN agent and when
we looked at DQN’s hyperparameters we realised that it had a fixed schedule. While that may be
desirable in some situations, we felt it hurt DQN’s performance because it was not allowed to explore
enough early on nor exploit what it learnt fully later. When we use regular structured environments,
the agent performances are freed of the "noise" that is present due to irregular transition functions
and this makes it easy to see high-level trends.
In the Ray version we used, we observed that the noisy nets (Fortunato et al., 2018) implementation
was broken on the toy environments and then we observed the same on more complex benchmarks.
This makes it easy to debug if something major is broken.
21
Under review as a conference paper at ICLR 2021
F Sample states used for Representation Learning
(b) Shift
(c) Scale
(d) Rotate
(a) No transforms
(e) All transforms
Figure 8: When using the meta-feature representation learning in discrete environments, each
categorical state corresponds to an image of a polygon (if the states were numbered beginning from
0, each state n corresponds to a polygon with n + 3 sides). Various transforms can be applied to the
polygons randomly at each time step. Samples shown correspond to states 3 and 0
22
Under review as a conference paper at ICLR 2021
G More Experiments and Additional Reward Plots
We continue with the experiments and results from the main paper here.
G. 1 Results for varying transition and reward noises
transition noise	transition noise	transition noise	transition noise
_ __________________________________________ _________________________________________ _________________________________________
(a) DQN	(b) Rainbow	(c) A3C	(d) A3C + LSTM
25
(a) DQN	(b) Rainbow
Figure 10: Mean episodic reward at the end of training for the different algorithms when varying
reward noise. Error bars represent 1 standard deviation. Note the different reward scales.
Figure 9: Mean episodic reward at the end of training for the different algorithms when varying
transition noise. Error bars represent 1 standard deviation. Note the different reward scales.
(c) A3C	(d) A3C + LSTM
We see a similar trend during training, as for delays and sequences, when we vary the transition noise
in Figure 9 and the reward noise in Figure 10. Performance degrades gradually as more and more
noise is injected. It is interesting that, during training, all the algorithms seem to be more sensitive to
noise in the transition dynamics compared to the reward dynamics: transition noise values as low as
0.02 lead to a clear handicap in learning while for the reward dynamics (with the reward scale being
1.0) reward noise standard deviation of。…=1 still resulted in learning progress.

(c) A3C
(a) DQN	(b) Rainbow
0.0 0.01 0.02 0.1 0.25
transition^ oise
(d) A3C + LSTM
Figure 11:	Mean episodic reward for evaluation rollouts (max 100 timesteps) at the end of training for
the different algorithms when varying transition noise. Error bars represent 1 standard deviation.
Interestingly, when we plot the evaluation performances3 in Figures 11 and 12, we see, on comparing
with the training plots, that the training performance of the algorithms is more sensitive to noise in
the transition dynamics (Figure 9) than the eventual evaluation performance is (Figure 11). While it
is obvious that the mean episodic reward during training would be perturbed when noise is injected
into the reward function, it is non-trivial that injecting noise into the transition function still leads
to good learning (as displayed in the evaluation rollout plots). An additional seeming anomaly is
that the evaluation rollouts for A3C variants especially (and DQN to a small extent), suggest that
it performs better in the presence of transition noise. This might indicate that A3C in the presence
of no transition noise does not explore enough (as was also conjectured in the unexpected results
for varying the sparsity meta-feature) and is actually helped when transition noise is present during
training.
As we mentioned earlier, one of the advantages of our platform is that it allows us to introduce all the
hardness dimensions on the same base environment at the same time. This is helpful to understand
interaction effects between them. We plot the most interesting interaction effects in Figure 13 where
we varied both transition and reward noise over respective grid values. This plot shows that our
3Here, for evaluation, and not for training because training is in the noisy environment, we evaluated in the
corresponding environment without noise to assess how well the true learning is proceeding.
23
Under review as a conference paper at ICLR 2021
0	1	5	10 25
reward noise
_________
reward noise
(b) Rainbow
O"
5
p」eMH
O 1	5	10
reward noise
_______
(c) A3C
25
X 50
25
reward noise
_______
(d) A3C + LSTM
(a) DQN
Figure 12:	Mean episodic reward for evaluation rollouts (max 100 timesteps) at the end of training
for the different algorithms when varying reward noise. Error bars represent 1 standard deviation.
Note the different reward scales.
observation, that transition noise helps A3C out during evaluation, is only clearly valid when the
reward noise is not so high 9r_n <= 1) as to disrupt training. The corresponding heatmap plot
for training when varying the noises and additional ones for jointly varying delay and rewardable
sequence length are present in the Appendix (Figures 37 - 39).
0.01
δ 0.02
髀25
80
40™
20
(a) DQN
5	10
rewarLnoiSe
0.01
δ 0.02
0.25
80
60R
40≡{
20
(b) Rainbow
5	10
rewarLnoiSe
0.01
δ 0.02
髀25
(c) A3C
5	10
rewarLnoiSe
80
20
0
6%
40
80
60R
4喧
20
0
rewarLnOiSe
(d) A3C + LSTM
0
0
Figure 13:	Mean episodic reward for evaluation rollouts (max 100 timesteps) at the end of training
for the different algorithms when varying transition and reward noise.
O O
5
p,JBMα
0.25 0.5 0.75
reward density
G.2 Results for varying reward sparsity
p,JBMωα
O O
5
p,JBMα
0.25 0.5 0.75
reward density
(a) DQN	(b) Rainbow	(c) A3C	(d) A3C + LSTM
Figure 14:	Mean episodic reward at the end of training for the algorithms when varying reward
sparsity. Error bars represent 1 standard deviation. Note the different reward scales.
Figure 14 shows the results of controlling the meta-feature sparsity in the environment. The DQN
variants were able to learn the important rewarding states in the environment even when these were
sparse while the behaviour of A3C was unexpected. One explanation could be that A3C’s exploration
was not very good, in which case increasing reward density would help as in Figure 14c. But
adding in an LSTM to the A3C agent seems to show the opposite trend (Figure 14d) as increasing
reward density leads to worsening performance. This could indicate that having a greater density of
rewarding states makes it harder for the LSTM to remember one state to stick to. This behaviour of
A3C warrants more investigation in the future.
We have observed A3C is more variant in general than its DQN counterparts and this should be
expected as it launches and collects data from several instances of the same environment which
induces more variance.
The make_denser configuration option, makes learning smoother and less variant across different
runs of an algorithm, as can be seen in Figure 15a for DQN when compared to Figure 17 for corre-
sponding sequence lengths. To evaluate the true learning of algorithms, we turn off the make_denser
option in the evaluation rollouts. The learning curves for these can be seen for DQN in Figure 15b.
The agent still does not perform as well as might be expected when making the reward signal denser
during training. This is probably due to the sequence lengths still violating the complete observability
24
Under review as a conference paper at ICLR 2021
assumption made by the algorithm. The plots for learning curves for the remaining algorithms are
present in Figures 70-75. The plots for final mean reward during training and evaluation are given in
Figures 14 and 21.
sequence length 3	- sequence length 4
sequence length 2
p ιoo
5
S 50
10000	20000
Train Timesteps
peMωα
peMωα
10000	20000	10000	20000
Train Timesteps	Train Timesteps
(a) Training Learning Curves
sequence length 3	sequence length 4
sequence length 2
150
ElOO
150
CC 50
0
10000	20000
Train Timesteps
peMωα
10000	20000
Train Timesteps
600
⅛400
S 200
0
10000	20000
Train Timesteps
(b) Evaluation Learning Curves
Figure 15:	Learning curves for DQN when make_denser is True for rewardable sequences. Please
note the different Y-axis scales and the fact that with longer rewardable sequences, a greater number
of seeds do not learn anything for the evaluation rollouts (reward ≈ 0).
G.3 Further results for varying reward delays and sequences
0	12	4	8
delay
SequenceJength
(a) A3C + LSTM delay (b) A3C + LSTM seq len
Figure 16:	Mean episodic reward at the end of training for different agents for varying delays (top)
and sequence lengths (bottom). Error bars represent 1 standard deviation. Note the reward scales.
sequence length 1	sequence length 2	sequence length 3	sequence length 4
50
5000 10000 15000 20000	5000 10000 15000 20000
Train Timesteps	Train Timesteps
Figure 17: Train Learning Curves for 10 runs with different seeds for DQN when varying sequence lengths.
Please note that each different colour corresponds to one of 10 seeds in each subplot.


5000 10000 15000 20000	5000 10000 15000 20000
Train Timesteps	Train Timesteps
Note that we varied delay on a logarithmic scale and sequence length on a linear one which means
that this effect is more pronounced than may first appear when looking at the figures. We additionally
also plot the learning curves, when varying sequence lengths, in Figure 17. We see how training
proceeds much more smoothly and is less variant across different seeds for the vanilla environment
(where the sequence length is 1) and that the variance across seeds is very large for sequence length 3.
G.4 Selecting Total Timesteps for Runs
We ran the experiments and plot the results for DQN variants up to 20 000 environment timesteps
and the ones for A3C variants up to 150 000 time steps since A3C took longer4 to learn as can be
seen in Figure 18. We refrain from fixing a single number of timesteps for our environments (as,
e.g., bsuite does), since the study of different trends for different families of algorithms will require
different numbers of timesteps. Policy gradient methods such as A3C are slower in general compared
to value-based approaches such as DQN. Throughout, we always run 10 seeds of all algorithms to
4In terms of environment steps. Wallclock time used was still similar.
25
Under review as a conference paper at ICLR 2021
10
10
50
p-leMωα
IO4	IO5
Train timesteps
03
1
O
Figure 18:	Evaluation rollouts (limited to 100 timesteps per episode) for DQN and A3C in the vanilla
environment which shows that DQN learns faster than A3C in terms of the number of timesteps.
obtain reliable results. We repeated many of our experiments with an independent set of 10 seeds and
obtained the same qualitative results.
O
p,l",M3α
(a) DQN
p",M3α
(b) Rainbow
p",M3α
O
(c) A3C
p",M3α
(d) A3C + LSTM
Figure 19:	Mean episodic reward for evaluation rollouts (limited to 100 timesteps) at the end of
training for the different algorithms when varying delay. Error bars represent 1 standard deviation.

50
p<JeMocc
0
12	3	4
sequence length
(a) DQN
O O
5
p<JeM
(b) Rainbow
12	3	4
sequence length
(c) A3C
p<JeMocc
50
o
12	3	4
sequence length
(d) A3C + LSTM
Figure 20: Mean episodic reward for evaluation rollouts (limited to 100 timesteps) at the end of
training for the different algorithms when varying sequence lengths. Error bars represent 1 standard
deviation.
reward density
reward density
(a) DQN
(b) Rainbow
reward density
(c) A3C
PJroMωα
(d) A3C + LSTM

Figure 21:	Mean episodic reward for evaluation rollouts (limited to 100 timesteps) at the end of
training for the different algorithms when varying reward sparsity. Error bars represent 1 standard
deviation.
proMωα
200
I 250
ω
B 0
2	3	4
sequence length
proMωα
sequence length
proMωα
sequence length
sequence length
0
2	3	4
(a) DQN	(b) Rainbow	(c) A3C	(d) A3C + LSTM
Figure 22:	Mean episodic reward at the end of training for the different algorithms when make_denser
is True for rewardable sequences. Error bars represent 1 standard deviation.
26
Under review as a conference paper at ICLR 2021
proMωα
250
°	2	3	4
sequence length
P
5 250
ω
°	2	3	4
sequence length
proMωα
250
°	2	3	4
sequence length
proMωα
200
°	2	3	4
sequence length
(a) DQN
(b) Rainbow
(c) A3C
(d) A3C + LSTM
Figure 23:	Mean episodic reward for evaluation rollouts (limited to 100 timesteps) at the end of
training for the different algorithms when make_denser is True for rewardable sequences. Error
bars represent 1 standard deviation.
(a) A3C + LSTM
O 07
5 æ
p-eM(
ΓΓΓΠ
I	ET U ∖
LMJMiUmmmJumJuulUlW uɪ J
image scale range
O O
5
p」eM ①
(c) DQN rotate
(b) DQN scale
Figure 24: Mean episodic reward at the end of training for the different algorithms when varying
representation learning. ’s’ represents shift, ’S’ represents scale, ’f’ represents flip and ’r’ represents
rotate in the labels in the first subfigure. scale_range represents scaling ranges in the second subfigure.
image_ro_quant is represents quantisation of the rotations in the third subfigure. Error bars represent
1 standard deviation.
2∙50∙0
0.1 0.25 0.5 1.0 2.0 4.0 8.0
action space max
5 O
PJeM"H
0.1 0.2 0.5 1.0 2.0 4.0 8.0
seM3H
(a) target radius rew.
(e) target radius len.
(b) action max rew.
≡l100
W
7?
8 °
(f) action max len.
(c) time unit rew.
(g) time unit len.
(d) irr. dims. rew.
(h) irr. dims. len.
Figure 25:	Mean episodic reward (above) and lengths (below) for TD3 at the end of training. Error
bars represent 1 standard deviation.
IlllI
PJeM*
5 O
p∙leM9α
5 O
PJeMəɑ
0.1 0.25 0.5 1.0 2.0 4.0 8.0
action space max
PJeM*
0.05 0.1 0.25 0.5 1.0
target radius
(a) target radius rew.
(e) target radius len.
(b) action max rew.
(f) action max len.
■■■■■・一
0.1 0.2 0.5 1.0 2.0 4.0 8.0
time unit
(c) time unit rew.
(g) time unit len.
(d) irr. dims. rew.
(h) irr. dims. len.
Figure 26:	Mean episodic reward (above) and lengths (below) for SAC at the end of training. Error
bars represent 1 standard deviation.
27
Under review as a conference paper at ICLR 2021
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(b)	TD3 episode rew.
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(a)	DDPG episode rew.
(c)	SAC episode rew.
(d) DDPG episode len.
uα,-lupo-dlu
(e) TD3 episode len.
(f) SAC episode len.
Figure 27: Mean episodic reward (above) and lengths (below) at the end of training for evaluation
rollouts for DDPG, TD3 and SAC when varying action_loss_weight. Error bars represent 1 standard
deviation.
5 0 5
-
PJeM
0.01 0.1 1.0 5.0 10.0 100.0
action」OSSJWeight
(a)	DDPG episode rew.
pl°
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(c)	SAC episode rew.
(b)	TD3 episode rew.
(d) DDPG episode len.
uα,-lupo-dlu
(e) TD3 episode len.
uα,-lupo-dlu
(f) SAC episode len.
Figure 28:	Mean episodic reward (above) and lengths (below) at the end of training for evaluation
rollouts for DDPG, TD3 and SAC when varying action_loss_weight. Error bars represent 1 standard
deviation.
5 0 5
-
PJeM
0.01 0.1 1.0 5.0 10.0 100.0
action」OSSJWeight
(a)	DDPG episode rew.
pl°
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(c)	SAC episode rew.
(b)	TD3 episode rew.
(d) DDPG episode len.
uα,-lupos-dlu
(e) TD3 episode len.
uα,-lupos-dlu
(f) SAC episode len.
Figure 29:	Mean episodic reward (above) and lengths (below) at the end of training for evaluation
rollouts for DDPG, TD3 and SAC when varying action_loss_weight. Error bars represent 1 standard
deviation.
28
Under review as a conference paper at ICLR 2021
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(b)	TD3 episode rew.
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(a)	DDPG episode rew.
(c)	SAC episode rew.
(d) DDPG episode len.
uα,-lupo-dlu
(e) TD3 episode len.
(f) SAC episode len.
Figure 30: Mean episodic reward (above) and lengths (below) at the end of training for evaluation
rollouts for DDPG, TD3 and SAC when varying action_loss_weight. Error bars represent 1 standard
deviation.
5 0 5
-
PJeM
0.01 0.1 1.0 5.0 10.0 100.0
action」OSSJWeight
(a)	DDPG episode rew.
pl°
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(c)	SAC episode rew.
(b)	TD3 episode rew.
(d) DDPG episode len.
uα,-lupo-dlu
(e) TD3 episode len.
uα,-lupo-dlu
(f) SAC episode len.
Figure 31:	Mean episodic reward (above) and lengths (below) at the end of training for evaluation
rollouts for DDPG, TD3 and SAC when varying action_loss_weight. Error bars represent 1 standard
deviation.
5 0 5
-
PJeM
0.01 0.1 1.0 5.0 10.0 100.0
action」OSSJWeight
(a)	DDPG episode rew.
pl°
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
0.01 0.1 1.0 5.0 10.0 100.0
action_loss_weight
(c)	SAC episode rew.
(b)	TD3 episode rew.
(d) DDPG episode len.
uα,-lupos-dlu
(e) TD3 episode len.
uα,-lupos-dlu
(f) SAC episode len.
Figure 32:	Mean episodic reward (above) and lengths (below) at the end of training for evaluation
rollouts for DDPG, TD3 and SAC when varying action_loss_weight. Error bars represent 1 standard
deviation.
29
Under review as a conference paper at ICLR 2021
H Plots for tabular baselines
p 50
ro
M
ω
CC 0
50
0	12	4	8
delay
0	12	4	8
delay
(c) Double Q-Learning
(a) Q-Learning
(b) SARSA
Figure 33:	Mean episodic reward (limited to 100 timesteps) at the end of training for three different
tabular baseline algorithms when varying reward delay. Error bars represent 1 standard deviation.
W O
eMΦ
SequenceJength
SARSA
V O
eMΦ
(c) Double Q-Learning
Figure 34:	Mean episodic reward (limited to 100 timesteps) at the end of training for three different
tabular baseline algorithms when varying sequence length. Error bars represent 1 standard deviation.
》 O
-
peMα
(a) Q-Learning
O 1	5 IO 25
reward noise
reward noise
(c) Double Q-Learning
(b) SARSA
Figure 35: Mean episodic reward (limited to 100 timesteps) at the end of training for three different
tabular baseline algorithms when varying reward noise. Error bars represent 1 standard deviation.
peMωα
(a) Q-Learning
(b) SARSA	(c) Double Q-Learning
Figure 36: Mean episodic reward (limited to 100 timesteps) at the end of training for three different
tabular baseline algorithms when varying transition noise. Error bars represent 1 standard deviation.
30
Under review as a conference paper at ICLR 2021
I Plots for varying 2 hardness dimensions together
12 4 8
A,-3 P
2	3
SequenceJength
(a) DQN
20
A,°θp
ReWard
60
2	3
SequenceJength
ReWard
QQQO
8 6 4 2
2 4 8
A,θp
2	3
SequenceJength
(c) A3C
80
ReWard
Q Q
6 4
20
12 4 8
A,θp
2	3
SequenceJength
ReWard
iQgo
8 6 4 2
(b) Rainbow
(d) A3C + LSTM
A,°θp
2	3
SequenceJength
(a) DQN
Figure 37: Mean episodic reward at the end of training for the different algorithms when varying
delay and sequence lengths. Please note the different colorbar scales.
2	3
SequenceJength
ReWard
A,°θp
ReWard
A,°θp
2	3
SequenceJength
(c) A3C
ReWard
U O
b 4
A,°θp
2	3
SequenceJength
ReWard
(b) Rainbow
(d) A3C + LSTM
0
Figure 38:	Mean episodic reward for evaluation rollouts (limited to 100 timesteps) at the end of
training for the different algorithms when varying delay and rewardable sequence lengths. Please
note the different colorbar scales.
80
60.
[40t
20
(a) DQN
0.01
3
CA
O 0.02
c
I
o 0.1
I 0-25
1	5	10
reward noise
_______
80
6°r
n
40I
20
0.01
S
O 0.02
o 0.1-
l 0-25-
1	5	10
reward noise
_______
ReWard
Oooo
8 6 4 2
I 0-25
1	5	10
reward noise
(b) Rainbow	(c) A3C	(d) A3C + LSTM
Figure 39:	Mean episodic reward at the end of training for the different algorithms when varying
transition noise and reward noise. Please note the different colorbar scales.
31
Under review as a conference paper at ICLR 2021
SequenceJength
2	3
SequenceJength
20
R
e
15∣
o.
io»
D
-5 ?
」0
SequenceJength
SequenceJength
(a) DQN	(b) Rainbow
(c) A3C
(d) A3C + LSTM
Figure 40:	Standard deviation of mean episodic reward at the end of training for the different
algorithms when varying delay and sequence lengths. Please note the different colorbar scales.
(a) DQN
ReWard Sfd Dev∙
12 4 8
A,θp
ReWard Sfd Dev∙
0 5 0
2 115
12 4 8
A,θp
2	3
sequence length
ReWard Sfd Des
0 5。，
2 115
A,°θp
ReWard Sfd Dev∙
5 Q I
r-Ir-IF5P
(b) Rainbow
(c) A3C
(d) A3C + LSTM
Figure 41:	Standard deviation of mean episodic reward for evaluation rollouts (limited to 100
timesteps) at the end of training for the different algorithms when varying delay and rewardable
sequence lengths. Please note the different colorbar scales.
0.01-
3
CA
o 0.02
U]
O o.l-
s
I 0-25-
1	5	10
reward noise
_______
o.oι-
cυ
CA
o 0.02
U]
O o.l-
s
I 0-25-
1	5	10
reward noise
_______
0.01-
ω
<Λ
o 0.02
o 0.1-
髀.25-
1	5	10
reward noise
(c) A3C
ReWard saDev.
6 4 2
o.oι-
cυ
CA
o 0.02
U]
O o.l-
s
I 0-25-
1	5	10
reward noise
R5*ardMtdDev∙
115
(a) DQN	(b) Rainbow
(d) A3C + LSTM
Figure 42:	Standard deviation of mean episodic reward at the end of training for the different
algorithms when varying transition noise and reward noise. Please note the different colorbar
scales.
0.01
3
S
,o 0.02
U]
O 0.1
I 0-25
5	10
reward noise
30
?
2。1
S
端
<
0
0.01-
δ 0.02
δ o.ι-
四25
5	10
reward noise
ReWard Std Dev∙
5 O
2Q,
1 1 7 5 2 0
0.01
3
S
O 0.02
U]
O 0.1
I 0-25
5	10
reward noise
5Rewa0asΞDev∙
115
(a) DQN	(b) Rainbow
(c) A3C
(d) A3C + LSTM
Figure 43:	Standard deviation of mean episodic reward at the end of training for evaluation rollouts
(limited to 100 timesteps) at the end of training for the different algorithms when varying transition
noise and reward noise. Please note the different colorbar scales.
32
Under review as a conference paper at ICLR 2021
0.02
0.1
⅞ 0.25
(Λ
U
B
匕0.5
0.25
0.5
target_radius
ReWard
6 4 2
ll0
0.02
ω
S
o 0.1
Ul
≡ 0.25
S
⊂
上0.5
0.25 0.5
targetradius
瞪sode -en mean
。QlQ-O
8 6 4 2
0.02
0.1
⅞ 0.25
(Λ
U
B
匕0.5
0.25 0.5
target_radius
ReWard
6 4 2
0.02
0.25
0.5
targetradius
e噂濡」姬mosn
8 6 4 2
q0
(a)	TD3 episode rew.
(b)	TD3 episode len.
(c)	SAC episode rew.
(d)	SAC episode len.
Figure 44:	Mean episodic reward and lengths at the end of training for the different algorithms when
varying P noise and target radius.
(a) DDPG P Order 2
(b) DDPG P Order 3
Figure 45: Mean episodic reward at the end of training for the different algorithms when varying
action space max and time unit for a given P order.
ReWard
2
0.05	0.1
time unit
ReWard
>一
(c) TD3 P Order 2	(d) SAC P Order 2
33
Under review as a conference paper at ICLR 2021
J
Additional Learning Curves
Reward	Reward	Reward	Reward	Reward
delay O, sequence Ienqth 1
5000 IOOOO 15000 20000
Train Timesteps
delay 1, SequenceJength 1
75
50
25
5000 IOOOO 15000 20000
Train Timesteps
delay 2, SequenceJength 1
5000 IOOOO 15000 20000
Train Timesteps
delay 4, sequence length 1
75
50
25
5000 IOOOO 15000 20000
Train Timesteps
delay 8, sequence Ienqth 1
75
P
⅛50
ω
~25
delay O, sequence Ienqth 2
5000 IOOOO 15000 20000
Train Timesteps
delay 1, sequence length 2
O O
4 2
PJeM3
5000 IOOOO 15000 20000
Train Timesteps
delay 2, SequenceJength 2
O
2
PJeM3
75
⅛50
ω
oi25
5000 IOOOO 15000 20000
Train Timesteps
delay 4, sequence length 2
7 5 2
delay O, sequence Ienqth 3
5000 IOOOO 15000 20000
Train Timesteps
6器Iay 1, SeqUence_length 3
5000 IOOOO 15000 20000
Train Timesteps
/ 2, SequenceJength 3
O O
4 2
p∙leM∂
5000 IOOOO 15000 20000
Train Timesteps
5000 IOOOO 15000 20000
Train Timesteps
delay 8, sequence Ienqth 2
7 5 2
PJeM3≈
4, sequence length 3
O O
4 2
p∙leM3
deIay 4, SeqUenCe Iength ,
601
I40
S2O
5000 IOOOO 15000 20000
Train Timesteps
delay 8, sequence Ienqth 3
delay O, sequence Ienqth 4
5000 IOOOO 15000 20000
TrainTimesteps
SeqUenCe Iength 4
W
PJeM3
5000 IOOOO 15000 20000
Train Timesteps
delay 2, sequence length 4
O O
4 2
p∙leM∂
5000 IOOOO 15000 20000
TrainTimesteps
5000 IOOOO 15000 20000
TrainTimesteps
delay 8, sequence Ienqth 4
O
PJeM3≈
O O
2 1
PJeM3
5000 IOOOO 15000 20000
TrainTimesteps
5000 IOOOO 15000 20000	5000 IOOOO 15000 20000	5000 IOOOO 15000 20000
Train Timesteps	Train Timesteps	Train Timesteps

Figure 46: Training Learning Curves for DQN when varying delay and sequence lengths. Please note the
different colorbar scales.
34
Under review as a conference paper at ICLR 2021
delay O, sequence length 1
50
delay O, sequence Ienqth 2
ιoo	一
IOOOO 20000
Train Timesteps
delay O, SequenceJength 3
delay O, SequenceJength 4
Train Timesteps
delay 4, sequence length 2
Train Timesteps
g50
OJ
10000	20000
Train Timesteps
delay 4, sequence length 3
delay 4, sequence length 4
Train Timesteps
Train Timesteps
Train Timesteps
Train Timesteps
Figure 47: Evaluation Learning Curves for DQN when varying delay and sequence lengths. Please note the
different colorbar scales.
35
Under review as a conference paper at ICLR 2021
transition noise 0.0, reward noιStransition noise 0.0, reward noιsir⅞ns∣t∣on noise 0.0, reward noisb'ðisition noise 0.0, reward noιsir⅞αsιtιon noise 0.0, reward noise 25
s0
eM3H
s0
eM3H
10000
10000	20C
TrainTimesteps
noise 0.01, reward
seM3H
10000
Train Timesteps
⅛50-
10000
TrainTmesteps
seM3H
50
10000	20
Train Timesteps
noise 0.01, reward
0 5 0 让5 0 5 0
5 2l7 5 2
PJeMUHPJeMUH
Train Timesteps
noise 0.01, reward
10000
Train Timesteps
so2so
PJeMUH
noise 0.01,
reward
石s02s
EeMUH
10000
Train Timesteps
noise 0.02, reward

50
^S
S 25
S
o
10000
TrainTmesteps
nπicp Γl ∩7 γpuuaγH
r2so
EeMUH
10000	20
Train Timesteps
noise 0.02, reward
noise 0.02, reward
10000
Train Timesteps
10000	2oc
TrainTimesteps
ion noise 0.02, reward
so2so
PJeMUH
10000	20
TrainTimesteps
noise 0.1, reward
PJeMUH
10000
Train Timesteps
10000
TrainTmesteps
2oo20
-
PJeMUH
25
10000
Train Timesteps
10000	2OC
TrainTimesteps
noise 0.25, reward
seM3H
10000
Train Timesteps
noise 0.1, reward
W 20
I
Ct
0
10000
TrainTmesteps
seM3H
10000	2(
Train Timesteps
noise 0.1. reward
noise 0.25, reward
ιoboo 2odoo
Train Time steps

ιoboo 2oboo
TrainTimesteps
noise 0.25, reward
noise 0.25, reward
EeMUH
10000	20000
Train Timesteps
PJeMUH
ιoboo 2odoo
Train Ti me steps
10000	20
Train Timesteps
noise 0.25. reward
2oloo
EeMUH
ιoboo 2oboo
Train Timesteps
Figure 48:	Training Learning Curves for DQN when varying transition noise and reward noise.
transition
100
50
PJeMUa
transition
100
50
EeMUa
transition
100
50
EeMUa
transition
100
50
EeMUa
transition
100
50
PJeMUa
50
PJeMUa
noise 0.0, reward n。ISbanSltlon
100
50
PJeMUa
10000	20C
TrainTimesteps
noise 0.01, reward
noise 0.0, reward noι⅛⅛ιsιtιon
100
50
PJeMUa
noise 0.0, reward noιsfraβsιtιon
∑Σ 100
a
10000	20000
TrainTimesteps
noise 0.01, reward nog。Sition
100
50
PJeMUa
0.
10000	20000
TrainTimesteps
noise 0.02, reward nog。Sition_
100	-
10000	20C
TrainTimesteps
noise 0.02, reward
50
EeMUa
50
PJeMUa
0.
ιoboo 2oboo
TrainTimesteps
noise 0.1, reward noisb'θnsition
100⅛
10000	20000
TrainTimesteps
noise 0.25, reward noitaβsition
1001
10000	20000
Train Timesteps
Jl
PJeMUa
Jl
PJeMUa
10000	20000
TrainTimesteps
noise 0.01, reward notsBr⅞ition
10000	20000
TrainTimesteps
noise 0.0, reward noise 25
50
PJeMUa
10000
TrainTimesteps
100
50
PJeMUa
0
10000	20000
TrainTimesteps
______,_:_________J O-
10000	20000
TrainTimesteps
PJeMUa
10000
TrainTimesteps
10000	20
TrainTimesteps
noise 0.1, reward
50
PJeMUa
50
PJeMUa
10000	20C
TrainTimesteps
noise 0.25, reward
50
PJeMUa
____________________ O-
10000	20000
Train Timesteps
noise 0.02, reward notsar^ition
—	100Γ-
0
ιoboo 2oboo
TrainTimesteps
noise 0.02, reward
50
PJeMUa
10000	20000
TrainTimesteps
60
40
20
O-
noise 0.02, reward
IOOOO
TrainTimesteps
25
noise 0.1, reward noi⅛⅛ιsition
100
O-
ιoboo 2oboo
TrainTimesteps
noise 0.25, reward nOtsarSition
^	-	100Γ-
0
ιoboo 2oboo
Train Timesteps
25
noise 0.1, reward
50
PJeMUa
o∙
10000	20000
TrainTimesteps
50
ion noise 0.1, reward
IOOOO
TrainTimesteps
noise 0.25, reward noi
50
PJeMUa
10000	20000
Train Timesteps
noise 0.25, reward
10000	20000
Train Timesteps


0
Figure 49:	Evaluation Learning Curves for DQN when varying transition noise and reward noise. Please
note the different Y-axis scales.
36
Under review as a conference paper at ICLR 2021
Reward	Reward	Reward	Reward	Reward
delay 0, sequence length 1
乃5025
5000 10000 15000 20000
Train Timesteps
delay 1, sequence Ienqth 1
O
5000 10000 15000 20000
Train Timesteps
delay 2, sequence Ienqth 1
7 5 2
5000 10000 15000 20000
Train Timesteps
delay 4, sequence Ienqth 1
7 5 2
5000 10000 15000 20000
Train Timesteps
delay 8, SequenceJength 1
乃5025
delay O, sequence length 2
5 0 5
7 5 2
PJeM3
5000 10000 15000 20000
Train Timesteps
delay 1, sequence Ienqth 2
O O
5
eM∂
5000 10000 15000 20000
Train Timesteps
delay 2, sequence Ienqth 2
7 5 2
peM3≈
5000 10000 15000 20000
Train Timesteps
delay 4, sequence Ienqth 2
6 4 2
PJeM3≈
5000 10000 15000 20000
Train Timesteps
delay 8, SequenceJength 2
O O
4 2
PJeM3
delay O, sequence length 3
O O
4 2
PJeM3
5000 IOOOO 15000 20000
Train Timesteps
delay 1, sequence Ienqth 3
O O
4 2
eM∂
5000 IOOOO 15000 20000
Train Timesteps
delay 2, sequence Ienqth 3
peM3≈
5000 10000 15000 20000
Train Timesteps
delay 4, sequence Ienqth 3
O
PJeM3≈
5000 10000 15000 20000
Train Timesteps
delay 8, SequenceJength 3
O O
4 2
PJeM3
5000 10000 15000 20000	5000 10000 15000 20000	5000 10000 15000 20000
Train Timesteps	Train Timesteps	Train Timesteps
delay O, sequence length 4
ŋ ŋ O
6 4 2
PJeM3
5000 10000 15000 20000
Train Timesteps
delay 1, sequence Ienqth 4
ŋ ŋ O
6 4 2
5000 10000 15000 20000
Train Timesteps
delay 2, sequence Ienqth 4
ŋ ŋ O
6 4 2
eM3
5000 10000 15000 20000
TrainTimesteps
delay 4, sequence Ienqth 4
ŋ ŋ O
6 4 2
PJeM3
5000 10000 15000 20000
Train Timesteps
delay 8, SequenceJength 4
O O
4 2
PJeM3
5000 10000 15000 20000
Train Timesteps
Figure 50:	Training Learning Curves for Rainbow when varying delay and sequence lengths. Please note
the different Y-axis scales.
37
Under review as a conference paper at ICLR 2021
delay O, SequenceJength 1
delay 0, SequenceJength 2
0.
B 50
⅛25
7gpay 0, SeqUence length 3
delay 0, SequenceJength 4
10000	20000
delay 1, sequence length 1
50
delay 1, sequence length 2
ιoo
10000	20000
Train Timesteps
delay 1, sequence length 3
0
60
E
ro 40
ω
工20
Train Timesteps
delay 1, sequence length 4
10000	20000
IOOOO 20000
Train Timesteps
IOOOO 20000
Train Timesteps
io；FIay 2, SeqUenCe Iength 2
IOOOO 20000
Train Timesteps
delay 2, sequence length 3
50
60
« 40
ω
工20
Train Timesteps
delay 2, sequence length 4
10000	20000
10000	20000
Train Timesteps
delay 4, sequence length 2
60
« 40
ω
工20
10000	20000
Train Timesteps
delay 4, sequence length 3
60
P 40
ω
工20
Train Timesteps
delay 4, sequence length 4
10000	20000
delay 8, sequence length 1
50
IOOOO 20000
Train Timesteps
E 40
工 20
10000	20000
Train Timesteps
delay 8, sequence length 2
60
10000	20000
Train Timesteps
ŋ 40
αi 20
0
10000	20000
Train Timesteps
delay 8, sequence length 3
60
10000	20000
Train Timesteps
Train Timesteps
delay 8, sequence length 4
10000	20000
Train Timesteps
0
0
Figure 51:	Evaluation Learning Curves for Rainbow when varying delay and sequence lengths. Please note
the different Y-axis scales.
38
Under review as a conference paper at ICLR 2021
transition noise 0.0, reward noιStransition noise 0.0, reward noιsir⅞ns∣t∣on noise 0.0, reward noisb'ðisιtιon noise 0.0, reward n□ιs4r⅞∂sιt∣on-noιse 0.0, reward noise 25
EeMUH
10000
Train Timesteps
noise 0.01, reward
5 2
seM3H
10000
Train Timesteps
PJeMUH
10000
TrainTimesteps
75
PJeMUH
10000
Train Timesteps
p 50
S
S 25
10000
TrainTmesteps
10000
Train Timesteps
noise 0.01, reward
10000
TrainTimesteps
noise 0.01, reward
10000
Train Timesteps
60
"540
Lo
O-
noise 0.01, reward
10000
TrainTmesteps
noise 0.01, reward
20O
eM3H
10000
Train Timesteps
25
noise 0.02, reward
s02s
EeMUH
10000
Train Timesteps
noise 0.02, reward
石s02s
PJeMUH
10000
TrainTimesteps
noise 0.02, reward
60w20o
PJeMUH
10000
Train Timesteps
noise 0.02, reward
P40
I 20
0
10000
TrainTmesteps
noise 0.02, reward
20O
PJeMUH
10000
Train Timesteps
25
noise 0.1, reward
2'1'
PJeMUH
10000
Train Timesteps
noise 0.1, reward
PJeMUH
10000
TrainTimesteps
noise 0.1. reward
3o2oloo
PJeMUH
10000
Train Timesteps
noise 0.1, reward
20
W
F0
0
noise 0.1, reward
noise 0.25, reward
10000	20000
TrainTimesteps
10000	20000
TrainTimesteps
0
10000	20000
Train Timesteps
10000	20
TrainTmesteps
noise 0.25, reward
PJeMUH
10000	20000
TrainTimesteps
PJeMUH
10000	20
Train Timesteps
noise 0.25, reward
10000	20000
Train Timesteps
PJeMUH

Figure 52:	Training Learning Curves for Rainbow when varying noises. Please note the different Y-axis
scales.
transition
100
noise 0.0, reward n。ISbanSltlon
100
noise 0.0, reward n。ISbanSltlon
100
noise 0.0, reward noι
noise 0.0, reward n?isb~a。Sltlon noise 0.0, reward^noιse 25
10000	20(
TrainTimesteps
noise 0.01, reward
100
IOOOO 20(
TrainTimesteps
noise 0.01, reward
a
ιoboo 2oboo
TrainTimesteps
.noise 0.01, rewarLnotsa嗯Sion
10000
TrainTimesteps
noise 0.01, reward
20000
60
"B 40
0
IOOOO
TrainTimesteps
10000
10000	20000
TrainTimesteps
50
0
10000	20000
TrainTimesteps
60
noise 0.01, reward
4'2'
PJeMaa
IOOOO
TrainTimesteps
noise 0.02, reward notsar^ition noise 0.02, reward
100
TrainTimesteps
noise 0.02, reward
PJeMaa
10000
TrainTimesteps
PJeMaa
10000	20C
TrainTimesteps
noise 0.02, reward
10000	20
TrainTimesteps
noise 0.1, reward
PJeMaa
o
100
10000	20000
TrainTimesteps
100
50
0
10000	20000
TrainTimesteps
"S 50
i
ac25
O-
10000
TrainTimesteps
noise 0.1, reward noi⅛⅛ιsition noise 0.1, reward
100
noise 0.1, reward
PJeMUa
IOOOO
TrainTimesteps
PJeMUa
100
noise 0.25, reward
10000	20000
Train Timesteps
10000	20(
TrainTimesteps
noise 0.25, reward
PJeMUa
100
10000	20000
TrainTimesteps
0」一
10000	20000
TrainTimesteps
15
^glθ
S
M 5
O-
noise 0.1, reward
10000	20
TrainTimesteps
noise 0.25, reward
PJeMUa
10000	20000
Train Timesteps
PJeMUa
100
noise 0.25, reward no⅛ar⅛ition noise 0.25, reward
75
5025
PJeMUa
I Vg . ⅛→~j V____________
10000	20000
Train Timesteps
口」...趴郃3K⅛⅛⅞H
10000	20000
Train Timesteps
O O
4 2
PJeMUa
10000	20000
Train Timesteps
PJeMUa
PJeMaa
EeMUa
PJeMUa
PJeMUa
PJeMUa
PJeMaa
PJeMaa
5
PJeMUa
0
Figure 53:	Evaluation Learning Curves for Rainbow when varying noises. Please note the different Y-axis
scales.
39
Under review as a conference paper at ICLR 2021
50
P∙IM3
50000 IOOOOO 150000
Train Timesteps
delay O, sequence length 2
deIay L SeqUenCe Iength 2
delay O, sequence Ienqth 3
40
P∙I"M3≈
50000 IOOOOO 150000
Train Timesteps
delay O, sequence length 4
deIay 1, SeqUenCe Iength 1
delay 1, sequence length 3
50
P∙IM3
50000 IOOOOO 150000
Train Timesteps
deIay 2, SeqUenCe Iength 1
50000 IOOOOO 150000
Train Timesteps
d#lay 4, SeqUenCe Iengtt 1
75	∖
O 5
5 2
P∙IM3
50000 IOOOOO 150000
Train Timesteps
delay 8, sequence length 1
60
50000 IOOOOO 150000
Train Timesteps
O O
2 1
M3
50000 IOOOOO 150000
Train Timesteps
d,lay 2, SeqUenCe Iengtq 2
40 厂	一
p"M3≈
50000 IOOOOO 150000
Train Timesteps
delay 2, SequenceJength 3
delay 1, sequence length 4
6
4 2
p"M3≈
50000 IOOOOO 150000
Train Timesteps
delay 2, sequence length 4
6
50000 IOOOOO 150000
Train Timesteps
需Iay 4, SeqUenCe IengtB 2
W
M3
50000 IOOOOO 150000
Train Timesteps
§ 10
ω
delay 8, sequence_length 2
50000 IOOOOO 150000
Train Timesteps
50000 IOOOOO 150000
Train Timesteps
d.ay4, SeqUenCe Iength 3
io	,.
p"M3≈
50000 IOOOOO 150000
Train Timesteps
delay 8,
sequence length 3
P∙I"M3≈
50000 100000 150000
TrainTimesteps
50000 IOOOOO 150000
Train Timesteps
delay 4, sequence length 4
6
4 2
p"M3≈
50000 IOOOOO 150000
Train Timesteps
delay 8, sequence length 4
0.04
E0.03
ID
I 0.02
0.01
50000 100000 150000
Train Timesteps
Figure 54:	Training Learning Curves for A3C when varying delay and sequence lengths. Please note the
different Y-axis scales.
40
Under review as a conference paper at ICLR 2021
p∙leM∂≈p,leM3≈p,leM3≈
delay 0, sequence length 1
ιoo
50
50000 100000 150000
Train Timesteps
delay 1, sequence length 1
60
p
⅛40
5
ω
工20
50
50000 100000 150000
Train Timesteps
2, sequence length 1
50
50000 100000 150000
Train Timesteps
delay 4, sequence length 1
75
E
«50
ω
~25
50000 100000 150000
Train Timesteps
delay 8, sequence length 1
50000 100000 150000
Train Timesteps
delay 0, sequence Ienqth 2
50
p∙leM∂
o
50000 100000 150000
Train Timesteps
delay 1, sequence length 2
30
O O
2 1
p∙leM3
o
50000 100000 150000
Train Timesteps
delay 2, sequence length 2
40
20
p∙leM3
0
50000 100000 150000
Train Timesteps
delay 4, sequence Ienqth 2
O O
2 1
PJeM3
0
50000 100000 150000
Train Timesteps
delay 8, sequence length 2
20
10
PJeM3
50000 100000 150000
Train Timesteps
delay 0, sequence length 3
d?Iay 0, SeqUenCe Iength 4
40
E
ID
I 20
0
50000 100000 150000
Train Timesteps
delay 1, sequence length 4
7.5
delay 1, SequenceJength 3
15
105
p∙leM3
50000 100000 150000
Train Timesteps
阴Iay 2, SeqUenCe Iength 3
lω5
p∙leM3
50000 100000 150000
Train Timesteps
delay 4, sequence Ienqth 3
105
PJeMaH
50000 IOObOO 150000
Train Timesteps
delay 8, sequence length 3
10
PJeM3≈
peM3≈
o.o
50000 100000 150000
Train Timesteps
delay 2, sequence length 4
7.5
o.o
peM3≈
50000 100000 150000
Train Timesteps
delay 4, sequence Ienqth 4
6 4 2
PJeM3≈
50000 100000 150000
Train Timesteps
0」…J 0.0
50000 100000 150000
Train Timesteps
delay 8, sequence length 4
0.4
2
O
PJeM3
50000 100000 150000
Train Timesteps
Figure 55:	Evaluation Learning Curves for A3C when varying delay and sequence lengths. Please note the
different Y-axis scales.
41
Under review as a conference paper at ICLR 2021
noise 0.0, reward noisfrθnsition
100
noise 0.0, reward n。IStranSIbon n。ISe 0.0, reward πoιstacβιsιt]pn-noi5e 0.0, reward no∣s⅛r⅞0s∣qon noise OareWard n°∣se 25
EeMUa
PJeMUa
PJeMUa
PJeMUa
O 50000 100000 150000
Train Timesteps
PJeMUa
20f
50000 100000 150000	0
Train Tmesteps
50000 100000 150000
TrainTimesteps
0	50000 100000 150000
Train Timesteps
-20L .	I .	」
0	50000 100000 150000
Train Timesteps
noise 0.01, reward noibs。Sition
ιoop^
noise OQl, reward Poit珏∙sition
iooɛ
noise OQl, reward po⅛ar⅛itign noise 0.01, reward Qoifcaiiqtion noise 0.01, reward noise 25
PJeMaa
PJeMaa
PJeMaa
50
PJeMaa
50000 100000 150000
Train Tmesteps
Q-Iλ ,,J
0	50000 100000 150000
TrainTimesteps
3	J
0	50000 100000 150000
TrainTimesteps
PJeMaa
0	50000 100000 150000
Train Timesteps
transiti□n-n□ise 0.02, reward n□i⅛0sitign
noise 0Q2, reward poit0Hsition
.A ʌ ιoo
50000 IObOOO 150000
Train Timesteps
noise 0Q2, reward po⅛ar⅛itign noise 0.02, reward QoifcaGOtion noise 0.02, reward noise 25
EeMUa
EeMUa
5。
EeMUa
EeMUa
0	50000 100000 15C
Train Timesteps
:ion noise 0.1, reward
50000 100000 150000
TrainTimesteps
0	50000 100000 150000
TrainTimesteps
noise 0.1, reward πoistrAnsition noise 0.1, reward
EeMUa
PJeMaa
50000 IOOOOO 150000 O
Train Tmesteps
PJeMaa
PJeMaa
PJeMaa
PJeMaa
50000 100000 150000	0
TrainTimesteps
50000 100000 150000
Train Timesteps
noise 0.25, reward noibs。SitiOn
2 OL
noise 0.25, reward
iɪ0
o-[ʌ
50000 IOOOOO 150000 O
Train Iimesteps
50000 100000 150000	0
Train Timesteps
50000 100000 150000
Train Timesteps
50000 100000 150000
Train Timesteps
noise 0.1, reward n°ise 25
o
20
PJeMUa
0	50000 100000 150000
Train Timesteps
PJeMUa
0	50000 100000 150000
Train Timesteps
noise 0.25, reward noise 25
0	50000 100000 150000
TrainTimesteps
Figure 56: Training Learning Curves for A3C when varying noises. Please note the different Y-axis scales.
transition
100
noise 0.0, reward n。ISbanSltlon
100
EeMUa
PJeMUa
PJeMUa
PJeMaa
PJeMUa
PJeMUa
PJeMUa
PJeMUa
PJeMaa
PJeMUa
50000 100000 150000	0
Train Timesteps
noise 0.0, reward n。ISbanSltlon
100
noise 0.0, reward noιs⅛aβsιtιon
50000 100000
TrainTimesteps
noise 0.01, reward
50000 100000
TrainTimesteps
noise 0.02, reward
50000 100000
TrainTimesteps
PJeMaa
noise 0.25, reward
PJeMUa
PJeMUa
PJeMUa
PJeMUa
50000 100000 150000	0
Train Timesteps
noise 0.0, reward noι⅛⅛ιsιtιon
100
50000 100000 150000	6
TrainTimesteps
PJeMUa
noise 0.0, reward noise 25
noise 0.01, reward notsar&ition
50000 100000 150000
TrainTimesteps
100Γ
PJeMUa
50000 100000
TrainTimesteps
noise 0.1, reward n
50000 100000
TrainTimesteps
PJeMUa
PJeMUa
50000 100000 150000
TrainTimesteps
noise 0.01, reward QoiSe 25
50000 100000 150000
TrainTimesteps
noise 0.02, reward QoiSe 25
PJeMUa
PJeMaa
50000 100000 150000	6
TrainTimesteps
noise 0.25, reward notsar&ition
PJeMUa
50000 100000 150000	0
Train Timesteps
50000 100000
TrainTimesteps
50000 100000
TrainTimesteps
PJeMUa
PJeMaa
PJeMUa
oξJ
5OOOO IOOOOO 150000 O
Train Timesteps
50000 100000 150000
TrainTimesteps
noise 0.1, reward n°ise 25
50000 100000 150000
TrainTimesteps
noise 0.25, reward n,oise 25
50000 100000 150000
Train Timesteps
Figure 57: Evaluation Learning Curves for A3C when varying noises. Please note the different Y-axis scales.
42
Under review as a conference paper at ICLR 2021
delay 0, sequence length 1
50
p∙leM∂
0
O 50000 IOOOOO 150000
Train Timesteps
delay 1, sequence length 1
delay 0, sequence length 2
O O
4 2
p∙leM∂
0 ,	,	,	,
O	50000 IOOOOO 150000
Train Timesteps
delay 1, sequence Ienqth 2
p 40
ID
l2θ-
O
O 50000 IOOOOO 150000
Train Timesteps
delay 1, sequence length 3
15
delay 0, sequence length 4
0
O 50000 IOOOOO 150000
Train Timesteps
delay 2, sequence length 1
75
20
p∙leM3
0
O 50000 IOOOOO 150000
Train Timesteps
delay 2, sequence length 2
40
105
p∙leM3
o .	.	.
O	50000 IOOOOO 150000
Train Timesteps
delay 2, sequence Ienqth 3
delay 1, sequence Ienqth 4
5 0 5
Il
p∙leM3
o
O 50000 IOOOOO 150000
Train Timesteps
0 ,	,	,
O	50000 IOOOOO 150000
Train Timesteps
delay 4, sequence Ienqth 1
O O
4 2
PJeM3
0
d 5θdoo 100000 150000
Train Timesteps
delay 8, sequence length 1
60
E 40
ID
5
ω
工 20
O
0 5θdoo ιooboo 150000
Train Timesteps
20
0 .	.	.
O	50000 IOOOOO 150000
Train Timesteps
delay 4, sequence Ienqth 2
5
p∙leM3≈
0
O 50000 IOOOOO 150000
Train Timesteps
2, sequence length 4
2
p∙leM3≈
0 .	.	.
O	50000 IOOOOO 150000
Train Timesteps
delay 4, sequence length 4
4
O O
2 1
PJeM3
0
O 50000 IOOOOO 150000
Train Timesteps
delay 8, sequence length 2
105
PJeMaH
O
5θdoo ιooboo isoboo
Train Timesteps
4, sequence length 3
5
PJeM3
0 ,
O 50000 IOOOOO 150000
Train Timesteps
delay 8, sequence length 3
2
PJeM3≈
0
d 5θdoo 100000 150000
Train Timesteps
delay 8, sequence length 4
0.0
O
5 0 5
7 5 2
PJeM3
5θdoo ιooboo isoboo
Train Timesteps
3 2 1
PJeM3≈
0	50000 100000 150000
Train Timesteps
Figure 58:	Training Learning Curves for A3C with LSTM when varying delay and sequence lengths. Please
note the different Y-axis scales.
43
Under review as a conference paper at ICLR 2021
delay 0, SequenceJength 1
ιoo
50
p∙leM∂
d 50000 100000 150000
Train Timesteps
delay 1, sequence Ienqth 1
delay 0, sequence length 2
delay 0, sequence_length 3
60
E 40
ID
工 20
0
O 50000 100000 150000
Train Timesteps
delay 1, sequence length 3
20
sequence length 4
P 40
ID
120
d 5θooo ιooboo 150000
Train Timesteps
delay 1, sequence length 4
20
delay 2, SeqUenCe Iength ll
delay 1, sequence_length 2
40
铲°
0	50000 100000 150000
Train Timesteps
delay 2, sequence_length 2
W
p∙leM3≈
0	50000 100000 150000
Train Timesteps
W
p∙leM3≈
50
p∙leM3
50000 100000 150000
Train Timesteps
o
40
20
eM3
50000 100000 150000
Train Timesteps
peM3≈
delay 2, SeqUenCe Iength 3
10
50000 100000 150000
Train Timesteps
0	50000 100000 150000
TrainTimesteps
delay 2, sequence length 4
6
4 2
peM3≈
0	50000 100000 150000
TrainTimesteps
delay 4, SeqUenCe Iength I
Ko	M对版
3护ay 4, SeqUenCe Iength 2
l：L3
0	50000 100000 150000
Train Timesteps
delay 8, sequence length 1
0	50000 100000 150000
Train Timesteps
delay 8, sequence length 2
delay 4, sequence Ienqth 3
5
PJeM3
d 5θdoo ιooooo isoboo
Train Timesteps
delay 8, sequence length 3
delay 4, sequence length 4
6
4 2
PJeM3≈
d 5θdoo ιooboo isoboo
TrainTimesteps
delay 8, sequence length 4
ŋ ŋ O
6 4 2
PJeM3
O O
2 1
PJeM3
0	50000 100000 150000
Train Timesteps
0	50000 100000 150000
Train Timesteps
5
PJeM3
d 5θdoo ιooooo isoboo
Train Timesteps
6 4 2
PJeM3≈
0	50000 100000 150000
TrainTimesteps
Figure 59:	Evaluation Learning Curves for A3C with LSTM when varying delay and sequence lengths.
Please note the different Y-axis scales.
44
Under review as a conference paper at ICLR 2021
transιtι
100
IElon noise 0.0, reward noisYnsipon noise 0.0, reward noιs⅛⅛nSltIon noise 0.0, reward noι⅛⅞ιSltIon noise 0.0, reward noιsfa⅜6SltIon noise 0.0, reward noιse 25
ιoo
O 让 乃5o25o
,PJeMUH
EeMUa
PJeMUa
50000 100000 150000	0
TrainTimesteps
PJeMUa
50000 100000 150000
Train Timesteps
noise 0.01, reward p。
PJeMUa
PJeMUa
20]
-2oL-------,------J!-----J
0	50000 100000 150000
Train Timesteps
noise OQl, reward Poit珏∙sition
50000 100000 150000
TrainTimesteps
EeMUa
50000 100000 150000
Train Timesteps
noise 0.02, reward p。
.	」	0也
50000 100000 150000	0
TrainTimesteps
50000 100000 150000
TrainTimesteps
noise 0.1, reward noi
I 25
PJeMUa
noise 0.01. reward noise 25
EeMUa
0
50000 100000 150000
Train Timesteps
noise 0.1, reward noise 25
PJeMUa
noise 0.1, reward πoistrAnsition
PJeMUa
50000 100000 150000
TrainTimesteps
PJeMUa
0	50000 100000 150000
Train Timesteps
noise 0.1, reward
PJeMUa
0	50000 100000 150000
Train Timesteps
ɔn noise 0.25, reward noise 25
0	50000 100000 150000
Train Iimesteps
o」y_____,______,_____j
0	50000 100000 150000
Train Timesteps
0	50000 100000 150000
0	50000 100000 150000
TrainTimesteps
Train Timesteps
0	50000 100000 150000
Train Timesteps
5
PJeMUa
PJeMUa
1i
0
Jl
PJeMUa
Figure 60:	Training Learning Curves for A3C with LSTM when varying noises. Please note the different
Y-axis scales.
EeMUa
PJeMUa
EeMUa
noise 0.01, reward
EeMUa
EeMUa
EeMUa
50000 100000 150000
TrainTimesteps
noise 0.01, reward QoiSe 25
EeMUa
EeMUa
EeMUa
EeMUa
EeMUa
0	50000 IOOOOO
TrainTimesteps
50000 100000 150000
TrainTimesteps
noise 0.02, reward
noise 0.02, reward
noise 0.02, reward noise 25
PJeMaa
PJeMUa
50000 100000 150000
Train Timesteps
PJeMUa
PJeMaa
O 50000 100000 150000
TrainTimesteps
0	50000 100000 15C
TrainTimesteps
:ion noise 0.1, reward
75
PJeMUa
5025
PJeMaa
O 50000 100000 150000
TrainTimesteps
50000 100000 150∣
TrainTimesteps
CCiUa ∩ ɔR rατuarH
PJeMUa
PJeMaa
O 50000 100000 150000
TrainTimesteps
50000 100000 150000
TrainTimesteps
PJeMUa
OL ' " 1『叫"Lll * I
0	50000 100000 150000
Train Timesteps
PJeMUa
PJeMUa
PJeMaa
Figure 61:	Evaluation Learning Curves for A3C with LSTM when varying noises. Please note the different
Y-axis scales.
45
Under review as a conference paper at ICLR 2021
reward density 0.25
O O
5
PJeMα
reward density 0.5
50
PJeMα
5000 IOOOO 15000 20000
Train Timesteps
reward density 0.75
5 0 5
7 5 2
PJeMα
5000 IOOOO 15000 20000
Train Ti me steps
O
5000 IOOOO 15000 20000
Train Timesteps
O
Figure 62:	Training Learning Curves for DQN when varying reward sparsity. Please note the different
Y-axis scales.
reward_density 0.25	reward density 0.5
—	100]
O O
5
PJeMα
O
IOOOO 20000
Train Timesteps
50
PJeMα
O
IOOOO 20000
Train Timesteps
reward density 0.75
O O
105
PJeMα
IOOOO 20000
Train Timesteps
Figure 63:	Evaluation Learning Curves for DQN when varying reward sparsity. Please note the different
Y-axis scales.
reward density 0.25	reward density 0.5	reward density 0.75
O O
5
PJeMα
5 0 5
7 5 2
PJeMα
5000 IOOOO 15000 20000
Train Timesteps
50
PJeMα
O
5000 IOOOO 15000 20000
Train Ti me steps
5000 IOOOO 15000 20000
Train Timesteps
Figure 64:	Training Learning Curves for Rainbow when varying reward sparsity. Please note the different
Y-axis scales.
reward density 0.25
O O
5
PJeMα
reward density 0.5
Ooo
15
PJeMα
reward density 0.75
100	―
50
PJeMα
o
IOOOO 20000	IOOOO 20000
Train Timesteps	Train Timesteps
Figure 65:	Evaluation Learning Curves for Rainbow when varying reward sparsity. Please note the different
Y-axis scales.
reward density 0.25
ιooι-----一二二一L ∙F
50
PJeMα
50000 IOOOOO 150000
Train Ti me steps
reward density 0.5
IOOi----------------------
50
PJeMα
50000 IOOOOO 150000
Train Timesteps
reward density 0.75
ιoo 一
50
PJeMα
O .	.	.	.
O	50000 IOOOOO 150000
Train Timesteps
Figure 66:	Training Learning Curves for A3C when varying reward sparsity. Please note the different Y-axis
scales.
O
O
O
O
O
46
Under review as a conference paper at ICLR 2021
reward density 0.25
100	—
O O
5
PJeMα
0	50000 100000 150000
Train Ti me steps
reward_density 0.5
ιoo 一
50
PJeMα
0
0	50000 100000 150000
Train Timesteps
reward density 0.75
100	―
50
PJeMα
0
0	50000 100000 150000
Train Timesteps
reward density 0.25
reward density 0.5
Figure 67: Evaluation Learning Curves for A3C when varying reward sparsity. Please note the different
Y-axis scales.
0	50000 100000 150000	0	50000 100000 150000
Train Timesteps	Train Timesteps
reward density 0.75
100
50
PJeMα
0
0	50000 100000 150000
Train Timesteps
Figure 68:	Training Learning Curves for A3C + LSTM when varying reward sparsity. Please note the
different Y-axis scales.
reward density 0.25
100
O O
5
PJeMα
ιoo
reward density 0.5
O O
5
PJeMα
reward density 0.75
Ooo
15
PJeMα
0	50000 100000 150000	0	50000 100000 150000	0	50000 100000 150000
Train Timesteps	Train Timesteps	Train Timesteps
Figure 69:	Evaluation Learning Curves for A3C + LSTM when varying reward sparsity. Please note the
different Y-axis scales.
SequenceJength 2
O O
5
PJeMα
SeqUenCejength 3
l0l0l°
,OO
3 2 1
PJeMα
10000	20000
Train Timesteps
sequence length 4
l0l0
g O
4 2
PJeMα
10000	20000
Train Timesteps
0
10000	20000
Train Timesteps
0
Figure 70:	Training Learning Curves for Rainbow when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
0
SeqUenCe iength 2
PIOO
i-
(0
f 50
SeqUenCe length 3
l0l0l°
,OO
3 2 1
PJeMα
10000	20000
Train Timesteps
sequence length 4
l0l0l°
QgO
6 4 2
PJeMα
10000	20000
Train Timesteps
0
10000	20000
Train Timesteps
0
Figure 71:	Evaluation Learning Curves for Rainbow when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
47
Under review as a conference paper at ICLR 2021
Figure 72:	Training Learning Curves for A3C when make_denser is True for rewardable sequences. Please
note the different Y-axis scales.
Train Ti me steps
Train Timesteps
Train Timesteps
Figure 73:	Evaluation Learning Curves for A3C when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
Train Timesteps
Train Timesteps
Train Timesteps
Figure 74:	Training Learning Curves for A3C + LSTM when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
Figure 75:	Evaluation Learning Curves for A3C + LSTM when make_denser is True for rewardable se-
quences. Please note the different Y-axis scales.
Train Timesteps
Train Timesteps
Train Timesteps
Figure 76:	Training Learning Curves for A3C + LSTM when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
48
Under review as a conference paper at ICLR 2021
SequenceJength 2	SequenceJength 3	SequenceJength 4
l0l0
g O
4 2
PJeMα
5 0 5 0
7 5 2
PJeMα
l0l0
O O
2 1
PJeMα
50000 IOOOOO 150000
Train Timesteps
O 50000 IOOOOO 150000
Train Timesteps
O
O
O
O 50000 IOOOOO 150000
Train Timesteps
Figure 77:	Training Learning Curves for A3C + LSTM when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
SeqUenCe length 2	SeqUerKe length 3
5 0 5 0
7 5 2
PJeMα
l0l0
O O
2 1
PJeMα
50000 IOOOOO 150000
Train Timesteps
sequence length 4
l0l0
g O
4 2
PJeMα
O 50000 IOOOOO 150000
Train Timesteps
O
O 50000 IOOOOO 150000
Train Timesteps
O
O
Figure 78:	Training Learning Curves for A3C + LSTM when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
SeqUenCe length 2
5 0 5 0
7 5 2
PJeMα
SeqUenCe length 3
l0l0
O O
2 1
PJeMα
50000 IOOOOO 150000
Train Timesteps
sequence length 4
l0l0
g O
4 2
PJeMα
O 50000 IOOOOO 150000
Train Timesteps
O
O 50000 IOOOOO 150000
Train Timesteps
O
O
Figure 79:	Training Learning Curves for A3C + LSTM when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
SeqUenCejength 2	SequenceJength 3
5 0 5 0
7 5 2
PJeMα
l0l0
O O
2 1
PJeMα
50000 IOOOOO 150000
Train Timesteps
sequence length 4
l0l0
g O
4 2
PJeMα
O 50000 IOOOOO 150000
Train Timesteps
O
O 50000 IOOOOO 150000
Train Timesteps
O
O
Figure 80:	Training Learning Curves for A3C + LSTM when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
SeqUenCe length 2
5 0 5 0
7 5 2
PJeMα
SeqUenCe length 3
l0l0
O O
2 1
PJeMα
50000 IOOOOO 150000
Train Timesteps
sequence length 4
l0l0
g O
4 2
PJeMα
O 50000 IOOOOO 150000
Train Timesteps
O
O 50000 IOOOOO 150000
Train Timesteps
O
O
Figure 81:	Training Learning Curves for A3C + LSTM when make_denser is True for rewardable sequences.
Please note the different Y-axis scales.
49
Under review as a conference paper at ICLR 2021
K Performance on Complex environments
ITrm
0	12	4	8
delay
(b) Rainbow breakout
IOO-
rrrm
(a) Rainbow beam_rider
0	12	4	8
delay
(d)	Rain. space_invaders
fe 1000
0 C
0.0 0.01 0.02 0.1 0.25
transition_noise
0
0.0 0.01 0.02 0.1 0.25
transition noise
(c) Rainbow qbert
I 2500
° 0.0 0.01 0.02 0.1 0.25
transition noise
O C
5
2
eM9
(e)	Rainbow beam_rider
0.0 0.01 0.02 0.1 0.25
transition noise
(h)	Rain. space_invaders
ŋ 1000
ΓΓΓΓΠ
2	4	8
delay
(f) Rainbow breakout
100
delay
(g) Rainbow qbert
S 2000
0
delay
O C
5
2
eM9
(i)	A3C beam_rider
(l) A3C space_invaders
P 1000
πτm
0.0 0.01 0.02 0.1 0.25
transition noise
(j) A3C breakout
0.0 0.01 0.02 0.1 0.25
transition noise
(k) A3C qbert
2000
0 0.0 0.01 0.02 0.1 0.25
transition noise
_____________________
O C
5
2
eM9
0
0
0
(m) A3C beam_rider (n) A3C breakout	(o) A3C qbert (p) A3C space_invaders
Figure 82: AUC of episodic reward for agents at the end of training. Error bars represent 1 standard
deviation. Note the different y-axis scales.
(c) action max
0.05 0.1 0.25 0.5 1.0 2.0
action space max
0.025 0.05 0.1 0.25 0.5 1.0 2.0 8.0
action space max
0.5 1.0 2.5 5.0 10.0
time-unit
(a) action max
(b) time unit
0.05 0.1 0.25 0.5 1.0 2.0
action space max
(e) action max
0.025 0.05 0.1 0.25 0.5 1.0 2.0
action space max
(d) time unit
time unit
(f) time unit	(g) action max
0.2 0.4 1.0 2.0 4.0
time unit
0.025 0.05 0.1 0.25 0.5 1.0 2.0
action space max
(h) time unit
1.0 2.5 5.0 10.0
time-unit
(l) time unit
(i)	action max
(j)	time unit
(k)	action max
Figure 83: AUC of episodic reward at the end of training on HalfCheetah varying action max (top)
and time unit (bottom). Error bars represent 1 standard deviation. Note the different y-axis scales.
50
Under review as a conference paper at ICLR 2021
L Learning Curves for Complex environments
0.0	0.5	1.0
Train Timesteps le7
0.0	0.5	1.0	0.0	0.5	1.0
Tfaln Timesteps le7	Train Timesteps le7
0.0	0.5	1.0	0：0	0：5	l：0
Tfaln Timesteps le7	TfaIn Timesteps le7
1,
Figure 85: Training Learning Curves for DQN on breakout when varying delay. Please note the different
Y-axis scales.
Figure 84: Training Learning Curves for DQN on beam_rider when varying delay. Please note the different
Y-axis scales.
ReMdH
0.5	1.0
Train Tlm esteps le7
ReMdH
0.5	1.0
TraInTImesteps le7
delay 2,
0.0	0.5	1.0
Train Timesteps le7
delay 4,
0.5	1.0
Train Timesteps le7
2
ReMdH
delay 8,
0.5	1.0
Train Timesteps le7




Figure 86: Training Learning Curves for DQN on qbert when varying delay. Please note the different Y-axis
scales.
51
Under review as a conference paper at ICLR 2021
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timesteps le7
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timesteps le7
0.00 0.25 0.50 0.75 1.00
TfaInTImesteps le7
0.00 0.25 0.50 0.75 1.00
Train Timesteps le7
0.00 0.25 0.50 0.75 1.00
Train Timestsps le7
Figure 87: Training Learning Curves for DQN on space_invaders when varying delay. Please note the
different Y-axis scales.
0.0	0.5	1.0	0.0	0.5	1.0	0.0	0.5	1.0	0.0	0.5	1.0	0.0	0.5	1.0
Train Timesteps le7	Train Timesteps le7	Train Timesteps le7	Tfaln Timesteps le7	Tl,a∣n Timesteps le7
Figure 88: Training Learning Curves for DQN on beam_rider when varying transition noise. Please note the
different Y-axis scales.
transition noise 0.0	transition noise 0.01	transition noise 0.02	transition noise 0.1	transition noise 0.25
transition noise 0.1	transition noise 0.25
0.00 0.25 0.50 0.75 1.00
Tl,aln Tlmesteps le7
Ti,alπ Timesteps
ι.oα
le7
ɪ0050
TralnTlmesteps
1.00
le7
Tt,aln Timesteps
ι.oα
le7
TYaIn Tlmestsps
ι.bo
le7

Figure 89:	Training Learning Curves for DQN on breakout when varying transition noise. Please note the
different Y-axis scales.
transition noise 0.01	transition noise 0.02
transition noise 0.1	transition noise 0.25
3000
B
S 2000
H 1000
transition noise 0.0
PjeMdH
0：5	l：0
Train Timesteps le7
0：5 LO
Train Timesteps le7
P IOOO
1 500
Train Timesteps le7
Figure 90:	Training Learning Curves for DQN on qbert when varying transition noise. Please note the
different Y-axis scales.
PjeMSU
transition noise 0.0
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timestsps le7
transition noise 0.01
g g g
4 3 2
PjeM 3
transition noise 0.02
g g g
4 3 2
PjeM 3
1.00
le7
transition noise 0.1
g g
3 2
PjeM 3
transition noise 0.25
8 g
3 2
PjeM 3
TralnTlmesteps
1.00
le7
Figure 91: Training Learning Curves for DQN on space_invaders when varying transition noise. Please note
the different Y-axis scales.
0.0	0.5	1.0
Tfaln Timesteps le7
0.0	0.5	1.0
Tfaln Timesteps le7
0.0	0.5	1.0
Train Timesteps le7
0.0	0.5	1.0
Train Timesteps le7
0.5	1.0
Tfaln Timesteps le7
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timestsps le7
Figure 92: Training Learning Curves for Rainbow on beam_rider when varying delay. Please note the different
Y-axis scales.
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timesteps le7
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timesteps le7
0.00 0.25 0.50 0.75 1.00
TfaInTImesteps le7
0.00 0.25 0.50 0.75 1.00
Tfa InTimesteps	le7
Figure 93:	Training Learning Curves for Rainbow on breakout when varying delay. Please note the different
Y-axis scales.
52
Under review as a conference paper at ICLR 2021
Train Timesteps le7
PjeMəu
0：0	0：5 LO
Train Timesteps le7
Train Timesteps le7
Figure 94:	Training Learning Curves for Rainbow on qbert when varying delay. Please note the different
Y-axis scales.
delay 0	delay 1	delay 2	delay 4	delay 8
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timestsps le7
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timesteps le7
Figure 95: Training Learning Curves for Rainbow on space_invaders when varying delay. Please note the
different Y-axis scales.
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timesteps le7
0.00 0.25 0.50 0.75 1.00
TfaInTImesteps le7
0.00 0.25 0.50 0.75 1.00
Tfa InTimesteps	le7
transition noise 0.0
transition noise 0.01
transition noise 0.02
transition noise 0.1
0.0	0.5 ι.o
Tfaln Timesteps le7
0.0	0.5	1.0
Tfaln Timesteps le7
0.0	0.5	1.0
Train Timesteps le7
0.0	0.5	1.0
Train Timesteps le7
Figure 96: Training Learning Curves for Rainbow on beam_rider when varying transition noise. Please note
the different Y-axis scales.
transition noise 0.25
0.5	1.0
Tfaln Timesteps le7
transition noise 0.0	transition noise 0.01	transition noise 0.02
300
p
Q 200
φ
≡:100
Tfaln Timesteps le7
g g
2 1
50g50
1 1
Ti,alπ Timesteps
ι.oα
le7
transition noise 0.1
20
TfaInTlmesteps
1.00
le7
transition noise 0.25
5 O <
1 1
IMeMd
TfaInTlmesteps
ι.bo
le7

Figure 97:	Training Learning Curves for Rainbow on breakout when varying transition noise. Please note
the different Y-axis scales.
transition noise 0.0
7500
⅛ 5000
2500
0.5
l：0
Train Timesteps le7
0
0.0
^4000
M
S 2000
transιtι on-noιse 0.01
6000	—
0.5
l：0
TraInTImesteps le7
0
0.0
transition noise 0.02
τ,4000-
£ 2000
0.5
Train Timesteps le7
PjeM 第
transition noise 0.1
Train Timesteps le7

Figure 98:	Training Learning Curves for Rainbow on qbert when varying transition noise. Please note the
different Y-axis scales.
TfaInTImesteps le7
Tfa InTimesteps le7
Figure 99:	Training Learning Curves for Rainbow on space_invaders when varying transition noise. Please
note the different Y-axis scales.
delay 0	delay 1	delay 2	delay 4	delay 8
1500
^1000
g
500
1500
ral000
I
H 500
Train Timesteps le7
1500
S 1000
g
≤ 500
Train Timesteps le7
1500
BlOOO
1 500
Train Timesteps le7
0
0
Figure 100:	Training Learning Curves for A3C on beam_rider when varying delay. Please note the different
Y-axis scales.
53
Under review as a conference paper at ICLR 2021
O O
2 1
PjeMdH
0.00 0.25 0.50 0.75 1.00
Tl,a∣n Timesteps le7
200
lɪoo
Q
p 200
lɪoo
Q
Figure 101: Training Learning Curves for A3C on breakout when varying delay. Please note the different
Y-axis scales.
delay 0	delay 1	delay 2	delay 4	delay 8
g g
4 2
PjeMdH
Tl,a∣n Timesteps le7
O O
O O
4 2
PjeM9
PjeM9≈
0：5	l：0
Train Timesteps le7
2
PjeM9≈
Train Timesteps le7
PjeM9≈
Tfaln Timesteps le7
Figure 102:	Training Learning Curves for A3C on qbert when varying delay. Please note the different Y-axis
scales.
'0'0
O O
4 2
PjeMSU
Tl,a∣n Timesteps le7
1
g g
4 2
PjeM 3
g g
4 2
PjeM 3
TfaInTImesteps le7
g g
4 2
PjeM 3
Train Timesteps le7
PjeM 3
Train Timesteps le7
Figure 103:	Training Learning Curves for A3C on space_invaders when varying delay. Please note the
different Y-axis scales.
1500
g IOOO
H 500
transition noise 0.0
transition noise 0.01
8 g
15
Train Timesteps le7
transition noise 0.02
0.5	1.0
Train Timesteps le7
i5□α
IOOO
I
H 500
transition noise 0.1
0.5	1.0
TfaInTlmestEps le7
IOOO
Loo
transition noise 0.25
0.5 tθ
Ti,alπ Tlmestsps le7

Figure 104:	Training Learning Curves for A3C on beam_rider when varying transition noise. Please note the
different Y-axis scales.
transition noise 0.0
'0'0
O O
2 1
PjeMsU
TYaln Timesteps le7
transition noise 0.01
'05
5 2
transition noise 0.02
g g
2 1
TYaInTImesteps le7
transition noise 0.1
20
Train Timesteps ie7
transition noise 0.25
5 0 5
1 1
Train Timestsps ie7
Figure 105:	Training Learning Curves for A3C on breakout when varying transition noise. Please note the
different Y-axis scales.
0
transition noise 0.01	transition noise 0.02
transition noise 0.0
g g
4 2
PjeMdH
Tfaln Timesteps le7
O O
O O
4 2
PjeM9
TfaInTImesteps le7
2
PjeM9≈
Train Timesteps le7
2
PjeM9≈
transition noise 0.1
Train Timesteps le7
2lL
PjeM9≈
transition noise 0.25
Tfaln Timesteps le7
0
Figure 106:	Training Learning Curves for A3C on qbert when varying transition noise. Please note the
different Y-axis scales.
600
S 400
^200
0
transition noise 0.0
Train Tlm esteps le7
20
transition noise 0.01
Ti,alπ Timesteps
ι.oα
le7
transition noise 0.02
TralnTlmesteps
1.00
le7
transition noise 0.1
g0°
4 2
Tt,aln Timesteps
ι.oα
le7
transition noise 0.25
TYaIn Tlmestsps
ι.bo
le7

Figure 107:	Training Learning Curves for A3C on space_invaders when varying transition noise. Please note
the different Y-axis scales.
54
Under review as a conference paper at ICLR 2021
PjeMsM
acbon_space_max 0.1,
PjeMsM
action_space_max 0.5,
g g
4 2
PjeMsM
12	3
TfaInTlmestEps le6
PjeMsM
ιoooα
Figure 108:	Training Learning Curves for SAC when varying action max. Please note the different Y-axis
scales.
time UnItO.4,
2.5	5.0	7.5
Ti,alπ Tlmestsps le6
time unit 1.0,
12	3
Ti,alπ Tlmestsps le6
PjeMsU
10000
time Unlt 2.0,
Figure 109:	Training Learning Curves for SAC when varying time unit. Please note the different Y-axis
scales.
0	200000 400000
TraIn Timesteps
0	200000 400000
TraInTlmesteps
f8∙U-------,------,--
0	200000 400000
TraInTImesteps

Figure 110:	Training Learning Curves for SAC when varying action max. Please note the different Y-axis
scales.
PjeMdH
time unit 0.2,
0	12
Ti,aln Timesteps le6
time unit 0.4,
0.5	1.0
Train Timesteps le6
time unit 1.0,
200000 400000
Train Timesteps
PjeMd≈
time unit 2.0,
100000 200000
TraInTImesteps
time unit 4.0,
T-
PjeMd
50000	100000
Train Timesteps
Figure 111:	Training Learning Curves for SAC when varying time unit. Please note the different Y-axis
scales.

?s>£



Figure 112:	Training Learning Curves for SAC when varying action max. Please note the different Y-axis
scales.
time unit 0.5,
time unit 1.0,
0.0	0.5	1.0
TYaIn Tlmestsps le6
0	200000 400000
Train Timesteps
Figure 113: Training Learning Curves for SAC when varying time unit. Please note the different Y-axis
scales.
PjeMsU
time unit 2.5,
-200
IOObOO 200000
Train Timesteps
PjeMsIj
time unit 5.0,
0	50000	100000
Train Timesteps
T-
PjeMsIj
i⅛∣υu
action space max 0.5,
0	12	3
TraInTImesteps le6
7500
5000
2500
0
TraInTImesteps le6





Figure 114:	Training Learning Curves for DDPG when varying action max. Please note the different Y-axis
scales.
55
Under review as a conference paper at ICLR 2021
time Unlt 0.4,	time Unlt 1.0,
time UnItO.2,
PjeMdH
10000
time unit 2.0,
g g
4 2
PjeMdH
ɪ00
PjeMdH
Figure 115:	Training Learning Curves for DDPG when varying time unit. Please note the different Y-axis
scales.

action soace max 0.05.
O ZOOOOO 400000
TraInTlmesteps
action soace max O.l.
O ZOOOOO 4 oo w
TraInTImesteps
T叫I
0	288。488。
TraInTImesteps
[Chon SPaCe max 2Q,,


Figure 116:	Training Learning Curves for DDPG when varying action max. Please note the different Y-axis
scales.
PjeMdH
time unit 0.2,
-50
0	12
Ti,aln Timesteps le6
PjeMdH
fame unit 0.4,
-50
0.5	1.0
Train Timesteps le6
PjeMd≈
time UnIt LO,
O 200000 400000
Train Timesteps
-s -ιoo
2-200
-300'
time unit 2.0,
O IOOOOO 200000
TraInTImesteps
time unit 4.0,
50g50
-T T
PjeMdH
50000	100000
Train Timesteps
Figure 117:	Training Learning Curves for DDPG when varying time unit. Please note the different Y-axis
scales.
a,ctιon SPaCe max 0.025：
-io]	.^⅛⅛⅜wm⅜^∣
PJCMeIl
PJCMeIl
,action SPaCe max 05
PJCMeIl
,action SpaCe max 1.0.
O 200000 488。
TralnFmestepS
PJCMeIl
Figure 118:	Training Learning Curves for DDPG when varying action max. Please note the different Y-axis
scales.

time Unit 0.5,
txo 03 Eb
time unit 1.0,
255075
- - -
200000 400000
Train Timesteps
time Unit 2.5,
255075
- - -
0	100000	200000
Train Timesteps
255075
- - -
TralnTlmesteps le6
time Unit 5.0,	time Unit 10.0,
0	50000	100000
Train Timesteps
50
-
0	20000	40000
Train Timesteps
Figure 119:	Training Learning Curves for DDPG when varying time unit. Please note the different Y-axis
scales.
TralnTlmesteps
Train Timesteps
PJCMeIl
Figure 120:	Training Learning Curves for TD3 when varying action max. Please note the different Y-axis
scales.
time Unlt 0.4,	time Unlt 1.0,
time UnItO.2,
IOOOO
2：5	5：0	7：5
Train Timesteps le6
0
6	12	3
Train Tlmesteps le6
time unit 4.0,
g
2
250000 500000 750000
Train Timesteps


Figure 121:	Training Learning Curves for TD3 when varying time unit. Please note the different Y-axis
scales.
56
Under review as a conference paper at ICLR 2021

0	200000 «0000
0	200000 400000
Train Timesteps
0	200000 400000
TralnTlmesteps
-400
Train Timesteps
0	200000 400000


Figure 122:	Training Learning Curves for TD3 when varying action max. Please note the different Y-axis
scales.
PjeMdH

g
-T T
PjeMdH
tame unit 0.4,
0.5	1.0
Train Timesteps le6
PjeMdH
PjeMd≈
time unit 2.0,
g
100000 200000
Train Timesteps
tame unit 4.0,
50
100150g
--
PjeMd
50000	100000
Train Timesteps
Figure 123:	Training Learning Curves for TD3 when varying time unit. Please note the different Y-axis
scales.
严tιon SpaCe max 0Q5,,
0	200000 400000
Train Tlmesteps
0	200000 400000
TraInTlmesteps
Train Timesteps
0	200000 400000


Figure 124:	Training Learning Curves for TD3 when varying action max. Please note the different Y-axis
scales.
time unit 0.5,	time unit 1.0,
time unit 2.5,
0.0	0.5	1.0
Ti,alπ Tlmestsps le6
200000 400000	0
Train Timesteps
time unit 5.0,	fame unit 10.0,
-80-
0	50000	100000
Train Timesteps
20000	40000
TraInTImesteps
100000	200000
Train Timesteps
Figure 125:	Training Learning Curves for TD3 when varying time unit. Please note the different Y-axis
scales.
57
Under review as a conference paper at ICLR 2021
M Hyperparameter Tuning
We gained some interesting insights into the significance of certain hyperparameters while tuning
them for the different algorithms. Thus, our toy environments might in fact be good test beds for
researching hyperparameters in RL, too. For instance, target network update frequency turned out to
be very significant for learning and sub-optimal values led to very noisy and unreliable training and
unexpected results such as networks with greater capacity not performing well. Once we tuned it,
however, training was much more reliable and, as expected, networks with greater capacity did well.
We now describe the tuning process and an example insight in more detail.
Hyperparameters were tuned for the vanilla environment; we did so manually in order to obtain
good intuition about them before applying automated tools. We tuned the hyperparameters in sets,
loosely in order of their significance and did 3 runs over each setting to get a more robust performance
estimate. We describe a small part of our hyperparameter tuning for DQN next. All hyperparameter
settings for tuned agents can be found in Appendix N.
We expected that quite small neural networks would already perform well for such toy environments
and we initially grid searched over small network sizes (Figure 126a). However, the variance in
performance was quite high (Figure 126b). When we tried to tune DQN hyperparameters learning
starts and target network update frequency, however, it became clear that the target network update
frequency was very significant (Figure 126c and 126d) and when we repeated the grid search over
network sizes with a better value of 800 for the target network update frequency (instead of the oldd
80) this led to both better performance and lower variance (Figure 126e and 126f).
We then changed the network number of neurons grid to [128, 256, 512] and changed target network
update frequency grid to [80, 800, 8000] and continued with further tuning using the grid values
specified in Appendix N.
(a) Reward
(b) Std dev. (c) Reward (d) Std. Dev. (e) Reward (f) Std dev.
Figure 126: Mean episodic reward at the end of training for different hyperparameter sets for DQN.
Please note the different colorbar scales.
58
Under review as a conference paper at ICLR 2021
N Tuned Hyperparameters
The code for corresponding experiments for both discrete and continuous environments can be found
in the accompanying code for the paper. The README describes how to run the experiments using
config files and which config files correspond to which experiments. Experiments on the discrete
environments were run with Ray 0.7.3, while for the continuous environments, they were run with
Ray 0.9.0. We had to use Ray 0.7.3 for the discrete environments and Ray 0.9.0 for the continuous
ones because we had run the discrete cases for a previous version of the paper on 0.7.3. DDPG was
not working and SAC was not implemented in Ray at that time. We tried to use Ray 0.9.0 also for the
discrete version but found for the 1st algorithms we tested that, for the same hyperparameters, the
results did not transfer even across implementations of the same library. This further makes our point
about using our platform to unit test algorithms.
Since we did not save the hyperparameter grids for discrete environments in separate files, they are
provided here. The names of the hyperparameters for the algorithms will match those used in Ray
0.7.3.
N.1 DQN
num_layerss = [1, 2, 3, 4]
layer_widths = [8, 32, 128] # at first
layer_widths = [128, 256, 512] # after setting
target_net_update_freq = 800 showed that 128 was the best
number of the old 3, we changed search grid for number of
neurons
fcnet_activations = ["tanh", "relu", "sigmoid"]
learning_startss = [500, 1000, 2000, 4000, 8000]
target_network_update_freqs = [8, 80, 800] # at first
target_network_update_freqs = [80, 800, 8000] # after seeing
target_net_update_freq = 800 is much better than 80, changed
the grid for it
double_dqn = [False, True]
learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]
adam_epsilons = [1e-3, 1e-4, 1e-5, 1e-6] # also tried [1e-1, 1e-4,
1e-7, 1e-10]
tune.run(
"DQN",
stop={
"timesteps_total": 20000,
},
config={
"adam_epsilon": 1e-4,
"beta_annealing_fraction": 1.0,
"buffer_size": 1000000,
"double_q": False,
"dueling": False,
"exploration_final_eps": 0.01,
"exploration_fraction": 0.1,
"final_prioritized_replay_beta": 1.0,
"hiddens": None,
"learning_starts": 1000,
"lr": 1e-4,
"n_step": 1,
"noisy": False,
"num_atoms": 1,
"prioritized_replay": False,
"prioritized_replay_alpha": 0.5,
"sample_batch_size": 4,
59
Under review as a conference paper at ICLR 2021
"schedule_max_timesteps": 20000,
"target_network_update_freq": 800,
"timesteps_per_iteration": 100,
"train_batch_size": 32,
"env": "RLToy-v0",
"env_config": {
’dummy_seed’: dummy_seed,
’seed’: 0,
’state_space_type’: ’discrete’,
’action_space_type’: ’discrete’,
’state_space_size’: state_space_size,
’action_space_size’: action_space_size,
’generate_random_mdp’: True,
’delay’: delay,
’sequence_length’: sequence_length,
’reward_density’: reward_density,
’terminal_state_density’: terminal_state_density,
’repeats_in_sequences’: False,
’reward_unit’: 1.0,
’make_denser’: False,
’completely_connected’: True
},
"model": {
"fcnet_hiddens": [256, 256],
"custom_preprocessor": "ohe",
"custom_options": {},
"fcnet_activation": "tanh",
"use_lstm": False,
"max_seq_len": 20,
"lstm_cell_size": 256,
"lstm_use_prev_action_reward": False,
},
"callbacks": {
"on_episode_end": tune.function(on_episode_end),
"on_train_result": tune.function(on_train_result),
},
"evaluation_interval": 1,
"evaluation_config": {
"exploration_fraction": 0,
"exploration_final_eps": 0,
"batch_mode": "complete_episodes",
’horizon’: 100,
"env_config": {
"dummy_eval": True,
}
},
},
)
N.2 Rainb ow
num_layerss = [1, 2, 3, 4]
layer_widths = [128, 256, 512]
fcnet_activations = ["tanh", "relu", "sigmoid"]
learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]
learning_startss = [500, 1000, 2000, 4000, 8000]
target_network_update_freqs = [80, 800, 8000]
60
Under review as a conference paper at ICLR 2021
double_dqn = [False, True]
tune.run(
"DQN",
stop={
"timesteps_total": 20000,
},
config={
"adam_epsilon": 1e-4,
"buffer_size": 1000000,
"double_q": True,
"dueling": True,
"lr": 1e-3,
"exploration_final_eps": 0.01,
"exploration_fraction": 0.1,
"schedule_max_timesteps": 20000,
"learning_starts": 500,
"target_network_update_freq": 80,
"n_step": 4,
"noisy": True,
"num_atoms": 10,
"prioritized_replay": True,
"prioritized_replay_alpha": 0.75,
"prioritized_replay_beta": 0.4,
"final_prioritized_replay_beta": 1.0,
"beta_annealing_fraction": 1.0,
"sample_batch_size": 4,
"timesteps_per_iteration": 1000,
"train_batch_size": 32,
"min_iter_time_s": 1,
"env": "RLToy-v0",
"env_config": {
’dummy_seed’: dummy_seed,
’seed’: 0,
’state_space_type’: ’discrete’,
’action_space_type’: ’discrete’,
’state_space_size’: state_space_size,
’action_space_size’: action_space_size,
’generate_random_mdp’: True,
’delay’: delay,
’sequence_length’: sequence_length,
’reward_density’: reward_density,
’terminal_state_density’: terminal_state_density,
’repeats_in_sequences’: False,
’reward_unit’: 1.0,
’make_denser’: False,
’completely_connected’: True
},
"model": {
"fcnet_hiddens": [256, 256],
"custom_preprocessor": "ohe",
"custom_options": {},
"fcnet_activation": "tanh",
"use_lstm": False,
"max_seq_len": 20,
"lstm_cell_size": 256,
"lstm_use_prev_action_reward": False,
61
Under review as a conference paper at ICLR 2021
},
"callbacks": {
"on_episode_end": tune.function(on_episode_end),
"on_train_result": tune.function(on_train_result),
},
"evaluation_interval": 1,
"evaluation_config": {
"exploration_fraction": 0,
"exploration_final_eps": 0,
"batch_mode": "complete_episodes",
’horizon’: 100,
"env_config": {
"dummy_eval": True,
}
},
},
)
N.3 A3C
Grids of value for the hyperparameters over which they were tuned:
num_layerss = [1, 2, 3, 4]
layer_widths = [64, 128, 256]
learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]
fcnet_activations = ["tanh", "relu", "sigmoid"]
lambdas = [0, 0.5, 0.95, 1.0]
grad_clips = [10, 30, 100]
vf_loss_coeffs = [0.1, 0.5, 2.5]
entropy_coeffs = [0.001, 0.01, 0.1, 1]
tune.run(
"A3C",
stop={
"timesteps_total": 150000,
},
config={
"sample_batch_size": 10,
"train_batch_size": 100,
"use_pytorch": False,
"lambda": 0.0,
"grad_clip": 10.0,
"lr": 0.0001,
"lr_schedule": None,
"vf_loss_coeff": 0.5,
"entropy_coeff": 0.1,
"min_iter_time_s": 0,
"sample_async": True,
"timesteps_per_iteration": 5000,
"num_workers": 3,
"num_envs_per_worker": 5,
"optimizer": {
"grads_per_step": 10
},
62
Under review as a conference paper at ICLR 2021
"env": "RLToy-v0",
"env_config": {
’dummy_seed’: dummy_seed,
’seed’: 0,
’state_space_type’: ’discrete’,
’action_space_type’: ’discrete’,
’state_space_size’: state_space_size,
’action_space_size’: action_space_size,
’generate_random_mdp’: True,
’delay’: delay,
’sequence_length’: sequence_length,
’reward_density’: reward_density,
’terminal_state_density’: terminal_state_density,
’repeats_in_sequences’: False,
’reward_unit’: 1.0,
’make_denser’: False,
’completely_connected’: True
},
"model": {
"fcnet_hiddens": [128, 128, 128],
"custom_preprocessor": "ohe",
"custom_options": {},
"fcnet_activation": "tanh",
"use_lstm": False,
"max_seq_len": 20,
"lstm_cell_size": 256,
"lstm_use_prev_action_reward": False,
},
"callbacks": {
"on_episode_end": tune.function(on_episode_end),
"on_train_result": tune.function(on_train_result),
},
"evaluation_interval": 1,
"evaluation_config": {
"exploration_fraction": 0,
"exploration_final_eps": 0,
"batch_mode": "complete_episodes",
’horizon’: 100,
"env_config": {
"dummy_eval": True,
}
},
},
)
N.4 A3C + LSTM
Grids of value for the hyperparameters over which they were tuned:
num_layerss = [1, 2, 3, 4]
layer_widths = [64, 128, 256]
learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]
fcnet_activations = ["tanh", "relu", "sigmoid"]
lambdas = [0, 0.5, 0.95, 1.0]
grad_clips = [10, 30, 100]
63
Under review as a conference paper at ICLR 2021
vf_loss_coeffs = [0.1, 0.5, 2.5]
entropy_coeffs = [0.001, 0.01, 0.1, 1]
lstm_cell_sizes = [64, 256, 512]
lstm_use_prev_action_rewards = [False, True]
tune.run(
"A3C",
stop={
"timesteps_total": 150000,
},
config={
"sample_batch_size": 10,
"train_batch_size": 100,
"use_pytorch": False,
"lambda": 0.0,
"grad_clip": 10.0,
"lr": 0.0001,
"lr_schedule": None,
"vf_loss_coeff": 0.1,
"entropy_coeff": 0.1,
"min_iter_time_s": 0,
"sample_async": True,
"timesteps_per_iteration": 5000,
"num_workers": 3,
"num_envs_per_worker": 5,
"optimizer": {
"grads_per_step": 10
},
"env": "RLToy-v0",
"env_config": {
’dummy_seed’: dummy_seed,
’seed’: 0,
’state_space_type’: ’discrete’,
’action_space_type’: ’discrete’,
’state_space_size’: state_space_size,
’action_space_size’: action_space_size,
’generate_random_mdp’: True,
’delay’: delay,
’sequence_length’: sequence_length,
’reward_density’: reward_density,
’terminal_state_density’: terminal_state_density,
’repeats_in_sequences’: False,
’reward_unit’: 1.0,
’make_denser’: False,
’completely_connected’: True
},
"model": {
"fcnet_hiddens": [128, 128, 128],
"custom_preprocessor": "ohe",
"custom_options": {},
"fcnet_activation": "tanh",
"use_lstm": True,
"max_seq_len": delay + sequence_length,
"lstm_cell_size": 64,
"lstm_use_prev_action_reward": True,
64
Under review as a conference paper at ICLR 2021
},
"callbacks": {
"on_episode_end": tune.function(on_episode_end),
"on_train_result": tune.function(on_train_result),
},
"evaluation_interval": 1,
"evaluation_config": {
"exploration_fraction": 0,
"exploration_final_eps": 0,
"batch_mode": "complete_episodes",
’horizon’: 100,
"env_config": {
"dummy_eval": True,
}
},
},
)
65
Under review as a conference paper at ICLR 2021
O	More on Conclusion and Future Work
Among the continuous environments, we have a toy task of moving along a line. Here, we hand
out greater rewards the closer a point object is to moving along a line. This is also a better task to
test exploration than the completely random discrete environments. It already gave some interesting
results and further work will follow. We are in the process of implementing plug and play model-
based metrics to evaluate model-based algorithms, such as the Wasserstein metric (likely a sampled
version because analytical calculation would be intractable in many cases) between the true dynamics
models and the learnt one to keep track of how model learning is proceeding. Our Environments plan
to allow using their transition and reward functions to perform imaginary rollouts without affecting
the current state of the system.
Another significant meta-feature is reachability in the transition graph. We believe a lot of insights
can be gained from graph theory to model toy environments which try to mimic specific real life
situations at a very high level. We plan that users can specify their own transition graphs and also
plan to add random generation of specific types of transition graphs.
Even though we have a playground to generate environments where the dimensions such as sequence
length are constant, being able to solve environments with variable delay and sequence lengths and
identifying them (i.e., segmentation of events in the time domain) is another area we are currently
working on with attention-based agents and various other ideas.
The fine-grained control of dimensions allows relating these to good hyperparameter choices. So, our
playground could also be used to learn a mapping from hardness dimensions to hyperparameters for
different types of environments and even to warm-start hyperparameter optimisation for environments
with similar hardness dimensions. This holds promise for future meta-learning algorithms. In a
similar vein, it could also be used to perform Combined Algorithm Selection and Hyperparameter
Optimisation (Thornton et al., 2013), since it’s clear that currently different RL algorithms do well in
different kinds of environments.
Further interesting toy experiments which are already possible with our platform are varying the
terminal state densities to have environments for benchmarking safe RL.
The states and actions contained in a rewardable sequence could just be a single compound state and
compound action if we discretised time in a suitable manner. This brings us to the idea of learning at
multiple timescales. HRL algorithms with formulations like the options framework (Sutton et al.,
1999), could try to identify these rewardable sequences at the higher level and then carry out atomic
actions at the lower level.
We also hope to benchmark other algorithms like PPO5 (Schulman et al., 2017), Rudder (Arjona-
Medina et al., 2019), MCTS (Silver et al., 2016), DDPG6 (Lillicrap et al., 2016) on continuous tasks
and table-based algorithms and to show theoretical results match with practice on toy benchmarks.
We also aim to promote reproducibility in RL as in (Henderson et al., 2018) and hope our benchmark
helps with that goal. To this end, we have already improved the Gym Box and Discrete Spaces
to allow their seeds to be controlled at initialization time as well.
We need different RL algorithms for different environments. Aside from some basic heuristics
such as applying DDPG (Lillicrap et al., 2016) to continuous environments and DQN to discrete
environments, it is not very clear when to use which RL algorithms. We hope this will be a first
step to being able to identify from the environment what sort of algorithm to use and to help build
adaptive algorithms which adapt to the environment at hand. Additionally, aside from being a great
benchmark for RL algorithms, it is also a great didactic tool for teaching how RL algorithms work in
different environments.
5We tried PPO but could not get it to learn
6We tried DDPG also but there seemed to be a bug in the implementation and it crashed even on tuned
examples from Ray
66
Under review as a conference paper at ICLR 2021
P CPU specifications
processor
vendor_id
cpu family
model
model name
stepping
microcode
cpu MHz
cache size
physical id
siblings
core id
cpu cores
apicid
initial apicid
fpu
fpu_exception
cpuid level
:0
: GenuineIntel
:6
: 158
: Intel(R) Core(TM) i7-8850H CPU @ 2.60GHz
: 10
: 0xb4
: 900.055
: 9216 KB
:0
: 12
:0
:6
:0
:0
: yes
: yes
: 22
wp	: yes
flags	: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr
pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss
ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art
arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc
cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor
ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid
sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave
avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb
invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi
flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2
smep bmi2 erms invpcid rtm mpx rdseed adx smap clflushopt
intel_pt xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln
pts hwp hwp_notify hwp_act_window hwp_epp md_clear flush_l1d
bugs	:	cpu_meltdown	spectre_v1	spectre_v2
spec_store_bypass l1tf mds swapgs
bogomips	:	5184.00
clflush size	:	64
cache_alignment : 64
address sizes : 39 bits physical, 48 bits virtual
power management:
67