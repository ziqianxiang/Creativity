Under review as a conference paper at ICLR 2021
Learning Causal Semantic Representation for
Out-of-Distribution Prediction
Anonymous authors
Paper under double-blind review
Ab stract
Conventional supervised learning methods, especially deep ones, are found to be
sensitive to out-of-distribution (OOD) examples, largely because the learned rep-
resentation mixes the semantic factor with the variation factor due to their domain-
specific correlation, while only the semantic factor causes the output. To address
the problem, we propose a Causal Semantic Generative model (CSG) based on
causality to model the two factors separately, and learn it on a single training
domain for prediction without (OOD generalization) or with unsupervised data
(domain adaptation) in a test domain. We prove that CSG identifies the semantic
factor on the training domain, and the invariance principle of causality subse-
quently guarantees the boundedness of OOD generalization error and the success
of adaptation. We also design novel and delicate learning methods for both effec-
tive learning and easy prediction, following the first principle of variational Bayes
and the graphical structure of CSG. Empirical study demonstrates the effect of our
methods to improve test accuracy for OOD generalization and domain adaptation.
1	Introduction
Deep learning has initiated a new era of artificial intelligence where the potential of machine learn-
ing models is greatly unleashed. Despite the great success, these methods heavily rely on the
independently-and-identically-distributed (IID) assumption. This does not always perfectly hold
in practice, and the prediction of output (label, response, outcome) y may be saliently affected in
out-of-distribution (OOD) cases, even from an essentially irrelevant change to the input (covariate)
x, like a position shift or rotation of the object in an image, or a change of background, illumination
or style (Shen et al., 2018; He et al., 2019; Arjovsky et al., 2019). These phenomena pose serious
concerns on the robustness and trustworthiness of machine learning methods and severely impede
them from risk-sensitive scenarios.
Looking into the problem, although deep learning models allow extracting abstract representation
for prediction with their powerful approximation capacity, the representation may be overconfident
in the correlation between semantic factors s (e.g., shape of an object) and variation factors v (e.g.,
background, illumination, object position). The correlation may be domain-specific and spurious,
and may change drastically in a new environment. So it has become a desire to learn representation
that separates semantics s from variations v (Cai et al., 2019; Ilse et al., 2019). Formally, the impor-
tance of this goal is that s represents the cause of y . Causal relations better reflect the fundamental
mechanisms of nature, bringing the merit to machine learning that they tend to be universal and in-
variant across domains (ScholkoPf et al., 2012; Peters et al., 2017; Scholkopf, 2019), thus providing
the most transferable and confident information to unseen domains. Causality has also been shown
to lead to proper domain adaptation (Scholkopf et al., 2012; Zhang et al., 2013), lower adaptation
cost and lighter catastrophic forgetting (Peters et al., 2016; Bengio et al., 2019; Ke et al., 2019).
In this work, we propose a Causal Semantic Generative model (CSG) for proper and robust OOD
prediction, including OOD generalization and domain adaptation. Both tasks have supervised data
from a single training domain, but domain adaptation has unsupervised test-domain data during
learning, while OOD generalization has no test-domain data, including cases where queries come
sequentially or adaptation is unaffordable. (1) We build the model by cautiously following the prin-
ciple of causality, where we explicitly separate the latent variables into a (group of) semantic factor
s and a (group of) variation factor v. We prove that under appropriate conditions CSG identifies the
1
Under review as a conference paper at ICLR 2021
semantic factor by fitting training data, even in presence of an s-v correlation. (2) By leveraging the
causal invariance, we prove that a well-learned CSG is guaranteed to have a bounded OOD general-
ization error. The bound shows how causal mechanisms affect the error. (3) We develop a domain
adaptation method using CSG and causal invariance, which suggests to fix the causal generative
mechanisms and adapt the prior to the new domain. We prove the identification of the new prior and
the benefit of adaptation. (4) To learn and adapt the model from data, we design novel and delicate
reformulations of the Evidence Lower BOund (ELBO) objective following the graphical structure
of CSG, so that the inference models required therein can also serve for prediction, and model-
ing and optimizing inference models in both domains can be avoided. To our best knowledge, our
work is the first to identify semantic factor and leverage latent causal invariance for OOD prediction
with guarantees. Empirical improvement in OOD performance and adaptation is demonstrated by
experiments on multiple tasks including shifted MNIST and ImageCLEF-DA task.
2	Related Work
There have been works that aim to leverage the merit of causality for OOD prediction. For OOD gen-
eralization, some works ameliorate discriminative models towards a causal behavior. Bahadori et al.
(2017) introduce a regularizer that reweights input dimensions based on their approximated causal
effects to the output, and Shen et al. (2018) reweight training samples by amortizing causal effects
among input dimensions. They are extended to nonlinear cases (Bahadori et al., 2017; He et al.,
2019) via linear-separable representations. Heinze-Deml & Meinshausen (2019) enforce inference
invariance by minimizing prediction variance within each label-identity group. These methods intro-
duce no additional modeling effort, but may also be limited to capture invariant causal mechanisms
(they are non-generative) and may only behave quantitatively causal in the training domain.
For domain adaptation/generalization, methods are developed under various causal assumptions
(ScholkoPf et al., 2012; Zhang et al., 2013) or using learned causal relations (Rojas-Carulla et al.,
2018; Magliacane et al., 2018). Zhang et al. (2013); Gong et al. (2016; 2018) also consider certain
ways of mechanism shift. The considered causality is among directly observed variables, which
may not be suitable for general data like image Pixels where causality rather lies between data and
concePtual latent factors (LoPez-Paz et al., 2017; Besserve et al., 2018; Kilbertus et al., 2018). To
consider latent factors, there are domain adaPtation (Pan et al., 2010; Baktashmotlagh et al., 2013;
Ganin et al., 2016; Long et al., 2015; 2018) and generalization methods (Muandet et al., 2013;
Shankar et al., 2018) that learn a rePresentation with domain-invariant marginal distribution, and
have achieved remarkable results. Nevertheless, Johansson et al. (2019); Zhao et al. (2019) Point
out that this invariance is neither sufficient nor necessary to identify the true semantics and lower the
adaPtation error (SuPPlement D). Moreover, these methods and invariance risk minimization (Ar-
jovsky et al., 2019) also assume the invariance in the inference direction (i.e., data → rePresentation),
which may not be as general as causal invariance in the generative direction (Section 3.2).
There are also generative methods for domain adaPtation/generalization that model latent factors.
Cai et al. (2019); Ilse et al. (2019) introduce a semantic factor and a domain-feature factor. They
assume the two factors are indePendent in both the generative and inference models, which may
not meet reality closely. They also do not adaPt the Prior for domain shift thus resort to inference
invariance. Zhang et al. (2020) consider a Partially observed maniPulation variable, while assume its
indePendence from the outPut in both the joint and Posterior, and the adaPtation is inconsistent with
causal invariance. Atzmon et al. (2020) consider similar latent factors, but use the same (uniform)
Prior in all domains. These methods also do not show guarantees to identify their latent factors.
Teshima et al. (2020) leverage causal invariance and adaPt the Prior, while also assume latent inde-
Pendence and do not seParate the semantic factor. They require some suPervised test-domain data,
and their deterministic and invertible mechanism also indicates inference invariance. In addition,
most domain generalization methods require multiPle training domains, with excePtions (e.g., Qiao
et al., 2020) that still seek to augment domains. In contrast, CSG leverages causal invariance, and
has guarantee to identify the semantic factor from a single training domain, even with a correlation
to the variation factor.
Generative suPervised learning is not new (Mcauliffe & Blei, 2008; Kingma et al., 2014), but most
works do not consider the encoded causality. Other works consider solving causality tasks, notably
causal/treatment effect estimation (Louizos et al., 2017; Yao et al., 2018; Wang & Blei, 2019). The
task does not focus on OOD Prediction, and requires labels for both treated and controlled grouPs.
2
Under review as a conference paper at ICLR 2021
Disentangling latent representations is also of interest in unsupervised learning. Despite some em-
pirical success (Chen et al., 2016; Higgins et al., 2017; Chen et al., 2018), Locatello et al. (2019) con-
clude that it is impossible to guarantee the disentanglement in unsupervised settings. Khemakhem
et al. (2019; 2020) show an encouraging result that disentangled representation can be identified
up to a permutation with a cause of the latent variable observed. But the methods cannot separate
the semantic factor from variation for supervised learning, and require observing sufficiently many
different values of the cause variable, making it hard to leverage labels.
Causality with latent variable has been considered in a rich literature (Verma & Pearl, 1991; Spirtes
et al., 2000; Richardson et al., 2002; Hoyer et al., 2008; Shpitser et al., 2014), while most works
focus on the consequence on observation-level causality. Others consider identifying the latent vari-
able. Janzing et al. (2009); Lee et al. (2019) show the identifiability under additive noise or similar
assumptions. For discrete data, a “simple” latent variable can be identified under various specifica-
tions (Janzing et al., 2011; Sgouritsa et al., 2013; Kocaoglu et al., 2018). Romeijn & Williamson
(2018) leverage interventional datasets. Over these works, we step further to separate and identify
the latent variable as semantic and variation factors, and show the benefit for OOD prediction.
3	The Causal S emantic Generative Model
To develop the model seriously and soberly based on causality, we require the formal definition of
causality: two variables have a causal relation, denoted as “cause→effect'', if externally interven-
ing the cause (by changing variables out of the considered system) may change the effect, but not
vice versa (Pearl, 2009; Peters et al., 2017). We then follow the logic below to build our model. 1
(1)	It may be a general case that neither y → x (e.g., adding
noise to the labels in a dataset does not change the images)
nor x → y holds (e.g., intervening an image by e.g. break-
ing a camera sensor unit when taking the image, does not
change how the photographer labels it), as also argued by
Peters et al. (2017, Section 1.4); Kilbertus et al. (2018).
So we employ a generative model (i.e., not only modeling
p(y|x)), and introduce a latent variable z to capture factors
with causal relations.
(2)	The latent variable z as underlying generating factors
(e.g., object features like shape and texture, background and
illumination in imaging) is plausible to cause both x (e.g.,
the change of object shape or background makes a different
image, but breaking a camera sensor unit does not change
the object shape or background) and y (e.g., the photogra-
pher would give a different label if the object shape, texture,
etc. had been replaced by those of a different object, but
Figure 1: The graphical structure of
the proposed Causal Semantic Gener-
ative model (CSG) for the semantic s
and variation v latent factors and su-
pervised data (x, y). Black solid ar-
rows represent invariant causal gen-
erating mechanisms p(x|s, v) and
p(y|s), the black undirected edge rep-
resent a domain-specific prior p(s, v),
and blue dashed bended arrows repre-
sent the inference model q(s, v|x) for
learning and prediction.
noise-corrupting the label does not change the object features). So we orient the edges in the gener-
ative direction z → (x, y), as also adopted by Mcauliffe & Blei (2008); Peters et al. (2017); Teshima
et al. (2020). This is in contrast to Cai et al. (2019); Ilse et al. (2019; 2020); Castro et al. (2020)
who treat y as the cause of a semantic factor, which, when y is also a noisy observation, makes
unreasonable implications (e.g., adding noise to the labels in a dataset automatically changes object
features and consequently the images, and changing the object features does not change the label).
This difference is also discussed by Peters et al. (2017, Section 1.4); Kilbertus et al. (2018).
(3)	We attribute all x-y relation to the existence of some latent factors (“purely common cause”, Lee
et al., 2019; Janzing et al., 2009), and exclude x-y edges. This can be achieved as long as z holds
sufficient information of data (e.g., with shape, background etc. fixed, breaking a sensor unit does
not change the label, and noise-corrupting the label does not change the image). Promoting this
restriction reduces arbitrariness in explaining x-y relation and benefits the identification of z. This
is in contrast to Kingma et al. (2014); Zhang et al. (2020); Castro et al. (2020) who treat y as a cause
of x since no latent variable is introduced between.
1Supplement C provides more explanations on the model.
3
Under review as a conference paper at ICLR 2021
(4)	Not all latent factors are the causes of y (e.g., changing the shape may alter the label, while
changing the background does not). We thus split the latent variable as z = (s, v) and remove the
edge v → y, where s represents the semantic factor of x that causes y, and v describes the variation
or diversity in generating x. This formalizes the intuition on the concepts in Introduction.
(5)	The variation v often has a relation to the semantics s, which is often a spurious correlation
(e.g., desks prefer a workspace background, but they can also appear in bedrooms and beds can also
appear in workspace). So we keep the undirected s-v edge. Although v is not a cause of y, modeling
it explicitly is worth the effort since otherwise it would still be implicitly incorporated in s anyway
through the s-v correlation. We summarize these conclusions in the following definition.
Definition 3.1 (CSG). A Causal Semantic Generative Model (CSG) p = (p(s, v),p(x|s, v), p(y|s))
is a generative model on data variables x ∈ X ⊂ RdX and y ∈ Y with semantic s ∈ S ⊂ RdS and
variation v ∈ V ⊂ RdV latent variables, following the graphical structure shown in Fig. 1.
3.1	The Causal Invariance Principle
The domain-invariance of causal relations translates to the following principle for CSG:
Principle 3.2 (causal invariance). The causal generative mechanisms p(x|s, v) and p(y|s) in CSG
are invariant across domains, and the change of prior p(s, v) is the only source of domain shift.
It is supported by the invariance of basic laws of nature (Scholkopf et al., 2012; Peters et al., 2017;
Besserve et al., 2018; Buhlmann, 2018; Scholkopf, 2019). Other works instead introduce domain
index (Cai et al., 2019; Ilse et al., 2019; 2020; Castro et al., 2020) or manipulation variables (Zhang
et al., 2020; Khemakhem et al., 2019; 2020) to model distribution change explicitly. They require
multiple training domains or additional observations, and such changes can also be explained under
causal invariance as long as the latent variable includes all shifted factors (e.g., domain change of
images can be attributed to a different preference of shape, style, texture, background, etc. and their
correlations, while the processes generating image and label from them remain the same).
3.2	Comparison with Inference Invariance
Domain-invariant-representation-based adaptation and
generalization methods, and invariant risk minimization
(Arjovsky et al., 2019) for domain generalization, use
a shared feature extractor across domains. This effec-
tively assumes the invariance of the process in the other
direction, i.e., inferring the latent representation from
data. We note that in its supportive examples (e.g., in-
ferring the object position from an image, or extracting
the fundamental frequency from a vocal audio), generat-
ing mechanisms are nearly deterministic and invertible,
so that the posterior is almost determined by the inverse
function, and causal invariance implies inference invari-
ance. For noisy or degenerate mechanisms (Fig. 2), am-
Figure 2: Examples of noisy (left) or de-
generate (right) generating mechanisms
that lead to ambiguity in inference. Left:
handwritten digit that may be generated
as either "3” or “5”. Right: Schroder's
stairs that may be generated with either A
or B being the nearer surface. Inference
results notably rely on the prior on the
digits/surfaces, which is domain-specific.
biguity occurs during inference since there may be multiple values of a latent feature that generate
the same observation. The inferred feature would notably rely on the prior through the Bayes rule.
Since the prior changes across domains, the inference rule then changes by nature, which challenges
the existence of a domain-shared feature extractor. In this case, causal invariance is more reliable
than inference invariance.
To leverage causal invariance, we adjust the prior conservatively for OOD generalization (CSG-ind)
and data-driven for domain adaptation (CSG-DA), so together with the invariant generative mecha-
nisms, it gives a different and more reliable inference rule than that following inference invariance.
4	Method
We develop learning, adaptation and prediction methods for OOD generalization and domain adap-
tation using CSG following the causal invariance Principle 3.2, and devise practical objectives using
variational Bayes. Supplement E.1 details all the derivations.
4
Under review as a conference paper at ICLR 2021
4.1	Method for OOD Generalization
For OOD generalization, a CSG p = (p(s, v),p(x|s, v), p(y|s)) needs to first learn from the super-
vised data from an underlying data distribution p* (x, y) on the training domain. Maximizing likeli-
hood Ep* (x,y) [log p(x, y)] is intractable since p(x, y) given by the CSG p is hard to estimate effec-
tively. We thus adopt the Evidence Lower BOUnd (ELBO)Lq,p(x, y) := Eq(s,v∣χ,y) [log P(S,,VjX,,y)]
(Jordan et al., 1999; Wainwright et al., 2008) as a tractable surrogate, which requires an auxiliary
inference model q(s, v|x, y) to estimate the expectation effectively. Maximizing Lq,p w.r.t q drives
q towards the posterior p(s, v|x, y) and meanwhile makes Lq,p a tighter lower bound of log p(x, y).
The expected ELBO Ep*(χ,y) [Lqp(x, y)] then drives p(x, y) towards p*(x, y).
However, the subtlety with supervised learning is that after fitting data, evaluating p(y|x) for pre-
diction is still hard. We thus propose to employ a model for q(s, v, y|x) instead. The required
inference model can be then expressed as q(s, v|x, y) = q(s, v, y|x)/q(y|x) where q(y|x) =
q(s, v, y|x) dsdv. It reformulates the expected ELBO as:
[p*(y∣x)	p(s,v,x,y)]
q(y∣χ) log q(s V y|x) 1.(I)
The first term is the common cross entropy loss (negative) driving q(y|x) towards p*(y∣χ). Once
this is achieved, the second term becomes the expected ELBO Ep*(x) [Lq(s,v,y|x),p(x)] that drives
q(s,v,y∣x) towards p(s,v,y∣x) (andp(x) towards p*(x)). Since the targetp(s,v,y∣x) admits the
factorization p(s, v|x)p(y|s) (since (v, x) ⊥ y|s ) where p(y|s) is already given by the CSG, we
can further ease the modeling of q(s, v, y|x) as q(s, v|x)p(y|s). The ELBO is then reformulated as:
Lq,p(χ,y) = log q(y|x)+ɪ1^) Eq(s,v|x)hp(y|s)log P(Sqv)p，；V) i,	⑵
q(y|x)	q(s, v|x)
where q(y|x) = Eq(s,v|x) [p(y|S)]. The CSG p and q(S, v|x) are to be optimized. The expectations
can be estimated by Monte Carlo, and their gradients can be estimated using the reparameterization
trick (Kingma & Welling, 2014). When well optimized, q(S, v|x) well approximates p(S, v|x), so
q(y|x) then well approximates p(y|x) = Ep(s,v|x) [p(y|S)] for prediction.
CSG-ind To actively mitigate the spurious S-v correlation from the training domain, we also con-
sider a CSG with an independent prior p ⊥(S, v) := p(S)p(v) for prediction in the unknown test
domain, where p(S) and p(v) are the marginals of p(S, v). The independent prior p ⊥(S, v) encour-
ages the model to stay neutral on the S-v correlation. It has a larger entropy than p(S, v) (Cover &
Thomas, 2006, Theorem 2.6.6), so it reduces the information of the training-domain-specific prior.
The model then relies more on the invariant generative mechanisms, thus better leverages causal
invariance and gives more reliable prediction than that following inference invariance.
For the method, note that the prediction is given by p⊥(y∣χ) = Ep⊥(s,v∣χ)[p(y∣s)], so We use an
inference model for q ⊥(S, v|x) that approximates p ⊥(S, v|x). However, learning on the training
domain still requires the original inference model q(S, v|x). To save the cost of building and learning
two inference models, we propose to use q ⊥(S, v|x) to represent q(S, v|x). Noting that their targets
nre reIntedbV τ√ Q 川^、一 P(S,V) P⊥(X) rt⊥q 川个、WefCrmnlnte <√ q -lʃʌ 一 P(S,V) P⊥(X) n⊥( q .,lʃʌ
arerelatedDy p(s,v|x) = p ⊥(s,v) P(X) p (s,v|x), we formulate q(s,v|x) = p ⊥(s,v) P(X) q (S,v|x)
accordingly, so that this q(S, v|x) achieves its target once q ⊥(S, v|x) does. The ELBO then becomes:
Lq,P(x, y) = logn(y|x) + π⅛yEq⊥(s,v∣χ)[τ⊥(⅛p(y|s) log p (S⊥vSpv[S,v)i，	⑶
π (y|X)	Lp (S,v)	q (S,V|X)
where ∏(y∣χ) := Eq⊥(s,ν∣χ)["：；%p(y∣s)]. The CSGp and q(s,v∣χ) are to be optimized (note that
p ⊥(s, v) is determined by p(s, v) intheCSG p). Prediction is given by p ⊥(y∣x) ≈ Eq ⊥(s,v∣χ) [p(y∣s)].
4.2	Method for Domain Adaptation
When unsupervised data is available from an underlying data distribution p* (x) on the test domain,
we can leverage it for adaptation. According to the causal invariance Principle 3.2, we only need to
adapt for the test-domain prior p(s, v) and the corresponding inference model q(s, v|x), while the
causal mechanisms p(x|S, v) and p(y|S) are not optimized. Adaptation is done by fitting the test
5
Under review as a conference paper at ICLR 2021
data via maximizing Ep* ⑺[Lq,p(x)], where the ELBO is in the standard form:
Lq,p(x) = %(s,v∣x) [ log (P(S, V)p(xls,。”贸s, VIx))].	⑷
Prediction is given by p(y∣x) ≈ Eq(s,v∣x)[p(y∣s)]. Similar to the case of CSG-ind, we need q(s,v∣x)
for prediction, but q(S, v|x) is still required for learning on the training domain. When data from
both domains are available during learning, we can save the effort of modeling and learning q(S, v|x)
using a similar technique. We formulate it using q(s, v|x) as q(s,v∣x) = KI) P(S,：) q(s, v∣x) follow-
ing the same relation between their targets, and the ELBO on the training domain becomes:
Lq,p(x,y) = log∏(y∣x) + ∕∣ ʌEq(s,v∣∣) [p(s, v)p(y∣s)logp(s,~V)P(X|s,V)],	(5)
∏(y∣x)	LP(S,v)	q(s,v∣x)	」
where π(y∣x) := Eq(s,v∣∣) [p(：：)p(y∣s)]. The CSG P and q(s, v∣x) are to be optimized (not for
p(s, v)). The resulting method, termed CSG-DA, solves both optimizations (4, 5) simultaneously.
For implementing the three methods, note that only one inference model is required in each case.
Supplement E.2 shows its implementation from a general discriminative model (e.g., how to select
its hidden nodes as S and v). In practice x often has a much larger dimension than y, making the
supervised part of the training-domain ELBO (i.e., the first term in its formulation Eq. (1)) scales
smaller than the unsupervised part. So we include an additional cross entropy loss in the objectives.
5	Theory
We now establish guarantee for the methods on identifying the semantic factor and the subsequent
merits for OOD generalization and domain adaptation. We only consider the infinite-data regime
to isolate another source of error from finite data. Supplement A shows all the proofs. Identifi-
ability is hard to achieve for latent variable models (Koopmans & Reiersol, 1950; Murphy, 2012;
Yacoby et al., 2019; Locatello et al., 2019), since it is a task beyond modeling observational relations
(Janzing et al., 2009; Peters et al., 2017). Assumptions are required to draw definite conclusions.
Assumption 5.1 (additive noise). There exist nonlinear functions f and g with bounded derivatives
UP to third-order, and independent random variables μ and V, such that p(x∣s, v) = p*(x - f (s, v)),
and p(y|S) = pν(y - g(S)) for continuous y orp(y|S) = Cat(y|g(S)) for categorical y.
This structure disables describing a bivariate joint distribution in both generating directions (Zhang
& Hyvarinen (2009, Theorem 8), Peters et al. (2014, Proposition 23)), and is widely adopted in
directed causal discovery (Janzing et al., 2009; Buhlmann et al., 2014). CSG needs this since it
should make the causal direction exclusive. It is also easy to implement with deep models (Kingma
& Welling, 2014), so does not essentially restrict model capacity.
Assumption 5.2 (bijectivity). Function f is bijective and g is injective.
It is a common assumption for identifiability (Janzing et al., 2009; Shalit et al., 2017; Khemakhem
et al., 2019; Lee et al., 2019). Under Assumption 5.1, it is a sufficient condition (Peters et al., 2014,
Proposition 17; Peters et al., 2017, Proposition 7.4) of causal minimality (Peters et al., 2014, p.2012;
Peters et al., 2017, Definition 6.33), a fundamental requirement for identifiability (Peters et al., 2014,
Proposition 7; Peters et al., 2017, p.109). Particularly, S and v are otherwise allowed to have dummy
dimensions that f and g simply ignore, raising another ambiguity against identifiability. On the
other hand, according to the commonly acknowledged manifold hypothesis (Weinberger & Saul,
2006; Fefferman et al., 2016) that data tends to lie on a lower-dimensional manifold embedded in
the data space, we can take X as the manifold and such a bijection exists as a coordinate map, which
is an injection to the original data space (thus allowing dS + dV < dX).
5.1	Identifiability Theory
We first formalize the goal of identifying the semantic factor.
Definition 5.3 (semantic-equivalence). We say two CSGs p and p0 are semantic-equivalent, if there
exists a homeomorphism2 Φ on S × V , such that (i) its output dimensions in S is constant of v:
2A transformation is a homeomorphism if it is a continuous bijection with continuous inverse.
6
Under review as a conference paper at ICLR 2021
ΦS(s, V) = Φs(S) for any V ∈ V, and (ii) it acts as a reparameterization from P to p0: Φ#[ps,v]=
P0s,v, p(x∣s,v) = p0(x∣Φ(s,v)) andp(y∣s) = p0(y∣ΦS(s)).
It is an equivalent relation if V is connected and is either open or closed in RdV (Supplement A.1).
Here, Φ#[ps,v] denotes the PUshed-forward distribution3 by Φ, i.e. the distribution of the trans-
formed random variable Φ(s,v) when (s,v)〜 ps,v. As a reparameterization, Φ allows the two
models to have different latent-variable Parameterizations while inducing the same distribution on
the observed data variables (x, y) (Supplement Lemma A.2). At the heart of the definition, the V-
constancy of ΦS implies that Φ is semantic-preserving: one model does not mix the other’s V into
its s, so that the s variables of both models hold equivalent information.
We say that a learned CSGp identifies the semantic factor ifit is semantic-equivalent to the ground-
truth CSGp*. This identification cannot be characterized by the statistical independence between S
and V (as in Cai et al. (2019); Ilse et al. (2019); Zhang et al. (2020)), which is not sufficient (Locatello
et al., 2019) nor necessary (due to the existence of spurious correlation). Another related concept is
disentanglement. It requires that a semantic transformation on x changes the learned S only (Higgins
et al., 2018; Besserve et al., 2020), while the identification here does not require the learned V to be
constant of the ground-truth S.
To identify the semantic factor, the ground-truth model could at most provide its information via
the data distribution p* (x, y). Although semantic-equivalent CSGs induce the same distribution on
(x, y), the inverse is nontrivial. The following theorem shows that the semantic-identifiability can
be achieved under appropriate conditions.
Theorem 5.4 (semantic-identifiability). With Assumptions 5.1 and 5.2, a well-learned CSG p with
p(x, y) = p* (x, y) is semantic-equivalent to the ground-truth CSGp*, if log p(S, V) and log p*(S, V)
have bounded derivatives up to the second-order, and that4 * (i) = → ∞ where σ' := E[μ>μ], or
(ii) pμ has an a.e. non-zero characteristic function (e.g., a Gaussian distribution).
Remarks. (1) The requirement on p(S, V) and p*(S, V) excludes extreme training data that show a
deterministic S-V relation, which makes the (S, V) density functions unbounded and discontinuous.
In that case (e.g., all desks appear in workspace and all beds in bedrooms), one cannot tell whether
the label y is caused by S (e.g., the shape) or by V (e.g., the background).
(2)	In condition (i), = measures the intensity of the causal mechanism p(x∣s, v). A strong p(x∣s, V)
helps disambiguating values of (S, V) in generating a given x. The condition makes p(x|S, V) so
strong that it is almost deterministic and invertible, so inference invariance also holds (Section 3.2).
Supplement A.2 provides a quantitative reference of large intensity for a practical consideration, and
Supplement B gives a non-asymptotic extension showing how the intensity trades-off the tolerance
of equalities in Definition 5.3. Condition (ii) covers more than inference invariance. It roughly
implies that different values of (S, V) a.s. produce different distributions p(x|S, V) on X, so their
roles in generating x become clear which helps identification.
(3)	The theorem does not contradict the impossibility result by Locatello et al. (2019), which con-
siders disentangling each latent dimension with an unconstrained (S, V) → (x, y), while we identify
S as a whole with the edge V → y removed which breaks the S-V symmetry.
5.2	OOD Generalization Theory
The causal invariance Principle 3.2 forms the ground-truth CSG on the test domain as p* =
(P*(s,v),P*(x∣s,v),P*(y∣s)) with the new ground-truth prior p*(s,v), which gives the optimal
*
predictor E [y|x] 5 on the test domain. The principle also leads to the invariance of identified causal
mechanisms, which shows that the OOD generalization error of a CSG is bounded:
Theorem 5.5 (OOD generalization error). With Assumptions 5.1 and 5.2, for a semantically-
identified CSG P on the training domain with reparameterization Φ, we have up to O(oj) that
3The definition of 中#[0$,冢]requires Φ to be measurable. This is satisfied by the continuity of Φ as a
homeomorphism (as long as the considered σ-field is the Borel σ-field) (Billingsley, 2012, Theorem 13.2).
4To be precise, the semantic-equivalent conclusions are that the equalities in Definition 5.3 hold asymptoti-
cally in the limit
5
σ⅛- → ∞ for condition (i), and hold a.e. for condition (ii).
For categorical y, the expectation of y is taken under the one-hot representation.
7
Under review as a conference paper at ICLR 2021
for any X ∈ SuPP(Px) ∩ SuPP(Px),
|E[y|x] - E*[y|x]| 6 σμINg(S)k2Jf-ι(x)∣∣22kvlog(P(S,v)/P(S,叫心仁片厂十x), ⑹
where SuPP denotes the support of a distribution, Jf-1 is the Jacobian matrix of f-1, and p§,v :=
①#^；" is the test-domain prior under the parameterization Ofthe identified CSG P. 6
The result shows that when the causal mechanism P(x|S, v) is strong, especially in the extreme
case σμ = 0 where inference invariance also holds, it dominates prediction over the prior and the
generalization error diminishes. In more general cases where only causal invariance holds, the prior
change deviates the prediction rule. The prior-change term ∣∣Vlog(P(s,v)∕P(s,v))k2 measures the
hardness or severity of OOD. It diminishes in IID cases, and makes the bound lose its effect when
the two priors do not share their support. Using a CSG to fit training data enforces causal invariance
and other assumptions, so its E[y∣x] behaves more faithfully in low p* (x) area and the boundedness
becomes more plausible in practice. CSG-ind further actively uses an independent prior whose
larger support covers more限~ candidates.
5.3	Domain Adaptation Theory
In cases of weak causal mechanism or violent prior change, the new ground-truth prior p；V is im-
portant for prediction. The domain adaptation method learns a new prior /§山 by fitting unsupervised
test-domain data, with causal mechanisms shared. Once the mechanisms are identified, p；V can also
be identified under the learned parameterization, and prediction can be made precise.
Theorem 5.6 (domain adaptation error). Under the conditions of Theorem 5.4, for a semantically-
identified CSG P on the training domain with reparameterization Φ, if its new prior Ps V for the test
1	■ ■ TTT	1 ∙,r~∕∖	~*∕∖,r	〜	工 「~*1	7E「I1 e*「I1/'
domain is well-learned with p(x) = P (x), then Ps,v = ①# [Ps,v], and E[y∣x] = E [y|x] for any
x ∈ SuPP(Px).
Different from existing domain adaptation bounds (Supplement D), Theorems 5.5 and 5.6 allow
different inference models in the two domains, thus go beyond inference invariance.
6 Experiments
For baselines of OOD generalization, apart from the conventional supervised learning optimizing
cross entropy (CE), we also consider a causal discriminative method CNBB (He et al., 2019), and
a generative method supervised VAE (sVAE) which is a counterpart of CSG that does not separate
its latent variable into S and v. For domain adaptation, we consider well-acknowledged DANN
(Ganin et al., 2016), DAN (Long et al., 2015) and CDAN (Long et al., 2018) methods implemented
in the dalib package (Jiang et al., 2020), and also sVAE using a similar method as CSG-DA. All
methods share the same optimization setup. We align the scale of the CE term in the objectives of
all methods, and tune their hyperparameters to lie on the margin that makes the final accuracy near
1 on a validation set from the training domain. See Supplement F for details.
6.1	Shifted MNIST
We consider an OOD prediction task on MNIST to classify digits “0” and “1”. In the training data,
“0"S are horizontally shifted at random by δ pixels with δ 〜N(-5,12), and “1”S by δ 〜N(5,12)
pixels. We consider two test domains where the digits are not moved, or are shifted δ 〜 N(0,22)
pixels. Both domains have balanced classes. We implement all methods using multilayer perceptron
which is not naturally shift invariant. We use a larger architecture for discriminative and domain
adaptation methods to compensate the additional generative components of generative methods.
The OOD performance is shown in Table 1. For OOD generalization, CSG gives more genuine
predictions in unseen domains, thanks to the identification of the semantic factor. CSG-ind performs
even better, demonstrating the merit of approaching a CSG with an independent prior. Other methods
are more significantly misled by the position factor from the spurious correlation. CNBB ameliorates
the position bias, but not as thoroughly without explicit structures for causal mechanisms. CSG
6The 2-norm ∣∣∙k2 for matrices refers to the induced operator norm (not the Frobenius norm).
8
Under review as a conference paper at ICLR 2021
Table 1:	Accuracy (%) of various methods (ours in bold) on OOD generalization (left) and domain
adaptation (right) for shifted MNIST. Averaged over 10 runs.
shift I CE CNBB SVAE CSG CSG-ind∣ DANN DAN CDAN SVAE-DA CSG-DA
=0 53.3± 8.8 74.4± 6.2 60.5±13.9 90.9± 6.8 94.5± 4.5 91.6± 5.5 11.3±11.5 60.1±41.4 50.3± 3.9 95.2±11.9
N(0,22) 52.7± 2.8 58.0± 1.7 58.6± 5.8 64.8± 2.7 66.5± 4.7 47.0± 2.9 50.1± 2.9 49.2± 5.6 68.0± 7.5 76.0± 3.4
Table 2:	ReSultS on ImageCLEF-DA (ima, 2014). ReSultS of CE, DANN, DAN and CDAN are taken
from Long et al. (2018).
task I CE CNBB SVAE CSG CSG-ind∣ DANN DAN CDAN SVAE-DA CSG-DA
C→P 65.5± 0.3 72.7± 1.1 73.3± 1.0 73.4± 0.8 73.7± 0.6 74.3± 0.5 69.2± 0.4 74.5± 0.3 74.3± 0.4 74.4± 0.6
P→C 91.2± 0.3 91.7± 0.2 91.6± 0.9 92.3± 0.5 92.5± 0.3 91.5± 0.6 89.8± 0.4 93.5± 0.4 92.6± 0.3 92.9± 0.3
alSo outperformS SVAE, Showing the benefit of Separating SemanticS and variation and modeling the
variation explicitly, So the model could conSciouSly drive Semantic repreSentation into s. For domain
adaptation, exiSting methodS differ a lot, and are hard to perform well on both teSt domainS. When
fail to identify, adaptation SometimeS even worSenS the reSult, aS the miSleading repreSentation baSed
on poSition getS Strengthened on the unSuperviSed teSt data. CSG iS benefited from adaptation by
leveraging teSt data in a proper way that identifieS the SemanticS.
6.2 ImageCLEF-DA
ImageCLEF-DA (ima, 2014) iS a Standard benchmark dataSet for the ImageCLEF 2014 domain
adaptation challenge. We Select a pair of adaptation taSkS between two of itS domainS: Caltech-256
and PaScal VOC 2012. Each domain haS 12 claSSeS and 600 imageS following a different diStribution
from each other. We adopt the Same Setup aS in Long et al. (2018), including the ReSNet50 Structure
(He et al., 2016) pretrained on ImageNet aS the backbone of the diScriminative/inference model. For
generative methodS, we leverage the DCGAN generator (Radford et al., 2015) pretrained on Cifar10.
Table 2 ShowS the reSultS. We See that CSG(-ind) achieveS the beSt OOD generalization reSult, and
performS comparable with modern domain adaptation methodS. On thiS taSk, the underlying cauSal
mechaniSm may be very noiSy (e.g., photoS taken from inSide and outSide both count for the aircraft
claSS), making identification hard. So CSG-DA doeS not make a Salient improvement.
7 Conclusion and Discussion
We tackle OOD generalization and domain adaptation taSkS by propoSing a CauSal Semantic Gener-
ative model (CSG), which buildS upon a cauSal reaSoning, and modelS Semantic and variation factorS
Separately while allowing their correlation. USing the invariance principle of cauSality, we develop
effective and delicate methodS for learning, adaptation and prediction, and prove the identification
of the Semantic factor, the boundedneSS of OOD generalization error, and the SucceSS of adaptation
under appropriate conditionS. ExperimentS Show the improved performance in both taSkS.
The conSideration of Separating SemanticS from variation extendS to broader exampleS regarding
robuStneSS. Convolutional neural networkS are found to change itS prediction under a different tex-
ture but the Same Shape (GeirhoS et al., 2019; Brendel & Bethge, 2019). AdverSarial vulnerability
(Szegedy et al., 2014; Goodfellow et al., 2015; Kurakin et al., 2016) extendS variation factorS to
human-imperceptible featureS, i.e. the adverSarial noiSe, which iS Shown to have a Strong Spuri-
ouS correlation with SemanticS (IlyaS et al., 2019). The Separation alSo matterS for fairneSS when
a SenSitive variation factor may change prediction due to a SpuriouS correlation. Our methodS are
potentially beneficial in theSe exampleS.
References
The imageclef-da challenge 2014. httpS://www.imageclef.org/2014, 2014.
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
9
Under review as a conference paper at ICLR 2021
Yuval Atzmon, Felix Kreuk, Uri Shalit, and Gal Chechik. A causal view of compositional zero-shot
recognition. Advances in Neural Information Processing Systems, 33, 2020.
Mohammad Taha Bahadori, Krzysztof Chalupka, Edward Choi, Robert Chen, Walter F Stewart, and
Jimeng Sun. Causal regularization. arXiv preprint arXiv:1702.02604, 2017.
Mahsa Baktashmotlagh, Mehrtash T Harandi, Brian C Lovell, and Mathieu Salzmann. Unsupervised
domain adaptation by domain invariant projection. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 769-776, 2013.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010a.
Shai Ben-David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain adap-
tation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics,pp. 129-136, 2010b.
Yoshua Bengio, Tristan Deleu, NaSim Rahaman, Rosemary Ke, SebaStien Lachapelle, Olexa Bila-
niuk, Anirudh Goyal, and Christopher Pal. A meta-transfer objective for learning to disentangle
causal mechanisms. arXiv preprint arXiv:1901.10912, 2019.
Michel Besserve, Naji Shajarisales, Bernhard Scholkopf, and Dominik Janzing. Group invariance
principles for causal generative models. In International Conference on Artificial Intelligence and
Statistics,pp. 557-565. PMLR, 2018.
Michel Besserve, Arash Mehrjou, Remy Sun, and Bernhard Scholkopf. Counterfactuals uncover the
modular structure of deep generative models. In Proceedings of the International Conference on
Learning Representations (ICLR 2020), 2020.
Irving Biederman. Recognition-by-components: a theory of human image understanding. Psycho-
logical review, 94(2):115, 1987.
Patrick Billingsley. Probability and Measure. John Wiley & Sons, New Jersey, 2012. ISBN 978-1-
118-12237-2.
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Wieland Brendel and Matthias Bethge. Approximating CNNs with bag-of-local-features models
works surprisingly well on ImageNet. In Proceedings of the International Conference on Learn-
ing Representations (ICLR 2019), 2019.
Peter Buhlmann. Invariance, causality and robustness. arXiv preprint arXiv:1812.08233, 2018.
Peter Buhlmann, Jonas Peters, Jan Ernest, et al. CAM: Causal additive models, high-dimensional
order search and penalized regression. The Annals OfStatiStiCS, 42(6):2526-2556, 2014.
Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, and Zhifeng Hao. Learning disentan-
gled semantic representation for domain adaptation. In Proceedings of the Conference of IJCAI,
volume 2019, pp. 2060. NIH Public Access, 2019.
Daniel C Castro, Ian Walker, and Ben Glocker. Causality matters in medical imaging. Nature
Communications, 11(1):1-10, 2020.
Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disen-
tanglement in variational autoencoders. In Advances in Neural Information Processing Systems,
pp. 2610-2620, 2018.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in Neural Information Processing Systems, pp. 2172-2180, 2016.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2006.
10
Under review as a conference paper at ICLR 2021
Dominik Maria Endres and Johannes E Schindelin. A new metric for probability distributions. IEEE
Transactions on Information theory, 49(7):1858-1860, 2003.
Charles Fefferman, Sanjoy Mitter, and Hariharan Narayanan. Testing the manifold hypothesis.
Journal of the American Mathematical Society, 29(4):983-1049, 2016.
Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model un-
certainty in deep learning. In Proceedings of the International Conference on Machine Learning,
pp. 1050-1059, 2016.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. Journal of Machine Learning Research, 17:1-35, 2016.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and
Wieland Brendel. ImageNet-trained CNNs are biased towards texture; increasing shape bias
improves accuracy and robustness. In Proceedings of the International Conference on Learning
Representations (ICLR 2019), 2019.
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Scholkopf. Domain adaptation with conditional transferable components. In International Con-
ference on Machine Learning, pp. 2839-2848, 2016.
Mingming Gong, Kun Zhang, Biwei Huang, Clark Glymour, Dacheng Tao, and Kayhan Batmanghe-
lich. Causal generative domain adaptation networks. arXiv preprint arXiv:1804.04333, 2018.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In Proceedings of the International Conference on Learning Representations (ICLR
2015), 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Yue He, Zheyan Shen, and Peng Cui. Towards non-i.i.d. image classification: A dataset and base-
lines. arXiv preprint arXiv:1906.02899, 2019.
Christina Heinze-Deml and Nicolai Meinshausen. Conditional variance penalties and domain shift
robustness. stat, 1050:13, 2019.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a
constrained variational framework. In Proceedings of the International Conference on Learning
Representations (ICLR 2017), 2017.
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende,
and Alexander Lerchner. Towards a definition of disentangled representations. arXiv preprint
arXiv:1812.02230, 2018.
Patrik O Hoyer, Shohei Shimizu, Antti J Kerminen, and Markus Palviainen. Estimation of causal
effects using linear non-gaussian causal models with hidden variables. International Journal of
Approximate Reasoning, 49(2):362-378, 2008.
Aapo Hyvarinen. Estimation of non-normalized statistical models by score matching. Journal of
Machine Learning Research, 6(Apr):695-709, 2005.
Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. DIVA: Domain invariant
variational autoencoders. arXiv preprint arXiv:1905.10427, 2019.
Maximilian Ilse, Jakub M Tomczak, and Patrick Forre. Designing data augmentation for simulating
interventions. arXiv preprint arXiv:2005.01856, 2020.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial examples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp. 125-136, 2019.
11
Under review as a conference paper at ICLR 2021
Dominik Janzing, Jonas Peters, Joris M Mooij, and Bernhard SchOlkopf. Identifying ConfoUnders
using additive noise models. In Proceedings of the 25th Conference on Uncertainty in Artificial
Intelligence (UAI2009),pp. 249-257. AUAI Press, 2009.
Dominik Janzing, Eleni Sgouritsa, Oliver Stegle, Jonas Peters, and Bernhard Scholkopf. Detecting
low-complexity Unobserved caUses. In 27th Conference on Uncertainty in Artificial Intelligence
(UAI 2011), pp. 383-391. AUAI Press, 2011.
Junguang Jiang, Bo Fu, and Mingsheng Long. Transfer-learning-library. https://github.
com/thuml/Transfer-Learning-Library, 2020.
Fredrik D Johansson, David Sontag, and Rajesh Ranganath. Support and invertibility in domain-
invariant representations. In The 22nd International Conference on Artificial Intelligence and
Statistics, pp. 527-536, 2019.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Nan Rosemary Ke, Olexa Bilaniuk, Anirudh Goyal, Stefan Bauer, Hugo Larochelle, Chris Pal, and
Yoshua Bengio. Learning neural causal models from unknown interventions. arXiv preprint
arXiv:1910.01075, 2019.
Ilyes Khemakhem, Diederik P Kingma, and Aapo Hyvarinen. Variational autoencoders and nonlin-
ear ICA: A unifying framework. arXiv preprint arXiv:1907.04809, 2019.
Ilyes Khemakhem, Ricardo Pio Monti, Diederik P Kingma, and Aapo Hyvarinen. ICE-BeeM: Iden-
tifiable conditional energy-based deep models. arXiv preprint arXiv:2002.11537, 2020.
Niki Kilbertus, Giambattista Parascandolo, and Bernhard Scholkopf. Generalization in anti-causal
learning. arXiv preprint arXiv:1812.00524, 2018.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proceedings of the
International Conference on Learning Representations (ICLR 2014), Banff, Canada, 2014. ICLR
Committee.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in Neural Information Processing Systems,
pp. 3581-3589, 2014.
Murat Kocaoglu, Sanjay Shakkottai, Alexandros G Dimakis, Constantine Caramanis, and Sriram
Vishwanath. Entropic latent variable discovery. arXiv preprint arXiv:1807.10399, 2018.
Tjalling C Koopmans and Olav Reiersol. The identification of structural characteristics. The Annals
of Mathematical Statistics, 21(2):165-181, 1950.
Alexey Kurakin, Ian Goodfellow, and Samy Bengio. Adversarial examples in the physical world.
arXiv preprint arXiv:1607.02533, 2016.
Ciaran M Lee, Christopher Hart, Jonathan G Richens, and Saurabh Johri. Leveraging directed causal
discovery to detect latent common causes. arXiv preprint arXiv:1910.10174, 2019.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard
Scholkopf, and Olivier Bachem. Challenging common assumptions in the unsupervised learn-
ing of disentangled representations. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceed-
ings of Machine Learning Research, pp. 4114-4124, Long Beach, California, USA, 09-15 Jun
2019. PMLR.
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with
deep adaptation networks. In International conference on machine learning, pp. 97-105, 2015.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
12
Under review as a conference paper at ICLR 2021
DavidLoPez-Paz, Robert Nishihara, SoUmith Chintala, Bernhard Scholkopf, and Leon Bottou. Dis-
covering causal signals in images. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 6979-6987, 2017.
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling.
Causal effect inference with deep latent-variable models. In Advances in Neural Information
Processing Systems, pp. 6446-6456, 2017.
Sara Magliacane, Thijs van Ommen, Tom Claassen, Stephan Bongers, Philip Versteeg, and Joris M
Mooij. Domain adaptation by using causal inference to predict invariant conditional distributions.
In Advances in Neural Information Processing Systems, pp. 10846-10856, 2018.
Jon D Mcauliffe and David M Blei. Supervised topic models. In Advances in Neural Information
Processing Systems, pp. 121-128, Vancouver, Canada, 2008. NIPS Foundation.
Krikamol Muandet, David Balduzzi, and Bernhard Schoikopf. Domain generalization via invariant
feature representation. In International Conference on Machine Learning, pp. 10-18, 2013.
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
Radford M Neal. Bayesian learning for neural networks. PhD thesis, University of Toronto, 1995.
Sinno Jialin Pan, Ivor W Tsang, James T Kwok, and Qiang Yang. Domain adaptation via transfer
component analysis. IEEE Transactions on Neural Networks, 22(2):199-210, 2010.
Judea Pearl. Causality. Cambridge university press, 2009.
Jonas Peters, Joris M Mooij, Dominik Janzing, and Bernhard Scholkopf. Causal discovery with
continuous additive noise models. Journal of Machine Learning Research, 15(1):2009-2053,
2014.
Jonas Peters, Peter Buhlmann, and Nicolai Meinshausen. Causal inference by using invariant pre-
diction: identification and confidence intervals. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 78(5):947-1012, 2016.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements ofcausal inference: foundations
and learning algorithms. MIT press, 2017.
Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12556-
12565, 2020.
Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.
Thomas Richardson, Peter Spirtes, et al. Ancestral graph Markov models. The Annals of Statistics,
30(4):962-1030, 2002.
Mateo Rojas-Carulla, Bernhard Scholkopf, Richard Turner, and Jonas Peters. Invariant models for
causal transfer learning. The Journal of Machine Learning Research, 19(1):1309-1342, 2018.
Jan-Willem Romeijn and Jon Williamson. Intervention and identifiability in latent variable mod-
elling. Minds and machines, 28(2):243-264, 2018.
Bernhard Scholkopf. Causality for machine learning. arXiv preprint arXiv:1911.10500, 2019.
Bernhard Scholkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun Zhang, and Joris M
Mooij. On causal and anticausal learning. In International Conference on Machine Learning
(ICML 2012), pp. 1255-1262. International Machine Learning Society, 2012.
Eleni Sgouritsa, Dominik Janzing, Jonas Peters, and Bernhard Scholkopf. Identifying finite mixtures
of nonparametric product distributions and causal inference of confounders. In Proceedings of the
29th Conference on Uncertainty in Artificial Intelligence (UAI 2013), pp. 556-575. AUAI Press,
2013.
13
Under review as a conference paper at ICLR 2021
Uri Shalit, Fredrik D Johansson, and David Sontag. Estimating individual treatment effect: general-
ization bounds and algorithms. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70,pp. 3076-3085.JMLR.org, 2017.
Shiv Shankar, Vihari Piratla, Soumen Chakrabarti, Siddhartha Chaudhuri, Preethi Jyothi, and Sunita
Sarawagi. Generalizing across domains via cross-gradient training. In Proceedings of the Inter-
national Conference on Learning Representations (ICLR 2018), 2018.
Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, and Peixuan Chen. Causally regularized learning with
agnostic data selection bias. In 2018 ACM Multimedia Conference on Multimedia Conference,
pp. 411-419. ACM, 2018.
Ilya Shpitser, Robin J Evans, Thomas S Richardson, and James M Robins. Introduction to nested
Markov models. Behaviormetrika, 41(1):3-39, 2014.
Peter Spirtes, Clark N Glymour, Richard Scheines, and David Heckerman. Causation, prediction,
and search. MIT press, 2000.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine
Learning Research, 15(1):1929-1958, 2014.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In Proceedings of the International
Conference on Learning Representations (ICLR 2014), 2014.
Takeshi Teshima, Issei Sato, and Masashi Sugiyama. Few-shot domain adaptation by causal mech-
anism transfer. arXiv preprint arXiv:2002.03497, 2020.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-RMSprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Thomas Verma and Judea Pearl. Equivalence and synthesis of causal models. UCLA, Computer
Science Department, 1991.
Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and variational
inference. Foundations and TrendsR in Machine Learning, 1(1-2):1-305, 2008.
Yixin Wang and David M Blei. The blessings of multiple causes. Journal of the American Statistical
Association, 114(528):1574-1596, 2019.
Kilian Q Weinberger and Lawrence K Saul. Unsupervised learning of image manifolds by semidef-
inite programming. International Journal of Computer Vision, 70(1):77-90, 2006.
Yaniv Yacoby, Weiwei Pan, and Finale Doshi-Velez. Learning deep bayesian latent variable
regression models that generalize: When non-identifiability is a problem. arXiv preprint
arXiv:1911.00569, 2019.
Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. Representation learn-
ing for treatment effect estimation from observational data. In Advances in Neural Information
Processing Systems, pp. 2633-2643, 2018.
Kaichao You, Ximei Wang, Mingsheng Long, and Michael Jordan. Towards accurate model selec-
tion in deep unsupervised domain adaptation. In International Conference on Machine Learning,
pp. 7124-7133, 2019.
Cheng Zhang, Kun Zhang, and Yingzhen Li. A causal view on robustness of neural networks. In
Advances in Neural Information Processing Systems, 2020.
KUn Zhang and Aapo Hyvarinen. On the identifiability of the post-nonlinear causal model. In
Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence (UAI 2009), pp.
647-655. AUAI Press, 2009.
14
Under review as a conference paper at ICLR 2021
KUn Zhang, Bernhard SchOlkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning, pp. 819-827,
2013.
Han Zhao, Remi Tachet Des Combes, Kun Zhang, and Geoffrey Gordon. On learning invariant
representations for domain adaptation. In International Conference on Machine Learning, pp.
7523-7532, 2019.
15
Under review as a conference paper at ICLR 2021
Supplementary Materials
A Proofs
We first introduce some handy concepts and results to make the proof succinct. We begin with
extended discussions on CSG.
Definition A.1. A homeomorphism Φ on S × V is called a reparameterization from CSG p to
CSGp0, if ①#^^] = pS,v, and p(x∣s,v) = p0(x∣Φ(s,v)) and p(y∣s) = p0(y∣ΦS(s,v)) for any
(s, v) ∈ S × V. A reparameterization Φ is called to be semantic-preserving, if its output dimensions
in S is constant ofv: ΦS (s, v) = ΦS (s) for any v ∈ V.
Note that a reparameterization unnecessarily has its output dimensions in S, i.e. ΦS (s, v), constant
of v. The condition that p(y∣s) = p0(y∣ΦS(s,v)) for any V ∈ V does not indicate that Φs(s,v)
is constant of v, since p0(y|s0) may ignore the change of s0 = ΦS (s, v) from the change of v.
The following lemma shows the meaning of a reparameterization: it allows a CSG to vary while
inducing the same distribution on the observed data variables (x, y) (i.e., holding the same effect on
describing data).
Lemma A.2. If there exists a reparameterization Φ from CSGp to CSG p0, then p(x, y) = p0(x, y).
Proof. By the definition ofa reparameterization, we have:
p(x, y) =	p(s, v)p(x|s, v)p(y|s) dsdv
/ ①#1 [pS,v](s, v)p0(x∣Φ(s, v))p0(y∣ΦS(s, V)) dsdv
p0s,v(s0, v0)p0(x|s0, v0)p0(y|s0) ds0dv0 =p0(x,y),
where we used variable substitution (s0, V0) := Φ(s, V) in the second-last equality. Note that by
the definition of PUshed-forward distribution and the bijectivity of Φ, Φ#[ps,v] = Psv implies
Ps,v =虫#1底加],and R f (s0,v0)ps,v(s0, v0) ds0dv0 = R f&(s,v)^#1 [ps,v](s, v) dsdv (can also
be verified deductively using the rule of change of variables, i.e. Lemma A.4 in the following). □
The definition of semantic-equivalence (Definition 5.3) can be rephrased by the existence of a
semantic-preserving reparameterization. With appropriate model assumptions, we can show that any
reparameterization between two CSGs is semantic-preserving, so that semantic-preserving CSGs
cannot be converted to each other by a reparameterization that mixes s with v .
Lemma A.3. For two CSGs p and p0, if p0(y|s) has a statistics M0(s) that is an injective function
of s, then any reparameterization Φ from p to p0, if exists, has its ΦS constant of v.
Proof. Let Φ = (ΦS, ΦV) be any reparameterization from p top0. Then the condition that p(y|s) =
p0(y∣ΦS(s,v)) for any V ∈ V indicates that M(S) = M0(ΦS(s,v)). If there exist S ∈ S and
v(1) 6= v(2) ∈ V such that ΦS (s, v(1)) 6= ΦS (s, v(2)), then M0(ΦS(s, v(1) )) 6= M0(ΦS(s, v(2) ))
since M0 is injective. This violates M(s) = M0(ΦS(s, v)) which requires both M0(ΦS(s, v(1) ))
and M0(Φs(s, v(2))) to be equal to M(s). So Φs(s, v) must be constant of v.	□
We then introduce two mathematical facts.
Lemma A.4 (rule of change of variables). Let z be a random variable on a Euclidean space RdZ
with density function pz(z), and let Φ be a homeomorphism on RdZ whose inverse Φ-1 is differen-
tiable. Then the distribution of the transformed random variable z0 = Φ(z) has a density function
①#^]^0) = Pz (Φ-1(z0))∣ Jφ-ι (z0)∣, where | Jφ-ι (z0)∣ denotes the absolute value of the determi-
nant of the Jacobian matrix (Jφ-ι (z0))ia := ∂zo (Φ-1 )a(z0) of Φ-1 at Z.
zi
Proof. See e.g., Billingsley (2012, Theorem 17.2). Note that a homeomorphism is (Borel) mea-
surable since it is continuous (Billingsley, 2012, Theorem 13.2), so the definition of ①#]。2] is
valid.	□
16
Under review as a conference paper at ICLR 2021
Lemma A.5. Let μ be a random variable whose characteristic function is a.e. non-zero. For two
functions f and f 0 on the same space, we have: f * pμ = f 0 * pμ ^⇒ f = f 0 a.e., where
(f * Pμ)(x) := f f (x)pμ(x — μ) dμ denotes convolution.
Proof. The function equality f * pμ = f 0 * pμ leads to the equality under Fourier transformation
F[f *pμ] = F[f 0 *pμ], which gives F[f]F[pμ] = F[f 0]F[pμ]. Since F[pμ] is the characteristic
function of pμ, the condition that it is a.e. non-zero indicates that F [f ] = F [f 0] a.e. thus f = f 0
a.e. See also Khemakhem et al. (2019, Theorem 1).	□
A. 1 Proof of the Equivalence Relation
Proposition A.6. The semantic-equivalence defined in Definition 5.3 is an equivalence relation if V
is connected and is either open or closed in RdV.
Proof. Let Φ be a semantic-preserving reparameterization from one CSG p =
(p(s, v),p(x|s, v), p(y|s)) to another p0 = (p0(s, v),p0(x|s, v),p0(y|s)). It has its ΦS con-
stant of v, so we can write Φ(s, v) = (ΦS (s), ΦV (s, v)) =: (φ(s), ψs (v)).
(1)	We first show that φ, and ψs for any s ∈ S, are homeomorphisms on S and V, respectively, and
that Φ-1(s0, v0) = (φ-1(s0), ψφ--11(s0)(v0)).
•	Since Φ(S × V) = S × V, so φ(S) = ΦS(S) = S, so φ is surjective.
•	Suppose that there exists s0 ∈ S such that φ-1 (s0) = {s(i)}i∈I contains multiple distinct
elements.
1.
2.
3.
Since Φ is surjective, for any v0 ∈ V, there exist i ∈ I and v ∈ V such that (s0, v0) =
Φ(s(i), v) = (φ(s(i)), ψs(i) (v)), which means that Si∈I ψs(i) (V) = V.
Since Φ is injective, the sets {ψs(i) (V)}i∈I must be mutually disjoint. Otherwise,
there would exist i 6= j ∈ I and v(1), v(2) ∈ V such that ψs(i) (v(1)) = ψs(j) (v(2)) thus
Φ(s(i), v(1)) = (s0, ψs(i) (v(1))) = (s0, ψs(j) (v (2))) = Φ(s(j), v(2)), which violates
the injectivity of Φ since s(i) 6= s(j) .
In the case where V is open, then so is any ψs(i) (V) = Φ(s(i) , V) since Φ is contin-
uous. But the union of disjoint open sets Si∈I ψs(i) (V) = V cannot be connected.
This violates the condition that V is connected.
A similar argument holds in the case where V is closed.
4.
So φ-1(s0) contains only one unique element for any s0 ∈ S. So φ is injective.
•	The above argument also shows that for any s0 ∈ S, we have Si∈I ψs(i) (V) =
ψφ-1(s0) (V) = V. For any s ∈ S, there exists s0 ∈ S such that s = φ-1 (s0), so we
have ψs(V) = V. So ψs is surjective for any s ∈ S.
•	Suppose that there exist v(1) 6= v(2) ∈ V such that ψs(v(1)) = ψs(v(2)). Then Φ(s, v(1)) =
(φ(s), ψs(v(1))) = (φ(s), ψs(v(2))) = Φ(s, v(2)), which contradicts the injectivity of Φ
since v(1) 6= v(2). So ψs is injective for any s ∈ S.
•	That Φ is continuous and Φ(s, v) = (φ(s), ψs (v)) indicates that φ and ψs are
continuous. For any (s0, v0)	∈ S × V, we have Φ(φ-1 (s0), ψφ--11(s0) (v0)) =
(φ(φ-1 (s0)), ψφ-1(s0) (ψφ--11 (s0) (v0))) = (s0, v0). Applying Φ-1 to both sides gives
Φ-1(s0, v0) = (φ-1(s0), ψφ--11(s0)(v0)).
•	Since Φ-1 is continuous, φ-1 and ψs-1 are also continuous.
(2)	We now show that the relation is an equivalence relation. It amounts to showing the following
three properties.
• Reflexivity. For two identical CSGs, we have p(s, v) = p0(s, v), p(x|s, v) = p0(x|s, v) and
p(y|s) = p0(y|s). So the identity map as Φ obviously satisfies all the requirements.
• Symmetry. Let Φ be a semantic-preserving reparameterization from p =
(p(s, v), p(x|s, v),p(y|s)) to p0 = (p0(s, v),p0(x|s, v), p0(y|s)). From the above conclu-
sion in (1), we know that (Φ-1)S(s0, v0) = φ-1(s0) is semantic-preserving. Also, Φ-1 is
a homeomorphism on S × V since Φ is. So we only need to show that Φ-1 is a reparame-
terization from p0 to p for symmetry.
17
Under review as a conference paper at ICLR 2021
1.	From the definition of PUshed-forward distribution, We have Φ∕1[pS,v] = Ps,v if
Φ#[ps,v] = pS,v. It can also be verified through the rule of change of vari-
ables (Lemma A.4) when Φ and Φ-1 are differentiable. From Φ#[ps,v] = pS,v,
we have for any (s0,v0), Ps,v(Φ-1(s0,v0))∣Jφ-ι(s0,v0)∣ = p0s,v(s0,v0). Since
for any (s, v) there exists (s0, v0) such that (s, v) = Φ-1(s0, v0), this imPlies
that for any (s,v), Ps,v(s,v)∣Jφ-ι(Φ(s,v))∣ = p；,。(Φ(s,v)), or Ps,v(s,v)=
pS,v (Φ(s, v))/| Jφ-i (Φ(s, v))| = pS,v (Φ(s, v))| Jφ(s, v)| (inverse function theorem),
which means that p；,。= Φ∕1[pS,v] by the rule of change of variables.
2.	For any (s0, v0), there exists (s, v) such that (s0, v0) = Φ(s, v), so p0(x|s0, v0) =
p0(x∣Φ(s,v)) = p(x∣s,v) = p(x∣Φ-1 (s0,v0)), and p0(y∣s0) = p0(y∣ΦS(s))=
p(y∣s) = p(y∣(Φ-1)S (s0)).
So Φ-1 is a reParameterization from p0 top.
• Transitivity. Given a third CSG p00 = (p00(s, v), p00(x|s, v), p00(y|s)) that is semantic-
equivalent to p0, there exists a semantic-Preserving reParameterization Φ0 from p0 to p00 .
It is easy to see that (Φ0 ◦ Φ)S (s, v) = Φ0S (ΦS (s, v)) = Φ0S (ΦS (s)) is constant of v thus
semantic-Preserving. As the comPosition of two homeomorPhisms Φ and Φ0 on S × V,
Φ0 ◦ Φ is also a homeomorPhism. So we only need to show that Φ0 ◦ Φ is a reParameteri-
zation from p to p00 for transitivity.
1.	From the definition of pushed-forward distribution, we have (Φ0 ◦①)#^；,。] =
◎#^#[ps,v]] = φ≠[pS,v] = PIv if 虫#必,。]=pS,v and φ≠[pS,v] = PIv. Itcan
also be verified through the rule of change of variables (Lemma A.4) when Φ-1 and
Φ0-1 are differentiable. For any (s00, v00), we have
(Φ0 ◦虫)#必,0](s00,v00)=Ps,v((Φ0 ◦ Φ)T(s00,v00))∣J(φo°Φ)-ι (s00,v00)∣
= Ps,v (Φ-1 (φT(s00,v00)))∣Jφ-ι (φT(s00,v00))∣∣Jφo-ι (s00,v00)∣
=虫#必,。]&0-1 (s00,V00))∣Jφo-1 (s00,v00)∣
=P0s,v (φ0T(s00,v0O))IJΦ0-1 (s00,v0O)I = φ4[pS,v](s00,v0O) =pS,v (s00,v0O).
2.	For any (s, v), we have:
p(xIs, v) = pO(xIΦ(s, v)) = pOO(xIΦO(Φ(s, v))) = pOO(xI(ΦO ◦ Φ)(s, v)),
p(y∣s) = p0(y∣ΦS(S))= p00(y∣Φ0S(Φs(S)))= p00(y∣(Φ0 ◦ Φ)s(s)).
So ΦO ◦ Φ is a reparameterization from p to pOO .
This completes the proof for an equivalence relation.	□
A.2 Proof of the Semantic-Identifiability Theorem 5.4
We present a more general and detailed version of Theorem 5.4 and prove it. The theorem in the
main context corresponds to conclusions (ii) and (i) below by taking the two CSGs pO and p as the
well-learnedP and the ground-truth CSGS p*, respectively.
Theorem 5.4’ (semantic-identifiability). Consider CSGs p and pO that have Assumptions 5.1 and 5.2
hold, with the bounded derivative conditions specified to be that for both CSGs, f-1 andg are twice
and f thrice differentiable with mentioned derivatives bounded. Further assume that their priors
have bounded densities and their log p(S, v) have bounded derivatives up to the second-order. If the
two CSGs have p(x, y) = pO(x, y), then they are semantic-equivalent, under the conditions that: 7
(i) pμ has an a.e. non-zero characteristic function (e.g., a Gaussian distribution);
(ii)	σ2 → ∞, where σj := E[μ>μ];
(iii)	σμ2》Bf-Imax{B0ogPBg + ɪBg + 以Bf-BfBg,BpBf-ι(B0θgp+B0Ogp+3dBf-ιBfBlogp+
3d 3 BfLIBfi2 + d3Bf00Bf-ι)}, where d := ds + dv, and for both CSGs, the ConStant Bp bounds
p(S, v), BfO -1 , BgO , BlOog p and BfOO, BgOO, BlOoO g p bound the 2-norms8 of the gradient/Jacobian and the
Hessians of the respective functions, and BfOOO bounds all the 3rd-order derivatives of f.
7To be precise, the conclusions are that the equalities in Definition 5.3 hold a.e. for condition (i), hold
asymptotically in the limit 表 → ∞ for condition (ii), and hold up to a negligible quantity for condition (iii).
8As an induced operator norm for matrices (not the Frobenius norm).
18
Under review as a conference paper at ICLR 2021
Proof. Without loss of generality, We assume that μ and V (for continuous y) have zero mean.
If it is not, we can redefine f (s,v) := f (s,v) + E[μ] and μ := μ - E[μ] (similarly for V for
continuous y) Which does not alter the joint distribution p(s, v, x, y) nor violates any assumptions.
Also without loss of generality, we consider one scalar component (dimension) l ofy, and abuse the
use of symbols y and g for yl and gl to avoid unnecessary complication. Note that for continuous y,
due to the additive noise structure y = g(s)+V and that V has zero mean, we also have E[y|s] = g(s)
as the same as the categorical y case (under the one-hot representation). We sometimes denote
z := (s, v ) for convenience.
First note that for both CSGs and both continuous and categorical y, by construction g(s) is a
sufficient statistics ofp(y|s) (not only the expectation E[y|s]), and it is injective. So by Lemma A.3,
we only need to show that there exists a reparameterization from p to p0. We will show that Φ :=
f0-1 ◦ f is such a reparameterization.
Since f and f0 are bijective and continuous, we have Φ-1 = f-1 ◦ f0, so Φ is bijective and Φ and
Φ-1 are continuous. So Φ is a homeomorphism. Also, by construction, we have:
p(x∣z) = Pμ(x - f (z)) = Pμ(x - f0(f0-1(f (Z)))) = Pμ(x - f'(Φ(z))) = P0(x∣Φ(z)).	(7)
So we only need to show that p(x,y) = p0(x,y) indicates ①#]。Z] = Pz and p(y∣s) =
p0(y∣ΦS(s, v)), ∀v ∈ V under the conditions.
Proof under condition (i). We begin with a useful reformulation of the integral t(z)p(x|z) dz
for a general function t of z. We will encounter integrals in this form. By Assumption 5.1, we have
p(x∣z) = pμ(x - f (z)), so we consider a transformation Ψχ(z) := X - f (Z) and let μ = Ψχ(z). It
is invertible, Ψ-1(μ) = f T(X - μ), and Jψ-ι (μ) = -Jf-1 (x - μ). By these definitions and the
rule of change of variables, we have:
/ t(z)p(x∣z)dz = / t(z)pμ (Ψχ(z))dz = / t(Ψ-1(μ))p(μ)Jψ-ι (μ) ∣ dμ
=/t(f-1 (X - μ))p(μ)∣Jf-ι(X - μ)∣dμ
=Ep(μ)[(tV)(x - μ)]	(8)
= (f#[t] * Pμ)(X),	⑼
where we have denoted functions t := t ◦ f-1, V := ∣ Jf-11, and abused the push-forward notation
f# [t] for a general function t to formally denote (t ◦ f T)IJf-11 = tV.
According to the graphical structure of CSG, we have:
p(X)
p(Z)p(X|Z) dZ,
E[y|X]
yp(X, y) dy =
yp(Z)p(X|Z)p(y|s) dZdy
p(Z)p(X|Z)E[y|s] dZ
g(s)p(Z)p(X|Z) dZ.
So from Eq. (9), we have:
P(X) = (f#[pz] *Pμ)(X),
E[y|X] = Px)(f#[gPz] *Pμ)3∙
(10)
(11)
(12)
Matching the data distribution P(X, y) = P0(X, y) indicates both P(X) = P0(X) and E[y|X] = E0[y|X].
Using Lemma A.5 under condition (i), this further indicates f# [Pz] = f#0 [P0z] a.e. and f# [gPz] =
f#[g0p0z] a.e. The former gives ①#[pz] = Pz. The latter can be reformed as gf#[pz] = g0f#[pz]
a.e., so g = g0 a.e., where we have denoted g := g ◦ (f T)S and g0 := g0 ◦ (f0-1)S similarly. From
g = g', we have for any V ∈ V,
g(s) = g((f-1 ◦ f )S (s,v)) = g((f-1)S (f (s,v))) = g(f (s,v))
=gg0(f(s,v)) =g0((f0-1)S(f(s,v)))=g0(ΦS(s,v)).	(13)
For both continuous and categorical y, g(s) uniquely determines P(y|s). So the above equality
means thatp(y∣s) = p0(y∣ΦS(s,v)) for any V ∈ V.
19
Under review as a conference paper at ICLR 2021
Proof under condition (ii). Applying Eq. (8) to Eqs. (10, 11), we have:
p(x) = Ep(μ) [(PzV)(x - 〃)],
E[y∣x]
P(X)Ep(μ) [(gpzV)(x - 〃)],
where we have similarly denoted PZ := pz ◦ f-1. Under condition (ii), E[μτμ] is infinitesimal, so
we can expand the expressions w.r.t μ. For p(x), we have:
P(X) = Ep(μ) [pzV — V(PZV)τμ + 1 μτWT(PZV)μ +。(斯]〃晦])]
=pz V + 2 Ep(“)[μTVVT(PZ V )μ] + o(σμ),
where all functions are evaluated at x. For E[y∣χ], we first expand 1∕p(x) using ^ɪɪ = ɪ - 言 +
O(ε2) to get:志=赤-2pry2 Ep(μ) [μTVVT(PZV)μ] +。(端).The second term is expanded
as: gpZV + 1 Ep(μ) [μTVVT (gpZV)μ] + O(σ^). Combining the two parts, we have:
E[y∣x] = g + 2Ep(μ)[μT ((VlogPZV)VgT + Vg(V logPZV)t + VVTg)μ] + O(σ3).	(14)
This equation holds for any X ∈ SUPP(Px) since the expectation is taken w.r.t the distributionp(x, y);
in other words, the considered X here is any value generated by the model. So up to O(σ2),
IP(X)-(PZV)(x)l = 11/“)[〃TVVT(PZV)μ]∣ 6 1 %“)□〃TVVT(PZV)μ∣]
6 2EP(")[kμk2IlVVT(PZV)∣∣2kμk2] = 1 E[μτμ]∣∣VVT(PZV)∣∣2
=1 E[μτμ]∣pZV∣∣∣VVτ logPZV + (VlogPZV)(VlogPZV)τ ∣ ∣ 2
6 2E[μτμ]∣PZV∣( ∣ ∣ VVτ logPzV∣∣2 + kVlogPzVk2),	(15)
∣E[y∣x] - g(x)∣ = 1∣Ep(μ) [μτ((VlogPZV)Vgτ + Vg(VlogPZV)τ + VVτg)μ] ∣
6 1 Ep(")[∣μT((V log pz V )Vgτ + Vg(V log PZ V )τ + VVτ g)μ∣]
6 2 EP(")[kμk2∣∣(V log Pz V )VgT + Vg(V log PZ V)T + VVTg∣∣2kμk2]
6 2E[μτμ]( ∣ ∣ (VlogPzV)Vgτ∣∣2 + ∣∣Vg(VlogPZV)τ∣∣2 + ∣∣VVTg∣∣2)
=E[μτμ](∣(V log Pz V )τVg∣ + 2∣∣VVτg∣∣2).	(16)
Given the bounding conditions in the theorem, the multiplicative factors to E[μTμ] in the last ex-
pressions are bounded by a constant. So when 或 → ∞, i.e. E[μτμ] → 0, we have p(x) and
E[y∣x] converge uniformly to (PZV)(x) = f#[pz](x) and g (x), respectively. Sop(x,y) = p0(x,y)
indicates f#[pz] = f#[pZ] and g = g0, which means Φ#[pz] = PZ andp(y∣s) = p0(y∣ΦS(s,v)) for
any v ∈ V, due to Eq. (13) and the explanation that follows.
Proof under condition (iii). We only need to show that when = is much larger than the given
quantity, we still have p(x, y) = p0(x, y) =⇒ PZV = PZV0, g = g 0 up to a negligible effect. This
task amounts to showing that the residuals ∣p(x) - (PZV)(x)∣ and ∣E[y∣x] - g(x)∣ controlled by
Eqs. (15, 16) are negligible. To achieve this, we need to further expand the controlling functions
using derivatives of f, g and PZ explicitly, and bound them by the bounding constants. In the
following, we use indices a, b, C for the components of X and i, j, k for those of z. For functions of
Z appearing in the following (e.g., f, g, PZ and their derivatives), they are evaluated at Z = f-1(x)
since we are bounding functions of x.
⑴ Bounding ∣E[y∣x] - g(x)∣ 6 E[μτμ](∣(V log PZ V )τVg∣ + 21∣ VVTg∣∣2) from Eq. (16).
From the chain rule of differentiation, it is easy to show that:
V logPz = Jf-IV logPz,	Vg = J(f-i)s Vg = Jf-I Vzg,	(17)
20
Under review as a conference paper at ICLR 2021
where Nzg = (Vgτ, 0> )> (recall that g is a function only of s). For the term V log V, we apply
Jacobi,s formula for the derivative of the log-determinant:
∂a log V(x) = ∂a lθg∣ Jf-1 (X)I= tr
EJf (f-1(x))ib ∂b∂afL(X) = E (Jf(VVT f”必
b,i	i
(18)
However, as bounding Eq. (17) already requires bounding IlJf-ι ∣∣2, directly using this expression
to bound ∣∣V log V∣∣2 would require to also bound ∣∣ Jf ∣∣2. This requirement to bound the first-order
derivatives of both f and f-1 is a relatively restrictive one. To ease the requirement, we would like
to express V log V in terms of Jf-1. This can be achieved by expressing VVτ f-1's in terms of
VVτfc's. To do this, first consider a general invertible-matrix-valued function A(α) on a scalar α.
We have 0 = ∂α (A(α)-1A(α)) = (∂0A-1)A + ATdaA, so we have ATdaA = -(∂0A-1)A,
consequently ∂αA = -A(∂αA-1 )A. Using this relation (in the fourth equality below), we have:
(VVTfiT)αb = dadbf-1 = da(Jf-ι )庆=(daJf-i )友
=-(Jf-I (daJf-i )Jf-I )bi = -(Jf-I (daJf IJfT )同
=-X(JfT)bj (da(∂j fc))(Jf-1 )ci = - X(Jf-Cbj (∂k ∂j fc)(∂af-)(Jf-ι)ci
-E(Jf-1 )ci E(Jf-1 )bj Idk djfC)(JfT )ak = - E(Jf-1 )Ci(Jf-I (VVTfC)JT-1 )ab,
c	jk	c
or in matrix form,
VVτf-1 = - X(Jf-1 )ciJf-1 (VVτfc)Jf-1 =: - X(Jf-1 )ciKc,	(19)
CC
where we have defined the matrix KC := Jf-1 (VVTfC)JfLI which is symmetric. Substituting
with this result, we can transform Eq. (18) into a desired form:
V log V (x)= X(Jf(WTfiT))T = - X(Jf X(Jf-1 )CiJf-1 (WTfC)JT-I)T
i	i	c	2:
-X(X(Jf-1 )ciJf J-I(VVTfC) JT-I)T = - X(Jf-1 )ci ((VVTfC)Jf-I)T
i C	Ci
-X (Jf-1 (VVTfC)Jf-1 )： = - X(Kc)T = - X KCC,
C	C	C
so its norm can be bounded by:
∣V log V (x)∣2 = ∣∣ X KC I I 2 =∣ I X(Jf-1) 0(VVTfC)Jf:11 I 2
C	C
6 EII(Jf-I)αII2IIVVTfCII2IIJfTII2 6Bf0Bf-1∑II(Jf-1 )cII2
C	C
6 dBf2-1 Bf0,
where we have used the following result in the last inequality:
X II (Jf-1 )c：II2 6 d1/2,∕XI(Jf-1 )c：I2
CC
d1/2IIJf-1IIF 6 d∣∣Jf-1II2 6 dBf-1.
Integrating Eq. (17) and Eq. (21), we have:
I (V log Pz V )τVg∣ = (Jf-1V log Pz + V log V )τJf-1 Vz g
6 ( I I Jf-1II2kV log pz∣2 + ∣V log V ∣∣2)IIJf-1IIkVg∣∣2
6 (Bf-1 BlOg P + dBf2-1 Bf0) Bf-1 Bg
=(Blog P + dBf-1 Bf0 )Bf2-1 Bg.
(20)
(21)
(22)
(23)
21
Under review as a conference paper at ICLR 2021
For the Hessian of g, direct calculus gives:
ds
VVTg = J(f-1)S(WT g)jτ-1)S +E(Vg)si(VV>f-1)
i=1
=Jf-1 (Vz VTg)JT-1 + X(Vz g)i(VVTfT).
i
To avoid the requirement ofbounding both VVTfC’s and VVt∕-1,s, we substitute VVTf-I using
Eq.(19):
VVTg = Jf-1 (VzVTg)Jf-ι - X(Vzg)i X(Jf-I)ciKC
iC
=Jf-1 (VzVTg)JT-I- X ((Jf-I)c,:(Vzg))κc.
C
So its norm can be bounded by:
UVVTgU2 6 Jf-1 俏 UVVTgU2 + Xl(Jf-I )c:(Vz g)∣kK ck2
C
6 BfI Bg + Xl (Jf-1 )c：(Vz g)∣Bf2-1 Bf
C
6 Bf-I (Bg +Bf XI I (Jf-1 )cj∣2 kVzgk2)
C
6 Bf-I (Bg +BfBg X ∣ ∣ (Jf-1 )c：|。
C
6 Bf2-1 (Bg+ dBf-I BfBg),	(24)
where we have used Eq. (22) in the last inequality. Assembling Eq. (23) and Eq. (24) into Eq. (16),
we have:
1	3
|E[y|x] - g(x)∖ 6 E[μTμ]Bf2-1 (BleIgPBg + ^Bg + 2dBf-BfBg).	(25)
So given the condition (iii), this residual can be neglected.
(2) Bounding Ip(X)-(PzV)(x)∣	6	2E[〃T〃]|pzV∣(∣∣VlogpzV∣∣2 + ∣∣VVt logPzU2 +
Il VVt log VU2) from Eq. (15).
To begin with, for any x, Pz(x) = pz(f-1(x)) 6 Bp, and V(x) = ∣ Jf-1 (x)∣ is the product of
absolute eigenvalues of Jf-1 (x). Since11 Jf-1 (x”1 2 is the largest absolute eigenvalue of Jf-1 (x),
so V(x) 6 UJfT(X)Ud 6 Bf-1.
For the first norm in the bracket of the r.h.s of Eq. (15), we have:
kV log Pz V k2 = kV log Pzk2 +2(V log Pz )τV log V + ∣∣V log V∣∣2
6 IIv log Pzk2 +2kV log Pzk2kV log V l∣2 + IIv log V k2
6 Bf2-1 Blegp + 2dBf3-1 BfBegp + kVlogV∣2,	(26)
where we have utilized Eq. (17) and Eq. (21) in the last inequality. We consider bounding
∣∣Vlog V∣∣2 separately. Using Eq. (20) (in the second equality below), we have:
∣V log Vk 2 = ∣(V log V )τ(V log V )∣ = ∣ XK)T X Kdd∖
Cd
=∣Xκcκdd∣ 6 X∣KCK%∣
Cd	Cd
=X∣(Jf-1 )c： (VVTfC)Jf-1 Jf-1 (VVτfd)(Jf-1 )T
Cd
22
Under review as a conference paper at ICLR 2021
6 Xl(Jf τ)c:(JfT )>:∣∣∣(VVτ∕c)J>-ι Jf-i (Wτ∕d)∣∣^
cd
6 X ∣ (Jf-1 )c：(Jf-1 )T ∣ Bf2-1 Bf = Bf2-IBf X ∣ (Jf-1JT-I)cd ∣
cd	cd
6 d3/2Bf2-1 Bf'2∣∣ Jf-I jT-1∣∣2 6 d3/2Bf4-1B片
where we have used the facts for general matrix A and (column) vectors α, β that
∣。丁Aβ∣ = IIa(Aβ)τ∣∣2 = ∣∣αβTAT∣∣2 6 ∣∣αβτ∣∣2kAk2 = ∣αTβ∣kAk2
in the fifth last inequality, and that
EIACd| 6 √d2
cd
X
cd
IAcd I2
d∣∣A∣∣F 6 d37∣A∣∣2
in the second last inequality. Substituting Eq.(27) into Eq.(26), we have:
kVlogPzVk2 6 Bf2-1 BlogP + 2dBf11 BfBlogP + d3/2Bf-1 Bf2.
For the second norm in the bracket of the r.h.s of Eq. (15), similar to Eq. (24), we have:
∣∣VVτlogPz∣∣2 6 Bf2-1 (Blogp + dBf-1 Bf'B∣θgPX
(27)
(28)
(29)
(30)
(31)
The third norm ∣∣ VVτ log V ∣∣2 in the bracket of the r.h.s of Eq. (15) needs some more effort. From
Eq. (20), we have ∂b log V = - PCij (Jf-1 )ci(∂i∂j fc)(Jf-1 )j,thus
∂a∂b log V = - X ∂a(Jf-1 )ci(∂i∂j fc)(Jf-1 )bj - X(Jf-1 )ci(∂i∂j fc)∂a(Jf-1 )bj
cij	cij
-	X(Jf-1 )ci∂a(∂i∂j fc)(Jf-1 )bj
cij
=-X(∂a∂cf∏(∂i∂j fc)(Jf-1 )bj - X(Jf-1 )ci(∂i∂j fc )(∂α∂bf-1)
cij	cij
-	X(Jf-1)ci(∂af-1)(∂k ∂i ∂j fc)(Jf-1 )bj
=X(Jf-1)di KdC(didj fc)(j-1 )bj + X(Jf T)Ci(didj fc)(JfT)dj Kab
-	〉:(Jf-I) ci (dk didj fc )(Jf-I)ak (Jf-I) bj
=X KaCKdb+X KCdKab- X(jf-1 )ci(∂fc ∂i∂j fc)(jf-1 嬴(Jf-1 )bj,
where we have used Eq. (19) in the third equality for the first two terms. In matrix form, we have:
VVτ log V = X Y KC： + X KCdKd - X(jf-1 )ci(∂fc ∂i∂j fc)(Jf-1 )：k (Jf-1 )Τ.
cd	cd	cij k
We now bound the norms of the three terms in turn. For the first term,
∣∣XKdCKdl2 6 X∣KkC∣∣2 = XIKdKI
cd	cd	cd
=X∣(Jf-1 )d：(VVTfc )Jf-1 Jf-1 (VVτfd)(Jf-1 )Τ∣
cd
6 X∣(Jf-1 )d:(jf-1 )Τ∣∣∣(VVτfc)jΤ-1 Jf-1 (VVτfd)∣∣2
cd
6 Bf2-1 Bf咆 X∣(Jf-1 jΤ-1 )dc∣ 6 d3∕2Bf2-1 Bf2∣Jf-1 jf-1∣∣2
cd
23
Under review as a conference paper at ICLR 2021
6 d3/2Bf04-1Bf002,	(32)
where we have used Eq. (28) in the fourth last inequality and Eq. (29) in the second last inequality.
For the second term,
XKccdKd26X|Kccd|Kd26Bf02-1Bf00X|Kccd|
cd	cd	cd
6 MBf-Bf0XsXIKcdI2 = d1/2Bf2-iBf0XkKCJ∣2
cd	c
6 d1∕2Bf2-ι Bf0XU(Jf-ι )cl UlW>fc)J"l26 d1∕2Bf3-ι Bf XU(Jf-I )cd∣2
6 d3/2Bf04-1Bf002,	(33)
where we have used Eq. (22) in the last inequality. For the third term,
|| X(Jf T )ci(dk didj fc)(JfT ):k (JfT
cijk
6 XI(Jf-I)i(dkdi∂fc)∖l∣(Jf-1 ):k(Jf-I)>l∣2 6 Bf XI(Jf-I)ci∣Xl∣(Jf-ι):k(Jf-I)>l∣2
cijk	ci	jk
6 d3/2Bf〃llJf-1∣∣2 X∖(Jf-1 )> (Jf-I )j∖6 d3/2Bf00Bf-1 X∖(J>-1 Jf-I )kj∖
jk	jk
6 d3Bf000Bf0-1 ∣∣∣Jf>-1Jf-1 ∣∣∣ 6 d3Bf000Bf03-1,	(34)
where we have used Eq. (29) in the fourth last and second last inequalities.
Finally, by assembling Eqs. (30, 31, 32, 33, 34) into Eq. (15), we have:
Ip(x) - (PzV)(x)1 6 1 E[μ>μ]BpBf-ι (Bf-I BlogP + 2dBf3-ι Bf&P + d3/2Bf-I Bf
+Bf02-1(Bl0o0gp+dBf0-1Bf00Bl0ogp)+2d3/2Bf04-1Bf002+d3Bf000Bf03-1
=1 E[μ>μ]BpBfd+2(Blog p + Blog p + 3dBf-ι Bf0Blog p
+3d3/2Bf02-1Bf002 + d3Bf000Bf0-1.
So given the condition (iii), this residual can be neglected.	□
A.3 Proof of the OOD Generalization Error Bound Theorem 5.5
We give the following more detailed version of Theorem 5.5 and prove it. The theorem in the
main context corresponds to conclusion (ii) below (i.e., Eq. (37) below recovers Eq. (6)) by taking
the CSGs p0, p and P as the semantic-identified CSG P on the training domain and the ground-
truth CSGs on the training p* and test p* domains, respectively. Here, the semantic-identification
requirement on the learned CSG P is to guarantee that it is semantic-equivalent to the ground-truth
CSG p* on the training domain, so that the condition in (ii) is satisfied.
Theorem 5.5’ (OOD generalization error). Let Assumptions 5.1 and 5.2 hold. (i) Consider two
CSGs p and P that share the same generative mechanisms p(x∣s, V) and p(y∣s) but have different
priors p§,v and p§,v. Then up to O(σμ) where σ∖ := E[μ>μ], we have for any X ∈ SuPP(Px) ∩
SUpp(Px),
∖E[ylx] — E[ylx]∖ 6 σμkVgbUJfTU2∣Mlog(Ps,v/ps,v)∣∣2∖(s°)=∕-1(x),	(35)
where Jf-1 is the Jacobian off-1. Further assume that the bounds B’s defined in Theorem 5.4’(iii)
hold. Then the error is negligible forany X ∈ supp(px) ∩ SUpp(Px) if 生》BlOgPBgBf-I, and:
Eρ(x)∖E[y∣x] - E[y∣x]∖ 6 σ%Bg2Bf-1 Eps,v [2∆logPs,v - ∆logPs,v + ∣∣VlogPs,v∣∣2]	(36)
if supp(px) = SUpp(Px), where ∆ denotes the Laplacian operator
24
Under review as a conference paper at ICLR 2021
(ii) Let p0 be a CSG that is semantic-equivalent to the CSG P introduced in (i). Then up to O(a\),
we havefor any x ∈ SUPP(Px) ∩ SUPP(Px),
∣E,[y∣x] — E[y∣χ]∣ 6σlWn2Jf0-ι∣∣2∣lvlog(Ps,v∕pS,v升(。尸尸十xy	(37)
where p's,v := Φ#[p)s,v ] is the prior of CSG P under the parameterization of CSG p0, derived as the
pushed-forward distribution by the reparameterization Φ := f0-1 ◦ f from P to p0.
For conclusion (i), in the expected OOD generalization error in Eq. (36), the term Ep [2∆ log PSV -
s,v	,
∆logps,v + ∣∣V logps,v Il 2] is actually the score matching objective (Fisher divergence) (Hyvarinen,
2005) that measures the difference between p§,v and p§,v. For Gaussian priors p(s, v) = N(0, Σ)
andp(s, v) = N(0, Σ), the term reduces to the matrix trace, tr(-2Σ-1 + Σ 1 + Σ-1ΣΣ-1). For
Σ = Σ, the term vanishes.
For conclusion (ii), note that since P and p0 are semantic-equivalent, we have px = px and E0 [y∣x]=
E[y∣x] (from Lemma A.2). So Eqs. (35, 37) bound the same quantity. Equation (37) expresses the
bound using the structures of the CSG p0. It is considered since recovering the exact CSG P from
(χ, y) data is impractical and we can only learn a CSG p0 that is semantic-equivalent to p.
Proof. Following the proof A.2 of Theorem 5.4’，we assume the additive noise variables μ and V
(for continuous y) have zero mean without loss of generality, and we denote Z := (s, v).
Proof under condition (i). Under the assumptions, we have Eq. (14) in the proof A.2 of Theo-
rem 5.4’ hold. Noting that the two CSGs share the same g and V (since they share the samep(χ∣s, v)
andp(y∣s) thus f and g), we have for any X ∈ supp(px) ∩ SUPP(Px),
E[y∣x] = g + 2Ep(μ) [μτ ((V logPzV)Vg> + Vg(V logPzV)τ + VVτg)μ] + O(σ3),
E[y∣x] = g+； Ep(“)[〃T ((V log Pz V )Vgτ + Vg(V log P V )τ + VVTg) 〃] + O(σj3), (38)
where we have similarly defined PZ := PZ ◦ f-1. By subtracting the two equations, we have that up
to o(σ;1),
IE[y∣x] - E[y∣x]∣ = 1∣Ep(“)[mt(vlog(pz/pz)VgT + VgVlog(pz/pz)τ)μ]∣
6 1 Ep(“)□〃>" log(Pz/PZ )VgT + VgV log(Pz/PZ )τ)μ∣]
61 Ep(“)[I〃I2(||V log(Pz阮)v5t∣∣2 + ∣∣VgV log(Pz/PZ )τ∣∣2)]
=∣ VgτV log(PZ/pZ) ∣ E[μτμ].	(39)
The multiplicative factor to E[μτμ] on the right hand side can be further bounded by:
IVgTV IOg(PZ/PZ )∣ = ∣J(f-i)S Vg)T(JfT V log(PZ/PZ )) ∣
=∣ VgTJf-1)S Jf-1V log(pZ/pZ) ∣
=∣((Vg)τ, 0>v )J>-1 Jf-1V log(pZ /75z ) ∣
6 ∣Vg∣2∣∣Jf-1∣∣2∣Vb虱pz/Pz)∣2,	(40)
where Vg and V log(pz/Pz) are evaluated at Z = f-1 (x). This gives:
IE[y∣x] - E[y∣x] ∣ 6 σ%INgbU JfTU2∣Vlog(Pz/pz)∣2,
i.e. Eq. (35) in conclusion (i). When the bounds B’s in Theorem 5.4,(iii) hold, we further have:
∣E[y∣x] - E[y∣x] ∣ 6 σ%INgkU JfT∣∣2∣VlogPZ- VlogPZl∣2
6 σμ INgkU JfT ∣2(IV log Pz∣2+ I∣v log pz ∣2)
25
Under review as a conference paper at ICLR 2021
6 2。： Bg Bf2-ι Blog P.
So when =》BlogPBgBf-i, this difference is negligible for any X ∈ SuPP(Px) ∩ SuPP(pχ).
We now turn to the expected OOD generalization error Eq. (36) in conclusion (i). When supp(px) =
Supp(Px), Eq.(35) hold on px. Together with the bounds in Theorem 5.4'(iii), we have:
Ep(x)|E[y|x] — E[y|x]| 6 σμBg2Bf4-IEp(x)∣∣VIOg(Pz∕pz)∣z=f-ι(x)∣∣2
=σμ Bg2Bf4-I EpzkV log(pz∕Pz )k2,
where the equality holds due to the generating process of the model. Note that the term
EpzllVlog(pz∕pz)∣∣2 therein is the score matching objective (Fisher divergence). By Hyvarinen
(2005, Theorem 1), We can reformulate it as Epz [2∆logPz 一 ∆logPz + ∣∣V logpz∣2], so we have:
Ep(x)∣E[y∣x] - E[y∣x]∣ 6 σμB：Bf-IEpz[2∆logPz - ∆logPz + ∣∣VlogPz∣2].
Proof under condition (ii). From Eq. (14) in the proof A.2 of Theorem 5.4’, we have for CSG Pl
that for any x ∈ SuPP(Plx) or equivalently x ∈ SuPP(Px),
E0[y∣x] = g0 + 1 Ep(μ)[μ>((VlogPzV0)Vg0> + Vg0(VlogPzV0)> + VV>g0)μ] + O(σj), (41)
where We have similarly defined Pz	:= Pz ◦	f0-1	and	g'	:=	g0	◦	(f0-1)S.	Since P and P are
semantic-equivalent with reparameterization Φ from P to P, we have P(y|s) = P0(y∣ΦS(s,v))
thus g(s) = gl(ΦS(s, v)) for any v ∈ V. So for any x ∈ SuPP(Px) or equivalently x ∈
SuPP(Plx), we have g((f-1)S(x)) = gl(ΦS((f-1)S(x), (f -1)V (x))) = gl(ΦS (f -1(x))) =
gz((fz-1)S(f(fT(x)))) = g0((f 0-1)S(x)), i.e., g = g0. For another fact, since Pz :=①#[Pz]=
(f0-1 ◦ f)#[Pz] by definition, we have f# [Pz] = f#[Pz], i.e., Pz VZ = Pz V. Subtracting Eqs. (41, 38)
and applying these two facts, we have UP to O(σμ), for any X ∈ SuPP(Px) ∩ SuPP(Px),
∣Eζ[y∣χ] - E[y∣χ]∣ = 2|Ep(“)[〃>(V log(Pz /Pz )VgZ> + VgZV log(Pz /Pz )>)μ]∣
6 ∣Vgζ>Vlog(∕z/Pz)∣E[μ>μ],
where the inequality follows Eq. (39). Using a similar result of Eq. (40), we have:
∣Eζ[y∣χ] - E[y∣χ]∣ 6kVgz∣2∣∣Jf,-i∣∣2∣∣vlog(Pz/Pz儿，
where VgZ and V log(Pz/Pz) are evaluated at Z = f z-1(x). This gives Eq. (37).	□
A.4 Proof of the Domain Adaptation Error Theorem 5.6
To be consistent with the notation in the proofs, we prove the theorem by denoting the semantic-
identified CSG P and the ground-truth CSG P* on the test domain as PZ and j?, respectively.
Proof. The new prior PZ(Z) is learned by fitting unsupervised data from the test domain P(x). AP-
plying the deduction in the proof A.2 of Theorem 5.4’ to the test domain, we have that under any
of the three conditions in Theorem 5.4’, P?(X) = P?Z(X) indicates f# [P?z] = f#Z [P?Zz]. This gives
Pz = (f'T ◦ f)#[Pz]=虫#忻以.
From Eq. (12) in the same proof, we have that:
,	.~	.	.	,	.	r	,	,	,	,	.	r、	,,
P(X)E[y|x] = (f#[gPz] * Pμ)(X) = ((f⅛[Pz]5) * Pμ)(x),
Z
P (X)E [y|x] = (f# [gPz] * Pμ)(X) = ((f# [Pz]g ) * Pμ)(x).
From the proof A.3 of Theorem 5.5’(ii) (the paragraph under Eq. (41)), the semantic-equivalence be-
tween CSGs P andpz indicates that g = gZ. So from the above two equations, we havep(x)E[j∣x]=
Z
P?Z(X)E? [y|X] (recall that P?(X) = P?Z(X) indicates f# [P?z] = f#Z [P?Zz]). Since P?(X) = P?Z(X) (that is how
26
Under review as a conference paper at ICLR 2021
Pz is learned), We have for any X ∈ SuPP(Px) or equivalently X ∈ SuPP(PX),
0
E [y|x] = E[y∣χ].	(42)
□
B Alternative Identifiability Theory for CSG
The presented identifiability theory, particularly Theorem 5.4, shoWs that the semantic-identifiability
can be achieved in the deterministic limit (3→ ∞), but does not quantitatively describe the extent
μ
of violation of the identifiability for a finite variance σ∖. Here we define a “soft” version of semantic-
equivalence and shoW that it can be achieved With a finite variance, With a trade-off betWeen the
“softness” and the variance.
Definition B.1 (δ-semantic-dependency). For δ > 0 and two CSGs P and P0, we say that they are
δ-semantic-dependent, if there exists a homeomorphism Φ on S × V such that: (i) P(X|s, V) =
p0(χ∣Φ(s, v)), (ii) suPv∈v ∣∣g(s) - g0(ΦS(s, V))II2 6 δ where we have denoted g(s) := E[y∣s], and
(iii) SuPv(1),v(2)∈V ΦS (s, V(1)) - ΦS (s, V(2))2 6 δ.
In the definition, We have released the prior conversion requirement, and relaxed the exact likelihood
conversion forP(y|s) in (ii) and the v-constancy of ΦS in (iii) to alloW an error bounded by δ. When
δ = 0, the v-constancy of ΦS is exact, and under the additive noise Assumption 5.1 We also have
the exact likelihood conversionp(y∣s) = p0(y∣ΦS(s, V)) for any V ∈ V. So 0-semantic-dependency
With the prior conversion requirement reduces to the semantic-equivalence.
Due to the quantitative nature, the binary relation cannot be made an equivalence relation but only
a dependency. Here, a dependency refers to a binary relation With reflexivity and symmetry, but no
transitivity.
Proposition B.2. The δ-semantic-dependency is a dependency relation if the function g := E[y|s]
is bijective and its inverse g-1 is 2 -Lipschitz.
Proof. Showing a dependency relation amounts to showing the following two properties.
• Reflexivity. For two identical CSGs P and P0, we have P(X|s, V) = P0(X|s, V) and P(y|s) =
P0(y|s). So the identity map as Φ obviously satisfies all the requirements in Definition B.1.
• Symmetry. Let CSG P be δ-semantic-dependent to CSG P0 with homeomorphism Φ. Ob-
viously Φ-1 is also a homeomorphism. For any (s0, V0) ∈ S × V, we have P0(X|s0, V0) =
p0(x∣Φ(φT(s0,v0)))	= p(x∣Φ-1(s0,v0)), and ∣∣g0(s0) - g((Φ-1)S (s0 ,v0))∣∣2	=
∣∣g0(ΦS(s, V)) - g(s)∣∣2 6 δ where we have denoted (s, V) := Φ-1(s0, V0) here. So Φ-1
satisfies requirements (i) and (ii) in Definition B.1.
For requirement (iii), we need the following fact: for any s(1) , s(2) ∈ S, ∣s(1) - s(2) ∣2 =
∣∣g-1(g(s⑴))-g-1(g(s⑵))∣∣2 6 1∣∣g(s(1)) - g(s⑵)∣∣2, where the inequality holds
since g-1 is 2 -Lipschitz. Then for any s0 ∈ S, we have:
SuP ∣∣(Φ-1)S(s0, V0(1)) - (Φ-1)S(s0, V0(2))∣∣
v0(1) ,v0(2) ∈V	2
6 SUP	1∣∣g((Φ-1)S (s',。'⑴))-g((Φ-1 )S (s0,v0(2)))∣∣
v0(1) ,v0(2) ∈V 2	2
=SUP	1∣∣(g((Φ-1)s (s'，。'⑴))-g0(s0)) - (g((Φ-1)S (s',。'⑵))-g'(s0))∣L
v0(1) ,v0(2) ∈V 2	2
6 SUP	1 (∣∣g((Φ-ι)s(s',。'⑴))-g'(s')∣l+∣∣g((Φ-ι)s(s',。'⑵))-g'(s')∣∣J
v0(1) ,v0(2) ∈V 2	2	2
=1 ( SUP ∣∣g((φT)S(s',v'(I))) -g'(SO)IL + SUP ∣∣g((φT)S(s',v'(2))) -g'(sO)IL)
2 v0(1) ∈V	2	v0(2) ∈V	2
6 δ,
27
Under review as a conference paper at ICLR 2021
where in the last inequality we have used the fact that Φ-1 satisfies requirement (ii). So p0
is δ-semantic-dependent to p via the homeomorphism Φ-1.
□
The corresponding δ-semantic-identifiability result follows.
Theorem B.3 (δ-semantic-identifiability). Assume the same as Theorem 5.4’ and Proposition B.2,
and let the bounds B’s defined in Theorem 5.4’(iii) hold. For two such CSGs p and p0, if they have
p(x,y) = P0(x,y), then they are δ-semantic-dependent for any δ > σ%Bf-ι(2BlogPBg + Bg +
3dBf0 -1 Bf00 Bg0 , where d := dS + dV.
Proof. Let Φ := f0-1 ◦ f, where f and f0 are given by the two CSGs p and p0 via Assumption 5.1.
We now show that p and p0 are δ-semantic-dependent via this Φ for any δ in the theorem. Obviously
Φ is a homeomorphism on S × V, and it satisfies requirement (i) in Definition B.1 by construction
due to Eq. (7) in the proof A.2 of Theorem 5.4’.
Consider requirement (ii) in Definition B.1. Based on the same assumptions as Theorem 5.4’, we
have Eq. (25) hold for both CSGs:
13
max{kE[y|x] - g(X)k2, kE [y|x] - g (x)k2} 6 σ2Bf-I (BIogPBg + 2Bg + 2dBf-1 Bf Bg),
where We have denoted σ' := E[μ>μ]. Since both CSGs induce the same p(y∣x), so E[y∣x]=
E0[y|x]. This gives:
kg(X)- g0(X)k2 = II(Eiy|x] - g0(X)) - (E[y|x] - g(X))Il2
6 IIE0[y|X] - g0(X)k2 + IIE[y|X] - g(X)k2
6 %Bf2-ι (2BlogPBg + Bg + 3dBf-ιBfBg).
So for any (s, v) ∈ S × V, by denoting X := f(s, v), we have:
IIg(S)-g0(φS(s,v))∣l2 = llg((fT)S(X))-g0((f0T)S(f(s,v)))∣∣2 = ||g(X) - g0(X)k2
6 σBf-1 (2BlogpBg + Bg + 3dBf-1 BfBg).
So the requirement is satisfied.
For requirement (iii), note from the proof of Proposition B.2 that when g is bijective and its inverse
is 2-Lipschitz, requirement (ii) implies requirement (iii). So this Φ is a homeomorphism that makes
P δ-semantic-dependent top0 for any δ > σ∖Bf-ι(2BlogPBg + Bg + 3dBf-Bf Bg).	□
Note that although the δ-semantic-dependency does not have transitivity, the above theo-
rem is still informative: for any two CSGs sharing the same data distribution, particu-
larly for a well-learned CSG P and the ground-truth CSG p*, the likelihood conversion
error sup(s,v)∈S×V Ig(S) - gl(ΦS (S, v))I2, and the degree of mixing v into S, measured
by SUpva),v(2)∈vII铲(s,v⑴)—铲(s,v⑵M2, are bounded by σ2BIf-I(2BlogPBg + Bg +
3dBfl -1BfllBgl ).
C More Explanations on the Model
Explanations on our perspective. We see the data generation process as generating the concep-
tual latent factors (S, v) first, and then generating both X and y based on the factors. This follows
Peters et al. (2017, Section 1.4) who promote the generation of an OCR dataset as the writer first
comes up with an intension to write a character, and then writes down the character and gives its
label based on the intension. It is also natural for medical image datasets, where the label may be
diagnosed based on more fundamental features (e.g., PCR test results showing the pathogen) that
are not included in the dataset but actually cause the medical image. This generation process is also
considered by Mcauliffe & Blei (2008); Kilbertus et al. (2018); Teshima et al. (2020).
On the labeling process from images that one would commonly think of, we also view it as a S → y
process. Human directly knows the critical semantic feature S (e.g., the shape and position of each
28
Under review as a conference paper at ICLR 2021
stroke) by seeing the image, through the nature gift of the vision system (Biederman, 1987). The
label is given by processing the feature (e.g., the angle between two linear strokes, the position of a
circular stroke relative to a linear stroke), which is a s → y process.
The causal graph in Fig. 1 implies that x ⊥ y|s. This does not indicate that the semantic factor
s generates an image x regardless of the label y . Given s, the generated image is dictated to hold
the given semantics regardless of randomness, so the statistical independence does not mean se-
mantic irrelevance. If an image x is given, the corresponding label is given by p(y|x), which is
p(s|x)p(y|s)ds by the causal graph. So the semantic concept to cause the label through p(y|s), is
inferred from the image through p(s|x).
Comparison with the graph ytx → s → x → yrx. This graph is considered by one of our
reviewers, under the perspective of a communication channel, where ytx is a transmitted signal and
yrx is the received.
If the observed label y is treated as ytx , the graph then implies y → s. This is argued at the end of
item (2) in Section 3 that it may make unreasonable implications. Moreover, the graph also implies
that y is a cause of x, as is challenged in item (1) in Section 3. The unnatural implications arise since
intervening y is different from intervening the “ground-truth” label. We consider y as an observation
that may be noisy, while the “ground-truth label” is never observed: one cannot tell if the labels at
hand are noise-corrupted, based on the dataset alone. For example, the label of either image in
Fig. 2 may be given by a labeler’s random guess. Our adopted causal direction s → y is consistent
with these examples and is also argued and adopted by Mcauliffe & Blei (2008); Peters et al. (2017,
Section 1.4); Kilbertus et al. (2018); Teshima et al. (2020).
If the observed label y is treated as yrx, the graph then implies x → y, as is challenged in item (1)
in Section 3. It is also argued by Scholkopf et al. (2012); Peters et al. (2017, Section 1.4); Kilbertus
et al. (2018). Treating the observed label y as yrx and ytx as the “ground-truth” label may be the
motivation of this graph. But the graph implies that ytx ⊥ yrx|x, that is, p(ytx|x, yrx) = p(ytx|x)
andp(yrx|x,ytx) = p(yrx|x). So modeling ytx (resp. yrx) does not benefit predicting yrx (resp. ytx)
from x.
D Relation to Existing Domain Adaptation Theory
Existing DA theory In existing DA literature, the objective is to find a labeling function h : X →
Y within a hypothesis space H that minimizes the target-domain risk R(h) := Ep(χ,y) ['(h(x), y)]
defined with a loss function ' : Y×Y → R. Since P(x, y) is unavailable, it is of practical interest to
consider the source-domain risk R(h) and investigate its relation to R(h). Ben-David et al. (2010a)
give a bound relating the two risks:
R(h) 6 R(h) + 2dι(pχ,Pχ)
--	. .,. , 、 r * , — —	. .,. , 、 r * ,、.一
+ mm{Ep(x)[∣h (X)- h (x)∣],Ep(χ)[∣h (X)- h (x)∣]},	(43)
where: dι(pχ,pχ) := SUp ∣pχ[X] -PxX]|.
X∈X
Here X denotes the σ-algebra on X, dι(px,px) is the total variation between the two distri-
*
butions, and h* ∈ argmιnh∈H R(h) and h ∈ argmιnh∈H R(h) are the oracle/ground-truth
labeling functions on the source and target domains, respectively (e.g., h*(X) = E[y|X] and
*
h (x) = E[y∣x] if supp(px) = SUpp(Px)). Zhao et al. (2019) give a similar bound in the case
of binary classification, in terms of the H-divergence d，H in place of the total variance d∖, where
T-L := {sign(∣h(χ) - h0(x)∣- t) : h,h0 ∈ H,t ∈ [0, 1]}.
Ben-David et al. (2010a) also argue that in this bound, the total variation d1 is overly strict and
hard to estimate, so they develop another bound which is better known (asymptotically; omitting
estimation error from finite samples):
R(h) 6 R(h) + dH∆H(px,Px) + λH,	(44)
where: dH∆H(px,Px)= SUp ∣Eρ(x) ['(h(x),h0(x))] - Ep(x)['(h(x),h0(x))]|,
h,h0∈H
29
Under review as a conference paper at ICLR 2021
λH :
inf
h∈H
∣R(h) + R(h)].
Here "hδh(px,Px) is the H∆H-divergence measuring the difference betweenp(χ) andp(χ) under
the discriminative efficacy of the labeling function family H, and λH is the ideal joint risk achieved
by H. Long et al. (2015) give a similar bound in terms of maximum mean discrepancy (MMD) dK
in place of dH∆H.
一一-	-	--	-	~ *	,
For successful adaptation, DA often makes the Covariate shift assumption: h = h* (or p(y∣x)=
p(y∣x)) on SUpp(Pχ,pχ) := SUpp(Px) ∪ SUpp(pχ).
DA-DIR DA based on learning domain-invariant representations (DA-DIR) (Pan et al., 2010; Bak-
tashmotlagh et al., 2013; Long et al., 2015; Ganin et al., 2016) aims to learn a deterministic represen-
tation extractor η : X → S to some representation space S, in order to achieve a domain-invariant
representation (DIR): P(S) = p(s), where P(S) := n#[px](s) and P(S) := n#[Px](s) are the rep-
resentation distributions on the two domains. The motivation is that, if DIR is achieved, then the
distribution difference term in bound Eq. (43) or Eq. (44) diminishes, thus the bound is hopefully
tighter on the representation space S than on the original data space X, so minimizing the source
risk is more effective to minimizing the target risk. Let g : S → Y be a labeling function on the
representation space. The end-to-end labeling function is effectively h = g◦ η. The typical objective
for DA-DIR thus combines the two desiderata:
R(g ◦ η) + d(n#[Px],n#[Px]),
min
η∈E,g∈G
where d(∙, ∙) is a metric or discrepancy (d(q,P) = 0 ^⇒ q = p) on distributions, and E and G are
the hypothesis spaces for η and g, respectively.
For the existence of the solution of this problem, it is often assumed stronger that there exist η * ∈ E
and g* ∈ G such that n# [Px ] = n# [Px] and R(g* ◦ η*) = R(h*). Assumption 3 of Johansson et al.
(2019) further assumes covariate shift and that g* ◦ n* = h* on SUpp(Px,Px); that is, there exist
*
n* ∈ E and g* ∈ G such that n# [Px] = n# [Px] and g* ◦ n* = h* = h on SUpp(Px,Px). They also
mention that this is not guaranteed to hold in practice.
Problems of DA-DIR Johansson et al. (2019); Zhao et al. (2019) give examples where even under
as strong an assumption as Assumption 3 of Johansson et al. (2019) (i.e., covariate shift and a strong
existence assumption), the two desiderata of DA-DIR (i.e., minimal source risk R(g ◦ n) = R(h*)
and DIR n# [Px] = n# [Px]) still allow the bounds to be uselessly loose and the target risk R(g ◦ n)
to take its worst value (particularly, the two desiderata cannot guarantee n = n * or g = g * or
*
g ◦ n = h* = h on SUpp(Px,Px)). This is essentially an identifiability problem.
The examples do not contradict existing DA bounds. Consider a given representation extractor n.
(1)	Under Eq. (43). Applying the bound on the representation space S gives:
R(g ◦ n) 6 R(g ◦ n) + 2dι (n# [Px], n# [Px])
+ min{En#[px](s)[|gn(S)- Cn(S)|], En#[Px](s)[|gn(S)- gη(S)|]},	(45)
where g； and gj are the optimal labeling functions on top of the representation extractor n. In the
covariate shift case, DIR n# [Px] = n# [PCx] and minimal source risk R(gη* ◦ n) = R(h*) are not
sufficient to guarantee gη* = gCη* (Ben-David et al., 2010b; Gong et al., 2016). Johansson et al. (2019)
argue that they are still not sufficient even under their Assumption 3.
In both examples by Johansson et al. (2019); Zhao et al. (2019), the considered n, although achieving
both desiderata, is not n*, and this n even renders different optimal g’s: gη* 6= gCη*. Johansson et al.
(2019) claim that it is necessary to require n to be invertible to make gη* = gC*η , and develop a bound
that explicitly shows the effect of the invertibility of n . The n in the examples is not invertible.
(2)	Under Eq. (44). Applying the bound on the representation space S gives:
EP(s,y)['(g(S),y)] 6 Ep(s,y)['(g(S),y)] + dG∆G5#[Px],n#IpxD
+ inf [Ep(s,y)['(g(S),y)] + Ep(s,y) ['(g(S),y)]],
g∈G
30
Under review as a conference paper at ICLR 2021
where ps,y := (η, idy)#[px,y] With idy : (x,y) → y and similarly for p§,y. Note that
Ep(s,y)['(g(s),y)] = Ep(χ,y)['(g(η(x),y))] = R(g ◦ η), So the last term on the r.h.s becomes:
infg∈G [R(g ◦ η) + R(g ◦ η)] = λgοη, where G ◦ η := {g ◦ η : g ∈ G}. So the bound becomes:
R(g ◦ n) 6 R(g ◦ η) + dG∆G(n#[Px],n#[Px]) + λG°η.	(46)
This result is shown by Johansson et al. (2019). They argue that finding η that achieves DIR and
minimal training risk cannot guarantee a tighter bound since the last term λg°η may be very large.
In both examples by Johansson et al. (2019); Zhao et al. (2019), SuPP(Px) ∩ SuPP(Px) = 0. It may
cause the problem that g ◦ η can be very different from h on SuPP(Px) even when R(g ◦ η)=
R(h*). The developed bound by Johansson et al. (2019) also explicitly shows the role of support
overlap, thus can be called a support-invertibility bound. They also give an example to show that
DIR (particularly implemented by minimizing MMD) is not necessary (“sometimes too strict”) for
learning the shared/invariant P(y|x).
(3)	A third bound. Zhao et al. (2019) develop another bound for binary classification, where Y :=
{0,1} and R(h) := Ep(x)[|h*(x) 一 h(x)∣]. Denote djs(P,q) := ,JS(p, q) as the JS distance
(Endres & Schindelin, 2003), where JS(P, q) is the JS divergence. It is bounded: 0 6 dJS(P, q) 6 1.
It is shown that (Zhao et al., 2019, Lemma 4.8):
djs(Py,P) 6 dJs(n#[Px],n#[Px]) +
pR(g ◦ η) + γR(g ◦ η).
If djs(Py,Py) > djs(n#[Px],n#[Px])9, it is shown that (Zhao et al., 2019, Theorem4.3):
R(g ◦ η) + R(g◦ η) > 1 (djs(Py,P) 一 djs(n#[Px],n#[Px]))2,	(47)
or when the two domains are allowed to have their own representation-level labeling functions g and
g (Zhao et al., 2019, Corollary 4.1),
R(g ◦ η) + R(I ◦ η) > 1 (djs(Py,Py)- djs(n#[Px],n#[Px]))2.
So when P(y) = P(y), we have djs(Py,Py) > 0, so DIR that minimizes djs(n#[Px],n#[Px])
becomes harmful to minimizing the target risk R(gI ◦ η).
Arjovsky et al. (2019) point out that in the covariate shift case, achieving a DIR P(s) = PI(s) implies
P(y) = PI(y) (since P(s) = PI(s) and P(y|s) = PI(y|s)). This may not hold in practice. When it does
not hold, the bound above shows that DIR can limit prediction accuracy.
Comparison with CSG Existing bounds Eqs. (43, 44, 45, 46) relate the source and target risks
of a general and common labeling function h ∈ H, i.e., R(h) 一 R(h), which is for bounding an
objective; while our bound Eq. (36) relates the target risks of the optimal labeling functions on the
source h* and target h* domains, i.e., ∣R(h*) — R(h*)∣ or ∣RR(h0*) 一 RR(h*)∣, which measures the
risk leap of the best source labeling function on the target domain. After adaptation, the prediction
analysis (Eq. (42)) shows that CSG-DA achieves the optimal labeling function on the target domain
in the infinite data limit.
For Eq. (47), we are not minimizing djs (n# [p(x)] , n# [P(x)]), so our method is good under that view.
In fact, in our model the representation distributions on the two domains are P(s) = P(s, v) dv
and PI(s) = PI(s, v) dv (replacing n#[P(x)] and n# [PI(x)]). We allow P(s, v) 6= PI(s, v) of course
and do not seek to match them. Essentially, we do not rely on the invariance of P(s|x) and P(y|x),
or n* and h*, but the invariance of p(x∣s, V) in the other direction (generative direction). This thus
................................................   .	..	.	~ *
allows P(s|x) 6= PI(s|x)	and P(y|x)	6= PI(y|x),	or n*	6=	nI* and h*	6=	h ,	so we rely on an assumption
different from the idea of all the bounds above. Since the data at hand is produced following a
certain mechanism of nature anyway, the invariance in the generative direction P(x|s, v) is thus
more plausible (see Section 3.2).
9Unfortunately, it seems that the opposite direction holds when there exist η* and g* (unnecessarily the ones
in the existence assumption or the Assumption 3 of Johansson etal. (2019)) such that: Py = (g* ◦ n*)#[px] and
Py = (g* ◦ n*)#[Px] and that η is a reparameterization of η*, due to the celebrated data processing inequality.
31
Under review as a conference paper at ICLR 2021
E Methodology Details
E.1 Derivation of Learning Objectives
The Evidence Lower BOund (ELBO). A common and effective approach to matching the data
distribution p* (x, y) is to maximize likelihood, that is to maximize Ep*(x,y)[logp(x, y)]. It is equiv-
alent to minimizing KL(p*(x,y)∣∣p(x,y)) (note that Ep* [logp*(x,y)] is a constant), so it drives
p(x, y) towards p*(x, y). But the likelihood function p(x, y) = /p(s, v, x, y) dsdv involves an in-
tractable integration, which is hard to estimate and optimize. To address this, the popular method
of variational expectation-maximization (variational EM) introduces a tractable (has closed-form
density function and easy to sample) distribution q(s, v|x, y) of the latent variables given observed
variables, and a lower bound of the likelihood function can be derived:
log p(x, y) = log Ep(s,v)[p(s, v, x, y)] = log Eq(s,v|x,y)
P(S,v,x,y)
.q(s,vlx,y)
p(s, v, x, y)
> Eq(S，v|x，y) [l°g q(s,v∣χ,y)] =: Lq,p(x,y),
where the inequality follows Jensen’s inequality and the concavity of the log function. The function
Lq，p(x, y) is thus called Evidence Lower BOund (ELBO). The tractable distribution q(S, v|x, y)
is called variational distribution, and is commonly instantiated by a standalone model (from the
generative model) called an inference model. Moreover, we have:
Lq，p(x, y) + KL(q(S, v|x, y)kp(S, v|x, y))
p(S, v, x, y)	q(S, v|x, y)
=Eq(s,v∣x,y) log 3——∣——Γ + Eq(s,v∣x,y) log ʒ~∣——V
，	，	q(S, v|x, y)	，	，	p(S, v|x, y)
p(S, v, x, y)
=Eq(s,v∣χ,y) log p(s v∣x y)	= Eq(s,v∣χ,y)[logP(X，y)]
= log p(x, y),
so maximizing Lq,p(x, y) w.r.t q is equivalent to (note that the r.h.s log P(x, y) is constant ofq) min-
imizing KL(qkP(S, v|x, y)) which drives q towards the true posterior (i.e., variational inference),
and once this is (perfectly) done, Lq,p(x, y) becomes a lower bound of log P(x, y) that is tight at the
current model P, so maximizing Lq,p(x, y) w.r.t P effectively maximizes log P(x, y), i.e., serves as
maximizing likelihood. So the training objective becomes the expected ELBO, Ep* (x,y) [Lq,p (x, y)].
Optimizing it w.r.t q and P alternately drives P(x, y) towards P*(x, y) and q(S, v|x, y) towards
P(S, v|x, y) eventually. The derivations and conclusions above hold for general latent variable mod-
els, with (S, v) representing the latent variables, and (x, y) observed variables (data variables).
Variational EM for CSG. In the supervised case, the expected ELBO objective
Ep* (x,y) [Lq,p (x, y)] can also be understood as the conventional supervised learning loss, i.e.
the cross entropy, regularized by a generative reconstruction term. As explained in the main
text (Section 4), after training, we only have the model P(S, v, x, y) and an approximation
q(S, v|x, y) to the posterior P(S, v|x, y), and prediction using P(y|x) is still intractable. So
we employ a tractable distribution q(S, v, y|x) to model the required variational distribution as
q(S, v|x, y) = q(S, v, y|x)/q(y|x), where q(y|x) = q(S, v, y|x) dSdv is the derived marginal
distribution of y (we will show that it can be effectively estimated and sampled from). With this
instantiation, the expected ELBO becomes:
Ep* (x,y) [Lq,p (x, y)]
* q(S, v, y|x)
JP(X，y) Fr
l p(s,v,χ,y)q(y∣χ)
og	q(s, v,y∣χ)
dSdvdXdy
∕p*(χ, y) q⅜v⅛yτlog q(y|x) dsdvdxdy+Z p*(x, y) q⅜v⅛rlog ⅛5χ⅛)dsdvdxdy
/ P*(x)( / P*(y∣x) ∕q(s,v,ylx)dsdv log q(y|x)dy) dx
q(y|X)
+ Z P*(x)( Z P :ylχ) q(s,v,y∣x)log P(SlvX^dSdvdy) dx
q(y|x)	q(S, v, y|x)
32
Under review as a conference paper at ICLR 2021
=Ep*(X)EpYyIx) [log q(yIx)]+ Ep*(X)Eq(S,v,y|X) ]；(£X： log Pq(Sv y,x),
which is Eq. (1). The first term is the (negative) expected cross entropy loss, which drives the infer-
ence model (predictor) q(y∣x) towards p*(y∣x) for p*(x)-ae x. Once this is (perfectly) done, the
second term becomes Ep*(x)Eq(s,v,y|x) [log p(s, v, x, y)/q(s, v, y|x)] which is the expected ELBO
Ep*(x)[Lq(s,v,y|x),p(x, y)] for q(s, v, y|x). It thus drives q(s, v, y|x) towards p(s, v, y|x) and p(x)
towards p*(x). It accounts for a regularization by fitting the input distribution p*(x) and align the
inference model (predictor) with the generative model.
The target of q(s, v, y|x), i.e. p(s, v, y|x), adopts a factorization p(s, v, y|x) = p(s, v|x)p(y|s) due
to the graphical structure (Fig. 1) of CSG (i.e., y ⊥ (x, v)|s). The factor p(y|s) is known (the
invariant causal mechanism to generate y in CSG), so we only need to employ an inference model
q(s, v|x) for the intractable factor p(s, v|x), so q(s, v, y|x) = q(s, v|x)p(y|s). Using this relation,
we can reformulate Eq. (1) as:
Ep* (x,y) [Lq,p (x, y)]
Ep*(x,y)[log q(y|x)] + Ep*(x)
Ep* (x,y) [log q(y|x)] + Ep* (x)
J q(s，v|x)p(y|s) ⅛⅛)log ⅛vRdsdvdy
/ p*(y∣x)
J q(y∣x)
Z Z q(s,v∣x)p(y∣s)↑og P(S^x) dsdv) dy
q(s, v|x)
1	p(s, v, x)
Ep*(x,y)[log q(yIx)] + Ep*(x,y) q(y∣x) Eq(s,v∣x) [p(y|S)Iog q(s v|x)
which is Eq. (2). With this form of q(s, v, y|x) = q(s, v|x)p(y|s), we have q(y|x) =
Eq(s,v|x) [p(yIs)] which can also be estimated and optimized using reparameterization. For pre-
diction, we can sample from the approximation q(yIx) instead of the intractablep(yIx). This can be
done by ancestral sampling: first sample (s, v) from q(s, vIx), and then use the sampled s to sample
y from p(yIs).
The conclusions and methods can also be applied to general latent generative models for supervised
learning, with (s, v) representing the latent variables. When a model does not distinguish the two
(groups of) latent factors s and v and treats them as one latent variable z = (s, v), following a
similar deduction gives:
Ep* (x,y) [Lq,p (x, y)] = Ep* (x,y) [log q(yIx)] + Ep* (x,y)
[q⅛) Eq(ZIx) hp(y|z)log
p(z,x) i
q(z∣x) L
, (48)
where q(yIx) = Eq(zIx) [p(yIz)]. This is the conventional supervised variational auto-encoder
(sVAE) baseline in our experiments.
Variational EM to learn CSG with independent prior (CSG-ind). See the main text in Sec-
tion 4.1 for motivation and basic methods. Since the prior is the only difference between p(s, v, x, y)
and p ⊥(s, v, x, y), we have p(s, v, x, y)/p ⊥(s, v, x, y) = p(s, v)/p ⊥(s, v) = p(s, v)/p(s)p(v) =
p(v∣s)∕p(v). So p(s,v,y∣x) = P(Vvs) Pp((X)P⊥(s,v,y∣x). As explained, inference models now only
need to approximate the posterior (s, v)Ix. Since p(s, v, yIx) = p(s, vIx)p(yIs) andp⊥(s, v, yIx) =
P ⊥(s, vIx)P(yIs) share the sameP(yIs) factor, we have P(s, vIx) = pPvvYpp⅛yP⊥(s,vlx). TheVari-
ational distributions q(s, vIx) and q ⊥(s, vIx) target P(s, vIx) andP⊥(s, vIx) respectively, so we can
express the former with the latter:
q(s, vIx)
P(vIs) P ⊥(x)
P(v) P(x)
q ⊥(s, v Ix).
Once q ⊥(s, vIx) achieves its goal, such represented q(s, vIx) also does so. So we only need to
construct an inference model for q ⊥(s, vIx) and optimize it. With this representation, we have:
P(vIs) P⊥(x)	P⊥(x)	P(vIs)
q(y|x) = Eq(s,v1x)[p(y|s)] = Eq ⊥(s,v∣x) [ -p(vy -p(x-p(y|s)] = -p(x- Eq ⊥(s,v∣x) [ ~(^ p(y|s)
P⊥(x)
即 n(y|X)，
33
Under review as a conference paper at ICLR 2021
where ∏(y∣χ) := Eq ⊥(s,v∣χ)[ Ppvvs) p(y∣s)] as in the main text, which can be estimated and optimized
using the reparameterization of q ⊥(s, v|x). From Eq. (2), the expected ELBO training objective can
be reformulated as:
Ep*(x,y) [Lq,p(x, y)]
Ep* (x,y)
Ep* (x,y)
1	p(S, v, x)
log q(y|x) + ⅛B Eq(s,v∣χ) lp(y|s)log
p ⊥(x)
log -p(χy + log∏(y∣x)
p(x)
p ⊥(x) π (y |x)
P(V|S) P⊥(X)	P(S, V)P(X|S, V)
Eq ⊥(S,v|x)[ M 而 P(y|S)lOg U p⊥x) q ⊥(s,ν∣χ)[
p ⊥(x)
Ep* (x,y) log -p(χy+ log n(y|X)
+ ∏(⅛y Eq ⊥(s,vlX) h ppvsr p(yls)(log p⊥⅜ + log
Ep* (x,y)
p ⊥(x)	1
log 而+log n(y|x) + π(y∏ Eq ⊥(S，v|x)
P(S)P(V)P(X|s,V) m^∣
q⊥(s, v|x)
p(y|s)ilog τ⊥⅛
P (X)
Ep*(x,y)
+ πyχ) Eq ⊥(s,vlx)
p⊥(s, v)p(x|s, v)
P(y|S)Iog	q ⊥(s,ν∣x)
P ⊥(X)	1	P(X)
log +log n(y|x) +	n(y|x) log p⊥x)
+ π(yχ Eq ⊥(s,vlx)
p⊥(s, v, x)
p(y|S)IOg
Ep*(x,y) log ∏(y∣x) +
π(y而 Eq ⊥(S，v|x)
P ⊥(S, V, X)
p(y|s)log q⊥^χ)∖ J，
+
1
where in the second-last equality We have used the definition of ∏(y∣χ). This gives Eq. (3). Note
that ∏(y∣χ) is not used in prediction, so there is no need to sample from it. Prediction is done
by ancestral sampling from q⊥(y∣x), that is to first sample from q⊥(s,v∣x) and then from p(y∣s).
Using this reformulation, we can train a CSG with independent prior even on data that manifests a
correlated prior. The objective Eq. (5) on the training domain for domain adaptation can be derived
similarly.
For numerical stability, we employ the log-sum-exp trick to estimate the expectations and compute
the gradients.
E.2 Instantiating the Inference Model
Although motivated from learning a generative model, the method
can be implemented using a general discriminative model (with
hidden nodes) with causal behavior. By parsing some of the hidden
nodes as S and some others as V, a discriminative model could for-
malize a distribution q(S, V, y|X), which implements the inference
model and the generative mechanism P(y|S). The parsing mode is
shown in Fig. 3, which is based on the following consideration.
(1) The graphical structure of CSG in Fig. 1 indicates that (V, X) ⊥
y |S, so the hidden nodes for S should isolate y from V and
X. The model then factorizes the distribution as q(S, V, y|X) =
q(S, V|X)q(y|S), and since the inference and generative models
share the distribution on y |S (see the main text for explanation),
we can thus use the component q(y|S) given by the discriminative
model to implement the generative mechanismP(y|S).
Figure 3: Parsing a gen-
eral discriminative model as
an inference model for CSG.
The black solid arrow spec-
ifies P(y|S) in the generative
model, and the blue dashed
arrows (representing computa-
tional directions but not causal
directions) specify q(S, V|X)
(or q⊥(s,v∣x) or q(s,v∣x)) as
the inference model.
34
Under review as a conference paper at ICLR 2021
(2) The graphical structure in Fig. 1 also indicates that s 6⊥ v|x due to the v-structure (collider) at
x (“explain away”). The component q(s, v|x) should embody this dependence, so the hidden nodes
chosen as v should have an effect on those as s. Note that the arrows in Fig. 3 represent computation
directions but not causal directions. We orient the computation direction v → s since all hidden
nodes in a discriminative model eventually contribute to computing y.
After parsing, the discriminative model gives a mapping (s, v) = η(x). We implement the distribu-
tionby10 q(s,v∣x) = N(s,v∖η(x), Σq). For all the three cases of CSG, CSG-ind and CSG-DA, only
one inference model for (s, v)|x is required. The component (s, v)|x of the discriminative model
thus parameterizes q⊥(s,v∖x) and q(s,v∖x) for CSG-ind and CSG-DA. The expectations in all ob-
jectives (except for expectations over p* which are estimated by averaging over data) are all under
the respective (s, v)∖x. They can be estimated using η(x) by the reparameterization trick (Kingma
& Welling, 2014), and the gradients can be back-propagated.
We need two more components beyond the discriminative model to implement the method, i.e. the
prior p(s, v) and the generative mechanism p(x∖s, v). The latter can be implemented using a gen-
erator or decoder architecture comparable to the component q(s, v∖x). The prior can be commonly
implemented using a multivariate Gaussian distribution, p(s, V) = N((V )∖( μ ), Σ = ( ςSs ∑sv)).
We parameterize Σ via its Cholesky decomposition, Σ = LL>, where L is a lower-triangular ma-
trix with positive diagonals, which is in turn parameterized as L = MLss L0 with smaller lower-
triangular matrices Lss and Lvv and any matrix Mvs . Matrices Lss and Lvv are parameterized
by a summation of positive diagonals (guaranteed via an exponential map) and a lower-triangular
(excluding diagonals) matrix. The conditional distribution p(v∖s) required by training CSG-ind is
given by p(v∖s) = N(v∖μ0∣s, ∑v∣s), where μ°∣s = μ° + MvsL-I(S — μs), ∑v∣s = LvvL> (see e.g.,
Bishop (2006)). This prior does not imply a causal direction between s and v (the linear Gaussian
case of Zhang & HyVarinen (2009)) thus well serves as a prior for CSG.
F Experiment Details
We use a validation set from the training domain for hyperparameter selection, to avoid overfitting
to the finite training-domain data samples. The training and validation sets are constructed under a
80%-20% random split in each task. We note that hyperparameter selection in OOD tasks is itself
controversial and nontrivial, and it is still an active research direction (You et al., 2019). It is argued
that if a validation set from the test domain is available, a better choice would be to incorporate
it in learning as the semi-supervised adaptation task, instead of using it just for validation. As
our methods are designed to fit the training domain data and our theory shows guarantees under a
good fit of the training-domain data distribution, hyperparameter selection using a training-domain
validation set is reasonable.
We align the scale of the CE term in the objectives of all methods, and tune the coefficients of the
ELBOs tobe their largest values that make the final accuracy near 1 on the validation set, so that they
wield the most power on the test domain while be faithful to explicit supervision. The coefficients
are preferred to be large to well fit p* (x) (and p* (x) for domain adaptation) to gain generalizability
in the test domain, while they should not affect training accuracy, which is required for a good fit of
the training distribution.
The supervised variational auto-encoder (sVAE) baseline method is a counterpart of CSG that does
not separate its latent variable z into s and v. This means that all its latent variables in z directly (i.e.,
not mediated by s) affect the output y. It is learned by optimizing Eq. (48) for OOD generalization,
and adopts a similar method as CSG-DA for domain adaptation. To align the model architecture for
fair comparison, this means that the latent variable z of sVAE can only be taken as the latent variable
s in CSG.
All the experiments are implemented in PyTorch.
10Other approaches to introducing randomness are also possible, such as employing stochasticity on the
parameters/weights as in Bayesian neural networks (Neal, 1995), or using dropout (Srivastava et al., 2014; Gal
& Ghahramani, 2016). Here we adopt this simple treatment to highlight the main contribution.
35
Under review as a conference paper at ICLR 2021
F.1 SHIFTED MNIST
We use a multilayer perceptron (MLP) with sigmoid activation with 784(for x)-400-200(first 100
for v)-50(for s or z)-1(for y) nodes in each layer for the inference model of generative methods, and
use an MLP with 50(for s)-(100(for v)+100)-400-784(for x) nodes in each layer for their generative
component p(x|s, v). We use a larger architecture with 784-600-300-75-1 nodes in each layer for
discriminative methods to compensate additional parameters of generative methods. For all the
methods, we use a mini-batch of size 128 in each optimization step, and use the RMSprop optimizer
(Tieleman & Hinton, 2012), with weight decay parameter 1 × 10-5, and learning rate 1 × 10-3 for
OOD generalization and 3 × 10-4 for domain adaptation. These hyperparameters are chosen and
then fixed, by running and then validating using CE and DANN. For generative methods, we take
the Gaussian variances of p(x|s, v) and q(s, v|x) as 0.032. The scale of the standard derivations
of these conditional Gaussian distributions are chosen small to meet the intense causal mechanism
assumption in our theory (e.g., in Theorem 5.4).
We train the models for 100 epochs when all the methods converge in terms of loss and training
accuracy. We align the scale of the CE term in the objectives of all methods, and scale the ELBO
terms with the largest weight that makes training accuracy near 1 in OOD generalization. We then
fix the tuned weight and scale the weight of adaptation terms in a similar way for domain adaptation.
Other parameters are tuned similarly. For generative methods, the ELBO weight is 1 × 10-5 selected
from {1, 3}×10{-6,-5}∪1×10{-2,-1,0,1,2}, and the adaptation weights for sVAE-DA and CSG-DA
are 1 × 10-2 selected from 1 × 10{0,-1,-2,-3,-4} and 1 × 10-5 selected from 1 × 10{0,-1,-2,-3,-4} ∪
{1, 3} × 10{-5,-6,-7,-8}. For domain adaptation methods, the adaptation weight is 1 × 10-4
except for CDAN which adopts 1 × 10-2, all selected from 1 × 10{0,-1,-2,-3,-4}. For CNBB, we
use regularization coefficients 1 × 10-4 and 3 × 10-6 to regularize the sample weight and learned
representation, and run 4 inner gradient descent iterations with learning rate 1 × 10-3 to optimize the
sample weight. These parameters are selected from a grid search where the range of the parameters
are: {1, 3} × 10{-2,-3,-4}, {1, 3} × 10{-4,-5,-6}, {4, 8}, 1 × 10{-1,-2,-3}.
F.2 ImageCLEF-DA
We adopt the same setup as in Long et al. (2018). We use the ResNet50 structure pretrained on
ImageNet as the backbone of the discriminative/inference model. Input images are cropped and
resized to shape (3, 224, 224). For CSG, we select the first 128 dimensions of the bottleneck layer
(the resized last fully connected layer of ResNet50, with output dimension 1024) as the variable v,
and the output of a subsequent fully connected layer with output dimension 1024 as the variable s.
The output is produced by a linear layer built on s.
For generative methods (i.e., our methods and sVAE(-DA)), we construct an image de-
coder/generator that uses the DCGAN model (Radford et al., 2015) pretrained on Cifar10 as the
backbone. The pretrained DCGAN is adapted from the PyTorch-GAN-Zoo11. The generator con-
nects to the DCGAN backbone by an MLP layer to match DCGAN’s input dimension 120, and
generates images of desired size (3, 224, 224) by appending to DCGAN’s output of size (3, 64, 64)
with an transposed convolution layer with kernel size 4, stride size 4 and padding size 16.
Following Long et al. (2018), we use a mini-batch of size 32 in each optimization step, and adopt
the SGD optimizer with Nesterov momentum parameter 0.9, weight decay parameter 5 × 10-4, and
a shrinking step size scheme with initial scale 1 × 10-3, shrinking exponent 0.75 and per-datum
coefficient 6.25 × 10-6. For CSG methods, the Gaussian variances of p(x|s, v) and q(s, v|x) are
taken as 0.1 and 3.0, respectively. The ELBO weight is 1 × 10-8 for CSG methods and 1 × 10-7 for
sVAE, both selected from 1 × 10{-2,-4,-6} ∪ {1, 3} × 10{-7,-8,-9,-10}. The adaptation weights for
sVAE-DA and CSG-DA are 1 × 10-8 for task C→P and 1 × 10-7 for task P→C, selected from the
same range. For CNBB, we use regularization coefficients 1 × 10-6 and 3 × 10-6 to regularize the
sample weight and learned representation, and run 4 inner gradient descent iterations with learning
rate 1 × 10-4 to optimize the sample weight. These parameters are selected from a grid search where
the range of the parameters are: 1 × 10{-4,-5,-6,-7} ∪ {3 × 10-6}, {1, 3} × 10{-5,-6,-7}, {4},
1 × 10{-2,-3,-4,-5}.
11https://github.com/facebookresearch/pytorch_GAN_zoo
36