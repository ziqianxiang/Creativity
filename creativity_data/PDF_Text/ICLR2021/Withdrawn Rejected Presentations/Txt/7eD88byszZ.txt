Under review as a conference paper at ICLR 2021
A Unified Spectral Sparsification Framework
for Directed Graphs
Anonymous authors
Paper under double-blind review
Ab stract
Recent spectral graph sparsification research allows constructing nearly-linear-
sized subgraphs that can well preserve the spectral (structural) properties of the
original graph, such as the first few eigenvalues and eigenvectors of the graph
Laplacian, leading to the development of a variety of nearly-linear time numerical
and graph algorithms. However, there is not a unified approach that allows for truly-
scalable spectral sparsification of both directed and undirected graphs. For the first
time, we prove the existence of linear-sized spectral sparsifiers for general directed
graphs, and introduce a practically-efficient yet unified spectral graph sparsification
approach that allows sparsifying real-world, large-scale directed and undirected
graphs with guaranteed preservation of the original graph spectra. By exploiting
a highly-scalable (nearly-linear complexity) spectral matrix perturbation analysis
framework for constructing nearly-linear sized (directed) subgraphs, it enables us to
well preserve the key eigenvalues and eigenvectors of the original (directed) graph
Laplacians. The proposed method has been validated using various kinds of directed
graphs obtained from public domain sparse matrix collections, showing promising
results for solving directed graph Laplacians, spectral embedding, and partitioning
of general directed graphs, as well as approximately computing (personalized)
PageRank vectors.
1	Introduction
Many research problems for simplifying large graphs leveraging spectral graph theory have been
extensively studied by mathematics and theoretical computer science (TCS) researchers in the past
decade (Batson et al., 2012; Spielman & Teng, 2011; Kolev & Mehlhorn, 2015; Peng et al., 2015;
Lee & Sun, 2017; Cohen et al., 2017; 2018). Recent spectral graph sparsification research allows
constructing nearly-linear-sized subgraphs that can well preserve the spectral (structural) properties of
the original graph, such as the the first few eigenvalues and eigenvectors of the graph Laplacian. The
related results can potentially lead to the development of a variety of nearly-linear time numerical
and graph algorithms for solving large sparse matrices and partial differential equations (PDEs),
graph-based semi-supervised learning (SSL), computing the stationary distributions of Markov chains
and personalized PageRank vectors, spectral graph partitioning and data clustering, max flow and
multi-commodity flow of undirected graphs, nearly-linear time circuit simulation and verification
algorithms, etc. (Koutis et al., 2010; Spielman & Teng, 2011; Christiano et al., 2011; Spielman &
Teng, 2014; Kelner et al., 2014; Cohen et al., 2017; 2018; Feng, 2016; 2018).
However, there is not a unified approach that allows for truly-scalable spectral sparsification of both
directed and undirected graphs. For example, the state-of-the-art sampling-based methods for spectral
sparsification are only applicable to undirected graphs (Spielman & Srivastava, 2011; Koutis et al.,
2010; Spielman & Teng, 2014); the latest algorithmic breakthrough in spectral sparsification of
directed graphs (Cohen et al., 2017; 2018) can only handle strongly-connected directed graphs 1,
which inevitably limits its applications when confronting real-world graphs, since many directed
graphs may not be strongly connected, such as the graphs used in chip design automation (e.g., timing
analysis) tasks as well as the graphs used in machine learning and data mining tasks.
1A strongly connected directed graph is a directed graph in which any node can be reached from any other
node along with direction.
1
Under review as a conference paper at ICLR 2021
Consequently, there is still a pressing need for the development of highly-robust (theoretically-
rigorous) and truly-scalable (nearly-linear complexity) algorithms for reducing real-world large-scale
(undirected and directed) graphs while preserving key graph spectral (structural) properties. In
summary, we make the following contributions:
•	We, for the first time, prove the existence of linear-sized spectral sparsifiers for general
directed graphs, and introduces a practically-efficient yet unified spectral sparsification
approach that allows simplifying real-world, large-scale directed and undirected graphs with
guaranteed preservation of the original graph spectra.
•	We exploit a highly-scalable (nearly-linear complexity) spectral matrix perturbation analysis
framework for constructing ultra-sparse (directed) subgraphs that can well preserve the key
eigenvalues and eigenvectors of the original graph Laplacians. Unlike the prior state-of-
the-art methods that are only suitable for handling specific types of graphs (e.g., undirected
or strongly-connected directed graphs (Spielman & Srivastava, 2011; Cohen et al., 2017)),
the proposed approach is more general and thus will allow for truly-scalable spectral
sparsification of a much wider range of real-world complex graphs.
•	Through extensive experiments on real-world directed graphs, we show how the proposed
directed graph spectral sparsification method can be exploited for computing PageRank
vectors, directed graph clustering and developing directed graph Laplacian solvers.
The spectrally-sparsified directed graphs constructed by the proposed approach will potentially lead
to the development of much faster numerical and graph-related algorithms. For example, spectrally-
sparsified social (data) networks allow for more efficient modeling and analysis of large social
(data) networks; spectrally-sparsified neural networks allow for more scalable model training and
processing in emerging machine learning tasks; spectrally-sparsified web-graphs allow for much faster
computations of personalized PageRank vectors; spectrally-sparsified integrated circuit networks will
lead to more efficient partitioning, modeling, simulation, optimization and verification of large chip
designs, etc.
2	Related works
Directed graph symmetrization. When dealing with the directed graph sparsification, it’s natural
to apply symmetrization methods for converting asymmetric directed graphs into symmetric undi-
rected graphs, so that we can apply the existing spectral graph theories for directed graphs after
symmetrization. In the following, given a directed graph or its corresponding adjacency matrix 2 A ,
we will review the most popular graph symmetrization methods:
•	A + A> symmetrization simply ignores the edges’ directions, which is the simplest and
most efficient way for directed graph symmetrization. However, edge directions may play
an important role in directed graphs. As shown in Figure 1, edges (8, 1) and (4, 5) seem to
have the equal importance in the symmetrized undirected graph A + A> . However, in the
original directed graph, edge (8, 1) is much more important than edge (4, 5), since removing
edge (8, 1) will lead to the loss of more connections in the directed graph. For example,
removing edge (4, 5) will only affect the walks from node 4 to any other nodes and walks
from any other nodes to node 5. However, if we remove edge (8, 1) in the directed graph, it
will not only affect walks from node 8 to any other nodes and walks from any other nodes to
node 1, there will be also no access from node 5 ,6, 7 and 8 to any of nodes 1, 2, 3 and 4.
•	Bibliographic symmetrization (Satuluri & Parthasarathy, 2011) adopts AA> + A>A as
the adjacency matrix after symmetrization to take the in-going and out-going edges into
consideration. However, it cannot be scaled to large-scale graphs since it will create much
denser undirected graphs after symmetrization. Also, disconnected graphs can be created
due to the AA> + A>A symmetrization, as shown in Figure 1.
•	Random-walk symmetrization (Chung, 2005) is based on random walks and allows
normalized cut to be preserved after symmetrization. This is also the symmetrization
approach used in recent work for spectral sparsification of directed graphs (Cohen et al.,
2017). However, it only works on strongly-connected aperiodic directed graphs. For
2 The concept the adjacency matrix for the directed graph will be further introduced in Section 3.1
2
Under review as a conference paper at ICLR 2021
Figure 1: Converting a directed graph G in (I) into undirected graphs using A + A> as shown in (II),
AA> + A>A as shown in (III), and the proposed LGLG symmetrization as shown in (IV).
Table 1: Summary of symbols used in this paper
Before symmetrization	After symmetrization
-G=7V7EG,wG):-directed"(undirected)graph- S = (V, ES, WS): sparsifier of graph G Lg： Laplacian matrix of graph G LS: Laplacian matrix of sparsifier S	Gu = (V, EGu, WGu )： UndireCted graph Su = (V, Esu ,wsu): sparsifier of graph Gu LGu: Laplacian matrix of graph Gu LSu: Laplacian matrix of sparsifier Su
example, we can not apply the random-walk based symmetrization for the directed graph
shown in Figure 1, since it,s a periodic directed graph.
Cheeger's inequality for directed graphs. In undirected graph problems, Cheeger,s inequality
plays a significant role in spectral analysis of undirected graphs, which connects Cheeger constant
(conductance) with spectral properties (eigenvalues of the graph Laplacian matrix) of a graph. In
(Chung, 2005) the Cheeger,s inequality has been extended to directed graphs based on the random-
walk Laplacian symmetrization scheme, as we mentioned earlier. It also provides the bound for the
smallest eigenvalue of the directed graph Laplacian. However, the related theoretical results can
only be applied to strongly-connected and aperiodic directed graphs, which are rare in real-world
applications.
Spectral sparsification of directed graphs. The latest algorithmic breakthrough in spectral
sparsification for strongly-connected aperiodic graphs has been introduced based on the results in
(Chung, 2005), which proposes to convert strongly-connected graphs into Eulerian graphs via Eulerian
scaling, and subsequently sparsify the undirected graphs (obtained via directed graph symmetrization
(Chung, 2005)) leveraging existing undirected graph spectral sparsification methods (Cohen et al.,
2017). It has been shown that such an approach can potentially lead to the development of almost-
linear-time algorithms for solving asymmetric linear systems, computing the stationary distribution
of a Markov chain and computing expected commute times in a directed graph, etc (Cohen et al.,
2017; 2018).
3	A Theoretical Framework for Unified Spectral Sparsification
3.1	Laplacians for directed and undirected graphs
Consider a directed graph G = (V, EG, wG) with V denoting the set of vertices, EG representing
the set of directed edges, and wG denoting the associated edge weights. Let n = |V |, m = |EG| be
the size of node and edge set. In the following, we denote the diagonal matrix by DG with DG (i, i)
being equal to the (weighted) outdegree of node i, as well as the adjacency matrix of G by AG:
wG(i, j)
AG(i,j) = 0
if (i, j) ∈EG
otherwise .
(1)
Then the directed Laplacian matrix can be constructed as follows (Cohen et al., 2017):
LG = DG - A>G. For better illustration, we have summarized symbols used in our paper in Table 1.
It can be shown that any directed (undirected) graph Laplacian constructed using (3.1) will satisfy the
following properties: I) Each column (and row) sum is equal to zero; II) All off-diagonal elements
are non-positive; III) The Laplacian matrix is asymmetric (symmetric) and indefinite (positive
semidefinite).
3
Under review as a conference paper at ICLR 2021
3.2	Spectral sparsification of (un)directed graphs
Graph sparsification aims to find a subgraph (sparsifier) S = (V, ES , wS) that has the same set
of vertices but much fewer edges than the original graph G. There are two types of sparsification
methods: the cut sparsification methods preserve cuts in the original graph through random sampling
of edges (Bencz缸 & Karger, 1996), whereas spectral SParsification methods preserve the graph
spectral (structural) properties, such as distances between vertices, effective resistances, cuts in the
graph, as well as the stationary distributions of Markov chains (Cohen et al., 2017; 2018; Spielman &
Teng, 2011). Therefore, spectral graph sparsification is a much stronger notion than cut sparsification.
For undirected graphs, spectral sparsification aims to find an ultra-sparse subgraph proxy that is
spectrally-similar to the original one. G and S are said to be σ-spectrally similar if the following
condition holds for all real vectors x ∈ RV :
X LSx ≤ x>LGX ≤ σx>Lsx,	(2)
σ
where LG and LS denote the symmetric diagonally dominant (SDD) Laplacian matrices of graphs G
and S, respectively. Relative condition number can be defined as κ(LG, LS) ≤ σ2, implying that a
smaller relative condition number or σ2 corresponds to a higher (better) spectral similarity between
two graphs.
For directed graphs : Spectrum-preserving LGL>G symmetrization the subgraph S can be con-
sidered spectrally similar to the original graph G if the condition number or the ratio between the
largest and smallest singular values of LS+LG is close to 1 (Cohen et al., 2017; 2018), where LS+ de-
notes the Moore-Penrose pseudoinverse of LS. Spectral sparsification of directed graphs is equivalent
to finding an ultra-sparse subgraph S such that the condition number of (LS+LG)> (LS+LG) is small
enough. Since the singular values of LS+LG are the square roots of eigenvalues of (LS+LG)>(LS+LG).
While (LS+LG)> (LS+LG) can be written into L>G (LSLS> )+LG, L>G (LSLS>)+LG is not equal to
(LSLS>)+(LGL>G). They do share the same eigenvalues under special conditions according to the
following theorem (Horn & Johnson, 2012):
Theorem 3.1. Suppose that matrices X ∈ Rm0,n0 and Y ∈ Rn0,m0 with m0 ≤ n0. Then the n0
eigenvalues of YX are the m0 eigenvalues of XY together with n0 - m0 zeroes; that is pYX (t) =
tn0 -m0 pXY (t). If m0 = n0 and at least one of X or Y is nonsingular, then XY and YX are similar.
Based on Therorem 3.1, L>G(LSLS>)+LG and (LSLS>)+(LGL>G) share the same eigenvalues when
a small value is added on each diagonal of LG. Under this condition, spectral sparsification of
directed graphs is equivalent to finding an ultra-sparse subgraph S such that the condition number of
(LSLS>)+(LGL>G) is small enough. Theorem 3.2 shows both LGL>G and LSLS> are the Laplacian
matrices for some undirected graphs.
Theorem 3.2. For any directed graph G = (V, EG, wG) and its directed Laplacian LG, its sym-
metrized undirected graph Gu = (V, EGu , wGu) can be obtained via Laplacian symmetrization
LGu = LGL>G. LGu is positive semi-definite (PSD) and will have the all-one vector as its null space,
while the corresponding undirected graph may include negative edge weights.
The proof is in the Appendix. If we can prove that there exists an ultra-sparse subgraph S such
that its corresponding undirected graph Su (with LSu = LSLS>) is the spectral sparsifier of Gu
(with LGu = LGL>G), then the directed subgraph S becomes the spectral sparsifier of G . More
detailed proofs are shown in the Appendix. The core idea of our approach is to leverage a novel
spectrum-preserving Laplacian symmetrization procedure to convert directed graphs into undirected
ones that may have negative-weighted edges (as shown in Figure 1). Such a Laplacian symmetrization
scheme will immediately allow us to exploit existing methods for spectral sparsification.
4	A Practical Framework for Unified Spectral Sparsification
To apply our theoretical results to deal with real-world directed graphs, the following concerns should
be addressed in advance:
•	The undirected graph LGL>G may become too dense to compute and thus may impose high
cost during spectral sparsification.
4
Under review as a conference paper at ICLR 2021
•	It can be quite challenging to convert the sparsified undirected graph to its corresponding
directed sparsifier LS, even when LSu is available.
To address the above concerns for unified spectral graph sparsification, we propose a practically-
efficient framework with following desired features: 1) our approach dose not require to explicitly
compute LGL>G but only the matrix-vector multiplications; 2) our approach can effectively identify
the most spectrally-critical edges for dramatically decreasing the relative condition number; 3) al-
though our approach requires to compute LSLS>, the LSu matrix density can be effectively controlled
by carefully pruning spectrally-similar edges through the proposed edge similarity checking scheme.
4.1	Initial subgraph sparsifier construction
Motivated by the recent research on low-stretch spanning trees (Elkin et al., 2008; Abraham &
Neiman, 2012) and spectral perturbation analysis (Feng, 2016; 2018) for nearly-linear-time spectral
sparsification of undirected graphs, we propose a practically-efficient algorithm for sparsifying
general directed graphs by first constructing the initial subgraph sparsifiers of directed graphs with
the following procedure:
•	Compute D-1(AG + A>G) as a new adjacency matrix, where D denotes the diagonal matrix with
each element equal to the row (column) sum of (AG + A>G). Recent research shows such split
transformations can effectively reduce graph irregularity while preserving critical graph connectivity,
distance between node pairs, the minimal edge weight in the path, as well as outdegrees and indegrees
when using push-based and pull-based vertex-centric programming (Nodehi Sabet et al., 2018).
•	Construct a maximum spanning tree (MST) based on D-1 (AG + A>G), which allows us to
effectively control the number of outgoing edges for each node so that the resultant undirected graph
after Laplacian symmetrization will not be too dense.
•	Recover the direction of each edge in the MST and make sure each node of its sparsifier has at least
one outgoing edge if there are more than one in the original graph for achieving stronger connectivity
in the initial directed sparsifier.
4.2	Spectral Sparsification via Riemannian Distance Minimization
The Riemannian distance δ2 between positive definite (PSD) matrices is arguably the most natural
and useful distance on the positive definite cone Sn++ (Bonnabel & Sepulchre, 2010), which can be
computed by (Lim et al., 2019):
1
n	2
δ2 : S+n+ × S+n+ -→R+ and δ2(LSu,LGu) = X log2 λi	,	(3)
i=1
where λmaχ = λι ≥ …≥ λn ≥ 1 ≥ 入靖+1 ≥,…，≥ λn denote the descending eigenvalues of
L+uLGu, and vι, v2,…，Vn denote the corresponding eigenvectors. Since both Su and Gu are
PSD matrices, we have λi ≥ 0, which leads to the following inequality:
0
nn
δ2(LSu,LGu) ≤ Xlogλi + X log λ^
i=1	i=n0+1	i
≤ max{nlogλι,nlog -^—}.
λn
(4)
The Courant-Fischer theorem allows computing generalized eigenvalues and eigenvectors by:
λ1
x>LGux
max 丁 --------
|x|6=0, x LSu x
x>1=0
x>LG x
λn = min 丁y——
|x|6=0, x LSu x
x> 1=0
(5)
where 1 ∈ Rn denotes the all-one vector. Assigning each node in the graph with an integer value
either 0 or 1, the corresponding Laplacian quadratic form measures the boundary size (cut) of a node
set. For example, if a node set Q is defined as a subset when its nodes are all assigned with an integer
value 1 while other nodes are assigned with a value 0, then node set Q and its boundary ∂Gu (Q) can
be represented as
Q = {p ∈ V : x(p) = 1} and ∂Gu (Q) = {(p, q) ∈ E,p ∈ Q,q ∈/ Q}.	(6)
5
Under review as a conference paper at ICLR 2021
Since the number of outgoing edges crossing the boundary can be computed by XTLGU X = | ∂gu (Q)|,
the following holds:
λ1
max
XTLGUX
max
XTLGUX _ |dGu(Q)|
IIIaX	≥	IIIaX	——	,
∣χ∣=o, XTLSUx —	∣χ∣=0,	XTLSUX	∣∂su(Q)|
(7)
x>1=0
x(p)∈{0,1}
which implies that the maximum mismatch between Gu and Su will be bounded by λ1. Leveraging
the dominant generalized eigenvectors will allow us to identify the edges crossing the maximally-
mismatched boundary ∂GU (Q). Including such crossing edges into Su will dramatically decrease
the maximum mismatch (λ1), thereby improving spectral approximation of the sparsifier. Conse-
quently, the following problem formulation for spectral graph sparsification is proposed:
XT LG X
jsn{max (x>⅛)+β kLSuk1
(8)
which allows effectively minimizing the largest generalized eigenvalue (upper bound of mismatch)
and the Riemaninan distance (δ2) by including the minimum amount of edges into the subgraph Su.
4.3	A unified spectral perturbation analysis framework
As aforementioned, spectral sparsification of (un)directed graphs can be effectively achieved by solv-
ing (8). To this end, we will exploit the following spectral perturbation analysis framework. Given the
generalized eigenvalue problem LGu Vi = λiLsuVi with i = 1, ∙∙∙ ,n,let matrix V = [vι, ∙∙∙ , Vn].
Then vi and λi can be computed to satisfy the following orthogonality requirement:
ViT LGuVj =	0λ,i,	ii	6==	jj	and	ViT	LSuVj	=	10,,	ii	6==jj.	(9)
Consider the following first-order generalized eigenvalue perturbation problem:
LGu(Vi+δVi)=(λi+δλi)(LSu+δLSu)(Vi+δVi),	(10)
where a small perturbation δLSu in LSu is introduced, leading to the perturbed generalized eigenval-
ues and eigenvectors λi + δλi and Vi + δVi. By only keeping the first-order terms, (10) becomes:
LGuδVi = λiLSu δVi + λiδLSu Vi + δλiLSu Vi.	(11)
Let δVi = Pj ψi,jVj, then (11) can be expressed as:
ψi,jLGuVj = λiLSu (	ψi,jVj) + λiδLSu Vi + δλiLSuVi	(12)
jj
Based on the orthogonality properties in (9), multiplying vi to both sides of (12) results in
λiδLSuVi + δλiLSu Vi = 0.	(13)
Then the task of spectral sparsification of general (un)directed graphs will require to recover as few
as possible extra edges to the initial directed subgraph S such that the largest eigenvalues, or the
condition number of LS+ LGu can be dramatically mitigated. Expanding (13) will simply lead to:
-= = -VT δLsu vi.	(14)
Expand δLSu with only the first-order terms as δLSu = δLS LST + LS δLST , where δLS =
wG (p, q)ep,qepT for (p, q) ∈ EG \ ES, ep ∈ Rn denotes the vector with only the p-th element
being 1 and others being 0, and ep,q = ep - eq. The spectral sensitivity for each off-subgraph edge
(p, q) can be expressed as:
ζp,q = ViT(δLSLST +LSδLST)Vi,	(15)
It is obvious that (15) can be leveraged to rank the spectral importance of each edge. Consequently,
spectral sparsification of general graphs can be achieved by only recovering a few dissimilar edges
6
Under review as a conference paper at ICLR 2021
with large sensitivity values. In this work, the following method based on t-step power iterations is
proposed to achieve efficient computation of dominant generalized eigenvectors
v1 ≈ ht = (LS+u LGu)t h0,	(16)
where h0 denotes a random vector. When the number of power iterations is small (e.g., t ≤ 3), ht
will be a linear combination of the first few dominant generalized eigenvectors corresponding to
the largest few eigenvalues. Then the spectral sensitivity for the off-subgraph edge (p, q) can be
approximately computed by
ζp,q ≈ ht>(δLSLS> + LSδLS>)ht.	(17)
The computation of ht through power iterations requires solving the linear system of equations
LSux = b for t times. We note that only LSu need to be explicitly computed for generalized power
iterations. The latest Lean Algebraic Multigrid (LAMG) solver has been leveraged for computing
ht , which can handle undirected graphs with negative edge weights and achieve O(m) runtime
complexity for solving large graph Laplacian matrices (Livne & Brandt, 2012).
4.4	Edge spectral similarities
To avoid recovering redundant edges into the subgraph, itis indispensable to check spectral similarities
between candidate off-subgraph edges. In other words, only the off-subgraph edges that are spectrally
critical (have higher spectral sensitivity scores) but not spectrally-similar to each other will be
recovered to the initial subgraph. To this end, we exploit the following spectral embedding of
off-subgraph edges using approximate dominant generalized eigenvectors ht computed by (16):
τp,q = ht>epqep>LS>ht,	(18)
which will help estimate spectral similarities among different off-subgraph edges. To improve the
accuracy, we can always compute r = O(log n) approximate dominant generalized eigenvectors
h(t1),...,h(tr) to obtain a r-dimensional spectral embedding vector Tp,q for each edge (p, q). The
edge spectral similarity between two edges (pi, qi) and (pj , qj) is defined as follows:
βi,j = ||Tpi
,qi - Tpj,qj ||/max(||Tpi,qi ||, ||Tpj,qj ||).	(19)
If (1 - βi,j) < % for a given constant %, edge (pi, qi) is considered spectrally dissimilar with edge
(pj , qj ).
4.5	Algorithm flow and complexity
Algorithm 1 Algorithm Flow for Directed Graph Sparsification
Input: LG, LS, dout, iter
max, λlimit, α, %
1:	Calculate largest generalized eigenvector ht, largest generalized eigenvalue λmax and let iter = 1;
2:	while λmax < λlimit, iter < itermax do
3:	Calculate the spectral sensitivities ζp,q for each off-subgraph edges (p, q) ∈ EG\S;
4:	Sort spectral sensitivities in descending order and obtain the top α% off-subgraph edges into
edge list Elist = [(p1,q1), (p2,q2), ...];
5:	Do Eaddlist = Edge_Similarities_Checking(Elist, LG , LS , dout, %) ;
6:	Update Snew = S + Eaddlist and calculate largest generalized eigenvector htnew, largest gener-
alized eigenvalue λmaxnew based on LG and LSnew ;
7:	if λmaxnew < λmax then
8:	Update S = Snew , ht = htnew, λmax = λmaxnew ;
9:	end if
10:	iter = iter + 1;
11:	end while
12:	Return graph S and LS .
Algorithm 1 shows the algorithm flow for directed graph sparsification, where LG is the Laplacian
matrix for original graph, LS is the Laplacian matrix of initial spanning tree, dout is the user-defined
outgoing degree for nodes, itermax is the maximum number of iterations, and λlimit is the desired
maximum generalized eigenvalue. Eaddlist = Edge_Similarities_Checking(Elist, LG , LS , dout, %) is
introduced in the Appendix. The complexity has been summarized as follows:
7
Under review as a conference paper at ICLR 2021
Table 2: Results of directed graph spectral sparsification
Test Cases	IVGI	EgI	!ESH Wl	ES l EgI		time (s)	λ1 λ1,fin	
gre_115	TΓE2	T2E2	0.46	0.71	"005	8.2E4X
gre_185	18E2	T0E3	"03	"06	"0Γ4	9.8E3X-
harvard500	^03E3	T6E3	"OI	"00	"064	1.2E3X
cell1	^OE4	3.0E4	"OI	"037	TΓ0	1.0E5X
pesa	TΓ2E4	T0E4	"027	"03I	T80	5.3E8X-
big	TT3E4	"OE5	"027	"049	!Σ86	4.1EnX-
gre_1107	TΓE3	T6E3	"026	"039	"024	38X
wordnet3	^08E5	T3E5	"04	"084	3000	^12X
p2p-Gnutella31	^OE5	T5E5	"05	"043	!T90	-6X
p2p-Gnutella05	T8E3	T2E4	"023	"065	-27:59	35X
mathworks100	1.0E2	5.5E2	0.20	0.50	0.04	30X
0.1
0.04
0.02
⑥ 0.08
.自 0.06
∣∙S<wided∣∕∣∙¾∣(%)
Figure 2: Runtime scalability for gre_1107 (left), big (middle), gre_115 (right)
(a)	Generate an initial subgraph S from the original directed graph in O(m log n) or O(m +
n log n) time;
(b)	Compute the approximate dominant eigenvector ht and the spectral sensitivity of each
off-subgraph edge in O(m) time;
(c)	Recover a small amount of spectrally-dissimilar off-subgraph edges into the latest subgraph
S according to their spectral sensitivities and similarities in O(m) time;
(d)	Repeat steps (b) and (c) until the desired condition number or spectral similarity is achieved.
5 Experimental Results
The proposed algorithm for spectral sparsification of directed graphs has been implemented using
MATLAB and C++. Extensive experiments have been conducted to evaluate the proposed method
with various types of directed graphs obtained from public-domain data sets (Davis & Hu, 2011).
Table 2 shows comprehensive results on directed graph spectral sparsification for a variety of real-
world directed graphs using the proposed method, where |VG|(|EG|) denotes the number of nodes
(edges) for the original directed graph G; |ES0 | and |ES| denote the numbers of edges in the initial
subgraph S0 and final spectral sparsifier S. Notice that we will directly apply the Matlab’s “eigs"
function if the size of the graph is relatively small (|ES0 | < 1E4); otherwise we will switch to the
LAMG solver for better efficiency when calculating the approximate generalized eigenvector ht . We
report the total runtime for the eigsolver using either the LAMG solver or "eigs” function. ` λ1
λ1,f in
denotes the reduction rate of the largest generalized eigenvalue of LS+ LGu .
Figure 2 shows the runtime scalability regarding to the number of off-subgraph edges (|Eadded |)
added in the final sparsifier for graph gre_1107 (left), big (middle) and gre_115 (right). As we can
see, the runtime scales linearly with the added number of edges for all three graphs.
Since there are no other existing directed graph sparsification methods to be compared, we compare
our proposed method with the existing undirected graph sparsification solver GRASS (Feng, 2016;
2018). To make sure the directed graphs can be applied to the GRASS, we first convert the directed
graphs into undirected ones (G0u) using A + A> symmetrization. Then undirected graph sparsifiers
Su0 will be generated by GRASS. Finally, the final directed graph sparsifiers can be formed by adding
the edge directions to the obtained undirected sparsifiers Su0 . The experimental results are shown in
Table 3, where κ represents the relative condition number between the original graph and its final
8
Under review as a conference paper at ICLR 2021
Table 3: Comparison of spectral SParsification results between the proposed method and GRASS.
Test cases	GRASS				Our method	
	IESu I 1EG0"	K(LGu ,LSU )	IEs I IEG∖	k(Lg, LS)	ESI ∖EG∖	k(Lg, LS)
gre_115	0.92	28	0.44	3760	0.43	522
gre_185	0.67	25	0.40	1140	0.41	170
gre_1107	0.86	9	0.43	2790	0.43	147
harvard500	0.36	13	0.39	5.22E5	0.36	2.40E4
p2p-Gnutella05	0.56	6	0.57	3.90E5	0.54	1.90E5
p2p-Gnutella31	0.68	3	0.68	1.0E5	0.68	8.20E4
big	0.60	7	0.60	8803	0.60	270
wordnet3	0.78	8	0.79	3.06E4	0.78	2.74E3
Table 4: Comparison of GMRES results.
Test cases	relres	No preconditioner	ILU preconditioner		Spar. preconditioner		
		iter	iter	nnz	iter	nnz	k(Lg,Ls )
-gre_115-	1E-7	64	22	536	177~	^^36	16
gre_185	1E-7	53	27	1,190	23	768	19
gre_1107	1E-7	118	24	6,771	22	4,327	18
harvard500	1E-7	80	25	3,563	18	2,527	20
big	1E-7	356	163	104,674	64	72,509	108
peta	1E-7	NC	241	91,304	85	67,007	28
sparsifier. By keeping the similar number of edges in the sparsifiers, we can observe that our method
can achieve much better sparsifiers than GRASS does.
More results regarding the directed graph sparsification are shown in Appendix.
The directed graph sparsifier can also be directly utilized as the preconditioner for directed Laplacian
solver when solving the linear system equation Lx = b with iterative solvers such as generalized
minimal residual method (GMRES) (Saad & Schultz, 1986). Table 4 shows comprehensive results
for GMRES iterations when no preconditioner is applied, Incomplete LU factorization (ILU) as the
preconditioner is applied, and the directed sparsifier Laplacian LS as the preconditioner is applied.
The MATLAB functions gmres and ilu with default setting are applied in our experiments.
“relres" is the relative residual to be achieve for three methods, “iter" is GMRES iteration number,
“nnz" is the number of non-zeros in the preconditioner, and “NC" represents it did not converge
when reaching maximum number of iterations (which is 500 in the experiments). We can conclude
that GMRES with directed graph sparsifier as the preconditioner has better convergence rate when
comparing to the other two methods. Meanwhile, the sparsifier has much fewer number of non-zeros
than ILU preconditioner. More results on directed graph Laplacian solver can be found in Appendix.
In the end, we also demonstrate the applications of our proposed directed graph sparsification
in solving directed graph Laplacians using both direct method and iterative method, as well as
the applications in computing (Personalized) PageRank vectors and directed graph partitioning in
Appendix.
6 Conclusions
For the first time, this paper proves the existence of linear-sized spectral sparsifiers for general directed
graphs, and proposes a practically-efficient yet unified spectral graph sparsification framework.
Such a novel spectral sparsification approach allows sparsifying real-world, large-scale directed and
undirected graphs with guaranteed preservation of the original graph spectral properties. By exploiting
a highly-scalable (nearly-linear complexity) spectral matrix perturbation analysis framework for
constructing nearly-linear sized (directed) subgraphs, it enables us to well preserve the key eigenvalues
and eigenvectors of the original (directed) graph Laplacians. The proposed method has been validated
using various kinds of directed graphs obtained from public domain sparse matrix collections, showing
promising spectral sparsification and partitioning results for general directed graphs.
9
Under review as a conference paper at ICLR 2021
References
Ittai Abraham and Ofer Neiman. Using petal-decompositions to build a low stretch spanning tree.
In Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC), pp.
395-406. ACM, 2012.
Joshua Batson, Daniel Spielman, and Nikhil Srivastava. Twice-Ramanujan Sparsifiers. SIAM Journal
on Computing, 41(6):1704-1721, 2012.
Andrgs A Benczur and David R Karger. Approximating st minimum cuts in G (n 2) time. In
Proceedings of the twenty-eighth annual ACM symposium on Theory of computing (STOC), pp.
47-55. ACM, 1996.
Silvere Bonnabel and Rodolphe Sepulchre. Riemannian metric and geometric mean for positive
semidefinite matrices of fixed rank. SIAM Journal on Matrix Analysis and Applications, 31(3):
1055-1070, 2010.
W. Briggs. A multigrid tutorial. SIAM Press, 1987.
P. Christiano, J. Kelner, A. Madry, D. Spielman, and S. Teng. Electrical flows, laplacian systems, and
faster approximation of maximum flow in undirected graphs. In Proc. ACM STOC, pp. 273-282,
2011.
Fan Chung. Laplacians and the cheeger inequality for directed graphs. Annals of Combinatorics, 9
(1):1-19, 2005.
Michael B Cohen, Jonathan Kelner, John Peebles, Richard Peng, Anup B Rao, Aaron Sidford, and
Adrian Vladu. Almost-linear-time algorithms for markov chains and new spectral primitives
for directed graphs. In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of
Computing, pp. 410-419. ACM, 2017.
Michael B. Cohen, Jonathan Kelner, Rasmus Kyng, John Peebles, Richard Peng, Anup B. Rao, and
Aaron Sidford. Solving Directed Laplacian Systems in Nearly-Linear Time through Sparse LU
Factorizations. In Foundations of Computer Science (FOCS), 2018 59st Annual IEEE Symposium
on, pp. 898-909. IEEE, 2018.
T. Davis and Y. Hu. The university of florida sparse matrix collection. ACM Trans. on Math. Soft.
(TOMS), 38(1):1, 2011.
Michael Elkin, Yuval Emek, Daniel A Spielman, and Shang-Hua Teng. Lower-stretch spanning trees.
SIAM Journal on Computing, 38(2):608-628, 2008.
Zhuo Feng. Spectral graph sparsification in nearly-linear time leveraging efficient spectral perturbation
analysis. In Proceedings of the 53rd Annual Design Automation Conference, pp. 57. ACM, 2016.
Zhuo Feng. Similarity-aware spectral sparsification by edge filtering. In Design Automation Confer-
ence (DAC), 2018 55nd ACM/EDAC/IEEE. IEEE, 2018.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.
Jonathan A Kelner, Yin Tat Lee, Lorenzo Orecchia, and Aaron Sidford. An almost-linear-time
algorithm for approximate max flow in undirected graphs, and its multicommodity generalizations.
In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete algorithms, pp.
217-226. SIAM, 2014.
Pavel Kolev and Kurt Mehlhorn. A note on spectral clustering. arXiv preprint arXiv:1509.09188,
2015.
I. Koutis, G. Miller, and R. Peng. Approaching Optimality for Solving SDD Linear Systems. In Proc.
IEEE FOCS, pp. 235-244, 2010.
Yin Tat Lee and He Sun. An SDP-based Algorithm for Linear-sized Spectral Sparsification. In
Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017,
pp. 678-687, New York, NY, USA, 2017. ACM. ISBN 978-1-4503-4528-6. doi: 10.1145/3055399.
3055477. URL http://doi.acm.org/10.1145/3055399.3055477.
10
Under review as a conference paper at ICLR 2021
Lek-Heng Lim, Rodolphe Sepulchre, and Ke Ye. Geometric distance between positive definite
matrices of different dimensions. IEEE Transactions on Information Theory, 65(9):5401-5405,
2019.
O. Livne and A. Brandt. Lean algebraic multigrid (LAMG): Fast graph Laplacian linear solver. SIAM
Journal on Scientific Computing, 34(4):B499-B522, 2012.
Fragkiskos D Malliaros and Michalis Vazirgiannis. Clustering and community detection in directed
networks: A survey. Physics Reports, 533(4):95-142, 2013.
Giovanni De Micheli. Synthesis and optimization of digital circuits. McGraw-Hill Higher Education,
1994.
Amir Hossein Nodehi Sabet, Junqiao Qiu, and Zhijia Zhao. Tigr: Transforming irregular graphs for
gpu-friendly graph processing. In Proceedings of the Twenty-Third International Conference on
Architectural Support for Programming Languages and Operating Systems, pp. 622-636. ACM,
2018.
Richard Peng, He Sun, and Luca Zanetti. Partitioning well-clustered graphs: Spectral clustering
works. In Proceedings of The 28th Conference on Learning Theory (COLT), pp. 1423-1455, 2015.
Youcef Saad and Martin H Schultz. Gmres: A generalized minimal residual algorithm for solving
nonsymmetric linear systems. SIAM Journal on scientific and statistical computing, 7(3):856-869,
1986.
Venu Satuluri and Srinivasan Parthasarathy. Symmetrizations for clustering directed graphs. In
Proceedings of the 14th International Conference on Extending Database Technology, pp. 343-354.
ACM, 2011.
D. Spielman and S. Teng. Nearly linear time algorithms for preconditioning and solving symmetric,
diagonally dominant linear systems. SIAM Journal on Matrix Analysis and Applications, 35(3):
835-885, 2014.
D. Spielman and Shanghua Teng. Spectral partitioning works: Planar graphs and finite element
meshes. In Foundations of Computer Science (FOCS), 1996. Proceedings., 37th Annual Symposium
on, pp. 96-105. IEEE, 1996.
Daniel Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM Journal
on Computing, 40(6):1913-1926, 2011.
Daniel Spielman and ShangHua Teng. Spectral sparsification of graphs. SIAM Journal on Computing,
40(4):981-1025, 2011.
11
Under review as a conference paper at ICLR 2021
Figure 3: Edge coupling during directed Laplacian symmetrization.
7 Appendix
7.1	Proof of Theorem 3.2
Proof. Each element (i, j) in LGu can be written as follows:
LGu (i, j)
DG2(i, i) + Pk AG2(k, i)
Pk (-AG(k, i)AG(k, j) + AG(k, i)DG(k, j) + DG(k, i)AG(k, j))
i=j
i 6= j.
(20)
It can be shown that the following is always true:
LGu (i, i) +	LGu(i,j) =	LG(i, k)LG(i, k) +	LG(j, k)LG(i, k)
j,j 6=i	k	j,j 6=i k
/	ʌ	(21)
=ELG(Mk) I LG(i,k) + E LG(j, k) I = 0,
k	j,j 6=i
which indicates the all-one vector is the null space of LGu. For directed graphs, it can be shown
that if a node has more than one outgoing edge, in the worst case the neighboring nodes pointed by
such outgoing edges will form a clique possibly with negative edge weights in the corresponding
undirected graph after symmetrization.
As an example shown in Figure 3, when edge e2 is added into the initial graph G that includes a
single edge e1, an extra edge (shown in red dashed line) coupling with e1 will be created in the
resultant undirected graph Gu ; similarly, when an edge e3 is further added, two extra edges coupling
with e1 and e2 will be created in Gu . When the last edge e4 is added, It forms a clique. Also, it can
be shown that Gu will contain negative edge weights under the following condition:
X (AG(k, i)DG(k, j) + DG(i, k)AG(j, k)) >XAG(k,i)AG(k,j).	(22)
In some cases, there may exist no clique even though all outgoing edges according to one node are
added into subgraph only because the weights of these edges satisfy
X(AG(k, i)DG(k,j) + DG(i, k)AG(j, k)) = X AG(k, i)AG(k, j).	(23)
kk
□
7.2 Existence of linear-sized spectral sparsifier for directed graphs
Existing spectral sparsification methods for undirected graphs (Spielman & Teng, 2011; Batson et al.,
2012; Feng, 2016) can not handle undirected graphs with negative-weighted edges. So we have to
first prove the existence of spectral sparsifier for undirected graph with negative-weighted edges, or
the existence of linear-sized spectral sparsifier for directed graphs .
12
Under review as a conference paper at ICLR 2021
	一 1	0	-1		一 1	0	-1
B =	0	-1	1	C =	0	-1	1
	0	1	-1		0	1	-1
	-1	1	0		-1	1	0
W
「1 0 0 0-
0 10 0
0 0 10
0 0 0 1
LG = B>WC
一1	-1	0-
0	2	-1
-1	-1	1
Figure 4: Forming a directed Laplacian with B and C matrices.
Theorem 7.1. FOra given directed graph G and its undirected graph Gu = (V, EGu, wgu ) obtained
via Laplacian Symmetrization, there exists a (1 + e)-spectral sparsifier S with O(n∕e2) edges such
that its undirected graph Su = (V, ESU, WSU) after SymmetrizatiOn satisfies the following COnditiOn
for any X ∈ Rn:
(1 - e)x>LGux ≤ x>LSux ≤(1 + e)x>LGux.	(24)
Before proving Theorem 7.1 for symmetrized undirected graph Gu with negative edge weight, we
need to introduce the following lemma 7.2 (Batson et al., 2012).
Lemma 7.2. Let d > 0, and u1, u2, ..., um denote a set of vectors in Rn that allow expressing the
identity decomposition as:
X uiui> = idRn ,	(25)
1≤i≤m
where idRn ∈ Rn×n denotes the identity matrix. Then there exists a series of non-negative coefficients
{ti }im=1 with |i : ti 6= 0| ≤ dn so that
X> idRn X ≤ X tiX>uiui>X ≤(1 + e)X>idRnX. ∀X ∈ Rn	(26)
Proof. Lemma (7.2) proves the existence of the sparsifier for an undirected graph with positive
weight edges. We have to extend the above lemma to prove the existence of the sparsifier for a
symmetrized undirected graph with negative weight edges. The key of our approach is to construct a
set of vectors uι,…，Um in Rn such that Ui can be expressed as an identity decomposition (25).
To construct ui, the Laplacian of an undirected graph can be written as LG = B>WB (Spielman &
Srivastava, 2011), where Bm×n is the signed edge-vertex incidence matrix:
B(i,v)= [-1
10
if v is i-th edge’s head
if v is i-th edge’s tail
otherwise .
(27)
Wm×m is the diagonal matrix with W(i, i) = wi. The Laplacian matrix of a directed graph can be
written as LG = B>WC, where Cm×n is a injection matrix defined as:
C(i, v) = [0
10
if v is i-th edge’s head
if v is i-th edge’s tail
otherwise.
(28)
13
Under review as a conference paper at ICLR 2021
Figure 4 shows an example for constructing a directed Laplacian matrix based on B and C matrices.
In the following, we show how to construct the vectors ui. The undirected Laplacian after symmetriza-
tion can be written as LGu = B>WoB with Wo = WCC>W. Since LGu and its Pseudoinverse
L+G can be written as
mm
LGu = Xλ0u0uj>,	LGu = X ^u0uj>,	(29)
j=1	j=1 j
it can be shown that LGu L+G = Pjm=1 u0ju0j> = idLGu , where idLGu is the identity on
im(LGu) = ker(LGu)>. Consequently, Un×m matrix with ui for i = 1, ..., m as its column
vectors can be constructed as
Un×m = [u1,...,um] =L+G/u2B>Wo1/2.	(30)
It can be shown that Un×m will satisfy the following equation:
Un×mUn×m> =	uiui> = LGu+/2B>WoBLGu+T/2
=LGu+/2LGuLGu+T/2=idLGu
(31)
According to Lemma 7.2, we can always construct a diagonal matrix T ∈ Rm×m with ti as its i-th
diagonal element. Then there will be at most O(n/2) positive diagonal elements in T, which allows
constructing LSu = B>Wo1/2TWo1/2B that corresponds to the directed subgraph S for achieving
(1 + )-spectral approximation of G as required by (24). It can be shown that each ui with a
nonzero ti coefficient corresponds to the outgoing edges pointed by the same node. Consequently,
for directed graphs with bounded degrees, there will be O(n/2) total number of directed edges in
the (1 + e)-spectral SParsifier S.	□
7.3	Algorithm flow
Algorithm 2 Edge_Similarities_Checking
Input: Elist, LG, LS, dout, %
1:	Perform t-step power iterations with r = O(log n) initial random vectors h(01) , ..., h(0r) to compute r
approximate dominant generalized eigenvectors h(t1) , ..., h(tr) ;
2:	Compute a r-dimensional embedding vector Tpi,qi ∈ Rr for ∀(pi, qi) ∈ Elist;
3:	let Eaddlist = [(p1, q1)];
4:	for i=2:|Elist | do
5:	Calculate the spectral similarity score βi,j between (pi, qi) and every edge (pj, qj) in Eaddlist;
6:	if 1 - βi,j < %, for ∀(pj, qj) ∈ Eaddlist then
7:	Eaddlist = [Eaddlist; (pi , qi )];
8:	end if
9:	end for
10:	Return Eaddlist ;
7.4	Results of directed graph sparsification
Figure 5 shows the spectral sensitivities of all the off-subgraph edges (e2 to e19 represented with
blue color) in both directed and undirected graphs calculated using MATLAB’s “eigs" function and
the proposed method based on (17) using the LAMG solver, respectively. Meanwhile, the spectral
sensitivities of all the off-subgraph edges (e2 to e19) with respect to the dominant eigenvalues (λmax
or λ1) in both directed and undirected graphs are plotted. We observe that spectral sensitivities for
directed and undirected graphs are drastically different from each other. The reason is that the spectral
sensitivities for off-subgraph edges in the directed graph depend on the edge directions. It is also
observed that the approximate spectral sensitivities calculated by the proposed t-step power iterations
with the LAMG solver match the true solution very well for both directed and undirected graphs.
14
Under review as a conference paper at ICLR 2021
4
2
0
0
A=>-=suθs -p.llɔəds
5	10	15
Edge Index
20
5	10	15
Edge Index
20
2 10
Ooo
Ill
əle- uo-lɔnpə:ʤ
əle- u04□npωJXDEn
Figure 5: The spectral sensitivity scores of off-subgraph edges (e2 to e19 in blue) for the undirected
(left) and directed graph (right).
Figure 6: Largest generalized eigenvalue reduction rates for “gre_115" (left) and “pesa" (right).
We plot the detailed reduction rates of the largest generalized eigenvalue when adding different
number of off-subgraph edges to the sparsifiers of graph “gre_115" and “peta" in Figure 6. It shows
that the largest generalized eigenvalue can be effectively reduced if sufficient off-subgraph edges are
included into the sparsifier.
7.5	Applications in developing directed Laplacian solver
Consider the solution of the following linear systems of equations:
Lx = b.	(32)
Recent research has been focused on more efficiently solving the above problem when L is a Laplacian
matrix of an undirected graph (Kelner et al., 2014; Koutis et al., 2010). In this work, we will mainly
focus on solving nonsymmetric Laplacian matrices that correspond to directed graphs.
7.5.1	Direct method for directed Laplacian solver
Lemma 7.3. When solving (32), the right preconditioning system is applied, leading to the following
alternative linear system of equations:
LGuy = b,	(33)
where vector b will lie in the left singular vector space. When the solution of (33) is obtained, the
solution of (32) is given by L>Gy = x.
It is obvious that solving the above equation is equivalent to solving the problem of LGL>GL+G>x = b.
In addition, LGu is a Laplacian matrix of an undirected graph that can be much denser than LG .
Therefore, we propose to solve the linear system of LSUy = b instead to effectively approximate
(33) since GSu is sparser than GGu and more efficient to solve in practice.
We analyze the solution errors based on the generalized eignvalue problem of LGU and LSU . We
have VLGU V> = λ and VLSU V> = I, where V = [v1, v2, ..vn], λ is the diagonal matrix with
15
Under review as a conference paper at ICLR 2021
-EnP-S①」①>_】-①年
O
-4 6 8 0
U
IIIIo
-EnP-S①」①>E-①便
-6-NO preconditioner
ILU PreConditioner (nnz=104,674)
-沫一-Spar. preconditioner (nnz=72,509 )
O
5
O
O
2
Q

Number of iterations	Number of iterations
(a)	(b)
Figure 7: GMRES convergence results for graphs of (a) pesa and (b) big
its generalized eigenvalues λi ≥ 1 on its diagonal. Since the errors can be calculated from the
following procedure:
LGUy - LSUy = LGU (y - y) + (LGU - LSU )y = 0,	(34)
we can write the error term as follows:
(y - y) = LGU (LGU - LSU )y.	(35)
Since y = Pi aiVi, the error can be further expressed as
(y - y) = X ai(1 - λ1 )vi.	(36)
i
Therefore, the error term (36) can be generally considered as a combination of high-frequency errors
(generalized eigenvectors with respect to high generalized eigenvalues) and low-frequency errors
(generalized eigenvectors with respect to low generalized eigenvalues). After applying GS relaxations,
the high-frequency error terms can be efficiently removed (smoothed), while the low-frequency errors
tend to become zero if the generalized eigenvalues approach 1 considering (1 - ±) tends to be
approaching zero. As a result, the error can be effectively eliminated using the above solution
smoothing procedure.
In summary, in the proposed directed Laplacian solver, the following steps are needed:
(a)	We will first extract a spectral sparsifier LS of a given (un)directed graph LG. Then, itis pos-
sible to compute an approximate solution by exploiting its spectral sparsifier LSu = LSLS>
via solving yy = LS+ b instead.
(b)	Then we improve the approximate solution yy by getting rid of the high-frequency errors via
applying a few steps ofGS iterations (Briggs, 1987).
(C) The final solution is obtained from x = L>Gyy.
Table 5: Relative errors between exact and approximate solutions of LGX = b w/ or w/o smoothing
Test Cases	gre_115	gre_185	cell1	pesa	big	gre_1107	wordnet3
w/o smooth.	0.41	0.42	^044	2.1E-4	4.3E-3	-0.6-	0.72
w/ smooth.	0.04	0.12	0.07	8.0E-9	1.1E-4	0.10	0.07
Table 5 shows the results of the directed Laplacian solver on different directed graphs. It reports
relative errors between the exact solution and the solution calculated by the proposed solver with
and without smoothing. It shows that errors can be dramatically reduced after smoothing, and our
proposed solver can well approximate the true solution of LGX = b
16
Under review as a conference paper at ICLR 2021
Figure 8: The correlation of PageRank between itself and its sparsifier for graph ibm_32 (left),
mathworks_100 (middle) and gre_1107 (right) after smoothing
7.5.2 Iterative method for directed Laplacian solver
Figure 7 shows the relative residual plot (versus GMRES iteration number) when no preconditioner
is applied, Incomplete LU factorization (ILU) as the preconditioner is applied, and the directed
sparsifier Laplacian LS as the preconditioner is applied for graph pesa and big. We can conclude that
GMRES with directed sparsifiers as preconditioners has faster convergence rate than the other two
methods. It is also observed that the number of nonzeros (nnz) in the preconditioner matrix created
by the directed sparsifier is the lowest.
7.6	Applications in computing (Personalized) PageRank vectors
The idea of PageRank is to give a measurement of the importance for each web page. For example,
PageRank algorithm aims to find the most popular web pages, while the personalized PageRank
algorithm aims to find the pages that users will most likely to visit. To state it mathematically, the
PageRank vector p satisfies the following equation:
p = A>GD-G1p,	(37)
where p is also the eigenvector of A>GD-G1 that corresponds to the eigenvalue equal to 1. Meanwhile,
p represents the stable distribution of random walks on graph G. However, D-G1 can not be defined
if there exists nodes that have no outgoing edges. To deal with such situation, a self-loop with a small
edge weight can be added for each node.
The stable distributions of (un)directed graphs may not be unique. For example, the undirected graphs
that have multiple strongly-connected components, or the directed graphs that have nodes without
any outgoing edges, may have non-unique distributions. In addition, it may take very long time for a
random walk to converge to a stable distribution on a given (un)directed graph.
To avoid such situation in PageRank, a jumping factor α that describes the possibility at α to jump to
a uniform vector can be added, which is shown as follows:
P =(1- α)AGDg1P + α 1,	(38)
n
P = α(I-(I- α)AG>DG1)-11,	(39)
n
where α ∈ [0, 1] is a jumping constant. After applying Taylor expansions, we can obtain that
P = n X ((1-α)AGDG1)i.	(40)
i
By setting the proper value of α (e.g., α = 0.15), the term (1 - α)i will be quickly reduced with
increasing i. Instead of starting with a uniform vector α 1, a nonuniform personalization vector pr
can be applied:
P = (1 - α)A>GD-G1P + αPr.	(41)
Figure 8 shows the application of the proposed directed graph sparsification for computing PageRank
vectors, where the correlation of PageRank results using the original graphs (x-axis) and sparsifiers
(y-axis) are plotted for graph ibm_32 (left), mathworks_100 (middle) and gre_1107 (right). Note
17
Under review as a conference paper at ICLR 2021
Figure 10: Spectral partitioning of directed (left) and undirected graphs (right). The nodes within the
same cluster are assigned the same color.
Figure 9: The correlation of personalized PageRank between itself and its sparsifier for graph ibm_32
(left), mathworks_100 (middle) and gre_1107 (right) after smoothing
that a few steps of Gauss-Seidel smoothing have been applied to remove the high-frequency errors
to obtain the smoothed PageRank vectors when using the sparsified graphs. We observe that the
PageRank vectors obtained from sparsifiers can well approximate the results computed with the
original graphs.
Similar to the results of PageRank in Figure 8, Figure 9 shows the application of the proposed directed
graph sparsification on the personalized PageRank, where the correlations of personalized PageRank
results using the original graphs (x-axis) and sparsifiers (y-axis) are plotted for graph ibm_32 (left),
mathworks_100 (middle) and gre_1107 (right). Gauss-Seidel smoothing are also applied when using
the sparsified graphs. We can observe that personalized PageRank vectors from sparsifiers match
very well with the ones generated from original graphs, which demonstrates the effectiveness of the
sparsifiers on the Personalized PageRank application.
7.7	Applications in directed graph partitioning
It has been shown that partitioning and clustering of directed graphs can play very significant roles in
a variety of applications related to machine learning (Malliaros & Vazirgiannis, 2013), data mining
and circuit synthesis and optimization (Micheli, 1994), etc. However, the efficiency of existing
methods for partitioning directed graphs strongly depends on the complexity of the underlying graphs
(Malliaros & Vazirgiannis, 2013).
Cλ" ,λ5 ,λ6 ,λ%	λ' ,λ!(	λ!!
λ.
Illll	Ir
λ), λ*	%
Figure 11: Eigenvalues distribution of LGU for the directed graph in Figure 10
In this work, we propose a spectral method for directed graph partitioning problems. For an undirected
graph, the eigenvectors corresponding to the first few smallest eigenvalues can be utilized for the
spectral partitioning purpose (Spielman & Teng, 1996). For a directed graph G on the other hand,
the left singular vectors of Laplacian LG will be required for directed graph partitioning. The
18
Under review as a conference paper at ICLR 2021
eigen-decomposition of its symmetrization LGU can be wirtten as
LGU = Xλivivi>,	(42)
i
where 0 = λ1 ≤ ...λk and v1, ..., vk, with k ≤ n denote the Laplacian eigenvalues and eigenvectors,
respectively. There may not be n eigenvalues when there are some nodes without any outgoing edges.
In addition, the spectral properties of LGU are more complicated since the eigenvalues always have
multiplicity (either algebraic or geometric multiplicities). For example, the eigenvalues according
to the symmetrization of the directed graph in Figure 10 have a a few multiplicities: λ2 = λ3 ,
λ4 = λ5 = λ6 = λ7, λ9 = λ10, as shown in Figure 11.
Therefore, we propose to exploit the eigenvectors (left singular vectors of directed Laplacian)
corresponding to the first few different eigenvalues (singular values of directed Laplacian) for directed
graph partitioning. For example, the partitioning result of the directed graph in Figure 10 will depend
on the eigenvectors of v1 , v2 , v4, v8 that correspond to eigenvalues of λ1 , λ2, λ4, λ8. As shown in
Figure 10, the spectral partitioning results can be quite different between the directed and undirected
graph with the same set of nodes and edges.
In general, it is possible to first extract a spectrally-similar directed graph before any of the prior
partitioning algorithms are applied. Since the proposed spectral sparsification algorithm can well
preserve the structural (global) properties of the original graphs, the partitioning results obtained
from the sparsified graphs will be very similar to the original ones.
Figure 12 shows the spectral graph partitioning results on the symmetrized graph Gu of original
directed graph and its symmetrized sparsifier Su . As observed, very similar partitioning results have
been obtained, indicating well preserved spectral properties within the spectrally-sparsified directed
graph.
Figure 12: The partitioning results between Gu (left) and its SParSifier Su (right) for the 'ibm32.mtx'
graph.
19