Under review as a conference paper at ICLR 2021
Symmetric Wasserstein Autoencoders
Anonymous authors
Paper under double-blind review
Ab stract
Leveraging the framework of Optimal Transport, we introduce a new family of
generative autoencoders with a learnable prior, called Symmetric Wasserstein Au-
toencoders (SWAEs). We propose to symmetrically match the joint distributions
of the observed data and the latent representation induced by the encoder and the
decoder. The resulting algorithm jointly optimizes the modelling losses in both the
data and the latent spaces with the loss in the data space leading to the denoising
effect. With the symmetric treatment of the data and the latent representation, the
algorithm implicitly preserves the local structure of the data in the latent space. To
further improve the latent representation, we incorporate a reconstruction loss into
the objective, which significantly benefits both the generation and reconstruction.
We empirically show the superior performance of SWAEs over the state-of-the-art
generative autoencoders in terms of classification, reconstruction, and generation.
1 Introduction
Deep generative models have emerged as powerful frameworks for modelling complex data. Widely
used families of such models include Generative Adversarial Networks (GANs) (Goodfellow et al.,
2014), Variational Autoencoders (VAEs) (Rezende et al., 2014; Kingma & Welling, 2014), and
autoregressive models (Uria et al., 2013; Van Oord et al., 2016). The VAE-based framework has been
popular as it yields a bidirectional mapping, i.e., it consists of both an inference model (from data to
latent space) and a generative model (from latent to data space). With an inference mechanism VAEs
can provide a useful latent representation that captures salient information about the observed data.
Such latent representation can in turn benefit downstream tasks such as clustering, classification, and
data generation. In particular, the VAE-based approaches have achieved impressive performance
results on challenging real-world applications, including image synthesizing (Razavi et al., 2019),
natural text generation (Hu et al., 2017), and neural machine translation (Sutskever et al., 2014).
VAEs aim to maximize a tractable variational lower bound on the log-likelihood of the observed
data, commonly called the ELBO. Since VAEs focus on modelling the marginal likelihood of the
data instead of the joint likelihood of the data and the latent representation, the quality of the latent
is not well assessed (Alemi et al., 2017; Zhao et al., 2019), which is undesirable for learning useful
representation. Besides the perspective of maximum-likelihood learning of the data, the objective
of VAEs is equivalent to minimizing the KL divergence between the encoding and the decoding
distributions, with the former modelling the joint distribution of the observed data and the latent
representation induced by the encoder and the latter modelling the corresponding joint distribution
induced by the decoder. Such connection has been revealed in several recent works (Livne et al.,
2019; Esmaeili et al., 2019; Pu et al., 2017b; Chen et al., 2018). Due to the asymmetry of the
KL divergence, it is highly likely that the generated samples are of a low probability in the data
distribution, which often leads to unrealistic generated samples (Li et al., 2017b; Alemi et al., 2017).
A lot of work has proposed to improve VAEs from different perspectives. For example, to enhance
the latent expressive power VampPrior (Tomczak & Welling, 2018), normalizing flow (Rezende &
Mohamed, 2015), and Stein VAEs (Pu et al., 2017a) replace the Gaussian distribution imposed on
the latent variables with a more sophisticated and flexible distribution. However, these methods are
all based on the objective of VAEs, which therefore are unable to alleviate the limitation of VAEs
induced by the objective. To improve the latent representation (Zhao et al., 2019) explicitly includes
the mutual information between the data and the latent into the objective. Moreover, to address the
asymmetry of the KL divergence in VAEs (Livne et al., 2019; Chen et al., 2018; Pu et al., 2017b)
leverage a symmetric divergence measure between the encoding and the decoding distributions.
1
Under review as a conference paper at ICLR 2021
Nevertheless, these methods typically involve a sophisticated objective function that either depends
on unstable adversarial training or challenging approximation of the mutual information.
In this paper, We leverage Optimal Transport (OT) (Villani, 2008; Peyre et al., 2019) to Symmetri-
cally match the encoding and the decoding distributions. The OT optimization is generally challeng-
ing particularly in high dimension, and We address this difficulty by transforming the OT cost into
a simpler form amenable to efficient numerical implementation. OWing to the symmetric treatment
of the observed data and the latent representation, the local structure of the data can be implicitly
preserved in the latent space. HoWever, We found that With the symmetric treatment only the per-
formance of the generative model may not be satisfactory. To improve the generative model We
additionally include a reconstruction loss into the objective, Which is shoWn to significantly benefit
the quality of the generation and reconstruction.
Our contributions can be summarized as folloWs. Firstly, We propose a neW family of generative au-
toencoders, called Symmetric Wasserstein Autoencoders (SWAEs). Secondly, We adopt a learnable
latent prior, parameterized as a mixture of the conditional priors given the learnable pseudo-inputs,
Which prevents SWAEs from over-regularizing the latent variables. Thirdly, We empirically perform
an ablation study of SWAEs in terms of the KNN classification, denoising, reconstruction, and sam-
ple generation. Finally, We empirically verify, using benchmark tasks, the superior performance of
SWAEs over several state-of-the-art generative autoencoders.
2 S ymmetric Wasserstein Autoencoders
In this section We introduce a neW family of generative autoencoders, called Symmetric Wasserstein
Autoencoders (SWAEs).
2.1 OT Formulation
Denote the random vector at the encoder as e , (xe, ze) ∈ X ×Z, Which contains both the observed
data Xe ∈ X and the latent representation Ze ∈ Z. We call the distribution p(e)〜P(Xe)p(ze∣Xe)
the encoding distribution, Where p(xe) represents the data distribution and p(ze|xe) characterizes
an inference model. Similarly, denote the random vector at the decoder as d , (Xd, zd) ∈ X × Z,
Which consists of both the latent prior zd ∈ Z and the generated data Xd ∈ X . We call the distribu-
tion p(d)〜p(zd)p(xd∣Zd) the decoding distribution, where p(zd) represents the prior distribution
and p(Xd |zd ) characterizes a generative model. The objective of VAEs is equivalent to minimizing
the (asymmetric) KL divergence between the encoding distribution p(e) and the decoding distri-
bution p(d) (see Appendix A.1). To address the limitation in VAEs, first we propose to treat the
data and the latent representation symmetrically instead of asymmetrically by minimizing the p-
th Wasserstein distance between p(e) and p(d) leveraging Optimal Transport (OT) (Villani, 2008;
Peyre et al., 2019).
OT provides a framework for comparing two distributions in a Lagrangian framework, which seeks
the minimum cost for transporting one distribution to another. We focus on the primal problem of
OT, and Kantorovich,s formulation (Peyre et al., 2019) is given by:
Wc(p(e), p(d))
inf
Γ ∈P(e~p(e),d-p(d))
E(e,d)〜Γ
c(e, d),
(1)
where P(e 〜P (e), d 〜p(d)), called the coupling between e and d, denotes the set of the joint
distributions of e and d with the marginals p(e) and p(d), respectively, and c(e, d) : (X, Z) ×
(X, Z) → [0, +∞] denotes the cost function. When ((X, Z) × (X, Z), d) is a metric space and
the cost function c(e, d) = dp(e, d) for P ≥ 1, Wp, the P-th root of Wc is defined as the P-th
Wasserstein distance. In particular, it can be proved that the P-th Wasserstein distance is a metric
hence symmetric, and metrizes the weak convergence (see, e.g., (Santambrogio, 2015)).
Optimization of equation 1 is computationally prohibitive especially in high dimension (Peyre et al.,
2019). To provide an efficient solution, we restrict to the deterministic encoder and decoder. Specif-
ically, at the encoder we have the latent representation ze = E(Xe) with the function E : X → Z,
and at the decoder we have the generated data Xd = D(zd) with the function D : Z → X. It
turns out that with the deterministic condition instead of searching for an optimal coupling in high
dimension, we can find a proper conditional distribution P(zd |Xe) with the marginalP(zd).
2
Under review as a conference paper at ICLR 2021
-10	12
(a) SWAE	(b) VAE
Figure 1: Latent representations of 100 GMM samples (mode 5 and dimension 10) with dim-z = 2.
The indexes of these latent representations are sorted based on the distance to a target sample in the
data space, i.e., Index 0 is associated with the target sample and Index 100 is associated with the
furthest sample to the target in the data space. With SWAE (left) data samples that are close in the
data space are also close in the latent space, while VAE (right) cannot preserve such correspondence.
Theorem 1 Given the deterministic encoder E : X → Z and the deterministic decoder D : Z →
X , the OT problem in equation 1 can be transformed to the following:
Wc(p(e),p(d)) = inf Ep(xe)Ep(zd|xe) c(e, d),	(2)
p(zd|xe)
where the observed data follows the distribution p(xe) and the prior follows the distribution p(zd).
The proof of Theorem 1 extends that of Theorem 1 in (Tolstikhin et al., 2018), and is provided in
Appendix A.2. If X × Z is the Euclidean space endowed with the Lp norm, then the expression in
equation 2 equals the following:
Wc(p(e),p(d)) =	inf Ep(xe)Ep(zd|xe) kxe - D(zd)kpp + kE(xe) -zdkpp,	(3)
p(zd|xe)
where in the objective we call the first term the x-loss and the second term the z-loss. With the above
transformation, we decompose the loss in the joint space into the losses in both the data and the latent
spaces. Such decomposition is crucial and allows us to treat the data and the latent representation
symmetrically.
The x-loss, i.e., kxe - D(zd)kpp, represents the discrepancy in the data space, and can be interpreted
from two different perspectives. Firstly, since D(zd) represents the generated data, the x-loss es-
sentially minimizes the dissimilarity between the observed data and the generated data. Secondly,
the x-loss is closely related to the objective of Denoising Autoencoders (DAs) (Vincent et al., 2008;
2010). In particular, DAs aim to minimize the discrepancy between the observed data and a partially
destroyed version of the observed data. The corrupted data can be obtained by means of a stochastic
mapping from the original data (e.g., via adding noises). By contrast, the x-loss can be explained
in the same way with the generated data being interpreted as the corrupted data. This is because
the prior zd in D(zd) is sampled from the conditional distribution p(zd|xe), which depends on the
observed data xe. Consequently, the generated data D(zd), obtained by feeding zd to the decoder,
is stochastically related to the observed data xe . With this insight, the same as the objective of DAs,
the x-loss can lead to the denoising effect.
The z-loss, i.e., kE(xe) - zdkpp, represents the discrepancy in the latent space. The whole objective
in equation 3 hence simultaneously minimizes the discrepancy in the data and the latent spaces.
Observe that in equation 3 E(xe) is the latent representation of xe at the encoder, while zd can be
thought of as the latent representation of D(zd) at the decoder. With such connection, the optimiza-
tion of equation 3 can preserve the local data structure in the latent space. More specifically, since
xe and D(zd) are stochastically dependent, roughly speaking, if two data samples are close to each
other in the data space, their corresponding latent representations are also expected to be close. This
is due to the symmetric treatment of the data and the latent representation. In Figure 1 we illustrate
this effect and compare SWAE with VAE.
3
Under review as a conference paper at ICLR 2021
Figure 2: Network architecture of SWAEs. To generate new data latent samples are first drawn from
the marginal prior p(zd) based on the conditional priors p(zd|uk), and then fed to the decoder.
Comparison with WAEs (Tolstikhin et al., 2018) The objective in equation 3 minimizes the OT cost
between the joint distributions of the data and the latent, i.e., Wc(p(e), p(d)), while the objective
of WAEs (Tolstikhin et al., 2018) minimizes the OT cost between the marginal distributions of the
data, i.e., Wc(p(xe), p(xd)), where p(xd) is the marginal data distribution induced by the decoding
distribution p(d). The problem of WAEs is first formulated as an optimization with the constraint
p(ze) = p(zd), where p(ze) is the marginal distribution induced by the encoding distribution p(e),
and then relaxed by adding a regularizer. With the deterministic decoder, the final optimization
problem of WAEs is as follows:
inf Ep(xe)Ep(ze|xe)	c(xe,D(ze)) + λD(p(ze), p(zd)),	(4)
p(ze |xe)
where D(, ) denotes some divergence measure. Comparing equation 4 to equation 3, we can see
that both methods decompose the loss into the losses in the data and the latent spaces. Differently,
in equation 4 the first term reflects the reconstruction loss in the data space and the second term
represents the distribution-based dissimilarity in the latent space; while in equation 3 the x-loss is
closely related to the denoising and the generation quality and the z-loss measures the sample-based
dissimilarity. Moreover, equation 4 is optimized over the posterior p(ze |xe) with a fixed prior p(zd),
while equation 3 is optimized over the conditional prior p(zd|xe) with a potentially learnable prior.
2.2	Improvement of latent representation
The objective in equation 3 only seeks to match the encoding and the decoding distributions. Besides
the encoder and the decoder structures, there is no explicit constraint on the correlation between the
data and the latent representation within each joint distribution. Lacking of such constraint typically
results in a low quality of reconstruction (Dumoulin et al., 2017; Li et al., 2017a). Therefore, we
incorporate a reconstruction-based loss into the objective associated with a controllable coefficient.
Additionally, since the dimension of the latent space is usually much smaller than that of the data
space, we associate a weighting parameter to balance these two types of losses. Overall, the objective
function can be represented as follows:
inf Ep(xe)Ep(zd|xe) βkxe - D(zd)kpp + (1 - β)kxe - D(ze)kpp +αkE(xe) -zdkpp,	(5)
p(zd |xe)
where kxe - D(ze)kpp denotes the reconstruction loss, and β(0 < β < 1) and α(α > 0) are the
weighting parameters. The weighting parameter β controls the trade-off between the x-loss and the
reconstruction loss, and a smaller value of β generally leads to better reconstruction. To achieve a
better trade-off between the generation and reconstruction β needs to be carefully chosen. We will
perform an ablation study of SWAEs and show the importance of including the reconstruction loss
into the objective for the generative model in Section 3.
2.3	Algorithm
Similar to many VAE-based generative models, we assume that the encoder, the decoder, and the
conditional prior are parameterized by deep neural networks. Unlike the canonical VAEs, where the
prior distribution is simple and given in advance, the proposed method adopts a learnable prior. The
4
Under review as a conference paper at ICLR 2021
benefits of a learnable prior, e.g., avoiding over-regularization and hence improving the quality of the
latent representation, have been revealed in several recent works (Hoffman & Johnson, 2016; Tom-
czak & Welling, 2018; Atanov et al., 2019; Klushyn et al., 2019). Obviously, the conditional prior is
related to the marginal prior via Exep(zd|xe) = p(zd). This indicates a way to design the prior as a
mixture of the conditional distributions, i.e., p*(zd) = * PN=I p(zd∣Xe,n), where Xe,ι,…,Xe,N
are the training samples. To avoid over-fitting, similar to (Tomczak & Welling, 2018), we replace
the training samples with learnable pseudo-inputs and parameterize the prior distribution p(zd) as
Py(Zd) = -K PK=IPγ(zd∣uk), where Y denotes the parameters of the conditional prior network,
uk ∈ X are the learnable pseudo-inputs, and K is the number of the pseudo-inputs. We empha-
size that the conditional prior p(zd|xe) (or approximated p(zd|uk)) is used to obtain the marginal
prior p(zd); while the posterior p(ze |xe) is used for inference. In experiment, we parameterize the
conditional prior as a Gaussian distribution.
We call the proposed generative model Symmetric Wasserstein Autoencoders (SWAEs) as we treat
the observed data and the latent representation symmetrically. We summarize the training algorithm
in Algorithm 1 and show the network architecture in Figure 2. As an example, we define the cost
function c(, ) as the squared L2 norm.
Algorithm 1: Symmetric Wasserstein Autoencoders (SWAEs)
Require: The number of the pseudo-inputs K. The weighting parameters β and a. Initialize
the parameters φ, θ, and γ of the encoder network, the decoder network, and the conditional
prior network, respectively.
while (φ, θ, γ, {uk}) not converged do
1.	Sample {xe,ι,…,Xe,N} from the training dataset.
2.	Find the closest pseudo-input u(n) of each training sample from the set {uι,…,UK}.
3.	Sample Zd,n from the conditional prior pγ (zd∣u(n)) for n = 1,…，N.
4.	Update (φ, θ, γ, {uk}) by descending the cost function
N PN=I βkXe,n - D(Zd,n)k2 + (1 — β)∣∣Xe,n - D(E(XW))k2 + α k E(x^ ) - Zd,n k 2.
Since we use the pseudo-inputs instead of the training samples in the conditional prior, given each
training sample we need to find the closest pseudo-input in Step 2. To measure the similarity, we
can use, e.g., the L2 norm or the cosine similarity. Since the dimension of the latent space is usually
much smaller than that of the data space, to reduce the searching time we can alternatively perform
Step 2 in the latent space as an approximation. Specifically, we can find the closest latent repre-
sentation of E(xe,n) from the set {E(uι),•…,E(UK)} so as to obtain the corresponding closest
pseudo-input. From the experiment we found that such approximation results in little performance
degradation, and we attribute it to the preservation of the local structure as explained before.
3	Experimental Results
In this section, we compare the performance of the proposed SWAE with several contemporary
generative autoencoders, namely VAE (Kingma & Welling, 2014), WAE-GAN (Tolstikhin et al.,
2018), WAE-MMD (Tolstikhin et al., 2018), VampPrior (Tomczak & Welling, 2018), and MIM
(Livne et al., 2019), using four benchmark datasets: MNIST, Fashion-MNIST, Coil20, and CIFAR10
with a subset of classes (denoted as CIFAR10-sub).
3.1	Experimental Setup
The design of neural network architectures is orthogonal to that of the algorithm objective, and
can greatly affect the algorithm performance (Vahdat & Kautz, 2020). Since MIM has the same
network architecture as that of VampPrior, for fair comparison we also build SWAE as well as VAE
based on the VampPrior network architecture. In particular, VampPrior adopts the hierarchical latent
structure with the convolutional layers (i.e., convHVAE (L = 2)), where the gating mechanism is
utilized as an element-wise non-linearity. The building block of the network structure of VAE and
SWAE is the same as that of VampPior except that the latent structure is non-hierarchical. Different
from SWAE, the prior of VampPrior and MIM is designed as a mixture of the posteriors (instead
5
Under review as a conference paper at ICLR 2021
Table 1: Classification accuracy of 5-NN (averaged over 5 trials). The standard deviation is generally
less than 0.01 and is omitted in the table._____________________________________________________
Dataset	dim-z	SWAE (β = 1)	SWAE (β = 0.5)	SWAE (β = 0)	VAE	WAE-GAN	WAE-MMD	VampPrior	MIM
	8	0.96	0.97	0.97	0.96	0.87	0.97	0.97	0.97
MNIST	40	0.97	0.97	0.97	0.80	0.68	0.96	0.93	0.97
	80	0.97	0.97	0.97	0.60	0.90	0.94	0.86	0.96
Fashion- MNIST	8	0.82	0.81	0.83	0.80	0.71	0.80	0.81	0.82
	40	0.84	0.83	0.84	0.54	0.62	0.82	0.81	0.82
	80	0.84	0.83	0.83	0.37	0.54	0.76	0.74	0.81
	8	0.95	0.97	0.97	0.95	0.98	0.78	0.91	0.89
Coil20	40	0.97	0.98	0.97	0.90	0.99	0.99	0.96	0.96
	80	0.97	0.98	0.98	0.97	0.98	0.98	0.96	0.98
CIFAR10- sub	80	0.69	0.67	0.65	0.67	0.61	0.68	0.68	0.66
	256	0.70	0.66	0.61	0.62	0.61	0.68	0.65	0.65
	512	0.70	0.66	0.62	0.55	0.60	0.68	0.64	0.66
of a mixture of the conditional priors as in SWAE) conditioned on the learnable pseudo-inputs.
The pseudo-inputs in SWAE, VampPrior, and MIM are initialized with the training samples. For
VampPrior and MIM, the number of the pseudo-inputs K is carefully chosen via the validation
set. Unlike these two algorithms, for SWAE we found that increasing K improves the algorithm
performance. The setup of K for SWAE, VampPrior, and MIM on all datasets can be found in
Appendix A.3. For SWAE, we set the weighting parameter α to 1 in all cases; in Step 2 we use the
L2 norm as the similarity measure in the data space. WAE-GAN and WAE-MMD are the WAE-
based models, where the divergence measure in the latent space is based on GAN and the maximum
mean discrepancy (MMD), respectively. The network structure of WAE-GAN and WAE-MMD is
the same as that used in (Tolstikhin et al., 2018). The prior of VAE, WAE-GAN, and WAE-MMD is
set as an isotropic Gaussian. A detailed description of the datasets, the applied network architectures,
and the training parameters can be found in Appendix A.3.
3.2	Latent Representation
The latent representation is expected to capture salient features of the observed data and be useful
for the downstream applications. The considered datasets are all associated with the labels. In the
experiment we use the latent representation for the K-Nearest Neighbor (KNN) classification and
compare the classification accuracy of 5-NN in Table 1, where dim-z denotes the dimension of the
latent space. The results of 3-NN and 10-NN are similar to those of 5-NN and thus are omitted. We
found that the classification results of all algorithms on CIFAR10 are unsatisfactory based on the
current networks (accuracy was around 0.3 - 0.4; this may due to the limited expressive power of
the shallow network architectures used), so instead we create a subset of CIFAR10 (CIFAR10-sub)
which contains 3 classes: bird, cat, and ship.
Since the prior of VAE, WAE-GAN, and WAE-MMD is an isotropic Gaussian, setting dim-z greater
than the intrinsic dimensionality of the observed data would force p(ze) to be in a manifold in the
latent space (Tolstikhin et al., 2018). This makes it impossible to match the marginal p(ze) with
the prior p(zd) and thus leads to unsatisfactory latent representation. Such concern can be verified
particularly on Fashion-MNIST where the classification accuracy of VAE and WAE-GAN drops
dramatically when dim-z is increased. For SWAE, we consider two cases: β = 1 (i.e., without the
reconstruction loss) and β = 0.5. The classification accuracy of SWAE (β = 1) is comparable to
SWAE (β = 0.5) and is generally superior for different values of dim-z to the benchmarks.
To further show the structure of the latent representation, we project the latent representation to
2D using t-SNE (Maaten & Hinton, 2008) as the visualization tool. As an example, we show the
projection of the latent representation on MNIST in Figure 3. We can see that SWAEs keep the local
structure of the observed data in the latent space and lead to tight clusters, which is consistent to our
expectation as explained in Section 2.1.
6
Under review as a conference paper at ICLR 2021
(d) WAE-GAN
(a) SWAE (β = 1)	(b) SWAE (β = 0.5)	(c) VAE
(e) WAE-MMD	(f) VampPrior	(g) MIM
Figure 3: Projection of the latent representation to 2D via t-SNE on MNIST. dim-z = 80 for all
methods.
Table 2: FreChet Inception Distance (FID) on generated images (smaller is better).
Dataset	dim-z	SWAE (β = 1)	SWAE (β = 0.5)	SWAE (β = 0)	SWAE (β *)	VAE	WAE-GAN	WAE-MMD	VampPrior	MIM
MNIST	8	50	40	24	21	24	17	34	24	74
Fashion-MNIST	8	65	57	48	47	60	41	100	51	83
Coil20	80	97	89	102	89	278	278	320	97	113
CIFAR10-sub	512	105	44	183	44~~	242	114	341	68	59
3.3	Generation and Reconstruction
To generate new data, latent samples are first drawn from the marginal prior distribution p(zd)
based on the conditional priors p(zd|uk), and then fed to the decoder. We put the generated images
of all methods in Appendix A.4, and show the FreChet Inception Distance (FID) (HeuSel et al.,
2017), which is commonly used for evaluating the quality of generated images, in Table 2. For
SWAEs, we observe that the reconstruction loss term is crucial for improving the generation quality
as SWAE (β = 1) generally cannot lead to the lowest FID. On MNIST and Fashion-MNIST, the
FID of the best SWAE (indicated as β*) is slightly higher than that of WAE-GAN, but lower than
all the other benchmarks. The visual difference between SWAE (β*) and WAE-GAN on MNIST
and Fashion-MNIST is however negligible. In Section 2.1, we compare the formulation of SWAEs
(β = 1) with WAE. In particular, the objective of WAE includes a distribution-based dissimilarity
in the latent space while the z-loss in SWAEs measures the sample-based dissimilarity. On Coil20
and CIFAR10-sub, SWAE (β*) achieves the lowest FID and generates new images that are visually
much better than those generated by the benchmarks.
In Table 3, we compare the reconstruction loss, defined as kxe - D(ze)k22, on the four datasets. As
expected, increasing the value of dim-z can reduce the reconstruction loss but the reduction becomes
marginal when dim-z is large enough. Additionally, since a smaller value of β leads to more em-
phasis on the reconstruction-based loss the quality of reconstruction is generally better. We observe
that SWAE (β = 0.5) results in the lowest reconstruction loss in all cases. The reconstructed images
of all methods are provided in Appendix A.4 for reference. Without including the reconstruction
loss into the objective, the reconstruction quality of SWAE (β = 1) can be unsatisfactory (e.g., on
CIFAR10-sub).
3.4	DENOISING EFFECT WITH SWAE (β = 1)
As discussed in Section 2.1, the x-loss has a close relationship to the objective of Denoising Autoen-
coders (DAs). After training, we feed the noisy images, which are obtained by adding the Gaussian
random samples with mean zero and standard deviation 0.3 to the clean test samples, to the encoder.
In Figure 4, as an example, we show the reconstructed images on Fashion-MNIST. Since the recon-
struction loss is highly related to the dimension of the latent space, for fair comparison we set dim-z
7
Under review as a conference paper at ICLR 2021
Table 3: Reconstruction loss (averaged over 5 trials).
Dataset	dim-z	SWAE (β = 1)	SWAE (β = 0.5)	VAE	WAE-GAN	WAE-MMD	VampPrior	MIM
	8	30.11 ± 0.14	23.20 ± 0.04	24.34 ± 0.07	26.86 ± 0.37	24.76 ± 0.31	24.05 ± 0.10	24.04 ± 0.10
MNIST	40	26.29 ± 0.13	6.93 ± 0.05	18.40 ± 0.08	16.06 ± 0.15	13.78 ± 0.77	17.32 ± 0.09	18.14 ± 0.33
	80	26.10 ± 0.09	1.25 ± 0.02	18.50 ±0.11	10.78 ± 0.11	9.63 ± 0.05	17.42 ± 0.06	17.29 ± 0.20
Fashion- MNIST	8 40	74.74 ± 0.04 73.39 ± 0.08	71.03 ± 0.06 57.90 ± 0.25	72.56 ± 0.02 69.85 ± 0.04	78.17 ± 1.41 74.84 ± 0.23	74.50 ± 0.60 75.86 ± 0.41	72.20 ± 0.04 68.67 ± 0.07	72.34 ± 0.03 70.22 ± 0.87
	80	73.35 ± 0.08	44.30 ± 0.71	69.90 ± 0.08	70.74 ± 1.16	71.28 ± 3.80	68.54 ± 0.10	69.10 ± 0.13
	8	7.07 ± 0.64	5.69 ± 0.51	7.90 ± 0.36	8.14 ± 0.34	21.20 ± 15.30	8.17 ± 1.02	13.84 ± 3.82
Coil20	40	5.52 ± 0.40	4.27 ± 0.66	5.67 ± 0.42	5.82 ± 0.84	8.07 ± 7.80	6.31 ± 0.62	5.75 ± 0.77
	80	5.56 ± 0.30	4.33 ± 0.40	5.71 ±0.67	5.62 ± 1.26	5.83 ± 1.92	6.32 ± 0.37	5.87 ± 0.69
CIFAR10-sub	512	50.82±3.78	6.50±0.08	9.41±0.27	13.37±1.62	13.09±1.92	12.06 ±0.91	10.02 ±0.37
□ ∣IU[J≡E匚]却脚盘均睨后搠然理□□E1IEE!□HΠΠ3
ιwm□π藜团缢生瘠⑥隧固皤翻
IimriCD剧囱靖濡第蒲嵯;阴4居u^≡n□ι□∣]m
ιa∣[]E≡□C我温园期她隙困四i留源□BEB!LfflΠ≡・忆员E
^R≡Iira≡≡≡SE ΠHΠΠΓ__________________ ____________
≡lM≡l≡i≡≡H Πlιl□E□E[ ]∣il□□雷滴洲武随判麻盘原印 ΠEKS-j□^ΠKR□
(a) Noisy real images (b) SWAE (β = 1)	(C) SWAE (β = 0.5)
(e) WAE-GAN
(f) WAE-MMD
(d) VAE
(h) MIM
(g) VampPrior
空・累而司■设龌围El
ΠIJKJJΠEΠJiH□
Figure 4: Denoising effect: reconstructed images on Fashion-MNIST. dim-z = 80 for all methods.
to 80 for all methods. We observe that only SWAE (β = 1) can recover clean images. This observa-
tion confirms the denoising effect induced by the x-loss, and thus the resultant latent representation
is robust to partial destruction of the observed data.
4	Related Work
The objective of VAEs uses the asymmetric KL divergence between the encoding and the decoding
distributions (see Appendix A.1). To improve VAEs (Livne et al., 2019; Chen et al., 2018; Pu et al.,
2017b) propose symmetric divergence measures instead of the asymmetric KL divergence in VAE-
based generative models. For example, MIM (Livne et al., 2019) adopts the Jensen-Shannon (JS)
divergence between the encoding and the decoding distributions together with a regularizer maxi-
mizing the mutual information between the data and the latent representation. Due to the difficulty
of estimating the mutual information and the unavailability of the data distribution, an upper bound
of the desired loss is proposed. AS-VAE (Pu et al., 2017b) and the following work (Chen et al.,
2018) propose a symmetric form of the KL divergence optimized with adversarial training. These
methods typically involve a difficult objective either depending on (unstable) adversarial training
or containing the mutual information that requires further approximation. In contrast, the proposed
SWAEs yield a simple expression of objective and do not involve adversarial training.
Compared to VAEs, GANs lack an efficient inference model thus are incapable of providing the
corresponding latent representation given the observed data. To bridge the gap between VAEs and
GANs, recent works attempt to integrate an inference mechanism into GANs by symmetrically treat-
ing the observed data and the latent representation, i.e., the discriminator is trained to discriminate
the joint samples in both the data and the latent spaces. In particular, the JS divergence between the
encoding and the decoding distributions is deployed in ALI (Dumoulin et al., 2017) and BiGANs
(Donahue et al., 2017). To address the non-identifiability issue in ALI (e.g., unfaithful reconstruc-
tion), later ALICE (Li et al., 2017a) proposes to regularize ALI using conditional entropy.
Generative modelling is closely related to minimizing a dissimilarity measure between two distribu-
tions. As opposed to many other commonly adopted dissimilarity measures, e.g., the JS and the KL
divergences, the Wasserstein distances that arise from the OT problem provide a weaker distance
8
Under review as a conference paper at ICLR 2021
between probability distributions (See (Santambrogio, 2015; Peyre et al., 2019; KoloUri et al., 2017)
for more background on OT). This is crucial as in many applications the observed data are essen-
tially supported on a low dimensional manifold. In such cases, common dissimilarity measures may
fail to provide a useful gradient for training. Consequently, the Wasserstein distances have received
a surge of attention for learning generative models (Arjovsky et al., 2017; Balaji et al., 2019; Sanjabi
et al., 2018; Kolouri et al., 2019; Patrini et al., 2019; Tolstikhin et al., 2018; Deshpande et al., 2019;
Nguyen et al., 2020). Particularly, the VAE-based models (Tolstikhin et al., 2018; Kolouri et al.,
2019; Patrini et al., 2019) are all based on minimizing the OT cost of the marginal distributions in
the data space with the difference of how to measure the divergence in the latent space: (Tolstikhin
et al., 2018) proposes the GAN-based and the MMD-based divergences, (Kolouri et al., 2019) adopts
the sliced-Wasserstein distance, and (Patrini et al., 2019) exploits the Sinkhorn divergence. Unlike
these works, our proposed SWAEs directly minimize the OT cost of the joint distributions of the
observed data and the latent representation with the inclusion of a reconstruction loss for further
improving the generative model.
5	Conclusion and Future Work
We contributed a novel family of generative autoencoders, termed Symmetric Wasserstein Autoen-
coders (SWAEs) under the framework of OT. We proposed to symmetrically match the encoding
and the decoding distributions with the inclusion of a reconstruction loss for further improving the
generative model. We conducted empirical studies on benchmark tasks to confirm the superior per-
formance of SWAEs over state-of-the-art generative autoencoders.
We believe that symmetrically aligning the encoding and the decoding distributions with a proper
regularizer is crucial to improving the performance of generative models. To further enhance the
performance of SWAEs, it is worthwhile to exploit other methods for the prior design, e.g., the flow-
based approaches (Rezende & Mohamed, 2015; Dinh et al., 2014; 2016), and other forms of the
reconstruction loss, e.g., the cross entropy.
9
Under review as a conference paper at ICLR 2021
References
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy.
Fixing a broken ELBO. arXiv preprint arXiv:1711.00464, 2017.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In ICML, 2017.
Andrei Atanov, Arsenii Ashukha, Kirill Struminsky, Dmitry Vetrov, and Max Welling. The deep
weight prior. In ICLR, 2019.
Yogesh Balaji, Hamed Hassani, Rama Chellappa, and Soheil Feizi. Entropic GANs meet VAEs: A
statistical approach to compute sample likelihoods in GANs. In ICML, 2019.
Liqun Chen, Shuyang Dai, Yunchen Pu, Erjin Zhou, Chunyuan Li, Qinliang Su, Changyou Chen,
and Lawrence Carin. Symmetric variational autoencoder and connections to adversarial learning.
In AISTATS, 2018.
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen
Zhao, David Forsyth, and Alexander G Schwing. Max-sliced wasserstein distance and its use for
GANs. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
10648-10656, 2019.
Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. arXiv
preprint arXiv:1605.08803, 2016.
JeffDonahue, PhiIiPP Krahenbuhl, and Trevor Darrell. Adversarial feature learning. In ICLR, 2017.
Vincent Dumoulin, Ishmael Belghazi, Ben Poole, Olivier Mastropietro, Alex Lamb, Martin Ar-
jovsky, and Aaron Courville. Adversarially learned inference. In ICLR, 2017.
Babak Esmaeili, Hao Wu, Sarthak Jain, Alican Bozkurt, Narayanaswamy Siddharth, Brooks Paige,
Dana H Brooks, Jennifer Dy, and Jan-Willem van de Meent. Structured disentangled rePresenta-
tions. In AISTATS, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and SePP Hochreiter.
GANs trained by a two time-scale uPdate rule converge to a local Nash equilibrium. In Advances
in neural information processing systems, 2017.
Matthew D Hoffman and Matthew J Johnson. ELBO surgery: yet another way to carve uP the
variational evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference,
NIPS, 2016.
Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P. Xing. Toward con-
trolled generation of text. In ICML, 2017.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, and Patrick van der Smagt. Learning
hierarchical Priors in VAEs. In NeurIPS, 2019.
Soheil Kolouri, Se Rim Park, Matthew ThorPe, Dejan SlePcev, and Gustavo K Rohde. OPtimal
mass transPort: Signal Processing and machine-learning aPPlications. IEEE signal processing
magazine, 34(4):43-59, 2017.
Soheil Kolouri, PhilliP E PoPe, Charles E Martin, and Gustavo K Rohde. Sliced-Wasserstein auto-
encoders. In ICLR, 2019.
10
Under review as a conference paper at ICLR 2021
Chunyuan Li, Hao Liu, Changyou Chen, Yuchen Pu, Liqun Chen, Ricardo Henao, and Lawrence
Carin. Alice: Towards understanding adversarial learning for joint distribution matching. In
NIPS, 2017a.
Jerry Li, Aleksander Madry, John Peebles, and Ludwig Schmidt. Towards understanding the dy-
namics of generative adversarial networks. In ICLR, 2017b.
Micha Livne, Kevin Swersky, and David J Fleet. MIM: Mutual information machine. arXiv preprint
arXiv:1910.03175, 2019.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of machine
learning research, 9(Nov):2579-2605, 2008.
Khai Nguyen, Nhat Ho, Tung Pham, and Hung Bui. Distributional sliced-wasserstein and applica-
tions to generative modeling. arXiv preprint arXiv:2002.07367, 2020.
Giorgio Patrini, Rianne van den Berg, Patrick Forre, Marcello Carioni, Samarth Bhargav, Max
Welling, Tim Genewein, and Frank Nielsen. Sinkhorn autoencoders. In UAI, 2019.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning,11(5-6):355-607, 2019.
Yuchen Pu, Zhe Gan, Ricardo Henao, Chunyuan Li, Shaobo Han, and Lawrence Carin. VAE learn-
ing via Stein variational gradient descent. In NIPS, 2017a.
Yuchen Pu, Weiyao Wang, Ricardo Henao, Liqun Chen, Zhe Gan, Chunyuan Li, and Lawrence
Carin. Adversarial symmetric variational autoencoder. In NIPS, 2017b.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
VQ-VAE-2. In NeurIPS, 2019.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
ICML, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In ICML, 2014.
Maziar Sanjabi, Jimmy Ba, Meisam Razaviyayn, and Jason D Lee. On the convergence and robust-
ness of training GANs with regularized optimal transport. In NeurIPS, 2018.
Filippo Santambrogio. Optimal transport for applied mathematicians. Birkauser NY, 55(58-63):94,
2015.
Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.
In NIPS, 2014.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. In ICLR, 2018.
Jakub Tomczak and Max Welling. VAE with a VampPrior. In AISTATS, 2018.
Benigno Uria, Iain Murray, and Hugo Larochelle. RNADE: The real-valued neural autoregressive
density-estimator. In NIPS, 2013.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020.
Aaron Van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In
ICML, 2016.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and
composing robust features with denoising autoencoders. In ICML, 2008.
11
Under review as a conference paper at ICLR 2021
Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local
denoising criterion. Journal ofmaChine learning research, 11(Dec):3371-3408, 2010.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Balancing learning and inference in
variational autoencoders. In AAAI, 2019.
A Appendix
A. 1 Objective of VAEs
The objective of VAEs is to maximize a tractable variational lower bound on the data log-likelihood,
called the Evidence Lower Bound (ELBO):
Ep(xe) Ep(ze|xe) [logp(xd|z)] - DKL(p(ze|xe)||p(zd)) .	(6)
It can be also shown that the objective of VAEs is equivalent to minimizing the KL divergence
(or maximizing the negative KL divergence) between the encoding and the decoding distributions
(Livne et al., 2019; Esmaeili et al., 2019; Pu et al., 2017b; Chen et al., 2018):
-DKL(p(xe,ze)||p(xd,zd)) = Ep(xe,ze)
lθg P(Xd⅛] - Ep(Xe) [lθg P(Xe)I.
(7)
The right hand side of equation 7 is only different from equation 6 in terms of a constant, which is
the entropy of the observed data.
A.2 Proof of Theorem 1
The proof extends that of Theorem 1 in (Tolstikhin et al., 2018). In particular, (Tolstikhin et al., 2018)
aims to minimize the OT cost of the marginal distributions p(Xe ) and p(Xd), and the proof there is
based on the joint probability of three random variables: the observed data, the generated data, and
the latent representation. In contrast, we propose to minimize the OT cost of the joint distributions
of the observed data and the latent representation induced by the encoder and the decoder. As
a result our proof is based on the joint distribution of four random variables (Xe, ze,Xd, zd) ∈
X × Z × X × Z. We assume that the joint distribution p(Xe, ze,Xd, zd) satisfies the following three
conditions:
1.	e , (Xe,Ze)〜P(Xe)P(Ze∣Xe);
2.	d，(xd,Zd)〜P(Zd)P(xd∣Zd); and
3.	Xd ⊥⊥ Xe|Zd (conditional independence).
The first two conditions specify the encoder and the decoder respectively, and the last condition
indicates that given the latent prior the generated data and the observed data are independent.
Denote the set of the above joint distributions as P(Xe, Ze,Xd,Zd). Obviously, we have
P (Xe, Ze, Xd, Zd) ⊆ P (e 〜 P (e), d 〜P(d)) due to the third condition. If the decoder is deter-
ministic, P(Xd ∣Zd) is a Dirac distribution thus P (Xe, Ze, Xd, Zd) = P (e 〜P (e), d 〜P(d)). With
this result, we can rewrite the objective of the underlying OT problem as follows:
Wc(P(e),P(d)) =	inf 、E(e,d)〜γ c(e, d)
Γ∈P(xe,ze,xd,zd)
inf
Γ∈P(xe,ze,zd)
E(Xe,Ze,Zd)~Γ
c(e, d)
inf	Ep(xe) Ep(ze |xe) Ep(zd |xe,ze)
p(ze|xe), p(zd |xe,ze)
inf Ep(xe)Ep(zd|xe) c(e, d),
p(zd|xe)
c(e, d)
(8)
(9)
(10)
where in equation 8 P(Xe, Ze, Zd) denotes the set of the joint distributions of (Xe, Ze, Zd) induced
by P(Xe, Ze, Xd, Zd) and it holds due to the deterministic decoder, and equation 10 holds due to the
deterministic encoder.
12
Under review as a conference paper at ICLR 2021
A.3 Datasets and Network Architectures
In this section, we briefly describe the datasets, the network architectures, and the hyperparameters
that are used in our training algorithm.
•	MNIST: The dataset includes 70, 000 binarized images of numeric digits from 0 to 9, each
of the size 28 × 28. There are 7, 000 images per class. The training set contains 50, 000
images, the validation set contains 10, 000 images for choosing the best model based on
the loss function, and the test set contains 10, 000 images.
•	Fashion-MNIST: The dataset includes 70, 000 binarized images of fashion products in 10
classes. This dataset has the same image size and the split of the training, validation, and
test sets as in MNIST.
•	Coil20: The dataset includes gray-scale images of 20 objects, each image of the size 32 ×
32. The training set contains 1040 images, the validation set contains 200 images for
choosing the best model based on the loss function, and the test set contains 200 images.
•	CIFAR10-sub: The CIFAR-10 dataset consists of 60, 000 32 × 32 colour images in 10
classes with 6, 000 images per class. There are 40, 000 training, 10, 000 validation, and
10, 000 test images. We randomly select three classes to form the CIFAR10-sub dataset,
namely bird, cat, and ship.
•	Celeba: The Celeba dataset is resized to 64 × 64 resolution. The training set contains
162, 770 images, the validation set contains 19, 867 images, and the test set contains 19, 962
images.
Network architecture of SWAE: The building block of the network structure of SWAE is based on
VampPrior, called GatedConv2d. GatedConv2d contains two convolutional layers with the gating
mechanism utilized as an element-wise non-linearity. The parameters in the function GatedConv2d()
represent the number of the input channels, the number of the output channels, kernel size, stride,
and padding, respectively. The conditional prior network outputs the mean and the log-variance of
a Gaussian distribution, based on which the latent prior is sampled.
•	The structure of the encoder network:	GatedConv2d(1,32,7,1,3)-
GatedConv2d(32,32,3,2,1)-GatedConv2d(32,64,5,1,2)-GatedConv2d(64,64,3,2,1)-
GatedConv2d(64,6,3,1,1), followed by one fully-connected layer with no activation
function.
•	The structure of the conditional prior network: The layers of GatedConv2d are the same
as those in the encoder network, which are followed by two fully-connected layers. One
produces the mean, and the other produces the log-variance with the activation function
Hardtanh.
•	The structure of the decoder network: Two fully-connected layers with the
gating mechanism, followed by GatedConv2d(1,64,3,1,1)-GatedConv2d(64,64,3,1,1)-
GatedConv2d(64,64,3,1,1)-GatedConv2d(64,64,3,1,1), followed by a convolutional layer
with the activation function Sigmoid.
The algorithm is trained by Adam with the learning rate = 0.001, β1 = 0.9, and β2 = 0.999.
Setup of the number of the pseudo-inputs K: As suggested in (Tomczak & Welling, 2018; Livne
et al., 2019) we set the value of K in VampPrior and MIM on MNIST and Fashion-MNIST to 500.
We found K = 500 is also suitable for VampPrior and MIM on Coil20, CIFAR10-sub, and Celeba.
Unlike VampPrior and MIM, for SWAE we found that increasing K improves the performance and
we set K to 4000 on MNIST, Fashion-MNIST, CIFAR10-sub, and Celeba. Coil20 is a relatively
small dataset and we set K to 500 for SWAE, VampPrior, and MIM.
A.4 More Experimental Results
In this section, we show more experimental results based on the comparison with the benchmarks.
13
Under review as a conference paper at ICLR 2021
(d) WAE-GAN
(a) SWAE (β = 1)	(b) SWAE (β = 0.5)	(c) VAE
(e) WAE-MMD	(f) VampPrior	(g) MIM
Figure 5: Latent representation on MNIST; dim-z = 2 for all methods. With more compressed
latent representation, the classification accuracy generally decreases except VAE (5-NN accuracy
with dim-z = 2: SWAE(β = 1)(0.75), SWAE(β = 0.5)(0.86), VAE(0.84), WAE-GAN(0.81), WAE-
MMD(0.81), VampPrior(0.82), and MIM(0.87)). Also, the quality of reconstruction and generation
decreases when dim-z = 2.
S□E9ΞQ
□nΞ□Ξ
SSΞQΞ
OQBQQ
□ΠΞΞB
DSDSS
(b) SWAE (β = 1)	(c) SWAE (β =
目αQ囹目
ΞQΞαH
QΞHΞQ
q□q□ħ
□E3E3□Ξ
ESΞBE3E3
ΞHQ口目
□□□ΞΞ
(e) WAE-GAN	(f) WAE-MMD
(g) VampPrior
(h) MIM
Figure 6: Generated new samples on MNIST. dim-z = 8 for all methods.
14
Under review as a conference paper at ICLR 2021
ΠH[]m∏≡□ EE□ΠE
LIHICH≡□OL] ΠQΠH□
□ΠΠΠUI∣I□≡ ∏[ i≡[ ]
UEI∣l[l≡C□i I □≡cπn
[]≡□ΠIJ[≡il B□SΞH
u
u
ΠIιlQ
EEH
ΠHΠ
cπsι
□ □C□∣w□EEERE
IRhl 1□
ιl Q uB
□ □≡B□L]
Iil E□Ξ□E*i
Π□HQ
ΠEEH
LrhIl
ΠETiHI
(a) SWAE (β* = 0.05)
(b) SWAE (β = 1)
(c) SWAE (β = 0)
eπħuhπ
JLjCL J
□□EEC□
□π□□□c
□ΠHLI Π≡BB□
ΞDΠFI
□E□
□ ^E3QDH□□r][≡
□OE□ □BECΠ□E□ΠI________
□EΠ□ ΠE≡□ΠΠLDaE DΞSΞ□≡ΠO≡
□HQE mHHΠΞ≡□HI-----------
E GŋΠΓI□iJ□□l IU
1ΠUE HΠ□EIB□EJH il Ξ□Π^m□H≡ε□
(d) VAE
(e) WAE-GAN
(f) WAE-MMD
□π□
EΠ□
口ΠW
Π□□EΠ
rsιυ≡E[
∣i∣□παra
IilEH
ra□□⅛iL
□ce™
IlZ QNIdEC^≡QΠH
e□ ME=Iiinmciin
B□ ΠCI∣IQI∣MHNΠΞ
∏□ EΠMHQMIICΠ
MEESBBCMgC
(g) VampPrior
(h) MIM
Figure 7: Generated new samples on Fashion-MNIST. dim-z = 8 for all methods.
=CSS
QHoa□回
□2XΞ
s□BEaQ
□【110□叼
□Q3XΞ
wħh□
SBQ
田昌E
(Igrl
Ξ0IOIEIIQ
so S
≡M0a
=nm0ŋ
用口3
ΞN□□n
EBE
glo□EI
□Q2XΞ
□Qg□
ɑŋenŋ
P a□Ξ
srt∑□
网上□口二
S□U3
ξ□ħc工
ɑ□u□口
口2口一
(a) SWAE (β* = 0.5)	(b) SWAE (β = 1)
(e) WAE-GAN
(c) SWAE (β =
(f) WAE-MMD
(d) VAE
(g) VampPrior
(h) MIM
Figure 8: Generated new samples on Coil20. dim-z = 80 for all methods.
15
Under review as a conference paper at ICLR 2021
(c) SWAE
(b) SWAE (β =
(a) SWAE (β* = 0.5)
WAE-MMD
≡9CB1I
田 4a
an
SHH
Hgr*⅛⅜
◎瑁上EJS
l^a≡
≡BBH□
iΞB
ViBB
3m
睢Eπq⅛第
lk∕∙=α
SOBS
HBtr
BH ⅛Λ
s>- S
IHS
N O?
E0⅜τ*
足
・驾玉晶■
Figure 9:	Generated new samples on CIFAR10-sub. dim-z = 512 for all methods.
(a) SWAE (β* = 0.5)
(b) SWAE (β = 1)
(c) SWAE (β = 0)
(e) WAE-GAN
(f) WAE-MMD
rBdh心

(h) MIM
(g) VampPrior
Figure 10:	Generated new samples on Celeba. dim-z = 512 for all methods. VampPrior has the best
generation quality visually. SWAE (β* = 0.5) is better than SWAE (β = 1) and SWAE (β = 0).
16
Under review as a conference paper at ICLR 2021
BQBΞBnSDBΞ BBΠE]Ein□□BQ BQDQQDI3□BΞ BQHQE1HHBBΞ
ξξħeiπħqħξd ξq□ei∏ħeiħξ□ ξξħξπħqħξd ξξħξπħqħξb
QHQBQΞHQΞΠ □BQB□ΞB□E1Π QHE3SSΞE1□ΞD QBE9SBΞHDEID
bπħqbξbπsd eikibqhbħπbπ bπbqbξbπqd bπbqbbqπqd
Sbqhhbdhqei πh□□hsdei□d πbqbhbdb□d πbqhqsdb□d
(a) Real images
(b) SWAE (β = 1)	(c) SWAE (β = 0.5)	(d) SWAE (β = 0)
E2□ΠΠE3
QBHQB
n≡ΞΞs
QDQBH
眄ξbπħ
BΞ□E1Π
BDnDQ
E3DQHH
HQDDD
QnDHH
BDnnQ
□ΞE1EID
DSΞΞQ
HnDBB
(h) VampPrior
(f) WAE-GAN	(g) WAE-MMD
(i) MIM
Figure 11:	Reconstructed images on MNIST. dim-z = 80 for all methods. As expected, for SWAEs
a smaller β leads to a higher quality of reconstruction.
(a) Real images
EDIiIIiI
□E≡rι
π≡≡c
巴 FwIn
ΠlιlΠQ[]E[JL
[JI∣IM□
[]∣∣1O0
ιl□□I
∏[1Ξ≡
E^blhl^ti≡B9H≡
EJSΞΠΠHmm
^≡ΞΞ∣imr≡EJ
^ħhħrbq≡ξξ
日M同gE加I般因
(b) SWAE (β = 1)	(c) SWAE (β = 0.5)	(d) SWAE (β = 0)
ΞΠOH
LlC=II
ΠESΞ
PBnn
[IlilDS
L
It∣m□aE Ξr≡llilN^tι≡≡BΞ 至EIIIEl国副脚回■鹫 E□Mh∣βlldfflBOBΞ
l∣≡[O i 圈底网口CEiIlIlEEl □"阿园匚目 UE≡πnι∣ιm□π
E1□Γ1EΠ ^Ξ□ΞE1≡1ΓIΞE1 ^ΞQΞU≡□ΠΞO ΠSSΞMH□ΠSE1
■≡≡□Q ^ΠH[iπ≡S≡ΞΞ ^HQ∏πHEHSE πħπππqe≡se
Enn□□ LlIilElEEimiiim ΠI∣1ΠE[≡Γ][≡≡ ΠlιlΠE∏ε∏I∣IUΠ
(e) VAE
(f) WAE-GAN
(g) WAE-MMD
(h) VampPrior
miιlMEllilHISl≡S
UBnrMMIAl
[1 S□≡lιlKILIΠΞU
Γ^HΠΠΓβlHΞ≡ΞC
ΠW□EΠEΠI∣IEJ□
(i) MIM
Figure 12:	Reconstructed images on Fashion-MNIST. dim-z = 80 for all methods.
17
Under review as a conference paper at ICLR 2021
ωΞQ[iri□QΞZ□ Ξ==□Ξ□I ISEin Ξ==□Ξ□Π3EK1 Ξ==□Ξ□I1□E1H
□O≡C□□□□H□□他口？口Eim匚町则&=0？匕&口匚
圜。回 DqB。已槐:：ce□□≈li□π□□ ccα□^ιιαπ□□ □□Q□smπ□□
二βJtmHE]LΣH=□ EiEIU=二曰=□【】【Zl EiBɪi==曰=qdQ EiBJEiH 二二=白Cm
κjuΞ^Ξ□=□□ω z□Li□□[∙ιι□□ω≡ ≡□m□[∣ι□3i3 ≡□ej□□m□cm≡
(a) Real images
(b) SWAE (β = 1)	(c) SWAE (β = 0.5)	(d) SWAE (β = 0)
M□M[J[d□QΣ2□ =G鼠DmNIiEI工 HOE1□□□SΠHE3 II1IHHI1I911□□Z□
□∏[QC□□□□L1C	□MmD□□U3□	□U□M□UHH□□
因 EiGa 口 £0 日国 α nM∣⅞≡Ξκ¾R=iir¾ πŋH∏πaamra□ ξ□□h□□s□s□□
□□□Π3□□C≡Z □QH33^j□I≡Ξ:	竭QnHDZ= ZUHIO□□□□IO□
DJQΞBl^□Ξ□□rJ	□ΞrJH≡□m□El
(e) VAE
(f) WAE-GAN
(g) WAE-MMD
(h) VampPrior
[≡HΠB1H□IΠC□
?lKKa□□cra□
□ElgBHHg
(i) MIM
Figure 13:	Reconstructed images on Coil20. dim-z = 80 for all methods. For SWAEs, the difference
of the reconstruction error for different values ofβ is insignificant, and the reconstructed images look
visually the same.
》注且二届0□BB空剧.∙∙-∙∙EI∙O∙上*房」程旧口•更肃上覆冬—超40E1至祸
a既於入出n ■大■♦■■•，■「q叫∙入2m4N圄匚 gn.∙z->43口
黑。与04玄K果聊中∙0/lfl∙∙∙∙∙∙疆口ZLB厚三」，通£、； *。公LS件三位2“军
修目■察刍绘立■扑A ■!!■■■■■■\▲ KES≡∣iS当卷口■鼻髭IHl审霄刍立，■界A
JMA IBik∙EiιS 唐・息・■■■■■■■ Jam v^^ακΞ JmAl 总3・口1SES
(a) Real images
(b) SWAE (β = 1)	(c) SWAE (β = 0.5)	(d) SWAE (β = 0)
上蹇受一墨心中■常用展箕WL建日留■涝剧IjE理」施启㈡■嘘剧上整・乙必例
AC3匚心笃』4>41匚4底工忆如*4队・口 ⅛ΓS3l⅛Eia.⅛¾U∏
盥Q石■便fc：Z曰廉:丁罡'。公LS便三匚烧静漕建。上■俱乐运起■母鼠。6・厚・工喜■厚
■国■■当■■■■■ ■国目Hi刍&厂・户事里包安■刍J「鞭唐■墓F1；谒二匕-■>海
7*3 K 息∙∙E1I1 目2 Xla	您 ^≡B∣⅞I3DΞ **lι> ^h⅜3HS
(e) VAE
(f) WAE-GAN
(g) WAE-MMD
(h) VampPrior
疆陶厚君口
二.®Sk
3 0½3A
Sasl
荆K7
自πlh∙Ξ
Iv
∙‰s∙3
n⅛a∙*
ga≡3g
(i) MIM
Figure 14:	Reconstructed images on CIFAR10-sub. dim-z = 512 for all methods. Excluding the
reconstruction loss in the objective, the reconstruction of SWAE (β = 1) is blurry.
18
Under review as a conference paper at ICLR 2021
A国剑电口±%AEIIlGnRa 0£等A直剑点口6电:泰工穿京货募
n∏ΓQ^BH; CΩ	帘曜费EIIa虚联C匹白K@费电摩 工C
段息金-LaF冠自!中区ɑ∙∙U0目❸。盲目㈢自百2一虱宿禽耳!3百舞息q lb。后昌1百
OO卷 ec30□α 位 ∩QFT∏ ∏.. y i∏ HnRC 自白重驯后台 3Π∣S A a ΞSβII9^
费⑥IilIL金■&号⑪度G二C ^ IlakB■立■厦'n∙a0∙∙盘■・麓・■叁号@离
(a) Real images
(b) SWAE (β = 1)	(c) SWAE (β = 0.5)	(d) SWAE (β = 0)
C©EW2∏C"f%芽A目且e∏0鱼贵$京0口ifll金FlR黔：泰工重A口£管门/QfJ¥7
rr‰忌，匕陵，鹿C rm 送r也摩 工C nr6您Cri摩FqC ππr^^κs γλ∏
鳗国百士.0曾其同于A舞急有T向00息I 3百铛息漕士a。自目■用娉息有-La冠制目3（各
石口・@。囱宾康面e ADll90 Q国堂面小电ee昂@Qe图塌由府oπħ∣A Ω	φ
学W宜Qj '.IlQ'7¾后 生©鼠含髭—2 癖尹3W豆SJ ,、扇J2「藤藤身侵e.≡J ，.H叁笈窗引
(e) VAE
(f) WAE-GAN
(g) WAE-MMD
(h) VampPrior
Atl ■僦 FIG§ 芍
FlnZ号费匕摩£求体
娉息南谴敏&Gi目尊百
ΩflHA O囱&康目修
今⑥豆9j ,fl3'归百岛
(i) MIM
Figure 15:	Reconstructed images on Celeba. dim-z = 512 for all methods. The average reconstruc-
tion error over three seeds is as follows: SWAE (β = 1) : 301.11 ±4.85, SWAE (β = 0.5): 37.78
±6.71, SWAE (β = 0): 30.14 ±0.18, VAE: 38.64±5.39, WAE-GAN: 23.63±5.79, WAE-MMD:
28.64±2.85, VampPrior: 37.96±0.71, and MIM: 36.40±2.84. As expected, for SWAE a smaller
value of β leads to a lower reconstruction loss.
QDnnD
BBEBΠ
QQHBD
ΠΠHHH
HE1□ΠΠ
(c) SWAE (β = 0.5)
(d) VAE
(a) Noisy real images	(b) SWAE (β = 1)
BaDQBinQQBH BBIIQKaQQQBH HHBHHBaHBIS SEHKQiiSDiEQ
ΞΞQQΠBQBΞQ ΞΞQQΠSΞBΞQ MHHaSHHM QEQBEQBE33!
BHΞBQΞa□ElD QBQBQΞE1QΞD EaHMBBaB BEHH□SR≡SΞΞ
BEIBΞHΞa∏QΠ HHSQHΞBΠBD HnHSHKMHB aSl≡≡≡HHa□!3
IIBE1QQΞHQ□D ΠQQQHΞDS□D HBnHHHRSHE KaQQBSQSlBia
(e) WAE-GAN
(f) WAE-MMD
(g) VampPrior
(h) MIM
Figure 16:	Denoising effect: reconstructed images on MNIST. dim-z = 80 for all methods. SWAE
(β = 1), WAE-GAN, and WAE-MMD can recover clean images. However, for WAE-GAN and
WAE-MMD, we can still see some noisy dots around the digits.
19
Under review as a conference paper at ICLR 2021
fJi3□□Li∑∑^UL^ □ι jŋ^n^nz
≈Π1□^L1□[J□□ Q=BJ CTBElE E□
□HHΞ□□=□Π3 QMD=□S=□[1□ □S□Π□□□C≡□
z□m□Bi□aa□ □□m□Bi□∏βa≡ hq□^□□ξ□□u
(a) Noisy real images (b) SWAE (β = 1)	(c) SWAE (β = 0.5)	(d) VAE
BI∣aHt9BiraΞ[∏C□
=G比回图目阂IiEl阻 Hm□CΞΞ□OE3 Dimi ]≡lΞΠΞ□ BBOHI≡I1ΞΠC□
田阳，国IIEJBK□/如山【砌口口口8匚：□U□M□HS1N2□ □UΣH□Q□Q□□
㈤㈤nEΞΞMNHKX3 □□Q□EIΞΠC[]□
Df]∣13至㈤。阿罐工工口9日Q=HDB= Zl lfll□□□D3Ql:J
ES^HIIΞHHE]£≡ CΞtJHE9□[3li2□[3 □□HC□ΞEJ□□L]
GmgZ 口HU
2ΠHΠ□□□□□[3
□OQZ□X□□EJQ
(e) WAE-GAN
(f) WAE-MMD
(g) VampPrior
(h) MIM
Figure 17:	Denoising effect: reconstructed images on Coil20. dim-z = 80 for all methods. Except
WAE-GAN and WAE-MMD, the other methods can produce clear images.
20