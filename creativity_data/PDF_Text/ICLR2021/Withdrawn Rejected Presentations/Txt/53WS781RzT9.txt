Under review as a conference paper at ICLR 2021
The Impact of the Mini-batch Size on the
Dynamics of SGD: Variance and Beyond
Anonymous authors
Paper under double-blind review
Ab stract
We study mini-batch stochastic gradient descent (SGD) dynamics under linear re-
gression and deep linear networks by focusing on the variance of the gradients
only given the initial weights and mini-batch size, which is the first study of this
nature. In the linear regression case, we show that in each iteration the norm of
the gradient is a decreasing function of the mini-batch size b and thus the vari-
ance of the stochastic gradient estimator is a decreasing function of b. For deep
neural networks with L2 loss we show that the variance of the gradient is a poly-
nomial in 1{b. The results theoretically back the important intuition that smaller
batch sizes yield larger variance of the stochastic gradients and lower loss function
values which is a common believe among the researchers. The proof techniques
exhibit a relationship between stochastic gradient estimators and initial weights,
which is useful for further research on the dynamics of SGD. We empirically pro-
vide insights to our results on various datasets and commonly used deep network
structures. We further discuss possible extensions of the approaches we build in
studying the generalization ability of the deep learning models.
1	Introduction
Deep learning models have achieved great success in a variety of tasks including natural language
processing, computer vision, and reinforcement learning (Goodfellow et al., 2016). Despite their
practical success, there are only limited studies of the theoretical properties of deep learning; see
survey papers (Sun, 2019; Fan et al., 2019) and references therein. The general problem underlying
deep learning models is to optimize (minimize) a loss function, defined by the deviation of model
predictions on data samples from the corresponding true labels. The prevailing method to train deep
learning models is the mini-batch stochastic gradient descent algorithm and its variants (Bottou,
1998; Bottou et al., 2018). SGD updates model parameters by calculating a stochastic approximation
of the full gradient of the loss function, based on a random selected subset of the training samples
called a mini-batch.
It is well-accepted that selecting a large mini-batch size reduces the training time of deep learning
models, as computation on large mini-batches can be better parallelized on processing units. For
example, Goyal et al. (2017) scale ResNet-50 (He et al., 2016) from a mini-batch size of 256 images
and training time of 29 hours, to a larger mini-batch size of 8,192 images. Their training achieves
the same level of accuracy while reducing the training time to one hour. However, noted by many
researchers, larger mini-batch sizes suffer from a worse generalization ability (LeCun et al., 2012;
Keskar et al., 2017). Therefore, many efforts have been made to develop specialized training pro-
cedures that achieve good generalization using large mini-batch sizes (Hoffer et al., 2017; Goyal
et al., 2017). Smaller batch sizes have the advantage of allegedly offering better generalization (at
the expense of a higher training time).
The focus of this study is on the behavior of SGD subject to the conditions on the initial point. This
is different from previous results which analyze SGD via stringing one-step recursions together. The
dynamics of SGD are not comparable if we merely consider the one-step behavior, as the model pa-
rameters change iteration by iteration. Therefore, fixing the initial weights and the learning rate can
give us a fair view of the impact of different mini-batch sizes on the dynamics of SGD. We hypoth-
esize that, given the same initial point, smaller sizes lead to lower training loss and, unfortunately,
1
Under review as a conference paper at ICLR 2021
decrease stability of the algorithm on average. The latter follows from the fact that the smaller is
the batch size, more stochasticity and volatility is introduced. After all, if the batch size equals to
the number of samples, there is no stochasticity in the algorithm. To this end, we conjecture that
the variance of the gradient in each iteration is a decreasing function of the mini-batch size. The
conjecture is the focus of the work herein.
Variance correlates to many other important properties of SGD dynamics. For example, there is
substantial work on variance reduction methods (Johnson & Zhang, 2013; Allen-Zhu & Hazan,
2016; Wang et al., 2013) which show great success on improving the convergence rate by controlling
the variance of the stochastic gradients. Mini-batch size is also akey factor deciding the performance
of SGD. Some research focuses on how to choose an optimal mini-batch size based on different
criteria (Smith & Le, 2017; Gower et al., 2019). However, these works make strong assumptions on
the loss function properties (strong or point or quasi convexity, or constant variance near stationary
points) or about the formulation of the SGD algorithm (continuous time interpretation by means of
differential equations). The statements are approximate in nature and thus not mathematical claims.
The theoretical results regarding the relationship between the mini-batch size and the variance (and
other performances, like loss and generalization ability) of the SGD algorithm applied to general
machine learning models are still missing. The work herein partially addresses this gap by showing
the impact of the mini-batch size on the variance of gradients in SGD. We further discuss possible
extensions of the approaches we build in studying the generalization ability.
We are able to prove the hypothesis about variance in the convex linear regression case and to show
significant progress in a deep linear neural network setting with samples based on a normal distribu-
tion. In this case we show that the variance is a polynomial in the reciprocal of the mini-batch size
and that it is decreasing if the mini-batch size is larger than a threshold (further experiments reveal
that this threshold can be as small as 2). The increased variance as the mini-batch size decreases
should also intuitively imply convergence to lower training loss values and in turn better prediction
and generalization ability (these relationships are yet to be confirmed analytically; but we provide
empirical evidence to their validity).
The major contributions of this paper are as follows.
•	For linear regression, we show that in each iteration the norm of any linear combination
of sample-wise gradients is a decreasing function of the mini-batch size b (Theorem 1).
As a special case, the variance of the stochastic gradient estimator and the full gradient at
the iterate in step t are also decreasing functions of b at any iteration step t (Theorem 2).
In addition, the proof provides a recursive relationship between the norm of gradients and
the model parameters at each iteration (Lemma 2). This recursive relationship can be used
to calculate any quantity related to the full/stochastic gradient or loss at any iteration with
respect to the initial weights.
•	For the deep linear neural network with L2 -loss and samples drawn from a normal distribu-
tion, we take two-layer linear network as an example and show that in each iteration step t
the trace of any product of the stochastic gradient estimators and weight matrices is a poly-
nomial in 1{b with coefficients a sum of products of the initial weights (Theorem 3). As a
special case, the variance of the stochastic gradient estimator is a polynomial in 1{b without
the constant term (Theorem 4) and therefore it is a decreasing function of b when b is large
enough (Theorem 5). The results and proof techniques can be easily extended to general
deep linear networks. As a comparison, other papers that study theoretical properties of
two-layer networks either fix one layer of the network, or assume the over-parameterized
property of the model and they study convergence, while our paper makes no such assump-
tions on the model capacity. The proof also reveals the structure of the coefficients of the
polynomial, and thus serving as a tool for future work on proving other properties of the
stochastic gradient estimators.
•	The proofs are involved and require several key ideas. The main one is to show a more
general result than it is necessary in order to carry out the induction. The induction is on
time step t. The key idea is to show a much more general result that lets us carry out
induction. New concepts and definitions are introduced in order to handle the more general
case. Along the way we show a result of general interest establishing expectation of several
rank one matrices sampled from a normal distribution intertwined with constant matrices.
2
Under review as a conference paper at ICLR 2021
•	We verify the theoretical results on various datasets and provide further understanding. We
further empirically show that the results extend to other widely used network structures and
hold for all choices of the mini-batch sizes. We also empirically verify that, on average, in
each iteration the loss function value and the generalization ability (measured by the gap
between accuracy on the training and test sets) are all decreasing functions of the mini-
batch size.
In conclusion, we study the dynamics of SGD under linear regression and a two-layer linear net-
work setting by focusing on the decreasing property of the variance of stochastic gradient estimators
with respect to the mini-batch size. The proof techniques can also be used to derive other properties
of the SGD dynamics in regard to the mini-batch size and initial weights. To the best of authors’
knowledge, the work is the first one to theoretically study the impact of the mini-batch size on the
variance of the gradient subject to the conditions on the initial weights, under mild assumptions on
the network and the loss function. We support our theoretical results by experiments. We further
experiment on other state-of-the-art deep learning models and datasets to empirically show the va-
lidity of the conjectures about the impact of mini-batch size on average loss, average accuracy and
the generalization ability of the model.
The rest of the manuscript is structured as follows. In Section 2 we review the literature while in
Section 3 we present the theoretical results on how mini-batch sizes impact the variance of stochastic
gradient estimators, under different models including linear regression and deep linear networks.
Section 4 introduces (part of) the experiments that verify our theorems and provide further insights
into the impact of the mini-batch sizes on SGD performance. We defer the complete experimental
details to Appendix A and the proofs of the theorems and other technical details to to Appendix B.
2	Literature Review
Stochastic gradient descent type methods are broadly used in machine learning (Bottou, 1991; Le-
Cun et al., 1998; Bottou et al., 2018). The performance of SGD highly relies on the choice of the
mini-batch size. It has been widely observed that choosing a large mini-batch size to train deep
neural networks appears to deteriorate generalization (LeCun et al., 2012). This phenomenon exists
even if the models are trained without any budget or limits, until the loss function value ceases to
improve (Keskar et al., 2017). One explanation for this phenomenon is that large mini-batch SGD
produces “sharp” minima that generalize worse (Hochreiter & Schmidhuber, 1997; Keskar et al.,
2017). Specialized training procedures to achieve good performance with large mini-batch sizes
have also been proposed (Hoffer et al., 2017; Goyal et al., 2017).
It is well-known that SGD has a slow asymptotic rate of convergence due to its inherent variance
(Johnson & Zhang, 2013). Variants of SGD that can reduce the variance of the stochastic gradient
estimator, which yield faster convergence, have also been suggested. The use of the information of
full gradients to provide variance control for stochastic gradients is addressed in Johnson & Zhang
(2013); Roux et al. (2012); Shalev-Shwartz & Zhang (2013). The works in Lei et al. (2017); Li
et al. (2014); Schmidt et al. (2017) further improve the efficiency and complexity of the algorithm
by carefully controling the variance.
There is prior work focusing on studying the dynamics of SGD. Neelakantan et al. (2015) propose
to add isotropic white noise to the full gradient to study the “structured” variance. The works in Li
et al. (2017); Mandt et al. (2017); Jastrzebski et al. (2017) connect SGD with stochastic differential
equations to explain the property of converged minima and generalization ability of the model.
Smith & Le (2017) propose an “optimal” mini-batch size which maximizes the test set accuracy
by a Bayesian approach. The Stochastic Gradient Langevin Dynamics (SGLD, a variant of SGD)
algorithm for non-convex optimization is studied in Zhang et al. (2017); Mou et al. (2018).
In most of the prior work about the convergence of SGD, it is assumed that the variance of stochastic
gradient estimators is upper-bounded by a linear function of the norm of the full gradient, e.g.
Assumption 4.3 in Bottou et al. (2018). Gower et al. (2019) give more precise bounds of the variance
under different sampling methods and Khaled & Richtarik (2020) extend them to smooth non-convex
regime. These bounds are still dependent on the model parameters at the corresponding iteration.
To the best of the authors’ knowledge, there is no existing result which represents the variance of
3
Under review as a conference paper at ICLR 2021
stochastic gradient estimators only using the initial weights and the mini-batch size. This paper
partially solves this problem.
3	Analysis
Mini-batch SGD is a lighter-weight version of gradient descent. Suppose that we are given a loss
function Lpwq where w is the collection (vector, matrix, or tensor) of all model parameters. At each
iteration t, instead of computing the full gradient PwLpwtq, SGD randomly samples a mini-batch
set Bt that consists of b “ |Bt | training instances and sets wt`i D wt — atVwLBt (wt), where the
positive scalar αt is the learning rate (or step size) and PwLBt pwtq denotes the stochastic gradient
estimator based on mini-batch Bt .
An important property of the stochastic gradient estimator VwLBt pwtq is that it is an unbiased
estimator, i.e. EVwLBt pwtq “ VwLpwtq, where the expectation is taken over all possible choices of
mini-batch Bt. However, it is unclear what is the value of var pVwLBt pwtqq fi E }VwLBt pwtq}2 —
}EVwLBt(wt)}2. Intuitively, we should have Var(VwLBt(wt)) 9 %~Var(VwLpwtq), where n
is the number of training samples and stochasticity on the right-hand side comes from mini-batch
samples behind wt (Smith & Le, 2017; Gower et al., 2019). However, even the quantities VwL(wtq
and Var (Vw L(wtqq are still challenging to compute as we do not have direct formulas of their
precise values. Besides, as we choose different b’s, their values are not comparable as we end up
with different wt ’s.
A plausible idea to address these issues is to represent EVwLBt (wtq and Var (VwLBt (wtqq using
the fixed and known quantities w0 , b, t, and αt . In this way, we can further discover the properties,
like decreasing with respect to b, of EVwLBt (wtq and Var (VwLBt (wtqq. The biggest challenge is
how to connect the quantities in iteration t with those of iteration 0. This is similar to discovering
the properties ofa stochastic differential equation at time t given only the dynamics of the stochastic
differential equation and the initial point.
In this section, we address these questions under two settings: linear regression and a deep linear
network. In Section 3.1 with a linear regression setting, we provide explicit formulas for calcu-
lating any norm of the linear combination of sample-wise gradients. We therefore show that the
Var (VwLBt (wtqq is a decreasing function of the mini-batch size b. In Section 3.2 with a deep linear
network setting and samples drawn from a normal distribution, we show that any trace of the product
of weight matrices and stochastic gradient estimators is a polynomial in 1{b with finite degree. We
further prove that Var (Vw LBt (wt)) is a decreasing function of the mini-batch size b > bo for some
constant b0.
For a random matrix M, we define Var (Mq fi E }vec(M q}2 — }Evec(M q}2 where vec(M q denotes
the vectorization of matrix M. We denote [m : n] fi {m, m ' 1,...,n} if m ≤ n, and H otherwise.
We use rns fi r1 : ns as an abbreviation. For clarity, we use the superscript b to distinguish the
variables with different choices of the mini-batch size b. In each iteration t, we use Btb to denote
the batch of samples (or sample indices) to calculate the stochastic gradient. We denote by Ftb the
filtration of information before calculating the stochastic gradient in the t-th iteration, i.e. Ftb fi
{ wo, B0,..., B"/.
3.1	Linear Regression
In this subsection, we discuss the dynamics of SGD applied in linear regression. Given data points
(x1,y1),…，(xn,yn), where Xi P Rp and y% P R, We define the loss function to be Lpwq =
n ∑n=ι Lipwq = n ∑n= ι 2 'wτXi — yi)2, where w P Rp are the model parameters. We consider
minimizing L(wq by mini-batch SGD. Note that the bias term in the general linear regression models
is omitted, however, adding the bias term does not change the result of this section. Formally, we first
choose a mini-batch size b and initial weights wo. In each iteration t, we sample Btb, a subset of rns
with cardinality b, and update the parameters by wtb`i = wb — atgt, where g? = 1b ∑ipBg VLi (wb).
We first show the relationship between the variance of stochastic gradient gtb and the full gradient
VL wtb and sample-wise gradient VLi wtb , i P rns, derived by considering all possible choices
4
Under review as a conference paper at ICLR 2021
of the mini-batch Btb . Readers should note that Lemma 1 actually holds for all models with L2 -loss,
not merely linear regression (since in the proof we do not need to know the explicit form of Li pwq).
Lemma 1. Let Cb fi bn´Lq 20. For any matrix A P Rpxp we have var 'Agb∣Fb) “
E IMgb>2∣Fb] ´ >AVL (wb)>2 “ Cb ´n ∑n=ι >AVLi (wb)>2 ´ >AVL 僧)>2) ∙
Lemma 1 provides a bridge to connect the norm and variance of gtb with sample-wise gradients
VLi (Wb), i P [n]. Therefore, if We can further discover the properties of VLi (Wb), i p [n], we
are able to calculate the variance ofgtb. Lemma 2 addresses this problem by showing the relationship
between any linear combination of VLi (Wbys and VLi (wb_J 's.
Lemma 2. For any set of square matrices {Aι, ∙∙∙ ,An} P Rpxp, if we denote A “
∑n=1 AixixT, then we have E ∣∣∑n=1 AiVLi(W品)>2同=E ∣∣∑n=1 BiVLi (wb)>2同 +
α2Cb ∑n=ι ∑Lι E [>∑n=ι BkVLi (wb)>2∣Fol, where Bi = Ai ´ ɑA； Bkl= A if i = k,i ‰ l,
Bikl = A if i = l, i ‰ k, and Bikl equals the zero matrix, otherwise.
Lemma 2 provides the tool to reduce the iteration t by one. Therefore, we can easily use it to recur-
sively calculate the norm of any linear combinations of the sample-wise gradients, for all iterations
t. Combining the fact that Cb is a decreasing function of b, we are able to show Theorem 1.
Theorem 1. For any t P N and any matrices Ai P Rpxp, i P rnS, E ∣>∑Nι AiVLi (wb) ∣∣2∣Fo] is
a decreasing function of b for b P rns.
Theorem 1 states that the norm of any linear combinations of the sample-wise gradients is a de-
creasing function of b. Combining Lemma 1 which connects the variance of gtb with the linear
combination of VLi (wb),s, and the fact that VL (wb) = ɪ ∑Nι VLi (wb), we have Theorem 2.
Theorem 2. Fixing initial weights W0, both var Bgtb ∣F0 and var BVL Wtb ∣F0 are decreasing
functions Ofmini-batch size b for all b P rnS, t P N, and all square matrices B P RpXp.
As a special case, Corollary 1 guarantees that the variance of the stochastic gradient estimator is a
decreasing function of b.
Corollary 1. Fixing initial weights W0, both var gtb ∣F0 and var VL Wtb ∣F0 are decreasing
functions of mini-batch size b for all b P rns and t P N.
In conclusion, we provide a framework for calculating the explicit value of variance of the stochastic
gradient estimators and the norm of any linear combination of sample-wise gradients. We further
show that the variance of both the full gradient and the stochastic gradient estimator are a decreasing
function of the mini-batch size b. Readers should note that the framework here is not limited to
showing the decreasing property of the variance, but can also be used in many other circumstance.
For example, we can use Lemma 2 to induct on t and easily show that E ∣>∑Nι AiVLi (Wb) >2 ∣Fo]
is a polynomial of b with degree at most t and estimate the coefficients therein.
3.2	Deep Linear Networks with Online Setting
In this section, we study the dynamics of SGD on deep linear networks. We take the two-layer
linear network as an example while the results and proofs can be easily extended to deep lin-
ear network with any depth (see Appendix B.3 for more details). We consider the population
loss L(W) = Ex„N(0,Ip)II }W2Wιx ´ W2swrx}2]
under the teacher-student learning frame-
work (Hinton et al., 2015) with W = (Wι,W2) a tuple of two matrices. Here Wi P Rp+p
and W2 P Rp2 xp1 are parameter matrices of the student network and W： and W2^ are the fixed
ground-truth parameters of the teacher network. We use online SGD to minimize the population
loss L(W). Formally, we first choose a mini-batch size b and initial weight matrices tW0,1, W0,2u.
In each iteration t, we draw b independent and identically distributed samples xt,i , i P rbs from
N(0, Ip) to form the mini-batch Bb and update the weight matrices by 卬九」= Wibι — αtgb,ι and
5
Under review as a conference paper at ICLR 2021
Wtb'l,2 “ Wtb2 — αtgb2, where
gb,ι “ b * Vwb ^ 2 >Wb"Wb,ιXt,i ´ W}w?xt,if} “ 1 W Wtb2T (w%wb,ι ´ W^ W*) x^x，，⑴
i“1	t,1	i“1
gb,2 “ 1 W Vwb ^2 >Wb,2Wb,1Xt,i ´ W*W*xt,i>2) “ 1 W (wb,2Wib,1 ´ W*W*) xt,ixTiWb,ιT ⑵
i“1	t,2	i“1
The derivation follows from the formulas in Petersen & Pedersen (2012). In the following, we use
Wb “ Wjb 2 Wjb ι — Wξ Wj to denote the gap between the product of model weights and ground-truth
weights.
For ease of developing our proofs, we first introduce the definition of a multiplicative term in Def-
inition 1. Intuitively, a multiplicative term is a matrix which equals to the product of its parameter
matrices and constant matrices (and their transpose). The degree of a matrix A in a multiplicative
term M is the number of appearance of A and AT in M . The degree of M is exactly the number of
appearances of all weight matrices in M .
Definition 1. For any set of matrices S, we denote Ss “ S Y tMT : M P Su. Given a set of
parameter matrices X “ {X1,X2,…，Xnv} and constant matrices C “ {C1,C2,…，g,c}, We
say that a matrix M is a multiplicative term of parameter matrices X and constant matrices C
if it can be written in the form of M “ M(X, C) “ 口：=] Ai, where Ai P X Y C. We write
deg(Xj; M) “ Xk=ι '1 {Xj = Ai} ' 1 { Xj “ AT() ,j P [nv] as the degree of parameter matrix
Xj in M, deg(Cj; M) “ Xk=ι '1 {Cj = Ai} ' 1 { Cj = AT}) ,j P [&] as the degree of constant
matrix Cj in M, and deg(M) = Xk=ι 1 { Ai P X( “ Xn: 1 deg(Xj; M) as the total degree of the
parameter matrices of M .
As pointed out in the Section 1, the difficulty of studying the dynamics of SGD is how to connect the
quantities in iteration t with fixed variables, like initial weights W0,1, W0,2 and mini-batch size b. We
overcome this challenge by carefully calculating the relationship between gb`i,i and g^,i, i “ 1, 2 so
that we can reduce the iteration t step by step. With the help of Lemmas 8 and 9 in Appendix B.2,
we can represent g" i,i = 1, 2 using multiplicative terms of g1i,i = 1,2 and some other constant
matrices. Theorem 3 precisely gives the representation in the form of a polynomial of ∣ and the
coefficients as the sum of multiplicative terms of parameter matrices W01,1, W01,2 and constant
matrices tW1j , W2j }.
Theorem 3.	Given t 20, for any multiplicative terms Mi, i P [0: m] of parameter matrices
gj1,1, gj1,2 and constant matrices Wj1,1, Wj1,2, W1j, W2j with degree di, respectively, we denote
M “ ∏mtι tr (Mi) Mo, d = ∑mto di and d1 “ Xm“0 'deg (Wtb1； Mi) ' deg(Wt,2; Mi)) .There
exists a set of multiplicative terms Mikj , i P rmks, j P r0 : mkis , k P r0 : qs of parameter matrices
{ W0),1, W12( and Constant matrices tW↑^,W2j} Such that E [M|Fo] “ N ' NI^ + …+ Nqbq,
where Nk “ XmkI ∏mmki tr (Mij) M⅛, k P [0 : q]. Here mk,mhi and q ≤ 1 (3t'1 — 1)d ' 1 (3t —
1)d1 are constants independent of b, and Xm“0 deg (Mij) ≤ 3t(3d ' d1).
As a special case of Theorem 3, Theorem 4 shows that the variance of the stochastic gradient esti-
mators is also a polynomial of 1 but with no constant term. This backs the important intuition that
the variance is approximately inversely proportional to the mini-batch size b. Besides, note that if
we consider b → 8, intuitively we should have Var (g1,i∣Fo) → 0, i = 1, 2. This observation aligns
with the statement of Theorem 4.
Theorem 4.	Given t20, value Var (gb,i∣Fo), i = 1,2 can be written as a polynomial of 1 with
degree at most 2 ∙ 3t with no constant term. Formally, we have Var (gb,,i∣Fo) = βι∣ '∙∙∙' βr∙1,
where r ≤ 2 ∙ 3t'1 and each βi is a constant independent of b.
One should note that the polynomial representation of Var gtb,i∣F0 , i = 1, 2 does not have the
constant term. Therefore, to show the that the variance is a decreasing function of b, we only need
to show that the leading coefficient β1 is non-negative. This is guaranteed by the fact that variance
is always non-negative. We therefore have Theorem 5.
6
Under review as a conference paper at ICLR 2021
Theorem 5.	Given t P N, there exists a constant b0 such that for all b > b0 function
Var (gb,i∣F0) ,i = 1, 2 is a decreasing function of b.
The constant b0 is the largest root of the equation β1 br´1 + β2br´2 + ∙ ∙ ∙ + βr = 0. See the proof
of Theorem 5 in Appendix B.2 for more details. Although we cannot calculate the precise value of
b0 , we verify that b0 is smaller than 1 in many experiments. From the proofs we conclude that the
scale of each βi is of the order O p}M}q, where M is a multiplicative term of parameter matrices
{W0,1, W0,2, W*, Wl U and constant matrix H with degree 2 ∙ 3t'1.
Unlike the linear regression setting where we can iteratively calculate the variance by Lemma 2, the
closed form expressions for the variance of the stochastic gradients in the deep linear network setting
are much harder to calculate. However, we are able to iteratively deducing t one by one and provide
a polynomial representation for any multiplicative terms of parameter matrices gtb,i, Wtb,i, i “ 1, 2
and constant matrices {W1*, W2*u using only the initial weights W0,1, W0,2 and the mini-batch size
b. As We further study the polynomial representation of Var (gbi∣F0),i = 1, 2, we are also able to
show the decreasing property of the variance of stochastic gradient estimators with respect to b.
4	Experiments
In this section, we present numerical results to support the theorems in Section 3 and provide further
insights into the impact of the mini-batch size on the dynamics of SGD. The experiments are con-
ducted on four datasets and models that are relatively small due to the computational cost of using
large models and datasets. We only report the results on the MNIST dataset here due to the limited
space. A complete empirical study is deferred in Appendix A.
For all experiments, we perform mini-batch SGD multiple times starting from the same initial
weights and following the same choice of the learning rates and other hyper-parameters, if applica-
ble. This enables us to calculate the variance of the gradient estimators and other statistics in each
iteration, where the randomness comes only from different samples of SGD.
4.1	Results on MNIST Dataset
The MNIST dataset is to recognize digits in handwritten images of digits. We use all 60,000 training
samples and 10,000 validation samples of MNIST. We build a three-layer fully connected neural
network with 1024, 512 and 10 neurons in each layer. For the two hidden layers, we use the ReLU
activation function. The last layer is the softmax layer which gives the prediction probabilities for
the 10 digits. We use mini-batch SGD to optimize the cross-entropy loss of the model. The model
deviates from our analytical setting since it has non-linear activations, it has the cross-entropy loss
function (instead of L2), and empirical loss (as opposed to population). MNIST is selected due to
its fast training and popularity in deep learning experiments. The goal is to verify the results in this
different setting and to back up our hypotheses.
(a) Different initial weights
(b) Log of loss
(c) Log of error
(d) Gap of accuracy (zoomed-in)
Figure 1: Experimental results for the MNIST dataset. (a) The median, min, and max of the log of variance of
the stochastic gradient estimators for two different mini-batch sizes (distinguished by colors) and five different
initial weights. The solid lines show the median of all five initial weights while the highlighted regions show
the min and max of the log of variance. (b) The log of the training and validation loss vs epochs. (c) The log of
training and validation error vs epochs. Here error is defined as one minus predicting accuracy. The plot does
not show the epochs if error equals to zero. (d) The gap of accuracy on training and test sets vs epochs starting
from epoch 100.
7
Under review as a conference paper at ICLR 2021
As shown in Figure 1(a), we run SGD with two batch sizes 64 and 128 on five different initial
weights with 50 runs for each initial point. This plot shows that, even the smallest value of the
variance among the five different initial weights with a mini-batch size of 64, is still larger than the
largest variance of mini-batch size 128. We observe that the sensitivity to the initial weights is not
large. This plot also empirically verifies our conjecture in the introduction that the variance of the
stochastic gradient estimators is a decreasing function of the mini-batch size, for all iterations of
SGD in a general deep learning model.
In addition, we also conjecture that there exists the decreasing property for the expected loss, error
and the generalization ability with respect to the mini-batch size. Figure 1(b) shows that the expected
loss (again, randomness comes from different runs of SGD through the different mini-batches with
the same initial weights and learning rates) on the training set is a decreasing function of b. However,
this decreasing property does not hold on the validation set when the loss tends to be stable or
increasing, in other words, the model starts to be over-fitting. We hypothesize that this is because
the learned weights start to bounce around a local minimum when the model is over-fitting. As the
larger mini-batch size brings smaller variance, the weights are closer to the local minimum found
by SGD, and therefore yield a smaller loss function value. Figure 1(c) shows that both the expected
error on training and validation sets are decreasing functions of b.
Figure 1(d) exhibits a relationship between the model’s generalization ability and the mini-batch
size. As suggested by Simard et al. (2013), we build a test set by distorting the 10,000 images of the
validation set. The prediction accuracy is obtained on both training and test sets and we calculate
the gap between these two accuracies every 100 epochs. We use this gap to measure the model
generalization ability (the smaller the better). Figure 1(d) shows that the gap is an increasing function
of b starting at epoch 500, which partially aligns with our conjecture regarding the relationship
between the generalization ability and the mini-batch size. We test this on multiple choices of the
hyper-parameters which control the degree of distortion in the test set and this pattern remains clear.
5	Summary and Future Work
We examine the impact of the mini-batch size on the dynamics of SGD. Our focus is on the variance
of stochastic gradient estimators. For linear regression and a two-layer linear network, we are able to
theoretically prove that the variance conjecture holds. We further experiment on multiple models and
datasets to verify our claims and their applicability to practical settings. Besides, we also empirically
address the conjectures about the expected loss and the generalization ability.
A challenging research direction is to theoretically investigate the impact of the mini-batch size on
the generalization ability. There are existing works studying the relationship between the variance
of the stochastic gradients and the generalization ability (Gorbunov et al., 2020; Meng et al., 2016).
Together with the tools developed herein, it would be possible to bridge the mini-batch size with the
generalization ability of a neural network. We can further choose an optimal mini-batch size which
minimizes the generalization ability by solving the polynomial equation if we have more precise
estimations of the coefficients.
Another appealing direction is using our variance estimations to develop better variance reduction
methods. As a results, the upper-bound of the variance decides the convergent rate of these al-
gorithms. Researchers usually assume a much larger upper-bound at each iteration, like a linear
function of the norm of the full gradient. With the help of our techniques, we should calculate the
variance more precisely and further improve the algorithms.
Further interesting work is to extend our techniques to more complicated and sophisticated networks.
Although the underlying model of this paper corresponds to deep linear network networks, we are
able to show a deeper relationship between the variance and the mini-batch size, the polynomial in
1{b, while the common knowledge is simply that the variance is proportional to 1{b. The extension to
other optimization algorithms, like Adam and Gradient Boosting Machines, are also very attractive.
We hope our theoretical framework can serve as a tool for future research of this kind.
8
Under review as a conference paper at ICLR 2021
References
Mohan S Acharya, Asfia Armaan, and Aneeta S Antony. A comparison of regression models for pre-
diction of graduate admissions. In 2019 International Conference on Computational Intelligence
in Data Science,pp.1-5, 2019.
Zeyuan Allen-Zhu and Elad Hazan. Variance reduction for faster non-convex optimization. In
International conference on machine learning, pp. 699-707, 2016.
Leon Bottou. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nimes, 91(8):
12, 1991.
Leon Bottou. Online learning and stochastic approximations. On-line Learning in Neural Networks,
17(9):142, 1998.
Leon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Jianqing Fan, Cong Ma, and Yiqiao Zhong. A selective overview of deep learning. arXiv preprint
arXiv:1904.05526, 2019.
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.
Eduard Gorbunov, Filip Hanzely, and Peter Richtarik. A unified theory of sgd: Variance reduc-
tion, sampling, quantization and coordinate descent. In International Conference on Artificial
Intelligence and Statistics, pp. 680-690, 2020.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Peter
Richtarik. SGD: General analysis and improved rates. In International Conference on Machine
Learning, pp. 5200-5209, 2019.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training Imagenet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Sepp Hochreiter and JUrgen Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997.
Elad Hoffer, Itay Hubara, and Daniel Soudry. Train longer, generalize better: closing the generaliza-
tion gap in large batch training of neural networks. In Advances in Neural Information Processing
Systems, pp. 1731-1741, 2017.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in SGD. arXiv preprint
arXiv:1711.04623, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013.
Nitish Shirish Keskar, Jorge Nocedal, Ping Tak Peter Tang, Dheevatsa Mudigere, and Mikhail
Smelyanskiy. On large-batch training for deep learning: Generalization gap and sharp minima.
In 5th International Conference on Learning Representations, 2017, 2017.
Ahmed Khaled and Peter Richtarik. Better theory for sgd in the nonconvex world. arXiv preprint
arXiv:2002.03329, 2020.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied
to document recognition. Proceedings of the Institute of Electrical and Electronics Engineers, 86
(11):2278-2324, 1998.
9
Under review as a conference paper at ICLR 2021
Yann LeCun, Leon Bottou, Genevieve B Orr, and KlaUs-Robert Muller. Efficient backprop. In
Neural networks: Tricks ofthe trade, pp. 9-48. Springer, 2012.
Lihua Lei, Cheng Ju, Jianbo Chen, and Michael I Jordan. Non-convex finite-sum optimization via
SCSG methods. In Advances in Neural Information Processing Systems, pp. 2348-2358, 2017.
Mu Li, Tong Zhang, Yuqiang Chen, and Alexander J Smola. Efficient mini-batch training for
stochastic optimization. In Proceedings of the 20th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, pp. 661-670, 2014.
Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive stochastic
gradient algorithms. In Proceedings of the 34th International Conference on Machine Learning,
pp. 2101-2110. PMLR, 2017.
Jan R Magnus. The moments of products of quadratic forms in normal variables. Instituut voor
Actuariaat en Econometrie, 1978.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907, 2017.
Qi Meng, Yue Wang, Wei Chen, Taifeng Wang, Zhi-Ming Ma, and Tie-Yan Liu. Generalization
error bounds for optimization algorithms via stability. arXiv preprint arXiv:1609.08397, 2016.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of SGLD for non-
convex learning: Two theoretical viewpoints. In Conference On Learning Theory, pp. 605-638,
2018.
Arvind Neelakantan, Luke Vilnis, Quoc V Le, Ilya Sutskever, Lukasz Kaiser, Karol Kurach, and
James Martens. Adding gradient noise improves learning for very deep networks. arXiv preprint
arXiv:1511.06807, 2015.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-
performance deep learning library. In Advances in Neural Information Processing Systems, pp.
8024-8035, 2019.
K. B. Petersen and M. S. Pedersen. The matrix cookbook, 2012. Version 20121115.
Nicolas L Roux, Mark Schmidt, and Francis R Bach. A stochastic gradient method with an expo-
nential convergence rate for finite training sets. In Advances in Neural Information Processing
Systems, pp. 2663-2671, 2012.
Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing finite sums with the stochastic
average gradient. Mathematical Programming, 162(1-2):83-112, 2017.
Shai Shalev-Shwartz and Tong Zhang. Stochastic dual coordinate ascent methods for regularized
loss minimization. Journal of Machine Learning Research, 14(Feb):567-599, 2013.
P. Y. Simard, D. Steinkraus, and J. C. Platt. Best practices for convolutional neural networks applied
to visual document analysis. In Seventh International Conference on Document Analysis and
Recognition, pp. 958-963, 2013.
Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic gradient
descent. arXiv preprint arXiv:1710.06451, 2017.
Ruoyu Sun. Optimization for deep learning: theory and algorithms. arXiv preprint
arXiv:1912.08957, 2019.
Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic
gradient optimization. In Advances in Neural Information Processing Systems, pp. 181-189,
2013.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in Neural
Information Processing Systems, pp. 5754-5764, 2019.
10
Under review as a conference paper at ICLR 2021
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
Sification. In Advances in Neural Information Processing Systems, pp. 649-657, 2015.
Yuchen Zhang, Percy Liang, and Moses Charikar. A hitting time analysis of stochastic gradient
langevin dynamics. In Conference on Learning Theory, pp. 1980-2022, 2017.
11
Under review as a conference paper at ICLR 2021
A	Experiments
In this section, we present numerical results to support the theorems in Section 3, to backup the
hypotheses discussed in the introduction, and provide further insights into the impact of the mini-
batch size on the dynamics of SGD. The experiments are conducted on four datasets and models
that are relatively small due to the computational cost of using large models and datasets.
Remark: We cannot present the complete numerical results in the main paper due to the space limit.
Therefore, we move the whole experimental section to Appendix. In order to keep a smooth reading,
some of the content is overlapping with Section 4.
A. 1 Datasets and Settings
For all experiments, we perform mini-batch SGD multiple times starting from the same initial
weights and following the same choice of the learning rates and other hyper-parameters, if applica-
ble. This enables us to calculate the variance of the gradient estimators and other statistics in each
iteration, where the randomness comes only from different samples of SGD. The learning rate αt is
selected to be inversely proportional to iteration t, or fixed, depending on the task at hand.
All models are implemented using PyTorch version 1.4 (Paszke et al., 2019) and trained on NVIDIA
2080Ti/1080 GPUs. We have also tested several other random initial weights and ground-truth
weights, and learning rates, and the results and conclusions are similar and not presented.
A.1.1 Graduate Admission Dataset
The Graduate Admission dataset1 (Acharya et al., 2019) is to predict the chance of a graduate ad-
mission using linear regression. The dataset contains 500 samples with 6 features and is normalized
by mean and variance of each feature. This is a popular regression dataset with clean data. We build
a linear regression model to predict the chance of acceptance (we include the intercept term in the
model) and minimize the empirical L2 loss using mini-batch SGD, as stated in Section 3.1.
For the experiment in Figure 2(a), we randomly select an initial weight vectors w0 and run SGD for
2,000 iterations where it appears to converge. We record all statistics at every iteration. There are in
total 1,000 runs behind each observation which yields a p-value lower than 0.05. As for Figure 2(b),
we select 20 different b’s and run SGD from the same initial point for 40 iterations. There are in total
of 200,000 runs to make sure the p-value of all statistics are lower than 0.05. In all experiments, the
learning rate is chosen to be at “ *,t P [2000] because this rate yields a theoretical convergence
guaranteed (factor 1/2 has been fine tuned). The purpose of this experiment is to empirically study
the rate of decrease of the variance. The theoretical study exhibited in Section 3.1 establishes the
non-increasing property but it does not state anything about the rate of decrease.
A.1.2 Synthetic Dataset
We build a synthetic dataset of standard normal samples to study the setting in Section 3.2. We
fix the teacher network with 64 input neurons, 256 hidden neurons and 128 output neurons. We
optimize the population L2 loss by updating the two parameter matrices of the student network
using online SGD, as stated in Section 3.2. In this case we have proved the functional form of the
variance as a function ofb and show the decreasing property of the variance of the stochastic gradient
estimators for large mini-batch sizes. However, we do not show the decreasing property for every b.
With this experiment we confirm that the conjecture likely holds. In the experiment, we randomly
select two initial weight matrices W0,1, W0,2 and the ground-truth weight matrices W*,W2^. We
run SGD for 1,000 iterations which appears to be a good number for convergence while there are
1,000 runs of SGD in total to again give a p-value below 0.05. We record all statistics at every
iteration. The learning rate is chosen to be at “ ɪɪt ,t P [1000] for the same reason as in the
regression experiment.
1https://www.kaggle.com/mohansacharya/graduate-admissions
12
Under review as a conference paper at ICLR 2021
A.1.3 MNIST DATASET
The MNIST dataset is to recognize digits in handwritten images of digits. We use all 60,000 training
samples and 10,000 validation samples of MNIST. The images are normalized by mapping each
entry to r´1, 1s. We build a three-layer fully connected neural network with 1024, 512 and 10
neurons in each layer. For the two hidden layers, we use the ReLU activation function. The last
layer is the softmax layer which gives the prediction probabilities for the 10 digits. We use mini-
batch SGD to optimize the cross-entropy loss of the model. The model deviates from our analytical
setting since it has non-linear activations, it has the cross-entropy loss function (instead of L2), and
empirical loss (as opposed to population). MNIST is selected due to its fast training and popularity
in deep learning experiments. The goal is to verify the results in this different setting and to back up
our hypotheses.
We run SGD for 1,000 epochs on the training set which is enough for convergence. The learning
rate is a constant set to 3 ∙ lθ´3 (which has been tuned). For the experiment in Figure 5, there are
in total 100 runs to give us the p-value below 0.05. For the experiment in Figure 4(a), we randomly
select five different initial points and we have 50 runs for each initial point. For the experiment
corresponding to Figure 4(b), we choose α “ 8 and σ “ 2 as in Simard et al. (2013). The initial
weights and other hyper-parameters are chosen to be the same as in Figure 5.
A.1.4 Yelp Review Dataset
The Yelp Review dataset from the Yelp Dataset Challenge (Zhang et al., 2015) contains 1,569,264
samples of customer reviews with positive/negative sentiment labels. We use 10,000 samples as our
training set and 1,000 samples as the validation set. We use XLNet (Yang et al., 2019) to perform
sentiment classification on this dataset. Our XLNet has 6 layers, the hidden size of 384, and 12
attention heads. There are in total 35,493,122 parameters. We intentionally reduce the number of
layers and hidden size of XLNet and select a relatively small size of the training and validation sets
since training of XLNet is very time-consuming (Yang et al. (2019) train on 512 TPU v3 chips for
5.5 days) and we need to train the model for multiple runs. This setting allows us to train our model
in several hours on a single GPU card. We train the model using the Adam weight decay optimizer,
and some other techniques, as suggested in Table 8 of Yang et al. (2019). This dataset represents
sequential data where we further consider the hypotheses.
We randomly select a set of initial parameters and run Adam with two different mini-batch sizes of
32 and 64. For computational tractability reasons, for each mini-batch size there are in total of 100
runs and each run corresponds to 20 epochs. We record the variance of the stochastic gradient, loss
and accuracy in every step of Adam. The statistics reported in Figure 6 are averaged through each
epoch. In all experiments, the learning rate is set to be 4 ∙ lθ´5 and the E parameter of Adam is set to
be 10—8 (these two have been tuned). The stochastic gradients of all parameter matrices are clipped
with threshold 1 in each iteration. We use the same setup for the learning rate warm-up strategy as
suggested in Yang et al. (2019). The maximum sequence length is set to be 128 and we pad the
sequences with length smaller than 128 with zeros.
A.2 Discussion
As observed in Figure 2(a), under the linear regression setting with the Graduate Admission dataset,
the variance of the stochastic gradient estimators and full gradients are all strictly decreasing func-
tions of b for all iterations. This result verifies the theorems in Section 3.1. Figure 2(b) further
studies the rate of decrease of the variance. From the proofs in Section 3.1 We see that Var (gb∣F⅛)
is a polynomial of b with degree t ' 1. Therefore, for every t, we can approximate this polynomial
by sampling many different b’s and calculate the corresponding variances. We pick b to cover all
numbers that are either a power of2 or multiple of40 in r2, 500s (there are a total of21 such values)
and fit a polynomial with degree 6 (an estimate from the analyses) at t “ 10, 20, 30, 40. Figure 2(b)
shows the fitted polynomials. As we observe, the value var gtb ∣F0 (approximated by the value of
the polynomial) is both decreasing with respect to the mini-batch size b and iteration t. Further, the
rate of decrease in b is slower as the b increasing. This provides a further insight into the dynamics
of training a linear regression problem with SGD.
13
Under review as a conference paper at ICLR 2021
≡- batch 4, full grad
•— batch 16, full grad
— batch 64, full grad
--- batch 256r full grad
batch 4, stochastic grad
batch 16, stochastic grad
batch 64l stochastic grad
batch 256r stochastic grad
0	250	500	750	1000 1250 1500 1750 2000
Iteration t
(a) Variance of stochastic gradients and full gradients
0	IOO 200	300	400	500
Mini-batch size b
(b) Fitting polynomials of mini-batch size b
Ig6 PUe (v)7EAJo ① UUeμe>M-O 6o^∣
batch size 4; full grad
-- batch size 16; full grad
----batch size 64; full grad
—— batch size 256; full grad
batch 4f stochastic grad
batch 16, stochastic grad
batch 64, stochastic grad
batch 256, stochastic grad
Figure 2: Experimental results for the Graduate Admission dataset. Left: log (Var (gb∣F0)) and
log (Var (VLPwbq ∣F0)) vs iteration t for 4 different mini-batch sizes. Right: The log of polynomial Val-
ues when fitting polynomials on selected mini-batch sizes at certain iterations.
10
_ _ _ _
2 O-2-4
Zg 6 PUe (v)7wam-O ① UUeμe> JO 6o^∣
0	200	400	600	800	1000
Iteration t
(a)	Variance of gradients with respect to W1
0	200	400	600	800	1000
Iteration t
(b)	Variance of gradients with respect to W2
Figure 3:	Experimental results for the Synthetic dataset. Left: log Var gtb,1 ∣F0
and log (Var(VWIL(We1, Wtb2) ∣F0))	VS iteration	t.	Right:	log `var (gb,2∣F0))	and
log (Var(V w2 L(Wtb,1 ,W1,2) ∣F0)) VS iteration t.
Under the two-layer linear network setting with the synthetic dataset, Figure 3 Verifies that the
Variance of the stochastic gradient estimators and full gradients are all strictly decreasing functions
of b for all iterations. This figure also empirically shows that the constant b0 in Theorem 5 could
be as small as b0 “ 4. In fact, we also experiment with the mini-batch size of 1 and 2, and the
decreasing property remains to hold. We also test this on multiple choices of initial weights and
learning rates and this pattern remains clear.
In aforementioned two experiments we use SGD in its original form by randomly sampling mini-
batches. In deep learning with large-scale training data such a strategy is computationally prohibitiVe
and thus samples are scanned in a cyclic order which implies fixed mini-batches are processed
many times. Therefore, in the next two datasets we perform standard “epoch” based training to
empirically study the remaining two hypotheses discussed in the introduction (decreasing loss and
error as a function of b) and sensitiVity with respect to the initial weights. Note that we are using
cross-entropy loss in the MNIST dataset and the Adam optimizer in the Yelp dataset and thus these
experiments do not meet all of the assumptions of the analysis in Section 3.
As shown in Figure 4(a), we run SGD with two batch sizes 64 and 128 on fiVe different initial
weights. This plot shows that, eVen the smallest Value of the Variance among the fiVe different
initial weights with a mini-batch size of 64, is still larger than the largest Variance of mini-batch size
128. We obserVe that the sensitiVity to the initial weights is not large. This plot also empirically
Verifies our conjecture in the introduction that the Variance of the stochastic gradient estimators is
14
Under review as a conference paper at ICLR 2021
---- batch size 64f median
---- batch size 128, median
0	200	400	600	800	1000
Epochs
(a)	Different initial weights
>U2⊃UUTO⅞d(u5
0.049
0.048
0.047
0.046
0.045
0.044
0.043
100	200	300	400	500	600	700	800	900
Epochs
(b)	Gap of accuracy (zoomed-in)
Figure 4: Experimental results for the MNIST dataset. Left: The median, min, and max of the log of variance
of the stochastic gradient estimators for two different mini-batch sizes (distinguished by colors) and five dif-
ferent initial weights. The solid lines show the median of all five initial weights while the highlighted regions
show the min and max of the log of variance. Right: The gap of accuracy on training and test sets vs epochs
starting from epoch 100.
Figure 5: Experimental results for the MNIST dataset. Left: The log of the training and validation loss vs
epochs. Right: The log of training and validation error vs epochs. Here error is defined as one minus predicting
accuracy. The plot does not show the epochs if error equals to zero.
(a) Variance of stochastic gradients
(b) Training and validation loss
(c) Training minus validation error
Figure 6: Experimental results for the XLNet model on the Yelp dataset. Left: The variance of stochastic
gradient estimators vs epochs. Middle: The training and validation loss vs epochs. Right: The training and
validation error vs epochs.
a decreasing function of the mini-batch size, for all iterations of SGD in a general deep learning
model.
In addition, we also conjecture that there exists the decreasing property for the expected loss, error
and the generalization ability with respect to the mini-batch size. Figure 5(a) shows that the expected
15
Under review as a conference paper at ICLR 2021
loss (again, randomness comes from different runs of SGD through the different mini-batches with
the same initial weights and learning rates) on the training set is a decreasing function of b. However,
this decreasing property does not hold on the validation set when the loss tends to be stable or
increasing, in other words, the model starts to be over-fitting. We hypothesize that this is because
the learned weights start to bounce around a local minimum when the model is over-fitting. As the
larger mini-batch size brings smaller variance, the weights are closer to the local minimum found
by SGD, and therefore yield a smaller loss function value. Figure 5(b) shows that both the expected
error on training and validation sets are decreasing functions of b.
Figure 4(b) exhibits a relationship between the model’s generalization ability and the mini-batch
size. As suggested by (Simard et al., 2013), we build a test set by distorting the 10,000 images of the
validation set. The prediction accuracy is obtained on both training and test sets and we calculate
the gap between these two accuracies every 100 epochs. We use this gap to measure the model
generalization ability (the smaller the better). Figure 4(b) shows that the gap is an increasing function
of b starting at epoch 500, which partially aligns with our conjecture regarding the relationship
between the generalization ability and the mini-batch size. We also test this on multiple choices of
the hyper-parameters which control the degree of distortion in the test set and this pattern remains
clear.
Figure 6 shows the similar phenomenon that the variance of stochastic estimators and the expected
loss and error on both training and validation sets are decreasing functions of b even if we train
XLNet using Adam. This example gives us confidence that the decreasing properties are not merely
restricted on shallow neural networks or vanilla SGD algorithms. They actually appear in many
advanced models and optimization methods.
B Lemmas and Proofs
B.1 Lemmas and Proofs of Results in Section 3.1
For two matrices A, B with the same dimension, we define the inner product xA, By fi tr AT B .
Lemma 3. Suppose that fpxq and gpxq are both smooth, non-negative and decreasing functions of
x P R. Then hpxq “ f pxqgpxq is also a non-negative and decreasing function of x.
Proof. It is obvious that hpxq is non-negative for all x. The first-order derivative of h is
h1(x) = f1(x)g(x) ' f (x)g1(x) ≤ O,
and thus hpxq is also a decreasing function of x.
□
ProofofLemma 1. Throughout the paper, We use Cn “ ^^!_k)! to denote the combinatorial num-
ber. Note that
E Igb (gb)T∣Fb] “ b12E -X VLi(Wb) X RLi'wIt)T Fb
[ipBb	ipBt
1
b2
n
XVLi `wtb) VLi `wtb)
i“1
b´2
T l Cn—2
+ CnF
X VLi (wb) VLj(Wb )T)
i‰j
“ b12(n W VLi(Wb) VLi(Wb)T ` nn⅛¾ XVLi(Wb) VLj(Wb)T)
“ b2 (n≡¾ ∑ VLi(Wb) VLi(Wb)T ` nn≡¾ X VLi(Wb)∑ VLi(Wb)T)
“ ⅛⅛ ∑ VLi (Wb) VLi (Wb)T+b≡⅞ vl (Wb) vl (Wb)T.
16
Under review as a conference paper at ICLR 2021
For any A P Rp” We have
E [>Agb>2Fb] “ E ∣(gb)T AT AgbFl= E ∣tr((gb)T AT Agb)F)]
“ E jtr (ATAgb H)T)∣Fbl
=tr (ATAElgb 'gb)T∣Fbl)
“tr (bnn⅛q E ATAVLi (Wb) VLi (Wb)T + bpn´´^ATAVL (Wb) VL (Wb)T)
=bnn⅛ ∑ > AVLi(Wb) >2 + bpnz⅞ >AVL (Wb) >2
=Cb (1 E > AVLi (Wb) >2 TAVL (Wb) >2) + >avl (Wb) >2.
Therefore, We have
Var(AgbFb)= E [>Agb>2Fb] ´ >E [AgbFb‰>2
“E [∣Agb>2Fb1 ´ >AVL (Wb)>2
=Cb (1 EMVLi(Wb)>2 ´ >avl (Wb)>2).
□
Lemma 4. For any set of square matrices {Aι,…，A" P Rpxp, ifwe denote A = Xn“1 AixixT,
then we have
X AiyLi (wb`i)	Fo	“ E
i“1	> ∣
E
n	>2∣	2
n	n	> n	>2∣
∑ BiVLi (Wb)	Fo ' 号 ∑ ∑ E ∑ BklVLi (Wb)	Fo
i“1	> ∣	n	k“1	l“1	>i“1	> ∣
Here Bi = Ai —詈 A; Bkl = A if i = k,i ‰ l, BkI = A if i = l,i ‰ k, and BkI equals the zero
matrix, otherwise.
17
Under review as a conference paper at ICLR 2021
Proof of Lemma 4. Let Ci “ xi xiT
A “ Xn“i AiCi. Then we have
「1-	I∣2∣
and C “ n Xn“1 G∙ For the given Ai,..., An, we denote
E - >£ AiVLi (wb'i)> ∣Fo
“E E
W AiVLi (wb'i)> ∣Fbfl∣Fo
“E E
£ Ai (xiT wtb`1 — yi) xi››› ∣∣Ftb fl∣∣Fo
“E
£ Ai (xiT	(wtb	—	αtgtb)	—	yi)	xi››	∣∣∣Ftb	fl∣∣∣Fo
“E E
W AiVLi (wb) — αtAgb> ∣Fb fl |fo
“ E -›£n AiVLi (wtb)› ∣∣∣Fo fl
+ α2E [e ]如b>2|用同
— 2αtE E
AiVLi (wtb) , Agtb	∣Ftb ∣Fo
“ E - >} AiVLi (Wb) > ∣Fo
` αt2E
“E
cb
— 2αtE
AiVLi (wtb) , AVL (wtb)	∣∣∣∣Fo
n
—£ >AVLi(wbq>2 — >AVL(wb)>2 + >AVLPwbq>2 Fo
n i“1	)
E ANLi (Wb) ´ αtA^LPw^ Fo	' α2"E ； E >A^Lipwbq>^ ´ >A^LPwbq>2 Fo
“E -››››£n AiVLi (wtb) —
αtAVLPWbqJFO	`—n2~
∑ E [>AVLi (wb) — AVLj(Wb) >2∣Fo]
i‰j
E
n
n
i
n
2
2
2
2
n
“E(Ai — OtA) VLi(Wb)> ∣Fofl + 02? WSLE[>AVLi (wb)—AVLj(Wb)>2同
Therefore, if We set Bi “ Ai — Ot A and
$A
Bikl “	—A
%0
i “ k,i ‰ l,
i “ l, i ‰ k,
otherWise,
We have
E -W AiVLi(WP'/ |fo
“ E -››››£n BiVLi (wtb)›››› ∣Fo
2 nn	n
'箸 kS SEE BiklVLi(Wb)
2∣∣Fo
2
□
Proof of Theorem 1. We use induction to show this statement.
When t “ 0, E ”>XNi AiVLi (wb)>2∣Fo] “ }XNi AiVLi(Wo)}2 which is invariant of b. There-
fore, it is a decreasing function of b.
18
Under review as a conference paper at ICLR 2021
Suppose the statement holds for t. For any set of matrices {Aι,..., An} in Rpxp, by Lemma 2 We
know that there exist matrices {Bι,…，Bn} and {Bfl : i,k,l p [n]} SUch that
E -E AiVLi(Wb`j 卜0
“ E IE BiVLi (Wb)JF0
2 nn	n
' 箸 S 空工 BiklVLi(Wb)
2∣∣∣F0
By induction, we know that E UXn“1 BiVLi (Wb)>2Fo] and all E UXn“1 BklVLi (wb)>:F。]
Pn bq
22
are non-negative and decreasing functions of b. Besides, clearly -nb “ b^pn—iq
is a non-negative
and decreasing function of b. By Lemma 3, we know that αnCbE |>Xn“i Bkl VLi (wb)>2Fo] is
also a non-negative and decreasing function of b. Finally, E |>Xn“i AiVLi (wb`i) >2]尸。],as the
sum of non-negative and decreasing functions in b, is a non-negative and decreasing function of b.
□
In order to prove Theorem 2, we split the task to two separate theorems about the full gradient and
the stochastic gradient and prove them one by one.
Theorem 6.	Fixing initial weights w。, Var (BVL (Wb) IFO) is a decreasing function of mini-batch
size b for all b P rnS, t P N, and all square matrices B P RpXp.
Theorem 7.	Fixing initial weights W0, var Bgtb IF0 is a decreasing function of mini-batch size b
for all b P rnS, t P N, and all square matrices B P RpXp.
Proof of Theorem 6. We induct on t to show that the statement holds. For t “ 0, we have
Var (BVL (Wb) ∣F0) “ 0 for any matrix B. Suppose the statement holds for t — 120. Note
that from
1n
vl (Wb) = n∑xi (XTWt ´ yi)
i“1
1n
“一Xxi (XT (Wb´i — ɑtgb´i) — yi)
n i“1
nn
=n £ xi (χTWb´i ´ yi) ´ £ £ xiχTgb´i
i“1	i“1
=vl (Wb´i) ´ atCgb—i,
19
Under review as a conference paper at ICLR 2021
we have
Var(BVL (Wb) ∣F0)
“ Var(BVL (wb´j ´ αtBCg^ ∣Fo)
“ E ”>BVL (wb´i) ´ αtBCgbτ>2∣Fb] ´ >E “BVL (wb´i) ´ αtBCgtι∣F0]>2
“ E ”>BVL (wbτ)>2 ´ 2αt @BVL (wb´i), BCgbQ +。2 >BCg3>2∣FM ´ >E “BVL (wb´i) ´ αtBCgh∣F0]>2
“ E ”>BVL (wb´i) >2∣Fol + α2E IE ∣>BCgbτ>2∣F3] ∣Fb] ´ 2αtE “E “@BVL (wb´i), BCgbτ>∣F3] ∣F°]
´ >E “E “BVL (wb´i) ´ αtBCgbτ∣F3] ∣Fb]>2
“ E [>BVL (wbτ)∣RFo]+ α2E
Cb (1 ?>BCVLi (wbτ)>2 TBCVL (wbτ)∣) + >BCVL (wbτ)>2 Fo
´ 2αtE “@BVL (wb´i) ,BCVL (wbLι)D∣Fo] ´ >E “BVL (wb´i) ´ αtBCVL (wbτ)∣Fb]>2
(3)
“ E ”>B (I ´ αtC q VL (wb´i) >2∣Fb] + α2cbE | (》W >BC VLi (wb´i) >2 TBC VL (wb´j >2)
— >E“B(I — atCq VL (wbLι)∣Fb]>2
1n	2	2
“ var (B (I ´ αQ) VL (wb´i) ∣Fo) + αfcb - £ E [>BCVLi(WbT) > 同 ´ E [>BCVL (wb´i) >
i“1
2
“ var (B (I ´ αQ) VL (wt´) ∣Fo) + T £ E [>BCVLi(Wb´j ´ BCVLj(Wb_J> ∣Fo],
n	i‰j
(4)
where (3) is by Lemma 1. By induction, we know that the first term of (4) is a decreasing function
of b. Taking Ai “ BC, Aj “ ´BC, Ak = 0,k P [nS∖{i,j} in Theorem 1, We know that
E [>BCVLi (wb´i) ´ BCVLj(Wbτ)>2∣Fo]
2
is also a decreasing function of b. Note that ʒt2b decreases as b increases. By Lemma 3 We learn
that (4) is a decreasing function of b and hence we have completed the induction.
□
Proof of Theorem 7. We have
Var(Bgb ∣Fo) “ E [>Bg"∣2∣F°] ´ >E [Bgb∣F0] >2
“ E [E [>Bgb>2∣Fbl∣Fo] ´ >E [E [Bgb∣Fb]∣Fo]>2
“ Cb (- E E [>BVLi (Wb) >2∣Fol ´ E [>BVL (Wb) >2 同)
+ E [>BVL (Wb) >2∣Fo] ´ >E “BVL (Wb) ∣Fo]>2
“片 £ E [>BVLi (Wb) ´ BVLj(Wb)>2∣Fo] + var (BVL (Wb) ∣Fo).
n i‰j
Taking Ai “ B, Aj “ 一B, Ak = 0,k P [n]∖{i, j} in Theorem 1, we know that
E [>BVLi (Wb) — BVLj(Wb) >2同
is a decreasing and non-negative function of b for all i, j P rns. By Theorem 6, we know that
var BVL Wtb ∣F0 is also a decreasing function ofb. Therefore, var Bgtb ∣F0 , as the sum of two
decreasing functions of b, is also a decreasing function of b.	□
20
Under review as a conference paper at ICLR 2021
ProofofCorollary 1. Simply taking B “ Ip in Theorem 1 yields the proof.	□
B.2 Proofs for Results in 3.2
Remark. We often rely on the trivial facts that x1x2T “ x1Ipx2T and x1x2Tx3x4T “ x1x2TIpx3x4T .
Lemma 5. Given a multiplicative term of parameter matrices ui viT : ui, vi P Rp, i P rn1s Y tAj :
Aj P Rpxp, j P [n2]U and constant matrix {Ip} such that deg(uιVT; M)21, we have
tr pMq “ v1TM1u1,
where	M1	is a multiplicative term of parameter matrices	uiviT	:	ui,	vi P Rp, i	P	rn1s	Y
{Aj : Aj P Rpxp,j P rn2SU and constant matrix {Ip} such that deg(M)	“ deg(M1) '
1, deg(Aj; M) “ deg(Aj; M1), j P rn2s, deg(uiviT; M) “ deg(ui viT; M1), i P r2 : n1s and
deg(u1v1T; M) “ deg(u1v1T; M1) ` 1.
Proof. By the definition of multiplicative terms, we know that there exist two multiplicative terms
M1,M2 of parameter matrices {uivf : u” Vi P Rp, i P [nJ} Y {Aj∙ : Aj P Rpxp,j P [n2]} and
constant matrix tIpu such that
M “ M1u1V1TM2,
where deg(M ) “ deg(M1) ` deg(M2) ` 1, deg(Aj; M) “ deg(Aj; M1) ` deg(Aj; M2),j P
[n2s, deg(uiViT; M) “ deg(uiViT; M1) ` deg(uiViT; M2), i P [2 : n1s and deg(u1 V1T; M) “
deg(u1V1T; M1) ` deg(u1V1T; M2) ` 1. Therefore we have
tr (M) “ tr 'MιUιvTM2) “ tr (VTM2M1U1) “ VTM2M1U1.
Note that M1 “ M2M1 satisfies that deg(M 1) “ deg(M1) ` deg(M2), deg(Aj, M1) “
deg(Aj; M1) ` deg(Aj; M2),j P [n2s,deg(uiViT; M) “ deg(ui ViT; M1) ` deg(uiViT; M2), i P
[2 :	n1s	and deg(u1V1T; M1)	“	deg(u1V1T; M1)	` deg(u1V1T;	M2)	`	1.	We have finished the
proof.	□
The following two lemmas focus on the expectation of the product of quadratic forms of the standard
normal samples. Lemma 6 focuses on single sample while 7 focuses on the same form with b i.i.d.
samples drawn from the standard normal distribution.
Lemma 6. Given matrices Aj P Rpxp, j Prm — 1], we have
Nm ni
Ex„Npo,ιpq [xxTAIxxTA2 …Am´1xxτ] “∑πtr (Mik) Mi0 ,
i“1 k“1
where Nm and ni, i P rNm] are constants depending on m and tMik, k P r0 : ni] , i P rNm]}
are multiplicative terms of parameter matrices tAj,j P rm — 1]} and constant matrix tIp}. Fur-
thermore, for every i P rNm], we have kn“i 0 deg(Aj; Mik) “ 1, j P rm — 1] and therefore
Xn“0 deg (Mik) = m ´ 1.
Proof. SeeMagnUs (1978).	□
Lemma 7. We are given matrices Aj P Rpxp, j Prm — 1] and random vectors xi, i P [b] indepen-
dently and identically drawn from N(0, Ip). We assume that the multi-set S = ij , i1j : j P rm]
satisfies that for every i P S, i is an element of rb] and the number of appearance of i in S is even.
Then
Nm ni
Eχi~N (0,Iρ) [xiι xT Alxi2 xT A2 ∙∙∙ Am´1xim xiT1m =	tr(Mik) Mi0,	(5)
i“1 k“1
where Nm and ni are constants depending on m (and independent ofb) and Mik, k P r0 : ni] , i P
rNm] are multiplicative terms of parameter matrices tAj,j P rm — 1]} and constant matrix tIp}.
Furthermore, for every i P rNm ], we have kn“i 0 deg(Aj; Mik) = 1, j P rm — 1] and therefore
Xkn“i 0deg(Mik) = m — 1.
21
Under review as a conference paper at ICLR 2021
Proof. Let βi , i P rbs be the number of appearances of i in S, which are even by assumption. We
induct on the quantityN = ∑b=ι l{βi ‰ 0}.
For the base case of N “ 1, all elements in the multi-set S have the same value. Without loss of
generality, we assume ij “ i1j “ 1, j P rms. Then
Exi „N (0,Ip)卜 iι XTAIxi2 XT …Am—1 Xim xTm∖ = EX1„N (0,Ip) “x1XT A1x1XT …AmTx1xT‰ ,
which is the statement of Lemma 6.
Suppose the statement holds for N21, and We consider the case of N ' 1. Note that XT Aj /j`i =
XiT Aj Xi1 is a scalar so that we can move it around without changing the value of the expression2.
We distinguish tWo cases.
• Let i1 ‰ i1m. Without loss of generality, We assume i1 = 1. We can alWays change the
order of XT Aj/j`i ,j Prm — 1] (and flip it to be XT 十IAj x% if necessary) such that all
X1’s appear in the form of X1X1T:
Xi1 XT AiXi2XT A2 …Am—iXimXT = Xi (XT AiXi2XT A2 …Am—iXim) XT
i	2	mm	i	2	m	m
“XIXTAIXIXTA2 …Aβι_]XiXTAβxXXT
Where Xr P tXi, i P rbsu , Xr ‰ X1 and Ari ’s are multiplicative terms of parameter matri-
ces tXuXvT : u, v P r2 : bsu Y tAj : j P rm — 1su and constant matrix tIpu such that
∑u,vp[2^S ∑kt 1 deg(XuXT； Nkq = m ´ 粤 ´ 1 and ∑ktι deg(Aj； Akq = 1,j Prm ´ 1s3.
Applying Lemma 6 and the laW of iterative expectations, We have
Exi„N(0,Ip) |Xii XiI A1Xi2 Xi2 .…Am´1Xim
TT	TT
XiXT A[X]XT A2 …A βι_]XiXT A βιXXT I
Nm ni
£ 口 tr (Mikq Mi0 Nβι XXT
i“ik“i	J 2 m_
Nm
=∑Ex2,-,xb I
ni
口 tr (Mikq Mi0 I N爸XXTn ,
k=1	2	2	:
2
Where Nm and ni are constant depending on m (and independent of b) and Mik, k P
[0: ni], i P [Nm] are multiplicative terms of parameter matrices {Aj,j Prβ2τ ´ 1]} and
ni
constant matrix tIpu. Furthermore, for every i P rNms, We have	k“i 0 deg(Aj; Mikq =
1,j Prɪ ´ 1] and therefore ∑nt0 deg (Mikq =号 ´ L
Combining the definition ofAj’s, We knoW that Mik, k P r0 : ni] , i P rNm] are multiplica-
tive terms of parameter matrices tXuXvT : u, v P r2 : b]u Y tAj : j P rm ´ 1]u and constant
2For example, We can reWrite
xi1 xiT11 A1 xi2 xiT12 A2 xi3 xiT13 “
xi1
xi1
xiT1i A1xi2	xiT12A2xi3 xiT13
xiT12 xiT1i A1xi2 A2xi3 xiT13
xi1
xi1
xiT12 A2 xi3	xiT11A1xi2 xiT13
xiT12 A2	xiT11A1xi2	xi3 xiT13.
3For example, We can reWrite
x1x2TA1x1x1TA2x3x3TA3x1x2
x1	x2TA1x1	x1TA2x3	x3TA3x1	x2 “ x1 x1TA1x2	x3TA2x1	x1TA3x3 x2
TTT	TT
“x1x1 A1x2x3 A2x1x1 A3x3x2 “ x1x1 A1x1x1 A2xXx2,
TT
Where A1 “ A1x2x3TA2, A2 “ A3 and xX “ x3. Besides, m “ 4, β1 “ 4, thus the degree of xuxvT in all Ak
sum up to m — β1 — 1 “ 1
22
Under review as a conference paper at ICLR 2021
matrix {Ip} SUch that for every i P [Nm], We have 丑叫^^巧]Xn“0 deg(χuXT; Mikq “
m ´ β21 ´ 1 and Xn“0 degpAj ; Mik) “ 1,j P rm ´ 1s.
Applying Lemma 5, for every k P r0 : nis and every i P rNms, there exists uik, vik P
txj : j P r2 : bsu and mUltiplicative term Mi1k of parameter matrices txuxvT : u, v P
r2 : bsu Y tAj : j P rm ´ 1su and constant matrix tIpu sUch that
tr (Mik q “ uiTk Mi1k vik.
Therefore, We have
(口 tr (Mik) Mi) N粤XxTm
ni	ni
“ 口(UTk Mik Vik ) MiθAr βι XxTm “ MiθAr 筌 X 口(UTk Mik Vik) XTm fi Ui.
k“1	2	2	k“1
Note that for every i P rNms, We have
m´1	ni
X deg(xi; Aj) “ X deg(xi; Mik) ' deg(xi； Mio) ' deg 卜i； N等)
j“1	k“1	2
` deg(xi; xX) ` deg xi; xiT1 ,
and for every j P rm ´ 1s, We have
ni
deg deg(Aj； M'ik) + deg(Aj； Mio) + deg (Aj； N萼)=1.
k“1	2
In other Words, for every i
A0xPιxT Alxp? XT ∙∙∙ AmTxp^ IxTmAmI
xpij, xpij P txj,j P r2 : bsu, and Api, i P
P	rNms, Ui has the form of
bUt there is no appearance of x1 . Here
r0 : ms are mUltiplicative terms of parameter
matrices tAj, j P rm — 1su and constant matrix tIpu. FUrthermore, for every j P rm — 1s,
We have Xkn“i o deg(Aj; Api) = 1. Note that here We Use the liberty of adding identity
matrices if more than tWo consecUtive x’s appear. Since We have redUced N + 1 by
one, we can use induction on xp xT√pιxp xT …Am—ixp xT and finish the proof.
i1 i11	i2 i12	m	im1 im
The two constant matrices Ao and Am do not change the result of expectation since
E (AoXAm) = AOE(X)Am，.
• If il = i1m , without loss of generality we assume, i1l = 1 and i1l ‰ il (note that all
xT Ajxij'i ,j Prm — 1S are inter-changeable and there is at least one element in S that
is not equal to iι). We change the orders of xT Ajxq'、,j P rm — 1] (and flip it to be
xiT Aj xi1 if necessary) such that all xl’s appear in a consecutive form ofxlxlT:
xiι xT A1xi2 xT A2 ∙∙∙ Am — lxim xTm = xiι
= xi1
(xT Aixi? xT A2 …Am—lxim.) xTm
(XT Ao ”xi XT Ai ∙∙∙ A βι ´ IxIxTI A βι X2) xT
Where xX1, xX2 P txi, i P rbsu , xX1, xX2 ‰ x1 and Ai’s are mUltiplicative terms of parameter
matrices txuxvT : U, V P r2 : bsu Y tAj : j P rm — 1su and constant matrix tIpu sUch that
β1
X X deg(xuxT; Ak)= m — β — 2
u,vPr2:bs k“o
βι

and Xk“o deg (Aj; Ak) = 1,j P rm — 1]. The remaining reasoning is the same as the
previoUs case.
□
23
Under review as a conference paper at ICLR 2021
Remark. If one of the βi numbers of appearance of xj , j P rbs is odd, then it is easy to see that the
result in (5) is the zero matrix.
As pointed out in the Section 1, the difficulty of studying the dynamics of SGD is how to connect
the quantities in iteration t with fixed variables, like initial weights W0,1, W0,2 and mini-batch size
b. We overcome this challenge by the following two lemmas. Lemma 8 provides the relationship
between gtb,i, i “ 1, 2 and Wtb,i, i “ 1, 2 by taking expectation over the distribution of random
samples in Btb. Lemma 9 shows the relationship between Wtbi, i “ 1, 2 and gtb´1 i, i “ 1, 2 using
(1) and (2).	,	,
Lemma 8. For multiplicative terms Mi, i P r0 : ms of parameter matrices gtb,1, gtb,2
and constant matrices Wbt,iw Wb2,Wι , W12} With degree di, respectively, we denote
M “ im“1 tr pMi q M0 and d “ im“0 di. There exists a set of multiplicative terms
Mikj, i P	rmks, j P	r0	:	mkis	, k P	r0 : qs	of parameter matrices	Wtb,1, Wtb,2	and constant ma-
trices {W：, W^U Such that
E [M∣Ff‰ “ No + Ni b + …+ Ndɪ,
where Nk “	im“k1	jm“k1i tr Mikj Mik0, k P r0 : ds. Here mk, mki are constants independent ofb,
and ∑m0 deg (Mj) & 3d + ∑Zo (deg (Wib1； Mi) + deg(Wb2； Mi))∙
Lemma 9. For multiplicative term Mi, i P r0 : ms of parameter matrices Wtb,1, Wtb,2 and con-
stant matrices {W：, W：U of degree di, let d “ 2d0'…'dm. There exists a set OfmultiPliCa-
tive terms {Mik , i P r0 : ms , k P rdsu of parameter matrices gtb,1, gtb,2 and constant matrices
Wtb,1,Wtb,2,W1：,W2：( such that
m	dm
∏ tr (Mi) Mo “ ∑ ∏ tr (Mik) Mok,
i“1	k“1 i“1
where ∑mX° deg (Mik) ≤ d.
Proof of Lemma 8. By (1) and (2) we have
m	bd m
M “ ∏ tr (Mi) Mo “ bd Xn tr (Mki) Mko,	⑹
i“1	k“1 i“1
where each Mki, k P rbds, i P r0 : ms is a multiplicative term of parameter matrices
{ xt,ix{,i, i P 网( and constant matrices { Wb1, Wb2, Wb(. Let Mk “ ∏Xι tr (Mki) Mko, k P
[bd‰. We split set {Mk : k P [bd‰} into disjoint and non-empty sets (equivalent classes)
S1 , . . . , SnM such that
1.	for every i P rnMs and every M1, M2 P Si, we have E [M1∣∣Ftb‰ “ E [M2∣∣Ftb‰,
2.	for every i, j P rnM s, i ‰ j and every M1 P Si and M2 P Sj, we have E M1∣Ftb ‰
E [M2∣Ftb‰.
Note that YnMLSi “ {Mk : k P [bd‰}. Let Mk P Sk represent the equivalent class Sk (it can be any
member of Sk). For every i P RmS, We can always write |Si | = ei,o + ei,ib +-----+ ei,dbd such that
ei,j P N, ei,j V b,j P [0: d] (actually ei,j,s are the digits of the base-b representation of |Si|). Then
24
Under review as a conference paper at ICLR 2021
we have
E “M∖Fb‰ “ E
“ bdE
nM	∖
X 'ei,0 ' ei,1b '…' ei,dbd Mi Ft
i“1	∖
1 nM
“ bdΣ 'ei,0 + ei,1b +∙∙∙'
i“1
ei,dbd) E [Mi∖Fb]
(7)
nM	1
“ M I ei,d ' ei,d´l b '	'
i“1
ei,0 bd)E IMilFb].
It is important to note that nM, the number of different equivalent classes, is independent
of b.
This follows from the fact that each E
[Mk∖Fb] (and
so as E Mxk∖∖Ftb )
includes
a finite number of weight matrices Wtb,1 and Wtb,2 with degree less than or equal to 3d `
Xm“0 'deg 'Wbi； Mi) ' deg(W%; Mi)) (See Lemma 7). Thus the number of partition sets is
bounded by a quantity independent of b.
Note that each Mki can be represented as
Mki = A0F：iixt：i「Aki …AkiTxk,2χ%iτ
for some matrices A0ki , . . . , Adki that are multiplicative term of parameter matrices
Wtb,1, Wtb,2andWtb constant matrix tIpu (we stress again that some A matrices can be
identities, based on the definition of multiplicative terms), and xtk,ii , . . . , xtk,ii	P txt,1, . . . , xt,bu.
We have
tr PMki) “ tr (Akix3xk,/Aki …若匕/,"x‰TAdi)
ki T ki ki ki ki T ki ki ki
“xt,idi Adi AO xt,iι xt,iι AI	Adi — 1xt,idi.
For every k P bd , we have
m
口 tr (Mki) MkO “
i“1
m
ki T ki ki ki ki T ki ki ki
IIxt,idi Adi aO xt,iι xt,iι A1 ∙∙∙ Adi´1 xt,idi
i“1
m
口 Tki TdkidkiTki Tki TAke …Ak Tki
IIxt,idi Adi AO xt,i1xt,i1 A1	Adi´l XtMi
i“1
kO kO kO T kO	kO	kO	kO T kO
AO xt,iι xt,iι	A1 …Ad0´1Xt,id0 Xt,idQ	AdO
ITkO T AkO AkO	kO 1 AkO kO kO T AkO
[xt,iι A1	Ad0 — 1 xt,idoJ AO xt,iι xt,ido Ad0，
which can be rewritten as
m
Mk = 口 tr (Mki) MkO “
i“1
(口 XTijAjxt,ij-) AkOxk,Oixk,Od0TAkO.
Note that the randomness of each Mk given Fb only comes from the randomness of Xtj 's, i.e. for
all k P bd we have
E ∣Mk∖Fb] “ Ext,j „N pO,I q	∏ xtT,ij Ajk xt,i1j	AOkxt,i10xtT,i0AOk1
“ Ext,j „N pO,I q	AOk xt,i10	∏ xtT,ij Ajk xt,i1j	xtT,i0 AOk
(8)
nkM nik
“ ∑∏ tr (Mij) MO,
i“1 j“1
25
Under review as a conference paper at ICLR 2021
where the last equation comes from Lemma 7. Here nkM , nik , i P nkM , k P bd are constants inde-
pendent of b, Mikj ’s are multiplicative terms of parameter matrices Wtb,1 , Wtb,2 , Wtb and constant
matrix tIpu such that for every i P nkM , we have
nik
∑ deg (Wb; Mk) “ d	(9)
j“0
and
k
ni	m
∑ (deg (Wbi； Mj ' deg (Wb2； Mj ) “ d'£ (deg 'W%; Mr) + deg(w^; Mr)) . (10)
j“0	r“0
These degree relationships can be observed from (1), (2), and the fact that each gtb,1 or gtb,1 con-
tributes one Wb and one of WtbI or Wtb2 in ∏* 1 tr (Mj Mk). Note that Wt = Wtb2卬a一
WξW^. For every i P “nM‰, if We replace all appearances of Wb in n；“ 1 tr (Mj Mk and
expand all parentheses Of(Wtb2 Wt12 ´ W2W1), We have
nik	2d nik
∏ tr ´Mj Mk “ £n tr ´Mj Mikl,
(11)
j“1
l“1 j“1
where Mikl's are multiplicative terms of parameter matrices { Wtb1, Wb,2( and constant matrices
{Wj,W2ii} such that
nik	m
E (deg (Wtb1； Mj + deg (Wtb2； Mikl)) ≤ 3d + ∑ (deg (Wt； Mr) + deg(Wb2; Mr)),
j“)	r“)
(12)
Where the inequality comes from (9) and (10) and the fact that each gtb,1 or gtb,2 contributes 2 or 0
degrees in the form of Wt,2WtbI or WξW^, respectively.
Combining (7), (8) and (11), We have
_一r	nM∕	1	1'_「Cl _1
E “MFtb‰ “ ∑ ( ek,d + ek,d-1 b + '■' + ek,0 凉 E MxkllFtb
k“1
sk
nM	nMk 2d nik
“ E (ek,d+ek,"1b+…+ek,0 ^^ ∑ ∑ ∏tr ´Mikl)Mikl
k“1	i“1 l“1 j“1
“ No + N11 + ∙∙∙ + Ndg,
b	bd
Where
nM	nsMk 2d nik
Nr= E ek,d´r EEntr(Mikl) Mikl .	(13)
k“1	i“1 l“1 j“1
Note that all constants in (13) are independent ofb and combining With (12), We have finished the
proof.
□
ProofofLemma 9. Simply using the fact that Wtbi = W^1i — atgb"i, i = 1,2, if we replace
each Wtbi in the left-hand-side of (13) by Wt”复一ɑtgb´ii and expand all the parentheses, then
each Mi, i P r0 : ms becomes the sum of 2di multiplicative terms of parameter matrices gtb,1, gtb,2
and constant matrices {Wt,1,Wtb,2, W*,W2^} with degree at most di. As aresult, nm“1 tr (Mi) Mo
becomes the sum of 2d terms in the form of nm“1 tr (Mik) MOk where deg (Mik) ≤ 2di, and
therefore Xm“)deg (Mik) ≤ ∏^o 2di = d	□
26
Under review as a conference paper at ICLR 2021
Proof of Theorem 3. We use induction on t to show this result. The base case of t “ 0 it is the same
as the statement in Lemma 8.
Suppose that the statement holds for t 2 0, and We consider the case of t ' 1. By Lemma 8,
there exists a set of multiplicative terms {M%ι,i,j,i P [mt`i,kS,j P [0 : mt'i,k,iS, k p [0： d]} of
parameter matrices {卬3中卬乙中} and constant matrices {W：, WξU SUch that
E [M∣Ft'ι‰ “ Nt'i,o ' Nt'ι,ιb '------------' Nt`i,dbd,	(14)
Where Nt`i,k “ ∑mt'1,k ∏mt'1,k,i tr 'Mk'ι,i,j) Mk'i,i,o, k Pr0: ds. Here mt`i,k, mt`i,k,i are
constants independent of b, and ∑m'1,k,i deg 'Mf'ij ≤ 3d ' d.
For each i P rmt`i,kS and each k p r0 : d], by Lemma 9, there exists a set of multiplicative terms
{Mt,i,j,k,ι,j p rmt`i,i,k S,l p rdt,i,k SU of parameter matrices { gb,i,gb" and constant matrices
Wtb,1,Wtb,2,W1：,W2：( such that
"mt'1,k,i	dt,i,k ,mt'1,k,i
口 tr 'Mtk'l,i,j) Mtk'l,i,0 “ X 口 tr pMt,i,j,k,lq Mt,i,0,k,l,	(15)
j“1	l“1 j“1
where dt,i,k = 2∑j='，，(deg(W,i；Mt,i,j,k,l)+deg(Wb,2；Mt,i,j,k,1)) is a constant independent of b and
mt`i,k,i
X	deg (Mt,i,j,k,ι q≤ 3d ' d,	(16)
j“0
and
,mt'1 ,k,i
X	(deg pWt,1; Mt,i,j,k,lq ' deg pWt,2; Mt,i,j,k,lq'q ≤ 3d ' d1.	(17)
j “0
Combining (14) and (15), We have for every k P r0 : dS
,mt'1 ,k dt,i,k ,mt'1 ,k,i
Nt'i,k “ XXn tr (Mt,i,j,k,ιq Mt,i,0,k,ι.	(18)
i“1 l“1 j“1
Note that
E rM∣FoS = E [E [M∣Ff+1‰∣F0‰ “ E rNt+ι,o∣FoS + E rNt+1,1∣F0S： + …+ E rNt+1,d∣F0Sb1d
mt`1,0 dt,i,0
“ X XE
i“1 l“1
mt`1,1 dt,i,1
+ X XE
i“1 l“1
mt`i,o,i
n tr (Mt,i,j,0,lq Mt,i,0,0,l∣F0	+
j“1	∣
mt^+ι,iii	ι
n tr pMt,i,j,ι,ιq Mt,i,o,ι,ι Fo b+…+
j“1	∣ b
mt`i,d dt,i,d
+X XE
i“1 l“1
mt`i,d,i	1
Π tr pMt,i,j,d,l'q Mt,i,0,d,l F0 收
j“1	∣
(19)
and each Mt,i,j,k,l is a multiplicative term of parameter matrices gtb,1, gtb,2 and constant matrices
{Wtb,1, Wtb,2, W1：, W2：u such that the degree is at most 1. Therefore, by induction, for every i, k, l,
We have
mt`i,k,i
E n tr (Mt,i,j,k,lq Mt,i,0,k,l F0
j “1	∣
“Nt,i,k,l,0 + Nt,i,k,l,1 b +	Nt,i,k,l,qt 而,
(20)
where qt ≤ d1 + ɪ (3t ´ 1)(3d + d1) and Nt,i,k,ι,o,…，Nt,i,k,iq are sum of multiplicative terms of
parameter matrices Wb,ι,W W0" and constant matrices {W：,卬力 with degree at most d ∙ 3t.
27
Under review as a conference paper at ICLR 2021
Combining (19) and (20), we can rewrite
E[M∣Fo]= N ' N ； + …+ Nq∖,
in the same form as in the statement. Here q ≤ d ' 3qt < 11 (3t+2 — 1)d ' ɪ (3t+1 — 1)d1 and
W“0 deg (Mj) ≤ 3 X 3t(3d ' d1) “ 3t'1(3d ' d1) follow from (16) and (17).
In conclusion, we have shown that the statement holds for t ` 1, and therefore finishes the proof.
□
By changing the role of parameter and constant matrices in Theorem 3, we obtain the following
corollary.
Corollary 2. Given t 20, for any multiplicative terms Mi, i P [0: m] of parameter matri-
ces { Wbι,Wb2, Wb( and constant matrices {Wj, WξU Such that X1=ι deg (Wj； M) “ d and
deg Wtb; M “ d1, we denote M “ im“1 tr (Miq M0. There exists a set of multiplicative terms
Mikj, i P rmks, j P r0 : mkis , k P r0 : qs of parameter matrices W0b,1, W0b,2 and constant matri-
ces {WJ, WξU SuCh that
E [M∣Fo] = No ' Ni E + …+ Nq的,
where Nk = XmI ∏jmki tr (Mij) Mik), k P [0: q]. Here mk, mki and q ≤ 3t (d ' 2d) are con-
stants independent of b, and XmO deg (Mij) ≤ 3t (d ' 2d1).
Proof of Corollary 2. We simply note that M can be written as the sum of at most 2d multiplica-
tive terms of parameter matrices {Wjbι,Wjb1, Wj,W1i} and constant matrix {I。}. Then we apply
Lemmas 8 and 9 iteratively in the same way as in the proof of Theorem 3 to finish the proof. □
Proof of Theorem 4. We only show the case for gt,1 since the proof for gt,2 can be tackled similarly.
Note that
var (gb,1∣F0) = Var
ʌ ι b
FO)= b Σ var (Wtb2, wbxt,ixTi IFo)
i“1
1
b
1
b
1
b
1
b
1
b
1
b

>E [Wtb2TWbxtjxTi∣FoU)
(E [tr (xt,IxTiWbTWtb2Wb2TWbxtJxTι)∣Fo] - >E [W，Wbxt,ιxTι∣Fol>2)
(E [e [tr (xt,IxTiWbTWtb2Wb2TWbxtJxTι)∣Fb]∣Fo] — >E [E [W，Wbxt,ιxTι∣Fbl∣Fo]>2)
^E [(P + 2)tr (WbTWtb2Wb2TWb) ∣Fo] — >E [W，Wb∣Fo]>)
((p + 2)tr(E [WbTWb2Wb2TWb∣Fo]) - >E [W，Wb∣Fo]>) ∙
((p + 2)tr(E [WbTWb2Wb2TWb∣Fo]) - >E [W，Wb∣Fo]>) ∙
Here we have used the fact that Ex„NpO,Ipqtr xxT AxxT = (p+2qtr (Aq. By Corollary 2 we know
that there exists a set of multiplicative terms Mikj, i P [mk], j P [0 : mki] , k P [0 : q] of parameter
matrices {Wlb,ι, Wlb,2} and constant matrices {W*,W<^} such that
tr (E [WbTWb2Wb2TWb∣Fo]) = γo + γιb + ∙∙∙ + Yq 1,	(21)
28
Under review as a conference paper at ICLR 2021
where Yk “ Xm“1 0匿 tr (Mj), k P [0： q]∙ Here mk, mki and q ≤ 6 ∙ 3t are constants indepen-
dent of b, and XmkO deg (Mj) ≤ 6 ∙ 3t. Note that Wlb,1, Wb^ are fixed, and We have γk, k p [0 : q]
are constants independent of b.
Similarly we observe that there exist constants q ≤ 2 ∙ 3t'1 and Yk, k p [0 : q] such that
>E [Wtb2TWbF01 >2 = Y0 + Y1 b + ∙∙∙+ Yq+.	(22)
By defining Yi = 0,i > q and Yi = 0,i > q, and combining (21) and (22) we have
var (gb,1∣Fo) “ b (pP + 2)tr ´e [wbTWtb2Wb2TWbF0]) ´ >E [W,Wb同 f)
“中 ^y0 + Y11 + …+ Yq bq) ´ 1 ^y0 + y1 1 + …+ Yq bq1)
maxtq,q1u
“	∑	((P + I)Yk ´ Yk) 1k.
k“1
Note that Yk,s and Yk,sare all constants independent of b, and max {q, q1} ≤ 2 ∙3t'1. This completes
the proof.
□
Proof of Theorem 5. We first show that in
var (gb,i∣F0) “ β 1 + —+ β 1r
we have βι20. If r = 1, the statement obviously holds. Let US assume that the statement does not
hold for r >	1, i.e.	βι	V	0.	Taking 1 large enough such that βι1rτ	+ /21『—2	'-' βr	V 0 yields
var (gb,i∣F0) = 1r (βι1rτ + /217-2 + ∙∙∙ + β) V 0,
which contradicts the fact that Var (gb,i∣F0) > 0. Therefore, we have βι20.
Let 10 be large enough such that for all 1210, we have βι1rτ + 2β2hr´2 + ∙∙∙ + rβr20. We
denote f(1) = β1b + β2b2 + …+ βrbr20. For all 1 > 10 we have
/1(1) = ´ 1r'1 (员厂1 + 2β21rτ2 + '∙∙ + rβr) ≤ 0.
Therefore, for all 1 > 10 we have (var ⑷/尸。))1 = — br`if (1) + brf (1) ≤ 0, and thus
Var (gb,i∣F0) is a decreasing function of 1 for all 1 > 10.
□
B.3 Extension to Deep Linear Networks
The extension from two-layer linear network to deep linear network is straightforward. Here we
only provide the ideas on how to translate the proof of two-layer network to d-layer network, but not
the strict proof. For simplicity, we remove all superscripts 1 of matrices in this subsection.
Assume that the d-layer linear network is given by f (x; W) = WdWdT1 ∙∙∙ W2W1x, where Wi, i P
[ds is the parameter matrix on the i-th layer and w = (W1, . . . , Wd). The population loss is defined
as
L(W) = Ex„n(0,Ip) 2 }Wd …Wix — W：…WiM2 .
Similar to (1) and (2), we have
gt,k = 1 ∑ Vwt,fc ^ 1 }Wt,d …Wt,ixt,i — W：…W：xt,i}2)
i“i
1b
“1∑ WTk`i …WtTi (Wd …Wi — W： ... W『)χt,iχTiWtτi …WTk´i, k P [d].
i“i
29
Under review as a conference paper at ICLR 2021
We denote Wt “ Wt,d ∙∙∙ Wt, 1 — Wdi ∙∙∙ W*. The remaining are all the same as the proofs in Ap-
pendix B.2, except We should replace all appearance of {Wt,2, Wt,ι} to {Wt,d, Wt,"i, ∙∙∙ , Wt,ι}
and all {W2∖W:} to { W*,Wd-ι,," , W*(. We can do this because the stochastic gradient
gt,k is still the sum of multiplicative terms of parameter matrices txt,iu and constant matrices
{Wt,d,…，Wt,ι,W*,…，W*U so the Lemmas in Appendix B.2 still apply.
In conclusion, we can again represent Var (gt,k |Fo), k P [d] as a polynomial of b with finite degree
and Without the constant term. By the same approach in the proof of Theorem 5, We can shoW that
the variance is a decreasing function of the mini-batch size b.
30