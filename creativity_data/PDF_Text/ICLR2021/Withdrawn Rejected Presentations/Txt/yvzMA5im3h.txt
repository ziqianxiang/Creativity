Under review as a conference paper at ICLR 2021
Graph Joint Attention Networks
Anonymous authors
Paper under double-blind review
Ab stract
Graph attention networks (GATs) have been recognized as powerful tools for
learning in graph structured data. However, how to enable the attention mech-
anisms in GATs to smoothly consider both structural and feature information
is still very challenging. In this paper, we propose Graph Joint Attention Net-
works (JATs) to address the aforementioned challenge. Different from previous
attention-based graph neural networks (GNNs), JATs adopt novel joint attention
mechanisms which can automatically determine the relative significance between
node features and structural coefficients learned from graph subspace, when com-
puting the attention scores. Therefore, representations concerning more structural
properties can be inferred by JATs. Besides, we theoretically analyze the expres-
sive power of JATs and further propose an improved strategy for the joint attention
mechanisms that enables JATs to reach the upper bound of expressive power which
every message-passing GNN can ultimately achieve, i.e., 1-WL test. JATs can
thereby be seen as most powerful message-passing GNNs. The proposed neural
architecture has been extensively tested on widely used benchmarking datasets,
including Cora, Cite, Pubmed, and OGBN-Arxiv, and has been compared with
state-of-the-art GNNs for node classification tasks. Experimental results show
that JATs achieve state-of-the-art performance on all the testing datasets.
1	Introduction
Many real-world data can be modeled as a graph, where a set of nodes (vertices), edges, and bag-of-
words features respectively represent data instances, instance-instance interrelationships, and con-
tents characterizing the nodes. For example, scientific articles in a research domain can be modeled
as a graph, where nodes, edges, and node features respectively represent published articles, citations,
and index information of the articles. Besides, social network users and interacted biological units
can also be similarly represented as graphs possessing different structural and descriptive informa-
tion. As graph data are widely available and they are related to various analytical tasks, learning in
graphs has been a hot-spot in machine learning community.
There have been a number of approaches proposed to effectively learn in graph structured data.
Amongst them, graph convolutional networks (GCNs) have shown to be powerful in learning low-
dimensional representations for various subsequent analytical tasks. Different from those empirical
convolutional neural networks (CNNs) which have achieved a great success in learning in image,
vision, and natural language data (Krizhevsky et al., 2012; Xu et al., 2014), and whose convolu-
tion operators are always defined to process a grid-like data structure, GCNs attempt to formulate
convolution operators aggregating the node features according to the observed graph structure, and
learn the information propagation through different neural architectures. Meaningful representations
which capture discriminative node features as well as intricate graph structure can thereby be learned
by GCNs. There have been several sophisticated GCNs proposed in the recent. According to the
ways through which GCNs make use of graph topology to define convolution operators for feature
aggregation, GCNs can generally be categorized as spectral, and spatial ones (Wu et al., 2020).
Spectral GCNs define the convolutional layer for aggregating neighbor features based on the spectral
representation of the graph. For example, Spectral CNN (Bruna et al., 2013) constructs the convo-
lution layer based on the eigen-decomposition of graph Laplacian in the Fourier domain. However,
such layer is computationally demanding. Aiming to reduce such computational burden, several
approaches adopting the convolution operators which are based on simplified/approximate spectral
graph theory are proposed. First, parameterized filters with smooth coefficients are introduced for
1
Under review as a conference paper at ICLR 2021
Spectral CNN to allow it to consider spatially localized nodes in the graph (Henaff et al., 2015).
Chebyshev expansion is then introduced in (Defferrard et al., 2016) to approximate graph Laplacian
rather than directly perform eigen-decomposition of it. Finally, the graph convolution filter is further
simplified by only considering connected neighbors of each node (Kipf & Welling, 2017) so as to
further make spectral GCNs computationally efficient.
In contrast, spatial GCNs define the convolution operators for feature aggregation directly making
use of local structural properties of the central node. The key of spatial GCNs is consequently how
to design an appropriate function for aggregating the effect brought by the features of candidate
neighbors selected according to a proper sampling strategy. To achieve this, it sometimes requires
to learn a weight matrix according to node degree (Duvenaud et al., 2015), utilize the power of
transition matrix to preserve the neighbor importance (Atwood & Towsley, 2016; Busch et al., 2020;
Klicpera et al., 2019), extract the normalized neighbors (Niepert et al., 2016), or sample a fixed
number of neighbors (Hamilton et al., 2017; Zhang et al., 2020).
As a representative spatial GCN, Graph attention network (GAT) (Velickovic et al., 2018; Zhang
et al., 2020) has shown a promising performance in various graph learning tasks. What makes GATs
effective in learning graph representations is they adopt the attention mechanism, which has been
successfully used in machine reading and translation (Luong et al., 2015; Cheng et al., 2016), and
video processing (Xu et al., 2015), to compute the node-feature-based attention weights (attention
scores) between a central node and its one-hop neighbors (including the central node itself). Then,
GATs utilize the attention scores to obtain a weighted aggregation of node features which are prop-
agated to the next layer. As a result, those neighbors possessing similar features may impact more
on the center node, and meaningful representations can be inferred by GATs.
Although GATs have been experimentally verified as powerful tools for various graph learning tasks,
they still confront several challenges. First, for attention-based GNNs, appropriate attention mech-
anisms which can automatically identify the relative significance between the graph structure and
node features are not many. As a result, most current attention mechanisms for GATs cannot ef-
fectively capture the joint effect brought by the underlying graph structure and node features for
seamlessly impacting the message-passing in the neural architecture. Second, whether the expres-
sive power of GNNs adopting the attention mechanisms which can effectively acquire the aforemen-
tioned joint effect may reach the upper bound of message-passing GNNs has not been theoretically
investigated. To address the mentioned challenges, in this paper, we propose novel attention-based
GNNs, dubbed Graph Joint Attention Networks (JATs). Different from previous works, the atten-
tion mechanisms adopted by JATs are able to automatically capture the relative significance between
structural coefficient learned from graph topology, and node features, so that higher attention scores
may be learned by those neighbors which are topologically and contextually correlated. JATs are
consequently able to smoothly adjust attention scores according to the contemporary structure and
node features, and truly capture the joint attention on structural and contextual information prop-
agated in the neural network. Besides, we theoretically analyze the expressive power of JATs and
further propose an improved strategy which enables JATs to distinguish all distinct graph structures
as 1-dimensional Weisfeiler-Lehman test (1-WL test) does. This means JATs can reach the upper
bound w.r.t. expressive power which all message-passing GNNs can ultimately achieve. JATs have
been extensively tested on four widely used datasets, i.e., Cora, Citeseer, Pubmed, and OGBN-
Arxiv, and have been compared with a number of strong baselines. The experimental results show
that JATs achieve the state-of-the-art performance.
The rest of the paper is organized as follows. In Section 2, we elaborate the proposed JATs, and
compare JATs with other GNNs. In Section 3, we prove the limitation w.r.t. expressive power of the
joint attention mechanisms presented in Section 2. A strategy is then proposed to improve JATs to
reach the upper bound of expressive power which all message-passing GNNs can at most achieve.
The comprehensive experiments which are used to validate the effectiveness of JATs are presented in
Section 4. Finally, we summarize the contributions of the paper and propose future works potentially
improving JATs.
2	Joint attention-based graph neural networks
In this section, we elaborate the proposed JATs. Mathematical preliminaries and notations used in
the paper are firstly illustrated. How JATs learn the structural coefficients which are used in the
2
Under review as a conference paper at ICLR 2021
joint attention mechanisms is then introduced. Following that, the joint attention layer, which is the
cornerstone of JATs is elaborated. At last, we compare the proposed JATs with their counterparts.
2.1	Notations and preliminaries
Throughout this paper, we assume a graph G = {V, E} containing N nodes, |E| edges, where V
and E respectively represent the node and edge set. We use A ∈ {0, 1}N×N and X ∈ RN×D to
represent graph adjacency matrix and node feature matrix, respectively. Ni denotes the union of
node i and its one-hop neighbors. Wl and {hli}i=1,...N denote the weight matrix and features of
node i at lth layer of JATs, respectively, and h0 is set to be the input feature, i.e., X. For the nodes
in Ni, their possible feature vectors form a multiset Mi = (Si, μi), where Si = {sι,...sn} is the
ground set of Mi which contains the distinct elements existing in Mi, and μ% : Si → N? is the
multiplicity function indicating the frequency of occurrence of each distinct s in Mi .
2.2	Learning structural coefficients from graph subspace
It is well known that topology is the corner stone of the graph. How to utilize such structural in-
formation to compute the attention scores naturally has a profound impact on the performance of
attention-based GNNs. Empirical attention based GNNs, e.g., GAT, compute the attention scores be-
tween connected neighbors only using node features, but overlook the structural correlation between
pairwise nodes. To allow attention-based GNNs to capture the higher-order structures in the graph,
we propose JATs to learn the topological coefficients from the graph subspace. Inspired by subspace
clustering (Elhamifar & Vidal, 2013), we may formulate the learning of structural coefficients as
follows. Given N nodes in the graph drawn from multiple linear subspaces {Si}i=1,...K, one can
represent a node in a subpace as a linear combination of other nodes. If each row in A is treated as
the structural information of each node, one can simply represent it using other nodes (other rows
in A) as one equation, i.e., Ai,: = Ci,:A, where C denotes the structural coefficient matrix as A is
used. It has been shown in previous works (Ji et al., 2014) that under the assumption the subsapces
are independent, by minimizing certain norm of C, C may have a block diagonal structure (after
finite permutations). In other words, Cij 6= 0 if and only if two nodes, vi and vj are in the same
subspace. So, we can utilize C to learn the structural correlations between neighbors in the graph.
And the above learning task can be formulated as the following optimization problem:
minimize kCkp, subject to A = CA, Cii = 0,
(1)
where ∣∣∙kp stands for a certain matrix norm, and the zero constraint on the diagonal of C may
prevent trivial solutions when ∣∣∙∣p is the norm considering sparsity. To make the data corruption
explainable, the equality constraint in Eq. (1) is often relaxed as a regularization term and the
learning of structural coefficients can be reformulated as follows:
minimize ∣C∣p + β∣A - CA∣2F, subject to Cii = 0.
(2)
By rewriting Eq. (2), we may reveal why subspace learning is effective in capturing the structural
correlations between pairwise nodes. Mathematically, Eq. (2) is equivalent to the following node-
wise minimization problem:
minimize EIICi,：kp + β∣Ai,: - CAi,： IlF, subject to Cii = 0.
(3)
As CAi,: is equal to j Cij Aj,: and Cii = 0, one may easily find that minimizing Eq. (3) is
equivalent to search for the optimal linear combination of other nodes that can be used to reconstruct
the ith node in the graph. As a result, Cij is high when Aj,:, i.e., the global structure of jth node
(rather than the local bias (Gong et al., 2018)), is similar to Ai,:. By minimizing Eq. (2), one
may identify those nodes which are in the same graph subspace, and the structural correlations
between neighboring nodes can therefore be inferred directly. As the above learning problem can be
effectively solved via gradient descent, JATs can optimize Eq. (2) together with the training of the
neural architecture. Also, we use l1 norm for C to force JATs to learn sparse structural coefficients.
2.3	Joint attention layer
Having made the structural coefficients available, we are now presenting joint attention layer, which
is the core module for building JATs and will be used in our experiments. Different from the at-
tention layers utilized by other GNNs, the joint attention layer in the proposed framework adopts
3
Under review as a conference paper at ICLR 2021
Figure 1: Graphical illustration of the joint attention mechanisms used in each layer of JATs. Left:
Joint attention mechanism using Implicit direction strategy. Right: Joint attention mechanism using
Explicit direction strategy. Both two mechanisms consider structural coefficients learned from graph
adjacency.
novel attention mechanisms, which may automatically identify the relative significance between in-
put features and structural information. Appropriate attention scores between central node and its
neighbors can be obtained and meaningful representations can be inferred by JATs.
Given a set of node features {hli}i=1,...N, each hli ∈ RDl , the joint attention layer of JATs is to map
them into Dl+1 dimensional space {hli+1}i=1,...N, according to the correlations of input features
and graph topology. JAT first attempts to compute the contextual correlation between two connected
nodes, say vi and vj . To do so, we directly adopt the feature-based attention mechanism utilized by
previous GATs (Velickovic et al., 2018):
exp(LeakyReLU(aT(Wlhi ∣∣ Wlhj)))
fij = Pk∈Ni eχp(LeakyReLUCaT(Wlhi I Wlhk))),
(4)
where a ∈ R2Fl+1
is a vector of parameters of the feedforward layer, ∣ stands for the concatenation
function, and Wl is Fl+1 × Fl variable matrix for feature mapping. Based on Eq. (4), JAT may
capture the feature correlations between connected nodes (first-order neighbors) by computing the
similarities w.r.t. features mapped to next layer.
However, determining the attention scores solely based on node features leads a GNN to overlook
the structural information hidden in the graph. To overcome this issue, we propose JAT to learn the
structural coefficients as mentioned in Section 2.2 and utilize the novel joint attention mechanisms
to appropriately compute the attention scores. Given learnable structural coefficient, Cij between
two nodes, JAT obtains the structural correlation as follows:
_	exp (Cj)
Sij	Pk∈Ni exP (Cik)
(5)
Given fij and sij , JAT has two strategies (attention mechanisms) to compute the final attention
scores. The first mechanism is dubbed Implicit direction. It aims at computing the attention scores
whose relative significance between structural and feature correlations can be automatically ac-
quired. To do so, for each layer of the neural network, JAT introduces two learnable parameters,
gf and gs to determine the relative significance between structure and feature correlations and it can
be obtained as follows:
exp(gf)	= exp (gs)
exp (gs) + exp (gf), S exp(gs)+ exp(gf)
(6)
where rs or rf represents the normalized significance related to structure or node features. Given
them, JAT is able to compute the attention score based on Implicit direction strategy:
α.. =____rf ∙ fij + Ts ∙ Sij_= r ∙ f. + r ∙ s..
j	Pk∈Ni[f∙ fik + rs ∙ Sik]	rf fij + TSs.
(7)
Given the attention mechanism shown in Eq. (7), αij attempts to capture the weighted mean atten-
tion in terms of structural and feature correlations between neighbors. Compared with the attention
4
Under review as a conference paper at ICLR 2021
mechanism solely based on features of one-hop neighbors, αij computed by Eq. (7) may be softly
adjusted according to the implicit impact brought by the structural coefficients. Moreover, the rel-
ative significance r can also be automatically inferred by JAT as it is involved into the back propa-
gation process. More smooth and appropriate attention scores can thereby be computed by JAT for
learning meaningful representations.
To enhance the structural impact, JAT has another strategy, named as Explicit direction, to compute
attention scores between neighbors. Given fij and sij , the attention scores obtained via Explicit
direction strategy is defined as follows:
_	fij ∙ Sij
j	Σk∈Ni fik ∙ Sik
(8)
Compared with Eq. (7), the structural coefficient explicitly influences the magnitude of fij , so
that those node pairs which are structurally irrelevant are impossible to assign with high attention
weights. Utilizing Explicit direction strategy, JAT becomes more structure dependent when perform-
ing message passing in its neural architecture.
Having obtained the attention scores, JAT is now able to compute a linear combination of features
corresponding to each node and its neighbors as the layer-wise output, which will be either propa-
gated to the higher layer, or be used as the final representations for subsequent learning tasks. The
mentioned output features can be computed as follows:
hli+1 =	αijWhlj .
j ∈Ni
(9)
In Fig. 1, the joint attention mechanisms proposed in this paper are graphically illustrated. And we
may use a particular number of joint attention layers utilizing the proposed attention mechanisms to
construct JATs. In practice, we also adopt the multi-head attention strategy (Vaswani et al., 2017) to
stabilize the learning process. JATs either concatenate the node features from multiple hidden layers
as the input for next layer, or compute the average of node features obtained by multiple units of
output layers as the final node representations. The details on how to implement multi-head attention
in graph neural networks can be checked in (Velickovic et al., 2018).
2.4	Comparison to related graph neural networks
Based on the structure of the introduced joint attention layer, it is found that Graph Joint Attention
Networks are quite different from previous related neural architectures and have several advantages.
When compared with spectral based GNNs, the proposed JATs provide a dynamic way to update
layer-wise representations. In each layer, the updating function for node representation in prevalent
spectral GNNs, can generally be formulated as h0i = σ( j∈N AijhjW), where A represents a
static matrix/tensor, which can be constructed according to graph normalization (Kipf & Welling,
2017), or graph diffusion (Klicpera et al., 2019; Busch et al., 2020), so as to preserve particular
spectral properties of graph structure. As it shows, the message passing in spectral GNNs is defined
according to the static matrix/tensor preserving structural properties. In contrast, JATs compute the
attention score (αij ) for each pair of neighbors based on structural coefficient and node features,
which naturally leads each layer in the neural network to pay more attention to those node pairs
sharing similar structures and features. Higher-order representations concerning such analogous
neighbors in the graph can therefore be learned by JATs.
When compared with spatial GNNs, especially those attention based ones (Velickovic et al., 2018;
Zhang et al., 2020), JATs adopt adaptive attention mechanisms which smoothly determine the rela-
tive significance between graph structure and node features, to compute more appropriate attention
scores for representation learning. Compared with GATs (Velickovic et al., 2018), JATs are able
to compute attention scores for first order neighbors according to both structural coefficients and
node features, so that the attention-based message passing considers more structural information.
JATs are also different from those spatial GNNs which update node representations via sampling a
fixed size of neighbors. For example, to reduce the computational demand, GraphSAGE (Hamilton
et al., 2017) performs the task of graph representation learning via max pooling. Another attention
based framework, ADSF (Zhang et al., 2020) also takes into consideration structural information
when computing attention scores. However, ADSF cannot infer the relative significance between
5
Under review as a conference paper at ICLR 2021
the utilized topological information and node features, and ADSF considers the best k neighbors for
computing attention scores. Unlike JATs, such spatial GNNs sampling neighbors for representation
updating cannot fully utilize the structural information provided by proximal nodes in the graph.
Due to the flexibility of the proposed attention mechanisms, JATs can be readily combined with any
paradigm which can learn the topological correlations, just like the structural coefficients used in
this paper. This property enhances the applicability of the proposed JATs.
3	More expressively powerful JATs
Study on the expressive power of various GNNs has drawn much attention in the recent. It concerns
whether a given GNN can distinguish different structures where vertices possessing various vector-
ized features. It has been found that what the neighborhood aggregation functions, or readout opera-
tors of all message-passing GNNs, including GCNs, GATs, and other related aim at are analogous to
what 1-dimensional Weisfeiler-Lehman test (1-WL test), which is injective and iteratively operated
in Weisfeiler-Lehman algorithm (Weisfeiler & Leman, 1968; Xu et al., 2018; Zhang & Xie, 2020),
does. In other words, both aggregation/readout functions in message-passing GNNs and 1-WL test
attempt to distinguish structures which are different in some ways. As a result, all message-passing
GNNs are as most powerful as 1-WL test (Xu et al., 2018). The theoretical validation of the expres-
sive power of a given GNN thereby lies in whether those adopted aggreation/readout functions are
homogeneous to 1-WL test.
Having the effective framework evaluating the expressive power of a given message-passing GNN,
one may naturally be interested in whether the proposed JATs can distinguish all different graph
structures as 1-WL test does. As we mainly consider node classification tasks, in this section, we
investigate the expressive power of the neighborhood aggregation function concerning the joint at-
tention mechanisms in Eqs. (7) and (8). We firstly show that the neighborhood aggregation function
utilized by JATs still fails to discriminate some graph structures possessing certain topological prop-
erties. Then, we propose a simple but effective strategy for the joint attention mechanisms to enable
JATs to successfully distinguish all those graph structures that previously cannot be discriminated.
For the neighborhood aggregation utilizing the attention mechanism shown in Eq. (7), we have
the following theorem pointing out the conditions under which the aggregation function fails to
distinguish different structures.
Theorem 1 Assume the feature space X is countable and the aggregation function concerns the
attention mechanism in Eq. (7) is represented as h(c, X) = x∈X αcxg(x), where c is the feature of
center node, X ∈ X is a multiset containing the feature vectors from nodes in Ni, g(∙) is afunction
for mapping input feature X, and αcx is the attention score between f(c) and f (x). For all g and the
joint attention mechanism in Eq. (7), h(c1,X1) = h(c2,X2) if and only if ci = c2, Xi = {S,μι},
X2 = {S,μ2} ,and Σy = χ,y∈Xι fciy- Σy = χ,y∈X2 f。2 y = 9 [£ y = χ,y∈X2 sc2 y - y=x,y∈X1 sc1 y],
for q = rs and X ∈ S .In other words, h will map different multiset into the same embedding iffthe
multisets have same central node feature, same underlying set, and the difference in feature-based
attention SCOreS is proportional (rs) to the opposite of that in attention weights corresponding to
structural coefficients.
We leave the proof of all the theorems and corollaries in the appendix. For the aggregation function
utilizing the attention mechanism shown in Eq. (8), we have the following theorem indicating the
structures which cannot be correctly distinguished.
Theorem 2 Under the same assumptions shown in Theorem 1, for all g and the joint attention
mechanism in Eq. (8), h(c1,X1) = h(c2, X2) if and only if ci = c2, Xi = {S, μι}, X2 = {S, μ2},
and q ∙ Py=x,y∈χι Φ(Ccix) = py=x,y∈χ2 Φ(Cc2y) ,for q > 0 and X ∈ S, where φ(∙) is anfunction
for mapping values to R+. In other words, h will map different multiset into the same embedding
iff the multisets have same central node feature, same node features whose corresponding mapped
structural coefficients are proportional.
Theorems 1 and 2 indicate that the joint attention mechanisms may still fail to distinguish some
graph structures, although the aggregation functions adopting the joint attention mechanisms may
be more expressively powerful than empirical GATs. As node features and graph structure are
6
Under review as a conference paper at ICLR 2021
heterogeneous, intuitively, sub-structures satisfying the mentioned conditions should be infrequent.
This may well explain why those attention-based GNNs concerning including structural factors
into the computation of attention scores may experimentally perform better than GATs. However,
when distinct multisets with corresponding structural properties meet the conditions mentioned in
Theorems 1 and 2, the joint attention mechanisms in Eqs. (7) and (8) cannot correctly distinguish
such multisets. Thus, JATs fail to reach the upper bound of expressive power of all message-passing
GNNs, i.e., 1-WL test.
However, we are able to readily improve the expressive power of JATs to meet the condition of 1-
WL test by slightly modifying the joint attention mechanisms. The modified attention mechanisms
are defined as follows:
αij	j ∈ Ni , j 6= i,
ij- [αij + J PNT j = i,e> 0,
where αij is the attention weight obtained by either Eq. (7) or (8). Then, the newly obtained
attention scores can be used to aggregate the node features passed to the higher layers. Next, we
prove that such improved attention mechanisms reach the upper bound of message-passing GNNs
via showing they can distinguish those structures possessing the properties mentioned in Theorems
1 and 2.
Corollary 1 Let T be the attention-based aggregator shown in Eq. (9) that considers the attention
mechanism in Eq. (7) or (8) and operates on a multiset H ∈ H, where H is a node feature space
mapped from the countable input feature space X. A H exists so that utilizing αij in Eq. (10), T
can distinguish all different multisets in aggregation that it previously cannot discriminate.
Based on Eq. (10), one may use original JATs to perform different classification tasks in graph
data by setting = 0. The expressive power of JATs can immediately reach the upper bound of
message-passing GNNs when is set as a positive value. Theoretically, the expressive power of
JATs is stronger than state-of-the-art attention-based GNNs, e.g., GATs (Velickovic et al., 2018),
and ADSF (Zhang et al., 2020). It should be noted that, all the theoretical validations presented in
this paper assume the input feature space is countable. The general framework for validating GNNs’
expressive power in the uncountable feature space can be checked in (Corso et al., 2020).
4	Experiments and analysis
In this section, we evaluate the proposed Graph Joint Attention Networks (JATs) against a variety of
prevalent baselines, on widely used network datasets.
4.1	Experimental set-up
To validate the effectiveness of JATs, we compare them with a number of state-of-the-art baselines,
including Gaussian fields and harmonic functions (GF) (Zhu et al., 2003), Manifold regularization
(Mani-reg) (Belkin et al., 2006), Deepwalk (Perozzi et al., 2014), Semi-supervised graph embedding
(Planetoid) (Yang et al., 2016), Node2Vec (Grover & Leskovec, 2016), Graph convolutional net-
works (GCN) (Kipf & Welling, 2017), GCN with Chebyshev filters (Chebyshev) (Defferrard et al.,
2016), GraphSAGE (Hamilton et al., 2017), mixture model CNN (MoNet) (Monti et al., 2017),
Graph attention networks (GAT) (Velickovic et al., 2018), Bayesian GCN (BGCN) (Hasanzadeh
et al., 2020), and Adaptive structural fingerprints (ADSF) (Zhang et al., 2020). Based on the exper-
imental results previously reported, these baselines may represent the most advanced techniques for
learning in graph structured data.
Four widely-used document networks, which are Cora, Citeseer, Pubmed (Lu & Getoor, 2003; Sen
et al., 2008), and OGBN-Arxiv (Hu et al., 2020), are used in our experiments. Cora, Citeseer, and
Pubmed are three classical network datasets for validating the effectiveness of GNNs. However, it is
recently found that these three datasets sometimes may not effectively validate the predictive power
of different graph learning approaches, due to the relatively small data size and data leakage (Shchur
et al., 2018; Hu et al., 2020). Thus, more massive datasets having better data quality have been
proposed to evaluate the performance of different approaches (Hu et al., 2020; Dwivedi et al., 2020).
To effectively test JATs, we additionally use OGBN-Arxiv, which is available in the Open graph
7
Under review as a conference paper at ICLR 2021
Table 1: Average Accuracy on classical document networks
Approaches	Cora	Citeseer	Pubmed
GF (ZhU et al., 2003)	-680%^^	45.3%	63.0%
Mani-reg (Belkin et al., 2006)	59.5%	60.1%	70.7%
DeePwalk (Perozzi et al., 2014)	67.2%	43.2%	65.3%
Planetoid (Yang et al., 2016)	75.7%	64.7%	77.2%
Chebyshev (Defferrard et al., 2016)	81.2%	69.8%	74.4%
GCN (KiPf & Welling, 2017)	81.5%	70.3%	79.0%
MoNet (Monti et al., 2017)	81.7%	一	78.8%
GAT (VeIickovic et al., 2018)	83.0%	72.5%	79.0%
BGCN Hasanzadeh et al. (2020)	82.2%	70.0%	—
ADSF (Zhang et al., 2020)	84.7%	73.8%	79.4%
JAT-E	85.5±0.4%	73.8±0.4%	82.0±0.3%
JAT-I	85.8±0.5%	74.3±0.4%	82.8±0.4%
Table 2: Average Accuracy on OGBN-Arxiv
Approaches	Train	Accuracy (%) Validation	Test
MLP	-63.6%^^	57.7%	55.5%
Node2Vec (Grover & Leskovec, 2016)	76.4%	71.3%	70.1%
GCN (KiPf & Welling, 2017)	78.9%	73.0%	71.7%
GraPhSAGE (Hamilton et al., 2017)	82.3%	72.8%	71.5%
JAT-E	81.3±0.2%	73.8±0.1%	72.6±0.06%
JAT-I	81.2±0.1%	73.4±0.08%	72.9±0.2%
benchmark database, as one of the testing datasets. The effectiveness of all methods is validated
via allowing them to perform semi-supervised node classification (transductive learning) in all the
benchmarking sets and the classified nodes are evaluated using Accuracy (M icro-F1). To compare
JATs impartially with other baselines, we closely follow the experimental paradigms used in the
related works (Yang et al., 2016; KiPf & Welling, 2017; VeliCkovic et al., 2018; HU et al., 2020). We
leave the details of testing datasets and experimental scenarios in appendix due to space limitation.
4.2	Results on node classification
We obtain the average classification accUracy over 10 rUns of JATs and comPare it with other state-
of-the-art aPProaches. The corresPonding resUlts are sUmmarized in Table 1 and 2. As the tables
show, JATs Utilizing two different joint attention mechanisms oUtPerform all the baselines on all the
foUr testing datasets. SPecifically, the average Accuracy on node classification obtained by JAT-I
(JAT Using Implicit direction strategy) achieves 85.8%, 74.3%, 82.8% and 72.9% on Cora, Citeseer,
PUbmed, and OGBN-Arxiv, resPectively. The Presented exPerimental resUlts demonstrate that the
ProPosed method is one of the most effective GNNs for learning in graPh strUctUred data.
4.3	Ablation study
Besides testing the Performance of transdUctive learning in network data, we fUrther investi-
gate how different settings of JATs may imPact their Performance. SPecifically, we first in-
vestigate how JATs Perform when different valUes of are Used. The sensitivity test on =
[10-8, 10-6, 10-4, 10-2, 10-1, 1, 5, 10] is Plotted in Fig. 2 (a) and (b). As the figUres dePict, both
two versions of JATs Perform robUstly when ≤ 1 and the imProved strategy shown in Eq. (10) may
boost the Performance of JATs in some datasets, e.g., PUbmed. ThUs, we recommend to set ≤ 1 for
both two versions of JATs. SUch setting may ensUre JATs leverage the joint attention mechanisms as
well as Preserve the exPressive Power, when classifying nodes in graPh strUctUred data.
Then, to fUrther investigate whether the strategy of aUtomatically determining the relative signifi-
cance between featUre and strUctUral attention (Eqs. (6) and (7)) may trUly imProve the Performance
8
Under review as a conference paper at ICLR 2021
Table 3: Performance comparison on JATs using different structural information. C, J, or SSL
means JAT uses Cosine or Jaccard similarity to compute the structural correlations for attention
score computation, or only uses structural coefficents to compute attention scores.
Different versions of JATs	Cora	Citeseer	Pubmed	OGBN-Arxiv
JAT-EC	85.3±0.1%	73.4±0.3%	80.0±0.1%	72.4±0.2%
JAT-IC	84.7±0.3%	73.5±0.3%	79.7±0.3%	72.5±0.2%
JAT-EJ	85.2±0.1%	73.2±0.4%	79.8±0.2%	72.6±0.2%
JAT-IJ	85.1±0.2%	73.8±0.1%	79.4±0.1%	72.4±0.1%
JAT-SSL	85.8 ±0.4%	73.1±0.7%	81.0±0.6%	72.4±0.07%
JAT-E	85.5±0.4%	73.8±0.4%	82.0±0.3%	72.6±0.06%
	JAT-I		85.8±0.5%	74.3±0.4%	82.8±0.4%	72.9+0.2%
on transductive learning, we compare JAT-I with GAT, ADSF, and JAT utilizing the attention strategy
shown in Eq. (7) but setting rf = rs = 0.5 (JAT-I w/o ad). As Fig. 2 (c) shows, JAT-I can out-
perform all the baselines on the used testing datasets, including Cora, Citesser, and Pubmed. When
compared with JAT-I w/o ad, JAT-I can significantly improve the performance on semi-supervised
node classification on all the used datasets, at 95% confidence level. The strategy of automatic de-
termination adopted by JATs is therefore effective in learning node representation for the subsequent
predictive tasks.
Finally, we further test the effectiveness of the proposed joint attention mechanisms by replacing the
structural coefficients (C) with other effective strategies. Specifically, we use Cosine and Jaccard
similarity measures to compute the structural correlations for connected node pairs and then let
JAT utilize them to compute attentions scores for the network training. Besides, we also allow one
version of the JATs (JAT-SSL) to only use the structural coefficients learned via minimizing Eq. (2)
to compute the attention scores in the training stage. The corresponding results are summarized
in Table 3. As the table shows, JAT still performs robustly when using simple measures, e.g.,
Cosine and Jaccard similarity to obtain node-node structural correlations, although its performance
is not better than the original versions of JAT. Given the results shown in Table 3, it is found that
the proposed joint attention mechanisms are effective in learning node representations. The JAT
framework can therefore be readily combined with many effective approaches which can infer node-
node structural correlations, such as network embedding (Grover & Leskovec, 2016; Armandpour
et al., 2019) and deep structural learning (Monti et al., 2017) for effective representation learning.
(％) >OS3OQ<
(a) JAT-I
(c) JAT-I with/without ad
Figure 2: Sensitivity test on and automatic determination of S/F significance
(b) JAT-E
5	Conclusion
In this paper, we propose novel attention-based GNNs, dubbed Graph Joint Attention Networks
(JATs). Different from previous related approaches, JATs adopt novel joint attention mechanisms
that can smoothly infer the relative significance between graph structure and node features, so that
structural properties and node features are appropriately preserved in node representations. Besides,
the expressive power of JATs is theoretically analyzed and the improved strategy to ensure JATs to
be most powerful message-passing GNNs is also proposed. In future, we will further improve the
effectiveness of JATs by considering different structural properties hidden in the network data and
explore JATs’ applicability by using them in multi-view and heterogeneous network data.
9
Under review as a conference paper at ICLR 2021
References
Mohammadreza Armandpour, Patrick Ding, Jianhua Huang, and Xia Hu. Robust negative sam-
pling for network embedding. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33,pp. 3191-3198, 2019.
James Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in neural
information processing systems, pp. 1993-2001, 2016.
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-
work for learning from labeled and unlabeled examples. Journal of machine learning research, 7
(Nov):2399-2434, 2006.
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Julian Busch, Jiaxing Pi, and Thomas Seidl. Pushnet: Efficient and adaptive neural message passing.
arXiv preprint arXiv:2003.02228, 2020.
Yaoming Cai, Zijia Zhang, Zhihua Cai, Xiaobo Liu, Xinwei Jiang, and Qin Yan. Graph convo-
lutional subspace clustering: A robust subspace clustering framework for hyperspectral image.
arXiv preprint arXiv:2004.10476, 2020.
Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language
Processing, pp. 551-561, 2016.
Gabriele Corso, LUca Cavalleri, DominiqUe Beaini, Pietro Lio, and Petar VeliCkovic. Principal
neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
David K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alan
Aspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular
fingerprints. In Advances in neural information processing systems, pp. 2224-2232, 2015.
Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier Bresson.
Benchmarking graph neural networks. arXiv preprint arXiv:2003.00982, 2020.
Ehsan Elhamifar and Rene Vidal. Sparse subspace clustering: Algorithm, theory, and applications.
IEEE transactions on pattern analysis and machine intelligence, 35(11):2765-2781, 2013.
Chengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Frage: Frequency-agnostic
word representation. In Advances in neural information processing systems, pp. 1334-1345, 2018.
Aditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings
of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 855-864, 2016.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024-1034, 2017.
Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick Duffield, Krishna
Narayanan, and Xiaoning Qian. Bayesian graph neural networks with adaptive connection sam-
pling. In International conference on machine learning, 2020.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. arXiv
preprint arXiv:2005.00687, 2020.
10
Under review as a conference paper at ICLR 2021
Pan Ji, Mathieu Salzmann, and Hongdong Li. Efficient dense subspace clustering. In IEEE Winter
Conference on Applications of Computer Vision, pp. 461-468. IEEE, 2014.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations, 2017.
Johannes Klicpera, Stefan WeiBenberger, and StePhan Gunnemann. Diffusion improves graph learn-
ing. In Advances in Neural Information Processing Systems, pp. 13354-13366, 2019.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the Twentieth International
Conference on International Conference on Machine Learning, pp. 496-503, 2003.
Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-
based neural machine translation. In Proceedings of the 2015 Conference on Empirical Methods
in Natural Language Processing, pp. 1412-1421, 2015.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5115-
5124, 2017.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-
works for graphs. In International conference on machine learning, pp. 2014-2023, 2016.
Bryan Perozzi, Rami Al-Rfou, and Steven Skiena. Deepwalk: Online learning of social repre-
sentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 701-710, 2014.
Leonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node
representations from structural identity. In Proceedings of the 23rd ACM SIGKDD international
conference on knowledge discovery and data mining, pp. 385-394, 2017.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan Gunnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale
information network embedding. In Proceedings of the 24th international conference on world
wide web, pp. 1067-1077, 2015.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In International Conference on Learning Representations,
2018.
B Weisfeiler and A Leman. The reduction of a graph to canonical form and the algebgra which
appears therein. NTI, Series, 2, 1968.
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A
comprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and
Learning Systems, 2020.
11
Under review as a conference paper at ICLR 2021
Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich
Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual
attention. In International conference on machine learning, pp. 2048-2057, 2015.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? arXiv preprint arXiv:1810.00826, 2018.
Li Xu, Jimmy SJ Ren, Ce Liu, and Jiaya Jia. Deep convolutional neural network for image decon-
volution. In Advances in neural information processing systems, pp. 1790-1798, 2014.
Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with
graph embeddings. In International conference on machine learning, pp. 40-48. PMLR, 2016.
Kai Zhang, Yaokang Zhu, Jun Wang, and Jie Zhang. Adaptive structural fingerprints for graph
attention networks. In International Conference on Learning Representations, 2020.
Shuo Zhang and Lei Xie. Improving attention mechanism in graph neural networks via cardinal-
ity preservation. In IJCAI: proceedings of the conference, volume 2020, pp. 1395. NIH Public
Access, 2020.
Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian
fields and harmonic functions. In Proceedings of the 20th International conference on Machine
learning (ICML-03), pp. 912-919, 2003.
12
Under review as a conference paper at ICLR 2021
A Proof of Theorem 1
To prove Theorem 1, we need to consider the two directions of the iff conditions (Zhang & Xie,
2020). If given c1 = c2, S1 = S2, and Py=x,y∈X1 fc1y - Py=x,y∈X2 fc2y = q[Py=x,y∈X2 sc2y -
Py=X y∈Xι Sciy]，for q = rs, for the aggregation function utilizing the joint attention mechanism
shown in Eq. (7), we have:
h(ci, Xi) =	αcixg(x),
x∈Xi
αCiX = rf ∙ fCiX + rs ∙ sCiX,
f =	exp(mcix)	S =	exp(Cciχ)
PX∈Xieχp (mCiX) SciX= pχ∈χi exP (CciX)
(11)
where mciX represents the feature similarity between ci and x. Given Eq. (11), we may directly
derive h(c1, X1) and h(c2, X2):
h(cι, Xi) = E αciχg(x) = £ [rf ∙ fcix + r ∙ scix] ∙ g(x)
X∈X1	X∈X1
h(c2, X2) = E αc2xg(X) = £ [rf ∙ fc2X + rs ∙ sc2X] ∙ g(X)
X∈X2	X∈X2
(12)
Considering c1	=	c2,	S1	=	S2,	and	y=X,y∈X1	fc1y	-	y=X,y∈X2	fc2y =	q[	y=X,y∈X2	Sc2y
Py=x,y∈x1 Sciy],for q = f ,Wedirectlyderive h(c1,X1) = h(c2,X2).
—
If we are given h(c1, X1) = h(c2, X2), we are able to prove that the conditions mentioned in the
theorem are necessary by showing contradictions occur when they are not satisfied. If h(c1, X1) =
h(c2 , X2 ), we have:
h(c1,X1)-h(c2,X2) =
E 吟∙ fciX + rs ∙ sciX] ∙ g(X)- £ [rf ∙ fc2 x + rs ∙ sc2X] ∙ g(X) = 0
X∈Xi	X∈X2
(13)
Firstly, assuming Si = S2, for any g(∙), we thereby have:
h(ci,Xi) - h(c2,X2)=	E [ E	[rf ∙ fciy + rs ∙ Sciy]
X∈Si ∩S2 y=X,y∈Xi
- E	[rf ∙ fc2y + rs ∙ Sc2y]] ∙ g(x)
y=X,y∈X2
+	[rf ∙ fciy + rs ∙ Sciy] ∙ g(X)
x∈S1 ∖S2y=x,y∈Xi
-E E	[rf ∙ fc2y + rs ∙ Sc2y]∙ g(X) = 0
X∈S2 \Si y=X,y∈X2
(14)
As Eq. (14) holds for any g(∙), we may define another function g0(∙) as follows:
g(X) = g0(X), for X ∈ Si ∩ S2
g(X) = g0 (X)	-	1, for X ∈	Si	\	S2
g(X) = g0 (X)	+	1, for X ∈	S2	\	Si
(15)
13
Under review as a conference paper at ICLR 2021
If Eq. (14) holds, we also have:
h(cι,Xι) - h(c2,X2)=	E [ E	[rf ∙ fcιy + Ts ∙ Sciy]
x∈S1∩S2 y=x,y∈X1
-	E	[rf ∙ fc2y+rs • sc2y]]∙ g0(x)
y=x,y∈X2
+	[rf • fc1 y + rs • sc1 y ] • g (x)
x∈S1 ∖S2y=χ,y∈X1
-	[rf • fc2y + rs • sc2y] • g0(x)
x∈S2 ∖S1 y=x,y∈X2
=	[	[rf • fc1y +rs • sc1y]
x∈S1∩S2 y=x,y∈X1
-	[rf • fc2y +rs • sc2y]] • g(x)
y=x,y∈X2
+	[rf • fc1y + rs • sc1 y] • [g(x) - 1]
x∈S1∖S2y=x,y∈X1
-	[rf • fc2y + rs • sc2y] • [g(x) + 1] = 0
x∈S2 ∖S1 y=x,y∈X2
As Eq. (14) equals Eq. (16), we have:
(16)
[rf • fc1y +rs • sc1y] +	[rf • fc2y +rs • sc2y] = 0
x∈S1∖S2y=x,y∈X1	x∈S2∖S1 y=x,y∈X2
(17)
Obviously, the above equation does not hold as the terms in the summation operator are positive.
Thus, S1 6= S2 is not true. We may now assume S1 = S2 = S. Eliminating the irrational terms in
Eq. (14), we have:
[	[rf • fc1y +rs • Sc1y] -	[rf • fc2y +rs • Sc2y]] • g(x) = 0
x∈S1 ∩S2 y=x,y∈X1
y=x,y∈X2
(18)
Thus we know each term in the summation equals zero:
[rf •	fc1y	+ rs	•	Sc1 y] -	[rf	•	fc2y	+ rs	•	Sc2y] = 0
y=x,y∈X1
y=x,y∈X2
(19)
As structural coefficients and node feature similarity are heterogeneous, we may
Py=x,y∈X1 rs • Sc1y = Py=x,y∈X2 rs • Sc2y. Eq. (19) can be simplified and rewritten as:
〃l(x) _ exP(mC2X) Pχ∈Sι Py = χ,y∈X exp (mc1x)
---:~~~ = ---:-----^=-----=-------------:-----
μ2(X)	exp (mCiX) Pχ∈S2 Py=χ,y∈X2 exp (mC2X)
assume
(20)
It is obvious that LHS of Eq. (20) is a rational number. However, the RHS of Eq. (20) can be an
irrational number. We may consider S = {S, S0 } and assume c1 = S0, c2 = S. We may also assume
the feature similarity between central node and others as follows:
mc1 x = 1, for x ∈ S
mc2s = 1, mc2s0 = 2
(21)
Consider x = S, we have:
μι(S)
|X1|
μ2(s)	∣X2∣ - n + ne
(22)
where n stands for the number of S0 in X2. It is obvious that the above equality does not hold as the
RHS is an irrational number, while LHS is a rational number. Thus c1 6= c2 is false. Given c1 = c2,
Eq. (19) can be rewritten as:
rf •	fc1y	-	rf	•	fc2y	+	rs	•	Sc1y	-	rs	• Sc2y = 0	(23)
y=X,y∈X1	y=X,y∈X2	y=X,y∈X1	y=X,y∈X2
14
Under review as a conference paper at ICLR 2021
To ensure above equation holds, we have:
E fcιy- E fc2y	= rs [ E sc2y- E	%y]	(24)
y=x,y∈X1	y=x,y∈X2	f y=x,y∈X2	y=x,y∈X1
Further denoting rs = q, we have Py = χ,y∈Xι fcιy - Py = x,y∈X2 f⅛y = q^y = x,y∈X2 sC2y -
y=x,y∈X1 sc1y].
B Proof of Theorem 2
To prove Theorem 2, we can follow the procedure which is used to prove Theorem 1. If given
c1
c2, S1 = S2, and q • y=x,y∈X1 φ(Cc1 y) =	y=x,y∈X1 φ(Cc2 y),for q > 0, we may directly
replace φ(∙) using exp{∙}. For the aggregation function using the joint attention mechanism in Eq.
(8) we have:
h(ci , Xi ) =	αcix g(x),
x∈Xi
fcix • scix
αci x
fci x
x∈Xi fcix • scix
exp (mcix)
-------------------S
x∈Xi exp (m
cix)
(25)
cix
exp (Ccix)
x∈Xi
where mcix represents the feature similarity between ci and x.
derive h(c1, X1) and h(c2, X2):
exp(Ccix),
Given Eq. (25), we may directly
h(c1,X1) =	αc1xg(x) =	[
fc1 x • sc1 x
x∈X1
x∈X1
h(c2, X2) =	αc2xg(x) =	[
x∈X1
fc2 x
fc1 x • sc1 x
]∙ g(X)
• sc2x
(26)
x∈X2
x∈X2
x∈X2 fc2x
] • g(x)
sc2x
Given S1 = S2 and c1 = c2, h(c2, X2) can be rewritten as:
h(C2, X2)= X [ P fc2xf Pyp,y∈X2 sc2y	] • g(x)
x∈S2	x∈S2 fc2 x •	y=x,y∈X2 sc2 y
exp (mc2x)	P
X [	py∈X2 exp(mc2y) .乙
P^ IP	exp (mc2x)
x∈S2 乙x∈s2 py∈χ2 eχp (mc2y)
eχp(Cc2y )
y=x,y∈X2 py∈x2 eχp (Cczy)
Σ
x∈S2
exp (mc2x )Ey=χ,y∈X2
y=x,y∈X2 P
exp(Cc2y)
eχp (Cc2y)
]∙ g(x)
(27)
;y∈X2 eXP (Cc2y )
x∈S2exp(mc2x) y=x,y∈X2 exp (Cc2y)
• g(x)

Given q •	x∈X1 exp{Cc1x} =	y=x exp{Cc2y}, Eq. (27) is equivalent to:
q • exp (mc2x) Py=x,y∈X1 exp (Cc1y)
h(c2,X2) = V ----------------Z---y-^y-1---------7g(—V • g(x)
S	x∈S1 q • exp (mc2x) y=x,y∈X1 exp (Cc1y)
(28)
Considering c1 = c2, we have h(c1 , X1) = h(c2, X2).
If we are given h(c1, X1) = h(c2, X2), we are able to prove the conditions mentioned in the the-
orem are necessary by showing contradictions occur when they are not satisfied. If h(c1, X1) =
h(c2, X2), we have:
h(c1,X1) -h(c2,X2) =
[P
x∈X1
fc1 x • sc1 x
x∈X1 fc1x • sc1x
] • g(x) -	[P
x∈X2
x∈X2 c2 x
] • g(x) = 0
sc2x
(29)
15
Under review as a conference paper at ICLR 2021
Firstly, assuming Si = S2, for any g(∙), We thereby have:
h(c1,X1) -h(c2,X2) =	[
x∈S1∩S2
eXp (mc1X) Ey = X ,y∈X1 eXR (CcIy )
PX∈S1 exp (mclX) Py = X,y∈Xι exp (Cc1 y)
eχp (mc2 X) Ey = X ,y∈X2 eXP (Cc2y )	] ^ (χ)
Px∈S2 exp (mc2X) Py=X,y∈X2 exp (Cc2# )
+[
X∈S1∖S2
[
X∈S2∖S1
exp (mciX) Ey = X,y∈X1 eXp (CcIy )	.
PX∈S1 exp (mclX) Py=X,y∈Xι exp (CcIy )「
[	exp(mc2X) ρy=X,y∈X2exp(Cc2y)
PX∈S2 exp (mc2X) Py=X,y∈X2exp (Cc2y )
] ∙ g(X)
]∙ g(x) = 0
(30)
—
—
As Eq. (30) holds for any g(∙), we may define another function g0(∙) as shown in Eq. (15). If Eq.
(30) holds, We also have:
h(c1 , X1) - h(c2 , X2) = X [
x∈S1∩S2
eχp (mC1 X) Ey = X ,y∈X1 eXR (CcIy )
x∈S1exp (mc1x)	y=x,y∈X1 exp (Cc1y)
—
eXp (mc2X) Ey = X ,y∈χ2 eXp (Cc2y )
PX∈S2exp (mc2X) Py=X,y∈X2exp (Cc2y)]
+[
X∈S1∖S2
[
X∈S2∖S1
exp (mciX) Ey=X,y∈χιexp (CcIy)
PX∈S1 exp (mclX) Py=X,y∈Xι exp (CcIy )
「	exp(mc2X) Py=X,y∈X2 exp(Cc2y )
]∙ g0(X)
[
X∈S1∩S2
X∈S2exp (mc2X)	y=X,y∈X2exp (Cc2y)
.	exp(mclX) Py = X,y∈Xι eXp(Cciy )
PX∈S1exp(mclX) Py=X,y∈Xι eXp(Cc1y)
]∙ g0(X)
(31)
—
—
eXP (mc2 X) Ey = X,y∈X2 MP (Cc2# )	] ^ (χ)
Px∈S2 exp (mc2X) Py=X,y∈X2 exp (Cc2y) g
+[
X∈S1∖S2
eχP (mcιχ) Ey = X,y∈X1 eXP (CcIy )
[
X∈S2∖S1
X∈S1eXp (mc1X)	y=X,y∈X1eXp (Cc1y)
[eXp(mc2X) Py=X,y∈χ2 eXp(Cc2y )
PX∈S2 exp (mc2X) Py=X,y∈X2exp (Cc2y )
]∙ [g(x)- 1]
]∙ [g(x) + 1] = 0
—
As Eq. (30) equals Eq. (31), We have:
,X [	eXp(mc1X) Ey = X,y∈X1 exp (CcIy)	]
x∈S∖ SPX∈S1 exp (mclX) Py = X,y∈Xι exp (CcIy )
+ ,X [	eXp(mc2X) Py = X,y∈X2 eXp(Cc2y)	] = 0
X∈S ∖S PX∈S2exp (mc2X)Py=X,y∈X2exp(Cc2 y )
(32)
Obviously, the above equation does not hold as softmax function is positive. Thus, S1 6= S2 is not
true. We may now assume S1 = S2 = S. Eliminating the irrational terms in Eq. (30), we have:
X [	exp (mclX)Ey = X,y∈X1 exp (CcIy )
X∈S1∩S2 Px∈Siexp (mciX) Py=X,y∈X1 exp (CcIy)
—
exp (mc2X)Ey=X,y∈X2exp(Cc2y)	] g(x) = 0
PX∈S2exp (mc2X) Py=X,y∈X2exp (Cc2y) ]
(33)
16
Under review as a conference paper at ICLR 2021
Thus we know each term in the summation equals zero:
eχP (mcix ) Ey=X,y∈Xι exP (CcIy )exP (mC2X) Ey = X,y∈X2 eXP (Cc?y )	=0
Px∈Sι exP (mcιx) Py=X,y∈Xι exP (CcIy )	Px∈S2 exP (mc2X)Py=χ,y∈X2exP (Cc2y )
(34)
Eq. (34) is equivalent to:
Ey=X,y∈Xl exP(CcIy ) = exP (mc2 X) Eχ∈Sι exP (mcix) fy=x,y∈X∖ exP (CcIy )
Py = X,y∈X2 exP (Cc2y ) exP (mclX) PX∈S2 exP (mc2X)Py = X,y∈X2 exP (Cc2y )
(35)
We may consider S = {s, s0 } and assume c1 = s0, c2 = s. We may also assume the feature
similarity between central node and others as follows:
mc1 X = 1, for x ∈ S
mc2s = 1, mc2s0 = 2
Consider x = s, we have:
Σs∈X1 exP(Ccis) _ e[e ∑s∈χ1 exP(Ccis) + e ∑s0∈χ1 exP (Cciso )]
Ps∈X2 exP(Cc2s) = eFp∈X^C□+^p;X^CzZM
(36)
(37)
As the learning of C is independent of feature mapping, and the computation of attention coeffi-
cients, exP (CcX) can be any positive value. By setting the exponential values in the above equation
as a, which is a positive value. We have ^1(|) = χ2∣Xn1^. Similar with Eq. (22), ci = c2 is not
true. Since c1 = c2 = c, Eq. (35) can be rewritten as:
Ey = X,y∈Xi exP(Ccy ) = Ex∈Si exP (mcX) Ey=X,y∈Xι exP (Ccy ) = const > 0
Py = X,y∈X2 exP (Ccy ) PX∈S2 exP(mcX)Py = X,y∈X2 exP(Ccy )	.
(38)
By setting ConSt as ɪ and exP (Ccy) = φ(Ccy) , we have q Py=Xy∈x1Φ(Ccy)
y=X,y∈X2φ(Ccy).
C Proof of Corollary 1
For any two distinct multisets X1
to Theorem 1, we denote X1
and X2 that cannot be distinguished by T, according
y=X,y∈Xi
fci y -	y=X,y∈X2
(S,μι), X2 = (S, μ2), c ∈ S.	We also
q[ y=X,y∈X2 Sc2y -	y=X,y∈Xi Sciy], for q
assume
_ rs
.
rf
When using the attention mechanism in Eq. (7) for the aggregation function, it is easy to verify
X∈X αcX f (x) = X∈X αcXf(x). When using the improved mechanism shown in Eq. (10),
we have PX∈X1 αcXf(x) - PX∈X2 αcXf(x) = e( ∣⅛ - IXT)αccf (c), where |Xi| = NH and
|X2| = |N2|. Since |X1| 6= |X2|, PX∈Xi αcXf(x) - PX∈X2 αcXf(x) 6= 0, meaning the improved
joint attention mechanism can distinguish those distinct multiset which T fails to distinguish. Sim-
ilarly, we can prove the aggregation function using the improved attention mechanism concerning
Eq. (8) also can distinguish those distinct multisets which T fails to distinguish.
Σ
D Detailed experimental set-up and additional tests
In this section, how the experiments used to validate the effectiveness of the proposed JATs are set
up is introduced with more details.
D. 1 Dataset description
Four document networks, which are Cora, Citeseer, Pubmed (Lu & Getoor, 2003; Sen et al., 2008),
and OGBN-Arxiv (Hu et al., 2020), are used in our experiments to validate the effectiveness of dif-
ferent GNNs. In these four networks, vertices, edges, and vertex features represent the documents,
citations between pairwise documents, and the bag-of-words representations of the documents, re-
spectively. Besides, each node in these four document networks has a class label.
17
Under review as a conference paper at ICLR 2021
Table 4: Characteristics of the testing datasets used in our experiments
	Cora	Citeseer	Pubmed	OGBN-Arxiv
N	2708	-^3327^^	19717	169343
|E|	5429	4732	44338	1166243
D	1433	3703	500	128
C	7	6	3	40
Training Nodes	140	120	60	90941
Validation Nodes	500	500	500	29799
Test Nodes	1000	1000	1000	48603
As graph transductive learning is mainly considered in our experiment to validate different ap-
proaches, we set up the experiment closely following the settings in the related works (Yang et al.,
2016; Velickovic et al., 2018; HU et al., 2020). All the datasets are split into three parts: training,
validation, and testing. For datasets Cora, Citeseer, and Pubmed, we use only 20 nodes per class for
training, bUt all the featUre vectors. For dataset OGBN-Arxiv, we Use a practical split strategy, seg-
menting the nodes which represent the academic papers, according to pUblication years (HU et al.,
2020). Papers pUblished Up to 2017 are in the training split. While papers pUblished in 2018 and
2019 are Used for validation and test, respectively. The performance of different approaches are
evalUated on the test split. The statistics of these benchmarking datasets are sUmmarized in Table
4, where N, |E|, D, and C denote the nUmber of vertices, edges, vertex featUres, and nUmber of
classes in each dataset, respectively.
D.2 Experimental scenarios
The effectiveness of different methods is validated via allowing them to perform semi-sUpervised
learning (transdUctive learning) in the aforementioned graph datasets. To compare the proposed
JATs impartially with other baselines, we closely follow the experimental paradigms Used in pre-
vious works (Yang et al., 2016; KiPf & Welling, 2017; Velickovic et al., 2018; Hu et al., 2020).
Specifically, only partial nodes in each class are labeled in the training stage, and all the graph data,
including vertex features and edges, can be accessible for the training of neural networks. The pre-
dictive ability of the approaches is validated using the nodes in the test split. Algorithm performance
is evaluated using Accuracy (M icro - F1).
For the network structure of JAT, following GAT (Velickovic et al., 2018), We use a similar two-
layer deep neural network to learn node representations in all the testing datasets. The first layer
consists of multiple attention heads computing multiple features, followed by an exponential linear
unit. In our experiment, eight attention heads are used for Cora, Citeseer, and Pubmed datasets and
three attention heads are used for OGBN-Arxiv. The second layer, composed of attention heads
and a softmax activation is used for node classification. For Cora, Citeseer and OGBN-Arxiv, a
single attention head is used, while for Pubmed, eight output attention heads are used, to compute
C features. As for the dimensionality of the hidden units, we also following the settings used in
previous works (Velickovic et al., 2018; Hu et al., 2020). The dimension of hidden units is set
as 8 for Cora, Cite, and Pubmed, and it is set as 256 for OGBN-Arxiv. Regularization is applied
within the model to cope with the small training set sizes. During training, L2 regularization with
λ = 0.0005 and dropout with p = 0.6 are applied for both layers’ inputs when JAT is training with
Cora, Citeseer, and Pubmed. In Ogbn-Arxiv dataset, the setting ofλ = 0 and dropout with p = 0.05
is used.
All the networks are initialized using Glorot initialization and trained to minimize cross-entropy on
the training nodes using the Adam SGD optimizer (Kingma & Ba, 2014). A learning rate of 0.01 is
used for datasets Cora, Citeseer, and Pubmed, while that of 0.002 is used in dataset OGBN-Arxiv.
As JAT requires structural coefficients to compute attention scores, Eq. (2) needs optimizing. In
our experiments, we use two different strategies to obtain optimal structural coefficients. For those
datasets with small size, e.g., Cora, Citeseer, and Pubmed, Eq. (2) is optimized together with the
training of the networks. In other words, a loss function aggregating cross-entropy on the training
nodes and structure reconstruction error Eq. (2) is used. Eq. (2) is iteratively optimized along with
the training of JAT and the structural coefficients obtained in each epoch can be used by JAT for
18
Under review as a conference paper at ICLR 2021
the computation of attention scores. For datasets possessing huge number of nodes, e.g., OGBN-
Arxiv, Eq. (2) is optimized ahead of training JATs for the purpose of dealing with limited computing
resources. Under such a case, the training efficiency of JATs can be significantly improved.
XCoraBCite Pubmed
70
0.01 0.1	1
5	10 20 50 100
β
70
0.01 0.1
5	10 20 50 100
β
(a) JAT-I
(b) JAT-E
Figure 3: Sensitivity test on β
D.3 SENSITIVITY TEST ONβ
For sensitivity test on β, which determines the relative significance of graph self-expression (Eq.
(2)), we set β = [0.01, 0.1, 1, 5, 10, 20, 50, 100] and let two versions of JAT to classify nodes on the
three smaller datasets. The corresponding results have been plotted in Fig. 3 (a) and (b). As the
figures depict, both two versions of JAT may obtain a robust performance when different settings
of β are used. It is experimentally found that the variations of β have a limited impact on the
performance of JATs. Based on the obtained results, we set β = 5 for JATs when they perform the
tasks of semi-supervised node classification on various datasets.
D.4 Performance on node-wise Implicit direction strategy
As defined in Eq. (6), gf and gs are layer-wise parameters which can capture the overall significance
of graph structure and features in each layer of the neural architecture. They naturally can be node-
level parameters in each layer of JAT, i.e., gif and gis . As a result, the original Implicit direction
strategy becomes node-wise Implicit direction strategy which can learn the relative significance of
graph structure and features of each node when computing attention scores. And such node-wise
Implicit direction strategy is defined as follows:
=	eχp (gif)	=
r,f	exp (gis) + exp (gif YriJi
=	rif ∙ fij + ris ∙ Sij
j	Pk∈Ni [rif . fik + ris ∙ Sik]
exp (gis )
exp (gis) + exp (gif),
= rif ∙ fij + ris ∙ Sij .
(39)
where rif or ris represents the normalized significance related to structure or node features asso-
ciated with node i. Given the node-level joint attention mechanism defined in Eq. (39), JAT also
can perform the task of representation learning in graph structured data. Here we compare its per-
formance on transductive learning in datasets Cora, Citeseer, and Pubmed with the original two
versions of JATs, i.e., JAT-I and JAT-E. The corresponding results are summarized in Table 5. As
the table shows, the performance of JAT utilizing the attention mechanism of node-wise Implicit
direction strategy is similar to that of JAT-E. According to the presented results, node-wise Implicit
direction strategy is a potentially effective method through which JAT can learn meaningful repre-
sentations for the subsequent predictive tasks. We will further investigate the effectiveness of this
new strategy in our future works.
19
Under review as a conference paper at ICLR 2021
Table 5: Performance comparison on JATs using different attention strategies
Different versions of JATs	Cora	Citeseer	Pubmed
JAT-E	85.5±0.4%	73.8±0.4%	82.0±0.3%
JAT-I	85.8±0.5%	74.3±0.4%	82.8±0.4%
JAT-I (node-wise)	85.4 ±0.3%	72.5±0.5%	82.6±0.2%
E	Discussions
In this section, we perform some detailed discussions that may help one to better understand the
proposed JATs.
E.1 Model complexity
Based on the descriptions in Section 2, we may analyze the computational complexity of the pro-
posed JATs. According to Eq. (3), for each node in the graph, the complexity for computing all the
structural coefficients can be expressed as O(N 2+N) for the worst case. However, as real graphs are
always very sparse, the complexity for learning structural coefficients can be reduced to O(e2 + e),
where e represents average degree of the nodes. As the learning of each Ci,: is independent of others,
Eq. (2) can be efficiently solved via parallel or distributed optimization strategies. For the computa-
tion of each attention head in the graph neural network, its complexity for learning F l+1 features for
each node can be represented as O(NDlDl+1 + (|E| +e)Dl+1), which is same to the classical GAT.
If there are K attention heads used, the complexity becomes O(K(N DlDl+1 + (|E| + e)Dl+1)).
E.2 Structural information for computing attention scores
In the proposed JATs, structural coefficients learned from graph subspace are used for computing at-
tention scores and the corresponding experimental performance is robust when compared with other
state-of-the-art baselines. The module for learning structural coefficients is motivated by subspace
clustering (Elhamifar & Vidal, 2013; Ji et al., 2014), which is an effective methodology for un-
covering data clusters through performing the spectral analysis on the coefficient matrix (C). Such
matrix is also named as self-expressiveness matrix as it contains the coefficients which can construct
a linear combination of each sample using others. Given Eq. (3), we know that the subspace learn-
ing module can directly infer the node-node correlations based on the graph data and those nodes
sharing similar global structures (i.e., graph connectivity) may have higher structural coefficients.
Rather than performing spectral analysis on C, JAT directly makes use of the coefficients in it to
compute attention scores for representation learning. This may allow JAT to pay more attention on
those nodes which are structurally correlated when aggregating features passed to the higher layers.
Though an effective method for structural coefficients learning, the proposed module is only one
of the alternatives that can be utilized by the generic framework JAT. As shown in the manuscript,
C learned by the subspace learning module is different from the one learned by those methods
for structural network embedding, such as Deepwalk (Perozzi et al., 2014), Node2Vec (Grover &
Leskovec, 2016), Struc2vec (Ribeiro et al., 2017), and LINE (Tang et al., 2015). For such network
embedding methods, the node-node correlation can be recovered via obtaining the inner product of
node-wise representations. Given the fact that these network embedding methods also can effec-
tively capture the node-node correlations, they are also effective alternatives for learning C for the
proposed JAT. In the ablation study, we show that JAT may perform robustly even working with
simple measures, e.g., Cosine and Jaccard similarity for obtaining node-node structural correlations,
which indicates the proposed joint attention mechanisms are the main reasons boosting the perfor-
mance of the attention-based GNNs. Thus, any measures/methods, that may effectively capture
structural relationship between pairwise nodes in the graph, including those network embedding ap-
proaches and deep structural learning approach such as MoNet (Monti et al., 2017) can be used by
JAT to capture node-node structural correlations for computing attention scores.
20
Under review as a conference paper at ICLR 2021
E.3 Relation between JAT and approaches to network embedding and others
Concatenating the learned structural representations and corresponding node features, effective net-
work embedding approaches, such as Deepwalk Perozzi et al. (2014), Node2Vec Grover & Leskovec
(2016), Struc2vec (Ribeiro et al., 2017), and LINE (Tang et al., 2015) can also consider both struc-
tural and feature information for the subsequent learning tasks. Compared with them, the proposed
JAT has its unique properties for learning graph representations. Rather than directly concatenating
the structural and feature vectors, JAT utilizes novel joint attention mechanisms which may automati-
cally determine whether graph structure or node feature is more important, to infer low-dimensional
node representations via multi-layer feature aggregation. As C is used for computing attention
scores, the node representations learned by JAT are automatically implanted with meaningful struc-
tural information hidden in the graph. It is seen that the strategy for automatically determining the
relative significance between graph structure and vertex features and the verified expressive power
distinguish the proposed JAT from other related works. Besides, for attention-based GNNs, the
proposed joint attention mechanisms are also novel attempts to effectively consider structural infor-
mation for computing attention scores. No previous mechanisms for GNNs can consider learning
the relative significance between graph structure and node features for computing attentions scores.
The proposed JAT is also different from those GNNs considering structural information, e.g., diffu-
sion (Klicpera et al., 2019; Busch et al., 2020) or GCNs for subspace clustering (Cai et al., 2020).
Those diffusion based GNNs mainly consider higher-order structural information among node pairs
to formulate the functions for feature aggregation. As for GNNs for subspace clustering, their main
task is to learn the self-expressive matrix, i.e., C, based on the deep neural networks and then data
clusters are inferred via performing spectral analysis on the learned C. Compared with them, JAT
directly makes use of C and node features to compute attention scores for aggregating the features
passed to the higher layers. C in the proposed JAT plays an important role in obtaining appropriate
feature aggregations, rather than clustering. Given the mentioned facts, the proposed JAT is novel
compared with existing methods.
E.4 Limitations of JAT
Though effective in predicting node labels in transductive learning tasks, the proposed JAT has some
limitations to overcome in the future. First, the efficiency for learning C in the current version of JAT
is somewhat unsatisfying. Based on the analysis on computational complexity, JAT has to pre-train
C for those massive datasets (e.g., OGBN-Arxiv) due to the limited computing resources. More
efficient techniques for capturing the node-node structural correlations can therefore be considered
by JAT. Based on the results shown in the ablation studies, the proposed joint attention mechanisms
ensure JAT to perform robustly as long as the used node-node correlations can effectively reveal the
structural properties of the graph. Thus, those network embedding approaches and other efficient
methods for deep structural learning can be combined with the generic JAT. Second, the expressive
power of JATs is theoretically validated mainly in the countable feature space in this paper. In those
graphs whose features are in uncountable space, the expressive power of JATs needs re-verifying fol-
lowing the recent framework proposed in (Corso et al., 2020). As mentioned in (Corso et al., 2020),
some theorems related to the expressive power in the countable feature space might not always hold
in the feature space uncountable. To the theoretical perspective, attention mechanisms which are
analogously powerful in both countable and uncountable feature spaces are thereby needed by both
JATs and other attention-based GNNs.
21