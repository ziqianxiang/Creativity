Under review as a conference paper at ICLR 2021
Revisiting Prioritized Experience Replay: A
Value Perspective
Anonymous authors
Paper under double-blind review
Ab stract
Reinforcement learning (RL) agents need to learn from past experiences. Priori-
tized experience replay that weighs experiences by their surprise (the magnitude
of the temporal-difference error) significantly improves the learning efficiency for
RL algorithms. Intuitively, surprise quantifies the unexpectedness of an expe-
rience to the learning agent. But how surprise is related to the importance of
experience is not well understood. To address this problem, we derive three value
metrics to quantify the importance of experience, which consider the extra reward
would be earned by accessing the experience. We theoretically show these value
metrics are upper-bounded by surprise for Q-learning. Furthermore, we success-
fully extend our theoretical framework to maximum-entropy RL by deriving the
lower and upper bounds of these value metrics for soft Q-learning, which turn out
to be the product of surprise and ”on-policyness” of the experiences. Our frame-
work links two important quantities in RL, i.e., surprise and value of experience,
and provides a theoretical basis to estimate the value of experience by surprise.
We empirically show that the bounds hold in practice, and experience replay using
the upper bound as priority improves maximum-entropy RL in Atari games.
1	Introduction
Learning from important experiences prevails in nature. In rodent hippocampus, memories with
higher importance, such as those associated with rewarding locations or large reward-prediction
errors, are replayed more frequently (Michon et al., 2019; Roscow et al., 2019; Salvetti et al., 2014).
People who have more frequent replay of high-reward associated memories show better performance
in memory tasks (Gruber et al., 2016; Schapiro et al., 2018). A normative theory suggests that
prioritized memory access according to the utility of memory explains hippocampal replay across
different memory tasks (Mattar & Daw, 2018). As accumulating new experiences is costly, utilizing
valuable past experiences is a key for efficient learning (OIafsdottlr et al., 2018).
Differentiating important experiences from unimportant ones also benefits reinforcement learning
(RL) algorithms (Katharopoulos & Fleuret, 2018). Prioritized experience replay (PER) (Schaul
et al., 2016) is an experience replay technique built on deep Q-network (DQN) (Mnih et al., 2015),
which weighs the importance of samples by their surprise, the magnitude of the temporal-difference
(TD) error. As a result, experiences with larger surprise are sampled more frequently. PER signif-
icantly improves the learning efficiency of DQN, and has been adopted (Hessel et al., 2018; Hor-
gan et al., 2018; Kapturowski et al., 2019) and extended (Daley & Amato, 2019; Pan et al., 2018;
Schlegel et al., 2019) by various deep RL algorithms. Surprise quantifies the unexpectedness of
an experience to a learning agent, and biologically corresponds to the signal of reward prediction
error in dopamine system (Schultz et al., 1997; Glimcher, 2011), which directly shapes the memory
of animal and human (Lisman & Grace, 2005; McNamara et al., 2014). However, how surprise is
related to the importance of experience in the context of RL is not well understood.
We address this problem from an economic perspective, by linking surprise to value of experience in
RL. The goal ofRL agent is to maximize the expected cumulative reward, which is achieved through
learning from experiences. For Q-learning, an update on an experience will lead to a more accurate
prediction of the action-value or a better policy, which increases the expected cumulative reward the
agent may get. We define the value of experience as the increase in the expected cumulative reward
resulted from updating on the experience (Mattar & Daw, 2018). The value of experience quantifies
1
Under review as a conference paper at ICLR 2021
the importance of experience from first principles: assuming that the agent is economically rational
and has full information about the value of experience, it will choose the most valuable experience
to update, which will yield the highest utility. As supplements, we derive two more value metrics,
which corresponds to the evaluation improvement value and policy improvement value due to update
on an experience.
In this work, we mathematically show that these value metrics are upper-bounded by surprise for
Q-learning. Therefore, surprise implicitly tracks the value of experience, and accounts for the impor-
tance of experience. We further extend our framework to maximum-entropy RL, which augments
the reward with an entropy term to encourage exploration (Haarnoja et al., 2017). We derive the
lower and upper bounds of these value metrics for soft Q-learning, which are related to surprise
and ”on-policyness” of the experience. Experiments in Maze and CartPole support our theoretical
results for both tabular and function approximation RL methods, showing that the derived bounds
hold in practice. Moreover, we also show that experience replay using the upper bound as priority
improves maximum-entropy RL (i.e., soft DQN) in Atari games.
2	Motivation
2.1	Q-learning and Experience Replay
We consider a Markov Decision Process (MDP) defined by a tuple {S, A, P, R, γ}, where S is a
finite set of states, A is a finite set of actions, P is the transition function, R is the reward function,
and Y ∈ [0,1] is the discount factor. A policy ∏ of an agent assigns probability ∏(a∣s) to each
action a ∈ A given state s ∈ S . The goal is to learn an optimal policy that maximizes the expected
discounted return starting from time step t, Gt = rt + γrt+1 + γ2rt+2 + ... = Pi∞=0 γirt+i, where
rt is the reward the agent receives at time step t. Value function vπ (s) is defined as the expected
return starting from state s following policy π, and Q-function qπ (s, a) is the expected return on
performing action a in state s and subsequently following policy π.
According to Q-learning (Watkins & Dayan, 1992), the optimal policy can be learned through policy
iteration: performing policy evaluation and policy improvement interactively and iteratively. For
each policy evaluation, we update Q(s, a), an estimate of qπ(s, a), by
Qnew(s, a) = Qold(s, a) + αTD(s, a, r, s0),
where the TD error TD(s, a, r, s0) = r + γ maxa0 Qold(s0, a0) - Qold(s, a) and α is the step-size
parameter. Qnew and Qold denote the estimated Q-function before and after the update respectively.
And for each policy improvement, we update the policy from πold to πnew according to the newly
estimated Q-function,
πnew = arg max Qnew(s, a).
a
Standard Q-learning only uses each experience once before disregarded, which is sample inefficient
and can be improved by experience replay technique (Lin, 1992). We denote the experience that the
agent collected at time k by a 4-tuple ek = {sk, ak, rk, s0k}. According to experience replay, the
experience ek is stored into the replay buffer and can be accessed multiple times during learning.
2.2	Value Metrics of Experience
To quantify the importance of experience, we derive three value metrics of experience. The utility of
update on experience ek is defined as the value added to the cumulative discounted rewards starting
from state sk, after updating on ek. Intuitively, choosing the most valuable experience for update
will yield the highest utility to the agent. We denote such utility as the expected value of backup
EVB(ek) (Mattar & Daw, 2018),
EVB(ek) = vπnew (sk) - vπold (sk)
=Ennew(a|sk )q∏new (sk ,a) - £nold⑷Sk )q∏oid (Sk, a)	(I)
where πold, vπold and qπold are respectively the policy, value function and Q-function before the up-
date, and πnew, vπnew , and qπnew are those after. As the update on experience ek consists of policy
2
Under review as a conference paper at ICLR 2021
a	Linear GridWorld	d
—$$$$$$ 令倒
一$$$$合伞©令I /
卜Q <◎ $ ◎ ◎ ◎	~I	/
I回$$$◎©<>©令 I	/Time
Figure 1: a. Illustration of the “Linear GridWorld” example: there are N grids and 4 actions (north,
south, east, west). Reward for entering the goal state (cheese) is 1; reward is 0 elsewhere. b-c.
Examples of prioritized experience replay by surprise and value of experience (EVB). The main
difference is that EVB only prioritizes the experiences that are associated with the optimal policy;
while surprise is sensitive to changes in value function and will prioritize non-optimal experiences,
such as those associated with north or south. Here squares represent states, triangles represent
actions, and experiences associated with the highest priority are highlighted. d. Expected number of
replays needed to learn the optimal policy, as the number of grids changes: uniform replay (blue),
prioritized by surprise (orange), and EVB (green).
evaluation and policy improvement, the value of experience can further be separated to evaluation
improvement value EIV(ek) and policy improvement value PIV(ek) by rewriting (1):
EVB(ek) = f[∏new(a∣Sk) - ∏oid(a∣Sk)]q∏∏ew(sk,a) + £∏oid(a∣Sk)[q∏∏ew(sk,a) — q∏id(sk,a)],
(2)
a
a
/
^^~{^^^™
PIV(ek)
^^™{^^^™
EIV(ek)
where PIV(ek) measures the value improvements due to the change of the policy, and EIV(ek) cap-
tures those due to the change of evaluation. Thus, we have three metrics for the value of experience:
EVB, PIV and EIV.
2.3	Value Metrics of Experience in Q-Learning
For Q-learning, we use Q-function to estimate the true action-value function. A backup over an
experience ek consists of policy evaluation with Bellman operator and greedy policy improvement.
As the policy improvement is greedy, we can rewrite value metrics of experience to simpler forms.
EVB can be written as follows from (1),
EVB(ek) = max Qnew(sk , a) - max Qold(sk , a).	(3)
Note that EVB here is different from that in Mattar & Daw (2018): in our case, EVB is derived from
Q-learning; while in their case, EVB is derived from Dyna, a model-based RL algorithm (Sutton,
1990). Similarly, from (2), PIV can be written as
PIV(ek) = max Qnew(sk , a) - Qnew (sk , aold),	(4)
a
where aold = arg maxa Qold(sk, a), and EIV can be written as
EIV(ek) = Qnew (sk , aold) - Qold (sk , aold).	(5)
2.4	A Motivating Example
We illustrate the potential gain of value of experience in a “Linear GridWorld” environment (Fig-
ure 1a). This environment contains N linearly-aligned grids and 4 actions (north, south, east, west).
3
Under review as a conference paper at ICLR 2021
The rewards are rare: 1 for entering the goal state and 0 elsewhere. The solution for this environment
is always choosing east.
We use this example to highlight the difference between prioritization strategies. Three agents per-
form Q-learning updates on the experiences drawn from the same replay buffer, which contains all
the (4N ) experiences and associated rewards. The first agent replays the experiences uniformly at
random, while the other two agents invoke the oracle to prioritize the experiences, which greedily
select the experience with the highest surprise or EVB respectively. In order to learn the optimal
policy, agents need to replay the experiences associated with action east in a reverse order.
For the agent with random replay, the expected number of replays required is 4N2 (Figure 1d). For
the other two agents, prioritization significantly reduces the number of replays required: prioriti-
zation with surprise requires 4N replays, and prioritization with EVB only uses N replays, which
is optimal (Figure 1d). The main difference is that EVB only prioritizes the experiences that are
associated with the optimal policy (Figure 1c), while the surprise is sensitive to changes in the value
function and will prioritize non-optimal experiences: for example, the agent may choose the experi-
ences associated with south or north in the second update, which are not optimal but have the same
surprise as the experience associated with east (Figure 1b). Thus, EVB that directily quantifies the
value of experience can serve as an optimal priority.
3	Upper Bounds of Value Metrics of Experience in Q-Learning
PER (Schaul et al., 2016) greatly improves the learning efficiency of DQN. However, the underlying
rationale is not well understood. Here, we prove that surprise is the upper bound of the value metrics
in Q-learning.
Theorem 3.1. The three value metrics of experience ek in Q-learning (|EVB|, |PIV| and |EIV|) are
bounded by a∣TD(Sk,a+ ,rk ,sk)∣, where a is a SteP-Size parameter.
Proof. See Appendix A.1.	□
In Theorem 3.1, we prove that |EVB|, |PIV|, and |EIV| are upper-bounded by the surprise (scaled
by the learning step-size) in Q-learning. As surprise intrinsically tracks the evaluation and policy
improvements, it can serve as an appropriate importance metric for past experiences. We will further
study these relationship in experiments.
4 Extension to Maximum-Entropy RL
In this section, we extend our framework to study the relationship between surprise and value of
experience in maximum-entropy RL, particularly, soft Q-learning.
4.1	Soft Q-Learning
Unlike regular RL algorithms, maximum-entropy RL augments the reward with an entropy term:
R = r + βH(∏(∙∣s)), where H(∙) is the entropy, and β is an optional temperature parameter that
determines the relative importance of entropy and reward. The goal is to maximize the expected
cumulative entropy-augmented rewards. Maximum-entropy RL algorithms have advantages at cap-
turing multiple modes of near optimal policies, better exploration, and better transfer between tasks.
Soft Q-learning is an off-policy value-based algorithm built on maximum-entropy RL principles
(Haarnoja et al., 2017; Schulman et al., 2017). Different from Q-learning, the target policy of soft Q-
learning is stochastic. During policy iteration, Q-function is updated through soft Bellman operator
Γsoft, and the policy is updated to a maximum-entropy policy:
Policy Evaluation: Qsnoefwt (s, a) = [ΓsoftQsooldft](s,a) = r +γVoslodft(s0)
Policy Improvement: ∏new(a∣s)
Softmaxa(1 QnoW (s,a)),
β
4
Under review as a conference paper at ICLR 2021
where softmaxi(x) = exp(xi)/	i exp(xi) is the softmax function, and the soft value function
Vπsoft(s) is defined as,
V∏S0ft(s) = Ea{Q∏Oft(s,a)- log(∏(a∣s))} = β log X exp(1 Q 票ft(s,a)).
β
a
Similar as in Q-learning, the TD-error in soft Q-learning (soft TD error) is given by:
TDsoft(s,a,r,s0)=r+γVoslodft(s0)-Qsooldft(s,a).
4.2 Value Metrics of Experience in Maximum-Entropy RL
Here, we extend the value metrics of experience to soft Q-learning. Similar as (1), EVB for
maximum-entropy RL is defined as,
EVBsoft(ek) =vnsoefwt(sk) -vosoldft(sk)
=	πnew (a|sk ){qnew (sk , a) - β log(πnew (a|sk ))}
a
-Enold(a∣Sk){qOodt(sk, a) - β log(∏0id(α∣Sk))}	(6)
a
EVBsoft can be separated into PIVsoft and EIVsoft, which respectively quantify the value of policy
and evaluation improvement in soft Q-learning,
PIVSoft(ek) = X∏new(a∣Sk){qnoW(sk,a) - βlog(∏new(a∣Sk))}
a
-Enold(a|sk){q；oW(Sk, a) - βlog(πold(a∣sk))}
a
=E {∏new(a∣Sk) - ∏old(a∣Sk)}q∖oW(sk, a) + β(H(∏new(∙∣s)) - H(∏old(∙∣Sk))), (7)
a
EIVSoft(ek) =Xnold(a|sk)[qnSoewft(sk,a)-qoSoldft(sk,a)].	(8)
a
Value metricS of experience in maximum-entropy RL have Similar formS aS in regular RL except for
the entropy term, becauSe changeS in policy leadS to changeS in the policy entropy and affectS the
entropy-augmented rewardS.
4.3 Lower and Upper B ounds of Value Metrics of Experience in Soft
Q-learning
We theoretically derive the lower and upper boundS of the value metricS of experience in Soft Q-
learning.
Theorem 4.1.	The three value metrics of experience ek in soft Q-learning (|EVBsoft |, |PIVsoft | and
|EIVsof|) are upper bounded by PmaX * ∣TDs°t∖, where p∏cax = max{nold(ak∣Sk),nnew(ak∣Sk)} is a
policy related term.
Proof. See Appendix A.2.	□
Theorem 4.2.	For soft Q-learning, |EVBsoft| and |EIVsoft| (but not |PIVsoft|) are lower bounded by
ρπmin * ∣TDsoft∣, where ρπmin = min{nold (ak |sk), nnew(ak |sk)} is a policy related term.
Proof. See Appendix A.3.	□
The lower and upper boundS in Soft Q-learning include a policy term with the surprise (the magni-
tude of the Soft TD error). The policy related term ρπ quantifieS the “on-policyneSS” of the experi-
enced action. And the boundS become tighter aS the difference between nold(ak |sk) and nnew(ak|sk)
becomeS Smaller. SurpriSingly, the coefficient of the entropy term β impactS the bound only through
5
Under review as a conference paper at ICLR 2021
the policy term, which makes it an excellent priority even β changes during learning (Haarnoja
et al., 2018). As 0 ≤ ρπmax ≤ 1, the value metrics are also upper bounded by surprise (TDsoft)
alone, which is similar as in Q-Iearning. However, as π(ak|sk) is usually less than 1, surprise is a
looser upper bound in soft Q-learning. This supports previous study, which empirically shows that
directly applying PER using surprise alone in soft Q-learning does not significantly improve the
sample efficiency (Wang & Ross, 2019). We will further study these relationship in experiments.
5	Experiments
Our experiments aim to answer following questions: (i) Do the theoretical bounds of value metrics
of experience hold true in practice? (ii) If the bounds hold, are they tight? (iii) Do the bounds derived
for maximum-entropy RL help to improve performance? First, we implement tabular versions of Q-
learning and soft Q-learning in Maze to verify the bounds in tabular methods. Second, by slightly
modifying the definition of the value metrics of experience, we extend our framework to function
approximation methods, which is far more powerful than the tabular methods (see Appendix A.4).
We implement DQN and soft DQN in CartPole to examine the bounds in function approximation
methods. Finally, we implement PER with the theoretical upper bound as priority for soft DQN and
evaluate its effectiveness.
Throughout the experiments, the value metrics of experience (EVB, PIV and EIV) for Q-learning
are calculated using (3), (4), and (5), and those for soft Q-learning are calculated using (6), (7),
and (8). For soft Q-learning, we do not model the actor explicitly: the policy is calculated as the
softmax of the soft Q-function (see Section 4.1). The upper bound for value metrics in Q-learning
is surprise (Theorem 3.1), while the lower and upper bounds for soft Q-learning are calculated
according to Theorem 4.1 and 4.2, which include a policy term and surprise. The experimental
details are described in Appendix A.5 and all the codes are available at: https://github.
com/RLforlife/VER.
5.1	Maze
The first set of experiments are conducted in a maze environment of a 5 × 5 square with walls. The
agent needs to reach the goal zone by moving one square in any of the four directions (north, south,
east, west) each time. We implement tabular versions of Q-learning and soft Q-learning to solve this
problem.
For each algorithm, the value metrics of experience as well as the theoretical bounds are illustrated
in Figure 2. As we can see from upper panel, all three value metrics of experience are bounded by
the surprise for Q-learning. As our theory predicts, the absolute values of EIV are either equal to the
surprise (if the action of the experience is the best action before update) or 0. For soft Q-learning,
the three value metrics of experience are bounded by the theoretical upper bound, and EVB and
EIV are bounded by the theoretical lower bound, supporting our theory (Theorem 4.1 and 4.2).
There is a large proportion of EVBs lies on the identity line, indicating the bounds are tight. The
proportion of non-zero values of experiences is higher in soft Q-learning than in Q-learning, because
all value metrics of experiences are affected by the “on-policyness” of the experienced actions. Q-
learning learns a deterministic policy that makes most actions of experiences off-policy, while soft
Q-learning learns a stochastic policy that results in less sparse values of experiences. In summary,
the experimental results in the maze environment support the theoretical bounds of value metrics of
experience in Q-learning and soft Q-learning.
5.2	CartPole
CartPole is a pendulum with a center of gravity above its pivot point. The goal is to keep the pole
balanced by moving the cart forward and backward. We implement DQN and soft DQN (DQN with
soft-update) in this environment. For DQN, we replace the Q-network in Mnih et al. (2015) with a
two-layer MLP. For soft-DQN, all the settings are the same with DQN, except for two modifications:
during policy evaluation, the (soft) Q-network is updated according to the soft TD error; during
policy improvement, the policy is updated following a maximum-entropy policy, as the softmax of
the Q values (see Section 4.1).
6
Under review as a conference paper at ICLR 2021
Figure 2: Results of Q-learning and soft Q-learning in Maze. a-c. surprise (the magnitude of TD
error) v.s. absolute value ofEVB (left), EIV (middle) and PIV (right) in Q-learning. d-f. Theoretical
upper bound and g-i. lower bound v.s. absolute value of EVB, EIV and PIV in soft Q-learning. The
red line is the identity line.
By slightly modifying the definition of value metrics of experience, we can extend our framework to
function approximation methods (see Appendix A.4). The value metrics of experience as well as the
theoretical bounds are illustrated in Figure 3. All value metrics of experience in DQN (Figure 3a-
c) and soft DQN (Figure 3d-f) are bounded by the theoretical upper bounds. For DQN, absolute
EVBs and PIVs are uniformly distributed in the bounded area, while absolute EIVs are equal to the
surprise or 0. Results are different in soft DQN, where absolute EVBs and EIVs are distributed
more closely towards the theoretical upper bounds, suggesting the upper bound in soft Q-learning
is tighter. Moreover, (Figure 3g-h) shows the EVBs and EIVs are lower bounded by Pmm * ∣ TDsoft ∣,
while PIVs are not . The experimental results confirm the bounds of value metrics in function
approximation methods.
5.3	Atari Games
In this set of experiments, we investigate whether the theoretical upper bound of value metrics of
experience, which balances the surprise and ”on-policyness” of the experience (Figure 6), can serve
as an appropriate priority for experience replay in soft Q-learning. More specifically, we compare
the performance of soft DQN with different prioritization strategies: uniform replay, prioritization
with surprise or the theoretical upper bound (Pπmax * ∣TDsoft ∣), which are denoted by soft DQN, PER
and VER (valuable experience replay) respectively. This set of experiments consists of nine games
from Atari 2600 games, whose goal is learning to play each of the games with the screen pixels
as the only input. We closely follow the experimental setting and network architecture outlined by
7
Under review as a conference paper at ICLR 2021
DQN
0.4	0.6	0.8	1.0	0.0	0.2	0.4	0.6	0.8	1.0	0.0	0.2	0.4
∣TD∣	∣TD∣	∣TD∣
Soft DQN (upper)
rho_max * ∣TD∣	rho_max * ∣TD∣	rho_max * ∣TD∣
. Soft DQN (lower)
g	h
rho_min * ∣TD∣	rho_min * ∣TD∣	rho_min * ∣TD∣
Figure 3: Results of DQN and soft DQN in CartPole. a-c. surprise (the magnitude of the TD error)
v.s. absolute value of EVB (left), EIV (middle) and PIV (right) in DQN. d-f. Theoretical upper
bound and g-i. lower bound v.s. absolute value of EVB, EIV and PIV in soft DQN. The red line is
the identity line.
Mnih et al. (2015). For each game, the network is trained on a single GPU for 40M frames, or
approximately 2 days.
Figure 4 shows that soft DQN prioritized by surprise or the theoretical upper bound significantly
outperforms uniform replay in most of the games. On average, soft DQN with PER or VER outper-
form vanilla soft DQN by 11.8% or 18.0% respectively. Moreover, VER shows higher convergence
speed and outperforms PER in most of the games (8.47% on average), which suggest that a tighter
upper bound on value metrics improves the performance of experience replay. These results suggest
that the theoretical upper bound can serve as an appropriate priority for experience replay in soft
Q-learning.
6	Discussion
In this work, we formulate a framework to study relationship between the importance of experience
and surprise (the magnitude of the TD error). To quantify the importance of experience, we derive
three value metrics of experience: expected value of backup, policy evaluation value, and policy
improvement value. For Q-learning, we theoretically show these value metrics are upper bounded
by surprise. Our claims are supported by the experiments of tabular Q-learning and DQN. Thus,
surprise implicitly tracks the value of the experience, which leads to high sample efficiency of
PER. Furthermore, we extend our framework to maximum-entropy RL, by showing that these value
8
Under review as a conference paper at ICLR 2021
Boxing
Epoch
Epoch
Spacelnvaders
Figure 4: Learning curve of soft DQN (blue lines), and soft DQN with prioritized experience replay
in term of soft TD error (PER, orange lines) and the theoretical upper bound of value metrics of
experience (VER, green lines) on nine Atari games.
Epoch
metrics are lower and upper bounded by the product of a policy term and surprise. The results in soft
Q-learning and soft DQN supports our theory. Moreover, we employ the upper bound as the priority
for experience relay, termed as VER, in soft DQN. And we empirically show that VER outperforms
PER and significantly improves the sample efficiency of soft DQN.
By linking surprise and value of experience, two important quantities in learning, our study has the
following implications. First, from a machine learning perspective, our study provide a framework to
derive appropriate priorities of experience for different algorithms, with possible extension to batch
RL (Fu et al., 2020) and sequence experience replay Brittain et al. (2019). Second, for neuroscience,
our work provides insight on how brain might encode the importance of experience. Since surprise
biologically corresponds to the reward prediction-error signal in the dopaminergic system (Schultz
et al., 1997; Glimcher, 2011) and implicitly tracks the value of the experience, the brain may account
on it to differentiate important experiences.
References
Marc Brittain, Josh Bertram, Xuxi Yang, and Peng Wei. Prioritized sequence experience replay.
arXiv preprint arXiv:1905.12726, 2019.
9
Under review as a conference paper at ICLR 2021
Brett Daley and Christopher Amato. Reconciling λ-returns with experience replay. In Advances in
Neural Information Processing Systems (NeurIPS), 2019.
Laurent El Ghaoui. Optimization models and applications. http://livebooklabs.com/
keeppies/c5a5868ce26b8125, 2018. Livebook visited Spring 2018.
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep
data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.
Paul W Glimcher. Understanding dopamine and reinforcement learning: the dopamine reward pre-
diction error hypothesis. Proceedings of the National Academy of Sciences, 108(Supplement 3):
15647-15654, 2011.
Matthias J. Gruber, Maureen Ritchey, Shao-fang Wang, Manoj K. Doss, and Charan Ranganath.
Post-learning Hippocampal Dynamics Promote Preferential Retention of Rewarding Events. Neu-
ron, 89(5):1110-1120, 2016.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In International Conference on Machine Learning (ICML), 2017.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and appli-
cations. arXiv preprint arXiv:1812.05905, 2018.
Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. In AAAI Conference on Artificial Intelligence (AAAI), 2018.
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado Van Has-
selt, and David Silver. Distributed prioritized experience replay. In International Conference on
Learning Representations (ICLR), 2018.
Steven Kapturowski, Georg Ostrovski, John Quan, Remi Munos, and Will Dabney. Recurrent ex-
perience replay in distributed reinforcement learning. In International Conference on Learning
Representations (ICLR), 2019.
Angelos KatharoPoUlos and Francois Fleuret. Not all samples are created equal: Deep learning with
importance sampling. arXiv preprint arXiv:1803.00942, 2018.
Long-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machine learning, 8(3-4):293-321, 1992.
John E Lisman and Anthony A Grace. The hippocampal-vta loop: controlling the entry of informa-
tion into long-term memory. Neuron, 46(5):703-713, 2005.
Marcelo G. Mattar and Nathaniel D. Daw. Prioritized memory access explains planning and hip-
pocampal replay. Nature Neuroscience, 21(11):1609-1617, 2018.
Colin G McNamara, Alvaro Tejero-Cantero, Stephanie Trouche, Natalia Campo-Urriza, and David
Dupret. Dopaminergic neurons promote hippocampal reactivation and spatial memory persis-
tence. Nature neuroscience, 17(12):1658-1660, 2014.
Frederic MiChon, Jyh-Jang Sun, Chae Young Kim, Davide Ciliberti, and Fabian Kloosterman. Post-
learning hippocampal replay selectively reinforces spatial memory for highly rewarded locations.
Current Biology, 29(9):1436-1444, 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, 2015.
H Freyja Olafsdottir, Daniel Bush, and Caswell Barry. The role of hippocampal replay in memory
and planning. Current Biology, 28(1):R37-R50, 2018.
10
Under review as a conference paper at ICLR 2021
Yangchen Pan, Muhammad Zaheer, Adam White, Andrew Patterson, and Martha White. Organizing
experience: a deeper look at replay mechanisms for sample-based planning in continuous state
domains. arXiv preprint arXiv:1806.04624, 2018.
Emma L Roscow, Matthew W Jones, and Nathan F Lepora. Behavioural and computational evidence
for memory consolidation biased by reward-prediction errors. bioRxiv, pp. 716290, 2019.
Beatrice Salvetti, Richard GM Morris, and Szu-Han Wang. The role of rewarding and novel events
in facilitating memory persistence in a separate spatial memory task. Learning & memory, 21(2):
61-72, 2014.
Anna C Schapiro, Elizabeth A McDevitt, Timothy T Rogers, Sara C Mednick, and Kenneth A Nor-
man. Human hippocampal replay during rest prioritizes weakly learned information and predicts
memory performance. Nature communications, 9(1):1-11, 2018.
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver. Prioritized experience replay. In
International Conference on Learning Representations (ICLR), 2016.
Matthew Schlegel, Wesley Chung, Daniel Graves, Jian Qian, and Martha White. Importance resam-
pling for off-policy prediction. In Advances in Neural Information Processing Systems (NeurIPS),
2019.
John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-
learning. arXiv preprint arXiv:1704.06440, 2017.
Wolfram Schultz, Peter Dayan, and P Read Montague. A neural substrate of prediction and reward.
Science, 275(5306):1593-1599, 1997.
Richard S Sutton. Integrated architectures for learning, planning, and reacting based on approxi-
mating dynamic programming. In Machine learning proceedings 1990, pp. 216-224. Elsevier,
1990.
Che Wang and Keith Ross. Boosting soft actor-critic: Emphasizing recent experience without for-
getting the past. arXiv preprint arXiv:1906.04009, 2019.
Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):279-292,
1992.
A Appendix
A.1 Proof of Theorem 3.1
In this section, we derive upper bounds of value metrics of experience in Q-learning. The absolute
EVB can be written as
|EVB(ek)| = | max Qnew (sk, a) - max Qold(sk, a)|
aa
≤ max |Qnew(sk, a) - Qold(sk, a)|
a
≤ α∣TD(sk,ak心,sk)|,	(9)
where the second line is from the contraction of max operator.
The absolute PIV can be written as
|PIV(ek)| = |maxQnew(sk,a) - Qnew(sk,argmaxQold(sk,a))|
aa
= max Qnew (sk , a) - Qnew (sk , arg max Qold (sk , a))
aa
= max Qnew (sk , a) - max Qold(sk , a) - 1aold=ak αTD(sk , ak , rk , sk )	(10)
aa
where the second line is from that the change in Q-function following greedy policy improvement is
greater or equal to 0, and the third line is from the update of Q-function. For TD(sk, ak, rk, s0k) ≥ 0,
we have
0 ≤ maxQnew(sk, a) - maxQold(sk, a) ≤ αTD(sk, ak,rk, s0k).
aa
11
Under review as a conference paper at ICLR 2021
And for TD(sk , ak, rk , s0k) ≤ 0, we have
max Qnew (sk , a) - max Qold (sk , a) ≤ 0.
aa
Bring above inequalities to 10, we have
∣PIV(ek)| ≤ α∣TD(sk,ak,rk,sk)∣	(11)
Similarly, the absolute EIV can be written as follows,
|EIV(ek)| = |Qnew(sk , aold) - Qold (sk, aold)|
=1s=sk,aoid=ak a|TD(Sk, ak, rk , sk )|
≤ α∣TD(sk,ak,rk, sk)|	(12)
For equations (9) and (11), the equality is reached if the experienced action is the same as the best
action before and after the update. For (12), the equality is met if the experienced action is the best
action before update.
A.2 Proof of Theorem 4.1
In this section, we derive upper bounds of value metrics of experience in soft Q-learning. For soft-Q
learning, |EVBsoft | can be written as
∣EVBsoft(ek)| = ∣β log Xexp(1 Qnew(Sk, a)) - βlog X exp(1 Qoodt(Sk, a))|
ββ
aa
=lβ log Xexp (1(Q0odt(sk, a) + 1a=akTDsoft))- βlog X exp 1 Qoodt(sk, a)|.
ββ
aa
Let US define the LOgSUmExP function F(~) = β log Ei exp (X). The LogSumExp function F(~)
is convex, and is strictly and monotonically increasing everywhere in its domain (El Ghaoui, 2018).
The partial derivative of F (~x) is a softmax function
∂F(~x)
∂xi
softmaxi (-^~) ≥ 0,
β
which takes the same form as the policy of soft Q-learning. For < 0, we have:
∂F(x1 , ..., xi , ...)
----------------- ≤ F (xi, ...,Xi + 3 ...) - F (xi, ..., Xi,...) ≤ 0.
Similarly, for ≥ 0, we have,
∂F(X1, ..., Xi + , ...)
0 ≤ F (X1, ..., Xi + €, ...) — F (X1, ..., Xi, ...) ≤ €---.
∂Xi
By substituting Xi by Qsooldft(Sk, ak) and € by TDsoft, and rewriting partial derivative of F (~X) into
policy form, we have following inequalities. For TDsoft ≤ 0
∏old(ak |sk)TDsoft ≤ β log X exp( 1 QnOw(Sk,a)) - β log X exp( 1 Qoodt(Sk,a)) ≤ 0
aa
Similarly, for TDsoft > 0, we have :
0 ≤ βlogXexp(1 QnOw(Sk,a)) -βlogXexp(1 Qoodt(Sk,a)) ≤ ∏new(ak∣Sk)TDsoft
ββ
aa
Thus, we have the upper bounds of |EVBsoft |,
∣EVBsoft(ek)∣ ≤ max{πold(ak∣Sk),∏new(ak∣Sk)} * ITDSoft∣ .
12
Under review as a conference paper at ICLR 2021
For |PIVsoft |, we have,
∣PIVs0ft(ek)∣ = | X ∏new(a∣s){QnoW(Sk,a) - β lθg(∏new(a∣s))}
a
-Enold(a∣s){QneW(Sk, a) - βlog(∏0id(a∣s))}∣
a
=Ennew(a∣s){QnoW(Sk, a) - βlog(∏new(a∣s))}
a
- Xnold(a|S){Qsooldft(Sk, a) - β log(nold(a|S))} - nold(ak|S)TDsoft
a
Q	Qno	Qnow(Sk,a)	Qooclt(sk, a)	soft
=β log)，exp-----3------β log)，exp -^ld-5---∏old(ak ∣s)TD ,
ββ
aa
where the second line is because the policy improvement value is always greater than or equal to 0,
and the third line is by reordering the equation.
For TDsoft > 0, we have:
0 ≤ β log X exp Qnewβk,a) - β log X exp Q吗k⑷-∏old(ak∣s)TDsoft ≤ ∏new(ak∣Sk)TDsoft
aa
For TDsoft ≤ 0, we have:
0 ≤ βlogXexp Qnew(S,a) - βlogXexp Q吗k, a) - ∏old(ak∣s)TDsoft ≤ ∏old(ak∣Sk)TDsoft
ββ
aa
Thus, we have the upper bounds of PIVsoft :
IPIVsoft(ek)∣ ≤ max{∏old(ak∣Sk),∏new(ak∣Sk)} * ∣TDsoft∣
Also, for |EIVsoft |, we have:
|EIVsoft(ek)| = |Xnold(a|S)[Qsnoefwt(Sk,a)-Qsooldft(Sk,a)]|
a
= nold(a|S) * ∣∣TDsoft∣∣
≤ max{nold (ak |Sk), nnew (ak |Sk)} * ∣TDsoft∣
There is no lower bound of the similar form for |PIVsoft |.
A.3 Proof of Theorem 4.2
In this section, we derive lower bounds of value metrics of experience in soft Q-learning. Similar
as deriving upper bounds in Appendix A.2, we derive the lower bounds for |EVB| using the the
LOgSUmExP function F(~) = βlog Pi exp (Xi). For e < 0,we have:
∂F(x1, ..., xi + , ...)
F (xι, ..., Xi + e,...) — F (xι,..., Xi,...) ≤ e--- ≤ 0.
∂xi
Similarly, for e ≥ 0, we have,
∂F(X1, ..., Xi, ...)
F (xι,..., Xi + e,...) — F (xι, ..., Xi, ...) ≥ e--- ≥ 0.
∂Xi
By substituting Xi by Qsooldft(Sk, ak) and e by TDsoft, and rewriting Partial derivative of F (~X) into
Policy form, we have following inequalities. For TDsoft ≤ 0
β log X exp(1 QnOw(Sk,a)) - β log X exp(1 Qoodt(sk,a)) ≤ ∏new(ak ∣Sk)TDsoft ≤ 0
ββ
aa
13
Under review as a conference paper at ICLR 2021
Similarly, for TDsoft > 0, we have :
β log X exp(1 Qnew(Sk,a)) - β log Xexp(1 Qoodt(Sk,a)) ≥ ∏oid(ak |sk)TDsoft ≥ 0
ββ
aa
Thus, we have the iower bounds of |EVBsoft |,
怛VBsoft(ek)∣ ≥ min{πoid(ak∣Sk),∏new(akS)} * ITDs叫.
For |EIVsoft|, we have:
∣EIVsoft(ek)1 = I X∏oid(a∣s)[Q∏nnft(Sk,a) - Qnofd(Sk,a)]∣
=∏oid(a∣s) * ITDsoftI
≥ min{∏oid(ak∣Sk),∏new(akS)} * ITDsoftI
A.4 Extension to function approximation method
In function approximation methods, we iearn a parameterized Q-function Q(S, a; θt). The parameter
is updated on experience ek through gradient-based method,
Θt+1 = θt + α(Qtarget(sk,ak) - Q(Sk, a，k； θt))VθtQ(sk,ak; θt)),
where α is the iearning rate and Qtarget is the target Q vaiue, defined as
Qtarget (Sk, ak) = rk + γ max Q(Sk, a ; θt).
a0
And the TD-error is defined as:
TD = Qtarget(Sk, ak) - Q(Sk, ak; θt).
As α in function approximation Q-iearning is usuaiiy very smaii, for each update, the parameterized
function moves to its target vaiue oniy by a smaii amount.
Our framework can be extended to function approximation method by siightiy modifying the defi-
nition of the vaiue metrics of experience: repiacing the Q-function after the update (Q(S, a; θt+1))
by the target Q vaiue (Qtarget(S, a)) in the vaiue metrics of experience (1-5 and 6-8). The intuition
behind this modification is simpie: the vaiue is defined by the cause of the update (target Q-vaiue),
but not the resuit of the update through gradient-based update. With this modification our theory
is appiicabie to aii function approximation methods, regardiess the specific forms of the function
approximator (iinear function or neurai networks). For Q-iearning, the vaiue metrics can be written
as:
EVB(ek) = maxQtarget(Sk, a) - maxQ(Sk, a; θt)
aa
PIV(ek ) = max Qtarget(Sk , a) - Qtarget(Sk , aoid)
a
EIV(ek ) = Qtarget(Sk , aoid) - Q(Sk , aoid; θt ).
And for soft Q-iearning, the vaiue metrics can be written as:
EVBsoft(ek) = β log X exp(1 Qsft (Sk ,a)) - β log X exp(1 Qsoft(Sk ,a； θt))
ββ
aa
soft	Qtsaorfgtet (Sk, a)	Qsoft (Sk, a; θt)	soft
PIV (ek) = β log 工 exp ——F-------β log ‰ exp------β-------∏oid(ak I S)TD
aa
EIVsoft(ek) = X∏oid(a∣S)[Qsoftet(Sk,a) - Qsoft(Sk,a； θt)].
a
After the modifications, the vaiue metrics of experience have simiiar form as the tabuiar case, and
aii Theorems derived in the tabuiar case can be appiied to function approximation methods.
14
Under review as a conference paper at ICLR 2021
a
Figure 5: Maze environment and learning curves.
A.5 Experimental Details
A.5.1 Maze
For the maze experiments in section 5.1, we use a maze environment of a 5 × 5 square with walls,
as depicted in Figure 5a. The agent needs to reach the goal zone in the bottom-right corner. At each
time step, the agent can choose to move one square in any of the four directions (north, south, east,
west). If the move is blocked by a wall or the border of the maze, the agent stays in place. Every
time step, the agent gets a reward of -0.004 or 1 if it enters the goal zone and the episode ends.
The discount factor is 0.99 throughout the experiments. For these experiments, we use a tabular
setting for Q-learning and soft Q-learning according to section 2.1 and 4.1. For Q-learning, the
behavior policy is -greedy, where decays exponentially from 1 to 0.001 during training. And we
set learning step size α = 1. For soft Q-learning, the temperature parameter β is set to 100. Total
trial number is 50 for each algorithm. During training, both algorithms successfully solve the maze
game, see Figure 5b-c for the learning curves.
A.5.2 CartPole
For CartPole, the goal is to keep the pole balanced by moving the cart forward and backward for
200 steps. We test our theoretical prediction on DQN and soft-DQN (DQN with soft-update). For
DQN, we implement the model according to Mnih et al. (2015), where we replace the original Q-
network with a two-layer MLP, with 256 Relu neurons for each layer. The in -greedy policy
decays exponentially from 1 to 0.01 for the first 10, 000 steps, and remains 0.01 afterwards. For
soft-DQN, all settings are the same with DQN, except for two modifications: for policy evaluation,
the (soft) Q-network is updated according to the soft TD error; the policy follows maximum-entropy
policy, calculated as the softmax of the soft Q values (see section 4.1). The temperature parameter
β is set to 0.5. For both algorithms, the discount factor is 0.99, the learning rate is 0.005, experience
buffer size is 1000, the batch size is 16 and total environment interaction is 50, 000.
A.5.3 Atari Games
For this set of experiments, we compare the performance of vanilla soft DQN and soft DQN with
PER, where we use surprise and the theoretical upper bound as priorities (Schaul et al., 2016),
respectively denoted as PER and VER (valuable experience replay). We select 9 Atari games for the
experiments: Alien, BattleZone, Boxing, BeamRider, DemonAttack, MsPacman, Qbert, Seaquest
and SpaceInvaders. The vanilla soft DQN is similar to that described in the above section, but the
Q-network the same with Mnih et al. (2015). We implement PER on soft-DQN according to Schaul
et al. (2016). For all algorithms, the temperature parameter β is 0.05, the discount factor is 0.99, the
learning rate is 1e-4, experience buffer size is 1M, the batch size is 32, total environment interaction
is 50, 000. For PER or VER, the parameters for importance sampling are αIS = 0.4 and βIS = 0.6.
For each game, the network is trained on a single GPU for 40M frames, or approximately 2 days.
15
Under review as a conference paper at ICLR 2021
0	12
Upper Bound
Figure 6: Illustration on the difference between VER and PER in soft Q-learning. VER uses the theo-
retical upper bound as priority (Pmax * |TDsoft |), which balances the TD error and the “on-poIicyness”
of the experience. Depicted are the theoretical upper bound (left), |TD| (middle), and the policy term
(right) of50 experiences from the replay buffer in the maze (upper panel) and CartPole (lower panel),
ordered by the theoretical upper bound.
16