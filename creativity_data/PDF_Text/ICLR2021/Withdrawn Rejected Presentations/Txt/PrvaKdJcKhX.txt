Under review as a conference paper at ICLR 2021
Differentiable	Approximations for Multi-
resource Spatial Coverage Problems
Anonymous authors
Paper under double-blind review
Ab stract
Resource allocation for coverage of physical spaces is a challenging problem
in robotic surveillance, mobile sensor networks and security domains. Recent
gradient-based optimization approaches to this problem estimate utilities of actions
by using neural networks to learn a differentiable approximation to spatial coverage
objectives. In this work, we empirically show that spatial coverage objectives with
multiple-resources are combinatorially hard to approximate for neural networks
and lead to sub-optimal policies. As our major contribution, we propose a tractable
framework to approximate a general class of spatial coverage objectives and their
gradients using a combination of Newton-Leibniz theorem, spatial discretization
and implicit boundary differentiation. We empirically demonstrate the efficacy of
our proposed framework on single and multi-agent spatial coverage problems.
1	Introduction
Allocation of multiple resources for efficient spatial coverage is an important component of many
practical single-agent and multi-agent systems, for e.g., robotic surveillance, mobile sensor networks
and security game modeling. Surveillance tasks generally involve a single agent assigning resources
e.g. drones or sensors, each of which can monitor physical areas, to various points in a target domain
such that a loss function associated with coverage of the domain is minimized (Renzaglia et al., 2012).
Alternatively, security domains follow a leader-follower game setup between two agents, where a
defender defends a set of targets (or a continuous target density in a geographical area) with limited
resources to be placed, while an attacker plans an attack after observing the defender’s placement
strategy using its own resources (Tambe, 2011).
Traditional methods used to solve single-agent multi-resource surveillance problems often rely on
potential fields (Howard et al., 2002), discretization based approaches (Kong et al., 2006), voronoi
tessellations (Dirafzoon et al., 2011) and particle swarm optimization (Nazif et al., 2010; Saska et al.,
2014). Similarly, many exact and approximate approaches have been proposed to maximize the
defender’s expected utility in two-agent multi-resource security domains against a best responding
attacker (Kiekintveld et al., 2009; Amin et al., 2016; Yang et al., 2014; Haskell et al., 2014; Johnson
et al., 2012; Huang et al., 2020). Notably, most existing traditional approaches focus on exploiting
some specific spatio-temporal or symmetry structure of the domain being examined.
Related Work: Since spatial coverage problems feature continuous action spaces, a common
technique used across most previous works is to discretize the area to be covered into grid cells and
restrict the agents’ actions to discrete sets (Kong et al., 2006; Yang et al., 2014; Haskell et al., 2014;
Gan et al., 2017) to find the equilibrium mixed strategies or optimal pure strategies using integer
linear programming. However, discretization quickly becomes intractable when the number of each
agent’s resources grows large. While some games can be characterized by succinct agent strategies
and can be solved efficiently via mathematical programming after discretizing the agents’ actions
spaces (Behnezhad et al., 2018), this is not true for most multi-resource games.
Recent works in spatial coverage domains have focused on incorporating advances from deep learning
to solve the coverage problems with more general algorithms. For instance, Pham et al. (2018) focus
on the multi-UAV coverage of a field of interest using a model-free multi-agent RL method while
StackGrad (Amin et al., 2016), OptGradFP (Kamra et al., 2018), PSRO (Lanctot et al., 2017) are
model-free fictitious play based algorithms which can be used to solve games in continuous action
spaces. However model-free approaches are sample inefficient and require many interactions with the
domain (or with a simulator) to infer expected utilities of agents’ actions. Secondly, they often rely
1
Under review as a conference paper at ICLR 2021
on the policy gradients to compute the derivative of the agents’ expected utilities w.r.t. their mixed
strategies, which induces a high variance in the estimate.
To alleviate these issues, more recent works take an actor-critic based approach (Lowe et al., 2017),
which additionally learns a differentiable approximation to the agents’ utilities (Kamra et al., 2019a;
Wang et al., 2019) and calculate gradients of strategies w.r.t. the utilities. But this requires learning
accurate reward/value functions which becomes combinatorially hard for multi-resource coverage.
Contributions: To address the above challenge, we present a framework to tractably approximate
a general class of spatial coverage objectives and their gradients via spatial discretization without
having to learn neural network based reward models. We only discretize the target domain to
represent integrals and all set operations over it, but not the action spaces of the agents. Hence we
mitigate the intractability caused by discretizing high dimensional action spaces of agents with large
number of resources, while also keeping agents’ actions amenable to gradient-based optimization. By
combining our framework with existing solution methods, we successfully solve both single-agent
and adversarial two-agent multi-resource spatial coverage problems.
2	Multi-resource spatial coverage problems
In this section, we formally introduce notation and definitions for multi-resource allocation problems
along with two example applications, which will be used for evaluation.
Multi-agent multi-resource spatial coverage: Spatial coverage problems comprise of a target space
Q ⊂ Rd (generally d ∈ {2, 3}) and a set of agents (or players) P with each agent p ∈ P having
mp resources. We will use the notation -p to denote all agents except p i.e. P \{p}. Actions: An
action up ∈ Rmp ×dp for agent p is the placement of all its resources in an appropriate coordinate
system of dimension dp . Let Up denote the compact, continuous and convex action set of agent
p. Mixed strategies: We represent a mixed strategy i.e. the probability density of agent p over its
action set Up as σp(up) ≥ 0 s.t. U σp(up)dup = 1. We denote agent p sampling an action up ∈ Up
from his mixed strategy density as Up 〜σ?. Joints: Joint actions, action sets and densities for all
agents together are represented as u = {up}p∈P, U = ×p∈P {Up} and σ = {σp}p∈P respectively.
Coverage: When placed, each resource covers (often probabilistically) some part of the target space
Q. Let cvgp : q × u → R be a function denoting the utility for agent p coming from a target point
q ∈ Q due to a joint action u for all agents. We do not assume a specific form for the coverage
utility cvgp and leave it to be defined flexibly, to allow many different coverage applications to be
amenable to our framework. Rewards: Due to the joint action u, each player achieves a coverage
reward rp : u → R of the form rp(u) = Q cvgp (q, u) impp(q) dq, where impp(q) denotes the
importance of the target point q for agent p. With a joint mixed strategy σ , player p achieves expected
utility: Eu~σ[rp] = fu rp(u)σ(u)du. Objectives: In single-agent settings, the agent would directly
optimize his expected utility w.r.t. action up. But in multi-agent settings, the expected utilities
of agents depend on other agents’ actions and hence cannot be maximized with a deterministic
resource allocation due to potential exploitation by other agents. Instead agents aim to achieve Nash
equilibrium mixed strategies σ = {σp}p∈P over their action spaces. Nash equilibria: A joint mixed
strategy σ* = {σp}p∈P is said to be a Nash equilibrium if no agent can increase its expected utility
by changing its strategy while the other agents stick to their current strategy.
Two-player settings: While our proposed framework is not restricted to the number of agents or
utility structure of the game, we will focus on single-player settings and zero-sum two-player games
in subsequent examples. An additional concept required by fictitious play in two-player settings is
that of a best response. A best response of agent p against strategy σ-p is an action which maximizes
his expected utility against σ-p:
brp(σ-p) ∈ argmaχ {Eu-p~σ-p [rp(Up, u-P)] }.
up
The expected utility of any best response of agent p is called the exploitability of agent -p:
C-p(σ-p):= max {Eu-p~σ-p Irp(Up, U-P)] }.
up
Notably, a Nash equilibrium mixed strategy for each player is also their least exploitable strategy.
Example 1 (Single-agent Areal Surveillance). A single agent, namely the defender (D), allocates
m areal drones with the ith drone Di having three-dimensional coordinates UD,i = (pD,i, hD,i) ∈
2
Under review as a conference paper at ICLR 2021
[—1, 1]2 × [0, 1] to surveil a two-dimensional forest Q ⊂ [—1, 1]2 of arbitrary shape and with a known
but arbitrary tree density ρ(q). Consequently, uD ∈ Rm×3. Each drone has a downward looking
camera with a circular lens and with a half-angle θ such that at position (pD,i, hD,i), the drone Di
sees the set of points SD,i = {q | ||q — pD,i ||2 ≤ hD,i tan θ}. A visualization of this problem with
m = 2 drones is shown for a sample forest in Figure 1a. We assume a probabilistic model of coverage
Khopt
with a point q being CoVered by drone Di with probability PH(hD,i) = eκ(hopt-hD,i') ( hDi )
if q ∈ SD,i and 0 otherwise. With multiple drones, the probability ofa point q being covered can


then be written as: cvg(q,UD) = 1 一 Hi∣q∈s□ . PH(h0,i) where PH Standsfor 1 一 PH. Hence, the
reward function to be maximized is: rD,ip(uD) = JQ(1 一 Qi∣q∈s^ . PH (ho,i)) ρ(q)dq with the
tree density ρ(q) being the importance of target point q (subscript 1p denotes one agent).
Example 2 (Two-agent Adversarial Coverage). Two agents, namely the defender D and the attacker
A, compete in a zero-sum game. The defender allocates m areal drones with the same coverage model
as in example 1. The attacker controls n lumberjacks each with ground coordinates uA,j ∈ [一1, 1]2
to chop trees in the forest Q. Consequently, uA ∈ Rn×2. Each lumberjack chops a constant fraction
κ of trees in a radius RL around its coordinates uA,j. We denote the area covered by the j-th
lumberjack as SA,j = {q | kq 一 pA,j k2 ≤ RL}. A visualization of this problem with m = n = 2
is shown for a sample forest in Figure 1b. A drone can potentially catch a lumberjack if its field of
view overlaps with the chopping area. For a given resource allocation u = (uD, uA), we define Ij =
{i | kPA,j -PD,i∣∣2 ≤ RL + h0,i tan θ} as the set ofall drones which overlap with the j -th lumberjack.
The areal overlap αij = S ∩S dq controls the probability of the j -th lumberjack being caught
by the i-th drone: PC (hD,i, αij) = PH (hD,i) PA (αij) where PH is the same as that in example 1
and captures the effect σfdroneis height on quality ofcoverage, while Pa(aj) = 1 - exp (— KRj)
captures the effect of areal overlap on probability of being caught. Hence, the reward achieved
by the j-th lumberjack can be computed as: rA,j (uD, uA,j ) = κ S ∩Q ρ(q)dq with probability
i∈Ij P (hD,i,αj), and —κ JSA ∩q ρ(q)dq otherwise i.e. the number of trees chopped if the j -th
lumberjack is not caught by any drone or an equivalent negative penalty if it is caught. Hence, the
total agent rewards are: rA,2p(UD, ua) = —r0,2p(uD,ua) = Ej rA,j(ud, UA,j) (subscript 2p
denotes two-agent).
Note that in the above examples drones provide best probabilistic coverage at a height hopt . By
increasing their height, a larger area can be covered at the cost of deterioration in coverage probability.
Further, the defender can increase coverage probability for regions with high tree density by placing
multiple drones to oversee them; in which case, the drones can potentially stay at higher altitudes
too. Example 2 further adds additional interactions due to overlaps between defender and attacker’s
resources1. Hence, these examples form a challenging set of evaluation domains with multiple
trade-offs and complex possibilities of coverage involving combinatorial interactions between the
players, resources. For both examples, We use the following constants: θ = 6, hwt = 0.2, K = 4.0,
RL = 0.1, Ka = 3.0, κ = 0.1. However, note that these values only serve as practical representative
values. The techniques that we introduce in this paper are not specific to the above probabilistic
capture models or specific values of game constants, but rather apply to a broad class of coverage
problems where the agents act by placing resources with finite coverage fields and agents’ rewards
are of the form: rp(u) = Q fp(u, q)dq.
Dealing with zero gradients: In the two-agent game, the attacker’s reward depends on the locations
of its resources, but the defender’s reward solely depends on overlaps with the attacker’s resources.
In absence of such overlap, the gradient of rD,2p w.r.t. uD,i becomes 0. Hence, we propose to use the
reward from the one-agent game as an intrinsic reward for the defender similar to how RL algorithms
employ intrinsic rewards when extrinsic rewards are sparse (Pathak et al., 2017). Then the reward
function for the defender becomes: r0,2p(uD, UA) = rD,2p(uD, UA) + μrD,ip(uD). We USe a small
μ = 0.001 to not cause significant deviation from the zero-sum structure of the game and yet provide
a non-zero gradient to guide the defender’s resources in the absence of gradients from rD,2p.
1In reality, lumberjacks might act independent of each other and lack knowledge of each others’ plans. By
allowing them to be placed via a single attacker and letting them collude, we tackle a more challenging problem
and ensure that not all of them get caught by independently going to strongly covered forest regions.
3
Under review as a conference paper at ICLR 2021
(a)	(b)
Figure 1: (a) Areal surveillance example with an arbitrary forest and m = 2 drones, (b) Adversarial
coverage example with m = 2 drones and n = 2 lumberjacks (red circles).
3 Methods
Solution approaches: The key idea for all solution approaches is to obtain a differentiable approx-
imation to the expected utility of the agents and then maximize it w.r.t. the agents’ actions (or
mixed strategies). For single-agent games, this boils down to performing direct gradient ascent on a
differentiable approximation to rD(uD), thereby converging at a (locally) optimal value of uD. For
two-agent adversarial games, DeepFP (Kamra et al., 2019b), an actor-critic based approach based on
fictitious play can be used. Briefly summarized in algorithm 1, it obtains a differentiable approxi-
mation to the reward functions r0,2p and rA,2p, creates an empty memory to store a non-parametric
representation of the agents’ mixed strategies σ = (σD, σA) and initializes best responses for both
agents randomly [lines 1-3]. Then it alternatively updates: (a) the agents’ strategies, by storing the
current best responses in the memory [line 5], and (b) the best responses, by maximizing each agent
p’s differentiable reward function against a batch of samples drawn from the other agent’s strategy
σ-p [lines 6-8]. Details of DeepFP hyperparameters used can be found in section A.6 in the appendix.
The key component required in both cases is a differentiable approximation to the reward function
and we propose a tractable framework for this challenging task in the subsequent sub-sections.
Mitigating sub-optimal local best responses: During our preliminary experiments with DeepFP,
we observed that updating best responses using purely gradient-based optimization can often get
stuck in sub-optimal local optima. While DeepFP maintains stochastic best responses to alleviate
this issue, it doesn’t eliminate it completely. We briefly describe our solution to this issue here
(please see section A.4 in the appendix for a more elaborate discussion on the issue and details of
the proposed solution). Motivated by Long et al. (2020), we propose a simple population-based
approach wherein we maintain a set of K deterministic best responses brpk(σ-p), forp ∈ {D, A} and
∀k ∈ [K]. During the best response optimization step for agent p [lines 6-8], we optimize the K best
responses independently and play the one which exploits agent -p the most. After the optimization
step, the top K best responses are retained while the bottom half are discarded and freshly initialized
with random placements for the next iteration. This allows retention and further refinement of the
current best responses over subsequent iterations, while discarding and replacing the ones stuck in
sub-optimal local minima.
3.1 Differentiable approximation for coverage objectives
First, we propose a method to approximate coverage objectives and their gradients w.r.t. agents
actions. Consider an objective of the form:
r(u)
f(u, q) dq
Q
(1)
where u denotes actions of one or more agents having multiple resources to place at their disposal
and q is any point in the target domain Q. We assume that the action u has m components with ui
representing the location of i-th resource (i ∈ [m]) and u\i representing the locations of all resources
other than i. Note that the imp(q) function has been subsumed into f (u, q) in this formulation. We
are interested in computing the gradient: 募.However, this is a hard problem since: (a) r(u) involves
integration over arbitrary (non-convex shaped) target domains which does not admit a closed-form
expression in terms of elementary functions and hence cannot be differentiated with autograd libraries
4
Under review as a conference paper at ICLR 2021
Algorithm 1: DeepFP
Result: Final strategies σ0 ,σA in mem
ι Obtain a differentiable approximation r = (td, rA) to the reward functions: (r0,2p, rA,2p);
2	Initialize best responses (brD, brA) randomly;
3	Create empty memory mem to store σ = (σD, σA);
4	for game ∈ {1,..., max_games} do
/* Update strategies	*/
5	Update σ by storing best responses {brD, brA} in mem;
/* Update best responses	*/
6	for agent p ∈ {D, A} do
7	Draw samples {ui-p}i=1:bs from σ-p in mem;
8	brp := maxUp b1s Pb= 1 rp(up, U-P); * * * * * *
like PyTorch and TensorFlow, and (b) most resources have a finite coverage area, outside of which
the coverage drops to zero. This often makes the function f(u, q) discontinuous w.r.t. q given a fixed
u especially at the coverage boundaries induced by the resources’ coordinates, for e.g., drones have a
circular probabilistic coverage area governed by their height and camera half-angle θ, outside which
the coverage probability suddenly drops to zero.
Theorem 1. Let the objective function be as shown in eq 1: r(u) = Q f (u, q) dq. Denoting the set
of points Covered by the i -th resource as Si, the interior of a Set with in (∙) and the boundary with
δ(∙), the gradient of r(u) w.r.t. the i-th resource's location Ui is given by:
dr(U) — /	df(u,q) d , [	(f(	CdqQ∩δSiT	d e`
工=L(Q∩Si) ~^uτdq+4δJf(u, q) - f(u\i, q))	nqQ∩δSidq ⑵
Proof. While function f can be potentially discontinuous in q across resources’ coverage boundaries,
r(U) integrates over q ∈ Q thereby removing the discontinuities. Hence, instead of directly taking
the derivative w.r.t. a particular resource’s location Ui inside the integral sign, we split the integral
into two parts - over the i-th resource’s coverage area Si and outside it:
r(U)
f(U,q)dq+	f(U, q) dq
Q∩Si	Q\Si
(3)
Splitting the integral at the boundary of the discontinuity allows us to explicitly capture the effect of
a small change in Ui on this boundary. Denoting the interior of a set with in(∙) and the boundary with
δ(∙), the derivative w.r.t. Ui can be expressed using the Newton-Leibniz formula as:
dr(U) — /	df (UM d , f	f(	) dqδ(Q∩Si) T	d
"∂ut=L(Q「si~^∂u~ q+「Si/，(Uqq	a如	nqδ(Q∩Si)q	⑷
l /	df(U\i，q) ,j l [	门 ∖dqδ(Q∖Si)T	J
+	—而-dq+	f(U\i，q)-ɪ— nqδ(Q∖Si) dq,
∕in(Q∖Si)	dUi	J δ(Q∖Si)	dUi
where dqδ∂Q∩Si, denotes the boundary velocity for δ(Q ∩ Si) and nqδ(Q∩s)denotes the unit-vector
normal to a point q on the boundary δ(Q ∩ Si) (similarly for δ(Q∖Si)). Since f(U\i, q) does not
depend on Ui, we can set df (u∖i,q) = 0. NeXt observe that the boundaries can be further decomposed
∂ui
as: δ(Q ∩ Si) = (δQ ∩ Si) ∪ (Q ∩ δSi) and similarly δ(Q∖Si) = (δQ∖Si) ∪ (Q ∩ δSi). However
since Ui does not change the boundary of the target domain δQ, we have:
dqδQ∩Si = 0, ∀q ∈ δQ ∩ Si	(5)
∂Ui
dqδQ∖Si = 0, ∀q ∈ δQ∖Si	(6)
∂Ui
Further on the boundary of Si , the following unit-vectors normal to the boundary are oppositely
aligned:
nqδ(Q∖Si)= -nqδ(Q∩Si) ∀q ∈ Q ∩ δSi.
(7)
5
Under review as a conference paper at ICLR 2021
Substituting the above results, we can simplify the gradient expression in eq 4 to:
dr(U) — /	df(u,q) , , /	(f(	CdqQ∩δSiT	,
丁=.「Si)~^uτdq+LδSifuq-fu'∖,q))nqQ∩δSidq ⑻
□
The first term in eq 2 corresponds to the change in f inside the coverage area of resource i due to
a small change in ui, while the second term elegantly factors-in the effects of movement or shape
change of the coverage area boundary due to changes in ui (e.g. when a drone moves or elevates in
∂q T
height). While We show the general result here, the term -Q； i nqQ∩δs- Can be simplified further
using implicit differentiation of the boundary of Si, which depends on the particular game under
consideration. We show the simplification for our example domains in section A.2 in the appendix.
3	.2 Discretization based approximations
While we now have a general form for r(u) and ∂r, both forms comprise of non closed-form
integrals over the target domain Q or its subsets. While evaluating r and Ir in practice, we adopt
a discretization based approach to approximate the integrals. Given a target domain Q ⊂ Rd with
d ∈ {2, 3}, we discretize the full Rd space into B1, . . . , Bd bins respectively in each of the d
dimensions. Approximating spatial maps: All spatial maps i.e. functions over the target domain
Q (e.g. f(u, q)), are internally represented as real tensors of dimension d with size: (B1, . . . , Bd).
Approximating sets: All geometric shapes (or sets of points) including Si for all resources (e.g., the
circular coverage areas of drones and lumberjacks) and the target domain Q itself (e.g., the irregular
shaped forest) are converted to binary tensors each of dimension d+1 with size: (B1, . . . , Bd, 3). The
final dimension of length 3 denotes interior, boundary and exterior of the geometric shape respectively,
i.e. a binary tensor T has Tb1,...,bd,0 = 1 if the bin at index (b1, . . . , bd) is inside the geometric
shape, Tb1,...,bd,1 = 1 if the bin is on the boundary of the geometric shape and Tb1,...,bd,2 = 1 if
the bin is outside the geometric shape. Approximating operators: Doing the above discretization
requires an efficient function for computing the binary tensors associated with the in(∙) and the
δ(∙) operators. This is performed by our efficient divide-and-conquer shape discretizer, which is
presented in section A.3 due to space constraints. The other set operations are approximated as
follows: (a) set intersections are performed by element-wise binary tensor products, (b) integrals
of spatial maps over geometric sets are approximated by multiplying (i.e. masking) the real tensor
corresponding to the spatial map with the binary tensor corresponding to the geometric set followed
by an across-dimension sum over the appropriate set of axes.
While our discretized bins growing exponentially with dimension d of the target domain may come off
as a limitation, our method still scales well for most real-world coverage problems since they reside on
two or three-dimensional target domains. Further, unlike previous methods which discretize the target
domain and simultaneously restrict the agents’ actions to discrete bins (Yang et al., 2014; Haskell
et al., 2014), we do not discretize the actions u of agents. Hence, we do not run into intractability
induced by discretizing high-dimensional actions of agents owning multiple resources and we keep
u amenable to gradient-based optimization. Our proposed framework acts as an autograd module
for r(u), differentiable w.r.t. input u, and provides both the forward and the backward calls (i.e.
evaluation and gradients). 4
4	Experiments
In our experiments on both our application domains, we differentiably approximate rewards using the
following variants: (a) feedforward neural networks [nn], (b) graph neural networks [gnn], and (c)
and with our differentiable coverage approximation [diff ]. For the nn and gnn baselines, we trained
neural networks, one per forest and per value of m (and n for two-agent games), to predict the reward
of the defender (and attacker in case of two-agent game). The neural networks take as input the action
UD of the defender (and UA also for two-agent game) and outputs a prediction for the reward r°,ip
(TD,2p and rA,2p for two-agent game). Please see section A.6 in appendix for network architectures
and hyperparameters. We also represent best responses with the following variants: (a) stochastic best
response nets [brnet] as originally done by DeepFP, and (b) our deterministic evolutionary population
[popK] with K being the population size. We use d = 2 dimensional forests and discretize them into
B1 = B2 = 200 bins per dimension for a total of 40K bins.
6
Under review as a conference paper at ICLR 2021
4.1	Results on Areal Surveillance domain
We maximized differentiable approximations of rD,1p using all three methods: nn, gnn and diff for
different values of m ∈ {1, 2, 4} over 5 different forest instances differing in shape and tree density.
The maximum true reward rD,1p achieved by the three methods in all cases averaged over all the
forest instances is summarized in Table 1. It is clear that diff always achieves the maximum true
reward. While the difference difference from nn and gnn is less pronounced for m = 1, as the number
of agent resources increases beyond 1, the approximation quality of nn and gnn deteriorates and the
difference becomes very significant. This is also reflected in the plots of true reward achieved vs
training iterations shown in Figure 2. Since diff is an unbiased approximator of the true reward2,
the true reward continues to increase till convergence for diff. For nn and gnn, the true reward
increases initially but eventually goes down as the defender action uD begins to overfit the biased
and potentially inaccurate approximations made by nn and gnn3. Figure 3 shows the final locations
computed for a randomly chosen forest and with m = 2 for all three methods.
Table 1: Maximum reward averaged across forest instances achieved for Areal Surveillance domain.
m = 1	m = 2	m = 4
diff (ours)^^9366.03 ± 657.18^^16091.09 ± 932.77^^25117.98 ± 1554.34
nn	9293.26 ± 646.37	14649.32 ± 1206.60	18962.87 ± 2018.54
gnn	9294.47 ± 664.28	14604.11 ± 1189.48	19353.93 ± 2701.81
Figure 2: Plots of true reward achieved over DeepFP iterations by diff, nn and gnn.
(a) Forest tree density (b) Action found via diff (c) Action found via nn (d) Action found via gnn
Figure 3: Visualizing final actions for a randomly chosen forest with m = 2.
4.2	Results on Adversarial Coverage game
We implemented different variants of DeepFP with variations of differentiable reward models in {nn,
gnn, diff } along with variations of best responses in {brnet, pop4}. We measured the exploitability
D(σD) of the defender strategy found by all methods to compare them against each other. To compute
the exploitability of the defender strategy found by any variant of DeepFP, we froze the defender
strategy σD and directly maximized EuD 〜σD [rA (uD, uA)] w.r.t. UA With ^A being approximated by
diff. This is a single-agent objective and can be directly maximized with gradient ascent. We perform
30 independent maximization runs to avoid reporting local maxima and report the best of them as
the exploitability. Note that nash equilibrium strategies are the least exploitable strategies, hence the
lower the value of D(σD) found, the closer σD is to the nash equilibrium strategy.
2The only bias in diff is the discretization bin sizes, which can be made arbitrarily small in principle.
3Please see section A.1 in the appendix for a detailed analysis of this phenomenon.
7
Under review as a conference paper at ICLR 2021
Table 2 shows the exploitability values for different variants of DeepFP. We observe that the ex-
ploitability when best responses are approximated by a population-based variant with K = 4 is
always lower than that of stochastic best response networks employed by original DeepFP. Further,
with few agent resources m = n = 1, the exploitability across diff, nn and gnn is nearly similar
but the disparity increases for larger number of agent resources and diff dominates over nn and
gnn with less exploitable defender strategies. Notably, the original DeepFP (nn + brnet) is heavily
exploitable while our proposed variant (diff + popK) is the least exploitable. In Figure 4, we show a
visualization of the points sampled from the defender and attacker’s strategies for m = n = 2 case
on the same forest from Figure 3a. The visualization confirms that diff + popK covers the dense
core of the forest with the defender’s drones so the attacking lumberjacks attack only the regions
surrounding the dense core, while nn + brnet drones often gets stuck and concentrated in a small
region thereby allowing lumberjacks to exploit the remaining dense forest. Please also see section A.5
in the appendix exploring the trade-offs in the choice of population size K.
Table 2: Exploitability of the defender averaged across forest instances.
D(σD)	m=n=1	m = n = 2	m=n=4
brnet			
diff (ours)	209.78 ± 49.94	399.95 ± 57.70	559.36 ± 164.21
nn	203.92 ± 54.67	323.00 ± 39.55	787.53 ± 194.82
gnn	204.55 ± 50.72	307.74 ± 62.67	597.23 ± 125.01
pop4 (ours)			
diff (ours)	116.41 ± 15.02	141.09 ± 13.90	141.54 ± 26.60
nn	113.61 ± 6.92	208.23 ± 22.76	339.31 ± 116.77
gnn	113.99 ± 13.74	176.25 ± 15.21	172.30 ± 34.08
(a) Strategy for diff + brnet (b) Strategy for nn + brnet (c) Strategy for gnn + brnet
(d) Strategy for diff + pop4 (e) Strategy for nn + pop4
(f) Strategy for gnn + pop4
Figure 4: Visualizing final strategies found via diff, nn and gnn with best responses of the form brnet
and pop4 on a randomly chosen forest with m = n = 2. The blue (red) dots are sampled from the
defender’s (attacker’s) strategy for the 2 drones (lumberjacks).
5 Conclusion
In this work, we show that spatial coverage objectives with multiple-resources are combinatorially
hard to approximate with neural networks. We propose to directly approximate a large class of multi-
agent multi-resource spatial coverage objectives and their gradients tractably without learning neural
network based reward models. By augmenting existing approaches with our spatial discretization
based approximation framework, we show improved performance in both single-agent and adversarial
two-agent multi-resource spatial coverage problems.
8
Under review as a conference paper at ICLR 2021
References
Kareem Amin, Satinder Singh, and Michael P Wellman. Gradient methods for stackelberg security
games. In UAI,pp. 2-11, 2016.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Soheil Behnezhad, Mahsa Derakhshan, Mohammadtaghi Hajiaghayi, and Saeed Seddighin. Spatio-
temporal games beyond one dimension. In Proceedings of the 2018 ACM Conference on Economics
and Computation, pp. 411-428, 2018.
Alireza Dirafzoon, Mohammad Bagher Menhaj, and Ahmad Afshar. Decentralized coverage control
for multi-agent systems with nonlinear dynamics. IEICE TRANSACTIONS on Information and
Systems, 94(1):3-10, 2011.
Jiarui Gan, Bo An, Yevgeniy Vorobeychik, and Brian Gauch. Security games on a plane. In AAAI,
pp. 530-536, 2017.
William Haskell, Debarun Kar, Fei Fang, Milind Tambe, Sam Cheung, and Elizabeth Denicola.
Robust protection of fisheries with compass. In IAAI, 2014.
Andrew Howard, Maja J Mataric, and Gaurav S Sukhatme. Mobile sensor network deployment
using potential fields: A distributed, scalable solution to the area coverage problem. In Distributed
Autonomous Robotic Systems 5, pp. 299-308. Springer, 2002.
Taoan Huang, Weiran Shen, David Zeng, Tianyu Gu, Rohit Singh, and Fei Fang. Green security
game with community engagement. arXiv preprint arXiv:2002.09126, 2020.
Matthew P. Johnson, Fei Fang, and Milind Tambe. Patrol strategies to maximize pristine forest area.
In AAAI, 2012.
Nitin Kamra, Umang Gupta, Fei Fang, Yan Liu, and Milind Tambe. Policy learning for continuous
space security games using neural networks. In AAAI, 2018.
Nitin Kamra, Umang Gupta, Kai Wang, Fei Fang, Yan Liu, and Milind Tambe. Deep fictitious play
for games with continuous action spaces. In Proceedings of the 18th International Conference on
Autonomous Agents and MultiAgent Systems (AAMAS), 2019a.
Nitin Kamra, Umang Gupta, Kai Wang, Fei Fang, Yan Liu, and Milind Tambe. Deepfp for finding nash
equilibrium in continuous action spaces. In Decision and Game Theory for Security (GameSec),
pp. 238-258. Springer International Publishing, 2019b.
Christopher Kiekintveld, Manish Jain, Jason Tsai, James Pita, Fernando Ordonez, and Milind Tambe.
Computing optimal randomized resource allocations for massive security games. In AAMAS, pp.
689-696, 2009.
Chan Sze Kong, New Ai Peng, and Ioannis Rekleitis. Distributed coverage with multi-robot system.
In Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA
2006., pp. 2423-2429. IEEE, 2006.
Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P6rolat,
David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 4190-4203, 2017.
Qian Long, Zihan Zhou, Abhibav Gupta, Fei Fang, Yi Wu, and Xiaolong Wang. Evolutionary popula-
tion curriculum for scaling multi-agent reinforcement learning. arXiv preprint arXiv:2003.10423,
2020.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems, pp. 6379-6390, 2017.
9
Under review as a conference paper at ICLR 2021
Ali Nasri Nazif, Alireza Davoodi, and Philippe Pasquier. Multi-agent area coverage using a single
query roadmap: A swarm intelligence approach. In Advances in practical multi-agent systems, pp.
95-112. Springer, 2010.
Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. Curiosity-driven exploration
by self-supervised prediction. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops, pp. 16-17, 2017.
Huy Xuan Pham, Hung Manh La, David Feil-Seifer, and Aria Nefian. Cooperative and distributed
reinforcement learning of drones for field coverage. arXiv preprint arXiv:1803.07250, 2018.
Alessandro Renzaglia, Lefteris Doitsidis, Agostino Martinelli, and Elias B Kosmatopoulos. Multi-
robot three-dimensional coverage of unknown areas. The International Journal of Robotics
Research, 31(6):738-752, 2012.
Martin Saska, Jan ChUdoba, Libor Preucil, Justin Thomas, Giuseppe Loianno, Adam Tresinak,
Vojtech Vonasek, and Vijay Kumar. Autonomous deployment of swarms of micro-aerial vehicles
in cooperative surveillance. In 2014 International Conference on Unmanned Aircraft Systems
(ICUAS), pp. 584-595. IEEE, 2014.
Milind Tambe. Security and Game Theory: Algorithms, Deployed Systems, Lessons Learned.
Cambridge University Press, New York, NY, 2011.
Yufei Wang, Zheyuan Ryan Shi, Lantao Yu, Yi Wu, Rohit Singh, Lucas Joppa, and Fei Fang. Deep
reinforcement learning for green security games with real-time information. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 33, pp. 1401-1408, 2019.
Rong Yang, Benjamin Ford, Milind Tambe, and Andrew Lemieux. Adaptive resource allocation for
wildlife protection against illegal poachers. In AAMAS, 2014.
10