Under review as a conference paper at ICLR 2021
Incremental Policy Gradient Estimation for
Online Control in Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
Policy gradient methods are built on the policy gradient theorem, which involves
a term representing the complete sum of rewards into the future: the return. Due
to this, one usually either waits until the end of an episode before performing
updates, or learns an estimate of What the return will be-a so-called critic. Our
emphasis is on the first approach in this work, detailing an incremental policy
gradient update which neither waits until the end of the episode, nor relies on
learning estimates of the return. We provide on-policy and off-policy variants
of our algorithm. Theoretically, we draw a connection between the traces our
methods use and the discounted future state visitation distribution. We conclude
with an experimental evaluation of our methods on both simple-to-understand and
complex domains.
1	Policy Gradient Methods
Policy gradient methods form a branch of reinforcement learning (Sutton & Barto, 2018) where
behavior is learned directly, in contrast with value-based methods which derive behavior from pre-
dicted long-term outcomes. One perceived advantage of policy gradient methods is that they extend
easily to high-dimensional action spaces, as opposed to value-based methods which tend to require
expensive search during the selection of actions. Policy gradient methods are not new (Sutton,
1984; Williams, 1992; Konda & Tsitsiklis, 2000; Kakade & Langford, 2002; Kakade, 2002; Perkins
& Pendrith, 2002; Perkins & Precup, 2003; Kober & Peters, 2009; Neumann et al., 2011) but recent
interest in this approach has exploded with the widespread use of deep neural network function ap-
proximation in reinforcement learning (Lillicrap et al.; Schulman et al., 2015; 2017; Abdolmaleki
et al., 2018; Haarnoja et al., 2018; Ahmed et al., 2019; Nachum et al., 2019; Fellows et al., 2019).
The most popular policy gradient methods (Lillicrap et al.; Schulman et al., 2015; 2017; Abdol-
maleki et al., 2018; Haarnoja et al., 2018) learn an estimate of the return-the critic, which is used to
improve behaviour-the actor; such methods are therefore known as actor-critic methods (Sutton &
Barto, 2018). The hope is that the critics enhance data-efficiency and reduce the variance of updates.
Yet, the community has devoted relatively little attention to approaches that do not learn a critic.
Indeed, one of the first policy gradient methods, REINFORCE, relies on Monte Carlo estimates of
the return (Williams, 1992). Although such an approach can suffer from high variance, we note
that critics tend to be extremely poor estimates of the return in practice (Ilyas et al., 2020). Indeed,
PPO (Schulman et al., 2017) tends to perform better when its critic is learned through a method
that is arguably closer to Monte Carlo estimation (i.e., setting λ = 0.995 in the λ-return). At the
same time, however, REINFORCE can be undesirable in environments with long time horizons as
updates may only take place at the end of environment interaction. This difficulty also precludes a
REINFORCE-type algorithm for continuing tasks. Perhaps surprisingly, despite having estimates
of the return readily available, little attention has been given to online actor-critic updates (Degris
et al., 2012). It is clear that inadequacies exist in the foundations of current policy gradient methods.
We aim to close the gap between policy gradient methods that can operate online and those that
are only suitable for episodic tasks with Monte Carlo estimates of the return. In particular, we
propose an incremental policy gradient update which does not require a critic, but may still estimate
the policy gradient for continuing tasks. The update accumulates feedback over time, and permits
updates to the policy at every time step. Our approach is similar in spirit to the backward-view of
1
Under review as a conference paper at ICLR 2021
TD(λ) (Sutton, 1988). While we do not by any means claim that our method produces state-of-the-
art results, we hope that it inspires future work in online policy gradient methods.
We highlight the following contributions of our work. 1) We derive an incremental policy gradient
update. 2) We show that the traces used in our method are connected to the future state visitation
distributions. 3) We empirically evaluate properties of our method on both simple-to-understand and
more complex environments, answering a series of precise experimental questions.
2	MDPs and the REINFORCE Algorithm
The reinforcement learning problem is often formalized with Markov Decision Processes (MDPs).
An MDP (Puterman, 2014) is a tuple (S, A, p, r, γ): S is the state space, A is the set of available
actions, p is the transition probability function, r : S × A → R is the reward function, and γ ∈ [0, 1)
is the discount factor. We will assume that both S and A are finite; the general case follows by
considering measure-theoretic arguments (for instance, Konda & Tsitsiklis (2000)). We further
assume that the reward function is bounded. A (stationary) policy is a mapping π : S → ∆A, where
∆A is the simplex over A. Usually, we parameterize π with parameters θ ∈ Rd for some d ∈ N.
The discounted total reward setting is typically considered in episodic tasks, and occasionally con-
sidered in continuing tasks. In this setting, given a starting state distribution p0 , the agent maximizes
Jγ(πθ) =. Ep0 [vγπθ (s)],	(1)
where vγθ (S) = E[Rt+ι + γRt+2 + …|St = s,∏θ] is the state-value function representing the
expected return (under πθ) from state s. The policy gradient theorem (Sutton et al., 2000) yields
Vθ JY(∏θ) = X d∏θ (s)∏θ(a | s)q∏θ (s,a)Vθ log ∏θ(a | s),	⑵
s,a
where dγπθ (s) =. Pt∞=0 γt Pr(St = s | p0, πθ) is the (unnormalized, discounted) future state visi-
tation distribution, and qjθ (s, a) = E[Rt+ι + γRt+2 + ∙∙∙∣St = s, At = a] is the action-value
function- the expected return conditioned on taking action a in state s, and following πθ thereafter.
The REINFORCE (Williams, 1992) algorithm sampling one or several trajectories from an environ-
ment according to the policy πθ, after which Monte Carlo estimates are formed of Equation 2 to be
used with stochastic gradient ascent. A limitation of REINFORCE lies in having to wait until the
agent has completed a (possibly lengthy) trajectory before an update to the policy can be performed.
3	Incremental Policy Gradient
In this section, we derive our incremental policy gradient update. All proofs are in Appendix A.2.
We consider the off-policy setting, where the behavior policy may differ from the one being learned,
as it generalizes the on-policy setting. Let μ be a behaviour policy. If s > t, ps：t = 1, otherwise
∏θ(Ai | Si)
μ(Ai | SiY
To understand where our algorithm comes from, it will be helpful to write Equation 2 in an equivalent
form. For completeness, we provide the full calculation in Appendix A.1.
∞t
VθJγ (πθ) = Ep0,πθ XγtRt+1 XVθlogπθ(Aj | Sj) .	(3)
t=0	j=0
Let us first add importance-sampling corrections to Equation 3.
VθJY(∏θ) = Epo,μ XYtρo-tRt+ι ( XVθlog∏θ(Aj | Sj)∖ .	(4)
t=0	j=0
t
Ps：t = Y
2
Under review as a conference paper at ICLR 2021
Grouping non-reward terms, this expression lets us define the following eligibility trace recursively:
eo = po:ovθ log∏θ(Ao | So),
∀j > 0, ej = YPjjej-ι + po：j Yj jV θ log π (Aj | Sj).	(5)
Hence, we can rewrite the policy gradient as
∞
VθJγ(∏θ) = Epo,μ ERt+ιet	(6)
t=o
This leads to an algorithm which updates πθ at each time step t with Rt+1et. While changing πθ
technically invalidates Equation 6, it is reasonable to suspect that small learning rates result in small
deviations from the policy gradient. This compromise may not be ideal, but We note that TD(λ)'s
incremental implementation is similarly justified. In Section 5, we characterize the error that such
policy updates introduce, and detail our algorithm in Algorithm 1, Which We denote the Incremental
Policy Gradient (IPG) algorithm.
One can observe that in a tabular setting, for identical trajectories, this algorithm eventually per-
forms the same update that REINFORCE Would have done. Such scenarios occur if states are never
revisited in a trajectory, or if the policy changes sloW enough that the same sequence of actions are
sampled. But What does IPG do in cases Where it’s not equivalent? Because the same update is per-
formed regardless of episode termination, the algorithm can be seen as performing a fixed-horizon
policy gradient update to preceding states (as if the return ended there), and adds later terms to the
gradient as they become available. That is, ifan action leads to an immediate poor outcome, it read-
ily reduces the odds of the state-action pair, but if it eventually leads to a long-term good outcome,
the odds are increased accordingly through the eligibility trace. This emphasizes the algorithm’s
online adaptation, and hoW deviations from REINFORCE may occur When states are revisited.
Of note, despite the policy updates invalidating Equation 6, each reWard is under the current time
step’s policy. Coupled With a reasonably sloWly changing policy, the return for the current state-
action pair Will at least have correct, on-policy near-term information. With the later potentially off-
policy information being attenuated by discounting. This results in multiple policy changes Where
the highest-Weighted reWards of the return (from each state-action pair) are correct. In contrast, With
REINFORCE, all of the information across an episode is batched to perform one correct change to
the policy. Should one Wish to perform more REINFORCE updates With the same information, none
of the reWards are sampled under the current policy.
Due to sometimes better empirical performance, many policy gradient methods omit the Yt term
in Equation 3, resulting in a semi-gradient update (Nota & Thomas, 2019). The corresponding
semi-gradient IPG update simply drops the Yj term in the eligibility trace update in Equation 5.
Algorithm 1 Incremental Policy Gradient (IPG)
Given: policy ∏ (parameters θ); trace e; behaviour policy μ (optional; could be equal to ∏θ);
learning rate α.
Draw starting state: So 〜po.
Initialize trace: e J 0,ρ J 1.
for t = 0, 1, . . . do
Draw action: At 〜 μ(∙ | St)
Observe transition (St, At, St+1, Rt+1).
Accumulate the importance-sampling ratio P J P 彳金第.
Update trace: e J Yn/(，陶)e + PYtV log ∏θ(At | St).	. Omit Yt for semi-gradient
Update policy: θ J θ + αRt+1 e.
end for
4 Interpretation of the Traces
In this section, we draw a connection between the eligibility traces proposed above and the stationary
distribution under the target policy.
3
Under review as a conference paper at ICLR 2021
We will need the following regularity assumption on the gradients of πθ .
Assumption 1. There is some C ∈ R such that for all (s, a) ∈ S ×A, ∣Nθlog ∏θ (a | s)k∞ ≤ C,
where ∣∣ ∙ ∣∣∞ is the infinity norm.
First, we provide a connection between dγπθ and a particular sum of traces.
Proposition 1 (Expected Sum of Traces, Discounted Setting). Under Assumption 1, the following
holds for every state s.
∞
Vθd∏θ (S) = Y X Pr(St = S | Po,∏θ)E[et-i | St = s]	⑺
t=0
That the traces are related to the gradient of dγπθ is not as surprising as it may appear. Indeed,
since the policy gradient expression in Equation 6 only involves multiplication by a reward term and
a summing of the resulting eligibility traces, the gradient of dγπθ must somehow be related to the
traces. It is also interesting that every term of Pr(St | p0, πθ) is multiplied by E[et-1 | St = S],
rather than by E[et | St = S]. One intuition is that Pr(St | p0, πθ) does not take into account the
next action At, the log probability of which is included in et but not in et-1.
Unfortunately, Proposition 1 does not provide a simple way to calculate Vθdγπθ (S). To use Proposi-
tion 1 in this way would require sampling et-1 for every state encountered along a trajectory, having
access to Pr(St = S | p0, πθ), and summing the resulting series.
5 Error Analysis
Here, we analyse the errors that policy changes introduce into the trace-based estimate of the policy
gradient. We will assume linear function approximation: in particular, there exist features x(S, a) ∈
Rd for each state-action pair such that policies are Boltzmann distributions over linear preferences:
∏θ(a | S) H exp(θ>x(s, a)),
for θ ∈ Rd. In the following, ∣∙ ∣∣ will denote any norm, and ∣∣∙∣ι will denote the 'ι-norm, and | ∙ |
will denote the absolute value function.
Proposition 2. Assume we have a trajectory (so, a0, ri, ∙∙∙ , St, at, rt+ι). Let θ, θj ∈ Rd be arbi-
trary, for 0 ≤ j ≤ t. Then,
tt
γtrt+1	Vθ log πθj (aj | Sj) - γtrt+1	Vθ logπθ(aj | Sj)
j=0	j=0
t
≤ Yt|rt+i|£kn(. | Sj) - πj(∙ | Sj)k1 max ∣x(Sj, a)∣,
a
j=0
≤ Yt|rt+i|(t + 1)max ( kπ(∙ | Sj) - πj(∙ | Sj)ki max kx(Sj,a)k
ja
(8)
(9)
(10)
The interpretation of Proposition 2 is as follows. We imagine that the {θj }tj=0 are a sequence of
parameters obtained through successive policy updates. We set θ =. θt and ask, what would my
update at time t have been if I had followed the same trajectory, but had instead started with the
policy πθ and refrained from performing updates? Proposition 2 tells us that the difference between
these two updates (Equation 8) decreases with the horizon length (γt(t + 1)) and is smaller if the
policy probabilities of ∏ and ∏j are close to each other (∣∏(∙ | Sj) 一 ∏j (∙ | Sj )∣ι). Note that because
this result focuses on a particular trajectory, the inequality depends only on the policy probabilities
on the actual trajectory, and not on parts of the state space the agent has not yet seen.
A limitation of Proposition 2 is its dependence on the maximum norm of the features,
maxa ∣x(S, a)∣. Although one might expect bounded features in some scenarios (e.g., a robotic
arm in a factory), it is plausible that in others the features are either unbounded, or the maximum
norm is quite large, rendering the bound uninformative. Another limitation is its assumption of linear
function approximation; however informative, there is dependence on the policy parameterization
given that Proposition 2 involves the gradient of the log probabilities.
4
Under review as a conference paper at ICLR 2021
6	Related Work
The most relevant work to ours is the AC(λ) algorithm (Degris et al., 2012), an existing incremental
policy gradient update with eligibility traces. In contrast with our work, AC(λ) uses on an online
TD(λ) critic to both provide complete estimates of the return to bootstrap off of, and be used as a
state-value baseline. The algorithm’s derivation assumes the use of a state-value baseline, that even
the Monte Carlo extreme (λ = 1) results in a fundamentally different incremental update from this
work. We empirically explore AC(λ)'s inclusion of a critic in Section 7. Beyond AC(λ), much work
on online, incremental learning algorithms with eligibility traces are in the value-based approaches
for reinforcement learning (Sutton, 1988; Seijen & Sutton, 2014), while the aforementioned recent
policy gradient methods generally process data from completed episodes in batches.
7	Empirical Evaluation
For our empirical evaluation, we focus first on precise experimental questions in small-scale do-
mains, before understanding the properties of our method in large-scale control environments. We
address only the on-policy setting to facilitate a deeper analysis of our algorithms.
7.1	Environments
AdaptChain is a 50-state 1D episodic grid world. An agent starts on the left, receives a reward of
-1 for moving left, and 0 for moving right. Moving off the left-most state keeps the agent in place,
while moving off the right-most state terminates with a reward of 10.
SwitchStay is a deterministic 2-state continuing MDP where in each state, an agent has the option
to stay in its current state, or switch to the other. In one state, staying and switching receives rewards
of 1 and -1 respectively, and in the other state, the same actions receive rewards of 2 and 0.
We further use OpenAI Gym’s CartPole-v0 and LunarLander-v2 environments (Brockman et al.,
2016). They are physics-based problems requiring the use of function approximation.
7.2	Trace Accuracy over Time
In this portion, we evaluate the extent to which our gradient expression in Equation 6 for the dis-
counted setting is invalidated as πθ changes. We are interested in the following questions.
1)	For a given policy πθ at time t with trace et , how close is et to what the trace would have been
at time t, if our initial policy had been πθ, ifwe had obtained the same trajectory, and ifwe had not
updated πθ at all?
We use the cosine similarity to measure closeness. If the cosine similarity between our current
trace et and the counterfactual trace is close to 1, that result would indicate that the policy gradient
expression in Equation 6 remained approximately valid, even for a changing πθ . We note that
although we could have added importance sampling to correct the trajectories, doing so would not
have changed the values of the cosine similarities.
We initialize 100 random policies (i.e., 100 different agents) and apply updates according to Algo-
rithm 1. At every time step t, for each agent, we compute the cosine similarity between the current
trace et and Yt Pj=0 Vθ log ∏θ0 (aj | Sj), where ∏θ0 is the initial policy of the given agent and
(s0, a。，…，st, at) is the current trajectory of that agent.
In Figure 4, we show a representative sample of our cosine similarity plots (additional figures for
other hidden layer sizes may be found in Appendix A.3). Before proceeding with the analysis, let
us first note that some of the lines cut off before 100 time steps; this cut-off happened because of
the inherent episode termination in the environment itself. For example, environment interaction in
CartPole ends when the pole falls over or the cart exceeds the horizontal boundaries. As different
policies lead to a different number of time steps before termination, when calculating the mean
cosine similarity we ignore the contribution of those agents that have terminated.
As expected, the cosine similarity tends to decrease over time as πθ is updated. The rate of decrease
is higher for larger learning rates, consistent with the intuition that large changes in πθ should in-
5
Under review as a conference paper at ICLR 2021
.08 6 4 20
LSS 6 6
səuejl 卬£Jo Al-Je=UJ-S φ⊂~ωou
Q,8S,42 Q 2/£
Lo.o.o,o.o.o.o.o.
- - -
ωφu2π ① Iμ JO Aaμe=∈ωφ⊂~ωou
(a)	(b)
Figure 1: Each line is the mean over 100 random policies; the standard errors are smaller than the
width of the lines. At every time step, the policy is updated as in Algorithm 1. Each value on the
plot is the cosine similarity of the trace at that time, compared to what the trace would have been if
the initial policy had been what the current policy is now, and if the exact same trajectory had been
followed-the so-called CounterfaCtual trace. The CounterfaCtual trace is calculated With Equation 5
using the current policy and the stored trajectory up until that point.
Figure 2: Each line is the mean over 100 random policies; the standard errors are smaller than the
width of the lines. At every time step, the policy is updated as in Algorithm 1. Each point on a solid
line is the cosine similarity of Pj=0 Rt+1 et with Vθ JY (πθt). The dotted lines represent the cosine
similarity of £二0 YtRt+ι Pj=0 Vθ∏θ°(aj | Sj) (REINFORCE) and Vθ JY(n&).
validate the trace expression to a greater degree. Moreover, for the lower range of learning rates
(≤ 0.0001), the cosine similarity seems to remain close to 1 for all 100 time steps.
For the settings where the cosine similarity did decrease, this decrease was not monotonic, and
indeed the cosine similarity sometimes increased. This result is consistent with Proposition 2: what
is important in assessing the estimation error is not necessarily the number of updates applied, but
the actual policy probabilities. It is possible that applying more updates to a policy could move it
closer in policy probabilities to the initial policy, although it may be further in terms of parameter
space. Lastly, we note that the spike in Figure 1b at 60 steps for the learning rate of 0.001 is an
artifact of the fact that at 60 time steps, all runs had already terminated except for one whose cosine
similarity consistently remained close to 0.8.
2)	For a given policy πθ at time t with trace et , how close is the IPG update to the exact policy gra-
dient? As this question requires calculation of the exact policy gradient, we limit our consideration
of this question to tabular environments.
6
Under review as a conference paper at ICLR 2021
As in the previous experiment, we initialize 100 random policies. This time, for each time step, we
measure the cosine similarity between Vθ JY (∏θj and Pt=0 Ri+ιej. At the end of 100 time steps,
we additionally compute what REINFORCE would have used for its end-of-episode update on this
trajectory (if the initial policy had not been modified), and compare it with the exact policy gradient.
In Figure 2, we show some representative plots for the respective environments, with additional
plots in Appendix A.3. First, cosine similarities tended to be quite poor in general, with the mean
cosine similarity being quite far from one. Yet, in some cases the cosine similarity was no worse,
and sometimes better, than the cosine similarity of REINFORCE, suggesting that using Equation
6 with a changing policy is not unreasonable. In SwitchStay, the mean consine similarity actually
tended to increase over time, which is consistent with our prior observation that the important factor
is not necessarily the number of updates, but the difference in poicy probabilities. For REINFORCE,
we emphasize that we are not computing Monte Carlo estimates of the policy gradient for a single
policy; ifwe were, we would expect that the cosine similarities of REINFORCE would be close to 1.
Our focus on the single-sample behaviour of REINFORCE is meant to capture how REINFORCE
might perform in practicewhere it is common not to have many trajectories of the same policy
available for policy gradient estimation.
Finally, there did not seem to be a consistent relationship between the learning rate and the cosine
similarity. Indeed, a higher learning rate seemed detrimental to cosine similarity in AdaptChain,
but was beneficial in SwitchStay. This result is strange since for the environment settings, a smaller
learning rate implied greater fidelity of the trace to either a true, counterfactual trace or to the policy
gradient.
7.3	Control
We conclude our experimental evaluation with a comparison between IPG and REINFORCE
(Williams, 1992) in two control problems. As typically done in empirical work on policy gradi-
ent methods, we consider the semi-gradient variants of each algorithm (Nota & Thomas, 2019).
1)	Our first comparison is in the tabular AdaptChain environment. In particular, this environment
emphasizes a scenario where IPG will perform a different update from REINFORCE through ex-
cessive state revisitations. We expect that through the interim fixed-horizon policy gradient updates,
IPG will be able to adapt mid-episode to the negative feedback for moving left, and tend to go right
upon revisiting previous states. We swept over various step sizes, and Figure 3a shows the number
of episodes completed over 25,000 steps, averaged across 1,000 independent runs.
Consistent with our hypotheses, IPG performed very different updates from REINFORCE, and con-
sistently outperformed REINFORCE because of being able to adapt online. IPG seems to support
larger learning rates, and we see the curves converge as learning rates decrease, in line with IPG
matching REINFORCE should the policy change slowly.
2)	Next, we perform an ablation study in CartPole-v0, where the policy is parameterized by a fully-
connected neural network with 2 hidden layers. Our emphasis is on whether our observations from
the tabular setting remain when function approximation is used. The use of state features can be
viewed as exacerbating the excessive state revisitation scenario, as relatively different states may
still have features in common. We further compare IPG with the AC(λ) algorithm (Degris et al.,
2012) to see the impact of an online-learned state-value baseline. We fix λ = 1 for AC(λ) so that
each algorithm aims to use Monte Carlo estimates of the return. We performed 30 independent
runs of 100,000 steps for each parameter setting, and at each step, we recorded the mean return
over the last 10 episodes. Figure 3b shows a representative comparison between IPG and REIN-
FORCE in terms of the area under each parameter setting’s learning curve (AUC). Figure 3c shows
a similar comparison between IPG and AC(1), and Figure 3d shows the learning curves under each
algorithm’s parameter setting with the largest AUC. Complete results can be found in Appendix A.3.
IPG and REINFORCE seemingly follow the same trends observed in AdaptChain. With AC(1), a
large value learning rate can greatly hinder learning, highlighting potential issues with an inaccurate
critic. With smaller value learning rates, AC(1) approaches but does not outperform IPG. We found
that AC(1) with small learning rates still had rather inaccurate values. We suspect that by performing
small updates, the value estimates remained small, resulting in an update similar to IPG.
7
Under review as a conference paper at ICLR 2021
(a) Episodes completed after 25,000 steps in the
AdaptChain environment. Results are averaged over
1,000 runs, and shaded regions represent one stan-
dard error. Note the log-scale of the x-axis.
(b) AUCs over 100,000 steps in the CartPole-v0 en-
vironment. Results are averaged over 30 runs, and
shaded regions represent one standard error. Note the
log-scale of the x-axis.
IPG vs R曰NFORCE on CartPOIe-VO (Hidden Size = 64)
on CartPole-vO (Hidden Size
Ooooooooo
1816141208642
①己 nɔ 6U'≡J(U①IjO 0⊃< P ① Z=PE」ON
Learning Curves on CartPole-vO
S①POMd①
5 0 5 0 5
12107 5 2
OI 4se-」① >0 ILlm①」u(ŋ① W
Ten Thousands of Frames
(d) Learning curves under each algorithm’s best pa-
rameters in terms of AUC. Results are averaged over
30 runs, and shaded regions represent one standard
error.
10-5	10-4	IO-3
α
(c) AUCs over 100,000 steps in the CartPole-v0 en-
vironment. Results are averaged over 30 runs, and
shaded regions represent one standard error. Note the
log-scale of the x-axis.
8 Conclusion
We introduce an incremental policy gradient method that 1) does not require a critic and 2) can
perform policy updates at each time step. This method is based upon an alternative formulation
of the policy gradient, from which we extract eligibility traces that may be updated incrementally.
Theoretically, we 1) draw a connection between the gradient of the discounted future state visitation
distribution and a particular sum of these traces and 2) analyze the policy-gradient estimation error
our method introduces through policy updates. Empirically, we 1) characterize the extent to which
this estimation error occurs and 2) investigate the practical performance of IPG.
There is much promising future work: 1) It would be interesting to adapt IPG to produce online
analogues of state-of-the-art methods that use a critic, such as PPO, to understand further when/if a
critic should be used, as our results provide an example where there was no clear benefit in using an
online-learned state-value baseline. 2) Our implementation of IPG through an eligibility trace seems
analagous to an online learning procedure, while recent work (Shani et al., 2019) has connected
policy gradient and online learning methods. Further work in this direction might unearth new
algorithms that can take advantage of recent advances in online learning, such as parameter-free
algorithms (Cutkosky & Orabona, 2018). 3) Our empirical evaluation focused on the on-policy case
to get clearer answers regarding properties of IPG. However, it would be fruitful to assess how it
fares in the online, off-policy setting.
8
Under review as a conference paper at ICLR 2021
References
Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Mar-
tin Riedmiller. Maximum a posteriori policy optimisation. In International Conference on Learn-
ing Representations, 2018. URL https://openreview.net/forum?id=S1ANxQW0b.
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding
the impact of entropy on policy optimization. In Kamalika Chaudhuri and Ruslan Salakhutdinov
(eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of
Proceedings ofMachine Learning Research, pp.151-160, Long Beach, California, USA, 2019.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.
Ashok Cutkosky and Francesco Orabona. Black-box reductions for parameter-free online learning
in banach spaces. arXiv preprint arXiv:1802.06293, 2018.
Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic. CoRR, abs/1205.4839,
2012. URL http://arxiv.org/abs/1205.4839.
Matthew Fellows, Anuj Mahajan, Tim GJ Rudner, and Shimon Whiteson. Virel: A variational
inference framework for reinforcement learning. In Advances in Neural Information Processing
Systems, pp. 7120-7134, 2019.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-
policy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint
arXiv:1801.01290, 2018.
Andrew Ilyas, Logan Engstrom, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry
Rudolph, and Aleksander Madry. A closer look at deep policy gradients. In International Con-
ference on Learning Representations, 2020.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
ICML, volume 2, pp. 267-274, 2002.
Sham M. Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Jens Kober and Jan R Peters. Policy search for motor primitives in robotics. In Advances in neural
information processing systems, pp. 849-856, 2009.
Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms. In Advances in neural information
processing systems, pp. 1008-1014, 2000.
Timothy P Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:
Policy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Gerhard Neumann et al. Variational inference for policy search in changing situations. In Pro-
ceedings of the 28th International Conference on Machine Learning, ICML 2011, pp. 817-824,
2011.
Chris Nota and Philip S. Thomas. Is the policy gradient a gradient? 2019.
Theodore J. Perkins and Mark D. Pendrith. On the existence of fixed points for q-learning and sarsa
in partially observable domains. In Proceedings of the Nineteenth International Conference on
Machine Learning, ICML ’02, pp. 490-497, San Francisco, CA, USA, 2002. Morgan Kaufmann
Publishers Inc. ISBN 1558608737.
Theodore J Perkins and Doina Precup. A convergent form of approximate policy iteration. In
Advances in neural information processing systems, pp. 1627-1634, 2003.
9
Under review as a conference paper at ICLR 2021
Martin L. Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 2014.
Walter Rudin et al. Principles of mathematical analysis, volume 3. McGraw-hill New York, 1964.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis Bach and David Blei (eds.), Proceedings of the 32nd International
Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp.
1889-1897, Lille, France, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Harm Seijen and Rich Sutton. True online td(lambda). volume 32 of Proceedings of Ma-
chine Learning Research, pp. 692-700, Bejing, China, 22-24 Jun 2014. PMLR. URL http:
//proceedings.mlr.press/v32/seijen14.html.
Lior Shani, Yonathan Efroni, and Shie Mannor. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized mdps. arXiv preprint arXiv:1909.02769, 2019.
R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):
9-44, 1988.
Richard S. Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, 1984.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT press,
2018.
Richard S. Sutton, David A. McAllester, Satinder P. Singh, and Yishay Mansour. Policy gradient
methods for reinforcement learning with function approximation. In Advances in Neural Infor-
mation Processing Systems, pp. 1057-1063, 2000.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
A Appendix
A. 1 Alternative Formulation of the Discounted Policy Gradient
Our goal in this section is to show the following.
X d∏θ(s)∏θ(a | s)q∏θ(s,a)Vθ log∏(a | S)
s,a
∞t
= Ep0,πθ XγtRt+1XVθlogπθ(Aj |Sj) .	(11)
t=0	j=0
10
Under review as a conference paper at ICLR 2021
First, since the partial sums of dγπθ actually converge absolutely to dγπθ, we can use Mertens’s theo-
rem on Cauchy products to write
X∞∞
γt Pr(St = s | p0,πθ) Eπθ XγtRt+1 | S0 = s, A0 = a
t=0
X∞∞
γt Pr(St = s | p0,πθ) XEπθ γtRt+1 | S0 = s, A0 = a
t=0
. boundedness of the reward and Fubini’s theorem
∞t
=	Yt Pr(Sj = S | PO, ∏θ)E∏θ [Rt-j+1 | So = s,Ao = a]
. Mertens’s theorem
∞t
=XXγt Pr(Sj = s | p0, πθ)Eπθ [Rt+1 | Sj = s, Aj = a]
. using the Markov property to reindex the reward term
Note that Pr(Sj = s | p0, πθ)πθ(a | s) = Pr(Sj = s, Aj = a | p0, πθ). Hence, from substituting
into the policy gradient term,
∞t
Vθ Jγ (∏θ )= ΣΣΣYt Pr(Sj = S | Po, ∏θ)∏θ(a | s)E∏θ [Rt+ι | Sj = s,Aj = a] ▽& log ∏θ(a | S)
s,a t=0 j=0
∞t
=	Yt	Pr(Sj	= s, Aj = a | Po,∏θ)E∏θ	[Rt+ι	|	Sj	= s,Aj	= a]	Nθ log∏θ(a	| S)
s,a t=o j=o
∞t
= Yt	Pr(Sj	= S, Aj = a | Po , πθ )Eπθ	[Rt+1	|	Sj	= S, Aj	= a]	Nθ log πθ (a	| S)
∞t
= XYt X ESj,Aj [Eπθ [Rt+1 | Sj,Aj] Nθ log πθ (Aj | Sj)]
∞t
=XYtXEp0,πθ[Nθlogπθ(Aj | Sj)Rt+1]
∞t
= Ep0,πθ XYtRt+1XNθlogπθ(Aj | Sj) ,
t=o	j=o
where the final expectation is over the trajectory distribution under the Markov chain induced by πθ
and Po .
A.2 Proofs
The following lemma will be useful.
Lemma 1. Under Assumption 1, we have that the following quantities on both sides exist, and that
equality holds.
∞∞
X VθYt Pr(St = S | Po, ∏θ) = Vθ X Yt Pr(St = S | Po, ∏θ).	(12)
t=o	t=o
Proof. Our general approach is to use uniform convergence and the Weierstrass M-test to allow the
exchange of the gradient and the series.
The partials sums Sn =. Ptn=o Yt Pr(St = S | Po, πθ) are differentiable as πθ is differentiable, and
each Pr(St = S | Po, πθ) is simply a product of terms that do not involve θ (i.e., the transition
11
Under review as a conference paper at ICLR 2021
distribution) and a product of terms with πθ . Moreover, Sn converges (component-wise) to the
unnormalized future state visitation distribution. And therefore the r.h.s exists.
Now, the question is whether Vθ Sn converges to anything (again, component-wise), that is, if the left
hand side exists. In fact, we will show that the Sn converge uniformly by applying the Weierstrass
M-test to the individual terms ft =. Vθγt Pr(St = s | p0, πθ). Recall that for the Weierstrass M-test,
We must find Mt ∈ R such that for any θ, kftk ≤ Mt, where k ∙ ∣∣ is any Banach space norm (e.g.,
the Euclidean norm), and show that Pn Mt ≤ ∞. This will be enough for uniform convergence.
convergence of VθSn .
Let ft =. Vθγt Pr(St = s | p0,πθ). We have
kftk = I∣VθYt Pr(St = s∣Po,∏θ)k,
t-1
=Y ):	Pr(SO = so,Aι = aι, ∙∙∙ ,St = S)〉:Vθ logπθ(aj | Sj) ,
si ,ai ,0≤i≤t-1	j=0
t-1
≤ Yt	E	Pr(So	= S0,Aι	= aι,…，St	=	S)E	I∣Vθ log ∏θ (aj | Sj )∣∣,
si ,ai ,0≤i≤t-1	j=0
≤ Yt	X	Pr(So	= so, Ai	= aι,…，St	=	S)Ct,
si ,ai ,0≤i≤t-1
≤ YtCt.
Now, note that
∞
CXYtt
t=0
CY
(1 - Y)2
< ∞.
(13)
Hence, the partial sums Sn = Ptn=0 VθYt Pr(St = S | p0， πθ) converge uniformly. The uniform
convergence of these partial sums implies that both quantities in Equation 12, and in particular that
We may interchange the series and the gradient (see (Rudin et al., 1964) for example).	□
Proposition 1 (Expected Sum of Traces, Discounted Setting). Under Assumption 1, the following
holds for every state S.
∞
Vθ dγπθ (S) = Y X Pr(St = S | p0， πθ)E[et-1 | St = S]	(7)
t=0
Proof. For notational compactness, let us define p(S0 | S-1， a-1) =. p0(S0). We can write
Pr(St = S | p0， πθ)
t-1
p(S | St-1， at-1)	πθ(aj | Sj)p(Sj | Sj-1， aj-1).
si,ai; 0≤i≤t-1	j=0
Hence, using the log-likelihood trick,
Vθ Pr(St = S | p0， πθ)
t-1	t-1
=	p(S	|	St-1，	at-1)	πθ(aj	|	Sj)p(Sj	|	Sj-1，	aj-1)	Vθ log πθ (aj	|	Sj)
si,ai; 0≤i≤t-1	j=0	j=0
Note that for a trajectory (so, ao,…Sj, aj) we defined our traces in the discounted setting as
j
ej =. Yj	Vθ log πθ (aj | Sj).
i=o
We define e-1 =. 1 as an edge case.
12
Under review as a conference paper at ICLR 2021
Hence, we can write Vθ Pr(St = S | Po ,∏θ) as a kind of marginalisation of et-ι. Strictly speaking,
as Pr(St | po, ∏o) is not equal to 1 in general, We cannot write V Pr(St = S | Po, ∏θ) as an expec-
tation. However, we can introduce a normalization term to write the expression more compactly.
Vθ Pr(St = S | Po, πθ)
Pr(St = S | Po,∏θ)
Pr(St = s | Po,∏θ)
P(S | St-1, at-1)	πθ(aj | Sj)P(Sj | Sj-1, aj-1) γ1-tet-1
si,ai; o≤i≤t-1	j=o
Pr(St = S
P(S | St-1, at-1) Qtj-=1oπθ(aj | Sj)P(Sj | Sj-1, aj-1)
| Po,πθ)	--------------------------------1------ʌ------------------
Pr(St = S | Po, πθ)
si,ai∙; 0≤i≤t-1 κ
——	-	{Z
(a)
Pr(St
γ1-tet-1.
}
Let us consider what (a) represents. The numerator is simply the joint probability of a trajectory
(so, ao, ∙∙∙ , st-1,at-1, s). The denominator is the probability that such a trajectory ends in s.
Hence, we can interpret (a) as the conditional probability of a trajectory (so,ao,… ,st-ι, at-ι)
given that such a trajectory will end in s. Hence, we will write
Vθ Pr(St = s | Po, πθ) = Pr(St = s | Po, πθ)γ1-tE[et-1 | St = s]
Putting it all together,
∞∞
Vθ Xγt Pr(St = s | Po, π) = X γtVθ Pr(St = s | Po, πθ),
t=o	t=o
∞
= γX Pr(St = s | Po, πθ)E[et-1 | St = s],
t=o
where Lemma 1 ensures that we can interchange the gradient and infinite sum in the first equality.
□
Proposition 2. Assume we have a trajectory (so, ao, ri, ∙∙∙ , St, at, rt+ι). Let θ, θj ∈ Rd be arbi-
trary, for 0 ≤ j ≤ t. Then,
tt
γtrt+1	Vθ log πθj (aj | sj) - γtrt+1	Vθ log πθ (aj | sj)
j=o	j=o
t
≤ Ytlrt+ιlEkπ(∙ | Sj) - πj(∙ | Sj)k1 max kx(sj, a)k,
j=o	a
≤ Yt∣rt+ι∣(t + 1)max k∏(∙ | Sj) - ∏(∙ | Sj)ki max ∣∣x(sj∙,a)k
ja
(8)
(9)
(10)
Proof. In the following, we will write πj in place of πθj and π in place of πθ to reduce mess.
For a linear Boltzmann policy, a straightforward calculation provides the following gradient.
Vθlogπθ(a | s) = x(s, a) -	πθ(b | s)x(s, b).
b
Hence,
∣Vθlogπj(a | S) - Vθ log π(a | S)∣ = X x(S, b)(π(b | S) -πj(b | S))
b
≤ X llx(s,b)k ∙|(n(b | s) - πj(b | S))I
b
≤ kπ(∙ | s) - πj(∙ | s)ki max llx(s,a)k
13
Under review as a conference paper at ICLR 2021
Substituting,
tt
γtrt+1	Vθ log πθj (aj | Sj) - γtrt+1	Vθ log πθ (aj | Sj)
j=0	j=0
t
≤ Yt∣rt+ι∣E Vθ log ∏θj (aj | Sj) - Vθ log π (aj | Sj),
j=0
t
≤ γtlrt+ιl∑Skπ(∙ | Sj)- πj(∙ | Sj)kιmax∣∣x(sj,a)k,
j=0
thus proving the first inequality. The second inequality follows immediately.
□
14
Under review as a conference paper at ICLR 2021
A.3 Additional Plots
q.9.8.7,6.5/.3
Lo.CiciS0.0.Ci
ωφu2H JO AlLJP-ES φ⊂-ωoo
40	60 8Q
Timesteps
(a)
CartPole-vl, hidden layer = 32
Q.8.6/2 Q 2/,6
Lo.SCi0.0.666
- - -
səubjh① qi Jo A±Je=Eω① uωou
(b)
LunarLander-v2, hidden layer = 64
q.9,8.7,6.54.3
Lo.6o.o.SC50.
ωφu2H Φ5 JO AILJP-ES φ⊂-ωoo
(c)
Figure 4: Each line is the mean over 100 random policies; the standard errors are smaller than the
width of the lines. At every time step, the policy is updated as in Algorithm 1. Each value on the
plot is the cosine similarity of the trace at that time, compared to what the trace would have been if
the initial policy had been what the current policy is now, and if the exact same trajectory had been
followed-the so-called CounterfaCtual trace. The CounterfaCtual trace is calculated with Equation 5
using the current policy and the stored trajectory up until that point.
(d)
15
Under review as a conference paper at ICLR 2021
IPG vs R曰NFORCE on CartPole-vO (Hidden Size = 64)
IPG vs R曰NFORCE on CartPole-vO (Hidden Size = 32)
(a)
VS R日NFORCE on CartPole-vO (Hidden Size = 128)	IPG vs R曰NFORCE on CartPole-vO (Hidden Size = 256)
(b)
(c)	(d)
Figure 5: AUCs over 100,000 steps in the CartPole-v0 environment. Results are averaged over 30
runs, and shaded regions represent one standard error. Note the log-scale of the x-axis.
16
Under review as a conference paper at ICLR 2021
IPG vs AC(I) on CartPole-vO (Hidden Size = 64)
IPG vs AC(I) on CartPole-vO (Hidden Size = 32)
① >」ro 6u⊂J03①，M-O 0⊃< P ①Z=(DllLI ON
(a)
(b)
(c)
(d)
Figure 6: AUCs over 100,000 steps in the CartPole-v0 environment. Results are averaged over 30
runs, and shaded regions represent one standard error. Note the log-scale of the x-axis.
① >」ro 6U-E(D①，M-O 0⊃< P ①Z=(DllLI ON
17
Under review as a conference paper at ICLR 2021
A.4 An Attempt at Deriving IPG for the Average Reward Setting
Unlike REINFORCE, IPG is suitable for both episodic and continuing tasks. In continuing tasks,
it’s typical to consider the average reward objective, and tempting to ask if there’s a version of IPG
which directly optimizes this objective. We were not, however, able to find the policy gradient of this
objective in a form similar to Equation 3, from which the IPG update can be derived. Nevertheless,
we provide our initial attempt in here and leave this as an avenue for future work.
In this section, we try to derive the policy gradient that takes a similar form as equation 3.
The average reward rate is defined as
r(∏θ)= lim E [R/S。，Ao：t-i 〜∏θ]
t→∞
(14)
The limit is assumed to exist and to be independent of the initial state S0 (an ergodicity assumption;
see Puterman (2014)). The policy gradient theorem for average reward setting (Sutton et al., 2000)
provides the gradient of Equation 14.
Vθr(∏θ) = J^dπθ (S) J^qπθ (s,a)∏θ(a∣s)Vθ log ∏(a|s)
sa
lim E
t→∞
∞
log πθ(At | St) X(Rj+1 - r(πθ))
j=t
(15)
where qπθ(s, a) = E∏g[E∞=0(Rt+1 — r(∏θ))∣So = s,A。= a] is the differential action-value of
(s, a) ∈ S × A (Sutton & Barto, 2018), and dπθ is the stationary distribution of πθ.
For simplicity, we only consider the on-policy case. Using the policy gradient theorem for continu-
ing tasks, we have
18
Under review as a conference paper at ICLR 2021
V0r(∏θ) = lim E∏θ
t-∞
∞
Vθ log不θ(At | St) X(Rj+ι - r(∏θ))
j=t
1	t	∞
lim ---Eπθ X Vθ log 不θ(Ai | Si) X(Rj+1 - r(πθ))
t→∞ t + 1	z—z—
i=0	j=i
(Cesaro convergence)
lim
t→∞
1
t + 1
Vθ log ∏θ (A0∣S0)((R1 - r(∏θ)) + (R2 - r(∏θ)) + (R3 - r(πθ)) + ...)
+ Vθ log∏θ(A1∣S1)((R2 - r(∏θ)) + (R3 - r(∏θ)) + (R4 - r(∏θ)) + …)
+...
+ Vθ log ∏θ (AtISt)((Rt+1 - r(∏θ)) + (Rt+2 - r(∏θ)) + (Rt+3 - r(∏θ)) + ...)]
=lim -ɪ- ( E∏θ[(R1 - r(∏θ))(Vθlog∏θ(A0∣S0))
t→∞ t + 1 ∖
+ (R2 - r(∏θ))(Vθ log∏θ(A0∣S0) + Vθ log ∏θ(Ai ∣Si))
+...
+ (Rt- r(πθ)) "θ log πθ(A0∣s0) + vθ log πθ (AI ∣s1) + …+ vθ log πθ (At-IISt-1))]
+ Eμ[
(Rt+1 - r(∏θ))(Vθ log∏θ(A0∣S0) + Vθ log∏θ(A1∣S1) +----+ Vθ log∏(AtISt))
+ (Rt+2 -	r(∏θ))(Vθ log∏θ(A0∣S0) +	Vθ log∏θ(A1∣S1)	+----+	Vθ log∏(AtISt))
+ (Rt+3 -	r(∏θ))(Vθ log∏θ(A0∣S0) +	Vθ log∏θ(A1∣S1)	+ …+	Vθ log∏(AtISt))
+...]
1 (	t-1	j
Jim 7—7 E∏θ E(Rj+1-rSθ)) Evθ logπθ(AkISk)
t→∞ t +1
∖ j=0	k=0
∞	t
X(Rj+1 -r(∏θ)) X Vθ log ∏θ(Ak ∣Sk)
j =t	k=0
This involves a term that is similar to equation 3
1	t-1	j
lim ——-Eμ X(Rj+1 - r(∏θ)) X Ne log∏θ(AkISk)
t→∞ t + 1
j=0	k=0
which should be well-defined, and the other term
1	∞	t
lim —— Eμ X(Rj+1-r(∏θ)) X Vθ log∏(Ak ∣Sk)
t→∞ t + 1
j=t	k=0
, which should also be well-defined and equal to 0.
However, it is not clear to us if both terms are well-defined and if the second term equals to 0.
19
Under review as a conference paper at ICLR 2021
A.5 UNIFYING IPG AND REINFORCE
While AdaptChain highlights a scenario where online adaptation via interim fixed-horizon policy
gradient updates is beneficial, there may exist scenarios where it can be detrimental. For example,
with large learning rates, it may over-greedify toward relatively myopic outcomes. Waiting a bit
before performing updates can also dampen the policy gradient discrepancies caused by a changing
policy. This motivates a unifying algorithm with an adjustable update frequency.
The REINFORCE update can be recovered by accumulating (but not applying) each step’s IPG
update, and applying them all at once at the end of an episode. This suggests that the extremes can
be interpolated by only applying some proportion of the stored update, saving the remainder for later
time steps. In particular, we specify the update frequency through a probability of updating: with
probability ω the algorithm applies (and clear) the accumulated IPG updates, and with probability
1 - ω it will store the current update to be applied later. At episode termination, all remaining
accumulated updates will be performed deterministically.
This gives the following, expected incremental update, denoted IPG(ω):
et — γet-ι + Y tVθ log π (AtISt)
∆θt J (1 - ω)∆θt-i + αRt+ιet
θt J θt-i + ω∆θt
with the following, additional update performed on episode termination:
θt J θt + (1 - ω)∆θt
20