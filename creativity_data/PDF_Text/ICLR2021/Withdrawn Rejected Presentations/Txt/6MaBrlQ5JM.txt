Under review as a conference paper at ICLR 2021
THE EFFICACY OF L1 REGULARIZATION IN NEURAL
Networks
Anonymous authors
Paper under double-blind review
Ab stract
A crucial problem in neural networks is to select the most appropriate number of
hidden neurons and obtain tight statistical risk bounds. In this work, we present
a new perspective towards the bias-variance tradeoff in neural networks. As an
alternative to selecting the number of neurons, we theoretically show that L1 reg-
ularization can control the generalization error and sparsify the input dimension.
In particular, with an appropriate L1 regularization on the output layer, the net-
work can produce a statistical risk that is near minimax optimal. Moreover, an
appropriate L1 regularization on the input layer leads to a risk bound that does not
involve the input data dimension. Our analysis is based on a new amalgamation
of dimension-based and norm-based complexity analysis to bound the generaliza-
tion error. A consequent observation from our results is that an excessively large
number of neurons do not necessarily inflate generalization errors under a suitable
regularization.
1	Introduction
Neural networks have been successfully applied in modeling nonlinear regression functions in var-
ious domains of applications. A critical evaluation metric for a predictive learning model is to
measure its statistical risk bound. For example, the L1 or L2 risks of typical parametric models such
as linear regressions are at the order of (d/n)1/2 for small d (Seber & Lee, 2012), where d and n
denote respectively the input dimension and number of observations. Obtaining the risk bound for a
nonparametric regression model such as neural networks is highly nontrivial. It involves an approxi-
mation error (or bias) term as well as a generalization error (or variance) term. The standard analysis
of generalization error bounds may not be sufficient to describe the overall predictive performance
of a model class unless the data is assumed to be generated from it. For the model class of two-layer
feedforward networks and a rather general data-generating process, Barron (1993; 1994) proved an
approximation error bound of O(r-1/2) where r denotes the number of neurons. The author further
developed a statistical risk error bound of O((d/n)1/4), which is the tightest statistical risk bound
for the class of two-layer neural networks up to the authors’ knowledge (for d < n). This risk bound
is based on an optimal bias-variance tradeoff involving an deliberate choice of r. Note that the risk
is at a convergence rate much slower than the classical parametric rate. We will tackle the same
problem from a different perspective, and obtain a much tighter risk bound.
A practical challenge closely related to statistical risks is to select the most appropriate neural net-
work architecture for a particular data domain (Ding et al., 2018). For two-layer neural networks,
this is equivalent to selecting the number of hidden neurons r. While a small r tends to underfit, re-
searchers have observed that the network is not overfitting even for moderately large r. Nevertheless,
recent research has also shown that an overly large r (e.g., when r > n) does cause overfitting with
high probability (Zhang et al., 2016). It can be shown under some non-degeneracy conditions that
a two-layer neural network with more than n hidden neurons can perfectly fit n arbitrary data, even
in the presence of noise, which inevitably leads to overfitting. A theoretical choice of r suggested
by the asymptotic analysis in (Barron, 1994) is at the order of (n/d)1/2, and a practical choice of r
is often from cross-validation with an appropriate splitting ratio (Ding et al., 2018). An alternative
perspective that we advocate is to learn from a single neural network with sufficiently many neurons
and an appropriate L1 regularization on the neuron coefficients, instead of performing a selection
from multiple candidate neural models. A potential benefit of this approach is easier hardware
1
Under review as a conference paper at ICLR 2021
implementation and computation since we do not need to implement multiple models separately.
Perhaps more importantly, this perspective of training enables much tighter risk bounds, as we will
demonstrate. In this work, we focus on the model class of two-layer feedforward neural networks.
Our main contributions are summarized below. First, we prove that L1 regularization on the co-
efficients of the output layer can produce a risk bound O((d/n)1/2) (up to a logarithmic factor)
under the L1 training loss, which approaches the minimax optimal rate. Such a rate has not been
established under the L2 training loss so far. The result indicates a potential benefit of using L1
regularization for training a neural network, instead of selecting from a number of neurons. Addi-
tionally, akey ingredient of our result is a unique amalgamation of dimension-based and norm-based
risk analysis, which may be interesting on its own right. The technique leads to an interesting obser-
vation that an excessively large r can reduce approximation error while not increasing generalization
error under L1 regularizations. This implies that an explicit regularization can eliminate overfitting
even when the specified number of neurons is enormous. Moreover, we prove that the L1 regulariza-
tion on the input layer can induce sparsity by producing a risk bound that does not involve d, where
d may be much larger compared with the true number of significant variables.
Related work on neural network analysis. Despite the practical success of neural networks, a
systematic understanding of their theoretical limit remains an ongoing challenge and has motivated
research from various perspectives. Cybenko (1989) showed that any continuous function could be
approximated arbitrarily well by a two-layer perceptron with sigmoid activation functions. Barron
(1993; 1994) established an approximation error bound of using two-layer neural networks to fit
arbitrary smooth functions and their statistical risk bounds. A dimension-free Rademacher com-
plexity for deep ReLU neural networks was recently developed (Golowich et al., 2017; Barron &
Klusowski, 2019). Based on a contraction lemma, a series of norm-based complexities and their cor-
responding generalization errors are developed (Neyshabur et al., 2015, and the references therein).
Another perspective is to assume that the data are generated by a neural network and convert its
parameter estimation into a tensor decomposition problem through the score function of the known
or estimated input distribution (Anandkumar et al., 2014; Janzamin et al., 2015; Ge et al., 2017;
Mondelli & Montanari, 2018). Also, tight error bounds have been established recently by assuming
that neural networks of parsimonious structures generate the data. In this direction, Schmidt-Hieber
(2017) proved that specific deep neural networks with few non-zero network parameters can achieve
minimax rates of convergence. Bauer & Kohler (2019) developed an error bound that is free from
the input dimension, by assuming a generalized hierarchical interaction model.
Related work on L1 regularization. The use of L1 regularization has been widely studied in
linear regression problems (Hastie et al., 2009, Chapter 3). The use of L1 regularization for training
neural networks has been recently advocated in deep learning practice. A prominent use of L1
regularization was to empirically sparsify weight coefficients and thus compress a network that
requires intensive memory usage (Cheng et al., 2017). The extension of L1 regularization to group-
L1 regularization (Yuan & Lin, 2006) has also been extensively used in learning various neural
networks (Han et al., 2015; Zhao et al., 2015; Wen et al., 2016; Scardapane et al., 2017). Despite
the above practice, the efficacy of L1 regularization in neural networks deserves more theoretical
study. In the context of two-layer neural networks, we will show that the L1 regularizations in the
output and input layers play two different roles: the former for reducing generalization error caused
by excessive neurons while the latter for sparsifying input signals in the presence of substantial
redundancy. Unlike previous theoretical work, we consider the L1 loss, which ranks among the
most popular loss functions in, e.g., learning from ordinal data (Pedregosa et al., 2017) or imaging
data (Zhao et al., 2016), and for which the statistical risk has not been studied previously. In practice,
the use of L1 loss for training has been implemented in prevalent computational frameworks such
as Tensorflow (Google, 2016), Pytorch (Ketkar, 2017), and Keras (Gulli & Pal, 2017).
2	Problem Formulation
2.1	Model assumption and evaluation
Suppose we have n labeled observations {(xi , yi)}i=1,...,n, where yi’s are continuously-valued re-
SPonses or labels. We assume that the underlying data generating model is yi = f*(χi) + ε for
some unknown function f*(∙), where Xi's ∈ X ⊂ Rd are independent and identically distributed,
2
Under review as a conference paper at ICLR 2021
Input dimension ! Hidden dimension "
Figure 1: A graph showing the two-layer neural network model considered in (2).
and ε∕s are independent and identically distributed that is symmetric at zero and
E (ε2 | Xi) ≤ τ2.	(1)
Here, X is a bounded set that contains zero, for example {x : kxk∞ ≤ M} for some constant M.
Our goal is learn a regression model fn : x 7→ fn(x) for prediction. The fn is obtained from the
following form of neural networks
r
ajσ(wj>x + bj ) + a0 ,	(2)
j=1
where a0 , aj , bj ∈ R, wj ∈ Rd, j = 1, . . . , r, are parameters to estimate. We let a =
[a0, a1, . . . , ar]T denote the output layer coefficients. An illustration is given Figure 1. The esti-
mation is typically accomplished by minimizing the empirical risk n-1 P2ι '(y%, f (xi)), for some
loss function l(∙) plus a regularization term. We first consider the Li regularization at the output
layer. In particular, we search for such f by the empirical risk minimization from the function class
FV = f : Rd → Rf (x) = Xajσ(wj>x + bj) + a0, kak1 ≤ V	(3)
j=1
where V is a constant. The following statistical risk measures the predictive performance of a
learned model f :
R(f ) = E '(y,f(χ))- E '(y,f*(χ)).
The loss function '(∙) is pre-determined by data analysts, usually the Li loss defined by '(y, y)=
|y 一 y| or the L? loss defined by '2(y,y) = (y 一 y)2. Under the Li loss, the risk is R(f)=
E ∣f*(x) + ε 一 f (x)| 一 E ∣ε∣, which is nonnegative for symmetric random variables ε. It is typical
to use the same loss function for both training and evaluation.
2.2	Notation
Throughout the paper, we use n, d, k, r to denote the number of observations, the number of input
variables or input dimension, the number of significant input variables or sparsity level, the number
of neurons (or hidden dimension), respectively. We write an & bn, bn . an, or bn = O(an), if
|bn/an| < c for some constant c for all sufficiently large n. We write an bn if an & bn as well as
an . bn. Let N(μ, V) denote Gaussian distribution with mean μ and covariance V. Let k ∙ k 1 and
k ∙ k2 denote the common Li and L2 vector norms, respectively. Let X denote the essential support
of X. For any vector z ∈ Rd, we define kzkX =∆ supx∈X |x>z|, which may or may not be infinity.
If X = {x : ∣∣xk∞ ≤ M}, ∣∣z∣∣χ is equivalent to MIlzki. Throughout the paper, fn denotes the
estimated regression function with n being the number of observations.
2.3	Assumptions and classical results
We introduce some technical assumptions necessary for our analysis, and state-of-the-art statistical
risk bounds built through dimension-based complexity analysis.
3
Under review as a conference paper at ICLR 2021
Assumption L The activation function σ(∙) is a bounded function on the real line satisfying σ(x) →
1 as x → ∞ and σ(x) → 0 as x → -∞, and it is L-Lipschitz for some constant L.
Assumption 2. The regularization constant V is larger than 2C + f*(0), where C is any constant
such that the Fourier transform of f*, denoted by F, satisfies
kωkXF (dω) ≤ C.
(4)
Assumption 3. σ(x) approaches its limits at least polynomially fast, meaning that ∣σ(x) 一 1{x >
0}| < ε for all |x| > Xε where Xε is a polynomial of 1∕ε. Also, the value of η = SuPjkwj ∣∣χ scales
with n polynomially meaning that logη = O(log n) as n → ∞.
Assumption 4. There exists a constant c > 0 and a bounded subset S ⊂ R such that P(X ∈ S) > c
and infχ∈s σ0(x) > c for X 〜N(0,1).
We explain each assumption below. The above notation of C, V follow those in (Barron, 1993;
1994). Assumption 1 specifies the class of the activation functions we consider. A specific case is
the popular activation function σ(x) = 1∕{1+exp(-x)}. Assumption 2, first introduced in (Barron,
1993), specifies the smoothness condition for f to ensure the approximation property of neural
networks (see Theorem 2.1). In Assumption 3, the condition for w is for technical convenience. It
could also be replaced with the following alternative condition: There exists a constant c > 0 such
that the distribution of x satisfies
suP P log(|w> x|) < c log ε < ε
w:kwk2=1
for any ε ∈ (0, 1). Simply speaking, the input data x is not too small with high probability. This
condition is rather mild. For example, it holds when each component of x has a a bounded density
function. This alternative condition ensures that for some small constant ε > 0 and any w ∈ Rd,
there exists a surrogate of w, w ∈ Rd with log kw∣2 = O(- log ε), such that
P(∣σ(w>x) 一 σ(w>x)∣ > ε) < ε.
And this can be used to surrogate the assumption ofw in Assumption 3 throughout the proofs in the
appendix. Assumption 4 means that σ(∙) is not a nearly-constant function. This condition is only
used to bound the minimax lower bound in Theorem 3.2.
Theorem 2.1 (Approximation error bound (Barron, 1993)). Suppose that Assumptions 1, 2, 3 hold.
We have
f科{((f(x)- f*(x))2μ(dx)} / ≤ 2C (√r + δη),
where μ denotes a probability measure on X,
δη =	inf 2ε + suP σ(ηx) 一 1{x > 0}	,	(5)
0<ε<1/2 I	∣x∣>ε	J
η is defined in Assumption 3, and C is defined in (4).
Theorem 2.2 (Statistical risk bound (Barron, 1994)). Suppose that Assumptions 1, 2, 3 hold. Then
the L2 estimator f in FV satisfies E {fn(x) — f*(x)}2 . V2∕r + (rdlog n)∕n. In particular, if
we choose r N Vʌ/n/(dlogn), then E {f (x) — f*(x)}2 . Vʌ/(dlogn)∕n.
It is known that a typical parametric rate under the L2 loss is at the order of O(d∕n), much faster than
the above result. This gap is mainly due to excessive model complexity in bounding generalization
errors. We will show in Section 3 that the gap in the rate of convergence can be filled when using L1
loss. Our technique will be based on the machinery of Rademacher complexity, and we bound this
complexity through a joint analysis of the norm of coefficients (‘norm-based’) as well as dimension
of parameters (‘dimension-based’).
2.4	Model complexity and generalization error
The statistical risk consists of two parts. The first part is an approximation error term non-increasing
in the number of neurons r, and the second part describes generalization errors. The key issue for
4
Under review as a conference paper at ICLR 2021
risk analysis is to bound the second term using a suitable model complexity and then tradeoff with
the first term. We will develop our theory based on the following measure of complexity.
Let F denote a class of functions each mapping from X to R, and x1, x2, . . . , xn ∈ X. Following a
similar terminology as in (Neyshabur et al., 2015), the Rademacher complexity, or simply ‘complex-
ity’，of a function class F is defined by E SuPf ∈f |n-1 Pn=ι ξif (xi)∖, where ξi,i = 1,2,...,n
are independent symmetric Bernoulli random variables.
Lemma 2.3 (Rademacher complexity of FV ). Suppose that Assumptions 1, 3 hold. Then for the
Rademacher complexity of FV , we have
n
1n
E SUP — fξif(Xi).
f∈FVn i=1
i=1
V√d log n
√n
(6)
The proof is included in Appendix A.1. The bound in (6) is derived from an amalgamation of
dimension-based and norm-based analysis elaborated in the appendix. It is somewhat surprising
that the bound does not explicitly involve the approximation error part (that depends on r and η).
This Rademacher complexity bound enables us to derive tight statistical risk bounds in the following
section.
3	Main Results
3.1	STATISTICAL RISK BOUND FOR THE L1 REGULARIZED NETWORKS IN (3)
Theorem 3.1	(Statistical risk bound). Suppose that Assumptions 1, 2, 3 hold. Then the constrained
L1 estimator fn over FV satisfies
R(fn) . (√r + δη)c + V√d√n + T,	⑺
where δη is defined in (5), andτ was introduced in (1). Moreover, choosing the parameters r, η large
enough, we have
R(fn) . VT + T .	(8)
n
The proof is in Appendix A.2. We briefly explain our main idea in deriving the risk bound (7). A
standard statistical risk bound contains two parts which correspond to the approximation error and
generalization error, respectively. The approximation error part in (7) is the first term, which involves
the hidden dimension r and the norm of input coefficients through η. This observation motivates
us to use the norm of output-layer coefficients through V and the input dimension d to derive a
generalization error bound. In this way, the generalization error term does not involve r already
used for bounding the approximation error, and thus a bias-variance tradeoff through r is avoided.
This thought leads to the generalization error part in (7), which is the second term involving V and
d. Its proof combines the machinery of both dimension-based and norm-based complexity analysis.
From our analysis, the error bound in Theorem 3.1 is a consequence of the L1 loss function and
the employed Li regularization. In comparison with the previous result of Theorem 2.2, the bound
obtained in Theorem 3.1 is tight and it approaches the parametric rate ,d/n for the d < n regime.
Though we can only prove for L1 loss in this work, we conjecture that the same rate is achieved
using L2 loss.
In the following, we further show that the above risk bound is minimax optimal. The minimax
optimality indicates that deep neural networks with more than two layers will not perform much
better than shallow neural networks when the underlying regression function belongs to FV .
iid
Theorem 3.2	(Minimax risk bound). SuPPosethatAssumPtionsI and 4 hold, and xi, X2,...,xn 〜
N (0, Id) ,then inf fn SUPf ∈Fv R(fn(x)) & V pd/n.
Here the FV is the same one as defined in (3). All the smooth functions f*(∙) that satisfy V >
2C + f* (0) and (4) belong to FV according to Theorem 2.1. The proof is included in Appendix A.3.
5
Under review as a conference paper at ICLR 2021
3.2	Adaptiveness to the input sparsity
It is common to input a large dimensional signal to a neural network, while only few components
are genuinely significant for prediction. For example, in environmental science, high dimensional
weather signals are input for prediction while few are physically related (Shi et al., 2015). In image
processing, the image label is relevant to few background pixels (Han et al., 2015). In natural
language processing, a large number of redundant sentences sourced from Wikipedia articles are
input for language prediction (Diao et al., 2019). The practice motivates our next results to provide
a tight risk bound for neural networks whose input signals are highly sparse.
Assumption 5. There exists a positive integer k ≤ d and an index set S ⊂ {1, . . . , d} with card(S) =
k, such that f*(x) = g*(xs) for some function g*(∙) with probability one.
The subset S is generally unknown to data analysts. Nevertheless, ifwe know k, named the sparsity
level, the risk bound could be further improved by a suitable regularization on the input coefficients.
We have the following result where d is replaced with k in the risk bound of Theorem 3.1.
Proposition 3.3. Suppose that that Assumptions 1, 2, 3, 5 hold. Suppose that f is the Li estimator
over the following function class
f : Rd → Rf (x) = Xajσ(wj>x + bj) + a0, kak1 ≤ V,sup kwj k0 ≤ k .
Then R(fn) . √{k log(dn)}∕n.
The proof is included in Appendix A.4. The above statistical risk bound is also minimax optimal
according to a similar argument in Theorem 3.2. From a practical point of view, the above L0
constraint is usually difficult to implement, especially for a large input dimension d. Alternatively,
one may impose an L1 constraint instead of an L0 constraint on the input coefficients. Our next
result is concerned with the risk bound when the model is learned from a joint regularization on the
output and input layers. For technical convenience, we will assume that X is a bounded set.
Theorem 3.4. Consider the following function class of two-layer neural networks
r
FV,η = f : Rd → Rf (x) = X aj σ(wj>x + bj) + a0, kak1 ≤ V, sup (kwj k1 + |bj |) ≤ η
j=1	1≤j≤r
Suppose that V & C, where C is defined in (4). Then the constrained Li estimator f over Fv,η
satisfies
R(fn). C (√r+δη)+ V√M,
where δη is defined in (5). In particular, choosing r large enough, we have
R(fn) . Cδη + V√+T
which does not involve the input dimension d and the number of hidden neurons r.
suppose that σ(x) = 1/(1 + e-x), η N l nlog2 n l , then R(fn) . V{(log n)/n
Moreover,
i/3
The proof is included in Appendix A.5. In the above result, the risk bound is at the order of
O(n-i/3), which is slower than the O(n-i/2) in the previous Theorem 3.1 and Proposition 3.3
if ignoring dand logarithmic factors ofn. However, for a large input dimension d that is even much
larger than n, the bound can be much tighter than the previous bounds since it is dimension-free.
4 Conclusion and Further Remarks
We studied the tradeoff between model complexity and statistical risk in two-layer neural networks
from the explicit regularization perspective. We end our paper with two future problems. First, in
Theorem 3.4, For a small d, the order of n-i/3 seems to be an artifact resulting from our technical
arguments. We conjecture that in the small d regime, this risk bound could be improved to O(n-i/2)
by certain adaptive regularizations. Second, it would be interesting to emulate the current approach
to yield similarly tight risk bounds for deep forward neural networks.
6
Under review as a conference paper at ICLR 2021
References
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M Kakade, and Matus Telgarsky. Tensor
decompositions for learning latent variable models. J. Mach. Learn. Res., 15:2773-2832, 2014.
Andrew R Barron. Universal approximation bounds for superpositions of a sigmoidal function.
IEEE Trans. Inf. Theory, 39(3):930-945,1993.
Andrew R Barron. Approximation and estimation bounds for artificial neural networks. Machine
learning, 14(1):115-133, 1994.
Andrew R Barron and Jason M Klusowski. Complexity, statistical risk, and metric entropy of deep
nets using total path variation. arXiv preprint arXiv:1902.00800, 2019.
Benedikt Bauer and Michael Kohler. On deep learning as a remedy for the curse of dimensionality
in nonparametric regression. Ann. Stat., 47(4):2261-2285, 2019.
Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A survey of model compression and acceleration
for deep neural networks. arXiv preprint arXiv:1710.09282, 2017.
George Cybenko. Approximations by superpositions ofa sigmoidal function. Math. Control Signals
Syst. ,2:183-192, 1989.
Enmao Diao, Jie Ding, and Vahid Tarokh. Restricted recurrent neural networks. 2019 IEEE Conf.
on Big Data, 2019.
Jie Ding, Vahid Tarokh, and Yuhong Yang. Model selection techniques: An overview. IEEE Signal
Process. Mag., 35(6):16-34, 2018.
Rong Ge, Jason D Lee, and Tengyu Ma. Learning one-hidden-layer neural networks with landscape
design. arXiv preprint arXiv:1711.00501, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. arXiv preprint arXiv:1712.06541, 2017.
Research Team Google. Tensorflow: A system for large-scale machine learning. Proc. 12th Symp.
Operating ^yst. Des. Implementation, pp. 265-283, 2016.
Antonio Gulli and Sujit Pal. Deep Learning with Keras. Packt Publishing Ltd, 2017.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural network. Advance. Neural Inf. Process. Sys., pp. 1135-1143, 2015.
Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data
mining, inference, and prediction. Springer Science & Business Media, 2009.
Majid Janzamin, Hanie Sedghi, and Anima Anandkumar. Beating the perils of non-convexity: Guar-
anteed training of neural networks using tensor methods. arXiv preprint arXiv:1506.08473, 2015.
Nikhil Ketkar. Introduction to pytorch. Deep learning with python, pp. 195-208, 2017.
Marco Mondelli and Andrea Montanari. On the connection between learning two-layers neural
networks and tensor decomposition. arXiv preprint arXiv:1802.07301, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. Conf. Learning Theory, pp. 1376-1401, 2015.
Fabian Pedregosa, Francis Bach, and Alexandre Gramfort. On the consistency of ordinal regression
methods. J. Mach. Learn. Res., 18(1):1769-1803, 2017.
Simone Scardapane, Danilo Comminiello, Amir Hussain, and Aurelio Uncini. Group sparse regu-
larization for deep neural networks. Neurocomputing, 241:81-89, 2017.
Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with relu activation
function. arXiv preprint arXiv:1708.06633, 2017.
7
Under review as a conference paper at ICLR 2021
George AF Seber and Alan J Lee. Linear regression analysis, volume 329. John Wiley & Sons,
2012.
Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo.
Convolutional LSTM network: A machine learning approach for precipitation nowcasting. In
Advance. Neural Inf Process. Sys., pp. 802-810, 2015.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. Advance. Neural Inf. Process. Sys., pp. 2074-2082, 2016.
Yuhong Yang and Andrew Barron. Information-theoretic determination of minimax rates of conver-
gence. Ann. Stat., pp. 1564-1599, 1999.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. J. R.
Stat. Soc. Ser. B Methodol., 68(1):49-67, 2006.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with
neural networks. IEEE Trans. Comput., 3(1):47-57, 2016.
Lei Zhao, Qinghua Hu, and Wenwu Wang. Heterogeneous feature selection with multi-modal deep
neural networks and sparse group LASSO. IEEE Trans. Multimed., 17(11):1936-1948, 2015.
A Appendix
A.1 Proof of Lemma 2.3
We first prove (6), which uses an amalgamation of dimension-based and norm-based analysis. For
the output layer, we use the following norm-based analysis
1 n
E sup - y2ξif (zi)
f∈FVn i=1
1n
E sup |ha, 一 fξiσ(W >zi + b)i∣
f∈FV	n i=1
(9)
≤ sup kak1 E sup
f∈FV
≤ VE sup
w∈Rd
1n
n fξ σ(W >4 + b)
i=1
1n >
-V^ξiσ(wτZi + b)
n
i=1
1 n
≤ VE sup max 一 ɪ2 ξiσ(w>Zi + bj)
f∈FV j n i=1
For notational convenience, we define	w0	=	0, b0	=	0,	and	a0 =σ	(0)-1a0σ(w0>	z + b0)	so that
a0 can be treated in a similar manner as other ai ’s. Without loss of generality, we do not separately
consider a0 in the following proofs.
Next, we prove that
1n
E SUp -yZξiσ(wτZi + b).
w∈Rd n i 1
i=1
d log n
n
(10)
∞
n
and thus conclude the proof. The proof will be based on an ε-net argument together with the union
bound. For any ε, let Wε ⊂ Rd denote the subset
Wε = W w = 2d (iι, i2,..., id)： ij ∈ Z, ∣∣w∣∣1 ≤ ηn }.
Then, for any w, b, there exists some element W ∈ Wε such that
sup ∣σ(wτz + b) 一 σ(Wτz + b)| ≤ sup I(WTz + b) — (WTz + b)| ≤ sup |(w — W)Tz| + |b — b|
z∈X	z	z
,ll … ll ll ,, ¢,
≤ ∣∣w - W∣∣1 sup kzk∞ + |b — b| ≤ ε,
z
8
Under review as a conference paper at ICLR 2021
where b = (ε∕2d) [(2db∕ε)C and [∙[ is the floor function. By Bernstein,s Inequality, for any w, b,
P(|N X ξiσ(w>Zi + b)∣ >t) ≤ 2exp{-
nt2
2(1+ t/3) ʃ.
By taking the union bound over Wε, and use the fact that log Card(Wε) . dlog(nd∕ε), We obtain
n
sUp
w∈Rd
1n
n∑ξiσ(w>zi+b) .
i=1
rd nd 1
nlog τlog j，
with probability at least 1 - δ. Then the desired result is obtained by taking ε 〜ʌ/(dlog n)∕n.
A.2 Proof of Theorem 3.1
The proof is based on the following contraction lemma used in (Neyshabur et al., 2015).
Lemma A.1 (Contraction Lemma). Suppose that g is L-Lipschitz and g(0) = 0. Then for any
function class F mapping from X to R and any set {X1, X2, . . . , Xn}, we have
n
n
1n
E SUp — fξig(f(xi))
f∈F n i=1
i=1
≤ 2LE sUp
f∈F
1n
n ∑ξif (χi).
i=1
With the above lemma, we have the following result.
Lemma A.2. The constrained Li estimator fn over F satisfies
R(fn) ≤ minE ∣f (x) - f*(x)∣ + 2E sup ∣1 Xξif(z,∣ +2
f∈F	f∈F n i=1
Ey2
n
(11)
(12)
Proof. Define the empirical risk as:
Rn(f) = E (- X If*(Xi) + εi- f (Xi)Pj - E ∣ε∣.
n i=1
(13)
Since fn minimizes n-1
_ , O .	_
Pn=1 ∣f*(xi) + εi - f (xi)∣ in F, we have
O.
,"	,"	-	,"	.'∖	-	,"	.	.八	.一	,
R(fn) ≤ R(fn) -{Rn(fn) - Rn (f)} = {R(fn) - Rn(fn)} + Rn(fθ),
where f0 = arg minf∈F R(f). We also have
Rn(fo) = R(fo) = minE (∣f*(x) + ε - f(xi)∣ - ∣ε∣) ≤ minEIf(X)- f*(x)∣.
(14)
(15)
O
O
O
O
O
O
In the following, we will analyze the term R(fn) - Rn(fn) in (14). Let zi's denote independent
and identically distributed copies of Xi ’s.
R(fn)-Rn(fn)= E J £{
O
O
lfn(zi) - f*∙(zi) - ε/ - Ifn(Xi) - f*(Xi) - ε/
≤ E sup 1 χ{If (zi) - f*(zi) - εi∣ - ∣f(xi) - f*(xi) -εi∣}
1n
≤ 2E sup — V2ξilf (Zi)- f* (Zi)- ε/,
f∈F n i=1
where ξ1 , . . . , ξn are independent and identically distributed symmetric Bernoulli random variables
that are independent with Zi’s. According to Lemma A.1, since g(X) = IXI is 1-Lipschitz and
g(0) = 0, we have
1n	1n
E sup - Xξi∣f(zi) - f*(zi) - εi∣ ≤ 2E sup |- Xξi(f (Zi)- f*(zi) - εi)∣
f∈F n i=1	f∈F n i=1
n
≤ 2E sUp
f∈F
1n
n X ξif(zi) +2
i=1
Ey2
n
Combining this and (15), we conclude the proof of Lemma A.2.
□
9
Under review as a conference paper at ICLR 2021
Proof of Theorem 3.1. The proof of (7) is a direct consequence of Lemma 2.3, Lemma A.2,
Theorem 2.1 and the fact that the first moment is no more than the second moment. The proof of (8)
follows from the fact that δ(η) → 0 as η → ∞.
A.3 Proof of Theorem 3.2
Define a subclass of FV by
F0 = f :Rd →RIIf(X) = V σ(w>X), ∣w∣2 = 1 .
In the following, we will prove the minimax bound for FV by analyzing F0 . Notice that
E ∣σ(w>x) — σ(w>x)∣ ≥ E inf σ0(u) ∙ ∣w>X — w>x∣∙ I(w>x, w>X ∈ S) & ∣∣wι — W2k2.
u
Let M1 (ε) denote the packing ε-entropy of F0 with L1 distance, then M1 (ε) is greater than the
packing ε-entropy of B1d with L2 distance, which means M1(ε) & d. Let Vk(ε) denote the covering
ε-entropy of F0 with the square root Kullback-Leibler divergence, then according to its relation with
the L2 distance shown in (Yang & Barron, 1999), we have
Vk (ε) ≤ M2(√2ε) . dlogε,
where M2 (ε) denote the packing ε-entropy of FV with L2 loss function. The second inequality
is proved in a similar way to the proof of Lemma 2.3, which is omitted here for brevity. Hence,
according to (Yang & Barron, 1999, Theorem 1),
O
O
.	一 ,'' . 一 ,'' , ,, . 
inf sup R(fn(x)) ≥ inf Sup R(fn(x)) & V
fn f ∈Fv	fn f ∈F0
This concludes the proof.
d
n
A.4 Proof of Proposition 3.3
To prove the proposition, it is sufficient to verify the following Rademacher complexity bound
n
_	14」丁 .、一 J~~--
E SUPnE ξiσ(W Zi +b) . ko logdlog
n,
i=1
which can be derived easily by adjusting the proof in Lemma 2.3. Then the result follows with a
similar analysis as in Theorem 3.1.
A.5 Proof of Theorem 3.4
It can be verified from the identity (9) that
n
r
n
1n	r	1n
E sup -∑2ξif(Xi) ≤ ∑2E sup |ajI -Σ2ξiσ(w>Xi + bj).
f∈FV n i=1	j=0 f∈FV	n i=1
(16)
Then according to Lemma A.1, we have
E sup
f∈FV
1n
n X ξiσ(wj
i=1
Xi + bj) . γ -g-(Iwjkx + |bj|).
(17)
Combining (16) and (17), we obtain the following lemma that may be interesting on its own right.
Lemma A.3. We have
E sup — X ξif (Xi)I . ∖[~g~ X Iaj |(k wjkX + |bj I).
f∈FV n i=1	n j=0
V V -n- max kwjkχ.
Since IlwkX . IIwkI and {w ： kwkX . η} ⊂ {w ： kwkι . η}, the k ∙ IlX can be replaced with
k ∙ kι in the bounds in Lemmas A.3 and A.2. Then, with a similar argument as in the proof of
Theorem 3.1, we conclude the proof of Theorem 3.4.
10