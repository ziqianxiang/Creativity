Under review as a conference paper at ICLR 2021
Learning	Continuous-Time	Dynamics	by
Stochastic Differential Networks
Anonymous authors
Paper under double-blind review
Ab stract
Learning continuous-time stochastic dynamics is a fundamental and essential
problem in modeling sporadic time series, whose observations are irregular and
sparse in both time and dimension. For a given system whose latent states
and observed data are multivariate, it is generally impossible to derive a pre-
cise continuous-time stochastic process to describe the system behaviors. To
solve the above problem, we apply Variational Bayesian method and propose a
flexible continuous-time stochastic recurrent neural network named Variational
Stochastic Differential Networks (VSDN), which embeds the complicated dynam-
ics of the sporadic time series by neural Stochastic Differential Equations (SDE).
VSDNs capture the stochastic dependency among latent states and observations
by deep neural networks. We also incorporate two differential Evidence Lower
Bounds to efficiently train the models. Through comprehensive experiments, we
show that VSDNs outperform state-of-the-art continuous-time deep learning mod-
els and achieve remarkable performance on prediction and interpolation tasks for
sporadic time series.
1	Introduction and Related Works
Many real-world systems experience complicated stochastic dynamics over a continuous time pe-
riod. The challenges on modeling the stochastic dynamics mainly come from two sources. First, the
underlying state transitions of many systems are often uncertain, as they are placed in unpredictable
environment with their states continuously affected by unknown disturbances. Second, the monitor-
ing data collected may be sparse and at irregular intervals as a result of the sampling strategy or data
corruption. The sporadic data sequence loses a large amount of information and system behaviors
hidden behind the intervals of the observed data. In order to accurately model and analyze dynamics
of these systems, it is important to reliably and efficiently represent the continuous-time stochastic
process based on the discrete-time observations.
In some domains, the derivation of the continuous-time stochastic model relies heavily on human
knowledge and many studies focus on its inference problem (Ryder et al., 2018; Sarkka et al.,
2015). But in more domains (e.g., video analysis (Vondrick et al., 2016) and human activity de-
tection (Rubanova et al., 2019)), it is difficult and sometimes intractable to derive an accurate model
to capture the underlying temporal evolution from the collected sequence of data. Although some
studies have been made on approximating the stochastic process from the data collected, the ma-
jority of these methods define the system dynamics with a linear model (Macke et al., 2011; Yu
et al., 2009b;a), which can not well represent high-dimensional data with nonlinear relationship.
Recently, the Neural Ordinary Differential Equation (ODE) studies (Chen et al., 2018; Rubanova
et al., 2019; Jia & Benson, 2019; De Brouwer et al., 2019; Yildiz et al., 2019; Kidger et al., 2020)
introduce deep learning models to learn an ODE and apply it to approximate continuous-time dy-
namics. Nevertheless, these methods generally neglect the randomness of the latent state trajectories
and posit simplified assumptions on the data distribution (e.g. Gaussian), which strongly limits their
capability of modeling complicated continuous-time stochastic processes.
Compared to ODE, Stochastic Differential Equation (SDE) (J0rgensen et al., 2020) is a more prac-
tical solution in modeling the continuous-time stochastic process. Recently there have been some
studies on bridging the gap between deep neural networks and SDEs (Ha et al., 2018). In (Hegde
et al., 2019; Liu et al., 2020; Peluchetti & Favaro, 2020; Wang et al., 2019; Kong et al., 2020), SDEs
1
Under review as a conference paper at ICLR 2021
are introduced to define more robust and accurate deep learning architectures for supervised learning
problems (e.g. classification and regression). These studies focus on the design of neural network
architectures, and are orthogonal to our work on the modeling of sporadic time series. In (Tzen &
Raginsky, 2019a;b) the authors studied the theoretical guarantees of the optimization and inference
problems of Neural SDEs. In (Li et al., 2020), a stochastic adjoint method is proposed to efficiently
compute the gradients for neural SDEs.
In this paper, we propose a new continuous-time stochastic recurrent network called Variational
Stochastic Differential Network (VSDN) that incorporates SDEs into recurrent neural model to
effectively model the continuous-time stochastic dynamics based only on sparse or irregular obser-
vations. Taking advantage of the capacity of deep neural networks, VSDN has higher flexibility and
generalizability in modeling the nonlinear stochastic dependency from high-dimensional observa-
tions.
Compared to Neural ODEs, VSDN incorporates the latent state trajectory to capture the underlying
factors of the system dynamics. The trajectory helps to more flexibly model the data distribution
and more accurately generate the output data than Neural ODEs. Parallel to the theoretical analysis
(Tzen & Raginsky, 2019a;b) and gradient computations (Li et al., 2020), our study focuses more
on exploring the feasible variational loss and flexible recurrent architecture for the Neural SDEs to
model the sporadic data.
The contributions of this paper are three-fold:
1.	We incorporate the continuous-time variants of VAE and IWAE losses into VSDN to train
the continuous-time stochastic neural networks with latent state trajectories.
2.	We propose the efficient and flexible network architecture of VSDN which can model the
complicated stochastic process under high-dimensional sporadic data sequences.
3.	We conduct comprehensive experiments to show that VSDN outperforms state-of-the-art
deep learning methods on modeling the continuous-time dynamics and achieves remarkable
performance in the prediction and interpolation of irregular or sporadic time series.
The rest of this paper is organized as follows. In Section 2, we first present the continuous-time
variants of VAE loss, and then derive a continuous-time IWAE loss to train continuous-time state-
space models with deep neural networks. In Section 3, we propose the deep learning structures of
VSDN. Comprehensive experiments are presented in section 4 and conclusion is given in section 5.
2	Continuous-Time Variational Bayes
In this section, we first introduce the basic notations and formulate our problem. We then define the
continuous-time variants of the Variational Auto-Encoding (VAE) and Importance-Weighted Auto-
Encoding (IWAE) lower bounds to enable the efficient training of our models. Due to the page limit,
we present all deductions in Appendix A.
2.1	Basic Notations and Problem Formulation
Throughout this paper, we define Xt ∈ Rd1 as the continuous-time latent state at time t and Yn ∈
Rd2 as the nth discrete-time observed data at time tn. d1 and d2 are the dimensions of the latent state
and observation, respectively. X<t is the continuous trajectory before time t and X≤t is the path up
to time t. Yn1:n2 is the sequence of data points and Xtn :tn is the continuous-time state trajectory
from tn1 to tn2. Yt = {Yn|tn < t} is the historical observations before t and Yt = {Yn|tn ≥ t} is
the current and future observations. For simplicity, we also assume that the initial value of the latent
state is constant. The results in this paper can be easily extended to the situation that the initial states
are also random variables. Given K data sequences 也也}, i = 1,…，K, the target of our study
is to learn an accurate continuous-time generative model G that maximizes the log-likelihood:
1K
G =argmaχ K ElOgPG(y(ini).	⑴
G
2
Under review as a conference paper at ICLR 2021
For Multivariate sequential data, there exists a complicated nonlinear relationship between the ob-
served data and the unobservable latent state, which can be either the physical state of a dynamic
system or the low-dimensional manifold of data. In our study, the latent state evolves in the contin-
uous time domain and generates the observation through some transformation.
2.2	Continuous-Time Variational Inference
In order to capture the underlying stochastic process from sporadic data, we design the generative
model as a neural continuous-time state-space model, which consists of a latent Stochastic Differ-
ential Equation (SDE) and a conditional distribution of the observation. The latent SDE describes
the stochastic process of the latent states and the conditional distribution depicts the probabilistic
dependency of the current data with the latent states and historical observations:
dXt =HG(Xt,Yt;t)dt+RG(Yt;t)dWt,	(2)
PG (γnXn-ι,Xtn) = Φ(γn∣f(γ‰-ι,Xtn)),	(3)
where HG and RG are the drift and diffusion functions of the latent SDE. Wt denotes the a Wiener
process, which is also called standard Brownian motion. To integrate the information of the observed
data, HG is the function of the current state Xt and the historical observations Yt . However, RG
only uses the historical data as input. It is not beneficial to include Xt as the input of the diffusion
function, as it will inject more noise into gradients of the network parameters. A detailed example
and analysis of the noise injection problem is given in Appendix B. Φ(∙) is a parametric family of
distributions over the data and f (∙) is the function to compute the parameters of Φ. With the advance
of deep learning methods, We parameterize HG, RG and f (∙) by deep neural networks.
Continuous-Time Auto-Encoding Variational Bayes: The exact log-likelihood of the generative
model is given as
logPG (y1:n) = log
Zn
PG(X≤tn) YPG(yi|yi：n-i,XQdX≤tn
i=1
(4)
which does not have the closed-form solution in general. Therefore, G can not be directly trained by
maximizing log-likelihood. To overcome this difficulty, an inference model Q is introduced to depict
the stochastic dependency of the latent state on observed data. Similar to the generative model, Q
consists of a posterior SDE:
dXt =HQ(Xt,Yt,Yt;t)dt+RG(Yt;t)dWt,
(5)
where HQ is the posterior drift function. Different from HG, HQ also uses the future observation Yt
as the input and therefore the inference model Q induces the posterior distribution PQ(X≤tn |y1:n).
Based on Auto-Encoding Variational Bayes (Kingma & Welling, 2014), it is straightforward to
introduce a continuous-time variant of the VAE lower bound of the log-likelihood:
n
LVAE (yi:n) = - eKL(PQ||PG ) + X EPQ(Xti ) log PG 3 |yi:n-1, Xti ),
i=1
KL(PQ||PG)
1	tn
2 J0
EPQ(Xt) (HQ-HG)T[RGRTG]-1(HQ-HG) dt.
(6)
(7)
where PG (X≤tn) and PQ (X≤tn) are the probability density of the latent states induced by the prior
SDE Eq. (2) and the posterior SDE Eq. (5). KL(∙∣∣∙) denotes the KL divergence between two
distributions and β is a hyper-parameter to weight the effect of the KL terms. In this paper, we fix
β as 1.0 and LVAE is the original VAE objective (Kingma & Welling, 2014). In β-VAE (Higgins
et al., 2017; Burgess et al., 2018), it is shown that a larger β can encourage the model to learn more
efficient and disentangled representation from the data. Eq. (5) is restricted to having the same
diffusion function as Eq. (2). A feasible LVAE can not be defined to train VSDN-SDE without this
restriction, as the KL divergence of two SDEs with different diffusions will be infinite (Archambeau
et al., 2008).
The VAE objective has been widely used for discrete-time stochastic recurrent modals, such as
LFADS (Sussillo et al., 2016), VRNN (Chung et al., 2015) and SRNN (Fraccaro et al., 2016). The
3
Under review as a conference paper at ICLR 2021
major difference between these models and our work is that we incorporate a continuous-time latent
state into our model while the latent states of the discrete-time models evolve only at distinct and
separate time slots.
Continuous-Time Importance Weighted Variational Bayes: LV AE (y1:n) equals the exact log-
likelihood when PQ(X≤tn ) of the inference model is identical to the exact posterior distribution
induced by the generative model. The errors of the inference model can result in the looseness of
the VAE loss for the model training. Under the framework of Importance-Weighted Auto-Encoder
(IWAE) (Burda et al., 2016; Cremer et al., 2017), we can define a tighter evidence lower bound:
1K	n
LeKWAE (y1：n ) =Ex≤tn ,…,xKt°~PQ (x≤tn) ( log K X Wk Y PG (yi |y1：n-1, Xtj),
k=1	i=1
where the importance weights satisfy the following SDE:
dlog wk =dlog PGXttn)) = -1(HQ - HG)T[RGRT]-1(HQ - HG)dt
- (HQ -HGn)T[RG]-1dWt.
(8)
(9)
Given the variational auto-encoding lower bound LVAE(∙) and the importance weighted auto-
encoding lower bound LKWAE(∙) for the continuous-time generative model, the tightness of the
lower bounds are given by the following inequality:
logPG(y1：n) ≥LeIKW+A1E(∙) ≥LeIKWAE(∙) ≥LVAE(∙),	(10)
for any positive integer K. Consequently, LeIKW AE (∙) is infinite if the diffusions of Eq. (2) and Eq.
(5) are different. In our implementation, we notice that the training of our models by LIKW AE is
not stable, possibly due to the drawbacks of importance sampling and the Signal-To-Noise problem
(Rainforth et al., 2018). To alleviate the problem, we train our model by a convex combination of
the VAE and IWAE losses:
LKWAE (y1：n) = (I - α)LVAE (y1:n) + αLeKWAE (y1：n), α ∈ (0, 1).	(II)
With the use of reparameterization (Kingma & Welling, 2014), both LVAE(y1：n) and LIKW AE (y1：n)
are differentiable with respect to the parameters of the generative and inference models. Therefore,
they can be applied to train continuous-time stochastic models with deep learning components.
3 Variational Stochastic Differential Networks
We propose a new continuous-time stochastic recurrent network called Variational Stochastic Dif-
ferential Network (VSDN) (Figure 1). VSDN introduces the latent state to capture the underlying
unobservable factors that generate the observed data, and incorporates efficient deep learning struc-
tures to compute the components in the generative model Eq. (2) - (3) and inference model Eq.
(5).
Figure 1: Model Architectures of (a) VSDN-F (filtering); (b) VSDN-S (smoothing).
4
Under review as a conference paper at ICLR 2021
Generative Model G : Inside the generative model, the latent SDE Eq. (2) depicts the dynamics of
the latent state trajectory controlled by the historical observations Yt . Both the drift and diffusion
functions have the dependency on Yt . Therefore, we first apply a forward ODE-RNN (Rubanova
et al., 2019) to embed the information of historical data into the hidden feature h t,pre. Two feed-
forward networks are defined to compute drift and diffusion respectively. The decoder network
further computes the parameters of the conditional distribution in Eq. (3) by the concatenation of
the latent state and forward feature
HG = Ndrift ([Xt, h t,pre = ODE-RNN1(Yt; t)]),	RG = exp(Ndif f ( h t,pre)),
PG (Yt∣Yt,Xt) = Φ(Yn∣f = D([Xt, → t,pre])).	(12)
Inference Model Q: We propose two types of inference models in VSDN: a filtering model, and a
smoothing model. LVAE(y1:n) and LIKW AE (y1:n) equal the exact log-likelihood when PQ(X≤tn )
is identical to the exact posterior distribution PG (X≤tn |y1:n). The inference model must process the
the whole data sequence to compute HQ at a time. According to d-separation (Bishop, 2006), the
latent state Xt is dependent on both the historical data Yt and future observations Yt . Therefore, we
first define Q as a smoothing model by introducing a backward ODE-RNN to embed the information
of the future observations into a hidden feature ht . The drift function is computed as:
Hq = Ndrift([Xt, → t,pre + ^-t]),	1t =ODE-RNN2(Yt； t)]).	(13)
In real-world applications, it is sometimes possible to have close performance in inference with-
out processing the future observations. Besides, the future measurements are intractable in online
systems. Therefore, we also design a filtering inference model that infers the latent state from the
historical and current data. The drift of the filtering model is given as:
Ndrift ([Xt, ht,pre +
HG
ht,post])
if there is yt at t
otherwise
(14)
where h t,post is the post-observation updated feature of the forward ODE-RNN (Rubanova et al.,
2019). The filtering model does not have to include a backward RNN to process the future observa-
tions and thus its running speed is faster.
The whole architectures of VSDN with filtering Q (VSDN-F) and smoothing Q (VSDN-S) are
shown in Figure 1 (a) - (b). The inference model and the generative model share the drift network.
This strategy can force the ODE-RNNs to embed more information into the hidden features and
reduce the model complexity.
Applications: VSDN consists of a generative model and an inference model. The generative model
is an online predictive model which can recurrently predict the future values of the sequence. The
inference models can be applied to either filtering or smoothing problems of the latent states accord-
ingly. Furthermore, the smoothing inference model infers the latent state trajectory from the whole
sequence, which can be further used in Eq. (3) to synthesize missing data. Therefore, the smoothing
inference model is capable of offline interpolation. The motivation of this paper is to design an effi-
cient continuous-time stochastic recurrent model. Therefore, VSDNs only use the generative model
to recurrently predict the future values in the experiments.
Discussions: VSDN has higher flexibility and model capability than current continuous-time deep
learning models in modeling the sporadic sequences. LatentODE (Chen et al., 2018) and ODE2VAE
(Yildiz et al., 2019) encode the information of the time series into the initial values of the latent state
trajectories and neglect the variance in the latent state transition. This strategy is impractical and
inefficient in real-world applications, as it requires the initial latent states to disentangle the property
of the long sequence. Furthermore, LatentODE, ODE2VAE are offline models, as the encoder used
during training of these models can not be directly used for online prediction. In contrast, VSDN
defines a latent SDE controlled by the historical observations and recurrently integrates the informa-
tion of the sequence along the time axis. It is more efficient than the initial state embedding and is
also applicable in online prediction. GRU-ODE (De Brouwer et al., 2019), ODE-RNN (Rubanova
et al., 2019) and NCDE (Kidger et al., 2020) also utilize recurrent scheme but does not explicitly
model the stochasticity of the underlying latent state. Therefore, they are less capable than VSDN
in modeling the complicated stochastic process of the irregular data.
5
Under review as a conference paper at ICLR 2021
4 Experiments
In this section, we conduct comprehensive experiments to validate the performance of our models
and demonstrate its advantages in real-world applications. We compare the performance of VSDN
with state-of-the-art continuous-time recurrent neural networks (i.e. ODE-RNN (Rubanova et al.,
2019) and GRU-ODE (De Brouwer et al., 2019)), LatentODE (Chen et al., 2018) and LatentSDE
(Li et al., 2020).
4.1	Human Motion Activities
We first evaluate the performance of different models on the prediction and interpolation problems
for human motion capturing. For a given sequence of data points sampled at irregular time intervals,
the prediction task is defined to estimate the next observed data in the time axis, and the interpo-
lation task is defined to recover the missing parts of the whole data trajectory. In both prediction
and interpolation tasks, only the generative models of VSDNs are evaluated. The experiments are
conducted on the following datasets:
•	Human3.6M (Ionescu et al., 2014): We apply the same data pre-processing as (Martinez
et al., 2017), after which the data frame at each time is a 51-dimensional vector. The long
data sequences are further segmented by 248 frames.
•	CMU MoCap*: We follow the data pre-processing in (LiU et al., 2019). In each data
frame, human activity is represented as a 62-dimensional vector and each dimension of the
frames is normalized by global mean and standard deviation. The long data seqUences are
fUrther segmented by 300 frames.
After data pre-processing, we randomly remove half of the frames in the data seqUence as missing
data. To qUantify the model performance, we consider two evalUation metrics: one is the negative
log-likelihood (NLL) per frame; the other is the frame-level mean sqUare error (MSE) between the
groUnd-trUe and estimated valUes. The model configUrations are given in Appendix C. The model
performance is shown in Tables 1 and 2.
VSDN incorporates SDE to model the stochastic dynamics, and also applies a recUrrent strUctUre
to embed the information of the irregUlar time series into the whole latent state trajectory. With
these advances, VSDN oUtperforms the baseline models in both the prediction and interpolation
tasks. VSDN has mUch smaller negative log-likelihood, which indicates that it can better model the
Underlying stochastic process of the data. FUrthermore, VSDN trained by IWAE losses has similar
and sometimes better performance than those with VAE losses. As the latent state in the inference
model has stochastic dependency on the fUtUre observations, VSDN-S Using the smoothing model
has slightly lower NLL and is a better choice than VSDN-F Using filtering model.
Table 1: Model performance on HUman3.6M dataset
	Prediction		Interpolation	
	NLL	MSE	NLL	MSE
LatentODE	-45.34 ± 0.85	1.6421 ± 0.041	-45.31 ± 0.85	1.6477 ± 0.041
LatentSDE	-63.01 ± 1.26	0.8278 ± 0.059	-62.93 ± 1.28	0.8311 ± 0.060
GRU-ODE	-93.70 ± 2.34	0.3201 ± 0.062	-93.71 ± 2.34	0.3207 ± 0.062
ODE-RNN	-93.78 ± 1.48	0.2981 ± 0.075	-93.78 ± 1.48	0.2984 ± 0.076
VSDN-F (VAE)	-122.64 ± 2.79	0.2373 ± 0.064	-122.62 ± 2.79	0.2367 ± 0.064
VSDN-S (VAE)	-126.93 ± 3.35	0.2374 ± 0.086	-126.88 ± 3.35	0.2368 ± 0.086
VSDN-F (IWAE)	-125.55 ± 6.64	0.1751 ± 0.092	-125.51 ± 6.62	0.1746 ± 0.092
VSDN-S (IWAE)	-127.12 ± 5.19	0.1797 ± 0.073	-127.08 ± 5.17	0.1790 ± 0.073
*http://mocap.cs.cmu.edu/
6
Under review as a conference paper at ICLR 2021
Table 2: Model performance on MoCap dataset
	Prediction		Interpolation	
	NLL	MSE	NLL	MSE
LatentODE	14.99 ± 1.64	49.51 ± 0.38-	14.91 ± 1.81	49.88 ± 0.39
LatentSDE	-59.83 ± 2.13	30.11 ± 0.28	-60.13 ± 2.59	30.43 ± 0.31
GRU-ODE	-51.83 ± 0.48	32.77 ± 0.11	-51.91 ± 0.49	33.17 ± 0.12
ODE-RNN	-51.75 ± 1.16	31.81 ± 0.21	-51.82 ± 1.18	33.22 ± 0.21
VSDN-F (VAE)	-110.71 ± 3.92	20.64 ± 0.39-	-111.40 ± 3.88	20.95 ± 0.39
VSDN-S (VAE)	-114.31 ± 4.44	19.05 ± 0.59	-114.97 ± 4.40	19.35 ± 0.59
VSDN-F (IWAE)	-109.84 ± 5.32	20.47 ± 0.53	-110.54 ± 5.33	20.76 ± 0.53
VSDN-S (IWAE)	-114.57 ± 2.58	19.84 ± 0.18	-115.24 ± 2.50	20.12 ± 0.18
Visualization: We further compare different models qualitatively through the visualization of the
interpolated human skeletons in Figure 2. VSDN models are able to generate vivid skeletons that are
closer to the ground-true ones. Instead, ODE-RNN and GRU-ODE can not interpolate the postures
correctly (e.g the angles of arms in each frame are significantly different from the ground-true ones).
We also observe that the motions generated by VSDNs are smooth and closer to the real data, while
there are a large vibration in the movements generated by the baseline models. The videos of these
human motions are provided in supplementary materials.
① FmIPUnOJO NNH山 ∞
山 ao—nHO (山 <MD
Figure 2:	Visualization for human skeleton interpolation of different models.
4.2 Toy Simulation and Climate Prediction
We conduct additional experiments on two sporadic time series datasets in (De Brouwer et al., 2019):
•	Double-OUt: The Double-OU dataset consists of data sequences synthesized by a 2-
dimensional Ornstein-Uhlenbeck process, which is a classic stochastic differential equa-
tions in finance and physics.
t https://github.com/edebrouwer/gru_ode_bayes
7
Under review as a conference paper at ICLR 2021
•	USHCN*: The United State Historical Climatology Network (USHCN) dataset contains
daily measurements of 5 climate variables from the meteorological stations in United
States. In our experiment, we use the pre-processed subset of the data given in (De Brouwer
et al., 2019).
Compared with the previous experiments, the data in Double-OU and USHCN are not only sampled
at irregular times, but also have missing dimensions at each sampled frames. That is a data sequence
is sparse in both time axis and frame dimension. We evaluate the model performance in predicting
future values based on the sporadic observations.
The results are shown in Table 3. All VSDN models outperform the baseline ones. On the USHCN
dataset, VSDN-S has better NLL than VSDN-F when using either VAE or IWAE losses in the
training processes. VSDNs trained by IWAE loss also have smaller NLL than those trained by VAE
loss. However, when running on the Double-OU dataset, the training with IWAE performs slightly
worse than the training using the VAE loss. This is possibly caused by the randomness of the training
process, as Double-OU process is a very simple stochastic differential equation and all VSDNs have
the smallest errors in the prediction tasks.
Table 3: Model Performance on Sporadic Time Series
	Double-OU		USHCN	
	NLL	MSE	NLL	MSE
LatentODE	0.351 ± 0.023	0.1201 ± 0.0071-	1.319 ± 0.156	0.772 ± 0.099
LatentSDE	0.334 ± 0.010	0.1118 ± 0.0029	1.304 ± 0.083	0.748 ± 0.116
GRU-ODE	-0.997 ± 0.021	0.0080 ± 0.0004	0.940 ± 0.058	0.443 ± 0.067
ODE-RNN	-1.002 ± 0.014	0.0082 ± 0.0003	0.866 ± 0.057	0.397 ± 0.064
VSDN-F (VAE)	-1.145 ± 0.029	0.0065 ± 0.0003	0.736 ± 0.111	0.384 ± 0.060
VSDN-S (VAE)	-1.145 ± 0.013	0.0065 ± 0.0002	0.716 ± 0.113	0.390 ± 0.057
VSDN-F (IWAE)	-1.143 ± 0.027	0.0065 ± 0.0004	0.661 ± 0.096	0.370 ± 0.062
VSDN-S (IWAE)	-1.139 ± 0.019	0.0065 ± 0.0003	0.654 ± 0.084	0.381 ± 0.058
NumTraj=5
NumTraj=IO
NumTraj=15
NumTraj=20
25	30	35
Epoch
25	30	35
Epoch
25	30	35
Epoch
25	30	35
Epoch
NumTraj=5
NumTraj=IO
NumTraj=15
NumTraj=20
—I— VSDN-F(VAE)
—I— VSDN-S(VAE)
—I- VSDn-F(IWAE)
—I— VSDn-S(IWAE)
—I— VSDN-F(VAE)
—I— VSDN-S(VAE)
—I- VSDn-F(IWAE)
—I— VSDn-S(IWAE)
—I— VSDN-F(VAE)
—I— VSDN-S(VAE)
—I- VSDn-F(IWAE)
—I— VSDn-S(IWAE)
—I— VSDN-F(VAE)
—I— VSDN-S(VAE)
—I- VSDn-F(IWAE)
—I— VSDn-S(IWAE)
25	30	35
Epoch
25	30	35
Epoch
25	30	35
Epoch
25	30	35
Epoch
Figure 3:	Training processes of our models with respect to the different number of sampled latent
state trajectories. (UP: training set; Bottom: validation set)
4.3 Quantitative Studies
In order to better understand the properties of VAE and IWAE losses in training VSDNs, we conduct
comprehensive quantitative evaluation by varying the number of sampled trajectories when comput-
^ https://cdiac.ess-dive.lbl.gov/epubs/ndp/ushcn/monthly _doc.html
8
Under review as a conference paper at ICLR 2021
ing these losses. We visualize the LVAE and LIKW AE of VSDN trained for 40 epoches on the
Human3.6M dataset in Figure 3. As the VSDN-S contains both forward and backward ODE-RNNs,
it is more difficult to train than VSDN-F. The looseness of LVAE further increases the training dif-
ficulty and results in a worse lower bound of VSDN-S (VAE). Therefore, VSDN-S (VAE) requires
more epochs to converge during the training. For the other cases, we observe that the LIKW AE is
tighter than LV AE in training when the number of trajectories is small. Therefore, LIKW AE has
faster convergence in training our models. For large number of trajectories, LV AE has similar tight-
ness as LIKW AE in training set.
5 Conclusions
In this paper, we propose a continuous-time stochastic recurrent neural network called VSDN to
learn the continuous-time stochastic dynamics from irregular or even sporadic data sequence. We
provide two variants, one is VSDN-F whose inference model is a filtering model, and the other is
VSDN-S with smoothing inference model. The continuous-time variants of the VAE and IWAE
losses are incorporated to efficiently train our model. We demonstrate the effectiveness of VSDN
through evaluations studies on different datasets and tasks, and our results show that VSDN can
achieve much better performance than state-of-the-art continuous-time deep learning models. In the
future work, we will investigate along several potential directions: First, we will apply our models to
higher dimensional and more complicated data, such as videos, which are more challenging to model
yet, especially under the premise of increasing demand for producing videos in high resolution and
frame-per-second (FPS); Second, as stochastic differential equations are the base of many significant
control methodologies, we will try to further extend the capacity of our models such that they can
be used in precise control scenarios.
References
Cedric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John S. ShaWe-taylor. Varia-
tional inference for diffusion processes. In Advances in Neural Information Processing Systems
20. 2008.
Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statis-
tics). Springer-Verlag, Berlin, Heidelberg, 2006.
Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance Weighted autoencoders. ArXiv,
abs/1509.00519, 2016.
Christopher P. Burgess, Irina Higgins, Arka Pal, Lolc Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-VAE. 2018.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary dif-
ferential equations. In Advances in Neural Information Processing Systems 31, pp. 6571-6583.
2018.
Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Ben-
gio. A recurrent latent variable model for sequential data. In Advances in Neural Information
Processing Systems 28, pp. 2980-2988. 2015.
Chris Cremer, Quaid Morris, and David Duvenaud. Reinterpreting importance-Weighted autoen-
coders. In International Conference on Learning Representations (ICLR) - Workshop Track, 2017.
EdWard De BrouWer, Jaak Simm, Adam Arany, and Yves Moreau. GRU-ODE-Bayes: Continuous
modeling of sporadically-observed time series. In Advances in Neural Information Processing
Systems 32, pp. 7379-7390. 2019.
Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. FMA: A dataset for
music analysis. In The 18th International Society for Music Information Retrieval Conference
(ISMIR), 2017.
9
Under review as a conference paper at ICLR 2021
Marco Fraccaro, S0ren Kaae S0 nderby, Ulrich Paquet, and Ole Winther. Sequential neural models
with stochastic layers. In Advances in Neural Information Processing Systems 29, pp. 2199-2207.
Curran Associates, Inc., 2016.
Jung-Su Ha, Young-Jin Park, Hyeok-Joo Chae, Soon-Seo Park, and Han-Lim Choi. Adaptive path-
integral autoencoders: Representation learning and planning for dynamical systems. In Advances
in Neural Information Processing Systems 31, pp. 8927-8938. 2018.
Pashupati Hegde, Markus Heinonen, Harri Lahdesmaki, and Samuel Kaski. Deep learning with
differential gaussian process flows. In 22nd International Conference on Artificial Intelligence
and Statistics, volume 89, pp. 1812-1821, Apr 2019.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning basic visual concepts with a con-
strained variational framework. In International Conference on Learning Representations (ICLR),
2017.
Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale
datasets and predictive methods for 3d human sensing in natural environments. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 36(7):1325-1339, jul 2014.
Junteng Jia and Austin R Benson. Neural jump stochastic differential equations. In Advances in
Neural Information Processing Systems 32, pp. 9843-9854. 2019.
M.j0rgensen, M. Deisenroth, and Hugh Salimbeni. Stochastic differential equations with variational
wishart diffusions. ArXiv, abs/2006.14895, 2020.
Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural Controlled Differential Equa-
tions for Irregular Time Series. ArXiv, abs/2005.08926, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In International Conference
on Learning Representations (ICLR), 2014.
Lingkai Kong, Jimeng Sun, and Chao Zhang. SDE-Net: Equipping deep neural network with un-
certainty estimates. In Proceedings of the 37th International Conference on Machine Learning,
2020.
Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Duvenaud. Scalable gradients
for stochastic differential equations. In 23rd International Conference on Artificial Intelligence
and Statistics, pp. 3870-3882, Aug 2020.
Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, and Cho-Jui Hsieh. How does noise
help robustness? explanation and exploration under the neural sde framework. In IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
Yingru Liu, Dongliang Xie, and Xin Wang. Generalized boltzmann machine with deep neural struc-
ture. In The 22nd International Conference on Artificial Intelligence and Statistics (AISTATS),
volume 89, pp. 926-934, Apr 2019.
Jakob H Macke, Lars Buesing, John P Cunningham, Byron M Yu, Krishna V Shenoy, and Maneesh
Sahani. Empirical models of spiking in neural populations. In Advances in Neural Information
Processing Systems 24, pp. 1350-1358. 2011.
Julieta Martinez, Rayat Hossain, Javier Romero, and James J. Little. A simple yet effective base-
line for 3d human pose estimation. In IEEE/CVF International Conference on Computer Vision
(ICCV), 2017.
Stefano Peluchetti and Stefano Favaro. Infinitely deep neural networks as diffusion processes. In
23rd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1126-
1136, Aug 2020.
Tom Rainforth, Adam R. Kosiorek, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. In In Proceedings of the
35th International Conference on Machine Learning, pp. 4274-4282, 2018.
10
Under review as a conference paper at ICLR 2021
Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for
irregularly-sampled time series. In Advances in Neural Information Processing Systems 32, pp.
5321-5331.2019.
Tom Ryder, Andrew Golightly, A. Stephen McGough, and Dennis Prangle. Black-box variational
inference for stochastic differential equations. In Proceedings of the 35th International Confer-
ence on Machine Learning, pp. 4423-4432, 2018.
Simo Sarkka, JoUni Hartikainen, Isambi Sailon Mbalawata, and Heikki Haario. Posterior inference
on parameters of stochastic differential equations via non-linear gaussian filtering and adaptive
MCMC. Statistics and Computing, 25(2):427-437, Mar 2015.
David Sussillo, Rafal Jozefowicz, L. F. Abbott, and Chethan Pandarinath. LFADS - latent factor
analysis via dynamical systems. ArXiv, abs/1608.06315, 2016.
Belinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative
models with latent diffusions. In 32nd Annual Conference on Learning Theory, volume 99, pp.
3084-3114, Jun 2019a.
Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian
models in the diffusion limit. ArXiv, abs/1905.09883, 2019b.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.
In Advances in Neural Information Processing Systems 29, pp. 613-621. 2016.
Bao Wang, Zuoqiang Shi, and Stanley Osher. Resnets ensemble via the feynman-kac formalism to
improve natural and robust accuracies. In Advances in Neural Information Processing Systems
32, pp. 1657-1667. 2019.
Cagatay Yildiz, Markus Heinonen, and Harri Lahdesmaki. Ode2vae: Deep generative second order
odes with bayesian neural networks. In Advances in Neural Information Processing Systems 32,
pp. 13412-13421. 2019.
Byron M. Yu, John P. Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V. Shenoy, and
Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of
neural population activity. Journal of Neurophysiology, 102(1):614-635, 2009a.
Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen I. Ryu, Krishna V Shenoy, and
Maneesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of
neural population activity. In Advances in Neural Information Processing Systems 21, pp. 1881-
1888. 2009b.
11
Under review as a conference paper at ICLR 2021
A Deductions of Continuous-Time Evidence Lower B ound
A. 1 Preliminaries of Stochastic Differential Equations
During the model design and implementation, We will use the Euler-Maruyama method to discretize
the stochastic differential equation. The details are given as follows.
Lemma 1 (Discretization of SDE). Fora SDE dX = H(X, t)dt + R(t)dW, we can discretize it as
Xk+1 = Xk + H (Xk ,t )∆t + R(tk )√∆tε,	(15)
where ε 〜N(0,1), tk = k∆t and ∆t is the sampling interval. Eq. (15) converges to the original
SDE when ∆t → 0.
Lemma 2. The state Xk+1 in Eq. (15) follows the conditional Gaussian distribution
P (Xk+1 |Xk) = N(Xk + H (Xk)∆t, ∆tR(Xk)T R(Xk)). The joint distribution of the sate se-
quence X1:K of Eq. (15) is given by
K-1
P(Xi：K |Xo) H exp ( - 0.5 X (Xk+ι - mk )T Σ-1(Xk+ι - mk)),	(16)
k=0
where mk = Xk + H (Xk)∆t and Σk = ∆tR(Xk)TR(Xk).
A.2 DEVIATION OF LVAE
The proof is similar as the evidence lower bound in (Archambeau et al., 2008). By applying Jensen’s
inequality, we can obtain that:
logPG (y1:n)
Zn
PG(X≤tn) Y PG(yi|y1:n-1,
i=1
Xti)dX≤tn
ICtT P G' X≤t	∖PG (X≤tn)Πn=1 PG (yi|yi：n-1,Xti ) ,y
= lθg∕ PQ(X≤tn ) ----------PQXD----------------dX≤tn
> [P (V M(WPG (X ≤tn ) Qn=1 PG (yi|y1：n-1，Xti ) ,y
≥J PQ(X≤tn )lθg -----------PQIXD---------------dX≤tn
=/ PQ(X≤tn )log PGm) dXti + / PQ(X≤tn )log YY PG (yi∣yn,Xti)dX≤tn
n
- KL PQ||PG	+ XEPQ(Xti) log PG	(yi|y1:n-1,
i=1
Xti).
The next step is to derive the KL divergence term for the prior and inference SDEs. After discretiza-
tion into K points via Lemma 1, the KL divergence of the two SDEs in VSDN-SDE will be:
kl(PqI∣Pg )= Z Pq(Xi：k )iog PQ(XIK) dXi：K
PG (X1:K)
j KX p (X ʌi PQ(Xk+1|Xk) .χ KX I P (Y Ilco PQ(Xk+1|Xk)〃Y
=J ⅛ PQ(XIK)lθg PG(Xk+ι∣Xk) dX1:K= U PQ(XLK)lθg PG(Xk+1∣Xk) dX1:K
=X Z PQ(Xk+2:K Xk+1)PQ(Xk+1∣Xk )PQ(XI:k )log PQ(X k+1∣∣Xk) dX1：K
k=0	G	k+1	k
K-1
XZ
k=0
PQ(Xk+1|Xk)PQ(Xk) log
PQ(Xk+ι∣Xk)
PG (Xk+ι∣Xk)
dXkdXk+1
K-1
XZ
k=0
PQ(Xk) ∙ KL(PQ(Xk+i|Xk)||Pg(Xk+ι∣Xk))dXk
K-1
X EXk〜PQ(Xk)KL(PQ(Xk+1|Xk)||PG(Xk+1∣Xk)),
k=0
12
Under review as a conference paper at ICLR 2021
Where PQ(Xk) is the marginal distribution of Xk in the inference SDE. According to lemma 2 and
the KL divergence of tWo Gaussian distribution, We further have
KL (PQ(Xk+1|Xk )∖∖pG (Xk+l∖Xk)) = 2 (tr(∑-,G ∑k,Q) + (mk,G - mk,Q)T ∑-,G (mk,G - mk,Q)
+ log
det∑k,G
det ∑k,Q
-d
1	RGRT
=2 (tr ((RGrg )	RQRQ)	+δ%(HG	-	HQ)	(RGrg )	(HG -	HQ)	+ log RQRT	- d)
Where d is the dimension of Xk+1 . When We restrict RG = RQ, We have
1 K-1
KL(Pq∖∖Pg) =5 E EXk〜PQ(Xk)(HG- Hq)τ(RGRT)-1 (HG- HqM
k=0
When We set ∆t → 0, the discretized SDEs converge to the original SDEs and KL(PQ∖∖PG) con-
verges to:
1	tn
kl(pq\\pg)=2，EPQ(Xt)
(HQ-HG)T[RGRGT]-1(HQ-HG) dt.
The expectation operator is removed as HG, HQ and RG are independent With Xt .
If RG does not equal to RQ, We have
1	K-1	const
KL(Pq∖∖Pg) =5 Rmn X EXk〜PQ(Xk) ((HG- HQ)T(RGRT)-1(Hg - Hq) + ~δ-)∆t,
2	∆t→0	t
k=0
=+∞
A.3 DEVIATION OF LIWAE
Given Xk+1 = Xk + HQ∆t + RG√∆tε = mk,Q + RG √∆tε, We have:
PG (x≤tn)
log Wig pQ(χ≤tn)=
1 K-1
=2 X -(Xk+ι -
PG (Xk+1 |Xk)
工 log PQ(XknXk)
mk,G)T ∑k-,1G (Xk+1 - mk,G) + (Xk+1 - mk,Q)T ∑k-,1G (Xk+1 - mk,Q)
k=0
1 K-1	T	.
=2)： — ((HQ	—	HG)∆t +	RG√∆tε)	[∆tRgRT]	1	((Hq	—	HG)∆t +	RG√∆tε
k=0
+ (RG√∆ε) [∆tRgRT]-1(Rg√∆ε)
1 KT
=2 E -(HQ - HG)T[RGRT]-1(HQ - HGDt - 2(HQ - hg)T[RG]-1 √∆tε
k=0
Let ∆t → 0, We have
log W =2 / -(Hq - Hg)T[RgRT]-1(Hq - Hg)dt - /(Hq - HG)T[Rg]-1dWt,
Which is equivalent to
dlogW =- (HQ -HG)T[2RGRTG]-1(HQ - HG)dt - (HQ -HG)T[RG]-1dWt.
(17)
(18)
(19)
B ILLUSTRATION OF THE NOISE INJECTION OF R(Xt)
In the section, we give an example to illustrate the noise injection problem when we include Xt as
the input for the diffusion function R(Xt) in a Neural SDE. For simplicity, we consider the scalar
case (i.e. Xt ∈ R).
13
Under review as a conference paper at ICLR 2021
B.1 Case A: RIS independent of Xt
Consider the following neural SDE:
dXt =Hφ(Xt; t)dt + Rθ(t)dWt,	(20)
where Hφ and Rθ are neural networks. φ denotes the parameters of the drift network and θ denotes
the parameters of the diffusion network.
X&t	X2∆t	X3∆t
L
Figure 4: A example to show the noise injection problem of R(Xt).
Now consider the following example (shown in Figure 4) that we have to compute the gradient of
the loss at t = 3∆t with respect to the network parameters, where the neural SDE is discretized by
Euler-Maruyama method:
X∆t =X0 + Hφ(X0; 0)∆t + Rθ(0)√∆tε1,	(21)
X2∆t =X∆t + Hφ(X△£ ∆t)∆t + rΘ(∆t)√∆tε2,	(22)
X3∆t =X2∆t + Hφ(X2∆t 2∆t)∆t + Rθ(2∆t)√∆tε3,	(23)
where ɛn 〜N(0,1). It is straight forward to prove the following lemma. For notation simplicity,
we define Hφ(n) = Hφ(X(n-1)∆t; (n — 1)∆t) and Rθ(n) = Rθ((n - 1)∆t).
Lemma 3. Eqs. (21) - (23)follows thefollowing relationship ofthe gradients:
∂Xn∆t	= 1 l ∆t ∂Hφ(n)
aX(n-1)∆t	aX(n-1)∆t
(24)
Therefore, the gradients of the parameters in the drift and diffusion functions can be given by:
∂L= ∂L ∂X3∆t ∂L ∂X3∆t ∂X2∆t ∂L ∂X3∆t ∂X2∆t ∂X∆t
.=∂X3∆t ∂φ + ∂X3∆t ∂X2∆t ∂φ + ∂X3∆t ∂X2∆t ∂X∆t ~∂φ~
∂L ∂Hφ(3)λ + _d£_ h ∂Hφ(3) i ∂Hφ ⑵
∂X3∆t ∂φ	+ ∂X3∆J + ∂X2∆t J ∂φ
∂L	∂Hφ(3)
+ 瓯∆t [1 + m1x∆^ [1 + 加
∂Hφ⑵i ∂Hφ(1)
∂X∆t J
∂φ
∆t.
(25)
and
∂L _ ∂L ∂X3∆t l ∂L ∂X3∆t ∂X2∆t ι	∂L ∂X3∆t ∂X2∆t ∂X∆t
————____________ +______——--------- +	——--——______________
∂θ ∂X3∆t	∂θ + ∂X3∆t	∂X2∆t	∂θ +	∂X3∆t	∂X2∆t	∂X∆t	∂θ
∂L ∂Rθ⑶.入 l	∂L r11 ʌaHφ⑶i ∂Rθ⑵K
∂X3∆t F V^t'3 + Ft【1 + 加贰痴 丽.△-
∂L	∂Hφ(3)
+ ∂X3∆t [1 + 加五/][1 + 加
∂Hφ(2) i ∂Rθ (1)
∂X∆/
∂θ
√∆tει.
(26)
According to Eq. (25) and Eq. (26), the gradient of φ of the drift network is deterministic except the
weight of first hidden layer and the gradient of θ of the diffusion network is obstructed by Gaussian
noise terms √z∆tει, √∆⅛2 and √∆tε3.
14
Under review as a conference paper at ICLR 2021
B.2 CASE B: R USES Xt AS INPUT
Now we consider the case when the diffusion network R also use Xt as input. Eq. (24) will change
to the following equation:
∂Xn∆t	= 1+ At ∂Hφ(n) + √λtε	∂Rθ(n)
∂X(n-1)∆t= +	∂X(n-i)∆t + V n ∂X(n-i)∆t
(27)
Inserting Eq. (27) into Eq. (25) and Eq. (26), we have
∂L = ∂L ∂X3∆t ∂L ∂X3∆t ∂X2∆t ∂L ∂X3∆t ∂X2∆t ∂X∆t
∂φ = ∂X3∆t ∂φ + ∂X3∆t ∂X2∆t ∂φ + ∂X3∆t ∂X2∆t ∂X∆t ~W
a dH詈 At + Q h1 + AtdHM + H3Mi 号∆
∂X3∆t	∂φ	∂X3∆t	∂ X2∆t	∂X2∆t	∂φ
+ 六 h1 + Atk) + R3 Mih1 + At* + RQ 辛i d¾*At.
∂X3∆t	∂X2∆t	∂ X2∆t	∂X∆t	∂X∆t	∂φ
(28)
and
∂L= ∂L ∂X3∆t ∂L ∂X3∆t ∂X2∆t ∂L ∂X3∆t ∂X2∆t ∂X∆t
丽=∂X3∆t ∂θ + ∂X3∆t ∂X2∆t ∂θ + ∂X3∆t ∂X2∆t ∂X∆t 汽厂
∂L ∂Rθ (3)
∂X3∆t ∂θ
√Atε3 +
恁 h1 + At∂≡
+ √Atε3
∂Rθ (3) i ∂Rθ (2)
^Xzd ^θ~
√Atε2
+ - h1 + AtdH四 + √Atε3 Mih1 + At* + Q 辛i 号R1.
∂X3∆t	∂X2∆t	∂ X2∆t	∂X∆t	∂X∆t	∂θ
(29)
According to Eq. (28), the gradient of φ is now also corrupted by noise terms (i.e. √A⅛3 and
Atε2ε3), while in previous case it is deterministic. What’s worse, more noise terms are added into
the gradient of θ. When we train our models in long data sequence, these injected noise terms will
cause a large variance of the parameters’ gradients. Therefore, we can conclude that introducing Xt
into the diffusion function is not beneficial.
C Model Configuration
C.1 Human Motion Activities
For all the models, the feed-forward network contains one hidden layer with 256 Relu units. At is set
as 0.25. The dimension of hidden features of ODE-RNN and GRU-ODE is 512 and the dimension of
latent states is 128. A single-layer feed-forward network with 128 Relu units is defined to compute
the initial states of the latent state. For LatentSDE, the posterior initial state is computed by using
the encoding feature of a backward ODE-RNN. The number of latent state trajectories generated to
compute VAE and IWAE losses is 5.
All models are trained by Adam optimizer with learning rate 0.0001 and weight-decay 0.0005. The
batch size is 64. Early stopping with 10 epoch tolerance is applied.
C.2 Toy Simulation and Climate Prediction
For all the models, the feed-forward network contains one hidden layer with 25 Relu units. At is
set as 0.1 for USHCN and 0.01 for Double-OU. The dimension of hidden features of ODE-RNN
and GRU-ODE is 15 and the dimension of latent states is 15 as well. A single-layer feed-forward
network with 128 Relu units is defined to compute the initial states of the latent state. The number
of latent state trajectories generated to compute VAE and IWAE losses is 5.
All models are trained by Adam optimizer with learning rate 0.0001 and weight-decay 0.0001. The
batch size is 500 for USHCN and 250 for Double-OU. Early stopping with 25 epoch tolerance is
applied.
15
Under review as a conference paper at ICLR 2021
C.3 Spectrogram Modeling
To further evaluate the performance of our model in high-dimensional data, we conduct a brief ex-
periment on the spectrogram data extracted from the FMA dataset (Defferrard et al., 2017), which is
a large collection of music and songs. We transform the first 500 songs in FMA-small into spectro-
gram and then split the spectrogram into segments. Each segment has 100 frames and each frame is
1025 dimensional. Each dimension of the data frame corresponds to a specific frequency component
of STFT. The definitions of prediction and interpolation tasks are same as those in Section 4.1.
Table 4: Model performance on Spectrogram dataset
	Prediction		Interpolation	
	NLL	MSE	NLL	MSE
LatentODE	-0.1356	~0.066-	-0.1369	0.066
LatentSDE	-0.1403	0.065	-0.1291	0.066
GRU-ODE	-0.1610	0.058	-0.1603	0.059
ODE-RNN	-0.1677	0.058	-0.1683	0.058
VSDN-F (VAE)	-0.1969	~0.059-	-0.1947	0.059
VSDN-S (VAE)	-0.1994	0.059	-0.1981	0.059
VSDN-F (IWAE)	-0.1931	0.059	-0.1935	0.059
VSDN-S (IWAE)	-0.2020	0.059	-0.2017	0.059
For all the models, the feed-forward network contains one hidden layer with 256 Relu units. The
dimension of hidden features of ODE-RNN and GRU-ODE is 256 and the dimension of latent states
is 64. A single-layer feed-forward network with 128 Relu units is defined to compute the initial
states of the latent state. For LatentSDE, the posterior initial state is computed by using the encoding
feature of a backward ODE-RNN. The number of latent state trajectories generated to compute VAE
and IWAE losses is 5. The batch size is 32.
The NLL and MSE (per dim) are shown in Table 4. Our model has similar mean square errors with
baselines but has much better NLL, which indicates that our model can better estimate the stochastic
process of the data.
16