Under review as a conference paper at ICLR 2021
Deep Jump Q-Evaluation for Offline Policy
Evaluation in Continuous Action Space
Anonymous authors
Paper under double-blind review
Ab stract
We consider off-policy evaluation (OPE) in continuous action domains, such as
dynamic pricing and personalized dose finding. In OPE, one aims to learn the
value under a new policy using historical data generated by a different behavior
policy. Most existing works on OPE focus on discrete action domains. To handle
continuous action space, we develop a brand-new deep jump Q-evaluation method
for OPE. The key ingredient of our method lies in adaptively discretizing the ac-
tion space using deep jump Q-learning. This allows us to apply existing OPE
methods in discrete domains to handle continuous actions. Our method is further
justified by theoretical results, synthetic and real datasets.
1	Introduction
Individualization proposes to leverage omni-channel data to meet individual needs. Individualized
decision making plays a vital role in a wide variety of applications. Examples include customized
pricing strategy in economics (Qiang & Bayati, 2016; Turvey, 2017), individualized treatment
regime in medicine (Chakraborty, 2013; Collins & Varmus, 2015), personalized recommendation
system in marketing (McInerney et al., 2018; Fong et al., 2018), etc. Prior to adopting any decision
rule in practice, it is crucial to know the impact of implementing such a policy. In many applications,
it is risky to run a policy online to estimate its value (see, e.g., Li et al., 2011). Off-policy evaluation
(OPE) thus attracts a lot of attention by learning the policy value offline using logged historical data.
Despite the popularity of developing OPE methods with a finite set of actions (See e.g., Dudlk et al.,
2011; 2014; Swaminathan et al., 2017; Wang et al., 2017), less attention has been paid to continuous
action domains, such as dynamic pricing (den Boer & Keskin, 2020) and personalized dose finding
(Chen et al., 2016). Recently, a few OPE methods have been proposed to handle continuous actions
(Kallus & Zhou, 2018; Sondhi et al., 2020; Colangelo & Lee, 2020). All these methods rely on
the use of a kernel function to extend the inverse probability weighting (IPW) or doubly robust
(DR) approaches developed in discrete action domains. They suffer from three limitations. First, the
validity of these methods requires the conditional mean of the reward given the feature-action pair to
be a smooth function over the action space. This assumption could be violated in applications such
as dynamic pricing, where the expected demand for a product has jump discontinuities as a function
of the charged price (den Boer & Keskin, 2020). Second, the value estimator could be sensitive to
the choice of the bandwidth parameter in the kernel function. It remains challenging to select this
hyperparameter. Kallus & Zhou (2018) proposed to tune this parameter by minimizing the mean
squared error of the resulting value estimator. However, their method is extremely computationally
intensive in moderate or high-dimensional feature space; see Section 5 for details. Third, these
kernel-based methods typically use a single bandwidth parameter. This is sub-optimal in cases
where the second-order derivative of the conditional mean function has an abrupt change in the
action space; see the toy example in Section 3.1 for details.
To address these limitations, we develop a deep jump Q-evaluation (DJQE) method by integrating
multi-scale change point detection (see e.g., Fryzlewicz, 2014), deep learning (LeCun et al., 2015)
and OPE in discrete action domains. The key ingredient of our method lies in adaptively discretizing
the action space using deep jump Q-learning. This allows us to apply IPW or DR methods to handle
continuous actions. It is worth mentioning that our method does not require kernel bandwidth selec-
tion. Theoretically, we show it allows the conditional mean to be either a continuous or piecewise
function of the action (Theorems 1 and 2) and converges faster than kernel-based OPE (Theorem 3).
Empirically, we show it outperforms state-of-the-art OPE methods in synthetic and real datasets.
1
Under review as a conference paper at ICLR 2021
2	Preliminaries
We first formulate the OPE problem. We next discuss the kernel-based OPE methods and multi-scale
change point detection, since our proposal is closely related to them.
2.1	Off-policy evaluation
The observed datasets can be summarized into {(Xi , Ai, Yi)}1≤i≤n where Oi = (Xi , Ai, Yi) de-
notes the feature-action-reward triplet for the ith subject and n denotes the total sample size. We
assume these data triplets are independent copies of some population variables (X, A, Y ). Let
X and A denote the feature and action space, respectively. We focus on the setting where A
is one-dimensional, as in dynamic pricing and personalized dose finding. A deterministic policy
π : X → A determines the action to be assigned given the observed feature. We use b to denote
the behavior policy that generates the observed data. Specifically, b(∙∣χ) denotes the probability
density or mass function of A given X = x, depending on whether A is continuous or not. Define
the expected reward function conditional on the feature-action pair as
Q(x, a) =E{Y|X =x,A= a}.
We refer to this function as the Q-function, to be consistent with the literature on developing indi-
vidualized treatment regime (Murphy, 2003).
As standard in the OPE and the causal inference literature (see e.g., Chen et al., 2016), we assume the
stable unit treatment value assumption (SUTVA), no unmeasured confounders assumption, and the
positivity assumption are satisfied. These assumptions guarantee that a policy’s value is estimable
from the observed data. Specifically, for a given target policy π, its value can be represented by
V (π) = E{Q(X, π(X))}.
The goal of the OPE is to learn the value V (π) based on the observed data.
2.2	Kernel-Based OPE
For discrete action, Zhang et al. (2012) and DUdIk et al. (2011) proposed a DR estimator of V(∏) by
nn
n X ψ(Oi,∏,Q, b) = 1X
i=1
i=1
Q …))+I⅛≡ {Yi- Qi)}
(1)
where I denotes the indicator fUnction, Q and b denote some estimators for the Q-fUnction and
the behavior policy. The second term b-1(Ai|Xi)I(Ai = π(Xi)){Yi - Q(Xi, π(Xi))} inside the
bracket corresponds to an aUgmentation term. Its expectation eqUals zero when Q = Q. The pUrpose
of adding this term is to offer additional protection against potential model misspecification of the
Q-fUnction. SUch an estimator is doUbly-robUst in the sense that its consistency relies on either Q or
b to be correctly specified. By setting Q = 0, eqUation 1 is redUced to the IPW estimator.
In continUoUs action domains, the indicator fUnction I(Ai = π(Xi)) eqUals zero almost sUrely.
Consequently, naively applying equation 1 yields the plug-in estimator Pn= ι Q(Xi, ∏(Xi))∕n. To
address this concern, the kernel-based OPE proposed to replace the indicator fUnction in eqUation 1
with a kernel function K{(Ai - ∏(Xi))∕h} with some bandwidth parameter h, i.e.,
1XΨh(θi,∏,Q,b) =1X Q(Xi,∏(Xi)) + K{(Ai二∏(Xi)"h}{Y -Q(Xi,∏(Xi))}
n i=1	n i=1	bb(Ai|Xi)
The bandwidth h represents a trade-off. The variance of the resulting value estimator decays with
h. Yet, its bias increases with h. More specifically, it follows from Theorem 1 of Kallus & Zhou
(2018) that the leading term of the bias is equal to
h2 ∕u2K (U)du E ∂ ∂2q(x, a
2	[	∂α2
a=π(X)
(2)
To ensure the term in 2 decays to zero as h goes to 0, it requires the expected second derivative of the
Q-function to exist, and thus Q(x, a) needs to be a smooth function of a. However, as commented
in the introduction, this assumption could be violated in applications such as dynamic pricing.
2
Under review as a conference paper at ICLR 2021
Table 1: The bias and the standard deviation (in parentheses) of the estimated values for V (1)
and V⑵，using DJQE and kernel-based methods. n = 100, X, A 〜Unif[0,1], Y|X, A 〜
N{Q(X, A), 1}. The target policy is given by π(x) = x.
Method	DJQE	Kernel (small h, h = 0.8)	Kernel (large h, h = 2)
V(1) (π)	0.34 (0.13)	0.65(0.17)	一	0.33 (0.09)
V(2)(π)	0.11 (0.33)	0.03 (0.32)	一	1.21 (0.12)
Oracle Q-function
Figure 1: Left panel: example of piece-wise constant function in change point literature. Middle
panel: the oracle Q-function on the feature-action space. Right Panel: the green curve presents the
oracle Q-function Q(x, π(x)) under policy π(x) = x; and the red curve is the fitted mean value by
DJQE and the pink dash line corresponds to the 95% confidence bound.
(n x IC{Y(Ti-I+1)："]+YK
2.3 Multi-Scale Change Point Detection
The change point analysis considers an ordered sequence of data, Y1:n = {Y1, ∙ ∙ ∙ ,Yn}, with un-
known change point locations, T = {τ1, ∙ ∙ ∙ ,τK} for some unknown integer K. Here, Ti is an
integer between 1 and n - 1 inclusive, and satisfies τi < τj for i < j . These change points split the
data into K+1 segment. Within each segments, the expected response is a constant function (see the
left panel of Figure 1 for details). A number of methods have been proposed on estimating change
points (see for example, Boysen et al., 2009; Killick et al., 2012; Frick et al., 2014; Fryzlewicz,
2014, and the references therein), by minimizing a penalized objective function:
arg min
τ,K
where C is a cost function that measures the goodness-of-the-fit of the constant function within each
segment and γK penalizes the number of change points with some regularization parameter γ . We
remark that all the above cited works focused on models without features. Our proposal goes beyond
these works in that we consider models with features and use deep neural networks (DNN) to capture
the complex relationship between the response and features.
3 Deep Jump Q-Evaluation
In section 3.1, we use a toy example to demonstrate the limitation of kernel-based methods. We
present the main idea of our algorithm in Section 3.2. Details are given in Section 3.3.
3.1	Toy Example
As discussed in the introduction, existing kernel-based OPE methods use a single bandwidth to
construct the value estimator. Ideally, the bandwidth h in the kernel K{(Ai - π(Xi))∕h} shall vary
with π(Xi) to improve the accuracy of the value estimator. To elaborate this, consider the Q-function
Q(x, a) = 10 max{a2 - 0.25, 0} log(x + 2) for any x, a ∈ [0, 1]. By definition, the Q-function is
smooth over the entire feature-action space. However, it has different “patterns” when the action
belongs to different intervals. Specifically, for a ∈ [0, 0.5], Q(x, a) is constant as a function of a.
For a ∈ (0.5, 1], Q(x, a) depends quadratically in a. See the middle panel of Figure 1 for details.
Consider the target policy π(x) = x. We decompose the value V (π) into V (1)(π) + V (2) (π) where
V (1) (π) =EQ(X,π(X))I(π(X) ≤ 0.5) and V (2) (π) =EQ(X,π(X))I(π(X) > 0.5).
Similarly, denote the corresponding kernel-based value estimators by
3
Under review as a conference paper at ICLR 2021
nn
芸1)(∏) = - XΨh(Oi,∏,Q,b)I(∏(Xi) ≤ 0.5) and 芸2)(π) = - Xψh(Oi,π,Q,b)I(π(Xi) > 0.5).
n i=1	n i=1
Since Q(x,a) is a constant function of a ∈ [0,0.5], its second-order derivative ∂2Q(x, a)∕∂a2
equals zero. In view of2, when π(x) ≤ 0.5, the bias of Vbh(1)(π) will be small even with a sufficiently
(1)
large h. As such, a large h is preferred to reduce the variance ofVh (π). When π(x) > 0.5, a small
(2)
h is preferred to reduce the bias of Vh (π). See Table 1 for details where we report the bias and
standard deviation of Vbh(1)(π) and Vbh(2)(π) with two different bandwidths.
Due to the use ofa single bandwidth, the kernel-based estimator suffers from either a large bias or a
large variance. To overcome this limitation, we propose to adaptively discretize the action space into
a union of disjoint intervals such that within each interval I, the Q-function {Q(x, a) : a ∈ I} can
be well-approximated by some function QI (x) that is constant in a ∈ I. Based on the discretization,
one can apply IPW orDR to evaluate the value. The advantage of adaptive discretization is illustrated
in the right panel of Figure 1. When a ≤ 0.5, the Q-function is constant in a. It is likely that our
procedure will not further split the interval [0, 0.5]. Consequently, the corresponding DR estimator
for V (1) (π) will not suffer from large variance. When a > 0.5, our procedure will split (0.5, -]
into a series of sub-intervals, approximating Q by a step function. This guarantees the resulting DR
estimator for V (2) (π) will not suffer from large bias. Consequently, the proposed value estimator
achieves a smaller mean squared error than kernel-based estimators. See Table 1 for details.
3.2	The Main Idea
For simplicity, we set the action space A = [0, 1]. From now on, we focus on a subset of intervals
in [0, 1]. By interval we always refer to those of the form [c, d) for some 0 ≤ c < d < 1, or
[c, 1] for some 0 ≤ c < 1, denoted as I. A discretization D for A is defined as a collection of
mutually disjoint intervals that covers A. Let |D| denote the number of intervals in D and |I| denote
the length of the interval I. We aim to identify an “optimal” discretization Db such that for each
interval I ∈ Db, Q(x, a) is approximately a constant function of a ∈ I. The number of intervals in
D represents a trade-off. If |D| is too large, then D will contain many short intervals, the resulting
IPW or DR estimator might suffer from large variance. Yet, a smaller value of |Db | might result in a
large bias. Our proposed method adaptively determines D and its size |D| as illustrated below.
To begin with, we cut the entire action space A into m initial intervals: [0, 1/m),[1/m, 2/m), . . . ,
[(m - 1)/m, 1]. The number m shall be sufficiently large such that the Q-function can be well-
approximated by a piecewise function on these intervals. In practice, we recommend to set the
initial number of intervals m to be proportional to the sample size n. Note the set of these initial
intervals is not the final partition D that we recommend, but only serve as the initial candidate
intervals. We next adaptively combine some of these initial intervals to form the final partition Db .
As shown in our numerical studies (see Table 5 in Appendix B for more details), the size of the final
partition |Db | is usually much less than m.
More specifically, denote by B(m) as the set of discretizations D such that the end-points of each
interval I ∈ D lie on the grid {j/m : j = 0,1, ∙∙∙ ,m}. We associate to each partition D ∈ B(m)
a collection of functions {QI}I∈D. These functions depend only on features, not the action. They
are used to produce a piecewise approximation of the Q-function such that Q(a, •) ≈ II(a ∈
I)QI (•). We model these QI using deep neural networks, to capture the complex dependence
between the response and features. When the Q-function is well-approximated, we expect the least
square loss Pi∈d [Pn=ι I(Ai ∈ I){Yi - QI(Xi)}2] , will be small.
Consequently, Db can be estimated by solving
(D, {qbI : I ∈ D}) =	arg min
(D∈B(m),{Qι 三Q∙∙I三D})
X I(Ai ∈I){Yi - QI (Xi) }2 + Y∣D∣), (3)
^

n
for some regularization parameter γ and DNN class Q. Here, the penalty term γ |D| in equation 3
controls the total number of intervals in Db, as in multi-scale change point detection. A large γ
results in few intervals in Db and a potential large bias of the value estimator, whereas a small γ
4
Under review as a conference paper at ICLR 2021
could procedure a large number of intervals in Db , leading to a noisy value estimator. In practice, we
use cross-validation to select the regularization parameter γ that minimizes the mean square error of
the fitted Q-function. We refer to this step as deep jump Q-learning. Details of this step are given in
the next section. Given Db, one can apply IPW or DR (see equation 1) to derive the value estimates.
To further reduce the bias of the value estimator, we employ a data splitting and cross-fitting strategy,
which is commonly used in statistics (Romano & DiCiccio, 2019). That is, we use different subsets
of data samples to learn the discretization Db and to construct the value estimator. A pseudocode
summarizing our algorithm is given in Algorithm 1 in Appendix A. We present the details below.
3.3	The Complete Algorithm
We present the details for DJQE in this section. It consists of three steps: data splitting, deep jump
Q-learning, and cross-fitting.
Step 1: Data Splitting: We divide all n samples into L subsets of equal size, where l` denotes
the indices of samples in the 'th subset for ' = 1,…,L. Let Lc = {1, 2, ∙∙∙ ,n} - l` as the
complement of l`.
Step 2: Deep Jump Q-Learning: For each ' = 1,…，L, we propose to apply deep jump Q-
learning to compute a discretization Db(') and {/) : I ∈ Db(')} by solving a version of equation 3
using the data subset in Lc only. We next present the computational details for solving this op-
timization. Our approach is motivated by the PELT method (Killick et al., 2012) in multi-scale
change point detection. Specifically, for any interval I, define bɪ) as the minimizer of
arg min-1- X I(Ai ∈I){Qi (Xi)-匕}2,	(4)
Qi∈q 叫| i∈LC
where |Lc| denotes the number of samples in Lc. Define the cost function C(')(I) as the minimum
value of the objective function 4, i.e, C(')(I)=击 Pi∈Lc I(Ai ∈ 工){bj)(Xi) - Yi}2. In our
implementation, we set Q as the class of multilayer perceptrons (MLPs, see Figure 3 in Appendix
A for an illustration) with L hidden layers and H nodes each hidden layer. The above optimization
can be solved via the MLP regressor implementation of Pedregosa et al. (2011).
Computation of Db(') relies on dynamic programming (Friedrich et al., 2008). For any integer 1 ≤
v* < m, denote by B(m, v*) the set consisting of all possible discretizations Dv* of [0, v*/m). Set
B(m, m) = B(m), we define the Bellman function as
Bell(v*) =	inf T C")+ y(∣Dv* I — 1) , andBell(0) = -γ.
Dv* ∈B(m,v*)
I ∈Dv*
Our algorithm recursively updates the Bellman function based on the following formula,
Bell(v*) = min {Bell(v)+ C(')([v/m, v*∕m)) + γ} ,	∀v* ≥ 1,
where Rv* is the candidate change points list updated by
{v ∈ Rv*-ι ∪ {v* - 1} : Bell(v) + C(')([v/m, (v* - 1)∕m)) ≤ Bell(v* - 1)},
(5)
(6)
during each iteration with R0 = {0}. The constraint listed in 6 is important as it facilitates the
computation by discarding change points not relevant to obtain the final discretization, leading to a
linear computational cost (Killick et al., 2012).
To solve equation 5, we search the optimal change point location v that minimizes Bell(v*). This
requires to apply the MLP regressor to learn 茶)mv*/m)and C (')([v∕m,v*∕m)) for each V ∈ Rv*.
Let v 1 be the corresponding minimizer. We then define the change points list τ (v*) = {v1 , τ(v1 )}.
This procedure is iterated to compute Bell(v*) and τ (v*) for v* = 1, . . . , m. The optimal partition
Db(') is determined by the values stored in T (see Algorithm 1 in Appendix A for details).
Step 3. Cross-Fitting: For each interval in the estimated optimal partition Db('), we estimate the
generalized propensity score function Pr(A ∈ I|X = x) via the MLP regressor using the train-
ing dataset Lc. Let b(')(I∣χ) denote the resulting estimate. The final estimated value for V(∏) is
5
Under review as a conference paper at ICLR 2021
constructed via cross-fitting, given by,
L
V (∏) = 1 X X X I(Ai ∈I) I⅞(X)∈I} {匕-bf)(Xi)} +I(Ai e I忌) (Xi).⑺
'=1 I∈Db(') i∈L' -	b(I )(I|Xi)
Note the samples used to construct V inside bracket are independent from those to estimate b', bɪ)
and Db('). This helps remove the bias induced by overfitting in the estimation of q，bɪ) and Db(').
4 Theory
We investigate the theoretical properties of the proposed estimator. For simplicity, let the support
X = [0, 1]p. We will show our estimator is consistent when the Q-function is either a piecewise
function or a continuous function of a. Specifically, consider the following two model assumptions.
Model 1 (Piece-wise constant function). Suppose
Q(x, a) = X qI (x)I(a ∈ I),	∀x ∈ X,	∀a ∈ A,	(8)
I∈D0
for some partition D0 of [0, 1] and a collection of continuous functions (qI )I∈D0.
Model 2 (Continuous function). Suppose Q is a continuous function of a and x.
We first consider the case where the value function takes the form of equation 8. We remark that
kernel-based estimators will fail under this model assumption, as it requires the second-order deriva-
tive of the Q-function to exist. Without loss of generality, assume qI1 6= qI2 for any two adjacent in-
tervals I1,I2 ∈ D0. This guarantees that the representation in equation 8 is unique. For any partition
D = {[0, τι), [τι, τ2), ∙∙∙ , [τκ, 1]}, We use J(D) to denote the set of change points {τι,…,TK}.
We impose the following conditions to establish our theories.
Assumption 1. The number of layers L and the number of nodes in each hidden layer H diverge
With n, in that HL = O(nρ), for some constant 0 < ρ < 1/2.
Assumption 2. Functions {qχ'(∙)}τ∈Db(') are uniformly bounded.
Assumption 1 is mild, as both H and L are parameters We specify. The part that HL = O(nρ)
ensures that the stochastic error resulting from the parameter estimation in the MLP is negligible.
Assumption 2 ensures that the optimizer would not diverge in the '∞ sense. Similar assumptions
are commonly imposed in the literature to derive the convergence rates of DNN estimators (see
e.g., Farrell et al., 2018). These two assumptions guarantee the uniform consistency of the DNN
estimator {qι}i∈D(') . See Lemma 1 in Appendix D for details.
Theorem 1 Suppose Model 1, Assumptions 1 and 2 hold. Suppose m diverges to infinity with n.
Then, there exists some constant γ0 such that as long as 0 < γ ≤ γ0, the following events occur
with probability approaching 1 (w.p.a.1),
(i)	∣DD(')∣ = |Do|; (ii) maxτ∈J(D°) minτ∈J(D(')) ∣τ - T| = 0p(1). (iii) V(π) = V(π) + op(1) for
any policy π such that for any τ0 ∈ J(D0), Pr(π (X) ∈ [τ0 - , τ0 + ]) → 0 as → 0.
Theorem 1 establishes the properties of our method under settings where the Q(a, x) is piecewise
function in a. Results in (i) imply that deep jump Q-learning correctly identifies the number of
change points. Results in (ii) imply that any change point in D0 can be consistently identified.
In particular, Dq(') corresponds to a subset of {1/m, 2/m, ∙∙∙ , (m - 1)∕m}. With a sufficiently
large m, for any true change point T in Do, there will be a change point in Dq(') that approaches T.
Consequently, the change point locations can be consistently estimated. To ensure the consistency of
the proposed value estimator, we require that the distribution of the random variable π(X) induced
by the target policy does not have point-masses at the change point locations. This condition is also
mild. For instance, it automatically holds when π(X) has a density function on [0, 1].
Theorem 2 Suppose Model 2, Assumptions 1 and 2 hold. Suppose m diverges to infinity and γ
decays to zero. Then we have
(i) maXz∈D(') sup°∈ι EIbI')(X) - Q(X, a)∣2 = o(1)； (ii) V(∏) - V(∏) = Op(1) forany ∏.
6
Under review as a conference paper at ICLR 2021
Figure 2: The box plot of the estimated values of the optimal policy under the proposed DJQE and
two kernel-based methods for Scenario 1-4.
Theorem 2 establishes the properties of our method under settings where Q is continuous in a.
Results in (i) imply that b(χ)(∙) can be used to uniformly approximate Q(a, •) for any a ∈ I. The
consistency of the value in (ii) thus follows.
Finally, we conduct in-depth theoretical analysis to demonstrate the advantage of our estimator. Due
to space constraint, we present an informal statement below. Details are given in Appendix C.
Theorem 3 (Informal Statement)
(i)	When the Q-function belongs to a class of piecewise constant functions of the action a, the
minimax rate of convergence of the proposed value estimator is Op(n-1/2). In contrast, the minimax
convergence rate of kernel-based estimator is Op(n-1/3).
(ii)	When the Q-function belongs to a class of smooth functions of a, the minimax convergence
rate of our estimator is faster than kernel-based estimators if the bandwdith undersmoothes or over-
smoothes the data.
5 Experiments
In this section, we investigate the finite sample performance of the proposed DJQE on the synthetic
and real datasets, in comparison to two kernel-based methods. The computing infrastructure used is
a virtual machine in the AWS Platform with 72 processor cores and 144GB memory.
5.1	Synthetic Data
Synthetic data are generated from the following model:
Y |X,A 〜N {Q(X,A), 1}, b(A∣X)〜Unif[0,1] and X ⑴,X ⑵，...，X(P) iid Unif[-1,1],
where X = [X(1), X(2), . . . , X(p)]. Consider the following different scenarios:
S1:	Q(x, a) = (1 + x(1))I(a < 0.35) + (x(1) - x(2))I(0.35 ≤ a < 0.65) + (1 - x(2))I(a ≥ 0.65);
S2: Q(x, a) = I(a < 0.25) + sin(2πx(1))I(0.25 ≤ a < 0.5){0.5 - 8(x(1) - 0.75)2}I(0.5 ≤ a <
0.75) + 0.5I(a ≥ 0.75);
S3 (toy): Q(x, a) = 10 max{a2 - 0.25, 0} log(x(1) + 2);
S4: Q(x, a) = 0.2(8 + 4x(1) - 2x(2) -2x(3)) - 2(1 + 0.5x(1) + 0.5x(2) -2a)2.
The Q-function is a piecewise function of a under Scenarios 1 and 2, and is continuous under Sce-
narios 3 (toy example considered in Section 3.1) and 4. We set the target policy to be the optimal
policy that achieves the highest possible mean reward. We list the oracle mean value under the
optimal policy for each scenario in the first column of Table 4 in Appendix B.
We apply the proposed DJQE and two kernel-based methods (Kallus & Zhou, 2018; Colangelo &
Lee, 2020) to Scenario 1-4 with 20-dimensional covariates (p = 20) and n ∈ {50, 100, 200, 300}.
For the DJQE, we select γ ∈ {0.1, 0.2, 0.3, 0.4, 0.5}n0.4 based on five-fold cross-validation. Here,
we set m = n/10 to achieve a good balance between the bias and the computational cost (see Figure
4 in Appendix B for the detailed computational cost of the DJQE and the resulting bias as a function
of m in Scenario 1 with n = 100). We find it extremely computationally intensive to compute
the optimal bandwidth h using Kallus & Zhou (2018)'s method (see the detailed comparison of
computational cost under different methods based on Scenario 1 in Table 3). Thus, as suggested in
Kallus & Zhou (2018), We first compute h using data with sample size n0 = 50. To accommodate
data with different sample sizes n, we adjust h by setting h*{no/n}0.2. To implement Colangelo
7
Under review as a conference paper at ICLR 2021
Table 2: The bias, the standard deviation, and the mean squared error of the estimated values under
the optimal policy via the proposed DJQE and two kernel-based methods for the Warfarin data.
Methods	Bias	Standard deviation	Mean squared error
DJQE	0.259	0.416	0.240
Kallus & Zhou (2018)	0.662	0742	0.989
Colangelo & Lee (2020)	0.442	1.164	1.550
& Lee (2020)’s estimator, we consider a list of bandwidths, given by h = cσAn-0.2 with c ∈
{0.5, 0.75, 1.0, 1.5} and σA is the sample standard deviation of the action. We then manually select
the best bandwidth such that the resulting value estimator achieves the smallest mean squared error.
The conditional mean value and generalized propensity score are fitted via the MLP regressor with
10 hidden layer and 10 neurons in each layer. The average estimated value and its standard deviation
over 100 replicates are illustrated in Figure 2 for different methods, with detailed values reported
in Table 4 in Appendix B. In addition, we provide the size of the final estimated partition under the
DJQE in Table 5 in Appendix B, which is much smaller than m in most cases.
It can be seen from Figure 2 that the proposed DJQE is very efficient and outperforms all competing
methods in almost all cases. We note that the proposed method performs reasonably well even when
the sample size is small (n = 50). In contrast, two kernel-based methods fail to accurately estimate
the value even in some cases when n = 300. Among the two kernel-based OPE approaches, we
observe that the method developed by Kallus & Zhou (2018) performs better in general.
5.2 Real Data: Personalized Dose Finding
Warfarin is commonly used for preventing thrombosis and thromboembolism. We use the dataset
provided by the International Warfarin Pharmacogenetics (Consortium, 2009) for analysis. We
choose p = 81 baseline covariates considered in Kallus & Zhou (2018). This yields a total of
3964 with complete records of baseline information. The response is defined as the absolute dis-
tance between the international normalized ratio (INR, a measurement of the time it takes for the
blood to clot) after the treatment and the ideal value 2.5, i.e, Y = -|INR-2.5|. We use the min-max
normalization to convert the range of the dose level A into [0, 1].
To compare among different methods, we calibrate the dataset to generate simulated outcomes.
Specifically, we first estimate the Q-function via a MLP regressor with 10 hidden layers and 50
neurons in each layer using the whole dataset. The goodness-of-the-fit of the fitted model un-
der the MLP regressor is reported in Table 6 in Appendix B. We next use the fitted Q-function
Q(X, A) to simulate the data. Given a randomly sampled feature-action pair (aj , xj ) from
{(A1,X1), ∙∙∙ , (An,Xn)},we set the reward rj to N{Q(xj,aj),σ2},where σ is the standard devi-
ation of the fitted residual {Yi - Q(Xi, Ai)}i. Given the simulated data {(xj, aj, rj) : 1 ≤ j ≤ n},
we are interested in evaluating the optimal policy: π*(X) ≡ argmaXα∈[o,i] Q(X, a). The oracle
value under the optimal policy is V = -0.278.
We apply the DJQE on the calibrated Warfarin data, against two kernel-based methods. Due to the
extremely intensive computation in Kallus & Zhou (2018), we directly apply the estimated optimal
bandwidth h in their real data analysis, since they used the same dataset. Biases, standard devia-
tions, and mean squared errors of the estimated values are reported in Table 2 over 20 replicates for
sample size n = 500 under different methods.
It can be observed from Table 2 that our proposed DJQE achieves much smaller mean squared error
than the two kernel-based methods, when evaluating the optimal policy. Specifically, the DJQE
yields bias as 0.259 with the standard deviation as 0.416, in contrast to the large bias as 0.662 with
the standard deviation as 0.742 under Kallus & Zhou (2018)’s method and the bias as 0.442 with the
large standard deviation as 1.164 under Colangelo & Lee (2020)’s method. Therefore, our proposed
DJQE for off-policy evaluation with continuous actions works better than the kernel-based methods.
6 Discussion
Currently, we focus on settings with a single decision point. It would be practically interesting to
extend our proposal to sequential decision making. A potential drawback of our method is that it
would be very computationally intensive for a large m as the runtime increases linearly in m.
8
Under review as a conference paper at ICLR 2021
References
Leif Boysen, Angela Kempe, Volkmar Liebscher, Axel Munk, Olaf Wittich, et al. Consistencies and
rates of convergence of jump-penalized least squares estimators. The Annals of Statistics, 37(1):
157-183, 2009.
Bibhas Chakraborty. Statistical methods for dynamic treatment regimes. Springer, 2013.
Guanhua Chen, Donglin Zeng, and Michael R Kosorok. Personalized dose finding using outcome
weighted learning. Journal of the American Statistical Association, 111(516):1509-1521, 2016.
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, and Whit-
ney Newey. Double/debiased/neyman machine learning of treatment effects. American Economic
Review, 107(5):261-65, 2017.
Kyle Colangelo and Ying-Ying Lee. Double debiased machine learning nonparametric inference
with continuous treatments. arXiv preprint arXiv:2004.03036, 2020.
Francis S Collins and Harold Varmus. A new initiative on precision medicine. New England journal
of medicine, 372(9):793-795, 2015.
International Warfarin Pharmacogenetics Consortium. Estimation of the warfarin dose with clinical
and pharmacogenetic data. New England Journal of Medicine, 360(8):753-764, 2009.
Arnoud V den Boer and N Bora Keskin. Discontinuous demand functions: estimation and pricing.
Management Science, 2020.
Miroslav Dudik, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. arXiv
preprint arXiv:1103.4601, 2011.
Miroslav Dudk DUmitrU Erhan, John Langford, Lihong Li, et al. Doubly robust policy evaluation
and optimization. Statistical Science, 29(4):485-511, 2014.
Max H Farrell, Tengyuan Liang, and Sanjog Misra. Deep neural networks for estimation and
inference: Application to causal effects and other semiparametric estimands. arXiv preprint
arXiv:1809.09953, 2018.
Christian Fong, Chad Hazlett, Kosuke Imai, et al. Covariate balancing propensity score for a con-
tinuous treatment: Application to the efficacy of political advertisements. The Annals of Applied
Statistics, 12(1):156-177, 2018.
Klaus Frick, Axel Munk, and Hannes Sieling. Multiscale change point inference. J. R. Stat. Soc.
Ser. B. Stat. Methodol., 76(3):495-580, 2014. ISSN 1369-7412. doi: 10.1111/rssb.12047. With
32 discussions by 47 authors and a rejoinder by the authors.
Felix Friedrich, Angela Kempe, Volkmar Liebscher, and Gerhard Winkler. Complexity penalized m-
estimation: fast computation. Journal of Computational and Graphical Statistics, 17(1):201-224,
2008.
Piotr Fryzlewicz. Wild binary segmentation for multiple change-point detection. Ann. Statist., 42
(6):2243-2281, 2014. ISSN 0090-5364. doi: 10.1214/14-AOS1245.
Masaaki Imaizumi and Kenji Fukumizu. Deep neural networks learn non-smooth functions effec-
tively. In The 22nd International Conference on Artificial Intelligence and Statistics, pp. 869-878.
PMLR, 2019.
Nathan Kallus and Angela Zhou. Policy evaluation and optimization with continuous treatments.
arXiv preprint arXiv:1802.06037, 2018.
Rebecca Killick, Paul Fearnhead, and Idris A Eckley. Optimal detection of changepoints with a
linear computational cost. Journal of the American Statistical Association, 107(500):1590-1598,
2012.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
9
Under review as a conference paper at ICLR 2021
Lihong Li, Wei Chu, John Langford, and Xuanhui Wang. Unbiased offline evaluation of contextual-
bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM inter-
national conference on Web search and data mining, pp. 297-306, 2011.
James McInerney, Benjamin Lacker, Samantha Hansen, Karl Higley, Hugues Bouchard, Alois Gru-
son, and Rishabh Mehrotra. Explore, exploit, and explain: personalizing explainable recommen-
dations with bandits. In Proceedings of the 12th ACM Conference on Recommender Systems, pp.
31-39, 2018.
Susan A Murphy. Optimal dynamic treatment regimes. Journal of the Royal Statistical Society:
Series B (Statistical Methodology), 65:331-355, 2003.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,
12:2825-2830, 2011.
Sheng Qiang and Mohsen Bayati. Dynamic pricing with demand covariates. Available at SSRN
2765257, 2016.
Joseph P Romano and Cyrus DiCiccio. Multiple data splitting for testing. Department of Statistics,
Stanford University, 2019.
Chengchun Shi, Sheng Zhang, Wenbin Lu, and Rui Song. Statistical inference of the value function
for reinforcement learning in infinite horizon settings. arXiv preprint arXiv:2001.04515, 2020.
Arjun Sondhi, David Arbour, and Drew Dimmery. Balanced off-policy evaluation in general action
spaces. In International Conference on Artificial Intelligence and Statistics, pp. 2413-2423, 2020.
Adith Swaminathan, Akshay Krishnamurthy, Alekh Agarwal, Miro Dudik, John Langford, Damien
Jose, and Imed Zitouni. Off-policy evaluation for slate recommendation. In Advances in Neural
Information Processing Systems, pp. 3632-3642, 2017.
Ralph Turvey. Optimal Pricing and Investment in Electricity Supply: An Esay in Applied Welfare
Economics. Routledge, 2017.
Yu-Xiang Wang, Alekh Agarwal, and Miroslav Dudik. Optimal and adaptive off-policy evaluation
in contextual bandits. In International Conference on Machine Learning, pp. 3589-3597, 2017.
Dmitry Yarotsky. Error bounds for approximations with deep relu networks. Neural Networks, 94:
103-114, 2017.
Baqun Zhang, Anastasios A Tsiatis, Eric B Laber, and Marie Davidian. A robust method for esti-
mating optimal treatment regimes. Biometrics, 68:1010-1018, 2012.
10
Under review as a conference paper at ICLR 2021
A	More on the implementation
We summarize our algorithm in Algorithm 1.
Global: data {(Xi, Ai, Yi)}1≤i≤n; number of initial intervals m; penalty term γ; target policy π.
Local: an upper triangular matrix of cost C ∈ Rm(m+1)/2 ; Bellman function Bell ∈ Rm ; partitions Db;
DNN functions {qbI, bI : I ∈ D}; a vector τ ∈ Nm; a set of candidate point lists R.
Output: the value estimator for target policy Vb (π).
I.	Split all n samples into L subsets as {L1, ∙ ∙ ∙ , LL}; V(π) — 0;
II.	Initialize an even segment on the action space with m pieces:
{I} = {[0,1/m),[1/m,2/m),...,[(m- 1)/m, 1]};
III.	For ' = 1,…，L:
1.	Set the training dataset as Lc = {1, 2, ∙ ∙ ∙ , n} - l`;
2.	Bell(0) - -γ; Db = [0,1]; T — Null; R(0) — {0};
3.	Collect cost function:
For r = 1, . . . , m: For l = 0, . . . , (r - 1):
(i)	. Let I = [l/m, r/m) if r < m else I = [l/m, 1];
(ii)	. FitaMLP regressor: GI(∙) — I(i ∈ Lc)I(Ai ∈ I)Y 〜I(Ai ∈ I)MLP(X∕;
(iii)	. Calculate the cost: C (I) 一 Pi∈Lc I(Ai ∈ I ){bI (Xi) — Yi }2;
4.	Apply the PELT method to get partitions: For V * = 1,..., m:
(i)	.Bell(v *) = minv∈R(v*){Bell(v) + C ([v/m,v */m)) + Y };
(ii)	. v1 — argminv∈R(v*){Bell(v) + C([v/m, v*/m)) + Y};
(iii)	. τ(v*) — {v1 ,τ(v1)};
(iv)	. R(V *) 一 {v ∈ R(V * - 1) ∪ {v * - 1} : Bell(v)+ C ([v/m, (v * - 1)∕m)) ≤ Bell(v * - 1)};
5.	Construct the DR value estimator: r — m; l — T[r]; While r > 0:
(i)	Let I = [l/m,r/m) if r < m else I = [l/m, 1]; D ― D ∪I;
(ii)	Recall fitted MLP: GI(∙) — I(i ∈ Lc)I(4 ∈ I)Yi 〜I(Ai ∈ I)MLP(Xi);
一 一	一	个，、 _，    	_、	___，__、
(iii)	Fit propensity score: bI(∙) — I(i ∈ Lc)I(Ai ∈ I)〜I(Ai ∈ I)MLP(Xi);
(iv)	r ― l; l ― T(r);
6.	Evaluation using testing dataset L':
V(∏)+ = PI∈D (Pi∈L' I(Ai ∈ I) hi⅞⅞I} {Yi - bI(Xi)} + bI(Xi)]);
,	r>∕ 、/
return V(π)/n .
Algorithm 1: Deep Jump Q-Evaluation
Figure 3: Illustration of multilayer perceptron with two hidden layers and three nodes each hidden
layer. Here μ is the input, A(I) and b(I) denote the corresponding parameters to produce the linear
transformation for the (l - 1)th layer.
11
Under review as a conference paper at ICLR 2021
2 1 0 9 8 7 6
3 3 3 2 2 2 2
0.25
8	10	12	14	16
initial number of Intervals (m)
"snu 一≡*js03φuill
8	10	12	14	16
initial number of Intervals (m)
Figure 4: The bias of the estimated value and the computational cost (in minutes) under the DJQE
with different initial number of intervals (m) when n = 100 in Scenario 1.
B	Additional Experimental Results
We include additional experimental results in this section.
Table 3: The averaged computational cost (in minutes) under the proposed DJQE and two kernel-
based methods for Scenario 1.___________________________________________________
Method		DJQE	Kallus & Zhou (2018)	Colangelo & Lee (2020)
n	= 50	<1	365	<1
n=	100	3	773	<1
n=	200	7	-^> 1440 (24 hours)^^	<1
n=	300	14	> 2880 (48 hours)	<1
Table 4: The bias and the standard deviation (in parentheses) of the estimated values of the optimal
policy under the proposed DJQE and two kernel-based methods for Scenario 1 to 4.
	n	50	100	200	300
Scenario 1	DJQE	0.445(0.381)=	0.398(0.391)=	0.253(0.269)	0.209(0.210)
V = 1.33	Kallus & Zhou (2018)	0.656(0.787)	0.848(0.799)	1.163(0.884)	0.537(0.422)
	Colangelo & Lee (2020)	1.285(1.230)	1.473(1.304)	1.826(1.463)	0.934(0.730)
Scenario 2	DJQE	0.696(0.376Γ	0.502(0.311)=	0.400(0.219)	0.411(0.168)
V = 1.00	Kallus & Zhou (2018)	1.061(1.124)	1.363(1.131)	1.679(1.032)	1.664(0.792)
	Colangelo & Lee (2020)	1.827(1.371)	2.292(1.458)	2.429(1.541)	2.264(1.062)
Scenario 3	DJQE	2.014(0.865^	1.410(0.987Γ	1.184(0.967)	1.267(0.933)
V = 4.86	Kallus & Zhou (2018)	2.196(2.369)	2.758(2.510)	3.573(2.862)	1.151(1.798)
	Colangelo & Lee (2020)	2.586(2.825)	3.172(3.027)	3.949(3.391)	1.367(2.110)
Scenario 4	DJQE	0.494(0.485)=	0.412(0.426Γ	0.349(0.383)	0.321(0.315)
V = 1.60	Kallus & Zhou (2018)	2.192(1.210)	2.740(1.034)	3.354(1.324)	1.555(0.500)
	Colangelo & Lee (2020)	2.975(1.789)	3.282(1.525)	3.921(1.927)	1.853(0.751)
Table 5: The averaged size of the final estimated partition (|Db |) in comparison to the initial number
of intervals (m) under the proposed DJQE for Scenario 1 to 4.
|Db | / m	n =50	n =100	n =200	n =300
Scenario 1	3/5	4/10	6 / 20	6 / 30
Scenario 2	4/5	6/10	9 / 20	11/30
Scenario 3	4/5	6/10	8 / 20	10/30
Scenario 4	4/5	6/10	8 / 20	10/30
12
Under review as a conference paper at ICLR 2021
Table 6: The mean squared error (MSE)5, the normalized root-mean-square-deviation (NRMSD)6,
the mean absolute error (MAE)7, and the normalized MAE (NMAE)8of the fitted model under the
MLP regressor, linear regression, and the random forest algorithm, via ten-fold cross-validation.
Method	MLP Regressor	Linear Regression	Random Forest
MSE	0-06	0.09	0.08
NRMSD	0-13	0:16	0.15
MAE	0-19	0:23	0.22
NMAE	0-10	0.12	0.12
C Additional Theoretical Results
In this section, we conduct in-depth theoretical analysis to compare the minimax convergence rate
of the proposed value estimator with kernel-based value estimator.
We first briefly summarize our theoretical findings. When the Q-function belongs to a class of
piecewise constant functions of the action a, the proposed estimator converges at a faster rate than
kernel-based estimators. Specifically, the kernel-based OPE converges at a rate of n-1/3 whereas
our estimator converges at a rate of n-1/2 .
When the Q-function belongs to a class of Lipschitz continuous functions of a, our estimator con-
verges at a rate of n-1/5 whereas kernel-based estimators converge at a slower rate when the band-
wdith undersmoothes or oversmoothes the data.
As we have commented, it remains extremely challenging to tune the bandwidth of kernel-based
OPE in practice, since OPE is an unsupervised problem. The use ofa suboptimal bandwidth would
lead to a slower rate of convergence for kernel-based estimators. In contrast, we develop a supervised
learning algorithm to adaptively discretize the action space. The tuning parameters in our procedure
can be selected via cross-validation.
All the above findings are supported by our observations in the toy example (Section 3.1) and nu-
merical experiments (Section 5) as well.
We next present our theoretical results. We first introduce some notations. Define the following
classes of Q-functions:
Qi = I Q ： Q(a,χ) = XX
I(a ∈ I)qI (x), max E|qI1 (X) - qI2 (X)|2 ≥ ,
I	I∈D0 a∈I	III=ID0	⑼
|Do| ≤ Cι,Q(α,∙) ∈ Λ(β,C2), ∀a},
Q2 = [q ： SUp ∣Q(aι,x)- Q(a2,x)∣≤ C3,Q(a,∙) ∈ Λ(β,C2),∀a∖,
a1,a2,x
for some sufficiently small constant > 0 and some sufficiently large constants C1 , C2 > 0 where
Λ(β, C2) denotes the class of β-smooth functions (see e.g., Section 3.1.1, Shi et al., 2020). By
definition, the first set Q1 consists of all piecewise constant functions of a with at most C1 - 1
change points. The second set Q2 contains the class of Lipschitz continuous functions of a.
Finally, define the policy class Π = {π : π(X) has a density function bounded by C4} for some
sufficiently large constant C4 > 0. To simplify the analysis, we assume the behavior policy is
5MSE = n1 Pi=I(Yi — Yi)2. See https://en.wikipedia.org/wiki/Mean_squared_
error.
6NRMSD	=	max(√M-SEn(Y) -	See https://en.wikipedia.org/wiki/
Root-mean-square_deviation.
7MAE = 1 Pi=I |Yi — Yil- See https://en.wikipedia.org/wiki/Mean_absolute_
error.
8NMAE = maχ(YMAmin(Y)- See https://en.wikipedia.org/wiki/Root-mean-square_
deviation.
13
Under review as a conference paper at ICLR 2021
known such that b = bg) = b. In addition, assume the conditional variance of Y given A and X is
uniformly bounded away from zero.
Theorem 3 Suppose 2β > p. Then for any π ∈ Π and any Q ∈ Q1, with proper choice of γ and
the MLP class, the minimax rate of convergence of the proposed value estimator is Op(n-1/2). In
contrast, the minimax convergence rate of kernel-based estimator is Op(n-1/3).
Suppose 4β > 3p. For any Q ∈ Q2, with proper choice of γ and the MLP class, the minimax rate
of convergence of the proposed value estimator is Op (n-1/5), up to some logarithmic factors. In
contrast, the minimax convergence rate of kernel-based estimator is slower when h n-κ for some
κ < 1/5 or κ > 3/5.
We briefly comment on the condition 2β > p. This condition allows that the MLP regressor to
converge at a rate faster than n-1/4 (see e.g., Imaizumi & Fukumizu, 2019). Such a condition is
commonly assumed in the literature on evaluating average treatment effects (see e.g., Chernozhukov
et al., 2017; Farrell et al., 2018). In the second part of Theorem 3, we further require the MLP
regressor to converge at a rate faster than n-3/10.
D Technical Proof
Throughout the proof, We use c, C, co, c, etc., to denote some universal constants whose values are
allowed to change from place to place. Let Oi = {Xi, Yi} denote the data summarized from the ith
observation. For any interval I, let b(I|x) denote the integral a∈I b(a|x)da.
Proofs of Theorems 1 and 2 rely on Lemmas 1 and 2. Specifically, Lemma 1 establishes the uniform
convergence rate of qbI for any I whose length is no shorter than o(γ) and belongs to the set of
intervals:
I(m) = {[i1/m, i2/m) : for some integers i1 and i2 that satisfy 0 ≤ i1 < i2 < m}
∪ {[i3/m, 1] : for some integers i3 that satisfy 0 ≤ i3 < m}.
To state this lemma, we first introduce some notations. For any such interval I, define the function
qI (x) = E(Y |A ∈ I, X = x). It is immediate to see that the definition of qI here is consistent with
the one defined in equation 8 for any I ⊆ D0 .
Lemma 1 The following holds when either the conditions in Theorem 1 or 2 hold:
maχ	E[∣qι(X)-清>(X)|2∣{Oi}i∈L'] = 0p(1),
I∈I(m),∣I∣≥cγ
for any positive constant c > 0. x
Lemma 2 When either the conditions in Theorem 1 or 2 hold, mi□ι∈D(') |I| ≥ CY w.p.a.1 for
some constant C > 0.
We first present the proofs for these two lemmas. Next we present the proofs for Theorems 1 and 2.
D.1 Proof of Lemma 1
For any sufficiently small constant > 0, it suffices to show
max	E[∣qι(X)-涌>(X)∣2∣{Oi}i∈L'] ≤ Ce2,
I∈I(m),∣I∣≥cγ
w.p.a.1, for some constant C > 1 whose value will be specified later. The proof is divided into two
parts. In the first part, we show there exist some constants {θ∣}i such that
max max ∣qι(x) — MLP(x; θ1)| ≤ e.	(10)
I∈I(m) x∈X
In the second part, we show
max
I∈I(m),∣I∣≥cγ
E[∣MLP(X; θI)—#(X)∣2∣{Oi}i∈L'] ≤ (C - 1)e2.
(11)
14
Under review as a conference paper at ICLR 2021
The proof is hence completed.
Part 1.	Under the step model assumption, for any I0 ∈ D0, qI0 (•) is continuous. Since the support
X is compact, qI0 (•) is uniformly continuous. Similarly, we can show b is uniformly continuous as
well. For any I ∈ Im, we have
qI (x)
a∈I I0∈D0 b(a|x)I(a ∈ I0)qI0(x)da
b(I|x)
It follows from the uniform continuity ofqI0, b and the positivity assumption that qI (•) is continuous
for any I ∈ I(m).
Similarly, under the model assumption in Theorem 2, we can show qI (•) is continuous for any
I ∈ I(m) as well. Consequently, when either the conditions in Theorem 1 or 2 hold, we obtain that
qI (•) is continuous for any I ∈ I(m).
By Stone-Weierstrass theorem, there exists a multivariate polynomial function qI such that the ab-
solute value of the residual qι - qI is uniformly bounded by e/2. By Theorem 1 of YarotSky (2017),
there exists a feedforward neural network with a bounded number of hidden units that uniformly
approximates qI, with the approximation error uniformly bounded by 〃2 in absolute value. By
Lemma 1 of Farrell et al. (2018), such a feedforward network can be embedded into an MLP with a
bounded number of hidden units. Since we allow H and L to diverge, such an MLP can be further
embedded into an MLP with L layers and the widths of all layers being proportional to H . This
yields equation 10. The proof for Part 1 is thus completed.
Part 2.	We aim to show equation 11 holds. Under the boundedness assumption on Y ,
{qι(∙)}ι∈i(m) are uniformly bounded. We first observe that MLP(∙, θI) is a bounded function,
by equation 10 and the fact that qI (•) is bounded.
Next, it follows from equation 10 that
EI(A ∈ I){Y - MLP(X; θI)}2 _ EI(A ∈ I){Y - E(Y|A ∈ I, X)}2
Pr(A ∈ I)	=	Pr(A ∈ I)
EI(A ∈ I ){E(Y |A ∈I ,X) - MLP(X; θI )}2	EI(A ∈ I ){Y - E(Y |A ∈ I, X )}2	?
+ 一_	Pr(A ∈I)-------≤ __ Pr(A ∈I)-------------------J +「
By definition,
Pi∈L I(Ai ∈I){Yi - MLP(X，; θI)}2〉Pi∈L I(Ai ∈ I){匕-MLP(Xi；*)}2
Pr(A ∈ I)	≥	Pr(A ∈ I)
(12)
(13)
Suppose we can show
Su	Pi∈L' I(Ai ∈ I){Yi - MLP(Xi； θι)}2 - EI(A ∈I){Y - MLP(X; θι)}2
I∈I(mM∣≥cγ	|L'|Pr(A ∈ I)	Pr(A ∈ I)	(14)
θi ∈Θ*
= op (1),
where the set Θ* consists of all θɪ such that SuPx ∣MLP(x; θɪ)| ≤ M for some sufficiently large
constant M such that both θI and b' satisfy this constraint. It thus follows from equation 12 and
equation 13 that
EI(A ∈ I){Y - MLP(X; b∣))}2	EI(A ∈ I){Y - E(Y|A ∈ I, X)}2	?
Pr(A ∈ I)	≤	Pr(A ∈ I)	+ '，
w.p.a.1. Similar to equation 12, we have
EI(A ∈ I){Y - MLP(X; b('))}2 _ EI(A ∈ I){Y - E(Y|A ∈ I, X)}2
Pr(A ∈ I)	=	Pr(A ∈ I)
ι EI(A ∈ I){E(Y|A ∈ I,X) -MLP(X; θ∣'))}2
+	Pr(A ∈ I)	.
15
Under review as a conference paper at ICLR 2021
It follows that
EI(A ∈ I){E(Y |A ∈ I ,X) - MLP(X; b(('))}2	Eb(IX ){E(Y |A ∈ I ,X) - MLP(X; b(('))}2
Pr(A ∈ I)	=	Pr(A ∈ I)
≤ 22.
Under the positivity assumption, supx b(I|x)/Pr(A ∈ I) ≥ 2(c - 1)-1 for some constant c > 1.
This yields equation 11.
To complete the proof, it remains to show equation 14 holds.
Using similar arguments in Section A.2.2 of Farrell et al. (2018), for any I, we have with probability
at least 1 - 2 exp(-7) that
Pi∈L' I(Ai ∈ I){Yi - MLP(Xi； θι)}2	EI(A ∈ I){Y - MLP(X; θι)}2
----------:--:--:~:--:	 - 	:~:-:-
∣L'∣Pr(A ∈I)----------------------------------------------------Pr(A ∈I)
≤ e ( Mɪ + rγσ(I ,θɪ) + σ(I ,θɪ) T (log ɪ +log n)),
n n	n σ (I, θI )
for some constant C > 0, where σ2 (I, θ) corresponds to the variance of
Pr(A16 I) I(A ∈I){Y - MLP(X; θ)}2.
Under the positivity assumption, we have σ2(I, θ) ≤ O(1)|I|-1 where O(1) denotes some positive
constant. Consequently, for any I whose length is greater than cγ-1, we have with probability at
least 1 - 2 exp(-γC) that
Pi∈L' I(Ai ∈ I){Yi - MLP(Xi； θι)}2 EI(A ∈ I){Y - MLP(X; θι)}2
----------: : :~: :------------ - ---------:~: :---------
∣L'∣Pr(A ∈I)	Pr(A ∈I)
≤ C0 (MY + 产+ Sh2l2(logγ + logn)),
n nγ nγ
for some constant cC0 > 0.
Note that the number of elements in I(m) is upper bounded by (m + 1)2 . Since m is proportional
to n, by setting Y = c* logn for some constant C* > 0, we have 1 - 2(m + 1)2 exp(-C) → 1.
Consequently, it follows from Bonferroni’s inequality that the following event occurs w.p.a.1 for any
I ∈ I(m) such that |I| ≥ cY,
Pi∈L' I(Ai ∈ I){Yi - MLP(Xi； θι)}2	EI(A ∈ I){Y - MLP(X; θI)}2
-----------: : :~:----:	 - 	:~:-:-
∣L'∣Pr(A ∈I)---------------------------------------------------Pr(A ∈I)
(15)
(
≤ O(1)
M log n
n
+
where O(1) denotes some positive constant. Under the given conditions, the RHS is o(1). The proof
is hence completed.
D.2 Proof of Lemma 2
Consider a given interval I ∈ Tb⑶.We can find some interval 10 ∈ I(m) ∩ Db⑶ that is adjacent to
I. Consequently, the interval I ∪ I0 belongs to I(m) as well. It follows that
高 X I(Ai ∈ I){匕-猫)(Xi)}2 + ɪ X I(Ai ∈ 10){匕-有(Xi)}
I ' i∈L'	I ' i∈L'
1	(16)
≤ I^ E I(Ai ∈I∪ 10){匕-b∪ιo (Xi )}2 - Y.
I ' i∈L'
16
Under review as a conference paper at ICLR 2021
By definition, b((')zo minimizes the loss Pi∈La I(Ai ∈ I ∪ 10){匕 一 bɪ[)ɪo (Xi)}2. It follows that
X I(Ai ∈I∪I 0){Yi 一 b∪ιo (Xi)}2 ≤ X I(Ai ∈I∪I 0){Yi 一 b^(Xi)}2.
i∈L'	i∈L'
Combining this together with equation 16 yields that
Y ≤lL1∣ X I(Ai ∈I){Yi - 渭(Xi)}2.	(17)
I ' i∈L'
As both Y and b' are uniformly bounded, We have Y ≤ c|L'|-1 Pi∈%g I(Ai ∈ I) for some
constant c > 0. By Bernstein’s inequality, we have for any t > 0 such that
~I	^X I(Ai	∈	I)	≤	Pr(A	∈	I)	+t,	(18)
I ' i∈L'
with probability 1 一 exp(-nt2∕c(∣I∣ +1)) for some constant c > 0.
Similar to equation 15, by setting tɪ = Co max(，|IInTlog n, n-1 log n) for some constant Co >
0, we obtain w.p.a.1 that
∣L^∣ X I(Ai ∈ I) ≤ Pr(A ∈ I) + tI,
I '' i∈L'
for any I ∈ I(m). Under the condition that b is continuous and A × X is compact, b is bounded.
Consequently, the probability Pr(A ∈ I) is proportional to the length ofI. In view of equation 18,
for any I ∈ Db⑶，it shall satisfy
工 ≤∣I∣ + 3 + 2rr ∣I∣ logn ≤ 2∣I∣ + 2lo邺,
cC1	n	n	n
for some constant cC1 > 0, w.p.a.1, where the last inequality follows from Cauchy-Schwarz inequal-
ity. As γ》n-1 log n, we obtain ∣I∣ ≥ γ∕C2 for any I ∈ Db⑶ and some constant C2 > 0, w.p.a.1.
The proof is hence completed.
D.3 Proof of Theorem 1
We begin with an outline of the proof. The proof is divide into four steps. In the first step, we show
Pr(IDD⑶ ∣ ≤ ∣D'∣) → 1.	(19)
In the second step, we show
max min	∣τb 一 τ ∣ < δmin ,	(20)
T ∈J (DO) b∈j (D('))
where δmin = minz∈D0 ∣I∣∕3. By the definition of δmin, this implies that
Pr(IDDC)I ≥ ∣D'∣) → 1.	(21)
Combining equation 22 together with equation 19 proves (i) in Theorem 1. This proves (ii) in
Theorem 1. In the third step, we show
max min	∣τb 一 τ ∣ = op(1).
T∈J(DO) τ∈j(D⑶)
(22)
In the last step, we show (iii) holds. The proof is thus completed.
We next detail the proof for each of the step.
Step 1. Assume ∣Do∣ > 1. Otherwise, equation 19 automatically holds. Consider the partition
D = {[0, 1]} which consists of a single interval and a zero q-function q[o,1] (x) = 0 for any x. By
definition, we have
XX
I(Ai ∈I){γi - bɪ(χi)}2 +IL'[70')] ≤ ^X Yi +IL`h
Z∈D(') i∈L'	i∈L'
17
Under review as a conference paper at ICLR 2021
Since Y is uniformly bounded and γ = o(1), the right-hand-side (RHS) is O(n). Consequently, we
obtain
|D(')| ≤ coγ-1
(23)
for some constant c0 > 0.
Notice that
X X I(Ai ∈i){Yi - bf)(Xi)}2 ≥ X X I(Ai ∈i){Yi - qι(Xi)}2
I∈Db(') i∈L'
I∈D⑼ i∈L'
η1
+ X X I(Ai ∈I){∕(Xi)-qι(Xi)}2
I∈Db(') i∈L'
{z
η2
(24)
-2
X X I(Ai ∈ 1){匕-qι(Xi)}{b^(Xi) - qι(Xi)}.
i∈D⑼
i∈L'
{z
η3
|
}
}
}
We next show η2 , η3 = op(1).
Consider η2 first. Under Lemma 2, we have w.p.a.1 that
η2=	X	X I(Ai ∈I)超 )(Xi)-qι (Xi)}2.
I∈D('),∣I∣≥Cγi∈L'
We decompose the RHS by
X	∣L'∣E[I(A ∈ I){br)(X) - qι(X)}2∣{Oi}i∈Li]
I∈D('),∣I∣≥Cγ
+ X	X[I(Ai ∈I){∕)(Xi) - qι(Xi)}2 - E{I(A ∈I)则)(X) - qι(X)}2|{。小R}].
I∈D('),∣I∣≥Cγi∈L'
The first line is op(n), by Lemma 1. Using similar arguments in equation 15, we can show the
second term is upper bounded by
肛`i	X	{Mogn+r
I∈D('),∣I∣≥Cγ I
|I|H2L2 logn
,
n
for some constant c > 0, w.p.a.1. In view of equation 23, the above expression can be further
bounded by
c0M logn
cn < -------
γn
+ X r 1I1H 2L2lθg n } ≤ O(1)n ( W + S H 2L：g n ),
where O(1) denotes some positive constant and the second inequality follows from the Cauchy-
Schwarz inequality. This yields that η2 = op(n).
Using similar arguments, we can show that η3 = op(n). It follows that
X X I(Ai ∈I){Yi - qI')(Xi)}2 ≥ ηι + Op(n).	(25)
I∈D(') i∈L'
18
Under review as a conference paper at ICLR 2021
Notice that
ηι =E	EI(Ai ∈I){Yi - Q(Ai,Xi)+ Q(Ai,Xi)- qI(Xi)}2
I∈D⑺ i∈L'
X X I(Ai ∈ 1){匕-Q(Ai,Xi)}2 + X X I(Ai ∈ 1 ){Q(Ai,Xi) - qI(Xi)}2
I∈D⑶ i∈L'
I∈D(') i∈L'
,
"^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^■"^	^^^^^^^^^^^^^^^^^^^^^^^^{z
η4	η5
+ 2 X X I(Ai ∈I){Yi - Q(Ai,Xi)}{Q(Ai,Xi)- qI(Xi)}.
I∈D⑶ i∈L'
''^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
η6
Using similar arguments in bounding η2 and η3, we can show ηe = θp(n) and that
η5 = ∣L'∣ X E Eb(a∣X)∣Q(X, a) - qι(X)∣2dα + θp(n).
i∈D⑼J工
The first term on the RHS is greater than c0n Pτ∈b(') JI E∣Q(X, a) - qι (X )∣2da for some constant
c0 > 0. It follows that
ηι ≥ η4 + c0∣L'∣ X I E∣Q(X, a) - qI(X)∣2da + Op(n).
I∈D(') J工
Note that under the step model assumption, η4 can be rewritten as PI∈Dcι Pii,∈Le I(Ai ∈ I){匕-
qI(Xi)}2. This together with equation 25, yields that
X X I(Ai ∈I)(Yi - #)(Xi))2
I∈D(') i∈L'
≥ X X I(Ai ∈ i)(匕-qI(Xi))2 + c0∣L'∣ X E EIQ(X,a)- qI(X)∣2da + op(n),
I∈D0 i∈L'	I∈D(') JI
For any integer k such that 1 ≤ k ≤ ∣D0∣-l, let τ0,∙k be the change point location that satisfies 丁以 =
i/m for some integer i and that ∣τ0,k - TjkI < m-1. Denoted by D* the oracle partition formed by
the change point locations {t氤}a尸.Set 瑶,。=0, TjD0∣ = 1 and q证一,小)=口办一"。,%)
for 1 ≤ k ≤ ∣D0∣ -1 and q茴,K-1,i] = q[τ0,k-ι,i]. Let δw = [τ0t,k-ι,τ0,k) ∩ [τ0,k-ι,τ0,k)c for
1 ≤ k ≤ ∣D0∣ - 1 and Δ∣d0∣ = [tq,∣d0∣-i, 1] ∩ [τ0,∣D0 ∣-ι, 1]c. The length of each interval ∆k is at
most m-1. It follows that
XX
I(Ai ∈ I){Yi - qI(Xi)}2 -XX
I(Ai ∈I){K - qI(Xi)}2
Z∈D* i∈L'
Z∈P0 i∈L'
∣D0∣
≤ XXI(Ai ∈ ∆k){Y2 + sup supqI(x)).
k=1 i∈Li	I	Z⊆[°,1] X	J
(26)
Using similar arguments in bounding the RHS of equation 17 in the proof of Lemma 2, we can show
the RHS of equation 26 is θp(n), as m diverges to infinity. Combining this together with equation 25
yields
X X I(Ai ∈ I) {匕-严(Xi)}2 ≥ XXI(Ai ∈ I){匕-qI(Xi)}2
I∈D⑹ i∈L'
I∈D* i∈L'
+c0∣L'∣〉： E E∣Q(X,a)- qI(X)∣2da + Op(n).
I∈D⑼ JI
(27)
,
,
By definition, we have
≤
X X I(Ai ∈ D(Yi - *(Xi))2 + ∣L'∣γ∣的∣
I∈D(') i∈L'
XX
I(Ai ∈ I){匕-qI(Xi)>2 + ∣L'∣γ∣D*∣.
I∈P* i∈L'
19
Under review as a conference paper at ICLR 2021
Since |D*| = |Do|, it follows from equation 27 that
X E E∣Q(X,a)-qi(X)∣2da ≤ Y{∣Do∣-∣D⑶∣} + Op(1).
I∈D(')ll
(28)
Note that the left-hand-side (LHR) is non-negative. As Y > 0, we obtain that ∣Db(') | ≤ |Do|, w.p.a.1.
This completes the proof for the first step.
Step 2. It follows from equation 28 that
X E|Q(X, a) - qI(X)|2da ≤ Y{|D0| - 1} + op(1),
ι∈Db(') J工
and hence
X E E∣Q(X, a)-qι(X)∣2da ≤ 2γ{∣Do∣ - 1},
I∈D(')ll
(29)
w.p.a.1. We aim to show equation 20 holds under the event defined in equation 29. Otherwise, there
exists some τo ∈ J(Po) such that ∣τ - τo∣ ≥ δmin, for all T ∈ J(P). Under the event defined in
equation 29, we obtain that
τ0 +δmin
/	E∣Q(X, a) - qι(X)∣2da ≤ 2γ(∣Do∣ - 1),
τ0 -δmin
(30)
w.p.a.1. On the other hand, since Q(X, a) is a constant function on [T0 - δmin, T0) or [T0, T0 + δmin),
we have
≥
≥
τ0 +δmin
E|Q(X, a) -qI(X)|2da
τ0 -δmin
min (δminE∣q[τ0-δmin,τ0)(X) - q(X)|2 + δminE∣q[τo,τo + δmin)(X) - q(X)|2)
^m2in Elq[τ0-δmin,To) (X ) - q[τ0,τ0+δmin) (X )|2 ≥ ^^^0 ,
where
κ0 ≡	min E|qI1(X) - qI2(X)|22 > 0.
I1 ,I2 ∈P0
I1 and I2 are adjacent
This apparently violates equation 30 where Y ≤ δminκ/(4∣Do -4|). equation 20 thus holds w.p.a.1.
Step 3: Combining the results obtained in the first two steps yields that ∣D(') | = |Do|, w.p.a.1. It
follows from equation 28 that
X E|Q(X, a) -qI(X)|2da=op(1).
I∈D(') J工
Using similar arguments in Step 2, we can show max「∈j(d0)minb∈j(d('))|b - T| < e w.p.a.1, for
any sufficiently small > 0. The proof is hence completed.
Step 4: We begin with some notations. For any ∏, We define a random policy 开力⑷ according to the
partition D(' as follows:
Xb(a|x)
I{π(x) ∈ I, a ∈ I} b(ι∣χ).
i⊆D(')	'
Note that R1 开力⑼(a∣x)da = Pi⊆D(' I{∏(x) ∈ I} = 1 for any x. Consequently,不书(^ is a “valid”
random policy.
20
Under review as a conference paper at ICLR 2021
The proposed value estimator Vb(') is doubly-robust to V(开ð⑼).By Lemma 1, the estimated Q-
function is consistent. Consequently, V(') is consistent to V(开力⑷).Since the propose estimator is
a weighted average of V('), it suffices to show V(开力⑼)is consistent to V(∏). Note that
V (πDb(') ) = E f Q(X,a) X I{π(X) ∈I,a ∈I} b(a|X) da
40,1]	I⊆D(')	b(IX)
XI
I0∈D0 I0
b(I∩Io∣X)
b(I∣X)
EqI0 (X)	I{π(X) ∈ I}
i⊆D(')
Similarly, we can show
V(π)
XI
I0∈D0 I0
EqI0 (X)I{π(X) ∈ I0}.
(31)
For each Io ∈ Do, there exists an interval I ∈ D(' such that ∣I ∪ Io∣∕∣I∣ → 1. We use b(') to
denote this interval. Under the given conditions, we have
V
(πDb(')) = E /
I0∈D0 I0
Eqio(X)I{π(X) ∈I0')} + Op(1).
In view of equation 31, the value difference ∣V (冗彷⑶)一 V (∏)∣ can be upper bounded by
X Pr(∏(X) ∈Io -I0'))+ X Pr(∏(X) ∈I0')-Io)+ Op(1).
I0 ∈D0	I0 ∈D0
The first two terms decay to zero under the given conditions on π(X). The proof is hence completed.
D.4 Proof of Theorem 2
We need the following lemma to prove Theorem 2.
Lemma 3 Assume conditions in Theorem 2 hold. Then for any interval I ∈ I(m) with ∣I∣ γ
and any interval I0 ∈ D(' with I ⊆ I0, we have w.p.a.1 that
E∣qI (X) 一 qI0(X)∣2 = o(1),
where the little-o term is uniform in I and I0.
The rest of the proof is divided into three steps. In the first step, we show Assertion (i) in Theorem 2
holds. In the second step, we show Assertion (ii) in Theorem 2 holds. Finally, we present the proof
for Lemma 3.
Step 1. Consider a sequence {dn }n such that dn → 0 and dn γ. We aim to show
inf a∈ι E[∣Q(X, a) 一 bio(X)∣2∣{Oi}i∈L'] = 0p(1). By Lemma 1, it suffices to show
I0∈D(')
inf a∈10 E∣Q(X,α)-qιo(X)∣2 = o(1).
I0∈D(')
Suppose ∣I0∣ ≥ dn. Then according to Lemma 3, we can find some I such that a ∈ I ⊆ I0, ∣I∣ → 0,
E∣qI(X) 一 qI0(X)∣2 = o(1). Since ∣I∣ → 0 and a ∈ I, it follows from the uniform continuity of
the Q-function that ∣qI(X) 一 Q(X, a)∣ → 0. The assertion thus follows.
Next, suppose ∣I0∣ < dn. Then ∣I0∣ → 0 as well. It follows from the uniform continuity of the
Q-function that infa∈I0 ∣qI0 (X) 一 Q(X, a)∣ → 0. The assertion thus follows. This completes the
proof for the first step.
Step 2. Using similar arguments in Step 4 of the proof of Theorem 1, it suffices to show V(开力⑷)=
V(π) + op(1). By definition,
V(∏D(')) - V(π)= X E EQ(X,a)I(π(X) ∈ I)b(aX)da - EQ(π(X),X)
ι∈Db(') I1	b(I∣X)
=X Z E{Q(X,a) - Q(∏(X),X)}I(∏(X) ∈I)b⅛≡da.
ι∈Db(')	b(I∣X)
21
Under review as a conference paper at ICLR 2021
It follows that
|V(盯⑹)-V(π)∣ ≤ X inf Z E∖Q(a',X) - Q(π(X),X)∣I(π(X) ∈ I)b(∣≡da
IeD⑷ Ph	b(I|X)
=X inf E∣Q(α0,X) - Q(π(X),X)∣I(π(X) ∈I)
a a a0 ∈I
I∈D(')
≤ inf	E∣Q(a0,X) - Q(a00,X)∣.
a"a"∈Z,Z∈D(')
In Step 1 of the proof, we have shown that infa∈II∈D(') E∣Q(X, a) - qI(X)∣2 = o(1). It follows
that infa,ao∈I,I∈D⑼ E∣Q(X, a) - Q(a0,X)∣2 = o(1) and hence infa'a” ∈I,I∈D⑺ E∣Q(a0,X)-
Q(a", X )∣ = o(1), by Cauchy-Schwarz inequality. This completes the proof for the second step.
Step 3. For a given interval 10 ∈ D('),the set of intervals I considered in Lemma 3 can be classified
into the following three categories.
Category 1: I = Iz. It is immediate to see that qI = qI，and the assertion automatically holds.
Category 2: There exists another interval I * ∈ I(m) that satisfies 10 = I * U I. Notice that the
partition D(`)* = D(`)U {I *} UI -{10} corresponds to another partition. By definition, we have
ɪ XX I(Ai ∈I0){K - bo (Xi)}2 + γ∣力⑶*∣
I ' i∈L' I0∈D(g)*
≥ ɪ XX I(Ai ∈I0){匕-bo (Xi)}2 + YIw)∣,
IL'∣ i∈L' Io∈Db(')
and hence
∣Λ X I(Ai ∈I){K - q (Xi) >2 +
I ' i∈L'
≥ 告 X I(Ai ∈ 10){Yi - M(Xi)}2 - Y.
I ' i∈L'
It follows from the definition of qI* that
I(Ai ∈I *){Yi - qI* (Xi)}2 ≤
l`i i∈L'
Therefore, we obtain
2≥
嵩 XI(Ai ∈i*){Yi -qI*(Xi)}2
I ' i∈L'
高 X I(Ai ∈I){Yi - qI(Xi)}
I '' i∈L'
高 X I(Ai ∈I *){Yi - qI0 (Xi)}2.
I ' i∈L'
TO X I(Ai ∈ I){匕-qI0 (Xi)F - γ.	(32)
I ' i∈L'
∈ I(m) that satisfy 10 = I * UIUI **. Using similar
Category 3: There exist two intervals I *, I **
arguments in proving equation 32, we can show that
-ɪ X I(Ai ∈ I){Yi - qI(Xi)}2 ≥ £ X I(Ai ∈ I){Yi - qI0(Xi)>2 - 2γ.
I ' i∈L'	I ' i∈L'
Hence, regardless of whether I belongs to Category 2, or it belongs to Category 3, we have
∣L-∣ X I(Ai ∈ I){匕-qI(Xi)}2 ≥ ∣L-∣ X I(Ai ∈ I){匕-qI0 (Xi)}2 - 2γ.	(33)
I ' i∈L'	I ' i∈L'
Using similar arguments in equation 15, we can show w.p.a.1 that
TO X I(Ai ∈ i){匕-bqI(Xi)}2 = E[I(A ∈ i){Y - qI(X。1{。山出/+ O(PY田),
I ' i∈L'
TO X I(Ai ∈ 1){匕-qI，(Xi)F = E[I(A ∈ 1 ){Y - qI0 (X。1{。小出/十 O(PY田),
I ' i∈L'
22
Under review as a conference paper at ICLR 2021
where the little-o terms are uniform in I and I0 . Combining these together with equation 33 yields
E[I(A ∈ I){Y - qI(X)}2 ∣{Oi}i∈L'] ≥ E[I(A ∈ I){Y - qI0 (X)}2 ∣{Oi}i∈L'] - 2Y + O(PYiIj^),
for any I and I0, w.p.a.1. Note that qI satisfies E[I(A ∈ I){Y - qI(X)}|X] = 0. We have
E[I(A ∈ I){qi (X) - bI (X )}2∣{Oi}i∈L'] ≥ E[I(A ∈ I){qi (X) - bI0 (X )}2∣{Oi}i∈L'] - 2γ + 0(PYN).
Consider the first term on the RHS. Note that
E[I(A ∈ I){qι(X) — bio(X)}2∣{Oi}i∈L'] = E[I(A ∈ I){qι(X) - q∣(X)}2∣{Oi}i∈L']
+E[I(A ∈ I ){qio (X) - qi，(X )}2∣{Oi}i∈L'] - 2E[I(A ∈ I){qι (X) - qio (X )}{b∣o (X) - q∣(X )}∣{Oi}i∈L'].
By Cauchy-Schwarz inequality, the last term on the RHS can be lower bounded by
-2E[I(A ∈ I ){qI(X ) - qI0 (X )}2l{Oi}i∈L'] - 2E[I(A ∈ I ){bI0 (X ) - qI0 (X )}2∣{Oi}i∈L'].
It follows that
E[I(A ∈ I){qI(X) - bI0(X)}2∣{Oi}i∈L'] ≥ 1 E[I(A ∈ I){qI(X'H)}2∣{Oi}i∈L']
-3E[I(A ∈ I){bI0(X) - qz，(X)}2∣{Oi}i∈L'],
and hence
2E[I(A ∈ I){qI(X) - qzo(X)}2∣{Oi}i∈L'] - 2γ + o(PYW) ≤ E[I(A ∈ I){qI(X) - bI(X)}2∣{Oi}i∈L']
+3E[I(A ∈ I){qI0(X) - bI0(X)}2∣{Oi}i∈L'].
By Lemma 1 and the positivity assumption, the RHS is op(|I|). Note that the little-o terms are
uniform in I and I0 . As |I|	Y , we obtain that
E[I(A ∈ I){qI(X) - qzo(X)}2∣{Oi}i∈L'] = Op(III),
uniformly for any I and I0 , or equivalently,
E [bɪ{qI(X) - qzo(X)}2∣{Oi}i∈L' = Op(1).
By the positivity assumption, we have
E[{qI(X) - qzo(X)}2∣{Oi}i∈L'] = Op(1),
uniformly for any I and I0 , or equivalently,
E{qI(X) -qI0(X)}2 = Op(1).
This yields that
E{qI (X) - qI0 (X)}2 = O(1),
uniformly for any I and I0 . The proof is hence completed.
D.5 Proof of Theorem 3
In the first two steps, we compare the minimax convergence rate when Q ∈ Q1. In the next two
steps, we compare the minimax convergence rate when Q ∈ Q2 .
Step 1: We provide a lower bound for the minimax convergence of kernel-based OPE when Q ∈ Q1
in this step. Consider a piecewise constant Q-function
Q(a, x)
0,
1,
if a ≤ 1/2,
otherwise.
Apparently, we have Q ∈ Q1 when C2 ≥ 1 ≥ . Define a policy π such that the density function of
π(X ) equals
4/3, if 1/4 ≤ π(x) ≤ 1/2,
2/3, else if 1/2 ≤ π(x) < 4/3,
0,	otherwise.
23
Under review as a conference paper at ICLR 2021
We aim to show for such Q and π, the best possible convergence rate of kernel-based estimator is
n-1/3.
We first consider its variance. Since the conditional variance ofY |A, X is uniformly bounded away
from 0 and 1, similar to Theorem 1 of Colangelo & Lee (2020), we can show the variance is lower
bounded by O(1)(nh)-1 where O(1) denotes some positive constant.
We next consider its bias. Since the behavior policy is know. The bias is equal to
K{(A - π(X))∕h}	K{(A - π(X))∕h}
E-hb(A∣X)—{Y - Q(MX )，X)} = E —hb(AX)—{Q(A，X)- QEX )，X)}
∕∙π(X)+h/2	a a - ∏(X)〕
=E	K∖ ——\ {I(π(X) ≤ 1/2 < a) - I(a ≤ 1/2 < π(X))}da.
Jn(X)-h/2	I h J
Using the change of variable a = ht + -(X), the bias equals
1/2
E	K(t){I(-(X) ≤ 1/2 < -(X) + ht) -I(-(X) + ht ≤ 1/2 < -(X))}dt.
-1/2
Consider any 0 < h ≤ for some sufficiently small > 0. The bias is then equal to
4 Z 1 Z 1 K(t){I(a ≤ 1/2 < a + ht) - I(a + ht ≤ 1/2 < a)}dtda
3	1/2-/2 -1/2
+ — I Z	K(t){I(a ≤ 1/2 < a + ht) — I(a + ht ≤ 1/2 < a)}dtda.
3	112	-112
Under the symmetric condition on the kernel function, the above quantity is equal to
2 Z1/2	Z1/2	K(t)dtda ≥ 2 Z1/2 h/4 [ 1/2	K(t)dtda
3	112-h12 (1-2a)12h	3 112-h12	(1-2a)12h
≥ 2 [ 1/2 h/4 ( 1/2 K(t)dtda = h /1/2 K(t)dt.
3 1/2-h/2	1/4	6 1/4
Consequently, the bias is lower bounded by O(1)h where O(1) denotes some positive constant.
To summarize, the root mean squared error of kernel based estimator is lower bounded by
O(1){(nh)-1/2 + h} where O(1) denotes some positive constant. The optimal choice of h that
minimizes such lower bound would be of the order O(n-1/3). Consequently, the convergence rate
is lower bounded by O(1)n-1/3.
Step 2: We derive an upper bound for the minimax convergence rate of our estimator when Q ∈ Q1
in this step. Using similar arguments in the proof of Lemma 1 (see Section D.1) and the proof of
Theorem 1 in Imaizumi & Fukumizu (2019), we can show that with proper choices of the MLP
networks, the following holds with probability at least 1 - O(n-C) for some sufficiently large
constant C > 0,
maX	E[∣qι(X) - b)(X)∣2∣{Oi}i∈L'] ≤ C(n|I|)-2e/(2e+d) log2 n,
I∈I(m),∖I∖≥cγ
(34)
where β is defined in Q1 and Q2 .
By equation 34 and the condition that Y》n-2e/(2e+P) log2 n, using similar arguments in Steps
1 and 2 of the proof of Theorem 1, we can show that the estimated change point locations are
consistent and D(') = Do with probability 1 一 O(n-C). Similar to equation 27, it follows from
equation 34 that
X E E∣Q(X,a)-qι(X)∣2da =Osp”,
ι∈Db(') ,I
up to some logarithmic factors. This further implies that the change point locations in Db (`) converge
at a rate of O(n-2e/(2e+p)) UP to some logarithmic factors.
24
Under review as a conference paper at ICLR 2021
We next establish the statistical properties of our value estimates. We first observe that when b is
known, our estimator is unbiased to the inverse-propensity score weighted estimator
1L
n XXX
'=1 I∈Db(') i∈L'
I(Ai ∈ I)
I(π(Xi) ∈I)
b(I∣Xi)
(35)
Consequently, its bias is equal to
1
L
L
X X EI(A ∈I)I(∏(7)：J){Q(A,X) - Q(∏(X),X)}
b(I |X )
'=1 ι∈Db(')	" 1 '
L
L X X E / 红
'=1 I∈D(')	a∈I
I(≡≡2 {Q(a,X )-qI(X )}b(a|X )da.
Since b is bounded away from zero and infinity, and that supIPr(π(X) ∈ I)/|I| ≤ C4, the absolute
value of the bias is upper bounded by
L
O(1) L X X E (“J」© IQ(a,X ) — qi (X )∣da
'=1 I∈D(') Ja∈I	1	1
L
≤ O(1)L X X / Zsup ∣Q(a,x) - qi(x)∣da,
where O(1) denotes some positive constant. For each I ∈ Db('), We could find some I ∈ Do such
that the lengths of I-Io andIo -I are upper bounded by O(n-2e/(2e+p)) UP to some logarithmic
factors. By definition, we have
qI (x)
a∈I I0∈D0 b(a|x)I(a ∈ Io)qI0(x)da
b(I|x)
Consequently, supχ |qi(x) - qz0 (x) | = O(n-2e/(2e+p)) UP to some logarithmic factors. It follows
that the absolute value of the bias of bias is upper bounded by O(|D(')|n-2e/(2e+p)) UP to some
logarithmic factors. As |D(') | = |Do | ≤ Ci with probability tending to 1, the absolute value of the
bias is upper bounded by
Op(n-2β∕(2β+p)),	(36)
up to some logarithmic factors.
We next consider the variance. The variance is of the same order of magnitude of that of equation 35.
Since L is finite, it suffices to consider the variance of
I(∏(Xi) ∈ I)
b(I∣Xi)
L
n X X I(Ai ∈ I)
I∈Db(') i∈L'
(37)
The variance of equation 37 is upper bounded by
O(n-1)E {ι∑ I(A ∈I)∖}2 = O(nτ]XEI(A ^}
= O(nτ{XJI⅛iXF= O(n-^Db(')D，
where the last equality is due to that b is uniformly bounded way from zero and that Pr(π(X) ∈
I) ≤ O(1)∣I∣ for some positive constant O(1). As ∣D(')∣ = |Do| ≤ Ci with probability tending
to 1, the variance of equation 37 is upper bounded by Op(n-1). Consequently, the variance of our
value estimator is bounded by Op(n-i) as well. In addition, we note this bound and the bias bound
25
Under review as a conference paper at ICLR 2021
in equation 36 is uniform in Q ∈ Q1 ∪ Q2 and π ∈ Π. The proposed value converges at a rate of
Op(n-2β∕(2β+p)) up to some logarithmic terms. When 2//(2/ + P) > 1/2, both the bias and the
variance decay at a rate of n-1/2 up to some logarithmic factors. This completes the proof for the
first part.
Step 3. We provide a lower bound for the minimax convergence of kernel-based OPE when Q ∈ Q2
in this step. Similar to Step 1, we can show its variance is lower bounded by O(n-1h-1). When
h N n-κ for some K > 3/5, the root mean squared error is larger than or equal to O(n-(1-κ)/2).
Consider the Q-function
Q(x, a) = ChTK (a - J")
for some constant C > 0. With proper choice of C, we can show that such a choice of Q-function
belongs to Q2. Using similar arguments in Step 1, we can show the bias equals
EC-1
K2{(A -∏(X))∕h}
h2b(A∣X)
≥ C-1EK2{(A - π(X)"h}
h2
Using similar arguments in Step 1, we can show the right-hand-side is lower bounded O(1)h. When
h N n-κ for some κ < 1/5, the root mean squared error is larger than or equal to O(n-κ).
To summarize, when h N n-κ for some κ < 1/5 or κ > 3/5, kernel-based OPE converges at a rate
of n-κ* for some κ* < 1/5.
Step 4. We provide an upper bound for the minimax convergence of our estimator when Q ∈ Q2 in
this step.
Consider its bias first. Using similar arguments in Step 2, the bias is upper bounded by
O(LT) PL=I Pio∈D(') Ra∈ι0 Supx |Q(a, x) - qι (x)∣da. Similar to Lemma 3, We can show for
any interval I ∈ I(m) with |I|》Y and any interval 10 ∈ Db(') with I ⊆ 10, we have w.p.a.1 that
E∣qι (X )-qi，(X )|2 = O(I)J春，	(38)
|I |
provided that Y is proportional to n-2e/(2e+p) UP to some logarithmic factors. For any a, there
exists an interval I whose length is proportional to γ log n that covers a. Since Q ∈ Q2, we have
supx |Q(a, x) - qI (x)| = O(|I|). This together with equation 38 yields that
X S	SUp ∣Q(a,χ) - qι(χ)∣da ≤ O(1) f|l| + ʌ/ɪʌ) ∙
ιo∈Db(') a∈Iτ, x	∖ V|I1/
Set I = Y1/3, the bias is upper bounded by O(1)Y1/3. Using similar arguments in Step 2 of the
proof, the standard deviation
is upper bounded by PL=I LT
JnTIDb⑶|. By equation
23, It is
upper bounded by O(n-1/2Y-1/2 ). By setting Y to be proportional to n-3/5 (this rate is achievable
under the condition that 4β > 3p), we obtain the rate of n-1/5.
26