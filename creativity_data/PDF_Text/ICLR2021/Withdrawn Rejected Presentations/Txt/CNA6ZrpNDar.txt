Under review as a conference paper at ICLR 2021
On the Decision B oundaries of Neural Net-
works. A Tropical Geometry Perspective
Anonymous authors
Paper under double-blind review
Ab stract
This work tackles the problem of characterizing and understanding the decision
boundaries of neural networks with piecewise linear non-linearity activations. We
use tropical geometry, a new development in the area of algebraic geometry, to
characterize the decision boundaries of a simple network of the form (Affine,
ReLU, Affine). Our main finding is that the decision boundaries are a subset of
a tropical hypersurface, which is intimately related to a polytope formed by the
convex hull of two zonotopes. The generators of these zonotopes are functions of
the network parameters. This geometric characterization provides new perspectives
to three tasks. (i) We propose a new tropical perspective to the lottery ticket
hypothesis, where we view the effect of different initializations on the tropical
geometric representation of a network’s decision boundaries. (ii) Moreover, we
propose new tropical based optimization reformulations that directly influence the
decision boundaries of the network for the task of network pruning. (iii) At last,
we briefly discuss the reformulation of the generation of adversarial attacks in a
tropical sense, where we elaborate on this in detail in the supplementary material.1
1	Introduction
Deep Neural Networks (DNNs) have demonstrated outstanding performance across a variety of
research domains, including computer vision (Krizhevsky et al., 2012), speech recognition (Hinton
et al., 2012), natural language processing (Bahdanau et al., 2015; Devlin et al., 2018), quantum
chemistry Schutt et al. (2017), and healthcare (Ardila et al., 2019; ZhoU et al., 2019) to name a few
(LeCun et al., 2015). Nevertheless, a rigorous interpretation of their success remains elusive (Shalev-
Shwartz & Ben-David, 2014). For instance, in an attempt to uncover the expressive power of DNNs,
the work of Montufar et al. (2014) studied the complexity of functions computable by DNNs that have
piecewise linear activations. They derived a lower bound on the maximum number of linear regions.
Several other works have followed to improve such estimates under certain assumptions (Arora
et al., 2018). In addition, and in attempt to understand some of the subtle behaviours DNNs exhibit,
e.g. the sensitive reaction of DNNs to small input perturbations, several works directly investigated
the decision boundaries induced by a DNN for classification. The work of Moosavi-Dezfooli et al.
(2019) showed that the smoothness of these decision boundaries and their curvature can play a vital
role in network robustness. Moreover, the expressiveness of these decision boundaries at perturbed
inputs was studied in He et al. (2018), where it was shown that these boundaries do not resemble the
boundaries around benign inputs. The work ofLi et al. (2018) showed that under certain assumptions,
the decision boundaries of the last fully connected layer of DNNs will converge to a linear SVM.
Also, Beise et al. (2018) showed that the decision regions of DNNs with width smaller than the input
dimension are unbounded.
More recently, and due to the popularity of the piecewise linear ReLU as an activation function, there
has been a surge in the number of works that study this class of DNNs in particular. As a result, this
has incited significant interest in new mathematical tools that help analyze piecewise linear functions,
such as tropical geometry. While tropical geometry has shown its potential in many applications such
as dynamic programming (JoSWig & Schroter, 2019), linear programming (Allamigeon et al., 2015),
multi-objective discrete optimization (Joswig & Loho, 2019), enumerative geometry (Mikhalkin,
2004), and economics (Akian et al., 2009; Mai Tran & Yu, 2015), it has only been recently used
1Code regenerating all our experiments is attached in the supplementary material.
1
Under review as a conference paper at ICLR 2021
to analyze DNNs. For instance, the work of Zhang et al. (2018) showed an equivalency between
the family of DNNs with piecewise linear activations and integer weight matrices and the family of
tropical rational maps, i.e. ratio between two multi-variate polynomials in tropical algebra. This
study was mostly concerned about characterizing the complexity of a DNN by counting the number
of linear regions, into which the function represented by the DNN can divide the input space. This
was done by counting the number of vertices of a polytope representation recovering the results of
Montufar et al. (2014) with a simpler analysis. More recently, Smyrnis & Maragos (2019) leveraged
this equivalency to propose a heuristic for neural network minimization through approximating the
tropical rational map.
Contributions. In this paper, we take the results of Zhang et al. (2018) several steps further and
present a novel perspective on the decision boundaries of DNNs using tropical geometry. To that
end, our contributions are three-fold. (i) We derive a geometric representation (convex hull between
two zonotopes) for a super set to the decision boundaries of a DNN in the form (Affine, ReLU,
Affine). (ii) We demonstrate a support for the lottery ticket hypothesis (Frankle & Carbin, 2019) from
a geometric perspective. (iii) We leverage the geometric representation of the decision boundaries,
referred to as the decision boundaries polytope, in two interesting applications: network pruning
and adversarial attacks. For tropical pruning, we design a geometrically inspired optimization to
prune the parameters of a given network such that the decision boundaries polytope of the pruned
network does not deviate too much from its original network counterpart. We conduct extensive
experiments with AlexNet (Krizhevsky et al., 2012) and VGG16 (Simonyan & Zisserman, 2014) on
SVHN (Netzer et al., 2011), CIFAR10, and CIFAR 100 (Krizhevsky & Hinton, 2009) datasets, in
which 90% pruning rate is achieved with a marginal drop in testing accuracy. For tropical adversarial
attacks, we show that one can construct input adversaries that can change network predictions by
perturbing the decision boundaries polytope.
2	Preliminaries to Tropical Geometry
For completeness, we first provide preliminaries to tropical geometry (Itenberg et al., 2009; Maclagan
& Sturmfels, 2015).
Definition 1. (Tropical Semiring2) The tropical semiring T is the triplet {R ∪ {-∞},㊉，Θ}, where
㊉ and Θ define tropical addition and tropical multiplication, respectively. They are denoted as:
X ㊉ y = max{x, y},	X Θ y = X + y,	∀x, y ∈ T.
It can be readily shown that -∞ is the additive identity and 0 is the multiplicative identity.
Given the previous definition, a tropical power can be formulated as xθa = X Θ X ∙ ∙ ∙ Θ X = a.x, for
X ∈ T, a ∈ N, where a.X is standard multiplication. Moreover, a tropical quotient can be defined as:
X y = X - y, where X - y is standard subtraction. For ease of notation, we write Xa as Xa.
Definition 2. (Tropical Polynomials) For x ∈ Td, ci ∈ R and ai ∈ Nd, a d-variable tropical
polynomial with n monomials f : Td → Td can be expressed as:
f(x) =	(ci	Θ xa1)㊉(C2	Θ xa2)㊉•…㊉(cn Θ	Xan),	∀	ai	=	aj	when i = j.
We use the more compact vector notation Xa = x：1 Θ x02 ∙ ∙ ∙ Θ Xdd. Moreover and for ease of
notation, we will denote ci Θ Xai as ciXai throughout the paper.
Definition 3. (Tropical Rational Functions) A tropical rational is a standard difference or a tropical
quotient of two tropical polynomials: f(X) - g(X) = f(X)	g(X).
Algebraic curves or hypersurfaces in algebraic geometry, which are the solution sets to polynomials,
can be analogously extended to tropical polynomials too.
Definition 4. (Tropical Hypersurfaces) A tropical hypersurface ofa tropical polynomial f(X) =
cιxa1 ㊉•一㊉ CnXan is the set ofpoints X where f is attained by two or more monomials in f, i.e.
T(f) := {X ∈ Rd : ciXai = cj Xaj = f (X), for some ai 6= aj}.
Tropical hypersurfaces divide the domain of f into convex regions, where f is linear in each region.
Also, every tropical polynomial can be associated with a Newton polytope.
2A semiring is a ring that lacks an additive inverse.
2
Under review as a conference paper at ICLR 2021
Figure 1: Tropical Hypersurfaces and their Corresponding Dual Subdivisions. We show three tropical
polynomials, where the solid red and black lines are the tropical hypersurfaces T(f) and dual subdivisions δ(f)
to the corresponding tropical polynomials, respectively. T(f) divides The domain of f into convex regions
where f is linear. Moreover, each region is in one-to-one correspondence with each node of δ(f). Lastly, the
tropical hypersurfaces are parallel to the normals of the edges of δ(f) shown by dashed red lines.
Definition 5. (Newton Polytopes) The Newton polytope of a tropical polynomial f (x) = c1 xa1 ㊉
•…㊉ CnXan is the convex hull of the exponents ai ∈ Nd regarded as points in Rd, i.e.
∆(f) := ConvHull{ai ∈ Rd : i = 1,. . . , n and ci 6= -∞}.
A tropical polynomial determines a dual subdivision, which can be constructed by projecting the
collection of upper faces (UF) inP(f) := ConvHull{(ai, ci) ∈ Rd ×R : i = 1, . . . , n} onto Rd. That
is to say, the dual subdivision determined by f is given as δ(f) := {π(p) ⊂ Rd : p ∈ UF(P (f))},
where π : Rd×R → Rd is the projection that drops the last coordinate. It has been shown by Maclagan
& Sturmfels (2015) that the tropical hypersurface T(f) is the (d-1)-skeleton of the polyhedral complex
dual to δ(f). This implies that each node of the dual subdivision δ(f) corresponds to one region in
Rd where f is linear. This is exemplified in Figure 1 with three tropical polynomials, and to see this
clearly, We Win elaborate on the first tropical polynomial example f (x,y) = X ㊉ y ㊉ 0. Note that as
per Definition 4, the tropical hypersurface is the set of points (x, y) where x = y, y = 0, and x = 0.
This indeed gives rise to the three solid red lines indicating the tropical hypersurfaces. As for the dual
subdivision δ(f), we observe that X ㊉y ㊉0 can be written as (x1 Θy0)㊉(x0 Θy1)㊉(x0 Θy0). Thus,
and since the monomials are bias free (ci = 0), then P(f) = ConvHull{(1, 0, 0), (0, 1, 0), (0, 0, 0)}.
It is then easy to see that δ(f) = ConvHull{(1, 0), (0, 1), (0, 0)}, since UP(P (f)) = P(f), which is
the black triangle in solid lines in Figure 1. One key observation in all three examples in Figure 1 is
that the number of regions where f is linear (that is 3, 6 and 10, respectively) is equal to the number
of nodes in the corresponding dual subdivisions. Second, the tropical hypersurfaces are parallel to
the normals to the edges of the dual subdivision polytope. This observation will be essential for the
remaining part of the paper. Several other observations are summarized by BrUgane & Shaw (2014).
Moreover, Zhang et al. (2018) showed an equivalency between tropical rational maps and a family of
neural network f : Rn → Rk with piecewise linear activations through the following theorem.
Theorem 1.	(Tropical Characterization of Neural Networks, (Zhang et al., 2018)). A feedforward
neural network with integer weights and real biases with piecewise linear activation functions
is a function f : Rn → Rk, whose coordinates are tropical rational functions of the input, i.e.,
f(x) = H(x) Q(x) = H(x) - Q(x), where H and Q are tropical polynomials.
While this is new in the context of tropical geometry, it is not surprising, since any piecewise linear
function can be written as a difference of two max functions over a set of hyperplanes (Melzer, 1986).
Before any further discussion, we first recap the definition of zonotopes.
Definition 6. Let u1 , . . . , uL ∈ Rn. The zonotope formed by u1, . . . , uL is defined as
Z(u1, . . . , uL) := {PiL=1 Xiui : 0 ≤ Xi ≤ 1}. Equivalently, Z can be expressed with respect
to the generator matrix U ∈ RL×n, where U(i, :) = ui> as ZU := {U>x : ∀x ∈ [0, 1]L}.
Another common definition for a zonotope is the Minkowski sum of the set of line segments
{u1, . . . , uL} (refer to appendix), where a line segment of the vector ui in Rn is defined as {αui :
∀α ∈ [0, 1]}. It is well-known that the number of vertices of a zonotope is polynomial in the number
of line segments, i.e. ∣vert (ZU) | ≤ 2 Pn-OI(L- 1) = O (LnT) (Gritzmann & Sturmfels, 1993).
3
Under review as a conference paper at ICLR 2021
Tropical Geometric View to Decision Boundaries
(5(B(x)) = ConvexHull(2Gι, Zq2)
Figure 2: Decision Boundaries as Geometric Structures. The decision boundaries B (in red) comprise two
linear pieces separating classes C1 and C2. As per Theorem 2, the dual subdivision of this single hidden neural
network is the convex hull between the zonotopes ZG1 and ZG2 . The normals to the dual subdivison δ(R(x))
are in one-to-one correspondence to the tropical hypersurface T (R(x)), which is a superset to the decision
boundaries B. Note that some of the normals to δ(R(x)) (in red) are parallel to the decision boundaries.
3 Decision B oundaries of Neural Networks as Polytopes3
In this section, we analyze the decision boundaries of a network in the form (Affine, ReLU,
Affine) using tropical geometry. For ease, we use ReLUs as the non-linear activation, but
any other piecewise linear function can also be used. The functional form of this network is:
f(x) = Bmax (Ax + c1, 0) + c2, where max(.) is an element-wise operator. The outputs of the
network f are the logit scores. Throughout this section, we assume4 that A ∈ Zp×n , B ∈ Z2×p ,
c1 ∈ Rp and c2 ∈ R2. For ease of notation, we only consider networks with two outputs, i.e. B2×p,
where the extension to a multi-class output follows naturally and is discussed in the appendix. Now,
since f is a piecewise linear function, each output can be expressed as a tropical rational as per Theo-
rem 1. If f1 and f2 refer to the first and second outputs respectively, we have f1(x) = H1(x) Q1(x)
and f2(x) = H2(x) Q2(x), where H1, H2, Q1 and Q2 are tropical polynomials. In what follows
and for ease of presentation, we present our main results where the network f has no biases, i.e.
c1 = 0 and c2 = 0, and we leave the generalization to the appendix.
Theorem 2.	For a bias-free neural network in the form f(x) : Rn → R2, where A ∈ Zp×n and
B ∈ Z2×p, let R(x) = H1 (x) Θ Q2 (x)㊉ H2 (x) Θ Q1 (x) be a tropical polynomial. Then:
• Let B = {x∈ Rn : f1(x) = f2(x)} define the decision boundaries of f, then B ⊆ T (R(x)).
• δ (R(x))	=	ConvHull (ZG1 , ZG2 ).	ZG1	is a zonotope in	Rn	with line segments
{(B+(1,j)+B-(2,j))[A+(j,:),A-(j,:)]}jp=1andshift(B-(1,:)+B+(2,:))A-,whereA+ =
max(A, 0) and	A-	= max(-A, 0).	ZG2	is a zonotope in	Rn	with line segments
{(B-(1,j)+B+(2,j))[A+(j,:),A-(j,:)]}jp=1andshift(B+(1,:)+B-(2,:))A-.
Digesting Theorem 2. This theorem aims at characterizing the decision boundaries (where f1 (x) =
f2(x)) of a bias-free neural network of the form (Affine, ReLU, Affine) through the lens of tropical
geometry. In particular, the first result of Theorem 2 states that the tropical hypersurface T (R(x)) of
the tropical polynomial R(x) is a superset to the set of points forming the decision boundaries, i.e. B.
Just as discussed earlier and exemplified in Figure 1, tropical hypersurfaces are associated with a
corresponding dual subdivision polytope δ(R(x)). Based on this, the second result of Theorem 2
states that this dual subdivision is precisely the convex hull of two zonotopes denoted as ZG1 and
ZG2 , where each zonotope is only a function of the network parameters A and B.
Theorem 2 bridges the gap between the behaviour of the decision boundaries B, through the superset
T (R(x)), and the polytope δ (R(x)), which is the convex hull of two zonotopes. It is worthwhile to
mention that Zhang et al. (2018) discussed a special case of the first part of Theorem 2 for a neural
network with a single output and a score function s(x) to classify the output. To the best of our
knowledge, this work is the first to propose a tropical geometric formulation of a superset containing
the decision boundaries of a multi-class classification neural network. In particular, the first result of
Theorem 2 states that one can perhaps study the decision boundaries, B, directly by studying their
superset T (R(x)). While studying T (R(x)) can be equally difficult, the second result of Theorem
2 comes in handy. First, note that, since the network is bias-free, π becomes an identity mapping
with δ(R(x)) = ∆(R(x)), and thus the dual subdivision δ(R(x)), which is the Newton polytope
∆(R(x)) in this case, becomes a well-structured geometric object that can be exploited to preserve
3All proofs are left for the appendix.
4Without loss of generality, as one can very well approximate real weights as fractions and multiply by the
least common multiple of the denominators as discussed in Zhang et al. (2018).
4
Under review as a conference paper at ICLR 2021
Figure 3: Effect of Different Initializations on the Decision Boundaries Polytope. From left to right:
training dataset, decision boundaries polytope of original network (before pruning), followed by the decision
boundaries polytope for networks pruned at different pruning percentages using different initializations. Note
that in the original polytope there are many more vertices than just 4, but they are very close to each other
forming many small edges that are not visible in the figure.
decision boundaries as per the second part of Theorem 2. Now, based on the results of Maclagan &
Sturmfels (2015) (Proposition 3.1.6) and as discussed in Figure 1, the normals to the edges of the
polytope δ(R(x)) (convex hull of two zonotopes) are in one-to-one correspondence with the tropical
hypersurface T (R(x)). Therefore, one can study the decision boundaries, or at least their superset
T (R(x)), by studying the orientation of the dual subdivision δ(R(x)).
While Theorem 2 presents a strong relation between a polytope (convex hull of two zonotopes)
and the decision boundaries, it remains unclear how such a polytope can be efficiently constructed.
Although the number of vertices of a zonotope is polynomial in the number of its generating line
segments, fast algorithms for enumerating these vertices are still restricted to zonotopes with line
segments starting at the origin Stinson et al. (2016). Since the line segments generating the zonotopes
in Theorem 2 have arbitrary end points, we present the next result that transforms these line segments
into a generator matrix of line segments starting from the origin as in Definition 6. This result is
essential for an efficient computation of the zonotopes in Theorem 2.
Proposition 1. The zonotope formed by p line segments in Rn with arbitrary end points {[ui1, ui2]}ip=1
is equivalent to the zonotope formed by the line segments {[ui1 - ui2 , 0]}ip=1 with a shift of ip=1 ui2.
We can now represent with the following corollary the arbitrary end point line segments forming the
zonotopes in Theorem 2 with generator matrices, which allow us to leverage existing algorithms that
enumerate zonotope vertices Stinson et al. (2016).
Corollary 1. The generators of ZG1 , ZG2 in Theorem 2 can be defined as G1	=
Diag[(B+(1, :)) + (B- (2, :))]A and G2 = Diag[(B+(2, :)) + (B- (1, :))]A, both with shift
(B-(1,:) + B+(2, :) + B+(1, :) + B- (2, :)) A-, where Diag(v) arranges v in a diagonal matrix.
Next, we show several applications for Theorem 2 by leveraging the tropical geometric structure.
4 Tropical Perspective to the Lottery Ticket Hypothesis
The lottery ticket hypothesis was recently proposed by Frankle & Carbin (2019), in which the
authors surmise the existence of sparse trainable sub-networks of dense, randomly-initialized, feed-
forward networks that when trained in isolation perform as well as the original network in a similar
number of iterations. To find such sub-networks, Frankle & Carbin (2019) propose the following
simple algorithm: perform standard network pruning, initialize the pruned network with the same
initialization that was used in the original training setting, and train with the same number of epochs.
They hypothesize that this results in a smaller network with a similar accuracy. In other words, a
sub-network can have decision boundaries similar to those of the original larger network. While we
do not provide a theoretical reason why this pruning algorithm performs favorably, we utilize the
geometric structure that arises from Theorem 2 to reaffirm such behaviour. In particular, we show
that the orientation of the dual subdivision δ(R(x)) (referred to as decision boundaries polytope),
where the normals to its edges are parallel to T (R(x)) that is a superset to the decision boundaries,
is preserved after pruning with the proposed initialization algorithm of Frankle & Carbin (2019).
Conversely, pruning with a different initialization at each iteration results in a significant variation in
the orientation of the decision boundaries polytope and ultimately in reduced accuracy.
To this end, we train a neural network with 2 inputs (n = 2), 2 outputs, and a single hidden layer with
40 nodes (p = 40). We then prune the network by removing the smallest x% of the weights. The
pruned network is then trained using different initializations: (i) the same initialization as the original
network (Frankle & Carbin, 2019), (ii) Xavier (Glorot & Bengio, 2010), (iii) standard Gaussian, and
5
Under review as a conference paper at ICLR 2021
(iv) zero mean Gaussian with variance 0.1. Figure 3 shows the decision boundaries polytope, i.e.
δ(R(x)), as we perform more pruning (increasing the x%) with different initializations. First, we
show the decision boundaries by sampling and classifying points in a grid with the trained network
(first subfigure). We then plot the decision boundaries polytope δ(R(x)) as per the second part
of Theorem 2 denoted as original polytope (second subfigure). While there are many overlapping
vertices in the original polytope, the normals to some of the edges (the major visible edges) are
parallel to the decision boundaries shown in the first subfigure of Figure 3. We later show the decision
boundaries polytope for the same network under different levels of pruning. One can observe that
the orientation of δ(R(x)) for all different initialization schemes deviates much more from the
original polytope as compared to the lottery ticket initialization. This gives an indication that lottery
ticket initialization indeed preserves the decision boundaries, since it preserves the orientation of the
decision boundaries polytope throughout the evolution of pruning. An alternative means to study
the lottery ticket could be to directly observe the polytopes representing the functional form of the
network, i.e. δ(H{1,2}(x)) and δ(Q{1,2} (x)), in lieu of the decision boundaries polytopes. However,
this strategy may fail to provide a conclusive analysis of the lottery ticket, since there can exist
multiple polytopes δ(H{1,2}(x)) and δ(Q{1,2}(x)) for networks with the same decision boundaries.
This highlights the importance of studying the decision boundaries directly. Additional discussions
and experiments are left for the appendix.
5	Tropical Network Pruning
Network pruning has been identified as an effective approach to reduce the computational cost and
memory usage during network inference. While it dates back to the work of LeCun et al. (1990) and
Hassibi & Stork (1993), network pruning has recently gained more attention. This is due to the fact
that most neural networks over-parameterize commonly used datasets. In network pruning, the task
is to find a smaller subset of the network parameters, such that the resulting smaller network has
similar decision boundaries (and thus supposedly similar accuracy) to the original over-parameterized
network. In this section, we show a new geometric approach towards network pruning. In particular
and as indicated by Theorem 2, preserving the polytope δ(R(x)) preserves a superset to the decision
boundaries, T (R(x)), and thus the decision boundaries themselves.
Motivational Insight. For a single hidden layer neural network, the dual subdivision to the decision
boundaries is the polytope that is the convex hull of two zonotopes, where each is formed by taking the
Minkowski sum of line segments (Theorem 2). Figure 4 shows an example, where pruning a neuron
in the network has no effect on the dual subdivision polytope and hence no effect on performance.
This occurs, since the tropical hypersurface T (R(x)) before and after pruning is preserved, thus,
keeping the decision boundaries the same.
Problem Formulation. In light of the motivational insight, a natural question arises: Given an
over-parameterized binary output neural network f(x) = B max (Ax, 0), can one construct a new
neural network, parameterized by sparser weight matrices A and B, such that this smaller network
has a dual subdivision δ(R(x)) that preserves the decision boundaries of the original network?
To address this question, we propose the following optimization problem to compute A and B:
min d(δ(R(x)),δ(R(x))) = min d 化onvHull (Za ɪ, Za ɔ , ConvHUll (Zgi , Zq?)).	⑴
The fUnction d(.) defines a distance between two geometric objects. Since the generators G1 and
G2 are fUnctions of A and B (as per Theorem 2), this optimization problem can be challenging
to solve. However, for prUning pUrposes, one can observe from Theorem 2 that if the generators
G1 and G2 had fewer nUmber of line segments (rows), this corresponds to a fewer nUmber of
rows in the weight matrix A (sparser weights). So, we observe that if G1 ≈ G1 and G2 ≈ G2,
then δ(R(x)) ≈ δ(R(x)), and thUs the decision boUndaries tend to be preserved as a conseqUence.
Therefore, we propose the following optimization problem as a sUrrogate to the one in Problem (1):
min 1 OG 1- GdlF TG 2 -G21)+λdlG 1B2,1+λ2BG 2B2,1.	⑵
The matrix mixed norm for C ∈ Rn×k is defined as kCk2,1 = Pin=1 kC(i, :)k2, which encoUrages
the matrix C to be row sparse, i.e. complete rows of C are zero. The first two terms in Problem
6
Under review as a conference paper at ICLR 2021
8(JR(X)) = ConVeXHUu(2g-2g2)6(A(X)) = ConVeXHU11(22，2品)
Figure 4: Tropical Pruning Pipeline. Pruning the 4th node, or equivalently removing the two yellow vertices
of zonotope ZG2 does not affect the decision boundaries polytope, which will lead to no change in accuracy.
(2) aim at approximating the original dual subdivision δ(R(x)) by approximating the underlying
generator matrices, G1 and G2 . This aims to preserve the orientation of the decision boundaries
of the newly constructed network. On the other hand, the second two terms in Problem (2) act
as regularizers to control the sparsity of the constructed network by controlling the sparsity in the
number of line segments. We observe that Problem (2) is not quadratic in its variables, since as
二	L	,二，	、、	，二，	、、r ~	E	L	，二,
Per Corollary, 1 G1 = Diag[ReLU(B(1,:)) + ReLU(-B(2,:))] A and G2 = Diag[ReLU(B(2,:
)) + ReLU(-B(1, :))]A. However, since Problem (2) is seParable in the rows of A and B, we solve
Problem (2) via alternating oPtimization over these rows, where each sub-Problem can be shown to
be convex and exhibits a closed-form solution leading to a very efficient solver. For ease of notation,
we refer to ReLU(B(i, :)) and ReLU(-B(i, :)) as B+(i, :) and B-(i, :), resPectively. As such, the
Per row uPdate for A (first linear layer) is given as follows:
1	λι pi + λ2 p2_________1_________ 0] ( Ci Gi (i,:)+ c2G2 (i,:)
2	1 (Ci + c2)	— ♦了+* *, I k	2 (ci + c2)
2 (ci +c2)	2 /
A(i,:) = max 1 —
where cii is the ith element of ci = ReLU(B(1, :)) + ReLU(-B(2, :)) and c2 = ReLU(B(2, :
)) + ReLU(—B(1, :)). Similarly, the closed form uPdate for the jth element of the second linear
layer is as follows:
B + (1, j) = max
Aj,:)> G i+ j,:) — λ∣∣A(j")∣∣2
kA(j,:)k2
where Gi+ = Diag(B+ (1, :))A. A similar argument can be used to update the variables B+ (2,:),
B-(1, :), and B-(2, :). The details of deriving the aforementioned update steps and the extension to
the multi-class case are left to the appendix. Note that all updates are cheap, as they are expressed
in a closed form single step. In all subsequent experiments, we find that running the alternating
optimization for a single iteration is sufficient to converge to a reasonable solution, thus, leading to a
very efficient overall solver.
Extension to Deeper Networks. While the theoretical results in Theorem 2 and Corollary 1 only
hold for a shallow network in the form of (Affine, ReLU, Affine), we propose a greedy heuristic to
prune much deeper networks by applying the aforementioned optimization for consecutive blocks of
(Affine, ReLU, Affine) starting from the input and ending at the output of the network. This extension
from a theoretical study of 2 layer network was observed in several works such as (Bibi et al., 2018).
Experiments on Tropical Pruning. Here, we evaluate the performance of the proposed pruning
approach as compared to several classical approaches on several architectures and datasets. In
particular, we compare our tropical pruning approach against Class Blind (CB), Class Uniform (CU)
and Class Distribution (CD) (Han et al., 2015; See et al., 2016). In Class Blind, all the parameters
of a layer are sorted by magnitude where the x% with smallest magnitude are pruned. In contrast,
Class Uniform prunes the parameters with smallest x% magnitudes per node in a layer. Lastly, Class
Distribution performs pruning of all parameters for each node in the layer, just as in Class Uniform,
but the parameters are pruned based on the standard deviation σc of the magnitude of the parameters
per node. Since fully connected layers in deep neural networks tend to have much higher memory
complexity than convolutional layers, we restrict our focus to pruning fully connected layers. We
train AlexNet and VGG16 on SVHN, CIFAR10, and CIFAR100 datasets. We observe that we can
prune more than 90% of the classifier parameters for both networks without affecting the accuracy.
Since pruning is often a single block within a larger compression scheme that in many cases involves
inexpensive fast fine tuning, we demonstrate experimentally that our approach can is competitive
7
Under review as a conference paper at ICLR 2021
AIexNet on CIFAR10	AIexNet on CIFAR100
_ _ _ _
Oooo
8 6 4 2
% >us⊃uu<
——CB-AUC = 0.837
CU - AUC = 0.852
——CD - AUC = 0.843
——Ours-AUC = 0.885
AIexNet on SVHN
CB - AUC = 0.63
CU - AUC = 0.61
CD - AUC = 0.606
Ours-AUC = 0.668
% xus⊃uu<
_50	60	70	80	90	100
Pruning Rate %
VGG16on SVHN
_50	60	70	80	90	100
Pruning Rate %
VGG16 on CIFAR10
20	40	60	80	100
Pruning Rate %
VGG16 on CIFAR100
Figure 5: Results of Tropical Pruning. Pruning-accuracy plots for AlexNet (top) and VGG16 (bottom) trained
on SVHN, CIFAR10, and CIFAR100, pruned with our tropical method and three other pruning methods.
and sometimes outperforms other methods even when all parameters or when only the biases are
fine-tuned after pruning. These experiments in addition to many others are left for the appendix.
Setup. To account for the discrepancy in input resolution, we adapt the architectures of AlexNet and
VGG16, since they were originally trained on ImageNet (Deng et al., 2009). The fully connected
layers of AlexNet and VGG16 have sizes (256,512,10) and (512,512,10) for SVHN and CIFAR10,
respectively, and with the last dimension increased to 100 for CIFAR100. All networks were trained
to baseline test accuracy of (92%,74%,43%) for AlexNet on SVHN, CIFAR10, and CIFAR100,
respectively and (92%,92%,70%) for VGG16. To evaluate the performance of pruning and following
previous work Han et al. (2015), we report the area under the curve (AUC) of the pruning-accuracy
plot. The higher the AUC is, the better the trade-off is between pruning rate and accuracy. For
efficiency purposes, we run the optimization in Problem 2 for a single alternating iteration to identify
the rows in A and elements of B that will be pruned.
Results. Figure 5 shows the comparison between our tropical approach and the three popular pruning
schemes on both AlexNet and VGG16 over the different datasets. Our proposed approach can
indeed prune out as much as 90% of the parameters of the classifier without sacrificing much of
the accuracy. For AlexNet, we achieve much better performance in pruning as compared to other
methods. In particular, we are better in AUC by 3%, 3%, and 2% over other pruning methods on
SVHN, CIFAR10 and CIFAR100, respectively. This indicates that the decision boundaries can indeed
be preserved by preserving the dual subdivision polytope. For VGG16, we perform similarly well on
both SVHN and CIFAR10 and slightly worse on CIFAR100. While the performance achieved here is
comparable to the other pruning schemes, if not better, we emphasize that our contribution does not lie
in outperforming state-of-the-art pruning methods, but in giving a new geometry-based perspective to
network pruning. More experiments were conducted where only network biases or only the classifier
are fine-tuned after pruning. Retraining only biases can be sufficient, as they do not contribute to the
orientation of the decision boundaries polytope (and effectively the decision boundaries), but only to
its translation. Discussions on biases and more results are left for the appendix.
Comparison Against Tropical Geometry Approaches. A recent tropical geometry inspired approach
was proposed to address the problem of network pruning. In particular, Smyrnis & Maragos (2019;
2020) (SM) proposed an interesting yet heuristic algorithm to directly approximate the tropical
rational by approximating the Newton polytope. For fair comparison and following the setup of
SM, we train LeNet on MNIST and monitor the test accuracy as we prune its neurons. We report
(neurons kept, SM, ours) triplets in (%) as follows: (100, 98.60, 98.84), (90, 95.71, 98.82), (75,
95.05, 98.8), (50, 95.52, 98.71), (25, 91.04, 98.36), (10, 92.79, 97.99), and (5, 92.93, 94.91). Itis
clear that tropical pruning outperforms SM by a margin that reaches 7%. This demonstrates that our
theoretically motivated approach is still superior to more recent pruning approaches.
6	Tropical Adversarial Attacks
DNNs are notorious for being sensitive to imperceptible noise at their inputs referred to as adversarial
attacks. Several works investigated DNNs’ decision boundaries in the presence of such adversaries.
For instance, Khoury & Hadfield-Menell (2018) analyzed the high dimensional geometry of adversar-
8
Under review as a conference paper at ICLR 2021
ial examples by means of manifold reconstruction while He et al. (2018) crafted adversarial attacks
by estimating the distance to the decision boundaries using random search directions. In this work,
we show how Theorem 2 can be leveraged to construct a tropical geometric adversarial attack. Due
to the space limitation, we leave the extensive formulation, the algorithm to find the adversary, and
the experimental results on synthetic and real datasets to the appendix.
7	Conclusion
We leverage tropical geometry to characterize the decision boundaries of neural networks in the
form (Affine, ReLU, Affine) and relate it to geometric objects such as zonotopes. We then provide a
tropical perspective to support the lottery ticket hypothesis, prune networks, and design adversarial
attacks. A natural extension is a compact derivation for the characterization of the decision boundaries
of convolutional neural networks and graphical convolutional networks.
9
Under review as a conference paper at ICLR 2021
References
Marianne Akian, Stphane Gaubert, and Alexander Guterman. Tropical polyhedra are equivalent to
mean payoff games. International Journal of Algebra and Computation, 2009.
Xavier Allamigeon, Pascal Benchimol, Stphane Gaubert, and Michael Joswig. Tropicalizing the
simplex algorithm. SIAM J. Discrete Math. 29:2, 2015.
Diego Ardila, Atilla P. Kiraly, Sujeeth Bharadwaj, Bokyung Choi, Joshua J. Reicher, Lily Peng,
Daniel Tse, Mozziyar Etemadi, Wenxing Ye, Greg Corrado, David P. Naidich, and Shravya
Shetty. End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest
computed tomography. Nature Medicine, 2019.
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. In International Conference on Learning Representations,
2018.
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. In International Conference on Learning Representations, 2015.
Hans-Peter Beise, Steve Dias Da Cruz, and Udo Schroder. On decision regions of narrow deep neural
networks. arXiv preprint arXiv:1807.01194, 2018.
Leonard Berrada, Andrew Zisserman, and M Pawan Kumar. Trusting svm for piecewise linear cnns.
arXiv preprint arXiv:1611.02185, 2016.
Adel Bibi, Modar Alfadly, and Bernard Ghanem. Analytic expressions for probabilistic moments
of pl-dnn with gaussian input. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 9099-9107, 2018.
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et al. Distributed optimization
and statistical learning via the alternating direction method of multipliers. Foundations and
TrendsR in Machine learning, 2011.
Erwan Brugalle and Kristin Shaw. A bit of tropical geometry. The American Mathematical Monthly,
2014.
Nicholas Carlini and David A. Wagner. Towards evaluating the robustness of neural networks. CoRR,
2016.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In Computer Vision and Patter Recognition Conference (CVPR),
2009.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR. OpenReview.net, 2019.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of 13th International Conference on Artificial Intelligence and Statistics,
2010.
Peter Gritzmann and Bernd Sturmfels. Minkowski addition of polytopes: computational complexity
and applications to grobner bases. SIAM Journal on Discrete Mathematics, 1993.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for
efficient neural networks. CoRR, 2015.
Babak Hassibi and David G Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In Advances in neural information processing systems, 1993.
Warren He, Bo Li, and Dawn Song. Decision boundary analysis of adversarial examples. In
International Conference on Learning Representations (ICLR), 2018.
10
Under review as a conference paper at ICLR 2021
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recognition:
The shared views of four research groups. IEEE Signal Processing Magazine, 2012.
Ilia Itenberg, Grigory Mikhalkin, and Eugenii I Shustin. Tropical algebraic geometry. Springer
Science & Business Media, 2009.
Michael Joswig and Georg Loho. Monomial tropical cones for multicriteria optimization. AIP
Conference Proceedings, 2019.
Michael JosWig and Benjamin Schroter. The tropical geometry of shortest paths. arXiv preprint
arXiv:1904.01082, 2019.
Marc Khoury and Dylan Hadfield-Menell. On the geometry of adversarial examples. arXiv preprint
arXiv:1811.00525, 2018.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images.
Technical report, Citeseer, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet classification With deep con-
volutional neural netWorks. In Advances in Neural Information Processing Systems (NeurIPS),
2012.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural
information processing systems, 1990.
Yann LeCun, Y Bengio, and Geoffrey Hinton. Deep learning. Nature, 2015.
Yu Li, Peter Richtarik, Lizhong Ding, and Xin Gao. On the decision boundary of deep neural
netWorks. arXiv preprint arXiv:1808.05385, 2018.
Qinghua Liu, Xinyue Shen, and Yuantao Gu. Linearized admm for nonconvex nonsmooth optimiza-
tion With convergence analysis. IEEE Access, 2019.
D. Maclagan and B. Sturmfels. Introduction to Tropical Geometry. Graduate Studies in Mathematics.
American Mathematical Society, 2015.
Ngoc Mai Tran and Josephine Yu. Product-mix auctions and tropical geometry. Mathematics of
Operations Research, 2015.
D Melzer. On the expressibility of pieceWise-linear continuous functions as the difference of tWo
pieceWise-linear convex functions. In Quasidifferential Calculus. Springer, 1986.
Grigory Mikhalkin. Enumerative tropical algebraic geometry in r2. Journal of the American
Mathematical Society, 2004.
Guido Montufar, Razvan Pascanu, Kyunghyun Cho, and Y Bengio. On the number of linear regions
of deep neural netWorks. Advances in Neural Information Processing Systems (NeurIPS), 2014.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein FaWzi, Jonathan Uesato, and Pascal Frossard. Robust-
ness via curvature regularization, and vice versa. CVPR, 2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and AndreW Y Ng. Reading
digits in natural images With unsupervised feature learning. Advances in Neural Information
Processing Systems (NeurIPS), 2011.
Kristof Schutt, Farhad Arbabzadah, Stefan Chmiela, Klaus-Robert Muller, and Alexandre Tkatchenko.
Quantum-chemical insights from deep tensor neural netWorks. Nature Communications, 2017.
Abigail See, Minh-Thang Luong, and Christopher D. Manning. Compression of neural machine
translation models via pruning. CoRR, 2016.
Shai Shalev-ShWartz and Shai Ben-David. Understanding machine learning: From theory to
algorithms. Cambridge university press, 2014.
11
Under review as a conference paper at ICLR 2021
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Georgios Smyrnis and Petros Maragos. Tropical polynomial division and neural networks. arXiv
preprint arXiv:1911.12922, 2019.
Georgios Smyrnis and Petros Maragos. Multiclass neural network minimization via tropical newton
polytope approximation. International Conference on Machine Learning, ICML 2020, 2020.
Kerrek Stinson, David F Gleich, and Paul G Constantine. A randomized algorithm for enumerating
zonotope vertices. arXiv preprint arXiv:1602.06620, 2016.
Kaidi Xu, Sijia Liu, Pu Zhao, Pin-Yu Chen, Huan Zhang, Deniz Erdogmus, Yanzhi Wang, and Xue
Lin. Structured adversarial attack: Towards general implementation and better interpretability.
arXiv preprint arXiv:1808.01664, 2018.
Liwen Zhang, Gregory Naitzat, and Lek-Heng Lim. Tropical geometry of deep neural networks. In
International Conference on Machine Learning, ICML 2018, 2018.
Jian Zhou, Christopher Y. Park, Chandra L. Theesfeld, Aaron Wong, Yuan Yuan, Claudia Scheckel,
John Fak, Julien Funk, Kevin Yao, Yoko Tajima, Alan Packer, Robert Darnell, and Olga G. Troyan-
skaya. Whole-genome deep-learning analysis identifies contribution of noncoding mutations to
autism risk. Nature Genetics, 2019.
12
Under review as a conference paper at ICLR 2021
(a)
(b)
Figure 6: The Newton Polygon and the Corresponding Dualsubdivision. The Figure on the left
shows the newton polygon P(f) for the tropical polynomial defined in the second example in Figure
1. The dual subdivision δ(f) is constructed by projecting the upper faces of P(f), shadowing, on R2.
8	Preliminaries and Definitions.
Fact 1. P+Q = {p + q, ∀p ∈ P and q ∈ Q} is the Minkowski sum between two sets P and Q.
Fact 2. Let f be a tropical polynomial and let a ∈ N. Then
P(fa) = aP(f).
Let both f and g be tropical polynomials. Then
Fact 3.
P(f θ g) = P(f)+ P(g).	(3)
Fact 4.
P(f ㊉ g) = ConvexHull(V (P(g)) ∪ V (P(g)) ).	(4)
Note that V(P(f)) is the set of vertices of the polytope P(f).
Definition 7. Upper Face of a Polytope P: UF(P) is an upper face of polytope P in Rn if x+ten ∈/
P for any x ∈ UF(P), t > 0 where en is a canonical vector. Formally,
UF(P) = {x : x ∈ P, x + ten ∈/ P ∀t > 0}
9	Examples
We revise the second example in Figure 1. Note that the two dimensional tropical polynomial f(x, y)
can be written as follows:
f(x, y) = (χ ㊉ y ㊉ 0) Θ ((x 0 1)㊉(y Θ 1)㊉ 0)
=(X Θ X 0 1)㊉(X Θ y Θ 1)㊉(X Θ 0)㊉(y Θ X 0 1)㊉(y Θ y Θ 1)
㊉(y Θ 0)㊉(0 Θ X 0 1)㊉(0 Θ y Θ 1)㊉(0 Θ 0)
二 (x2 0 1)㊉(X Θ y Θ 1)㊉(x)㊉(y Θ X 0 1)㊉(y2 Θ 1)㊉(y)㊉(X 0 1)㊉(y Θ 1)㊉(0)
二 (x2 0 1)㊉(x Θ y Θ 1)㊉(x)㊉(y2 Θ 1)㊉(y Θ 1)㊉(0)
= (x2 Θ y0 0 1)㊉(x Θ y Θ 1)㊉(x Θ y0 Θ 0)㊉(x0 Θ y2 Θ 1)㊉(x0 Θ y Θ 1)㊉(x0 Θ y0 Θ 0)
13
Under review as a conference paper at ICLR 2021
First equality follows since multiplication is distributive in rings and semi rings. The second equality
follows since 0 is the multiplication identity. The penultimate equality follows since y 1 ≥ y,
x y 1 ≥ x y 1 and x ≥ x 1 ∀ x, y. Therefore, the tropical hypersurface T(f) is defined
as the of (x, y) where f achieves its maximum at least twice in its monomials. That is to say,
T(f) ={f (x, y) = (x2	1) = (xy	1)} ∪ {f (x, y) = x2	1 = x}∪
{(f(x,y) = x = 0}∪ {f (x, y) = x = xy 1}∪
{f (x, y) = y 1 = 0} ∪ {f (x, y) = y 1 = x y 1}∪
{f (x, y) = y 1 = y2	1} ∪ {f (x, y) = y2	1 = xy	1}.
This set T(f) is shown by the red lines in the second example in Figure 1. As for constructing the
dual subdivision δ(f), we project the upperfaces in the newton polygon P(f) to R2. Note that P(f)
with biases as per the definition in Section 2 is given as P(f) = ConvHull{(ai , ci) ∈ R2 × R ∀i =
1, . . . , 6} where (ai , ci) are the exponents and biases in the monomials of f, respectively. Therefore,
P(f) = ConvHull{(2, 0, -1), (1, 1, 1), (1, 0, 0), (0, 2, 1), (0, 1, 1), (0, 0, 0)} as shown in Figure 6(a).
As per Definition 7, the set of upper faces of P is:
UP(P(f)) = ConvHull{(0, 2, 1), (1, 1, 1), (0, 1, 1)} ∪ ConvHull{(0, 1, 1), (1, 1, 1), (1, 0, 0)}
∪ ConvHull{(0, 1, 1), (1, 0, 0), (0, 0, 0)} ∪ ConvHull{(1, 1, 1), (2, 0, -1), (1, 0, 0)}.
This set UP(P (f)) is then projected, through π, to R2 shown in the yellow dashed lines in Figure
6(a) to construct the dual subdivision δ(f) in Figure 6(b). For example, note that the point (0, 2, 1) ∈
UF(f) and thereafter, π(0, 2, 1) = (0, 2, 0) ∈ δ(f).
10	Proof Of Theorem 2
Theorem 2. For a bias-free neural network in the form of f(x) : Rn → R2 where A ∈ Zp×n and
B ∈ Z2 ×p, let R(x) = Hι(x) Θ Q2(x)㊉ H2(x) Θ Qι(x) be a tropical polynomial. Then:
•	Let B = {x ∈ Rn : f1(x) = f2 (x)} define the decision boundaries of f, then B ⊆
T (R(x)).
•	δ (R(x)) = ConvHull (ZG1 , ZG2 ). ZG1 is a zonotope in Rn with line segments
{(B+(1,j)+B-(2,j))[A+(j,:),A-(j,:)]}jp=1andshift(B-(1,:)+B+(2,:))A-. ZG2
is a zonotope in Rn with line segments {(B- (1, j) + B+(2, j))[A+(j, :), A- (j, :)]}jp=1
and shift (B+ (1, :) + B-(2, :))A-. The line segment (B+(1,j) + B-(2,j))[A+(j, :
), A- (j, :)] has end points A+(j, :) and A- (j, :) in Rn and scaled by (B+(1, j) +
B-(2,j)).
Note that A+ = max(A, 0) and A- = max(-A, 0) where the max(.) is element-wise. The line
segment (B(1,j)+ + B(2, j)-)[A(j, :)+, A(j, :)-] is one that has the end points A(j, :)+ and
A(j, :)- in Rn and scaled by the constant B(1,j)+ + B(2, j)-.
Proof. For the first part, recall from Theorem1 that both f1 and f2 are tropical rationals and hence,
f1(x) = H1(x) - Q1(x)	f2(x) = H2(x) - Q2(x)
Thus
B = {x ∈ Rn	:	f1(x) = f2(x)}	= {x ∈ Rn : H1(x)	-	Q1(x)	= H2(x)	-	Q2(x)}
= {x ∈ Rn	:	H1(x) + Q2(x)	= H2(x) + Q1(x)}
= {x ∈ Rn	:	H1(x) Θ Q2(x)	= H2(x) Θ Q1(x)}
Recall that the tropical hypersurface is defined as the set of x where the maximum is attained by two
or more monomials. Therefore, the tropical hypersurface of R(x) is the set of x where the maximum
is attained by two or more monomials in (H1(x) Θ Q2(x)), or attained by two or more monomials in
14
Under review as a conference paper at ICLR 2021
(H2(x)	Q1(x)), or attained by monomials in both of them in the same time, which is the decision
boundaries. Hence, we can rewrite that as
T (R(x)) = T (H1(x)	Q2(x)) ∪ T (H2(x)	Q1(x)) ∪ B.
Therefore B ⊆ T (R(x)). For the second part of the Theorem, we first use the decomposition pro-
posed by Zhang et al. (2018); Berrada et al. (2016) to show that for a network f(x) = B max (Ax, 0),
it can be decomposed as tropical rational as follows
f (x) = (B+ — B-) (max(A+x, A-X) — A-X)
= B+ max(A+x, A-x) + B-A-x
— B- max(A+X, A-X) + B+A-X .
Therefore, we have that
H1 (x) + Q2(x)
H2(x) + Q1 (x)
= B+(1, :) + B- (2, :) max(A+x, A-x)
+B-(1,:)+B+(2,:)A-x
= B-(1, :) + B+(2, :) max(A+x, A-x)
+ B+(1,:) + B-(2,:)A-x.
Therefore, note that:
δ(R(x)) = δ ((HI(X) Θ Q2(x))㊉(H2(x) Θ QI(X)))
= ConvexHull δ (H1(X)	Q2(X)), δ(H2(X)	Q1(X))
=ConvexHUll ( δ(Hi (x)) + δ(Q2 (x)) ,δ (H (x)) + δ(Q ι(x))
Now observe that H1(X) = Pjp=1 B+(1, j) + B-(2, j) max A+ (j, :), A- (j, :)X tropically is
given as follows Hι(x) = Θj= [xA+ (j,:)㊉ XA-j,[	'	( "[thus We have that:
δ(H1(x)) =(B+ (1,1) + B-(2,1)) δ (XA+(1，:)㊉ XA-(I,：)) + ...
+ (B+(1,p) + B-(2,p))(δ(XA+(p，:)㊉ XA-(P，:)))
=(B+ (1,1) + B-(2,1))ConvexHull(A+(1,:), A-(1,:))+ ...
+(B+(1,p) + B-(2,p))ConvexHUll(A+(p,:), A-(p,:)).
The operator + indicates a MinkoWSki sum between sets. Note that ConvexHull(A+(i,:), A-(i,:
) is the convexhull between two points which is a line segment in Zn with end points that are
{A+(i, :), A- (i, :)} scaled with B+(1, i) + B-(2, i). Observe that δ(F1(X)) is a Minkowski sum
of line segments which is is a zonotope. Moreover, note that Q2 (X) = (B- (1, :) + B+ (2, :))A-X
tropically is given as follows Q2(X) = Θp=ιXA (j,:)(B (I,j)0B (2,j)). One cansee that δ(Q2(X)) is
the Minkowski sum of the points {(B-(1,j) 一 B+(2,j))A-(j,:)}Vjin Rn (which is a standard
SUm) resulting in a point. Lastly, 6(Hi(x))+δ(Q2(X)) is a Minkowski sum between a zonotope and
a single point which corresponds to a shifted zonotope. A similar symmetric argument can be applied
for the second part δ(H(x))+6(Qi(x)).	□
15
Under review as a conference paper at ICLR 2021
It is also worthy to mention that the extension to network with multi class output is trivial. In that case
all of the analysis can be exactly applied studying the decision boundary between any two classes
(i, j) where B = {x ∈ Rn : fi(x) = fj (x)} and the rest of the proof will be exactly the same.
11 Proof of Proposition 1
Proposition 1. The zonotope formed by p line segments in Rn with arbitrary end points {[ui1, ui2]}ip=1
is equivalent to the zonotope formed by the line segments {[ui1 - ui2 , 0]}ip=1 with a shift of ip=1 ui2.
Proof. Let Uj be a matrix with Uj(:, i) = uij, i = 1, . . . ,p, w be a column-vector with w(i) =
wi, i = 1, . . . ,p and 1p is a column-vector of ones of length p. Then, the zonotope Z formed by the
Minkowski sum of line segments with arbitrary end points can be defined as:
p
Z =	Xwiui1 + (1 - wi)ui2; wi ∈ [0, 1], ∀ i
i=1
= nU1w-U2w+U21p, w∈ [0,1]po
= n(U1-U2)w+U21p, w∈ [0,1]po
=n (Ui- U2) w, W ∈ [0,1]p} + {U2lp}.
Since the Minkowski sum of between a polytope and a point is a translation; thereafter, the proposition
follows directly from Definition 6.	□
Corollary 2. The generators of ZG1 , ZG2 in Theorem 2 can be defined as G1	=
Diag[(B+(1, :)) + (B- (2, :))]A and G2 = Diag[(B+(2, :)) + (B-(1, :))]A, both with shift
(B-(1, :) + B+ (2, :) + B+(1, :) + B- (2, :)) A-, where Diag(v) arranges v in a diagonal matrix.
Proof. This follows directly by applying Proposition 1 to the second bullet point of Theorem 2. □
11.1 Optimization of Objective 2 of the B inary Classifier
min 2 BG 1-G11十
1
G2 - G2
22	2
F+λ1BG Ii+λ2BG 2IL
(5)
- ~, ,二,. , ~ , . . . ∖ ~ ~. ,二,. 、、
Note that Gi = Diag ReLU(B(1,:)) + ReLU(-B(2,:)) A, G2 = Diag ReLU(B(2,:)) +
ReLU(-B(1,:))] A. Note that Gi = DiagIReLU(B(1,:)) + ReLU(-B(2,:))] A and G2 =
Diag ReLU(B(2, :)) + ReLU(-B(1, :)) A.
For ease of notation, we refer to ReLU(B(i,:)) and
ReLU(-B(i, :)) as B+(i, :) and B- (i, :), respectively. We solve the problem with co-ordinate
descent an alternate over variables.
Update A.
A — argmin1
A 2
Diag (ci) A - Gι∣∣p + 2 BDiag(c2)A - G2BF + λι BDiag(CI)AB? ɪ + λ2
Diag(C2)A∣L i
where ci = ReLU(B(1, :)) + ReLU(-B(2, :)) and c2 = ReLU(B(2, :)) + ReLU(-B(1, :)). Note
that the problem is separable per-row of A. Therefore, the problem reduces to updating rows of A
independently and the problem exhibits a closed form solution.
16
Under review as a conference paper at ICLR 2021
~ . .
A(i,:)
1
arg min 一
A(i,:)	2
KIA(i,:) - Gι(i, :)||2 + 2 *2A(i,:) - G2(i, :)||2 + (λιq∕c1 + λ2^Ci)IA(i, :)||2
1
arg min 一
A(i,:)	2
clGι(i,:) + c2G2(i,:)
2(C1 + c2)
2
2
+ 1 λι √Cf + λ2 √C∣
2	2(C1 + c2)
2
1 λι√Ci + λ2√C∣	1	∖ f clGι(i,:)+ c2G2(i,:)
2	1 (C1+c2)	||c1Gl2t+g2(i，:) ||2,八-1(C^-
Update B +(1,:).
~ I
B+ (1,:)
1
arg min -
B+(1,：) 2
||Diag (B +(1,:)) A - C1||： + λι ||Diag (B +(1,:)) A + C2( ɪ
s.t. B +(1,:) ≥ 0.
Note that Ci = Gi - Diag (B-(2,:)) A and where Diag (B-(2,:)) A. Note the problem is
separable in the coordinates of B +(1,:) and a projected gradient descent can be used to solve the
problem in such a way as:
B + (1,j) = argmin 1 ||B+(1,j)A(j,:) - Ci(j, :)||： + λι ||B +(1,j )A(j,:) + C2(j,:)( , s.t. B+(1,j) ≥ 0.
A similar symmetric argument can be used to update the variables B+(2, :), B+(1, :) and B- (2, :).
12 Adapting Optimization 2 for Multi-Class Classifier
Note that Theorem 2 describes a superset to the decision boundaries of a binary classifier through
the dual subdivision R(x), i.e. δ(R(x)). For a neural network f with k classes, a natural extension
for it is to analyze the pair-wise decision boundaries of of all k-classes. Thus, let T (Rij (x)) be the
superset to the decision boundaries separating classes i and j . Therefore, a natural extension to the
geometric loss in Equation 1 is to preserve the polytopes among all pairwise follows:
min d ConvexHull
A,B ∀[i,j]∈S
(ZG (i+,j-)
ConvexHull ZG(i+,j-), ZG(j+,i-)	. (6)
The set S is all possible pairwise combinations of the k classes such that S = {{i, j }, ∀i 6=
j, i = 1, . . . , k, j = 1, . . . , k}. The generator Z(G(i,j)) is the zonotope with the generator matrix
G(i+,j-) = Diag ReLU(B(i, :)) + ReLU(-B(j, :)) A. However, such an approach is generally
computationally expensive, particularly, when k is very large. To this end, we make the following
observation that G (计,j-)can be equivalently written as a Minkowski sum between two sets zonotopes
with the generators Gi+ = Diag ∣ReLU(B(i,:)] A and Gj- = Diag ∣ReLU(Bj-)] A. That is to
〜
say, Zg(十 _)= ZGa十+Zg》_. ThiS follows from the associative property of Minkowski sums
given as follows:
Fact 5. Let {Si}in=1 be the set of n line segments. Then we have that
~ ~
〜
S = Si+ ... + Sn = P+V
where the sets P = +j∈C1 Sj and V = +j∈C2 Sj where C1 and C2 are any complementary partitions
of the set {Si }in=1.
17
Under review as a conference paper at ICLR 2021
ɪ ɪ	1	,	, ♦ F ,	1	EI	.Λ 1 ∙	♦/	1
Hence, G(i+,j-)can be seen a concatenation between Gi+ and Gj-. Thus, the objective in 6 can be
expanded as follows:
min ɪ2 d (ConvexHull
A，B ∀{i,j}∈S
(十._), Zg(十 _ J ,ConvexHull
十，厂),ZG(j+,i-)))
min X d(,
a,b ∀{i,j}∈S
ConvexHull (ZG 十 + ZdL，ZG十 + ZG J , ConvexHull
〜
+ZGj-, ZG+ + ZGi-
2
Gi--Gi-IlF
Gi-
Gj十
min	X I IIggi+- Gi+11尸+；
A，B ∀{ij∈S 211	11f 2
+2 IlG j+- Gj十
+2 IIGj-- Gj-IIF
m,in ⅛1 XUG i+
,	i=1
-Gi 十］WG i- - Gi-IIF
〜
〜
〜
〜
2
F
2
F
The approximation follows in a similar argument to the binary classifier case. The last equality follows
from a counting argument. We solve the objective for all multi-class networks in the experiments
with alternating optimization in a similar fashion to the binary classifier case. Similarly to the binary
classification approach, We introduce the ∣∣. ∣∣2,1 to enforce sparsity constraints for pruning purposes.
Therefore the overall objective has the form:
m in 2 XIIGi 十-Gi 十 L+iiGi--Gi- L+λ QGi 十 IL+iiGi- Li
, i=1	,	,
1—1	1	1	∙	, 1	1	, i' ɪ	F ^ΓΛ
For completion, we derive the updates for A and B.
Update A.
A = arg 1 (∣∣Diag
A i=1
A - Gi+∣∣2 + ∣∣Diag (B-(i,:
+λ	IIDiag (B+(i,：))A∣∣2,1+∣∣Diag (B飞 J)A∣∣2,ι
Similar to the binary classification, the problem is separable in the rows of A. and a closed form
solution in terms of the proximal operator of '2 norm follows naturally for each A(i,:).
Update B +(i,:).
~ I
B+(i,：)
-1
arg min -
B+(i,：) 2
∣∣Diag (B +(i,:)) A - -1∣2 + λ ∣∣Diag (B +(i,:)) A∣∣? ɪ
s.t. B +(i,:) ≥ 0.
Note that the problem is separable per coordinates of B+(i,:) and each subproblem is updated as:
B+ (i,j) = arg min 1 ∣∣B +(i, j)A(j,:) - G^ (j,:)(+ λ ∣∣B +(i, j)A(j,:)( , s.t. B +(i,j) ≥ 0
=arg min 2 ∣∣B +(i, j)A(j,:) - G^ (j,:)(+ λ ∣B(i, j )∣ ∣∣A(j,:)( , s.t. B +(i, j) ≥ 0
ma√0 A(j,:)TG什 j,:)- λ∣∣A(j,:)G!
IliaX 0,	.
kA(j, :)k2	J
A similar argument can be used to update B (i,:) ∀i. Finally, the parameters of the pruned network
• 11 i	-> ɪ	T 1 -rʌ -rʌ -L -rʌ 一
will be constructed A — A and B - B+ - B .
18
Under review as a conference paper at ICLR 2021
13 Tropical Adversarial Attacks.
Dual View to Adversarial Attacks. For a classifier f : Rn → Rk and input x0 classified as c, a
standard formulation for targeted adversarial attacks to a different class t is defined as:
min D(η) s.t. arg max fi(x0 + η) = t 6= c	(7)
ηi
This objective aims to compute the lowest energy input noise η (measured by D) such that the the
new sample (x0 + η) crosses the decision boundaries of f to a new classification region. Here, we
present a dual view to adversarial attacks. Instead of designing a sample noise η such that (x0 + η)
belongs to a new decision region, one can instead fix x0 and perturb the network parameters to move
the decision boundaries in a way that x0 appears in a new classification region. In particular, let
A1 be the first linear layer of f, such that f(x0) = g(A1x0). One can now perturb A1 to alter the
decision boundaries and relate this parameter perturbation to the input perturbation as follows:
g((A1 + ξA1)x0) = g (A1x0 +ξA1x0) = g(A1x0 + A1η) = f(x0 + η).	(8)
From this dual view, we observe that traditional adversarial attacks are intimately related to perturbing
the parameters of the first linear layer through the linear system: A1η = ξA1x0. The two views and
formulations are identical under such condition. With this analysis, Theorem 2 provides explicit
means to geometrically construct adversarial attacks by perturbing the decision boundaries. In
particular, since the normals to the dual subdivision polytope δ(R(x)) of a given DNN represent
the tropical hypersurface T (R(x)), which is a superset to the decision boundaries set B, ξA1 can be
designed to sufficiently perturb the dual subdivision resulting in a change in the network prediction of
x0 to the targeted class t. Based on this observation, we design an optimization problem that generates
two sets of perturbations, an input perturbation and parameter perturbation, that are equivalent to
each other.
Formulation. Based on this observation, we formulate the problem as follows:
min	D1 (η) + D2 (ξA1)	s.t. - loss(g(A1(x0 + η)), t) ≤ -1;	kηk∞ ≤ 1;
η,ξA1	(9)
- loss(g(A1 + ξA1)x0,t) ≤ -1; (x0 + η) ∈ [0, 1]n,	kξA1 k∞,∞ ≤ 2, A1η = ξA1x0.
The loss is the standard cross-entropy loss. The first row of constraints ensures that the network
prediction is the desired target class t when the input x0 is perturbed by η, and equivalently by
perturbing the first linear layer A1 by ξA1 . This is identical to f1 as proposed by Carlini & Wagner
(2016). Moreover, the third and fourth constraints guarantee that the perturbed input is feasible
and that the perturbation is bounded, respectively. The fifth constraint is to limit the maximum
perturbation on the first linear layer, while the last constraint enforces the dual equivalence between
input perturbation and parameter perturbation. The function D2 captures the perturbation of the
dual subdivision polytope upon perturbing the first linear layer by ξA1 . For a single hidden layer
neural network parameterized as (A1 + ξA1 ) ∈ Rp×n and B ∈ R2×p for the first and second layers
respectively, D2 can capture the perturbations in each of the two zonotopes discussed in Theorem 2
and we define it as:
12	2	2
D2(ξA1) = 2 £ IlDiag(B+(j, ：))Sa/f + IlDiag(B-(j,：)低】||尸∙	(10)
j=1
We solve Problem (9) with a penalty method on the linear equality constraints, where each penalty
step is solved with ADMM Boyd et al. (2011) in a similar fashion to the work of Xu et al. (2018).
The function D2 (ξA) captures the perturbation in the dual subdivision polytope such that the
dual subdivision of the network with the first linear layer A1 is similar to the dual subdivision
of the network with the first linear layer A1 + ξA1 . This can be generally formulated as an ap-
Proximation to the following distance function d(ConvHUll (Zcgi , ZG2), ConvHUll (Zgi , ZG2 )
where G1 = Diag ReLU(B(1,:)) 十
))+ ReLU(-B(1, :))i (A + ξA1), G
[,二,. ，二，.
ReLU(B(2,:)) + ReLU(-B(1,:
ReLU(-B(2,:))] (A + ξA1), G2 = DiagiReLU(B(2,:
[,二,. , ~ , . . . \ ~
ReLU(B(1,:)) + ReLU(-B(2,:)) A and G2
))
A. In ParticUlar, to aPProximate the fUnction d, one can
19
Under review as a conference paper at ICLR 2021
use a similar argument as in used in network pruning 5 such that D2 approximates the generators of
the zonotopes directly as follows:
D2(ξAι )=1 BG 1- Gd∣F+1BG 2 -Mf
=11∣Diag(B+(1, *Aι∣∣F+11∣Diag(B-(I, *A1∣∣F
+21∣Diag(B+⑵：))£ai ∣∣F+11∣Diag(B-⑵ ^ξAi ∣∣F.
This can thereafter be extended to multi-class network with k classes as follows D2(ξA1 ) =
2 Pk=1 ∣∣Diag(B+(j, 9)ξAι∣∣尸 + ∣∣Diag(B-(j, j)ξAι∣1. Following XUetal.(2018 ), we take
D1(η) = 1 ∣∣ηk2. Therefore, we can write 9 as follows:
min
η,ξA
D1(η) + X ∣∣Diag(B+(j, :。ξA∣∣2 + ∣∣Diag(B-(j, ]ξA∣∣2 ∙
j=1	F	F
s.t. - loss(g(A1(x0 + η)), t) ≤ -1,	-loss(g((A1 + ξA1)x0),t) ≤ -1,
(x0 +η)	∈	[0,1]n,	∣η∣∞ ≤	1,	∣ξA1	∣∞,∞ ≤ 2, A1η -	ξA1 x0	=	0.
To enforce the linear eqUality constraints A1η - ξA1 x0 = 0, we Use a penalty method, where each
iteration of the penalty method we solve the sUb-problem with ADMM Updates. That is, we solve
the following optimization problem with ADMM with increasing λ sUch that λ → ∞. For ease
of notation, lets denote L(x0 + η) = -loss(g(A1(x0 + η)),t), and L(A1) = -loss(g((A1 +
ξA1 )x0), t).
min
η,z,w,ξA1
s.t.
kηk2 + X ∣∣Diag(ReLU(B(j, ：))€ai ∣∣2 + ∣∣Diag(ReLU(-B(j, ^)ξAι ∣∣2
j=1	F	F
+ L(XO + z) + h1(w) + h2(ξAι ) + λk AIn - ξAι x0k2 + L(A1)∙
η = z z = w.
where
h1(η)=	0∞,
if (X0 +η) ∈ [0,1]n, ∣η∣∞ ≤ 1
else
h2(ξA1)=	0∞,
if ∣ξA1 ∣∞,∞ ≤ 2
else
The aUgmented Lagrangian is given as follows:
k
L(n, w, z, ξAι, U, V) ：= knk2 + L(X0 + z) + h1(w) + X ∣∣Diag(B+(j, ：))€ai ∣∣F + ∣∣Diag(B-(j, ：))€ai ∣∣F
j=1
+ L(AI) + h2(ξAι) + λ∣A1n 一 ξAι X0∣∣2 + u>(n — z) + v>(w — Z)
+ P (In—zk2 + kw - zk2)∙
Thereafter, ADMM Updates are given as follows:
{nk+1, wk+1} = argminL(n,w,zk,ξAk 1,uk,vk),
η,w
zk+1 = arg min L(nk+1, wk+1, z, ξAk , uk, vk),
ξAk+1 = arg min L(nk+1, wk+1, zk+1, ξA1, uk, vk)∙
1	ξA1
20
Under review as a conference paper at ICLR 2021
uk+1 = uk + ρ(ηk+1 - zk+1), vk+1 = vk + ρ(wk+1 - zk+1).
Updating η:
ηk+1 = arg min Ilηk2 + λk AIn - ξAι χok2 + u>η + P ∣∣η - zk2
η2
= 2λA1>A1 + (2 + ρ)I-12λA1>ξAk 1 x0 + ρzk -uk.
Updating w:
wk+1 = arg min vk> W + hι(w) + PkW - zk∣2
w2
1
arg min -
w2
w- z
k vk 2
+ 1 hι(w).
ρ
—
ρ 2
The update w is separable in coordinates as follows:
{min(1 — xo,eι)
max(-x0, -1)
Zk — 1∕ρVk
:zk — 1∕ρvk > min(1 — xo, eι)
: zk - 1∕ρvk < max(-x0, -e1)
: otherwise
Updating z:
zk+1 = argminL(xo + z) — uk>z — vk>z + P (∣∣nk+1 — z∣∣2 + ∣∣wk+1 — z∣∣2).
z2
Liu et al. (2019) showed that the linearized ADMM converges for some non-convex problems.
Therefore, by linearizing L and adding Bergman divergence term ηk∕2kz — zkk22, we can then update
z as follows:
zk+1 = ɪ— (ηkzk + ρ(nk+1 + 1 uk + wk+1 + 1 vk) — VL(Zk + x。)).
ηk + 2P	P	P
Itis worthy to mention that the analysis until this step is inspired by Xu et al. (2018) with modifications
to adapt our new formulation.
Updating ξA :
ξA+1 = argmin ∣ξAιkF + λ∣ξAιxo - Aιη∣2 + L(AI) s.t. ∣ξAιk∞,∞ ≤ ea
ξA
The previous problem can be solved with proximal gradient methods.
Experimental Setup. For the tropical adversarial attacks experiments, there are five different hyper
parameters which are
e1 : The upper bound for the infinite norm of δ.
e2 : The upper bound for thek.k∞,∞of the perturbation on the first linear layer.
λ : Regularizer to enforce the equality between input perturbation and first layer perturbation
η : Bergman divergence constant.
P : ADMM constant.
21
Under review as a conference paper at ICLR 2021
Algorithm 1: Solving Problem (9)
Input: Ai ∈ Rp×n,B ∈ Rk×p,xo ∈ Rn,t,λ> 0,γ > 1,K> 0,ξA1 = 0p×n,η1 = z1 = w1 =
z1 = u1 = w1 = 0n.
Output: η, ξA1
Initialize: ρ = ρ0
while not converged do
for k ≤ K do
η update: ηk+1 = (2λA1>A1 + (2 + ρ)I)-1 (2λA1>ξAk x0 + ρzk - uk)
min(1 — x0, e1)	:	Zk —	1∕ρvk	> min(1 — x0, e1)
W update: wk+i =	max(-x0, -e1)	:	Zk —	1∕ρvk	< max(-x0, -e1)
Izk — 1∕ρvk	: otherwise
z update: zk+i = ηk+1+2ρ (ηk+1 zk + ρ(ηk+i + 1∕ρuk + Wk + 1∕ρvk) — VL(zk + x0))
ξA1 update:
ξA+1 =argminξA kξAιkF + λkξAιx0- AIηk+11∣2 + L(AI)StIlξAιk∞,∞ ≤ e2
u update: uk+1 = uk + ρ(ηk+1 - Zk+1)
v update: vk+1 = vk + ρ(Wk+1 - Zk+1))
L P J YP
λ J γλ
PJP0
Figure 7: Dual View of Tropical Adversarial Attacks. We show the effects of tropical adversarial attacks on
a synthetic binary dataset at two different input points (in black). From left to right: the decision regions of the
original and perturbed models, and decision boundaries polytopes (green for original and blue for perturbed).
For all of the experiments, we set the values of 2, λ, η and P to 1, 10-3, 2.5 and 1, respectively. As
for 1 it is set to 0.1 upon attacking MNIST images of digit 4 set to 0.2 for all other MNIST images.
Motivational Insight to the Dual View. We train a network with 2 inputs, 50 hidden nodes and 2
outputs on a synthetic dataset where we then then solve Equation 9 for a given x0 shown in black in
Figure 7. We show the decision boundaries with and without the perturbation ξA1 at the first linear
layer. As show in Figure 7, perturbing an edge of the dual subdivision polytope, by perturbing the first
linear layer, corresponds to perturbing the decision boundaries and results in the misclassification of
x0 . As expected, perturbing different decision boundaries corresponds to perturbing different edges
of the dual subdivision. Note that the generated input perturbation η is sufficient as well into fooling
the network in classifying x0 + η, and by construction is equivalent to perturb the decision boundaries
of the network. We show later another example where we alternate the position of x0 and construct
successful adversaries in both the input space, and the parameter space. Furthermore, we conduct
experiments on MNIST images in a later section, which show that successful adversarial attacks
η can be designed by solving Problem (9). Figure 7 shows another example where the sampled to
be attacked is closer to a different decision boundary. Observe how the edge corresponding to that
decision boundary of the decision boundary polytope has respectively been altered.
MNIST Experiments. Here, we design perturbations to misclassify MNIST images. Figure 8 shows
several adversarial examples that change the network prediction for digits 8 and 9 to digits 7, 5,
and 4, respectively. In some cases, the perturbation η is as small as = 0.1, where x0 ∈ [0, 1]n.
Several other adversarial results are reported in Figure 9. We again emphasize that our approach is
22
Under review as a conference paper at ICLR 2021
not meant to be compared with (or beat) state of the art adversarial attacks but rather to provide a
novel geometrically inspired perspective that can shed new light in this field.
Figure 8: Effect of Tropical Adversarial Attacks on MNIST Dataset. We show qualitative examples of
adversarial attacks, produced by solving Problem (9), on two digits (8,9) from MNIST. From left to right, images
are classified as [8,7,5,4] and [9,7,5,4] respectively.
Figure 9: Effect of Tropical Adversarial Attacks on MNIST Images. First row from the left:
Clean image, perturbed images classified as [7,3,2,1,0] respectively. Second row from left: Clean
image, perturbed images classified as [9,8,7,3,2] respectively. Third row from left: Clean image,
perturbed images classified as [9,8,7,5,3] respectively. Fourth row from left: Clean image, perturbed
images classified as [9,4,3,2,1] respectively. Fifth row from left: Clean image, perturbed images
classified as [8,4,3,2,1] respectively.
14	Experimental Details and Supplemental Results
In this section, we describe the settings and the values of the hyper parameters used in the experiments.
Moreover, we will show some further supplemental results to the results in the main manuscript
paper.
23
Under review as a conference paper at ICLR 2021
14.1	Tropical View to the Lottery Ticket Hypothesis.
We first conduct some further supplemental experiments to those conducted in Section 4. In particular,
we conduct further experiments re-affirming the lottery ticket hypothesis on three more synthetic
datasets in a similar experimental setup to the one shown in Figure 3. The new supplemental experi-
ments are shown in Figure 10. A similar conclusion is present where the lottery ticket initialization
consistently better preserves the decision boundaries polytope compare to other initialization schemes
over different percentages of pruning.
Figure 10: Effect of Different Initializations on the Decision Boundaries Polytope. From left to right:
training dataset, decision boundaries polytope of original network followed by the decision boundaries polytope
during several iterations of pruning with different initializations.
A natural question is whether it is necessary to visualize the dual subdivision polytope of the decision
boundaries, i.e. δ(R(x)), where R(X) = Hι(x) Θ Q2(x)㊉H(x) Θ Qι(x) as opposed to visualizing
the tropical polynomials δ(H{1,2}(x)) and δ(Q{1,2}(x)) directly for the tropical re-affirmation of
the lottery ticket hypothesis. That is similar to asking whether it is necessary to visualize and study
the decision boundaries polytope δ(R(x)) as compared to the the dual subdivision polytope of the
functional form of the network since for the 2-output neural network described in Theorem 2 we have
that f1(x) = H1(x) Q1(x) and f2(x) = H2(x) Q2(x). We demonstrate this with an experiment
that demonstrates the differences between these two views. For this purpose, we train a single hidden
layer neural network on the same dataset shown in Figure 3. We perform several iterations of pruning
in a similar fashion to Section 5 and visualise at each iteration both the decision boundaries polytope
and all the dual subdivisions of the aforementioned tropical polynomials representing the functional
form of the network, i.e. δ(H{1,2} (x)) and δ(Q{1,2} (x)). It is to be observed from Figure 11 that
despite that the decision boundaries were barely affected with the lottery ticket pruning, the zonotopes
representing the functional form of the network endure large variations. That is to say, investigating
the dual subdivisions describing the functional form of the networks through the four zonotopes
δ(H{1,2}(x)) and δ(Q{1,2}(x)) is not indicative enough to the behaviour of the decision boundaries.
14.2	Tropical Pruning
Toy Setup. To verify our theoretical work, we first start by pruning small networks that are in the
form of Affine followed by ReLU followed by another Affine layer. We train the aforementioned
network on two 2D datasets with a varying number of hidden nodes (100, 200, 300). In this setup,
we observe from Figure that when Theorem 2 assumptions hold, our proposed tropical pruning is
indeed competitive, and in many cases outperforms, the other non decision boundaries aware pruning
schemes.
Experimental Setup. In all experiments of the tropical pruning section, all algorithms are run
for only a single iteration where λ increases linearly from 0.02 with a factor of 0.01. Increasing λ
corresponds to increasing weight sparsity and we keep doing until sparsification is 100%.
24
Under review as a conference paper at ICLR 2021
Figure 11: Comparison between the decision boundaries polytope and the polytopes represent-
ing the functional representation of the network. First column: decision boundaries polytope
δ(R(x)) while the remainder of the columns are the zonotopes δ(H1(x)), δ(Q1 (x)), δ(H2(x)) and
δ(Q2(x)) respectively. Under varying pruning rate across the rows, it is to be observed that the
changes that affected the dual subdivisions of the functional representations are far smaller compared
to the decision boundaries polytope.
Figure 12: Pruning Ressults on Toy Networks. We apply tropical pruning on the toy network that
is in the form of Affine followed by a ReLU followed by another Affine. From left to right: (a) dataset
used for training (b) pruning networks with 100 hidden nodes (c) 200 hidden nodes (d) 300 hidden
nodes.
Supplemental Experiments. We conduct more experimental results on AlexNet and VGG16 on
SVHN, CIFAR10 and CIFAR100 datasets. We examine the performance for when the networks have
only the biases of the classifier fine tuned after tuning as shown in Figure 13. Moreover, a similar
experiments is reported for the same networks but for when the biases for the complete networks are
fine tuned as in Figure 14.
25
Under review as a conference paper at ICLR 2021
求
50	60	70	80	90	100
Pruning Rate %
VGG16 on CIFAR10
VGG16 on SVHN
Pruning Rate %
Pruning Rate %
Figure 13: Results of Tropical Pruning with Fine Tuning the Biases of the Classifier. Tropical
pruning applied on AlexNet and VGG16 trained on SVHN, CIFAR10, CIFAR100 against different
pruning methods with fine tuning the biases of the classifier only.
AIexNet on CIFAR10
50	60	70	80	90	100
Pruning Rate %
Figure 14: Results of Tropical Pruning with Fine Tuning the Biases of the Network. Tropical
pruning applied on AlexNet and VGG16 trained on SVHN, CIFAR10, CIFAR100 against different
pruning methods with fine tuning the biases of the network.
26