Under review as a conference paper at ICLR 2021
Shape Matters: Understanding the Implicit
Bias of the Noise Covariance
Anonymous authors
Paper under double-blind review
Ab stract
The noise in stochastic gradient descent (SGD) provides a crucial implicit reg-
ularization effect for training overparameterized models. Prior theoretical work
largely focuses on spherical Gaussian noise, whereas empirical studies demon-
strate the phenomenon that parameter-dependent noise — induced by mini-
batches or label perturbation — is far more effective than Gaussian noise. This pa-
per theoretically characterizes this phenomenon on a quadratically-parameterized
model introduced by Vaskevicius et al. and Woodworth et al. We show that in
an over-parameterized setting, SGD with label noise recovers the sparse ground-
truth with an arbitrary initialization, whereas SGD with Gaussian noise or gradi-
ent descent overfits to dense solutions with large norms. Our analysis reveals that
parameter-dependent noise introduces a bias towards local minima with smaller
noise variance, whereas spherical Gaussian noise does not.
1	Introduction
One central mystery of deep artificial neural networks is their capability to generalize when having
far more learnable parameters than training examples Zhang et al. (2016). To add to the mystery,
deep nets can also obtain reasonable performance in the absence of any explicit regularization. This
has motivated recent work to study the regularization effect due to the optimization (rather than
objective function), also known as implicit bias or implicit regularization Gunasekar et al. (2017;
2018a;b); Soudry et al. (2018); Arora et al. (2019). The implicit bias is induced by and depends on
many factors, such as learning rate and batch size Smith et al. (2017); Goyal et al. (2017); Keskar
et al. (2016); Li et al. (2019b); Hoffer et al. (2017), initialization and momentum Sutskever et al.
(2013), adaptive stepsize Kingma and Ba (2014); Neyshabur et al. (2015); Wilson et al. (2017), batch
normalization Ioffe and Szegedy (2015) and dropout Srivastava et al. (2014).
Among these sources of implicit regularization, the SGD noise is believed to be a vital one (LeCun
et al., 2012; Keskar et al., 2016). Previous theoretical works (e.g., Li et al. (2019b)) have studied
the implicit regularization effect from the scale of the noise, which is directly influenced by learning
rate and batch size. However, people have empirically observed that the shape of the noise also
has a strong (if not stronger) implicit bias. For example, prior works show that mini-batch noise
or label noise (label smoothing) - noise in the parameter updates from the perturbation of labels
in training - is far more effective than adding spherical Gaussian noise (e.g., see (Shallue et al.,
2018, Section 4.6) and Szegedy et al. (2016); Wen et al. (2019)). We also confirm this phenomenon
in Figure 1 (left). Thus, understanding the implicit bias of the noise shape is crucial. Such an
understanding may also apply to distributed training because synthetically adding noise may help
generalization if parallelism reduces the amount of mini-batch noise (Shallue et al., 2018).
In this paper, we theoretically study the effect of the shape of the noise, demonstrating that it can
provably determine generalization performance at convergence. Our analysis is based on a nonlinear
quadratically-parameterized model introduced by (Woodworth et al., 2020; Vaskevicius et al., 2019),
which is rich enough to exhibit similar empirical phenomena as deep networks. Indeed, Figure 1
(right) empirically shows that SGD with mini-batch noise or label noise can generalize with arbitrary
initialization without explicit regularization, whereas GD or SGD with spherical Gaussian noise
cannot. We aim to analyze the implicit bias of label noise and Gaussian noise in the quadratically-
parametrized model and explain these empirical observations.
1
Under review as a conference paper at ICLR 2021
Figure 1: The effect of noise covariance in neural network and quadratically-parameterized
models. We demonstrate that label noise induces a stronger regularization effect than Gaussian
noise. In both real and synthetic data, adding label noise to large batch (or full batch) SGD updates
can recover small-batch generalization performance, whereas adding Gaussian noise with optimally-
tuned variance σ2 cannot. Left: Training and validation errors on CIFAR100 for VGG19. Adding
Gaussian noise to large batch updates gives little improvement (around 2%), whereas adding label
noise recovers the small-batch baseline (around 15% improvement). Right: Training and validation
error on a 100-dimensional quadratically-parameterized model defined in Section 2. Similar to
deep models, label noise or mini-batch noise leads to better solutions than optimally-tuned spherical
Gaussian noise. Moreover, Gaussian noise causes the parameter to diverge after sufficient mixing,
as suggested by our negative result for Langevin dynamics (Theorem 2.2). More details are in
Section A.
We choose to study label noise because it can replicate the regularization effects of minibatch noise
in both real and synthetic data (Figure 1), and has been used to regularize large-batch parallel train-
ing (Shallue et al., 2018). Moreover, label noise is less sensitive to the initialization and the opti-
mization history than mini-batch noise, which makes it more amenable to theoretical analysis. For
example, in an extreme case, if we happen to reach or initialize at a solution that overfits the data
exactly, then mini-batch SGD will stay there forever because both the gradient and the noise van-
ish (Vaswani et al., 2019). In contrast, label noise will not accidentally vanish, so the analysis is
more tractable. Understanding label noise may lead to understanding mini-batch noise or replacing
it with other more robust choices.
In our setting, we prove that with a proper learning rate schedule, SGD with label noise recovers
a sparse ground-truth classifier and generalizes well, whereas SGD with spherical Gaussian noise
generalizes poorly. Concretely, SGD with label noise biases the parameter towards the low sparsity
regime and exactly recovers the sparse ground-truth, even when the initialization is arbitrarily large
(Theorem 2.1). In this same regime, noise-free gradient descent quickly overfits because it trains in
the NTK regime (Jacot et al., 2018; Chizat and Bach, 2018). Adding Gaussian noise is insufficient
to fix this, as this algorithm would end up sampling from a Gibbs distribution with infinite partition
function and fail to converge to the ground-truth (Theorem 2.2). In summary, with not too small
learning rate or noise level, label noise suffices to bias the parameter towards sparse solutions without
relying on a small initialization, whereas Gaussian noise cannot.
Our analysis suggests that the fundamental difference between label or mini-batch noise and Gaus-
sian noise is that the former is parameter-dependent, and therefore introduces stronger biases than
the latter. The conceptual message highlighted by our analysis is that there are two possible implicit
biases induced by the noise: 1. prior work (Keskar et al., 2016) shows that by escaping sharp local
minima, noisy gradient descent biases the parameter towards more robust solutions (i.e, solutions
with low curvature, or “flat” minima), and 2. when the noise covariance varies across the param-
eter space, there is another (potentially stronger) implicit bias effect toward parameters where the
noise covariance is smaller. Label or mini-batch noise benefits from both biases, whereas Gaussian
noise is independent of the parameter, so it benefits from the first bias but not the second. For the
quadratically-parameterized model, this first bias is not sufficient for finding solutions with good
generalization because there is a large set of overfitting global minima of the training loss with
reasonable curvature. In contrast, the covariance of label noise is proportional to the scale of the
parameter, inducing a much stronger bias towards low norm solutions which generalize well.
1.1	Additional Related Works
Closely related to our work, Blanc et al. (2019) and Zhu et al. (2019) also theoretically studied
implicit regularization effects that arise due to shape, rather than scale, of the noise. However, they
only considered the local effect of the noise near some local minimum of the loss. In contrast, our
2
Under review as a conference paper at ICLR 2021
work analyzes the global effect of noise. For a more detailed comparison with (Blanc et al., 2019),
see Section 2.2.
Woodworth et al. (2020); Vaskevicius et al. (2019) analyze the effect of initialization for the same
model that we study, showing that large initialization trains in the NTK regime (shown to generalize
poorly (Wei et al., 2019; Ghorbani et al., 2019)) whereas small initialization does not. We show that
when the initialization is large, adding noise helps avoid the NTK regime (Li and Liang, 2018; Jacot
et al., 2018; Du et al., 2018b; Woodworth et al., 2020) without explicit regularization.
Several previous works have studied generalization bounds and training dynamics of SGD with state-
dependent noises for more general models. Hardt et al. (2015) derived stability-based generalization
bounds for mini-batch SGD based on training speed. Cheng et al. (2019) proved that SGD with state-
dependent noises has iterate distribution close to the corresponding continuous stochastic differential
equation with the same noise covariance. Meng et al. (2020); Xie et al. (2020) showed that SGD
with state-dependent noises escapes local minimum faster than SGD with spherical Gaussian noise .
There has been a line of work empirically studying how noise influences generalization. Keskar
et al. (2016) argued that large batch training will converge to “sharp” local minima which do not
generalize well. Hoffer et al. (2017) argued that large batch size doesn’t hurt generalization much if
training goes on long enough and additional noise is added with a larger learning rate. Goyal et al.
(2017) and Shallue et al. (2018) showed large batch training with proper learning rate and additional
label noise can achieve similar generalization as small batch. Wei and Schwab (2019); Chaudhari
and Soatto (2018); Yaida (2018) (heuristically) suggested that SGD may encourage solutions with
smaller noise covariance. Martin and Mahoney (2018) used random matrix theory to analyze im-
plicit regularization effects of noises. The noise induced by dropout has been shown to change the
expected training objective, hence provides a regularization effect (Mianjy et al., 2018; Mianjy and
Arora, 2019; Wei et al., 2020; Arora et al., 2020). Wei et al. (2020) showed that there also exists an
implicit bias induced by dropout noise.
Langevin dynamics or the closely-related stochastic gradient descent with spherical Gaussian noise
has been studied in previous works Welling and Teh (2011); Teh et al. (2016); Raginsky et al. (2017);
Zhang et al. (2017); Mou et al. (2017); Roberts et al. (1996); Ge et al. (2015); Negrea et al. (2019);
Neelakantan et al. (2015); Mou et al. (2018). In particular, Raginsky et al. (2017) and Li et al.
(2019a) provided generalization bounds for SGLD using algorithmic stability.
Several works have theoretically analyzed other types of implicit biases in simplified set-
tings (Soudry et al., 2018; Gunasekar et al., 2018b; Ji and Telgarsky, 2018a). Gunasekar et al.
(2017) and Li et al. (2017) showed that gradient descent finds low rank solutions in matrix com-
pletion. Gradient descent has also been shown to maximize the margin in linear and homogeneous
models (Soudry et al., 2018; Ji and Telgarsky, 2018b; Nacson et al., 2018; Lyu and Li, 2019; Gu-
nasekar et al., 2018a; Nacson et al., 2019; Poggio et al., 2017). Du et al. (2018a) showed that gradient
descent implicitly balances the layers of deep homogeneous models. Other works showed that it may
not always be possible to characterize implicit biases in terms of norm (Arora et al., 2019; Razin and
Cohen, 2020). Gissin et al. (2019) showed that gradient descent dynamics exhibit different implicit
biases based on depth. Li et al. (2019b) studied the implicit regularization effect of a large initial
learning rate.
Guo et al. (2018) studies the notion of “elimination singularities” in RBF networks, where optimiza-
tion runs into a regime with small weight and small gradient, therefore the training can be slowed
down. Our paper also involves training trajectory with small weight norm, but instead focuses on its
influnece on the generalization performance.
2	Setup and Main Results
2.1	Setup and Backgrounds
Parameterization. We focus on the nonlinear model parametrization: fv (x) , hv2, xi, where
v ∈ Rd is the parameter of the model, x ∈ Rd is the data, and v2 denotes the element-wise square
ofv. Prior works (Woodworth et al., 2020; Vaskevicius et al., 2019; Li et al., 2017) have studied this
model because it is an interesting and informative simplification of nonlinear models. As SGD noise
exhibits many of the same empirical behaviors in this simplified model as in deep networks,1 we use
1In contrast, the implicit bias of noise wouldn’t show up in a simpler linear regression model.
3
Under review as a conference paper at ICLR 2021
this model as a testbed to develop a mathematical understanding of various sources of implicit biases.
As shown in Figure 1, both SGD with mini-batch noise and label noise generalize better than GD or
SGD with spherical Gaussian noise.
Data distribution assumptions and overparametrization. We assume that there exists a ground-
truth parameter v? ∈ Rd that generates the label y = hv?2 , xi given a data point x, which is
assumed to be generated fromN(0,Id×d). A dataset D = (x(i), y(i)) in=1 ofn i.i.d data points are
generated from this distribution. The implicit bias is only needed in an over-parameterized regime,
and therefore we assume that n	d. To make the ground-truth vector information-theoretically
recoverable, we assume that the ground-truth vector v? is r-sparse. Here r is much smaller than
d, and casual readers can treat it as a constant. Because the element-wise square in the model
parameterization is invariant to any sign flip, we assume v? is non-negative without loss of generality.
For simplicity, we also assume it only takes value in {0, 1}.2 We use S ⊂ [d] with |S| = r to denote
the support of v? throughout the paper.
We remark that we can recover v? by re-parameterizing u = v2 and applying LASSO (Tibshirani,
1996) in the u-space when n ≥ O(r), which is minimax optimal (Raskutti et al., 2012). However,
the main goal of the paper, similar to several prior works (Woodworth et al., 2020; Vaskevicius et al.,
2019; Li et al., 2017), is to prove that the implicit biases of non-convex optimization can recover the
ground truth without explicit regularization in the over-parameterized regime when n = poly(r)
d.3 We also assume throughout the paper that n, d are larger than some sufficiently large universal
constant.
Loss function. We use the mean-squared loss denoted by '(%)(v)，1 fVv(x(i)) - y(i)) for the i-th
example. The empirical loss is written as L(V)，1 Pn=1'(i)(v).
Initialization. We use a large initialization of the form V[0] = T ∙ 1 where 1 denotes the all 1's
vector, where we allow τ to be arbitrarily large (but polynomial in d).
Algorithm 1 Stochastic Gradient Descent with Label Noise
Require: Number of iterations T, a sequence of step sizes η [0:T], noise level δ, initialization V [0]
1:	for t = 0 to T - 1 do
2:	Sample index it 〜[n] uniformly and add noise St 〜{±δ} to y(it).
3:	Let 认it)(V) = 1 (fv(X(i)) - y(i) - St)2
4:	v[t +1] J v[t] — η[t] V^(it)(v[t])	> update with label noise
SGD with label noise. We study SGD with label noise as shown in Algorithm 1. We sample an
example, add label noise sampled from {±δ} to the label, and apply the gradient update. Computing
the gradient, we obtain the update rule written explicitly as:
v[t+1] J v[t] - η[t] (v[t]	- v?2)>x(it) x(it) v[t] + η[t] Stx(it) v[t] .	(1)
Langevin dynamics/diffusion. We compare SGD with label noise to Langevin dynamics, which
adds spherical Gaussian noise to gradient descent (Neal et al., 2011):
V[t+1] J V[t] - ηVL(V[t]) + √2η∕λ .ξ,	(2)
where the noise ξ 〜N(0, T×d) and λ > 0 controls the scale of noise. Langevin dynamics (LD)
or its more computationally-efficient variant, stochastic gradient Langevin dynamics (SGLD), is
known to converge to the Gibbs distribution μ(V) 8 bλ(V) under various settings with sufficiently
small learning rate (Roberts et al., 1996; Dalalyan, 2017; Bubeck et al., 2018; Raginsky et al., 2017).
In our negative result about Langevin dynamics/diffusion, we directly analyze the Gibbs distribution
in order to disentangle the convergence and the generalization. In our negative result about Langevin
dynamics/diffusion, we directly analyze the Gibbs distribution to disentangle the convergence and
the generalization.
Notations. Unless otherwise specified, we use O(∙), Ω(∙), Θ(∙) to hide absolute multiplicative fac-
tors and O(∙), Θ(∙), Ω(∙) to hide poly-logarithmic factors in problem parameters such as d and T. For
2Our analysis can be straightforwardly extended to v? with other non-zero values.
3We also remark that it’s common to obtain only sub-optimal sample complexity guarantees in the sparsity
parameters with non-convex optimization methods (Li et al., 2017; Ge et al., 2016; Vaskevicius et al., 2019;
Chi et al., 2019) due to technical limitations.
4
Under review as a conference paper at ICLR 2021
example, every occurrence of O(x) is a placeholder for a quantity f(x) that satisfies that for some
absolute constants C1, C2 > 0, ∀x, ∣f (x)| ≤ C1 ∣x∣ ∙ logC2 (dτ).
2.2	Main Results
Our main result can be summarized by the following theorem, which suggests that stochastic gradi-
ent descent with label noise can converge to the ground truth despite a potentially large initialization.
Theorem 2.1. In the setting of Section 2.1, given a target error > 0. Suppose we have n ≥ Θ(r2)
samples. For any label noise level δ ≥ Θ(τ2d2 ), we run SGD with label noise (Algorithm 1) with
the following learning rate schedule:
1.	learning rate η0 = Θ(1 /δ) for To = Θ(1) iterations,
2.	learning rate η 1 = Θ(1 /δ2) for T1 = Θ(1 /η 1) iterations,
3.	learning rate η2 = Θ(E2/δ2) for T2 = Θ(1 /η2) iterations.
Then, with probability at least 0.9, the final iterate v [T] at time T = T0 + T1 + T2 satisfies
IW [°- o*h ≤ E.	(3)
Here Θ(∙) omits poly-logarithmic dependencies on 1 /e, d and τ.
In other words, with arbitrarily large initialization scale τ, we can choose large label noise level
and the learning rate schedule so that SGD with label noise succeeds in recovering the ground truth.
In contrast, when τ is large, gradient flow without noise trains in the “kernel” regime as shown
by (Woodworth et al., 2020; Chizat and Bach, 2018). The solution in this kernel regime minimizes
the RKHS distance to initialization, and in our setting equates to finding a zero-error solution with
minimum IW2 - W[0]2 I2. Such a solution could be arbitrarily far away when initialization scale
τ is large and therefore have poor generalization. Figure 1 (right) confirms GD performs poorly
with large initialization whereas SGD with minibatch or label noise works. We outline the proof of
Theorem 2.1 in Section 3.
Blanc et al. (2019) also study the implicit bias of the label noise. For our setting, their result implies
that when the iterate is near a global minimum for sufficient time, the iterates will locally move to the
direction that reduces the `2 -norm ofW by a small distance (that is larger than random fluctuation).
However, it does not imply the global convergence to a solution with good generalization with large
(or any) initialization, which is what we prove in Theorem 2.1.4 Moreover, our analysis captures
the effect of the large noise or large learning rate - We require the ratio between the noise and the
gradient, which is captured by the value ηδ2, to be sufficiently large. This is consistent with the
empirical observation that good generalization requires a sufficiently large learning rate or small
batch (Goyal et al., 2017).
On the other hand, the following negative result for Langevin dynamics demonstrates that adding
Gaussian noise fails to recover the ground truth even when W? = 0. This suggests that spherical
Gaussian noise does not induce a strong enough implicit bias towards low-norm solutions.
Theorem 2.2. Assume in addition to the setting in Section 2.1 that the ground truth W? = 0. When
n ≤ d/3, with probability at least 0.9 over the randomness of the data, for any λ > 0, the Gibbs
distribution is not well-defined because the partition function explodes:
d e-λL(v)dW = ∞.
(4)
As a consequence, Langevin diffusion does not converge to a proper stationary distribution.
Theorem 2.2 helps explain the behavior in Figure 1, where adding Gaussian noise generalizes poorly
for both synthetic and real data. In particular, in Figure 1 (right) adding Gaussian noise causes the
parameter to diverge for synthetic data, and Theorem 2.2 explains this observation. A priori, the
intuition regarding Langevin dynamics is as follows: as λ → +∞, the Gibbs distribution (if it exists)
should concentrate on the manifold of global minima with zero loss. The measure on the manifold of
4It also appears difficult to generalize the local analysis directly to a global analysis because once the iterate
leaves the local minimum, all the local tools do not apply anymore, and it’s unclear whether the iterate will
converge to a new local minimum or getting stuck at some region.
5
Under review as a conference paper at ICLR 2021
global minima should be decided by the geometry of L (∙), and in particular, the curvature around the
global minimum. As λ → +∞, the mass should likely concentrate at the flattest global minimum
(according to some measure of flatness), which intuitively is v? = 0 in this case.
However, our main intuition is that when n < d, even though the global minimum at v? is the flattest,
there are also many bad global minima with only slightly sharper curvatures. The vast volume of
bad global minima dominate the flatness of the global minimum at v? = 0 for any λ,5 and hence the
partition function blows up and the Gibbs distribution doesn’t exist. The proof of Theorem 2.2 can
be found in Section F.
3	Analysis Overview of SGD with Label Noise (Theorem 2.1)
3.1	Warm-up: Updates with Only Parameter-dependent Noise
Towards building intuition and tools for analyzing the parameter-dependent noise, in this subsection
we start by studying an extremely simplified random walk in one dimensional space. The random
walk is purely driven by mean-zero noisy updates and does not involve any gradient updates:
V J V + ηξ ∙ v, where ξ 〜 {±1}.
(5)
Indeed, attentive readers can verify that when dimension d = 1, sample size n = 1, and V? = 0,
equation (1) degenerates to the above random walk if we omit the gradient update term (second to
last term in equation (1)). We compare it with the standard Brownian motion (which is the analog
of gradient descent with spherical Gaussian noise under this extreme simplification)
v J v + ηξ, where ξ 〜N(0, 1).
(6)
We initialize at v = 1. We observe that both random walks have mean-zero updates, so the mean
is preserved: E[v] = 1. The variances of the two random walks are also both growing because
any mean-zero update increases the variance. Moreover, the Brownian motion diverges because
it has a Gaussian marginal with variance growing linearly in t, and there is no limiting stationary
distribution.
However, the parameter-dependent random walk (5) has dramatically different behavior when η < 1:
the random variable v will eventually converge to v = 0 with high probability (though the variance
grows and the mean remains at 1.). This is because the variance of the noise depends on the scale of
v. The smaller v is, the smaller the noise variance is, and so the random walk tends to get “trapped”
around 0. This claim has the following informal but simple proof that does not strongly rely on the
exact form of the noise and can be extended to more general high-dimensional cases.
Consider an increasing concave potential function φ : R≥0 → R≥0 with φ00 < 0 (e.g., φ(v) = √v
works). Note that when η < 1, the random variable v stays nonnegative. We can show that the
expected potential function decreases after any update
E[φ(v + ηξv)] ≈ E[φ(v) + φ0(v)ηξv + φ00(v)η2ξ2v2]	(by Taylor expansion)
E[φ(v)] + E[φ00(v)η2v2] < E[φ(v)]
(by φ00(v) < 0 and E[ξ] = 0.)
With more detailed analysis, we can formalize the Taylor expansion and control the decrease of the
potential function, and conclude that E [φ(v)] converges to zero. Then, by Markov’s inequality, with
high probability, φ(v) is tiny and so is v.6
From the 1-D case to the high-dimensional case. In one dimension, the bias is introduced because
of the varying scale of noise (i.e., the norm of the covariance). However, in the high dimensional
case, the shape of the covariance also matters. For example, if we generalize the random walk (1)
to high-dimensions by running d of the random walks in parallel, then we will observe the same
phenomenon, but the noise variances in different dimensions are not identical — they depend on the
current scales of the coordinates. (Precisely, the noise variance for dimension k is η2vk2 .) However,
suppose we instead add noise of the same variance to all dimensions. Even if this variance depends
on the norm ofv (say, η2kvk22), the implicit bias will be diminished, as the smaller coordinates will
have relatively outsized noise and the larger coordinates will have relatively insufficient noise.
5In fact, one can show that if this phenomenon happens for some λ > 0, then it happens for all other λ.
6The same proof strategy fails for the Brownian motion because v is not always nonnegative, and there is
no concave potential function over the real that can be bounded from below.
6
Under review as a conference paper at ICLR 2021
Outline of the rest of the subsections. We will give a proof sketch of Theorem 2.1 that consists of
three stages. We first show in the initial stage of the training that label noise effectively decreases the
parameter on all dimensions, bringing the training from large initialization to a small initialization
regime, where better generalization is possible (Section 3.2). Then, we show in Section 3.3 that
when the parameter is decently small, with label noise and a decayed learning rate, the algorithm
will increase the magnitude of those dimensions in support set of v?, while keep decreasing the norm
of the rest of the dimensions. Finally, with one more decay, the algorithm can recover the ground
truth.
3.2	Stage 0: Label Noise with Large Learning Rate Reduces the Parameter
Norm
We first analyze the initial phase where we use a relatively large learning rate. When the initialization
is of a decent size, GD quickly overfits to a bad global minimum nearest to the initialization. In
contrast, we prove that SGD with label noise biases towards the small norm region, for a similar
reason as the random walk example with parameter-dependent noise in Section 3.1.
Theorem 3.1. In the setting of Theorem 2.1, recall that we initialize with V [0] = T ∙ 1. Assume
n ≥ Θ(log d). Suppose we run SGD with label noise with noise level δ ≥ Θ(τ2d2) and learning
rate η0 ∈ [Θ(τ2d2/δ2), Θ(1 /δ)] for To = Θ(1 /(η2δ2)) iterations. Then, with probability at least
0.99 over the randomness of the algorithm,
kv[T0]k∞ ≤ 1/d.	(7)
Moreover, the minimum entry of v[T0] is bounded below by exp(-Oe((ηδ)-1)).
We remark that our requirement of η being large is consistent with the empirical observation that
a large initial learning rate helps generalization Goyal et al. (2017); Li et al. (2019b). We provide
intuitions and a proof sketch of the theorem in the rest of the subsection and defer the full proof
to Section B . Our proof is based on the construction of a concave potential function Φ similar to
Section 3.1. We will show that, at every step, the noise has a second-order effect on the potential
function and decrease the potential function by a quantity on the order of η2δ2 (omitting the d
dependency).7 On the other hand, the gradient step may increase the potential by a quantity at
most on the order of η (omitting d dependency again). Therefore, when η2δ2 & η, we expect the
algorithm to decrease the potential and the parameter norm.
In particular, We define Φ(v)，Pd=1 φ(Vk) = P： =1 √Vk. By the update rule 2.1, the update for
a coordinate k ∈ [d] can be written as
vkt +1] - vktt] - ηstχk)vkt] - η[t] ((V[t]°2 -v*°2)>χ(it)) χk)vkt],	(8)
Where	st	is sampled from {-δ, δ} and	it	is sampled from	[n].	Let	gk(it)	,	((V[t]2 -
v?°2)>x(it))x(kit) be the component coming from the stochastic gradient. Using the fact that
φ(ab) = φ(a)φ(b) for any a, b > 0, We can evaluate the potential function at time t + 1,
E hφ(vk[t+1])i = E hφ(vk[t])φ(1 - ηstx(kit) - ηgk(it))i = φ(vk[t])E hφ(1 -ηstx(kit) - ηgk(it))i . (9)
Here the expectation is over st and it. We perform Taylor-expansion on the term φ(1 - ηstx(kit) -
ηgk(it)) to deal With the non-linearity and use the fact that ηstx(kit) is mean-zero:
E hφ(1 - ηstxkt) - ηgk )i ≈ φ⑴-φ⑴ηEhgkt] + 2φ0(1)Eh(ηstx(it) - ηg'iiit)^2i
≤ Φ⑴-Φ⑴ηEhgki + 2Φ0(ι)Eh(ηstχkit))2 i
≤ φ⑴-Φ⑴ηEhgkti - Ω(η2δ2).	(10)
7In general, any mean-zero noise has a second-order effect on any potential function. Therefore, When the
noise level is fixed, as η → 0, the effect of the noise diminishes. This is Why a loWer bound on the learning rate
is necessary for the noise to play a role.
7
Under review as a conference paper at ICLR 2021
In the second line we used φ00(1) < 0 from the concavity and E[ηstx(kit)] = 0, and the third line uses
(it)2
the fact that St 〜{±δ} and E[xkt) ] ≈ 1 (by the data assumption). The rest of the proof consists of
bounding the second term in equation (10) from above to show the potential function is contracting.
We first note for every it, it holds that |gkit | ≤ kv[t]2 - v?2 k1 kx(it) k2∞ ≤ (kv[t] k22 + r)kx(it) k2∞.
Furthermore, we can bound the `2 norm of v[t] with the following lemma:
Lemma 3.2. In the setting of Theorem 3.1, for some failure probability ρ > 0, let bo ，6τd∕ρ.
Then, with probability at least 1 — ρ/3, we have that ∣∣v[ t ] k 2 ≤ b 0 Jbrany t ≤ To.
Note that V[0] has '2 norm TJd, and here We prove that the norm does not exceed Td with high
probability. At the first glance, the lemma appears to be mostly auxiliary, but we note that it dis-
tinguishes label noise from Gaussian noise, which empirically causes the parameter to blow up as
shown in Figure 1. The formal proof is deferred to Section B.
By Lemma 3.2 and the bound on |gkit | in terms of ∣v[t] ∣2, we have |gkit | ≤ (b20 + r)∣x(it) ∣2∞ ≤
O(b02 + r) with b0 defined in Lemma 3.2 (up to logarithmic factors). Here we use again that each
entry of the data is from N (0, 1). Plugging these into equation (10) we obtain
E hφ(1 — ηstx(kit) — ηgk)] ≤ 1 + ηO(b2 + r) — Ω(η2δ2) < 1 — Ω(η2δ2)
where in the last inequality we use the lower bound on η to conclude η2δ2 & ηOe(b02 +r). Therefore,
summing equation (9) over all the dimensions shows that the potential function decreases exponen-
tially fast: E[Φ(v[t +1])] < (1 一 Ω(η2δ2))Φ(v[t]). After T ≈ log(d)/(η2δ2) iterations, V[T] will
already converge to a position such that E[Φ( v [ T ])] .，1 /d, which implies ∣∣v [T]∣∣g . 1 /d with
probability at least 1 — ρ and finishes the proof.
3.3	STAGE 1: GETTING CLOSER TO v? WITH ANNEALED LEARNING RATE
Theorem 3.1 shows that the noise decreases the ∞-norm of V to 1 /d. This means that' 1 or '2-norm
ofv is similar to or smaller than that ofv? ifr is constant, and we are in a small-norm region where
overfitting is less likely to happen. In the next stage, we anneal the learning rate to slightly reduce
the bias of the label noise and increase the contribution of the signal. Recall that v? is a sparse
vector with support S ⊂ [d]. The following theorem shows that, after annealing the learning rate
(from the order of 1 /δ2 to 1 /δ), SGD with label noise increases entries in VS and decreases entries
in VS simultaneously, provided that the initialization has '∞-norm bounded by 1 /d. (For simplicity
and self-containedness of the statement, we reset the time step to 0.)
Theorem 3.3. In the setting of Section 2.1, given a target error bound 1 > 0, we assume that
n ≥ Θe(r2 log2(1/1)). We run SGD with label noise (Algorithm 1) with an initialization V[0] whose
entries are all in [emi∏, 1 /d], where Emi∏ ≥ exp(-O(1)). Let noise level δ ≥ Θ(log(1 /e 1)) and
learning rate η = Θ(1 /δ2), and number ofiterations T = Θ(log(1 /e 1)/η). Then, with probability
at least 0.99, after T iterations, we have
IlVF] - vSk∞ ≤ 0∙ 1 and I∣vS^ - vSk1 ≤ e 1 ∙	(II)
We remark that even though the initialization is relatively small in this stage, the label noise still
helps alleviate the reliance on small initialization. Li et al. (2017); Vaskevicius et al. (2019) showed
that GD converges to the ground truth with sufficiently small initialization, which is required to be
smaller than target error E1. In contrast, our result shows that with label noise, the initialization does
not need to depend on the target error, but only need to have an '∞ -norm bound on the order of
1 /d. In other words, V gets closer to v? on both S and S in our case, whereas in Li et al. (2017);
Vaskevicius et al. (2019) the VS grows slowly.
The proof of this theorem balances the contribution of the gradient against that of the noise on S and
S. On S, the gradient provides a stronger signal than label noise, whereas on S, the implicit bias of
the noise, similar to the effect in Section 3.2, outweighs the gradient and reduces the entries to zero.
The analysis is more involved than that of Theorem 3.1, and we defer the full proof to Section C.
Stage 2: Convergence to the ground-truth v?: The conclusion of Theorem 3.3 still allows constant
error in the support, namely, ∣vS — vS? ∣∞ ≤ 1/10. In Theorem D.1, we show that further annealing
the learning rate will let the algorithm fully converge to v? with any target error E.
8
Under review as a conference paper at ICLR 2021
Proof of Theorem 2.1. In Section E of Appendix, we combine Theorem 3.1, Theorem 3.3, and
Theorem D.1 to prove our main Theorem 2.1.
4	Conclusion
In this work, we study the implicit bias effect induced by noise. For a quadratically-parameterized
model, we theoretically show that the parameter-dependent noise has a strong implicit bias, which
can help recover the sparse ground-truth from limited data. In comparison, our negative result
shows that such a bias cannot be induced by spherical Gaussian noise. Our result explains the
empirical observation that replacing mini-batch noise or label noise with Gaussian noise usually
leads to degradation in the generalization performance of deep models.
References
D. Amelunxen, M. Lotz, M. B. McCoy, and J. A. Tropp. Living on the edge: Phase transitions
in convex programs with random data. Information and Inference: A Journal of the IMA, 3(3):
224-294, 2014.
R.	Arora, P. Bartlett, P. Mianjy, and N. Srebro. Dropout: Explicit forms and capacity control. arXiv
preprint arXiv:2003.03397, 2020.
S.	Arora, N. Cohen, W. Hu, and Y. Luo. Implicit regularization in deep matrix factorization. In
Advances in Neural Information Processing Systems, pages 7411-7422, 2019.
G. Blanc, N. Gupta, G. Valiant, and P. Valiant. Implicit regularization for deep neural networks
driven by an ornstein-uhlenbeck like process. arXiv preprint arXiv:1904.09080, 2019.
S. Bubeck, R. Eldan, and J. Lehec. Sampling from a log-concave distribution with projected langevin
monte carlo. Discrete & Computational Geometry, 59(4):757-783, 2018.
P. Chaudhari and S. Soatto. Stochastic gradient descent performs variational inference, converges
to limit cycles for deep networks. In 2018 Information Theory and Applications Workshop (ITA),
pages 1-10. IEEE, 2018.
X. Cheng, D. Yin, P. L. Bartlett, and M. I. Jordan. Stochastic gradient and langevin processes. arXiv
preprint arXiv:1907.03215, 2019.
Y. Chi, Y. M. Lu, and Y. Chen. Nonconvex optimization meets low-rank matrix factorization: An
overview. IEEE Transactions on Signal Processing, 67(20):5239-5269, 2019.
L. Chizat and F. Bach. A note on lazy training in supervised differentiable programming. arXiv
preprint arXiv:1812.07956, 8, 2018.
A. S. Dalalyan. Theoretical guarantees for approximate sampling from smooth and log-concave
densities. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3):
651-676, 2017.
S. S. Du, W. Hu, and J. D. Lee. Algorithmic regularization in learning deep homogeneous models:
Layers are automatically balanced. In Advances in Neural Information Processing Systems, pages
384-395, 2018a.
S. S. Du, J. D. Lee, H. Li, L. Wang, and X. Zhai. Gradient descent finds global minima of deep
neural networks. arXiv preprint arXiv:1811.03804, 2018b.
R. Ge, F. Huang, C. Jin, and Y. Yuan. Escaping from saddle pointsonline stochastic gradient for
tensor decomposition. In Conference on Learning Theory, pages 797-842, 2015.
R. Ge, J. D. Lee, and T. Ma. Matrix completion has no spurious local minimum. In Advances in
Neural Information Processing Systems, pages 2973-2981, 2016.
B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. Limitations of lazy training of two-layers
neural network. In Advances in Neural Information Processing Systems, pages 9108-9118, 2019.
9
Under review as a conference paper at ICLR 2021
D. Gissin, S. Shalev-Shwartz, and A. Daniely. The implicit bias of depth: How incremental learning
drives generalization. arXiv preprint arXiv:1909.12051, 2019.
P. Goyal, P. Doll念 R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia,
and K. He. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint
arXiv:1706.02677, 2017.
S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regular-
ization in matrix factorization. In Advances in Neural Information Processing Systems, pages
6151-6159, 2017.
S. Gunasekar, J. Lee, D. Soudry, and N. Srebro. Characterizing implicit bias in terms of optimization
geometry. arXiv preprint arXiv:1802.08246, 2018a.
S. Gunasekar, J. D. Lee, D. Soudry, and N. Srebro. Implicit bias of gradient descent on linear
convolutional networks. In Advances in Neural Information Processing Systems, pages 9461-
9471, 2018b.
W. Guo, H. Wei, Y.-S. Ong, J. R. Hervas, J. Zhao, H. Wang, and K. Zhang. Numerical analysis near
singularities in rbf networks. The Journal of Machine Learning Research, 19(1):1-39, 2018.
M. Hardt, B. Recht, and Y. Singer. Train faster, generalize better: Stability of stochastic gradient
descent. arXiv preprint arXiv:1509.01240, 2015.
E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better: closing the generalization gap
in large batch training of neural networks. In Advances in Neural Information Processing Systems,
pages 1731-1741, 2017.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization in
neural networks. In Advances in neural information processing systems, pages 8571-8580, 2018.
Z. Ji and M. Telgarsky. Gradient descent aligns the layers of deep linear networks. arXiv preprint
arXiv:1810.02032, 2018a.
Z. Ji and M. Telgarsky. Risk and parameter convergence of logistic regression. arXiv preprint
arXiv:1803.07300, 2018b.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training
for deep learning: Generalization gap and sharp minima. arXiv preprint arXiv:1609.04836, 2016.
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Y. A. LeCun, L. Bottou, G. B. Orr, and K.-R. Muller. Efficient backprop. In Neural networks: Tricks
of the trade, pages 9-48. Springer, 2012.
J.	Li, X. Luo, and M. Qiao. On generalization error bounds of noisy gradient methods for non-convex
learning. arXiv preprint arXiv:1902.00621, 2019a.
Y. Li and Y. Liang. Learning overparameterized neural networks via stochastic gradient descent on
structured data. In Advances in Neural Information Processing Systems, pages 8157-8166, 2018.
Y. Li, T. Ma, and H. Zhang. Algorithmic regularization in over-parameterized matrix sensing and
neural networks with quadratic activations. arXiv preprint arXiv:1712.09203, 2017.
Y. Li, C. Wei, and T. Ma. Towards explaining the regularization effect of initial large learning
rate in training neural networks. In Advances in Neural Information Processing Systems, pages
11669-11680, 2019b.
K.	Lyu and J. Li. Gradient descent maximizes the margin of homogeneous neural networks. arXiv
preprint arXiv:1906.05890, 2019.
10
Under review as a conference paper at ICLR 2021
C. H. Martin and M. W. Mahoney. Implicit self-regularization in deep neural networks: Evidence
from random matrix theory and implications for learning. arXiv preprint arXiv:1810.01075, 2018.
Q. Meng, S. Gong, W. Chen, Z.-M. Ma, and T.-Y. Liu. Dynamic of stochastic gradient descent with
state-dependent noise. arXiv preprint arXiv:2006.13719, 2020.
P. Mianjy and R. Arora. On dropout and nuclear norm regularization. arXiv preprint
arXiv:1905.11887, 2019.
P. Mianjy, R. Arora, and R. Vidal. On the implicit bias of dropout. arXiv preprint arXiv:1806.09777,
2018.
W. Mou, L. Wang, X. Zhai, and K. Zheng. Generalization bounds of sgld for non-convex learning:
Two theoretical viewpoints. arXiv preprint arXiv:1707.05947, 2017.
W. Mou, L. Wang, X. Zhai, and K. Zheng. Generalization bounds of sgld for non-convex learning:
TWo theoretical viewpoints. In Conference on Learning Theory, pages 605-638, 2018.
M. S. Nacson, J. Lee, S. Gunasekar, P. H. Savarese, N. Srebro, and D. Soudry. Convergence of
gradient descent on separable data. arXiv preprint arXiv:1803.01905, 2018.
M. S. Nacson, S. Gunasekar, J. D. Lee, N. Srebro, and D. Soudry. Lexicographic and depth-sensitive
margins in homogeneous and non-homogeneous deep models. arXiv preprint arXiv:1905.07325,
2019.
R. M. Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2
(11):2, 2011.
A.	Neelakantan, L. Vilnis, Q. V. Le, I. Sutskever, L. Kaiser, K. Kurach, and J. Martens. Adding
gradient noise improves learning for very deep networks. arXiv preprint arXiv:1511.06807, 2015.
J. Negrea, M. Haghifam, G. K. Dziugaite, A. Khisti, and D. M. Roy. Information-theoretic gen-
eralization bounds for sgld via data-dependent estimates. In Advances in Neural Information
Processing Systems, pages 11013-11023, 2019.
B.	Neyshabur, R. R. Salakhutdinov, and N. Srebro. Path-sgd: Path-normalized optimization in deep
neural networks. In Advances in Neural Information Processing Systems, pages 2422-2430, 2015.
T. Poggio, K. Kawaguchi, Q. Liao, B. Miranda, L. Rosasco, X. Boix, J. Hidary, and H. Mhaskar. The-
ory of deep learning iii: explaining the non-overfitting puzzle. arXiv preprint arXiv:1801.00173,
2017.
M.	Raginsky, A. Rakhlin, and M. Telgarsky. Non-convex learning via stochastic gradient langevin
dynamics: a nonasymptotic analysis. arXiv preprint arXiv:1702.03849, 2017.
G. Raskutti, M. J. Wainwright, and B. Yu. Minimax-optimal rates for sparse additive models over
kernel classes via convex programming. Journal of Machine Learning Research, 13(Feb):389-
427, 2012.
N.	Razin and N. Cohen. Implicit regularization in deep learning may not be explainable by norms.
arXiv preprint arXiv:2005.06398, 2020.
G. O. Roberts, R. L. Tweedie, et al. Exponential convergence of langevin distributions and their
discrete approximations. Bernoulli, 2(4):341-363, 1996.
C. J. Shallue, J. Lee, J. Antognini, J. Sohl-Dickstein, R. Frostig, and G. E. Dahl. Measuring the
effects of data parallelism on neural network training. arXiv preprint arXiv:1811.03600, 2018.
K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition.
arXiv preprint arXiv:1409.1556, 2014.
S. L. Smith, P.-J. Kindermans, C. Ying, and Q. V. Le. Don’t decay the learning rate, increase the
batch size. arXiv preprint arXiv:1711.00489, 2017.
11
Under review as a conference paper at ICLR 2021
D. Soudry, E. Hoffer, M. S. Nacson, S. Gunasekar, and N. Srebro. The implicit bias of gradient
descent on separable data. The Journal of Machine Learning Research ,19(1):2822-2878, 2018.
N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple
way to prevent neural networks from overfitting. The journal of machine learning research, 15
(1):1929-1958, 2014.
I. Sutskever, J. Martens, G. Dahl, and G. Hinton. On the importance of initialization and momentum
in deep learning. In International conference on machine learning, pages 1139-1147, 2013.
C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture
for computer vision. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pages 2818-2826, 2016.
Y. W. Teh, A. H. Thiery, and S. J. Vollmer. Consistency and fluctuations for stochastic gradient
langevin dynamics. The Journal of Machine Learning Research, 17(1):193-225, 2016.
R.	Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267-288, 1996.
T. Vaskevicius, V. Kanade, and P. Rebeschini. Implicit regularization for optimal sparse recovery. In
Advances in Neural Information Processing Systems, pages 2968-2979, 2019.
S.	Vaswani, F. Bach, and M. Schmidt. Fast and faster convergence of sgd for over-parameterized
models and an accelerated perceptron. In The 22nd International Conference on Artificial Intelli-
gence and Statistics, pages 1195-1204, 2019.
C. Wei, J. D. Lee, Q. Liu, and T. Ma. Regularization matters: Generalization and optimization of
neural nets vs their induced kernel. In Advances in Neural Information Processing Systems, pages
9709-9721, 2019.
C. Wei, S. Kakade, and T. Ma. The implicit and explicit regularization effects of dropout. arXiv
preprint arXiv:2002.12915, 2020.
M. Wei and D. J. Schwab. How noise affects the hessian spectrum in overparameterized neural
networks. arXiv preprint arXiv:1910.00195, 2019.
M. Welling and Y. W. Teh. Bayesian learning via stochastic gradient langevin dynamics. In Pro-
ceedings of the 28th international conference on machine learning (ICML-11), pages 681-688,
2011.
Y. Wen, K. Luk, M. Gazeau, G. Zhang, H. Chan, and J. Ba. Interplay between optimization and gen-
eralization of stochastic gradient descent with covariance noise. arXiv preprint arXiv:1902.08234,
2019.
A.	C. Wilson, R. Roelofs, M. Stern, N. Srebro, and B. Recht. The marginal value of adaptive
gradient methods in machine learning. In Advances in Neural Information Processing Systems,
pages 4148-4158, 2017.
B.	Woodworth, S. Gunasekar, J. D. Lee, E. Moroshko, P. Savarese, I. Golan, D. Soudry, and N. Sre-
bro. Kernel and rich regimes in overparametrized models. arXiv preprint arXiv:2002.09277,
2020.
Z. Xie, I. Sato, and M. Sugiyama. A diffusion theory for deep learning dynamics: Stochastic gradient
descent escapes from sharp minima exponentially fast. arXiv preprint arXiv:2002.03495, 2020.
S. Yaida. Fluctuation-dissipation relations for stochastic gradient descent. arXiv preprint
arXiv:1810.00004, 2018.
C.	Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires
rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.
Y. Zhang, P. Liang, and M. Charikar. A hitting time analysis of stochastic gradient langevin dynam-
ics. arXiv preprint arXiv:1702.05575, 2017.
Z. Zhu, J. Wu, B. Yu, L. Wu, and J. Ma. The anisotropic noise in stochastic gradient descent: Its
behavior of escaping from sharp minima and regularization effects. 2019.
12
Under review as a conference paper at ICLR 2021
A	Experimental Details
A.1 Experimental Details for the Quadratically-Parameterized Model
In the experiment of our quadratically-parameterized model, we use a 100-dimensional model with
n = 40 data randomly sampled from N (0,I100×100). We set the first 5 dimensions of the ground-
truth v? as 1, and the rest dimensions as 0. We always initialize with v[0] = 1. We use a constant
learning rate 0.01 for all the experiments except for label noise. For label noise, we start from 0.01
and then decay the learning rate by a factor of 10 after 1 × 105 and 2 × 105 iterations. For “full batch”
experiment, we run full batch gradient descent without noise. For “small batch” experiment, in order
to fully disentangle the effect of learning rate and mini-batch sgd noise (i.e., to avoid implicit biases
from large lr rather than noise), we add small batch noise to full gradient via the following sampling
method: for each iteration, We randomly sample two data i and j from [n], and add δ(N'i)(v) 一
V'((j)(v)) to the full gradient (We set δ = 1.0 in our experiment). For label noise, we randomly
sample i ∈ [n] and S ∈ {δ, 一δ} (we set δ = 1.0 in our experiment), and add noise V/(()(v) 一
V'(i)(v) to full gradient, where 'i)(v)，1 (f (x(i)) 一 y(i) 一 S)2. For Gaussian noise experiments,
we add noise ξ 〜N(0, σ2Id×d) to full gradient every iteration, where the values of σ are shown
in Figure 1. For experiments except for Gaussian noises, we train a total of 3 × 105 iterations. For
a more generous comparison, we run all the Gaussian noise experiments for 4 times longer (i.e.,
1.2 × 106 iterations) while plotting them in the same figure after scaling the x-axis by a factor of 4.
The test error is measured by the square of`2 distance between v2 and v?2, which is the same as
the expectation of loss on a freshly randomly sampled data. The trianing and test error are plotted
in Figure 1.
A.2 Experimental Details for Deep Neural Networks on CIFAR 1 00
We train a VGG19 model (Simonyan and Zisserman, 2014) on CIFAR100, using a small and large
batch baseline. We also experiment with adding Gaussian noise to the parameters after every gradi-
ent update as well as adding label noise in the following manner: with some probability that depends
on the current iteration count, we replace the original label with a randomly chosen one.
To add additional mean-zero noise to the gradient which simulates the effect of label noise in the
regression setting, we compute a noisy gradient of the cross-entropy loss 'ce with respect to model
output f(x) as follows:
∙-v
V f ( x) 'ce ( f ( x ), y ) = V f( x) 'ce ( f ( x ), D ) + σlnz	(12)
where z is a 100-dimensional vector (corresponding to each class) distributed according to
N(0, I100×100), and y is the (possibly flipped) label. We backpropagate using this noisy gradi-
ent when we compute the gradient of loss w.r.t. parameters for the updates. After tuning, we choose
the initial label-flipping probability as 0.1, and reduce it by a factor of 0.5 every time the learning
rate is annealed. We choose σin such that σn,E[∣∣z∣∣2] =0.1, and also decrease σin by a factor of
0.5 every time the learning rate is annealed.
To add spherical Gaussian noise to the parameter every update, we simply set W J W + σz after
every gradient update, where z is a mean-zero Gaussian whose coordinates are drawn independently
from N(0, 1). We tune this σ over the values shown in Figure 1.
We turn off weight decay and BatchNorm to isolate the regularization effects of just the noise alone.
Standard data augmentation is still present in our runs. Our small batch baseline uses a batch size of
26, and our large batch baseline uses a batch size of 256. In runs where we add noise, the batch size
is always 256. For all runs, we use an initial learning rate of 0.004. We train for 410550 iterations
(i.e., minibatches), annealing the learning rate by a factor of 0.1 at the 175950-th and 293250-th
iteration. Our models take around 20 hours to train on a single NVIDIA TitanXp GPU when the
batch size is 256. The final performance gap between label noise or small minibatch training v.s.
large batch or Gaussian noise is around 13% accuracy.
13
Under review as a conference paper at ICLR 2021
A.3 Additional Plots
Here we show empirical evidence that training with Gaussian noise fails to converge to a stationary
distribution. We train VGG19 network on CIFAR100, and plot the norm of model weight along the
training trajectory. As shown in Figure 2, the weigth norm of large batch (LB) and large batch with
label noise (LB+LN) both converge to some finite value, while the weight norm of large batch with
Gaussian noise (LB+GN) keeps increasing and fails to converge.
B ON≤6φm 6ioo>
100
0.0	0.5	1.0	1.5	2.0	2.5
Iteration	le5
large batch (LB)
lb, label noise (LB+LN)
∣b, σ=le-4 (LB+GN)
Figure 2: The norm of model weight along training trajectory. We demonstrate that the model
weight fails to converge when training with Gaussian noise. In contrast, the weigth norm converges
for training with label noise or without noise.
14
Under review as a conference paper at ICLR 2021
B Proof of Theorem 3.1
In this section, we will first prove several lemmas on which the proof of Theorem 3.1 is built upon.
Then we will provide a proof of Theorem 3.1.
Definition B.1. (b-bounded coupling) Let V [0] ,v [1],…,v [ T ] be a trajectory oflabel noise gradient
descent with initialization V[0L We call thefollowing random sequence V[t] a b-bounded coupling of
v [t ]: starting from V[0] = V [0] ,for each time t < T, if∣∣ V[t ] ∣∣1 ≤ b, we let V[t +1]，V [t+1]; otherwise
if ∖∖V[t] ∣∣1 > b we don't update, i.e., V[t+1]，V[t].
Lemma B.2. In the setting of Theorem 3.1, assume ∣x(i) ∣∞ ≤ bx for any i ∈ [n]. Let η ≤
6Tb2P2+7), where b0 = 费.Let V[t] be the b0-bounded coupling of V [t]. If V[t] is always positive on
each dimension, then with probability at least 1 — P, there is
∣∣V[T]∣∣1 ≤ b0.	(13)
Proof of Lemma B.2. Recall the update at t-th iteration is:
V[t+1] = V[t]	— η((V[t]	—	V?2)>x(it))x(it)	V[t]	—	ηstx(it)	V[t]	.	(14)
We first bound the increase of ∣∣ V[t] ∣∣1 in expectation. When ∣∣ V[t] ∣∣ 1 ≤ b0, there is:
E hVkt+1]i = Vkt] — ηE[((V[t]θ2 - V*o2)τx(())XkVkt]]	(15)
≤ Vkt]+ η (∣∣ V[ ^θ 2∣∣1 + ∣∣v*θ 2∣∣1) b x^∖tt]	(16)
≤ V[t ]+ η (b 0 + r) b X^∖tt]	(17)
where the first inequality is because We can separate the last term into V[t] θ2 part and v？θ2 part and
∣θ2 ∣	2	2
V[t]	∣∣ and ∣∣v？θ ∣∣1 respectively, the second inequality is by∣V[t]∣2 ≤ ∣V[t]∣1
and sparsity of v?. So summing over all dimensions we have E [∣∣ V[t+1] ∣∣1] ≤ ∣∣ V[t] ∣∣1 + ηb0bχ(b2 +
r). This bound is obviously also true when ∣∣ V[t] ∣∣ 1 > b0, in which case V[t+1] = V[t].
We then bound the probability of ∣∣ V[T] ∣∣ 1 being too large:
Pr (∣∣V[T]∣∣1>b0)≤「	(18)
≤ Td + Tnb 0b X(b 0 + T)	(19)
-	b 0	,，
≤ ρ,	(20)
where the first inequality is Markov Inequality, the second is by the previous equation, and the third
is by assumption of n and the definition of b0.	□
ProofofLemma 3.2. Notice that when ∣∣V[T]∣∣1 ≤ b0, there is V[T] = V[T], Lemma 3.2 naturally
follows from Lemma B.2.	□
Definition B.3. (b-bounded potential function) For a vector V that is positive on each dimension, we
define the b-bounded potential function Φ( v v) as follows: f∣∣v∣∣1 ≤ b, we let Φ( v )，P d =1 √^;
otherwise Φ(V) , 0.
Lemma B.4. In the setting of Theorem 3.1, let 0 = 1/d. Assume ∣x(i) ∣∞ ≤ bx for i ∈ [n] with
some bχ > 0, Ei[(Xk))2] ≥ 3 for all k ∈ [d]. Let b0 = 6pd. Assume nδbχ + n(b0 + r)bX ≤ ɪ6,
nδ2 ≥ 32(b0 + r)bX and T = ∣^^3∣2 log(p√)]. Let v[t] be the b0-bounded coupling of V[t], and
Φ(∙) is the b0-bounded potential function. If V[t] is always positive on each dimension, then with
probability at least 1 一 P, there is
Φ(V[T]) ≤√o.	(21)
15
Under review as a conference paper at ICLR 2021
,t,it ∖!vkt] - ηstxkit)vkt] - η((v[t]θ2 - v?&2)>x(it))Xkit)vkt]	(23)
vkt]Est,it V1 + ηstx(it) + η(b2 + r)bX	(24)
ProofofLemma B.4. We first show Φ(V[t]) decreases exponentially in expectation. If Uv[tU ≤ b0,
we have:
E [φ(V[t+1])i ≤ XXE [qvp11	(22)
k=1
d
=X
k=1
d
≤X
k=1
(25)
where the second inequality is because ∣∣v[t] ∣∣2 = ∣∣v[t] ∣∣2 ≤ b0. Toward bounding the expectation,
We notice that by Taylor expansion theorem, there is for any general function g (x) = √1 + x, there
is
g(I + X) ≤ g⑴ + g0⑴X + 1g0⑴X2 + MMIxl3,	(26)
where M is upper bound on ∣g000 ( 1 + x0 ) ∣ for x0 in 0 to x, which is less than 3 if ∣x∣ ≤ ɪ. So in our
theorem if ∆，ηstx^t) + η(b0 + r)bX ∈ [-2,11 ], We have
ʌ/1 + ηstχ(it) + η(b0 + r)bX ≤ 1 + 1δ - ∣δ2 + 11δ13.	(27)
28	2
Also since Est,it[ʌl = η(b0 + r)bX, Est,itA2] ≥ η2δ2Eit[(Xkit))2] ≥ 3η2δ2, we have when
IδI ≤ 116 and ηδ ≥ 32(b2 + r)bX, we have Est,it [√ 1 + △] ≤ 1- Est,it [1-16δ2] ≤ 1- 32η2δ2.
So
E [φ(v[t+1])i ≤ (1 - 32η2δ2)Φ(v[t]).	(28)
Also notice that when ∣∣v[t]∣∣ι > b0, there is Φ(V[t+1]) = Φ(V[t]) = 0, so obviously we have
E[Φ(V[t+1])] ≤ (1 - 32η2δ2)Φ(v[t]) always true.
Next we prove that Φ(V[T]) ≤ √0 with probability more than 1 - P. This is because:
E / n ʌ E [Φ(V[T])1
Pr (Φ(V[T]) > √0) ≤ L√0 "	(29)
≤ (1 - 32η2δ2)T&√τ	(30)
-	√0
≤ P	(31)
where the first inequality ifby Markov Inequaltiy, the second inequality is by the previous inequality,
and the last inequality is because T = ∣^^^ log(P√√T)].	□
Proof of Theorem 3.1. Let ρ = 0.01, 0 = 1/d. By Lemma G.1, and Lemma G.2, when n ≥
Θ(log d), with probability at least 1 - P there is ∣∣x(i)∣∣æ ≤ bfx for all i ∈ [n] with some bfx =
Θ(Plog(nd)), and Ei[(Xk))2] ≥ 3 for all k ∈ [d].
Let b0 = 6τd. We try to define η and δ such that when T = Γ -⅛ log( 3d√τ)], the assumptions η ≤
P	η δ P 0
6Tb2 Pb0+r) and V[t] always being positive in Lemma B.2 and assumptions ηδbX + η(b0 + Ir)bX ≤ 16
and ηδ2 ≥ 32(b02 + r)b2X in Lemma B.4 are satisfied.
Assume δ ≥ 6× 322bX雪+r) log(3√√τ), then weonly needη ∈ h6×32χ (b2 + r) log(3√√τ), 321bχi,
and then all the above assumptions are satisfied.
16
Under review as a conference paper at ICLR 2021
Let V[t] be the b0-bounded coupling of V[t]. According to Lemma B.4, We know with probability
at least 1 一 P, Φ(V[T]) ≤ √0, which means that either Pk =1 JVT] ≤ √0 or ∣∣V[τ]∣l > b0.
According to Lemma B.2, we know with probability at most P, ∣∣V[T] ∣∣] > b0. Combining these
two statements, we know with probability at least 1 一 2ρ, ∣∣ v[T]∣∣ι ≤ b 0 andP k=1 VvF ≤ √o.
Notice that ∣∣V[T] ∣∣ι ≤ b0 implies V[T] = V[T], while Pk =1 JVT] ≤ √0 implies VkT] ≤ E0 for all
dimension k, so we’ve finished the proof for the upper bound.
We then give a lower bound for each dimension of V[T]. We can bound the decrease of any dimension
k at time t:
Vktt+1] ≥ (1 — ηδ — η(b0 + r))Mt]
≥ (1 — 2ηδ)v[t].
(32)
(33)
where the first inequality is by update rule and the second is because δ > (b02 + r). Putting in the
value of T, we have
VkT] ≥ (1 — 2ηδ)TT
> exp
64
(ηδ
log(
(34)
(35)
□
17
Under review as a conference paper at ICLR 2021
C Proof of Theorem 3.3
In this section, we will first prove several lemmas on which the proof of Theorem 3.3 is built upon.
Then we will provide a proof of Theorem 3.3.
Definition C.1. ((b, e)-bounded coupling) Let V [0] ,v [1],… ,v [ T ] be a trajectory of label noise gra-
dient descent with initialization V [0L Recall S ⊂ [d] is the support set of v?, we notate vS] a
r-dimensional vector composed with those dimensions in S of V[t], and VS] the other d — r dimen-
sions. We call the following random sequence V[t] a (b, E) -bounded coupling of V [t]: starting from
V[0] = v[0], for each time t <	T,	if ∣∣vSt]	∣∣ ≤ e and	∣∣vSt] ∣∣	≤	b,	we let	V[t+1]，V[t+1];	otherwise
V[t+1]，V[t]
Lemma C.2. In the
maxj6=k |Ei[x(ji)x(ki)]|.
for k ∈ [d]. Let V[t]
e + r)CxbX ≤ C0
llVT]IL ≤1+c 1∙
setting of Theorem 3.3, let P ，忐,c 1 ， ɪθ, €1 ， 1p, Cx ，
Assume ∣∣x(i) ∣∣L ≤ bx for i ∈ [n] for some bx > 0, and Ei[(Xk))2] ≥ 3
be a (1 + C 1, €1)-bounded coupling of V [t]. Assume 8/2 b2 ≥ log 6rρT2,
and δ ≥ bx (<≡2 + r). Then, with probability at least 1 — P, there is
Proof of Lemma C.2. For any fixed 1 ≤ t1 < t2 ≤ T and dimension k ∈ S, we consider the event
that V[11 ] ∈ [1 + c3, 1 + 号],and at time 12 it is the first time in the trajectory such that V[12] > 1 + C 1.
We first bound the probability of this event happens, i.e., the following quantity:
Pr (vlt2] > 1 + c1 ∧ vltt 1] ≤ 1 + c- ∧ Vk11:12] ∈ [1 + C1, 1 + c1]) ,	(36)
where VF 1:12] ∈ [1 + c1, 1 + C 1 ] means that for all t such that 11 ≤ t < 12, there is 1 + c1 ≤ v[t] ≤
1 + C1 .
Notice that when ∣∣vS] ∣∣ ≤ €1 and ∣∣ V[t] ∣∣	≤ 1 + C 1 and V[t 1:t +1] ∈ [1 + c1, 1 + C 1], there is
E[vlt +1] — 1] =Es,,i J(1 + ηstxk - η((v[t]θ2 - V*132)τx(it))Xk)) vf] - 1]	(37)
≤(v[t] -1) - Iηvlt](vlt] + 1)(vlt] -1) + η陪CCx + rex)bxv[tt]	(38)
≤(1 - η)(Vk] -1) ∙	(39)
where the first inequality is because ∣∣vS] ∣∣ ≤ €2 and Eit [(Xk))2] ≥ ɪ, the second inequality is
because (€彳 + r)bxCx ≤ ∣o. Also, We can bound the variance of this martingale as
Var hV[t+1] - 1 I V[t]	-	1]	= Var	[ηstx^t)V[tt]]	+ Var	[η((V[t]°2	-	v*32)τx(it))Xat)V[tt]]	(40)
≤ (ηδbx(1 + C 1))2 + η2(€1 + r)2bX(1 + C 1 )2	(41)
≤ 4η2δ2b2x ,	(42)
where the first inequality is because ηstX(kit )V€k[t] is mean-zero, the second inequality is by ∣X(i) ∣L ≤
bx, the third inequality is by δ ≥ bx (€€12 + r).
By Lemma G.3, we have
Pr(V€k[t2] - 1 > C1)
-c11
≤e 8η2δ2bχpt=-t1-γ(-)2t
(43)
(44)
(45)
-c 2
≤e8ηδ2bX ,
where the first inequality is by Lemma G.3, the second inequality is by taking the sum of denomina-
tor.
18
Under review as a conference paper at ICLR 2021
Finally, we finish the proof with a union bound. Since if ∣ VS ] ∣∣	> 1+ C 1, the event in Equation 36
has to happen for some k ∈ S and 1 ≤ t1 < t2 ≤ T, so we have
Pr
> 1 + c1
∞
≤∑ E	Pr V12]> 1 + q ∧ v^t11]≤ 1 + % ∧ V^t11t 2]
k∈S 1≤t1<t2≤T
(46)
∈ [1 + -161 + c1])	(47)
≤
≤
-C 1
rT 2e 8 ηδ 2 b X
P
6 6
(48)
(49)
where the last inequality is by assumption.
□
Lemma C.3. In the setting ofLemma C.2, assume 伍2 + r) Cx ≤
least1 - 6，there is
∙-v
€i.
12Tnb2. Then, with probability at
ProofofLemma C.3. We first bound the increase OflvS] ∣∣ in expectation. When
≤ 1 + c1
∞
and
∈ι, for any k ∈ S, there is:
E Ivkt+1]] = vktt] — nEJ((V[t]θ2 — v?&2)>χ(i)
≤ v[t]+n信+ r)Cxbxv[kt].
(50)
(51)
because we can bound the dimensions in S and those not in S respectively. So summing over all
dimensions not in S we have E
≤
∙-v
> €].
1
+ η(<≡2 + r)CxbX^ι. This bound is obviously
also true when
> 1 + c1 and
∞
We then bound the probability of
being too large:
Pr
≤ Emj.
∙-v
__ ~
€1
≤ 1 + Tn€i(€1 + T) Cxb x
∙-v
(52)
∙-v
€1
(53)
≤ P
(54)
where the first inequality is Markov Inequality, the second is by
1 since every dimension
is less than 1 /d, the third inequality is because ∈ι = 162 and (<≡2 + r)Cx ≤ Tnb2.
□
Lemma C.4. In the setting of Lemma C.2, assume 伍2 + r)Cxbx < c2 — c1, nδ2 ≤
16 log ^1- and δT ≥ 29 log 6r. Then, for any k ∈ S, with probability at least 1 一
⅛, Tn ≥
6r, either
maxt≤τ VP ≥ 1 - c1, or
> 1 + c1 , or
∞
∙-v
€1.
ProofofLemma C.4. Fix k ∈ S. Let V[t] be the following coupling of V[t]: starting from V[0] = V[0],
for each time t < T , if
€1 and
≤ 1 + C1 and It] ≤ 1 一 号,we let V[t+1]，V[t +1];
∞
otherwise V[t+1]，(1 + 号n)v[t]. Intuitively, whenever v[t] exceeds the proper range, We only times
v[t] by 1 + 号n afterwards, otherwise we let it be the same as v[t].
19
Under review as a conference paper at ICLR 2021
We first show that -t log(1 + c1 η) + log ^k] is a supermartingale, i.e., E[log vkt+1] ∣ v[t]] ≥ log(1 +
C1 η) + logvk]. This is obviously true if 卜SR > 6ι or 卜^J > 1 + C1 or vk] > 1 —号.
Otherwise, there is	∞
E[log^kt +1] ∣ ^[t]] = E[logvkt+1] ∣ v[t]]	(55)
=Est,it [log(1 + ηst — η(v[t]θ2 — v*o2)>x(it)x(kt))i + logvk]	(56)
≥ Est log(1 + ηst + 3 η (IT vkt ] )2) - η 陪 + r) Cxb X)+log vkt ] (57)
≥ log(i + clη) + log 乩t],	(58)
where the first inequality is by the update rule, the second inequality is because 陪 + r)Cxbx <
2
患 一 寸 and 4ηδ2 ≤ 号 and δ ≥ e2 + r. So by Azuma inequality, We have
2(T Iog (I + ? η)+log e∙min-log(1 - C1))
≤e	T (2 ηδ )2
(2 T 1og(1 + c1 η ))2
≤e-	2 Tη 2 δ 2
Tc2
≤e-29δ2
≤
6r
(59)
(60)
(61)
(62)
(63)
where the first inequality is because Azuma inequality and Var[log Vkt+1] ∣ v[t]] ≤ (2ηδ)2, and the
second inequality is because T log(1 + c1 η) ≥ 2 log -ɪ which is true because Tη ≥ 16 log -^-,
4	min	c1	min
the third inequality is because log(1 + c1 η) ≥ C1 η, the last inequality is because T ≥ IT log 6. □
Lemma C.5. In the setting ofLemma C.2, assume (<≡2 + r)Cxbx ≤ ∣o and 肃2 ≥ log 6rT2. Then,
for any k ∈ S, with probability at least 1 —含,either maxt<τ ^[t] < 1 — c1 or Vk] ≥ 1 — C1 .
Proof of Lemma C.5. For any fixed 1 ≤ t1 < t2 ≤ T and dimension k ∈ S, we consider the event
that V[11] ∈ [1 — c1, 1 — c1 ], andattime 12 it is the first time in the trajectory such that V[12] > 1 < cι.
We first bound the probability of this event happens, i.e., the following quantity:
Pr Vkt212] < 1 — cι ∧ &11] ≥ 1 — cl ∧ Vkt1:12] ∈ [1 — c 1, 1 — c1 ]) ,	(64)
where V[t 1:12] ∈ [1 — c ι, 1 — c1 ] means that for all t such that 11 ≤ t <t 2, there is 1 — c ι ≤ Vkt ] ≤
1 一 ∣1
1	3 .
Notice that when 卜S] ∣∣ ≤ ∈ι and 卜S]∣∣	≤ 1 + Cι and vk11:t +1] ∈ [1 — Cι, 1 — c1 ],
E[1 — Vkt+1]] =Est,it [1 — (1 + ηstxik - η(V[t]©2 — v*。2)>x(it)X(it))v[kt]]	(65)
≤ (1 — v[t]) — 2 ηvkt] (vkt] + 1)(1 —vkt])+ η (?2 + r) Cxb xv[kt]	(66)
≤ (1 — η )(1 — vkt])∙	(67)
where the first inequality is because ∣∣vS] ∣∣ ≤ €2, the second inequality is because (<≡2 + r)Cxbx ≤
2∣0. Also, we can bound the variance of this martingale as
Var [1 — VF] ∣ vkt]] ≤ (2ηδ)2.
(68)
20
Under review as a conference paper at ICLR 2021
By Lemma G.3, we have	Pr(1 — vEk[t2] > ci)	(69) 	-c1	 ≤e8n262 Pt =-t1 -1(1 -η)21	(70) -C 2 ≤e中,	(71)
where the first inequality is by Lemma G.3, the second inequality is by taking the sum of denomina-
tor.
Finally, We finish the proof with a union bound. Since if maxt<τ vk] > 1 -号 but vkT] < 1 — C1,
the event in Equation 64 has to happen for some 1 ≤ t1 < t2 ≤ T , so we have
	Pr (maxVkt] > 1 - ci ∧ Ekt] < 1 一 ci)	(72) ≤ X Pr (vk12]< 1 — ci ∧ Vk11]≥ 1 — c1 ∧ Ek11t2]∈[1-ci,1- c~])	(73) i≤t1<t2≤T -C1 ≤T 2 e 中	(74) ≤ P	(75) 6r
□
Definition C.6. ((b, )-bounded potential function) Fora vector v that is positive on each dimension,
we define the (b, E)-bounded potential function Φ( v) as follows: if ∣vs ∣∣ 1 ≤ e and ∣vs ∣∣m ≤ b, we
let Φ( v)，fk/s √vk; otherwise Φ( v)，0.
Lemma C.7. In the setting of Lemma C.2, assume ∣ηδ2 > 32(e2 + r)CxbX and Tη2δ2 ≥
16log (jp√√dy Then, with probability at least 1 — P, there is Φ(v[T]) ≤ √1.
ProofofLemma C.7. We first show Φ(v[t]) decreases exponentially in expectation. For any 0 ≤
t ≤ T, if UvS] U ≤ 1 + cι and 卜S] ∣∣ ] ≤ Ei, we have:
E [①⑻t+1])i ≤ Xe[≠k+r1	(76)
k//S
d
= X Est,it
k//S
≤ X VzvFEst,it VZ 1 + ηstxkit + η(E∣ + T)CxbX	(78)
k//S
≤ (1 — ɪη2δ2)Φ(v[t]),	(79)
vEk[t] + ηstxiktvEk[t] — η(vE[t] 2 — v?2)>x(it)x(kit)vEk[t]	(77)
where the second inequality is because ∣∣vE[t] ∣∣22 ≤ EE12, the last inequality is by Taylor expansion
and 3ηδ2 > 32(Ei + r)CxbX. Also notice that when
> 1 + ci OrUVS] ∣∣ι > Ei, there is
Φ(v[t +i]) = p(v[t]) = 0, so obviously we have E[Φ(v[t+i])] ≤ (1 一 ɪ6η2δ2)Φ(v[t]) always true.
21
Under review as a conference paper at ICLR 2021
Next We bound the probability of Φ(V[T]) ≤ √1:
Pr(Φ(V[T]) > √1) ≤
E[Φ(V[T])]
√1
(I 一 ι⅛η2δ2)T√d
(80)
(81)
≤
≤
(82)
ρ
6.
≤
(83)
where the first inequality if by Markov Inequaltiy, the second inequality is by the previous inequality
and initially Φ(v[0]) ≤ √d, the third is by 1 一 x ≤ e-x for any x ∈ R, and the last inequality is by
Tη2 δ2 ≥ 16log ( p6√d ).
□
ProofofTheorem 3.3. Let P = 0.01, cι = 0. 1, eɪ，12, Cx，maxj=k ∣Ei[xj* iXki)]|. Let bx =
y2log 30d2 = Θ(1). According to Lemma G.1, when n ≤ d, there is with probability at least
1 - -P we have ∣∣x(i)∣∣∞ ≤ bx for i ∈ [d].
Assume δ be positive number such that ∣6 log Pe 6v√τ ≤ 1 and δ ≥ bx(e2 + r). (since emin
exp(-O(1)) this means δ ≥ Θ(r + log(1/1)) .) Let P
min{Q,急} = Θ(δ-2), T 一η 2 δ 2 log
Pemin √e 1
2
32 δ2 bX∖og P , Q = 2log P, η
Θ(log(1 /e 1)/η). Assume Cxbx(<≡2 + r)
min
θ(ρ∕l°g(1 Ie-)). (this means Cx ≤ θ( r logP-/e 1) ).)
We show the assumptions in the previous lemmas are all satisfied. The assumption ^^^
log 6rT2 in Lemma C.2 is satisfied by
C 2
8ηδ2 b x
6rT2
≥ log----
ρ
6r 1
≥ log-----+4 log —
(84)
U η log η ≤
ρ
C 2
32δ2 bx log 6r
(85)
P,
(86)
≥


≤
≥
u
η
where the first is by T ≤ a,the second is by log ^r +4log 1 ≤ 4 log 6r log 1, and the last line is
true because
η log1
η
JQ
≤ Q ɪɑg P
__ pzlog Q . log1 /P、
=P F + "ɪ)
≤ P.
(87)
(88)
(89)
The assumption δ ≥ bχ(e2 + r) in Lemma C.2 is satisfied by definition of δ. The assumption
信2 + r)CxbX ≤ 2Co in Lemma C.2 is satisfied by
CxbX(转+ T) ≤ η48 ≤ 96
≤ 20，
Where We use
ηδ2 ≤ δ2 P ≤ 号.
Q2
(90)
(91)
22
Under review as a conference paper at ICLR 2021
The assumption (<≡2 + r)Cxbx ≤ IT in Lemma C.3 is satisfied by assumption of Cx.
The assumption T ≥ c9 Iog ^r in Lemma C.4 is satisfied by
T 16	6r	26	6r	29	6r
不 ≥ η2δ4logP ≥ C4logP ≥ Crlog
(92)
The other two assumptions (€2 + r)Cxbx < 12 一 CL and ηδ2 ≤ c1 in Lemma C.4 follows from
Cxbx (€2 + r) < 嗒 and ηδ2 ≤ CL. The assumption Tη ≥ c6 log ^ɪ- in Lemma C.4 is satisfied by
the definition of T .
22
The assumptions 传2 + r)Cx ≤ co and ^^ ≥ log 6T in Lemma C.5 are satisfied by the same
reason as that of Lemma C.2. The assumption Tη2 δ2 ≥ 16 log
in Lemma C.7 is satisfied
by the definition of T, the assumption 22ηδ2 > 32(61 + r)CxbX in Lemma C.7 is satisfied by the
definition of Cx .
IogPl/1) )2) data, there is with probability at
> =Θ(log(1 /e 1)/η). Meanwhile, according
Since data are randomly from N(0, I), with n ≥ Θe((rl
least 1 一 P there is Cxbx(e2 + r) ≤ min { nδ2,适为 }
there is Ei[(Xk))2] ≥ 3 for
to Lemma G.2, with n ≥ Θ(1) data with probability at least 1 一 P
all k ∈ [d]. According to definition of bx, We know when n ≤ d, with probability at least 1 一 盒
there is also ∣∣x(i) ∣∣∞ ≤ bx for all i ∈ [n]. In summary, with d ≥ n ≥ Θ((r logy1) )2) data, with
probability at least 1 一 P there is Cxbx(€2 + r) ≤ min { %, ^^Tn } and Ei[(Xk))2] ≥ ∣ for all
k ∈ [d] and ∣∣x(i) ∣∣∞ ≤ bx for all i ∈ [n].
Now we use these lemmas to finish the proof of the theorem. Let v€[t] be a (1 + C1, €€1)-bounded
coupling of v[t], we only need to prove with probability at least 1 一 ρ, there is UvT] 一 1∣∣	≤ c 1
and ∣∣vS] ∣∣ ≤ € 1, which follows from a union bound of the previous propositions. In particular,
Lemma C.2 and Lemma C.3 tell us that probability of
> 1 + c 1 or UvS] ∣∣ι > €1 is
at most P. Lemma C.4 and Lemma C.5 tell us for any k ∈ S, probability of v∖T ] < 1 一 Cι
and UvT] ∣∣	≤ 1 + C1 and UvT] ∣∣ ≤ €1 is at most 3pr. Lemma C.7 tells us the probability of
UvT] ∣∣ > e 1 and UvT] ∣∣ ≤ 1 + C1 and UvT] ∣∣ ≤ €1 is at most ρ. Combining them together tells
us that probability of ∣vS] 一 1∣∣ > c 1 OrUvT]∣∣ >
□
1 is at most ρ.
23
Under review as a conference paper at ICLR 2021
D Proof of Convergence to Ground Truth
The conclusion of Theorem 3.3 still allows constant error in the support, namely, kvS - vS? k∞ ≤
1/10. The following theorem shows that further annealing the learning rate will let the algorithm
fully converge to v? with any target error. The end-to-end proof of the convergence to ground truth
can be found in the next section (Section E).
Theorem D.1. Let s ≥ 2 be the index of the current round of bootstrapping. Let constant c0 = 1/10.
In the setting of Theorem 2.1, assume v[0] is an initial parameter satisfying kvS[0] - vS? k∞ ≤ cs-1
and ∣∣v[0] — VS ∣∣ ι ≤ ts-1, where 0 < βs-1 ≤ Cs-1 ≤ c0. Given a failure rate ρ > 0. Assume
n ≥ Θ(r2). Suppose we run SGD with label noise with noise level δ ≥ 0 and learning rate
η ≤ Θ(cs/(δ2 + r2)) for T = log(4/co)/η iterations. Then, with probability at least 1 — P over the
randomness of the algorithm and data, there is ∣∣v[T ] — VS ∣∣m ≤ Cs，cs-1 c o and ∣∣v[T ] — VS ∣∣ ι ≤
es，(4/c o)2 CsT es-1. Here Θ (∙) omits poly logarithmic dependency on ρ.
In the rest of this section, we will first prove several lemmas on which the proof of Theorem D.1 is
built upon. Then we will provide a proof of Theorem D.1.
Definition D.2. ((b, )-to-v? coupling) Let V [0] ,v [1], •…,V [t] be a trajectory oflabel noise gradient
descent with initialization V[0]. Recall S ⊂ [d] is the support set of V?. We call the following random
sequence V[t ] a (b,e)-to-V? coupling of V [t ]: starting from V[o] = V [0], for each time t < T ,if
UVS] U ≤ e and UVS] — 1∣∣ ≤ b, we let V[t+1]，V[t +1]; otherwise V[t +1]，V[t].
Lemma D.3. In the setting of Theorem D.1, let Cx , maxj6=k |Ei[x(ji)x(ki)]|. Assume x(i) ∞ ≤ bx
for i ∈ [n] for some bχ > 0, and Ei[(Xk))2] ≥ 2 for k ∈ [d]. Let V[t] be a (2cs- 1, es)-to-V?
coupling of V [t ]. Assume 2 ”b X( ^ z++-； ^+ r )2) ≥ log 1oτ and ((?s + 4 rcs- 1) Cxb X ≤ 储.Then,
with probability at least 1 — P, there is ∣ VST] — 11∣	≤ 2cs- 1.
Proof of Lemma D.3. For any fixed 1 ≤ t1 < t2 ≤ T and dimension k ∈ S, we consider the
event that V[11] ∈ [1 + 2cs- 1, 1 + cs- 1], and at time 12 it is the first time in the trajectory such that
V[12] > 1 + 2cs- 1. We first bound the probability of this event happens, i.e., the following quantity:
Pr (Vk12] — 1 > 2cs- 1 ∧ Vk11] — 1 ≤ cs- 1 ∧ Vk11:12] ∈ [1 + 2cs-1, 1 + 2cs-1]) ,	(93)
where Vk 1:12] ∈ [1 +1cs- 1,1+2cs- 1] means that for all t such that 11 ≤ t < 12, there is 1 +1cs- 1 ≤
Vkt] ≤ 1 + 2cs- 1.
Notice that when ∣∣vS] ∣∣ ≤ es and ∣∣V[t] — 1∣∣	≤ 2cs- 1 and V[t 1:t+1] ∈ [1 + ∣cs- 1,1 + 2cs- 1],
there is
E[V[tt +1] — 1] =Est,it h(1 + ηstXk — η(V[t]G2 — V?G2)>x(it)Xkt))VJt] — 1]	(94)
≤ (Vkt] — 1) — 3 ηVkt](vkt] +1)(vkt] — 1) + η (≡2 + 4 rcs-1) Cx b xv[kt]	(95)
≤ (1 — η)(Vlkt] — 1).	(96)
where the first inequality is because ∣∣ VS] ∣∣ ≤ e2 and properties of the data, the second inequality is
because (e2 + 4rcs- 1)bxCx ≤ c--. Also, We can bound the variance of this martingale as
Var hV[t+1] — 1 I	V[t]	— 1]	= Var	[ηstx(^t)V[tt]]	+ Var	[η((V[t]θ2	-	v?g2)>x(it))Xkit)V[tt]]	(97)
≤ (ηδbx (1 + 2cs-1))2 + η2(2s + r)2b4x(1 + 2cs-1)2	(98)
≤ 4η2b2x(δ2 +b2x(s2 + r)2),	(99)
24
Under review as a conference paper at ICLR 2021
By Lemma G.3, we have
Prek2] - 1 > 2CS-1 ∧ vk- 1 ≤ CS-1 ∧ v,k∈ [1 + 3CS-1,1 + 2CS-1])
—c S- 1
(100)
(101)
(102)
≤e 2η2bX (δ2十bX (W 十r)2) Pt =-t1-1(ι-η)21
-C S-1
≤e2ηbX(δ2十bX(W十T)2),
where the first inequality is by Lemma G.3, the second inequality is by taking the sum of denomina-
tor.
Similarly, we bound
Pr (1 — Vkt2] > 2Cs-1 ∧ 1 - Vkt1] ≤ Cs-1 ∧ Vkt1：12] ∈ [1 - 2Cs-i, 1 - 2Cs-i]) .	(103)
Notice that when 卜S]|| ≤ g and 卜S] - 1( ≤ 2CS-1 and vk1：t+1] ∈ [1 - 2CS-1, 1 - ∣CS-1],
there is
E[1 - vk+1]] =EStdhI - (1+ ηstχik - η(V[t]G∣ - V?01)τx(2)XP)vkt]]	(104)
≤(1 -	v[t])	- 3ηvf](vkt+1)(1 - vkt])	+ η(e∣	+4F-I)CXb泯]	(105)
≤(1 -	η)(1	- VF]).	(106)
where the first inequality is because ∣∣vS] ∣∣
because (e∣ + 4vcs- 1)CXbX ≤ C-I. So
≤ e∣ and the properties of data, the second inequality is
Pr(1 - Vk2] > 2CS-1 ∧ 1 - at1] ≤ CS-1 ∧ VF1：t2] ∈ [1 - 2CS-1, 1 - 3CS-1])
⅛
(107)
(108)
(109)
≤e 2n2bX (δ2十bX(W十r)2) Pt =-t1-1(1-n)21
-C W-1
≤e2nbX(δ2十bX(W十T)2),
where the first inequality is by Lemma G.3, the second inequality is by taking the sum of denomina-
tor.
Finally, we finish the proof with a union bound. Since if ∣∣VT] - 1∣∣	> 2CS-1, either event in
Equation 93 or in Equation 103 has to happen for some k ∈ S and 1 ≤ 11 <t∣ ≤ T, so we have
Pr (∣∣vT]- 1L >2 J)	(110) 111 112
≤ X X Pr ⅛12] - 1 > 2 Cs- 1 ∧ Vk11] - 1 ≤ Cs- 1 ∧ V[t1： t 2] ∈ [1 + 3 Cs- 1, 1+2 Cs-1])
k∈S 1 <t 1 <t 2 ≤T	'	)
(111)
+ X X Pr (1 - Vk 引 > 2 CS- 1 ∧ 1 - vf" ≤ CS- 1 ∧ VFl "H ∈ [1 - 2 cs- 1, 1 - 3 cS- 1])
k∈S 1 <t 1 <t 2 <T	'	)
(112)
_____-C c2_1_
≤ 2 rT ∣ e 2 nb X (δ 2十b X (e2 十 T )2)
≤P
5
(113)
(114)
where the first inequality is by union bound, the second inequality is by previous results, the third
inequality is by assumption of this lemma.	□
25
Under review as a conference paper at ICLR 2021
Lemma D.4. In the setting of Lemma D.3, assume (s2 + 4cs-1r)Cxb2x ≤ cs-1, s > (1 +
ηcs-1)TCs-1 and (T蒜/九2 ：：—；夕)：2 ≥ log P ∙ Then, With probability at least 1 - 5, there
is HT] ∣∣1 ≤ es.
Proof of Lemma D.4. When
归 ]- 1L
≤ 2cs-1 and 卜S]( ≤
Cs, for any k ∈/ S, there is:
E Ht+1]] = vkt] - ηE* [((v[[]θ2 - V*θ2)>x(it))xktvkt]]	(115)
≤ v[t]+η(卜^θ2∣∣1 +4Cs-1 r)CxbXi[kt]	(116)
≤ vkt]+η(我 +4Cs-1 r)Cxbxi,[kt]	(117)
≤ (1+ ηCs-1)vkt]∙	(118)
where the first inequality is because we can bound the dimensions in S and those not in S with
θ2
∣1 Cxb2x and 4Cs-1rCxb2x respectively, the second inequality is by
third is because (Cs2 + 4Cs-1r)Cxb2x ≤ Cs-1. Summing over all k ∈/ S we have E[
(1 + ηcs-1) ^vSJ . This bound is obviously also true when ∣∣vS] — 1∣∣	> 2Cs-1 or
2
≤
2
in which case vv[t+1] = vv[t] .
Therefore we know (1 + ηCs-1)-t ∣vv[t] ∣1
is a supermartingale.
Also notice
E『STIj
≤ ηδCs, By Azuma Inequality,
怩TL-
Pr (∣∣VT]∣∣1 >G
((I + ηcs- 1 )-Tes-% — I)2
≤e- 2 Tη 2 b X ( δ 2+ b X ( e S + r )2) e2
≤ ρ.
-4
(119)
(120)
(121)
here We are using e5 > (1 + nc§_ 1)TCs-1 by assumption and the last step is by assumption. □
Lemma D.5. In the setting of Lemma D.3, assume (1 一 η)T2Cs-1 < cs,
(c2 +4Cs-1 r)Cxbx ≤ Co. Then,forany k ∈ S, with probability at least 1 — &
11 ≥ cs, or ∣∣VS] - 1∣∣ > 2Cs-1, or ∣∣VST]∣L > g.
cηδ1 ≥ log 5r,and
either min≤T |vvk[t] -
ProofofLemma D.5. We first consider when Vk] ∈ [1 + Cs, 1 + 2Cs-1]. For some t < T2, if
∣∣VSt] - 1∣∣	≤ Cs-1 and ∣∣vS] ∣∣ ≤ cs, there is
E[Vvk[t+1] - 1]	(122)
=VF] - ηEit[((V[t]θ2 - v*θ2)>x(i))Xkit)]vkt] - 1	(123)
≤(vkt] -1) - 3ηvkt](vkt] + 1)(vkt] -1)+ η(≡2 +4Cs-1T)Cxbxv[kt]	(124)
≤(1 - η)(Vk[t] -1).	(125)
Here the first inequality is by assumption, the second inequality is because (Cs2 + 4Cs-1r)Cxb2x ≤
10 Cs and Cs-1 ≤ 10.
26
Under review as a conference paper at ICLR 2021
We define the event Et as ∣∣vSt] - 1∣∣	> 2Cs-1 or 卜/] ∣∣ > g. Since (1 - η)T22Cs-1 ‹ cs by
assumption, if v[0] ∈ [1 + cs, 1 + 2Cs-1], by Lemma G.4 we know:
Pr (mn vkt]> 1+cs ∧ ∣∣vT] — 1L ≤2 cs- 1 ∧ ∣∣vT] ∣∣ι ≤ ^s)
(2 CS (1-η)-T-cs-1)2
≤ e- 2ηbx(δ2+bx(≡S+r)2)
________cSS-ι_____
≤ e 2ηbX(δ2+bX“S+r)2)
≤ f
5r
where the second inequality is because of assumption.
Similarly, when v[0] ∈ [1 - 2Cs- 1, 1 - cs], there is
Pr (max Vkt<1 - 'q ∧ ∣∣vT2- 1∣L ≤2 cs- 1 ∧ ∣∣vT] ∣∣1 ≤ 力
C S- 1
≤ e 2ηbX(δ2+bX(eS+r)2)
≤ 5r∙
(126)
(127)
(128)
(129)
(130)
(131)
(132)
Since ∣V[0] - 11 ≤ Cs- 1 by assumption of Theorem D.1, by bounding the probability for V[0] ∈
[1 + cS, 1 + 2Cs- 1] and [1 - 2Cs- 1, 1 - C] repectively We finished the proof.	口
Lemma D.6. In the setting of Lemma D.3, assume
8ηbX (δ2 + bX (eS + r )2)
≥ log 5rpT2 and (e22 +
2rCs)CxbX ≤ 2S. Then, for any dimension k ∈ S, with probability at most 5r, there is
mint≤ ∣vk] - 11 ≤ 2Cs and ∣v∖τ] - 11 > Cs and ∣∣vST] - 1∣∣	≤ 2Cs- 1 and 卜T] ∣∣ ≤ es.
ProofofLemma D.6. For any fixed 1 ≤ 11 < 12 ≤ T, we consider the event that V[11] ∈ [1 +
31 Cs, 1 + 2Cs], and at time 12 it is the first time in the trajectory such that V[12] > 1 + Cs. We first
bound the probability of this event happens, i.e., the following quantity:
Pr 卜 k12]-1 > Cs ∧ 乩11 ] -1 ≤ 2 Cs ∧ vk11:12]
∈ [1 + 3Cs,1 + Cs]) ∙
(133)


Notice that when ∣∣vS] ∣∣ ≤ es and ∣∣vS] - 1∣∣	≤ 2Cs- 1 and vk 1:t+1]
∈ [1 + 3Cs, 1 + Cs], there is
E[乩t +1] - 1] =Esd [(1+ ηstxik - ηeit[(v[t]θ2 - v?θ2)τx((i))Xk)])vkt] - 1]	(134)
≤(ν[t] -1) - 3ηvkt] (vkt] + 1)(vkt] - 1)+ η(e2 + 22∙Cs)Cxbxv[tt]	(135)
≤(1 - η)(ν[t] -1).	(136)
where the first inequality is because ∣∣vS]∣∣	≤ eS, the second inequality is because (e2 +
2rCs)Cxbx ≤ 20. Also, we can bound the variance of this martingale as
Var hvktt+1] - 1 ∣ v[t] - 1] = Var [ηstx^t)vktt] + Var [η((v[t]°? - v?θ2)τx(it))Xat)^[t]] (137)
≤ (ηδbx(1 + Cs))2 + η2(es2 + r)2b4x(1 + Cs)2	(138)
≤ 4η2b2x(δ2 + b2x(es2 + r)2),	(139)
27
Under review as a conference paper at ICLR 2021
By Lemma G.3, we have
Pr(Vkt2] - 1 >cs ∧ Vkt 1] - 1 ≤ 2Cs ∧ Vk11:12] ∈ [1 + ∣c,, 1 + Cs])
-2
≤e 8 η 2 b X ( δ 2+ b X ( e S 十 r )2) P T-1(1 -η )2 t
-C2
≤e 8 ηb X ( δ 2+ b X (e 2 + r )2)
(140)
(141)
(142)
where the first inequality is by Lemma G.3, the second inequality is by taking the sum of denomina-
tor.
Similarly, we bound
Pr(1 -乩t2] > C ∧ 1 - Vk 1]
______-Cl_______
≤ e 8 ηb X ( δ 2+ b X ( e2+ r )2) .
≤ 2Cs ∧ ^kttI t2] ∈ [1 - CsJ - 3CsD
(143)
(144)
Finally, we finish the proof with a union bound:
Pr (mmin lvkt] - 1 | ≤ 1 Cs ∧ lvl∣TT] - 1 | > Cs ∧ J^ls ] - 1L ≤ 2CsT ∧ JvSR ≤ es) (145)
≤ X Pr(Vkt2] - 1〉C ∧ Vkt 1] - 1 ≤ 2Cs ∧ Vkt1:12] ∈ [1 + 3Cs, 1 + Cs])	(146)
1≤t1<t2≤T
+ X Pr(1 - Vltt2] >Cs ∧ 1 - Vltt 1] ≤ 2Cs ∧ Vkt 1:t2] ∈ [1 - Cs, 1 - ∣Cs])	(147)
1≤t1<t2≤T
________-cSS_______
≤ T2e8ηbX(δ2+b2(eS十r)2)	(148)
≤ ρ,	(149)
5r
where the first inequality is by union bound, the second inequality is by previous results, the third
inequality is by assumption of this lemma.	□
ProofofTheorem D.1. Let Cx，maxj=k |Ei [xjixk)] |, bχ = J2 log 3d = Θ(1). According to
Lemma G.1, when n ≤ d, there is with probability at least 1 -卷 We have Jx(i) Jæ ≤ bχ for i ∈ [d].
Set η small enough such that 8砧2(^2+曲(声+r)2) ≥ log 10rps2. Obviously we only need η ≤
c2	2	2
Θ( J2+r2), where Θ( ∙) omits poly logarithmic dependency on d and ρ. Assume (e2+4Cs-1r)CXbX ≤
C0, which can be represented as CX ≤ Θ(11). Recall T = ɪ log 卷,es = e2cs-1 log c0es-1.
We first show that the additional assumptions in the previous lemmas are satisfied. There is
(1 + ηCs-1)t = (1+ ηCs-1) 1log c0	(150)
≤ ecs-1 log c⅛，P.	(151)
The assumption s > (1 + ηCs-1)T s-1 in Lemma D.4 is therefore satisfied by definition of s. The
assumption 2T+C；-九2 ：；2+"：2 ≥ log P in Lemma D.4 is satisfied because:
((1 + ηCs-I)-TEsis-1)2 ≥	也P-1-P-2)2
2Tn2bX(J2 + bX(eS + r)2)eS ≥ 2Tn2bX(δ2 + bX(襦 + r)2)eS
>	(P -1)2
≥ 2Tn2bX(J2 + bX(e2 + r)2)
≥________C-__________
≥ 2nbX(J2 + bX(E + T)2),
(152)
(153)
(154)
28
Under review as a conference paper at ICLR 2021
which is larger than log P by the definition of η. The assumption (1 一 η)T2Cs-1 ≤ cs inLemmaD.5
is satisfied because (1 ~η)T2Cs-1 ≤ (ɪ)log 访 2Cs-1 = cs. All the other assumptions in Lemma D.3,
Lemma D.4, Lemma D.5and Lemma D.6 naturally follows from the definition ofη and the require-
ment of Cx .
Since data are randomly from N (0, I), with n ≥ Θ( r2) data, there is with probability at least 1 一 行
there is (/ + 4Cs-1r)CxbX ≤ 相.Meanwhile, according to Lemma G.2 with n ≥ Θ⑴ data with
probability at least 1 一 代 there is E i [(ʃ (k ))2] ≥ 3 for all k ∈ [d]. According to definition of bx, we
know when n ≤ d, with probability at least 1 一 卷 there is also ∣∣x(i) ∣∣æ ≤ bx for all i ∈ [n]. In
summary, with d ≥ n ≥ Θ(r2) data, with probability at least 1 一 P there is (e2 +4Cs-1r)Cxbx ≤ Co
and Ei[(Xk))2] ≥ 3 for all k ∈ [d] and ∣∣x(i)∣∣∞ ≤ bx for all i ∈ [n].
Now we finish the proof with the above lemmas. Lemma D.3 and Lemma D.4 together tell us that
with probability at least 1 一 25P, there is ∣∣vS] 一 1∣∣	≤ 2Cs-1 and ∣∣vS] ∣∣ ≤ es, in which case
∞
there is also v[T] = v[T] by the definition of v[T]. Lemma D.5 and Lemma D.6 together tell us
the probability of	∣∣VT] 一	1∣∣	≤	2Cs-1 and	∣∣VT] ∣∣	≤	es	and	∣∣VT]	一 1∣∣	>	Cs	is no more
∞1	∞
than 2P. So together we know with probability at least 1 一 ρ, there is ∣∣vT] 一 1∣∣	≤ Cs and
∞
29
Under review as a conference paper at ICLR 2021
E Proof of Theorem 2.1
ProofofTheorem 2.1. Starting from initialization T ∙ 1, by Theorem 3.1, running SGD with label
noise with noise level δ > O( τpd2) and ηo = Θ (1) for To = Θ (1) iterations gives Us that with
probability at least 0.99,5说 ≤ Vk0] ≤ ɪ where 5加=exp(-Θ(1)). Now v[T0] satisfies the
initial condition of Theorem 3.3.
Recall the final target precision is g set Eι = 4I3 E. By Theorem3.3, with n ≥ Θ(r2 log2(1 /e)) data,
after running SGD with label noise with learning rate η 1 = Θ (点)for Ti = Θ (bog:/)) iterations,
with probability at least 0.99, there is,
W0+T1]-VSIl	≤ c 1 , ɪ,	(155)
∞	10
and
IvS0+T1]-vS∣∣ι ≤ E1.	(156)
So we have V[T0+T1] satisfies the initial condition of Theorem D.1.
Finally, set ρ = 0.01/dlog10(1/E)e, and apply Theorem D.1 for ns = dlog10(1/E)e = Θ(1) rounds.
Since Cs gets smaller by 1 /10 for each round, the final Cns satisfies ɪ0 E ≤ Cns ≤ E. Since the
22
requirement of η for round S is η ≤ Θ(”+72), We can set η2 ≤ Θ(k)to satisfy all the rounds at
the same time. Set T2 be the total number of iterations in all of these rounds, obviously T2 = Θ( 卡).
Notice that Es ≤ 卓 ∞=2 2 CsTlog 青 EI ≤ 403E1 = E, we have with probability at least 0.99,
∣∣∣vS[T0+T1+T2] - vS? ∣∣∣∞ ≤C1,E,	(157)
and
IlVT0+ti+T2]-vs∣∣i ≤ E.	(158)
The total failure rate of above three stages is 0.03, so with probability at least 0.97, there is
Ilv[to+ T1+T2] - v? I∞ ≤ e, which finishes the proof.	□
30
Under review as a conference paper at ICLR 2021
F	Proof of Theorem 2.2
Lemma F.1. Assume n ≤ d — 9∖fd. Let C ⊂ Rd be the convex cone where each coordinate is
positive, K be a random subspace of dimension d - n. Then with probability at least 0.999, there is
K ∩C 6= {0}
Proof of Lemma F.1. By Theorem 1 of Amelunxen et al. (2014), we only need to prove
δ (C) + δ (K) ≥ d +9√d.	(159)
where δ(∙) is the statistical dimension of a set. By equation (2.1) of AmeIUnxen et al. (2014), there
is
δ(K) = d— n.	(160)
To calculate δ(C), we use Proposition 2.4 from Amelunxen et al. (2014),
δ(C) =E[kΠC(g)k2],	(161)
where g is a standard random vector, ΠC is projection of g to C, the expectation is over g. Since C
is the set of all points with element-wise positive coordinate, ΠC(g) is simply setting all the negative
dimension of g to 0 and keep the positive ones. Therefore,
δ (C) = E[H ∏ C (g) b 2] = d∙	(162)
Therefore we have
δ(C) + δ(K) = 3 d — n ≥ d + 9√d∙	(163)
□
Proof of Theorem 2.2. Let X⊥ be the subspace that is orthogonal to the subspace X spanned by
data. Since data is random, with probability 1 the random subspace X is ofn dimension. Therefore,
according to the previous lemma, with probability at least 0∙999, there is X⊥ ∩ C 6= {0}, where C
is the coordinate-wise positive cone. Let μ ∈ X⊥ be such a vector such that μ% > 0 for ∀i ∈ [d],
and We scale it such that ∣∣μ∣∣2 = 1. We can construct the following orthonormal matrix
A = [ a1,... ,ad ] ∈ R d×d,	(164)
such that span{a1, •…an} = X and an+1 = μ. Consider the following transformation
AU = U = v。2,	(165)
∙-v
since only the projection of u to the span of data influences L(V), we can write L(V) = L(Ui：n) as
a function of the first n dimensions of U.
We can lower bound the partition function with
v∈Rd e-λL(v)dV ≥ v>0 e-λL(v)dV
=Z	e-λL(ɪɪ1:九) det dv det ^dU
JAu > 0	du du
=B / 4小) UAu > 0 Y √u=du n+i：d) du1n∙
(166)
(167)
(168)
Here the inner loop is integrating over the last d — n dimensions of U in the set such that AU is
coordinate-wise positive. Now we prove that for each iUi：n such that S = {U,n+1；d∣AU > 0} is not
empty set, the inner loop integral is always +∞.
Fix Ui：n, let Un+1：d be one possible solution such that U = AU > 0. Define constant
ain+1
C = mm	max ---------i---------,
i≤[1：d] j∈[n+2：d] (d — n — 1) ∣aj I
(169)
31
Under review as a conference paper at ICLR 2021
we can define the following set
S = {un +1:d∖un +1 ≥ Un+1∧ ∖^^j j-吗 ∖ ≤ c (U n +1 - U n+1) Xj ∈ [ n + 2 ,d]}	(170)
In other words, this is a convex cone where constraint of Uj is linear in Un+1 for j ∈ [n + 2 : d]. By
definition of C, it is easy to verify that S0 is a subset of S. Also, for every Un+1：d ∈ S0, Ui is upper
bounded by
(A	U 1：n	) =	Ui	+	an+1(Un +1	-	Un +1) + X	aj (Uj	- Uj )
n LUn +1: d^i	j = n+2
≤ U +2 an +1( u n +1 - U n +1).
Here the inequality is because of the definition of C.
Let Z = U n +1 — U n+1 we have
Π ɪ du n +1： d
Jun +1:∈ Q 7U
d	d	1
≥	(2 cz)d-n-1 ∏	dz
J z≥0	i=1 个 U + 2 an+1Z
=+ ∞.
(171)
(172)
(173)
(174)
(175)
Here the last step is because n < d/2, so the integrand is essentially a polynomial of Z with degree
larger than -1, so integrating it over all positive Z has to be + ∞. So we finish the proof that
V∈RRd e-λL(v) dv = + ∞.
□
32
Under review as a conference paper at ICLR 2021
G Extra Lemmas
Lemma G.1. Suppose x(i) 〜N(0,14乂&) where i ∈ [n] are random data. Then with probability at
least 1 - ρ, for every i ∈ [n] there is
x (i )B ∞ ≤ flog2r
(176)
Proof. By Gaussian tail bound, there is Pr |x(ki) | > bx
Pr (maxi,k ∣xk) ∣ > bχ) ≤ 2nde-bL. Let bχ =《2 log
上
≤ 2e 一 ^W. Soby union bound We have
2nd We finished the proof.
□
Lemma G.2. Suppose x(i) 〜N(0, Id×d) where i ∈ [n] are random data. Then when n > 24 log d,
with probability at least 1 - ρ, for every k ∈ [d] there is
Ei[x(i)2] ≥ 3∙	(177)
Proof. Since Ex[xk2] = 1, Ex[xk4] = 3, by Hoeffding inequality We have
Pr (1 XXki)2 < 2! ≤ e-2n ∙	(178)
Therefore, when n ≥ 24 log d, by union bound we finish the proof.	□
Lemma G.3. Let c > 0, 1 > γ > 0 be real constants. Let A[0], A[1], ∙∙∙ , A[t], be a series of ran-
dom variables, such thatgiven A[0],…,A[t] for some t < T with A[t] ∈ [C ,c], there is either A[t]=
A[t +1] =…=A[t], or E[A[t+1]] ≤ (1 — Y)A[t] with variance Var[A[t +1] | A[0], ∙∙∙ ,A[t]] ≤ a.
Then there is
Pr A[T ]
>c ∧ a [0]≤C ∧
-c
A[0:t] ∈ [3,c]) ≤ e2αPT^-o(1 -γ)21 ∙
(179)
where A[0:t] ∈ [ C, c] meansfor any 0 ≤ t < T, A[t] ∈ [ C, c].
ProofofLemma G.3. We only need to consider when A[0] ≤ C. Let A[t] be the following coupling
of A[t]: starting from A[0] = A[0], for each time t < T, if A[t] = A[t +1] = ∙∙∙ = A[t] or there is
t0 ≤ t such that A[t0] / [C ,c],we let A[t +1]，(1 — Y)A[t +1]; otherwise A[t+1] = A[t+1]. Intuitively,
whenever A[t] stops updating or exceeds proper range, we only times A[t] by 1 — Y afterwards,
otherwise we let it be the same as A[t]. Notice that if the event in Equation 179 happens, there has
to be A[T] = A[t] (otherwise A[t] stops updating or exceeds range at some time, contradicting the
event). So we only need to bound Pr(A[T] > c).
We notice that (1 — Y)—tA[t] for t = 0 ∙∙∙ T is a supermartingale, i.e., given history there is
E[A[t +1] |A[t]] ≤ (1 — γ)A[t]. This is obviously true when v[t +1] = (1 — Y)A[t], and also true
otherwise by assumption of the lemma. So we have
Pr(A[T] > c)	(180)
= Pr((1 — Y厂TA[T] > (1 — Y)-TC)	(181)
2( c (i-γ)-T-A[0])2
≤e 一-P T-o1(1-γ)-2 Q	(182)
_	C 2(1 -Y ) - 2 T
≤e 2PT=o(1 -γ)-2(t +1)a	(183)
________C 2
=e 2a Pτ=01(1 -γ)21 ∙	(184)
33
Under review as a conference paper at ICLR 2021
where the first inequality is because of Azuma Inequality and Var [(1 — Y)-t+1 Al^t+1] ∣ A[tJ ≤
(1 — γ)-2(t +1)a. , the second inequality is because A[0] ≤ C. Since the event in Equation 179 only
happens when AT] > C, we,ve finished the proof.	□
Lemma G.4. Let 0 < c 1 < c2 be real constants. Let A[0] ,A[1], ∙∙∙ ,A[T], be a series of random
variables, such that given A [0], ∙∙∙ ,A [t ] for some t <T with A [t ] ∈ [ C 1 ,c 2], there is either event Et
happens, or E[A[t+1]] ≤ (1 — Y)A[t] with variance Var[A[t +1] ∣ A[0], ∙∙∙ ,A[t]] ≤ a. Then when
A[0] ∈ [C1, C2] and (1 — γ)TC2 < C1 there is
Pr mt≤iTn A[t] > C1 ∧ mt≤aTx A[t] ≤ C2 ∧ -E[0:T] ≤ e
2(c ι(1 -7 )-T-A [0])2
1∑
7
(185)

where -E [0： t ] meansfor any 0 ≤ t <T, Et doesn't happen.
ProofofLemma G.4. Let A[t] be the following coupling of A[t]: starting from A[0] = A[0], for
each time t < T ,if exists t0 ≤ t such that Et happens or A [ t0 ] / [ C 1 ,c 2], We let A[ t+1] ，(1 —
γ)A[t+1]; otherwise A[t +1] = A[t+1]. Intuitively, whenever A[t] exceeds proper range, We only
times A[t] by 1 — Y afterwards, otherwise we let it be the same as A[t]. Notice that if the event in
Equation 185 happens, there has to be A[T] = A[T] (otherwise Et happens sometimes or A[t] /
[C 1, C2], contradicting the event). So we only need to bound Pr(A[T] > C 1).
We notice that (1 — Y)-tA[t] for t = 0 ∙∙∙ T is a supermartingale, i.e., given history there is
E[A[t +1]∣A[t]] ≤ (1 — γ)A[t]. This is obviously true when ^[t +1] = (1 — Y)A[t], and also true
otherwise by assumption of the lemma. So we have
Pr(A[T] > c 1)	(186)
=Pr((1 — Y)-TAT] > (1 — Y)-Tc 1)	(187)
2( c 1(1-7 )-T-A [0])2
≤e 一 —P T=1(1 -7)- 2ta	(188)
2( c 1(1-7 )-T-A [0])2
≤e	7a	(189)
where the first inequality is because of Azuma Inequality and Var [(1 — Y)-t+1A[t+1] ∣ A[tJ ≤
2(c1(1-7 )-T-c 2)2
(1 — Y)-2(t +1) a and A[0] ≤ C2 ≤ e PT=-o1(1 -7)-2ta . Since the event in Equation 185 only happens
when A[T] > c 1, we,ve finished the proof.	□
34