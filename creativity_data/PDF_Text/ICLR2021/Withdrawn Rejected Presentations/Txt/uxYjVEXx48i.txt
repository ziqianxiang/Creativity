Under review as a conference paper at ICLR 2021
An Examination of Preference-based Rein-
forcement Learning for Treatment Recom-
MENDATION
Anonymous authors
Paper under double-blind review
Ab stract
Treatment recommendation is a complex multi-faceted problem with many con-
flicting objectives, e.g., optimizing the survival rate (or expected lifetime), mit-
igating negative impacts, reducing financial expenses and time costs, avoiding
over-treatment, etc. While this complicates the hand-engineering of a reward func-
tion for learning treatment policies, fortunately, qualitative feedback from human
experts is readily available and can be easily exploited. Since direct estimation
of rewards via inverse reinforcement learning is a challenging task and requires
the existence of an optimal human policy, the field of treatment recommendation
has recently witnessed the development of the preference-based Reinforcement
Learning (PRL) framework, which infers a reward function from only qualitative
and imperfect human feedback to ensure that a human expert’s preferred policy has
a higher expected return over a less preferred policy. In this paper, we first present
an open simulation platform to model the progression of two diseases, namely
Cancer and Sepsis, and the reactions of the affected individuals to the received treat-
ment. Secondly, we investigate important problems in adopting preference-based
RL approaches for treatment recommendation, such as advantages of learning
from preference over hand-engineered reward, addressing incomparable policies,
reward interpretability, and agent design via simulated experiments. The designed
simulation platform and insights obtained for preference-based RL approaches
are beneficial for achieving the right trade-off between various human objectives
during treatment recommendation.
1	Introduction
With recent advances in deep learning and open access to large-scale Electronic Health Records
(EHRs), Deep Reinforcement Learning (RL) approaches have gained popularity for treatment recom-
mendation (Raghu et al., 2017; Lopez-Martinez et al., 2019). But the success ofRL applications often
crucially depends on the prior knowledge that goes into the definition of the reward function (Wirth
et al., 2017). However, treatment recommendation is a multi-faceted problem where the reward
function is hard to engineer and requires quantifying the trade-off between diverse types of realistic
objectives. For instance, clinicians often aim to optimize the survival rate (or expected lifetime) while
mitigating negative impacts of the treatment (Raghu et al., 2017; Lopez-Martinez et al., 2019; Wang
et al., 2018). However, they also keep in mind, the patient’s considerations of financial expenses
and time costs in accepting treatment strategies (Faissol et al., 2007; Denton et al., 2009). Moreover,
unnecessary or over-treatment needs to be avoided and certain agreements based on the patient’s
medical insurance plan also need to be followed for an affordable treatment (Nemati et al., 2016).
To explicitly reflect human’s objectives in the reward function, prior work jointly considers multiple
objectives weighted linearly to reduce the problem to a single-objective MDP (Faissol et al., 2007;
Denton et al., 2009). However, the linearly weighted reward function induces negative interference
between objectives, especially when representations are learned using neural networks and shared
among different objectives, which goes against human’s actual intentions (Pham et al., 2018; Schaul
et al., 2019). Further, given clinicians’ treatment strategies, the intrinsic reward function cannot
be inferred accurately with existing inverse reinforcement learning (IRL) methods (Abbeel & Ng,
1
Under review as a conference paper at ICLR 2021
2004; Ho & Ermon, 2016), since they require access to samples from an optimal policy, which is not
guaranteed in reality (Komorowski et al., 2018; Saria, 2018).
Fortunately, qualitative feedback according to humans’ preferences can be easily obtained and
efficiently leveraged to infer reward functions. In this paper, we investigate preference-based Re-
inforcement Learning approaches (Furnkranz et al., 2012; Cheng et al., 2011; Akrour et al., 2012;
Schafer & Hullermeier, 2018; Christiano et al., 2017) for treatment recommendation, where the
reward is estimated based on preferences over a pair of treatment strategies. Specifically, the reward
estimator ensures that in a policy pair, the policy preferred according to a human’s objectives has
a higher expected return. However, acceptance of PRL approaches for treatment recommendation
requires significant exploration of their practical utility, reliability and interpretability.
Contributions: In this work, we first present an open simulation platform to investigate the
preference-based Reinforcement Learning approaches from the above aspects. The constructed
simulator models the dynamic state transitions of different individuals with Cancer or Sepsis and their
reactions to the received medication or operation treatment, which enables efficient model training
and reliable performance evaluation. Next, we conduct comprehensive simulated experiments to
address the following questions: 1) Does the preference-based qualitative feedback really benefit
the policy learning compared to handcrafted rewards and other existing treatment recommendation
approaches? 2) How to better optimize human’s objectives by learning a reward representation
which can deal with policies which are incomparable? 3) Is the reward function inferred by PRL
interpretable and does it faithfully follow human intentions? and 4) How to design agent types so
that resulting policies together with the preference feedback lead to more accurate reward estimation
and better treatment outcomes? Our experiments provide useful insights and guidance in developing
preference-based RL approaches to realize the right trade-off between human objectives during
treatment recommendation.
2	Problem Definition
We cast the treatment policy learning as a Markov Decision Process (MDP). At time-step t, st is a
vector composed of multiple health-related features, at is either a scalar value representing dosage
amount or a boolean value denoting whether to perform an operation. Besides effects from the
conducted actions, features in the state influence each other’s progression, which is simulated by
the state transition probability function P(st+1 |st, at). The agent is targeted at learning the optimal
policy π* that maximizes the expected return V∏* (so) = max∏ E[P∞=o γtrt], where Y ∈ [0,1] is the
discount factor and rt is the estimated reward based on preference feedback. Given two policies πm
and πn starting with the same initial state si, πm(si) πn(si) represents the preference of policy πm
to πn based on human’s objectives. Rather than using hand-crafted reward functions of the MDP, we
are aimed at finding a parameterized reward function rθP that approximates the true reward function
r underlying human’s preference.
3	Simulation Platform Design
3.1	General Cancer and Drug treatment Simulation
Following prior work (Furnkranz et al., 2012), we use the mathematical model proposed by Zhao
et al. (2009) to simulate the general cancer evolution and drug treatment effects.
State Transformation: The values of the next tumor size yt+1 and the toxicity level xt+1 are
determined by the current drug amount dt, their current values yt, xt and initial values y0, x0:
yt+1 = ReLU(yt + [aι ∙ max(xt,xo) — bi ∙ (dt — mi)] X I(yt > 0))
xt+i = ReLU(Xt + a? ∙ max(yt,yo) + b2 ∙ (dt — m2)),
where I is the indicator function which outputs 1 if the current tumor size yt > 0 and 0 otherwise.
3.2	Sepsis Infection and Blood Purification Simulation
We employ the mathematical model derived by Song et al. (2012) to simulate the acute inflammation
process in response to an infection. There are 19 physiological features that govern sepsis dynamics,
2
Under review as a conference paper at ICLR 2021
8 of which are observable while the remaining 11 are unmeasurable conceptual variables. Whenever a
blood purification operation is made, three components in the circulation are eliminated, i.e., activated
neutrophils Na and the pro- and anti-inflammatory mediators PI and AI. Besides effects from the
blood purification operation, the variables influence each others’ progression through Ordinary
differential equations (ODEs).
State Transformation: There are 18 ODEs to describe feature interactions and 3 ODEs for hypo-
thetic mechanism of blood purification. For a simple demonstration, we only list the equations of
activated neutrophils (Na) with(out) blood purification operation here.
dNa NrPIn 1	NpPIn 1 Na
NO operation: 丁 = hN Y pin T— + hNp-Na+PI" G - /
NaPIn	1
hNa-Ns + PInTNa-Ns
With operation:
dNaHA	dNa	Na /N∞
-----=---------------:--：---
dt	dt	hAIHA + (Na∕N∞)
where Nr , Np , Na are resting, primed and activated blood neutrophils respectively, PI is
the systemic pro-inflammatory response, TNr-Na , TNp-Na , TNa-Ns are constant parameters and
hnNr-Na,hnNp-Na,hnNa-Ns,hAIHA are hill equations.
4 Method
4.1	Existing Treatment Recommendation Approaches
Learning from Hand-crafted Reward: When the optimization objective is to maximize the clinical
efficacy alone, the reward function for intermediate timesteps is either 0 or hand-crafted based on
indicators of patient health, while the rewards for positive and negative outcomes at terminal timesteps
are normally of the same scale, but of opposite directions (Raghu et al., 2017; Wang et al., 2018;
Nemati et al., 2016). Besides obtaining the optimal clinical efficacy, another line of work has also
included auxiliary objectives like mitigating negative impacts or improving health conditions. Most
of existing work specified linear scalarization functions based on domain knowledge to project the
multi-objective MDP to a single-objective MDP (Denton et al., 2009; Lopez-Martinez et al., 2019;
Zhao et al., 2009). Due to the sensitivity of the learned policy and resulted performance to relative
values of the manually specified rewards, the employed reward functions in these approaches are
difficult to be quantified by experts to achieved distinct goals in treatment recommendation.
Learning from Human Feedback: Given demonstrations from domain experts, inverse Reinforce-
ment Learning (IRL) methods have been proposed to seek the reward function that models the
intention of the demonstrator first and then train RL agents to match the demonstrations (Abbeel &
Ng, 2004; Ho & Ermon, 2016). Though explicit quantitative reward signals are no longer needed
in IRL settings, learning treatment policies from clinicians’ demonstrations is challenging, since
optimal demonstrations are difficult to provide by clinicians while the general treatment regimens
in demonstrations can hardly reflect the actual intentions (Gao et al., 2018; Brown et al., 2019).
Fortunately, even non-experts can provide feedback in the form of preference, which has been utilized
to replace conventional numerical reward signals with relative utility values (Furnkranz et al., 2012;
Cheng et al., 2011; Akrour et al., 2012; Schafer & Hullermeier, 2018; Christiano et al., 2017).
4.2	Prefence-based Reinforcement Learning Framework
We display the framework in Algorithm 1 to show the reward and policy learning procedure given
preference feedback. Firstly, we are aimed at learning a reward function, based on which the
preference between two policies could be approximated.
Learning from Qualitative Feedbacks: We denote by πm (si) πn (si) the case that given si,
πm is preferred to πn . We here treat the qualitative feedback learning problem as a classic binary
classification task, where two policies are given and a model learns to approximate the preference
between the two. We assume the probability that one policy is preferred to the other is a function of
their received reward estimations (explained later in Preference Probability Representation), then the
3
Under review as a conference paper at ICLR 2021
Algorithm 1 PREFERENCE-BASED RL FOR TREATMENT RECOMMENDATION
Require:
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
S0 : initial states of sampled subjects
N : number of training iterations
T : the maximum simulation time to treat each subject
Randomly initialize θP , θA1 , θA2
D = 0, Γ1 = 0, Γ2 = 0 // Initialize empty lists to store samples for reward and agent learning
for n = 0 to N - 1 do
for all s ∈ S0 do
si J s,s0 J s, T1 - 0,τ2 - 0 // Initialize state vector and trajectory list
for t = 0 to T - 1 do
at J π(S1; θA),s1+ι J SIMULATE(S1,a1),r1P,t J REWARD(S1,a1； θp)
a2J π(S2;θA),s2+tJ SIMULATE(S2,a2),r2p,tJ REWARD(S2, a2； θp)
τ1 J τ1 ∪ {(St1,at1,rθ1P,t), St1+1},τ2 J τ2 ∪ {(St2,at2,rθ2P,t, St2+1)}
end for
Γ1 J Γ1 ∪ {τ1},Γ2 J Γ2 ∪ {τ2}
pre(τ 1, τ2) JEVALUATEPREFERENCE(τ1, τ2) // Preference feedback from humans
D J D ∪ (τ 1,τ2,pre(τ 1,τ2))
end for
Drawing minibatches Γ3 〜Γ1, Γn 〜Γ2, Dn 〜D
Update reward network θP with Dn by PREFERENCE-BASED REWARD LEARNING// See
Algorithm 2 (Appendix)
Update agent network θA1 with Γ1n , θA2 with Γ2n by PREFERENCE-GUIDED AGENT LEARN-
ING// See Algorithm 3 (Appendix)
18: end for
cross-entropy loss function is:
L(θp) = -Esi 〜S [l(∏m(Si) » ∏n(Si)) log p(∏m(Si ) » ∏n(Si) θp)	(1)
+ I πn(Si)	πm(Si) logp πn(Si)	πm(Si); θP ,
where I(∙ * ∙) is an indicator function equal to 1 if the first policy is preferred to the second, 0
otherwise.
Preference Probability Representation: Bradley-Terry model (Bradley & Terry, 1952) is a widely
used probability model to predict the preference of a paired comparison: p(i A j) = pɪ-, where Pi
is a positive real-valued score assigned to individual i. In order to compute the probability that πm is
preferred to πn given state Si, we employ its implementation introduced in (Agresti & Kateri, 2011) :
p(nm(Si) A ∏n (Si)) = expR(nm,Si； θp)	(2)
exp R(∏m, Si； θp) + exp R(∏n, Si； θp ),
where capital R denotes the expected return of conducting a policy given one specific initial state.
Given the learned reward, parameters of the RL agent are updated with either of the following two
methods.
Action-based Reward Modification (AbRM): Hand-crafted rewards assigned to different outcomes
influence the agent performance a lot, even if we keep the ratio but change the magnitude only. Instead
of designing the scalar rewards for different outcomes manually, we send the preference-based reward
rθP (St , at ) to the agent at each time-step.
State-based Reward Modification (SbRM): We derivea new state value hθP from rθP to represent
how good the state is: hθP (St) = maxa rθP (St, a). We further compute the advantage value of
the current state over the previous one, hθP (St) - hθP (St-1), as the instant reward to encourage
appropriate behaviors in accordance with preference.
Problems to be Resolved: Before applying preference-based RL approaches to treatment recom-
mendation, we need to address the following problems to ensure that the preference-based reward
estimation is consistent with human’s objectives and the well-trained agent provides reliable and
interpretable policies to clinicians:
4
Under review as a conference paper at ICLR 2021
Table 1: Performance for Cancer medication recommendations. The best result per metric is marked
in boldface. We present avg ± stdev values for all experiments averaged over 10 independent runs.
Method Type	Method Name	Clinical Efficacy	Other Factors
		Survival Rate	Tumor + Toxicity
	Constant Best (0.4)	19.91%±0.58%	2.22±0.04
Non-learning	Constant Worst (0.1)	4.89%±0.68%	3.72±0.03
	Random	17.81%±0.91%	2.23±0.04
Preference Learning	PBPI	21.79%±0.64%	2.21±0.07
Reinforcement Learning (handcrafted reward)	Single-objective RL	26.96%±3.02%	1.16±0.48
	Single-objective RL (Ensemble)	27.38%±3.32%	1.14±0.49
	Existing Multi-objective RL	18.84%±5.77%	2.28±0.66
	Grid-search Multi-objective RL	28.98%±3.42%	0.66±0.45
Reinforcement Learning (Preference-based reward)	AbRM (CE)	31.52%±1.38%	0.46±0.06
	AbRM (CE & OF-I)	31.33%±1.18%	0.39±0.02
	SbRM (CE) SbRM (CE&OF-D	30.54%±3.46% 31.72%±1.08%	0.68±0.45 0.43±0.06
•	Does the preference-based qualitative feedback really benefit the policy learning compared to
handcrafted rewards and other existing treatment recommendation approaches?
•	How to better optimize human’s objectives by learning a reward representation which can deal
with policies which are incomparable?
•	Is the reward function inferred by PRL interpretable and does it faithfully follow human intentions?
•	How to design agent types so that resulting policies together with the preference feedback lead to
more accurate reward estimation and better treatment outcomes?
5	Experiments
5.1	Settings
Medication Recommendation for General Cancer: For 6-month simulation, the agent makes
dosage amount decisions in each month. 10, 000 subjects are randomly sampled for training, 2, 000
for validation and 2, 000 for testing. We are aimed at learning optimal policies with three kinds of
intentions: 1) maximizing survival rate to obtain optimal clinical efficacy (CE); 2) and mitigating
negative effects represented by the sum of the tumor size and the toxicity level after treatment
(CE&OF-I); 3) and mitigating negative effects represented by two separate health signs, the highest
toxicity level during the treatment and the final tumor size (CE&OF-II).
Blood Purification Recommendation for Sepsis: During the 100-hour simulation, the agent is
asked whether to perform a 2-hour operation in every 2 hours. We randomly sample 3, 000 subjects
for training, 1, 000 for validation and testing. This is a partially observable MDP and LSTMs are
utilized for agent modeling. The agent learns policies to fulfill two intentions: 1) maximizing survival
rate to obtain optimal clinical efficacy (CE); 2) and avoiding too frequent operations (CE&OF).
5.2	Compared Approaches
We benchmark results from the following existing approaches from treatment recommendation
literature:
•	Non-learning (Zhao et al., 2009; Furnkranz et al., 2012): 1) Constant: A static dosage amount is
given to all the subjects throughout the six months; 2) Random: One of the four dosage options is
randomly selected at each time-step; 3) Upper Bound: The subjects with Sepsis receive operations all
the time throughout the simulation period.
•	Preference Learning (Furnkranz et al., 2012): in Preference-Based Policy Iteration (PBPI), one
action is preferred to the other based on their outcomes after certain times of simulations. Every time
the dosage with the highest preference is selected.
•	Reinforcement Learning with handcrafted Reward: 1) Single-objective RL (Schulman et al., 2015):
the conventional policy gradient approach; it receives +1 for survival outcome, -1 for death, and 0
for all intermediate steps. 2) Single-objective RL (Ensemble): among two agents, the one with better
performance on the validation set is evaluated on the testing set. It is developed for fair comparison
5
Under review as a conference paper at ICLR 2021
Figure 1: Performance for Sepsis blood purification recommendation.
(a) Optimize clinical efficacy only
(b) Optimize clinical efficacy & mitigate negative impacts
Number of Operations
as two agents with different parameter initializations are used in the PRL framework. 3) Existing
Multi-objective RL (Zhao et al., 2009): manually defined rewards based on key factors are assigned to
each time-step. 4) Grid-search Multi-objective RL (Kim & De Weck, 2006): both the clinical efficacy
and the negative impacts from other factors are treated as objectives and the linear scalarization with
the best weight retrieved from grid search is employed.
• Reinforcement Learning with Preference-based Reward: to guide the RL agent learning, both
AbRM and SbRM are trained based on preference determined by human’s intentions. Since the
preference-based reward is a non-stationary value approximated by a neural network, we implement
agents with the policy gradient, which is robust to changes in the reward function (Ho & Ermon,
2016; Christiano et al., 2017).
5.3	Benchmark Results
Medication Recommendation for General Cancer: We evaluate different approaches to pursue
treatment goals in terms of maximizing survival rate CE, mitigating negative impact CE&OF-I (sum
of tumor size and toxicity level) in Table 1 and CE&OF-II (highest toxicity level and final tumor
size) in Table 2 (Appendix). Considering the Survival Rate as the only metric to derive preference on
two policies, agents learning from either action-based (31.52%) or state-based (30.54%) preference
reward have much better performance in saving lives than Single-objective RL (26.96%), where
the handcrafted reward is used to penalize policies with death outcomes. When negative impacts
are expected to be mitigated besides saving lives, agents receiving rewards from preference are
capable to maintain the performance on the clinical efficacy with much fewer negative impacts at
the same time. When human’s preference is defined as CE&OF-II, the highest toxicity level during
the treatment and the final tumor size are two contradictory objectives to minimize. In Table 2, we
observe that preference-based reward guides the agent to policies with the highest survival rates,
while one negative impact gets reduced but the other increases compared with other approaches.
Blood Purification Recommendation for Sepsis: Performance bar charts of different approaches
evaluated by Survival Rate and Number of Operations are illustrated in Fig. 1. When guided by
preference-based reward rather than manually crafted reward, a slightly higher Survival Rate is
achieved by both AbRM and SbRM, while the average number of operations has fallen considerably,
by 6.79% with AbRM and 14.50% with SbRM. Note that although the approach Multi-objective RL
leads to the fewest number of operations, the performance in Survival Rate has dropped to make
undesired trade-offs between clinical efficacy optimization and negative impacts mitigation.
5.4	Examination of Preference-based RL Approaches
Advantages of Preference-based Reward over handcrafted Reward: In Fig. 5 (Appendix), we
first show the sensitivity of policy behaviors to small changes in handcrafted rewards leads to unstable
clinical efficacy even when the relative importance of obtaining positive outcomes against negative
ones keeps unchanged. The three heatmaps of the agent’s performance in response to different reward
scalars reflect the difficulty in specifying an appropriate reward function to enable policy learning
with the optimal clinical efficacy during treatment recommendation. In Fig. 6 (Appendix), we further
show the difficulty of selecting reward scalars for three factors - survival rate, last tumor size and
maximum toxicity level - in the grid-search Multi-objective RL approach to appropriately prioritize
the clinical efficacy over negative impacts. To study whether the preference-based reward estimation
can fully capture the human’s intentions, we provide the RL agents with linear combinations of
6
Under review as a conference paper at ICLR 2021
(a) SbRM Training
Figure 2: Sepsis treatment strategies recommended by the SbRM agent: (a) clinical efficacy and
expected return during training, (b) reward transferability among different configurations.
(b) SbRM Transferability
preference-based reward and handcrafted reward. Results in Table 3 (Appendix) show that learning
from preference-based reward alone is adequate to achieve high clinical efficacy, while its combination
with handcrafted rewards distracts the RL agent from optimizing the human’s actual intentions and
finally leads to inferior performance. To study the generalizability of the reward estimator, we extract
the well-trained reward model in 2-hour operation configuration for Sepsis treatment and use it as the
pre-trained model for 4-hour operation experiments. In Fig. 2b and Fig. 9b (Appendix), the reward
estimator with knowledge transfer helps the agent speed up learning: compared with learning from
scratch, the reward estimator with good initialization from a different configuration can provide better
guidance to the agent.
Figure 3:	Cancer treatment recommendation for 10,000 training subjects with the PRL framework.
Curves in green describe scenarios when incomparable policies are discarded while curves in blue
show cases when comparable policies are preserved and efficiently utilized.
(b) Number of Samples for Reward Estimator Update
10000	------------------------------------------------
8000
6000
4000
0	50 100 150 200 250 300 350 400
Epoch
Disregard
Preserve
Addressing Incomparable policies: Given two policies for one sampled subject, if they have
identical performance according to human’s objectives, then the two policies are deemed to be
incomparable. Since no clear preference conclusion can be drawn between the two incomparable
policies, the majority of existing work in preference learning disregarded them directly (Furnkranz
et al., 2012; Cheng et al., 2011; Akrour et al., 2012; Schafer & Hullermeier, 2018; Christiano
et al., 2017). Only comparable pairs, either πm preferred to πn (I πm(si) πn(si) = 1) or πn
preferred to πm (I πn(si) πm(si) = 1) , are included in the training set to optimize preference
approximation. However, preference learning based on comparable policies alone achieves quite
unsatisfactory clinical efficacy in our treatment recommendation tasks. As shown in Fig. 3c for
Cancer treatment recommendation, the survival rate (green curve) progresses with little improvement
but great fluctuation during 400 epochs of training. Two reasons are likely to contribute to the failure:
1) polarized preference (one preferred with probability 0.85, and the other 0.15 in Fig. 3a) is inferred
between two incomparable policies although the preference label is never provided in the training
set; 2) only around one-fifth of the policy pairs (2,000 comparable from 10,000 sampled subjects)
are leveraged in each epoch for preference model update (green line in Fig. 3b). After the above
performance analysis, we find that excluding incomparable pairs from the training set leaves the
parameterized model exploring the preference space arbitrarily and inferring random preference
over two policies although they are incomparable. To avoid arbitrary exploration in the preference
space, we handle the incomparable pairs with a simple approach: treating both policies from the
incomparable pair equally, i.e., I πm(si) πn(si) = I πn(si) πm(si) = 0.5. With the small
but important augmentation to the preference indicator function, incomparable policies are efficiently
7
Under review as a conference paper at ICLR 2021
Figure 4:	Cancer treatment recommendation: (a) clinical efficacy and expected return during training,
and (b, c) expected return of policies ending with different negative impacts during testing.
----Training Expected
----Validation Expected
----Training Survival
Validation Survival
I 200 250 300 350 400
Epochs
EnwH PaZPadXW
Survive
Die
0	1	2	3	4	5
Negative Impacts (LastTumor + Toxicity)
(a) SbRM Training	(b) PG on Negative Impacts (c) SbRM on Negative Impacts
utilized for better preference space exploration (preference approaching 0.5 as expected in Fig. 3a),
more samples for preference model update (all the 10,000 samples from the training set participate in
the loss function minimization in Fig. 3b), and much higher clinical efficacy (more than 30% survival
rate achieved after the model converges in Fig. 3c).
Interpretability in Inferred Rewards: To demonstrate whether the preference-based reward match
human’s actual intentions, we visualize the SbRM and AbRM agent’s expected return for Cancer
treatment during training and its relationship with the resulted negative impacts during testing (OF-I)
in Fig. 4 and Fig. 7 (Appendix), respectively. From Fig. 4a, we can observe that the rising trend of
the expected return matches the improving Survival Rate quite well, although the parameters of the
reward estimator are updated at the same time. The estimated reward offers reasonable explanations
for the policy performance: the higher the expected return, the better the policy. After the model
converges, we analyze the distribution of expected returns for policies with different negative impacts.
Since penalties or rewards are assigned to policies based on their outcomes only, the conventional
Policy Gradient approach treats policies ending with survivals but different negative impacts equally
(horizontal blue dots in Fig. 4b). After adopting preference-based reward, policies resulting in
survival outcomes can distinguish from each other: policies with smaller negative impacts have
much higher expected return. In Fig. 4c, policies leading to deaths have extremely low expected
return (approaching zero), while the expected return for policies with survival outcomes is negatively
proportional to the amount of negative impacts. As shown in Fig. 2a and Fig. 9a (Appendix), the
expected return received by the agent for Sepsis treatment also shares the common trend with the
Survival Rate: if more lives have been saved by the agent, then higher expected return is achieved.
Influence of Agent Types in Treatment Outcomes: As depicted in Algorithm 1, the studied
preference-based RL framework adopts two RL agents controlled by different parameters to infer
the reward and learn the policy that optimizes human’s intentions. We here study the influence of
different agent designs on reward approximation and resulted performance. Specifically, the reward
function is estimated to approximate the preference over policies, among which the first policy is
performed by one RL agent while the second policy can be executed by different agent types. The
clinical efficacy curves shown in Fig. 8 (Appendix) empirically prove the effectiveness of the current
design of two different preference-based RL agents.
6 Conclusions and Future Directions
To obtain optimal treatment policies based on human’s diverse objectives, we investigate performance
of the preference-based Reinforcement Learning approaches, where higher rewards are automati-
cally estimated and assigned to actions following human’s actual intentions underlying the provided
preference feedback. During interacting with the developed simulation platform, we resolve criti-
cal implementation problems and gain a deeper understanding in designing preference-based RL
approaches, in order to better aid clinicians in treatment decision making. In future work, we will
consider tackling some more practical aspects about human’s preferences in adopting treatment
strategies: 1) how to efficiently leverage preference in reward learning if human’s feedback is limited,
2) how to fully reflect human’s actual intentions in reward learning if both preference feedback and
clinicians’ demonstrations are provided.
8
Under review as a conference paper at ICLR 2021
References
Pieter Abbeel and Andrew Y Ng. Apprenticeship learning via inverse reinforcement learning. In
Proceedings of the twenty-first international conference on Machine learning, pp. 1, 2004.
Alan Agresti and Maria Kateri. Categorical data analysis. Springer, 2011.
Riad Akrour, Marc Schoenauer, and Michele Sebag. April: Active preference learning-based
reinforcement learning. In Joint European Conference on Machine Learning and Knowledge
Discovery in Databases, pp. 116-131. Springer, 2012.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method
of paired comparisons. Biometrika, 39(3/4):324-345, 1952.
Daniel S Brown, Wonjoon Goo, Prabhat Nagarajan, and Scott Niekum. Extrapolating beyond
suboptimal demonstrations via inverse reinforcement learning from observations. arXiv preprint
arXiv:1904.06387, 2019.
Weiwei Cheng, Johannes Furnkranz, Eyke Hullermeier, and Sang-Hyeun Park. Preference-based
policy iteration: Leveraging preference learning for reinforcement learning. In Joint European
Conference on Machine Learning and Knowledge Discovery in Databases, pp. 312-327. Springer,
2011.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcement learning from human preferences. In Advances in Neural Information Processing
Systems, pp. 4299-4307, 2017.
Brian T Denton, Murat Kurt, Nilay D Shah, Sandra C Bryant, and Steven A Smith. Optimizing the
start time of statin therapy for patients with diabetes. Medical Decision Making, 29(3):351-367,
2009.
Daniel M Faissol, Paul M Griffin, and Julie L Swann. Timing of testing and treatment of hepatitis c
and other diseases. In Proceedings, pp. 11, 2007.
Johannes Furnkranz, Eyke Hullermeier, Weiwei Cheng, and Sang-Hyeun Park. Preference-based
reinforcement learning: a formal framework and a policy iteration algorithm. Machine learning,
89(1-2):123-156, 2012.
Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement learning
from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, pp. 4565-4573, 2016.
Il Yong Kim and OL De Weck. Adaptive weighted sum method for multiobjective optimization:
a new method for pareto front generation. Structural and multidisciplinary optimization, 31(2):
105-116, 2006.
Matthieu Komorowski, Leo A Celi, Omar Badawi, Anthony C Gordon, and A Aldo Faisal. The
artificial intelligence clinician learns optimal treatment strategies for sepsis in intensive care.
Nature Medicine, 24(11):1716, 2018.
Daniel Lopez-Martinez, Patrick Eschenfeldt, Sassan Ostvar, Myles Ingram, Chin Hur, and Rosalind
Picard. Deep reinforcement learning for optimal critical care pain management with morphine
using dueling double-deep q networks. arXiv preprint arXiv:1904.11115, 2019.
Shamim Nemati, Mohammad M Ghassemi, and Gari D Clifford. Optimal medication dosing from
suboptimal clinical examples: A deep reinforcement learning approach. In 2016 38th Annual
International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), pp.
2978-2981. IEEE, 2016.
Tu-Hoa Pham, Giovanni De Magistris, and Ryuki Tachibana. Optlayer-practical constrained opti-
mization for deep reinforcement learning in the real world. In 2018 IEEE International Conference
on Robotics and Automation (ICRA), pp. 6236-6243. IEEE, 2018.
9
Under review as a conference paper at ICLR 2021
Aniruddh Raghu, Matthieu Komorowski, Imran Ahmed, Leo Celi, Peter Szolovits, and Marzyeh
Ghassemi. Deep reinforcement learning for sepsis treatment. arXiv preprint arXiv:1711.09602,
2017.
Thomas Rimmele and John A Kellum. Clinical review: blood purification for sepsis. Critical Care,
15(1):205, 2011.
Claudio Ronco, Rinaldo Bellomo, Peter Homel, Alessandra Brendolan, Maurizio Dan, Pasquale
Piccinni, and Gluseppe La Greca. Effects of different doses in continuous veno-venous haemofil-
tration on outcomes of acute renal failure: a prospective randomised trial. The Lancet, 356(9223):
26-30, 2000.
Suchi Saria. Individualized sepsis treatment using reinforcement learning. Nature medicine, 24(11):
1641-1642, 2018.
Dirk Schafer and Eyke Hullermeier. Preference-based reinforcement learning using dyad ranking. In
International Conference on Discovery Science, pp. 161-175. Springer, 2018.
Tom Schaul, Diana Borsa, Joseph Modayil, and Razvan Pascanu. Ray interference: a source of
plateaus in deep reinforcement learning. arXiv preprint arXiv:1904.11455, 2019.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438,
2015.
Sang OK Song, Justin Hogg, Zhi-Yong Peng, Robert Parker, John A Kellum, and Gilles Clermont.
Ensemble models of neutrophil trafficking in severe sepsis. PLoS computational biology, 8(3),
2012.
Ivan Stojkovic, Mohamed , Xi Hang Cao, and Zoran Obradovic. Effectiveness of multiple blood-
cleansing interventions in sepsis, characterized in rats. Scientific reports, 6:24719, 2016.
Lu Wang, Wei Zhang, Xiaofeng He, and Hongyuan Zha. Supervised reinforcement learning with
recurrent neural network for dynamic treatment recommendation. In Proceedings of the 24th
ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 2447-2456.
ACM, 2018.
Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes Furnkranz. A survey of preference-
based reinforcement learning methods. The Journal of Machine Learning Research, 18(1):4945-
4990, 2017.
Yufan Zhao, Michael R Kosorok, and Donglin Zeng. Reinforcement learning design for cancer
clinical trials. Statistics in medicine, 28(26):3294-3315, 2009.
10
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Figures and Tab les
Figure 5: Performance of RL in different hand-crafted reward designs to optimize clinical efficacy.
(a) Random Seed 2001
PJeMaB We ① Cl
-30
-2B
-26
-24
-22
1 10 20 30 40 50 60 70 BO 90 100
Survival Reward
(b) Random Seed 2002
I，01，Omr OT O9,0rr08, 06, 00τ,
p」eM①B We①Cl
(c) Random Seed 2003
01，O * OTO▼，OT 09, OZi 08,06, 0≡,
p」eM①B We①Cl
1 10 20 30 40 50 60 70 BO 90 100
Survival Reward
Figure 6: Performance of Multi-objective RL with linear scalarization on three reward factors:
survival rate, last tumor size, and maximum toxicity levels. The linear weight assigned to each factor
is one of four values: {1, 2, 4, 8}.
-→- Grid-search MOMDP -→- Preference: AbRM (CE&OF-II). SbRM (CE&OF-II)
0	10	20	30
Different ratios among three reward factors
0	10	20	30
Different ratios among three reward factors
0	10	20	30
Different ratios among three reward factors
(a) AbRM Training
Figure 7: Cancer treatment strategies recommended by agent AbRM: (a) clinical efficacy and expected
return during training, and (b) expected return of policies ending with different negative impacts
during testing.
(b) AbRM on Negative Impacts
11
Under review as a conference paper at ICLR 2021
Figure 8:	Effects of different agent designs on performance.
(a)	AbRM
(b)	CE&OF-I - ASent 0 - Agent 1 ce&OF-II
(a) CE
£EK-ENAJnS
Pref Pref Pref Pref Pref
& Pref & Pref & Env & Rand & Const
(Twin) (Single)
Different agent combinations
Pref Pref Pref Pref Pref
& Pref & Pref & Env & Rand & Const
(Twin) (Single)
Different agent combinations
Pref Pref Pref Pref Pref
& Pref & Pref & Env & Rand & Const
(Twin) (Single)
Different agent combinations
(b) SbRM
(b) CE&OF-I - Agent 0 - Agent 1、坨 ce&OF-II
(a) CE
£EK-E>-ΛJns
£EK-E>-ΛJns
£EK-E>-ΛJns
Pref Pref Pref Pref Pref
& Pref & Pref & Env & Rand & Const
(Twin) (Single)
Different agent combinations
Pref Pref Pref Pref Pref
& Pref & Pref & Env & Rand & Const
(Twin) (Single)
Different agent combinations
30
20
Pref Pref Pref Pref Pref
& Pref & Pref & Env & Rand & Const
(Twin) (Single)
Different agent combinations
Figure 9:	Sepsis treatment strategies recommended by the AbRM agent: (a) clinical efficacy and
expected return during training, (b) reward transferability among different configurations.
(a) AbRM Training
(b) AbRM Transferability
u-IΓUtuα ptu3utudx 山
0	10	20	30	40	50
Epochs
30-
40
0	2	4	6	8	10	12
Epoch
12
Under review as a conference paper at ICLR 2021
Figure 10: For Cancer experiments, true expected return of DQN learning from behavioral policies of
Policy Gradient and its estimations from different off-policy evaluation methods.
luBoα pətj① dx 山
(-0.77)
(-0.00)
(-0.11)
(0.81)
(0.81)
Estimators
Table 2: Performance for Cancer medication recommendation considering negative impacts from two
factors: the tumor size in the end and the ever experienced maximum toxicity.
Method Type	Method Name	Clinical Efficacy	Other Factors	
		SUrvival Rate	Last Tumor	Max Toxicity
	Constant Best (0.4)	19.91%±0.58%	1.76±0.02	0.57±0.01
Non-learning	Constant Worst (0.1)	4.89%±0.68%	3.72±0.03	0.48±0.04
	Random	17.81%±0.91%	0.82±0.02	1.64±0.04
Preference Learning	PBPI	20.80%±0.56%	1.38±0.12	1.01±0.12
Reinforcement Learning (Hand-crafted reward)	Single-objective RL	26.96%±3.02%	0.48±0.28	1.37±0.28
	Single-objective RL (Ensemble)	27.38%±3.32%	0.47±0.29	1.38±0.29
	Existing Multi-objective RL	18.84%±5.77%	0.25±0.10	2.16±0.60
	Grid-Search MUlti-objective RL	25.10%±1.44%	0.63±0.20	1.21±0.16
Reinforcement Learning (Preference-based reward)	AbRM (CE)	31∙52%±1.38%	0.11±0.10	1.74± 0.06
	AbRM (CE&OF-II)	30.34%±2.71%	0.23±0.22	1.63±0.19
	SbRM(CE) SbRM(CE&。尸-〃)	30.54%±3.46% 30.76%±2.64%	0.18±0.25 0.08±0.09	1.67±0.22 1.75±0.09
13
Under review as a conference paper at ICLR 2021
Table 3: Evaluating the clinical efficacy (survival rate) achieved by the proposed preference-based
RL framework when hand-crafted and preference-based rewards are linear combined with different
ratios.
Ratios	AbRM (CE)	AbRM (CE&OF-I)	AbRM (CE&OF-II)	SbRM (CE)	SbRM (CE&OF-I)	SbRM (CE&OF-II)
8:1	28.36%±3.09%	28.07%±3.50%	28.23%±3.82%	29.10%±3.19%	26.51%±2.47%	28.94%±3.17%
4:1	28.92%±2.82%	27.41%±3.66%	27.9%3±3.61%	27.77%±3.16%	28.45%±3.18%	27.54%±3.09%
2:1	26.30%±3.19%	27.50%±3.64%	27.71%±3.47%	27.55%±3.40%	27.81%±3.15%	28.43%±3.12%
1:1	27.42%±3.24%	27.70%±3.73%	27.62%±2.88%	27.99%±2.9%	28.49%±3.23%	28.71%±3.50%
1:2	26.96%±2.95%	27.55%±2.90%	28.66%±3.19%	28.59%±2.94%	28.79%±3.21%	28.30%±3.29%
1:4	27.46%±3.53%	26.59%±3.40%	28.25%±3.47%	29.91%±1.93%	28.75%±3.06%	27.08%±3.64%
1:8	27.15%±3.00%	29.21%±2.42%	27.67%±3.38%~	29.18%±2.45%	28.55%±2.85%	28.48%±3.28%
0:1 (Ours)	31.52%±1.38%	31.33%±1.18%	30.34%±2.71%	30.54%±3.46%	31.72%±1.08%	30.76%±2.64%
14
Under review as a conference paper at ICLR 2021
A.2 Preference-based Reinforcement Learning Algorithms
The preference-based Reinforcement Learning framework is composed of two main modules,
Preference-based Reward Learning and Preference-guided Agent Learning. In Preference-based
Reward Learning, the reward estimator parameterized by θP delivers step-wise rewards to the two
agents parameterized by θA1 and θA2 based on their policy preference. In Preference-guided Agent
Learning, the agents update their parameters so as to optimize the clinicians’ objectives. The pair of
policies performed by the two agents on the sampled subject is stored in the policy pool and leveraged
for parameter update in reward estimator, with the aim to ensure higher expected return for the
preferred policy. We list the pseudo codes for collaborative learning in Algorithm 1, Preference-based
Reward Learning in Algorithm 2, and Preference-guided Agent Learning in Algorithm 3, respectively.
Collaborative Learning Algorithm 1 illustrates the collaborative learning process between the two
modules in order to estimate reward and learn policies in personalized treatment recommendation. In
the beginning, the model parameters are randomly initialized (line 1), and the policy pools for the
reward estimator and the two agents are created as empty sets (line 2). In each iteration, one subject
is sampled from the training set for agent learning (line 3 to 5). At each simulation step, the two
agents are asked to make decisions based on the current state and the reward estimator generates
corresbonding step-wise reward for each of them (line 6 to 10). The subject’s internal state keeps
on updating until the simulation time has reached or the subject dies intermediately according to
the underlying mathematical modeling. The policy pools of the two agents are augmented with
the trajectories on the newest sampled subject (line 9 and 11), while the policy pool for the reward
estimator is also updated (line 13) after computing the ground-truth preference label (line 12). After all
the samples have been utilized for policy generation, the reward estimator minimizes the classification
loss during policy preference inference with Algorithm 2 (line 16), while the RL agents optimize the
expected return with Algorithm 3 (line 17).
Algorithm 2 PREFERENCE-BASED REWARD LEARNING
Require:
Dn : sampled policy pairs in n-th iteration
θP : parameters to update in reward function
YP : discounted factor on reward
β : step size for parameter update
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
L — 0
for all (τ1, τ2, pre(τ 1, τ2)) ∈ Dn do
R(T 1; θp) J 0, R(T2; θp) J 0
for all (st1,at1,rθ1P,t,st1+1) ∈ τ1 do
R(T 1 ； θP) J R(T 1; θP) + YPrθP,t
end for
for all (st2,at2,rθ2P,t,st2+1) ∈ T2 do
R(T2； θP) J R(T2; θP) + YPr2p,t
end for
Compute p(T1	T2 )
if T 1 T2 then
L J L + log p(T 1	T2)
else if T2	T 1 then
L J L + log(1 - p(t 1 * T2))
else if T 1 〜τ 2 then
L J L + 0.5logp(τ 1 * τ2) + 0.5log(1 — p(τ 1 * τ2))
end if
end for
Update θP J θP - β∆θPL
return θP
Preference-based Reward Learning Given pairs of policies with corresponding preferences, the
reward estimator updates its parameters to maximize the probability that the preferred policy achieves
15
Under review as a conference paper at ICLR 2021
higher expected return than the other. As shown in Algorithm 2, the discounted expected returns
achieved by each agent are firstly calculated respectively for each sampled policy pair (line 3 to 9).
Then the probability that policy τ 1 is preferred to τ 2 is positively correlated to the expected return
of τ1, and is computed as (Agresti & Kateri, 2011) introduced (line 10). Hence p(τ2 τ1) is
equal to 1 - p(τ1 τ2). Then the loss value is computed considering different kinds of preference
relationships between the two policies (line 11 to 17). Incomparable policy pairs are also leveraged
in reward learning for better preference space exploration (line 15 to 16).
Algorithm 3 PREFERENCE-GUIDED AGENT LEARNING
Require:
Γn : sampled policies from one agent in n-th iteration
α: step size for parameter update
M: one of the two reward assignment methods
LθA : loss function in any deep RL approach parameterized by agent parameters θA
1:	ε = 0
2:	for all (st, at, rθP,t, st+1) ∈ Γn do
3:	if M is Action-based Reward Modification then
4：	Yt — rθp (st,at)
5:	else if M is State-based Reward Modification then
6：	rt — hθp (St) - hθp (st-l)
7:	end if
8： ε J ε ∪{(st,at,rt,st+1)}
9: end for
10： UPdate θA J θA - a'θa P lΘa (x)
x∈ε
11： return θA
Preference-guided Agent Learning Each agent uPdates their Parameters individually as Algo-
rithm 3 dePicts. The agent receives rewards comPuted by either Action-based Reward Modification
(line 3 to 4) or State-based Reward Modification (line 5 to 6). Then we leverage (st, at, rt, st+1) to
uPdate the agent model imPlemented by any deeP Reinforcement Learning aPProach.
A.3 S imulation Platform Design
A.3.1 Medication Recommendation for General Cancer
Survival Analysis Within time interval (t - 1, t], where (1 ≤ t ≤ 6), the survival status is assumed
to dePend on both the current tumor size yt and the toxicity level xt. The Probability of a Patient’s
death is modeled as follows：
Hazard function： λ(t) = exp(-4 + yt + xt),
Survival function： ∆F (t) = exp(-∆∆(t)),
Cumulative hazard function：
∆∆(t)=Zt
t-1
λ(s)d(s),
Death Probability： pdeath = 1 - ∆F (t).
Implementation Details The action sPace is discrete and the dosage amount decisions are selected
among 4 options: 0.1, 0.4, 0.7, 1.0 (Furnkranz et al., 2012). For state initialization, the tumor size
and the toxicity level in the 0th month are generated indePendently from the uniform distribution
U(0, 2). The simulation terminates after t = 6th month or if the patient dies intermediately.
Model Implementation and Training For 6-month simulation, we randomly sample 10, 000
subjects for training, 2, 000 for validation, and 2, 000 for testing. The neural networks for all deep
learning approaches including preference learning and reinforcement learning share the similar
network structure and hyper-parameters： 2 fully-connected layers, the first followed by ReLU
activation and the second followed by different activation functions for different approaches. In one
epoch, the agent gets updated after seeing all the training samples. The learning rate is set to 0.01 and
all the networks converge after 400 epochs. For deep RL methods, we set the discount factor γ to 1.
16
Under review as a conference paper at ICLR 2021
A.3.2 Blood Purification Recommendation for Sepsis
Mathematical Modeling in Simulation Sepsis is initiated by spillover of pathogens into blood,
where the pathogen is allowed to spread throughout the organism in which systemic inflammation
takes place (Stojkovic et al., 2016). Motivated by the promising results of blood purification in
other critical illness conditions like acute kidney failure (Ronco et al., 2000), blood purification has
gained attention as a potentially effective solution for septic subjects (Rimmele & Kellum, 2011). In
blood purification treatment, the patient is connected to an extracorporeal hemoadsorption device that
removes harmful particles from the blood and leads the patient towards a healthy state.
We employ the mathematical model derived by Song et al. to simulate the acute inflammation process
in response to an infection (Song et al., 2012). Both heuristic knowledge about the mechanism
underlying infection and real measurements from experiments on CLP-induced septic rats were
leveraged for the model design. The distribution of initial physiological features and their interactions
are derived from domain knowledge. The initial physiological features that characterize a subject
accords with the probability distributions based on real experimental measurements for septic rats.
The parameters in transition functions are calibrated so that the generated trajectories closely follow
experimentally observed temporal patterns in septic rats.
Figure 12 demonstrates the feature interaction network. There are 19 physiological features that
govern sepsis dynamics, 8 of which are observable (features above the horizontal dashed line) while
the remaining 11 are conceptual variables (features below the horizontal dashed line). When a blood
purification operation is made, three components in the circulation are eliminated (features marked
by red dashed ring), i.e., activated neutrophils Na and the pro- and anti-inflammatory mediators PI
and AI. Besides effects from the blood purification operation, the variables influence each others’
progression through Ordinary differential equations (ODEs).
State Transition There are 18 ODEs to describe feature interactions and 3 ODEs for the hypothetic
mechanism of blood purification. The hypothetic mechanisms of action of the blood purification
are implemented by assuming the hemoadsorption device eliminates only three components in the
circulation: activated neutrophils (Na), pro-inflammatory mediators (P I), and anti-inflammatory
mediators (AI) during the treatment period. We here only show the transition equation of these three
key features with and without operation, ODEs concerning other features can be found in (Song et al.,
2012).
The variable PI stands for the extent of the systemic inflammation and progresses as follows:
dPI = (	B∕B∞	(i - D	)(i - AIn(1 - PI)
dt	\hpi_B + B∕B∞	hPi_D + Dn	hpi_Ai + AIn
(3)
+ 1-
B∕B∞	) Dn (1 - AIn(1 - PI)
hpi_B + B∕B∞ h hpI D + Dn( — hpI_AI + AIn
PI(t0 + 1)
+	B∕B∞______Dn
hpI B + B∕B∞ hpI D + Dn
1-
AIn(1 - PI)
hPI_AI + AIn
)- PI)τ⅛,
IPI (t0) + dPtI (t0)
"I(t0) + dPtI (t0) - hpiHPA+Pi
If no operation is performed
Otherwise
(4)
where B is the population of bacteria in the peritoneum, D is a coarse-grained representation of
integrated tissue damage, variables hPI_B , hPI_D, hPI_AI, τPI are subject-specific parameters, B∞
is a predefined upper bound of B, hPIHA = 0.3 and n = 3.
17
Under review as a conference paper at ICLR 2021
Table 4: Configurations for Sepsis treatment simulation. h represents hour in the simulation platform.
Step Size T	Horizon Length T	Operation Time Interval L	Decision-making Frequency f	Duration per Operation l
T=0.1h	T = 100h	5th to 18th h	f = 2 h or 4h	l = 2 h or 4h
The variable AI describes the level of the anti-inflammation corresponding to systemically acting
anti-inflammatory mediators and gets updated as follows:
dAI _ f	PIn1	a	Na∕N∞	) a	PIn1	)	Na∕N∞
~di~ = Uai pi + PInι	- hAI_Na + Na∕N∞+ +( - hAl PI + PIn h hAI_Na + Na∕N∞
(5)
+	PIn2	Na∕N∞	- AI) ɪ
hAI_pi + PIn2 hAI_Na + Na/N∞	T τAI，
E o 1.	I AI(t0) + dAI (t0)	If no operation is performed	“、
AI(t+1) = IAI(t0) + 智(t0) - hAIH⅛ Otherwise	,	⑹
where variables hAI_PI, hAI_Na ,τAI are subject-specific parameters, N∞ is a predefined upper
bound of neutrophils, hAIHA = 0.3, n1 = 1 and n2 = 3.
The variable Na represents the activated blood neutrophils and transits in each simulation step as
follows:
dNa
dt
NrPIn	1	NpPIn	1	Na
-----------∑--∑	+ τ----------∑--∑----------
hNr Na + PIn TNr_Na---------------------------------hNp Na+ PIrnTNp_N TNa
'---------{------------} '-----------{----------}
transmission from Nr to Na	transmission from Np to Na
NaPIr	1
hNa Ns+ PIn TNa_Ns ,
S-----------V------------}
transmission from Na to Ns
,	(Na (t0) + *(t0)
Na(t +1)= INa (t0) + dNa (t0) - hNaN+N∞/N∞ (t0)
If no operation is performed
Otherwise
(8)
where Nr is resting blood neutrophils, Np is blood neutrophils, Ns is neutrophils sequestered in
the lung capillaries, variables hNr _Na, hNp_Na, hNa_Ns, TNr_Na, TNp_Na, TNa_Ns are subject-specific
parameters, hNaHA = 0.3, and n = 3.
Survival Analysis The survival status of the subject only depends on the value of the systemic
pro-inflammatory response PI at the end of the simulation. When the PI value at the last time-step
is smaller than the pre-defined threshold 0.5, then the subject is assumed to be alive, otherwise
dead. Note that after the blood purification process, the PI value reduces as time passes, hence one
cannot conclude whether the subject is alive in the intermediate time-steps. After the pre-defined
simulation horizon is reached, we can confirm which subjects survive with the help of treatment. The
mathematical model is quite different from the general Cancer Treatment model where subjects have
a probability to die intermediately.
Implementation Details Due to phenotype differences, some subjects survive without any blood
purification operation while some die. This is consistent with laboratory experiments where 30%
of rats survived till seven days while the remaining died between two to five days after CLP (Zhao
et al., 2009). We call the survivor group Survival Population and the non-survivor group Death
Population. The survival status of the Survival Population gets no influence from blood purification
operations. Subjects from Death Population have the potentials to survive if proper treatment
policies are delivered. Since we are primarily concerned about the outcomes on subjects from Death
Population, we only sample subjects from the Death Population in this paper to train and evaluate
treatment policies.
There are a few hyper-parameters that should be set in advance: 1) Simulation step size T : every
T time, the simulator updates the internal status of subjects by computing the ODEs with feature
values from the last simulation step and the current action. 2) Simulation horizon length T : we can
evaluate the performance of a policy by checking outcomes of subjects after time T . 3) Valid time
18
Under review as a conference paper at ICLR 2021
range L for patients to receive treatment: operations can take place at any time-step (L = [0, T - 1])
or be constrained to predefined time intervals (L [0, T - 1]). 4) Frequency of decision-making f
: subjects can receive operations at each simulation step τ or less frequently. 5) Duration of each
blood purification operation l: it takes some costs to turn on/off the purification device and it is also
unrealistic to attach and detach the device from the subject too frequently. Therefore, there should be
a pre-defined value for the purification duration to rule out the possibility of too frequent actions.
To generate testable hypotheses that guide future laboratory experiments (Song et al., 2012; Stojkovic
et al., 2016), the simulation of sepsis evolution should be configured to make the generated trajectory
closely follow experimentally observed temporal patterns (Song et al., 2012). Further, several
constraints can be imposed on the simulation in accordance with previous blood purification studies
(Song et al., 2012; Stojkovic et al., 2016). Therefore we use the configuration listed in Table. 4 for
experiments.
Model Implementation and Training We randomly sample 3,000 subjects for training, 1,000 for
validation, and 1,000 for testing. Implementation details of the deep RL approaches are similar to
those mentioned in the Cancer task, except that the backend network is LSTM-based since this is a
POMDP.
Learning efficient treatment policies for Septic subjects is more difficult for Cancer due to the larger
state space and the partially observable environment. Therefore, we adopt the following methods to
ensure robust learning: 1) Mini-batch gradient descent with batch size 10,000 is adopted to update
parameters in reward estimator and RL agents. 2) The learning rate for RL agents is 0.01 while
0.001 for the reward estimator. 3) As discussed in Experiment Section, experience replay makes the
estimated reward positively proportional to the Survival Rate. We randomly extract policy pairs from
the latest 30,000 samples for model updates.
19
Under review as a conference paper at ICLR 2021
Figure 11: Distribution of state features (tumor size and toxicity level) from Cancer subjects without
tre
No Treatment
4
3 2 10
VN-W 5lun1-
2	4	6
Month
se≈">wns
6 5 4 3
■ ■ ■ ■


Constant Worst (0.1)
4 3 2 1
VN-W Lown1-
2	4	6
seu"Λ!AJnS
6 5 4 3
■ ■ ■ ■

Month
PBPl
----Tumor Size
Survival Rate
3 2 1
VN-W 5lun1-
-eAwns
0.1

4
Month
6 00
Single-objective RL (Ensemble)
VN-W 5lun1-
0.1
2	4
Month
seu"Λ!AJnS
8 7 6 5 4 3 2
■ ■■■■■■
Ooooooo
6 00
Grid-search Multi-objective RL
----Tumor Size
-→- Survival Rate
VN-W ħ∈β
-eAwns
No Treatment
2.5
I I I I 1
U Λw∙ax2
-¢5 λmv!xoj.
I I I I 1
-φδjff5xo1-
2	4	6
Month
AbRM (CE & OF-I)
—Tumor Size
-→- Survival Rate
se≈">wns
I I I I 1
-¢5 λmv!xoj.
2	4	6
Month
SbRM (CE&OF-I)
3 2 10
。-BS LOEn-L
2	4	6
Month
2eu"Λ!AJnS
I I I I 1
-φδjff5xo1-
se≈">wns
87654321
0.0-
2	4	6
Month
Constant Worst (0.1)
-φδjff5xo1-
0.1
2	4
Month
PBPI
seu"Λ!AJnS
8 7 6 5 4 3 2
6 00
-φδjffsxo1-
2	4
Month
-eAwns
87654321
6 00
Single-objective RL (Ensemble)
----Toxicity Level
-→- Survival Rate
2	4
Month
seu"Λ!AJnS
87654321
6 00
Grid-SearCh MUIti-ObjeCtiVe RL
-eAwns
a;----3-
Ooooooo
2	4
Month
0.1
0.0
AbRM (CE & OF-I)
0∙0∙0∙0∙
0∙0∙0∙0∙
se≈">IΛJns
543∙
2	4	6
Month
SbRM (CE&OF-I)
2	4
Month
2eu"≥ΛJns
0;∙∙∙4∙∙∙Q
Ooooooooo
Constant Best (0.4)
--2 1
VN-W 5lun1-
2	4
Month
se≈">wns
6 00
Random
0.1
2	4
Month
Single-objective RL
----Tumor Size
Survival Rate
6 00

--2 1
VN-W 5lun1-
Month
Existing Multi-objective RL
-21
VN-W 5lun1-
Month

6 00
AbRNl (CE)
--2 1
VN-W ħ∈β
2	4
Month
6
SbRM (CE)
-21
VN-W 5lun1-
-eAwns
■ _ I I
se≈">wns
■ ___ I I
Constant Best (0.4)
Existing Multi-objective RL
AbRM (CE)
I I I I 1
-φδjff5xo1-
I I I I 1
-¢5 ⅛-V-XOH
2	4
Month
SbRNl (CE)
se≈">wns
6 5 4 3 2 1
■ ■■■■■
Oooooo
0.8
0.7
0.6
2
・0.5 W
0.45
⅛
63台
0.2
0.1
0.0

，0.0
2	4	6
Month
20
Under review as a conference paper at ICLR 2021
Figure 12: Interaction network of inflammatory responses and hypothetic hemoadsorption mecha-
nisms of action in CLP-induced sepsis. Nodes in green represent components in peritoneum, nodes
in orange stand for blood components, and nodes in purple stand for lung components. Edges
represent network interactions under blood purification treatment compiled from literature. When
blood purification is performed, only features PI, AI and Na are influenced (marked by red dashed
rings).
A activation .............■	inhibition
A mass flow
operation influence
Figure 13: Distribution of 19 state features from Septic subjects without treatment or with treatment
from SbRM.
CLP
Pl
Al
le7 Np
le7 Nr
0	500	1000
0	500	1000
0	500	1000
0	500	1000
21