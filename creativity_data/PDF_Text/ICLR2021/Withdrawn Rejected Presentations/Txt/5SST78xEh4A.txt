Under review as a conference paper at ICLR 2021
ProSelfLC: Progressive Self Label Correc-
tion for Training Robust Deep Neural Net-
WORKS
Anonymous authors
Paper under double-blind review
Ab stract
To train robust deep neural networks (DNNs), we systematically study several
target modification approaches, which include output regularisation, self and non-
self label correction (LC). Two key issues are discovered: (1) Self LC is the most
appealing as it exploits its own knowledge and requires no extra models. How-
ever, how to automatically decide the trust degree of a learner as training goes is
not well answered in the literature? (2) Some methods penalise while the others
reward low-entropy predictions, prompting us to ask which one is better?
To resolve the first issue, taking two well-accepted ProPositions-deep neural net-
works learn meaningful patterns before fitting noise (Arpit et al., 2017) and mini-
mum entropy regularisation principle (Grandvalet & Bengio, 2006)-we propose a
novel end-to-end method named ProSelfLC, which is designed according to learn-
ing time and entropy. Specifically, given a data point, we progressively increase
trust in its predicted label distribution versus its annotated one if a model has been
trained for enough time and the prediction is of low entropy (high confidence). For
the second issue, according to ProSelfLC, we empirically prove that it is better to
redefine a meaningful low-entropy status and optimise the learner toward it. This
serves as a defence of entropy minimisation. We demonstrate the effectiveness of
ProSelfLC through extensive experiments in both clean and noisy settings.
1	Introduction
There exist many target (label) modification approaches. They can be roughly divided into two
groups: (1) Output regularisation (OR), which is proposed to penalise overconfident predictions for
regularising deep neural networks. It includes label smoothing (LS) (Szegedy et al., 2016; Muller
et al., 2019) and confidence penalty (CP) (Pereyra et al., 2017); (2) Label correction (LC). On the
one hand, LC regularises neural networks by adding the similarity structure information over training
classes into one-hot label distributions so that the learning targets become structured and soft. On
the other hand, it can correct the semantic classes of noisy label distributions. LC can be further
divided into two subgroups: Non-self LC and Self LC. The former requires extra learners, while the
latter relies on the model itself. A typical approach of Non-self LC is knowledge distillation (KD),
which exploits the predictions of other model(s), usually termed teacher(s) (Hinton et al., 2015).
Self LC methods include Pseudo-Label (Lee, 2013), bootstrapping (Boot-soft and Boot-hard) (Reed
et al., 2015), Joint Optimisation (Joint-soft and Joint-hard) (Tanaka et al., 2018), and Tf-KDself
(Yuan et al., 2020). According to an overview in Figure 1 (detailed derivation is in Section 3 and
Table 1), in label modification, the output target of a data point is defined by combining a one-hot
label distribution and its corresponding prediction or a predefined label distribution.
Firstly, we present the drawbacks of existing approaches: (1) OR methods naively penalise confident
outputs without leveraging easily accessible knowledge from other learners or itself (Figure 1a); (2)
Non-self LC relies on accurate auxiliary models to generate predictions (Figure 1b). (3) Self LC is
the most appealing because it exploits its own knowledge and requires no extra learners. However,
there is a core question that is not well answered:
In Self LC, how much should we trust a learner to leverage its knowledge?
As shown in Figure 1b, in Self LC, for a data point, we have two labels: a predefined one-hot q and a
predicted structured p. Its learning target is (1 - )q+ p, i.e., a trade-off between q and p, where
defines the trust score of a learner. In existing methods, is fixed without considering that a model’s
1
Under review as a conference paper at ICLR 2021
(a) OR includes LS (SZegedy et al., 2016) and CP (Pereyra et al., 2017). LS softens a target by adding a uniform
label distribution. CP changes the probability 1toa smaller value 1 — e in the one-hot target. The double-ended
arrow means factual equivalence, because an output is definitely non-negative after a softmax layer.
(b) LC contains Self LC (Lee, 2013; Reed et al., 2015; Tanaka et al., 2018; Yuan et al., 2020) and Non-self LC
(Hinton et al., 2015). The parameter e defines how much a predicted label distribution is trusted.
Figure 1: Target modification includes OR (LS and CP), and LC (SeIfLC and Non-selfLC). Assume
there are three training classes. q is the one-hot target. U is a uniform label distribution. P denotes
a predicted label distribution. The target combination parameter is ∈ [0, 1].
knowledge grows as the training progresses. For example, in bootstrapping, is fixed throughout
the training process. Joint Optimisation stage-wisely trains a model. It fully trusts predicted labels
and uses them to replace old ones when a stage ends, i.e., = 1. Tf-KDself trains a model by two
stages: = 0 in the first one while is tuned for the second stage. Note that P is generated by a
preceding-stage model in stage-wise training, which requires significant human intervention and is
time-consuming in practice.
To improve Self LC, we propose a novel method named Progressive Self Label Correction (ProS-
elfLC), which is end-to-end trainable and needs negligible extra cost. Most importantly, ProSelfLC
modifies the target progressively and adaptively as training goes. Two design principles of ProS-
elfLC are: (1) When a model learns from scratch, human annotations are more reliable than its own
predictions in the early phase, during which the model is learning simple meaningful patterns before
fitting noise, even when severe label noise exists in human annotations (Arpit et al., 2017). (2) As
a learner attains confident knowledge as time progresses, we leverage it to revise annotated labels.
This is surrounded by minimum entropy regularisation, which is widely evaluated in unsupervised
and semi-supervised scenarios (Grandvalet & Bengio, 2005; 2006).
Secondly, note that OR methods penalise low entropy while LC rewards it, intuitively leading to a
second vital question:
Should we penalise a low-entropy status or reward it?
Entropy minimisation is the most widely used principle in machine learning (Hartigan & Wong,
1979; Rumelhart et al., 1986; Grandvalet & Bengio, 2005; 2006; LeCun et al., 2015). In standard
classification, minimising categorical cross entropy (CCE) optimises a model towards a low-entropy
status defined by human annotations, which contain noise in very large-scale machine learning. As a
result, confidence penalty becomes popular for reducing noisy fitting. In contrast, we prove that it is
better to reward a meaningful low-entropy status redefined by our ProSelfLC. Therefore, our work
offers a defence of entropy minimisation against the recent confidence penalty practice (SZegedy
et al., 2016; Muller et al., 2019; Pereyra et al., 2017; Dubey et al., 2018).
Finally, we summarise our main contributions:
•	We provide a theoretical study on popular target modification methods through entropy and
KL divergence (Kullback & Leibler, 1951). Accordingly, we reveal their drawbacks and
propose ProSelfLC as a solution. ProSelfLC can: (1) enhance the similarity structure in-
formation over training classes; (2) correct the semantic classes of noisy label distributions.
ProSelfLC is the first method to trust self knowledge progressively and adaptively.
•	Our extensive experiments: (1) defend the entropy minimisation principle; (2) demonstrate
the effectiveness of ProSelfLC in both clean and noisy settings.
2
Under review as a conference paper at ICLR 2021
2	Related Work
Label noise and semi-supervised learning. We test target modification approaches in the setting of
label noise because it is generic and connected with semi-supervised learning, where only a subset of
training examples are annotated, leading to missing labels. Then the key to semi-supervised training
is to reliably fill them. When these missing labels are incorrectly filled, the challenge of semi-
supervised learning changes to noisy labels. For a further comparison, in semi-supervised learning,
the annotated set is clean and reliable, because the label noise only exists in the unannotated set.
While in our experimental setting, we are not given information on whether an example is trusted or
not, thus being even more challenging. We summarise existing approaches for solving label noise:
(1) Loss correction, in which we are given or we need to estimate a noise-transition matrix, which
defines the distribution of noise labels (Li et al., 2017; Goldberger & Ben-Reuven, 2017; Sukhbaatar
& Fergus, 2014; Vahdat, 2017; Yao et al., 2019; Han et al., 2018a; Patrini et al., 2017; Xiao et al.,
2015). A noise-transition matrix is difficult and complex to estimate in practice; (2) Exploiting an
auxiliary trusted training set to differentiate examples (Veit et al., 2017; Lee et al., 2018; Hendrycks
et al., 2018). This requires extra annotation cost; (3) Co-training strategies, which train two or more
learners (Malach & Shalev-Shwartz, 2017; Jiang et al., 2018; Han et al., 2018b; Yu et al., 2019;
Wei et al., 2020; Qiao et al., 2018) and exploit their ‘disagreement’ information to differentiate data
points; (4) Label engineering methods (Song et al., 2019; Lee, 2013; Reed et al., 2015; Tanaka
et al., 2018; Yao et al., 2019), which relate to our focus in this work. Their strategy is to annotate
unlabelled samples or correct noisy labels.
LC and knowledge distillation (KD) (Bucila et al., 2006; Hinton et al., 2015). Mathematically, we
derive that some KD methods also modify labels. We use the term label correction instead of KD
for two reasons: (1) label correction is more descriptive; (2) the scope of KD is not limited to label
modification. For example, multiple networks are trained for KD (Furlanello et al., 2018). When
two models are trained, the consistency between their predictions of a data point is promoted in
(Ba & Caruana, 2014; Zhang et al., 2018), while the distance between their feature maps is reduced
in (Romero et al., 2015). Regarding self KD, two examples of the same class are constrained to
have consistent output distributions (Xu & Liu, 2019; Yun et al., 2020). In another self KD (Zhang
et al., 2019), the deepest classifier provides knowledge for shallower classifiers. In a recent self KD
method (Yuan et al., 2020), Tf-KDself applies two-stage training. In the second stage, a model is
trained by exploiting its knowledge learned in the first stage. Our focus is to improve the end-to-end
self LC. Finally, we acknowledge that exploiting ProSelfLC to improve non-self KD and stage-wise
approaches is an area for future work, e.g., a better teacher model can be trained using ProSelfLC.
3	Mathematical analysis and theory
Let X = {(xi, yi)}iN=1 represent N training examples, where (xi, yi) denotes i-th sample with
input xi ∈ RD and label yi ∈ {1, 2, ..., C}. C is the number of classes. A deep neural network z
consists of an embedding network f (∙) : RD → RK and a linear classifier g(∙) : RK → RC, i.e.,
zi = z(xi) = g(f (xi)) : RD → RC. For the brevity of analysis, we take a data point and omit
its subscript so that it is denoted by (x, y). The linear classifier is usually the last fully-connected
layer. Its output is named logit vector z ∈ RC . We produce its classification probabilities p by
normalising the logits using a softmax function:
p(j|x) = exp(zj)/XC	exp(zm),	(1)
m=1
where p(j|x) is the probability ofx belonging to class j. Its corresponding ground-truth is usually
denoted by a one-hot representation q: q(j|x) = 1 ifj = y, q(j|x) = 0 otherwise.
3.1	Semantic class and similarity structure in a label distribution
Definition 1 (Semantic Class). Given a target label distribution q(x) ∈ RC, the semantic class is
defined by arg maxj q(j∣x), i.e., the class whose probability is the largest.
Definition 2 (Similarity Structure). In qq(x), x has C probabilities of being predicted to C classes.
The similarity structure of x versus C classes is defined by these probabilities and their differences.
3
Under review as a conference paper at ICLR 2021
Table 1: Summary of CCE, LS, CP and LC.
CCE	LS	CP	LC
Learning Target	q	OLS = (1 - e)q +eu OCP = (1 - e)q -ep OLC = (1 - e)q +ep
Cross Entropy	Eq(- log P)	E@ls(- log P)	EqCP (- log P)	EqLC (- log P)
KL Divergence	KL(q||P)	(I- HKLgIIP)	(1 -OKL同 P)	(1 - o^KLgIIP)
	+eKL(u∣∣p)	+eKL(P∣∣u)	-eKL(P∣∣u)
Entropy minimisation	一 Semantic class	Annotated Similarity structure	No	PenaliSe over CCE Annotated No	PenaliSe over CCE Annotated No	Reward over CCE Annotated and Learned Yes
3.2	REVISIT OF CCE, LS, CP AND LC
Standard CCE. For any input (x, y), the minimisation objective of standard CCE is:
LCCE(q, p) = H(q, p) = Eq(- log p),	(2)
where H(∙, ∙) represents the cross entropy. Eq(-log P) denotes the expectation of negative log-
likelihood, and q is the probability mass function.
Label smoothing. In LS (Szegedy et al., 2016; Hinton et al., 2015), we soften one-hot targets by
adding a uniform distribution: Qls = (1 - e)q + eu, U ∈ RC, and ∀j, Uj = C. Consequently:
LccE+Ls(q, p; e) = H(QLS, P) = E%s(-log p) = (1 - e)H(q, p)+eH(u, p).	(3)
Confidence penalty. CP (Pereyra et al., 2017) penalises highly confident predictions:
LCCE+CP(q, P; e) = (1 - e)H(q, P)-eH(P, P).	(4)
Label correction. As illustrated in Figure 1, LC is a family of algorithms, where a one-hot label
distribution is modified to a convex combination of itself and a predicted distribution:
QqLC = (1 - e)q+ eP ⇒ LCCE+LC(q, P; e) = H(QqLC, P) = (1 - e)H(q, P)+eH(P, P).	(5)
We remark: (1) P provides meaningful information about an example’s relative probabilities of
being different training classes; (2) If e is large, and P is confident in predicting a different class,
i.e., arg maxj P(j|x) 6= arg maxj q(j|x), qQLC defines a different semantic class from q.
3.3	Theory
Proposition 1. LS, CP and LC modify the learning targets of standard CCE.
Proof. LCCE+CP (q, P; e) = (1 - e)H(q, P)-eH(P, P) = E(1-)q-p(- log P). Therefore, qQCP =
(1 - e)q-eP. Additionally, QqLS = (1 - e)q+eU, qQLC = (1 - e)q+eP.
Proposition 2. Some KD methods, which aim to minimise the KL divergence between predictions of
a teacher and a student, belong to the family of label correction.
Proof. In general, a loss function of such methods can be defined to be LKD (q, Pt, P) = (1 -
e)H(q, p) + eKL(pt∣∣p) (YUanetaL,2020). KL(∙∣∣∙) denotes the KL divergence. As KL(pt∣∣p)=
H(Pt, P)-H(Pt, Pt), Pt is from a teacher and fixed when training a student. We can omitH(Pt, Pt):
LKD(q,Pt,P) = (1 - )H(q, P) + H(Pt, P) = E(1-)q+pt (- log P) ⇒ OqKD = (1 - )q + Pt. (6)
Consistent with LC in Eq (5), LKD(q, Pt, P) revises a label using Pt.
Proposition 3. Compared with CCE, LS and CP penalise entropy minimisation while LC reward it.
Proposition 4. In CCE, LS and CP, a data point x has the same semantic class. In addition, x has
an identical probability of belonging to other classes except for its semantic class.
The proof of propositions 3 and 4 is presented in the Appendix A. Only LC exploits informative
information and has the ability to correct labels, while LS and CP only relax the hard targets. We
summarise CCE, LS, CP and LC in Table 1. Constant terms are ignored for concision.
4	ProSelfLC: Progressive and Adaptive Label Correction
In standard CCE, a semantic class is considered while the similarity structure is ignored. It is
mainly due to the difficulty of annotating the similarity structure for every data point, especially
4
Under review as a conference paper at ICLR 2021
Table 2: Instantiating ProSelfLC under different cases. For all terms, we use concrete values for
concise interpretation. We bold the special case when the semantic class is changed.
l(p): Consistency is defined by whether p and q share the semantic class or not.
0.1(non-confident) 0.9(confidently consistent) 0.9(confidently inconsistent)
Earlier phase g(t) = 0.1	0.01	0.09	0.09
Later phase g(t) = 0.9	0.09	0.81	0.81
when C is large (Xu et al., 2020). Fortunately, recent progress demonstrates that there are some
effective approaches to define the similarity structure of data points without annotation: (1) In KD,
an auxiliary teacher model can provide a student model the similarity structure information (Hinton
et al., 2015; Muller et al., 2019); (2) In Self LC, e.g., Boot-soft, a model helps itself by exploiting
the knowledge it has learned so far. We focus on studying the end-to-end Self LC.
In Self LC, indicates how much a predicted label distribution is trusted. In ProSelfLC, we propose
to set it automatically according to learning time t and prediction entropy H(p), i.e., ProSelfLC
trusts self knowledge according to training time and confidence. For any x, we summarise:
LoSS: L(⅞ProSelfLC, p; ∈ProSelfLC) = H(GProSeIfLC, P) = Eq
ProSelfLC (- log p).
Label: qGProSelfLC = (1 - ProSelfLC)q + ProSelfLCp.
g(t) = h(t∕Γ — 0.5, B) ∈ (0,1),
Self trust: ProSelfLC = g(t) × l(p) l(p) = 1 - H(p)/H(u) ∈ (0, 1).
(7)
t and Γ are the iteration counter and the number of total iterations, respectively. h(η, B) =
1∕(1 + exp(-η × B)). B, Γ are task-dependent and searched on a validation set. We clarify:
Global trust score g(t) denotes how much we trust a learner. It is independent of data points, thus
being global. g(t) grows as t rises. B adjusts the exponentiation’s base and growth speed ofg(t).
The local trust score l(p) indicates how much we trust an output distribution p, which is data-
dependent. l(p) rises as H(p) becomes lower, rewarding a confident distribution.
Design reason. (1) Regarding g(t), in the earlier learning phase, i.e., t < Γ∕2, g(t) < 0.5 ⇒
ProSelfLC < 0.5, ∀p, so that the human annotations dominate and ProSelfLC only modifies the
similarity structure. When a learner has not seen the training data for enough time at the earlier stage,
its knowledge is less reliable and a wrong confident prediction may occur. Our design assuages the
bad impact of such unexpected cases. When it comes to the later training phase, i.e., t > Γ∕2, we
have g(t) > 0.5 as it has been trained for more than half of entire iterations. (2) Regarding l(p),
it affects the later learning phase. If p is less confident, l(p) will be smaller, then ProSelfLC will
be smaller, hence we trust p less when it is of higher uncertainty. If p is highly confident, we trust
its confident knowledge. Ablation study of our design is in Figure 2, where three variants of are
presented. In our experiments, note that when is fixed, we try three values (0.125, 0.25, 0.50) and
display the best instantiation, i.e., = 0.50.
We conduct the case analysis of ProSelfLC in Table 2 and summarise its core tactics as follows:
(1) Correct the similarity structure for every data point in all cases, thanks to exploiting the self
knowledge of a learner, i.e., p. (2) Revise the semantic class when t is large enough and p
is confidently inconsistent. As highlighted in Table 2, when two conditions are met, we have
ProSelfLC > 0.5 and arg maxj p(j|x) 6= arg maxj q(j|x), then p redefines the semantic class.
For example, if p = [0.95, 0.01, 0.04], q = [0, 0, 1], ProSelfLC = 0.8 ⇒ qGProSelfLC = (1 -
ProSelfLC)q+ProSelfLCp = [0.76, 0.008, 0.232]. Note that ProSelfLC also becomes robust against
lengthy exposure to the training data, as demonstrated in Figures 2 and 3.
5 Experiments
In our experiments, we re-implement CCE, LS and CP. Regarding Self LC methods, we re-
implement Boot-soft (Reed et al., 2015), where is fixed throughout training. We do not re-
implement stage-wise Self LC and KD methods, e.g., Joint Optimisation and Tf-KDself respec-
tively, because time-consuming tuning is required. We fix the random seed and do not use any
random accelerator for an entirely fair comparison. In standard and synthetic cases, we train on
80% training data (corrupted in synthetic cases) and use 20% trusted training data as a validation set
to search hyperparameters, e.g., , Γ, B and settings of an optimiser. Note that Γ and an optimiser’s
5
Under review as a conference paper at ICLR 2021
86 /2o
Oooo
⅛sqns u(ðəɪoθq+≡tSbβUMH
Q 6 /2C
Uooo
⅛sqns A.souυτμ J。uoppaJJO。
(a) Correct fitting.	(b) Semantic class correction	(c) Generalisation.
Figure 2: Comparison of setting using different schemes. Experiments are done on CIFAR-100
with asymmetric label noise r = 0.4. For data-dependent items, mean results are reported.
settings are searched first and then shared by all methods. Finally, we retrain a model on the entire
training data (corrupted in synthetic cases) and report its accuracy on the test data to fairly compare
with prior results. In real-world label noise, the used dataset has a separate clean validation set for
searching hyperparameters. Code will be released once this work is accepted.
5.1	Standard image classification
Datasets and training details. (1) CIFAR-100 (Krizhevsky, 2009) has 20 coarse classes, each
containing 5 fine classes. There are 500 and 100 images per class in the training and testing sets,
respectively. The image size is 32 × 32. We apply simple data augmentation (He et al., 2016), i.e.,
we pad 4 pixels on every side of the image, and then randomly crop it with a size of 32 × 32. Finally,
this crop is horizontally flipped with a probability of 0.5. We choose SGD with its settings as: (a)
a learning rate of 0.1; (b) a momentum of 0.9; (c) a weight decay of 5e - 4; (d) the batch size is
256 and the number of training iterations is 30k. We divide the learning rate by 10 at 15k and 22k
iterations, respectively. (2) We train ResNet-50 (He et al., 2016) on ImageNet 2012 classification
dataset, which has 1k classes and 50k images in the test set (Russakovsky et al., 2015). We use SGD
with a start learning rate of 2e - 3. A polynomial learning rate decay with a power of 2 is used. We
set the momentum to 0.95 and the weight decay to 1e - 4. We train on a single V100 GPU and the
batch size is 64. We report the final test accuracy when the training ends at 500k iterations. We use
the standard data augmentation: an original image is warped to 256 × 256, followed by a random
crop of 224 × 224. This crop is randomly flipped. We fix common settings to fairly compare CCE,
LS, CP, Boot-soft and ProSelfLC.
Result analysis. In Table 3, we observe the superiority of ProSelfLC in standard setting without
considering label noise. Being probably surprising, LS and CP reduce the performance consistently
as increases on ImageNet. Instead, Boot-soft and ProSelfLC improve versus CCE. We remark that
both test sets are large so that their differences are noticeable.
5.2	Synthetic label noise
Noise generation. (1) Symmetric label noise: the original label of an image is uniformly changed
to one of the other classes with a probability of r; (2) Asymmetric label noise: we follow (Wang
et al., 2019) to generate asymmetric label noise to fairly compare with their reported results. Within
each coarse class, we randomly select two fine classes A and B. Then we flip r × 100% labels of A
to B , and r × 100% labels of B to A. We remark that the overall label noise rate is smaller than r.
Baselines.1 We compare with the results reported recently in SL (Wang et al., 2019). Forward is
a loss correction approach that uses a noise-transition matrix (Patrini et al., 2017). D2L monitors
the subspace dimensionality change at training (Ma et al., 2018). GCE denotes generalised cross
entropy (Zhang & Sabuncu, 2018) and SL is symmetric cross entropy (Wang et al., 2019). They are
robust losses designed for solving label noise. Training details are the same as Section 5.1.
Result analysis. For all methods, we directly report their final results when training terminates.
Therefore, we test the robustness of a model against not only label noise, but also a long time being
1We do not consider DisturbLabel (Xie et al., 2016), which flips labels randomly and is counter-intuitive. It
weakens the generalisation because generally the accuracy drops as the uniform label noise increases.
6
Under review as a conference paper at ICLR 2021
Table 3: Test accuracy (%) in the standard setting. We report three settings of hyperparameters.
Dataset
LS ()	CP ()	Boot-soft ()	ProSelfLC (B)
0.125	0.25	0.50	0.125	0.25	0.50	0.125	0.25	0.50	^^8	10	12
CIFAR-100	69.0	69.9	69.6	68.4	69.5	69.3	68.7	68.9	69.1	69.1	70.1	70.3	69.8
ImageNet 2012	75.5	75.3	75.2	74.9	75.2	74.8	74.6	75.7	75.8	75.8	76.0	76.0	75.9
86 /2o
Oooo
⅛sqns Ueəp θq+≡J。bfi∙a芸 Ia
6 5 4 3 2 1 0
Oooooo
⅛gqns A∙soɑυqa J。bouμ≤I⅛
i.∙ 304
86 /2o
Oooo
⅛sqns A.SOUφq+aJ。UoI-8Mlo。
1	2
Iterations
(d) Entropy of clean subset.
(e) Entropy of noisy subset.
(f) Generalisation.
Figure 3: Comprehensive learning dynamics on CIFAR-100 with asymmetric label noise r = 0.4.
For data-dependent items, mean results are reported. At training, a learner is NOT GIVEN whether
a label is trusted or not. We store intermediate models and analyse them when the training ends.
exposed to the data. In Table 4, we observe that: (1) ProSelfLC outperforms all baselines, which
is significant in most cases; (2) In both implementation, Boot-hard and Boot-soft perform worse
than the others. However, our ProSelfLC makes Self LC the best solution. Furthermore, learning
dynamics are visualised in Figure 3, which helps to understand why ProSelfLC works better.
Results of different B, are in Table 5. Appendix B shows the learning dynamics when r changes.
Revising the semantic class and similarity structure. In Figures 3b and 3c, we show dynamic
statistics of different approaches on fitting wrong labels and correcting them. ProSelfLC is much
better than its counterparts. Semantic class correction reflects the change of similarity structure.
To redefine and reward a low-entropy status. On the one hand, we observe that LS and CP work
well, being consistent with prior claims. In Figures 3d and 3e, the entropies of both clean and
noisy subsets are much higher in LS and CP, correspondingly their generalisation is the best except
for ProSelfLC in Figure 3f. On the other hand, ProSelfLC has the lowest entropy while performs
the best, which proves that a learner’s confidence does not necessarily weaken its generalisation
performance. Instead, a model needs to be careful with what to be confident in. As shown by
Figures 3b and 3c, ProSelfLC has the least wrong fitting and most semantic class correction, which
indicates that a meaningful low-entropy status is redefined.
5.3 Real-world label noise
Clothing 1M (Xiao et al., 2015) has around 38.46% label noise in the training data and about 1
million images of 14 classes from shopping websites. Its internal noise structure is agnostic.
Baselines. For loss correction and estimating the noise-transition matrix, S-adaption (Goldberger &
Ben-Reuven, 2017) uses an extra softmax layer, while Masking (Han et al., 2018a) exploits human
cognition. MD-DYR-SH (Arazo et al., 2019) is a combination of three techniques: dynamic mixup
(MD), dynamic bootstrapping together with label regularisation (DYR) and soft to hard (SH). The
7
Under review as a conference paper at ICLR 2021
Table 4: Accuracy (%) on the CIFAR-100 clean test set. All compared methods use ResNet-44.
Asymmetric Noisy Labels	Symmetric Noisy Labels
	Method	r=0.2	r=0.3	r=0.4	r=0.2	r=0.4	r=0.6
	Boot-hard	63.4	63.2	62.1	57.9	48.2	12.3
Results From	Forward	64.1	64.0	60.9	59.8	53.1	24.7
	D2L	62.4	63.2	61.4	59.2	52.0	35.3
SL (Wang et al., 2019)	GCE	63.0	63.2	61.7	59.1	53.3	36.2
	SL	65.6	65.1	63.1	60.0	53.7	41.5
	CCE	66.6	63.4	59.5	58.0	50.1	37.9
Our Trained Results	LS	67.9	66.4	65.0	63.8	57.2	46.5
	CP	67.7	66.0	64.4	64.0	56.8	44.1
	Boot-soft	66.9	65.3	61.0	63.2	59.0	44.8
	ProSelfLC	68.7	68.5	67.9	64.8	59.3	47.7
Table 5: The results of different hyperparameters on CIFAR-100 using ResNet-44. Under different
noise rates, the best instantiation of each approach is bolded except for CCE.
Method (hyperparameter)	Value of hyperparameter	Asymmetric label noise			Symmetric label noise			Clean
		20%	30%	40%	20%	40%	60%	
CCE	None or = 0	66.6	63.4	59.5	58.0	50.1	37.9	69.0
	0.125	66.4	65.6	63.1	61.7	52.5	39.1	69.9
LS ()	0.25	67.9	66.4	65.0	62.8	55.9	40.9	69.6
	0.50	66.8	65.8	64.6	63.8	57.2	46.5	68.4
	0.125	65.7	64.2	60.3	59.8	52.3	39.6	69.5
CP ()	0.25	66.8	65.1	61.6	61.0	53.3	40.9	69.3
	0.50	67.7	66.0	64.4	64.0	56.8	44.1	68.7
	0.125	65.8	64.1	60.7	59.7	51.2	40.6	68.9
Boot-soft ()	0.25	66.2	64.1	60.3	61.1	54.4	43.3	69.1
	0.50	66.9	65.3	61.0	63.2	59.0	44.8	69.1
	8	67.8	67.4	67.9	64.7	57.7	47.7	70.1
	10	68.5	68.5	66.8	63.9	59.0	47.5	70.3
ProSelfLC (B)	12	68.6	67.9	67.4	64.0	59.3	47.5	69.8
	14	68.7	68.0	67.8	64.8	59.0	47.4	69.6
	16	68.4	67.2	67.3	63.7	59.0	32.3	69.9
Table 6: Test accuracy (%) on the real-world noisy dataset Clothing 1M.
Bhoaordt- Forward D2L GCE SL adapSta-tion	MD- Joint-	Our Trained Results Masking DYRSH Soft 	 DYR-SH soft CCE LS CP Boot-soft ProSelfLC
68.9	69.8	69.5 69.8 71.0	70.3	71.1	71.0	72.2 71.8 72.6 72.4	72.3	73.4
other baselines have been introduced heretofore.
Training details. We follow (Tanaka et al., 2018) to train ResNet-50 and initialise it by a trained
model on ImageNet. We follow Section 5.1 with small changes: the initial learning rate is 0.01 and
we train 10k iterations. They are searched on the separate clean validation set.
Result analysis. In Table 6, analogously to CIFAR-100, we report our trained results of CCE, LS,
CP, Boot-soft and ProSelfLC for an entirely fair comparison. ProSelfLC has the highest accuracy,
which demonstrates its effectiveness again.
6 Conclusion
We present a thorough mathematical study on several target modification techniques. Through anal-
ysis of entropy and KL divergence, we reveal their relationships and limitations.
To improve and endorse self label correction, we propose ProSelfLC. Extensive experiments prove
its superiority over existing methods under standard and noisy settings. ProSelfLC enhances the
similarity structure information over classes, and rectifies the semantic classes of noisy label distri-
butions. ProSelfLC is the first approach to trust self knowledge progressively and adaptively.
ProSelfLC redirects and promotes entropy minimisation, which is in marked contrast to recent prac-
tices of confidence penalty (Szegedy et al., 2016; Pereyra et al., 2017; Dubey et al., 2018).
8
Under review as a conference paper at ICLR 2021
References
Eric Arazo, Diego Ortego, Paul Albert, Noel O’Connor, and Kevin Mcguinness. Unsupervised label
noise modeling and loss correction. In ICML, 2019.
Devansh Arpit, StanisIaW Jastrzebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxin-
der S. Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, and Simon
Lacoste-Julien. A closer look at memorization in deep netWorks. In ICML, 2017.
Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NeurIPS, 2014.
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In KDDM,
2006.
Abhimanyu Dubey, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik. Maximum-entropy fine grained
classification. In NeurIPS, 2018.
Tommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.
Born again neural netWorks. In ICML, 2018.
Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-netWorks using a noise adaptation
layer. In ICLR, 2017.
Yves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In
NeurIPS, 2005.
YveS Grandvalet and Yoshua Bengio. Entropy regularization. Semi-supervised learning, pp. 151—
168, 2006.
Bo Han, Jiangchao Yao, Gang Niu, Mingyuan Zhou, Ivor Tsang, Ya Zhang, and Masashi Sugiyama.
Masking: A neW perspective of noisy supervision. In NeurIPS, 2018a.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural netWorks With extremely noisy labels. In
NeurIPS, 2018b.
John A Hartigan and Manchek A Wong. Algorithm as 136: A k-means clustering algorithm. Journal
ofthe Royal Statistical Society. Series C (Applied Statistics), 28(1):100-108, 1979.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep netWorks on labels corrupted by severe noise. In NeurIPS, 2018.
Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knoWledge in a neural netWork. In
NeurIPS Deep Learning and Representation Learning Workshop, 2015.
Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-
driven curriculum for very deep neural netWorks on corrupted labels. In ICML, 2018.
Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathe-
matical statistics, pp. 79-86, 1951.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, pp. 436, 2015.
Dong-Hyun Lee. Pseudo-label: The simple and efficient semi-supervised learning method for deep
neural netWorks. 2013.
Kuang-Huei Lee, Xiaodong He, Lei Zhang, and Linjun Yang. Cleannet: Transfer learning for
scalable image classifier training With label noise. In CVPR, 2018.
Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from
noisy labels With distillation. In ICCV, 2017.
9
Under review as a conference paper at ICLR 2021
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah M Erfani, Shu-Tao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In ICML,
2018.
Eran Malach and Shai Shalev-Shwartz. Decoupling "when to update" from "how to update". In
NeurIPS, 2017.
Rafael Muller, Simon Komblith, and Geoffrey E Hinton. When does label smoothing help? In
NeurIPS, 2019.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In CVPR, 2017.
Gabriel Pereyra, George Tucker, Jan Chorowski, Eukasz Kaiser, and Geoffrey Hinton. Regularizing
neural networks by penalizing confident output distributions. In ICLR Workshop, 2017.
Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan Yuille. Deep co-training for semi-
supervised image recognition. In ECCV, 2018.
Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Ra-
binovich. Training deep neural networks on noisy labels with bootstrapping. In ICLR Workshop,
2015.
Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and
Yoshua Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.
David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning representations by back-
propagating errors. Nature, pp. 533-536, 1986.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual
recognition challenge. International Journal of Computer Vision, pp. 211-252, 2015.
Hwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust
deep learning. In ICML, 2019.
Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv
preprint arXiv:1406.2080, 2014.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking
the inception architecture for computer vision. In CVPR, 2016.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization frame-
work for learning with noisy labels. In CVPR, 2018.
Arash Vahdat. Toward robustness against label noise in training deep discriminative neural networks.
In NeurIPS, 2017.
Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhinav Gupta, and Serge Belongie. Learning
from noisy large-scale datasets with minimal supervision. In CVPR, 2017.
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross
entropy for robust learning with noisy labels. In ICCV, 2019.
Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combating noisy labels by agreement: A joint
training method with co-regularization. In CVPR, 2020.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In CVPR, 2015.
Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, and Qi Tian. Disturblabel: Regularizing cnn
on the loss layer. In CVPR, 2016.
Ning Xu, Jun Shu, Yun-Peng Liu, and Xin Geng. Variational label enhancement. In ICML, 2020.
10
Under review as a conference paper at ICLR 2021
Ting-Bing Xu and Cheng-Lin Liu. Data-distortion guided self-distillation for deep neural networks.
In AAAI, 2019.
Jiangchao Yao, Hao Wu, Ya Zhang, Ivor W Tsang, and Jun Sun. Safeguarded dynamic label regres-
sion for noisy supervision. In AAAI, 2019.
Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor W Tsang, and Masashi Sugiyama. How does
disagreement help generalization against label corruption? In ICML, 2019.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. Revisiting knowledge distillation
via label smoothing regularization. In CVPR, 2020.
Sukmin Yun, Jongjin Park, Kimin Lee, and Jinwoo Shin. Regularizing class-wise predictions via
self-knowledge distillation. In CVPR, 2020.
Linfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your
own teacher: Improve the performance of convolutional neural networks via self distillation. In
ICCV, 2019.
Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In CVPR,
2018.
Zhilu Zhang and Mert R Sabuncu. Generalized cross entropy loss for training deep neural networks
with noisy labels. In NeurIPS, 2018.
11
Under review as a conference paper at ICLR 2021
A Proof of propositions
Proposition 3. Compared with CCE, LS and CP penalise entropy minimisation while LC reward it.
Proof. We can rewrite CCE, LS, CP, and LC from the viewpoint of KL divergence:
LCCE (q, p) = H(q, p) = KL(q||p) + H(q, q) = KL(q||p),	(8)
where we have H(q, q) = 0 because q is a one-hot distribution.
LCCE+LS (q, p; ) = (1 - )KL(q||p) + KL(u||p) + H(u, u)	(9)
=(1 - e)KL(q∣∣p) + eKL(u∣∣p) + e ∙ constant,
LCCE+CP(q,p;e) = (1 - e)KL(q||p) - e(H(p, u) - KL(p||u))
=(1 - e)KL(q∣∣p) + eKL(p∣∣u) - e ∙ constant,
where H(p, u) = H(u, u) = constant. Analogously, LC in Eq (5) can also be rewritten:
LccE+Lc(q, p; e) = (1 - e)KL(q∣∣p) - eKL(p∣∣u) + e ∙ constant.
(10)
(11)
In LS and CP, both +KL(u||p) and +KL(p||u) pulls p towards u. While in LC, the term
-KL(p||u) pushes p away from u.
Proposition 4. In CCE, LS and CP, a data point x has the same semantic class. In addition, x has
an identical probability of belonging to other classes except for its semantic class.
Proof. In LS, the target is Qls = (1-e)q+eu. For any 0 ≤ e < 1, the semantic class is not changed,
because 1 - e + e * C > e * C. In addition, j1 = y,j2 = y ⇒ qLs(jι∣x) = QLs(j2∣x) = C.
In CP, qQCP = (1 - e)q - ep. In terms of label definition, CP is against intuition because these
zero-value positions in q are filled with negative values in QqCP. A probability has to be not smaller
than zero. So we rephrase qQCP(y|x) = (1 - e) - e * p(y|x), and ∀j 6= y, qQCP (j |x) = 0 by replacing
negative values with zeros, as illustrated in Figure 1a.
B	Learning dynamics of different noise rates
In Figure 4, we store a model every 1000 iterations to monitor the learning process.
C	THE CHANGES OF ENTROPY STATISTICS AND ProSelfLC AT TRAINING
In Figure 5, we visualise how the entropies of noisy and clean subsets change at training.
12
Under review as a conference paper at ICLR 2021
0.8
Iterations ×ιo4
0.7
0.6
0.5
0.4
0.3
1.5	2	2.5	3
×104
(a) r = 20%.	(b) r = 30%.	(c) r = 40%.
19876543210
a①S 6UEQ匕 Asou©A0n5」nuu‹
(d) r = 20%.	(e) r = 30%.	(f) r = 40%
Figure 4: Learning dynamics on CIFAR-100 under asymmetric noisy labels. We show all iterations
only in (a) and (d). In the others, we show the second half iterations, which are of higher interest. As
the noise rate increases, the superiority of ProSelfLC becomes more significant, i.e., avoiding fitting
noise in the 2nd row and leading to better generalisation in the 1st row.
(a) Asymmetric label noise rate = 20%.
(b) Asymmetric label noise rate = 40%.
Figure 5: The changes of entropy statistics and ProSelfLC at training. We store a model every 1000
iterations to monitor the learning process. For data-dependent metrics, after training, we split the
corrupted training data into clean and noisy subsets according to the information about how the
training data is corrupted before training. Finally, we report the mean results of each subset.
13