Under review as a conference paper at ICLR 2021
Adaptive Learning Rates for Multi-Agent Re-
inforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
In multi-agent reinforcement learning (MARL), the learning rates of actors and
critic are mostly hand-tuned and fixed. This not only requires heavy tuning but
more importantly limits the learning. With adaptive learning rates according to
gradient patterns, some optimizers have been proposed for general optimizations,
which however do not take into consideration the characteristics of MARL. In this
paper, we propose AdaMa to bring adaptive learning rates to cooperative MARL.
AdaMa evaluates the contribution of actors’ updates to the improvement of Q-
value and adaptively updates the learning rates of actors to the direction of maxi-
mally improving the Q-value. AdaMa could also dynamically balance the learning
rates between the critic and actors according to their varying effects on the learn-
ing. Moreover, AdaMa can incorporate the second-order approximation to capture
the contribution of pairwise actors’ updates and thus more accurately updates the
learning rates of actors. Empirically, we show that AdaMa could accelerate the
learning and improve the performance in a variety of multi-agent scenarios, and
the visualizations of learning rates during training clearly explain how and why
AdaMa works.
1	Introduction
Recently, multi-agent reinforcement learning (MARL) has been applied to decentralized cooperative
systems, e.g., autonomous driving (Shalev-Shwartz et al., 2016), smart grid control (Yang et al.,
2018), and traffic signal control (Wei et al., 2019). Many MARL methods (Lowe et al., 2017;
Foerster et al., 2018; Rashid et al., 2018; Iqbal & Sha, 2019; Son et al., 2019) have been proposed
for multi-agent cooperation, which follow the paradigm of centralized training and decentralized
execution. In many of these methods, a centralized critic learns the joint Q-function using the
information of all agents, and the decentralized actors are updated towards maximizing the Q-value
based on local observation.
However, in these methods, the actors are usually assigned the same learning rates, which is not
optimal for maximizing the Q-value. This is because some agents might be more critical than others
to improving the Q-value and thus should have higher learning rates. On the other hand, the learning
rates of actors and critic are often hand-tuned and fixed, and hence require heavy tuning. More
importantly, over the course of training, the effect of actors and critic on the learning varies, so the
fixed learning rates will not always be the best at every learning stage. The artificial schedules,
e.g., time-based decay and step decay, are pre-defined and require expert knowledge about model
and problem. Some optimizers, e.g., AdaGrad (Duchi et al., 2011), could adjust the learning rate
adaptively, but they are proposed for general optimization problems, not specialized for MARL.
In this paper, we propose AdaMa for adaptive learning rates in cooperative MARL. AdaMa dy-
namically evaluates the contribution of actors and critic to the optimization and adaptively updates
the learning rates based on their quantitative contributions. First, we examine the gain of Q-value
contributed by the update of each actor. We derive the direction along which the Q-value improves
the most. Thus, we can update the vector of learning rates of all actors towards the direction of
maximizing the Q-value, which leads to diverse learning rates that explicitly captures the contribu-
tions of actors. Second, we consider the critic and actors are updated simultaneously. If the critic’s
update causes a large change of Q-value, we should give a high learning rate to the critic since it is
leading the learning. However, the optimization of actors, which relies on the critic, would strug-
1
Under review as a conference paper at ICLR 2021
gle with the fast-moving target. Thus, the learning rates of actors should be reduced accordingly.
On the other hand, if the critic has reached a plateau, increasing the learning rates of actors could
quickly improve the actors, which further generates new experiences to boost the critic’s learning.
These two processes alternate during training, promoting the overall learning. Further, by incor-
porating the second-order approximation, we additionally capture the pairwise interaction between
actors’ updates so as to more accurately update the learning rates of actors towards maximizing the
improvement of Q-value.
We evaluate AdaMa in four typical multi-agent cooperation scenarios, i.e., going together, coop-
erative navigation, predator-prey, and clustering. Empirical results demonstrate that dynamically
regulating the learning rates of actors and critic according to the contributions to the change of Q-
value could accelerate the learning and improve the performance, which can be further enhanced by
additionally considering the effect of pairwise actors’ updates. The visualizations of learning rates
during training clearly explain how and why AdaMa works.
2	Related Work
MARL. We consider the formulation of decentralized partially observable Markov decision process
(Dec-POMDP). There are N agents interacting with the environment. At each timestep t, each agent
i receives a local observation oit, takes an action ati , and gets a shared reward rt . The agents aim
to maximize the expected return E PtT=0 γtrt, where γ is a discount factor and T is the episode
time horizon. Many methods (Lowe et al., 2017; Foerster et al., 2018; Rashid et al., 2018; Iqbal &
Sha, 2019; Son et al., 2019) have been proposed for Dec-POMDP, which adopt centralized learning
and decentralized execution (CTDE). In many of these methods, a centralized critic learns a joint
Q-function by minimizing the TD-error. In training, the critic is allowed to use the information of
all agents. The actors, which only have access to local information, learn to maximize the Q-value
learned by the critic. In execution, the critic is abandoned and the actors act in a decentralized
manner.
Adaptive Learning Rate. Learning rate schedules aim to reduce the learning rate during training
according to a pre-defined schedule, including time-based decay, step decay, and exponential decay.
The schedules have to be defined in advance and depend heavily on the type of model and problem,
which requires much expert knowledge. Some optimizers, such as AdaGrad (Duchi et al., 2011),
AdaDelta (Zeiler, 2012), RMSprop (Tieleman & Hinton, 2012), and Adam (Kingma & Ba, 2015),
provide adaptive learning rate to ease manual tuning. AdaGrad performs larger updates for more
sparse parameters and smaller updates for less sparse parameters, and other methods are derived
from AdaGrad. However, these methods only deal with the gradient pattern for general optimization
problems, offering no specialized way to boost multi-agent learning. WoLF (Bowling & Veloso,
2002) provides variable learning rates for stochastic games, but not for cooperation.
Meta Gradients for Hyperparameters. Some meta-learning methods employ hyperparameter gra-
dients to tune the hyperparameter automatically. Maclaurin et al. (2015) utilized the reverse-mode
differentiation of hyperparameters to optimize step sizes, momentum schedules, weight initializa-
tion distributions, parameterized regularization schemes, and neural network architectures. Xu et al.
(2018) computed the meta-gradient to update the discount factor and bootstrapping parameter in
reinforcement learning. OL-AUX (Lin et al., 2019) uses the meta-gradient to automate the weights
of auxiliary tasks. The proposed AdaMa can also be viewed as a meta-gradient method for adaptive
learning rates in MARL.
3	Method
In this section, we first introduce the single-critic version of MADDPG (Lowe et al., 2017), on which
we instantiate AdaMa. However, AdaMa can also be instantiated on other MARL methods, and the
instantiation on MAAC (Iqbal & Sha, 2019) for discrete action space is also given in Appendix A.1.
Then, we use the Taylor approximation to evaluate the contributions of the critic and actors’ updates
to the change of Q-value. Based on the derived quantitative contributions, we dynamically adjust
the direction of the vector of actors’ learning rates and balance the learning rates between the critic
and actors. Further, we incorporate higher-order approximation to estimate the contributions more
accurately.
2
Under review as a conference paper at ICLR 2021
3.1	SINGLE-CRITIC MADDPG
In mixed cooperation and competition, each MADDPG agent TD error -―► Q(3,2)
learns an actor ∏ and a critic for the local reward. However,
since the agents share the reward in Dec-POMDP, We only
maintain a single shared critic, which takes the observation	Critic
vector ~ and the action vector ~ and outputs the Q-value, as
illustrated in Figure 1. The critic parameterized by φ is trained	→
by minimizing the TD-error δ	MGZ) ∂a1
∂aι ∂θ^
2	~ o
E(~,a,r,~0)〜D [(Q(o,a) - y)2] , where y = r+γQ (o0,π- (oi)).
Q- is the target critic, ∏- is the target actor, and D is replay
buffer. Each actor ∏ (parameterized by θ%) is updated to max-
imize the learned Q-value by gradient ascent. The gradient of
ACtorl	Actor N
Ol
on
θi is
∂Q(~,~) ∂ai
∂ai 函
Figure 1: Single-Critic MADDPG
°N
We denote the learning rates of each actor i and the critic as l^ and lc respectively.
3.2	ADAPTIVE la DIRECTION
First, suppose that the critic is trained and frozen, and We only update the actors. By expanding
the Q-function, we can estimate the gain of Q-value contributed by actors, updates by the Taylor
approximation:
∆Q = Q(~,~ +∆~) - Q(~,~)
≈ Q(~, ~) + X∆a aQ(o, a) - Q(~, ~)
i=1
N
X[πi(θi + lai
i=1
N
≈	lai
i=1
N
= X lai
i=1
∂Q(~,~)	∂Q(o,~) T
-∂θ-- )-πi(θi)]
∂Q(~,~) ∂aiτ ∂Q(~,~) T
∂θi	∂θi	∂ai
∂Q(o,~) ∂Q(o,~) T
∂θi
∂θi
~ ∂Q∂QT
a ^丽丽.
..→..
→
Assuming the magnitude of the learning rate vector kl~a k is a fixed small constant kl~a k, the largest
~T
∆Q is obtained when the direction of ~ is consistent with the direction of vector d∂Q 零 .Thus, we
~T
can softly update ~ to the direction of 需焉 to improve the Q-value:
~	~	而 ∂Q∂Q T dQdQT
la = α~ + (1 - α)klak丽丽 /k丽丽 k
..-* ..
~	~ k~k
la = la 而
(1)
d
where the second line normalizes the magnitude of l~a to kl~ak, and α is a parameter that controls the
soft update. From another perspective, the update rule (1) can be seen as updating l~a by gradient
~T
ascent to increase the Q-value the most, since ^Q =焉焉 .
3
Under review as a conference paper at ICLR 2021
ʌ ʌ	,	∙,	I I」I I
3.3	ADAPTIVE lc AND klak
In the previous section, we assume that the critic is frozen. However, in MADDPG and other MARL
methods, the critic and actors are trained simultaneously. Therefore, we investigate the change of
Q-value by additionally considering the critic’s update:
∆Q = Q(φ + ∆φ, ~o, ~a + ∆~a) - Q(φ, ~o, ~a)
≈ Q(Φ, ~, ~) + X ∆ad∞T + ∆φdQ⅛~^T -Q(φ, ~ ~)
i=1	∂ai	∂φ
. TE	一 一 F
〜~ ∂Q∂Q T ∂δ∂Q T
≈ a • 丽丽 - lc∂φ∂φ .
We can see that ∆Q is contributed by the updates of both the critic and actors. In principle, the
critic’s learning is prioritized since the actor’s learning is determined by the improved critic. When
the critic’s update causes a large change of the Q-value, the critic is leading the learning, and we
should assign it a high learning rate. However, the optimization of actors, which relies on the cur-
rent critic, would struggle with the fast-moving target. Therefore, the actors’ learning rates should
be reduced. On the other hand, when the critic has reached a plateau, increasing the actors’ learning
rates could quickly optimize the actors, which further injects new experiences into the replay buffer
to boost the critic’s learning, thus promoting the overall learning. The contributions of actors’ up-
dates are always nonnegative, but the critic’s update might either increase or decrease the Q-value.
Therefore We use the absolute value | 瑞端 | to evaluate the contribution of critic to the change of
Q-value. Based on the principles above, We adaptively adjust lc and klak by the update rules:
lc = αlc + (I - α)l • CIiP(I =/I |/m, 3 1 - C)
∂φ ∂φ
kl~a k = l - lc .
(2)
The hyPerParameters α, m, l, and C have intuitive interPretations and are easy to tune. α controls the
soft uPdate and m controls the target value of lc. The cliP function and the small constant C Prevent
the learning rate being too large or too small. Therefore, AdaMa Works as folloWs: first uPdate lc
and get klak using (2), then regulate the direction and magnitude of la according to (1).
As Liessner et al. (2019) Pointed out, the actor should have a loWer learning rate than the critic, and a
high learning rate of actor leads to a Performance breakdoWn. Also, emPirically, in DDPG (LillicraP
et al., 2016) the critic’s learning rate is set to 10 times higher than the actor’s learning rate. HoWever,
We believe such a setting only Partially addresses the Problem. During training, if the learning rates
of actor are alWays loW, actors learn sloWly and thus the learning is limited. Therefore, AdaMa
decreases lc and increases kla k When the learning of critic reaches a Plateau, Which could avoid the
fast-moving target and sPeed uP the learning.
3.4	Second-Order Approximation
Under the first-order Taylor aPProximation, the actor i’s contribution to ∆Q is only related to the
change of ai , Without caPturing the joint effect With other agents’ uPdates. HoWever, When there
are strong correlations betWeen agents, the increase of the Q-value cannot be sufficiently estimated
as the sum of individual contributions of each actor’ uPdate, Which instead is a result of the joint
uPdate. To estimate the actors’ contributions more Precisely, We extend AdaMa to the second-order
Taylor aPProximation to take PairWise agents’ uPdates into account:
∆Q = Q(~o, ~a + ∆~a) - Q(~o, ~a)
N	∂Q(~o,~a)T	1 N	∂2Q(~o,~a)	T
≈ Q(o,a) + ∑.∆ai	+2^4^ ∂ai∂aj ∆ɑj - QMa)
i=1	i,j=1
〜X l ∂Q(o,a) ∂Q(o,~)
≈2γ ai -∂θ	∂θ~
i=1
T 1 XX	∂Q(~, ~) ∂ai T ∂2Q(~, ~) ∂aj ∂Q(~, ~) T
+ 2	ai aj ∂θi 函 ∂ai∂aj 西 ∂θj
4
Under review as a conference paper at ICLR 2021
Figure 2: Illustration of experimental scenarios: going together, cooperative navigation, predator-
prey, and clustering (from left to right).
As the actors are updated by the first-order gradient, We still estimate ∆~ utilizing the first-order
approximation and compute the second-order ∆Q on the first-order ∆~a. Then, the gradient of lai
i ∂∆Q	_ ∂Q∂Qt . 1 PN I ∂Q∂aiT ∂2Q	∂aj	∂Q T . 1 PN 】∂Q	∂aj T ∂2Q ∂ai	∂Q T
is ∂lai	= ∂θi ∂θi + 2 乙j=1 laj ∂θi ∂θi ∂ai∂aj	∂θj	∂θj + 2 乙j = 1 laj ∂θj	∂θj ∂a,∂a, ∂θ,	∂θ,	.
Similarly, la can be updated as:
_ _	..—> ..
~ 一 ~，门 、方I ∂∆Q	∂∆Q	~	~ klak	⑶
la = αla + (I - α)klak	T /∣ T k,	la = la	-	(3)
∂la	∂la	klak
4 Experiments
We validate AdaMa in four cooperation scenarios With continuous observation space and continu-
ous action space, Which are illustrated in Figure 2. In these scenarios, agents observe the relative
positions of other agents, landmarks, and other items, and take tWo-dimensional actions ∈ [-1, 1]
as physical velocity.
•	Going Together. In the scenario, there are 2 agents and 1 landmark. The reWard is -0.5(di+
dj ) - dij , Where di is the distance from agent i to the landmark, and dij is the distance
betWeen the tWo agents. The agents have to go to the landmark together, avoiding moving
aWay from each other.
•	Cooperative Navigation. In the scenario, there are 4 agents and 4 corresponding landmarks.
The reWard is -maxi(di), Where di is the distance from agent i to the landmark i. The
sloWest agent determines the reWard in this scenario.
•	Predator-Prey. In the scenario, 4 sloWer agents learn to chase a faster rule-based prey. Each
time one of the agents collide With the prey, the agents get a reWard +1.
•	Clustering. In the scenario, 8 agents learn to cluster together. The reWard is - P di, Where
di is the distance from agent i to the center of agents’ positions. Since the center is changing
along With the agents’ movements, there are strong interactions betWeen agents.
To investigate the effectiveness of AdaMa and for ablation, We evaluate the folloWing methods:
•	AdaMa adjusts lc and kla k using (2), and la according to (1).
•	Fixed lr uses grid search to find the optimal combination of lc and klak from 0.01 to 0.001
with step 0.001. The learning rate of each agent is set to ∣∣lak∕√N.
•	Adaptive la direction sets lc and ∣la ∣ as that in Fixed lr and only adjusts the direction
of la using (1). Additionally, Adaptive la direction (2nd) uses the update rule (3) for the
second-order approximation.
•	Adaptive lc and ∣~∣∣ adjusts lc and ∣∣lak using (2) and sets lai = klak∕√N.
•	AdaGrad is an adaptive learning rate optimizer that performs larger updates for more
sparse parameters and smaller updates for less sparse parameter. The initial learning rates
are sets as that in Fixed lr.
Except AdaGrad, all other methods use SGD optimizer without momentum. More details about
experimental settings and hyperparameters are available in Appendix A.3. We trained all the models
for five runs with different random seeds. All the learning curves are plotted using mean and standard
deviation.
5
Under review as a conference paper at ICLR 2021
1 × 104	2 × 104
Episodes
3 × 104
0	0.5 × 104	1 × 104	1.5 × 104	2 × 104
Episodes
(b) cooperative navigation
(a) going together
5 O 5 0
2 2 LL
p-eM9工 Ue9≡
(c) predator-prey
0.0
0	1 × 104	2 × 104	3 × 104
Episodes
4 X 104	5 X 104
P-JeMəm ueə2
(d) clustering
Figure 3: Learning curves in the four scenarios.
2 × 104	4 × 104	6 × 104	8 × 104
Episodes
4.1	PERFORMANCE OF ADAPTIVE la DIRECTION
As shown in Figure 3(a) and 3(c), Adaptive l~a direction converges to a higher reward than Fixed
lr that treats each agent as equally important. To make an explicit explanation, we visualize the
normalized actors’ learning rates la /kla k in Figure 4 for one run and more results are available in
Appendix A.4. In going together and predator-prey, the actors’ learning rates fluctuate dynamically
and alternately as depicted in Figure 4(a) and 4(b). An actor has a much higher learning rate than
other actors in different periods, meaning that the actor is critical to the learning. The direction of
la is adaptive to the changing contributions during the learning, assigning higher learning rates to
the actors that make more contributions to ∆Q. In clustering, the center is determined by all agents’
positions, and the actors’ updates make similar contributions to ∆Q, leading to similar learning rates
for the actors. That is the reason Adaptive l~a direction is not beneficial in this scenario. Moreover,
(a) going together
(b) predator-prey
(c) going together (2nd)
(d) cooperative navigation (2nd)
Figure 4: Normalized actors’ learning rates.
6
Under review as a conference paper at ICLR 2021
(a) going together
(b) cooperative navigation
(c) predator-prey
(d) clustering
TΓ-1∙	L 7	F II 7^→ II
Figure 5: lc and kla k.
in single-critic MADDPG, the gradient of an actor depends on the current policies of other actors.
If other actors are updating at a similar rate, the update of this actor will become unstable, since
the changes of others’ policies are invisible and unpredictable. In our method, the agents critical to
increasing the Q-value learn fast while other agents have low learning rates, which partly attenuates
the instability.
Λ ʌ …	*	7	1 1」ɪ ɪ
4.2	PERFORMANCE OF ADAPTIVE lc AND klak
As illustrated in Figure 3(b), 3(c), and 3(d), Adaptive lc and kla k learns faster than Fixed lr. To
interpret the results, we plot lc and kla k during the training in Figure 5 and find that lc and kla k
rise and fall alternately and periodically. When the update of the critic impacts greatly on ∆Q, e.g.,
at the beginning with large TD-error, the fast-moving Q-value, which is the optimization target of
actors, might cause a performance breakdown if the actors are also learning fast. In this situation, our
method could adaptively speed up the learning of the critic and slow down the learning of actors for
stability. After a while, the TD-error becomes small and makes the critic reach a plateau. According
to the update rules (2), the learning of actors is accelerated whilst the learning rate of the critic falls,
which keeps the target of actors stable and thus avoids the breakdown. The fast-improving actors
generate new experiences, which change the distribution in the replay buffer and increase the TD-
error. As a consequence, the learning rate of the critic rises again. Therefore, the learning rates of the
critic and actors fluctuate alternately, promoting the overall learning continuously. In going together,
the alternate fluctuation is not obvious, so Adaptive lc and kla k performs worse than Fixed lr with
grid search.
Combined with the two adaptive mechanisms, AdaMa learns faster and converges to a higher reward
than all other baselines in Figure 3(c). In other scenarios, AdaMa produces similar results to the
0	1 × 104	2 × 104	3 × 104
Episodes
(a) going together
0	0.5 × 104	1 × 104	1.5 × 104	2 × 104
Episodes
(b) cooperative navigation
Figure 6: Learning curves with the second-order approximation.
7
Under review as a conference paper at ICLR 2021
⑸ CUrVeOf | dδdφτ |
FigUre 7: Learning CUrVes and VisUalizations with different m in Predator-Prey.
meChanism that brings the main improVement. SinCe Fixed lr has to searCh 100 Combinations,
the Cost is prohibitiVe. Despite adaptiVely adjUsting the learning rates, AdaGrad does not show
CompetitiVe performanCe, sinCe it only foCUses on the gradient pattern, ignoring the CharaCteristiCs
of MARL.
4.3	Performance of Second-Order Approximation
In FigUre 3, the performanCe gain of Adaptive l~a direction is limited, whiCh we think is attribUted
to that the first-order approximation is relatiVely roUgh when an aCtor’s Update affeCts other aCtors’
Updates. We apply the seCond-order approximation to Adaptive l~a direction (2nd) and find that it
aChieVes better resUlts as shown in FigUre 6. Comparing the later episodes in FigUre 4(a) and 4(C),
there is a larger gap between the two aCtors’ learning rates Under the seCond-order approximation.
This aCCUrately refleCts the later training is dominated by one aCtor, whiCh is the reason for the
higher reward. In FigUre 4(d), there are obVioUs Ups and downs in the learning rates of aCtors before
ConVergenCe (1 × 104 episodes), and after that the flUCtUation beComes gentle. The seCond-order
approximation that CaptUres the pairwise effeCt of agents’ Updates on ∆Q obtains a more aCCUrate
Update on the learning rates, whiCh eVentUally leads to better performanCe.
4.4	TUNING HYPERPARAMETER m
The hyperparameter m Controls the target ValUe of the CritiC’s learning rate. If m is too large or too
small, the learning rate will reaCh the boUndary ValUe l or (1 - )l, whiCh destroys the adaptability
and hampers the learning proCess. An empiriCal approaCh for tUning is setting m to be the mean
| ∂φ∂∂φ | of the first K updates in a trial run. In predator-prey, we test m = 40,50,60, among
whiCh 50 is the roUnding mean ValUe of 100 Updates, and plot the resUlts in FigUre 7(a). The three
settings show similar performanCe, reVealing our method is robust to the hyperparameter m. HaVing
noticed that lc and k~k change violently in Figure 5, We visualize |籍翁 | during the training in
Figure 7(b) to interpret the robustness. Since most of the time | ∂δ∂φT∣ is higher than 60 or lower
than 40, similar learning rate patterns is observed when m is between 40 and 60, which verifies that
there is high fault tolerance in m. Although Adaptive lc and kla k converges to a similar reward
with Fixed lr, the former learns faster and is much easier to tune.
5 Conclusion
In this paper, we proposed AdaMa for adaptive learning rates in MARL. AdaMa adaptively updates
the vector of learning rates of actors to the direction of maximally improving the Q-value. It also
dynamically balances the learning rates between the critic and actors during learning. Moreover,
AdaMa can incorporates the higher-order approximation to more accurately update the learning
rates of actors. Empirically, we show that AdaMa could accelerate the learning and improve the
performance in a variety of multi-agent scenarios.
8
Under review as a conference paper at ICLR 2021
References
Michael Bowling and Manuela Veloso. Multiagent learning using a variable learning rate. Artificial
Intelligence,136(2):215-250, 2002.
John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochastic optimization. Journal of machine learning research, 12(7), 2011.
Jakob N Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon White-
son. Counterfactual multi-agent policy gradients. In AAAI Conference on Artificial Intelligence
(AAAI), 2018.
Shariq Iqbal and Fei Sha. Actor-attention-critic for multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations (ICLR), 2015.
Roman Liessner, Jakob Schmitt, Ansgar Dietermann, and Bernard Baker. Hyperparameter optimiza-
tion for deep reinforcement learning in vehicle energy management. In International Conference
on Agents and Artificial Intelligence (ICAART), 2019.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Inter-
national Conference on Learning Representations (ICLR), 2016.
Xingyu Lin, Harjatin Baweja, George Kantor, and David Held. Adaptive auxiliary task weighting
for reinforcement learning. In Advances in Neural Information Processing Systems (NeurIPS),
2019.
Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-agent
actor-critic for mixed cooperative-competitive environments. In Advances in Neural Information
Processing Systems (NeurIPS), 2017.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning (ICML), 2015.
Tabish Rashid, Mikayel Samvelyan, Christian Schroeder De Witt, Gregory Farquhar, Jakob Foer-
ster, and Shimon Whiteson. Qmix: monotonic value function factorisation for deep multi-agent
reinforcement learning. In International Conference on Machine Learning (ICML), 2018.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learning for autonomous driving. arXiv preprint arXiv:1610.03295, 2016.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. Qtran: Learning
to factorize with transformation for cooperative multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning (ICML), 2019.
Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5-rmsprop: Divide the gradient by a running
average of its recent magnitude. COURSERA: Neural networks for machine learning, 4(2):26-
31, 2012.
Hua Wei, Nan Xu, Huichu Zhang, Guanjie Zheng, Xinshi Zang, Chacha Chen, Weinan Zhang,
Yanmin Zhu, Kai Xu, and Zhenhui Li. Colight: Learning network-level cooperation for traffic
signal control. In International Conference on Information and Knowledge Management (CIKM),
2019.
Zhongwen Xu, Hado P van Hasselt, and David Silver. Meta-gradient reinforcement learning. In
Advances in neural information processing systems (NeurIPS), 2018.
Yaodong Yang, Jianye Hao, Mingyang Sun, Zan Wang, Changjie Fan, and Goran Strbac. Recurrent
deep multiagent q-learning for autonomous brokers in smart grid. In International Joint Confer-
ence on Artificial Intelligence (IJCAI), 2018.
Matthew D Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701,
2012.
9
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Instantiating AdaMa on MAAC
AdaMa could be applied to other multi-agent actor-critic methods, modified according to the gradi-
ent computation. We present how to instantiate AdaMa on MAAC (Iqbal & Sha, 2019). In discrete
action space, ∆ ~a does not meet the assumption of the Taylor approximation. But in practice, it is
feasible to learn the value function Q(φ, ~o, ~π), taking as input the action distribution πi instead of the
action ai. Since the update of the actor is different from that in MADDPG, the change of Q-value is
written as
∆Q = Q(φ + ∆φ, ~o, ~π + ∆~π) - Q(φ, ~o, ~π)
≈ Q(Φ, ~, ~) + XX ∆∏i皆产T + ∆φdQ*T -Q(Φ, ~, ~)
XN	d log πiAi	9Q(φ,o,~) T MdQ (φ,~,~) T
⅛tπi(θi + lai ~^θ~ U	∂∏	+年 ∂φ
N	∂ log πiAi ∂πiT ≈ J ai	∂θi	∂θi i=1	dQ(φ,~,~) T 一 ∂∏	l ∂δ ∂Q(φ, ~, ~)T C ∂φ	∂φ一
_ XX l ∂ log ∏iAi ∂Q T =J ai —∂θi — ∂θi i=1	I ∂δ∂Q T	
	- lc∂φ∂φ .	
Ai is the advantage function of agent i, proposed in MAAC. ReWritting the gradient of 1@石 as d∂=Q
d lodUAi ∂QT, the AdaMa implementation on MAAC is the same as that on MADDPG.
A.2 AdaMa Algorithm
For completeness, We provide the AdaMa algorithm on MADDPG beloW.
Algorithm 1 AdaMa on MADDPG
1:	Initialize critic network φ, actor networks θi, and target networks
Initialize the learning rates lc and la
Initialize replay buffer D
2:	for episode = 1, . . . , M do
3:	for t = 1, . . . , T do
4:	Select action ait = πi(oit) + Nti for each agent i
5:	Execute action ati, obtain reward rt, and get new observation oit+1 for each agent i
6:	Store transition (~ot, ~at, rt, ~ot+1) in D
7:	end for
8:	Sample a random minibatch of transitions from D
AdjUSt lc and ∣∣lak by lc = αlc + (I - α)l ∙ CIiP(Iddf 翁 ∖/m, e, 1 -。, ∣∣~∣∣ = l - lc
d
AdjUSt la by la = αla + (1 - a) IllaIl -∣~^/k ~~~^ k, ~ = laj~⅛
la	la	kla k
Update the critic φ by φ = φ 一 lc⅞δ, where δ is the TD-error
Update the actor θi by θi = θi + l,a. dQ(:,a)翁 for each agent
ai	i
Update the target networks
9:	end for
A.3 Experimental Settings and Hyperparameters
In each task, the experimental settings and hyperparameters are sUmmarized in Table 1. Initially, we
set lc = k~∣∣ and laτ = ∣∣~k∕√N in AdaMa. For exploration, we add random noise to the action
10
Under review as a conference paper at ICLR 2021
(1 - ε)ai +εη, where the uniform distribution η ∈ [-1, 1]. We anneal ε linearly from 1.0 to 0.1 over
104 episodes and keep it constant for the rest of the learning. We update the model every episode
and update the target networks every 20 episodes.
Table 1: Hyperparameters
Hyperparameter	Going Together	Cooperative Navigation	Predator-Prey	Clustering
horizon (T)	10	6	20	10
discount (γ)	0.96	0.9	0.97	0.95
replay buffer size	5 × 105	5 × 105	1 × 106	1 × 106
lc (grid search)	8 × 10-3	9 × 10-3	7 × 10-3	8 × 10-3
kl~a k (grid search)	3 × 10-3	2 × 10-3	2 × 10-3	1 × 10-3
batch size		1024		
MLP units		(64, 64)		
MLP activation		ReLU		
m	10	5	50	80
α		0.99		
l		1 × 10-2		
		0.1		
A.4 Additional Results
Here, we show the visualizations in the other four runs. Similar curve patterns are observed in all
runs of each scenario.
Figure 8: Normalized actors’ learning rates in going together
11
Under review as a conference paper at ICLR 2021
Figure 9: Normalized actors’ learning rates in predator-prey
Figure 10: Normalized actors’ learning rates in going together (2nd)
Figure 11: Normalized actors’ learning rates in cooperative navigation (2nd)
12
Under review as a conference paper at ICLR 2021
TΓ-1∙	<A7	1 I I 7→ I I ∙	∙	,	,1
Figure 12:	lc and ∣∣∕α ∣∣ in going together.
τr-ι∙	YC 7	1 11 7→ 11 ∙	, ∙	∙	, ∙
Figure 13:	lc and ∣∣∕α ∣∣ in cooperative navigation.
TΓ-1∙	t A 7	F II 7^→ II ♦	1	,
Figure 14:	lc and ∣∣lα∣ in predator-prey.
13
Under review as a conference paper at ICLR 2021
Figure 15: lc and ∖∖la∖∖ in clustering.
ll⅛l
14