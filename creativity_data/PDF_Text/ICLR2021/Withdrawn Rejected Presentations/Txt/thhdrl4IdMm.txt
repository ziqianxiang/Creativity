Under review as a conference paper at ICLR 2021
A Chain Graph Interpretation
of Real-World Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The last decade has witnessed a boom of deep learning research and applications
achieving state-of-the-art results in various domains. However, most advances
have been established empirically, and their theoretical analysis remains lacking.
One major issue is that our current interpretation of neural networks (NNs) as
function approximators is too generic to support in-depth analysis. In this pa-
per, we remedy this by proposing an alternative interpretation that identifies NNs
as chain graphs (CGs) and feed-forward as an approximate inference procedure.
The CG interpretation specifies the nature of each NN component within the
rich theoretical framework of probabilistic graphical models, while at the same
time remains general enough to cover real-world NNs with arbitrary depth, multi-
branching and varied activations, as well as common structures including convo-
lution / recurrent layers, residual block and dropout. We demonstrate with con-
crete examples that the CG interpretation can provide novel theoretical support
and insights for various NN techniques, as well as derive new deep learning ap-
proaches such as the concept of partially collapsed feed-forward inference. It is
thus a promising framework that deepens our understanding of neural networks
and provides a coherent theoretical formulation for future deep learning research.
1	Introduction
During the last decade, deep learning (Goodfellow et al., 2016), the study of neural networks (NNs),
has achieved ground-breaking results in diverse areas such as computer vision (Krizhevsky et al.,
2012; He et al., 2016; Long et al., 2015; Chen et al., 2018), natural language processing (Hinton
et al., 2012; Vaswani et al., 2017; Devlin et al., 2019), generative modeling (Kingma & Welling,
2014; Goodfellow et al., 2014) and reinforcement learning (Mnih et al., 2015; Silver et al., 2016),
and various network designs have been proposed. However, neural networks have been treated
largely as “black-box” function approximators, and their designs have chiefly been found via trial-
and-error, with little or no theoretical justification. A major cause that hinders the theoretical anal-
ysis is the current overly generic modeling of neural networks as function approximators: simply
interpreting a neural network as a composition of parametrized functions provides little insight to
decipher the nature of its components or its behavior during the learning process.
In this paper, we show that a neural network can actually be interpreted as a probabilistic graphical
model (PGM) called chain graph (CG) (Koller & Friedman, 2009), and feed-forward as an efficient
approximate probabilistic inference on it. This offers specific interpretations for various neural
network components, allowing for in-depth theoretical analysis and derivation of new approaches.
1.1	Related work
In terms of theoretical understanding of neural networks, a well known result based on the function
approximator view is the universal approximation theorem (Goodfellow et al., 2016), however it only
establishes the representational power of NNs. Also, there have been many efforts on alternative NN
interpretations. One prominent approach identifies infinite width NNs as Gaussian processes (Neal,
1996; Lee et al., 2018), enabling kernel method analysis (Jacot et al., 2018). Other works also
employ theories such as optimal transport (Genevay et al., 2017; Chizat & Bach, 2018) or mean
field (Mei et al., 2019). These approaches lead to interesting findings, however they tend to only
hold under limited or unrealistic settings and have difficulties interpreting practical real-world NNs.
1
Under review as a conference paper at ICLR 2021
Figure 1: Neural networks can be interpreted as layered chain graphs where activation functions are
determined by node distributions. Left: An example neural network interpreted as a chain graph
with three chain components which represent its layers; Right: A variety of activation functions
(softplus, ReLU, leaky ReLU) approximated by nodes following rectified Gaussian distributions
(e, q as in Eq. (7)). We visualize the approximations stochastically by averaging over 200 samples.
Alternatively, some existing works study the post-hoc interpretability (Lipton, 2018), proposing
methods to analyze the empirical behavior of trained neural networks: activation maximization
(Erhan et al., 2009), typical input synthesis (Nguyen et al., 2016), deconvolution (Zeiler & Fergus,
2014), layer-wise relevance propagation (Bach et al., 2015), etc. These methods can offer valuable
insights to the practical behavior of neural networks, however they represent distinct approaches and
focuses, and are all limited within the function approximator view.
Our work links neural networks to probabilistic graphical models (Koller & Friedman, 2009), a rich
theoretical framework that models and visualizes probabilistic systems composed of random vari-
ables (RVs) and their interdependencies. There are several types of graphical models. The chain
graph model (also referred to as the LWF chain graph model) (Koller & Friedman, 2009; Lauritzen
& Wermuth, 1989; Frydenberg, 1990) used in our work is a general form that unites directed and
undirected variants, visualized as a partially directed acyclic graph (PDAG). Interestingly, there
exists a series of works on constructing hierarchical graphical models for data-driven learning prob-
lems, such as sigmoid belief network (Neal, 1992), deep belief network (Hinton et al., 2006), deep
Boltzmann machine (Salakhutdinov & Hinton, 2012) and sum product network (Poon & Domingos,
2011). As alternatives to neural networks, these models have shown promising potentials for gen-
erative modeling and unsupervised learning. Nevertheless, they are yet to demonstrate competitive
performances over neural network for discriminative learning.
Neural networks and graphical models have so far been treated as two distinct approaches in general.
Existing works that combine them (Zheng et al., 2015; Chen et al., 2018; Lample et al., 2016) mainly
treat either neural networks as function approximators for amortized inference, or graphical models
as post-processing steps. Tang & Salakhutdinov (2013) create a hybrid model, the stochastic feed-
forward neural network (SFNN), by concatenating deterministic neurons with stochastic Bernoulli
random variables, in order to represent multimodal distributions. Some also consider neural net-
works as graphical models with deterministic hidden nodes (Buntine, 1994). However this is an
atypical degenerate regime. To the best of our knowledge, our work provides the first rigorous and
comprehensive formulation ofa (non-degenerate) graphical model interpretation for neural networks
in practical use.
1.2	Our contributions
The main contributions of our work are summarized as follows:
•	We propose a layered chain graph representation of neural networks, interpret feed-forward as
an approximate probabilistic inference procedure, and show that this interpretation provides an
extensive coverage of practical NN components (Section 2);
2
Under review as a conference paper at ICLR 2021
•	To illustrate its advantages, we show with concrete examples (residual block, RNN, dropout) that
the chain graph interpretation enables coherent and in-depth theoretical support, and provides
additional insights to various empirically established network structures (Section 3);
•	Furthermore, we demonstrate the potential of the chain graph interpretation for discovering new
approaches by using it to derive a novel stochastic inference method named partially collapsed
feed-forward, and establish experimentally its empirical effectiveness (Section 4).
2	Chain graph interpretation of neural networks
Without further delay, we derive the chain graph interpretation of neural networks in this section.
We will state and discuss the main results here and leave the proofs in the appendix.
2.1	The layered chain graph representation
We start by formulating the so called layered chain graph that corresponds to neural networks we use
in practice: Consider a system represented by L layers of random variables (X1, . . . , XL), where
Xil is the i-th variable node in the l-th layer, and denote Nl the number of nodes in layer l. We
assume that nodes Xil in the same layer l have the same distribution type characterized by a feature
function Tl that can be multidimensional. Also, we assume that the layers are ordered topologically
and denote Pa(Xl) the parent layers of Xl. To ease our discussion, we assume that X1 is the input
layer and XL the output layer (our formulation can easily extend to multi-input/output cases). A
layered chain graph is then defined as follows:
Definition 1. A layered chain graph that involves L layers of random variables (X1, . . . , XL) is a
chain graph that encodes the overall distribution P(X2, . . . , XL|X1) such that:
1.	It can be factored into layerwise chain components P(Xl|Pa(Xl)) following the topological or-
der, and nodes Xil within each chain component P(Xl|Pa(Xl)) are conditionally independent
given their parents (this results in bipartite chain components), thus allowing for further decom-
position into nodewise conditional distributions P(Xil|Pa(Xl)) . This means we have
L	L Nl
P(X2,...,XL|X1) =YP(Xl|Pa(Xl)) =YYP(Xil|Pa(Xl));	(1)
l=2	l=2 i=1
2.	For each layer l with parent layers Pa(Xl) = {Xp1 , . . . Xpn},p1, . . . ,pn ∈ {1, . . . , l - 1}, its
nodewise conditional distributions P (Xil |Pa(Xl)) are modeled by pairwise conditional random
fields (CRFs) with with unary (bli) and pairwise (Wjp,,il) weights (as we will see, they actually
correspond to biases and weights in NN layers):
P(Xi ∖Pa(Xl) = fl(Tl(Xi), ei(Tpι(Xp1),…，Tpn(Xpn)))	(2)
pn Np
With ei(Tp1(Xp1),..., Tpn (Xpn)) = bli + X X Wjp,,ilTp(Xjp).	(3)
p=p1 j=1
Figure 1 Left illustrates an example three-layer netWork as layered chain graph and its chain com-
ponent factorization. In Eq. (2), fl is an arbitrary function that represents a probability distri-
bution. For exponential family distributions (Koller & Friedman, 2009), Eq. (2) simply becomes
P (χi∣Pa(Xl)) (X exp (TI(Xj) ∙ eli (TpI(XpI),..., Tpn(Xpn))).
Note that layered chain graph has a globally directed graph structure and has an equivalent modeling
based on directed graphical model (Bayesian netWork) (Koller & Friedman, 2009), We elaborate on
this point for interested readers in Appendix A.
2.2	Feed-forward as approximate probabilistic inference
To identify layered chain graphs With real-World neural netWorks, We need to shoW that they can
behave the same Way during inference and learning. For this, We establish the fact that feed-forWard
can actually be seen as performing an approximate probabilistic inference on a layered chain graph:
3
Under review as a conference paper at ICLR 2021
Given an input sample x1, We consider the problem of inferring the marginal distribution Qi of a
node Xil and its expected features qli , defined as
Qi(χi∣χ1) = P(Xi = χi∣χ1 = χ1); q = EQiT(Xi)] (q1 = χ1).	⑷
Consider a non-input layer l With parent layers p1 , . . . , pn, the independence assumptions encoded
by the layered chain graph lead to the folloWing recursive expression for marginal distributions Q:
Qi(Xi∣χ1) = EQp1 ,...,Qpn [P(xli|Pa(Xl))].	(5)
HoWever, the above expression is in general intractable, as it integrates over the entire admissible
states of all parents nodes in Pa(Xl). To proceed further, simplifying approximations are needed.
Interestingly, by using linear approximations, We can obtain the folloWing results (in case of discrete
random variable the integration in Eq. 7 is replaced by summation):
Proposition 1. If we make the assumptions that the corresponding expressions are approximately
linear w.r.t. parent features Tp1 (Xp1 ), . . . , Tpn (Xpn), we obtain the following approximations:
Qi (xi∣x1) ≈ fl (Tl (xi), ei(qp1,..., qpn ))；
qi ≈ T Tl(Xlif(Tl (Xi), ei (qp1,…,qpn ))dxi, := gl (ei(qp1,..., qpn)).
xli
(6)
(7)
Especially, Eq. (7) is a feed-forward expression for expected features qli with activation function gl
determined by Tl and fl, i.e. the distribution type of random variable nodes in layer l.
The proof is provided in Appendix B.1. This alloWs us to identify feed-forWard as an approximate
probabilistic inference procedure for layered chain graphs. For learning, the loss function is typically
a function of (QL , qL ) obtainable via feed-forWard, and We can folloW the same classical neural
netWork parameter update using stochastic gradient descent and backpropagation. Thus We are able
to replicate the exact neural netWork training process With this layered chain graph frameWork.
The folloWing corollary provides concrete examples of some common activation functions g (We
emphasize their names in bold, detailed formulations and proofs are given in Appendix B.2):
Corollary 2. We have the following node distribution - activation function correspondences:
1.	Binary nodes taking values {α, β} results in sigmoidal activations, especially, we obtain sigmoid
with α = 0, β = 1 and tanh with α = -1, β = 1 (α, β are interchangeable);
2.	Multilabel nodes characterized by label indicator features result in the softmax activation;
3.	Variants of (leaky) rectified Gaussian distributions (Ti(Xi) = Xi = max(eYil, Yil) with Yil 〜
N eli, (sli(eli))2 ) can approximate activations such as softplus ( = 0, sli ≈ 1.7761) and -leaky
rectified linear unit (ReLU) (sli = tanh(eli)) including ReLU ( = 0) and identity ( = 1).
Figure 1 Right illustrates activation functions approximated by various rectified Gaussian variants.
We also plotted (in orange) an alternative approximation of ReLU With sigmoid-modulated standard
deviation proposed by Nair & Hinton (2010) Which is less accurate around the kink at the origin.
The linear approximations, needed for feed-forWard, is coarse and only accurate for small pairWise
Weights (kWk	1) or already linear regions. This might justify Weight decay beyond the general
“anti-overfit” argument and the empirical superiority of pieceWise linear activations like ReLU (Nair
& Hinton, 2010). Conversely, as a source of error, it might explain some “failure cases” of neural
netWorks such as their vulnerability against adversarial samples, see e.g., GoodfelloW et al. (2015).
2.3 Generality of the chain graph interpretation
The chain graph interpretation formulated in Sections 2.1 and 2.2 is a general frameWork that can
describe many practical netWork structures. To demonstrate this, We list here a Wide range of neural
netWork designs (marked in bold) that are chain graph interpretable.
•	In terms of netWork architecture, it is clear that the chain graph interpretation can model net-
Works of arbitrary depth, and With general multi-branched structures such as inception modules
(Szegedy et al., 2015) or residual blocks (He et al., 2016; He et al., 2016) discussed in Sec-
tion 3.1. Also, it is possible to built up recurrent neural networks (RNNs) for sequential data
4
Under review as a conference paper at ICLR 2021
learning, as we will see in Section 3.2. Furthermore, the modularity of chain components justifies
transfer learning via partial reuse of pre-trained networks, e.g., backbones trained for image
classification can be reused for segmentation (Chen et al., 2018).
•	In terms of layer structure, we are free to employ sparse connection patterns and shared/fixed
weight, so that we can obtain not only dense connections, but also connections like convolu-
tion, average pooling or skip connections. Moreover, as shown in Section 3.3, dropout can
be reproduced by introducing and sampling from auxiliary random variables, and normalization
layers like batch normalization (Ioffe & Szegedy, 2015) can be seen as reparametrizations of
node distributions and fall within the general form (Eq. (2)). Finally, we can extend the layered
chain graph model to allow for intra-layer connections, which enables non-bipartite CRF layers
which are typically used on output layers for structured prediction tasks like image segmenta-
tion (Zheng et al., 2015; Chen et al., 2018) or named entity recognition (Lample et al., 2016).
However, feed-forward is no longer applicable through these intra-connected layers.
•	Node distributions can be chosen freely, leading to a variety of nonlinearities (e.g., Corollary 2).
3	Selected case studies of existing neural network designs
The proposed chain graph interpretation offers a detailed description of the underlying mechanism
of neural networks. This allows us to obtain novel theoretical support and insights for various net-
work designs which are consistent within a unified framework. We illustrate this with the following
concrete examples where we perform in-depth analysis based on the chain graph formulation.
3.1	Residual block as refinement module
The residual block, proposed originally in He et al. (2016) and improved later (He et al., 2016) with
the preactivation form, is an effective design for building up very deep networks. Here we show
that a preactivation residual block corresponds to a refinement module within a chain graph. We use
modules to refer to encapsulations of layered chain subgraphs as input-output mappings without
specifying their internal structures. A refinement module is defined as follows:
Definition 2. Given a base submodule from layer Xl-1 to layer Xl, a refinement module augments
this base submodule with a side branch that chains a copy of the base submodule (sharing weight
with its original) from Xl-1 toa duplicated layer Xl, and then a refining submodule from Xl to Xl.
Figure 2: Example of a refinement module (left) and its corresponding computational graph (right),
composed of a base submodule Xl-1 → Xl (blue background) and a refining submodule Xl →
Zl → X l (red background). In the computational graph each W, b represents a linear connection
(Eq. (3)) and σ an activation function. Same color identifies corresponding parts in the two graphs.
We see that this refinement module corresponds exactly to a preactivation residual block.
Proposition 3. A refinement module corresponds to a preactivation residual block.
We provide a proof in Appendix B.3 and illustrate this correspondence in Figure 2. An interesting
remark is that the refinement process can be recursive: the base submodule of a refinement module
can be a refinement module itself. This results in a sequence of consecutive residual blocks.
While a vanilla layered chain component encodes a generalized linear model during feed-forward
(c.f. Eqs. (7),(3)), the refinement process introduces a nonlinear extension term to the previously lin-
ear output preactivation, effectively increasing the representational power. This provides a possible
explanation to the empirical improvement generally observed when using residual blocks.
Note that it is also possible to interpret the original postactivation residual blocks, however in a
somewhat artificial manner, as it requires defining identity connections with manually fixed weights.
5
Under review as a conference paper at ICLR 2021
3.2	Recurrent neural networks
Recurrent neural networks (RNNs) (Goodfellow et al., 2016) are widely used for handling sequential
data. An unrolled recurrent neural network can be interpreted as a dynamic layered chain graph
constructed as follows: a given base layered chain graph is copied for each time step, then these
copies are connected together through recurrent chain components following the Markov assumption
(Koller & Friedman, 2009): each recurrent layer Xl,t at time t is connected by its corresponding
layer Xl,t-1 from the previous time step t - 1. Especially, denoting Pat (Xl,t) the non-recurrent
parent layers of Xl,t in the base chain graph, we can easily interpret the following two variants:
Proposition 4. Given a recurrent chain component that encodes P(Xl,t|Pat(Xl,t), Xl,t-1),
1.	It corresponds to a simple (or vanilla / Elman) recurrent layer (Goodfellow et al., 2016) if the
connection from Xl,t-1 to Xl,t is dense;
2.	It corresponds to an independently RNN (IndRNN) (Li et al., 2018) layer if the conditional inde-
pendence assumptions among the nodes Xil,t within layer l are kept through time:
∀i ∈ {1,.. .,Nl}, P (Xil,t|P at(Xl,t), Xl,t-1) = P(Xil,t|Pat(Xl,t),Xil,t-1).	(8)
We provide a proof in Appendix B.4 and illustrates both variants in Figure 3.
Figure 3: Comparison of an simple recurrent layer (left) v.s. an IndRNN (right) recurrent layer.
IndRNN, the better variant, enforces the intra-layer conditional independence through time.
The simple recurrent layer, despite its exhaustive dense recurrent connection, is known to suffer from
vanishing/exploding gradient and can not handle long sequences. The commonly used long-short
term memory (Hochreiter & Schmidhuber, 1997) and gated recurrent unit (Cho et al., 2014) alleviate
this issue via long term memory cells and gating. However, they tend to result in bloated structures,
and still cannot handle very long sequences (Li et al., 2018). On the other hand, IndRNNs can
process much longer sequences and significantly outperform not only simple RNNs, but also LSTM-
based variants (Li et al., 2018; 2019). This indicates that the assumption of intra-layer conditional
independence through time, analogue to the local receptive fields of convolutional neural networks,
could be an essential sparse network design tailored for sequential modeling.
3.3	Dropout
Dropout (Srivastava et al., 2014) is a practical stochastic regularization method commonly used
especially for regularizing fully connected layers. As we see in the following proposition, from the
chain graph point of view, dropout corresponds to introducing Bernoulli auxiliary random variables
that serve as noise generators for feed-forward during training:
Proposition 5. Adding dropout with drop rate 1 - pl to layer l corresponds to the following chain
graph construction: for each node Xil in layer l we introduce an auxiliary Bernoulli random variable
Di 〜Bernoulli(Pl) and multiply it with the pairwise interaction terms in all preactivations (Eq. (3))
involving Xil as parent (this makes Dil a parent of all child nodes of Xil and extend their pairwise
interactions with Xil to ternary ones). The behavior of dropout is reproduced exactly if:
•	During training, we sample auxiliary nodes Dil during each feed-forward. This results in dropping
each activation qli of node Xil with probability 1 - pl ;
•	At test time, we marginalize auxiliary nodes Dil during each feed-forward. This leads to deter-
ministic evaluations with a constant scaling of pl for the node activations qli.
We provide a proof in Appendix B.5. Note that among other things, this chain graph interpretation
of dropout provides a theoretical justification of the constant scaling at test time. This was originally
proposed as a heuristic in Srivastava et al. (2014) to maintain consistent behavior after training.
6
Under review as a conference paper at ICLR 2021
4	Partially collapsed feed-forward
The theoretical formulation provided by the chain graph interpretation can also be used to derive
new approaches for neural networks. It allows us to create new deep learning methods following
a coherent framework that provides specific semantics to the building blocks of neural networks.
Moreover, we can make use of the abundant existing work from the PGM field, which also serves as a
rich source of inspiration. As a concrete example, we derive in this section anew stochastic inference
procedure called partially collapsed feed-forward (PCFF) using the chain graph formulation.
4.1	PCFF: chain graph formulation
A layered chain graph, which can represent a neural network, is itself a probabilistic graphical model
that encodes an overall distribution conditioned on the input. This means that, to achieve stochastic
behavior, we can directly draw samples from this distribution, instead of introducing additional
“noise generators” like in dropout. In fact, given the globally directed structure of layered chain
graph, and the fact that the conditioned input nodes are ancestral nodes without parent, it is a well-
known PGM result that we can apply forward sampling (or ancestral sampling) (Koller & Friedman,
2009) to efficiently generate samples: given an input sample x1, We follow the topological order and
sample each non-input node Xil using its nodewise distribution (Eq. (2)) conditioned on the samples
(xp1 , . . . , xpn ) of its parents. Compared to feed-forward, forward sampling also performs a single
forward pass, but generates instead an unbiased stochastic sample estimate.
While in general an unbiased estimate is preferable and the stochastic behavior can also introduce
regularization during training (Srivastava et al., 2014), forward sampling can not directly replace
feed-forward, since the sampling operation is not differentiable and will jeopardize the gradient flow
during backpropagation. To tackle this, one idea is to apply the reparametrization trick (Kingma &
Welling, 2014) on continuous random variables (for discrete RVs the Gumbel softmax trick (Jang
et al., 2017) can be used but requires additional continuous relaxation). An alternative solution is to
only sample part of the nodes as in the case of dropout.
The proposed partially collapse feed-forward follows the second idea: we simply “mix up” feed-
forward and forward sampling, so that for each forward inference during training, we randomly
select a portion of nodes to sample and the rest to compute deterministically with feed-forward.
Thus for a node Xil with parents (Xp1 , . . . , Xpn), its forward inference update becomes
ι	[gl (ei(qp1,..., qpn))	if collapsed (feed-forward);
qi J ∣Tl(χi), Xi 〜fl(Tl(Xi), ei(qp1,..., qpn))	if uncollapsed (forward sampling).
Following the collapsed sampling (Koller & Friedman, 2009) terminology, we call this method the
partially collapsed feed-forward (PCFF). PCFF is a generalization over feed-forward and forward
sampling, which can be seen as its fully collapsed / uncollapsed extremes. Furthermore, it offers a
bias-variance trade-off, and can be combined with the reparametrization trick to achieve unbiased
estimates with full sampling, while simultaneously maintaining the gradient flow.
Relation to stochastic feedforward neural network While PCFF can also be seen as a stochastic
generalization of the feed-forward inference, it represents a substantially distinct approach compared
to SFNN: Apart from the clear difference that PCFF uses forward sampling and SFNN uses impor-
tance sampling, a major dissimilarity is that SFNN makes a clear distinction between deterministic
neurons and stochastic random variables, whereas PCFF identifies neurons with random variables
thanks to the layered chain graph interpretation. This is why PCFF can freely choose a different sub-
set of nodes to sample during each forward pass. From the chain graph interpretation perspective,
SFNN can be seen as a layered chain graph having a fixed subset of nodes with stochastic behavior,
and it performs a hybrid of feed-forward and importance sampling for inference.
4.2	PCFF: experimental validation
In the previous sections, we have been discussing existing approaches whose empirical evaluations
have been thoroughly covered by prior work. The novel PCFF approach proposed in this section,
however, requires experiments to check its practical effectiveness. For this we conduct here a series
7
Under review as a conference paper at ICLR 2021
of experiments1. Our emphasis is to understand the behavior of PCFF under various contexts and
not to achieve best result for any specific task. We only use chain graph interpretable components,
and we adopt the reparameterization trick (Kingma & Welling, 2014) for ReLU PCFF samples.
The following experiments show that PCFF is overall an effective stochastic regularization method.
Compared to dropout, it tends to produce more consistent performance improvement, and can some-
times outperform dropout. This confirms that our chain graph based reasoning has successfully
found an interesting novel deep learning method.
Figure 4: Comparison of stochastic methods (None/Dropout/PCFF) in terms of image classification
test errors (lower is better) under various settings. Left: MNIST/FashionMNIST datasets with a
simple dense network and tanh/ReLU activation functions; Right: CIFAR-10 dataset with ResNet20
and varying drop/sample rates. All reported results are average values of three runs. Compared to
dropout, PCFF can achieve comparable results, and tend to deliver more consistent improvements.
CIFAR-10
0.00.1 0.2 0.30.40.50.60.70.80.91.0
Drop/SamPIe rate
Regularization
—Dropout
—None
—PCFF
Simple dense network We start with a simple network with two dense hidden layers of 1024
nodes to classify MNIST (Lecun et al., 1998) and FashionMNIST (Xiao et al., 2017) images. We
use PyTorch (Paszke et al., 2017), train with stochastic gradient descent (learning rate 0.01, mo-
mentum 0.9), and set up 20% of training data as validation set for performance monitoring and
early-stopping. We set drop rate to 0.5 for dropout, and for PCFF we set the sample rate to 0.4 for
tanh and 1.0 (full sampling) for ReLU. Figure 4 Left reports the test errors with different activation
functions and stochastic regularizations.
We see that dropout and PCFF are overall comparable, and both improve the results in most cases.
Also, the ReLU activation consistently produces better results that tanh. Additional experiments
show that PCFF and dropout can be used together, which sometimes yields improved performance.
Convolutional residual network To figure out the applicability of PCFF in convolutional residual
networks, we experiment on CIFAR-10 (Krizhevsky, 2009) image classification. For this we adapt
an existing implementation (Idelbayev) to use the preactivation variant. We focus on the ResNet20
structure, and follow the original learning rate schedule except for setting up a validation set of
10% training data to monitor training performance. Figure 4 Right summarizes the test errors under
different drop/sample rates.
We observe that in this case PCFF can improve the performance over a wide range of sample rates,
whereas dropout is only effective with drop rate 0.1, and large drop rates in this case significantly
deteriorate the performance. We also observe a clear trade-off of the PCFF sample rate, where a
partial sampling of 0.3 yields the best result.
Independently RNN We complete our empirical evaluations of PCFF with an RNN test case. For
this we used IndRNNs with 6 layers to solve the sequential/permuted MNIST classification problems
based on an existing Implementation2 provided by the authors of IndRNN (Li et al., 2018; 2019).
We tested over dropout with drop rate 0.1 and PCFF with sample rate 0.1 and report the average test
accuracy of three runs. We notice that, while in the permuted MNIST case both dropout (0.9203)
and PCFF (0.9145) improves the result (0.9045), in the sequential MNIST case, dropout (0.9830)
seems to worsen the performance (0.9841) whereas PCFF (0.9842) delivers comparable result.
1Implementation available at: (Github link placeholder, provided as supplementary material.)
2https://github.com/Sunnydreamrain/IndRNN_pytorch
8
Under review as a conference paper at ICLR 2021
5	Conclusions and discussions
In this work, we show that neural networks can be interpreted as layered chain graphs, and that feed-
forward can be viewed as an approximate inference procedure for these models. This chain graph
interpretation provides a unified theoretical framework that elucidates the underlying mechanism of
real-world neural networks and provides coherent and in-depth theoretical support for a wide range
of empirically established network designs. Furthermore, it also offers a solid foundation to derive
new deep learning approaches, with the additional help from the rich existing work on PGMs. It is
thus a promising alternative neural network interpretation that deepens our theoretical understanding
and unveils a new perspective for future deep learning research.
In the future, we plan to investigate a number of open questions that stem from this work, especially:
•	Is the current chain graph interpretation sufficient to capture the full essence of neural networks?
Based on the current results, we are reasonably optimistic that the proposed interpretation can
cover an essential part of the neural network mechanism. However, compared to the function
approximator view, it only covers a subset of existing techniques. Is this subset good enough?
•	On a related note: can we find chain graph interpretations for other important network designs (or
otherwise some chain graph interpretable alternatives with comparable or better performance)?
The current work provides a good start, but it is by no means an exhaustive study.
•	Finally, what other new deep learning models and procedures can we build up based on the chain
graph framework? The partially collapsed feed-forward inference proposed in this work is just a
simple illustrative example, and we believe that many other promising deep learning techniques
can be derived from the proposed chain graph interpretation.
References
Sebastian Bach, Alexander Binder, Gregoire Montavon, Frederick Klauschen, Klaus-Robert Muller,
and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise
relevance propagation. PLOS ONE, 10(7):1-46, 07 2015.
Wray L. Buntine. Operations for learning with graphical models. J. Artif. Int. Res., 2(1):159-225,
December 1994.
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and
fully connected crfs. TPAMI, 40(4):834-848, 2018.
Lenaic Chizat and Francis Bach. On the global convergence of gradient descent for over-
parameterized models using optimal transport. In NeurIPS, 2018.
Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder
for statistical machine translation. In EMNLP, 2014.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, 2019.
Dumitru Erhan, Yoshua Bengio, Aaron Courville, and Pascal Vincent. Visualizing higher-layer
features of a deep network. 2009.
Morten Frydenberg. The chain graph markov property. Scandinavian Journal of Statistics, pp.
333-353, 1990.
Aude Genevay, Gabriel Peyre, and Marco Cuturi. GAN and VAE from an optimal transport point of
view. arXiv preprint arXiv:1706.01807, 2017.
I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y. Bengio. Generative adversarial networks. In NIPS, 2014.
Ian Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. In ICLR, 2015.
9
Under review as a conference paper at ICLR 2021
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV, 2016.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen,
T. N. Sainath, and B. Kingsbury. Deep neural networks for acoustic modeling in speech recogni-
tion: The shared views of four research groups. IEEE Signal Processing Magazine, 29(6):82-97,
2012.
Geoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief
nets. Neural Comput., 18(7):1527-1554, 2006.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-
1780, November 1997.
Yerlan Idelbayev. ProPer ResNet imPlementation for CIFAR10/CIFAR100 in PyTorch. https:
//github.com/akamaster/pytorch_resnet_cifar10.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deeP network training by
reducing internal covariate shift. In ICML, 2015.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In NeurIPS, 2018.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reParameterization with gumbel-softmax. In
ICLR, 2017.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
DaPhne Koller and Nir Friedman. Probabilistic Graphical Models: Principles and Techniques. MIT
Press, 2009.
A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classification with deeP convolutional
neural networks. In NIPS, 2012.
Alex Krizhevsky. Learning multiPle layers of features from tiny images. Technical rePort, 2009.
Guillaume LamPle, Miguel Ballesteros, SandeeP Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. In NACCL, 2016.
Steffen Lilholt Lauritzen and Nanny Wermuth. GraPhical models for associations between variables,
some of which are qualitative and some quantitative. The annals of Statistics, PP. 31-57, 1989.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning aPPlied to document recog-
nition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. DeeP neural networks as gaussian Processes. In ICLR, 2018.
Shuai Li, Wanqing Li, Chris Cook, Ce Zhu, and Yanbo Gao. IndePendently recurrent neural network
(indrnn): Building a longer and deePer RNN. In CVPR, 2018.
Shuai Li, Wanqing Li, Chris Cook, Yanbo Gao, and Ce Zhu. DeeP indePendently recurrent neural
network (indrnn). arXiv preprint arXiv:1910.06251, 2019.
Zachary C. LiPton. The mythos of model interPretability. Commun. ACM, 61(10):36-43, SePtember
2018.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic
segmentation. In CVPR, 2015.
10
Under review as a conference paper at ICLR 2021
Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Mean-field theory of two-layers neural
networks: dimension-free bounds and kernel limit. Proceedings of Machine Learning Research,
(99), 2019.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529-533, February 2015.
Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted boltzmann machines.
In ICML, 2010.
Radford M. Neal. Connectionist learning of belief networks. AI, 56(10):71-113, July 1992.
Radford M. Neal. Bayesian Learning for Neural Networks. Springer-Verlag, 1996.
Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, and Jeff Clune. Synthesizing the
preferred inputs for neurons in neural networks via deep generator networks. In NIPS, 2016.
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,
Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in
pytorch. In NIPS-W, 2017.
Hoifung Poon and Pedro Domingos. Sum-product networks: A new deep architecture. In UAI,
2011.
Ruslan Salakhutdinov and Geoffrey Hinton. An efficient learning procedure for deep boltzmann
machines. Neural Comput., 24(8):1967-2006, August 2012.
Yuesong Shen, Tao Wu, Csaba Domokos, and Daniel Cremers. Probabilistic discriminative learning
with layered graphical models. CoRR, abs/1902.00057, 2019.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,
Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine
Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game ofGo with
deep neural networks and tree search. Nature, 529(7587):484-489, January 2016.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15(56):1929-1958,
2014.
C. Szegedy, Wei Liu, Yangqing Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke,
and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.
Charlie Tang and Russ R Salakhutdinov. Learning stochastic feedforward neural networks. In
Advances in Neural Information Processing Systems, pp. 530-538, 2013.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms. CoRR, abs/1708.07747, 2017.
Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In
ECCV, 2014.
Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-Paredes, Vibhav Vineet, Zhizhong Su, Da-
long Du, Chang Huang, and Philip H. S. Torr. Conditional random fields as recurrent neural
networks. In ICCV, 2015.
11
Under review as a conference paper at ICLR 2021
A	Globally directed structure of layered chain graph
With the absence of (undirected) intra-layer connection in the chain components P (Xl |Pa(Xl)),
the layered chain graph defined in Definition 1 has a globally directed structure. This means that
equivalently it can also be modeled by a directed graphical model (Bayesian network) which admits
the same nodewise decomposition
L Nl
P(X2,...,XL|X1) =YYP(Xil|Pa(Xl))	(10)
l=2 i=1
and whose factorized nodewise conditional distributions P (Xil |P a(Xl)) are additionally modeled
by pairwise CRFs:
P (Xi ∣Pa(Xl)) = fl (Tl (Xi), ei(Tp1(Xp1),..., Tpn (Xpn))).	(11)
The expressions Eq. (10), (11) are identical to Eq. (1), (2), which shows the equivalence. This is a
rather straightforward result as directed graphical model is just a special case of chain graph. We
employ the more general chain graph modeling in this paper out of two concerns:
1.	Neural network designs rely heavily on the notion of layers. The layered chain graph formulation
provides us with the notion of chain component that can correspond exactly to a neural network
layer. This is missing in a directed graphical model formulation;
2.	As we have discussed in Section 2.3, using the layered chain graph formulation allows for a
straightforward extension from the classical neural network case (layered chain graph) to the
more complex case with intra-connected layers that corresponds to general chain graphs with
non-bipartite CRF chain components, which can be useful for, e.g., structured prediction tasks.
B Proofs
B.1 Proof of Proposition 1
The main idea behind the proof is that for a linear function, its expectation can be moved inside and
directly applied on its arguments. With this in mind let’s start the actual deductions:
•	To obtain Eq. (6), we start from Eqs. (5) and (2):
Qi(XiIx1) = EQPi,...,QPn [P(xi∣Pa(Xl))]	(12)
=EQPi,...,QPn [fl(Tl (Xi), ei(Tpι(Xp1),..., Tpn (Xpn)))].	(13)
Now, we make the assumption that the following mapping is approximately linear:
(vι,..., Vn) → fl (Tl (Xi), ei(vι,..., Vn)).	(14)
This allows us to move the expectation inside, resulting in (racall the definition of q in Eq. (4))
Qi(XiIx1) ≈ fl (Tl (Xi), ei (Eqpi [Tp1 (Xp1)],..., EQpn [Tpn (Xpn)]))	(15)
≈ f l(Tl(Xi), ei(qp1,..., qpn)).	(16)
•	To obtain Eq. (7), we go through a similar procedure (for discrete RVs we replace integrations by
summations):
From Eqs. (4), (5) and (2) we have
qli = EQli [Tl(Xil)]
=T Tl(Xi)Qi(XiIx 1)dχi
xli
=Z Tl(Xi)EQpι,...,Qρn[fl(Tl(xi),ei(Tp1 (Xp1),...,Tpn(Xpn)))]dxi
xli
=EQpi,...,Qpn h / Tl(Xi)fl(Tl(Xi),ei(Tp1 (Xp1),...,Tpn(Xpn)))dXii
xli
(17)
(18)
(19)
(20)
12
Under review as a conference paper at ICLR 2021
Defining the activation function gl as (thus eli corresponds to the preactivation)
gl (v) =
xli
Tl(Xi)fl(Tl(xi), v)dxi,
we then have
qi = Eqpi ,...,Qpn [gl (ei(TpI(XpI),..., Tpn (Xpn)))].
Again, we make another assumption that the following mapping is approximately linear:
(vi, . . . , Vn) → gl(ei(vι, ... , Vn)).
This leads to the following approximation in a similar fashion:
qi ≈ gl(ei(EQpi [Tpι(Xp1)],..., EQpn [Tpn (Xpn)]))
≈ gl(ei(qp1,..., qpn)).
(21)
(22)
(23)
(24)
(25)
□
B.2 Proof (and other details) of Corollary 2
Let’s consider a node Xii connected by parent layers Xp1 , . . . , Xpn. To lessen the notations we use
the shorthands e for ei(Tp1 (Xp1),..., Tpn (Xpn)) and e； for ei(qp1,..., qpn).
1.	For the binary case we have
Ti(Xii) = Xii ∈ {α, β},	(26)
P(Xi∣Pa(Xl)) = fl(Xil,ei) = 7,p∖e exp(Xl +	(27)
Z(Pa(Xi))
with the partition function
Z(Pa(Xi)) = exp(α eii) + exp(β eii)	(28)
that makes sure P (Xii |Pa(Xi)) is normalized. This means that, since Xii can either be α or β,
we can equivalently write (σ : x 7→ 1/(1 + exp(-x)) denotes the sigmoid function)
fl(Xi, ei) = P(XiIPa(xl)) = {；((a - β) ei) if Xi = α = σ(3i - α - β) ei).	(29)
σ((β - α) ei) if xi = β
Using the feed-forward expression (Eq. (7)), we have
qi ≈ X Xifl(χi,≡i) = ασ((α - β) ei) + β σ((β - α) M)	(3O)
xli∈{α,β}
=β-α tanh (β-α ∙ °i)+ +.	(31)
Especially,
When α = 0, β = 1, we have qil ≈ σ(ei);	(32)
When α = -1, β = 1, We have q ≈ tanh©).	(33)
Furthermore, the roles of α and β are interchangeable.
2.	For the multilabel case, let’s assume that the node Xil can take one of the c labels {1, . . . , c}. In
this case, We have an indicator feature function Which outputs a length-c feature vector
Tl(Xil) = (1Xil=1,..., 1Xil=c)>.	(34)
This means that for any given label j ∈ {1, . . . , c}, Tl(j) is a one-hot vector indicating the j-th
position. Also, ei and ei will both be vectors of length c, and we denote e% and Eijtheir j-th
entries. We have then
P(Xi∣Pa(Xl)) = fl(Tl(Xi),ei) = Z(Pa(Xl)) exp(Tl(Xi) ∙ M)	(35)
13
Under review as a conference paper at ICLR 2021
with the normalizer (i.e. partition function)
c
Z(Pa(Xl)) = X exp(eli,j).	(36)
j=1
This means that
∀j ∈ {1,..., c}, fl(Tl (j), ei) = (softmax(ei,ι,..., ei,C))j,	(37)
and, using the feed-forward expression (Eq. (7)), we have
cc
qi ≈ X fl (Tl (XIi), M)Tl(Xi) = X( softmax尾 ι,...总,c))j Tl(j)	(38)
xli=1	j=1
=softmax(M,ι,...,阻)，	(39)
i.e. the expected features qli of the multi-labeled node Xil is a length-c vector that encodes the
result of a softmax activation.
3.	The analytical forms of the activation functions are quite complicated for rectified Gaussian
nodes. Luckily, it is straight-forward to sample from rectified Gaussian distributions (get Gaus-
sian samples, then rectify). meaning that we can easily evaluate them numerically with sample
averages. A resulting visualization is displayed in Figure 1 Right. Specifically:
•	The ReLU nonlinearity can be approximated reasonably well by a rectified Gaussian node with
no leak ( = 0) and tanh-modulated standard deviation (sli = tanh(eli)), as shown by the red
plot in Figure 1 Right;
•	Similar to the ReLU case, the leaky ReLU nonlinearity can be approximated by a leaky ( 6= 0)
rectified Gaussian node with tanh-modulated standard deviation (sli = tanh(eli)). See the
green plot in Figure 1 Right which depict the case with leaky factor = 1/3;
•	We discover that a rectified Gaussian node with no leak ( = 0) and an appropriately-chosen
constant standard deviation sli can closely approximate the softplus nonlinearity (see the blue
plot in Figure 1 Right). We numerically evaluate sli = 1.776091849725427 to minimize the
maximum pointwise approximation error.
Averaging over more samples would of course lead to more accurate (visually thinner) plots,
however in Figure 1 Right we deliberately only average over 200 samples, because we also want
to visualize their stochastic behaviors: the perceived thickness of a plot can provide a hint to the
output sample variance given the preactivation eli .
□
B.3 Proof of Proposition 3
Given a refinement module (c.f. Definition 2) that augments a base submodule m from layer Xl-1
to layer Xl using a refining submodule r from Xl to layer Xl, denote gl the activation function
corresponding to the distribution of nodes in layer Xl, we assume that these two submodules alone
would represent the following mappings during feed-forward
ql = gl(em(ql-1))	(base submodule)
Iql = gl(er(ql))	(refining submodule)
(40)
where em and er represent the output preactivations of the base submodule and the refining sub-
module respectively. Then, given an input activation ql-1 from Xl-1, the output preactivation of
the overall refinement module should sum up contributions from both the main and the side branches
(c.f. Eq. (3)), meaning that the refinement module computes the output as
ql= gl(em(ql-1)+ er (ql))	(41)
with ql the output of the duplicated base submodule with shared weight, given by
ql= gl(em(ql-1)).	(42)
We have thus (Id denotes the identity function)
ql = gl ◦ (Id +er ◦ gl) ◦ em(ql-1)	(43)
where the function Id +er ◦ gl describes a preactivation residual block that arises naturally from the
refinement module structure.	□
14
Under review as a conference paper at ICLR 2021
B.4 Proof of Proposition 4
To match the typical deep learning formulations and ease the derivation, we assume that the base
layered chain graph has a sequential structure, meaning that Pat(Xl,t) contains only Xl-1,t, and
we have
P (Xil,t|P at(Xl,t), Xl,t-1) = P(Xil,t|Xl-1,t,Xl,t-1)	(44)
=f l,t(Tl(χi,t), ei,t (Tl-1(Xl-1,t), Tl(Xl,t-1))).	(45)
1.	When the connection from Xl,t-1 to Xl,t is dense, we have that for each i ∈ {1, . . . , Nl},
Nl-1	Nl
e" TlT(XlT,t), τl (χl,t-1)) = χ wj,i Tl-IXT,t)+ X ^产”厂1)+ bi. (46)
j=1	k=1
Thus the feed-forward update for layer l at time t is
Nl-1	Nl
∀i∈{1,...,Nl}, qi,t≈gl( X Wj,iqj-1,t + XUkRkt-1 + %	(47)
j=1	k=1
which corresponds to the update of a simple recurrent layer.
2.	The assumption of intra-layer conditional independence through time means that we have
P(Xil,t|Pat(Xl,t),Xl,t-1) = P(Xil,t|Xl-1,t,Xil,t-1),	(48)
which in terms of preactivation function means that for each i ∈ {1, . . . , Nl},
ei,t(TlT(Xlτ,t), Tl(Xl,t-1)) = ei,t(TlT(Xlτ,t), Tl(Xi,t-1))	(49)
Nl-1
= X Wlj,iTl-1(Xjl-1,t)+UliTl(Xil,t-1)+bli.	(50)
j=1
In this case the feed-forward update for layer l at time t is
Nl-1
∀i∈{1,...,Nl}, qi,t≈gl( X Wj,iqj-1,t + Uiqi,t-1 + bi)	(5i)
j=1
which corresponds to the update of an independently RNN layer (c.f. Eq. (2) of Li et al. (2018)).
□
B.5 Proof of Proposition 5
Again, to match the typical deep learning formulations and ease the derivation, we assume that the
layered chain graph has a sequential structure, meaning that Xl is only the parent layer of Xl+1.
With the introduction of the auxiliary Bernoulli RVs Dl, the l + 1-th chain component represents
Nl+1
P(Xl+1∣Xl, Dl)= Y fj+1(Tl+1(Xj+1), ej+1(Tl(Xl), Dl))	(52)
j=1
with
Nl-1
ej+1(Tl(Xl),Dl) = X DIiWli+j1T(Xi)+ bj.	(53)
i=1
• For a feed-forward pass during training, we draw a sample dli for each Bernoulli RV Dil , and the
feed-forward update for layer l + 1 becomes
Nl
∀j ∈{1,...,N l+1}, qj+1 ≈ gl+1(X diW'i+1qi + bj).	(54)
i=1
Since dli = 1 with probability pl and dli = 0 with probability 1 - pl, each activation qli will be
“dropped out” (i.e. dli qli = 0) with probability 1 - pl (this affects all qlj+1 simultaneously).
15
Under review as a conference paper at ICLR 2021
• For a feed-forward pass at test time, we marginalize each Bernoulli RV Dil . Since we have
∀i∈ {1,...,Nl}, E[Dil] =pl,	(55)
the feed-forward update for layer l + 1 in this case becomes
Nl
∀j ∈{1,...,N l+1}, qj+1≈ gl+1(X PlWi+1qi + bj)	(56)
i=1
where we see the appearance of the constant scale pl .
□
C Remark on input modeling
A technical detail that was not elaborated in the discussion of chain graph interpretation (Section 2)
is the input modeling: How to encode an input data sample? Ordinarily, an input data sample is
treated as a sample drawn from some distribution that represents the input. In our case however,
since the feed-forward process only pass through feature expectations, we can also directly interpret
an input data sample as a feature expectation, meaning as a resulting average rather than a single
sample. Using this fact, Shen et al. (2019) propose the “soft clamping” approach to encode real
valued input taken from an interval, such as pixel intensity, simply as an expected value of a binary
node which chooses between the interval boundary values.
This said, since only the conditional distribution P(X2, . . . , XL|X1) is modeled, our discriminative
setting actually do not require specifying an input distribution P (X1).
16