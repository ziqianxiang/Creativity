Under review as a conference paper at ICLR 2021
Deep Learning Solution of the Eigenvalue
Problem for Differential Operators
Anonymous authors
Paper under double-blind review
Ab stract
Solving the eigenvalue problem for differential operators is a common problem in
many scientific fields. Classical numerical methods rely on intricate domain dis-
cretization, and yield non-analytic or non-smooth approximations. We introduce
a novel Neural Network (NN)-based solver for the eigenvalue problem of differ-
ential self-adjoint operators where the eigenpairs are learned in an unsupervised
end-to-end fashion. We propose three different training procedures, for solving
increasingly challenging tasks towards the general eigenvalue problem. The pro-
posed solver is able to find the M smallest eigenpairs for a general differential
operator. We demonstrate the method on the Laplacian operator which is of par-
ticular interest in image processing, computer vision, shape analysis among many
other applications. Unlike other numerical methods such as finite differences, the
partial derivatives of the network approximation of the eigenfunction can be ana-
lytically calculated to any order. Therefore, the proposed framework enables the
solution of higher order operators and on free shape domain or even on a manifold.
Non-linear operators can be investigated by this approach as well.
1	Introduction
Eigenfunctions and eigenvalues of the Laplacian (among other operators) are important in various
applications ranging, inter alia, from image processing to computer vision, shape analysis and quan-
tum mechanics. It is also of major importance in various engineering applications where resonance
is crucial for design and safety [Benouhiba & Belyacine (2013)]. Laplacian eigenfunctions allow
us to perform spectral analysis of data measured at more general domains or even on graphs and
networks [Shi & Malik (2000)]. Additionally, the M -smallest eigenvalues of the Laplace-Beltrami
operator are fundamental features for comparing geometric objects such as 3D shapes, images or
point clouds via the functional maps method in statistical shape analysis [Ovsjanikov et al. (2012)].
Moreover, in quantum mechanics, the smallest eigenvalues and eigenfunction of the Hamiltonian
are of great physical significance [Han et al. (2019)]. In this paper we present a novel numerical
method for the computation of these eigenfunctions (efs) and eigenvalues (evs), where the efs are
parameterized by NNs with continuous activation functions, and the evs are directly calculated via
the Rayleigh quotient. The resulting efs are therefore smooth functions defined in a parametric way.
This is in contrast to the finite element [Pradhan & Chakraverty (2019)] and finite difference [Saad
(2005); Knyazev (2000)] methods in which the efs are defined on either a grid or as piecewise lin-
ear/polynomial functions with limited smoothness. In these matrix-based approaches one has to
discretize first the problem and to represent it as an eigenvalue problem for a matrix. This in itself
is prone to numerical errors.
Following [Bar & Sochen (2019)], we suggest an unsupervised approach to learn the eigenpairs of
a differential operator on a specified domain with boundary conditions, where the network simul-
taneously approximates the eigenfunctions at every entry x. The method is based on a uniformly
distributed point set which is trained to satisfy two fidelity terms of the eigenvalue problem formu-
lated as the L2 and L∞-likenorms, boundary conditions, orthogonality constraint and regularization.
There are several advantages of the proposed setting: (i) the framework is general in the sense that
it can be used for non linear differential operators with high order derivatives as well. (ii) Since
we sample the domain with a point cloud, we are not limited to standard domains. The problem
can be therefore solved in an arbitrary regular domain. (iii) The framework is generic such that
additional constraints and regularizers can be naturally integrated in the cost function. (iv) Unlike
1
Under review as a conference paper at ICLR 2021
previous methods, the suggested framework solves simultaneously multiple eigenpairs. This means
that we handle a family of PDEs (one for each eigenvalue and finding the eigenvalues themselves)
in one network that solves these multiple PDEs together. The method is applied in 1D and 2D
for both known and multiple unknown eigenvalues of the Laplacian operator. Quantitative analysis
demonstrates the robustness of the method compared with the classical matrix-based methods.
2	Related Work
Many recent approaches have shown promise in using the power of NNs to approximate solutions
of differential equations. Classical methods are often prone to weakness due to the discretization of
the domain Ω. In [Bar & Sochen (2019)], the authors propose a solver for both forward and inverse
problems, using NNs to model the solution, and penalizing using both the automatic differentia-
tion, and boundary conditions. In [Raissi et al. (2017)], a similar approach was taken to solve both
continuous and discrete time models. In [Chen et al. (2018)], differential equation solvers are used
as part of the network architecture, and are shown to enhance the smoothness and convergence of
the solutions. In order to properly solve differential equations, a representation that captures high-
order derivatives is desired. Recently, [Sitzmann et al. (2020)] proposed a network architecture that
illustrates these requirements using periodic activation functions with the proper initialization. Ad-
ditionally, [Rippel et al. (2015)] proposed leveraging Discrete Fourier Transform (DFT) to represent
the network in spectral space.
The well-known power method and its variants [Eastman & Estep (2007)] has been the main
method for addressing the eigenvalue problem. The method works on specific Linear operators,
L : L2(Rd) → L2(Rd). It is done after the continuous equation is reduced numerically to an eigen-
pair equation for matrices. This process introduces numerical errors even before the solution of the
eigen problem. The usage of the power method for spectral operators on Hilbert Spaces was recently
shown inn [Erickson et al. (1995)]. In [Hait-Fraenkel & Gilboa (2019)] a modified method for non-
linear differential operators was proposed. Furthermore, most power method variants for operators,
converge to a single eigenpair. Finding the M smallest eigenpairs can be both computationally and
algorithmically challenging.
Advanced methods addressing the eigenvalue problem via deep networks were recently introduced.
These methods are based on variational Monte Carlo (VMC) and diffusion Monte Carlo (DMC)
methods. VMC relies on leveraging physical knowledge to propose an ansatz of the eigenfunc-
tion and incorporates the essential physics [Han et al. (2019); Hermann et al. (2019); Pfau et al.
(2019); Choo et al. (2019)]. Recently, [Han et al. (2020)] formulated the eigenvalue problem by
the stochastic backward equation using the DMC method, where the loss function optimizes the
eigenvalue, eigenfunction and the scaled gradient of the eigenfunction. The loss function consists of
L2 norm of two fidelity terms with additional normalization. The algorithm yields the first eigenpair
with an optional second eigenpair given some mild prior estimate of the eigenvalue. In the suggested
work, we formulate the eigenvalue problem in a direct setting with flexible number of eigenpairs.
Additionally, we use L∞ norms for fidelity and boundary condition terms to accomplish a strong
(pointwise) solution.
3	Preliminaries
Let H be a Hilbert space where the inner product for u, v ∈ H is hu, vi. Let A ∈ O(H) be an
operator. Let A* be the adjoined operator defined by(A*u, Vi =(u, Avi ∀u, V ∈ H. Then A is
said to be self-adjoint if A = A*. We start with a short Lemma on self-adjoint operators [Conway
(1985)].
Lemma 3.1 Let H be a Hilbert space. Let A ∈ O(H) be a self-adjoint operator. Then all eigenval-
ues of A are real.
In this work we focus on self-adjoint operators. An eigenpair of an operator L is defined as: (u, λ)
s.t. λ ∈ R, where u is the eigenfunction of L and λ is the corresponding eigenvalue. Let L be a
self-adjoint operator L : L2(Rd) → L2(Rk). Our objective is to search for eigenpairs {ui, λi} such
that
Lui + λiui = 0 ∀i.	(1)
2
Under review as a conference paper at ICLR 2021
Figure 1: The solution to u00 + 4u = 0 with u(0) = u(π∕2) = 0 at iterations (from left to right) 1,
100 and 2500. At the last iteration RMSE = 6.52e-4 and PSNR = 28.84.
The proposed algorithm approximates the eigenfunction set ui(x) by a NN ui(x; θui) parameterized
by θui which denotes the network weights and biases. The network consists of few fully connected
layers with smooth activation function φ and linear sum in the last layer. For example, four layers
architecture is given by
u(x) =W5φW4φW3φW2φW1x+b1 +b2 +b3 +b4 +b5,	(2)
and φ(∙) = tanh(∙) or SIREN. The input to the network is X ∈ Rd, one input node for each
dimension. The network is trained to satisfy the PDE with boundary conditions by minimizing a
cost function.
4	Single Known Eigenvalue
We first address the problem of finding a single eigenfunction u(x) given its corresponding eigen-
value λ. We approximate u(x) by a NN and optimize the following cost function
Fi (u(x,θu)) = αkTuk2 + μkTu∣∣∞ + δ∣∣u - u∣∣ι,∂Ω + β∣ku∣∣2 - c∣ + P∣∣θuk2,	(3)
where
Tu := Lu + λu,
with the Laplacian operator L = ∆. The first two terms in equation 3 are the L2 and L∞ fidelity
terms where the latter promotes a pointwise solution such that the equation is satisfied for isolated
points as well [Bar & Sochen (2019)]. The third term imposes boundary conditions and the forth
normalizes the squared length of u to c = 1 due to scale invariance of the eigenfunction (since
(αu, λ) is a valid eigenpair for α 6= 0). The final term is the standard weight decay term which sta-
bilizes the network weights. The L2 norm ∣.∣22 is defined as the Monte-Carlo Integral approximation
norm:
I∣uk2 = VONQ) X Iu(X)12 〜/ 1U(X)12dx.	(4)
x=x1
In the first example we apply the Laplacian operator in 1D where λ = 4 and u(0) = u(π∕2) = 0.
Then,
Tu = u00(X) + 4u(X).
The normalized analytical solution is therefore
2
u(x) = √p sιn(2x).
Figure 1 demonstrates the outcome of the algorithm at iterations 1, 100 and 2500. As can be seen,
the approximated solution approaches the sin function with Relative Mean Square Error (RMSE) =
6.52e-4 and peak signal-to-noise ratio (PSNR) = 28.84.
3
Under review as a conference paper at ICLR 2021
Figure 2: The solution to u00 + λu = 0 with u(0) = u(π) = 0 at iterations 1, 500 and 1000. The
associated eigenvalues are λ = 70 ± 53, 5.5 ± 0.5, 1 ± 0.03. Note that the solution is up to a sign.
5 Single Eigenpair with the Smallest Eigenvalue
Next, we address the case where the eigenvalue is not known in advance. We therefore limit our-
selves to the smallest nontrivial eigenpair. This approach is analogue to power method approach,
where only one dominant eigenpair is found. Recall the Rayleigh quotient defined as [Miller (2016);
Feld et al. (2019)]
R(u) := -
hLu, ui
hu,ui
(5)
It can be shown that the eigenfunction u is a critical point of R(u), where R(u) is its corresponding
(nontrivial) eigenvalue λ. Furthermore, if U is a function which is close to U then R(U) approximates
λ. In the following cost function we replace λ by R(u). Then equation 3 is modified to
F(u(χ,θu)) = α∣∣Tuk2 + μ∣∣Tuk∞ + δIlU-U0kι,∂ω + β∣∣∣u∣∣2-c∣ + ρ∣∣θuk2 + Yl∣R(u)k2, (6)
where
TU = LU + R(U)U.
The last term minimizes R(U) and therefore attracts the solution to the lowest nontrivial eigenvalue.
The ground truth eigenpair is given by
(VZlSin(X),1)
for Ω = [0, π]. Figure 2 shows the outcome
of the proposed method at three iterations. The approximated eigenfunction is obtained up to a sign.
The eigenvalue which is the value of the Rayleigh quotient converges to the true value λ = 1 with
decreasing standard deviation along the mini-batches. Quantitative results are shown in row 1 of
tables 1 and 2 where MAE stands for Mean Absolute Error and MRE for Mean Relative Error.
6 Multiple EigenPairs with the Smallest EigenValues
A generalization of the former case is to find M eigenpairs with the corresponding bottom-M eigen-
values. Following [Hait-Fraenkel & Gilboa (2019)], and using the orthogonality property of the
eigenfunctions we optimize the following cost function:
M
F(u, (χ,θu)) = X (α∣∣TUiIl2 + μ∣∣TUik∞ + δIlUi- u∣∣ι,∂ω + β∣∣Uik2 - c∣ + YiIlR(Ui)112)
i=1
+ ρIθu I22 + ν XhUi, Uj i,
i<j
(7)
4
Under review as a conference paper at ICLR 2021
Figure 3: Left panel: eigenfunctions for M = 3. Right panel: eigenfunctions for M = 4 are shown
on the left and the convergence of the eigenvalues are shown on the right. Clearly the convergence
of the functions are un (x) =
sin(nx), λn
= n2, up to a sign.
Table 1: Dimension= 1, Eigenfunction Error Analysis
M	Mean PSNR (Per ef.)	Mean RMSE (Per ef.)
1	28.84	1.30e-3
2	56.58	2.21e-6
3	52.21	8.54e-6
4	48.78	2.05e-5
where the last term is explicitly given by
Ns M M
ν	ui (xk)uj (xk),
k=1 i=1 j=i+1
with point set size Ns. For each eigenfunction ui we impose the L2 and L∞ terms, boundary con-
ditions and normalization as before. The weight of the Rayleigh quotient is multiplied by γi = 1/i
since we want a monotonic penalty of the size of λi. The last term enforces the orthogonality of dis-
tinct eigenfunctions. Figure 3 demonstrates the outcome of the algorithm in 1D. In this case we have
one network with M output values, one for every eigenfunctuion. The ground truth eigenfunctions
of the Laplacian with u(0) = u(π) = 0 are given by
Un(x) = ʌ/- sin(nx), λn = n2, n = 1, 2, 3,....
π
The left panel of Figure 3 shows the results for M = 3 and in the right for M = 4. The right figure
in the right panel shows the convergence of the four eigenvalues which as expected have the values
1, 4, 9 and 16. Quantitative results are summarized in tables 1 and 2.
Next, we tested the method in two-dimensions where we trained M different networks simultane-
ously, each with a single output, one for each eigenfunction. We found this architecture adequate
for the multiple eigenpairs problem in the 2D case. The ground truth solution is then
2
Unm(x, y) = 一 sin(nx) sin(my), λnm = n2 + m2, n,m = 1, 2, 3...
π
for Ω = [0,π]2 and u(x, 0) = u(x,π) = u(0,y) = u(π,y) = 0. Figure 4 shows the results
at different iterations for M = 1. The expected eigenvalue is the lowest one (n = 1, m = 1).
5
Under review as a conference paper at ICLR 2021
Table 2: Dimension= 1, Eigenvalue Error Analysis
M	Ground Truth	Predicted	Mean MAE (per ev.)	Mean MRE (per ev.)
1	1	1.02	0.02	0.02
2	1,4	1.11, 4.09	0.10	0.07
3	1,4,9	1.08, 8.93, 4.13	0.09	0.04
4	1,4,9,16	1.12, 4.09, 9.03, 15.95	0.07	0.04
Figure 4: The solution to the 2D Laplace eigevalue problem, with M = 1 at iterations (from left to
right) 1, 100 and 1000.
As the algorithm converges, the expected eigenfunction is clearly seen (right image). We further
tested our performance for M = 4, see Figure 5. The figures from left to right stand for the four
eigenfunctions. The rows from top to bottom are iterations 1, 500 and 1000 respectively. As can
be easily shown, these results are with accordance with the theoretical eigenfunctions (n, m) =
(1, 1), (1, 2), (2, 1), (2, 2). Quantitative results are summed up in tables 3 and 4 where SSIM stands
for structural similarity index measure.
As the proposed method does not depend on the discretization of the domain, it can be easily adapted
to free-form domains. It is demonstrated with M = 1, using the same square domain, with a circular
piece removed. The boundary conditions along the circular curve are not provided. We tested three
different sizes of circles as can be seen in Figure 6. The left column is the outcome of the algorithm
inferred at the whole rectangle. The ground truth is on the right column and is as for the full shape.
The error is depicted in the central column. It is easy to see that the error concentrates on the missing
regions, and is extrapolated in a smooth fashion.
7 Comparison to matrix-based methods
In this section we compare our algorithm with matrix-based methods where the continuous operator
is discretized by finite differences scheme to form the 2D Laplacian matrix with spacing h. The ap-
proximation of the continuous operator, therefore, may yield significant numerical errors [Knyazev
(2000)]. In addition, the incorporation of the boundary condition is not straightforward since the
boundary conditions may affect the construction of the matrix approximation of the operator. We
calculate the eigenpairs of the matrix by standard Matlab solver with h = 0.03. Quantitative results
are shown in the last lines of tables 3 and 4. As can be seen, the proposed algorithm outperforms
in the eigenvalue analysis (Table 4), while in the eigenfunction analysis the results are comparable.
Next, we compare our results to the power method. This method is a well known iterative algo-
rithm where given a diagonalizable matrix A the algorithm finds the maximal eigenvalue and its
corresponding eigenfunction, while the inverse power method finds the lowest one [Bronson et al.
(2014)]. We adopted the inverse power method to our problem following [Bozorgnia (2016)]. Fig-
ure 7 shows the outcome of the inverse power method implemented in Matlab with h = 0.03 and
initial eigenvalue λ0 = 1 in two dimensions. On the left is the estimated eigenfunction which cor-
responds to the lowest eigenvalue (m = 1, n = 1). In the middle is the error |ugt(x, y) - u(x, y)|
and in the right is the convergence plot of the eigenvalue which was converged to 2.02. In our ex-
6
Under review as a conference paper at ICLR 2021
Figure 5: The solution to the 2D Laplace eigenvalue problem for M = 4. The Iterations 1, 100, 1000
are shown from top to bottom, for each of the 4 eigenfunctions.
Figure 6: Free-form domain results for M = 1 excluding points in the right half circles. Left: The
outcome of the proposed algorithm inferred in the full rectangle. The solution in the missing domain
seems to be a smooth extrapolation. Middle: error from the full ground truth |u - full(ugt)|. The
colorbar is referred only to this column. Right: Ground Truth. It is clear that the error is condensed
into the missing domain, where the information is missing.
7
Under review as a conference paper at ICLR 2021
Figure 7: Inverse power method. Left: Approximated eigenfunction, Middle: error |ugt - u|. Right:
convergence of the eigenvalue. λ = 2.02, RMSE=8.76e-3, PSNR=20.57, SSIM=0.99
Table 3: Dimension= 2, Eigenfunction Error Analysis
M	Mean PSNR(per ef.)	Mean SSIM(per ef.)	Mean RMSE(per ef.)
1	36.67	0.93	2.14e-4
2	31.66	0.89	1.33e-3
3	24.84	0.84	3.92e-3
4	18.97	0.52	1.52e-1
4 (Laplacian matrix)	19.39	0.75	2.45e-1
periments, the algorithm seemed to be sensitive to discretization and initialization. Table 5 shows
the performance of the proposed algorithm compared with the inverse power method. Better results
are obtained in almost all quantitative measures. Furthermore, the inverse power method finds only
a single eigenpair, while the proposed method outputs the M smallest eigenpairs. Although it is
possible to calculate more eigenpairs based on the previous one via orthogonality constraints, an
undesired accumulated error may emerge.
8 Implementation Details
Our network was constructed as a dense fully connected network with 5 hidden layers architecture,
each with a varying number of neurons, from 26-100. Our code was implemented in PyTorch. Each
of our models was trained for 5000 iterations on NVIDIA QUADRO RTX-5000 Graphics card. In
the 2D case all the networks were trained simultaneously. As for the activation function, since we are
modeling smooth functions, we found that ReLU and its variants were less suitable both theoretically
and experimentally. For the 1D experiments we used the tanh activation function, and in the 2D
experiments we used the SIREN [Sitzmann et al. (2020)] activation function. The SIREN activation
has been shown to excel in modeling complex signals, and their higher-order derivatives. We used
an Adam optimizer [Kingma & Ba (2014)] with default parameters. A starting LR (Learning Rate)
of 4e-3 was used such that it was reduced every 100 epochs by the factor of 0.7 until the min LR
reached 5e-5.
We found that the weight initialization [Katanforoosh & Kunin (2019)] was important for conver-
gence. In our 1D experiments a Gaussian initialization with 0.0 mean and 1.0 std was used. In
the 2D experiments, we used the standard initialization proposed in [Sitzmann et al. (2020)]. Our
hyperparameters were set as follows: α = 1e-1,μ = 1e-1,δ = 5e-1,β = 1.5, c = 1,ρ =
1e-8, Yi = ：, ν = 2. Our point set consisted of 45, 000 inner-points, and 1200 boundary points. In
the one-dimensional case, the boundary conditions had two actual points. In the two-dimensional
case, points were taken along the boundary of the square. Instead of using an exact kT uk∞, we used
a relaxed approximation: K Ek∈top (ILj∣) |Lk |, With K = 40 for broader effect of the L∞ fidelity
term.	K
8
Under review as a conference paper at ICLR 2021
Table 4: Dimension= 2, Eigenvalue Error Analysis
M	GT	Predicted	MAE	MRE	Rayleigh Quotient std
1	2	2.01	0.0073	0.0037	0.0170
1	2	1.99	0.05	0.01	0.0432
	5	4.91			
3	2	1.94	0.07	0.019	0.0603
	5	5.08			
	5	4.93			
4	2	1.98	0.0490	0.0093	0.0234
	5	4.93			
	5	4.96			
	8	7.91			
Laplacian Matrix					
4	2	1.96	0.1	0.02	
	5	4.90			
	5	4.90			
	8	7.84			
Table 5: Comparison to the Inverse Power Method where λgt = 2
	λ	PSNR	SSIM	RMSE
Inverse power method	2.02	20.57	0.99	8.76e-3
Proposed	2.01	36.67	0.93	2.14e-4
9 Conclusion
Interesting works have been recently published in the field of solving ODEs and PDEs with NNs
[Raissi et al. (2017); Flamant et al. (2020); Bar & Sochen (2019); Sirignano & Spiliopoulos (2018)].
We propose a novel method to exploit these new innovations, along with specific constraints that
exist in the mathematical nature of such problems, and shed some light on the well-known eigen-
value problem. Finding eigenpairs of both linear and non-linear differential operators is a problem
that appears in many different areas in science and in many problems that arise in nature [Davydov
(1976); Lpez-Blanco et al. (2013)]. Due to the possible complex nature of such operators, finding
a robust method for solving this problem is challenging. Additionally, most such problems do not
necessarily have a rectangular domain, and will therefore make the task more difficult across meth-
ods. Our method fits any free-form domain, and is able to alleviate these constraints. Furthermore,
the proposed method can be used in principle on higher order, non-linear operators and on higher
dimensional manifolds. When using the Rayleigh quotient to learn both the eigenfunction and its
corresponding eigenvalue, the end-to-end learning process seems to assist the convergence, each
acting as a constraint towards the other. When learning multiple eigenpairs at once, the orthogonal-
ity term eliminates degeneration. Empirically, it seems that these constraints also assist both the efs
and the evs to converge towards more accurate solutions. This suggests that the orthogonality term
serves as a regularizer on each eigenpair during the training process.
The training process produces smooth solutions, due to the fact that the networks are composed of
linear layers and smooth activation functions (namely tanh and SIREN). Since these solutions are
deterministic and parameterized by the learned weights, they are infinitely differentiable. The fact
that the solution is given in an explicit form, enables an exact analytical differentiation. Rigorous
analysis of the approximation error and its relation to the network architecture and design are under
current study. Future research includes higher dimensions and non-linear operators.
9
Under review as a conference paper at ICLR 2021
References
L. Bar and N. Sochen. Unsupervised deep learning algorithm for PDE-based forward and inverse
problems. arXiv 1904.05417v1, 2019.
N. Benouhiba and Z. Belyacine. On the solutions of the (p,q)-laplacian problem at resonance.
Nonlinear Analysis, 77, 2013.
F. Bozorgnia. Convergence of inverse power method for first eigenvalue of p-laplace operator. Nu-
merical Functional Analysis and Optimization, 37, 2016.
R.	Bronson, G. B. Costa, and J. T. Saccoman. Chapter 4 - eigenvalues, eigenvectors,
and differential equations. In Linear Algebra (Third Edition), pp. 237 - 288. Academic
Press, third edition edition, 2014. ISBN 978-0-12-391420-0. doi: https://doi.org/10.1016/
B978-0-12-391420-0.00004-4. URL http://www.sciencedirect.com/science/
article/pii/B9780123914200000044.
R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. Duvenaud. Neural ordinary dif-
ferential equations. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp.
6571-6583. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7892- neural- ordinary- differential- equations.pdf.
K. Choo, A. Mezzacapo, and G. Carleo. Fermionic neural-network states for ab-initio electronic
structure. arXiv 1909.12852, 2019.
J. B. Conway. A Course in Functional Analysis. Springer-Verlag New York, 1985.
A. S. Davydov. Quantum mechanics. In Quantum Mechanics. Translated, edited, and with additions
by D. ter Haar (2nd ed.), pp. 237 - 288. Oxford: Pergamon Press, second edition, 1976.
S.	Eastman and D. Estep. A power method for nonlinear operators. Applicable Analysis, 86(10):
1303-1314, 2007. doi: 10.1080/00036810701667305. URL https://doi.org/10.1080/
00036810701667305.
M. A. Erickson, R. S. SMITH, and A. J. LAUB. Power methods for calculating eigenvalues and
eigenvectors of spectral operators on hilbert spaces. International Journal of Control, 62(5):
1117-1128, 1995. doi: 10.1080/00207179508921586. URL https://doi.org/10.1080/
00207179508921586.
T.	Feld, J. F. Aujol, G. Gilboa, and N. Papadakis. Rayleigh quotient minimization for absolutely
one-homogeneous functionals. Inverse Problems, 2019.
C. Flamant, P. Protopapas, and D. Sondak. Solving Differential Equations Using Neural Network
Solution Bundles. arXiv e-prints, art. arXiv:2006.14372, June 2020.
E. Hait-Fraenkel and G. Gilboa. Numeric solutions of eigenvalue problems for generic nonlinear
operators. arXiv 1909.12775, 2019.
J. Han, L. Zhang, and E. Weinan. Solving many-electron Schrodinger equation using deep neural
networks. Journal of Computational Physics, 2019.
J. Han, J. Lu, and M. Zhoh. Solving high-dimensional eigenvalue problems using deep neural
networks: A diffusion Monte Carlo like approach. arXiv 2002.02600v2, 2020.
J. Hermann, Z. Schatzle, and Frank Noe. Deep neural network solution of the electronic Schrodinger
equation. arXiv 1909.08423, 2019.
Katanforoosh and Kunin. Initializing neural networks, 2019.
D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. arXiv e-prints, art.
arXiv:1412.6980, December 2014.
A. Knyazev. Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned
conjugate gradient method. SIAM Journal on Scientific Computing, 23(2), 2000.
10
Under review as a conference paper at ICLR 2021
J.R. Lpez-Blanco, R. Reyes, J. I. Aliaga, R. M. Badia, P. Chacon, and E. S. Quintana-Orti. Ex-
ploring large macromolecular functional motions on clusters of multicore processors. Journal of
Computational Physics, 246, 2013.
G. Miller. Spectral Graph Theory and The Laplacian Paradigm, Lecture 27, 2016.
http://www.cs.cmu.edu/afs/cs/academic/class/15859n-f16/Handouts/
TrefethenBau/RayleighQuotient-27.pdf.
M. Ovsjanikov, M. Ben-Chen, J. Solomon, A. Butscher, and L. Guibas. Functional maps: a flexible
representation of maps between shapes. ACM Transactions on Graphics, 2012.
D. Pfau, J. S. Spencer, A. G. de G Matthews, and W. M. C. Foulkes. Solution of the many-electron
Schrodinger equation with deep neural networks. arXiv 1909.02487, 2019.
K. K. Pradhan and S. Chakraverty. Chapter four - finite element method. In Computational Struc-
turalMechanics,pp. 25 - 28. Academic Press, 2019. ISBN 978-0-12-815492-2. doi: https://doi.
org/10.1016/B978-0-12-815492-2.00010-1. URL http://www.sciencedirect.com/
science/article/pii/B9780128154922000101.
M. Raissi, P. Perdikaris, and G. Em Karniadakis. Physics Informed Deep Learning (Part
I): Data-driven Solutions of Nonlinear Partial Differential Equations. arXiv e-prints, art.
arXiv:1711.10561, November 2017.
O. Rippel, J. Snoek, and R. Adams. Spectral Representations for Convolutional Neural Networks.
arXiv e-prints, art. arXiv:1506.03767, June 2015.
M. H. Saad. 5 - formulation and solution strategies. In MARTIN H. SADD (ed.), Elasticity, pp.
83 - 102. Academic Press, Burlington, 2005. ISBN 978-0-12-605811-6. doi: https://doi.org/10.
1016/B978-012605811-6/50006-3. URL http://www.sciencedirect.com/science/
article/pii/B9780126058116500063.
J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE TRANSACTIONS ON PATTERN
ANALYSIS AND MACHINE INTELLIGENCE, 22, 2000.
J. Sirignano and K. Spiliopoulos. DGM: A deep learning algorithm for solving partial differential
equations. Journal of Computational Physics, 375:1339-1364, December 2018. doi: 10.1016/j.
jcp.2018.08.029.
V. Sitzmann, J. N. P. Martel, A. W. Bergman, D. B. Lindell, and G. Wetzstein. Implicit Neural
Representations with Periodic Activation Functions. arXiv 2006.09661, 2020.
11