Under review as a conference paper at ICLR 2021
Redefining the Self-Normalization Property
Anonymous authors
Paper under double-blind review
Ab stract
The approaches that prevent gradient explosion and vanishing have boosted the
performance of deep neural networks in recent years. A unique one among them
is the self-normalizing neural network (SNN), which is generally more stable than
initialization techniques without explicit normalization. The self-normalization
property of SNN in previous studies comes from the Scaled Exponential Linear
Unit (SELU) activation function. However, it has been shown that in deeper neural
networks, SELU either leads to gradient explosion or loses its self-normalization
property. Besides, its accuracy on large-scale benchmarks like ImageNet is less
satisfying. In this paper, we analyze the forward and backward passes of SNN
with mean-field theory and block dynamical isometry. A new definition for self-
normalization property is proposed that is easier to use both analytically and
numerically. A proposition is also proposed which enables us to compare the
strength of the self-normalization property between different activation functions.
We further develop two new activation functions, leaky SELU (lSELU) and scaled
SELU (sSELU), that have stronger self-normalization property. The optimal pa-
rameters in them can be easily solved with a constrained optimization program.
Besides, analysis on the activation’s mean in the forward pass reveals that the self-
normalization property on mean gets weaker with larger fan-in, which explains the
performance degradation on ImageNet. This can be solved with weight centraliza-
tion, mixup data augmentation, and centralized activation function. On moderate-
scale datasets CIFAR-10, CIFAR-100, and Tiny ImageNet, the direct application
of lSELU and sSELU achieve up to 2.13% higher accuracy. On Conv MobileNet
V1 - ImageNet, sSELU with Mixup, trainable λ, and centralized activation func-
tion reaches 71.95% accuracy that is even better than Batch Normalization.
1	Introduction
In recent years, deep neural networks (DNNs) have achieved state-of-the-art performance on dif-
ferent tasks like image classification (He et al., 2015; Zheng et al., 2019). This rapid development
can be largely attributed to the initialization and normalization techniques that prevent the gradient
explosion and vanishing. The initialization techniques (He et al., 2015; Xiao et al., 2018) initialize
the parameters in networks to have good statistical property at beginning, and assume that this prop-
erty can be more or less maintained throughout the training process. However, this assumption is
likely to be violated when the network gets deeper or is trained with higher learning rate. Hence,
normalization techniques are proposed to explicitly normalize the network parameters (Salimans &
Kingma, 2016; Arpit et al., 2016) or the activations (Ioffe & Szegedy, 2015b; Ulyanov et al., 2016)
during training. In particular, Batch Normalization (BN) (Ioffe & Szegedy, 2015a) has become
a standard component in DNNs, as it not only effectively improves convergence rate and training
stability, but also regularizes the model to improve generalization ability.
However, BN still has several drawbacks. First, when calculating the mean and variance, the ac-
cumulation must be done under FP32 to avoid underflow (Micikevicius et al., 2018). This brings
challenges when training neural networks in low bit width. Second, the performance degradation
under micro batch size also makes it more difficult to design training accelerators, as the large batch
size increases the memory size to store the intermediate results for backward pass (Deng et al., 2020).
Besides, Chen et al. (2020a); Wu et al. (2018) show that BN introduces considerable overhead.
The self-normalizing neural network (SNN) provides a promising way to address this challenge.
SNN initializes the neural network to have a good statistical property at the beginning just like
1
Under review as a conference paper at ICLR 2021
initialization techniques. However, the statistics deviation in forward and backward passes can be
gradually fixed during propagation, thus it is more robust to the deviation from initial properties
(Chen et al., 2020b). For instance, the mean and variance of output activations with SELU in Klam-
bauer et al. (2017) automatically converge the fixed point (0, 1). Chen et al. (2020b) analyze the
Frobenius norm of backward gradient in SNN activated with SELU. They reveal a trade-off between
the self-normalization property and the speed of gradient explosion in the backward pass, and the
hyper-parameters need to be configured according to the depth of the network. The resulting acti-
vation function, depth-aware SELU (dSELU), has achieved even higher accuracy than the original
configuration on moderate-scale datasets like CIFAR-10, and makes the SNN trainable on ImageNet.
However, in deeper neural networks, the dSELU gradually degenerates to ReLU and loses its self-
normalization property. Moreover, even with dSELU, the test accuracy on ImageNet with Conv
MobileNet V1 (Howard et al., 2017) is still 1.79% lower than BN (Chen et al., 2020b). Therefore,
we aim to answer the following three questions in this paper: 1). Is SELU the only activation
function that has self-normalization property? 2). If it is not, is there a better choice? And how do
we compare the strength of self-normalization property between different activation functions? 3).
Why the performance of SNN on ImageNet is less satisfying? Is there any way to alleviate that?
In this paper, we analyze the signal propagation in both forward and backward passes in serial deep
neural networks with mean-field theory (Poole et al., 2016) and block dynamical isometry (Chen
et al., 2020b). Our main theoretical results are summarized as follows:
•	We illustrate that an activation function would demonstrate self-normalization property if
the second moment of its Jacobian matrix’s singular values φ(q) is inversely proportional
to the second moment of its input pre-activations q, and the property gets stronger when
φ(q) gets closer to 1/q. A new definition of the self-normalization property is proposed
that can be easily used both analytically and numerically.
•	We propose leaky SELU (lSELU) and scaled SELU (sSELU). Both of them have an addi-
tional parameter, β, that can be configured to achieve stronger self-normalization property.
The hyper-parameters can be solved with a constrained optimization program, thus no ad-
ditional hyper-parameter relative to dSELU is introduced.
•	We reveal that models with larger fan-in have weaker normalization effectiveness on the
mean of the forward pass signal. This can be solved with explicit weight centralization,
mixup data augmentation (Zhang et al., 2018), and centralized activation function.
On CIFAR-10, CIFAR-100, and Tiny ImageNet, lSELU and sSELU achieves up to 2.13% higher test
accuracy than previous studies. On ImageNet - Conv MobileNet V1, sSELU with Mixup, trainable
λ, and centralized activation function achieves comparable test accuracy (71.95%) with BN. Besides,
we provide a CUDA kernel design for lSELU and sSELU that has only 2% overhead than SELU.
2	Related Work
In this section, we present an overview of existing studies on the self-normalizing neural networks
(SNN) as well as statistical studies on forward and backward signals in deep neural networks.
Self-normalizing Neural Network. Scaled Exponential Linear Unit (SELU) (Klambauer et al.,
2017) scales the Exponent Linear Unit (ELU) by a constant scalar λ. The λ and original parameter
α in ELU are configured such that the mean and variance of output activation have a fixed point
(0, 1). The authors further prove that this fixed point is still stable and attractive even when the
input activations and the weights are unnormalized. Chen et al. (2020b) investigate the fixed point
in backward gradient. They reveal that the gradient of SNN is exploding with the rate (1 + ) per
layer, where is a small positive value. The self-normalizing property gets stronger when is larger,
whereas the gradient will explode at a higher rate. Therefore, they propose the depth-aware SELU
in which the ≈ 1/L is used to derive the optimal α and λ in SELU for a network with depth L.
Statistical Analysis of Deep Neural Networks. Schoenholz et al. (2016); Poole et al. (2016);
Burkholz & Dubatovka (2018) investigate the forward activations under the limit of large layer width
with mean-field theory. They have identified an Order-to-Chaos phase transition characterized by
the second moment of singular values of the network’s input-output Jacobian matrix. The neural
network has good performance when it is on the border of the order and chaos phases. On the other
2
Under review as a conference paper at ICLR 2021
hand, Chen et al. (2020b) develop a very handy framework for analyzing the Frobenius norm of
gradient. They illustrate that the gradient norm equality is a universal philosophy behind various
different initialization, normalization techniques, and even some neural network structures. The
gradient norm equality means the Frobenius Norm of the gradient is more or less equal in different
layers so that the information flow in the backward pass can be well preserved. (Arpit & Bengio,
2020)
3	Self-Normalization Property
In this section, we formally define the self-normalization property under the problem formulation,
notations, and assumptions as follows.
Problem Formulation. Let’s consider aDNN with L layers. Each layer performs a linear transform
followed by a non-linear element-wise activation function f, i.e.
xl=f(hl), hl = Wlxl-1 + bl, l=1,...,L,	(1)
where xl ∈ RNl is the output feature vector of layer l, hl is the pre-activation vector, Wl is the
weight of fully-connected layer or the expanded doubly block circulant matrix (Sedghi et al., 2019)
of 2D convolution, bl is the vector of biases, and we denote the loss as L. Besides, without loss of
generality, for f and X 〜N(0, q), We have
(1 + δq)E [f2(x)] = E[(df(x)∕dx)2] E[x2],	⑵
Where δq is a function of q. FolloWing previous studies (Poole et al., 2016; Chen et al., 2020b), for
∀ l, We make the assumptions as folloWs:
Assumption 1 The mean of entries in Wl and bl are zero.
Assumption 2 With central limit theory, the entries in hi follow i.i.d. N(0, qι), qι = NNhThi.
Assumption 3 The eigenvalues of WlT Wl are independent with entries in hl-1.
Klambauer et al. (2017) first define the self-normalization property of a neural netWork as folloWs.
Definition 1 (Self-normalizing Neural Network) A neural network is self-normalizing if it possesses
a mapping g : Ω → Ω for each activation y that maps mean and Variancefrom one layer to the next
and has a Stable and attracting fixed point depending on (ω, T) in Ω. Furthermore, the mean and
the variance remain in the domain Ω, that is g(Ω) ⊆ Ω, where Ω = {(μ, ν)∣μ ∈ [μmin, μmaX],ν ∈
[νmin, νmax]}. When iteratively applying the g, each point within Ω converges to this fixed point.
This definition imitates the explicit normalization techniques like BN, Which ensures that the feed-
forWard signal is normalized. Based on Definition 1, Klambauer et al. (2017) propose the SELU:
f(x) = λ
x	if x > 0
αex - α if x ≤ 0
(3)
Besides, Klambauer et al. (2017) initialize the entries in Wi With N(0, 1∕Ni-1), so that the output
pre-activation Will have the same second moment of input activation. With the stable fixed points of
mean and variance around (0, 1), the optimal choice for λ and α can be derived from
22
Z f (Z) √7- dz = 0, Z	f2(Z) √7- dz = 1.
-∞	2π	-∞	2π
(4)
Furthermore, the authors prove that the fixed points for mean and variance are still attractive even
When the statistical properties of the parameters in the neural netWork deviate from the initial setup.
HoWever, the statistical fixed point in the forWard pass doesn’t necessarily lead to good dynamics
of gradient. Chen et al. (2020b) analyze the Frobenius norm of the gradient in neural netWorks
activated by SELU. With the same activation function shoWn in equation 3, their analysis shoWs that
the optimal λ and α can be configured by preserving the Frobenius norm of backWard gradient and
second moment of forWard activations With equations as folloWs:
∞	e-z22
L∞f (Z) √2∏dz = 1.
(5)
3
Under review as a conference paper at ICLR 2021
where E is a small positive constant, without which the only solution for equation 5 would be λ = √2
and α = 0, and the activation function degenerates back to ReLU with the initialization technique
proposed in He et al. (2015). Thus it will lose the self-normalization property. Conversely, a rel-
atively large E will bring stronger self-normalization property, but meanwhile make the Frobenius
norm of gradient explode with rate (1+E) per layer. Notably, the original configuration of SELU can
be obtained by setting E = 0.0716. Therefore, Chen et al. (2020b) assert that having E ≈ = could
bring a good trade-off between gradient norm stability and self-normalization property. Experiments
on CIFAR-10 and ImageNet show that the new configuration results in higher accuracy.
Inspired by Chen et al. (2020b), we formally redefine the self-normalization property as follows:
Definition 2 (Self-normalization Property) Given an activation function f, we define operand φ as
φ(q)=「(df√z))2e-z2dz.
J-∞ d d√qz ) √2π
(6)
If f satisfies:
φ(1) = 1 + E,
min(1, -) < φ(q) < max(1,-),
(7)
then we say f has the self-normalization property.
While the first two equations in equation 7 are identical to equation 5 that constructs fixed-points for
both the second moment of activations and the Frobenius norm of the gradient, the third one makes
these fixed points attractive, as we have the proposition as follows.
Proposition 3.1 (Strength of Self-normalization Property) Under all the three Assumptions and
Definition 2, we represent φ(q) as a linear interpolation between 1 and 1/q as follows.
φ(a) = J 1 + (I-Yq<I)(I∕q - I) q< 1	(8)
φ(q) = 1/q + γq>1(1 - 1/q) q> 1 .	(8)
where γq ∈ (0, 1) is a function of q. Then the following conclusions hold (Proof: Appendix A.2):
•	The self-normalization property gets stronger when γq<1 and γq>1 get closer to 0. In
particular we have ∣Yq<ι∣ ≈ ∣Yq>ι∣ ≈ | dφ(q) ∣q=ι + 1| when q is around 1.
•	For layer l, the gradient explodes under the rate (1 + δql), i.e.	Πli=1(1 +
δqi-I)Eh||需||2i = q0E h||∂L0||2i.
Proposition 3.1 is derived based on Assumption 1, whereas the mean of weight matrices may shift
during training. Fortunately, Proposition 3.2 shows that the deviation of the mean of forward activa-
tions can also be normalized by simply multiplying with the weight matrix.
Proposition 3.2 (Normalization of Mean) Under the assumption that the entries in the weight ma-
trix Wij are independent with the input activations, and their expectation has an upper bound μ,
i.e. ∀ i, j, E[wij] ≤ μ. Then we say multiplication with the weight matrix normalizes the mean if
μ < Niγ holds, where Nl-I is thefan-in of the current layer l. Moreover, the mean is scaled down
by ratio smaller than μNl-ι. (Proof: APPendixA.3)
4 Novel Self-Normalizing Activation functions
Proposition 3.1 reveals that f with φ(q) closer to 1∕q may have stronger self-normalization property.
Therefore, on the basis of SELU, we propose to add an additional hyper-parameter β that can be
configured to bring φ(q) closer to 1∕q and encode other interesting properties. As demos, we find
the following two activation functions are quite promising.
4
Under review as a conference paper at ICLR 2021
Scaled Scaled Exponential Linear Unit (sSELU). The sSELU is defined as follows
f(x) = λ
x	if x > 0
αeβx - α if x ≤ 0
(9)
The negative pre-activations are scaled by β before fed into the activation function. This design is
motivated by the observation that without the curvature provided by the exponential term αex, φ(q)
of SELU will be a constant value without self-normalization property.
Leaky Scaled Exponential Linear Unit (lSELU). The lSELU is defined as follows
f(x) = λ
x	if x > 0
αex + βx - α if x ≤ 0
(10)
which has an additional negative slope βx. This is inspired by the observation that leaky ReLU
helps to avoid the saturation of negative activations. Besides, Chen et al. (2020b) show that leaky
ReLU alone also improves the stability of the training process.
Figure 1: The φ(q)〜q and f(χ)〜X of SSELU (a) & (b) and ISELU (C) & (d) under different λ.
X
Determine the optimal λ, α, and β. Figure 1 shows that given proper parameters λ, α, and
β, our sSELU and dSELU can be configured to get closer to 1/q, which indicates stronger self-
normalization property. With the first conclusion in Proposition 3.1 and equation 7, the λ, α, and β
can be obtained by solving the optimization problem below when is provided:
min∣d≠ω∣
λ,α,β dq q=1
z2
∕*∞ C e 2
+ 1 , St φ(1) = 1 + e, J	f2(z) √-dz = 1, λ ≥ 1.	(11)
In particular, the constraint λ ≥ 1 is inspired by the argument in Klambauer et al. (2017) that “a
slope larger than one can increase the variance if it is too small in the lower layer”. In this paper, we
find that constraining λ ≥ 1 provides two other benefits. First, having λ ≈ 1 helps to maintain the
mean of the output activations around 0. Second, having larger λ slows down the gradient explosion
in the backward pass. The detailed discussion can be found in Appendix A.4.
Determine the . While Chen et al. (2020b) propose to have e < 1/L
to avoid gradient explosion, where L is the depth of the network, their
derivation is based on the assumption that δq ≈ 0 in equation 2. How-
ever, after taking the nonzero δq into consideration, our Proposition
3.1 shows that the rate is actually (1 +δq) rather than (1 +e). We plot
the relationship between (1 + δq) and q under different e in Figure 2.
First of all, because of the first term in equation 7, we have δq = e
when q = 1, this illustrates the intuition behind using (1 + e) to
characterize the rate of gradient explosion. Therefore, e ≈ 1/L is
still a good choice to arbitrarily determine e, especially for relatively
Figure 2: (1 + δq)〜q of
sSELU & lSELU under dif-
ferent e.
shallow networks. As lSELU and sSELU has relatively higher δq>1
in Figure 2, a e relatively smaller than that of dSELU may yield better performance. Last but not
least, in deeper neural networks, q has more chance to deviate from the fixed point q = 1, and δq
gets larger when q gets larger. Therefore, the trade-off between the strength of self-normalization
property and the speed of gradient explosion may become too complex to be captured by e ≈ 1/L,
and it might be more promising to determine the proper e on the validation set.
5
Under review as a conference paper at ICLR 2021
5 Large-Scale Self-Normalizing Neural Network
Chen et al. (2020b) evaluate SELU and dSELU on Conv MobileNet V1 (Howard et al., 2017) on Im-
ageNet. While SELU suffers from gradient explosion, the accuracy of dSELU is 1.79% lower than
the BN baseline. We observe that there are two major reasons behind this performance degradation
and propose several tricks that improve the performance on large-scale SNNs.
Nonzero Mean in the Forward Pass. Proposition 3.2 reveals that the nonzero mean can be dimin-
ished by multiplying with the weight matrices when μ < n^. On small-scale SNNs, as Nj is
relatively small, this condition is easy to satisfy, and we don’t have to worry about the deviation
of the mean from 0. However, in large-scale SNNs for large datasets like ImageNet, larger fan-in
is required to ensure the network has enough parameters to model the more complex problem. In
Appendix A.5, We empirically show that models with larger fan-in tend to have larger μN1-1, which
implies weaker self-normalization property on the mean. As our Proposition 3.1 is based on As-
sumption 1, a greatly biased mean may violate the assumption. As a result, for large-scale SNNs,
we have to consider the influence of nonzero-mean.
While the influence of the weight matrix on the mean is well captured by Proposition 3.2, the in-
fluence of the activation function is more complex. In particular, for layer l, we assume the pre-
activations follow i.i.d. N(E[hl], σ2), and the output mean can be computed with
E[xl]
Z∞
∞
f(x)7^= e-
√πσ
(X-E[hl])2
2σ2 dx
(12)
We plot the relationship in Figure 3, in which the solid line represents the theoretical value and the
dash line is the value measured via numerical experiments. When the variance σ2 is large, there will
be a positive bias on the mean of output. The explanation is quite intuitive: the saturated region in
the negative axis has an asymmetric growth rate compared with the positive axis. Hence, when the
variance is large, the positive part contributes more than the negative part, which increases the mean.
SELU	dSELU	sSELU	ISELU
-10	1	-10	1	-10	1	-10	1
E[hl]	E[hl]	E[hl]	E[hl]
Figure 3: The absolute value of the output mean under different input mean and variance.
Lack of Regularization during Training. Luo et al. (2018) show that batch normalization also
regularizes the training process. In particular, using the statistics of minibatch μB and σB for explicit
normalization introduces additional Gaussian noise that regularizes the training process, and it also
discourages the reliance on a single neuron and penalizes correlations among neurons. However,
activation functions with the self-normalization property don’t have these features as they do not
rely on the statistics from minibatch.
Based on the analysis above, we find that three techniques can be used to improve the performance
of large-scale SNNs: mixup data augmentation (Zhang et al., 2018), weight centralization, and
centralized activation functions.
Mixup Data Augmentation. The mixup is a simple data augmentation routine that constructs
virtual training examples via linear interpolation: xe = γxi + (1 -γ)xj, ye = γyi + (1 -γ)yj, where
(xi, yi) and (xj, yj) are two training samples randomly drawn from the training set, γ ∈ (0, 1). In
particular, we find that using Mixup with SNN brings two benefits.
First, Mixup reduces the variance / second moment of the inputs. Under the assumption that the
corresponding entries xi and xj in the two samples are independent and E[xi2 ] = E[xj2 ] := E[x2 ],
E[xi] = E[xj] = 0, we have E (γxi + (1 - γ)xj)2 = (γ2 + (1 -γ2 ))2 E[x2 ]. For instance, when
γ = 0.7, the second moment of the sample entries is 0.58 of the original training samples, hence
the variance of the input samples is implicitly decreased. With a smaller q in the first few layers, on
6
Under review as a conference paper at ICLR 2021
one hand, as shown in Figure 2, a smaller second moment q leads to smaller δq , which reduces the
gradient explosion rate in the backward pass. On the other hand, as shown in Figure 3, a smaller
variance also reduces the shift of output mean caused by the activation function.
Second, Mixup creates additional training samples from the dataset, which provides additional reg-
ularization that could further boost the accuracy. The same property is also used in Zhang et al.
(2019). Besides, we empirically find that making λ trainable is also helpful when applying lSELU
and sSELU to large datasets like ImageNet. The trainable λ can be viewed as the scalar multiplier
initialized at 1 used in Zhang et al. (2019). Together with the bias of each layer, they serve as
the affine transform applied in batch normalization (Ioffe & Szegedy, 2015a), which increases the
representational power of the network (Michalski et al., 2019).
Weight Centralization. When μ < 1∕Nι-ι, multiplication with the weight can effectively nor-
malize the mean of activations. Therefore, we can explicitly centralize the weights, i.e W =
W - mean(W). As the weights are usually much smaller than the feature maps, the overhead
of Weight Centralization is usually quite small. Moreover, as it doesn’t rely on the batch, Weight
Centralization can still be utilized under micro-batch scenarios.
Centralized Activation Function. When the network with a large fan-in is relatively shallow, we
can trade the strength of self-normalization property with the deviation of the mean caused by the
activation function. While φ(1) = 1 + , E [f (x)] = 0, and E[f2(x)] = 1 can not simultaneously
hold in SELU and dSELU as they only have two parameters, the λ, α, and β in our sSELU and
lSELU can be solved with
z2	z2
φ(1) = 1 + e, E[f(x)] = /	f(z)e-ɪdz = 0, E[f2(x)] = /	f2(z)e-ɪdz = 1, (13)
-∞	2π	-∞	2π
which ensures that the output activations still have zero-mean when the input is at the fixed point.
6 Experiments
In this section, we validate our activation functions on multiple image classification benchmarks. In
Appendix B, we present an efficient CUDA kernel design, under which the overhead of lSELU and
sSELU are only 2% higher than SELU. The experiment setup is in Appendix C and the value of the
parameters λ, α, β, and the resulting γq=1 are summarized in Appendix D.
6.1	Normalization Effectivenes s
We empirically show that our new activation functions have better normalization effectiveness than
existing studies, which is demonstrated by the second moment of the output pre-activation of each
convolutional layer (E[h2]) and the FrobeniUs norm of the gradient of the weight (|| ∂L ||f ).
-0.5
SELU	dSELU	sSELU	ISELU
2.0
1.5
1.0
0.5
0.0
2.0
i 1-5
5 1.0
B
W 0.5
S>
0.0
0.0
Layer
Layer
Layer
Layer
2.0
1.5
1.0
0.5
D.0
D.0
0.0
2.0
1.5
1.0
0.5
D.0
0.0
[	7
Figure 4: The distribution of the second-moment of forward pre-activation and the Frobenius norm
of backward gradient on weights.
As shown in Figure 4, in the forward pass, sSELU and lSELU normalize the second moment in
the forward pass better than dSELU. In the backward pass, compared with SELU and dSELU, both
sSELU and lSELU have much flatter and more concentrated distribution of the Frobenius norm in
the backward pass. Notably, SELU has e ≈ 0.0716, and higher e lead to stronger self-normalization
property. This explains why it also has good dynamics in the forward pass. However, e = 0.0716
7
Under review as a conference paper at ICLR 2021
also increases the speed of gradient explosion, which explains why SELU has worse backward
dynamics. Last but not least, the further the E[h2] deviates from 0 in the forward pass, the faster the
|| ∂∂WL ||f increases in the backward pass. As larger q = E[h2] will lead to larger δq, this observation
justifies the second conclusion in Proposition 3.1.
6.2	Moderate-Scale Datasets
We summarize the results on CIFAR-10, CIFAR-100, and Tiny ImageNet in Table 1.
Table 1: Test accuracy on CIFAR-10, CIFAR-100, and Tiny ImageNet (Cl=95%).
Method	ε	CIFAR-10	CIFAR-100	Tiny ImageNet
BN+ReLU		85.64% ± 1.15%	52.60% ± 1.98%	43.45% ± 0.58%
SELU(Klambauer et al., 2017)		85.69% ± 1.10%	55.08% ± 0.37%	45.50% ± 0.47%
dSELU(Chen et al., 2020b)	0.01	86.07% ± 0.40%	54.09% ± 0.79%	45.63% ± 0.49%
	0.017	86.39% ± 0.37%	53.92% ± 0.88%	46.92% ± 0.96%
	0.03	86.93% ± 0.61%	54.49% ± 0.56%	46.45% ± 0.33%
	0.05	85.99% ± 1.30%	54.33% ± 0.33%	47.01% ± 0.36%
	0.07	86.88% ± 0.48%	54.83% ± 0.29%	46.55% ± 0.59%
sSELU (Ours)	0.01	87.18% ± 0.38%	57.21% ± 0.50%	46.26% ± 0.49%
	0.017	87.29% ± 0.20%	56.30% ± 0.79%	46.67% ± 0.98%
	0.03	87.25% ± 0.26%	56.15% ± 0.70%	46.73% ± 0.46%
	0.05	86.09% ± 1.14%	55.43% ± 0.85%	46.82% ± 0.67%
	0.07	86.99% ± 0.33%	55.06% ± 0.39%	46.69% ± 0.64%
ISELU (Ours)	0.01	83.19% ± 0.23%	54.61% ± 0.82%	44.77% ± 0.38%
	0.017	85.80% ± 0.28%	55.51% ± 0.99%	46.46% ± 0.66%
	0.03	86.69% ± 0.84%	56.78% ± 0.44%	47.80% ± 0.58%
	0.05	86.15% ± 1.29%	56.35% ± 0.71%	47.71% ± 0.34%
	0.07	86.64% ± 0.47%	54.47% ± 1.59%	47.27% ± 0.45%
First of all, under most of , our lSELU and sSELU are comparable or even better than dSELU. In
particular, sSELU achieves consistent accuracy improvement on CIFAR-10 and CIFAR-100, while
lSELU has better performance on Tiny ImageNet. Second, the results show that ≈ 1/L ≈ 0.017
is not always the best choice for dSELU, lSELU, and sSELU, but the best accuracy achieved in
our sSELU and lSELU are under relatively smaller than dSELU (Chen et al., 2020b). These two
observations accord with our arguments in Section 4 on the selection of proper .
6.3 Large Scale SNN
In this part, we evaluate our conclusions in Sec-
tion 5. First, adding Weight Centralization or
Mixup successfully solve the gradient explo-
sion problem in SELU, lSELU, and sSELU.
Second, in dSELU, lSELU, and sSELU, mak-
ing λ trainable brings additional performance
improvement than using Mixup alone. More-
over, after relaxing the constraint λ ≥ 1 to
λ ≥ 0.5, the test accuracy “lSELU (λ ≥
0.5)+Mixup” drops by 8.25%, this demon-
strates the importance of constraining λ to be
no less than 1. Last but not least, by combining
centralized lSELU and sSELU with Mixup and
trainable λ, we achieve 71.82% and 71.95%
top-1 accuracy.
7 Conclusion
Table 2: Test accuracy under different configura-
tions on ImageNet (Cl=95%).
Method	Test Acc.
BN + ReLU	71.42% ± 0.07%	=
BN + ReLU + Mixup	71.54% ± 0.36%	—
SELU(KIambaUer et al., 2017)	Explode in the first epoch
SELU + Weight Centralization	69.40% ± 0.09%
SELU + Mixup	71.37% ± 0.14%
dSELU(Chen et al.,2020b)	69.63% ± 0.10%	=
dSELU + Weight Centralization	69.11% ± 0.08%
dSELU + MixUP	71.65% ± 0.09%
dSELU + Mixup (trainable λ)	71.74% ± 0.09%	—
lSELU	Explode in the first epoch
lSELU + Weight Centralization	70.33% ± 0.05%
lSELU + Mixup	70.48% ± 0.06%
lSELU + Mixup (trainable λ)	71.55% ± 0.09%
lSELU (central) + Mixup (trainable λ)	71.82 ± 0.09%
lSELU (λ ≥ 0.5)+Mixup	63.30% ± 0.25%	—
SSELU	Explode in the first epoch
sSELU + Weight Centralization	69.07% ± 0.11%
sSELU + Mixup	71.42% ± 0.04%
sSELU + Mixup (trainable λ)	71.77% ± 0.09%
sSELU (central) + Mixup (trainable λ)	71.95% ± 0.07%	-
In this paper, we analyze the forward and backward pass signals in SNNs and redefine the self-
normalization property. Two novel activation functions, lSELU and sSELU, are developed under
this definition. A constrained optimization program is proposed to solve the optimal configurations.
Moreover, we reveal the reason behind the performance degradation of SNN under large fan-in, and
several solutions are proposed. With our novel methods, advanced results are achieved on multiple
benchmarks. Our study demonstrates a new research direction for the design of activation functions.
8
Under review as a conference paper at ICLR 2021
References
Devansh Arpit and Yoshua Bengio. The benefits of over-parameterization at initialization in deep
re{lu} networks, 2020. URL https://openreview.net/forum?id=rJggX0EKwS.
Devansh Arpit, Yingbo Zhou, Bhargava Kota, and Venu Govindaraju. Normalization propagation:
A parametric technique for removing internal covariate shift in deep networks. In Maria Florina
Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on
Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 1168-1176,
New York, New York, USA, 20-22 JUn 2016. PMLR. URL http://proceedings.mlr.
press/v48/arpitb16.html.
Rebekka Burkholz and Alina Dubatovka. Exact information propagation through fully-connected
feed forward neural networks. arXiv preprint arXiv:1806.06362, 2018.
Zhaodong Chen, Lei Deng, Guoqi Li, Jiawei Sun, Xing Hu, Ling Liang, Yufei Ding, and Yuan Xie.
Effective and efficient batch normalization using a few uncorrelated data for statistics estimation.
IEEE Transactions on Neural Networks and Learning Systems, 2020a.
Zhaodong Chen, Lei Deng, Bangyan Wang, Guoqi Li, and Yuan Xie. A comprehensive and modu-
larized statistical framework for gradient norm equality in deep neural networks. arXiv preprint
arXiv:2001.00254, 2020b.
Lei Deng, Guoqi Li, Song Han, Luping Shi, and Yuan Xie. Model compression and hardware
acceleration for neural networks: A comprehensive survey. Proceedings of the IEEE, 108(4):
485-532, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on computer vision, pp. 1026-1034, 2015.
Nicholas J Higham. The accuracy of floating point summation. SIAM Journal on Scientific Com-
puting, 14(4):783-799, 1993.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015a.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training
by reducing internal covariate shift. In Francis Bach and David Blei (eds.), Proceedings of
the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Ma-
chine Learning Research, pp. 448-456, Lille, France, 07-09 Jul 2015b. PMLR. URL http:
//proceedings.mlr.press/v37/ioffe15.html.
GUnter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Self-normalizing
neural networks. In Advances in neural information processing systems, pp. 971-980, 2017.
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng. Towards understanding regularization
in batch normalization. arXiv preprint arXiv:1809.00846, 2018.
Vincent Michalski, Vikram Voleti, Samira Ebrahimi Kahou, Anthony Ortiz, Pascal Vincent, Chris
Pal, and Doina Precup. An empirical study of batch normalization and group normalization in
conditional computation. arXiv preprint arXiv:1908.00061, 2019.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=r1gs9JgRZ.
Ben Poole, Subhaneil Lahiri, Maithra Raghu, Jascha Sohl-Dickstein, and Surya Ganguli. Exponen-
tial expressivity in deep neural networks through transient chaos. In Advances in neural informa-
tion processing systems, pp. 3360-3368, 2016.
9
Under review as a conference paper at ICLR 2021
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in neural information processing Systems, pp. 901-
909, 2016.
Samuel S Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep information
propagation. arXiv preprint arXiv:1611.01232, 2016.
Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers. In
International Conference on Learning Representations, 2019. URL https://openreview.
net/forum?id=rJevYoA9Fm.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
Shuang Wu, Guoqi Li, Lei Deng, Liu Liu, Dong Wu, Yuan Xie, and Luping Shi. l1-norm batch nor-
malization for efficient training of deep neural networks. IEEE transactions on neural networks
and learning systems, 30(7):2043-2051, 2018.
Lechao Xiao, Yasaman Bahri, Jascha Sohl-Dickstein, Samuel S. Schoenholz, and Jeffrey Penning-
ton. Dynamical isometry and a mean field theory of cnns: How to train 10, 000-layer vanilla con-
volutional neural networks. In ICML, pp. 5389-5398, 2018. URL http://proceedings.
mlr.press/v80/xiao18a.html.
Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup: Beyond em-
pirical risk minimization. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=r1Ddp1-Rb.
Hongyi Zhang, Yann N Dauphin, and Tengyu Ma. Fixup initialization: Residual learning without
normalization. arXiv preprint arXiv:1901.09321, 2019.
Wenzhao Zheng, Zhaodong Chen, Jiwen Lu, and Jie Zhou. Hardness-aware deep metric learning.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 72-81,
2019.
Denny Zhou, Mao Ye, Chen Chen, Tianjian Meng, Mingxing Tan, Xiaodan Song, Quoc Le, Qiang
Liu, and Dale Schuurmans. Go wide, then narrow: Efficient training of deep thin networks. arXiv
preprint arXiv:2007.00811, 2020.
10
Under review as a conference paper at ICLR 2021
A Proofs
A.1 Signal Propagation in Deep Neural Networks
For convenience, We denote the Jacobian matrix ”,-1) as D1-1, and tr(WιW∣τ)tr(Dl-IDD
as χl , where tr is the normalized trace.
Proposition A.1 (Forward Signal under Mean Field Theory) Under the formulation, notations,
and assumptions above, the evolution of the second moment of pre-activations ql∈[1,L] in the forward
pass can be described with
ql = q0πi=1 J + ,- , l = 1,…,L.
(14)
Proof. Under Assumption 1 & 2, the pre-activation vector of input in layer l can be characterized
with a Gaussian random variable X = √qlz, where Z is a random variable following N(0,1). With
these definitions, We can investigate hoW q evolves betWeen layer l - 1 and l:
ql =鼻(Wlf(hl-i) + bl)T (Wlf(hl-i) + bl)=忒 + ɪf(hl-i)TUTΛUf(hl-i),	(15)
Nl	Nl
where σb2 = 木bTbl, U is an Orthogonal matrix, and Λ is a diagonal matrix of eigenvalues in
WlT Wl . We characterize the diagonal entries in Λ with random variable λ whose probability den-
sity function is p(λ). With Assumption 3, we have
ql
_z2	z.	z.	_z2
√ ql-iz) √2=dz / λp(λ)dλ = σ2 +tr(WlWl,T) / f 2(√ ql-iz) √2=dz∙
(16)
Then, we substitute equation 2 into equation 16 which yields
C	-	CL z2	C
ql = σb + 三Ftr(WlWlT)“f0(√-1z)[2 -= dz =咪 + 三Ftr(WlWT)tr(D-ιD-ι).
1+δq-1	2=	1+δq-1	-
(17)
As the bias vector is usually initialized with zero and shared among multiple feature entries, σb
has a lower impact than the second term. Therefore, if we neglect the σb2, with the notation χl =
tr(WlWlT)tr(Dl-1DlT-1), we have
ql = qoπi=ι ι+δ—,	l = 1,∙∙∙,L∙
(18)
Proposition A.2 (Backward Gradient under Block Dynamical Isometry) Under the formulation,
notations, and assumptions above, the evolution of the Frobenius norm of the gradient in the back-
ward pass can be described with
≈ πi=1 X.
(19)
Proof. Given the gradient ∂∂∂L, with the chain rule, we have ∂h∂L~ = DT WlT ∂∂L and
焉=∏i=ιDT WT 联
(20)
In particular, we are interested in the Frobenius norm of 需 represented as ||∂L||2. According to
Chen et al. (2020b), its expectation can be computed with
(21)
Chen et al. (2020b) proves the theorem as follows.
11
Under review as a conference paper at ICLR 2021
Definition 3 Chen et al. (2020b) (kth Moment Unitarily Invariant) Let {Ai} := {A1 , A2..., AL}
be a series independent random matrices. Let {Ui} := {U1 , U3..., UL} be a series independent
haar unitary matrices independent of {A1, A2..., AL}. We say that (ΠiAi)(ΠiAi)T is the kth
moment unitarily invariant if ∀0 < p ≤ k, we have
tr ((CniAi)CniAi)T)p) = tr ((CniUiAi)CniUiAi)T)p) .	(22)
Theorem A.1 Chen et al. (2020b) (Multiplication). Given J := ni1=LJi, where {Ji ∈ Rmi×mi-1 }
is a series of independent random matrices. If Cni1=L Ji)Cni1=L Ji)T is at least the 1st moment
unitarily invariant (Definition 3), we have
tr ((ΠI=LJi)(∏1=LJi)T) ∏1=ltr (JiJiT) .	(23)
Therefore, equation 21 can be further simplified with Theorem A.1 as follows.
l 1----------------------=nl 1-.
BI tr(Wi WT )tr(Di-1DT-1)	i=1 Xi
(24)
A.2 Proof of Proposition 3.1
Proposition 3.1 (Strength of Self-normalization Property) Under Definition 2, we represent φCq)
as a linear interpolation between 1 and 1/q as follows.
φ( )= J 1 + (1 - γq<1)(Uq - 1) if q< 1	(25)
φCq) =	1/q + γq>1C1 - 1/q)	if q>1 .	(25)
where γq ∈ C0, 1) is a function of q. Then the following conclusions hold:
•	The self-normalization property gets stronger when γq<1 and γq>1 get closer to 0. In
particular, ∣γq<1∣ ≈ ∣γq>ι∣ ≈ |dφqq)∣q=ι + 1| When q is around 1.
•	For layer l, the gradient explodes under rate (1 + δqj, i.e. ∏i=1(1 + bq-)E | || ∂∂L || 2 ]=
q0E h||∂L0|同.
Proof. When γq<1 and γq>1 are approaching 0, φCq) gets closer to 1/q. With equation 6, We have
φCq) = trCDl DlT ) Where Dl is the Jacobian matrix of the activation function in layer l. As the
weights are initialized with N(0,志),we have tr(WιWT) = 1.
In the forward pass, with equation 2, we have qι+ι = / Φ(qι)qι. We can substitute equation 8
and get
(1 - qι+1	=	γq⅛(1 - ql + (1 -	1⅛)	if qι < 1	(26)
[qι+1 -1	=	ι+>-(qι - 1) - (1 -	ι+⅛q;)	if qι > 1	.
In the backward pass, with equation 14 and equation 19, we have
E h|| 黑 ||2i =	1∕qι
qoE h∣l舞闾	ni=ι(1 + δqi-ι).
Because of
F r||*||2] = E [隰间	=E h1|瑞|12i
Fdhι+J2]	tr(Dι DT )tr(Wι+ιW+ι)	φ(qι)
(27)
(28)
12
Under review as a conference paper at ICLR 2021
we have
{ Eh11 ∂⅛+ι||2
J q0Eh∣∣ ~∂L ∣∣2
I Eh∣∣∂⅛ι∣∣2
I qoEh∣∣∂⅛∣∣2'
where
_______1________________ι______________ 1+)； ι<ι(i∕qι-1)
πi=1(1 + δqa-1 ) ql + (1-γq%<1)(1-ql)_πi=1(1 + δqa-1 )
ι	ι	_ ι/史十(1-Yqi>1)(1-ι∕qz)
πi=1(1 + δqa-1 ) 1+Yqi>1Sl-I) =	πi = 1(1 + δqa-1 )
if qι < 1
if qι > 1
(29)
Yq i<ι
Yq i>ι
______Yql<1ql______
ql + (1-γqi<I)(I-qZ)
Yqi>1ql
1+γqz>1(qi-I)
∈ (0,1)
∈ (0,1)
(30)
(
are the monotonically increasing functions of Yqι<ι and Yqι>ι, respectively.
Similarly, we can derive how the deviation from the fixed point evolves during the back propagation.
Eh∣∣ ∂hL11∣2
I E[∣∣∂hM ∣∣2
1 -
q0E [∣∣BK01∣2
-1
(E[∣∣ 籥∣∣ 2]	-1∖
VoEh 11 籁 ∣∣2] J
(Eh∣∣ 湍 ∣∣ 2] ʌ
卜	qoEh ∣ ∣ ∂Lo ∣ ∣ 2] /
+ (1 - CYQ) ∏i=1(i+δj) - 1),
+ (1 - Yqi>1)(1 - ∏i=1(i+δq-1))，
if qι < 1
if qι > 1
(31)
First of all, when δqi are neglectable, equation 26 and equation 31 can be simplified as
1 - ql + 1 = γqι<1(1 - ql),
ql+1 - 1 = γqι>1(ql - 1),
Eh∣∣ G ∣∣ 2] _]_Y0	( Eh∣∣弱 ∣∣ 2]	_]、
qoEh ∣ ∣ 篇 ∣ ∣ 2]	"qι<1 VoEh ∣ ∣ 枭 ∣ ∣ 2] J
I - Eh∣∣ d⅛r∣∣ 2] _ Yo	(1 - Eh ∣∣ ⅞⅛∣∣ 2] ʌ
qoEh∣ ∣ ∂Lo∣∣ 2	zqι>1 ∖ qoEh∣∣∂⅛∣∣ 2] √
if ql < 1
.
if ql > 1
(32)
As Yqι<ι and Yq〉i are the monotonically increasing functions of Yqι<ι and Yqι>ι, it is obvious
that with smaller y5 <1 and Yqι>ι, the deviation from the fixed point in both forward and backward
passes shrinks faster.
In particular, when q is around the fixed point q _ 1 as ensured by the second term in equation 7,
we can approximate φ(q) and 1/q with their first-order Taylor expansion around q _ 1, with the
definition of Yqι<ι and Yqι>ι in equation 8, we have
1/(1+^g)-0(1+^q)〜]_|_ dφ(q)
-1∕(1+∆q)-1 —〜1 + ~W~
〜/	〜〜 一 0(1+Aq)-1/(1+Aq)〜1 I dφ(q)
Yq>1 ~ Tq>1 —	1∕(1-1+∆q)	〜1 + dq
+ ±i, if ∆q< 0
q=1	.	(33)
+ 盘,if ∆q> 0
q=ι
As a result, we can reduce the number of layers required to diminish the deviation by minimizing
I W Iq=I + 1|.
Then, we discuss the influence of δq. The fixed point of the two recursive functions in equation 26
and equation 31 can be computed as
1 - q _ T ° qι<1-------
1	1 + δqι<1-γq<1 ,
q-1
-δqι<1
1 + δqι<1-γq>1
Eh∣∣ ⅞⅛∣∣ 2]	_ I =	1	_ I
qoEh∣∣ 篇 ∣∣ 2] - 1 = ∏i=1(I+6q-1)- 1,
I _ Eh ∣∣ 弱∣∣ 2]	_ I___________1
1 - qoEh∣∣ 籁∣∣ 2] = 1 - ∏i=1(1+δqi-1),
if q < 1
if q > 1
(34)
While the fixed point of deviation slightly deviates from 0, in the backward pass, we have ∏i=1 (1 +
δqi-1)E 卜隔 lli] _ q0E [ll 薪 ||2],
at layer l.
which suggests that the gradient explodes with rate (1 + δqι )
A.3 Proof of Proposition 3.2
Proposition 3.2 (Normalization of Mean) Under the assumption that the entries in the weight
matrix Wij are independent with the input activations, and their expectation has an upper bound μ,
13
Under review as a conference paper at ICLR 2021
i.e. ∀ i, j, E[wij] ≤ μ. Then We say multiplication with the weight matrix normalizes the mean if
μ < n1^ holds, where N- is the fan-in of the current layer l. Moreover, the mean is scaled down
by ratio smaller than μN∣-ι
Proof. With equation 1, the j th entry in the output pre-activation hl can be computed with
Nl-1
hl,j =	wj,i xl-1,i .	(35)
i=1
Therefore, with the assumption on independence between weight and input activations, we have
Nl-1	1
E[hι,j] = X E[wj,iXi-ι,i] ≤ Ni-iμE ——ITxl-1 .	(36)
i=1	Nl-1
As the term E [n^ITxι-ι] can be viewed as the mean of the input activations, when μ < N,
equation 36 reveals that the mean is reduced after multiplying with the weight matrix.
A.4 B ENEFITS OF HAVING λ ≥ 1
First of all, we show that having λ ≈ 1 helps to maintain the mean of the output activations around
0. As we normalize the mean by multiplying with the weights, we don’t require the E[f (x)] = 0
when X 〜 N(0,1) like Klambauer et al. (2017). However, according to Proposition 3.2, the speed of
the mean converging to 0 gets slower when the expectation of entries in the weight matrix deviates
from 0 and when the fan-in gets larger. Therefore, it’s still ideal to avoid shifting the mean too much
when the activations flowing through the activation functions. As shown in Figure 5, we simulate
the forward pass in a 64-layer fully-connected neural network, and plot the distribution of output
activations in layer 1, 4, 16, and 64.
Figure 5: The distribution of output activations of sSELU and lSELU in different layers.
It is obvious that when λ < 1, a spike around 0 is observed for both sSELU and lSELU, and this
leads to a large negative mean of the output activations. On the other hand, for instance, by solving
φ(1) = 1 + ,
Z∞
∞
f(z)
(37)
0
under = 0.03, we have λ ≈ 1.0360 and 1.0362 for sSELU and lSELU, respectively. Therefore,
λ = 1 is a good starting point for the optimization.
14
Under review as a conference paper at ICLR 2021
Second, we show that having larger λ slows down the gradient ex-
plosion in the backward pass. According to the second conclusion in
Proposition 3.1, the backward gradient explodes under rate (1 + δq),
thus keeping δq low is critical for avoiding gradient explosion. Ac-
cording to the definition in equation 2, (1 +δq) can be computed with
1	l δ _	φ(q)
1+ δq =	ZZ2
qR∞ f2(z) e√∏ dz
Figure 6: Influence of λ on
1 + δq
(38)
We plot the relationship between the maximum (1 + δq) under q ∈
(0, 2] and λ in Figure 6. Obviously, the maximum (1 + δq) decreases
when λ gets larger. This observation is quite intuitive. The 1 + δi
characterizes the relative deviation between E[f2(x)] and E[(df(x)/dx)2]E[x2]. For the positive
pre-activations, we have
22
∞	e-Z-	∞	e—⅞-
E[f 2(x+)] = J	λ√qz √= dz = λ J	√qz √= dz = E[(df (x+)∕dx+)2]E [x+].	(39)
Hence, the deviation is contributed only by the negative part. With a larger λ, the positive activations
are scaled up, thus the negative activations have to be scaled down to preserve the overall second
moment. Therefore, the negative part contributes less to the overall second moment, and the relative
deviation between E[f2 (x)] and E[(df(x)∕dx)2]E[x2] gets smaller. All in all, a larger λ leads to
smaller δq, and a smaller δq reduces the gradient explosion rate (1 + δq).
A.5 THE μN∣-ι WITH INCREASING FAN-IN
Here We empirically illustrate that μNι-ι increases When the fan-in Nι-ι gets larger. The ex-
periment is performed on a 32-layer CNN activated with dSELU. We collect the μNι-ι of each
convolutional layer and the final fully-connected layer after each epoch among total 10 epochs. The
learning rate is set to 0.005. Let the number of input channels of layer l be cl , the k in each title
dSELU×k indicates that the number of input channel is scaled to k × cl.
μN∣-ι	μM∕-ι	μW∕-ι	μN∣-ι
Figure 7: The μNι-ι with Increasing Fan-in.
As shown in Figure 7, with larger fan-in, the layers tend to have larger μNι-ι. According to Propo-
sition 3.2, larger μNι-ι will lead to weaker self-normalization property on the mean. Notably, if
we further scaling the number of input channels with k greater than 4, gradient explosion happens.
These two observations justify our conclusion that the shift of mean is more influential in networks
with larger fan-in.
B	Efficient Implementation of lSELU and sSELU on GPU
In this section, we present an efficient implementation for lSELU and sSELU on GPU. In particular,
we take lSELU as an example, as the same strategy can be directly applied to sSELU.
15
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 1: Forward Kernel oflSELU.
Data: Input Feature: X ∈ RN, Output Feature: Y ∈ RN, λ,α,β ∈ R; ThreadIdx: t;
BlockIdx: b; Thread Block Size: T; Number of Thread Blocks: B;
begin
for i = b × T + t to N step B × T do
if X[i] > 0 then
L Y [i] = λ X x [i]
else
Y Y[i] = λ × (α × eX[i] + β × X[i] — α)
Forward Pass. The forward pass kernel for lSELU is shown in Algorithm 1. In the forward pass,
we have
1	if x > 0
y =	αex + βx - α	if x ≤ 0 .
(40)
The implementation is quite straightforward, all the threads stride across the feature map and
element-wisely compute the output activations. The input and output feature maps are treated as
1D array, therefore the kernel achieves good coalescing in both read and write. While we take
T = 1024, the number of thread blocks B is computed by B = b(N + T - 1)/T c, so the number
of thread blocks is large enough to achieve a high utilization rate.
Algorithm 2: Backward Kernel oflSELU.
Data: Gradient of Input Feature: GX ∈ RN, Gradient of Output Feature: GY ∈ RN; Input
Feature: X ; λ, α, β ∈ R; Gradient of λ: Gλ ∈ R; ThreadIdx: t; BlockIdx: b; Thread
Block Size: T; Number of Thread Blocks: B;
begin
float p = 0, float c = 0
for i = b × T + t to N step B × T do
float X = X [i], float dy, float y, float gy = Gy [i]
if X[i] > 0 then
L ddy = λ, y = gy × X - C
else
L ddy = λ × (α × ex + β), y = gy × (α × ex + β × X — α) — C
float t = p + y, c = (t - p) - y, p = t
LGx [i] = gy × dX
__syncthreads()
sum = BlockReduce(p)
if t = 0 then
L atomicAdd(&Gλ, sum)
Backward Pass. The backward pass kernel is shown in Algorithm 2. When the λ is trainable, the
backward pass of lSELU is shown as follows:
∂L = ∂L∂y = ∂L λ∫ 1	if x> 0
∂x ∂y ∂x ∂y X α αex + β	if x ≤ 0
∂L ^X y ∂L
∂λ 二X λ∂y
(41)
As the ∂LL is used to compute both ∂L and ∂λ, 让 Can be cached in registers for data reuse (line 4).
When the threads stride through the whole feature map, each thread holds a partial sum in a private
register p (line 2). In order to avoid the underflow of floating point accumulation, Khan summation
algorithm (Higham, 1993) is applied (line 9). At last, we use the block reduction in the CUB library
to get the partial sum of the whole thread block, and the final result is atomically added to the ∂L.
Different from the forward pass, we choose B to be a few thousands (usually much smaller than
B = b(N + T - 1)/T c). The motivation behind this is that while it is large enough to keep all the
16
Under review as a conference paper at ICLR 2021
streaming multiprocessors busy, it is also small enough to keep most of the reduction on chip and
reduce the atomic transactions.
We evaluate our new CUDA kernels on NVIDIA V100 GPU. We randomly generate an input fea-
ture map with size of [512, 64, 56, 56] as the input of the activation function, and we compare the
kernel latency of forward and backward passes with the native SELU in PyTorch. The results are
summarized in the table below:
Table 3: Forward and Backward pass Latency of SELU and lSELU.
	Forward Pass	Backward Pass
SELU (Klambauer et al., 2017)	10.487 ms =	14.833 ms
lSELU	-10.571 ms-	15.284 ms
sSELU	10.570 ms —	15.260 ms
Compared with the original SELU, our new implementation with the trainable λ only increases the
latency by around 2%, which is neglectable. The reason behind this is that the latency of activation
functions are bounded by the DRAM bandwidth of GPU (Chen et al., 2020b), and the computation
units are underutilized. As our CUDA kernels don’t introduce additional DRAM access, it has low
impact on the latency.
C	Experiment Setup for Moderate- S cale Benchmarks
The experiments in Section 6.1 and 6.2 are based on a 56-layer Convolutional Neural Network shown
in Table 4. The H and W are 32 for CIFAR-10, CIFAR-100, and 64 for Tiny ImageNet. Following
Klambauer et al. (2017); Chen et al. (2020b), the weights in the convolving kernels are initialized
with i.i.d. N(0, c. ), where kh and kw are the height and width of the filters and Cin is the input
channels. The models are optimized with SGD with momentum=0.9, weight decay=0.0005.
Table 4: 56-layer Convolutional Neural Network.
	Out Size	Serial Network		
conv1	W × H	3 × 3,16, si		
block1	W × H		3× 3, 16, s1 3× 3, 16, s1	×8
ds1	当× H		3 × 3, 32, s2 3 × 3, 32, s1	×1
block2	当× H		3 × 3, 32, s1 3 × 3, 32, s1	×8
ds2	学× H		3 × 3, 64, s2 3 × 3, 64, s1	×1
block3	学× H		3 × 3, 64, s1 3 × 3, 64, s1	×8
	1 × 1	average pool, fc		
For Section 6.1, we train the model from scratch for 3190 iterations (10 epochs) on CIFAR-10
under the learning rate 0.015. The choice of learning rate is based on the observation that it is
large enough to simulate the fierce update of parameters but also small enough to avoid gradient
explosion. Following Chen et al. (2020b), we set = 0.017 for dSELU, lSELU, and sSELU. We
collect the second-moment of the output pre-activations of each convolutional layer as well as the
Frobenius norm of backward gradient on the convolving kernels in each iteration.
For Section 6.2, as the model has a relatively small fan-in, we directly apply sSELU and lSELU
without techniques mentioned in Section 5. Besides, We clip the gradient by ”[-2, 2]” for all the
experiments to increases stability. All the results are averaged among 4 independent runs to reduce
fluctuation. For CIFAR-10 and CIFAR-100, the models are trained with batch size 128 for 130
epochs with the initial learning rate set to 0.01, and decayed to 0.001 at epoch 80. For Tiny Ima-
geNet, the models are trained with batch size 64 for 200 epochs. The initial learning rate is set to
0.001, and decayed by 10 at epoch 130, 180.
17
Under review as a conference paper at ICLR 2021
For Section 6.3, following Chen et al. (2020b), we choose the Conv MobileNet V1 (Howard et al.,
2017). The “Conv” indicates that traditional convolutions rather than depthwise separable convo-
lution is used, as the latter one requires more epochs to converge (Zhou et al., 2020). The model
is trained for 90 epochs with batch size 512 under leaning rate 0.02 (decayed by 10× at epoch
60 and 75). Following Zhang et al. (2018), the γ for interpolation is drawn from Beta distribu-
tion Beta(0.7, 0.7). For all the experiments of dSELU, lSELU, and sSELU, we follow Chen et al.
(2020b) and set = 0.06.
D Comparison of Parameters between different Activation
Functions
We summarize the value of the parameters λ, α, β, and the corresponding γq under different
configurations in Table 5. According to Proposition 3.1, smaller γq=1 will lead to stronger self-
normalization property. As shown in Table 5, under the same , our sSELU and lSELU have lower
γq=1 compared with dSELU, this justifies our intuition that lSELU and sSELU can be configured
to have stronger self-normalization property. Second, the result shows that for each activation func-
tion, γq=1 increases when gets larger. However, as shown in Figure 2, larger also leads to larger
δq, which increases the speed of gradient explosion in the backward pass. Last but not least, for
experiments on MobileNet V1, our lSELU and sSELU under = 0.06 achieve approximately the
same γq=1 with SELU with ≈ 0.0716, whereas the latter one has a higher gradient explosion rate.
Table 5: λ, α, β, and the corresponding Yq under different configurations.
Method	J E J λ Ialel Yq=ι
56-layer CNN
SELU(Klambauer et al., 2017)		1.05	1.67		0.806
dSELU(Chen et al., 2020b)	0.01	1.37	0.48		0.973
	0.017	1.34	0.64		0.954
	0.03	1.27	0.89		0.919
	0.05	1.17	1.25		0.865
	0.07	1.06	1.64		0.810
sSELU (Ours)	0.01	1.00	4.09	0.31	0.905
	0.017	1.00	3.28	0.41	0.881
	0.03	1.02	2.46	0.58	0.855
	0.05	1.00	2.17	0.75	0.819
	0.07	1.06	1.94	0.92	0.794
lSELU (Ours)	0.01	1.00	0.66	0.65	0.914
	0.017	1.00	0.86	0.55	0.890
	0.03	1.00	1.14	0.39	0.857
	0.05	1.00	1.47	0.21	0.821
	0.07	1.00	1.74	0.07	0.794
MobileNet V1
SELU(Klambauer et al., 2017)	—	1.05	1.67		0.806
dSELU(Chen et al., 2020b)	0.06	1.12	1.44		0.837
sSELU	0.06	1.00	2.04	0.84	0.805
lSELU	0.06	1.00	1.61	0.14	0.807
sSELU (central)	0.06	1.05	1.79	0.89	0.817
lSELU (central)	0.06	1.05	1.54	0.08	0.818
E	Experiments on Fully-Connected Neural Networks
The performance of SELU proposed in Klambauer et al. (2017) is first demonstrated on fully-
connected neural networks. In this section, we compare our sSELU and lSELU against the orig-
inal SELU in a 64-layer fully-connected neural network on three typical datasets: UCI_miniboone,
UCLadult, and HTRU. The results are summarized in Table 6. As the neural network has 64 layers,
we only evaluate sSELU and lSELU at ∈ {0.01, 0.017, 0.03}. The results show that with all these
18
Under review as a conference paper at ICLR 2021
three , our sSELU and lSELU achieve consistent improvement over SELU, which further justifies
the effectiveness of our activation functions.
Table 6: Test accuracy on UCI_miniboone, UCLadult, and HTRU2 (Cl=95%).
Method	€	UCI_miniboone	UCLadult	HTRU2
SELU(Klambauer et al., 2017)		92.80% ± 0.13%	84.55% ± 0.35%	97.64% ± 0.26%
sSELU (Ours)	0.01	93.01% ± 0.30%	84.68% ± 0.23%	98.03% ± 0.16%
	0.017	92.96% ± 0.29%	84.66% ± 0.24%	97.73% ± 0.29%
	0.03	92.85% ± 0.17%	84.56% ± 0.27%	97.92% ± 0.34%
ISELU (Ours)	0.01	93.29% ± 0.11%	84.86% ± 0.18%	97.79% ± 0.20%
	0.017	93.19% ± 0.19%	84.63% ± 0.24%	98.03% ± 0.21%
	0.03	92.81% ± 0.23%	84.67% ± 0.25%	98.01% ± 0.20%
19