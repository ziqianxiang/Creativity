Under review as a conference paper at ICLR 2021
What About Taking Policy as Input of Value
Function: Policy-extended Value Function
Approximator
Anonymous authors
Paper under double-blind review
Ab stract
The value function lies in the heart of Reinforcement Learning (RL), which de-
fines the long-term evaluation of a policy in a given state. In this paper, we pro-
pose Policy-extended Value Function Approximator (PeVFA) which extends the
conventional value to be not only a function of state but also an explicit policy
representation. Such an extension enables PeVFA to preserve values of multiple
policies in contrast to a conventional one with limited capacity for only one policy,
inducing the new characteristic of value generalization among policies. From both
the theoretical and empirical lens, we study value generalization along the policy
improvement path (called local generalization), from which we derive a new form
of Generalized Policy Iteration with PeVFA to improve the conventional learning
process. Besides, we propose a framework to learn the representation of an RL
policy, studying several different approaches to learn an effective policy represen-
tation from policy network parameters and state-action pairs through contrastive
learning and action prediction. In our experiments, Proximal Policy Optimization
(PPO) with PeVFA significantly outperforms its vanilla counterpart in MuJoCo
continuous control tasks, demonstrating the effectiveness of value generalization
offered by PeVFA and policy representation learning.
1	Introduction
Reinforcement learning (RL) has been widely considered as a promising way to learn optimal poli-
cies in many decision making problems (Mnih et al., 2015; Lillicrap et al., 2015; Silver et al., 2016;
You et al., 2018; Schreck et al., 2019; Vinyals et al., 2019; Hafner et al., 2020). Lying in the heart of
RL is the value function which defines the long-term evaluation of a policy. With function approxi-
mation (e.g., deep neural networks), a value function approximator (VFA) is able to approximate the
values of a policy under large and continuous state spaces. As commonly recognized, most RL algo-
rithms can be described as Generalized Policy Iteration (GPI) (Sutton & Barto, 1998). As illustrated
in the left of Figure 1, at each iteration the VFA is trained to approximate the true values of current
policy, regarding which the policy are improved. However, value approximation can never be per-
fect and its quality influences the effectiveness of policy improvement, thus raising a requirement
for better value approximation (v. Hasselt, 2010; Bellemare et al., 2017; Fujimoto et al., 2018).
Since a conventional VFA only approximates the values (i.e., knowledge (Sutton et al., 2011)) for
one policy, the knowledge learned from previously encountered policies is not preserved and utilized
for future learning in an explicit way. For example in GPI, a conventional VFA cannot track the val-
ues of the changing policy by itself and has no idea of the direction of value generalization when
approximating the values of a new policy. In this paper, we propose Policy-extended Value Func-
tion Approximator (PeVFA), which additionally takes an explicit policy representation as input in
contrast to conventional VFA. PeVFA is able to preserve values for multiple policies and induces an
appealing characteristic, i.e., value generalization among policies. We study the formal generaliza-
tion and contraction conditions on the value approximation error of PeVFA, focusing specifically on
value generalization along the policy improvement path which we call local generalization. Based
on both theoretical and empirical evidences, we propose a new form of GPI with PeVFA (the right of
Figure 1) which can benefit from the closer approximation distance induced by local value general-
ization under some conditions; thus, GPI with PeVFA is expected to be more efficient in consecutive
value approximation along the policy improvement path.
1
Under review as a conference paper at ICLR 2021
兀2 ---►…
以 1
vi1
Vθ-ιC⅛)
Evaluation
Improvement
V%C⅛)	VO1(∕.1)
VθoC⅛)	v&d
,Approximation -------► Generalization
Figure 1: Generalized Policy Iteration (GPI) with function approximation. Left: GPI with con-
ventional value function approximator Vφ. Right: GPI with PeVFA V (Xn) (Sec. 3) where extra
generalization steps exist. The subscripts of policy ∏ and value function parameters φ, θ denote the
iteration number. The squiggle lines represent non-perfect approximation of true values.
兀2 ---► •…
Moreover, we propose a framework to learn effective policy representation for an RL policy from
policy network parameters and state-action pairs alternatively, through contrastive learning and an
auxiliary loss of action prediction. Finally, based on Proximal Policy Optimization (PPO), we derive
a practical RL algorithm PPO-PeVFA from the above methods. Our experimental results demon-
strate the effectiveness of both value generalization offered by PeVFA and policy representation
learning. Our main contributions are summarized as follows:
•	We propose PeVFA which improves generalization of values among policies and provide a
theoretical analysis of generalization especially in local generalization scenario.
•	We propose a new form of GPI with PeVFA resulting in closer value approximation along
the policy improvement path demonstrated through experiments.
•	To our knowledge, we are the first to learn a representation (low-dimensional embedding)
for an RL policy from its network parameters (i.e., weights and biases).
2	Background
2.1	Reinforcement Learning
We consider a Markov Decision Process (MDP) defined as hS, A, r, P, γi where S is the state space,
A is the action space, r is the reward function, P is the transition function and γ ∈ [0, 1) is the
discount factor. The goal of an RL agent is to learn a policy ∏ ∈ Π where ∏(a∣s) is a distribution of
action given state that maximizes the expected long-term discounted return. The state-value function
vπ(s) is defined in terms of the expected discounted return obtained through following the policy
π from a state s: vπ (s) = Eπ [Pt∞=0 γtrt+1 |s0 = s] for all s ∈ S where rt+1 = r(st, at). We
use Vπ = vπ (∙) to denote the vector of values for all possible states. Value function is determined
by policy π and environment models (i.e., P and r). For a conventional value function, policy is
modeled implicitly within a table or a function approximator, i.e., a mapping from only state to
value. One can refer to Appendix E.1 to see a more detailed description.
2.2	Extensions of Conventional Value Function
Schaul et al. (2015) introduced Universal Value Function Approximators (UVFA) that generalize
values over goals in goal-conditioned RL. Similar ideas are also adopted for low-level learning of
Hierarchical RL (Nachum et al., 2018). Such extensions are also studied in more challenging RL
problems, e.g., opponent modeling (He & Boyd-Graber, 2016; Grover et al., 2018; Tacchetti et al.,
2019), and context-based Meta-RL (Rakelly et al., 2019; Lee et al., 2020). General value function
(GVF) in (Sutton et al., 2011) are proposed as a form of general knowledge representation through
cumulants instead of rewards. In an unified view, each approach generalizes different aspects of the
conventional VFA, focusing on different components of the vector form Bellman equation (Sutton
& Barto, 1998) expanded on as discussed in Appendix E.2.
Concurrent to our work, several works also study to take policy as an input of value functions.
Harb et al. (2020) propose Policy Evaluation Networks (PVN) to approximate the objective func-
tion J(π) = Eso〜ρo [vπ(so)] of different policy π, where ρo is the initial state distribution. Later
2
Under review as a conference paper at ICLR 2021
in (Raileanu et al., 2020), Policy-Dynamics Value Function (PDVF) is proposed which takes both
policy and task context as additional inputs, for the purpose of value generalization among policies
and tasks so that to adapt quickly to new tasks. PDVF can be viewed as an integration of PVN and
task-specific context learning (Rakelly et al., 2019; Zhou et al., 2019a). Both PVN and PDVF con-
duct value approximation for a given collection of policies and then optimize policy with gradients
through policy-specific inputs (shorted as GTPI below) of well trained value function variants in a
zero-shot manner. We view this as a typical case of global generalization discussed further in Sec.
3. In contrast, we focus more on a local generalization scenario and utilize value generalization to
improve learning during standard GPI process with no prior policies given.
Closely related to our work, Faccio & Schmidhuber (2020) propose a class of Parameter-based Value
Functions (PVFs) which take policy parameters as inputs. Based on PVFs, new policy gradients
are introduced in the form of a combination of conventional policy gradients plus GTPI (i.e., by
backpropagating through policy parameters in PVFs). Beyond zero-shot policy optimization, PVFs
also utilize value generalization of PVFs in the iterative learning process. Our work differs with
PVFs at two aspects: first, we consider a general policy representation in our proposed PeVFA along
with a framework of policy representation learning, in contrast to parsing the policy parameters
directly; second, we do not resort to GTPI for the policy update in our algorithms and only utilize
value generalization for more efficient value estimation in GPI. Moreover, in these previous works,
it is not clear how the value generalization among policies can be and whether it is beneficial for
learning or not. In this paper, we study the condition of beneficial value generalization from both
theoretical and empirical lens.
3	Policy-extend Value Function Approximator
In this paper, we propose Policy-extended Value Function Approximator (PeVFA), an extension
of the conventional value function that explicitly takes policy (representation) as input. Formally,
consider a function g : Π → X = Rn in a general form that maps any policy π to a n-dimensional
representation χπ = g(π) ∈ X . The policy-extended value function V : S × X → R, defines the
values over state and policy space:
∞
V(s,X∏) = En EY trt+1∖s0
t=0
, for all s ∈ S , π ∈ Π.
(1)
s
When only one policy π is considered, V(s, χπ) is equivalent in definition to a conventional value
function vπ (s). Note that if the policy πω is explicitly parameterized and its parameters are used
as the representation, i.e., χπω = ω, PeVFA is equivalent to Parameter-based State Value Function
(PSVF) in PVF family (Faccio & Schmidhuber, 2020). Similarly, we can define policy-extended
action-value function Q(s, a, χπ). We use V(s, χπ) for demonstration in this paper.
The key point is, V(s, ∙) is not defined for a specific policy and is able to preserve the values of
multiple policies. With function approximation, a PeVFA is expected to approximate values among
policy space, i.e., {vπ (s)}π∈Π. Formally, with a finite parameter space Θ, we consider the optimal
parameter θ* with minimum approximation error over Ml possible states and policies as:
θ* = arg min F(θ,∏), where F(θ,Π) = max kVθ(π) 一 Vπk = max kVθ(∙,π) — vπ(∙)k∞,
θ∈Θ	π∈Π	π∈Π
where F(θ, Π) is the overall approximation error of Vθ which is defined as the maximum L∞
norm of value vector distance among Π, as a typical metric commonly adopted in study on value
estimation approximation and policy iteration (Kakade & Langford, 2002; Munos, 2003; Lagoudakis
& Parr, 2003). Therefore, the learning process of a PeVFA Vθ is θ → θ*.
As commonly recognized, a conventional value function approximator (VFA) can generalize the
values to unseen states or state-action pairs after properly training on known states and state-action
pairs under a specific policy. Beyond this, PeVFA provides an appealing characteristic that allows
values to be generalized among policies, evaluating new policies with the knowledge of old ones. In
the following, we study the characteristic of value generalization theoretically (Sec. 3.1) and provide
some empirical evidences (Sec. 3.2), from which we finally introduce a new form of GPI (Sec. 3.3).
3
Under review as a conference paper at ICLR 2021
Value Space
(a) Global generalization
Value Space
(assume0矿(崂=匹)	PolicyImProvement
%*(3- %*g+ι)∣∣
Vπ0	Vπ1	.... 匹	Vπt+1
ɑɔ Optimal Approximation (i.e., true VaIUeS)
Value Approximation
Value Generalization
p.s., fβ(∏^) = 11%⑺-%*⑺Il
(b) Local generalization
Figure 2: Illustrations of value generalization with PeVFA among policies. Each circle represents
the value function of a policy. (a) Global generalization: values can generalize to unlearned policies
(∏0 ∈ ∏ι) from already learned policies (∏ ∈ ∏o). (b) Local generalization: values of previous
policies ({πi}it=0) can generalize to the successive policy (πt+1) along policy improvement path.
3.1	Value Generalization among Policies
Concretely, we introduce two scenarios of value generalization: global generalization and local gen-
eralization, which are closely related to many RL problems. An illustration is show in Figure 2.
In this section, we first focus on a general two-policy case and study whether the learned value ap-
proximation of one policy can generalize to that of the other one. We propose a common form of
generalization approximation error (Theorem 1) and analyze the condition of generalization contrac-
tion (Corollary 1). Further, we go to the two generalization scenarios as illustrated in Figure 2 and
especially introduce the closer approximation distance along the policy improvement path induced
by PeVFA (Corollary 2), from which a more efficient form of GPI can be derived (Sec. 3.3).
Formally, consider two policies π1 and π2 and a PeVFA Vθ to approximate the values of them. For
convenience of demonstration, we consider an identical representation space (i.e., X = Π) and an
infinite parameter space Θ where zero approximation error can be achieved, i.e., F(θ*, Π) = 0. We
then define the approximation loss f of V for each policy ∏ ∈ Π as fθ(∏) = ∣∣Vθ(∏) - Vθ* (∏)k =
∣∣Vθ(∙, ∏) - Vθ* (∙, ∏)k∞ ≥ 0, which measures the value distance of θ to the optimal parameter at
some policy π. In the following, we analyze the generalization performance on value estimation of
unlearned policy π2 when PeVFA Vθ is only trained to approximate the values of policy π1 . We first
introduce the two assumptions below:
Assumption 1 (Contraction). The learning process P of value approximation for π1 is γ-
P(π1)
contraction, that Is fj(∏ι) ≤ γfθ(∏ι) where Y ∈ (0,1] and θ-----→ θ.
θ and θ are the parameters of PeVFA before and after some learning of ∏ι S values. We use f and
f as abbreviations for fθ and f@ below for the ease of demonstration. Recent works (Keskar et al.,
2017; Novak et al., 2018; Wang et al., 2018) suggest that generalization performance is related to
local properties of the model. We then assume following smoothness property for following analysis:
Assumption 2 (Smoothness). f is (or has) Lipschitz continuous (gradient/Hessian) at π1 with Lip-
schitz constant Lo, e.g., |f (π) — f (∏ι)∣ ≤ Lo ∙ d(π, ∏ι) for ∏ ∈ Π with some metric space (Π, d).
With above two assumptions, next we derive a general form of an upper bound of the generalized
value approximation for unlearned policy π2 (i.e., f (π2))) as follows:
Theorem 1. Under Assumption 1 and 2, when f(π1) ≤ f(π2), we have the following bound:
O ,	. ,	. . ,	^ .
f(∏2) ≤	γf(∏2)	+ M(∏1,∏2,L).	(2)
l~{^~}	、 {z /
generalized contraction locality margin
Ft r	r κ λ ι	ι	. ι	. ι	.	» √ /	τ ∖ τ τ/ ∖ ι P
The form of M depends on the smoothness property, e.g., M(∏1,∏2, L) = Lo ∙ d(∏, ∏ι) when f
is Lipschitz continuous. See Appendix A.1 for more instances of M when higher-order smoothness
properties (Nesterov & Polyak, 2006) are considered.
4
Under review as a conference paper at ICLR 2021
Proof. See Appendix A.1. The main idea is to chain the Lipschitz continuity upper bound (Assump-
tion 2), the contraction upper bound (Assumption 1), and the inequality of f.	□
Remark 1. The case f(π1) ≤ f(π2) considered in Theorem 1 can usually exist since only π1 ’s
values are learned. Under such circumstances, we suggest that the complementary case f(π1) >
f(π2) is acceptable since the approximation error of π2 is already lower than the trained one.
Theorem 1 provides a generalization upper bound for f(π2), as a generalized contraction on f(π2)
plus a locality margin term M which is related to π1 , π2 and the smoothness property L. Further,
we analyze the condition when a PeVFA can also obtain a contraction approximation for unlearned
policy π2 though only trained on π1 , as below:
Corollary 1. Followed by Theorem 1, consider f is Lipschitz continuous and f(π2) 6= 0, we have,
f(∏2) ≤ Ygf (∏2) where Yg = Y + ^0f*；：2. When f (∏2) ≥ L0,(二；"2), θ P(π→→ θ is also a
γg -contraction for value approximation of π2, with γg ∈ (0, 1].
Proof. Replace M with Lipschitz continuous upper bound and transform the RHS in Equation 2,
then let RHS not greater than f (∏2). See Appendix A.2 for complete derivation.	□
Remark 2. From the generalization contraction condition provided in Corollary 1, we can find that:
as i. γ → 0, or ii. d(∏ι, ∏2) T 0, or iii. Lo → 0, the contraction condition is easier to achieve (or
the contraction gets tighter), i.e., the generalization on unlearned policy π2 is better.
In the above, we discuss value generalization in a two-policy case, i.e., from one learned policy
to another unlearned policy. Global generalization in Figure 2(a) is an extension scenario of the
two-policy case where values generalize from a known policy set (π ∈ Π0) of which the values are
learned to unseen policies (π0 ∈ Π1). In this paper, we focus more on local generalization of PeVFA
as shown in Figure 2(b), Recall the GPI form shown in the left of Figure 1, at each iteration t, value
generalization from current policy πt to improved policy πt+1 is exactly the two-policy case we
discussed above. However, it is unclear how local generalization can impact the value estimation in
GPI (i,e., policy evaluation). To this end, we propose the following Corollary to see a connection
between local generalization of PeVFA and a closer value approximation distance:
Corollary 2. At iteration t in local generalization scenario of PeVFA (Figure 2(b)), if fθt (πt) +
fθt (∏t+ι) ≤ ∣∣Vθ*(∏t) - Vθ*(∏t+ι)k, then fθt (∏t+ι) ≤ ∣∣Vθt(∏t) - Vθ*(∏t+ι)∣∣.
Proof. Proof can be obtained by applying Triangle Inequality. See Appendix A.3.	□
Corollary 2 indicates that local generalization of PeVFA can induce a more preferable start point
(Vθt(πt+1)) which is closer to the optimal approximation target (Vθ* (πt+1)) than the conventional
one (Vθπt, equivalent to Vθt (πt) in definition) for policy evaluation process at iteration t + 1.
Remark 3. With closer start points Vθt (πt+1), policy evaluation (i.e., minimize fθt(πt+1)) can be
more efficient with PeVFA. One can assume an ideal case with perfect local generalization, where
policy evaluation is no longer necessary and policy improvement can be performed consecutively.
3.2	Empirical Evidences for Two Kinds of Generalization
Beyond theoretical discussion, we empirically investigate whether two kinds of generalization can
be achieved by a PeVFA neural network. First, we demonstrate global generalization in a continuous
2D Point Walker environment. We build the policy sets Π with synthetic policies, each of which is
a randomly initialized 2-layer tanh-activated neural policy network with 2 units for each layer. Here
we use the concatenation of all weights and biases of the policy network as representation χπ for
each policy π for demonstration, called Raw Policy Representation (RPR), while more advanced
policy representation methods will be introduced in Sec. 4. We rollout the policies in the environ-
ment to collect trajectories T = {τi}ik=0 and then obtain the dataset {(χπj , Tπj )}jn=0. We separate
the synthetic policy set into training set (i.e., known policies Π0) and testing set (i.e., unseen poli-
cies Π1). Figure 3(a) shows that a PeVFA trained with data collected by Π0 achieves comparable
testing approximation performance when evaluating values of policies in Π1 (the average MSE on
training/testing set is 2.909 and 4.155), as well as almost maintains the order of optimality among
policies. More experimental details are provided in Appendix B.1.
5
Under review as a conference paper at ICLR 2021
Average true V values
Average true V values
10	20	30	40
Steps (2e4 steps)
SOJ uolsUJ - XOJdd<
10	20	30	40	50
Steps (4e4 steps)
(a) Global generalization	(b) Local generalization
Figure 3: Empirical evidences of two kinds of generalization of PeVFA. (a) Global generalization:
PeVFA shows comparable value estimation performance on the testing policy set (red) after learning
on the training policy set (blue) while maintains reasonable order of optimality. (b) Local general-
ization: PeVFA (Vθ (χπ)) shows lower losses (i.e., closer distance to approximation target) than
convention VFA (Vφπ) before and after the training for values of successive policies along policy
improvement path, which demonstrates Corollary 2. The left axis is for approximation loss (lower
is better) and the right axis is for average return as a reference of the learning process of PPO policy.
We then demonstrate local generalization of PeVFA to examine the existence of Corollary 2. We
also use a 2-layer 8-unit policy network trained by PPO (Schulman et al., 2017) algorithm in Ope-
nAI MuJoCo continuous control tasks. Parallel to the conventional value network Vφπ (s) (i.e., VFA)
in PPO, we set a PeVFA network Vθ (s, χπ) with RPR as input. Compared with Vφπ, the PeVFA
networkVθ(s, χπ) is additionally trained with data from historical policies ({πi}it=0) along the pol-
icy improvement path. Note that Vθ (χπ) does not interfere with PPO training here, and is only
referred as a comparison with Vφπ on the approximation error to the true values (i.e., collected re-
turns) of successive policy πt+1. To see whether the local generalization exists and can be beneficial
or not, as in Figure 3(b), we illustrate the value approximation losses for Vφπ (s) (red curves) and
Vθ (s, χπ) (blue curves), before (solid) and after (dashed) updating with on-policy samples, at each
iteration during the learning process of PPO (green curves). The results demonstrate the existence
and benefits of local generalization of PeVFA in InvertedPendulum-v1 and Ant-v1. By comparing
approximation losses before updating (red and blue solid curves), we can observe that the good
generalization of value approximation for successive policies is obtained with PeVFA Vθ (s, χπ).
The approximation loss ofVθ(s, χπ) is almost consistently lower than that of Vφπ (s) before update,
providing the evidence of Corollary 2. For the dashed curves, it indicates that PeVFA can achieve
lower approximation loss than conventional VFA after updating with the same on-policy samples
and number of training. We emphasize the importance of the empirical evidence of beneficial lo-
cal generalization, based on which we assume that PeVFA can improve conventional GPI and then
propose our approaches in the following. See Appendix B.2 for complete results in 7 MuJoCo tasks
and more experimental details.
3.3	Reinforcement Learning with PeVFA
Since PeVFA extends the capacity of conventional VFA and induces value generalization among
policies, we expect to derive improved RL algorithms with PeVFA by utilizing the two kinds of
generalization we discussed in previous sections. In this paper, we focus on local generalization and
propose anew GPI form with PeVFA. As illustrated in the right of Figure 1, the key difference is that
the generalized value approximation Vθt (χπt+1) of successive policy πt+1 is taken as the start point
of policy evaluation at each iteration. One can see the difference by comparing Vθt (χπt+1) with
the conventional one Vφπt, which is equivalent to Vθt (χπt) in definition. From Corollary 2 and the
evidence in Figure 3(b), such generalized starting points are closer to approximation target (i.e., true
values vπ) in some cases, thus we suggest that these local generalization steps (gray arrows) help
to reduce approximation error with finite updates and improve the efficiency of the overall learning
process. See a pseudo-code of GPI with PeVFA (Algorithm 2) and more discussion in Appendix C.
For global generalization, we assume that one can obtain a high-quality evaluation of unseen policies
by training a PeVFA only on known policies. This indicates an appealing potential to circumvent
extra sample collection and training cost for policy evaluation in many problems, e.g., evolutionary
6
Under review as a conference paper at ICLR 2021
Figure 4: Framework of policy representation training. Policy network parameters (OPR) or policy
state-action pairs (SPR) are fed in policy encoder with permutation-invariant (PI) transformations,
producing representation Xn. Xn can be trained by gradients from PeVFA value approximation (i.e.,
End-to-End), as well as with optional auxiliary action recovery loss and unsupervised contrastive
learning, shown with separation by the gray dotted line.
strategy (Salimans et al., 2017), opponent modeling (He & Boyd-Graber, 2016) and policy adapta-
tion (Arnekvist et al., 2019; Raileanu et al., 2020), which are beyond the scope of this paper.
4	Policy Representation Learning
In the last section, we analyze value generalization among policies induced by PeVFA and propose
a new GPI for more efficient learning process. To derive practical deep RL algorithms, one key
problem is policy representation, i.e., a low-dimensional embedding of RL policy. This is differ-
ent from the notion of policy representation in some works (Zhou et al., 2019b; Ma et al., 2020),
which is related to parameterization of policy, usually advanced policy network models or architec-
tures. A usual policy network may have large number of parameters, thus making it inefficient and
even impractical to use Raw Policy Representation (RPR) for optimization and generalization (Harb
et al., 2020). To our knowledge, it remains unclear about how to obtain an effective and compact
representation for an RL policy.
We propose a framework of policy representation learning as shown in Figure 4. Our intuitions come
from answering the question: what makes a good policy representation, particularly when the policy
is a neural network. Our proposed policy representations are motivated by a geometric perspective
of the policy as a curved surface of the policy distribution; details are provided in the Appendix D.1.
Concretely, we provide two policy representations below:
•	Surface Policy Representation (SPR): The first way is to extract policy representation from
the state-action pairs (or trajectories), since they reflect the information about how policy
may behave under such states. We view this as scattering sample points on policy curved
surface, which should be able to capture the features as the number of samples increases.
•	Origin Policy Representation (OPR): Moreover, we propose a novel way to learn a low-
dimensional embedding from the policy network parameters directly. Generally, we con-
sider a policy network to be an MLP with well represented state features (e.g., features
extracted by CNN for pixels or by LSTM for sequences) as input and deterministic or
stochastic policy output. We then define a lossy mapping to compress all the weights and
biases of the MLP to obtain the corresponding OPR.
For both SPR and OPR, we use permutation-invariant transformations, i.e., mainly use MLP then
Mean-Reduce operation. The above two policy representations proposed are scalable to encode
most RL policies from different stream of policy data (as illustrated in Figure 4). In this paper, we
use OPR and SPR to encode the typical MLP policy. Implementation details and a discussion on
encoding sophisticated RL policy like that operates images can been seen in Appendix D.2.
The related work PVN (Harb et al., 2020) proposes Network Fingerprint to circumvent the difficulty
of learning policy representation from network parameters. However, it introduces additional non-
7
Under review as a conference paper at ICLR 2021
trivial optimization for a set of probing states. For PVFs (Faccio & Schmidhuber, 2020), all the
policy weights are simply parsed as inputs to the value function without embedding, even in the
nonlinear case. Besides, a few works also involve representation or embedding learning for RL
policy in Multiagent Learning (Grover et al., 2018), Hierarchical RL (Wang et al., 2020), Policy
Adaptation and Transfer (Hausman et al., 2018; Arnekvist et al., 2019; Raileanu et al., 2020; Harb
et al., 2020). We found almost all of them belongs to the scope of SPR. A detailed review for them is
provided in Appendix D.5. For OPR, our goal is to learn a representation of the policy itself from its
parameters rather than of the architecture of a policy network as is common in Neural Architecture
Search (NAS) (Zhou et al., 2019b; Ma et al., 2020).
The most straightforward way is to train policy representation χπ together with PeVFA end-to-
end as shown in Figure 4 by backpropagating the value approximation error through the policy
representation. Intuitively, a good representation should be able to predict the policy’s behavior. To
this end, we resort to an auxiliary loss to recover the actions of π from χπ under different states
(Grover et al., 2018; Raileanu et al., 2020). Concretely, an auxiliary policy decoder (or a master
policy) π(∙∣s,χ∏) is trained through behavioral cloning, i.e., to minimize cross-entropy objective
-P(S a)〜Tr log∏(a∣s, Xn). In addition, we propose to improve the representation learning of Xn
with unsupervised Contrastive Learning (Oord et al., 2018; Srinivas et al., 2020; Schwarzer et al.,
2020). In this way, policies are encouraged to be close to similar ones (i.e., positive samples) and to
be apart from different ones (i.e., negative samples) in policy representation space. For each policy,
we construct positive samples by data augmentation on policy data, depending on SPR or OPR
considered; and different policies along the policy improvement path naturally provide negative
samples for each other. The policy representation network is then trained with InfoNCE loss (Oord
et al., 2018) as commonly done in (He et al., 2020; Chen et al., 2020). More implementation details
are in Appendix D.
5	Experiments
We conduct several experiments with our proposed methods and try to answer the following ques-
tions: i. How much does the local generalization property of PeVFA benefit a deep RL learning
algorithm? ii. Is our policy representation learning framework effective to learn good representation
for a policy network? To answer the above questions, we derive a practical deep RL algorithm with
PeVFA based on Proximal Policy Optimization (PPO) (Schulman et al., 2017) to demonstrate and
evaluate GPI with PeVFA and our approaches of policy representation learning.
Setting. Due to the popularity and simplicity in implementation, we use PPO as the base algorithm
for a representative implementation of GPI with PeVFA. PPO is a policy optimization algorithm
that learns both a policy network π and a value network Vφ that approximates the values of π ,
typically following GPI in the left of Figure 1. The policy π is optimized with respect to a surrogate
objective (Schulman et al., 2015) using advantages calculated by Vφ and GAE (Schulman et al.,
2016). We propose to replace the conventional value network Vφ(s) by a PeVFA network Vθ(s, Xn)
with parameters θ as given in Figure 4, and perform GPI with PeVFA as in the right of Figure 1.
Without modifying the policy optimization process of vanilla PPO, we then obtain a new algorithm,
called PPO-PeVFA. Note that what is actually changed here is simply a drop-and-replacement of
the value function. The policy update scheme is the same for PPO and PPO-PeVFA, which means
that the policy of PPO-PeVFA is not updated by gradients through policy representation feeded
in Vθ(s, Xn). For all the experiments in this section, we use a normal-scale policy network with
2 layers and 64 units for each layer, rather than the small policy networks used in Sec. 3.2 for
demonstration. As a result, the number of policy network parameters can be over 10k for example
in Ant-v1, thus prohibiting the rationality of directly using RPR for PeVFA Vθ(s, Xn).
Training. The only difference of PPO-PeVFA is the training of policy representation Xn and PeVFA
network Vθ(s, Xn). For policy representation Xn, it depends on policy data used and training losses
adopted as introduced in Sec. 4. See Algorithm 3 in Appendix D.4 for an overall pseudo-code
of policy representation training. For PeVFA network Vθ(s, Xn), it is trained to approximate the
values of multiple policies along the policy improvement (i.e. optimization) path, as Algorithm 2 in
Appendix C. Both of above training use historical policies, whose data (i.e., network parameters and
state-action pairs) are stored. Note that for PPO (and PPO-PeVFA), new policies are only produced
after policy optimization process. We do not assume access to pre-collected policies and thus the
size of policy set increases from 1 during the learning process. In our experiments, about 1k to 2k
8
Under review as a conference paper at ICLR 2021
Table 1: Max Average Return over 10 trials of 2M time steps (4M for Ant-v1). First two maximum
values for each task are bolded. ± corresponds to half a std over trials.
Environments	Benchmarks		Origin Policy Representation (Ours)			Surface Policy Representation (Ours)		
	PPO	Ran PR	E2E	CL	AUX	E2E	CL	AUX
HalfCheetah-v1	2621 ± 259.31	2470 ± 291.65	3171 ± 427.63	3725 ± 348.55	3175 ± 517.52	2774 ± 233.39	3349 ± 341.42	3216 ± 506.39
Hopper-v1	1639 ± 294.47	1226 ± 348.10	2085 ± 310.91	2351 ± 231.11	2214 ± 360.78	2227 ± 297.35	2392 ± 263.93	2577 ± 217.73
Walker2d-v1	1505 ± 320.55	1269 ± 209.61	1856 ± 305.51	2038 ± 315.51	2044 ± 316.32	1930.57 ± 456.02	2203 ± 381.95	1980 ± 325.54
Ant-v1	2835 ± 152.04	2742 ± 71.11	3581 ± 185.43	4019 ± 162.47	3784 ± 268.99	3173 ± 184.75	3632 ± 134.27	3397 ± 200.03
InvDouPend-v1	9344 ± 11.02	9355 ± 0.40	9357 土 0.29	9355 ± 0.64	9355 ± 0.68	9355 ± 0.89	9356 ± 0.96	9355 ± 1.42
LunarLander-v2	219 ± 5.33	226 ± 2.83	238 ± 3.37	239 ± 3.70	234 ± 3.47	236 ± 3.13	234 ± 3.13	235 ± 5.70
policies are produced in an experimental trial. We use all historical policies in our training and for
the case with many more policies, other sophisticated training methods can be considered.
Results. We conduct the following experiments in OpenAI Gym MuJoCo continuous control tasks
(Brockman et al., 2016; Todorov et al., 2012). Beside the comparison to PPO, we consider an-
other benchmark, PPO-PeVFA with fixed randomly generated policy representation for each policy,
namely Ran PR. The overall experimental results are in Table 1. More experimental details are in
Appendix F.1 and complete learning curves are provided in Appendix F.2 due to space limitation.
Question 1. Can PPO-PeVFA with only End-to-End (E2E) policy representation outperform PPO?
From Table 1, we can find that both PPO-PeVFA with OPR and with SPR trained in E2E fashion
outperforms PPO in all 6 tasks, especially in Ant-v1. This indicates that effective policy representa-
tion can be learned from value approximation loss in our experiments, based on which PeVFA can
benefit from local generalization thus improve the learning process. Additionally, Ran PR shows an
overall degeneration compared to PPO, demonstrating the significance of OPR and SPR.
Question 2. Can representation learned through contrastive learning (CL) and auxiliary loss (AUX)
further benefit performance ofPPO-PeVFA?
For both OPR and SPR, We observe consistent improvements in-
duced by CL and AUX over E2E, which means that additional
learning to maintain unsupervised consistency (CL) and capture
policy behavior (AUX) further benefits policy representation,
and eventually results in substantial improvement over PPO in
all tasks except for the first two simple tasks. In an overall view,
OPR slightly outperforms SPR and as CL does over AUX. We
hypothesize that it is due to the stochasticity of state-action pairs
which serve as inputs of SPR and training samples for AUX. We
suggest that this remains space for future improvement.
«00
3000
2000
1000
0
Figure 5: t-SNE visualization
for OPR (E2E) of 6k policies
from 5 trials (denoted by differ-
ent markers) in Ant-v1. Policies
are colored by average return.
Question 3. How is the learned representation like when view-
ing in a low-dimensional space?
We show a 2D t-SNE (Maaten & Hinton, 2008) view in Figure
5 and more visualisations in Appendix F. Policies from differ-
ent trials are locally continuous (multimodality) while a global
evolvement emerges with respect to policy performance.
6	Conclusion and Future Work
In this paper, we propose Policy-extended Value Function Approximator (PeVFA) which induces
value generalization among policies. We propose a new form of GPI with PeVFA which can po-
tentially benefit from local value generalization along the policy improvement path. Moreover, we
propose several approaches to learn an effective low-dimensional representation of an RL policy.
Our experiments demonstrate the effectiveness of the generalization properties of PeVFA and our
proposed methods of representing the RL policy.
Our work opens up new ways to potentially improve the policy learning process from global or local
perspectives using PeVFAs that receive an explicit policy representation to improve generalization
of the value approximation. We believe our work can guide future research directions on policy
9
Under review as a conference paper at ICLR 2021
representations and PeVFA in many RL problems. We plan to further study policy representation
learning, as well as more complex value generalization scenarios, including TD learning with non-
stationary approximation targets. In addition, optimizing policies in the representation space can
also be an interesting direction.
References
M. Andrychowicz, D. Crow, A. Ray, J. Schneider, R. Fong, P. Welinder, B. McGrew, J. Tobin,
P. AbbeeL and W. Zaremba. Hindsight experience replay. In NeurIPS,pp. 5048-5058, 2017.
M. Andrychowicz, A. Raichuk, P. Stanczyk, M. Orsini, S. Girgin, R. Marinier, L. Hussenot,
M. Geist, O. Pietquin, M. Michalski, S. Gelly, and O. Bachem. What matters in on-policy re-
inforcement learning? A large-scale empirical study. CoRR, abs/2006.05990, 2020.
I.	Arnekvist, D. Kragic, and J. A. Stork. VPE: variational policy embedding for transfer reinforce-
ment learning. In ICRA, pp. 36-42, 2019.
M. G. Bellemare, W. Dabney, and R. Munos. A distributional perspective on reinforcement learning.
In ICML, volume 70, pp. 449-458, 2017.
G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai
gym. CoRR, abs/1606.01540, 2016.
T. Chen, S. Kornblith, M. Norouzi, and G. E. Hinton. A simple framework for contrastive learning
of visual representations. CoRR, abs/2002.05709, 2020.
L. Engstrom, A. Ilyas, S. Santurkar, D. Tsipras, F. Janoos, L. Rudolph, and A. Madry. Implementa-
tion matters in deep RL: A case study on PPO and TRPO. In ICLR, 2020.
F. Faccio and J. Schmidhuber. Parameter-based value functions. CoRR, abs/2006.09226, 2020.
H. Fu, H. Tang, J. Hao, C. Chen, X. Feng, D. Li, and W. Liu. Towards effective context for meta-
reinforcement learning: an approach based on contrastive learning. CoRR, abs/2009.13891, 2020.
S. Fujimoto, H. v. Hoof, and D. Meger. Addressing function approximation error in actor-critic
methods. In ICML, 2018.
A. Grover, M. Al-Shedivat, J. K. Gupta, Y. Burda, and H. Edwards. Learning policy representations
in multiagent systems. In ICML, volume 80, pp. 1797-1806, 2018.
D. Hafner, T. P. Lillicrap, J. Ba, and M. Norouzi. Dream to control: Learning behaviors by latent
imagination. In ICLR, 2020.
J.	Harb, T. Schaul, D. Precup, and P. Bacon. Policy evaluation networks. CoRR, abs/2002.11833,
2020.
K.	Hausman, J. Tobias Springenberg, Z. Wang, N. Heess, and M. A. Riedmiller. Learning an em-
bedding space for transferable robot skills. In ICLR, 2018.
H. He and J. L. Boyd-Graber. Opponent modeling in deep reinforcement learning. In ICML, vol-
ume 48, pp. 1804-1813, 2016.
K. He, H. Fan, Y. Wu, S. Xie, and R. B. Girshick. Momentum contrast for unsupervised visual
representation learning. In CVPR, pp. 9726-9735, 2020.
M. Igl, G. Farquhar, J. Luketina, W. Boehmer, and S. Whiteson. The impact of non-stationarity on
generalisation in deep reinforcement learning. CoRR, abs/2006.05826, 2020.
S.	M. Kakade and J. Langford. Approximately optimal approximate reinforcement learning. In
ICML, pp. 267-274, 2002.
N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang. On large-batch training
for deep learning: Generalization gap and sharp minima. In ICLR, 2017.
I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep
reinforcement learning from pixels. CoRR, abs/2004.13649, 2020.
10
Under review as a conference paper at ICLR 2021
M. G. Lagoudakis and R. Parr. Least-squares policy iteration. J. Mach. Learn. Res., 4:1107-1149,
2003.
M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning with
augmented data. CoRR, abs/2004.14990, 2020.
K. Lee, Y. Seo, S. Lee, H. Lee, and J. Shin. Context-aware dynamics model for generalization in
model-based reinforcement learning. CoRR, abs/2005.06800, 2020.
A. Levy, G. D. Konidaris, R. P. Jr., and K. Saenko. Learning multi-level hierarchies with hindsight.
In ICLR. OpenReview.net, 2019.
T.	P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra. Contin-
uous control with deep reinforcement learning. In ICLR, 2015.
H. Liu, K. Simonyan, and Y. Yang. DARTS: differentiable architecture search. In ICLR, 2019.
R. Luo, F. Tian, T. Qin, E. Chen, and T. Liu. Neural architecture optimization. In NeurIPS, pp.
7827-7838, 2018.
X. Ma, J. K. Gupta, and M. J. Kochenderfer. Normalizing flow model for policy representation in
continuous action multi-agent systems. In AAMAS, pp. 1916-1918, 2020.
L. V. D. Maaten and G. E. Hinton. Visualizing data using t-sne. Journal of Machine Learning
Research, 9:2579-2605, 2008.
V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. A.
Riedmiller, A. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529-533, 2015.
V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. P. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu.
Asynchronous methods for deep reinforcement learning. In ICML, 2016.
R. Munos. Error bounds for approximate policy iteration. In ICML, pp. 560-567, 2003.
O. Nachum, S. Gu, H. Lee, and S. Levine. Data-efficient hierarchical reinforcement learning. CoRR,
abs/1805.08296, 2018.
O. Nachum, S. Gu, H. Lee, and S. Levine. Near-optimal representation learning for hierarchical
reinforcement learning. In ICLR. OpenReview.net, 2019.
Y. E. Nesterov and B. T. Polyak. Cubic regularization of newton method and its global performance.
Math. Program., 108(1):177-205, 2006.
R. Novak, Y. Bahri, D. A. Abolafia, J. Pennington, and J. Sohl-Dickstein. Sensitivity and general-
ization in neural networks: an empirical study. In ICLR, 2018.
A. Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. CoRR,
abs/1807.03748, 2018.
R. Raileanu, M. Goldstein, A. Szlam, and R. Fergus. Fast adaptation via policy-dynamics value
functions. CoRR, abs/2007.02879, 2020.
K. Rakelly, A. Zhou, C. Finn, S. Levine, and D. Quillen. Efficient off-policy meta-reinforcement
learning via probabilistic context variables. In ICML, volume 97, pp. 5331-5340, 2019.
T. Salimans, J. Ho, X. Chen, and I. Sutskever. Evolution strategies as a scalable alternative to
reinforcement learning. CoRR, abs/1703.03864, 2017.
T. Schaul, D. Horgan, K. Gregor, and D. Silver. Universal value function approximators. In ICML,
volume 37, pp. 1312-1320, 2015.
J. S Schreck, C. W Coley, and K. JM Bishop. Learning retrosynthetic planning through simulated
experience. ACS central science, 5(6):970-981, 2019.
11
Under review as a conference paper at ICLR 2021
J. Schulman, S. Levine, P. Abbeel, M. I. Jordan, and P. Moritz. Trust region policy optimization. In
ICML,pp.1889-1897, 2015.
J. Schulman, P. Moritz, S. Levine, M. I. Jordan, and P. Abbeel. High-dimensional continuous control
using generalized advantage estimation. In ICLR, 2016.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. CoRR, abs/1707.06347, 2017.
M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. C. Courville, and P. Bachman. Data-efficient
reinforcement learning with momentum predictive representations. CoRR, abs/2007.05929, 2020.
D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. A. Riedmiller. Deterministic policy
gradient algorithms. In ICML, pp. 387-395, 2014.
D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser,
I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbren-
ner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mas-
tering the game of go with deep neural networks and tree search. Nature, 529(7587):484-489,
2016.
A. Srinivas, M. Laskin, and P. Abbeel. CURL: contrastive unsupervised representations for rein-
forcement learning. CoRR, abs/2004.04136, 2020.
R. S. Sutton and A. G. Barto. Reinforcement learning - an introduction. Adaptive computation and
machine learning. MIT Press, 1998.
R. S. Sutton, J. Modayil, M. Delp, T. Degris, P. M. Pilarski, A. White, and D. Precup. Horde: a scal-
able real-time architecture for learning knowledge from unsupervised sensorimotor interaction.
In AAMAS, pp. 761-768, 2011.
A. Tacchetti, H. F. Song, P. A. M. Mediano, V. Flores Zambaldi, J. Kramar, N. C. Rabinowitz, Th.
Graepel, M. Botvinick, and P. W. Battaglia. Relational forward models for multi-agent learning.
In ICLR, 2019.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012.
Y. H. Tsai, Y. Wu, R. Salakhutdinov, and L. Morency. Demystifying self-supervised learning: An
information-theoretical framework. CoRR, abs/2006.05576.
T. Unterthiner, D. Keysers, S. Gelly, O. Bousquet, and I. O. Tolstikhin. Predicting neural network
accuracy from weights. CoRR, abs/2002.11448, 2020.
H. v. Hasselt. Double q-learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel,
and A. Culotta (eds.), NeurIPS, pp. 2613-2621, 2010.
O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Pow-
ell, T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai,
J. P. Agapiou, M. Jaderberg, A. S. Vezhnevets, R. Leblond, T. Pohlen, V. Dalibard, D. Bud-
den, Y. Sulsky, J. Molloy, T. L. Paine, C. Gulcehre, Z. Wang, T. Pfaff, Y. Wu, R. Ring, D. Yo-
gatama, D. Wunsch, K. McKinney, O. Smith, T. Schaul, T. Lillicrap, K. KavUkcUoglu, D. Has-
sabis, C. Apps, and D. Silver. Grandmaster level in starcraft ii using multi-agent reinforcement
learning. Nature, 575(7782):350-354, 2019.
H. Wang, N. S. Keskar, C. Xiong, and R. Socher. Identifying generalization properties in neural
networks. CoRR, abs/1809.07402, 2018.
R. Wang, R. Yu, B. An, and Z. Rabinovich. I2hrl: Interactive influence-based hierarchical reinforce-
ment learning. In IJCAI, pp. 3131-3138, 2020.
J. You, B. Liu, Z. Ying, V. S. Pande, and J. Leskovec. Graph convolutional policy network for
goal-directed molecular graph generation. In NeurIPS 2018, pp. 6412-6422, 2018.
W. Zhou, L. Pinto, and A. Gupta. Environment probing interaction policies. In ICLR, 2019a.
W. Zhou, Y. Yu, Y. Chen, K. Guan, T. Lv, C. Fan, and Z. Zhou. Reinforcement learning experience
reuse with policy residual representation. In IJCAI, pp. 4447-4453, 2019b.
12
Under review as a conference paper at ICLR 2021
A Proofs
A.1 Proof of Theorem 1
Proof. Start from Assumption 2, We derive the upper bound of f(∏2):
r>. r>. . .. ʌ,
f(∏2) ≤ f(∏ι) + M(∏1,∏2,L),	(3)
where M(∏ι, ∏2,L) can have different forms that depends on the specific smoothness property, for
examples:
Lo ∙ d(∏1,∏2)	①
/0/	1	、2
M(∏1,∏2,L) =	hf (π1),π2 - π1i + 2 L1 ∙ d(π1 ,π2)	②
/ S/ /	∖	1,2〃/、/	、	∖	1 2	,/	、Q -=,
hf (∏l),∏2 - ∏li + 2 hf (∏l)(∏2 - ∏l),∏2 - ∏li + 己 L ∙ d(∏l,∏2)'	③
①，②，③ correspond to Lipschitz Continuous, Lipschitz Gradients and Lipschitz Hessian (Nesterov
& Polyak, 2006).
Combined with Assumption 1 and consider the case f(π1) ≤ f(π2), Equation 3 can be further
transformed as follows:
Λ ,	Λ ,	. ,	ʌ ,
f(∏2) ≤ f(∏ι) + M(∏1,∏2, L)
≤ γf(∏ι) + M(∏1,∏2,L)
≤	γf(∏2)	+ M(∏1,∏2,L),
、--{z--}	、-----{z-----}
generalized contraction locality margin
(4)
which yields the generalization upper bound in Theorem 1. We note the first term of RHS of Equa-
tion 4 as generalized contraction since it is from the contraction on f(π1), and the second term as
locality margin since it is determined by specific local property.	□
A.2 Proof of Corollary 1
Proof. Consider the concrete form of locality margin M(∏1,∏2 ,L) in the case of Lipschitz contin-
uous, i.e.,
O ,	. ,	ʌ _ ,	、
f(∏2) ≤ Yf(∏2) + L0 ∙ d(∏ι,∏2),	(5)
and consider f(π2) 6= 0 (note that f(π2) ≥ 0):
∖/	f /	∖	1
f(π2) ≤ γgf(π2), where γg
γ+
L0 ∙ d(∏1,∏2)
f(π2)
(6)
Then we get the contraction condition of value generalization on π2 in Corollary 1, by letting the
RHS of Equation 5 not greater than f(π2), i.e., γg ≤ 1 (note that γ ∈ (0, 1]):
γf(∏2) + L0 ∙ d(∏1,∏2) ≤ f(∏2)
(1 - γ)f(∏2) ≥ Lo
f(π2) ≥ L0
d(π1,π2)
d(π1,π2)
1 - Y
(7)
Intuitions observed from Equation 7 are discussed in Remark 3. In another word, the tighter the
contraction on learned policy π1 is, the closer the two policies are, the smoother the approximation
loss function f is, the generalization on unlearned policy ∏2 is better.	□
Corollary 1 provides the generalization contraction condition on f(π2 ). We also provide another
view from the perspective of f(π1), by additionally considering the smoothness property of f.
Similar as in Assumption 2, we assume that f is Lipschitz continuous at π1 with Lipschitz constant
Lo, e.g., |f (π) - f (∏ι)| ≤ Lo ∙ d(π, ∏ι) with some metric space (Π, d). Higher-order smoothness
property can also be considered here. We then obtain a lower bound of f(π2) below:
f (∏ι) - Lo ∙ d(∏,∏ι) ≤ f (∏2).
(8)
13
Under review as a conference paper at ICLR 2021
We then chain the second line of RHS in Equation 4 (M in the case of Lipschitz continuous) and
the LHS of Equation 8:
O ,	. ,	^	-，	.	. ,	_	-，	.	.,
f(∏2) ≤ Yf(∏ι) + L0 ∙ d(∏, ∏ι) ≤ f(∏ι)- Lo ∙ d(∏, ∏ι) ≤ f(∏2),	(9)
re-arrange and yield the generalization contraction condition on f(π1):
γf(∏ι) + L0 ∙ d(∏1, ∏2) ≤ f (∏ι) - Lo ∙ d(∏ι, ∏2)
(1 - Y)f(∏ι) ≥ (Lo + L0)d(∏1, ∏2)
(10)
f(π1) ≥
(Lo + Lo)d(π1, π2)
1-γ
A.3 Proof of Corollary 2
Proof. Due to Triangle Inequality, we have:
fθt (∏t) + kVθt (∏t) - Vθ*(∏t+ι)k ≥ kVθ*(∏t)- Vθ*(∏t+ι)k
i.e., kVθt(∏t) - Vθ*(∏t)k + kVθt(∏t) - Vθ*(∏t+ι)k ≥ ∣∣Vθ*(∏t) - Vθ*(∏t+ι)k	( )
Combined with condition
fθt(∏t) + fθt(∏t+ι) ≤ kVθ*(∏t)-Vθ*(∏t+ι)k
i.e., kvθt(Kt)- vθ*(πt)k + IIVθt(πt+1)- Vθ*(πt+I)Il ≤ IIVθ*(π∙ - Vθ*(πt+I)I∣.
(12)
Chain above two inequality,
fθt (πt) + fθt(πt+1) ≤ IVθ* (πt) - Vθ* (πt+1)I ≤ fθt(πt) + IVθt(πt) -Vθ*(πt+1)I
fθt(πt+1)	≤ IVθt(πt) - Vθ* (πt+1)I,	(13)
'---{—}	'------------{------------}
generalizated VAD with PeVFA	conventional VAD
then we have that with local generalization of values for successive policy πt+1, the value approxi-
mation distance (VAD) can be closer in contrast to the conventional one (RHS of Equation 13). □
In practice, it is also possible for farther distance to exist, e.g., the condition in above Corollary is
not satisfied. Moreover, under nonlinear function approximation, it is not necessary that a closer
approximation distance (induced by Corollary 2) ensures easier approximation or optimization pro-
cess. This is related to many things, e.g., the underlying function space, the optimization landscape,
the learning algorithm used and etc. In this paper, we provide a condition for potentially beneficial
local generalization and we resort to empirical examination as shown in Sec. 3.2. Further study of
beneficial cases especially under nonlinear function approximation is planned for future work.
B Details of Empirical Evidence of Two Kinds of Generalization
B.1	Global Generalization in 2D Point Walker
Global generalization denotes the generalization scenario that values can generalize to unlearned
policies (π0 ∈ Π1) from already learned policies (π ∈ Πo). We conduct the following experiments
to demonstrate global generalization in a 2D continuous Point Walker environment with synthetic
simple policies.
Environment. We consider a point walker on a 2D continuous plane with
•	state: (x, y, sin(θ), cos(θ), cos(x), cos(y)), where θ is the angle of the polar coordinates,
•	action: 2D displacement, a ∈ R[2-1,1] ,
•	a deterministic transition function that describes the locomotion of the point walker, de-
pending on the current position and displacement issued by agent, i.e., hx0, y0i = hx, yi +a,
• a reward function: r = ut+j-ut with utility Ut = x2 一 y2, as illustrated in Figure 6(a).
14
Under review as a conference paper at ICLR 2021
(a) Utility function heat map
(b) Examples of synthetic policy population
Figure 6: 2D Pointer Walker. (a) The heat map of the utility function of the 2D plane. The darker
regions have higher utilities. (b) Demonstrative illustrations of trajectories generated by 30 synthetic
policies, showing diverse behaviors and patterns. Each subplot illustrates the trajectories generated
in 50 episodes by a randomly synthetic policy, with different colors as separation. For each trajectory
(the same color in one subplot), transparency represents the dynamics along timesteps, i.e., fully
transparent and non-transparent denotes the positions at first and last timesteps.
Synthetic Policy. We build the policy sets Π = Π0 ∪ Π1 and Π0 ∩ Π1 = 0 with synthetic policies.
Each synthetic policy is a 2-layer tanh-activated neural policy network with random initialization.
Each policy is deterministic, taking an environmental state as input and outputting a displacement
in the plane. We find that the synthetic population generated by such a simple way can show diverse
behaviors. Figure 6(b) shows the motion patterns of an example of such a synthetic population. Note
that the synthetic policies are not trained in this experiment.
Policy Dataset. We rollout each policy in environment to collect trajectories T = {τi }ik=0 . For
SUCh small synthetic policies, it is convenient to obtain policy representation. Here We use the
concatenation of all weights and biases of the policy network (22 in total) as representation χπ for
each policy π, called raw policy representation (RPR). Therefore, combined with the trajectories
collected, we obtain the policy dataset, i.e., {(χπj, T∏j)}n=0. In total, 20k policies are synthesized
in our experiments and we collected 50 trajectories with horizon 10 for each policy.
We separate the synthetic policies into training set (i.e., un-
known policies Π0) and testing set (i.e., unseen policies Π1) in
a proportion of 8 : 2. We set a PeVFA network Vθ(s, χπ) to
approximate the values of training policies (i.e., π ∈ Π0), and
then conduct evaluation on testing policies (i.e., π ∈ Π1). We
use Monte Carlo return (Sutton & Barto, 1998) of collected
trajectories as approximation target (true value of policies) in
this experiment. The network architecture of Vθ(s, Xn) is il-
lustrated in Figure 7. The learning rate is 0.005, batch size is
256. K-fold validation is performed through shuffling training
and testing sets.
Figure 2(a) shows that a PeVFA trained with data collected by
training set Π0 achieves reasonable value prediction of unseen
testing policies in Π1, as well as maintains the order of opti-
mality among testing policies. This indicates that value gener-
alization can exist among policy space with a properly trained
PeVFA. RPR can also be one alternative of policy representa-
tion when policy network is of small scale.
S	XF
Ve(S,2Q
Figure 7: An illustration of ar-
chitecture of PeVFA network. FC
is abbreviation for Fully-connected
layer.
15
Under review as a conference paper at ICLR 2021
SSol UoQeUJIxoα,2
(a) InVertedPendulum-V1
1R93-
IW2O-
»«，
Wnl£ MV
ssσι uq-jeul-XOJs<
E≡φtf 6Λ4
HjIIChMUb-Vl
20βS-
17«-
U»l-
K 一
g Wl-
l
& 2
«4-
IW-
—70 - l	I	I	，
OlO 2«	30	4«
Steps We4 steps)
(d) HalfCheetah-VI
(b) InvertedDoublePendulum-v1
(c) LunarLanderContinuous-V2
S3 UOIIeul-XoJdW
UJBeM w><
二SM
Figure 8:	Complete empirical eVidence of local generalization of PeVFA across 7 MuJoCo tasks.
The learning rate of policy and Value function approximators are 0.0001 and 0.001 respectiVely.
Each plot has two Vertical axes, the left one for approximation error (red and blue curVes) and
the right one for aVerage return (green curVes). Red and blue denotes the approximation error of
conVentional VFA (Vφπ (s)) and of PeVFA (Vθ (s, χπ)) respectiVely; solid and dashed curVes denote
the approximation error before and after the training for Values of successiVe policy (i.e., policy
eValuation) with conVentional VFA and PeVFA, aVeraged oVer 6 trials. The shaded region denotes
half a standard deViation of aVerage eValuation. PeVFA consistently shows lower losses (i.e., closer
to approximation target) across all tasks than conVention VFA before and after policy eValuation
along policy improVement path, which demonstrates Corollary 2.
B.2 Local Generalization in MuJoCo
Continuous Control Tasks
We demonstrate local generalization of PeVFA, especially to examine the existence of Corollary 2,
i.e., PeVFA can induce closer approximation distance (i.e., lower approximation error) than conVen-
tional VFA along the policy improVement path.
We use a 2-layer 8-unit policy network trained by PPO (Schulman et al., 2017) algorithm in Ope-
nAI MuJoCo continuous control tasks. As in preVious section, using a Very small policy network
is for the conVenience of training and acquisition of policy representation in this demonstratiVe
experiment. We use all weights and biases of the small policy network (also called raw policy rep-
resentation, RPR), whose number is about 10 to 100 in our experiments, depending on the specific
enVironment (i.e., the state and action dimensions). We train the small policy network as commonly
done with PPO (Schulman et al., 2017) and GAE (Schulman et al., 2016). The conVention Value
network Vφπ(s) (VFA), is a 2-layer 128-unit ReLU-actiVated MLP with state as input and Value as
output. Parallel to the conVentional VFA in PPO, we set a PeVFA network Vθ (s, χπ) with RPR as
16
Under review as a conference paper at ICLR 2021
Ant-Vl {learning rate =0.001)
SSCnUoneUJ-XOħ,d‹
u-bbm 6至
SSon UoHeE-XCUdW
SSCn Uo4eujpo>a.d<
二SM
-VFA loss (before)
Avg Return
loss (before)
A loss (afterj
0	10	2«	So	4«	So
Steps (4β4 steps)
(e) Ant-v1 (lr = 1e-3)
ΓH9	112«-
ι<n	«7s-
Ant-Vl {learning rate =0.005)
ssσι uq-jeul-XOJs<
E≡φtf 6Λ4
10	2«	W	40
Steps (4β4 steps)
(f) Ant-V1(lr = 5e-3)
E≡φtf 6Λ4
Figure 9:	Empirical eVidence of local generalization of PeVFA on InVertedPendulum-V1 and Ant-
V1 with different learning rates of policy, i.e., {0.0001, 0.001, 0.005}. Results are aVeraged oVer 6
trials.
additional input. The structure of PeVFA differs at the first hidden layer which has two input streams
and each of them has 64 units, as illustrated in Figure 7, so that making VFA and PeVFA haVe simi-
lar scales of parameter number. In contrast to conVention VFA Vφπ which approximates the Value of
current policy (e.g., Algorithm 1), PeVFA Vθ(s, χπ) has the capacity to preserVe Values of multiple
policies and thus is additionally trained to approximate the Values of all historical policies ({πi }it=0)
along the policy improVement path (e.g., Algorithm 2). The learning rate of policy is 0.0001 and the
learning rate of Value function approximators (Vφπ(s) and Vθ(s, χπ)) is 0.001. The training scheme
of PPO policy here is the same as that described in Appendix F.1 and Table 2.
Note that Vθ (χπ) does not interfere with PPO training here, and is only referred as a comparison
with Vφπ on the approximation error to the true Values of successiVe policy πt+1. We use the MC
returns of on-policy data (i.e., trajectories) collected by current successiVe policy as unbiased esti-
mates of true Values, similarly done in (V. Hasselt, 2010; Fujimoto et al., 2018). Then we calculate
the approximation error for VFA Vφπ and PeVFA Vθ (χπ) to the approximation target before and af-
ter Value network training of current iteration. Finally, we compare the approximation error between
VFA and PeVFA to approximately examine local generalization and closer approximation target in
Corollary 2. Complete results of local generalization across all 7 MuJoCo tasks are show in Figure
8. The results show that PeVFA consistently shows lower losses (i.e., closer to approximation target)
across all tasks than conVention VFA before and after policy eValuation along policy improVement
path, which demonstrates Corollary 2. MoreoVer, we also proVide similar empirical eVidence when
policy is updated with larger learning rates, as in Figure B.1.
A common obserVation across almost all results in Figure 8 and in Figure B.1 is that the larger the
extent of policy change (see the regions with a sheer slope on green curVes), the higher the losses of
conVentional VFA tend to be (see the peaks of red curVes), where the generalization tends to be bet-
ter and more significant (see the blue curVes). Since InVertedPendulum-V1 is a simple task while the
complexity of the solution for Ant-V1 is higher, the difference between Value approximation losses
of PeVFA and VFA is more significant at the regions with fast policy improVement. Besides, the
Raw Policy Representation (RPR) we used here does not necessarily induce a smooth and efficient
policy representation space, among which policy Values are easy to generalize and optimize. Thus,
RPR may be sufficient for a good generalization in InVertedPendulum-V1 but may be not in Ant-V1.
OVerall, we think that the quantity of Value approximation loss is related to seVeral factors of the
enVironment such as the reward scale, the extent of policy change, the complexity of underlying so-
17
Under review as a conference paper at ICLR 2021
lution (e.g., value function space) and some others. A further investigation on this can be interesting.
C Generalized Policy Iteration with PeVFA
C.1 Comparison between Conventional GPI and GPI with PeVFA
A graphical comparison of conventional GPI and GPI with PeVFA is shown in Figure 1. Here we
provide another comparison with pseudo-codes.
From the lens of Generalized Policy Iteration (Sutton & Barto, 1998), for most model-free policy-
based RL algorithms, the approximation of value function and the update of policy through pol-
icy gradient theorem are usually conducted iteratively. Representative examples are REINFORCE
(Sutton & Barto, 1998), Advantage Actor-Critic (Mnih et al., 2016), Deterministic Policy Gradi-
ent (DPG) (Silver et al., 2014) and Proximal Policy Optimization (PPO) (Schulman et al., 2017).
With conventional value function (approximator), policy evaluation is usually performed in an on-
policy or off-policy fashion. We provide a general GPI description of model-free policy-based RL
algorithm with conventional value functions in Algorithm 1.
Algorithm 1 Generalized policy iteration for model-free policy-based RL algorithm with conven-
tional value functions (V π(s) or Qπ(s, a))
1:	Initialize policy π0 and V-π1(s) or Qπ-1(s, a)
2:	Initialize experience buffer D
3:	for iteration t = 0, 1, 2, . . . do
4:	Rollout policy πt in the environment and obtain trajectories Tt = {τi }ik=0
5:	Add experiences Tt in buffer D
6:	if on-policy update then
7:	Prepare training samples from rollout trajectories Tt
8:	else if off-policy update then
9:	Prepare training samples by sampling from buffer D
10:	end if
11:	Calculate approximation target {yi}i from training samples (e.g., with MC or TD)
12:	# Generalized Policy Evaluation
13:	Update	V-I(S)	or	Q∏-ι(s,	a)	With {(si,yi)}i or {(si, ai, yi)}i,	i.e.,	Vn	<—	V-I	or
Qn — Q∏-ι
14:	# Generalized Policy Improvement
15:	Update policy πt With regard to Vtπ (s) or Qtπ (s, a) through policy gradient theorem, i.e.,
∏t+14一 ∏t
16:	end for
Note that We use subscript t - 1 → t (Line 13 in Algorithm 1) to let the updated value functions to
correspond to the evaluated policy πt during policy evaluation process in current iteration.
As a comparison, a neW form of GPI With PeVFA is shoWn in Algorithm 2. Except for the different
parameterization of value function, PeVFA can perform additionally training on historical policy
experiences at each iteration (Line 7-8). This is naturally compatible With PeVFA since it develops
the capacity of conventional value function to preserve the values of multiple policies. Such a
training is to improve the value generalization of PeVFA among a policy set or policy space. Note
that for value approximation of current policy πt (Line 10-14), the start points are generalized values
of πt from historical approximation, i.e., Vt-1(s, χπt) and Qt-1(s, a, χπt ). In another Word, this
is the place Where local generalization steps (illustrated in Figure 2(b)) are. One may compare With
conventional start points (Vtπ-1(s) and Qtπ-1(s, a), Line 13 in Algorithm 1) and see the difference,
e.g., Vtπ-1(s) ⇔ V πt-1 (s) ⇔ Vt-1(s, χπt-1 ) is different With Vt-1(s, χπt), Where ⇔ is used
to denote an equivalence in definition. As discussed in Sec. 3.2 and 3.3, We suggest that such
local generalization steps help to reduce approximation error and thus improve efficiency during the
learning process.
18
Under review as a conference paper at ICLR 2021
Algorithm 2 Generalized policy iteration of model-free policy-based RL algorithm with PeVFAs
(V(S,Xπ) or Q(S,a, XnD___________________________________________________________________________
1:	Initialize policy π0 and PeVFA V-1(s, χπ) or Q-1(s, a, χπ)
2:	Initialize experience buffer D
3:	for iteration t = 0, 1, 2, . . . do
4:	Rollout policy πt in the environment and obtain trajectories Tt = {τi }ik=0
5:	Get the policy representation χπt for policy πt (from policy network parameters or policy
rollout experiences)
6:	Add experiences (χπt , Tt ) in buffer D
7:	# Value approximation training for historical policies {πi}it=-01
8:	Update PeVFA Vt-1(S, χπi) or Qt-1(S, a, χπi ) with all historical policy experiences
{(χπi,Ti)}it=-01
9:	# Conventional value approximation training for current policy πt
10:	if on-policy update then
11:	Update PeVFA Vt-1 (S, χπt ) or Qt-1 (S, a, χπt ) for πt with on-policy experiences
(χπt,Tt)
12:	else if off-policy update then
13:	Update PeVFA Vt-1(S, χπt) or Qt-1(S, a, χπt) for πt with off-policy experiences χπt
and {Ti }ti=0 from experience buffer D
14:	end if
15:	Vt《-------Vt-I or Qt《-----Qt-I
16:	Update policy πt with regard toVt(S, χπt) orQt(S, a, χπt) through policy gradient theorem,
i	.e., πt+14-πt
17:	end for
C.2 More Discussions on GPI with PeVFA
Off-Policy Learning. Off-policy Value Estimation (Sutton & Barto, 1998) denotes to evaluate the
values of some target policy from data collected by some behave policy. As commonly seen in
RL (also shown in Line 6-10 in Algorithm 1), different algorithms adopt on-policy or off-policy
methods. For GPI with PeVFA, especially for the value estimation of historical policies (Line 8 in
Algorithm 2), on-policy and off-policy methods can also be considered here. One interesting thing
is, in off-policy case, one can use experiences from any policy for the learning of another one, which
can be appealing since the high data efficiency of value estimation of each policy can strengthen
value generalization among themselves with PeVFA, which further improve the value estimation
process.
Convergence of GPI with PeVFA. Convergence of GPI is usually discussed in some ideal cases,
e.g., with small and finite state action spaces and with sufficient function approximation ability. In
this paper, we focus on the comparison between conventional VFA and PeVFA in value estimation,
i.e., Policy Evaluation, and we make no assumption on the Policy Improvement part. We conjecture
that with the same policy improvement algorithm and sufficient function approximation ability, GPI
with conventional VFA and GPI with PeVFA finally converge to the same policy. Moreover, based
on Corollary 2 and our empirical evidence in Sec. 3.2, GPI with PeVFA can be more efficient in
some cases: with local generalization, it could take less experiences (training) for PeVFA to reach
the same level of approximation error than conventional VFA, or with the same amount of experience
(training), PeVFA could achieve lower approximation error than conventional VFA. We believe that
a deeper dive in convergence analysis is worth further investigation.
PeVFA with TD Value Estimation. In this paper, we propose PPO-PeVFA as a representative
instance of re-implementing DRL algorithms with PeVFA. Our theoretical results and algorithm 2
proposed under the general policy iteration (GPI) paradigm are suitable for TD value estimation
as well in principle. One potential thing that deserves further investigation is that, it can be a more
complex generalization problem since the approximation target of TD learning is moving (in contrast
to the stationary target when unbiased Monte Carlo estimates are used). The non-stationarity induced
by TD is recognized to hamper the generalization performance in RL as pointed out in recent work
(Igl et al., 2020). Further study on PeVFA with TD learning (e.g., TD3 and SAC) is planned in the
future as mentioned in Sec. 6.
19
Under review as a conference paper at ICLR 2021
D	Policy Representation Learning Details
D. 1 Policy Geometry
A policy π ∈ Π = P(A)S, defines the behavior (action distribution) of the agent under each state.
For a more intuitive view, we consider the geometrical shape of a policy: all state s ∈ S and all
action a ∈ A are arranged along the x-axis and y-axis of a 2-dimensional plane, and the probability
(density) ∏(a∣s) is the value of z-axis over the 2-dimensional plane. Note that for finite state space
and finite action space (discrete action space), the policy can be viewed as a |S| × |A| table with each
entry in it is the probability of the corresponding state-action case. Without loss of generality, we
consider the continuous state and action space and the policy geometry here. Illustrations of policy
geometry examples are shown in Figure 10.
Figure 10(a) shows the policy geometry in a general case, where the policy can be defined arbitrarily.
Generally, the policy geometry can be any possible geometrical shape (s.t.Vs ∈S, ∑a∈A ∏(a∣s) =
1). This means that the policy geometry is not necessarily continuous or differentiable in a general
case. Specially, one can imagine that the geometry of a deterministic policy consists of peak points
(z = 1) for each state and other flat regions (z = 0). Figure 10(b) shows an example of synthetic
continuous policy which can be viewed as a 3D curved surface. In Deep RL, a policy may usually
be modeled as a deep neural network. Assume that the neural policy is a function that is almost
continuous and differentiable everywhere, the geometry of such a neural policy can also be contin-
uous and differentiable almost everywhere. As shown in Figure 10(c), we provide a demo of neural
policy by smoothing an arbitrary policy along both state and action axes.
(a) Arbitrary policy
0.06
0.05-
S
0∙<l*∣
003S
0.02%
DX)1
(b) Continuous policy
(c) Demo of neural policy
Figure 10:	Examples of policy geometry. (a) An arbitrary policy, where p(s, a) is sampled from
N(0, 1) for a joint space of 40 states and 40 actions and then normalized along action axis. States
are squeezed into the range of [-1, 1] for clarity. (b) A synthetic continuous policy with p(s, a) =
(1 - a5 + s5) exp(-s2 - a2) for a joint space of s ∈ [-2, 2] and a ∈ [-2, 2] (each of which
are discretized into 40 ones) and then normalized along action axis. (c) A general demo of neural
network policy, generated from an arbitrary policy (as in (a)) over a joint space of 200 states and
100 actions with some smoothing skill. States are squeezed into the range of [-1, 1] for clarity and
the probability masses of actions under each state are normalized to sum into 1.
D.2 Implementation Details of Surface Policy Representation (SPR) and
Origin Policy Representation (OPR)
Here we provide a detailed description of how to encode different policy data for Surface Policy
Representation (SPR) and Origin Policy Representation (OPR) we introduced in Sec. 4.
Encoding of State-action Pairs for SPR. Given a set of state-action pairs {si, ai}in=1 (with size
[n, s-dim + a_dim]) generated by policy π (i.e., a% 〜π(∙∣si)), We concatenate each state-action
pair and obtain an embedding of it by feeding it into an MLP, resulting in a stack of state-action
embedding with size [n, e_dim]. After this, we perform a mean-reduce operator on the stack and
obtain an SPR with size [1, e_dim]. A similar permutation-invariant transformation is previously
adopted to encode trajectories in (Grover et al., 2018).
Encoding of Network Parameters for OPR. We propose a novel way to learn low-dimensional
embedding from policy network parameters directly. To our knowledge, we are the first to learn
20
Under review as a conference paper at ICLR 2021
policy embedding from neural network parameters in RL. Note that latent space of neural networks
are also studied in differentiable Network Architecture Search (NAS) (Liu et al., 2019; Luo et al.,
2018), where architecture-level embedding are usually considered. In contrast, OPR cares about
parameter-level embedding with a given architecture.
Consider a policy network to be a MLP with well-represented state (e.g., CNN for pixels, LSTM for
sequences) as input and deterministic or stochastic policy output. We compress all the weights and
biases of the MLP to obtain an OPR that represents the decision function. The encoding process of
an MLP with two hidden layers is illustrated in Figure 11. The main idea is to perform permutation-
invariant transformation for inner-layer weights and biases for each layer first. For each unit of some
layer, we view the unit as a non-linear function of all outputs, determined by weights, a bias term
and activation function. Thus, the whole layer can be viewed as a batch of operations of previous
outputs, e.g., with the shape [ht, ht-1 + 1] for t ≥ 1 and t = 0 is also for the input layer. Note
that we neglect activation function in the encoding since we consider the policy network structure is
given. That is also why we call OPR as parameter-level embedding in contrast to architecture-level
enbedding in NAS (mentioned in the last paragraph). We then feed the operation batch in an MLP
and perform mean-reduce to outputs. Finally we concatenate encoding of layers and obtain the OPR.
We use permutation-invariant transformation for OPR because that we suggest the operation batch
introduced in the last paragraph can be permutation-invariant. Actually, our encoding shown in
Figure 11 is not strict to obtain permutation-invariant representation since inter-layer dependencies
are not included during the encoding process. We also tried to incorporate the order information
during OPR encoding and we found similar results with the way we present in Figure 11, which we
adopt in our experiments.
Operations
PreproceSS
Permutation-
Invariant
Transform
W	[s_dlm, h1]	[h1, h2]	[h2, a_dlm]
b	[l,h1]	[l,h2]	[1, a_dim]
Concatenate & Transpose
c	[hi,s_dim + 1]	[h2,hι + 1]	[a_dtm, h2 + 1]
MLP Transformation & Mean-Reduce
e	[1,e1]	[1,e2]	[1,e3]
Concatenate
OPR	[1, eι + £2 + e3]

<
Figure 11:	An illustration for policy encoder of Origin Policy Representation (OPR) for a two-layer
MLP. h1 , h2 denotes the numbers of hidden units for the first and second hidden layers respectively.
The main idea is to perform permutation-invariant transformation for inner-layer weights and biases
for each layer first and then concatenate encoding of layers.
Towards more sophisticated RL policy that operates images. Our proposed two policy represen-
tations (i.e., OPR and SPR) can basically be applied to encode policies that operate images, with the
support of advanced image-based state representation. For OPR, a policy network with image in-
put usually has a pixel feature extractor like Convolutional Neural Networks (CNNs) followed by a
decision model (e.g., an MLP). With effective features extracted, the decision model can be of mod-
erate (or relatively small) scale. Recent works on unsupervised representation learning like MoCo
21
Under review as a conference paper at ICLR 2021
(He et al., 2020), SimCLR (Chen et al., 2020), CURL (Srinivas et al., 2020) also show that a linear
classifier or a simple MLP which takes compact representation of images learned in an unsupervised
fashion is capable of solving image classification and image-based continuous control tasks. In an-
other direction, it is promising to develop more efficient even gradient-free OPR, for example using
the statistics of network parameters in some way instead of all parameters as similarly considered in
(Unterthiner et al., 2020).
For SPR, to encode state-action pairs (or sequences) with image states can be converted to the
encoding in the latent space. The construction of latent space usually involves self-supervised repre-
sentation learning, e.g., image reconstruction, dynamics prediction. A similar scenario can be found
in recent model-based RL like Dreamer (Hafner et al., 2020), where the imagination is efficiently
carried out in the latent state space rather than among original image observations.
Overall, we believe that there remain more effective approaches to represent RL policy to be devel-
oped in the future in a general direction of OPR and SPR, which are expected to induce better value
generalization in a different RL problems.
D.3 Data Augmentation for SPR and OPR in Contrastive Learning
Data augmentation is studied to be an important component in contrastive learning in deep RL re-
cently (Kostrikov et al., 2020; Laskin et al., 2020). Contrastive learning usually resorts to data
augmentation to build postive samples. Data augmentation is typically performed on pixel inputs
(e.g., images) problems (He et al., 2020; Chen et al., 2020). In our work, we train policy repre-
sentation with contrastive learning where data augmentation is performed on policy data. For SPR,
i.e., state-action pairs as policy data, there is no need to perform data augmentation since different
batches of randomly sampled state-action pairs naturally forms positive samples, since they all re-
flect the behavior of the same policy. A similar idea can also be found in (Fu et al., 2020) when
dealing with task context in Meta-RL.
For OPR, i.e., policy network parameters as policy data, it is unclear how to perform data aug-
mentation on them. In this work, we consider two kinds of data augmentation for policy network
parameters as shown in Figure 12. We found similar results for both random mask and noise cor-
ruption, and we use random mask as default data augmentation in our experiments.
Original
Random Mask
Noise Corruption
Figure 12:	Examples of data augmentation on policy network parameters for Origin Policy Repre-
sentation (OPR). Left: an example of original policy network. Middle: dropout-like random masks
are performed on original policy network, where gray dashed lines represent the weights masked
out. Right: randomly selected weights are corrupted by random noises, denoted by orange lines.
As an unsupervised representation learning method, contrastive Learning encourages policies to be
close to similar ones (i.e., positive samples ∏+) and to be apart from different ones (i.e., negative
samples π-) in policy representation space. The policy representation network is then trained with
InfoNCE loss (Oord et al., 2018), i.e., to minimize the cross-entropy loss below:
LCL = -E
l __________exp(χT Wχ∏+)________-
.ɑg exp(χTWX∏+) + Pn- exp(χTWχ∏-) 一
D.4 Pseudo-code of Policy Representation Learning Framework
The pseudo-code of the overall framework of policy representation learning is in Algorithm 3. Note
that in Line 21, the positive samples χπ+ is obtained from a momentum policy encoder (He et al.,
22
Under review as a conference paper at ICLR 2021
2020) with another augmentation for corresponding policy data, while negative samples χπ- are
πi
other policy embeddings in the same batch, i.e., Xn- ∈ B∖{x∏i}.
i
Algorithm 3 A Framework of Policy Representation Learning
Input: policy dataset D = {(πi, ωi, Dπi)}in=1, consisting of policy πi, policy parameters ωi and
state-action pairs Dπi = {(sj , aj)}jm=1
1:	Initialize the policy encoder gα with parameters α
2:	Initialize the policy decoder (or master policy) (network) ∏β(a|s, Xn) for SPR and the weight
matrix W for ORP respectively
3:	for iteration i = 0, 1, 2, . . . do
4:	Sample a mini-batch of policy data B from D
5:	# Encode and obtain the policy embedding Xni with SPR or OPR
6:	if Use OPR then
7:	if Use Contrastive Learning then
8:	Perform data augmentation on each wi ∈ B
9:	end if
10:	Xni = gαOPR(ω∏i) for each (∏i,ωi, ∙) ∈ B
11:	else if Use SPR then
12:	Xni = gαSPR(Bi) where Bi is a mini-batch of state-action pairs sampled from Dni, for
each (∏i, ∙, Dni) ∈ B
13:	end if
14:	# Train policy encoder gα in different ways (i.e., AUX or CL)
15:	if Use Auxiliary Loss then
16:	Sample a mini-batch of state-action pairs B = (si, ai)ib=1 from Dni for each πi
17:	Compute the auxiliary loss, LAUX = - P(si,ai)∈B log∏α(ai∣Si, Xni)
18:	Update parameters α, β to minimize LAux
19:	end if
20:
21:
22:
23:
if Use Contrastive Learning then
Calculate contrastive loss, LCL = - Pχ ∈B log
where Xn+, Xn- are positive and negative samples
ni	ni
Update parameters α, W to minimize LCL
end if
exp(χπTi W χπ+)
eχP(χTiWχπ+ )+Pπ- exP(XTi Wxπ-),
ii	i
24:	# Train policy encoder gα with the PeVFA approximation loss (E2E)
25:	Calculate the value approximation loss of PeVFA, LVal
26:	Update parameters α to minimize LVal
27:	end for
D.5 A Review of Related Works on Policy Representation/Embedding Learning
Recent years, a few works involve representation or embedding learning for RL policy (Hausman
et al., 2018; Grover et al., 2018; Arnekvist et al., 2019; Raileanu et al., 2020; Wang et al., 2020;
Harb et al., 2020). We provide a brief review and summary for above works below.
The most common way to learn a policy representation is to extract from interaction trajectories
through action recovery (i.e., behavioral cloning). For Multiagent Opponent Modeling (Grover
et al., 2018), a policy representation is learned from interaction episodes (i.e., state-action trajec-
tories) through a generative loss and discriminate loss. Generative loss is the same as the action
prediction auxiliary loss; discriminate loss is a triplet loss that minimize the representation distance
of the same policy and maximize those of different ones, which has the similar idea of Contrastive
Learning (Oord et al., 2018; Srinivas et al., 2020). Such opponent policy representations are used for
prediction of interaction outcomes for ad-hoc teams and are taken in policy network for some learn-
ing agent to facilitate the learning when cooperating or competing with unknown opponents. More
recently, in Hierarchical RL (Wang et al., 2020), a representation is learned to model the low-level
policy through generative loss mentioned above. The low-level policy representation is taken in
high-level policy to counter the non-stationarity issue of co-learning of hierarchical policies. Later,
Raileanu et al. (2020) resort to almost the same method and the learned policy representation is taken
23
Under review as a conference paper at ICLR 2021
in their proposed PDVF. Along with a task context, the policy for a specific task can be optimized in
policy representation space, inducing a fast adaptation in new tasks. In summary, such a represen-
tation learning paradigm can be considered as Surface Policy Representation (SPR) for policy data
encoding (trajectories as a special form of state-action pairs) plus action prediction auxiliary loss
(AUX) as we introduced in Sec. 4.
A recent work (Harb et al., 2020) proposes Policy Evaluation Network (PVN) to approximate objec-
tive function J(π). We consider PVN as an predecessor of PDVF we mentioned above since offline
policy optimization is also conducted in learned representation space in a single task after similarly
well training the PVN on many policies. The authors propose Network Fingerprint to represent
policy network. To circumvent the difficulty of representing the parameters directly, policy network
outputs (policy distribution) under a set of probing states are concatenated and then taken as pol-
icy representation. Such probing states are randomly sampled for initialization and also optimized
with gradients through PVN and policies, like a search in joint state space. In principle, we also
consider this as a special instance of SPR, because network fingerprint follows the idea of reflecting
the information of how policy can behave under some states. Intuitively from a geometric view, this
can be viewed as using the concatenation of several representative cross-sections in policy surface
(e.g., Figure 10) to represent a policy. For another view, one can imagine an equivalent case between
SPR and network fingerprint, when state-action pairs of a deterministic policy are processed in SPR
and a representation consisting of a number of actions under some key states or representative states
is used in network fingerprint. Two potential issues may exist for network fingerprint. First, the
dimensionality of representation is proportional to the number of probing states (i.e., n|A|), where
a dilemma exists: more probing states are more representative while dimensionality can increase
correspondingly. Second, it can be non-trivial and even unpractical to optimize probing states in
the case with relatively state space of high dimension, which induces additional computational com-
plexity and optimization difficulty.
In another concurrent work (Faccio & Schmidhuber, 2020), a class of Parameter-based Value Func-
tions (PVFs) are proposed. Instead of learning or designing a representation of policy, PVFs simply
parse all the policy weights as inputs to the value function (i.e., Raw Policy Representation as also
mentioned in our paper), even in the nonlinear case. Apparently, this can result in a unnecessarily
large representation space which increase the difficulty of approximation and generalization. The
issues of naively flattening the policy into a vector input are also pointed out in PVN (Harb et al.,
2020).
Others, several works in Policy Adaptation and Transfer (Hausman et al., 2018; Arnekvist et al.,
2019), Gaussian policy embedding representations are construct through Variantional Inference.
D.6 Criteria of A Good Policy Representation
To answer the question: what is a good representation for RL policy ought to be? We assume the
following criteria:
•	Dynamics. Intuitively, a good policy representation should contain the information of how
the policy influences the environment (dynamics and rewards).
•	Consistency. A good policy representation should keep the consistency among both pol-
icy space and presentation space. Concretely, the policy representation should be distin-
guishable, i.e., different policies also differ among their representation. In contrast, the
representation of similar polices should lay on the close place in the representation space.
•	Geometry. Additionally, from the lens of policy geometry as shown in Appendix D.1, a
good policy representation should be an reflection of policy geometry. It should show a
connection to the policy geometry or be interpretable from the geometric view.
From the perspective of above criteria, SPR follows Dynamics and Geometry while OPR may ren-
der them in an implicit way since network parameters determine the nonlinear function of policy.
Auxiliary loss for action prediction (AUX) is a learning objective to acquire Dynamics; Contrastive
Learning (CL) is used to impose Consistency.
Based on the above thoughts, we hypothesize the reasons of several findings as shown in the com-
parison in Table 1. First, AUX naturally overlaps with SPR and OPR to some degree for Dynamics
while CL is relatively complementary to SPR and OPR for Consistency. This may be the reason
24
Under review as a conference paper at ICLR 2021
why CL improves the E2E representation more than AUX in an overall view. Second, the noise of
state-action samples for SPR may be the reason to OPR’s slightly better overall performance than
that of SPR (similar results are also found in our visualizations as in Figure 19).
Moreover, the above criteria are mainly considered from an unsupervised or self-supervised perspec-
tive. However, a sufficiently good representation of all the above properties may not be necessary for
a specific downstream generalization or learning problem which utilizes the policy representation.
A problem-specific learning signal, e.g., the value approximation loss in our paper (E2E represen-
tation), can be efficient since it is to extract the most relevant information in policy representation
for the problem. A recent work Tsai et al. also studies the relation between self-supervised repre-
sentation and downstream tasks from the lens of mutual information. Therefore, we suggest that a
trade-off between good unsupervised properties and efficient problem-specific information of policy
representation should be considered when using policy representation in a specific problem.
E Complete Background and More
E.1	Reinforcement Learning
Markov Decision Process. We consider a Markov Decision Process (MDP) defined as
hS , A, r, P , γ, ρ0 i with S the state space, A the action space, r the reward function, P the tran-
sition function, γ ∈ [0, 1) the discount factor and ρ0 the initial state distribution. A policy
π ∈ Π = P(A)|S|, defines the distribution over all actions for each state. The agent interacts with
the environment with its policy, generating the trajectory s0, a0, r1, s1 , a1, r2, ..., st, at, rt+1, ...,
where rt+1 = r(st, at). An RL agent seeks for an optimal policy that maximizes the expected
long-term discounted return, J(π) = Es°〜°。,。〜∏ [ P∞=0 Ytr⅛+ι].
Value Function. Almost all RL algorithms involve value functions (Sutton & Barto, 1998), which
estimate how good a state or a state-action pair is conditioned on a given policy. The state-value
function vπ (s) is defined in terms of the expected return obtained through following the policy π
from a state s:
∞
vπ(s)
Eπ	γt rt+1 |s0
for all s ∈ S .
t=0
s
Similarly, action-value function is defined for all state-action pairs as qπ (s, a)	=
Eπ [Pt∞=0 γt rt+1 |s0 = s, a0 = a]. Typically, value functions are learned through Monte Carlo (MC)
or Temporal Difference (TD) algorithms (Sutton & Barto, 1998).
Bellman equations defines the recursive relationships among value functions. The Bellman Expec-
tation equation of vπ(s) has a matrix form as below (Sutton & Barto, 1998):
Vπ =rπ+γPπVπ = (I - γPπ)-1rπ
(14)
where Vπ is a |S|-dimensional vector, Pπ is the State-to-state transition matrix Pπ (Sls) =
Pa∈A π(a∣s)P(s0∣s,a) and rπ is the vector of expected rewards rπ(s) = P°∈a π(a∣s)r(s, a).
Equation 14 indicates that value function is determined by policy π and environment models (i.e.,
P and r. For a conventional value function, all of them are modeled implicitly within a table or a
function approximator, i.e., a mapping from only states (and actions) to values.
E.2 An Unified View of Extensions of Conventional Value Function from the
Vector Form of Bellman Equation
Recall the vector form of Bellman equation (Equation 14), it indicates that value function is a func-
tion of policy π and environmental models (i.e., P and r). In conventional value functions and
approximators, only state (and action) is usually taken as input while other components in Equation
14 are modeled implicitly. Beyond state (and action), consider explicit representation of some of
components in Equation 14 during value estimation can develop the ability of conventional value
functions in different ways, to solve challenging problems, e.g., goal-conditioned RL (Schaul et al.,
2015; Andrychowicz et al., 2017), Hierarchical RL (Nachum et al., 2018; Wang et al., 2020), op-
ponent modeling and ad-hoc team (He & Boyd-Graber, 2016; Grover et al., 2018; Tacchetti et al.,
2019), and context-based Meta-RL (Rakelly et al., 2019; Lee et al., 2020).
25
Under review as a conference paper at ICLR 2021
As discussed in Sec. 2.2, most extensions of conventional VFA mentioned above are proposed for
the purpose of value generalization (among different space). Therefore, we suggest such extensions
are derived from the same start point (i.e., Equation 14) and differ at the objective to represent and
take as additional input explicitly of conventional value functions. We provide a unified view of
such extensions below:
•	Goal-conditioned RL and context-based meta-RL usually focus on a series of tasks with
similar goals and environment models (i.e., P and r). With goal representation as input,
usually a subspace of state space (Schaul et al., 2015; Andrychowicz et al., 2017), a value
function approximation (VFA) can generalize values among goal space. Similarly, with
context representation (Rakelly et al., 2019; Fu et al., 2020; Raileanu et al., 2020), values
generalize in meta tasks.
•	Opponent modeling, ad-hoc team (He & Boyd-Graber, 2016; Grover et al., 2018; Tacchetti
et al., 2019) seek to generalize among different opponents or teammates in a Multiagent
System, with learned representation of opponents. This can be viewed as a special case
of value generalization among environment models since from one agent view, other op-
ponents are part of the environment which also determines the dynamics and rewards. In
multiagent case, one can expand and decompose the corresponded joint policy in Equation
14 to see this.
•	Hierarchical RL is also a special case of value generalization among environment mod-
els. In goal-reaching fashioned Hierarchical HRL (Nachum et al., 2018; Levy et al., 2019;
Nachum et al., 2019), high-level controllers (policy) issue goals for low-level controls at
an abstract temporal scale, while low-level controls take goals also as input and aim to
reach the goals. For low-level policies, a VFA with a given or learned goal representa-
tion space can generalize values among different goals, similar to the goal-conditioned RL
case as discussed above. Another perspective is to view the separate learning process of
hierarchical policies for different levels as a multiagent learning system. Recently, a work
(Wang et al., 2020) follows this view and extends high-level policy with representation of
low-level learning.
The common thing of above is that, they learn a representation of the environment (we call external
variables). In contrast, we study value generalization among agent’s own policies in this paper,
which cares about internal variables, i.e., the learning dynamics inside of the agent itself.
Relation between PeVFA Value Approximation and Context-based Meta-RL. For a given MDP,
performing a policy in the MDP actually induces a Markov Reward Process (MRP) (Sutton & Barto,
1998). One can view the policy and actions are absorbed in the transition function of MRP. A value
function defines the expected long-term returns starting from a state. Therefore, different policies
induces different MRPs and PeVFA value approximation can be considered as a meta prediction
task. In analogy to context-based Meta-RL where a task context is learned to capture the underlying
transition function and reward function of a MDP (i.e., task), one can view policy representation as
the context of corresponding MRP, since it is the underlying variable that determines the transition
function of MRPs.
F	Experimental Details and Complete Results
F.1 Experimental Details
Environment. We conduct our experiments on commonly adopted OpenAI Gym1 continuous con-
trol tasks (Brockman et al., 2016; Todorov et al., 2012). We use the OpenAI Gym with version 0.9.1,
the mujoco-py with version 0.5.4 and the MuJoCo products with version MJPRO131. Our codes
are implemented with Python 3.6 and Tensorflow.
Implementation. We use Proximal Policy Optimization (PPO) (Schulman et al., 2017) with Gen-
eralized Advantage Estimator (GAE) (Schulman et al., 2016) as our baseline algorithm. Recent
works (Engstrom et al., 2020; Andrychowicz et al., 2020) point out code-level optimizations in-
fluence the performance of PPO a lot. For a fair comparison and clear evaluation, we perform no
1http://gym.openai.com/
26
Under review as a conference paper at ICLR 2021
code-level optimization in our experiments, e.g., state standardization, reward scaling, gradient clip-
ping, parameter sharing and etc. Our proposed algorithm PPO-PeVFA is implemented based on
PPO, which only differs at the replacement for conventional value function network with PeVFA
network. Policy network is a 2-layer MLP with 64 units per layer and ReLU activation, outputting
a Gaussian policy, i.e., a tanh-activated mean along with a state-independent vector-parameterized
log standard deviation. For PPO, the convention value network Vφπ(s) (VFA) is a 2-layer 128-unit
ReLU-activated MLP with state as input and value as output. For PPO-PeVFA, the PeVFA network
Vθ(s, χπ) takes as input state and policy representation χπ which has the dimensionality of 64, with
the structure illustrated in Figure 7. We do not use parameter sharing between policy and value
function approximators for more clear evaluation.
Training and Evaluation. We use Monte Carlo returns for value approximation. In contrast to
convention VFA Vφπ which approximates the value of current policy (e.g., Algorithm 1), PeVFA
Vθ(s, χπ) is additionally trained to approximate the values of all historical policies ({πi}it=0) along
the policy improvement path (e.g., Algorithm 2). The policy network parameterized by ω is then
updated with following loss function:
LPPO(ω) = -E∏ω- [min (PtA(St, at), CliP(Pt, 1 - e, 1 + e)A(st, a/)] ,	(15)
where A(st, at) is advantage estimation of old policy ∏ω-, which is calculated by GAE based on
conventional VFA Vφ or PeVFA V(s,χ∏) respectively, and Pt = ；：(；：/：))is the importance
πω- (at ,st )
sampling ratio. Note that both PPO and PPO-PeVFA update the policy according to Equation 15
and only differ at advantage estimation based on conventional VFA Vφπ or PeVFA Vθ(S, χπ). This
ensures that the performance difference comes only from different approximation of policy values.
Common learning parameters for PPO and PPO-PeVFA are shown in Table 2. For each iteration, we
update value function approximators first and then the policy with updated values. Such a training
scheme is used for both PPO and PPO-PeVFA. For evaluation, we evaluate the learning algorithm
every 20k time steps, averaging the returns of 10 episodes. Fewer evaluation points are selected and
smoothed over neighbors and then plotted in our learning curves below.
Table 2: Common hyperparameter choices of PPO and PPO-PeVFA.
Hyperparameters for PPO & PPO-PeVFA
Policy Learning Rate	10-4
Value Learning Rate	10-3
Clipping Range Parameter (e)	0.2
GAE Parameter (λ)	0.95
Discount Factor (γ)	0.99
On-policy Samples Per Iteration	5 episodes or 2000 time steps
Batch Size	128
Actor Epoch	10
Critic Epoch	10
Optimizer	Adam
Details for PPO-PeVFA. For PeVFA, the training process also involves value approximation of
historical policies and learning of policy representation. Training details are shown in Table 3.
PeVFA Vθ(S, χπ) is trained every 10 steps with a batch of64 samples from an experience buffer with
size of 200k steps. Policy representation model is trained at intervals of 10 or 20 steps depending on
OPR or SPR adopted. Due to 1k - 2k policies are collected in total in each trial, a relatively small
batch size of policy is used. For OPR, Random Mask (Figure 12) is performed on all weights and
biases of policy network except for the output layer (i.e., mean and log-std). For SPR, two buffers
of state-action pairs are maintained for each policy: a small one is sampled for calculating SPR and
the relatively larger one is sampled for auxiliary training (action prediction).
27
Under review as a conference paper at ICLR 2021
Table 3: Training details for PPO-PeVFA, including value approximation of historical policies and
policy representation learning. CL is abbreviation for Contrastive Learning and AUX is for auxiliary
loss of action prediction. In our experiments, grid search is performed for the best hyperparamter
configuration regarding terms with multiple alternatives (i.e., {}).
Value Approximation for Historical Policies
Value Learning Rate	10-3
Training Frequency	Every 10 time steps
Batch Size	64
Experience Buffer Size	200k (steps)
Policy Representation Learning
Training Frequency Policy Num Per Batch SPR s,a Pair Num CL Learning Rate CL Momentum CL Mask Ratio for OPR CL Sample Ratio for SPR AUX Learning Rate AUX Batch Size	Every {10, 20} time steps {16, 32} {200, 500} {10-3,5 ∙10-4} {5∙10-2,10-2,5∙10-3 } {0.1, 0.2} 0.8 10-3 {128,256}
F.2 Complete Learning Curves for Table 1
Corresponding to Table 1, an overall view of learning curves of all variants of PPO-PeVFA as well
as baseline algorithms are shown in Figure 16. One can refer to Figure 13 for Question 1 and Figure
14, 15 for Question 2, for clearer comparisons.
F.3 Other Experimental Analysis
In Figure 17, we analyze the sensitivity of value generalization (experimental results of PPO-
PeVFA) with respect to the number of historical policies that PeVFA is trained on. The results
show that the performance of PPO-PeVFA is not sensitive to relatively large numbers (i.e., 200k,
500k, 1000k) of historical state-action samples (proportional to the number of recent policies) used.
However, a small number of historical samples may induce less sufficient and stable training of
PeVFA, thus results in a slightly less improvement over the vanilla PPO.
F.4 Visualization of Learned Policy Representation
To answer Question 3, we visualize the learned representation of policies encountered during the
learning process.
Visualization Design. We record all policies on the policy improvement path during the learning
process of a PPO-PeVFA agent. For each trial in our experiments in MuJoCo continuous control
tasks, about 1k - 2k policies are collected. We run 5 trials and 5k - 12k policies are collected in
total for each task. We also store the policy representation model at intervals for each trial, and we
use last three checkpoints to compute the representation of each policy collected. For each policy
collected during 5 trials, its representation for visualization is obtained by averaging the results of
3 checkpoints of each trial and then concatenating the results from 5 trials. Finally, we plot 2D
embedding of policy representations prepared above through t-SNE (Maaten & Hinton, 2008) and
Principal Component Analysis (PCA) in sklearn2.
Results and Analysis. Visualizations of OPR and SPR learned in an end-to-end fashion in
HalfCheetah-v1 and Ant-v1 are in Figure 18 and 19. We use different types of markers to distin-
guish policies from different trials to see how policy evolves in representation space from different
random initialization. Moreover, we provide two views: performance view and process view, to
2https://scikit- learn.org/stable/index.html
28
Under review as a conference paper at ICLR 2021
HaIfCheetah-Vl
ULnlaH 96e3><
(a) HalfCheetah-v1
Hopper-vl
ULnlaH 96e3>v
(b) Hopper-v1
Wall<er2d-vl
2s0*-
-→- PPO
(c) Walker2d-v1
Ant∙vl
ULnlaH 96e」3>v
(d) Ant-v1
lnverted□oublePendulurn-vl
98« ■
(e) InvertedDoublePendulum-v1
(f) LunarLanderContinuous-v2
UJnlaH 36e3>v
(g) InvertedPendulum-v1


Figure 13:	Evaluations of PPO-PeVFA with end-to-end (E2E) trained OPR and SPR in MuJoCo
continuous control tasks. The results demonstrate the effectiveness of PeVFA and two kinds of
policy representation, answering the Question 1. The results are average returns and the shaded
region denotes half a standard deviation over 10 trials.
29
Under review as a conference paper at ICLR 2021
HaIfCbeetah-Vl
4299-
(a) HalfCheetah-v1
Hopper-Vl
279f-
(b) Hopper-v1
Walker2d-vl
ZnZ-
(c) Walker2d-v1
UJnlaH 36e3ΛV
(d) Ant-v1
UJnlaH 36e3ΛV
InvertedDoubIePenduIum-Vl
98C ■
(e) InvertedDoublePendulum-v1
UJnlaH 36e3ΛV
LUnarLanderCOntlnUOuS-V2
(f) LunarLanderContinuous-v2
Figure 14:	Evaluations of PPO-PeVFA with OPR and SPR trained through contrastive learning
(CL) in MuJoCo continuous control tasks. The results are average returns and the shaded region
denotes half a standard deviation over 10 trials.
3935
3«33-
281«-
2<β8-
IW>3-
υsβ-
889-
382-
-125-
HaIfCheetah-Vl
(a) HalfCheetah-v1
Ob I.S it 7∙S lie U!.S 130 lis 2«.«
Time Steps (le5}
-→- ppo
PPO-PeVFA w/ Rand pr
M 2、 s'o r'i ι⅛0 i2s ιi.t ιτ∙s 2«.«
Time Steps (le5}
(b) Hopper-v1
ULnlaH 96e3>v
ULnlaH 96e3>v
ULnlaH 96e3>v
WaIkerZd-Vi
2533-
2244-
U54-
Wβ5-
U75-
1085-
78β-
50β-
217-

(c) Walker2d-v1
M z：s s⅛ τ's ι⅛o ιi∙s ɪio its z«.«
Time Steps (le5}
(d) Ant-v1
InvertedDoubIePenduIum-Vl
988 ∙
(e) InvertedDoublePendulum-v1
LUnarLanderCOnHnUOuS-V2
275.1 -
(f) LunarLanderContinuous-v2
Figure 15:	Evaluations of PPO-PeVFA with OPR and SPR trained through auxiliary loss of action
prediction (AUX) in MuJoCo continuous control tasks. The results are average returns and the
shaded region denotes half a standard deviation over 10 trials.
30
Under review as a conference paper at ICLR 2021
Hopper-vi
2930-
一
2599-
226θ-
1936-
HaIfCheetah-Vl
4299-
一
3749-
3199
W 2100-
IOOO-
450-
-99
-649
(a) HalfCheetah-v1
Walker2d-vl
2712-
2092-
U 1781-
B50
Time Steps (Ie5)
(c) Walker2d-v1
InvertedDoubIePenduIum-Vl
9B50-
B709 -
756B-
3003-
Iβ62-
721 -
6
g 1550-
£ 1471
3
6
•0 1160
E 2650-
E 6426-
B 52β5
Ol
6
(O 4144
10.0	12.5	15.0	17.5	20.0
Time Steps (le5)
195.2 -
114.5 -
33.8-
-46.9 -
-127.6-
-20β.2 -
-2BB.9 -
-369.6-
-450.3 -
0.0	2.5	5.0	7.5	10.0	12.5	15.0	17.5	20.0
Time Steps (5e4)
Time Steps (Ie5)
(b) Hopper-v1
Ant-vl
4396-
1255-
Time Steps (5e4)
UJn⅛α:ωCTEω><
PPO
PPO-PeVFA Wl Rand PR
PPO-PeVFA wl OPR (E2E)
PPO-PeVFA wl SPR (E2E)
PPO-PeVFA w/ OPR (CL)
PPO-PeVFA wl SPR (CL)
PPO-PeVFA w/ OPR (AUX)
PPO-PeVFA wl SPR (AUX)
2 1605-
9
6
ta 1273
PPO
PPO-PeVFA Wl Rand PR
PPO-PeVFA wl OPR (E2E)
PPO-PeVFA wl SPR (E2E)
PPO-PeVFA w/ OPR (CL)
PPO-PeVFA wl SPR (CL)
PPO-PeVFA wl OPR (AUX)
PPO-PeVFA w/ SPR (AUX)
2402-
-→- PPO
PPO-PeVFA Wl Rand PR
-→- PPO-PeVFA wl OPR (E2E)
-→- PPO-PeVFA wl SPR (E2E)
--PPO-PeVFA wl OPR (CL)
—PPO-PeVFA wl SPR (CL)
PPO-PeVFA Wl OPR (AUX)
PPO-PeVFA w/ SPR (AUX)
T- PPO
f PPO-PeVFA w/ Rand PR
-*- PPO-PeVFA w/ OPR (E2E)
-→- PPO-PeVFA w/ SPR (E2E)
-*- PPO-PeVFA w/ OPR (CL)
-*- PPO-PeVFA w/ SPR (CL)
PPO-PeVFA w/ OPR (AUX)
PPO-PeVFA w/ SPR (AUX)
3β72-
3349-
2Θ25-
2302-
-→- PPO
PPO-PeVFA Wl Rand PR
-→- PPO-PeVFA wl OPR (E2E)
-→- PPO-PeVFA wl SPR (E2E)
--PPO-PeVFA wl OPR (CL)
—PPO-PeVFA wl SPR (CL)
PPO-PeVFA w/ OPR (AUX)
PPO-PeVFA w/ SPR (AUX)
1778-
Time Steps (2e5)
(d) Ant-v1
LcinarLandercontinciocis-VZ





(e) InvertedDoublePendulum-v1	(f) LunarLanderContinuous-v2
Figure 16:	An overall view of performance evaluations of different algorithms in MuJoCo contin-
uous control tasks. The results are average returns and the shaded region denotes half a standard
deviation over 10 trials.
see how policies are aligned in representation space regarding performance and ‘age’ of policies
respectively.
Visualization of OPR trained in end-to-end fashion is shown in Figure 18. From the performance
view, it is obvious that policies of poor and good performances are aligned from left to right in t-
SNE representation space and are aligned at two distinct directions in PCA representation space. An
evolvement of policies from different trials can be observed in subplot (b) and (d). Thus, policies
31
Under review as a conference paper at ICLR 2021
4104-
Ant-Vl
Time Steps (2e5)
Figure 17: Results for PPO-PeVFA with End-to-End (E2E) OPR on Ant-v1 with respect to the
number of historical policies that PeVFA is trained on. The setting varies at the numbers of historical
samples generated by recent policies to different degrees, e.g., 50k denotes 50k steps of state-action
samples from recent historical policies used for the training of PeVFA. The results are average
returns and the shaded region denotes half a standard deviation over 10 trials.
from different trials are locally continuous; while policies are globally consistent in representation
space with respect to policy performance. Moreover, we can observe multimodality for policies with
comparable performance. This means that the obtained representation not only reflects optimality
information but also maintains the behavioral characteristic of policy.
-IOO -75 -50 -25	0	25	50	75 100	-5.0 -2.5 0.0 2.5 5.0 7.5 10.0
(a) Performance VieW in HalfCheetah-v1
-100 -75 -50 -25	0	25	50	75 100	-5.0 -2.5 0.0 2.5 5.0 7.5 10.0
(b) Process View in HalfCheetah-v1
-100 -75 -50 -25 0	25	50	75 100	-4	-2	0	2	4 β
(c) Performance View in Ant-VI
(d) Process View in Ant-VI
Figure 18: Visualizations of end-to-end (E2E) learned Origin Policy Representation (OPR) for
policies collected during 5 trials (denoted by different kinds of markers). In total, about 6k policies
are plotted for HalfCheetah-V1 (a-b) and 12k for Ant-V1 (c-d). In each subplot, t-SNE and PCA
2D embeddings are at left and right respectiVely. In performance View, each policy (i.e., marker) is
colored by its performance eValuation (aVeraged return). In process View, each policy is colored by
its corresponding iteration ID during GPI process.
-100 -75 -50 -25	0	25	50	75 100
-4
32
Under review as a conference paper at ICLR 2021
Parallel to OPR, end-to-end trained SPR is visualized in Figure 19. A more obvious multimodality
can be observed in both t-SNE and PCA space: policies from different trials start from the same
region and then diverge during the following learning process. Different from OPR, SPR shows
more distinction among different trials since SPR is a more direct reflection of policy behavior
(dynamics property as mentioned in Sec. D.6). Another thing is, policies from different trials forms
wide ‘strands’ especially in t-SNE representation space. We conjecture that it is because SPR is a
more stochastic way to obtain representation as random selected state-action pairs are used.
tSNE
75
50
25
O-
-25
-50
-75
-IOO -75 -50 -25 O 25	50	7 5 IOO
■ 3000
25∞
■ 2000
15∞
■ IOOO
500
tSNE
75 ■
50
25 ■
O
-25 -
-50
-75 -
-IOO -75 -50 -25 O 25	50	75 IOO
PCA
800
600
400
200
O
(a)	Performance VieW in HalfCheetah-v1
(b)	Process View in HalfCheetah-v1
100
75 ■
50
25 ■
O-
-25 -
-50-
-75 -
tSNE
-75 -50 -25	0	25	50	75 IOO
100
75
50
25
O-
-25
-50
-75
tSNE
-75 -50 -25 O 25	50	75 IOO
PCA
-5 O 5	10 15 20 25 30
-1500
-1250
-IOOO
-750
-500
-250
-O
(c) Performance View in Ant-VI
(d) Process View in Ant-VI
Figure 19: Visualizations of end-to-end (E2E) learned Surface Policy Representation (SPR) for
policies collected during 5 trials (denoted by different kinds of markers). In performance View, each
policy (i.e., marker) is colored by its performance eValuation (aVeraged return). In process View,
each policy is colored by its corresponding iteration ID during GPI process.
33