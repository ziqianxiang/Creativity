Under review as a conference paper at ICLR 2021
Error Controlled Actor-Critic Method to
Reinforcement Learning
Anonymous authors
Paper under double-blind review
Ab stract
In the reinforcement learning (RL) algorithms which incorporate function ap-
proximation methods, the approximation error of value function inevitably causes
overestimation phenomenon and impacts algorithm performances. To mitigate
the negative effects caused by approximation error, we propose a new actor-critic
algorithm called Error Controlled Actor-critic which ensures confining the ap-
proximation error in value function. In this paper, we derive an upper boundary
of approximation error for Q function approximator in actor-critic methods, and
find that the error can be lowered by keep new policy close to the previous one
during the training phase of the policy. The results of experiments on a range of
continuous control tasks from OpenAI gym suite demonstrate that the proposed
actor-critic algorithm apparently reduces the approximation error and significantly
outperforms other model-free RL algorithms.
1	Introduction
Reinforcement learning (RL) algorithms are combined with function approximation methods to
adapt to the application scenarios whose state spaces are combinatorial, large, or even continuous.
Many function approximation methods RL methods, including the Fourier basis (Konidaris et al.,
2011), kernel regression (Xu, 2006; Barreto et al., 2011; Bhat et al., 2012), and neural neworks (Bar-
to et al., 1982; Tesauro, 1992; Boyan et al., 1992; Gullapalli, 1992) have been used to learn value
functions. In recent years, many deep reinforcement learning (DRL) methods were implemented by
incorporating deep learning into RL methods. Deep Q-learning Network (DQN) (Mnih et al., 2013)
reported by Mnih in 2013 is a typical work that uses a deep convolutional neural network (CNN) to
represent a suitable action value function estimating future rewards (returns); it successfully learned
end-to-end control policies for seven Atari 2600 games directly from large state spaces. Thereafter,
deep RL methods, such as Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2016),
Proximal Policy Optimization (PPO) (Schulman et al., 2017), Twin Delayed Deep Deterministic
policy gradient (TD3) (Fujimoto et al., 2018), and Soft Actor-Critic (SAC) (Haarnoja et al., 2018),
started to become mainstream in the field of RL.
Althouth function approximation methods have assisted reinforcement learning (RL) algorithms to
perform well in complex problems by providing great representation power; however, they also
cause an issue called overestimation phenomenon that jeopardize the optimization process of RL
algorithms. Thrun & Schwartz (1993) presented a theoretical analysis of this systematic overes-
timation phenomenon in Q-learning methods that use function approximation methods. Similar
problem persists in the actor-critic methods employed function approximation methods. Thomas
(2014) reported that several natural actor-critic algorithms use biased estimates of policy gradient
to update parameters when using function approximation to approximate the action value function.
Fujimoto et al. (2018) proved that the value estimation in the deterministic policy gradient method
also lead to overestimation problem. In brief, the approximation errors of value functions caused the
inaccuracy of estimated values, and such inaccuracy induced the overestimation on value function;
so that poor performances might be assigned to high reward values. As a result, policies with poor
performance might be obtained.
Previous works attempted to find direct strategies to effectively reduce the overestimation. Hasselt
(2010) proposed Double Q-learning, in which the samples are divided into two sets to train two
independent Q-function estimators. To diminish the overestimation, one Q-function estimator is
1
Under review as a conference paper at ICLR 2021
used to select actions, and the other one is applied to estimate its value. Fujimoto et al. (2018)
proposed mechanisms, including clipped double Q-learning and delayed policy updates, to minimize
the overestimation.
In contrast to these methods, we focus on actor-critic setting and manage to reduce the approxima-
tion error of value function, which is the source of the overestimation, in an indirect but effective
way. We use the concepts of domain adaptation (Ben-David et al., 2010) to derive an upper boundary
of the approximation error in Q function approximator. Then, we find that the least upper bound of
this error can be obtained by minimizing the Kullback-Leibler divergence (KL divergence) between
new policy and its previous one. This means minimizing the KL divergence when traning policy can
stabilize the critic and then confine the approximation error in Q function. Interestingly, we arrive
at similar conclusion as two literatures Geist et al. (2019); Vieillard et al. (2020) by a somewhat dif-
ferent route. In their works, the authors directly studied the effect of KL and entropy regularization
in RL and proved that a KL regularization indeed leads to averaging errors made at each iteration of
value function update. While our idea is very different from theirs: It is impracticable to minimize
the approximation error directly, so instead we try to minimize an upper bound of approximation
error. This is similar to Expectation-Maximization Algorithm (Bishop, 2006) which maximize a
lower bound of log-likelihood instead of log-likelihood directly. We derive an upper boundary of
approximation error for Q function approximatorin actor-critic methods, and arrive at a more general
conclusion: approximation error can be reduced by keep new policy close to the previous one. Note
that KL penalty is a effective way, but not the only way.
Furthermore, the mentioned indirect operation (i.e. the KL penalty) can work together with the men-
tioned direct strategies for reducing overestimation, for example, clipped double Q-learning. Then,
a new actor-critic method called Error Controlled Actor-critic (ECAC) is established by adopting an
effective operation that minimizes the KL divergence to keep the upper bound as low as possible.
In other words, this method ensures the similarity between every two consecutive polices in training
process and reduces the optimization difficulty of value function, so that the error in Q function
approximators can be decreased. Ablation studies were performed to examine the effectiveness of
our proposed strategy for decreasing the approximation error, and comparative evaluations were
conducted to verify that our method can outperform other mainstream RL algorithms.
The main contributions of this paper are summarized as follow: (1) We presented an upper boundary
of the approximation error in Q function approximator; (2) We proposed a practical actor-critic
method—ECAC which decreases the approximation error by restricting the KL divergence between
every two consecutive policies and adopt a mechanism to automatically adjust the coefficient of KL
term.
2	Preliminaries
2.1	Reinforcement Learning
Reinforcement learning (RL) algorithms are modeled as a mathematical framework called Markov
Decision Process (MDP). In each time-step of MDP, an agent generates an action based on current
state of its environment, then receives a reward and anew state from the environment. Environmental
state and agent’s action at time t are denoted as st ∈ S and at ∈ A, respectively; S and A denote
the state and action spaces respectively, which may be either discrete or continuous. Environment
is described by a reward function, r(st, at), and a transition probability distribution, P r(st+1 =
s0|st = s, at = a). Transition probability distribution specifies the probability that the environment
will transition to next state. Initial state distribution is denoted as Pr0 (s).
Let π denotes a policy, η(π) denotes its expected discounted rewards:
∞
η(π) = En [Ri + γR2 + γ2R3 + …]=E∏[X Yk Rt+ι],	(1)
t=0
where Y denotes a discount rate and 0 ≤ Y ≤ 1. The goal of RL is to find a policy, ∏*, that
maximizes a performance function over policy, J (π), which measures the performance of policy:
π* = arg max J(π).	(2)
π
2
Under review as a conference paper at ICLR 2021
A natural form of J(π) is η(π). Different interpretations of this optimization goal lead to different
routes to the their solutions.
Almost all reinforcement learning algorithms involve estimating value functions, including state-
value and action-value functions. State-value function, V π(s), gives the expected sum of discounted
reward when starting in s and following a given policy, π. V π (s) specified by:
∞
Vπ(s)=Eπ[XγkRt+k+1 |st = s].	(3)
k=0
Similarly, action-value function, Qπ (s, a), is given by:
∞
Qn(s, a) = En[X YkRt+k+ι∣St = s, at = a].	(4)
k=0
2.2	Actor-critic Architecture
To avoid confusion, by default, we discuss only RL methods with function approximation in this
section.
RL methods can be roughly divided into three categories: 1) value-based, 2) policy-based, and 3)
actor-critic methods. Value-based method only learn value functions (state-value or action-value
functions), and have the advantage of fast convergence. Policy-based methods primarily learn pa-
rameterized policies. A parameterized policy (with parameter vector, θ) is either a distribution
over actions given a state, ∏θ(a|s), or a deterministic function, a = ∏θ(s). Their basic update is
θn+ι = θn + αVJ(θn) Where is learning rate. Policy based methods show better convergence guar-
antees but have high variance in gradient estimates. Actor-critic methods learn both value functions
and policies and use value functions to improve policies. In this way, they trade off small bias in
gradient estimates to low variance in gradient estimates.
Actor-critic architecture (Peters & Schaal, 2008; Degris et al., 2013; Sutton & Barto, 2018) consists
of two components: actor and critic modules. Critic module learns learns state-value function,
Vφ(s), or action-value function, Qφ(s, a) or both of them, usually by temporal-difference (TD)
methods. Actor module learns a stochastic policy, ∏θ(a|s), or a deterministic policy, a = ∏θ(s),
and utilizes value function to improve the policy. For example, in actor module of DDPG (Lillicrap
et al., 2016), the policy is updated by using the following performance function
J(θ) = Eπθ [Qφ(st, πθ(st))],	(5)
where πθ(st) is a deterministic policy.
2.3	Domain Adaptation
Domain adaptation is a task which aims at adapting a well performing model from a source domain
to a different target domain. It is used to describe the task of critic module in section 3.2. The
learning task of critic module is viewed as adapting a learned Q function approximator to next one,
and the target error equates to the approximation error at current iteration of critic update. Here, we
present some concepts in domain adaptation, including domain, source error, and target error.
Domain is defined as a specific pair consisting of a distribution, P, on an input space, X, and a
labeling function, f : X → R. In domain adaption, source and target domains are denoted as
hPS, fSi and hPT, fTi, respectively. A function, h : X → R, is defined as a hypothesis.
Source error is the difference between a hypothesis, h(x), and a labeling function of source domain,
fS(x), on a source distribution which is denoted as follow:
eS (h, fS) = E [|h(x) - fS (x)|].	(6)
X〜PS
Target error is the difference between a hypothesis, h(x), and a labeling function of target domain,
fT (x), on a target distribution which is denoted as follow:
eT (h, fT) = E [|h(x) - fT (x)|].	(7)
X〜PT
For convenience, we use the shorthand eS(h) = eS(h, fS) and eT (h) = eT (h, fT).
3
Under review as a conference paper at ICLR 2021
3	Error Controlled Actor-critic
To reduce the impact of the approximation error, we propose a new actor-critic algorithm called error
controlled actor-critic (ECAC). We present the details of a version of ECAC for continous control
tasks, and, more importantly, explain the rationale for confining the KL divergence. In section 3.2,
we will show that, at each iteration of critic update, the target error equals to the approximation error
of Q function approximator. Then, we derive an upper boundary of the error, and find that the error
can be reduced by limiting the KL divergence between every two consecutive policies. Although this
operation is conducted when training the policy, it can indirectly reduce the optimization difficulty of
Q function. Moreover, this indirect operation can work together with the strategies for diminishing
overestimation phenomenon. We incorporate clipped double-Q learning into the critic module.
3.1	Critic module-learning action value functions
The learning task of critic module is to approximate Q functions. In the critic module of ECAC,
two Q functions are approximated by two neural networks with weight φ(1) and φ(2), respectively.
As noted previously, we adopt clipped double-Q strategy (Fujimoto et al., 2018) to directly reduce
overestimation. Furthermore, we adopt experience replay mechanism (Lin, 1992)—agent’s experi-
ences at each timestep, (st, at, rt+1, st+1), are stored in a replay buffer, D; and training samples
are uniformly drawn from this buffer. The two Q networks, Qφ(1) and Qφ(2) , are trained by using
temporal-difference learning. Notice that the bootstrapping operation in this setting—uses function
approximation—means to minimize the following two TD errors of Q funtion:
δt(j) = Qφ(j) (st, at) - (Rt+1 +γ min Qφ(i) (st+1, at+1)).	(8)
i=1,2
Notice that clipped double-Q learning uses the smaller of the two Q values to form the TD error. With
the minimum operator, it decreases the likelihood of overestimation by increasing the likelihood of
underestimation. The two Q networks are respectively trained by minimizing the following two loss
functions:
L(φ(1)) =	E	[Qφ(1) (s, a) - (r+γ min Qφ(i) (s0, a0))]2	(9)
(s,a,r,s0)〜PD	i=1,2
s0 〜Pr(T s,a),
a0 〜∏(∙∣s0)
L(φ(2)) =	E	[Qφ(2) (s, a) - (r+γ min Qφ(i) (s0, a0))]2	(10)
(s,a,r,s0)〜PD	i=1,2
s0 〜Pr(T s,a),
a0 〜∏(∙∣s0)
where D denotes replay buffer, PD denotes the distribution that describes the likelihoods of samples
drawn from D uniformly, Pr(∙∣s, a) denotes the transition probability distribution, and π denotes
the target policy.
3.2 An upper bound of the approximation error of Q function
For convenience, we analyze the setting with only one Q function. The concept of domain adaptation
is used to describe the task of critic module. The learning task of critic module can be viewed as
adapting the learned Q-network to a new Q function for newly learned policy. Thus, naturally, target
error, i.e. Eq. (7), equates to the approximation error at current iteration of critic update. in this
section, we derive an upper bound of the approximation error of Q function and find that the upper
bound of approximation error will be smaller if the more similar the two consecutive policies.
Fig 1 illustrates the training process ofan actor-critic method which is a alternating process of value
function. At (n + 1)-th iteration, the critic module tries to fit the value function, Qπθn , by means of
πθn . But because of appriximation error, the actually obtained Q-network (approximator), Qφn+1 ,
is not equal to Qπθn . This can be expressed by the following equation:
Qφn+1 (s, a) = Qπθn (s, a) + sn,+a1,	(11)
where sn,+a 1 denotes the appriximation error in Q function given state, s, and action, a. This can
be viewed as adapting the learned Q-network, Qφn, to the value function, Qπθn . Hence, the source
distribution is PDn, and the target distribution is PDn+1. As mentioned at the argument following
4
Under review as a conference paper at ICLR 2021
Eq. (9) and (10), PD denotes the distribution that describes the likelihoods of samples drawn from
replay buffer D uniformly. Dn and Dn+1 are replay buffers at n-th and (n + 1)-th iteration, respec-
tively. Qφn is the labeling function. Clearly, the target error here equals to the approximation error
at current iteration of critic update. This can be expressed by the following equation:
(12)
eτ(Qφ) =	E	[∣Qφ(s, a) - Qπθn(s, a)|] = E	尾：ι].
s,a 〜PDn+1	s,a 〜PDn+1
Figure 1: Alternating process of Actor-critic alternates between value function and policy updates.
In addition, because TD method is use to estimate Qφn, the real labeling function in target domain
is actually the following one:
y∏θn (s, a) = Rt+1 + γQφn (s0, a0), s0 〜Pr(∙∣s, a), a0 〜∏θn (∙∣s),	(13)
where Pr(∙∣s, a) denotes the transition probability distribution, and ∏θn denotes the policy updated
in actor module by using Qφn . Hence, the target error here means the difference between the
Q-network, Qφ(s, a), and its target (labeling function) at (n + 1)-th iteration, yπθ , on a target
distribution. The actual target error is given by
eτ (Qφ) =	E	[∣Qφ(s, a) - y∏θn (s, a)|] = E	[∣Qφ(s, a) - [Rt+ι + γQφn (s0, a0)]∣].
s,a 〜PDn+1	s,a 〜PDn+1 ,
s0 〜Pr(T s,a),
a0 〜πθn
(14)
Furthermore, two types of error are used to derive upper bound of appximation error, including
source error eS(Qφ) and error eS(Qφ, yπθ ). Source error error here is the difference between the
Q-network, Qφ(s, a), and its target (labeling function) at the n-th iteration, yπθ , on a source
distribution. Hence, the approximation error is given by
es(Qφ) =	E	[∣Qφ(s, a)-y∏θn-1 (s, a)|] = E	[∣Qφ(s, a) — [Rt+1 + γQφn- (s', a')]∣]∙
s,a〜PDn	s,a〜PDn ,
s0 〜Pr(T s,a),
a0 〜∏θ 1
n-1
(15)
Error eS(Qφ, yπθn ) is the difference between the Q-network, Q(s, a), and its target at (n + 1)-th
iteration, yπθ , on a source distribution, which is given by
es(Qφ,y∏θn) = E	[lQΦ(s,a) -y∏θn(S,a)|] = E	[lQΦ(s,a) - [Rt+ι + γQΦn(S',a')]|].
s,a〜PDn	s,a〜PDn ,
s0 〜Pr(∙∣ s,a),
a0 〜πθn
(16)
Target error can be derived into the following inequality:
eT(Qφ) =eT(Qφ) + eS(Qφ) - eS(Qφ) + eS(Qφ, yπθn ) - eS(Qφ, yπθn )
≤eS (Qφ) + |eS (Qφ, y∏θn ) — es (Q。)| + 归 T(Qφ ) — es (Qφ, y∏θn ) |	(17)
≤eS (Qφ) + EP [ly∏θn (s, a) — y∏θn-1 (s，a)|] + leT (Q φ) — es (Qφ, y∏θn )t
5
Under review as a conference paper at ICLR 2021
where the third term in the third line, |eT (Qφ) - eS(Qφ, yπθn )|, is transformed further as:
IeT(Qφ) - es(Qφ,y∏θn)|
= E	[lQΦ (s, a) — y∏θn (s, a)|] 一 E	[lQΦ(s, a) — y∏θn (s, a)|]
s,a~PDn+ι	s,α~PDn
D	(18)
≤γ E	E	[Qφn(s0,a0)] - E	[Qφn(s0,a0)] .
S,a~P°n s0~Pr(∙∣s,a),	s0~ Pr (∙∣ s,a),
a0~πθn G| SO)	a0~πθn-ι G|s0)
Recall that D is actually the replay buffer; and PD denotes the distribution that describes the likeli-
hoods of samples drawn from replay buffer D uniformly. Because, in experience replay mechanism
(Lin, 1992), the number of samples in Dn+1 is only a little more than in Dn, the difference between
PDn and PDn+1 can be ignored. Finally, the upper bound of error in Q-network is determined by
eτ(Qφ) ≤es(Qφ) + sα%ιJy∏θn(s,a)- y∏θn-1(S, a)|]
+ γ E	E	[Qφn(s0, a0)] - E	[Qφn(s0, a0)]
S,a~P°n s0~Pr(∙∣s,a),	s0~Pr (∙∣ s,a),
a0~πθn ("s')	a0~πθn-ι (TSO)
(19)
It is noticeable that the third term in the upper bound will be smaller if the more similar the two
consecutive policies, πθn-1 and πθn are. Hence, we can conclude that confining the KL divergence
between every two consecutive policies can help limit the approximation error during the optimiza-
tion process of actor-critic. This conclusion is used to design the learning method of the policy.
3.3 Actor module-learning a policy
The learning task of the actor module is to learn a parameterized stochastic policy, ∏θ(a∣s).In order
to lower the upper bound of approximation error of Q function (or to reduce the optimization dif-
ficulty of the Q functions), the goal of the policy is converted from maximizing expect discounted
return two parts: maximizing the estimated Q values—the minimum of the two Q approximators—
and, concurrently, minimizing the KL divergence between two successive policies. The optimization
objective is specified by
max E [minQφi(s,eθ(S)) — dkl3(∙ls),πθoid(is))],	(20)
θ s~Pd i=1,2
where D is the replay buffer; PD denotes the distribution that describes the likelihoods of sam-
ples drawn from D uniformly; θ is the parameters of the policy network; θold is the parameters
of the policy updated in the last iteration; aeθ (S) is the samples drawn from the target stochastic
PoliCy∏θ(α∣s). Note that, in order to back-propagate the error through this sampling operation, We
use a Diagonal Gaussian policy and the reparameterization trick, i.e. samples are obtained according
to
Gθ(s)= μθ(s) + σθ(s) Θ ξ, ξ 〜N(0, I),	(21)
where μe(s) and σe(s) are the output of the policy network, and denotes the mean and covariance
matrix’s diagonal elements, respectively.
The KL divergence between two distributions, for example p(x) and q(x), can be thought of the
difference between the cross entropy and entropy, which is specified by
DKL(p||q) = H(p,q) - H (p),	(22)
where H(p, q) denotes the cross entropy between p(x) and q(x), and H(p) denotes the entropy of
p(x). In practice, we find it is more effective to minimize the cross entropy between two succes-
sive policies and to maximize the entropy of current policy, separately, than to minimize the KL
divergence directly. Hence, we expand the original objective Eq. 20 into the following one:
∣mX sM[mi*φi(s, eθ (S))- αH (πθ (∙ls),πθoid (∙ls)) + BH (πθ *))]，
(23)
6
Under review as a conference paper at ICLR 2021
where α and β denote the coefficients of the cross entropy and the entropy, respectively.
Moreover, we adopt a mechanism to automatically adjust α and β. To do this, α is adjusted by keep-
ing the current cross entropy close to a target value. This mechanism is specified by the following
optimization objective:
min E [logα ∙ ((δκL + δentropy) - H(∏θ(∙∣s),∏θ0id(∙∣s)))],	(24)
α S 〜PD
where δKL denotes the target KL value and δentropy denotes the target entropy. Note that log(∙) is
use to ensure that α is greater than 0. β is adjusted in the same way:
min E [logβ ∙ (H(∏θ(∙∣s)) - δentropy)].	(25)
β SZPD
The overall training process is summarized in Appendix A, and code can be found on our GitHub
https://github.com/SingerGao/ECAC.
4	Experiments
The experiments aim to evaluate the effectiveness of the proposed strategy to lower the approxima-
tion error and to verify that the proposed method can outperform other mainstream RL algorithms.
The experiments for ablation study and comparative evaluation are conducted on a few challeng-
ing continuous control tasks from the OpenAI Gym(Brockman et al., 2016) environments, which
includes MUjoCo(TodoroV et al., 2012) and PybUllet (CoUmans & Bai, 2016-2019) versions. Imple-
mentation details and hyperparameters of ECAC are presented in Appendix B.
4.1	Ablation S tudy
Ablation stUdies are performed to verify the contribUtion of the operation of KL limitation. We
compared the performance of ECAC with the method removing KL limitation from ECAC.
0.0	0.2	0.4	0.6	0.8	1.0
Million steos
(a) Hopper-v3
0.0	0.2	0.4	0.6	0.8	1.0
Million steos
(b) Walker2d-v3
Figure 2: Performance comparison of the method with KL limitation and the one without KL lim-
itation on the Hopper-v3 and Walker2d-v3 benchmark. The method with KL limitation performs
better. Curves are smoothed uniformly for visual clarity.
Figure 2 compares five different instances for both methods with and without KL limition using dif-
ferent random seeds; and each instance performs five evaluation episodes every 1, 000 environment
steps. The solid curves corresponds to the mean and the shaded region to the minimum and maxi-
mum returns over the five runs. The experimental result shows that our method with KL limitation
performs better than the one without KL limitation. Figure 3 demonstrates that by using ECAC the
KL divergence remains comfortably low during all the training process.
Furthermore, to verify that confining the KL divergence can decrease the approximation error in
Q function, we measured the normalized approximation error in 100 random states every 10, 000
environment steps. The normalized approximation error is specified by
eQ
pprox
Qtrue
(26)
ure
where Qapprox is the approximate Q value given by the current Q network, and Qture is the true
discounted return. The true value is estimated using the average discounted return over 100 episodes
7
Under review as a conference paper at ICLR 2021
with KL limitation
without KL limitation
φo⊂ΦEω>~Q
(a) Hopper-v3	(b) Walker2d-v3
with KL limitation
without KL limitation
one without KL
Figure 3: The KL divergence comparison of the method with KL limitation and the
limitation on the Hopper-v3 and Walker2d-v3 benchmark.
1O3
with KL Iimitaion
without KL Iimitaion
----- with KL Iimitaion
—— without KL Iimitaion
(a) Hopper-v3	(b) Walker2d-v3
Figure 4: The approximation error comparison of the method with KL limitation and the one without
KL limitation on the Hopper-v3 and Walker2d-v3 benchmark.
following the current policy, starting from states sampled from the replay buffer. Figure 4 shows that
the method with KL limitation has lower error in the Q function.
The results of all the ablation studies indicates that the approximation error of Q functioncan be
decreased and the performance of the RL algorithm can be improved by placing restrictions on KL
divergence between every two consecutive policies.
4.2	Comparative Evaluation
Comparative evaluation are conducted to verify that our method can outperform other traditional RL
methods including A2C, PPO, TD3, and SAC. Five individual runs of each algorithm with different
random seeds are done; and each run performs five evaluation episodes every 1, 000 environment
steps. Our results are reported five random seeds (one random seed for one individual run) of the
Gym simulator, the network initialization, and sampling actions from policy during the training.
The results of max average return over five runs on all the 10 tasks are presented in Table 1. ECAC
outperforms all other algorithms on the tasks except Hopper-v3 and HumanoidBulletEnv-v0 are
only next to TD3 on Hopper-v3 and HumanoidBulletEnv-v0. Figure 5 and Figure 6 demonstrates
learning curves of comparative evaluation on the 10 continuous control tasks (Mujoco and PyBullet
version, respectively).
5	Conclusion
This paper presented a model-free actor-critic method based on a finding that the approximation
error in value function of RL methods can be decreased by placing restrictions on KL-divergence
between every two consecutive policies. Our method increases the similarity between every two
consecutive polices in the training process and therefore reduces the optimization difficulty of value
function. In the ablation studies, we compare the approximation error in Q function, KL divergence,
and performance of the methods with and without KL limitation. The results of ablation study show
that the proposed method can decrease the approximation error and improved the performance.
Moreover, the results of comparative evaluation demonstrate that ECAC outperforms other model-
free deep RL algorithm including A2C, PPO, TD3, and SAC.
8
Under review as a conference paper at ICLR 2021
(a) Hopper-v3	(b) Walker2d-v3
0.0	0.2	0.4	0.6	0.8	1.0
Million steps
⑹ HalfCheetah-v3
(d) Ant-v3
(e) Humanoid-v3
Figure 5: The results of comparative evaluation on Mujoco version of the OpenAI gym continuous
control tasks. Curves are smoothed uniformly for visual clarity.
0.0	0.2	0.4	0.6	0.8	1.0
Million steps
0.0	0.2	0.4	0.6	0.8
Million steps
0.0	0.2	0.4	0.6	0.8
Million steps
(a) HopperBulletEnv-v0
(c) HalfCheetahBulletEnv-v0
(b) Walker2DBulletEnv-v0
0.0	0.2	0.4	0.6	0.8	1.0
Million steps
(d)	AntBulletEnv-v0
0.0	0.5	1.0	1.5	2.0	25
Million steps
(e)	HumanoidBulletEnv-v0
Figure 6: The results of comparative evaluation on Pybullet version of the OpenAI gym continuous
control tasks. Curves are smoothed uniformly for visual clarity.
9
Under review as a conference paper at ICLR 2021
Table 1: Max average return over five runs on all the 10 tasks. Maximum value for each task is
bolded. ± corresponds to standard deviation over runs.
Environment (Total steps)	ECAC	A2C	PPO	TD3	SAC
Hopper (106)	3395.4 ± 69.5	2875.8 ± 118.0	3317.6 ± 150.3	3493.6 ± 134.2	3175.2 ± 110.0
Walker2d (106)	5146.2 ± 243.8	2479.4 ± 561.7	4811.1 ± 295.7	3960.3 ± 382.5	4455.1 ± 945.4
HalfCheetah (106)	11744.7 ± 746.9	2653.5 ± 975.9	3422.1±1039.4	9492.6 ± 852	9211.6±1051.3
Ant (106)	5420.8 ± 1069.6	1651.6 ± 140.5	2502.1 ± 442.8	3462.4 ± 488.5	3832.2 ± 927.7
HumanOid (5 ∙ 106)	8953.4 ± 244.8	751 ± 34.2	844.2 ± 83.2	6969.2 ± 403.2	8265.8 ± 937.1
HopperBulletEnv (106)	2642.5 ± 39.3	1688.7 ± 68.9	2255.5 ± 342.9	2488.4 ± 172.6	2397.7 ± 56.9
Walker2DBulletEnv (106)	2323 ± 161.8	1005.4 ± 5.7	1035 ± 201.2	2116.6 ± 151.9	1651.6 ± 408.9
HalfCheetahBulletEnv (106)	2794.9 ± 281.8	2027.7 ± 87.8	1730 ± 639	2012.8 ± 182.2	2202.4 ± 243
AntBulletEnv-v0 (106)	2997.6 ± 220.9	2292.4 ± 175.1	969.7 ± 31.5	2953.9 ± 84.3	2650.2 ± 188
HumanOidBulletEnv (3 ∙ 106)	1226.7 ± 45.3	110.5 ± 5.4	208.6 ± 10.9	1471.1 ± 113.9	1052.5 ± 85.6
References
Andre Barreto, Doina Precup, and Joelle Pineau. Reinforcement learning using kernel-based
stochastic factorization. In J. Shawe-Taylor, R. S. Zemel, P. L. Bartlett, F. Pereira, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 24, pp.
720-728. Curran Associates, Inc., 2011. URL http://papers.nips.cc/paper/
4217- reinforcement- learning- using- kernel- based- stochastic- factorization.
pdf.
Andrew G Barto, Charles W Anderson, and Richard S Sutton. Synthesis of nonlinear control sur-
faces by a layered associative search network. Biological Cybernetics, 43(3):175-185, 1982.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Mach. Learn., 79(1-2):151-
175, 2010. doi: 10.1007/s10994-009-5152-4. URL https://doi.org/10.1007/
s10994-009-5152-4.
Nikhil Bhat, Vivek Farias, and Ciamac C Moallemi. Non-parametric approximate dynam-
ic programming via the kernel method. In F. Pereira, C. J. C. Burges, L. Bottou, and
K. Q. Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp.
386-394. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
4547- non- parametric- approximate- dynamic- programming- via- the- kernel- method.
pdf.
Christopher M Bishop. Pattern Recognition and Machine Learning (Information Science and Statis-
tics). Springer-Verlag New York, Inc., 2006.
Justin A. Boyan, Justin A. Boyan, Justin A. Boyan, and Justin A. Boyan. Modular neural network-
s for learning context-dependent game strategies. Technical report, Masters thesis, Computer
Speech and Language Processing, 1992.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Erwin Coumans and Yunfei Bai. Pybullet, a python module for physics simulation for games,
robotics and machine learning. http://pybullet.org, 2016-2019.
Thomas Degris, Martha White, and Richard S. Sutton. Off-policy actor-critic, 2013.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th In-
ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 1587-1596, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL
http://proceedings.mlr.press/v80/fujimoto18a.html.
10
Under review as a conference paper at ICLR 2021
Matthieu Geist, Bruno Scherrer, and Olivier Pietquin. A theory of regularized markov decision
processes, 2019.
Vijaykumar Gullapalli. Reinforcement Learning and Its Application to Control. PhD thesis, USA,
1992.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
ICML2018, Stockholmsmassan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings
ofMachine Learning Research, pp.1856-1865. PMLR, 2018. URL http://Proceedings.
mlr.press/v80/haarnoja18b.html.
Hado V. Hasselt. Double q-learning. In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor,
R. S. Zemel, and A. Culotta (eds.), Advances in Neural Information Processing Systems 23,
pp. 2613-2621. Curran Associates, Inc., 2010. URL http://papers.nips.cc/paper/
3964- double- q- learning.pdf.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv: Learning,
2014.
George Dimitri Konidaris, Sarah Osentoski, and Philip S. Thomas. Value function approximation in
reinforcement learning using the fourier basis. In Wolfram Burgard and Dan Roth (eds.), Proceed-
ings of the Twenty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco,
California, USA, August 7-11, 2011. AAAI Press, 2011. URL http://www.aaai.org/
ocs/index.php/AAAI/AAAI11/paper/view/3569.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR
2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings, 2016. URL http:
//arxiv.org/abs/1509.02971.
Long-Ji Lin. Reinforcement Learning for Robots Using Neural Networks. PhD thesis, USA, 1992.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wier-
stra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR, ab-
s/1312.5602, 2013. URL http://arxiv.org/abs/1312.5602.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7):1180 - 1190, 2008.
ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2007.11.026. URL http://www.
sciencedirect.com/science/article/pii/S0925231208000532. Progress in
Modeling, Theory, and Application of Computational Intelligenc.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv.org/abs/
1707.06347.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction (Second Edition).
Bradford Books, 2018. ISBN 0262039249. URL https://web.stanford.edu/class/
psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf.
Gerald Tesauro. Practical issues in temporal difference learning. Mach. Learn., 8:257-277, 1992.
doi: 10.1007/BF00992697. URL https://doi.org/10.1007/BF00992697.
Philip Thomas. Bias in natural actor-critic algorithms. In Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, volume 32
of JMLR Workshop and Conference Proceedings, pp. 441-448. JMLR.org, 2014. URL http:
//proceedings.mlr.press/v32/thomas14.html.
Sebastian Thrun and Anton Schwartz. Issues in using function approximation for reinforcement
learning. In In Proceedings of the Fourth Connectionist Models Summer School. Erlbaum, 1993.
11
Under review as a conference paper at ICLR 2021
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, 2012.
Nino Vieillard, Tadashi Kozuno, Bruno Scherrer, Olivier Pietquin, Rmi Munos, and Matthieu Geist.
Leverage the average: an analysis of kl regularization in rl, 2020.
Xin Xu. A sparse kernel-based least-squares temporal difference algorithm for reinforcemen-
t learning. In Licheng Jiao, Lipo Wang, Xinbo Gao, Jing Liu, and Feng Wu (eds.), Advances
in Natural Computation, Second International Conference, ICNC 2006, Xi’an, China, Septem-
ber 24-28, 2006. Proceedings, Part I, volume 4221 of Lecture Notes in Computer Science, pp.
47-56. SPringer, 2006. doi: 10.1007∕11881070∖.8. URL https://doi.org/10.1007/
11881070_8.
12
Under review as a conference paper at ICLR 2021
APPENDICES
A Pseudo code of ECAC
Algorithm 1 Error controlled Actor-critic.
Require: initial policy Patameters, θ; Q function parameters, φι and φ2; discount rate, γ; the
coefficient of KL term, β ; empty replay buffer D; the number of episodes, M; the maximum
number of steps in each episode, T; minibatch size, N.
Ensure: optimal policy parameters θ*.
1: for episode = 1, M do
2: Reset environment.
3: for t = 1, T do
4:	Obeserve state S and select action a 〜∏(∙∣s).
5:	Execute a in the enviroment.
6:	Observe next state s0 and reward r.
7:	Store transition (s, a, r, s0) in replay buffer D.
8:	Randomly sample a minibatch ofN transitions, B = {(s, a, r, s0)} from D.
9:	Compute targets for the Q functions:
y(r, s') = r + Y min Qφ"s', e0), e0 ~ ∏θ(∙∣s')∙
i=1,2
10:	Update Q functions by one step of gradient descent using
vφNn X	(Qφi(S, a)- y(r, SO))2, i =* 1, 112.
(s,a,r,s0)∈B
11:	Backup old policy, θoid J θ.
12:	Update α and β by one step of gradient ascent using
Va N X [log α ∙ ((δKL + δentropy ) - H (πθ ("s), πθoid (IS)))],
s∈B
vβ N X[log β ∙ (H (πθ (∙ls)) - δentropy )],
s∈B
where δKL and δentropy denote target KL divergence and target entropy, respectively.
13:	Update policy by one step of gradient ascent using
VθN X(Qφι (S, aθ(S)) + αH(∏θ(∙∣S))- βDκL(∏θ(忖/矶(∙∣s))),
s∈B
where aθ(s) is a sample from ∏θ(∙∣s), which is differentiable with respect to θ via the
reparameterization trick.
14:	end for
15: end for
13
Under review as a conference paper at ICLR 2021
Table 2: ECAC Hyperparameters
Parameter	Value
learning rate
discount(γ)
replay buffer size
batch size
target KL
target entropy
target smoothing coefficient (τ)
number of hidden layers (all networks)
number of hidden units per layer
nonlinearity
10-3
0.99
5 ∙ 105
128
5∙10-3
-dim(A)/2.0 (e.g. , -3 for Walker2d-v3)
5∙10-3
2
256
ReLU
Table 3: Reward Scale Parameter
Environment	Reward Scale
Hopper-v3	5
Walker2d-v3	5
HalfCheetah-v3	5
Ant-v3	5
Humanoid-v3	20
HopperBulletEnv-v0	5
Walker2DBulletEnv-v0	5
HalfCheetahBulletEnv-v0	5
AntBulletEnv-v0	5
HumanoidBulletEnv-VO	20
B Implementation details and hyperparameters of ECAC
For the implementation of ECAC, a two layer feedforward neural network of 256 hidden units, with
rectified linear units (ReLU) between each layer are used to build the two Q functions an the policy.
The parameters of the neural networks and the two coefficients (i.e α and β) are optimized by using
Adam(Kingma & Ba, 2014). The hyperparameters of ECAC are listed in Table 2. Moreover, we
adopt the target network technique in ECAC, which is common in previous works (Lillicrap et al.,
2016; Fujimoto et al., 2018). We also adopt reward scale trick which is presented in Haarnoja et al.
(2018); and the reward scale parameter is listed in Table 3.
14