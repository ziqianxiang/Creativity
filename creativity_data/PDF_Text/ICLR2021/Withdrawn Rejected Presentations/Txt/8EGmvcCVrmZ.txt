Under review as a conference paper at ICLR 2021
Deep Learning is Singular, and That’s Good
Anonymous authors
Paper under double-blind review
Ab stract
In singular models, the optimal set of parameters forms an analytic set with singu-
larities and classical statistical inference cannot be applied to such models. This is
significant for deep learning as neural networks are singular and thus “dividing”
by the determinant of the Hessian or employing the Laplace approximation are
not appropriate. Despite its potential for addressing fundamental issues in deep
learning, singular learning theory appears to have made little inroads into the de-
veloping canon of deep learning theory. Via a mix of theory and experiment, we
present an invitation to singular learning theory as a vehicle for understanding
deep learning and suggest important future work to make singular learning theory
directly applicable to how deep learning is performed in practice.
1	Introduction
It has been understood for close to twenty years that neural networks are singular statistical models
(Amari et al., 2003; Watanabe, 2007). This means, in particular, that the set of network weights
equivalent to the true model under the Kullback-Leibler divergence forms a real analytic variety
which fails to be an analytic manifold due to the presence of singularities. It has been shown by
Sumio Watanabe that the geometry of these singularities controls quantities of interest in statistical
learning theory, e.g., the generalisation error. Singular learning theory (Watanabe, 2009) is the study
of singular models and requires very different tools from the study of regular statistical models.
The breadth of knowledge demanded by singular learning theory - Bayesian statistics, empirical
processes and algebraic geometry - is rewarded with profound and surprising results which reveal
that singular models are different from regular models in practically important ways. To illustrate
the relevance of singular learning theory to deep learning, each section of this paper illustrates a key
takeaway idea1.
The real log canonical threshold (RLCT) is the correct way to count the effective number of
parameters in a deep neural network (DNN) (Section 4). To every (model, truth, prior) triplet
is associated a birational invariant known as the real log canonical threshold. The RLCT can be
understood in simple cases as half the number of normal directions to the set of true parameters. We
will explain why this matters more than the curvature of those directions (as measured for example
by eigenvalues of the Hessian) laying bare some of the confusion over “flat” minima.
For singular models, the Bayes predictive distribution is superior to MAP and MLE (Section
5). In regular statistical models, the 1) Bayes predictive distribution, 2) maximum a posteriori (MAP)
estimator, and 3) maximum likelihood estimator (MLE) have asymptotically equivalent generalisa-
tion error (as measured by the Kullback-Leibler divergence). This is not so in singular models. We
illustrate in our experiments that even “being Bayesian” in just the final layers improves generali-
sation over MAP. Our experiments further confirm that the Laplace approximation of the predictive
distribution Smith & Le (2017); Zhang et al. (2018) is not only theoretically inappropriate but per-
forms poorly.
Simpler true distribution means lower RLCT (Section 6). In singular models the RLCT depends
on the (model, truth, prior) triplet whereas in regular models it depends only on the (model, prior)
pair. The RLCT increases as the complexity of the true distribution relative to the supposed model
increases. We verify this experimentally with a simple family of ReLU and SiLU networks.
1The code to reproduce all experiments in the paper will be released on Github. For now, see the zip file.
1
Under review as a conference paper at ICLR 2021
2	Related work
In classical learning theory, generalisation is explained by measures of capacity such as the l2 norm,
Radamacher complexity, and VC dimension (Bousquet et al., 2003). It has become clear however
that these measures cannot capture the empirical success of DNNs (Zhang et al., 2017). For instance,
over-parameterised neural networks can easily fit random labels (Zhang et al., 2017; Du et al., 2018;
Allen-Zhu et al., 2019b) indicating that complexity measures such as Rademacher complexity are
very large. There is also a slate of work on generalisation bounds in deep learning. Uniform con-
vergence bounds (Neyshabur et al., 2015; Bartlett et al., 2017; Neyshabur & Li, 2019; Arora et al.,
2018) usually cannot provide non-vacuous bounds. Data-dependent bounds (Brutzkus et al., 2018;
Li & Liang, 2018; Allen-Zhu et al., 2019a) consider the “classifiability” of the data distribution
in generalisation analysis of neural networks. Algorithm-dependent bounds (Daniely, 2017; Arora
et al., 2019; Yehudai & Shamir, 2019; Cao & Gu, 2019) consider the relation of Gaussian initial-
isation and the training dynamics of (stochastic) gradient descent to kernel methods (Jacot et al.,
2018).
In contrast to many of the aforementioned works, we are interested in estimating the conditional
distribution q(y∖χ). Specifically, We measure the generalisation error of some estimate qn(y∣x)
in terms of the KUllback-Leibler divergence between q and ^n, see (8). The next section gives a
crash course on singular learning theory. The rest of the paper illustrates the key ideas listed in the
introduction. Since we cover much ground in this short note, we will review other relevant work
along the way, in particular literature on “flatness”, the Laplace approximation in deep learning, etc.
3	Singular Learning Theory
To understand why classical measures of capacity fail to say anything meaningful about DNNs, it is
important to distinguish between two different types of statistical models. Recall we are interested in
estimating the true (and unknown) conditional distribution q(y∖x) with a class of models {p(y∖x, w) :
w ∈ W} where W ⊂ Rd is the parameter space. We say the model is identifiable if the mapping
w 7→ p(y∖x, w) is one-to-one. Let q(x) be the distribution of x. The Fisher information matrix
associated with the model {p(y∖x, w) : w ∈ W} is the matrix-valued function on W defined by
/ / ∂Wi [log p(y∖χ,w)]
I(w)ij
∂
∂W [log p(y∖x,w)]q(y∖x)q(x)dxdy,
if this integral is finite. Following the conventions in Watanabe (2009), we have the following
bifurcation of statistical models. A statistical modelp(y∖x, w) is called regular ifit is 1) identifiable
and 2) has positive-definite Fisher information matrix. A statistical model is called strictly singular
if it is not regular.
Let 夕(W) be a prior on the model parameters w. To every (model, truth, prior) triplet, we can
associate the zeta function, Z(z) = / K(w)z夕(W) dw, Z ∈ C, where K(w) is the Kullback-Leibler
(KL) divergence between the model p(y∖x, w) and the true distribution q(y∖x):
K(W)=〃q(y1x)log PlxxWy q(χ) dχdy.
(1)
For a (model, truth, prior) triplet (p(y∖x, w), q(y∖x),夕)，let -λ be the maximum pole of the corre-
sponding zeta function. We call λ the real log canonical threshold (RLCT) (Watanabe, 2009) of
the (model, truth, prior) triplet. The RLCT is the central quantity of singular learning theory.
By Watanabe (2009, Theorem 6.4) the RLCT is equal to d/2 in regular statistical models and
bounded above by d/2 in strictly singular models if realisability holds: let
W0 = {w ∈ W : p(y ∖x, w) = q (y ∖x)}
be the set of true parameters, we say q(y∖x) is realisable by the model class if W0 is non-empty.
The condition of realisability is critical to standard results in singular learning theory. Modifications
to the theory are needed in the case that q(y∖x) is not realisable, see the condition called relatively
finite variance in Watanabe (2018).
Neural networks in singular learning theory. Let W ⊆ Rd be the space of weights of a neural
network of some fixed architecture, and let f(x, w) : RN × W -→ RM be the associated function.
2
Under review as a conference paper at ICLR 2021
We shall focus on the regression task and study the model
p(y|x, w) = (2π1M∕2 exp (- 2 ky- f (X,W) k2)	⑵
but singular learning theory can also apply to classification, for instance. It is routine to check (see
Appendix A.1) that for feedforward ReLU networks not only is the model strictly singular but the
matrix I(w) is degenerate for all nontrivial weight vectors and the Hessian of K(w) is degenerate
at every point of W0.
RLCT plays an important role in model selection. One of the most accessible results in singular
learning theory is the work related to the widely-applicable Bayesian information criterion (WBIC)
Watanabe (2013), which we briefly review here for completeness. Let Dn = {(xi, yi)}in=1 be a
dataset of input-output pairs. Let Ln (w) be the negative log likelihood
1n
Ln(W) = -— Elog p(yi∣Xi,w)	(3)
i=1
and p(Dn |W) = exp(-—Ln(W)). The marginal likelihood of a model {p(y|x, W) : W ∈ W} is
given by P(Dn) = JW P(Dn |w)夕(W) dw and can be loosely interpreted as the evidence for the
model. Between two models, we should prefer the one with higher model evidence. However, since
the marginal likelihood is an intractable integral over the parameter space of the model, one needs
to consider some approximation.
The well-known Bayesian Information Criterion (BIC) derives from an asymptotic approximation
of - logP(Dn) using the Laplace approximation, leading to BIC = —Ln(WMLE) + d log —. Since
we want the marginal likelihood of the data for some given model to be high one should almost
never adopt a DNN according to the BIC, since in such models d may be very large. However, this
argument contains a serious mathematical error: the Laplace approximation used to derive BIC only
applies to regular statistical models, and DNNs are not regular. The correct criterion for both regular
and strictly singular models was shown in Watanabe (2013) to be —Ln (w0)+λ log — where wo ∈ Wo
and λ is the RLCT. Since DNNs are highly singular λ may be much smaller than d/2 (Section 6) it
is possible for DNNS to have high marginal likelihood - consistent with their empirical success.
4 Volume dimension, effective degrees of freedom, and flatness
Volume codimension. The easiest way to understand the RLCT is as a volume codimension (Watan-
abe, 2009, Theorem 7.1). Suppose that W ⊆ Rd and Wo is nonempty, i.e., the true distribution is
realisable. We consider a special case in which the KL divergence in a neighborhood of every point
vo ∈ Wo has an expression in local coordinates of the form
d0
K(W) = XciWi2,	(4)
i=1
where the coefficients c1 , . . . , cd0 > 0 may depend on vo and d0 may be strictly less than d. If the
model is regular then this is true with d = d0 and if it holds for d0 < d then we say that the pair
(P(y|x, W), q(y|x)) is minimally singular. It follows that the set Wo ⊆ W of true parameters is a
regular submanifold of codimension d0 (that is, Wo is a manifold of dimension d - d0 where W has
dimension d). Under this hypothesis there are, near each true parameter vo ∈ Wo, exactly d - d0
directions in which vo can be varied without changing the model P(y|x, W) and d0 directions in
which varying the parameters does change the model. In this sense, there are d0 effective parameters
near vo.
This number of effective parameters can be computed by an integral. Consider the volume of the
set of almost true parameters V(t, vo) = JK(W)<t 2(w)dw where the integral is restricted to a small
closed ball around vo. As long as the prior 夕(W) is non-zero on Wo it does not affect the relevant
features of the volume, so We may assume 夕 is constant on the region of integration in the first d0
directions and normal in the remaining directions, so up to a constant depending only on d0 we have
V(t,vo) H
√C1 …Cd
(5)
3
Under review as a conference paper at ICLR 2021
and we can extract the exponent of t in this volume in the limit
,0 ɔ]. log{V(at,v0)/V(t,v0)}
d = 2 lim------------------------
t→0	log(a)
for any a > 0, a 6= 1. We refer to the right hand side of (6) as the volume codimension at v0.
(6)
The function K(w) has the special form (4) locally with d0 = d if the statistical model is regular
(and realisable) and with d0 < d in some singular models such as reduced rank regression (Appendix
A.2). While such a local form does not exist for a singular model generally (in particular for neural
networks) nonetheless under natural conditions (Watanabe, 2009, Theorem 7.1) we have V(t, v0) =
ctλ + o(tλ) where c is a constant. We assume that in a sufficiently small neighborhood of v0 the
point RLCT λ at v0 (Watanabe, 2009, Definition 2.7) is less than or equal to the RLCT at every
point in the neighborhood so that the multiplicity m = 1, see Section 7.6 of (Watanabe, 2009) for
relevant discussion. It follows that the limit on the right hand side of (6) exists and is equal to λ. In
particular λ = d0/2 in the minimally singular case.
Note that for strictly singular models such as DNNs 2λ may not be an integer. This may be discon-
certing but the connection between the RLCT, generalisation error and volume dimension strongly
suggests that 2λ is nonetheless the only geometrically meaningful “count” of the effective number
of parameters near v0 .
RLCT and likelihood vs temperature. Again working with the model in (2), consider the expec-
tation over the posterior at temperature T as defined in (17) of the negative log likelihood (3)
n	nM
E(T)= EW/T [nLn(w)] = Ew/T[2 X kyi - f 3,w)k[ + -2- log(2∏).
i=1
Note that when - is large Ln(VO) ≈ M log(2∏) for any vo ∈ Wo so for T ≈ 0 the posterior
concentrates around the set Wo of true parameters and E(T) ≈ 喏 log(2∏). Consider the increase
∆E = E(T + ∆T) - E(T) corresponding to an increase in temperature ∆T. It can be shown that
∆E ≈ λ∆T where the reader should see (Watanabe, 2013, Corollary 3) for a precise statement.
As the temperature increases, samples taken from the tempered posterior are more distant from Wo
and the error E will increase. If λ is smaller then for a given increase in temperature the quantity E
increases less: this is one way to understand intuitively why a model with smaller RLCT generalises
better from the dataset Dn to the true distribution.
Flatness. It is folklore in the deep learning community that flatness of minima is related to gen-
eralisation (Hinton & Van Camp, 1993; Hochreiter & Schmidhuber, 1997) and this claim has been
revisited in recent years (Chaudhari et al., 2017; Smith & Le, 2017; Jastrzebski et al., 2017; Zhang
et al., 2018). In regular models this can be justified using the lower order terms of the asymptotic
expansion of the Bayes free energy (Balasubramanian, 1997, §3.1) but the argument breaks down
in strictly singular models, since for example the Laplace approximation of Zhang et al. (2018) is
invalid. The point can be understood via an analysis of the version of the idea in (Hochreiter &
Schmidhuber, 1997). Their measure of entropy compares the volume of the set of parameters with
tolerable error to (our almost true parameters) to a standard volume
-log hV(t0/：0)i = d-2d- log(to) + 2 X log Ci.	(7)
to	i=1
Hence in the case d = d0 the quantity - 2 Pi log©) is a measure of the entropy of the set of true
parameters near wo, a point made for example in Zhang et al. (2018). However when d0 < d this
conception of entropy is inappropriate because of the d - d0 directions in which K(w) is flat near
vo, which introduce the to dependence in (7).
5 generalisation
The generalisation puzzle (Poggio et al., 2018) is one of the central mysteries of deep learning.
Theoretical investigations into the matter is an active area of research Neyshabur et al. (2017). Many
of the recent proposals of capacity measures for neural networks are based on the eigenspectrum of
the (degenerate) Hessian, e.g., Thomas et al. (2019); Maddox et al. (2020). But this is not appropriate
for singular models, and hence for DNNs.
4
Under review as a conference paper at ICLR 2021
Since we are interested in learning the distribution, our notion of generalisation is slightly different,
being measured by the KL divergence. Precise statements regarding the generalisation behavior in
singular models can be made using singular learning theory. Let the network weights be denoted
θ rather than w for reasons that will become clear. Recall in the Bayesian paradigm, prediction
proceeds via the so-called Bayes predictive distribution, p(y|x, Dn) = / p(y|x, θ)p(θ∣Dn) dθ. More
commonly encountered in deep learning practice are the MAP and MLE point estimators. While in
a regular statistical model, the three estimators 1) Bayes predictive distribution, 2) MAP, and 3)
MLE have the same leading term in their asymptotic generalisation behavior, the same is not true
in singular models. More precisely, let qn(y∣x) be some estimate of the true unknown conditional
density q(y|x) based on the dataset Dn The generalisation error of the predictor qn(y∣x) is
G(n) := KL(q(y|x)||qn(y|x))
q(y|x)
J J q(y∣χ) log q(y|x)q(χ) dydx.
(8)
To account for sampling variability, we will work with the average generalisation error, EnG(n),
where En denotes expectation over the dataset Dn. By Watanabe (2009, Theorem 1.2 and Theorem
7.2), we have
EnG(n) = λ∕n + o(1∕n) if q^n is the Bayes predictive distribution,	(9)
where λ is the RLCT corresponding to the triplet (p(y∣x, θ), q(y∣x)"(θ)). In contrast, We should
note that Zhang et al. (2018) and Smith & Le (2017) rely on the Laplace approximation to explain
the generalisation of the Bayes predictive distribution though both works acknowledge the Laplace
approximate is inappropriate. For completeness, a quick sketch of the derivation of (9) is provided
in Appendix A.4. Now by (Watanabe, 2009, Theorem 6.4) we have
EnG(n) = C/n + o(1∕n) if qn is the MAP or MLE,	(10)
where C (different for MAP and MLE) is the maximum of some Gaussian process. For regular mod-
els, the MAP, MLE, and the Bayes predictive distribution have the same leading term for EnG(n)
since λ = C = d/2. However in singular models, C is generally greater than λ, meaning we should
prefer the Bayes predictive distribution for singular models.
That the RLCT has such a simple relationship to the Bayesian generalisation error is remarkable.
On the other hand, the practical implications of (19) are limited since the Bayes predictive distribu-
tion is intractable. While approximations to the Bayesian predictive distribution, say via variational
inference, might inherit a similar relationship between generalisation and the (variational) RLCT,
serious theoretical developments will be required to rigorously establish this. The challenge comes
from the fact that for approximate Bayesian predictive distributions, the free energy and generali-
sation error may have different learning coefficients λ. This was well documented in the case of a
neural network with one hidden layer (Nakajima & Watanabe, 2007).
We set out to investigate whether certain very simple approximations of the Bayes predictive distri-
bution can already demonstrate superiority over point estimators. Suppose the input-target relation-
ship is modeled as in (2) but we write θ instead of w. We set q(x) = N(0, I3). For now consider
the realisable case, q(y|x) = p(y|x, θ0) where θ0 is drawn randomly according to the default initial-
isation in PyTorch when model (2) is instantiated. We calculate EnG(n) using multiple datasets Dn
and a large testing set, see Appendix A.5 for more details.
Since f is a hierarchical model, let's write it as fθ(∙) = h(g(∙; v); W) with the dimension of W
being relatively small. Let θMAP = (vMAP, wMAP) be the MAP estimate for θ using batch gradient
descent. The idea of our simple approximate Bayesian scheme is to freeze the network weights at
the MAP estimate for early layers and perform approximate Bayesian inference for the final layers2.
e.g., freeze the parameters of g at vMAP and perform MCMC over W. Throughout the experiments,
g : R3 → R3 is a feedforward ReLU block with each hidden layer having 5 hidden units and
h : R3 → R3 is either BAx or B ReLU(Ax) where A ∈ R3×r, B ∈ Rr×3. We set r = 3. We shall
consider 1 or 5 hidden layers for g.
To approximate the Bayes predictive distribution, we perform either the Laplace approximation
or the NUTS variant of HMC (Hoffman & Gelman, 2014) in the last two layers, i.e., performing
inference over A, B in h(g(∙; vmap); A, B). Note that MCMC is operating in a space of 18 di-
2This is similar in spirit to Kristiadi et al. (2020) who claim that even “being Bayesian a little bit” fixes
overconfidence. They approach this via the Laplace approximation for the final layer of a ReLU network. It is
also worth noting that Kristiadi et al. (2020) do not attempt to formalise what it means to ”fix overconfidence”;
the precise statement should be in terms of G(n).
5
Under review as a conference paper at ICLR 2021
Figure 1:	Realisable and full batch gradient descent for MAP. Average generalisation errors EnG(n)
are displayed for various approximations of the Bayes predictive distribution. The results of the
Laplace approximations are reported in the Appendix and not displayed here because they are higher
than other approximation schemes by at least an order of magnitude. Each subplot shows a different
combination of hidden layers in g (1 or 5) and activation function in h (ReLU or identity). Note that
the y-axis is not shared.
Table 1: Companion to Figure 1. The learning coefficient is the slope of the linear fit 1/n versus
EnG(n) (no intercept since realisable). The R2 value gives a sense of the goodness-of-fit.
(a) 1 hidden layer(s) in g, identity activation in h			(b) 5 hidden layer(s) in g, identity activation in h		
method	learning coefficient	R squared	method	learning coefficient	R squared
last two layers (A,B) MCMC	9.709027	0.966124	last two layers (A,B) MCMC	1.286290	0.985161
last layer only (B) MCMC	6.410380	0.988921	last layer only (B) MCMC	1.298504	0.982298
last two layers (A,B) Laplace	inf	NaN	last two layers (A,B) Laplace	inf	NaN
last layer only (B) Laplace	2154.989266	0.801077	last layer only (B) Laplace	2038.605589	0.803736
MAP	10.714216	0.951051	MAP	1.437473	0.983411
(c) 1 hidden layer(s) in g, ReLU activation in h			(d) 5 hidden layer(s) in g, ReLU activation in h		
method	learning coefficient	R squared	method	learning coefficient	R squared
last two layers (A,B) MCMC	3.117187	0.977313	last two layers (A,B) MCMC	0.835593	0.957824
last layer only (B) MCMC	3.152710	0.980132	last layer only (B) MCMC	1.466273	0.920716
last two layers (A,B) Laplace	inf	NaN	last two layers (A,B) Laplace	inf	NaN
last layer only (B) Laplace	1120.648298	0.742412	last layer only (B) Laplace	1416.294288	0.808991
MAP	5.343311	0.972212	MAP	1.981483	0.889519
mensions in this case, which is small enough for us to expect MCMC to perform well. We also
implemented the Laplace approximation and NUTS in the last layer only, i.e. performing infer-
ence over B in h2(h1(g(∙; vmap); Amap); B). Further implementation details of these approximate
Bayesian schemes are found in Appendix A.5.
From the outset, we expect the Laplace approximation over w = (A, B) to be invalid since the
model is singular. We do however expect the last-layer-only Laplace approximation over B to be
sound. Next, we expect the MCMC approximation in either the last layer or last two layers to
be superior to the Laplace approximations and to the MAP. We further expect the last-two-layers
MCMC to have better generalisation than the last-layer-only MCMC since the former is closer to
the Bayes predictive distribution. In summary, we anticipate the following performance order for
6
Under review as a conference paper at ICLR 2021
these five approximate Bayesian schemes (from worst to best): last-two-layers Laplace, last-layer-
only Laplace, MAP, last-layer-only MCMC, last-two-layers MCMC.
The results displayed in Figure 1 are in line with our stated expectations above, except for the sur-
prise that the last-layer-only MCMC approximation is often superior to the last-two-layers MCMC
approximation. This may arise from the fact that MCMC finds the singular setting in the last-two-
layers more challenging. In Figure 1, we clarify the effect of the network architecture by varying
the following factors: 1) either 1 or 5 layers in g, and 2) ReLU or identity activation in h. Table
1 is a companion to Figure 1 and tabulates for each approximation scheme the slope of 1/n ver-
sus EnG(n), also known as the learning coefficient. The R2 corresponding to the linear fit is also
provided. In Appendix A.5, we also show the corresponding results when 1) the data-generating
mechanism and the assumed model do not satisfy the condition of realisability and/or 2) the MAP
estimate is obtained via minibatch stochastic gradient descent instead of batch gradient descent.
6 Simple functions and complex singularities
In singular models the RLCT may vary with the true distribution (in contrast to regular models) and
in this section we examine this phenomenon in a simple example. As the true distribution becomes
more complicated relative to the supposed model, the singularities of the analytic variety of true
parameters should become simpler and hence the RLCT should increase (Watanabe, 2009, §7.6).
Our experiments are inspired by (Watanabe, 2009, §7.2) where tanh(x) networks are considered
and the true distribution (associated to the zero network) is held fixed while the number of hidden
nodes is increased.
Consider the model p(y|x, w) in (2) where f(x, w) = c+PiH=1 qi ReLU(hwi, xi +bi) is a two-layer
ReLU network with weight vector w = ({wi}iH=1, {bi}iH=1, {qi}iH=1, c) ∈ R4H+1 and wi ∈ R2, bi ∈
R, qi ∈ R for 1 ≤ i ≤ H . We let W be some compact neighborhood of the origin.
Figure 2:	Increasingly complicated true distributions qm (x, y) on [-1, 1]2 × R.
Table 2: RLCT estimates for ReLU and SiLU networks. We observe the RLCT increasing as m increases, i.e.,
the true distribution becomes more “complicated” relative to the supposed model.
m	Nonlinearity	RLCT	Std	R squared
3	ReLU	0.526301	0.027181	0.983850
3	SiLU	0.522393	0.026342	0.978770
4	ReLU	0.539590	0.024774	0.991241
4	SiLU	0.539387	0.020769	0.988495
5	ReLU	0.555303	0.002344	0.993092
5	SiLU	0.555630	0.021184	0.990971
Given an integer 3 ≤ m ≤ H we define a network sm ∈ W and qm(y|x) := p(y|x, sm) as follows.
Let g ∈ SO(2) stand for rotation by 2π∕m, set w1 = √g (1, 0)T. The components of Sm are the
vectors Wi = gi-1 w1 for 1 ≤ i ≤ m and Wi = 0 for i > m, bi = - 3 and qi = 1 for 1 ≤ i ≤ m
and bi = qi = 0 for i > m, and finally C = 0. The factor of ɪ ensures the relevant parts of the
decision boundaries lie within X = [-1, 1]2. We let q(x) be the uniform distribution on X and
define qm(x, y) = qm(y|x)q(x). The functions f(x, sm) are graphed in Figure 2. It is intuitively
clear that the complexity of these true distributions increases with m.
We let φ be a normal distribution N(0, 502) and estimate the RLCTS of the triples (p, qm, φ). We
conducted the experiments with H = 5, n = 1000. For each m ∈ {3, 4, 5}, Table 2 shows the
7
Under review as a conference paper at ICLR 2021
estimated RLCT. Algorithm 1 in Appendix A.3 details the estimation procedure which we base on
(Watanabe, 2013, Theorem 4). As predicted the RLCT increases with m verifying that in this case,
the simpler true distributions give rise to more complex singularities.
Note that the dimension of W is d = 21 and so if the model were regular the RLCT would be 10.5.
It can be shown that when m = H the set of true parameters W0 ⊆ W is a regular submanifold of
dimension m. If such a model were minimally singular its RLCT would be 2((4m + 1) - m)=
1 (3m +1). In the case m = 5 We observe an RLCT more than an order of magnitude less than the
value 8 predicted by this formula. So the function K does not behave like a quadratic form near W0.
Strictly speaking it is incorrect to speak of the RLCT of a ReLU network because the function
K(w) is not necessarily analytic (Example A.4). However we observe empirically that the predicted
linear relationship between Ew [nLn(w)] and 1∕β holds in our small ReLU networks (see the R2
values in Table 2) and that the RLCT estimates are close to those for the two-layer SiLU network
(Hendrycks & Gimpel, 2016) which is analytic (the SiLU or sigmoid weighted linear unit is σ(x) =
x(1 + e-τx)-1 which approaches the ReLU as τ → ∞. We use τ = 100.0 in our experiments).
The competitive performance of SiLU on standard benchmarks (Ramachandran et al., 2017) shows
that the non-analyticity of ReLU is probably not fundamental.
7 Future directions
Deep neural networks are singular models, and that’s good: the presence of singularities is neces-
sary for neural networks with large numbers of parameters to have low generalisation error. Sin-
gular learning theory clarifies how classical tools such as the Laplace approximation are not just
inappropriate in deep learning on narrow technical grounds: the failure of this approximation and
the existence of interesting phenomena like the generalisation puzzle have a common cause, namely
the existence of degenerate critical points of the KL function K(w). Singular learning theory is a
promising foundation for a mathematical theory of deep learning. However, much remains to be
done. The important open problems include:
SGD Vs the posterior. A number of works (SimSekli, 2017; Mandt et al., 2017; Smith et al., 2018)
suggest that mini-batch SGD may be governed by SDEs that have the posterior distribution as its
stationary distribution and this may go towards understanding why SGD works so well for DNNs.
RLCT estimation for large networks. Theoretical RLCTs have been cataloged for small neural
networks, albeit at significant effort3 (Aoyagi & Watanabe, 2005b;a). We believe RLCT estimation
in these small networks should be standard benchmarks for any method that purports to approxi-
mate the Bayesian posterior of a neural network. No theoretical RLCTs or estimation procedure
are known for modern DNNs. Although MCMC provides the gold standard it does not scale to
large networks. The intractability of RLCT estimation for DNNs is not necessarily an obstacle to
reaping the insights offered by singular learning theory. For instance, used in the context of model
selection, the exact value of the RLCT is not as important as model selection consistency. We also
demonstrated the utility of singular learning results such as (9) and (10) which can be exploited even
without knowledge of the exact value of the RLCT.
Real-world distributions are unrealisable. The existence of power laws in neural language model
training (Hestness et al., 2017; Kaplan et al., 2020) is one of the most remarkable experimental
results in deep learning. These power laws may be a sign of interesting new phenomena in singular
learning theory when the true distribution is unrealisable.
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameter-
ized neural networks, going beyond two layers. In Advances in Neural Information Processing
Systems, pp. 6155-6166, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252, 2019b.
3Hironaka’s resolution of singularities guarantees existence. However it is difficult to do the required
blowup transformations in high dimensions to obtain the standard form.
8
Under review as a conference paper at ICLR 2021
Shun-ichi Amari, Tomoko Ozeki, and Hyeyoung Park. Learning and inference in hierarchical mod-
els with singularities. Systems and Computers in Japan, 34(7):34-42, 2003.
Miki Aoyagi and Sumio Watanabe. Resolution of Singularities and the Generalization Error with
Bayesian Estimation for Layered Neural Network. In IEICE Trans., pp. 2112-2124, 2005a.
Miki Aoyagi and Sumio Watanabe. Stochastic complexities of reduced rank regression in Bayesian
estimation. Neural Networks, 18(7):924-933, 2005b.
Sanjeev Arora, R Ge, B Neyshabur, and Y Zhang. Stronger generalization bounds for deep nets via
a compression approach. In 35th International Conference on Machine Learning, ICML 2018,
2018.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of op-
timization and generalization for overparameterized two-layer neural networks. In International
Conference on Machine Learning, pp. 322-332, 2019.
Vijay Balasubramanian. Statistical inference, Occam’s razor and statistical mechanics on the space
of probability distributions. Neural Computation, 9(2):349-368, 1997.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
William M Boothby. An introduction to differentiable manifolds and Riemannian geometry. Aca-
demic press, 1986.
Olivier Bousquet, Stephane Boucheron, and Gabor Lugosi. Introduction to statistical learning the-
ory. In Summer School on Machine Learning, pp. 169-207. Springer, 2003.
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz. Sgd learns over-
parameterized networks that provably generalize on linearly separable data. In International
Conference on Learning Representations, 2018.
Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide and
deep neural networks. In Advances in Neural Information Processing Systems, pp. 10835-10845,
2019.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian
Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-SGD: Biasing gradi-
ent descent into wide valleys. In International Conference on Learning Representations, 2017.
Amit Daniely. SGD learns the conjugate kernel class of the network. In Advances in Neural Infor-
mation Processing Systems, pp. 2422-2430, 2017.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. In International Conference on Learning Representations,
2018.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv preprint
arXiv:1606.08415, 2016.
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory F. Diamos, Heewoo Jun, Hassan Kianine-
jad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. CoRR, abs/1712.00409, 2017.
Geoffrey E Hinton and Drew Van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Proceedings of the sixth annual conference on Computational
learning theory, pp. 5-13, 1993.
Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 9(1):142, 1997.
Matthew D Hoffman and Andrew Gelman. The No-U-Turn sampler: adaptively setting path lengths
in hamiltonian monte carlo. J. Mach. Learn. Res., 15(1):1593-1623, 2014.
9
Under review as a conference paper at ICLR 2021
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and gen-
eralization in neural networks. In Advances in Neural Information Processing Systems, pp. 8571-
8580, 2018.
Stanislaw Jastrzebski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua
Bengio, and Amos Storkey. Three factors influencing minima in SGD. arXiv preprint
arXiv:1711.04623, 2017.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXiv preprint arXiv:2001.08361, 2020.
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, even just a bit, fixes
overconfidence in ReLU networks. arXiv preprint arXiv:2002.10118, 2020.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson. Rethinking parameter counting in
deep models: Effective dimensionality revisited. arXiv preprint arXiv:2003.02139, 2020.
Stephan Mandt, Matthew D Hoffman, and David M Blei. Stochastic gradient descent as approximate
Bayesian inference. The Journal of Machine Learning Research, 18(1):4873-4907, 2017.
Shinichi Nakajima and Sumio Watanabe. Variational Bayes Solution of Linear Neural Networks
and Its Generalization Performance. Neural Computation, 19(4):1112-53, 2007.
Behnam Neyshabur and Zhiyuan Li. Towards understanding the role of over-parametrization in gen-
eralization of neural networks. In International Conference on Learning Representations (ICLR),
2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in neural information processing systems, pp. 5947-5956,
2017.
Jeffrey Pennington and Pratik Worah. The spectrum of the Fisher information matrix of a single-
hidden-layer neural network. In Advances in Neural Information Processing Systems, pp. 5410-
5419, 2018.
Mary Phuong and Christoph H. Lampert. Functional vs. parametric equivalence of ReLU networks.
In International Conference on Learning Representations, 2020.
Tomaso A. Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco, Xavier Boix,
Jack Hidary, and Hrushikesh Mhaskar. Theory of deep learning III: explaining the non-overfitting
puzzle. CoRR, abs/1801.00173, 2018.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Swish: a self-gated activation function. arXiv
preprint arXiv:1710.05941, 2017.
Levent Sagun, Leon Bottou, and Yann LeCun. Singularity of the Hessian in deep learning. CoRR,
abs/1611.07476, 2016.
UmUt SimSekli. Fractional Langevin Monte Carlo: exploring Levy driven stochastic differential
equations for Markov Chain Monte Carlo. In Proceedings of the 34th International Conference
on Machine Learning-Volume 70, pp. 3200-3209, 2017.
Samuel L Smith and Quoc V Le. A Bayesian perspective on generalization and stochastic gradient
descent. arXiv preprint arXiv:1710.06451, 2017.
10
Under review as a conference paper at ICLR 2021
Samuel L Smith, Daniel Duckworth, Semon Rezchikov, Quoc V Le, and Jascha Sohl-Dickstein.
Stochastic natural gradient descent draws posterior samples in function space. arXiv preprint
arXiv:1806.09597, 2018.
Valentin Thomas, Fabian Pedregosa, Bart van Merrinboer, Pierre-Antoine Mangazol, Yoshua Ben-
gio, and Nicolas Le Roux. Information matrices and generalization. arXiv:1906.07774 [cs, stat],
2019. arXiv: 1906.07774.
Sumio Watanabe. Almost All Learning Machines are Singular. In 2007 IEEE Symposium on Foun-
dations of Computational Intelligence, pp. 383-388, 2007.
Sumio Watanabe. Algebraic Geometry and Statistical Learning Theory. Cambridge University
Press, USA, 2009.
Sumio Watanabe. A Widely Applicable Bayesian Information Criterion. Journal of Machine Learn-
ing Research, 14:867-897, 2013.
Sumio Watanabe. Mathematical Theory of Bayesian Statistics. CRC Press, 2018.
Gilad Yehudai and Ohad Shamir. On the power and limitations of random features for understanding
neural networks. In Advances in Neural Information Processing Systems, pp. 6594-6604, 2019.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In Proceedings of the 5th International Confer-
ence on Learning Representations, 2017. arXiv: 1611.03530.
Yao Zhang, Andrew M. Saxe, Madhu S. Advani, and Alpha A. Lee. Energy-entropy competition
and the effectiveness of stochastic gradient descent in machine learning. Molecular Physics, 116
(21-22):3214-3223, 2018.
A Appendix
A. 1 Neural networks are strictly singular
Many-layered neural networks are strictly singular (Watanabe, 2009, §7.2). The degeneracy of the
Hessian in deep learning has certainly been acknowledged in e.g., Sagun et al. (2016) which recog-
nises the eigenspectrum is concentrated around zero and in Pennington & Worah (2018) which
deliberately studies the Fisher information matrix of a single-hidden-layer, rather than multilayer,
neural network.
We first explain how to think about a neural network in the context of singular learning theory. A
feedforward network of depth c parametrises a function f : RN -→ RM of the form
f = Ac ◦ σc-i ◦ Ac-i …σι ◦ Ai
where the Al : Rdl-1 -→ Rdl are affine functions and σl : Rdl -→ Rdl is coordinate-wise some
fixed nonlinearity σ : R -→ R. Let W be a compact subspace of Rd containing the origin, where
Rd is the space of sequences of affine functions (Al)lc=1 with coordinates denoted w1, . . . , wd so
that f may be viewed as a function f : RN × W -→ RM. We define p(y|x, w) as in (2). We
assume the true distribution is realisable, q(y|x) = p(y|x, w0) and that a distribution q(x) on RN
is fixed with respect to which p(x, y) = p(y|x)q(x) and q(x, y) = q(y|x)q(x). Given some prior
夕(W) on W We may apply singular learning theory to the triplet (p, q,夕).
By straightforward calculations we obtain
K(w)
1 /kf (x,w)
- f(x, w0)k2q(x)dx
∂w‰j K (W) = / D 悬 f (X,w), ∂Wj f (X,w)〉q(X)dx
+	(f(x,w) - f (X,wo),高叫 f (χ,w)〉q(X)dx
I(Wbj = 2(M-3)∕2Π(M-2)/2 / D 悬 f(X，w)，∂Wj f(X,w)Eq(X)dX
(11)
(12)
(13)
11
Under review as a conference paper at ICLR 2021
where h-, -i is the dot product. We assume q(x) is such that these integrals exist.
It will be convenient below to introduce another set of coordinates for W. Let wjlk denote the weight
from the kth neuron in the (l - 1)th layer to the jth neuron in the lth layer and let blj denote the bias
of the jth neuron in the lth layer. Here 1 ≤ l ≤ c and the input is layer zero. Let ulj and alj denote
the value of the jth neuron in the lth layer before and after activation, respectively. Let ul and al
denote the vectors with values ulj and alj , respectively. Let dl denote the number of neurons in the
lth layer. Then
dl-1
ulj = Xwjlkalk-1 +blj,	1 ≤ l ≤ c,1 ≤j ≤ dl
k=1
alj = σ(ulj )	1 ≤ l < c, 1 ≤ j ≤ dl
with the convention that a0 = x is the input and uc = y is the output.
In the case where σ = ReLU the partial derivatives ∂W~ f do not exist on all of RN. However
given w ∈ W we let D(w) denote the complement in RN of the union over all hidden nodes of the
associated decision boundary, that is
RN \ D(w) = [	[ {x ∈ RN : ulj(x) = 0}.
1≤l<c 1≤j ≤dl
The partial derivative ∂W f exists on the open subset {(x, w) : X ∈ D(w)} of RN X W.
Lemma A.1. Suppose σ = ReLU and there are c > 1 layers. For any hidden neuron 1 ≤ j ≤ dl
in layer l with 1 ≤ l < c there is a differential equation
dl-1	dl+1
{ X Wjk品 + bj嘉-X wi+1 Z f= = 0
1£1	dwjk	dbj ⅛ j dwi+1∕
which holds on D(w) for any fixed w ∈ W .
Proof. Without loss of generality assume M = 1, to simplify the notation. Let ei ∈ Rdl+1 denote a
unit vector and let H(x)= 今 ReLU(x). Writing dUf for a gradient vector
_^^_/Jl_ ∂ul+1 ∖	/ ∂f 1 ∖ ∂f l H l
∂wi+1 = ∖∂u1+1, ∂w1+1∕ = ∖∂u1+1 ,ajei∕ = ∂ui+1 UjH(Uj)
ij	ij	i
df __ D df du1+1 \ — / df X 1+1 1-Itr/ 1、E — X df 1+1 1-Itr/ 1、
∂Wξ = ∖∂U1+ι,ιwς/ = ∖∂u∏ι,Twij ak H(uj)ei) = TFWij	k H(Uj)
jk	jk	i=1	i=1 Ui
奈=/ ∂Uf+ι, ⅞τ\=/f, X wi+1H (Uj)eiE=X ∂f wi+1H (Uj).
j	j	i=1	i=1	i
The claim immediately follows.	□
Lemma A.2. Suppose σ = ReLU, c > 1 and that w ∈ W has at least one weight or bias at a
hidden node nonzero. Then the matrix I(w) is degenerate and if w ∈ W0 then the Hessian of K at
w is also degenerate.
Proof. Let w ∈ W be given, and choose a hidden node where at least one of the incident weights
(or bias) is nonzero. Then Lemma A.1 gives a nontrivial linear dependence relation Pi λi 系 f = 0
as functions on D(w). The rows of I(w) satisfy the same linear dependence relation. At a true
parameter the second summand in (12) vanishes so by the same argument the Hessian is degenerate.
□
Remark A.3. Lemma A.2 implies that every true parameter for a nontrivial ReLU network is a de-
generate critical point of K . Hence in the study of nontrivial ReLU networks it is never appropriate
to divide by the determinant of the Hessian of K at a true parameter, and in particular Laplace or
saddle-point approximations at a true parameter are invalid.
12
Under review as a conference paper at ICLR 2021
The well-known positive scale invariance of ReLU networks (Phuong & Lampert, 2020) is responsi-
ble for the linear dependence of Lemma A.1, in the precise sense that the given differential operator
is the infinitesimal generator (Boothby, 1986, §IV.3) of the scaling symmetry. However, this is only
one source of degeneracy or singularity in ReLU networks. The degeneracy, as measured by the
RLCT, is much lower than one would expect on the basis of this symmetry alone (see Section 6).
Example A.4. In general the KL function K(w) for ReLU networks is not analytic. For the minimal
counterexample, let q(x) be uniform on [-N, N] and zero outside and consider
K(b) =
q(x)(ReLU(x - b) - ReLU(x))2dx .
It is easy to check that up to a scalar factor
K(b)
J-1 b3 + b2N
t-1 b3 + b2N
0≤b≤N
-N ≤ b ≤ 0
so that K is C2 but not C3 let alone analytic.
A.2 Reduced rank regression
For reduced rank regression, the model is
p(y|x，W)= (2πσ1)N∕2exp (-2⅛ |y - BAx|2),
where x ∈ RM, y ∈ RN, A an M × H matrix and B an H × N matrix; the parameter w denotes the
entries of A and B, i.e. w = (A, B), and σ> 0 is a parameter which for the moment is irrelevant.
If the true distribution is realisable then there is w0 = (A0, B0) such that q(y|x) = p(y|x, w0).
Without loss of generality assume q(x) is the uniform density. In this case the KL divergence from
p(y |x, w) to q (y |x) is
K(W) = q q(y∣x) log q(y⑶、dxdy = kBA - B0A0∣∣2 (1 + E(W))
p(y|x, w)
where the error E is smooth and E(W) = O(kBA - B0A0k2) in any region where kBA-B0A0k <
C, so K(W) is equivalent to kBA - B0A0k2. We write K(W) = kBA - B0A0k2 for simplicity
below.
Now assume that B0A0 is symmetric and that B0 is square, i.e. N = H. Then the zero locus of
K(W) is explicitly given as follows
W0 = {(A,B) : detB 6= 0andA = B-1B0A0}.
It follows that W0 is globally a graph over GL(H; R). Indeed, the set (B-1B0A0, B) with
B ∈ GL(H; R) is exactly W0. Thus W0 is a smooth H 2 -dimensional submanifold of RH2 ×
RH ×M . To prove that W0 is minimally singular in the sense of Section 4 it suffices to show that
rank(DA2 ,BK) ≥ HM where DA2 ,BK denotes the Hessian, but as it is no more difficult to do so, we
find explicit local coordinates (u, V) near an arbitrary point (A, B) ∈ Wo for which {v = 0} = Wo
and K(u, v) = a(u, v)|u|2 in this neighborhood, where a is a C∞ function with a ≥ c > 0 for
some c. Write
A(V) = (B + v)-1B0A0.
Then u, v → (A(V) + u, B + v) gives local coordinates on RH2 X RH×M near (A, B), and
K(u,v) = |(B + V)((B + V)TBOAO + u) 一 BoAo∣
=∣BoAo + (B + V)U - BoAo∣2
= I(B + V)u|2,
so for V sufficiently small (and hence B + V invertible) we can take a(u, V) = ∣(B + v)u∣2∕∣u∣2.
A.3 RLCT estimation
In this section we detail the estimation procedure for the RLCT used in Section 6. Let Ln (W) be
the negative log likelihood as in (3). Define the data likelihood at inverse temperature β > 0 to be
13
Under review as a conference paper at ICLR 2021
Algorithm 1 RLCT via Theorem 4 in Watanabe (2013)
Input: range of β's, set of training sets T each of size n, approximate samples {wι,..., wr}
from pβ (w|Dn) for each training set Dn and each β
for training set Dn ∈ T do
for β in range of β,s do
Approximate Ew [nLn(w)] with R PR=I nLn(wr) where wi,...,wr are approximate sam-
ples from pβ (w|Dn)
end for
Perform generalised least squares to fit λ in (18), call result λ(Dn)
end for
OUtPUt：才 PDn∈τ λDn)
pβ(Dn |w) = ∏n=ιp(yi∣xi, w)β which can also be written
Pβ (DnIw) = exp(-βnLn(w)).	(14)
The posterior distribution, at inverse temperature β, is defined as
β/ ID、_	πn=lP(yilxi,w)β ψ(w) - Pe (DnIwW(W)	Z151
P(WIDn)= RW ∏n=ιp(yi ∣Xi,w)β 小 w) =	Pβ (Dn)	(15)
where 夕 is the prior distribution on the network weights w and
Pe (Dn) = P Pe (DnIwW(w) dw	(16)
W
is the marginal likelihood of the data at inverse temperature β . Finally, denote the expectation of a
random variable R(w) with respect to the tempered posterior Pe (w IDn) as
Eew[R(w)]
R(w)Pe (wIDn) dw
(17)
In the main text, we drop the superscript in the quantities (14), (15), (16), (17) when β = 1, e.g.,
P(Dn) rather thanP1(Dn).
Assuming the conditions of Theorem 4 in Watanabe (2013) hold, we have
Ew [nLn(w)] = nLn(wθ) + 耳 + Un y	+ Op(I)
where β0 is a positive constant and Un is a sequence of random variables satisfying En Un
(18)
0. In
Algorithm 1, we describe an estimation procedure for the RLCT based on the asymptotic result in
(18).
For the estimates in Table 2 the a posteriori distribution was approximated using the NUTS variant
of Hamiltonian Monte Carlo (Hoffman & Gelman, 2014) where the first 1000 steps were omitted
and 20, 000 samples were collected. Each λ(Dn) estimate in Algorithm 1 was performed by lin-
ear regression on the pairs {(1∕βi, Eei [nLn(w)])}5=ι where the five inverse temperatures βi are
centered on the inverse temperature 1/ log(20000).
A.4 Connection between RLCT and generalisation
For completeness, we sketch the derivation of (9) which gives the asymptotic expansion of the
average generalisation error EnG(n) of the Bayes prediction distribution in singular models. The
exposition is an amalgamation of various works published by Sumio Watanabe, but is mostly based
on the textbook (Watanabe, 2009).
To understand the connection between the RLCT and G(n), we first define the so-called Bayes free
energy as
F(n) = - logP(Dn)
whose expectation admits the following asymptotic expansion (Watanabe, 2009):
EnF (n) = EnnSn + λlog n + o(log n)
14
Under review as a conference paper at ICLR 2021
where Sn = -1 Pn=IlOg q(y∕χi) is the entropy. The expected Bayesian generalisation error is
related to the Bayes free energy as follows
EnG(n) = EF(n + 1) - EF (n)
Then for the average generalisation error, we have
EnG(n) = λ/n + o(1∕n).	(19)
Since models with more complex singularities have smaller RLCTs, this would suggest that the
more singular a model is, the better its generalisation (assuming one uses the Bayesian predictive
distribution for prediction). In this connection it is interesting to note that simpler (relative to the
model) true distributions lead to more singular models (Section 6).
A.5 Details for generalisation error experiments
Simulated data The distribution of x ∈ R3 is set to q(x) = N(0, I3). In the realisable case, y ∈ R3
is drawn according to q(y∣x) = p(y ∣x,θo). Inthe nonrealisable setting, we set q(y∣x) 8 exp{-∣∣y -
hw0 (x)||2/2}, where w0 = (A0, B0) is drawn according to the PyTorch model initialisation of h.
MAP training The MAP estimator is found via gradient descent using the mean-squared-error loss
with either the full data set or minibatch set to 32. Training was set to 5000 epochs. No form of
early stopping was employed.
Calculating the generalisation error Using a held-out-test set Tn0 = {(x0i, yi0)}in=01, we calculate
the average generalisation error as
00
1n	1n
—0 X log q(yi|Xi)- En i X log qn(yi |xi)	QO)
n0	n0
i=1	i=1
Assume the held-out test set is large enough so that the difference between EnG(n) and (20) is neg-
ligible. We will refer to them interchangeably as the average generalisation error. In our experiments
we use n0 = 10, 000 and 30 draws of the dataset Dn to estimate En.
Last layer(s) inference Without loss of generality, we discuss performing inference in the w pa-
rameters of h while freezing the parameters of g at the MAP estimate. The steps easily extend to
performing inference over the final layer only of f = h ◦ g. Let Xi = gvMAp (xi). Define a new
transformed dataset Dn = {(Xi, yi)}i=1. We take the prior on W to be standard Gaussian. Define
the posterior over w given Dn as:
p(w∣Dn) H P(DnIw)2(W) = Πi=ι exp{-∣∣yi - hw(Xi)∣∣2∕2}P(W)	(21)
Define the following approximation to the Bayesian predictive distribution
p(y∣x, Dn) = /p(y∣x, (vmap, w))p(w∣Dn) dw.
Let wι,..., wR be some approximate samples from p(w∣Dn). Then We approximate p(y|x, Dn)
with
1R
R £p(y|x, (VMAP,wr ))
r=1
where R is a large number, set to 1000 in our experiments. We consider the Laplace approximation
and the NUTS variant of HMC for drawing samples from p(w|Dn):
•	Laplace in the last layer(s) Recall θMAP = (vMAP, wMAP) is the MAP estimate for fθ
trained with the data Dn . With the Laplace approximation, we draw w1, . . . wR from the
Gaussian
N (wMAP, Σ)
where Σ = (-V2 log p(w∣Dn) ∣wmap )-1 is the inverse Hessian4 of the negative log poste-
rior evaluated at the MAP estimate of the mode.
•	MCMC in the last layer(s) We used the NUTS variant of HMC to draw samples from
(21) with the first 1000 samples discarded.. Our implementation used the pyro package
in PyTorch.
4Following Kristiadi et al. (2020), the code for the exact Hessian calculation is borrowed from https:
//github.com/f-dangel/hbp
15
Under review as a conference paper at ICLR 2021
Figure 3: Realisable and minibatch gradient descent for MAP training.
Table 3: Companion to Figure 3.
(a)	1 hidden layer(s) in g, identity activation in h
(b)	5 hidden layer(s) in g, identity activation in h
method	learning coefficient	R squared
last two layers (A,B) MCMC	36.721594	0.992839
last layer only (B) MCMC	20.676920	0.983695
last two layers (A,B) Laplace	inf	NaN
last layer only (B) Laplace	1768.655088	0.838035
MAP	inf	NaN
(c) 1 hidden layer(s) in g, ReLU activation in h
method	learning coefficient	R squared
last two layers (A,B) MCMC	13.729278	0.924049
last layer only (B) MCMC	9.170642	0.945613
last two layers (A,B) Laplace	inf	NaN
last layer only (B) Laplace	1943.793236	0.794679
MAP	14.123308	0.917502
(d) 5 hidden layer(s) in g, ReLU activation in h		
method	learning coefficient	R squared
last two layers (A,B) MCMC	22.175448	0.975450
last layer only (B) MCMC	10.675455	0.968584
last two layers (A,B) Laplace	inf	NaN
last layer only (B) Laplace	inf	NaN
MAP	35.647464	0.983284
method	learning coefficient	R squared
last two layers (A,B) MCMC	4.652483	0.922693
last layer only (B) MCMC	3.533366	0.862125
last two layers (A,B) Laplace	inf	NaN
last layer only (B) Laplace	1004.852367	0.901899
MAP	6.256696	0.940437
16
Under review as a conference paper at ICLR 2021
Figure 4:	Nonrealisable and full batch gradient descent for MAP training.
Table 4: Companion to Figure 4. The learning coefficient is the slope of the linear fit 1/n versus
EnG(n) (with intercept since nonrealisable).
(a) 1 hidden layer(s) in g, identity activation in h			(b) 5 hidden layer(s) in g, identity activation in h		
method	learning coefficient	R squared	method	learning coefficient	R squared
last two layers (A,B) MCMC	11.086023	0.969991	last two layers (A,B) MCMC	0.808601	0.144260
last layer only (B) MCMC	7.377871	0.957824	last layer only (B) MCMC	0.799114	0.127686
last two layers (A,B) Laplace	NaN	NaN	last two layers (A,B) Laplace	NaN	NaN
last layer only (B) Laplace	30.692954	0.029238	last layer only (B) Laplace	-33.817429	0.009074
MAP	12.947959	0.970173	MAP	1.204743	0.242671
(c) 1 hidden layer(s) in g, ReLU activation in h			(d) 5 hidden layer(s) in g, ReLU activation in h		
method	learning coefficient	R squared	method	learning coefficient	R squared
last two layers (A,B) MCMC	5.987187	0.848490	last two layers (A,B) MCMC	0.794055	0.088305
last layer only (B) MCMC	5.384686	0.801313	last layer only (B) MCMC	1.141580	0.162585
last two layers (A,B) Laplace	NaN	NaN	last two layers (A,B) Laplace	NaN	NaN
last layer only (B) Laplace	38.629167	0.059012	last layer only (B) Laplace	-5.682602	0.000365
MAP	8.560722	0.816794	MAP	1.648073	0.284088
17
Under review as a conference paper at ICLR 2021
Figure 5:	Nonrealisable and minibatch gradient descent for MAP training. Missing points on the
MAP learning curve are due to estimated probabilities too close to 0.
Table 5: Companion to Figure 5. The learning coefficient is the slope of the linear fit 1/n versus
EnG(n) (with intercept since nonrealisable).
(a) 1 hidden layer(s) in g, identity activation in h			(b) 5 hidden layer(s) in g, identity activation in h		
method	learning coefficient	R squared	method	learning coefficient	R squared
last two layers (A,B) MCMC	11.086023	0.969991	last two layers (A,B) MCMC	0.808601	0.144260
last layer only (B) MCMC	7.377871	0.957824	last layer only (B) MCMC	0.799114	0.127686
last two layers (A,B) Laplace	NaN	NaN	last two layers (A,B) Laplace	NaN	NaN
last layer only (B) Laplace	30.692954	0.029238	last layer only (B) Laplace	-33.817429	0.009074
MAP	12.947959	0.970173	MAP	1.204743	0.242671
(c) 1 hidden layer(s) in g, ReLU activation in h			(d) 5 hidden layer(s) in g, ReLU activation in h		
method	learning coefficient	R squared	method	learning coefficient	R squared
last two layers (A,B) MCMC	5.987187	0.848490	last two layers (A,B) MCMC	0.794055	0.088305
last layer only (B) MCMC	5.384686	0.801313	last layer only (B) MCMC	1.141580	0.162585
last two layers (A,B) Laplace	NaN	NaN	last two layers (A,B) Laplace	NaN	NaN
last layer only (B) Laplace	38.629167	0.059012	last layer only (B) Laplace	-5.682602	0.000365
MAP	8.560722	0.816794	MAP	1.648073	0.284088
18
Under review as a conference paper at ICLR 2021
Figure 6: Realisable and full batch gradient descent for MAP. average generalisation errors of
Laplace approximations of the predictive distribution. The last-two-layers Laplace approximation
results in numerical instabilities due to degenerate Hessian. Any missing points are due to estimated
probabilities too close to 0.
Figure 7: Realisable and minibatch gradient descent for MAP training. Details are same as for
Figure 6
19
Under review as a conference paper at ICLR 2021
Figure 8: Nonrealisable and full batch gradient descent for MAP training. Details are same as for
Figure 6
Figure 9: Nonrealisable and minibatch gradient descent for MAP training. Details are same as for
Figure 6
20