Under review as a conference paper at ICLR 2021
AR-ELB O: Preventing Posterior Collapse
Induced by Oversmoothing in Gaussian VAE
Anonymous authors
Paper under double-blind review
Ab stract
Variational autoencoders (VAEs) often suffer from posterior collapse, which is
a phenomenon that the learned latent space becomes uninformative. This is re-
lated to local optima of the objective function that are often introduced by a fixed
hyperparameter resembling the data variance. We suggest that this variance pa-
rameter regularizes the VAE and affects its smoothness, which is the magnitude
of its gradient. An inappropriate choice of this parameter causes oversmoothness
and leads to posterior collapse. This is shown theoretically by analysis on the
linear approximated objective function and empirically in general cases. We pro-
pose AR-ELBO, which stands for adaptively regularized ELBO (Evidence Lower
BOund). It controls the strength of regularization by adapting the variance pa-
rameter, and thus avoids oversmoothing the model. Generation models trained by
proposed objectives show improved Frechet inception distance (FID) of images
generated from the MNIST and CelebA datasets.
1	Introduction
The variational autoencoder (VAE) framework (Kingma & Welling, 2014; Higgins et al., 2017; Zhao
et al., 2019) is a popular approach to achieve generative modeling in the field of machine learning.
In this framework, a model that approximates the true posterior of observation data, is learned by a
joint training of encoder and decoder, which creates a stochastic mapping between the observation
data and the learned deep latent space. The latent space is assumed to follow a prior distribution. The
generation of a new data sample can be done by sampling the latent space and passing the sample
through the decoder. It is common to assume that both the prior on the latent space and the posterior
of the observation data follow a Gaussian distribution. This setup is also known as the Gaussian
VAE. In this case, the variance of the decoder output is usually modeled as an isotropic matrix σx2I
with a scalar parameter σx2 ≥ 0. Furthermore, in order to deal with the intractable log-likelihood of
the true posterior, the evidence lower bound (ELBO) (Jordan et al., 1999) is adopted as the objective
function instead.
While VAE-based generative models are usually considered to be more stable and easier to train
than generative adversarial networks (Goodfellow et al., 2014), they often suffer from the problem
of posterior collapse (Bowman et al., 2015; S0nderby et al., 2016; Alemi et al., 2017; XU & Durrett,
2018; He et al., 2019; Razavi et al., 2019a; Ma et al., 2019), in which the latent space has little
information of the input data. The phenomenon is generally mentioned as “the posterior collapses
to the prior in the latent space” (Razavi et al., 2019a). Recently, several works have suggested
that the variance parameter σx2 in the ELBO is strongly related to posterior collapse. For example,
Lucas et al. (2019) analyzed posterior collapse through the analysis on a linear VAE. It revealed that
an inappropriate choice of σx2 will introduce sub-optimal local optima and cause posterior collapse.
Moreover, Lucas et al. (2019) reveals that contrary to the popular belief, these local optima are not
introduced by replacing the log-likelihood with the ELBO, but by an excessively large σx2 . On the
other hand, it can be shown that fixing σx2 to an excessively small value leads to under-regularization
of the decoder, which can cause overfitting. However, in most implementations of a Gaussian VAE,
the variance parameter σx2 is a fixed constant regardless of the input data and is usually 1.0. In
another work, Dai & Wipf (2019) proposed a two-stage VAE and treated σx2 as a training parameter.
Besides the inappropriate choice of the variance parameter, posterior collapse can also induced by
other causes. For example, Dai et al. (2020) found that, small nonlinear perturbation introduced in
1
Under review as a conference paper at ICLR 2021
the network architecture can also result into extra sub-optimal local minima. However, in this work
we will keep our focus on the variance parameter.
We suggest that σx2 affects the strength of regulation over the gradient magnitude of the decoder. We
call the expected gradient magnitude the smoothness throughout this paper. The smaller the gradient
magnitude, the smoother the model. In particular, we would like to focus on the local smoothness of
the model, which is the smoothness evaluated within the neighborhood of the encoded latent variable
of the observation data. Thus, we begin with the following hypothesis:
Main Hypothesis. The value of σx2 controls the regularization strength of the smoothness of the
decoder. Therefore, an excessively large σx2 causes oversmoothness, which results in posterior col-
lapse.
Following the hypothesis, the estimation of σx2 should be related to properties of the approximated
posterior of the latent space, such as its local smoothness. We will start with analyzing how σx2
regularizes the local smoothness of the stochastic decoder and then propose new objective functions
that inherently determine σx2 via maximum likelihood estimation (MLE). This proposed objective
function is named AR-ELBO (Adaptively Regularized ELBO), which controls the regularization
strength via σx2 . Furthermore, several variations are derived for different parameterizations of vari-
ance parameters.
Our main contributions are listed as follows:
1.	We show that our main hypothesis holds for linear approximated ELBO and empirically
holds in the general case in Section 3. This also suggests that the variance parameter σx2
should be estimated from properties of the approximated posterior instead of being treated
as a hyperparameter.
2.	We propose the AR-ELBO, an ELBO-based objective function that adaptively regularizes
the smoothness of the decoder by MLE of the variance parameter σx2 in Section 4. Vari-
ations of AR-ELBO for several variance parameterizations of posterior distributions are
also derived. AR-ELBO prevents the model from the posterior collapse induced by over-
smoothing and improves the quality of generation, which is shown in Section 5.
The organization of this paper is as follows. In Section 2, we propose a mathematical definition of
posterior collapse in the form of mutual information. This also includes the conventional definition:
“the posterior collapses to the prior in the latent space”. In Section 3, the theoretical analysis and
empirical support of the main hypothesis are given. We perform an analysis showing that σx2 affects
the smoothness of the decoder via variance parameters of the latent space learned by the encoder.
In Section 4, we propose new AR-ELBO objective functions for various variance parameterizations
of posterior distributions, which can relieve the decoder from being oversmoothed in the training
and prevent posterior collapse. These objective functions no longer include any hyperparameters
and can adaptively estimate the variance parameter σx2 from the observation data. It should be noted
that if we adaptively determine σx2 with the proposed AR-ELBO, the strength of regularization of
the decoder smoothness will gradually decrease as training progresses. In Section 5, we conduct an
experiment on the MNIST and CelebA datasets, which shows that utilizing the proposed AR-ELBO
with the standard Gaussian VAE can be competitive with many other variations of VAE models in
most situations.
Throughout this paper, we use a, a and A for a scalar, a column vector and a matrix, and ln and log
denote the natural logarithm and common logarithm, respectively. Our code is available from the
following URL1.
2	Posterior Collapse in Gaussian VAE
We begin with the standard formulation of the Gaussian VAE, which is the foundation of our re-
search. A definition of posterior collapse is proposed by using mutual information (MI).
1URL hidden due to blind review.
2
Under review as a conference paper at ICLR 2021
2.1	GAUSSIAN VAE
Consider a data space X ⊂ Rdx and a sample set {xi}N=ι ⊂ X, where Xi 〜Pdata(x). The empirical
distribution Pdata(x) on X can be evaluated by Pdata(X)= 得 PnN=I δ(x - Xn), where δ(∙) denotes
the Dirac delta function. In the standard VAE framework, a latent space Z ⊂ Rdz is learned and the
sampled latent variables Z ∈ Z are used to generate data samples x0 ∈ X. Let qφ(z∣x) andpθ(x|z)
denote the stochastic encoder and decoder, respectively. Trainable parameters of the two neural
networks are denoted as φ and θ. The decoder generates data samples by pθ(x) := Ep(Z) [pθ(x|z)],
where P(z) is the prior distribution on Z. The encoder and decoder are jointly trained by minimizing
the following objective function:
L = -Epdata(X) [lnPθ(X)] + Epdata(X)DKL (q0(ZIX) k Pθ(ZIX)) + Epdata(X) [lnPdata(X)]
=DKL (Pdata(X) k Pθ(x)) + Epdata(X)DKL ® (z∣x) k Pθ(z∣x)) .	(1)
This objective function was derived in Zhao et al. (2019), which represents everything in the form of
Kullback-Leibler divergence, and is equivalent to ELBO maximization UP to an additive constant.
In the context of the Gaussian VAE, the encoder and decoder are assumed to satisfy
qφ(z∣x) = N (z∣μφ(x), diag(σ2 (x)))	and	pθ (x∣z) = N (x∣μθ (z),σ2I).	(2)
The prior P(Z) is also assumed to be the Gaussian distribution as P(Z) = N (ZI0, I). Substituting (2)
into (1) while omitting terms independent of θ and φ leads to the following objective:
Jσ2 (θ, φ) = Epdata(X) 2σ2 Eqφ(z∣x)[kx - μθ (Z)k2]+ DKL∣⅛(ZIX) k P(Z)),	⑶
which can be interpreted as the sum of the expected values of the reconstruction loss and the reg-
ularization term. In the case of a Gaussian prior and posterior, the regularization term is equal to
1 Pd=ι(σφ,i(X) + μφ,i(χ)2 - logσ2,i(X) -1).
2.2	Posterior collapse
Posterior collapse is a major problem, where the encoder learns to map inputs to the latent space
while ignoring the data distribution. In this phenomenon, the MI between input data and recon-
structed data through the encoder-decoder path is reduced because the latent space has less informa-
tion about the data distribution. Here, we suggest the following definition of posterior collapse.
Definition 1. Posterior collapse is defined as the MI I(x; x0) becoming nearly zero, where x0 :=
μθ(z) with z 〜qφ(z∣χ).
In many works (Bowman et al., 2015; S0nderby et al., 2016; Alemi et al., 2017; He et al., 2019;
Razavi et al., 2019a), the phenomenon denoted as posterior collapse has been mathematically rep-
resented as Epdata(X)DKL(qφ(zIx) k P(z)) → 0, which we hereafter refer to as KL collapse (Xu &
Durrett, 2018). However, posterior collapse is not always caused by the diminished KL divergence,
i.e., posterior collapse with Epdata(X)DKL(qφ(zIx) k P(z)) 6= 0 can occur. The proposed definition of
posterior collapse includes KL collapse from the following theorem, which is proven in Appendix A.
Theorem 2. I(x; x0) → 0 as Epdata(X)DKL(qφ(zIx) k P(z)) → 0 holds for anyPθ(xIz).
In Appendix E, it is demonstrated that posterior collapse can happen even if the KL divergence is
nonzero when the posterior variance is fixed in Z.
3	VARIANCE PARAMETER σx2 AND THE LOCAL SMOOTHNESS
In this section, we provide mathematical and empirical support of the main hypothesis. Throughout
this section, we adopt the following parameterization for the encoder for simplicity: qφ,σ2 (zIx) =
N (z∣μφ(χ), σz2I), where the variance is parameterized as an isotropic matrix unlike the conventional
VAE. A similar analysis on the conventional VAE can be found in Appendix B. It begins with
showing that the choice of σx2 affects the convergence point of σz2, which is the variance parameter
of the latent space. Then, we show that σz2 acts as the weight of the gradient penalty, which is
3
Under review as a conference paper at ICLR 2021
implicitly included in (3). This supports the main hypothesis that the over-regulation imposed by a
large σx2 via σz2 causes the oversmoothness of the decoder and leads to posterior collapse. It is also
empirically supported by observing the tendencies of the convergence point of σz2, the smoothness
and the MI I(x, x0). Ultimately, these items of evidence motivated us to develop a method that
adapts σx2 to prevent oversmoothing of the decoder.
3.1	REGULARIZATION EFFECT OF σ2 IN LINEAR APPROXIMATED ELBO
x
The effect of σx2 on the convergence point of the variance parameter σz2 can be observed
from two extreme cases, σ2 → 0+ and σ2 → ∞. In the first case, Jσ2 reduces to
Epdata(X)Eq力 b2(z∣χ)["x - μθ(z)k2], and it becomes zero only if σZ = 0. In the second case,
Jσ2 reduces to Epdata(X)DKL(qg# (z|x) k P(Z)), and σZ becomes 1 at the minimum point, from
DκL(qφ,σ2 (z∣x) k P(Z)) = dz冠 — logσZ - 1) + ∣∣μφ(x)k2. This shows that a small σ2 makes σZ
converge to a value around 0, while a large σx2 makes σz2 converge to a value around 1.
If σz2 is sufficiently small, as the training progresses to a certain extent, the perturbed decoding
process μθ(z + Wz) around Z = μg(x) with Wz 〜 N(金|0, σ∣I) can be approximated as a linear
function. The ELBO can be approximated as follows by using the linear approximation of μθ(∙) and
omitting terms independent of θ and φ:
Jσ2 (θ,φ,σ2) ≈ 2σ2Epdata(X)hkx — μθ(μφ(X))k2 + σZ ∣∣Vμθ(μ0(X))kF + 2σ2 kμφ(χ)k2].⑷
In the approximation above, k ∙ kF is the Frobenius norm and σz is treated as a function parameter.
The derivation of the above approximation can be found in Appendix B. Equation (4) decomposes
the objective function into three terms: a reconstruction error term, gradient penalty term and L2
regularization term. As one can see from (4), σz2 regularizes the smoothness of the decoder by
penalizing its gradient norm in training. Although the linear approximation above is derived for
the simplified VAE parameterization, we also provide the linear approximation of the ELBO for the
standard VAE parameterization (2) in Appendix B, where the second term in (4) becomes a weighted
gradient penalty.
Summarizing the above observations shows that σx2 affects the smoothness via σz2, while σz2 di-
rectly regularizes the smoothness. This means that if σx2 is excessively large, it will cause over-
regularization of the decoder and suppress I(Z, x0)(≥ I(x, x0)), which finally leads to posterior
collapse. To avoid such over-regularization, σx2 and σz2 should be determined appropriately. In ad-
dition, an experiment shows that posterior collapse can be triggered by directly manipulating σz2, as
discussed in Appendix E.
3.2	Empirical study on smoothness of decoder in the general case
Section 3.1 shows the impact of σx2 on the regularization of the decoder smoothness through the
linear approximated objective function. To support the main hypothesis in the general case, an ex-
periment on the MNIST dataset (LeCun et al., 1998) is conducted. Several criteria are accessed
to provide evidence for the regularization effect of σx2 on the decoder smoothness and its conse-
quential effect on MI I(x, x0). To confirm that σx2 affects the smoothness via σz2, we conduct the
experiment for two cases: stochastic encoding and deterministic encoding. While the stochastic
encoder qφ,σ2 (Z|x) is used in the former case, a VAE equipped with a deterministic encoder, i.e.,
σz2 is fixed to zero during the training, is investigated in the latter case. Observing the difference
between the two cases provides empirical support for Section 3.1. To investigate the relation be-
tween σx2 and the smoothness of the decoder clearly, common generalization techniques such as
batch normalization (Ioffe & Szegedy, 2015; Santurkar et al., 2018) and weight decay are not used.
Criteria We used several criteria to observe the impact of σx2 in the experiment, such as the recon-
struction error (MSE), KL divergence value Epdata(X)DKL(qg(z∣x) k P(Z)) and the final converged
value ofσz2. In addition, we also estimate the local smoothness of the decoder and the MI between x
and x0, denoted as I(x, x0). To access the local smoothness, the expected local smoothness (ELS)
is introduced, which is the lower bound of the Lipschitz constant of the decoder. Consider a sam-
ple that is decoded with perturbation μθ(μg(x) + Wz), where the perturbation follows a zero-mean
4
Under review as a conference paper at ICLR 2021
Table 1: Evaluation of various criteria for different σx2: the expected value of kx0 - xk22 (MSE),
KL divergence, the converged value of σz2 , the upper bound of the MI I(x0 ; z), the expected gap
(perturbation variance sz2 are set to 10-2 and 10-3) and expected local smoothness (ELS).
	Stochastic encoding							Deterministic encoding				
log σx2	MSE	KL	σ2	MI	Expected gap		ELS	MSE	MI	Expected gap		ELS
			z		10-2	10-3				10-2	10-3	
0.0	52.74	0.00	1.00	0.03	6.31e-5	6.35e-6	3.97e-4	5.95	12.5	74.6	25.3	7.43e+2
-0.1	18.03	9.39	9.56e-2	9.7	1.05	0.108	6.76	5.69	14.7	69.1	22.5	6.82e+2
-0.2	15.15	10.93	6.48e-2	12.5	1.30	0.135	8.34	5.38	17.9	63.4	20.7	6.37e+2
-0.3	13.08	12.54	4.36e-2	16.0	1.51	0.157	9.72	5.37	21.4	58.1	17.9	5.78e+2
-0.4	11.38	14.13	3.01e-2	20.6	1.77	0.184	1.14e+1	5.31	25.8	58.2	15.5	5.40e+2
-0.5	10.18	15.30	2.14e-2	26.3	1.99	0.208	1.28e+1	5.26	30.6	53.1	12.9	4.74e+2
-0.6	9.16	16.72	1.55e-2	33.2	2.16	0.227	1.40e+1	5.14	38.9	48.9	11.8	4.42e+2
-0.7	8.25	18.05	1.11e-2	42.3	2.31	0.244	1.50e+1	5.17	46.1	45.5	10.1	3.98e+2
-0.8	7.72	19.27	8.21e-3	52.9	2.40	0.254	1.56e+1	5.06	58.0	43.2	9.15	3.71e+2
-0.9	7.13	20.55	5.97e-3	64.9	2.43	0.257	1.58e+1	4.98	71.9	39.2	7.83	3.29e+2
-1.0	6.70	21.75	4.45e-3	82.3	2.57	0.272	1.67e+1	5.01	89.1	35.3	6.61	2.89e+2
Guassian distribution with variance S2, Wz 〜N& |0, S21). Let Wz and W be i.i.d. random variables.
We define the expected gap ∆2 (sz2) as
∆ (sz) := Epdata(x)EN(z |0,sz2 I)N (0z |0,s2z I) [∆ (x, Wz, Wz)]	(5)
with ∆2(x, Wz, Wz):= ∣∣μθ(μφ(x) + Wz) - μθ(μφ(x) + Wz)∣∣2∙
As s2 decereases, the ratio ∆2(s2)∕(2s2) converges and becomes an indicator of
Epdata(x)[∣Vμθ(μφ(x))∣F], which is regularized by σ2 in (4). Therefore, We can now define
the ELS as
Epdata(x)[kV“θ 3φ(X))kF].	⑹
Further details of the ELS are described in Appendix C. As a reference, we estimate the upper bound
ofI(X, X0), which is I(X0; z), by Monte Carlo estimation.
Results Table 1 summarizes the results for different σx2. In the stochastic encoding case, a larger σx2
consistently leads to a larger σz2. This results in a smaller expected gap, a smaller ELS and a lower
upper bound of MI. This supports the main hypothesis that a larger σx2 makes the decoder smoother.
In the case of σx2 = 1.0, all the criteria except MSE become nearly zero, where KL collapse and
posterior collapse both occur due to the over-regularization of the smoothness of the latent space.
On the other hand, in the deterministic encoding case, the ELS increases with σx2 . This is because
σx2 does not directly regularize the decoder via the gradient penalty as in (4). As a result, the MI
upper bound does not shrink to zero even if σx2 = 1.0, where posterior collapse occurs in the case of
stochastic encoding. The difference in the results between the two cases clearly suggests that a large
σx2 triggers the oversmoothness via σz2, which is consistent with the discussion in Section 3.1. These
results provide empirical support of the main hypothesis as well as the discussion in Section 3.1.
Further details and examples of images are shown in Appendix D.
3.3	DIFFICULTY OF CHOOSING AN APPROPRIATE σ2
x
According to previous sections, a large σx2 will cause oversmoothness. Therefore, we consider the
case of fixing σx2 to a sufficiently small value to avoid the problem. We arrive at the following
theorem, whose proof can be found in Appendix F:
Theorem 3. Consider the global optimum of Jσ2 (θ, φ, σz2) w.r.t. a given σx2. If σx2 → 0, then
σ2 → 0.
z
In Theorem 3, Jσ2 (θ, φ, σz2 ) is optimized on the basis of the true data distribution instead of the
empirical data distribution. According to the theorem, σz2 converges to zero as σx2 approaches zero,
which leads to zero gradient penalty for the decoder as the VAE training progresses. In practice, we
have no access to Pdata (x), but We have access to the empirical distribution Pdata (x). Theorem 3 is
satisfied even when Pdata(X) is replaced with Pdata(x). In this case, where σ2 is chosen to be small,
the optimization process of Jσ2 will fitPθ,σ2 (x) to the empirical distribution Pdata(x), which usually
results in overfitting.
5
Under review as a conference paper at ICLR 2021
As shown above, choosing an appropriate σx2 that avoids both oversmoothness and overfitting is
nontrivial. Moreover, it is likely that σx2 should be adapted depending on the status of training.
Therefore, it is intuitive to adapt σx2 instead of fixing it, which will be described in the next section.
4	Adaptively Regularized ELBO
A modified ELBO-based objective function is proposed, which can be interpreted as an implicit
update scheme that simultaneously updates σx2 and the rest of the parameters. We also derive corre-
sponding objective functions for models with different variance parameterizations.
4.1	ELBO WITH ADAPTIVE σx2
In this subsection, we newly optimize the VAE objective function (1) w.r.t. all the parameters includ-
ing σx2, which is usually fixed in existing implementations. Following the process of establishing (3)
but keeping the terms related to σx2 , we arrive at:
J(θ,φ, σ2 ) = Epdata(X) 212 Eqφ(z∣x) hkx - μθ (Z)k2i + DKL Sφ(ZIX) k P(Z)) + dln σ2 .⑺
x
From the partial derivative of J w.r.t. σ2, the MLE of σ2, denoted as σ2, can be evaluated with
the other parameters fixed. Then, the ordinary network parameters θ and φ can be updated by
optimizing (7) with the variance σx2 fixed. This combination of MLE and the alternative update
between (θ,φ) and σ2 guarantees that (i) if θ and φ are fixed, then there exists σ2 such that
J(θ, φ, σX) ≤ J(θ, φ, σ2) and (ii) for the σ2 obtained in the previous step, there exist θ and φ,
such that J(θ, φ,σ2) ≤ J(θ, φ,σ2). In this respect, the convergence of the optimization is assured
and the parameter σx2 is kept as the MLE during the whole training stage. This inspired us to develop
a weight scheduling scheme for σ2, leading to a modified ELBO-based objective function. Consider
the trainable network parameters (θ, φ) and the variance parameter σx2 . The update of the objective
J(θ, φ, σ2) is divided as
σ2(t+1) = 丁Epdaia(x)Eqφ(t) (z|x) hkX - μθ(t) (Z) Il2]
θ(t+1),φ(t+1) = argmin J 2(t+i)(θ,φ),
σx
(8a)
(8b)
θ,φ
where t is the iteration index. The step updating (θ, φ) is the same as that in the standard VAE;
the step updating σx2 in (8a) can be interpreted as determining an appropriate balance between the
reconstruction error and the KL term in Jσ2 (θ, φ). As the learning progresses, the parameter σx2 will
decrease along with the MSE Epdata (χ)Eqφ(z∣χ) [∣∣x-μθ (z)k2], which is consistent with the discussion
in Dai & Wipf (2019).
Proposed objective function (AR-ELBO) The update scheme above can be further simplified by
substituting (8a) into (7), which converts J(θ, φ, σ2) into
JAR(θ,φ) = d ln Epdata(X)Eqφ(z∣x) [kX - μθ (Z)k2] + Epdata(x)DKL (q0(z|x) || P(Z)) ,	(9)
where all constant terms w.r.t. the parameters are omitted. Optimizing (9) also makes σx2 remain
as the MLE during the VAE training. Moreover, (9) is equivalent to the standard Gaussian VAE
plus weight balancing with (8b). This relieves the VAE from the problem of imbalance between the
KL divergence term and the reconstruction loss. Also, as stated by Theorem 3, decreasing σx2 also
decreases σz2. This gradually relieves the regularization of the ELS (6), which can be observed from
(4). However, this eventually diminishes the gradient penalty; therefore, we suggest using early-
stopping and learning rate scheduling to deal with this situation, which can give the decoder both
appropriate smoothness and generalization capability.
4.2	Objectives for various parameterizations
In the standard VAE given by (2), the variance of the decoded distribution on X, denoted as Σx, is
modeled as an identity matrix, i.e., Σx = σx2I. In this case, σx2 is simply a scalar value and the re-
construction objective is the same as conventional MSE and is minimized as in (9). However, out of
6
Under review as a conference paper at ICLR 2021
Table 2: Parameterizations of posterior variance in X and corresponding reconstruction objectives.
	Variance model (Σχ)	T-I	.	. ∙	Λ ∙	. ∙	，i	/ t∖	∖	<∖	∖ ∖ Reconstruction objective (Jrec(θ, φ, Σx))
(Iso-I)	σ21	与 ln Epdata(X)Eqφ(z∣x) [kx - μθ (Z)k2]
(Iso-D)	σ2 (Z)I	2x^Epdata(X)Eqφ(z∣χ) [ln Ilx - μθ(Z) k 2]
(Diag-I)	diag(σ2)	2 Pd= ι ln Epdata(X)Eqφ(z|x) [(◎ - μθ,i(z))2]
(Diag-D)	diag(σ2(Z))	2 Pi= 1 Epdata(X)Eqφ (z|x) [ln(Xi - μθ,i(Z))2]
curiosity, we would like to explore three other variance parameterizations in addition to (2) and de-
rive corresponding reconstruction objectives for these cases, in which the reconstruction objectives
are no longer equal to MSE.
In fact, the variance Σx can not only be parameterized by an isotropic/diagonal matrix but also
be chosen to be independent or dependent on z. We denote these in Table 2 as Iso-I (Isotropic-
Independent), Iso-D (Isotropic-Dependent), Diag-I (Diagonal-Independent) and Diag-D (Diagonal-
Dependent). The first case, Iso-I, corresponds to the standard variance model Σx = σx2I. For these
parameterizations, the corresponding objectives may be summarized as
JARm Φ, ∑x) = Jrec(θ, Φ, ∑x) + Epdata(X)DKL S(z∣x) k P(Z))	(10a)
Jrec(θ,φ, Σχ) = 1 Epdata(X)Eq(Z∣χ) [trace (∑-1(x - "θ(z))(x - "θ(z))>) + ln l∑χ∣]∙ (10b)
By evaluating the partial derivative of Jrec w.r.t. Σx, i.e., using its MLE, the reconstruction loss
Jrec corresponding to each case can be derived. All the derivations can be found in Appendix G.
The final reconstruction objectives with different parameterizations of Σx are listed in Table 2. It
is interesting to note that the reconstruction error for each dimension in the data space has to be
calculated separately for Diag-D; meanwhile, only MSE for the whole minibatch is needed in Iso-I.
Considering the optimization stability in practical situations, we suggest adding a small constant,
e.g., 10-6, before taking the logarithm except for in the case of Iso-I.
Although the proposed objective functions are capable of determining Σx appropriately, it should
be noted that a gap still exists between the prior p(z) and the aggregated posterior qφ(z) obtained
by the proposed methods. The cause can be observed from the reformulated (1) (see Appendix H):
L = DKL (Pdata(x) k Pθ(x)) + DKL (Pdata(X)qφ(z∣x) k qφ(z)pθ(x∣z)) + DKL (qφ(z) k p(z)).
(11)
The first two terms in (11) can eventually become dominant in the VAE training. As a consequence,
generation through sampling latent variables from the prior can cause off-distribution samples to
be generated. To overcome this prior-posterior mismatch, at least two approaches can be adopted:
(i) conduct another posterior estimation after the ordinary VAE training (van den Oord et al., 2017;
Razavi et al., 2019b; Dai & Wipf, 2019; Ghosh et al., 2020; Morrow & Chiu, 2020) or (ii) add
another regularizing term to the objective function (Makhzani et al., 2015; Tolstikhin et al., 2018;
Zhao et al., 2019). The former approach is adopted in our work since it is effective (Ghosh et al.,
2020) and is applicable to any VAE variation.
To summarize, the proposed AR-ELBO regularizes the Gaussian VAE with appropriate weighting
for the gradient penalty without the need for an extra hyperparameter. Moreover, the remaining
mismatch between the prior and posterior is mitigated by an extra pass of posterior estimation. A
detailed discussion comparing the proposed objective function with previous works can be found in
Appendix I.
5	Experiments
We compare the proposed methods with the following models: VAE, RAE (Ghosh et al., 2020),
WAE-MMD (Tolstikhin et al., 2018) and plain autoencoder (AE). The quality of generated im-
ages is evaluated using FreChet Inception Distance (FID) (Heusel et al., 2017) on the MNIST and
7
Under review as a conference paper at ICLR 2021
Table 3: Numerical evaluation on MNIST and CelebA. The MSE of sample reconstruction is evalu-
ated on the test set. The quality of generated samples is measured by FID for three cases: sampling
the latent variables from the prior and from the estimated posterior by 2nd VAE and GMM.
		MNIST						CeIebA					
	MSE	FID			MSE	FID			
		Prior	2ndVAE	GMM-10		Prior	2ndVAE	GMM-10	GMM-100
VAE (σx2 = 1.0)	20.25	55.85	182.64	-58.96—	121.91	55.46	139.32	54.66	53.94
WAE-MMD	4.34	22.76	15.00	13.70	62.66	52.89	51.32	43.57	41.88
AE	4.31	-	20.66	13.20	61.44	-	62.34	46.47	43.41
RAE	4.28	-	18.54	13.68	61.49	-	57.26	46.50	43.89
RAE-GP	4.30	-	18.89	13.71	61.48	-	54.54	43.63	41.10
Iso-I w/ trainable ∑X	4.28	20.93	14.83 去^^	13.39-	61.42	63.21	^^63.12 去~	51.40	49.61
Diag-I w/ trainable ∑X	5.70	26.08	18.44	15.72	62.65	59.66	54.75	45.86	43.50
Iso-D w/ trainable ∑X	4.45	27.40	16.38	13.67	65.29	195.58	53.13	50.09	47.31
Diag-D w/ trainable ∑X	5.33	146.43	26.02	27.90	85.33*	354.20*	217.06*	188.30*	188.05*
Ours (Iso-I w/ AR-ELBO)	4.40	22.78	15.77	1221-	62.02	82.20	52.48	42.82	41.03
Ours (Diag-I w/ AR-ELBO)	5.35	24.15	17.18	13.38	63.51	87.44	53.60	45.85	42.83
Ours (Iso-D w/ AR-ELBO)	4.31	22.94	17.57	12.89	61.38	78.24	49.97	43.27	40.39
Ours (Diag-D w/ AR-ELBO)	6.80	16.64	10.49	10.05	70.75	64.40	55.30	46.63	45.27
∑x is learned as a trainable parameter like in Dai & WiPf (2019). However, the work does not include Diag-L
Iso-D and Diag-D parameterizations.
"2ndVAE is the second-stage VAE proposed in Dai & Wipf (2019) after the main VAE.
*
The training of Diag-D with a trainable Σx does not converge to a feasible local optima. Moreover, the trend of
MSE diverges with that of the loss function. Therefore, the result which achieves the best MSE on the test set is
reported here. On the other hand, Diag-D with the proposed AR-ELBO does not suffer from this issue.
CelebA (LiU et al., 2015) datasets with the default train/test split. Regarding the prior-posterior
mismatch, three approaches are tested on all the models. The first approach is the conventional
case, which samples the latent variables from the prior. The other two approaches are applied after
the ordinary training. The second approach forms an aggregated posterior qφ (z) by a second-stage
VAE (Dai & Wipf, 2019). The third approach uses a Gaussian mixture model (GMM) with 10 to
100 components (Ghosh et al., 2020) to fit the posterior. The baseline is the standard VAE, and two
methods of choosing σx2 are tested: (i) σx2 fixed to 1.0 as in general implementations; and (ii) σx2
learned with (7) by an optimizer as an usual trainable parameter (Dai & Wipf, 2019). In RAE, its
objective function is the sum of the reconstruction error, the regularization of the decoder and the
L2 regularization of the latent space. In RAE-GP, gradient penalty is used as the regularization of
the decoder. The objective function of RAE-GP is equivalent to (4) except for that the weighting
parameters are determined manually. For WAE-MMD, inverse multi-quadratic kernels with seven
scales are used as proposed by Tolstikhin et al. (2018). Note that both WAE and RAE have hy-
perparameters in their objective functions. In contrast, the other methods, as well as the proposed
objective functions, include no hyperparameters. However, regarding the major factor that affects
the smoothness of the decoder, the proposed methods and VAE control smoothness by regularizing
terms in their objective functions, while WAE and AE rely only on the network architecture and
generalization techniques.
The latent space dimensions for MNIST and CelebA were set to dz = 16 and 64, respectively,
consistent with Ghosh et al. (2020). A common network architecture, which is adopted from Chen
et al. (2016) and described in Appendix J, is used for all models. In Table 3, we report the evaluation
result of each method as: (i) the MSE of the reconstructed test data and (ii) the FID of generated
images. The proposed methods (Diag-I, Iso-D and Diag-D) except for Iso-I do not necessarily
achieve the best MSE, since the MSE is no longer the reconstruction loss for them. In the case
of sampling z ∈ Z from the prior, a low FID is achieved by WAE due to the relatively strong
regularization of the aggregated posterior with MMD. The images generated by the proposed method
achieve the best FID score on MNIST, and is competitive on CelebA. It should be noted that the
learned σx2 values on MNIST and CelebA from Iso-I are 0.0056 and 0.0050, respectively, which are
much smaller than 1.0. Examples of reconstructed and generated images are shown in Appendix K.
As shown in Table 3, different parameterizations of variances can affect the FID greatly. In order
to clearly observe the advantage of estimating Σx by MLE rather than estimating it as an usual
trainable parameter, we examined the two approaches on all the four parameterizations (Iso-I, Iso-
D, Diag-I and Diag-D): (i) solve (10b) using MLE as (9) (AR-ELBO); (ii) simply treat Σx as a
8
Under review as a conference paper at ICLR 2021
Table 4: Evaluation of FID scores of interpolated images on MNIST and CelebA. The interpolation
ratio for each image pair is designated as (i) the mid-point; and (ii) a random-point between the two.
		MNIST				CelebA		
	Mid-point	Random-point	Mid-point	Random-point
VAE (σx2 = 1.0)	-62.18	63.86	-57.87	55.54
WAE-MMD	17.27	12.41	41.93	38.80
AE	18.49	12.81	50.35	45.01
RAE	17.71	12.99	48.78	43.97
RAE-GP	17.98	12.96	45.22	40.58
Iso-I w/ trainable Σx	-15.66	1273	-52.01	48.83
Diag-I w/ trainable Σx	17.34	14.77	44.22	41.23
Iso-D w/ trainable Σx	17.03	13.33	48.51	43.97
Diag-D w/ trainable Σx	51.65	31.66	238.74	212.39
Ours (Iso-I w/ AR-ELBO)	-14.77	11:27	-42.59	39.19
Ours (Diag-I w/ AR-ELBO)	17.24	13.08	45.55	41.92
Ours (Iso-D w/ AR-ELBO)	15.30	12.01	42.49	38.93
Ours (Diag-D w/ AR-ELBO)	11.96	8.68	46.86	44.15
trainable parameter (Dai & Wipf, 2019). The comparative result can be obtained from the bottom
eight lines of Table 3. It shows that applying AR-ELBO improves FID scores in most of the cases.
Furthermore, in order to examine the feasibility of these learned latent spaces, we also evaluated the
FID scores of the images generated by interpolating two latent variables with ratios between [0, 1]
with all the models above. The proposed method still shows the best performance in terms of FID
on MNIST and is competitive on CelebA. This result suggests the feasibility of proposed method on
downstream tasks. The detail and image samples can be found in Appendix L.
6	Conclusion
We analyzed the posterior collapse phenomenon on the Gaussian VAE and investigated how strongly
the variance parameter impacts the local smoothness of the decoder. The relation between the
variance parameter and the local smoothness is examined both theoretically and empirically. We
proposed optimization schemes to regulate the local smoothness appropriately, which leads to the
prevention of posterior collapse due to oversmoothness. The proposed AR-ELBO implicitly opti-
mizes the variance parameter to avoid over-regularizing of the smoothness. In addition, we proposed
several parameterizations (Iso-D, Diag-I, Diag-D) of posterior variances, which are the extensions
of the conventional VAE (Iso-I). The corresponding AR-ELBOs for these parameterizations are
also derived. Our experiments show that the Gaussian VAE equipped with the proposed objective
functions is competitive with other state-of-the-art models in terms of FID for both generated and
interpolated images. Moreover, the proposed method remains stable in the most complicated pa-
rameterization (Diag-D). In this work, the prior-posterior mismatch was covered by extra posterior
estimation methods; however, we would like to seek a thorough solution for this in the future.
References
Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dillon, Rif A Saurous, and Kevin Murphy.
Fixing a broken ELBO. arXiv preprint arXiv:1711.00464, 2017.
Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Ben-
gio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349, 2015.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable representation learning by information maximizing generative adversarial nets. In
Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 2172-2180, 2016.
Bin Dai and David Wipf. Diagnosing and enhancing VAE models. In Proc. International Conference
on Learning Representation (ICLR), 2019.
Bin Dai, Ziyu Wang, and David Wipf. The usual suspects? Reassessing blame for VAE posterior
collapse. In Proc. International Conference on Machine Learning (ICML), 2020.
9
Under review as a conference paper at ICLR 2021
Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin.
Cyclical annealing schedule: A simple approach to mitigating KL vanishing. arXiv preprint
arXiv:1903.10145, 2019.
Partha Ghosh, Mehdi SM Sajjadi, Antonio Vergari, Michael Black, and Bernhard SchOlkopf. From
variational to deterministic autoencoders. In Proc. International Conference on Learning Repre-
sentation (ICLR), 2020.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proc. Advances in Neural
Information Processing Systems (NeurIPS), pp. 2672-2680, 2014.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of Wasserstein GANs. In Proc. Advances in Neural Information Processing Sys-
tems (NeurIPS), pp. 5767-5777, 2017.
Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference
networks and posterior collapse in variational autoencoders. In Proc. International Conference
on Learning Representation (ICLR), 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Proc.
Advances in Neural Information Processing Systems (NeurIPS), pp. 6626-6637, 2017.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a
constrained variational framework. In Proc. International Conference on Learning Representation
(ICLR), 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proc. International Conference on Machine Learning (ICML),
2015.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction
to variational methods for graphical models. Machine Learning, 37(2):183-233, 1999.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. Interna-
tional Conference on Learning Representation (ICLR), 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In Proc. International
Conference on Learning Representation (ICLR), 2014.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proc. IEEE, 86(11):2278-2324, 1998.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.
In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 3730-3738,
2015.
James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don’t blame the ELBO! A
linear VAE perspective on posterior collapse. In Proc. Advances in Neural Information Processing
Systems (NeurIPS), pp. 9403-9413, 2019.
Xuezhe Ma, Chunting Zhou, and Eduard Hovy. MAE: Mutual posterior-divergence regulariza-
tion for variational autoencoders. In Proc. International Conference on Learning Representation
(ICLR), 2019.
Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine
Learning Research, 9:2579-2605, 2008.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial
autoencoders. In Proc. International Conference on Learning Representation (ICLR) Workshop,
2015.
10
Under review as a conference paper at ICLR 2021
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for
generative adversarial networks. In Proc. International Conference on Learning Representation
(ICLR), 2018.
Rogan Morrow and Wei-Chen Chiu. Variational autoencoders with normalizing flow decoders.
arXiv preprint arXiv:2004.05617, 2020.
Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with
delta-VAEs. In Proc. International Conference on Learning Representation (ICLR), 2019a.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with
VQ-VAE-2. In Proc. Advances in Neural Information Processing Systems (NeurIPS),pp.14866-
14876, 2019b.
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry. How does batch nor-
malization help optimization? In Proc. Advances in Neural Information Processing Systems
(NeurIPS),pp. 2483-2493, 2018.
Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang,
and Tarek Abdelzaher. Controllable variational autoencoder. In Proc. International Conference
on Machine Learning (ICML), 2020.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Lad-
der variational autoencoders. In Proc. Advances in Neural Information Processing Systems
(NeurIPS), pp. 3738-3746, 2016.
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. Wasserstein auto-
encoders. In Proc. International Conference on Learning Representation (ICLR), pp. 5885-5892,
2018.
Aaron van den Oord, Oriol Vinyals, and Koray KavUkcUoglu. Neural discrete representation learn-
ing. In Proc. Advances in Neural Information Processing Systems (NeurIPS), pp. 6306-6315,
2017.
Jiacheng Xu and Greg Durrett. Spherical latent spaces for stable variational autoencoders. 2018.
Shengjia Zhao, Jiaming Song, and Stefano Ermon. InfoVAE: Balancing learning and inference in
variational autoencoders. In Proc. AAAI Conference on Artificial Intelligence (AAAI), pp. 5885-
5892, 2019.
A Proof of Theorem 2
Let xbe the input sample. We denote its corresponding latent space vector as z and the reconstructed
sample as x0 . We have the following relation:
I(x; z) ≥ I(x; x0),
(12)
which can be proved similarly to the proof of Lemma 5 in Appendix F. On the other hand, I(x; z)
can be evaluated by using the definition of the MI as
I(X; Z) = DKL(Pdata(X)qφ(zlX) ∣∣ Pdata(X)qφ(Z))
=Epdata(X)Eqφ(z∣x) [ln qφ (z|x) - ln qφ(z)]
=Epdata(X)DKL(qφ(ZIX) k P(Z))- DKL(qφ(Z) k P(Z))
≤ Epdata(X)DKL∣⅛(ZIX) Il P(Z)),
(13)
(14)
where I(X; Z), DKL(qφ(ZIX) ∣ P(Z)) and DKL(qφ(Z) ∣ P(Z)) are all non-negative. Inequalities (12)
and (14) lead to the proof.
11
Under review as a conference paper at ICLR 2021
B LINEAR APPROXIMATION OF THE ELB O-BASED OBJECTIVE Jσx2
We start with parameterizing the encoder while following the assumption in (2). Given a sufficiently
small perturbation with P(Wz) = N(z|0, diag(σφ(x))), the linear approximation of μθ(∙) at μφ(x)
can be represented as
μθ (μφ(χ) + wz) = μθ (μφ(χ)) + J“° (μφ (χ))ez,
(15)
where Jμg (μφ(x)) represents the Jacobian matrix of μθ(Z) at Z = μφ(x). Substituting (15) into (3)
leads to
Eqφ(ZIx)[kχ -μθ(Z)k2]
=EN (zl0,diag(σφ(x))) [kχ - (μθ (μφ(Xy) + Jμθ (μφ(Xy)wz )k2]
=kχ - μθ (μφ (χ))k2 + EN (z∣0,diag(σ2 (x))) [e>j“θ (Nφ(Xy)TJμθ (“φ(χ))ez]
+ EN(z∣0,diag(σφ(x))) [(χ - μθ(“φ(X)))>j“θ (“φ(X))/],	(16)
'----------------------------------------------------------}
{ζ^
=0
where the last term is zero under the assumption that the perturbation is sufficiently small. The
expectation in the second right-hand-side term can be evaluated as
EN(z∣0,diag(σφ(x))) [€z Jμθ (μφ(X)) Jμθ (μφ(X))Wz]
=trace (EN(z∣0,diag(σ2(x))) [wzw>] J“°("Φ(X))> J“°("Φ(X)))
=trace (diag(σφ(X))J“°(“Φ(X))> J“°("Φ(X)))
=XXσφ,j(χ) (dμ∂jz)∣	J,	(17)
i=ι j=ι	j	lz=μφ(z)/
which can be interpreted as the gradient penalty for the decoder weighted by σφ2 (X). By substituting
the above result into (3), its linear approximation can be obtained as
1
jσ2(θ,φ) ≈ 2σ^Epdata(x)
kχ - μθ (μφ(χ))k2
+ X Xσφj(X) (dμ∂Z㈤I	) +2σ2kμφ(χ)k2
i=1 j=1	∖ j ∣z=μφ(x)/
(18)
In the case of the simplified parameterization described in Section 3, the second right-hand-side
term in (18) can be further reduced to σ∣kVμθ(μφ(χ))kF. In the simplified case, the perturbation
follows a multivariate i.i.d. Gaussian distribution, Wz 〜 N(z|0, σzI). Under this assumption, We
have
Ep&) [w> jμθ (“φ(X))>j“θ (“φ (X))Wz] = Ep&)}> (X λiui (X)Ui(X)T) Wz
dz
=	λiUi(X)TEp(z)[WzWzT]Ui(X)
i=1
dz
= 2σz2 Xλi,	(19)
i=1
where λi is the ith eigenvalue of Jμ° (μφ(χ))Jμ° (μφ(χ))>, which is a symmetrical positive definite
matrix, and the corresponding eigenvectors are (Ui(X))id=z 1. Following the simplified assumption,
the second right-hand-side term in (16) now becomes EN (z∣o,σ2i) [W>Jμ° (μφ(χ))>Jμ° (μφ (X))Wz].
Combining (19) and the fact that Pd= ι λi = trace(Jμ° (μφ(χ))Jμ° (μφ(χ))>) =
12
Under review as a conference paper at ICLR 2021
∣∣Vμθ(μφ(x))k2, We can finally obtain the following linear approximation for the simplified pa-
rameterization:
EN (z∣0,σ2l) [ez Jμθ (μφ(x)) Jμθ (μφ (X))Wz]
=EN (z∣0,σzI) [trace jμθ (μφ(x))>Jμθ (“φ(x)))]
2
dμθ,i(Z)	∖
dzj	z=μφ(ZJ
σ2 kvμθ 3φ(X))kF.	QO)
dx	dz
σz2XX
i=1 j=1
C Expected local smoothness of decoder
Here, we describe the relation between the expected local smoothness Epdata(X)[kVμθ(μφ(x))kF]
and the expected gap ∆2(sz2). First, consider the relation
δ2(X, Ez, Wz) = kμθ (μφ(X) + Ez) 一 μθ (μφ(χ) + Wz)k2 = K 3φ(X), Ez, Wz)2I匕一W k2,
(21)
with the perturbation Wz following the Gaussian distributionN(Wz|0, sz2I). Applying the expectation
operator to (21) leads to
δ2(x, Ez , Ez ) = Ep(ez,ez) [Kθ (〃0(x), Ez, Ez )2kEz — Ez I∣2]	(22)
≤ Ep(χZ) [Kθ(μφ(x), Ez, Ez)2] Ep(χζ) [kEz - Ezk2]	(23)
= 2K2(μφ(x),s2 )dz sz,	(24)
whereP(Ez, Ez) := N(Ez |0, szI)N(ez∣0, szI), Ez — Ez 〜N(Ez — Ez|0,2szI) and Kz(μ0(x)):=
Ep(ez,e0) [Kθ(μφ(x), Ez, Ez)z]. Note that in (23), we assume that Kz(μφ(x)) is independent of Ez
and Ez. Consider the case that the variance sz is sufficiently small to approximate μθ (z) linearly
around Z = μφ(x), which is perturbed with variance Sz. In such a case, Kθ(μφ(x), Ez, Ez) is
independent of Ez and E0z, which fits the assumption in (23). Under this local linearity assumption,
Kz(μφ(x)) is bounded as
Kz(μφ(x),sz) ≤ Kz,	(25)
where Kθ denotes the Lipschitz constant of the decoder.
Following the assumption, Kz(μφ(x), Sz) can be formulated by invoking (15) as
Kθ(μφ(x),sz)
Ep(eζ,q)[kμe(μφ(X) + Ez) - μθ(μ0(X) + Ez)kz]
Ep(eζ,eZ )[kEz-Ezkz]
=Ep(eζ,q)[(Ez - Ez )>J“° (μφ(x))>Jμ° (“φ(x))(Ez - Ez)]
=	2dzsz
=trace (Ep(eζ,4)[(Ez - Ez )(Ez - Ez )>] Jμθ (“φ(X))> J"° (“φ(X)))
=	2dz Sz
=trace (Jμ° (μφ(X))> Jμθ (μφ(X)))
dz	.
Applying the expectation operator to (26) leads to
Kθ(sz) := Epdata [Kz (μφ(x), sz)]
Epdata(X) [trace Jμθ (μφ(X))> j“θ (μφ(X)))]
dz
EpdatahkVμθ 3。(X))kF i
dz	.
(27)
Finally, combining (24) and (27) yields the following connection between the expected gap and the
expected local smoothness:
∆z(sz) = 2Epdata hkVμθ(μφ(x))kFi Sz.	(28)
13
Under review as a conference paper at ICLR 2021
D Experimental details for Section 3.2
D.1 Experimental setup
In the experiment, the model is trained with the Adam optimizer with a learning rate of 10-3. The
dimension of the latent space is set to 8. We run 200 epochs with a minibatch size of 64 for all σx2 .
We use the following DNN architectures for the encoder and decoder, respectively:
x ∈ R28×28 → Conv64 → ReLU	size of (64, 14, 14)
→ Conv128 → ReLU → Reshape(128, 7, 7)	size of (128, 7, 7)
→ Flatten → FC1024 → ReLU
→ FC16,
z ∈ R16 → FC1024 → ReLU
→ FC128×7×7 → ReLU	size of (128, 7, 7)
→ ConvT64 → ReLU	size of (64, 14, 14)
→ ConvT1 → Sigmoid	size of (1, 28, 28).
Here, FCk, Convk, ConvTk and ReLU denote the fully connected layer mapping to Rk, the con-
volutional layer mapping to k channels, the transpose convolutional layer mapping to k channels
and the rectified linear units (ReLU), respectively. The 3-tuple (channels, height, width) in the
right column represents the output shape of each layer. In all the Convk and ConvTk layers, 4 × 4
convolutional filters are used with a common stride of (2, 2).
Regarding the evaluation of criteria, MSE and KL are evaluated on the training set because the aim
of the experiment is to validate the relation between σz2 and the smoothness of the decoder. The
upper bound of the MI is obtained by calculating
-Epdata(X)Eqφ,σ2 (z|x)	ln Epdata (x0) eXp (一 ^2 Ilz - μφ(XO)Il2)	- 2χ	(29)
for each minibatch and then taking their mean, where the batch size is 10, 000 for all the evaluations.
D.2 Samples of generated images and t-SNE visualization of latent spaces
Figure 1 shows several images decoded from μφ(x) + Wz with Wz 〜N&|0, S21) for the cases
with σx2 = 1.0 and 0.1. Posterior collapse can be observed from these blurry images decoded from
the stochastic encoding case with σx2 = 1.0. This is due to the removal of batch normalization,
which makes σx2 = 1.0 become an inappropriate choice. However, if σx2 is determined or adapted
appropriately such as by using the proposed method, posterior collapse will not happen. In the other
settings, the tendency of how the image changes with the perturbation is similar, as shown in Table 1.
	Stochastic encoding		Deterministic encoding				
		s，= 10-2		S? = IOT	城=10^2		s≡ = 10^3		
⅛ = i∙θ		H∏f ⅝⅝⅝1				g∣0∕ 3 U 4 4 q	
σx = o.ι	■1。IaIJ /3 3∣	∙s∣0j 3 J 3 0 3∣				训3|。夕 J 2 2 2	
Figure 1:	Images in red boxes are the original images sampled from the MNIST dataset. The images
in blue boxes are reconstructed by μθ(μφ(x)). The other images are decoded from neighbor points
of μφ(x), which are perturbed by Wz 〜N(wz|0, s2I).
The latent spaces are also visualized via t-SNE (Maaten & Hinton, 2008) in Figure 2. The dots
with different colors represent the latent vectors encoded from images of different labels (numbers),
and the pink dots are the sampling points generated from the prior p(z). As mentioned earlier,
to observe the effect of σx2 clearly, we remove batch normalization, which usually helps prevent
posterior collapse to a certain extent. As a result, the latent space with σx2 = 1.0 completely collapses
and qφ(z) approaches p(z) as shown in Figures 1 and 2(a). In this case, both KL collapse and
posterior collapse occur.
14
Under review as a conference paper at ICLR 2021
(b) log σx2 = -0.2
(c) log σx2 = -0.4
Figure 2:	Visualization of latent space via t-SNE. Pink dots are sampling points generated from the
prior p(z).
Table 5: Evaluation of various criteria for different σz2 . These criteria are the expected value of
kx0-xk22 (MSE), KL divergence, the upper bound ofMII(x0, z), the expected gap (the perturbation
variance sz2 is set to 10-2 and 10-3) and expected local smoothness (ELS).
log σz2	MSE	KL	MI	Expect 10-2	ed gap -~10-3	ELS
1.0	52.74	26.79	8.0e-3	6.20e-6	6.15e-7	3.81e-5
0.9	52.74	19.48	6.8e-3	7.19e-6	7.11e-7	4.42e-5
0.8	22.96	134.83	8.5e+1	2.03e-2	2.03e-3	1.27e-1
0.7	20.50	139.09	1.0e+2	2.37e-2	2.37e-3	1.48e-1
0.6	19.30	132.61	1.2e+2	2.87e-2	2.87e-3	1.80e-1
0.5	17.38	132.05	1.7e+2	3.44e-2	3.44e-3	2.16e-1
0.4	15.91	128.96	1.9e+2	3.94e-2	3.94e-3	2.47e-1
0.3	14.63	125.97	2.3e+2	4.50e-2	4.50e-3	2.82e-1
0.2	13.50	122.07	2.8e+2	5.16e-2	5.16e-3	3.23e-1
0.1	12.40	118.89	3.1e+2	5.83e-2	5.81e-3	3.67e-1
0.0	11.71	112.50	3.6e+2	6.79e-2	6.80e-3	4.26e-1
-0.1	10.97	107.32	4.0e+2	7.53e-2	7.55e-3	4.74e-1
-0.2	10.23	103.87	4.2e+2	8.69e-2	8.70e-3	5.46e-1
-0.3	9.63	98.48	4.6e+2	9.84e-2	9.88e-3	6.18e-1
-0.4	9.12	93.86	5.2e+2	1.12e-1	1.12e-2	7.05e-1
-0.5	8.71	88.35	5.2e+2	1.25e-1	1.26e-2	7.88e-1
-0.6	8.26	83.68	5.9e+2	1.42e-1	1.43e-2	8.94e-1
-0.7	7.82	79.70	6.6e+2	1.62e-1	1.62e-2	1.02
-0.8	7.55	74.75	7.1e+2	1.80e-1	1.80e-2	1.13
-0.9	7.26	70.63	7.3e+2	2.04e-1	2.05e-2	1.29
-1.0	7.05	66.14	7.6e+2	2.28e-1	2.30e-2	1.45
E	Fixing the posterior variance of latent space
From the previous sections, we know that σx2 affects the smoothness via σx2 . However, it would be
interesting to see what will happen if σz2 is fixed while σx2 is optimized. In this experiment, the vari-
ance parameter σz2 is fixed while σx2 is optimized with the AR-ELBO (9) under the parameterization
in Section 3. The other settings remain the same as those in Section 3.2. We evaluate the numerical
results for different σz2 with the criteria listed in Section 3.2. According to Table 5, the tendencies
of the expected gap and ELS show that a large σz2 makes the decoder smoother, which is consistent
with the discussion in Section 3.1. However, the tendency of the KL divergence is different from
that in Section 3.2. Although a larger σz2 consistently leads to a smaller MI, and eventually the MI
collapses to zero; the KL divergence still remains far from zero, which means that posterior collapse
15
Under review as a conference paper at ICLR 2021
(a) log σz2 = 1.0
(b) log σz2 = 0.6
(c) log σz2 = 0.2
Figure 3:	Visualization of latent space via t-SNE. Pink dots are sampling points generated from the
prior p(z).
can happen without KL collapse. This phenomenon can be visually confirmed by observing the
t-SNE plot in Figure 3. The cause of this phenomenon can be roughly reasoned from the linear ap-
proximated ELBO (4), in which σz2 directly affects the gradient penalty and causes oversmoothness.
It should be pointed out that the strength of L2 regularization in (4) is gradually decreased with
decreasing σx2; therefore, it does not dominate the whole objective function. As a result, the mean of
the approximated posterior qφ(z) is far from the mean of the prior p(z) (which is 0), and therefore
DKL(qφ(z) k p(z)) in (13) does not diminish to zero.
F	Proof of Theorem 3
According to Theorem 4 in Dai & Wipf (2019), we know that
limn Epdata(x)Eqφσ2(z∣x) [k* -门。(z)k2 ] =0，	^)
σx2 →0	φ,σz
which also leads to σ2 → 0 (σ2 → 0). Here, σ2 is estimated through MLE and is given by
σ2 = dj~ Epdata(x)Eqφσ2 (z|x) [llX ― μθ (Z)Il2] .	(3I)
dx	, z
To prove Theorem 3, we need the following auxiliary theorem:
Theorem 4. In the training stage of VAE, we have σZ → 0 (^22 → 0).
First, we state the two lemmas with proofs.
Lemma 5. In a VAE, I(x, x0) ≤ I(z, ze) always holds, where ze is the encoded latent variable
ze = μφ(x) with X 〜Pddata(x).
Proof. The data processing flow of the VAE is X → Ze → Z → x0; Ze = μφ(x), Z = Ze + Wz, and
x0 = μθ(z), where Wz 〜N(ez |0, σ2I). The MII(x; z, x0) can be represented as
I(X; Z, X0) = I(X; X0) + I(X; Z|X0)	(32)
= I(x; Z) + I(x; x0|Z).	(33)
Since x and x0 are conditionally independent on the given z, it follows that I(x; x0|Z) = 0. From
the non-negativity of MI, we have I(x; Z) ≥ I(x; x0). Repeating the same procedure forI(x; Ze, Z)
leads to the proof.	□
16
Under review as a conference paper at ICLR 2021
Lemma 6. The MI between x and x0 diverges to positive infinity as σx2 → 0, where x0 is obtained
from X 〜Pdata(x) as x0 = μθ (μφ(x) + Wz)∙
Proof. A lower bound of I(x; x0) is
I(x; X0) = DKL (Pdata(X)Pθ,φ(x0∣x) ∣∣ Pdata(X)Pθ,φ(x0))
=Epdata(x)Epθ,φ(χO∣x) [lnPθ,φ(XlX) - lnPθ,φ(x')]
= H [Pθ,φ(X0)] -
Epdata(x)H [pθ,φ(XIx)]
≥Η [Pθ,φ(X0)] - Epdata(X)H (σ 江),	(34)
wherePθ,φ(x0∣x) ：= Eqφ(z∣x)[Pθ(X0|z)] andpθ,φ(x0) := Epdata(χ)Eqφ(z∣χ)[pθ(XIZ)]∙ Here，we de-
note the differential entropy of the Gaussian with variance σ21 as
H(σ21) := 2ln(2πeσ2dx).	(35)
Since σ2 → 0 as σ2 → 0, H[pθ,φ(x0)] → H[pdata(x)] and H(σ2) → -∞ in the inequality of (34).
Therefore, I(x; x0) → ∞ as σ2 → 0.	□
Now we prove Theorem 3. The MI I(z; ze) satisfies
Z(Z; Ze) = DKL (qφ(ze)qσ2 (ZIZe) k qφ(ze)qφ,σ2 (Z))
= Eqφ (ze) Eqσ2 (z|ze) ln qσz2 (z|ze) - ln qφ,σz2 (z)
z
=H [qφ,σ2 (z)] - Eqφ(ze)H [qσ2 (Z|Ze)]
≤ H(Σφ,σz2) - H(σz2I)
=^ ln (det?2)),
where Σφ,σ2 denotes the variance of qφ,σ2 (Z). Invoking Lemma 5 and (36) leads to
(36)
I(x; x0) ≤ d2z ln
det(Σφ,σz2)
σz2
(37)
Now, consider σz2 6→ 0 as L → 0. According to Lemma 6, it follows that det(Σφ,σ2 ) → +∞,
which contradicts the fact that qφ,σ2 (Z) → P(Z). Thus, we must have σz2 → 0 as σx2 converges to
zero.
G	Derivation of proposed objectives
Here, we derive the objectives listed in Table 2. Consider an arbitrary Σx without any condition.
The MLE of Σχ, Σχ, can be obtained by
ς X = Epdata(X)Eqφ(z∣x) [(x - μθ (Z))(X - μθ (Z))>] .	(38)
From the partial derivative of Jrec(θ, φ, Σx) w.r.t. Σx, we have
JreC	X) = 1 (EpdataEqφ(z∣x) [(x - "θ (Z))(X - "θ (z))>] + ∑-1) .	(39)
∂Σx	2
EI » « T 1 - ɪ' √∖	1 .1	1♦ . ∙	Γ∙ .∖ 1 ∙ /'/'	.	.	. ∙	1	♦[ 1 ∙ .) CIl	♦
The MLE of ΣX and the objectives for the different parameterizations are described in the following.
G.1 ISO-I
For Iso-I, the MLE of σX2 can be given as
σ2 = d EPdata(X)Eqφ(z∣x) hkx - μθ (Z)k2] .	(40)
17
Under review as a conference paper at ICLR 2021
Substituting (40) into (7) leads to
JAR(θ,φ,σX ) = Epdata(x) ]2^2 Eqφ(z∣x)[kx - μθ (Z)k2]+ DKlSφ(ZIX) k P(Z)) + dχ ln σ2
=dχ + Epdata(X)DKL (q0(ZIX) k P(Z))
+ ^2x ln Epdata(X)Eqφ(z∣x) hkX - μθ (Z)k2 i---2x ln dx,	(41)
where the first and fourth terms are constants and thus omitted in (9).
G.2 ISO-D
First, substitute Σx = σx2 (Z)I into (10b):
Jrec(θ,φ,Σx) =
Epdata(X)Eqφ(z∣x) ["⑶ kχ - μθ(Z)k2 + dχ lnσX(Z)].	(42)
Also, we know that the MLE of σx2 (Z) is
σX (Z) = -1 kχ - μθ(Z)k2.	(43)
dx
Substituting (43) into (42) leads to the reconstruction objective of Iso-D:
Jrec(θ, φ, Σ X ) = Epdata(X)Eqφ(z∣χ) ^^	-从"⑶葭 + dx	^2 (Z)	中)
=~X^ + 2χE Epdata(X)Eqφ(z∣x) hln kx - μθ (Z)k2i----2χ ln -X.	(45)
G.3 Diag-I
First, substitute Σx = diag(σx2) into (10b):
/n √. x'' ∖ πr
Jrec(θ, φ, ςX)= Epdata(X)
x1
Σ2σ^ Eqφ(z∣X)
i=1	X,i
h(xi - μθ,i(Z))2]	+ X 2ln σX,i∙
i=1
Also, we know that the MLE of σX2,i is
σX,i = Epdata(X)Eqφ(z∣X) kxi - μθ,i(Z)) ] .
Substituting (47) into (46) leads to the reconstruction objective for Diag-I:
Jrec(θ, φ, ςX)= Epdata(X)
dx	1	dx 1
X 2σ^Eqφ(z∣X) kxi - μθ,i(Z))2]	+ X 2lnσ2,i
i=1	X,i	i=1
2χ + 2 X ln Epdata(X)Eqφ(z∣X) h(Xi - μθ,i (Z))2i .
i=1
(46)
(47)
(48)
(49)
〜
ʌ
G.4 Diag-D
First, substitute ΣX = diag(σX2 (Z)) into (10b):
方 /八，口 、 πr	πr
Jrec(θ, φ, ςX)= Epdata(X)Eqφ(z∣X)
X (2σ21(z) (Xi - μθ,i(Z))2 + 1lnσ2,i(Z))]
Also, we know that the MLE of σX2,i(Z) is
σX,i(Z) = (Xi - μθ,i(Z)).
(50)
(51)
18
Under review as a conference paper at ICLR 2021
Substituting (51) into (50) leads to the reconstruction objective for Diag-D:
Jrec(θ, φ, ςX)= Epdata(X)Eqφ(z∣x)
dx
X
i=1
(Xi -μθ,i(Z))2 + 2lnσ2,i(Z))]
-2 + 2 ^X Epdata(X)Eqφ(z∣x) hln (Xi - μθ,i(Z)) i .
i=1
(52)
(53)
H Derivation of (11)
The KL divergence terms of (1) can be represented as
Epdala(X)DKLSφ(ZIX) k Pθ (ZIX)) = Epdata(X)qφ(z∣x)[ln 9φ(ZIX) — lnPθ (ZIX)]	(54)
_ E	ι Pdata(X)qφ(ZIX)
=Epdata(X)qφ(ZIX) [ln p(Z)pθ(xIz)
and
Epdata(X)qφ(z∣X) [ln qφ(Z) — lnP(Z)] = DKL(qφ(Z) k P(Z)) .	(55)
Substituting the two equations above into (1), then L can be reformulated into (11).
I	Related works
To the best of our knowledge, Lucas et al. (2019) were among the first to suggest that posterior
collapse may be caused by a sub-optimal σx2. In the past, one of the common approaches for dealing
with posterior collapse was to anneal the weight of the KL term in the ELBO. The first such attempt
was KL annealing (Bowman et al., 2015). Bowman et al. (2015) introduced a weighting coefficient
on the KL term in the cost function during training. The weighting scheduling is determined in
advance, e.g., the weight increases monotonically (Bowman et al., 2015; S0nderby et al., 2016) or
changes cyclically (Fu et al., 2019) as the training progresses.
The weighting coefficient also appears in Higgins et al. (2017), and is interpreted as a hyperpa-
rameter that controls the information capacity of the latent space. The suggested value for such
hyperparameter is larger than 1. This is analogous to setting σ2 larger than the MLE value σ2 for (7)
and (8a), which enforces a stronger smoothness in exchange of better latent space disentanglement.
The differences between Higgins et al. (2017) and the proposed method are: (i) σx2 changes between
every minibatch; and (ii) the estimation of σx2 is aimed to prevent the oversmoothness.
Shao et al. (2020) proposed ControlVAE, which combined control theory with the VAE, and applied
PI/PID control to determine the weight on the KL term. Although applying control theory to the
weighting of the KL term makes it possible to reflect the status of the optimization, ControlVAE
needs extra hyperparameters to be tuned in advance. On the other hand, our method can be inter-
preted as automatic KL annealing that estimates σx2 through MLE without the need of tuning an
extra hyperparameter.
Ghosh et al. (2020) interpreted the stochastic autoencoder with the reparameterization trick as noise
injection process and proposed replacing such a mechanism with an explicit regularized autoencoder
(RAE). RAE regularizes its decoder in several ways: L2 regularization, a gradient penalty (Gulrajani
et al., 2017) and spectral normalization (Miyato et al., 2018). As discussed in Section 3.1, if σz2 is
sufficiently small, the ELBO can also be approximately represented as a sum of three losses (4),
which correspond to the terms included in the basic RAE objective function. The approximated
objective function (4) can be obtained when RAE with a gradient penalty (RAE-GP) is used and
tuned appropriately.
Dai & Wipf (2019) optimized σx2 using an optimizer, which is the most similar approach to our
Iso-I model in that (7) is used as an objective function. Our proposed method provides simplified
objective functions that enable the variance parameter to be optimized automatically and guarantee
that σx2 decreases as the reconstruction loss decreases, enabling the gradient penalty to be gradually
weakened.
19
Under review as a conference paper at ICLR 2021
Comparing our work with these works showed that the proposed AR-ELBO and its variations are
also capable of regularizing the Gaussian VAE via weighting of the gradient penalty. In the standard
parameterization such as (2), the second term in (4) is a weighted gradient penalty, which makes it
possible to regularize each dimension of the latent space differently according to the property of the
input data. In addition, the combination of the proposed objective functions together with standard
Gaussian VAE allows implicit gradient regularization on the decoder with lower computational cost
than that of explicitly adding the gradient penalty to the objective function, such as in RAE.
J Details of experimental setup in Section 5
In this experiment, the Adam optimizer (Kingma & Ba, 2015) is used and the maximum number of
epochs is set to 100 for MNIST and 70 for CelebA. The learning rates are 0.001 for MNIST and
0.0002 for CelebA. A minibatch size of 64 is used. All the FID2 values are evaluated with 10, 000
generated samples.
For the posterior estimation by the second-stage VAE, we adopt the same networks for the encoder
and decoder as those in Dai & Wipf (2019). For GMM fitting, we use the same settings as those in
Ghosh et al. (2020). Experimental details including the network architectures for each dataset are
described in the following.
J.1 MNIST
We construct the encoder and decoder for the MNIST dataset using the architecture in Chen et al.
(2016). The encoder is constructed as
x ∈ R28×28 → Conv64 → ReLU
→ Conv128 → ReLU → Reshape(128, 7, 7)
→ Flatten → FC1024 → BatchNorm → ReLU
→ FC16×2.
The decoder is constructed as
z ∈ R16 → FC1024 → BatchNorm → ReLU
→ FC128×7×7 → BatchNorm → ReLU
→ ConvT64 → BatchNorm → ReLU
→ ConvT1 → Sigmoid
size of (64, 14, 14)
size of (128, 7, 7)
size of (128, 7, 7)
size of (64, 14, 14)
size of (1, 28, 28).
In all the Convk layers and all the ConvTk layers except for the last, 5 × 5 convolutional filters with
stride (2, 2) are used. The difference between this architecture and those used in Appendix D.1 is
whether batch normalization is applied or not. Although in the original work of Chen et al. (2016),
the discriminator used leaky ReLU (lReLU), we adopt ReLU for the encoder part, which improves
the performance for all the models evenly.
J.2 CelebA
The CelebA images are preprocessed with center cropping of 140 × 140, then resized to 64 × 64
as described in Tolstikhin et al. (2018) and Ghosh et al. (2020). It should be noted that the size of
cropping differs among the previous works, and it markedly affects the FID score. We choose the
above cropping size as is the largest among the related works and seems to be the most difficult
case for image generation. Moreover, Tolstikhin et al. (2018) and Ghosh et al. (2020) also used
this cropping size. Similarly to in the previous section, the encoder and decoder are constructed on
the basis of the discriminator and generator for CelebA used in Chen et al. (2016). The encoder is
2We used the PyTorch version of the FID implementation from https://github.com/mseitzer/
pytorch-fid for all the models. However, the result may slightly differ from that obtained with the Tensor-
Flow implementation https://github.com/bioinf-jku/TTUR.
20
Under review as a conference paper at ICLR 2021
constructed as
x ∈ R64×64 → Conv128 → ReLU	size of (128, 32, 32)
→ Conv256 → BatchNorm → ReLU	size of (256, 16, 16)
→ Conv512 → BatchNorm → ReLU	size of (512, 8, 8)
→ Conv1024 → BatchNorm → ReLU → Flatten → FC64×2. The decoder is constructed as z ∈ R64 → FC8×8×1024	size of (1024, 4, 4)
→ ConvT512 → ReLU	size of (512, 16, 16)
→ ConvT256 → BatchNorm → ReLU	size of (256, 32, 32)
→ ConvT128 → BatchNorm → ReLU	size of (128, 64, 64)
→ ConvT3	size of (3, 64, 64).
In all the Convk layers and all the ConvTk layers except for the last, 5 × 5 convolutional filters with
stride (2, 2) are used. We use ReLU instead of leaky ReLU due to the performance consideration
described in the previous subsection. To fit the size of the input images in our experiment, one extra
convolutional layer is added for the encoder and the channel size is twice as large as that in Chen
et al. (2016),
K Examples of reconstructed and generated images in Section 5
We show examples of reconstructed images and images generated by sampling the learned approxi-
mated posterior from the proposed method and other works in Figures 4 and 5.
Figure 4: Reconstructed images and examples of images generated from the prior and the estimated
posterior on MNIST. “GT” stands for ground truth.
L	Interpolation of latent variables
This section aims to investigate the feasibility of the learned latent space of the methods mentioned
in Section 5. If high quality images can be generated by interpolating the latent variables in a latent
21
Under review as a conference paper at ICLR 2021
Reconstructions
Prior
Random samples
2nd VAE
GMM-10
GMM-100
GT
VAE (蟾=1 .0 )
WAE-MMD
AE
RAE
RAE-GP
ISo-I w/ trainable ∑a.
Diag-I w/ trainable ∑ιc
Iso-D w. trainable ∑,c
Diag-D w/ trainable EjB
Ours (Iso-I w/ AR-ELBO)
Ours (Diag-I w/ AR-ELBO)
Ours (Iso-D w/ AR-ELBO)
Ours (Diag-D w/ AR-ELBO)
唾；@ &
♦守■■
.，
位博合
* w
∙AΓ
3O⅛
Esfl
⅛aarιl
τι⅛s;
∣i≡ι
IHSl
Iyrl
EZ2ij
r . bi
l.i n I
&♦疝Sl
囹0牙隐
「切Y5
⅛ι⅞⅛ 门
B⅛K∙τ
F函
f [⅜⅛
-La
__
L 【'
取S0
必
⅜B⅛M
和懵«图
班一卜I"
I n π

L LT I
期厘啕置
Figure 5: Reconstructed images and examples of images generated from the prior and the estimated
posterior on CelebA.
space, the corresponding latent space is more likely to be feasible for other downstream tasks. There-
fore, in this section, we evaluate the FID scores of images generated by latent variable interpolation
for various models mentioned in section 5.
First, we choose 10,000 random pairs of images from both MNIST and CelebA datasets. The
interpolation is done by applying spherical interpolation (Ghosh et al., 2020) in latent spaces and
then generate the interpolated images with the decoders. In the end, we evaluate the FID of these
interpolated images.
Furthermore, the experiment has been proceeded with two different setups of mixing ratios: (i) a
fixed ratio of 0.5, i.e., the mid-point of two latent variables; and (ii) a uniformly distributed ran-
dom ratio between [0, 1] for each image pair. The result is shown in Table 4, where the proposed
method achieved the best score on MNIST and is competitive on CelebA. This suggests that a gen-
erative model with proper smoothness achieved via the proposed method is also feasible for other
applications such as interpolation and possibly applicable for other semantic controls.
22
Under review as a conference paper at ICLR 2021
23