Under review as a conference paper at ICLR 2021
Automated Concatenation of Embeddings
for Structured Prediction
Anonymous authors
Paper under double-blind review
Ab stract
Pretrained contextualized embeddings are powerful word representations for
structured prediction tasks. Recent work found that better word representations
can be obtained by concatenating different types of embeddings. However, the se-
lection of embeddings to form the best concatenated representation usually varies
depending on the task and the collection of candidate embeddings, and the ever-
increasing number of embedding types makes it a more difficult problem. In this
paper, we propose Automated Concatenation of Embeddings (ACE) to automate
the process of finding better concatenations of embeddings for structured predic-
tion tasks, based on a formulation inspired by recent progress on neural archi-
tecture search. Specifically, a controller alternately samples a concatenation of
embeddings, according to its current belief of the effectiveness of individual em-
bedding types in consideration for a task, and updates the belief based on a reward.
We follow strategies in reinforcement learning to optimize the parameters of the
controller and compute the reward based on the accuracy of a task model, which is
fed with the sampled concatenation as input and trained on a task dataset. Empir-
ical results on 6 tasks and 21 datasets show that our approach outperforms strong
baselines and achieves state-of-the-art performance with fine-tuned embeddings
in the vast majority of evaluations.
1	Introduction
Recent developments on pretrained contextualized embeddings have significantly improved the per-
formance of structured prediction tasks in natural language processing. Approaches based on con-
textualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin
et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art
for various structured prediction tasks. Concurrently, research has also showed that word represen-
tations based on the concatenation of multiple pretrained contextualized embeddings and traditional
non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings
(Santos & Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018;
Strakovd et al., 2019; He & Choi, 2020). Given the ever-increasing number of embedding learning
methods that operate on different granularities (e.g., word, subword, or character level) and with dif-
ferent model architectures, choosing the best embeddings to concatenate for a specific task becomes
non-trivial, and exploring all possible concatenations can be prohibitively demanding in computing
resources.
Neural architecture search (NAS) is an active area of research in deep learning to automatically
search for better model architectures, and has achieved state-of-the-art performance on various tasks
in computer vision, such as image classification (Real et al., 2019), semantic segmentation (Liu
et al., 2019a), and object detection (Ghiasi et al., 2019). In natural language processing, NAS has
been successfully applied to find better RNN structures (Zoph & Le, 2017; Pham et al., 2018b) and
recently better transformer structures (So et al., 2019; Zhu et al., 2020). In this paper, we propose
the Automated Concatenation of Embeddings (ACE) approach to automate the process of finding
better concatenations of embeddings for structured prediction tasks, formulated as an NAS problem.
In this approach, an iterative search process is guided by a controller based on its belief that models
the effectiveness of individual embedding candidates in consideration for a specific task. At each
step, the controller samples a concatenation of embeddings according to the belief model and feeds
the concatenated word representations as inputs to a task model, which in turn is trained on the
1
Under review as a conference paper at ICLR 2021
task dataset and returns the model accuracy as a reward signal to update the belief model. We use
the policy gradient algorithm (Williams, 1992) in reinforcement learning (Sutton & Barto, 1992)
to solve the optimization problem. In order to improve the efficiency of the search process, we
also design a special reward function by accumulating all the rewards based on the transformation
between the current concatenation and all previously sampled concatenations.
Our approach is different from previous work on NAS in the following aspects:
1.	Unlike most previous work, we focus on searching for better word representations rather than
better model architectures.
2.	We design a unique search space for the embedding concatenation search. Instead of using RNN
as in previous work of Zoph & Le (2017), we design a more straightforward controller to generate
the embedding concatenation. We design a novel reward function in the objective of optimization
to better evaluate the effectiveness of each concatenated embeddings.
3.	Our approach is efficient and practical. ACE can find a strong word representation on a sin-
gle GPU with only a few GPU-hours for structured prediction tasks, while a lot of the NAS
approaches require dozens of or even thousands of GPU-hours to search for good neural archi-
tecture.
4.	The task model from ACE achieves high accuracy without the need for retraining, while in pre-
vious work of NAS the resulting neural network usually requires retraining from scratch.
Empirical results show that ACE outperforms strong baselines. Furthermore, we show that when
ACE is applied to concatenate pretrained contextualized embeddings which are already fine-tuned
on specific tasks, we can achieve state-of-the-art or competitive accuracy on 6 structured prediction
tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose,
1988), chunking (Tjong Kim Sang & Buchholz, 2000), aspect extraction (Hu & Liu, 2004), syntactic
dependency parsing (Tesn论re, l959) and semantic dependency parsing (OePen et al., 2014) over
21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the
baselines and show the advantage of ACE over ensemble models.
2	Related Work
2.1	Embeddings
Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al.,
2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (San-
tos & Zadrozny, 2014) are trained together with the task and applied in many structured prediction
tasks (Ma & Hovy, 2016; Lample et al., 2016; Dozat & Manning, 2018). For pretrained contextu-
alized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding gen-
erated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art
approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embed-
dings, which is a kind of contextualized character embeddings and achieved strong performance in
sequence labeling tasks. Recently, Devlin et al. (2019) proposed BERT, which encodes contextual-
ized sub-word information by Transformers and significantly improves the performance on a lot of
NLP tasks. Much research such as RoBERTa (Liu et al., 2019c) has focused on improving BERT
model’s performance through stronger masking strategies. Moreover, multilingual contextualized
embeddings become popular. Pires et al. (2019) and Wu & Dredze (2019) showed that Multilin-
gual BERT (M-BERT) could learn a good multilingual representation effectively with strong cross-
lingual zero-shot transfer performance in various tasks. Conneau et al. (2020) proposed XLM-R,
which is trained on a larger multilingual corpus and significantly outperforms M-BERT on various
multilingual tasks.
2.2	Neural Architecture Search
Recent progress on deep learning has shown that network architecture design is crucial to the model
performance. However, designing a strong neural architecture for each task requires enormous ef-
forts, high level of knowledge, and experiences over the task domain. Therefore, automatic de-
sign of neural architecture is desired. A crucial part of NAS is search space design, which defines
2
Under review as a conference paper at ICLR 2021
the discoverable NAS space. Previous work (Baker et al., 2017; Zoph & Le, 2017; Xie & Yuille,
2017) designs a global search space (Elsken et al., 2019) which incorporates structures from hand-
crafted architectures. For example, Zoph & Le (2017) designed a chained-structured search space
with skip connections. The global search space usually has a considerable degree of freedom. As
an example, the approach of Zoph & Le (2017) takes 22,400 GPU-hours to search on CIFAR-10
dataset. Based on the observation that existing hand-crafted architectures contain repeated structures
(Szegedy et al., 2016; He et al., 2016; Huang et al., 2017), Zoph et al. (2018) explored cell-based
search space which can reduce the search time to 2,000 GPU-hours.
In recent NAS research, reinforcement learning and evolutionary algorithms are the most usual ap-
proaches. In reinforcement learning, the agent’s actions are the generation of neural architectures
and the action space is identical to the search space. Previous work usually applies an RNN layer
(Zoph & Le, 2017; Zhong et al., 2018; Zoph et al., 2018) or use Markov Decision Process (Baker
et al., 2017) to decide the hyper-parameter of each structure and decide the input order of each struc-
ture. Evolutionary algorithms have been applied to architecture search for many decades (Miller
et al., 1989; Angeline et al., 1994; Stanley & Miikkulainen, 2002; Floreano et al., 2008; Jozefowicz
et al., 2015). The algorithm repeatedly generates new populations through recombination and mu-
tation operations and selects survivors through competing among the population. Recent work with
evolutionary algorithms differ in the method on parent/survivor selection and population generation.
For example, Real et al. (2017), Liu et al. (2018a), Wistuba (2018) and Real et al. (2019) applied
tournament selection (Goldberg & Deb, 1991) for the parent selection while Xie & Yuille (2017)
keeps all parents. Suganuma et al. (2017) and Elsken et al. (2018) chose the best model while Real
et al. (2019) chose several latest models as survivors.
3	Automated Concatenation of Embeddings
In ACE, a task model and a controller interact with each other repeatedly. The task model pre-
dicts the task output, while the controller searches for better embedding concatenation as the word
representation for the task model to achieve higher accuracy. Given an embedding concatenation
generated from the controller, the task model is trained over the task data and returns a reward to the
controller. The controller receives the reward to update its parameter and samples a new embedding
concatenation for the task model. Figure 1 shows the general architecture of our approach.
3.1	Task Model
For the tasks model, we emphasis on sequence-structured and graph-structured outputs. Given a
structured prediction task with input sentence x and structured output y, we can calculate the prob-
ability distribution P (y|x) by:
P( ∣χ) =	exp(Score(x, y))
yX 一 pyo∈γ(x) exP(SCOre(X, yO))
where Y(x) represents all possible output structures given the input sentence x. Depending on
different struCtured prediCtion tasks, the output struCture y Can be label sequenCes, trees, graphs
or other struCtures. In this paper, we use sequenCe-struCtured and graph-struCtured outputs as two
exemplar struCtured prediCtion tasks. We use BiLSTM-CRF model (Ma & Hovy, 2016; Lample
et al., 2016) for sequenCe-struCtured outputs and use BiLSTM-Biaffine model (Dozat & Manning,
2017) for graph-struCtured outputs:
P seq(y|x) = BiLSTM-CRF(V, y); Pgraph(y|x) = BiLSTM-Biaffine(V, y)
where V = [vi； •…;Vn], V ∈ Rd×n is a matrix of the word representations for the input sentence
x with n words, d is the hidden size of the ConCatenation of all embeddings. The word representation
vi of i-th word is a concatenation of L types of word embeddings:
vil = embedli(x); vi = [vi1;vi2; . . .;viL]
where embedl is the model of l-th embeddings, vi ∈ Rd, vil ∈ Rdl . dl is the hidden size of embedl .
3
Under review as a conference paper at ICLR 2021
一一{P」EM①巴
Controller
Task Model
Figure 1: The main paradigm of our approach is shown in the middle, where an example of reward
function is represented in the left and an example of a concatenation action is shown in the right.
3.2	Search Space Design
The neural architecture search space can be represented as a set of neural networks (Elsken et al.,
2019). A neural network can be represented as a directed acyclic graph with a set of nodes and
directed edges. Each node represents an operation, while each edge represents the inputs and outputs
between these nodes. In ACE, we represent each embedding candidate as a node. The input to the
nodes is the input sentence x, and the outputs are the embeddings vl . Since we concatenate the
embeddings as the word representation of the task model, there is no connection between nodes
in our search space. Without considering the connections between nodes, the search space can be
significantly reduced. For each node, there are a lot of options to extract word features. Taking
BERT embeddings as an example, Devlin et al. (2019) concatenated the last four layers as word
features while Kondratyuk & Straka (2019) applied a weighted sum of all twelve layers. However,
the empirical results (Devlin et al., 2019) do not show a significant difference in accuracy. We follow
the typical usage for each embedding to further reduce the search space. As a result, each embedding
only has a fixed operation and the resulting search space contains 2L-1 possible combinations of
nodes.
In NAS, weight sharing (Pham et al., 2018a) shares the weight of structures in training different
neural architectures to reduce the training cost. In comparison, we fixed the weight of pretrained
embedding candidates in ACE except for the character embeddings. Instead of sharing the parame-
ters of the embeddings, we share the parameters of the task models at each step of search. However,
the hidden size of word representation varies over the concatenations, making the weight sharing of
structured prediction models difficult. Instead of deciding whether each node exists in the graph,
we keep all nodes in the search space and add an additional operation for each node to indicate
whether the embedding is masked out. To represent the selected concatenation, we use a binary
vector a = [aι,…，aι,…，。工]as an mask to mask out the embeddings which are not selected:
vi = [vi1a1; . . . ;vilal; . . . ;viLaL]	(1)
where al is a binary variable. Since the input V is applied to a linear layer in the BiLSTM layer,
multiplying the mask with the embeddings is equivalent to directly concatenating the selected em-
beddings:
L
W = [W1;W2; . . . ;WL]; W>vi = XWl>vilal; W ∈ Rd×h and Wl ∈ Rdl×h	(2)
l=1
Therefore, the model weights can be shared after applying the embedding mask to all embedding
candidates’ concatenation. Another benefit of our search space design is that we can remove the
unused embedding candidates and the corresponding weights in W for a lighter task model after the
best concatenation is found by ACE.
3.3	Searching in the Space
During search, the controller generates the embedding mask for the task model iteratively. We use
parameters θ = [θ1; θ2; . . . ; θL] for the controller instead of the RNN structure applied in previous
approaches (Zoph & Le, 2017; Zoph et al., 2018). The probability distribution of selecting an
concatenation a is Pctrl(a; θ) = QlL=1 Plctrl(al; θl). Each element al of a is sampled independently
from a Bernoulli distribution, which is defined as:
Plctrl(al = 1; θl) = σ(θl); Plctrl(al = 0; θl) = 1 - Plctrl(al = 1; θl)	(3)
4
Under review as a conference paper at ICLR 2021
where σ is the sigmoid function. Given the mask, the task model is trained until convergence and
returns an accuracy R on the development set. As the accuracy cannot be back-propagated to the
controller, we use the reinforcement algorithm for optimization. The accuracy R is used as the
reward signal to train the controller. The controller’s target is maximizing the expected reward
J(θ) = EPctri(a；e)[R] through the policy gradient method (Williams, 1992). In our approach, since
calculating the exact expectation is intractable, the gradient of J(θ) is approximated by sampling
only one selection following the distribution P ctrl (a; θ) at each step for training efficiency:
L
Vθ J(θ) ≈ X Vθ logPictrl(al； Θ1 )(R - b)	(4)
l=1
where b is the baseline function to reduce the high variance of the update function. The baseline
usually can be the highest accuracy during the search process. Instead of merely using the highest
accuracy of development set over the search process as the baseline, we design a reward function on
how each embedding candidate contributes to accuracy change by utilizing all searched concatena-
tions’ development scores. We use a binary vector |at - ai | to represent the change between current
embedding concatenation at at current time step t and ai at previous time step i. We then define the
reward function as:
t-1
rt = X(Rt - Ri)|at - ai|	(5)
i=1
where rt is a vector with length L representing the reward of each embedding candidate. Rt
and Ri are the reward at time step t and i. When the Hamming distance of two concatenations
Hamm(at, ai) gets larger, the changed candidates’ contribution to the accuracy becomes less no-
ticeable. The controller may be misled to reward a candidate that is not very helpful. We apply
a discount factor to reduce the reward for two concatenations with a large Hamming distance to
alleviate this issue. Our final reward function is:
t-1
rt = X(Rt - Ri)YHamm(Qt,Qi)TIat - ai|	(6)
i=1
where γ ∈ (0, 1). Eq. 4 is then reformulated as:
L
Vθ Jt(θ) ≈ X Vθ logPlCtri(at； θi)rt	⑺
l=1
3.4	Training
To train the controller, we use a dictionary D to store the concatenations and validation scores. At
t = 1, we train the task model with all embedding candidates concatenated. From t = 2, we repeat
the following steps until a maximum iteration T :
•	Sample a concatenation at based on the probability distribution in Eq. 3.
•	Train the task model with at following Eq. 1 and evaluate the model on the development
set to get the accuracy Rt .
•	Given the concatenation at, accuracy Rt and D, compute the gradient of the controller
following Eq. 7and update the parameters of controller.
•	Add at and Rt into D, set t = t + 1.
When sampling at, we avoid selecting the previous concatenation at-1 and the all-zero vector (i.e.,
selecting no embedding). If at is in the dictionary D, we compare the Rt with the value in the
dictionary and keep the highest one.
4	Experiments
4.1	Datasets and Configurations
To show ACE’s effectiveness, we conduct extensive experiments ona variety of structured prediction
tasks varying from syntactic tasks to semantic tasks.The tasks are named entity recognition (NER),
5
Under review as a conference paper at ICLR 2021
Part-Of-Speech (POS) tagging, Chunking, Aspect Extraction (AE), Syntactic Dependency Parsing
(DP) and Semantic Dependency Parsing (SDP). The details of the tasks are in Appendix A.1.
We train the controller for 30 steps and save the task model with the highest accuracy on the devel-
opment set as the final model for testing. For all experiments, we report the averaged accuracy of 3
runs. For other settings, please refer to Appendix A.3 for more details.
4.2	Embeddings
Basic Settings: For the candidates of embeddings on English datasets, we use the language-
specific model for ELMo, Flair, base BERT, GloVe word embeddings, fastText word embeddings,
non-contextual character embeddings (Lample et al., 2016), multilingual Flair (M-Flair), M-BERT
and XLM-R embeddings. The size of the search space in our experiments is 211 - 1 = 20471. For
language-specific models of other languages, please refer to Section A.4 for more details. In AE,
there is no available language-specific BERT, Flair and ELMo embeddings for Russian and there is
no available language-specific Flair and ELMo embeddings for Turkish. We use the corresponding
English embeddings instead so that the search spaces of these datasets are almost identical to those
of the other datasets. All embeddings are fixed during training except that the character embeddings
are trained over the task. The empirical results are reported in Section 4.3.1.
Embedding Fine-tuning: Fine-tuning transformer-based embeddings is a usual approach to get
better accuracy. In sequence labeling, most of the work follows the fine-tuning pipeline of BERT that
connects the BERT model with a linear layer for word-level classification. However, when multiple
embeddings are concatenated, fine-tuning a specific group of embeddings becomes difficult. It is
impractical to train multiple embeddings because of complicated hyper-parameter settings and mas-
sive GPU memory consumption. To alleviate this problem, we first fine-tune the transformer-based
embeddings with the task and then concatenate these embeddings together with other embeddings
in the basic setting to apply ACE. The empirical results are reported in Section 4.3.2.
Table 1: Comparison with concatenating all embeddings and random search baselines on 6 tasks.
	NER		POS			AE		
	de en es nl		Ritter ARK TB-v2		14Lap 14Res 15Res 16Res es nl ru	tr
All	83.1 92.4 88.9 89.8		90.6 92.1^^946^		-8Σ7^^88.5^^742^^73.2 74.6 75.0 67.1	67.5
Random	84.0 92.6 88.8 91.9		91.3 92.6 94.6		83.6	88.1	73.5	74.7 75.0 73.6 68.0 70.0	
ACE	84.2 93.0 88.9 92.1		91.7 92.8 94.8		83.9	88.6	74.9	75.6 75.7 75.3 70.6 71.1	
	-Chunk-	-DP-		SDP		Avg
	CoNLL 2000	UAS LAS		DM-ID DM-OOD PAS-ID PAS-OOD PSD-ID PSD-OOD		
All	96.7	96.7 95.1		-94.3	908	946	929	824	817-	^85Γ
Random	96.7	96.8 95.2		94.4	90.8	94.6	93.0	82.3	81.8	85.7
ACE	96.8	96.9 95.3		94.5	90.9	94.5	93.1	82.5	82.1	86.2
4.3	Results
We use the following abbreviations in our experiments: UAS: Unlabeled Attachment Score; LAS:
Labeled Attachment Score; ID: In-domain test set; OOD: Out-of-domain test set; F & G (2019):
Femgndez-Gonzdlez & GGmez-Rodriguez (2019); F & G (2020): Femgndez-Gonzdlez & G6mez-
RodrigUez (2020); D & M (2018): Dozat & Manning (2018). In all tables, We use ISO 639-1
language codes to represent each language.
4.3.1	Comparison With Baselines
To shoW the effectiveness of our approach, We compare our approach With tWo strong baselines.
For the first one, We let the task model learn by itself the contribution of each embedding candidate
that is helpful to the task. We set a to all-ones (i.e., the concatenation of all the embeddings) and
train the task model (All). The linear layer Weight W in Eq. 2 reflects the contribution of each
candidate. For the second one, We use the random search (Random), a strong baseline in NAS (Li &
1Flair embeddings have tWo models (forWard and backWard) for each language.
6
Under review as a conference paper at ICLR 2021
Table 2: Comparison with state-of-the-art approaches in NER and POS tagging. *: Models are
trained on both train and development set. t Models are trained with document information. ◊:
Results are from Conneau et al. (2020).
		NER								POS			
	de	deo6	en	es	nl		Ritter	ARK	TB-v2
Akbik etal. (2018)t-	-	88.3	93.1	-	-	Owoputi et al. (2013)	90.4	93.2	94.6
Baevski et al. (2019)	-	-	93.5	-	-	Gui et al. (2017)	90.9	-	92.8
StrakOvA et al.(2019),	85.1	-	93.4	88.8	92.7	Gui et al. (2018)	91.2	92.4	-
Yu et al.(2020产	86.4	9o.3	93.5	9o.3	93.7	Nguyen et al. (2020)	90.1	94.1	95.2
XLM-R+Fine-tune	^851^	-	92.9	89.7	^9ΣT	XLM-R+Fine-tune-	93.0	93.4	95.0
ACE+Fine-tune	87.0	90.5	93.5	91.7	94.6	ACE+Fine-tune	93.4	93.8	95.6
Table 3: Comparison with state-of-the-art approaches in chunking and aspect extraction. *: We
report the results reproduced by Wei et al. (2020).
	Chunk			AE	
	CoNLL 2000		14Lap 14Res 15Res 16Res es nl ru tr
Akbik etal. (2018)	967	Xu etal. (2018)t	84.2^^84.6 72.0 75.4^^-	-	-
Clarketal. (2018)	97.0	Xu etal. (2019)	84.3	-	-	78.0	-	-	-	-
Liu etal. (2019b)	97.3	Wang et al. (2020a)	-	-	-	72.8 74.3 72.9 71.8 59.3
Wang et al. (2020b)	-	Wei et al. (2020)	82.7 87.1 72.7 77.7	-	-	-	-
XLM-R+Fine-tune	96.5	XLM-R+Fine-tune	^8T3^^88.4 77.3^^78.5 77.8 72.1 75.7 66.7
ACE+Fine-tune	97.0	ACE+Fine-tune	85.0 89.8 78.5 81.2 78.8 76.7 76.7 77.7
Talwalkar, 2020). For Random, we run the same maximum iteration as in ACE. Table 1 shows that
ACE outperforms both baselines in 6 tasks over 23 test sets with only two exceptions. Comparing
Random with All, Random outperforms All by 0.4 on average and surpasses the accuracy of
All on 14 out of 23 test sets, which shows that concatenating all embeddings may not be the best
solution to most structured prediction tasks. In general, searching for the concatenation for the word
representation is essential in most cases, and our search design can usually lead to better results
compared to both of the baselines.
4.3.2	Comparison With State-of-the-Art approaches
As we have shown, ACE has an advantage in searching for better embedding concatenations. We
further show that ACE is competitive or even stronger than state-of-the-art approaches. In some
tasks, We have several additional settings to better compare with previous work. In NER, we also
conduct a comparison on the revised version of German datasets in the CoNLL 2006 shared task
(Buchholz & Marsi, 2006). In parsing tasks, we use XLNet (Yang et al., 2019), which is significantly
stronger than BERT in DP (Zhou & Zhao, 2019). In SDP, the state-of-the-art approaches used POS
tags and lemmas as additional word features to the network. We add these two features to the
embedding candidates and train the embeddings together with the task. We use the fine-tuned BERT
embeddings, XLM-R embeddings, and XLNet embeddings on each task instead of the pretrained
version of these embeddings as the candidates. For the NER tasks, we use the XLM-R models
fine-tuned by Hugging Face instead.2
We additionally compare with fine-tuned XLM-R model for NER, POS tagging, chunking and AE,
and compare with fine-tuned XLNet model for DP and SDP, which are strong fine-tuned models in
most of the experiments. Results are shown in Table 2, 3, 4. Results show that ACE with fine-tuned
embeddings achieves state-of-the-art performance in 22 out of 24 test sets and is competitive with
the state-of-the-art approaches in the other 2 test sets. Our approach is competitive or even stronger
than the approaches using additional information in NER and DP, which shows that finding a good
word representation helps structured prediction tasks. We also show that ACE is stronger than the
fine-tuned models, which shows the effectiveness of concatenating the fine-tuned embeddings.
2Please refer to Appendix A.4 and B.1 for more details.
7
Under review as a conference paper at ICLR 2021
Table 4: Comparison with state-of-the-art approaches in DP and SDP. 1: For reference, they addi-
tionally used constituency dependencies in training. t For reference, We confirmed with the authors
of He & Choi (2020) that they used a different data pre-processing script with previous work.
	DP				Sdp						
	-PTB			DM		PAS		PSD	
	UAS	LAS		ID	OOD	ID	OOD	ID	OOD
Zhou&Zhao (2019)才	97.2	95.7	He & Choi (2020)*	94.6	90.8	96.1	94.4	86.8	79.5
F&G(2019)	96.0	94.4	D&M(2018)	940Γ	89.7	94.1	91.3	81.4	79.6
He & Choi (2020)	96.8	95.3	Wang etal. (2019)	93.7	88.9	93.9	90.6	81.0	79.4
Zhang et al. (2020)	96.1	94.5	F&G (2020)	94.4	91.0	95.1	93.4	82.6	82.0
XLNet+Fine-tune	97.0	95.4	XLNet+Fine-tune	^949^	92.0	94.8	93.4	82.6	82.2
ACE+Fine-tune	97.2	95.7	ACE+Fine-tune	95.3	92.6	95.3	93.9	83.6	83.2
Figure 2: Comparing the efficiency of random search (Random) and ACE. The x-axis is the number
of time steps. The left y-axis is the averaged best validation accuracy on CoNLL English NER
dataset. The right y-axis is the averaged validation accuracy of the current selection.
5	Analysis
5.1	Efficiency of Search Methods
To show how efficient our approach is compared with the random search algorithm, we compare the
algorithm in two aspects on CoNLL English NER dataset. The first aspect is the best development
accuracy during training. The left part of Figure 2 shows that ACE is consistently stronger than the
random search algorithm in this task. The second aspect is the searched concatenation at each time
step. The right part of Figure 2 shows that the accuracy of ACE gradually increases and gets stable
when more concatenations are sampled.
5.2	Ablation S tudy on Reward Function Design
To show the effectiveness of the designed reward function, we
compare our reward function (Eq. 6) with the reward function
without discount factor (Eq. 5) and the traditional reward func-
tion (reward term in Eq. 4). We sample 2000 training sentences
on CoNLL English NER dataset for faster training and train the
controller for 50 steps. Table 5 shows that both the discount fac-
tor and the binary vector |at - ai| for the task are helpful in both
development and test datasets.
Table 5: Comparison of reward
functions.
	DEV	Test
"ACE	93:18	90.00
No discount (Eq. 5)	92.98	89.90
SimPIe (Eq. 4)	92.89	89.82
5.3	Comparison with Additional Baselines
We compare ACE with two more approaches to further show the effectiveness of ACE. One is a vari-
ant of All, which uses a weighting parameter b = [bi,…，bι,… 也]passing through a sigmoid
function to weight each embedding candidate. Such an approach can explicitly learn the weight of
each embedding in training instead of a binary mask. We call this approach All+Weight. Another
one is model ensemble, which trains the task model with each embedding candidate individually and
uses the trained models to make joint prediction on the test set. We use voting for ensemble as it is
simple and fast. For sequence labeling tasks, the models vote for the predicted label at each position.
For DP, the models vote for the tree of each sentence. For SDP, the models vote for each potential
8
Under review as a conference paper at ICLR 2021
Table 6: A comparison among All, Random, ACE, Table 7: Results of models with document
All+Weight and Ensemble. CHK: chunking. context on NER. +sent/+doc: models with
sentence-/document-level embeddings.
	—―			 DP	SDP NT 17» DCa Λ 17 Γ,HV	ιyr	Qly工			
	UAS LAS ID OOD	de de06 en es nl
All	92.4 90.6 73.2 96.7 96.7 95.1 94.3 90.8 All+sent	86.8 90.1 93.3 90.0 94.4
Random	92.6 91.3 74.7 96.7 96.8 95.2 94.4 90.8 ACE+sent	87.1 90.5 93.6 92.4 94.6
ACE	93.0 91.7 75.6 96.8 96.9 95.3 94.5 90.9 BERT(2019)	-	- 92.8 -
All+Weight	92.7 90.4 73.7 96.7 96.7 95.1 94.3 90.7 Akbiketal.(2019)	-88.3 93.2 - 90.4
Ensemble	92.2 90.6 68.1 96.5 96.1 94.3 94.1 90.3 Yuetal.(2020)	86.4 90.3 93.5 90.3 93.7
Ensembledev	92.2 90.8 70.2 96.7 96.8 95.2 94.3 90.7 All+doc	87.6 91.0 93.5 93.3 93.7
EnsembleteSt	92.7 91.4 73.9 96.7 96.8 95.2 94.4 9W ACE+doc		88.0 91.4 94.1 95.6 95.5
labeled arc. We use the confidence of model predictions to break ties if there are more than one
agreement with the same counts. We call this approach Ensemble. One of the benefits of voting is
that it combines the predictions of the task models efficiently without any training process. We can
search all possible 2L-1 model ensembles in a short period of time through caching the outputs of
the models. Therefore, we search for the best ensemble of models on the development set and then
evaluate the best ensemble on the test set (Ensembledev). Moreover, we additionally search for
the best ensemble on the test set for reference (Ensembletest), which is the upper bound of the
approach. We use the same setting as in Section 4.3.1 and select one of the datasets from each task.
For NER, POS tagging, AE, and SDP, we use CoNLL 2003 English, Ritter, 16Res, and DM datasets,
respectively. The results are shown in Table 6. Empirical results show that ACE outperforms all the
settings of these approaches and even Ensembletest, which shows the effectiveness of ACE and
the limitation of ensemble models. All, All+Weight and Ensembledev are competitive in most
of the cases and there is no clear winner of these approaches on all the datasets. These results show
the strength of embedding concatenation. Concatenating the embeddings incorporates information
from all the embeddings and forms stronger word representations for the task model, while in model
ensemble, it is difficult for the individual task models to affect each other through ensemble.
5.4	ACE with Document-Level Representations
Recently, models with document-level word representations extracted from transformer-based em-
beddings significantly outperform models with sentence-level word representations in NER (Devlin
et al., 2019; Yu et al., 2020). To show the effectiveness of ACE with document-level representations,
we replace the sentence-level word representations from transformer-based embeddings (i.e., XLM-
R and BERT embeddings) with the document-level word representations. We generate document-
level representations following Yu et al. (2020). Results are shown in Table 7. We report the test
results of All to show how the gap between ACE and All changes with different kinds of represen-
tations. We report the test accuracy of the models with the highest development accuracy following
Yu et al. (2020) for a fair comparison. Empirical results show that the document-level representa-
tions can significantly improve the accuracy of ACE. Comparing with models with sentence-level
representations, the averaged accuracy gap between ACE and All is enhanced from 0.7 to 1.1 with
document-level representations, which shows that the advantage of ACE becomes stronger with
document-level representations.
6	Conclusion
In this paper, we propose the Automated Concatenation of Embeddings, which automatically
searches for better embedding concatenation for structured prediction tasks. We design a simple
search space and use the reinforcement learning with a novel reward function to efficiently guide the
controller to search for better embedding concatenations. We take the change of embedding con-
catenations into the reward function design and show that our new reward function is stronger than
the simpler ones. Results show that ACE outperforms strong baselines. Together with fine-tuned
embeddings, ACE achieves state-of-the-art performance in 6 tasks over 19 out of 21 datasets.
9
Under review as a conference paper at ICLR 2021
References
Alan Akbik, Duncan Blythe, and Roland Vollgraf. Contextual string embeddings for sequence la-
beling. In Proceedings of the 27th International Conference on Computational Linguistics, pp.
1638-1649, Santa Fe, New Mexico, USA, August 2018. Association for Computational LingUis-
tics. URL https://www.aclweb.org/anthology/C18- 1139.
Alan Akbik, Tanja Bergmann, and Roland Vollgraf. Pooled contextualized embeddings for named
entity recognition. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 724-728, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics. doi: 10.18653/v1/N19-1078. URL https://www.aclweb.org/anthology/
N19-1078.
Peter J Angeline, Gregory M Saunders, and Jordan B Pollack. An evolutionary algorithm that
constructs recurrent neural networks. IEEE transactions on Neural Networks, 5(1):54-65, 1994.
Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, and Michael Auli. Cloze-driven
pretraining of self-attention networks. In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the 9th International Joint Conference on Nat-
ural Language Processing (EMNLP-IJCNLP), pp. 5360-5369, Hong Kong, China, November
2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1539. URL https:
//www.aclweb.org/anthology/D19-1539.
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architec-
tures using reinforcement learning. In ICLR, 2017.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with
subword information. Transactions of the Association for Computational Linguistics, 5:135-146,
2017. ISSN 2307-387X.
Sabine Buchholz and Erwin Marsi. CoNLL-x shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL-
X), pp. 149-164, New York City, June 2006. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/W06-2920.
Kevin Clark, Minh-Thang Luong, Christopher D. Manning, and Quoc Le. Semi-supervised se-
quence modeling with cross-view training. In Proceedings of the 2018 Conference on Empir-
ical Methods in Natural Language Processing, pp. 1914-1925, Brussels, Belgium, October-
November 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1217. URL
https://www.aclweb.org/anthology/D18-1217.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek,
Francisco Guzmdn, Edouard Grave, Myle Ott, LUke ZettIemoyer, and Veselin Stoyanov. Un-
supervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pp. 8440-8451, Online, July 2020.
Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL https:
//www.aclweb.org/anthology/2020.acl-main.747.
Steven J. DeRose. Grammatical category disambiguation by statistical optimization. Computational
Linguistics, 14(1), 1988. URL https://www.aclweb.org/anthology/J88- 1003.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep
bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of
the North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186, Minneapolis, Minnesota, June
2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https:
//www.aclweb.org/anthology/N19-1423.
Timothy Dozat and Christopher D Manning. Deep biaffine attention for neural dependency parsing.
In International Conference on Learning Representations, 2017.
10
Under review as a conference paper at ICLR 2021
Timothy Dozat and Christopher D. Manning. Simpler but more accurate semantic dependency
parsing. In Proceedings of the 56th Annual Meeting of the Association for Computational Lin-
guistics (Volume 2: Short Papers), pp. 484-490, Melbourne, Australia, July 2θl8. Association
for Computational Linguistics. doi: 10.18653/v1/P18-2077. URL https://www.aclweb.
org/anthology/P18-2077.
Thomas Elsken, Jan-Hendrik Metzen, and Frank Hutter. Simple and efficient architecture search for
convolutional neural networks. In ICLR workshop, 2018.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
Journal of Machine Learning Research, 20:1-21, 2019.
Daniel Femgndez-GonzdIez and Carlos G6mez-Rodriguez. Left-to-right dependency parsing with
pointer networks. In Proceedings of the 2019 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers), pp. 710-716, Minneapolis, Minnesota, June 2019. Association for Computational
Linguistics. doi: 10.18653/v1/N19-1076. URL https://www.aclweb.org/anthology/
N19-1076.
Daniel Ferndndez-Gonzdlez and Carlos G6mez-Rodriguez. Transition-based semantic dependency
parsing with pointer networks. In Proceedings of the 58th Annual Meeting of the Association
for Computational Linguistics, pp. 7035-7046, Online, July 2020. Association for Computa-
tional Linguistics. doi: 10.18653/v1/2020.acl-main.629. URL https://www.aclweb.org/
anthology/2020.acl-main.629.
Dario Floreano, Peter Durr, and Claudio Mattiussi. Neuroevolution: from architectures to learning.
Evolutionary intelligence, 1(1):47-62, 2008.
Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid architec-
ture for object detection. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 7036-7045, 2019.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein,
Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. Part-of-speech tagging
for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pp. 42-47,
Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL https:
//www.aclweb.org/anthology/P11-2008.
David E Goldberg and Kalyanmoy Deb. A comparative analysis of selection schemes used in genetic
algorithms. In Foundations of genetic algorithms, volume 1, pp. 69-93. Elsevier, 1991.
Tao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and Xuanjing Huang. Part-of-speech tagging
for twitter with adversarial neural networks. In Proceedings of the 2017 Conference on Empirical
Methods in Natural Language Processing, pp. 2411-2420, Copenhagen, Denmark, September
2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1256. URL https:
//www.aclweb.org/anthology/D17-1256.
Tao Gui, Qi Zhang, Jingjing Gong, Minlong Peng, Di Liang, Keyu Ding, and Xuanjing Huang.
Transferring from formal newswire domain with hypernet for twitter POS tagging. In Proceedings
of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2540-2549,
Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi:
10.18653/v1/D18-1275. URL https://www.aclweb.org/anthology/D18- 1275.
Han He and Jinho Choi. Establishing strong baselines for the new decade: Sequence tagging,
syntactic and semantic parsing with bert. In The Thirty-Third International Flairs Conference,
2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
11
Under review as a conference paper at ICLR 2021
Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the
Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
KDD ,04, pp. 168-177, New York, NY, USA, 2004. Association for Computing Machinery.
ISBN 1581138881. doi: 10.1145/1014052.1014073. URL https://doi.org/10.1145/
1014052.1014073.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of recurrent
network architectures. In International conference on machine learning, pp. 2342-2350, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Dan Kondratyuk and Milan Straka. 75 languages, 1 model: Parsing Universal Dependencies univer-
sally. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Pro-
cessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pp. 2779-2795, Hong Kong, China, November 2019. Association for Computational
Linguistics. doi: 10.18653/v1/D19-1279. URL https://www.aclweb.org/anthology/
D19-1279.
Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer.
Neural architectures for named entity recognition. In Proceedings of the 2016 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, pp. 260-270, San Diego, California, June 2016. Association for Computational
Linguistics. doi: 10.18653/v1/N16-1030. URL https://www.aclweb.org/anthology/
N16-1030.
Liam Li and Ameet Talwalkar. Random search and reproducibility for neural architecture search. In
Uncertainty in Artificial Intelligence, pp. 367-377. PMLR, 2020.
Xin Li, Lidong Bing, Wenxuan Zhang, and Wai Lam. Exploiting BERT for end-to-end aspect-
based sentiment analysis. In Proceedings of the 5th Workshop on Noisy User-generated Text
(W-NUT 2019), pp. 34-41, Hong Kong, China, November 2019. Association for Computational
Linguistics. doi: 10.18653/v1/D19-5505. URL https://www.aclweb.org/anthology/
D19-5505.
Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig Adam, Wei Hua, Alan L Yuille, and Li Fei-
Fei. Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 82-92,
2019a.
Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu. Hier-
archical representations for efficient architecture search. In International Conference on Learning
Representations, 2018a.
Yijia Liu, Yi Zhu, Wanxiang Che, Bing Qin, Nathan Schneider, and Noah A. Smith. Parsing tweets
into universal dependencies. In Proceedings of the 2018 Conference of the North American Chap-
ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1
(Long Papers), pp. 965-975, New Orleans, Louisiana, June 2018b. Association for Computational
Linguistics. doi: 10.18653/v1/N18-1088. URL https://www.aclweb.org/anthology/
N18-1088.
Yijin Liu, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen, and Jie Zhou. GCDT: A
global context enhanced deep transition architecture for sequence labeling. In Proceedings of
the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2431-2441, Flo-
rence, Italy, July 2019b. Association for Computational Linguistics. doi: 10.18653/v1/P19-1233.
URL https://www.aclweb.org/anthology/P19- 1233.
12
Under review as a conference paper at ICLR 2021
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019c.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-
ence on Learning Representations, 2018.
Xuezhe Ma and Eduard Hovy. End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF.
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 1064-1074, Berlin, Germany, August 2016. Association for Com-
putational Linguistics. doi: 10.18653/v1/P16-1101. URL https://www.aclweb.org/
anthology/P16-1101.
Xuezhe Ma, Zecong Hu, Jingzhou Liu, Nanyun Peng, Graham Neubig, and Eduard Hovy. Stack-
pointer networks for dependency parsing. In Proceedings of the 56th Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pp. 1403-1414, Melbourne,
Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1130.
URL https://www.aclweb.org/anthology/P18- 1130.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajic. Non-Projective dependency pars-
ing using spanning tree algorithms. In Proceedings of Human Language Technology Conference
and Conference on Empirical Methods in Natural Language Processing, pp. 523-530, Vancou-
ver, British Columbia, Canada, October 2005. Association for Computational Linguistics. URL
https://www.aclweb.org/anthology/H05-1066.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-
tations of words and phrases and their compositionality. In Advances in neural information pro-
cessing systems, pp. 3111-3119, 2013.
Geoffrey Miller, Peter Todd, and Shailesh Hegde. Designing neural networks using genetic algo-
rithms. In 3rd International Conference on Genetic Algorithms, pp. 379-384, 01 1989.
Dat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. BERTweet: A pre-trained language model
for English Tweets. In Proceedings of the 2020 Conference on Empirical Methods in Natural
Language Processing: System Demonstrations, 2020.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Dan Flickinger, Jan Hajic, An-
gelina Ivanova, and Yi Zhang. Semeval 2014 task 8: Broad-coverage semantic dependency pars-
ing. SemEval 2014, 2014.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Silvie Cinkovd, Dan Flickinger,
Jan Hajic, and Zdenka Uresova. Semeval 2015 task 18: Broad-coverage semantic dependency
parsing. In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval
2015), pp. 915-926, 2015.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A.
Smith. Improved part-of-speech tagging for online conversational text with word clusters. In
Proceedings of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pp. 380-390, Atlanta, Georgia,
June 2013. Association for Computational Linguistics. URL https://www.aclweb.org/
anthology/N13-1039.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 conference on empirical methods in natural language
processing (EMNLP), pp. 1532-1543, 2014.
Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the 2018 Con-
ference of the North American Chapter of the Association for Computational Linguistics: Hu-
man Language Technologies, Volume 1 (Long Papers), pp. 2227-2237, New Orleans, Louisiana,
June 2018. Association for Computational Linguistics. doi: 10.18653/v1/N18-1202. URL
https://www.aclweb.org/anthology/N18-1202.
13
Under review as a conference paper at ICLR 2021
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search
via parameters sharing. In International Conference onMachine Learning, pp. 4095-4104,2018a.
Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. Efficient neural architecture
search via parameter sharing. In International Conference on Machine Learning, 2018b.
Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual BERT? In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,
pp. 4996-5001, Florence, Italy, July 2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1493. URL https://www.aclweb.org/anthology/P19-1493.
Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings
of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pp. 27-35, Dublin,
Ireland, August 2014. Association for Computational Linguistics. doi: 10.3115/v1/S14-2004.
URL https://www.aclweb.org/anthology/S14- 2004.
Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos.
SemEval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th International
Workshop on Semantic Evaluation (SemEval 2015), pp. 486-495, Denver, Colorado, June 2015.
Association for Computational Linguistics. doi: 10.18653/v1/S15-2082. URL https://www.
aclweb.org/anthology/S15-2082.
Maria Pontiki, Dimitris Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar,
Mohammad AL-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, OrPhee De Clercq,
VeroniqUe Hoste, Marianna Apidianaki, Xavier Tannier, Natalia LoUkachevitch, Evgeniy Kotel-
nikov, Nuria Bel, Salud Maria Jimenez-Zafra, and GUISen Eryigit. SemEVal-2016 task 5: As-
pect based sentiment analysis. In Proceedings of the 10th International Workshop on Seman-
tic Evaluation (SemEval-2016), pp. 19-30, San Diego, California, June 2016. Association for
Computational Linguistics. doi: 10.18653/v1/S16-1002. URL https://www.aclweb.org/
anthology/S16-1002.
Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan,
Quoc V Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In International
Conference on Machine Learning, pp. 2902-2911, 2017.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. In Proceedings of the aaai conference on artificial intelligence,
volume 33, pp. 4780-4789, 2019.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. Named entity recognition in tweets: An experi-
mental study. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language
Processing, pp. 1524-1534, Edinburgh, Scotland, UK., July 2011. Association for Computational
Linguistics. URL https://www.aclweb.org/anthology/D11-1141.
Cicero D Santos and Bianca Zadrozny. Learning character-level representations for part-of-speech
tagging. In Proceedings of the 31st international conference on machine learning (ICML-14), pp.
1818-1826, 2014.
Tal Schuster, Ori Ram, Regina Barzilay, and Amir Globerson. Cross-lingual alignment of contextual
word embeddings, with applications to zero-shot dependency parsing. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1599-1613, Minneapolis,
Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1162.
URL https://www.aclweb.org/anthology/N19- 1162.
David R So, Chen Liang, and Quoc V Le. The evolved transformer. In International Conference on
Machine Learning, 2019.
Kenneth O Stanley and Risto Miikkulainen. Evolving neural networks through augmenting topolo-
gies. Evolutionary computation, 10(2):99-127, 2002.
14
Under review as a conference paper at ICLR 2021
Jana Strakov直 Milan Straka, and Jan Hajic. Neural architectures for nested NER through Iineariza-
tion. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-
tics, pp. 5326-5331, Florence, Italy, July 2019. Association for Computational Linguistics. doi:
10.18653/v1/P19-1527. URL https://www.aclweb.org/anthology/P19-1527.
Masanori Suganuma, Shinichi Shirakawa, and Tomoharu Nagao. A genetic programming approach
to designing convolutional neural network architectures. In Proceedings of the genetic and evo-
lutionary computation conference, pp. 497-504, 2017.
Beth M. Sundheim. Named entity task definition, version 2.1. In Proceedings of the Sixth Message
Understanding Conference, pp. 319-332, 1995.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 1992.
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-
ing the inception architecture for computer vision. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pp. 2818-2826, 2016.
Lucien Tesniere. 6l6ments de syntaxe structurale. Editions Klincksieck, 1959.
Erik F. Tjong Kim Sang. Introduction to the CoNLL-2002 shared task: Language-independent
named entity recognition. In COLING-02: The 6th Conference on Natural Language Learning
2002 (CoNLL-2002), 2002. URL https://www.aclweb.org/anthology/W02-2024.
Erik F. Tjong Kim Sang and Sabine Buchholz. Introduction to the CoNLL-2000 shared task chunk-
ing. In Fourth Conference on Computational Natural Language Learning and the Second Learn-
ing Language in Logic Workshop, 2000. URL https://www.aclweb.org/anthology/
W00-0726.
Erik F. Tjong Kim Sang and Fien De Meulder. Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In Proceedings of the Seventh Conference
on Natural Language Learning at HLT-NAACL 2003, pp. 142-147, 2003. URL https:
//www.aclweb.org/anthology/W03-0419.
Xinyu Wang, Jingxian Huang, and Kewei Tu. Second-order semantic dependency parsing with end-
to-end neural networks. In Proceedings of the 57th Annual Meeting of the Association for Com-
putational Linguistics, pp. 4609-4618, Florence, Italy, July 2019. Association for Computational
Linguistics. doi: 10.18653/v1/P19-1454. URL https://www.aclweb.org/anthology/
P19-1454.
Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Fei Huang, and Kewei Tu. Structure-level
knowledge distillation for multilingual sequence labeling. In Proceedings of the 58th Annual
Meeting of the Association for Computational Linguistics, pp. 3317-3330, Online, July 2020a.
Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.304. URL https:
//www.aclweb.org/anthology/2020.acl-main.304.
Xinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang, Huang Zhongqiang, Fei Huang, and Kewei Tu.
More embeddings, better sequence labelers? In Findings of EMNLP, Online, November 2020b.
Zhenkai Wei, Yu Hong, Bowei Zou, Meng Cheng, and Jianmin Yao. Don’t eclipse your arts due
to small discrepancies: Boundary repositioning with a pointer network for aspect extraction. In
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
3678-3684, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/
2020.acl-main.339. URL https://www.aclweb.org/anthology/2020.acl-main.
339.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Martin Wistuba. Deep learning architecture search by neuro-cell-based evolution with function-
preserving mutations. In Joint European Conference on Machine Learning and Knowledge Dis-
covery in Databases, pp. 243-258. Springer, 2018.
15
Under review as a conference paper at ICLR 2021
Shijie Wu and Mark Dredze. Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT.
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),
pp. 833-844, Hong Kong, China, November 2019. Association for Computational Linguistics.
URL https://www.aclweb.org/anthology/D19- 1077.
L. Xie and A. Yuille. Genetic cnn. In 2017 IEEE International Conference on Computer Vision
(ICCV), pp. 1388-1397, 2017.
Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. Double embeddings and CNN-based sequence
labeling for aspect extraction. In Proceedings of the 56th Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 2: Short Papers), pp. 592-598, Melbourne, Aus-
tralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2094. URL
https://www.aclweb.org/anthology/P18-2094.
Hu Xu, Bing Liu, Lei Shu, and Philip Yu. BERT post-training for review reading compre-
hension and aspect-based sentiment analysis. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 2324-2335, Minneapolis, Minnesota,
June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1242. URL
https://www.aclweb.org/anthology/N19-1242.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.
Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in neural
information processing systems, pp. 5753-5763, 2019.
Juntao Yu, Bernd Bohnet, and Massimo Poesio. Named entity recognition as dependency parsing.
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
6470-6476, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/
2020.acl-main.577. URL https://www.aclweb.org/anthology/2020.acl-main.
577.
Yu Zhang, Zhenghua Li, and Min Zhang. Efficient second-order TreeCRF for neural dependency
parsing. In Proceedings of the 58th Annual Meeting of the Association for Computational Lin-
guistics, pp. 3295-3305, Online, July 2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.acl-main.302. URL https://www.aclweb.org/anthology/2020.
acl-main.302.
Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. Practical block-wise neural
network architecture generation. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 2423-2432, 2018.
Junru Zhou and Hai Zhao. Head-driven phrase structure grammar parsing on Penn treebank. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp.
2396-2408, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/
v1/P19-1230. URL https://www.aclweb.org/anthology/P19-1230.
Wei Zhu, Xiaoling Wang, Xipeng Qiu, Yuan Ni, and Guotong Xie. Autotrans: Automating trans-
former design via reinforced architecture search. arXiv preprint arXiv:2009.02070, 2020.
Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In ICLR, 2017.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In Proceedings of the IEEE conference on computer vision and
pattern recognition, pp. 8697-8710, 2018.
A Detailed Configurations
We use ISO 639-1 language codes to represent languages in the table3.
3https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes
16
Under review as a conference paper at ICLR 2021
A. 1 Datasets
The details of the 6 structured prediction tasks in our experiments are shown in below:
•	NER: We use the corpora of 4 languages from the CoNLL 2002 and 2003 shared task (Tjong
Kim Sang, 2002; Tjong Kim Sang & De Meulder, 2003) with standard split.
•	POS Tagging: We use three datasets, Ritter11-T-POS (Ritter et al., 2011), ARK-Twitter (Gimpel
et al., 2011; Owoputi et al., 2013) and Tweebank-v2 (Liu et al., 2018b) datasets (Ritter, ARK and
TB-v2 in simplification). We follow the dataset split of (Nguyen et al., 2020).
•	Chunking: We use CoNLL 2000 (Tjong Kim Sang & Buchholz, 2000) for chunking. Since there
is no standard development set for CoNLL 2000 dataset, we split 10% of the training data as the
development set.
•	Aspect Extraction: Aspect extraction is a subtask of aspect-based sentiment analysis (Pontiki
et al., 2014; 2015; 2016). The datasets are from the laptop and restaurant domain of SemEval
14, restaurant domain of SemEval 15 and restaurant domain of SemEval 16 shared task (14Lap,
14Res, 15Res and 16Res in short). Additionally, we use another 4 languages in the restaurant
domain of SemEval 16 to test our approach in multiple languages. We randomly split 10% of the
training data as the development set following Li et al. (2019).
•	Syntactic Dependency Parsing: We use Penn Tree Bank (PTB) 3.0 with the same dataset pre-
processing as (Ma et al., 2018).
•	Semantic Dependency Parsing: We use DM, PAS and PSD datasets for semantic dependency
parsing (Oepen et al., 2014) for the SemEval 2015 shared task (Oepen et al., 2015). The three
datasets have the same sentences but with different formalisms. We use the standard split for SDP.
In the split, there are in-domain test sets and out-of-domain test sets for each dataset.
Among these tasks, NER, POS tagging, chunking and aspect extraction are sequence-structured
outputs while dependency parsing and semantic dependency parsing are the graph-sectured outputs.
POS Tagging, chunking and DP are syntactic structured prediction tasks while NER, AE, SDP are
semantic structured prediction tasks.
A.2 Evaluation
To evaluate our models, We use F1 score to evaluate NER, Chunking and AE, use accuracy to
evaluate POS Tagging, use unlabeled attachment score (UAS) and labeled attachment score (LAS)
to evaluate DP, and use labeled F1 score to evaluate SDP.
A.3 Task Models and Controller
For sequence-structured tasks (i.e., NER, POS tagging, chunking, aspect extraction), we use a batch
size of 32 sentences and an SGD optimizer with a learning rate of 0.1. We anneal the learning
rate by 0.5 when there is no accuracy improvement on the development set for 5 epochs. We set
the maximum training epoch to 150. For graph-structured tasks (i.e., DP and SDP), we use Adam
(Kingma & Ba, 2015) to optimize the model with a learning rate of 0.002. We anneal the learning
rate by 0.75 for every 5000 iterations following Dozat & Manning (2017). We set the maximum
training epoch to 300. For DP, we run the maximum spanning tree McDonald et al. (2005) algorithm
to output valid trees in testing. We fix the hyper-parameters of the task models.
We tune the learning rate for the controller among {0.1, 0.2, 0.3, 0.4, 0.5} and the discount factor
among {0.1, 0.3, 0.5, 0.7, 0.9} on the same dataset in Section 5.2. We search for the hyper-parameter
through grid search and find a learning rate of 0.1 and a discount factor of 0.5 performs the best on
the development set. The controller’s parameters are initialized to all 0 so that each candidate is
selected evenly in the first two time steps. We use Stochastic Gradient Descent (SGD) to optimize
the controller. The training time depends on the task and dataset size. Take the English NER CoNLL
dataset as an example. It takes 45 GPU hours to train the controller for 30 steps on a single Tesla
P100 GPU, which is an acceptable training time in practice.
17
Under review as a conference paper at ICLR 2021
A.4 Sources of Embeddings
The sources of the embeddings that we used are listed in Table 8.
Table 8: The embeddings we used in our experiments. The URL is where we downloaded the
embeddings. Note that we have confirmed that the XLM-R models fine-tuned on CoNLL 2002/2003
datasets are only trained on the training data.
Embedding	Resource	URL	
GloVe	Pennington et al. (2014)	nlp.stanford.edu/PrOjects/glove
fastText	BojanowSki et αl.(2017)	github.com/facebookresearch/fastText
ELMo	PeterSetaL(2018)	github.com/allenai/allennlp
ELMo (Other languages)	SChuSter et al. (2019)	github.com/TalSchuster/CrOssLingualCOntextualEmb
BERT	Devlinetal. (2019)	huggingface.co/bert-base-Cased
M-BERT	Devlinetal. (2019)	huggingface.co/bert-base-multilingual-CaSed
BERT (Dutch)	wietsedv	huggingface.co/WietSedv/bert-base-dutCh-CaSed
BERT (German)	dbmdz	huggingface.co/bert-base-german-dbmdz-CaSed
BERT (Spanish)	dccuchile	huggingface.co/dccuchile/bert-base-SpaniSh-Wwm- cased
BERT (Turkish)	dbmdz	huggingface.co/dbmdz/bert-base-turkish-cased
XLM-R	Conneau et al. (2020)	huggingface.co/xlm-roberta-large
XLM-R (CoNLL 02 Dutch)	Hugging Face	huggingface.co/xlm-roberta-large-finetuned-conll02-dutch
XLM-R (CoNLL 02 Spanish)	Hugging Face	huggingface.co/xlm-roberta-large-finetuned-conll02-Spanish
XLM-R (CoNLL 03 English)	Hugging Face	huggingface.co/xlm-roberta-large-finetuned-conll03-english
XLM-R (CoNLL 03 German)	Hussins Face	huggingface.co/xlm-roberta-large-finetuned-conll03-german
XLNet		Yangetal.(2019)		huggingface.co/Xlnet-Iarge-CaSed
B Additional Analysis
B.1	Fine-tuned Models Versus ACE
To fine-tune the embeddings, we use AdamW (Loshchilov & Hutter, 2018) optimizer with a learning
rate of 5e-5 and trained the contextualized embeddings with the task for 10 epochs. We use a batch
size of 32 for BERT, M-BERT and use a batch size of 16 for XLM-R and XLNet. A comparison
between ACE and the fine-tuned embeddings that we used in ACE is shown in Table 9, 10. Results
show that ACE can further improve the accuracy of fine-tuned models.
Table 9: A comparison between ACE and the fine-tuned embeddings that are used in ACE for NER
and POS tagging.
		NER							POS			
	de	de (Revised)	en	es	nl	Ritter	ARK	TB-v2
BERT+Fine-tune	78.1	82.2	91.0	83.1	83.3	91.2	91.7	94.4
MBERT+Fine-tune	81.9	86.2	91.3	87.6	90.7	90.8	91.5	93.9
XLM-R+Fine-tune	85.8	-	92.9	89.7	92.5	93.0	93.4	95.0
ACE+Fine-tune	87.0	90.5	—	93.5	91.7	94.6	93.4	93.8	95.6
Table 10: A comparison between ACE and the fine-tuned embeddings we used in ACE for chunking
and AE.
	Chunk		AE								
	CoNLL 2000	14Lap	14Res	15Res	16Res	es	nl	ru	tr
BERT+Fine-tune-	967	81.2	87.7	71.8	73.9	76.9	73.1	64.3	75.6
MBERT+Fine-tune	96.6	83.5	85.0	69.5	73.6	74.5	72.6	71.6	58.8
XLM-R+Fine-tune	96.5	81.3	88.4	77.3	78.5	77.8	72.1	75.7	66.7
ACE+Fine-tune	97.0	—	85.0	89.8	78.5	81.2	^788~	ɪr	ɪr	^777F
B.2	Retraining
Most of the work (Zoph & Le, 2017; Zoph et al., 2018; Pham et al., 2018b; So et al., 2019; Zhu et al.,
2020) in NAS retrains the searched neural architecture from scratch so that the hyper-parameters of
the searched model can be modified or trained on larger datasets. To show whether our searched
embedding concatenation is helpful to the task, we retrain the task model with the embedding con-
catenations on the same dataset from scratch. For the experiment, we use the same dataset settings
18
Under review as a conference paper at ICLR 2021
Table 11: A comparison between ACE and the fine-tuned embeddings that are used in ACE for DP
and SDP.
	DP			SDP						
	PTB		DM		PAS		PSD	
	UAS	LAS	ID	OOD	ID	OOD	ID	OOD
BERT+Fine-tune	-966^^	95.1	^94Z	91.4	94.4	93.0	82.0	81.3
MBERT+Fine-tune	96.5	94.9	93.9	90.4	93.9	92.1	81.2	80.0
XLM-R+Fine-tune	96.6	95.1	94.3	91.1	94.5	92.8	82.0	81.6
XLNET+Fine-tune	97.0	95.4	94.9	92.0	94.8	93.4	82.6	82.2
ACE+Fine-tune	97.2	^57~	~^53	92.6	^^53	93.9	83.6	83.2
as in Section 5.3. We train the searched embedding concatenation of each run from ACE 3 times
(therefore, 9 runs for each dataset).
Table 12 shows the comparison between retrained models with the searched embedding concate-
nation from ACE and All. The results show that the retrained models are competitive with ACE
in SDP and in chunking. However, in another three tasks, the retrained models perform inferior
to ACE, which shows our approach’s advantage. The retrained models outperform All in all tasks,
which shows the effectiveness of the searched embedding concatenations.
Table 12: A comparison among retrained models, All and ACE. We use the one dataset for each
task.
NER POS Chunk AE DP-UAS DP-LAS SDP-ID SDP-OOD
All	92.4	90.6	96.7	73.2	96.7	95.1	94.3	90.8
Retrain	92.6	90.8	96.8	73.6	96.8	95.2	94.5	90.9
ACE	93.0	91.7	96.8	75.6	96.9	95.3	94.5	90.9
B.3	Effect of Embeddings in the Searched Embedding Concatenations
There is no clear conclusion on what concatenation of embeddings is helpful to most of the tasks.
We analyze the best searched embedding concatenations by ACE over different structured outputs,
semantic/syntactic type, and monolingual/multilingual tasks. The percentage of each embedding
selected by the best concatenations from all experiments of ACE are shown in Table 13. The best
embedding concatenation varies over the output structure, syntactic/semantic level of understand-
ing, and the language. The experimental results show that it is essential to select embeddings for
each kind of task separately. However, we also find that the embeddings are strong in specific set-
tings. In comparison to the sequence-structured and graph-structured tasks, we find that M-BERT
and ELMo are only frequently selected in sequence-structured tasks while XLM-R embeddings are
always selected in graph-structured tasks. For Flair embeddings, the forward and backward model
are evenly selected. We suspect one direction of Flair embeddings is strong enough. Therefore con-
catenating the embeddings from two directions together cannot further improve the accuracy. For
non-contextualized embeddings, pretrained word embeddings are frequently selected in sequence-
structured tasks, and character embeddings are not. When we dig deeper into the semantic and
syntactic type of these two structured outputs, we find that in all best concatenations, BERT embed-
dings are selected in all syntactic sequence-structured tasks, and Flair, M-Flair, word, and XLM-R
embeddings are selected in syntactic graph-structured tasks. In multilingual tasks, all best concate-
nations in multilingual NER tasks select M-BERT embeddings while M-BERT is rarely selected
in multilingual AE tasks. The monolingual Flair embeddings are always selected in NER tasks,
and XLM-R is more frequently selected in multilingual tasks than monolingual sequence-structured
tasks (SS).
19
Under review as a conference paper at ICLR 2021
Table 13: The percentage of each embedding candidate selected in the best concatenations from
ACE. F and MF are monolingual and multilingual Flair embeddings. We count these two em-
beddings are selected if one of the forward/backward (fw/bw) direction of Flair is selected in the
concatenation. We count the Word embedding is selected if one of the fastText/GloVe embeddings
is selected. SS: sequence-structured tasks. GS: graph-structured tasks. Sem.: Semantic-level tasks.
Syn.: Syntactic-level tasks. M-NER: Multilingual NER tasks. M-AE: Multilingual AE tasks. We
only use English datasets in SS and GS. English datasets are removed for M-NER and M-AE.
	BERT M-BERT		Char ELMo		F	F-bw	F-fw MF MF-bw MF-fw				Word XLM-R	
"SS	0.81	0.74	0.37	0.85	0.70	^048^	0.59	0.78	0.59	0.41	0.81	0.70
GS	0.75	0.17	0.50	0.25	0.83	0.75	0.42	0.83	0.58	0.58	0.50	1.00
Sem. SS	0.67	0.73	0.40	0.80	0.60	^O0^	0.53	0.87	0.60	0.53	0.80	0.60
Syn. SS	1.00	0.75	0.33	0.92	0.83	0.58	0.67	0.67	0.58	0.25	0.83	0.83
Sem. GS	0.78	0.22	0.67	0.33	0.78	^O7^	0.56	0.78	0.56	0.67	0.33	1.00
Syn. GS	0.67	0.00	0.00	0.00	1.00	1.00	0.00	1.00	0.67	0.33	1.00	1.00
M-NER	0.67	1.00	0.56	0.83	1.00	^078^	1.00	0.89	0.78	0.44	0.78	0.89
M-AE	1.00	0.33	0.75	0.33	0.58	0.42	0.42	0.75	0.25	0.75	0.50	0.92
20