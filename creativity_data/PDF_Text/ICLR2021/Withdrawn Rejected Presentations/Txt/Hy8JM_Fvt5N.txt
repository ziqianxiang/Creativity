Equal Bits: Enforcing Equally Distributed Binary Network Weights
YUnqiang Li*, Silvia L. Pintea* and Jan C. van Gemert
Computer Vision Lab, Delft University of Technology,
Delft, Netherlands
Abstract
Binary networks are extremely efficient as they use only t-
wo symbols to define the network: {+1, -1}. One can make
the prior distribution of these symbols a design choice. The
recent IR-Net of Qin et al. argues that imposing a Bernoulli
distribution with equal priors (equal bit ratios) over the bi-
nary weights leads to maximum entropy and thus minimizes
information loss. However, prior work cannot precisely con-
trol the binary weight distribution during training, and there-
fore cannot guarantee maximum entropy. Here, we show that
quantizing using optimal transport can guarantee any bit ratio,
including equal ratios. We investigate experimentally that e-
qual bit ratios are indeed preferable and show that our method
leads to optimization benefits. We show that our quantization
method is effective when compared to state-of-the-art bina-
rization methods, even when using binary weight pruning.
Our code is available at https://github.com/liyun
qianggyn/Equal-Bits-BNN.
1 Introduction
Binary networks allow compact storage and swift computa-
tions by limiting the network weights to only two bit sym-
bols {-1, +1}. In this paper we investigate weights priors
before seeing any data: is there a reason to prefer predomi-
nantly positive bit weights? Or more negative ones? Or is e-
quality preferable? Successful recent work (Qin et al. 2020b)
argues that a good prior choice is to have an equal bit-ratio:
i.e. an equal number of +1 and -1 symbols in the network.
This is done by imposing an equal prior under the standard
Bernoulli distribution (Qin et al. 2020b; Peters and Welling
2018; Zhou et al. 2016). Equal bit distributions minimize in-
formation loss and thus maximizes entropy, showing benefit-
s across architectures and datasets (Qin et al. 2020b). How-
ever, current work cannot add a hard constraint of making
symbol priors exactly equal, and therefore cannot guarantee
maximum entropy.
Here, we propose a method to add a hard constraint to
binary weight distribution, offering precise control for any
desired bit ratio, including equal prior ratios. We add hard
constraints in the standard quantization setting (Bulat and
Tzimiropoulos 2019; Qin et al. 2020b; Rastegari et al. 2016)
*Both authors contributed equally.
Copyright c 2022, Association for the Advancement of Artificial
Intelligence (www.aaai.org). All rights reserved.
making use of real-valued latent weights that approximate
the binary weights. We quantize these real-valued weight-
s by aligning them to any desired prior Bernoulli distribu-
tion, which incorporates our preferred binary weight prior.
Our quantization uses optimal transport (Villani 2003) and
can guarantee any bit ratio. Our method makes it possible
to experimentally test the hypothesis in (Qin et al. 2020b)
that equal bit ratios are indeed preferable to other bit ratios.
We baptize our approach with equal bit ratios: bi-half . Fur-
thermore, we show that enforcing equal priors using our ap-
proach leads to optimization benefits by reducing the prob-
lem search-space and avoiding local minima.
We make the following contributions: (i) a binary network
optimization method based on optimal transport; (ii) exact
control over weight bit ratios; (iii) validation of the assump-
tion that equal bit ratios are preferable; (iv) optimization
benefits such as search-space reduction and good minima;
(v) favorable results compared to the state-of-the-art, and
can ensure half-half weight distribution even when pruning
is used.
2 Related Work
For a comprehensive survey on binary networks, see (Qin
et al. 2020a). In Table 1 we show the relation between our
proposed method and pioneering methods, that are repre-
sentatives of their peers, in terms of the binarization choic-
es made. The XNOR method (Table 1(a)) was the first to
propose binarizing latent real-valued weights using the sign
function (Rastegari et al. 2016). Rather than making each bi-
nary weight depend only on its associated real-value weight
or gradient value, IR-Net (Qin et al. 2020b) (Table 1(b)) is
a prototype method that uses filter-weight statistics to up-
date each individual binary weight. Here, we also use filter-
weight statistics to update the binary weights, however sim-
ilar to (Helwegen et al. 2019) Table 1(d)) we do not rely
on the sign function for binarization, but instead use binary
weight flips. This is a natural choice, as flipping the sign of a
binary weight is the only operation one can apply to binary
weights.
Sign versus bit flips. The front-runners of binary networks
are BinaryConnect (Courbariaux, Bengio, and David 2015)
and XNOR (Rastegari et al. 2016) and rely on auxiliary re-
al weights and the sign function to define binary weights.
	⑧ Sign, no filter statistics (Rastegari et al. 2016)	⑪ Sign, filter statistics (Qin et al. 2020b)	(c) Flip, filter statistics Our bi-half		(Flip, no filter statistics (Helwegen et al. 2019)
Initialization	Gradient g; Latent weight w;	Gradient g; Latent weight w;	Gradient g; Latent weight w; Threshold dependent on w;		Gradient g; Predefined threshold T;
Binarization, b	b — Sign(W)	b ^ sign (Stdw--V⅞W⅛))	b - flip(b), if [ rank(w) < rank(w) ≥	D2, and rank(w — αg) ≥ 乌 D2, and rank(w — αg) < D	b—flip(b), if ʃ τ< M, and sign(g) = sign(b)
Table 1: Optimization perspectives. (a) Classical binarization methods tie each binary weight b to an associated real-valued latent variable
w, and quantize each weight by only considering its associated real-valued by using the sign function. (b) Rather than updating the weights
independent of each other, recent work uses filter-weight statistics when updating the binary weights. (c) Our proposed optimization method
does not focus on using the sign function, but rather flips the binary weights based on the distribution of the real weights, thus the binary
weight updates depend on the statistics of the other weights through the rank of w. (d) Recent work moves away from using the sign of the
latent variables, and instead trains the binary network with bit sign flips, however they still consider independent weight updates.
These works are subsequently extended with focus on the s-
caling factors in XNOR++ (Bulat and Tzimiropoulos 2019)
and BNN+ (Darabi et al. 2019), while HWGQ (Li et al.
2017) uses the sign function recursively for binarization. Bi-
Real (Liu et al. 2018) also uses the sign function for bina-
rization and analyzes better approximations of the gradien-
t of the sign function. From a different perspective, recent
work tries to sidestep having to approximate the gradient of
the sign function, and uses bit flips to train binary network-
s (Helwegen et al. 2019). The sign of the binary weights can
be flipped based on searchable (Yang et al. 2020) or learn-
able thresholds (Liu et al. 2020). Here, we also rely on bit
flips based on a dynamic thresholding of the real weights,
entailed by our optimal transport optimization strategy.
Using filter statistics or not. Commonly, binarization meth-
ods define each binary weight update by considering only its
associated value in the real-valued latent weights (Bulat and
Tzimiropoulos 2019; Li et al. 2017; Rastegari et al. 2016;
Liu et al. 2018) or in the gradient vector (Helwegen et al.
2019). However, binary weights can also be updated using
explicit statistics of the other weights in the filter (Lin, Zhao,
and Pan 2017) or implicitly learned through a function (Han
et al. 2020). The real-valued filter statistics are used in IR-
Net (Qin et al. 2020b) to enforce a Bernoulli distribution
with equal priors. Similarly, our optimal transport optimiza-
tion leads to ranking the real weights, and therefore making
use of the statistics of the real-weights in each filter.
Network pruning. Pruning has been shown to improve the
efficiency of deep networks (Frankle et al. 2020; Huang
et al. 2018; Lin et al. 2017; Xiao, Wang, and Rajasekaran
2019; Ye et al. 2020). However, the reason why pruning
can bring improvements remains unclear in real-valued net-
works. It is commonly believed (Evci et al. 2020; Frankle
and Carbin 2020; Malach et al. 2020; Zhou et al. 2019)
that finding the “important” weight values is crucial for re-
training a small pruned model. Specifically, the “important”
weight values are inherited (Han, Mao, and Dally 2016) or
re-winded (Frankle and Carbin 2020) from a large trained
model. In contrast, (Liu et al. 2019) claims that the select-
ed important weights are typically not useful for the small
pruned model, while the pruned architecture itself is more
relevant. The lottery-ticket idea has recently been applied to
binary networks (Diffenderfer and Kailkhura 2021). Here,
we show that having equal +1 and -1 ratios is also opti-
mal when the networks rely on pruning and that our optimal
transport optimization can easily be adapted to work with
methods using pruning.
3	Binarizing with optimal transport
3.1	Binary weights
We define a binary network where the weights B take binary
values {1, -1}D. The binary weights B follow a Bernoulli
distribution B 〜Be(Ppos), describing the probabilities of
individual binary values b ∈ {-1, 1} in terms of the hyper-
parameters ppos and pneg :
p(b) = Be(b | ppos) = ppos = 1 -	if b = -1
pneg = - ppos ,	= -
(1)
To be consistent with previous work, we follow XNOR-
Net (Rastegari et al. 2016) and apply the binary optimization
per individual filter.
Because the matrix B is discrete, we follow (Courbariaux,
Bengio, and David 2015; Rastegari et al. 2016) by using
real-valued latent weights W to aid the training of discrete
values, where each binary weight in B has an associated
real-valued weight in W. In the forward pass we quantize
the real-valued weights W to estimate the matrix B. Then,
we use the estimated matrix B to compute the loss, and
in the backward pass we update the associated real-valued
weights W.
3.2	Optimal transport optimization
The optimization aligns the real-valued weight distribution
W with the prior Bernoulli distribution in Eq. (1) and quan-
tizes the real-valued weights W to B.
The empirical distribution Pw of the real-valued variable
W ∈ RD and the empirical distribution Pb for the discrete
variable B can be written as:
D2
Pw = Xpiδwi, Pb = Xqjδbj,	(2)
i=1	j=1
where δx is the Dirac function at location x. The pi and qj
are the probability mass associated to the corresponding dis-
tribution locations wi and bj , where Pb has only 2 possible
locations in the distribution space {-1, 1}.
To align Pw with the Bernoulli prior Pb in Eq. (1) we use
optimal transport (OT) (Villani 2003) which minimizes the
cost of moving the starting distribution Pw to the target dis-
tribution Pb . Because Pw and Pb are only accessible through
a finite set of values, the corresponding optimal transport
cost is:
π0 =	min	hπ, CiF ,
π∈Π(Pw ,Pb)	F
(3)
where Π(Pw , Pb) is the space of the joint probability with
marginals Pw and Pb, and π is the general probabilistic cou-
pling that indicates how much mass is transported to push
distribution Pw towards the distribution Pb. The h., .iF de-
notes the Frobenius dot product, and C ≥ 0 is the cost
function matrix whose element C(wi, bj) denotes the cost of
moving a probability mass from location wi to location bj in
distribution space. When the cost is defined as a distance, the
OT becomes the Wasserstein distance. We minimize the 1-
Wasserstein distance between Pw and Pb. This minimization
has an elegant closed-form solution based on simply sorting.
For a continuous-valued weights vector W ∈ RD , we first
sort the elements of W, and then assign the top pposD el-
ements to +1, and the bottom (1 - ppos)D portion of the
elements to -1:
B=π0(W) =	+1,
top pposD of sorted W
bottom (1 - ppos)D of sorted W
(4)
Rather than using the sign function to define the binariza-
tion, we flip the binary weights based on the distribution of
W. Thus the flipping of a binary weight depends on the dis-
tribution of the other binary weights through W, which is
optimized to be as close as possible to B.
When applying our method in combination with pruning
as in (Diffenderfer and Kailkhura 2021), we first mask the
binary weights B0 = M B with a mask M ∈ {0, 1}D.
This leads to a certain percentage of the weights being
pruned. Subsequently, we apply the Eq. (4) to the remain-
ing non-pruned weights, where D in Eq. (4) become the L1
norm of the mask, |M|.
3.3	Bi-half : Explicitly controlling the bit ratio
Our optimal transport optimization allows us to enforce a
hard constraint on precise bit ratios by varying the ppos val-
ue. Therefore, we can test a range of prior binary weight
distributions.
Following (Qin et al. 2020b), a good prior over the bi-
nary weights is one maximizing the entropy. Using optimal
transport, we maximize the entropy of the binary weights by
setting the bit ratio to half in Eq. (4):
Ppos = argmaxppos H(B 〜Be(Ppos)) = 2,	⑸
where H(∙) denotes the entropy of the binary weights B.
We dub this approach bi-half . Unlike previous work (Qin
et al. 2020b), we can guarantee equal symbol distribution-
s and therefore maximum entropy throughout the complete
training procedure.
Initialization and scaling factor. We initialize the real-
valued weights using Kaiming normal (He et al. 2015). The
binary weights are initialized to be equally distributed per
filter according to Eq. (5). To circumvent exploding gradi-
ents, we use one scaling factor α per layer for the binary
weights to keep the activation variance in the forward pass
close to 1. Based on the ReLU variance analysis in (He et al.
2015) it holds that 2D ∙ Var(aB) = 1, where D is the num-
ber of connections and B are our binary weights. B is reg-
ularized to a bi-half distribution, thus Var(B) = 1, which
gives α = p2/D.
To better clarify, for an L-layer network with input data
y1 standardized to Var[y1] = 1, where the variance of each
binary layer l is V ar[Bl] = 1, and Dl is the number of con-
nections in that layer: i) Without the scaling, the output vari-
ance is Var[yL^] = Var[yι] QL=2 DVar[Bι] =Q匕2 D2l.
Typically Dl is large, leading to exploding gradients; ii)
With the scaling, We scale Bl by α =，2/Dl, leading to
Var[yL↑ = Var[yι] QL= D2lVar(αBl) = 1 which stabi-
lizes learning.
4	Experiments
Datasets and implementation details. We evaluate on
Cifar-10, Cifar-100 (Krizhevsky, Hinton et al. 2009) and Im-
ageNet (Deng et al. 2009), for a number of network architec-
tures. Following (Frankle and Carbin 2020; Ramanujan et al.
2020) we evaluate 4 shallow CNNs: Conv2, Conv4, Conv6,
and Conv8 with 2/4/6/8 convolutional layers. We train the
shallow models on Cifar-10 for 100 epochs, with weight de-
cay 1e-4, momentum 0.9, batch size 128, and initial learn-
ing rate 0.1 using a cosine learning rate decay (Loshchilov
and Hutter 2016). Following (Qin et al. 2020b) we also e-
valuate their ResNet-20 architecture and settings on Cifar-
10. On Cifar-100, we evaluate our method on 5 different
models including VGG16 (Simonyan and Zisserman 2015),
ResNet18 (He et al. 2016), ResNet34 (He et al. 2016), In-
ceptionV3 (Szegedy et al. 2016), ShuffleNet (Zhang et al.
2018). We train the Cifar-100 models for 350 epochs using
SGD with weight decay 5e-4, momentum 0.9, batch size
128, and initial learning rate 0.1 divided by 10 at epochs 150,
250 and 320. For ImageNet we use ResNet-18 and ResNet-
34 trained for 100 epochs using SGD with momentum 0.9,
weight decay 1e-4, and batch size 256. Following (Liu et al.
2018; Qin et al. 2020b), the initial learning rate is set as 0.1
and we divide it by 10 at epochs 30, 60, 90. All our model-
s are trained from scratch without any pre-training. For the
shallow networks we apply our method on all layers, while
for the rest we follow (Liu et al. 2018; Qin et al. 2020b),
and apply it on all convolutional and fully-connected layers
except the first, last and the downsampling layers.
4.1	Hypothesis: Bi-half maximizes the entropy
Here we test whether our proposed bi-half model can in-
deed guarantee maximum entropy and therefore an exactly
equal ratio of the -1 and +1 symbols. Fig. 2 shows the bit
flips performed in our proposed bi-half method during train-
ing when compared to two baselines: sign (Rastegari et al.
2016) and IR-Net (Qin et al. 2020b). We train a Conv2 net-
work on Cifar-10 and plot the flips of binary weights in a sin-
gle binary weight filter during training. The binary weights
009896
0 8 6 4
L000
sə,JmeφAdaJu 山
0	50	100	150	200	250	300	350
Epoch
(b) Activation entropy on ResNet-18.
O 20	40	60	80 IOO
Epoch
(a)	Binary weight entropy on Conv2.
Figure 1: Hypothesis: bi-half maximizes the entropy. Entropy of binary weights and activations. We compare our bi-half method to sign
(Rastegari et al. 2016) and IR-Net (Qin et al. 2020b). (a) Entropy of the binary weights during training for Conv2 on Cifar10. (b) Entropy of
the network activations for ResNet-18 on Cifar100. Our bi-half model can guarantee maximum entropy during training for the binary weight
distribution and it is able to better maximize the entropy of the activations.
(a) Sign has uneven flips by independently updating binary weight.
(b) IR-Net has uneven flips by balancing the latent weights.
J^qlunN P ① dd=u-
」① qιu∏N P ① dd=u-
12	3
Iter. (Ie4)
(c) Bi-half has even flips by precisely controlling the flipping.
Figure 2: Hypothesis: bi-half maximizes the entropy. Bit flip-
s during training. We compare the bit flips during training in our
bi-half with the sign (Rastegari et al. 2016) and IR-Net (Qin et al.
2020b) on the Conv2 network on Cifar-10. The x-axis shows the
training iterations. Left: Bit flips during training to +1 (dark blue)
or to -1 (cyan). Right: Accumulated bit flips over the training iter-
ations, as well as the difference between the bit flips from (+1 to
-1) and the ones from (-1 to +1). In contrast to sign and IR-Net,
our bi-half method can guarantee an equal bit ratio.
are initialized to be equal distributed (half of the weight-
s positive and the other half negative). The classical sign
method (Rastegari et al. 2016) in Fig. 2(c) binarizes each
weight independent of the other weights, therefore during
training the flips for (+1 to -1) and (-1 to +1) are uneven.
The recent IR-Net (Qin et al. 2020b) in Fig. 2(b) balances
the latent weights by using their statistics to obtain evenly
distributed binary weight values. However, it can not guar-
antee evenly distributed binary weights throughout training.
Our bi-half model in Fig. 2(c) updates the binary weight
based on the statistics of the other weights. For our method
the binary weights are evenly flipped during training, offer-
ing exact control of bit weight ratios.
Fig. 1(a) shows the binary weights entropy changes dur-
ing training on Conv2 when compared to sign (Rastegar-
i et al. 2016) and IR-Net (Qin et al. 2020b). IR-Net aims
to maximize entropy by subtracting the mean value of the
weights, yet, this is not exact. In contrast, we maximize
the information entropy by precisely controlling the bina-
ry weight distribution. In Fig. 1(b) we show the entropy of
the binary activations. Adjusting the distribution of bina-
ry weights retains the information in the binary activation.
For our bi-half method, the binary activation of each chan-
nel is close to the maximum information entropy under the
Bernoulli distribution.
4.2 Empirical analysis
(a) Effect of hyper-parameters. In Fig. 3 we study the
effectiveness of the commonly used training techniques of
varying the weight decay and learning rate decay, when
training the Conv2 network on Cifar-10. Fig. 3(a) shows that
using a higher weight decay reduces the magnitude of latent
weights during training and therefore the magnitude of the
cut-off point (threshold) between the positive and negative
values. Fig. 3(b) compares the gradient magnitude of two d-
ifferent learning rate (lr) schedules: “constant lr” and “cosine
lr”. The magnitude of the gradients reduces during training
when using the cosine learning rate. In Fig. 3(c) we find that
increasing the weight decay for binary network with a con-
stant learning rate schedule, increases binary weights flips.
l0Ql°Q
0 5 0 5
17 5 2
JBqEnU PBdd=L
25	50	75
Epoch
JBqEnU PBdd=L
(a) Latent weights magnitude (b) Gradient magnitude (c) flips with constant lr.
25	50	75
Epoch
(d) flips with cosine lr.
80.0-
>77.5-
i75.0-
U
U
« 72.5-
70.0-
100	0 le-5 le-4 5e-4 le-3
Weight Decay
(e) accuracy
Figure 3: Empirical analysis (a): Effect of hyper-parameters. We show the effect of weight decay and learning rate decay on binary
weights flips using the Conv2 network on Cifar-10. Carefully tuning these hyper-parameters is important for adequately training the binary
networks.
12
Figure 4: Empirical analysis (c): Optimization benefits. We train our bi-half model 100 times on Cifar-10 and plot the distribution of the
losses and accuracies over the 100 repetitions. We compare our results using optimal transport to the results using the standard sign function.
On average our bi-half model tends to arrive at better losses and accuracies than the baseline.
108 6 4 2 O
4un8
!.5
83.0 83.5 84.0
Training Accuracy
2 0 8 6 4 2
1 1
4un8
4un8
12108 6 4 2 O
0.46	0.48
Training Loss
5 O
1 1
uno□
■44
6
■ ♦ CifarlO ■ ■- CifarlOO
10	20	30	40	50	60	70	80	90
Prior ppos (%)
Figure 5: Empirical analysis (b): Which bit-ratios are pre-
ferred? We varying the bit-ratios on Cifar-10 and Cifar-100 using
Conv2 the choice of the priorppos under the Bernoulli distribution.
The x-axis is the probability of the +1 connections denoted by ppos
in the Bernoulli prior distribution, while the y-axis denotes the top-
1 accuracy values. Results are in agreement with the hypothesis of
Qin et al. (Qin et al. 2020b) that equal priors as imposed in our
bi-half model are preferable.
Fig. 3(d) shows that decaying the learning rate when using a
cosine learning rate schedule gradually decreases the num-
ber of flipped weights. Fig. 3(e) shows that the choice of
weight decay and learning rate decay affect each other. Our
bi-half method uses the rank of latent weights to flip the bi-
nary weights. A proper tuned hyper-parameter of weight de-
cay and learning rate decay will affect the flipping threshold.
Therefore in the experiments, we carefully tune the hyper-
parameters of weight decay and learning rate decay to build
a competitive baseline.
(b)	Which bit-ratio is preferred? In Fig. 5, we evaluate
the choice of the prior ppos in the Bernoulli distribution for
Conv2 on Cifar-10 and Cifar-100. By varying the bit-ratio,
the best performance is consistently obtained when the neg-
ative and positive symbols have equal priors as in the bi-
half model. Indeed, as suggested in (Qin et al. 2020b), when
there is no other a-priori reason to select a different ppos ,
having equal bit ratios is a good choice.
(c) Optimization benefits with bi-half. The uniform prior
over the -1 and +1 under the Bernoulli distribution regular-
izes the problem space, leading to only a subset of possible
weight combinations available during optimization. We il-
lustrate this intuitively on a 2D example for a simple fully-
connected neural network with one input layer, one hidden
layer, and one output layer in a two-class classification set-
ting. We consider a 2D binary input vector x = [x1 , x2]|,
and define the network as: σ(w2|σ(w1 |X + b1)), where
σ(∙) is a sigmoid nonlinearity, w1 is a [2 X 3] binary weight
matrix, b1 is [3 × 1] binary bias vector, and w2 is a [3 × 1]
binary vector. We group all 12 parameters as a vector B. We
enumerate all possible binary weight combinations in B, i.e.
212 = 4096, and plot all decision boundaries that separate
the input space into two classes as shown in Fig. 6(a). Al-
l possible 4096 binary weights combinations offer only 76
unique decision boundaries. In Fig. 6.(b) the Bernoulli dis-
tribution over the weights with equal prior (bi-half ) regular-
izes the problem space: it reduces the weight combinations
to 924, while retaining 66 unique solutions, therefore the ra-
tio of the solutions to the complete search spaces is increased
nearly 4 times. Fig. 6.(c) shows in a half-log plot how the
numbers of weight combinations and unique network solu-
tions change with varying bit-ratios. Equal bit ratios is opti-
mal.
O
-> andu- ZX
-4-	■
-4	0	4
Xi input value
Weight combinations: 100% (4096).
Unique solutions: 100% (76).
(a)	Full binary network
4
O
->φndw ZX
-4 ■	■
-4	0	4
Xi input value
Weight combinations: 22.5% (924).
Unique solutions: 86.8% (66).
(b)	Bi-half network
IO3
2 1
O O
1 1
WCO⅞CSEOO
2	4	6	8	10
Number of negative binary weights
(c) Solution space
vs bit-ratios
Figure 6: Empirical analysis (c): Optimization benefits. Bi-half regularization: 2D example for a 12-parameter fully connected binary
network σ(w2lσ(w1 Tx + bι)), where σ(∙) is a sigmoid nonlinearity. Weights are in {-1,1}. (a) Enumeration of all decision boundaries for
12 binary parameters (4096 = 212 combinations). (b) Weight combinations and unique solutions when using our bi-half constraint. (c) The
weight combinations and unique decision boundaries for various bit-ratios. When the number of negative binary weights is 6 on the x-axis,
we have equal bit-ratios, which is the optimal ratio. Using the bi-half works as a regularization, reducing the search-space while retaining the
majority of the solutions.
5 0 5
7 7 6
(％) AORmOOV
Bit-width (1/32) I__ɪ_.∣~1~~H , I . ∙ L JL 82 76.29 76.9977.0977.21	HHH 67 0467.82 68.25 	61：8562：05 62.17	Bit-width (1/1) ■■■III 70 93 71.43 71.60 72.4872.6772.82	Architectures ResNetl 8 ResNet34 VGG16 InceptionV3 ShuffleNet
60
弓项a'og弓项a'θg弓项a8θg弓演武8 °g弓承公8。乐弓演西8。乐弓^
Figure 7: Architecture variations: Different architectures on Cifar-100. We evaluate on Cifar-100 over 5 different architectures: VG-
G16 (Simonyan and Zisserman 2015), ResNet18 (He et al. 2016), ResNet34 (He et al. 2016), InceptionV3 (Szegedy et al. 2016), Shuf-
fleNet (Zhang et al. 2018). We compare sign (Rastegari et al. 2016), IR-Net (Qin et al. 2020b) and our bi-half . The 1/32 and 1/1 indicate
the bit-width for weights and for activations, where 1/1 means we quantize both the weights and the activations to binary code values. Our
method achieves competitive accuracy across different network architectures.
In Fig. 4 we train the Conv2 networks 100 times on Cifar-
10 and plot the distribution of the training and test losses
and accuracies. We plot these results when using the bi-half
model optimization with optimal transport and by training
the network using the standard sign function. The figure
shows the bi-half method consistently finds better solution-
s with lower training and test losses and higher training and
test accuracy. To better visualize this trend we sort the values
of the losses for our bi-half and the baseline sign method
over the 100 repetitions and plots them next to each other.
On average the bi-half finds better optima.
4.3	Architecture variations
In Table 2 we compare the Sign (Rastegari et al. 2016), IR-
Net (Qin et al. 2020b) and our bi-half on four shallow Con-
v2/4/6/8 networks on Cifar-10 (averaged over 5 trials). As
the networks become deeper, the proposed bi-half method
consistently outperforms the other methods.
In Fig. 7, we further evaluate our method on Cifar-100
over 5 different architectures: VGG16 (Simonyan and Zis-
serman 2015), ResNet18 (He et al. 2016), ResNet34 (He
et al. 2016), InceptionV3 (Szegedy et al. 2016), Shuf-
fleNet (Zhang et al. 2018). Our method is slightly more ac-
curate than the other methods, especially on the VGG16 ar-
chitecture, it never performs worse.
4.4	Comparison with state-of-the-art
(a)	Comparison on ImageNet. For the large-scale Im-
ageNet dataset we evaluate a ResNet-18 and ResNet-34
backbone (He et al. 2016). Table 3 shows a number
of state-of-the-art quantization methods over ResNet-18
and ResNet-34, including: ABC-Net (Lin, Zhao, and Pan
2017), XNOR (Rastegari et al. 2016), BNN+ (Darabi et al.
2019), Bi-Real (Liu et al. 2018), RBNN (Lin et al. 2020),
XNOR++ (Bulat and Tzimiropoulos 2019), IR-Net (Qin

8180797877
0 10 20 30 40 50 60 70 80
Percent of Pruned Weights
Conv2
0 10 20 30 40 50 60 70 80
Percent of Pruned Weights
Conv4
11219阳,7,e
9 9 8 8 8co
0 10 20 30 40 50 60 70 80
Percent of Pruned Weights
Conv6
ιi→- BihaIf+Prune
Ml⅛ I
T	→- Bihalf+Prune
71.0-	-→- MPT
0 10 20 30 40 50 60 70 80
Percent of Pruned Weights
Conv8
0	20	40	60	80
Percent of Pruned Weights
ResNet-18
Figure 8: Comparison with state-of-the-art (b): Pruned networks. Test accuracy of Conv2/4/6/8 on CIFAR-10, and ResNet-18 on CIFAR-
100 when varying the % pruned weights. We compare with the MPT baseline (Diffenderfer and Kailkhura 2021) using binary weight masking
and the sign function. Having equal +1 and -1 ratios is also optimal when the networks rely on pruning and that our optimal transport
optimization can easily be adapted to work in combination with pruning.
Method	Conv2 Conv4 Conv6 Conv8
Sign	77.86±	0.69	86.49±	0.24	88.51± 0.35	89.17±0.26
IR-Net	78.32±	0.25	87.20±	0.26	89.61± 0.11	90.06±0.06
Bi-half (ours)	79.25±	0.28	87.68±	0.32	89.92± 0.19	90.40±0.17
Table 2: Architecture variations. Accuracy comparison of
sign (Rastegari et al. 2016), IR-Net (Qin et al. 2020b) and our bi-
half on Conv2/4/6/8 networks using Cifar-10, over 5 repetitions.
As the depth of the network increases, the accuracy of our method
increases.
et al. 2020b), and Real2binary (Martinez et al. 2020). Of all
the methods, RBNN is the closest in accuracy to our bi-half
model. This is because RBNN relies on the sign function
but draws inspiration from hashing, and adds an activation-
aware loss to change the distribution of the activations be-
fore binarization. On the other hand, our method uses the s-
tandard classification loss but outperforms most other meth-
ods by a large margin on both ResNet-18 and ResNet-34
architectures.
(b)	Comparison on pruned networks. In Fig. 8 we show
the effect of our bi-half on pruned models. Following the
MPT method (Diffenderfer and Kailkhura 2021) we learn
a mask for the binary weights to prune them. However, in
our bi-half approach for pruning we optimize using opti-
mal transport for equal bit ratios in the remaining unpruned
weights. We train shallow Conv2/4/6/8 networks on CIFAR-
10, and ResNet-18 on CIFAR-100 while varying the per-
centage of pruned weights. Each curve is the average over
five trials. Pruning consistently finds subnetworks that out-
perform the full binary network. Our bi-half method with
optimal transport retains the information entropy for the
pruned subnetworks, and consistently outperforms the MPT
baseline using the sign function for binarization.
5 Conclusion
We focus on binary networks for their well-recognized effi-
ciency and memory benefits. To that end, we propose a novel
method that optimizes the weight binarization by aligning
a real-valued proxy weight distributions with an idealized
distribution using optimal transport. This optimization
allows to test which prior bit ratio is preferred in a binary
Backbone	Method	Bit-width (W/A)	Top-1(%)	Top-5(%)
	FP	32/32	69.3	89.2
	ABC-Net	1/1	42.7	67.6
	XNOR	1/1	51.2	73.2
	BNN+	1/1	53.0	72.6
ResNet-18	Least-squares	1/1	58.9	81.4
	XNOR++	1/1	57.1	79.9
	IR-Net	1/1	58.1	80.0
	RBNN	1/1	59.9	81.9
	Sign (Baseline)	1/1	59.98	82.47
	Bi-half (ours)	1/1	60.40	82.86
	FP	32/32	73.3	91.3
	ABC-Net	1/1	52.4	76.5
	Bi-Real	1/1	62.2	83.9
ResNet-34				
	IR-Net	1/1	62.9	84.1
	RBNN	1/1	63.1	84.4
	bi-half (ours)	1/1	64.17	85.36
Table 3: Comparison with state-of-the-art (a): ImageNet re-
sults. We show Top-1 and Top-5 accuracy on ImageNet for a num-
ber of state-of-the-art binary networks. Sign is our baseline by
carefully tuning the hyper-parameters. Our proposes bi-half mod-
el consistently outperforms the other binarization methods on this
large-scale classification task.
network, and we show that the equal bit ratios, as advertised
by (Qin et al. 2020b), indeed work better. We confirm that
our optimal transport binarization has optimization benefits
such as: reducing the search space and leading to better local
optima. Finally, we demonstrate competitive performance
when compared to state-of-the-art, and improved accuracy
on 3 different datasets and various architectures. We
additionally show accuracy gains with pruning techniques.
References
Bulat, A.; and Tzimiropoulos, G. 2019. XNOR-Net++: Im-
proved Binary Neural Networks. arXiv:1909.13863. 1, 2,
6
Courbariaux, M.; Bengio, Y.; and David, J.-P. 2015. Bi-
naryconnect: Training deep neural networks with binary
weights during propagations. In NeurIPS. 1, 2
Darabi, S.; Belbahri, M.; Courbariaux, M.; and Nia, V. P.
2019. Bnn+: Improved binary network training. ICLR. 2, 6
Deng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-
Fei, L. 2009. Imagenet: A large-scale hierarchical image
database. In 2009 IEEE conference on computer vision and
pattern recognition, 248-255. Ieee. 3
Diffenderfer, J.; and Kailkhura, B. 2021. Multi-prize lottery
ticket hypothesis: Finding accurate binary neural networks
by pruning a randomly weighted network. ICLR. 2, 3, 7
Evci, U.; Gale, T.; Menick, J.; Castro, P. S.; and Elsen, E.
2020. Rigging the lottery: Making all tickets winners. ICM-
L. 2
Frankle, J.; and Carbin, M. 2020. The lottery ticket hypoth-
esis: Finding sparse, trainable neural networks. ICLR. 2,
3
Frankle, J.; Dziugaite, G. K.; Roy, D. M.; and Carbin, M.
2020. Pruning Neural Networks at Initialization: Why are
We Missing the Mark? CoRR. 2
Han, K.; Wang, Y.; Xu, Y.; Xu, C.; Wu, E.; and Xu, C. 2020.
Training binary neural networks through learning with noisy
supervision. In International Conference on Machine Learn-
ing, 4017-4026. PMLR. 2
Han, S.; Mao, H.; and Dally, W. J. 2016. Deep Compression:
Compressing Deep Neural Networks with Pruning, Trained
Quantization and Huffman Coding. arXiv:1510.00149. 2
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2015. Delving deep
into rectifiers: Surpassing human-level performance on ima-
genet classification. In ICCV. 3
He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual
learning for image recognition. In CVPR, 770-778. 3, 6
Helwegen, K.; Widdicombe, J.; Geiger, L.; Liu, Z.; Cheng,
K.-T.; and Nusselder, R. 2019. Latent weights do not ex-
ist: Rethinking binarized neural network optimization. In
Advances in neural information processing systems, 7533-
7544. 1, 2
Huang, Q.; Zhou, K.; You, S.; and Neumann, U. 2018.
Learning to prune filters in convolutional neural networks.
In WACV. 2
Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple
layers of features from tiny images. Technical report, Cite-
seer. 3
Li, Z.; Ni, B.; Zhang, W.; Yang, X.; and Gao, W. 2017. Per-
formance guaranteed network acceleration via high-order
residual quantization. In Proceedings of the IEEE Interna-
tional Conference on Computer Vision, 2584-2592. 2
Lin, J.; Rao, Y.; Lu, J.; and Zhou, J. 2017. Runtime neural
pruning. In NeurIPS. 2
Lin, M.; Ji, R.; Xu, Z.; Zhang, B.; Wang, Y.; Wu, Y.; Huang,
F.; and Lin, C.-W. 2020. Rotated binary neural network.
ECCV. 6
Lin, X.; Zhao, C.; and Pan, W. 2017. Towards accurate bi-
nary convolutional neural network. In NeurIPS. 2, 6
Liu, Z.; Shen, Z.; Savvides, M.; and Cheng, K.-T. 2020. Re-
actnet: Towards precise binary neural network with general-
ized activation functions. In European Conference on Com-
puter Vision, 143-159. Springer. 2
Liu, Z.; Sun, M.; Zhou, T.; Huang, G.; and Darrell, T. 2019.
Rethinking the value of network pruning. ICLR. 2
Liu, Z.; Wu, B.; Luo, W.; Yang, X.; Liu, W.; and Cheng, K.-
T. 2018. Bi-real net: Enhancing the performance of 1-bit c-
nns with improved representational capability and advanced
training algorithm. In ECCV. 2, 3, 6
Loshchilov, I.; and Hutter, F. 2016. Sgdr: Stochastic gra-
dient descent with warm restarts. arXiv preprint arX-
iv:1608.03983. 3
Malach, E.; Yehudai, G.; Shalev-Shwartz, S.; and Shamir, O.
2020. Proving the Lottery Ticket Hypothesis: Pruning is All
You Need. CoRR. 2
Martinez, B.; Yang, J.; Bulat, A.; and Tzimiropoulos, G.
2020. Training binary neural networks with real-to-binary
convolutions. arXiv preprint arXiv:2003.11535. 7
Peters, J. W.; and Welling, M. 2018. Probabilistic binary
neural networks. CoRR. 1
Qin, H.; Gong, R.; Liu, X.; Bai, X.; Song, J.; and Sebe, N.
2020a. Binary neural networks: A survey. Pattern Recogni-
tion. 1
Qin, H.; Gong, R.; Liu, X.; Shen, M.; Wei, Z.; Yu, F.; and
Song, J. 2020b. Forward and Backward Information Reten-
tion for Accurate Binary Neural Networks. In CVPR. 1, 2,
3, 4, 5, 6, 7
Ramanujan, V.; Wortsman, M.; Kembhavi, A.; Farhadi, A.;
and Rastegari, M. 2020. What’s Hidden in a Randomly
Weighted Neural Network? In CVPR. 3
Rastegari, M.; Ordonez, V.; Redmon, J.; and Farhadi, A.
2016. Xnor-net: Imagenet classification using binary convo-
lutional neural networks. In European conference on com-
puter vision, 525-542. Springer. 1, 2, 3, 4, 6, 7
Simonyan, K.; and Zisserman, A. 2015. Very deep convolu-
tional networks for large-scale image recognition. ICLR. 3,
6
Szegedy, C.; Vanhoucke, V.; Ioffe, S.; Shlens, J.; and Wojna,
Z. 2016. Rethinking the inception architecture for computer
vision. In CVPR, 2818-2826. 3, 6
Villani, C. 2003. Topics in optimal transportation. 58.
American Mathematical Soc. 1, 3
Xiao, X.; Wang, Z.; and Rajasekaran, S. 2019. Autoprune:
Automatic network pruning by regularizing auxiliary pa-
rameters. In NeurIPS. 2
Yang, Z.; Wang, Y.; Han, K.; Xu, C.; Xu, C.; Tao, D.; and
Xu, C. 2020. Searching for Low-Bit Weights in Quantized
Neural Networks. In NeurIPS. 2
Ye, M.; Gong, C.; Nie, L.; Zhou, D.; Klivans, A.; and Liu,
Q. 2020. Good Subnetworks Provably Exist: Pruning via
Greedy Forward Selection. ICML. 2
Zhang, X.; Zhou, X.; Lin, M.; and Sun, J. 2018. Shufflenet:
An extremely efficient convolutional neural network for mo-
bile devices. In CVPR, 6848-6856. 3, 6
Zhou, H.; Lan, J.; Liu, R.; and Yosinski, J. 2019. Decon-
structing lottery tickets: Zeros, signs, and the supermask. In
NeurIPS. 2
Zhou, S.; Wu, Y.; Ni, Z.; Zhou, X.; Wen, H.; and Zou, Y.
2016. DoReFa-Net: Training Low Bitwidth Convolutional
Neural Networks with Low Bitwidth Gradients. CoRR. 1