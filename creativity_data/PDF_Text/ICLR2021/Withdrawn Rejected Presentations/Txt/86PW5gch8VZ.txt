Under review as a conference paper at ICLR 2021
DQSGD: Dynamic Quantized Stochastic Gra-
dient Descent for Communication-efficient
Distributed Learning
Anonymous authors
Paper under double-blind review
Ab stract
Gradient quantization is widely adopted to mitigate communication costs in dis-
tributed learning systems. Existing gradient quantization algorithms often rely on
design heuristics and/or empirical evidence to tune the quantization strategy for
different learning problems. To the best of our knowledge, there is no theoretical
framework characterizing the trade-off between communication cost and model
accuracy under dynamic gradient quantization strategies. This paper addresses
this issue by proposing a novel dynamic quantized SGD (DQSGD) framework,
which enables us to optimize the quantization strategy for each gradient descent
step by exploring the trade-off between communication cost and modeling error.
In particular, we derive an upper bound, tight in some cases, of the modeling error
for arbitrary dynamic quantization strategy. By minimizing this upper bound, we
obtain an enhanced quantization algorithm with significantly improved modeling
error under given communication overhead constraints. Besides, we show that
our quantization scheme achieves a strengthened communication cost and model
accuracy trade-off in a wide range of optimization models. Finally, through exten-
sive experiments on large-scale computer vision and natural language processing
tasks on CIFAR-10, CIFAR-100, and AG-News datasets, respectively. we demon-
strate that our quantization scheme significantly outperforms the state-of-the-art
gradient quantization methods in terms of communication costs.
1	Introduction
Recently, with the booming of Artificial Intelligence (AI), 5G wireless communications, and Cyber-
Physical Systems (CPS), distributed learning plays an increasingly important role in improving the
efficiency and accuracy of learning, scaling to a large input data size, and bridging different wireless
computing resources (Dean et al., 2012; Bekkerman et al., 2011; Chilimbi et al., 2014; Chaturapruek
et al., 2015; Zhu et al., 2020; Mills et al., 2019). Distributed Stochastic Gradient Descent (SGD) is
the core in a vast majority of distributed learning algorithms (e.g., various distributed deep neural
networks), where distributed nodes calculate local gradients and an aggregated gradient is achieved
via communication among distributed nodes and/or a parameter server.
However, due to limited bandwidth in practical networks, communication overhead for transferring
gradients often becomes the performance bottleneck. Several approaches towards communication-
efficient distributed learning have been proposed, including compressing gradients (Stich et al.,
2018; Alistarh et al., 2017) or updating local models less frequently (McMahan et al., 2017). Gradi-
ent quantization reduces the communication overhead by using few bits to approximate the original
real value, which is considered to be one of the most effective approaches to reduce communication
overhead (Seide et al., 2014; Alistarh et al., 2017; Bernstein et al., 2018; Wu et al., 2018; Suresh
et al., 2017). The lossy quantization inevitably brings in gradient noise, which will affect the con-
vergence of the model. Hence, a key question is how to effectively select the number of quantization
bits to balance the trade-off between the communication cost and its convergence performance.
Existing algorithms often quantize parameters into a fixed number of bits, which is shown to be
inefficient in balancing the communication-convergence trade-off (Seide et al., 2014; Alistarh et al.,
2017; Bernstein et al., 2018). An efficient scheme should be able to dynamically adjust the number
1
Under review as a conference paper at ICLR 2021
of quantized bits according to the state of current learning model in each gradient descent step to
balance the communication overhead and model accuracy. Several studies try to construct adaptive
quantization schemes through design heuristics and/or empirical evidence. However, they do not
come up with a solid theoretical analysis (Guo et al., 2020; Cui et al., 2018; Oland & Raj, 2015),
which even results in contradicted conclusions. More specifically, MQGrad (Cui et al., 2018) and
AdaQS (Guo et al., 2020) suggest using few quantization bits in early epochs and gradually increase
the number of bits in later epochs; while the scheme proposed by Anders (Oland & Raj, 2015) states
that more quantization bits should be used for the gradient with larger root-mean-squared (RMS)
value, choosing to use more bits in the early training stage and fewer bits in the later stage. One of
this paper’s key contributions is to develop a theoretical framework to crystallize the design tradeoff
in dynamic gradient quantization and settle this contradiction.
In this paper, we propose a novel dynamic quantized SGD (DQSGD) framework for minimizing
communication overhead in distributed learning while maintaining the desired learning accuracy.
We study this dynamic quantization problem in both the strongly convex and the non-convex op-
timization frameworks. In the strongly convex optimization framework, we first derive an upper
bound on the difference (that we term the strongly convex convergence error) between the loss af-
ter N iterations and the optimal loss to characterize the strongly convex convergence error caused
by sampling, limited iteration steps, and quantization. In addition, we find some particular cases
and prove the tightness for this upper bound on part of the convergence error caused by quantiza-
tion. In the non-convex optimization framework, we derive an upper bound on the mean square of
gradient norms at every iteration step, which is termed the non-convex convergence error. Based
on the above theoretical analysis, we design a dynamic quantization algorithm by minimizing the
strongly convex/non-convex convergence error bound under communication cost constraints. Our
dynamic quantization algorithm is able to adjust the number of quantization bits adaptively by taking
into account the norm of gradients, the communication budget, and the remaining number of itera-
tions. We validate our theoretical analysis through extensive experiments on large-scale Computer
Vision (CV) and Natural Language Processing (NLP) tasks, including image classification tasks on
CIFAR-10 and CIFAR-100 and text classification tasks on AG-News. Numerical results show that
our proposed DQSGD significantly outperforms the baseline quantization methods.
To summarize, our key contributions are as follows:
•	We propose a novel framework to characterize the trade-off between communication cost and
modeling error by dynamically quantizing gradients in the distributed learning.
•	We derive an upper bound on the convergence error for strongly convex objectives and non-convex
objectives. The upper bound is shown to be optimal in particular cases.
•	We develop a dynamic quantization SGD strategy, which is shown to achieve a smaller conver-
gence error upper bound compared with fixed-bit quantization methods.
•	We validate the proposed DQSGD on a variety of real world datasets and machine learning models,
demonstrating that our proposed DQSGD significantly outperforms state-of-the-art gradient quanti-
zation methods in terms of mitigating communication costs.
2	Related Work
To solve large scale machine learning problems, distributed SGD methods have attracted a wide at-
tention (Dean et al., 2012; Bekkerman et al., 2011; Chilimbi et al., 2014; Chaturapruek et al., 2015).
To mitigate the communication bottleneck in distributed SGD, gradient quantization has been inves-
tigated. 1BitSGD uses 1 bit to quantize each dimension of the gradients and achieves the desired
goal in speech recognition applications (Seide et al., 2014). TernGrad quantizes gradients to ternary
levels {-1, 0, 1} to reduce the communication overhead (Wen et al., 2017). Furthermore, QSGD is
considered in a family of compression schemes that use a fixed number of bits to quantize gradients,
allowing the user to smoothly trade-off communication and convergence time (Alistarh et al., 2017).
However, these fixed-bit quantization methods may not be efficient in communication. To further
reduce the communication overhead, some empirical studies began to dynamically adjust the quan-
tization bits according to current model parameters in the training process, such as the gradient’s
mean to standard deviation ratio (Guo et al., 2020), the training loss (Cui et al., 2018), gradient’s
root-mean-squared value (Oland & Raj, 2015). Though these empirical heuristics of adaptive quan-
2
Under review as a conference paper at ICLR 2021
tization methods show good performance in some certain tasks, their imprecise conjectures and
the lack of theoretical guidelines in the conjecture framework have limited their generalization to a
broad range of machine learning models/tasks.
3	Problem Formulation
We consider to minimize the objective function F : Rd → R with parameter x
minχ∈Rd F(x) = Eξ〜D[l(x； ξ)],	(1)
where the data point ξ is generated from an unknown distribution D, and a loss function l(x; ξ)
measures the loss of the model x at data point ξ. Vanilla gradient descent (GD) will solve this
problem by updating model parameters via iterations x(n+1) = x(n) - ηVF(x(n)), where x(n) is
the model parameter at iteration n; η is the learning rate; VF (x(n)) is the gradient of F (x(n)). A
modification to the GD scheme, minibatch SGD, uses mini-batches of random samples with size K,
AK = {ξ0, ..., ξK-1}, to calculate the stochastic gradient g(x) = 1/K PiK=-01 Vl(x; ξi).
In distributed learning, to reduce the communication overhead, we consider to quantize the minibach
stochastic gradients:
x(n+1) = x(n) - ηQsn [g(x(n))],	(2)
where QsnH is the quantization operation that works on each dimension of g(x(n) ). The i-th com-
ponent of the stochastic gradient vector g is quantized as
Qs(gi) = kgkp ∙ sgn(gi) ∙ ζ(gi, s),	(3)
where kgkp is the lp norm of g; sgn(gi) = {+1, -1} is the sign of gi; s is the quantization level;
and ζ(gi, s) is an unbiased stochastic function that maps scalar |gi|/kgkp to one of the values in set
{0, 1/s, 2/s, . . ., s/s}: if |gi|/kgkp ∈ [l/s, (l + 1)/s], we have
with probability 1 - p,
l/s,
ζ(gi,s)	I (l + 1)/s, with probability P = S-Jgil——l
kgkp
(4)
Note that, the quantization level is roughly exponential to the number of quantized bits. If we use
B bits to quantize gi , we will use one bit to represent its sign and the other B - 1 bits to represent
ζ(gi, s), thus resulting in a quantization level s = 2B-1 - 1. In total, we use Bpre + dB bits for the
gradient quantization at each iteration: a certain number of Bpre bits of precision to construct kgkp
and dB bits to express the d components ofg.
Given a total number of training iterations N and the overall communication budget C to upload
all stochastic gradients, we would like to design a gradient quantization scheme to maximize the
learning performance. To measure the learning performance under gradient quantization, we follow
the commonly adopted convex/non-convex-convergence error δ(F, N, C) (Alistarh et al., 2017):
F (X(N ),C) — F (x*,C),
δ(F, N, C) =	1
[NN PNOIkVF (Xs) )k2,
for strongly convex F,
for non-convex F,
(5)
where x* is the optimal point to minimize F. In general, this error δ(F, N, C) is hard to determine;
instead, we aim to lower and upper bound this error and design corresponding quantization schemes.
4	Dynamic Quantized SGD
In this part, we derive upper bounds on the strongly convex/non-convex convergence error
δ(F, N, C) and lower bounds on the strongly convex-convergence error. By minimizing the up-
per bound on this convergence error, we propose the dynamic quantized SGD strategies for strongly
convex and non-convex objective functions.
3
Under review as a conference paper at ICLR 2021
4.1	Preliminaries
We first state some assumptions as follows.
Assumption 1 (Smoothness). The objective function F (x) is L-smooth, if ∀x, y ∈ Rd, kVF (x)-
VF(y)k2 6 LkX - yk2.
It implies that ∀x, y ∈ Rd, we have
F(y) ≤ F(x) + VF(x)T(y - x) + L∙∣∣y - χ∣∣2	(6)
kVF(x)k2 ≤ 2L[F(x) - F(x*)]	⑺
Assumption 2 (Strongly convexity). The objective function F(x) is μ-strongly convex, if ∃μ > 0,
F(x) — μxTX is a ConvexfunCtion.
From Assumption 2, we have: ∀x, y ∈ Rd,
F(y) ≥ F(x) + VF(x)τ(y - x) + μky - x∣∣2	(8)
Assumption 3 (Variance bound). The stochastic gradient oracle gives us an independent unbiased
estimate Vl(x; ξ) with a bounded variance:
Eξ 〜D [Vl(x; ξ)] = VF (x),	(9)
Eξ〜D[∣∣Vl(x;ξ) -VF(x)k2] ≤ σ2.	(10)
From Assumption 3, for the minibatch stochastic gradient g(x) = PK-I Vl(x; ξi)]∕K, We have
Eξ 〜D [g(x)] = VF (x)	(11)
Eξ〜D[∣∣g(x; ξ)k2] ≤ kVF(x)k2 + σ2∕K.	(12)
We have the relationship of gradients before and after quantization: Qs[g(x)] = g(x) + ^, where €
represents the quantization noise, folloWing the probability distribution that can be shoWn in Propo-
sition 1. τhe proof of Proposition 1 is given in Appendix A.
Proposition 1 (Quantization Noise magnitude). For the stochastic gradient vector g, if the quanti-
zation level is s, then the i-th component of quantization noise follows as:
p(^i) = j 叱kp
〔+
s2
≡J
s2
≡J
0 < ^i ≤ kgkp,
s
-kgkɪ ≤ ^i ≤ 0.
s
(13)
Following Proposition 1, we can get E^JQs[g]] = g and E^JkQs[g] - g∣∣2] = -^kgkp. This
6s2	p
indicates that the quantization operation is unbiased, and the variance bound of Qs [g] is directly
proportional to kgk2p and inversely proportional to s2, which means that gradients with a larger norm
should be quantized using more bits to keep E[kQs [g] - gk22] below a given noise level. τherefore,
we have the following lemma to characterize the quantization noise Qs [g].
Lemma 1. For the quantized gradient vector Qs [g], we have
E[Qs [g]] = VF(x)
σ2 d
E[kQs[g]k2] ≤ kVF(x)k2 + K +6s2kgkp
(14)
(15)
4
Under review as a conference paper at ICLR 2021
We can see that the noise various of Qs [g] contains two parts: the first part is the sampling noise σ-,
K
the second part is the quantization noise -^IlgIl 2.
6s2	p
4.2	Convergence error of strongly convex objectives
Firstly, we consider a strongly convex optimization problem. Putting the QSGD algorithm (2) on
smooth, strongly convex functions yield the following result with proof given in Appendix B.
Theorem 1	(Convergence Error Bound of Strongly Convex Objectives). For the problem in Eq.
(1) under Assumption 1 and Assumption 2 with initial parameter x(0), using quantized gradients in
Eq. (2) for iteration, we can upper and lower bound the convergence error by
E[F(X(N)) — F(x*)] ≤ αN[F(X⑼)-F(x*)]+ "*1 - ；"
2K(1 - α)
2 N-1
Ldη
n=0
αN-1-n
E[F(X(N)) — F(x*)] ≥ βN[F(x(0)) — F(x*)] + "忆((1 — 彳"
2K(1 - β)
+萼NT	Ty∣g(x(n))kp,
12 n=0	sn
where α = 1 一 2μη + Lμη2, β = 1 一 2Lη + Lμη2. The convergence error consists of three parts:
the error of the gradient descent method, which which tends to 0 as the number of iterations N
increases and also depends on the learning rate η (from the expression of α, we can see that when
η ≤ 1/L, with the increase of η, α decrease, and the convergence rate of the model is accelerated);
the sampling error, which can be reduced by increasing the batch size K or decaying the learning
rate; and the convergence error due to quantization, which we want to minimize. Note that there
is a positive correlation between the upper bound of convergence error due to quantization and the
variance of the quantization noise. The contribution of quantization noise to the error is larger at the
late stage of training. Therefore, noise reduction helps improve the accuracy of the model. In other
words, more quantization bits should be used in the later training period.
In addition, we can show that the upper and lower bound matches each other in some particular
cases. As a simple example, we consider a quadratic problem: F(x) = xTHx + ATx + B, where
the Hessian matrix is isotropic H = λI, A ∈ Rd and B is a constant. Clearly, L = μ, so α = β and
the upper is equal to the lower bound.
Theorem 2	(Convergence Error of Quadratic Functions). For a quadratic optimization problem
F(x) = xTHx + ATx + B, we consider a Gaussian noise case
x(n+1) = x(n) — ηVF(x(n)) — ηe(n), €(n) 〜N(0, Σ(x(n))).	(16)
We achieve
E [F (X(N)) — F (x*)] = 1(x(0) — x*)T(PN )THρN (x(0) — x*)
2N-1	(17)
+ ɪ X Tr[ρNT-nΣ(x(n))H(ρN-1-n)T],
n=0
where ρ = I — ηH and H is the Hessian matrix.
Detailed proof is in Appendix C.
5
Under review as a conference paper at ICLR 2021
4.3	DQSGD for strongly convex objectives
We will determine the dynamic quantization strategy by minimizing the upper bound of convergence
error due to quantization. The optimization problem is:
N-1	1
IBin X aN-1-n (2Bn-1 - 1)2 kg(x(n))kp,
N-1
X (dBn + Bpre) = C.
n=0
By solving this optimization problem, we can get
Bn = log2 [kα(N-n)/2kg(x(n))kp + 1] + 1,	(18)
where k depends on the total communication overhead C, and α is related to the convergence rate
of the model. The larger the total communication cost C is, the greater k is; the faster the model’s
convergence rate is, the smaller α is. In Appendix E, we prove that our scheme outperforms the
fixed bits scheme in terms of the convergence error.
4.4	DQSGD for non-convex objectives
In general, if we consider non-convex smooth objective functions, we can get the following theorem
with proofs given in Appendix D.
Theorem 3 (Convergence Error Bound of Non-Convex Objectives). For the problem in Eq. (1)
under Assumption 1, with initial parameter x(0), using quantized gradients in Eq. (2) for iteration,
we can upper bound the convergence error by
1 N-1	2
N x E0∣vF(x(n))k2] ≤ 2Nη - LNη2∣F(X(O))- F2"
Lησ2
(2 - Ln)K
N-1
+ 6(2 - Ln)N X 京kg(X ) )kp.
n=0 n
(19)
Similarly, the convergence error consists of three parts: the error of the gradient descent method,
which tends to 0 as the number of iterations N increases; the sampling error, which can be reduced
by increasing the batch size K or decaying the learning rate; and the convergence error due to
quantization, which we want to minimize. Thus, the optimization problem is:
N-1 1
min X 手kg(XS))Ip
Bn	sn
n=0 n
N-1
X (dBn + Bpre) =C
n=0
By solving this optimization problem, we can get
Bn = log2 [tkg(X(n))kp + 1] + 1,	(20)
where t depends on the total communication overhead C . In Appendix E, we also give a detailed
comparison of our scheme’s the upper bound of convergence error compared with fixed-bit schemes.
4.5	DQSGD in distributed learning
Next, we consider the deployment of our proposed DQSGD algorithm in the distributed learning
setting. We have a set of W workers who proceed in synchronous steps, and each worker has a
complete copy of the model. In each communication round, workers compute their local gradients
and communicate gradients with the parameter server, while the server aggregates these gradients
6
Under review as a conference paper at ICLR 2021
from workers and updates the model parameters. If gl(x(n)) is the quantized stochastic gradients
in the l-th worker and x(n) is the model parameter that the workers hold in iteration n, then the
updated value of X by the end of this iteration is: x(n+1) = x(n) + η<G(x(n)), where G(x(n))=
W PW=1 gl(x(n)). The pseudocode is given in Algorithm 2 in Appendix E .
5	Experiments
In this section, we conduct experiments on CV and NLP tasks on three datasets: AG-News (Zhang
et al., 2015), CIFAR-10, and CIFAR-100 (Krizhevsky et al., 2009), to validate the effectiveness of
our proposed DQSGD method. We use the testing accuracy to measure the learning performance and
use the compression ratio to measure the communication cost. We compare our proposed DQSGD
with the following baselines: SignSGD (Seide et al., 2014), TernGrad (Wen et al., 2017), QSGD
(Alistarh et al., 2017), Adaptive (Oland & Raj, 2015), AdaQS (Guo et al., 2020). We conduct ex-
periments for W = 8 workers and use canonical networks to evaluate the performance of different
algorithms: BiLSTM on the text classification task on the AG-News dataset, Resnet18 on the im-
age classification task on the CIFAR-10 dataset, and Resnet34 on the image classification task on
the CIFAR-100 dataset. A detailed description of the three datasets, the baseline algorithms, and
experimental setting is given in Appendix F.
Test Accuracy vs Compression Ratio. In Table 1, we compare the testing accuracy and com-
pression ratio of different algorithms under different tasks. We can see that although SignSGD,
TernGrad, QSGD (4 bits) have a compression ratio greater than 8, they cannot achieve more than
0.8895, 0.8545, 0.6840 test accuracy for AG-News, CIFAR-10, CIFAR-100 tasks, respectively. In
contrast, QSGD (6 bits), Adaptive, AdaQS, and DQSGD can achieve more than 0.8986, 0.8785,
0.6939 test test accuracy. Among them, our proposed DQSGD can save communication cost by
4.11% - 21.73%, 22.36% - 25%, 11.89% - 24.07% than the other three algorithms.
Table 1: Accuracy vs. compression ratio.
	AG-News		CIFAR-10		CIFAR-100	
	Top-1 Accuracy	Compression Ratio	Top-1 Accuracy	Compression Ratio	Top-1 Accuracy	Compression Ratio
Vanilla SGD	0.9016	1	0.8815	1	0.6969	1
SignSGD	0.8663	32	0.5191	32	0.3955	32
TernGrad	0.8480	16	0.7418	16	0.6174	16
QSGD (4 bits)	0.8894	8	0.8545	8	0.6837	8
QSGD (6 bits)	0.9006	5.33	0.8803	5.33	0.6969	5.33
Adaptive	0.8991	6.53	0.8787	5.52	0.6943	5.93
AdaQS	0.9001	6.53	0.8809	5.35	0.6960	5.11
DQSGD (Ours)	0.8997	6.81	0.8793	7.11	0.6959	6.73
Fixed Bits vs. Adaptive Bits. Figure 1 shows the comparison results of fixed bit algorithm QSGD
and our proposed DQSGD on CIFAR-10. Figure 1 (a) and Figure 1 (b) show the testing accuracy
curves and the training loss curves, respectively. Figure 1 (c) shows the bits allocation of each
iteration of DQSGD, and Figure 1 (d) represents the communication overhead used in the training
process of different quantization schemes. From these results, we can see that although QSGD
(2 bits) and QSGD (4 bits) have less communication cost, they suffer up to about 14% and 2.7%
accuracy degradation compared with Vanilla SGD. The accuracy of QSGD (6 bits) and DQSGD
is similar to that of Vanilla SGD, but the communication overhead of DQSGD is reduced up to
25% compared with that of QSGD (6 bits). This shows that our dynamic quantization strategy can
effectively reduce the communication cost compared with the fixed quantization scheme. Figure 2
shows the accuracy of QSGD and DQSGD under different compression ratios. It can be seen that
DQSGD can achieve higher accuracy than QSGD under the same communication cost.
7
Under review as a conference paper at ICLR 2021
(a) Testing accuracy
(b) Training loss
(c) Bits allocation
Figure 1: The comparison results of QSGD and DQSGD on CIFAR-10.
(d) Communication overhead
Compression Ratio
Figure 2: Testing accuracy of QSGD and DQSGD under different compression ratios on CIFAR-10.
8
Under review as a conference paper at ICLR 2021
References
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. QSGD:
Communication-efficient SGD via gradient quantization and encoding. In Advances in Neural
Information Processing Systems,pp. 1709-1720, 2017.
Ron Bekkerman, Mikhail Bilenko, and John Langford. Scaling up machine learning: Parallel and
distributed approaches. Cambridge University Press, 2011.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. SIGNSGD:
Compressed optimisation for non-convex problems. arXiv preprint arXiv:1802.04434, 2018.
Sorathan Chaturapruek, John C Duchi, and Christopher Re. Asynchronous stochastic convex op-
timization: the noise is in the noise and SGD don’t care. In Advances in Neural Information
Processing Systems, pp. 1531-1539, 2015.
Trishul Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. Project adam:
Building an efficient and scalable deep learning training system. In 11th USENIX Symposium
on Operating Systems Design and Implementation (OSDI), pp. 571-582, 2014.
Guoxin Cui, Jun Xu, Wei Zeng, Yanyan Lan, Jiafeng Guo, and Xueqi Cheng. MQGrad: Rein-
forcement learning of gradient quantization in parameter server. In Proceedings of the 2018 ACM
SIGIR International Conference on Theory of Information Retrieval, pp. 83-90, 2018.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Marc’aurelio
Ranzato, Andrew Senior, Paul Tucker, Ke Yang, et al. Large scale distributed deep networks. In
Advances in Neural Information Processing Systems, pp. 1223-1231, 2012.
Jinrong Guo, Wantao Liu, Wang Wang, Jizhong Han, Ruixuan Li, Yijun Lu, and Songlin Hu. Ac-
celerating distributed deep learning by adaptive gradient quantization. In IEEE International
Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1603-1607. IEEE, 2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
2009.
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Artificial Intelli-
gence and Statistics, pp. 1273-1282. PMLR, 2017.
Jed Mills, Jia Hu, and Geyong Min. Communication-efficient federated learning for wireless edge
intelligence in IoT. IEEE Internet of Things Journal, 2019.
Anders Oland and Bhiksha Raj. Reducing communication overhead in distributed learning by an
order of magnitude (almost). In IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 2219-2223, 2015.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and its
application to data-parallel distributed training of speech DNNs. Conference of the International
Speech Communication Association, pp. 1058-1062, 2014.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified SGD with memory. In
Advances in Neural Information Processing Systems, pp. 4447-4458, 2018.
Ananda Theertha Suresh, Felix X Yu, Sanjiv Kumar, and H Brendan Mcmahan. Distributed mean
estimation with limited communication. International Conference on Machine Learning, pp.
3329-3337, 2017.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. TernGrad:
Ternary gradients to reduce communication in distributed deep learning. Advances in Neural
Information Processing Systems, pp. 1508-1518, 2017.
9
Under review as a conference paper at ICLR 2021
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang. Error compensated quantized SGD
and its applications to large-scale distributed optimization. arXiv preprint arXiv:1806.08054,
2018.
Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text clas-
Sification. In Advances in Neural Information Processing Systems, pp. 649-657, 2015.
Guangxu Zhu, Dongzhu Liu, Yuqing Du, Changsheng You, Jun Zhang, and Kaibin Huang. Toward
an intelligent edge: wireless communication meets machine learning. IEEE Communications
Magazine, 58(1):19-25, 2020.
10
Under review as a conference paper at ICLR 2021
A Proof of Proposition 1
Suppose 7rgiπ 〜 U(ɪ ^ɪɪ) and let Ei = 7-gii——Z(gi, s), for 0 < ∈o < LWe have:
∣g∣p S S	i ∣g ∣p	S
p{ei = e0} = p{ ττgp =
∣g ∣p
1
S - e0
l+e0}∙ p{ ⅛
l
一 十 e0}
S
Similarly, for -- < e0 < 0, we have:
S
0 0 _ I _ g gi _ l + 1 I l J + 1I gi
p{ei = e0} = pV∣j-1∣- =--------+ e0} ∙ p{----lT^fT
i	∣g∣p S	S	∣g ∣p
1
S + e0
S
= S + S20
l +1
丁+e0}
Considering that Qs(gi) = ∣∣gkp ∙ sgn(gi) Y(gi, S) and let ^ = gi - Qs(gi), so We have:
p(^i) = ∖ kgSlp
------+
kgkp 十
S2 ʌ
≡τi
S2 .
≡τi
o < ^i ≤ ∣g∣p
S
-∣g∣p ≤ ^i ≤ o
S
S
S - S 0
—
B Proof of Theorem 1
Considering function F is L - smooth, and using Assumption 1, we have:
F(x(n+1)) ≤ F(Xs)) + VF(Xs))T(x(n+1) — Xs)) + Lkx(n+1) — x(n)∣2
For QSGD, x(n+1) = x(n) - ηQsn[g(x(n))], so:
F(x(n+1)) ≤ F(x(n)) + VF(Xs))T(-ηQsn[g(x⑺)]) + L∣-ηQsn[g(x(n))]∣2
Lη2
=F(x(n)) - ηVF(x(n))TQsn[g(x(n))] + LηkQsn [g(x(n))]k2
Taking total expectations, and using Lemma 1, this yields:
e[f(x(n+1))] ≤ F(x(n)) + (-η + ⅛∣vf(x(n))k2 + Lη2σ2- + Lη⅛⅛g(x⑺)kp
2	2K	12S2n
11
Under review as a conference paper at ICLR 2021
Considering that function F is μ - strongly convex, and using Assumption 2, so:
Lη2 σ2
E[F(x(n+1))] ≤ F(x(n)) - (2μη - Lμη2)[F(X⑺)-F(x*)] + L^
2K
+ L2S2dkg(XS))kp
Subtracting F(x*) from both sides, and let α = 1 - 2μη + Lμη2, so:
E[F(x(n+1)) - F(x*)] ≤ α[F(XS))- F(x*)] + LK2 + L^kgWn))kp
Applying this recursively:
E[F(X(N)) - F(x*)] ≤ αN[F(x(0)) - F(x*)] +
Lη2σ2 (1 - αN )
2 N-1
+ LdL X QN-1-n
12乙
n=0
2K(1 - α)
= kg(x(n) )kp
sn
Similarly, let β = 1 - 2Lη + Lμη2
E[F(x(N)) - F(x*)] ≥ βN[F(x(0)) - F(x*)] +
μη2σ2(1 — βN)
2K(1 -β)
d 2 N-1	1
+* X βN-1-n ɪ kg(x(n))kp
12 n=0	sn
C Proof of Theorem 2
Both SGD and QSGD can be considered a general kind of optimization dynamics, namely, gradient
descent with unbiased noise. Based on the central limit theorem, it is assumed that the noise caused
by sampling and quantization obeys Gaussian distribution, that is, Qsn [g(x(n))] = VF(x(n)) +
€(n), €(n)〜 N(0, Σ(x(n))). Therefore, we can consider Equation (2) as the discrimination of the
Gaussian process:
x(n+1) = x(n) - ηVF(x(n)) - ηe(n), e(n) 〜N(0, Σ(x(n)))	(21)
The error for general Gaussian processes is hard to analyze due to the intractableness of the integrals,
so we only consider a quadratic problem:
x(n+1) = x(n) - ηVF(x(n)) - η(n)
= x(n) - η[Hx(n) + A] - η(n)
= (I - ηH)x(n) -ηA-η(n)
Considering VF(x*) = ηA + ηHx* = 0, subtracting x* from both sides, and rearranging, this
yields:
x(n+1) -x* = (I - ηH)x(n) -ηA-x* - η(n)
= (I- ηH)(x(n) -x*) -ηA-ηHx* - η(n)
= (I - ηH)(x(n) -x*) - η(n)
12
Under review as a conference paper at ICLR 2021
Applying this recursively, let ρ = I - ηH, we have:
N-1
X(N) - x* = PN (X(O)-x*) - X [ηPNT-ne(n)]
n=0
Considering that €(n) 〜N(0, Σ(x(n))), then:
N-1	N-1
X [ηρN-1-%(n)] = X [ηρN-1-n∑(x(n)) 1 N(o,i)]
n=0	n=0
N-1
=X {√ηPN-1-n∑(x(n))2 [W(n + 1) - W(n)]}
n=0
≡ I(N)
where, W is a standard d - dimensional Wiener process, and I(N) is an Ito integral. Hence x(N)
x* + PN (x(0) - x*) - I(N), then:
F (X(N )) = IX(N )THx(N) + ATX(N) + B
=1(x(0) - X*)T(PN)tHPN(X(O) - x*) + 1i(N)tHI(N)
- [PN (x(0) -x*) +x* + A]THI(N) +F(x*)
Subtracting F(x*) from both sides, taking total expectations, and rearranging, this yields:
E[F(X(N)) - F(x*)] = 1(x(0) - x*)τ(ρN)τHρN(X(O) - x*) + ∣E[I(N)tHI(N)]
-[PN (x(O) -x*) +x* + A]THE[I(N)]
The property of Ito integral I(N) is:
E[I(N)] = 0
N-1
E[I(N)THI(N)] = X η2Tr[PN-1-nΣ(x(n))H(PN-1-n)T]
n=O
Using this property, we have:
E[F(x(N)) - F(x*)] = 1(x(0) - x*)τ(ρN)τHρN(X(O) - x*)
2 N-1
+ n X Tr[ρNT-nΣ(x(n))H(PN-1-n)t]
n=O
D Proof of Theorem 3
Considering function F is L - smooth, using the result of Appendix B, we have:
E[F(x(n+1))] ≤ F(x(n)) + (-η+ ⅛2)kVF(x(n))k2 + Lη2σ2 + Lη⅞kg(x(n))kp
2	2K	12s2n
13
Under review as a conference paper at ICLR 2021
Subtracting F (x(n)) from both sides, then applying it recursively, this yields:
E[F(X(N)) - F(x(0))] ≤ -(η - Lη2) X E[kVF(x(n))k2] + LNK⅛2
n=0
2N-1
+鲁 S *F
Considering that F(X(N)) ≥ F(x*), so:
1 N-1	2	L 2
M X E[kVF(x(n))k2] ≤ *-2-F-χ[F(X(O))- F(X*)]+	η
N	2N η - LN η2	(2 - Lη)K
N-1
+ 而―T W X Fkg(Xs))kp
6(2 - Lη)N n=0 s2n	p
E Algorithm
Algorithm 1 Dynamic quantized SGD
1:	Input: Learning rate η, Initial point X(0) ∈ Rd, Hyperparametric k, α
2:	for each iteration n = 0,1, ..., N -1: do
3:	g(X(n)) J compute gradient of a batch of data
4:	∣∣g(X(n))k J calculate the norm of g(X(n))
5:	Bn J determine the quantization bits
6:	g(X(n)) J quantize (g(X(n)), Bn)
7:	Update the parameter: X(n+1) = X(n) 一 ng(X(n))
8:	end for
We make an assumptions as follows.
Assumption 4. (Second moment bound). If F(X) is L - smooth, so the lp norm of the minibach
stochastic gradient g(X) satisfied:
kg(X)kp ≤ 2L[F(X)- F(X*)]γ	(22)
It is noted that Assumption 4 is a generalization of Equation (7).
Based on this quantization scheme 18 and Assumption 4, we can get the quantization error:
0
δDQSGD ≤
L2η2dαN-1[F(X(O))- F(X*)]γN…)(N-1)/2
6 X 4(C-32N-dN)/dN	Na
Accordingly, if we fix the bits, the quantization error is:
δ <L2η2daN-1[F(X(O))- F(X*)]γX* 1 (γ-i)n
δFixed ≤	6 × 4(C-32N-dN)/dN	乙 0
n=O
(23)
(24)
Comparing (23) and (24), we can see that our scheme reduces the error bound about:
δFixed - δDQSGD ≈
L2η2daN-1[F(X(O)) - F(x*)]Y∖
6 × 4(C-32N-dN)/dN	λ1
(25)
14
Under review as a conference paper at ICLR 2021
Algorithm 2 Dynamic QSGD in Distributed Learning
1:	Input: Learning rate η, Initial point x(0) ∈ Rd, Hyperparametric k, α
2:	for each iteration n = 0, 1, ..., N - 1: do
3:	On each worker l = 1, ..., W:
4:	gl (x(n)) J compute gradient w.r.t. a batch of data
5:	gl(x(n)) J quantize (gl(x(n)),Bn)
6:	send gl(x(n)) to server
7:	receive g(x(n)) and Bn+1 from server
8:	On server:
9:	collect all W gradients gl (x(n)) from workers
10:	average: G(X(n))=+ PW=I gl(x(n))
11:	IlG(X(n))k J calculate the norm of g(X(n))
12:	Bn+ι J Determine the quantization bits for the next iteration (k(G(x(n))k)
13:	send G(X(n)) and Bn+ι to all workers
14:	end for
where λ1 = PN=01 α(γ-1)n 一 Na(YT)(N-i)/2 is the difference between arithmetic mean and
geometric mean. When γ = 1, λ1= 0. Ifγ 6= 1, λ1> 0.
Based on quantization scheme (20) and Assumption 4, the quantization error is:
δ000	≤	L2ηd[F(X(O)) - F(x*)]γ	N γ(Ν-1)/2	(26)
δDQSGD ≤ 3N(2- Lη) × 4(C-32N-dN)/dNNa	(26)
Accordingly, if we fix the bits, the quantization error is:
δ000	<	L2ηd[F(X(O))- F(X*)]γ	X Yn
δFixed ≤ 3N(2 一 Ln) × 4(C-32N-dN)/dN J a
Comparing (26) and (27), we can see that our scheme reduces the error bound about:
000	000	〜	L2ηd[F(X(O)) - F(x*)]Y
δFiXed - Sdqsgd ≈ 3N(2 - Lη) × 4(C-32N-dN)/dNλ2
(27)
(28)
where λ2 = PnN=-O1 aYn 一 N aY(N-1)/2 is the difference between arithmetic mean and geometric
mean. Consider that γ 6= 0, so λ2 > 0.
F	Experiments
F.1 Datasets and baseline
We evaluate our method DQSGD on three datasets: AG-News, CIFAR-10, and CIFAR-100. AG-
News dataset (Zhang et al., 2015) contains four categorized news articles, and the number of training
samples for each class is 30000 and testing 1900. CIFAR-10/100 (Krizhevsky et al., 2009) dataset
are all contain 60,000 32 × 32 RGB images, which are divided into 10 and 100 classes, respectively.
We compare DQSGD with the following gradients quantization methods:
• SignSGD (Seide et al., 2014): To take the sign of each coordinate of the stochastic gradient vector.
• TernGrad (Wen et al., 2017): Quantizes gradients to ternary levels {一1; 0; 1}.
15
Under review as a conference paper at ICLR 2021
•	QSGD (Alistarh et al., 2017): It is a family of compression schemes. The specific quantization
operation is shown in equation 3. In our experiments, we replace the kgk2 in the original text with
kgk∞ .
•	Adaptive (Oland & Raj, 2015): This dynamic scheme considers that for the gradient with larger
root-mean-squared (RMS) value, more quantization bits are used.
•	AdaQS (Guo et al., 2020): It is an adaptive quantization scheme that using few quantization bits
in the early epochs and gradually increase bits in the later epochs.
Table 2: Baselines
	Unbiased	Basis for determining bits
SignSGD (Seide et al., 2014)	No	Fixed bits
TernGrad (Wen et al., 2017)	-Yes	Fixed bits
QSGD (Alistarh et al., 2017)~	-Yes	Fixed bits	一
Adaptive (Oland & Raj, 2015)	YeS	Gradient,s root-mean-squared value
AdaQS(GUo et al., 2020)	Yes	Gradient,s mean to standard deviation ratio, Iteration number
F.2 Experimental setup
We conduct simulations for W = 8 workers. For AG-News, we use 300-dim embeddings pre-trained
on Glove; then, each word can be further encoded sequentially using two layers bidirectional LSTM
(BiLSTM). Furthermore, we use the self-attention mechanism to obtain the sentence embedding.
The classifier is two fully connected layers of size 128 and 4 neurons, respectively. We training
CIFAR-10 on Resnet18 (He et al., 2016) and CIFAR-100 on Resnet34 (He et al., 2016), respectively.
Other parameters information is shown in Table 3. All results were the average of four random runs.
Table 3: Parameters
Dataset	Net	Learning rate	Batchsize	Interations	Hyperparameters in DQSGD
AG-News	BiLSTM	0005	-32-	-1000-	k = 5,α = 0.994
CIFAR-10	ReSnet18	01	-32-	-6000-	k = 20,α = 0.999
CIFAR-100	ReSnet34	0.01	64	6000	k =10,α = 0.999
16