Under review as a conference paper at ICLR 2021
Orthogonal Over-Parameterized Training
Anonymous authors
Paper under double-blind review
Ab stract
The inductive bias of a neural network is largely determined by the architecture
and the training algorithm. To achieve good generalization, how to effectively
train a neural network is of great importance. We propose a novel orthogonal
over-parameterized training (OPT) framework that can provably minimize the hy-
perspherical energy which characterizes the diversity of neurons on a hypersphere.
By maintaining the minimum hyperspherical energy during training, OPT can
greatly improve the empirical generalization. Specifically, OPT fixes the randomly
initialized weights of the neurons and learns an orthogonal transformation that
applies to these neurons. We consider multiple ways to learn such an orthogonal
transformation, including unrolling orthogonalization algorithms, applying orthogo-
nal parameterization, and designing orthogonality-preserving gradient descent. For
better scalability, we further propose the stochastic OPT which performs orthogonal
transformation stochastically for partial dimensions of the neuron. Interestingly,
OPT reveals that learning a proper coordinate system for neurons is crucial to
generalization. We provide some insights on why OPT yields better generalization.
Extensive experiments validate the superiority of OPT.
1 Introduction
The inductive bias encoded in a neural network is generally determined by two major aspects: how
the neural network is structured (i.e., network architecture) and how the neural network is optimized
(i.e., training algorithm). For the same network architecture, using different training algorithms could
lead to a dramatic difference in generalization performance [33, 54] even if the training loss is already
close to zero, implying that different training procedures lead to different inductive biases. Therefore,
how to effectively train a neural network that can generalize well remains an open challenge.
Recent theories [15, 15, 16, 31, 42] suggest the importance
of over-parameterization in linear neural networks. For
example, [16] shows that optimizing an underdetermined
quadratic objective over a matrix M with gradient descent
on a factorization of M leads to an implicit regulariza-
tion that may improve generalization. There is also strong
empirical evidence [11, 46] that over-parameterzing the
convolutional filters under some regularity is beneficial to
generalization. Our paper aims to leverage the power of
over-parameterization and explore more intrinsic structural
priors in order to train a well-performing neural network.
Final neurons
Figure 1: Overview of OPT.
[Learning the coordinate system J
Motivated by this goal, we propose a generic orthogonal over-parameterized training (OPT) frame-
work for neural networks. Different from earlier methods, OPT over-parameterizes a neuron w ∈Rd
with the multiplication of a learnable layer-shared orthogonal matrix R∈Rd×d and a fixed randomly
initialized weight vector v ∈Rd, and it follows that the equivalent weight for the neuron is w =Rv.
Once each element of the neuron weight v has been randomly initialized by a zero-mean Gaussian dis-
tribution [14, 19], we fix them throughout the entire training process. Then OPT learns a layer-shared
orthogonal transformation R that is applied to all the neurons (in the same layer). An illustration
of OPT is given in Fig. 1. In contrast to standard neural training, OPT decomposes the neuron into
an orthogonal transformation R that learns a proper coordinate system and a weight vector v that
controls the specific position of the neuron. Essentially, the weights {vι, •一，Vn ∈Rd} of different
neurons determine the relative positions, while the layer-shared orthogonal matrix R specifies the
coordinate system. The decoupled parameterization enables strong modeling flexibility.
1
Under review as a conference paper at ICLR 2021
Another motivation of OPT comes from an empirical observation [45] that neural networks with lower
hyperspherical energy generalize better. Hyperspherical energy quantifies the diversity of neurons
on a hypersphere, and essentially characterizes the relative positions among neurons via this form
of diversity. [45] introduces hyperspherical energy as a regularization in the network but does not
guarantee the hyperspherical energy can be effectively minimized (due to the existence of data fitting
loss). To address this issue, we leverage the property of hyperspherical energy that it is independent
of the coordinate system in which the neurons live and only depends on their relative positions.
Specifically, we prove that, if we randomly initialize the neuron weight v with certain distributions,
these neurons are guaranteed to attain minimum hyperspherical energy in expectation. It follows that
OPT maintains the minimum energy during training by learning a coordinate system (i.e., layer-shared
orthogonal matrix) for the neurons. Therefore, OPT can well minimize the hyperspherical energy.
We consider several ways to learn the orthogonal transformation. First, we unroll different orthogonal-
ization algorithms such as Gram-Schmidt process, Householder reflection and LoWdin's symmetric
orthogonalization. Different unrolled algorithms yield different implicit regularizations to construct
the neuron Weights. For example, symmetric orthogonalization guarantees that the neW orthogonal
basis has the least distance in the Hilbert space from the original non-orthogonal basis. Second, We
consider to use a special parameterization (e.g., Cayley parameterization) to construct the orthogonal
matrix, Which is more efficient in training. Third, We consider an orthogonal-preserving gradient
descent to ensure that the matrix R stays orthogonal after each gradient update. Last, We relax
the original optimization problem by making the orthogonality constraint a regularization for the
matrix R. Different Ways of learning the orthogonal transformation may encode different inductive
biases. We note that OPT aims to use orthogonalization as a tool to learn neurons that preserve
the hyperspherical energy, rather than to study a specific orthogonalization method. Moreover, We
propose a refinement strategy to further reduce the hyperspherical energy for the randomly initialized
neuron weights {vι, ∙∙∙ , Vn}. In specifical, we directly minimize the hyperspherical energy of these
random Weights as a preprocessing step before training them on actual data.
To improve scalability, we further propose the stochastic OPT that randomly samples neuron dimen-
sions to perform orthogonal transformation. The random sampling process is repeated many times
such that each dimension of the neuron is sufficiently learned. Finally, we provide some theoretical
insights and discussions to justify the effectiveness of OPT. The advantages of OPT is as follows:
•	OPT is a generic neural training framework with strong interpretability and flexibility. There are
many ways to learn the orthogonal transformation and each one imposes a unique inductive bias.
•	OPT is the first training method where the hyperspherical energy is provably minimized, leading to
better empirical generalization. OPT reveals that learning a proper coordinate system is crucial to
generalization, and the relative neuron positions are well characterized by hyperspherical energy.
•	There is no extra computational cost for OPT-trained neural networks in inference. It has the same
inference speed and model size as its standard counterpart. Our experiments also show that OPT
performs well on different neural networks and therefore is architecture-agnostic.
•	Stochastic OPT can greatly improve the scalability of OPT while having comparable performance.
2	Related Work
Orthogonality in Neural Networks. Orthogonality has been widely adopted to improve neural
networks. [4, 7, 48] uses orthogonality as a regularization for neural network. [3, 24, 28, 39, 52, 65]
uses principled orthogonalization methods to guarantee the neurons are orthogonal to each other. In
contrast to these works, OPT does not encourage orthogonality among neurons. Instead, OPT utilizes
principled orthogonalization methods to learn orthogonal transformations on neurons.
Parameterization of Neurons. There are various ways to parameterize a neuron for different
applications. [11] over-parameterizes a 2D convolution kernel by combining a 2D kernel of the
same size and two additional 1D asymmetric kernels. The resulted convolution kernel has the same
effective parameters during testing but more parameters during training. [46] constructs a neuron
with a bilinear parameterization and regularizes the bilinear similarity matrix. [68] reparameterizes
the neuron matrix with an adaptive fastfood transform to compress model parameters. [27, 44, 63]
employ sparse and low-rank structures to construct convolution kernels for a efficient neural network.
Hyperspherical Learning. [9, 47, 48] propose to learn representations on hypersphere and show that
the angular information in neural networks, in contrast to magnitude information, preserves the most
semantic meaning. [45] defines hyperspherical energy that quantifies the diversity of neurons on a
hypersphere and empirically shows that the minimum hyperspherical energy improves generalization.
2
Under review as a conference paper at ICLR 2021
3	Orthogonal Over-Parameterized Training
3.1	General Framework
OPT parameterizes the neuron as the multiplication of an orthogonal matrix R∈Rd×d and a neuron
weight vector V ∈ Rd, and the equivalent neuron weight becomes W = Rv. The output y of this
neuron can be represented by y = (Rv)>x where X ∈Rd is the input vector. In OPT, We fix the
randomly initialized neuron weight v and only learn the orthogonal matrix R. In contrast, the
standard neuron is directly formulated as y = v>x, where the weight vector V is learned in training.
As an illustrative example, we consider a two-layer linear MLP with a loss function L (e.g., the least
squares loss: L(e1, e2) = (e1 - e2)2). Specifically, the learning objective of the standard training is
min{vi,ui,∀i} Pjm=1 L y, Pin=1 uiVi>xj , while differently, our OPT is formulated as
mn
min X L(y, X Ui(Rvi)>xj) s,t. R>R = RR> = I	(1)
{R,ui,∀i}
j=1	i=1
where Vi ∈Rd is the i-th neuron in the first layer, and U = {uι, ∙∙∙ , un}∈Rn is the output neuron
in the second layer. In OPT, each element of Vi is usually sampled from a zero-mean Gaussian
distribution, and is fixed throughout the entire training process. In general, OPT learns an orthogonal
matrix that is applied to all the neurons instead of learning the individual neuron weight. Note that,
we usually do not apply OPT to neurons in the output layer (e.g., u in this MLP example, and the
final linear classifiers in CNNs), since it makes little sense to fix a set of random linear classifiers.
Therefore, the central problem is how to learn these layer-shared orthogonal matrices.
3.2	Hyperspherical Energy Perspective
We delve into OPT from the hyperspherical energy perspective. Following [45], the hyperspherical
energy of n neurons is defined as E(Vi∣ii=ι)= Pn=ι Pn=Ij=i kVi -VjkT in which Vi = is
the i-th neuron weight projected onto the unit hypersphere Sd-1 = {v ∈Rd∣∣∣Vk = 1}. Hyperspherical
energy is used to characterize the diversity of n neurons on a unit hypersphere. Assume that we
have n neurons in one layer, and we have learned an orthogonal matrix R for these neurons. The
hyperspherical energy of these n OPT-trained neurons is given by
nn	nn
E(Rviin=I) = X X kRvi- RvjrI = X X kvi- vjk-1 = E(vi∣n=ι)	⑵
i=1 j=1,j6=i	i=1 j=1,j6=i
which shows that the hyperspherical energy does not change in OPT. Moreover, [45] proves that
minimum hyperspherical energy corresponds to the uniform distribution over the hypersphere. As a
result, if the initialization of the neurons in the same layer follows the uniform distribution over the
hypersphere, then we can guarantee that the hyperspherical energy is minimal in a probabilistic sense.
Theorem 1. For the neuron h={hι, •一，hd} where hi, ∀i are initialized i.i.d. following a zero-mean
GaussIan distribution (ι.e., h 〜N(0, σ2)), the projections onto a unit hypersphere h=h∕kh∣∣ where
khk = /∑ih2 are uniformly distributed on the unit hypersphere Sd-1. The neurons with minimum
hyperspherical energy attained asymptotically approach to the uniform distribution on Sd-1.
Theorem 1 implies that, if we initialize the neurons in the same layer with zero-mean Gaussians, the
corresponding expected hyperspherical energy is guaranteed to be small. It is because the neurons are
uniformly distributed on the unit hypersphere and hyperspherical energy quantifies the uniformity on
the hypersphere in some sense. More importantly, prevailing neuron initializations such as [14] and
[19] are zero-mean Gaussian distribution. Therefore, our neurons naturally have low hyperspherical
energy from the beginning. Appendix L gives geometric properties of the random initialized neurons.
3.3 Unrolling Orthogonalization Algorithms
To learn the orthogonal transformation, we consider to unroll
classic orthogonalization algorithms and embed them into the
neural network such that the training is still end-to-end. We
need to make every step of the orthogonalization algorithm
differentiable, and the brief training flow is shown in Fig. 2.
Trainable
matrix: P
Untrainable neuron weight:
{V1,V2, ...,Vn}
Orthogonalization:	Final neuron weight:
R - GIth(P)	{Rvι,Rv2,…,Rvn}
Forward
Pass
Backward
Gradient
Figure 2: Unrolled orthogonalization.
Gram-Schmidt Process. This method takes a linearly independent set and produces an orthogonal
set based on it. The Gram-Schmidt Process (GS) usually takes the following steps to orthogonalize
3
Under review as a conference paper at ICLR 2021
a set of vectors {uι, ∙∙∙ , Un}∈Rn×n and obtain an orthogonal set {eι, ∙∙∙ , ei,…,en}∈ Rn×n.
First, when i = 1, we have eι = ^^ where eι = uι. Then, when n ≥ i ≥ 2, we have ei = ^^ where
ei=Ui - Pj=I Projej (Ui). Projb(a) = habbib is the projection operator. For better orthogonality, we
can unroll an iterative version [23] of GS with multiple steps.
Householder Reflection. A Householder reflector is defined as H = I - 2 UU>2 where U is perpen-
dicular to the reflection hyperplane. In QR factorization, Householder reflection (HR) is used to
transform a (non-singular) square matrix into an orthogonal matrix and an upper triangular matrix.
Given a matrix U = {uι,…，Un}∈ Rn×n, we consider the first column vector uι. We use House-
holder reflector to transform uι to ei = {1,0,…，0}. Specifically, we construct an orthogonal matrix
Hi with Hi = I - 2 ⑷1-#"-L)Uuk-ku21keι) The first column of HiU becomes {∣∣uι∣∣, 0, ∙∙∙ , 0}.
At the k-th step, we can view the sub-matrix U(k:n,k:n) as a new U, and use the same procedure to
construct the Householder transformation Hk ∈R(n-k)×(n-k). We construct the final Householder
transformation as Hk =Diag(Ik, Hk). Now we can gradually transform U to an upper triangular
matrix with n Householder reflections. Therefore, we have that Hn •…H2Hi U = Rup where Rup
is an upper triangular matrix and the obtained orthogonal set is Q> = Hn …H2Hi.
LoWdin's Symmetric Orthogonalization. Let the matrix U = {uι, ∙ , Un}∈Rn×n be a given set
of linearly independent vectors in an n-dimensional space. A non-singular linear transformation A
can transform the basis U to an orthogonal basis R: R= UA. The matrix R will be orthogonal
if R>R=(UA)>UA=A>MA=I where M =U>U is the Gram matrix of the given set U.
We obtain a general solution to the orthogonalization problem via the substitution: A = M- 2 B
where B is an arbitrary unitary matrix. The specific choice B = I gives the LOWdin's symmetric
orthogonalization (LS): R = UM- 2. We can analytically obtain the symmetric orthogonalization
from the singular value decomposition: U= WΣV >. Then LS gives R= WV > as the orthogonal
set for U. LS has a unique property which the other orthogonalizations do not have. The orthogonal
set resembles the original set in a nearest-neighbour sense. Specifically, LS guarantees that Pi kRi -
Ui k2 (where Ri and Ui are the i-th column of R and U, respectively) is minimized. Intuitively, LS
indicates the gentlest pushing of the directions of the vectors in order to get them orthogonal.
Discussion. These orthogonalization algorithms are fully differentiable and end-to-end trainable.
For better orthogonality, these algorithms can be used repeatedly and unrolled with multiple steps.
Empirically, one-step unrolling already works well. Givens rotations can also construct the orthogonal
matrix, but it requires traversing all lower triangular elements in the original set U, which takes
O(n2) complexity and is too costly. Interestingly, each orthogonalization encodes a unique inductive
bias to the neurons by imposing implicit regularizations (e.g., least distance in Hilbert space for LS).
Details about these orthogonalizations are in Appendix A. Unrolling orthogonalization has been
considered in different scenarios [24, 50, 62]. Many more advanced orthogonalization methods [38]
can also be applied in OPT, but exhaustively applying them to OPT is out of the scope of this paper.
3.4	Orthogonal Parameterization
A convenient way to ensure orthogonality while learning the matrix R is to use a special parameteri-
zation that inherently guarantees orthogonality. The exponential parameterization use R=exp(W)
(where exp(∙) denotes the matrix exponential) to represent an orthogonal matrix from a skew-
symmetric matrix W. The Cayley parameterization (CP) is a Pade approximation of the exponential
parameterization, and is a more natural choice due to its simplicity. CP uses the following transform
to construct an orthogonal matrix R from a skew-symmetric matrix W: R = (I + W)(I - W)-i
where W = -W>. We note that CP only produces the orthogonal matrices with determinant 1,
which belong to the special orthogonal group and thus R∈ SO(n). Specifically, it suffices to learn
the upper or lower triangular of the matrix W with unconstrained optimization to obtain a desired
orthogonal matrix R. Cayley parameterization does not cover the entire orthogonal group and is less
flexible in terms of representation power, which serves as a explicit regularization for the neurons.
3.5	Orthogonality-Preserving Gradient Descent
An alternative way to guarantee orthogonality is to modify the gradient update for the matrix R. The
idea is to initialize R with an arbitrary orthogonal matrix and then ensure each gradient update is to
apply an orthogonal transformation to R. It is essentially conducting gradient descent on the Stiefel
manifold [3, 21, 30, 39, 41, 64, 65]. Given a matrix U(0) ∈ Rn×n that is initialized as an orthogonal
4
Under review as a conference paper at ICLR 2021
matrix, We aim to construct an orthogonal transformation as the gradient update. We use the Cayley
transform to compute a parametric curve on the Stiefel manifold Ms = {U ∈ Rn×n : U>U =I} With
a specific metric via a skeW-symmetric matrix W and use it as the update rule:
Y(λ) = (I - 2W厂 1(I + λW)U(i), U(i+1) = Y(λ)	(3)
where W = Vf (U(i))U>) - 1 Uci)(U>)Vf (U(i)U>)) and W = W- W>. U(i)denotes the or-
thogonal matrix in the i-th iteration. Vf (U(i)) denotes the original gradient of the loss function w.r.t.
U(i). We term this gradient update as orthogonal-preserving gradient descent (OGD). To reduce the
computational cost of the matrix inverse in Eq. 3, we use an iterative method [41] to approximate the
Cayley transform without matrix inverse. We arrive at the fixed-point iteration from Eq. 3:
Y (λ) = U(i) + 2 W (U(i) + Y(λ))	(4)
which converges to the closed-form Cayley transform with a rate of o(λ2+n) (n is the iteration
number). In practice, we find that two iterations will suffice for a reasonable approximation accuracy.
3.6	Relaxation to Orthogonal Regularization
We consider relaxing the original optimization with an orthogonality constraint to an unconstrained
optimization with orthogonality regularization (OR). Specifically, we remove the orthogonality
constraint, and adopt an orthogonality regularization for R, i.e., kR>R- Ik2F. However, OR cannot
guarantee the energy stays unchanged. Taking Eq. 1 as an example, the objective becomes
mn
Rmin ∙X L® X Ui(Rvi)>xj) + βkR>R - I kF	⑸
R,u ,∀i
, i, j=1	i=1
where β is a hyperparameter. This serves as an relaxation of the original OPT objective. Note that, OR
is imposed to R and is quite different from the existing orthogonal regularization on neurons [4, 48].
3.7	Refining the Random Initialization as Preprocessing
Minimizing hyperspherical energy before training. Because we randomly initialize the neurons
{vι, ∙∙∙ , Vn}, there exists a variance that makes the hyperspherical energy deviate from the minima
even if the hyperspherical energy is minimal in a probabilistic sense. To further reduce the hyper-
spherical energy, we propose to refine the random initialization by minimizing its hyperspherical
energy as a preprocessing step before the OPT training. Specifically, before feeding these neurons to
OPT, we first minimize the hyperspherical energy of the initialized neurons with gradient descent
(without fitting the training data). Moreover, since the randomly initialized neurons cannot guarantee
to get rid of the collinearity redundancy as shown in [45] (i.e., two neurons are on the same line but
have opposite directions), we can perform the half-space hyperspherical energy minimization [45].
Normalizing the neurons. The norm of the randomly initialized neurons may have some influence
on OPT, serving a role similar to weighting the importance of different neurons. Moreover, the
norm makes the hyperspherical energy less expressive to characterize the diversity of neurons, as
discussed in Section 5.3. To address this, we propose to normalize the neuron weights such that the
weight of each neuron has the unit norm. Because the weights of the neurons {vι, ∙∙∙ , Vn} are fixed
throughout the training process and OPT will not change the norm of the final neurons, we only need
to normalize the randomly initialized neuron weights as a preprocessing before the OPT training.
4 Towards S calab le Orthogonal Over-Parameterized Training
If the dimension of neurons become is extremely large, then the orthogonal
matrix to transform the neurons will also be large. Therefore, it may take large
GPU memory and time to train the neural networks with the original OPT.
To address this, We propose a scalable OPT variant - stochastic orthogonal
over-parameterized training (S-OPT). The key idea of S-OPT is to randomly
select some dimensions of neurons in the same layer and construct a small
orthogonal matrix to transform these dimensions together. The selection of
neuron dimensions is stochastic in each outer iteration, so a small orthogonal
Neurons
O
O-
O
O
xxxx
xxxx
xxxx
xxxx
OPT
「x xI
X xj
S-OPT
Figure 3: OPT& S-OPT
matrix is still able to cover all the neuron dimensions. Intuitively, S-OPT aims to approximate the
orthogonal transformation of all the neuron dimensions With small orthogonal transformations of
random subsets of the neuron dimensions. The approximation Will be more accurate if the procedure
is randomized over many times. Fig. 2 compares the size of the orthogonal matrix in OPT and S-OPT.
6000〕
5
Under review as a conference paper at ICLR 2021
Most importantly, S-OPT can still preserve the hyperspherical
energy of neurons because of the following Theorem 2.
Theorem 2. For n d-dimensional neurons, selecting any p
(p ≤ d) dimensions and applying an shared orthogonal trans-
formation (orthogonal matrix of size p × p) to these p dimen-
sions of all neurons will not change the hyperspherical energy.
A description of S-OPT is given in Algorithm 1. S-OPT has
outer and inner iterations. In each inner iteration, the training
is almost the same as OPT, except that the orthogonal matrix
Algorithm 1 Stochastic OPT
for i = 1, 2,…，Nout do
for j = 1, 2,…，Nin do
1.	Randomly select P dimensions from d-
dimensional neurons in the same layer.
2.	Construct an orthogonal matrix Rp ∈
Rp×p and initialize it as identity matrix.
3.	Update Rp with one iteration of OPT.
end
4.	Multiply Rp back to the p-dim sub-vectors
from the d-dim neurons for transformation.
end
transforms a subset of the dimensions and the learnable orthogonal matrix has to be re-initialized as
an identity matrix. The selection of neuron dimension is randomized in every outer iteration such
that all neuron dimensions are sufficiently covered as the number of outer iteration increases. As a
parallel direction to improve the scalability of OPT, we further propose a parameter-efficient OPT in
Appendix I. This OPT variant explores structure priors in R to improve parameter efficiency.
5	Insights and Discussions
5.1	Optimization Landscape
We follow [40] to visualize the loss landscapes of both
standard training and OPT in Fig. 4. For standard train-
ing, we perturb the parameter space of all the neurons
(i.e., filters). For OPT, we perturb the parameter space of
all the trainable matrices (i.e., P in Fig. 2), because OPT
does not directly learn neuron weights. The general idea
is to use two random vectors (e.g., normal distribution) to
perturb the parameter space and obtain the loss value with
the perturbed network parameters. Details and full results
about the visualization are given in Appendix E. The loss
landscape of standard training has extremely sharp min-
ima. The red region is very flat, leading to small gradients.
In contrast, the loss landscape of OPT is more smooth
Standard training	OPT
Figure 4: Loss landscape visualization.
and convex with flatter minima, matching the finding that flat minimizers generalize well [8, 22, 26].
Moreover, the testing error landscape in Appendix E and additional loss landscape visualization
results in Appendix F (with uniform perturbation distributions) well support our argument.
5.2	Optimization and Generalization
This subsection discusses why OPT improves optimization and generalization. On one hand, [67]
proves that once the neurons are hyperspherically diverse enough in a one-hidden-layer network, the
training loss is on the order of the square norm of the gradient and the generalization error will have
an additional term O(1∕√m) where m is the number of samples. This suggests that SGD-OPtimized
networks with minimum hyperspherical energy (MHE) attained have no spurious local minima. Since
OPT is guaranteed to achieve MHE in expectation, OPT-trained networks enjoy the inductive bias
induced by MHE. On the other hand, [1, 12, 16, 31, 42] shows that over-parameterization in neural
networks improves the first-order optimization, leads to better generalization, and imposes implicit
regularizations. In the light of this, OPT also introduces over-parameterization to each neuron in
the neural network, which shares similar spirit to [43]. Specifically, one d-dimensional neuron has
d2 + d parameters in OPT, compared to d parameters in the baseline (d2 parameters are shared across
neurons of the same layer). Although OPT uses more parameters to represent a neuron in training,
the equivalent number of parameters for a neuron stays unchanged and will not affect testing speed.
5.3	Discussions
Coordinate system and relative position. OPT shows that learning the coordinate system yields
better generalization than learning neuron weights directly. This implies that the coordinate system is
crucial to generalization. However, the relative position does not matter only when the hyperspherical
energy is sufficiently low, indicating that hyperspherical energy well characterizes the relative
positions among neurons. Lower hyperspherical energy generally leads to better generalization.
The effects of neuron norm. Because we will normalize the neuron norm when computing the
hyperspherical energy, the effects of neuron norm will not be taken into consideration. Moreover,
6
Under review as a conference paper at ICLR 2021
simply learning the orthogonal matrices will not change the neuron norm. Therefore, the neuron norm
may affect the training. We use an extreme example to demonstrate the effects. Assume that one of
the neurons have norm 1000 and the other neurons have norm 0.01. Then no matter what orthogonal
matrices we have learned, the final performance will be bad. In this case, the hyperspherical energy
can still be minimized to a very low value, but it can not capture the norm distribution. Fortunately,
such an extreme case is unlikely to happen, because we are using zero-mean Gaussian distribution to
initialize the neuron and every neuron also has the same expected value for the norm. To eliminate
the effects of norms, we can normalize the neuron weights in the training, as proposed in Section 3.7.
6	Experiments and Results
Method	FN	LR	CNN-6	CNN-9
Baseline	-	-	37.59	33.55
UPT	X	U	48.47	46.72
UPT	✓	U	42.61	39.38
OPT	X	GS	37.24	32.95
OPT	✓	GS	33.02	31.03
Table 1: Error (%) on C-100.
The section aims to show that OPT can effectively improve all kinds of neural networks. Detailed ex-
perimental settings are given in Appendix D. We put many additional experiments in Appendix I,K,M.
6.1	Ablation Study and Exploratory Experiments
Necessity of orthogonality. We test 6-layer and 9-layer CNN (Ap-
pendix D) on CIFAR-100. OPT is compared to unconstrained over-
parameterized training (UPT) which learns an unconstrained matrix R
(with only weight decay) using the same network. In Table 1, “FN” de-
notes whether the randomly initialized neuron weights are fixed in the training. “LR” denotes whether
the learnable transformation R is unconstrained (“U”) or orthogonal (“GS” for Gram-Schmidt
process). Table 1 shows that without ensuring orthogonality, UPT performs much worse than OPT.
Fixed weights vs. learnable weights. From Table 1, we can see that using fixed neuron weights is
consistently better than learnable neuron weights in both UPT and OPT. It indicates that fixing the
neuron weights while learning the transformation matrix R is very beneficial to generalization.
Refining neuron initialization. We evaluate two refinement methods
(introduced in Section 3.7) for neuron initialization. First, we consider
the hyperspherical energy minimization as a preprocessing for the neuron
weights. Our experiment uses CNN-6 on CIFAR-100. Specifically,
we run gradient descent for 5k iterations to minimize the objective of
Method	Original	MHE	HS-MHE
OPT (GS)	33.02	32.99	32.78
OPT (LS)	34.48	34.43	34.37
OPT (CP)	33.53	33.50	33.42
Energy	3.5109	3.5003	3.4976
Table 2: Refining energy.
MHE/HS-MHE [45] before the training starts. Table 2 shows the hyperspherical energy before and
after the preprocessing. All methods start with the same random initialization, so all hyperspherical
energies start at 3.5109. Testing errors (%) in Table 2 show that the refinement well improves OPT.
Then we evaluate the neuron weight normalization. Section 5.3 justifies the
superiority of normalizing the neurons. After random initialization, we nor-
malize the scale of all the neuron weights to 1. These randomly initialized
neurons still possess the important property of achieving minimum energy.
We conduct classification with CNN-6 on CIFAR-100. Testing errors in
Table 3 show that normalizing the neurons greatly improves OPT. Note
that, these two refinements are not used by default in other experiments.
Method	w/o Norm	w/ Norm
Baseline	-37.59-	36.05
OPT (GS)	33.02	32.54
OPT (HR)	35.67	35.30
OPT (LS)	34.48	32.11
OPT (CP)	33.53	32.49
OPT (OGD)	33.37	32.70
OPT (OR)	34.70	33.27
Table 3: Normalization (%).
6.2 Empirical Evaluation on OPT
Multi-layer perceptrons. We evaluate OPT on
MNIST with a 3-layer MLP. Appendix D gives
specific settings. Table 4 shows the testing er-
ror with normal initialization (MLP-N) or Xavier
initialization [14] (MLP-X). GS/HR/LS denote
different orthogonalization unrolling. CP de-
notes Cayley parameterization. OGD denotes
orthogonal-preserving gradient descent. OR de-
Method	MNIST		CIFAR-100			
	MLP-N	MLP-X	CNN-6	CNN-9	ResNet-20	ResNet-32
Baseline	6.05	2.14	37.59	33.55	31.11	30.16
Orthogonal [7]	5.78	1.93	36.32	33.24	31.06	30.05
SRIP [4]	-	-	34.82	32.72	30.89	29.70
HS-MHE [45]	5.57	1.88	34.97	32.87	30.98	29.76
OPT (GS)	5.11	1.45	33.02	31.03	30.49	29.34
OPT (HR)	5.31	1.60	35.67	32.75	30.73	29.56
OPT (LS)	5.32	1.54	34.48	31.22	30.51	29.42
OPT (CP)	5.14	1.49	33.53	31.28	30.47	29.31
OPT (OGD)	5.38	1.56	33.33	31.47	30.50	29.39
OPT (OR)	5.41	1.78	34.70	32.63	30.66	29.47
Table 4: Error (%) of OPT for MLPs and CNNs.
notes relaxed orthogonal regularization. All OPT variants outperform the others by a large margin.
Convolutional networks. We evaluate OPT with 6/9-layer plain CNNs and ResNet-20/32 [20]
on CIFAR-100. Detailed settings are given in Appendix D. All neurons (i.e., convolution kernels)
are initialized by [19]. Batch normalization [25] is used by default. Table 4 shows that all OPT
variants outperform both baseline and HS-MHE [45] by a large margin. HS-MHE puts the half-space
hyperspherical energy into the loss function and naively minimizes it along with the CNN. We observe
that OPT (HR) performs the worse among all OPT variants partially because of its intensive unrolling
computation. OPT (GS) achieves the best testing error on CNN-6/9, while OPT (CP) achieves the best
testing error on ResNet-20/34, implying that different OPT may encode different inductive biases.
7
Under review as a conference paper at ICLR 2021
Training dynamics. We further look
into how hyperspherical energy and
testing error changes while training
with OPT. Fig. 5 shows that the en-
ergy of the baseline will increase dra-
matically at the beginning and then
gradually go down, but it still stays in
a high value in the end. HS-MHE can
effectively reduce the hyperspherical
energy at the end of the training. In
Figure 5: Training dynamics on CIFAR-100. Left: HyPersPheriCal
energy vs. iteration. Right: Testing error vs. iteration.
contrast, all OPT variants maintain minimal energy in training. OPT with GS, CP and OGD keePs
exactly the same energy as the randomly initialized neurons, while OPT (OR) slightly increases the
energy due to the relaxation. For convergence, all OPT variants converge efficiently and stably.
ImageNet. We test OPT on the large-scale ImageNet-2012 dataset.
SPecifically, we use OPT with OGD and CP to train a Plain 10-layer
CNN (APPendix D) on ImageNet. Our PurPose is to validate the suPe-
riority of OPT over the corresPonding baseline. Table 5 shows that OPT
(CP) reduces top-1 and top-5 error by 〜0.7% and 〜0.9%, respectively.
Method	Top-1 Err.	Top-5 Err.
Baseline	-44.32-	21.13
Orthogonal [7]	44.13	20.97
HS-MHE [45]	43.92	20.85
OPT (OGD)	43.81	20.49
OPT (CP)	43.67	20.26
Table 5: ImageNet (%).
Few-shot learning. To evaluate the cross-task generalization of OPT, we
conduct few-shot learning on Mini-ImageNet, following the same exper-
imental setting as [10]. Detailed settings are in Appendix D. Specifically,
we apply OPT with CP to train the baseline and baseline++ described in
[10], and immediately obtain obvious improvements. Therefore, OPT-
trained networks also generalize well in the challenging few-shot case.
Method	5-shotAcc. (%)
MAML[13]	62.71 ± 0.71
ProtoNet [58]	64.24 ± 0.72
BaSeline[10]	62.53 ± 0.69
Baseline w/ OPT	63.27 ± 0.68
Baseline++ [10]	66.43 ±0.63
Baseline++ w/ OPT	66.82 ± 0.62
Table 6: Few-shot learning.
Geometric networks. We apply OPT to graph convolution network
(GCN) [34] and point cloud neural network (PointNet) [51] for graph
node and point cloud classification, respectively. The training of GCN
and PointNet is conceptually similar to MLP, and the detailed training
procedures are given in Appendix D. For GCN, we evaluate OPT on
Cora and Pubmed datsets [56]. For PointNet, we conduct experiments
Method	GCN		PointNet MN-40
	Cora	Pubmed	
Baseline	ɪr	790-	87.1
OPT (GS)	81.9	79.4	87.23
OPT (CP)	82.0	79.4	87.81
OPT (OGD)	82.3	79.5	87.86
Table 7: Geometric networks.
on ModelNet-40 dataset [66]. Table 7 shows that OPT effectively improves both GCN and PointNet.
6.3 Empirical Evaluation on S-OPT
Convolutional networks. S-OPT is a scalable
OPT variant, and we evaluate its performance
in terms of number of trainable parameters and
testing error. Training parameters are learnable
Method	CNN-6	CIFAR-100		Params	ImageNet	
		Params	Wide CNN-9		ResNet-18	Params
Baseline	37.58	249K	28.03	2.97M	-32.95-	11.2M
HS-MHE [45]	37.58	249K	25.96	2.97M	32.50	11.2M
OPT (GS)	33.02	1.73M	OOM	16.6M	OOM	92.9M
S-OPT (GS)	33.70	170K	25.59	2.02M	32.26	5.77M
Table 8: OPT vs. S-OPT on CIFAR-100 & ImageNet.
variables in training, which are different from model parameters in testing. In testing, all methods
have the same number of model parameters. We perform classification on CIFAR-100 with CNN-6
and wide CNN-9. We also evaluate S-OPT with wide ResNet-18 on ImageNet. Detailed settings
are given in Appendix D. For S-OPT, we set the sampling dimension as 25% of the original neuron
dimension in each layer. Table 8 shows that S-OPT achieves a good trade-off between accuracy
and efficiency. Most importantly, S-OPT is able to train on wide neural networks on large-scale
datasets, making OPT more useful in practice. As a parallel result, we also propose and experiment an
orthogonal transformation with structural parameter sharing to save parameters in OPT in Appendix I.
Effect of sampling dimension. Since the sampling dimen-
sion p is a hyperparameter, we study its effect by performing
classification with wide CNN-9 on CIFAR-100. In Table 9,
p=d/4 means that we randomly sample 1/4 of the original
Dimension p
S-OPT (GS)
Params
-d4 d/8 d/16	16-
25.59^^28.61 ~32.52~~33.03
2.02M 1.29M 1.11M 1.051M
3
45.22
1.049M
Table 9: Effect of sampling dimension.
neuron dimension in each layer, so p may vary in different layer. p= 16 means that we sample 16
dimensions in each layer. 1M trainable parameters come from the last classifier layer, which cannot
be saved. Table 9 shows that larger p yields better accuracy but consumes more trainable parameters.
7	Concluding Remarks
We propose a novel training framework that effectively minimizes hyperspherical energy for neural
networks. OPT over-parameterizes neurons with neuron weights (randomly initialized and fixed) and
a layer-shared orthogonal matrix (learnable). Experiments validate OPT’s strong generalizability.
8
Under review as a conference paper at ICLR 2021
References
[1]	Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. arXiv preprint arXiv:1811.03962, 2018. 6, 36
[2]	Ramesh NaidU Annavarapu. Singular value decomposition and the centrality of IOwdin orthogonalizations.
American Journal of Computational and Applied Mathematics, 3(1):33-35, 2013. 15
[3]	Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In ICML,
2016. 2, 4
[4]	Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from orthogonality regularizations
in training deep cnns? In NeurIPS, 2018. 2, 5, 7
[5]	Johann S Brauchart, Alexander B Reznikov, Edward B Saff, Ian H Sloan, Yu Guang Wang, and Robert S
Womersley. Random point sets on the sphere—hole radii, covering, and separation. Experimental
Mathematics, 27(1):62-81, 2018. 42, 43
[6]	Anna Breger, Martin Ehler, and Manuel Graf. Points on manifolds with asymptotically optimal covering
radius. Journal of Complexity, 48:1-14, 2018. 42
[7]	Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective
adversarial networks. In ICLR, 2017. 2, 7, 8
[8]	Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide
valleys. In ICLR, 2017. 6
[9]	Beidi Chen, Weiyang Liu, Animesh Garg, Zhiding Yu, Anshumali Shrivastava, and Animashree Anandku-
mar. Angular visual hardness. In ICML, 2020. 2
[10]	Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look at
few-shot classification. arXiv preprint arXiv:1904.04232, 2019. 8, 19, 20
[11]	Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. Acnet: Strengthening the kernel skeletons
for powerful cnn via asymmetric convolution blocks. In ICCV, 2019. 1, 2
[12]	Simon S Du, Jason D Lee, Yuandong Tian, Barnabas Poczos, and Aarti Singh. Gradient descent learns
one-hidden-layer cnn: Don’t be afraid of spurious local minima. arXiv preprint arXiv:1712.00779, 2017.
6, 36
[13]	Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In ICML, 2017. 8
[14]	Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, 2010. 1, 3, 7
[15]	Suriya Gunasekar, Jason D Lee, Daniel Soudry, and Nati Srebro. Implicit bias of gradient descent on linear
convolutional networks. In NeurIPS, 2018. 1
[16]	Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro.
Implicit regularization in matrix factorization. In NeurIPS, 2017. 1, 6
[17]	David Ha, Andrew Dai, and Quoc V Le. Hypernetworks. arXiv preprint arXiv:1609.09106, 2016. 40
[18]	DP Hardin and EB Saff. Minimal riesz energy point configurations for rectifiable d-dimensional manifolds.
Advances in Mathematics, 193(1):174-204, 2005. 17
[19]	Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In ICCV, 2015. 1, 3, 7
[20]	Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.
In CVPR, 2016. 7, 19, 21
[21]	Mikael Henaff, Arthur Szlam, and Yann LeCun. Recurrent orthogonal networks and long-memory tasks.
arXiv preprint arXiv:1602.06662, 2016. 4
[22]	Sepp Hochreiter and Jurgen Schmidhuber. Flat minima. Neural Computation, 1997. 6
9
Under review as a conference paper at ICLR 2021
[23]	Walter Hoffmann. Iterative algorithms for gram-Schmidt orthogonalization. Computing, 41(4):335-348,
1989. 4, 13
[24]	Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, and Bo Li. Orthogonal weight
normalization: Solution to optimization over multiple dependent stiefel manifolds in deep neural networks.
In AAAI, 2018. 2, 4
[25]	Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In ICML, 2015. 7
[26]	Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averag-
ing weights leads to wider optima and better generalization. In UAI, 2018. 6
[27]	Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with
low rank expansions. In BMVC, 2014. 2
[28]	Kui Jia, Shuai Li, Yuxin Wen, Tongliang Liu, and Dacheng Tao. Orthogonal deep neural networks. TPAMI,
2019. 2
[29]	Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In NeurIPS,
2016. 40
[30]	Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and Marin
Soljacic. Tunable efficient unitary neural networks (eunn) and their application to rnns. In ICML, 2017. 4
[31]	Kenji Kawaguchi. Deep learning without poor local minima. In NeurIPS, 2016. 1, 6, 36
[32]	Kenji Kawaguchi, Bo Xie, and Le Song. Deep semi-random features for nonlinear function approximation.
In AAAI, 2018. 37
[33]	Nitish Shirish Keskar and Richard Socher. Improving generalization performance by switching from adam
to sgd. arXiv preprint arXiv:1712.07628, 2017. 1
[34]	Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
arXiv preprint arXiv:1609.02907, 2016. 8
[35]	Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional
neural networks. In NeurIPS, 2012. 21
[36]	N.S. Landkof. Foundations of modern potential theory. Springer-Verlag, 1972. 17
[37]	Jason D Lee, Max Simchowitz, Michael I Jordan, and Benjamin Recht. Gradient descent only converges to
minimizers. In COLT, 2016. 36
[38]	Mario Lezcano-Casado. Trivializations for gradient-based optimization on manifolds. In NeurIPS, 2019. 4
[39]	Mario Lezcano-Casado and David Martinez-Rubio. Cheap orthogonal constraints in neural networks: A
simple parametrization of the orthogonal and unitary group. In ICML, 2019. 2, 4
[40]	Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape of
neural nets. In NeurIPS, 2018. 6, 22, 29
[41]	Jun Li, Fuxin Li, and Sinisa Todorovic. Efficient riemannian optimization on the stiefel manifold via the
cayley transform. In ICLR, 2020. 4, 5
[42]	Yuanzhi Li, Tengyu Ma, and Hongyang Zhang. Algorithmic regularization in over-parameterized matrix
sensing and neural networks with quadratic activations. In COLT, 2018. 1, 6
[43]	Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400, 2013. 6,
40
[44]	Baoyuan Liu, Min Wang, Hassan Foroosh, Marshall Tappen, and Marianna Pensky. Sparse convolutional
neural networks. In CVPR, 2015. 2
[45]	Weiyang Liu, Rongmei Lin, Zhen Liu, Lixin Liu, Zhiding Yu, Bo Dai, and Le Song. Learning towards
minimum hyperspherical energy. In NeurIPS, 2018. 2, 3, 5, 7, 8, 22, 29, 36, 43, 44
[46]	Weiyang Liu, Zhen Liu, James M Rehg, and Le Song. Neural similarity learning. In NeurIPS, 2019. 1, 2,
40
10
Under review as a conference paper at ICLR 2021
[47]	Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep
hypersphere embedding for face recognition. In CVPR, 2017. 2
[48]	Weiyang Liu, Yan-Ming Zhang, Xingguo Li, Zhiding Yu, Bo Dai, Tuo Zhao, and Le Song. Deep
hyperspherical learning. In NeurIPS, 2017. 2, 5, 21
[49]	Arun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple
tasks by learning to mask weights. In ECCV, 2018. 37
[50]	Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efficient orthogonal parametri-
sation of recurrent neural networks using householder reflections. In ICML, 2017. 4
[51]	Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d
classification and segmentation. In CVPR, 2017. 8, 20
[52]	Haozhi Qi, Chong You, Xiaolong Wang, Yi Ma, and Jitendra Malik. Deep isometric learning for visual
recognition. arXiv preprint arXiv:2006.16992, 2020. 2
[53]	Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In NeurIPS, 2008. 37
[54]	Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. arXiv preprint
arXiv:1904.09237, 2019. 1
[55]	Matthias Reitzner. Stochastical approximation of smooth convex bodies. Mathematika, 51(1-2):11-29,
2004. 42
[56]	Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 2008. 8, 20
[57]	Ian H Sloan and Robert S Womersley. Extremal systems of points and numerical integration on the sphere.
Advances in Computational Mathematics, 21(1-2):107-125, 2004. 43
[58]	Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In NeurIPS,
2017. 8
[59]	Daniel Soudry and Yair Carmon. No bad local minima: Data independent training error guarantees for
multilayer neural networks. arXiv preprint arXiv:1605.08361, 2016. 36
[60]	Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout:
a simple way to prevent neural networks from overfitting. JMLR, 2014. 37
[61]	Vipin Srivastava. A unified view of the orthogonalization methods. Journal of Physics A: Mathematical
and General, 33(35):6219, 2000. 15
[62]	Yun Tang, Jing Huang, Guangtao Wang, Xiaodong He, and Bowen Zhou. Orthogonal relation transforms
with graph context modeling for knowledge graph embedding. In ACL, 2020. 4
[63]	Min Wang, Baoyuan Liu, and Hassan Foroosh. Factorized convolutional neural networks. In ICCV
Workshops, 2017. 2
[64]	Zaiwen Wen and Wotao Yin. A feasible method for optimization with orthogonality constraints. Mathe-
matical Programming, 2013. 4
[65]	Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas. Full-capacity unitary
recurrent neural networks. In NeurIPS, 2016. 2, 4
[66]	Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao.
3d shapenets: A deep representation for volumetric shapes. In CVPR, 2015. 8, 20
[67]	Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In AISTATS,
2017. 6, 36, 37
[68]	Zichao Yang, Marcin Moczulski, Misha Denil, Nando de Freitas, Alex Smola, Le Song, and Ziyu Wang.
Deep fried convnets. In ICCV, 2015. 2
11
Under review as a conference paper at ICLR 2021
Appendix
A Details of Unrolled Orthogoanlization Algorithms
A.1 Gram-Schmidt Process
Gram-Schmidt Process. GS process is a method for orthonormalizing a set of vectors in an inner
product space, i.e., the Euclidean space Rn equipped with the standard inner product. Specifically, GS
process performs the following operations to orthogonalize a set of vectors {uι,…，Un} ∈ Rn×n:
SteP L eι = uι, eι = H 〜H
ke1k
〜~	C . /	、	©2
Step 2： ©2 = U2 — Projg1 (u2), ©2 = Pɪ^
Step 3: ©3 = u3 — PrOjgI (u3) - PrOje2 (u3),
©3
Step 4: ©4 = u4 — Projg1 (u4) — PrOje2 (u4) — PrOjg3(u4),
©4
©4
百
Step n: ©n = Un
©g
— PrOjeI (Un) — PrOje2 (Un) — PrOjg3 (Un)--PrOjen- (un), ©n = P"J
(6)
where Proja(b) = habia denotes the projection of the vector b onto the vector a. The set
{©1, ©2,…，©n} denotes the output orthonormal set. The algorithm flowchart can be described as
follows:
Algorithm 2 Gram-Schmidt Process
Input： U = {ui, U2, ∙∙∙ , Un} ∈ Rn×n
Output: R = {©1, ©2,…,©n} ∈ Rn×n
R=0
for j = 1, 2,…，n do
qj = R> Uj
t = Uj — Rqj
qjj = ktk2
©j = t—
j	qjj
end
The vectors qj , ∀j in the algorithm above are used to compute the QR factorization, which is not
useful in orthogonalization and therefore does not need to be stored. When the GS process is
implemented on a finite-precision computer, the vectors ©j, ∀j are often not quite orthogonal, because
of rounding errors. Besides the standard GS process, there is a modified Gram-Schmidt (MGS)
algorithm which enjoys better numerical stability. This approach gives the same result as the original
formula in exact arithmetic and introduces smaller errors in finite-precision arithmetic. Specifically,
GS computes the following formula:
j-1
©j = Uj- EPrOjek(Uj)
k=1	(7)
_ ©j
©j =百
Instead of computing the vector ©j as in Eq. 7, MGS computes the orthogonal basis differently. MGS
does not subtract the projections of the original vector set, and instead remove the projection of
12
Under review as a conference paper at ICLR 2021
the previously constructed orthogonal basis. Specifically, MGS computes the following series of
formulas:
ej1) = Uj- PrOjgI (Uj )
ej2) = ej1)-PrOje2(ej1))
e(j-2) = ej-3) -Proje2 (ej-3))
e(jT) = ej-2)-Proje2(e尸))
e. = j
[同T)Il
(8)
where each step finds a vector eji) that is orthogonal to eji-1). Therefore, eji) is also orthogonalized
against any errors brought by the computation of eji-1). In practice, although MGS enjoys better
numerical stability, we find the empirical performance of GS and MGS is almost the same in OPT.
However, MGS takes longer time to complete since the computation of each orthogonal basis is an
iterative process. Therefore, we usually stick to classic GS for OPT.
Iterative Gram-Schmidt Process. Iterative Gram-Schmidt (IGS) process is an iterative version of
the GS process. It is shown in [23] that GS process can be carried out iteratively to obtain a basis
matrix that is orthogonal in almost full working precision. The IGS algorithm is given as follows:
Algorithm 3 Iterative Gram-Schmidt Process
Input： U = {ui, U2, ∙∙∙ , Un} ∈ Rn×n
Output: R = {eι, e?,…，en} ∈ Rn×n
R=0
for j = 1, 2,…，n do
qj = 0
t = Uj
while t ⊥ span©,…，ej∙-ι) is False do
p=t
s = R>p
v = Rs
t =p-v
qj — qj + S
end
qjj = ktk2
ej = t-
j	qjj
end
The vectors qj , ∀j in the algorithm above are used to compute the QR factorization, which is not
useful in orthogonalization and therefore does not need to be explicitly computed. The while loop in
IGS is an iterative procedure. In practice, we can unroll a fixed number of steps for the while loop in
order to improve the orthogonality. The resulting qj in the j -th step corresponds to the solution of
the equation R > Rqj = R > uj where R = {eι,…，ej-ι}. The IGS process corresponds to the
Gauss-Jacobi iteration for solving this equation.
Both GS and IGS are easy to be embedded in the neural networks, since they are both differentiable.
In our experiments, we find that the performance gain of unrolling multiple steps in IGS over GS is
not very obvious (partially because GS has already achieved nearly perfect orthogonality), but IGS
costs longer training time. Therefore, we unroll the classic GS process by default.
13
Under review as a conference paper at ICLR 2021
A.2 Householder Reflection
Let v ∈ Rn be a non-zero vector. A matrix H ∈ Rn×n of the form
H=I-
2vv>
v>v
(9)
is a Householder reflection. The vector v is the Householder vector. If a vector x is multiplied by the
matrix H, then it will be reflected in the hyperplane span(v)⊥. Householder matrices are symmetric
and orthogonal.
For a vector X ∈ Rn, we let V = X ± ∣∣xk2 eι where eι is a vector of {1,0, ∙∙∙ , 0} (the first element
is 1 and the remaining elements are 0). Then we construct the Householder reflection matrix with v
and multiply it to X:
HX
干 Ilxk2 e1
(10)
which indicates that we can make any non-zero vector become αe1 where α is some constant by
using Householder reflection. By left-multiplying a reflection we can turn a dense vector X into a
vector with the same length and with only a single nonzero entry. Repeating this n times gives us
the Householder QR factorization, which also orthogonalizes the original input matrix. Householder
reflection orthogonalizes a matrix U = {uι, ∙∙∙ , Un} by triangularizing it:
U = H1H2 …HnR	(11)
where R is a upper-triangular matrix in the QR factorization. Hj , j ≥ 2 is constructed by
Diag(Ij-ι, Hη-j+ι) where Hn-j+ι ∈ R(n-j+1)×(n-j+1) is the Householder reflection that is
performed on the vector U(j:n,j). The algorithm flowchart is given as follows:
Algorithm 4 Householder Reflection Orthogonalization
Input： U = {ui, U2,…，Un} ∈ Rn×n
Output: U = QR, where Q = {eι, e2,…,en} ∈ Rn×n is the
orthogonal matrix and R ∈ Rn×n is a upper triangular
matrix
for j = 1, 2,…，n 一 1 do
{v, β} = Householder(Uj:n,j)
Ujnjn J Ujnjn 一 βv (V> Uj：n,j:n)
Uj+1:n,j j- v(2:end)
end
function {v, β} = Householder(X)
O2 = ∣ x2:end 12
vJ
1
X2:end
if σ2 = 0 then
I β = 0
else
if x 1 ≤ 0 then
I Vi = Xi - vzχ1 + σ2
else
Vi
end
Xi + vzχi + σ2
β=
VJ
end
2v2
σ2+v2
V
V1
end function
The algorithm follows the Matlab notation where Uj:n,j:n denotes the submatrix of U from the j-th
column to the n-th column and from the j -th row to the n-th row. Note that, there are a number of
14
Under review as a conference paper at ICLR 2021
variants for the Householder reflection orthogonalization, such as the implicit variant where we do
not store each reflection Hj explicitly. Here Q is the final orthogonal matrix we need.
A.3 LOWDIN’s Symmetric Orthogonalization
Let U = {uι, u2,…，Un} be a set of linearly independent vectors in a n-dimensional space. We
define a general non-singular linear transformation A that can transform the basis U to a new basis
R:
R= UA	(12)
where the basis R will be orthonormal if (the transpose will become conjugate transpose in complex
space)
R>R= (UA)> (UA) = A>U>UA = A>MA = I	(13)
where M = U>U is the gram matrix of the given basis U.
A general solution to this orthogonalization problem can be obtained via the substitution:
A = M-1B	(14)
in which B is an arbitrary orthogonal (or unitary) matrix. When B = I, we will have the symmetric
orthogonalization, namely
R := Φ = UM- 1	(15)
When B = V in which V diagonalizes M, then we have the canonical orthogonalization, namely
Λ = UVd-2.	(16)
Because V diagonalizes M, We have that M = VdV>. Therefore, We have the M-2 transfor-
mation as M- 1 = Vd- 1 V>. This is essentially an eigenvalue decomposition of the symmetric
matrix M = U> U.
In order to compute the LOWdin's symmetric orthogonalized basis sets, we can use singular value
decomposition. Specifically, SVD of the original basis set U is given by
U = WΣV>	(17)
Where both W ∈ Rn×n and U ∈ Rn×n are orthogonal matrices. Σ is the diagonal matrix of singular
values. Therefore, We have that
R = UM- 2
=W ΣV >Vd-1V >	(18)
=W Σd-1V >
where we have Σ = d2 due to the connections between eigenvalue decomposition and SVD.
Therefore, We end up With
R=WV>	(19)
which is the output orthogonal matrix for LOWdin's symmetric orthogonalization.
An interesting feature of the symmetric orthogonalization is to ensure that
R
argP∈mortihn(U)	kPi - Uik
i
(20)
where Pi and Ui are the i-th column vectors of P ∈ Rn×n and U, respectively. orth(U) denotes the
set of all possible orthonormal sets in the range of U. This means that the symmetric orthogonalization
functions Ri (or Φi) are the least distant in the Hilbert space from the original functions Ui. Therefore,
symmetric orthogonalization indicates the gentlest pushing of the directions of the vectors in order to
make them orthogonal.
More interestingly, the symmetric orthogonalized basis sets has unique geometric properties [2, 61]
ifwe consider the Schweinler-Wigner matrix in terms of the sum of squared projections.
15
Under review as a conference paper at ICLR 2021
B Proof of Theorem 1
To be more specific, neurons with each element initialized by a zero-mean Gaussian distribution are
uniformly distributed on a hypersphere. We show this argument with the following theorem.
Theorem 3. The normalized vector of Gaussian variables is uniformly distributed on the sphere.
Formally, let x1,x2, ∙∙∙ ,Xn 〜N(0,1) and be independent. Then the vector
x
X1 X2	Xn
^z,τ,∙∙∙ ,τ
(21)
follows the uniform distribution on Sn-1, where z =，x2 + x2 + … + Xn is a normalization factor.
Proof. A random variable has distribution N(0, 1) if it has the density function
f(x) = √2∏e-1X.	(22)
A n-dimensional random vector x has distribution N(0, 1) if the components are independent and
have distribution N(0, 1) each. Then the density of x is given by
f (x) = —1— e-2 hx,χi.	(23)
L	(√2∏)n
Then we introduce the following lemma (Lemma 1) about the orthogonal-invariance of the normal
distribution.
Lemma 1. Let x be a n-dimensional random vector with distribution N(0, 1) and U ∈ Rn×n be an
orthogonal matrix (UU> = U>U = I). Then Y = Ux also has the distribution ofN(0, 1).
Proof. For any measurable set A ⊂ Rn , we have that
P(Y ∈ A) = P(X ∈ U>A)
A
ZA
IU>A (√∏Fe-1 hx,xi
1	e-2 hUx,Uxi
(√2π)n
___1___e-2 hx,xi
(√2∏)n
(24)
because of orthogonality of U. Therefore the lemma holds.
□
Because any rotation is just a multiplication with some orthogonal matrix, we know that normally
distributed random vectors are invariant to rotation. As a result, generating x ∈ Rn with distribution
N(0,1) and then projecting it onto the hypersphere SnT produces random vectors U =向 that are
uniformly distributed on the hypersphere. Therefore the theorem holds.	□
Then we show the normalized vector y where each element follows a zero-mean Gaussian distribution
with some constant variance σ2 :
y
yι丝
r , 丁
Vn
r
(25)
where r = vzy2 + v2 +-----+ yn. Because We have that y 〜
following random vector:
N(0, 1), we can rewrite y as the
(26)
where r∕σ = ,(yι∕σ)2 + (y2∕σ)2 +-----------+ (Vn∕σ)2. Therefore, we directly can apply Theorem 3
and conclude that y also follows the uniform distribution on Sn-1. Now we obtain that any random
vector with each element following a zero-mean Gaussian distribution with some constant variance
follows the uniform distribution on Sn-1.
16
Under review as a conference paper at ICLR 2021
Then we show that the minimum hyperspherical energy asymptotically corresponds to the uniform
distribution over the unit hypersphere. We first write down the hyperspherical energy of N neurons
{wι,…，WN ∈ Rd+1} (We also define that Wi = kWij ∈ Sd):
Es…1) = XX X fs(kwL Wjk) = {pi= 1⅛W"ks >0 S = 0	(27)
i=1 j =1,j6=i	i6=j	g i j ,
Where s is a hyperparameter that controls the behavior of hyperspherical energy. We then define a
N -point minimal hyperspherical s-energy over A With
εs,d(A, WN)：= inf Es,d(W^i∖NLι)	(28)
WN ⊂a
where we denote that WN = {Wι,…，WN}. Typically, we will assume that A is compact. Based on
[18], we discuss the asymptotic behavior (as N → ∞) of εs,d(A, WN) in three different scenarios:
(1) 0 < s < d; (2) s = d; and s >d. The reason behind is the behavior of the following energy
integral:
Is(μ) = H	ku — vk-sdμ(u)dμ(v),
Sd ×Sd
(29)
is quite different under these three scenarios. In scenario (1), Eq. 29 that is taken over all probability
measures μ supported on Sd will be minimal for normalized Lebesgue measure H((Sdid on Sd. In
the case of S ≥ d, we will have that Is (μ) is positive infinity for all such measures μ. Therefore, the
behaviour of the minimum hyperspherical energy is different in these three cases. In general, as the
parameter S increases, there is a transition from the global effects to the more local influences (from
nearest neighbors). The transition happens when S = d. However, we typically have 0 < S < d
in the neural networks. Therefore, we will mostly study the case of 0 < S < d and the theoretical
asymptotic behavior is quite standard results from the potential theory [36]. From the classic potential
theory, we have the following known lemma:
Lemma 2. If 0 < S < d, we have that
lim
N→∞
εs,d(Sd, WN)
N2
(30)
Moreover, any sequence of S-energy configuration of minimal hyperspherical energy ((WWN )∞ ⊂ Sd)
is asymptotically uniformly distributed in the sense that for the weak-star topology of measures,
1 X δ → HdS∖sd
N v∈WwJ HW
as N → ∞
(31)
where δv denotes the unit point mass at v.
The lemma above concludes that the neuron configuration with minimal hyperspherical energy
asymptotically corresponds to the uniform distribution on Sd when 0 < S < d. From [18], we also
have the following lemma that shows the same conclusion holds for the the case of S = d and S >d:
Lemma 3. Let Bd := 5(0,1) denote the closed unit ball in Rd. For the case of S = d, we have that
lim
N→∞
εs,d(Sd, WN) = Hd(Bd) = 1 Γ(d+1)
N2 log N	= Hd(Sd) = d √∏Γ(d)
(32)
and any sequence (WN) ⊂ Sd of minimal S-energy configurations satisfies Eq. 31.
The lemma above shows that the same conclusion holds for S = d. For the case of S > d, the
theoretical analysis is more involved, but the conclusion that the neuron configuration with minimal
hyperspherical energy asymptotically corresponds to the uniform distribution on Sd still holds. Note
that, we usually will not have the case of S >d in our applications.
17
Under review as a conference paper at ICLR 2021
C Proof of Theorem 2
We consider a set of n d-dimensional neurons W = {wι,…，Wn} ∈ Rd×n. The hyperspherical
energy of the original set of neurons can be written as:
nn
E(wi|in=1) = X X
i=1 j=1,j 6=i
-1
wi	wj
kwik2	IIwj k
(33)
which means that if the pairwise angle between any two neurons stays unchanged, then the hyper-
spherical energy will also stay unchanged. Now we consider the cosine value of the angle θ(wi ,wj )
between any two neuron wi and wj :
cos(θ 、)=	w>wj	= Pk=I W ∙ Wjk
((wi,wj))	kwik∙kwjk	kwik∙kwj k
(34)
where wik is the k-th element of the neuron wi . From the equation above, we can observe that
permuting the order of the elements in the neurons together will not change the angle. For example,
switching the i-th and j -th element in all the neurons will not change the hyperspherical energy.
Assume that we randomly select p dimensions from the d dimensions and denote the setofp dimension
as S = {sι,…，sp} ∈ Rp. Therefore We can construct a new set of neurons W = {wι,…，w n}
by permuting the p dimensions in s to become the first p elements for all the neurons. Essentially, we
use permutation to make wi = ws, for i ∈ [1,p]. Therefore, we can have the following equation:
E(wi∣n=ι) = E(wi∣n=ι)
(35)
Then we consider an orthogonal matrix Rp ∈ Rd×d that is used to transform the p dimension in the
neurons. The equivalent orthogonal transformation for the d-dimensional neurons W is
-Rp	0	…0-
Q	Rpp	0 ]	0	1	... 0	CG
R =	op	i =	(36)
n-p	.	.	.
..	..	.. 0
_ 0	…0 L
where In-p is an identity matrix of size (n - p) × (n - p). It is easy to verify that R is also an
orthogonal matrix: R> R = In . Then we permute the order of W back to the original neuron
set W and obtain a new set of neurons Wt = {wt,…,wtn }. Wt is in fact the result of directly
performing orthogonal transformation to the p dimensions in W. Because any order permutation of
elements in neurons does not change the hyperspherical energy, we have the following equation
E(wi|in=1)=E(wQi|in=1)=E(RQwQi|in=1)=E(RQwit|in=1)	(37)
which concludes our proof.
18
Under review as a conference paper at ICLR 2021
D Experimental Settings
Layer	CNN-6(CIFAR-100)	CNN-9(CIFAR-100)	CNN-10(ImageNet-2012)
Conv1.x	[3×3,64]×2	[3×3,64]×3	[7×7, 64], Stride 2 3×3, Max Pooling, Stride 2 [3×3, 64]×3
Pool1	2×2Max Pooling, Stride 2		
Conv2.x	[3×3,64]×2	[3×3,64]×3	[3×3,128]×3
Pool2	2×2Max Pooling, Stride 2		
Conv3.x	[3×3,64]×2	[3×3,64]×3	[3×3,256]×3
Pool3	2×2Max Pooling, Stride 2		
Fully Connected		64		64		256	
Table 10: Our plain CNN architectures with different convolutional layers. Conv1.x, Conv2.x and Conv3.x
denote convolution units that may contain multiple convolution layers. E.g., [3×3, 64]×3 denotes 3 cascaded
convolution layers with 64 filters of size 3×3.
Layer	ResNet-20 (CIFAR-100)			ResNet-32 (CIFAR-100)		
Conv1.x		[3×3, 16] 3 × 3, 16 3 × 3, 16	×1 ×3		[3×3, 16] 3 × 3, 16 3 × 3, 16	×1 ×5
Conv2.x		3 × 3, 32 3 × 3, 32	×3		3 × 3, 32 3 × 3, 32	× 5
Conv3.x		3 × 3, 64 3 × 3, 64	×3		3 × 3, 64 3 × 3, 64	× 5
	Average Pooling					
Table 11: Our ResNet architectures with different convolutional layers. Conv0.x, Conv1.x, Conv2.x, Conv3.x
and Conv4.x denote convolution units that may contain multiple convolutional layers, and residual units are
shown in double-column brackets. Conv1.x, Conv2.x and Conv3.x usually operate on different size feature
maps. These networks are essentially the same as [20], but some may have a different number of filters in each
layer. The downsampling is performed by convolutions with a stride of 2. E.g., [3×3, 64]×4 denotes 4 cascaded
convolution layers with 64 filters of size 3×3, S2 denotes stride 2.
Reported Results. For all the experiments on MLPs and CNNs (except CNNs in the few-shot
learning), we report testing error rates. For the few-shot learning experiment, we report testing
accuracy. For all the experiments on both GCNs and PointNets, we report testing accuracy. All results
are averaged over 10 runs of the model.
Multilayer perceptron. We conduct digit classification task on MNIST with a three-layer multilayer
perceptrons following this repository1 . The input dimension of each MNIST digit is 28 × 28, which is
784 dimensions after flattened. Our two hidden layers have 256 output dimensions, i.e., 256 neurons.
The output layer will output 10 logits for classification. We use a cross-entropy loss with softmax
function. For the optimization, we use a momentum SGD with learning rate 0.01, momentum 0.9 and
batch size 100. The training stops in 100 epochs.
Convolutional neural networks. The network architectures used in the paper are elaborated in
Table 10 and Table 11. For CIFAR-100, we use 128 as the mini-batch size. We use momentum SGD
with momentum 0.9 and the learning rate starts with 0.1, divided by 10 when the performance is
saturated. For ImageNet-2012, we use batch size 128 and start with learning rate 0.1. The learning rate
is divided by 10 when the performance is saturated, and the training is terminated at 700k iterations.
For ResNet-20 and ResNet-32 on CIFAR-100, we use exactly the same architecture used on CIFAR-
10 as [20]. The rotation matrix is initialized with random normal distribution (mean is 0 and variance
is 1). Note that, for all the compared methods, we always use the best possible hyperparameters to
make sure that the comparison is fair. The baseline has exactly the same architecture and training
settings as the one that OPT uses. If not otherwise specified, standard `2 weight decay (5e-4) is
applied to all the neural network including baselines and the networks that use OPT training.
Few-shot learning. The network architecture (Table 12) we used for few-shot learning experiments
is the same as that used in [10]. In our experiments, we show comparison of our OPT training with
standard training on ‘baseline’ and ‘baseline++’ settings in [10]. In ‘baseline’ setting, a standard CNN
model is pretrained on the whole meta-train dataset (standard non-MAML supervised training) and
1https://github.com/hwalsuklee/tensorflow-mnist-MLP-batch_
normalization-weight_initializers
19
Under review as a conference paper at ICLR 2021
Layer	CNN-4
Convl	3×3,64	—
Pooll	2 × 2 Max Pooling, Stride 2
Conv2	3×3,64	—
Pool2	2×2 Max Pooling, Stride 2
Conv3	3×3,64	—
Pool3	2×2 Max Pooling, Stride 2
Conv4	3×3,64	—
Pool4	2×2 Max Pooling, Stride 2
Linear Classifier	number of classes
Table 12: Architecture for few-shot learning. The number of classes is different for pretraining and finetuning.
Layer	Wide CNN-9(CIFAR-100)	Wide ResNet-18 (ImageNet-2012)		
Conv0.x	N/A	[7×7,64],Stride2 3 × 3, Max Pooling, Stride 2		
Convl.x	[3×3, 64] ×3 2×2 Max Pooling, Stride 2		3 × 3, 64 3 × 3, 64	× 2
Conv2.x	[3×3, 128]×3 2×2 Max Pooling, Stride 2		3 × 3, 128 3 × 3, 128	× 2
Conv3.x	[3×3, 256] × 3 2×2 Max Pooling, Stride 2		3 × 3, 256 3 × 3, 256	× 2
Conv4.x	N/A		3 × 3, 512 3 × 3, 512	× 2
Final	256-dim Fully Connected	Average Pooling		
Table 13: Our wide CNN-9 and wide ResNet-18 architectures with different convolutional layers.
later only the classifier layer is finetuned on few-shot dataset. ‘baseline++’ differs from ‘baseline’ on
the classifier: in ‘baseline’, each output dimension of the classifier is computed as the inner product
between weight W and input x, i.e. W ∙ x; while in 'baseline++, it becomes the scaled cosine distance
C kww/k where C is a positive scalar. Following [10], we set C = 2.
During pretraining, the model is trained for 200 epochs on the meta-train set of mini-ImageNet with
an Adam optimzer (learning rate 1e - 3, weight decay 5e - 4) and the classifier is discarded after
pretraining. The model is later finetuned, with a new classifier, on the few-shot samples (5 way,
support size 5) with a momentum SGD optimizer (learning rate 1e - 2, momentum 0.9, dampening
0.9, weight decay 1e - 3, batch size 4) for 100 epochs. We re-initialize the classifier for each few-shot
sample.
Graph neural networks. We implement the OPT training for GCN on the official repositories2. The
experimental settings also follow the official repository to ensure a fair comparison. For OPT (CP)
method, we use the original hyperparameters and experimental setup except the added rotation matrix.
For OPT (OGD) method, we use our own OGD optimizer in Tensorflow to train the rotation matrix
in order to maintain orthogonality and use the original optimizer to train the other variables.
Training a GCN with OPT is not that straightforward. Specifically, the forward model of GCN is
11
Z = Softmax (A ∙ReLU(A∙ X ∙ Wo)∙ Wι) where A = D 2 AD 2. We note that A is the adjacency
matrix of the graph, A=A+I (I is an identity matrix), and D= j Aij. X ∈ Rn×d is the feature
matrix of n nodes in the graph (feature dimension is d). W1 is the weights of the classifiers. W1 is
the weights of the classifiers. W0 is the weight matrix of size d × h where h is the dimension of the
hidden space. We treat each column vector of W0 as a neuron, so there are h neurons in total. Then
we apply OPT to train these h neurons of dimension d in GCN. We conduct experiments on Cora
and Pubmed datsets [56]. We aim to verify the effectiveness of OPT on GCN instead of achieving
state-of-the-art performance on this task.
Point cloud recognition. To simplify the comparison and remove all the bells and whistles, we
use a vanilla PointNet (without T-Net) as our backbone network. We apply OPT to train the MLPs
in PointNet. We follow the same experimental settings as [51] and evaluate on the ModelNet-40
dataset [66]. We exactly follow the same setting in the original paper [51] and the official repositories3.
2https://github.com/tkipf/gcn
3https://github.com/charlesq34/pointnet
20
Under review as a conference paper at ICLR 2021
Specifically, we multiply the rotation matrix to the original fixed neurons in all the 1 × 1 convolution
layers and the fully connected layer except the final classifier. All the rotation matrix is initialized
with random normal distribution. For PointNet experiments, we use point number 1024, batch size
32 and Adam optimizer started with learning rate 0.001, the learning rate will decay by 0.7 every
200k iterations, and the training is terminated at 250 epochs.
Experimental settings for S-OPT. For the experiment of S-OPT, the architecture of wide CNN-9
and wide ResNet-18 is given in Table 13. CNN-6 is the same as the one in Table 10. We use
standard data augmentation for CIFAR-100, following [48]. For ImageNet-2012, we use the same
data augmentation in [35, 48]. This data augmentation does not contain as many transformation as
the one in [20], so the final performance may be worse than [20]. However, all the compared methods
use the same data augmentation in our experiments, so the experiment is still a fair comparison. For
CIFAR-100, we use Nout = 300 and Nin = 750. For ImageNet, we use Nout = 700 and Nin = 1000.
We decrease the learning rate by a factor of 10 when the performance is saturated in the outer iteration.
21
Under review as a conference paper at ICLR 2021
E Loss Landscape Visualization (Normal Distribution
Perturbation)
E.1	Visualization Procedure
We generally follow the visualization procedure in [40]. However, since OPT has a different training
process, we use a modified visualization method but still make it comparable to the baseline.
Specifically, if we want to plot the loss landscape of a neural neural with loss L(θ) where θ is the
learnable model parameters, We need to first choose pretrained model parameters θ* as a center point.
Then we choose two random direction vectors δ and η. The 2D plot f (α, β) is defined as
f(α,β) =L(θ*+αδ+βη)	(38)
Which can be used as a 2D surface visualization. Note that, after We randomly initialize the direction
vectors δ and η (With normal distribution), We need to perform the filter normalization [40]. Specifi-
cally, We normalize each filter in δ and η to have the same norm as the corresponding filter in θ* .
The loss landscape of our baseline is plotted using this visualization approach.
In contrast, the learnable parameters in OPT are no longer the Weights of neurons. Instead, the
learnable parameters are the orthogonal matrices. More precisely, the trainable matrices are used
to perform orthogonalization in the neural netWorks (i.e., P in Fig. 2). We denote the combination
of all the trainable matrices as R, and the corresponding pretrained matrices as R* . Then the 2D
visualization of OPT is
f (α, β) = L(R* + αγ + βκ)	(39)
Where γ and κ are tWo random direction vectors (Which folloW the normal distribution) to perturb R*.
The visualization procedures of baseline and OPT are essentially the same except that the trainable
variables are different. Therefore, their loss landscapes are comparable.
E.2 Experimental Details
In Fig. 4, We vary α and β from -1.5 to 1.5 for both baseline and OPT, and then plot the surface of
2D function f . We use the CNN-6 (as specified in Appendix D) on CIFAR-100. We use the same
data augmentation as [45]. We train the netWork With SGD With momentum 0.9 and batch size 128.
We start With learning rate 0.1, divide it by 10 at 30k, 50k and 64k iterations, and terminate training
at 75k iterations. The training details basically folloWs [45]. We mostly use CP for OPT due to
efficiency. Note that, the other orthogonalization methods in OPT yields similar loss landscapes in
general. The pretrained model for standard training yields 37.59% testing error on CIFAR-100, While
the pretrained model for OPT yields 33.53% error. This is also reported in Section 6.
E.3 Full Visualization Results for the Main Paper
(a) Standard training
Figure 6: High-quality rendered loss landscapes of standard training and OPT.
FolloWing the same experimental settings in Appendix E.2, We render the 3D loss landscapes With
some color and lighting effects for Fig. 13. The visualization data is exactly the same as Fig. 4, and
22
Under review as a conference paper at ICLR 2021
we simply use ParaView to plot the figure. The rendered loss landsacpe better reflects that OPT yields
a much more smooth loss geometry.
We also give the large and full version of Fig. 4(b) (in the main paper) in the following figure. Fig. 7
is identical to Fig. 4(b) in the main paper except that Fig. 7 has larger size. From Fig. 7, we can
better observe the dramatically different loss landscape between standard training and OPT. From the
contour plots, we can better see that the red region of standard training is extremely flat and is highly
non-convex around the edge. In comparison to standard training, OPT has very smooth and relatively
convex contour shape.
(a) Standard training
(b) OPT
Figure 7: Comparison of loss landscapes between standard training and OPT (full results of Fig. 4(b) in the
main paper). Top row: loss landscape visualization with Cartesian coordinate system; Bottom row: loss contour
visualization.
Then we visualize the testing error landscape. Additionally, we include a high-quality visualization
using ParaView. We render the plot with lighting and color effects in order to better demonstrate the
difference between OPT and standard training. The visualization results are given in Fig. 8. The
comparison of testing error landscape shows that the parameter space of OPT is more robust than
standard training, because the testing error of OPT is increased in a slower speed while the model
is perturbed away from the pretrained parameters. In other words, the parameter space of OPT is
more robust to the random perturbation than standard training. Combining the visualization of loss
landscape, it is well justified that OPT can significantly alleviate training difficulty and improve
generalization.
23
Under review as a conference paper at ICLR 2021
(a) Standard training	(b) OPT
Figure 8: Comparison of testing error landscapes between standard training and OPT. Top row: high-quality
rendered testing error landscape visualization with lighting effects; Bottom row: testing error landscape
visualization with Cartesian coordinate system.
24
Under review as a conference paper at ICLR 2021
E.4 Loss Landscape Visualization of Different Neural Network
In order to show that the loss landscape of OPT is quite general and consistent across different neural
network architectures, we also visualize the loss landscape using a deep network network (CNN-9
as specified in Appendix D). The experiments are conducted on CIFAR-100. The results are given
in Fig. 9. We can see that the loss landscape of OPT is much more smooth than standard training,
similar to the observation for CNN-6. Therefore, the loss landscape difference between OPT and
standard training is consistent across different network architectures, and OPT consistently shows
better optimization landscape.
(a) Standard training
Figure 9: Comparison of loss landscapes between standard training and OPT on CIFAR-100 (CNN-9). Top row:
loss landscape visualization with Cartesian coordinate system; Bottom row: loss contour visualization.
(b) OPT
Then we show the landscape of testing error for CNN-9 on CIFAR-100. The results are given in
Fig. 10. Similar to CNN-6, the testing error landscape of OPT is more smooth and convex than
standard training. Moreover, OPT has a more flat local minima of testing error, while standard
training has a sharp local minima. The testing error landscape in Fig. 10 generally follows the same
pattern as the loss landscape in Fig. 9. The visualization further verifies the superiority of OPT is
very consistent across different network architectures.
25
Under review as a conference paper at ICLR 2021
(a) Standard training
Figure 10: Comparison of testing error landscapes between standard training and OPT on CIFAR-100 (CNN-9).
(b) OPT
26
Under review as a conference paper at ICLR 2021
E.5 Loss Landscape Visualization of Different Dataset
Similar to Appendix F.5, we also visualize the loss landscape of OPT and standard training on a
different dataset (CIFAR-10) with CNN-6. The loss landscape visualization is given in Fig. 11.
Although the loss landscape on CIFAR-10 is quite different from the one on CIFAR-100, we can still
observe that the loss landscape of OPT has a very flat local minima and the loss values are increasing
smoothly and slowly. In contrast, the loss landscape of standard training has a sharp local minima and
the loss values quickly increase to a large value. The red region of standard training will lead to very
small gradient, potentially affecting the training. From the contour plots, the comparison apparently
shows that the loss landscape of OPT is much more smooth than standard training.
(a) Standard training
Figure 11: Comparison of loss landscapes between standard training and OPT on CIFAR-10 (CNN-6). Top row:
loss landscape visualization with Cartesian coordinate system; Bottom row: loss contour visualization.
We also visualize the landscape of testing error in Fig. 12. The testing error landscape generally
follows the pattern in the loss landscape. One can easily observe that the parameter space of standard
training is very sensitive to perturbations. A small perturbation can make the model parameters
completely fail (i.e., the testing error dramatically increase). Differently, the parameter space of
OPT is much more robust to perturbations. The model parameter can still work well with a small
perturbation. Both Fig. 11 and Fig. 12 validate that the superiority of OPT is consistent across
different training datasets.
27
Under review as a conference paper at ICLR 2021
(a) Standard training
(b) OPT
Figure 12: Comparison of testing error landscapes between standard training and OPT on CIFAR-10 (CNN-6).
28
Under review as a conference paper at ICLR 2021
F	Loss Landscape Visualization (Uniform Distribution
Perturbation)
F.1 Visualization Procedure
Different from Appendix E, we choose two random direction vectors δ and η based on [0, 1] uniform
distribution to further justify the effectiveness of OPT. The 2D plot f (α, β) is defined as
f(α,β ) = L(θ* + αδ + βη)	(40)
which can be used as a 2D surface visualization. Note that, after we randomly initialize the direction
vectors δ and η with [0, 1] normal distribution, we need to perform the filter normalization [40].
Specifically, we normalize each filter in δ and η to have the same norm as the corresponding filter in
θ* . The loss landscape of our baseline is plotted using this visualization approach.
In contrast, the learnable parameters in OPT are no longer the weights of neurons. Instead, the
learnable parameters are the orthogonal matrices. More precisely, the trainable matrices are used
to perform orthogonalization in the neural networks (i.e., P in Fig. 2). We denote the combination
of all the trainable matrices as R, and the corresponding pretrained matrices as R* . Then the 2D
visualization of OPT is
f(α,β) = L(R * + αγ + βκ)	(41)
where γ and κ are two random direction vectors (which follow the [0, 1] uniform distribution) to
perturb R* . The visualization procedures of baseline and OPT are essentially the same except that
the trainable variables are different. Therefore, their loss landscapes are comparable.
F.2 Experimental Details
In Fig. 4, we vary α and β from -1 to 1 for both baseline and OPT, and then plot the surface of 2D
function f . We use the CNN-6 (as specified in Appendix D) on CIFAR-100. We use the same data
augmentation as [45]. We train the network with SGD with momentum 0.9 and batch size 128. We
start with learning rate 0.1, divide it by 10 at 30k, 50k and 64k iterations, and terminate training
at 75k iterations. The training details basically follows [45]. We mostly use CP for OPT due to
efficiency. Note that, the other orthogonalization methods in OPT yields similar loss landscapes in
general. The pretrained model for standard training yields 37.59% testing error on CIFAR-100, while
the pretrained model for OPT yields 33.53% error. This is also reported in Section 6.
F.3 Full Visualization Results
(a) Standard training
(b) OPT
Figure 13: High-quality rendered loss landscapes of standard training and OPT.
Following the same experimental settings in Appendix F.2, we render the 3D loss landscapes with
some color and lighting effects for Fig. 13. We first use ParaView to plot a high-qualify loss landscape
29
Under review as a conference paper at ICLR 2021
comparison between standard training and OPT. As expected, the loss landscape of OPT is much
more smooth than standard training. Note that, for the flat red region in standard training, we can still
observe numerous small local minima, while the red region of OPT is very smooth. Fig. 13 better
validates our analysis and discussion in Section 5.1, and also shows the superiority of OPT.
We provide the visualization results in the rest of the subsection. From Fig. 14, we can better observe
the dramatically different loss landscape between standard training and OPT.
(a) Standard training
(b) OPT
Figure 14: Comparison of loss landscapes between standard training and OPT (full results of Fig. 4(a) in the
main paper). Top row: loss landscape visualization with Cartesian coordinate system; Bottom row: loss contour
visualization.
To better understand the difference of the training dynamics between standard training and OPT, we
also plot the testing error landscapes in Fig. 15 for both methods. The testing error is computed
on the testing set of CIFAR-100 with the perturbed pretrained model (α and β are the perturbation
parameters). From the testing error landscape comparison in Fig. 15, we can see that once the baseline
pretrained model is slightly perturbed, the testing error will immediately increase to 99.99% which
is random selection-level testing error (because we have 100 balanced classes in total, randomly
picking a class leads to 0.01% accuracy). In contrast, the testing error landscape of OPT is much more
smooth. Even if we perturb the OPT pretrained model, we still end up with a reasonably low testing
error, show that the parameter space of OPT is more smooth and continuous. All these evidences
suggest that OPT is a better training framework for neural networks and can significantly alleviate
the optimization difficulty. In this following subsections, we aim to show that the loss and testing
error landscape difference between standard training and OPT is not a coincidence. We will show
that the improvement of OPT on the loss and testing error landscape is both dataset-agnostic and
architecture-agnostic.
30
Under review as a conference paper at ICLR 2021
ɪ
(a) Standard training	(b) OPT
Figure 15: Comparison of testing error landscapes between standard training and OPT. Top row: high-quality
rendered testing error landscape visualization with lighting effects; Bottom row: testing error landscape
visualization with Cartesian coordinate system.
31
Under review as a conference paper at ICLR 2021
F.4 Loss Landscape Visualization of Different Neural Network
Figure 16: Comparison of loss landscapes between standard training and OPT on CIFAR-100 (CNN-9). Top
row: loss landscape visualization with Cartesian coordinate system; Bottom row: loss contour visualization.
(a) Standard training
Figure 17: Comparison of testing error landscapes between standard training and OPT on CIFAR-100 (CNN-9).
(b) OPT
To show that the difference of the loss landscape between standard training and OPT is consistent
across different neural networks. We use a deeper CNN-9 (as specified in Appendix D) to visualize
the loss landscape. The experimental settings generally follow Appendix F.2. We use CNN-9 as our
backbone architecture and train it on CIFAR-100. The visualization of the loss landscapes is given in
32
Under review as a conference paper at ICLR 2021
Fig. 16. We observe that the loss landscapes of OPT with CNN-9 and CNN-6 are very similar. In
general, the conclusion that OPT yields better loss landscape still holds for a deeper neural network,
showing that the effectiveness of OPT is not architecture-dependent.
Moreover, we also visualize the landscape of testing error in Fig. 17. We can observe that the testing
error landscapes are somewhat similar to the loss landscape in Fig. 16. The results further validate
the superiority of OPT. We can observe in Fig. 17 that OPT has more smooth testing error landscape
and can make the training parameter space of the neural network less sensitive to perturbations.
33
Under review as a conference paper at ICLR 2021
F.5 Loss Landscape Visualization on Different Dataset
We also perform the same loss and testing error landscape visualization on CIFAR-10. The training
details basically follows Appendix F.2. For CIFAR-10, we use the same data augmentation as in
Appendix D. The results are given in Fig. 18. From Fig. 18, we can observe even more dramatic
difference of the loss landscape between standard training and OPT. In standard training, the loss
landscape exhibits highly non-convex and non-smooth behavior. There are countless local minima in
the loss landscape. Different from the results in Fig. 16, the loss landscape of standard training on
CIFAR-10 has some huge local minima that are hard to escape from. In contrast, the loss landscape
of OPT on CIFAR-10 does not show obvious and huge local minima and is far more convex and
smooth than standard training. The contour maps show more significant difference between standard
training and OPT. The contour map of OPT shows the shape of a single symmetric and convex valley,
while the contour map of standard training presents the shape of multiple highly irregular valleys.
The visualization further validate that the improvement of OPT on optimization landscape is very
consistent across different training datasets.
Figure 18: Comparison of loss landscapes between standard training and OPT on CIFAR-10 (CNN-6). Top row:
loss landscape visualization with Cartesian coordinate system; Bottom row: loss contour visualization.
-1.00	-0.75	-0.50	-0.25	0.00	0.25	0.50	0.75	1.00
(b) OPT
Then we also visualize the testing error landscape of standard training and OPT in Fig. 19. As
expected, the testing error landscape of standard training shows that changing the pretrained model
parameters with a very small perturbation could lead to a dramatic increase in testing error. It
indicates that the parameter space of standard training is very sensitive to even a small perturbation.
In comparison, the testing error landscape of OPT shows similar shape to the training loss landscape
which is the shape of a single regular, smooth and convex valley. We can conclude that OPT has huge
advantages over standard training in terms of the optimization landscape. Although the conclusion
34
Under review as a conference paper at ICLR 2021
is drawn from a simple visualization method, it can still shed some light on why OPT yields better
training dynamics and generalization ability.
(a) Standard training	(b) OPT
Figure 19: Comparison of testing error landscapes between standard training and OPT on CIFAR-10 (CNN-6).
35
Under review as a conference paper at ICLR 2021
G Theoretical Discussion on Optimization and Generalization
The key problem we discuss in this section is why OPT may lead to easier optimization and better
generalization. We have already shown that OPT can guarantee the minimum hyperspherical en-
ergy (MHE) in a probabilistic sense. Although empirical evidences [45] have shown significant and
consistent performance gain by minimizing hyperspherical energy, why lower hyperspherical energy
will lead to better generalization is still unclear. We argue that OPT leads to better generalization from
two aspects: how OPT may affect the training and generalization, and why minimum hyperspherical
energy serves as a good inductive bias. We note that rigorously proving that OPT generalizes better is
out of the scope of this paper and remains our future work. The section serves as a very preliminary
discussion for this topic, and hopefully the discussion can inspire more theoretical studies about OPT.
Our goal here is to leverage and apply existing theoretical results [1, 12, 31, 37, 59, 67] to explain the
role that MHE plays rather than proving sharp and novel generalization bounds. We emphasize that
our paper is NOT targeted as a theoretical one that proves novel generalization bounds.
We simply consider one-hidden-layer networks as the hypothesis class:
nn
F = {f (x) = Xvjσ(wj>x) : vj ∈ {±1}, X kwjk ≤ Cw}	(42)
j=1	j=1
where σ(∙)=max(0, ∙) is ReLU. Since the magnitude of Vj can be scaled into Wj, We can restrict Vj
to be ±1. Given a set of i.i.d. training sample {xi, yi}im=1 where x∈Rd is drawn uniformly from the
unit hypersphere, we minimize the least square loss L = 2m Pm=ι(yi - f (xi))2. The gradient w.r.t.
wi is
∂L
d Wj
1m
一 fX (f (Xi) - yi)Vjσ0(w>Xi)Xi.
m i=1
(43)
Let W := {w>,…，w> }> be the column concatenation of neuron weights. We aim to identify the
conditions under which there are no spurious local minima. We rewrite that
∂L	∂L >	∂L > >
∂w = 1∂wτ ,∙∙∙,∂wn)= r
(44)
where r ∈Rm r = ∖ f(xi) -yi, D ∈Rn×m, and Dij = Viσ0(w>Xj)xj. Therefore, we can obtain
the following inequality:
∂ L
∂W
krk≤ Sm(D)
(45)
where krk is the training error and sm(D) is the minimum singular value of D. If we need the
training error to be small, then we have to lower bound sm (D) away from zero. Therefore, the
essential problem now becomes the relationship between MHE and the lower bound of sm(D). We
have the following result from [67]:
Lemma 4. With probability larger than 1 — m exp(-mγm/∕8) — 2m2 exp(-4 log2 d) 一 δ, we will
have that Sm(D)2 ≥ 1 nmγm — cnρ(W) where
P(W) ≤ logd√2L2(W)m(ɪlogj)1/4
d	mδ
(46)
+ 21θg m m ʌ/ɪ log I + √7=m mL2(W) + 2,
d	3m	δ d
and L2(W) = n12 £%=1 k(wi, Wj)2 — E%v[k(u,v)2]. The kernel function k(u, V) is 2 —
2∏ arccos( k⅛V⅛ ).
Once MHE is achieved, the neurons will be uniformly distributed on the unit hypersphere. From
Lemma 4, we can see that if the neurons are uniformly distributed on the unit hypersphere, L2 (W)
will be very small and close to zero. Then P(W) will also be small, leading to large lower bound for
36
Under review as a conference paper at ICLR 2021
Sm(D). Therefore, MHE can result in small training error once the gradient norm ∣∣ ∂∂L ∣∣ is small.
The result implies no spurious local minima if we use OPT for training.
Furthermore, suppose that k ∂∂WLk2 ≤ e, [67] also proves a training error bound O(e) and a gener-
alization bound bound O(e + √m) based on the assumption that W belongs to a specific set GW
(for the definition of GW , please refer to [67]). Therefore, MHE is also connected to the training and
generalization error. Note that, the analysis here is highly simplified and the purpose here is to give
some justifications rather than rigorously proving any bound.
We further argue that MHE induced by OPT serves as an important inductive bias for neural networks.
As the standard regularizer for neural networks, weight decay controls the norm of the neuron weights,
regularizing essentially one dimension of the weight. In contrast, MHE completes an important
missing pieces by regularizing the remaining dimensions of the weight. MHE encourages minimum
hyperspherical redundancy between neurons. In the linear classifier case, MHE impose a prior of
maximal inter-class separability.
H	More Discussions
Semi-randomness. OPT fixes the randomly initialized neuron weight vectors and simply learns
layer-shared orthogonal matrices, so OPT naturally imposes strong randomness to the neurons. OPT
well combines the good generalizability from randomness and the strong approximation power from
neural networks. Such randomness suggests that the specific configuration of relative position among
neurons does not matter that much, and the coordinate system is more crucial for generalization.
[32, 53, 60] also show that randomness can be beneficial to generalization.
Flexible training. First, OPT can used in multi-task training [49] where each set of orthogonal
matrices represent one task. OPT can learn different set of orthogonal matrices for different tasks
with the neuron weights remain the same. Second, we can perform progressive training with OPT. For
example, after learning a set of orthogonal matrices on a large coarse-grained dataset (i.e., pretraining),
we can multiple the orthogonal matrices back to the neuron weights and construct a new set of neuron
weights. Then we can use the new neuron weights as a starting point and apply OPT to train on a
small fine-grained dataset (i.e., finetuning).
Limitations and open problems The limitations of OPT include more GPU memory consumption
and heavy computation during training, more numerical issues when ensuring orthogonality and
weak scalability for ultra wide neural networks. Therefore, there will be plenty of open problems
in OPT, such as scalable and efficient training. Most significantly, OPT opens up a new possibility
for studying theoretical generalization of deep networks. With the decomposition to hyperspherical
energy and coordinate system, OPT provides a new perspective for future research.
37
Under review as a conference paper at ICLR 2021
I On Parameter-Efficient OPT
I.1 Formulation
Since OPT over-parameterizes the neurons, it will consume more GPU memory in training (note
that, the number of parameters will not increase in testing). For a d-dimensional neuron, OPT will
learn an orthogonal matrix of size d × d that applies to the the neuron. Therefore, we will need d2
extra parameters for one layer of neurons, making the training more expensive in terms of the GPU
memory. Although the extra training overhead in OPT will not affect the inference speed of the
trained neural networks, we still desire to achieve better parameter efficiency in OPT. To this end, we
discuss some design possibilities for the parameter-efficient OPT (PE-OPT) in this section.
Original OPT over-parameterize a neuron v ∈ Rn×n with Rv where R is a layer-shared orthogonal
matrix of size d × d. We aim to reduce the effective parameters of this d × d orthogonal matrix. We
incorporate a block-diagonal structure to the orthogonal matrix R. Specifically, we formulate R as
Diag(R⑴,R(2),…,R(k)) where R(i) is an orthogonal matrix with size d% X d% (it is easy to see
that we need d = Pi di). As an example, we only consider the case where all R(i are of the same
size (i.e., di = d2 =…=dk = d). It is also obvious that as long as each block is an orthogonal
matrix, then the overall matrix R remains an orthogonal matrix.
Unconstrained Block Matrix Ru
Figure 20: Comparison between the block-shared matrix Rs and the unconstrained block matrix Ru.
First, we consider that all the block matrices on the diagonal of the orthogonal matrix R are shared,
meaning that R = Diag(R⑴,R⑴,…，R⑴)(i.e., R⑴=R⑵=…=R(k)). Therefore, we
have a block-diagonal matrix Rs with shared block R(1) as the final orthogonal matrix for the neuron
v:
R(1)	0	…0
0
.
.
.
0
R(1)
(47)
..	0
0	R(1)
where R(I) ∈ Rk × d. The effective number of parameters for the orthogonal matrix Rs immediately
reduces to d2. The left figure in Fig. 20 gives an intuitive illustration for the block-shared matrix Rs.
Therefore, PE-OPT only needs to learn R(1) in order to construct the orthogonal matrix of size d × d.
Second, we consider that all the diagonal block matrices are independent, indicating that R =
Diag(R(1), R⑵，…，R(k)) where R(i), ∀i are different orthogonal matrices in general. We term
such matrix R as unconstrained block matrix. Therefore, we have the unconstrained block diagonal
matrix Ru as
(48)
where the orthogonal matrices R(i), ∀i will be learned independently. The effective number of
parameters for the orthogonal matrix Ru is d2, making it more flexible than the block-shared matrix
Rs.
38
Under review as a conference paper at ICLR 2021
Let’s consider a convolution neuron (i.e., convolution filter) v ∈ Rc1×c2×c3 (e.g., a typical convolution
neuron is of size 3 × 3 × 64) as an example. The orthogonal matrix R for the convolution neuron
is of size (c1c2c3) × (c1c2c3). Typically, we will divide the neuron into k sub-neuron along the
c3-axis, each with size ci X c2 X c3. Then in order to learn a block-shared orthogonal matrix Rs,
We will essentially learn a shared orthogonal matrix of size (1 c1c2c3) X (1 c1c2 c3) that applies to
each sub-neuron (there are k sub-neurons of size ci X c2 X c3 in total). For the case of learning a
unconstrained block-diagonal orthogonal matrix Ru, we simply learn different orthogonal matrices
for different sub-neurons.
I.2 Experiments and Results
We conduct the image recognition experiments on CIFAR-100 with CNN-6 described in Table 10.
The setting is exactly the same as Section 6.2. For the convolution filter, we use the size of 3 X 3 X 64,
i.e., ci = 3, c2 = 3, c3 = 64. The results are given in Table 14 and Table 15. “# Parameters” in
both tables denote the number of effective parameters for the orthogonal matrix R in a single layer.
The baseline with fixed neurons is only to train the final classifiers with the randomly initialized
neuron weights staying fixed. It means that this baseline basically removes the learnable orthogonal
matrices but still fixes the neuron weights, so it only achieves 73.81% testing error. As expected,
as the number of effective parameters goes down, the performance of PE-OPT generally decreases.
One can also observe that using separate orthogonal matrices generally yields better performance
than shared orthogonal matrices. k = 2 and k = 4 seems to be a reasonable trade-off between better
accuracy and less parameters.
When k becomes larger (i.e., the number of parameters become less) in the case of block-shared
orthogonal matrices, we find that PE-OPT (LS) performs the best among all the variants. When
k becomes larger (i.e., the number of parameters become less) in the case of unconstrained block
orthogonal matrices, we can see that both PE-OPT (GS) and PE-OPT (LS) performs better than the
other variants.
Method	# Parameters	PE-OPT (CP)	PE-OPT (GS)	PE-OPT (HR)	PE-OPT (LS)	PE-OPT (OGD)
c3/k = 64 (k = 1) (i.e., Original OPT)	331.7K	3353	33.02	35.67	34.48	33.33
c3/k = 32 (k = 2)	82.9K	34.93	34.39	35.83	34.50	35.06
c3/k = 16 (k = 4)	20.7K	39.40	39.13	39.67	37.58	39.80
c3/k = 8 (k = 8)	5.2K	47.77	46.65	46.69	45.62	47.43
c3/k = 4 (k = 16)	1.3K	56.65	55.91	55.69	54.75	57.15
c3/k = 2 (k = 32)	0.3K	63.46	62.65	62.38	61.60	62.46
c3/k = 1 (k = 64)	0.1K	67.36	67.11	67.05	66.61	67.23
Baseline	-			37.59		
Baseline with fixed random neurons	-			73.81		
Table 14: Testing error (%) on CIFAR-100 with different settings of PE-OPT (with block-shared orthogonal
matrix Rs ).
Method	# Parameters	PE-OPT (CP)	PE-OPT (GS)	PE-OPT (HR)	PE-OPT (LS)	PE-OPT (OGD)
c3/k = 64 (k = 1) (i.e., Original OPT)	331.7K	33.53	33.02	35.67	34.48	33.33
c3/k = 32 (k = 2)	165.9K	33.54	33.15	35.65	34.09	34.27
c3/k = 16 (k = 4)	82.9K	34.77	34.50	35.71	34.96	35.97
c3/k = 8 (k = 8)	41.5K	37.25	36.43	36.40	36.17	39.75
c3/k = 4 (k = 16)	20.7K	40.74	39.89	39.98	39.93	43.43
c3/k = 2 (k = 32)	10.4K	45.36	44.77	44.83	44.61	48.98
c3/k = 1 (k = 64)	5.2K	50.94	49.16	49.57	49.23	54.93
Baseline	-			37.59		
Baseline with fixed random neurons	-			73.81		
Table 15: Testing error (%) on CIFAR-100 with different settings of PE-OPT (with unconstrained block
orthogonal matrix Ru).
39
Under review as a conference paper at ICLR 2021
J On Generalizing OPT: Over-Parameterized Training with
Constraint
OPT opens many new possibilities in training neural networks. We consider a simple generalization
to OPT in this section to showcase the great potential of OPT. Instead of constraining the over-
parameterization matrix R ∈ Rd×d in Eq. 1 to be orthogonal, we can use any meaningful structural
constraints for this matrix, and even regularize it in a task-driven way. Furthermore, instead of a linear
over-parameterization (i.e., multiplying a matrix R) to the neuron, we can also consider nonlinear
mapping. We come up with the following straightforward generalization to OPT (the settings and
notations exactly follow Eq. 1):
mn
Standard: min L y,	uivi>xj
vi ,ui ,∀i
j=1	i=1
mn
Original OPT: min ^X L(y, ^X Ui(Rvi)>Xj)
R,ui,∀i j=1	i=1
s.t. R>R = RR> = I
(49)
mn
Generalized OPT: min T L(y, X Ui(T(Vi))>xQ
R,ui ,∀i
,ui, j=1	i=1
s.t. Some constraints on T(∙)
where T(∙) : Rd → Rd denotes some transformation (including both linear and nonlinear). Notice
that the generalized OPT (G-OPT) no longer requires orthogonality. Such formulation of G-OPT can
immediately inspire a number of instances. We will discuss some obvious ones here.
If We consider T(∙) to be a linear mapping, We may constrain R to be symmetric positive definite
other than orthogonal. A simple way to achieve that is to use Cholesky factorization LL> where L
is a loWer triangular matrix to parameterize the matrix R. Essentially, We learn a loWer triangular
matrix L and use LL> to replace R in OPT. The positive definiteness provides the transformation R
With some geometric constraint. Specifically, a positive definite R only transforms the neuron Weight
v to the direction that has the angle less than 2 to v, because v>Rv > 0. Moreover, we can also
require the transformation to have structural constraints on R. For example, R can be upper (loWer)
triangular, banded, symmetric, skew-symmetric, upper (lower) Hessenberg, etc.
We can also consider T(∙) to be a nonlinear mapping. A obvious example is to use a neural
network (e.g., MLP, CNN) as T(∙). Then the nonlinear G-OPT will share some similarities with
HyperNetworks [17] and Network-in-Network [43]. If we further consider T(∙) to be dependent on
the input, then the nonlinear G-OPT will have close connections to dynamic neural networks [29, 46].
To summarize, OPT provides a novel and effective framework to train neural networks and may
inspire many different threads of future research.
40
Under review as a conference paper at ICLR 2021
K Hyperspherical Energy Training Dynamics of Individual Layers
We also plot the hyperspherical energy (E(Vi ∣n=J = Pn=ι Pn=ι,j=i IM - Vj IIT in which Vi =高
is the i-th neuron weight projected onto the unit hypersphere.) in every layer of CNN-6 during training
to show how these hyperspherical energies are being minimized. From Fig. 21, we can observe that
OPT can always maintain the minimum hyperspherical energy during the entire training process,
while the MHE regularization cannot. Moreover, the hyperspherical energy of the baseline will also
decrease as the training proceeds, but it is still much higher than the OPT training.
Conv 2.2
Conv 1.1
HS-MHE
...OPT-OR
...OPT-CP
...OPT-OGD
0.520
g 0.515
0.510
! 0.505
0.500
Iterations
Iterations
___
---OPT-OR
OPT-CP
-OPT-OGD
iOPT-GS
Iterations
Conv 2.1
----Beselinc
HS-MHE
----OPT-OR
...OPT-CP
----OPT-OOD
XSIəug I3μ3qdsjadXH
Conv 3.1
0.0	1.0	2.0	3.0	4.0	5.0	6.0	7.0
Iterations
Conv 3.2
XSIəuwPOPaqdSJOdXH
0.0	1.0	2.0	3.0	4.0	5.0	6.0	7.0
Iterations
fclayer
κ9J9uwlEOU3qds.βdXH
Figure 21: Training dynamics of hyperspherical energy in each layer of CNN-6. We average results with 10 runs.
41
Under review as a conference paper at ICLR 2021
L Geometric Properties of Randomly Initialized Neurons
There are many interesting geometric properties [5, 6] of random points distributed independently
and uniformly on the unit hypersphere. We summarize a few of them that make randomly initialized
neurons distinct from any deterministic neuron configuration. Note that, there exist many deterministic
neuron configurations that can also achieve very low hyperspherical energy, and this section aims to
describe a few unique geometric properties of randomly initialized neurons.
There are two widely used geometric properties corresponding to a neuron configuration (i.e., a set of
neurons) WZN = {W 1,…，WN ∈ Sd}. In the main paper, We define neurons on Sd-1, but without
loss of generality we define neurons on Sd here for convenience. The first one is the covering radius:
α(WN):= α(WN; SdT) =max min arccos(u, Wi)	(50)
u∈Sd 1≤i≤N
which is the biggest geodesic distance from a neuron in Sd to the nearest point in WZN. The second
one is the separation distance:
Ψ(WN )：= 一 min	arccos(Wi, Wj)	(51)
N	1≤i,j,≤N,i6=j	i j
which gives the least geodesic distance between arbitrary two points in WZN. Random points (i.e.,
randomly initialized neurons) typically have poor separation properties, since the separation is
sensitive to the specific placement of points. [5] shows an example on S1 to illustrate this observation.
[5] considers a different but related quantity, i.e., the sume of powers of the “hole radii”. A set of
neurons WZN on Sd uniquely defines a convex polytope, which can be viewed as the convex hull
of the neuron configuration. Each facet of the polytope defines a “hole”. Such a hole denotes the
maximal spherical cap for a facet that contains neurons of WZN only on the boundary. It is easy to see
that the geodesic radius of the largest hole is the covering radius α(W⅞). We assume that for the set
of neurons WN, there are fd holes (i.e., facets) in total. Therefore, the i-th hole radius is defined as
Pi = Pi(WZN) which is the Euclidean distance in Rd+1 between the center of the i-th spherical cap
and the boundary. The i-th spherical cap is located on the sphere corresponding to the i-th facet. We
have that Pi = 2sin(爸)where ai is the geodesic radius of the i-th spherical cap. We are interested
in the sums of the p-th powers of the hole radii, i.e.,
fd
P = X(Pi)p
i=1
where p is larger than zero. For large p, the largest hole dominates:
lim (P)1 = lim f X(Pi)P) P = 1max Pi = 2sin( α(WN))
p→∞	p→∞	1≤i≤fd	2
i=1
(52)
(53)
where P(WN):=max1≤i≤fd Pi. Then we introduce some useful notations to state the geometric
properties. Let ψd be the surface area of Sd, and we have that
n d+1
,	2∏ 2
ψd = Γ(ψ),	(54)
and we also define the following quantities (with ψ0 = 2):
=1 ψd-1 = 1 r( d+1)
Kd : = d~ψ^ = d √∏γ( d)	(55)
B“： = —--κ-
d d + 1 (κd)d
where Kd can be alternatively defined with the recursion: κι = ∏ and Kd = 2*'T . [55] gives the
expected number of facets constructed from N random neurons that are independently and uniformly
distributed on the unit hypersphere Sd :
E[fd] = BdN (1 + o(1))	(56)
42
Under review as a conference paper at ICLR 2021
where N → ∞. Then we introduce the main results of [5] (asymptotics for the expected moments of
the hole radii) in the following lemma:
Lemma 5. If P ≥ 0 and Wι, •一WN are N neurons on Sd that are independently and randomly
distributed with respect to the normalized surface area measure σd on Sd, then we have that
Ep]=Bd(κd)-d ?+T )NN+)1)(1+O(N-2))
(57)
=cd,pN1-P (1 + O(N- d))
as N → ∞, where ρi = ρi,N is the Euclidean hole radius associated with the i-th facet of the convex
hull of WN, cd,p := BdBd,p, and Bd,p := r(d(dd) (Kd)- P. The O-terms above depend on d and P.
As we mentioned, there are many deterministic point (i.e., neuron) configurations such as minimizing
hyperspherical energy (i.e., Riesz s-energy) [45] (as s → ∞, the minimal s-energy points approach
the best separation), maximizing the determinant for polynomial interpolation [57], Fibonacci
points, spherical t-designs, minimizing covering radius (i.e., best covering problem), maximizing the
separation (i.e., best packing problem) and maximizing the s-polarization, etc. We note that randomly
initialized neurons are quite different from these deterministic neuron configurations and have unique
geometric properties.
43
Under review as a conference paper at ICLR 2021
M Additional Experimental Results
M.1 High vs. low hyperspherical energy
We empirically verify that high hyperspherical energy corresponds to inferior generalization perfor-
mance. To initialize neurons with high hyperspherical energy, we use a random initialization with
mean equal to 1e - 3, 1e - 2, 2e - 2, 3e - 2 and 5e - 2. We use CNN-6 to conduct experiments on
CIFAR-100. The results in Table 16 (“N/C” denotes not converged) show that the network with higher
hyperspherical energy is more difficult to converge. Moreover, we find that if the hyperspherical
energy is larger than a certain value, then the network cannot converge at all. Note that, when the
hyperspherical energy is small (near the minima), a little change in hyperspherical energy (e.g., from
3.5109 to 3.5160) can lead to dramatic generalization gap (e.g., from 32.49% error rate to 39.51%).
As expected, one can also observe that higher hyperspherical energy leads to worse generalization.
Mean	Energy	Error (%)
0	3.5109	32.49
1e-3	3.5117	33.11
1e-2	3.5160	39.51
2e-2	3.5531	53.89
3e-2	3.6761	N/C
5e-2	4.2776	N/C
Table 16: Testing error for different initial energy.
M.2 Training without batch normalization
We further evaluate how OPT performs without batch normalization (BN). In specific, we use CNN-6
as our backbone network and test on CIFAR-100. From Table 17, one can observe that all OPT
variants again outperform both the baseline and HS-MHE [45], validating that OPT is robust and
can work reasonably well even without batch normalization. Among all the OPT variants, Cayley
parameterization achieves the best testing error with approximately 4% lower than the standard
training.
Method	Error (%)
Baseline	38.95
HS-MHE	36.90
OPT (GS)	35.61
OPT (HR)	37.51
OPT (LS)	35.83
OPT (CP)	34.88
OPT (OGD)	35.38
Table 17: Training without BN on CIFAR-100.
M.3 S tandard Deviation
Due to the space limit, we have not provided the standard deviation for the results of OPT. In general,
OPT performs quite stably and consistently outperforms all the compared methods. We show the
standard deviation for the results in Table 4. The results are given in Table 18. We can observe
that the standard deviations are quite small for OPT, showing that OPT’s performance gain is very
consistent and significant.
Method	CNN-6	CNN-9
Baseline	37.59±0.24	33.55±0.26
HS-MHE	34.97±0.15	32.87±0.17
OPT (GS)	33.02±0.11	31.03±0.14
OPT (HR)	35.67±0.14	32.75±0.15
OPT (LS)	34.48±0.09	31.22±0.11
OPT (CP)	33.53±0.13	31.28±0.12
OPT (OGD)	33.33±0.21	31.47±0.23
OPT (OR)	34.70±0.18	32.63±0.17
Table 18: Error (%) with standard deviation on CIFAR-100.
M.4 Training Runtime
One of the weaknesses of OPT is its training runtime. As expected, OPT is more costly than standard
training, but fortunately, OPT does not affect the inference speed of a trained model. We use CNN-6
44
Under review as a conference paper at ICLR 2021
and perform image classification on CIFAR-100. The batch size is 128. We use a single RTX 6000
for the experiment. The results are given in Table 19. We can see taht the slowest OPT is around 35X
slower than the baseline. However, OPT with OGD, OR and CP performs just slightly worse than
the baseline. In general, the training runtime does not matter that much in practice, and we usually
care more about the inference speed during testing. We emphasize again that OPT does not affect the
inference speed of neural networks.
Method	Runtime
Baseline	3.28
OPT (GS)	14.01
OPT (HR)	71.32
OPT (LS)	107.47
OPT (CP)	7.06
OPT (OGD)	4.30
OPT (OR)	3.43
Table 19: Training runtime (s) per 100 iterations.
45