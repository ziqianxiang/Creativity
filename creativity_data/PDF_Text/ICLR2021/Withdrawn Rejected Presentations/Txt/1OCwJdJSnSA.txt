Under review as a conference paper at ICLR 2021
Disentangled cyclic reconstruction for do-
MAIN ADAPTATION
Anonymous authors
Paper under double-blind review
Ab stract
The domain adaptation problem involves learning a unique classification or regres-
sion model capable of performing on both a source and a target domain. Although
the labels for the source data are available during training, the labels in the target
domain are unknown. An effective way to tackle this problem lies in extracting
insightful features invariant to the source and target domains. In this work, we
propose splitting the information for each domain into a task-related representa-
tion and its complimentary context representation. We propose an original method
to disentangle these two representations in the single-domain supervised case. We
then adapt this method to the unsupervised domain adaptation problem. In partic-
ular, our method allows disentanglement in the target domain, despite the absence
of training labels. This enables the isolation of task-specific information from
both domains and a projection into a common representation. The task-specific
representation allows efficient transfer of knowledge acquired from the source do-
main to the target domain. We validate the proposed method on several classical
domain adaptation benchmarks and illustrate the benefits of disentanglement for
domain adaptation.
1 Introduction
The wide adoption of Deep Neural Networks in practical supervised learning applications is hin-
dered by their sensitivity to the training data distribution. This problem, known as domain shift,
can drastically weaken, in real-life operating conditions, the performance of a model that seemed
perfectly efficient in simulation. Learning a model with the goal of making it robust to a specific
domain shift is called domain adaptation (DA). Often, the data available to achieve DA consist of a
labeled training set from a source domain and an unlabeled sample set from a target domain. This
yields the problem of unsupervised domain adaptation (UDA).
In this work, we take an information disentanglement perspective on UDA. We argue that a key to
efficient UDA lies in separating the necessary information to complete the network’s task (classifica-
tion or regression), from a task-orthogonal information which we call context or style. Disentangle-
ment in the target domain seems however a difficult endeavor since the available data is unlabeled.
Our contribution is two-fold. We propose a formal definition of the disentanglement problem for
UDA which, to the best of our knowledge, is new. Then we design a new learning method, called
DiCyR (Disentangled Cyclic Reconstruction), which relies on cyclic reconstruction of inputs in or-
der to achieve efficient disentanglement, including in the target domain. We derive DiCyR both in
the supervised learning and in the UDA cases.
This paper is organized as follows. Section 2 presents the required background on supervised learn-
ing and UDA, and proposes a definition of disentanglement for UDA. Section 3 reviews recent work
in the literature that allow for a critical look at our contribution and put it in perspective. Section 4
introduces DiCyR, first for the single-domain supervised learning case and then for the UDA prob-
lem. Finally, Section 5 empirically evaluates DiCyR against state-of-the-art methods and discusses
its strengths, weaknesses and variants. Section 6 summarizes and concludes this paper.
1
Under review as a conference paper at ICLR 2021
2 Problem definition
In this section, we introduce the notations and background upon which we build the contributions
of Section 4. Let X be an input space of descriptors and Y an output space of labels. A supervised
learning problem is defined by a distribution ps(x, y) over elements of X × Y. In what follows,
ps will be called the source distribution. One wishes to estimate a mapping f that minimizes a
loss function of the form E(x,y)〜Ps [l(∕(χ), y)]. The optimal estimator is denoted f and one often
writes the distribution P(y|x) as y 〜f (x) + η, where η captures the deviations between y and
f (x). Hence, one tries to learn f. In practice, the loss can only be approximated using a finite set
of samples {(xi, yi)}in=1 all independently drawn from ps and f is a parametric function (such as a
deep neural network) of the form y = f(x; θ).
Domain adaptation (DA) consists in considering a target distribution pt over X × Y that differs from
ps, and the transfer of knowledge from learning in the source domain (ps) to the target domain (pt).
Specifically, unsupervised DA exploits the knowledge of a labelled training set {(xis, yis)}in=1 sam-
pled according tops, and an unlabelled data set {(xit)}im=1 sampled according topt. For instance, the
source domain data could be a set of labelled photographs of faces, and the target domain data, a set
of unlabelled face photographs, taken with a different camera under different exposure conditions.
The problem consists in minimizing the target loss E(χ,y)〜Pt [l(∕(x), y)].
We suppose that a necessary condition to benefit from the knowledge available in the source domain
and transfer it to the target domain is the existence of a common information manifold between do-
mains, where an input’s projection is sufficient to predict the labels. We call this useful information
task-specific or task-related. The complimentary information should be called task-orthogonal; it is
composed of information that is present in the input but is not relevant to the task at hand. For the
sake of naming simplicity, we will call this information style. However we insist that this should not
be confused with the classical notion of style.
Let Πτ : X → T and Πσ : X → S denote two projection operators, where T and S denote
respectively the latent task-related information space and the latent style-related information space.
Let Π be the joint projection Π(x) = (∏τ(χ), ∏σ (x)). Conversely, We shall note Π : T ×S → X
a reconstruction operator. And finally, C : T → Y will denote the labeling operator which only uses
information from T. We consider that the information of the elements ofX is correctly disentangled
—
(∏τ, ∏σ) if one can find ∏ and C such that:
by Π
C1:
C2:
C3:
C4:
C ◦ Πτ minimizes the loss (and thus fits f on the appropriate domain),
Π ◦ Π fits the identity operator idχ,
With X, T, S the random variables in X, T, S, the mutual information I(T, S|X) = 0,
There is no function g : T → X such that g ◦ Πτ = idX,
Condition C1 imposes that the projection into T retains enough information to correctly label sam-
ples. Condition C2 imposes that all the information necessary for the reconstruction is preserved by
the separation performed by Π. Condition C3 states that no information is present in both T and
S . Condition C4 impose that the information contained in T alone is insufficient to reconstruct an
input, and thus the information of S is necessary. Note that the symmetrical condition is unneces-
sary, since the combination of C1 and C3 already guarantees that S cannot contain the task-related
information. Overall, solving this disentanglement problem for DA implies finding a quadruplet
h∏τ, ∏σ, ∏, Ci that meets the conditions above. In particular, note that conditions C3 and C4 open a
perspective to a formulation of disentanglement in the general case.
3 Related work
Disentanglement between the domain-invariant, task-related information and the domain-specific,
task-orthogonal, style information is a desirable property to have for DA. In the next paragraphs, we
cover important work in representation disentanglement, domain adaptation and their interplay.
Before deep learning became popular, Tenenbaum & Freeman (2000) presented a method using
bi-linear models able to separate style from content. More recently, methods based on generative
2
Under review as a conference paper at ICLR 2021
models have demonstrated the ability to disentangle factors of variations from elements of a single
domain (Rifai et al., 2012; Mathieu et al., 2016; Chen et al., 2016; Higgins et al., 2017; Sanchez
et al., 2019). In a cross-domain setting, Gonzalez-Garcia et al. (2018) use pairs of images with the
same labels from different domains to separate representations into a shared information common to
both domains and a domain-exclusive information. We note that these approaches do not explicitly
aim at respecting all conditions listed in Section 2. Additionally, most require labeled datasets (and
in some cases even paired datasets) and thus do not address the unsupervised DA problem.
One approach to UDA consists in aligning the source and target distributions statistics, a topic
closely related to batch normalization (Ioffe & Szegedy, 2015). Sun et al. (2017) minimize the dis-
tance between the covariance matrices of the features extracted from the source and target domains.
Assuming the domain-specific information is contained inside the batch normalization layers, Li
et al. (2017) align the batch statistics by adopting a specific normalization for each domain. Cari-
ucci et al. (2017) aim to align source and target feature distributions to a reference one and introduce
domain alignment layers to automatically learn the degree of feature alignment needed at different
levels of the network. Similarly, Roy et al. (2019) replace batch normalization layers with domain
alignment layers implementing a so-called feature whitening. A major asset of these methods is
the possibility to be used jointly with other DA methods (including the one we propose in Section
4). These methods jointly learn a common representation for elements from both domains. Con-
versely, Liang et al. (2020) freeze the representations learned in the source domain before training a
target-specific encoder to align the representations of the target elements by maximizing the mutual
information between intermediate feature representations and outputs of the classifier.
Ensemble methods have also been applied to UDA (Laine & Aila, 2017; Tarvainen & Valpola,
2017). French et al. (2018) combine stochastic data augmentation with self-ensembling to minimize
the prediction differences between a student and a teacher network in the target domain.
Another approach involves learning domain-invariant features, that do not allow to discriminate
whether a sample belongs to the source or target domain, while still permitting accurate labeling in
the source domain. This approach relies on the assumption that such features allow efficient labeling
in the target domain. Ghifary et al. (2016) build a two-headed network sharing common layers; one
head performs classification in the source domain, while the second is a decoder that performs
reconstruction for target domain elements. Ganin et al. (2016) propose the DANN method and
introduce Gradient Reversal Layers to connect a domain discriminator and a feature extractor. These
layers invert the gradient sign during back-propagation so that the feature extractor is trained to fool
the domain discriminator. Shen et al. (2018) modify DANN and replace the domain discriminator
by a network that approximates the Wasserstein distance between domains. Tzeng et al. (2017)
optimize, in an adversarial setting, a generator and a discriminator with an inverted label loss.
Other methods focus on explicitly disentangling an information shared between domains (analogous
to the domain-invariant features above) from a domain-specific information. Inspired by Chen et al.
(2016), Liu et al. (2018b) isolate a latent factor, representing the domain information, from the
rest of an encoding, by maximizing the mutual information between generated images and this
latent factor. Some domain information may still be present in the remaining part of the encoding
and thus may not comply with conditions C3 and C4. Liu et al. (2018a) combine an encoder, an
image generator, a domain discriminator, and a fake images discriminator to produce cross-domain
images. The encoder is trained jointly with the domain discriminator to produce domain-invariant
representations. Li et al. (2020) disentangle a latent representation into a global code and a local
code. The global code captures category information via an encoder with a prior, and the local
code is transferable across domains, which captures the style-related information via an implicit
decoder. Bousmalis et al. (2016) also produce domain-invariant features by training a shared encoder
to fool a domain discriminator. They train two domain-private encoders with a difference loss that
encourages orthogonality between the shared and the private representations (similarly to condition
C3). Cao et al. (2018); Cai et al. (2019); Peng et al. (2019) combine a domain discriminator with an
adversarial classifier to separate the information shared between domains from the domain-specific
information. All these methods build a shared representation that prevents discriminating between
source and target domains, while retaining enough information to correctly label samples from the
source domain. However, because they rely on an adversarial classifier that requires labeled data,
they do not guarantee that the complimentary, domain-specific information for samples in the target
domain does not contain information that overlaps with the shared representation. In other words,
3
Under review as a conference paper at ICLR 2021
they only enforce C3 in the source domain. They rely on the assumption that the disentanglement
will still hold when applied on target domain elements, which might not be true.
Another identified weakness in methods that achieve a domain-invariant feature space is that their
representations might not allow for accurate labeling in the target domain. Indeed, feature alignment
does not necessarily imply a correct mapping between domains. To illustrate this point, consider a
binary classification problem (classes c1 and c2) and two domains (d1 and d2). Let (c1 , d1) denote
samples of class c1 in d1. It is possible to construct an encoding that projects (c1, d1) and (c2, d2)
to the same feature values. The same holds for (c1 , d2) and (c2, d1) for different feature values.
This encoding allows discriminating between classes in d1. It also fools a domain discriminator
since it does not allow predicting the original domain of a projected element. However, applying the
classification function learned on d1 to the projected d2 elements leads to catastrophic predictions.
Transforming a sample from one domain to the other, while retaining its label information can be
accomplished by image-to-image translation methods. Hoffman et al. (2018) extend CycleGAN’s
cycle consistency (Zhu et al., 2017) with a semantic consistency to translate from source to target
domains. The translated images from the source domain to the target domain are then used to train
a classifier on the target domain using the source labels. Similarly, Russo et al. (2018) train two
conditional GANs (Mirza & Osindero, 2014) to learn bi-directional image mappings constrained by
a class consistency loss and use a source domain classifier to produce pseudo-labels on source-like
transformed target samples. By relaxing CycleGAN’s cycle consistency constraint and integrating
the discriminator in the training phase, Hosseini-Asl et al. (2019) address the DA problem in the spe-
cific setting where the number of target samples is limited. Takahashi et al. (2020) use a CycleGAN
to generate cross-domain pseudo-pairs and train two domain-specific encoders to align features ex-
tracted from each pseudo-pair in the feature space. A major asset of the method is to address the
class-unbalanced UDA problem by oversampling with the learned data augmentation. Yang et al.
(2019) use separate encoders to produce domain-invariant and domain-specific features in both do-
mains. They jointly train these encoders with two generators to produce cross-domain elements able
to fool domain-specific discriminators. Using a cyclic loss on features, they force the information
contained in the representation to be preserved during the generation of cross-domain elements.
However, the cyclic loss on features does not prevent the information sharing between features ex-
pressed in C3. More importantly it does not prevent the domain-specific features to be constant.
A major drawback of these methods lies in the instability during training that might be caused by
min-max optimization problem induced by the adversarial training of generators and discriminators.
In the next section, we introduce a method that does not rely on a domain discriminator and an
adversarial label predictor, but directly minimizes the information sharing between representations.
This allows to guarantee that there is no information redundancy between the task-related and the
task-orthogonal style information in both the source and the target domains. Along the way, it pro-
vides an efficient mechanism to disentangle the task-related information from the style information
in the single domain case. Our method combines information disentanglement, intra-domain and
cross-domain cyclic consistency to enforce a more principled mapping between each domain.
4 Disentanglement with Gradient Reversal Layers and cyclic
RECONSTRUCTION
First, we propose an original method to disentangle the task-related information from the style in-
formation for a single domain in a supervised learning setting. In a second step, we propose an
adaptation of this method to learn these disentangled representations in both domains for UDA. This
disentanglement allows, in turn, to efficiently predict labels in the target domain.
4.1	Task-style disentanglement in the supervised case
Our approach consists in estimating jointly Π, Π and C as a deep feed-forward neural network. We
shall note θ∏, θ∏, and θc the parameters of the respective sub-parts of the network. Π ◦ Π takes the
form of an auto-encoder, while Π ◦ c is a task-related (classification or regression) network. Figure
1a summarizes the global architecture which we detail in the following paragraphs.
4
Under review as a conference paper at ICLR 2021
x
(b) Unsupervised domain adaptation
(a) Supervised learning
Figure 1: Network architectures
Conditions C1 and C2 are expressed through the definition of a task-specific loss Ltask (e.g. cross-
entropy for classification, L2 loss for regression) and a reconstruction loss Lreco . Therefore, the
update of θ∏ should follow -Vθ∏ (Ltask + Lreco), the update of θ∏ relies on -Vθ∏ Lrec°, and that
of θc uses -VecLtask.
In order to achieve condition C3, we exploit Gradient Reversal Layers (Ganin et al., 2016, GRL).
We train two side networks rτ : S → T and rσ : T → S whose purpose is to attempt to predict
T given S, and S given T respectively. For a given x, let us write (τ, σ) = Π(x), τb = rτ (σ), and
σb = rσ(τ). We train rτ and rσ to minimize the losses Lrτ = kτ - τbk2 and Lrσ = kσ - σbk2. Let
Linfo = Lrτ + Lrσ denote the combination of these losses. We connect these two sub-networks to
the whole architecture using GRLs. GRLs behave as the identity function during the forward pass
and invert the gradient sign during the backward pass, hence pushing the parameters to maximize
the output loss. During training, this architecture constrains Π to produce features in T and S with
the least information shared between them. Consequently, the update of θΠ follows +VθΠ Linfo.
This constraint efficiently avoids information redundancy between T and S . However, it does not
avoid all the information being pushed into T. Preventing this undesirable behavior is the purpose
of condition C4. To that end, we use a cyclic reconstruction scheme. Consider two elements x and
x0 from X, and their associated (τ,σ) = Π(x) and (τ0,σ0) = Π(x0). Let X = Π(τ,σ0) be the
reconstruction of τ that uses the style σ0 of x0 . A correct allotment of the information between T
and S requires that the task and style information be preserved in (τ, σ) = Π(X). So, We wish to
have τ as close as possible to τ, or, alternatively, to have c(T) as close as possible to c(τ). Similarly,
we wish to have σ as close as possible to σ0. To enforce C4 and avoid the degenerate case where the
encoder predicts a constant style for all samples, we force σ to lie sufficiently far from σ to avoid
style confusion. We achieve this with a triplet loss (Schroff et al., 2015) using σ as the anchor, σ0 and
σ as, respectively, the positive and negative inputs, and a margin m. Thus C4 results in minimizing
the cyclic reconstruction loss LcycIic = ∣∣τ 一 T|卜 + max{∣∣σ — σ0∣∣2 — ∣∣σ 一 σ∣b + m, 0}.
Overall, the gradient-based update procedure of the network parameters boils down to:
θΠ J θΠ 一 αvθ∏ (Ltask + LTeco
一 Linf o + Lcyclic) ,
θ∏ 一 θ∏ 一 avθ∏ ILTeco + Lcyclic),
θrτ J θrτ 一 αVθrτ Lrτ ,
θc J θc 一 αVθc Ltask,
θrσ J θrσ 一 αVθrσ Lrσ.
We call this method DiCyR for Disentangled Cyclic Reconstruction.
4.2 Task-style disentanglement in the unsupervised domain adaptation case
We propose a variation of DiCyR for UDA, where we replace the decoder Π by two domain-specific
decoders, Πs and Πt. We shall compensate for the lack of labeled data in the target domain by
computing cross-domain cyclic reconstructions.
Let (xs, ys) be a sample from the source domain and xt be a sample from the target domain. Let us
denote (τs, σs) = Π(xs) and (τt, σt) = Π(xt), the corresponding projections in the latent task and
style-related information spaces. Then one can define, as in the previous section, Ltask as the task-
specific loss on the source domain, and Lrecos and Lrecot as the reconstruction losses in the source
5
Under review as a conference paper at ICLR 2021
Figure 2: Swapping styles on SVHN and 3D Shapes
and target domains respectively. As previously, we constrain the task-related representation and the
style representation not to share information using two networks rτ and rσ , connected to the main
architecture by GRL layers (Figure 1b), allowing the definition of the Lrτ , Lrσ and Linf o losses.
Lastly, we exploit cyclic reconstructions in both domains to correctly disentangle the information
and hence define the same Lcyclic loss as above.
This disentanglement in the target domain separates the global information in two but does not
guarantee that what is being pushed into τ is really the task-related information. This can only be
enforced by cross-domain knowledge (since no correct labels are available in the target domain).
Thus, finally, we would like to allow projections from one domain into the other while retaining the
task-related information, hence allowing domain adaption. Using the notations above, we construct
xts = Πt(τs, σt), the reconstruction of xs’s task-related information, in the style of xt. This creates
an artificial sample in the target domain, whose label is ys. Then, with (τts , σts) = Π(xts), one
wishes to have τts match closely τs (or, alternatively, c(τts) match closely ys) in order to prevent the
loss of task information during the cross-domain projection and thus to constrain the task representa-
tions to be domain-invariant. Symmetrically, one can construct the artificial sample Xst = ΠΠS (Tt, σs)
and enforce that τst closely matches τt . Note that the label of xst is unknown and yet it is still pos-
sible to enforce the disentanglement by cyclic reconstruction.
Overall, these terms boil down to a cross-domain cyclic reconstruction loss for UDA
_cyclic = ∣∣τs - Tts k2 + ∣∣τt -
τst k2.
Ldomain
Finally, the network parameters are updated according to:
θΠ
J θ∏ - αVθπ (Ltask + Lrecos
+ ,Lr
ecot
-Linfo + Lcyclic + Ldomain-CyCIiC)
θΠS J θ∏s - aVΘπS(LreCos + Lcyclic), θ∏t J θΠt - avθπt (Lrecot + Lcyclic),
θc J θc - αVθc Ltask,
θrτ	Jθrτ	-	αVθrτ Lrτ,	θrσ	Jθrσ	-	αVθrσ Lrσ.
5 Experimental results and discussion
We first evaluate DiCyR’s ability to disentangle the task-related information from the style informa-
tion in the supervised context. Then we demonstrate DiCyR’s efficiency on UDA.1.
5.1	Supervised disentanglement
We evaluate the disentanglement performance of DiCyR by following the protocol introduced by
Mathieu et al. (2016). Since we do not use generative models, we only focus on their two first items:
swapping and retrieval. We evaluate DiCyR on the SVHN (Netzer et al., 2011), and 3D Shapes
(Burgess & Kim, 2018) disentanglement benchmarks. The task is predicting the central digit in the
image for the SVHN dataset, and the shape of the object in the scene for the 3D Shapes dataset.
Swapping involves swapping styles between samples and visually assessing the realism of the gen-
erated image. It combines the task-related information Ti of a sample xi with the style σj of another
sample Xj. We use the decoder to produce an output Xij. Figure 2 shows randomly generated outputs
on the two datasets. DiCyR produces visually realistic artificial images with the desired styles.
1Code and pre-trained networks available at https://github.com/AnonymousDiCyR/DiCyR.
6
Under review as a conference paper at ICLR 2021
Method	SVHN	3D Shape	floor hue	wall hue	object hue	scale	orientation
Full features	:0.98	1	0.94	0.94	0.89	0.6	0.5	[
Task-related features	0.98	1	0.11	0.12	0.13	0.15	0≡
Style features only	0.17	-^026	0.89	0.95	0.88	0.59	0.42
Random guess	0.10	0.25	0.10	0.10	0.10	0.125	0.067 j
Table 1: Accuracies of a classifier trained to predict factors of style variation on 3D shapes
Retrieval concerns finding, in the dataset, the nearest neighbors in the embedding space for an image
query. We carry out this search for nearest neighbors using the Euclidean distance on both the
task-related and the style representations. A good indicator of the effectiveness of the information
disentanglement would be to observe neighbors with the same labels as the query when computing
distances on the task-related information space, and neighbors with similar style when using the style
information. Figure 3 demonstrate that the neighbors found when using the task-related information
are samples with the same label as the query’s label and that the neighbors found using the style
representation share many characteristics with the query but not necessarily the same labels.
We ran a quantitative evaluation of disentanglement by training a neural network classifier with a
single hidden layer of 32 units to predict labels, using either the task-related information alone,
or the style information alone. If the information is correctly separated, we expect the classifier
trained with task-related information only to get similar performance to a classifier trained with full
information. Conversely, the classifier trained with the style information only should reach similar
performance to a random guess (10% accuracy on SVHN, 25% on 3D Shapes). Table 1 reports the
obtained testing accuracies. It appears that the task-related representation contains enough informa-
tion to correctly predict labels. We also observe that full disentanglement is closely but not perfectly
achieved as the classifier trained only with style information behaves slightly better that random
choice. To quantify how much style information is being unduly encoded in the task-related repre-
sentation, we ran a similar experiment to predict the five other style variation factors in 3D Shapes
(floor hue, wall hue, object hue, scale and orientation). The trained classifier reaches accuracies that
are very close to a random guess, thus validating the disentanglement quality.
5.2	Unsupervised domain adaptation problem
We evaluate DiCyR by performing domain adaptation between the MNIST (LeCun et al., 1998),
SVHN, and USPS (Hull, 1994) datasets, and between the Syn-Signs (Ganin & Lempitsky, 2015)
and the GTSRB (Stallkamp et al., 2011) datasets. Following common practice in the literature, we
trained our network on five different settings: MNIST→USPS, USPS→MNIST, SVHN→MNIST,
MNIST→SVHN, and Syn-Signs→GTSRB. We measure the classification performance in the target
domain and compare it with state-of-the-art methods (Table 2). We also compare with a base-
line classifier that is only trained on the source domain data. Values reported in Table 2 are
quoted from their original papers2. Our method, without extensive hyperparameter tuning, appears
to be on par with the best state-of-the-art methods. DiCyR also outperforms others disentangle-
2Comparisons might be inexact due to reproducibility concerns (Pineau et al., 2020) and these figures mostly
indicate which are the top competing methods.
7
Under review as a conference paper at ICLR 2021
ment and image-to-image methods. Specifically, DiCyR is only slightly outmatched by DWT and
SEDA on the MNIST-USPS and by SEDA and SHOT in the SVHN—MNIST benchmarks. The
MNIST→SVHN case is a particularly challenging benchmark since MNIST images are greyscale
and the adaptation to SVHN requires adapting to color images. SEDA makes extensive use of data
augmentation to tackle this challenge and is thus the only method displaying convincing results. This
hints to a possible enhancement of DiCyR in order to improve its performance. Finally, by intro-
ducing a new variation on batch normalization, DWT’s contribution is orthogonal to ours and both
could be combined. We emphasize that beyond these enhancements, a major advantage of DiCyR
lies in the ability to disentangle the information in the target domain without direct supervision.
DiCyR uses GRLs to ensure that no information is shared between T and S. Similarly to most
methods in Table 2, GRLs induce an adversarial optimization problem which is known to yield
instability and variance in the resolution performance. In our case, this induces several distinct
modes in the distribution of accuracies. It is interesting to note that the majority mode (that of
the median) on SVHN→MNIST matches the best known performance. For this reason, we report
both the mean and the median on this specific experiment. One might object that condition C3 was
expressed in terms of mutual information. Thus, DiCyR only indirectly implements this condition
using GRLs. An alternative could be to use an estimator of the mutual information, such as proposed
by Belghazi et al. (2018), to directly minimize it (and thus avoid the adversarial setting altogether).
Such an approach was explored in the work of Sanchez et al. (2019) to disentangle representations
between pairs of images, and would constitute a promising perspective of research for DA.
Source Method	Target	MNIST USPS	USPS MNIST	SVHN MNIST	MNIST SVHN	Syn-Signs GTSRB
Baseline	78.1	58.0	60.2	20.0	79.0
DANN(Ganinetal.,2016)	ɪi	73.0	73.9	35.7	88.6	∙
ADDA(TzengetaL,2017)	ɪl	90.1	76.0	-	-
DSN (Bousmalis et al., 2016)	-9T3	-	82.7	-	93.1	∙
DRCN(GhifaryetaL,2016)	ɪg	73.7	82.0	40.1	-
DiDA(CaoetaL,2018)	ɪs	-	83.6	-	-
SBADA-GAN (Russo et al., 2018)	-976	95.0	76.1	61.1	96.7	∙
CyCADA(HofmanetaL,2018)-	-956	96.5	90.4	-	-
DWT(RoyetaL,2019)	ɪi	98.8	97.7	28.9	-
SEDA(FrenchetaL,2018)	-982	99.5	99.3	97.0	99.3
ACAL(HoSSeini-ASletaL,2019)	-983	97.2	96.5	60.8	-
SHOT(LiangetaL,2020)	""98^	98.0	98.9	-	-
DiCyR(OUrS)	98.4	98.3	98.51	23.8	97.4	∙
1 median accuracy reported (average accuracy: 95.7 full results distribution are reported in Appendix F).
Table 2: Target domain accuracy, reported as percentages
As in Section 5.1, we evaluate qualitatively the effectiveness of disentanglement, especially in the
target domain, and produce visualizations of cross-domain style and task swapping. Here, we com-
bine one domain’s task information with the other domain’s styles to reconstruct the images of
Figures 4a, 4b, 4d, and 4e. The most important finding is that the style information was correctly
disentangled from the task-related information in the target domain without the use of any label.
Specifically, the rows in these figures show that the class information is preserved when a new style
is applied, while the columns illustrate the efficient style transfer allowed by disentanglement.
A desirable property of the task-related encoding is its domain invariance. To evaluate this aspect,
we built a t-SNE representation (Hinton & Roweis, 2003) of the task-related features, in order to
verify their alignment between domains. Figures 4c and 4f demonstrate this property.
The previous experiments illustrated the use of DiCyR in the context of image classification. The
method is, however, quite generic and can be applied in many more contexts. Figure 5 reports the
improvement due to applying DiCyR for domain adaptation between the GTA5 (Richter et al., 2016)
and the Cityscapes (Cordts et al., 2016) segmentation problems (detailed results in Appendix G).
8
Under review as a conference paper at ICLR 2021
(b) MNIST target class with (c) t-SNE on task-related features,
SVHN source style SVHN (blue) → MNIST (red)
(a) SVHN source class with
MNIST target style
(d) Syn-Signs source class
with GTSRB target style
(e) GTSRB target class (f) t-SNE on task-related features,
with Syn-Signs source style Syn-Signs (blue) → GTSRB (red)
Figure 4: Cross-domain swapping and feature alignment visualization
(a) Test image	(b) Source prediction	(c) DiCyR	(d) Ground truth
Figure 5: GTA5 to Cityscapes segmentation
Finally, directly computing the distances on the task-related features in LdOmainCyCliC often leads
to unstable results. As hinted in Section 4, using instead a task oriented loss LdOmainCyCliC =
kc(τs) - yk2+ kc(τt) - c(τst)k2 stabilizes training and improves the target domain accuracy. Train-
ing c with cross-domain projections from the source domain and the corresponding labels improves
its generalization to the target domain and forces the encoder to produce task-related features com-
mon to both domains. To illustrate this property, consider the following example. In one domain,
the digit “7” is written with a middle bar, while in the other it has none. This domain-specific middle
bar feature should not be expressed in T; it should be considered as a task-orthogonal style feature.
Thus using c’s predictions within the domain cyclic loss, instead of distances in T, prevents the
encoder from representing the domain-specific features in T and encourages their embedding in S .
6 Conclusion
In this work, we introduced a new disentanglement method, called DiCyR, to separate task-related
and task-orthogonal style information into different representations in the context of unsupervised
domain adaptation. This method also provides a simple and efficient way to obtain disentangled
representations for supervised learning problems. Its main features are its overall simplicity, the use
of intra-domain and cross-domain cyclic reconstruction, and information separation through Gradi-
ent Reversal Layers. The design of this method stems from a formal definition of disentanglement
for domain adaptation which, to the best of our knowledge, is new. The empirical evaluation shows
that DiCyR performs as well as state-of-the-art methods, while offering the additional benefit of
disentanglement, including in the target domains where no label information is available.
9
Under review as a conference paper at ICLR 2021
References
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. Mutual information neural estimation. In 35th International Con-
ference on Machine Learning,pp. 531-540, 2018.
Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan.
Domain separation networks. In Advances in neural information processing systems, pp. 343-351,
2016.
Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018.
Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, and Zhifeng Hao. Learning disentangled
semantic representation for domain adaptation. In International Joint Conference on Artificial
Intelligence, pp. 2060, 2019.
Jinming Cao, Oren Katzir, Peng Jiang, Dani Lischinski, Danny Cohen-Or, Changhe Tu, and Yangyan
Li. Dida: Disentangled synthesis for domain adaptation. arXiv, pp. arXiv-1805, 2018.
Fabio Maria Cariucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulo. Autodial:
Automatic domain alignment layers. In 2017 IEEE International Conference on Computer Vision
(ICCV), pp. 5077-5085. IEEE, 2017.
Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. InfoGAN:
Interpretable representation learning by information maximizing generative adversarial nets. In
Advances in neural information processing systems, pp. 2172-2180, 2016.
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo
Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic
urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), 2016.
Geoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation.
In International Conference on Learning Representations, 2018.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International conference on machine learning, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Muhammad Ghifary, W Bastiaan Kleijn, Mengjie Zhang, David Balduzzi, and Wen Li. Deep
reconstruction-classification networks for unsupervised domain adaptation. In European Con-
ference on Computer Vision, pp. 597-613, 2016.
Abel Gonzalez-Garcia, Joost Van De Weijer, and Yoshua Bengio. Image-to-image translation for
cross-domain disentanglement. In Advances in neural information processing systems, pp. 1287-
1298, 2018.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. Beta-VAE: Learning basic visual concepts with a
constrained variational framework. In 5th International Conference on Learning Representations,
2017.
Geoffrey E Hinton and Sam T Roweis. Stochastic neighbor embedding. In Advances in neural
information processing systems, pp. 857-864, 2003.
Judy Hoffman, Dequan Wang, Fisher Yu, and Trevor Darrell. Fcns in the wild: Pixel-level adversar-
ial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International
Conference on Machine Learning, pp. 1989-1998, 2018.
10
Under review as a conference paper at ICLR 2021
Ehsan Hosseini-Asl, Yingbo Zhou, Caiming Xiong, and Richard Socher. Augmented cyclic ad-
versarial learning for low resource domain adaptation. In International Conference on Learning
Representations, 2019.
Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on
Pattern Analysis and Machine Intelligence,16(5):550-554, 1994.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In International Conference on Machine Learning, pp. 448-456,
2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International
Conference on Learning Representations, 2015.
Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. In 5th Interna-
tional Conference on Learning Representations, 2017.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Haoliang Li, Renjie Wan, Shiqi Wang, and Alex C Kot. Unsupervised domain adaptation in the wild
via disentangling representation learning. International Journal of Computer Vision, pp. 1-17,
2020.
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou. Revisiting batch normaliza-
tion for practical domain adaptation. In 5th International Conference on Learning Representa-
tions, 2017.
Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source
hypothesis transfer for unsupervised domain adaptation. arXiv preprint arXiv:2002.08546, 2020.
Alexander H Liu, Yen-Cheng Liu, Yu-Ying Yeh, and Yu-Chiang Frank Wang. A unified feature
disentangler for multi-domain image translation and manipulation. In Advances in neural infor-
mation processing systems, pp. 2590-2599, 2018a.
Yen-Cheng Liu, Yu-Ying Yeh, Tzu-Chien Fu, Sheng-De Wang, Wei-Chen Chiu, and Yu-Chiang
Frank Wang. Detach and adapt: Learning cross-domain disentangled deep representation. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8867-
8876, 2018b.
Michael F Mathieu, Junbo Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo Sprechmann, and Yann
LeCun. Disentangling factors of variation in deep representation using adversarial training. In
Advances in neural information processing systems, pp. 5040-5048, 2016.
Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint
arXiv:1411.1784, 2014.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading
digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning
and Unsupervised Feature Learning 2011, 2011.
Xingchao Peng, Zijun Huang, Ximeng Sun, and Kate Saenko. Domain agnostic learning with dis-
entangled representations. In International Conference on Machine Learning, pp. 5102-5112,
2019.
Joelle Pineau, PhiliPPe Vincent-Lamarre, Koustuv Sinha, Vincent Lariviere, Alina Beygelzimer,
Florence d'Alche Buc, Emily Fox, and Hugo Larochelle. Improving reproducibility in ma-
chine learning research (a report from the neurips 2019 reproducibility program). arXiv preprint
arXiv:2003.12206, 2020.
Stephan R. Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground
truth from computer games. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling (eds.),
European Conference on Computer Vision (ECCV), volume 9906 of LNCS, pp. 102-118. Springer
International Publishing, 2016.
11
Under review as a conference paper at ICLR 2021
Salah Rifai, Yoshua Bengio, Aaron Courville, Pascal Vincent, and Mehdi Mirza. Disentangling fac-
tors of variation for facial expression recognition. In European Conference on Computer Vision,
pp. 808-822, 2012.
EdUardo Romera, Jose M Alvarez, LUis M Bergasa, and Roberto Arroyo. Erfnet: Efficient resid-
ual factorized convnet for real-time semantic segmentation. IEEE Transactions on Intelligent
Transportation Systems, 19(1):263-272, 2017.
SUbhankar Roy, Aliaksandr Siarohin, Enver Sangineto, SamUel Rota BUlo, NicU Sebe, and Elisa
Ricci. UnsUpervised domain adaptation Using featUre-whitening and consensUs loss. In IEEE
Conference on Computer Vision and Pattern Recognition, pp. 9471-9480, 2019.
Paolo RUsso, Fabio M CarlUcci, Tatiana Tommasi, and Barbara CapUto. From soUrce to target and
back: symmetric bi-directional adaptive GAN. In IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8099-8108, 2018.
EdUardo HUgo Sanchez, MathieU SerrUrier, and Mathias Ortner. Learning disentangled representa-
tions via mUtUal information estimation. arXiv preprint arXiv:1912.03915, 2019.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A Unified embedding for face
recognition and clUstering. In IEEE conference on computer vision and pattern recognition, pp.
815-823, 2015.
Jian Shen, YanrU QU, Weinan Zhang, and Yong YU. Wasserstein distance gUided representation
learning for domain adaptation. In AAAI, 2018.
Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign
recognition benchmark: a mUlti-class classification competition. In The 2011 international joint
conference on neural networks, pp. 1453-1460. IEEE, 2011.
Baochen SUn, Jiashi Feng, and Kate Saenko. Correlation alignment for UnsUpervised domain adap-
tation. In Domain Adaptation in Computer Vision Applications, pp. 153-171. Springer, 2017.
RyUhei Takahashi, AtsUshi Hashimoto, MotoharU Sonogashira, and Masaaki Iiyama. Partially-
shared variational aUto-encoders for UnsUpervised domain adaptation with target shift. arXiv
preprint arXiv:2001.07895, 2020.
Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-
tency targets improve semi-sUpervised deep learning resUlts. In Advances in neural information
processing systems, pp. 1195-1204, 2017.
JoshUa B TenenbaUm and William T Freeman. Separating style and content with bilinear models.
Neural computation, 12(6):1247-1283, 2000.
Eric Tzeng, JUdy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation. In IEEE conference on computer vision and pattern recognition, pp. 7167-7176,
2017.
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-
gredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.
JUnlin Yang, Nicha C Dvornek, Fan Zhang, JUliUs Chapiro, MingDe Lin, and James S DUncan.
UnsUpervised domain adaptation via disentangled representations: Application to cross-modality
liver segmentation. In International Conference on Medical Image Computing and Computer-
Assisted Intervention, pp. 255-263. Springer, 2019.
JUn-Yan ZhU, TaesUng Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image transla-
tion Using cycle-consistent adversarial networks. In IEEE international conference on computer
vision, pp. 2223-2232, 2017.
12
Under review as a conference paper at ICLR 2021
A Cross-domain disentanglement visualizations
We report extra cross-domain visualizations similar to those of Section 5.2 in Figures 6 and 7.
(a) Combination of source class
with target style
(b) Combination of target class
with source style
Figure 6: Cross-domain swapping, USPS→MNIST

ħhhħξ
hhhħħ
⑻【，】【,】㈤同
MHMMK
(b) Combination of target class
with source style
(a) Combination of source class
with target style
Figure 7: Cross-domain swapping, MNIST→USPS
B Network architecture and training hyper-parameters
The paragraphs below detail the network architctures used in the experiments of Section 5. It should
be noted that neither these architectures, nor the associated hyper-parameters have been extensively
and finely tuned to their respective tasks, as the goal of this contribution was to provide a generic,
robust method. Thus it is likely that performance gains can still be obtained on this front.
B.1	Single domain supervised disentanglement experiments
This section describes the network architecture and the hyper-parameters used in the experiments
of Section 5.1. The encoder Π is composed of shared layers (layers 1 to 6 in the table below),
followed by the specific task-related and style encodings. Those final layers are denoted Πτ and Πσ
in the tables below. For the sake of implementation simplicity, we chose to project samples from
the source domain and samples from the target domain into two separate style embeddings (one for
each domain). Thus Πσ is actually duplicated in two heads Πσ,s and Πσ,t with the same structure
and output space. We used the exact same network architectures for both the 3D shapes and SVHN
datasets, the only difference being the dimension of the embeddings T and S. In all experiments,
we applied a coefficient βreco = 5 to Lreco and βcyclic = 0.1 to Lcyclic in the global loss. We also
use a βinfo on Linfo; this coefficient increases linearly from 10-2 to 10 over the first 10 epochs
and remains at 10 afterwards (see Section C for a discussion on this coefficient). Convergence was
reached within 50 epochs. We used Adam (Kingma & Ba, 2015) as an optimizer with a learning rate
Ir = 5 • 10-4 for the first 30 epochs and lr = 5 ∙ 10-5 for the last 20 epochs. The following tables
summarize the architectures of all sub-networks.
13
Under review as a conference paper at ICLR 2021
Architecture of Π, single domain case		
Layer	Type	一	ParameterS
'~L	Conv2D 一	filters=32, kernel=5 X 5, stride=1, padding=2, activation=ReLU
	Max Pooling	filters=2 × 2, stride=2
	Conv2D	filters=64 for 3DShapes or 32 for SVHN kernel=5 × 5, stride=1, padding=2, activation=ReLU
	Max Pooling	filters=2 × 2, stride=2
	Conv2D	filters=128 for 3DShapes or 64 for SVHN kernel=3 × 3, stride=1, padding=1, activation=ReLU
^6	Dense	nb_neurons=1024, activation=ReLU
	Dense	nb_neurons=20 for 3D shapes or 150 for SVHN, activation=ReLU
∏σ	Dense	nb_neurons=20 for 3D shapes or 150 for SVHN, activation=ReLU
Architecture of ∏, single domain case		
Layer	TyPe	ParameterS
'~L	Dense	nb_neurons=1024, activation=ReLU
F	Dense	nb_neurons=64, activation=ReLU
	Conv2D	filters=64, kernel=3 × 3, stride=1, padding=1, activation=ReLU
	Upsample	scale_factor=2
'ɜ	Conv2D	filters=32, kernel=5 × 5, stride=1, padding=2, activation=ReLU
^6	Upsample	scale_factor=2
. 7	Conv2D	filters=3, kernel=5 × 5, stride=1, padding=2, activation=Sigmoid
ArChiteCtUre of c, Single domain CaSe		
Layer	TyPe	Parameters	∣^
1	Dropout	p=0.2 for 3DShapes and 0.55 for SVHN-[
一 2	Dense	nb_neurons=nb」abels, activation=Softmax
ArChiteCtUre of r and r, 3DShapes Single domain CaSe		
Layer	TyPe	ParameterS
1	Gradient Reversal Layer	
F	Dense	nb_neurons=100, activation=ReLU
. 3	Dense	nb_neurons=20, activation=Linear
ArChiteCtUre of r and r, SVHN Single domain CaSe		
Layer	Type	ParameterS
1	Gradient Reversal Layer	
F	Dense	nb_neurons=100, activation=ReLU
	Dense	nb_neurons=100, activation=ReLU
4	Dense	nb_neurons=150, activation=Linear
B.2	Unsupervised domain adaptation experiments
This section describes the network architecture and the hyper-parameters used in the experiments
of Section 5.2. The encoder Π is composed of shared layers (layers 1 to 9 in the tables below),
followed by the specific task-related and style encodings. Those final layers are denoted Πτ and Πσ
in the tables below. For the sake of implementation simplicity, we chose to project samples from
the source domain and samples from the target domain into two separate style embeddings (one for
each domain). Thus Πσ is actually duplicated in two heads Πσ,s and Πσ,t with the same structure
and output space. In all experiments, in the global loss, we applied a coefficient βcyclic = 0.1 to
Lcyclic and β domain _cyclic to L domain .cyclic, with β domain _cyclic increasing linearly from 0 to 10
during the 10 first epochs and remaining at 10 afterwards (see Section C for a discussion on this
coefficient). Convergence was reached within 50 epochs (generally within 30 epochs). We used
Adam (Kingma & Ba, 2015) as an optimizer with a learning rate lr = 5 ∙ 10-4 for the first 30 epochs
and lr = 5 ∙ 10-5 for the last 20 epochs. The following tables summarize the architectures of all
sub-networks.
14
Under review as a conference paper at ICLR 2021
ArchitectUre of Π, SVHN什MNIST Case		
Layer	Type	~	Parameters
'~L	Conv2D	filters=32, kernel=5 X 5, stride=1, Padding=2, activation=Linear Instance Normalization
	Max Pooling	filters=2 X 2, stride=2
	Conv2D	filters=32 for SVHN→MNIST 64 for MNIST→SVHN kernel=5 X 5, stride=1, padding=2, activation=Linear Instance Normalization
	Instance Normalization	
	Max Pooling	filters=2 x 2, stride=2
'~6	Conv2D	filters=32 for SVHN→MNIST 128 for MNIST→SVHN kernel=3 X 3, stride=1, padding=1, activation=Linear Instance Normalization
	Dense	nb_neurons=1024, activation=ReLU
	Dense	nb_neurons=75 for SVHN→MNIST activation=ReLU nb_neurons=200 for MNIST→SVHN, activation=ReLU
	Dense	nb_neurons=75 for SVHN→MNIST activation=ReLU nb_neurons=200 for MNIST→SVHN, activation=ReLU	
ArChiteCtUre of ΠS and ∏t, SVHN仆MNIST Case		
Layer	TyPe	Parameters
'~L	Dense	nb_neurons=1024, activation=ReLU
	Dense	nb_neurons=64, activation=ReLU
	Conv2D-	filters=32 for SVHN→MNIST 64 for MNIST→SVHN kernel=3 X 3, stride=1, padding=1, activation=ReLU
	Upsample	scale_factor=2
	Conv2D	filters=32, kernel=5 X 5, stride=1, padding=2, activation=ReLU
'~6	Upsample	scale_factor=2
. 7	Conv2D	filters=3, kernel=5 X 5, stride=1, padding=2, activation=Sigmoid
ArChiteCtUre of c, SVHN什MNIST Case		
Layer	TyPe	Parameters
1	Dropout	p=0.55
. 2	Dense	nb_neurons=10, activation=Softmax
ArChiteCtUre of rr and %, SVHN什MNIST Case		
Layer	TyPe	Parameters
1	Gradient Reversal Layer	
F	Dense	nb_neurons=100, activation=ReLU
	Dense	nb_neurons=75 for SVHN→MNIST activation=Linear nb_neurons=200 for MNIST→SVHN, activation=Linear
Architecture of Π, MNIST^USPS case		
Layer	Type	~	Parameters	-[
'~L	Conv2D	filters=50, kernel=5 X 5, stride=1, padding=2, activation=ReLU, Batch Norm.
	Max Pooling	filters=2 X 2, stride=2
	Conv2D 一	filters=75, kernel=5 X 5, stride=1, padding=2, activation=ReLU, Batch Norm.
	Max Pooling	filters=2 X 2, stride=2
	Conv2D 一	filters=100, kernel=3 X 3, stride=1, padding=1, activation=ReLU, Batch Norm.
'~6	Dense	nb_neurons=1024, activation=ReLU
	Dense	nb_neurons=150, activation=ReLU
^σ	Dense	nb_neurons=150, activation=ReLU
15
Under review as a conference paper at ICLR 2021
ArChiteCtUre of ΠS and ∏t, MNIST仆USPS Case		
Layer	TyPe	Parameters
'~L	Dense	nb_neurons=1024, activation=ReLU
	Dense	nb_neurons=64, activation=ReLU
	Conv2D	filters=100, kernel=3 X 3, stride=1, Padding=1, activation=ReLU
	Upsample	scalefactor=2
	Conv2D	filters=50, kernel=5 × 5, stride=1, Padding=2, activation=ReLU
'~6	Upsample	scalefactor=2
一 7	Conv2D	filters=1, kernel=5 × 5, stride=1, padding=2, activation=Sigmoid
Architecture of c, MNIST^USPS case		
Layer	TyPe	Parameters
Layer	TyPe	Parameters	∣^
1	Dropout	p=0.55
一 2	Dense	nb_neurons=10, activation=Softmax
ArChiteCtUre of r「and r%, MNIST什USPS Case		
Layer	Type	Parameters
1	Gradient Reversal Layer	
F	Dense	nb_neurons=100, activation=ReLU
. 3	Dense	nb_neurons=150, activation=Linear
Architecture of Π, Syn-Signs^GTSRB case		
Layer	TyPe	一	Parameters	-[
'~L	Conv2D	filters=32, kernel=5 × 5, stride=1, padding=2, activation=ReLU, Batch Norm.
	Max Pooling	filters=2 X 2, stride=2
	Conv2D 一	filters=64, kernel=5 X 5, stride=1, padding=2, activation=ReLU, Batch Norm.
	Max Pooling	filters=2 x 2, stride=2
	Conv2D	filters=128, kernel=3 X 3, stride=1, padding=1, activation=ReLU, Batch Norm.
'~6	Max Pooling	filters=2 X 2, stride=2
	Conv2D 一	filters=128, kernel=3 X 3, stride=1, padding=1, activation=ReLU, Batch Norm.
-8	Dense	nb_neurons=1024, activation=ReLU
	Dense	nb_neurons=75, activation=ReLU
∏σ	Dense	nb_neurons=75, activation=ReLU
	ArChiteCtUre of ΠS and Πt, Syn-Signs什GTSRB Case			
Layer	TyPe	Parameters
'~L	Conv2D	filters=64, kernel=4 X 4, stride=1, padding=1, activation=ReLU
	Upsample	scale_factor=2
	Conv2D	filters=32, kernel=3 X 3, stride=1, padding=2, activation=ReLU
	Upsample	scale_factor=2
'ɜ	Conv2D	filters=32, kernel=3 X 3, stride=1, padding=2, activation=ReLU
'~6	Upsample	scale_factor=2
. 7	Conv2D	filters=1, kernel=3 X 3, stride=1, padding=2, activation=Sigmoid
ArChiteCtUre of C Syn-Signs什GTSRB		
Layer	TyPe	Parameters
1	Dropout	p=0.55
. 2	Dense	nb_neurons=10, activation=Softmax
ArChiteCtUre of IrT and r%, Syn-Signs什GTSRB Case		
Layer	Type	Parameters
1	Gradient Reversal Layer	
F	Dense	nb_neurons=100, activation=ReLU
. 3	Dense	nb_neurons=150, activation=Linear
16
Under review as a conference paper at ICLR 2021
For the GTA5 → Cityscapes experiment, we based our network’s architecture on the one proposed
by (Romera et al., 2017).
Architecture of Π GTA5 → Cityscapes case	
Layer	TyPe
'~L	Downsampler block, filters=16
	Downsampler block, filters=64
	Non-bt-1D.
-8	Downsampler block, filters=128
'~9	Non-bt-1D (dilated 2)
∏0	Non-bt-1D (dilated 2)
'~n	Non-bt-1D (dilated 2)
∏2	Non-bt-1D (dilated 2)
T3	Non-bt-1D (dilated 2)
'~L4	Non-bt-1D (dilated 2)
	Non-bt-1D (dilated 2)
'~L6 ∏τ	Non-bt-1D (dilated 2) Conv2d filter=114, kernel=1, activation=ReLU
∏σ	Conv2d filter=14, kernel=1, activation=ReLU
ArChiteCtUre of c, ΠS and Πt GTA5 → CitySCaPeS CaSe	
Layer	TyPe
'~L	Conv2D filters=64 kernel=3 X 3, stride=2, padding=1, activation=ReLU
	Upsample scalefactor=2
^-4-	non-bottleneck-1D
'ɜ	Conv2D filters=16 kernel=3 × 3, stride=2, padding=1, activation=ReLU
'~6	Upsample scalefactor=2
■^-8-	non-bottleneck-1D
'~9	Upsample scalefactor=2
10	Conv2D filters=20 for C and filters=3 for ΠS and Πt kernel=3 × 3, stride=2, padding=1, activation=ReLU
C Discussion on THE βinfo and βdomain上yciic coefficients schedule
Although the schedule on βiηfο (single domain case) and Bdomaincyciic (domain adaptation case)
is not absolutely necessary, we found out it helped the overall convergence. These coefficients
gradually increase the weight of the information disentanglement objective and the cross-domain
reconstruction objective. This assigns more importance to learning a good predictor c ◦ Π during
early stages. From this perspective, gradually increasing βinf o can be seen as gradually removing
task-useless information from T and transferring it to S. Similarly, increasing Bdomainqyciic cor-
responds to letting the network discover disentangled representations before aligning them across
domains.
As previously mentioned, our goal in this study was to provide a robust disentanglement method
that permits domain adaptation. Therefore, no complete hyper-parameter study and tuning was
performed and these findings are thus reported as such and might be incomplete. Refining the
understanding of the influence of the different B coefficients is closer to the problem of meta-learning
and is beyond the scope of this paper.
D Influence of batch normalization and dropout on DiCyR
Batch normalization (ioffe & szegedy, 2015) is an efficient way to reduce the discrepancy between
the source and target distributions statistics. We noticed that, for the specific sVHN → MNisT
setting, using instance normalization (ulyanov et al., 2016) slightly improves the target domain
accuracy. Normalizing across chanels, the instance normalization layers helps the networks to be
agnostic to the image contrast which is particularly strong in MNisT. We also noticed that using
a large dropout in the sub-network c, and small embedding dimensions for Π's ouputs improves
17
Under review as a conference paper at ICLR 2021
both the disentanglement quality and the target domain accuracy. We conjecture that the informa-
tion bottleneck induced forces the task-related representation to be as concise as possible and thus
encourages disentanglement.
E	Complimentary information related to the Machine Learning
Reproducibility Checklist
All the experiments from section 5 were run on a Google Cloud Platform n1-standard-8 virtual
machine (8 virtual cores, 30Go RAM, Nvidia P100 GPU) except the experiment on GTA5 →
Cityscapes for which two Nvidia P100 GPU were used. The code corresponding to the experi-
ments, a list of dependencies, and pre-trained models are available at https://github.com/
AnonymousDiCyR/DiCyR Details about each experiments are reported on Table 3.
Experiment	Image size	Batch size	Number of epochs	Time by epoch	Number of experiment repetitions
5.1 SVHN	一	32×32	64	50	35 s	50	1
5.13DShaPes	32×32	^64	^30	^Γ5s	
5.2MNIST→USPS	32×32	128	150	^ΓΓs	^0
5.2USPS→MNIST	32×32	128	150	^ΓΓs	^0
. 5.2MNIST→SVHN	32×32	128	^30	^40^s	^30
'5.2SVHN→MNIST	32×32	^64	^30	^40^s	^30
5.2Syn-Signs→GTSRB	64×64	^64	150	^65s	Tg
5.2 GTA5→Cityscapes	256 X 256 一	1001	200	128 s	3	—
1 To account for GPU memory limits, the gradients were accumulated over 5 batches of size 20.
Table 3: Experimental setup
F	Distribution of results
In Figure 8 we report the distribution of testing accuracies for the SVHN→MNIST benchmark of
Section 5.2. Most of the accuracies are distributed within three modes with low variance, centered
respectively around 89%, 94% and 98%, the latter being also the median and the majority mode.
This sheds a more detailed light on the results reported in Section 5.2. As discussed therein, we con-
jecture these modes stem from the local equilibria of the adversarial optimization problem induced
by the GRLs in DiCyR. Thus we expect that avoiding this problem altogether in the formulation of
DiCyR could improve these resulting distributions.
Figure 8: Distribution of testing accuracies across experiments for SVHN→MNIST
18
Under review as a conference paper at ICLR 2021
G DOMAIN ADAPTATION FOR THE GTA5→CITYSCAPES SEGMENTATION
TASK
Here we report on the application of DiCyR for the segmentation task in GTA5 (Richter et al., 2016)
and Cityscapes (Cordts et al., 2016). GTA5 is the source domain, where the ground truth of image
segments is provided, and the goal is to reach efficient segmentation on the Cityscapes dataset.
Figure 5 provided a first visual illustration of the benefits provided by DiCyR. Figure 9 provides
yet other such examples. Column 9a presents the testing image from the Cityscapes dataset, col-
umn 9b shows the application on the testing image of a classifier trained only on the GTA5 images,
column 9c shows the segmentation obtained by DiCyR, which can be compared to the ground truth
(column 9d).
Figure 9: GTA5 to Cityscapes segmentation
Table 4 reports the Intersection over Union criterion (the Jaccard index) for each object class in the
Cityscapes images. We compare the seminal approach of Hoffman et al. (2016) coined “FCNs in
the wild”, which served as a baseline for CyCADA (Hoffman et al., 2018), and the application of
DiCyR. These results are preliminary3 and did not benefit from any hyperparameter or architecture
tuning. The purpose of this table is to report the out-of-the-box performance of DiCyR and validate
the rationale behind performing disentanglement, on a challenging, large scale problem.
3They were obtained following a very valuable suggestion by a reviewer.
19
Under review as a conference paper at ICLR 2021
°。IIU
elcyci
elcycroto
niar
su
kcur
ra
redi
nosre
yks
niarre
noitatege
ngis cfifar
thgil cfifar
elo
ecne
lla
gnidliu
klawedis
68.6	11.8	57.4	5.4	2.5
70.4	32.4	62.1	14.9	5.4
85.2	37.2	76.5	21.8	15.0
69.2	15.5	68.2	15.4	9.2
11.3 6.6 0.9 65.5 12.9 42.1 13.1 1.7 41.9 4.7 3.8 2.8 1.8 0.0	18.7
10.9 14.2 2.7 79.2 21.3 64.6 44.1 4.2 70.4 8.0 7.3 0.0 3.5 0.0	27.1
23.8 22.9 21.5 80.5 31.3 60.7 50.5 9.0 76.9 17.1 28.2 4.5 9.8 0.0	35.4
12 9.6 1.2 70.1 32.1 60.9 28.6 0.3 59.6 8.9 8.7 2.0 4.4 0.0	25.0
Source only
FCNs in the wild
CyCADA
DiCyR
Table 4: Intersection over Union (IoU) criterion for each object class
20