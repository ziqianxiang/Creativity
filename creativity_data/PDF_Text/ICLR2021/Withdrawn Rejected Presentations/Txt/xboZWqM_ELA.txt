Under review as a conference paper at ICLR 2021
Debiased Graph Neural Networks with
Agnostic Label Selection Bias
Anonymous authors
Paper under double-blind review
Ab stract
Most existing Graph Neural Networks (GNNs) are proposed without considering
the selection bias in data, i.e., the inconsistent distribution between the training
set with test set. In reality, the test data is not even available during the training
process, making selection bias agnostic. Training GNNs with biased selected nodes
leads to significant parameter estimation bias and greatly impacts the generalization
ability on test nodes. In this paper, we first present an experimental investigation,
which clearly shows that the selection bias drastically hinders the generalization
ability of GNNs, and theoretically prove that the selection bias will cause the biased
estimation on GNN parameters. Then to remove the bias in GNN estimation, we
propose a novel Debiased Graph Neural Networks (DGNN) with a differentiated
decorrelation regularizer. The differentiated decorrelation regularizer estimates a
sample weight for each labeled node such that the spurious correlation of learned
embeddings could be eliminated. We analyze the regularizer in causal view and it
motivates us to differentiate the weights of the variables based on their contribution
on the confounding bias. Then, these sample weights are used for reweighting
GNNs to eliminate the estimation bias, thus help to improve the stability of predic-
tion on unknown test nodes. Comprehensive experiments are conducted on several
challenging graph datasets with two kinds of label selection bias. The results
well verify that our proposed model outperforms the state-of-the-art methods and
DGNN is a flexible framework to enhance existing GNNs.
1	Introduction
Graph Neural Networks (GNNs) are powerful deep learning algorithms on graphs with various
applications (Scarselli et al., 2008; KiPf & Welling, 2016; VeliCkovic et al., 2017; Hamilton et al.,
2017). Existing GNNs mainly learn a node embedding through aggregating the features from its
neighbors, and such message-passing framework is supervised by node label in an end-to-end manner.
During this training procedure, GNNs will effectively learn the correlation between the structure
pattern and node feature with node label, so that GNNs are capable of learning the embeddings of
new nodes and inferring their labels.
One basic requirement of GNNs making precise prediction on unseen test nodes is that the distribution
of labeled training and test nodes is same, i.e., the structure and feature of labeled training and test
nodes follow the similar pattern, so that the learned correlation between the current graph and label
can be well generalized to the new nodes. However, in reality, there are two inevitable issues. (1)
Because it is difficult to control the graph collection in an unbiased environment, the relationship
between the collected real-world graph and the labeled nodes is inevitably biased. Training on such
graph will cause biased correlation with node label. Taking a scientist collaboration network as an
example, if most scientists with “machine learning” (ML) label collaborate with those with “computer
vision” (CV) label, existing GNNs may learn spurious correlation, i.e., scientists who cooperate
with CV scientist are ML scientists. If a new ML scientist only connects with ML scientists or the
scientists in other areas, it will be probably misclassified. (2) The test node in the real scenario is
usually not available, implying that the distribution of new nodes is agnostic. Once the distribution is
inconsistent with that in the training nodes, the performance of all the current GNNs will be hindered.
Even transfer learning is able to solve the distribution shift problem, however, it still needs the prior
of test distribution, which actually cannot be obtained beforehand. Therefore, the agnostic label
selection bias greatly affects the generalization ability of GNNs on unknown test data.
1
Under review as a conference paper at ICLR 2021
Figure 1: Effect of selection bias on GCN and GAT.
In order to observe selection bias in real graph data, we conduct an experimental investigation to
validate the effect of selection bias on GNNs (details can be seen in Section 2.1). We select training
nodes with different biased degrees for each dataset, making the distribution of training nodes and test
nodes inconsistent. The results clearly show that selection bias drastically hinders the performance of
GNNs on unseen test nodes. Moreover, with heavier bias, the performance drops more. Further, we
theoretically analyze how the data selection bias results in the estimation bias in GNN parameters
(details can be seen in Section 2.2). Based on the stable learning technique (Kuang et al., 2020), we
can assume that the learned embeddings consist of two parts: stable variables and unstable variables.
The data selection bias will cause the spurious correlation between these two kinds of variables.
Thereby we prove that with the inevitable model misspecification, the spurious correlation will further
cause the parameter estimation bias. Once the weakness of the current GNNs with selection bias is
identified, one natural question is “how to remove the estimation bias in GNNs?”
In this paper, we propose a novel Debiased Graph Neural Network (DGNN) framework for stable
graph learning by jointly optimizing a differentiated decorrelation regularizer and a weighted GNN
model. Specifically, the differentiated decorrelation regularizer is able to learn a set of sample weights
under differentiated variable weights, so that the spurious correlation between stable and unstable
variables would be greatly eliminated. Based on the causal view analysis of decorrelation regularizer,
we theoretically prove that the weights of variables can be differentiated by the regression weights.
Moreover, to better combine the decorrelation regularizer with GNNs, we prove that adding the
regularizer to the embedding learned by the second to last layer could be both theoretically sound and
flexible. Then the sample weights learned by decorrelation regularizer are used to reweight GNN loss
so that the parameter estimation could be unbiased.
In summary, the contributions of this paper are three-fold: i) We investigate a new problem of
learning GNNs with agnostic label selection bias. The problem setting is general and practical for real
applications. ii) We bring the idea of variable decorrelation into GNNs to relieve bias influence on
model learning and propose a general framework DGNN which could be adopted to various GNNs.
iii) We conduct the experiments on real-world graph benchmarks with two kinds of agnostic label
selection bias, and the experimental results demonstrate the effectiveness and flexibility of our model.
2	Effect of Label Selection Bias on GNNs
In this section, we first formulate our target problem as follows:
Problem 1 (Semi-supervised Learning on Graph with Agnostic Label Selection Bias). Given a
training graph Gtrain = {Atrain, Xtrain, Ytrain}, where Atrain ∈ RN×N (N nodes) represents
the adjacency matrix, Xtrain ∈ RN×D (D features) refers to the node features and Ytrain ∈ Rn×C
(n labeled nodes, C classes) refers to the available labels for training (n ≪ N), the task is to
learn a GNN gθ G) with parameter θ to precisely predict the label of nodes on test graph GteSt =
{Atest, Xtest, Ytest}, where distribution Ψ(Gtrain) ≠ Ψ(GteSt).
2.1	Experimental Investigation
We conduct an experimental investigation to examine whether the state-of-the-art GNNs are sensitive
to the selection bias. The main idea is that we will perform two representative GNNs: GCN (Kipf
2
Under review as a conference paper at ICLR 2021
& Welling, 2016) and GAT (Velickovic et al., 2017) on three widely used graph datasets: Cora,
Citeseer, Pubmed (Sen et al., 2008) with different degrees of bias. If the performance drops sharply
in comparison with the scenarios without selection bias, this will demonstrate that GNNs cannot
generalize well in selection bias setting.
To simulate the agnostic selection bias scenario, we first follow the inductive setting in Wu et al.
(2019) that masks the validation and test nodes as the training graph Gtrain in the training phase,
and then infer the labels of validation and test nodes with whole graph Gtest . In this way, the
distribution of test node can be considered agnostic. Following Zadrozny (2004), we design a biased
label selection method on training graph Gtrain . The selection variable e is introduced to control
whether the node will be selected as labeled nodes, where e = 1 means selected and 0 otherwise. For
node i, we compute its neighbor distribution ratio: r = ∣{j∣j ∈ Ni, yj ≠ yi}∣∕∣Ni∣, where Ni is
neighborhood of node i in Gtrain and yj ≠ yi means the label of central node i is not the label of its
neighborhood node j . And ri measures the difference between the label of central node i with the
labels of its neighborhood. Then we average all the nodes’ r to get a threshold t. For each node, the
{	ri ≥ t
1 - ri < t , where ∈ (0.5, 1) is used to control
the degree of selection bias and the larger means heavier bias. We set as {0.7, 0.8, 0.9} to get three
bias degrees for each dataset, termed as Light, Medium, Heavy, respectively. We select 20 nodes for
each class for training and the validation and test nodes are same as Yang et al. (2016). Furthermore,
we take the unbiased datasets as baselines, where the labeled nodes are selected randomly.
Figure 1 is the results of GCN and GAT on biased datasets. The dashed lines mean the performances
of GCN/GAT on unbiased datasets and the solid lines refer to the results on biased datasets. We can
find that: i) The dashed lines are all above the corresponding coloured solid lines, indicating that
selection bias greatly affects the GNNs’ performance. ii) All solid lines decrease monotonically with
the increase of bias degree, demonstrating that heavier bias will cause larger performance decrease.
2.2	Theoretical analysis
The above experiment empirically verifies the effect of selection bias on GNNs. Here we theoretically
analyze the effect of selection bias on estimating the parameters in GNNs. First, because biased
labeled nodes have biased neighborhood structure, GNNs will encode this biased information into
the node embeddings. Based on stable learning technique (Kuang et al., 2020), we make following
assumption:
Assumption 1. All the variables of embeddings learned by GNNs for each node can be decomposed
as H = {S, V}, where S represents the stable variables and V represents the unstable variables.
Specifically, for both training and test environment, E(Y∣S = s, V = v) = E(Y∣S = s).
Under Assumption 1, the distribution shift between training set and test set is mainly induced by the
variation in the joint distribution over (S, V), i.e., P(Strain, Vtrain) ≠ P(Stest, Vtest). However,
there is an invariant relationship between stable variable S and outcome Y in both training and test
environments, which can be expressed as P(Ytrain ∣Strain) = P(Ytest∣Stest). Assumption 1 can be
guaranteed by Y⊥V∣S. Thus, one can solve the stable prediction problem by developing a function
f (∙) based on S. However, one can hardly identify such variables in GNNs.
Without loss of generality, we take Y as continuous variable for analysis and have the following
assumption:
Assumption 2. The true generation process of target variable Y contains not only the linear
combination of stable variables S, but also the nonlinear transformation of stable variables.
Based on the above assumptions, we formalize the label generation process as follows:
Y = f (X, A) + ε = G(X, A; θg)sβs + G(X, A;储)vβv + g(G(X, A;储)s) + ε, (1)
where G(X, A; θg) ∈ RN×p denotes an unknown function of X and A that learns node embedding
and it can be learned by a GNN, such as GCN and GAT, the output variables of G(X, A; θg ) can
be decomposed as stable variables G(X, A; θg )S ∈ RN×m and unstable variables G(X, A; θg)V ∈
RN×q (m + q = p), βS ∈ Rm×1 and βV ∈ Rq×1 are the linear coefficients can be learned by the last
layer of GNNs, ε is the independent random noise, and g(∙) is the nonlinear transformation function
3
Under review as a conference paper at ICLR 2021
of stable variables. According to Assumption 1, we know that coefficients of unstable variables
G (X, A; θg)V are actually 0 (i.e., βV =0).
For a classical GNN model with linear regressor, its prediction function can be formulated as:
ʌ ʌ , ʌ ʌ , ʌ
Y = G(X, A; θg)sβs + G(X, A; θg)vβv + ε.	(2)
Compared with Eq. (1), we can find that the parameters of GNN could be unbiasedly estimated if the
nonlinear term g(G (X, A; θg)S) = 0, because the GNN model will have the same label generation
mechanism as Eq. (1). However, limited by the nonlinear power of GNNs (Xu et al., 2019), it is
reasonable to assume that there is a nonlinear term g(G (X, A; θg)S) ≠ 0 that cannot be fitted by
the GNNs. Under this assumption, next, we taking a vanilla GCN (Kipf & Welling, 2016) as an
example to illustrate how the distribution shift will induce parameter estimation bias. A two-layer
GCN can be formulated as Aσ(AXW(O))W⑴，where A is the normalized adjacency matrix, W
is the transformation matrix at each layer and σ(∙) is the Relu activation function. We decompose
GCN as two parts: one is embedding learning part Aσ(AXW(0)), which can be decomposed as
[ST, VT ], corresponding to CG(X, A; θg)s and G(X, A; θg )v in Eq. (2), and the other part is W(1),
where the learned parameters can be decomposed as [βS, βV], corresponding to [βS, βV] in Eq. (2).
We aim at minimizing the square loss: LGCN = ∑n=ι(STβs + VTeV- Yi)2.
According to the derivation rule of partitioned regression model, we have:
nn	nn
BV-	βv = (n ∑ VTVi)	(1 ∑ VTg(Si))	+ (n ∑ VTVi)	(1 ∑	VTSi)(βs	- βs),	(3)
i=1	i=1	i=1	i=1
nn	nn
βs —	βs	= (n ∑ STSi)-1(1	∑ STg(Si))	+ (1 ∑ STSi)-1(1	∑	STVi)(βv	— βv),
i=1	i=1	i=1	i=1
(4)
where n is labeled node size, Si is i-th sample of S, 1 ∑M VTg(Si) = E(VTg(S)) + 0p(1),
n ∑n=1 VTSi = E(VtS) + 0p(1) and Op(1) is the error which is negligible. Ideally, βv - βv = 0
indicates that there is no bias between the estimated and the real parameter. However, if E(VTS) ≠ 0
T
or E(V g(S)) ≠ 0 in Eq. (3), βV will be biased, leading to the biased estimation on βS in Eq. (4) as
well. Since the correlation between V and S (or g(S)) might shift in test phase, the biased parameters
learned in training set is not the optimal parameters for predicting testing nodes. Therefore, to increase
the stability of prediction, we need to unbiasedly estimate the parameters of βV by removing the
correlation between V and S (or g(S)) on training graph, making E(VTS) = 0 or E(VTg(S)) = 0.
Note that 1 ∑n=1 STg(Sj in Eq. (4) can also cause estimation bias, but the relation between S and
g(S) is stable across environments, which do not influence the stability to some extent.
3	Proposed Model
3.1	Revisiting on Variable Decorrelation in Causal View
To decorrelate V and S (or g(S)), we should decorrelate the output variables ofG (X, A; θg) (Kuang
et al., 2020). They propose a Variable Decorrelation (VD) term with sample reweighting technique
to eliminate the correlation between each variable pair, in which the sample weights are learned by
jointly minimizing the moment discrepancy between each variable pair:
p
LVD(H) = ∑ ∣∣HTΛwH.-j/n - HTw/n ∙ HT.jw∕n∣∣2,	(5)
where H ∈ Rn×p means the variables needed to be decorrelated, H.j is j-th variable of H, H.-j =
H\H.j means all the remaining variables by setting the value of j-th variable in H as zero, w ∈ Rn×1
are sample weights, ∑n=1 Wi = n and Λw = diag(w1, ∙∙∙, Wn) is the corresponding diagonal matrix.
As we can see, LVD(H) can be reformulated as ∑j≠k ∣∣HTΛwH.k∕n - HTw/n ∙ HTkw∕n∣∣2, and
it aims to let E(H.Ti H.j) = E(H.Ti)E(H.j) for each variable pair j and k. LVD (H) decorrelates all
4
Under review as a conference paper at ICLR 2021
the variable pairs equally. However, decorrelating all the variables requires sufficient samples Kuang
et al. (2020), i.e., n → ∞, which is hard to be satisfied, especially in the semi-supervised setting. In
this scenario, we cannot guarantee LV D (H) = 0. Therefore the key challenge is how to remove the
correlation influencing the unbiased estimation most when LV D(H) ≠ 0.
Inspired by confounding balancing technique in observational studies (Hainmueller, 2012), we revisit
the variable decorrelation regularizer in causal view and show how to differentiate each variable
pair. Confounding balancing techniques are often used for causal effect estimation of treatment
T , where the distributions of confounders X are different between treated (T = 1) and control
(T = 0) groups because of non-random treatment assignment. One could balance the distribution of
confounders between treatment and control groups to unbiased estimate causal treatment effects (Yao
et al., 2020). Most balancing approaches exploit moments to characterize distributions, and balance
them by adjusting sample weights w as follows: w = arg minIl Zi:Ti = 1 Xi- Zi:Ti=0 wi ∙ xi∣l2.
After balancing, the treatment T and confounders X tend to be independent.
Given one targeted variable j, under the variables only have linear relation assumption1, its decor-
relataion term, LV Dj = IlHTAwHT/n - HTW/n ∙ H.-jw∕n∣∣2, is to make Hj independent
of H.-j, which is same as the confounding balancing term making treatment and confounders
independent. Thereby, LV Dj can also be viewed as a confounding balancing term, where H.j is
treatment and H.-j is confounders, illustrated in Fig. 2(a). Hence, our target can be explained as
unbiasedly estimate causal effect of each variable which is invariant across training and test set. As
different variable may contribute unequally to the confounding bias, it is necessary to differentiate
the confounders. The target of differentiating confounders exactly matches our target that removes
the correlation of variables influencing the unbiased estimation most.
3.2	Differetiated Variable Decorrelation
Considering the continuous treatment, the causal effect of treatment can be measured by
Marginal Treatment Effect Function (MTEF) (Kreif et al., 2015), and defined as: MTEF =
吼乂")]段:"-~)], where Yi(t) represents the potential outcome of sample i with treatment status
T = t, E(∙) refers to the expectation function, and ∆t denotes the increasing level of treatment. With
the sample weights w decorrelating treatment and confounders, we can estimate the MTEF by:
——	∑Ti=t Wi
MTEF =——i----
∙匕⑴-∑"j∙=t-∆t wj ∙ Yj(t - δD
(6)
∆t
Next we theoretically analyze how to differentiate confounders’ weights with following theorem.
Theorem 1. In observational studies, different confounders make unequal confounding bias on
Marginal Treatment Effect Function (MTEF) with their own weights, and the weights can be learned
via regressing outcome Y on confounders X and treatment variable T.
We prove Theorem 1 with the following assumption:
Assumption 3 (Linearity). The regression of outcome Y on confounders X and treatment variable
T is linear, that is Y = ∑k≠t αkX.k + αtT + c + ε, where αk ∈ α is the linear coefficient.
Under Assumption 3, we can write estimator of MTEF as:
——	∑i.Ti=t Wi ,匕⑴-∑j∕=t-∆t wj ∙ Yj(t - δð
MTEF = ——i---------------j-j----------------
∆t	(7)
= MTEF + ∑k* a αk( " =t "i Xik-1。…tw Xjk ) + φ(ε),
k≠t	∆t
where MTEF is the ground truth, φ(ε) means the noise term, and φ(ε)二 0 with Gaussian noise.
The detailed derivation can be found in Appendix A. To reduce the bias of MTEF, we need
∑f ∑i:Ti =t wiXik-Σj:Tj =t-∆t WjiXjk、	1	∑i:Ti = t wiXik-Σj:Tj =t-∆t wj'Xjk
k≠t a (--------∆T^----------), where---------短-----------
1Nonlinear relation between variables can be incorporated by considering high-order moments in Eq. (5).
5
Under review as a conference paper at ICLR 2021
Differentiated
(a) Decorrelation in Causal View
K-th layer
(b) GNN-DVD Framework
Figure 2: (a) Diagram of decorrelating node embedding with confounding balance. H(K-1) is the
node embedding to be decorrelated. T is the treatment, corresponding to one target variable in
H(K-1). X is the confounders, corresponding to the remaining variables of the target variable in
H(K-1). Y is the outcome, corresponding to labels. (b) The framework of GNN-DVD. The same
color in the two figures represents the same kind of variable.
means the difference of the k-th confounder between treated and control samples. The parameter
αk represents the confounding bias weight of the k-th confounder, and it is the coefficient of X.k.
Moreover, because our target is to learn the weight of each variable pair, i.e., between treatment and
each confounder, we need to learn the weight αt of treatment that is the coefficient of T . Hence, the
confounder weights and treatment weight can be learned from the regression of observed outcome Y
on confounders X and treatment T under Linearity assumption.
Due to the connection between treatment effect estimation with variable decorrelation as analyzed
in Section 3.1, we utilize Theorem 1 to reweight the variable weight in variable decorrelation term.
When apply the Theorem 1 to GNNs, the confounders X should be H.-j and treatment is H.j ,
where the embedding H is learned by G (X, A; θg) in Eq. (2). And the variable weights α could
be computed from the regression coefficients for H, hence α is equal to β in Eq. (2). Then the
Differentiated Variable Decorrelation (DVD) term can be formulated as follows:
minLDVD(H) = ∑p 1(αT ♦ abs(HTAWH.-j/n - HTw∕n∙ HrTjWln))
w	λj =	1	(8)
λ1 n 2	1 n	2
+ 方 ∑i=ι Wi + λ2(n ∑i=ι WiT) ,s.t.w 二 0
where abs(∙) means the element-wise absolute value operation, preventing positive and negative
values from eliminating. Term，∑n=1 w2 is added to reduce the variance of sample weights to
achieve stability, and the formula λ2( 1 ∑n=1 wi - 1)2 avoids all the sample weights to be 0. The term
w > 0 constrains each sample weight to be non-negative. After variable reweighting, the weighted
22 T	T	T	2
decorrelation term in Eq. (8) can be rewritten as ∑ ∙≠k αj∙ αk ∣∣Hj AW H.k/n - Hj w/n ♦ H.k w∕n∣∣2,
and the weight for variable pair j and k would be αj2αk2 , hence it considers both the weights of
treatment and confounder. We prove the uniqueness property of W in Appendix B, as follows:
Theorem 2 (Uniqueness). If λ1n ≫ p2 + λ2, p2 ≫ max(λ1, λ2), ∣Hi,j ∣ ≤ c and ∣αi ∣ ≤ cfor some
constant c, the solution W ∈ {w : ∣wi∣ ≤ c} to minimize Eq. (8) is unique.
3.3 Debiased GNN Framework
In this section, we describe the framework of Debiased GNN that incorporates DVD/VD term with
GNNs in a seamless way. As analyzed in Section 2.2, decorrelating Aσ( AXW(0)) could make GCN
stable. However, most GNNs follow a layer-by-layer stacking structure, and the output embedding
of each layer is more easy to obtain in implementing. Since Aσ( AXW(0)) is the aggregation of
6
Under review as a conference paper at ICLR 2021
the first layer embedding σ(AXW(O)), decorrelating these variables may lack the flexibility that
incorporates DVD/VD term with other GNN structure. Fortunately, we have the following theorem
to identify a more flexible way to combine variable decorrelation with GNNs.
Theorem 3. Given P pairwise uncorrelated variables Z = (Zι, Z2,…,Zp), with a linear aggrega-
tion operator A, the variables of Y = AZ are still pairwise uncorrelated.
Proof can be found in Appendix C. The theorem indicates that if the variables of embeddings Z are
uncorrelated, after any form of linear neighborhood aggregation A, e.g., average, attention or sum,
the variables of transformed embeddings Y would be also uncorrelated. Therefore, decorrelating
(0)
σ(AXW ) can also reduce the estimation bias. For a K layers of GNN, we can directly decorrelate
the output of (K - 1)-th layer, i.e., σ(A…σ(AXW(0))…W(K-2)) for a K layers of GCN.
The previous analysis finds a flexible way to incorporate DVD/VD term with GNNs, however, recall
that we analyze GNNs based on the least squares loss, and most existing GNNs are designed for
classification. Therefore, in the following, we analyze that the previous conclusions are still applicable
in classification. We consider the cases that softmax layer is used as the output layer of GNNs and
loss is the cross-entropy error function. We use the Newton-Raphson update rule (Bishop, 2006) to
bridge the gap between linear regression and multi-classification. According to the Newton-Raphson
update rule, the update formula for transformation matrix W(K-1) of the last layer of GCN can be
derived:
W.(jnew) = W.(jold) - (HTRH)-1HT(HW.(jold) - Y.j)
= (HTRH)-1{HTRHW.(jold) - HT(HW.(jold) - Y.j)} = (HTRH)-1HTRz,	()
where Rkj = - ∑nN=1 HnW.(kold)(Ikj - HnW.(jold)) is a weighing matrix and Ikj is the element of
the identity matrix, and z = HW.(jold) - R-1(Y.j - W.jH) is an effective target value. Eq. (9) takes
the form of a set of normal equations for a weighted least-squares problem. As the weighing matrix
R is not constant but depends on the parameter vector W.(jold), we must apply the normal equations
iteratively. Each iteration uses the last iteration weight vector W.(jold) to compute a revised weighing
matrix R and regresses the target value z with HW.(jnew). Therefore, the variable decorrelation can
also be applied to the GNNs with softmax classifier to reduce the estimation bias in each iteration.
Figure 2(b) is the framework of GNN-DVD, and we input the labeled nodes, embeddings HH(KT)
into the regularizer LDVD(H(KT)). AS GCN has the formula Softmax(AH(KT)W(KT)),
the variable weights of H(KT) used for differentiating LDVD(H(KT)) can be computed from
α = Var(W(K-1), axis = 1), where Var(∙, axis = 1) refers to calculating the variance of each row of
some matrix and it reflects each variable’s weight for classification which is similar to the regression
coefficients. Note that when incorporating VD term with GNNs, we do not need compute the variable
weights. Then the sample weights W learned by DVD term have the ability to remove the correlation
in HH(K-1). We propose to use this sample weights to reweight softmax loss:
minLG = ∑ Wi ∙ ln(q(HH(K)) ∙ Yι),
θ	l∈YL
(10)
where q(∙) is the SoftmaX function, YL is the set of labeled node indices and θ is the set of parameters
of GCN. The complexity analysis as well as the optimization of whole algorithm are summarized in
AppendiX D.
4 Experiments
Datasets Here, we validate the effectiveness of our method on node classification with two kinds
of selection biased data, i.e., label selection bias and small sample selection bias. For label selection
bias, we empoly three widely used graph datasets: Cora, Citeseer and Pubmed (Sen et al., 2008).
As in Section 2.1, we make the inductive setting for each graph and get three biased degrees for
each graph. For small sample selection bias, we conduct the experiments on NELL dataset (Carlson
7
Under review as a conference paper at ICLR 2021
Table 1: Performance of three citation networks. The ‘*’ indicates the best results of the baselines.
Best results of all methods are indicated in bold. ‘% gain over GCN/GAT’ means the improvement
percent of GCN/GAT-DVD against GCN∕GAT,respectively.
Method	Cora			Citeseer			Pubmed		
	Light	Medium	Heavy	Light	Medium	Heavy	Light	Medium	Heavy
MLP	0.5624	0.5197	0.5087	0.4532	0.3757	0.3893	0.6852	0.6620	0.6378
Planetoid (Yang et al., 2016)	0.5890	0.5240	0.5180	0.5160	0.5140	0.4880	0.7160	0.6770	0.6680
Chebyshev (Defferrard et al., 2016)	0.7116	0.7006	0.6809	0.6542	0.6276	0.5920	0.7358	0.6862	0.6732
SGC (WU et al., 2019)	0.7800	0.7800	0.7530	0.6780	0.6730*	0.6200	0.7880*	0.7560	0.6800
APPNP (Klicpera et al., 2019)	0.7913	0.7689	0.7629	0.6478	0.6052	0.5903	0.7639	0.7369	0.6862
-GNM-GCN (Zhou et al., 2019)-	0.7423	0.7531	0.7196	0.5793	0.5717	0.5125	0.7552	0.7381	0.7072
GNM-GAT (Zhou et al., 2019)	0.7875	0.7638	0.7404	0.6524	0.6487	0.5865	0.7438	0.7568	0.6891
GCN (Kipf & Welling, 2016)	0.7851	0.7775	0.7422	0.6786	0.5952	0.5551	0.7673	0.7545	0.7247
GCN-VD	0.7951	0.7855	0.7522	0.6844	0.6676	0.6408	0.7727	0.7729	0.7399
GCN-DVD	0.7959	0.7885	0.7555	0.6908	0.6769	0.6496	0.7741	0.7746	0.7542
% gain over GCN	1.38%	1.41%	1.79%	1.8%	14.2%	17.0%	0.89%	2.67%	4.07%
-GAT (VeIiCkOViC et al.,2017)	0.8067*	0.8019*	0.7578	0.7033*	0.6683	0.6475*	0.7665	0.7579*	0.7068
GAT-VD	0.8146	0.8079	0.7708	0.7149	0.6833	0.6611	0.7783	0.7689	0.7149
GAT-DVD	0.8179	0.8119	0.7694	0.7172	0.6825	0.6627	0.7788	0.7723	0.7210
% gain over GAT	1.39%	1.26%	1.53%	1.97%	2.12%	2.34%	1.6%	1.9%	2.0%
et al., 2010) that each class only has one labeled node for training. Due to the large scale of this
dataset, the test nodes are easily to have distribution shift from training nodes. The details of the
datasets and experimental setup are given in Appendix E. One can download codes and datasets for
all experiments from the supplementary material.
Baselines Under our proposed framework, we incorporate the VD/DVD term with GCN and GAT
called GCN-VD/DVD and GAT-VD/DVD (details in Appendix F), and thus GCN and GAT are
two basic baselines. We compare with GNM-GCN/GAT (Zhou et al., 2019) that considers the
label selection bias in transductive setting. Moreover, several state-of-the-art GNNs are included:
Chebyshev filter (Kipf & Welling, 2016), SGC (Wu et al., 2019) and APPNP (Klicpera et al., 2019).
Additionally, we compare with Planetoid (Yang et al., 2016) and MLP trained on the labeled nodes.
Results on Label Selection Bias Dataset The results are given in Table 1, and we have the
following observations. First, the proposed models (i.e., GCN/GAT with VD/DVD terms) always
achieve the best performances in most cases, which well demonstrates that the effectiveness of our
proposed debiased GNN framework. Second, comparing with base models, our proposed models
all achieve up to 17.0% performance improvements, and gain larger improvements under heavier
bias scenarios. Since the major difference between our model with base models is the VD/DVD
regularizer, we can safely attribute the significant improvements to the effective decorrelation term
and its seamless joint with GNN models. Moreover, GCN/GAT-DVD achieve better results that
GCN/GAT-VD in most cases. It validates the importance and effectiveness of differentiating variables’
weights in semi-supervised setting. Additional experimental results about the sample weight analysis
and parameter sensitivity analysis can be found in Appendix G.
Results on Small Sample Selection Bias Dataset As NELL is a large-scale graph, we cannot run
GAT on a single GPU with 16GB memory. We only perform GCN-VD/DVD and compare with
representative methods which can perform on this dataset. The results are shown in Table 2. First,
GCN-VD/DVD achieve significant improvements over GCN. It indicates that selection bias could
be induced by a small number of labeled nodes and our proposed method can relieve the estimation
bias. Moreover, GCN-DVD further improves GCN-VD with a large margin. It further validates that
decorrelating all the variable pairs equally is suboptimal, and our differentiated strategy is effective
when labeled nodes are scarce. The reason that GNM-GCN fails is the GNM relies on the accuracy of
the IPW estimator that predicts the probability of a node to be selected, however, in this dataset, the
ratio of positive and negative samples are extremely unbalanced influencing the performance of IPW.
Table 2: Performance of NELL
Dataset MLP	Planetoid^^SGC	GNM-GCN^^GCN	GCN-VD^^GCN-DVD
NELL~~02385^^03901	04128^^0.1589	04416^^04652	04734
8
Under review as a conference paper at ICLR 2021
5	Related Works
In the past few years, Graph Neural Networks (GNNs) (Scarselli et al., 2008; Kipf & Welling, 2016;
Velickovic et al., 2017; XU et al., 2019; KlicPera et al., 2019) have become the major technology
to capture patterns encoded in the graph due to its powerful representation capacity. Although the
cUrrent GNNs have achieved great sUccess, when aPPlied to indUctive setting, they all assUme that
training nodes and test nodes follow the same distribUtion. However, this assUmPtion does not always
hold in real aPPlications. GNM (ZhoU et al., 2019) first Pays attention on the label selection Problem
on graPh learning, and it learns a IPW estimator to estimate the Probability of each node to be selected
and Uses this Probability to reweight the labeled nodes. However, it heavily relies on the accUracy of
IPW estimator, which dePends on the label assignment distribUtion of whole graPh, hence it is more
sUitable for transdUctive setting.
To enhance the stability in Unseen varied distribUtions, some literatUres (Shen et al., 2020b; KUang
et al., 2020) have revealed the connection between correlation and Prediction stability Under model
missPecification. However, these methods are bUilt on the simPle regressions, bUt GNNs have more
comPlex strUctUre and ProPerties needed to be considered. We also notice that Shen et al. (2020a)
ProPose a differentiated variable decorrelation term for linear regression. However, this decorrelation
term reqUires mUltiPle environment with different correlations between stable variable and Unstable
variable available in the training stage while oUr method do not reqUire.
6	Conclusion
In this PaPer, we investigate a general and Practical Problem: learning GNNs with agnostic selection
bias. The selection bias will inevitably caUse the GNNs to learn the biased correlation between
aggregation mode and class label and make the Prediction Unstable. We then ProPose a novel
differentiated decorrelated GNN, which combines the debiasing techniqUe with GNNs in a Unified
framework. Extensive exPeriments well demonstrate the effectiveness and flexibility of GNN-DVD.
9
Under review as a conference paper at ICLR 2021
References
Christopher M Bishop. Pattern recognition and machine learning. springer, 2006.
Andrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R Hruschka, and Tom M
Mitchell. Toward an architecture for never-ending language learning. In AAAI, 2010.
Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfit-
ting in deep networks by decorrelating representations. In ICLR, 2016.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In NeurIPS, pp. 3844-3852, 2016.
Jens Hainmueller. Entropy balancing for causal effects: A multivariate reweighting method to produce
balanced samples in observational studies. Political analysis, pp. 25-46, 2012.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
NeurIPS, pp. 1024-1034, 2017.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In ICLR, 2016.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate: Graph
neural networks meet personalized pagerank. In ICLR, 2019.
Noemi Kreif, Richard Grieve, Ivan Diaz, and David Harrison. Evaluation of the effect of a continuous
treatment: a machine learning approach with an application to treatment for traumatic brain injury.
Health economics, 24(9):1213-1228, 2015.
Kun Kuang, Ruoxuan Xiong, Peng Cui, Susan Athey, and Bo Li. Stable prediction with model
misspecification and agnostic distribution shift. In AAAI, 2020.
Yuji Nakatsukasa. Absolute and relative weyl theorems for generalized eigenvalue problems. Linear
Algebra and its Applications, 432(1):242-248, 2010.
Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE Transactions on Neural Networks, 20(1):61-80, 2008.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Zheyan Shen, Peng Cui, Jiashuo Liu, Tong Zhang, Bo Li, and Zhitang Chen. Stable learning via
differentiated variable decorrelation. In KDD, pp. 2185-2193, 2020a.
Zheyan Shen, Peng Cui, Tong Zhang, and Kun Kuang. Stable learning via sample reweighting. In
AAAI, pp. 5692-5699, 2020b.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2017.
Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplify-
ing graph convolutional networks. In ICML, pp. 6861-6871. PMLR, 2019.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR, 2019. URL https://openreview.net/forum?id=ryGs6iA5Km.
Zhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with
graph embeddings. In ICML, 2016.
Liuyi Yao, Zhixuan Chu, Sheng Li, Yaliang Li, Jing Gao, and Aidong Zhang. A survey on causal
inference. arXiv preprint arXiv:2002.02770, 2020.
Bianca Zadrozny. Learning and evaluating classifiers under sample selection bias. In ICML, pp. 114,
2004.
Fan Zhou, Tengfei Li, Haibo Zhou, Hongtu Zhu, and Ye Jieping. Graph-based semi-supervised
learning with non-ignorable non-response. In NeurIPS, pp. 7015-7025, 2019.
10
Under review as a conference paper at ICLR 2021
A Derivation of MTEF
—
MTEF =
+
∑iTi=t wi ∙ γi(t) - ∑j:Tj=t-∆t wj ∙ Yj(t- ∆t)
∆t
∑iTi=t wi ∙ (∑k≠t αkXik + αtt + C + e) - Σj∙,Tj =t-∆t wj ∙ (∑k≠t αkXjk + at(t - △t) + C + e)
∆t
∑i∙.Ti=t Wiatt - ∑jTj=t-∆t Wjat(t - δ%)
∆t
(∑i：Ti=t Wi ∑k≠t ak Xik - ∑jTj=t-∆t Wj ∑k≠t QkXik)
---------------∆fj--------------------+ 63
∑	，Σi:Ti=t wi ∙ Xik - ∑jTj=t-∆t wj ∙ Xjk、	，、
=MTEF + ∑ Qk (—i------------j------------------) + 6()
k≠t
(11)
where *i：Ti=t Wia/ 工飞；δ Wj"" "" is the ground truth of MTEF, φ(e) means the noise term,
and φ(e)= 0 with Gaussian noise.
B Proof of Theorem 2
W
P	ʌ n
arg min ∑(αT ∙abs(HTΛwH.-j /n-HTw∕n∙HT-jw/n))2 + V ∑
1 n
Wi+%(n ∑ Wi-1)2
i = 1
(12)
Proof For simplicity, we denote L1 = ∑P=ι(QT ∙ abs(HTΛwH.-j∕n - HjW/n ∙ HLW/n))2,
£2 = 1 ∑n=ι w2, L3 = (n ∑n=ι Wi - 1)2 and F(w) = Li + λ1L1 + 人心.We first calculate
the Hessian matrix of F(w), denoted as He, to prove the uniqueness of the optimal solution W, as
follows:
He
∂ 2Li	∂2L2	∂2L3
∂W2 + 1 ∂w2 + 2 ∂w2
For the term L1, we can rewrite it as:
L 2 2 . 1 n- — —	. 1 n- —	. . 1 n- ―	— 2
LI = ∑ Qiαk(n	∑ Hi,jHi,kwi- (n ∑	Hi,jwi)(n	∑	Hi,kWi))
j≠k	i=1	i=1	i=1
In	n n	In	In
=∑ Q2αk((n ∑ Hi,jHi,kWi)2 - (n ∑ Hi,jH*W头ɪ ∑ 珥g)(ɪ ∑ 珥ɪ)
j≠k	i=1	i=1	i=1	i=1
1n	1n
+ (( n Σ HijWi)( n Σ Hi,k Wi)))
i=1	i=1
And when ∣Hi,j∣	≤ c, for any variable j and k, and ∣w∕	≤ c, we have
∂W2 ( n ∑n=1 Hi,jHi,k Wi)2 = O( ni2 ), ∂W2 (1 ∑n=1 HijWi)(1 ∑n=1 Hi,k Wi)= O( ^⅛ ) and
怎((2 ∑n=1 HiyHi,k Wi)( n ∑n=1 HijWi)( n ∑i=i Hi,k Wi)) = O( ni2 ). Thenwith |Qi ∣ ≤ c,
wehave a2ak∂dw2 (n ∑n=ι Hij Hi,k Wi- (n ∑n=ι HijWi)(n ∑n=ι 珥需Wi))2 = O( *). LI is
sum of p(p - 1) such terms. Then we have
∂2L1
∂w2
2
=O( p2).
n2
With some algebras, we can also have
∂2L2 _ 1T
∂w2 _ n ：
11
Under review as a conference paper at ICLR 2021
d 2L3 = ɪ 11T
∂ w2 = n2	,
thus,
He = O(p2) + λ11 + λ211T = λ11 + O( p2+λ2).
e 'n2' n	n2	n " n2	'
Therefore, if λ1 ≫ P ：j2, equivalent to λ1n ≫ p2 + λ2, He is an almost diagonal matrix. Hence, He
is positive definite (Nakatsukasa, 2010). Then the function F(w) is convex on C = {w ： ∣ Wi ∣ ≤ c},
and has unique optimal solution W.
Moreover, because L1 is our major decorrelation term, we hope L1 to dominate the terms
XIL and λ2L3. On C, We have L1 = O(1), L = O(1), and α2ak(1 ∑n=1 Hi,jHi,fcWi -
(1 ∑n=ι HijWi)1 ∑n=ι Hi,kWi))2 = O(1). Thus Li = O(p2). When p2 ≫ max(λ1,λ2),
L1 Will dominate the regularization terms L2 and L3 .
C Proof of Theorem 3
Let Z = {Zι, Z2, …, Zp} be P pairwise uncorrelated variables. ∀Zi, Zj ∈ Z, (zi1), Z(2,…, Zin))
and (Zj1), Zj2, …, Zjn)) are n simple random samples drawn from Zi and Zj respectively, and have
same distribution with Zi and Zj. Given a linear aggregation matrix A = (aij-), ∀s,v ∈ (1,2,∙∙∙,n),
let Yi(s) = ∑kn=1 askZi(k) and Yj(v) = ∑ln=1 avlZ(jl), and we have following derivation:
nn
Cov(Yi(s), Yj(v)) = Cov(∑ askZi(k), ∑ avlZ(jl))
k=1	l=1
nn
nn
= ∑ ∑ askavlCov(Zi(k), Z(jl)) = ∑∑ask avl δij ,
k=1 l=1
k=1 l=1
where δij = 0 when i ≠ j, otherwise δij = 1. Therefore, when i ≠ j, we have Cov(Yi(s), Yj(v)) = 0
and Cov(Yi, Yj ) = 0. Extended the conclusion to multiple variable, Y = (Y1, Y2, …, Yn ) are
pairwise uncorrelated. Proof completes.
D Pseudocode of GNN-DVD
Algorithm 1: GNN-DVD Algorithm
Input : Training graph Gtrain = {A, X, Y}, and indices of labeled nodes YL; Max
iteration:maxI ter
Output : GNN parameter θ and sample weights W
Initialization: Let W = ω Θ ω and initialize sample weights ω with 1; Initialize GNN's
parameters θ with random uniform distribution; Iteration t - 0
1	while not converged or t < maxI ter do
2	Optimize θ(t) to minimize LG;
3	Calculate variable weights α(t) from W(K-1);
4	Optimize ω(t) to minimize LDVD(H(K-1));
5	t =t+ 1;
6	end
7	Return: θ and W = ω Θ ω
To optimize our GNN-DVD algorithm, we propose an iterative method. Firstly, we let W = ω Θ ω
to ensure non-negativity of W and initialize sample weight ωi = 1 for each sample i and GNN’s
parameters θ with random uniform distribution. Once the initial values are given, in each iteration,
12
Under review as a conference paper at ICLR 2021
we fix the sample weights ω and update the GNN’s parameters θ by LG with gradient descent,
then compute the confounder weights α from the linear transform matrix W(K-1). With α and
fixing the GNN’s parameters θ, we update the sample weights ω with gradient descent to minimize
LDV D(H(K-1)). We iteratively update the sample weights w and GNN’s parameters θ until LG
converges.
Complexity Analysis Compared with base model (e.g., GCN and GAT), the mainly incremental
time cost is the complexity from DVD term. The complexity of DVD term is O(np2), where n is the
number of labeled nodes and p is the dimension of embedding. And it is quite smaller than the base
model (e.g., the complexity of GCN is linear to the number of edges).
E	Dataset Description and Experimental Setup
E.1 Dataset Description
Table 3: DataSet StatiSticS
Dataset	Type	Nodes	Edges	Classes	Features	Bias degree ()	Bias type
Cora	Citation network	2,708	5,429	7	1,433	0.7/0.8/0.9	Label selection bias
Citeseer	Citation network	3,327	4,732	6	3,703	0.7/0.8/0.9	Label selection bias
Pubmed	Citation network	19,717	44,338	3	500	0.7/0.8/0.9	Label selection bias
NELL	Knowledge graph	65,755	266,144	210	5,414	One labeled node per class	Small sample selection bias
Some statistics of datasets used in our paper are presented in Table 3, including the number of nodes,
the number of edgeS, the number of claSSeS, the number of featureS, the biaS degree and biaS type.
For three citation networkS, we conduct the biaSed labeled node Selection proceSS to get three degreeS
of dataSetS for each dataSet to validate the effect of label Selection biaS, in which each claSS in each
dataSet containS 20 labeled nodeS in training Set and the validation Set and teSt Set are Same aS Yang
et al. (2016). For NELL, becauSe it only haS a Single labeled node per claSS in training Set, the training
nodeS are hard to cover all the neighborhood diStribution happened in the teSt Set. Hence, we uSe thiS
dataSet to validate the effectiveneSS of our method on the extreme Small labeled nodeS Size biaS. The
data SplitS are alSo Same aS Yang et al. (2016). A deScription of each of dataSet iS given aS followS:
•	Cora (Sen et al., 2008) iS a citation network of Machine Learning paperS that collected
from 7 claSSeS:{Theory, CaSe BaSed, Reinforcement Learning, Genetic AlgorithmS, Neural
NetworkS, ProbabiliStic MethodS, Rule Learning }. NodeS repreSent paperS, edgeS refer to
the citation relationShip, and featureS are bag-of-wordS vectorS for each paper.
•	CiteSeer (Sen et al., 2008) iS a citation network of Machine Learning paperS that collected
from 6 claSSeS:{AgentS, Artificial Intelligence, DatabaSe, Information Retrieval, Machine
Learning, Human Computer Interaction }. NodeS repreSent paperS, edgeS refer to the citation
relationShip, and featureS are bag-of-wordS vectorS for each paper.
•	Pubmed (Sen et al., 2008) iS a citation network from the PubMed databaSe, which containS
a Set of articleS (NodeS) related to diabeteS and the citation relationShip among them. The
node featureS are bag-of-wordS vectorS, and the node label are the diabeteS type reSearched
in the articleS.
•	NELL CarlSon et al. (2010) iS a dataSet extracted from the knowledge graph, which iS a Set
of entitieS connected with directed, labeled edgeS (relationS). Our pre-proceSSing Scheme
iS Same aS Yang et al. (2016), where each entity pair (e1, r, e2) iS aSSigned with Separate
relation nodeS r1 and r2 aS (e1, r1) and (e2, r2). We uSe text bag-of-wordS repreSentation
aS feature vector of the entitieS.
E.2 Experimental Setup
AS the Section 2.1 haS deScribed, for all dataSetS, to Simulate the agnoStic Selection biaS Scenario, we
firSt follow the inductive Setting in Wu et al. (2019) that maSkS the validation and teSt nodeS in the
training phaSe and validation and teSt with whole graph So that the teSt nodeS will be agnoStic. For
GCN and GAT, we utilize the Same two-layer architecture aS their original paper (Kipf & Welling,
2016; Velickovic et al., 2017). We use the following sets of hyperparameters for GCN on Cora,
13
Under review as a conference paper at ICLR 2021
Citeseer, Pubmed: 0.5 (dropout rate), 5 ∙ 10-4 (L2 regularization) and 32 (numbder of hidden units);
and for NELL: 0.1 (dropout rate), 1 ∙ 10-5 (L2 regularization) and 64 (number of hidden units). For
GAT on Cora, Citeseer, we use: 8 (first layer attention heads), 8 (features each head), 1 (second
layer attention head), 0.6 (dropout), 0.0005 (L2 regularization); and for Pubmed: 8 (second layer
attention head), 0.001 (L2 regularization), other parameters are same with Cora and Citeseer. To
fair comparison, the GNN part of our model uses the same architecture and hyper-parameters with
base model and we grid search λ1 and λ2 from {0.01, 0.1, 1, 10, 100}. For other baselines, we use
the optimal hyper-parameters in literatures on each dataset. For all the experiments, we run 10 times
with different random seed and report its average Accuracy results.
F	Extend to GAT
We can easily incorporate VD/DVD term to other GNNs. We combine them with GAT and more
extensions leave as future work. GAT utilizes attention mechanism to aggregate neighbor information.
It also follows the linear aggregation and transformation steps. Similar with GCN, the hidden
(K-1)
embedding H is the input of VD/DVD term, and the variable weights α are calculated from
the transformation matrix W(K-1) and the sample weights w are used to reweight the softmax loss.
Note that original paper utilizes same transformation matrix W(K-1) for transforming embedding
and learning attention values. Because α means the importance of each variable for classification,
and it should be computed from transformation matrix W(K-1) for transforming embedding, hence
we use separate matrix for transforming embedding and learning attention values respectively. This
modification does not change the performance of GAT in experiments.
G Additional Experiments
G.1 Sample weight analysis
Here we analyze the effect of sample weights w in our model. We compute the amount of correlation
in the labeled nodes, embeddings IH(KT) learned by standard GCN and the weighted embeddings of
the same layer learned by GCN-DVD. Note that, the weights are the last iteration of sample weights
of GCN-DVD. Following Cogswell et al. (2016), the amount of correlation of GCN and GCN-DVD
are measured by Frobenius norm of cross-corvairance matrix computed from vectors of IH(KT)
(K-1)
and weighted H respectively. Figure 3 shows the amount of correlation in unweighted and
weight embeddings, and we can observe that the embeddings’ correlation in all datasets are reduced,
demonstrating that the weights learned by GCN-DVD can reduce the correlations between embedded
variables. Moreover, one can observe that it is hard to reduce the correlation to zero. Therefore, the
necessity of differentiating variables’ weights will be further validated.
(a) Cora
Figure 3: Embedding correlation analysis on unweighted and weighted GCN.
Different bias
(c) Pubmed
G.2 Parameter sensitivity
We study the sensitiveness of parameters and report the results of GCN-DVD on three citation
networks in Fig. 4-6. The experimental results show that GCN-DVD is relatively stable to λ1 and λ2
with wide ranges in most cases, indicating the robustness of our model.
14
Under review as a conference paper at ICLR 2021
(a) Light	(b) Medium	(c) Heavy
Figure 4: Accuracy of GCN-DVD with different λ1 and λ2 on different biased Cora datasets.
(a) Light
(b) Medium
(c) Heavy
Figure 5: Accuracy of GCN-DVD with different λ1 and λ2 on different biased Citeseer datasets.
(a) Light
(b) Medium
(c) Heavy
Figure 6: Accuracy of GCN-DVD with different λ1 and λ2 on different biased Pubmed datasets.
15