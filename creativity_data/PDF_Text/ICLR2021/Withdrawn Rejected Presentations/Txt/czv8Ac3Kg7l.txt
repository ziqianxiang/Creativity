Under review as a conference paper at ICLR 2021.
Sparse Gaussian Process Variational Autoen-
CODERS
Anonymous authors
Paper under double-blind review
Ab stract
Large, multi-dimensional spatio-temporal datasets are omnipresent in modern sci-
ence and engineering. An effective framework for handling such data are Gaus-
sian process deep generative models (GP-DGMs), which employ GP priors over
the latent variables of DGMs. Existing approaches for performing inference in
GP-DGMs do not support sparse GP approximations based on inducing points,
which are essential for the computational efficiency of GPs, nor do they handle
missing data - a natural occurrence in many spatio-temporal datasets - in a Prin-
cipled manner. We address these shortcomings with the development of the sparse
Gaussian process variational autoencoder (SGP-VAE), characterised by the use of
partial inference networks for parameterising sparse GP approximations. Lever-
aging the benefits of amortised variational inference, the SGP-VAE enables infer-
ence in multi-output sparse GPs on previously unobserved data with no additional
training. The SGP-VAE is evaluated in a variety of experiments where it outper-
forms alternative approaches including multi-output GPs and structured VAEs.
1	Introduction
Increasing amounts of large, multi-dimensional datasets that exhibit strong spatio-temporal depen-
dencies are arising from a wealth of domains, including earth, social and environmental sciences
(Atluri et al., 2018). For example, consider modelling daily atmospheric measurements taken by
weather stations situated across the globe. Such data are (1) large in number; (2) subject to strong
spatio-temporal dependencies; (3) multi-dimensional; and (4) non-Gaussian with complex depen-
dencies across outputs. There exist two venerable approaches for handling these characteristics:
Gaussian process (GP) regression and deep generative models (DGMs). GPs provide a frame-
work for encoding high-level assumptions about latent processes, such as smoothness or periodicity,
making them effective in handling spatio-temporal dependencies. Yet, existing approaches do not
support the use of flexible likelihoods necessary for modelling complex multi-dimensional outputs.
In contrast, DGMs support the use of flexible likelihoods; however, they do not provide a natural
route through which spatio-temporal dependencies can be encoded. The amalgamation of GPs and
DGMs, GP-DGMs, use latent functions drawn independently from GPs, which are then passed
through a DGM at each input location. GP-DGMs combine the complementary strengths of both
approaches, making them naturally suited for modelling spatio-temporal datasets.
Intrinsic to the application of many spatio-temporal datasets is the notion of tasks. For instance:
medicine has individual patients; each trial in a scientific experiment produces an individual dataset;
and, in the case of a single large dataset, it is often convenient to split it into separate tasks to im-
prove computational efficiency. GP-DGMs support the presence of multiple tasks in a memory
efficient way through the use of amortisation, giving rise to the Gaussian process variational au-
toencoder (GP-VAE), a model that has recently gained considerable attention from the research
community (Pearce, 2020; Fortuin et al., 2020; Casale et al., 2018; Campbell & Lio, 2020; Ram-
chandran et al., 2020). However, previous work does not support sparse GP approximations based
on inducing points, a necessity for modelling even moderately sized datasets. Furthermore, many
spatio-temporal datasets contain an abundance of missing data: weather measurements are often
absent due to sensor failure, and in medicine only single measurements are taken at any instance.
Handling partial observations in a principled manner is essential for modelling spatio-temporal data,
but is yet to be considered.
1
Under review as a conference paper at ICLR 2021.
Our key technical contributions are as follows:
i)	We develop the sparse GP-VAE (S GP-VAE), which uses inference networks to parameterise
multi-output sparse GP approximations.
ii)	We employ a suite of partial inference networks for handling missing data in the SGP-VAE.
iii)	We conduct a rigorous evaluation of the SGP-VAE in a variety of experiments, demonstrat-
ing excellent performance relative to existing multi-output GPs and structured VAEs.
2	A Family of Spatio-Temporal Variational Autoencoders
Consider the multi-task regression problem in which we wish to model T datasets D = {D(t)}tT=1,
each of which comprises input/output pairs D(t) = {x(nt),y(nt)}nN=t 1, x(nt) ∈ RD and y(nt) ∈ RP. Fur-
ther, let any possible permutation of observed values be potentially missing, such that each observa-
tion y(nt) = yon(t) ∪ yun (t) contains a set of observed values yon(t) and unobserved values yun(t), with
On(t) denoting the index set of observed values. For each task, we model the distribution of each ob-
servation y(nt), conditioned on a corresponding latent variable f (nt) ∈ RK, as a fully-factorised Gaus-
sian distribution parameterised by passing f (nt) through a decoder deep neural network (DNN) with
parameters θ2. The elements of f (nt) correspond to the evaluation of a K-dimensional latent function
f(t) = (f1(t) , f2(t) , . . . , fK(t)) at input x(nt) . That is, f (nt) = f(t) (x(nt)). Each latent function f(t) is
modelled as being drawn from K independent GP priors with hyper-parameters θ1 = {θ1,k}kK=1,
giving rise to the complete probabilistic model:
K	Nt
f ㈤〜Y GP (0,kθ1,k (x, x0)),	y(t)|f⑴〜Y N (μθ2(fnt)), diag σV(f町	⑴
k = 1、	- {z	/	n =1 1	{z	}
pθ1 (fk )	pθ2 (yon (t) |f (t),x(nt),On(t))
where μ02 (f a))and σ02 2 (f 心))are the outputs of the decoder indexed by Ont). We shall refer to the
set θ = {θ1, θ2} as the model parameters, which are shared across tasks. The probabilistic model in
equation 1 explicitly accounts for dependencies between latent variables through the GP prior. The
motive of the latent structure is twofold: to discover a simpler representation of each observation,
and to capture the dependencies between observations at different input locations.
2.1	Motivation for Sparse Approximations and Amortised Inference
The use of amortised inference in DGMs and sparse approximations in GPs enables inference in
these respective models to scale to large quantities of data. To ensure the same for the GP-DGM
described in equation 1, we require the use of both techniques. In particular, amortised inference is
necessary to prevent the number of variational parameters scaling with O PtT=1 N (t) . Further,
the inference network can be used to condition on previously unobserved data without needing to
learn new variational parameters. Similarly, sparse approximations are necessary to prevent the
computational complexity increasing cubically with the size of each task O PtT=1 N(t)3 .
Unfortunately, it is far from straightforward to combine sparse approximations and amortised infer-
ence in a computationally efficient way. To see this, consider the standard form for the sparse GP
approximate posterior, q(f) = pθ1 (f\u|u)q(u) where q(u) = N (u; m, S) with m, S and Z, the
inducing point locations, being the variational parameters. q(u) does not decompose into a product
over N (t) factors and is therefore not amendable to per-datapoint amortisation. That is, m and S
must be optimised as free-form variational parameters. A naive approach to achieving per-datapoint
amortisation is to decompose q(u) into the prior pθ1 (u) multiplied by the product of approximate
likelihoods, one for each inducing point. Each approximate likelihood is itself equal to the product
of per-datapoint approximate likelihoods, which depend on both the observation yon and the distance
of the input xn to that of the inducing point. An inference network which takes these two values of
inputs can be used to obtain the parameters of the approximate likelihood factors. Whilst we found
that this approach worked, it is somewhat unprincipled. Moreover, it requires passing each data-
point/inducing point pair through an inference network, which scales very poorly. In the following
2
Under review as a conference paper at ICLR 2021.
section, we introduce a theoretically principled decomposition ofq(f) we term the sparse structured
approximate posterior which will enable efficient amortization.
2.2	The Sparse S tructured Approximate Posterior
By simultaneously leveraging amortised inference and sparse GP approximations, we can perform
efficient and scalable approximate inference. We specify the sparse structured approximate poste-
rior, q(f (t)), which approximates the intractable true posterior for task t:
1	Nt
Pθ (f (t) ∣y(t), X(t)) = Z1 Pθι (f(t)) Y Pθ2 (yn(t) If(t), χf),Ont))
p	n=1	(2)
1	Nt
≈ ZF Pθι(f(t))Y iφι(u;yn(t), Xn), Z) = q(f⑴)
Zq	n=1
Analogous to its presence in the true posterior, the approximate posterior retains the GP prior, yet
replaces each non-conjugate likelihood factor with an approximate likelihood, lφl (u; yon(t), X(nt), Z),
over a set of KM ‘inducing points’, u = ∪kK=1 ∪mM=1 umk, at ‘inducing locations’, Z = ∪kK=1 ∪mM=1
zmk. For tractability, we restrict the approximate likelihoods to be Gaussians factorised across each
latent dimension, parameterised by passing each observation through a partial inference network:
lφl (Uk ; ynQ xn ), Zk)= N (μφι,k (丫/")； kf (t)uk K-kuk Uk，σφι ,k(yn"'))	⑶
where φl denotes the weights and biases of the partial inference network, whose outputs are shown
in red. This form is motivated by the work of Bui et al. (2017), who demonstrate the optimality
of approximate likelihoods of the form N gn; kf(t) u Ku-1u uk, vn , a result we prove in Ap-
pendix A.1. Whilst, in general, the optimal free-formn values of gn and vn depend on all of the data
points, we make the simplifying assumption that they depend only on yon (t). For GP regression with
Gaussian noise, this assumption holds true as gn = yn and vn = σy2 (Bui et al., 2017).
The resulting approximate posterior can be interpreted as the exact posterior induced by a surrogate
regression problem, in which ‘pseudo-observations’ gn are produced from a linear transformation
of inducing points with additive 'pseudo-noise' vn, gn = k √t) KuIuU Uk + √vne. The inference
f u kk
nk k
network learns to construct this surrogate regression problem such that it results in a posterior that
is close to our target posterior.
By sharing variational parameters φ = {φl , Z} across tasks, inference is amortised across both
datapoints and tasks. The approximate posterior for a single task corresponds to the product of K
independent GPs, with mean and covariance functions
mkt)(x) = kf(t)uk 吸 Kuk f 产Sj μS,k
律)(x, χ0) = kfkt)fk (t) - kf(t)uk K-k1uk kukfk ⑸ +kfktuk 吸 kuk fk (t)
(4)
Where φkt) 1 = Kukuk + Kukfkt)球),k 1Kfkt)uk, [μφtι),kii = μφι,k(yo(t)) and [球[]ij =
δij σφ2 ,k(yio(t)). See Appendix A.2 for a complete derivation. The computational complexity asso-
ciated with evaluating the mean and covariance functions is O KM2N(t) , a significant improve-
ment over the O P3N(t)3 cost associated with exact multi-output GPs for KM2	P3N(t)2.
We refer to the combination of the aforementioned probabilistic model and sparse structured ap-
proximate posterior as the SGP-VAE.
The SGP-VAE addresses three major shortcomings of existing sparse GP frameworks. First, the
inference network can be used to condition on previously unobserved data without needing to learn
new variational parameters. Suppose we use the standard sparse GP variational approximation
q(f) = pθ1 (f\u|U)q(U) where q(U) = N (U; m, S). If more data are observed, m and S have to
be re-optimised. When an inference network is used to parameterise q(U), the approximate posterior
3
Under review as a conference paper at ICLR 2021.
is ‘automatically’ updated by mapping from the new observations to their corresponding approxi-
mate likelihood terms. Second, the complexity of the approximate posterior can be modified as
desired with no changes to the inference network, or additional training, necessary: any changes in
the morphology of inducing points corresponds to a deterministic transformation of the inference
network outputs. Third, if the inducing point locations are fixed, then the number of variational
parameters does not depend on the size of the dataset, even as more inducing points are added. This
contrasts with the standard approach, in which new variational parameters are appended to m and
S.
2.3	TRAINING THE SGP-VAE
Learning and inference in the SGP-VAE are concerned with determining the model parameters θ
and variational parameters φ. These objectives can be attained simultaneously by maximising the
evidence lower bound (ELB O), given by LELBO = PtT=1 L(EtL) BO where
LEtLBO = Eq(f(t)) pθ∖f(f)(t) = Eq(f(t))[logPθ(y(t)∣f(t))i - KL (产(U) k Pθι (U))	(5)
and q(t)(u) H pθι (u) QnN=1 lψl (u; Nnt, Xnt), Z). Fortunately, since both q(t)(u) and pθ1 (u) are
multivariate Gaussians, the final term, and its gradients, has an analytic solution. The first term
amounts to propagating a Gaussian through a non-linear DNN, so must be approximated using a
Monte Carlo estimate. We employ the reparameterisation trick (Kingma & Welling, 2014) to account
for the dependency of the sampling procedure on both θ and φ when estimating its gradients.
We mini-batch over tasks, such that only a single L(EtL) BO is computed per update. Importantly, in
combination with the inference network, this means that we avoid having to retain the O TM1 2
terms associated with T Cholesky factors if we were to use a free-form q(U) for each task. Instead,
the memory requirement is dominated by the O (KM2 + KNM + ∣φι∣) terms associated with
storing KukUk, Kukf⑴ and φι, as instantiating μφ,k and Σφt)k involves only O (KN) terms.1 This
corresponds to a considerable reduction in memory. See Appendix C for a thorough comparison of
memory requirements.
2.4	Partial Inference Networks
Partially observed data is regularly encountered in spatio-temporal datasets, making it necessary to
handle it in a principled manner. Missing data is naturally handled by Bayesian inference. However,
for models using inference networks, it necessitates special treatment. One approach to handling
partially observed data is to impute missing values with zeros (Nazabal et al., 2020; Fortuin et al.,
2020). Whilst simple to implement, zero imputation is theoretically unappealing as the inference
network can no longer distinguish between a missing value and a true zero.
Instead, we turn towards the ideas of Deep Sets (Zaheer et al., 2017). By coupling the observed
value with dimension index, we may reinterpret each partial observation as a permutation invariant
set. We define a family of permutation invariant partial inference networks2 as
Mφ(Yn), log σφ(Yn)) = Pφ2	E hφι (Snp)	⑹
p∈On
where hφ1 : R2 → RR and ρφ2 : RR → R2P are DNN mappings with parameters φ1 and φ2,
respectively. snp denotes the couples of observed value ynp and corresponding dimension index P.
The formulation in equation 6 is identical to the partial variational autoencoder (VAE) framework
established by Ma et al. (2019). There are a number of partial inference networks which conform to
this general framework, three of which include:
1This assumes input locations are shared across tasks, which is true for all experiments we considered.
2Whilst the formulation in equation 6 can describe any permutation invariant set function, there is a caveat:
both hφ1 and ρφ2 can be infinitely complex, hence linear complexity is not guaranteed.
4
Under review as a conference paper at ICLR 2021.
PointNet Inspired by the PointNet approach of Qi et al. (2017) and later developed by Ma et al.
(2019) for use in partial VAEs, the PointNet specification uses the concatenation of dimen-
sion index with observed value: snp = (p, ynp). This specification treats the dimension
indices as continuous variables. Thus, an implicit assumption of PointNet is the assumption
of smoothness between values of neighbouring dimensions. Although valid in a computer
vision application, it is ill-suited for tasks in which the indexing of dimensions is arbitrary.
IndexNet Alternatively, one may use the dimension index to select the first DNN mapping:
hφ1 (snp) = hφ1,p (ynp). Whereas PointNet treats dimension indices as points in space, this
specification retains their role as indices. We refer to it as the IndexNet specification.
FactorNet A special case of IndexNet, first proposed by Vedantam et al. (2017), uses a sepa-
rate inference network for each observation dimension. The approximate likelihood is fac-
torised into a product of Gaussians, one for each output dimension: lφl (uk; yon, xn, Zk) =
Qp∈On N (μΦl,Pk (ynp)； kfnkUk K-Ak Uk, σφι,pk (ynP)} wetermthis approach FaCtOrNet.
See Appendix G for corresponding computational graphs. Note that FactorNet is equivalent to
IndexNet with ρφ2 defined by the deterministiC transformations of natural parameters of Gaussian
distributions. SinCe IndexNet allows this transformation to be learnt, we might expeCt it to always
produCe a better partial inferenCe network for the task at hand. However, it is important to Consider
the ability of inferenCe networks to generalise. Although more Complex inferenCe networks will
better approximate the optimal non-amortised approximate posterior on training data, they may
produCe poor approximations to it on the held-out data.3 In partiCular, FaCtorNet is Constrained
to Consider the individual Contribution of eaCh observation dimension, whereas the others are not.
Doing so is neCessary for generalising to different quantities and patterns of missingness, henCe we
antiCipate FaCtorNet to perform better in suCh settings.
3	Related Work
we foCus our Comparison on approximate inferenCe teChniques; however, Appendix D presents a
unifying view of GP-DGMs.
Structured Variational Autoencoder Only reCently has the use of struCtured latent variable priors
in VAEs been Considered. In their seminal work, Johnson et al. (2016) investigate the Combination
of probabilistiC graphiCal models with neural networks to learn struCtured latent variable represen-
tations. The authors Consider a two stage iterative proCedure, whereby the optimum of a surrogate
objeCtive funCtion — Containing approximate likelihoods in plaCe of true likelihoods — is found
and substituted into the original ELBO. The resultant struCtured VAE (SVAE) objeCtive is then op-
timised. In the Case of fixed model parameters θ, the SVAE objeCtive is equivalent to optimising
the ELBO using the structured approximate posterior over latent variables q(z) 8 pθ(z)lφ(z∣y).
ACCordingly, the SGP-VAE Can be viewed as an instanCe of the SVAE. Lin et al. (2018) build upon
the SVAE, proposing a structured approximate posterior of the form q(z) 8 qφ(z)lφ⑵y). The
authors refer to the approximate posterior as the structured inference network (SIN). Rather than
using the latent priorpθ(z), SIN incorporates the model’s latent structure through qφ(z). The core
advantage of SIN is its extension to more complex latent priors containing non-conjugate factors —
qφ(z) can replace them with their nearest conjugate approximations whilst retaining a similar latent
structure. Although the frameworks proposed by Johnson et al. and Lin et al. are more general than
ours, the authors only consider Gaussian mixture model and linear dynamical system (LDS) latent
priors.
Gaussian Process Variational Autoencoders The earliest example of combining VAEs with GPs
is the GP prior VAE (GPPVAE) (Casale et al., 2018). There are significant differences between
our work and the GPPVAE, most notably in the GPPVAE’s use of a fully-factorised approximate
posterior — an approximation that is known to perform poorly in time-series and spatial settings
(Turner & Sahani, 2011). Closely related to the GPPVAE is Ramchandran et al.’s (2020) longitu-
dinal VAE, which also adopts a fully-factorised approximate posterior, yet uses additive covariance
functions for heterogeneous input data. Fortuin et al. (2020) consider the use of a Gaussian ap-
3This kind of ‘overfitting’ is different to the classical notion of overfitting model parameters. It refers to
how well optimal non-amortised approximate posteriors are approximated on the training versus test data.
5
Under review as a conference paper at ICLR 2021.
proximate posterior with a tridiagonal precision matrix Λ, q(f) = N f ; m, Λ-1 , where m and
Λ are parameterised by an inference network. Whilst this permits computational efficiency, the
parameterisation is only appropriate for regularly spaced temporal data and neglects rigorous treat-
ment of long term dependencies. Campbell & Lio (2020) employ an equivalent sparsely structured
variational posterior as that used by Fortuin et al., extending the framework to handle more general
spatio-temporal data. Their method is similarly restricted to regularly spaced spatio-temporal data.
A fundamental difference between our framework and that of Fortuin et al. and Campbell & Lio
is the inclusion of the GP prior in the approximate posterior. As shown by Opper & Archambeau
(2009), the structured approximate posterior is identical in form to the optimum Gaussian approxi-
mation to the true posterior. Most similar to ours is the approach of Pearce (2020), who considers
the structured approximate posterior q(f)=击pθ1 (f) QN=I lφl(fn； yn). We refer to this as the
GP-VAE. Pearce’s approach is a special case of the SGP-VAE for u = f and no missing data.
Moreover, Pearce only considers the application to modelling pixel dynamics and the comparison to
the standard VAE. See Appendix B for further details.
4	Experiments
We investigate the performance of the SGP-VAE in illustrative bouncing ball experiments, followed
by experiments in the small and large data regimes. The first bouncing ball experiment provides a
visualisation of the mechanics of the SGP-VAE, and a quantitative comparison to other structured
VAEs. The proceeding small-scale experiments demonstrate the utility of the GP-VAE and show
that amortisation, especially in the presence of partially observed data, is not at the expense of
predictive performance. In the final two experiments, we showcase the efficacy of the SGP-VAE
on large, multi-output spatio-temporal datasets for which the use of amortisation is necessary. Full
experimental details are provided in Appendix E.
4.1	Synthetic Bouncing Ball Experiment
The bouncing ball experiment — first introduced by Johnson et al. (2016) for evaluating the SVAE
and later considered by Lin et al. (2018) for evaluating SIN — considers a sequence of one-
dimensional images of height 10 representing a ball bouncing under linear dynamics, (x(nt) ∈ R1 ,
y(nt) ∈ R10). The GP-VAE is able to significantly outperform both the SVAE and SIN in the origi-
nal experiment, as shown in Figure 1a. To showcase the versatility of the SGP-VAE, we extend the
complexity of the original experiment to consider a sequence of images of height 100, y(nt) ∈ R100 ,
representing two bouncing balls: one under linear dynamics and another under gravity. Furthermore,
the images are corrupted by removing 25% of the pixels at random. The dataset consists of T = 80
noisy image sequences, each of length N = 500, with the goal being to predict the trajectory of the
ball given a prefix of a longer sequence.
(a)
Figure 1: a) Comparing the GP-VAE’s predictive performance to that of the SVAE, SIN, an LDS
and independent GPs (IGP). b) Top: sequence of images representing two bouncing balls. Middle:
mean of the SGP-VAE’s predictive distribution conditioned on partial observations up to the red
line. Bottom: latent approximate GPs posterior, alongside the inducing point locations (crosses).
(b)
Using a two-dimensional latent space with periodic kernels, Figure 1b compares the posterior latent
GPs and the mean predictive distribution with the ground truth for a single image sequence. Observe
that the SGP-VAE has ‘disentangled’ the dynamics of each ball, using a single latent dimension to
6
Under review as a conference paper at ICLR 2021.
model each. The SGP-VAE reproduces the image sequences with impressive precision, owing in
equal measure to (1) the ability of the GPs prior to model the latent dynamics and (2) the flexibility
of the likelihood function to map to the high-dimensional observations.
4.2	Small-Scale Experiments
EEG Adopting the experimental procedure laid out by Requeima et al. (2019), we consider an EEG
dataset consisting of N = 256 measurements taken over a one second period. Each measurement
comprises voltage readings taken by seven electrodes, FZ and F1-F6, positioned on the patient’s
scalp (xn ∈ R1, yn ∈ R7). The goal is to predict the final 100 samples for electrodes FZ, F1 and
F2 having observed the first 156 samples, as well as all 256 samples for electrodes F3-F6.
Jura The Jura dataset is a geospatial dataset comprised of N = 359 measurements of the topsoil
concentrations of three heavy metals — Cadmium Nickel and Zinc — collected from a 14.5km2
region of the Swiss Jura (xn ∈ R2, yn ∈ R3) (Goovaerts, 1997). Adopting the experimental
procedure laid out by others (Goovaerts, 1997; Alvarez & Lawrence, 2011; Requeima et al., 2019),
the dataset is divided into a training set consisting of Nickel and Zinc measurements for all 359
locations and Cadmium measurements for just 259 locations. Conditioned on the observed training
set, the goal is to predict the Cadmium measurements at the remaining 100 locations.
Table 1: A comparison between multi-output GP models on the EEG and Jura experiments.
	Metric	IGP,	GPAR,	GP-VAE				
				ZI	PointNet	IndexNet	FactorNet	FF
EEG	SMSE	1.75	0.26	0.27 (0.03)	0.60 (0.09)	0.24 (0.02)	0.28 (0.04)	0.25 (0.02)
	NLL	2.60	1.63	2.24 (0.37)	3.03 (1.34)	2.01 (0.28)	2.23 (0.21)	2.13 (0.28)
Jura	MAE	0.57	0.41	0.42 (0.01)	0.45 (0.02)	0.44 (0.02)	0.40 (0.01)	0.41 (0.01)
	NLL	-	-	1.13 (0.09)	1.13 (0.08)	1.12 (0.08)	1.00 (0.06)	1.04 (0.06)
,Results taken directly from Requeima et al. (2019).
Table 1 compares the performance of the GP-VAE using the three partial inference networks pre-
sented in Section 2.4, as well as zero imputation (ZI), with independent GPs (IGP) and the GP
autoregressive regression model (GPAR), which, to our knowledge, has the strongest published per-
formance on these datasets. We also give the results for the best performing GP-VAE4 using a
non-amortised, or ‘free-form’ (FF), approximate posterior, with model parameters θ kept fixed to
the optimum found by the amortised GP-VAE and variational parameters initialised to the output
of the optimised inference network. All GP-VAE models use a two- and three-dimensional latent
space for EEG and Jura, respectively, with squared exponential (SE) kernels. The results highlight
the poor performance of independent GPs relative to multi-output GPs, demonstrating the impor-
tance of modelling output dependencies. The GP-VAE achieves impressive SMSE and MAE5 on
the EEG and Jura datasets using all partial inference networks except for PointNet. In Appendix F
we demonstrate superior performance of the GP-VAE relative to the GPPVAE, which can be at-
tributed to the use of the structured approximate posterior over the mean-field approximate posterior
used by the GPPVAE. Importantly, the negligible difference between the results using free-form
and amortised approximate posteriors indicates that amortisation is not at the expense of predictive
performance.
Whilst GPAR performs as strongly as the GP-VAE in the small-scale experiments above, it has two
key limitations which severely limit the types of applications where it can be used. First, it can only
be used with specific patterns of missing data and not when the pattern of missingness is arbitrary.
Second, it is not scalable and would require further development to handle the large datasets con-
sidered in this paper. In contrast, the SGP-VAE is far more flexible: it handles arbitrary patterns of
missingness, and scales to large number of datapoints and tasks. A distinct advantage of the SGP-
VAE is that it models P outputs with just K latent GPs. This differs from GPAR, which uses P
4i.e. using IndexNet for EEG and FactorNet for Jura.
5The two different performance metrics are adopted to enable a comparison to the results of Requeima et al..
7
Under review as a conference paper at ICLR 2021.
GPs. Whilst this is not an issue for the small-scale experiments, it quickly becomes computation-
ally burdensome when P becomes large. The true efficacy of the SGP-VAE is demonstrated in the
following three experiments, where the number of datapoints and tasks is large, and the patterns of
missingness are random.
4.3	Large-Scale EEG Experiment
We consider an alternative setting to the original small-scale EEG experiment, in which the datasets
are formed from T = 60 recordings of length N = 256, each with 64 observed voltage readings
(yn ∈ R64). For each recording, we simulated electrode ‘blackouts’ by removing consecutive sam-
ples at random. We consider two experiments: in the first, we remove equal 50% of data from both
the training and test datasets; in the second, we remove 10% of data from the training dataset and
50% from the test dataset. Both experiments require the partial inference network to generalise to
different patterns of missingness, with the latter also requiring generalisation to different quantities
of missingness. Each model is trained on 30 recordings, with the predictive performance assessed
on the remaining 30 recordings. Figure 2 compares the performance of the SGP-VAE with that
of independent GPs as the number of inducing points varies, with M = 256 representing use of
the GP-VAE. In each case, we use a 10-dimensional latent space with SE kernels. The SGP-VAE
using PointNet results in substantially worse performance than the other partial inference networks,
achieving an average SMSE and NLL of 1.30 and 4.05 on the first experiment for M = 256. Sim-
ilarly, using a standard VAE results in poor performance, achieving an average SMSE and NLL of
1.62 and 3.48. These results are excluded from Figure 2 for the sake of readability.
Figure 2: Variation in performance of the SGP-VAE on the large-scale EEG experiment as the
number of inducing points, M, varies.
For all partial inference networks, the SGP-VAE achieves a significantly better SMSE than inde-
pendent GPs in both experiments, owing to its ability to model both input and output dependencies.
For the first experiment, the performance using FactorNet is noticeably better than using either In-
dexNet or zero imputation; however, this comes at the cost of a greater computational complexity
associated with learning an inference network for each output dimension. Whereas the performance
for the SGP-VAE using IndexNet and zero imputation significantly worsens on the second exper-
iment, the performance using FactorNet is comparable to the first experiment. This suggests it is
the only partial inference network that is able to accurately quantify the contribution of each output
dimension to the latent posterior, enabling it to generalise to different quantities of missing data.
The advantages of using a sparse approximation are clear — using M = 128 inducing points results
in a slightly worse average SMSE and NLL, yet significantly less computational cost.
4.4	Japanese Weather Experiment
Finally, we consider a dataset comprised of 731 daily climate reports from 156 Japanese weather
stations throughout 1980 and 1981, a total of 114,036 multi-dimensional observations. Weather
reports consist of a date and location, including elevation, alongside the day’s maximum, minimum
and average temperature, precipitation and snow depth (x(nt) ∈ R4 , y(nt) ∈ R5), any number of
which is potentially missing. We treat each week as a single task, resulting in T = 105 tasks with
8
Under review as a conference paper at ICLR 2021.
Figure 3: An illustration of the Japanese weather experiment. The dotted red lines highlight the
missing data, with the SGP-VAE’s predictive mean shown below.
N = 1092 data points each. The goal is to predict the average temperature for all stations on the
middle five days, as illustrated in Figure 3. Each model is trained on all the data available from
1980. For evaluation, we use data from both 1980 and 1981 with additional artificial missingness
— the average temperature for the middle five days and a random 25% of minimum and maximum
temperature measurements6 . Similar to the second large-scale EEG experiment, the test datasets
have more missing data than the training datasets. Table 2 compares the performance of the SGP-
VAE using 100 inducing points to that of a standard VAE using FactorNet and a baseline of mean
imputation. All models use a three-dimensional latent space with SE kernels.
Table 2: A comparison between model performance on the Japanese weather experiment.
	Metric	MI	IGP	VAE	SGP-VAE			
					ZI	PointNet	IndexNet	FactorNet
1980	RMSE	9.16	2.49 (0.00)	2.31 (0.35)	3.21 (0.35)	3.53 (0.13)	2.59 (0.11)	1.60 (0.08)
	NLL	-	2.84 (0.00)	2.68 (0.15)	2.68 (0.15)	2.80 (0.05)	2.44 (0.04)	2.18 (0.02)
1981	RMSE	9.27	2.41 (0.00)	3.20 (0.37)	3.20 (0.37)	3.61 (0.14)	2.55 (0.11)	1.68 (0.09)
	NLL	-	2.73 (0.00)	2.67 (0.15)	2.67 (0.15)	2.83 (0.06)	2.43 (0.04)	2.20 (0.02)
All models significantly outperform the mean imputation baseline (MI) and are able to generalise
inference to the unseen 1981 dataset without any loss in predictive performance. The SGP-VAE
achieves better predictive performance than both the standard VAE and independent GPs, showcas-
ing its effectiveness in modelling large spatio-temporal datasets. The SGP-VAE using FactorNet
achieves the best predictive performance on both datasets. The results indicate that FactorNet is
the only partial inference network capable of generalising to different quantities and patterns of
missingness, supporting the hypothesis made in Section 2.4.
5	Conclusion
The SGP-VAE is a scalable approach to training GP-DGMs which combines sparse inducing point
methods for GPs and amortisation for D GMs. The approach is ideally suited to spatio-temporal
data with missing observations, where it outperforms VAEs and multi-output GPs. Future research
directions include generalising the framework to leverage state-space GP formulations for additional
scalability and applications to streaming multi-output data.
References
MauriCio A AlVarez and Neil D Lawrence. Computationally efficient convolved multiple output
Gaussian processes. The Journal of Machine Learning Research, 12:1459-1500, 2011.
6This prevents the model from using solely output dependencies to impute the missing temperature readings.
9
Under review as a conference paper at ICLR 2021.
Gowtham Atluri, Anuj Karpatne, and Vipin Kumar. Spatio-temporal data mining: A survey of
problems and methods. ACM Computing Surveys (CSUR), 51(4):1-41, 2018.
Edwin V Bonilla, Kian M Chai, and Christopher Williams. Multi-task Gaussian process prediction.
In Advances in Neural Information Processing Systems, pp. 153-160, 2008.
Thang D Bui, Josiah Yan, and Richard E Turner. A unifying framework for Gaussian process
pseudo-point approximations using power expectation propagation. The Journal of Machine
Learning Research, 18(1):3649-3720, 2017.
Alex Campbell and Pietro Lio. tvGP-VAE: Tensor-variate Gaussian process prior variational au-
toencoder. arXiv preprint arXiv:2006.04788, 2020.
Francesco P Casale, Adrian Dalca, Luca Saglietti, Jennifer Listgarten, and Nicolo Fusi. Gaussian
process prior variational autoencoders. In Advances in Neural Information Processing Systems,
pp. 10369-10380, 2018.
Andreas Damianou and Neil D Lawrence. Deep Gaussian processes. In Artificial Intelligence and
Statistics, pp. 207-215, 2013.
Vincent Fortuin, Dmitry Baranchuk, Gunnar Ratsch, and Stephan Mandt. GP-VAE: Deep ProbabiliS-
tic time series imputation. In International Conference on Artificial Intelligence and Statistics,
pp. 1651-1661, 2020.
Pierre Goovaerts. Geostatistics for natural resources evaluation. Oxford University Press on De-
mand, 1997.
Matthew J Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta.
Composing graphical models with neural networks for structured representations and fast infer-
ence. In Advances in Neural Information Processing Systems, pp. 2946-2954, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International
Conference on Learning Representations, 12 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. In 2nd International
Conference on Learning Representations, 2014.
Neil D Lawrence. Gaussian process latent variable models for visualisation of high dimensional
data. In Advances in Neural Information Processing Systems, pp. 329-336, 2004.
Neil D Lawrence and Andrew J Moore. Hierarchical Gaussian process latent variable models. In
Proceedings of the 24th International Conference on Machine Learning, pp. 481-488, New York,
NY, USA, 2007. Association for Computing Machinery.
Wu Lin, Nicolas Hubacher, and Mohammad Emtiyaz Khan. Variational message passing with struc-
tured inference networks. In International Conference on Learning Representations, 2018.
Chao Ma, Sebastian Tschiatschek, Konstantina Palla, Jose Miguel Hernandez-Lobato, Sebastian
Nowozin, and Cheng Zhang. EDDI: efficient dynamic discovery of high-value information with
partial VAE. In Proceedings of the 36th International Conference on Machine Learning, vol-
ume 97, pp. 4234-4243, 2019.
Alfredo Nazabal, Pablo M Olmos, Zoubin Ghahramani, and Isabel Valera. Handling incomplete
heterogeneous data using VAEs. Pattern Recognition, pp. 107501, 2020.
Manfred Opper and CedriC Archambeau. The variational Gaussian approximation revisited. Neural
computation, 21(3):786-792, 2009.
Michael Pearce. The Gaussian process prior VAE for interpretable latent dynamics from pixels. In
Symposium on Advances in Approximate Bayesian Inference, pp. 1-12, 2020.
Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. PointNet: Deep learning on point
sets for 3D classification and segmentation. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pp. 652-660, 2017.
10
Under review as a conference paper at ICLR 2021.
Siddharth Ramchandran, Gleb Tikhonov, Miika Koskinen, and Harri Lahdesmaki. Longitudinal
variational autoencoder. arXiv preprint arXiv:2006.09763, 2020.
James Requeima, William Tebbutt, Wessel Bruinsma, and Richard E Turner. The Gaussian process
autoregressive regression model (GPAR). In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 1860-1869, 2019.
Yee Whye Teh, Matthias W Seeger, and Michael I Jordan. Semiparametric latent factor models. In
Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics, 2005.
Michael E Tipping and Christopher M Bishop. Probabilistic principal component analysis. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611-622, 1999.
Michalis Titsias. Variational learning of inducing variables in sparse Gaussian processes. In Artificial
Intelligence and Statistics, pp. 567-574, 2009.
Rich E Turner and Maneesh Sahani. Two problems with variational expectation maximisation for
time-series models. In D. Barber, T. Cemgil, and S. Chiappa (eds.), Bayesian Time series models,
chapter 5, pp. 109-130. Cambridge University Press, 2011.
Ramakrishna Vedantam, Ian Fischer, Jonathan Huang, and Kevin Murphy. Generative models of
visually grounded imagination. arXiv preprint arXiv:1705.10762, 2017.
Byron M Yu, John P Cunningham, Gopal Santhanam, Stephen I Ryu, Krishna V Shenoy, and Ma-
neesh Sahani. Gaussian-process factor analysis for low-dimensional single-trial analysis of neural
population activity. In Advances in Neural Information Processing Systems, pp. 1881-1888, 2009.
Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and
Alexander J Smola. Deep sets. In Advances in Neural Information Processing Systems, pp.
3391-3401, 2017.
A Mathematical Derivations
A. 1 Optimality of Approximate Likelihoods
To simplify notation, we shall consider the case P = 1 and K = 1. Separately, Opper & Archam-
beau (2009) considered the problem of performing variational inference in a GP for non-Gaussian
likelihoods. They consider a multivariate Gaussian approximate posterior, demonstrating that the
optimal approximate posterior takes the form
1N
q(f) = Z P(f) ∏N (fn； gn, Vn) ,	(7)
n=1
requiring a total of 2N variational parameters ({gn, vn}nN=1).
In this section, we derive a result that generalises this to inducing point approximations, showing that
for fixed M the optimal approximate posterior can be represented by max(M (M + 1)/2+M, 2N).
Following Titsias (2009), we consider an approximate posterior of the form
q(f) = q(u)p(f\u|u)	(8)
where q(u) = N (u; Imu, KKUu) is constrained to be a multivariate Gaussian with mean mU and
covariance R uu. The ELBOiS given by
LELBO = Eq(f) [log p(y|f)] - KL (q(u) k p(u))
= Eq(u) Ep(f |u) [log p(y|f)] - KL (q(u) k p(u))
N	(9)
= X Eq(u) EN(fn; Anu+an, Kfn|u) [logp(yn|fn] - KL (q(u) k p(u))
n=1
11
Under review as a conference paper at ICLR 2021.
where
An = Kfn u Ku-u1	(10)
an = mfn — KfnUK-Um U.	(11)
Recall that for a twice-differentiable scalar fUnction h	
v∑eN(u; μ, ∑) [h(U)] = EN(u; μ, ∑) [Hh(U)]	(12)
where Hh(U) is the Hessian of h at u. Thus, the gradient of the ELBO with respect to KUU	can be
rewritten as	
N	11 VKUuLELBO = 7 E EN(u; mu, Kuu) [Hhn(U)I - 2Kuu + 2Kuu n=1	(13)
where hn(U) = EN(fn; Anu+an, Kfn|u) [logp(yn|fn].	
To determine an expression for Hhn, we first consider the gradients of hn. Let	
αn(βn) = EN(fn; βn, Kfn|u) [log p(yn|fn)]	(14)
βn(U) = AnU + an.	(15)
The partial derivative of hn with respect to the jth element of U can be expressed as	
∂hn	∂αn	∂βn ∂Uj (U) =西(en(U)) ∂Uj (U).	(16)
Taking derivatives with respect to the ith element of U gives	
∂2 hn	∂2αn	∂βn	∂βn	∂αn	∂2βn ∂uj∂ui(U) = -∂βn- (en(U)) ∂uj(U)西(U) + 西(en(U)) ∂uj∂ui(U).	(17)
Thus, the Hessian is given by	
∂2α	∂α Hhn (U)= S (βn(U)) Ven(U)Vβn(U)]T + 舟(βnC∏)) Hen (u). ∂β	∂ βs	1	_}	(18)
V-ZLn	{	{z	nn,	{z |			} N ×1	1×N	|	{	' N ×N	
(19)
(20)
RR
Since βn(u) = AnU + an, We have Vβn(u) = An and Hen(U) = 0. This allows Us to write
VKUu LELBO as
N	∂2α	1	1
VKUuLELBO =	X EN(u; mu, Kuu)	∂β2	(βn(u))	AnAn- 2Kuu +	2Kuu.
n=1	n
The optimal covariance therefore satisfies
1	N	∂2α
KUU= K-U - 2 X EN(u; mu, Kuu) ∂β2 (βn(u)) AnAn.
n=1	n
Similarly, the gradient of the ELBO with respect to ι^U can be written as
N
VmuLELBO = ^X VmuEN(u； mu, Kuu) [hn(U)] - K-U(ImU - mu)
n=1
N
=X EN(u; mu, Kuu) [Vhn(u)] - K-U(mU - mu)
n=1
where we have Used the fact that for a differentiable scalar fUnction h
V“EN(u; μ, Σ) [g(u)] = EN(u; μ, Σ) [Vg(u)].
(21)
(22)
12
Under review as a conference paper at ICLR 2021.
Using equation 16 and βn(u) = Anu + an, we get
∂α
Vhn(U) = d-n (βn(u))An
∂βn
giving
N
VmuLELBO = E EN(u; mu, Kuu
n=1
The optimal mean is therefore
∂α
J忒 (β-(U))
N
mU = mu - X EN(u; mu, KUu
n=1
一K-U (m U — mu).
)τT3n (3(U)) KuuAn.
∂βn
(23)
(24)
(25)
Equation 20 and equation 25 show that each nth observation contributes only a rank-1 term to the
optimal approximate posterior precision matrix, corresponding to an optimum approximate posterior
of the form
N
q(f) <X P(f) Y N (KfnUK-u* gn, Vn)	(26)
n=1
where
gn = -EN(u； mu, Kuu) ∂β~(βn(U)) VnKUU KUU + Anmu	(27)
∂2α
1/vn = -2EN(u； mu, Kuu) ∂β2 (βn(U)) .	(28)
For general likelihoods, these expressions cannot be solved exactly so gn and Vn are freely optimised
as variational parameters. When N = M , the inducing points are located at the observations and
An ATn is zero everywhere except for the nth element of its diagonal we recover the result of Opper
& Archambeau (2009). Note the key role of the linearity of each βn in this result - without it Hβn
would not necessarily be zero everywhere and the contribution of each nth term could have arbitrary
rank.
A.2 Posterior Gaussian Process
For the sake of notational convenience, we shall assume K = 1. First, the mean and covariance of
q(u) = N (u; mu, KUU) Y Pθι (u) Qn= 1 lφι (u; yn, Xn, Z) are given by
m U = KUUφKuf Cφl1k μφι
K UU = KuuΦKuu
(29)
where Φ-1 = KUU + KUf ∑-1Kfu. The approximate posterior over some latent function value f
is obtained by marginalisation of the joint distribution:
q(f*) = /Pθι (f*∣u)q(u)du
=/N (f*; kf*uK-Uu, kf*f* - kf*UKUUkuf*) N (u; mU, Kuu) du
=N (f*; kf*uKUUmu, kf*f* - kf*uKUUkUf* + kf*uKUUKUUKUUkuf*
Substituting in equation 29 results in a mean and covariance function of the form
m(x) = kf U KUU ΦKuf ∑Uι1k μφι
01
k(x, x ) = kff0 - kfUKUUkUf0 + kfUΦkUf0 .
(30)
(31)
13
Under review as a conference paper at ICLR 2021.
B THE GP-VAE
As discuss in Section 3, the GP-VAE is described by the structured approximate posterior
1N
q(f ) = zqwφ) pθ1 (f) Yιlφl(f n; yn),
(32)
where lφι (f n； y) = QK=I N f n； Nφl Hrn), diag σφl (yn)), and corresponding ELBO
LELBO = Eq(f)
log	Pθι (f )pe2(y|f)
_	z⅛ pθi (f )lΦι(f； y)
Eq(f) [log Pθ2τyf ] + log Zq (θ,φ).
lφl (f； y)
(33)
B.1 Training the GP-VAE
The final term in equation 33 has the closed-form expression
KK
Zq(θ,Φ) = ∏ ElOgN (μφι,k, 0, Kfkfk
k = 1 k=1'--------------------{z-----
+ Σφl,k .
(34)
log Zqk (θ,φ)
which can be derived by noting that each Zqk (θ, φ) corresponds to the convolution between two
multivariate Gaussians:
Zqk(θ,φ) = /n(fk; 0, Kfkfk)N("φι,k-fk； 0, Σφι,k) dfk.
Similarly, a closed-form expression for Eq(f) [lφl (f； y)] exists:
(35)
KN
Eq(f) [log lφι (f ； y)]= XXEq(fnk) [log lΦl (fnk； yn)]
k=1 n=1
KN
XXE
k=1 n=1
KN
XX-
k=1 n=1
KN
q(fnk)
(fnk2-r y (yn))2 - 2log ∣2∏σφ1,k (yn)∣
I' k in n + (μk，n - μΦl,k (yn))2
2σ2ι,k (yn)
- 1log l2πσ2ι,k (yn)|
ΣΣlogN (μk,n μφι,k(yn), σ2l,k(yn))
[ς k]
nn
k=1 n=1
K
2σφι,k (yn)
ElOgN (瓦；μφι,k, ςΦi,G - E
[ς k]
nn
k=1
n=1
2σ2ι,k (yn
(36)
—
—
N
where Σk =嬴(X, X0) and μk = rnk(X), With
m k (X) = kfk Uk (Kukuk + 'φι,k )	μφι,k
-1
kk(X) = kfkfk0 - kfkuk (Kuk uk + Σφl,k)	kuk fk .
(37)
Eq(f) [log pθ2 (y|f)] is intractable, hence must be approximated by a Monte Carlo estimate. Together
with the closed-form expressions for the other two terms we can form an unbiased estimate of
the ELBO, the gradients of which can be estimated using the reparameterisation trick (Kingma &
Welling, 2014).
14
Under review as a conference paper at ICLR 2021.
B.2 An Alternative Sparse Approximation
An alternative approach to introducing a sparse GP approximation is directly parameterise the struc-
tured approximate posterior at inducing points u:
1N
q(f) = Z (θ φ) Pθι (J) ∏ lφl (U； yn, xn, Z)
(38)
where lφl (u; yon, xn, Z), the approximate likelihood, is a fully-factorised Gaussian distribution pa-
rameterised by a partial inference network:
KM
lφl (u; yon , xn , Z) =ππN (Umk ; μφ,k (yn), σφ,k (yn)) .
(39)
k=1 m=1
In general, each factor lφl (umk; yon, zmk, xn) conditions on data at locations different to that of
the inducing point. The strength of the dependence between these values is determined by the two
input locations themselves. To account for this, we introduce the use of an inference network that,
for each observation/inducing point pair (umk, yn), maps from (zmk, xn, yon) to parameters of the
approximate likelihood factor.
Whilst this approach has the same first order computational complexity as that used by the SGP-
VAE, having to making forward and backward passes through the inference network KNM renders
it significantly more computationally expensive for even moderately sized datasets. Whereas the
approach adopted by the SGP-VAE employs an deterministic transformation of the outputs of the
inference network based on the covariance function, this approach can be interpreted as learning an
appropriate dependency between input locations. In practice, we found the use of this approach to
result in worse predictive performance.
C Memory Requirements
Assuming input locations and inducing point locations are shared across tasks, we require
storing {K f⑴ + Kukuk}K=ι and Kf(t)f⑴ in memory, which is O (KMN + KM2 + N2).
ukfk	fk fk
For the SGP-VAE, We also require storing φ and instantiating {μφt)k, ∑Φt)j3ι, which
is O (∣φι∣ + KMD + 2KN). Collectively, this results in the memory requirement
O (KNM + KM2 + N2 + ∣φι∣ + KMD + 2KN).
If we were to employ the same sparse structured approximate posterior, but replace the
output of the inference network with free-form variational parameters, the memory require-
ment is O (KNM + KM2 + N2 + KMD + 2TKN)7 Alternatively, if we were to let q(u)
to be parameterised by free-form Cholesky factors and means, the memory requirement is
O (KNM + KM2 + N2 + KMD + TKM(M + 1)/2 + TKM). Table 3 compares the first or-
der approximations. Importantly, the use of amortisation across tasks stops the memory scaling with
the number of tasks.
Table 3: A comparison between the memory requirements of approximate posteriors.
q(u)	Amortised?	Memory requirement
p(u) Qn ln (u)	Yes	O (KNM + KM2 + N2 + ∣φι∣)
p(u) Qn ln (u)	No	O (KNM + KM2 + N2 + TKN)
q(u)	No	O (KNM + TKM2)
D Multi- Output Gaussian Processes
Through consideration of the interchange of input dependencies and likelihood functions, we can
shed light on the relationship between the probabilistic model employed by the SGP-VAE and other
multi-output GP models. These relationships are summarised in Figure 4.
7 Note we only require evaluating a single K (t) (t) at each update.
fk fk
15
Under review as a conference paper at ICLR 2021.
DGP
GP-FA
SGP-VAE
fk 〜GP(0, k(χ, χ0))
y |f n 〜N(μ(f n ), Σ(f n))
Figure 4: A unifying perspective on multi-output GPs.
fn 〜N(0, I)
Vn |f n 〜N("(f n ), ∑(f n))
Linear Multi-Output Gaussian Processes Replacing the likelihood with a linear likelihood func-
tion characterises a family of linear multi-output GPs, defined by a linear transformation of K inde-
pendent latent GPs:
K
f 〜Y GP (0,kθ1,k (x, x0))
k=1
N
y|f 〜Y N Bn； Wfn, ∑).
(40)
n=1
The family includes Teh et al.’s (2005) semiparametric latent factor model, Yu et al.’s (2009) GP
factor analysis (GP-FA) and Bonilla et al.’s (2008) class of multi-task GPs. Notably, removing
input dependencies by choosing kθ1,k (x, x0) = δ(x, x0) recovers factor analysis, or equivalently,
probabilistic principal component analysis (Tipping & Bishop, 1999) when Σ = σ2 I. Akin to the
relationship between factor analysis and linear multi-output GPs, the probabilistic model employed
by standard VAEs can be viewed as a special, instantaneous case of the SGP-VAE’s.
Deep Gaussian Processes Single hidden layer deep GPs (DGPs) (Damianou & Lawrence, 2013)
are characterised by the use of a GP likelihood function, giving rise to the probabilistic model
K
f 〜Y GP (0,kθ1,k (x, x0))
k=1
P
y|f 〜Y GP (0,kθ2,p(f(x)f(x0)))
(41)
p=1
where yn = y(xn). The GP latent variable model (GP-LVM) (Lawrence & Moore, 2007) is the
special, instantaneous case of single layered DGPs. Multi-layered DGPs are recovered using a
hierarchical latent space with conditional GP priors between each layer.
E Experimental Details
Whilst the theory outlined in Section 2 describes a general decoder parameterising both the mean
and variance of the likelihood, we experienced difficulty training SGP-VAEs using a learnt variance,
especially for high-dimensional observations. Thus, for the experiments detailed in this paper we
use a shared variance across all observations. We use the Adam optimiser (Kingma & Ba, 2014)
with a constant learning rate of 0.001. Unless stated otherwise, we estimate the gradients of the
16
Under review as a conference paper at ICLR 2021.
ELBO using a single sample and the ELBO itself using 100 samples. The predictive distributions
are approximated as Gaussian with means and variances estimated by propagating samples from
q(f) through the decoder. For each experiment, we normalise the observations using the means and
standard deviations of the data in the training set.
The computational complexity of performing variational inference (VI) in the full GP-VAE, per
update, is dominated by the O KN3 cost associated with inverting the set of K N × N matrices,
{Kfkfk + Σφl,k}kK=1. This can quickly become burdensome for even moderately sized datasets. A
pragmatic workaround is to use a biased estimate of the ELBO using N < N data points:
LN _ N
LELBO = N
~
qf)
^ ， .≈.'
log f
~ . .
+ log Zq (θ,φ)
(42)
E
~ f`f , ,ι	∙ ∙ i , i r∙ τCr ι	, ∙	ι,ι	τ ι ,	,	∙ ι ι
y and f denote the mini-batch of N observations and their corresponding latent variables, re-
spectively. The bias is introduced due to the normalisation constant, which does not satisfy
NE [log Zq (θ, φ)] = E [log Zq(θ, φ)]. Nevertheless, the mini-batch estimator will be a reason-
able approximation to the full estimator provided the lengthscale of the GP prior is not too large.8
Mini-batching cannot be used to reduce the O KN3 cost of performing inference at test time,
hence sparse approximations are necessary for large datasets.
E.1	Small-Scale EEG
For all GP-VAE models, we use a three-dimensional latent space, each using squared exponential
(SE) kernels with lengthscales and scales initialised to 0.1 and 1, respectively. All DNNs, except for
those in PointNet and IndexNet, use two hidden layers of 20 units and ReLU activation functions.
PointNet and IndexNet employ DNNs with a single hidden layer of 20 units and a 20-dimensional
intermediate representation. Each model is trained for 3000 epochs using a batch size of 100, with
the procedure repeated 15 times. Following (Requeima et al., 2019), the performance of each model
is evaluated using the standardised mean squared error (SMSE) and negative log-likelihood (NLL).
The mean ± standard deviation of the performance metrics for the 10 iterations with the highest
ELBO is reported.9
E.2 JURA
We use a two-dimensional latent space for all GP-VAE models with SE kernels with lengthscales
and scales initialised to 1. This permits a fair comparison with other multi-output GP methods which
also use two latent dimensions with SE kernels. For all DNNs except for those in IndexNet, we use
two hidden layers of 20 units and ReLU activation functions. IndexNet uses DNNs with a single
hidden layer of 20 units and a 20-dimensional intermediate representation. Following Goovaerts
(1997) and Lawrence (2004), the performance of each model is evaluated using the mean absolute
error (MAE) averaged across 10 different initialisations. The 10 different initialisations are identified
from a body of 15 as those with the highest training set ELBO. For each initialisation the GP-VAE
models are trained for 3000 epochs using a batch size of 100.
E.3 LARGE-SCALE EEG
In both experiments, for each trial in the test set we simulate simultaneous electrode ‘blackouts’ by
removing any 4 sample period at random with 25% probability. Additionally, we simulate individual
electrode ‘blackouts’ by removing any 16 sample period from at random with 50% probability from
the training set. For the first experiment, we also remove any 16 sample period at random with
50% probability from the test set. For the second experiment, we remove any 16 sample period at
random with 10% probability. All models are trained for 100 epochs, with the procedure repeated
five times, and use a 10-dimensional latent space with SE kernels and lengthscales initialised to 1
8In which case the off-diagonal terms in the covariance matrix will be large making the approximation
pθ1 (f) =	pθ1 (f) extremely crude.
9We found that the GP-VAE occasionally got stuck in very poor local optima. Since the ELBO is calculated
on the training set alone, the experimental procedure is still valid.
17
Under review as a conference paper at ICLR 2021.
and 0.1, respectively. All DNNs, except for those in PointNet and IndexNet, use four hidden layers
of 50 units and ReLU activation functions. PointNet and IndexNet employ DNNs with two hidden
layers of 50 units and a 50-dimensional intermediate representation.
E.4 Bouncing Ball
To ensure a fair comparison with the SVAE and SIN, we adopt an identical architecture for the
inference network and decoder in the original experiment. In particular, we use DNNs with two
hidden layers of 50 units and hyperbolic tangent activation functions. Whilst both Johnson et al.
and Lin et al. use eight-dimensional latent spaces, we consider a GP-VAE with a one-dimensional
latent space and periodic GP kernel. For the more complex experiment, we use a SGP-VAE with
fixed inducing points placed every 50 samples. We also increase the number of hidden units in each
layer of the DNNs to 256 and use a two-dimensional latent space - one for each ball.
E.5 Weather Station
The spatial location of each weather station is determined by its latitude, longitude and elevation
above sea level. The rates of missingness in the dataset vary, with 6.3%, 14.0%, 18.9%, 47.3% and
93.2% of values missing for each of the five weather variables, respectively. Alongside the average
temperature for the middle five days, we simulate additional missingness from the test datasets by
removing 25% of the minimum and maximum temperature values. Each model is trained on the
data from 1980 using a single group per update for 50 epochs, with the performance evaluated on
the data from both 1980 and 1981 using the root mean squared error (RMSE) and NLL averaged
across five runs. We use a three-dimensional latent space with SE kernels and lengthscales initialised
to 1. All DNNs, except for those in PointNet and IndexNet, use four hidden layers of 20 units and
ReLU activation functions. PointNet and IndexNet employ DNNs with two hidden layers of 20 units
and a 20-dimensional intermediate representation. Inducing point locations are initialised using k-
means clustering, and are shared across latent dimensions and groups. The VAE uses FactorNet.
We consider independent GPs modelling the seven point time series for each variable and each
station, with model parameters shared across groups. No comparison to other sparse GP approaches
is made and there is no existing framework for performing approximate inference in sparse GP
models conditioned on previously unobserved data.
F	Further Experimentation
F.1 Small Scale Experiments
Table 4: A comparison between multi-output GP models on the EEG and Jura experiments.
	Metric	GP-VAE	GPPVAE
EEG	SMSE	0.24 (0.02)	0.524 (0.11)
	NLL	2.01 (0.28)	1.92 (0.05)
Jura	MAE	0.40 (0.01)	0.49 (0.02)
	NLL	1.00 (0.06)	0.99 (0.04)
Table 4 compares the performance of the GP-VAE to that of the GPPVAE, In all cases, FactorNet
is used to handle missing data. We emphasise that the GP-VAE and GPPVAE employ identical
probabilistic models, with the only difference being the form of the approximate posterior. The su-
perior predictive performance of the GP-VAE can therefore be accredited to the use of the structured
approximate posterior as opposed to the mean-field approximate posterior used by the GPPVAE.
F.2 Synthetic Bouncing Ball Experiment
The original dataset consists of 80 12-dimensional image sequences each of length 50, with the task
being to predict the trajectory of the ball given a prefix of a longer sequence. The image sequences
are generated at random by uniformly sampling the starting position of the ball whilst keeping the
18
Under review as a conference paper at ICLR 2021.
⅛∕⅝∕wwvs>
Truth
GP-VAE
√∖∕W∖A∕∖A/
Figure 5: A comparison between the mean of the GP-VAE’s posterior predictive distribution (mid-
dle) and the ground truth (top) conditioned on noisy observations up to the red line. The latent
approximate GP posterior is also shown (bottom).
bouncing frequency fixed. Figure 5 compares the posterior latent GP and mean of the posterior
predictive distribution with the ground truth for a single image sequence using just a single latent
dimension. As demonstrated in the more more complex experiment, the GP-VAE is able to recover
the ground truth with almost exact precision.
Following Lin et al. (2018), Figure 1a evaluates the τ -steps ahead predictive performance of the
GP-VAE using the mean absolute error, defined as
Ntest T -τ
XX
n=1 t=1
1
NteSt(T - T)d
llyn,t + τ - EdCiyn^t, + τ Wn,Lt)
yn,t+τ	1
(43)
where Ntest is the number of test image sequences with T time steps and yn,t+τ denotes the noiseless
observation at time step t + τ .
G Partial Inference Network Computational Graphs
19
Under review as a conference paper at ICLR 2021.
(c) FactorNet
Figure 6: An illustration of the three different partial inference network specifications discuss in
Section 2.4. η denotes the vector of natural parameters of the multi-variate Gaussian being parame-
terised.
20