Under review as a conference paper at ICLR 2021
Solving Non-Stationary Bandit Problems with
an RNN and an Energy Minimization Loss
Anonymous authors
Paper under double-blind review
Ab stract
We consider a Multi-Armed Bandit problem in which the rewards are non-
stationary and are dependent on past actions and potentially on past contexts. At
the heart of our method, we employ a recurrent neural network, which models
these sequences. In order to balance between exploration and exploitation, we
present an energy minimization term that prevents the neural network from be-
coming too confident in support of a certain action. This term provably limits the
gap between the maximal and minimal probabilities assigned by the network. In a
diverse set of experiments, we demonstrate that our method is at least as effective
as methods suggested to solve the sub-problem of Rotting Bandits, can solve in-
tuitive extensions of various benchmark problems, and is effective in a real-world
recommendation system scenario.
1	Introduction
The cliche “insanity is doing the same thing over and over again and expecting different results”
is obviously wrong. Our experience tells us that we cannot exploit the same action repeatedly and
expect to enjoy the same outcome.
In this work, we are concerned with Multi-Armed Bandits (MAB). In the conventional MAB setting,
the reward distribution of each arm is assumed to be stationary. Motivated by real-world scenarios,
such as online advertising and content recommendation, Levine et al. (2017) have considered the
Rotting Bandits setting, in which the reward decays in accordance with the number of times that
an arm has been pulled. Rotting Bandits, however, do not address the cases in which the reward
is dependent on the complete history of the arm pulling actions, which also takes into account the
pulling of other arms, as well as the order of the actions.
A simple scenario, in which the reward for a given arm depends on the timing of past events and on
the pulling of other arms is the intuitive scenario in which after k consecutive pulls of the same arm,
its reward vanishes. In this scenario, with the pulling of any other arm, the exhausted arm is reset.
In our model, we employ a Recurrent Neural Network (RNN) in order to model the non-stationary
reward distributions. For problems in which the decision at each round can benefit from a set of
observations, this context can be put as the input to the RNN. Thus, our method naturally extends
a non-stationary form of Contextual Bandits (Langford & Zhang, 2007). In our model, unlike the
conventional Contextual Bandit case, the optimal action predictions are thus conditioned not only
on the most recent context, but also on all previous contexts.
The learned policy selects the action based on a softmax that is applied to a set of logits. Similar
to what is observed in other contexts of Reinforcement Learning (Tijsma et al., 2016) and also in
Continual Learning (Serra et al., 2018), this may lead to an overconfident network. Such a confident
decision increases exploitation at the expense of exploration. In order to overcome this, we add a
novel regularization term that reduces the Boltzmann energy of the logits. This term is shown to
directly lead to an upper bound on the ratio between the maximal probability assigned by the model
to an arm and the minimal probability.
Our experiments are performed along four axes. First, we show that the new method performs
well on well-known stationary problems. Second, that it outperforms multiple leading baselines on
non-stationary extensions of these problems. Third, we show that our generic method outperforms,
1
Under review as a conference paper at ICLR 2021
in terms of convergence time, the leading methods for Rotting Bandits. Lastly, we show that our
method is appropriate for capturing real-world scenarios. In a set of ablation experiments, we also
demonstrate that the energy regularization method we advocate for, outperforms other exploration
enhancing alternatives.
2	Related Work
Multi-Arm Bandits The MAB field started with the seminal work of Thompson (1933). Gittins
(1974) were the first to propose a scenario where an arm’s reward may change. Non-stationary
arises when considering time-dependent priors(Besbes et al., 2014) or when only a subset of the
bandit arms is available at a given time such as in the Sleeping Bandit(Kanade et al., 2009; Li et al.,
2019). However, it also appears when there is no explicit time-dependency. For instance, when
the reward of a given arm decays with each pull such as in the Rotting Bandit(Levine et al., 2017;
Seznec et al., 2019) or when the rewards are received in delay(Liu et al., 2019).
Contextual Bandits(Li et al., 2010) aim to relate between a given context and the bandit problem.
It is a highly applicable method, which was developed to support the personalization of the bandit
algorithm per user. The method has been successfully applied for recommendation tasks(Li et al.,
2010) and non-linear tasks(Krishnamurthy et al., 2016).
Exploration Strategies In Neural Networks The nature of the Multi-Arm Bandit problem re-
quires the balancing between exploration and exploitation. Several methods have been proposed for
the task. The -greedy method, performs exploration with probability . Tijsma et al. (2016) have
shown that a better alternative is to apply softmax regularization, which smooths the probability
distribution using a high temperature. Modifying the temperature of the softmax is also beneficial
in other contexts, such as knowledge distillation(Hinton et al., 2015).
Recurrent Neural Networks RNNs have been the architecture of choice, when solving sequen-
tial tasks. Many of the successful RNN methods utilize a gating mechanism, where the hidden state
ht can be either suppressed or scaled, depending on a function of the previous hidden state and the
input. Among these solutions, there is the Long-Short Term Memory network (LSTM) of Hochre-
iter & Schmidhuber (1997) that utilizes a gating mechanism together with a memory cell and the
powerful Gated Recurrent Unit (GRU) by Cho et al. (2014). The GRU updates at each time a hidden
state ht given an input xt,
ht = (1 - ft) ht-1 + ft tanh (W1xt + W2ht-1 + bh)	(1)
ft = σ (W1,f xt + W2,f ht-1 + bf) ,	(2)
where W1, W2, W1,f, W2,f are weight matrices and bf and bh are biases.
3	Method
Let A be a set of n possible actions, {ai}in=1. At each time step 1 ≤ t ≤ T, a context, ct ∈ Rm, is
presented to the observer and a reward, ri,t, is assigned to each action. This framework applies both
to cases in which the context is meaningful m > 0 and in cases in which the context is void (m=0).
In the bandit setting, only the reward ri,t associated with the action selected at time t is presented to
the observer. Let {ait}tT=1 be the set of actions selected by the observer. The objective of the learner
is to minimize the regret,
R =	max{rk,t} - rit,t .	(3)
k
t
In order to select an action at time step t, a l-layer stacked GRU(Cho et al., 2014), f, with a time-
dependent hidden state, ht, of size d is employed. The GRU receives as input the previous hidden
state, ht-1, together with the context vector, ct, and updates the hidden state
ht = f(ct,ht-1) .	(4)
Next, an affine layer, o, acts on the hidden state, ht ,
zi = o (ht) ,	(5)
2
Under review as a conference paper at ICLR 2021
and is fed to a softmax layer, which provides the probability of selecting action ai as
ezi
pi 一 √z√n	Σ- ∙
j=1 ezj
(6)
Under this setting, only partial information is available, as only the reward for action it is re-
vealed. In order to minimize the non-differentiable regret R (equation 3), we employ the REIN-
FORCE(Williams, 1992) algorithm. The loss function at time t is
Lr = — (rit,t - rt) log pit ,	(7)
where 尸t is the mean of the obtained rewards UP to time t.
Since the observer is unaware of the outcomes of other actions at time t, selecting the next action
reqUires a sUbtle balance between exPloration and exPloitation. Whereas in neUral networks, ex-
Ploitation occUrs natUrally, we encoUrage exPloration in three manners. First, we aPPly a droPoUt
with Probability pdropout after each GRU layer, thUs adding Uncertainty to the Predicted actions.
Second, we samPle the next action by considering the Probability, pi , of the action and not by
choosing the one with maximal pi . Third, we add an energy conservation term to the loss fUnction,
thUs regUlating the valUes of the network’s Parameters, so the oUtPUt of classification layer cannot
become biased.
The softmax fUnction can be interPreted as the Boltzmann’s distribUtion with so that each neUron zi
carries an energy of Ei = -zi. The mean energy of the system, hEi, is
n
hEi = XpiEi .
i=1
LEC = hEi2
In order to conserve energy, the following regUlarization term is added to the loss fUnction,
n
pitEi
i=1
Since LEC is qUadratic, it has a minimUm only when
hEi = 0.
(8)
(9)
(10)
This ensUres that if an activation, zi , is drifting to a high valUe (which resUlts in a high Probability,
pi , for selecting action ai), LEC will lead to a comPensation by increasing the valUes of the other
neUrons zj as well, sUch that eqUation 10 is satisfied. SPecifically, the following boUnd holds.
Theorem 1. Let zi be a set of n logits that are passed to a softmax to obtain probabilities pi and
are sufficiently regularized by LEC. Denote by pmax and pmin the maximal and minimal values of
pi. Then
pmax ≤ J,	(11)
pmin	eW (-e )
where W is the Lambert-W function defined as
W (x) eW (x) = x .
(12)
Proof. Denote by zmax and zmin the maximal and minimal zi valUes and by imax the index of zmax.
It follows from Pin=1 pit zi = 0 that
ezmaxzmax= - X ezizi ≤ - (n - 1) ezmin zmin .	(13)
i6=imax
The minimum of the function y = Xex is located at y = 一 e which means ezmin Zmin ≥ 一 1.
PlUgging this to eqUation 13,
ezmax Zmax ≤ n-—	(14)
e
Since Zmax must be Positive to satisfy equation 10, and because xex is monotonic for x > 0,
Zmax ≤ W( n-1)	(15)
e
3
Under review as a conference paper at ICLR 2021
Algorithm 1 The Recurrent Bandit Algorithm
Input: αEC
h0 = 0
for t = 1 to T do
ht = f (ct, ht-1)
pi = SOFTMAX(o(ht))	Compute probability for all actions pi, Eq. 6
Select the next action, ait by sampling the multinomial distribution pi
Observe the reward rit ,t
L = LR + αEC LEC	Compute the loss terms in Eq. 7, 9
Perform back-propagation and set L = 0
end for
The ratio between the probabilities pmax and pmin is
Z	W(n-1)
Pmax = ezmax ≤ e ( e )
Pmn — ezmn ^ IWFiT
For small values of n, the bound grows almost linearly (see right). It limits
the highest probability gap between the arms, thus allowing less visited
actions to be explored.
The two loss terms are combined to the final term L = LR + αECLEC,
where αEC is a regularization hyperparameter. The entire procedure pro-
cedure follows Alg. 1.
4 Experiments
(16)
paG ytilibaborP lamixaM
We apply our method on a variety of different multi-arm bandit benchmarks. For all the tasks, except
for the Yahoo! Recommendation Prediction Dataset and the Wheel Bandit, the context vector, ct ,
that is fed to the Recurrent Bandit is null. Thus, information from previous time may only flow
through the hidden state, ht-1.
We optimize our method using RMSprop(Bengio, 2015) with a learning rate of 0.001. The network
f for all the experiments is a 2-layer stacked GRU, each with a hidden size of dh = 128. The
dropout used for all the experiments is pdropout = 0.1, except for the Rotting Bandits, where it is
set to pdropout = 0, since it does not require much exploration due to its nature. The regularization
parameter is fixed at a value of αEC = 0.1 for all the benchmarks.
An ablation study is performed to verify the benefits of using the novel loss term LEC. -greedy
ablation uses the same GRU architecture, however at each time step chooses a random action with
a probability of = 0.01, and the most probable action with probability 1 - . The Softmax-reg
ablation uses Softmax regularization to smooth the action selection using a temperature of T = 2.
The no EC ablation is obtained by setting αEC = 0. The Bernoulli Bandit baseline is taken from
Algorithm 2 in (Chapelle & Li, 2011).
Bernoulli Multi-Arm Bandit The most basic MAB scenario is the Bernoulli one. A set of n
ʌ
handles or actions, are assigned uniformly with a prior {θk}kn=1. At each time step, the user has
to pick one action. each of the actions carries a reward, rk ∈ {0, 1}, sampled from the Bernoulli’s
distribution,
rk 〜Bernoulli (θk) ∙	(17)
We compare our method to the Bernoulli Bandit (Chapelle & Li, 2011), which is based on
Thompson-Sampling, to SWA(Levine et al., 2017) that was designed for the Rotting Bandit prob-
lem, and to the three ablation variants of our method. Fig. 1(a) shows the mean accumulated regret
for 100 simulations for each of the methods. As can be seen, all methods are able to reduce the mean
accumulated regret over time. However, SWA takes longer to accomplish this. Our method works
best in this case if regularization is turned off, and its regret over the Bernoulli Bandit is not large.
4
Under review as a conference paper at ICLR 2021
0.6
tergeR evitalumuC nae
4
.
0
1
∙105
0	0	0.2	0.4	0.6	0.8
T
tergeR evitalumuC nae
1
1
∙105
0	0	0.2	0.4	0.6	0.8
T
tergeR evitalumuC nae
4
.
0
0
0	0.2	0.4	0.6	0.8	1
T	∙105
(c)
1864
...
000
tergeR evitalumuC nae
0
0	0.2	0.4	0.6	0.8	1
T	∙105
(d)
6
.
0
b
Figure 1: (a) AMAB with 10 actions. (b) A sinusidial MAB with 10 actions. (c) AMAB with 10
actions. We further limit the number of consecutive pulls of a handle by 10. (d) Sinusidial MAB
with 10 actions. We further limit the number of consecutive pulls of a handle by 10.
Time Dependent Multi-Arm Bandit One way to create a non-stationary bandit problem, is to add
time-dependent priors to the Bernoulli Multi-Arm Bandit task. To accomplish this, time dependency
is inserted to the priors θk using the mirrored sine function:
ʌ
θk (t) = θk ∣sin(ωt + φk)|	(18)
where ω = ^2π^. A phase, φk =等,between the arms, is also added such that the arm holding
the highest reward is changes place through time. For this experiment, there are n = 10 available
arms and a periodicity of Tperiod = 10000. The mean accumulated reward is presented in Fig. 1(b).
As can be seen, the drifting of the rewards over time makes it hard for both the Bernoulli Bandit,
and the baselines without the Energy Conservation to keep a low regret. The SWA algorithm fails to
solve this task, since it was designed specifically for decaying rewards. The variant of our methods
that are meant to evaluate other forms of encouraging exploration also fail (this is also true for all
other values we tested for their regularization parameter).
Time Dependent Multi-Arm Bandit with Correlative Arms In order to further investigate the
non-stationary behavior, we limit the number of consecutive selections of a single arm to 10. After
an arm has been selected for 10 times in a row, its reward is set to 0 unless another arm has been
selected. The results of applying this setting to the Bernoulli Multi-Arm Bandit problem are shown
in Fig. 1(c). In this case, only the Bernoulli Bandit and our method learn fast and keep a low regret.
Under this setting, SWA is not able to solve this task.
We then combine the two MAB variants and apply zero the reward after ten consecutive pulls for the
Time Dependent MAB. The results are shown in Fig. 1(d). Our method is the only one to maintain
5
Under review as a conference paper at ICLR 2021
Figure 2:	The Wheel Bandit setup. Each colored region describes a different reward distribution
based on points sampled in that region. (a) The Wheel Bandit. (b) The Rotating Wheel Bandit with
an angular velocity of ω.
a low mean cumulative regret. The Bernoulli Bandit regret increases with time, and the ablation
variants are not able to converge to low solution.
Wheel Bandit The Wheel Bandit by Riquelme et al. (2018) has been introduced to examine the
expressiveness of contextual bandits in non-linear tasks. The context ct = (xt, yt) ∈ R2 in this task
is uniformly sampled in the unit circle (R = 1). There are five available actions, a〃 The first, always
grants a reward, r 〜N (μι ,σ) independent of the context, ct. Inside the region d = ，x2 + y2 ≤ δ,
the other four actions grant a reward of r 〜N (μ2 ,σ),for μ2 < μι. For the region d > δ, depending
on the signs on xand y (to which quarter ofR2 they belong to) one of the four arms grants a reward
of r 〜N (μ3,σ), where μ3》μι, whereas the remaining arms grant a reward of r 〜 N (μ2,σ).
We use the same settings as the original Wheel bandit,
μι = 1.2 μ2 = 1.0 μ3 = 50 Q = 0.01
with an exploration motivating parameter δ = 0.5.
To make this problem non-stationary, a time-dependent rotation is applied. This is accomplished by
making the actions’ reward distributions depend on the sign ofx0 and y0,
x0	cos (ωt) sin (ωt)	x
y0 = - sin (ωt) cos (ωt)	y
(19)
Both setups are depicted in Fig. 2. We run both setups for T = 10000, and for the Rotating Wheel
Bandit, we use an ω = 毫0 (Total of five rotations). Riquelme et al. (2018) report that the best
algorithm to solve the stationary task is their NeuralLinear algorithm, as can also be seen in Fig. 3(a)
(our method is second best in this setting). However, when time-dependency is introduced, it quickly
diverges, whereas our method is still able to learn the dynamics, as can be seen in Fig. 3(b).
Rotting Bandit The Rotting Bandit framework was introduced by Levine et al. (2017) to handle
cases where the Bandit’s arms are not stationary, but rather decay over time with each pull. We
follow the original scenario, by considering two available actions, a1 and a2 for 30000 time steps,
with the following rewards, ri , sampled from,
ri 〜 N (μι = 0.5, σ2 = 0.2)
〜 ʃ N (μ2 = 1,σ2 =0.2) n2 < 7500
r2	N N (μ2 = 0.4, σ2 =0.2)	else
(20)
(21)
where ni is the number of times action ai has been selected. For this experiment, the regret presented
is the Policy Regret (Arora et al., 2012), since the optimal policy (Levine et al., 2017) for this task
is,
ait = arg max {ri (Ni,t + 1)} ,	(22)
i
6
Under review as a conference paper at ICLR 2021
50
tergeR evititamluC nae
0000
4321
00
0.2	0.4	0.6	0.8
Step
1
•104
tergeR evititamluC
50
0
1
0	0.2	0.4	0.6	0.8	1
Step	∙104
(a)	(b)
Figure 3:	(a) The Wheel Bandit. (b) Rotating Wheel Bandit with Tperiod = 2000. LinTS is by Cortes
(2018) and NeuralLinear is the method of Riquelme et al. (2018).
tergeR evititamlu
Figure 4: Cumulative regret for Rotting Bandits.
Table 1: Click Through Rate (CTR) for the
Yahoo! Front Page Today Module User
Click Log Dataset for different contextual
bandits.
Method	CTR
LinUCB(Li et al., 2010)	0.040
LinTS(Cortes, 2018)	0.036
BootstrappedUCB(Cortes, 2018) 0.044
Softmax-reg ablation	0.090
-greedy ablation	0.052
No energy constraint ablation	0.090
Ours	0.167
where Ni,t is the number of times action i was selected up to time t, and ri (ni) is the reward given
by action i after it was selected for ni times.
We compare our method to SWA(Levine et al., 2017), FEWA(Seznec et al., 2019) (two Rotting
Bandit methods), and to the Vanilla Bernoulli Bandit. The cumulative policy regret obtained by
these methods averaged for 10 simulations is shown in Fig. 4. As can be seen, the Bernoulli Bandit
and the GRU baselines fail to solve this task. Our method is able to converge much faster than
SWA and FEWA. In contrast with SWA and FEWA, our method does not assume anything about
the reward distribution, and therefore has to continuously explore. SWA and FEWA were designed
to solve tasks with decaying rewards, and therefore do not require further exploration after a certain
point. This is the reason our method keeps on accumulating regret, unlike SWA and FEWA that
converge to the exact solution.
Yahoo! Recommendation Prediction Dataset The Yahoo! Front Page Today Module User Click
Log Dataset (v2.0) by Li et al. (2010) is a real-life multi-arm bandit benchmark that allows the
offline evaluation of different algorithms. The entries in this dataset are sorted by the time in which
they occurred. Each entry is composed of (i) an article that was randomly presented to a user (ii) the
user’s response, whether the article was selected or not (iii) an anonymized 136-dimensional vector
descriptor of the user (iv) a list of the possible articles that could have been presented at that time.
In order to evaluate the different approaches, we have taken 1.5M samples from the original dataset,
together with 100K samples for evaluation. At each time step, all algorithms are fed with a binary
7
Under review as a conference paper at ICLR 2021
tergeR evititamluC naeM
000000
54321
tergeR evititamluC naeM
000000
54321
p
e
St
4
10
-
p
e
St
4
10
-
2
.
0
4
.
0
6
.
0
2
.
0
4
.
0
6
.
0
(a)	(b)
Figure 5: The Mean Cumulative Regret on the Wheel Dataset for different values of αEC. (a) The
stationary wheel. (b) The rotating wheel with Tperiod = 2000.
user descriptor vector as the context, ct ∈ {0, 1}1 36, and are tasked with predicting which article
should be presented to that user. If the article was indeed presented to that user and the user clicked
on the article, the algorithm is granted a reward of rt = 1.
Since the (counterfactual) information on the user’s response to other articles in the pool is absent,
the evaluation of the various methods on this dataset is not trivial. To remedy this, we use the
offline policy evaluation of Li et al. (2011). Only events in which a method has agreed with the
originally displayed article are evaluated. A comparison is made to several algorithms available
under the contextualbandits package(Cortes, 2018). LinUCB(Li et al., 2010), LinTS(Cortes, 2018),
and BootstrappedUCB(Cortes, 2018) together with a comparison to the ablation variants of our
method.
We report the click-through-rate (CTR), i.e., the ratio of users’ selected articles to the number of ar-
ticles presented by the algorithm in Tab. 1. As can be seen, our method greatly outperforms all other
contextual bandits. In addition, the alternative means for encouraging exploration are outperformed
by LEC that our method uses.
Sensitivity to the regularization parameter In order to test the sensitivity of our method to dif-
ferent values of αEC, We run our method on the Wheel Bandit and the Rotating Wheel Bandit tasks
using different αEC values. The results for this sensitivity test appear in Fig. 5. As can be seen, the
performance is stable in the range [0.01,1] for the stationary version and in the somewhat smaller
range of [0.1,1] for the rotating wheel.
5 Conclusions
The rewards obtained for a specific action are very often time dependent. In many cases, they are
also dependent on the previous actions that were taken. In this work, we propose to employ an RNN
in order to solve the non-stationary bandit problem. The solution is general enough to solve both the
vanilla case, as well as the contextual case, and is robust enough to address stationary cases.
We note that training without proper regularization results in a learner who is too confident and
neglects exploration. Such an exploration is especially crucial when the rewards vary over time.
We, therefore, suggest a new regularization term that minimizes the Boltzmann energy. This term
leads to a bounded gap between the maximal and minimal probability assigned to each arm. Our
experiments show that our method addresses multiple non-stationary rewards that vary according to
time only or also by action. In addition, our method successfully tackles the specific case of non-
stationary bandits called the rotting bandits. The advantage of our method is especially clear on a
real world recommendation dataset, where it obtains more than triple the click through rate of the
literature baselines.
8
Under review as a conference paper at ICLR 2021
References
Raman Arora, Ofer Dekel, and Ambuj Tewari. Online bandit learning against an adaptive adversary:
from regret to policy regret. arXiv preprint arXiv:1206.6400, 2012.
Yoshua Bengio. Rmsprop and equilibrated adaptive learning rates for nonconvex optimization. corr
abs/1502.04390, 2015.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-
stationary rewards. In Advances in neural information processing systems, pp. 199-207, 2014.
Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. In Advances in
neural information processing systems, pp. 2249-2257, 2011.
KyUnghyUn Cho, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-
ger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder
for statistical machine translation. In EMNLP, 2014.
David Cortes. Adapting multi-armed bandits policies to contextual bandits scenarios. arXiv preprint
arXiv:1811.04383, 2018.
John Gittins. A dynamic allocation index for the sequential design of experiments. Progress in
statistics, pp. 241-266, 1974.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Sepp Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-
1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL http://dx.
doi.org/10.1162/neco.1997.9.8.1735.
VarUn Kanade, H Brendan McMahan, and Brent Bryan. Sleeping experts and bandits with stochastic
action availability and adversarial rewards. Proceedings of the Twelth International Conference
on Artificial Intelligence and Statistics, 2009.
Akshay KrishnamUrthy, Alekh Agarwal, and Miro DUdik. ContextUal semibandits via sUpervised
learning oracles. In Advances In Neural Information Processing Systems, pp. 2388-2396, 2016.
John Langford and Tong Zhang. The epoch-greedy algorithm for contextUal mUlti-armed bandits.
In Proceedings of the 20th International Conference on Neural Information Processing Systems,
pp. 817-824. Citeseer, 2007.
Nir Levine, Koby Crammer, and Shie Mannor. Rotting bandits. In Advances in neural information
processing systems, pp. 3074-3083, 2017.
Fengjiao Li, Jia LiU, and Bo Ji. Combinatorial sleeping bandits with fairness constraints. IEEE
Transactions on Network Science and Engineering, 2019.
Lihong Li, Wei ChU, John Langford, and Robert E Schapire. A contextUal-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference
on World wide web, pp. 661-670, 2010.
Lihong Li, Wei ChU, John Langford, and XUanhUi Wang. Unbiased offline evalUation of contextUal-
bandit-based news article recommendation algorithms. In Proceedings of the fourth ACM inter-
national conference on Web search and data mining, pp. 297-306, 2011.
Larkin LiU, Richard Downe, and JoshUa Reid. MUlti-armed bandit strategies for non-stationary
reward distribUtions and delayed feedback processes. arXiv preprint arXiv:1902.08593, 2019.
Carlos RiqUelme, George TUcker, and Jasper Snoek. Deep bayesian bandits showdown: An
empirical comparison of bayesian deep networks for thompson sampling. arXiv preprint
arXiv:1802.09127, 2018.
9
Under review as a conference paper at ICLR 2021
Joan Serra, Didac Suris, Marius Miron, and Alexandros Karatzoglou. Overcoming catastrophic
forgetting with hard attention to the task. In International Conference on Machine Learning, pp.
4548-4557, 2018.
Julien Seznec, Andrea Locatelli, Alexandra Carpentier, Alessandro Lazaric, and Michal Valko. Rot-
ting bandits are no harder than stochastic ones. In The 22nd International Conference on Artificial
Intelligence and Statistics, pp. 2564-2572, 2019.
William R Thompson. On the likelihood that one unknown probability exceeds another in view of
the evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Arryon D Tijsma, Madalina M Drugan, and Marco A Wiering. Comparing exploration strategies
for q-learning in random stochastic mazes. In 2016 IEEE Symposium Series on Computational
Intelligence (SSCI), pp. 1-8. IEEE, 2016.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
10