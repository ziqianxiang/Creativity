Under review as a conference paper at ICLR 2021
Guiding Neural Network Initialization via
Marginal Likelihood Maximization
Anonymous authors
Paper under double-blind review
Ab stract
We propose a simple approach to help guide hyperparameter selection for neural
network initialization. We leverage the relationship between neural network and
Gaussian process models having corresponding activation and covariance func-
tions to infer the hyperparameter values desirable for model initialization. Our
experiment shows that marginal likelihood maximization provides recommenda-
tions that yield near-optimal prediction performance on MNIST classification task
under experiment constraints. Furthermore, our empirical results indicate consis-
tency in the proposed technique, suggesting that computation cost for the proce-
dure could be significantly reduced with smaller training sets.
1	Introduction
Training deep neural networks successfully can be challenging. However, with proper initializa-
tion trained models could improve their prediction performance. Various initialization strategies in
neural network have been discussed extensively in numerous research works. Glorot and Bengio
(2010) focused on linear cases and proposed the normalized initialization scheme (also known as
Xavier-initialization). Their derivation was obtained by considering activation variances in the for-
ward path and the gradient variance in back-propagation. He-initialization (He et al., 2015) was
developed for very deep networks with rectifier nonlinearities. Their approach imposed a condition
on the weight variances to control the variation in the input magnitudes. Because of its success,
He-initialization has become the de facto choice for deep ReLU networks. While Glorot- and He-
initialization schemes recognize the importance of and make use of the hidden layer widths in their
formulation, other methods were also suggested to improve training in deep neural networks.
Mishkin and Matas (2016) demonstrated that pre-initialization with orthonormal matrices followed
by output variance normalization produces prediction performance comparable to, if not better than,
standard techniques. Additionally, Schoenholz et al. (2017) developed the bound on the network
depth based on the principle of ’Edge of Chaos’ given a particular set of initialization hyperparame-
ters. Furthermore, Hayou et al. (2019) showed that theoretically and in practice proper initialization
parameter tuning with appropriate activation function is important to model training for improved
performance.
Neal (1996) showed that as a fully-connected, single-hidden-layer feedforward untrained neural
network becomes infinitely wide, Gaussian prior distributions over the network hidden-to-output
weights and biases converge to a Gaussian process, under the assumption that the parameters are
independent. In other words, the untrained infinite neural network and its induced Gaussian process
counterpart are equivalent. Also, as a result of the central limit theorem, the covariance between
network output evaluated at different inputs can be represented as a function of the hidden node
activation function. Intuitively, we could therefore relate the prediction performance of an untrained,
finite-width, single-hidden-layer, fully-connected feedforward neural network to a Gaussian process
model with a covariance function corresponding to the network’s activation function.
In this work we propose a simple and efficient method that learns from training data to guide the
selection of initialization hyperparameters in neural networks. Marginal likelihood is a popular tool
for choosing kernel hyperparameters in model selection. Its applications in convolutional Gaussian
processes and deep kernel learning are discussed, respectively, in (van der Wilk et al., 2017; Wilson
et al., 2016). Our method aims to synergize this powerful functionality of marginal likelihood and
1
Under review as a conference paper at ICLR 2021
the relationship between untrained neural networks and Gaussian process models to make recom-
mendations for neural network initialization. We first derive the covariance function corresponding
to the activation function of the network whose prediction performance we wish to evaluate. We then
employ marginal likelihood optimization for the Gaussian process model to learn hyperparameters
from data. We hypothesize that the optimal set of hyperparameter values could improve initialization
of the neural network.
2	Approach
To assess our proposed method, we build a neural network and a Gaussian process model with
corresponding activation and covariance functions. With the Gaussian process we estimate the co-
variance hyperparameters from training data. These hyperparameter values are then applied in the
neural network to evaluate and compare its prediction accuracy among various hyperparameter sets.
We first describe the structure of the neural network, followed by the Gaussian process model and
the underlying reason for employing the marginal likelihood. Then, given the network activation
function we proceed to derive a closed form representation of its counterpart covariance function.
2.1	Single-hidden-layer Neural Networks
Our neural network model is a fully-connected, single-hidden-layer feedforward network with 2000
hidden nodes and rectified linear unit (ReLU) activation function. Following (Lee et al., 2018),
we conduct our empirical study by considering classifying MNIST images as regression prediction.
Inasmuch as the network is designed for regression, we choose the mean square error (MSE) loss
as its objective function, along with Adam optimizer, and accuracy as the performance metric. In
addition, one-hot encoding is utilized to generate class labels, where an incorrectly labeled class is
designated -0.1, and a correctly labeled class 0.9 . For example, the one-hot representation of the
integer 7 is given by [-0.1, -0.1, -0.1, -0.1, -0.1, -0.1, -0.1, 0.9, -0.1, -0.1].
Figure 1: A single-hidden-layer, fully-connected feedforward neural network for regression predic-
tion. Left panel: Structural diagram of the neural network. Right panel: ReLU activation function:
φ(a) := (a)+ = max(0, a) = a for a ≥ 0; φ(a) = 0 otherwise.
As shown in the left panel of Figure (1), the single-hidden-layer neural network has a set of inputs
denoted by X = {χk}, k ∈ {1, 2,…，din} With input layer width din = No = 28x28 = 784.
The model's weight and bias parameters from kth input node to jth hidden node are W0k iid
N(0, Nw), b0 iid N(0, σb), and W0k Il b0. Similarly, the weight and bias parameters from jth hid-
den node to ith output node with hidden layer width N1 = din = 2000 are Wij iid N(0, Nw), b： iid
N(0, σb2), and Wi1j bi1. For regression models the output layer has a single node, and therefore
i ∈ {1}. The ReLU nonlinearity is depicted in the right panel of Figure (1).
2
Under review as a conference paper at ICLR 2021
The input to each hidden node nonlinearity (the pre-activation) is represented by zj0 (x) = bj0 +
Pkdi=n1 Wj0kx0k, while the hidden unit output after the nonlinearity (the post-activation) is denoted by
χi(x) = φ(z0(x)), j ∈ {1,2,…，Nι}. Since We typically apply linear activation function in the
output stage ofa regression model, the model output is simply zi1(x) = bi1 + PjN=11 Wi1jxj1(x) .
2.2	Gaussian Processes
A Gaussian process (MacKay, 1998; Neal, 1998; Williams and Rasmussen, 2006; Bishop, 2006) is
a set of random variables any finite collection of Which folloWs a multivariate normal distribution.
A Guassian process prediction model exploits this unique property and offers a Bayesian approach
to solving machine learning problems. The model is completely specified by its mean function and
covariance function.
By choosing a particular covariance function, a prior distribution over functions is induced Which,
together With observed inputs and targets, can be used to generate prediction distribution for making
predictions and uncertainty measures on unknoWn test points. These capabilities alloW Gaussian
processes to be used effectively in many important machine learning applications such as human
pose inference (Urtasun and Darrell, 2008) and object classification (Kapoor et al., 2010). Recent
research Works also apply Gaussian processes in deep structures for image classification (van der
Wilk et al., 2017) and regression tasks (Wilson et al., 2016).
To help achieve optimal performance for Guassian process prediction We select a suitable covariance
function and tune the model by adjusting hyperparameters characterizing the covariance function.
This can be accomplished by applying the marginal likelihood Which is a crucial feature that enables
Gaussian processes to learn proper hyperparameter values from training data.
2.3	Hyperparameters and Marginal Likelihood Optimization
We briefly describe the procedure for estimating optimal hyperparamter values via maximizing the
Gaussian process marginal likelihood function.
Consider a set of N multidimensional input data X = {xi}iN=1, xi ∈ RD, and target set y =
{yi}iN=1, yi ∈ R . For each input xi We have a corresponding input-output pair (xi, yi), Where the
observed output target is given by yi = f (xi) + g, with data noise Ei 〜 N(0, σn2 ). We model the
input-output latent function f as a Gaussian process :
f (Xi) ~ GP(μ(xi),k(xi,Xj)),
where we customarily set the mean function μ(χi) := E[f (xi)] = 0, and denote k(xi, Xj) as the
covariance function.
The marginal likelihood (or evidence) (Williams and Rasmussen, 2006; Bishop, 2006) measures the
probability of observed targets given input data and can be expressed as the integral of the product
of likelihood and the prior, marginalized over the latent function f :
p(y|X) =	p(y, f|X) df =	p(y|f,X)p(f|X)df.	(1)
The marginal likelihood can be obtained by either evaluating the integral (1) or by noticing
{yi}N=ι = {f (xi)+ei}N=ι, which gives us y|X ~N(0, K+σηI) where K = [k(xi,xj)]Nj=ι and I
are N by N covariance matrix and identify matrix, respectively. As a result,
p(y|X) = (2∏)N∕2∣K + σnI∣1/2 exp ( - 2yT(K + σnI) ") .
To facilitate computation, we evaluate the log marginal likelihood which is given by
logp(y|X) = -2yτ(K + σ2I)-1y - 2log IK + σI| - Nlog 2∏.	⑵
We are reminded here that the marginal likelihood is applied directly on the entire training dataset,
rather than a validation subset. In addition, Cholesky decomposition (Neal, 1998) can be employed
to calculate the term (K + σnI) 1 in equation (2).
3
Under review as a conference paper at ICLR 2021
2.4	ReLU Covariance Function
With the structure of the single-hidden-layer ReLU neural network defined, we proceed to study its
corresponding ReLU Gaussian process.
The ReLU covariance function is developed to estimate the covariance at the output of the ReLU
neural network model. Our alternative derivation was inspired by the work on arc-cosine family
of kernels developed in (Cho and Saul, 2009). In our work we first derive the expectation of the
product of post-activations, instead of on the input to the nonlinearity (Lee et al., 2018). Then, we
apply the output layer activation function on the post-activation expected value. It can be shown that
the resulting representations are equivalent. The complete derivation of our expression is provided
in the Appendix 5.
Referring to Figure 1, we consider input vectors x0, y0 ∈ Rdin . The initial weight value is drawn
randomly from the Gaussian distribution fw= N(0, #) and bias value from 加 = N(0, σb).
The expected value of the product of post-activations at the output of the jth hidden node is com-
puted as
E[Xj(x0)Xj(y0)]
=	…	max(b0 + wj ∙ x0) max(b0 + wj ∙ y0)fbθ,w0 (b,w) dwj dbj
-∞	j, j
=/…/	(b0 + w0 ∙x0) + (b0 + wj ∙ yj)+fb0,wj0 (b,w) dwj db
-∞
(3)
Suppose we denote the pre-activations as
din
U = bj + Wj0 ∙ x0 = bj + X WjkXk 〜N(O σ + σW ∣∣x∣∣2),
k=1
din
V = bj + Wj ∙ y0 = bj + X Wjk，屋〜N (O,戊 + σW kyk2).
k0=1
It can be shown that the random variables U, V have a joint Gaussian distribution:
〜N(0, Σ), where Σ
σb2 + σw2 kxk2
Vσ2+ σW(X ∙ y)
σ2 + σW(X ∙ y)∖
σb2 + σw2 kyk2	,
for simplicity we let X = Xj , y = yj . We can therefore write expression (3) as
U U uv  -ι exp ( — ɪ(u, v)Σ-1(u, V)T) du dv.
JJo 2π∣Σ∣2 v 2	’
NoWIet D :=园=(σ2 + σWkxk2)(σ2 + σWkyk2) - (σ2 + σW(X ∙ y))2,and
ςT=	(a；；	a；：)	, where a11 = -1(σ2 + σWl∣yk2),	a22 = -1(σ2	+ σWIlxk2),
a21	a22	D	D
a12 = a21 = -1(σ2 + σW(x ∙ y)).
With polar coordinate transformation: U = √= cos α, v = √= sin α, expression (3) can be
further reduced to
π
1	/ 2	2 sin 2ɑ
4πD1∕2aιιa22 L0 (1— cos φ Sin 2α)2 ɑ
1
2nD1/2ai；a22 sin3φ
sin(φ) + (∏ — φ) cos(φ)), where φ = cos-1 ( / a12
a；；a22
4
Under review as a conference paper at ICLR 2021
With some algebraic operations and after computing the entries in Σ-1, we arrive at
E[Xj(x)Xj(y)]
=2∏但	1	1 2 + llxk2σW)2 卜2 + l∣yk2σW)2 卜in φ + (π — φ) cosφ
where φ	c cos-ι (	σ2+ (x ∙ y)σW	). ɪ (σ2 + llxk2σW) / (σ2+ kyk2σW) / ʃ
To compute the expected value, E[Xj(x)] = J max(b + W ∙ χ)fb°,w0Jb,w) dwdb, We denote
U = b + w ∙ X 〜 N(0,σ2 + σW∣∣x∣∣2), and apply the change in variable -ɪʒ-u2 = t, where σ2 =
b w	2σ 2
σb2 + σw2 kxk2 to obtain
E[Xj (x)] =	(u)+fU (u)du
-∞
Γ 1	- ɪ U2〃
= U _______ e 2σ2 du
0	2πσ
=Z∞ σ2dt-71-e-t
0	2πσ
σ
=—:—
√2∏
_ Pσ2 + σWkχk2
√2∏
The covariance function at the network output is therefore determined to be
N1	N1	N1	N1
E [(b1 + X WijXj(χ))(b1 + X WikXk(y))] - EW + X WijXj(x)] [bl + X WikXk (y)]
j =1	k=1	j=1	k=1
Ni	ι ____________ ____________
=E[(b1)2] + XE[(Wj)2]E[Xj(x)Xj(y)] - 2-√σ2 + σW∣∣x∣∣2《* + σW∣∣y∣∣2
j=1	2π
=σ + NNιE[Xj(X)Xj(y)] - 2∏ Jσ2 + σW∣∣x∣∣2 Jσ2 + σW∣∣y∣∣2
2	ι	ι
=σb+ 2w 卜2 + llχk2σW)2 (σ2 + l∣yk2σW)2 卜in φ + (π - O) Cosφ - 1).	■
2.5 Gaussian Process Prediction: A S imulation
Performing simulations allows us to explore and understand some properties of the models we wish
to study. Simulation results also offer the opportunity for evaluating model precision and insight
into observed events.
To demonstrate making predictions with Gaussian process regression model, we borrow equations
from (Williams and Rasmussen, 2006) where the formulation of Gaussian process predictive distri-
bution is treated in great detail.
Given the design matrix X = {xi }iN=1 , xi ∈ RD, observed targets y = {yi }iN=1 , yi ∈ R, unknown
test data X*, and their function values f := f (X*), the joint distribution of the target and function
values is computed as
「y 1 〜N(0 ∖k(χ,χ)+σI	k(x,x*ι!
f*∖	N S [	K(X*,X)	K(X*,X*)]),
where K(X, X) represents the covariance matrix of all pairs of training points, K(X, X*) denotes
that of pairs of training and test points, and K(X*, X*) gives the covariance matrix of pairs of test
points.
5
Under review as a conference paper at ICLR 2021
The prediction distribution is the conditional distribution
f*∣X,y,X* 〜N(μ*, Σ*)
with mean function μ* = K (X*, X) [K (X, X) + σ%I] 1 y
and covariance Σ* =K(X*,X*) -K(X*,X)K(X,X) +σn2I-1K(X,X*).
The simulation starts out with setting the hyperparameters of the ReLU covariance function to
(3.6, 0.02), chosen from σw2 ∈ [0.4, 1.2, 2.0, 2.8, 3.6], and σb2 ∈ [0.0001, 0.01, 0.02]. We randomly
select a set of 70 training and 30 test location points from 100 values evenly spaced in the interval
[0.0, 1.0]. Ten sample paths, as shown in the top left panel of Figure (2), are generated from the
design Gaussian process model. Their sample mean produces 70 training target and 30 test values.
We then estimate the optimal hyperparameters from the training targets via evaluating the marginal
likelihood, equation (2), over the design ranges of σw2 and σb2 .
The maximum marginal likelihood is obtained at {σW, σ2} = {3.6, 0.02} which is the design hy-
perparameter pair. The minimum marginal likelihood is obtained at {σW, σb2} = {3.6, 0.0001}. A
Gaussian process model is then built with the optimal hyperparameter pair to make predictions for
the 30 test location points. The model accuracy is assessed with a RMSE of 0.00051. Additionally
we overlay the predicted and true test target values, as shown in the top middle panel of Figure
(2), to detect any prediction errors. We plot the line of equality to further validate the estimated
hyperparameters, as depicted in the top right panel of the figure.
The evaluation process is repeated applying the hyperparameter pair {σW, σb} which produces a
prediction RMSE of 0.00188, over 3 times as large as the optimal case. The accuracy plots shown
in the bottom panels of Figure (2) indicate some prediction errors.
Figure 2: Gaussian process regression prediction on simulated data. Top left: 10 sample paths gen-
erated from a Gaussian process model with hyperparameters (σw2 , σb2) = (3.6, 0.02). Top middle:
Point-wise visual comparison between predicted and true target values for the optimal hyperpa-
rameter pair {3.6, 0.02}, showing good prediction results. Top right: The line of equality further
confirming the prediction accuracy. Bottom left: Point-wise visual comparison for hyperparameter
pair {3.6, 0.0001}. Bottom right: Prediction errors revealed with the line of equality.
Our simulation results agree with the principle that through optimizing the marginal likelihood of
the Gaussian process model, we could estimate from training data the hyperparameter values most
appropriate for its chosen covariance function.
6
Under review as a conference paper at ICLR 2021
3 MNIST Classification Experiment
We conduct a classification experiment on the MNIST handwritten digit dataset (LeCun, 1998)
making use of corresponding ReLU neural network and Gaussian process models. As in (Lee et al.,
2018), the classification task on the class labels is treated as Gaussian process regression (also known
as kriging in spatial statistics (Cressie, 1993)).
It is necessary to point out that the goal of this work is to examine using the marginal likelihood to
estimate the best available initial hyperparameter setting for neural networks, rather than determin-
ing the networks’ optimal structure.
Our experiment consists of three main steps: (A) searching within a given grid of hyperparameter
values for the pair {σW, σ2} that maximizes the log marginal likelihood function of the Gaussian
process model, (B) evaluating prediction accuracy of the corresponding neural network at each grid
point {σW ,σb} including {σW, σ2}, and (C) assessing neural network performance over all tested
hyperparameter pairs.
3.1	Procedure
The workflow for the experiment is as follows: we set up a grid map of σw2 ∈ {0.4, 1.2, 2.0, 2.8, 3.6},
σb2 ∈ {0.0, 1.0, 2.0}. Then, N samples are randomly selected from the MNIST training set to form
a training subset, where N is the training size. This is followed by computing the log marginal like-
lihood (equation 2) at each grid point. This allows us to identify the hyperparameter pair {σW, σ2}
that yields the maximum log marginal likelihood value.
On the neural network side, we build a fully-connected feedforward neural network with a single hid-
den layer width, hidden_width, of 2000 nodes, Adam optimizer, and mse loss function. Since the net-
work model is fully connected, the size of the input layer din is 28(pixels) x 28(pixels) = 784. Prior
to training, the initialization parameters {w, b} are set by sampling the distributions N(0, σw2 /din)
and N(0, σb2) for weights and biases from the input to the hidden layer, and N(0, σw2 /2000) and
N(0, σb2) for weights and biases from the hidden to the output layer. The neural network is then
trained with the training subset generated previously. We compute the model classification accuracy
on the MNIST test set and repeat the procedure over the entire grid map of hyperparameter pairs.
To investigate the usefulness of our proposed approach for assisting model initialization, we employ
He-initialization approach as a benchmark to measure numerically and graphically our neural net-
work performance over all tested hyperparameter pairs. Additionally, we check for recommendation
consistency.
3.2	Results
Applying the method described in Section 2.3 for estimating model hyperparameter pair we obtain
a consistent recommendation of (σw2 , σb2) = (3.6, 0.0).
Figure 3: Comparing MNIST training accuracy over various training sizes. We observe that the
convergence rate based on our method approaches that using He-initialization as the training size
increases. This suggests that our technique may potentially be efficient for guiding deep neural
network initialization. Left: train_size=1000. Middle: train_size=3000. Right: train_size=5000.
7
Under review as a conference paper at ICLR 2021
After running 250 training epochs, convergence of the neural network model and its prediction
accuracy are studied for different training sizes. We observe that training based on our initialization
approach converges to that based on He-initialization as the size of training samples increases, as
shown in Figure 3. This seems to suggest that our approach may be used as an efficient tool for
recommending initialization in deep learning.
It is worth noting that the Gaussian process model marginal likelihood consistently suggests the
hyperparameter pair (σw2 , σb2) = (3.6, 0). The fact that the bias variance σb2 is estimated to be 0
coincides with the assumption that bias vector being 0 in (He et al., 2015).
Table 1 lists neural network model prediction accuracy based on, respectively, our approach and
He-initialization scheme, against the best and the worst performers. The results indicate that more
frequently our approach achieves slightly better accuracy than based on He-initialization. However,
neither approach reliably gives an estimate of weight variance close to that for the best case.
Table 1: Single-hidden-layer fully-connected neural network model prediction accuracy on MNIST
test set, and associated hyperparameter pair.
Size	Best Case		Worst Case		He-Init		Ours	
	Acc.	9W ,σb2)	Acc.	(σW ,σb)	Acc.	(σW ,σ22)	Acc.	(σW ,σ2)
10000	96.85	(2, 0)	96.04	(0.4, 2)	96.85	(2, 0)	96.60	(3.6, 0)
20000	97.25	(2.8, 0)	96.70	(3.6, 1)	97.01	(2, 0)	97.09	(3.6, 0)
30000	97.50	(1.2, 0)	96.91	(2, 2)	97.07	(2, 0)	97.29	(3.6, 0)
40000	97.43	(0.4, 0)	97.16	(0.4, 2)	97.35	(2, 0)	97.42	(3.6, 0)
50000	97.71	(3.6, 0)	97.29	(0.4, 2)	97.50	(2, 0)	97.71	(3.6, 0)
4 Discussion and Future Work
In this work we propose a simple, consistent, and time-efficient method to guide the selection of
initial hyperparameters for neural networks. We show that through maximizing the log marginal
likelihood we can learn from training data hyperparameter setting that leads to accurate and efficient
initialization in neural networks.
We develop an alternative representation of the ReLU covariance function to estimate the covariance
at the output of the ReLU neural network model. We first derive the expectation of the product of
post-activations. Then, we apply the output layer activation function on the post-activation expected
value to generate the output covariance function. Utilizing marginal likelihood optimization with
the derived ReLU covariance function we perform a simulation to demonstrate the effectiveness of
Gaussian process regression.
We train a fully-connected single-hidden-layer neural network model to perform classification
(treated as regression) on MNIST data set. The empirical results indicate that applying the rec-
ommended hyperparameter setting for initialization the neural network model performs well, with
He-initialization scheme as the benchmark method.
A further examination of the results reveals consistency of the process. This implies that smaller
training subsets could be used to provide reasonable recommendation for neural network initializa-
tion on sizable training data sets, reducing the computation time which is otherwise required for
inverting considerably large covariance matrices.
The main goal of our future research is to investigate if our proposed method is adequate for deep
neural networks with complicated data sets. We wish to ascertain if consistent recommendation
could be attained by learning from larger data sets of color images via marginal likelihood maxi-
mization. We will attempt to derive or approximate multilayer covariance functions corresponding
to various activation functions. Deep fully-connected neural network models will be built to perform
classification on CIFAR-10 data set. Our hypothesis is that learning directly from training data helps
to improve neural network initialization strategy.
8
Under review as a conference paper at ICLR 2021
References
Christopher Bishop. Pattern recognition and machine learning. 2006.
Youngmin Cho and Lawrence K. Saul. Kernel Methods for Deep Learning. In Y. Bengio, D. Schu-
urmans, J. D. Lafferty, C. K. I. Williams, and A. Culotta, editors, Advances in Neural Information
Processing Systems 22, pages 342-350. Curran Associates, Inc., 2009. URL http://papers.
nips.cc/paper/3628-kernel-methods- for-deep-learning.pdf.
Noel AC Cressie. Statistics for spatial data. John Willy and Sons. Inc., New York, 1993.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. AISTATS, page 8, 2010.
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau. On the Impact of the Activation Function on
Deep Neural Networks Training. ICML, May 2019. URL http://arxiv.org/abs/1902.
06853. arXiv: 1902.06853.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving Deep into Rectifiers: Surpassing
Human-Level Performance on ImageNet Classification. arXiv:1502.01852 [cs], February 2015.
URL http://arxiv.org/abs/1502.01852. arXiv: 1502.01852.
Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Gaussian Processes for
Object Categorization. International Journal of Computer Vision, 88(2):169-188, June 2010.
ISSN 1573-1405. doi: 10.1007/s11263-009-0268-3. URL https://doi.org/10.1007/
s11263-009-0268-3.
Yann LeCun. THE MNIST DATABASE of handwritten digits, 1998. URL http://yann.
lecun.com/exdb/mnist/.
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel S. Schoenholz, Jeffrey Pennington, and Jascha
Sohl-Dickstein. Deep Neural Networks as Gaussian Processes. arXiv:1711.00165 [cs, stat],
March 2018. URL http://arxiv.org/abs/1711.00165. arXiv: 1711.00165.
David MacKay. Introduction to Gaussian processes. Citeseer, 1998.
Dmytro Mishkin and Jiri Matas. All you need is a good init. ICLR, February 2016. URL http:
//arxiv.org/abs/1511.06422. arXiv: 1511.06422.
Radford M. Neal. Bayesian Learning for Neural Networks, volume 118 of Lecture Notes in Statis-
tics. Springer New York, New York, NY, 1996. ISBN 978-0-387-94724-2 978-1-4612-0745-
0. doi: 10.1007/978-1-4612-0745-0. URL http://link.springer.com/10.1007/
978-1-4612-0745-0.
Radford M. Neal. Regression and classification using Gaussian process priors. Bayesian statistics,
6:475, 1998.
Samuel S. Schoenholz, Justin Gilmer, Surya Ganguli, and Jascha Sohl-Dickstein. Deep Information
Propagation. ICLR, April 2017. URL http://arxiv.org/abs/1611.01232. arXiv:
1611.01232.
Raquel Urtasun and Trevor Darrell. Sparse probabilistic regression for activity-independent human
pose inference. CVPR, 2008.
Mark van der Wilk, Carl Edward Rasmussen, and James Hensman. Convolutional Gaus-
sian Processes. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,
pages 2849-2858. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
6877-convolutional-gaussian-processes.pdf.
Christopher KI Williams and Carl Edward Rasmussen. Gaussian processes for machine learning,
2006.
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P. Xing. Deep kernel learning.
In Artificial Intelligence and Statistics, pages 370-378, 2016.
9
Under review as a conference paper at ICLR 2021
5 Appendix
Covariance Function at the Output of ReLU Neural Network
Our derivation follows the work on arc-cosine family of kernels developed in (Cho and Saul,
2009). However, instead of applying coplanar vector rotation in calculating the kernel integral, we
recognize that the integrand can be written in terms of two jointly normal random variables. This
helps to facilitate the computation which becomes more involved when both the weight and bias
parameters are included.
The derivation is also made to conform to the arc-cosine kernel by utilizing the identities (Cho and
Saul, 2009, equation (17), (18)) to give us
π
/2
η=0
1-----1A-------dη = π-φ,
1 cos φ cos η sin φ
π
/2
θ=0
Sin 2θ
(1 - Cos φ Sin 2θ)2
dθ
sin3φ
sin(φ) + (π - φ) cos
(4)
π
Equation (4) is derived, With the substitution η = 2(θ - ɪ), as follow:
π
f 2 —
θ=0 1
π
=∕2
η=0
∂
sin 2θ
万dθ
- Cos φ sin 2θ
Cos η
Td---7----F dη
1 - Cos φ Cos η
π
C J —L— dη
∂ cos φ η=001 — cos φ cos η
∂ π-φ -1	∂ π-φ
∂cos φ sin φ	sin(φ) ∂φ sin φ
卡(sin(φ)+(π - φ) Cos⑷).
Denote the input layer (layer 0) weight and bias parameters as bj 〜N(0, σb) and Wjlk iid N(0, dw),
where bjɪWjIk for all k ∈ {1,…，din},j ∈ {1,…，Nι}.
The expected value of the product of post-activations at the output of the j th hidden node is com-
puted as
E[Xj(x0)Xj(y0)]
/
Z
Z∞
max(bj + wj ∙ x0) max(bj + wj ∙ yo)fb0,wο (b, W) dwj dbj
∞	jj
Z∞
(bj + wjj ∙ x0) + (bj + wj ∙ yθ)+fb0,wo (b,w) dwj db
∞
(5)
Each pre-activation can be written in terms of a random variable:
din
U = bj + Wj ∙ x0 = bj + X WjOkXk 〜N(0,σ + σW ∣∣x∣∣2),
k=1
din
V = bj + Wj ∙ yj = bj + X WjOkOyj，〜N(O 次 + σWkyk2).
k0=1
10
Under review as a conference paper at ICLR 2021
Since E [U] = E[V ] = 0, their covariance can be expressed as
cov(U, V) = E[(b0 + W0 ∙ x0)(bj + W0 ∙ y0)]
din din
= E[(bj0)2] + Eh X X Wj0kWj0k0x0kyk00i
k=1 k0=1
din din
= σb2 + X X E Wj0kWj0k0 x0kyk00
k=1 k0=1
din
= σb2 + σw2 X x0kyk0
k=1
=σ2 + σW(x ∙ y)	(For simplicity we set x = x0,y = y0)
This implies that the random variables U, V have a joint Gaussian distribution:
(V)〜N(0，∑), where ∑
σb2 + σw2 kxk2
Vσ2 + σW(x ∙ y)
σb + σW(x ∙ y)
σb2 + σw2 ky k2 .
We can, therefore, rewrite equation (5) as
U U Uv C L exp ( - 1(u,v)Σ-1(u,v)T) dudv	(6)
J Jo	2π∣∑∣ 2	2
Denote D :=园=(σb + σWkχk2)(σ2 + σWl∣yk2) - (σ2 + σW(x ∙ y))2,and
Σ-1 = a11 a12
a21 a22	,
With ail = D(σb + σW∣∣yk2), a22 = -1(σb, + σW∣∣xk2), and
-1
a12 = a21 =万 (σ2 + σW (x ∙ y)).
We therefore have:
D(aιιa22- ai2) = D( D (σb + σW llyl2) D (σ2 + σW l∣x∣2)-(万 (σb + σW(X ∙y))2)
=D ((σ2 + σWkx∣2)(σ2 + σWlly12) - (σ2 + σW(x ∙ y))2)
= 1.	(by definition)	(7)
The exponential term in equation (6) then becomes:
一 2(u, v)∑-1(u, v)T = 一2 (aiiu2 + 2ai2Uv + a22v2
We now make use of the transformation from Cartesian to polar coordinates by setting
U = r cos α, v = r Sin α
√O11	√O22
=⇒ a11u2 = r2cos2α, a22v2 = r2sin2α.
The Jacobian J is calculated as
∂(u, v)	r
I ∂(r, α) I	√a11a22
11
Under review as a conference paper at ICLR 2021
Equation (6) can in turn be expressed as
1
2πD1/2
产 ∞
α=0 r=0
r2 sin2α exp( -⅛2cos2α +
2√a11a22	1 2
2ai2r2 sin a cos &十 2 . 2 Λ rdrda
√a11a22	√ √α11022
1
4πD1∕2aιιa22
J'2
α=0
sin 2αdα 广 r3exp(-2 [1 + a12 Sin %])dr
√r=0	、2	√ɑ11α22)
(8)
Next, We need to show that H :=1+ a12 Sin	≥ 0 to ensure the expression in (8) is bounded.
√α11022
First, since ∣∣x - y∣∣2 = ∣∣xk2 + ∣∣y∣∣2 - 2(x ∙ y) ≥ 0 =⇒ ∣∣xk2 + ∣∣y∣∣2 ≥ 2(x ∙ y), and let the angle
(X ∙ y ∖	1
......),we have
∣x∣∣y ∣
(σ2 + σW(X ∙ y))
卜 2 + σW ∣∣xk2)(σ2 + σW ∣∣yk2)
=	σ4 + (σW 产(X ∙ y 产 + 2σ29W)(X ∙ y)
=σ4 + (σW )2(|团[2|训2) + σ2(σW )(∣∣x∣∣2 + |加2)
=	σ4 + (媛)2(kχkkyk cos θ产 + σ29W)2(X ∙ y)
=。4 + (σW )2(|团[2|训2) + σ2(σW )(∣∣x∣∣2 + |加2)
≤ 1.
This means that we can define a quantity φ as
φ = cos-1
cos
cos
/	\ 1/2
(σb2 +σw2 ∣X∣2)(σb2 +σw2 ∣y∣2)
-1	D (σ2 + σW(X ∙ y))
D ((σ2 + σW 限炉乂甫 + σW 电|2))
-1	-a12
√αi1022
=⇒ cos φ =
一012
√α11ɑ22
(9)
This also leads to
a12 sin 2α
：=1 +——/
a11a22
-1
σ2+b + σW(X ∙ y))sin 2-
=1 +--1-----------------------------
万((σ2 + σW限炉乂苏+ σW的12))
(σ2 + σW(X ∙ y)) sin 2α
=1	/	\ 1/2
(σb2 +σw2 ∣X∣2)(σb2 +σw2 ∣y∣2)
≥ I__________(σ2 + σW(X ∙ y)_______
_	/	、 1/2
(σb2 +σw2 ∣X∣2)(σb2 +σw2 ∣y∣2)
≥0
12
Under review as a conference paper at ICLR 2021
With a change of variables, we now evaluate the integral involving the parameter r in expression
(8) as follows.
Let η = ɪH. Then r = ^^n =⇒ dr = ɪʌ^ɪn-1/2 dη. We have
/=0 (H)3η 2 e-η 2 yι后 dη
∞ 22 1
=L=O * 2ηe dη
2	∞
=祁 Z=οη e η dη
22
=H2 r(2) = H2
2
]+ a12 sin 2a)2
√a11a22)
2
1 - cos φ sin 2α
from equation (9)
The complete expression (8) becomes
4nD1/2aiia22
π
/ 2	2 sin 2ɑ
Ja= (1 — cos φ sin 2α)2
2nD1/2aiia22 sin3φ
sin(φ) + (π - φ) cos(φ) .
from equation (4)
1
1
-1	-a12
where φ = CoS 1
×√a11a22
Finally,
2πD1∕2aιιa22 sin3φ
=2πD1∕2aιιa22 (1 一 cos2φ)3/2
=2πD1∕2aιιa22 (1-----a12—) /
a11a22
=2nD1/2 (a11a22)	(a11a22 ― a22)
=2∏(D2a11a22)	/ (D(aiia22 一 a22)) /
-1/2	3/2
=2π ((σ2 + σW kxk2)(σ2 + σW l∣yk2)j	(1)	(from equation (7))
The expected value of the product of post-activations at the output of the jth hidden node in the first
hidden layer iS therefore determined to be
E[Xj(x)Xj(y)]
=21∏ ((σ2 + σWkχk2)(σ2 + σWl∣yk2)) / (sin(φ) + (π - φ) CoS(O)),
where φ = cos-1
(σ2 + σW(X ∙ y))	I
\1/2 I
+σw2 kxk2)(σb2 +σw2 kyk2)
13
Under review as a conference paper at ICLR 2021
To compute the expected value, E[Xj(x)] = J max(b + W ∙ χ)fb°,w0Jb,w) dwdb, We denote
U = b + w ∙ X 〜 N(0,σ2 + σW ∣∣xk2), and apply the change in variables:	u2 = t, where σ2 =
σb2 + σw2 kxk2 to obtain
E[Xj(x)] =	(u)+ fU (u)du
-∞
∞
1	1	- -I2u2 八
= U _______ e 2σ2 du
0	2πσ
=Z∞ σ2dt-71-e-t
0	2πσ
σ
=—:—
√2∏
_ Pσ2 + σWkχk2
√2∏
The covariance function at the network output is therefore determined to be
N1	N1	N1	N1
E [(b1 + X WijXj(χ))(b1 + X WikXk(y))] - E W + X WijXj(x)] [bl + X WikXk (y)]
j=1	k=1	j=1	k=1
Ni	1	____________ __________
=E[(b1)2] + XE[(Wj)2]E[Xj(X)Xj(y)] - 2π√σ2 + σW∣∣x∣∣2个σ + σW∣y∣2
j=1	π
=α + NN1E[Xj(X)Xj(y)] - 2∏ Jσ2 + σW∣∣x∣∣2 Jσ2 + σW∣∣y∣∣2
2	1	1
=σ2+ 2w (σ2 + kxk2σW)2 (σ2 + l∣yk2σW)2 (Sin φ + (π — φ Cosφ -1).
14