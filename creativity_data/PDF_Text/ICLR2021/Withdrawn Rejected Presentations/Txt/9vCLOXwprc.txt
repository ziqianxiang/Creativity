Under review as a conference paper at ICLR 2021
Iterated graph neural network system
Anonymous authors
Paper under double-blind review
Ab stract
We present Iterated Graph Neural Network System (IGNNS), a new framework
of Graph Neural Networks (GNNs), which can deal with undirected graph and
directed graph in a unified way. The core component of IGNNS is the Iterated
Function System (IFS), which is an important research field in fractal geometry.
The key idea of IGNNS is to use a pair of affine transformations to characterize
the process of message passing between graph nodes and assign an adjoint prob-
ability vector to them to form an IFS layer with probability. After embedding
in the latent space, the node features are sent to IFS layer for iterating, and then
obtain the high-level representation of graph nodes. We also analyze the geomet-
ric properties of IGNNS from the perspective of dynamical system. We prove
that if the IFS induced by IGNNS is contractive, then the fractal representation
of graph nodes converges to the fractal set of IFS in Hausdorff distance and the
ergodic representation of that converges to a constant matrix in Frobenius norm.
We have carried out a series of semi-supervised node classification experiments
on citation network datasets such as citeser, Cora and PubMed. The experimen-
tal results show that the performance of our method is obviously better than the
related methods.
1 Introduction
GNN (Scarselli et al., 2009) has been proved to be effective in processing graph structured data, and
has been widely used in natural language processing, computer vision, data mining, social network
and biochemistry. In recent years, GNN has developed a variety of architectures, such as GCN
(KiPf & Welling, 2017), GraPhSAGE (Hamilton et al., 2017), GAT (Velickovic et al., 2018), DGI
(Velickovic et al., 2019), GIN (XU et al., 2019), GCNn(Ming Chen et al., 2020) and GEN (Li et al.,
2020). These architectures have a common feature, that is, the rePresentation of each node is uPdated
Using messages from its neighbors bUt withoUt distingUishing the direction (or angle) of message
Passing between two nodes. Recent stUdies have shown that considering directed message Passing
between nodes can imProve the Performance of GNN and achieve sUccess in related fields. For
examPle, DimeNet (KlicPera et al., 2020) considers the sPatial direction from one atom to another
and can learn both molecUlar ProPerties and atomic forces. R-GCN (SchlichtkrUll et al., 2018) and
Bi-GCN (Marcheggiani & Titov, 2017; FU et al., 2019) are models for directed graPh, aPPlied in the
field of natUral langUage Processing. We note that the above direction based model does not consider
the bidirectional mixed Passing of messages.
BUt in real life, message Passing is interactive in different directions. For examPle, node A obtains
a message from node B. After Processing the message, node A not only Passes it to the next node
C, bUt also feeds back to node B. SUPPose there are only two directions for message Passing, for-
ward and backward, rePresented by 0 or 1, resPectively. The symbol sPace of the first generation
message Passing Path is {0, 1} = {0, 1}1, and that of the second generation message Passing Path
is {00, 01, 10, 11} = {0, 1}2. Generally, the symbol sPace of the n-th generation message Passing
Path is {0, 1}n and the size of the symbol sPace is 2n. This means that the scoPe of message Passing
sPreads with exPonent 2. However, in Bi-GCN (similar to Bi-LSTM) and R-GCN architectUres, the
symbol sPace is {{0}n, {1}n}, and its size is 2, which indicates that a lot of information will be lost
in the Process of message Passing (see APPendix A).
How to characterize the above message Passing Patterns? We Use two maPPings to rePresent message
Passing Process in two directions. Then the interactive Passing of messages in different directions
is eqUivalent to the comPosite oPeration of corresPonding maPPings. In addition, the direction of
1
Under review as a conference paper at ICLR 2021
Figure 1: Message passing patterns. Where the symbol H is the representations of all the notes.
(a) An undirected graph is transformed into a directed graph in a natural way. (b) Regardless of
direction, simply gather information from neighbors. (c) Message is passed in the same direction
(forward or backward), and get two hidden representations independently. (d) Message passing not
only occurs in the same direction, but also occurs interactively in different directions, which is more
in line with the actual situation. For example, in layer 1, node 2 passes the processed message
f1(m2) to node 1, and then, in layer2, node 1 processes the received message f1(m2) and returns
the processed message f0(f1 (m2)) to node 2.
message passing is often random, so we endow the two mappings with an adjoint probability vector
to reflect the randomness. Because the symbol space of the iterative path of the Iterated Function
System (IFS) with two mappings is also {0, 1}n and the mapping is selected with a certain prob-
ability, the iterative process of IFS is similar to the message passing process. In other words, the
above message passing pattern can be described perfectly by IFS with probabilities. We natural-
ly present the Iterative Graph Neural Network System (IGNNS), whose core layer is constructed
by IFS. Figure 1 describes the differences in message passing patterns among GCN, Bi-GCN and
IGNNS. At the same time, we regard undirected graph as a directed graph with equal probability
of bidirectional message passing (see Figure 1(a)), so the IGNNS architecture can handle directed
graph and undirected graph in a unified way.
2 Preliminaries
A graph G = (V, E) is defined by its note set V = {v1, v2, ..., vN} and edge set E =
{(vi, vj)|vi, vj ∈ V }. Let A ∈ RN denote the adjacency matrix of G, providing with relation-
al information between nodes. A[i, j] denote i, jth element of A, A[i, :] means the ith row, and
A[:, j] means the jth column. In this paper, we assume that all nodes of G are self adjacent, that
is A[i, i] = 1, i = 1, 2, ..., N. let D = diag(d1 , d2, ..., dN) be the degree matrix of A, where
di = PjN=1 A[i, j].
Neighborhood Normalization. There are two ways to normalize A. One approach is the following
mean-pooling employed by Hamilton et al. (2017) and VelickoVic et al. (2019) for inductive learning:
Amp = D-1A.
Another approach is the following symmetric normalization employed by Kipf & Welling (2017):
Asym = D- 1 AD- 2 .
Iterated Function System. A mapping f : RN → RN is said to be a contractive mapping on RN if
there exists a constant 0 < c < 1 such that kf(x1) - f(x2)k2 < ckx1 - x2 k2 for all x1, x2 ∈ RN.
An iterated function system (Hutchinson, 1981) is defined by
IFS = {RN;f1,f2,...,fm;p},
where each fi : RN → RN is a contractive mapping andp = (p1,p2, ...,pm) is an adjoint probabil-
ity vector meaning that fi is selected by probability pi for each iteration. Hutchinson (1981) showed
that there exists a unique nonempty compact set F such that
m
F= [ fi(F).
i=1
2
Under review as a conference paper at ICLR 2021
Figure 2: An overview of IGNNS. The upper part of the Figure describes how to generate two affine
transformations on R4, where we use the mean-pooling method to normalize A, p0 = 0.6, p1 = 0.4,
	1		1	0	0	∖		/	1 2	1 2	0	0	∖		1		0	0	0	∖
A =		1 0	1 1	0 1	1 1		,Ao =		0 0	1 2 0	0 1 0	1 1 2		and A1 =		1 2 0	1 1 1 3	0 1 1 3	0 0	
	∖	0	1	1	1	)		∖	0	0			)		∖	0			1 3	)
We call F the fractal set or invariant set of IFS. More conclusions on IFS can be found in the Ap-
PendiX D. It is Wen known that there exists a unique probability measure μ With support F satisfying
the equation
m
μ = X Pi μ ◦ f-1.	(I)
i=1
The probability measure μ in (1) is called the self-similar measure of IFS with probability vector p.
3 IGNNS Architecture
In this section, we will introduce the architecture of the IGNNS according to the input layer, IFS
layer, representation layer and output layer, which is described in Figure 2.
3.1	Input Layer
Given a graph structure data X ∈ RN×F of G = (V, E), called as the feature matrix of node set V .
A row of X represents the F -dimensional feature vector of a node in V . Let Wint ∈ RF×H be a
learnable parameter matrix, where H is the dimension of the latent space. Then XWint ∈ RN×H.
The output of input layer is defined by
Xint = σ(XWint) ∈ RN×H,	(2)
where σ(∙) is the activation function. Generally, ReLU(X) = max(0,x) is used as the nonlinear
activation function. Here, each column of Xint is regarded as a point in RN, so Xint is the set of H
points in RN and arranged in a certain order. The vector composed of the ith component of these
points (the ith row of Xint) is a feature representation of the ith node of graph G.
3
Under review as a conference paper at ICLR 2021
3.2	IFS LAYER
Let A be the adjacency matrix of G. Let triu(A) denote the upper triangular matrix of A and
tril(A) denote the lower triangular matrix of A. The symmetric normalization of triu(A) and
tril(A) are
_1	_ 1	_1	_1
Ao = D-2triu(A)D-2 and Ai = D-2tril(A)D-2,
where D0 and D1 are degree matrices of triu(A) and tril(A) respectively. Sometimes, we use the
mean-pooling of triu(A) and tril(A), i.e. A0 = D0-1triu(A), A1 = D1-1tril(A). Let f0, f1 be
the two affine transformations on RN , induced by A0 , A1 respectively, defined as follows:
f0 : x → A0x+b0,x ∈ RN,b0 ∈ R,f1 : x → A1x+b1,x ∈ RN,b1 ∈ R,
where b0 and b1 are learnable biases, namely add constants b0 and b1 to each component of A0x and
A1 x respectively. Constructing iterated function system
IFS = {RN;f0,f1;p},
where p = (p0, p1) is a learnable adjoint probability vector, satisfying p0 > 0, p1 > 0 and p0 +p1 =
1. Using the symbol space Ωm = {0,1}m, then for each i = (i1,i2,…，im) ∈ Ωm the length of i is
m, denoted as |i| = m, and defining Pi = PiIpi2 ・一 Pim and f = fi1 ◦ fi2 ◦・•・◦力小.Let n be
the number of iterations of IFS. For IGNNS, n is a preset parameter. The iterative process of IFS is
described as follows:
The first iteration (|i| = 1). The result of the first iteration is denoted by
H(1) ={f0(Xint),f1(Xint)} = {Hi}|i|=1,
where Hi = fi(Xint), ∀i ∈ Ωι. Since IFS selects the iteration branch f with probability pi, the
mathematical expectation of H(1) is computed by
E1 =P0f0(Xint)+P1f1(Xint) = P0H0 + P1H1 = XpiHi.
|i|=1
If choose to use bias in iterations, then H0 = A0Xint + b0, H1 = A1Xint + b1, where b0 and b1
are learnable H-dimensional vectors.
The second iteration (|i| = 2). Using the results of the first iteration as the input of the second
iteration, then the result of the second iteration is denoted by
H(2) = {f0(f0(Xint)),f0(f1(Xint)),f1(f0(Xint)),f1(f1(Xint))}
= {f00(Xint),f01(Xint),f10(Xint),f11(Xint)} = {Hi}|i|=2,
where Hi = fi(Xint), ∀i ∈ Ω2. Note that IFS selects the iteration path f with probability pi, then
the mathematical expectation of H(2) is computed by
E2 = X pifi(Xint) = X piHi.
|i|=2	|i|=2
We expand the expression of E2 and perceive its powerful feature representation ability. First,
H00 =f0(H0) =A0(A0Xint+b0)+b0,H01 =f0(H1) =A0(A1Xint+b1)+b0,
H10=f1(H0)=A1(A0Xint+b0)+b1,H11 =f1(H1) =A1(A1Xint+b1)+b1.
Then
E2 = p00H00 + p01H01 + p10H10 + p11H11
= (p00A00 + p01A01 + p10A10 + p11A11)Xint
+ (p00 A0 b0 + p01A0b1 + p10A1b0 + p11A1b1) + (p00b0 + p01b0 + p10b1 + p11b1),
where Ai = A” Ai2, ∀i = (i1,i2) ∈ Ω2.
The n-th iteration (|i| = n). Inductively, we have
H(n) = {Hi}|i|=n, Hi=fi(Xint), En= XpiHi.	(3)
|i|=n
Note that Hi ∈ RN ×H and each column of Hi is a point in RN, so we regard it as a subset of RN
with H elements. Thus H(n) is a subset of RN with H × 2n elements (including duplicate elements).
Because of Theorem 4.1, we call H(n) the fractal representation with depth n of notes.
4
Under review as a conference paper at ICLR 2021
3.3 Representation Layer
After n iterations of IFS layer, the dynamic trajectory of IFS is obtained:
O = {E1 , E2 , ..., En}.
In general, the global representation R of notes is obtained by time average or concatenation opera-
tions on O.
1n
R = — EEi ∈ RN×H or R =IIn=I Ei ∈ RN×nH,
n i=1
where I is the concatenation operator. Because of Theorem 4.2, we call Rthe ergodic representation
of notes. In practice, we adopt weighted time average or weighted concatenation. According to the
Theorem C.1, We use heuristic weights (Here, We understand it as the average expansion factor of
the distance between two points after affine transformation). Let r = Sn(N) + Y, where Y ≈
0.577215664 is the Euler constant. Suppose r = (r1, r2, ..., rn) is a learnable n-dimensional vector
with initial value r = (11 )i 1. Then the ergodic representation of notes is
n
R= XriEi ∈ RN×H or R =Iin=1 riEi ∈ RN×nH.
i=1
3.4	Output Layer
Let W out ∈ RH ×P be a learnable parameter matrix, where P is the dimension of the output layer
out	nH ×P
(such as the number of class labels). If R is generated by O concatenation, then let W ∈ R .
There are two ways to construct output layer, one is to usea Single-Layer Perception (SLP) as output,
that is
O = RWout+bout;
the other is to use f0, f1 for Mixed Propagation (MP), that is, let R0 = f0(RW out) and R1
f1(RWout), where the biases of f0, f1 are removed, then the output
O = p0R0 +p1R1 +bout.
Where the bias bout ∈ RP is an optional learnable parameter vector.
3.5	Initialization of Learnable Variables
The learnable parameters of IGNNS include input layer matrix Wint ∈ RF×H, adjoint proba-
bility vector p = (p0,p1) ∈ R2 of IFS, biases b0, b1 ∈ RH of IFS layer, weight coefficien-
t r = (r1, r2, ..., rn) of representation layer, matrix Wout ∈ RH×P of output layer and bias
bout ∈ RP of output layer. Among them, W int and W out are the required learnable parameters,
using the initialization described in Glorot & Bengio (2010); b0, b1 and bout are optional learnable
parameters with an initial value 0; p is a optional learnable parameter, for undirected graph, setting
p0 ∈ [0.5 - 0.1, 0.5 + 0.1], and for directed graph, setting (For the reasons, see Appendix G)
_ det D1	_	det Do
p0	det Do + det D1 , p1	det Do + det D1;
r is a optional learnable parameter, and its initial value as defined in 3.3. Let n be the number of
iterations of IFS. We regard n as the depth of IGNNS. Thus, IGNNS is denoted as
O = IGNNS(X, A; Wint, n, p, bo, b1, r, Wout, bout) or simply O = IGNNS(X, IFS),
where IFS is induced by relational matrix A. The output of IGNNS can be used as the input of
downstream tasks, and can also be connected to other network architectures.
3.6	Theoretical Time Complexity of IGNNS
Let n, N, H, P be defined as above. Let T(∙) denote the number of calculations of an object. For
input layer,
T(inputlayer) = NFH+ NH = O(NFH).
5
Under review as a conference paper at ICLR 2021
For IFS layer, during the iterative calculation, we will store the previous calculation results, thus
T(H(1)) = 2N2 * *H, T (H(i)) = 2×T(H(i-1)),i = 2, 3,..., n. It follows that T (H(i)) = 2iN2H, i =
1, 2, ..., n. Similarly, T ({pi ||i| = i}) = 2i, i = 1, 2, ..., n. Complete the above calculation, it is easy
to see that T (Ei) = 2iNH. Thus
n
T (IFS layer) = X T(H(i))+T({pi||i| =i}) +T(Ei)	= O(2nN2H).
i=1
It is easy to verify that T (representation layer) = O(nN H). We assume that the
is constructed by mixed propagation, then T (output layer) = O(N2P) if Wout ∈
T (output layer) = O(N2P +nNHP) ifWout ∈ RnH×P. Then
output layer
RH×P, and
T (IGNNS) = O(2nN2H + N2P) or O(2nN2H + N2P + nNHP).
In practice, for large graphs, 2nN2H N2P nNHP, thus 2nN2H is the main factor affect-
ing the time complexity of IGNNS. Furthermore, for large graphs of the same size, n is the main
important factor affecting time complexity. For citation network datasets such as citeser, Cora and
PubMed, we suggest that n ≤ 8 (see Appendix B).
4 Geometric Properties of IGNNS
The discussion here assumes that affine f0 , f1 are contractive. Otherwise, let f0 : x →
IaIf+i Aox + bo and fι : X → Ι出储+1 Aix + bι∙ In practice, IGNNS does not use Contrac-
tive affine. If contractive affine is used in IGNNS, it can be seen from the following theorems
that the characterization ability of IGNNS decreases with the increase of IFS iterations, which is
similar to the performance of Graph Convolution Network (GCN). Such a phenomenon is called
over-smoothing (Li et al., 2018b; Xu et al., 2019; Chen et al., 2020), which suggests that as the
number of layers increases, the representations of the nodes in GCN are inclined to converge to a
certain value and thus become indistinguishable. In order to overcome the over-smoothing problem
of deep GNN, people need to use some new methods such as Skip connection (Xu et al., 2018), Drop
edge (Rong et al., 2020), Residual connection (Klicpera et al., 2019a), Identity mapping (Chen et al.,
2020), Generalized message aggregation functions (Li et al., 2020) and so on. Generally speaking,
deep network may lead to the decrease of generalization performance. To analyze which type of
deep GNN would achieve better generalization performance, Xu et al. (2020) proposes a guiding
theoretical framework.
Theorem 4.1 (Fractal generation) Let H(n) = {Hi}|i|=n, which is a subset of RN with H × 2n
elements (including duplicate elements), then
dH (H(n), F) → 0, n → ∞,
where dH is the Hausdorff distance defined on H(RN), the set of all nonempty compact subsets of
RN, andF is the fractal set of IFS in IGNNS. In other words, as the number of iterations increases,
H(n) will be independent of node feature X, only related to the graph structure described by A.
Let T be the Hutchinson operator on H(RN), defined as T(B) = fo(B) S fi(B), ∀B ∈ H(RN).
Then the updated rule of H(n) satisfies
H(n) = T (H(n-i))=…=T n(H(0)),
where H(o) = {Xint} is a subset of RN with H elements. In fractal geometry, H(n) is used to draw
the fractal image on the plane. First, taking initial value H(o) = {xo}, where xo is a point in plane.
For enough n, printing all the points of H(n) on the screen to obtain the approximate fractal image.
Theorem 4.2 (Ergodic property) Let En = |i|=n piHi be the mathematical expectation of
H(n) = {Hi}|i|=n, then En converges to a constant matrix E ∈ RN×H in Frobenius norm, i.e.
limn→∞ En = E, where E[i, :] = (ei, ei, ..., ei)> ∈ RH and ei ∈ R is a constant, i = 1, 2, ..., N.
Furthermore, the time average of the dynamic trajectory O of IFS satisfies
1n
lim 一	Ei
n→∞ n
i=i
lim En = E
n→∞
and series	i∞=i ri Ei ∈ RN ×H converges in Frobenius norm.
6
Under review as a conference paper at ICLR 2021
Table 1: Summary statistics of the benchmark datasets used in the experiment.							
Dataset	Nodes	Edges	Features	Classes	Training	Validation	Testing
Cora	2708	5429	1433	7	140	500	1000
Citeseer	3327	4732	3703	6	120	500	1000
Pubmed	19717	44338	500	3	60	500	1000
Theorem 4.2 shows that as long as the number of iterations is large enough, the embeddings of nodes
will be close to linear correlation, and the representation ability of IGNNS will decline. However, in
the framework of IGNNS, because the spectral radius ρ(A0) = ρ(A1) = 1, IFS is not contractive
in general, and IGNNS still has the ability of depth feature representation.
5 Experiments
5.1	Experimental Task: Semi-supervised Node Classification
Let Z = Softmax(O), where Z ∈ RN and Softmax(∙) is the Softmax activation function, defined
as Softmax(Xi) = Z exp(x∕ with Z = Pi exp(x∕, is applied row-wise. For semi-supervised
multiclaSS claSSification, we employ the following croSS-entropy to evaluate error over all labeled
examples: L = - Pl∈Y PiP=1 Y [l, i] ln Z[l, i], where YL is the set of node indices that have labels
with P classes, Y [l, :] is a one-hot vector of size P representing the class of node l and Z[l, :] is the
row l of the matrix Z.
5.2	Experimental Setup
Datasets. In our experiment, we use three standard citation network benchmark datasets for eval-
uation, including Cora, Citeseer, Pubmed and apply the standard fixed training/validation/testing
split (Yang et al., 2016; KiPf & Welling, 2017; VeliCkovic et al., 2018) on above datasets, with 20
nodes per class for training, 500 nodes for validation and 1,000 nodes for testing. In these citation
networks, papers are represented as nodes, and citations of one paper by another are denoted as
edges. Node features are the bag-of-words vector of papers, and node label is the only one academic
topic of a paper. See Table 1 for more details.
Parameter Setting. Random seed for Tensorflow and Numpy is set to 1234. ReLU (Nair & Hinton,
2010) is used as the activation function in input layer and output layer. Dropout (Srivastava et al.,
2014) is applied to input layer, IFS layer and output layer. In representation layer, we adopt weighted
time average to get the global representation of notes. In output layer, we adopt the method of mixed
propagation to get the output of IGNNS. We use the AdamOptimizer (P.Kingma & Ba, 2015) during
training. More details of hyper-parameters are shown in Table 2. During training stage, we select
the best model to maximize the accuracy of the validation set and use early stopping with a patience
of 100 epochs.
5.3	Experimental Result
We compare with those models that strictly follow the standard of experiment setup of semi-
supervised node classification, i.e. the standard fixed training/validation/testing split (Yang et al.,
2016; Kipf & Welling, 2017) is applied on dataset. For baselines, we include recent deep GNN
models such as JKNet (Xu et al., 2018), APPNP (Klicpera et al., 2019a), Attention-based models
such as GAT (VeliCkoviC et al., 2018), AGNN (Thekumparampil et al., 2018) and H-GAT (Gulcehre
et al., 2019) , and other models such as TAGCN (Du et al., 2017) and N-GCN (Abu-El-Haija et al.,
2018). We also include three state-of-the-art shallow GNN models: Planetoid (Yang et al., 2016),
GCN (Kipf & Welling, 2017) and DGCN (Zhuang & Ma, 2018). The detailed results are shown in
Table 3.
We can see from Table 3 that the improved performance of model IGNNS in dataset Cora and
Citeseer is much higher than that in dataset Pubmed. To understand why this happens, we analyze
7
Under review as a conference paper at ICLR 2021
Table 2: hyper-parameters in experiment.
Setting	Cora	Citeseer	Pubmed
Neighborhood Normalization	symmetric	mean-pooling	symmetric
Learning rate	0.005	0.002	0.01
Initial value of p0	0.5	0.5	0.5
Dropout	0.9	0.9	0.8
Weight decay	5e-3	5e-3	5e-3
Epochs	1000	1000	1000
Hidden dimensions	48	72	72
Number of iterations in IFS layer	5	4	4
Learnable adjoint probability vector	False	True	False
Learnable representation layer coefficient	True	True	True
Use bias for IFS layer	True	False	False
Use bias for output layer	False	False	False
Table 3: Summary of classification accuracy (%) results on Cora, Citeseer and Pubmed. The results
are taken from the corresponding papers. The first value in brackets indicates the total training time
in seconds and the second value in brackets indicates the average training time in seconds per epoch.
Method
Cora
Citeseer	Pubmed
Planetoid (Yang et al., 2016)	75.7	64.7	77.2
GCN (Kipf & Welling, 2017)	81.5	70.3	79.0
GAT (VeIiCkOviC et al., 2018)	83.0	72.5	79.0
TAGCN (Du et al., 2017)	83.3	71.4	81.1
JKNet (Xu et al., 2018)	81.1	69.8	78.1
AGNN (Thekumparampil et al., 2018)	83.1	71.7	79.9
N-GCN (Abu-El-Haija et al., 2018)	83.0	72.2	79.5
DGCN (Zhuang & Ma, 2018)	83.5	72.6	80.0
APPNP (Klicpera et al., 2019a)	83.3	71.8	81.1
H-GAT (Gulcehre et al., 2019)	83.5	72.9	-
IGNNS (ours)	86.3(44s, 0.17s)	75.1(65s, 0.16s)	80.5(221s, 1.47s)
the characteristics of these citation networks. We consider two statistical properties of networks,
one is the Network Density d(G), which is defined as d(G) = N萧-、), where N is the number of
nodes and L is the number of edges, and the other is the Average Clustering Coefficient C, which
is defined as C = N Pi∈v Ci, where V is the set of nodes, Ci = k送-1),k is the number of
the neighbors of node vi and ei is the number of undirected edges between ki neighbors. The small
Network Density means the strong global sparsity of the network, and the small Average Clustering
Coefficient means the strong sparsity of the neighbors of nodes. The calculation results of the
statistical characteristics of the network are shown in Table 4. We can see from Table 4 that Pubmed
is more sparse than Cora and Citeseer. The performance of IGNNS benefits from the bidirectional
mixed propagation of information between nodes, but this sparsity weakens the gain of IGNNS.
5.4	Performance of completely linear IGNNS
In Nonlinear IGNNS, we use the nonlinear activation function ReLU(x), learn adjoint probability
vector n — (nn n.) bv ∏c √________ReLU(PO)+0.1	√________ReLU(PI)+0.1	and learn the reη
Vector P = (p0,p1) Dy p0 J ReLU(Po)+ReLU(pι)+0.2，P1 J ReLU(Po )+ReLU(pι ) + 0.2 and learn the rep
resentation layer coefficient r = (r1,r2,…,rn) by Iri J ReLU(%) with initial value r = (11)i
where r =，ln(N) + 0.577215664. In this experiment, to get a completely linear IGNNS, We let
all the activation functions be the identity function, i.e. σ(x) = x, and let the adjoint probability
vector p = (p0,p1) and the representation layer coefficient r = (r1, r2, ..., rn) be hyperparameters
8
Under review as a conference paper at ICLR 2021
Table 4:	Statistical characteristics of the networks. Bold for minimum.
Statistical characteristics Cora	Citeseer Pubmed
Network Density	0.00144000	0.00084514 0.00022805
Average Clustering Coefficient 0.24067330	0.14147102 0.06017521
Table 5:	Performance of completely linear IGNNS on Cora, Citeseer and Pubmed.
Method	Cora	Citeseer	Pubmed
GCN (Kipf & Welling, 2017)	81.5	70.3	79.0
IGNNS(Linear)	83.9	72.4	79.9
without learning. For Citeseer, we let p0 = 0.6, and for Pubmed, we use bias for IFS layer. Ex-
cept for the above changes, experimental task and the other experimental setups remain unchanged
as showed in section 5.1 and 5.2 respectively. We can see from Table 5 that the performance of
completely linear IGNNS is better than that of baseline model GCN. Compared with other models,
completely linear IGNNS is still competitive. This is due to the fact that the IFS can extract more
features than spectral filters. For more discussion, Let A be the normalization adjacency matrix of
graph G, and let A0 and A1 be defined as in section 3.2. We further assume that the dimension
of the hidden space is equal to 1. This means that the input of GNN (GCN or IGNNS) is a point
x0 = Xint = XWint ∈ RN×1. Let n be the depth of GNN, for IGNNS, equal to the number
of iterations of IFS. For convenience, we ignore the activation function and parameter matrix. Let
f(x) = Ax + b, f0(x) = A0x + b0 and f1(x) = A1x + b1. For GCN, the message passing results
are
{f(x0)}, {f* 2 * * * 6(x0)},..., {fn(x0)}.
We see that each iteration only gets one value, i.e. |{fn(x0)}| = 1. For IGNNS, the message
passing results are as follows:
{f0(x0),f1(x0)},{f0(f0(x0)),f0(f1(x0)),f1(f0(x0)),f1(f1(x0))},...,{fi(x0)}|i|=n.
If f0 and f1 satisfy separation condition, i.e. f0(x) = f1(y) implies x = y, then |{fi(x0)}|i|=n| =
2n . This means that IGNNS can extract more information than GCN. Even if f0 and f1 are con-
tractive mappings, by Theorem 4.1, we have {fi(x0)}|i|=n → F, where F is the fractal set of IFS
induced by f0 and f1 . Generally speaking, F is a uncountable compact set. This means that when n
is large, the features may still be distinguishable.
6 Conclusion
In this paper, we propose a new framework of graph neural networks, IGNNS, which give a con-
nection between Graph Neural Networks and Iterated Function System. We use IFS to simulate the
bidirectional message passing process of graph neural network, and obtain the fractal representa-
tion and ergodic representation of graph nodes, which are very helpful for downstream tasks. The
experiments show that we have achieved good results in semi-supervised node classification task.
Interesting directions for future work include pruning the iterative path space {0, 1}n to reduce the
computational complexity, coding graph structured data with IFS, and establishing more interesting
connections between IFS and graph neural networks.
References
Sami Abu-El-Haija, Amol Kapoor, Bryan Perozzi, and Joonseok Lee. N-GCN: multi-scale graph
convolution for semi-supervised node classification. CoRR, abs/1802.08888, 2018. URL http:
//arxiv.org/abs/1802.08888.
M.F. Barnsley. Fractals Everywhere. Academic Press, 1988.
9
Under review as a conference paper at ICLR 2021
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In International Conference on Machine Learning, 2020.
Jian Du, Shanghang Zhang, Guanhang Wu, Jose M. F. Moura, and Soummya Kar. Topology adaptive
graph convolutional networks. CoRR, abs/1710.10370, 2017. URL http://arxiv.org/
abs/1710.10370.
J.H. Elton. An ergodic theorem for iterated maps. Journal of Ergodic theory and Dynamical Systems,
7:481-488, 1987.
K. Falconer. Fractal Geometry: Mathematical Foundations and Applications. Wiley, 1990.
Tsu-Jui Fu, Peng-Hsuan Li, and Wei-Yun Ma. GraphRel: Modeling Text as Relational Graphs for
Joint Entity and Relation Extraction. Association for Computational Linguistics, July 2019. URL
https://www.aclweb.org/anthology/P19-1136.
Xavier Glorot and Yoshua Bengio. Understanding the diffificulty of training deep feedforward neural
networks. In AISTATS, volume 9, pp. 249-256, 2010.
Caglar Gulcehre, Misha Denil, Mateusz Malinowski, Ali Razavi, Razvan Pascanu, Karl Moritz
Hermann, Peter Battaglia, Victor Bapst, David Raposo, Adam Santoro, and Nando de Freitas.
Hyperbolic attention networks. In ICLR, 2019.
William L. Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NIPS, 2017.
J. Hutchinson. Fractals and self-similarity. Indiana University Journal of Mathematics, 30:713-747,
1981.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. In International Conference on Learning Representations (ICLR), 2017.
J. Klicpera, A. Bojchevski, and S. Gunnemann. Predict then propagate: Graph neural networks meet
personalized pagerank. In ICLR, 2019a.
Johannes Klicpera, Janek Groβ, and Stephan Gunnemann. Directional message passing for molec-
ular graphs. In International Conference on Learning Representations (ICLR), 2020.
Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. Deepergcn: All you need to train
deeper gcns. 2020.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In AAAI, 2018b.
Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for
semantic role labeling. In EMNLP, 2017.
P.R. Massopust. Fractal Functions, Fractal Surfaces, and Wavelets. Academic Press, 2017.
Zhewei Wei Ming Chen, Bolin Ding Zengfeng Huang, and Yaliang Li. Simple and deep graph
convolutional networks. 2020.
V. Nair and G. E Hinton. Rectified linear units improve restricted boltzmann machines. In ICML,
2010.
Diederik P.Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimizaiton. In ICLR,
2015.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In International Conference on Learning Repre-
sentations, 2020. URL https://openreview.net/forum?id=Hkx1qkrKPr.
F. Scarselli, M. Gori, A.C. Tsoi, M. Hagenbuchner, and G. Monfardini. The graph neural network
model. IEEE Trans. Neural Netw. Learn. Syst, 20(1):61-80, 2009.
10
Under review as a conference paper at ICLR 2021
M.	Schlichtkrull, Thomas Kipf, P. Bloem, R. V. Berg, Ivan Titov, and M. Welling. Modeling rela-
tional data with graph convolutional networks. ArXiv, abs/1703.06103, 2018.
N.	Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way
to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):
1929-1958, 2014.
Kiran K. Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based Graph Neural
Network for Semi-supervised Learning. arXiv e-prints, art. arXiv:1803.03735, March 2018.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph Attention Networks. International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=rJXMpikCZ. accepted as poster.
Petar VeliCkoviC, William Fedus, William L. Hamilton, Pietro Lio, Yoshua Bengio, and R Devon
Hjelm. Deep Graph Infomax. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=rklz9iAcKQ.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken iChi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks. In ICML, 2018.
KeyUlU XU, WeihUa HU, and JUre LeskoveC. How powerfUl are graph neUral networks. In ICLR,
2019.
KeyUlU XU, Jingling Li, Mozhi Zhang, Simon S. DU, Ken iChi Kawarabayashi, and Stefanie Jegelka.
What Can neUral networks reason aboUt? 2020.
Zhilin Yang, William W. Cohen, and RUslan SalakhUtdinov. Revisiting semi-sUpervised learning
with graph embeddings. In International Conference on Machine Learning, 2016.
Chenyi ZhUang and Qiang Ma. DUal graph ConvolUtional networks for graph-based semi-sUpervised
ClassifiCation. In WWW, 2018.
A THE FRACTAL REPRESENTATION OF GRAPH G WITH ONLY ONE SELF
ADJACENT NODE v.
For the sake of disCUssion, we assUme that the dimension of the hidden spaCe is eqUal to 1. We
assUme that messages are sent from node v, propagate in two direCtions (CloCkwise and antiCloCk-
wise), and are finally reCeived by node v. The reCeived messages in the CloCkwise direCtion beCome
one-third of the original, and the reCeived messages in the antiCloCkwise direCtion beCome one-third
of the original plus a constant of 3. It is expressed by mathematical formula as follows
x	x2
f0(X) = 3,	fι(X) = 3 + 3, X ∈ r.
For Bi-GCN, messages are delivered independently in both directions. In other words, there are two
independent channels, and the message passing (transmitting or receiving) can only be carried out
by their own channels. Let X0 ∈ R be the initial message. In the clockwise direction, after n passes,
the received messages are
X0 X0 X0	→ 0
In the anticlockwise direction, the received messages are
X0	2 X0	2	2	X0	n 2
T + 3,32 + 32 + 3,…,3n + ^3i T 1.
i=1
For IGNNS, the two channels have a connection point at node v. First, node v sends the message X0
in both directions, and the connection point of node v will receive two messages {f0(X0), f1(X0)}.
In the second launch, any message (f0(X0) or f1(X0)) can be sent in both directions, so the received
messages are
{f0(f0(x0)),f0(f1(x0)),f1(f0(x0)),f1(f1(x0))}.
11
Under review as a conference paper at ICLR 2021
①6tπss①IU P①>a。①工
	Initial message x0 = 2
1.4 ■	
	•	∙ Bi-GCN (Clockwise)
1.2 -	• Bi-GCN (Anticlockwise)
	.	∙ Fractal representation of IGNNS
1.0 -	?	♦	t	t	♦	♦
	:	I	I	I	I
0.8 -	ɪ	ɪ	I	I	I	I
	•	:	I	I	I	I
0.6 -	
	
0.4 -	
	:	I	I	I	I
0.2 -	∙	:	I	I	I	I
	•	ɪ	I	I	I	I
0.0 -	«	i	A	A	A
	12345678
	Initial message x0 = 1/4
1.0 -	∙	f	f	f	t
0.8 -	•	∙	:	：	I	I	I
	•	∙	：	I	I	I	I
	•	・	:	：	I	I	I
0.6 -	• Bi-GCN (Clockwise)
	• Bi-GCN (Anticlockwise)
0.4 -	• Fractal representation of IGNNS
	•	:	I	I	I	I
0.2 -	・	:	：	I	I	I
	•	∙	：	I	I	I	I
0.0 -	
	12345678
	Depth of network
Figure 3: Comparison of feature extraction ability between Bi-GCN and IGNNS. Bi-GCN gets
boundary messages and IGNNS gets all messages.
In summary, After n passes, the received messages are
H(n) = {fi(x0)}|i|=n → C,
where C is the famous Cantor Set. This means that we have not only received one message, but
2n , since f0 and f1 satisfy the separation condition. We can see from Figure 3 that Bi-GCN gets
boundary messages and IGNNS gets all messages. Letp = (p0,p1) be the adjoint probability vector,
then the mathematical expectation En of H(n) is P|i|=n pifi(x0). We interpret En as the average
of all messages received. Now the question is, fractal representation gets enough messages, but is
there redundancy in these messages? How to select the valid message from the fractal representation
becomes the focus of our research in the next stage.
B	Analysis of time complexity on Cora, Citeseer and Pubmed.
From section 3.6, we have known that the time complexity in Experiment 5.1 is O(2nN2H +N2P),
where n is the number of iterations of IFS (the depth of IGNNS), N is the number of nodes, H is
the dimension of the latent space and P is the dimension of the output layer. In this section, we
compare the real running time (100 epochs) of IGNNS on Cora, Citeseer and Pubmed. Let H = 8,
then 2nN 2H is the main factor affecting the time complexity of IGNNS. Let the depth of IGNNS
12
Under review as a conference paper at ICLR 2021
hidden=8, epoch=100
(S)①UJ--M6U-U-Π3J1
Figure 4: Real training time on Cora, Citeseer and Pubmed.
go from 1 to 8, and compare the real training time on Cora, Citeseer and Pubmed. We can see from
Figure 4 that the actual results are basically consistent with the theoretical analysis results.
C Frobenius Norm of Matrix
Theorem C.1 Let A ∈ RN ×N be the adjacency matrix of a graph with no weights, i.e. Ai,j = 1
if there exists an edge i → j in the graph and Ai,j = 0 otherwise, and A1 = D1-1tril(A) (or
_ 1	_ 1	'
A1 = D1 2 tril (A)D-2 ) as defined in IGNNS, then
IIAI IlF ≥ Pln(N) + γ,
where γ ≈ 0.577215664 is the Euler constant.
Proof. Case 1: A1 = D1-1tril(A). Let tril(A) = (aij)N×N, D1 = diag(d1, d2, ..., dN) be the
degree matrix of tril(A) and A1 = (bij)N×N. Note that tril(A) is an lower triangular matrix, then
di = PjN=1 aij = Pij=1 aij. Since aij ∈ {0, 1}, we have i ≥ di. Computing the Frobenius Norm
of A1 as follows:
NN	Ni	Ni	2
kAιkF = XXb2j = XXb2j = XX (j .	(4)
i=1 j=1	i=1 j=1	i=1 j=1	i
∀i ∈ {1, 2, ..., N}, note that di elements in {aij}ij=1 are 1 and the rest are 0. It follows that
X(W) =G) ×Xa2j=G) ×di=I≥τ	⑸
13
Under review as a conference paper at ICLR 2021
It follows from (4) and (5) that
N1
kA1kF ≥ X i∙
i=1
So k Ai ∣∣f ≥ JPN=I i ≈ Pln(N) + Y, where Y ≈ 0.577215664 is the EUler constant.
_ 1
_1
Case 2: Ai = D- 2 tril(A)D- 2. Computing the FrobeniUs Norm of Ai as follows:
NN	Ni	Ni 2
IAikF = XX b2j = XX b2j = XX 言.
i=i j=i	i=i j=i	i=i j=i i j
∀i ∈ {1, 2, ..., N}, it follows from j ≥ dj that
i a2	1 i 1
=1 成 ≥ i X 1 ∙ a2j.
(6)
(7)
Note that di elements in {aij}ij=i are 1 and the rest are 0. It follows from Rearrangement inequality
that
X12	1	2	1	2	12
Kj aij ≥ i-di + 1 ai(i-di+i) + i-di + 2 a"+2)+	+ 7 aii,	⑻
Where ai(i-di + i) = ai(i-di+2) = ∙∙∙ = αii = 1. ThuS
i1
X j ∙ a：
j=i
• 12 + ɪ ∙ 12 + ∙∙∙ + ɪ ∙ 12
di X —.
(9)
1
卷≥ i
It follows from (6), (7) and (9) that
N1
IAikF ≥ X 丁
i=i
Which completes the proof.
D Introduction to Iterated Function System
In order to prove Theorem 4.1 and Theorem 4.2, we will briefly introduce the relevant conclusions
on IFS in this section, and we will not give the proof here. More details of IFS Theory can be found
in Hutchinson (1981); Elton (1987); Barnsley (1988); Falconer (1990); Massopust (2017).
D. 1 Fractal Space
Let (X; d) be a complete metric space. Let H(X) denote a set consisting of all nonempty compact
subsets ofX. Hausdorff distanceon dH on H(X) defined by
dH (A, B) = max{max min d(a, b), max min d(a, b)}, ∀A, B ∈ H(X).	(10)
a∈A b∈B	b∈B a∈A
Theorem D.1 (H(X); dH) is a complete metric space.
We call (H(X); dH) a Fractal space. Let {fi}in=i be a set of mappings on (X; d). Hutchinson
operator T : (H(X); dH) → (H(X); dH) defined as
n
T(B) = [fi(B),∀B ∈H(X).	(11)
i=i
Theorem D.2 If {fi}in=i is a set of contractive mappings on (X; d), then Hutchinson operator T is
a contractive mapping on (H(X); dH).
14
Under review as a conference paper at ICLR 2021
D.2 Markov Operator of IFS
Let (X; d) be a complete metric space. Let M(X) be the set of all probability measures on X.
Let C(X) be the set of all continuous functions mapping X to R. We say that f ∈ Lip1, if
|f(x) — f (y)| ≤ d(x, y), ∀x, y ∈ X. It is easy to see that if f ∈ Lip1 then f ∈ C(X). Hutchinson
metric dM on M defined as
dM(μ, V) = sup
ZX
fdμ — I
X
fdν ∣f ∈ Lipl ,∀ ∀μ,ν ∈M(X).
(12)
X
Theorem D.3 (M(X); dM) is a complete metric space.
Let IFS = {X; f1, f2, ..., fn;p}, the Markov operator M : M(X) → M(X) of IFS defined as
n
Mμ = XPiμ ◦ f-1,μ ∈ M(X).
i=1
(13)
Theorem D.4 Markov operator M of IFS is a contractive mapping on space (M(X); dM).
Let measure sequence {μi} ⊂ M and μ ∈ M,we call {μi} weakly convergent to μ if the following
equation holds:
UI	il→∞ IX fdTx fdμ, ∀f ∈C(X),
denoted as μ力 → μ as i → ∞.
(14)
TheoremD.5 If μi —→ μ as i → ∞, then μi → μ as i → ∞.
E The Proof of Theorem 4.1
Proof. By Theorem D.1, (H(RN); dH) is a complete metric space. Let T be the Hutchinson
operator on H(RN), defined as T(B) = f0(B) S f1(B), ∀B ∈ H(RN). By Theorem D.2, T is a
contractive mapping on (H(RN); dH). It follows from the Banach fixed point theorem that there
exits a unique compact subset F ofH(RN) such that
F=T(F) =f0(F)[f1(F),
which implies that F is the fractal set of IFS. Further more, ∀B ∈ H(RN), we have Tn(B) —→ F.
The above convergence is independent of the choice of initial value. Thus, let H(0) = {Xint} =
{x1, x2, ..., xH}, xi ∈ RN, we have
Hs) = τ(HST)) = ... = τn(H⑼)-→ F.
The above result indicates that when n is large enough, H(n) is close to the fractal set F of IFS in
the sense of Hausdorff distance, and has nothing to do with the choice of initial value H(0).
F	The Proof of Theorem 4.2
Proof. It suffices to prove that ∀j ∈ {1, 2, ..., H} , En[:, j], the j column of En, satisfies
/ eι ∖
e2
lim En[:,j]=	.	.
n→∞	.
.
eN N×1
For this purpose, let xj be the j column of Xint as defined in (2), then xj is a point in RN. Define a
Dirac measure as follows:
δx(B) = 1 x∈B,	(15)
x	0 other.
15
Under review as a conference paper at ICLR 2021
It is easy to see that δx ∈ M(RN). The Markov operator M : M(RN) → M(RN) of IFS, defined
as
1
Mμ = XPiμ ◦ f-1,μ ∈ M(RN).	(16)
i=0
Now take μo = δχj, and the results of iterative calculation are as follows:
11	1
μι = Mμo = Epiμo	◦ f-1	= Epi乐◦ f-1	= fpiδfiaj)= E2防⑶).
i=0	i=0	i=0	|i|=1
11
μ2 = M2μo = Mμι = Epiμι ◦ f-1 = Epi(E pSfi(χj)) ◦ f-1
i=0	i=0	|i|=1
= p0p0δf0(f0(xj)) +p0p1δf0(f1(xj)) +p1p0δf1(f0(xj)) +p1p1δf1(f1(xj))
=	piδfi(xj )
|i|=2
Inductively, we have
μn = Mnμθ = X Piδfi(xj).	(17)
|i|=n
By Theorem D.4, it follows from the Banach fixed point theorem that there exits a unique probability
measure μ* such that
dM
μn	→ μ* .
The above μ* is actually the self-similar measure of IFS. By Theorem D.5, We have μn → μ*, i.e.
lim
n→∞
/ Fdμn
/ Fdμ*, ∀F ∈ C(RN).
It follows from (17) and (3) that
/ Fdμ*
lim
n→∞
/ Fdμn
nl→im∞ Fd( X piδfi(xj))
|i|=n
lim	piF(fi(xj))
n→∞
|i|=n
lim	piF(Hi[:,j]), ∀F∈C(RN).
n→∞
|i|=n
(18)
In (18), ∀i ∈ {1, 2, ..., N}, take the continuous function Fi to satisfy
Fi(t) = ti, ∀t = (t1, t2, ..., tN)> ∈ RN.
It follows from (18) that ∀i ∈ {1, 2, ..., N},
lim En [i, j]
n→∞
lim	piHi[i,j] = lim	piFi(Hi[:,j])
n→∞	n→∞
|i|=n	|i|=n
/ Fi(t)dμ*(t) = / tidμ*(t) = Ltidμ*(t)=
ei
(19)
Which combined with simple mathematical analysis technology completes the proof.
16
Under review as a conference paper at ICLR 2021
G How to Set the Initial Value of Adjoint Probability Vector?
The geometric meaning of matrix determinant det A is the expansion factor of graph volume under
linear transformation A. Let F be the fractal set of IFS as defined in IGNNS. Then
F=f0(F)[f1(F).
Note that
A = volumefi(F))
i = VolUme(F)
i =0,1.
It can be seen that if the valUe of det Ai is large, it reflects thatfi(F) has a large share in F. therefore,
when selecting the iterative fUnction, fi shoUld have a greater probability of being selected. So set
det A0
P0 det Ao + det Ai
det A1
Pi =-----------------;
det A0 + det A1
Note that A0, Ai are triangUlar matrixes and the diagonals of triu(A), tril(A) are eqUal to 1. It
_1	_1
follows from Ao = D-Itril(A) (or Ao = D0 2 tril(A)Do 2) that det Ao = detD. Similarly,
det Ai = , ɪ . It follows that
i	detD1
_	det Di	_	det Do
p0	det Do + det Di , p1	det Do + det DJ
where Do and Di are degree matrices of triu(A) and tril(A) respectively.
17