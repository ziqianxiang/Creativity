Under review as a conference paper at ICLR 2021

PAC-BAYESIAN  RANDOMIZED  VALUE  FUNCTION
WITH  INFORMATIVE  PRIOR

Anonymous authors

Paper under double-blind review

Ab stract

Randomized value function has been shown as an effective exploration strategy
for reinforcement learning (RL), which samples from a learned estimation of the
distribution over the randomized Q-value function and then selects the optimal
action.  However, value function methods are known to suffer from value estima-
tion error.  Overfitting of value function is one of the main reasons to estimation
error. To address this, in this paper, we propose a Bayesian linear regression with
informative prior (IP-BLR) operator to leverage the data-dependent prior in the
learning process of randomized value function, which can leverage the statistics of
training results from previous iterations.  We theoretically derive a generalization
error bound for the proposed IP-BLR operation at each learning iteration based
on PAC-Bayesian theory, showing a trade-off between the distribution obtained
by IP-BLR and the informative prior.  Since the optimal posterior that minimizes
this generalization error bound is intractable, we alternatively develop an adaptive
noise parameter update algorithm to balance this trade-off. The performance of the
proposed IP-BLR deep Q-network (DQN) with adaptive noise parameter update
is validated through some classical control tasks.  It demonstrates that compared
to existing methods using non-informative prior, the proposed IP-BLR DQN can
achieve higher accumulated rewards in fewer interactions with the environment,
due to the capabilities of more accurate value function approximation and better
generalization.

1    INTRODUCTION

Deep reinforcement learning (RL) has demonstrated outstanding performance over many challeng-
ing tasks (Mnih et al., 2015; Silver et al., 2018).  However, the design of an efficient exploration
strategy for the intractably large state and action spaces still remains a main challenge. Noise 
injec-
tion is a commonly used exploration strategy in deep RL. For example, the ϵ-greedy strategy that
takes action uniformly with probability ϵ is effective for tabular and linear function 
approximation-
based Q-learning, and has also succeeded in deep Q-network-based methods (Mnih et al., 2015).
By adding random perturbation into the coefficients of value function, randomized value function
method has been introduced for exploration. In standard RL setting, the value function is 
determinis-
tic w.r.t. the state and action, which explilcitly indicates how valuable a certain state or action 
is. Pol-
icy built on randomized value function, on the other hand, can behave stochastically and be induced
to a more efficient exploration. Using linear function approximation, randomized least-squares value
iteration (RLSVI) (Osband et al., 2019) leverages Bayesian linear regression (BLR) (Bishop, 2006)
to learn an approximation to the posterior distribution of the randomized Q-value function. In deep
RL, randomized noise is injected into the parameters of neural network, while the noise parameter
is trained together with the network (Fortunato et al., 2017; Plappert et al., 2017).

RL algorithms are known to suffer from value estimation error, which leads to suboptimal policy.
It has been proved by Hasselt et al. (2016) that estimation error of any kind can lead to overesti-
mation of action’s value in Q-learning. In actor-critic methods, the overestimation bias exists in 
the
approximation of critic (Fujimoto et al., 2018).  In randomized value function method,  the value
estimation error is also inevitable.  One of the main reasons for the occurrence of estimation error
in function approximation is overfitting on a finite training set by minimizing the empirical error
(risk).  If we can learn a regressor by minimizing the expected error (risk) of the true 
distribution,
e.g., ideally in the setting of supervised learning, then the regressor can be generalized to the 
unseen

1


Under review as a conference paper at ICLR 2021

data (Bishop, 2006).  The distance between the empirical risk and true risk is called the 
generaliza-
tion error (Germain et al., 2016).  Theoretically, the generalization error can be reduced to zero 
by
minimizing directly on the true risk, which, however, is usually infeasible since the true 
distribution
is unfortunately not available in practice.

Fortunately, by assuming a distribution over coefficients, we can bound the generalization error by
PAC-Bayes. In classification tasks, algorithms derived from PAC-Bayesian theory have been proved
to guarantee a tight generalization error bound (Germain et al., 2009). PAC-Bayes provides an upper
bound for generalization error in terms of the empirical risk and Kullback-Leibler (KL) divergence
between the prior and posterior distributions, which prevents the posterior of coefficients from 
being
updated to overfitting by minimizing the empirical risk. In previous works, the assumption of prior 
is
often non-informative, such as fixed Gaussian distribution (Germain et al., 2016). Through a careful
design, however, the prior distribution can also be optimized for a better generalization. For 
example,
previous works that use data-dependent priors have achieved a tighter bound and better performance
on classification tasks   (Parrado-Herna´ndez et al., 2012),  where the data-dependent prior can be
obtained by pre-learning on a part of the whole training dataset.  This motivates us to incorporate
PAC Bayesian theory with informative prior into the batch RL algorithms, since iterative training
does provide rich data-dependent prior information that can be exploited to reduce the 
generalization
error incurred by function approximation.

In  this  paper,  we  propose  to  use  informative  prior  for  the  posterior  update  in  
randomized  value
function exploration. To further reduce the generalization error incurred by function approximation,
we propose to optimize an upper bound provided by employing PAC-Bayesian theory during the
coefficient update. Our main contributions are as follows.

We propose to use the Bayesian linear regression with an informative prior (IP-BLR) distribu-
tion to learn the distribution estimation of the randomized Q-value function.  For each IP-BLR
iteration, we construct the informative prior by using posterior from the previous iteration.

We  derive  an  upper  bound  for  the  generalization  error  in  each  IP-BLR iteration  based  
on  the
PAC-Bayesian theory.  This generalization error bound shows a trade-off between the posterior
of IP-BLR operation and the informative prior distribution.   We further provide an analogy to
the optimal Gibbs posterior (Alquier et al., 2016) that minimizes this generalization error bound,
which is unfortunately intractable and not piratically feasible in the training process.

Alternatively, we aim to manipulate the noise parameter β to balance the IP-BLR posterior and
the prior in the final posterior distribution, based on which propose a practical deep RL algorithm
called IP-BLR DQN.

Through  extensive  experiments,  we  demonstrate  that  the  proposed  IP-BLR  DQN  presents  a
quicker jumpstart for the learning process and significantly improves the asymptotic performance
than the randomized value function baselines, which mainly benefits from the proposed informa-
tive prior and adaptive noise parameter update.

2    BACKGROUND

Reinforcement Learning: In RL setting, the environment is modeled as an MDP, denoted by a tuple

<    ,    ,    ,     > with state space    , action space    , reward space      and environment 
dynamic    .
An agent improves its policy π by interacting with the environment.  At each time step t, the agent
observes the state st  ∈  S  and takes an action at  ∈  A  according to its current policy π(at|st) 
 :
S  → A. Afterwards, the agent will receive a reward r(st, at) : S × A → R. Then the state of the
vironment changes to the next state st₊₁ with probability P (st₊₁|st, at) : S × A × S  →  [0, 1],

[0, 1] is a discount factor.  The state-action value function Q is defined as the expectation of 
return
Q(st, at)  =  E      ∞k=t γᵏ−ᵗr(sk, ak) st, at  ,  with the following Bellman equation specifying 
the
relationship between the current state-action value and its successor:

Q(st, at) = E [r(st, at))] + γEP,π[Q(st₊₁, at₊₁)].                                  (1)

2


Under review as a conference paper at ICLR 2021

The optimal state-action value function Q∗ is given under the optimal policy π∗ that solves the RL
problem. Then the Bellman optimality operator T  is given by:

∗Q(st, at) = E [r(st, at)] + γEP  max Q(st₊₁, at₊₁)  ,

at+1

which  has  been  proved  to  be  a  contraction  mapping  with  a  unique  fixed  point  at  Q∗.   
It  hence
guarantees the convergence of Q-learning based approaches.

PAC-Bayesian Theory:  Suppose that we have M  training samples {(Xi, Yi)}M     ∈  (X  × Y)M ,


where

1       2              p  T  and

i=1

Xi  =  (Xi , Xi , ..., Xi )           Yi is a one-dimensional label.  In the context of Q-learning,

X  corresponds to S  × A  and Y  is the temporal difference (TD)-target.   The goal of supervised
learning is to learn a predictor f  ∈  F,  where F  is the function space :  X   →  Y.   To quantify
the prediction performance, a loss function l  :  F  × X  × Y  →  R is given.  Hence the empirical

risk  RˆA(f )  is  defined  as  the  average  loss  of  these  M  samples  RˆA(f )  =    ¹  ΣM    
l(f (Xi), Yi).

Suppose  that  we  have  the  underlying  data  distribution      over  (    ,    ),  the  true  
risk  is  given  by

RA(f )  =  E₍X,Y ₎     [l(f (X), Y )], which measures the predictor f ’s generalization 
performance.

During  the  training  process,  we  only  have  access  to  the  empirical  risk  that  is  used  
to  train  this
predictor, which may cause overfiting. PAC-Bayesian theory provides an appealing oracle inequality
that bounds the true risk with empirical risk, given in the following (Alquier et al., 2016).

Theorem  1.  For  a  given  distribution  D  over  X  ×  Y,  any  prior  distribution  Jf  and  
posterior
distribution  Pf  over  the  function  space  F,  a  non-negative  parameter  λ,  and  a  loss  
function
l             : F × X  × Y  → R, with a probability of at least 1 − δ we have:

E    [R  (f )] ≤ E    [Rˆ  (f )] +     ΣD     (P  ||J  ) + ln     Σ ,                            
(2)

1                                    C

								

where DKL(Pf ||Jf ),denotes the Kullbac,k-Leibler (KL) divergence between Pf and Jf , and C  =

  

3    DEEP  BLR-Q LEARNING  WITH  INFORMATIVE  PRIOR

In randomized least-squares value iteration (RLSVI) (Osband et al., 2019), following the basic as-
sumption of Bayesian linear regression (BLR) (Bishop, 2006), the true Q-value qs,ₐ is considered as
the regression from a linear regression model with Gaussian noise ϵ ∼ N(0, β−¹):

qs,ₐ = Q(s, a) = wTφθ(s, a) + ϵ,                                               (3)
where  φθ(s, a)  is  the  feature  vector  extracted  by  the  deep  neural  network  with  weights 
 θ,  and

w  denotes  the  model  parameter  vector  with  dimensionality  of  Ω.   For  the  ease  of  
notation,  we

interchangeably use φ(s, a) and φθ(s, a) throughout this paper.  Suppose that we have a batch of


transitions {(si, ai, ri, s′i)}M

for current training iteration. The TD-target value yi ∈ Y  is

yi = ri + γ max wTφθ(s′i, a′).                                                 (4)

a'

Considering statistical independence of different transitions in this batch, which holds by randomly
sampling from the replay buffer (Mnih et al., 2015), the likelihood function can be expressed as

M

p (y|w, β) =       N   yi|wTφθ(s, a), β−¹  .                                       (5)

i=1

Assuming the prior distribution as w         (m, S) and referring to the Bayes rule, we can obtain 
the
posterior distribution of w, which is still a multivariate Gaussian distribution:

p(w|y, β) = N(w|m⁰, S⁰),                                                    (6)

with the distribution parameters given by (Bishop, 2006):

m⁰ = S⁰  S−¹m + βΦTy   , S⁰ =   S−¹ + βΦTΦ  −1 ,                             (7)

where Φ = (φθ(s₁, a₁), · · ·  , φθ(sM , aM ))T. Refer to Appendix A.1 for more details.

3


Under review as a conference paper at ICLR 2021

4                                                                                                   
                                        4                                                           
                                                                                4

2                                                                                                   
                                        2                                                           
                                                                                2

0                                                                                                   
                                        0                                                           
                                                                                0

2                                                                                                   
                                        2                                                           
                                                                                2


4                                                                                            mean

ground truth
uncertainty

0                 2                 4                 6                 8                10

Feature X

(a)

4                                                                                            mean

ground truth
uncertainty

0                 2                 4                 6                 8                10

Feature X

(b)

4                                                                                            mean

ground truth
uncertainty

0                 2                 4                 6                 8                10

Feature X

(c)

Figure 1:  Example to illustrate the motivation of introducing informative prior to BLR: BLR with
the non-informative prior trained on (a) T[₀,₆] and (b) T[₄,₁₀],  respectively,  and (c) BLR with 
the
informative prior trained on T[₄,₁₀].

The training of RL is an iterative process.  In each iteration, previous randomized value function-
based algorithms assume a non-informative prior distribution to learn the model parameters, which
however is constrained on the current batch.  In fact, the posterior distribution of model 
parameters
w from the last iteration can be utilized as an informative prior for the current training 
iteration,
able to provide richer information including the estimation of mean and variance parameters from
the previous batches.  The advantage of using informative prior comes from the nature of Bayesian
approach, which can be verified by a simple regression task as shown in Figure 1.  Here, we aim

to fit function f (x)  =  √x sin( ³ x)  with the test set    [₀,₁₀]  ,   (xi, yi) xi      [0, 10]  
.   Under a

non-informative prior assumption on the Gaussian distribution of zero mean and fixed variance, we
first run BLR on training set T[₀,₆]. The test result for BLR in Figure 1(a) shows huge uncertainty 
in
unseen area T[₆,₁₀]. Then we conduct BLR by treating the posterior distribution of model parameters
from Figure 1(a) as informative prior in training set T[₄,₁₀].  The result in Figure 1(c) 
demonstrates
that the learned estimation for T[₀,₄] has been retained even though it is not contained in the 
current
training set T[₄,₁₀].  In comparison, with the non-informative prior assumption, Figure 1(b) shows
that the test result for BLR trained on the training set T[₄,₁₀] holds huge uncertainty in T[₀,₄].

Therefore, we propose to utilize the informative prior in BLR to learn a randomized Q-value func-
tion.  Note that in RL training, due to the slow update for policy π, the training set may change
slowly and IP-BLR may quickly lose uncertainty, which makes the agent prone to get stuck in a
suboptimal policy. To keep the exploration of uncertainty, we use σ²I as the prior variance instead
of      the posterior covariance matrix S′ in (7) from the last iteration.  We then give the 
convergence
proof for the BLR Q-learning with informative prior. Analogous to Bellman optimality operator, we

formally define the IP-BLR Bellman optimality operator, as follows.

Definition 1.  (IP-BLR operator) In each iteration, we set σ²I  and the posterior mean m′p  from
the  pervious  iteration  as  the  informative  prior  N(m′p, σ²I)  for  the  distribution  of  w,  
based
on  which  the  prior  predictive  distribution  for  Q-value  qs,ₐ  can  be  derived  as  Jq:   
p(qs,ₐ)   ∼

(m'Tφ(s, a), β−¹I + σ²φθ(s, a)Tφθ(s, a)).  We define the following IP-BLR Bellman optimal-
ity operator H∗ that changes the estimation of Q-value distribution based on BLR with the above
informative prior.

H∗qs,ₐ ∼ N(m⁰Tφ(s, a), β−¹I + φ(s, a)TS′φ(s, a)),                              (8)

where we have m′ = (I + σ²βΦTΦ)−¹(m′p + σ²βΦTy) based on (7). The proof follows eq.(3.59)
in Bishop (2006).

We update the posterior of w by IP-BLR and optimize θ together to the direction of the TD-target.
Thus we can have:


ǁEq − q∗ǁ

≤      1     ǁEq − EH∗qǁ

,                                           (9)


∞

where the equality holds when Eq = q∗.

Proof.  See Appendix A.2 for details.

1 − γ                        ∞

4


Under review as a conference paper at ICLR 2021

4    PAC-BAYESIAN  GENERALIZATION  BOUND  FOR  RANDOMIZED  VALUE
FUNCTION  APPROXIMATION

Randomized value function Q         :                     in (3) provides an efficient method for 
exploration.
Iteratively applying IP-BLR Bellman operator and update θ to the TD-target can lead the mean of
the estimated distribution for Q-value qs,ₐ to the optimal state-action value qs∗,a, which 
theoretically
requires the exhausting visit of state-action space.   However,  in practical RL training,  
especially
for deep RL algorithms,  the true distribution     π under current policy π  over                   
    is not
available. Since we cannot get samples for the whole state-action space, a commonly used alternative
is      to sample a batch of transitions for training.  In this section, we derive an oracle 
generalization

inequality for each IP-BLR training process based on the PAC-Bayesian theory.

Theorem 2.  Given the true distribution    π over                 ᵗᵃʳᵍᵉᵗ under the current policy 
π, in each
IP-BLR training iteration with fixed φ(s, a), for all prior distributions Jw, posterior 
distributions
Pw over model parameters w and corresponding predictive distribution Jq and Pq over Q-value
qs,ₐ, a non-negative λ, with a probability of at least 1 − δ we have:


ǁE   q

− q∗  ǁ

≤  EPq  (ǁqs,ₐ − EH∗qs,ₐǁ₂) +  1 ΣD

(P  ||J  ) + ln EDπ  [ψJ (s, a, y)] Σ ,


Pq    s,a

s,a   ∞

1 − γ

λ       KL      q      q

δ

(10)


where ψJ (s, a, y) = EJq

exp [λϕ(qs,ₐ)], ϕ (qs,ₐ) = ǁqs,ₐ − qs∗,aǁ∞ −     1    ǁqs,ₐ − EH∗qs,ₐǁ₂.

Proof.  See Appendix A.3 for details.

In Theorem 2, we provide an upper bound for the proposed IP-BLR optimality Bellman operator.
The LHS of (10) provides a oracle distance between optimal Q-function and current mean of ran-
domized Q-function. On the RHS, the first term indicates that the posterior Pq should match with the
mean of IP-BLR, while the second term specifies a divergence between Pq and Jq.  The third term
measures the distance between the optimal Q-function and the mean of randomized value function
after IP-BLR operation. When δ and prior Jw are selected, the model parameters w’ posterior distri-
bution Pw is the unique one that can be optimized. Therefore the bound in (10) suggests a trade-off

redbetween the IP-BLR operator H∗qs,ₐ and the prior distribution Jq,  resulting in the following
optimization problem:


min E        1     ǁq

− EH∗q

ǁ   + D

(P  ||J  ).                               (11)

This objective function is convex over Pq. By applying the KKT condition, we can get the optimal
Gibbs posterior (Alquier et al., 2016) as


Pq : p(qs,ₐ

1

) =     Jq(qs,ₐ

Z

   λ   

) exp{− 1 − γ ǁq

s,a

− EH∗q

s,ₐǁ₂}                            (12)

where Z  = ∫ Jq(q′) exp{−    λ   ǁq′ − EH∗qs,ₐ)ǁ₂}dq′ is the normalization term.  This Gibbs pos-

terior is hard to compute and sample due to the intractable term Z.  Markov Chain Monte-Carlo
(MCMC)  method  is  a  commonly  used  method  to  sample  the  Gibbs  posterior  (Alquier  &  Biau,
2013),  but it is too slow for big datasets.   Another method is to approximate Gibbs posterior by
a  set of Gaussian distribution based on the variantional inference (VI) (Alquier et al., 2016).  
How-
ever, solving the variantional lower bound is still costly in our iterative setting.  Instead, in 
the next
section, we alternatively propose to manipulate noise parameter β.

5    ADAPTIVE  NOISE  PARAMETER  UPDATE

Reviewing (7), the relationship between the informative prior m, S and the current information Φ,
y is controlled by the noise distribution parameter β. In BLR, β is a preset hyper-parameter 
(Bishop,
2006).  Recalling the suggestion of Theorem 2, we should solve the trade-off between the IP-BLR
and prior. In this section, we assume that the prior of β follows uniform distribution and propose 
an
adaptive noise parameter update.

5


Under review as a conference paper at ICLR 2021


From Bayes rule, we have

p(y|w, β)p(w) = p(w|y, β)p(y|β).                                           (13)

Substituting (5), (6) and prior distribution p(w) = N(w; m, σ²I) into (13), we obtain

p(y|β) ∝ |Σ  |−    exp .−   (y − µ)T Σ−¹(y − µ)Σ ,                             (14)

1                     1

		

where µT = βσ−²mT S′ΦΣy and Σ−y 1  = βI +β²ΦS′ΦT . According to the maximum likelihood
estimation,  we want to maximize β’s posterior p(β y)       p(y β)p(β) (Tipping, 2001),  which is
equivalent to the following optimization problem:


1

max −    log |Σ

| −  1 (y − µ)TΣ−¹(y − µ).                                    (15)

The objective function is concave over β.  By setting derivatives w.r.t.  β of (15) to zero, we get 
the
update for the noise parameter β:


βnew

= 2M/ ΣTr(Σy

) + ǁy − Φm′ǁ²Σ                                    (16)

The first term in the denominator is the trace of the covariance matrix of the Q-value’s posterior 
dis-
tribution conditioned on β, which reflects the uncertainty for the estimation of Q-value. The second
term is the mean square error (MSE) with respect to the BLR’s posterior mean.  The value of these
two terms consistently reflect the performance of the posterior parameter in the IP-BLR operator.
A larger trace of posterior covariance matrix or a larger MSE will incur a worse performance and
a smaller β, which in turn will make the posterior distribution closer to the prior distribution.  
As

shown in (7) and (15), the update of β will result in the change of the posterior parameters (m′, 
S′)
and  vice  versa.   Hence  we  alternately  perform  the  IP-BLR  operation  and  β  update  as  
shown  in
Algorithm 2 to find an appropriate update for (m′, S′, β).

To summarize, we propose IP-BLR DQN with adaptive noise parameter in Algorithm 1 in Appendix

A.4.  The agent explores the environment guided by the variance of Q-value estimation instead of
randomly injected noise.  The parameter θ from feature extraction layer is learned by vanilla DQN
backpropagation with w fixed.  Besides, we periodically update the distribution parameters of w
using Algorithm 2 in Appendix A.4.

6    EXPERIMENTS

In this section, we conduct several empirical experiments to validate the effectiveness of our pro-
posed IP-BLR methods.  We compare our method with Bayesian DQN (BDQN) (Azizzadenesheli
et al., 2018) and NoisyNet DQN (Fortunato et al., 2017), which all are randomized value function
baselines.  In NoisyNet, noise is added to the weights of deep RL to introduce randomized value
function for exploration.  Besides, we empirically show the outcome of the adaptive noise param-
eter update.   We also conduct experiments to study how the vital hyper-parameters (i.e.,  feature
dimension and prior variance σ) affect the overall performance.

6.1    COMPARISON ON CLASSICAL CONTROL TASKS

We choose classical control tasks: MountainCar, CartPole and Acrobot provide by OpenAI (Brock-
man et al., 2016) and use the basic reward function setting. In MountainCar and Acrobot, the agent
will get reward    1 for each time step, until the episode ends. In CartPole, the agent will get 
reward

+1 for keeping the pole upright until the pole and cart deviate too much from the perpendicular and
the center, respectively.  We set the maximum time steps for one episode to 1000 for CartPole and
MountainCar, and to 500 for Acrobot.

We use a neural network to represent Q-value function in NoisyNet and feature extraction in BDQN
and IP-BLR DQN, which has the same dense layers setting in all the three algorithms.  Besides, in
BDQN and our IP-BLR DQN, we use the same feature dimension and σ². In our IP-BLR DQN, we
perform 20 iterations in Algorithm 2.

6


Under review as a conference paper at ICLR 2021


1000

800

200

100


600

400

200


400

600

300


200

0

IP-BLR DQN
BDQN

NoisyNet

0                            200                          400                          600          
                800                         1000

episodes

(a) CartPole

800

1000

IP-BLR DQN
BDQN

NoisyNet

0            200          400          600          800         1000

episodes

(b) MountainCar

400

500

IP-BLR DQN
BDQN

NoisyNet

0          200        400        600        800       1000

episodes

(c) Acrobot

Figure 2: Comparison between IP-BLR DQN, BDQN and NoisyNet on CartPole, MountainCar and
Acrobot.


1000

800

= 2, tanh

= 2, Relu

1000

800

= 6, tanh

= 6, Relu

1000

800

= 128, tanh

= 128, Relu

1000

800

= 256, tanh

= 256, Relu


600

600

600

600


400

400

400

400


200

200

200

200


0     0                200             400             600             800            1000

episodes

(a) Ω = 2

0     0                200             400             600             800            1000

episodes

(b) Ω = 6

0     0                200             400             600             800            1000

episodes

(c) Ω = 128

0     0                200             400             600             800            1000

episodes

(d) Ω = 256

Figure 3:  IP-BLR DQN with Tanh and ReLU after the last dense layer with the feature dimension

Ω=[2, 6, 128, 256] on CartPole.

We run each training process for 1000 episodes and plot the learning curves on three test environ-
ments in Figure 2.  The results are obtained by running each algorithms with five different random
seeds. The accumulated reward averages on the last 100 episodes. We can see that IP-BLR DQN has
the best asymptotic performance on all the three environments, while BDQN is mostly better than
NoisyNet. IP-BLR DQN has less performance degradation during the training process compared to
the other two comparison methods, and presents a quicker jumpstart on the training process.

6.2    HYPER-PARAMETERS

In order to demonstrate the effectiveness of the proposed IP-BLR operator and the adaptive noise
parameter update, we conduct several experiments to study and provide an empirical analysis of the
hyper-parameter selection for our IP-BLR DQN.

The feature dimension Ω is a vital hyper-parameter for both BDQN and IP-BLR DQN. A small
dimension may not be representative for the value function of a task, while a large dimension may
exceed the capability of BLR in the high-dimensional space.  We plot the learning curves of IP-
BLR DQN with Tanh and ReLU activation after the last dense layer in MountainCar, with feature
dimension set to [2, 6, 128, 256] in Figure 3. The results show that the feature dimension affects 
the
performance of IP-BLR DQN, and the training process suffers from a too small or too large feature
dimension, which is mainly dependent on the task and the activation function used.  With Tanh as
the activation function, we suggest that the feature dimension should be comparable to the range
of Q-value in tasks, since Tanh ranges from    1 to 1.  In comparison, with ReLU as the activation
function, the performance is not that sensitive to Ω.  However, as shown in Figure 3(a) and 3(d), a
too large or too small Ω still will degrade the performance.

In addition,  IP-BLR DQN reduces to BDQN without the proposed informative prior distribution
and adaptive noise parameter update.  To validate the effectiveness of the utilization of 
informative
prior distribution,  we further compare IP-BLR DQN with BDQN and IP-BLR DQN without the
adaptive β  update.  Referring to Theorem 2, we propose the adaptive update for β  to balance the
trade-off between the IP-BLR operation and informative prior.   We compare the performance on
Cartpole achieved by BDQN with β  =  100, IP-BLR DQN with adaptive noise parameter update,
and IP-BLR DQN with fixed β       [0.1, 1, 100, 10000]  in Figure 4(a).   It shows that the training
process of IP-BLR DQN is more stable and has a quick jumpstart, which mainly benefits from the
adaptive noise parameter update.  In addition, under the same preset β  =  100, IP-BLR DQN has
faster convergence than BDQN, which demonstrates the effectiveness of utilizing the informative
prior.  The curves of β during the training process in three environments are shown in Figure 4(b),

7


Under review as a conference paper at ICLR 2021


1000

800

600

400

200

adaptive beta
BDQN

beta=0.1
beta=100
beta=10000

3.0

2.5

2.0

1.5

1.0

0.5

Acrobot
CartPole
MountainCar

100

80

60

40

20


0

0              200           400           600           800          1000

episodes

(a) Ω = 6

0.0    0               10              20              30              40              50

(ep / update_interval) in Algorithm 1

(b) Ω = 2

0   0.0      2.5      5.0      7.5     10.0    12.5    15.0    17.5    20.0

Adaptive update iterations for

(c) Ω = 12

Figure 4:  (a) Comparison between IP-BLR DQN with/without adaptive β update and BDQN, (b)
dynamic trend of β during training, (c) trend of the MSE reduction.

presenting a dynamic trend. The trend of MSE reduction that is averaged on all the update based on
Algorithm 2 is shown in Figure 4(c), demonstrating that our iterative update for β can effectively
reduce the MSE loss during training in DQN.

6.3    EXPERIMENT SETUP

In all the experiments,  we use Adam optimizer for learning the neural network with two hidden
layers of dimensions 50 and 256, respectively, by using the learning rate 3     10−⁴.  For RL hyper-
parameters, we set the discounted factor to γ  = 0.99, replay buffer size to 1     10⁵ and 
mini-batch
size       to 64. For BDQN and IP-BLR DQN, we set the training set size as 1000.

7    RELATED  WORK

Previous attempts to extend randomized value function strategy to deep RL include the direct Q-
function approximation by DNN (Azizzadenesheli et al., 2018) and duplicates of Q-networks to
approximate the posterior distribution (Osband et al., 2016).   In statistic learning,  PAC-Bayesian
theory firstly provides a probably approximately correct (PAC) guarantee on generalization error
for Bayesian algorithms (McAllester, 1999).  For regression setting, a PAC-Bayesian bound with
unbounded loss function is given by   (Germain et al., 2016) and a practical Gaussian process re-
gression algorithm is studied in (Reeb et al., 2018).  In RL, a PAC-Bayes generalization bound on
reparameterized RL is provided with randomization on policy parameters by Wang et al. (2019),
while  our  work  is  done  on  standard  RL  with  randomization  on  value  function  parameters. 
 Fard
& Pineau (2010) propose a PAC-Bayes bound for model-free RL on discrete state spaces without
introducing function approximation,  while our bound and algorithm are derived for the complex
continuous state space and randomized value function.

PAC learning has also been introduced to RL for the measure of exploration efficiency, which is
known as probably approximately correct in Markov decision processes (PAC-MDP). PAC-MDP
requires RL algorithms to behave non-optimal compared to optimal performance within a small
range in a certain number of time steps that is polynomial w.r.t.  the size of state space and 
action
space.  RL algorithms such as E³ (Kearns & Singh, 2002) and R-max (Brafman & Tennenholtz,
2003) that satisfy PAC-MDP all show efficient exploration since near-optimal policy can be achieved
in polynomial time. Recent studies have advanced PAC-MDP to being capable of dealing with rich
observations (Dann et al., 2018; Du et al., 2019).

8    CONCLUSION

In this paper, we have proposed for the randomized value function-based RL an IP-BLR operator
by leveraging the data-dependent prior in training to reduce the estimation error of value function.
Based on PAC-Bayesian theory, we theoretically derived a generalization error bound for the pro-
posed IP-BLR operation at each training iteration, and developed an practically feasible algorithm
for adaptive noise parameter update to balance the trade-off between the posterior of IP-BLR op-
eration and the informative prior distribution.  Experiments on classical control tasks have shown
that due to the more accurate value function approximation and better generalization, the proposed
IP-BLR DQN could achieve higher accumulated rewards in fewer interactions with the environment.

8


Under review as a conference paper at ICLR 2021

REFERENCES

Pierre Alquier and Ge´rard Biau.  Sparse single-index model.  J. Mach. Learn. Res., 14(1):243–280,
January 2013.

Pierre Alquier, James Ridgway, and Nicolas Chopin. On the properties of variational approximations
of gibbs posteriors. J. Mach. Learn. Res., 17(1):8374–8414, January 2016.

K. Azizzadenesheli, E. Brunskill, and A. Anandkumar. Efficient exploration through bayesian deep
q-networks. In 2018 Information Theory and Applications Workshop (ITA), pp. 1–9, Feb 2018.

Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and Statis-
tics). Springer-Verlag, Berlin, Heidelberg, 2006.

Ronen I. Brafman and Moshe Tennenholtz.  R-max–a general polynomial time algorithm for near-
optimal reinforcement learning. Journal of Machine Learning Research, 3(2):213, 2003.

Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym. CoRR, abs/1606.01540, 2016.

Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E
Schapire.     On  oracle-efficient  pac  rl  with  rich  observations.     In  S.  Bengio,  H.  
Wallach,

H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Infor-
mation Processing Systems 31, pp. 1422–1432. Curran Associates, Inc., 2018.

Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
Provably efficient RL with rich observations via latent state decoding.  In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 1665–1674, Long Beach,
California, USA, 09–15 Jun 2019. PMLR.

Mahdi M Fard and Joelle Pineau.   Pac-bayesian model selection for reinforcement learning.   In

Advances in Neural Information Processing Systems, pp. 1624–1632, 2010.

Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick, Ian Osband, Alex Graves,
Vlad Mnih, Re´mi Munos, Demis Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg.
Noisy networks for exploration. CoRR, abs/1706.10295, 2017.

Scott Fujimoto, Herke Hoof, and David Meger.  Addressing function approximation error in actor-
critic methods. In International Conference on Machine Learning, pp. 1587–1596, 2018.

Pascal Germain, Alexandre Lacasse, Franc¸ois Laviolette, and Mario Marchand. Pac-bayesian learn-
ing of linear classifiers. In Proceedings of the 26th Annual International Conference on Machine
Learning, ICML ’09, pp. 353–360, New York, NY, USA, 2009. Association for Computing Ma-
chinery.

Pascal Germain, Francis Bach, Alexandre Lacoste, and Simon Lacoste-Julien.   Pac-bayesian the-
ory meets bayesian inference.   In Proceedings of the 30th International Conference on Neural
Information Processing Systems, NIPS’16, pp. 1884–1892, Red Hook, NY, USA, 2016. Curran
Associates Inc.

Hado van Hasselt,  Arthur Guez,  and David Silver.   Deep reinforcement learning with double q-
learning.  In Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI’16,
pp. 2094–2100. AAAI Press, 2016.

Michael Kearns and Satinder Singh.  Near-optimal reinforcement learning in polynomial time.  Ma-
chine Learning, 49(2):209–232, 2002.

David A. McAllester. Some pac-bayesian theorems. Machine Learning, 37(3):355–363, 1999.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-

mare,  Alex Graves,  Martin Riedmiller,  Andreas K. Fidjeland,  Georg Ostrovski,  Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, 2015.

9


Under review as a conference paper at ICLR 2021

Ian Osband,  Charles Blundell,  Alexander Pritzel,  and Benjamin Van Roy.   Deep exploration via
bootstrapped dqn.  In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 29, pp. 4026–4034. Curran Associates, Inc.,
2016.

Ian Osband, Benjamin Van Roy, Daniel J. Russo, and Zheng Wen. Deep exploration via randomized
value functions. Journal of Machine Learning Research, 20(124):1–62, 2019.

Emilio Parrado-Herna´ndez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun.  Pac-bayes
bounds with data dependent priors. J. Mach. Learn. Res., 13(1):3507–3531, December 2012.

Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter space noise for exploration.
CoRR, abs/1706.01905, 2017.

David Reeb, Andreas Doerr, Sebastian Gerwinn, and Barbara Rakitsch. Learning gaussian processes
by minimizing pac-bayesian generalization bounds.   In Proceedings of the 32nd International
Conference  on  Neural  Information  Processing  Systems,  NIPS’18,  pp.  3341–3351,  Red  Hook,
NY, USA, 2018. Curran Associates Inc.

Jean-Francis Roy,  Mario Marchand,  and Franc¸ois Laviolette.   A column generation bound mini-
mization approach with pac-bayesian generalization guarantees.   In Arthur Gretton and Chris-
tian C. Robert (eds.), Proceedings of the 19th International Conference on Artificial Intelligence
and Statistics, volume 51 of Proceedings of Machine Learning Research, pp. 1241–1249, Cadiz,
Spain, 09–11 May 2016. PMLR.

David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, Timothy Lillicrap, Karen Si-
monyan, and Demis Hassabis.   A general reinforcement learning algorithm that masters chess,
shogi, and go through self-play. Science, 362(6419):1140–1144, 2018.

Michael E Tipping. Sparse bayesian learning and the relevance vector machine. Journal of machine
learning research, 1(Jun):211–244, 2001.

Huan Wang, Stephan Zheng, Caiming Xiong, and Richard Socher.   On the generalization gap in
reparameterizable reinforcement learning. In International Conference on Machine Learning, pp.
6648–6658, 2019.

10


Under review as a conference paper at ICLR 2021

A    APPENDIX

A.1    PROOF OF (7)

Proof.  Assuming   Gaussian   prior   for   w   as   N(w; µ, β−¹I),   for   a   batch   of   
training   data


{(si, ai, yi)}M

, the likelihood function can be expressed as

M

p (y|w, β) =       N   yi; wTφθ(si, ai), β−¹  .

i=1

= ( 2π )− M   exp    − β (y − Φw)T (y − Φw)                              (17)

β                        2

Then the posterior p (y w, β) is also Gaussian distribution.  By reformulating (17) as a standard
Gaussian distribution, we can get the posterior mean m′ and covariance S′:

m⁰ = S⁰ .S−¹m + βΦTyΣ , S⁰ = .S−¹ + βΦTΦΣ−1


A.2    PROOF OF (9)

Proof.

Then we have:

ǁEq − q∗ǁ∞ =ǁEq − EH∗q + EH∗q − q∗ǁ∞

≤ǁEq − EH∗qǁ∞ + ǁT ∗Eq − T ∗q∗ǁ∞

≤ǁEq − EH∗qǁ∞ + γǁEq − q∗ǁ∞


ǁEq − q∗ǁ

≤      1     ǁEq − EH∗qǁ

∞       1 − γ                        ∞

A.3    PROOF OF THEOREM 2

Proof.  Referring to the change of measure inequality (Roy et al., 2016), we obtain:

λEPq ϕ(qs,ₐ) ≤ DKL(Pq||Jq) + ln ψJ                                   (18)


Hence,

λEPq ǁqs,a

− qs∗,aǁ∞

    1    

≤ λEPq  1 − γ

s,a

− EH∗q

s,aǁ2 + D

KL(Pq||Jq) + ln ψJ .

From Jensen’s inequality, we have

ǁEPq qs,a − qs∗,aǁ∞ ≤ EPq ǁqs,a − qs∗,aǁ∞.                                      (19)
Applying Markov’s inequality, we have


p (ψJ

(s, a, y)) ≤ α) ≥ 1 −  Es,a,y [ψJ (s, a, y)] .                                  (20)

By letting δ =  Es,a,y [ψJ (s,a,y)]  and integrating (19) and (20) into (18), the inequality in 
(10) holds.

11


Under review as a conference paper at ICLR 2021

A.4    ALGORITHMS

Algorithm 1 IP-BLR DQN

1:  Initialize feature extraction parameters θ

2:  Initialize target network’s feature extraction parameters θ−     θ

3:  Initialize IP-BLR parameters m, σ, S, β

4:  Initialize replay buffer

5:  for episode ep = 1, . . . , N do

6:       for step t = 1, . . . , T  do

7:           Draw w from     (m, S)

8:           Select an action at = argmaxₐwTφθ(st, a)

9:           Execute action at and receive rt and s′t

10:           Store transition (st, at, rt, s′t) in B                              ′    M

11:           Samp.le a random batch of transitions {(si, ai, ri, si)}i₌₁ from B

13:           Update θ through one step gradient descent on (yi    mTφθ(st, a))²

14:       end for

15:       if ep mod update interval == 0 then

16:           (m, S) ←−Algorithm 2(m, σ, {(φθ(si, ai), yi)}M   )

17:       end if

18:       update target network: θ−     θ

19:  end for

Algorithm 2 IP-BLR with adaptive noise parameter

Input: prior parameters m, σ and training set {(φθ(si, ai), yi)}M

Output: posterior parameters m′, S′

1:  for each iteration do

2:       Update (m′, S′) through (7) with S = σ²I

3:       Update β through (16)

4:  end for

12

