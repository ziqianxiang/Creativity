Under review as a conference paper at ICLR 2021
Formalizing Generalization and Robustness
of Neural Networks to Weight Perturbations
Anonymous authors
Paper under double-blind review
Ab stract
Studying the sensitivity of weight perturbation in neural networks and its impacts
on model performance, including generalization and robustness, is an active re-
search topic due to its implications on a wide range of machine learning tasks
such as model compression, generalization gap assessment, and adversarial at-
tacks. In this paper, we provide the first formal analysis for feed-forward neural
networks with non-negative monotone activation functions against norm-bounded
weight perturbations, in terms of the robustness in pairwise class margin func-
tions and the Rademacher complexity for generalization. We further design a new
theory-driven loss function for training generalizable and robust neural networks
against weight perturbations. Empirical experiments are conducted to validate our
theoretical analysis. Our results offer fundamental insights for characterizing the
generalization and robustness of neural networks against weight perturbations.
1	Introduction
Neural network is currently the state-of-the-art machine learning model in a variety of tasks, includ-
ing computer vision, natural language processing, and game-playing, to name a few. In particular,
feed-forward neural networks consists of layers of trainable model weights and activation functions
with the premise of learning informative data representations and the complex mapping between data
samples and the associated labels. Albeit attaining superior performance, the need for studying the
sensitivity of neural networks to weight perturbations is also intensifying owing to several practical
motivations. For instance, in model compression, the robustness to weight quantification is crucial
for reducing memory storage while retaining model performance (Hubara et al., 2017; Weng et al.,
2020). The notion of weight perturbation sensitivity is also used as a metric to evaluate the general-
ization gap at local minima (Keskar et al., 2017; Neyshabur et al., 2017). In adversarial robustness
and security, weight sensitivity can be leveraged as a vulnerability for fault injection and causing
erroneous prediction (Liu et al., 2017; Zhao et al., 2019). However, while weight sensitivity plays
an important role in many machine learning tasks and problem setups, theoretical characterization
of its impacts on generalization and robustness of neural networks remains elusive.
This paper bridges this gap by developing a novel theoretical framework for understanding the gen-
eralization gap (through Rademacher complexity) and the robustness (through classification margin)
of neural networks against norm-bounded weight perturbations. Specifically, we consider the multi-
class classification problem setup and multi-layer feed-forward neural networks with non-negative
monotonic activation functions. Our analysis offers fundamental insights into how weight pertur-
bation affects the generalization gap and the pairwise class margin. To the best of our knowledge,
this study is the first work that provides a comprehensive theoretical characterization of the interplay
between weight perturbation, robustness in classification margin, and generalization gap. Moreover,
based on our analysis, we propose a theory-driven loss function for training generalizable and ro-
bust neural networks against norm-bounded weight perturbations. We validate its effectiveness via
empirical experiments. We summarize our main contributions as follows.
•	We study the robustness (worst-case bound) of the pairwise class margin function against weight
perturbations in neural networks, including the analysis of single-layer (Theorem 1), all-layer
(Theorem 2), and selected-layer (Theorem 3) weight perturbations.
•	We characterize the generalization behavior of robust surrogate loss for neural networks under
weight perturbations (Section 3.4) through Rademacher complexity (Theorem 4).
1
Under review as a conference paper at ICLR 2021
•	We propose a theory-driven loss design for training generalizable and robust neural networks
(Section 3.5). The empirical results in Section 4 validate our theoretical analysis and demonstrate
the effectiveness of improving generalization and robustness against weight perturbations.
2	Related Works
In model compression, the robustness to weight quantization is critical to reducing memory size
and accesses for low-precision inference and training (Hubara et al., 2017). Weng et al. (2020)
showed that incorporating weight perturbation sensitivity into training can better retain model per-
formance (standard accuracy) after quantization. For studying the generalization of neural networks,
Keskar et al. (2017) proposed a metric called sharpness (or weight sensitivity) by perturbing the
learned model weights around the local minima of the loss landscape for generalization assess-
ment while An (1996) introduced weight noise into the training process and concluded that random
noise training improves the overall generalization. Neyshabur et al. (2017) made a connection be-
tween sharpness and PAC-Bayes theory and found that some combination of sharpness and norms
on the model weights may capture the generalization behavior of neural networks. Additionally,
Bartlett et al. (2017) discovered normalized margin measure to be useful towards quantifying gener-
alization property and a bound was therefore constructed to give an quantitative description on the
generalization gap. Moreover, Golowich et al. (2019) incorporated additional assumptions to offer
tighter and size-independent bounds from the setting of (Neyshabur et al., 2015) and (Bartlett et al.,
2017) respectively. Despite development of various generalization bounds, empirical observations
in (Nagarajan & Kolter, 2019) showed that once the size of the training dataset grows, generalization
bounds proposed in (Neyshabur et al., 2017) and (Bartlett et al., 2017) will enlarge thus become vac-
uous. Discussions on the relation between (Nagarajan & Kolter, 2019) and our works can be found
at Appendix E, where we show that in our studied setting the associated generalization bounds are
non-vacuous. On the other hand, Barron & Klusowski (2018) and Theisen et al. (2019) applied
several techniques in tandem with a probabilistic method named path sampling to construct a rep-
resenting set of given neural networks for approximating and studying the generalization property.
Another approach considered by Petzka et al. (2020) consists of segmenting the neural networks
into two functions, predictor and feature selection respectively where two measures (representa-
tiveness and feature robustness) concerning these aforementioned functions were later combined to
offer a meaningful generalization bound. However, these works only focused on the generalization
behavior of the local minima and did not consider the generalization and robustness under weight
perturbations. Weng et al. (2020) proposed a certification method for weight perturbation retaining
consistent model prediction. While the certification bound can be used to train robust models with
interval bound propagation (Gowal et al., 2019), it requires additional optimization subroutine and
computation costs when comparing to our approach. Moreover, the convoluted nature of certification
bound complicates the analysis when studying generalization, which is one of our main objectives.
In adversarial robustness, fault-injection attacks are known to inject errors to model weights at the
inference phase and causing erroneous model prediction (Liu et al., 2017; Zhao et al., 2019), which
can be realized at the hardware level by changing or flipping the logic values of the corresponding
bits and thus modifying the model parameters saved in memory (Barenghi et al., 2012; Van Der Veen
et al., 2016). Zhao et al. (2020) proposed to use the mode connectivity of the model parameters in
the loss landscape for mitigating such weight-perturbation-based adversarial attacks.
Although, to the best of our knowledge, theoretical characterization of generalization and robustness
for neural networks against weight perturbations remains elusive, recent works have studied these
properties under another scenario - the input perturbations. Both empirical and theoretical evidence
have been given to the existence of a fundamental trade-off between generalization and robustness
against norm-bounded input perturbations (Xu & Mannor, 2012; Su et al., 2018; Zhang et al., 2019;
Tsipras et al., 2019). The adversarial training proposed in (Madry et al., 2018) is a popular training
strategy for training robust models against input perturbations, where a min-max optimization prin-
ciple is used to minimize the worst-case input perturbations of a data batch during model parameter
updates. For adversarial training with input perturbations, Wang et al. (2019) proved its convergence
and Yin et al. (2019) derived bounds on its Rademacher complexity for generalization. Different
from the case of input perturbation, we note that min-max optimization on neural network training
subject to weight perturbation is not straightforward, as the minimization and maximization steps
are both taken on the model parameters. In this paper, we disentangle the min-max formulation for
2
Under review as a conference paper at ICLR 2021
weight perturbation by developing bounds for the inner maximization step and provide quantifiable
metrics for training generalization and robust neural networks against weight perturbations.
3	Main Results
We provide an overview of the presentation flow for our main results as follows. First, we introduce
the mathematical notations and preliminary information in Section 3.1. In Section 3.2, we establish
our weight perturbation analysis on a simplified case of single-layer perturbation. We then use the
single-layer analysis as a building block and extend the results to the multi-layer perturbation setting
in Section 3.3. In Section 3.4, we define the framework of robust training with surrogate loss and
study the generalization property using Rademacher complexity. Finally, we propose a theory-driven
loss toward training robust and generalizable neural networks in Section 3.5.
3.1	Notation and Preliminaries
Notation We start by introducing the mathematical notations used in this paper. We define the
set [L] := {1, 2, ..., L}. For any two non-empty sets A, B, FA7→B denotes the set of all functions
from A to B. We mark the indicator function of an event E as 1(E), which is 1 if E holds and
0 otherwise. We use sgn(∙) to denote element-wise sign function that outputs 1 when input is
nonnegative and -1 otherwise. Boldface lowercase letters are used to denote vectors (e.g., x), and
the i-th element is denoted as [x]i . Matrices are presented as boldface uppercase letters, say W.
Given a matrix W ∈ Rk×d , we write its i-th row, j-th column and (i,j) element as Wi,:, W:,j,
and Wi,j respectively. Moreover, we write its transpose matrix as (W)T . The matrix (p, q) norm
is defined as kWk
we have kWkp =
p,q
[kW:,1kp, kW:,2kp, ..., kW:,dkp]	for any p,q ≥ 1. For convenience,
kWkp,p and write the spectral norm and Frobenius norm as kWkσ and kWkF
respectively. We mark one matrix norm commonly used in this paper - the matrix (1, ∞) norm.
With a matrix W, we express its matrix (1, ∞) norm as kWk1,∞, which is defined as kWk1,∞ =
maxj kW:,jk1 and WT 1,∞ = maxi ∣∣Wi∕∣r We use IB∞(e) to express an element-wise '∞
,
norm ball of matrix W within radius G i.e., IBW (E) = {W || Wi,j - Wij | ≤ e, ∀i ∈ [k],j ∈ [d]}.
Preliminaries In order to formally explain our theoretical results, we introduce the considered
learning problem, neural network model and complexity definition. Let X and Y be the feature space
and label space, respectively. We place the assumption that all data are drawn from an unknown
distribution D over X × Y and each data point is generated under i.i.d condition. In this paper,
we specifically consider the feature space X as a subset of d-dimensional Euclidean space, i.e.,
X ⊆ Rd. We denote the symbol F ⊆ FX 7→Y to be the hypothesis class which we use to make
predictions. Furthermore, we consider a loss function ` : X × Y -→ [0, 1] and compose it with
the hypothesis class to make a function family written as 'f := {(x, y) —→ '(f (x), y)| f ∈ F}.
The optimal solution of this learning problem is a function f * ∈ F such that it minimizes the
population risk R(f) = E(x,y)〜D['(f (x),y)]. However, since the underlying data distribution is
generally unknown, one typically aims at reducing the empirical risk evaluated by a set of training
data {(xi, yi)}n=ι, which can be expressed as Rn(f) = ɪ Pn=ι '(f (xi), yi). The generalization
error is the gap between population and empirical risk, which could serve as an indicator of model’s
performance under unseen data from identical distribution D.
To study the generalization error, one would explore the learning capacity of a certain hypothesis
class. In this paper, we adopt the notion of Rademacher complexity as a measure of learning capac-
ity, which is widely used in statistical machine learning literature (Mohri et al., 2018). The empirical
Rademacher complexity of a function class F given a set of samples S = {(xi, yi)}in=1 is
1n
RS ('f ) = EV [sup — V Vi'(f (xi),yi)]
f∈F n i=1
(1)
where {νi}n=ι is a set of i.i.d Rademacher random variables with P{νi = -1} = P{νi = +1} = ɪ.
The empirical Rademacher complexity measures on average how well a function class F correlates
with random noises on dataset S. Thus, a richer or more complex family could better correlate with
random noise on average. With Rademacher complexity as a toolkit, one can develop the follow-
ing relationship between generalization error and complexity measure. Specifically, it is shown in
3
Under review as a conference paper at ICLR 2021
(Mohri et al., 2018) that given a set of training samples S and assume that the range of loss function
`(f (x), y) is [0, 1]. Then for any δ ∈ (0, 1), with at least probability 1 - δ we have ∀f ∈ F
Son
R(f) ≤ Rn(f)+2Rs('f)+3
(2)
Note that when the Rademacher complexity is small, it is then viable to learn the hypothesis class F
by minimizing the empirical risk and thus effectively reducing the generalization gap.
Finally, we define the structure of neural networks and introduce a few related quantities. The
problem studied in this paper is a multi-class classification task with the number of classes being K .
Consider an input vector x ∈ X ⊆ Rd, an L-layer neural network is defined as
fW (x) = WL(...ρ(W1x)...) ∈ FX 7→R
(3)
with W being the set containing all weight matrices, i.e., W := {Wi | ∀i ∈ [L]}, and the
notation ρ(∙) is used to express any non-negative monotone activation function and We further
assume that ρ(∙) is I-LiPSchitz, which includes popular activation functions such as ReLU ap-
plied element-Wise on a vector. Moreover, the i-th component of neural netWorks’ output is
written as fWi (x) = [fW (x)]i and a pairwise margin between i-th and j-th class, denoted as
fWij (x) := fWj (x) - fWj (x), is said to be the difference between two classes in output of the
neural network. Lastly, we use the notion of Zk and Zk to represent the output vector of the
k-th layer (k ∈ [L - 1]) under natural and weight perturbed settings respectively, which are
Zk = P(Wk(…ρ(W1x)…))and Zk = P(Wk(…P(W 1x)…))，where Wi ∈ IB∞i(∕) denotes the
perturbed weight matrix bounded by its element-wise '∞-norm with radius Ei for some i ∈ [k].
3.2	Building Block: Single-Layer Weight Perturbation
We study the sensitivity of neural network to weight perturbations through the pairwise margin
bound fWij (x). Specifically, when i and j corresponds to the top-1 and the second-top class pre-
diction of x, respectively, the margin can be used as an indicator of robust prediction under weight
perturbation to W. For ease of understanding, we first consider a simple example with a three-layer
neural network and explain the bound through the error propagation incurred by weight perturbation.
We define the neural network as fw(x) = W3ρ(W2ρ(W 1x)) with Wi being the weight matrix of
the i-th layer and assume that one could only perturb any element in the first weight matrix within an
'∞ norm ball of radius e, i.e., W 1 ∈ IB∞ι (e). We also define an error vector as ei, which stands for
the entry-wise error after propagating through the i-th layer. Since no perturbations happened prior
to the first layer, we would directly take input vector x and derive an upper bound on the entry-wise
error e1 . While every element in the first weight matrix is allowed to change its magnitude by at
most E, the maximum error for any entry by matrix-vector multiplication becomes
[eι]i := |W 1,:X - Wi,：x| ≤ £|WIj- WiIj||[x]j| ≤ £d[x]j| = e kxkι .	⑷
jj
Since the following layer weight is not subject to perturbation, we simply take the magnitude of
each element in the subsequent weight matrix to calculate the next error vector. In this case, we
have the next layer’s error e2 with W2 as [e2]i = Pj |Wi2,j| [e1]j = E kxk1 Pj |Wi2,j |. Eventually,
with error propagation over layers, we arrive at the final layer and are able to assess the maximum
change of any entry in output value. By recalling the pairwise class margin fWij (x), we would like to
inspect the relative change in error between any two classes. Specifically, we derive an upper bound
on the pairwise margin between any two classes α and β. In the above example, the difference in
entry-wise maximum error can be deduced in the following manner:
(i)
[e3]α - [e3]β = E(∣wα,k∣- ∣Wz3,kI)[e2]k ≤ ∑(∣wα,k - Wz3,k∣)β kxkι E ∣W2,1∣	⑸
k	kl
(ii)
≤ e kxkι maxk IlWki,Jh £(严嬴-W^|) = e ∣∣x∣∣ J(W2)[∣	IlWa,： - WQ1 ,
k	,∞
(6)
4
Under review as a conference paper at ICLR 2021
where inequality (i) comes from triangle inequality and inequality (ii) results from taking the row
in W2 with maximum `1 norm. It is worth noting that there exist possible scenarios for the above
inequalities to hold and therefore achieving the worst-case error. Specifically, using the example in
Section 3.2, as we trace down the associated inequality bound in (i), we see that the first inequality
can be achieved when the final weight layer possesses all positive weights and that the row associated
with label α is greater than label β in all individual entries. Furthermore, as long as the second
weight matrix W2 has equal `1 norm throughout all rows, we can then tighten the bound to give
the worst-case error in (ii). Nevertheless, we could see from the above example that the difference
of maximum error between entries in output would be propagating at the rate of weight matrices’
(1, ∞) norm. By utilizing this essential concept, we introduce the first theorem of our results, which
provides an upper bound on the pairwise margin under single-layer weight perturbation.
Theorem 1 (N -th layer weight perturbation (N 6= L)) Let fW (x) = WL(...ρ(W1x)...) denote
an L-layer neural network and let fw(X) = WL(..WWN…ρ(W1 x)...) with IVN ∈ IB∞n (e), N = L,
denote the corresponding network subject to N-th layer perturbation. For any set of perturbed and
unperturbed pairwise margin fcij (x) and fWij (x), we have
fW(X) ≤ fW(X) + TWL:- WjLJI JZNTIlInL-NT ∣∣(WL-k)T∣∣ι∞
where zk = ρ(Wk(...ρ(W1x)...)).
Proof : See Appendix A.1
Since the final layer does not have any activation function, the margin bound on the margin difference
when only perturbing the final layer can be simply derived, which is given in the following lemma.
Lemma 1 (Final-layer weight perturbation) Consider the case N = L in Theorem 1, we have
fcij (X) ≤ fWij (X) + 2e ZL-1 1, where ZL-1 is the output of the (L - 1)-th layer.
Proof : See Appendix A.1.
3.3 General Setting: Multi-Layer Weight Perturbation
With the developed single-layer analysis in Section 3.2 as a building block, we now extend our
analysis to the general setting of multi-layer weight perturbation, which is further divided into two
cases: (i) the case of perturbing all L layers; and (ii) the case of perturbing I out of L layers.
3.3.1	Perturbing All Layers
Once equipped with the concept of error propagation over subsequent layers, we consider the sce-
nario where every layer in a neural network is subject to weight perturbation. We denote the model
under this circumstance as the all-perturbed setting. The following theorem states an upper bound
on the pairwise margin between the natural (unperturbed) and all-perturbed settings.
Theorem 2 (all-layer perturbation) Let fW (X) = WL(...ρ(W1X)...) denote an L-layer (natural)
LN	1	m ∞
neural network and let fWc	(X)	= W	(..W	...ρ(W X)...) with W ∈	IBW∞m (ei),	∀m	∈	[L],	denote
its perturbed version. For any set of pairwise margin ficj (X) and fWij (X), we have
fWicj (X) ≤ fWij (X) + IIWiL,: - WjL,:II1	1 kXk1 ΠlL=-12 II(WL-l)T II1 ∞ +
〈I	、z	，,
L-3
XmmWkWm )τ Bι∞ )≡k+ι 卜 W
k=1
I	、z	/
^^^^^"^™{^^^^^^^^^
Error from layer 1 to L-2
{	}
Input Layer Error
+ :LT 卜L-2*II1} + 2eL BzL-1*I∣1
I 、z / I 、z }
^^^≡^≡{^^^^^^≡
Error of layer L-1
^^^^≡{^^^^^≡
Error of layer L
*	*	*	*	Wm* = Wm + €m, Vi, j and ∀m ∈ [L] ∖{1}
where Zk = ρ(Wk …ρ(W1 X) with Wm defined as	'
W1,j = W1,j + sgn([X]j) 1, ∀i, j
5
Under review as a conference paper at ICLR 2021
Proof : See Appendix A.2.2.
Here, we provide some intuition on deriving the upper bound of the margin in the all-perturbed
setting. The scheme behind this all-perturbed scenario can be viewed as an inductive layer-wise
error propagation. Specifically, we can choose any perturbed layer as the commencement point of
propagation, then fix any other weight matrices’ values and further calculate the propagation of error
from that layer using the concept in Section 3.2. In such manner, after iterating through all these
weight matrices subject to weight perturbation, one could obtain the final change in output value
and therefore establish the pairwise margin bound. A close inspection of the bound shows that the
propagation of error causes the first term since the input layer and the rest of the terms are errors
propagating since the i-th layer in the neural network, where i ∈ [L].
3.3.2 Perturbing Multiple Layers
The all-perturbed setting is a special case of perturbing layers from an index set I when I = [L].
We extend our analysis to the general multi-layer weight perturbation setting with I ⊆ [L], which
includes the single-layer setting (I = {N }) and all-perturbed setting (I = [L]) as special cases.
Theorem 3 (multiple-layer perturbation) Let an L-Layer neural network be written as fW (x) =
WL(...ρ(W1x)...). Given an index set I ⊆ [L], we define the perturbed neural network as
f7X)= WL( jvN p(Wy1 χ) )with (W= W, ∀i.∈ [L] \ I
fW(X) = W	(.…队	)...) with [Wi	=	Wi,	WZ IB∞iE),∀i	∈ I
for any pairwise margin between ficj (x) and fWij (x) we have
fW(X) ≤ fW(X) + ML： - WjLJIi ( P'∈I∖{L,L-1} e` W'-1* ∣∣1 (πL='+ι ∣∣(Wj)T∣∣ι,∞) + 1(L -1 ∈
I)L-1 zL-2*	+1(L ∈ I)2L zL-1*	:= fWij (X) + ηWij (X|I)
with Wm defined as 4
*
Wm
i,j
where zk* = ρ(Wk* ...ρ(W1* X)
(wmm + em, ∀i,j ∀m ∈ [L] ∩ I ∖{1}
=γwmj, ∀i,j ∀m ∈ [L] \ (I ∪{1})
(Wi1,j + sgn([X]j)1 , ∀i, j if 1 ∈ I
Wi1,j , ∀i, j	otherwise
and z0* = X
Proof : See Appendix A.2.3.
3.4	Surrogate Loss and Generalization B ound
3.4.1	Construction of Robust S urrogate Loss on Pairwise Margin
We aim to construct a surrogate loss function based on a standard loss function and study its behavior
against weight perturbations. Specifically, given a perturbation radius and the original loss function
'(fw(x), y), robust training aims to minimize the following objective function:
'(fW (X),y) = max∀m∈[L],w m∈B∞m (e)'(fW (X),y)	(7)
which we call it as the robustness (worst-case) loss. Even for a single data point (x, y), it is hard to
assess the exact robustness loss since it requires the maximization of a non-concave function over a
norm ball. To make the problem of robust training against weight perturbations more computation-
ally tractable, we aim to design a surrogate loss as an upper bound on the worst-case loss.
We focus on the construction of surrogate loss by means of pairwise margin bound in Section 3.3.
We first define mathematically two popular loss functions in the classification problem, ramp loss
and cross entropy, and derive their surrogate versions. Define the margin function M(fW (X), y) as
M (fW (X), y) = min[fW (X)]y - [fW (X)]y0 = [fW (X)]y - max[fW (X)]y0	(8)
y0 6=y	y0 6=y
6
Under review as a conference paper at ICLR 2021
The ramp loss for a given data point (x, y) and neural network fw (∙) is written as 'ramp (fw (x), y)=
φγ (M (fW (x), y)), where the function φγ : R 7→ [0, 1] is defined as φγ (t) = 1 ift ≤ 0, φγ(t) = 0
if t ≥ γ, and φγ(t) = 1 - W if t ∈ [0, γ]. Since the ramp loss is a piece-wise linear function, its
surrogate loss can be directly obtained with the pairwise margin bound in Section 3.3. The cross
~ 〜〜
entropy is written as CE(fW(x), y) = - ln([fW(x)]y), where fW (x) represents a neural network
with its output passing through a softmax layer. That is, [fW (x)]i
exp[fW (x)]i
Pk∈[K] eχp[fw(x)]k .
For ease of demonstration, we will be using ramp loss and its pairwise margin under single-layer per-
turbation in the following lemma. The surrogate loss analysis for cross entropy and robust surrogate
loss for multiple-layer perturbation is given in Appendix B.
Lemma 2 (robust surrogate ramp loss) Let N ∈ [L] denote the perturbed layer index and let
'(fw(X),y):= φγ{M(fw(X),y)-2max≡∣∣W⅛∣∣ ∏N=1kWmkι∞∏L-NT∣(WL-k)T∣∣	kxkι}
、 一 V一 /	k∈[K]	1	,	1,∞
margin	'---------------------------{---------------------------}
worst-case error
Then we have upper and lower bounds of ` in terms of 0-1 losses expressed as
Mr /	Γ t- (.Λ1 ~ι / r / ..∖	∖	/
maxWN∈ib∞n(e)1{y = argmaχyo∈[κ][fw(X)]y0} ≤ '(fw(X),y) ≤
1{M (fw (X),y) - 2maXk∈[K] e∣∣W⅛∣∣ι∏N=1 k Wm kι,∞ ∏L-N-1 ∣∣( WJ)T∣∣ι,∞ k x kι ≤ γ}∙
Proof : Please see Appendix B.1
One could observe in the formula that the margin function M (fW (x), y) serves as an accuracy
objective similar to the standard training process while the latter term could be conceived as the
worst-case error caused by weight perturbation that should be suppressed. Therefore, by training
under such an objective, we can simulate the scenario of robust training. Another intuition on the
surrogate loss function is that the surrogate loss also implies the difficulty of training robust and
generalizable models against large weight perturbations. Since error caused by perturbations would
be surging rapidly through layers, only small perturbations can be applied in training and practice,
permitting the worst-case error term to be smaller than the margin term. However, one follow-up
question that naturally arises is whether or not the generalization gap will be widened when training
with the robust surrogate loss. The following section investigates the generalization property while
conducting robust training and provides some theoretical insights to training toward a generalizable
and robust model under weight perturbation.
3.4.2	Generalization Gap for Robust S urro gate Loss
We consider the robust surrogate loss established in Lemma 2 and study its generalization bound
via Rademacher complexity in Theorem 4, where S = {(xi, yi)}in=1 denotes the set of i.i.d training
samples, X := [x1, x2, . . . , xn] ∈ Rd×n denotes the matrix composed of training data samples, and
dmax = max{d, d1, . . . , dL} denotes the maximum dimension among all weight matrices.
Theorem 4 (generalization gap for robust surrogate loss) With Lemma 2, consider the neural
network hypothesis class F = {fW (X)|W = (W1, W2, ..., WL), Wh σ ≤ sh, (Wh)T 2,1 ≤
bh, h ∈ [L]}. For any γ > 0, with probability at least 1 一 δ, we havefor all fw(∙) ∈ F
P(X ,y)~D{∃WN ∈ IB∞N (e) St y = argmaXyθ∈[κ][fW(X儿，} ≤ 1 Pn=11 ( [fw(Xi)]yi ≤ Y +
maxy，=y」fw(Xi)]y0 +2maXk∈κe∣∣WLJ∣ι∏N=1 kWmh∞ ∏L-N-1 ∣∣(WLi)T∣∣ι,∞ kXikι
+
60 log(n) log(2dmax)
H
2e su
n⅛+
n
k X kF (∏L=ιSh)(pL=ι (Sj )2∕3y2 +
|
叫,FπN=1kWmh∞,πL-NT ∣∣(WLJ kXkι,2,)+3与
^^^^"{^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
complexity term caused by robust training
7
Under review as a conference paper at ICLR 2021
Proof : Please see Appendix C.1
As highlighted in the bracket term of Theorem 4, if the product of multiple weight norm bounds
is not well confined, the model will suffer from a notable generalization gap. Consequently, our
analysis suggests a solution to reduce the generalization gap by imposing norm penalty functions on
all weight matrices for training generalizable neural networks subject to weight perturbations.
3.5	Theory-driven loss toward robustness and generalization
With our theoretical insights, we now propose a robust and generalizable loss function. Standard
neural network classifier training uses a classification loss 'cis(fw(x),y) that aims to widen the
pairwise margin so as to raise accuracy, but won’t necessarily be able to curb the error in output
once weight perturbation is imposed. To address this issue, we propose to train under a mixed and
regularized objective given a data sample (x, y), which would, in turn, balance the tradeoff between
standard accuracy, robustness, and generalization. The designed loss takes the form:
'* (fw (x), y) = 'cis (fw (x) ,y)+λ ∙
V-----------{z----------}
standard loss
L
m0ax{ηW (x|I)} +μ ∙ X(II(W m)T∣∣	+ kW mh ∞)⑼
y06=y	1,∞
V} m = 1
'	{z	}	、▼/
robustness loss from Thm. 3	generalization gap regularization from Thm. 4
The first term in the proposed loss function originates from standard classification problem for task-
specific accuracy, while the second term results from the maximum error on pairwise margin (the
0
term ηWy y(x|I) defined in Theorem 3) induced by weight perturbation. Finally, we contribute the last
term to the theoretical findings in Theorem 4, where imposing norm constraints on weight matrices
could benefit generalization and prevent the generalization gap from widening.
4	Numerical Validation
We validate our theoretical results and the designed loss function in (9) through two sets of exper-
iments: empirical generalization gap with matrix norm regularization and robust accuracy against
adversarial weight perturbations.
Experiment setup We used the MNIST dataset comprised of gray-scale images of hand-written
digits with ten categories. We trained neural network models as in (3) with four dense layers (number
of neurons are 128-64-32-10) and the ReLU activation function without the bias term. We used the
loss function ' in (9) with all-layer perturbation bound (i.e., I = [L]), identical weight perturbation
radius E (or Etrain), cross entropy as the standard classification loss '3, and a batch size of 32 with
20 epochs. Stochastic gradient descent with momentum is used for training, with the learning rate
set to be 0.01. For the generalization experiment, we follow the same setting as in (Yin et al., 2019),
which uses 1000 data samples to train the neural network with 100 epochs. All experiments were
conducted using an Intel Xeon E5-2620v4 CPU, 125 GB RAM, and an NVIDIA TITAN Xp GPU
with 12 GB RAM. For reproducibility, our codes are given in the supplementary material.
Empirical generalization gap Figure 1 (a) shows the empirical generalization gap (training accu-
racy - test accuracy) with respect to the matrix norm regularization coefficient μ defined in (9). As
indicated in Theorem 4, increasing μ effectively suppresses the Rademacher complexity and thus
reduces the generalization gaps, which are consistently observed on neural networks trained with
different weight perturbation level E.
Robustness against adversarial weight perturbation To evaluate the robustness against weight
perturbations, we modified the projected gradient descent (PGD) attack originally designed for input
perturbation (Madry et al., 2018), which we call as weight PGD attack. Starting from a trained neural
network weight W , the perturbed weight W is crafted by iterative gradient ascent using the signed
gradient of the standard loss denoted as sgn(Vw'必(/宅(x), y)), followed by an element-wise E
clipping centered at W . The attack iteration with the step size α is expressed as
W⑼=W, W(t+1) = Clipw,e {W(t) + α sgn(Vw'^(/访㈤(x),y))}	(10)
8
Under review as a conference paper at ICLR 2021
(a) Empirical generalization gap
Figure 1: (a) Empirical generalization gaps when varying the matrix norm regularization coefficient
μ in (9). Consistent with the theoretical results, the gap reduces as μ increases for every e value used
for training. (b) Comparison of test accuracy of neural networks trained with different coefficients λ
and μ under weight PGD attack (200 steps) with e perturbation level. AUC refers to the area under
curve score. Joint training with the two theory-driven terms as described in (9) indeed yields more
generalizable and robust neural networks against weight perturbations.
(b) Robust accuracy under adversarial perturbation
We trained neural networks with different combinations of the coefficients λ and μ in (9) using
train = 0.01. Figure 1 (b) shows the test accuracy under different weight perturbation level (i.e.,
the robust accuracy) with 200 attack steps. The standard model (λ = μ = 0) is fragile to weight PGD
attack. On the other hand, neural networks trained only with the robustness loss (λ > 0 and μ = 0)
or the generalization gap regularization (λ = 0 and μ > 0) can improve the robust accuracy due
to improved generalization and classification margin. Moreover, joint training using the proposed
loss with proper coefficients can further boost model performance (e.g., (λ,μ) = (0.0125,0.01)),
as seen by the significantly improved area under curve (AUC) score of robust accuracy over all
tested values. The AUC of the best model is about 2× larger than that of the standard model.
Similar results can be concluded for the attack with 100 steps (see Appendix D). In Appendix D.3,
we conduct additional experiments on the coefficients λ and μ and discuss their tradeoffs. We also
report the run-time analysis in Appendix F. Training with our loss function has a comparable per-
epoch run time when comparing to standard training.
5	Conclusion
In this paper, we developed a formal analysis of the robustness of the pairwise class margin for
neural networks against weight perturbations. We also characterized its generalization gap through
Rademacher complexity. A theory-driven loss function was proposed, and the empirical results
showed significantly improved performance in generalization and robustness. Our analysis offers
theoretical insights and training loss design principles for studying the generalization and robustness
of neural networks subject to weight perturbations.
References
Guozhong An. The effects of adding noise during backpropagation training on a generalization
performance. Neural computation, 8(3):643-674, 1996.
Alessandro Barenghi, Luca Breveglieri, Israel Koren, and David Naccache. Fault injection attacks
on cryptographic devices: Theory, practice, and countermeasures. Proceedings of the IEEE, 100
(11):3056-3076, 2012.
Andrew R Barron and Jason M Klusowski. Approximation and estimation for high-dimensional
deep learning networks. arXiv preprint arXiv:1809.03090, 2018.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks, 2019.
9
Under review as a conference paper at ICLR 2021
Sven Gowal, Krishnamurthy Dj Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan
Uesato, Relja Arandjelovic, Timothy Mann, and Pushmeet Kohli. Scalable verified training for
provably robust image classification. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 4842-4851, 2019.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. The Jour-
nal of Machine Learning Research, 18(1):6869-6898, 2017.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Pe-
ter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. Inter-
national Conference on Learning Representations, 2017.
Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu. Fault injection attack on deep neural network. In
IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pp. 131-138, 2017.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. International Conference on Learning
Representations, 2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generaliza-
tion in deep learning. In Advances in Neural Information Processing Systems, pp. 11615-11626,
2019.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in neural information processing systems, pp. 5947-5956,
2017.
Henning Petzka, Linara Adilova, Michael Kamp, and Cristian Sminchisescu. Feature-robustness,
flatness and generalization error for deep neural networks. arXiv preprint arXiv:2001.00939,
2020.
Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, and Yupeng Gao. Is robustness
the cost of accuracy?-a comprehensive study on the robustness of 18 deep image classification
models. In Proceedings of the European Conference on Computer Vision (ECCV), pp. 631-648,
2018.
Ryan Theisen, Jason M Klusowski, Huan Wang, Nitish Shirish Keskar, Caiming Xiong, and Richard
Socher. Global capacity measures for deep relu networks via path sampling. arXiv preprint
arXiv:1910.10245, 2019.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. In International Conference on Learning Representa-
tions, 2019.
Victor Van Der Veen, Yanick Fratantonio, Martina Lindorfer, Daniel Gruss, Clementine Maurice,
Giovanni Vigna, Herbert Bos, Kaveh Razavi, and Cristiano Giuffrida. Drammer: Deterministic
rowhammer attacks on mobile platforms. In ACM SIGSAC conference on computer and commu-
nications security, pp. 1675-1689, 2016.
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the
convergence and robustness of adversarial training. In International Conference on Machine
Learning, volume 1, pp. 2, 2019.
Tsui-Wei Weng, Pu Zhao, Sijia Liu, Pin-Yu Chen, Xue Lin, and Luca Daniel. Towards certifi-
cated model robustness against weight perturbations. In Proceedings of the AAAI Conference on
Artificial Intelligence, pp. 6356-6363, 2020.
10
Under review as a conference paper at ICLR 2021
Huan XU and Shie Mannor. Robustness and generalization. Machine learning, 86(3):391-423,
2012.
Dong Yin, Ramchandran Kannan, and Peter Bartlett. Rademacher complexity for adversarially
robust generalization. In International Conference on Machine Learning, pp. 7085-7094, 2019.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically principled trade-off between robustness and accuracy. International Conference on
Machine Learning, 2019.
Pu Zhao, Siyue Wang, Cheng Gongye, Yanzhi Wang, Yunsi Fei, and Xue Lin. Fault sneaking attack:
A stealthy framework for misleading deep neural networks. In ACM/IEEE Design Automation
Conference (DAC), pp. 1-6, 2019.
Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging
mode connectivity in loss landscapes and adversarial robustness. In International Conference
on Learning Representations, 2020.
A Margin Bound
A. 1 Single-Layer B ound
We shall first prove when N 6= L and follow similar reasoning to prove the case when N = L.
Consider the difference between set of pairwise margin fcij (x) - fWij (x), we have
fWcij (x) - fWij (x)
={WL- W*}(ZLT-zLT)
≤) ∣∣WiL: - W*∣∕ρ(WLTzL-2) -ρ(WLTzL-2)L
≤1wl- WjL：iii||WLT(ZL-2-zL-2)∣∣∞
≤) ∣∣WiL-WjL:lli ∣∣(WLT)T∣∣1,∞ ∣∣(zL-2 - ZL-2)∣∣∞
≤) IlWiL - WjL:lli ll(WLT )T∣∣1,∞ …∣∣(WN+1)T∣∣1,∞ ∣∣(W N-WN )zn-1∣∣∞
(e)
≤ e∣∣WiL-WjLJ∣1∣∣ZN-1∣∣1 ∏L-N TII(WL-k)τllι,∞,
(11)
(12)
(13)
(14)
(15)
(16)
where inequality (a) results from applying Holder inequality, and inequality (b) comes from the
contractive property (I-Lipschitz) of activation function ρ(∙). Inequality (c) and (d) come from
triangle inequality applied element-wise on vector WL-I(ZL-2 - ZL-2) combined with induction
while inequality (e) simply comes from the fact that every element in
matrix W N — WN
has at most
in magnitude.
With similar reasoning, we proof the scenario when N = L as following
fWicj (x) - fWij (x)
={WiL - WjLjZL-1 -{WL: - WL：}ZLT
(≤i) 2 1T ZL-1
= 2 ∣∣ZL-1∣∣1,
(17)
(18)
(19)
where inequality (i) comes from problem definition (within element-wise '∞ norm ball) and since
the activation function ρ(∙) is non-negative, we could transform the inner product to its '1 norm.
11
Under review as a conference paper at ICLR 2021
A.2 Multi-Layer Scenario
A.2.1 Key Prerequisite
Before going through the proof for all-perturbed bound, we shall be introducing a maximization
problem and search for its solution. Recall that we have defined in Section 3.1 the notion of output
vector under weight perturbation setting, we now consider maximizing its `1 norm over a given
perturbed matrix set W and give the following solution. Using the notation in Section 3.1, we
have Zk as the output vector under perturbation setting and write the optimal vector that achieve its
maximum '1 norm as zk*. We then obtain the following solution
UZklI = maxw Iizk ∣∣1,
. *	,
Wimj =Wimj+m, ∀i,j∀m∈ {2,..., L}
i,*j	i,j
Wi1,j = Wi1,j +sgn([x]j)1, ∀i,j
The reasoning behind uses the non-negative property of activation function, for any element in
a matrix W, the choice to maximize it’s `1 norm matrix-vector product is to go in the direction
identical to the sign of the vector’s element-wise value since the activation function is applied after
the first layer, we would then obtain the solution above.
A.2.2 All-Perturbed Bound
In the following proof for Theorem 2, we apply similar steps in Appendix A.1 and consider the dif-
ference between set of pairwise margin under natural and weight perturbation setting, recall in The-
orem 2 We defined that fw(x) = WL(..WN...ρ(W1x)...) and fw(x) = WL(..WN…P(W 1x)…)
Thus for any set of pairwise margin ficj (x) and fWij (x), we have
fwcij (x) - fwij (x)
={WL- WjL：}ZL-1 - {WiL: - WjLJzL-1
(20)
≤ ∣∣WL:- W*∣1∣∣P(WLT ZL-2) - ρ(WL-1zL-2)∣∣oo +2eL1TZLT	(21)
≤) ∣∣ WL： - WjL: ∣∣1 { ∣∣WLT(ZL-2 - ZL-2)L + ∣∣(WLT- WLT)ZL-2L } + 2eL ∣∣ZL-1∣∣ι
(22)
≤) ∣∣WiL -WjL儿{∣∣(WLT)T∣∣1,∞ ∣∣ρ(W L-2 ZL-3) - ρ(WL-2ZL-3)∣∞ + J ∣∣zl-2∣∣i }
+ 2eL∣∣ZL-1∣∣ι	(23)
(d)	L-3
≤ ∣∣WiL： - WjL儿卜 1 kxkiπL=ι2 ∣∣(WLT)T∣∣1,∞ + X (πM+2 ∣∣(Wk)T∣∣1,∞H+1 忸(
j =1
+ J W-2∣∣ι}"L ∣∣ZLT∣∣1	(24)
(e)	L-3
≤ ∣∣ WiL-WjL：L 卜 ι kχkι πL-21∣(WLT)T∣∣ι,∞+X mL-j1+21∣(Wk )t∣∣i,∞ m ∣∣Zj*∣∣1
j=1	1
+ L-1 ∣∣∣ZL-2* ∣∣∣1	+ 2L ∣∣∣ZL-1* ∣∣∣1	(25)
In the above proof, inequality (a) comes from the problem definition (perturbation of final layer
within EL) and inequality (b) results from the contractive property of ρ(∙) (1-LiPSchitz) combined
with the use of triangle inequality. Inequality (c) was achieved through triangle inequality applied
on elements of WL-1 (ZL-2 - ZL-2) and using the fact that WL 1 - WL-1 has every element less
12
Under review as a conference paper at ICLR 2021
than or equal to L-1 in magnitude. By the process of induction and maximizing the `1 norm of
perturbed output under weight perturbation zk, We arrive at inequality (d) and (e).
A.2.3 Multi-Layer B ound
We now proceed to utilize similar reasoning to establish the multi-layer bound when weight pertur-
bation is imposed according to an index set I
fW (χ)-fW (x)
={WiL:- WL}ZL-1 - {WiL: - WjLJzLT	(26)
≤ IlWL： — WjiIII P(W LT ZL-2) - ρ(WL-1zL-2)∣∣oo + 1(L ∈ I )2eL1T ZLT	(27)
≤ ∣∣WiL： - W*∣1 {∣∣WLT(ZL-2 -zL-2)∣∣oo +1∣(WLT- WLT)ZL-2L}
+ 1(L ∈ I)2eL∣∣ZL-1∣∣]	(28)
≤ ∣∣ WL-WjL：Ui {∣∣(WLT)T∣∣1,∞ ∣∣p(w L-2 ZL-3)-P(WL-2ZL-3)L
+ 1(L - 1 ∈ I)eL-i ∣∣ZL-2∣∣ι} + 1(L ∈ I)2eL ∣∣ZL-11∣ɪ	(29)
≤ ∣∣WL- wL.∣∣i {1(1 ∈ I旧 kxkι Π=12 ∣∣(WLT)T∣∣1,∞ + I(L - 1 ∈ I)eL-i W-21
L-3
+X 1(j+1 ∈I)(πL-1+21∣(Wk)t∣∣i,∞h+i bHJ+I(L∈IRL WIL	(3o)
≤∣∣wl-WjL儿{ X	(⅛1+ι∣∣(Wk)T∣∣ι,∞)e'∣∣Z'-1*∣∣1
I '∈I∖{L,L-1}
+ 1(L - 1 ∈ I)eL-i ∣∣ZL-2*∣∣ι I + 1(L ∈ I)2eL ∣∣ZL-1* ]	(31)
The proof for multi-layer bound follows same reasoning from the all-perturbed setting except in-
dicator function was added to check whether a certain layer m is in the index set I and at last we
rewrite the expression using the members of set I.
B Surrogate Loss
B.1 Case on Ramp Loss
We now provide a proof for Lemma 2. Recall the definition of ramp function in Section 3.4.1,
we have that ramp loss for a given data point (x, y) and neural network fw(∙) is written as
'ramp(fw(x), y) = φγ(M(fw(x), y)), where the function φγ : R → [0,1] is defined as
f 1	if	t	≤ 0
φγ(t) =	0	if	t	≥ γ
(1- t	if	t	∈ [0,γ]
(32)
13
Under review as a conference paper at ICLR 2021
Then for any (x, y), using ReLU as activation function, we have
1(y 6= arg mya0 x[fWc (x)]y0)
max
Wc
≤ φγ(minM(fWc(x),y))	(33)
Wc
(b)
≤ φγ (min min[fWc (x)]y - [fWc (x)]y0)	(34)
y0 6=y Wc
(c)
≤ φγ(m=n[fw(X)]y-[fw(X)]y0 - maxeIIWL,: - WLJ∣1 IIzN-1llι nL=NTU(WLi)Th∞)
(35)
(d)
≤ φγ( mn[fw (X)]y-[fw (χ)]y0- 2 max eUWL：^ ||zN-1||i nfc=ι - ||(W - )T||i ∞ )
y 6=y	k∈[K ]
(36)
(e)
≤ φγ(M(fw(χ),y) -2max]e||WL：||i||ZN-1||inL=N-1|(wL-k)Th∞)	(37)
(f)
≤ φγ(M(fw(χ),y) - 2max]e||wL：||ikxki∏N=1 kWmkι,∞∏L=NT||(WL-k)τh∞)
(38)
ʌ,. ,. 、
:='(fw (χ),y)	(39)
(g)
≤ 1(M(fw(χ),y) - 2maxJ|wL：||ikxkinm=1 kwmkι,∞∏L=N-1 ||(wL-k)T||i,g ≤ Y),
(40)
where inequality (a) is due to the property of ramp loss while inequality (b) is by the definition of
margin and inequality (c) comes from applying Theorem 1. Inequality (d) results from using triangle
inequality and taking its maximum, inequality (e) is by the definition of margin and inequality (f)
comes from the fact that with ReLU we have kρ(Aχ)k1 ≤ kAχk1. Lastly, inequality (g) is a direct
consequence from property of ramp loss.
B.1.1	Ramp Loss on Multiple Layer B ound
We now follow a similar course and prove robust ramp loss using the multi-layer bound in Theorem
3. We consider the robust loss form proposed in Section 3.4.1 and have that,
max 'ramp (fw (χ), y)
Wc
(a)
≤ φγ(min M (fWc (χ), y))
Wc
(b)	0
≤ φγ( min[fW (χ)]y - [fW (X)]y0 - max nW (XII))
y0 6=y	y0 6=y
(c)	0
≤ φγ (M (fw (χ),y) - max nW (χ∣I)) ：= '(fw (χ),y)
y0 6=y
(41)
(42)
(43)
B.2 Cross Entropy
We further consider the case of cross entropy and prove an upper bound for it. We denote the loss
function as CE(∙), and during training, hard label was applied. Recall the definition of fW(χ) m
14
Under review as a conference paper at ICLR 2021
Section 3.4.1, we have the difference of loss function between natural and perturbation settings as,
〜
〜
CE(fw (χ),y)- CE (fw (x),y)
-yln
-~ ，一
[fW（X）]y
-~ ，一
[fW(X)]y
(44)
ln
-~ ，一
[fW(X)]y
-~ ，一
[fW(X)]y
(45)
ln
P	e[fWc (X)]k
(e[fW (X)]y-[fW (X)]y 乙 k∈[K]_____}
(	Pk∈[κ]e[fW(X)]k}
(46)
(a)
≤ ln
(maxe[fw(X)]y-[fw(X)]y ∙ e[fw(X)]y0-[fw(X)]y0
y06=y
(47)
≤) ln (emaxy0=y fWy(X)-fWy(X)
=max nW (XII),
y0 6=y
(48)
(49)
where inequality (a) comes from taking the maximum in the set of all ratios { ：fW(X)： } and in-
equality (b) comes from monotonicity of exponential function. Finally, the last expression (c) can
be referred to Theorem 3. Thus with the above proof, we could establish the following robust surro-
gate loss for cross entropy, denoted as CE(fW (X), y) :
0
CcE(fW(X),y) = CE(fW (X), y) + maxy06=y ηWy y(X|I)
C Generalization B ound on Rademacher Complexity
C.1 Proof on Single Layer Bound
To show the Rademacher complexity and generalization gap on single layer robust surrogate loss,
we first introduce a result proven in (Bartlett et al., 2017) and another classical result in statistical
learning theory and proceed to give a proof on Theorem 4. Given a set S = {(Xi, yi)}in=1 of i.i.d
training samples, denote X := [X1, X2, . . . , Xn] ∈ Rd×n as the matrix composed of training data and
let dmax = max{d, d1, . . . , dL} as the maximum dimension among all weight matrices.
Lemma 3 (Mohri et al. (2018)) Assume that the range of loss function '(∙) is [0,1]. Then, for any
δ ∈ (0, 1), with probability as least 1 - δ, we have for all f ∈ F
R(f) ≤ Rn(f) + 2R('f) + 3~,
where R(f) and Rn(f) stand for population risk and empirical risk, respectively.
Lemma 4 (Bartlett et al. (2017)) Consider the neural network hypothesis class,
F={fW(x) |	W= (W1,W2,...,WL),Whσ ≤sh,(Wh)T2,1	≤bhh∈	[L]}
We have an upper bound on the Rademacher complexity,
R(F) ≤ 令 + 26log(n)lng(2dmax) kXkF (∏L=ιSh)(pL=ι (bj)2/3)3/2
We now study the Rademacher Complexity of the function class
ʌ , , 、	ʌ, . , . . . _____
'f = {(X,y) → '(fw(X),y)I f ∈ F},
15
Under review as a conference paper at ICLR 2021
where '(∙) is denoted in Lemma 2 and let MF = {(x,y) → M (fw (x),y)∣ f ∈ F}. Then we could
obtain,
1	2	n
R('f) ≤ -(R(Mf) + -Eν[sup X Vi maχ∣∣WLJ∣ι ∏m=1 kWmkι ∏L-NTll(WL-k)τ∣h ∞ kXik」),
γ	n f∈F i=1 k∈[K]	,
(50)
where the inequality was achieved by using the Ledoux-Talagrand contraction inequality and the
convexity of the supreme operation. Consider the second term, we have that
2	n
-Eν [sup X Vi max∣∣wL,-.∣∣ι πN-1 kWmkι πL-N τ∣∣(wL-k )T∣∣1∞ kXiki]	(51)
n f∈F i=1	k∈[K]
(a)	2	n
≤ —(Supm喻 ∣∣wLj∣ι∏m=ιkwmkι∏l-n-ι∣(wL-k)t∣∣i∞)e“【iXVikχi∣g](52)
n f∈F k∈[K]	,	i=1
(b)	2
≤ —(Supnm=I kWmkι ∏L-N τ∣∣(WL-k)T∣∣ι ∞ ) kXk1,2,	(53)
n f∈F
where inequality (a) is achieved by separating all neural network related parameters and inequality
(b) is a result of applying Khintchine’s inequality.
Thus, combined with Lemma 4, we have that
阳击)≤ Y (n4∕2 +
60 log(n) log(2dmax )
n
kXkF
+ 2∣ (Supnm=I kWmkι nL-N τ∣∣(wL-k)τ∣∣ι ∞) kχk1,2
n f∈F
(54)
z ʌ	1	1	1.1	1	1 i' zT'> /二 ∖	.1	EI	A ∙	1∙	i'
Once We have calculated an upper bound for R('f), then Theorem 4 is a direct consequence of
Lemma 2 and 3.
C.2 Extension to Multiple Layer Bound
In this section, we consider the robust surrogate loss under multi-layer bound and study its
Rademacher complexity, We first give the expression of the robust surrogate loss then give an result
on generalization bound.
Lemma 5 Define the robust lossfunction '(fw(X), y) as
'(fw (x ),y) = φγ (M (fw (X ),y)
-2 max] ∣∣WLJ∣1 { X	Mni=IW *∣∣1 ∞)( j'+ι ∣∣( Wj)T∣∣ι,∞) k X kι
'∈I∖{L,L-1}	,∞
+ 1(L - 1 ∈ I )eL-ι(n"ι2 ∣∣ 犷 *∣∣1oq) k x kι } - 1(L ∈ I ”“(口31 ∣∣ 犷 * ∣∣ IOj k X kJ (55)
We would have that
1(y = argm^ax[fW(X)]y，)≤ '(fw(X),y)
max
Wc
≤ 1 M (fW (X), y) - 2 max ∣∣WkL,: ∣∣1 X	e'M=ι ∣M *∣∣ι,∞)(j+ι∣∣( W呼 ∣∣1,∞) k X kι
'	[ ]	'∈I∖{L,L-1}	,
+ 1(L - 1 ∈ I )eL-i(∏L-2 W *∣∣ioo) k X kι } - 1(L ∈ I ""(口仁1 W * ∣∣ItJ k X h ≤ Y)
Using the above loss, we could further establish an upper bound on robust surrogate loss and provide
statements on generalization bound. Given the following function class
16
Under review as a conference paper at ICLR 2021
ʌ
ʌ
'F = {(x,y)→ '(fw(x),y)∣f ∈ F}
We have that,
12
R('f) ≤ - (R(Mf) + -EV
Y	n
n
fuFX VimaK]∣∣wL』1{ 'ei∖Xl-JM=1
(∏L-+1 ∣∣ (Wj)T∣∣1,∞)kxik1 + 1(L - 1 ∈ I)≡l-1(∏L-12
2 L -
H— EV sup
Lf ∈F
n
X ViI(L ∈ i)"(∏L=II
i=1
(56)
n
which the second term can be bounded as,
2	n
-Eν sup V
n Lf ∈F £
i=1
Vimax]∣∣*∣1{	E	e'(∏i=1∣∣Wil1,J×
L J	'∈I∖{L,L-1}	,
n
(∏L-+1∣∣(Wj)T ∣∣1,∞)1阂11 + I(L -1 ∈ i )
2
≤ n泮max]{∣∣〜∣∣1{ X	Mni=
f ∈F	[ ]	'∈I∖{L,L-1}
(57)
+ I(L - 1 ∈ i)el-1(πL=12
(58)
≤ — sup max
n f ∈F k∈[K]
n
)}}Eν[∣XVi kxikι∣]
i=1
α∏i=ι∣∣Wi1,J(∏L='+ι∣∣(Wj)T∣∣ι,∞)
+ 1(L - 1 ∈ I)6l-1
'∈I∖{L,L-1}
(∏L=ι2
(59)
while the last term can as well be bounded as,
2
-EV sup
Lf ∈F
n
X ViI(L ∈ I)eL(∏”11
i=1
kXk1,2
XikI
2supf∈F 1(L ∈ I)6L(∏i⅛1∣∣Wi*
n
21(L ∈ I)α SUPf ∈F (∏L-l1 ∣∣Wi*
n
4Eν[∣ £ % ∣∣xi∣∣1 ∣]
i=1
」kXk1,2
(60)
(61)
(62)
"儿{ E
n
≤
≤
n
With all of the upper bounds above and Lemma 4, 5, we have the following theorem,
Theorem 5 (generalization gap for robust surrogate loss) With Lemma 5, consider the neural
(W1,W2,...,WL),∣∣Wh∣∣σ ≤ Sh,∣∣(Wh)T∣∣2,1 ≤
network hypothesis class F = {fw(x)∣W
bh, h ∈ [L]}. Forany γ > 0, with probability at least 1 - δ, we havefor all fw (∙) ∈ F
1 n
P(x,y)~D {∃W s∙t∙ y = arg max [fw(χ)]/} ≤ -X 1( [fw(χ/为 ≤ 7 + max[fw(χ∕/ + 2 max ||WL：||
y ∈[K]	n i=ι ∖	y ≠Vi	k∈[K∖ Il	∣∣1
× { X	ee(∏i≠1∣W*∣l1	)(∏L≠'+1∣∣(w')t∣∣i )+1(L - 1 ∈ I)ez—1(Π≠12∣∣Wi*∣l	)} kXikι
'∈I∖{L,L-1}	,∞	,∞	,∞
+ 21(L ∈ I)eL(nL—l1∣∣Wiɪ J kXikι ) + 1
+ 60log(n) log(2dmaχ)
n
+n SuF m 筋{IwML
X ee(∏ 川 Wi *∣∣1 J(⅛⅛1
'∈I∖{L,L — 1}
+ i(l -1 ∈ i "—1 (∏L-21∣ 犷 * ∣∣ico)} + 21 (L ∈ i )α (∏L-11∣ 犷 *
k X kF
(63)
17
Under review as a conference paper at ICLR 2021
D Additional Experiments
D.1 Supplement of Figure 1 (a)
For completeness, Figure 2 adds the curve of = 0 (the standard generalization setting) in Figure 1
(a). The generalization gap is obviously lower than others when = 0, but its trend with respect to
regularization is similar to the setting when 6= 0.
Figure 2: Empirical generalization gaps when altering the matrix norm regularization coefficient μ
in (9).
D.2 weight PGD attack (100 steps) on MNIST
Figure 3 shows the accuracy and AUC score of different models against weight PGD attack with
100 iterations.
Figure 3: Comparison of test accuracy of neural networks trained with different coefficients λ and
μ against weight PGD attack (100 steps) with perturbation level e.
18
Under review as a conference paper at ICLR 2021
D.3 More details on the trade-off between λ and μ
(a) weight PGD ( = 0.01)
Figure 4: Comparison of test accuracy (%) trained With different coefficients λ and μ against weight
PGD attack (200 steps) with = 0.01 and 0.02. The experiment setting follows Figure 1(b).
(b) weight PGD ( = 0.02)
Figure 4 shows the trade-off with a fine grid search between coefficients λ and μ, we present test
accuracy under weight PGD attack using perturbation radius of ( = 0.01 and 0.02) with different
combinations of λ (from 0.01 to 0.015) and μ (from 0 to 0.05). We find that there is indeed a sweet
spot with proper values of λ and μ leading to significantly better robust accuracy. When both λ and
μ are too large or too small, the robustness of the model will decrease.
D.4 Alternative Robust Loss Function
In addition to the proposed loss function in equation (9), one can consider an alternative general-
ization gap regularization term derived from Theorem 4, which is Pm=ι(log (∣∣(Wm)>∣∣ι,∞) +
log (||Wm ∣∣ι,∞)). We compare its performance following the same experiment setting as in Figure
1 (b) with finetuned coefficients λ and μ. It can be observed that this alternative loss function also
yields robust models with comparable (sometimes slightly better) performance to those in Figure
1 (b), verifying the effectiveness in using theory-driven insights to reduce the generalization gap
against weight perturbation.
Figure 5: Test accuracy under different weight perturbation level with 200 attack steps. The models
are trained using the alternative loss described in Section D.4.
—入=0, μ=0, auc=17.78%
→- λ=0.001, μ=0.1, auc=32.88%
→- λ≡0.002, μ≡0.1f auc≡28.52%
→- λ=0.00125, μ=0.1, auc≡35.88%
→-入=0.00175, μ=0.1, auc=39.0%
—入=0.0001, μ=0.1, auc=38.1%
→- λ=0, μ=0.1, auc=33.96%
19
Under review as a conference paper at ICLR 2021
E	On the vacuity of generalization bound
In (Nagarajan & Kolter, 2019), empirical observations were made to point out the fact that when
given increase in the size of the training data, the error bound proposed in (Bartlett et al., 2017)
grows rapidly, loosing the ability to describe generalization gap and thus becomes vacuous. How-
ever, we note that under our settings with models trained using the loss function in Section 3.5, the
bound would not grow in a polynomial rate and instead shows a decreasing trend. We conducted
experiments and presented results under the same setting as (Nagarajan & Kolter, 2019) in Figure 6.
Here we verify two existing generalization bounds from different literature, one from (Bartlett et al.,
2017) while another one from (Barron & Klusowski, 2018) in which the former one is composed
mainly of product of weight matrices’ norm and the latter is comprised of the norm of matrices’
product. Empirical results in Figure 6(a) show that under the standard settings the main components
of generalization bound in (Bartlett et al., 2017) and Barron & Klusowski (2018) both grows rapidly
with respect to the increase in size of the training dataset, as confirmed in (Nagarajan & Kolter,
2019). Another empirical finding in the last column of Figure 6(a) shows that the multiplicative dif-
ference between bounds in (Bartlett et al., 2017) and (Barron & Klusowski, 2018) exhibit a constant
rate, demonstrating the vacuity of both bounds. However, when measuring the same component
under our setting in Figure 6(b), new results showed decreasing bounds as the size of the training
dataset increases, concluding the non-vacuity of the associated generalization bounds in our settings.
(a) Standard Model (λ = 0, μ = 0)
(b) Robust Model (λ = 0.01,μ = 0.01)
Figure 6: Statistics associated with generalization bounds for standard model and robust model
trained using equation (9). The experiment setting follows Figure 1(a). In the first column, we
present test error under weight PGD attack with perturbation level . The curve with = 0 (no
weight perturbation) corresponds to the standard generalization setting. In the second column, we
present the product of spectral norms of the weights matrices, related to bounds in (Bartlett et al.,
2017). In the third column, we show spectral norm of the product of the weight matrices, related to
bounds in (Barron & Klusowski, 2018). In the fourth column, we show the product of spectral norms
of the weights matrices divided by the spectral norm of the product of the weight matrices. Notably,
we present each value into the logarithm function. For the standard model, both types of bound
increase with respect to training set size and are shown to be vacuous, consistent with the results
in (Nagarajan & Kolter, 2019). For the robust model, both types of bound exhibit same decreasing
trend as the test error and therefore are non-vacuous. Moreover, these two bounds demonstrate
similar scaling behavior (nearly constant log ratio) in both standard and robust models.
20
Under review as a conference paper at ICLR 2021
As an ablation study, Figure 7 shows the performance of the robust model with (λ = 0.01, μ = 0).
That is, training a neural network without the generalization regularization term in equation (9).
Unlike the standard model, it is observed that the generalization bounds still show decreasing trend
with the test error. This is due to the fact that the robustness term in equation (9), which is induced
from our analysis of the worst-case error propagation from weight perturbation, also plays a role
in regularizing the network weights, and therefore making the resulting generalization bound non-
vacuous.
Training Set Size
Training Set Size	Training Set Size
Training Set Size
Figure 7: Ablation study of the robust model with (λ = 0.01, μ = 0). The experiment setting is
the same as Figure 6. In the absence of the generalization regularization term in equation (9), the
generalization bounds still show decreasing trend with the test error.
F Run Time Analysis
Table 1 reports the per-epoch run time of the models trained with the standard model (λ = 0, μ = 0)
and the robust model (λ = 0.01, μ = 0.1) using equation (9). We train both models with 20 epochs
and use the same hyperparameters, including setting the SGD optimizer with learning rate as 0.01,
and setting the batch size as 32. The cost of training both models are comparable.
Standard model (λ = 0, μ = 0) Robust model (λ = 0.01, μ = 0.01)
Avg. per-epoch run time	5.125 sec.	6.69 sec.
Table 1: Per-epoch run time (in seconds) averaged over 20 epochs with the same hyperparameters
for the models trained with (λ = 0, μ = 0) and (λ = 0.01, μ = 0.1) based on equation (9).
21