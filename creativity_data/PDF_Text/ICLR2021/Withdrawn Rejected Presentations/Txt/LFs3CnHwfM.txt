Under review as a conference paper at ICLR 2021
A Robust Fuel Optimization Strategy For Hy-
brid Electric Vehicles: A Deep Reinforcement
Learning Based Continuous Time Design Ap-
PROACH
Anonymous authors
Paper under double-blind review
Ab stract
This paper deals with the fuel optimization problem for hybrid electric vehicles in
reinforcement learning framework. Firstly, considering the hybrid electric vehicle
as a completely observable non-linear system with uncertain dynamics, we solve
an open-loop deterministic optimization problem to determine a nominal optimal
state. This is followed by the design of a deep reinforcement learning based op-
timal controller for the non-linear system using concurrent learning based system
identifier such that the actual states and the control policy are able to track the
optimal state and optimal policy, autonomously even in the presence of external
disturbances, modeling errors, uncertainties and noise and signigicantly reducing
the computational complexity at the same time, which is in sharp contrast to the
conventional methods like PID and Model Predictive Control (MPC) as well as
traditional RL approaches like ADP, DDP and DQN that mostly depend on a set
of pre-defined rules and provide sub-optimal solutions under similar conditions.
The low value of the H-infinity (H∞) performance index of the proposed opti-
mization algorithm addresses the robustness issue. The optimization technique
thus proposed is compared with the traditional fuel optimization strategies for hy-
brid electric vehicles to illustate the efficacy of the proposed method.
1 Introduction
Hybrid electric vehicles powered by fuel cells and batteries have attracted great enthusiasm in mod-
ern days as they have the potential to eliminate emissions from the transport sector. Now, both
the fuel cells and batteries have got several operational challenges which make the separate use of
each of them in automotive systems quite impractical. HEVs and PHEVs powered by conventional
diesel engines and batteries merely reduce the emissions, but cannot eliminate completely. Some
of the drawbacks include carbon emission causing environmental pollution from fuel cells and long
charging times, limited driving distance per charge, non-availability of charging stations along the
driving distance for the batteries. Fuel Cell powered Hybrid Electric Vehicles (FCHEVs) powered
by fuel cells and batteries offer emission-free operation while overcoming the limitations of driv-
ing distance per charge and long charging times. So, FCHEVs have gained significant attention in
recent years. As we find, most of the existing research which studied and developed several types
of Fuel and Energy Management Systems (FEMS) for transport applications include Sulaiman et
al. (2018) who has presented a critical review of different energy and fuel management strategies for
FCHEVs. Li et al. (2017) has presented an extensive review of FMS objectives and strategies for
FCHEVs. These strategies, however can be divided into two groups, i.e., model-based and model-
free. The model-based methods mostly depend on the discretization of the state space and therefore
suffers from the inherent curse of dimensionality. The coumputational complexity increases in an
exponential fashion with the increase in the dimension of the state space. This is quite evident in the
methods like state-based EMS (Jan et al., 2014; Zadeh et al., 2014; 2016), rule-based fuzzy logic
strategy (Motapon et al., 2014), classical PI and PID strategies (Segura et al., 2012), Potryagin’s
minimum principle (PMP) (Zheng et al., 2013; 2014), model predictive control (MPC) (Kim et al.,
2007; Torreglosa et al., 2014) and differential dynamic programming (DDP) (Kim et al., 2007). Out
of all these methods, differential dynamic programming is considered to be computationally quite
1
Under review as a conference paper at ICLR 2021
efficient which rely on the linearization of the non-linear system equations about a nominal state
trajectory followed by a policy iteration to improve the policy. In this approach, the control policy
for fuel optimization is used to compute the optimal trajectory and the policy is updated until the
convergence is achieved.
The model-free methods mostly deal with the Adaptive Dynamic Programming (Bithmead et al.,
1991; Zhong et al., 2014) and Reinforcement Learning (RL) based strategies (Mitrovic et al., 2010;
Khan et al., 2012) icluding DDP (Mayne et al., 1970). Here, they tend to compute the control pol-
icy for fuel optimization by continous engagement with the environment and measuring the system
response thus enabling it to achieve at a solution of the DP equation recursively in an online fash-
ion. In deep reinforcement learning, multi-layer neural networks are used to represent the learning
function using a non-linear parameterized approximation form. Although a compact paremeterized
form do exist for the learning function, the inability to know it apriori renders the method suffer
from the curse of dimensionality (O(d1 2) where, d is the dimension of the state space), thus making
it infeasible to apply to a high-dimemsional fuel managememt system.
The problem of computational complexity of the traditional RL methods like policy iteration (PI)
and value iteration (VI) (Bellman et al., 1954; 2003; Barto et al., 1983; Bartsekas, 2007) can be
overcome by a simulation based approach (Sutton et al., 1998) where the policy or the value function
can be parameterized with sufficient accuracy using a small number of parameters. Thus, we will be
able to transform the optimal control problem to an approximation problem in the parameter space
(Bartesekas et al., 1996; Tsitsiklis et al., 2003; Konda et al., 2004) side stepping the need for model
knowledge and excessive computations. However, the convergence requires sufficient exploration
of the state-action space and the optimality of the obtained policy depends primarily on the accuracy
of the parameterization scheme.
As a result, a good approximation of the value function is of utmost importance to the stability
of the closed-loop system and it requires convergence of the unknown parameters to their optimal
values. Hence, this sufficient exploration condition manifests itself as a persistence of excitation
(PE) condition when RL is implemented online (Mehta et al., 2009; Bhasin et al., 2013; Vrabie,
2010) which is impossible to be guaranteed a priori.
Most of the traditional approaches for fuel optimization are unable to adrress the robustness issue.
The methods described in the literature including those of PID (Segura et al.,2012), Model Predic-
tive Control (MPC) (Kim et al.,2007;Torreglosa et al., 2014) and Adaptive Dynamic Programming
(Bithmead et al.,1991;Zhong et al., 2014) as well as the simulation based RL strategies (Bartesekas
et al., 1996; Tsitsiklis et al., 2003; Konda et al., 2004 ) suffer from the drawback of providing a sub-
optimal solution in the presence of external disturbances and noise. As a result, application of these
methods for fuel optimization for hybrid electric vehicles that are plagued by various disturbances
in the form of sudden charge and fuel depletion, change in the environment and in the values of the
parameters like remaining useful life, internal resistance, voltage and temperature of the battery, are
quite impractical.
The fuel optimization problem for the hybrid electric vehicle therefore have been formulated as a
fully observed stochastic Markov Decision Process (MDP). Instead of using Trajectory-optimized
LQG (T-LQG) or Model Predictive Control (MPC) to provide a sub-optimal solution in the presence
of disturbances and noice, we propose a deep reinforcement learning-based optimization strategy
using concurrent learning (CL) that uses the state-derivative-action-reward tuples to present a robust
optimal solution. The convergence of the weight estimates of the policy and the value function to
their optimal values justifies our claim. The two major contributions of the proposed approch can be
therefore be summarized as follows:
1) The popular methods in RL literature including policy iteration and value iteration suffers from the
curse of dimensionality owing to the use of a simulation based technique which requires sufficient
exploration of the state space (PE condition). Therefore, the proposed model-based RL scheme
aims to relax the PE condition by using a concurrent learning (CL)-based system identifier to reduce
the computational complexity. Generally, an estimate of the true controller designed using the CL-
based method introduces an approximate estimation error which makes the stability analysis of the
system quite intractable. The proposed method, however, has been able to establish the stability
of the closed-loop system by introducing the estimation error and analyzing the augmented system
trajectory obtained under the influnece of the control signal.
2
Under review as a conference paper at ICLR 2021
2) The proposed optimization algorithm implemented for fuel management in hybrid electric vehi-
cles will nullify the limitations of the conventional fuel management approaches (PID, Model Pre-
dictive Control, ECMS, PMP) and traditional RL approaches (Adaptive Dynamic Proagramming,
DDP, DQN), all of which suffers from the problem of sub-optimal behaviour in the presence of ex-
ternal disturbances, model-uncertainties, frequent charging and discharging, change of enviroment
and other noises. The H-infinity (H∞) performance index defined as the ratio of the disturbance to
the control energy has been established for the RL based optimization technique and compared with
the traditional strategies to address the robustness issue of the proposed design scheme.
The rest of the paper is organised as follows: Section 2 presents the problem formulation including
the open-loop optimization and reinforcement learning-based optimal controller design which have
been described in subsections 2.1 and 2.2 respectively. The parametric system identification and
value function approximation have been detailed in subsections 2.2.1 and 2.2.2. This is followed by
the stability and robustness analysis (using the H-infinity (H∞ ) performance index ) of the closed
loop system in subsection 2.2.4. Section 3 provides the simulation results and discussion followed
by the conclusion in Section 4.
2 Problem Formulation
Considering the fuel management system of a hybrid electric vehicle as a continous time affine
non-linear dynamical system:
X = f (x, W) + g(x)u,
y = h(x,v)
(1)
where, x ∈ Rnx, y ∈ Rny , u ∈ Rnu are the state, output and the control vectors respectively, f(.)
denotes the drift dynamics and g(.) denotes the control effectivenss matrix. The functions f and h
are assumed to be locally LiPschitz continuous functions such that f(0) = 0 and Nf(x) is ContinoUs for
every bounded x ∈ Rnx. The process noise w and measurement noise v are assumed to be zero-mean,
uncorrelated Gausssian white noise with covariances W and V, resPectively.
Assumption 1: We consider the system to be fully observed:
y = h(x, v) = x
(2)
Remark 1: This assumPtion is considered to Provide a tractable formulation of the fuel management
Problem to side steP the need for a comPlex treatment which is required when a stochastic control
Problem is treated as Partially observed MDP (POMDP).
Optimal Control Problem: For a continous time system with unknown nonlinear dynamics f(.), we
need to find an oPtimal control Policy πt in a finite time horizon [0, t] where πt is the control Policy at
time t such that πt = u(t) to minimize the cost function given by J =
where, Q,F > 0 and R ≥ 0.
2.1 Open Loop Optimization
Considering a noise-free non-linear stochastic dynamical system with unknown dynamics:
X = f (x, 0) + g(x)u,
y = h(x, v) = x
(3)
where, x0 ∈ Rnx , y ∈ Rny , u ∈ Rnu are the initial state, outPut and the control vectors resPec-
tively, f(.) have their usual meanings and the corresPonding cost function is given by Jd (x0, ut) =
Z t(xT
0
Qx + uRuT)dt + xTFx.
Remark: We have used Piecewise convex function to aPProximate the non-convex fuel function
globally which has been used to formulate the cost function for the fuel oPtimization.
3
Under review as a conference paper at ICLR 2021
The open loop optimization problem is to find the control sequence ut such that for a given initial
state x0 ,
Ut = arg min Jd(xo, ut),
subject to X = f(x, 0) + g(x)u,	(4)
y = h(x, v) = x.
The problem is solved using the gradient descent approach (Bryson et al., 1962; Gosavi et al., 2003),
and the procedure is illustrated as follows:
Starting from a random initial value of the control sequence U(0) = [ut(0)] the control policy is updated
iteratively as
U(n+1) = U(n) - αVu Jd(xo, U⑺)，	(5)
until the convergence is achieved upto a certain degree of accuracy where U(n) denotes the control
value at the nth iteration and α is the step size parameter. The gradient vector is given by:
VU Jd(X°, U ⑺)=(J, ∂ji , ∂jj2 ,..…,J)|(x0,Ut)	⑹
The Gradient Descent Algorithm showing the approach has been detailed in the Appendix A.1.
Remark 2: The open loop optimization problem is thus solved using the gradient descent approach
considering a black-box model of the underlying system dynamics using a sequence of input-output
tests without having the perfect knowlegde about the non-linearities in the model at the time of the
design. This method proves to be a very simple and useful strategy for implementation in case of
complex dynamical systems with complicated cost-to-go functions and suitable for parallelization.
2.2 Reinforcement Learning Based Optimal Controller Design
Considering the affine non-linear dynamical system given by equation (1), our objective is to design
a control law to track the optimal time-varying trajectory x(t) ∈ Rnx. A novel cost function is
formulated in terms of the tracking error defined by e = x(t) 一 x(t) and the control error defined
by the difference between the actual control signal and the desired optimal control signal. This
formulation helps to overcome the challenge of the infinte cost posed by the cost function when it
is defined in terms of the tarcking error e(t) and the actual control signal signal u(t) only (Zhang et
al., 2011; Kamalapurkar et al., 2015). The following assumptions is made to determine the desired
steady state control.
Assumption 2: (Kamalapurkar et al., 2015) The function g(x) in equation (1) is bounded, the matrix
g(x) has full column rank for all X(t) ∈ Rnx and the function g+ : Rn → RmXn which is defined as
g+ = (gT g)-1 is bounded and locally Lipschitz.
Assumption 3: (Kamalapurkar et al., 2015) The optimal trajectory is bounded by a known positive
constant b E R such that ∣∣xk ≤ b and there exists a locally Lipschitz function hd such that X = hd
(X) and g(x) g+(X)(hd(x) - f (X)) = hd(x) - f (x).
Using the Assumption 2 and Assumption 3, the control signal ud required to track the desired trajec-
tory X(t) is given as Ud(X) = gd(hd(X) 一 fd) where fd = f (X) and gd = g+(X). The control error
is given by μ = u(t) - Ud(X). The system dynamics can now be expressed as
, ..
Z = F (Z)+ G(Z)μ	(7)
where, the merged state Z(t) ∈ R2n is given by Z(t) = [eT, XT]T and the functions F(Z) and G(Z)
are defined as F(Z) = [f T(e + X) - hT + UT(X)gT(e + X), hT]T and G(Z) = [gT(e + X), 0mχn]T
where, 0mχn denotes a matrix of zeroes. The control error μ is treated hereafter as the design
variable. The control objective is to solve a finite-horizon optimal tracking problem online, i.e.,
to design a control signal μ that will minimize the cost-to-go function, while tracking the desired
J (Z,μ) = Zt
0
trajectory, is given by
r(Z(τ), μ(τ))dτ where, the local cost r : R2nXRm → R is
given as r(Z, T) = Q(e) + μτRμ, R E RmXm is a positive definite symmetric matrix and Q : Rn
→ R is a continous positive definite function.
Based on the assumption of the existence of an optimal policy, it can be character-
ized in terms of the value function V * : R2n → R which is defined as V *(Z)=
4
Under review as a conference paper at ICLR 2021
min
μ(τ)eU∣τeRt>0 Zo r(φ (n, t,
Z),μ(τ))dτ, where U ∈ Rm
is the action space and φu(t; t0, ζ0)
is the trajectory of the system defined by equation (10) with the control effort μ : R>o → Rm with
the initial condition Zo ∈ R2n and the initial time to ∈ R>o. Taking into consideration that an op-
timal policy exists and that V * is continously differentiable everywhere, the closed-form solution
(Kirk, 2004) is given as μ*(Z) = -1/2 RTGT(Z)(▽《V*(Z))T where, NZ(.) = d(1). This satisfies
the Hamilton-Jacobi-Bellman (HJB) equation (Kirk, 2004) given as
▽z V *(Z)(F(Z) + G(Z)μ*(Z)) + Q(Z) + μ*τ (Z )Rμ*(Z )= 0	(8)
where, the initial condition V * = 0, and the funtion Q : R2n → R is defined as Q([eτ ,Xt]t )=
Q(e) where, (e(t),X(t)) ∈ Rn.
Since, a closed-form solution of the HJB equation is generally infeasible to obtain, we sought an
approximate solution. Therefore, an actor-critic based method is used to obtain the parametric
estimates of the optimal value function and the optimal policy which are given as V(Z, Wc) and
μ(Z, Wa) where, Wc ∈ RL and Wa ∈ RL define the vector paramater estimates. The task of the
actor and critic is to learn the corresponding parameters. Replacing the estimates V and μ for V * and
μ* in the HJB equation, we obtain the residual error, also known as the Bell Error (BE) as δ(Z, Wc,
Wa) = Q(Z) + μτ(Z, Wa)Rμ(Z, Wa) + VZV(Z, WC)(F(Z) + G(Z)μ(Z, Wa)) where, δ : R2n X RL
X RL → R. The solution of the problem requires the actor and the critic to find a set of parameters
Wa and WC respectively such that δ(Z, W2, Wa) = 0 andμτ(Z, Wa) = -1/2 RTGT(Z)(VZV*(Z))T
where, ∀Z ∈ Rn . As the exact basis fucntion for the approximation is not known apriori, we seek
to find a set of approximate parameters that minimizes the BE. However, an uniform approximation
of the value function and the optimal control policy over the entire operating domain requires to
find parameters that will able to minimize the error Es : RL X RL → R defined as Es(Wc, Wa)
=SupZ(∣δ, Wc, Wa|) thus, making it necessary to have an exact knowledge of the system model.
Two of the most popular methods used to render the design of the control strategy robust to system
uncertainties in this context are integral RL (Lewis et al., 2012; Modares et al., 2014) and state
derivative estimation (Bhasin et al., 2013; Kamalapurkar et al., 2014). Both of these methods suf-
fer from the persistence of exitation(PE) condition that requires the state trajectory φu(t; to, Zo) to
cover the entire operating domain for the convergence of the parameters to their optimal values. We
have relaxed this condition where the integral technique is used in augmentation with the replay of
the experience where every evaluation of the BE is intuitively formalized as a gained experience,
and these experiences are kept in a history stack so that they can be iteratively used by the learning
algorithm to improve data efficiency.
Therefore, to relax the PE condition, the we have developed a CL-based system identifier which
is used to model the parametric estimate of the system drift dynamics and is used to simulate the
experience by extrapolating the Bell Error (BE) over the unexplored territory in the operating domain
thereby, prompting an exponential convergence of the parameters to their optimal values.
2.2. 1 Parametric System Identification
Defined by any compact set C ⊂ R, the function f can be defined using a neural network (NN)
as f(x) = θTσf(YTx1) + o(x) where, x1 = [1, xT]T ∈ Rn+1, θ ∈ Rn+1Xp and Y ∈ Rn+1Xp
indicates the constant unknown output-layer and hidden-layer NN weight, σf : Rp → Rp+1 denotes
a bounded NN activation function, θ : Rn → Rnis the function reconstruction error, p ∈ N denotes
the number of NN neurons. Using the universal functionional approximation property of single
layer NNs, given a constant matrix Y such that the rows of σf(YTx1) form a proper basis, there
exist constant ideal weights θ and known constants θ ∈θ, cθ ∈ R such that ∣∣θ∣∣ < θ < ∞, SupxeC
lkθ(x)|| < eθ, SupxeC ∣∣Vχ∈θ(x)|| < <∈θ where, ||.|| denotes the Euclidean norm for vectors and the
Frobenius norm for matrix (Lewis et al., 1998).
Taking into consideration an estimate θ ∈ Rp+1Xn of the weight matrix θ , the function f can be
approximated by the function f : R2n X Rp+1Xn → Rn which is defined as /(Z, θ) = θτσθ(Z),
where σθ : R2n → Rp+1 can be defined as σθ(Z) = σf (YT[1, eτ + xτ]τ). An estimator for online
identification of the drift dynamics is developed
X = θτ σθ (Z) + g(x)u + kx
(9)
5
Under review as a conference paper at ICLR 2021
where, X = X - X and k e R Risa positive constant learning gain.
Assumption 4: A history stack containing recorded state-action pairs {xj , uj }jM=1 along with
numerically computed state derivatives {Xj}，] that satisfies λmin (PM=I σfjσfjj = σθ >
0,	∣∣Xj	-	Xj k	< d, ∀j	is available a priori, where σfj，σf	(YT	[1, XT]T), d ∈ R is a known
positive constant, Xj = f (Xj) + g (Xj) Uj and λmin(∙) denotes the minimum eigenvalue.
EI	∙	< .	. ∙	. A	1	.	1	♦	,1 Cll ♦	Z ■ I I	1	1	.	1
The weight estimates θ are updated using the following CL based update law:
M
θ = Γθσf (YtXi) XT + kθΓθ ^X σfj (Xj — gj-uj
j=1
(10)
where kθ ∈ R is a constant positive CL gain, and Γθ ∈ Rp+1×p+1 is a constant, diagonal, and
positive definite adaptation gain matrix. Using the identifier, the BE in (3) can be approximated as
δ (ζ, θ, WC Wə = Q(Z) + μτ (ζ, Wə R^ (ζ, Wa)
+VζV (ζ, Wj @(ζ, θ) + Fι(ζ) + G(Z)μ (ζ, Wə)
The BE is now approximated as
δ (ζ, θ, WC WO) = Q(Z) + μτ (ζ, WO) Rμ (ζ, Wə
+VζV (ζ, Wa) (Fθ(z, θ) + Fi (Z) + G(Z)μ (ζ, WO))
(11)
(12)
T .∙	,,C' a ∕* Zh	θτσθ(Z) — g(X)g+ (Xd) θτσθ ( θn×1 )	, 口,八
In equation (12), Fθ(Z, θ) =	θ	d θ	Xd	, and F1(Z)
0n×1
(-hd + g (e + Xd) g+ (Xd) hd)τ , hdτ .
2.2.2	Value Function Approximation
As V * and μ* are functions of the state Z, the optimization problem as defined in Section 2.2 is
quite an intractable one, so the optimal value function is now represented as C ⊂ R2n using a NN as
V* (Z) = Wτσ(Z)+(Z), where W ∈ RL denotes a vector of unknown NN weights, σ : R2n → RL
indicates a bounded NN activation function, : R2n → R defines the function reconstruction error,
and L ∈ N denotes the number of NN neurons. Considering the universal function approximation
property of single layer NNs, for any compact set C ⊂ R2n, there_exist constant ideal weights W
and known positive constants W, e, and e0 ∈ R such that ∣W∣∣ ≤ W < ∞ supζ∈c IIE(Z)Il ≤ e, and
supζ∈c ∣∣Vζe(Z)k ≤ e0 (Lewis etal.,1998).
A NN representation of the optimal policy is obtained as
μ*(Z) = - 2 RTGT (Z)(Vζστ (Z )W + Vζ eτ (Z))
(13)
Taking the estimates Wc and WO for the ideal weights W, the optimal value function and the optimal
policy are approximated as V(Z, WC) = WTσ(Z), μ(Z, Wə = -ɪRTGT(Z)VZστ(Z)Wɑ.
The optimal control problem is therefore recast as to find a set of weights Wc and WO online to
minimize the error E^ ( Wc, Wa) = supζ∈χ ∣δ ( Z, θ, Wc, Wa JI fora given θ, while simultaneously
improving θ using the CL-based update law and ensuring stability of the system using the control
law
U = μ (Z, Wa) + Ud(Z, θ)	(14)
where, Ud(Z,θ) =	g+	(hd	-	θτσjd)	,	and σθd	=	σ0	([	0ι×n	XT ]T).	σj ([ 0ι×n	XT	]T).
The error between Ud and Ud is included in the stability analysis based on the fact that the error
trajectories generated by the system e = f (x) + g(X)U - Xd under the controller in (14) are identical
to the error trajectories generated by the system Z = F(Z) + G(Z)μ under the control law μ =
μ(Z, Wɑ) + g+θτσθd + g+6θd, where ^d，的(xd).
6
Under review as a conference paper at ICLR 2021
2.2.3	Experience S imulation
The simulation of experience is implemented by minimizing a squared sum of BEs over finitely
many points in the state space domain as the calculation of the extremum (supremum) in EJ^ is
not tractable. The details of the analysis has been explained in Appendix A.2 which facilitates the
aforementioned approximation.
2.2.4	Stability and Robustness Analysis
To perform the stability analysis, we take the non-autonomous form of the value function
(KamaIaPUrkar et al., 2015) defined by Vt* : Rn X R → R which is defined as Vt* (e,t)=
V * QeT ,xT (t)]T) ,∀e ∈ Rn,t ∈ R, is positive definite and decrescent. Now, Vt*(0,t) = 0, ∀t ∈
R and there exist class K functions V : R → R and v : R → R such that V(Ilek) ≤ Vt*(e,t) ≤
v(kek), for all e ∈ Rn and for all t ∈ R. We take an augemented state given as Z ∈ R2n+2L+n(p+1)
is defined as
Z = eT, WT, WT, XT, (vec(θ))TiT	(15)
and a candidate Lyapunov function is defined as
VL(Z,t) = Vt*(e,t) + 2 WTΓ-1WC + 1WTWa2XTX + 2 tr (θTΓ-町	(16)
where, vec (∙) denotes the vectorization operator. From the weight update in Appendix A.2 we
get positive constants γ,γ ∈ R such that Y ≤ IlrT (t)∣∣ ≤ γ, ∀t ∈ R. Taking the bounds on Γ
and Vt* and the fact that tr (θTΓ-1θ) = (vec(θ))T (Γ-1 0 Ip+ι) (vec(石))the candidate Lyapunov
function be bounded as
Vι(kZk) ≤ VL(Z,t) ≤ Vl(IIZk)	(17)
for all Z ∈ R2n+2L+n(p+1) and for all t ∈ R, where vl : R → R and V : R → R are class K
functions. Now, Using (1) and the fact that Vt*(e(t),t) = V* (Z(t)), ∀t ∈ R, the time-derivative of
the candidate Lyapunov function is given by
VL = VZ V * (F + Gμ*) - WT Γ-1WC - 1WT Γ-1ΓγTWC	(]8)
-WTWa + % + VZV*Gμ - VZV*Gμ*
Under sufficient gain conditions (Kamalapurkar et al., 2014), using (9), (10)-(13), and the update
laws given by Wc, Γ and Wa the time-derivative of the candidate Lyapunov function can be bounded
as
VL ≤ -vl(kZk),∀kZk ≥ v-1(∣),∀Z ∈ X	(19)
where ι is a positive constant, and χ ⊂ R2n+2L+n(p+1) is a compact set. Considering (13) and (15),
the theorem 4.18 in (Khalil., 2002) can be used to establish that every trajectory Z(t) satisfying
∣∣Z (to)k ≤ Vl-1 (Vl(P)), where P is a positive constant, is bounded for all t ∈ R and satisfies
limsupt→∞ ∣∣Z(t)k ≤ 竺T (Vr (V-I(I))). This aforementioned analysis addresses the stability
issue of the closed loop system.
The robustness criterion requires the algorithm to satisfy the following inequality (Gao et al., 2014)
in the presence of external disturbances with a pre-specified performance index γ known as the
H-infinity (H∞ ) performance index, given by
Z t ky(t)k
0
2 dt < γ2
Z t kw(t)k
0
dt
(20)
where, y(t) is the output of the system, w(t) is the factor that accounts for the modeling errors,
parameter uncertainties and external disturbances and γ is the ratio of the output energy to the
disturbance in the system.
Using (1) and the fact that Vt*(e(t),t) = V*(Z(t)),∀t ∈ R, the time-derivative of the candidate
Lyapunov function is given by
VL = Vζ V * (F + Gμ*) - WT Γ-1WC - 1WT Γ-1ΓγTWC
~ E 八.
-WTWa + V0 + VZV*Gμ - VZV*Gμ*
(21)
7
Under review as a conference paper at ICLR 2021
Gao et al. (2014) has shown if (22) and (23) is satisfied, then it can written that
0 < VL(T) = Z VL(t) ≤- Z yτ(t)y(t)dt + γ2 Z WT(t)w(t)dt
(22)
Thus, the performance inequality constraint given by R0t ky(t)k2dt < γ2 R0t kw(t)kdt in terms of γ
is satisfied.
3 Simulation Results and Discussion
Here, we are going to present the simulation results to demonstrate the performance of the proposed
method with the fuel management system of the hybrid electric vehicle. The proposed concurrent
learning based RL optimization architecture has been shown in the Figure 1.
Figure 1: Reinforcement Learning-based Optimization Architecture
In this architecture, the simulated state-action-derivative triplets performs the action of concurrent
learning to approximate the value function weight estimates to minimize the bell error (BE). The
history stack is used to store the evaluation of the bell error which is carried out by a dynamic
system identifier as a gained experience so that it can iteratively used to reduce the computational
burden.
A simple two dimensional model of the fuel management system is being considered for the simula-
tion purpose to provide a genralized solution that can be extended in other cases of high dimensional
system.
We consider a two dimensional non-linear model given by
f
x1	x2	0
0	0	x1
a
0	] b
x2(l — (cos(2xι +2)2)) * c
d
0
cos(2x1 + 2)
, w(t) = sin(t)
(23)
where a, b, c, d ∈ R are unknown positive parameters whose values are selected as a = -l, b =
l, c = -0.5, d = -0.5, x1 and x2 are the two states of the hybrid electric vehicle given by the
charge present in the battery and the amount of fuel in the car respectively and w(t) = sin(t)
is a sinusoidal disturbance that is used to model the external disturbance function. The control
objective is to minimize the cost function given by J(Z, μ) =	r(Z(τ), μ(τ))dτ where, the local
0
cost r : R2nXRm → R is given as r(Z,τ) = Q(e) + μτRμ, R e RmXm is a positive definite
symmetric matrix and Q : Rn → R is a continous positive definite function, while following the
desired trajectory X We chhose Q = I2χ2 and R = 1. The optimal value function and optimal
control for the system (15) are V*(x) = ∣x2 + ∣x2 and u*(x) = -cos(2(xι) + 2)x2. The basis
8
Under review as a conference paper at ICLR 2021
function σ : R2 → R3 for value function approximation is σ = [x21, x21x22, x22]. The ideal weights
are W = [0.5, 0, 1]. The initial value of the policy and the value function weight estimates are Wc
= Wa = [1,1, 1]t , least square gain is Γ(0) = I00i3χ3 and that of the system states are χ(0)=
[-1, -1]t. The state estimates X and θ are initialized to 0 and 1 respectively while the history stack
for the CL is updated online. Here, Figure 2 and Figure 3 shows the state trajectories obtained by the
Figure 3: State Trajectories
Figure 2: State Trajectories
Wal
Wa2
Wa3
4	6	8	10
Time(S)
Figure 4: Control Input

Figure 5: Value Function Figure 6: Policy Function Figure 7: Performance Index
traditional RL methods and that obtained by the CL-based RL optimization technique respectively
in the presence of disturbances. It can be stated that settling time of trajectories obtained by the
proposed method is significantly less (almost 40 percent) as compared with that of the conventional
RL strategies thus justifying the uniqueness of the method and causing a saving in fuel consumption
by about 40-45 percent. Figure 4 shows the corresponding control inputs whereas Figure 5 and
Figure 6 indicates the convergence of the NN weight functions to their optimal values. The H∞
performance index in Figure 7 shows a value of 0.3 for the CL-based RL method in comparison
to 0.45 for the traditional RL-based control design which clearly establishes the robustness of our
proposed design. 4
4 Conclusion
In this paper, we have proposed a robust concurrent learning based deep Rl optimization strategy for
hybrid electric vehicles. The uniqueness of this method lies in use of a concurrent learning based
RL optimization strategy that reduces the computational complexity significanty in comparison to
the traditional RL approaches used for the fuel management system mentioned in the literature.
Also, the use of the the H-infinity (H∞ ) performance index in case of RL optimization for the first
time takes care of the robustness problems that most the fuel optimization nethods suffer from. The
simulation results validate the efficacy of the method over the conventional PID, MPC as well as
traditional RL based optimization techniques. Future work will generalize the approach for large-
scale partially observed uncertain systems and it will also incorporate the movement of neighbouring
RL agents.
9
Under review as a conference paper at ICLR 2021
References
R. Akrour, A. Abdolmaleki, H. Abdulsamad, and G. Neumann. Model Free Trajectory Optimization
for Reinforcement Learning. In Proceedings of the International Conference on Machine Learning
(ICML), 2016.
A. Barto, R. Sutton, and C. Anderson. Neuron-like adaptive elements that can solve difficult learning
control problems. IEEE Transaction on Systems, Man, and Cybernetics, 13: 834-846, 1983.
R. Bellman. The theory of dynamic programming. DTIC Document, Technical Representations.
1954.
D. Bertsekas and J. Tsitsiklis. Neuro-Dynamic Programming. Athena Scientific, 1996.
D. Bertsekas. Dynamic Programming and Optimal Control. Athena Scientific, 2007.
S. Bhasin, R. Kamalapurkar, M. Johnson, K. Vamvoudakis, F. L. Lewis, and W. Dixon. A novel
actor-critic-identifier architecture for approximate optimal control of uncertain nonlinear systems.
Automatica, 49: 89-92, 2013.
R. P. Bithmead, V. Wertz, and M. Gerers. Adaptive Optimal Control: The Thinking Man’s G.P.C.
Prentice Hall Professional Technical Reference, 1991.
A. Bryson and H. Y.-C. Applied Optimal Control: Optimization, Estimation and Control. Washing-
ton: Hemisphere Publication Corporation, 1975.
G. V. Chowdhary and E. N. Johnson. Theory and flight-test validation of a concurrent-learning
adaptive controller. Journal of Guidance Control and Dynamics,34:, 592-607, 2011.
G. ChoWdhary, T. Yucelen, M. Muhlegg, and E. N. Johnson. Concurrent learning adaptive control
of linear systems with exponentially convergent bounds. International Journal of Adaptive Control
and Signal Processing, 27: 280-301, 2013
P. Garcia, J. P. Torreglosa, L. M. Fernandez, and F. Jurado. Viability study of a FC-batterySC
tramWay controlled by equivalent consumption minimization strategy. International Journal of Hy-
drogen Energy, 37: 9368-9382, 2012.
A. Gosavi. Simulation-based optimization: Parametric optimization techniques and reinforcement
learning. Kluwer Academic Publishers, NorWell , MA, USA, 2003.
J. Han, and Y. Park. A novel updating method of equivalent factor in ECMS for prolonging the
lifetime of battery in fuel cell hybrid electric vehicle. In IFAC Proceedings, volume 45(4), pp.
227-232, 2012.
J. Han, J.F. Charpentier, and T. Tang. An Energy Management System of a Fuel Cell/Battery Hybrid
Boat. Energies, 7: 2799-2820, 2014.
J. Han, Y. Park, and D. Kum. Optimal adaptation of equivalent factor of equivalent consumption
minimization strategy for fuel cell hybrid electric vehicles under active state inequality constraints.
Journal of Power Sources, 267: 491-502, 2014.
D. Jacobsen and D. Mayne. Differential Dynamic Programming. Elsevier, 1970.
R. Kamalapurkar, L. AndreWs, P. Walters, and W. E. Dixon. Model-based reinforcement learning for
infinite-horizon approximate optimal tracking. In Proceedings of the IEEE Conference on Decision
and Control (CDC), pp. 5083-5088, 2014.
R.	Kamalapurkar, H. Dinh, S. Bhasin, and W. E. Dixon. Approximate optimal trajectory tracking
for continuous-time nonlinear systems. Automatica, 51: 40-48, 2015.
S.	G. Khan et al. Reinforcement learning and optimal adaptive control: An overvieW and implemen-
tation examples. Annual Reviews in Control, 36: 42-59, 2012.
M.J. Kim and H. Peng. PoWer management and design optimization of fuel cell/battery hybrid
vehicles. Journal of Power Sources, 165: 819-832, 2007.
D. Kirk. Optimal Control Theory: An Introduction. Mineola, NY, Dover, 2004.
10
Under review as a conference paper at ICLR 2021
V. Konda and J. Tsitsiklis. On actor-critic algorithms. SIAM Journal on Control and Optimization,
42: 1143-1166, 2004.
S. Levine and P. Abbeel. Learning Neural Network Policies with Guided Search under Unknown
Dynamics. In Advances in Neural Information Processing Systems (NeurIPS), 2014.
F. L. Lewis, S. Jagannathan, and A. Yesildirak. Neural network control of robot manipulators and
nonlinear systems. CRC Press, Philadelphia, PA, 1998.
F. L. Lewis, D. Vrabie, and V. L. Syrmos. Optimal Control, 3rd edition. Wiley, NJ, 2012.
H. Li, A. Ravey, A. N’Diaye, and A. Djerdir. A Review of Energy Management Strategy for Fuel
Cell Hybrid Electric Vehicle. In IEEE Vehicle Power and Propulsion Conference (VPPC), pp. 1-6,
2017.
W.S. Lin and C.H. Zheng. Energy management of a fuel cell/ultracapacitor hybrid power system
using an adaptive optimal control method. Journal of Power Sources, 196(6): 3280- 3289, 2011.
P. Mehta and S. Meyn, “Q-learning and pontryagin’s minimum principle. In Proceedings of IEEE
Conference on Decision and Control, pp. 3598-3605, 2009.
D. Mitrovic, S. Klanke, and S. Vijayakumar. Adaptive Optimal Feedback Control with Learned
Internal Dynamics Models. Springer, pp. 65-84, Berlin, 2010.
H. Modares and F. L. Lewis. Optimal tracking control of nonlinear partially-unknown constrained-
input systems using integral reinforcement learning. Automatica, 50(7): pp. 1780-1792, 2014.
S. J. Moura, D. S. Callaway, H. K. Fathy, and J. L. Stein. Tradeoffs between battery energy capacity
and stochastic optimal power management in plug-in hybrid electric vehicles. Journal of Power
Sources, 1959(9): pp. 2979-2988, 2010.
S. N. Motapon, L. Dessaint, and K. Al-Haddad. A Comparative Study of Energy Management
Schemes for a Fuel-Cell Hybrid Emergency Power System of More-Electric Aircraft. IEEE Trans-
actions on Industrial Electronics, 61: pp. 1320-1334, 2014.
G. Paganelli, S. Delprat, T. M. Guerra, J. Rimaux, and J. J. Santin. Equivalent consumption mini-
mization strategy for parallel hybrid powertrains. In IEEE 55th Vehicular Technology Conference,
VTC Spring 2002 (Cat. No.02CH37367), pp. 2076-2081, 2002.
F. SegUra and J. M. Andujar. Power management based on sliding control applied to fuel cell Sys-
tems: A further step towards the hybrid control concept. Applied Energy, 99: 213-225, 2012.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge,
MA, USA, 1998.
E. Theoddorou, Y. Tassa, and E. Todorov. Stochastic Differential Dynamic Programming. In Pro-
ceedings of American Control Conference, 2010.
E. Todorov and Y. Tassa. Iterative Local Dynamic Programming. In Proceedings of the IEEE
International Symposium on ADP and RL, 2009.
J. P. Torreglosa, P. Garcia, L. M. Fernandez, and F. Jurado. Predictive Control for the Energy Man-
agement of a Fuel-Cell-Battery-Supercapacitor Tramway. IEEE Transactions on Industrial Infor-
matics, 10(1): 276-285, 2014.
D. Vrabie. Online adaptive optimal control for continuous-time systems. Ph.D. dissertation, Uni-
versity of Texas at Arlington, 2010.
Dan Yu, Mohammadhussein Rafieisakhaei, and Suman Chakravorty. Stochastic Feedback Control
of Systems with Unknown Nonlinear Dynamics. In IEEE Conference on Decision and Control,
2017.
M. K. Zadeh. Stability Analysis Methods and Tools for Power Electronics-Based DC Distribution
Systems: Applicable to On-Board Electric Power Systems and Smart Microgrids. NTNU, 2016.
M. K. Zadeh, L. Saublet, R. Gavagsaz-Ghoachani, B. Nahid Mobarakeh, S. Pierfederici, and M.
Molinas. Energy management and stabilization of a hybrid DC microgrid for transportation appli-
11
Under review as a conference paper at ICLR 2021
cations. In IEEE Applied Power Electronics Conference and Exposition (APEC), pp. 3397-3402,
2016.
X. Zhang, C. C. Mi, A. Masrur, and D. Daniszewski. Wavelet transform-based power management
of hybrid vehicles with multiple on-board energy sources including fuel cell, battery and ultracapac-
itor. Journal of Power Sources, 185(2): 1533-1543, 2008.
C. Zheng, S. W. Cha, Y. Park, W. S. Lim, and G. Xu. PMP-based power management strategy of
fuel cell hybrid vehicles considering multi-objective optimization. International Journal Precision
Engineering and Manufacturing, 14(5): 845-853, 2013.
C. H. Zheng, G. Q. Xu, Y. I. Park, W. S. Lim, and S. W. Cha. Prolonging fuel cell stack lifetime
based on Pontryagin’s Minimum Principle in fuel cell hybrid vehicles and its economic influence
evaluation. Journal Power Sources, 248: 533-544, 2014.
X. Zhong, H. He, H. Zhang, and Z. Wang. Optimal Control for Unknown Diiscrete-Time Nonlin-
ear Markov Jump Systems Using Adaptive Dynamic Programming. IEEE Transactions on Neural
networks and learning systems, 25(12): pp. 2141-2155, 2014.
A Appendix
A.1 The Gradient Descent Algorithm
The Gradient Descent Algorithm has been explained as follows:
Algorithm; Gradient Descent
Input : Design Parameters U (0) = ut0 , α, h R
Output: Optimal control sequence {Ut}
1.	n — 0, Vu Jd (x0, U(0)) - E
2.	while VU Jd (x0, U(n)) ≥ C do
3.	Evaluate the cost function with control U(n)
4.	Perturb each control variable u(n) by h, i = 0, ∙∙∙ ,t, and calculate the gradient vector
VU Jd (x0, U(n)) using (7) and (8)
5.	Update the control policy: U(n+1) - U(n) 一 αVu Jd (x0, U(n))
6.	n - n + 1
7.	end
8.	{Ut} 一 U(n)
A.2 Experience S imulation
Assumptioη5: (Kamalapurkaretal., 2014) There exists a finite set of points {Zi ∈ C | i = 1, ∙∙∙ , N}
and a constant C ∈ R such that 0 < c = N ɑnft∈R≥^ Qm® {pN=ι ωipω^})) where Pi =1 +
νωTΓωi ∈ R, and cωi = VZσ (Zi)(F© (金,θ) + F (Zi) + G(Zi) μ Q, Wa)).
Using Assumption 5, simulation of experience is implemented by the weight update laws given by
N
Wc = -ηciΓωδt-等γ X 竺δti	(24)
ρ N i=1 ρi
γ = (βr — "cir-2^-γ) 1{kΓ∣≤Γ}, kr (t0)k ≤ γ,	(25)
Wa = fl (Wa 一 Wc) — 〃a2 W a + (	+ X	NWaωT ) Wc	(26)
4ρ	i=1	4Nρi
12
Under review as a conference paper at ICLR 2021
where, ω = RZσ(ζ) (Fe(ζ, θ) + Fι(ζ) + G(ζ)μ & Wɑ)) , Γ ∈ RL×L is the least-squares gain
matrix, Γ ∈ R denotes a positive saturation constant, β ∈ R indicates a constant forgetting factor,
ηci,ηc2,ηai, ηa2 ∈ R defines constant positive adaptation gains, 1{∙} denotes the indicator function
of the set {∙},Gσ = RZσ(Z)G(ζ)R-1GT(Z)▽《στ(Z), and P = 1 + νωTΓω, where V ∈ R is
a positive normalization constant. In the above weight update laws, for any function ξ(ζ, ∙), the
notation ξi, is defined as ξi = ξ (Zi, ∙), and the instantaneous BEs δt and δti are given as δt =
ʌ / ʌ ʌ ʌʌ ʌ ʌ / ʌ ʌ ʌʌ
δ (ζ, WC, Wa, θ) and 配=δ (金，W2, Wa,θ).
13