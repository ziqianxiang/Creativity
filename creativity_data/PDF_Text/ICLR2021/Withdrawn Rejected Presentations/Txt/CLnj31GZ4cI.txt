Under review as a conference paper at ICLR 2021
K-Adapter:	Infusing Knowledge into Pre-
Trained Models with Adapters
Anonymous authors
Paper under double-blind review
Abstract
We study the problem of injecting knowledge into large pre-trained models like
BERT and RoBERTa. Existing methods typically update the original parame-
ters of pre-trained models when injecting knowledge. However, when multiple
kinds of knowledge are injected, they may suffer from catastrophic forgetting. To
address this, we propose K-Adapter, which retains the original parameters of
the pre-trained model fixed and supports continual knowledge infusion. Taking
RoBERTa as the pre-trained model, K-Adapter has a neural adapter for each
kind of infused knowledge, like a plug-in connected to RoBERTa. There is no
information flow between different adapters, thus different adapters are efficiently
trained in a distributed way. We inject two kinds of knowledge, including fac-
tual knowledge obtained from automatically aligned text-triplets on Wikipedia and
Wikidata, and linguistic knowledge obtained from dependency parsing. Results on
three knowledge-driven tasks (total six datasets) including relation classification,
entity typing and question answering demonstrate that each adapter improves the
performance, and the combination of both adapters brings further improvements.
Probing experiments further indicate that K-Adapter captures richer factual and
commonsense knowledge than RoBERTa.
1	Introduction
Language representation models, which are pre-trained on large-scale text corpus through unsu-
pervised objectives like (masked) language modeling, such as BERT (Devlin et al., 2019), GPT
(Radford et al., 2018; 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019) and T5 (Raffel
et al., 2019), have established state-of-the-art performances on various NLP downstream tasks.
Despite the huge success of these pre-trained models in empirical studies, recent studies suggest that
models learned in such an unsupervised manner struggle to capture rich knowledge. For example,
Poerner et al. (2019) suggest that although language models do well in reasoning about the surface
form of entity names, they fail in capturing rich factual knowledge. IKassner & Schutze (2019)
observe that BERT mostly did not learn the meaning of negation (e.g. “not”). These observations
motivate us to study the injection of knowledge into pre-trained models like BERT and RoBERTa.
Recently, some efforts have been made to exploit injecting knowledge into pre-trained language
models (Zhang et al., 2019; Lauscher et al., 2019; Levine et al., 2019; Peters et al., 2019; He et al.,
2019; Xiong et al., 2020). Most previous works (as shown in Table 1) augment the standard language
modeling objective with knowledge-driven objectives and update model parameters in a multi-task
learning manner. Although these methods, with updated pre-trained models, obtain better perfor-
mance on downstream tasks, they fail at continual learning (Kirkpatrick et al., 2017). Model param-
eters need to be retrained when new kinds of knowledge are injected, which may result in the catas-
trophic forgetting of previously injected knowledge. Meanwhile, the resulting pre-trained models
produce entangled representations, which makes it hard to investigate the effect of each knowledge
when multiple kinds of knowledge are injected.
In this paper, we propose K-Adapter, a flexible and simple approach that infuses knowledge
into large pre-trained models. K-Adapter has attractive properties including supporting continual
knowledge infusion and producing disentangled representations. It leaves the original representation
ofa pre-trained model unchanged and exports different representations for different types of infused
knowledge. This is achieved by the integration of compact neural models, dubbed adapters here.
1
Under review as a conference paper at ICLR 2021
Table 1: Comparison between our approach (K-Adapter) and previous works on injecting knowl-
edge into BERT.
Model		Knowledge Source	Objective	BERT fixed in training?	Continual knowl- edge infusion?
ERNIE (Zhang et al., 2019)		Wikipedia, WikiData	entity linking	N	N
LIBERT (Lauscher		WordNet	synonym word prediction, hyponym-hypernym prediction	from scratch	^N
et al., 2019)					
SenseBERT (Levine		WordNet	Word-SUPerSenSe prediction	from scratch	"N
et al., 2019)					
KnowBERT	(Peters	Wordnet, Wikipedia, CrossWikis	entity linking , hypernym link- ing		"N	"N
et al., 2019)					
WKLM (Xiong et al., 2020)		WikiPedia, WikiData	replaced entity detection	^N	^N
BERT-MK (He et al., 2019)		Unified Medical Lan- guage System	discriminate between real and fake facts	^N	^N
K-Adapter (this work)		Wikipedia, Wikidata, dependency parser	predication prediction, depen- dency relation prediction	^Y	^Y
Adapters are knowledge-specific models plugged outside of a pre-trained model, whose inputs are
the output hidden-states of intermediate layers of the pre-trained model. We take RoBERTa (Liu
et al., 2019) as the base pre-trained model and integrate two types of knowledge, including factual
knowledge obtained by aligned Wikipedia text to Wikidata triplets, linguistic knowledge obtained
by applying off-the-shell dependency parser to web texts. In the pre-training phase, we train two
adapters independently on relation classification task and dependency relation prediction task re-
spectively, while keeping the original parameters of RoBERTa frozen. Since adapters have much
less trainable parameters compared with RoBERTa, the training process is memory efficient.
We conduct extensive experiments on six benchmark datasets across three knowledge-driven tasks,
i.e., relation classification, entity typing and question answering. Experiments show that K-
Adapter consistently performs better than RoBERTa, and achieves state-of-the-art performance on
five datasets, and comparable performance compared with CosmosQA SOTA. Probing experiments
on LAMA (Poerner et al., 2019) and LAMA-UHN (Petroni et al., 2019), further demonstrates that
K-Adapter captures richer factual and commonsense knowledge than RoBERTa.
The contributions of this paper are summarized as follows:
•	We propose K-Adapter, a flexible approach that supports continual knowledge infusion
into large pre-trained models (e.g. RoBERTa in this work).
•	We infuse factual knowledge and linguistic knowledge, and show that adapters for both
kinds of knowledge work well on downstream tasks.
•	K-Adapter achieves superior performance by fine-tuning parameters on three down-
stream tasks, and captures richer factual and commonsense knowledge than RoBERTa on
probing experiments.
2	Related Work
Our work relates to the area of injecting knowledge into pre-trained models. As stated in Table 1,
previous works mainly differ from the knowledge sources and the objective used for training.
ERNIE (Zhang et al., 2019) injects a knowledge graph into BERT. They align entities from
Wikipedia sentences to fact triples in WikiData, and discard sentences with less than three enti-
ties. In the training process, the input includes sentences and linked facts, and the knowledge-aware
learning objective is to predict the correct token-entity alignment. Entity embeddings are trained on
fact triples from WikiData via TransE (Bordes et al., 2013). LIBERT (Lauscher et al., 2019) injects
pairs of words with synonym and hyponym-hypernym relations in WordNet. The model takes a pair
of words separated by a special token as the input, and is optimized by a binary classification prob-
lem, which predicts whether the input holds a particular relation or not. SenseBERT (Levine et al.,
2019) considers word-supersense knowledge. It inject knowledge by predicting the supersense of
the masked word in the input, where the candidates are nouns and verbs and the ground truth comes
2
Under review as a conference paper at ICLR 2021
Lingustic Knowledge
Language
Model
Pre-train
taski
TRM
Language
Model
TRMMk
Input Tokens
---A---- Adapteri
IKIAK ∣-
Pre-traιn
task2
Pre-train
task2
Pre-train
taski
concatenate
(a) Multi-Task Learning
Input Tokens
(b) K-Adapter
Figure 1: (a) Pre-trained language models inject multiple kinds of knowledge with multi-task learn-
ing. Model parameters need to be retrained when injecting new kinds of knowledge, which may
result in the catastrophic forgetting (b) Our K-ADAPTER injects multiple kinds of knowledge by
training adapters independently on different pre-train tasks, which supports continual knowledge
infusion. When we inject new kinds of knowledge, the existing knowledge-specific adapters will
not be affected. KIA represents the adapter layer and TRM represents the transformer layer, both of
which are shown in Figure 2.
AdaPter2
concatenate
from WordNet. KnowBERT (Peters et al., 2019) incorporates knowledge bases into BERT using
Knowledge attention and recontextualization, where the knowledge comes from synset-synset and
lemma-lemma relationships in WordNet, and entity linking information in Wikipedia. If entity link-
ing supervision is available, the model is learned with an additional knowledge-aware log-likelihood
or max-margin objective. WKLM (Xiong et al., 2020) replaces entity mentions in the original doc-
ument with names of other entities of the same type. The model is trained to distinguish the correct
entity mention from randomly chosen ones. BERT-MK (He et al., 2019) integrates fact triples from
knowledge graph. For each entity, it sample incoming and outcoming instances from the neighbors
on the knowledge graph, and replaces head or tail entity to create negative instances. The model is
learned to discriminate between real and fake facts.
As shown in Table 1, our model (K-Adapter)
differs from previous studies in three aspects.
First, we consider both fact-related objec-
tive (i.e. predicate/relation prediction) and
linguistic-related objective (i.e. dependency re-
lation prediction). Second, the original param-
eter of BERT is clamped in the knowledge in-
fusion process. Third, our approach supports
continual learning, which means that the learn-
ing of different adapters are not entangled. This
flexibility enables us to efficiently inject differ-
ent types of knowledge independently, and in-
ject more types of knowledge without any loss
on the previously injected knowledge.
Figure 2: Structure of the adapter layer (left). The
adapter layer consists of two projection layers and
N=2 transformer layers, and a skip-connection
between two projection layers.
3	K-Adapter
As illustrated in Figure 1 (a), most of the previous works enhance pre-trained language models
by injecting knowledge and update model parameters through multi-task learning. Regardless of
these different versions of knowledge-injected methods with multi-task learning, common issues
not fully studied are catastrophic forgetting of previous knowledge. To address this, we present K-
Adapter as shown in Figure 1(b), where multiple kinds of knowledge are injected into different
compact neural models (i.e., adapters in this paper) individually instead of directly injecting knowl-
edge into pre-trained models. It keeps the original representation of a pre-trained model fixed and
3
Under review as a conference paper at ICLR 2021
supports continual knowledge infusion, i.e., injecting each kind of knowledge into the correspond-
ing knowledge-specific adapter and producing disentangled representation. Specifically, adapters
are knowledge-specific models (with few parameters) plugged outside of a pre-trained model. The
inputs of adapters are the output hidden-states of intermediate layers of the pre-trained model. Each
adapter is pre-trained independently on different tasks for injecting discriminative knowledge while
the original parameters of the pre-trained model are frozen. In this paper, we exploit RoBERTa (Liu
et al., 2019) as the pre-trained model, and mainly infuse factual knowledge and linguistic knowledge
with two kinds of adapters, i.e., factual adapter and linguistic adapter which are pre-trained on the
relation classification task and dependency relation prediction task respectively. In this section, we
first describe the structure of our adapter, and then present the process of pre-training knowledge-
specific adapters.
3.1	Adapter Structure
In this work, we present a different adapter structure as shown in Figure 2, which is referred to as
the knowledge-specific adapter. In contrast to Houlsby et al. (2019) add adapter layers into each
transformer layer, our adapter works as outside plug-ins. Each adapter model consists of K adapter
layers that contain N transformer (Vaswani et al., 2017) layers and two projection layers. A skip-
connection is applied across two projection layers. Specifically, for each adapter model, we plug
adapter layers among different transformer layers of the pre-trained model. We concatenate the
output hidden feature of the transformer layer in the pre-trained model and the output feature of the
former adapter layer, as the input feature of the current adapter layer. For each knowledge-specific
adapter, we concatenate the last hidden features of the pre-trained model and adapter as the final
output feature of this adapter model.
In the pre-training procedure, we train each knowledge-specific adapter on different pre-training
tasks individually. For various downstream tasks, K-Adapter can adopt the fine-tuning procedure
similar to RoBERTa and BERT. When only one knowledge-specific adapter is adopted, we can take
the final output feature of this adapter model as the input for task-specific layers of the downstream
task. When multiple knowledge-specific adapters are adopted, we concatenate the output features of
different adapter models as the input for task-specific layers of the downstream task.
3.2	Pre-training settings
We use RoBERTaLARGE (L=24, H=1024, A=16, 355M params) implementation by Huggingface1
as the pre-trained model in all our experiments. As for each adapter layer, we denote the number
of transformer layer as N , the hidden dimension of transformer layer as HA , the number of self-
attention heads as AA, the hidden dimension of down-projection and up-projection layers as Hd and
Hu. In detail, we have the following adapter size: N =2, HA = 768, AA = 12, Hu = 1024 and
Hd = 768. The RoBERTa layers where adapter layers plug in are {0,11,23}, and different adapter
layers do not share parameters. Thus the total parameters for each adapter model are about 42M,
which are much smaller than RoBERTaLARGE and make the training process memory efficient. It
should be noticed that RoBERTa is fixed during training and the parameters of adapters are trainable
and initialized randomly. Then we describe how to inject different knowledge into knowledge-
specific adapters as below.
3.3	Factual Adapter
Factual knowledge can be described as the basic information that is concerned with facts. In this
work, we acquire factual knowledge from the relationships among entities in natural language. We
extract a sub-dataset T-REx-rc from T-REx (ElSahar et al., 2018) which is a large scale alignment
dataset between Wikipedia abstracts and Wikidata triples. We discard all relations having less than
50 entity pairs, collecting 430 relations and 5.5M sentences. In order to inject factual knowledge,
we propose to pre-train a knowledge-specific adapter called facAdapter on the relation classification
task. This task requires a model to classify relation labels of given entity pairs based on context.
Specifically, the last hidden features of RoBERTa and facAdapter are concatenated as the input
representation, and the pooling layer is applied to the input representations of the given entities.
Then, we concatenate two entity representations to perform relation classification.
1https://github.com/huggingface/transformers
4
Under review as a conference paper at ICLR 2021
Table 2: Results on two entity typing datasets OpenEntity and FIGER.
Model	OpenEntity			FIGER		
	P	R	Mi-F1	AcC	Ma-F1	Mi-F1
NFGEC (Shimaoka et al., 2016)	68.80	53.30	60.10	55.60	75.15	71.73
BERT-base (Zhang et al., 2019)	76.37	70.96	73.56	52.04	75.16	71.63
ERNIE (Zhang et al., 2019)	78.42	72.90	75.56	57.19	75.61	73.39
KnowBERT (Peters et al., 2019)	78.60	73.70	76.10	-	-	-
KEPLER (Wang et al., 2019)	77.20	74.20	75.70	-	-	-
WKLM (Xiong et al., 2020)	-	-	-	60.21	81.99	77.00
RoBERTa	77.55	74.95	76.23	56.31	82.43	77.83
RoBERTa + multitask	77.96	76.00	76.97	59.86	84.45	78.84
K-Adapter (F+L)	78.99	76.27	77.61	61.81	84.87	80.54
K-Adapter (F)	79.30	75.84	77.53	59.50	84.52	80.42
K-Adapter (L)	80.01	74.00	76.89	61.10	83.61	79.18
K-Adapter (w/o knowledge)	74.47	74.91	76.17	56.93	82.56	77.90
3.4 Linguistic Adapter
Linguistic knowledge is implicitly contained in natural language texts, e.g., syntactic and seman-
tic information. In this work, we acquire linguistic knowledge from dependency relationships
among words in natural language text. We build a dataset consisting of 1M examples. In partic-
ular, we run the off-the-shell dependency parser from Stanford Parser2 on a part of Book Corpus
(Zhu et al., 2015). To inject linguistic knowledge, we pre-train another knowledge-specific adapter
called linAdapter on the task of dependency relation prediction. This task aims to predict the head
index of each token in the given sentence. We concatenate the last hidden features of RoBERTa and
linAdapter as the input representation, and then apply a linear layer to input representations of each
token to perform classification. More training details of facAdapter and linAdapter can be found in
the supplementary material.
4	Experiments
We evaluate our K-Adapter on three knowledge-driven downstream tasks, i.e., entity typing, ques-
tion answering and relation classification. Furthermore, we conduct probing experiments to exam-
ine the ability of models for learning factual knowledge. The notations of K-Adapter (F+L),
K-Adapter (F), and K-Adapter (L) denote our model which consists of both factual adapter and
linguistic adapter, only factual adapter and only linguistic adapter, respectively. The implementation
details, and statistics of datasets are in the supplementary material.
4.1	Entity Typing
We conduct experiments on fine-grained entity typing which aims to predict the types of a given
entity and its context. We evaluate our models on OpenEntity (Choi et al., 2018) and FIGER (Ling
et al., 2015) following the same split setting as Zhang et al. (2019). To fine-tune our models for
entity typing, we modify the input token sequence by adding the special token “@” before and after
a certain entity, then the first “@” special token representation is adopted to perform classification.
As for OpenEntity, we adopt micro F1 score as the final metric to represent the model performance.
As for FIGER, we adopt strict accuracy, loose macro, loose micro F1 scores (Ling & Weld, 2012)
for evaluation following the same evaluation criteria used in previous works.
Baselines NFGEC (Shimaoka et al., 2016) employs attentive recursive neural networks to com-
pose context representations. KEPLER (Wang et al., 2019) integrates factual knowledge with the
supervision of the knowledge embedding objective. RoBERTa+multitask is our RoBERTa model
pre-trained with multi-task learning (as shown in Figure 1(a)) for injecting multiple kinds of knowl-
edge on two pre-training tasks. K-ADAPTER (w/o knowledge) consists of a RoBERTa model and
an adapter without being injected knowledge. Other baseline models, such as BERT-base, ERNIE,
KnowBERT and WKLM are described in Section 2.
2http://nlp.stanford.edu/software/lex-parser.html
5
Under review as a conference paper at ICLR 2021
Table 3: Results on question answering datasets including: CosmosQA, SearchQA and Quasar-T.
Model	SearchQA		Quasar-T		CosmosQA
	EM	FI	EM	F1	Accuracy
BiDAF (Seo et al., 2016)	28.60	34.60	25.90	28.50	-
AQA (Buck et al., 2018)	40.50	47.40	-	-	-
R^3 (Wang et al., 2017a)	49.00	55.30	35.30	41.70	-
DSQA (Lin et al., 2018)	49.00	55.30	42.30	49.30	-
Evidence Agg. (Wang et al., 2018)	57.00	63.20	42.30	49.60	-
BERT (Xiong et al., 2020)	57.10	61.90	40.40	46.10	-
WKLM (Xiong et al., 2020)	58.70	63.30	43.70	49.90	-
WKLM + Ranking (Xiong et al., 2020)	61.70	66.70	45.80	52.20	-
BERT-FTRACE+SW AG (Huang et al., 2019)	-	-	-	-	68.70
RoBERTa	59.01	65.62	40.83	48.84	80.59
RoBERTa + multitask	59.92	66.67	44.62	51.17	81.19
K-Adapter (F+L)	61.96	67.31	46.32	53.00	81.83
K-Adapter (F)	61.85	67.17	46.20	52.86	80.93
K-Adapter (L)	61.15	66.82	45.66	52.39	80.76
Results and Discussion The results on OpenEntity and FIGER are shown in Table 2. K-ADAPTER
(F+L) achieves consistent improvements across these datasets. As for OpenEntity, our RoBERTa
achieve better results than other baseline models. K-Adapter (F+L) further achieves improve-
ment of 1.38% F1 over RoBERTa, which means factual knowledge and linguistic knowledge help
to predict the types more accurately. As for FIGER, it covers more entity types, and is more fine-
grained than OpenEntity. Compared with WKLM, K-Adapter (F+L) improves the macro F1 by
2.88%, micro F1 by 2.54% and accuracy by 1.60%. This demonstrates that K-Adapter (F+L)
benefits fine-grained entity typing. In addition, we further conduct several experiments on our ab-
lated model K-Adapter (w/o knowledge), to explore whether the performance gains came from
introducing knowledge or additional parameters. Results show that K-Adapter (F) significantly
outperforms K-Adapter (w/o knowledge). Moreover, itis worth noting that on OpenEntity dataset,
K-Adapter (w/o knowledge) even performs slightly worse than RoBERTa. These results demon-
strate that our model gains improvement from knowledge instead of more parameters. Thus, for
simplicity, we don’t discuss K-Adapter (w/o knowledge) in the following experiments.
4.2	Question Answering
We conduct experiments on two question answering (QA) tasks, i.e., commonsense QA and open-
domain QA. Commonsense QA aims to answer questions with commonsense. We adopt CosmosQA
(Huang et al., 2019) to evaluate our models. CosmosQA requires commonsense-based reading com-
prehension, formulated as multiple-choice questions. To fine-tune our models for CosmosQA, the
input token sequence is modified as “<SEP>context </SEP>question</SEP>answer</SEP>”,
then the representation of the first token is adopted to perform classification, and will get a score for
this answer. After getting four scores, the answer with the highest score will be selected. We report
accuracy scores obtained from the leaderboard.
Open-domain QA aims to answer questions using external resources such as collections of docu-
ments and webpages. We evaluate our modes on two public datasets, i.e., Quasar-T (Dhingra et al.,
2017) and SearchQA (Dunn et al., 2017). Specifically, we first retrieve paragraphs correspond-
ing to the question using the information retrieval system and then extract the answer from these
retrieved paragraphs through the reading comprehension technique. Following previous work(Lin
et al., 2018), we use the retrieved paragraphs provided by Wang et al. (2017b) for these two datasets.
To fine-tune our models for this task, the input token sequence is modified as “<SEP>question
</SEP>paragraph</SEP>”. We apply linear layers over the last hidden features of our model to
predict the start and end position of the answer span. We adopt two metrics including ExactMatch
(EM) and loose F1 (Ling & Weld, 2012) scores to evaluate our models.
Baselines BERT-FTRACE+SWAG (Huang et al., 2019) is the BERT model sequentially fine-
tuned on both RACE and SWAG datasets. BiDAF (Seo et al., 2016) adopts a bi-directional attention
network. AQA (Buck et al., 2018) proposes to re-write questions and aggregate the answers gen-
erated by the re-written questions. R^3 (Wang et al., 20l7a) is a reinforced model making use of a
6
Under review as a conference paper at ICLR 2021
ranker for selecting most confident paragraph. Evidence Agg. (Wang et al., 2018) proposes making
use of the aggregated evidence from across multiple paragraphs to better determine the answer with
re-rankers. BERT (Xiong et al., 2020) is the BERT re-implementation. WKLM (Xiong et al.,
2020) is described in Section 2, which is adopted as the reader model to read multiple paragraphs to
predict a single answer. WKLM + Ranking (Xiong et al., 2020) is a WKLM paragraph reader plus
with a BERT based paragraph ranker to assign each paragraph a relevance score.
Results and Discussion The results on CosmosQA are shown in Table 3. Compared with
BERT-FTRACE+SW AG, our RoBERTa significantly achieves 11.89% improvement of accuracy.
Compared to RoBERTa, K-Adapter (F+L) further improves the accuracy by 1.24%, which indi-
cates that K-Adapter can obtain better commonsense inference ability. Moreover, the performance
of ablated K-Adapter models, i.e., K-Adapter (F) and K-Adapter (L) are clearly better than
RoBERTa, but slightly lose compared with RoBERTa+multitask. It is notable that K-Adapter
(F+L) makes obvious improvement comparing with RoBERTa+multitask. This demonstrates that
the combination of multiple knowledge-specific adapters could achieve better performance.
The results for open-domain QA are shown in Table 3. K-Adapter models achieve better results
compared to other baselines. This indicates that K-Adapter models can make full use of the
infused knowledge and accordingly benefit understanding the retrieved paragraphs to answer the
question. Specifically, on SearchQA, K-Adapter (F+L) makes significant improvement of 4.01%
F1 scores, comparing with WKLM where the ranking scores are not used, and even has a slight
improvement as compared to WKLM+Ranking. It is worth noting that K-Adapter models do not
consider the confidence of each retrieved paragraph, while WKLM+Ranking utilizes ranking scores
from a BERT based ranker. On the Quasar-T dataset, K-Adapter (F+L) also outperforms WKLM
by 3.1% F1 score and slightly outperforms WKLM+Ranking.
4.3	Relation Classification
Relation classification aims to determine the correct relation between two entities in a given sen-
tence. We adopt a large-scale relation classification dataset TACRED (Zhang et al., 2017).To fine-
tune our models for this task, we modify the input token sequence by adding special token “@”
before and after the first entity, adding “#” before and after the second entity. Then the token repre-
sentations of the first special token “@” and “#” are concatenated to perform relation classification.
We adopt micro F1 score as the metric to represent the model performance as previous works.
Baselines C-GCN (Zhang et al., 2018) employs graph convolutional networks to model de-
pendency trees. BERT-large (Baldini Soares et al., 2019) is a baseline BERT-large model.
BERT+MTB (Baldini Soares et al., 2019) is a method of training relation representation with-
out supervision from a knowledge base by matching the blanks. Other baseline models, such as
ERNIE, KnowBERT, KEPLER and RoBERTa+multitask are described in Section 2 and 4.1.
Results and Discussion Table 4
shows the performances of differ- Table 4: Results on the relation classification dataset TA-
ent models on TACRED. The re- CRED.
sults indicate that K-Adapter mod-						
els significantly outperform all base- lines, which directly demonstrate our models can benefit relation classifica- tion. In particular, (1) K-Adapter	Model			P	R	F1
	C-GCN (Zhang et al., 2018)			69.90	63.30	66.40
	BERT-base (Zhang et al.,	2019)		67.23	64.81	66.00
	ERNIE (Zhang et al., 20	9)		69.97	66.08	67.97
models outperform RoBERTa, which	BERT-large (Baldini Soares et al., 2019) BERT+MTB (Baldini Soares et al., 2019)			-	-	70.10
proves the effectiveness of infus-				-	-	71.50
ing knowledge into pre-trained model	KnowBERT (Peters et al.,		2019)	71.60	71.40	71.50
with adapters. (2) K-Adapter mod-	KEPLER (Wang et al., 20		9)	70.43	73.02	71.70
els gain more improvement compared	RoBERTa			70.17	72.36	71.25
with RoBERTa+multitask. This di-	RoBERTa + multitask			70.18	73.11	71.62
rectly demonstrates injecting knowl-	K-ADAPTER (F+L)			70.14	74.04	72.04
edge individually in K-Adapter	K-ADAPTER (F)			69.39	74.59	71.89
way would help models make full use of knowledge.	K-ADAPTER (L)		—	68.85	75.37	71.96
7
Under review as a conference paper at ICLR 2021
4.4	Prob ing Experiments
Although K-Adapter models have shown superior performance on knowledge-driven downstream
tasks, it does not directly provide insights into whether our models infuse richer factual knowledge.
Thus we utilize a LAMA (LAnguage Model Analysis) probe (Petroni et al., 2019) to examine the
ability to memorize factual knowledge. Specifically, the LAMA probing task is under a zero-shot
setting, which requires the language model to answer cloze-style questions about relational facts
without fine-tuning, e.g., “Simon Bowman was born in [MASK]”. The model needs to predict a
distribution over a limited vocabulary to replace [MASK]. We report mean precision at one (P@1)
macro-averaged over relations.
Table 5: P@1 on LAMA and LAMA-UHN across Google-RE and T-REx corpora.
Corpus	Models					
	ELMo	ELMo5.5B	TransformerXL	BERT-large	RoBERTaLARGE	K-Apdater
LAMA-Google-RE	2.2	3.1	1.8	12.1	4.8	7.0
LAMA-UHN-Google-RE	2.3	2.7	1.3	6.5	2.5	3.7
LAMA-T-REx	0.2	0.3	19.5	33.9	27.1	29.1
LAMA-UHN-T-REx	0.2	0.2	12.6	26.2	20.1	23.0
Table 6: Examples of generation for RoBERTaLARGE and K-Adapter. The last column reports
the top ranked predicted tokens. Correct predictions are in bold.
Query	Answer	Model	Generation
The native language of Mammootty is [MASK].	Malayalam	RoBERTa K-Adapter	English, Tamil, Hindi, Sanskrit, Arabic, Chinese Malayalam, Tamil, Hindi, Mandarin, English
			
Ravens can [MASK].	fly	RoBERTa	win, play, score, lose, run, drink, fly, roll, wait
		K-Adapter	fly, swim, sing, shoot, kill, go, fish, drink, die
Sometimes virus causes [MASK].	infection	RoBERTa	cancer, death, illness, blindness, paralysis
—		K-Adapter	cancer, illness, death, infection, disease
Sunshine Coast, British Columbia	Canada	RoBERTa	Florida, California, Texas, Hawaii, Mexico
is located in [MASK].		K-Adapter	Canada, Vancouver, Victoria, BC, Australia
iPod Touch is produced by	Apple	RoBERTa	Apple, Samsung, Qualcomm, LG, Microsoft
[MASK].		K-Adapter	Apple, HTC, Samsung, Motorola, Intel
			
Settings We consider several language models including: ELMo (Peters et al., 2018), ELMo5.5B
(Peters et al., 2018), Transformer-XL (Dai et al., 2019), BERTLARGE and RoBERTaLARGE. We
focus on LAMA-GoogleRE and LAMA-T-REx, which are aimed at factual knowledge. We also con-
duct probe experiments on LAMA-UHN (Poerner et al., 2019), a more “factual” subset of LAMA,
by filtering out queries that are easy to answer from entity names alone. Different models have dif-
ferent vocabulary sizes. To conduct a more fair comparison experiment, we adopt the intersection
of vocabularies and let every language model rank only tokens in this vocabulary following Petroni
et al. (2019). For simplicity, we only compare K-Apdater (F) which is infused with factual knowl-
edge, with other baseline models.
Results and Discussion Results are shown in Table 5. It is surprising that BERTLARGE performs
better than RoBERTaLARGE. There is one possible reason: BERT uses a character-level BPE (Gage,
1994) vocabulary, while RoBERTa considers byte-level BPE vocabulary. This finding indicates
that, although using bytes makes it possible to learn a subword vocabulary that can encode any text
without introducing “unknown” tokens, it might indirectly harm the model’s ability to learn factual
knowledge, e.g., some proper nouns may be divided into bytes. Thus in the following experiments,
we do not take BERT into account.
K-Adapter outperforms other models (except for BERT) by a huge margin. As for LAMA, com-
pared to RoBERTaLARGE, K-Adapter obtains 2.2% and 1.2% P@1 improvement across Google-
RE and T-REx, respectively. Moreover, compared to RoBERTaLARGE, K-Adapter still achieves
better results on LAMA-UHN. The results demonstrate that K-Adapter captures richer factual
and commonsense knowledge than RoBERTa. Furthermore, Table 6 shows several examples for the
generation of RoBERTaLARGE and K-Adapter for LAMA queries. From these examples, we can
find that the objects predicted by K-Adapter are more accurate.
8
Under review as a conference paper at ICLR 2021
4.5	Case Study
Table 7 gives a qualitative comparison example between K-Adapter and RoBERTa on rela-
tion classification dataset TACRED. The results show that, in most cases, the wrongly predicted
logit value of RoBERTa and the logit value of the true label are actually quite close. For exam-
ple, given “New Fabris closed down JuneJ16"，RoBERTa predicts “no_relation”, but the true label
‘'city _Ofbirth' ranks in second place. If a model could correctly predict the relationship between
“New Fabris” and “June 16”, then it needs to know that “New Fabris” is a company. Thanks to the
factual knowledge in K-ADAPter, it can help the model from predicting “no_relation" to predicting
the correct category label.
Table 7: A case study for K-Adapter and RoBERTa on relation classification dataset TACRED.
UnderIineS and Wavyjines highlight the subject entities and object entities respectively. We report
the top 3 ranked predictions.
Input	True label	Model	Predicted label	Predicted logits
His former student Mark Devlin of the University of ^^jjjjjj^jjjj jPjejnjnsjyjljvjajnjiajwas co-leader of the other , known as the Microwave Anisotropy Telescope .	schools_attended	K-Adapter	[’schools_attended'，'no_relation',‘founded']	[12.67, 9.58,5.26]
		RoBERTa	['no_reIatiOn'，‘founded'，"member_of”]	[9.18, 6.54, 5.07]
Graham had been in custody injVyicouver, British Columbia , since June .	CitieS_of_residence	K-Adapter	['cities_of_residence'，‘countries_of_residence'，'no_relation']	[13.52,6.88,6.61]
		RoBERTa	[,countries_of_residence', ,country_of_death', ,alternate_names']	[7.14,7.03,6.83]
Vladimir Ladyzhenskiy of Russia died after she suffered a shock in the final of the spa world championship in Heinola jjjjj , a southern city of Finland , on Saturday .	cause_of_death	K-Adapter	[‘cause_of_death',‘Origin',‘no_relation']	[11.05,7.65,7.14]
		RoBERTa	[,no_relation', ,cause_of_death', ,origin']	[6.32, 5.90, 5.52]
You can’t have a good season unless it starts well, ” said Bill Martin, co-founder of ShopperTrak, on Saturday . jjjjjjjjj，	founded_by	K-Adapter	[,founded_by,, 'member_of'，’employee_of']	[10.25,9.32,7.37]
		RoBERTa	['no_relation', 'founded_by','employee_of']	[10.01, 8.57, 5.42]
NeW Fahria ClCaed down Tntift 16 ew ars cose own une .	dissolved	K-Adapter	['dissolved', 'no_relation', 'date_of_death']	[12.94, 8.79, 6.83]
	 jjjjjj		RoBERTa	['no_relation', 'dissolved', 'date_of_birth']	[11.44, 9.84, 3.31]
At Countrywide , which is finishing up a round of 12,000 job cuts, Chief Executive Angelo Mozilo said in announcing the Bank of America takeover last week that jjjjjjjj the housing and mortgage sectors were being strained “ as never seen since the Great Depression .	dissolved	K-Adapter	['dissolved', 'no_relation', 'date_of_death']	[11.76, 6.89, 6.42]
		RoBERTa	['date_of_birth', 'date_of_death','no_relation']	[7.44, 6.88, 6.33]
5	Conclusion and Future work
In this paper, we propose a flexible and simple approach, called K-Adapter, to infuse knowledge
into large pre-trained models. K-Adapter remains the original parameters of pre-trained mod-
els unchanged and supports continual knowledge infusion, i.e., new kinds of injected-knowledge
will not affect the parameters learned for old knowledge. Specifically, factual knowledge and lin-
guistic knowledge are infused into RoBERTa with two kinds of adapters, which are pre-trained on
the relation classification task and dependency relation prediction task, respectively. Extensive ex-
periments on three knowledge-driven downstream tasks demonstrate that the performance of each
adapter achieves a significant improvement individually, and even more together. Probing experi-
ments further suggest that K-Adapter captures richer factual and commonsense knowledge than
RoBERTa. In future work, we will infuse more types of knowledge, and apply our framework to
more pre-trained models.
References
Livio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. Matching the
Blanks: Distributional Similarity for Relation Learning. In ACL, pp. 2895-2905, 2019.
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In NIPS, pp. 2787-2795, 2013.
9
Under review as a conference paper at ICLR 2021
Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Andrea Gesmundo, Neil Houlsby, Woj-
ciech Gajewski, and Wei Wang. Ask the Right Questions: Active Question Reformulation with
Reinforcement Learning. In ICLR, 2018.
Eunsol Choi, Omer Levy, Yejin Choi, and Luke Zettlemoyer. Ultra-fine entity typing. In ACL, pp.
87-96, 2018.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.
Transformer-xl: Attentive language models beyond a fixed-length context. In ACL, pp. 2978-
2988, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. In NAACL, pp. 4171-4186, 2019.
Bhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answer-
ing by search and reading. In arXiv preprint arXiv:1707.03904, 2017.
Matthew Dunn, Levent Sagun, Mike Higgins, V. UgUr GUney, Volkan Cirik, and KyUnghyUn Cho.
SearchQA: A New Q&A Dataset Augmented with Context from a Search Engine. In ArXiv
preprint arXiv:1704.05179, 2017.
Hady ElSahar, Pavlos VoUgioUklis, Arslen Remaci, Christophe Gravier, Jonathon S. Hare,
FrederiqUe Laforest, and Elena SimPerL T-REx: A Large Scale Alignment of Natural Language
with Knowledge Base Triples. In LREC, 2018.
PhiliP Gage. A new algorithm for data comPression. The C Users Journal, 12(2):23-38, 1994.
Bin He, Di ZhoU, JinghUi Xiao, QUn LiU, Nicholas Jing YUan, Tong XU, et al. Integrating GraPh Con-
textUalized Knowledge into Pre-trained LangUage Models. In arXiv preprint arXiv:1912.00147,
2019.
Neil HoUlsby, Andrei GiUrgiU, Stanislaw Jastrzebski, BrUna Morrone, QUentin De LaroUssilhe, An-
drea GesmUndo, Mona Attariyan, and Sylvain Gelly. Parameter-Efficient Transfer Learning for
NLP. In ICML, PP. 2790-2799, 2019.
LifU HUang, Ronan Le Bras, Chandra BhagavatUla, and Yejin Choi. Cosmos QA: Machine reading
comPrehension with contextUal commonsense reasoning. In EMNLP, PP. 2391-2401, 2019.
Nora Kassner and Hinrich SchUtze. Negated LAMA: Birds cannot fly. arXiv preprint
arXiv:1911.03343, 2019.
James KirkPatrick, Razvan PascanU, Neil Rabinowitz, Joel Veness, GUillaUme Desjardins, Andrei A.
RUsU, Kieran Milan, John QUan, Tiago Ramalho, Agnieszka Grabska-Barwinska, and et al. Over-
coming catastroPhic forgetting in neUral networks. Proceedings of the National Academy of Sci-
ences, 114(13):3521-3526, Mar 2017. ISSN 1091-6490. doi: 10.1073/Pnas.1611835114.
Anne Lauscher, Ivan Vulic, Edoardo Maria Ponti, Anna Korhonen, and Goran Glavas. Informing
UnsUPervised Pretraining with external lingUistic knowledge. arXiv preprint arXiv:1909.02339,
2019.
Yoav Levine, Barak Lenz, Or Dagan, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua,
and Yoav Shoham. Sensebert: Driving some sense into bert. arXiv preprint arXiv:1908.05646,
2019.
Yankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly suPervised oPen-
domain question answering. In ACL, PP. 1736-1745, 2018.
Xiao Ling and Daniel S. Weld. Fine-grained entity recognition. In Jorg Hoffmann and Bart Selman
(eds.), Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence, July 22-26,
2012, Toronto, Ontario, Canada. AAAI Press, 2012.
Xiao Ling, Sameer Singh, and Daniel S. Weld. Design challenges for entity linking. TACL, 3:
315-328, 2015.
10
Under review as a conference paper at ICLR 2021
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and
LUke Zettlemoyer. Deep contextualized word representations. In NAACL, pp. 2227-2237, 2018.
Matthew E Peters, Mark Neumann, IV Logan, L Robert, Roy Schwartz, Vidur Joshi, Sameer Singh,
and Noah A Smith. Knowledge enhanced contextual word representations. In EMNLP, pp. 43-54,
2019.
Fabio Petroni, Tim Rocktaschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,
and Sebastian Riedel. Language Models as Knowledge Bases? In EMNLP, pp. 2463-2473, 2019.
Nina Poerner, Ulli Waltinger, and Hinrich SchUtze. BERT is Not a Knowledge Base (Yet): Factual
Knowledge vs. Name-Based Reasoning in Unsupervised QA. arXiv preprint arXiv:1911.03681,
2019.
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-
standing by generative pre-training. OpenAI Blog, 2018.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional Attention
Flow for Machine Comprehension. ArXiv, abs/1611.01603, 2016.
Sonse Shimaoka, Pontus Stenetorp, Kentaro Inui, and Sebastian Riedel. An attentive neural architec-
ture for fine-grained entity type classification. In Proceedings of the 5th Workshop on Automated
Knowledge Base Construction(AKBC), pp. 69-74, 2016.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pp. 5998-6008,
2017.
Shuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,
Gerald Tesauro, Bowen Zhou, and Jing Jiang. Reinforced reader-ranker for open-domain question
answering. arXiv preprint arXiv:1709.00023, 2017a.
Shuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim
Klinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in
open-domain question answering. In ICLR, 2018.
Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang, and Ming Zhou. Gated self-matching networks
for reading comprehension and question answering. In ACL, pp. 189-198, 2017b.
Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan Liu, Juanzi Li, and Jian Tang. KEPLER:
A Unified Model for Knowledge Embedding and Pre-trained Language Representation. arXiv
preprint arXiv:1911.06136, 2019.
Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. Pretrained Encyclopedia:
Weakly Supervised Knowledge-Pretrained Language Model. In ICLR, 2020.
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.
XLNet: Generalized Autoregressive Pretraining for Language Understanding. arXiv preprint
arXiv:1906.08237, 2019.
Yuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli, and Christopher D. Manning. Position-
aware Attention and Supervised Data Improve Slot Filling. In EMNLP, pp. 35-45, 2017.
11
Under review as a conference paper at ICLR 2021
Yuhao Zhang, Peng Qi, and Christopher D. Manning. Graph Convolution over Pruned Dependency
Trees Improves Relation Extraction. In EMNLP, pp. 2205-2215, 2018.
Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: Enhanced
Language Representation with Informative Entities. In ACL, pp. 1441-1451, 2019.
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and
Sanja Fidler. Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching
Movies and Reading Books. In ICCV, pp. 19-27, 2015.
12