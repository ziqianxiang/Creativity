Under review as a conference paper at ICLR 2021
Pareto Adversarial Robustness: Balancing
Spatial Robustness and Sensitivity-based Ro-
BUSTNESS
Anonymous authors
Paper under double-blind review
Ab stract
Adversarial robustness, mainly including sensitivity-based robustness and spa-
tial robustness, plays an integral part in the robust generalization. In this paper,
we endeavor to design strategies to achieve comprehensive adversarial robust-
ness. To hit this target, firstly we investigate the less-studied spatial robustness
and then integrate existing spatial robustness methods by incorporating both lo-
cal and global spatial vulnerability into one spatial attack design. Based on this
exploration, we further present a comprehensive relationship between natural ac-
curacy, sensitivity-based and different spatial robustness, supported by the strong
evidence from the perspective of representation. More importantly, in order to bal-
ance these mutual impact within different robustness into one unified framework,
we incorporate the Pareto criterion into the adversarial robustness analysis, yield-
ing a novel strategy towards comprehensive robustness called Pareto Adversarial
Training. The resulting Pareto front, the set of optimal solutions, provides the set
of optimal balance among natural accuracy and different adversarial robustness,
shedding light on solutions towards comprehensive robustness in the future. To
the best of our knowledge, we are the first to consider comprehensive robustness
via the multi-objective optimization.
1	Introduction
Robust generalization can serve as an extension of tradition generalization, i.e., Empirical Risk
Minimization in the case of i.i.d. data (Vapnik & Chervonenkis, 2015), where the test environments
might differ slightly or dramatically from the training environment (Krueger et al., 2020). Improving
the robustness of deep neural networks has been one of the crucial research topics, with various
different threads of research, including adversarial robustness (Goodfellow et al., 2014; Szegedy
et al., 2013), non-adversarial robustness (Hendrycks & Dietterich, 2019; Yin et al., 2019), Bayesian
deep learning (Neal, 2012; Gal, 2016) and causality (Arjovsky et al., 2019). In this paper, we focus
on the adversarial robustness where adversarial examples are carefully manipulated by human to
drastically fool the machine learning models, e.g., deep neural networks, posing a serious threat
especially on safety-critical applications. Currently, adversarial training (Goodfellow et al., 2014;
Madry et al., 2017; Ding et al., 2018) is regarded as one promising and widely accepted strategy to
address this issue.
However, similar to Out-of-Distribution (OoD) robustness, one crucial issue is that adversarial ro-
bustness also has many aspects (Hendrycks et al., 2020), mainly including sensitivity-based robust-
ness (Tramer et al., 2020), i.e. robustness against pixel-wise perturbations (normally within the
constraints of an lp ball), and spatial robustness, i.e., robustness against multiple spatial transfor-
mations. Firstly, in the computer vision and graphics literature, there are two main factors that
determine the appearance of a pictured object (Xiao et al., 2018; Szeliski, 2010): (1) the lighting
and materials, and (2) geometry. Most previous adversarial robustness focus on the (1) factor (Xiao
et al., 2018) based on pixel-wise perturbations, e.g., Projected Gradient Descent (PGD) attacks, as-
suming the underlying geometry stays the same after the adversarial perturbation. The other rising
research branch tackled with the second factor, such as Flow-based (Xiao et al., 2018) and Rotation-
Translation (RT)-based attacks (Engstrom et al., 2017; 2019). Secondly, by explicitly exploring the
human perception, Sharif et al. (2018) pointed out that sensitivity-based robustness, i.e., lp-distance
1
Under review as a conference paper at ICLR 2021
measured robustness, is not sufficient to adversarial robustness in order to maintain the perceptual
similarity. This is owing to the fact that although spatial attacks or geometric transformations also
result in small perceptual differences, they yield large lp distances.
In order to head towards the comprehensive adversarial robustness, we find that the crucial issue
to investigate the aforementioned whole part of adversarial robustness is the relationships among
accuracy, sensitivity-based robustness and spatial robustness. Prior to our work, a clear trade-off
between sensitivity-based robustness and accuracy has been revealed by a series of works (Zhang
et al., 2019; TsiPras et al., 2018; RaghUnathan et al., 2020). Besides, recent work (Tramer & Boneh,
2019; Kamath et al., 2020) exhibited that there seems to exist an obscure trade-off between Rotation-
Translation and sensitivity-based robUstness. However, this conclUsion lacks considering Flow-
based attacks (Xiao et al., 2018; Zhang & Wang, 2019), another non-negligible Part in the sPatial
robUstness evalUation, making the PrevioUs conclUsion less comPrehensive or reliable. As sUch, the
comPrehensive relationshiPs among all the qUantities mentioned above are still Unclear and remain
to be fUrther exPlored. More imPortantly, new robUst strategy that can harmonize all the considered
correlations is needed, in order to achieve oPtimal balance within the comPrehensive robUstness.
In this PaPer, in order to design a new aPProach towards comPrehensive robUstness, we firstly ex-
Plore the two main branches in the sPatial robUstness, i.e., Flow-based sPatial attack (Xiao et al.,
2018) and Rotation-Translation (RT) attack (Engstrom et al., 2019). By investigating the different
imPacts of these two attacks on the sPatial sensitivity, we ProPose an integrated differentiable sPatial
attack framework, considering both local and global sPatial vUlnerability. Based on that, we Present
a comPrehensive relationshiP among accUracy, sensitivity-based robUstness and two branches of sPa-
tial robUstness. EsPecially we show that the trade-off between sensitivity-based and RT robUstness is
fUndamental trade-off as oPPosed to the highly interwoven correlation between sensitivity-based and
Flow-based sPatial robUstness. We fUrther Provide strong evidence based on their different saliency
maPs from the PersPectives of shaPe-bias, sParse or dense rePresentation. Lastly, to balance these
different kinds of mUtUal imPacts within a Unified adversarial training framework, we introdUce the
Pareto criterion (Kim & De Weck, 2005; 2006; Zeleny, 2012) in the mUlti-objective oPtimization,
thUs develoPing an oPtimal balance between the interPlay of natUral accUracy and different adversar-
ial robUstness. By additionally incorPorating the two-moment term caPtUring the interaction between
losses of accUracy and different robUstness, we finally ProPose a bi-level oPtimization framework
called Pareto Adversarial Training. The resUlting Pareto front Provides the set of oPtimal solUtions
that balance Perfectly all the considered relationshiPs, oUtPerforming other existing strategies. OUr
contribUtions are sUmmarized as follows:
•	We ProPose an integrated sPatial attack framework that incorPorates both local and global
sPatial vUlnerability based on Flow-based and RT attacks, Paving the way towards the com-
Prehensive sPatial robUstness analysis in the fUtUre.
•	We Present comPrehensive relationshiPs within accUracy, sensitivity-based, different sPatial
robUstness, sUPPorted by strong evidence from the PersPective of rePresentation.
•	We incorPorate the Pareto criterion into adversarial robUstness analysis, and are the first
attemPt to consider mUltiPle adversarial robUstness via the mUlti-objective oPtimization.
2	Towards Comprehensive Spatial Robustness
2.1	Motivation
In order to better investigate the relationshiPs between accUracy and different kinds of adversarial
robUstness, we need to firstly Provide a fine-grained Understanding of sPatial robUstness, which has
been less stUdied as oPPosed to sensitivity-based robUstness. We sUmmarize two major branches
among a flUrry of related work aboUt sPatial robUstness (Engstrom et al., 2017; 2019; Xiao et al.,
2018; Zhang & Wang, 2019; Tramer & Boneh, 2019; Kamath et al., 2020): (1) Flow-based Attacks,
and (2) Rotation-Translation (RT) Attacks. SPecifically, we find that the former mainly focUses on
the local sPatial vUlnerability while the latter tends to caPtUre the global sPatial sensitivity. OUr
motivation is to firstly shed light on the fUndamental difference between these two aPProaches, and
then ProPose an integrated sPatial robUstness evalUation metric.
2
Under review as a conference paper at ICLR 2021
Benign Examples
PGD Attack
Flow-based Attack
RT Attack
Integrated Spatial Attack
Figure 1: Visualization of Flow-based, RT and Our Integrated Spatial adversarial examples on
MNIST, CIFAR-10 and Caltech-256. More discussions can refer to Appendix A.1.
2.2	Integrated SPATIAL Attack： Combing Local and Global SPATIAL SENSrnVrrY
Local Spatial Robustness: Flow-based Attacks The most representative Flow-based Attack
is Spatial Transformed Attack (Xiao et al., 2018), in which a differentiable flow vector WF =
(∆μ, ∆v) is defined in the 2D coordinate (μ,v) to craft adversarial spatial transformation. The
vanilla targeted Flow-based attack (Xiao et al., 2018) (κ = 0) follows the optimization manner:
WF = arg min max fθ (XwF) - fθ(XwF) + TLflow (wf),	(i)
wF	i6=t
where fθ (X) = fθ1 (X), . . . , fθK (X) is the classifier in the K -classification problem. XwF is Flow-
based adversarial example parameterized by flow vector WF . Lf low measures the local smoothness
of spatial transformation further balanced by τ .
Interestingly, in our empirical study shown in the left part of Figure 1, we find that Flow-based
attack tends to yield local permutations among pixels in some specific regions irrespective of the
option of τ , rather than the global spatial transformation based on their shapes. We analyze that
this phenomenon is owing to two factors: 1) Local permutations, especially in regions where colors
of pixels change dramatically, are already sufficiently sensitive to manipulate, demonstrated by our
empirical results shown above. 2) The optimization manner does not incorporate any sort of shape
transformation information, e.g., a parameter equation of rotation, as opposed to vanilla Rotation-
Translation attack, which we present in the following. Thus, Flow-based attacks tend to capture the
local spatial vulnerability. Further, for the need to design the integrated spatial attack, we transform
Eq 1 into its untargeted version under cross entropy loss with flow vector bounded by an F -ball:
WF = arg max LCE(XwF ,y) s.t. kwF k ≤ CF	(2)
wF
where LθCE(X, y) = log Pj exp fθj (X) - fθy (X). one difference compared with Eq. 1 is that
we replace local smoothness term Lf low with our familiar lp constraint. Moreover, vanilla Flow-
based attack (Xiao et al., 2018) follows the maX operation suggested in (Carlini & Wagner, 2017).
However we leverage cross entropy loss instead in pursuit of a uniform optimization form in our
integrated spatial attack. Proposition 1 reveals the correlation between the two loss, indicating that
the smooth approximation version of maX operation in Eq. 1, denoted as LθS, has a parallel updating
direction with Cross Entropy loss regarding WF. Proof can be found in Appendix A.2.
Proposition 1. For a fixed	(XwF,	y)	and θ,	consider	LθS (X, y)	= log	i6=y exp	fθi(X)	- fθy (X),
the smooth version loss of Eq. 1 without local smoothness term, then we have
VwFLCE(XwF, y) = r(xwF,y)VwFLS(xwF,y), where r(xwF,y)=
Pi=y exP (fθ (XwF )
PieXP (fθ (XwF A
(3)
Global Spatial Robustness: Rotation-Translation (RT)-based Attacks The original Rotation-
Translation attack (Engstrom et al., 2017; 2019) applies parameter equations constraint on the 2D
coordinate, thus capturing the global spatial information:
0
u
v0
cos θ - sin θ
sin θ cos θ
u δu
v + δv
(4)
0
3
Under review as a conference paper at ICLR 2021
To design a generic spatial transformation matrix that can simultaneously consider rotation, trans-
lation, cropping and scaling, we re-parameterize the transform matrix as a generic 6-dimensional
affine transformation matrix, inspired by (Jaderberg et al., 2015):
u
v
1
u0	1
v0	= (	0
11
wRT
21
wRT
12	13
wRT	wRT
22	23
wRT	wRT
(5)
0
1
0
0
+
)∙
where we denote AwRT as the generic 6-dimensional affine transformation matrix, in which each
wRT indicates the increment on different spatial aspects. For example, (wR13T , wR23T ) determines the
translation. Finally, the optimization form of the resulting generic and differentiable RT-based attack
bounded by RT -ball is exhibited as:
WRT = arg max LCE(XwRT ,y) s.t. kwRTk ≤ £rt.	(6)
wRT
Integrated Spatial Robustness The key to achieve this goal is to design an integrated parameter-
ized sampling grid TwRT,wF (G) that can warp the regular grid with both affine and flow transfor-
mation, where G is the generated grid. We show our integrated approach as follows:
u
v
1
TwRT ,wF
(G)
AwRT
wF
1
(7)
xadv
TwRT,wF
(G) ◦ x.
+
Then we sample new xadv by TwRT,wF (G) via differentiable bilinear interpolation (Jaderberg et al.,
2015). Then the loss function of the differentiable integrated spatial attack can be presented as:
w* = arg maxLCE(x + ηw,y), s.t. kw∣∣ ≤ e,
w
(8)
where w = [wF , wRT]T and ηw is the crafted integrated spatial perturbation. Note that ηw itself
does not necessarily satisfy the lp constraint directly. For the implementation, we follow the PGD
procedure (Madry et al., 2017), a common practice in sensitivity-based attacks. We consider the
infinity norm of w and different learning rates for the two sorts of spatial robustness:
w
w
t+1
t+1
RT
wFt
wRT
αF
αRT
CliPe(Sign(Vw LCE(Xwt ,y))),
(9)
F
+
xwt+1 = Twt+1 (G) ◦ xwt ,
where we denote wt+1 = [wFt+1, wRt+T1]T and = [F, RT]T. From Figure 1, we can observe
that our Integrated SPatial Attack can construct both local and global sPatial transformations on
images. Then, we visualize the loss surface under this Integrated SPatial Attack leveraging “filter
normalization” (Li et al., 2018) as illustrated in Figure 2. It is worth noting that the highly non-
concave loss landscape with respect to only rotation and translation raised by (Engstrom et al., 2019)
has been largely alleviated by considering both local and global spatial vulnerability, verifying the
efficiency of our Integrated Spatial Attack.
Figure 2: Loss landscape of Integrated Spatial Attack on CIFAR-10. (Left) A distant view of loss
landscape w.r.t W before the optimization in Eq. 8. (Middle) A close view before the optimization
that shows a highly convex surface near the initialization point. (Right) The loss landscape around
the maxima w* after the optimization in Eq. 8. More explanation can refer to Appendix A.3
4
Under review as a conference paper at ICLR 2021
3	Relationships B etween Sensitivity and Spatial Robustness
3.1	Relationships
Based on the analysis above, next we focus on investigating the relationships between the strength of
one specific robustness and other types of robustness. Firstly, we empirically explore these relation-
ships through conducting thorough experiments on MNIST, CIFAR-10 and Caltech-256 datasets.
By adversarially training multiple PGD (sensitivity-based) robust models with different iteration
steps, we further test their Flow-based and RT-based spatial robustness via methods proposed above.
(％ SSeUISnqo^ IBnBdsAoWn8<一器1 -SnqOH
(％ SSeUISnqo^ IBnBdsAoWn8<一器1 -SnqOH
(％ SSeUISnqo^ IBnBdsAoWn8<一器1 -SnqOH
Caltech-256
oo
80
60
40
20
0
0	2	4	6
PGD Tra⅛ed Models (PGD Robustness)
Figure 3: Relationship between sensitivity (PGD) robustness and two spatial robustness on three
datasets. The X-axis represents the PGD-trained models under different PGD iterations while the Y-
axis represents the robust accuracy on test data perturbed by Flow Attack (red) and RT Attack (blue).
As shown in Figure 3, it turns out that Flow-based spatial robustness (red) measured by its robust
test accuracy presents a steady ascending tendency across three datasets as the PGD robustness
increases, while the trend of RT-based spatial robustness fluctuates conversely. Itis worth noting that
we test the accuracy on correctly classified test data for the considered model for a fair comparison.
The trade-off between sensitivity-based and RT-based spatial robustness is consistent with previous
conclusion (Kamath et al., 2020; Tramer & Boneh, 2019), but it does not (even on the contrary)
apply to Flow-based spatial robustness that delicately measures the local spatial sensitivity of an
image. We provide the strong evidence from the perspective of representation in the next subsection.
3.2	Explanation from the Viewpoint of Shape-bias Representation
We go first with our brief conclusion: the
sensitivity-based robustness corresponds to the
shape-bias representation (Shi et al., 2020;
Zhang & Zhu, 2019), indicating that sensitivity-
based robust models rely more on global shape
when making decisions rather than local tex-
ture. By contrast, the spatial robustness is
associated with different representation strate-
gies, serving as a significant supplement toward
the comprehensive robust representation. To
demonstrate this conclusion, We visualize the
saliency maps of naturally trained, PGD, Flow-
based and RT adversarially trained models on
some randomly selected images on Caltech-256
exhibited in Figure 4. Specifically, visualizing
the saliency maps aims at assigning a sensitivity
value, sometimes also called “attribution”, to
show the sensitivity of the output to each pixel
of an input image. Following (Shi et al., 2020;
Natural PGD AT	Flow AT	RT AT
Figure 4: Saliency map of four types of train-
ing models on some randomly selected images on
Caltech-256.
Zhang & Zhu, 2019), we leverage SmoothGrad (Smilkov et al., 2017) to calculate saliency map
S (x):
5
Under review as a conference paper at ICLR 2021
S(χ) = 1 XX4
n	∂xi
i=1
(10)
Figure 4 manifests that PGD trained models tend to learn a scarce and shape-biased representation
among all pixels of an image, while two types of spatially adversarially trained models suggest
converse representation. In particular, the resulting representation from the Flow-based training
model has the tendency towards a shape-biased one as it places extreme values on the pixels around
the shape of objects, e.g., the edge between the horse and the background shown in Flow AT in
Figure 4. On the contrary, RT-based models have less reliance on the shape of objects, and at
the meantime, the saliency values tend to be dense, scattering around more pixels of an image.
Quantitatively, We calculate the distance of saliency maps from different models across all test data
on Caltech-256 dataset, and then compute their skewness shown in Figure 5.
SIupon IBJnJBZ sʌ ssuUMu*s JO s⅞s
mmM151°5 O
Təpɑn IV QaDdSA ∞l J∞⅛ UB≡9n
Figure 5: Median of skewness of Sanency maps difference among robust models across all test data
compared with other models. The first three sub-pictures are compared with the naturally trained
model while the last one is compared with the PGD trained model.
Specifically, we compute the pixel-wise distance between the saliency map of a robust model and
that from a considered model, and then we calculate the median of skewness of the saliency map
difference among all test data. Note that if two saliency maps have no difference, then the difference
values will be a normal distribution with skewness 0. A negative skewness indicates that the original
saliency map (representation) tends to be sparse compared with a considered model. We plot the
tendency of skewness as the strength of some specific robustness increases shown in Figure 5. Based
on the observations in Figure 5, we summarize the following conclusions:
1) Based on the first and forth sub-pictures, both PGD and Flow-based robust models tend to learn a
sparse and shape-biased representation compared with the natural model, but the Flow-based trained
model is less sparse or shape-biased in comparison with the PGD trained one. 2) On the contrary,
RT-based robust models have the trend to learn a dense representation, which is also intuitive as the
RT trained model is expected to memorize broader pixel locations to cope with potential rotation
and transformation in the test data. The fundamental representation discrepancy of RT-based and
sensitivity-based robustness provides deep insights to explain why the trade-off of these two robust-
ness occurs. In the Appendix A.4, we provide a sketch map that better illustrates their relationships.
4	Pareto Adversarial Robustness with Pareto Front
4.1	Motivation
Pareto Optimization. Based on the aforementioned analysis on the relationships between natural
accuracy and different kinds of adversarial robustness, a natural question is how to design a training
strategy that can perfectly balance their mutual impacts, which mainly sources from their different
representation manners. In particular, in most cases their relationships reveal trade-off ones, except
when the sensitivity robustness increases, Flow-based spatial robustness is enhanced. To better ad-
dress these competing optimization objectives, we introduce Pareto optimization (Kim & De Weck,
2005; Lin et al., 2019), and the resulting Pareto front, the set of Pareto optimal solutions, can offer
valuable trade-off information between objectives. We provide more background information about
Pareto optimization in Appendix A.5.
6
Under review as a conference paper at ICLR 2021
Companson with Single PGD AT
g' 20
I io
第o
⅞-w
I -20
W -30
⅞ -40
⅛ -50
g-60
0.10	0.15	0.20	0.25	0.30
E in PGD attacks Used for Max AT
Figure 6: The difference between the
model trained by the PGD method and
Max AT with different parameter for
the PGD attack in the adversarial train-
ing.
Limitation of Existing Strategies. Given perturbation
sets Si, i = 1, ..., m, and its corresponding adversarial
risk Radv(f; Si)= E(χ,y)〜D [maXr∈Si L(f(x + r), y)],
our goal is to find fθ that can achieve the uniform risk
minimization across all Si as well as the minimal risk
in the natural data. 1) Average adversarial training (Ave
AT) (Tramer & Boneh, 2019), i.e., Rave(f; S):=
E(χ,y)~D [m1 Pm=ι maXr∈Si L(f(x + r), y)],	regards
each adversarial robustness as the equal status. It
may yield unsatisfactory solutions when the strength
of different attacks mixed in training are not bal-
anced, which we demonstrate in our experiments.
2) Max adversarial training (Max AT) (Tramer &
Boneh, 2019; Maini et al., 2019), i.e., Rmax(f;S) :=
E(x,y)〜D[maxi{maXr∈Si L(f(x + r),y)}] may overfit
to specific type of adversarial robustness if its adversarial
attack used for training is too strong. Figure 6 demon-
strates that as the strength of PGD attack used in Max
AT increases, the comprehensive robustness of Max AT
degenerates to a single PGD adversarial training, owing
to the fact that the PGD loss tends to dominate among
all losses. Appendix A.6 provides more details about Figure 6 and also introduces Proposition 2 to
illuminate that Max AT is also closely linked with specific weights of Ave AT.
4.2	Pareto Adversarial Robustness and Pareto Adversarial Training
The key to Pareto adversarial robustness is to find the optimal combination (trade-off in most cases)
between natural accuracy, sensitivity-based and spatial robustness. Specifically, we hope to compute
optimal α in the following formula:
min Lθ = α0Lnat + α1LPGD + α2LFlow + α3LRT,	(11)
θ,α
where α = (α0 , α1, α2 , α3 ) and Lθ is the cross entropy loss based on the integrated framework we
previously analyzed. Lnat, LPGD, LFlow and LRT represent the natural loss, the PGD adversarial loss,
the Flow-based and the RT-based adversarial loss, respectively. Note that we additionally introduce
natural loss to guarantee a high-level natural accuracy (Raghunathan et al., 2020). However, direct
joint minimization over Eq. 11 will degenerate to the trivial solution and the introduction of vali-
dation dataset to tune α, e.g., DARTS (Liu et al., 2018), is also computationally expensive for the
adversarial training with multiple iterations. To avoid these, our approach is to introduce the Pareto
criteria to choose optimal α to balance the mutual impacts between different adversarial robustness.
Specifically, based on Eq 11, we additionally introduce the two-moment term regarding all losses
into a bi-level optimization framework, in order to compute the optimal combination α during the
whole training process. We name this bi-level optimization approach as Pareto Adversarial Training,
and the lower-level optimization regarding α can be formulated as follows:
33	3	3
min XX
Ex (αi Li - αj Lj ) s.t. X αiEx (Li) = r, X αi = 1, αi ≥ 0, ∀i = 0, 1, 2, 3,
i=0 j=0	i=1	i=0	(12)
where L0, L1, L2, L3 represent Lnat, LPGD, LFlow, LRT respectively for simplicity, sharing the same
model parameter θ. r indicates the expectation of one-moment over all robust losses, i.e., spatial
and sensitivity-based losses, which reflects the strength of comprehensive robustness we require
after solving this quadratic optimization. In particular, given the fixed Ex (Li) following the up-
dating of θ based on Eq. 11, the larger r we require will push the resulting αi, i = 1, 2, 3 larger
as well, thus putting more weights on the robust losses while the whole process of Pareto Adver-
sarial Training. For the understanding of the two-moment objective function, firstly we regard all
losses as random variables with its stochasticity arising from the mini-batch sampling from data.
The weighted quadratic difference is to measure the trade-off within natural accuracy and various
robustness, and then the minimization is to alleviate this mutual trade-off under certain constraints.
In addition, we leverage sliding windows technique to compute the expectation and CVXOPT tool to
7
Under review as a conference paper at ICLR 2021
......................................................................................... 
.0.5.0.5.0.5.0.5
3.ZZLL0.0.-0.
(％) AOEJnQaV UBsOPSO≡JOES
MNIST
rareto Frlxrt
因TetoAT
PGDAT
SpatialAT
MaxAT
AveAT
Pareto Front
ParetoAT
PGDAT
Spatial AT
MaxAT
AveAT
CIFAR-IO
3.0
2.5-
2.0-
1.5-
1.0-
0.5
Caltech-256
Pareto Front
ParetoAT
PGDAT
SpatialAT
MaxAT
AveAT
・★★★★

2.5-
2.0-
1.5-
1.0-
0.5-
60 80 100 120 140 160	60 70 80 90 100 110 120 130	70 80 90 100 110 120
Robustness Score	Robustness Score	Robustness Score
Figure 7: The Pareto front between the robustness score and sacrificed clean accuracy on MNIST,
CIFAR-10 and Caltech-256. The vertical axis is the decrease of the natural accuracy compared with
the naturally trained model and has been under the log transformation along two directions.
solve this quadratic optimization within each mini-batch training. Overall, for the upper level opti-
mization in our bi-level Pareto adversarial training method, we leverage our familiar SGD method to
update θ in Eq. 11 with α calculated from the lower level problem. In the lower level procedure, we
solve the quadratic optimization regarding α to obtain the optimal combination among natural loss,
sensitivity-based and spatial adversarial loss. We provide the proof about the quadratic formulation
in Eq. 12 and our algorithm description in Appendix A.7.
4.3	Pareto Front in Empirical S tudy
By adjusting the upper bound of expected adversarial robustness loss, i.e., r, we can evenly generate
Pareto optimal solutions where the obtained models will have different levels of robustness under op-
timal combinations. The set of all Pareto optimal solutions then forms the Pareto front. Concretely,
we train deep neural networks under different adversarial training strategies, and then evaluate their
robustness by PGD, Flow-based and RT attacks, which we proposed previously, under different it-
eration steps. After equally averaging robust accuracy for each category of these attacks, we then
compute the difference of robust accuracy between different training strategies and standard train-
ing, attaining Robustness Score to evaluate the comprehensive robustness of all adversarial training
strategies. Finally, we plot the Robustness Score and sacrificed clean accuracy of all methods across
three datasets in Figure 7. Experimental details can be found in Appendix A.8.
The Pareto criterion (Appendix A.5) exhibited in Figure 7 can be interpreted that Pareto Adversarial
Training can achieve the best comprehensive robustness compared with other training strategies,
given a certain level of sacrificed clean accuracy we can tolerate. By adjusting the different levels
of expected comprehensive robustness r in Pareto Adversarial Training, we can develop the set of
Pareto optimal solutions, i.e., the Pareto front. It manifests that all other methods are above our
Pareto front, thus lacking effectiveness compared with our proposal. Overall, our proposed Pareto
Adversarial Training develops an optimal (Pareto) criterion, by which we can maintain the optimal
balance among the mutual impacts of natural accuracy and different robustness, based on the deep
understanding of their relationships.
5	Discussion and Conclusion
The essential purpose of our work is to design a novel approach towards comprehensive adversarial
robustness. To achieve this goal, we firstly analyze the two main branches of spatial robustness
and then integrate them into one framework. Based on that, we further investigate the thorough
relationships between sensitivity-based and two distinct spatial robustness from the perspective of
representation. More importantly, having understanding the mutual impacts of different kinds of
adversarial robustness, we introduce Pareto criterion into adversarial training framework, yielding
the Pareto Adversarial Training. The resulting Pareto front provides optimal performance under the
Pareto Criterion over existing baselines. In the future, we hope to apply Pareto analysis into more
general Out-of-Distribution generalization settings.
8
Under review as a conference paper at ICLR 2021
References
Martin Arjovsky, Leon BottoU,Ishaan Gulrajani, and David Lopez-Paz. Invariant risk minimization.
arXiv preprint arXiv:1907.02893, 2019.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
ieee symposium on security and privacy (sp), pp. 39-57. IEEE, 2017.
Gavin Weiguang Ding, Yash Sharma, Kry Yik Chau Lui, and Ruitong Huang. Max-margin ad-
versarial (mma) training: Direct input space margin maximization through adversarial training.
International Conference on Learning Representations, ICLR 2020, 2018.
Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A rotation and a
translation suffice: Fooling cnns with simple transformations. arXiv preprint arXiv:1712.02779,
1(2):3, 2017.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. Ex-
ploring the landscape of spatial robustness. In International Conference on Machine Learning,
pp. 1802-1811, 2019.
Yarin Gal. Uncertainty in deep learning. University of Cambridge, 1(3), 2016.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. International Conference on Learning Representations, 2014.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. International Conference on Learning Representations, 2019.
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul
Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical
analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241, 2020.
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Ad-
vances in neural information processing systems, pp. 2017-2025, 2015.
Sandesh Kamath, Amit Deshpande, and KV Subrahmanyam. Invariance vs. robustness of neural
networks. arXiv preprint arXiv:2002.11318, 2020.
Il Yong Kim and OL De Weck. Adaptive weighted sum method for multiobjective optimization:
a new method for pareto front generation. Structural and multidisciplinary optimization, 31(2):
105-116, 2006.
Il Yong Kim and Oliver L De Weck. Adaptive weighted-sum method for bi-objective optimization:
Pareto front generation. Structural and multidisciplinary optimization, 29(2):149-158, 2005.
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan Binas, Remi Le
Priol, and Aaron Courville. Out-of-distribution generalization via risk extrapolation (rex). arXiv
preprint arXiv:2003.00688, 2020.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss land-
scape of neural nets. In Advances in Neural Information Processing Systems, pp. 6389-6399,
2018.
Xi Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong. Pareto multi-task learning.
In Advances in Neural Information Processing Systems, pp. 12060-12070, 2019.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv
preprint arXiv:1806.09055, 2018.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. To-
wards deep learning models resistant to adversarial attacks. International Conference on Learning
Representations, ICLR 2018, 2017.
Pratyush Maini, Eric Wong, and J Zico Kolter. Adversarial robustness against the union of multiple
perturbation models. International Conference on Machine Learning, 2019.
9
Under review as a conference paper at ICLR 2021
Radford M Neal. Bayesian learning for neural networks, volume 118. Springer Science & Business
Media, 2012.
Aditi Raghunathan, Sang Michael Xie, Fanny Yang, John Duchi, and Percy Liang. Understand-
ing and mitigating the tradeoff between robustness and accuracy. International Conference on
Machine Learning, 2020.
Mahmood Sharif, Lujo Bauer, and Michael K Reiter. On the suitability of lp-norms for creating and
preventing adversarial examples. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition Workshops ,pp.1605-1613, 2018.
Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, and Jingdong Wang. Informative
dropout for robust representation learning: A shape-bias perspective. International Conference
on Machine Learning, 2020.
Daniel Smilkov, Nikhil ThoraL Been Kim, Fernanda Viegas, and Martin Wattenberg. Smoothgrad:
removing noise by adding noise. arXiv preprint arXiv:1706.03825, 2017.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Richard Szeliski. Computer vision: algorithms and applications. Springer Science & Business
Media, 2010.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations. In
Advances in Neural Information Processing Systems, pp. 5866-5876, 2019.
Florian Tramer, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and Jorn-Henrik Jacobsen.
Fundamental tradeoffs between invariance and sensitivity to adversarial perturbations. ICML,
2020.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry.
Robustness may be at odds with accuracy. International Conference on Learning Representations,
ICLR 2019, 2018.
Vladimir N Vapnik and A Ya Chervonenkis. On the uniform convergence of relative frequencies of
events to their probabilities. In Measures of complexity, pp. 11-30. Springer, 2015.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially trans-
formed adversarial examples. International Conference on Learning Representations, ICLR 2018,
2018.
Dong Yin, Raphael Gontijo Lopes, Jon Shlens, Ekin Dogus Cubuk, and Justin Gilmer. A fourier per-
spective on model robustness in computer vision. In Advances in Neural Information Processing
Systems, pp. 13276-13286, 2019.
Milan Zeleny. Multiple criteria decision making Kyoto 1975, volume 123. Springer Science &
Business Media, 2012.
Haichao Zhang and Jianyu Wang. Joint adversarial training: Incorporating both spatial and pixel
attacks. arXiv preprint arXiv:1907.10737, 2019.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric P Xing, Laurent El Ghaoui, and Michael I Jordan.
Theoretically principled trade-off between robustness and accuracy. International Conference on
Machine Learning, 2019.
Tianyuan Zhang and Zhanxing Zhu. Interpreting adversarially trained convolutional neural net-
works. International Conference on Machine Learning, 2019.
10