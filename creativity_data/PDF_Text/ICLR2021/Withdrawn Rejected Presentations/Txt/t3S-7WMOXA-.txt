Under review as a conference paper at ICLR 2021
Probabilistic	Multimodal	Representation
Learning
Anonymous authors
Paper under double-blind review
Ab stract
Learning multimodal representations is a requirement for many tasks such as
image-caption retrieval. Previous work on this problem has only focused on find-
ing good vector representations without any explicit measure of uncertainty. In
this work, we argue and demonstrate that learning multimodal representations as
probability distributions can lead to better representations, as well as providing
other benefits such as adding a measure of uncertainty to the learned representa-
tions. We show that this measure of uncertainty can capture how confident our
model is about the representations in the multimodal domain, i.e, how clear it is
for the model to retrieve/predict the matching pair. We experiment with similar-
ity metrics that have not been traditionally used for the multimodal retrieval task,
and show that the choice of the similarity metric affects the quality of the learned
representations.
1	Introduction
Representation learning is a core problem of machine learning that deals with the embedding of
raw input data (e.g., an image or a document) into a latent space. Learning such embeddings is
a pre-requisite for automated information processing tasks, such as language modeling (Mikolov
et al., 2013), face recognition (Schroff et al., 2015), and image retrieval (Ustinova & Lempitsky,
2016). In recent years, there has been a surge of interest in deep representation learning, with
remarkable empirical successes in a variety of applications. Many have gone beyond representing
data of a single type, and have proposed methods to learn joint embeddings that simultaneously
encode several different modalities (such as image, video, text, speech, or audio) into a common
space (Kiros et al., 2014; Vendrov et al., 2016; Faghri et al., 2018; Harwath et al., 2018; Lee et al.,
2018; Zablocki et al., 2018; Lu et al., 2019; Liu et al., 2019). Multimodal techniques can learn
richer representations that enable comparison across modalities, which is required by applications
such as image captioning (Socher et al., 2014; Karpathy & Fei-Fei, 2015) and language-based image
retrieval (Faghri et al., 2018; Lee et al., 2018).
Most representation learning techniques learn the latent embeddings by mapping each data item
(e.g., a single image-caption pair) to a dense vector representing a point in a k-dimensional space,
with no explicit encoding of uncertainty. In the case of word embeddings, a clear drawback is that
such point estimates cannot easily address issues such as polysemy, i.e., when a word such as star can
have two or more different meanings, i.e., movie star and celestial body. A few recent studies have
advocated for density-based or probabilistic word embeddings that capture such nuances of meaning
(Vilnis & McCallum, 2014; Athiwaratkun & Wilson, 2017; 2018; Athiwaratkun et al., 2018). In a
multimodal setting, Mukherjee & Hospedales (2016) show that drawing on density-based word and
image representations improves zero-shot image classification.
Inspired by findings of density-based learning approaches, we ask the question whether joint mul-
timodal representations benefit from such methods in a more general setting. Specifically, we seek
to understand if we can learn higher-quality multimodal representations based on probabilistic em-
beddings of captions and images, as opposed to the standard joint spaces learned over vector-based
representations. We propose to learn Gaussian embeddings of captions and images, by using an
extension of the word learning model of Vilnis & McCallum (2014). From these probabilistic em-
beddings, we learn a common latent space that can capture nuances of meaning with respect to
image-caption correspondences. We evaluate our models by measuring their performance on the
1
Under review as a conference paper at ICLR 2021
standard task of multimodal (image-text or text-image) retrieval, and compare them with a baseline
(vector-based) model. Our results confirm the superiority of the probabilistic representations, both
in terms of overall retrieval performance, and in terms of capturing nuances of meaning when it
comes to choosing the most appropriate response (e.g., a caption) for a given query (e.g., an image).
Our contributions are as follows: To the best of our knowledge, we are the first to propose models
for learning joint multimodal representations from Gaussian embeddings of images and captions.
Through extensive experimentation, we show that non-standard similarity metrics (that have not
been previously used in deep representation learning) can generate high-quality representations.
Additionally, we provide an explicit measure of uncertainty (or entropy), and show that the measure
indeed correlates with the model’s confidence in retrieving the correct match for a given query. Fi-
nally, we provide a qualitative analysis and show that with probabilistic representations the retrieved
images/captions are more aligned with the query in terms of entropy.
2	Model
In this work, we propose models that learn to represent images and textual captions as probability
distributions. These learned distributions are fused to map images and captions into a joint latent
space, enriching the representations and enabling cross-modality comparisons. To train our model
We assume we have pairs of images and corresponding captions, (in,cn) for n ∈ {1, ∙∙∙ ,N} where
N is the total number of pairs in the dataset. A pair (ij, ck) forms a matching pair if j = k anda non-
matching pair if j 6= k. For ease of exposition, we use (i, c) to refer to a matching pair and (i0, c)
and (i, c0 ) to represent a non-matching pair. We use contrastive learning to learn a joint (image
and caption) embedding space in which the elements of a matching pair are closer to each other
compared to those of non-matching pairs. Specifically, we use the hinge-based triplet ranking loss
(Chechik et al., 2010) with semi-hard negative mining(Schroff et al., 2015), defined for a positive
pair as:
L(i, c) = max [α + sim(i, c0) - sim(i, c)]+ + max [α + sim(i0, c) - sim(i, c)]+ ,	(1)
c0	+	i0	+
where [x]+ , max(x, 0), α is a hyperparameter for the margin, and sim(i, c) is a measure of
similarity. The total loss is the sum of L(i, c) for all positive pairs, i.e.:
L = Σ L(i, c).	(2)
(i,c)
We build upon the multimodal representation learning system of Faghri et al. (2018) as it has a sim-
ple architecture that does not rely on object detectors for capturing the image encodings and it has
good performance given its simplicity. Similar to the VSE++ model (Faghri et al., 2018), our model
consists of a pretrained image encoder and a GRU-based caption encoder with learnable parameters
θφ and θψ, respectively. We represent the encoding for image i as φ(i; θφ) ∈ RDφ and the encoding
for caption c as ψ(c; θψ) ∈ RDψ . We use these encodings to learn probabilistic representations for
the images and captions in a D dimensional space. Following Vilnis & McCallum (2014), we as-
sume diagonal multivariate ellipsoidal or spherical Gaussian distributions to model our probabilistic
representations. We train three variations of our model: One with Gaussian caption embeddings and
vector image representations (GCE; Section 2.1), one with Gaussian image embeddings and vector
caption representations (GIE; Section 2.2), and one with Gaussian caption and image embeddings
(GCIE; Section 2.3). Throughout the paper, we use N(μ, Σ) to indicate a Gaussian distribution with
mean μ and covariance matrix Σ. We use diag(a) to refer to a diagonal matrix, where a is a vector
containing the elements on the diagonal.
2.1	Gaussian Caption Embedding (GCE)
In this model, images are represented as vectors i, and captions as Gaussian distributions N(μc, ∑c)
with a per-caption mean μc and a per-caption covariance matrix ∑c, where ∑c = diag(σ2). Image
representations are learned via a linear transformation (fully-connected layer) of the image encoding
φ(i; θφ). To learn the sufficient statistics of the Gaussian caption distributions, we use two caption
2
Under review as a conference paper at ICLR 2021
encoders with no shared parameters, and add fully connected layers to transform the encodings:
i = f(i； Wf, θφ) = Wf。& θφ),	(3)
μc = gμ(c; Wgμ , θψμ ) = Wgμ ψ μ9 θψμ ),	⑷
σc2 = gσ2 (c; Wgσ2, θψσ2) = Wgσ2 ψσ2 (c; θψσ2),	(5)
where Dψμ = Dψ(T2 = Dψ, Wf ∈ RDφ×D, and Wgμ, Wg(T2 ∈ RDψ×d. With no fine-tuning of
the image encoder, the model parameters are: θ , [θψμ; θψ(T2; Wf; Wgμ; Wg(T2]. When We fine-
tune the image encoder, θφ is added to the model parameter vector θ. The model parameters are
learned by minimizing the loss function in equation 2 with the negative of Mahalanobis distance as
the measure of similarity:1
sim(i, C) = - J- - μc)TΣ-1(i - μc).	(6)
2.2	Gaussian Image Embedding (GIE)
In this model, captions are represented as vectors c, and images as Gaussian distributions N(μi, ∑i)
with a per-image mean μi and a per-image covariance matrix ∑i, where ∑i = diag(σ2). Caption
representations are learned via a linear transformation of the caption encoding ψ(c; θψ). To learn the
sufficient statistics of the Gaussian image distributions, we add two fully connected layers (with no
shared parameters) to transform the image encoding φ(i; θφ). Note that unlike for the GCE model,
here we use a single image encoding, but use two independent linear transformations to learn the
parameters of the Gaussians:2
μi = fμ(i; Wfμ , θφ) = Wfμφ(i; θφ),	(7)
σ = fσ2 (i； Wfσ2 , θφ) = Wfσ2 φ(i; θφ),	(8)
C = g(c; Wg, θψ) = Wg ψ(c; θψ),	(9)
where Wμ, fσ2 ∈ RDφ×D, and Wg ∈ RDψ×d. Without fine-tuning the image encoder, the model
parameters are: θ = [θψ; Wμ； fσ2 ； Wg], and when we fine-tune the image encoder, θφ is added to
the model parameter vector θ. The model parameters are learned by minimizing the loss function
in equation 2 with the negative of Mahalanobis distance as the measure of similarity:
sim(i, c)
-ʌ/(C - Ni)T>i I(C - μi).
(10)
2.3	Gaussian Caption and Image Embedding (GCIE)
In this model, both images and captions are represented as Gaussian distributions, N(μi, ∑i) and
N(μc, ∑c), where ∑i = diag(σ2) and ∑c = diag(σ2). We follow the same approach described
in Sections 2.1 and 2.2 to learn the sufficient statistics for the image and caption representations,
respectively:
μi = fμ(i; Wf μ, θφ) = Wfμφ(i θφ),	(11)
σ = fσ2 (i； Wfσ2 , θφ)= Wfσ2φ(i; θφ),	(12)
μc = gμ(c; Wgμ , θΨμ ) = Wgμ ψ ” (C θΨμ ),	(13)
σc2 = gσ2 (c； Wgσ2, θψσ2) = Wgσ2 ψσ2 (c； θψσ2),	(14)
where	Dψμ	=	Dψ,2	=	Dψ,	Wf“，Wf,2	∈	RDφ×D,	and	Wg“，Wg,2
Without fine-tuning the image encoder, for the model parameters we
∈ RDψ×D.
have θ =
[θψμ; θψσ2; Wfμ; Wfb2; Wgμ; Wgb2 ], and when we fine-tune the image encoder, θφ is added to
the model parameter vector θ . We learn the model parameters by minimizing the loss function
in equation 2 with one of the following three similarity functions to estimate sim(i, c):
1The leading negative sign is to turn the distance measure into a measure of similarity. We use Mahalanobis
distance because we need to measure similarity between a point and a probability distribution.
2Unlike the caption encoder parameters that are learned from scratch, for the image encoder we start from a
pretrained network. As such, there is no need to use two such networks (especially with no fine-tuning). With
fine-tuning, using two separate networks as image encoders would require a significant increase in the number
of model parameters.
3
Under review as a conference paper at ICLR 2021
1.	Negative Kullback-Liebler (KL) divergence. This metric is the only asymmetric similar-
ity metric that we use. Since the images contain more details than captions and captions
cannot always verbalize all the details in the images we expect the captions to have higher
entropies and as a result, we take the KL divergence with respect to the captions distribu-
tions, i.e., KL(N(μi, ∑i)||N(μc, ∑c)), With the following closed form:
-2 (trN-12) + (μi -40尸夕-1(乩-μc) - D +ln( det∑c )) ,	(15)
where tr(A) and det(A) are the trace and determinant of matrix A, respectively.
2.	Negative Minimum (KL) divergence. We define this measure as a symmetric variant of
the KL divergence:
-min(KL(N(μi, ∑i)∣∣N(μ0, ∑c)), KL(N(μ°, ∑c)∣∣N(μi, ∑i))),	(16)
where KL(P ||Q) is the KL divergence from probability distribution Q to P.
3.	Negative 2-Wasserstein distance. Wasserstein distance is a symmetric distance metric be-
tween probability distributions. We use the 2nd order Wasserstein (2-Wasserstein) because
it can be evaluated using the following closed form for Gaussians (Mallasto & Feragen,
2017):
-{∖∖μi - μc"2 + tr (ς, + ςc - 2 (ς^ ςcς^ )
(17)
3	Evaluation
3.1	Experimental Setup
Data. For training and evaluation, we use the (Microsoft) MS-COCO dataset (Lin et al., 2014). We
work with the splits from Karpathy & Fei-Fei (2015), containing 82, 783 training, 5000 validation,
and 5000 test images, where each image is associated with five manually-provided captions that
describe the contents of the image. Following Faghri et al. (2018), we add to our training data
an additional set of 30, 504 images (and their captions) that were in the original validation set of
MS-COCO, but have been left out from the Karpathy & Fei-Fei (2015) split.
Training details. We use the VSE++ (Faghri et al., 2018) code3 as our baseline with the modifi-
cations described in Section 2. Specifically, for learning probabilistic image representations in GIE
and GCIE, we replace the last logit layer of the image encoder and add two fully connected layers
to learn the means and the log variances4 of the Gaussian image representations. For learning the
probabilistic caption representations for GCE and GCIE, we duplicate the GRU-based encoder and
use one encoder for estimating the means and the other one for learning the log variances of the
caption representation. For spherical distributions, we take the mean of the log variances over the
embedding dimensions. Following Vilnis & McCallum (2014) we bound all variance values to the
range of [0.1, 10].
We train and test our models following the same settings as in VSE++, i.e., we use pretrained ResNet
152 (He et al., 2016) with Dφ = 2048 as our base image encoder and a GRU-based caption encoder
with Dψ = 1024 an input size of 300, and train our models for 15 epochs with a learning rate of
2e-4, followed by another 15 epochs with a lower learning rate of 2e-5. The dimensionality of
our joint embedding space is D = 1024. When we fine-tune the image encoder in our model, we
start from the trained models with a fixed image encoder and train for 15 epochs at the learning
rate of 2e-5 for all models, except GIE spherical, where we use the learning rate of 2e-6. We use
the hinge-based triplet ranking loss (equation 1) with α = 0.2, with the different similarity metrics
described in Sections 2.1-2.3.
3https://github.com/fartashf/vsepp
4We estimate the log variances instead of variances to ensure numerical stability.
4
Under review as a conference paper at ICLR 2021
Table 1: Recall values of all models (no fine-tuning) on MS-COCO test set; results are averages over
five folds of 1000 images each; best performances are shown in boldface; performances similar to
or better than the baseline VSE++ are shown in italics.
Model	similarity metric	image to text			text to image		
		R@1	R@5	R@10	R@1	R@5	R@10
VSE++	cosine	58.3	86.1	^933-	43.6	77.6	^878-
GCE spherical	Mahalanobis	58.1	86.1	^3.8	43.3	78.0	~883-
GCE ellipsoidal	Mahalanobis	57.3	85.5	93.1	43.9	77.9	88.4
GIE spherical	Mahalanobis	58.0	85.8	92.7	43.6	77.6	88.1
GIE ellipsoidal	Mahalanobis	58.2	86.0	93.5	44.2	78.2	88.5
GCIE spherical	KL	56.4	85.0	92.4	42.0	76.4	87.3
GCIE ellipsoidal	KL	57.8	86.6	94.0	44.8	79.0	89.0
GCIE spherical	min KL	55.9	84.2	92.6	41.8	76.3	87.2
GCIE ellipsoidal	min KL	58.2	85.6	93.1	44.9	78.6	89.0
GCIE spherical	Wasserstein	57.3	85.9	93.2	43.8	77.7	88.1
GCIE ellipsoidal	Wasserstein	59.0	87.0	93.6	45.1	79.2	89.2
Evaluation. As is standard in multimodal representation learning, we report R(ecall)@K (with
K = 1, 5, 10) for both image-to-text and text-to-image retrieval tasks. All reported results are
averages over five folds of 1000 test images each.
To measure the entropy of our representations in the joint embedding space, we use log(detΣi) and
log(detΣc) for images and captions, respectively, since the differential entropy of a multivariate
Gaussian with covariance matrix Σ ∈ Rd×d can be calculated as:
2 (d + dlog(2π) + log(detΣ)).	(18)
This measure can help us understand the uncertainty associated with retrieving an image given a
query caption, or a caption given a query image.
3.2	Quantitative Results
Table 1 reports retrieval results for the original point-based vector representations learned by
VSE++, as well as variations of the density-based representations learned by our models. Results
are reported for test set, with the best configurations of each model identified by looking at the sum
of recall values (for text-to-image and image-to-text) for the validation set. As can be seen, the best
performances (given in boldface) are generally achieved by our ellipsoidal GCIE model, using the
Wasserstein distance. These results especially point to the Wasserstein distance as an appropriate
metric that has been largely overlooked in representation learning. We can see that in many cases,
our models outperform VSE++ (italicized recall values), especially for text-to-image retrieval.
Table 2 shows the retrieval results, with fine-tuning of the image encoding network. Here again we
can see that several of our models outperform VSE++. In particular, the GCIE ellipsoidal model with
the min KL and Wasserstein distances produce the best results. Once again, these results provide
evidence for the effectiveness of probabilistic representations in the multimodal setting, and further
suggest the importance of considering new metrics for representation learning.
In both Tables 1 and 2, we observe that the GCIE ellipsoidal models (with different similarity met-
rics) all have notably higher recall values compared to their spherical counterparts (on both image-
to-text and text-to-image retrieval tasks). We believe this is due to the fact that with ellipsoidal image
distributions, the model can better capture the variations of variances along different dimensions and
for different images.
3.3	Qualitative Results
In Figure 1 we provide four examples where we take crops from random images in the test set. We
truncate the captions for these images to match the crops, and we feed them one pair at a time to both
the GCIE and the VSE++ models to compare the rankings given by the two models. Since GCIE
5
Under review as a conference paper at ICLR 2021
Table 2: Recall values of all models (with fine-tuning) on MS-COCO test set; results are averages
over five folds of 1000 images each; best performances are shown in boldface; performances similar
to or better than the baseline VSE++ are shown in italics.
Model	similarity metric	image to text			text to image		
		R@1	R@5	R@10	R@1	R@5	R@10
VSE++	cosine	64.6	90.0	^957-	52.0	84.3	^9Σ0
GCE spherical	Mahalanobis	63.9	89.4	-95.4-	51.6	84.1	~9∑i
GCE ellipsoidal	Mahalanobis	63.4	89.0	95.5	522	84.2	92.4
GIE spherical	Mahalanobis	54.4	82.7	91.3	40.0	73.7	85.0
GIE ellipsoidal	Mahalanobis	61.8	88.3	95.0	48.7	82.2	91.1
GCIE spherical	KL	61.0	88.9	95.0	49.9	83.3	91.7
GCIE ellipsoidal	KL	64.7	90.8	96.3	51.6	84.0	92.2
GCIE spherical	min KL	60.4	87.7	94.6	49.0	82.7	91.5
GCIE ellipsoidal	min KL	66.4	91.3	96.5	52.5	84.6	92.7
GCIE spherical	Wasserstein	61.8	88.8	94.7	50.9	83.7	92.1
GCIE ellipsoidal	Wasserstein	66.9	90.6	962	52.4	84.6	92.7
GCIE
VSE+ +
GCIE
VSE+ +
Query
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
Query
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A cinderblock wall.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A swimming pool.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A cinderblock wall.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A swimming pool.
GCIE
VSE+ +
GCIE	VSE++
Query
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A dog sitting.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
Query
A man with a red helmet on a
small moped on a dirt road.
A man with a red helmet on a
small moped on a dirt road.
A man with a red
helmet.
A man with a red helmet on a
small moped on a dirt road.
A swimming pool in a yard that
has a cinderblock wall all the
way around it and a dog sitting
at the edge of the pool.
A dog sitting.
A man with a red helmet on a
small moped on a dirt road.
A man with a red
helmet.
Figure 1: Retrieval results from our model and VSE++ on image crops and associated caption
truncations on random images from the test set.
6
Under review as a conference paper at ICLR 2021
Table 3: Image/caption pairs with their crops/truncations and their associated measure of entropy.
caption	image	measure of entropy	
		caption	image
			
A man with a red helmet on a	I	、	18.19	26.14
small moped on a dirt road.			
A man with a red helmet.		15.51	19.05
	■UJ^B		
A swimming pool in a yard that			
has a cinderblock wall all the	■ ■		
way around it and a dog sitting		19.20	21.51
at the edge of the pool.	I -承 E I		
			
			
			
A swimming pool.	I `	10.52	12.39
	■ ■		
A cinderblock wall.	I Λ	15.48	14.66
			
	ΓI		
A dog sitting.	F I	14.28	20.65
	Q		
can capture the level of details through the covariance matrices of the Gaussian representations, we
expect its retrieval results to be better than VSE++. The results confirm that this is indeed the case.
For instance, in response to an image depicting a pool, a brickwall, a tree, and a dog, both GCIE
and VSE++ provide the long detailed caption as the match. But, with the cropped image that depicts
the brickwall only, GCIE (but not VSE++) is returning the short caption that is a more appropriate
match. The same pattern is observed for all four sets of examples in the figure.
Recall that our uncertainty measure in equation 18 estimates the uncertainty or entropy associated
with retrieving an image given a query caption (or vice versa). To better depict the relationship
between the image crops and the caption truncations in Figure 1, we provide the measure of entropy
for them in Table 3. Interestingly, this measure can capture how uncertain the GCIE model is for
retrieving the matching caption or image for a given query. Specifically, we can see for a cropped
image when we have a more focused subject in the image, the entropy is lower. Similarly for
captions, when we have a truncation the model is more certain in grounding the shorter caption in
the image and retrieving the matching image.
7
Under review as a conference paper at ICLR 2021
3.4	Analysis of Learned Probabilistic Representations
In our qualitative analysis in Section 3.3, we observe that our probabilistic representations are bet-
ter at capturing nuances of meaning, compared to those learned by a model such as VSE++. In
this section, we perform similar experiments using more realistic data on a larger scale, and report
performance of our best model (GCIE ellipsoidal with Wasserstein as similarity metric) vs. VSE++.
In popular image retrieval datasets, captions vary in quality and degree of specificity. Often times
captions describe only a particular (salient) part of an image, and leave other parts of the image
unmentioned. We believe a probabilistic representation can better model the level of specificity in
the visual and linguistic aspects of the data. We demonstrate this by testing our model’s ability in
differentiating between parts of an image versus the whole, given a more or less specific query. To
this end, we use the Visual Genome dataset (Krishna et al., 2016), which comes with annotated pairs
of image regions and descriptions (mostly phrases). We remove the portion of Visual Genomen
images that overlap with those in MS-COCO to avoid any unwanted information leak (since our
model has been trained on MS-COCO). We randomly pick a set of images from the remaining
Visual Genome data, and for each image select M = 3 largest crops that cover an area no bigger
than 20% of the image — we set this area constraint so that there are meaningful differences between
the crop and the image. We try to choose crops with minimal overlap, but still observe some overlap
between the selected crops. Note that the image regions (crops) come with their own linguistic
annotation/description (usually in the form of a phrase), but the whole image does not have an
associated caption in Visual Genome. As such, we form a description for each of our selected
images by concatenating the phrases of its three selected crops.
We test our model in two conditions: (a) image retrieval, where we ask both our model (GCIE-best)
and VSE++ to retrieve one of two images (a full image and one of its selected crops) in response
to a text query that either describes the full image or the selected crop; (b) caption retrieval, where
we ask the two models to retrieve one of two captions in response to an image. For image retrieval,
out of a sample of 2, 850 test items, when given the crop description as query, GCIE-best retrieves
the correct response 61% of the time, vs. 58% accuracy for VSE++. When given the full-image
description as query, GCIE-best correctly retrieves the original image at an accuracy of 77%, while
VSE++ is correct only 69% of the time. When performing caption retrieval for the same 2, 850 test
items, VSE++ shows a strong bias towards shorter captions, correctly retrieving the crop description
(a short phrase) when given the crop as query 97% of the time, compared to 82% accuracy for
our model. But, when the full images are given as query, VSE++ has a very low accuracy of 13%
while our model returns the correct description 53% of the time. These results further suggest that a
probabilistic representation is better suited for capturing nuances of meaning.
4	Conclusions
In this paper, we proposed new models that learn image and text representations as probability dis-
tributions. We showed that by fusing such density-based representations we can learn higher-quality
joint embeddings that can result in better multimodal retrieval performance. Through extensive eval-
uation of our models, we observed that the best model is one where both images and captions are
represented as probability distributions, and that symmetric similarity metrics with bounded values
are the most effective. Additionally, our qualitative analysis reveals an interesting behaviour of such
a probabilistic model: Unlike a vector-based model, ours is sensitive to nuances of meaning, and is
better at finding the most appropriate match (e.g., an image) with the right level of detail, given a
query (e.g., a caption).
References
Ben Athiwaratkun and Andrew Gordon Wilson. Multimodal Word Distributions. In Annual Meeting
of the Association for Computational Linguistics, 2017.
Ben Athiwaratkun and Andrew Gordon Wilson. Hierarchical Density Order Embeddings. In Inter-
national Conference on Learning Representations, 2018.
8
Under review as a conference paper at ICLR 2021
Ben Athiwaratkun, Andrew Gordon Wilson, and Anima Anandkumar. Probabilistic FastText for
Multi-Sense Word Embeddings. In Annual Meeting of the Association for Computational Lin-
guistics, 2018.
G. Chechik, V. Sharma, U. Shalit, and S. Bengio. Large scale online learning of image similarity
through ranking. Machine Learning Research, 11:1109-1135, 2010.
Fartash Faghri, David J. Fleet, Jamie Ryan Kiros, and Sanja Fidler. VSE++: Improving Visual-
Semantic Embeddings with Hard Negatives. In British Machine Vision Conference, 2018.
David Harwath, Adria Recasens, Didac Suris, Galen Chuang, Antonio Torralba, and James Glass.
Jointly discovering visual objects and spoken words from raw sensory input. In European Con-
ference on Computer Vision, 2018.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Conference
on Computer Vision and Pattern Recognition, pp. 770-778, 2016.
A. Karpathy and L. Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In
Conference on Computer Vision and Pattern Recognition, pp. 3128-3137, 2015.
Ryan Kiros, Ruslan Salakhutdinov, and Richard S. Zemel. Unifying visual-semantic embeddings
with multimodal neural language models, 2014.
Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie
Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual
genome: Connecting language and vision using crowdsourced dense image annotations. 2016.
URL https://arxiv.org/abs/1602.07332.
Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked Cross Attention for
Image-Text Matching. In European Conference on Computer Vision, pp. 201-216, 2018.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dollar, and C. Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In David Fleet,
Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars (eds.), European Conference on Computer
Vision, pp. 740-755. Springer International Publishing, 2014.
Y. Liu, S. Albanie, A. Nagrani, and A. Zisserman. Use what you have: Video retrieval using repre-
sentations from collaborative experts. In arXiv preprint arxiv:1907.13487, 2019.
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining task-agnostic visi-
olinguistic representations for vision-and-language tasks. In Conference on Neural Information
Processing Systems, 2019.
Anton Mallasto and Aasa Feragen. Learning from uncertain curves: The 2-Wasserstein metric for
Gaussian processes. In Conference on Neural Information Processing Systems, pp. 5660-5670,
2017.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed represen-
tations of words and phrases and their compositionality. In Conference on Neural Information
Processing Systems, 2013.
Tanmoy Mukherjee and Timothy Hospedales. Gaussian visual-linguistic embedding for zero-shot
recognition. In Empirical Methods in Natural Language Processing, pp. 912-918, 2016.
Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face
recognition and clustering. In Conference on Computer Vision and Pattern Recognition, pp. 815-
823, 2015.
Richard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and Andrew Y. Ng.
Grounded compositional semantics for finding and describing images with sentences. Trans-
actions of the Association for Computational Linguistics, 2, 2014.
Evgeniya Ustinova and Victor Lempitsky. Learning deep embeddings with histogram loss. In
Conference on Neural Information Processing Systems, 2016.
9
Under review as a conference paper at ICLR 2021
Ivan Vendrov, Ryan Kiros, Sanja Fidler, and Raquel Urtasun. Order-Embeddings of Images and
Language. In ICLR, 2016.
Luke Vilnis and Andrew McCallum. Word Representations via Gaussian Embedding. In ICLR,
2014.
Eloi Zablocki, Benjamin PiWoWarski, Laure Soulier, and Patrick Gallinari. Learning Multi-Modal
Word Representation Grounded in Visual Context. In Association for the Advancement of Artifi-
cial Intelligence, 2018.
10