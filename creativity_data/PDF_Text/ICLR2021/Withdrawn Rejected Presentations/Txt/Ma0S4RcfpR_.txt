Under review as a conference paper at ICLR 2021
A Representational Model of Grid Cells’ Path
Integration Based on Matrix Lie Algebras
Anonymous authors
Paper under double-blind review
Ab stract
The grid cells in the mammalian medial entorhinal cortex exhibit striking hexagon
firing patterns when the agent navigates in the open field. It is hypothesized that
the grid cells are involved in path integration so that the agent is aware of its self-
position by accumulating its self-motion. Assuming the grid cells form a vector
representation of self-position, we elucidate a minimally simple recurrent model for
grid cells’ path integration based on two coupled matrix Lie algebras that underlie
two coupled rotation systems that mirror the agent’s self-motion: (1) When the
agent moves along a certain direction, the vector is rotated by a generator matrix.
(2) When the agent changes direction, the generator matrix is rotated by another
generator matrix. Our experiments show that our model learns hexagonal grid
response patterns that resemble the firing patterns observed from the grid cells in
the brain. Furthermore, the learned model is capable of near exact path integration,
and it is also capable of error correction. Our model is novel and simple, with
explicit geometric and algebraic structures.
1	Introduction
Imagine walking in the darkness. Purely based on your sense of self-motion, you can gain a sense of
self-position by integrating the self movement - a process often referred to as path integration (Darwin,
1873; Etienne & Jeffery, 2004; Hafting et al., 2005; Fiete et al., 2008; McNaughton et al., 2006).
While the exact neural underpinning of path integration remains unclear, it has been hypothesized
that the grid cells (Hafting et al., 2005; Fyhn et al., 2008; Yartsev et al., 2011; Killian et al., 2012;
Jacobs et al., 2013; Doeller et al., 2010) in the mammalian medial entorhinal cortex (mEC) may be
involved in this process (Gil et al., 2018; Ridler et al., 2019; Horner et al., 2016). The grid cells are so
named because individual neurons exhibit striking firing patterns that form hexagonal grid patterns
when the agent (such as a rat) navigates in a 2D open field (Fyhn et al., 2004; Hafting et al., 2005;
Fuhs & Touretzky, 2006; Burak & Fiete, 2009; Sreenivasan & Fiete, 2011; Blair et al., 2007; Couey
et al., 2013; de Almeida et al., 2009; Pastoll et al., 2013; Agmon & Burak, 2020). The grid cells
interact with the place cells in the hippocampus (O’Keefe, 1979). Unlike a grid cell that fires at the
vertices of a lattice, a place cell often fires at a single or a few locations.
The purpose of this paper is to understand how the grid cells may perform path integration (or
“path integration calculations”). We propose a representational model in which the self-position is
represented by the population activity vector formed by grid cells, and the self-motion is represented
by the rotation of this vector. Specifically, our model consists of two coupled systems: (1) When
the agent moves along a certain direction, the vector is rotated by a generator matrix of a Lie
algebra. (2) When the agent changes movement direction, the generator matrix itself is rotated by yet
another generator matrix of a different Lie algebra. Our numerical experiments demonstrate that our
model learns hexagon grid patterns which share many properties of the grid cells in the rodent brain.
Furthermore, the learned model is capable of near exact path integration, and it is also capable of
error correction.
Our model is novel and simple, with explicit geometric and algebraic structures. The population
activity vector formed by the grid cells rotates in the “mental” or neural space, monitoring the
egocentric self-motion of the agent in the physical space. This model also connects naturally to the
basis expansion model that decomposes the response maps of place cells as linear expansions of
response maps of grid cells (Dordek et al., 2016; Sorscher et al., 2019). Overall, our model provides a
1
Under review as a conference paper at ICLR 2021
new conceptual framework to study the grid cell systems in the brain by considering the structure of
the intrinsic symmetry (through Lie algebra) of the task which the path integration system is solving.
2	Representational model for path integration
Consider an agent navigating within a squared domain (theoretically the domain can be R2). Let
x = (x1, x2) be the self-position of the agent in a 2D environment. At self-position x, if the agent
makes a displacement δr along the direction θ ∈ [0, 2π], then the self-position is changed to x + δx,
where δx = (δx1, δx2) = (δr cos θ, δr sin θ). In our model, we use a polar coordinate system
(see figure 1a,b) by directly using (θ, δr), while only keeping (δx1, δx2) implicit. (θ, δr) is the
biologically plausible egocentric representation of self-motion.
We assume that the location x in the 2D environment is encoded by the response pattern of a
population of d neurons (e.g., d = 200), which correspond to a d-dimensional vector v (x) =
(vi(x), i = 1, ..., d)>, with each element representing the firing rate of one neuron when the animal
is at location x. From the embedding point of view, essentially we embed the 2D domain in R2 as a
2D manifold in a higher dimensional space Rd. Locally We embed the 2D local polar system centered
at X (see figure 1a,b) into Rd so that it becomes a local system around v(x) (see figure 1c).
(a)
(b)
器>n(∕ + δx)
yB(Θ)δr×
(c)
Figure 1: Illustration of the proposed representational model. (a) 2D local polar system centered at X for
egocentric self-motion, to be embedded in Rd. (b) 2D local displacement δr and local change of direction δθ. (c)
Mirroring relations in (b). X is mirrored by v(x). Local displacement δr from X along direction θ is mirrored
by B(θ)δr applied to v(x). Local change of direction δθ is mirrored by Cδθ applied to B(θ).
2.1	The proposed representational model: coupling two rotation systems
Assuming δr to be infinitesimal, We propose the folloWing model
v(x + δx) = (I + B(θ)δr)v(x) + o(δr),	(1)
Which parameterizes a recurrent neural netWork (Hochreiter & Schmidhuber, 1997), Where I is the
identity matrix, and B(θ) is a d-dimensional matrix depending on the direction θ, Which Will need to
be learned.
Rotation. We assume B(θ) = -B(θ)>, i.e., skeW-symmetric, so that I + B(θ)δr is a rotation
or orthogonal matrix, due to that (I + B(θ)δr)(I + B(θ)δr)> = I + O(δr2). Because the upper
triangle part of B(θ) is the negative of the transpose of the loWer triangle part (the diagonal elements
are zeros), in fact We only need to learn its loWer triangle part. The geometric interpretation is that,
if the agent moves along the direction θ, the vector v(x) is rotated by the matrix B(θ), While the
`2 norm kv(x)k2 remains stable (figure 1c). We may interpret kv(x)k2 = Pid=1 vi(x)2 as the total
energy of grid cells, Which is stable across different locations. From embedding point of vieW, the
local polar system in figure 1a is embedded into a d-dimensional sphere in neural response space.
When the agent makes an infinitesimal change of direction from θ to θ + δθ, B(θ) is changed to
B(θ + δθ). We assume
B(θ + δθ) = (I + Cδθ)B(θ) + o(δθ),	(2)
Where C is a d-dimensional matrix, Which is also to be learned. We again assumes C = -C>, so
that I + Cδθ is a rotation matrix. The geometric interpretation is that if the agent changes direction,
B(θ) is rotated by C.
Equations (1) and (2) together define our proposed model for path integration, Which couples tWo
rotation systems.
2
Under review as a conference paper at ICLR 2021
2.2	A mirror of egocentric motion: preserving local geometric relations
As a representational model, equations (1) and (2) form a mirror in the d-dimensional “mental”
(or neural) space for the egocentric motion in the 2D physical space. Importantly, the embedding
preserves the local geometric relations of the local polar system.
Let δθv(x) = v(x + δx) - v(x) be the displacement of v(x) when the agent moves from x by δr
along direction θ. It follows from equation (1) that δθv(x) = (B(θ)δr + o(δr))v(x). Ignoring high
order terms, we obtain δθ+δθv(x) = (I + Cδθ)δθv(x). That is, with δr fixed, the local change of
v(x) along different θ are rotated versions of each other, mirroring the local polar system at x. See
figure 1 for an illustration. As for the angle between v(x) and v(x + δx), i.e., how much the vector
v rotates in the neural space as the agent moves in 2D physical space by δr, we have
Proposition 1 In the above notation, let δα be the angle between v(x) and v(x + δx), we have
δα = βδr + O(δr2), where δr = ∣∣δxk, and β = ∣∣B(θ)v(x)k2∕kv(x)k2 is independent of θ.
See Supplementary A.1 for a proof. That means, the angle δα in the d-dimensional neural space
is proportional to the Euclidean distance δr in the 2D space, and more importantly the angle δα is
independent of direction θ, i.e., β is isotropic.
2.3	Hexagon grid patterns
For the learned model, β can be much bigger than 1, so that the vector v(x) will rotate back to
itself in a short distance, causing the periodic patterns of v(x). Moreover, β does not depend on
the direction of self-motion, and this isotropic property appears to underly the emergent hexagonal
periodic patterns, as suggested by the following result.
The hexagon grid patterns can be created by linearly mixing three Fourier plane waves whose
directions are 2∏∕3 apart. In the following, We state a theoretical result adapted from Gao et al.
(2018) that connects such linearly mixed Fourier waves to the geometric property in Proposition 1 in
the previous subsection.
Proposition 2 Let e(x) = (exp(ihaj, xi), j = 1, 2, 3)>, where (aj, j = 1, 2, 3) are three 2D
vectors of equal norm, and the angle between every pair ofthem is 2π∕3. Let v(x) = Ue(X), where
U is an arbitrary unitary matrix, i.e., UP = I. Let δα be the angle between v(x) and v(x + δx),
we have δα = βδr + O(δr2), where δr = ∣∣δx∣, and β H ∣∣aj ∣ is independent of the direction of
δx.
See Supplementary A.1 for a proof, which relies on the fact that (aj,j = 1, 2, 3) forms a tight frame
in 2D. Proposition 2 says that the geometric property that emerges from our model as elucidated by
Proposition 1 is satisfied by the orthogonal mixing of three Fourier plane waves that creates hexagonal
grid patterns. We are currently pursuing a more general analysis of our model, i.e., equations (1) and
(2).
2.4	Justification as minimally simple recurrent model
Now we justify equation 1 as a minimally simple recurrent model. To start, the general form of the
model is v(x + δx) = F (v(x), δr, θ). For infinitesimal δr, a first-order Taylor expansion gives
v(x + δx) = v(x) + f(v(x), θ)δr + o(δr),	(3)
where the function F(v, δ, θ) satisfies F(v, 0, θ) = v, i.e., the vector representation stays the same if
there is no self-displacement, and f(v, θ) = ∂F(v,δ,θ) Iδ=0 , i.e., the first derivative at δ = 0.
The function f(v, θ) transforms v to another vector of the same dimension and the transformation
depends on θ. A minimally simple model is a linear transformation that depends on θ, i.e., we can
assume v(x + δx) = v(x) + B(θ)v(x)δr + o(δr), which leads to equation 1, where the linear
transformation is B(θ). Equation 2 can be similarly justified.
In this paper, we assume a linear recurrent model for its simplicity and its explicit geometric meaning
as rotation. It is important to emphasize that our arguments are not mutually exclusive with the
work based on non-linear recurrent neural network model (Burak & Fiete, 2009; Couey et al., 2013).
3
Under review as a conference paper at ICLR 2021
In fact, our linear rotation model may serve as a prototype approximation which may help better
understand these nonlinear models - a direction we did not pursue here.
2.5	Matrix Lie algebras and groups: from infinitesimal to finite
For a finite, non-infinitesimal, self-displacement ∆r, we can divide ∆r into N steps, so that δr =
△r/N → 0 as N → ∞, and
v(x + ∆x) = (I + B(θ)(∆r∕N) + o(1∕N))nv(x) → exp(B(θ)∆r)v(x).	(4)
The above math underlies the relationship between matrix Lie algebra and matrix Lie group (Taylor,
2002). For a fixed θ, the set of Mg(∆r) = exp(B(θ∆r)) for ∆r ∈ R forms a matrix Lie group,
which is both a group and a manifold. The tangent space of Mg(∆r) at identity I is called matrix
Lie algebra. B(θ) is the basis of this tangent space, and is often referred to as the generator matrix.
Similarly for a finite change of direction ∆θ, we obtain
B(θ + ∆θ) = exp(C ∆θ))B(θ).	(5)
The set of R(θ) = exp(C∆θ) for all θ ∈ [0,2π] (with mod 2π addition arithmetics) forms another
matrix Lie group, with C being the generator matrix of its matrix Lie algebra.
Approximation to exponential map. For a finite but small ∆r, exp(B(θ)∆r) can be approximated
by a second-order Taylor expansion
exp(B(θ)∆r) = I + B(θ)∆r + B(θ)2∆r2∕2 + o(∆r2).	(6)
Similarly, exp(C∆θ) can be approximated by exp(C∆θ) = I + C∆θ + C2∆θ2∕2 + o(∆θ2).
Path integration. Now we can cast path integration in the language of Lie group. Specifically, the
input includes the initial position x(0), and the self-motions (θ(t), △r(t)) for t = 1, ..., T. Initializing
v(0) = v(x(0)), the vector is updated recurrently according to
V⑶=exp(B(θ⑶)∆r⑴)v(t-1).	(7)
That is, the vector v(t) is rotated by B(θ(t)) according to △r(t) geometrically.
Modules. Experimentally, itis well established that grid cells are organized in discrete modules (Barry
et al., 2007; Stensola et al., 2012) or blocks. We thus partition the vector v(x) into K blocks, v(x) =
(vk(x), k = 1, ..., K). Correspondingly the generator matrices B(θ) = diag(Bk(θ), k = 1, ..., K)
and C = diag(Ck , k = 1, ..., K ) are block diagonal. This greatly reduces the number of parameters
to be learned. Note that each sub-vector vk (x) is rotated by a sub-matrix Bk (θ), which is in turn
rotated by Ck .
Metric. By the same argument as in Proposition 1, for a module k, let vk be the sub-vector. Let δαk
be the angle between vk(x) and vk(x + δx), then the angle δαk = βkδr, where δr = kδxk, and βk
is independent of θ. That is, if the agent moves by δr, the vector vk (x) rotates by an angle βkδr. βk
determines the metric or scale of the response maps of the k-th block of grid cells, i.e., it tells us how
fast the sub-vector vk rotates as the agent moves.
3 Integration with basis expansion model
For each v(x), we need to uniquely decode x. We thus need to integrate the path integration model
with the basis expansion model that connects grid cells to place cells. Each place cell fires when
the agent is at a specific position. Let Ax0 (x) be the response map for the place cell associated with
position x0. It measures the adjacency between x and x0. A commonly used form of Ax0 (x) when the
agent navigates in the open field is the Gaussian adjacency kernel Ax0 (x) = exp(-kx-x0k2∕(2σ2)).
3.1	Basis expansion
A popular model that connects place cells and grid cells is the following basis expansion model (or
PCA-based model) (Dordek et al., 2016):
d
Ax0 (x) =	ui,x0 vi (x) = hv(x), u(x )i,	(8)
i=1
4
Under review as a conference paper at ICLR 2021
where v(x) = (Vi(X),i = 1,…,d)>, and u(x0) = (ui,χo, i =
1,…,d)>. Here (Vi(x), i = 1,…,d) forms a set of d basis functions
for expanding Aχ0 (x) for all places x0, while u(x0) is the read-out
weight vector for place cell x0, and needs to be learned.
Experimental results have shown that the connections from grid
cells to place cells are excitatory (Zhang et al., 2013; Rowland et al.,
2018). We thus assume that uix ≥ 0 for all i and x0. We can also
make v(x) to be non-negative by adding a bias term. Please see
Supplementary A.4 for details.
3.2	Decoding, re-encoding, and error correction
For a neural response vector v, such as v(t) in equation (7), the
response of the place cell centered at location x0 is hv, u(x0)i. We
can decode the position X by examining which place cell has the
maximal response, i.e.,
X = argmax(v, u(x0)).
x0
Figure 2: Illustration of ba-
sis expansion model Aχθ(x)=
Pd=I Ui,χθVi(x), where Vi(x) is
the response map of i-th grid cell,
shown at the bottom, which shows
5 different i. Aχ0 (x) is the re-
sponse map of place cell associ-
ated with x0, shown at the top,
which shows 3 different x0. ui,x0
is the connection weight.
(9)
After decoding X, we can re-encode V J v(X), which amounts to projecting V onto the 2D manifold
formed by v(X) for all X. The set of v(X) forms a codebook, and the projection via re-encoding
enables error correction by removing the possible errors or noises in V (see Supplementary A.2).
3.3	Unitary representation and harmonic analysis
Underlying the integration of our proposed path integration model (equation 1) and (equation 2) and
the basis expansion model (i.e., equation 8) is the group representation theory.
our path integration model leads to unitary group representation. Let Mθ(∆r) = exp(B(θ)∆r) with
finite (non-infinitesimal) ∆r, let X = (∆r cos θ, ∆r sin θ), and M(X) = Mθ (∆r). For each given
X, M(X) is an orthogonal matrix. collectively, M(X) forms a unitary representation of X ∈ R2, i.e.,
2D Euclidean group, where the additive group action in R2 is represented by matrix multiplication.
For each element of the matrix, Mij (X) is a function of X. According to the fundamental theorems
of Schur (Zee, 2016) and Peter-Weyl (Taylor, 2002), if M is an irreducible representation of a finite
group or compact Lie group, then {Mij (X)} form a set of orthogonal basis functions of X. This leads
to a deep generalization of harmonic analysis or Fourier analysis. Let V(X) = M (X)V(0) (where we
choose the origin 0 as the reference point). The elements of V(X), i.e., (Vi(X), i = 1, ..., d), are also
basis functions of X. These basis functions serve to expand (Ax0 (X), ∀X0) that parametrizes the place
cells, and these basis functions are generated by the matrix Lie algebras of our path integration model.
Thus group representation provides a unifying theoretical framework for the two hypothesized roles
of grid cells, namely path integration and basis expansion.
in our work, we do not assume each block matrix Mk (X) to be irreducible. Thus each learned Vi (X)
within a block is a linear mixing of orthogonal basis functions in an irreducible representation, and
different Vi(X) within the same block are not necessarily orthogonal. However, different Vi(X) in
different blocks are close to orthogonal in our experiments (see Supplementary A.3 for details). 4
4 Learning
The unknown parameters are (1) (V(X), ∀X). (2) (u(X0), ∀X0). (3) (B(θ), ∀θ). (4) C. To learn these
parameters, we define a loss function: L = L0 + λ1 L1 + λ2L2, where
L0	=	Ex,x0 [Ax0 (X) - hV(X), u(X0)i]2,	(10)
L1	=	Ex,∆xkV(X + ∆X) - exp(B(θ)∆r)V(X)k2,	(11)
L2	=	Eθ,∆θkB(θ + ∆θ) - exp(C∆θ)B(θ)k2,	(12)
where k ∙ k2 denotes the sum of squares of the elements of the vector or matrix. λι and λ? are
chosen so that the three loss terms are of similar magnitudes. in L0, Ax0 (X) are given as Gaussian
adjacency kernels, and we aim to learn the basis functions V(X). L1 and L2 serve to constrain
5
.suoɔ p⅛Jo (6sz TIE jowɔSJOS -9I0cl∙IE JoVPJOα) SlOPOUl U.2SUEdXOS-SEq Sno>oJd
.sPOUmSSE SE (PUnoxmS 口0汇 qN∙sPUE Euooh030XoPOOUEIEq≡IM) UEjEd JEWEOIXOn E UE≡
WJ 巴 CədEqS UEESnED E EqNXO OJS、&F Plgy oɔsd IEnP>IP∙SOUmSSE əʌv CSUO=EAJωsqo WJUωyωcb^
əip W-M JUəjs-suoɔ CjPW Munou IfUOM OS 展 S 二ɪ .∞J sə1pɔs JO əbjɔupj B UPdS S=əɔ p⅛OW JO SdBUl
OSUOdS 巴 PolnEoI O≡UEUIOP JoynOH OW.sSgOUOnbO.IJ JO 弘UEJ OlOqM E su-sjuoɔ (CC) J OSnBOOH
•6 JUOJOJJ-P JOJ (6)g^ Mupɪ-sɪlsuoɔ Aq 口-xoIdUlθɔ 10POUl M.sɔnpoj』OJ JUBEOdUŋ OSlE £
Jl ∙SJUωu!yωcb≈』no OJ M.spjoɔɔp SiuOJJEd p⅛UoMEXOq JO əɔuə^əuɪə ωq 二 OJ JUEEOdUI-S 二 PUE CJOqJO
ippə JO Suoisjoa Pojejoj ωq OJsnsu-sɪlsuoɔ N7 .(S.V AmUOUIOIddnSoəs) IEUOMOqEO ωq OJ puəj
sɔpoIq juəjəjep Ulo.IJ (CC) 2。qMnoqJIP ClEUoMoqjJo JoU ω⅛ 上OOIq əuws əɪp Uμp-Λ∖ (CC) 2。pəɪnEωl ətp CJOEJ
Ul ∙Z7 PUE17 SUUOJ SSOI OqJ £A8,PUE (I) IOPOUI uo=E⅛,ωJ∙s⅛Ed .mo Aq Ulətp UWJSU oɔ əʌv CPEOJSUI
.WJO eɔpə OjIBUOMOqEO ωq oj (P r: Cl = 3C(CC)B) = (CC)Q suopɔunj S-SBq ωw.spbsuoɔ Jou OP əʌv
C ((91。Z TIE 记 上ωPJ0α) ppoui POSEqISd Jo) Iopolu UOISUEdXO ESEq MU-spəɪ Uo ɔpoʌv Sno-Aəjdω3ΠIU∩
ohuμπEoI JOJ soɪduɪES OHEUOJUOn Mu号ouoM Jo SUBjOP JOJ I∙PQT⅛uoUIoIddnsOOS ∙(icv N
PUB (飞a JO S⅛.UIBS OPBɔ OJUOn Kq PwIU-XOJddB ω⅛ SUo 君 ɔədXə ωw OWM GIOZ dPQʤ
EulM.s⅛JωzμLφdo IPy>?IE ɔu-ɔəds CJUəɔsəp JugpE⅛O=SEqOOJS Aq UO=OUnJ SSoIωqj əzpuɑpu OM
∙∞sz -E SPU9MOX -SIWIE S MUEqZ) AoJE=Oxo
ω⅛ S=əɔ əɔsd OJ S=əɔ PyM 日 OjJ suo-jɔəuuoɔ ωw JBW OSOAI(飞)^ɪ OUmSSB JgqEnJ PUB "一(、8)^一
UO 口展 uωd E PPEω安d.2JBZyEInW』0H⅛IOZ C.IE Jω90suωJS) SUOnBAJωsqo 展 JUωUIyωdxωωw WW
JUəjsɪsuoɔɑʃnpouɪ B OJbjɔ,spuodsəɪɪoɔ ^ɔoIq qopo q=M 展 UO.^,PqOOIq ωq OJ。PUBsg^ əumssp
WEnJ OM ∙POIΠE± Oq OJ POOU soobEUI ω≡ JO SEEd oτnUEy二 OMOI ω≡⅞,0 JEW OS gsuIUIAS—M，S
ωq OJPəumssp WOq ω⅛。PUBsg^ CSUlojSKS UonBJOJ pə-dnoɔ OM 二 OJ səɔbpuɪ JOw əuəbjɔOW SV
∙(6∙≡s Av C^soɔ A<H CCV 二 7 Ul⅛UOHEnbo) PUE (I UOnEnbO)
IOPOUI POSOdOJd mo UOPOspq pəimojjəd ωq upɔ UOnP⅛ωj∙sqi1JPW OS (H)Q suo-jɔunj S-SPq əip
IZoZ XqUI JE JodEdOOUOJOJU0。E SE Mo3o=opu∩
9
ωq 二 OJ ZmhBJUOUIOIddnSoəs .SdOOIq JO Jωqumu JOU əz-s dɔoIq ɔɪnɔods UO Aləj JOU səop SUEJEd
UOMEXOq JO ωosM⅛sωOWKIqBJON ∙SJJ≡S IWjEdS JO sosBqd JUO&JIP q=M JnqOUIESOW Al⅛noj ω⅛
SUOnBJUωyo pup sω展 ɔsωwωInpou!」0 dɔoIq qopωU≡JIM ・(CC)Tl PUE (H)Q WOq JOJ ωM⅛sω SUEjEd
PI⅛UOMPXOq WnMəX . OInPOUI JO dɔoIqωuws ωw OJ 8u∙5nUOIωq SjPm pəttKωl ωw SMOqS MOjbωAw
.Cc Joωo∙≡EI 0∞ × 0∞ωq 二ωAO (Pr: d = 3 C(CC)20) = (CC)Q UJωJJEd Muyy pωUJEωIωw SAVOqS S 区一一招工
(.3=Enb ⅛≡ JOJ Ul ɪɪɪooz) .əɪnpouɪ jo XOOIq OWES ətp.≡q∙-sM
SUOJnəu ətp JO SUEjEdbβuyy ətp SM.OqS m.oj ajəawuuəuiuojtuə α<NUqJ.S(0 JO juəuɪə一ə əuoω∙D UOJnUU əuo
JO ujəjjEdbβ'uyy ətp SAVOqS deuɪ əsuodsəj jjəaw.光 OΛMωu PUUJEaI UqJ UI əbjɔjəuɪə SUJəjjEdbβuyy pyφG OJnMIH
殿菊≡ħ辍灌糙禺≡s≡≡鼠遨≡ħ耦≡≡强搦相精慈越H避藤潮磐醵酸球
≡≡≡爨嬲爨B≡a≡≡爨≡≡鬻鬻爨S爨≡≡≡≡≡爨≡≡鬻翳舞SK
鑫籁娥窿舞≡H≡≡麋藤≡≡≡≡懒健≡爨H≡翳翻艘舞≡≡≡≡畿犍酸
医隔Sg诬翁归目虫眼法欣法注司遏系糅定我安塞菽至关重法符窿0用为冷
鹿猿第寤绫粽猿镁皴舞遨瓣藤邈皴缴褊蠹谶邃骏翌跳≡≡鬻媛疑镶腹整整
落勃跋逝助吸潴吸琏卷般像期矮菊黑猫般契黑混感圜箱涅凄密缱送辗辗外
SNXwɪɪvd Qso NoDVXWH I ∙g
R∙PQA』BJUOUlOIddnS00S) SUoɔ JO Joqumuoq二 O soɪnpouɪ JO Joqumuo≡JOoɔ-ðlp
ɔsɔəds əip OJjSnqoj ω⅛ SJInSOJ ⅛o .snoɔ Zg SPq qυ≡M JO ippə CsOInPOUI 9 = M 0J.Spωu∙2∙≡JBd
S" lp≡M CSUO-SUəuɪ-p Z61 U PJOE (H)Q Noou b W-M ɪoujəd UB-SSnBOBωsnωM01 UOHBnbə.S
。7.S(H)、&F -əujəd K ɔuəɔprpp ωwjoH .sωω⅛ωp g∙ZI UBq 二 O=PUIS S; VPUB əɔ-spɪ ωw UO SP⅛∞
UPq 二 OUPUIS S二 V C.ə." CSaUPJ 展 ɔoɪ OJPə,spɪlsuoɔ ω⅛ f VPUB AV U.2JOullPɔoɪ ωql⅛v⅛dx9
PUB (AVS巴 dx9 SdBUl -uəuodXə ωw OjBUIIXOJddB OJ (9) U.2SUBdXə Jop⅛ jəpjolpuoɔəs əip
əsn OM .uəəʌuəel.ssωnIPA JOJ SUo 君 OdE.sJBOU= JOqaəu JSə.Kəu əsn əʌv CsuonpznOJOSIP OAOqP
ωqj qJ-M .SuoHɔəj-p 寸寸 1 OJU" 一匕 ZO一oɪɔjɪɔωqj əznəjɔs-p əʌv CUOn ɔəj-p』0H .oɔnjpɪ 0∞x 0∞B OJU"
pəz-jəjɔs-p S- q∙yqM Ua × 日 Z OZE £1M juəuiuoj-auə OJEnbS EωsnωM CSJUOlUy ωdxω IEOyOUmU ətp Ul
sHNwnlxwdxx S
Under review as a conference paper at ICLR 2021
patterns of learned V(X) with different block sizes, learned B(θ) and learned u(X). For the learned
B(θ), each element shows regular sine/cosine tuning over θ.
We further investigate the characteristics of the learned firing rate patterns (i.e., V(X)) using measures
adopted from the grid cell literature. Specifically, the hexagonal regularity, scale and orientation of
grid-like patterns are quantified using the gridness score, grid scale and grid orientation (Langston
et al., 2010; Sargolini et al., 2006), which are determined by taking a circular sample of the auto-
correlogram of the response map. All learned patterns exhibit significant hexagonal periodicity in
terms of gridness scores (mean 1.08, range 0.60 to 1.57). Specifically, a unit is considered to be
grid-like if the gridness score exceeds the 95th percentile of null distribution obtained by applying
spatial field shuffles to the response map, following the standard procedure in (Hafting et al., 2005;
Barry & Burgess, 2017). On average, the 95th percentile is 0.35 for all the units. Figure 4a shows
six examples of the autocorrelograms of the response maps and the corresponding gridness scores,
each of which is from a different module. The grid scales of learned patterns (mean 0.39, range 0.24
to 0.61), as shown in Figure 4b, follows a multi-modal distribution. The ratio between neighboring
modes are roughly 1.44 and 1.51, which closely matches the theoretical predictions (Wei et al., 2015;
Stemmler et al., 2015) and also the empirical results from rodent grid cells (Stensola et al., 2012).
The grid orientations of learned patterns, as shown in Figure 4c, are also multi-modal distributed,
consistent to the observations on rat grid cells (Stensola et al., 2012). See Supplementary B.3 for the
detailed spatial profile of every unit of V (X). Collectively, these results reveal striking, quantitative
correspondence between the properties of our model neurons and those of the grid cells in the brain.
b
至suəp 3三qeqo」d
a⅛subp A4=-qeqoJd
5
# ⅜ #
Figure 4: Model grid cells exhibit modular structure that is consistent with experimental data.(a) Examples of
autocorrelograms of the response maps and the corresponding gridness scores, each of which is from a different
module. (b) Multi-modal distribution of grid scales. The scale ratios closely match the real data (Stensola et al.,
2012). (c) Multi-modal distribution of grid orientations.
5.2	Path integration and error correction
We then examine the ability of the learned system on performing path integration, by recurrently
updating v(t) as shown in equation 7 and decoding v(t) to x(t) for t = 1, ..., T using equation 9.
Re-encoding v(t) - V(X⑴)after decoding is adopted. Figure 5a shows an example trajectory of
accurate path integration for T = 80. As shown in figure 5b, with re-encoding, the path integration
error remains close to zero over a duration of 500 time steps (< 0.01 cm, averaged over 1,000
episodes), although the model is trained by the single-time-step loss in equation 11. Without re-
encoding, the error goes slight higher but still remains reasonable (ranging from 0.0 to 5.4 cm, mean
3.8 cm). The performance of path integration would be improved as the block size becomes larger,
i.e., more units or cells in each module (figure 5c). When block size is larger than 20, path integration
is almost exact for the time steps tested.
We further assess the ability of error correction of the learned system. Specifically, along the way of
path integration, at every time step t, two types of errors are introduced to v(t): (1) Gaussian noise or
(2) dropout masks, i.e., certain percentage of units are randomly set to zero. Figure 5d summarizes
the path integration performance with different levels of introduced errors for T = 100. For Gaussian
noise, We use the average magnitude of units in V(X) as the reference standard deviation (s), i.e.,
S =，|v(x)|2/d. The results show that re-encoding is crucial for error correction. Notably, with
re-encoding, the path integration works reasonably well even if Gaussian noise with magnitude of s
is added or 50% units are dropped out at each step, indicating that the learned system is quite robust
to different sources of errors.
7
Under review as a conference paper at ICLR 2021
Figure 5: The learned model can perform path integration. (a) Black: example trajectory. The decoded
self-positions (red) accurately matches the real path. (b) Path integration error over number of time steps. (c)
Path integration error over different block sizes, for 50 and 100 time steps. For (b) and (c), averaged error and ±
1 standard deviation band over 1,000 episodes are shown. (d) Path integration error with introduced errors. Left:
Gaussian noise. Right: dropout mask.
5.3	Ablation study
We conduct systematic ablation study to assess the impact of individual components of the model
on learning hexagonal grid patterns (measured by gridness score) and the ability to perform path
integration (T = 100 time steps). Table 1 summarizes the results where the model is trained with
certain component removed. Specifically, for the emergence of regular hexagonal patterns, all
components appear to be important, especially L2 which is crucial. In comparison, u(x) ≥ 0 is not
entirely critical. Another observation is that L0, L1 and penalty on |u(x)|2 are crucial for accurate
path integration. See Supplementary B.4 for details.
Table 1: Ablation study. A certain component of the model is removed and the learned model is evaluated in
terms of gridness score and path integration error over T = 100 time steps. Skew-symmetry is for B(θ) and C.
L0	L1	L2	Regularize |u(x)|2	Skew-symmetry	u(x) ≥ 0	Gridness	path integration error (cm)
X	✓	✓	✓	✓	✓	-0.10 ± 0.15	44.2 ± 15.4
✓	X	✓	✓	✓	✓	0.23 ± 0.22	30.6 ± 11.8
✓	✓	X	✓	✓	✓	0.32 ± 0.31	0.00 ± 0.00
✓	✓	✓	X	✓	✓	-0.07 ± 0.21	26.3 ± 10.2
✓	✓	✓	✓	X	✓	0.70 ± 0.41	0.00 ± 0.00
✓	✓	✓	✓	✓	X	0.89 ± 0.27	0.00 ± 0.00
✓	✓	✓	✓	✓	✓	1.08 ± 0.31	0.00 ± 0.00
6 Discussion and conclusion
6.1	Related work on models of grid cells
Our work is related to several lines of previous research. First, RNN models have been used to
model grid cells and path integration. The traditional approach uses simulation-based models with
hand-crafted connectivity (Zhang, 1996; Burak & Fiete, 2009; Couey et al., 2013; Pastoll et al.,
2013; Agmon & Burak, 2020). More recently, two pioneering papers (Cueva & Wei, 2018; Banino
et al., 2018) developed an optimization-based RNN approach to learn the path integration model and
discovered that grid-like response pattern could emerge in the optimized network. These results are
further substantiated in following-up research (Sorscher et al., 2019; Cueva et al., 2020). Compared
to these studies, our path integration model is more explicit, coupling two rotation systems in neural
space to mirror the egocentric motion in physical space. In doing so, our results reveals new insights
regarding why lattice and in particular hexagonal response patterns may emerge in neural networks
trained to perform path integration.
Second, as discussed in Sec.3, our model is naturally connected to the basis expansion models of grid
cells. A key ingredient of this line of work (Dordek et al., 2016; Sorscher et al., 2019; Stachenfeld
et al., 2017) is that, under certain conditions, the principal components of the place cell activities
exhibit grid patterns. Importantly, our work differs from these models in that, unlike PCA, we make no
8
Under review as a conference paper at ICLR 2021
assumption about the orthogonality between the basis functions, and the basis expansion formulation
is obtained via group representation from our path integration model. Group representation unifies
path integration and basis expansion, which are two roles hypothesized for grid cells. Furthermore,
in previous basis expansion models (Dordek et al., 2016; Sorscher et al., 2019), place fields with
Mexican-hat patterns (with balanced excitatory center and inhibitory surround) were assumed in
order to obtain hexagonal grid firing patterns. However, experimentally measured place fields were
instead well characterized by Gaussian functions. Crucially, in our model, hexagonal grids emerge
after learning with Gaussian place fields, and there is no need to assume any additional surround
mechanisms.
In another related paper, Gao et al. (2018) proposed vector representation of 2D position and matrix
representation of 2D displacement. Our work goes beyond that work in revealing two coupled Lie
groups and Lie algebra structure, as well as further integrating the path integration model with the
basis expansion model.
6.2	Conclusion
This paper entertains a representation model of grid cells’ path integration, which mirrors the
egocentric self-motion by coupling two rotation systems. The proposed model can be justified as a
minimally simple recurrent model, and has explicit geometric and algebraic structures. As we have
shown, this simple model framework leads to a system that captures many of the experimentally
observed properties of the grid cell system in the brain.
Our path integration model is linear in the vector v(x), but it is non-linear in the displacement ∆x.
Since the basis expansion model is linear in v(x), it may be preferable to have the path integration
model linear in v(x) too. The rotation of v(x) is capable of path integration, and the way v(x)
rotates can explain the hexagon periodic patterns and the metric of each module. The connection
between the two models is based on the group representation theory.
As to modularity, in the context of our path integration model, it means that each sub-vector vk (x)
is rotated by a separate generator sub-matrix, or driven by a separate recurrent network, so that the
dynamics of the sub-vectors are disentangled. This appears to be biologically plausible. We believe
modularity is part of the design of the network.
In terms of representation learning, it is common to represent the physical state by a vector, i.e.,
embedding the physical state into a higher dimensional neural space. However, the problem of
representing continuous motion in the physical space, or continuous transformation of the physical
state, or continuous relation between the states has not received an in-depth treatment. The continuous
motion in the physical space often has a native Lie algebra and Lie group structure. Our work provides
an explicit representation of the continuous motion and its algebraic structure by matrix Lie algebra
and matrix Lie group that act in the neural space. We believe our method can be applied to modeling
head direction system as well as modeling the motor cortex for representing the continuous motions
of arm, hand, pose, etc.
Last but not least, the proposed model can also be used for path planning. See Supplementary C for
the preliminary results).
References
Haggai Agmon and Yoram Burak. A theory of joint attractor dynamics in the hippocampus and the
entorhinal cortex accounts for artificial remapping and grid cell field-to-field variability. eLife, 9:
e56894, 2020.
Andrea Banino, Caswell Barry, Benigno Uria, Charles Blundell, Timothy Lillicrap, Piotr Mirowski,
Alexander Pritzel, Martin J Chadwick, Thomas Degris, Joseph Modayil, et al. Vector-based
navigation using grid-like representations in artificial agents. Nature, 557(7705):429, 2018.
Caswell Barry and Neil Burgess. To be a grid cell: Shuffling procedures for determining’gridness’.
BioRxiv, pp. 230250, 2017.
Caswell Barry, Robin Hayman, Neil Burgess, and Kathryn J Jeffery. Experience-dependent rescaling
of entorhinal grids. Nature neuroscience, 10(6):682-684, 2007.
9
Under review as a conference paper at ICLR 2021
Hugh T Blair, Adam C Welday, and Kechen Zhang. Scale-invariant memory representations emerge
from moire interference between grid fields that produce theta oscillations: a computational model.
Journal OfNeuroscience, 27(12):3211-3229, 2007.
Yoram Burak and Ila R Fiete. Accurate path integration in continuous attractor network models of
grid cells. PLoS computational biology, 5(2):e1000291, 2009.
Jonathan J Couey, Aree Witoelar, Sheng-Jia Zhang, Kang Zheng, Jing Ye, Benjamin Dunn, Rafal
Czajkowski, May-Britt Moser, Edvard I Moser, Yasser Roudi, et al. Recurrent inhibitory circuitry
as a mechanism for grid formation. Nature neuroscience, 16(3):318-324, 2013.
Christopher J Cueva and Xue-Xin Wei. Emergence of grid-like representations by training recurrent
neural networks to perform spatial localization. arXiv preprint arXiv:1803.07770, 2018.
Christopher J Cueva, Peter Y Wang, Matthew Chin, and Xue-Xin Wei. Emergence of functional and
structural properties of the head direction system by optimization of recurrent neural networks.
International Conferences on Learning Representations (ICLR), 2020.
Charles Darwin. Origin of certain instincts, 1873.
Licurgo de Almeida, Marco Idiart, and John E Lisman. The input-output transformation of the
hippocampal granule cells: from grid cells to place fields. Journal of Neuroscience, 29(23):
7504-7512, 2009.
Christian F Doeller, Caswell Barry, and Neil Burgess. Evidence for grid cells in a human memory
network. Nature, 463(7281):657, 2010.
Yedidyah Dordek, Daniel Soudry, Ron Meir, and Dori Derdikman. Extracting grid cell characteristics
from place cell inputs using non-negative principal component analysis. Elife, 5:e10094, 2016.
Ariane S Etienne and Kathryn J Jeffery. Path integration in mammals. Hippocampus, 14(2):180-192,
2004.
Ila R Fiete, Yoram Burak, and Ted Brookings. What grid cells convey about rat location. Journal of
Neuroscience, 28(27):6858-6871, 2008.
Mark C Fuhs and David S Touretzky. A spin glass model of path integration in rat medial entorhinal
cortex. Journal of Neuroscience, 26(16):4266-4276, 2006.
Marianne Fyhn, Sturla Molden, Menno P Witter, Edvard I Moser, and May-Britt Moser. Spatial
representation in the entorhinal cortex. Science, 305(5688):1258-1264, 2004.
Marianne Fyhn, Torkel Hafting, Menno P Witter, Edvard I Moser, and May-Britt Moser. Grid cells in
mice. Hippocampus, 18(12):1230-1238, 2008.
Ruiqi Gao, Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning grid cells as vector rep-
resentation of self-position coupled with matrix representation of self-motion. arXiv preprint
arXiv:1810.05597, 2018.
Mariana Gil, Mihai Ancau, Magdalene I Schlesiger, Angela Neitz, Kevin Allen, Rodrigo J De Marco,
and Hannah Monyer. Impaired path integration in mice with disrupted grid cell firing. Nature
neuroscience, 21(1):81-91, 2018.
Torkel Hafting, Marianne Fyhn, Sturla Molden, May-Britt Moser, and Edvard I Moser. Microstructure
of a spatial map in the entorhinal cortex. Nature, 436(7052):801, 2005.
SePP Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Aidan J Horner, James A Bisby, Ewa Zotow, Daniel Bush, and Neil Burgess. Grid-like Processing of
imagined navigation. Current Biology, 26(6):842-847, 2016.
Joshua Jacobs, ChristoPh T Weidemann, Jonathan F Miller, Alec Solway, John F Burke, Xue-Xin Wei,
Nanthia Suthana, Michael R SPerling, Ashwini D Sharan, Itzhak Fried, et al. Direct recordings of
grid-like neuronal activity in human sPatial navigation. Nature neuroscience, 16(9):1188, 2013.
10
Under review as a conference paper at ICLR 2021
Nathaniel J Killian, Michael J Jutras, and Elizabeth A Buffalo. A map of visual space in the primate
entorhinal cortex. Nature, 491(7426):761, 2012.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Rosamund F Langston, James A Ainge, Jonathan J Couey, Cathrin B Canto, Tale L Bjerknes, Menno P
Witter, Edvard I Moser, and May-Britt Moser. Development of the spatial representation system in
the rat. Science, 328(5985):1576-1580, 2010.
Bruce L McNaughton, Francesco P Battaglia, Ole Jensen, Edvard I Moser, and May-Britt Moser.
Path integration and the neural basis of the’cognitive map’. Nature Reviews Neuroscience, 7(8):
663, 2006.
John O’Keefe. A review of the hippocampal place cells. Progress in neurobiology, 13(4):419-439,
1979.
Hugh Pastoll, Lukas Solanka, Mark CW van Rossum, and Matthew F Nolan. Feedback inhibition
enables theta-nested gamma oscillations and grid firing fields. Neuron, 77(1):141-154, 2013.
Thomas Ridler, Jonathan Witton, Keith G Phillips, Andrew D Randall, and Jonathan T Brown.
Impaired speed encoding is associated with reduced grid cell periodicity in a mouse model of
tauopathy. bioRxiv, pp. 595652, 2019.
David C Rowland, Horst A Obenhaus, Emilie R Skyt0en, Qiangwei Zhang, Cliff G Kentros, Edvard I
Moser, and May-Britt Moser. Functional properties of stellate cells in medial entorhinal cortex
layer ii. Elife, 7:e36664, 2018.
Francesca Sargolini, Marianne Fyhn, Torkel Hafting, Bruce L McNaughton, Menno P Witter, May-
Britt Moser, and Edvard I Moser. Conjunctive representation of position, direction, and velocity in
entorhinal cortex. Science, 312(5774):758-762, 2006.
Ben Sorscher, Gabriel Mel, Surya Ganguli, and Samuel Ocko. A unified theory for the origin of grid
cells through the lens of pattern formation. In Advances in Neural Information Processing Systems,
pp. 10003-10013, 2019.
Sameet Sreenivasan and Ila Fiete. Grid cells generate an analog error-correcting code for singularly
precise neural computation. Nature neuroscience, 14(10):1330, 2011.
Kimberly L Stachenfeld, Matthew M Botvinick, and Samuel J Gershman. The hippocampus as a
predictive map. Nature neuroscience, 20(11):1643, 2017.
Martin Stemmler, Alexander Mathis, and Andreas VM Herz. Connecting multiple spatial scales to
decode the population activity of grid cells. Science Advances, 1(11):e1500816, 2015.
Hanne Stensola, Tor Stensola, Trygve Solstad, Kristian Fr0land, May-Britt Moser, and Edvard I
Moser. The entorhinal grid map is discretized. Nature, 492(7427):72, 2012.
Michael Taylor. Lectures on lie groups. Lecture Notes, available at http://www. unc.
edu/math/Faculty/met/lieg. html, 2002.
Sathamangalam R Srinivasa Varadhan. On the behavior of the fundamental solution of the heat
equation with variable coefficients. Communications on Pure and Applied Mathematics, 20(2):
431-455, 1967.
Xue-Xin Wei, Jason Prentice, and Vijay Balasubramanian. A principle of economy predicts the
functional architecture of grid cells. Elife, 4:e08362, 2015.
Michael M Yartsev, Menno P Witter, and Nachum Ulanovsky. Grid cells without theta oscillations in
the entorhinal cortex of bats. Nature, 479(7371):103, 2011.
Anthony Zee. Group theory in a nutshell for physicists. Princeton University Press, 2016.
Kechen Zhang. Representation of spatial orientation by the intrinsic dynamics of the head-direction
cell ensemble: a theory. Journal of Neuroscience, 16(6):2112-2126, 1996.
11
Under review as a conference paper at ICLR 2021
Sheng-Jia Zhang, Jing Ye, Chenglin Miao, Albert Tsao, Ignas Cerniauskas, Debora Ledergerber, May-
Britt Moser, and Edvard I Moser. Optogenetic dissection of entorhinal-hippocampal functional
connectivity. Science, 340(6128), 2013.
12
Under review as a conference paper at ICLR 2021
A Theoretical analysis
A.1 Proof of propositions
Proof of Proposition 1. For an infinitesimal δx = δr(cos θ, sin θ), v(x) is changed to v(x + δx).
Let δα be the angle between v(x) and v(x + δx). It can be obtained from
hv(x), v(x + δx)i = v(x)> exp(B(θ)δr)v(x)	(13)
= v(x)>(I + B(θ)δr + B(θ)2δr2∕2 + o(δr2))v(x)	(14)
=kv(x)k2 -kB(θ)v(x)k2δr2∕2 + o(δr2),	(15)
where v(x)>B(θ)v(x) = 0 because B(θ) = -B(θ)>. Let
β2 = kB(θ)v(x)k2∕kv(x)k2.	(16)
It is independent of θ, because for any ∆θ, B(θ+ ∆θ)v = R(∆θ)B(θ)v, and R(∆θ) is orthogonal,
thus kB(θ + ∆θ)vk2 = kB(θ)vk2 for any ∆θ. Note kv(x)k2 = kv(x + δx)k2 because Mθ (δr)
is orthogonal,
cos(δα) = 1 - δα2∕2 + o(δα2)	(17)
=hv(x), v(x + δx)i∕∣v(x)∣2	(18)
= 1 - (βδr)2∕2 + o(δr2).	(19)
Thus the angle δα = βδr.
Proof of Proposition 2. Suppose v(x) is defined as in Proposition 2. That is, v(x) = U e(x), where
U is an arbitrary unitary matrix. e(x) = (exp(ihaj, xi), j = 1, 2, 3)>, where (aj, j = 1, 2, 3) are
three 2D vectors of equal norm, and the angle between every pair of them is 2π∕3. Then we have
kv(x)k2 = 3, and the angle between v(x) and v(x + δx), denoted as δα, is
cos(δα) = RE ( JvRv(X + δx% )
Uv(X)kkv(χ+δχ)k√
=3RE(hv(x), v(x + δx)i)
=1RΕ(e(x)*U *Ue(x + δx))
1RE (Xeχp(ihaj,δXi))
13
3 ∑Scos( haj,δXi)
j=1
3 χ (ι - 2 haj, δxi2)+o(δr2)
j=1
(1 — gδr2) + o(δr2)
cos(βδr) + o(δr2 ).
(20)
(21)
(22)
(23)
(24)
(25)
(26)
(27)
Thus we have δα = βδr + O(δr2). Here the key is that (aj, j = 1, 2,3) forms a tight frame in the
2D space, in that for any 2D vector δx, P；=i hδx, aj)2 8 kajk2kδXk2. Thus β U kaj k.
A.2 Error correction
Suppose v = v(X) + is a noisy version of v(X), can we still decode X accurately from v? Here we
assume E 〜N(0, T2(∣v(x)∣2∕d)I), where d is the dimensionality of v, and T2 measures the variance
of noise relative to ∣v(x)∣2∕d, i.e., the average of (Vi(X)2, i = 1,..., d).
13
Under review as a conference paper at ICLR 2021
The heat map
h(x0) = hv, u(x0)i = hv(x), u(x0)i + h, u(x0)i = A(x, x0) + e(x0),	(28)
where e(x0)〜N(0,τ2∣v(x)∣2∣u(x0)∣2∕d). For A(x, x0) = exp(-∣x — x0∣2∕(2σ2))=
hv(x), u(x0)i, if σ2 is small, A(x, x0) decreases to 0 quickly, i.e., if |x0 - x| > δ, then
A(x, x0) < exp(-δ2∕(2σ2)), and the chance for the maximum of h(x0) to be achieved at an
x0 so that |x0 - x| > δ can be very small.
The above analysis also provides a justification for regularizing |u(x0)|2 in learning.
For error correction, we want d to be big, and we want σ2 to be small. But for path planning, we also
need big σ2. That is, we need A(x, x0) at multiple scales.
A.3 Orthogonality relations
For (x) that form a group, a matrix representation M(x) is equivalent to another representation M(x)
if there exists a matrix P such that M(x) = P M(x)P -1 for each x. A matrix representation is
reducible if it is equivalent to a block diagonal matrix representation, i.e., we can find a matrix P , such
that PM(x)P-1 is block diagonal for every x. Suppose the group is a finite group or a compact Lie
group, and M is a unitary representation. If M is block-diagonal, M = diag(Mk, k = 1, ..., K),
with non-equivalent blocks, and each block Mk cannot be further reduced, then the matrix elements
(Mkij(x)) are orthogonal basis functions ofx. Such orthogonality relations are proved by Schur (Zee,
2016) for finite group, and by Peter-Weyl for compact Lie group (Taylor, 2002). For our case,
theoretically the group of displacements in the 2D domain is R2, but we learn our model within a
finite range, and we further discretize the range into a lattice. Thus the above orthogonal relations are
relevant.
In our model, we also assume block diagonal M, and we call each block a module. However, we
do not assume each module is irreducible, i.e., each module itself may be further diagonalized into
a block diagonal matrix of irreducible blocks. Thus the elements within the same module vk (x)
may be linear mixing of orthogonal basis functions, and they themselves may not be orthogonal.
However, different modules may be linear mixings of different sets of irreducible blocks, and thus
different modules can be orthogonal to each other. Figure 6 visualizes the correlations between each
pair of the learned vi(x) and vj (x), i, j = 1, ..., d. For vi(x) and vj (x) from different modules,
the correlations are close to zero; i.e., vi(x) and vj(x) from different blocks are approximately
orthogonal to each other. But vi(x) and vj (x) from the same block are not orthogonal to each other.
Figure 6: Correlation heatmap for each pair of the learned vi (x) and vj (x). The correlations are
computed over 80 × 80 lattice of x.
A.4 Adding bias term
Since kv(x)k is fixed under rotation, we can add a bias b to the vector v(x) so that the elements
become non-negative. Specifically, let V(x) = v(x) + b, where b is d-dimensional vector with equal
14
Under review as a conference paper at ICLR 2021
elements, then V(x) = V(x) - b. The recurrent model for V(X + δx) = (I + B(θ)δr)v(x) can
then be translated into V(X + δx) = (I + B(θ)δr)V(x) — cδr, where C = -B(θ)b. The basis
expansion model Axo (x) =(v(x), u(x0))can be translated into Axo (x) =(V(x), u(x0)〉— b(x0),
where b(x0) = (b, u(x0)). V(X) rotates around b.
B	Experiments
B.1	Monte Carlo sampling
For learning, the expectations in loss terms are approximated by Monte Carlo samples. Here we detail
the generation of Monte Carlo samples. For (X, X0) used in L0 = Ex,x0 [A(X, X0) - (V(X), u(X0)i]2,
X is first sampled uniformly within the entire domain, and then the displacement dX between X
and X0 is sampled from a normal distribution N(0, σ2I2), where σ = 0.48. This is to ensure that
nearby samples are given more emphasis. We let X0 = X + dX, and those pairs (X, X0) within
the range of domain (i.e., 2m × 2m, 80 × 80 lattice) are kept as valid data. For (X, ∆X) used in
L1 = Ex,δx |v(X + ∆x) — exp(B(θ)∆r)v(x) |2, ∆x is sampled uniformly within a circular domain
with radius equal to 3 grids and (0, 0) as the center. Specifically, ∆r2, the squared length of ∆X, is
sampled uniformly from [0, 3] grids, and θ is sampled uniformly from [0, 2π]. We take the square
root of the sampled ∆r2 as ∆r and let ∆X = (∆r cos θ, ∆r sin θ). Then X is uniformly sampled
from the region such that both X and X + ∆X are within the range of domain. For (θ, ∆θ) used in
L2 = Eθ,δθ |B (θ + ∆θ) — exp(C ∆θ)B (θ)∣2, we enumerate all the pairs of discretized θ (i.e., 144
directions discretized for circle [0, 2π]) and ∆θ (i.e., 5 directions within range [0, 12.5] degrees) as
samples.
B.2	Learned patterns
Figure 7 shows the learned patterns of u(X) with 6 blocks of size 32 cells in each block. Figure 8
shows the learned patterns of V(X) and u(X) with 6 blocks of size 16. Figure 9 shows the learned
patterns of V(X) with 16 blocks if size 12. Regular hexagon patterns emerge in all these settings.
000如02阳阳困&圉阳附闲8炎H鼠雄囹&器壁困囊芟装好疑型f翅
磔取油绻超汨。得方EI展区嗨双弱R用蝎*超妁M潴为您隔州第
爨寤展懑酒提鹿随鹿跑襟蹄虢肝逐殿密霰懒附覆爨盍爨嬲爨麻爨蠲^蠕蟒
≡≡≡≡≡≡≡≡≡≡≡≡≡≡ffiffi≡≡≡≡≡≡≡≡≡≡HH≡fflffi≡
然强翻麴森耦联彝朝■漫爨凯展寓翻爨闺缝够雷眼耦雳用海跟国图翻朝阳
Figure 7: Learned patterns of u(X) with 6 blocks of size 32 cells in each block. Every row shows the
learned patterns within the same block.
We further evaluate the spatial profile of the learned patterns with 6 blocks of size 16 using the
same measures as in the main text. All learned patterns exhibit significant hexagonal periodicity in
terms of gridness scores (mean 1.06, std 0.27, range 0.58 to 1.48), which exceed the 95 percentile of
null distributions obtained by applying spatial field shuffle to each response map. The grid scales
of learned patterns (mean 0.38, range 0.27 to 0.56), as shown in figure 10a, follows a multi-modal
distribution. The ratio between neighbouring modes are roughly 1.37 and 1.38. As shown in figure
10b, the grid orientations of learned patterns are also multi-modal distributed.
Figure 11 shows the learned patterns of a block of B(θ) over θ from 0 to 2π. Regular sine/cosine
tunings emerge. (B(θ)) can be isometric to (θ), in the sense that for each column i, the angle between
Bi(θ) and Bi(θ + ∆θ) is ∆θ.
B.3 Spatial profile of learned hexagon grid patterns
Figure 12 shows the spatial profile of the patterns of V(X) over the 80 lattice.
15
Under review as a conference paper at ICLR 2021
客造绛逐凄型潴矮短线深卷食透读赛
麴麴露嬲藏巍鬻翳鬻麒馥巍鬻舞皴蕊
■HHMHMHHHHHHHHHB
鼓稔速藤靛愈履融战魅爨靓辎程航姿
用附明或癖索娜混幽取除直接便西力
舞殿籁懒剧懒熬蹒嬲畿娥嬲耀巍凝翻
(a)	Learned patterns of v(x)
宓通5S8SS品项SSRP总工宓意我改
≡^≡≡≡≡≡g≡≡≡≡≡≡≡≡
≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡
放电拒度曜腐■鬣睡Ii逾赛用降源现
隔Sim生。RitR1汨0逐星MYiR
跑超解装展脂篇隔雅虢疆摩睡重递漱
(b)	Learned patterns of u(x)
Figure 8: Learned patterns with 6 blocks of size 16. Top: v (x). Bottom: u(x). Every row shows the
learned patterns within the same block.
B.4 Learned patterns in ablation study
Figure 13 shows the learned patterns of v (x) where the model is trained with a certain component
removed.
C Path planning
Define and model the adjacency according to Stachenfeld et al. (2017),
∞
Aγ (x, x0) = E Xγt1(xt = x0)|x0 = x) = hv (x), uγ (x0)i,	(29)
t=0
where E is with respect to a random walk exploration policy, and γ is the discount factor that controls
the temporal and spatial scales. We can discretize γ into a finite list of scales. The above model,
i.e., v(x) and uγ (x0), can be learned by temporal difference learning. The basis expansion model
with d N2 enables efficient learning from small amount of explorations, so that we can fill in
unexplored Aγ (x, x0) based on the learned v(x) and uγ (x0).
For random Walk diffusion in open field, AY(x, x0) a exp(一∣x — x0∣2∕2σγ), where σj depends on
γ.
For random walk in a field with obstacles or non-Euclidean geometry, using Varadhan’s for-
mula (Varadhan, 1967), Aγ (x, x0) can still be approximated by Gaussian kernel except |x — x0| is
replaced by geodesic distance.
After learning v(x), Uγ(x0), we propose to use the following method for path planning. Let X be the
target. Let x(t) be the current position, encoded by v(x(t)), we propose to plan the next displacement
by
∆x(t+1) = arg maxhM(∆x)v(x(t)), UY(X)i,	(30)
∆x	γ
16
Under review as a COnferenCe PaPer ar ICLR 2021

FigUre 9《Leamed Parre3S Wirh 16 blocks Of SiZe 12∙ Left.. F(CC)∙ RighI.. Tl(CC)∙ EVeryroW ShoWS the
Ieamed ParremS Wirhin rhe Same block∙
Probability density
ɛ
o
4
0∙l 0∙2 0∙3 0∙4 0LΠ
Gr-Cl o□.entation
(b)
FigUre 19(a) MUIti—modal distribution Of grid SCaleS∙ (b) MUlti—modal distribution Of grid Orienta—
ro∙ns∙
and Iet CC (工1) “ CCS+ ∆ic(上一)“ encoded byAic(上一)) “ TVf (∆ic(上一) )Λics)∙ In the above
maximizaro∙p>ic is ChOSen from all rhe allowed disppcemenrs for a SingIe SreP “ and We also need〔0
SeIeCr an OPrimaI q Claris mosr SenSiriVe S the Change Of A An example Of SCaIe SeleCrion SCheme
is that We ChooSe the smallest Q that satisfiesmax>"Λl(>ic)Aics)"wγ(⅛) V 0∙2∙ When CCS
is far from "the SeleCted>is big∙ When CCSis COSe to "the SeleCted>is SmaI1∙ The above
method enables auεmaric SeleCrion Of SCaIe∙ We ShalI explore Orher SChemeS Of SeleCring q in future
Work∙
We test Path PIanning in OPen Held USing the Ieamed model∙ SPeCifiCa=y“ the model is first learned
USinga SingIe SCaIe Ay(CCa、)“ Where QaU oo7∙ Then We assume a HSr Ofrhree SCaIeS Of Ay(CC
i.e." Qa Uoo7" 0∙14" 0∙2∞“ and Ieanl three corresponding SetS Of Wy(Xr). FOr planning” We Create
a PooI Of allowed displacements from WhiCh >ic is ChoSen二he Iength Of >ic Can be 1 or 2 grids”
and the direction Can be ChoSen from 200 discretized angles OVerOɪ- FigUre 14 depicts SeVeraI
17
Under review as a conference paper at ICLR 2021
Figure 11: Learned patterns of a block of B(θ). Each curve shows the patterns of one element of
B(θ) over θ ∈ [0, 2π]. Since B(θ) is skew-symmetric, the diagonal values are zeros and the value of
Bij(θ) is the same as the value of Bji(θ). Regular sine/cosine tunings emerge.
examples of planned paths. As the examples show, when x(t) is far from the target, kernel with large
σγ is chosen, and as x(t) approaches the target, kernel with smaller σγ is chosen. A planning episode
is treated as successful if the distance between x(t) and target is smaller than 0.5 grid within 40 time
steps. In the cases where the distance between the starting point of the agent and the target is smaller
than 20 grids, the successful rate is 100% (test for 10, 000 episodes).
We shall explore this method in irregular fields with obstacles in future work.
18
Under review as a conference paper at ICLR 2021
0.88. 0.51. 0.05
0.77,0.51. 0.05
3.67, 0.52, 0.05
0.70, 0.52, 0.05
D.74. 0.51. 0.02
L.bb. 0.34, 0.04
0.96, 0.5/. 0.29
594, 0.5/. U.20
1.21. 0.56. 0.29
1.11. 0.61. 0.29
1.02, 0.56, 0.27
0.88. 0.55, 0.29
1.17. 0.54, 0.29
L.12. 0.56, 0.29
1.14. 0.32. 0.45
L.16. 0.32. 0.45
1.14. 0.32. 0.45
L.14. 0.32. 0.45
1.19. 0.32, 0.45
1.17. 0.32, 0.45
1.16. 0.32. 0.45
Figure 12: Spatial profile of the patterns of v(x) over the 80 × 80 lattice.
1.21, 0.32, 0.45
1.16. 0.32. 0.45
1.16, 0.32, 0.45
1.20. 0.32, 0.45
1.17,0.32.0.45
59/, 0.58, 0.29
0.99, 0.57, 0.29
确⅛
1.09,0.56, 0.27
0.77, 0.53, 0.05
U.95, 0.58, 0.29
0.90, 0.54f 0.29
0.89, 0.56, 0.29
0.73, 0.58, 0.29
1.17. 0.32, 0.45
1.05, 0.57, 0.27
e
0.73, 0.58, 0.27
1.19, 0.32, 0.45
1.17. 0.32, 0.45
For each unit, the
autocorrelogram is visualized. Gridness score, scale and orientation are listed sequentially on top of
the autocorrelogram.
19
Under review as a conference paper at ICLR 2021
(a) Learned patterns of v(x) with L0 = Ex,x0 [A(x, x0) - hv(x), u(x0)i]2 removed.
(b) Learned patterns of v(x) with Li = Eχ,∆χ |v(x + ∆x) - exp(B(θ)∆r)v(x)∣2 removed.
(C) Learned patterns of V(X) with L2 = Eθ,δθ ∣B(θ + ∆θ) — exp(C∆θ)B(θ)∣2 removed.
(d) Learned patterns of v(x) with the regularization of |u(x)|2 removed.
(e) Learned patterns of v(x) with the assumption of skew-symmetry for B(θ) and C removed.
(f) Learned patterns of v(x) with the assumption of u > 0 removed.
Figure 13: Learned patterns of v(x) in ablation study, where the model is trained with a Certain
Component removed.
20
Under review as a conference paper at ICLR 2021
Figure 14: Examples of path planning in open field. Red star denotes the target. Three scales of
Aγ (x, x0) are used (σγ = [0.07, 0.14, 0.28]). Different colors denote the kernel chosen at each step.
21