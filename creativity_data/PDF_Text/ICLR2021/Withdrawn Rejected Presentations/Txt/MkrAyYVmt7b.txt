Under review as a conference paper at ICLR 2021
Perfect density models cannot guarantee
ANOMALY DETECTION
Anonymous authors
Paper under double-blind review
Ab stract
Thanks to the tractability of their likelihood, some deep generative models show
promise for seemingly straightforward but important applications like anomaly
detection, uncertainty estimation, and active learning. However, the likelihood
values empirically attributed to anomalies conflict with the expectations these
proposed applications suggest. In this paper, we take a closer look at the behavior
of distribution densities and show that these quantities carry less meaningful
information than previously thought, beyond estimation issues or the curse of
dimensionality. We conclude that the use of these likelihoods for out-of-distribution
detection relies on strong and implicit hypotheses, and highlight the necessity of
explicitly formulating these assumptions for reliable anomaly detection.
1	Introduction
Several machine learning methods aim at extrapolating a behavior observed on training data in order
to produce predictions on new observations. But every so often, such extrapolation can result in
wrong outputs, especially on points that we would consider infrequent with respect to the training
distribution. Faced with unusual situations, whether adversarial (Szegedy et al., 2013; Carlini &
Wagner, 2017) or just rare (Hendrycks & Dietterich, 2019), a desirable behavior from a machine
learning system would be to flag these outliers so that the user can assess if the result is reliable and
gather more information if need be (Zhao & Tresp, 2019; Fu et al., 2017). This can be critical for
applications like medical decision making (Lee et al., 2018) or autonomous vehicle navigation (Filos
et al., 2020), where such outliers are ubiquitous.
What are the situations that are deemed unusual? Defining these anomalies (Hodge & Austin, 2004;
Pimentel et al., 2014) manually can be laborious if not impossible, and so generally applicable,
automated methods are preferable. In that regard, the framework of probabilistic reasoning has been
an appealing formalism because a natural candidate for outliers are situations that are improbable or
out-of-distribution. Since the true probability distribution density PX of the data is often not provided,
one would instead use an estimator, p(Xθ) , from this data to assess the regularity of a point.
Density estimation has been a particularly challenging task on high-dimensional problems. However,
recent advances in deep probabilistic models, including variational auto-encoders (Kingma & Welling,
2014; Rezende et al., 2014; Vahdat & Kautz, 2020), deep autoregressive models (Uria et al., 2014;
van den Oord et al., 2016b;a), and flow-based generative models (Dinh et al., 2014; 2016; Kingma
& Dhariwal, 2018), have shown promise for density estimation, which has the potential to enable
accurate density-based methods (Bishop, 1994) for anomaly detection.
Yet, several works have observed that a significant gap persists between the potential of density-based
anomaly detection and empirical results. For instance, Choi et al. (2018), Nalisnick et al. (2018),
and Hendrycks et al. (2018) noticed that generative models trained on a benchmark dataset (e.g.,
CIFAR-10, Krizhevsky et al., 2009) and tested on another (e.g., SVHN, Netzer et al., 2011) are not
able to identify the latter as out-of-distribution with current methods. Different hypotheses have
been formulated to explain that discrepancy, ranging from the curse of dimensionality (Nalisnick
et al., 2019) to a significant mismatch between PX) and PX (Choi et al., 2018; Fetaya et al., 2020;
Kirichenko et al., 2020; Zhang et al., 2020).
In this work, we propose a new perspective on this discrepancy and challenge the expectation that
density estimation should enable anomaly detection. We show that the aforementioned discrepancy
1
Under review as a conference paper at ICLR 2021
Figure 1: There is an infinite number of ways to partition a distribution in two subsets, Xin and Xout
such that PX(Xin) = 0.95. Here, We show several choices for a standard Gaussian PX = N(0,1).
persists even with perfect density models, and therefore goes beyond issues of estimation, approxima-
tion, or optimization errors (Bottou & Bousquet, 2008). We highlight that this issue is pervasive as
it occurs even in low-dimensional settings and for a variety of density-based methods for anomaly
detection.
2	Density-based anomaly detection
2.1	Unsupervised anomaly detection: problem statement
Unsupervised anomaly detection is a classification problem (Moya et al., 1993; Scholkopf et al.,
2001), where one aims at distinguishing between regular points (inliers) and irregular points (outliers).
However, as opposed to the usual classification task, labels distinguishing inliers and outliers are not
provided for training, if outliers are even provided at all. Given a input space X ⊆ RD, the task can
be summarized as partitioning this space between the subset of outliers Xout and the subset of inliers
Xin, i.e., Xout ∪ Xin = X and Xout ∩ Xin = 0. When the training data is distributed according to
the probability measure PX (with density PX 1), one would usually pick the set of regular points Xin
such that this set contains the majority (but not all) of the mass (e.g., 95%) of this distribution, i.e.,
PX (Xin) = 1 - α ∈ (2, 1). But, for any given a, there exists in theory an infinity of corresponding
partitions into Xin and Xout (see Figure 1). How are these partitions defined to match our intuition
of inliers and outliers? We will focus in this paper on recently used methods based on probability
density.
2.2	Density scoring
When talking about outliers, infrequent observations, the association with probability can be quite
intuitive. For instance, one would expect an anomaly to happen rarely and be unlikely. Since the
language of statistics often associate the term likelihood with quantities like P(Xθ) (x), one might
consider an unlikely sample to have a low “likelihood"，that is a low probability density PX(x).
Conversely, regular samples would have a high density PX (x) following that reasoning. This is an
intuition that is not only prevalent in several modern anomaly detection methods (Bishop, 1994; Blei
et al., 2017; Hendrycks et al., 2018; Kirichenko et al., 2020; Rudolph et al., 2020; Liu et al., 2020)
but also in techniques like low-temperature sampling (Graves, 2013) used for example in Kingma &
Dhariwal (2018) and Parmar et al. (2018).
The associated approach, described in Bishop (1994), consists in defining the inliers as the points
whose density exceed a certain threshold λ > 0 (for example, chosen such that inliers include a
predefined amount of mass, e.g., 95%), making the modes the most regular points in this setting.
Xout and Xin are then respectively the lower-level and upper-level sets {x ∈ X,pX (x) ≤ λ} and
{χ ∈ X,pX (x) > λ} (see Figure 2b).
1We will also assume in the rest of the paper that for any X ∈ X ,pχ (x) > 0.
2
Under review as a conference paper at ICLR 2021
x
(a) An example of a distribution
density PX.
(b) Density scoring method ap-
plied to the distribution PX.
(c) Typicality test method (with
one sample) applied to the distri-
bution PX.
Figure 2: Illustration of different density-based methods applied to a particular one-dimensional
distribution PX. Outliers are in red and inliers are in blue. The thresholds are picked so that inliers
include 95% of the mass. In Figure 2b, inliers are considered as the points with density above the
threshold λ > 0 while in Figure 2c, they are the points whose log-density are in the -interval around
the negentropy -H (PX).
2.3	Typicality Test
The Gaussian Annulus theorem (Blum et al., 2016) (generalized in Vershynin, 2019) attests that most
of the mass of a high-dimensional standard Gaussian N(0, ID) is located close to the hypersphere of
radius √D. However, the mode of its density is at the center 0. A natural conclusion is that the curse
of dimensionality creates a discrepancy between the density upper-level sets and what we expect as
inliers (Choi et al., 2018; Nalisnick et al., 2019; Morningstar et al., 2020; Dieleman, 2020). This
motivated Nalisnick et al. (2019) to propose another method for testing whether a point is an inlier
or not, relying on a measure of its typicality. This method relies on the notion of typical set (Cover,
1999) defined by taking as inliers points whose average log-density is close to the average log-density
of the distribution (see Figure 2c).
Definition 1 (Cover, 1999). Given independent and identically distributed elements x(n) n≤N from
a distribution with density PX, the typical set AeN) (PX) ⊂ X N is made of all sequences that satisfy:
1 N
H(PX) + N X logPx Sn)) ≤ e,
n=1
where H(X) = -E[logPX (X)] is the (differential) entropy and e > 0 a constant.
This method matches the intuition behind the Gaussian Annulus theorem on the set of inliers of a
high-dimensional standard Gaussian. Indeed, using a concentration inequality, we can show that
IimN→+∞ (P(Xa)ι< <N(A,N))) = 1, which means that with N large enough, A∈N)(pX) will
contain most of the mass of (PX )N, justifying the name typicality.
3	The role of reparametrization
Given the anomaly detection problem formulation Subsection 2.1, we are interested in reasoning
about the properties a solution ought to satisfy, in the ideal case of infinite data and capacity. For
density-based methods this means that PX) = PX. This setting is appealing as it gives space
for theoretical results without worrying about the underfitting or overfitting issues mentioned by
Hendrycks et al. (2018); Fetaya et al. (2020); Morningstar et al. (2020); Kirichenko et al. (2020);
Zhang et al. (2020).
Although we work in practice on points (e.g., vectors), it is important to keep in mind that these points
are actually representations of an underlying outcome. As a random variable, X is by definition the
function from this outcome ω to the corresponding observation x = X(ω). However, at its core, an
anomaly detection solution aims at classifying outcomes through these measurements. How is the
3
Under review as a conference paper at ICLR 2021
x
(a) An example of a distribution
density PX.
(b) Example of an invertible func-
tion f from [0, 1] to [0, 1].
f(x)
(C) Resulting density Pf(X) from
applying f to X 〜PX as a func-
tion of the new axis f (x).
Figure 3: Illustration of the change of variables formula and how much the application of a bijection
can affect the density of the points considered in a one-dimensional case. In Figures 3a and 3c, points
X with high density PX (x) are in blue and points with low density PX (x) are in red.
choice of X affecting the problem of anomaly detection? While several papers studied the effects of
a change of representation through the lens of inductive bias (Kirichenko et al., 2020; Zhang et al.,
2020), we investigate the more fundamental effects of reparametrizations f. To sidestep concerns
about loss of information (Winkens et al., 2020), we study the particular case of an invertible map f .
The measurements x = X(ω) and f(x) = (f ◦ X)(ω) represent the same outcome ω (although
differently), and, since x and f(x) are connected by an invertible transformation f, the same method
applied respectively to X or f(X) should classify them with the same label, either as an inlier or an
outlier. The target of these methods is to essentially assess the regularity of the outcome ω. From this,
we could ideally make the following requirement for a solution to anomaly detection.
Principle. In an infinite data and capacity setting, the result of an anomaly detection method should
be invariant to any continuous invertible reparametrization f.
Do density-based methods follow this principle? To answer that question, we look into how density
behaves under a reversible change of representation. In particular, the change of variables formula
(Kaplan, 1952) (used in Tabak & Turner, 2013; Dinh et al., 2014; Rezende & Mohamed, 2015),
formalizes a simple intuition of this behavior: where points are brought closer together the density
increases whereas this density decreases when points are spread apart. The formula itself is written
as:
Pf(X)(f(X)) = PX(X) ∂XT(x),
where ∣ ∂f (x) ∣ is the Jacobian determinant of f at x, a quantity that reflects a local change in volume
incurred by f. Figure 3 already illustrates how the function f (Figure 3b) can spread apart points
close to the extremities to decrease the corresponding density round 0 and 1, and, as a result, turns the
density on the left (Figure 3a) into the density on the right (Figure 3c). With this example, one can
wonder to which degree an invertible change of representation can affect the density and the anomaly
detection methods presented in Subsections 2.2 and 2.3 that use it.
4	Leveraging the change of variables formula
4.1	Uniformization
We start by showing that unambiguously defining outliers and inliers with any density-based approach
becomes impossible when considering a particular type of invertible reparametrization of the problem,
irrespective of dimensionality.
Under weak assumptions, one can map any distribution to a uniform distribution using an invertible
transformation (HyVarinen & Pajunen, 1999). This is in fact a common strategy for sampling from
4
Under review as a conference paper at ICLR 2021
x
(a) An example of a distribution
density PX. Points x with high
density PX (x) are in blue and
points with low density PX (x) are
in red.
0.0	0.2	0.4	0.6	0.8	1.0
x
(b) The corresponding cumulative
distribution function CDFp* .
(叵苫ao)(x) * Ha以
0.0	0.2	0.4	0.6	0.8	1.0
CDFpX (x)
(c) The resulting density from ap-
plying CDFp } to X 〜PX is
PCDFr* (X) = U([0, 1]), there-
pX
fore we color all the points the
same.
Figure 4: Illustration of the one-dimensional case version of a Knothe-Rosenblatt rearrangement,
which is just the application of the cumulative distribution function CDFp* on the variable x.
complicated one-dimensional distributions (Devroye, 1986). Figure 4 shows an example of this where
a bimodal distribution (Figure 4a) is pushed through an invertible map (Figure 4b) to obtain a uniform
distribution (Figure 4c).
To construct this invertible uniformization function, we rely on the notion of Knothe-Rosenblatt
rearrangement (Rosenblatt, 1952; Knothe et al., 1957). A Knothe-Rosenblatt rearrangement (notably
used in Hyvarinen & Pajunen, 1999) is defined for a random variable X distributed according to a
strictly positive density PX with a convex support X, as a continuous invertible map f(KR) from X
onto [0, 1]D such that f(KR) (X) follows a uniform distribution in this hypercube. This rearrangement
is constructed as follows: ∀d ∈ {1, ..., D}, f(KR) (x) = CDFp*	(xd | x<d) where CDFp is
the cumulative distribution function corresponding to the density p.
In these new coordinates, neither the density scoring method nor the typicality test approach can
discriminate between inliers and outliers in this uniform D-dimensional hypercube [0, 1]D. Since the
resulting density Pf (KR)(X) = 1 is constant, the density scoring method attributes the same regularity
to every point. Moreover, a typicality test on f(KR) (X) will always succeed as
1N
∀e > 0,N ∈ N*, V (x(n))n≤N , H (Pf(KR)(X)) + N X logPf(KR)(X) (f (KR) (X⑺))
n=1
1 N
=H (U ([0,1]D)) + NFllog⑴=0 ≤ &
However, these uniformly distributed points are merely a different representation of the same initial
points. Therefore, if the identity of the outliers is ambiguous in this uniform distribution, then
anomaly detection in general should be as difficult.
4.2 Arbitrary scoring
While a particular parametrization can prevent density-based outlier detection methods from separat-
ing between outliers and inliers, we find that it is also possible to build a reparametrization of the
problem to impose to each point an arbitrary density level in the new representation. To illustrate this
idea, consider some points from a distribution whose density is depicted in Figure 5a and a score
function indicated in red in Figure 5b. In this example, high-density regions correspond to areas
with low score value (and vice-versa). We show that there exists a reparametrization (depicted in
Figure 5c) such that the density in this new representation (Figure 5d) now matches the desired score,
which can be designed to mislead density-based methods into a wrong classification of anomalies.
5
Under review as a conference paper at ICLR 2021
(a) An example of a dis-
tribution density PX.
(b) The distribution PX
(in black) and the de-
sired density scoring s
(in red).
(c) A continuous
invertible reparametriza-
tion f (s) such that
Pf(S) (Xz (S)(X))=
s(x).
(d) Resulting density
Pf(S)(X) from applying
f(S) to X 〜PX as a
function of f(s) (x).
Figure 5:	Illustration of how we can modify the space with an invertible function so that each point x
follows a predefined score. In Figures 5a and 5d, points X with high density PX (x) are in blue and
points with low density PX (x) are in red.
Proposition 1. For any variable X 〜PX with PX continuous strictly positive (with X convex) and
any measurable continuous function S : X → R* bounded below by a strictly positive number,
there exists a continuous bijection f(s) such that for any x ∈ X, Pf(S) (X) f(s) (x) = s(x) almost
everywhere.
Proof. We write x to denote (x1, . . . , xD-1, xD) and (x<D, t) for (x1, . . . , xD-1, t). Let f(s) :
X → Z ⊂ RD be a function such that
xD
f j=/
PX ((X<D,t))
s (x<D, t)
and ∀d ∈ {1,..., D - 1}, f(s) (X) d = Xd. As s is bounded below,f(s) is well defined and invertible.
By the change of variables formula,
∀x ∈ x,	Pf(S)(X)(/(S)(X))	=	Px(X)	Jddfɪ(X)	=	PX(X)	∙	(IpX(X))	=	s(x).
∂ X	s(X)
□
If Xin and Xout are respectively the true sets of inliers and outliers, we can pick a ball A ⊂ Xin
such that Pχ (A) = α < 0.5, we can choose S such that for any X ∈ (X \ A),s(x) = 1 and for any
X ∈ A, s(X) = 0.1. With this choice of s (or a smooth approximation) and the functionf (s) defined
earlier, both the density scoring and the (one-sample) typical set methods will consider the set of
inliers to be (X \ A) while Xout ⊂ (X \ A), making their results completely wrong. While we can
also reparametrize the problem so that these methods may succeed, such reparametrization requires
knowledge of (PX/s)(x). Without any constraints on the space considered, individual densities can
be arbitrarily manipulated, which reveals how little these quantities say about the underlying outcome
in general.
4.3 Canonical distribution
Since our analysis in Subsections 4.1 and 4.2 reveals that densities or low typicality regions are not
sufficient conditions for an observation to be an anomaly, whatever its distribution or its dimension,
we are now interested in investigating whether additional realistic assumptions can lead to some
guarantees for anomaly detection. Motivated by several representation learning algorithms which
attempt to learn a mapping to a predefined distribution (e.g., a standard Gaussian, see Chen &
Gopinath, 2001; Kingma & Welling, 2014; Rezende et al., 2014; Dinh et al., 2014; Krusinga et al.,
2019) we consider the more restricted setting of a fixed distribution of our choice, whose regular
regions could for instance be known. Surprisingly, we find that it is possible to exchange the densities
of an inlier and an outlier even within a canonical distribution.
6
Under review as a conference paper at ICLR 2021
(a) Points Xin and Xout in a uniformly distributed
subset. f (rot) will pick a two-dimensional plane and
use the polar coordinate using the mean Xm of Xin
and Xout as the center.
(b) Applying a bijection f (rot) exchanging the points
Xin and Xout . f (rot) is a rotation depending on the
distance from the mean Xm of Xin and Xout in the
previously selected two-dimensional plane.
Figure 6:	Illustration of the norm-dependent rotation, a locally-acting bijection that allows us to swap
two different points while preserving a uniform distribution (as a volume-preserving function).
Proposition 2. For any strictly positive density function PX over a convex space X ⊆ RD with
D > 2, for any xin, xout in the interior Xo ofX, there exists a continuous bijection f : X → X
such that PX = Pf(X),Pf(X) f (xCin)))= PX (X(Out)) ,and Pf(X) (f (x(Out))) = PX (x(in)) ∙
We provide a sketch of proof and put the details in Appendix A. We rely on the transformation
depicted in Figure 6, which can swap two points while acting in a very local area. If the distribution of
points is uniform inside this local area, then this distribution will be unaffected by this transformation.
In order to arrive at this situation, we use the uniformization method presented in Subsection 4.1,
along with a linear function to fit this local area inside the support of the distribution (see Figure 7).
Once those two points have been swapped, we can reverse the functions preceding this swap to
recover the original distribution overall.
Since the resulting distribution Pf(X) is identical to the original fX, then their entropies are the same
H(Pf(X)) = H fX). Hence, whenxin and xOut are respectively an inlier and an outlier, whether
in terms of density scoring or typicality, there exists a reparametrization of the problem conserving
the overall distribution while still exchanging their status as inlier/outlier. We provide an example
applied to a standard Gaussian distribution in Figure 8.
(a) When taking two points Xin and Xout inside the
hypercube [0, 1]D, there is sometimes no L2-ball cen-
tered in their mean Xm containing both Xin and Xout.
L(Xout)
(b) However, given Xin and Xout, one can apply an in-
vertible linear transformation L such that there exists
a L2-ball centered in their new mean L(Xm) contain-
ing both L(Xin) and L(Xout). If the distribution was
uniform inside [0, 1]D, then it is now also uniform
inside L ([0,1]D)
Figure 7:	We illustrate how, given xin and xOut in a uniformly distributed hypercube [0, 1]D, one
can modify the space such that f (rOt) shown in Figure 6 can be applied without modifying the
distribution.
7
Under review as a conference paper at ICLR 2021
x1	f(x) 1	f(x) 1
(a) Points sampled from PX = (b) Applying a bijection f that Pre-(C) The original distribution PX with
N(0, I2).	serves the distribution Pf(X) = respect to the new coordinates f (x),
N(0, I2) to the points in Figure 8a. PX ◦ f-1.
Figure 8:	Application of a transformation using the bijection in Figure 6 to a standard Gaussian
distribution N(0, I2), leaving it overall invariant.
This result is important from a representation learning perspective and a complement to the general
non-identifiability result in several representation learning approaches (Hyvarinen & Pajunen, 1999;
Locatello et al., 2019). It means that learning a representation with a predefined, well-known
distribution and knowing the true density PX are not sufficient conditions to control the individual
density of each point and accurately distinguish outliers from inliers.
5 Discussion
Fundamentally, density-based methods for anomaly detection rely on the belief that density, as a
quantity, conveys useful information to assess whether an outcome is an outlier or not. For example,
several density-based methods operate in practice on features learned independently from the anomaly
detection task (Lee et al., 2018; Krusinga et al., 2019; Morningstar et al., 2020; Winkens et al., 2020)
or on the original input features (Nalisnick et al., 2018; Hendrycks et al., 2018; Kirichenko et al.,
2020; Rudolph et al., 2020; Nalisnick et al., 2019). In general, there is no evidence that the density in
these representations will carry any useful information for anomaly detection bringing into question
whether performance of probabilistic models on this task (e.g., Du & Mordatch, 2019; Grathwohl
et al., 2019; Kirichenko et al., 2020; Liu & Abbeel, 2020) reflects goodness-of-fit of the density
model. On the contrary, we have proven in this paper that density-based anomaly detection methods
are inconsistent across a range of possible representations 2, even under strong constraints on the
distribution, which suggests that finding the right input representation for meaningful density-based
anomaly detection requires privileged information, as discussed in Subsection 4.2. Moreover, several
papers have pointed to existing problems in commonly used input representations; for example, the
geometry of a bitmap representation does not follow our intuition of semantic distance (Theis et al.,
2016), or images can come from photographic sensors tuned to specific populations (Roth, 2009;
Buolamwini & Gebru, 2018). This shows how strong of an otherwise understated assumption it is
to suppose that the methods presented in Subsection 2.2 and Subsection 2.3 would work on input
representations. This is particularly problematic for applications as critical as autonomous vehicle
navigation or medical decision-making.
While defining anomalies might be impossible without prior knowledge (Winkens et al., 2020)
as out-of-distribution detection is an ill-posed problem (Choi et al., 2018; Nalisnick et al., 2019;
Morningstar et al., 2020), several approaches make these assumptions more explicit. For instance, the
density scoring method has also been interpreted in Bishop (1994) as a likelihood ratio method (Ren
et al., 2019; Serra et al., 2020; Schirrmeister et al., 2020), which, on the one hand, relies heavily on
the definition of an arbitrary reference density as a denominator of this ratio but, on the other hand, is
invariant to reparametrization. Inspired by the Bayesian approach from Choi et al. (2018), one can
work on defining a prior distribution on possible reparametrizations over which to average (similarly
to J0rgensen & Hauberg, 2020).
2Alternatively, this can be seen as a change of base distribution used to define a probability density as a
Radon-Nikodym derivative.
8
Under review as a conference paper at ICLR 2021
References
Christopher M Bishop. Novelty detection and neural network validation. IEE Proceedings-Vision,
Image and Signal processing,141(4):217-222,1994.
David Blei, Katherine Heller, Tim Salimans, Max Welling, , and Zoubin Ghahramani. Panel: On
the foundations and future of approximate inference. In Symposium on Advances in Approxi-
mate Bayesian Inference, AABI 2017, 2017. URL https://www.youtube.com/watch?
v=x1UByHT60mQ&feature=youtu.be&t=46m2s.
Avrim Blum, John Hopcroft, and Ravindran Kannan. Foundations of data science. Vorabversion
eines Lehrbuchs, 5, 2016.
Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in neural
information processing systems, pp. 161-168, 2008.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commercial
gender classification. In Conference on fairness, accountability and transparency, pp. 77-91, 2018.
Nicholas Carlini and David Wagner. Adversarial examples are not easily detected: Bypassing ten
detection methods. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and
Security, pp. 3-14, 2017.
Scott Saobing Chen and Ramesh A. Gopinath. Gaussianization. In T. K. Leen, T. G. Dietterich, and
V. Tresp (eds.), Advances in Neural Information Processing Systems 13, pp. 423-429. MIT Press,
2001. URL http://papers.nips.cc/paper/1856-gaussianization.pdf.
Hyunsun Choi, Eric Jang, and Alexander A Alemi. Waic, but why? generative ensembles for robust
anomaly detection. arXiv preprint arXiv:1810.01392, 2018.
Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th
conference on Winter simulation, pp. 260-265. ACM, 1986.
Sander Dieleman. Musings on typicality, 2020. URL https://benanne.github.io/2020/
09/01/typicality.html.
Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components
estimation. arXiv preprint arXiv:1410.8516, 2014.
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv
preprint arXiv:1605.08803, 2016.
Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In
Advances in Neural Information Processing Systems, pp. 3608-3618, 2019.
Ethan Fetaya, Joern-Henrik Jacobsen, Will Grathwohl, and Richard Zemel. Understanding the limita-
tions of conditional generative models. In International Conference on Learning Representations,
2020.
Angelos Filos, Panagiotis Tigas, Rowan McAllister, Nicholas Rhinehart, Sergey Levine, and Yarin
Gal. Can autonomous vehicles identify, recover from, and adapt to distribution shifts? arXiv
preprint arXiv:2006.14911, 2020.
Justin Fu, John Co-Reyes, and Sergey Levine. Ex2: Exploration with exemplar models for deep
reinforcement learning. In Advances in neural information processing systems, pp. 2577-2587,
2017.
Will Grathwohl, Kuan-Chieh Wang, Jorn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi,
and Kevin Swersky. Your classifier is secretly an energy based model and you should treat it like
one. arXiv preprint arXiv:1912.03263, 2019.
Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,
2013.
9
Under review as a conference paper at ICLR 2021
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=HJz6tiCqYm.
Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
exposure. arXiv preprint arXiv:1812.04606, 2018.
Victoria Hodge and Jim Austin. A survey of outlier detection methodologies. Artificial intelligence
review, 22(2):85-126, 2004.
AaPo Hyvarinen and Petteri Pajunen. Nonlinear independent component analysis: Existence and
uniqueness results. Neural networks, 12(3):429-439, 1999.
Martin J0rgensen and S0ren Hauberg. Reparametrization invariance in non-parametric causal
discovery. arXiv preprint arXiv:2008.05552, 2020.
Wilfred Kaplan. Advanced calculus. Pearson Education India, 1952.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR’2014,
arXiv:1312.6114, 2014.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In
Advances in neural information processing systems, pp. 10215-10224, 2018.
Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Why normalizing flows fail to detect
out-of-distribution data. arXiv preprint arXiv:2006.08545, 2020.
Herbert Knothe et al. Contributions to the theory of convex bodies. The Michigan Mathematical
Journal, 4(1):39-52, 1957.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Ryen Krusinga, Sohil Shah, Matthias Zwicker, Tom Goldstein, and David Jacobs. Understanding
the (un) interpretability of natural image distributions using generative models. arXiv preprint
arXiv:1901.01499, 2019.
Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting
out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing
Systems, pp. 7167-7177, 2018.
Hao Liu and Pieter Abbeel. Hybrid discriminative-generative training via contrastive learning. arXiv
preprint arXiv:2007.09070, 2020.
Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection.
Advances in Neural Information Processing Systems (NeurIPS), 2020.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of dis-
entangled representations. In international conference on machine learning, pp. 4114-4124,
2019.
Warren R Morningstar, Cusuh Ham, Andrew G Gallagher, Balaji Lakshminarayanan, Alexander A
Alemi, and Joshua V Dillon. Density of states estimation for out-of-distribution detection. arXiv
preprint arXiv:2006.09273, 2020.
Mary M Moya, Mark W Koch, and Larry D Hostetler. One-class classifier networks for target
recognition applications. STIN, 93:24043, 1993.
Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan. Do
deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136, 2018.
Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, and Balaji Lakshminarayanan. Detecting out-of-
distribution inputs to deep generative models using typicality. arXiv preprint arXiv:1906.02994,
2019.
10
Under review as a conference paper at ICLR 2021
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. 2011.
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and
Dustin Tran. Image transformer. Proceedings of Machine Learning Research, pp. 4055-4064,
Stockholmsmassan, Stockholm Sweden, 2018. PMLR.
Marco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty
detection. Signal Processing, 99:215-249, 2014.
Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon, and
Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In Advances in
Neural Information Processing Systems, pp. 14707-14718, 2019.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Proceedings
of Machine Learning Research, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of Machine Learning Research,
2014.
Murray Rosenblatt. Remarks on a multivariate transformation. The annals of mathematical statistics,
23(3):470-472, 1952.
Lorna Roth. Looking at shirley, the ultimate norm: Colour balance, image technologies, and cognitive
equity. Canadian Journal of Communication, 34(1), 2009.
Marco Rudolph, Bastian Wandt, and Bodo Rosenhahn. Same same but differnet: Semi-supervised
defect detection with normalizing flows. arXiv preprint arXiv:2008.12577, 2020.
Robin Tibor Schirrmeister, Yuxuan Zhou, Tonio Ball, and Dan Zhang. Understanding anomaly
detection with deep invertible networks through hierarchies of distributions and features. arXiv
preprint arXiv:2006.10848, 2020.
Bernhard Scholkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.
Estimating the support of a high-dimensional distribution. Neural computation, 13(7):1443-1471,
2001.
Joan Serra, David Alvarez, ViCenC Gomez, Olga Slizovskaia, Jose F. NUnez, and Jordi Luque.
Input complexity and out-of-distribution detection with likelihood-based generative models. In
International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=SyxIWpVYvr.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Esteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algorithms.
Communications on Pure and Applied Mathematics, 66(2):145-164, 2013.
Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative
models. In ICLR’2016 arXiv:1511.01844, 2016.
Benigno Uria, Iain Murray, and Hugo Larochelle. A deep and tractable density estimator. In
International Conference on Machine Learning, pp. 467-475, 2014.
Arash Vahdat and Jan Kautz. Nvae: A deep hierarchical variational autoencoder. arXiv preprint
arXiv:2007.03898, 2020.
Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional
image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems,
pp. 4790-4798, 2016a.
Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.
In International Conference on Machine Learning, pp. 1747-1756, 2016b.
11
Under review as a conference paper at ICLR 2021
Roman Vershynin. High-dimensional probability, 2019.
Jim Winkens, Rudy Bunel, Abhijit Guha Roy, Robert Stanforth, Vivek Natarajan, Joseph R Ledsam,
Patricia MacWilliams, Pushmeet Kohli, Alan Karthikesalingam, Simon Kohl, et al. Contrastive
training for improved out-of-distribution detection. arXiv preprint arXiv:2007.05566, 2020.
Hongjie Zhang, Ang Li, Jie Guo, and Yanwen Guo. Hybrid models for open set recognition. arXiv
preprint arXiv:2003.12506, 2020.
Rui Zhao and Volker Tresp. Curiosity-driven experience prioritization via density estimation. arXiv
preprint arXiv:1902.08039, 2019.
12
Under review as a conference paper at ICLR 2021
A	Proof of Proposition 2
Proposition 3. For any strictly positive density function PX over a convex space X ⊆ RD with
D > 2, for any xin, xout in the interior Xo ofX, there exists a continuous bijection f : X → X
such that PX = Pf(X),Pf(X) (f (xCin)))= PX (X(Out)) ,and Pf(X) f …%=PX 3in)) ∙
Proof. Our proof will rely on the following non-rigid rotation f (rot). Working in a hyperspherical
coordinate system consisting of a radial coordinate r > 0 and (D - 1) angular coordinates (φi)i<D,
d-1
∀d < D, xd = r	sin(φi) cos(φd)
Di=-12
xD = r	sin(φi) sin(φD-1),
where for all i ∈ {1, 2, ..., D - 2}, φi ∈ [0, π) and φD-1 ∈ [0, 2π), given rmax > r0 > 0, we define
the continuous mapping f (rot) as:
f (rot)((r, φ1,...,φD-2 ,ΦD-ι)) = (r,φι,... ,φD-2, φD-ι + ∏ (rmaxr)+ [mod 2π]
rmax - r0
where (•)+ = max(∙, 0). This mapping only affects points inside B2(0, rmax), and exchanges two
points corresponding to (r0, φ1, . . . , φD-2, φD-1) and (r0, φ1, . . . , φD-2, φD-1 +π) in a continous
way (see Figure 6). Since the Jacobian determinant of the hyperspherical coordinates transformation
is not a function of φD-1, f (rot) is volume-preserving in cartesian coordinates.
Let f(KR) be a Knothe-Rosenblatt rearrangement of PX, f (KR)(X) is uniformly distributed in [0,1]D.
Let z(in) = f(KR) (x(in)) and Z(Out) = f(KR) (X(Out)). Since f(KR) is continuous, z(in), z(Out)
are in the interior (0, 1)D. Therefore, there is an > 0 such that the L2-balls B? (z(in), e) and
B2 z(Out), are inside (0, 1)D. Since (0, 1)D is convex, so is their convex hull.
Let ro = 1 ∣∣z(in) - Z(Out) ∣∣2 and rmax = ro + 邑 Given Z ∈ (0,1)D, we write Zk and z⊥ to denote
its parallel and orthogonal components with respect to z(in) - z(Out) . We consider the linear
bijection L defined by
L(Z) = Zk +	rmaxZ⊥ .
Let f(z) = L ◦ f(KR). Since L is a linear function (i.e., with constant Jacobian), f(z) (X) is
uniformly distributed inside L ([0,1]D). If Zlm) is the mean of ZIin and Z(Out), then f(Z)(X)
contains B2 L Z(m) , rmax (see Figure 7). We can then apply the non-rigid rotation f (rOt) defined
earlier, centered on L Z(m) to exchange L Z(in) and L Z(Out) while maintaining this uniform
distribution.
We can then apply the bijection (f (Z)) 1 to obtain the invertible map f = (f (Z)) 1 ◦ f(rOt) ◦ f(Z)
such that Pf (X)= fX, Pf(X) (f (x(in))) = PX (X(Out)),and Pf (f (X(Out))) = PX (X(in)). □
13