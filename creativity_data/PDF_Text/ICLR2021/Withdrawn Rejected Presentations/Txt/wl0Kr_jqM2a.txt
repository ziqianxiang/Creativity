Under review as a conference paper at ICLR 2021
Testing Robustness Against Unforeseen Ad-
VERSARIES
Anonymous authors
Paper under double-blind review
Ab stract
Most existing adversarial defenses only measure robustness to Lp adversarial
attacks. Not only are adversaries unlikely to exclusively create small Lp perturba-
tions, adversaries are unlikely to remain fixed. Adversaries adapt and evolve their
attacks; hence adversarial defenses must be robust to a broad range of unforeseen
attacks. We address this discrepancy between research and reality by proposing
a new evaluation framework called ImageNet-UA. Our framework enables
the research community to test ImageNet model robustness against attacks not
encountered during training. To create ImageNet-UA’s diverse attack suite, we
introduce a total of four novel adversarial attacks. We also demonstrate that,
in comparison to ImageNet-UA, prevailing L∞ robustness assessments give a
narrow account of adversarial robustness. By evaluating current defenses with
ImageNet-UA, we find they provide little robustness to unforeseen attacks. We
hope the greater variety and realism of ImageNet-UA enables development of
more robust defenses which can generalize beyond attacks seen during training.
1	Introduction
Neural networks perform well on many datasets (He et al., 2016) yet can be consistently fooled by
minor adversarial distortions (Goodfellow et al., 2014). The research community has responded by
quantifying and developing adversarial defenses against such attacks (Madry et al., 2017), but these
defenses and metrics have two key limitations.
First, the vast majority of existing defenses exclusively defend against and quantify robustness to
Lp-constrained attacks (Madry et al., 2017; Cohen et al., 2019; Raff et al., 2019; Xie et al., 2018).
Though real-world adversaries are not Lp constrained (Gilmer et al., 2018) and can attack with
diverse distortions (Brown et al., 2017; Sharif et al., 2019), the literature largely ignores this and
evaluates against the Lp adversaries already seen during training (Madry et al., 2017; Xie et al.,
2018), resulting in optimistic robustness assessments. The attacks outside the Lp threat model that
have been proposed (Song et al., 2018; Qiu et al., 2019; Engstrom et al., 2017; Evtimov et al., 2017;
Sharif et al., 2016) are not intended for general defense evaluation and suffer from narrow dataset
applicability, difficulty of optimization, or fragility of auxiliary generative models.
Second, existing defenses assume that attacks are known in advance (Goodfellow, 2019) and use
knowledge of their explicit form during training (Madry et al., 2017). In practice, adversaries can
deploy unforeseen attacks not known to the defense creator. For example, online advertisers use
attacks such as perturbed pixels in ads to defeat ad blockers trained only on the previous generation
of ads in an ever-escalating arms race (Tramer et al., 2018). However, current evaluation setups
implicitly assume that attacks encountered at test-time are the same as those seen at train-time,
which is unrealistic. The reality that future attacks are unlike those encountered during training
is akin to a train-test distribution mismatch—a problem studied outside of adversarial robustness
(Recht et al., 2019; Hendrycks & Dietterich, 2019)—but now brought to the adversarial setting.
The present work addresses these limitations by proposing an evaluation framework ImageNet-UA
to measure robustness against unforeseen attacks. ImageNet-UA assesses a defense which may
have been created with knowledge of the commonly used L∞ or L2 attacks with six diverse at-
tacks (four of which are novel) distinct from L∞ or L2 . We intend these attacks to be used at
test-time only and not during training. Performing well on ImageNet-UA thus demonstrates gen-
eralization to a diverse set of distortions not seen during defense creation. While ImageNet-UA
1
Under review as a conference paper at ICLR 2021
s*o2lv SnOIAgd s*o2lv MDN Jno
L∞	L2	L1	Elastic
JPEG	Fog	Snow	Gabor
Figure 1: Adversarially distorted chow chow dog images created with old attacks and our new
attacks. The JPEG, Fog, Snow, and Gabor adversarial attacks are visually distinct from previous
attacks, result in distortions which do not obey a small Lp norm constraint, and serve as unforeseen
attacks for the ImageNet-UA attack suite.
does not provide an exhaustive guarantee over all conceivable attacks, it evaluates over a diverse
unforeseen test distribution similar to those used successfully in other studies of distributional shift
(Rajpurkar et al., 2018; Hendrycks & Dietterich, 2019; Recht et al., 2019). ImageNet-UA works
for ImageNet models and can be easily used with our code available at https://github.com/
anon-submission-2020/anon-submission-2020.
Designing ImageNet-UA requires new attacks that are strong and varied, since real-world attacks
are diverse in structure. To meet this challenge, we contribute four novel and diverse adversarial
attacks which are easily optimized. Our new attacks produce distortions with occlusions, spatial
similarity, and simulated weather, all of which are absent in previous attacks. Performing well on
ImageNet-UA thus demonstrates that a defense generalizes to a diverse set of distortions distinct
from the commonly used L∞ or L2 .
With ImageNet-UA, we show weaknesses in existing evaluation practices and defenses through a
study of 8 attacks against 48 models adversarially trained on ImageNet-100, a 100-class subset
of ImageNet. While most adversarial robustness evaluations use only L∞ attacks, ImageNet-UA
reveals that models with high L∞ attack robustness can remain susceptible to other attacks. Thus,
L∞ evaluations are a narrow measure of robustness, even though much of the literature treats this
evaluation as comprehensive (Madry et al., 2017; Qian & Wegman, 2019; Schott et al., 2019; Zhang
et al., 2019). We address this deficiency by using the novel attacks in ImageNet-UA to evaluate
robustness to a more diverse set of unforeseen attacks. Our results demonstrate that L∞ adversarial
training, the current state-of-the-art defense, has limited generalization to unforeseen adversaries,
and is not easily improved by training against more attacks. This adds to the evidence that achieving
robustness against a few train-time attacks is insufficient to impart robustness to unforeseen test-time
attacks (Jacobsen et al., 2019; Jordan et al., 2019; Tramer & Boneh, 2019).
In summary, we propose the framework ImageNet-UA to measure robustness to a diverse set of
attacks, made possible by our four new adversarial attacks. Since existing defenses scale poorly
to multiple attacks (Jordan et al., 2019; Tramer & Boneh, 2019), finding defense techniques which
generalize to unforeseen attacks is crucial to create robust models. We suggest ImageNet-UA as a
way to measure progress towards this goal.
2	Related Work
Adversarial robustness is notoriously difficult to correctly evaluate (Papernot et al., 2017; Athalye
et al., 2018a). To that end, Carlini et al. (2019a) provide extensive guidance for sound adversarial
robustness evaluation. By measuring attack success rates across several distortion sizes and using a
broader threat model with diverse differentiable attacks, ImageNet-UA has several of their recom-
mendations built-in, while greatly expanding the set of attacks over previous work on evaluation.
2
Under review as a conference paper at ICLR 2021
Randomly Initialized
Adversarially
Randomly Initialized
Adversarially
Randomly Initialized
Snow
Otter (100.0%)
Adversarially
Optimized Snow
Loafer (98.0%)
Randomly Initialized
Gabor
Otter (100.0%)
Adversarially
Optimized Gabor
Zebra (100.0%)
Figure 2: Randomly sampled distortions and adversarially optimized distortions from our new at-
tacks, targeted to the target class in red. Stochastic average-case versions of our attacks affect
classifiers minimally, while adversarial versions are optimized to reveal high-confidence errors. The
snowflakes in Snow decrease in intensity after optimization, demonstrating that lighter adversarial
snowflakes are more effective than heavy random snowfall at uncovering model weaknesses.
We are only aware of a few prior works which evaluate on unforeseen attacks in specific limited
circumstances. Wu et al. (2020) evaluate against physically-realizable attacks from Evtimov et al.
(2017) and Sharif et al. (2016), though this limits the threat model to occlusion attacks on narrow
datasets. Outside of vision, Pierazzi et al. (2020) proposes constraining attacks by a more diverse set
of problem-space constraints in diverse domains such as text and malware or source code generation;
however, even in this framework, analytically enumerating all such constraints is impossible.
Within vision, prior attacks outside the Lp threat model exist, but they lack the general applicability
and fast optimization of ours. Song et al. (2018) and Qiu et al. (2019) attack using variational
autoencoders and StarGANs, respectively, resulting in weaker attacks which require simple image
distributions suitable for VAEs and GANs. Engstrom et al. (2017) apply Euclidean transformations
determined by brute-force search. Zhao et al. (2019) use perceptual color distances to align human
perception and L2 perturbations. Evtimov et al. (2017) and Sharif et al. (2016) attack stop signs
and face-recognition systems with carefully placed patches or modified eyeglass frames, requiring
physical object creation and applying only to specific image types.
3	New Attacks for a B roader Threat Model
There are few diverse, easily optimizable, plug-and-play adversarial attacks in the current literature;
outside of Elastic (Xiao et al., 2018), most are Lp attacks such as L∞ (Goodfellow et al., 2014), L2
(Szegedy et al., 2013; Carlini & Wagner, 2017), L1 (Chen et al., 2018). We rectify this deficiency
with four novel adversarial attacks: JPEG, Fog, Snow, and Gabor. Our attacks are differentiable and
fast, while optimizing over enough parameters to be strong. We show example adversarial images
in Figure 1 and compare stochastic and adversarial distortions in Figure 2.
Our novel attacks provide a range of test-time adversaries visually and semantically distinct from
L∞ and L2 attacks. Namely, they cause distortions with large L∞ and L2 norm, but result in images
that are perceptually close to the original. These attacks are intended as unforeseen attacks not used
during training, allowing them to evaluate whether a defense can generalize from L∞ or L2 to
a more varied set of distortions than current evaluations. Though our attacks are not exhaustive,
performing well against them already demonstrates robustness to occlusion, spatial similarity, and
simulated weather, which are absent from previous evaluations.
3
Under review as a conference paper at ICLR 2021
Our attacks create an adversarial image x0 from a clean image x with true label y. Let model f map
images to a softmax distribution, and let `(f (x), y) be the cross-entropy loss. Given a target class
y0 6= y, our attacks attempt to find a valid image x0 such that (1) the attacked image x0 is obtained
by applying a distortion (of size controlled by a parameter ε) to x, and (2) the loss `(f (x0), y0) is
minimized. An unforeseen adversarial attack is a white- or black-box adversarial attack unknown to
the defense designer which does not change the true label of x according to an oracle or human.
3.1	Four New Unforeseen Attacks
JPEG. JPEG applies perturbations in a JPEG-encoded space of compressed images rather than
raw pixel space. More precisely, JPEG compression is a linear transform JPEG which applies color-
space conversion, the discrete cosine transform, and then quantization. Our JPEG attack imposes
the L∞ -constraint
kJPEG(x) - JPEG(x0)k∞ ≤ε
on the attacked image x0. We optimize z = JPEG(x0) under this constraint to find an adversarial
perturbation in the resulting frequency space. The perturbed frequency coefficients are quantized,
and we then apply a right-inverse of JPEG to obtain the attacked image x0 in pixel space. We use
ideas from Shin & Song (2017) to make this differentiable. The resulting attack is conspicuously
distinct from Lp attacks.
Fog. Fog simulates worst-case weather conditions. Robustness to adverse weather is a safety crit-
ical priority for autonomous vehicles, and Figure 2 shows Fog provides a more rigorous stress-test
than stochastic fog (Hendrycks & Dietterich, 2019). Fog creates adversarial fog-like occlusions by
adversarially optimizing parameters in the diamond-square algorithm (Fournier et al., 1982) typi-
cally used to render stochastic fog effects.
This algorithm starts with random perturbations to the four corner pixels of the image. At step t, it
iteratively perturbs pixels at the centers of squares and diamonds formed by those pixels perturbed at
step t- 1. The perturbation of a step t pixel is the average of the neighboring step t- 1 perturbations
plus a parameter value which we adversarially optimize. We continue this process until all pixels
have been perturbed; the outcome is a fog-like distortion to the original image.
Snow. Snow simulates snowfall with occlusions of randomly located small image regions rep-
resenting snowflakes. Because the distortions caused by snowflakes are not differentiable in their
locations, we instead place occlusions representing snowflakes at randomly chosen locations and
orientations and adversarially optimize their intensities. This choice results in a fast, differentiable,
and strong attack. Compared to synthetic stochastic snow (Hendrycks & Dietterich, 2019), our ad-
versarial snow is faster and includes snowflakes at differing angles. Figure 2 shows adversarial snow
exposes model weaknesses more effectively than the easier stochastic, average-case snow.
Gabor. Gabor spatially occludes the image with visually diverse Gabor noise Lagae et al. (2009).
Gabor noise is a form of band-limited anisotropic procedural noise which convolves a parameter
mask with a Gabor kernel which is a product ofa Gaussian kernel and a harmonic kernel. We choose
the Gabor kernel randomly and adversarially optimize the parameters of the mask starting from a
sparse initialization. We apply spectral variance normalization (Co et al., 2019) to the resulting
distortion and add it to the input image to create the attack.
3.2	Improving Existing Attacks
Elastic modifies the attack of Xiao et al. (2018); it warps the image by distortions x0 = Flow(x, V ),
where V : {1, . . . , 224}2 → R2 is a vector field on pixel space, and Flow sets the value of pixel
(i, j) to the bilinearly interpolated original value at (i, j) + V (i, j). We construct V by smoothing a
vector field W by a Gaussian kernel (size 25 × 25, σ ≈ 3 for a 224 × 224 image) and optimize W
under kW (i, j)k∞ ≤ ε for all i,j. The resulting attack is suitable for large-scale images.
The other three attacks are L1 , L2, L∞ attacks, but we improve the L1 attack. For L∞ and L2
constraints, we use randomly-initialized projected gradient descent (PGD), which applies gradient
descent and projection to the L∞ and L2 balls (Madry et al., 2017). Projection is difficult for L1,
and previous Li attacks rely on computationally intensive methods for it (Chen et al., 2018; Tramer
& Boneh, 2019). We replace PGD with the Frank-Wolfe algorithm (Frank & Wolfe, 1956), which
4
Under review as a conference paper at ICLR 2021
0	2000	4000
-2 distortion size
(a) L2 vs. L2-training
0	5	10	15
Elastic distortion size
(b) Elastic vs. L2-training
Figure 3: Accuracies of L2 and Elastic attacks at different distortion sizes against a ResNet-50
model adversarially trained against L2 at ε = 9600 on ImageNet-100. At small distortion sizes, the
model appears to defend well against Elastic, but large distortion sizes reveal that robustness does
not transfer from L2 to Elastic.
optimizes a linear function instead of projecting at each step (pseudocode in Appendix D). This
makes our L1 attack more principled than previous implementations.
4	ImageNet-UA: MEASURING ROBUSTNESS TO UNFORESEEN ATTACKS
We propose the framework ImageNet-UA and its CIFAR-10 analogue CIFAR-10-UA to measure
and summarize model robustness while fulfilling the following desiderata: (1) defenses should be
evaluated against a broad threat model through a diverse set of attacks, (2) defenses should exhibit
generalization to attacks not exactly identical to train-time attacks, and (3) the range of distortion
sizes used for an attack must be wide enough to avoid misleading conclusions caused by overly
weak or strong versions of that attack (Figure 3).
The ImageNet-UA evaluation framework aggregates robustness information into a single measure,
the mean Unforeseen Adversarial Robustness (mUAR). The mUAR is an average over six different
attacks of the Unforeseen Adversarial Robustness (UAR), a metric which assesses the robustness
of a defense against a specific attack by using a wide range of distortion sizes. UAR is normalized
using a measure of attack strength, the ATA, which we now define.
Adversarial Training Accuracy (ATA). The Adversarial Training Accuracy ATA(A, ε) estimates
the strength of an attack A against adversarial training (Madry et al., 2017), one of the strongest
known defense methods. For a distortion size ε, it is the best adversarial test accuracy against A
achieved by adversarial training against A. We allow a possibly different distortion size ε0 during
training, since this can improves accuracy, and we choose a fixed architecture for each dataset.
For ImageNet-100, we choose ResNet-50 for the architecture, and for CIFAR-10 we choose ResNet-
56. When evaluating a defense with architecture other than ResNet-50 or ResNet-56, we recommend
using ATA values computed with these architectures to enable consistent comparison. To estimate
ATA(A, ε) in practice, we evaluate models adversarially trained against distortion size ε0 for ε0 in a
large range (we describe this range at this section’s end).
UAR: Robustness Against a Single Attack. The UAR, a building block for the mUAR, averages a
model’s robustness to a single attack over six distortion sizes ε1, . . . , ε6 chosen for each attack (we
describe the selection procedure at the end of this section). It is defined as
UAR(A) := 100 ×
Pk=IACC(A,εk,M)
Pk=ι ATA(A,εk)
(1)
where ACC(A, εk, M) is the accuracy ACC(A, εk, M) of a model M after attack A at distortion size
εk. The normalization in (1) makes attacks of different strengths more commensurable in a stable
way. We give values of ATA(A, εk) and εk for our attacks on ImageNet-100 and CIFAR-10 in
Tables 4 and 5 (Appendix B), allowing computation of UAR of a defense against a single attack
with six adversarial evaluations and no adversarial training.
mUAR: Mean Unforeseen Attack Robustness. We summarize a defense’s performance on
ImageNet-UA with the mean Unforeseen Attack Robustness (mUAR), an average of UAR scores
5
Under review as a conference paper at ICLR 2021
Defense Robustness Under Different Attacks
φsu4sφα pφu-e.JI=∙-,Jes-Jφ>p4
None
L∞
L2
Li
JPEG
Elastic
Fog
Snow
Gabor
7	17	22	0	31	16	10	5
88	42	15	14	49	20	37	55
80	88	79	67	48	18	38	53
62	71	89	56	43	18	31	47
65	70	54	92	40	19	31	52
23 25 11 1191 25 40 41
1	3	8	0	28	91	43	54
13	15	9	1	39	37	93	60
12	19	14	0	39	29	40	82
Figure 5: UAR(L∞) and mUAR for L∞-trained models
at different distortion sizes. Increasing distortion size in
L∞-training improves UAR(L∞) but hurts the mUAR,
suggesting models heavily fit L∞ at the cost of general-
ization.
Adversarial Attack
Figure 4: UAR for adv trained de-
fenses (row) against attacks (col) on
ImageNet-100. Defenses from L∞ to
Gabor were trained with ε = 32, 4.8k,
612k, 2, 16, 8192, 8, and 1.6k.
for the L1 , Elastic, JPEG, Fog, Snow, and Gabor attacks:
mUAR := 1 [uAR(L1) + UAR(Elastic) + UAR(JPEG)+UAR(Fog)+UAR(Snow) + UAR(Gabor)i.
Our measure mUAR estimates robustness to a broad threat model containing six unforeseen attacks
at six distortion sizes each, meaning high mUAR requires generalization to several held-out attacks.
In particular, it cannot be achieved by the common practice of engineering defenses to a single
attack, which Figure 4 shows does not necessarily provide robustness to different attacks.
Our four novel attacks play a crucial role in mUAR by allowing us to estimate robustness to a
sufficiently large set of adversarial attacks. As is customary when studying train-test mismatches and
distributional shift, we advise against adversarially training with these six attacks when evaluating
ImageNet-UA to preserve the validity of mUAR, though we encourage training with other attacks.
Distortion Sizes. We explain the ε0 values used to estimate ATA and the choice of ε1 , . . . , ε6 used
to define UAR. This calibration of distortion sizes adjusts for the fact (Figure 3) that adversarial
robustness against an attack may vary drastically with distortion size. Further, the relation between
distortion size and attack strength varies between attacks, so too many or too few εk values in a
certain range may cause an attack to appear artificially strong or weak according to UAR.
We choose distortion sizes between εmin and εmax as follows. The minimum distortion size εmin is
the largest ε for which the adversarial accuracy of an adversarially trained model at distortion size
ε is comparable to that of a model trained and evaluated on unattacked data (for ImageNet-100,
within 3 of 87). The maximum distortion size εmax is the smallest ε which either reduces adversarial
accuracy of an adversarially trained model at distortion size ε below 25 or yields images confusing
humans (adversarial accuracy can remain non-zero in this case).
As is typical in recent work on adversarial examples (Athalye et al., 2018b; Evtimov et al., 2017;
Dong et al., 2019; Qin et al., 2019), our attacks can be perceptible at large distortion sizes. We make
this choice to reflect perceptibility of attacks in real world threat models per Gilmer et al. (2018).
For ATA, we evaluate against models adversarially trained with ε0 increasing geometrically from
εmin to εmax by factors of 2. We then choose εk as follows: We compute ATA at ε increasing
geometrically from εmin to εmax by factors of 2 and take the size-6 subset whose ATA values have
minimum '1-distance to the ATA values of the L∞ attack in Table 4 (Appendix B.1). For example,
for Gabor, (εmin, εmax) = (6.25, 3200), so we compute ATAs at the 10 values ε = 6.25, . . . , 3200.
Viewing size-6 subsets of the ATAs as vectors with decreasing coordinates, we select εk for Gabor
corresponding to the vector with minimum `1 -distance to the ATA vector for L∞ .
6
Under review as a conference paper at ICLR 2021
Table 1: Clean Accuracy, UAR, and mUAR scores for models adv trained against L∞ and L2 attacks.
L∞ training, the most popular defense, provides less robustness than L2 training. Comparing the
highest mUAR achieved to individual UAR values in Figure 4 indicates a large robustness gap.
Clean Accuracy	L∞ L2 mUAR	Clean Accuracy	L∞ L2 mUAR
Normal Training	86.7	7.3 17.2^^140~	Normal Training	86.7	7.3 17.2^^14.0-
L∞ ε = 1	86.2	46.4 54.2 30.7	L2 ε = 150	866	38.0 49.4^^2∏T~
L∞ ε = 2	85.5	59.8 64.4 36.9	L2 ε = 300	85.9	49.7 60.1 33.3
L∞ ε = 4	83.9	72.1 73.6 42.3	L2 ε = 600	84.7	61.9 71.6 40.0
L∞ ε = 8	79.8	82.6 72.0 42.2	L2 ε = 1200	82.3	72.9 82.0 46.8
L∞ ε = 16	74.5	89.1 60.0 37.5	L2 ε = 2400	76.8	79.6 88.5 50.7
L∞ ε = 32	70.8	88.1 41.9 31.8	L2 ε = 4800	68.3	80.4 87.7 50.5
Table 2: Clean Accuracy, UAR, and mUAR scores for models jointly trained against (L∞, L2). Joint
training does not provide much additional robustness.
Clean Accuracy					L∞	L2	mUAR
L∞	ε	1, L2 ε =	300	86.1	50.3	60.2	33.6
L∞	ε	2, L2 ε =	600	85.1	62.8	72.5	41.0
L∞	ε	4, L2 ε =	1200	81.3	72.9	81.2	46.9
L∞	ε	8, L2 ε =	2400	76.5	80.0	87.3	50.8
L∞	ε	16, L2 ε	= 4800	68.4	81.5	87.9	50.9
5	NEW INSIGHTS FROM ImageNet-UA
We use ImageNet-UA to assess existing methods for adversarial defense and evaluation. First,
ImageNet-UA reveals that L∞ trained defenses fail to generalize to different attacks, indicating
substantial weakness in current L∞ adversarial robustness evaluation. We establish a baseline
for ImageNet-UA using L2 adversarial training which is difficult to improve upon by adversarial
training alone. Finally, we show non-adversarially trained models can still improve robustness on
ImageNet-UA over standard models and suggest this as a direction for further inquiry.
5.1	Experimental Setup
We adversarially train 48 models against the 8 attacks from Section 3 and evaluate against targeted
attacks. We use the CIFAR-10 and ImageNet-100 datasets for ImageNet-UA and CIFAR-10-UA.
ImageNet-100 is a 100-class subset of ImageNet-1K (Deng et al., 2009) containing every tenth class
by WordNet ID order; we use a subset of ImageNet-1K due to the high compute cost of adversarial
training. We use ResNet-56 for CIFAR-10 and ResNet-50 from torchvision for ImageNet-100
(He et al., 2016). We provide training hyperparameters in Appendix A.
To adversarially train against attack A, at each mini-batch we select a uniform random (incorrect)
target class for each training image. For maximum distortion size ε, we apply targeted attack A
to the current model with distortion size ε0 〜 Uniform(0, ε) and take a SGD step using only the
attacked images. Randomly scaling ε0 improves performance against smaller distortions.
We train on 10-SteP attacks for attacks other than Elastic, where we use 30 steps due to a harder
optimization. For Lp, JPEG, and Elastic, We use step size ε∕√steps; for Fog, Gabor, and Snow,
We use step size ,0.001/steps because the latent space is independent of ε. These choices have
optimal rates for non-smooth convex functions (Nemirovski & Yudin, 1978; 1983). We evaluate on
200-step targeted attacks with uniform random (incorrect) target, using more steps for evaluation
than training per best practices (Carlini et al., 2019b).
Figure 4 summarizes ImageNet-100 results. Full results for ImageNet-100 and CIFAR-10 are in
Appendix E and robustness checks to random seed and attack iterations are in Appendix F.
5.2	ImageNet-UA REVEALS WEAKNESSESS IN L∞ TRAINING AND TESTING
We use ImageNet-UA to reveal weaknesses in the common practices of L∞ robustness evaluation
and L∞ adversarial training. We compute the mUAR and UAR(L∞) for models trained against
the L∞ attack with distortion size ε and show results in Figure 5. For small ε ≤ 4, mUAR and
7
Under review as a conference paper at ICLR 2021
Table 3: Non-adversarial defenses can noticeably improve ImageNet-UA performance. ResNeXt-
101 (32×8d) + WSL is trained on approximately 1 billion images Mahajan et al. (2018). Stylized
ImageNet is trained on a modification of ImageNet using style transfer Geirhos et al. (2019). Patch
Gaussian augments using Gaussian distortions on small portions of the image Lopes et al. (2019).
AugMix mixes simple random augmentations of the image Hendrycks et al. (2020). These results
suggest that ImageNet-UA performance may be achieved through non-adversarial defenses.
	Clean Acc.	L∞	L2	Li	Elastic	JPEG	Fog	Snow	Gabor	mUAR
SqUeezeNet	84.1	5.2	lɪɪ	T49^	25.9	1.9	20.1	9.8	4.4	12.8
ResNeXt-101 (32×8d)	95.9	2.5	5.5	20.7	26.5	1.8	14.1	12.4	5.3	13.4
ResNeXt-101 (32×8d) + WSL	97.1	3.0	5.7	28.3	29.4	1.9	26.2	20.3	8.0	19.0
ResNet-18	91.6	2.7	8.2	13.5	22.6	1.8	20.3	9.5	4.2	12.0
ResNet-50	94.2	2.7	6.6	20.1	24.9	1.8	15.8	11.9	4.9	13.2
ResNet-50 + Stylized ImageNet	94.6	2.9	7.4	22.8	26.0	1.8	16.2	12.5	8.1	14.6
ResNet-50 + Patch GaUssian	93.6	4.5	10.9	27.4	28.2	1.8	23.9	10.5	5.2	16.2
ResNet-50 + AUgMix	95.1	6.1	13.4	34.3	38.8	1.8	28.6	24.7	11.1	23.2
UAR(L∞) increase together with ε. For larger ε ≥ 8, UAR(L∞) continues to increase with ε, but
the mUAR decreases, a fact which is not apparent from L∞ evaluation.
The decrease in mUAR while UAR(L∞) increases suggests that L∞ adversarial training begins to
heavily fit L∞ distortions at the expense of generalization at larger distortion sizes. Thus, while it
is the most commonly used defense procedure, L∞ training may not lead to improvements on other
attacks or to real-world robustness.
Worse, L∞ evaluation against L∞ adversarial training at higher distortions indicates higher robust-
ness. In contrast, mUAR reveals that L∞ adversarial training at higher distortions in fact hurts
robustness against a more diverse set of attacks. Thus, L∞ evaluation gives a misleading picture of
robustness. This is particularly important because L∞ evaluation is the most ubiquitous measure of
robustness in deep learning (Goodfellow et al., 2014; Madry et al., 2017; Xie et al., 2018).
5.3	LIMITS OF ADVERSARIAL TRAINING FOR ImageNet-UA
We establish a baseline on ImageNet-UA using L2 adversarial training but show a significant perfor-
mance gap even for more sophisticated existing adversarial training methods. To do so, we evaluate
several adversarial training methods on ImageNet-UA and show results in Table 1.
Our results show that L2 trained models outperform L∞ trained models and have significantly im-
proved absolute performance, increasing mUAR from 14.0 to 50.7 compared to an undefended
model. The individual UAR values in Figure 7 (Appendix E.1) improve substantially against all
attacks other than Fog, including several (Elastic, Gabor, Snow) of extremely different nature to L2 .
This result suggests pushing adversarial training further by training against multiple attacks simul-
taneoUsly via joint adversarial training (Jordan et al., 2019; Tramer & Boneh, 2019) detailed in
Appendix C. Table 2 shows that, despite using twice the compute of L2 training, (L∞ , L2) joint
training only improves the mUAR from 50.7 to 50.9. We thUs recommend L2 training as a baseline
for ImageNet-UA, thoUgh there is sUbstantial room for improvement compared to the highest UARs
against individUal attacks in FigUre 4, which are all above 80 and often above 90.
5.4	ImageNet-UA ROBUSTNESS THROUGH NON-ADVERSARIAL DEFENSES
We find that methods can improve robUstness to Unforeseen attacks withoUt adversarial training. Ta-
ble 3 shows mUAR for SqUeezeNet (Iandola et al., 2017), ResNeXts (Xie et al., 2016), and ResNets.
For ImageNet-1K models, we mask 900 logits to predict ImageNet-100 classes.
A popUlar defense against average case distortions (Hendrycks & Dietterich, 2019) is Stylized Im-
ageNet (Geirhos et al., 2019), which modifies training images Using image style transfer in hopes
of making networks rely less on textUral featUres. Table 3 shows it provides some improvement on
ImageNet-UA. More recently, Lopes et al. (2019) propose to train against GaUssian noise applied to
small image patches, improving the mUAR by 3% over the ResNet-50 baseline. The second largest
mUAR improvement comes from training a ResNeXt on approximately 1 billion images (Maha-
jan et al., 2018). This three orders of magnitUde increase in training data yields a 5.4% mUAR
8
Under review as a conference paper at ICLR 2021
increase over a vanilla ResNeXt baseline. Finally, Hendrycks et al. (2020) create AugMix, which
randomly mixes stochastically generated augmentations. Although AugMix did not use random nor
adversarial noise, it improves robustness to unforeseen attacks by 10%.
These results imply that defenses not relying on adversarial examples can improve ImageNet-UA
performance. They indicate that training on more data only somewhat increases robustness
on ImageNet-UA, unlike many other robustness benchmarks (Hendrycks & Dietterich, 2019;
Hendrycks et al., 2019) where more data helps tremendously (Orhan, 2019). While models with
lower clean accuracy (e.g., SqueezeNet and ResNet-18) have higher UAR(L∞) and UAR(L2) than
many other models, there is no clear difference in mUAR. Last, these non-adversarial defenses have
minimal cost to accuracy on clean examples, unlike adversarial defenses. Much remains to explore,
and we hope non-adversarial defenses will be a promising avenue toward adversarial robustness.
6	Conclusion
This work proposes a framework ImageNet-UA to evaluate robustness of a defense against
unforeseen attacks. Because existing adversarial defense techniques do not scale to multiple
attacks, developing models which can defend against attacks not seen at train-time is essential for
robustness. Our results using ImageNet-UA show that the common practice of L∞ training and
evaluation fails to achieve or measure this broader form of robustness. As a result, it can provide
a misleading sense of robustness. By incorporating our 4 novel and strong adversarial attacks,
ImageNet-UA enables evaluation on the diverse held-out attacks necessary to measure progress
towards robustness more broadly.
References
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of
security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420,
2018a.
Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adver-
sarial examples. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th Interna-
tional Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Re-
search, pp. 284-293, Stockholmsmssan, Stockholm Sweden, 10-15 JUl 2018b. PMLR. URL
http://proceedings.mlr.press/v80/athalye18b.html.
Tom B. Brown, Dandelion Mane, Aurko Roy, Martin Abadi, and Justin Gilmer. Adversarial patch.
CoRR, abs/1712.09665, 2017. URL http://arxiv.org/abs/1712.09665.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP), pp. 39-57. IEEE, 2017.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian G Goodfellow, and Aleksander Madry. On evaluating adversarial robustness: Princi-
ples of rigorous evaluations. 2019a.
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris
Tsipras, Ian J. Goodfellow, Aleksander Madry, and Alexey Kurakin. On evaluating adversarial
robustness. CoRR, abs/1902.06705, 2019b. URL http://arxiv.org/abs/1902.06705.
Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. EAD: Elastic-net attacks
to deep neural networks via adversarial examples. In Thirty-second AAAI conference on artificial
intelligence, 2018.
Kenneth T. Co, Luis Munoz-Gonzalez, and Emil C. Lupu. Sensitivity of deep convolutional net-
works to Gabor noise. CoRR, abs/1906.03455, 2019. URL http://arxiv.org/abs/
1906.03455.
Jeremy M. Cohen, Elan Rosenfeld, and J. Zico Kolter. Certified adversarial robustness via ran-
domized smoothing. CoRR, abs/1902.02918, 2019. URL http://arxiv.org/abs/1902.
02918.
9
Under review as a conference paper at ICLR 2021
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. IEEE, 2009.
Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial
examples by translation-invariant attacks. In Proceedings of the IEEE Computer Society Confer-
ence on Computer Vision and Pattern Recognition, 2019.
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander Madry. A
rotation and a translation suffice: Fooling CNNs with simple transformations. arXiv preprint
arXiv:1712.02779, 2017.
Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir
Rahmati, and Dawn Xiaodong Song. Robust physical-world attacks on deep learning models.
2017.
Alain Fournier, Don Fussell, and Loren Carpenter. Computer rendering of stochastic models. Com-
mun. ACM, 25(6):371-384, June 1982. ISSN 0001-0782. doi: 10.1145/358523.358553. URL
http://doi.acm.org/10.1145/358523.358553.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95-110, 1956.
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A. Wichmann, and
Wieland Brendel. Imagenet-trained CNNs are biased towards texture; increasing shape bias im-
proves accuracy and robustness. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=Bygh9j09KX.
Justin Gilmer, Ryan P. Adams, Ian J. Goodfellow, David Andersen, and George E. Dahl. Motivating
the rules of the game for adversarial example research. ArXiv, abs/1807.06732, 2018.
Ian J. Goodfellow. A research agenda: Dynamic models to defend against correlated attacks. ArXiv,
abs/1903.06293, 2019.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572, 2014.
Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola, An-
drew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet
in 1 hour. arXiv preprint arXiv:1706.02677, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-
ruptions and perturbations. In International Conference on Learning Representations, 2019.
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. arXiv preprint arXiv:1907.07174, 2019.
Dan Hendrycks, Norman Mu, Ekin D. Cubuk, Barret Zoph, Justin Gilmer, and Balaji Lakshmi-
narayanan. AugMix: A simple data processing method to improve robustness and uncertainty.
Proceedings of the International Conference on Learning Representations (ICLR), 2020.
Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally, and Kurt
Keutzer. Squeezenet: AlexNet-level accuracy with 50x fewer parameters and <1mb model size.
ArXiv, abs/1602.07360, 2017.
Jrn-Henrik Jacobsen, Jens Behrmannn, Nicholas Carlini, Florian Tramr, and Nicolas Papernot. Ex-
ploiting excessive invariance caused by norm-bounded adversarial robustness, 2019.
Matt Jordan, Naren Manoj, Surbhi Goel, and Alexandros G. Dimakis. Quantifying perceptual dis-
tortion of adversarial examples. arXiv e-prints, art. arXiv:1902.08265, Feb 2019.
10
Under review as a conference paper at ICLR 2021
Ares Lagae, Sylvain Lefebvre, George Drettakis, and Philip Dutre. Procedural noise using sparse
Gabor convolution. ACMTrans. Graph., 28(3):54:1-54:10, July 2009. ISSN 0730-0301. doi: 10.
1145/1531326.1531360. URL http://doi.acm.org/10.1145/1531326.1531360.
Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin Dogus Cubuk. Improving ro-
bustness without sacrificing accuracy with patch gaussian augmentation. ArXiv, abs/1906.02611,
2019.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,
Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised
pretraining. In Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss (eds.),
Computer Vision - ECCV 2018, pp. 185-201, Cham, 2018. Springer International Publishing.
ISBN 978-3-030-01216-8.
Arkadi Nemirovski and D Yudin. On Cezari’s convergence of the steepest descent method for
approximating saddle point of convex-concave functions. In Soviet Math. Dokl, volume 19, pp.
258-269, 1978.
Arkadi Nemirovski and D Yudin. Problem Complexity and Method Efficiency in Optimization.
Intersci. Ser. Discrete Math. Wiley, New York, 1983.
A. Emin Orhan. Robustness properties of Facebook’s ResNeXt WSL models.	ArXiv,
abs/1907.07640, 2019.
Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram
Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM
on Asia conference on computer and communications security, pp. 506-519. ACM, 2017.
Fabio Pierazzi, Feargus Pendlebury, Jacopo Cortellazzi, and Lorenzo Cavallaro. Intriguing prop-
erties of adversarial ml attacks in the problem space. In 2020 IEEE Symposium on Secu-
rity and Privacy (SP), pp. 1308-1325. IEEE Computer Society, 2020. doi: 10.1109/SP40000.
2020.00073. URL https://doi.ieeecomputersociety.org/10.1109/SP40000.
2020.00073.
Haifeng Qian and Mark N. Wegman. L2 -nonexpansive neural networks. In International Conference
on Learning Representations (ICLR), 2019. URL https://openreview.net/forum?
id=ByxGSsR9FQ.
Chongli Qin, James Martens, Sven Gowal, Dilip Krishnan, Krishnamurthy Dvijotham, Alhussein
Fawzi, Soham De, Robert Stanforth, and Pushmeet Kohli. Adversarial robustness through local
linearization, 2019.
Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan, Honglak Lee, and Bo Li. Semanticadv: Gener-
ating adversarial examples via attribute-conditional image editing. ArXiv, abs/1906.07927, 2019.
Edward Raff, Jared Sylvester, Steven Forsyth, and Mark McLean. Barrage of random transforms
for adversarially robust defense. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 6528-6537, 2019.
P. Rajpurkar, R. Jia, and P. Liang. Know what you don’t know: Unanswerable questions for SQuAD.
In Association for Computational Linguistics (ACL), 2018.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? In ICML, 2019.
L. Schott, J. Rauber, W. Brendel, and M. Bethge. Towards the first adversarially robust neural
network model on MNIST. May 2019. URL https://arxiv.org/pdf/1805.09190.
pdf.
11
Under review as a conference paper at ICLR 2021
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K. Reiter. Accessorize to a crime:
Real and stealthy attacks on state-of-the-art face recognition. In Proceedings of the 23rd ACM
SIGSAC Conference on Computer and Communications Security, 2016.
Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. A general framework for
adversarial examples with objectives. ACM Transactions on Privacy and Security (TOPS), 22(3):
1-30, 2019.
Richard Shin and Dawn Song. JPEG-resistant adversarial images. In NIPS 2017 Workshop on
Machine Learning and Computer Security, 2017.
Yang Song, Rui Shu, Nate Kushman, and Stefano Ermon. Constructing unrestricted adversarial
examples with generative models. In NeurIPS, 2018.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.
Florian Tramer and Dan Boneh. Adversarial training and robustness for multiple perturbations.
arXiv e-prints, art. arXiv:1904.13000, Apr 2019.
Florian Tramer, Pascal Dupre, Gili Rusak, Giancarlo Pellegrino, and Dan Boneh. Ad-VerSarial:
Defeating perceptual ad-blocking. CoRR, abs/1811.03194, 2018. URL http://arxiv.org/
abs/1811.03194.
Tong Wu, Liang Tong, and Yevgeniy Vorobeychik. Defending against physically realizable attacks
on image classification. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=H1xscnEKDr.
Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu, and Dawn Song. Spatially trans-
formed adversarial examples. arXiv preprint arXiv:1801.02612, 2018.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising
for improving adversarial robustness. arXiv preprint arXiv:1812.03411, 2018.
Saining Xie, Ross B. Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual
transformations for deep neural networks. 2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 5987-5995, 2016.
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan.
Theoretically principled trade-off between robustness and accuracy. In Kamalika Chaudhuri
and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine
Learning, volume 97 of Proceedings of Machine Learning Research, pp. 7472-7482, Long Beach,
California, USA, 09-15 Jun 2019. PMLR. URL http://proceedings.mlr.press/
v97/zhang19p.html.
Zhengyu Zhao, Zhuoran Liu, and Marisa Larson. Towards large yet imperceptible adversarial image
perturbations with perceptual color distance. ArXiv, abs/1911.02466, 2019.
12
Under review as a conference paper at ICLR 2021
A Training hyperparameters
For ImageNet-100, we trained on machines with 8 NVIDIA V100 GPUs using standard data aug-
mentation (He et al., 2016). Following best practices for multi-GPU training (Goyal et al., 2017),
we ran synchronized SGD for 90 epochs with batch size 32×8 and a learning rate schedule with 5
“warm-up” epochs and a decay at epochs 30, 60, and 80 by a factor of 10. Initial learning rate after
warm-up was 0.1, momentum was 0.9, and weight decay was 10-4. For CIFAR-10, we trained on a
single NVIDIA V100 GPU for 200 epochs with batch size 32, initial learning rate 0.1, momentum
0.9, and weight decay 10-4. We decayed the learning rate at epochs 100 and 150.
B CALIBRATION OF ImageNet-UA AND CIFAR-10-UA
B.1	CALIBRATION FOR ImageNet-UA
Calibrated distortion sizes and ATA values are in Table 4.
B.2	CALIBRATION FOR CIFAR-10-UA
The ε calibration procedure for CIFAR-10 was similar to that used for ImageNet-100. We started
with small εmin values and increased ε geometrically with ratio 2 until adversarial accuracy of an
adversarially trained model dropped below 40. Note that this threshold is higher for CIFAR-10 than
ImageNet-100 because there are fewer classes. The resulting ATA values for CIFAR-10 are shown
in Table 5.
C Joint adversarial training
Our joint adversarial training procedure for two attacks A and A0 is as follows. At each training step,
we compute the attacked image under both A and A0 and backpropagate with respect to gradients
induced by the image with greater loss. This corresponds to the “max” loss of Tramer & Boneh
(2019). We train ResNet-50 models for (L∞, L2), (L∞, L1), and (L∞, Elastic) on ImageNet-100.
Table 6 shows training against (L∞, L1) is worse than training against L1 at the same distortion
size and performs particularly poorly at large distortion sizes. Table 7 shows joint training against
Table 4: Calibrated distortion sizes and ATA values for different distortion types on ImageNet-100.
Attack	ε1	ε2	ε3	ε4	ε5	ε6	ATA1 ATA2 ATA3 ATA4 ATA5 ATA6					
L∞	1	2	4	8	16	F~~	84.6	82.1	76.2	66.9	40.1	12.9
L2	150	300	600	1200	2400	4800	85.0	83.5	79.6	72.6	59.1	19.9
L1	9562.5	19125 76500		153000 306000 612000 84.4				82.7	76.3	68.9	56.4	36.1
Elastic 0.25		0.5	2	4	8	16	85.9	83.2	78.1	75.6	57.0	22.5
JPEG	0.062	0.125	0.250	0.500	1	2	85.0	83.2	79.3	72.8	34.8	1.1
Fog	128	256	512	2048	4096	8192	85.8	83.8	79.0	68.4	67.9	64.7
Snow	0.0625 0.125		0.25	2	4	8	84.0	81.1	77.7	65.6	59.5	41.2
Gabor	6.25	12.5	25	400	800	1600	84.0	79.8	79.8	66.2	44.7	14.6
Table 5: Calibrated distortion sizes and ATA values for ResNet-56 on CIFAR-10
Attack	ε1	ε2	ε3	ε4	ε5	ε6	ATA1	ATA2	ATA3	ATA4	ATA5	ATA6
L∞	1	2	4	8	16	F	91.0	87.8	81.6	71.3	46.5	23.1
L2	40	80	160	320	640	2560	90.1	86.4	79.6	67.3	49.9	17.3
L1	195	390	780	1560	6240	24960	92.2	90.0	83.2	73.8	47.4	35.3
JPEG	0.03125	0.0625	0.125	0.25	0.5	1	89.7	87.0	83.1	78.6	69.7	35.4
Elastic	0.125	0.25	0.5	1	2	8	87.4	81.3	72.1	58.2	45.4	27.8
13
Under review as a conference paper at ICLR 2021
Table 6: UAR scores for L1-trained models and (L∞, L1)-jointly trained models. At each distortion
size, L1-training performs better than joint training.
	UARl∞			UARL1
L∞ ε	= 2, L1 ε	二 76500	48	66
L∞ ε	= 4, L1 ε	153000	51	72
L∞ ε	= 8, L1 ε	二 306000	44	62
L1 ε =	76500		50	70
L1 ε =	153000		54	81
L1 ε =	306000		59	87
Table 7: UAR scores for L∞- and Elastic-trained models and (L∞, Elastic)-jointly trained models.
No jointly trained model matches a Elastic-trained model on UAR vs. Elastic.
			UARL∞	UARElastic
L∞ ε	4, Elastic ε =	二 2	68	63
L∞ ε	8, Elastic ε =	4	35	65
L∞ ε	16, Elastic ε	二8	69	43
Elastic	ε=2		37	68
Elastic	ε=4		36	81
Elastic	ε=8		31	91
(L∞, Elastic) also performs poorly, never matching the UAR score of training against Elastic at
moderate distortion size (ε = 2).
D	The Frank-Wolfe Algorithm
We chose to use the Frank-Wolfe algorithm for optimizing the L1 attack, as Projected Gradient
Descent would require projecting onto a truncated L1 ball, which is a complicated operation. In
contrast, Frank-Wolfe only requires optimizing linear functions g>x over a truncated L1 ball; this
can be done by sorting coordinates by the magnitude of g and moving the top k coordinates to the
boundary of their range (with k chosen by binary search). This is detailed in Algorithm 1.
E	Full evaluation results
E.1 Full evaluation results and analysis for ImageNet- 1 00
We show the full results of all adversarial attacks against all adversarial defenses for ImageNet-100
in Figure 6. These results also include L1-JPEG and L2-JPEG attacks, which are modifications of
the JPEG attack applying Lp-constraints in the compressed JPEG space instead of L∞ constraints.
Full UAR scores are provided for ImageNet-100 in Figure 7.
E.2 Full evaluation results and analysis for CIFAR- 1 0
We show the results of adversarial attacks and defenses for CIFAR-10 in Figure 8. We experienced
difficulty training the L2 and L1 attacks at distortion sizes greater than those shown and have omitted
those runs, which we believe may be related to the small size of CIFAR-10 images. Full UAR values
for CIFAR-10 are shown in Figure 9.
F Robustness of our results
F.1 Replication
We replicated our results for the first three rows of Figure 6 with different random seeds to see the
variation in our results. As shown in Figure 10, deviations in results are minor.
14
Normal training -
	14 50	O O 2 O	O O
03 02 74 22 O 79 7976 59 6 74 74 73 67 34 71 70 69 6240			O O 1 B
			
B6B4 6610
05 04 77 34
04 03 79 54
79 7B 7043
73 70 59 26
68 6134 7
1
5
L2 ε-150
l2ε-300
L2 ε-600
£.2 ε-1200
⅛ £-2400
La e-4000
■7422 O O
B4B156 4 O
82 8174 28 O
77 7β74 56 6
6B 68 67 6128
E-9562.44 -『 1ε-19125 - J	71 7B	24 41	1 O 3 O	
ε-3θ250.1 - J ιε-76500 - J ε-153000 - J ε-306000 - £ ε-612000 -E	Bl 6211 O 02 7120 1 79 72 43 3 77725310 71 69 59 24			O O O O O O O O 10
				
U-JPEG £-0.03125 - J
La-JPEG £-0.0625 -，
J-JPEG ε-0.125 -,
U-JPEG ε-0.25 -]
U-JPEG ε≡ 0.5 - J
U-JPEGe-I- j
La-JPEGe-2 -E
75 2B 0047	1 O 3 O	O O O O
83 6814 O B3 7742 3 βθ 786617 79 77 68 27 77 7663 19		O O O O 1 O 1 O 10
		
1
⅛JP∈Ge-2
L2-JPEG £-4
L2-JPEG ε"θ
L2-JPEG ε≡ 16
L2-JPEG £-32
L2-JPEG «-64
L3-JPEG ε-12θ
L2-JPEG E-256
6412 O .26 1 ■50 4 B3 7015 03 7844	O O O O 3 17 40 36	O O O O O O O O O O O O 3 O 2 O
Bl 79 67 7θ7672. 777671		
77 60 24 2 0
00 67 37 7 0
BO 73 53 IB 2
7B 74 62 35 7
77 75 60 50 21
77 75 68 48 17
04 7119 O
84 7943 2
03 βl6613
BO 797441
79 7074 50
7B 77 7347
B0 4∣ 2 O O
B3 59 B O O
■7424 1 O
848050 3 O
03 02 7010 O
01 00 76 49 4
7B 77 75 6316
77 77 75 6215
W
B3 765417 1	0	0	0	0	0	0
04 817041 θ	0	0	0	0	0	0
05 0379 6420	3	0	0	0	0	0
05 04 03 70 60 24	2	0	0	0	0
B4B3B3 Bl 75 57 22 3 0 0 0
01 0101 00 70 73 60 3410 2 1
77 77 77 77 76 75 72 67 55 38 32
78 78 77 77 77 75 7470 63 56 57
B2 5910 O O O O 02 6515 O O O O 03 6919 1 O O O 83 73 30 1 O O O ^H42 3 0 0 0 BO7654 9000 77 7562 19 IOO
77 75 6419 1 O O
Li-JPEG ε-12θ
Li-JPEGe-256
Li-JPEG ε-512
Li-JPEG ε-1024
Li-JPEG E-204B
Ll-JPEG ε-4096
Li-JPEG E-Θ192
Li-JPEG £- 163B4
Li-JPEG E-3276θ
Li-JPEG ε-65536
Li-JPEG ε -131072
B3 6612 O
B2 7534 1
B2 7955 5
BO 786414
79787026
77 77 71 33
75 74 69 37
73 7165 29
■6311 O O
H7426 1 O
B4B051 3 O
04 Bl 67 13 O
03 Bl 75 34 ɪ
BO BO 76 40 3
79 79 76 50 B
7B 77 75 60 11
75 75 73 5913
73 72 69 5410
Elastic E-0.25 -H
Elastic e≡0.5 - y
Elastic £-1-3
Elastic £-2-3
Elastic e≡4 - 5
Elastice-B- ɪ
Elastice ≡16 - !
I
AdVerSaria- accuracy
UlIderreVieW as a COnferenCe PaPer at ICLR 2021
Fog 3128 -
Fog ε "256 -
Fog ε-512 -
Fog ε ≡ 1024 -
Fog ε ≡ 204B -
Fog ε≡4096 -
FOg £■ 8192 -
Fogε-163B4-
Fogε-3276θ-
FogE - 65536-
Gabor £-6.25
Gabor f-12.5
Gabor ε≡ 25
Gabor ε≡ 50
Gabore >100
Gabor £>200
Gabor ε≡ 400
Gaborc "BOO
GabOrg - 1600
Gabor ε> 3200
Snow £-0.03125
Snowε> 0.0625
Snowε> 0.125
Snow ε≡ 0.25
Snow ε-0.5
Snow e≡l
Snow e≡2
Snow £ - 4
Snow £■ B
Snowe-16
4 3 4 51013
7 2 5 4 4 4
2 2 2 2 3 3

I
5B15 1
5915 1
6016 1
5615 1
B3 69 33 3 O O O O O O
lB5 70 59 21 2 O O O O O
8682 745116 2 1111
06 04 70674316 5 2 1 1
04 03 79 7156 3416 7 3 1
79 7B 76 72 67 5B 46 2910 3
7070 70 6B6B 6B 65 57 3917
6465 6565 64 64 64 62 5333
56 57 57 57 57 57 55 514123
15150 5150 50 49 40 45 39 20
l∖∖∖∖∖
83 7438 2
02 73 37 3
01 7440 3
BO 72 39 4
BO 72 30 4
79 7142 6
7β 7043 9
77 69 4510
75 6847 13
Figure 6: Accuracy of adversarial attack (column) against adVersarially trained model (row) on ImageNet-IOO.
Under review as a conference paper at ICLR 2021
Algorithm 1 Pseudocode for the Frank-Wolfe algorithm for the L1 attack.
1: 2: 3: 4: 5: 6: 7: 8: 9:	Input: function f, initial input x ∈ [0, 1]d, L1 radius ρ, number of steps T. Output: approximate maximizer X of f over the truncated Li ball Bι(ρ; x) ∩ [0,1]d centered at x. x(0) J RandomInit(X) {Random initialization} for t = 1, . . . , T do g J Vf (χ(t-i)) {Obtaingradient} for k = 1, . . . , d do sk J index of the coordinate of g by with kth largest norm end for
10:	Sk J {s1, . . . , sk}.
11: 12:	{Compute move to boundary of [0, 1] for each coordinate.}
13:	for i = 1, . . . , d do
14:	if gi > 0 then
15:	bi J 1 - Xi
16:	else
17:	bi J -Xi
18:	end if
19:	end for
20:	Mk J Pi∈S |bi| {Compute L1-perturbation of moving k largest coordinates.}
21: ɔɔ. 22:	k J max{k | Mk ≤ ρ} {Choose largest k satisfying Li constraint.}
23:	{Compute X maximizing g>X over the Li ball.}
24:	for i = 1, . . . , d do
25:	if i ∈ Sk* then
26:	Xi J Xi + bi
27:	else if i = sk*+i then
28:	Xi J Xi + (p — Mk*) sign(gi)
29:	else
30:	Xi J Xi
31: 32:	end if end for
33:	X(t) J (ι — 1)X(t-1) + 1X {Average X with previous iterates}
34:	end for
35:	X J X(T)
F.2 Convergence
We replicated the results in Figure 6 with 50 instead of 200 steps to see how the results changed
based on the number of steps in the attack. As shown in Figure 11, the deviations are minor.
16
Under review as a conference paper at ICLR 2021
Normal Training -	7 17 22	0	0	31 16 5 10	Normal Training -	7 17 22	0	0	31 16 5	10	Normal Training -	7 17 22	0	0	31	16 5 10
													
L∞ ε = 1 -	46 54 37	24 21	40 29 29 25	L2 ε = 150 -	38 49 38	15 13	39 29 22	20	Li ε = 9562 -	26 40 43	5	6	37	22 14 16
L∞ ε = 2 "	60 64 42	36 30	42 29 41 31	L2 ε = 300 -	50 60 44	27 24	40 29 33	26	Li ε = 19125 -	33 47 49	12 14	39	23 21 20
L∞ ε = 4 -	72 74 48	45 37	44 27 53 37	L2 ε = 600 -	62 72 53	40 36	42 28 44	31	Li ε = 76500 -	50 63 70	34 35	41	24 38 27
L∞ ε = 8 -	83 72 42	42 32	47 23 60 41	L2 ε = 1200	73 82 65	54 49	46 26 54	37	Li ε = 153000 -	54 66 81	42 42	41	24 44 30
L∞ ε = 16 -	89 60 27	30 24	49 19 58 41	L2 ε = 2400	80 88 75	63 58	48 22 57	40	Li ε = 306000 -	59 70 87	51 50	43	21 48 33
L∞ ε = 32-	88 42 15	14 11	49 20 55 37	L2 ε = 4800	80 88 79	67 63	48 18 53	38	Li ε = 612000 -	62 71 89	56 55	43	18 47 31
													
L∞-JPEG ε = 0.0625 -	36 44 34	49 48	38 23 17 16	L2-JPEG ε = 8-	37 46 36	49 50	38 23 17	17	Elastic ε = 0.25 -	21 32 30	4	3	41	24 14 18
L∞-JPEG ε = 0.125 -	46 52 38	63 59	39 22 27 20	L2-JPEG ε = 16 -	47 55 41	63 62	39 24 26	20	Elastic ε = 0.5 -	27 38 34	10 7	46	27 22 24
L∞-JPEG ε = 0.25 -	56 61 43	73 69	40 22 39 24	L2-JPEG ε = 32 -	57 63 46	72 74	40 24 36	25	Elastic ε = 2 -	37 46 37	19 15	68	30 42 36
L∞-JPEG ε = 0.5-	67 69 48	85 80	41 21 50 30	L2-JPEG ε = 64-	67 73 53	84 84	41 23 48	31	Elastic ε = 4 -	36 44 30	11 9	81	29 46 40
L∞-JPEG ε = 1 -	69 72 56		41 21 53 32	L2-JPEG ε = 128	74 77 59	90 93	43 21 53	36	Elastic ε = 8 -	31 36 19	4	3		26 45 39
L∞-JPEG ε = 2 -	65 70 54		40 19 52 31	L2-JPEG ε = 256	72 76 61		43 21 53	36	Elastic ε = 16 -	23 25 11	1	1		25 41 40
													
Fog ε = 128 -	12 21 22	0	0	34 41 6 15	Gabor ε = 6.25 ■	17 28 26	1	0	39 30 46	30	Snow ε = 0.0625 -	15 24 22	0	0	32	35 18 39
Fog ε = 256 -	11 22 21	0	0	35 50 8 19	Gabor ε = 12.5 ■	11 20 22	0	0	40 32 59	36	Snow ε = 0.125 -	14 22 20	0	0	33	36 28 52
Fog ε = 512
Gabor ε = 25
Snow ε = 0.25
8
18
18
0
0
36
58
10
23
7
15
18
0
0
39
29
64
39
10
16
16
0
0
34
39
39
59
Fog ε = 2048
Fog ε = 4096
Fog ε = 8192
5
12
15
0
0
36
20
31
2
7
8
0
0
34
29
34
1
3
8
0
0
28
43
54
Gabor ε = 400
Gabor ε = 800
Gabor ε = 1600
10
11
18
19
14
14
0
0
0
0
39
39
25
27
68
36
Snow ε = 2
8
9
4
0
0
34
38
52
71
73
37
Snow ε = 4
Figure 7: UAR
ImageNet-100.
scores for adv.
12
19
14
0
0
39
29
82
40
Snow ε = 8
78
8
8
4
0
0
34
36
50
13
15
9
1
0
39
37
60
trained defenses (rows) against distortion types (columns) for
17
Normal training
91 83 49
∞
μy]
	93	■	89 79 37 1	O O
	93		91 86 64 12	O O
	91		90 88 78 40	1 O
	89		88 87 82 63	13 O
	83		83 82 80 71	39 2
BSl	84	I	83 82 77 66	47 23
90 86 73 44	9	O	OOO		83 50	3 0 0
90 87 78 54	17	1	OOO		87 69	14 O O
90 88 80 62	28	3	OOO		88 79	40 1 O
87 86 80 65	35	5	OOO		86 80	53 4 O
83 81 78 69	48	16	IOO		81 78	64 20 O
81 78 73 63	48	27	8 10		82 77	62 29 6
L2 ε = 10	92	■ 81 48	4 O	O O		91 89 80 42	3	3 5 5 3
L2 ε = 20	94	.88 67	13 O	O O		93 92 87 62	8	0 3 3 2
G £ = 40	93	H 90 81	42 1	O O		93 92 90 78	32	13 3 2
L2 ε = 80	92	H 90 86	66 13	O O		92 91 90 85	61	9 0 3 2
2 E = 160	90	.89 87	77 38	1 O		90 90 89 86	75	33 O O 1
2 ε = 320	87	.86 85 80 60		11 O		87 86 86 85 80		57 8 O O
2 6 = 640	80	H 79 79 77 68		34 1		80 80 80 79 77		67 32 O O
ε = 1280	73	H 73 73 71 66		49 14		73 73 73 73 71 66 50 15 O		
ε = 2560 -	69	，69 68 67 63		49 33		69 68 68 68 67 62 50 32 17		
ε = 5120	77	■ 76 746855		30 7		77 76 75 74 67 54 24 3 O		
Lw-JPEGε = 0.03125
L00-JPEGe = 0.0625
L00-JPEGe = 0.125
L00-JPEGe = 0.25
L00-JPEGe = 0.5
U-JPEG ε = l
88 75 31 1	O	O
88 79 46 3	O	O
87 81 59 10	O	O
84 80 65 22	1	O
81 79 70 39	4	O
79 77 69 44	6	O
91 90 86 64 14	O 3 3 2 11
90 89 87 74 30	112 2 1
88 88 86 78 44	4 0 2 1 1
85 85 83 78 56	12 O 1 1 O
82 82 81 78 64	27 1 O O O
79 79 78 75 65	30 2 O O OI
L2-JPEG £ = 0.0625
L2-JPEG ε = 0.125
L2-JPEG E = 0.25
L2-JPEGe = 0.5
L2-JPEG ε=l
L2-JPEG ε = 2
L2-JPEG £ = 4
L2-JPEG E = 8
92 87 63
92 88 70
93 91 81
92 91 86
91 89 85
89 88 86
88 87 85
85 84 83
18 0 2
42 2 1
61 9 0
63 13 0
76 38 2
79 52 7
78 62 19
68 17 O O O
74 29 1 O O
55 10 O O O
40 4 O O O
38 5 O O O
39 6 O O O
44 9 O O O
67 34 7 1 O
48 49 45 39 25
88 81 62 27	3 0 0 0 0		90 83 50 2	O
88 83 68 38	7 0 0 0 0		90 87 76 28	O
87 83 73 48	13 1 O O O		88 87 83 63	5
84 81 74 56	23 2 O O O		85 85 83 75	30
82 80 75 63	37 7 O O O		82 82 82 79	61
79 78 74 62	38 9 O O O		79 79 78 77	70
86 70 35	5 0 0 0 0 0	.5。	4	O	O	O
86 72 40	7 0 0 0 0 0	.69	15	O	O	O
88 78 52	15 O O O O O	.83	48	2	O	O
89 82 63	25 2 O O O O	.89	76	27	O	O
88 82 66	31 3 O O O O	.90 85		62	7	O
87 84 73	46 11 O O O O	.89 87		79 44		1
86 84 77	56 22 2 O O O	■ 87 87 84 70				15
84 82 77	63 34 5 O O O	.85 84 83 78				48
Li-JPEG ε=l
Li-JPEG £ = 2
LrJPEG ε = 4
Li-JPEG £ = 8
LrJPEGε = 16
LrJPEGe = 32
LrJPEGe = 64
LrJPEGε = 128
LrJPEGe = 256
LrJPEG ε = 512
LrJPEG ε = 1024
93
93
91
91
89
88
88
86
85
84
82
92 89 74	26	1	1 3 3 2 1
92 90 83	50	5	1 4 4 3 1
90 87 77	40	2	0 2 2 2 1
90 89 86	70	23	0 12 11
89 88 86	76	37	113 2 1
87 87 85	77	46	4 0 2 1 1
87 87 85 79		53	7 0 2 2 1
86 85 84 79		58	12 O O 1 O
85 84 83 79		62	18 O O 1 O
84 83 82 79		67	28 O O O O
82 82 81 78		67	31 1 O O O
87 77 50 13 89 82 63 26 87 80 60 21 88 85 72 42 88 85 76 51 86 84 77 56	O O 2 O 1 O 8 O 14 O 20 1	O O O O O O	O O O O O O	O O O O O O		71 21 83 52 88 75 89 83 88 86 87 85	O O 5 O 29 O 59 9 75 34 80 55	O O O O O 5	O O O O O O
86 84 78 60 85 83 78 63 84 83 78 65 83 82 78 68 81 80 77 67	25 2 30 3 34 5 43 9 44 11	O O O O O	O O O O O	O O O O O		87 86 83 67 86 85 82 72 85 84 82 75 84 83 82 77 82 82 81 76		16 32 45 56 59	O O 1 4 9
Elastice = 0.125
Elastic ε = 0.25
Elastic ε = 0.5
Elastic ε = 1
Elastic ε = 2
Elastic ε = 4
Elastic ε = 8
Elastic ε = 16
92 90
90 88
87 85
82 80
80 78
75 73
68 66
60 59
82 50
81 54
79 57
76 59
70 48
66 47
59 42
53 39
92 92 91 88 72	22	O	O	■ 91 90 84	66 28 3	O	OOOO
92 92 91 90 81	41	1	O	∣92 91 86	73 39 7	O	OOOO
91 90 90 89 84	59	8	O	∣90 89 86	77 52 16	1	OOOO
88 88 88 87 83	66	16	O	.88 87 85	77 57 24	3	OOOO
83 83 83 82 80	71	33	1	■ 83 82 81 76 65 39		11	IOOO
84 84 84 83 79	66	36	8	.84 83 81 76 66 49		32 18 9 4 1	
							
92 91 89 79 37	1	2	3	■ 90 87 74	42 8 O	O	OOOO
93 93 91 86 55	4	O	1	■ 92 89 80	53 12 O	O	OOOO
93 92 92 89 76	26	O	1	■ 92 90 85 69 30 3		O	OOOO
92 92 91 90 85	58	6	O	∣91 90 87 78 51 14		1	OOOO
90 90 90 89 87	75	25	O	∣90 89 87 82 64 31		5	OOOO
86 86 86 86 85 81		56	6	∣86 86 85 83 76 58		24	3 0 0 0
79 79 80 79 79 77		69 30		.79 79 79 78 76 69		51	23 4 O O
73 73 73 73 72	72	67	54	■ 73 73 73 72 71 68 61			50 37 25 16
69 69 68 68 68 67 64 52				.68 68 68 68 67 64 58 49 40 35 32			
77 77 76 76 75 72 61 26				∣76 76 76 74 71 64 47 27 11 4 2			
							
93 92 91 84 50	4	O	2	■ 91 89 81	55 15 1	O	OOOO
93 93 91 86 62	11	O	1	■ 92 91 87	73 38 7	O	OOOO
93 92 89 76 35	2	O	1	■ 92 90 84 67 29 3		O	OOOO
92 91 86 64 19	1	O	1	■ 91 89 83	69 43 11	O	OOOO
92 91 85 61 20	1	O	O	■ 91 87 77 58 28 6		O	OOOO
92 90 84 61 19	1	O	O	■ 90 87 79	63 38 14	2	OOOO
91 90 84 62 22	2	O	O	∣89 86 77	57 30 10	2	OOOO
89 88 85 78 53	17	2	O	，88 86 82	71 52 28	10	2 0 0 0
47 48 48 49 49 48 45 35				.50 52 54	57 57 56 54 46 30 14 6		
							
91 91 91 90 87 70 17			O	■ 91 90 88 82 63 29		5	OOOO
90 90 90 90 89 84 56			5	■ 90 90 89 87 80 61 30			5 0 0 0
89 89 89 88 88 86 77 37				.89 88 88 87 85 78 63 32 5 O O			
86 86 86 85 85 84 80 63				.86 85 85 85 84 81 73 54 24 4 O			
83 83 83 82 82 82 80 74				.83 83 82 82 82 80 77 69 52 28 11			
80 80 80 79 79 79 77 73				∣80 80 80 79 79 78 76 74 69 62 53			
							
93 93 90 75 25	O	1	1	■ 91 86 71	36 4 O	O	OOOO
93 93 91 84 48	2	O	2	■ 91 88 78	51 11 O	O	OOOO
93 93 92 89 74	22	O	1	■ 92 90 85 67 28 3		O	OOOO
93 93 92 91 87 60		6	O	∣92 91 89 82 60 22		2	OOOO
92 91 91 91 88 80		39	1	∣91 91 90 87 78 54		17	IOOO
90 89 89 89 88 85		72 20		∣90 89 89 88 85 77		55	19 1 O O
88 88 88 88 87 86 81 56				.88 88 88 87 86 82		73	49 15 1 O
85 85 85 85 84 84 81 72				■ 85 85 85 84 83 82 79			69 47 16 3
							
93 93 91 86 58	6		1	.92 90 85 66 26 2		O	OOOO
93 92 91 89 79 35			2	∣92 91 89 81 57 17		1	OOOO
92 92 92 91 86 67			O	■ 92 91 90 87 76 45		9	OOOO
91 91 90 90 88 80			2	■ 91 90 90 88 83 69 37			5 0 0 0
89 89 89 89 88 84			21	.89 89 89 88 86 79 60 22 1 O O			
88 88 88 87 87 85			43	，88 87 87 87 85 82 71 47 12 O O			
88 88 88 87 87 85 80			58	.88 88 87 87 86 83 76 59 27 3 O			
86 86 86 86 85 85 81			66	∣86 86 86 85 84 83 79 69 46 15 2			
85 85 85 85 85 84 81			70	■ 85 85 85	85 84 83	80	73 59 36 13
84 84 84 84 84 83 81			74	.84 84 84 84 83 82 80 76 66 49 26			
82 82 82 82 82 81 79			73	■ 82 82 82 82 82 81 79 75 69 56 38			
							
92 91 87 69 24	2	O	O	.88 79 57	23 3000000		
							
90 89 86 71 32	3	O	O	，86 79 58	26 4000000		
							
87 86 84 74 45	9	O	O	，84 77 61	35 10 1 O O O O O		
							
81 81 80 74 54	20	2	O	.79 74 64	45 20 6 IOOOO		
							
78 78 77 70 50	20	3	O	.77 73 64	47 26 10	3	1000
74 74 73 66 49	21	4	O	∣72 69 61	46 27 11	4	IOOO
68 67 65 59 43	19	4	O	，65 62 54	40 24 11	4	IOOO
60 59 58 53 39	17	3	O	∣58 55 49	37 21 8	2	IOOO
67 11	O (
78 26	1 (
83 50	3 (
83 66	12 (
80 72	34 ■
80 66	22 :
86 60 5 O
87 78 33 O
85 81 62 8
81 79 72 40
79 76 71 58
74 72 68 58
68 67 63 53
61 60 58 49
37 19 11 7
45 33 23 10
42 34 28 15
39 29 23 18
AdVerSaria- accuracy
UlIderreVieW as a COnferenCe PaPer at ICLR 2021
Vk Vk IVM S S
Figure 8: Accuracy of adversarial attack (column) against adversarially trained model (row) on CIFAR-10.
Under review as a conference paper at ICLR 2021
Normal Training -
17 16 48 5 25 3
Normal Training -
17 16 48 5 25 3
Normal
Training -
17 16 48 5 25 3
L∞ ε = 1
L∞ ε = 2
L∞ ε = 4
L∞ ε = 8
L∞ ε = 16
L∞ ε = 32
51	49	69	31	37	21
63	59	73	38	39	28
74	67	76	47	40	36
83	72	77	50	39	43
89	75	78	55	40	51
94	72	76	58	48	45
L? ε = 40 -
L2 ε = 80 -
L2 ε = 160 -
L2 ε = 320 -
L2 ε = 640 -
L2 ε = 2560 -
53 53 74 32 38 22
64 63 80 44 40 30
73 73 84 54 41 38
80 81 88 64 46 45
84 86 88 70 51 52
87 85 86 78 71 66
Li ε = 195 -
Li ε = 390 -
Li ε = 780 -
Li ε = 1560 -
Li ε = 6240 -
Li ε = 49920 -
36 38 70 19 34 12
40 41 79 24 39 13
26 26 79 15 37 9
18 15 80 10 37 7
17 13 77 10 36 10
49 47 61 47 51 29
L∞-JPEG ε = 0.03125 -
L∞-JPEG ε = 0.0625 -
L∞-JPEG ε = 0.125 -
L∞-JPEG ε = 0.25 -
L∞-JPEG ε = 0.5 -
L∞-JPEG ε = 1 -
LI-JPEG ε = 2 -
LI-JPEG ε = 8 -
LI-JPEG ε = 64 -
LI-JPEG ε = 256 -
LI-JPEG ε = 512 -
LI-JPEG ε = 1024-
Elastic ε = 0.125 -
Elastic ε = 0.25 -
Elastic ε = 0.5 -
Elastic ε = 1 -
Elastic ε = 2 -
Elastic ε = 8 -
40 37 63 19 24 41
41 38 65 23 25 53
43 40 65 28 27 64
47 41 57 33 29 75
49 35 51 32 29 89
45 31 37 27 25 86
Figure 9:	UAR scores on CIFAR-10. Displayed UAR scores are multiplied by 100 for clarity.
(M≡u一et--.Jes,Jgpe) *0s5
L∞ ε = 1
L∞ ε = 2
L∞ ε = 4
L∞ ε = 8
L∞ ε =16
L∞ ε = 32
L2 ε = 150
L2 ε = 300
L2 ε = 600
L2 ε = 1200
L2 ε = 2400
L2 ε = 4800
Li ε =
Li ε
Li ε =
Li ε
Li ε = 153000
Li ε = 306000
Li ε = 612000
184 70
13 0 0
85 81 50 2 0
83 82
74 23
80 79 77 59
74 74 73 67
6
34
185 81 47
85 83 71
84 83 78
18
47
80 78 73
74 72 64
46
35
180 66
81 72
80 74
18
29
72 62
61 48
23
10
84∣13
84 50
83 67
79 54
73 41
0
0
0
0
0
!71∣
71 70 69 63
40
8
69 62 35
6
0
0
40 22
2
0
0
0
65 9
0
0
0
82 54 3 0
84 74 21
0
77 76 71
56
22
1
69 69 66 61
45 14
170 22
0
0
0
0
∣84 77 13 1 0
0
6
2
9
5
4
4
77 76 74
69 68 67
77 43
80 60
81 70
80 75
77 72
73 68
29
47
51
52
3
10
56
61
10
15
1
4
0
0
6
27
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
84 74 33
84 79 54
83 80 66
81 79 72
78 76 70
73 72 67
77 76 73 48
69 69 67 60
21
40
45
46
1
5
220
0
0
0
0
81 68 5
82 76 19
83 79 44
81 80 65
78 78 75 67 40 4
73 73 71 66 47 12
8
32
0
4
0
0
0
0
0
0
0
1
38250.1
=76500
9562.44
=19125
) 79 33
) 82 65
I 83 78
? 81 79
84 80 56
82 80 73
ɑ § § § § §
84 81 33 3 0
84 83 53 12 0
83 83 71 41 5
81 81 76 63 24
序令令令
殳9五联总
0 0
8 0
3
28
41 1
68 15
0 0
1 0
、，V <⅛ ʌ
,.⅛> <V N <b ⅛
XG .9
§ <r <r <r∕</ 为
Attack (evaluation)
84 1
84 3
84 17
83 46
81 64
3
20
78 67
73 63
35
45
Adversarial accuracy
6 4
■ ■

4
4

0
2 0
0
6
0
0
0
0
0
0
0
0
0
1
0
2
5
3
0
0
0
0
1
5
6
2
0
0
0
0
0
0
0
0
1
7
0
0
8
4
0
0
0
0
0
0
0
0
0
0
0
0
2
8
0
0
0
0
0
0
0
0
0
0
0
0
Figure 10:	Replica of the first three block rows of Figure 6 with different random seeds. Deviations
in results are minor.
19
Normal training -
L2 ε-150
L2 ε-300
L2 ε-600
L2E- 1200
⅛e≡ 2400
/.jε-4B00
Ll ε-9562.44
I1 ε-19125
Li ε-3θ250.1
I1 £-76500
Lie-153000
lie-306000
i1ε-612000
U-JPEG ε-0.03125
U-JPEG £-0.0625
L.-JPEG ε-0.125
J-JPEGE=0.25
La-JPEGe-0.5
Le-JPEGc-I
Lw-JPEGe-Z
L2-JPEG £-2
L2-JPEG C-4
L2-JPEGe-B
L2-JPEG C-16
L2-JPEG ε≡ 32
L2-JPEGe-64
L2-JPEG E-12θ
L2-JPEGe-256
07
87
86
05
84
Bl
77
∣643710 1
1.0

Li-JPEG ε-12θ
Li-JPEG £-256
Li-JPEG ε-512
Li-JPEG £-1024
Li-JPEG ε-204B
Li-JPEG ε-4096
Li-JPEG ε-θl92
Li-JPEG ε- 163B4
Li-JPEG £-3276θ
Li-JPEG ε-65536
Li-JPEG E -131072
Elastice-0.25 -E
Elastic E-0.5 - ∏
Elastic f≡l - 5
Elastic t≡2 - ∏
Elastic f≡4 - J
Elastic^"B - J
Elastic C" 16-]
Fogε-12B - 1
Fog ε-256-1
Fog ε-512 -，
Fog ε ≡ 1024 - I
Fog ε"204B - J
Foge-4096-≡
FOg 38192 - j
Fogε-163β4- j
Fog 332768- 2
Foge-65536-G
Gabor ε≡ 6.25
Gabor ε≡ 12.5
Gabcre- 25
Gabcr ε- 50
Gabor £>100
Gabor C" 200
Gabor C" 400
Gabor ε≡ BOO
Gabor ε≡ 1600
Gabor ε≡ 3200
Snow £-0.03125
Snowε> 0.0625
Snow ε ≡ 0.125
Snow ε≡ 0.25
Snow ε-0.5
Snow c≡l
Snow £ - 2
Snow f≡4
Sπowε≡B
Snow C" 16
■7015 OOO
B5 BO 51 3 0 0
B4 B2 74 24 O O
79 797659 7 O
74 74 73 67 35 1
7170 69 634314
82 53 4 O O O
^≡23 OOO
B4B15B 4 0 0
B2 Bl 74 29 O O
77 7673 57 7 O
68 68 67 6128 1
					
		i 3			
		113			
					
					
					
					
B5 B2 50 3 O
05 03 71 21 O
04 03 79 50 3
79 7β73 52 9
73 7164 32 4
68 6035 7 O
■7B36 1 O O
≡B2 6610 O O
04 03 70 41 1 O
02 02 BO 6017 O
77 7676 7249 3
68 68 68 67 6020
82 6516 O O O
B4 7534 IOO
B4B057 7 0 0
B3 Bl 69 23 O O
00 7970 39 2 O
70 77 72 49 B O
72 7168 57 22 1
Bl 704310 1
Bl 74 57 24 3
01 7664 39 9
73 65 53 31 9
59 462710 2
39 24 9 3 O
84 7216 O B4B149 2 B3 82 7013 00 77 6015	OOO OOO OOO OOO
73 60 39 5 65 4511 1	OOO OOl
BO 714310 IOO
B3 77 6126 3 0 0
03 00 73 5013 O O
01βO 77 67 30 6 O
77 76 75 71 5β 26 3
6B 6868 66 614616
		O C O C		
				
				
				
				
				
B3 79 6121 1 O
04 917241 5 O
B5 B37B5Θ IB 1
04 03 Bl 73 43 5
Bl BO 79 77 6424
79 79 7076 7149
耳
1
[⅜		
04 7121 O O H7B44 3 O B3 Bl 6616 O BO 797442 3 79 70 74 53 9 7B 77 73 50 θ		I
		
Hββl9 1 O
卜27548 5 O
∣79 7663 20 ɪ
∣77 75 69 47 4
∣71 70 66 5418
	3 0( ∣20 O C		)( )(	
	60 2 (		)(	
		t	)(	
		1 C		
		41 C		
				

	32 2 O		O C	
		W		
				
				
				
				
				
82 67 22 1 O	O O
83 7132 2 O	O O
83 7543 4 O	O O
B2 70 53 B O	O O
79 75 50 14 O	O O
77 7462 23 1	O O
70 68 6135 4	OO
B2 6l 族	»10 1 ∣22 1 J 31 2		
			
			
			
			
		
		
		
		
2 51B46521
6 15 9 4
1 4 6 6 6
2 7 0 3 9
6 7 7 7 6
1 2 9 4 0
0 0 7 7 7
1 2 4112121
7 4 3 81411
12 3 4 4
3 7 9 5 1 6
3 4 5 6 6 5
7 3 6 3 6 1
6 7 7 7 6 6
O O 9 5 θ 3
8 8 7 7 6 6

65 21 4 1	OOOOOO
7B45 B 1	OOOOOO
Bl 67 20 2	OOOOOO
Bl 7β45 6	OOlOOO
76 746119	IOllOO
68 67 6131	3 1 1 1 1 1∣
7θ 61 24 4	O
79 69 35 7	1
7973 5014	2
7672 6026	5
69 68 613B12	
60 59 554219	
		
S3 7429 1 O B5 BO 50 4 O B4 Bl 67 14 O 03 B2 75 35 1 00 0075 49 3 79 79 76 5β 9		I
		
75 74 73 6015 ∣73 72 69 5411		
		
B 6 Bl 51 06Q475	I 2 O 19 O	
		
		
		
		
		
∣βl 6114 62 66 IB		IOOO IOOO	
Θ3 70 B3 74	23 1 O O O 33 2 O O O		
B2 7544 4 BO 765610 77 7462 21 77 75 64 22			OOO IOO 2 0 0 IOO
			
6516 1	OOOO
1136 2	OOOO
竭 5711	OOOO
79 5014	OOOO
70 39 5	OOOO
5516 1	OOOO
30 4 O	OOoO
06 B5 7943 2 0 0	O
06 B5 03 63 BOO	O
06 05 04 76 31 O O	O
B6B5B5 02 59 4 O	O
05 05 0402 74 25 O	O
03 03 03 02 79 52 1	O
Bl Bl BO BO 7β 65 9	O
BO 79 79 79 78 7125	O
7B7β 70 77 76 7132	O
7575 757574 6940	1
7373 7373716636	1
B6B476 561β 2 O	O O	O O
B5B4B16Θ 35 5 O	O O	O O
05 05 03 75 5417 1	O O	O O
06 05 04 00 70 40 7	1 O	O O
04 04 03 02 76612θ	5 1	O O
B3B3B2B2 79 72 5322 4		1 O
Bl BO BO BO 79 76 6B49 22		6 3
BO 79 79 79 7B 77 73 64 4423 IB		
7B 77 77 77 77 75 73 67 5436 28		
75 75 75 75 74 74 72 68 6150 45		
73 73 73 7372 7169 65 5746 42		

Bl 6012 1
02 63 16 O
82 68 22 1
82 73 32 2
02 75 41 4
Bl 7652 B
79 75 5511
7B 75 59 14
77 746017
74 7158 17
72 69 54 14
量
03 73 37 3
02 7441 4
02 7441 4
02 7343 5
BO 7341 5
BO 72 42 7
79 7144 B
7θ 714511
76 704713
74 68 4816
73 53 1β 2
75 5B 23 3
76 65 32 6
7667 39 9
72 664615
69 64 49 21
60564625
,044	10 2000000
,349	13 2000000
,453	16 2000000
,662	23 4100000
,7 66	34 6100000
,6 69	4310 1 O O O O O
,570	5016 3 1 0 0 0 1
S 69 R 68 17 64 14 60	52 21 4 1 1 O 1 1 5325 6 2 1 1 0 1 52 24 7 2 1 1 1 1 47 21 5 1 1 O 1 1
AdVerSaria- accuracy
UlIderreVieW as a COnferenCe PaPer at ICLR 2021

026030
29
23B12
瑞
5620 5 1 0	O	O	O	O	O
67 3βl0 3 1	O	O	O	O	O
75 60 25 7 1	O	1	O	O	O
74 6B 44 14 3	1	1	1	O	O
70 6139 14 4	2	2	1	1	O
70 60 59 35 9	3	3	2	2	1
65 62 57 4110	B	B	5	3	2
65 65 63 5B 42 25 23 20 1611					
61 6159 55 46 33 3128 2417					
Figure 11: Replica of Figure 6 with 50 steps instead of 200 at evaluation time. Deviations in results are minor.