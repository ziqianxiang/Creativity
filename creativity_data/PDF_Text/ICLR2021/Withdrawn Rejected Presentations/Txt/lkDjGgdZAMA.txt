Under review as a conference paper at ICLR 2021
Transferred Discrepancy:	Quantifying the
Difference B etween Representations
Anonymous authors
Paper under double-blind review
Ab stract
Understanding what information neural networks capture is an essential problem
in deep learning, and studying whether different models capture similar features
is an initial step to achieve this goal. Previous works sought to define metrics
over the feature matrices to measure the difference between two models. However,
different metrics sometimes lead to contradictory conclusions, and there has been
no consensus on which metric is suitable to use in practice. In this work, we
propose a novel metric that goes beyond previous approaches. Recall that one of
the most practical scenarios of using the learned representations is to apply them to
downstream tasks. We argue that we should design the metric based on a similar
principle. For that, we introduce the transferred discrepancy (TD), a new metric that
defines the difference between two representations based on their downstream-task
performance. Through an asymptotic analysis, we show how TD correlates with
downstream tasks and the necessity to define metrics in such a task-dependent
fashion. In particular, we also show that under specific conditions, the TD metric
is closely related to previous metrics. Our experiments show that TD can provide
fine-grained information for varied downstream tasks, and for the models trained
from different initializations, the learned features are not the same in terms of
downstream-task predictions. We find that TD may also be used to evaluate the
effectiveness of different training strategies. For example, we demonstrate that the
models trained with proper data augmentations that improve the generalization
capture more similar features in terms of TD, while those with data augmentations
that hurt the generalization will not. This suggests a training strategy that leads to
more robust representation also trains models that generalize better.
1	Introduction
Deep neural networks have achieved great success in many real-world applications, such as image
classification (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), and natural language
processing (Devlin et al., 2018). It is generally agreed in the community that deep learning captures
better representations than the previous hand-crafted feature engineering, which contributes to
a significant performance improvement (Bhardwaj et al., 2018). Therefore, it is worthwhile to
investigate what features1 a neural network learns in practice, which helps us understand the nature
of deep learning. As an initial step towards this challenging task, many people study how different
the features learned by different neural networks are.
Recent works (Li et al., 2015; Raghu et al., 2017; Wang et al., 2018; Morcos et al., 2018; Kornblith
et al., 2019; Liang et al., 2020) have proposed several metrics measuring the difference between a
pair of features learned by two different models. Quite confusingly though, two metrics that both
seem reasonable can even draw opposite conclusions on the same issue. For example, Wang et al.
(2018) used the maximum match under linear transformations as the evaluation metric and found that
the representations of two neural networks trained from different random initializations were utterly
different. In sharp contrast, Kornblith et al. (2019) measured with the central kernel alignment (CKA)
and concluded that the models capture almost identical features. The contradiction caused by using
different metrics is rooted in the disagreement towards the following question (Kornblith et al., 2019):
1Without any confusion, we use the terms feature and representation interchangeably.
1
Under review as a conference paper at ICLR 2021
What does it mean for two representations to be different?
Existing metrics measure the difference directly by feature values, which we believe may not be the
best way. In representation learning, the quality of features learned by a neural network is hardly
evaluated on their values, but rather on the performances they achieve when applied to downstream
tasks (Devlin et al., 2018; Kolesnikov et al., 2019; Chen et al., 2020). Given a trained feature
extractor, we train an additional output head on the features of the data for each downstream task and
consider one feature (or feature extractor) better than the other if it achieves higher downstream-task
performance. Such a way of evaluation has been widely adopted and proven useful in computer
vision (Chen et al., 2020) and natural language processing (Devlin et al., 2018).
Based on the above discussion, we argue that a reasonable representation difference metric should
also be designed by incorporating the downstream tasks. To achieve this, we propose a new metric
which takes the downstream task as an input, and refer to it as the transferred discrepancy (TD).
Given two feature extractors and a set of downstream tasks, we train an output head on top of each
feature extractor per task, and define the TD metric as the difference between the predictions over
the downstream task data. The more different the two feature extractors are, the more likely they
may lead to different predictions on the same downstream task data, and the higher TD value they
will achieve. We analyze the theoretical properties of the TD metric under the linear probing setting,
where the downstream tasks are limited to linear regression. Under this setting, we prove that the TD
metric is invariant under reasonable transformations and analyze its asymptotic limits. Furthermore,
we show that by properly selecting downstream tasks, the TD metric is closely related to existing
metrics such as the maximum match, canonical correlation analysis (CCA), and CKA.
Apart from theoretical analysis, we conduct extensive experiments and demonstrate that the TD metric
can reveal faithful information for various downstream tasks in practice. Importantly, we observe that
the features learned by models trained from different initializations are not quite the same according
to their performances on downstream tasks. Furthermore, we study a quantity called TD robustness,
which is defined as the difference between the predictions of two models trained from different
initializations on the same downstream task. We investigate how TD robustness varies upon changes
in factors such as data augmentation methods and training strategies. Remarkably, we find that TD
robustness is closely connected with the quality of the representation. For instance, data augmentation
methods that are proven to improve the quality of the representation (Shorten & Khoshgoftaar, 2019;
Chen et al., 2020) are also observed to increase TD robustness while transformations that harm the
quality lower it. Such a relationship is also observed for other factors, providing a new perspective on
how various factors in deep learning affect the representation the model learns.
2	Related Work
Many previous works try to understand whether two neural networks with drastically different
parameters but similar high performances learn similar representations (Li et al., 2015; Raghu et al.,
2017; Wang et al., 2018; Morcos et al., 2018; Kornblith et al., 2019; Liang et al., 2020). Using a
neural network’s hidden states as features, these works obtain a feature matrix over a set of samples
for each model, and evaluate the correlation between the two feature matrices with some metrics
taken from matrix theory. For example, Wang et al. (2018) measured the size of the intersection
between two matrices’ row spaces. Raghu et al. (2017) applied CCA to large singular vectors of the
two matrices, which is further improved by Morcos et al. (2018). Kornblith et al. (2019) computed
the norms of the generalized cross-correlation operator between two matrices. However, all of these
works cast the quantification of the difference between two models as a matrix correlation problem,
while the practical usage of the feature is largely ignored.
The proposed TD is motivated by representation learning. In representation learning, the quality of
the learned representations is often evaluated based on their performance on downstream tasks. In
computer vision, the representation of images can be pre-trained using SimCLR (Chen et al., 2020),
and such learned representations can help the model training on twelve downstream classification
tasks. Another widely known milestone in natural language processing is the BERT model (Devlin
et al., 2018). Thus, we think it is more reasonable to define whether two representations are similar
based on their performances on downstream tasks as well.
2
Under review as a conference paper at ICLR 2021
3	Transferred Discrepancy (TD)
Let X be the input space, and xι, ∙∙∙ , Xn be n i.i.d. samples from an underlying distribution
Pdata defined on X. Let Φ(∙) and Φ0(∙) be the two feature extractors, typically neural networks.
Let Zi = Φ(xi) ∈ Rp and Zi = Φ0(xi) ∈ Rp0 be the feature of Xi extracted by Φ(∙) and Φ0(∙)
respectively, i = 1,… ,n. Note that the dimensions of the features, P and p0, are not necessarily
equal, and we assume that P ≤ p0 without loss of generality. Denote Z = (zι,…,Zn) ∈ Rp×n and
Z0 = (z1,…，Zn) ∈ Rp ×n as the feature matrices. Our goal is to quantify the difference between
the two feature extractors Φ(∙) and Φ0(∙).
As previously discussed, in representation learning, the feature extractor is designed and trained to
improve the model’s performance on downstream tasks. Thus, we argue that the difference between
Φ(∙) and Φ0(∙) should be evaluated based on their corresponding performances on downstream tasks.
In practice, a downstream task can be specified by a label vector Y = (yι,…，yn), where yi is the
label of Xi. yi can be a categorical value for classification tasks or a numeric value for regression
tasks. Let hW(Zi) and h0W 0 (Z0i) be the output heads built upon the two feature extractors, W and W0
are the learnable parameters. Given a loss function '(y, y), the parameters W and W0 are obtained
by minimizing the empirical risks:
1n
W = argmin- V'(hw (Zi),yi),
Wn
i=1
1n
W0 = argmin- £'(hw 0 (Zi ),yi).
W0 n	i
i=1
(1)
* i'. TTT-	1 TT-T- ,	Λ .	1	.1	1 ∙ i'i'	Λ	Γ7	1 Γ7 ! ♦	11	.1	1∙
After W and W0 are obtained, the difference between Z and Z0 is measured by the divergence
between hW(Z) and h0W0 (Z0). Let d(u, v) be a symmetric divergence function. The difference
between Z and Z0 on the task with label Y is defined as2:
1n
TD(Z,Z0； Y) = — fd(hw(Zi),hWo(Zi)).
n
i=1
(2)
As we quantify the difference between representations from a feature-transferring perspective, we
name it transferred discrepancy. The TD metric defined on one downstream task may be insufficient
to measure the difference, so we further define the transferred discrepancy on a family of tasks.
Suppose S is a set of label vectors with each vector Y as a task. The TD measured on set S is:
TD(Z, Z0; S) = max TD(Z, Z0; Y).
(3)
The TD metric can reveal the difference between Z and Z0 . Intuitively, when Z and Z0 are very
similar, W and W0 will be similar and h1^(z) and h1^0(z) will not 出ffer much from each other on
every example in the downstream task. Thus, Z and Z0 will have a small TD value. On the contrary,
when Z and Z0 are different features, the prediction h1^ (z) and h1^0 (z) are more likely different and
the value of TD will be large. Although the TD metric is designed from a practitioner’s perspective
and is subject to the downstream tasks, we will show that it has nice theoretical properties and close
relation with previous metrics in the next section.
4	Theoretical Properties of the TD metric
In this section, we theoretically analyze the TD metric under the linear probing setting. We show
that TD is invariant to orthogonal transformation and isotropic scaling (4.1), and further investigate
its asymptotic behavior as n approaches infinity (4.2). Finally, we demonstrate that under certain
conditions, the TD metric and three previous metrics, maximum match, CCA, and CKA, depend on
the same statistics (4.3).
2For ease of understanding, Eqn (2) defines the metric over the training data. Generally, one can also quantify
this distance on unseen data, e.g., the test set in the downstream task, to take the generalization into account. In
our experiments, we use TD metrics over the test data and find the value is similar to that over the training data.
3
Under review as a conference paper at ICLR 2021
4.1	Transformation Invariance
The linear probing setting is widely studied in literature (Alain & Bengio, 2016; Oord et al., 2018;
Hjelm et al., 2018) and applied in practice (Chen et al., 2020; Anand et al., 2019). In this setting,
a linear model is trained on top of a feature extractor, and its performance is used as a proxy
for the quality of the features. Define hW (z) = Wz + b and h0W0 (z0) = W0z0 + b0, where
W ∈ Rp, W0 ∈ Rp0 and b, b0 ∈ R. Let '(y, y) = (y - y)2 be the square loss and d(u, v) = (u - v)2
be the squared distance. Following Kornblith et al. (2019), we assume that both Z and Z> have been
preprocessed to center the rows and Y is centered3 . Besides, we assume that we have enough data
such that n > max{p, p0}, and both empirical covariance matrices ZZ> and Z0Z0> are invertible4 .
Under these assumptions, the optimization problem (1) can be rewritten as
1n	2
W, b = arg min 一 Y^(WZi + b — y∕2,
W,b n i=1
1n
W0, b0 = arg min 一(W(W0zi + b0 — yi)2.
W0,b0 n i=1
(4)
As Z, Z0 are all row-centered, problem (4) has a simple closed-form solution: b = b = 0 and
W = YZ>(ZZ>)-1, W0 = YZ0>(Z0Z0>)-1. Plugging this solution into Eqn (2) yields
12
TD(Z,Z0; Y) = - ∣∣Y[Z>(ZZ>)-1Z - Z0T(Z0Z0T)TZ0] ∣∣2.	(5)
A reasonable metric should have some basic properties. Kornblith et al. (2019) proposed that
a similarity metric of features should have two invariance properties, the invariance to isotropic
scaling, and the invariance to orthogonal transformation. It is straightforward to see that (1). For
anyβ,β0 ∈ R+, TD(Z, Z0; Y) = TD(βZ, β0Z0; Y). (2). For any unitary matrices Q ∈ Rp×p and
Q0 ∈ Rp0×p0, TD(Z, Z0; Y) = TD(QZ, Q0Z0; Y). Thus, Eqn (5) derives that TD is invariant to
isotropic scalings and orthogonal transformations. In fact, such transformation invariance holds not
only for the square loss function but also for many other choices of ` as long as ` is strongly convex.
We leave the discussion in Appendix A.4.
4.2	Convergence Analysis
Since xι,…，Xn are independently sampled frompd&ta, it is natural to require that as n goes to infin-
ity, TD(Z, Z0; S) converges to a value which represents the difference of two feature extractors over
the data distribution on downstream task set S. We denote this value by TD(Φ(pdata), Φ0(pdata); S).
Denote the joint feature distribution for Z and Z0 as (P, P0) := (Φ(pdata), Φ0(pdata)) where P and
P 0 are corresponding marginal distributions. Since the rows of Z and Z0 are centered, we have
Ez~p [z] = 0 and Ez，~po [z0] = 0. Let the covariance matrix of the joint distribution (P, P0) be
AB
BT C . We also assume that the covariance matrices A and C are invertible. Since we seek
to define TD on downstream tasks, the following representative task set contains all possible tasks
associated with Z and Z0 . All proofs can be found in the Appendix A.
Representative Task Set Define S * = {Y = αA- 1 Z + α0C- 1 Z 0 : α ∈ Rp, α ∈ Rp0, kα∣∣2 ≤
-, kα0k2 ≤ -}. This set contains all tasks linearly realizable by Z and Z0 thus covers a wide range of
tasks. A- 1 and C- 1 are used to normalize Z and Z0 so that they have the same level of contribution
to S*. For any downstream task Y in S*, the following theorem rigorously states how the TD metric
depends on Y and the representations Z, Z0.
Theorem 1. Suppose A, B, and C are defined as above. Denote D = A- 1 BC- 1. For downstream
task Y = αA- 1 Z + α0C- 1 Z0, we have
TD(Z, Z0; Y) -a-.s→. α(Ip - DDT)αT + α0(Ip0 - DTD)α0T + 2α(D - DDTD)α0T,	(6)
3Note that this assumption only simplifies the proof. Without such a preprocessing, we can get similar results.
4In practice we have n > max{p, p0} in most if not all cases. Even if ZZ> is not invertible, adding a tiny
noise to Z makes ZZ> invertible.
4
Under review as a conference paper at ICLR 2021
Theorem 1 indicates that in this setting, the transferred discrepancy primarily depends on the task
(i.e., α and α0) and the matrix D which leverages the covariance matrix B between Z and Z0. Here
we show two special cases where Eqn 6 has a simpler form for better understandings:
•	TD for linearly correlated features: If p = p0 and there exists a unitary matrix Q ∈ Rp×p
such that (z, z0) drawn from (P, P0) satisfies z0 = Qz, i.e., Z and Z0 are linearly correlated.
We have D = Q>,so TD(Z, Z0; aA-2Z + α0C-2Z0) -→ 0.
•	TD for independent features: If P and P 0 are independent, then B = 0 and D = 0.
Consequently, TD(Z, Z0; aA-2 Z + α0C-2 Z0) -→ 2.
Next, we provide the convergence analysis of the TD metric on a set of tasks. We first study the
asymptotic limit for TD on the representative task set, and then extend the analysis to the restricted
task sets. Both results show that TD highly relates to the singular value distribution of matrix D .
Theorem 2. Under the notations in Theorem 1, denote σι ≥ ∙∙∙ ≥ σp ≥ 0 be the singular values
of D. If p = p0, or p < p0 and (1 - σp)(1 + σp)2 ≥ 1, we have a closed-form limit of TD on the
representative task Set S *:
TD(Z,Z0; S*) -→ max 2(1 - σj)(1 + σj)2.	(7)
j=ι,…,p
The result shows that the TD metric on the representative task set is closely related to the singular
values of D. For example, if σp is small, there exists a task Y ∈ S* that is close to the row space of
one feature matrix but nearly orthogonal to the row space of the other. Two representations will have
drastically different performance on Y, and TD(Z, Z0; S*) is large. If σp is close to 1, so do all the
singular values, and any task Y ∈ S* will be close to both the row spaces of Z and Z0 . Thus, the
performance is similar, and TD(Z, Z0; S*) is small. When there are both large and small singular
values, the difference between the performance varies a lot on diverse tasks, and evaluating the
difference on specific tasks may be helpful. The following corollary gives a convergence guarantee
for a more practical case for subsets of the representation task set:
Restricted Task Set We study smaller task sets called restricted task sets that allow us to jointly
consider σι, •…，σp instead of σp alone. Particularly, We construct a cascade of sets Si ⊂ ∙∙∙ ⊂
Sp ⊂ S, and the two representations are considered more similar if they have similar performance
on a larger task set. Define Sr = {Y = αA- 1 Z + α0C- 1 Z0 : ∣∣α∣∣2 ≤ 1,kα0∣∣2 ≤ 1, αu7-=
0 and α0vj = 0 if j > r}, r = 1,…，p, where Uj and Vj are the j-th singular vectors of U, V,
in D = UΣV> respectively. When p = p0, We have Sp = S*. The folloWing corollary gives the
asymptotic limits of the TD metric on the restricted task sets:
Corollary 1. Under the notations in Corollary 2 and for the restricted task set Sr defined as above,
we have
TD(Z, Z0; Sr) -a-.s→.	max 2(1 - σj)(1 +σj)2.	(8)
j = 1,,r
We conduct experiments to empirically analyze the singular values of D for some Z and Z0 trained
on the Cifar-10 dataset (Krizhevsky & Hinton, 2009). Results are shown in A.5.
4.3	Connection With Previous Metrics
In this section, we show that our TD metric has a close connection with previous metrics by showing
the matrix D defined in section 4.2 is also the key to derive previous metrics. Previous works lead to
contradictive conclusions by using D in different ways. We only discuss the relationship with CCA
in the main body and leave the discussions on maximum match and CKA in Appendix B.
Canonical Correlation Analysis (CCA) CCA measures the relationship between Z and Z0 by
finding two bases of their row spaces such that when projected onto these bases, the correlation
between the two matrices is maximized. Formally, for 1 ≤ j ≤ p, define the maximum correlation
coefficient ρi by the following optimization problem:
cov(wj>Z, wj0>Z0)
Pj = max — JJ ,
wj ,w0j	var(wj>Z)var(wj0>Z0)	(9)
subject to cov(wk>Z, wj>Z) = 0, cov(wk0>Z0, wj0>Z0) = 0, ∀k < j.
5
Under review as a conference paper at ICLR 2021
Then the summary statistics of CCA is defined as RCCA := Pj=PI Pj. Equivalently, let Q =
Z>(ZZ>)- 1 and Q0 = Z0>(Z0Z0>)-2, then RCCA = kQ PQkF . Since data are independently
sampled from the distribution, we have EZ,Z0[Q0>Q] = D>. By kDk2F = PjP=1 σj2, we have:
Ez,z0 [Rcca]= j 说.	(10)
CCA	p
Moreover, take A = ZZ> and C = Z Z0> as the empirically estimators of A and C. CCA is directly
related toTD on an empirical representative task set S* with Y = αA-2 Z + α0C-2 Z0 as below:
Theorem 3. Let a and ɑ0 be uniformly distributed on the unit ball and A, C defined above. If P = p0,
Eα,α0[TD(Z,Z0; Y = αA-2Z + α0C-2Z0)]=$(1 - RCca).	(11)
p+2
This result reveals the connection between CCA and our proposed metric. CCA is equivalent
to averaging TD on all linearly realizable tasks in the linear probing setting, which makes CCA
downstream-task agnostic. However, when evaluated on a subset of tasks, or several tasks of
interest, CCA cannot provide fine-grained information, while the TD metric is always faithful to the
downstream tasks. In the next section, we conduct extensive experiments to study the performance of
the TD metric in practical applications.
5 Experiments
In this section, we empirically compare one TD induced metric, TDcls, with CCA and CKA for
classification tasks. We directly use the CKA with linear kernel since it performs similarly with CKA
using other kernels (Kornblith et al., 2019). For convenience, we define the three metrics as below:
•	DCCA(Z, Z0) = 1 - R2CCA
DCKA(Z, Z0) = 1 - SCKA(Z>Z, Z0>Z0) = 1-
kZZ0>kF ,
PZTFlZZTF
•	TDcls(Z,Z ; Y) = n PPi=I IargmaX h(zi) = arg max h0(zi)
where arg max h(z) denotes the output prediction for z. It is easy to see that TDcls measures the
fraction of different predictions between two representations on the downstream task Y. We also try
some other distance metrics and obtain similar conclusions. The results are in Appendix D. We will
firstly show that the TD metric is valid with a sanity check, and then investigate whether models that
are trained using different initializations learn similar features. Lastly, we will define TD robustness
and study the TD robustness of different training strategies. We will also establish a connection
between TD robustness and the quality of the representation.
5.1	A Sanity Check
In this section, we demonstrate the validity of the TD metric by a simple sanity check: given three
feature extractors Φ1 , Φ2, Φ3, such that Φ1 and Φ2 capture similar features than Φ1 and Φ3 by some
prior knowledge, we check whether the value of the TD metric is consistent with the prior.
To build this sanity check, we train Φ1 and Φ2 using similar datasets, but train Φ3 using a completely
different dataset. Specifically, we design Cifar-2 and Cifar-5 by grouping Cifar-10’s labels into 2
and 5 groups respectively. Therefore, Cifar-2, Cifar-5, and Cifar-10 datasets share the same input
data but use slightly different labels. These three datasets serve as candidates for training Φ1 and
Φ2. Φ3 is trained using the SVHN dataset. All feature extractors use ResNet32 architecture, and
the classification head is removed after training. For simplification, we use the Cifar-10 task as the
downstream task. We apply Φ1, Φ2, Φ3 to the training data, and train a classification head for Cifar-10
using logistic regression. After training, we apply the three classifiers to the test set and calculate
TDcls. For a fair comparison, both DCCA and DCKA are also calculated on the test samples. Each
experiment is conducted for ten times. All details can be found in Appendix C.
6
Under review as a conference paper at ICLR 2021
We compare the difference between pairs of models using different metrics and list all results in Table
1. It can be seen that both DCCA and DCKA have smaller values in the first three rows compared
with the other rows, which indicates that the features learned from Cifar-2/5/10 are more similar, but
are very different from the features learned from SVHN. TDcls also has a similar trend: the values of
TDcls in the first three rows are smaller than 0.5 while the values in the other rows are all larger than
0.8. Thus, all these three metrics are reasonable and consistent with our prior knowledge.
Table 1: The sanity check result.
Model 1	Model 2	Downstream	DCCA	DCKA	TDcls
Cifar-10	Cifar-5		0.7642	0.3024	0.2158
Cifar-10	Cifar-2	Cifar-10	0.8323	0.5330	0.4958
Cifar-5	Cifar-2		0.8030	0.4530	0.4944
Cifar-10	SVHN		0.9218	0.9713	0.8001
Cifar-5	SVHN	Cifar-10	0.9793	0.9710	0.8049
Cifar-2	SVHN		0.9163	0.9782	08163
5.2	Does Initialization Affect Learned Features ?
In this section, we address the problem widely studied by previous works (Wang et al., 2018; Kornblith
et al., 2019; Morcos et al., 2018): whether models trained from different random initializations learn
different features. We train two ResNet32 networks on the training set of Cifar-5/2 using the same
setting (dataset, algorithm, hyperparameters, etc.) but with different initializations. We use three
downstream tasks, Cifar-10/5/2. DCCA DCKA are directly computed over the test set. For TD, we
first train the output heads of the models on the training data and then compute TDcls on the test data.
In Table 2, we repeat the experiment ten times and report the average values for each row.
Table 2: Similarity of the models learned with different initializations.
Model 1	Model 2	Downstream	DCCA	DCKA	TDcls
Cifar-5	Cifar-5	Cifar-10	0.6961	0.0835	0.2139
		Cifar-5			0.0442
		Cifar-2			0.0109
Cifar-2	Cifar-2	Cifar-10	0.6931	0.0402	03745
		Cifar-5			0.2631
		Cifar-2	—			0.0164
It can be seen that TDcls provides more information than CCA and CKA. Since CCA and CKA are
downstream-task-agnostic, they can only present one scalar value for each pair of models. However,
TDcls outputs different values for different downstream tasks. For example, the two models trained
on Cifar-5 have very consistent predictions on Cifar-2 and Cifar-5, but their predictions are much
more different on Cifar-10: they disagree on 21.39% of the Cifar-10 test samples. It can be also
observed that the two models trained on Cifar-2 behave similarly on the Cifar-2 downstream task but
quite differently on Cifar-5 and Cifar-10.
We find that the two feature extractors trained from different initializations do not learn the same
features. First of all, we show that our experimental setting is reasonable since the downstream tasks
have a close connection with the upstream tasks. To verify this, we find that the model trained from
the Cifar-5 task can reach 80% accuracy on the Cifar-10 downstream task. This result indicates
that for those models, the features can be transferred from one task to the other. However, in this
reasonable setting, the disagreements between representations are high, i.e., 21.39% for two Cifar-5
models and 37.45% for two Cifar-2 models. Such difference indicates that the features of these
models are not the same when evaluated on Cifar-10. Our finding is consistent with the results in
Morcos et al. (2018) that models trained with different initializations can capture different features,
but differs from the results in Kornblith et al. (2019).
5.3	TD Robustness and Its Application to Training Strategy Evaluation
Deep learning practitioners design different training strategies in the hope of enhancing the quality
of features captured by the model. In this section, we build a connection between the quality of the
7
Under review as a conference paper at ICLR 2021
learned representations and the TD robustness of the implemented training strategy. A training strategy
is said to be TD-robust if it leads to models with consistent predictions on the same downstream task
when trained from different initializations. We study the effect of different factors on TD robustness.
Specifically, we focus on three factors: data augmentation, learning rate schedules and adversarial
training. Experimental details are left in the Appendix C.
Data Augmentation We study three augmentation methods: random flipping, random cropping,
and adding Gaussian noise. Random flipping and cropping are widely used in practice and believed
helpful to generalization (Shorten & Khoshgoftaar, 2019). Gaussian additive noise is used for learning
smooth classifiers (Cohen et al., 2019).
	Table 3: The effect of difference factors on the TD robustness.					
Factors	Configuration	Upstream	Downstream	TDcls
Data Augmentation	Without augmentation Random flipping Random flipping + croping Random flipping + croping + Gaussian additive noise	Cifar-5	Cifar-10	0.2812 0.2739 0.2150 0.2542
LR Schedule	Small LR + without decay Large LR + without decay Large LR + with decay	Cifar-5	Cifar-10	0.2521 0.2770 0.2313
Std / Adv training	Standard Training Adversarial Training	Cifar-5	Cifar-10	0.2150 0.1823
We report the results in Table 3. It can be seen that using cropping and flipping leads to more similar
representations, and cropping has a significant impact on TDcls. On the other hand, adding Gaussian
noise has the opposite effect and makes representations less similar. This result aligns well with
previous works that show random flipping and cropping can improve the quality of the representation,
but Gaussian noise hurts the quality (Chen et al., 2020).
Learning Rate Schedule Practically, a learning rate decay scheduler is usually used for the sake
of finding the local minima. We here show that models using a learning-rate decay schedule also
learn more similar features across different initializations. The result in Table 3 shows that the
"Large LR + with decay" schedule leads to more similar representations than the other two schedules.
This observation coincides with the fact that learning rate decay helps improve the quality of the
representation in practice, and validates the theoretical finding in Li et al. (2019).
Adversarial Training Recently, Tsipras et al. (2019) and Ilyas et al. (2019) showed that adversarial
training helps neural networks learn features that align better with human perceptions and Santurkar
et al. (2019) used robust network to boost performance in synthesis tasks. We find that adversarial
training is more TD-robust than standard training, as shown in Table 3. Our result suggests that
adversarial training may capture features with better qualities even though it lowers the accuracy.
TD Robustness Versus the Quality of the Representation Our experimental results demonstrate
a strong connection between TD robustness and the quality of the representation: a training strategy
leading to better qualities produces better TD robustness. This connection makes TD a useful tool for
investigating the effect of different factors on representations and designing new training strategies.
6 Conclusion and Future Work
In this work, we propose the Transferred Discrepancy, a metric that quantifies the difference between
two representations using the difference between their performance on the same set of downstream
tasks. Our theoretical analysis founds a solid basis for TD and reveals the connection between TD and
previous metrics. We also conduct extensive experiments to study how different training factors affect
the difference between two representations trained from different initializations. We believe that
using downstream-task performances to study the difference between learned features is promising,
and in the future we will investigate more factors such as normalization and dropout. At the same
time, we would also like to extend the current setting to study deep transfer learning.
8
Under review as a conference paper at ICLR 2021
References
Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes.
arXiv preprint arXiv:1610.01644, 2016.
Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-AleXandre Cδte, and R Devon Hjelm.
Unsupervised state representation learning in atari. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d’Alche Buc, E. FoX, and R. Garnett (eds.), Advances in Neural Information Processing Systems
32, pp. 8769-8782. Curran Associates, Inc., 2019.
Anurag Bhardwaj, Wei Di, and Jianing Wei. Deep Learning Essentials: Your hands-on guide to the
fundamentals of deep learning and neural network modeling. Packt Publishing Ltd, 2018.
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certified adversarial robustness via randomized
smoothing. In ICML 2019 : Thirty-sixth International Conference on Machine Learning, pp.
1310-1320, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, and Aleksander
Madry. Learning perceptually-aligned representations via adversarial robustness. arXiv preprint
arXiv:1906.00945, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
R Devon Hjelm, AleX Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maXimization. arXiv preprint arXiv:1808.06670, 2018.
Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Adversarial eXamples are not bugs, they are features. In Advances in Neural Information
Processing Systems, pp. 125-136, 2019.
AleXander Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representa-
tion learning. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition,
pp. 1920-1929, 2019.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey Hinton. Similarity of neural
network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.),
Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings
of Machine Learning Research, pp. 3519-3529, Long Beach, California, USA, 09-15 Jun 2019.
PMLR.
A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Handbook of
Systemic Autoimmune Diseases, 1(4), 2009.
AleX Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
YiXuan Li, Jason Yosinski, Jeff Clune, Hod Lipson, and John Hopcroft. Convergent Learning: Do
different neural networks learn the same representations? arXiv e-prints, art. arXiv:1511.07543,
November 2015.
9
Under review as a conference paper at ICLR 2021
Yuanzhi Li, Colin Wei, and Tengyu Ma. Towards explaining the regularization effect of initial
large learning rate in training neural networks. In H. Wallach, H. Larochelle, A. Beygelzimer,
F. d’ Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems
32, pp.11669-11680. Curran Associates, Inc., 2019.
Ruofan Liang, Tianlin Li, Longfei Li, Jing Wang, and Quanshi Zhang. Knowledge consistency
between neural networks and beyond. In International Conference on Learning Representations,
2020.
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083,
2017.
Ari Morcos, Maithra Raghu, and Samy Bengio. Insights on representational similarity in neural
networks with canonical correlation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 5727-5736. Curran Associates, Inc., 2018.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
Maithra Raghu, Justin Gilmer, Jason Yosinski, and Jascha Sohl-Dickstein. Svcca: Singular vector
canonical correlation analysis for deep learning dynamics and interpretability. In I. Guyon, U. V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 30, pp. 6076-6085. Curran Associates, Inc., 2017.
Shibani Santurkar, Andrew Ilyas, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander
Madry. Image synthesis with a single (robust) classifier. In H. Wallach, H. Larochelle, A. Beygelz-
imer, F. dAlche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing
Systems 32, pp. 1262-1273. Curran Associates, Inc., 2019.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of Big Data, 6(1):60, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and Aleksander Madry. Ro-
bustness may be at odds with accuracy. In International Conference on Learning Representations,
2019.
Liwei Wang, Lunjia Hu, Jiayuan Gu, Zhiqiang Hu, Yue Wu, Kun He, and John Hopcroft. Towards
understanding learning representations: To what extent do different neural networks learn the
same representation. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 31, pp. 9584-9593. Curran
Associates, Inc., 2018.
10
Under review as a conference paper at ICLR 2021
A Asymptotic Limits of Transferred Dis crepancy
A.1 Proof of Theorem 1
Suppose the covariance matrix of the joint distribution is (Φ(pdata), Φ0(pdata)). By the law of
large numbers, as n → ∞, ZZ> → A, ZZ> → B and ZZ> → C almost surely. Denote
D = A-2 BC- 1. For downstream task Y = αA- 1 Z + α0C- 1 Z0, We have
TD(Z,Z0; Y) =1 ∣∣ɑA-2 [ZZ>(ZZ>)-1Z - ZZ0>(Z0Z0>)-1Z0]
+α0A-2 [Z0Z>(ZZ>)-1Z - Z0Z0>(Z0Z0>)-1Z0] 11：
-→ 1 ∣∣αA-2(Z - BC-1Z0) + α0C-2(B>A-1 Z - Z0)∣∣j
=1 ∣∣αA-1 (Z - BC-1Z0)∣∣2 + ∣∣α0C-2(B>ATZ - Z0)∣∣j
+ 2aA-2(Z - BC-1Z0)(B>A-1Z - Z0)>C-2α0>]
-a-.s→. α(Ip - DD>)α> + α0(Ip0 - D>D)α0> + 2α(D - DD>D)α0>,
Thus, we have the results in Theorem 1. It shows that the difference between Z’s and Z0’s performance
primarily depends on the matrix D and the task Y (α, α0).	□
A.2 Proof of Theorem 2
When evaluated on the representative task Set, S * = {Y = aA- 1 Z + a0C- 2 Z 0 : a ∈ Rp, a ∈
Rp0, kαk2 ≤ 1, kα0 k2 ≤ 1}, the transferred discrepancy is
TD(Z, Z0; S*) =	sup	TD(Z, Z0; YαA-2Z + α0C-2Z0)
kαk2 ≤1,kα0 k2 ≤1
-→ lim sup	TD(Z, Z0; YαA-2Z + α0C-1Z0).
n→∞ kαk2≤1,kα0k2≤1
(13)
The TD metric is a polynomial function of α and α0 and the limit only affect the coefficients, and
A = {(α, α0)∣kα∣∣2 ≤ 1, kα0∣∣2 ≤ 1} is a compact set in Rp+p0. Thus, the lim and the sup in Eqn
(13) is interchangeable. According to Theorem 1, there is
TD(Z, Z0; S*) —→ sup lim TD(Z, Z0; YαA-2Z + α0C-2Z0)
(α,α)∈A n→∞
= sup [α(Ip - DD>)α> +α0(Ip0 - D>D)α0> + 2α(D - DD>D)α0>].
(α,α)∈A
(14)
Let the singular values of D be σι ≥ …≥ σp, and the SVD of D be D = UΣV>. Since the
AB
covariance matrix B> C 0, its schur complement A - BC-1B>	0. Thus,
A - BC-1B> = A2 (Ip - A-1BCTB>A-2) A1 = A2 (Ip - DD>) A1 占 0,	(15)
which implies that 1 ≥ σι ≥ ∙∙∙ ≥ σp ≥ 0. Denote αU = β = (βι,…，βp) and α0V = β0 =
(β1 ,∙∙∙,βp 0). Then kβ∣∣2 ≤ 1,kβ 几 ≤ 1,and
p	p0
TD(Z, Z0; S*) -a-.s→. sup[X(1 - σj2)(βj2 + βj02 + 2σj βj βj0) + X βk02].	(16)
β,β0 j=1	k=p+1
11
Under review as a conference paper at ICLR 2021
When p = p0, Cauchy-Schwarz inequality guarantees that 2βj βj0 ≤ βj2 + βj02 , where the equation
holds when βj = βj0 . It follows that
p
TD(Z, Z0； S*) -→ SUp E(I- σj)(1 + σj)2居 + βj2)
β,β0 j=1
= max 2(1 -σj)(1+σj)2.
j = 1,…，p
(17)
When p < p0, if there exists an i such that (1 - σj)(1 + σj)2 ≥ 1, then the optimal β satisfies
βp+1 = •…=βp0 = 0, so we can achieve the same result as (17). Thus, We prove the results in
Theorem 2.	□
A.3 Proof of Corollary 1
Here, we use the same notation as the previous section. Based on the results in Theorem 2, we now
consider restricted task sets. In Sr = {Y = αA- 1 Z + α0C- 1 Z0 : kα∣∣2 ≤ 1, kα0∣∣2 ≤ 1, αuj-=
0 and α0vj = 0 ifj > r}, we have βj = βj0 = 0 for j > r. It yields
r
TD(Z, Z0; Sr) -a-.s→. sUp X(1 - σj)(1 + σj)2 (βj2 + βj02)
β,β0 j=1
= max 2(1 -σj)(1 +σj)2.
j=1,…，r
(18)
which concludes the proof.
□
A.4 The Invairance Properties of the TD metric
For a general loss function '(hw(z), y) that is Lipschitz, strongly convex in hw(Z) and satisfies
'(y, y) = 0, we will show that it also satisfy the two invariance properties mentioned in Section 4.
When training the output head’s parameter W from
1n
W = argmin- ɪ2'(hw(zi),yi)),	(19)
W ni=1
there exists only one optimal hW (zi ) due to the convexity of the loss function `. Denote the output
set of hw(Z) as SZ := {h1^(z)∣∀W}. It is easy to see that, if there is a linear transformation
directly applied to z by hW (z), the output set will be invariant to isotropic scaling and orthogonal
transformation. SZ = SβZ for all β and SZ = SQZ for all unitary matrix Q. Although the optimization
algorithm may find different W, the output hw (zi ) remains unchanged for each data. Therefore, the
TD metric defined as TD(Z, Z0; Y) = η Pn=ι d(hw(zi), hw0 (Zi)) is invariant to isotropic scaling
and orthogonal transformation.
A.5 Empirical Analysis of the Distribution of Singular Values
Previous theoretical analysis shows that in the linear probing setting, the TD metric is closely related
to the distribution of singular values of D = A- 1 BC-2. If all the singular values are large, then
the two representations will have similar performance on all the tasks in the representative set. If
there exists a small singular value, then there exists a direction that is close to one representation
matrix’s row space but nearly orthogonal to the other representation matrix’s row space. On a task
y ∈ S * in this direction, these two representations will have drastically different performance. When
all the singular values are small, these two representations will perform differently on all tasks in
the representative task set. Now, we empirically look at the distribution of singular values of D in
practice.
We train two ResNet32 models on the Cifar-10 training set with different random initializations
(i.e. seeds), extract the representation matrix on the Cifar-10 test set, and calculate the empirically
12
Under review as a conference paper at ICLR 2021
Figure 1: Singular values of D
estimator D = (ZZ>)- 1 ZZ0>(Z0Z0>)-2. The training hyperparameters are the same as in C.1.
We plot the distribution of the singular values of D in Figure 1. It can be seen that the singular values
larger than 0.6 take only a small fraction among all the singular values. Most singular values are
small, and there exist singular values close to 0. Thus, the two representations will have similar
performance on the task related to the large singular value and have drastically different performance
on the task related to the small singular value. Therefore, a universal similarity index may not reveal
all the difference, and one may consider evaluating the two representations on a set of downstream
tasks of interest. When evaluated on the restricted task set we proposed in Section 4.2, the TD is
0.288 on S1 and 0.649 on S5 . When using TD in practice, the task set can also be selected as various
tasks we want to deal with using the pre-trained feature extractor.
In Section 4.3, Theorem 3 provides that the CCA index obtains a universal index by taking the average
over all tasks in the representative set or over the squares of the singular values. Since most of the
singular values are small and the performance is different on most tasks, the CCA value shall be large
and varies little for different models. In Section D’s experimental results, the CCA value is always
larger than 0.6 and remains stable when trained with different techniques of data augmentation and
different training strategies.
B Transferred Discrepancy versus the Maximum Match, CCA and
CKA
B.1 Main Results for the Maximum Match and CKA
In Section 4.3 we discussed the relationship between CCA and TD. Here we present our main results
for the maximum match and CKA, which show that both of them are closely related to the matrix D.
Maximum Match Max-match measures the intersection between two subspaces of the row spaces
of Z and Z0. Denote the row vectors of A-2 Z by Zrow = {z(1), ∙∙∙ , z(P)} and the row vectors of
C-2Z0 by Zrow = {z0⑴,•…,z0(p0)}. Let Zrow and Z0row be the largest subspaces of Zrow and
Zrow such that they are e-close to each other (e ≥ 0). Here closeness means that for any Z⑴ ∈ Zrow,
minz0∈span(Zo ) l∣z(" — z0l∣2 ≤ √ne and vice versa. The max-match similarity index is defined as
Smax-match
(e)
I ^	^0
|Zrow | + |Z row |
p + p0
(20)
Compared with the original definition in Wang et al. (2018), We normalize Z and Z0 by A- 1 and
C-1, so that they are at the same scale with respect to e. We also add √n in the definition of e-
closeness so that the result is meaningful as n → ∞. The following theorem provides the relationship
between Smax-match and D:
13
Under review as a conference paper at ICLR 2021
Theorem 4.	Let S = Ez,z0 [Smaχ-match (e)]. Denote the singular values of D by σι ≥ …≥ σp.
Let k be the largest integer such that
σ2 + …+ σ2 ≥ k(1 - e2),
Then we have
(21)
2k
s ≤ ------;
p + p0
(22)
(Wang et al., 2018) found that the maximum match similarity is very small when ≤ 0.3. Figure 1
shows that large singular values account for a very small fraction of all singular values of D. Thus,
it is a natural consequence of Theorem 4 that Smax-match () should be very small. This metric
investigate the two feature matrix under linear transformation, and Liang et al. (2020) extended it to
non-linear transformation using several convolution and activation layers.
Centered Kernel Alignment (CKA) Kornblith et al. (2019) proposed CKA based on dot product
and its extension in reproducing Hilbert spaces. Particularly, the linear CKA similarity index is
defined as
SCKA(Z>Z,Z0>ZO)= kZZ>ZFZZ0Z0>kF .	(23)
Theorem 5.	The expectation of CKA is
E[CKA(Z>Z, Z0>Z>)] =	Jr(DCDɪA=.	(24)
,	ptr(A2)ptr(c2)	' '
This result is straightforward from the definition. It indicates that the CKA gets a universal similarity
index by re-weighting the matrix D with matrices A and C.
B.2 Lemma for Theorem 3
Lemma 1. If X ∈ Rp is uniformly distributed in the unit ball, then Ex 1 =万+^.
Proof We directly compute this expectation:
Ex21
R-1 Rx2 + ∙∙∙+xp≤1-x1 x21dVdx1
R-1 Rx2 +…+xp≤1-x2 dVdxι
(25)
Denote the volume of the unit ball in Rp by Vp. We have fχ2+…十乂2登2 dV = tp-1Vp-ι. Denote the
Beta function by B(P, Q). For the numerator, let t2 = 1 - x12, and we have
x12dVdx1 = Z tp-1Vp-1x21dVdx1
…+xρ≤1-x2	--1
1
Vp-IJ (1 - x2)ɪɪx2dxι
1
(1 — x2) P-2^ dxι
-1
「Cosp θdθ -
-Z (1 - x2)p+1 dx)
-1	(26)
Z 2 cosp+2 θdθ
11, 2)- B( p+3，I)
J B( p+1,1).
1
P + 2
14
Under review as a conference paper at ICLR 2021
Similarly, the denominator is equal to Vp-ιB(p++1,11). Thus, Ex2 = p+112.	口
B.3	Proof of Theorem 3
Suppose DD is defined as the empirical estimator of D, i.e, DD = (Z0Z0>)-2Z0Z>(ZZ>)-2. Denote
the singular values of D as 1 ≥ σι ≥ ... ≥ σp ≥ 0. Similar to Eqn (6) and Eqn (12), We have
Eα,α0 ITD(Z, Z0; Y = αA-2Z + α0C-2Z0)]
=Eα,αo Ia(IP - DD>)a> + α'(Ipo - D>D)α0> + 2a(D - DD>D)α0>]
0
p	p0
=Eβ,β0	X(I - σ2)(β2	+	β02	+	2σiβiβi)	+ X	βk2	.
j=1	k=p+1
(27)
Here we also change the variable a, a0 to β, β0 with β = aU and β0 = a0V. U and V are
the orthogonal matrices in D’s singular value decomposition D = U ΣV > . Since a is uniformly
distributed in the unit ball in Rp, so does β. Similarly, β0 is uniformly distributed in the unit ball in
Rp0. Thus, Eejej = 0∙ It follows by Lemma 1 that Eβ1 = p++1 for all j ∈ [p], and Eβk1 = pr^ for
all k ∈ [p0]. Therefore,
Eαa [TD(Z, Z0; Y = aA- 1 Z + a0C- 1 Z0)] = P - Pj= % + P - P=1 % .	(28)
p + 2	p0 + 2
Similar to Eqn (10), RCCA = kDpkF = PpPI ^2. Thus, we have
Eα,α0[TD(Z, Z0; Y = aA-2Z + a0C-2Z0)]=冬(1 - RCca).	(29)
P+2
This results shows that the calculation of RCCA is equivalent to averaging on the TD over all linearly
realizable tasks. This averaging produces a downstream-task-agnostic metric but dismissing many
information. Consequently, the CCA varies little in all the experiments and is insensitive to the
change of training strategies.	口
B.4	Lemma for Theorem 4
Lemma 2. Suppose that A ∈ Rn×n is a symmetric semi-positive definite matrix. Let the eigenvalues
of A be 0 ≤ λ1 ≤ λ1 ≤ ∙∙∙ ≤ λn. For a constant E ≥ 0, let k be the largest integer such that
λ1 + •…+ λk ≤ ke. Suppose that v1, ∙∙∙ , Vm ∈ Rn are unit vectors orthogonal to each other, i.e.
IIviIl2 = 1 forall i and ViV> = 0 forall i = j. If ViAv> ≤ E for all i = 1,…，m, then m ≤ k.
Proof There exists a unitary matrix Q such that A = QΛQ>, where Λ = diag{λ1, λ1, •…，λn}.
Let Ui = Qvi, i = 1, ∙∙∙ , m. Due to the property of unitary matrices, Ui are unit vectors orthogonal
to each other. Denote Ui = (u1, u2,…，u?). Then, we have
n
E ≥ viAvi> = UiΛUi> = Xλj(Uij)2.	(30)
j=1
Let xj = Pim=1(Uij)2. Since Ui are unit vectors orthogonal to each other, we have 0 ≤ xj ≤ 1 and
x1 +	+ Xn = m. Thus,
mn	n
me ≥ XX λj (uj)1 = X λjXj ≥λ1 + …+ λm.
i=1 j=1	j=1
Therefore, by the definition of k we have m ≤ k .
(31)
□
15
Under review as a conference paper at ICLR 2021
B.5	Proof of Theorem 4
Let Zrow and Z0row be the two largest subspaces described in the theorem. For each z(i) ∈ Zrow ,
there exists an ai ∈ Rp0 such that ∣∣z(i) - aiC- 1 Z0∣∣ ≤ √ne. Let 1i = (0,…，0,1,0,…，0) be
a p-dimensional row vector whose ith element is 1 and the rest are 0. Thus, bi = (1i, -ai) satisfy
A-2ZZ>A-2
C- 1 Z 0Z >A-2
A- 1 ZZ 0>C- 2
C - 1 Z 0Z 0>C - 2
b> = ∣∣z(i) - aQ- 1 Z0k2 ≤ ne2.
(32)
By taking the expectation we have
2 ≥bi	DIp>	ID	bi>	=	1i(Ip - DD>)1i> +	∣ai	-	1iD∣22	(33)
Thus, for every z(i) ∈ Zrow We have
1i (Ip - DD> )1i> ≤ 2	(34)
The eigenvalues of Ip 一 DD> are 0 ≤ 1 一 σ2 ≤ ∙∙∙ ≤ 1 一 σp. Since 1i are unit vectors orthogonal
to each other, using Lemma 2 we have |ZZrow | ≤ k. Similarly, for each z0(j) ∈ Z0row,
1j(Ip0 一 D>D)1j> ≤ 2 .	(35)
ConSeqUently, ∣Z0row∣ ≤ k. Thus, S ≤ p++kp7.	□
C Experimental Settings
All experimental settings are listed in this section and results are reported in Section D.
C.1 Settings of Sections 5.1 and 5.2
The Design of Upstream Tasks and Downstream Tasks We design several tasks based on the
CIFAR-10 dataset and the SVHN dataset. For the CIFAR-10 dataset, we manually group the labels to
design three different tasks: Cifar-10, Cifar-5 and Cifar-2, each of which can be used either as an
upstream task or as a downstream task. The Cifar-10 task uses the original CIFAR-10 labels, and
Cifar-5 and Cifar-2 are built by semantically regrouping those 10 classes into 5 and 2 categories, as
shown in Table 4. Figure 2 provides a visualization of the categories of Cifar-5 and Cifar-10. For the
SVHN dataset, we directly use its original labels.
Table 4: Two tasks induced from CIFAR-10: Cifar-2 task(first table) and Cifar-5 task(second table).
Cifar-2 Category	Classes
Man-made transport	Airplane, Automobile, ship, truck
animals	Bird, cat, deer, dog, horse, frog
Cifar-5 Category	Classes
Cars	Automobile, truck
Large mammals	Deer, horse
Medium mammals	Cat, dog
Large transport	Ship, airplane
Non-mammals	Frog, bird
Architecture We use ResNet-32 (He et al., 2016) for all tasks.
16
Under review as a conference paper at ICLR 2021
Figure 2: A visualization of Cifar-2 (left) and Cifar-5 (right).
Animals
Man-made
Things
Upstream Task Training (Feature Extractor) For any upstream task, we train the model for 200
epochs with SGD and the batch size is set to 128. The learning rate is initially set to 0.1 and decayed by
0.1 at epochs 60 and 120. The momentum is 0.9, and the weight decay rate is 5e-4. Unless explicitly
stated, random cropping and random horizontal flipping are applied for data augmentation. After
training, we remove the network’s last fully-connected layer (output head) and take the remaining
network as the feature extractor.
Downstream Task Training For any downstream task, we add a linear layer with softmax on top
of the feature extractor as the output head. We freeze the feature extractor and only fine-tune the
output head using the training set of the downstream task. The output head is trained for 50 epochs
with SGD and the batch size is set to 128. All other hyperparameters are set to the same value as the
upstream task training. The model’s performance on the test set is reported.
More TD Metrics for Evaluation Recall that the TD metric is defined as d(hwι (z), h1^2 (z)) in
(2), where d(∙, ∙) is a distance metric. We consider two TD metrics defined with different d(∙, ∙):
•	Soft Distance: TDsoft(Z,Z0) = W1(hw1 (Z),hw2(Z0)) = n Pn=1 2l∣hwι(Zi)-
hw2(Zi0)k1,
•	Hard Distance: TDhard(Z, Z0) = n1 P乙 1arg max hW1 (Zi )=arg max hW2 (Zi0 ) .
TDhard is used in the paper as TDcls, which directly measures the fraction of samples on which
the two models have different predictions. TDsoft defined with the l1 distance is equivalent to the
Wasserstein distance between the two probability densities. Both TDhard and TDsoft are metrics
between [0,1], and small numbers indicate more similar representations. Although TDsoft and
TDhard are based on different distance metrics, they exhibit similar trends in most experiments.
Tasks Based on CIFAR-100 We also conduct experiments with the CIFAR-100 dataset, in which
each sample has two levels of labels: a fine label (100 classes) and a coarse label (20 superclasses)
as listed in Table 5. The two levels of original labels produce two tasks: C 100 and C 20. We
further design two tasks, C 10 and C 4, by semantically regrouping the samples based on the labels’
relationship extracted from the WordTree in ImageNet (Krizhevsky et al., 2012). Details are shown
in Table 6.
17
Under review as a conference paper at ICLR 2021
Table 5: Two original labelings from CIFAR-100: 100 classes and 20 superclasses.
Superclass (C 20 Category)	Classes
aquatic mammals	beaver, dolphin, otter, seal, whale
fish	aquarium fish, flatfish, ray, shark, trout
flowers	orchids, poppies, roses, sunflowers, tulips
food containers	bottles, bowls, cans, cups, plates
fruit and vegetables	apples, mushrooms, oranges, pears, sweet peppers
household electrical devices	clock, computer keyboard, lamp, telephone, television
household furniture	bed, chair, couch, table, wardrobe
insects	bee, beetle, butterfly, caterpillar, cockroach
large carnivores	bear, leopard, lion, tiger, wolf
large man-made outdoor things	bridge, castle, house, road, skyscraper
large natural outdoor scenes	cloud, forest, mountain, plain, sea
large omnivores and herbivores	camel, cattle, chimpanzee, elephant, kangaroo
medium-sized mammals	fox, porcupine, possum, raccoon, skunk
non-insect invertebrates	crab, lobster, snail, spider, worm
people	baby, boy, girl, man, woman
reptiles	crocodile, dinosaur, lizard, snake, turtle
small mammals	hamster, mouse, rabbit, shrew, squirrel
trees	maple, oak, palm, pine, willow
vehicles 1	bicycle, bus, motorcycle, pickup truck, train
vehicles 2	lawn-mower, rocket, streetcar, tank, tractor
Table 6: CIFAR-100-induced task: C 4 (first table) with four classes and C 10 (second table) with 10
classes.____________________________________________________________________________
C 4 CategOry		Superclass
mammals	Aquatic mammals, large carnivores, large omnivores and herbivores, small mammals, medium-sized mammals, people
Non-mammals	Fish, Reptiles, insects, non-insect invertebrates,
Man-made things	Vehicles 1, vehicles 2, Food containers, household electrical devices, household furniture, Large man-made outdoor things
Natural things and plants	Trees, flowers, fruit and vegetables, Large natural outdoor scenes
C 10 Category	Superclass
Aquatic animals	Aquatic mammals, fish
Large animals	large carnivores, large omnivores and herbivores
Medium and small mammals	small mammals, medium-sized mammals
Vehicles	Vehicles 1, vehicles 2.
Other animals	Reptiles, insects, non-insect invertebrates
People	people
Plants	Trees, flowers, fruit and vegetables
Household	Food containers, household electrical devices, household furniture
Large man-made outdoor things	Large man-made outdoor things
Large natural outdoor scenes	Large natural outdoor scenes
C.2 Settings of Section 5.3
Data Augmentation We study three data augmentation techniques: random flipping, random
cropping (padding = 4), and adding Gaussian noise (σ = 0.1).
18
Under review as a conference paper at ICLR 2021
Learning Rate Schedule We train the feature extractor for 100 epochs with three learning rate
schedules. In the “Small LR + without decay" schedule, the learning rate is fixed at 0.01; in the
“Large LR + without decay" schedule, the learning rate is fixed at 0.1; and in the “Large LR + with
decay" schedule, the learning rate is 0.1 in the first 50 epochs and 0.01 in the last 50 epochs.
Adversarial training Adversarial training (Madry et al., 2017) is one of the successful training
method to improve the robustness of neural networks to adversarial attacks. The main idea to add
adversarial examples into the training set during training to improve robustness. We further investigat
how adversarial training affect TD robustness. We use PGD-7 to generate adversarial examples and
set the perterbation to 篇 and step size to 短 during training. When trained on CIFAR-10, this
setting of the adversarial training can achieve 79.22% clean accuracy and 48.23 % robustness under
the same level of attacks.
Apart from the three factors above, we also study how other factors affect TD robustness.
Batch Size We train the feature extractor with different batch sizes. We set the batch size to 32, 64,
128, 256, and 512, following the common practice of setting the batch size to a power of 2.
Architectures We investigate how the model’s width and depth affect the TD robustness. For
the ResNet models (He et al., 2016), we study models with different depths including ResNet20,
ResNet32, ResNet44, ResNet56 and ResNet110, and models with different widths (Wide ResNet)
such as 2xResNet32, 5xResNet32, and 10xResNet32. We also experiments on VGG models (Si-
monyan & Zisserman, 2014) including VGG 13-bn, VGG 16-bn, and VGG 19-bn.
Upstream Tasks We study how the choice of upstream task affects the representation learned by
the feature extractor. We use the four tasks based on the CIFAR-100 dataset: C 4/10/20/100.
D Experimental Results
D.1 A Sanity Check
The sanity check results are reported in Section 5.1 in Table 7, including TDsof t for each row. It is
clear that the first three rows have smaller DCCA, DCKA and TDhard values than the other rows,
which indicates that the features learned from Cifar-2/5/10 are more similar, but are very different
from the features learned from SVHN.
Table 7: The sanity check results
Model 1	Model 2	Downstream	DCCA	DCKA	TDsoft	TDhard
Cifar-10	Cifar-5		0.7642	0.3024	0.3344	0.2158
Cifar-10	Cifar-2		0.8323	0.5330	0.6717	0.4958
Cifar-5	Cifar-2	Cifar-10	0.8030	0.4530	0.5057	0.4944
Cifar-10	SVHN		0.9218	0.9713	0.8528	0.8001
Cifar-5	SVHN		0.9793	0.9710	0.7521	0.8049
Cifar-2	SVHN		0.9163	0.9782	0.5000	0.8163
D.2 Does Initialization Affect Learned Features ?
In this section, we further study whether models trained from different random initializations learn
similar representations using a variety of downstream tasks. The results are reported in Table 8.
As shown by the results, TDsof t and TDhard exhibit similar trends and output different values for
different downstream tasks. When we train the model on Cifar-5, both models provide consistent
predictions on Cifar-2, but the variance gets much bigger on Cifar-10. Furthermore, it can be
observed that from C 4 to C 100, as the task becomes more difficult, the difference between the two
representations increases. On C 4, the two models trained on Cifar-5 disagree on 32.77% of the test
samples. When evaluated on C 100, however, the same two models disagree on as much as 80.05% of
the data. The variation of TD as demonstrated in the table implies that a reasonable metric measuring
the difference between two representations should take the downstream tasks into consideration.
19
Under review as a conference paper at ICLR 2021
Table 8: Similarity of the models learned With different initializations.
Model 1	Model 2	Downstream	DCCA	DCKA	TDsoft	TDhard
		Cifar-10			0.1617	0.2139
		Cifar-5	0.6961	0.0835	0.0200	0.0442
		Cifar-2			0.0124	0.0109
Cifar-5	Cifar-5	C 100			0.3113	0.8005
		C 20	0.8003	0.4210	0.2599	0.6377
		C10			0.2347	0.5010
		C4			0.1845	0.3277
		SVHN	0.8663	0.6940	0.1765	0.5312
		Cifar-10			0.1489	0.3745
		Cifar-5	0.6931	0.0402	0.1281	0.2631
		Cifar-2			0.0168	0.0164
Cifar-2	Cifar-2	C 100			0.1762	0.7651
		C 20	0.7533	0.3036	0.1595	0.6018
		C10			0.1485	0.4259
		C4			0.1243	0.2644
		SVHN	0.8330	0.6305	0.1296	0.4157
Now we try to answer whether the features learned by two models trained from different initializations
are similar. We Want to emphasize that the question depends on hoW We evaluate the features, i.e.,
Which doWnstream task set We choose. Among all the tasks, Cifar-2/5/10 are the most correlated. The
models trained on Cifar-5 can achieve 80% accuracy on Cifar-10, indicating that the features can be
successfully transferred. HoWever, even on such a correlated task, the tWo models disagree on 21.39%
of the test samples. Thus, the features captured by models trained from different initializations are
not the same. The TD metric alloWs a practitioner to choose the doWnstream tasks of their interest,
and both TDhard and TDsoft have clear practical meanings: TDhard shoWs the fraction of samples
that tWo models disagree on While TDsoft measures the difference betWeen predictions in terms of
the likelihood indicated by the softmax output.
D.3 TD Robustness and Its Application to Training Strategy Evaluation
Data Augmentation We calculate TD robustness under various configurations of data augmenta-
tion’s techniques, including random flipping (F), random cropping (C), and Gaussian additive noise
(G). Full results are reported in Table 9.
Table 9: The effect of data augmentation: We study random flipping (F), random cropping (C) and
adding Gaussian noise (G). +/- indicates whether the data augmentation method is applied.
Upstream	Downstream	F	C	G	DCCA	DCKA	TDsoft	TDhard
		-	-	-	0.7677	0.0866	0.2199	0.2812
		+	-	-	0.7552	0.0887	0.1692	0.2739
		-	+	-	0.7306	0.0758	0.1614	0.2260
Cifar-5	Cifar-10	+	+	-	0.7285	0.0833	0.1593	0.2150
		-	-	+	0.7783	0.1310	0.1940	0.3107
		+	-	+	0.7668	0.1319	0.1900	0.2790
		-	+	+	0.7603	0.1370	0.2051	0.2773
		+	+	+	0.7500	0.1319	0.1976	0.2542
In the results, TDsoft and TDhard share similar trends. The results shoW that both random flipping and
random cropping have positive effects on TD robustness while the Gaussian additive noise decrease
TD robustness. For TDhard, random cropping has a more significant effect than random flipping.
Applying random cropping can reduce the value of TDhard from 0.2812 to 0.2260, while flipping
only reduces it to 0.2739. For TDsoft, random flipping and random cropping have comparable effects.
20
Under review as a conference paper at ICLR 2021
On the other hand, both TDhard and TDsof t increase when Gaussian noise is added. The results
reveal that TD robustness matches previous understandings that flipping and cropping help learn a
good representation while Gaussian noise may harm the generalization (Shorten & Khoshgoftaar,
2019; Chen et al., 2020).
Learning Rate Schedule Full results are reported in Table 10. All four metrics show that the
models trained with the “Large LR + with decay" schedule capture more similar features than the
models trained with the other two schedules. These observations coincide with the fact that initialing
with a large learning rate and then decaying it in the middle of training helps improve the quality of
the representation. A theoretical analysis of this phenomenon can be found in Li et al. (2019).
Table 10: The effect oflearning rate schedule.
Upstream	Downstream	Schedule	DCCA	DCKA	TDsoft	TDhard
		Small LR + without decay	0.7739	0.1502	0.2118	0.2521
Cifar-5	Cifar-10	Large LR + without decay	0.7417	0.2855	0.2470	0.2770
		Large LR + with decay	0.7264	0.1232	0.1896	0.2313
Adversarial training In Table 11, we report the results for standard training and adversarial
training. When the training strategy changes from standard training to adversarial training, TDhard
decreases from 0.2139 to 0.1823, and TDsof t drops from 0.1617 to 0.0883. It can be concluded
that although adversarial training lowers the classification accuracy of the model, it improves TD
robustness and makes models trained from different initializations learn more similar features. This
conclusion coincides with the widespread belief that adversarial training helps models capture features
that align better with human perception (Tsipras et al., 2019; Ilyas et al., 2019; Engstrom et al., 2019).
Table 11: The difference between training with standard methods and adversarial training.
Upstream	Downstream	Training	DCCA	DCKA	TDsoft	TDhard
Cifar-5	Cifar-10	Standard	0.6961	0.0835	0.1617	0.2139
		Adversarial	0.6339	0.0717	0.0883	0.1823
Batch Size The results on how batch size affects TD robustness are reported in Table 12. All four
metrics verify that batch size 128 is the optimal setting for the default learning rate schedule. In
practice, batch size 128 achieves the highest accuracy, and batch size 256 decreases the average
accuracy by 1%.
Table 12: The effect ofbatch size.
Upstream	Downstream	Batch Size	DCCA	DCKA	TDsoft	TDhard
		^^2	0.7422	0.1484	0.2201	0.2858
		64	0.7244	0.1271	0.1772	0.2436
Cifar-5	Cifar-10	128	0.6961	0.0835	0.1617	0.2139
		256	0.7054	0.0690	0.1704	0.2142
		512	0.7252	0.0744	0.1646	0.2214
Number of Training Epochs We plot TD robustness during training in Figure 3. The two red
vertical lines indicate the epochs when the learning rate is decayed with factor 0.1. The plot discloses
the properties of training that the performance curve does not manifest. For instance, after learning
rate decay, all curves of four metrics drop significantly and then rise up again, indicating that the
models are overfitting the training samples at a lower learning rate.
Architectures We also experiment with different architectures, including ResNet and VGG of
different depths and widths, and report the results in Table 13. For ResNet models, when the depth
21
Under review as a conference paper at ICLR 2021
Epochs
DCCA - DCKA - TDSOft - TDlIard
Figure 3: The changes in difference metrics during training.
increases from 20 to 110, TDhard also decreases steadily, and the disagreement decreases from
22.53% to 19.76%. TDsoft also shares similar trends, but the minimum point occurs for ResNet56
models. DCCA first goes up and then down from ResNet32 to ResNet 44 and to ResNet56. DCKA
indicates that ResNet56 models capture similar features than other models. In practice, it is also
believed that increasing the depth is helpful for the quality of the representation and the model’s
generalization, aligning with the trends of TDhard. In our experiments, when transferred to Cifar-10,
the ResNet110 trained on Cifar-5 can reach 83% average accuracy while the number for ResNet20 is
only 76%. The increased depth helps learn better features for transferring to Cifar-10.
As we use ResNet of different widths, both TDhard and TDsoft decreases when the width increases.
Simply doubling the width from 1xResNet32 to 2xResNet32 results in a significant reduction in
TDhard. Intuitively, when the width increases, the representation’s dimension also increases, and
more features are learned. In this case, two representations are more likely to have similar features.
Although it is widely believed that the depth has a more significant impact on the performance than
the width, our experiments show that the width may be more important for the similarity between
two representations than the depth.
Table 13: The effect of model architectures.
Upstream	Downstream	Architecture	DCCA	DCKA	TDsoft	TDhard
		ReSNet20	0.7414	0.0982	0.1617	0.2253
		ResNet32	0.6961	0.0835	0.1617	0.2139
		ResNet44	0.7023	0.0736	0.1583	0.2021
		ResNet56	0.6997	0.0710	0.1561	0.2015
		ResNet110	0.6887	0.0736	0.1627	0.1976
Cifar-5	Cifar-10	1xResNet32	0.6961	0.0835	0.1617	0.2139
		2xResNet32	0.6917	0.0473	0.1315	0.1580
		5xResNet32	0.7038	0.0349	0.1082	0.1530
		10xResNet32	0.7125	0.0422	0.1285	0.1478
		VGG13	0.7411	0.0415	0.1297	0.3753
		VGG16	0.7767	0.0419	0.1408	0.4567
		VGG19	0.7974	0.0455	0.1179	0.3838
Upstream Tasks Using which upstream task to train the feature extractor is the core question in
representation learning. In our experiment, we study four upstream tasks: C 4/10/20/100. We evaluate
the results on Cifar-10 and Cifar-5 and report the results in Table 14. From C 4 to C 100, the upstream
task becomes more difficult and contains more information. In practice, the models trained on harder
22
Under review as a conference paper at ICLR 2021
Table 14: The effect of different UPstream tasks
UPstream	Downstream	DCCA	DCKA	TDsoft	TDhard
C 100		0.7379	0.4352	0.3028	0.3340
C 20	Cifar-10	0.8221	0.5287	0.3307	0.4362
C10		0.8153	0.4750	0.3242	0.4386
C4		0.8016	0.4002	0.3072	0.4953
C 100		0.7379	0.4352	0.2124	0.2169
C 20	Cifar-5	0.8221	0.5287	0.2511	0.2807
C10		0.8153	0.4750	0.2472	0.2673
C4		0.8016	0.4002	0.2541	0.3179
upstream tasks have higher accuracy on both the Cifar-5 and Cifar-10 downstream tasks. TDsoft,
TDhard, and DCCA all show that the models trained on the C 100 task learn the most similar features
in the table while DCKA outPuts the smallest number for models trained with C 4.
23