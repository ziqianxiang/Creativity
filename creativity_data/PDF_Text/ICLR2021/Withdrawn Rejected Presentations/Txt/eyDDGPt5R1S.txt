Under review as a conference paper at ICLR 2021
Learning Deep Latent Variable Models via
Amortized Langevin Dynamics
Anonymous authors
Paper under double-blind review
Ab stract
How can we perform posterior inference for deep latent variable models in an
efficient and flexible manner? Markov chain Monte Carlo (MCMC) methods,
such as Langevin dynamics, provide sample approximations of such posteriors
with an asymptotic convergence guarantee. However, it is difficult to apply these
methods to large-scale datasets owing to their slow convergence and datapoint-
wise iterations. In this study, we propose amortized Langevin dynamics, wherein
datapoint-wise MCMC iterations are replaced with updates of an inference model
that maps observations into latent variables. The amortization enables scalable in-
ference from large-scale datasets. Developing a latent variable model and an infer-
ence model with neural networks, yields Langevin autoencoders (LAEs), a novel
Langevin-based framework for deep generative models. Moreover, if we define
a latent prior distribution with an unnormalized energy function for more flexible
generative modeling, LAEs are extended to a more general framework, which we
refer to as contrastive Langevin autoencoders (CLAEs). We experimentally show
that LAEs and CLAEs can generate sharp image samples. Moreover, we report
their performance of unsupervised anomaly detection.1
1	Introduction
Latent variable models are widely used for generative modeling (Bishop, 1998; Kingma & Welling,
2013), principal component analysis (Wold et al., 1987), and factor analysis (Harman, 1976). To
learn a latent variable model, it is essential to estimate the latent variables, z, from the observations,
x. Bayesian inference is a probabilistic approach for estimation, wherein the estimate is represented
as a posterior distribution, i.e., p (z | x) = p (z) p (x | z) /p (x). A major challenge while using the
Bayesian approach is that the posterior distribution is typically intractable. Markov chain Monte
Carlo (MCMC) methods such as Langevin dynamics (LD) provide sample approximations for pos-
terior distribution with an asymptotic convergence guarantee. However, MCMC methods converge
slowly. Thus, it is inefficient to perform time-consuming MCMC iterations for each latent variable,
particularly for large-scale datasets. Furthermore, when we obtain new observations that we would
like to perform inference for, we would need to re-run the sampling procedure for them.
In the context of variational inference, a method to amortize the cost of datapoint-wise optimization
known as amortized variational inference (AVI) (Kingma & Welling, 2013; Rezende et al., 2014)
was recently proposed. In this method, the optimization of datapoint-wise parameters of variational
distributions is replaced with the optimization of an inference model that predicts the variational
parameters from observations. This amortization enables posterior inference to be performed effi-
ciently on large-scale datasets. In addition, inference for new observations can be efficiently per-
formed using the optimized inference model. AVI is widely used for the training of deep generative
models, and such models are known as variational autoencoders (VAEs). However, methods based
on variational inference have less approximation power, because distributions with tractable densi-
ties are used for approximations. Although there have been attempts to improve their flexibility (e.g.,
normalizing flows (Rezende & Mohamed, 2015; Kingma et al., 2016; Van Den Berg et al., 2018;
Huang et al., 2018)), such methods typically have constraints in terms of the model architectures
(e.g., invertibility in normalizing flows).
1An implementation is available at: https://bit.ly/2Shmsq3
1
Under review as a conference paper at ICLR 2021
(B1)
(A)
Figure 1: (A) Directed graphical model under consideration. (B1) In traditional Langevin dynamics,
the samples are directly updated in the latent space. (B2) Our amortized Langevin dynamics replace
the update of latent samples with the update of an inference model fz|x that map the observations x
into the latent variables z .
Therefore, we propose an amortization method for LD, amortized Langevin dynamics (ALD). In
ALD, datapoint-wise MCMC iterations are replaced with updates of an inference model that maps
observations into latent variables. This amortization enables simultaneous sampling from posteriors
over massive datasets. In particular, when a minibatch training is used for the inference model, the
computational cost is constant with data size. Moreover, when inference is performed for new test
data, the trained inference model can be used as initialization of MCMC to improve the mixing,
because it is expected that the properly trained inference model can map data into the high-density
area of the posteriors. We experimentally show that the ALD can accurately perform sampling from
posteriors without datapoint-wise iterations. Furthermore, we demonstrate its applicability to the
training of deep generative models. Neural networks are used for both generative and inference
models to yield Langevin autoencoders (LAEs). LAEs can be easily extended for more flexible
generative modeling, in which the latent prior distribution, p (z), is also intractable and defined
with unnormalized energy function, by combining them with contrastive divergence learning (Hin-
ton, 2002; Carreira-Perpinan & Hinton, 2005). We refer to this extension of LAEs as contrastive
Langevin autoencoders (CLAEs). We experimentally show that our LAEs and CLAEs can generate
sharper images than existing explicit generative models, such as VAEs. Moreover, we report their
performance of unsupervised anomaly detection.
2	Preliminaries
2.1	Problem Definition
Consider a probabilistic model with observations x, continuous latent variables z, and model pa-
rameters θ, as described by the probabilistic graphical model shown in Figure 1(A). Although the
posterior distribution over the latent variable is proportional to the product of the prior and like-
lihood: p (z | x) = p (z) p (x | z) /p (x), this is intractable owing to the normalizing constant
p (x) = p (z) p (x | z) dz. This study aims to approximate the posterior p (z | x) for all n ob-
servations x(1), . . . x(n) efficiently by obtaining samples from it.
2.2	Langevin Dynamics
Langevin dynamics (LD) (Neal, 2011) is a sampling algorithm based on the following Langevin
equation:
dz = -VzU (x, Z) dt + P2βTdB,	⑴
where U is a potential function that is Lipschitz continuous and satisfies an appropriate growth
condition, β is an inverse temperature parameter, and B is a Brownian motion. This stochastic dif-
ferential equation has exp (-βU (x, z)) / exp (-βU (x, z0)) dz0 as its equilibrium distribution.
We set β = 1 and define the potential as follows to obtain the target posterior p (z | x) as its equi-
librium:
U(x,z) = -logp(z) -logp(x | z).	(2)
2
Under review as a conference paper at ICLR 2021
Algorithm 1 Amortized Langevin dynamics (training time)
φ J Initialize parameters
Z(1),...,Z(n) J- 0	. Initialize sample sets for all n datapoints
repeat
Φ J Φ0 〜N (Φ'; Φ — ηφ Pn=I vφU Hi), z(i) = fz∣χ Mi)； Φ)) , 2ηφI)
Z(I),..., Znn J Z(I) ∪ {fφ (X⑴)} ,..., Z(N) ∪ {fφ (x(n))}	. Add samples
until convergence of parameters
return Z(1), . . . , Z(n)
Algorithm 2 Amortized Langevin dynamics (test time)
Z J fz∣x (x； φ*)	. Initialize a sample using a trained inference model
Z J 0	. Initialize a sample set
repeat
z J z0 〜N (z0; z 一 ηVzU (x, Z), 2ηI)	. Update the sample using traditional LD
Z J Z ∪ {z}	. Add samples
until convergence of parameters
return Z
We can obtain samples from the posterior by simulating Eq. (1) using the Euler-Maruyama method
(Kloeden & Platen, 2013) as follows:
z J z0 〜N (z0; z 一 NzU (x, Z), 2ηI),	(3)
where η is the step size for the discretization. When the step size is sufficiently small, the samples
asymptotically move to the target posterior by repeating this sampling iteration. LD can be applied
to any posterior inference problems for continuous latent variables provided the potential energy
is differentiable on the latent space. However, to obtain samples of the posterior p (z | x) for all
observations x(1), . . . x(n), we should perform an iteration on Eq. (3) per datapoint as shown in
Figure 1(B1). It is inefficient particularly if the dataset is large. In the next section, we demonstrate
a method that addresses the inefficiency by amortization.
3 Amortized Langevin Dynamics
In traditional LD, we perform MCMC iterations for each latent variable per datapoint. This is
inefficient particularly if managing massive datasets. As an alternative to performing the simulation
of latent dynamics directly, we define an inference model, fz|x , which is a differentiable mapping
from observations into latent variables, and consider the dynamics of its parameter φ as follows:
n
dφ = 一 X VφU (x(i), z(i) = fz∣χ (x(i); φ)) + √2dB.	(4)
i=1
Because function fz|x outputs latent variables, the stochastic dynamics on the parameter space in-
duces other dynamics on the latent space and is represented as the total gradient of fz|x :
dimφ∂ (i)
dz(i) = X ∂⅛-dφk
k=1 ∂φk
= -dXmM (卷U (χ(i),fz∣χ 卜
k=1
dt
dimφ ∂ r(i)	n ∂	/	/	、、\	dim φ ∂ r(i)
一 X ∂φ-	X ∂Lu (Xj),fz∣χ (Xj)；φ)) dt + √2 X ¾rdB.	(5)
k=1 ∂φk
j=1,j6=i ∂φk	, z|x	；
k=1 ∂φk	.
3
Under review as a conference paper at ICLR 2021
	MSE	ESS(1 std.)	MSCE
LD	0.00145	570.24 (25.25)	0.00136
ALD	0.00233	634.27 (40.94)	0.00151
Table 1: Quantitative comparison of the sample quality be-
tween traditional LD and our ALD. The mean squared error
Figure 2:	Groud truth posteriors (left) (MSE) between the true mean and the sample average, the
and their samples by ALD (right) in bi- effective sample size (ESS), and the Monte Carlo standard
vaiate Gaussian examples.	error (MSCE) are provided as evaluation metrics.
Number of MCMC steps
Figure 3:	Evolution of sample values across MCMC iterations for traditional LD and our SGALD
in univariate Gaussian examples. The black lines denote the ground truth posteriors (the solid lines
show the mean values, and the dashed lines show the standard deviation).
The first term of Eq. (5) approximates -▽之⑸ U (X⑴，N⑴)dt in Eq. (1), and the remaining terms
introduce a random walk behavior to the dynamics as in the Brownian term of Eq. (1). For the
simulation of Eq. (4), we use the Euler-Maruyama method, as in traditional LD:
φ — φ0 〜N (φ0; φ - ηφ X VφU (X⑶,Zu) = fz∣χ (X⑴;φ)), 2η°ι) ,	(6)
where ηφ is the step size. Through the iterations, the posterior sampling is implicitly performed
by collecting outputs of the inference model for all datapoints in the training set as described in
Algorithm 1. When we perform inference for new test data, the trained inference model can be used
as initialization of a MCMC method (e.g., traditional LD) as shown in Algorithm 2, because it is
expected that the trained inference model can map data into the high-density area of the posteriors.
For minibatch training, we can substitute the minibatch statistics of m datapoints for the derivative
for all n data in Eq. (6):
nm
X VφU (X⑴,Z⑴=fz|x (X⑴;φ)) ≈ W X VφU (X⑶,Z⑶=fz|x (X⑺;φ)) .	(7)
i=1	mi=1
In this case, we refer to the algorithm as stochastic gradient amortized Langevin dynamics
(SGALD). SGALD enables us to sample from posteriors of a massive dataset with a constant com-
putational cost. By contrast, performing traditional LD requires a linearly increasing cost with data
size. For minibatch training of LD, adaptive preconditioning is known to be effective to improve
convergence, which is referred to as preconditioned stochastic gradient Langevin dynamics (pS-
GLD) (Li et al., 2015). This preconditioning technique is also applicable to our SGALD, and we
employ it throughout our experiments.
Figure 2 shows a simple example of sampling from a posterior distribution, where its prior and
likelihood are defined using conjugate bivariate Gaussian distributions (see Appendix F for more
details). ALD produces samples that match well the shape of the target distributions. The mean
squared error (MSE) between the true mean and the sample average, the effective sample size (ESS),
and the Monte Carlo standard error (MSCE) are provided for quantitative comparison, as shown in
Table 1. It can be observed that the sample quality of ALD is competitive to standard LD, even
though ALD does not perform direct update of samples in the latent space. Figure 3 shows the
evolution of obtained sample values by traditional LD and our SGALD for posteriors defined by a
4
Under review as a conference paper at ICLR 2021
simple univariate conjugate Gaussian (see Appendix F.2 for more experimental details). SGALD’s
samples converges much faster than traditional LD.
The advantage of our ALD over amortized variational in-
ference (AVI) is the flexibility of posterior approxima-
tion. Figure 4 is an example where the likelihood p (x | z)
is defined using a neural network, therefore the poste-
rior p (z | x) is highly multimodal. AVI methods typ-
ically approximate posteriors using variational distribu-
tions, which have tractable density function (e.g., Gaus-
sian distributions). Hence, their approximation power is
limited by the choice of variational distribution family,
and they often fail to approximate such complex posteri-
ors. On the other hand, ALD can capture well such poste-
riors by obtaining samples. The results in other examples
are summarized in Appendix F.
GT	AVI ALD
Figure 4: Visualizations of a ground
truth posterior distribution (left), a vari-
ational distribution by AVI (center) and
sample apprroximation by ALD (right)
in the neural likelihood example.
4	Langevin Autoencoders
Suppose we consider sampling the model parameter θ in addition to the local latent variables z from
the joint posterior p (z, θ | x); then, we can ingenuously extend ALD to a full Bayesian approach
by combining it with standard Langevin dynamics. Herein, the prior of the model parameter p (θ)
is added to the potential U , and θ is sampled using standard LD or its minibatch version, stochastic
gradient Langevin dynamics (SGLD) (Welling & Teh, 2011) as follows:
n
U(X,Z,θ) = -logp(θ) -Xlogp z(i) | θ +logp x(i) | z(i), θ ,	(8)
i=1
θ J θ0 ~N (θ0; θ - ηθ Vθ U (x, z = fz∣χ (x； φ)), 23 I),	(9)
where ηθ is a step size. If we omit the Gaussian noise injection in Eq. (9), it corresponds to gradient
descent for maximum a posteriori (MAP) estimation of θ; if we additionally use a flat prior forp (θ),
it yields the maximum likelihood estimation (MLE). In this study, we assume a flat prior for p (θ)
and omit the notation for simplicity.
Typically, the latent prior p (z | θ) and the likelihood p (x | z, θ) are defined as diagonal Gaussians:
P (z | θ) = N (z； μz, diag(σZ)) , P (x | z, θ) = N (x; μ* = fχ∣z (z; θ), σXI) ,	(10)
where μz, μχ and σZ,σX are mean and variance parameters of Gaussian distributions respectively.
fx|z (z ; θ) is a mapping from the latent space to the observation space. The parameters of the
latent prior μz and σZ can be included to θ as learnable model parameters, or be fixed to manually
decided values (e.g., μz = 0, σ2 = 1). For the observation variance σX, many existing works treat
it as a hyperparameter. However, its tuning is difficult, and often requires heuristic techniques for
proper training (Fu et al., 2019). Instead, here, we apply a different approach, in which the variance
parameter is marginalized out, and the likelihood can be calculated only with the mean parameter μχ
(see Appendix B for further details). Furthermore, when the original data are quantized into discrete
representation (e.g., 8-bit RGB images), it is not desirable to use continuous distributions, such as
Gaussians, as likelihood functions. Thus, we should map quantized data into the continuous space in
advance. This process is often referred to as dequantization (Salimans et al., 2017; Ho et al., 2019).
Our ALD is also applicable as a dequantization method by formulating it as the posterior inference
problem. Further detailed explanations are provided in Appendix C.
We can choose arbitrary differentiable functions for the generative model fx|z and the inference
model fz|x. If neural networks are chosen for both, we achieve Langevin autoencoder (LAE), a new
deep generative model within the auto-encoding scheme. The algorithm of LAEs is summarized in
Algorithm 3 in the appendix.
5	Contrastive Langevin Autoencoders
Currently, we have dealt with the case where the latent prior distribution P (z | θ) is tractable. To en-
able more flexible modeling, here, we consider that an energy-based model (EBM) (Du & Mordatch,
5
Under review as a conference paper at ICLR 2021
2019; Pang et al., 2020; Han et al., 2020) is used for the latent prior as follows.
p (z | θ)
exp(-fz (Z θ))
-Z(θ)-
(11)
where fz (z; θ)is an energy function that maps the latent variable into a scalar value, and Z is
a normalizing constant, i.e., Z(θ) = exp (-fz (z; θ))dz. In this case, the derivative of the
potential energy VθU (X, Z, θ) is intractable owing to the normalizing constant. However, We can
obtain the unbiased estimator of the derivative by obtaining samples from the prior p (z | θ).
n
VθU (X, Z,	θ) = X	Vθfz	(Z⑺;θ)	+	Vθ logZ	(θ)	- Vθ logp (x⑺ |	Z⑴,θ)	(12)
i=1
n
=X Vθfz (Z⑺;θ) - Ep(z∣θ) [Vθfz (z; θ)] - Vθ logP (X⑴ | Z⑴,θ) . (13)
i=1
See Appendix D for the derivation. This algorithm used for the training of EBM is known as con-
trastive divergence learning (Hinton, 2002; Carreira-Perpinan & Hinton, 2005). To obtain samples
from the latent prior, we can use standard LD as follows:
Z — Z0 ~N (z0; Z — ηz Vzfz (z; θ), 2ηzI),	(14)
where ηz is a step size. However, we found that our amortized Langevin algorithm works well even
for the case of sampling from an unconditional prior distribution. In the unconditional case, we
prepare a sampler function fz|u (u; ψ) that maps its input u into the latent variable Z. Here, the
input vector u is fixed, because the prior distribution does not have conditional variables as in the
posterior inference case except the model parameters θ. To run multiple MCMC chains in parallel,
we prepare k fixed inputs u(1), . . . , u(k), and update the function fz|u as follows.
ψ ― ψ0 ~N (ψ - ηψ X Vψfz (Z⑴=fz∣u (u(i)；ψ) ； θ), 2ηψI),	(15)
where ηψ is a step size. Typically, the fixed input vector is
chosen from samples of a standard Gaussian distribution
(i.e., u(1), . . . ,u(k)〜N (u; 0, I)). Figure 5 shows an
example of sampling from a mixture of eight Gaussians
using ALD. We can observe that ALD properly captures
the multimodality of the true density and works well also
in the unconditional case. For minibatch training, we can
substitute the gradient for all k chains with the stochastic
gradient of m minibatch chains:
Figure 5: A mixture of eight Gaussians
(left) and its samples by ALD (right).
k
X Vψ fz
i=1
m
(z(i) = fz|u (u(i); ψ) ； θ) ≈ —X Vψfz (Z⑴=fz|u (U⑴；ψ) ； θ).
m i=1
(16)
The advantage of using amortization in the unconditional case is that we can run massive chains
in parallel with a constant computational cost using minibatch training. Here, we assume that the
number of chains is equal to the number of datapoints for simplicity, i.e., k = n.
In summary, the encoder fz|x, the decoder fx|z, and the latent energy function fz are trained by
minimizing the following loss function L, whereas the latent sampler fz|u are trained by maximizing
it, while stochastic noise of Brownian motion is injected in their update to avoid shrinking to MAP
estimates (or MLE).
n
L(θ, Φ, ψ) = X fz (fz|x (x(i)； Φ) ； θ) - fz (fz|u (U⑴;ψ ； θ)
i=1
-logp (x(i) | Z⑺=fz|x (X⑺；φ , θ).
(17)
Furthermore, when the energy function fz and the sampler fz|u are parameterized using neural
networks, we refer to the whole model as contrastive Langevin autoencoders (CLAEs). At the
6
Under review as a conference paper at ICLR 2021
convergence of the CLAE’s training, the inference model fz|x and the model parameter θ match to
the true posterior p (z | x, θ) and p (θ | X), respectively. Typically, when the number of datapoints
gets infinity (i.e., n → ∞), the generative model p (x | θ) = p (z | θ) p (x | z, θ) dz converges
to the data distribution pdata (x). Moreover, when the sampler function’s outputs corresponds to the
marginal latent distribution Epdata(x) [p (z | x, θ)], the first and second terms on the right hand side
in Eq. (17) are canceled out; therefore the energy function fz and the sampler function fz|u also
converge to equilibrium.
6 Related works
Amortized inference is well-investigated in the context of variational inference, and it is often
referred to as amortized variational inference (AVI) (Rezende & Mohamed, 2015; Shu et al., 2018).
The basic idea of AVI is to replace the optimization of the datapoint-wise variational parameters
with the optimization of shared parameters across all datapoints by introducing an inference model
that predicts latent variables from observations. Currently, the AVI is commonly used in fields,
such as the training of generative models (Kingma & Welling, 2013), semi-supervised learning
(Kingma et al., 2014), anomaly detection (An & Cho, 2015), machine translation (Zhang et al.,
2016), and neural rendering (Eslami et al., 2018; Kumar et al., 2018). However, in the MCMC
literature, there are few works on such amortization. (Han et al., 2016) uses traditional LD to
obtain samples from posteriors for the training of deep latent variable models. Such Langevin-
based algorithms for deep latent variable models are known as alternating back-propagation (ABP)
and are widely applied in several fields (Xie et al., 2019; Zhang et al., 2020; Xing et al., 2018; Zhu
et al., 2019). However, ABP requires datapoint-wise Langevin iterations, causing slow convergence.
Moreover, when we perform inference for new data in test time, ABP requires to re-run MCMC
iterations from randomly initialized samples. Although (Li et al., 2017; Hoffman, 2017) propose
amortization methods for MCMC, they only amortize the cost of initialization in MCMC by using
an inference model. Therefore, they do not completely remove datapoint-wise MCMC iterations.
Autoencoders (AEs) (Hinton & Salakhutdinov, 2006) are a special case of LAEs, wherein the Gaus-
sian noise injection to the update of the inference model (encoder) is omitted in Eq. (6), and a flat
prior is used forp (z | θ). When a different distribution is used as a latent prior, it is known as sparse
autoencoders (SAEs) (Ng et al.). In these cases, the latent dynamics in Eq. (5) are dominated by gra-
dient VφU; thereafter, the latent variables converge to MLE or MAP estimates, arg maxz P (Z | x),
or other stationary points. That is, AEs (and SAEs) can be regarded as a MLE (and MAP) algorithms
for both the parameter θ and the latent variables z. Conversely, LAEs can be considered as a special
case of (S)AEs, in which whole models are trained with SGLD instead of stochastic optimization
methods like stochastic gradient decent (SGD).
Variational Autoencoders (VAEs) are based on AVI, wherein an inference model (encoder)
is defined as a variational distribution q (z | x; φ) using a neural network.
is optimized by maximizing the evidence lower bound (ELBO)Eq(z|x；@)lo
Its parameter φ
exP(-U (X,Z))	=
q(Z|叫的 j ―
-Eq(z|x；e)[U (x, z)]-H (q). There is a contrast between VAEs and LAEs relative to when stochas-
tic noise is used. In VAEs, noise is used to sample from the variational distribution in the calculation
of potential U, i.e., in forward calculation. However, in LAEs, noise is used for calculating gradient
VφU, i.e., in backward calculation. This contrast characterizes their two different approaches to ap-
proximate posteriors: the optimization-based approach of VAEs and the sampling-based approach of
LAEs. The advantage of LAEs over VAEs is that LAEs can flexibly approximate complex posteriors
by obtaining samples, whereas VAEs’ approximation ability is limited by the choice of variational
distribution q (z | x; φ) because it requires a tractable density. Although there are several consider-
ations in the improvement of the approximation flexibility, these methods typically have constraints
in terms of model architectures (e.g., invertibility and ease of Jacobian calculation in normalizing
flows (Rezende & Mohamed, 2015; Kingma et al., 2016; Van Den Berg et al., 2018; Huang et al.,
2018; Titsias & Ruiz, 2019)), or they incur more computational costs (e.g., MCMC sampling for the
reverse conditional distribution in unbiased implicit variational inference (Titsias & Ruiz, 2019)).
Energy-based Models’ training is challenging, and many researchers have been studying methodol-
ogy for its stable and practical training. A major challenge is that it requires MCMC sampling from
EBMs, which is difficult to perform in high dimensional data space. Our CLAEs avoid this difficulty
by defining the energy function in latent space rather than data space. A similar approach is taken by
7
Under review as a conference paper at ICLR 2021
Table 2: Quantitative results of the image generation for BMNIST, MNIST, SVHN, CIFAR-10, and
CelebA. We report the reconstruction error (RE) for test sets and the Frechet Inception Distance
(FID) as evaluation metrics. The RE is calculated as the binary cross entropy for BMNIST and the
mean squared error for the others between the original image and the reconstructed image. Our LAE
and CLAE are compared with three baseline methods, VAE, ABP and DLGM.
	BMNIST	MNIST	SVHN		CIFAR-10		CelebA	
	RE	RE	RE	FID	RE	FID	RE	FID
VAE	108.3	0.01653	0.01019	167.6	0.02587	249.0	0.02129	123.6
ABP	185.9	0.01884	0.02144	292.6	0.03078	304.6	0.02416	172.4
DLGM	106.9	0.01634	0.01025	136.3	0.02563	262.8	0.02124	108.2
LAE	108.4	0.01860	0.009903	134.1	0.02654	241.6	0.02303	111.1
CLAE	40.54	0.00294	0.00352	91.11	0.01408	223.9	0.00919	86.4
(Pang et al., 2020), but they do not use amortization for the sampling of the latent prior and posterior
as in CLAEs. On the other hand, (Han et al., 2020) proposes to learn VAEs and EBMs in latent
space, but their energy function is defined for the joint distribution of the observation and the latent
variable rather than the latent prior. For a more direct approach, in which EBMs are directly defined
in observation space, (Du & Mordatch, 2019) uses spectral normalization (Miyato et al., 2018) for
the energy function to smoothen its density, and stabilize its training. (Nijkamp et al., 2019) shows
short-run MCMC is effective for the training of EBMs.
Generative adversarial networks (GANs) (Goodfellow et al., 2014) can be regarded as a special
case of CLAEs by interpreting their discriminator and generator as energy function and sampler
function, respectively (see Appendix E for further details).
7 Image Generation
To demonstrate the applicability of our framework to the generative model training, we perform
an experiment on image generation tasks using binarized MNIST (BMNIST), MNIST, SVHN, CI-
FAR10, and CelebA datasets. As baselines, we use VAEs and the ABP (Han et al., 2016), which is
an algorithm to train deep latent variable models using LD without amortization. We also provide
the performance of deep latent Gaussian models (DLGMs) (Hoffman, 2017), in which VAE-like en-
coders are used to initialize MCMC for posterior inference, as an alternative approach of amortiza-
tion. For quantitative evaluation, we report the reconstruction error (RE) as an alternative of marginal
likelihood p (x | θ), which cannot be calculated for LAEs and CLAEs. Because the RE cannot be
a measure of sample quality, We provide the FreChet Inception Distance (FID) (Heusel et al., 2017)
for SVHN, CIFAR-10 and CelebA. The results are summarized in Table 2. We also provide the per-
formance on denoising by trained VAEs, LAEs and CLAEs in Table 5 in the appendix. CLAEs
consistently outperform the others, and LAEs also provide competitive results to the baselines.
In addition, LAEs’ training is faster than ABP due to
amortization as shown in Figure 6. ABP cannot update
the inference for datapoints that are not included in a
minibatch, whereas LAEs can through the update of their
inference model (encoder). This amortization enables
scalable inference for large scale datasets, and accelerates
the training of generative models. Qualitatively, images
generated by LAEs and CLAEs are sharper than those of
VAEs and ABPs as shown in Figure 7. Other examples
are summarized in the appendix.
Figure 6: Learning curves of ABP and
LAE on SVHN. The error bars denote
8 Anomaly detection
the standard deviations with three seeds.
In addition to image generation, the potential energy in Eq. (2) can be useful for performing un-
supervised anomaly detection, because it can be a measure of the probability density. For CLAEs,
the potential energy itself cannot be calculated, because it includes the logarithm of the normalizing
8
Under review as a conference paper at ICLR 2021
Figure 7: Generated samples of MNIST and CelebA by trained five models. The images are gener-
ated by the decoder fx|z (z), and the latent variable z is sampled from the sampler function fz|u for
CLAEs, and the Gaussian prior for the others.
Table 3: Evaluation of the area under the precision-recall curve (AUPRC) on anomaly detection task
for MNIST. Our LAEs and CLAEs are compared with autoencoders (AEs) and VAEs. The top row
shows which digit class is dealt as normal examples.
	0	1	2	3	4	5	6	7	8	9
AE	0.974	0.996	0.762	0.845	0.863	0.706	0.837	0.952	0.765	0.872
VAE	0.971	0.995	0.768	0.837	0.844	0.699	0.845	0.927	0.754	0.874
LAE	0.915	0.997	0.749	0.875	0.853	0.844	0.914	0.936	0.779	0.913
CLAE	0.976	0.862	0.788	0.752	0.915	0.777	0.899	0.790	0.798	0.865
constant of their latent prior log Z (θ). However, it can be ignored because it is constant with values
of the observation x and the latent z. Therefore, we use the pseudo potential energy, U, as a measure
as follows.
U (x, Z) = - logP (X | z; θ) - fz (z; θ).	(18)
We test the efficacy of our LAEs and CLAEs for anomaly detection using MNIST. We assume that
each digit class is normal and treat the remaining nine digits as anomaly examples. We use AEs and
VAEs as baselines, and provide the area under the precision-recall curve (AUPRC) as the metric for
comparing the models. We use the RE as a measure of anomaly for AEs and the negative ELBO for
VAEs. From Table 3, it can be observed that our LAEs and CLAEs outperforms AEs and VAEs.
9 Conclusion
We proposed amortized Langevin dynamics (ALD), which is an efficient MCMC method for latent
variable models. The ALD amortizes the cost of datapoint-wise iteration by using inference models.
By experiments, we demonstrated that the ALD can accurately approximate posteriors. Using ALD,
we derived a novel scheme of deep generative models called Langevin autoencoders (LAEs). LAEs
are extended to a more general setting, where the latent prior is defined with an unnormalized energy
function, and we refer to it as contrastive Langevin autoencoders (CLAEs). We demonstrated that
our LAEs and CLAEs can generate sharp images, and they can be used for unsupervised anomaly de-
tection. Furthermore, we investigated the relationship between our framework and existing models,
and showed that traditional autoencoders (AEs) and generative adversarial networks (GANs) can be
regarded as special cases of our LAEs and CLAEs. For future research on ALD, theories providing
a solid proof of convergence, deriving a Metropolis-Hastings rejection step, and deriving algorithms
based on more sophisticated Hamiltonian Monte Carlo approaches should be investigated.
9
Under review as a conference paper at ICLR 2021
References
Jinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using reconstruc-
tion probability. Special Lecture on IE, 2(1), 2015.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Christopher M Bishop. Latent variable models. In Learning in graphical models, pp. 371-403.
Springer, 1998.
Miguel A Carreira-Perpinan and Geoffrey E Hinton. On contrastive divergence learning. In Aistats,
volume 10, pp. 33-40. Citeseer, 2005.
Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. arXiv
preprint arXiv:1903.08689, 2019.
SM Ali Eslami, Danilo Jimenez Rezende, Frederic Besse, Fabio Viola, Ari S Morcos, Marta Gar-
nelo, Avraham Ruderman, Andrei A Rusu, Ivo Danihelka, Karol Gregor, et al. Neural scene
representation and rendering. Science, 360(6394):1204-1210, 2018.
Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence Carin.
Cyclical annealing schedule: A simple approach to mitigating kl vanishing. arXiv preprint
arXiv:1903.10145, 2019.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-
mation processing systems, pp. 2672-2680, 2014.
Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. Alternating back-propagation for generator
network. arXiv preprint arXiv:1606.08571, 2016.
Tian Han, Erik Nijkamp, Linqi Zhou, Bo Pang, Song-Chun Zhu, and Ying Nian Wu. Joint training
of variational auto-encoder and latent energy-based model. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 7978-7987, 2020.
Harry H Harman. Modern factor analysis. University of Chicago press, 1976.
Hao He, Hao Wang, Guang-He Lee, and Yonglong Tian. Probgan: Towards probabilistic gan with
theoretical guarantees. In ICLR (Poster), 2019.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances
in neural information processing systems, pp. 6626-6637, 2017.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural
computation, 14(8):1771-1800, 2002.
Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural
networks. science, 313(5786):504-507, 2006.
Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flow-
based generative models with variational dequantization and architecture design. arXiv preprint
arXiv:1902.00275, 2019.
Matthew D Hoffman. Learning deep latent gaussian models with markov chain monte carlo. In
International conference on machine learning, pp. 1510-1519, 2017.
Chin-Wei Huang, David Krueger, Alexandre Lacoste, and Aaron Courville. Neural autoregressive
flows. arXiv preprint arXiv:1804.00779, 2018.
Rie Johnson and Tong Zhang. Composite functional gradient learning of generative adversarial
models. arXiv preprint arXiv:1801.06309, 2018.
10
Under review as a conference paper at ICLR 2021
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In Advances in neural information processing systems, pp.
3581-3589, 2014.
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Im-
proved variational inference with inverse autoregressive flow. In Advances in neural information
processing systems, pp. 4743-4751, 2016.
Peter E Kloeden and Eckhard Platen. Numerical solution of stochastic differential equations, vol-
ume 23. Springer Science & Business Media, 2013.
Ananya Kumar, SM Eslami, Danilo J Rezende, Marta Garnelo, Fabio Viola, Edward Lockhart,
and Murray Shanahan. Consistent generative query networks. arXiv preprint arXiv:1807.02033,
2018.
Chunyuan Li, Changyou Chen, David Carlson, and Lawrence Carin. Preconditioned stochastic
gradient langevin dynamics for deep neural networks. arXiv preprint arXiv:1512.07666, 2015.
Yingzhen Li, Richard E Turner, and Qiang Liu. Approximate inference with amortised mcmc. arXiv
preprint arXiv:1702.08343, 2017.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. arXiv preprint arXiv:1802.05957, 2018.
Radford M Neal. Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo,
pp. 113, 2011.
Andrew Ng et al. Sparse autoencoder.
Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-
persistent short-run mcmc toward energy-based model. In Advances in Neural Information Pro-
cessing Systems, pp. 5232-5242, 2019.
Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space
energy-based prior model. arXiv preprint arXiv:2006.08205, 2020.
Danilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing flows. arXiv
preprint arXiv:1505.05770, 2015.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
Yunus Saatci and Andrew G Wilson. Bayesian gan. In Advances in neural information processing
systems, pp. 3622-3631, 2017.
Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the
pixelcnn with discretized logistic mixture likelihood and other modifications. arXiv preprint
arXiv:1701.05517, 2017.
Rui Shu, Hung H Bui, Shengjia Zhao, Mykel J Kochenderfer, and Stefano Ermon. Amortized
inference regularization. In Advances in Neural Information Processing Systems, pp. 4393-4402,
2018.
Michalis K Titsias and Francisco Ruiz. Unbiased implicit variational inference. In The 22nd Inter-
national Conference on Artificial Intelligence and Statistics, pp. 167-176, 2019.
Benigno Uria, Iain Murray, and Hugo Larochelle. Rnade: The real-valued neural autoregressive
density-estimator. In Advances in Neural Information Processing Systems, pp. 2175-2183, 2013.
Rianne Van Den Berg, Leonard Hasenclever, Jakub M Tomczak, and Max Welling. Sylvester nor-
malizing flows for variational inference. In 34th Conference on Uncertainty in Artificial In-
telligence 2018, UAI 2018, pp. 393-402. Association For Uncertainty in Artificial Intelligence
(AUAI), 2018.
11
Under review as a conference paper at ICLR 2021
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
Proceedings of the 28th international conference on machine learning (ICML-11), pp. 681-688,
2011.
Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and
intelligent laboratory systems, 2(1-3):37-52, 1987.
Jianwen Xie, Ruiqi Gao, Zilong Zheng, Song-Chun Zhu, and Ying Nian Wu. Learning dynamic
generator model by alternating back-propagation through time. In Proceedings of the AAAI Con-
ference on Artificial Intelligence, volume 33, pp. 5498-5507, 2019.
Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, and Ying Nian Wu. Deformable gen-
erator network: Unsupervised disentanglement of appearance and geometry. arXiv preprint
arXiv:1806.06298, 2018.
Biao Zhang, Deyi Xiong, Jinsong Su, Hong Duan, and Min Zhang. Variational neural machine
translation. arXiv preprint arXiv:1605.07869, 2016.
Jing Zhang, Jianwen Xie, and Nick Barnes. Learning noise-aware encoder-decoder from noisy
labels by alternating back-propagation for saliency detection. arXiv preprint arXiv:2007.12211,
2020.
Yizhe Zhu, Jianwen Xie, Bingchen Liu, and Ahmed Elgammal. Learning feature-to-feature transla-
tor by alternating back-propagation for generative zero-shot learning. In Proceedings of the IEEE
International Conference on Computer Vision, pp. 9844-9854, 2019.
A Notation
	Numbers and Arrays
a	A scalar (integer or real)
a I diag(a) a	A vector Identity matrix with dimensionality implied by context A square, diagonal matrix with diagonal entries given by a A scalar random variable
a	A vector-valued random variable Sets and Graphs
A R {0, 1} {0,1,.. .,n} [a, b] (a, b]	A set The set of real numbers The set containing 0 and 1 The set of all integers between 0 and n The real interval including a and b The real interval excluding a but including b Indexing
ai or a[i] ai or a[i]	Element i of vector a, with indexing starting at 1 Element i of the random vector a Calculus
12
Under review as a conference paper at ICLR 2021
dy dx	Derivative of y with respect to x
∂y ∂x ▽xy f (x)dx	Partial derivative of y with respect to x Gradient of y with respect to x Definite integral over the entire domain of x Probability and Information Theory
P(a) p(a)	A probability distribution over a discrete variable A probability distribution over a continuous variable, or over a variable whose type has not been specified
a ~ P Eχ~p[f (x)] or Ef(X) H(x) orH(P)	Random variable a has distribution P Expectation of f(x) with respect to P(x) Shannon entropy of the random variable x that has distri- bution P
DKL(PkQ) N (x; μ, ∑)	Kullback-Leibler divergence of P and Q Gaussian distribution over X with mean μ and covariance Σ
U(x; a, b)	Uniform distribution over x with lower range a and upper range b
Gam(x; α, β)	Gamma distribution over x with shape α and rate range β Functions
f : A → B f(x; θ)	The function f with domain A and range B A function of x parametrized by θ. (Sometimes we write f(x) and omit the argument θ to lighten notation)
log x σ(x)	Natural logarithm of x Logistic sigmoid,  	ʒ——- 1 + exp(-x) ∞
Γ(z)	Gamma function,	tz-1e-tdt 0
bxc 1condition	Integer part of x, i.e., bxc = max{n ∈ N | n ≤ x} is 1 if the condition is true, 0 otherwise
B Marginalizing Out Observation Variance
When we use a diagonal Gaussian distribution for the likelihood function (i.e., p (x | z, θ) =
N (x; μχ = fχ∣z (z; θ), σ21)), We have to decide the parameter of observation variance σX. A
simple and popular way is to manually choose the parameter in advance (e.g., σx2 = 1). However,
it is difficult to choose a proper value, and it is desirable that the variance is calibrated for each
datapoint. To address it, We use an alternative approach, in Which the variance is marginalized out
and the likelihood can be calculated only With the mean parameter. First, We define the precision pa-
rameter, which is the reciprocal of variance, i.e., λχ = 1∕σX. The precision is defined per datapoint,
and shared across all dimension of the observation. When We define the prior distribution of the pre-
cision using an uninformative flat prior (e.g., p (λx) = Gam (λx; 0, 0)), the marginal distribution
13
Under review as a conference paper at ICLR 2021
Algorithm 3 Langevin Autoencoders
θ, φ J Initialize parameters
repeat
θ J θ0 〜N (θ0; θ - ηθ Vθ U (X, z = fz∣χ (X ； φ), θ), 2磔1)
φ J φ0 〜N (φ0; φ - ηφVφU (X, Z = fz|x (X; φ), θ) , 2ηφI)
until convergence of parameters
return θ, φ
Algorithm 4 Contrastive Langevin Autoencoders
θ, φ, ψ J Initialize parameters
repeat
θ J θ0 〜N (θ0; θ - ηθVθL (θ, φ, ψ), 2ηθI)
φ J φ0 〜N (φ0; φ 一 ηφVφL (θ, φ, ψ), 2ηφI)
ψ J ψ0 〜N (ψ0; ψ + ηψ VψL (θ, φ, ψ), 2ηψI)
until convergence of parameters
return θ, φ, ψ
will be simply the integral of the Gaussian likelihood over λx :
/N (x; μχ
fx|z (Z； θ) ,λχ1I) dλχ
(19)
(20)
(21)
(22)
where d is the dimensionality of x and Γ is a gamma function. This marginalized distribution is an
improper distribution, whose integral over x diverges and does not correspond to 1. We refer to the
distribution as the marginalized Gaussian distribution. Marginalized Gaussians can be widely used
for mean parameter estimation of a Gaussian distribution, especially when its variance is unknown.
C	Langevin Dequantization
Many image datasets, such as MNIST and CIFAR10, are recordings of continuous signals quantized
into discrete representations. For example, standard 8-bit RGB images of the resolution of H × W
are represented as {0, 1, . . . , 255}3×H×W. If we naively train a continuous density model to such
discrete data, the model will converge to a degenerate solution that places all probability mass on
discrete datapoints (Uria et al., 2013). A common solution to this problem is to first convert the
discrete data distribution into a continuous distribution via a process called dequantization, and then
model the resulting continuous distribution using the continuous density model. Here, we denote the
random variable of original discretized data as x ∈ Nd (d = 3 × H × W), dequantized continuous
variable as X ∈ Rd, and the continuous density model as P (X). The simplest way of dequantization
is to add uniform noise to the discrete data, and deal with it as a continuous distribution, which we
refer to as uniform dequantization:
X 〜U (x; x, X + 1).	(23)
However, this approach introduces flat step-wise regions into the data distribution, and it is unnatural
and difficult to fit parametric continuous distributions. Moreover, the range of the dequantized data
14
Under review as a conference paper at ICLR 2021
is still bounded (i.e., X ∈ (0,256)d), therefore it is not desirable to fit the continuous density model
defined for unbounded range, e.g., Gaussian distributions.
To investigate a more sophisticated approach, we first consider the quantization process, the inverse
process of dequantization, in which the continuous data x ∈ Rd is discretized into {0, 1, . . . , 255}d.
This process is represented as a conditional distribution P (X | X). For example, it is defined as
follows:
P(X | X) = 1χ=[256∙σ(X)C .	(24)
In this definition, the continuous data is first compressed into (0, 256)d using the logistic sigmoid,
then discretized into {0, 1, . . . , 255}d with its integer part. Although the quantization process could
be formulated with another definition, we here discuss based on this formulation. When we have a
density model of continuous data P (X), the dequantization process can be formulated as a posterior
inference problem of P (X | x) 8 P (X) P (x | X). Although this posterior is typically intractable,
we can obtain samples from it using our ALD algorithm in the same way as the posterior sampling
of latent variable models. When We construct the inference model fχχ : {0,1,..., 255}d → Rd as
follows, the likelihood will be constant, i.e., P (X | X = fχ∣χ (x; ξ)) = 1.
fx∣x (X； ξ)=σ-1(X + σ2gΓ ξ))),	(25)
where g (X; ξ) is a mapping from discretized data X into real d-space. Therefore, the potential
energy corresponds to the negative log likelihood.
U(X, X = fx∣x (x; ξ)) = - log P (X = fx∣x (x; ξ))-里PJHx=f藕TxT布∙	(26)
The parameter of inference model fχ∣χ is updated using our AEDalgorithm as in Eq. (6).
ξ 一 ξ0 〜N (ξ0; ξ - ηξ XX VξU (X⑺=fχ∣χ (X⑴;ξ)), 2ηξι) ,	(27)
where ηξ is a step size.
When we use this Langevin dequantization for latent variable models like LAEs, the potential energy
is rewritten as follows, and all parameters {θ, φ, ξ} are trained in an end-to-end fashion:
U(X, X = fx∣x (X； ξ) , Z = fz|x (X； φ) , θ)
n
=-log P (θ) - X log P(Z ⑴=fz|x(X(i)； φ))
i=1	(28)
+ logP(X⑴=fx|x (X⑴;ξ) | Zei) = fz|x (X⑴;φ)).
Figure 8 shows the comparison of SVHN data distributions of the top left corner pixel before and
after dequantization. Before dequantization, the data are concentrated on discrete points. After
dequantization, it can be observed that the distribution gets continuous enough to fit continuous
density models. Furthermore, Figure 9 shows the comparison of generated MNIST samples by
LAEs to see the effect of Langevin dequantization. The dequantization seems to affect the sharpness
of generated samples.
D Derivation of Eq. (13)
Vθ log Z (z) = -ɪ Vθ Z (θ)	(29)
Z (θ)
=V Vθ exp(-fz (z; θ)) dz	(30)
Z (θ)
_	/ exp(-fz (z; θ))v7 , /	f,n
=-J	Z (θ) vθfz (z; θ) dz	(31)
=-/P (Z | θ) Vθfz (z; θ) dz	(32)
=-Ez〜p(z∣θ) [Vθfz (z; θ)]	(33)
15
Under review as a conference paper at ICLR 2021
Figure 8: Histograms of data before and after Langevin dequantization. Dequantized data is passed
through a sigmoid function to keep the support range same.
夕 / 02 / 9q3
w/o dequantization
w/ dequantization
Figure 9: Generated samples of LAEs with or without dequantization.
E	Additional Related Work
Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) are similar to CLAEs in
that both are trained by minimax game between two functions (i.e., the energy function and the
sampler function in CLAEs; the discriminator and the generator in GANs). However, there are some
differences between them. First, the minimax game is performed in the latent space in CLAEs, while
it is performed in the observation space in GANs. In other words, the latent variable is identical to
the observation (i.e., p (x | z) = 1x=z) in GANs2. Second, the loss function is slightly different. In
GANs, the loss function is as follows:
n
LGAN (θ, Ψ) = — X log D (X⑴;θ) + log (1 - D(G (U⑴;ψ ； θ)) ,	(34)
i=1
where G denotes the generator that maps its inputs u into the observation space, and D denotes the
discriminator that maps from the observation space into (0,1), and u(i)〜N (u; 0, I). Thediscrim-
inator is trained to minimize this loss function, whereas the generator is trained to maximize it. The
main difference to Eq. (17) is the second term. When we substitute it with - log D G u(i); ψ ; θ ,
it becomes more similar to Eq. (17). This modification is known as the - log D trick, and often used
to stabilize the training of GAN’s generator (Goodfellow et al., 2014; Johnson & Zhang, 2018). In
this formulation, the counter parts of the energy function and the sampler function are - log D (∙; θ)
and G (∙; ψ), respectively. However, there is still a difference that the range of - log D (∙; θ) is
bounded within (0, ∞), whereas the range of CLAE’s energy function is unbounded.
Another difference between CLAEs and GANs is that the input vector of the sampler function is
fixed through the training in CLAEs, whereas the input of the generator changes per iteration by
sampling from N (u; 0, I) in GANs. Furthermore, CLAEs are trained using noise injected gradient,
whereas GANs are trained with a standard stochastic optimization method like SGD. In the training
of CLAEs, as the number of inputs of the sampler function increases, the noise magnitude in Eq.
(15) will relatively decreases. Therefore, in the infinite case (i.e., k → ∞), Eq. (15) corresponds
to standard (stochastic) gradient descent. Thus, GANs can be interpreted as the infinite case of
CLAEs with regard to their training of generators. In the infinite case, the generator (sampler) may
converge to a solution where it always generates maximum density points rather than samples from
the distribution defined by the energy function. This nature can cause mode collapsing, which is
known as a major challenge of GAN’s training. In GANs, the discriminator is also trained with
2Note that the latent variable z is different from the input of GAN’s generators. Here, the input of the
GAN’s generators is denoted as u for the analogy with CLAEs.
16
Under review as a conference paper at ICLR 2021
standard stochastic optimization, which corresponds to the MLE case where noise injection in Eq.
(9) is omitted. Although there are some investigations to apply Bayesian approach to GANs (Saatci
& Wilson, 2017; He et al., 2019), their discriminators are not defined as energy functions.
In summary, GANs with - logD trick can be considered as a special case of CLAEs, where the
latent variable is identical to the observation (i.e., p (x | z) = 1x=z); the energy function and the
sampler function are respectively defined as - log D (∙; θ) and G (∙; ψ); the number of inputs of the
sampler function tends to infinity; and the model parameter θ is point-estimated with MLE.
Wasserstein GANs (WGANs) (Arjovsky et al., 2017) also has a loss function similar to CLAE’s:
n
LWGAN (θ, ψ) = - X D (X⑴;θ) - D(G (U⑴;ψ ； θ) ,	(35)
i=1
where D denotes the discriminator of WGANs that maps from the observation space into the real
space R. In this case, the counter part of the energy function is -D (x; θ), although D has a
constraint of 1-Lipschitz continuity, which the energy function of CLAE does not has.
F Experimental Settings
Unless we explicitly state otherwise, we use tanh activation instead of ReLU for all experiments,
because it is desirable that the whole model is differentiable at all points when performing sampling
algorithms based on Langevin dynamics, which require the differentiability of the potential energy.
In fact, we found that our ALD experimentally performs better when using tanh rather than ReLU.
F.1 Conjugate Bivariate Gaussian Example
In the experiment of conjugate bivariate Gaussian example in Section 3, we initially generate three
synthetic data x(1), x(2), x(3), where each x(i) is sampled from a bivariate Gaussian distribution as
follows:
P (Z) = N (z; μz, Σz), P (X | Z) = N (x; z, Σχ).
In this experiment, we set μz =	0	, Σz =	10	, and Σx =	0.7 0.6	. We can
	0		01		0.7 0.8	
calculate the exact posterior as folloWs:						
P (Z | X) = N (Z; μz∣x, ςz∣x),
with μz∣x = ∑z∣x (∑z1μz + ∑-1x) , ∑z∣x = (∑z1 + ∑x1)-1
In this experiment, we obtain 10,000 samples using ALD. We use four fully-connected layers of 128
units with tanh activation for the inference model and set the step size ηφ to 0.003.
F.2 Conjugate Univariate Gaussian Example
In the experiment of conjugate univariate Gaussian example in Section 3, we initially generate 100
synthetic data X(1), X(2), . . . , X(100), where each X(i) is sampled from a univariate Gaussian distri-
bution as follows:
p (Z)= N (z; μz, σ2), p(X | Z)= N (χ;z, σX).
In this experiment, We set μz = 0,σ2 = 1,σ2 = 0.01. In this case, We can calculate the exact
posterior as follows:
P (z | x) = N z;
1
1 I 1
σ2 + σ2
In this experiment, We obtain 20,000 samples using SGALD. We use four fully-connected layers of
128 units With tanh activation for the inference model and set the step size ηφ to 0.001. We set the
batch size to 10.
17
Under review as a conference paper at ICLR 2021
F.3 Neural likelihood example
We perform an experiment with a complex posterior, wherein the likelihood is defined with a ran-
domly initialized neural network fθ. Particularly, we parameterize fθ by four fully-connected layers
of 128 units with ReLU activation and two dimensional outputs like p (x | z) = N fθ (z) , σx2I .
We initialize the weight and bias parameters with N (0, 0.2I) and N (0, 0.1I), respectively. In ad-
dition, we set the observation variance σx to 0.25. We used the same neural network architecture for
the inference model fφ . Other settings are same as the previous conjugate Gaussian experiment.
The results are shown in Figure 10. The left three columns show the density visualizations of the
ground truth or approximation posteriors of AVI methods; the right two columns show the visual-
izations of 2D histograms and samples obtained using ALD. For AVI method, we use two different
models. One uses diagonal Gaussians, i.e., N (μ (x; φ), diag (σ2 (x; φ))), for the variational dis-
tribution, and the oher uses GauSSianS with full covariance N (μ (x; φ), Σ(x; φ)). From the den-
sity visualization of GT, the true posterior is multimodal and skewed; this leads to the failure of the
Gaussian AVI methods notwithstanding considering covariance. In contrast, the samples of ALD
accurately capture such a complex distribution, because ALD does not need to assume any tractable
distributions for approximating the true posteriors. The samples of ALD capture well the multimodal
and skewed posterior, while Gaussian AVI methods fail it even when considering covariance.
F.4 Image Generation
In the experiment of image generation, we resize the original image into 32 × 32 for all datasets.
For MNIST, we pad original image with zeros to make the size 32 × 32. We use a diagonal Gaus-
sian N (μz, diag (σZ)) as a latent prior for VAEs, ABH DLGM and LAEs, and treat the param-
eter μz, σ2 as learnable parameters. We use a diagonal Gaussian for the approximate posterior of
VAEs. The architecture of neural networks is summarized in Table 4. Conv k x s x p x c de-
notes a convolutional layer with k × k kernel, s × s stride, p × p padding, and c output channels.
ConvTranpose k x s xpx c denotes a transposed convolutional layer with k × k kernel, s × s stride,
p × p padding, and c output channels. Upsample denotes a nearest neighbor upsampling with scale
factor of 2. Lineard is a fully connected layer of output dimension d. We apply tanh activation after
each convolution, transposed convolution or linear layer except the last one. dx , dz and du are the
18
		Z m 石 酰	g y	T Conv4xlx0xdθut				8 m 石 声	Encoder		Table 4: NeUraI network arct
		e m 石 a.		T Conv3xlx2x64 T Conv3xlx2xdx				Z m 石 於		
			Sampler						Decoder	IiteetUreS
										
UlIderreVieW as a COnferenCe PaPer at ICLR 2021
TableH'COmPariSOIl Of den2∙s5'g PerfOnnanCe by VAE-LAES and CLAE∙The models are eval—
Uated by the mean SqUared error between original (Umlsy) data and data reconstructed by the
models from noisy data，NoiSy data is Created 6y adding GaUSSian IIoiSe Sampled from N (Oj12)
to Origal dat
MNlST SVHN CIFAR—10 CeIebA
VAEol71401041o2587o2131
LAEo2243ololo02703o2435
CLAE 0∙007778 0∙003676 0∙01440 0∙009690
dimensionality Of xy Z and UreSPeCtiVely∙ d。2 is equal to dN for LAES and CLAE-and 2f⅛ for
VAEFor all dataset-we Setthe minibatch SiZe Tn and the SteP SiZe (W9 W"ηe'} to Soo-ol∕72
respectivelyy where TZiSa SiZe Of training seWe Set dN U 32"u U 8 throughout the experiment
We USe the Same Sett5'g for the experiment Of anomaly detection.
19
Under review as a conference paper at ICLR 2021
Figure 11: MNIST samples
20
Under review as a conference paper at ICLR 2021
VAE	ABP
DLGM	LAE
CLAE
Figure 12: SVHN samples
21
Under review as a conference paper at ICLR 2021
VAE
DLGM
ABP
LAE
CLAE
Figure 13: CIFAR-10 samples
22
Under review as a conference paper at ICLR 2021
ABP
VAE
DLGM
CLAE
Figure 14: CelebA samples
LAE
23