Under review as a conference paper at ICLR 2021
Sparsifying Networks via Subdifferential In-
CLUSION
Anonymous authors
Paper under double-blind review
Ab stract
Sparsifying deep neural networks is of paramount interest in many areas, especially
when those networks have to be implemented on low-memory devices. In this
article, we propose a new formulation of the problem of generating sparse weights
for a neural network. By leveraging the properties of standard nonlinear activation
functions, we show that the problem is equivalent to an approximate subdifferential
inclusion problem. The accuracy of the approximation controls the sparsity. We
show that the proposed approach is valid for a broad class of activation functions
(ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to
induce sparsity whose convergence is guaranteed. Because of the algorithm flexibil-
ity, the sparsity can be ensured from partial training data in a minibatch manner. To
demonstrate the effectiveness of our method, we perform experiments on various
networks in different applicative contexts: image classification, speech recognition,
natural language processing, and time-series forecasting.
1	Introduction
Deep neural networks have evolved to the state-of-the-art techniques in a wide array of applications:
computer vision (Simonyan & Zisserman, 2015; He et al., 2016; Huang et al., 2017), automatic
speech recognition (Hannun et al., 2014; Dong et al., 2018; Li et al., 2019; Watanabe et al., 2018;
Hayashi et al., 2019; Inaguma et al., 2020), natural language processing (Turc et al., 2019; Radford
et al., 2019; Dai et al., 2019b; Brown et al., 2020), and time series forecasting (Oreshkin et al., 2020).
While their performance in various applications has matched and often exceeded human capabilities,
neural networks may remain difficult to apply in real-world scenarios. Deep neural networks leverage
the power of Graphical Processing Units (GPUs), which are power-hungry. Using GPUs to make
billions of predictions per day, thus comes with a substantial energy cost. In addition, despite their
quite fast response time, deep neural networks are not yet suitable for most real-time applications
where memory-limited low-cost architectures need to be used. For all those reasons, compression
and efficiency have become a topic of high interest in the deep learning community.
Sparsity in DNNs has been an active research topic generating numerous approaches. DNNs achieving
the state-of-the-art in a given problem usually have a large number of layers with non-uniform
parameter distribution across layers. Most sparsification methods are based on a global approach,
which may result in a sub-optimal compression for a reduced accuracy. This may occur because
layers with a smaller number of parameters may remain dense, although they may contribute more
in terms of computational complexity (e.g., for convolutional layers). Some methods, also known
as magnitude pruning, use a hard or soft-thresholding to remove less significant parameters. Soft
thresholding techniques achieve a good sparsity-accuracy trade-off at the cost of additional parameters
and increased computation time during training. Searching for a hardware efficient network is another
area that has been proven quite useful, but it requires a huge amount of computational resources.
Convex optimization techniques such as those used in (Aghasi et al., 2017) often rely upon fixed
point iterations that make use of the proximity operator (Moreau, 1962). The related concepts are
fundamental for tackling nonlinear problems and have recently come into play in the analysis of
neural networks (Combettes & Pesquet, 2020a) and nonlinear systems (Combettes & Woodstock,
2020).
This paper shows that the properties of nonlinear activation functions can be utilized to identify highly
sparse subnetworks. We show that the sparsification ofa network can be formulated as an approximate
1
Under review as a conference paper at ICLR 2021
subdifferential inclusion problem. We provide an iterative algorithm called subdifferential inclusion
for sparsity (SIS) that uses partial training data to identify a sparse subnetwork while maintaining good
accuracy. SIS makes even small-parameter layers sparse, resulting in models with significantly lower
inference FLOPs than the baselines. For example, SIS for 90% sparse MobileNetV3 on ImageNet-1K
achieves 66.07% top-1 accuracy with 33% fewer inference FLOPs than its dense counterpart and
thus provides better results than the state-of-the-art method RigL. For non-convolutional networks
like Transformer-XL trained on WikiText-103, SIS is able to achieve 70% sparsity while maintaining
21.1 perplexity score. We evaluate our approach across four domains and show that our compressed
networks can achieve competitive accuracy for potential use on commodity hardware and edge
devices.
2	Related Work
2.1	Inducing sparsity post training
Methods inducing sparsity after a dense network is trained involve several pruning and fine-tuning
cycles till desired sparsity and accuracy are reached (Mozer & Smolensky, 1989; LeCun et al., 1990;
Hassibi et al., 1993; Han et al., 2015; Molchanov et al., 2017; Guo et al., 2016; Park et al., 2020).
(Renda et al., 2020) proposed weight rewinding technique instead of vanilla fine-tuning post-pruning.
Net-Trim algorithm (Aghasi et al., 2017) removes connections at each layer of a trained network by
convex programming. The proposed method works for networks using rectified linear units (ReLUs).
Lowering rank of parameter tensors (Jaderberg et al., 2014; vahid et al., 2020; Lu et al., 2016),
removing channels, filters and inducing group sparsity (Wen et al., 2016; Li et al., 2017; Luo et al.,
2017; Gordon et al., 2018; Yu et al., 2019; Liebenwein et al., 2020) are some methods that take
network structure into account. All these methods rely on pruning and fine-tuning cycle(s) often from
full training data.
2.2	Inducing sparsity during training
Another popular approach has been to induce sparsity during training. This can be achieved by
modifying the loss function to consider sparsity as part of the optimization (Chauvin, 1989; Carreira-
Perpindn & Idelbayev, 2018; Ullrich et al., 2017; Neklyudov et al., 2017). Dynamically pruning
during training (Zhu & Gupta, 2018; Bellec et al., 2018; Mocanu et al., 2018; Dai et al., 2019a;
Lin et al., 2020b) by observing network flow. (Mostafa & Wang, 2019; Dettmers & Zettlemoyer,
2020; Evci et al., 2020) computes weight magnitude and reallocates weights at every step. Bayesian
priors (Louizos et al., 2017), L0, L1 regularization (Louizos et al., 2018), and variational dropout
(Molchanov et al., 2017) get accuracy comparable to (Zhu & Gupta, 2018) but at a cost of2× memory
and 4× computations during training. (Liu et al., 2019; Savarese et al., 2020; Kusupati et al., 2020;
Lee, 2019; Xiao et al., 2019; Azarian et al., 2020) have proposed learnable sparsity methods through
training of the sparse masks and weights simultaneously with minimal heuristics. Although these
methods are cheaper than pruning after training, they need at least the same computational effort as
training a dense network to find a sparse sub-network. This makes them expensive when compressing
big networks where the number of parameters ranges from hundreds of millions to billions (Dai et al.,
2019b; Li et al., 2019; Brown et al., 2020).
2.3	Training sparsely initialized networks
(Frankle & Carbin, 2019) showed that it is possible to find sparse sub-networks that, when trained
from scratch, were able to match or even outperform their dense counterparts. (Lee et al., 2019)
presented SNIP, a method to estimate, at initialization, the importance that each weight could have
later during training. In (Lee et al., 2020) the authors perform a theoretical study of pruning at
initialization from a signal propagation perspective, focusing on the initialization scheme. Recently,
(Wang et al., 2020) proposed GraSP, a different method based on the gradient norm after pruning, and
showed a significant improvement for moderate levels of sparsity. (Ye et al., 2020) starts with a small
subnetwork and progressively grow it to a subnetwork that is as accurate as its dense counterpart.
(Tanaka et al., 2020) proposes SynFlow that avoids flow collapse of a pruned network during training.
(Jorge et al., 2020) proposed FORCE, an iterative pruning method that progressively removes a small
number of weights. This method is able to achieve extreme sparsity at little accuracy expense. These
2
Under review as a conference paper at ICLR 2021
methods are not usable for big pre-trained networks and are expensive as multiple training rounds are
required for different sparse models depending on deployment scenarios (computing devices).
2.4	Efficient Neural Architecture Search
Hardware-aware NAS methods (Zoph et al., 2018; Real et al., 2019; Cai et al., 2018; Wu et al., 2019;
Tan et al., 2019; Cai et al., 2019; Howard et al., 2019) directly incorporate the hardware feedback into
efficient neural architecture search. (Cai et al., 2020) proposes to learn a single network composing of
a large number of subnetworks from which a hardware aware subnetwork can be extracted in linear
time. (Lin et al., 2020a) proposes a similar approach wherein they identify subnetworks that can be
run efficiently on microcontrollers (MCUs).
Our proposed algorithm applies to possibly large pre-trained networks. In contrast with methods
presented in Section 2.1, ours can use a small amount of training data during pruning and fewer
epochs during fine-tuning. As we will see in the next section, a key feature of our approach is that it
is based on a fine analysis of the mathematical properties of activation functions, so allowing the use
of powerful convex optimization tools that offer sound convergence guarantees.
3	Proposed Method
3.1	Variational principles
A basic neural network layer can be described by the relation:
y = R(Wx + b)	(1)
where x ∈ RM is the input, y ∈ RN the output, W ∈ RN ×M is the weight matrix, b ∈ RN the
bias vector, and R is a nonlinear activation operator from RN to RN . A key observation is that
most of the activation operators currently used in neural networks are proximity operators of convex
functions (Combettes & Pesquet, 2020a;b). We will therefore assume that there exists a proper
lower-semicontinuous convex function f from RN to R ∪ {+∞} such that R = proxf . We recall
that f is a proper lower-semicontinuous convex function if the area overs its graph, its epigraph
(y, ξ) ∈ RN × R f (y) 6 ξ , is a nonempty closed convex set. For such a function the proximity
operator of f at z ∈ RN (Moreau, 1962) is the unique point defined as
ProXf(Z)= argmin 1 ∣∣z - p『+ f(p).	(2)
p∈RN 2
It follows from standard subdifferential calculus that Eq. (1) can be re-expressed as the following
inclusion relation:
Wx+b-y∈∂f(y),	(3)
where ∂f(y) is the Moreau subdifferential of f at y defined as
∂f(y) = {t ∈ RN I (Vz ∈ RN)f(z) > f(y) + h | Z - y)}.	(4)
The subdifferential constitutes a useful extension of the notion of differential, which is applicable to
nonsmooth functions. The set ∂f (y) is closed and convex and, if y satisfies Eq. (1), it is nonempty.
The distance to this set of a point z ∈ RN is given by
d∂f(y)(z) = inf ∣z-t∣.	(5)
t∈∂f(y)
We thus see that the subdifferential inclusion in Eq. (3) is also equivalent to
d∂f(y)(Wx+ b -y) = 0.	(6)
Therefore, a suitable accuracy measure for approximated values of the layer parameters (W, b) is
d∂f (y)(W x +b - y).
3
Under review as a conference paper at ICLR 2021
3.2	Optimization problem
Compressing a network consists of a sparsification of its parameters while keeping a satisfactory
accuracy. Assume that, for a given layer, a training sequence of input/output pairs is available which
results from a forward pass performed on the original network for some input dataset of length K .
The training sequence is split in J minibatches of size T so that K = JT . The j-th minibatch with
j ∈ {1, . . . , J} is denoted by (xj,t, yj,t)16t6T . In order to compress the network, we propose to
solve the following constrained optimization problem.
Problem 1 We want to
minimize g(W, b)	(7)
(W,b)∈C
with
T
C = {(W, b) ∈ RN ×M X RNl (Vj ∈ {1,...,J}) X d∂f(yj,t)(Wxj,t + b-yj,t) 6 Tη}, (8)
t=1
where g is a sparsity measure defined on RN ×M X RN and η ∈ [0, +∞[ is some accuracy tolerance.
Since, for every j ∈ {1, . . . , J}, the function (W, b) 7→ PtT=1 d2∂f(y )(W xj,t + b - yj,t) is
continuous and convex, C is a closed and convex subset of RN×M X RN. In addition, this set
is nonempty when there exist W ∈ RN×M and b ∈ RN such that, for every j ∈ {1,...,J} and
t ∈ {1,..., T}, d∂f(yj ±)(Wxj,t + b 一 yj,t) = 0. As We have seen in Section 3.1, this condition is
satisfied when (W, b) are the parameters of the uncompressed layer. Often, the sparsity of the weight
matrix is the determining factor whereas the bias vector represents a small number of parameters, so
that we can make the following assumption.
Assumption 2 For every W ∈ RN×M and b ∈ RN, g(W, b) = h(W) where h is a func-
tion from RN×M to R ∪ {+∞}, which is lower-semicontinuous, convex, and coercive (i.e.
lim∣∣w∣∣f→+∞ h(W) = +∞). In addition, there exists (W,b) ∈ C such that h(W) < +∞
and there exists (j *,t*) ∈ {1,...,J} X {1,...,T} such that y7∙* ,t* lies in the interior of the range
of R.
Under this assumption, the existence of a solution to Problem 1 is guaranteed (see Appendix A).
A standard choice for such a function is the '「norm of the matrix elements, h = ∣∣∙∣∣ 1, but other
convex sparsity measures could also be easily incorporated within this framework, e.g. group sparsity
measures. Another point worth being noticed is that constraints other than (8) could be imposed. For
example, one could make the following alternative choice for the constraint set
C = {(W,b) ∈ RN×M X RN l	SUp	d∂f(yjt)(Wxj,t + b — yj,t) 6 √η}.	(9)
j ∈{1,...,J},t∈{1,...,T}
Although the resulting optimization problem could be tackled by the same kind of algorithm as the
one we will propose, Constraint (8) leads to a simpler implementation.
3.3	Optimization algorithm
A standard proximal method for solving Problem 1 is the Douglas-Rachford algorithm (Lions &
Mercier, 1979; Combettes & Pesquet, 2007). This algorithm alternates between a proximal step
aiming at sparsifying the weight matrix and a projection step allowing a given accuracy to be reached.
Assume that a solution to Problem 1 exists. Then, this algorithm reads as shown on the top of the
next page.
The Douglas-Rachford algorithm uses parameters γ ∈ ]0, +∞[ and (λn)n∈N in ]0, 2[ such that
Pn∈N λn(2 一 λn) = +∞. Throughout this article, projS denotes the projection onto a nonempty
closed convex set S. Under these conditions, the sequence (Wn, bn)n∈N generated by Algorithm 1 is
guaranteed to converge to a solution to Problem 1 if there exists (W, b) ∈ C such W is a point in the
relative interior of the domain of h Combettes & Pesquet (2007) (see illustrations in Appendix E).
4
Under review as a conference paper at ICLR 2021
Algorithm 1: Douglas-Rachford algorithm for network compression
Initialize: W0 ∈ RN×M and b0 ∈ RN
for n = 0, 1, . . . do
Wn = proxγh (Wn)
(Wfn , ebn) = projC (2Wn - Wcn , bn)
Wcn+1 =Wcn+λn(Wfn-Wn)
k — k I λ /ɪ	k ∖
_ bn+1 = bn + λn (bn	bn).
The proximity operator of function γh has a closed-form for standard choices of sparsity measures1.
For example, when h = ∣∣T∣ι, this operator reduces to a soft-thresholding (with threshold value Y) of
the input matrix elements. In turn, since the convex set C has an intricate form, an explicit expression
of projC does not exist. Finding an efficient method for computing this projection for large datasets
thus constitutes the main challenge in the use of the above Douglas-Rachford strategy, which we will
discuss in the next section.
3.4	Computation of the projection onto the constraint set
For every mini-batch index j ∈ {1, . . . , J}, let us define the following convex function:
T
(∀(W, b) ∈RN×M ×RN) cj(W,b)=Xd2∂f(yj,t)(Wxj,t+b-yj,t)-Tη. (10)
t=1
Note that, for every j ∈ {1, . . . , J}, function cj is differentiable and its gradient at (W, b) ∈
RN ×M × RN is given by
RCj(W, b) = (VwCj(W, b), VbCj(W, b)),	(11)
where
TT
VW Cj (W, b) = 2X ej,txj>,t,	VbCj(W, b) = 2Xej,t	(12)
t=1	t=1
with
(∀t ∈ {1, . . . , T}) ej,t = Wxj,t + b - yj,t - proj∂f(yj,t)(Wxj,t + b - yj,t).	(13)
A pair of weight/bias parameters belongs to C if and only if it lies in the intersection of the 0-lower
level sets of the functions (Cj)16j6J. To compute the projection of some (W, b) ∈ RN×M × RN
onto this intersection, we use Algorithm 2 (k ∙ ∣∣f denotes here the Frobenius norm).
This iterative algorithm has the advantage of proceeding in a minibatch manner. It allows us to
choose the mini-batch index jn at iteration n in a quasi-cyclic manner. The simplest rule is to
activate each minibatch once within J successive iterations of the algorithm so that they correspond
to an epoch. The proposed algorithm belongs to the family of block-iterative outer approximation
schemes for solving constrained quadratic problems, which was introduced in (Combettes, 2003).
The convergence of the sequence (Wn, bn)n∈N generated by Algorithm 2 to projC (W, b) is thus
guaranteed. One of the main features of the algorithm is that it does not require to perform any
projection onto the 0-lower level sets of the functions Cj , which would be intractable due to their
expressions. Instead, these projections are implicitly replaced by subgradient projections, which are
much easier to compute in our context.
3.5	Dealing with various nonlinearities
For any choice of activation operator R, we have to calculate the projection onto ∂f (y) for every
vector y satisfying Eq. (1). This projection is indeed required in the computation of the gradients
of functions (Cj)16j 6J, as shown by Eq. (13). Two properties may facilitate this calculation. First,
if f is differentiable at y, then ∂f(y) reduces to a singleton containing the gradient Vf (y) of
1http://proximity-operator.net
5
Under review as a conference paper at ICLR 2021
Algorithm 2: Minibatch algorithm for computing projC (W, b)
Initialize: W0 = W and b° = b
for n = 0, 1, . . . do
Select a batch of index jn ∈ {1, . . . , J}
ifcjn(Wn,bn) > 0 then
Compute VwCjn (Wn, bn) and VbCjn (Wn, bn) by using Eq.(12)andEq.(13)
cjn( _ ____Cjn (Wn ,bn) ^Wcjn (Wn,bn )___
n = gwCjn,n(Wn,bn )kF + gbCjn (Wn,bn)∣∣2
cl. _ _____Cjn (Wn ,bn ) ^bCjn (Wn ,bn )_
δbn = kVW Cjn,n(Wn,bn)kF+kVbCjn (Wn,bn)k2
πn = tr((W0 - Wn)>δWn) + (b0 - bn)>δbn
μn =IlWO - WnllF + kb0 - bnk2
νn = kδWnk2F+ kδbnk2
ζn = μn νn ― πn
if ζn = 0 and πn > 0 then
Wn+1 = Wn — δWn
L bn+1 = bn — δbn
else if ζn > 0 and πn νn > ζn then
Wn+1 = Wθ -(I + ∏n )δWn
bn+1 = bO - (I + πnn)δbn
else
Wn+1 = Wn + Vn (∏n(Wθ - Wn)- MnδWn)
bn+1 = bn + Zζn (πn(b0 — bn) — μnδbn)
else
I Wn+1 = Wn
L bn+1 = bn
f at y, so that, for every z ∈ RN, proj∂f(y) (z) = Vf (y). Second, R is often separable, i.e.
consists of the application of a scalar activation function ρ: R → R to each component of its input
argument. According to our assumptions, there thus exists a proper lower-semicontinuous convex
function 夕 from R to R ∪ {+∞} such that P = ProxW and, for every z = (ζ(k))16k6N ∈ RN,
f(z) = pN=ι P(Z(k)). This implies that, for every Z = (Z(k))i6k6N ∈ RN, Projdf(y)(Z)=
(Proj∂W(υ(k))(ζ(k)))16k6N , where the components of y are denoted by (υ(k))16k6N . Based on these
properties, a list of standard activation functions P is given in Table 1, for which we provide the
associated expressions of the projection onto ∂φ. The calculations are detailed in Appendix B.
An example of non-separable activation operator frequently employed in neural network architectures
is the softmax operation defined as: (∀Z = (ζ(k))16k6N ∈ RN) R(Z)
e	exp(Z(k))	A
PN= exp(Z(j))
It is shown in Appendix C that, for every y = (υ(k))16k6N in the range ofR,
(∀z ∈ Rn) Projdf(y)(z) = Q(y)+ 1>(Z NQ(y)) 1,
16k6N
(14)
where 1 = [1, . . . , 1]> ∈ RN and Q(y) = (ln υ(k) + 1 - υ(k))16k6N.
3.6 SIS on multi-layered networks
Algorithm 3 describes how we make use of SIS for a multi-layered neural network. We use a
pretrained network and part of the training sequence to extract layer-wise input-output features. Then
we apply SIS on each individual layer l by passing η, layer parameters (W(l), b(l)) and extracted
input-output features (Y (l-1), Y (l)) to Algorithm 1. The benefit of applying SIS to each layer
independently is that we can apply SIS on all the layers of a network in parallel. This reduces the
time required to process the whole network and compute resources are optimally utilized.
6
Under review as a conference paper at ICLR 2021
			
Name	ρ(Z)		Proj∂W(υ) (Z)
Sigmoid	(I + e-ζ)-1 - 2		ln(U + 1/2) - ln(U - 1/2) - U
Arctangent	(2∕π) arctan(Z)		tan(πυ∕2) — U
ReLU	max{Z, 0}		0 if U > 0 or Z > 0 Z otherwise
Leaky ReLU	Z if Z > 0 αZ otherwise		0	if U > 0 I (1/a — 1)υ otherwise
Capped ReLU	ReLUα (Z) = min	{max{Z, 0}, α}	(Z if (υ = 0 and Z < 0) or (U = α and Z > 0) 10 otherwise
ELU	Z α exP(Z) - 1	if Z > 0 otherwise	0	if U > 0 Iln (Uoa) — υ otherwise
QUadReLU	(Z + a)ReLU2a (Z + α)
4α
υ	if υ = 0 and ζ 6 -α
-U + 2ʌ/ɑv 一 α if U ∈]0, α]
or (υ = 0 and ζ >
U - α	otherwise
Table 1: Expression of ProjdW(U)(Z) for Z ∈ R and U in the range of ρ, for standard activation
functions ρ. α is a positive constant.
Algorithm 3: Parallel SIS for multi-layered network
InpUt: input sequence X ∈ RM ×K , compression parameter η > 0, weight matrices
W(1) , . . . , W(L), and bias vectors b(1), . . . , b(L)
Y⑼-X
for l = 1, . . . , L do
Y(I) = R'(W (I)T Y (IT) + b(l))
_ C(I),b(I) ― SIS(η, W(I), b(l),γ(I),y(IT))
OUtpUt: Wc(1), . . . , Wc(L) andbb(1), . . . , bb(L)
4	Experiments
In this section, we conduct various experiments to validate the effectiveness of SIS in terms of the test
accuracy vs. sparsity and inference time FLOPs vs. sparsity by comparing against RigL (Evci et al.,
2020). We also include SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al.,
2020), STR (Kusupati et al., 2020), and FORCE (Jorge et al., 2020). These methods start training
from a sparse network and have some limitations when compared to methods that prune a pretrained
network Blalock et al. (2020); Gale et al. (2019). For a fair comparison we also include LRR (Renda
et al., 2020) which uses a pretrained network and multiple rounds of pruning and retraining by
leveraging learning rate rewinding. The experimental setup is described in Appendix D.
4.1	Modern ConvNets on CIFAR and ImageNet
We compare SIS with competitive baselines on CIFAR-10/100 for three different sparsity regimes
90%, 95%, 98%, and the results are listed in Table 2. It can be observed that LRR, RigL and SIS are
able to maintain high accuracy with increasing sparsity. LRR performs better than both RigL and SIS
for VGG19 on CIFAR-10 at 90% and 95% sparsity. When compared to SNIP, our method achieves
impressive performance for VGG19 on CIFAR-100 (58.46 -> 71.17). In the case of ResNet50, SIS
outperforms all the other methods for CIFAR-10/100 except for CIFAR-100 at 90%.
7
Under review as a conference paper at ICLR 2021
Dataset	CIFAR-10	CIFAR-100
Pruning ratio	90%	95%	98%	90%	95%	98%
VGG19 (Baseline)	94.23	-	-	74.16	-	-
SNIP (Lee et al., 2019)	93.63	93.43	92.05	72.84	71.83	58.46
GraSP (Wang et al., 2020)	93.30	93.04	92.19	71.95	71.23	68.90
SynFlow (Tanaka et al., 2020)	93.35	93.45	92.24	71.77	71.72	70.94
STR (Kusupati et al., 2020)	93.73	93.27	92.21	71.93	71.14	69.89
FORCE (Jorge et al., 2020)	93.87	93.30	92.25	71.9	71.73	70.96
LRR (Renda et al., 2020)	94.03	93.53	91.73	72.12	71.36	70.39
RigL (Evci et al., 2020)	93.47	93.35	93.14	71.82	71.53	70.71
SIS (Ours)	93.99	93.31	93.16	72.06	71.85	71.17
ResNet50 (Baseline)	94.62	-	-	77.39	-	-
SNIP (Lee et al., 2019)	92.65	90.86	87.21	73.14	69.25	58.43
GraSP (Wang et al., 2020)	92.47	91.32	88.77	73.28	70.29	62.12
SynFlow (Tanaka et al., 2020)	92.49	91.22	88.82	73.37	70.37	62.17
STR (Kusupati et al., 2020)	92.59	91.35	88.75	73.45	70.45	62.34
FORCE (Jorge et al., 2020)	92.56	91.46	88.88	73.54	70.37	62.39
LRR (Renda et al., 2020)	92.62	91.27	89.11	74.13	70.38	62.47
RigL (Evci et al., 2020)	92.55	91.42	89.03	73.77	70.49	62.33
SIS (Ours)	92.81	91.69	90.11	73.81	70.62	62.75
Table 2: Test accuracy of sparse VGG19 and ResNet50 on CIFAR-10 and CIFAR-100 datasets.
Sparsity	60%	80%	90%	96.5%
	Top-1 Acc(%)	FLOPs	Top-1 Acc(%)	FLOPs	Top-1 Acc(%)	FLOPs	Top-1 Acc(%)	FLOPs
SNIP (2019)	73.95	1.88G	69.67	941M	65.30	409M	54.70	292M
GraSP (2020)	74.02	1.63G	72.06	786M	68.14	470M	51.31	290M
SynFlow (2020)	74.13	1.61G	72.18	776M	68.26	465M	60.14	288M
STR (2020)	76.82	1.59G	75.29	705M	74.13	341M	67.20	117M
FORCE (2020)	73.71	1.39G	70.96	685M	69.78	455M	59.00	276M
LRR (2020)	77.10	2.04G	76.61	918M	75.92	584M	72.13	371M
RigL (2020)	76.50	1.79G	75.10	920M	73.00	515M	72.70	257M
SIS (Ours)	77.05	1.34G	76.96	647M	76.31	298M	73.11	101M
Table 3: Test Top-1 accuracy and inference FLOPs of sparse ResNet50 on ImageNet where baseline
accuracy and inference FLOPs are 77.37% and 4.14G, respectively.
Sparsity	75%			90%		
	LRR	RigL	SIS (Ours)	LRR	RigL	SIS (Ours)
V1 (70.90)	68.79	69.97	70.11	66.59	67.10	67.15
FLOPs (569M)	498M	461M	367M	401M	331M	284M
V2 (71.88)	68.83	69.60	69.83	64.17	65.23	65.11
FLOPs (300M)	267M	211M	182M	192M	174M	162M
V3 (72.80)	68.97	70.21	70.47	64.32	65.13	66.07
FLOPs (226M)	187M	198M	172M	185M	167M	151M
Table 4: Test accuracy and inference FLOPs of sparse MobileNet versions using RigL and SIS on
ImageNet, baseline accuracy and inference FLOPs shown in brackets.
Due to its small size and controlled nature, CIFAR-10/100 may not appear sufficient to draw solid
conclusions. We thus conduct further experiments on ImageNet using ResNet50 and MobileNets.
Table 3 shows that, in the case of ResNet50, LRR performs marginally better than SIS at 60% sparsity.
8
Under review as a conference paper at ICLR 2021
At 80%, 90%, and 96.5% sparsity SIS outperforms all other methods. For all sparsity regimes,
SIS achieves least inference FLOPs. RigL achieves these results in same training time as SIS with
less training time FLOPs. This may be related to the fact that SIS can achieve better compression
in the last layer before SoftMax. MobileNets are compact architectures designed specifically for
resource-constrained devices. Table 4 shows results for RigL and SIS on MobileNets. We observe
that SIS outperforms all MobileNet versions at 75% sparsity level. For a 90% sparsity level, SIS
outperforms RigL for MobileNet V1 and V3 whereas, for MobileNetV2, RigL performs slightly
better than SIS at 90% sparsity level. In all the cases, we can see that the resulting SIS sparse network
uses fewer FLOPs than RigL. A possible explanation for this fact is that SIS leverages activation
function properties during the sparsification process.
4.2	Sequential Tasks
Network	JASPER		TranSformer-XL		N-BEATS	
	WER	FLOPS	PPL	FLOPs	SMAPE	FLOPs
Dense	12.2	4.53G	18.6	927.73G	8.3	41.26M
SNIP (Lee et al., 2019)	14.3	2.74G	24.6	398.92G	10.1	21.45M
LRR (Renda et al., 2020)	13.7	2.61G	23.1	339.21G	9.3	14.47M
RigL (Evci et al., 2020)	13.9	2.69G	22.4	326.56G	10.2	15.13M
SIS (Ours)	13.1	2.34G	21.1	290.38G	9.7	14.21M
Table 5: Test accuracy and inference FLOPs of JASPER, Transformer-XL, and N-BEATS at 70%
sparsity.
Jasper on LibriSpeech. Jasper is a speech recognition model that uses 1D convolutions. The
trained network is a 333 million parameter model and has a word error rate (WER) of 12.2 on the test
set. We apply SIS on this network and compare it with RigL and SNIP in terms of sparsity. Table 5
reports WER and inference FLOPs for all three methods. SIS marginally performs better than SIS on
this task in terms of WER and FLOPs for 70% sparsity. The main advantage of our approach lies in
the fact that we can use a single pre-trained Jasper network and achieve different sparsity level for
different types of deployment scenarios with less computational resources than RigL.
Transformer-XL on WikiText-103. Transformer-XL is a language model with 246 million param-
eters. The trained network on WikiText-103 has a perplexity score (PPL) of 18.6. In Table 5, we see
that SIS performs better than SNIP and RigL in terms of PPL and has 68% fewer inference FLOPs.
This is due to the fact that large language models can be efficiently trained and then compressed
easily, but training a sparse sub-network from scratch is hard (Li et al., 2020), as is the case with
SNIP and RigL. SNIP uses one-shot pruning to obtain a random sparse sub-network, whereas RigL is
able to change its structure during training, which allows it to perform better than SNIP.
N-BEATS on M4. N-BEATS is a very deep residual fully-connected network to perform forecasting
in univariate time-series problems. It is a 14 million parameter network. The Symmetric Mean
Absolute Percentage Error (SMAPE) of the dense network on the M4 dataset is 8.3%. We apply SIS
on this network and compare its performance with respect to RigL and SIS. SIS performs better than
both methods and results in 65% fewer inference FLOPs.
5	Conclusion
In this article, we have proposed a novel method for sparsifying neural networks. The compression
problem for each layer has been recast as the minimization of a sparsity measure under accuracy
constraints. This constrained optimization problem has been solved by means of advanced convex
optimization tools. The resulting SIS algorithm is i) reliable in terms of iteration convergence
guarantees, ii) applicable to a wide range of activation operators, and iii) able to deal with large
datasets split into mini-batches. Our numerical tests demonstrate that the approach is not only
appealing from a theoretical viewpoint but also practically efficient.
9
Under review as a conference paper at ICLR 2021
References
Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep
neural networks with performance guarantee. In NeuIPS, 2017.
Kambiz Azarian, Yash Bhalgat, Jinwon Lee, and Tijmen Blankevoort. Learned threshold pruning.
arXiv preprint arXiv:2003.00075, 2020.
Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in
Hilbert Spaces. Springer, 2019.
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein. Deep rewiring: Training
very sparse deep networks. In ICLR, 2018.
Davis Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John Guttag. What is the state of
neural network pruning? InI. Dhillon, D. Papailiopoulos, and V. Sze (eds.), Proceedings of Machine
Learning and Systems, volume 2, pp. 129-146, 2020. URL https://Proceedings.mlsys.
org/paper/2020/file/d2ddea18f00665ce8623e36bd4e3c7c5-Paper.pdf.
Tom B. Brown, Benjamin Pickman Mann, Nick Ryder, Melanie Subbiah, Jean Kaplan, et al. Language
models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020.
Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. Path-level network transformation
for efficient architecture search. arXiv preprint arXiv:1806.02639, 2018.
Han Cai, Ligeng Zhu, and Song Han. ProxylessNAS: Direct neural architecture search on target task
and hardware. In ICLR, 2019.
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once-for-all: Train one network
and specialize it for efficient deployment. In ICLR, 2020.
Miguel A. Carreira-Perpindn and Yerlan Idelbayev. "Learning-Compression” algorithms for neural
net pruning. In CVPR, 2018.
Yves Chauvin. A back-propagation algorithm with optimal use of hidden units. In NeurIPS. 1989.
Patrick Louis Combettes. A block-iterative surrogate constraint splitting method for quadratic signal
recovery. IEEE TSP, 2003.
Patrick Louis Combettes and Jean-Christophe Pesquet. A Douglas-Rachford splitting approach to
nonsmooth convex variational signal recovery. IEEE JSTSP, 2007.
Patrick Louis Combettes and Jean-Christophe Pesquet. Deep neural network structures solving
variational inequalities. SVVA, 2020a.
Patrick Louis Combettes and Jean-Christophe Pesquet. Lipschitz certificates for layered network
structures driven by averaged activation operators. SIMODS, 2020b.
Patrick Louis Combettes and Zev Woodstock. A fixed point framework for recovering signals from
nonlinear transformations. arXiv preprint arXiv:2003.01260, 2020.
Xiaoliang Dai, Hongxu Yin, and Niraj K. Jha. NeST: A neural network synthesis tool based on a
grow-and-prune paradigm. IEEE TC, 2019a.
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, et al. Transformer-XL: Attentive
language models beyond a fixed-length context. In ACL, 2019b.
Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance. arXiv preprint arXiv:1907.04840, 2020.
Linhao Dong, Shuang Xu, and Bo Xu. Speech-transformer: A no-recurrence sequence-to-sequence
model for speech recognition. In ICASSP, 2018.
Utku Evci, Erich Elsen, Pablo Castro, and Trevor Gale. Rigging the lottery: Making all tickets
winners. In ICML, 2020.
10
Under review as a conference paper at ICLR 2021
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In ICLR, 2019.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
e-prints, arXiv:1902.09574, 2019. URL https://arxiv.org/abs/1902.09574.
Ariel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao Wu, et al. Morphnet: Fast simple resource-
constrained structure learning of deep networks. In CVPR, 2018.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient dnns. In NeurIPS,
2016.
Song Han, Jeff Pool, John Tran, and William J. Dally. Learning both weights and connections for
efficient neural networks. In NeurIPS, 2015.
Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, et al. Deep-
speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567, 2014.
Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network
pruning. In ICNN, 1993.
Tomoki Hayashi, Ryuichi Yamamoto, Katsuki Inoue, Takenori Yoshimura, Shinji Watanabe, et al.
ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit.
arXiv preprint arXiv:1910.10909, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun
Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, Quoc V. Le, and Hartwig Adam. Searching
for mobilenetv3. In ICCV, 2019.
Gao Huang, Zhuang Liu, and Kilian Q. Weinberger. Densely connected convolutional networks. In
CVPR, 2017.
Hirofumi Inaguma, Shun Kiyono, Kevin Duh, Shigeki Karita, Nelson Enrique Yalta Soplin, et al.
ESPnet-ST: All-in-one speech translation toolkit. arXiv preprint arXiv:2004.10234, 2020.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. In BMVC, 2014.
PaU Jorge, Amartya SanyaL Harkirat BehL Philip Torr, Gregory Rogez, and PUneet Dokania. Pro-
gressive skeletonization: Trimming more fat from a network at initialization. arXiv preprint
arXiv:2006.09081, 2020.
Aditya KUsUpati, Vivek RamanUjan, Raghav Somani, Mitchell Wortsman, Prateek Jain, Sham Kakade,
et al. Soft threshold weight reparameterization for learnable sparsity. In ICML, 2020.
Yann LeCUn, John S. Denker, and Sara A. Solla. Optimal brain damage. In NeurIPS, 1990.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: Single-shot network prUning based
on connection sensitivity. In ICLR, 2019.
Namhoon Lee, Thalaiyasingam Ajanthan, Stephen GoUld, and Philip H. S. Torr. A signal propagation
perspective for prUning neUral networks at initialization. In ICLR, 2020.
Yognjin Lee. Differentiable sparsification for deep neUral networks. arXiv preprint arXiv:1910.03201,
2019.
Hao Li, Asim Kadav, Igor DUrdanovic, Hanan Samet, and Hans Peter Graf. PrUning filters for
efficient convnets. In ICLR, 2017.
Jason Li, Vitaly LavrUkhin, Boris GinsbUrg, Ryan Leary, Oleksii KUchaiev, Jonathan M. Cohen,
HUyen NgUyen, and Ravi Teja Gadde. Jasper: An end-to-end convolUtional neUral acoUstic model.
In Interspeech, 2019.
11
Under review as a conference paper at ICLR 2021
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, et al. Train large,
then compress: Rethinking model size for efficient training and inference of transformers. arXiv
preprint arXiv:2002.11794, 2020.
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus. Provable filter pruning
for efficient neural networks. In ICLR, 2020.
Ji Lin, Wei-Ming Chen, Yujun Lin, John Cohn, Chuang Gan, and Song Han. Mcunet: Tiny deep
learning on iot devices. In NeurIPS, 2020a.
Tao Lin, Sebastian U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi. Dynamic model pruning
with feedback. In ICLR, 2020b.
Pierre-Louis Lions and Bertrand Mercier. Splitting algorithms for the sum of two nonlinear operators.
SIAM Journal on Numerical Analysis, 1979.
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell. Rethinking the value of
network pruning. In ICLR, 2019.
Christos Louizos, Karen Ullrich, and Max Welling. Bayesian compression for deep learning. In
NeurIPS, 2017.
Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0
regularization. In ICLR, 2018.
Zhiyun Lu, Vikas Sindhwani, and Tara N. Sainath. Learning compact recurrent neural networks. In
ICASSP, 2016.
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural
network compression. In ICCV, 2017.
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture
models. In ICLR, 2017.
Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity
inspired by network science. Nature Communications, 2018.
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsifies deep neural
networks. In ICML, 2017.
Jean-Jacques Moreau. Fonctions convexes duales et points proximaux dans un espace hilbertien.
Comptes rendus hebdomadaires des seances de l,Academie des sciences, 1962.
Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks
by dynamic sparse reparameterization. In ICML, 2019.
Michael C. Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In NeurIPS, 1989.
Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Structured bayesian
pruning via log-normal multiplicative noise. In NeurIPS, 2017.
Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, and Yoshua Bengio. N-BEATS: Neural basis
expansion analysis for interpretable time series forecasting. In ICLR, 2020.
V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: An ASR corpus based on public
domain audio books. In ICASSP, 2015.
Sejun Park, Jaeho Lee, Sangwoo Mo, and Jinwoo Shin. Lookahead: A far-sighted alternative of
magnitude-based pruning. In ICLR, 2020.
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. 2019.
12
Under review as a conference paper at ICLR 2021
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V. Le. Regularized evolution for image
classifier architecture search. In AAAI, 2019.
Alex Renda, Jonathan Frankle, and Michael Carbin. Comparing rewinding and fine-tuning in neural
network pruning. In ICLR, 2020.
Pedro Savarese, Hugo Silva, and Michael Maire. Winning the lottery with continuous sparsification.
arXiv preprint arXiv:arXiv:1912.04427, 2020.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In ICLR, 2015.
Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and
Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In CVPR, 2019.
Hidenori Tanaka, Daniel Kunin, Daniel Yamins, and Surya Ganguli. Pruning neural networks without
any data by iteratively conserving synaptic flow. arXiv preprint arXiv:2006.05467, 2020.
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Well-read students learn better:
On the importance of pre-training compact models. arXiv preprint arXiv:1908.08962v2, 2019.
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression.
In ICLR, 2017.
Keivan Alizadeh vahid, Anish Prabhu, Ali Farhadi, and Mohammad Rastegari. Butterfly transform:
An efficient fft based neural architecture design. In CVPR, 2020.
Chaoqi Wang, Guodong Zhang, and Roger Grosse. Picking winning tickets before training by
preserving gradient flow. In ICLR, 2020.
Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, et al. Espnet:
End-to-end speech processing toolkit. In Interspeech, 2018.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In NeurIPS. 2016.
Bichen Wu, Xiaoliang Dai, Peizhao Zhang, Yanghan Wang, Fei Sun, Yiming Wu, et al. FBNet:
Hardware-aware efficient convnet design via differentiable neural architecture search. In CVPR,
2019.
Xia Xiao, Zigeng Wang, and Sanguthevar Rajasekaran. Autoprune: Automatic network pruning by
regularizing auxiliary parameters. In NeurIPS. 2019.
Mao Ye, Chengyue Gong, Lizhen Nie, Denny Zhou, Adam Klivans, and Qiang Liu. Good subnetworks
provably exist: Pruning via greedy forward selection. In ICML, 2020.
Jiahui Yu, Linjie Yang, Ning Xu, Jianchao Yang, and Thomas Huang. Slimmable neural networks. In
ICLR, 2019.
Michael Zhu and Suyog Gupta. To prune, or not to prune: Exploring the efficacy of pruning for
model compression. In ICLR, 2018.
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le. Learning transferable architectures
for scalable image recognition. In CVPR, 2018.
13
Under review as a conference paper at ICLR 2021
Appendices
A Existence of a solution to Problem 1 under Assumption 2
Under Assumption 2, Problem 1 is equivalent to
minimize h(W)	(15)
(W,b)∈C
with
C = {(W,b) ∈ RN×M × RN I max Cj(W,b) 6 0},	(16)
j∈{1,...,J}
where the functions (cj)16j6J are defined in Eq. (10). These functions being convex, Φ =
maxj∈{1,...,J} cj is convex (Bauschke & Combettes, 2019, Proposition 8.16). We deduce that
Ψ = infb∈RΝ Φ(∙, b) is also a convex function (BaUschke & Combettes, 2019, Proposition 8.35).
Since Φ > -ηT, Ψ is finite valued. It is thus continuous on RN×M (Bauschke & Combettes, 2019,
Corollary 8.40). Let us now consider the problem:
minimize h(W)	(17)
W ∈lev60 Ψ
where lev60 Ψ is the 0-lower level set of Ψ defined as
lev60 Ψ = W ∈ RN×M II Ψ(W) 6 0},	(18)
Ψ being both convex and continuous, lev60 Ψ is closed and convex. According to Assumption 2,
there exists (W,b) ∈ RN×M × RN such that h(W) < +∞ and Φ(W,b) 6 0, which implies that
Ψ(W) 6 0. This shows that "wo Ψ has a nonempty intersection with the domain of h. By invoking
now the coercivity property of h, the existence of a solution Wc to Problem (17) is guaranteed by
standard convex analysis results (Bauschke & Combettes, 2019, Theorem 11.10).
N
To show that (W, b) is a solution to (15), it is sufficient to show that there exists b ∈ RN such that
Φ(W, b) = Ψ(W). This is equivalent to prove that there exists a solution b to the problem:
minimize Φ(Wc, b).	(19)
b∈RN
We know that Φ(W, ∙) is a continuous function. In addition, We have assumed that there exists
(j*,t*) ∈ {1,...,J} × {1,...,T} such that yj*,t* is an interior point of R(RN), which is also
equal to the domain of ∂f and a subset of the domain of f . Since f is continuous on the interior
of its domain, ∂f(yj*,t* ) is bounded (Bauschke & Combettes, 2019, Proposition 16.17(ii)). Then
d∂f(yj* t*)is coercive, hence cj* (W, ∙) IS coercive, and so IS Φ(W, ∙) > cj* (W, ∙). The existence of
b thus follows from the Weierstrass theorem.
B Results in Table 1
The results are derived from the expression of the convex function 夕 associated with each activation
function ρ (Combettes & Pesquet, 2020a, Section 2.1) (Combettes & Pesquet, 2020b, Section 3.2).
Sigmoid
1
(Z + 1∕2)ln(Z + 1/2) + (1/2 - Z) ln(1∕2 — Z)- 2(Z2 + 1/4)
(∀ζ ∈ R) φ(Z) =	if lζ| < 1/2	(20)
— 1/4	if |Z| = 1/2
.十∞	if |Z| > 1/2.
The range of the Sigmoid function is ] - 1/2, 1/2[ and the above function is differentiable on this
interval and its derivative at every υ ∈] - 1/2, 1/2[ is
g0(υ) = ln(υ + 1/2) — ln(υ — 1/2) — υ.	(21)
We deduce that, for every Z ∈ R, ProjdW(U)(Z)=夕0(υ).
14
Under review as a conference paper at ICLR 2021
Arctangent	(VZ ∈ R) φ(ζ) = (-2 ln (cos (苧))-2Z2, if |Z1 < 1	(22) [+∞,	if |Z| > 1.
By proceeding for this function similarly to the Sigmoid function, we have, for every υ ∈ P(R)
]-1,1[,	(VZ ∈ R) proj∂奴U)(Z) = d(v) = tan(πv∕2) - v.	(23)
ReLU	(VZ ∈R) "{+∞ oftherw0se.	(24)
For every V ∈ P(R) = [0, +∞ [, we have
	%(V) = {{0}∞, 0] ifv=0.	(25)
We deduce that	(VZ ∈ R)PrOjMU)(Z) = {0 :蓝心"0	(26)
Leaky ReLU	(VZ ∈ R)	O(Z)= {01∕0- 1)Z2/2 if Z > 0.	(27)
Since this function is differentiable on R, for every V ∈ R,
	(VZ ∈ R)PrOjdRU)(Z) = "(V)= {1/a- 1)v OthVrWi0e.	(28)
Capped ReLU	(VZ ∈ R)O(Z)= {+∞ ofthe∈±]	(29)
We have thus for every V ∈ [0,α],
	j{0}	ifv ∈]0,α[ ∂θ(v) = ∖ ] - ∞, 0] if v = 0	(30) [[0, +∞[	if v = α.
This leads to	(Z if (v = 0 and Z < 0) (VZ ∈ R) PrOjMU)(Z) = { or (v = α and Z > 0)	(31) 10 otherwise.
ELU
(VZ ∈ R)夕(Q
0	if		Z > 0;
(Z + a)ln (ζ+α)	_Z_ U Z 2，	if	-α<Z < 0
α -号,		if	Z = -α
+∞,		if	Z < -α∙
(32)
This function being differentiable on P(R) =] - α, +∞[, we have for every V ∈] - α, +∞[,
(0	if v > 0
(VZ ∈ R)ProjM0)© = "(v) = kn (υ+α) - v otherwise.	(33)
15
Under review as a conference paper at ICLR 2021
QuadReLU Unlike the previous ones, this function does not seem to have been investigated before.
It can be seen as a surrogate to the hard swish activation function, which is not a proximal activation
function. Let us define
j+∞	if Z < 0
(∀ζ ∈ R)	以Z)= I - ζ2 + 3 √αζ3/2 - αZ if Z ∈ [0,α]	(34)
(ζ2 - αZ + α2	if Z > α.
夕 is a lower-SemicontinUoUs convex function whose subdifferential is
(]—∞, —α]	if U = 0
(∀υ ∈ [0, +∞[) ∂夕(U) = < {—υ + 2√αυ — α} if υ ∈]0, a]	(35)
({υ - α}	ifυ > α.
From the definition of the proximity operator, for every (υ, Z) ∈ R2, We have U = ProxW(Z) if and
only if
Z ∈] - ∞, -α]	ifU = 0
Z ∈ u + ∂中(U)	⇔ Z = 2√aυ — α if U ∈]0, a]
(Z = 2U - α ifU > α.
(0	2	if Z ∈] —2—α]
⇔ U =	(Z毅	if Z ∈] — ɑ, α]
[ζ+2α	if Z>α.
(36)
This shows that ProxW(Z) = (4α)-1(Z + α) ReLU2α(Z + α). In addition, for every U ∈ [0, +∞[, it
follows from Eq. (35) that the projection onto ∂f(U) is
(∀Z ∈ R)Projdf(U) (Z) = [ —υ + 2√αυ - a
if
if
or
if
U = 0 and Z 6 —a
U ∈]0, α]
(U = 0 and Z > —a)
U > α.
(37)
u — α
C S oftmax Activation
Let C denote the closed hypercube [0, 1]N, let V be the vector hyperplane defined as
N
V = {z = (Z(k))i6k6N ∈ RNlX Z(k) = 0},	(38)
k=1
and let A be the affine hyperplane defined as
N
A = {z = (Z(k))16k6N ∈RN ll X Z(k) = 1} =V+u,	(39)
k=1
where u = [1, . . . , 1]> = 1/N ∈ RN. If R is the Softmax activation operator, the convex function f
such that Proxf = R is (Combettes & Pesquet, 2020a, Example 2.23):
(Vz = (Z(k))i6k6N) f (z)= (pi=1 以2) ifZ ∈ C ∩ A	(40)
+∞	otherwise,
where
Z2
(∀Z ∈ [0,+∞[)夕(Z) = Z ln Z — z2	(41)
(with the convention 0 ln 0 = 0). The latter function can be extended on R, say by a quadratic
function on ] — ∞, 0[, yielding a convex function 0 which is differentiable on R. We have then
N
(∀z = (Z(k))16k6N)∈ RN) f (z) = X 0(Z(k)) + ∣C∩Α(z),	(42)
i=1
16
Under review as a conference paper at ICLR 2021
where ιC∩A denotes the indicator function of the intersection of C and A (equal to 0 on this set
and +∞ elsewhere). It then follows from standard subdifferential calculus rules that, for every
y= (υ(k))16k6N ∈RN,
∂f(y) = (0'(υ(kk))i6k6N + NC (y) + NA(y),	(43)
where 00 is the derivative of 0 and ND denotes the normal cone to a nonempty closed convex set D,
which is defined as
ND(y) = {t ∈ RNl (Vz ∈ D) ht | Z - yi 6 0}∙	(44)
Thus NA(y) = NV (y) is the orthogonal space V ⊥ of V .
Let us now assume that y ∈ R(RN) ⊂]0, 1[N. Then, since y is an interior point of C, NC (y) = {0}.
We then deduce from Eq. (43) that
∂f(y) =Q(y)+V⊥,	(45)
where Q(y) = (00(υ(k)))16k6N = (ln υ(k) + 1 - υ(k))
16k6N . It follows that, for every z ∈ RN,
proj∂f(y) (z) = Q(y) + projV⊥(z - Q(y))∙	(46)
By using the expression of the projection projV = Id - projV⊥ onto hyperplane V , we finally obtain
1>(z - Q(y))
Projdf(y)(z) = Q(y) +〈 N'' ⑺ 1∙	(47)
D Experimental Setup
PyTorch is employed to implement our method. We use and extend SNIP and RigL code available
here2, LRR3, GraSP4, SynFlow5, STR6, and FORCE7. In order to manage our experiments we use
Polyaxon8 on a Kubernetes9 cluster and use five computing nodes with eight V100 GPUs each.
Floating point operations per second (FLOPs) is calculated as equal to one multiply-add accumulator
using the code10.
SIS has the following parameters: number of iterations of Algorithm 1, number of iterations of
Algorithm 2, step size parameter γ in Algorithm 1, constraint bound parameter η used to control
the sparsity, and relaxation parameter λn ≡ λ of Algorithm 1. In our experiments, the maximum
numbers of iterations of Algorithm 1 and Algorithm 2 are set to 2000 and 1000, respectively. λ is set
to 1.5 and γ is set to 0.1 for all the SIS experiments. η value depends on the network and dataset.
With few experiments, we search for a good η value that gives suitable sparsity and accuracy.
VGG19 and ResNet50 on CIFAR-10/100. We train VGG19 on CIFAR-10 for 160 epochs with a
batch size of 128, learning rate of 0.1 and weight decay of 5 × 10-4 applied at epochs 81 and 122.
A momentum of 0.9 is used with stochastic gradient descent (SGD). We make use of 1000 images
per training class when using SIS. We fine-tune the identified sparse subnetwork for 10 epochs at a
learning rate of 10-3. For CIFAR-100 we keep the same training hyperparameters as for CIFAR-10.
When applying SIS to the dense network, we use 300 images per class from the training samples.
We fine-tune the identified sparse subnetwork for 40 epochs on the training set with a learning rate
of 10-3. ResNet50 employs the same hyperparameters as VGG19, except the weight decay that we
set to 10-4. When applying SIS to train dense ResNet50, we use the same partial training set and
the same hyperparameters during fine-tuning. In case of VGG19 for CIFAR-10 and CIFAR-100, we
found that η values in range (1∙5, 2) works best for sparsity range (90%, 98%). In case of ResNet50,
η values in range (1, 2) is used.
2https://github.com/google-research/rigl
3https://github.com/lottery-ticket/rewinding-iclr20-public/tree/master/
vision/gpu- src/official
4https://github.com/alecwangcq/GraSP
5https://github.com/ganguli-lab/Synaptic-Flow
6https://github.com/RAIVNLab/STR
7https://github.com/naver/force
8https://github.com/polyaxon/polyaxon
9https://kubernetes.io/
10https://github.com/Lyken17/pytorch-OpCounter
17
Under review as a conference paper at ICLR 2021
ResNet50 on ImageNet We use the weights of ResNet50 pre-trained on ImageNet available at
PyTorch hub11. When applying SIS to the dense pre-trained network we use 20% samples per class
from the training set. We fine-tune the identified sparse subnetwork for 40 epochs on the training
set with a learning rate of 10-4. We use different η values in range (0.7, 1.5) for sparsity range
(60%, 90%). We found that η = 2.3 achieves 96.5% sparsity.
MobileNets on ImageNet We use MobileNetV1 dense pre-trained model from here12 and Mo-
bileNetV2 from PyTorch hub13. In case of MobileNetV3, we replace the hard swish activation
function used in the original paper Howard et al. (2019) with our QuadReLU function (see the last
row of Table 1). We use hyperparameters provided in the original paper to train MobileNetV3. When
applying SIS to the dense pre-trained MobileNets, we use 20% samples per class from the training
set. We fine-tune the identified sparse subnetwork for 30 epochs on the training set with a learning
rate of 10-4. For MobileNets, we search η values in range (0.6, 1.75) for sparsity range (75, 90).
Jasper on LibriSpeech A BxR Jasper network has B blocks, each consisting of R repeating
sub-blocks. Each sub-block consists of 1D-Convolution, Batch Normalization, ReLU activation, and
Dropout. The kernel size of convolutions increases with depth. The network has one convolution
block at the beginning and three at the end. We train a network of 13 encoding blocks and one
decoding block, having 54 1D-Convolution layers on the LibriSpeech dataset. The total number
of parameters in our trained network is 333 million. Jasper network is trained on train-clean-100,
train-clean-360, and train-other-500 splits of the LibriSpeech dataset (Panayotov et al., 2015). The
training configuration can be found here14. We use train-clean-100 when using SIS. We fine-tune
the identified sparse sub-network on the completed training set for ten epochs with a learning rate of
10-4. We use η values in range (0.6, 1.75) for sparsity range (70, 90).
Transformer-XL on WikiText-103 We train the Transformer-XL network (Dai et al., 2019b) on
the base version of WikiText-103 (Merity et al., 2017). We use the training configuration available
here15. We use 10% of the training set articles when using SIS. We use η values in range (0.5, 0.75)
for sparsity range (40, 70).
N-BEATS on M4 We train the interpretable architecture network of N-BEATS on the M4 dataset.
The trained network has six residual blocks. Each block consists of four fully-connected layers and
two linear projection layers. With 24 fully-connected layers, this network has 14 million trainable
parameters. To compare different methods, we only train a single network on a 48-hour window
instead of 180 networks on different timescales. We use the training configuration available here16.
The training set has 50K time-series samples. We use 10K training samples to generate a sparse
sub-network using SIS. We use η values in range (0.75, 1.5) for sparsity range (70, 90).
E Empirical Convergence Analysis
We illustrate the convergence of our method on LeNet-FCN trained on MNIST. LeNet-FCN is a
fully-connected network having four layers with 784-300-1000-300-10 nodes (two 300 nodes and one
1000 node hidden layers). Figure 1 shows the convergence of SIS when applied to dense LeNet-FCN.
We observe that the convergence is smooth and SIS finds a global solution for the first (ReLU
activated) and last (softmax) layer cases. This fact is in agreement with our theoretical claims. SIS
attains a sparsity of 99.21% at an error of 1.86%. The trained dense network has an error of 1.65%.
This result is obtained at η = 2.
11https://pytorch.org/hub/pytorch_vision_resnet/
12https://github.com/RAIVNLab/STR
13https://pytorch.org/hub/pytorch_vision_mobilenet_v2/
14https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/
SpeechRecognition/Jasper/configs/jasper10x5dr_sp_offline_specaugment.toml
15https://github.com/NVIDIA/DeepLearningExamples/blob/master/PyTorch/
LanguageModeling/Transformer-XL/pytorch/wt103_base.yaml
16https://github.com/ElementAI/N-BEATS/blob/master/experiments/m4/
interpretable.gin
18
Under review as a conference paper at ICLR 2021
0.15
0.10
0.05
0.00
Iterations
Iterations
(c)
80
二 70
=60
50
(a)	(b)
3000
2500
2000
11500
υ
1000
500
0
0	350	700	0	350	700
Iterations	Iterations
(d)	(e)
(f)
Figure 1: Convergence of SLIC: Top row shows the first layer (ReLU activated) and bottom row
shows the last layer (softmaxed) in LeNet-FCN. (a) and (d) shows the evolution of the maximum value
cmax of the constraint functions (cj)16j6J, (b) and (e) shows the evolution of kW k1 in Algorithm 1
iterations. (c) and (f) shows kW k1 evolution in Algorithm 2.
ɪθθ-1 * ∙ ∙	∙ ∙	∙---∙ , φ - ⅜---⅜~∙------∙ ^ ɪθθ
-∣—90
-80
(<⅛) ADaJnDOV
Figure 2: Effect of η on LeNet-FCN
The η parameter in our algorithm controls the accuracy tolerance. The higher, the more tolerant we
are on the loss of precision and the sparser the network is. Thus, this parameter also controls the
network sparsity. The choice of this parameter should be the result of an accuracy-sparsity trade-off.
This is illustrated in Figure 2.
19