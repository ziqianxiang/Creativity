Under review as a conference paper at ICLR 2021
CorrAttack: Black-box Adversarial Attack
with Structured Search
Anonymous authors
Paper under double-blind review
Ab stract
We present a new method for score-based adversarial attack, where the attacker
queries the loss-oracle of the target model. Our method employs a parameterized
search space with a structure that captures the relationship of the gradient of the loss
function. We show that searching over the structured space can be approximated
by a time-varying contextual bandits problem, where the attacker takes feature of
the associated arm to make modifications of the input, and receives an immediate
reward as the reduction of the loss function. The time-varying contextual bandits
problem can then be solved by a Bayesian optimization procedure, which can
take advantage of the features of the structured action space. The experiments on
ImageNet and the Google Cloud Vision API demonstrate that the proposed method
achieves the state of the art success rates and query efficiencies for both undefended
and defended models.
1	Introduction
Although deep learning has many applications, it is known that neural networks are vulnerable to
adversarial examples, which are small perturbations of inputs that can fool neural networks into
making wrong predictions (Szegedy et al., 2014). While adversarial noise can easily be found when
the neural models are known (referred to as white-box attack) (Kurakin et al., 2016). However, in real
world scenarios models are often unknown, this situation is referred to as black-box attack.
Some methods (Liu et al., 2016; Papernot et al., 2016) use the transfer-based attack, which generates
adversarial examples on a substitute model and transfer the adversarial noise to the target model.
However, the transferability is limited and its effectiveness relies highly on the similarity between the
networks (Huang & Zhang, 2020). If two networks are very different, transfer-based methods will
have low success rates.
In practice, most computer vision API such as the Google Cloud Vision API allow users to access the
scores or probabilities of the classification results. Therefore, the attacker may query the black-box
model and perform zeroth order optimization to find an adversarial example without the knowledge
of the target model. Due to the availability of scores, this scenario is called score-based attack.
There have been a line of studies on black-box attack which directly estimate the gradient direction
of the underlying model, and apply (stochastic) gradient descent to the input image (Ilyas et al., 2018;
2019; Chen et al., 2017; Huang & Zhang, 2020; Tu et al., 2018; Li et al., 2019). In this paper, we take
another approach and formulate score-based attack as a time-varying contextual bandits problem. At
each state, the attacker may change the adversarial perturbation and get the reward as the reduction of
the loss. And the attacker would receive some features about the arms before making the decision. By
limiting the action space to image blocks, the associated bandits problem exhibits local correlation
structures and the slow varying property suitable for learning. Therefore, we may use the location
and other features of the blocks to estimate the reward for the future selection of the actions.
Using the above insights, we propose a new method called CorrAttack, which utilizes the local
correlation structure and the slow varying property of the underlying bandits problem. CorrAttack
uses Bayesian optimization with Gaussian process regression (Rasmussen, 2003) to model the
correlation and select optimal actions. A forgetting strategy is added to the algorithm so that the
Gaussian process regression can handle the time-varying changes. CorrAttack can effectively find
1
Under review as a conference paper at ICLR 2021
blocks with the largest rewards. The resulting method achieves much lower numbers of average
queries and higher success rates than prior methods with a similar action space (Moon et al., 2019).
It is worth noting that BayesOpt (Ru et al., 2020) and Bayes-Attack (Shukla et al., 2019) also
employ Bayesian optimization for score-based attack. However, their Gaussian process regression
directly models the loss as a function of the image, whose dimension can be more than one thousand.
Therefore, their speed is slow especially for BayesOpt, which uses slow additive kernel. CorrAttack,
on the other hand, searches over a much limited action space and models the reward as a function
of the low dimensional feature. Therefore, the optimization of CorrAttack is more efficient, and the
method is significantly faster than BayesOpt.
We summarize the contributions of this work as follows:
1.	We formulate the score-based adversarial attack as a time-varying contextual bandits, and
show that the reward function has slow varying properties. In our new formulation, the
attacker could take advantage of the features to model the reward of the arms with learning
techniques. Compared to the traditional approach, the use of learning in the proposed
framework greatly improves the efficiency of searching over optimal actions.
2.	We propose a new method, CorrAttack, which uses Bayesian optimization with Gaussian
process regression to learn the reward of each action, by using the feature of the arms.
3.	The experiments show that CorrAttack achieves the state of the art performance on ImageNet
and Google Cloud Vision API for both defended and undefended models.
2	Related Work
There have been a line of works focusing on black-box adversarial attack. Here, we give a brief
review of various existing methods.
Transfer-Based Attack Transfer-based attack assumes the transferability of adversarial examples
across different neural networks. It starts with a substitute model that is in the same domain as the
target model. The adversaries can be easily generated on the white-box substitute model, and be
transferred to attack the target model (Papernot et al., 2016). The approach, however, depends highly
on the similarity of the networks. If two networks are distinct, the success rate of transferred attack
would rapidly decrease (Huang & Zhang, 2020). Besides, we may not access the data for training the
substitute model in practice.
Score-based Attack Many approaches estimate the gradient with the output scores of the target
network. However, the high dimensionality of input images makes naive coordinate-wise search
impossible as it requires millions of queries. ZOO (Chen et al., 2017) is an early work of gradient
estimation, which estimates the gradient of an image block and perform block-wise gradient descent.
NES (Wierstra et al., 2008) and CMA-ES (Hansen, 2016) are two evolution strategies that can perform
query efficient score-based attack Ilyas et al. (2018); Meunier et al. (2019). Instead of the gradient
itself, SignHunter (Al-Dujaili & O’Reilly, 2020a) just estimates the sign of gradient to reduce the
complexity. AutoZOOM (Tu et al., 2018) uses bilinear transformation or autoencoder to reduce
the sampling space and accelerate the optimization process. In the same spirit, data prior can be
used to improve query efficiency (Ilyas et al., 2019). Besides, MetaAttack (Du et al., 2020) takes a
meta learning approach to learn gradient patterns from prior information, which reduces queries for
attacking targeted model.
Many zeroth order optimization methods for black-box attacks rely on gradient estimation. However,
there are some research works using gradient free methods to perform black-box attack. BayesOpt
and Bayes-Attack (Ru et al., 2020; Shukla et al., 2019) employ Bayesian optimization to find the
adversarial examples. They use Gaussian process regression on the embedding and apply bilinear
transformation to resize the embedding to the size of image. Although the bilinear transformation
could alleviate the high dimensionality of images, the dimension of their embeddings are still in
the thousands, which makes Bayesian optimization very ineffective and computationally expensive.
A different method, PARSL poses the attack on '∞ norm as a discrete optimization problem over
{-ε, ε}d (Moon et al., 2019). It uses a Lazy-Greedy algorithm to search over the space {-ε, ε}d to
find an adversarial example. SimBA (Guo et al., 2018) also employs a discrete search space targeted
at `2 norm.
2
Under review as a conference paper at ICLR 2021
Decision-based Attack Decision-based attack assumes the attacker could only get the output label
of the model. Boundary Attack and its variants (Brendel et al., 2017; Chen et al., 2020; Li et al.,
2020) are designed for the setting. However, the information received by the attacker is much smaller
than score-based attack, and it would take many more queries than score-based attack to successfully
attack an image.
3	Preliminaries
A Gaussian process (Rasmussen, 2003) is a prior distribution defined on some bounded set Z, and
is determined by a mean function μ : Z → R and a covariance kernel K : Z ×Z → R. Given n
observations Dn = {(zi, f(zi))}in=1, the prior distribution on f(z1:n) is
f (zl:n) 〜Normal(μ0 (zl:n), K0(z1：n, ZLn)),	(I)
where we use compact notation for functions applied to collections of input points: z1:n indi-
cates the sequence zι,…，Zn, f(zi：n) = [f(zι), ∙ ∙ ∙ ,f(zn)], μo(z±n) = [μ0(z1),…，μo(zn)],
K0 (zi:n, zi:n) = [κ0(z1, Zι), ∙ ∙ ∙ ,K0(Zi,Zn)； ∙∙∙ ； K0(Zn,Zi),…，K0 (Zn,Zn )；].
Now we wish to infer the value of f(Z) at some new point Z, the posterior process f(Z)|Dn is also a
Gaussian process (GP) with mean μn and covariance σnn:
f(z)∣Dn 〜NOrmal(μn(z),σn(z)),	(2)
μn (Z) = κ0 (z, z1:n )K0(z1:n, z1:n ) (f (ZLn) - μ0(ZLn)) + μ0(Z),
σnn (z) = Ko(z,z) - Ko(Z, Zl：n)K0 (Zl：n,Zl：n ) - 1K0 (/上冗,z).
As a optimization method to maximize a function f , Bayesian optimization models the function to
make decisions about where to evaluate the next point Z. Assuming we already obtained observations
Dt-1 = {(Zi, f(Zi))}it=-11, to determine the next point Zt for evaluation, we first use the posterior GP
to define an acquisition function Wt : Z → R, which models the utility of evaluating f (Z) for any
Z ∈ Z. We then evaluate f(Zt) with
Zt = argmaxWt(Z).	(3)
Z
In this work, we use the expected improvement (EI) acquisition function (Mockus et al., 1978)
Wt(Z) = Pσn (Z)(Y(Z)φ(Y(Z)) + φ(Y(Z)))	with	Y(Z) = μn(Z) -f^best),	(4)
σn2 (Z)
which measures the expected improvement over the current best value Zbest = arg maxzi f(Zi)
according to the posterior GP. Here Φ(∙) and φ(∙) are the cdf and pdf of N(0, I) respectively.
4	Score-based Black-box Attack
Suppose a classifier F(x) has input x and label y. An un-targeted adversarial example xadv satisfies:
arg max F (xadv)j 6= y	and kxadv - xkp ≤ ε,	(5)
j∈{1,…C}
where C is the number of classes. While an adversarial example for targeted attack means the
maximum position of F(x) should be the targeted class q: argmaxj∈{i,…c} F(Xadv)j = q. In order
to find Xadv, we may optimize a surrogate loss function '(x, y) (e.g hinge loss).
In this work, we consider adversarial attack as a time-varying contextual bandits problem. At each
time t, we observe a state Xt which is a modification of the original input X0 . Before taking arm
at ∈ A ⊂ Rd, we could observe the feature Z of arms. And at would modify state Xt to Xt+1
according to
Xt+1 = arg min '(∏Bp(x,ε) (s) ,y)	(6)
s∈{xt+at ,xt}
with reward function r(xt, at) = '(xt+ι,y) - '(xt, y) and the checking step tries to remove negative
reward. In this frame, we would like to estimate the reward r(Xt, at) with feature Zt using learning,
and then pick at to maximize the reward. Observe that
r(xt,at) ≈ Vχ'(xt,y)>(xt+ι - Xt),	(7)
3
Under review as a conference paper at ICLR 2021
where the gradient Yxt '(xt, y) is unknown. It follows from the formulation that We may rewrite
r(xt, at) as a function
r(xt, at) ≈ f(xt, xt+1 - xt).
Since in general, we make small steps from one iteration to the next iteration, δt (at) = xt+1 - xt is
small. We may approximate the reward with fixed gradient locally with
., _ , ~ ,
f (xt,δt) = ft(at),
We may consider the learning of reward as a time-varying contextual bandits problem with reward
function ft(at) for arm at at time t. Since xt+1 - xt is small, this time-varying bandits has slow-
varying property: the function ft changes slowly from time t to time t + 1.
In the proposed framework, our goal is to learn the time-varying bandits reward ft(at) with feature
zt . We use Gaussian process regression to model the reward function using recent historic data since
the reward function is slow-varying, and describe the details in the subsequent sections.
We note that the most general action space contains all at ∈ Rd , where d is the number of image
pixels. However, it is impossible to explore the arms in such a large space. In this work, we choose a
specific class of actions A = {ai}in=1, n is the image blocks of different sizes. It covers the space of
the adversarial perturbations while maintaining good complexity. We also find the location and the
PCA of the blocks a good component of the feature z associated with the arm. Besides, modifying a
block only affects the state locally. Therefore the reward function remains similar after state changes.
4.1	Structured Search with Gaussian Process Regression and Bayesian
Optimization
Define the block size as b, We divide the image into several blocks E = {e000, e001, ∙∙∙ , ehwc},
where the block is b × b square of pixels and (h, w, c) = (height/b, width/b, channel). Each block
eijk is associated with the feature zeijk such as the location of the block.
Suppose we have time-varying bandits with state xt and unknown reward function ft at time t. By
taking the action aeijk , we change the individual block eijk of xt and get xt+1 with reward ft(aeijk ).
We consider two ways of taking action aeijk on block eijk : CorrAttackDiff and CorrAttackFlip.
Finite Difference CorrAttackDiff： For action。亡白飞,the attacker will query '(xt + ηejk,y) and
'(xt 一 ηej ,y), and choose
aeijk =	arg min	'(xt + s,y)∙	(8)
s∈{ηeijk,-ηeijk}
The action space A = {aeijk |eijk ∈ E}.
In our framework, the bandits problem can also be regarded as learning the conditional gradient over
actions. That is, when η is small, we try to choose action at with
at = arg mine>jkγxt '(χt,y)	(9)
eijk ∈E
which is the conditional gradient over the set of blocks.
Discrete Approximation CorrAttackFHp: In general, adversarial attack with '∞ budget can be
formulated as constrained optimization with kxadv 一 xk∞ ≤ . However, PARSI (Moon et al., 2019)
limits the space to {-ε, +ε}d, which leads to better performance for black-box attack (Moon et al.,
2019). The continuous optimization problem becomes a discrete optimization problems as follows:
maximize '(xadv ,y)	=⇒ maximize '(xadv ,y)	(10)
subject to kxadv - xk∞ ≤ subject to xadv - x ∈ {, -}d.
Following PARSL We consider two stages to perform structured search. When flipping ε to -ε,
aeijk changes the block to -ε and A = {-2εejk匕jk ∈ E}. When changing -ε to ε, A =
{2εeijk |eijk ∈ E} instead.
Gaussian Process (GP) Regression: We model the difference function
gt(at) = '(πBp(x,ε) (Xt + at), y) — '(Xt, y)	(II)
4
Under review as a conference paper at ICLR 2021
instead of the reward function ft (at) ≥ 0, as the difference function could be negative, providing
more information about the negative arms in A. We would collect historic actions with feature and
difference {zk, gk(ak))}tk=1 and learn the difference to make choices at a later stage. At each time t,
we use the Gaussian process regression to model the correlation between the features zeijk and use
Bayesian optimization to select the next action. More specifically, the same as eq. (2), we let
gt5ijk )|Dt 〜NOrmaKμt (Zeijk ),σ2(Zeijk )),	(12)
where Dt = {zk, gk ® ))}tk=t-τ is the difference of evaluated blocks et—「t with feature 豆已1:七 and
τ is a parameter to forget old samples. Then we use EI acquisition function to pick up the next action
at+1 in A. More specifically, the same as eq. (4), we let
at+1 = arg max( σt2(zeijk)(γ(zeijk)Φ(γ(zeijk)) + φ(γ(zeijk))))	(13)
As the difference function gt is varying, we take two strategies in Algorithm 2 to update the previous
samples to make sure GP regression learns the current difference function well. The first strategy is to
remove old samples in Dt. Even if the bandits are slowly varying, the difference function will change
significantly after a significant number of rounds. Therefore, we need to forget samples before t - τ .
The second strategy is to remove samples near the last block eitjtkt in Dt. As we discuss later, the
difference function may change significantly in a local region near the last selected block. Therefore
previous samples in this local region will be inaccurate. The resulting algorithm for CorrAttack is
shown in Algorithm 1, which mainly follows standard procedure of Bayesian optimization.
Algorithm 1 CorrAttack
Require: Loss function '(∙, ∙), Input xo and its label y, Action space A = {aejk ∣eijk ∈ E}, Parameter c, T,
α
1:	Build set D0 = {(zeipjpkp,g0(aeipjpkp))}pm=1 using latin hypercube sampling from A
2:	repeat
3:	Fit the parameter of Normal(μt (Zeijk), σ2 (Zeijk)) on Dt according to Equation (12)
4:	Calculate acquisition function g t (Zeijk) and according to Equation (13)
5:	Select aeitjtkt = arg maxA gt (Zeijk) according to Equation (13)
6:	xt+1 = argmins∈{χt+aeijk, ,χt} '(πBp(χ,ε) (s) ,y)
7:	Update sample set Dt with Algorithm 2
Dt+1 = UPDATESAMPLES(Dt, xt, xt+1, eitjtkt, gt+1 (aeitjtkt), τ, α)
8:	until maxA 夕t(zejk) < C
9:	return xT ;
Algorithm 2 Update Samples
Require: Sample set Dt, State xt, xt+1, Block eitjtkt, Difference gt+1(aei j k ), Paramter τ, α
1:	if xt+1 6= xt then
2:	Dt+1	=	Dt	\ {(zejk ,g)	∈Dt∣∣i -	it|	+	|j	- jt| ≤ α}
3:	else
4:	Dt+1 =Dt ∪ {(Zeitjtkt,gt+1(aeitjtkt))}
5:	end if
6:	Remove the earliest sample from D if the cardinality |D| > τ
7:	return Dt+1
4.2	Features and Slow Varying Property
Features of Contextual Bandits: We use a four dimensional vector as the feature zeijk:
zeijk = (i, j, k, pca)	(14)
where i, j, k is the location of the block. And pca is the first component of PCA decomposition of
[xo(eo00), x0(e001),∙…xo(ehwc)]. xo(ejk) means the block of natural image at the given position.
The reward function depends on the gradient in Equation (7). It has been shown that the gradient
Vχ'(χ, y) has local dependencies (Ilyas et al., 2019). Suppose two coordinates e^k and ep are
close, then Vχ'(χ, y)ijk ≈ Vχ'(χ, y)ipq. We consider the finite difference of the block ej
∆t(eijk) = ' (Xt + ηeijk, y) - ' (xt - ηej, y) ≈ 2ηe>jkV χt' (xt, y)	(15)
5
Under review as a conference paper at ICLR 2021
where η is a small step size. When η is small, the reward can be approximated by the average of the
gradients around a small region, which also has local dependencies. In fact, the local structure of the
reward will also be persevered when the block size and η is large. Figure 1 shows one example of
the finite difference ∆t (eijk) obtained on ImageNet dataset with ResNet50. This shows blocks with
closer locations are more likely to have similar reward. Therefore, we add the location of the block as
the feature so that it uses historic data to find the arm with the largest reward.
Figure 1: Finite difference of the perturbation for three channels on one image from ImageNet with
ResNet50. h = w = 28, b = 8 and η = 0.05. Lighter block means larger finite difference.
In addition to the location of the difference, we may add other features. The block of the image itself
forms a strong feature for the regression, but the dimension of the block is too high for GP regression.
Therefore, we use PCA to lower the dimension and add the first component into the feature vector.
Slow Varying Property In addition to the local dependencies of finite difference, the difference
would also be slow varying if we just change a small region of xt. Let xt+1 = xt - ηeitjtkt , Figure 2
shows the difference of ∆t(eijk) and ∆t+1(eijk), which is centralized in a small region near eitjtkt.
Reward function is based on the finite difference, which also has the slow varying property. It could
be explained by the local property of convolution. When η is small, the finite difference can be
approximated with gradient and the local Hessian:
∆t+1(eijk) - ∆t(eijk) ≈ η eijk ^xt '(xt, y)eitjtkt	(16)
The difference is much smaller than ∆t (eijk). Today’s neural networks are built with stacks of
convolutions and non-linear operations. Since these operations are localized in a small region, the
Hessian of a neural network is also localized and the reward function only changes near eitjtkt .
Figure 2: Difference of finite difference on each block after changing block e15,18,1 of Figure 1,
which is the lightest pixel in the picture. Darker blocks imply smaller difference in finite difference,
which is almost zero in the majority of the image except the part near the changed block.
4.3	Hierarchical Bayesian Optimization Search
Recent black-box approaches (Chen et al., 2017; Moon et al., 2019) exploit the hierarchical image
structure for query efficiency. Following these approaches, we take a hierarchical approach and
perform the accelerated local search in Algorithm 1 from a coarse grid (large blocks) to a fine
grid (smaller blocks). The algorithm for hierarchical attack iteratively performs Algorithm 1 at
one block size, and then divides the blocks into smaller sizes. At each block size, we build a
Gaussian process to model the difference function, and perform structured search with the blocks
until maxA 中心已仃卜)< c. When dividing the blocks into smaller sizes, We will build a new block
set E with action aeijk and new feature zeijk, but keep the xt in last block size as x0 in new block
size. Define the stage as S = {0,1,…，s} and initial block size as b. The block at stage S is 2bS X 今
square of pixels and (h, w, C) = (2s * height/b, 2s * width/b, channel).
The overall hierarchical accelerated local search algorithm is shown in Appendix A. It is important to
note that most of the attacks terminate in the early stages and rarely need to run on fine scales.
6
Under review as a conference paper at ICLR 2021
Table 1: Success rate and average queries of un-targeted attack on 1000 samPles of ImageNet. ε = 0.05. Since BayesOPt and Bayes-Attack needs thousands of hours to run all samPles, we only test 20 samPles, which are marked as *, the comPlexity and running time could be referred to C.6.						
Attack	VGG16		Resnet50		Densenet121	
	Success Queries		Success	Queries	Success	Queries
ZOO	81.93%	2003	63.68%	1795	76.73%	1864
NES	99.72%	700	99.19%	1178	99.72%	1074
NAttack	100%	293	99.73%	401	100%	375
Bandits	94.75%	389	96.92%	433	98.09%	635
PARSI	100%	365	99.73%	432	100%	387
Square Attack	100%	79	100%	112	100%	86
SignHunter	100%	104	100%	145	100%	118
CorrAttackDiff	100%	389	99.86%	419	99.86%	334
CorrAttackFliP	100%	130	100%	150	100%	113
BayesOPt*	100%	182	100%	214	100%	223
Bayes-Attack*	100%	244	100%	254	100%	213
CorrAttackFliP*	100%	110	100%	96	100%	87
Table 2: Success rate and average queries of targeted attack on ImageNet. ε = 0.05 and query limit is
10000. As BayesOPt and Bayes-Attack run very slow, We do not include them for the targeted attack.
Attack	VGG16		Resnet50		Densenet121	
	Success	Queries	Success	Queries	Success	Queries
ZOO	1.1%	2884	08%^^	3018	1.1%	3309
NES	80.82%	4582	52.73%	5762	64.21%	5427
NAttack	91.86%	4045	89.05%	3799	91.97%	4389
Bandits	50.62%	5379	40.18%	5672	43.53%	5434
PARSI	76.28%	3229	64.88%	3403	75.09%	3246
Square Attack	96.69%	2060	89.52%	2807	95.38%	2280
SignHunter	93.52%	2999	83.71%	3905	90.75%	3632
CorrAttackDiff	88.41%	3826	81.84%	4064	91.29%	3513
CorrAttackFliP	98.07%	2191	96.39%	2531	99.41%	2019
5 Experiments
We evaluated the number of queries versus the success rates of CorrAttack on both undefended and
defended network on ImageNet (Russakovsky et al., 2015). Moreover, we attacked Google Cloud
Vision API to show that CorrAttack can generalize to a true black-box model.
We used the common hinge loss ProPosed in the CW attack (Carlini & Wagner, 2017). We comPared
two versions of CorrAttack : CorrAttackDiff and CorrAttackFliP, to ZOO (Chen et al., 2017), NES
(Ilyas et al., 2018), NAttack (Li et al., 2019), Bandits (Ilyas et al., 2019), PARSI (Moon et al., 2019),
Square Attack (Andriushchenko et al., 2020), SignHunter(Al-Dujaili & O’Reilly, 2020b), BayesOPt
(Ru et al., 2020) and Bayes-Attack (Shukla et al., 2019). We only test adversarial attack on '∞ norm.
The details of the Gaussian Processes regression and the hyPerParameters of CorrAttack are given
in the APPendix B. We shall mention that CorrAttack is not sensitive to the hyPerParameters. The
hyPerParameters of other methods follow those suggested by the original PaPers.
5.1	Undefended Network
We randomly select 1000 images from the validation set of ImageNet and only attack correctly
classified images. The query efficiency of CorrAttack is tested on VGG16 (Simonyan & Zisserman,
2014), Resnet50 (He et al., 2016) and Densenet121 (Huang et al., 2017), which are the most commonly
used network structures. We set ε = 0.05 and the query limit to be 10000 excePt for BayesOPt and
Bayes-Attack. For targeted attacks, we randomly choose the target class for each image and the target
classes are maintained the same for the evaluation of different algorithms. The results are shown in
Table 1 and 2. CorrAttackFliP outPerforms other methods by a large margin.
7
Under review as a conference paper at ICLR 2021
Table 3: Success rate and average queries of un-targeted attack on defended model. Since BayesOpt
and Bayes-Attack take thousands of hours to run, we only tested on 10 samples from ImageNet with
ε = 0.05 and 1000 query limit, which are marked as *.
Method	ZOO	NES	NAttack	Bandits	PARSI
Success	-28.57%	24.13%	74.38%	55.82%	73.40%
Queries	1954	3740	1078	1873	1529
Method	SignHunter	Square Attack	CorrAttaCkDiff		CorrAttackFlip
Success	-68.97%	73.89%	64.86%	79.15%
Queries	1392	1086	1599	1036
Method	BayesAttack*		BayesOpt*	CorrAttackFlip*	
Success	50.00%		50.00%	60.00%
Queries	129		406	206
Table 4: Success rate and average queries of un-targeted attack on Google Cloud Vision API. ε = 0.05
Method	NAttack	BayesOpt	PARSI	CorrAttackFlip
Success	70.00%	30.00%	70.00%	80.00%
Queries	142	129	235	155
As BayesOpt and Bayes-Attack takes tens of thousands of hours to attack 1000 images, we compare
them with CorrAttackFlip only on 20 images and un-targeted attack. The query limit is also reduced
to 1000 as the time for BayesOpt and Bayes-Attack quickly increases as more samples add into the
Gaussian distribution. The time comparison between three models is shown in Appendix C.6.
Optimality of Bayesian optimization Appendix C.1 shows the rank the actions found by CorrAttack.
The attacker could find the action with large reward quickly.
Varying ε We also test the algorithms at different budget of adversarial perturbations at ε = 0.04
and ε = 0.06 on Resnet50. As it is shown in Appendix C.2, CorrAttack shows a consistently better
performance at different ε.
Ablation study on random choices Appendix C.3 shows the ablation study of random version of
CorrAttackDiff and CorrAttackFlip. In both cases, Bayesian optimization helps to gain better query
efficiency.
Ablation study on hierarchical attack We perform ablation study on the hierarchical attack and the
result is shown in Appendix C.4. Hierarchical structure accelerates the CorrAttack and eliminates the
sensitivity of choosing initial block size.
Ablation study on features Appendix C.5 demonstrates how the feature of the contextual bandits
affects the performance of attack. PCA would help to improve the efficiency of attack.
5.2	Defended Network
To evaluate the effectiveness of CorrAttack on adversarially defended networks, we tested our method
on one of the SOTA robust model (Xie et al., 2018) on ImageNet. The weight is downloaded from
Github1. "ResneXt DenoiseAll" is chosen as the target model as it achieves the best performance.
We set ε = 0.05 and the maximum number of queries is 10000. As BayesOpt runs very slowly, the
attack is also performed on 10 images and the query limit is 1000. The result is shown in Table 3.
CorrAttackFlip still outperforms other methods.
5.3	Google Cloud Vision API
We also attacked Google Cloud Vision API, a real world black-box model for classification. The
target is to remove the top-1 label out of the classification output. We choose 10 images for the
ImageNet dataset and set the query limit to be 500 due to high cost to use the API. We compare
CorrAttackFlip with NAttack, BayesOpt and PARSI. The result is shown in Table 4. We also show
one example of the classification output in Appendix C.9
1https://github.com/facebookresearch/ImageNet-Adversarial-Training
8
Under review as a conference paper at ICLR 2021
6 Conclusion and Future Work
We formulate the score-based adversarial attack as a time-varying contextual bandits and propose a
new method CorrAttack. By performing structured search on the blocks of the image, the bandits
has the slow varying property. CorrAttack takes advantage of the the features of the arm, and uses
Bayesian optimization with Gaussian process regression to learn the reward function. The experiment
shows that CorrAttack can quickly find the action with large reward and CorrAttack achieves superior
query efficiency and success rate on ImageNet and Google Cloud Vision API.
We only include basic features for learning the bandits. Other features like embedding from the
transfer-based attack Huang & Zhang (2020) may be taken into account in the future work. While our
work only focuses on adversarial attack on '∞ norm, the same contextual bandits formulation could
be generalized to other `p norm to improve query efficiency. Besides, defense against CorrAttack
may be achieved with adversarial training on CorrAttack , but it may not be able to defend other
attacks in the meantime.
References
Abdullah Al-Dujaili and Una-May O’Reilly. Sign bits are all you need for black-box attacks. In
International Conference on Learning Representations, 2020a. URL https://openreview.
net/forum?id=SygW0TEFwH.
Abdullah Al-Dujaili and Una-May O’Reilly. Sign bits are all you need for black-box attacks. In
International Conference on Learning Representations, 2020b. URL https://openreview.
net/forum?id=SygW0TEFwH.
Maksym Andriushchenko, Francesco Croce, Nicolas Flammarion, and Matthias Hein. Square attack:
a query-efficient black-box adversarial attack via random search. 2020.
Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable
attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248, 2017.
Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017
IEEE Symposium on Security and Privacy (SP),pp. 39-57. IEEE, 2017.
Jianbo Chen, Michael I Jordan, and Martin J Wainwright. Hopskipjumpattack: A query-efficient
decision-based attack. In 2020 IEEE Symposium on Security and Privacy (SP), pp. 1277-1294.
IEEE, 2020.
Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. Zoo: Zeroth order
optimization based black-box attacks to deep neural networks without training substitute models.
In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pp. 15-26. ACM,
2017.
Kun Dong, David Eriksson, Hannes Nickisch, David Bindel, and Andrew G Wilson. Scalable log
determinants for gaussian process kernel learning. In Advances in Neural Information Processing
Systems, pp. 6327-6337, 2017.
Jiawei Du, Hu Zhang, Joey Tianyi Zhou, Yi Yang, and Jiashi Feng. Query-efficient meta attack to
deep neural networks. In International Conference on Learning Representations, 2020. URL
https://openreview.net/forum?id=Skxd6gSYDS.
Jacob Gardner, Geoff Pleiss, Kilian Q Weinberger, David Bindel, and Andrew G Wilson. Gpytorch:
Blackbox matrix-matrix gaussian process inference with gpu acceleration. In Advances in Neural
Information Processing Systems, pp. 7576-7586, 2018.
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens van der Maaten. Countering adversarial
images using input transformations. In International Conference on Learning Representations,
2018. URL https://openreview.net/forum?id=SyJ7ClWCb.
Nikolaus Hansen. The cma evolution strategy: A tutorial. arXiv preprint arXiv:1604.00772, 2016.
9
Under review as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Zhichao Huang and Tong Zhang. Black-box adversarial attack with transferable model-based
embedding. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=SJxhNTNYwB.
Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with
limited queries and information. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 2137-2146, Stockholmsmassan, Stockholm Sweden, 10-15 JUl 2018. PMLR.
Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial
attacks with bandits and priors. In International Conference on Learning Representations, 2019.
URL https://openreview.net/forum?id=BkMiWhR5K7.
Alexey KUrakin, Ian Goodfellow, and Samy Bengio. Adversarial machine learning at scale. arXiv
preprint arXiv:1611.01236, 2016.
HUichen Li, XiaojUn XU, XiaolU Zhang, ShUang Yang, and Bo Li. Qeba: QUery-efficient boUndary-
based blackbox attack. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pp. 1221-1230, 2020.
Yandong Li, LijUn Li, Liqiang Wang, Tong Zhang, and Boqing Gong. Nattack: Learning the
distribUtions of adversarial examples for an improved black-box attack on deep neUral networks.
arXiv preprint arXiv:1905.00441, 2019.
Yanpei LiU, XinyUn Chen, Chang LiU, and Dawn Song. Delving into transferable adversarial examples
and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.
LaUrent MeUnier, Jamal Atif, and Olivier TeytaUd. Yet another bUt more efficient black-box adversarial
attack: tiling and evolUtion strategies. arXiv preprint arXiv:1910.02244, 2019.
Jonas MockUs, VytaUtas Tiesis, and Antanas Zilinskas. The application of bayesian methods for
seeking the extremUm. Towards global optimization, 2(117-129):2, 1978.
SeUngyong Moon, Gaon An, and HyUn Oh Song. ParsimonioUs black-box adversarial attacks via
efficient combinatorial optimization. arXiv preprint arXiv:1905.06635, 2019.
Nicolas Papernot, Patrick McDaniel, and Ian Goodfellow. Transferability in machine learning: from
phenomena to black-box attacks Using adversarial samples. arXiv preprint arXiv:1605.07277,
2016.
Carl Edward RasmUssen. GaUssian processes in machine learning. In Summer School on Machine
Learning, pp. 63-71. Springer, 2003.
Binxin RU, Adam Cobb, Arno Blaas, and Yarin Gal. Bayesopt adversarial attack. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=Hkem-lrtvH.
Olga RUssakovsky, Jia Deng, Hao SU, Jonathan KraUse, Sanjeev Satheesh, Sean Ma, Zhiheng HUang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visUal recognition
challenge. International journal of computer vision, 115(3):211-252, 2015.
Satya Narayan ShUkla, Anit KUmar SahU, Devin Willmott, and J Zico Kolter. Black-box adversarial
attacks with bayesian optimization. arXiv preprint arXiv:1909.13857, 2019.
Karen Simonyan and Andrew Zisserman. Very deep convolUtional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
10
Under review as a conference paper at ICLR 2021
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations, 2014. URL http://arxiv.org/abs/1312.6199.
Chun-Chen Tu, Paishun Ting, Pin-Yu Chen, Sijia Liu, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh, and
Shin-Ming Cheng. Autozoom: Autoencoder-based zeroth order optimization method for attacking
black-box neural networks. arXiv preprint arXiv:1805.11770, 2018.
Daan Wierstra, Tom SchaUL Jan Peters, and Jurgen Schmidhuber. Natural evolution strategies.
2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational
Intelligence),pp. 3381-3387, 2008.
Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan Yuille, and Kaiming He. Feature denoising for
improving adversarial robustness. arXiv preprint arXiv:1812.03411, 2018.
11
Under review as a conference paper at ICLR 2021
A Algorithm
Algorithm 3 Split Block
Require: Set of blocks E, Block size b, E0 = 0
1:	for each block e ∈ E do
2:	Split the block e into 4 blocks {e1, e2, e3, e4} with size b/2
3:	E0 <— E0 ∪ {e1,e2,e3,e4}
4:	end for
5:	return E0 ;
Algorithm 4 Hierarchical CorrAttackDiff
Require: Loss function '(∙, ∙), Input image X and its label y, Initial Block size b, Set of blocks E containing all
blocks of the image, Threshold c, τ, α, Step size η, Adversarial budget ε
1:	x0 = x
2:	repeat
3:	Choose A = {aeijk |eijk ∈ E } with Equation (8)
4:	Run CorrAttack on current block size
X = CORRATTACK ('(∙, ∙),x,y,A,c,τ, α)
5:	if b > 1 then
6:	Split the blocks into finer blocks using Algorithm 3
E = SPLITBLOCK(E, b)
7:	b — b/2
8:	end if
9:	until ` converges
10:	return XK ;
Algorithm 5 Hierarchical CorrAttackFlip
Require: Loss function '(∙, ∙), Input image X and its label y, Block size b, Set of blocks E containing all blocks
of the image, Threshold c, τ, α, Adversarial budget ε
1:	X0 = X
2:	for eijk ∈ E do
3:	Randomly draw v from {-ε, ε}
4:	X0 [eijk] = v + X0 [eijk]
5:	end for
6:	repeat
7:	An = {2εeijk ∈ E|ei>jk (Xk - X) < 0}
8:	Run CorrAttack flipping -ε to ε
Xk = CorrAttack ('(•, ∙),xk,y,An, c,τ, a)
9:	Ap = {-2εeijk ∈ E|e>k (Xk - χ) > 0}
10:	Run CorrAttack flipping ε to -ε
Xk+1 = CorrAttack ('(∙, ∙),Xk,y,Ap,c,τ, α)
11:	if b > 1 then
12:	Split the blocks into finer blocks using Algorithm 3
E = SPLITBLOCK(E, b)
13:	b — b/2
14:	end if
15:	until ` converges
16:	return XK ;
B Details of Experiment Setting
We use the hinge loss for all the experiments. For un-targeted attacks,
'untarget(x, y) = max F F(x)y - max F(x)j, -ω
j6=y
and for targeted attacks,
'target(x, y) = max m max F (xj - F (x)t, -ω 卜
(17)
(18)
12
Under review as a conference paper at ICLR 2021
Here F represents the logits of the network outputs, t is the target class, and ω denotes the margin.
The image will be projected into the ε-ball. Besides, the value of the image will be clipped to range
[0, 1].
B.1	Gaussian Process Regression and B yaesian Optimization
We further provide details on both the computational scaling and modeling setup for the GP regression.
To address computational issues, we use GPyTorch (Gardner et al., 2018) for scalable GP regression.
GPyTorch follows (Dong et al., 2017) to solve linear systems using the conjugate gradient (CG)
method and approximates the log-determinant via the Lanczos process. Without GPyTorch, running
BO with a GP regression for more than a few thousand evaluations would be infeasible as classical
approaches to GP regression scale cubically in the number of data points.
On the modeling side, the GP is parameterized using a Matern-5/2 kernel with ARD and a constant
mean function for all experiments. The GP hyperparameters are fitted before proposing a new
batch by optimizing the log-marginal likelihood. The domain is rescaled to [0, 1]d and the function
values are standardized before fitting the GP regression. We use a Matern-5/2 kernel with ARD for
CorrAttack and use the following bounds for the hyperparameters: (length scale) λi ∈ [0.005, 2.0],
(output scale) λ0i ∈ [0.05, 20.0], (noise variance) σ2 ∈ [0.0005, 0.1].
B.2	Hyperparameters
For CorrAttack in Algorithm 4 and Algorithm 5, we set the initial block size b to be 32 and the step
size η for CorrAttackDiff is 0.03. In Algorithm 1, we use the initial sampling ratio m = 0.03n at
the start point for Gaussian process regression, the threshold c = 10-4 to decide when to stop the
search of current block size. In Algorithm 2, the threshold is different for different block size. For
CorrAttackFlip, α = 1, 1, 2, 2, 3 for block size 32, 16, 8, 4, 2 and for CorrAttackDiff, α = 0, 0, 1, 1, 2
for block size 32, 16, 8, 4, 2. We set τ = 3m = 0.09n to remove the earliest samples from D once
|D| > α. The Adam optimizer is used to optimize the mean μ and covariance K of Gaussian process,
where the iteration is 1 and the learning rate is 0.1.
For PARSI, the block size is set to 32 as CorrAttack , other hyperparameters are the same as the
original paper.
For Bandits, Bayes-Attack and BayesOpt, the hyperparameters are the same as the original paper.
We optimize the hyperparameters for ZOO, NES. For un-targeted attack on NES, we set the sample
size to be 50, learning rate to be 0.1. For targeted attack on NES, the sample size is also 50 and the
learning rate is 0.05. The learning is decay by 50% if the loss doesn’t decrease for 20 iterations.
For NAttack, we set the hyperparameters the same as NES and add momentum and learning rate
decay, which are not mentioned in the original paper.
For ZOO, we set the learning rate to 1.0 and sample size to be 50. Other setting follows the original
paper.
C Additional Experiments
C.1 Optimality of Bayesian Optimization
Figure 3 shows the reward function that the Bayesian optimization could find in the action set.
CorrAttack could find the action with high reward within just a few queries. It shows that the
Gaussian process regression could model the correlation of the reward function and the Bayesian
optimization could use it to optimize the time-varying contextual bandits.
C.2 Varying the Adversarial Budget
We test CorrAttack on different adversarial budget on ImageNet for both un-targeted attack and
targeted attack. Table 5 and Table 6 show the success rate and average queries for ε = 0.04, 0.05, 0.06.
CorrAttackFlip achieves the best performance among all methods.
13
Under review as a conference paper at ICLR 2021
-∙- Block size 32
-∙- Block size 16
-∙- Block size 8
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
0.025 0.050 0.075 0.100 0.125 0.150 0.175 0.200
Queries
Queries
Figure 3: The rank of the reward function that the Bayesian optimization could find in the action set
for different block size. The rank and query are normalized by the cardinality of the action set.
Table 5: Success rate and average queries of un-targeted attack on different ε. Query limit is 10000
Attack	ε=	0.04	ε =	0.05	ε=	0.06
	Success	Queries	Success	Queries	Success	Queries
ZOO	63.28%	1915	63.68%	1794	64.88%	1507
NES	99.06%	1230	99.19%	1178	99.19%	1160
NAttack	99.73%	529	99.73%	401	99.73%	369
Bandits	95.86%	898	96.92%	694	97.06%	567
PARSI	99.73%	508	99.73%	432	100%	387
CorrAttackDiff	99.86%	479	99.86%	419	99.78%	373
CorrAttackFlip	100%	203	100%	150	100%	107
Table 6: Success rate and average queries of targeted attack on different ε. Query limit is 10000
Attack	ε=	0.04	ε =	0.05	ε=	0.06
	Success	Queries	Success	Queries	Success	Queries
ZOO	0.80%	3514	0.80%	3018	0.93%	1938
NES	49.13%	5901	52.73%	5762	56.48%	5884
NAttack	78.24%	5019	89.05%	3799	90.25%	4321
Bandits	31.78%	5721	40.19%	5672	43.39%	5609
PARSI	57.00%	3599	64.88%	3403	68.75%	3250
CorrAttackDiff	78.70%	4472	81.84%	4064	85.31%	3837
CorrAttackFlip	93.44%	2689	96.39%	2531	97.50%	2194
C.3 Ablation S tudy on Random Choices
Table 7 and Table 8 show the ablation study on the strategy to choose action xt+1 in the line 6 of
Algorithm 1. The process of Bayesian optimization helps to accelerate the optimization. As targeted
attack is more complicated and requires larger number of queries, CorrAttack has more advantage in
this scenario.
C.4 Ablation S tudy on Hierarchical Attack
We perform un-targeted attack on Resnet50 as shown in Table 10. Hierarchical attack lowers the
average queries and improves the query efficiency. Besides, hierarchical attack avoids the problem
of choosing block size. As shown in Table 10, block size for non-hierarchical is essential for the
performance.
C.5 Ablation S tudy on Features
Table 11 shows the success rate and average queries for CorrAttackwith different features. We
perform ablation study on the features of the contextual bandits. One contains just the location of the
14
Under review as a conference paper at ICLR 2021
Table 7: Ablation study on random choices with success rate and average queries of un-targeted
attack on ImageNet. ε = 0.05 and query limit is 10000
Attack	VGG16		Resnet50		Densenet121	
	Success	Queries	Success	Queries	Success	Queries
CorrAttackDiffRandom	100%	-^456^^	99.86%	491	100%	375
CorrAttackDiffBayes	100%	389	99.86%	419	100%	334
CorrAttackFlip Random	100%	143~~	100%	176	100%	132
CorrAttackFlipBayes	100%	130	100%	150	100%	113
Table 8: Ablation study on random choices with success rate and average queries of targeted attack
on ImageNet. ε = 0.05 and query limit is 10000
Attack	VGG16	Resnet50	Densenet121 Success Queries Success Queries Success Queries
CorrAttackDiffRandom CorrAttackDiffBayes	-^83.72%	4388	74.76%	4644	85.30%	4∏3- 88.41%	3826	81.84%	4064	91.29%	3513
CorrAttackFlipRandom CorrAttackFlip Bayes	-^96.42%	2545	92.92%	3066	96.87%	2556- 98.07%	2191	96.39%	2531	99.41%	2019
Table 9: Ablation study on random choices with success rate and average queries of un-targeted
attack on defended model ImageNet. ε = 0.05 and query limit is 10000
Method	CorrAttackDiffRandom	COrrAttaCkDifBayes	CorrAttackFlipRandom	CorrAttackFlipB ayes
Success	57.47%	64.86%	71.42%	79.15%
Queries	1645	1599		1159	1036
Table 10: Success rate and average queries of un-targeted attack on Resnet50 for Hierarchical
Strategy.
Method	Fixed Size 4	Fixed Size 8	Fixed Size 16	Fixed Size 32	Hierarchical
Success	100%	100.0%	99.47%	89.32%	100%
Queries	763	351	168	96	150
block and the other contains both the location and the PCA feature. PCA helps the learning process
of the reward and achieve higher success rate and lower number of queries. PCA feature achieves
significant improvement on CorrAttackFlip. We may find more useful features in the future.
C.6 Comparison between CorrAttackFLIP, BayesOpt and Bayes-Attack
The main difference between BayesOpt and Bayes-Attack is using different types of GP regression
(Standard GP for Bayes-Attack and Additive GP for BayesOpt), so we will consider these two models
as a group when comparing with our model CorrAttack.
Difference between CorrAttack, BayesOpt and Bayes-Attack: For l∞ attacks, assume there are
no hierarchical structure, we have blocks E = {e000, e001,…，ehwc}, where the blockis b X b square
of pixels and (h, w, c) = (height/b, width/b, channel). CorrAttack, BayesOpt (Ru et al., 2020) and
Bayes-Attack (Shukla et al., 2019) all try to search the adversarial noise on E with perturbation
δ ∈ [-, ]d where d = h × w × c, the perturbation of block eijk at time t is δetijk .
BayesOpt and Bayes-Attack use a GP regression directly on δ ∈ [-, ]d (all blocks),
f(δ)∣Dn 〜NOrmal(μn(δ),σn(δ)).	(19)
CorrAttack define an action space A and use a standard GP regression on features zeijk = (i, j, k, pca)
(single block),
gt(aeijk ) IDt 〜NOrmaI(Mt(Zeijk ),σ2(zeijk )).	(20)
15
Under review as a conference paper at ICLR 2021
Table 11: Ablation study on features with success rate and average queries of targeted attack
on ImageNet. ε = 0.05 and query limit is 10000. We use feature zeijk = (i, j, k, pca) for
CorrAttackDiffw pca and CorrAttackFlipw pca, use zeijk = (i, j, k) for CorrAttackDiffw/o pca and
CorrAttaCkFlipW/o pca.
Attack	VGG16		Resnet50		Densenet121	
	Success	Queries	Success	Queries	Success	Queries
CorrAttackDiff w/o pca	88.69%	-^3892^^	81.71%	4066	90.88%	3540
CorrAttackDiffw pca	88.41%	3826	81.84%	4064	91.29%	3513
CorrAttackFlip w/o pca	98.11%	-^2233^^	95.86%	2682	98.10%	2195
CorrAttackFlip w pca	98.07%	2191	96.39%	2531	99.41%	2019
At each iteration, in BayesOpt and Bayes-Attack, the changes of overall perturbation is
δt - δt-ι = {葭00 ∪ 般01 ∪…∪ δLJ-{C10 ∪ CII ∪…∪ CI}.	(21)
However, in CorrAttaCk,
δt - δt-1 = δetijk - δet-ijk1.	(22)
In ConClusion, BayesOpt and Bayes-AttaCk view eaCh bloCk as a dimension, try to searCh the overall
perturbation direCtly. CorrAttaCk defines a low dimension feature spaCe, keep an overall perturbation
and try to searCh an aCtion on single bloCk.
Time complexity and running time: The time Complexity of fitting GP regression is O(dn2) where
d is the dimension of input and n is the number of samples. And the dimension for CorrAttaCk
(d = 4 for zeijk = (i, j, k, pca)) is muCh smaller than BayesOpt and Bayes-AttaCk (d = 6912 if
h = w = 48, c = 3). Moreover, we Can Convert the Continuous searCh spaCe of BayesOpt and
Bayes-AttaCk from [-e, e]6912 to discrete search space E = {e000, e001,…,ehwc}, whose number
is only 6912, smaller searCh spaCe Could save the Computation time of aCquisition funCtion.
We compare the running time for CorrAttackFlip with BayesOpt and Bayes-Attack on 20 images
from ImageNet. Table 12 shows the running time for the un-targeted attack. We use PyTorch2
to develop these two models. All experiments were conducted on a personal workstation with 28
Intel(R) Xeon(R) Gold 5120 2.20GHz CPUs, an NVIDIA GeForce RTX2080Ti 11GB GPU and
252G memory.
BayesOpt models the loss function with a very high dimensional Gaussian process. The decomposi-
tion of additive kernel also needs to be restarted several times. Even though we try to optimize the
speed of BayesOpt with GPU acceleration, it is still very slow and takes hundreds of times more
computational resources than CorrAttack .
Bayes-Attack could be regarded as a simpler version of BayesOpt, which does not add additive
kernel. We do not evaluate it on targeted task (when query>1000) since GP inference time grows fast
as evaluated query increases, e.g. For Bayes-Attack, when 150 <query< 200, Time=1.6s/query;
800 <query< 1000, Time = 10.5s/query. CorrAttack solves this problem with Time=0.1s/query
even when query reaches 10000. Since we forget the previous samples before t - τ , our input
sample n will be smaller than τ . The forgetting technique can not be applied into the Bayes-Attack
and BayesOpt since they are searching the perturbation of all blocks so each sample needs to be
remembered.
C.7 Growing curve of success rate
The number of average queries is sometimes misleading due to the the heavy tail distribution of
queries. Therefore in Figure 4, we plot the success rates at different query levels to show the detailed
behaviors of different attacks. It shows that CorrAttack is much more efficient than other methods at
all query levels.
2https://pytorch.org/
16
Under review as a conference paper at ICLR 2021
Table 12: Comparsion of running time between CorrAttackFlipand BayesOpt on un-targeted attack.
"Per Query" means the average time needed to perform one query to the loss-oracle and "Per Image"
denotes the average time to successfully attack an image. Since BayesOpt needs thousands of hours
to run all samples, We only tested on 20 samples from ImageNet, which will be marked as *.
Time	VGG16 —	ReSnet50	DenSenet121
	Per Query	Per Image	Per Query	Per Image	Per Query	Per Image
BayesOpt*	28.94s	5268s	-^40.53s^^	8673s	39.57s	8825s
Bayes-Attack*	3.03s	739s	3.42s	869s	2.96s	630s
CorrAttackFlip*	0.12s	19s	0.11s	15s	0.15s	20s
VGG16
3sκSsguns
ResnetSO
3sκSsguns
Densenetl21
--*- ZOO
-→-- NES
-→∙- NAttack
-→-- Bandits
-→-- PARSl
SignHunter
--*- Square Attack
__∙--- CorrAttackniff
VGG16
100%
3sκSSgUnS
ResnetSO
100%
(a) Un-targeted Attack
3sκSSgUnS
Densenetlzl
100%
-→-- ZOO
-→-- NES
--- NAttack
-→-- Bandits
-→-- PARSI
-→∙- SignHunter
-→-- Square Attack
-→-- CorrAtta ckn/ff
Im	ιos	ιo∙ ιo2	ιo9	ιo4 i©	ws	ιo4 -→-- COrrAttaCl⅛
Queries	Queries	Queries
(b) Targeted Attack
Figure 4: Success rate of black-box attack at different query levels for undefended ImageNet models.
C.8 Visualization of Local Property and Slow varying Property
Figure 5 shows more examples of finite difference for different network architectures and different
dataset. They all have local correlation structure as shown in Figure 1. And Figure 6 shows more
examples like Figure 2, the slow varying properties exist for different architectures and different
datasets.
C.9 Google Cloud Vision API
Figure 7 shows the example of attacking the Google Cloud Vision API. CorrAttackFlip and PARSI suc-
cessfully change the classification result. BeyesOpt, however, can not remove the top 1 classification
result out of the output.
C.10 Visualization of Adversarial Examples
17
Under review as a conference paper at ICLR 2021
(a) VGG16 on Imagenet	(b) Resnet50 on Imagenet
(c) Densenet121 on Imagenet	(d) LeNet on MNIST
Figure 5: Finite difference of perturbation like Figure 1. h = w = 28. For Imagenet, b = 8 and
η = 0.05. For MNIST, b = 1 andη = 0.2
18
Under review as a conference paper at ICLR 2021
Figure 6: Difference of finite difference of perturbation like Figure 2. h = w = 28. For Imagenet,
b = 8 andη = 0.05. For MNIST, b = 1 andη = 0.2
19
Under review as a conference paper at ICLR 2021
(a) Natural Image
(b) CorrAttackFlip
(c) BayesOpt
(d) PARSI
(e) NAttack
Figure 7: Example result of attacking Google Cloud Vision API
20
Under review as a conference paper at ICLR 2021
Figure 8: Visualization of adversarial examples for targeted attack on Densenet121. ε = 0.05
21