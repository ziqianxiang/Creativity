Under review as a conference paper at ICLR 2021
Feature-Robust Optimal Transport
for High-Dimensional Data
Anonymous authors
Paper under double-blind review
Ab stract
Optimal transport is a machine learning problem with applications including
distribution comparison, feature selection, and generative adversarial networks.
In this paper, we propose feature-robust optimal transport (FROT) for high-
dimensional data, which solves high-dimensional OT problems using feature se-
lection to avoid the curse of dimensionality. Specifically, we find a transport plan
with discriminative features. To this end, we formulate the FROT problem as a
min-max optimization problem. We then propose a convex formulation of the
FROT problem and solve it using a Frank-Wolfe-based optimization algorithm,
whereby the subproblem can be efficiently solved using the Sinkhorn algorithm.
Since FROT finds the transport plan from selected features, itis robust to noise fea-
tures. To show the effectiveness of FROT, we propose using the FROT algorithm
for the layer selection problem in deep neural networks for semantic correspon-
dence. By conducting synthetic and benchmark experiments, we demonstrate that
the proposed method can find a strong correspondence by determining important
layers. We show that the FROT algorithm achieves state-of-the-art performance
in real-world semantic correspondence datasets.
1	Introduction
Optimal transport (OT) is a machine learning problem with several applications in the computer
vision and natural language processing communities. The applications include Wasserstein distance
estimation (Peyre et al., 2019), domain adaptation (Yan et al., 2018), multitask learning (Janati et al.,
2019), barycenter estimation (Cuturi & Doucet, 2014), semantic correspondence (Liu et al., 2020),
feature matching (Sarlin et al., 2019), and photo album summarization (Liu et al., 2019). The OT
problem is extensively studied in the computer vision community as the earth mover’s distance
(EMD) (Rubner et al., 2000). However, the computational cost of EMD is cubic and highly expen-
sive. Recently, the entropic regularized EMD problem was proposed; this problem can be solved
using the Sinkhorn algorithm with a quadratic cost (Cuturi, 2013). Owing to the development of the
Sinkhorn algorithm, researchers have replaced the EMD computation with its regularized counter-
parts. However, the optimal transport problem for high-dimensional data has remained unsolved for
many years.
Recently, a robust variant of the OT was proposed for high-dimensional OT problems and used for
divergence estimation (Paty & Cuturi, 2019; 2020). In the robust OT framework, the transport plan
is computed with the discriminative subspace of the two data matrices X ∈ Rd×n and Y ∈ Rd×m.
The subspace can be obtained using dimensionality reduction. An advantage of the subspace robust
approach is that it does not require prior information about the subspace. However, given prior
information such as feature groups, we can consider a computationally efficient formulation. The
computation of the subspace can be expensive if the dimensionality of data is high, for example,
104.
One of the most common prior information items is a feature group. The use of group features is
popular in feature selection problems in the biomedical domain and has been extensively studied in
Group Lasso (Yuan & Lin, 2006). The key idea of Group Lasso is to prespecify the group variables
and select the set of group variables using the group norm (also known as the sum of `2 norms).
For example, if we use a pretrained neural network as a feature extractor and compute OT using
the features, then we require careful selection of important layers to compute OT. Specifically, each
1
Under review as a conference paper at ICLR 2021
(a) OT on clean data.
(b) OT on noisy data.
Figure 1: transport plans between two synthetic distributions with 10-dimensional vectors xe =
(x>, z>), ye = (y>, z>), where two-dimensional vectors X 〜 N (μx, Σχ) and y 〜 N (μy, Σy)
are true features; and Zx 〜 N(08, I8)and Zy 〜 N(08, Ig) are noisy features. (a) OT between
distribution x and y is a reference. (b) OT between distribution xe and ye. (c) FROT transport plan
between distribution xe and ye where true features and noisy features are grouped, respectively.
(c) FROT on noisy data (η = 1).
layer output is regarded as a grouped input. Therefore, using a feature group as prior information is
a natural setup and is important for considering OT for deep neural networks (DNNs).
In this paper, we propose a high-dimensional optimal transport method by utilizing prior information
in the form of grouped features. Specifically, we propose a feature-robust optimal transport (FROT)
problem, for which we select distinct group feature sets to estimate a transport plan instead of deter-
mining its distinct subsets, as proposed in (Paty & Cuturi, 2019; 2020). To this end, we formulate
the FROT problem as a min-max optimization problem and transform it into a convex optimization
problem, which can be accurately solved using the Frank-Wolfe algorithm (Frank & Wolfe, 1956;
Jaggi, 2013). The FROT’s subproblem can be efficiently solved using the Sinkhorn algorithm (Cu-
turi, 2013). An advantage of FROT is that it can yield a transport plan from high-dimensional data
using feature selection, using which the significance of the features is obtained without any addi-
tional cost. Therefore, the FROT formulation is highly suited for high-dimensional OT problems.
Through synthetic experiments, we initially demonstrate that the proposed FROT is robust to noise
dimensions (See Figure 1). Furthermore, we apply FROT to a semantic correspondence problem
(Liu et al., 2020) and show that the proposed algorithm achieves SOTA performance.
Contribution:
•	We propose a feature robust optimal transport (FROT) problem and derive a simple and effi-
cient Frank-Wolfe based algorithm. Furthermore, we propose a feature-robust Wasserstein
distance (FRWD).
•	We apply FROT to a high-dimensional feature selection problem and show that FROT is
consistent with the Wasserstein distance-based feature selection algorithm with less com-
putational cost than the original algorithm.
•	We used FROT for the layer selection problem in a semantic correspondence problem and
showed that the proposed algorithm outperforms existing baseline algorithms.
2	Background
In this section, we briefly introduce the OT problem.
Optimal transport (OT): The following are given: independent and identically distributed (i.i.d.)
samples X = {xi}in=1 ∈ Rd×n from a d-dimensional distribution p, and i.i.d. samples Y =
{yj }jm=1 ∈ Rd×m from the d-dimensional distribution q . In the Kantorovich relaxation of OT,
admissible couplings are defined by the set of the transport plan:
U(μ, V) = {Π ∈ R+×m : Π1m = a, Π>1n = b},
where Π ∈ Rn+×m is called the transport plan, 1n is the n-dimensional vector whose elements are
ones, and a = (a1 , a2, . . . , an)> ∈ Rn+ and b = (b1, b2, . . . , bm)> ∈ R+m are the weights. The
OT problem between two discrete measures μ = Pin=1 aiδxi and ν = Pjm=1 bj δyj determines the
2
Under review as a conference paper at ICLR 2021
optimal transport plan of the following problem:
nm
∏∈m(n,ν) E ∑>jc3, yj),
i=1 j=1
(1)
where c(x, y) is a cost function. For example, the squared Euclidean distance is used, that is,
c(x, y) = kx - yk22. To solve the OT problem, Eq. (1) (also known as the earth mover’s dis-
tance) using linear programming requires O(n3), (n = m) computation, which is computationally
expensive. To address this, an entropic-regularized optimal transport is used (Cuturi, 2013).
min
Π∈U(μ,ν)
nm
πijc(xi,yj)+H(Π),
i=1 j=1
where ≥ 0 is the regularization parameter, and H(Π) = Pin=1 Pjm=1 πij (log(πij) - 1) is the
entropic regularization. If = 0, then the regularized OT problem reduces to the EMD problem.
Owing to entropic regularization, the entropic regularized OT problem can be accurately solved
using Sinkhorn iteration (Cuturi, 2013) with a O(nm) computational cost (See Algorithm 1).
Wasserstein distance: If the cost function is defined as c(x, y) = d(x, y) with d(x, y) as a dis-
tance function and p ≥ 1, then we define the p-Wasserstein distance of two discrete measures
μ = Pn=1 aiδχi and V = Pm=I bjδyj. as
Wp(μ, V)
nm
∏∈m(μ,ν) ∑ ∑ πij d(xi, yj )p
1/p
Recently, a robust variant of the Wasserstein distance, called the subspace robust Wasserstein dis-
tance (SRW), was proposed (Paty & Cuturi, 2019). The SRW computes the OT problem in the
discriminative subspace. This can be determined by solving dimensionality-reduction problems.
Owing to the robustness, it can compute the Wasserstein from noisy data. The SRW is given as
1
(n m
min	max	X X πij kU>xi - U>yj k22
∏∈U (μ,ν) U ∈Rd×k,U >U=Ik W j	j
i=1 j=1
where U is the projection matrix with k ≤ d, and Ik ∈ Rk×k is the identity matrix. The SRW
or its relaxed problem can be efficiently estimated using either eigenvalue decomposition or the
Frank-Wolfe algorithm.
3	Proposed Method
This paper proposes FROT. We assume that the vectors are grouped as x = (x(1)>, . . . , x(L)>)>
and y = (y(I)>,..., y(L)>)>. Here, x(') ∈ Rd' and y(') ∈ Rd' are the d` dimensional vectors,
where PL= 1 d` = d. This setting is useful if we know the explicit group structure for the feature
vectors a priori. In an application in L-layer neural networks, we consider x(') and y(') as outputs
of the `th layer of the network. Ifwe do not have a priori information, we can consider each feature
independently (i.e., d1 = d2 = . . . = dL = 1 and L = d). All proofs in this section are provided in
the Appendix.
3.1	Feature-Robust Optimal Transport (FROT)
The FROT formulation is given by
nm L
∏∈m(n,ν) α∈∑L E ∑>j∑>c(X化 y"
7 ' '	i=1j=1	'=1
(3)
3
Under review as a conference paper at ICLR 2021
where ΣL = {α ∈ RL+ : α>1L = 1} is the probability simplex. The underlying concept of FROT
is to estimate the transport plan Π using distinct groups with large distances between {x(') }n=ι and
{yj')}m=ι. We note that determining the transport plan in nondistinct groups is difficult because
the data samples in {x(')}n=ι and {yj')}m=ι overlap. By contrast, in distinct groups, {χi')}n=ι
and {yj')}m=ι are different, and this aids in determining an optimal transport plan. This is an
intrinsically similar idea to the subspace robust Wasserstein distance (Paty & Cuturi, 2019), which
estimates the transport plan in the discriminative subspace, while our approach selects important
groups. Therefore, FROT can be regarded as a feature selection variant of the vanilla OT problem
in Eq. (1), whereas the subspace robust version uses dimensionality-reduction counterparts.
Algorithm 1 Sinkhorn algorithm.		Algorithm 2 FROT with the Frank-Wolfe.	
1	: Input: a, b, C, , tmax	1	: Input: {xi}in=1, {yj}jm=1, η, and .
2	: Initialize K = e-C/, u = 1n, v =	2	Initialize Π, compute {C'}L=)
	1m ,t = 0	3	: for t = 0 . . . T do
3	: while t ≤ tmax and not converge do	4	π = argmin ∏∈u(μ,ν)hπ, Mn(G +
4	: u = a/(K v)		H(Π)
5 6	: v = b/(K > u) : t=t+1	5 6	:	Π(t+1) = (1 - γ)Π(t) +γΠb with Y = ɪ.
7 8	: end while : return Π = diag(u)Kdiag(v)	7 8	2+t . : end for : return Π(T )
Using FROT, we can define a p-feature robust Wasserstein distance (p-FRWD).
Proposition 1 For the distance function d(x, y),
(n m L
min max ΣΣπij X α'd(Xi'), yj'))p
π∈u MV) α∈zL M M
1/p
(4)
is a distance for p ≥ 1.
Note that we can show that 2-FRWD is a special case of SRW with d(x, y) = kx - yk2 (See
Appendix). The key difference between SRW and FRWD is that FRWD can use any distance, while
SRW can only use d(x, y) = kx - yk2.
3.2	FROT Optimization
Here, We propose two FROT algorithms based on the Frank-Wolfe algorithm and linear program-
ming.
Frank-Wolfe: We propose a continuous variant of the FROT algorithm using the Frank-Wolfe
algorithm, which can be fully differentiable. To this end, we introduce entropic regularization for α
and rewrite the FROT as a function of Π. Therefore, we solve the following problem for α:
min max
Π∈U(μ,ν) a∈ΣL
nm L
Jη(Π, α), with Jη(Π, α) = ΣΣ∏ij X α'c(x('), yj')) - ηH(α),
i=1 j=1	'=1
where η ≥ 0 is the regularization parameter, and H(α) = PL= 1 a`(log(a`) - 1) is the entropic
regularization for α. An advantage of entropic regularization is that the nonnegative constraint is
naturally satisfied, and the entropic regularizer is a strong convex function.
Lemma 2 The optimal solution of the optimization problem
L
α* = argmax Jn(Π, α), with Jn(Π, α) = ɪ2 αgφg — ηH(α)
4
Under review as a conference paper at ICLR 2021
with a fixed admissible transport plan Π ∈ U (μ, V), is given by
C *
α'
LeXp Q φ?	With Jn(∏, α*) = 〃 log (X eχp (1 φj) + 4
Σ'0=1 exp(1 φ'°)	V=1	'η
Using Lemma 2 (or Lemma 4 in Nesterov (2005)) together with the setting φ' =
Pn=I Pm=I ∏ijc(x('), y(')) = h∏, C'i, [C']ij = c(x('), y(')), the global problem is equivalent to
π∈U(n ) Gn(Π), with Gn(Π) = ηlog (Xexp (1 h∏, C'i)) ∙
(5)
Note that this is knoWn as a smoothed max-operator (Nesterov, 2005; Blondel et al., 2018). Specifi-
cally, regularization parameter η controls the “smoothness” of the maximum.
Proposition 3 Gn (Π) is a convex function relative to Π.
The derived optimization problem of FROT is convex. Therefore, we can determine globally optimal
solutions. Note that the SRW optimization problem is not jointly convex (Paty & Cuturi, 2019) for
the projection matrix and the transport plan. In this study, we employ the Frank-Wolfe algorithm
(Frank & Wolfe, 1956; Jaggi, 2013), using which we approximate Gn(Π) with linear functions at
Π(t) and move Π toward the optimal solution in the convex set (See Algorithm 2).
The derivative of the loss function Gn(Π) at Π(t) is given by
∂Gn (∏)
∂Π
=X ɑ't)e` = M∏(t) With α(t) =	JXp (1 No)
∏=∏(t)	'=1	PL=ι exp (I h∏⑴,C'0i
Then, we update the transport plan by solving the EMD problem:
Π(t+1) = (1- γ)Π(t) +γΠb with Πb = argmin	hΠ, MΠ(t) i,
Π∈U (μ,ν)
where γ = 2/(2 + k). Note that MΠ(t) is given by the weighted sum of the cost matrices. Thus, we
can utilize multiple features to estimate the transport plan Π for the relaxed problem in Eq. (5).
Using the Frank-Wolfe algorithm, we can obtain the optimal solution. However, solving the EMD
problem requires a cubic computational cost that can be expensive if n and m are large. To address
this, we can solve the regularized OT problem, which requires O(nm). We denote the Frank-Wolfe
algorithm with EMD as FW-EMD and the Frank-Wolfe algorithm with Sinkhorn as FW-Sinkhorn.
Computational complexity: The proposed method depends on the Sinkhorn algorithm, which
requires an O(nm) operation. The computation of the cost matrix in each subproblem needs
an O(Lnm) operation, where L is the number of groups. Therefore, the entire complexity is
O(T Lnm), where T is the number of Frank-Wolfe iterations (in general, T = 10 is sufficient).
Proposition 4 For each t ≥ 1, the iteration Π(t) of Algorithm 2 satisfies
Gn(∏(t)) — Gn(∏*) ≤ ^mat^^l + δ),
where σmax (Φ>Φ) is the largest eigenvalue of the matrix Φ>Φ and Φ =
(vec(C1 ), vec(C2), ∙ ∙ ∙ , vec(CL))>; and δ ≥ 0 is the accuracy to which internal linear
subproblems are solved.
Based on Proposition 4, the number of iterations depends on η, , and the number of groups. If we
set a small η, convergence requires more time. In addition, if we use entropic regularization with a
large , the δ in Proposition 4 can be large. Finally, if we use more groups, the largest eigenvalue of
the matrix Φ>Φ can be larger. Note that the constant term of the upper bound is large; however, the
Frank-Wolfe algorithm converges quickly in practice.
5
Under review as a conference paper at ICLR 2021
Linear Programming: BeCaUselimη→o+ G〃(n) = maX'∈{i,2,...,L} Pn=I Pm=I ∏jc(x化 yj')),
the FROT problem can also be written as
min maX
∏∈U(μ,ν) '∈{1,2,...,L}
nm
XX ∏", y"
i=1 j =1
(6)
BeCaUse the objeCtive is the max of linear fUnCtions, it is Convex with respeCt to Π. We Can solve
the problem via linear programming:
min t, s.t. h∏, CQ≤ t,' =1,2,...,L.	(7)
∏∈U (μ,ν),t
This optimization Can be easily solved Using an off-the-shelf LP paCkage. However, the CompUta-
tional Cost of this LP problem is high in general (i.e., O(n3), n = m).
3.3	Application: Semantic Correspondence
We applied oUr proposed FROT algorithm to semantiC CorrespondenCe. The semantiC Correspon-
denCe is a problem that determines the matChing of objeCts in two images. That is, given inpUt
image pairs (A, B), with Common objeCts, we formUlated the semantiC CorrespondenCe problem to
estimate the transport plan from the key points in A to those in B ; this framework was proposed in
(LiU et al., 2020). In FigUre 2, we show an overview of oUr proposed framework.
Cost matrix computation e`: In our framework, We employed a pretrained convolutional neural
network to extraCt dense featUre maps for eaCh ConvolUtional layer. The dense featUre map of the
`th layer output of the sth image is given by
f('q+)(r-i)hs ∈ Rd', q = 1, 2,... ,hs,r = 1, 2,... ,Ws,' = 1, 2,...,L,
where ws and hs are the width and height of the sth image, respectively, and d` is the dimension of
the `th layer’s feature map. Note that because the dimension of the dense feature map is different
for each layer, we sample feature maps to the size of the 1st layer’s feature map size (i.e., hs × ws).
The `th layer’s cost matrix for images s and s0 is given by
[C']ij = kff,S)- f尸)k2, i = 1, 2,...,wshs, j = 1, 2,...,ws0 hs0.
A potential problem with FROT
is that the estimation depends
significantly on the magnitude
of the cost of each layer (also
known as a group). Hence,
normalizing each cost matrix
is important. Therefore, we
normalized each feature vector
by 于严—fi(',s)/kfi(',s)k2.
Consequently, the cost matrix
is given by [e`]j = 2 -
2fi(',s)>fj(',s0). We can use dis-
tances such as the L1 distance.
C'
Feature Robust Optimal Transport (FROT)
■ b
Source image
I arget image
χ(2)
y网
Γι∏rr-
Computation of a and b with Figure 2: Semantic correspondence framework based on FROT.
staircase re-weighting: For semantic correspondence, setting a ∈ Rhs ws and b ∈ Rhs0 ws0 is im-
portant because semantic correspondence can be affected by background clutter. Therefore, we gen-
erated the class activation maps (Zhou et al., 2016) for the source and target images and used them
as a and b, respectively. For CAM, we chose the class with the highest classification probability and
normalized it to the range [0, 1].
4	Related Work
OT algorithms: The Wasserstein distance can be determined by solving the OT problem. An ad-
vantage of the Wasserstein distance is its robustness to noise; moreover, we can obtain the transport
6
Under review as a conference paper at ICLR 2021
(a) Objective score.	(b) MSE (η).	(c) MSE ().
Figure 3: (a) Objective scores for LP, FW-EMD, and FW-Sinkhorn. (b) MSE between transport plan
ofLP and FW-EMD and that with LP and FW-Sinkhorn with different η. (c) MSE between transport
plan of LP and FW-Sinkhorn with different .
plan, which is useful for many machine learning applications. To reduce the computation cost for
the Wasserstein distance, the sliced Wasserstein distance is useful (Kolouri et al., 2016). Recently, a
tree variant of the Wasserstein distance was proposed (Evans & Matsen, 2012; Le et al., 2019; Sato
et al., 2020); the sliced Wasserstein distance is a special case of this algorithm.
The approach most closely related to FROT is a robust variant of the Wasserstein distance, called the
subspace robust Wasserstein distance (SRW) (Paty & Cuturi, 2019). SRW computes the OT problem
in a discriminative subspace; this is possible by solving dimensionality-reduction problems. Owing
to the robustness, SRW can successfully compute the Wasserstein distance from noisy data. The
max-sliced Wasserstein distance (Deshpande et al., 2019) and its generalized counterpart (KoloUri
et al., 2019) can also be regarded as subspace-robust Wasserstein methods. Note that SRW (Paty &
Cuturi, 2019) is a min-max based approach, while the max-sliced Wasserstein distances (Deshpande
et al., 2019; Kolouri et al., 2019) are max-min approaches. The FROT is a feature selection variant
of the Wasserstein distance, whereas the subspace approaches are used for dimensionality reduction.
As a parallel work, a general minimax optimal transport problem called the robust Kantorovich prob-
lem (RKP) was recently proposed (Dhouib et al., 2020). RKP involves using a cutting-set method
for a general minmax optimal transport problem that includes the FROT problem as a special case.
The approaches are technically similar; however, our problem and that of Dhouib et al. (2020) are
intrinsically different. Specifically, we aim to solve a high-dimensional OT problem using feature
selection and apply it to semantic correspondence problems, while the RKP approach focuses on
providing a general framework and uses it for color transformation problems. As a technical dif-
ference, the cutting-set method may not converge to an optimal solution if we use the regularized
OT (Dhouib et al., 2020). By contrast, because we use a Frank-Wolfe algorithm, our algorithm con-
verges to a true objective function with regularized OT solvers. The multiobjective optimal transport
(MOT) is an approach (Scetbon et al., 2020) parallel to ours. The key difference between FROT and
MOT is that MOT tries to use the weighted sum of cost functions, while FROT considers the worst
case. Moreover, as applications, we focus on the cost matrices computed from subsets of features,
while MOT considers cost matrices with different distance functions.
5	Experiments
5.1	Synthetic Data
We compare FROT with a standard OT using synthetic datasets. In these experiments, we initially
generate two-dimensional vectors X 〜N(μχ, Σχ) and y 〜N(μy, Σy). Here, We set μχ =
(-5,0)>, μy = (5,0)>, Σχ = Σy = ((5,1)>, (4,1)>). Then, we concatenate Zx 〜N(08,瓜)
and Zy 〜 N(08, I8)to X and y, respectively, to give e = (x>, z>), e = (y>, z>).
For FROT, we set η = 1.0 and the number of iterations of the Frank-Wolfe algorithm as T = 10.
The regularization parameter is set to = 0.02 for all methods. To show the proof-of-concept, we
set the true features as a group and the remaining noise features as another group.
Fig. 1a shows the correspondence from X and y with the vanilla OT algorithm. Figs. 1b and 1c
show the correspondence of FROT and OT with Xe and ye, respectively. Although FROT can identify
7
Under review as a conference paper at ICLR 2021
Methods
I aero bike bird boat bottle bus Car Cat Chair CoW dog horse moto Person Plant sheep train tv ∣ all
SPair-71k finetuned models	CNNGeo (Rocco et al., 2017)	23.4	16.7	40.2	14.3	36.4	27.7	26.0	32.7	12.7	27.4	22.8	13.7	20.9	21.0	17.5	10.2	30.8	34.1	20.6
	A2Net (Hongsuck Seo et al., 2018)	22.6	18.5	42.0	16.4	37.9	30.8	26.5	35.6	13.3	29.6	24.3	16.0	21.6	22.8	20.5	13.5	31.4	36.5	22.3
	WeakAlign (Rocco et al., 2018a)	22.2	17.6	41.9	15.1	38.1	27.4	27.2	31.8	12.8	26.8	22.6	14.2	20.0	22.2	17.9	10.4	32.2	35.1	20.9
	NC-Net (RocCO et al., 2018b)	17.9	12.2	32.1	11.7	29.0	19.9	16.1	39.2	9.9	23.9	18.8	15.7	17.4	15.9	14.8	9.6	24.2	31.1	20.1
Table 1: Per-class PCK (αbbox = 0.1) results using SPair-71k. All models use ResNet101. The
numbers in the bracket of SRW are the input layer indicies.
SPair-71k validation	HPF (Minetal., 2019a)	25.2	18.9	52.1	15.7	38.0	22.8	19.1	52.9	17.9	33.0	32.8	20.6	24.4	27.9	21.1	15.9	31.5	35.6	28.2
	OT-HPF (LiU et al., 2020)	32.6	18.9	62.5	20.7	42.0	26.1	20.4	61.4	19.7	41.3	41.7	29.8	29.6	31.8	25.0	23.5	44.7	37.0	33.9
	FROT(η = 0.2,6 = 0.4)	35.1	20.3	59.8	21.1	42.9	27.7	21.2	63.5	18.8	39.7	37.9	29.2	28.8	29.9	28.2	24.3	52.1	39.5	34.7
Without
SPair-71k
validation
OT
FROT (η = 0.3, T = 3)
FROT (η = 0.3, T = 10)
FROT (η = 0.5, T = 3)
FROT (η = 0.5, T = 10)
FROT (η = 0.7, T = 3)
FROT (η = 0.7, T = 10)
SRWaayers = {1,32-34})
SRWaayers = {1, 31-34})
SRW (layers = {1, 30-34})
FROT (layers = {1, 32-34})
FROT (layers = {1, 31-34})
FROT (layers = {1, 30-34})
.0.0.2.9.8.5.4.5.8.7.6.2
8.8.8.6.6.7.7.5.5.5.3.5.
222222222222
.1.9.8.6.6.0.0.8.0.6.5.4
277556690092
222222212212
322555551799
1.6.6.6.6.6.6.6.7.7.3.6.
333333322222
.4.8.8.7.8.0.0.3.6.0.1.9
8.8.8.8.8.8.8.3.3.4.3.5.
233333322222
.0.5.5.4.3.9.9.9.1.3.3.4
171717171717171213131011
300994405351
4.2.2.1.1.2.2.8.8.9.4.8.
566666644444
790221168066
9.1.2.9.9.9.9.7.7.8.0.1.
122111111122
.9.2.2.6.6.1.0.0.3.5.5.0
277556611125
222222222222
077006582870
8.0.0.0.0.9.9.3.4.4.0.3.
344443333333
.3.4.4.9.9.0.9.6.7.9.4.5
733990955580
122112111112
.4.3.4.9.0.6.6.7.3.6.1.3
066676634535
555555544444
.5.9.9.8.9.4.5.0.3.7.7.5
162020181819191414141516
.1.0.9.1.0.4.3.4.7.8.3.3
054443399925
333333322233
.5.4.2.2.4.5.5.6.1.7.0.0
9.0.0.7.7.6.6.7.8.8.2.5.
133222211122
845689950502
8.8.8.8.8.8.8.0.1.1.8.1.
233333322212
677337613745
299009977770
444554433334
.8.1.9.1.1.6.6.2.2.3.3.1
732221155557
122222211111
.0.9.9.3.4.5.6.7.9.2.7.5
166665566746
222222211111
28.3
33.7
33.7
32.8
32.8
32.7
32.7
24.5
24.8
25.2
24.3
26.6
28.8
a suitable matching, the OT fails to obtain a significant correspondence. We observed that the α
parameter corresponding to a true group is α1 = 0.9999. Moreover, we compared the objective
scores of the FROT with LP, FW-EMD, and FW-Sinkhorn ( = 0.1). Figure 3a shows the objective
scores of FROTs with the different solvers, and both FW-EMD and FW-Sinkhorn can achieve almost
the same objective score with a relatively small η. Moreover, Figure 3b shows the mean squared error
between the LP method and the FW counterparts. Similar to the objective score cases, it can yield a
similar transport plan with a relatively small η. Finally, we evaluated the FW-Sinkhorn by changing
the regularization parameter η. In this experiment, we set η = 1 and varied the values. The result
shows that we can obtain an accurate transport plan with a relatively small .
5.2	Semantic correspondence
We evaluated our FROT algorithm for semantic correspondence. In this study, we used the SPair-
71k (Min et al., 2019b). The SPair-71k dataset consists of 70, 958 image pairs with variations in
viewpoint and scale. For evaluation, we employed a percentage of accurate key points (PCK), which
counts the number of accurately predicted key points given a fixed threshold (Min et al., 2019b). All
semantic correspondence experiments were run on a Linux server with NVIDIA P100.
For the optimal transport based frameworks, we employed ResNet101 (He et al., 2016) pretrained
on ImageNet (Deng et al., 2009) for feature and activation map extraction. The ResNet101 consists
of 34 convolutional layers and the entire number of features is d = 32, 576. Note that we did not
fine-tune the network. We compared the proposed method with several baselines (Min et al., 2019b)
and the SRW1. Owing to the computational cost and the required memory size for SRW, we used the
first and the last few convolutional layers of ResNet101 as the input of SRW. In our experiments, we
empirically set T = 3 and = 0.1 for FROT and SRW, respectively. For SRW, we set the number
of latent dimension as k = 50 for all experiments. HPF (Min et al., 2019a) and OT-HPF (Liu et al.,
2020) are state-of-the-art methods for semantic correspondence. HPF and OT-HPF required the
validation dataset to select important layers, whereas SRW and FROT did not require the validation
dataset. OT is a simple optimal transport-based method that does not select layers.
Table 1 lists the per-class PCK results obtained using the SPair-71k dataset. FROT (η = 0.3) out-
performs most existing baselines, including HPF and OT. Moreover, FROT (η = 0.3) is consistent
with OT-HPF (Liu et al., 2020), which requires the validation dataset to select important layers. In
this experiment, setting η < 1 results in favorable performance (See Table 3 in the Appendix). The
computational costs of FROT is 0.29, while SRWs are 8.73, 11.73, 15.76, respectively. Surprisingly,
FROT outperformed SRWs. However, this is mainly due to the used input layers. Therefore, scaling
up SRW would be an interesting future work.
We further evaluated FROT by tuning hyperparameters η and using validation sets, where the max-
imum search ranges for η and are set to 0.2 to 2.0 and 0.1 to 0.6 with intervals of 0.1, respectively.
Figure 6 in Appendix shows the average PCK scores for (η, ) pairs on the validation split of SPair-
71k. By using hyperparameter search, we selected (η = 0.2, = 0.4) as an optimal parameter. The
FROT with optimal parameters outperforms the state-of-the-art method (Liu et al., 2020).
1https://github.com/francoispierrepaty/SubspaceRobustWasserstein
8
Under review as a conference paper at ICLR 2021
(a) Colon dataset.
(b) Leukemia dataset. (C) Prostate_ge dataset.	(d) GLL85 dataset.
Figure 4: Feature selection results. We average over 50 runs of accuracy (on test set) of SVM trained
with top k features selected by several methods.
5.3	Feature Selection Experiments
Since FROT finds the transport plan and discriminative features between X and Y , we can use
FROT as a feature-selection method. We considered X ∈ Rd×n and Y ∈ Rd×m as sets of samples
from classes 1 and 2, respectively. The optimal important feature is given by
exp (1 h∏⅛, C'i)	d d ∕1
&' =--------三一一/	, with Πb = argmin η log exp ( T∏, C'i ) I ,
P(LI exp (1 h∏, C'0i	π∈U(μ,ν)	G 1η
where [C']ij = (xi(') - yj('))2. Finally, we selected the top K features by the ranking αb . Hence, α
changes to a one-hot vector for a small η and to ak ≈ L for a large η.
Here, we compared FROT with several baseline algorithms in terms of solving feature-selection
problems. In this study, we employed a high-dimensional and a few sample datasets with two class
classification tasks (see Table 2). All feature selection experiments were run on a Linux server with
an Intel Xeon CPU E7-8890 v4 with 2.20 GHz and 2 TB RAM.
In our experiments, we initially randomly split the data into two sets (75% for training and 25%
for testing) and used the training set for feature selection and building a classifier. Note that we
standardized each feature using the training set. Then, we used the remaining set for the test. The
trial was repeated 50 times, and we considered the averaged classification accuracy for all trials.
Considered as baseline methods, we computed the Wasserstein distance, maximum mean discrep-
ancy (MMD) (Gretton et al., 2007), and linear correlation2 for each dimension and sorted them in
descending order. Note that the Wasserstein distance is computed via sorting, which is computation-
ally more efficient than the Sinkhorn algorithm when d = 1. Then, we selected the top K features
as important features. For FROT, we computed the feature importance and selected the features that
had significant importance scores. In our experiments, we set η = 1.0 and T = 10. Then, we trained
a two-class SVM3 with the selected features.
Fig. 4 shows the average classification accuracy relative to the number of selected features. From
Figure 4, FROT is consistent with the Wasserstein distance-based feature selection and outperforms
the linear correlation method and the MMD for two datasets. Table 2 shows the computational
time(s) of the methods. FROT is about two orders of magnitude faster than the Wasserstein distance
and is also faster than MMD. Note that although MMD is as fast as the proposed method, it cannot
determine the correspondence between samples.
6	Conclusion
In this paper, we proposed FROT for high-dimensional data. This approach jointly solves feature
selection and OT problems. An advantage of FROT is that it is a convex optimization problem
and can determine an accurate globally optimal solution using the Frank-Wolfe algorithm. We
used FROT for high-dimensional feature selection and semantic correspondence problems. Through
extensive experiments, we demonstrated that the proposed algorithm is consistent with state-of-the-
art algorithms in both feature selection and semantic correspondence.
2 https://scikit- learn.org/stable/modules/feature_selection.html
3https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html
9
Under review as a conference paper at ICLR 2021
References
David Alvarez-Melis, Tommi Jaakkola, and Stefanie Jegelka. Structured optimal transport. In
AISTATS, 2018.
David Alvarez-Melis, Youssef Mroueh, and Tommi S Jaakkola. Unsupervised hierarchy matching
with optimal transport over hyperbolic spaces. AISTATS, 2020.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein generative adversarial networks.
In ICML, 2017.
Mathieu Blondel, Vivien Seguy, and Antoine Rolet. Smooth and sparse optimal transport. In AIS-
TATS, 2018.
Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative
models across incomparable spaces. In ICML, 2019.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In NIPS, 2013.
Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. ICML, 2014.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchical image database. In CVPR, 2009.
Ishan Deshpande, Yuan-Ting Hu, Ruoyu Sun, Ayis Pyrros, Nasir Siddiqui, Sanmi Koyejo, Zhizhen
Zhao, David Forsyth, and Alexander G Schwing. Max-sliced Wasserstein distance and its use for
GANs. In CVPR, 2019.
Sofien DhoUib, Ievgen Redko, Tanguy Kerdoncuff, Remi Emonet, and Marc Sebban. A swiss army
knife for minimax optimal transport. In ICML, 2020.
Steven N Evans and Frederick A Matsen. The phylogenetic kantorovich-rubinstein metric for en-
vironmental sequence samples. Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 74(3):569-592, 2012.
Marguerite Frank and Philip Wolfe. An algorithm for quadratic programming. Naval research
logistics quarterly, 3(1-2):95-110, 1956.
Bolin Gao and Lacra Pavel. On the properties of the softmax function with application in game
theory and reinforcement learning. arXiv preprint arXiv:1704.00805, 2017.
Arthur. Gretton, Kenji. Fukumizu, C. Hui. Teo, Le. Song, Bernhard. Scholkopf, and Alex Smola. A
kernel statistical test of independence. In NIPS, 2007.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, 2016.
Paul Hongsuck Seo, Jongmin Lee, Deunsol Jung, Bohyung Han, and Minsu Cho. Attentive semantic
alignment with offset-aware correlation kernels. In ECCV, 2018.
Martin Jaggi. Revisiting frank-wolfe: Projection-free sparse convex optimization. In ICML, 2013.
Hicham Janati, Marco Cuturi, and Alexandre Gramfort. Wasserstein regularization for sparse multi-
task regression. In AISTATS, 2019.
Soheil Kolouri, Yang Zou, and Gustavo K Rohde. Sliced wasserstein kernels for probability distri-
butions. In CVPR, 2016.
Soheil Kolouri, Kimia Nadjahi, Umut Simsekli, Roland Badeau, and Gustavo Rohde. Generalized
sliced wasserstein distances. In NeurIPS, 2019.
Tam Le, Makoto Yamada, Kenji Fukumizu, and Marco Cuturi. Tree-sliced approximation of wasser-
stein distances. NeurIPS, 2019.
10
Under review as a conference paper at ICLR 2021
Yanbin Liu, Makoto Yamada, Yao-Hung Hubert Tsai, Tam Le, Ruslan Salakhutdinov, and Yi Yang.
Lsmi-sinkhorn: Semi-supervised squared-loss mutual information estimation with optimal trans-
port. arXiv preprint arXiv:1909.02373, 2019.
Yanbin Liu, Linchao Zhu, Makoto Yamada, and Yi Yang. Semantic correspondence as an optimal
transport problem. In CVPR, 2020.
Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Hyperpixel flow: Semantic correspondence
with multi-layer neural features. In ICCV, 2019a.
Juhong Min, Jongmin Lee, Jean Ponce, and Minsu Cho. Spair-71k: A large-scale benchmark for
semantic correspondence. arXiv preprint arXiv:1908.10543, 2019b.
Yu Nesterov. Smooth minimization of non-smooth functions. Mathematical programming, 103(1):
127-152, 2005.
Francois-Pierre Paty and Marco CUtUrL SUbsPace robust Wasserstein distances. In ICML, 2019.
Francois-Pierre Paty and Marco Cuturi. Regularized optimal transport is ground cost adversarial.
ICML, 2020.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport. Foundations and Trends® in
Machine Learning, 11(5-6):355-607, 2019.
Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. Convolutional neural netWork architecture for
geometric matching. In CVPR, 2017.
Ignacio Rocco, Relja Arandjelovic, and Josef Sivic. End-to-end weakly-supervised semantic align-
ment. In CVPR, 2018a.
Ignacio Rocco, Mircea Cimpoi, Relja Arandjelovic, Akihiko Torii, Tomas Pajdla, and Josef Sivic.
Neighbourhood consensus networks. In NeurIPS, 2018b.
Yossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for
image retrieval. International journal of computer vision, 40(2):99-121, 2000.
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue:
Learning feature matching with graph neural networks. arXiv preprint arXiv:1911.11763, 2019.
Ryoma Sato, Makoto Yamada, and Hisashi Kashima. Fast unbalanced optimal transport on tree. In
NeurIPS, 2020.
Meyer Scetbon, Laurent Meunier, Jamal Atif, and Marco Cuturi. Handling multiple costs in optimal
transport: Strong duality and efficient computation. arXiv preprint arXiv:2006.07260, 2020.
Hongteng Xu, Dixin Luo, and Lawrence Carin. Scalable gromov-wasserstein learning for graph
partitioning and matching. arXiv preprint arXiv:1905.07645, 2019a.
Hongteng Xu, Dixin Luo, Hongyuan Zha, and Lawrence Carin Duke. Gromov-wasserstein learning
for graph matching and node embedding. In ICML, 2019b.
Yuguang Yan, Wen Li, Hanrui Wu, Huaqing Min, Mingkui Tan, and Qingyao Wu. Semi-supervised
optimal transport for heterogeneous domain adaptation. In IJCAI, 2018.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49-67, 2006.
Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, and Justin M Solomon.
Hierarchical optimal transport for document representation. In NeurIPS, 2019.
Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep
features for discriminative localization. In CVPR, 2016.
11
Under review as a conference paper at ICLR 2021
Appendix
Related work
In addition to accelerating the computation, structured optimal transport incorporates structural in-
formation directly into OT problems (Alvarez-Melis et al., 2018). Specifically, they formulate the
submodular optimal transport problem and solve the problem using a saddle-point mirror prox algo-
rithm. Recently, more complex structured information was introduced in the OT problem, including
the hierarchical structure (Alvarez-Melis et al., 2020; Yurochkin et al., 2019). These approaches
successfully incorporate structured information into OT problems with respect to data samples. By
contrast, FROT incorporates the structured information into features.
OT applications: OT has received significant attention for use in several computer vision tasks.
Applications include Wasserstein distance estimation (Peyre et al., 2019), domain adaptation (Yan
et al., 2018), multitask learning (Janati et al., 2019), barycenter estimation (Cuturi & Doucet, 2014),
semantic correspondence (Liu et al., 2020), feature matching (Sarlin et al., 2019), photo album
summarization (Liu et al., 2019), generative model (Arjovsky et al., 2017; Bunne et al., 2019),
graph matching (Xu et al., 2019a;b), and the semantic correspondence (Liu et al., 2020).
Proof of Proposition 1
For the distance function d(x, y), we prove that
FRWDp (μ,ν) =	min max
∏ ∏∈U(μ,ν) α∈ΣL
nm L
X X FjX α'd(Xf), yj') )p
i=1j=1	'=1
1/p
is a distance for p ≥ 1.
The symmetry can be read directly from the definition as we used distances that are symmetric.
For the identity of indiscernibles, when FRWDp (μ, V) = 0 with the optimal a and Π, there ex-
ists ' such that ɑ` > 0 (as a is in the simplex set). As there is a max in the definition and
Pij πjα'd(xi'), yj'))p = 0, this means that ∀', Pijnijd(x('), yj'))p = 0 and ∀', μ(') = V(').
Therefore, We have μ = V when FRWDp (μ, V) = 0.
When μ = V, this means that Xi = yi, a% = bi, ∀ i, and n = m, and we have d(xi, yj-) = 0 for
i = j. Thus, for any ɑ` ≥ 0, the optimal transport plan is π^ > 0 for d(xi, yj∙) = 0 and πj = 0 for
d(xi, yj) > 0. Therefore, when μ = V, we have FRWDp(μ, V) = 0.
Triangle Inequality
Let μ = Pi=I aiδxi, V = Pj=I bjδyj , Y = Pk=I Ckδzk and α ∈ ς', WePrOve that
FRWDp(μ,γ) ≤ FRWDp (μ, v) + FRWDP(V,γ).
To simplify the notations in this proof, we define d` as the distance “matrix" such that [D']ij-=
d(xi'), yj')) is the ith-row and jth-column element of the matrix d`, [D']jk = d(yj'), zk')), and
[D']ik = d(xi'), zk')). Moreover, note that DP is the "matrix," where each element is an element
of D' raised to the power p.
Consider that P ∈ U(μ, ν) is the optimal transport plan of FRWDp(μ, V), and Q ∈ U(ν, γ) is the
optimal transport plan of FRWDp (V, γ), where γ = Prk=1 ciδzi is a discrete measure. Similar to
the proof for the Wasserstein distance in (Peyre et al., 2019), let S = Pdiag(1∕b)Q with b be a
vector such that bj = bj if bj > 0, and bj = 1 otherwise. We can show that S ∈ U(μ,γ).
12
Under review as a conference paper at ICLR 2021
1	1
L	L	∖ P (L
Remi(n,Y)EαehR,Dp>) ≤ (Eα烙琢	ɑ` £Sik[De]ffc
1
P
≤ (£ ɑ` ∑[D']fk E - Y = (⅛ ɑ` £[D']Pk jk] P
∖'=1 ik	j	bj )	∖'=1	ijk	bj )
≤ (E ɑ` E([D' ]j + [D' ]jk )p j j
∖'=1 ijk	bj )
By letting gijk` = [D']j (ɑ`pjjk/bj )1/p and hjk` = [d` ]j (ɑ`pjjk/bj )1/p,the right-hand side
of this inequality can be rewritten as
1
p
(E ɑ` E([D']ij + [D']jk)p- j =
∖'=1	ijk	bj )
≤
E E(j + hijk`)
'=1 ijk
1	1
p L L	∖ p
+ 忤 j %。
E ɑ` E[D']j j Y + (E a` ED© j Y
'=1 ijk	bj J ∖'=1 ijk	bj J
E E gPk'
≤
by the Minkovski inequality.
1
p
(.
min
∖R∈U(μ,γ)
L
∑>ehRDPi
2=1
≤
Eα' E[D']pj pij E 等Y + (E a'E[DeIpkqjk E Pj Y
'=1	ij	k bj )	∖e=1 ik	j bj )
≤
,	1
E αeE[D']j Pi+ (E
e=ι ij	) v=ι
1
a ED ©
ik
≤
m∑L E ɑ' E[De]pjPij) + ( m∑L E a ED']pk9jk I
e=ι	ij	/	∖	e=ι	ik	)
≤ FRWDp(μ, V) + FRWDp(V Y)
This inequality is valid for all α. Therefore, we have
FRWDp(μ, v) ≤ FRWDp(μ, V) + FRWDp(V Y)
口
FROT WITH LINEAR PROGRAMMING
Linear Programming: The FROT is a convex piecewise-linear minimization because the objective
is the max of linear functions. Thus, we can solve the FROT problem via linear programming:
min t, s.t. h∏, C^ ≤ t,' = 1, 2,..., L.
Π∈U (μ,ν),t
13
Under review as a conference paper at ICLR 2021
This optimization can be easily solved using an off-the-shelf LP package. However, the computa-
tional cost of this LP problem is high in general (i.e., O(n3), n = m).
The FROT problem can be written as
min max h∏, C'i,
∏	'∈{1,2,...,L}
s.t. Π1m = a,Π>1n = b,Π ≥ 0.
This problem can be transformed to an equivalent linear program by first forming an epigraph prob-
lem:
min t,
Π,t
s.t. max h∏, C'〉≤ t
'∈{1,2,...,L}
Π1m = a, Π>1n =b,Π ≥0.
Thus, the linear programming for FROT is given as
min t
Π,t
s.t. h∏, C'i ≤ t,' = 1,2,...,L
Π1m = a,Π>1n = b,Π ≥ 0.
Next, we transform this linear programming problem into the canonical form. For matrix Π =
(π1 π2 . . . πn)> ∈ Rn×m and πi ∈ Rn, we can vectorize the matrix using the following linewise
operator:
vec(Π) = (π1> π2> . . . πn>)> ∈ Rnm.
Using this vectorization operator, We can write h∏, CG ≤ t as
vec(C1)>	-1		
vec(C2)> . .	-1 . .	(vec(Π)	≤ 0L,
. vec(CL)>	. -1		
where 0L ∈ RL is a vector whose elements are zero.
For the constraints Π1m = a and Π>1n = b, we can define vectors q1, . . . , qn ∈ Rnm and
r1, . . . , rm ∈ Rnm such that qi>vec(Π) = ai and rj>vec(Π) = bj in this way:
q1 = (1m, 0m, . . . , 0m) ,
q2 = (0m, 1m, . . . , 0m) ,
qn
(0m, 0m, . . . , 1m)
>
and
r1 = (1, 0m-1, 1, 0m-1 , . . . , 1, 0m-1) ,
r2 = (0,1,0>m-1,1,0>m-1,...,1,0>m-2)>,
We can collect these vectors to obtain the vectorized constraints:
(q> 0 ∖	( r>
q> 0( vec(Π) ) = a	r>
q.n> 0.	r.m>
0
0
vec(Π)
t
0
14
Under review as a conference paper at ICLR 2021
Thus, we can rewrite the linear programming as
min e>u
u
s.t. (A> - 1L)u ≤ 0L, (Q> 0n)u = a, (R> 0m)u = b, u ≥ 0,
where u = (vec(Π)> t)> ∈ Rnm+1, e = (0n>m 1)> ∈ Rnm+1 is the unit vector whose nm + 1-
th element is 1, A = (vec(C1),. . . ,vec(CL)) ∈ Rnm×L, Q = (q1, . . . ,qn) ∈ Rnm×n, and
R= (r1,...,rm) ∈ Rnm×m. Q= (In In ...In)
Proof of Lemma 2
We optimize the function with respect to α:
max J(α)
α
s.t. α>1K = 1, α1, . . . , αK ≥ 0,
where
LL
J (α) = Ea'φ' - nfa` (log a` - 1).	(8)
'=1	'=1
Because the entropic regularization is a strong convex function and its negative counterpart is a
strong concave function, the maximization problem is a concave optimization problem.
We consider the following objective function with the Lagrange multiplier :
LL
J(α) = fagφ' - nɪ^a`(log a` - 1) + e(α>1κ - 1)
'=1	'=1
Note that owing to the entropic regularization, the nonnegative constraint is automatically satisfied.
Taking the derivative with respect to ɑ`, We have
∂Je(α)	1
-T；--- = φ' - η log ɑ` - 1 + a` ——
da`	∖	a`)
+ = 0.
Thus, the optimal a` has the form
a`
a`
satisfies the sum to one constraint.
exp
Hence, the optimal a` is given by
exp (JI Φ')
PL=I exP (1 φ'0)
Substituting this into Eq.(8), we have
L
J (α*) = ∑
'=1
L
φ'- n£
'=1
η log
exp (1 φ') ) + η
15
Under review as a conference paper at ICLR 2021
Therefore, the final objective function is given by
J (α*) = η log
+η

Proof of Proposition 3
Proof: For 0 ≤ θ ≤ 1 and η > 0, we have
Xexp (1 hθ∏ι + (1 — θ)∏2,D'i) = XXXexp (θh∏ι, D'i + ʒ^)h∏2,D'i)
L	1	θ 1	1-θ
=N exp In h∏ι,D'i) exp In h∏2, D'i j
≤(X exp (η hπ1,D'i
1-θ
exp (n hπ2, D'i))
Here, We use Holder,s inequality withP =1∕θ, q = 1/(1 - θ), and 1/p +1/q = 1.
Applying a logarithm on both sides of the equation and then premultiplying η, we have
ηlog (XXexp (ηhθ∏ι + (1 - θ)∏2, D'i)) ≤ θηlog (XXexp (1 h∏ι,D'>))
+ (1- θ)η log (Xexp (η h∏2, D'i

Proof of Proposition 4
Theorem 5 (Jaggi, 2013) For each t ≥ 1, the iterates Π(t) of Algorithms 1, 2, 3, and 4 in (Jaggi,
2013) satisfy
f(Π(t)) - f(∏*) ≤ t2Cf2(1 + δ),
where Π* ∈ D is an optimal solution to problem
Π* = argmin f (∏),
Π∈D
Cf is the curvature constant defined as
2
Cf= SUP F (f(∏0)- f(∏)-h∏0- ∏, Vf (Π)i
Π,Πb ,γ γ
s.t. Π, Πb ∈ D, γ ∈ [0, 1], Π = Π + γ (Πb - Π),
δ ≥ 0 is the accuracy with which internal linear subproblems are solved.
Lemma 6 (Jaggi, 2013) Let f be a convex and differentiable function with its gradient Vf being
Lipschitz-continuous w.r.t. some norm k ∙ k over the domain D with Lipschitz-constant L > 0. Then,
Cf ≤ diamk∙k(D)2L
16
Under review as a conference paper at ICLR 2021
Definition 7 The softmax function is given by
σ(z)
1
PL0=1 exp(λze)
/ exp(λzι) ∖
exp(λz2)
.
.
.
exp(λzL)
where λ > 0 is referred to as the inverse temperature constant.
Lemma 8 (Gao & Pavel, 2017) The Softmaxfunction σ(∙) is L-Lipschitz with respect to ∣∣ ∙ ∣∣2 with
L = λ, that is for all z, z0 ∈ Rn,
∣σ(z) -σ(z0)∣2 ≤ λ∣z-z0∣2,
where λ is the inverse temperature constant.
The derivative of Gη(Π) is given as
∂Gη(口)_ X exp 1hgCe'i	C
c	c C =	M.
dπ	£1 PL=I exp (η h∏, CQ
Thus, we have
vec(MΠ) — ΦpΠ,
where
Φ — (vec(C1),vec(C2),...,vec(CL)) ∈ Rnm×L,
/	exp (1 h∏, Ci))	exp (ɪ h∏, CLi)	∖> L
Pn —	τ~∣v,...,	∈R .
∖∑L0=1 exp (1 h∏, C,i)	EL=I exp (1 h∏, CGi))
Here, Pn is the Softmax function σ(z) with z` — (Π, CQ.
We have
∣VGη(Π) - VGn(Π0)∣∣2 — ∣∣Φpn - Φp∏k2
≤ ∣Φ∣op ∣Pn - Pn0 ∣2 ,
≤ 1 kΦkopkΦ>vec(∏) — Φ>vec(Π0)k2 (Lemma 8 with λ — 1)
ηη
≤ 1 ∣ΦkopkΦ>kopkvec(∏)-vec(Π0)k2
where ∣∣ ∙ ∣op is the operator norm. We have ∣∣Φ∣∣op — ∣∣Φ>∣op, ∣∣Φ>Φ∣op — ∣Φ∣o2p, and
∣vec(Π) - vec(Π0)∣2 ≤ √2. Therefore, the LiPschitz constant for the gradient is L — 1 ∣∣Φ∣2p —
1 σmax(Φ>Φ), and the curvature constant is bounded above by Cf ≤ 2L, where σmaχ(Φ>Φ) is the
largest eigenvalue of the matrix Φ>Φ. By plugging Cf in Theorem 5, we have
Gn(∏(t)) — Gn(∏*) ≤ ^max+^% +。).
Max/Min formulation
We define the max-min formulation of the FROT as
L	nm
α∈∑L ∑α' n∈m%'Q "jC(X化 j
'=1	i=1j=1
where ΣL — {α ∈ RL+ : α>1L — 1} is the probability simplex, the set of probability vectors in
RL.
17
Under review as a conference paper at ICLR 2021
This problem can be solved by computing the group that maximizes the optimal transport distance
k* = argmax 卜 Wι(μ('), ν(')) and then by considering α* = δk* as a one-hot vector.
The result of this formulation provides an intuitive idea (the same as for the robust Wasserstein
method). Hence, we maximize the group (instead of the subspace) that provides the optimal result.
However, the formulation requires solving the OT problem L times. This approach may not be
suitable if we have a large L. Moreover, the argmax function is generally not differentiable.
Relation to the max-sliced Wasserstein distance: The max-sliced Wasserstein-2 distance can be
defined as (Deshpande et al., 2019)
1
n	n m	∖ 2
max-W2(μ,ν) = max min	∏j (w>Xi — w>yj )2	,
I w∈Ω π∈u(a`,b`) = j=1 八	J J
where Ω ⊂ Rd is the set of all possible directions on the unit sphere.
The max-sliced Wasserstein is a max-min approach. That is, for each w, it requires solving the
OT problem. The max-min approach is suited for simply measuring the divergence between two
distributions. However, it is difficult to interpret features using the max-sliced Wasserstein, where
it is the key motivation of FROT.
Relation to Subspace Robust Wasserstein (Paty & Cuturi, 2019): Here, we show that
2-FRWD with d(x, y) = ∣∣x — y∣∣2 is a special case of SRW. Let us define U =
(√Oιeι, √α^e2,..., √0ded)> ∈ Rd×d, where e` ∈ Rd is the one-hot vector whose 'th element is
1 and α> 1 = 1, a` ≥ 0. Then, the objective function of SRW can be written as
nm	nm
i=1 j=1 πij ∣U>xi — U>yj ∣22 = X1 X1 πij(xi — yj)>UU> (xi — yj)
nm
=	πij(xi — yj)>diag(α)(xi — yj)
i=1 j=1
nm	d
=XX TijX αe(x" y*.
i=1j=1	'=1
Therefore, SRW and 2-FRWD are equivalent if we set U = (√OTeι, √02e2,..., √0ded)> and
d(x, y) = ∣x — y∣2.
18
Under review as a conference paper at ICLR 2021
Table 2: Computational time comparison (s) for feature selection from biological datasets.
Data	d	n	WaSSerStein (Sort)	Linear	MMD	FROT
Colon	2000	-62-	12.57 (± 3.27)	0.00 (± 0.00)	1.36 (± 0.15)	0.41 (± 0.07)
Leukemia	7070	72	46.76 (± 19.47)	0.01 (± 0.00)	5.03 (± 0.79)	1.13 (± 0.14)
ProState-GE	5966	102	51.99 (± 16.37)	0.02 (± 0.00)	6.01 (± 1.17)	1.04 (± 0.11)
GLL85	22283	85	142.1 (± 21.65)	0.04 (± 0.00)	23.6 (± 1.21)	3.44 (± 0.36)
(a) FROT (η = 0.3).
Figure 5: One-to-one matching results of FROT (η = 0.3) and feature importance of FROT.
(b) Feature importance of FROT.
Additional Semantic Correspondence Experiments
Figure 5a shows an example of key points matched using the FROT algorithm. Fig.5b shows the
corresponding feature importance. The lower the η value, the smaller the number of layers used.
The interesting finding here is that the selected important layer in this case is the third layer from
the last. More qualitative results are presented in the Figure 7.
19
Under review as a conference paper at ICLR 2021
Figure 6: Average PCK scores on the validation split of SPair-71k with different pairs of hyperpa-
rameters.
Table 3: Per-class PCK (αbbox = 0.1) results using the SPair-71k. All models use ResNet101 as the
backbone.
Authors’
original
models
SPair-71k
finetuned
models
SPair-71k
validation
Without
SPair-71k
validation
Methods
CNNGeo (Rocco et al., 2017)
A2Net (Hongsuck Seo et al., 2018)
WeakAlign (Rocco et al., 2018a)
NC-Net (Rocco et al., 2018b)
CNNGeo
A2Net
WeakAlign
NC-Net
HPF
OT-HPF
OT
FROT (η = 0.2)
FROT (η = 0.3)
FROT (η = 0.4)
FROT (η = 0.5)
FROT (η = 0.6)
FROT (η = 0.7)
FROT (η = 0.8)
FROT (η = 0.9)
FROT (η = 1.0)
FROT (η = 2.0)
FROT (η = 3.0)
FROT (η = 4.0)
FROT (η = 5.0)
SRW (k = 10, = 0.1, T = 10, layer=34)
SRW (k = 20, = 0.1, T = 10,layer=34)
SRW (k = 30, = 0.1, T = 10,layer=34)
SRW (k = 40, = 0.1, T = 10,layer=34)
SRW (k = 50, = 0.1, T = 10, layer=34)
SRW (k = 60, = 0.1, T = 10, layer=34)
SRW (k = 70, = 0.1, T = 10, layer=34)
SRW (k = 80, = 0.1, T = 10, layer=34)
SRW (k = 90, = 0.1, T = 10, layer=34)
SRW (k = 100, = 0.1, T = 10, layer=34)
SRW (k = 50, e = 0.1,T = 3, layers = {1, 32-34})
SRW (k = 501e = OLT = 3, layers = {1, 31-34，)
SRW (k = 50, = 0.1, T = 3, layers = {1, 30-34})
aero	bike	bird	boat	bottle	bus	car	cat	chair	cow	dog	horse	moto	person	plant	sheep	train	tv	all
21.3	15.1	34.6	12.8	31.2	26.3	24.0	30.6	11.6	24.3	20.4	12.2	19.7	15.6	14.3	9.6	28.5	28.8	18.1
20.8	17.1	37.4	13.9	33.6	29.4	26.5	34.9	12.0	26.5	22.5	13.3	21.3	20.0	16.9	11.5	28.9	31.6	20.1
23.4	17.0	41.6	14.6	37.6	28.1	26.6	32.6	12.6	27.9	23.0	13.6	21.3	22.2	17.9	10.9	31.5	34.8	21.1
24.0	16.0	45.0	13.7	35.7	25.9	19.0	50.4	14.3	32.6	27.4	19.2	21.7	20.3	20.4	13.6	33.6	40.4	26.4
23.4	16.7	40.2	14.3	36.4	27.7	26.0	32.7	12.7	27.4	22.8	13.7	20.9	21.0	17.5	10.2	30.8	34.1	20.6
22.6	18.5	42.0	16.4	37.9	30.8	26.5	35.6	13.3	29.6	24.3	16.0	21.6	22.8	20.5	13.5	31.4	36.5	22.3
22.2	17.6	41.9	15.1	38.1	27.4	27.2	31.8	12.8	26.8	22.6	14.2	20.0	22.2	17.9	10.4	32.2	35.1	20.9
17.9	12.2	32.1	11.7	29.0	19.9	16.1	39.2	9.9	23.9	18.8	15.7	17.4	15.9	14.8	9.6	24.2	31.1	20.1
25.2	18.9	52.1	15.7	38.0	22.8	19.1	52.9	17.9	33.0	32.8	20.6	24.4	27.9	21.1	15.9	31.5	35.6	28.2
32.6	18.9	62.5	20.7	42.0	26.1	20.4	61.4	19.7	41.3	41.7	29.8	29.6	31.8	25.0	23.5	44.7	37.0	33.9
30.1	16.5	50.4	17.3	38.0	22.9	19.7	54.3	17.0	28.4	31.3	22.1	28.0	19.5	21.0	17.8	42.6	28.8	28.3
34.0	17.2	55.6	19.7	39.6	24.3	19.9	57.9	15.8	33.1	34.0	24.8	26.1	28.5	23.1	21.2	43.4	33.6	30.8
35.0	20.9	56.3	23.4	40.7	27.2	21.9	62.0	17.5	38.8	36.2	27.9	28.0	30.4	26.9	23.1	49.7	38.4	33.7
34.0	18.7	57.0	20.0	39.9	25.9	19.7	61.6	17.2	38.1	36.6	26.5	26.6	27.4	26.8	22.6	49.8	38.4	32.8
34.1	18.8	56.9	19.9	40.0	25.6	19.2	61.9	17.4	38.7	36.5	25.6	26.9	27.2	26.3	22.1	50.3	38.6	32.8
33.8	19.3	56.5	19.9	39.9	25.9	19.2	62.3	17.7	38.4	36.6	26.0	27.2	27.0	26.1	22.2	50.1	39.2	32.8
33.4	19.4	56.6	20.0	39.6	26.1	19.1	62.4	17.9	38.0	36.5	26.0	27.5	26.5	25.5	21.6	49.7	38.9	32.7
33.2	19.0	56.2	19.8	39.4	26.2	19.6	62.3	17.3	37.5	36.5	25.8	26.5	26.0	25.2	21.3	48.9	38.2	32.3
32.9	19.1	56.0	19.6	39.3	26.1	19.8	61.9	17.2	37.1	36.4	25.5	27.0	25.3	24.8	21.3	48.2	37.8	32.1
32.8	19.1	55.8	19.8	39.1	25.7	19.7	61.5	17.2	37.1	35.9	25.1	27.2	25.0	24.7	21.4	47.7	37.8	32.0
30.0	17.5	54.6	18.2	36.6	24.3	18.9	57.7	16.8	33.8	34.7	23.1	25.8	21.1	21.5	19.5	41.6	34.0	29.5
28.8	16.2	53.0	17.1	34.7	23.0	18.3	54.5	15.7	31.0	32.4	21.5	24.3	17.9	19.4	18.2	37.1	30.7	27.5
27.8	15.2	52.0	16.4	33.7	21.6	17.0	51.0	15.8	28.9	30.9	20.3	22.7	16.3	18.4	16.9	34.0	28.2	26.1
26.9	14.9	50.8	16.0	32.4	20.5	16.3	48.4	15.0	27.0	30.2	19.4	21.0	14.8	17.5	15.9	32.1	26.7	24.9
23.1	9.6	26.3	12.8	27.3	15.4	12.9	29.9	11.6	16.0	13.8	12.4	19.2	8.5	11.7	10.0	31.1	12.7	17.2
24.1	10.7	28.6	12.9	27.8	16.9	13.7	34.7	11.1	17.1	15.9	13.4	19.7	9.7	12.2	10.3	32.5	14.4	18.3
24.4	11.2	29.8	13.2	28.3	16.7	14.1	37.1	11.5	17.3	16.2	13.9	21.1	9.8	12.9	11.7	32.6	14.7	18.9
25.0	11.5	31.0	13.3	27.9	16.6	14.1	37.5	11.4	17.4	16.8	14.5	21.5	10.0	13.1	11.2	33.0	14.9	19.1
25.3	11.4	31.2	12.9	28.0	17.2	14.8	38.0	11.4	17.4	16.9	14.8	21.7	10.4	12.9	11.6	33.2	15.0	19.3
25.3	11.6	31.5	13.1	28.0	17.2	14.8	38.3	11.3	17.5	17.4	14.5	21.8	10.5	13.3	11.4	32.9	15.0	19.4
25.2	11.7	31.3	13.1	27.8	17.3	14.8	38.4	11.4	17.6	17.0	14.6	21.6	10.4	13.1	11.5	33.0	14.9	19.3
25.1	11.7	31.2	13.0	27.8	17.2	14.8	38.5	11.5	17.5	16.9	14.7	21.9	10.3	13.0	11.4	32.9	14.9	19.3
25.2	11.5	31.2	13.2	27.8	17.4	14.9	38.4	11.5	17.4	16.9	14.7	21.8	10.4	12.6	11.5	32.9	15.0	19.3
25.2	11.5	31.1	13.0	27.8	17.4	14.8	38.4	11.5	17.4	16.8	14.6	21.8	10.7	12.9	11.4	33.0	14.9	19.3
29.4	14.0	43.7	15.6	33.8	21.0	17.6	48.0	12.9	23.3	26.5	19.8	25.5	17.6	16.7	15.2	37.1	20.5	24.5
29.7	14.3	44.3	15.7	34.2	21.3	17.8	48.5	13.1	23.6	27.1	20.0	25.8	18.1	16.9	15.2	37.3	21.0	24.8
29.8	14.7	45.6	15.9	34.8	21.5	18.0	49.3	13.3	24.0	27.7	20.6	25.7	18.7	17.2	15.3	37.7	21.5	25.2
20
Under review as a conference paper at ICLR 2021
(a) FROT
Figure 7: Qualitative examples sampled from SPair-71k. For each image, the red points represent
the keypoints to be matched while the green points denote the prediction and the blue ones denote
(b) SRW
the ground truth.
21