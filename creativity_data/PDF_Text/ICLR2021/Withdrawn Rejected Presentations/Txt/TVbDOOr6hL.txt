Under review as a conference paper at ICLR 2021
Variational	Auto-Encoder	Architectures
that Excel at Causal Inference
Anonymous authors
Paper under double-blind review
Ab stract
This paper provides a generative approach for causal inference using data from
observational studies. Inspired by the work of Kingma et al. (2014), we propose
a sequence of three architectures (namely Series, Parallel, and Hybrid) that each
incorporate their M1 and M2 models as building blocks. Each architecture is an
improvement over the previous one in terms of estimating causal effect, culminating
in the Hybrid model. The Hybrid model is designed to encourage decomposing
the underlying factors of any observational dataset; this in turn, helps to accurately
estimate all treatment outcomes. Our empirical results demonstrate the superiority
of all three proposed architectures compared to both state-of-the-art discriminative
as well as other generative approaches in the literature.
1	Introduction
As one of the main tasks in studying causality (Peters et al., 2017; Guo et al., 2018), the goal of
Causal Inference is to figure out how much the value of a certain variable would change (i.e., the
effect) had another certain variable (i.e., the cause) changed its value. A prominent example is the
counterfactual question (Rubin, 1974; Pearl, 2009) “Would this patient have lived longer [and by how
much], had she received an alternative treatment?”. Such question is often asked in the context of
precision medicine, which attempts to identify which medical procedure t ∈ T will benefit a certain
patient x the most, in terms of the treatment outcome y ∈ R (e.g., survival time).
A fundamental problem in causal inference is the unobservablity of the counterfactual outcomes (Hol-
land, 1986). That is, for each subject i, any real-world dataset can only contain the outcome of the
administered treatment (aka the observed outcome: yi), but not the outcome(s) of the alternative
treatment(s) (aka the counterfactual outcome(s) ) — i.e., yit for t ∈ T \ {ti}. In other words, the
causal effect is never observed (i.e., missing in any training data) and cannot be used to train predictive
models, nor can it be used to evaluated a proposed model. This makes estimating causal effects a
more difficult problem than that of generalization in the supervised learning paradigm.
In general, we can categorize most machine learning algorithms into two general approaches, which
differ in how the input features x and their target values y are modeled (Ng & Jordan, 2002):
Discriminative methods focus solely on modeling the conditional distribution p(y|x) with the goal
of direct prediction of y for each instance x. For prediction tasks, discriminative approaches are often
more accurate since they use the model parameters more efficiently than generative approaches. Most
of the current causal inference methods are discriminative, including the Balancing Neural Network
(BNN) (Johansson et al., 2016), CounterFactual Regression Network (CFR-Net) (Shalit et al., 2017),
and CFR-Net’s extensions — cf., (Yao et al., 2018; Hassanpour & Greiner, 2019; 2020) — as well as
Dragon-Net (Shi et al., 2019).
Generative methods, on the other hand, describe the relationship between x and y by their joint
probability distribution p(x, y). This, in turn, would allow the generative model to answer arbitrary
queries, including coping with missing features x using the marginal distribution p(x) or [similar
to discriminative models] predicting the unknown target values y via p(y|x). A promising direction
forward for causal inference is developing generative models, using either Generative Adverserial
Network (GAN) (Goodfellow et al., 2014) or Variational Auto-Encoder (VAE) (Kingma & Welling,
2014; Rezende et al., 2014). This has led to two generative approaches for causal inference: GANs
for inference of Individualised Treatment Effects (GANITE) (Yoon et al., 2018) and Causal Effect
1
Under review as a conference paper at ICLR 2021
VAE (CEVAE) Louizos et al. (2017). However, neither of the two achieve competitive performance
in terms of treatment effect estimation compared to the discriminative approaches.
Although discriminative models have excellent predictive performance, they suffer from two draw-
backs: (i) overfitting, and (ii) making highly-confident predictions, even for instances that are “far”
from the observed training data. Generative models based on Bayesian inference, on the other hand,
can handle both of these drawbacks: issue (i) can be minimized by taking an average over the poste-
rior distribution of model parameters; and issue (ii) can be addressed by explicitly providing model
uncertainty via the posterior (Gordon & Herndndez-Lobato, 2020). Although the exact inference is
often intractable, efficient approximations to the parameter posterior distribution is possible through
variational methods. Here, we use the Variational Auto-Encoder (VAE) (Kingma & Welling, 2014;
Rezende et al., 2014) for the Bayesian inference component of our causal inference method.
Contribution: In this paper, we propose three interrelated Bayesian model architectures (namely
Series, Parallel, and Hybrid) that employ the VAE framework to address the task of causal inference
for binary treatments. We find that the best performing architecture is the Hybrid model, that is
[partially] successful in decomposing the underlying factors of any observational dataset. This is a
valuable property, as that means it can accurately estimate all all treatment outcomes. We demonstrate
that these models significantly outperform the state-of-the-art in terms of treatment effect estimation
performance on two publicly available benchmarks, as well as a fully synthetic dataset that allows for
detailed performance analyses.
2	Related works
CFR-Net Shalit et al. (2017) considered the binary treatment task and attempted to learn a
representation space Φ that reduces selection bias by making Pr( Φ(x) | t=0 ) and Pr( Φ(x) | t= 1 )
as close to each other as possible, provided that Φ( x ) retains enough information that the learned
regressors {ht (Φ(∙)) : t ∈{0,1}} can generalize well on the observed outcomes. Their objective
function includes L yi, hti Φ(xi) , which is the loss of predicting the observed outcome for sample
i (described as χ/ weighted by ωi = 2tu + 211--tu), where U = Pr( t = 1). This is effectively setting
ωi = 2 pr( t)where Pr( t ) is the probability of selecting treatment t over the entire population.
DR-CFR Hassanpour & Greiner (2020) argued against the standard implicit assumption that
all of the covariates X are confounders (i.e., contribute to both treatment assignment and outcome
determination). Instead, they proposed a graphical model similar to that in Figure 1 and designed a
discriminative causal inference approach accordingly — built on top of the CFR-Net. Specifically,
their model, named Disentangled Representations for CFR (DR-CFR), includes three representation
networks, each trained with constraints to insure that each component corresponds to its respective
underlying factor. While the idea behind DR-CFR provides an interesting intuition, it is known that
only generative models (and not discriminative ones) can truly identify the underlying data generating
mechanism. This paper is a step in this direction.
Dragon-Net Shi et al. (2019)’s main objective was to estimate the Average Treatment Effect
(ATE), which they explain requires a two stage procedure: (i) fit models that predict the outcomes for
both treatments; and (ii) find a downstream estimator of the effect. Their method is based on a classic
result from strong ignorability — i.e., Theorem 3 in (Rosenbaum & Rubin, 1983) — that states:
(y1, y0) ⊥ t | x	&	Pr( t = 1 | x ) ∈ (0, 1)	=⇒
(y1,y0) ⊥ 11 b(x)	&	Pr( t = 11 b(x)) ∈ (0,1)
where b(x) is a balancing score1. They consider propensity score as a balancing score and argue that
only the parts of X relevant for predicting T are required for the estimation of the causal effect 2. This
theorem only provides a way to match treated and control instances though — i.e., it helps finding
potential counterfactuals from the alternative group to calculate ATE. Shi et al. (2019), however, used
this theorem to derive minimal representations on which to regress to estimate the outcomes.
1That is, X ⊥ T | b(X) (Rosenbaum & RUbin,1983).
2The authors acknowledge that this would hurt the predictive performance for individual outcomes. As a
result, this yields inaccurate estimation of Individual Treatment Effects (ITEs).
2
Under review as a conference paper at ICLR 2021
Figure 1: Underlying factors of any observa-
tional dataset (Hassanpour & Greiner, 2020)
Figure 2: Graphical model of CEVAE
(Louizos et al., 2017)
GANITE Yoon et al. (2018) proposed the counterfactual GAN, whose generator G, given
{x, t, yt}, estimates the CoUnterfaCtUal outcomes (y-t); and whose discriminator D tries to identify
which of {[x, 0, y0], [x, 1, y1]} is the factual outcome. It is, however, unclear why this requires that
G mUst prodUce samples that are indistingUishable from the factUal oUtcomes, especially as D can
jUst learn the treatment selection mechanism instead of distingUishing the factUal oUtcomes from
coUnterfactUals. AlthoUgh this work is among the few generative approaches for caUsal inference, oUr
empirical resUlts (in Section 4) show that it does not effectively estimate coUnterfactUal oUtcomes.
CEVAE LoUizos et al. (2017) Used VAE to extract latent confoUnders from their observed proxies
in X . While this is an interesting step in the right direction, empirical resUlts show that it does
not always accUrately estimate treatment effect (see Section 4). The aUthors note that this may be
becaUse CEVAE is not able to address the problem of selection bias. Another reason that we think
contribUtes to CEVAE’s sUb-optimal performance is its assUmed graphical model of the Underlying
data generating mechanism (depicted in FigUre 2). This model assUmes that there is only one latent
variable Z (confoUnding T and Y ) that generates the entire observational data; however, we know
from (KUang et al., 2017) and (HassanpoUr & Greiner, 2020) that there mUst be more (see FigUre 1).
R2: M1 and M2 VAEs In an attempt to enhance the conventional representation learning with
VAES — referred to as the M1 model (Kingma & Welling, 2014; ReZende et al., 2014) — in a semi-
supervised manner, Kingma et al. (2014) ProPoSed the M2 VAE. While the M1 model helps learning
latent representations from the CoVariate matriX X alone, the M2 model allows the target information
also to guide the representation learning ProCeSs. In our work, the target information includes the
treatment bit T as Well as the ObSerVed outcome Y. ThiS additional information helps learning more
expressive representations, that was not possible with the unsupervised M1 model. Appendix A.1
presents a more detailed overview of the M1 and M2 VAEs. 3
3 Method
Following (Hassanpour & Greiner, 2020) and without loss of generality, we assume that the random
variable X follows an unknown joint probability distribution Pr( X | Γ, ∆, Υ, Ξ ), where Γ, ∆, Υ,
and Ξ are non-overlapping independent factors. Moreover, we assume that treatment T follows
Pr(T | Γ, ∆ ) (i.e., Γ and ∆ are the responsible factors for selection bias) and outcome YT follows
PrT ( YT | ∆, Υ ); see Figure 1. Observe that the factor Γ (resp., Υ) partially determines only
T (resp., Y), but not Y (resp., T); and ∆ includes the confounding factors between T and Y.
Our goal is to design generative model architectures that encourage learning disentangled repre-
sentations of these four underlying latent factors (see Figure 1). In other words, it is an attempt to
decompose and separately learn the underlying factors that are responsible for determining T and
Y. To achieve this, we propose three architectures (as illustrated in Figures 3(a), 3(b), and 3(c)),
each employing a VAE (Kingma & Welling, 2014; ReZende et al., 2014) that each include a decoder
(generative model) and an encoder (variational posterior). Specifically, we use the M1 and M2
models from (Kingma et al., 2014) as our building blocks, leading to a Series architecture, a Parallel
architecture, and a Hybrid one. Each component is parametriZed as a deep neural network.
3.1 The Variational Auto-Encoder Component
3.1.1 The Series Architecture
The architecture of the Series model is illustrated in Figure 3(a). LouiZos et al. (2015) proposed a
similar architecture to address fairness in machine learning, but using a binary sensitive variable S
3
Under review as a conference paper at ICLR 2021
(a) The Series model
Figure 3: Belief nets of the proposed architectures.
(e.g., gender, race, etc.) rather than the treatment T . Here, we employ this architecture for causal
inference and explain Why it should work. R4: We hypothesize that this structure functions as a
distillation tower: the bottom M2 VAE attempts to decompose Γ (guided by T) from ∆ and Y (cap-
ered by Z1); and the top M2 VAE attempts to learn ∆ and Y (guided by Y). Decoder and encoder
components of the Series model (parametrized by θs and φs respectively) involve the following
distributions:
Priors	Likelihood	Posteriors
Pθs (Z2) Pθs (Z1 Iy, Z2)	Pθs (x|Zi ,t)	qφs(Z1IX,t) qφs (yIZ1) qφs(Z2Iy,Z1)
The goal is to maximize the conditional log-likelihood of the observed data (left-hand-side of the
following inequality) by maximizing the Evidence Lower BOund (ELBO; right-hand-side) — i.e.,
NN
ElOg P(XiIti 加 ≥ EEqφs (zι |x,t) [ log Pθs (XiIzI i ,ti )]	⑴
i=1	i=1
-KL(qφs(Zi|x,t) ||Pθs(ZI|y,z2)) -kl(qφs(Z2|y,zi) ||pθ,(Z2))⑵
where KL denotes the Kullback-Leibler divergence, Pθs (z2) is the unit multivariate Gaussian (i.e.,
N (0, I)), and the other distributions are parameterized as deep neural networks.
3.1.2 The Parallel Architecture
R4: The SerieS model is composed of two M2 StaCked models. However, Kingma et al. (2014) ShOWed
that an M1+M2 StaCked architecture IearnS better representations than an M2 model alone for a
downstream prediction task. This motivated us to design a double M1+M2 Parallel model; where one
arm is for the outcome to guide the representation Iearning Via Z1 and another for the treatment to
guide that Via Z3. This architecture is illustrated in Figure 3(b). We hypothesize that Z1 would learn
∆ and Y, and Z3 would learn Γ (and perhaps partially ∆). Decoder and encoder components of the
Parallel model (parametrized by θp and φp respectively) involve the following distributions:
Priors	Likelihood	PoSteriorS
Pθp (Z2)	Pθp (Z4) Pθp(Z1Iy,Z2)	Pθp(Z3It,Z4)	Pθp (x|Z1 ,z3 )	qφp (ZI|x,t)	qφp (z3 |x,y) qφp (y|zi)	qφp (t|z3) qφp (z2 |y,zi) qφp (z4 lt,z3)
Here, the conditional log-likelihood can be upper bounded by:
NN
Elog P(XiIti ,yi ) ≥ EEqφp (zι ,Z3 ∣χ,t,y) [ log Pθp (XiIzI i , Z3 i)]	⑶
i=1	i=1
-kl(qφp(ZI|x,t) ||Pθp(ZI|y,z2)) -kl(qφp(Z2|y,zi) ||Pθp(Z2))⑷
-kl(qφp(Z3|x,y) ||Pθp(Z3|t,Z4)) -kl(qφp(Z4|t,Z3) ||Pθp(Z4))⑸
4
Under review as a conference paper at ICLR 2021
3.1.3 The Hybrid Architecture
R4: The final architecture, Hybrid, attempts to get the best CaPabilities of the Previous two architectures.
The backbone of the Hybrid model has a SerieS architecture, that SeParateS Γ (factors related to the
treatment T; CaPtured by the right module With Z3 as its head), from ∆ and Y (factors related to the
outcome Y; captured by the left module with Z7 as its head). The left module, itself, consists of a
Parallel model that attempts to PrOCeed one SteP further and decompose ∆ from Y. This is done with
the help of a discrepancy penalty (see Section 3.3). Figure 3(c) illustrates our designed architecture
for the Hybrid model. Decoder and encoder comPonents of the Hybrid model (Parametrized by θh
and φh respectively) involve the following distributions:
Priors	Likelihood	Posteriors
pθh(Z2)	pθh(Z4)	pθh (Z6) Pθh (Z1|y,Z2) Pθh (Z3|t,Z4) Pθh (Z5|y, z6) Pθh (Z7|Z1,Z5)	Pθh (x|Z3,Z7)	qφh(Z7|x,t) qφh(Zi|Z7)	qφh(和厮) qφh(Z3|x,y) qφh(y|Zi,Z5) qφh(t|Z3) qφh(Z2|y,Zi) qφh(Z6|y,Z5) qφh(Z4|t,Z3)
Here, the conditional log-likelihood can be upper bounded by:
NN
Elogp(xi∣ti,yi) ≥ EEqφh(z3,z7∣x,t,y) [logPθh(xi∣z3i,z7i)]	(6)
i=1	i=1
-	KL(qφh(ZIIQ) ||Pθh(Z1"z2D -kl(qφh(Z2|y,zI) ||pθ.(z2)] G)
-	kl(qφh(z3|x,y) ||Pθh(Z3LM) -kl(qφh(Z4|t,z3) ||pθ,(z4))⑻
-	kl(qφh(Z5|Z7) ||Pθh(Z5|y,Z6)) -kl(qφh(Z6|y,Z5) ||pθ,(Z6))⑼
-kl( qφh (Z7|x,t) || Pθh (Z7|Z1,Z5))	(IO)
The first term in the ELBO (i.e., right-hand-side of Equations (1), (3), or (6)) is called the Recon-
struction Loss (RecL) and the next term(s) (i.e., Equation (2), summation of Equations (4) and (5),
or summation of Equations (7), (8), (9), and (10)) is referred to as the KL R1: Divergence (KLD).
Concisely, the ELBO can be written as: RecL - KLD, which is to be maximized.
3.2	DISENTANGLEMENT WITH β-VAE
As mentioned earlier, we want the learned latent variables to be disentangled, to match our assumption
of non-overlapping factors Γ, ∆, and Y. To ensure this, we employ the β-VAE (Higgins et al., 2017)
which adds a hyperparameter β as a multiplier of the KLD part of the ELBO. This adjustable
hyperparameter facilitates a trade-off that helps balance the latent channel capacity and independence
constraints (handled by the KL terms) with the reconstruction accuracy — i.e., including the β
hyperarameter grants a better control over the level of disentanglement in the learned representations
(Burgess et al., 2018). Therefore, the generative objective to be minimized becomes:
LVAE =	-RecL + β ∙ KLD	(11)
Although Higgins et al. (2017) suggest the β to be set greater than 1 in most applications, Hoffman
et al. (2017) show that having a β < 1 weight on the KL term can be interpreted as optimizing the
ELBO under an alternative prior, which functions as a regularization term to prevent degeneracy.
3.3	Discrepancy
Although all the three proposed graphical models suggest that T and Z1 are statistically independent
(see, for example, the collider structure (at X): T → X — Zi in Figure 3(a)), an information leak is
quite possible due to the correlation between the outcome y and treatment t in the data. We therefore
require an extra regularization term on qφ(Zι∣t) in order to penalize the discrepancy (denoted by
disc) between the conditional distributions of Z1 given t = 0 versus given t = 1. To achieve this
regularization, we calculate the disc using an Integral Probability Metric (IPM) (Mansour et al.,
2009) c1 that measures the distance between the two above-mentioned distributions:
Ldisc =	IPM( {Z1}i,ti=0, {Z1}i,ti=1 )	(12)
c1In this work, we use the Maximum Mean Discrepancy (MMD) (Gretton et al., 2012).
5
Under review as a conference paper at ICLR 2021
3.4	Predictive Loss
Note, however, that neither the VAE nor the disc losses contribute to training a predictive model for
outcomes. To remedy this, we extend the objective function to include a discriminative term for the
regression loss of predicting y : c1
1N
Lpred	= N∑Sωi∙L[ yi, yi]	(13)
N i=1
where the predicted outcome yi is derived as the mean of the qφ (yi∣zii) posterior trained for the
respective treatment h; L [yi, yi] is the factual loss (i.e., L2 loss for real-valued outcomes and log
loss for binary-valued outcomes); and ωi represent the weights R1: that attempt to account for SeIeCtiOn
bias. We COnSider two approaches in the IiteratUre to derive the Weights:(i) the PoPuIation-Based (PB)
WeightS as proposed in CFR-Net (Shalit et al., 2017); and (ii) the Context-Aware (CA) WeightS as pro-
posed by HaSSanPOUr & Greiner (2019). Note that disentangling ∆ from Y is only beneficial When
USing the CA weights, SinCe We needjust the ∆ factors to derive them (HaSSanPoUr & Greiner, 2020).
3.5	Final Model(s)
Putting everything together, the overall objective function to be minimized is then:
J =	Lpred + α ∙ Ldisc + Y ∙ LVAE + λ ∙ Reg	(14)
where Reg penalizes the model complexity.
This objective function is motivated by the work of McCallum et al. (2006), which suggested
optimizing a convex combination of discriminative and generative losses would indeed improve
predictive performance. As an empirical verification, note that for γ = 0, the Series and Parallel
models effectively reduce to CFR-Net. However, our empirical results (cf., Section 4) suggest that
the generative term in the objective function helps learning representations that embed more relevant
information for estimating outcomes than that of Φ in CFR-Net.
We refer to the family of our proposed methods as VAE-CI (Variational Auto-Encoder for Causal
Inference); specifically: {S, P, H}-VAE-CI, for Series, Parallel, and Hybrid respectively.
4	Experiments, Results, and Discussion
4.1	Benchmarks
Infant Health and Development Program (IHDP) The original IHDP randomized controlled
trial was designed to evaluate the effect of specialist home visits on future cognitive test scores of
premature infants. Hill (2011) induced selection bias by removing a non-random subset of the treated
population. The dataset contains 747 instances (608 control and 139 treated) with 25 covariates. We
use the same benchmark (with 100 realizations of outcomes) provided by and used in (Johansson
et al., 2016) and (Shalit et al., 2017).
Atlantic Causal Inference Conference 2018 (ACIC’18) ACIC’18 is a collection of binary-
treatment datasets released for a data challenge. Following (Shi et al., 2019), we use a subset of the
datasets with instances N ∈ {1, 5, 10}× 103 (four datasets in each category). The covariates matrix
for each dataset involves 177 features and is sub-sampled from a table of medical measurements
taken from the Linked Birth and Infant Death Data (LBIDD) (MacDorman & Atkinson, 1998), that
contains information corresponding to 100,000 subjects.
Fully Synthetic Datasets We generated a set of synthetic datasets according to the procedure
described in (Hassanpour & Greiner, 2020); see Section A.2 for an overview. We considered all the
viable datasets in a mesh generated by various sets of variables, of sizes mΓ, m∆ , mΥ ∈ {0, 4, 8}
and mΞ = 1. This creates 24 scenariosc1 that consider all possible relative sizes of the factors Γ, ∆,
c1This is similar to the way (Kingma et al., 2014) included a classification loss in their Equation (9).
c1There are 33 = 27 combinations in total; however, we removed three of these combinations that generate
pure noise outcomes — i.e., ∆ = Υ = 0: (0, 0, 0), (4, 0, 0), and (8, 0, 0).
6
Under review as a conference paper at ICLR 2021
Figure 4: Performance analysis for decomposition of the underlying factors on the synthetic dataset
with mΓ,∆,Υ =8, mΞ = 1.
Table 1: IHDP (100 realizations) benchmark
Method	PEHE	ATE
CFR	0.75 (0.57)	0.08 (0.10)
DR-CFR	0.65 (0.37)	0.03 (0.04)
Dragon	NA	0.14 (0.15)
GANITE	2.81 (2.30)	0.24 (0.46)
CEVAE	2.50 (3.47)	0.18 (0.25)
S-VAE-CI	0.51 (0.37)	0.00 (0.02)
P-VAE-CI	0.52 (0.36)	0.01 (0.03)
H-VAE-CI (PB	)	0.49 (0.36)	0.01 (0.02)
H-VAE-CI (CA	卜)0.48 (0.35)	0.01 (0.01)
Table 2: ACIC’18 (N ≤ 10K) benchmark
Method	PEHE	ATE
CFR	5.13 (5.59)	1.21 (1.81)
DR-CFR	3.86 (3.39)	0.80 (1.41)
Dragon	NA	0.48 (0.77)
GANITE	3.55 (2.27)	0.69 (0.65)
CEVAE	5.30 (5.52)	3.29 (3.50)
S-VAE-CI	2.73 (2.39)	0.51 (0.82)
P-VAE-CI	2.62 (2.26)	0.37 (0.75)
H-VAE-CI (PB	)	1.78 (1.27)	0.44 (0.77)
H-VAE-CI (CA	l)	1.66 (1.30)	0.39 (0.75)
PEHE and ATE measures (lower is better) represented in the form of “mean (standard deviation)”.
and Υ. For each scenario, we synthesized multiple datasets with various initial random seeds in order
to allow for statistical significance testing of the performance comparisons between various methods.
4.2	Evaluating Identification of the Underlying Factors
To evaluate the identification performance of the underlying factors, we use a fully synthetic dataset
with mΓ =m∆ =mΥ =8 and mΞ = 1. We set x to be one of the four dummy vectors V1..4 and input it
to each trained representation network Zj . Three of these vectors had “1” in the 8 positions associated
with Γ, ∆, and Υ respectively, and the remaining 17 positions of each vector was filled with “0”. The
fourth vector was all “1” except for the last position (the noise) which was “0”. This helps measure
the maximum amount of information that is passed to the final layer of each representation network.
We let Oi,j be the elu output (here, ∈ R200) of the encoder network Zj when x= Vi. The average
of the 200 values of Oi,j (Avg(Oi,j )) represents the power of signal that was produced by the
Zj channel on the input Vi . The values shown in Figure 4’s tables are the ratios of Avg(O1,j ),
Avg(O2,j), and Avg(O3,j) divided by Avg(O4,j) for each of the learned representation networks.
Note that, a larger ratio indicates that the respective representation network Zj has allowed more of
the input signal Vi to pass through. Section A.3 includes more details on this procedure.c0
As expected, Z3 and Z4 capture Γ (e.g., the Z3 ratios for Γ in the {P, H}-VAE-CI tables are largest),
and Z1 , Z2 , Z5 , Z6 , and Z7 capture ∆ and Υ. Note that decomposition of ∆ from Υ has not been
achieved by any of the methods except for H-VAE-CI, which captures Υ by Z1 and ∆ by Z5 R4:
(note the ratios are largest for Z1 and Z5). This decomposition is vital for deriving context-aware
importance sampling weights because they must be calculated from ∆ only (HaSSanPoUr & Greiner,
2020). Also observe that both {P, H}-VAE-CI are able to separate Γ from ∆. However, DR-CFR,
which tried to disentangle all factors, failed not only to disentangle ∆ from Υ, but also Γ from ∆.
4.3	Evaluating Treatment Effect Estimation
Evaluation of treatment effect estimation is often done with semi- or fully- synthetic datasets that
include both factual and counterfactual outcomes. There are two categories of performance measures:
c0Unlike the evaluation strategy presented in (Hassanpour & Greiner, 2020) that only looked at the first layer’s
weights of each representation network, we propagate the values through the entire network and check how
much of each factor is exhibited in the final layer of every representation network. R1 & R4: Yet, the proposed
ProCedUre Stiu CrUdely evaluates the quality of disentanglement of the Underlying factors. We did explore USing
the Mutual Information (Belghazi et al., 2018) for this task (not shown here); however, it appears that it does not
work for high-dimensional data SUCh as ours. All in all, more research is needed to address this task.
7
Under review as a conference paper at ICLR 2021
Individual-based: “Precision in Estimation of Heterogeneous Effect” PEHE
Jq PN=I (ei-ei)2
uses ^i = yi - y0 as the estimated effect and ei = yi1 - y0 as the true effect (Hill, 2011); and
Population-based: “Bias of the Average Treatment Effect” ATE = ATE - AdTE, where ATE
N PN=I yi - NN PN=ι y0 and ATE is calculated based on the estimated outcomes.
In this paper, we compare performances of our proposed methods {S, P, H}-VAE-CI versus the
following treatment effect estimation methods: CFR-Net (Shalit et al., 2017), DR-CFR (Hassanpour
& Greiner, 2020), Dragon-Net (Shi et al., 2019), GANITE (Yoon et al., 2018), and CEVAE (Louizos
et al., 2017). The basic search grid for hyperparameters of the CFR-Net based algorithms (including
our proposed methods) is available in Section A.4. For the other algorithms, we searched around
their default hyperparameters’ setting. We ran the experiments for the contender methods using their
publicly available code-bases; note the following points:
•	Since Dragon-Net is designed to estimate ATE only, we did not report its performance results for
the PEHE measure (which, as expected, were significantly inaccurate).
•	The original GANITE code-base was implemented for binary outcomes only. We modified the
code (losses, etc.) such that it could process real-valued outcomes also.
•	We were surprised that CEVAE diverged when running on the ACIC’18 datasets. To avoid this, we
had to run the ACIC’18 experiments on the binary covariates only.
Tables 1, 2, and 3 summarize the mean and standard deviation of the PEHE and ATE measures
(lower is better) on the IHDP, ACIC’18, and Synthetic benchmarks respectively. VAE-CI achieves
the best performance among the contending methods. These results are statistically significant (in
bold; based on Welch’s unpaired t-test with α = 0.05) for the IHDP and Synthetic benchmarks.
Although VAE-CI also achieves the best performance on the ACIC’18 benchmark, the results are not
statistically significant due to the high standard deviation of the contending methods’ performances.
Figure 5 visualizes the PEHE measures on the entire synthetic datasets with sample size ofN = 10,000.
We observe that both plots corresponding to H-VAE-CI method (PB as well as CA) are inscribed
by plots of all other methods, showcasing H-VAE-CI’s superior performance under every possible
selection bias scenario. R1: Note that for SCenariOS Where m∆ = 0 (i.e., the ones of form m「_0_mY on
Perimeter of the radar Chart in FigUre 5), the performances of H-VAE-CI (PB) and H-VAE-CI (CA)
are almost identical. ThiS is expected, SinCe for these scenarios, the Iearned representation for ∆
would be degenerate, and therefore, the COntext-aware WeightS would reduce to popu山tion-based
ones.On the other hand, for ScenariOS Where m∆ = 0, the H-VAE-CI (CA) often PerfOrmS better
than H-VAE-CI (PB). ThiS Can be attributed to the fact that H-VAE-CI has CorreCtIy disentangled ∆
from Υ. ThiS facilitates Iearning good COntext-aware WeightS that better account for SeIeCtiOn bias,
WhiCh in turn, results in a better CaUSaI effect estimation performance.
We also performed hyperparameters’ sensitivity analyses in terms of PEHE (see Figure 6). We discuss
the results in the following:
•	For the α hyperparameter (i.e., coefficient of the discrepancy penalty), Figure 6(a) suggests that
DR-CFR and H-VAE-CI methods have the most robust performance R1&R4: throughout various
VaIUeS of α. ThiS is expected, because, UnIike CFR and {S, P}-VAE-CI, DR-CFR and H-VAE-CI
POSSeSS an independent node for representing △. ThiS helps them still CaPtUre ∆ as a grows; SinCe
for them, a only affects Iearning a representation of Υ.
ComParing H-VAE-CI (PB) With (CA), We observe that for all α > 0.01, H-VAE-CI (CA) outper-
forms H-VAE-CI (PB). ThiS is because the discrepancy PenaIty would force Zi to only CaPtUre
Y and Z5 to only CaPtUre △. ThiS results in deriving better CA WeightS (that should be Iearned
from △; here, from its Iearned representation Z5). H-VAE-CI (PB), on the Other hand, Cannot take
advantage of this disentanglement, WhiCh explains its SUb-OPtimaI performance.
•	According to Figure 6(b), various β values (i.e., coefficient of KL divergence penalty) R1 &R3&R4:
do not make much difference for H-VAE-CI (except for β ≥ 1; SinCe this large value means the
learned representations will be close to Gaussian noise). We initially thought using β-VAE might
help further disentangle the UnderIying factors. However, FigUre 6(b) SUggeStS that close-to-zero or
even zero βs also WOrk effectively. We now think that the H-VAE-CrS architecture itself SUffiCientIy
decomposes the Γ, △, and Y factors, without needing the help of a KLD penalty. Appendix A.5
includes more evidence and a detailed discussion on Why this interpretation should hold.
8
Under review as a conference paper at ICLR 2021
Figure 5: Radar graphs of PEHE (on the radii;
lower is better) for the entire synthetic benchmark
(24 × 3 with N = 10,000; each vertex denotes the
respective dataset). Figure is best viewed in color.
Table 3: PEHE and ATE measures (lower is
better) represented in the form of “mean (stan-
dard deviation)” on the entire synthetic bench-
mark (average performance of 24 × 3 datasets,
each with sample size of 10,000).
Method	PEHE	ATE
CFR	0.39 (0.08)	0.027 (0.020)
DR-CFR	0.26 (0.07)	0.007 (0.004)
Dragon	NA	0.007 (0.005)
GANITE	1.28 (0.43)	0.036 (0.015)
CEVAE	1.39 (0.32)	0.287 (0.217)
S-VAE-CI	0.28 (0.05)	0.004 (0.003)
P-VAE-CI	0.28 (0.05)	0.004 (0.003)
H-VAE-CI (PB)	0.20 (0.03)	0.003 (0.002)
H-VAE-CI (CA)	0.18 (0.02)	0.003 (0.002)
(a) α
(c) γ
Figure 6: Hyperparameters’ (x-axis) sensitivity analysis based on PEHE (y-axis) on the synthetic
dataset with mΓ,∆,Υ =8, mΞ = 1. Legend is the same as Figure 5. Plots are best viewed in color.
• R1&R4: For hyperparameter Y (i.e., coefficient of the generative loss penalty), H-VAE-CI achieves
the most Stable PerfermanCe COmPared to the {S, P}-VAE-CI models — See FigUre 6(c).Of
PartiCUIar interest is the SUPerior PerfOrmanCe of H-VAE-CI for Y ≤ 0.01 ComPared to that of
{S, P}-VAE-CI. ThiS means that having the generative loss term (i.e., LVAE) is more imPortant
for {S, P}-VAE-CI than for H-VAE-CI to Perform well - note an extreme CaSe haPPens at Y = 0,
Where the Iatter PerformS SignifiCantly better than the former. We hyPothesize that this is be-
CaUSe H-VAE-CI already IearnS exPressive rePresentations Z and ZR, meaning the oPtimization
no longer really requires the LVAE term to imPose that. This is in contrast to Zi in S-VAE-CI and
Zi and Z3 in P-VAE-CL
5 Future works and Conclusion
DesPite the sUCCess of the ProPosed methods, esPeCially the Hybrid model, in addressing CaUsal
inference, no known algorithms can yet learn to Perfectly disentangle factors ∆ and Υ. R1: This
goal is imPortant because We know isolating △, and learning ConteXt-aware WeightS from it, does
enhance the quality of the causal effect estimation PerformanCe — note the SUPerior PerformanCe of
H-VAE-CI (CA). R1&R3: The resUlts of oUr ablation StUdy (in FigUre 6(b)), however, revealed that the
CUrrentIy USed β-VAE does not helP With disentanglement of the UnderIying factors. ThiS shows that
We Can attribUte all the decomPosition We get to the ProPoSed architectUres and objective fUnction.
A fUtUre direction of this research is to exPlore the USe of better disentangling ConStraintS - e.g.,
WorkS of Chen et al.(2018) and LoPeZ et al.(2018) — to See if they woUld yield SharPer resUlts.
The goal of this PaPer was to estimate treatment effects (either for individUals or the entire PoPUlation)
from observational data. We designed three VAE-based (Kingma & Welling, 2014; Rezende et al.,
2014) architectUres (namely Series, Parallel, and Hybrid), that emPloyed (Kingma et al., 2014)’s
M1 and M2 models. The Hybrid model, as the best Performing architectUre, Partially sUcceeded at
decomPosing the Underlying factors Γ, △, and Υ; which helPed in accUrate estimation of treatment
oUtcomes. OUr emPirical resUlts demonstrated the sUPeriority of the ProPosed methods, comPared to
both state-of-the-art discriminative as well as generative aPProaches in the literatUre.
9
Under review as a conference paper at ICLR 2021
References
Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron
Courville, and Devon Hjelm. MINE: Mutual information neural estimation. In ICML, 2018.
Christopher Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and
Alexander Lerchner. Understanding disentangling in β-VAE. arXiv preprint:1804.03599, 2018.
Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of
disentanglement in variational autoencoders. In NeurIPS, pp. 2610-2620, 2018.
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,
Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.
Jonathan Gordon and Jose Miguel Hernandez-Lobato. Combining deep generative and discriminative
models for bayesian semi-supervised learning. Pattern Recognition, 100:107156, 2020.
Arthur Gretton, Karsten M Borgwardt, Malte J Rasch, Bernhard Scholkopf, and Alexander Smola. A
kernel two-sample test. JMLR, 13(March), 2012.
Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn, and Huan Liu. A survey of learning
causality with data: Problems and methods. arXiv preprint:1809.09337, 2018.
Negar Hassanpour and Russell Greiner. Counterfactual regression with importance sampling weights.
In IJCAI, 2019.
Negar Hassanpour and Russell Greiner. Learning disentangled representations for counterfactual
regression. In ICLR, 2020.
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,
Shakir Mohamed, and Alexander Lerchner. β-VAE: Learning basic visual concepts with a
constrained variational framework. ICLR, 2017.
Jennifer L Hill. Bayesian nonparametric modeling for causal inference. Journal of Computational
and Graphical Statistics, 20(1), 2011.
Matt Hoffman, Carlos Riquelme, and Matthew Johnson. The β-VAE's implicit prior. 2017. URL
http://bayesiandeeplearning.org/2017/papers/66.pdf.
Paul W Holland. Statistics and causal inference. Journal of the American statistical Association, 81
(396):945-960, 1986.
Fredrik Johansson, Uri Shalit, and David Sontag. Learning representations for counterfactual
inference. In ICML, 2016.
Diederik P Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised
learning with deep generative models. In NeurIPS, 2014.
Kun Kuang, Peng Cui, Bo Li, Meng Jiang, Shiqiang Yang, and Fei Wang. Treatment effect estimation
with data-driven variable decomposition. In AAAI, 2017.
Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Scholkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentan-
gled representations. In ICML, 2019.
Romain Lopez, Jeffrey Regier, Michael I Jordan, and Nir Yosef. Information constraints on auto-
encoding variational bayes. In NeurIPS, pp. 6114-6125, 2018.
Christos Louizos, Kevin Swersky, Yujia Li, Max Welling, and Richard Zemel. The variational fair
autoencoder. arXiv preprint:1511.00830, 2015.
10
Under review as a conference paper at ICLR 2021
Christos Louizos, Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling. Causal
effect inference with deep latent-variable models. In NeurIPS. 2017.
Marian F MacDorman and Jonnae O Atkinson. Infant mortality statistics from the 1996 period linked
birth/infant death dataset. Monthly Vital Statistics Report, 46(12), 1998.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning bounds
and algorithms. arXiv preprint:0902.3430, 2009.
Andrew McCallum, Chris Pal, Gregory Druck, and Xuerui Wang. Multi-conditional learning:
Generative/discriminative training for clustering and classification. In AAAI,, pp. 433-439, 2006.
Andrew Y Ng and Michael I Jordan. On discriminative vs. generative classifiers: A comparison of
logistic regression and naive bayes. In NeurIPS, pp. 841-848, 2002.
Judea Pearl. Causality. Cambridge University Press, 2009.
Jonas Peters, Dominik Janzing, and Bernhard Scholkopf. Elements of causal inference: foundations
and learning algorithms. MIT press, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. ICML, 2014.
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational
studies for causal effects. Biometrika, 1983.
Donald B Rubin. Estimating causal effects of treatments in randomized and nonrandomized studies.
Journal of Educational Psychology, 66(5), 1974.
Uri Shalit, Fredrik D. Johansson, and David Sontag. Estimating individual treatment effect: General-
ization bounds and algorithms. In ICML, 2017.
Claudia Shi, David Blei, and Victor Veitch. Adapting neural networks for the estimation of treatment
effects. In NeurIPS, 2019.
Liuyi Yao, Sheng Li, Yaliang Li, Mengdi Huai, Jing Gao, and Aidong Zhang. Representation learning
for treatment effect estimation from observational data. In NeurIPS, 2018.
Jinsung Yoon, James Jordon, and Mihaela van der Schaar. GANITE: Estimation of individualized
treatment effects using generative adversarial nets. In ICLR, 2018.
11
Under review as a conference paper at ICLR 2021
A	Appendix
A.1 M1 and M2 Variational Auto-Encoders
As the first proposed model, the M1 VAE is the conventional model that is used to learn representations
of data (Kingma & Welling, 2014; Rezende et al., 2014). These features are learned from the covariate
matrix X only. Figure 7(a) illustrates the encoder and decoder of the M1 VAE. Note the graphical
model on the left depicts the encoder; and the one on the right depict the decoder, which has arrows
going the other direction.
Proposed by Kingma et al. (2014), the M2 model was an attempt to incorporate the information
in target Y into the representation learning procedure. This results in learning representations that
separate specifications of individual targets from general properties shared between various targets.
In case of digit generation, this translates into separating specifications that distinguish each digit
from writing style or lighting condition. Figure 7(b) illustrates the encoder and decoder of the M2
VAE.
We can stack the M1 and M2 models as shown in Figure 7(c) to get the best results. This way, we
can first learn a representation Z1 from raw covariates, then find a second representation Z2, now
learning from Z1 instead of the raw data.
(a) M1 model
(b) M2 model
(c) M1+M2 model
Figure 7: Decoders (parametrized by θ) and encoders (parametrized by φ) of the M1, M2, and
M1+M2 VAEs.
A.2 Procedure of Generating the Synthetic Datasets
Given as input the sample size N; dimensionalities [mΓ, m∆, mΥ] ∈ Z+(3) ; for each factor L ∈
{ Γ, ∆, Y }, the means and covariance matrices (μL, ΣL); and a scalar Z that determines the slope of
the logistic curve.
•	For each latent factor L ∈ { Γ, ∆, Υ }, form L by drawing N instances (each of size mL) from
N(μL, ΣL). The covariates matrix X is the result of concatenating Γ, ∆, and Y. Refer to the
concatenation of Γ and ∆ as Ψ and that of ∆ and Υas Φ (for later use).
•	For treatment T, sample mΓ+m∆ tuple of coefficients θ fromN(0, 1)mΓ+m∆. Define the logging
policy as π0 (t = 1 | Z) = 1+exp(-ζz), where Z = Ψ ∙ θ. For each instance Xi, sample treatment ti
from the Bernoulli distribution with parameter π0( t= 1 | zi ).
•	For outcomes Y0 and Y1, sample m^ + mγ tuple of coefficients 俨 and "1 from N(0, i)mA+mY
Define y0 = (Φ ◦ Φ ◦ Φ + 0.5) ∙俨/(m& + mγ) + ε and y1 = (Φ ◦ Φ) ∙ ∂1 /(m& + mγ) + ε,
where ε is a white noise sampled from N (0, 0.1) and ◦ is the symbol for element-wise product.
A.3 Evaluating Identification of the Underlying Factors
Here, we elaborate on the procedure we followed to evaluate identification performance of the
underlying factors. We produced four dummy vectors Vi ∈ RmΓ+m∆ +mΥ+mΞ as depicted on the
left-side of Figure 8. The first to third vectors had ones (constant) in the positions associated with Γ,
12
Under review as a conference paper at ICLR 2021
∆, and Υ respectively, and the remainder of them were filled with zeroes. The fourth vector was all
ones, so we can measure the maximum amount of information that is passed to the final layer of each
representation network.
Figure 8: The four dummy x-like vectors (left); and the input/output vectors of the representation
networks (right).
In the next step, each vector Vi is fed to each trained network Zj , and the output Oi is recorded (see
the right-side of Figure 8). The average of Oi represents the power of signal that was communicated
from Vi and passed through the Zj channel. The values reported in the tables illustrated in Figure 4
are the ratios of {average of O1, O2, O3} divided by {average of O4} for all the learned representation
networks.
A.4 Hyperparameters
For all CFR, DR-CFR, and VAE-CI methods, we trained the neural networks with 3 layers (each
consisting 200 hidden neurons)c0, non-linear activation function elu , regularization coefficient of
λ=1E-4, Adam optimizer (Kingma & Ba, 2015) with a learning rate of 1E-3, batch size of 300, and
maximum number of iterations of 10, 000. See Table 4 for our hyperparameter search space.
Table 4: Hyperparameters and ranges
Hyperparameter	Range
Discrepancy coefficient α	{0,1E{-3,-2,-1, 0,1}}
KLD coefficient β	{0, 1E{-3, -2,-1,0, 1,2}}
Generative coefficient γ	j。，1E{-5,-4,-3,-2,-1,0}}
A.5 A DETAILED ANALYSIS OF THE EFFECT OF β
R1 & R3: OUr initial hypothesis in using β-VAE was that it might help further disentangle the under-
lying factors, in addition to the other constraint already in place (i.e., the architecture as well as
the discrepancy penalty). However, Figure 6(b) suggests that close-to-zero or even zero βS also
work effectively. To further explore this hypothesis, We examined the decomposition tables (similar
to Figure 4) of H-VAE-CI for extreme configurations with β = 0 and observed that they were all
effective at decomposing the underlying factors Γ, ∆, and Y (similar to the performance reported in
the green table in Figure 4). Figure 9 shows several of these tables.
R1 & R3: Our interpretation of this observation is that the H-VAE-CrS architecture already takes care
of decomposing the Γ, ∆, and Y factors, without needing the help of a KLD penalty. This means
either of the following is happening: (i) β-VAE is not the best performing disentangling method
and other disentangling COnStraintS should be USed instead — e.g., works of Chen et al. (2018) and
Lopez et al. (2018); or (ii) it is theoretically impossible to achieve disentanglement WithOUt some
c0R1 & R2: In addition to this basic configuration, We also Perform our grid SearCh With an UPdated number
of layers and/or number of neurons in each layer. This makes sure that all methods enjoy a similar model
complexity.
13
Under review as a conference paper at ICLR 2021
	H∙VAE∙CI		
Zl	Z2	Z3	Z4	ZS	Z6	Z7
Γ 0.2181	10.2185 1.8791	1.7711 0.2164	10.2190 0.2039
Δ [ 0.6041	0.6051 0.6142	0.6308 0.868S	0.8315 0,6138
丫 0.8523	0.8552 0.3321	0.3834 0.7552	0.7859 0.7384
	H∙VAE∙CI								
	Zl	Z2	Z3	Z4	ZS	Z6	Z7
Γ	0.2242	0.2182	0.7439	0.6770	0.2373	0.2583	0.2189
Δ	0.3014	0.2963	0.2612	03169	0.6051	0.5845	0.704S
Y	0.5385	0.5430	0.3221	0.3303	0.4394	0.4412	0.4571
	H∙VAE∙CI		
Zl	Z2	Z3	Z4	ZS	Z6	Z7
Γ 0.4254	0.4493 0.7090	0.6872 0.3823	0.3771 0.3874
Δ 10.6438	0.6461 0.2750	0.3129 0.7452	0.7569 0.8237
丫 0.7760	L1200 0.3137	0.3480 0.7240	0.7464 0.6717
	H∙VAE∙CI		
Zl	Z2	Z3	Z4	ZS	Z6	Z7
Γ 03646	0.3643 1.1457	0.8659 0.3942	0.4069 0.3166
Δ [0.5127	0.5307 0.6463	0.5794 0.7016	0.6717 0.7652
丫 0.5565	0.5780 0.4260	0.3964 0.4119	0.4234 0.4534
	H-VAEy								
	Zl	Z2	Z3	Z4	Z5	Z6	Z7
Γ	0.8821	0.8752	0.5S05	0.5782	0.3326	[0.3309	0.4006
Δ	1.2542	1.2480	0.2843	0.3488	0.8553	0.8568	0.9392
Y	1.914	L923	0.4498	0.4797	0.7791	0.7757	0.7969
	H-VAEy								
	Zl	Z2	Z3	Z4	Z5	Z6	Z7
Γ	0.0850	0.0875	1.8791	1.7711	0.1464	I 0.14S9	0.1833
Δ	0.6107	0.6085	0.6142	0.6308	0.7851	0.7937	0.6878
Y	0.8349	0.8242	0.3321	03834	0.5177	0.5073	0E832
Figure 9: Decomposition tables for H-VAE-CI with β=0.
SUPervision (LOcatellO et al., 2019), WhiCh might not be Possible to Provide in this task. ExPloring
these options is out of the scope of this PaPer and is left to future work.
14