Under review as a conference paper at ICLR 2021
Cubic Spline Smoothing Compensation for
Irregularly Sampled Sequences
Anonymous authors
Paper under double-blind review
Ab stract
The marriage of recurrent neural networks and neural ordinary differential networks
(ODE-RNN) is effective in modeling irregularly sampled sequences. While ODE
produces the smooth hidden states between observation intervals, the RNN will
trigger a hidden state jump when a new observation arrives and thus cause the
interpolation discontinuity problem. To address this issue, we propose the cubic
spline smoothing compensation, which is a stand-alone module upon either the
output or the hidden state of ODE-RNN and can be trained end-to-end. We derive its
analytical solution and provide its theoretical interpolation error bound. Extensive
experiments indicate its merits over both ODE-RNN and cubic spline interpolation.
1	Introduction
Recurrent neural networks (RNNs) are commonly used for modeling regularly sampled se-
quences (Cho et al., 2014). However, the standard RNN can only process discrete series without
considering the unequal temporal intervals between sample points, making it fail to model irregu-
larly sampled time series commonly seen in domains, e.g., healthcare (Rajkomar et al., 2018) and
finance (Fagereng & Halvorsen, 2017). While some works adapt RNNs to handle such irregular
scenarios, they often assume an exponential decay (either at the output or the hidden state) during the
time interval between observations (Che et al., 2018; Cao et al., 2018), which may not always hold.
To remove the exponential decay assumption and better model the underlying dynamics, Chen et al.
(2018) proposed to use the neural ordinary differential equation (ODE) to model the continuous
dynamics of hidden states during the observation intervals. Leveraging a learnable ODE parametrized
by a neural network, their method renders higher modeling capability and flexibility.
However, an ODE determines the trajectory by its initial state, and it fails to adjust the trajectory
according to subsequent observations. A popular way to leverage the subsequent observations is
ODE-RNN (Rubanova et al., 2019; De Brouwer et al., 2019), which updates the hidden state upon
observations using an RNN, and evolves the hidden state using an ODE between observation intervals.
While ODE produces smooth hidden states between observation intervals, the RNN will trigger a
hidden state jump at the observation point. This inconsistency (discontinuity) is hard to reconcile,
thus jeopardizing continuous time series modeling, especially for interpolation tasks (Fig. 1 top-left).
We propose a Cubic Spline Smoothing Compensation (CSSC) module to tackle the challenging
discontinuity problem, and it is especially suitable for continuous time series interpolation. Our CSSC
employs the cubic spline as a means of compensation for the ODE-RNN to eliminate the jump, as
illustrated in Fig. 1 top-right. While the latent ODE (Rubanova et al., 2019) with an encoder-decoder
structure can also produce continuous interpolation, CSSC can further ensure the interpolated curve
pass strictly through the observation points. Importantly, we can derive the closed-form solution
for CSSC and obtain its interpolation error bound. The error bound suggests two key factors for
a good interpolation: the time interval between observations and the performance of ODE-RNN.
Furthermore, we propose the hidden CSSC that aims to compensate for the hidden state of ODE-RNN
(Fig. 1 bottom), which not only assuage the discontinuity problem but is more efficient when the
observations are high-dimensional and only have continuity on the semantic level. We conduct
extensive experiments and ablation studies to demonstrate the effectiveness of CSSC and hidden
CSSC, and both of them outperform other comparison methods.
1
Under review as a conference paper at ICLR 2021
ODE-RNN
OUr methods
Figure 1: The illustration of ODE-RNN and our methods. The top left is ODE-RNN showing the
interpolation curve jump at the observation points. The top right is the smoothed output by our CSSC,
where the jump is eliminated, and the output strictly passes the observation. The bottom left shows
the ODE-RNN ’s discontinuous output caused by the hidden state discontinuity. The bottom right
shows that our hidden CSSC is applied to the hidden state of ODE-RNN, resulting in the smooth
hidden state and so as the output.
2	Related Work
Spline interpolation is a practical way to construct smooth curves between a number of points (De Boor
et al., 1978), even for unequally spaced points. Cubic spline interpolation leverages the piecewise
third order polynomials to avoid the Runge’s phenomenon (Runge, 1901) and is applied as a classical
way to impute missing data (Che et al., 2018).
Recent literature focuses on adapting RNNs to model the irregularly sampled time series, given their
strong modeling ability. Since standard RNNs can only process discrete series without considering
the unequal temporal intervals between sample points, different improvements were proposed. One
solution is to augment the input with the observation mask or concatenate it with the time lag ∆t
and expect the network to use interval information ∆t in an unconstrained manner (Lipton et al.,
2016; Mozer et al., 2017). While such a flexible structure can achieve good performance under some
circumstances (Mozer et al., 2017), a more popular way is to use prior knowledge for missing data
imputation. GRU-D (Che et al., 2018) imputes missing values with the weighted sum of exponential
decay of the previous observation and the empirical mean. Shukla & Marlin (2019) employs the
radial basis function kernel to construct an interpolation network. Cao et al. (2018) let hidden state
exponentially decay for non-observed time points and use bi-directional RNN for temporal modeling.
Another track is the probabilistic generative model. Due to the ability to model the missing data’s
uncertainty, Gaussian processes (GPs) are adopted for missing data imputing (Futoma et al., 2017;
Tan et al., 2020; Moor et al., 2019). However, this approach introduced several hyperparameters, such
as the covariance function, making it hard to fine-tune in practice. Neural processes (Garnelo et al.,
2018) eliminate such constraints by introducing a global latent variable that represents the whole
process. Generative adversarial networks are also adopted for imputing (Luo et al., 2018).
Recently, neural ODEs (Chen et al., 2018) utilize a continuous state transfer function parameterized by
a neural network to learn the temporal dynamics. Rubanova et al. (2019) combine the RNN and ODE
to reconcile both the new observation and latent state evolution between observations. De Brouwer
et al. (2019) update the ODE with the GRU structure with Bayesian inference at observations.
While the ODE produces the smooth hidden states between observation intervals, the RNN will
trigger a jump of the hidden state at the observation point, leading to a discontinuous hidden state
along the trajectory. This inconsistency (discontinuity) is hard to reconcile, thus jeopardizing the
modeling of continuous time series, especially for interpolation tasks. The neural CDE (Kidger
2
Under review as a conference paper at ICLR 2021
et al., 2020) directly apply cubic splines interpolation at the input sequence to make the sparse input
continuous and thus produce continuous output. On the contrary, our method tackles this jumping
problem by introducing the cubic spline as a compensation for the vanilla ODE-RNN, at either the
output or hidden space.
3	Methods
In this section, we first formalize the irregularly sampled time series interpolation problem (Sec. 3.1),
then introduce the background of ODE-RNN (Sec. 3.2). Based upon ODE-RNN, we present CSSC
and its closed-form solution (Sec. 3.3), and illustrate the inference and training procedure (Sec. 3.4).
Finally, we provide the interpolation error bound of CSSC (Sec. 3.5) and describe an useful extension
of CSSC (Sec. 3.6).
3.1	Problem Definition
We focus on the interpolation task. Given an unknown underlying function x(t) : R → Rd, t ∈ [a, b],
and a set of n + 1 observations {xk|xk = x(tk)}kn=0 ∈ Rd sampled from x(t) at the irregularly
spaced time points Π : a = t0 < t1 < ... < tn = b, the goal is to learn a function F(t) : R → Rd to
approximate x, such that F (tk) = xk.
3.2	Background of ODE-RNN
ODE-RNN (Rubanova et al., 2019) achieves the interpolation by applying ODE and RNN inter-
changeably through a time series, illustrated in top-left of Fig. 1. The function F on time interval
t ∈ [tk, tk+1) is described by a neural ODE with the initial hidden state h(tk):
,,. ,..
hɪ(t)= f(h(t));	(1)
o(t) = g(h(t)),	(2)
where the h ∈ Rm is the hidden embedding of the data, hi =需 is the temporal derivative of the
hidden state, o ∈ Rd is the interpolation output of F (t). Here, f : Rm → Rm and g : Rm → Rd are
the transfer function and the output function parameterized by two neural networks, respectively. At
the observation time t = tk, the hidden state will be updated by an RNN as:
h(tk) =RNNCell(h(tk-),xk);	(3)
o(tk) = g(h(tk)),	(4)
where the input x ∈ Rd, tk- and tk+ are the left- and right-hand limits oftk. The above formulation
has two downsides. The first is the discontinuity problem: while the function described by ODE
is right continuous o(tk) = o(tk+), the RNN cell in Eq. (3) renders the hidden state discontinuity
h(tk-) 6= h(tk+) and therefore output discontinuity o(tk-) 6= o(tk+). The second is that the model
cannot guarantee o(tk) = xk without explicit constraints.
3.3	Cubic Spline Smoothing Compensation
To remedy the two downsides, we propose the module Cubic Spline Smoothing Compensation (CSSC),
manifested in the top-right of Fig. 1. It computes a compensated output ^(t) as:
^(t) = c(t) + o(t),	(5)
where o(t) is the ODE-RNN output, and the c(t) is a compensation composed of piecewise continu-
ous functions. Our key insight is that adding another continuous function to the already piecewise
continuous o(t) will ensure the global continuity. For simplicity, we set c(t) as a piecewise polyno-
mials function and then narrow it to a piecewise cubic function since it is the most commonly used
polynomials for interpolation (Burden & Faires, 1997). As the cubic spline is computed for each
dimension of C individually, w.l,o.g., We will discuss one dimension of the o, c, O, X and thus denote
them as o, c, o, x, respectively. c(t) is composed with pieces as c(t) = Pn-1 ck(t) with each piece
Ck defined at domain [tk,tk+ι). To guarantee the smoothness, we propose four constraints to 0(t):
3
Under review as a conference paper at ICLR 2021
L o(t-) = o(t+) =Xk, k = 1,…，n 一 1, O(to) = xo, 0(tn) = Xn (output continuity);
2.	o>(t-) = o>(t+),k = 1,…, n 一 1 (first order output continuity);
3.	o(t-) = o(t+), k =1,…, n 一 1 (second order output continuity);
4.	o(to) = o(tn) = 0 (natural boundary condition).
The constraint 1 ensures the interpolation curves continuously pass through the observations. Con-
straint 2 and 3 enforce the first and second-order continuity at the observation points, which usually
holds when the underline curve X is smooth. And constraint 4 specifies the natural boundary condition
owing to the lack of information of the endpoints (Burden & Faires, 1997).
Given o(t) and such four constraints, c(t) has unique analytical solution expressed in Theorem 1.
Theorem 1.	Given the first order and second order jump difference of ODE-RNN as
r® = o(t+)- o(t-);	(6)
rk = o(t+)- o(t-).	⑺
where the analytical expression of 0 and o can be obtained as
..—,τ ∂2g j i ∂g T ∂f
0 = f ∂h2 f + ∂h ∂hf
(8)
and the error defined as
k+ = Xk 一 o(tk+);	(9)
k- = Xk 一 o(tk-),	(10)
then ck can be uniquely determined as
Ck (t)= Mk+1 + f+1- Mk (t-tk )3 + M (t-tk)2 +
6τk	2
(—f+ 一 Tk(Mk+1 +rk+1 + 2Mk)	tk) + e+,	(11)
τk	6
where Mk is obtained as
M = A-1d,	(12)
/ 2
μ2
A =
∖
λ1
2
λ2
..
..
μn-2	2
μn-1
/ Mi ∖
M2
.	,d
.
Mn-2
Mn-1
/ di ∖
d2
dn-2
dn-i
(13)
Tk = tk+i - tk，μk = Tk-—1τk，λk = Tk-T+τk，
e[t+,tk + i] = k+Tk k，MO = Mn = 0.
dk
6 e[t+,tk+ι]-e[t+-ι,tk ] + 6rk-2rkTk-1-rk+1Tk
τk — 1 + τk	τk-1+τk
The proof for Theorem. 1 is in Appx. A. The c(t) is obtained by computing each c(t) individually
according to Theorem. 1.
Computational Complexity. The major cost is the inverse of A, a tridiagonal matrix, whose inverse
can be efficiently computed in O(n) complexity with the tridiagonal matrix algorithm (implementation
detailed in Appx. C.1). Another concern is that 0 and 0 needs to compute Jacobian and Hessian
in Eq. (8). We can circumvent this computing cost by computing the numerical derivative or an
empirical substitution, detailed in Appx. C.2.
Model Reduction. Our CSSC can reduce to cubic spline interpolation if setting o in Eq. (5) as zero.
In light of this, we further analyze our model with techniques used for cubic spline interpolation and
experimentally show our advantages against it in Sec. 4.2.
4
Under review as a conference paper at ICLR 2021
3.4	Inference and Training
For inference, firstly compute the predicted value o from ODE-RNN (Eq. (1-4)), then calculate
the compensation C with CSSC(Eq. (11)); thus yielding smoothed output o (Eq. (5)). For training,
the CSSC is a standalone nonparametric module (since we have its analytical solution) on top of
the ODE-RNN that allows the end-to-end training for ODE-RNN parameters. We employ Mean
Squire Error (MSE) loss to supervise o. In addition, We expect the compensation C to be small to
push ODE-RNN to the leading role for interpolation and take full advantage of its model capacity.
Therefore a 2-norm penalty for C is added to construct the final loss:
1N
L = NFE(I∣x(ti)- ^(ti)∣∣2 + α∣∣c(ti)∣∣2)	(14)
i=1
The ablation study (Sec. 4.5) shows that the balance weight α can effectively arrange the contribution
of ODE-RNN and CSSC.
Gradient flow. Although C is non-parametric module, but the gradient can flow from it into the
ODE-RNN because c(t) depends on the left and right limit of o(tk), O(tk), O(tk). We further analyze
that O(tk) plays a more important role than O(tk) in the contribution to c(t), elaborated in Appx. C.3.
3.5	Interpolation Error B ound
With CSSC, we can even derive an interpolation error bound, which is hard to obtain for ODE-RNN.
Without loss of generality, we analyze one dimension of o, which is scalable to all dimensions.
Theorem 2.	Given the CSSC at the output space as Eq. (5), if x ∈ C4 [a, b], f ∈ C3, g ∈ C4, then
the error and the first order error are bounded as
||(x-0)(r)∣∣∞ ≤ Cr||(x-o)⑷I∣∞τ4-r, (r = 0,1),	(15)
where ∣∣∙∣∣∞ is uniform norm, (∙)r is r -th derivative, Co =羡，Ci ==，T is the maximum interval
over Π.
The proof of Theorem 2 is in Appx. B. The error bound guarantee the error can converge to zero
if τ → 0 or ||(x - o)(4) || → 0. This suggests that a better interpolation can come from a denser
observation or a better ODE-RNN output. Interestingly, Eq. (15) can reduce to the error bound for
cubic spline interpolation (Hall & Meyer, 1976) if o is set zero. Compared with ODE-RNN, which
lacks the convergence guarantee for τ , our model more effectively mitigates the error for the densely
sampled curve at complexity O(τ 4) ; compared with the cubic spline interpolation, our error bound
has an adjustable o that can leads smaller ||(x - o)(4) || than ||x(4) ||.
An implicit assumption for this error bound is that the x should be 4-th order derivable; hence this
model is not suitable for sharply changing signals.
3.6 Extend to interpolate hidden state
Although CSSC has theoretical advantages from the error bound, it is still confronted with two
challenges. Consider the example of video frames: each frame is high-dimensional data, and each
pixel is not continuous through the time, but the spatial movement of the content (semantic manifold)
is continuous. So the first challenge is that CSSC has linear complexity w.r.t. data dimension, which
is still computation demanding when the data dimension becomes very high. The second challenge is
that CSSC assumes the underlying function x is continuous, and it cannot handle the discontinuous
data that is continuous in its semantic manifold (e.g. video).
To further tackle these two challenges, we propose a variant of CSSC that is applied to the hidden
states, named as hidden CSSC, illustrated in bottom of Fig. 1. As we only compute the compensation
to hidden state, which can keep a fixed dimension regardless the how large the data dimension is, the
computational complexity of hidden CSSC is weakly related to data dimension. Also, the hidden
state typically encodes a meaningful low-dimensional manifold of the data. Hence smoothing the
hidden state is equivalent to smoothing the semantic manifold to a certain degree.
Hence, the Eq. (5) is modified to h(t) = C(t) + h(t), and the output (Eq. (2),(4)) to o(t) = g(h(t))
However, since there is no groundtruth for the hidden state like the constraint 1, we assume the h(tk+)
5
Under review as a conference paper at ICLR 2021
Figure 2: The visual result for Sinuous Wave dataset with 10 observations. (a) shows the comparison
of CSSC against other methods; (b) demonstrates the effect of C to smooth the o; (c) shows the hidden
CSSC output is smooth because its unsmooth hidden state h is smoothed into h.
are the knots passed through by the compensated curve, rendering the constraints 1 as h(tk )=
h(t+) = h(t+). The rationality for the knots is that h(tj) is updated by x(tk) and thus contain
more information than h(t-). Given the above redefined variable, the closed-form solution of C is
provided in Theorem. 3.
Theorem 3.	Given the second order jump at hidden state as
rk = h(t+) - h(t-)；	rk = h(t+) - h(t-),	(16)
where h = f, h = ∂∂h f, and the error defined as
k+ =	h(tk+)	-	h(tk+)	= 0;	k-	=	h(tk+)	- h(tk-),	(17)
the ck is uniquely determined as Eq. (11).
Theorem 3 suggests another prominent advantage: hidden CSSC can be more efficiently implemented
because its computation does not involve Hessian matrix.
4	Experiments
4.1	Baseline
We compare our method with baselines: (1) Cubic Spline Interpolation (Cubic Spline) (2) A classic
RNN where ∆t is concatenated to the input (RNN-∆t) (3) GRU-D (Che et al., 2018) (4) ODE-
RNN (Rubanova et al., 2019) which is what our compensation adds upon (5) Latent-ODE (Rubanova
et al., 2019) which is a VAE structure employing ODE-RNN as the encoder and ODE as decoder
(6) GRU-ODE-Bayes (De Brouwer et al., 2019) which extends the ODE as a continuous GRU. Our
proposed methods are denoted as CSSC for data space compensation and hidden CSSC for hidden
space compensation. Implementation Details is in Appx. D.
4.2	Toy Sinuous Wave Dataset
The toy dataset is composed of 1,000 periodic trajectories with variant frequency and amplitude.
Following the setting of Rubanova et al. (2019), each trajectory contains 100 irregularly-sampled
time points with the initial point sampled from a standard Gaussian distribution. Fixed percentages
of observations are randomly selected with the first and last time points included. The goal is to
interpolate the full set of 100 points.
The interpolation error on testing data is shown in Table 1, where our method outperforms all
baselines in different observation percentages. Figure 2 (a) illustrates the benefit of the CSSC over
cubic spline interpolation and ODE-RNN. Cubic spline interpolation cannot interpolate the curve
when observations are sparse, without learning from the dataset. ODE-RNN performs poorly when
a jump occurs at the observation. However, CSSC can help eliminate such jump and guarantee
smoothness at the observation time, thus yielding good interpolation. In Figure 2 (b), we visualized
the ODE-RNN output o and compensation c, and the CSSC output o in detail to demonstrate how the
the bad o becomes a good o by adding c. Finally, Fig. 2 (c) demonstrates the first dimension of hidden
states before and after smoothing, where the smoothed hidden state leads to a smoothed output.
6
Under review as a conference paper at ICLR 2021
Table 1: Interpolation MSE on toy, MuJoCo, Moving MNIST test sets with different percentages of
observations.
Observation	Toy			MuJoCo			Moving MNist	
	10%	30%	50%	10%	30%	50%	20%	30%
Cubic Spline	0.801249	0.003142	0.000428	0.016417	0.000813	0.000125	0.072738	0.252647
RNN-∆t	0.449091	0.245546	0.102043	0.028457	0.019682	0.008975	0.040392	0.037924
GRU-D	0.473954	0.247522	0.121482	0.056064	0.018285	0.008968	0.037503	0.035681
Latent-ODE	0.013768	0.002282	0.002031	0.010246	0.009601	0.009032	0.035167	0.031287
GRU-ODE-Bayes	0.117258	0.010716	0.000924	0.028457	0.006782	0.002352	0.034093	0.030975
ODE-RNN	0.025336	0.001429	0.000441	0.011321	0.001572	0.000388	0.022141	0.016998
hidden CSSC	0.027073	0.001503	0.000244	0.004554	0.000378	0.000106	0.019475	0.015662
CSSC	0.024656	0.000457	0.000128	0.006097	0.000375	0.000087	-	-
Figure 3: The visual result for MuJoCo. We visualize part of the interpreted trajectory with 5 frames
interval. 10% frames are observed out of 100 frames. The observation is highlighted with white box.
4.3	MuJoCo Physics Simulation
We test our proposed method with the "hopper" model provided by DeepMind Control Suite (Tassa
et al., 2018) based on MuJoCo physics engine. To increase the trajectory’s complexity, the hopper is
thrown up, then rotates and freely falls to the ground (Fig. 3). We will interpolate the 7-dimensional
state that describes the position of the hopper. Both our hidden CSSC and CSSC achieve improved
performance from ODE-RNN, especially when observations become sparser, as Tab. 1 indicates. The
visual result is shown in Fig. 3, where CSSC resemble the GT trajectory most.
4.4	MOVING MNIST
In addition to low-dimensional data, we further evaluate our method on high-dimensional image
interpolation. Moving MNIST consists of 20-frame video sequences where two handwritten digits
are drawn from MNIST and move with arbitrary velocity and direction within the 64×64 patches,
with potential overlapping and bounce at the boundaries. As a matter of expediency, we use a subset
of 10k videos and resize the frames into 32×32. 4 (20%) and 6 (30%) frames out of 20 are randomly
observed, including the starting and ending frames. We encode the image with 2 ResBlock (He et al.,
2016) into 32-d hidden vector and decode it to pixel space with a stack of transpose convolution
layers. Since the pixels are only continuous at the semantic level, only hidden CSSC is evaluated with
comparison methods. As shown in Tab. 1, the hidden CSSC can further improve ODE-RNN’s result,
and spline interpolation behaves the worse since it can only interpolate at the pixel space, which is
discontinuous through time. The visual result (Fig. 4) shows that the performance gain comes from
the smoother movement and the clearer overlapping.
4.5	Ablation Study
The effect of end-to-end training. Apart from our standard end-to-end training of CSSC, two
alternative training strategies are pre-hoc and post-hoc CSSN. Pre-hoc CSSC is to train a standard
CSSC but only use the ODE-RNN part when inference. On the contrary, post-hoc CSSC is to train
an ODE-RNN without CSSC, but apply CSSC upon the output of ODE-RNN when inference. The
7
Under review as a conference paper at ICLR 2021
Figure 4: The visual result for Moving MNIST. The observation is indicated in white box. The
comparison of discontinuity is highlighted in red box.
Table 2: The MSE for on MuJoCo test set for the study of different training strategies.
	10%	30%	50%
ODE-RNN	0.011321	0.001572	0.000388
pre-hoc CSSC	0.053217	0.006131	0.002062
post-hoc CSSC	0.013574	0.000514	0.000110
CSSC	0.006097	0.000375	0.000087
comparison of pre-hoc, post-hoc, and standard CSSC is presented in Tab. 2, where standard CSSC
behaves the best. The pre-hoc CSSC is the worst because training with CSSC can tolerate the error of
the ODE-RNN; thus, inference without CSSC exposes the error of ODE-RNN, and it even performs
worse than standard ODE-RNN. The post-hoc CSSC can increase the performance of ODE-RNN
with simple post-processing when inference. However, such performance gain is not guaranteed
when the observation is sparse. For example, the post-hoc CSSC even decreases the performance of
ODE-RNN in 10% observation setting. Standard CSSC has higher performance than ODE-RNN in
all our experiments, indicating the importance of end-to-end training.
The effect of α. We study the effect of α ranging from 0 to 10000 given different percentages of
observation on MuJoCo dataset. The performance of CSSC is quite robust to the choice of α, shown
in Tab. 4 (in Appendix), especially the MSE only fluctuated from 0.000375 to 0.000463 as α ranges
from 1 to 10000 in 30% observation setting. Interestingly, Fig. 5 (in Appendix) visually compares
the interpolation for o, c, and o under variant α and indicates higher α contributes to lower c, thus o
is more dominant of smoothed output o. On the other hand, the smaller α will make o less correlated
with the ground truth, but the CSSC C can always make o a well-interpolated curve.
5	Discussion
Limitations. While the CSSC model interpolates the trajectory that can strictly cross the observation
points, such interpolation is not suitable for noisy data whose observations are inaccurate. Moreover,
interpolation error bound (Eq. (15)) requires the underlying data is fourth-order continuous, which
indicates that CSSC is not suitable to interpolate sharply changed data, e.g., step signals.
Future work. The CSSC can not only be applied to ODE-RNN but can smooth any piecewise
continuous function. Applying CSSC to other more general models is a desirable future work. Also,
while interpolation for noisy data is beyond this paper’s scope, but hidden CSSC shows the potential
to tolerate data noise by capturing the data continuity at the semantic space rather than the observation
space, which can be a future direction.
6	Conclusion
We introduce the CSSC that can address the discontinuity issue for ODE-RNN. We have derived the
analytical solution for the CSSC and even proved its error bound for the interpolation task, which
is hard to obtain in pure neural network models. The CSSC combines the modeling ability of deep
neural networks (ODE-RNN) and the smoothness advantage of cubic spline interpolation. Our
experiments have shown the benefit of such a combination. The hidden CSSC extends the smoothness
from output space to the hidden semantic space, enabling a more general format of continuous signals.
8
Under review as a conference paper at ICLR 2021
References
Garrett Birkhoff and Arthur Priver. Hermite interpolation errors for derivatives. Journal of Mathe-
matics and Physics, 46(1-4):440-447, 1967.
Richard L Burden and J Douglas Faires. Numerical analysis, brooks. Cole, Belmont, CA, 1997.
Wei Cao, Dong Wang, Jian Li, Hao Zhou, Lei Li, and Yitan Li. Brits: Bidirectional recurrent
imputation for time series. In NeurIPS, 2018.
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent neural
networks for multivariate time series with missing values. Scientific reports, 8(1):1-12, 2018.
Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differen-
tial equations. In NeurIPS, 2018.
Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger
Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for
statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.
Carl De Boor, Carl De Boor, Etats-Unis Mathematicien, Carl De Boor, and Carl De Boor. A practical
guide to splines, volume 27. springer-verlag New York, 1978.
Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous
modeling of sporadically-observed time series. In NeurIPS, 2019.
Andreas Fagereng and Elin Halvorsen. Imputing consumption from norwegian income and wealth
registry data. Journal of Economic and Social Measurement, 42(1):67-100, 2017.
Joseph Futoma, Sanjay Hariharan, and Katherine Heller. Learning to detect sepsis with a multitask
gaussian process rnn classifier. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 1174-1182. JMLR. org, 2017.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional neural processes.
In ICML, 2018.
Charles A Hall and W Weston Meyer. Optimal error bounds for cubic spline interpolation. Journal
of Approximation Theory, 16(2):105-122, 1976.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equations
for irregular time series. arXiv preprint arXiv:2005.08926, 2020.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Zachary C Lipton, David Kale, and Randall Wetzel. Directly modeling missing data in sequences
with rnns: Improved classification of clinical time series. In Machine Learning for Healthcare
Conference, pp. 253-270, 2016.
Yonghong Luo, Xiangrui Cai, Ying Zhang, Jun Xu, et al. Multivariate time series imputation with
generative adversarial networks. In NeurIPS, 2018.
Michael Moor, Max Horn, Bastian Rieck, Damian Roqueiro, and Karsten Borgwardt. Early recogni-
tion of sepsis with gaussian process temporal convolutional networks and dynamic time warping.
In Proceedings of the 4th Machine Learning for Healthcare Conference, volume 106, pp. 2-26,
2019.
Michael C Mozer, Denis Kazakov, and Robert V Lindsey. Discrete event, continuous time rnns.
arXiv preprint arXiv:1710.04110, 2017.
9
Under review as a conference paper at ICLR 2021
Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M Dai, Nissan Hajaj, Michaela Hardt, Peter J Liu,
Xiaobing Liu, Jake Marcus, Mimi Sun, et al. Scalable and accurate deep learning with electronic
health records. NPJ Digital Medicine, 1(1):18, 2018.
Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for
irregularly-sampled time series. In NeurIPS, 2019.
Carl Runge. Uber empirische funktionen Und die interpolation ZWischen Wquidistanten ordinaten.
Zeitschrvftfur Mathematik und Physik, 46(224-243):20, 1901.
Satya Narayan Shukla and Benjamin Marlin. Interpolation-prediction netWorks for irregularly
sampled time series. In ICLR, 2019.
Qingxiong Tan, Mang Ye, Baoyao Yang, Si-Qi Liu, and Andy Jinhua Ma. Data-gru: Dual-attention
time-aWare gated recurrent unit for irregular multivariate time series. 2020.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, AndreW Lefrancq, et al. Deepmind control suite. arXiv preprint
arXiv:1801.00690, 2018.
A Proof of interpolation
Substitute o with Eq. (5) We will have
ck(tk+) +o(tk+) =xk	(18)
ck (tk-+1) + o(tk-+1) = xk+1	(19)
Ck-i(t-) + O(t-) = Ck (t+) + 0(t+)	(20)
Ck-i(t-) + 0(t-) = Ck (t+) + 0(t+)	(21)
co(t+) + 0(t+)=0	(22)
cCn-1(tn-) + oC(tn-) = 0	(23)
And we let cCk(tk+) = Mk, k = 0, 1, ..., n - 1, and cCn-1(tn-) = Mn. We define the first order and
second order jump difference of ODE-RNN as
rk = o(t+)- o(t-);	(24)
rCk = oC(tk+) - oC(tk-).	(25)
With Eq. (21), we have
Mk+1 = Ck(t-+ι) = Mk+1 + r(tk+ι).	(26)
Using constraint Eq. (18)(19), we denote
ck (tk ) = xk - o(tk+ ) = k+	(27)
ck(tk+1) = xk+1 - o(tt+1) = k+1 .	(28)
Also denote the step size τk = tk+1 - tk. Then applying constraint Eq. (18)(19)(21) we have the
piece cubic function expressed as
+
r MM Mk+1 - Mk (十十 \3」_ Mk (十十 '2 工 ∕ek + 1 - €k Tk(Mk+1 + 2Mk) w, , ʌ , + 少⑺
ck (t) =  6Tk--------(t-tk ) +■—2-(t-tk ) +( ----------------------©--------)(t-tk )+∈k . (29)
Next, We try to solve all Mk. We firstly express Ck-ι(t-) and Ck(t+) as
. 匕-_ Mk - Mk-I 7	1 , a ʃ	J- - e+-1
Ck-1(tk ) = --ɑ-----Tk - 1 + Mk-ITk-1 +--------
2	τk-1
+ 2Mk-1
Tk-1,
Ck (t+)
E-+1 - €+
Tk
(30)
(31)
3生Tk
—
10
Under review as a conference paper at ICLR 2021
Applying Eq. (20) we have
2Mk+「Mk-1+： Mk+1
6 e[t+,t-+1]-e[t+-1,t-] +6rk.
τk-1 + τk
-2港Tk-1 -港+iTk
τk-1 + τk
(32)
where e[t+,t-+ι] = lk+∈k
μk
Tk-I
τk-1 + τk
(33)
τk-1 + τk
dk
6 e[t+，tk+1] - e[t+-1, tk ] + 6rk - 2rkτk-1 - rk+1τk
τk-1 + τk	τk-1 + τk
Then Mk can be obtained by solving by system of linear equations:
AM = d,
where
(2	λι
μ2	2	λ2
A =	...	...
μn-2
∖
2
μn-1
,M
( Ml ∖
M2
.,d
.
Mn-2
Mn-1
( d1 ∖
d2
.
.
.
dn-2
dn-1
(34)
(35)
(36)
(37)
∖
And A is non-singular since it is strict diagonally dominant matrix, hence guarantee single solution
for Mk. Hence
M = A-1d.	(38)
Now we still need to calculate 0(t) and o(t).
・(” _ ∂g | ∂h ∂g |
0( ) = ∂h 瓦=∂h f
币八—df (h(t)) _ ∂f | dh _ ∂f |
() = —dt — = ∂h 加=∂h f,
四=d∂h |h+dg |h=(熏 f )∣ f+dg |f
dt ∂h ∂	h2 ∂ h∂ h
(39)
(40)
(41)
B	Proof for Error B ound
The following proof are based on the default notation and setting for scalar time series interpolation:
given a set of n+1 points {x(ti)}in=0 irregularly spaced at time points Π : a = t0 < t1 < ... < tn = b,
the goal is to approximate the underlying ground truth function x(t), t ∈ Ω = [a, b]. Let o(t) as the
ODE-RNN prediction, and o(t) = c(t) + o(t) as our smoothed output where c(t) is the compensation
defined by Eq. (11). To simplify the notation, we drop the argument t for a function, e.g. x(t) → x.
We firstly introduce several lemmas, then come the the main proof for the interpolation error bound.
Lemma 4. (Hall & Meyer, 1976) Let e be any function in C2 (Π) kn=1 C4(tk-1, tk) with constraints
x(tk) = 0, k = 0, ..., n, and natural boundary e00(a) = e00(b) = 0, then
∣e(r)(tk)∣ ≤ Pr∣∣e⑷∣∣∞τ4-r	(k = 0,...,n; r = 1, 2),	(42)
where Pi = 214, P2 = 1 ∙
Lemma 5. Let e = X — O, if X ∈ C 4(Ω), f ∈ C3, g ∈ C 4 ,then e ∈ C 2(Π) Tn=i C 4(tk-ι, tk)
Proof Since o ∈ C2(∏), and C ∈ Tn=i C4(tk-ι,tk), hence We only have to prove that o ∈
Tkn=i C4(tk-i, tk). Since o(t) in each time period (tk-i, tk) is calculated by ODE (Eq. (1), and (2)),
we can express different orders of the derivative of o as:
o = dg ∣f,	(43)
∂h
o=(乜 )∣f + 辿 | ff	(44)
=(∂h2 f) f + ∂h ∂hf,	()
11
Under review as a conference paper at ICLR 2021
⑶一	∂f ∂2g	∂3g	∂2g∂f	∂g | ∂2f	∂f ∂f
o = f (∂h ∂h2 + ∂h3 ◦f + 2∂h2 ∂h)f + ∂h (∂h2 ◦f + ∂h ∂h )f,	(45)
o(4)
=f|( ∂h4
◦ f ◦ f+d3g
f J + ∂h3
+ 2 ∂f ∂3g
∂ h ∂ h3
◦f+3
∂2g ∂2f
∂h2 ∂h2
◦f
3∂2g∂f∂f I ∂2f o f ∂2g ∂f ∂f ∂2g ∂f ∂2g ∂f)
+ 3 ∂h2 ∂h ∂h + ∂h2 ◦ f∂h2 + ∂h ∂h ∂h2 + 3 ∂h ∂h2 ∂hf
+ 也 |( ∂3f ◦ f ◦ f + ∂f ◦ (f f ) + 2 ∂f ◦ f∂f + ∂f∂f ◦ f + fff f
+ ∂ h Mh3 f f + ∂ h2	(∂hf)+ ∂ h2 f ∂h + ∂ h ∂h2 f + ∂ h ∂h ∂h",
(46)
where ∂h is the Jacobian matrix since f is a multi-valued function,舒 is the Hessian matrix since o
is a scalar and g is single-valued function. The higher order derivative than second order (Hessian)
becomes multi-dimensional matrix which can not be mathmatically expressed for standard matrix
production, so we indicate the product of a multi-dimensional matrix and a vector as ◦, which has
higher computing priority then normal matrix production. From Eq.(46), o(4) depends on 铝 and
∂hf. Hence given g ∈ C4 and f ∈ C3, we obtain o ∈ Tn=ι C4(t k-1 , tk ).
□
Lemma 6. (Birkhoff & Priver, 1967) Given any function v ∈ C4(tk, tk + 1), let u be the cubic
Hermite interpolation matching v, we have
∣(v(t) - u(t))(r)∣ ≤ Ar(t)∣∣v⑷∣∣∞τ4-r	(r = 0,1),
with
Ar(t) = Tir[(tr-4t-(2ir+!1I"	I,t ∈ [ti，tj).
This is the error bound for cubic Hermite interpolation.
Given the above lemmas, we are ready to the formal proof for Theorem 2.
Proof of Theorem 2:
Proof. According to Lemma 6, we let v = x — o (since x — o ∈ C4(tk, tk+1) according to Lemma 5),
let u be the cubic Hermite interpolation matching v, then we have
∣(x(t) — o(t) — u(t))(r)∣ ≤ Ar(t)||(x — o)(4)∣∣∞τ4-r	(r = 0,1),
(47)
(48)
(49)
with
A W _ Tn(t - ti)(ti+1 - t)]2-r
Ar (t) = —r!(4 — 2r)!τ4—-
if t ∈ [ti, tj).
(50)
Let e = X — o, ε = U — c, then ε is cubic Hermite interpolation implying
ε(tk) = x(tk) — o(tk) — c(tk) = x(tk) — O(tk) = 0,
(51)
and
ε(tk) = x(tk) — o(tk) — c(tk) = x(tk) — o(tk) = e(tk).
(52)
Therefore, ε(t) in [tk, tk+1) is a cubic function with endpoints satisfying can be constructed for each
interval [tk, tk+1) as Eq. (51) and (52), which can be reconstructed as
ε(t)= e(tk)H1(t) + e(tk+1)H2(t),
where
Hι(t) = (t — tk )(t — tk+ι)2∕τ2,
H2(t) = (t — tk+1 )(t — tk )2∕τ2.
(53)
(54)
(55)
From Lemma 4 and Lemma 5, e is bounded as Eq. (42). Combining Eq. (42) and (53) yields:
Hr)(t)| ≤ ρι∣∣e⑷ll∞(∣H(r)(t)l + H2r)(t)l)τ3	(r = 0, 1),
(56)
12
Under review as a conference paper at ICLR 2021
Table 3: The interpolation MSE for on toy sinuous wave test set at different observation ratio.
We compare the analytical and numerical differentiation of O and 0 for CSSC under different
settings, where block means block gradient, drop indicates set as zero, and CSSC is the standard
implementation.
observation	Analytical			Numerical		
	10%	30%	50%	10%	30%	50%
Block O, o	10.601951	0.001408	0.000150	-	-	-
Block O	0.811334	0.000721	0.000142	-	-	-
Block O0	0.406867	0.000348	0.000121	-	-	-
Drop O0	0.072519	0.000356	0.000123	-	-	-
CSSC	0.024656	0.000457	0.000128	0.067881	0.000397	0.000126
which is rewritten as
∣(x(t) - 0(t))(T)I ≤ Br(x)∣∣e⑷∣∣∞τ4-r = B「(x)||(x - o)⑷∣∣∞τ4-r,	(57)
with Br(t) = Pι(∣H^^t)∣ + Mr)(t)∣)τr-1.
Using Triangle inequality, Lemma 6, and Eq. 57, it issues
Ix(T) (t) - o(r)(t) I = Ix(T) (t) - o(r) (t) — u(r) (t) + u(r) (t) - c(r) (t) I
≤ |x(r)(t) - o(r)(t) - u(r)(t)| + |u(r) (t) -c(r)(t)|
≤(AT(x)+BT(x))II(x-o)(4)II∞τ4-T.	(58)
Let CT(x) = AT(x) + BT(x), and an analysis of the optimality (Hall & Meyer, 1976) yields
51
Co(x) ≤ 384;	Cι(x) ≤ 12.	(59)
□
C Computational Complexity
C.1 Implementation of the Inverse of Matrix
For sake of simplicity, our Pytorch implementation adopts torch.inverse to compute A-1 in
Eq. (12), which is actually the implementation of the LU composition using partial pivoting with
best complexity O(n2). Its complexity is higher than the complexity of tridiagonal matrix algorithm
O(n), whose implementation will be left for future work.
C.2 Computation of O and O
As Eq. 8 indicates, computing O and O requires the Hessian and Jacobian of g and the Jacobian of
f . These Jacobians and Hessians will first participate in the inference stage and then are involved
in the gradient backpropagation. However, the latest Pytorch1 does not support the computing of
Jacobian and Hessian in batch, so the training process will be very slow in practice, rendering the
computing cost prohibitively high. Therefore, we propose two ways to circumvent such issues from
both numerical and analytical views.
1Pytorch Version 1.6.0, updated in July 2020.
13
Under review as a conference paper at ICLR 2021
Numerical Differentiation. The first solution is to use a numerical derivative. We approximate the
left limitation and right limitation of 0(t) and o(t) as
o(t-)
o(t-)
o(t+)
o(t+)
o(t-) — o(t — ∆t)
∆
o(t-) — 2o(t — ∆) + o(t — 2∆)
∆2
o(t + ∆t) — o(t+)
∆
o(t + 2∆t) — 2o(t + ∆t) + o(t+)
∆2
(60)
(61)
(62)
(63)
where ∆ = 0.001. In this way, we can avoid computing Jacobian or Hessian, and the computational
complexity almost remains the same as ODE-RNN. The last row of Table. 3 shows that the numerical
differentiation can maintain the same performance with the analytical solution with 30% and 50%
observation. However, when observations become sparser, e.g., 10%, the analytical differentiation
will gain a better performance.
Analytical Approximation. For analytical solution, the major computing burden is the Hessian
matrix; thus, We approximate o with Eq.(65) by simply dropping the Hessian term. The motivation
and rationality is detailed in Appx. C.3.
C.3 Computation Reduction for Analytical Derivative
To further reduce the computation for the analytical derivative of o and o, We investigate whether
blocking the gradient of o or o will affect the interpolation performance. From first three rows in
Table. 3, we can see that performance of blocking the gradient of o is worse than that of blocking o,
indicating o is more important than o in terms of computing the compensation c. In light of this, we
further drop o in the Eq.(7), meaning the second order jump difference rk is zero. According to the
performance shown in the fourth row of Table. 3, o has a minor impact on the compensation C when
the observation is dense. We investigate the reason by check how o impact the computation of C and
find that they are correlated by dk in Eq. (12). We write dk again here for better clarification:
d = 6 E[t+，tk+1] — E[t+-1, tk ] + 6rk — 2rkτk-1 — rk+1τk
k	τk-1 + τk	τk-1 + τk
(64)
From (Eq. (6, 7)) and second term of above equation (Eq. (64)), we noticed that 亍卜,亍k serve
as the bridge between o and o and dk, indicating that the importance of o and o in generating
compensation C can be examined by estimating the relative significance of rk,rk appear in dk. We
can denote the relative significance as fraction of terms included rk and rrk as S = l2rkτk-6+rk+1τkl ≤
3maχ{lrkTk-；||,|rk+1 Tk|}. Without loss of generality, we let max{∣rkTk-ι∣, ∣rk+ιTk∣} = ∣r"k-ι∣,
then we have S ≤ 3max{|rkTk-1∣l,lrk+ιτk|} = lr2Tk-1l. It shows that rk has a coefficient of τk-ι on
6|rk |	2|rk |
the numerator, which is the time interval between two adjacent observations. In our experiment,
100 samples is uniformly sampled in 5s, and a certain percent of the samples will be selected as
observations. In this setting, if we have 50% samples as observations, the average τk = 5/50 = 0.1;
thus s can be informally estimated as S ≤ 20∣r∣∣, indicating o is more important than o in our
experiments. As the observation ratio is higher, the τk becomes smaller, hence the relative significance
of rk and rrk will become even larger. Armed with the above intuition that o is less important and the
fact that he Hessian contribute the major complexity O(W2), we drop the term with Hessian and find
a better approximation of o, leading the final approximation of o as:
o= dg । ff.	(65)
∂ h ∂ h
Such approximation yields descent performance in practice. In addition, because o is multi-variable
and g is a multi-valued function in practice, and such Jacobian needs to run in batch. We implement
our Jacobian operation to tackle these difficulties and can run fast.
14
Under review as a conference paper at ICLR 2021
Table 4: The MSE of CSSC for different α on MuJoCo test set. It compares the different data
samples.
α	0	1	10	100	1000	10000
10%	0.006551	0.011915	0.009691	0.005421	0.006097	0.005886
30%	0.000745	0.000463	0.000426	0.000400	0.000375	0.000422
50%	0.000126	0.000102	0.000074	0.000072	0.000087	0.000057
Figure 5: The interpolation MSE of CSSC with different α of MuJoCo dataset. The curve is the 4-th
dimension of the hopper,s state.
D Implementation Details
The neural ODE state h has size 15. f is a 5-layer MLP with hidden state size 300. g is a 2-layer
MLP with hidden state size 300. RNNCell has hidden state size 100. To obey the condition f ∈ C3
and g ∈ C4 required by the error bound in Theorem 2, we select all the nonlinear function as tanh
function. The α is selected as 1000 based on the ablation study of α in Sec. 4.5. The network is
optimized by AdaMax (Kingma & Ba, 2014) with β1 = 0.9, β2 = 0.999, and learning rate 0.02.
15