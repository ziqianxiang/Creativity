Under review as a conference paper at ICLR 2021
Rethinking Compressed Convolutional Neu-
ral Networks from a Statistical Perspective
Anonymous authors
Paper under double-blind review
Ab stract
Many designs have recently been proposed to improve the model efficiency of
convolutional neural networks (CNNs) at a fixed resource budget, while there is
a lack of theoretical analysis to justify them. This paper first formulates CNNs
with high-order inputs into statistical models, which have a special ”Tucker-like”
formulation. This makes it possible to further conduct the sample complexity
analysis to CNNs as well as compressed CNNs via tensor decomposition. Tucker
and CP decompositions are commonly adopted to compress CNNs in the litera-
ture. The low rank assumption is usually imposed on the output channels, which
according to our study, may not be beneficial to obtain a computationally effi-
cient model while a similar accuracy can be maintained. Our finding is further
supported by ablation studies on CIFAR10, SVNH and UCF101 datasets.
1 Introduction
The introduction of AlexNet (Krizhevsky et al., 2012) spurred a line of research in 2D CNNs, which
progressively achieve high levels of accuracy in the domain of image recognition (Simonyan &
Zisserman, 2015; Szegedy et al., 2015; He et al., 2016; Huang et al., 2017). The current state-
of-the-art CNNs leave little room to achieve significant improvement on accuracy in learning still-
images, and attention has hence been diverted towards two directions. The first is to deploy deep
CNNs on mobile devices by removing redundancy from the over-parametrized network, and some
representative models include MobileNetV1 & V2 (Howard et al., 2017; Sandler et al., 2018).
The second direction is to utilize CNNs to learn from higher-order inputs, for instance, video clips
(Tran et al., 2018; Hara et al., 2017) or electronic health records (Cheng et al., 2016; Suo et al.,
2017). This area has not yet seen a widely-accepted state-of-the-art network. High-order kernel
tensors are usually required to account for the multiway dependence of the input. This notoriously
leads to heavy computational burden, as the number of parameters to be trained grows exponentially
with the dimension of inputs. Subsequently, model compression becomes the critical juncture to
guarantee the successful training and deployment of tensor CNNs.
Tensor methods for compressing CNNs. Denil et al. (2013) showed that there is huge redundancy
in network weights such that the entire network can be approximately recovered with a small fraction
of parameters. Tensor decomposition recently has been widely used to compress the weights in
a CNN network (Lebedev et al., 2015; Kim et al., 2016; Kossaifi et al., 2020b; Hayashi et al.,
2019). Specifically, the weights at each layer are first summarized into a tensor, and then tensor
decomposition, CP or Tucker decomposition, can be applied to reduce the number of parameters.
Different tensor decomposition to convolution layers will lead to a variety of compressed CNN
block designs. For instance, the bottleneck block in ResNet (He et al., 2016) corresponds to the
convolution kernel with a special Tucker low-rank structure, and the depthwise separable block
in MobileNetV1 (Howard et al., 2017) and the inverted residual block in MobileNetV2 (Sandler
et al., 2018) correspond to the convolution kernel with special CP forms. All the above are for 2D
CNNs, and Kossaifi et al. (2020b) and Su et al. (2018) considered tensor decomposition to factorize
convolution kernels for higher-order tensor inputs.
Tensor decomposition can also be applied to fully-connected layers since they may introduce a large
number of parameters (Kossaifi et al., 2017; 2020a); see also the discussions in Section 5. Moreover,
Kossaifi et al. (2019) summarized all weights of a network into one single high-order tensor, and
then directly imposed a low-rank structure to achieve full network compression. While the idea is
1
Under review as a conference paper at ICLR 2021
highly motivating, the proposed structure of the high-order tensor is heuristic and can be further
improved; see the discussions in Section 2.4.
Parameter efficiency of the above proposed architectures was heuristically justified by methods, such
as FLOPs counting, naive parameter counting and/or empirical running time. However, there is still
lack of a theoretical study to understand the mechanism of how tensor decomposition can compress
CNNs. This paper attempts to fill this gap from statistical perspectives.
Sample Complexity Analysis. Du et al. (2018a) first characterized the statistical sample complexity
ofa CNN; see also Wang et al. (2019) for compact autoregressive nets. Specifically, consider a CNN
model, y = FCNN (x, W) + ξ, where y and x are output and input, respectively, W contains all
weights and ξ is an additive error. Given the trained and true underlying networks FCNN(x, Wc) and
FCNN(x, W*), the root-mean-square prediction error is defined as
E(Wc) =	Ex|FCNN(x,Wc) -FCNN(x,W*)|2,	(1)
where Wc and W* are trained and true underlying weights, respectively, and Ex is the expectation
on x. The sample complexity analysis is to investigate how many samples are needed to guarantee a
given tolerance on the prediction error. It can also be used to detect the model redundancy. Consider
two nested CNNs, where F1 is more compressed than F2. Given the same true underlying networks,
when the prediction errors from trained F1 and F2 are comparable, we then can argue that F2 has
redundant weights comparing with F1. As a result, conducting sample complexity analysis to CNNs
with higher order inputs will shed light on the compressing mechanism of popular compressed CNNs
via tensor decomposition.
The study in Du et al. (2018a) is limited to 1-dimensional convolution with a single kernel, followed
by weighted summation, and its theoretical analysis cannot be generalized to CNNs with compressed
layers. In comparison, our paper presents a more realistic modeling of CNN by introducing a gen-
eral N -dimensional convolution with multiple kernels, followed by an average pooling layer and a
fully-connected layer. The convolution kernel and fully-connected weights are in tensor forms, and
this allows us to explicitly model compressed CNNs via imposing low-rank assumption on weight
tensors. Moreover, we used an alternative technical tool, and a sharper upper bound on the sample
complexity can be obtained.
Our paper makes three main contributions:
1.	We formulate CNNs with high-order inputs into statistical models, and show that they have an
explicit “Tucker-like” form.
2.	The sample complexity analysis can then be conducted to CNNs as well as compressed CNNs
via tensor decomposition, with weak conditions allowing for time-dependent inputs like video data.
3.	From theoretical analysis, we draw an interesting finding that forcing low dimensionality on
output channels may introduce unnecessary parameter redundancy to a compressed network.
1.1 Comparison with other existing works
Deep neural networks are usually over-parametrized, yet empirically, they can generalize well. It
is an important topic in the literature to theoretically study the generalization ability of deep neural
networks, including deep CNNs (Li et al., 2020; Arora et al., 2018). The generalization error, defined
as the difference between test and training errors, is commonly used to evaluate such ability, and
many techniques have been developed to control its bound; see, for example, the VC dimension
(Vapnik, 2013), the Rademacher complexity & covering number (Bartlett & Mendelson, 2002), the
norm-based capacity control (Neyshabur et al., 2017; Golowich et al., 2018; Bartlett et al., 2017;
Neyshabur et al., 2015) and low-rank compression based methods (Li et al., 2020; Zhou & Feng,
2018; Arora et al., 2018). These works use a model-agnostic framework, and hence relies heavily on
explicit regularization such as weight decay, dropout or data augumentation, as well as algorithm-
based implicit regularization to remove the redundancy in the network.
We, however, attempt to theoretically explain how much compressibility is achieved in a compressed
network architecture. Specifically, we make a comparison between a CNN and its compressed
version, and makes theoretically-supported modification to the latter to further increase efficiency.
2
Under review as a ConferenCe PaPer at ICLR 2021
SUbSeqUentIy“ OUr analysis requires an explicit formulation for the IIetWOrk architectUeWhiCh is
PrOVided in "ection 2“ and the PrediCtion error at (1) is adopted as OUr evaluation criteria，We IIOtiCe
that Li et al∙ (2020) also PrOPoSeS to USe the CP LayerS to COmPreSS the WeightSin each COnVOIUtion
layer BUttheir StUdy is StiII model—agnostic” SinCe the ranks Ofthe UnderIying CP LayerS depend
On the trained WeightS∙ In details" their PrOPoSed approach USeS regularization assumptions On the
WeightS and henc尸 their derived theoretical bound is influenced by training and IIOt SUitabIe to
analyze the IIetWOrk design exclusively-
Other existing WOrkS “ that aim to PrOVide theoretical UnderStanding for the IleUraI IletWOrks“ include
the UnderStanding Of Parameter recovery With gradient—based algorithms for deep IIeUraI IIetWOrkS
(Zhong et aΓ2017b; FU et al.“ 2020; GOeI et aΓ2018; Zhong et a - 2017a)二he development Of
Other ProVably efficient algorithms (Cao 2 G尸 2019; Du 济 GOeL 2018)； and the investigation Of
ConVergenCe in an OVer—parameterized regime (A=en—zhu et aΓ2019; Li 济 Liang” 201∞bu et aΓ
2018b)∙ OUr WOrk differs greatly from these WOrkSin both target and methodology，Wedo IIOt
ConSider COmPUtato∙nal complexity Or algorithm convergence- Insteadze focus Onthe StatiStiCaI
SamPIe COmPleXity to depict the mechanism Of COmPreSSed block designs for CNNS∙
2 FORMULATING CNNS WlTH HlGHER—ORDER INPUTS
2∙1 NoTATloN
TenSOr nofafions∙ We follow the IIOtations in KoIda 浮 Bader (2009) to denote vectors byow—
ercase boldface Ietter∞eg α; matrices by capital boldface Ietter∞eg AtenSOrS Of Order 3
Or higher by Euler SCriPt BoIdfaCe Ietter∞eg A∙ For an NEOrder tensor A ∈ 国 ZIX: XZ产 de—
IlOte its elements by⅛l(0r02: •二 QN) and the "—mode UnfOlding by As; Where the CoIUmnS
Of As) are the "—mode VeCtOrS Of A “ for 1 ≤ " ≤ N. And A(L ∙ ∙ ∙ 3r∙3 ∙ ∙ •..) denotes
the SUbtenSOr Of A holding Only the ah -ndeX HXed∙ The VeCtOriZation OPeration is denoted
by vec(∙)∙ TheimIerPrOdUCt OftWo tensors A 3ω∈¾lx∙: Xzz is defined as (A"ω) H
Mαl ∙∙∙Mαz A(01: •二 0N)ω(01: •二 2NLandtheFrObe≡.us norm is =A=F U ʌ/⅛l3⅛l. The
modem multipHCation 义前 Ofa tensor A ∈ ¾-×∙∙∙×? and a matriX B ∈ IRpxΓis defined as
(A XnBWi7 •二 jm ••二iN- HMΓΓU1 A(r∙∙ ∙二r∙: ∙ ∙ 2N)Bi‰r∙L for 1 ≤ " ≤ M respec—
tively∙ The mode—" multipHCation×IN of a tensor A ∈ ×∙∙∙×? and a VeCtor b ∈ IRΓis defined
as⅛Xnb) (el 二.二r-—— 13r∙+1::3 0N)UMr∙"l A (01 ::3r∙":二jN )bΓ)“ for 1IΛ72IΛN.
The SymbolC0二 Sthe KroneCker PrOdUCt and No"is the OUter product，We extend the definition
OfKhatri—Rao product to tensors: given tensors A ∈ 冠IXZ2x∙:ZNXKandω∈ 唱 IXP2 义…PNXK ”
their Khatri—Rao product is a tensor Of Swe 港 IPlXL2P2X∙:LNPNXKe denoted by C U A0ω"where
C(L …⅛) U A(L …⅛)0ω(J∙…⅛L 1 ≤ /c ≤ κ∙
CP decompos≡on∙ The Canonical POIyadiC(CP) decomposition (KOIda 浮 BadeL 2009) factorizes
the tensor A ∈ ¾-×∙∙∙×? into a SUm Of rank—1 tens or sr∙e∙ A HM祥一 Qrhs1)。hL2o…。h"N)”
Where¾) is a unit norm VeCtOr Of SiZe 冠.for all 1 ≤ j ≤ N∙ The CP rank is the IlUmber Of rank—1
tensors R.
TUCker decomposifion∙ The TUCkerrankS Of an Nm—order tensor A ∈ IRZI ×∙∙∙×? are defined as the
matrix ranks Ofthe UnfOIdingS Of A along all modes∙ If the TUCker ranks Of A are (Rl: •二 RNL
then there exist a COre tensor 9 ∈ IRRIX :• XRZ and matrices H(Z) ∈ 冠0XRJ SUChthat A H
g XlH(I)义 2 H(2) ∙ ∙ ∙ XN^NL known as TUCker decomposition (TUCkeLl 966)“
FigUre 1: (1) 3—pyer CNN for a 3D input X With One kernel tensor A average pooling” and fully—
COmIeCted WeightSω∙ (2)jɪj)s act as PoSito∙≡.ng factors，The White SPaCeSindiCate ZerO entries，
Under review as a conference paper at ICLR 2021
2.2	Basic three-layer CNNs
Consider a three-layer CNN with one convolution, one average pooling and one fully-connected
layers, and we assume linear activations for simplicity. Specifically, for a general tensor-structured
input X ∈ Rdι×d2× ×dN, We first perform its convolution with an Nth-order kernel tensor
A ∈ Rl1×l2××lN to get an intermediate output Xc ∈ Rm1 ×m2××mN, and then use average pool-
ing with pooling sizes (qι,…,qN) to derive another intermediate output Xcp ∈ Rp1×p2× ×pN.
Finally, Xcp goes through a fully-connected layer, with weight tensor B ∈ Rp1×p2× ×pN to pro-
duce a scalar output; see Figure 1(1).
We consider the convolution layer with the stride size of sc along each dimension. Assume that
mj = (dj - lj)/sc + 1 are integers for 1 ≤ j ≤ N, otherwise zero-padding will be needed. Let
Ui(jj) = [ |{0z} |{Iz}	|{0z}	]0 ∈Rdj×lj for1 ≤ij ≤mj, 1 ≤j ≤ N, (2)
(ij -1)sc lj dj -(ij -1)sc -lj
which act as positioning factors to stretch the kernel tensor A into a tensor of the same size as X,
while the rest of entries are filled with zeros; see Figure 1(2). As a result, Xc has entries
Xc(il, i2,..., iN) = hX, A ×1 Ui(II) ×2 U(2) ×3 …×N Ui(N )i.
For the pooling layer with the stride size of sp along each dimension, we assume the pool-
ing sizes	{qj}jN=1	satisfy the relationship of	mj	=	qj	+	(pj	- 1)sp,	so the sliding windows
can be overlapped. But for ease of notation, we can simply take qj = mj /pj . The average
pooling operation is equivalent to forming p1p2p3 •…PN consecutive blocks within Xc, each of
size Rq1×q2× ×qN, and then take the average per block. The resulting tensor Xcp has entries
Xcp(i1,i2,...,iN) = (Qjqj)-1PjPikjjq=j(ij-1)qj+1Xc(k1,k2,...,kN), for1 ≤ ij ≤ pj and
1 ≤ j ≤ N. If we denote UF(j,)ij = qj-1 Pikj=qj(ij -1)qj +1 Uk(j), then equivalently,
Xcp(i1, i2,..., iN ) = hX, A ×1 Uffi1 ×2 UFi2 ×3 …×N UFNN i.
The fully-connected layer performs a weighted summation over Xcp, with weights given by entries
of the tensor B ∈ Rp1×p2×…×pN. Denote UFj) = (UFjI,…，UFjpJ for 1 ≤ j ≤ N, and the
predicted output has the form of
b = hXcp, Bi = hX, (B 乳 A) ×ι UF1) ×2 UF2) ×3 …×n UF)i.
Similarly, for a CNN with K kernels, denote {Ak, Bk}kK=1 to be the set of kernels and fully-
connected weights, where Bk ∈ Rp1×p2× ×pN and Ak ∈ Rl1×l2× ×lN. The model can then
be explicitly represented by
yi=ybi+ξi= hXi,WXi+ξi, 1 ≤i ≤n,	(3)
where ξi is the additive error, Xi ∈ Rd1 ×d2 × × dN, and the composite weight tensor
K
WX = (XBk 乳 Ak) ×1 UF1) ×2 UF ×3 …×N UF).	(4)
k=1
The innate weight-sharing compactness of CNN can be equivalently represented as a “Tucker-like”
form at (4). The factor matrices {UF(j) ∈ Rdj ×ljpj }jN=1 are fixed and solely determined by CNN
operations on the inputs. They have full column ranks given that pjlj ≤ dj always holds. The core
tensor is a special Kronecker product that depicts the layer-wise interaction between weights.
2.3	Sample complexity analysis
We can now derive the non-asymptotic error bound for the CNN model. Let Zi = Xi ×1 UF(1)0 ×2
UF2)0 ×3 •…×N UFN)0 ∈ Rl1p1×l2p2×…×lNPN. Model (3) is equivalent to
K
yi = hZi, Wi + ξi = XhZi, Bk 乳 Aki + ξi,	(5)
k=1
4
Under review as a conference paper at ICLR 2021
where W = PK=I Bk 0 Ak. The trained weights have the form of W = PK=I Bk 0 Ak, where
1n	K	2
{Bk, Ak}ι≤k≤κ =	arg min — yi - £(Zi，Bk ® Aki
Bk,Ak,1≤k≤K ni=1	k=1
(6)
Denote xi	=	vec(Xi ) and zi	= vec(Zi	),	and it can be verified that zi	= UG0 xi	, where	UG	=
UFI) 0 {UjN) 0 [U(NT) 0 …0 (Uj3) 0 UF2)]} represents the CNN operations on the inputs.
Let X = (x10, x20,..., xn0)0, and We make the following technical assumptions.
Assumption 1. (Time-dependent inputs) X is normally distributed with mean zero and variance
Σ = E(XX0), where CxI ≤ Σ ≤ Cx I for some 0 < cx < Cx.
Denote by λmax(A) and λmin(A) the maximum and minimum eigenvalues of a symmetric matrix
A, respectively. When {Xi } is a stationary time series with spectral density function fX (θ), we can
take cx = inf-π≤θ≤π λmin (fX (θ)) and Cx = sup-π≤θ≤π λmax(fX (θ)); see Basu & Michailidis
(2015). For independent inputs, Σ = diag{Σ11, . . . , Σnn} is block diagonal, and it holds that
cx = min1≤j≤n λmin (Σjj ) and Cx = max1≤j ≤n λmax (Σjj ), where Σjj = E(X X ).
Assumption 2. (Sub-Gaussian errors) {ξi } are independent σ2 -sub-Gaussian random variables
with mean zero, and is independent of {Xj, 1 ≤ j ≤ i} for all 1 ≤ i ≤ n.
Assumption 3. (Restricted isometry property) cuI ≤ UG0 UG ≤ CuI for some 0 < cu < Cu.
Denote KU = CxCu and KL = Cxcu. Let W* = PK=I Bk 0 Ak be the true weight. The model
complexity of CNN at (5) is d，M = K(P + L + 1), where P = p1p2 …PN and L = 1也…In.
Theorem 1 (CNN). Suppose that Assumptions 1-3 hold and n & (KU/KL)2dM. Then,
kW - W*kF ≤ 16σ√aRSM frdM + rδ! and, E(W) ≤√KUkW - W*kF
αRSC	n n
with probability 1 - 4 exp{-[CH(0.25KL/KU)2n - 9dM]} - exp{-dM - 8δ}, where δ = Op(1),
CH is a positive value, αRSC = KL/2, and αRSM = 3KU /2.
It can be seen that the prediction error E(W), is Op( dM/n), and hence the sample complexity is
of order O(dM∕ε2), to achieve a prediction error ε. Technical proofs of all theorems and corollary
in the paper are deferred to Appendix A.3.
Note that, for simplicity, we assume a simple regression model at (5). The theoretical analysis
can indeed be established for classification problems as well. In details, we provide corresponding
theorem and corollaries for both binary and multiclass classification problems in Appendix A.4.
2.4 CNNs with more layers
Consider a 5-layer CNN with “convolution → pooling → convolution → pooling → fully con-
nected” layers, where the settings for the first “convolution → pooling” layers are the same as those
in Section 2.2.
Denote the kernel for the second convolution by A ∈ Rl1 ×l2 × × lN, and the fully-connected weight
tensor B is of size Rp1×p2× ×pN. We similarly define matrices Uij) and UFji , which are both of
size Rpj ×lj with 1 ≤ j ≤ Pj and 1 ≤ j ≤ N, and can be used to represent the second convolution
and pooling operations, respectively. Note that the output from the first pooling layer, Xcp, is the
input to the second convolution layer, and hence the output from the second pooling layer has entries
X cp(il,i2, ...,iN) = hXcp, A ×1 UF,i1 ×2 UFi2 ×3 …XN U(NN).
Stack the matrices {Ujj'ζ, ∈ Rdj xlj}ι≤ij≤pj and {Uj ∈ Rpj ×j}ι≤j≤pj into 3D tensors Uj) ∈
Rdj ×lj ×pj and Uj) ∈ Rpj ×j ×pj, respectively. Let UDjF = (Uj) ×3 Uj))(i) for 1 ≤ j ≤ N and,
by some algebra in Appendix A.2, we can show that the predicted output has the form of
y =(Xcp, Bi	=	hɪ,	(B 0 A 0 A) ×ι	UDF ×2 UDF ×3	…×n	Ua,	(7)
5
Under review as a conference paper at ICLR 2021
whereU(j) ×3Ue yields a tensor of size Rdj ×lj ×lj ×pej , with Pip=j 1 U(j) (k1, k2, i)Ue (i, k3, k4) as
its (k1, k2, k3, k4)th entry. The case with multiple kernels will have a form similar to (4). Following
the same logic, it can indeed be generalized to an even deeper CNN by adding more “convolution-
pooling” layers. Since the composite weight tensor always has a “Tucker-like” form, techniques in
proving Theorem 1 can be adopted to conduct the sample complexity analysis for deep CNNs.
Note that, at model (7), the kernels at different convolution layers appear in the form of A0A, which
actually provides a theoretical justification for exploring the layer-wise low-dimensional structure in
the literature. On the other hand, we summarize all weights ofa network into one single tensor, akin
to Kossaifi et al. (2019). But our summarized tensor has an explicit “nested doll” structure, where
the weight structure of the previous layer is nested within that of the current layer.
In practice, zero-padding can be used before each convolution layer to preserve the input size. This
is compatible with our framework, since we can propagate the padding of zeros to previous layers
and all the way back to the input tensor X.
3 Compressed CNNs
For a high-order input, a deep CNN with a larger number of kernels may involve heavy computation,
which renders its training difficult for portable devices with limited resources. In real applications,
many compressed CNN block designs have been proposed to improve the efficiency, and most of
them are based on either matrix factorization or tensor decomposition; see Lebedev et al. (2015);
Kim et al. (2016); Astrid & Lee (2017); Kossaifi et al. (2020b).
Tucker decomposition can be used to compress CNNs, which is same as introducing a multilayer
CNN block; see Figure 3 in Kim et al. (2016). Specifically, We stack the kernels Ak ∈ Rl1 ×l2× ×lN
with 1 ≤ k ≤ K into a higher order tensor Astack ∈ Rl1×l2× ×lN×K, and assume that Astack has
Tucker ranks of (Ri, R2, ∙∙∙ , RN, RN +ι). As a result,
Astack = G ×ι H⑴ ×2 H(2)…×n +1 H(N +i),	(8)
where G ∈ RR1× ×RN×RN+1 is the core tensor, and H(j) ∈ Rlj ×Rj are factor matrices for 1 ≤
j ≤ N + 1. From Section 2, to train such a compressed CNN block with linear activations is
equivalent to search for the least-square estimators at (6) with Astack being constrained to have the
TU TU
form of (8). Denote the estimators by {Bk , Ak }1≤k≤K. The trained weights then have a form of
TU TU
WTU = Pk=i Bbk XAk , and the model complexity is dMU = Qj=Il Rj+Pi=ι IiRi+Rn +iP.
CP decomposition is more popular in compressing CNNs; see Kossaifi et al. (2020b); Lebedev
CP CP
et al. (2015); Astrid & Lee (2017). Let {Bk , Ak }1≤k≤K be the least-square estimators at (6)
with Astack having a CP decomposition with a rank of R. Then, the trained weights are WcCP =
PkK=1 Bb kCP X AbkCP, and the model complexity is dCMP = RN+1 + R(PiN=1 li + P).
TU
Theorem 2 (Compressed CNN). Let (W, dM ) be (WTU, dTMU) for Tucker decomposition, or
(Wcp,dMp) for CP decomposition. Suppose that AssumPtions1-3 hold and n & (KU∕kl)2cnd^.
Then,
kW - W*kF ≤ 16σ√aRSM ( rCNdM + rɪ! and, E(W) ≤√KUkW - W*kF
αRSC	n	n
with probability 1 - 4 exp{-[cH (0.25κL /κU)2 n - 3cN dM ]} - exp{-dM -8δ}, where δ = Op(1),
cN = 2N+1 log(N + 2), cH is a positive value and αRSC, αRSM are defined in Theorem 1.
Theorem 2 shows that, as expected, the sample complexity is proportional to the model complexity.
Compared to Theorem 1, we see that the sample complexity of the compressed CNN depends on
PiN=1 li instead of QiN=1 li . When the input dimension N is large, compressed CNN can indeed
reduce a large number of parameters.
However, we notice that the sample complexity of a compressed CNN depends on the rank RN+1
or R, rather than the number of kernels K . This differs from the naive parameter counting, and we
hence provide the rationale behind this counter-intuitive observation in the corollary below.
6
Under review as a conference paper at ICLR 2021
Corollary 1. (a) If A has a Tucker decomposition with the ranks of (R1, R2, ∙ ∙ ∙ , RN+1) ,the CNN
is equivalent to one with RN+1 kernels, and each kernel Ar, where 1 ≤ r ≤ RN+1, has a Tucker
decomposition with ranks of (R1, R2, ∙ ∙ ∙ , RN). Moreover, (b) if the stacked kernel tensor A has
a CP decomposition with rank R, the corresponding CNN can be reparameterized into one with R
kernels, and each kernel Ar, where 1 ≤ r ≤ R, has a CP decomposition form with rank R.
Define the K/R ratio to be the value of K/RN+1 for Tucker decomposition or K/R for CP de-
composition, and in practice, it always holds that K ≥ R. Corollary 1 essentially states that when
K/R > 1, there exists model redundancy in the CNN model with linear activations. In other words,
we can obtain a more compressed network by setting K = R such that it has the same sample
complexity as the previous one. The K/R ratio appears in various block designs; see Figure 2. And
we can use this finding to evaluate their model efficiency.
(T1) Standard bottleneck block. The basic building block in ResNet (He et al., 2016) can be
exactly replicated by a Tucker decomposition on Astack ∈ Rl1 ×l2×C×K, with ranks of (l1, l2, R, R),
where C is the number of input channels (Kossaifi et al., 2020b). As shown by the ablation studies
in Session 4, when K R, this design may not be the most parameter efficient.
(T2) Funnel block. As a straightforward revision on a standard bottleneck block, the funnel block
maintains an output channel size of K = R. This is hence an efficient block design. Also, when
R = C, we obtain a normalized funnel block, with the same number of input and output channels.
(C1) Depthwise separable block. This block is the basic module in MobileNetV1 (Howard et al.,
2017) with a pair of depthwise and pointwise separable convolution layers. It is equivalent to as-
suming a CP decomposition on Astack ∈ Rl1 ×l2×C×K with the rank of C (Kossaifi et al., 2020b).
The 1 × 1 pointwise convolution expands the numbers of channels from C to K . When K C ,
this design is parameter inefficient from a computation viewpoint.
(C2) Inverted residual block. Sandler et al. (2018) later proposed this design in MobileNetV2. It
includes expansive layers between the input and output layers, with the channel size of X ∙ C(X ≥ 1),
where x represents the expansion factor. As discussed by Kossaifi et al. (2020b), it can heuristically
correspond to a CP decomposition on AStaCk, with CP rank equals to X ∙ C. Since, the rank of output
channel dimension can be at most X ∙ C, as long as K ≤ X ∙ C holds, it is theoretically efficient and
provides leeway for exploring thicker layers within blocks.
With nonlinear activations, the model redundancy may bring some benefit. Such benefit is often not
guaranteed. And when K R, it will, without a doubt, greatly hinder computational efficiency.
We will show, in our ablation studies in the next section, that under realistic settings with nonlin-
ear activations, our finding on the K/R ratio still applies. Specifically, networks with K/R = 1
maintains comparable performance as networks with K/R > 1, while using much less parameters.
4	Experiments
This section first verifies the theoretical results with synthetic datasets, and then conducts ablation
studies on the bottleneck blocks in ResNet (He et al., 2016) with different K/R ratios.
4.1	Numerical analysis for theoretical results
We choose four settings to verify the sample complexity in Theorem 1; see Table 1. The parame-
ter tensors {Ak , Bk }kK=1 are generated to have standard normal entries. We consider two types of
Figure 2: We use thickness of each layer to indicate its relative number of channels. The last layer
indicates the output of the block. The dashed layer represents the ”bottleneck” of the block.
7
Under review as a conference paper at ICLR 2021
Table 1: Different settings for verifying Theorem 1 (left), and Theorem 2 (right).
	Input sizes	Kernel sizes	Pooling sizes	# Kernels	Input sizes	Kernel sizes	Pooling sizes	Tucker ranks	# Kernels
Setting 1 (S1)	(7, 5, 7)	(2, 2, 2)	(3, 2, 3)	1	(10, 10, 8,3)	(5, 5, 3)	(3, 3, 3)	(2, 2, 2, 1)	2
Setting 2 (S2)	(7, 5, 7)	(2, 2, 2)	(3, 2, 3)	3	(10, 10, 8,3)	(5, 5, 3)	(3, 3, 3)	(2, 2, 2, 1)	3
Setting 3 (S3)	(8, 8, 3)	(3, 3, 3)	(3, 3, 1)	1	(12, 12, 6, 3)	(7, 7, 3)	(3, 3, 2)	(2, 3, 2, 1)	2
Setting 4 (S4)	(8, 8, 3)	(3, 3, 3)	(3, 3, 1)	3	(12, 12, 6, 3)	(7, 7, 3)	(3, 3, 2)	(2, 3, 2, 1)	3
Figure 3: (1)-(2) are the experiment results for Theorem 1 with independent and dependent inputs,
respectively. (3) is the experiment results for Tucker CNN in Theorem 2.
inputs: the independent inputs are generated with entries being standard normal random variables,
and the time-dependent inputs {xi} are generated from a stationary VAR(1) process. The random
additive noises are generated from the standard normal distribution. The number of training samples
n varies such that ∖∕dM /n is equally spaced in the interval [0.15, 0.60], with all other parameters
fixed. For each n, we generate 200 training sets to calculate the averaged estimation error in Frobe-
nius norm. It can be seen that the estimation error increases linearly with the square root of dM /n,
which is consistent with our finding in Theorem 1.
For Theorem 2, we also adopt four different settings; see Table 1. Here, we consider 4D input
tensors. The stacked kernel Astack is generated by (8), where the core tensor has standard normal
entries and the factor matrices are generated to have orthonormal columns. The number of training
samples n is similarly chosen such that JdMJ/n is equally spaced. A linear trend between the
estimation error and the square root of dTMU /n can be observed, which verifies Theorem 2.
For details of implementation, we employed the gradient descent method for the optimization with a
learning rate of 0.01 and a momentum of 0.9. The procedure is deemed to have reached convergence
if the target function drops by less than 10-8.
4.2	Ablation studies on computationally-efficient block designs
Following the notation in Section 3, we denote by K, the number of output channels of a bottleneck
block, and by R, the Tucker rank of Astack along the output channels dimension. Since K ≥ R, we
let t = K/R be the expansive ratio of the block. As t increases, the number of parameters within
the block increases. Here, ablation studies are conducted to show that the increase in t does not
necessarily lead to the increase in test accuracy. We analyze two image recognition datasets, CIFAR-
10 (Krizhevsky et al., 2009) and Street View House Numbers (SVHN) (BNetzer et al., 2011), and
an action recognition dataset, UCF101 (Soomro et al., 2012). Standard data pre-processing and
augmentation techniques are adopted to all three datasets (He et al., 2016; Hara et al., 2017).
Network architecture. Two network architectures are considered in our study; as shown in Figure
4. For image recognition datasets, we adopt a 41-layer Residual Network: it consists of a 3 × 3
convolution layer, a max pooling layer, followed by 3 groups of A + x × B residual blocks with
different R, and ends with an average pooling and fully-connected layer. Block B represents the
bottleneck structure that we are interested in. The standard bottleneck block corresponds to t =
4. When t = 1, it corresponds to the normalized funnel block in Section 3 (T2). For sake of
Table 2: Test accuracy(%) on CIFAR-10, SVHN and UCF101. For UCF101, we only count the
number of parameters and FLOPS for the added blocks.
t	CIFAR-10	SVHN	#FLOPs	#Params	UCF101	#Added FLOPs	#Added Params
1	93.53	96.30	0.17GMac	5.56M-	79.31	0.125GMac	6.97M
4	93.10	96.09	0.26GMac	8.55M	79.36	0.145GMac	9.16M
8	93.59	96.32	0.43GMac	14.15M	79.25	0.175GMac	11.11M
16	94.03	96.31	0.87GMac	29.28M	79.52	0.235GMac	15.00M
8
Under review as a conference paper at ICLR 2021
⑴
BlockAt > 1
#in ChanneIS
1 × 1 conv
#out channels
Figure 4: (1) 41-layer Residual Network for CIFAR-10&SVNH. (2) 30-layer 3D Residual Network
for UCF101. Here, t = K/R represents the expansion ratio, and takes values of 1,4,8,16.
comparison, we also take t = 8 or 16. When t > 1, Block A is the standard downsampling block
that contains convolution with the stride size of 2. It is inefficient according to our study. So when
t = 1, we propose minor revisions to simultaneously increase the channels while performing the
stride-2 3 × 3 convolution. For UCF101 dataset, we use 3D ResNet-18 as the basic framework and
insert 4 stacks of Block B with R = 256 before the average pooling and fully-connected layer.
Results. The results in Table 2 provide empirical support for our theoretical finding. Though the
number of parameters in the network increases with larger t, the overall test accuracy remains
roughly comparable. For CIFAR-10, it may appear that the test accuracy for t = 16 is slightly
better than t = 1 but it uses 5 times as many parameters.
Implementation details. All experiments are conducted in PyTorch and on Tesla V100-DGXS.
Following the practice in He et al. (2016), we adopt batch normalization(BN) (Ioffe & Szegedy,
2015) right after each convolution and before the ReLU activation. For network (1), we initialize
the weights as in He et al. (2015). We use stochastic gradient descent with weight decay 10-4,
momentum 0.9 and mini-batch size 128. The learning rate starts from 0.1 and is divided by 10 for
every 100 epochs. We stop training after 300 epochs, since the training accuracy hardly changes.
The training accuracy is approximately 93% for CIFAR-10 and 96% for SVHN. We set seeds 1-5 and
report the worst case scenario of the test accuracy in Table 2. For network (2), we follow Hara et al.
(2018) and use the Kinetics pretrained 3D ResNet-18. The weights are fixed for the first few layers,
and backward propagation is only applied to the added Block B layers and the fully-connected layer.
We use stochastic gradient descent with weight decay 10-5, momentum 0.9 and mini-batch size 64
with 16 frames per clip. The learning rate starts from 0.001 and divided by 10 for every 30 epochs.
Training is stopped after 80 epochs and top-1 clip accuracy is around 79% for UCF101.
5	Conclusion and discussion
Our paper proposes a unified theoretical framework that can adopt sample complexity analysis as
a tool to study the effective number of parameter in a tensor decomposed CNN block. The main
practical takeaways are: (i) For a large kernel tensor A (Peng et al., 2017), it is always effective to
impose low-rank structure on its input dimensions, such as height, width or time-length, etc; and
(ii) It is essential to maintain K/R = 1 for a CNN block in order to increase accuracy in a more
parameter efficient way. In this regard, one can either choose a small R with deeper networks or a
larger R with shallower networks. In fact, when K/R = 1, the block corresponds exactly to the
“straightened” bottleneck in Zagoruyko & Komodakis (2016), who showed in their empirical study
that by increasing R in shallow networks, the test accuracy can indeed improve quite significantly.
Since the low-dimensional structure at convolution layers is our focus, tensor decomposition has
been applied to the kernel weights Astack only in Section 3. Kossaifi et al. (2017) applied tensor
decomposition to fully-connected layers since they may contain a large number of parameters as
well. Along the line, We can stack the fully-connected weights Bk ∈ Rp1 ×p2 × …×pN With 1 ≤ k ≤
K into a higher order tensor BStaCk ∈ Rp1 ×p2 × …×pN ×K, and further consider a Tensor Contraction
Layer (TCL) or Tensor Regression Layer (TRL) as in (Kossaifi et al., 2020a). Similar theoretical
analysis can be obtained, and we leave it for future research.
9
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In Proceedings of the 36th International Conference on Machine Learning,
volume 97,pp. 242-252, 2019.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Proceedings of the 35th International Conference on
Machine Learning, volume 80, pp. 254-263, 2018.
Marcella Astrid and Seung-Ik Lee. Cp-decomposition with tensor power method for convolutional
neural networks compression. In 2017 IEEE International Conference on Big Data and Smart
Computing (BigComp), pp. 115-118. IEEE, 2017.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. Journal of Machine Learning Research, 3(Nov):463-482, 2002.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Sumanta Basu and George Michailidis. Regularized estimation in sparse high-dimensional time
series models. The Annals of Statistics, 43(4):1535-1567, 2015.
Yuval BNetzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Read-
ing digits in natural images with unsupervised feature learning. In In NIPS Workshop on Deep
Learning and Unsupervised Feature Learning 2011, 2011.
Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a
minimal number of noisy random measurements. IEEE Transactions on Information Theory, 57
(4):2342-2359, 2011.
Yuan Cao and Quanquan Gu. Tight sample complexity of learning one-hidden-layer convolutional
neural networks. In Advances in Neural Information Processing Systems, pp. 10611-10621, 2019.
Yu Cheng, Fei Wang, Ping Zhang, and Jianying Hu. Risk prediction with electronic health records:
A deep learning approach. In Proceedings of the 2016 SIAM International Conference on Data
Mining, pp. 432-440. SIAM, 2016.
Misha Denil, Babak Shakibi, Laurent Dinh, Marc’Aurelio Ranzato, and Nando De Freitas. Pre-
dicting parameters in deep learning. In Advances in Neural Information Processing Systems, pp.
2148-2156, 2013.
Simon S Du and Surbhi Goel. Improved learning of one-hidden-layer convolutional neural networks
with overlaps. arXiv preprint arXiv:1805.07798, 2018.
Simon S Du, Yining Wang, Xiyu Zhai, Sivaraman Balakrishnan, Russ R Salakhutdinov, and Aarti
Singh. How many samples are needed to estimate a convolutional neural network? In Advances
in Neural Information Processing Systems, pp. 373-383, 2018a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably optimizes
over-parameterized neural networks. arXiv preprint arXiv:1810.02054, 2018b.
Jianqing Fan, Wenyan Gong, and Ziwei Zhu. Generalized high-dimensional trace regression via
nuclear norm regularization. Journal of econometrics, 212(1):177-202, 2019.
Haoyu Fu, Yuejie Chi, and Yingbin Liang. Guaranteed recovery of one-hidden-layer neural networks
via cross entropy. IEEE Transactions on Signal Processing, 2020.
Surbhi Goel, Adam Klivans, and Raghu Meka. Learning one convolutional layer with overlapping
patches. arXiv preprint arXiv:1802.02547, 2018.
Noah Golowich, Alexander Rakhlin, and Ohad Shamir. Size-independent sample complexity of
neural networks. In Conference On Learning Theory, pp. 297-299. PMLR, 2018.
10
Under review as a conference paper at ICLR 2021
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Learning spatio-temporal features with 3d
residual networks for action recognition. In Proceedings of the IEEE International Conference
on Computer Vision Workshops, pp. 3154-3160, 2017.
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history
of2d cnns and imagenet? In Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition, pp. 6546-6555, 2018.
Kohei Hayashi, Taiki Yamaguchi, Yohei Sugawara, and Shin-ichi Maeda. Exploring unexplored
tensor network decompositions for convolutional neural networks. In Advances in Neural Infor-
mation Processing Systems, pp. 5552-5562, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing
human-level performance on imagenet classification. In Proceedings of the IEEE international
conference on Computer Vision, pp. 1026-1034, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp.
770-778, 2016.
Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,
Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for
mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on Computer Vision and Pattern
Recognition, pp. 4700-4708, 2017.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In Proceedings of the 32nd International Conference on Inter-
national Conference on Machine Learning, pp. 448-456, 2015.
Yong-Deok Kim, Eunhyeok Park, Sungjoo Yoo, Taelim Choi, Lu Yang, and Dongjun Shin. Com-
pression of deep convolutional neural networks for fast and low power mobile applications. In
International Conference on Learning Representations, 2016.
Tamara G Kolda and Brett W Bader. Tensor decompositions and applications. SIAM Review, 51(3):
455-500, 2009.
Jean Kossaifi, Aran Khanna, Zachary Lipton, Tommaso Furlanello, and Anima Anandkumar. Ten-
sor contraction layers for parsimonious deep nets. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition Workshops, pp. 26-32, 2017.
Jean Kossaifi, Adrian Bulat, Georgios Tzimiropoulos, and Maja Pantic. T-net: Parametrizing fully
convolutional nets with a single high-order tensor. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 7822-7831, 2019.
Jean Kossaifi, Zachary C Lipton, Arinbjorn Kolbeinsson, Aran Khanna, Tommaso Furlanello, and
Anima Anandkumar. Tensor regression networks. Journal of Machine Learning Research, 21
(123):1-21, 2020a.
Jean Kossaifi, Antoine Toisoul, Adrian Bulat, Yannis Panagakis, Timothy M Hospedales, and Maja
Pantic. Factorized higher-order cnns with an application to spatio-temporal emotion estimation.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6060-
6069, 2020b.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.
Tech Report, 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
11
Under review as a conference paper at ICLR 2021
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.
Speeding-up convolutional neural networks using fine-tuned cp-decomposition. In International
Conference on Learning Representations, 2015.
Jingling Li, Yanchao Sun, Jiahao Su, Taiji Suzuki, and Furong Huang. Understanding generalization
in deep learning via tensor methods. In Proceedings of the Twenty Third International Conference
OnArtificial Intelligence and Statistics, volume 108, pp. 504-515, 2020.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic gradient
descent on structured data. In Advances in Neural Information Processing Systems, pp. 8157-
8166, 2018.
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. Shufflenet v2: Practical guidelines for
efficient cnn architecture design. In Proceedings of the European conference on computer vision
(ECCV), pp. 116-131, 2018.
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro. Norm-based capacity control in neural
networks. In Conference on Learning Theory, pp. 1376-1401, 2015.
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro. A pac-bayesian approach to
spectrally-normalized margin bounds for neural networks. arXiv preprint arXiv:1707.09564,
2017.
Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters-improve
semantic segmentation by global convolutional network. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition, pp. 4353-4361, 2017.
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen. Mo-
bilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on
Computer Vision and Pattern Recognition, pp. 4510-4520, 2018.
Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning
without mixing: Towards a sharp analysis of linear system identification. Proceedings of Machine
Learning Research vol, 75:1-35, 2018.
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. In International Conference on Learning Representations, 2015.
Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions
classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.
Jiahao Su, Jingling Li, Bobby Bhattacharjee, and Furong Huang. Tensorial neural networks:
Generalization of neural networks and application to model compression. arXiv preprint
arXiv:1805.10352, 2018.
Qiuling Suo, Fenglong Ma, Ye Yuan, Mengdi Huai, Weida Zhong, Aidong Zhang, and Jing Gao.
Personalized disease prediction using a cnn-based similarity learning method. In 2017 IEEE
International Conference on Bioinformatics and Biomedicine (BIBM), pp. 811-816. IEEE, 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 1-9, 2015.
Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer
look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference
on Computer Vision and Pattern Recognition, pp. 6450-6459, 2018.
Ledyard R Tucker. Some mathematical notes on three-mode factor analysis. Psychometrika, 31(3):
279-311, 1966.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
2013.
12
Under review as a conference paper at ICLR 2021
R. Vershynin. High-dimensional probability: an introduction with applications in data science.
Cambridge University Press, Cambridge, 2018.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Di Wang, Feiqing Huang, Jingyu Zhao, Guodong Li, and Guangjian Tian. Compact autoregressive
network. arXiv preprint arXiv:1909.03830, 2019.
Saining Xie, Ross Girshick, Piotr Dollar, ZhUoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
andpattern recognition, pp. 1492-1500, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. arXiv preprint
arXiv:1605.07146, 2016.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional neural
netWorks With multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L Bartlett, and Inderjit S Dhillon. Recovery guarantees
for one-hidden-layer neural netWorks. In Proceedings of the 34th International Conference on
Machine Learning, pp. 4140-4149, 2017b.
Pan Zhou and Jiashi Feng. Understanding generalization and optimization performance of deep
cnns. arXiv preprint arXiv:1805.10767, 2018.
13
Under review as a conference paper at ICLR 2021
A	Appendix
This appendix contains five sections. In the first section, we provide details for our mathematical
formulation for a high-order 3-layer CNN. In the next section, we discuss the formulation of a 5-
layer CNN as an illustrating example of how to add more layers to our framework. A complete
version of Theorems 1&2 together with the technical proofs for Theorems 1&2 and Corollary 1
are provided in the third section. In the fourth section, we extend our settings to classification
and subsequently establish theorem and corollaries for binary and multiclass classification problem.
More implementation details and results for additional experiments are presented in the last section.
A. 1 CNN formulation
For a tensor input X ∈ Rd1 × ×dN, it first ConvolUtes With an N-dimensional kernel tensor A ∈
Rl1× ×lN with stride sizes equal to so and then performs an average pooling with pooling sizes
equal to (qι,…，qN). It ends with a fully-connected layer with the weight tensor B and produces
a scalar output. We assume that mj = (dj - lj )/sc + 1 are integers for 1 ≤ j ≤ N, otherwise
zero-padding will be needed. For ease of notation, we take the pooling sizes {qj }jN=1 to satisfy the
relationship mj = pj qj .
To duplicate the operation of the convolution layer using a simple mathematical expression, we first
need to define a set of matrices {Ui(j) ∈ Rdj ×lj }, where
Ui(jj) = [ |{0z} |{Iz}	|{0z}	]0 ∈Rdj×lj for1 ≤ij ≤mj, 1 ≤j ≤ N. (9)
(ij -1)sc lj dj -(ij -1)sc -lj
Ui(j) acts as a positioning factor to transform the kernel tensor A into a tensor of same size as the
input X, with the rest of the entries equal to zero.
We are ready to construct our main formulation for a 3-layer tensor CNN. To begin with, we first
illustrate the process using a vector input x ∈ Rd, with a kernel vector a ∈ Rl; see Fig 5. Using the
ith positioning matrix Ui(1), we can propagate the small kernel vector a, into a vector Ui(1)a of size
Rd, by filling the rest of the entries with zeros. The intermediate output vector has entries given by
xc(i) = hx, Ui(1)ai, for 1 ≤ i ≤ m. The average pooling operation is equivalent to forming p con-
secutive vectors within xc, each of size Rq, and then take the average. This results in a vector xcp of
size Rp, with ith entry equal to xcp(i) = q-1 Pkq=(i-1)q+1 xc(i) = hx, q-1 Pkq=(i-1)q+1 Uk()ai.
The fully-connected layer performs a weighted summation over the p vectors, with weights given
by entries of the vector b ∈ Rp. This gives us the predicted output yb = hb, xcpi = hx, wXi, where
p	1 iq
WX = X bi -	X	UkIa = uF}(b 0 a) = (b 0 a) ×ι U^,
i=1 k=(i-1)q+1
and U*) = q-1(Pk=1 Uk1),…，Pm=m-q+ι Uk1)), and”×1” represents the mode-1 product.
For matrix input X with matrix kernel A, however, we need 2 sets of positioning matrices,
{Ui(1)}im=1 1 and {Ui(2)}im=2 1, one for the height dimension and the other for width. Then, the
intermediate output from convolution has entries given by Xc(i1, i2) = hX, Ui(1)AUi(2)0i, for
- ≤ i1 ≤ p1, - ≤ i2 ≤ p2. For the average pooling, we form p1p2 consecutive matrices from Xc,
each of size Rq1 ×q2 and take the average. This results in Xcp ∈ Rp1 ×p2, with Xcp(i1, i2) =
hX,(q1-1Pik11q=1(i1-1)q1+1Uk(11))A(q2-1Pik22q=2(i2-1)q2+1Uk(22))0i. The output Xcp goes through
fully-connected layer with weight matrix B and gives the predicted output yb = hX, WXi, where
p2 p1	i1q1	i2q2
WX = XX biibi2qν(	X	UkI))A(	X	U^) = (B0A)×1UF)×2UF),
i2=1 i1=1	1 2 k1=(i1 -1)q1+1	k2=(i2-1)q2+1
with UF = q-1 (Pk=I Uj …，Pm=mj j + 1 Uk(j ) ) for j = - or 2. And ”×1”, ”×2” represent
the mode-1 and mode-2 product.
14
Under review as a conference paper at ICLR 2021
Figure 5: Formulating the process with an input vector x. Note that we combine the convo-
lution, pooling and fully connected layers into the composite weight vector wX, where wX =
q-1 pq=ι Ui(I)a + + q-1 Pq=m-q+ι Ui(I)a. The ~ represents the convolution operation.
From here, together with the case of high-order tensor input discussed in Session 2.2, we can then
derive the form of predicted outcome as yb = hX, WsXinglei, where
WXngle = (B 0 A) ×ι UP ×2 U2 × …X UF),
with UFj = q-1(Pk=ι Ukj),…，Pm=mj-qj+1 Up)), and ”x/'represents the mode-j product
for 1 ≤ j ≤ N .
With K kernels, we denote the set of kernels and the corresponding fully-connected weight tensors
as {Ak, Bk}kK=1. Since the convolution and pooling operations are identical across kernels, we can
use a summation over kernels to derive the weight tensor for multiple kernels, which is
K
WX =(X Bk 0 Ak) ×1 UP ×2 UF X∙∙∙X UF),
k=1
and we arrive at the formulation for 3-layer tensor CNN.
A.2 Five-layer CNN formulation
Consider a 5-layer CNN with ''convolution→pooling→convolution→pooling→fully connected”
layers and a 3D tensor input X ∈ Rd1 ×d2×d3. Here, we denote the intermediate output from the
first convolution by Xc ∈ Rm1×m2×m3, from the second convolution by Xc ∈ Rm 1×m2×m3,
and the output from the first pooling by Xcp ∈ Rp1 ×p2×p3 and from the second pooling by
Xcp ∈ Rp1×p2×p3. We can first see that the predicted output from the 5-layer CNN is the same
as directly feeding Xcp to the second convolution layer, followed by an average pooling layer and a
fully-connected layer.
Denote the first convolution kernel tensor by A ∈ Rl1 ×l2×l3, the second convolution kernel tensor
by A ∈ Rl1×l2×l3, and the fully-connected weight tensor by B ∈ Rpι×p2×p3. Define the set
of matrices {Ui(j) ∈ Rdj ×lj }, for 1 ≤ ij ≤ mj, 1 ≤ j ≤ N as in (2). And similarly, define
{Ui(j) ∈ Rpj ×j}, with
Uei(jj) = [ |{0z} |{Iz}	|{0z}	]0,	(10)
(ij -1)sc Ij pj-(ij-1)sc-lj
where sc is the stride size for the second convolution. Let UFj = (UF)I,…,UF)Pj =
(q-1 Pk=I Ukj),…滴-1 Pmjmj-qj+ι Ukj)). We further stack the matrices {UF,ij ∈
Rdj×lj}ι≤ij≤pj and {Ujji. ∈ Rpj×j}ι≤j≤Pj into 3D tensors U(j) ∈ Rdj×lj×pj and U(j) ∈
Rpj × lj ×pj .
15
Under review as a conference paper at ICLR 2021
We have that the predicted output
b = hB ㊈ A, Xcp × 1 UFI)O ×2 UF2) ×3 UF3∖
p1 p2 p3
=b ㊈ AWEEhA, X ×1 UF(1,)i01 ×2 UF(2,)i02 ×3UF(3,)i03iuei1 ◦ uei2 ◦ uei3
i1=1 i2=1 i3=1
p1 p2	p3
=B ㊈屐 A,XXXX ×1 (UiI X UF,i1) ×2 (Ui2 X UFx) ×3 (Ui3 X UFX))
i1=1 i2=1 i3=1
= B XAeXA,X ×1 (U(1) ×3 Ue(1))0(1) ×2 (U(2) ×3Ue(2))0(1) ×3 (U(3) ×3Ue(3))0(1)
X, (B ㊈ Ae ㊈ A) ×1 (U(I) ×3 U⑴)⑴ ×2 (U⑵ ×3 U⑵)⑴ ×3 (U⑶ ×3 U⑶)⑴
=(x, (B 乳 A 乳 a) ×ι UDF ×2 UDF ×3 UDF)
where Uij = VeC(U°)(ij,:,:)) ∈ Rjpj and UDF = (U(j) ×3 U⑶)(1), for 1 ≤ ij ≤ Pj and
1 ≤ j ≤ 3.
A.3 Theoretical results and technical proofs
Let Zi = Xi ×1 UF1)0 ×2 UF2' ×3 •一 ×N UF)0 ∈ Rlιpι ×l2p2×…×lNpN.Thetrainedweights have
K
the form of W =	k=1 Bk X Ak, where
1n	K	2
{Bbk, Abk}1≤k≤K =	arg min n∑ yi-∑>i,BkX Ak i	.
Bk ,Ak ,1≤k≤K n i=1	k=1
Given a test sample (X, y), where y = hX, WXi +ξ, and (X, ξ) satisfies Assumptions 1 and 2. The
mean-square training and test error are defined as
—∙^~∙
errtrain(W) = t
n X (yi -hXi, WXi)2 and err(W) = $园③⑨(y -〈X, WX〉)2.
i=1
Denote KU = CxCu and KL = CχCu. Let W* = PK=I Bk X Ak be the true weights, and
kWk2n := n-1 Pin=1(hZi, Wi)2 be the empirical norm with respect to W. The model complexity
of CNN at equation (1) or (2) is dM = K(P + L + 1).
Theorem 1 (Complete Version). Suppose that Assumptions 1-3 hold and n & (KU/KL)2dM. Then,
for some δ > 0,
(a)
(b)
(c)
∣Wc - W* ∣n ≤
∣∣W - W*kF ≤ 16OvzaRSM
αRSC
16σaRSM
αRSC
2 c	16σαRSM
err2(W) ≤ ---------
αRSC
with probability 1 - 4 exp{-[cH (0.25KL /KU)2 n - 9dM]} - exp{-dM -8δ}, where cH is a positive
value, αRSC = KL/2, and αRSM = 3KU /2.
*	* . *	* 1-»*
It can be seen that the estimation error, ∣W - W* ∣F, is Op( dM/n), and hence the sample
complexity is of order O(dM/ε2), to achieve a training error ε. The empirical norm is equivalent
to the training error minus the sample standard deviation of noise. And we can see that the training
error errtrain(W) = Op( dM/n) + σb and test error err(W) = Op( dM/n) + σ, where σb and σ
represent the sample and population standard deviation of noise, respectively.
16
Under review as a conference paper at ICLR 2021
Now we proceed to provide a complete version of the sample complexity of compressed CNNs. Let
dTMU =QjN=+11 Rj +PiN=1 liRi + RN+1P and dCMP =RN+1+R(PiN=1li+P).
TU
Theorem 2 (Complete Version). Let (W, dM) be (WTU, dTMU) for Tucker decomposition, or
(Wcp,dMP) for CP decomposition. Suppose that AssumPtions 1-3 hold and n & (KU∕kl)2cnd^.
Then, for some δ > 0,
(a)kW - W*kF ≤ 16σ√aRSM 八/CNdM + JA ,
αRSC	n	n
(b)kW—W*kn ≤ 16σαRsM (r∕cNdM+Jδ!,
αRSC	n	n
2 c	16σαRSM 2 cNdM	δ 2
©err(W) ≤((丁+n+σ,
with probability 1 - 4 exp{-[cH (0.25κL /κU)2 n - 3cN dM ]} - exp{-dM - 8δ}, where cN
2N+1 log(N + 2), cH is a positive value andαRSC, αRSM are defined in Theorem 1.
Because CP decomposition can be considered as a special case of Tucker decomposition where
Rj = R, for 1 ≤ j ≤ N + 1. We only provide the proof for Tucker CNN.
A.3.1 Proof of Theorem 1
Denote the sets SK = {PK=ι Bk 0 Ak : Ak ∈ Rl1×l2×…×lN and Bk ∈ Rp1×p2×…×pN} and
一 ^ - ^	■—■	.,.
SK = {W ∈Sk : kWIlF = 1}. Let ∆ = W — W*,and then
n
1 X(yi-hZi, Wi)2 ≤
n i=1
1n
—X(yi-hZi, W*i)2,
n i=1
which implies that
2n	1n
k∆kn ≤ n ∑ξihZi, ∆i ≤ 2k∆kF ʌsup^ n ∑ξihZi, ∆i,	(11)
where k∆k2n := n-1 Pin=1(hZi, ∆i)2 is the empirical norm with respect to ∆, and ∆b ∈ Sb2K.
Consider a ε-net S?k, with the cardinality of N(2K, ε), for the set S2κ. For any ∆ ∈ S2κ, there
exists a ∆∆j ∈ S2κ such that ∣∣∆ 一 ∆∆j ∣∣f ≤ ε. Note that ∆ 一 ∆∆j ∈ S4κ and, from Lemma 1(a),
We can further find ∆ι, ∆2 ∈ S2κ such that (∆ι, 4£ = 0 and ∆ 一 ∆j = ∆ι + ∆[. It then
holds that ∣∆ιkF + ∣∆2∣f ≤ √2∣∆ 一 Aj∙∣F ≤ √2ε since ∣∣∆ 一 Aj∙∣∣F = ∣∣∆ι∣F + ∣∣∆2kF. As
a result,
1n
n X ξihZi, A
i=1
nnn
-XξihZi, Aji + - XξihZi, Aii + n XξihZi, ∆
i=1
i=1
i=1
≤
1 n .	. _
max 一	ξihZi, Aji + √2ε sup
1≤j≤N(2K,ε) n i=1	∆∈S2K
-n
n ∑ξihZi, Ai,
i=1
which leads to
nn
sup - XξihZi, Ai ≤ (1 一^£)-1	max - XξihZi, Aj〉.	(12)
∆∈S2K n	1≤j≤N (2K,ε) n	j
2K	i=1	i=1
17
Under review as a conference paper at ICLR 2021
Note that, from Lemma 1(b), logN(2K, ε) ≤ 2dM log(9∕ε), where dM = K(P + L + 1). Let
ε = (2√2)-1, and then 8 - 2log(9∕ε) > 1. As a result, by (12) and Lemma 3,
P
1 n .	.	______
sup — ξihZ ξ hZ ,∆ ≥ 8σ √αRSM
∆∈S2K ni=1
sup k∆k2n ≤ αRSM
∆∈S2K
≤P
1 n .	. _	____
max — ξihz ξ hZ, ∆ji ≥ 4σ√αRSM
1≤j ≤N (2K,ε) n
i=1
sup k∆k2n ≤ αRSM
∆∈S2K
N (2K,ε)	n
≤ X P - X 白况 Aj i
j=1	i=1
≥ 4σ√αRSM
IlAj∣∣n ≤ αRSM
≤ exp{-dM - 8δ}.
(13)
Note that
P
1n
△% n X ξi W δ
≥ 8σ√αRSM
≤P
1n
△X 吟
≥ 8σ √αRSM
sup kAk2n ≤ αRSM
∆∈S2κ
+ P sup kAk2n ≥ αRSM
∖∆∈S2κ
From (13) and Lemma 2, We then have that, with probability 1 - 4 exp{-[ch (0.25κl/ku)2n -
9dM]} - exp{-dM - 8δ},
1n
△X 在 S Ai
≤ 8σ√ɑRSM
and	kAk2n ≥ αRSC kAk2F ,
which, together with (11), leads to
k∆kF ≤ ^gRM
αRSC
and k∆kn ≤ 16σαRSM
αRSC
Finally we prove (c). Given a test sample (X, y), and let Z = X ×ι U?” ×2 UF2) ×3 ∙∙∙×n UF )0.
It holds that
E(y-hX,WXi)2 =E(y-hZ,Wi)2 = E(hZ, Ai)2 + σ2,
and
E(hZ, ∆i)2 = ∆0E(ZZ0)∆ = A0UGE(Xx0)UG∆ ≤ κUk∆k2.
As a result, from (a) of this theorem, we have
err2(W) ≤ (1^MY (dM + δ) + σ2.
αRSC	n n
This accomplishes the proof.
A.3.2 Proof of Theorem 2
Denote the sets STU(RI,…，Rn +1) = {PK=ιBk 0 Ak : the stacked kernel Astack ∈
Rl1×l2×…×lN×k has the ranks of (Ri,...,Rn,Rn +1) and Bk ∈ Rp1×p2×…×pN},极=
{W1 + W ： Wι,W ∈ STU(RI,…，Rn +ι)}, and STK = {W ∈ STU ： kWkF = 1}.
Note that Wtu, W* ∈ Stu(Ri,…，RN +ι), and ∆ = WTU — W* ∈ SbTK.
18
Under review as a conference paper at ICLR 2021
We first consider a ε-net for S2TKU. For each 1 ≤ k ≤ K, Let bk = (bk1, . . . , bkP)0 = vec(Bk), and
We can rearrange Bk 0 Ak into the form of Ak ◦ bk, which is a tensor of size Rl1×l2× ×lN×P,
where P =P1P2 •…PN. Denote B = (bkj) ∈ RP×K, and it holds that
K
XAk ◦ bk = A XN +ι B = G ×ι H(1) ×2 H(2) •…XN+ι B,
k=1
which is a tensor with the size of Rl1×l2× ×lN×p and the ranks of (Ri,…，Rn,Rn+1) where
Be = BH(N+1) ∈ RP×RN+1 . Essentially, in this step, we rewrite the model into one with Rk+1
kernels instead. Specifically, we now have W = PrR=N1+1 Aer 0 Ber, where Aer = Gr X1 H(1) X2
H⑵ …XN H(N) with Gr = G(:,:,…，r) and the r-th column of B is the vectorization of Br
for 1 ≤ r ≤ RN+1.
As a result, Sb2TKU consists of tensors with the ranks of (2R1, ..., 2RN, 2RN+1) at most.
Denote STucker(ri,…，「n, rN+1) = {T ∈ Rl1×…×lN ×lN+1 :炉|山=1, T has the Tucker ranks
of (ri,…,rn,「n +i)}, where In+i = P = P1P2 •…PN. Then the ε-covering number for S^
satisfies
|S2TU| ≤ ISTucker(2Rl,…，2Rn, 2Rn +l)∣.
For each T ∈ STucker(ri, •…,rn,「n +1), We have
T = G ×1 U⑴ ×2 …XN+1 U(N+1),
where G ∈ Rr1×…×rN×rN+1 with ∣∣G∣∣f = 1, and U(i) ∈ Rli×ri with 1 ≤ i ≤ N + 1 are
orthonormal matrices. We now construct an ε-net for STucker(r1, •…,「n,「n+1) by covering the
sets of G and all U(i)s, and the proof hinges on the covering number of low-multilinear-rank tensors
in Wang et al. (2019). Treating G as QjN=+11 「j -dimensional vector with kGkF = 1, we can find an
ε∕(N + 2)-net for it, denoted by G, with the cardinality of ∣G∣ ≤ (3(N + 2)∕ε)QN+1 rj.
Next, let On,r = {U ∈ Rn×r : U>U = Ir}. To cover On,r, it is beneficial to use the ∣∣ ∙ ∣∣1,2 norm,
defined as
∣X∣1,2 = miax ∣Xi ∣2,
where Xi denotes the ith column of X. Let Qn,r = {X ∈ Rn×r : ∣X∣1,2 ≤ 1}. One can
easily check that On,r ⊂ Qn,r, and then an ε∕(N + 2)-net (3n,r for On,r has the cardinality
of |On,r| ≤ (3(N +'2)∕ε)nr.'Denote STucker(「1, ∙∙∙ ,r∙N,r∙N +1) = {G ×1 U ⑴ ×2 …XN +1
U(N +1) : G ∈ G, U(i) ∈ Oli,%, 1 ≤ i ≤ N + 1}. By a similar argument presented in
Lemma A.1 of Wang et al. (2019), we can show that STucker(「1, •…，「n, IrN+1) is a ε-net for the
set STucker(「1, •…,「n,「n+1) With the cardinality of
3N + 6 ∖ QN=11 rj +PN+ι1 Ij rj
where lN+1 = P. Thus, the ε-covering number of S2TKU is
3N+ 6	2N+1 QjN=+11 Rj +2 PiN=1 liRi+2RN+1P
NTυ(ε) = ISTucker(2R1,…，2Rn, 2Rn +1)∣ ≤ (-----ε---)
Let ε = 1∕2. It then holds that log(3(N + 2)∕ε) < 3 log(N + 2) and logNTU(ε) ≤
2N+1dTMυ log[(3N + 6)∕ε], where dTMυ = QjN=+11 Rj + PiN=1 liRi + RN+1P. By a method similar
to the proof of Lemma 2, we can show that
and
P
P
sup ∆∈S2TKU	∣∆∣2n	≥ αRSM ≤ 2 exp -cHn	(KL、 14κU,	2 + 3cNdTMυ	,	(14)
( inf ∆∈S2TKU	∣∆∣2n	≤ αRSC ≤ 2 exp -cHn	(KL、 ∖4κU∕	2 + 3cNdTMυ	.	(15)
19
Under review as a conference paper at ICLR 2021
where cN = 2N+1 log(N + 2). Moreover, similar to equation 13, by applying Lemma 3, we can
show that
P
sup
∆∈S2TKU
—X ξihZi, ∆ ≥ 8σ√αRSM (∖ CN"M + Jl. ∣ , Sup k∆kj ≤ αRSM
n i=1	n	n	∆∈S2TKU
P
≤
max	— XξihZi, δji ≥ 4σ√αRSM (N m m + J~—), Sup iakn ≤ &rsm
1≤j≤NTU(ε) n i=1	n	n	∆∈S2TKU	n
≤ x P(1 x ξihZi, ^δ j i ≥ 4σ√αRSM (rCM nM+rɪ),归力：≤ *乂)
≤ exp{-dTMU - 8δ},
(16)
where ε = (2√2)-1 and 2m+4log(N + 2) — 2m +1 log(3(N + 1)∕ε) > 1. By a method similar
to the proof of Theorem 1, We can show that, with probability 1 — 4 exp{ — [ch (0.25κl∕ku)2n —
3cM log(N + 2)dTMU]} — exp{—dTMU — 8δ},
Sup	X XξihZi,	δ>≤	8σ√αRSM	∖∣nm +	and	iakn ≥	αRsc iakF,
∆∈S2TKU n i=1	n	n	n	F
which, together with equation 11, leads to
k∆ IlF ≤ ι6σ√αRSM rcNdMUL+rδ),
αRSC	n	n
k∆Iln ≤ 16σαRSM (rcNd≡ +
αRSC	n
err2(WTU) ≤ (1^θRSMY (CMdMU + 2)十 02.
αRSC	n n
We accomplished the proof.
A.3.3 Proofs of Corollary 1
When the kernel tensor A has a Tucker decomposition form of A = G×ι H ⑴ ×2 H ⑵ × 3 ∙∙∙×m+1
H(M+1), and the multilinear rank is (Ri, R?,…，RN +ι). Let H = G ×ι H ⑴ ×2 H ⑵ ×3 …XN
H(N), which is a tensor of size Rl1× ×lN×RN+1. The mode-(N + 1) unfolding of H is a matrix
with RN +ι row vectors, each of size RL, and we denote them by gi,…,gRN+ι. Fold grS back
,h- 、 一	，	一,、,、,，	一.	一
Vec(Hr), where Hr ∈ Rl1×…×lN and 1 ≤ r ≤ RN +i.
into tensors, i.e. gr
Moreover, let
H(N+1) = (hiN +1), ∙∙∙ , h(N+1))0, where h(N +1) is a vector of size RRN+1 and we denote its rth
entry as h(kN,r+i), for 1 ≤ r ≤ RN+i. It can be verified that, for each 1 ≤ k ≤ K,
Ak = G × i H⑴ ×2 H⑵…×n H(N) XN+ihkN+1) = HXN+ihkN+1),
and hence,
K	RN+1 K	RN+1	K
X Bk X Ak = χ χ Bk “Hr ∙ hkN+i))= X	X hkN+i)Bk X Hr.
k=i	r=i k=i	r=i	k=i
By letting Ber = PkK=i h(kN,r+i)Bk and Aer = Her for 1 ≤ r ≤ RN+i , we can reformulate the
model into
RN+1
yi = h X BerXAer,Zii+ξi,
r=i
and the proof of (a) is then accomplished.
20
Under review as a conference paper at ICLR 2021
Suppose that the kernel tensor A has a CP decomposition form of A = PrR=1 αrh(r1) ◦ h(r2) ◦
・…。h(N +1), where hj) ∈ Rlj and kh(rj) k = 1 for all 1 ≤ j ≤ N. Moreover, h(rN+1) =
(h(rN,1+1),..., h(rN,K+1))0 ∈ RK and kh(rN+1)k = 1. Note that
R
Ak=X αr hrN+1)hr1)。h2)。…。hN),
r=1
for all 1 ≤ k ≤ K, and hence
K	RK
X Bk ㊈ Ak=XX Bk “% hrN+1)hr1)。hr2)。…。h(N ))
R
=X (hrN+1)Bk)“a, hr1)。h2。…。hN )).
r=1
By letting B, = hN+'1Bk and Ar = a,h,1)。h,2)。∙∙∙。hrN) for all 1 ≤ r ≤ R, We can
reparameterize the model into
R
yi= hX Br 区 Ar, Zii + ξi,
,=1
and the proof of (b) is then accomplished.
A.3.4 Lemmas Used in the Proofs of Theorems
Lemma 1. (Partition and covering number ofrestricted spaces) Consider SK = {PK=1 Bk 0 Ak :
Ak ∈ Rl1×l2×…×lN andBk ∈ Rp1 ×p2×…×pN} andSK = {W ∈ Sbκ : kWkF = 1}.
(a)	For any ∆ ∈ S2K, there exist W1, W2 ∈ SK such that ∆ = W1 + W2 and hW1, W2i =
0.
(b)	The ε-covering number of the set SK is
N(K,ε) ≤ (9∕ε)K(L+P +1),
where L = 11卜…IN and P = p1p2 •…PN.
Proof. For each K, we first define amap TK : SbK → RP×L. For any W = PkK=1 Bk 0Ak ∈ SbK,
we define that TK (W) = PkK=1 vec (Bk) vec(Ak)0 ∈ RP×L, which has the rank ofat most K.
1 -	A 一含	.1	-I C	m / * ∖ ∙	. rʌ	AI	τ-,∙	X	ι∙ . .1
For any ∆ ∈ S2K, the rank of matrix TK (∆) is at most 2K. As shown in Figure 6, we can split the
singular value decomposition (SVD) ofTK(∆) into two parts, and it can be verified that TK (∆) =
U1Λ1V10 + U2Λ2V20 and hU1Λ1V10, U2Λ2V20i = 0. Denote Ui = (u(1i),u(2i),. ..,u(Ki)) and
ViΛ0i = (v1(i), v2(i), . . . , vK(i)) for i = 1 and 2. We then fold uj(i)s and vj(i)s into tensors, i.e.
u(ki) = vec(B(ki)) and vk(i) = vec(A(ki)) with 1 ≤ k ≤ K. Let W1 = PkK=1 B(k1) 0 A(k1) and
W2 = PkK=1 B(k2) 0 A(k2). It then can be verified that W1, W2 ∈ SbK, ∆ = W1 + W2 and
hW1, W2i = 0. Thus, we accomplish the proof of (a).
Denote by Smatrix ⊂ RP ×L the set of matrices with unit Frobenius norm and rank at most K. Note
that T(SK) ⊂ Smatrix, while the ε-covering number for Smatrix is
|Smatrix| ≤ (9∕ε)K(L+P+1);
see Candes & Plan (2011). This accomplishes the proof of (b).
□
21
Under review as a conference paper at ICLR 2021
Figure 6: Splitting matrix T(∆) based on its singular value decomposition.
Lemma 2. (Restricted strong convexity and smoothness) Suppose that n & (KU/kl)2"m. It holds
that, with probability at least 1 _ 4 exp{- [ch (0.25κl/ku)2n _ 9"m]},
1n
αRsc ∣∣δ∣∣F ≤ - X(hZi, δ))2 ≤ αRSM IAlF,
n i=1
for all ∆ ∈ S2K, where S2K is defined in Lemma 1, cH is a positive constant from the Hanson-
Wright inequality, αRsc = kl/2 and grsm = 3ku/2.
Proof. It is sufficient to show that sup∆∈S2K k∆k2n ≤ αRSM and inf∆∈S2K k∆k2n ≥ αRSC hold
with a high probability, where k∆k2n := n-1 Pin=1 (hZi, ∆i)2 is the empirical norm, and S2K is
defined in Lemma 1. Without confusion, in this proof, we will also use the notation of ∆ for its
vectorized version, vec(∆). Note that
nnn
k∆∣n = n XgZ = - X Zi0AA0Zi = n X XPGAAUGXi
i=1	i=1	i=1
=-X0 [ln 乳(UG∆∆0UG)i X = WWQw,
(17)
where Q = n-1∑2 [In 乳(UgAA0UG)]∑2, W ∈ RnD is a random vector with i.i.d. standard
normal entries, zi = UGxi, xi = Vec(Xi), X = (x10, x20,..., xn0)0, and Σ = E(XX0) ∈ RnD×nD.
Denote WG = UGA ∈ RD and Sn-1 = {v ∈ Rn : kvk2 = 1}. Let λmax (Σii)be the maximum
0
eigenvalue of Σii = E(vec (Xi) vec (Xi) ) for 1 ≤ i ≤ n, and it holds that λmax(Σii) ≤ Cx for all
1 ≤ i ≤ n. For matrix Q, we have
tr(Q) = -tr (∑1 [In X WGnIn X wG]Σ2) = -tr([In X wG]Σ[In X Wg]) = - X(WG∑hWg)
i=1
≤ - X ( SUp	wG^iiwG) ( sup	A0UGUG A) ≤
n i=1 wG∈SPL-1 WG0 WG	∆∈SPL-1	G
1n
-]Tλmaχ(∑ij )Cu ≤ KU,
n i=1
(18)
and similarly, we can show that tr(Q) ≥ KL, where KL= cxcu	, KU =CxCu	and KL≤ KU. Thus,
KL≤EkAk2n = tr(Q) ≤KU.	(19)
Moreover, denote Qi = n-1Σ2 [In X WG] and note that Q = QιQ1. To bound the operator norm
of Q, we have
kQkop ≤
≤
≤
kQ1 k2op
SUp U0QlQιU = — SUp U0 [In X WG]∑[In X Wg]u
u∈Sn-1
n u∈Sn-1
-(SUp
n u∈Sn-1
IZInXWGG*") L∈un-1 u0[In X wG WG]u
(20)
—kWG k2 ≤ — kUGkOp kAk2 ≤ KU.
n
22
Under review as a conference paper at ICLR 2021
Finally we can use (18) and (20) to bound the Frobenius norm of Q. By some algebra, for any
square matrices A, B ∈ Rn×n, kAB k2F ≤ kAk2op kB k2F holds. Hence,
2
kQkF = kQιQlkF ≤ kQιk2p kQιkF = kQιkOptr(Q) ≤ KU.
This, together with (17), (20) and the Hanson-Wright inequality (Vershynin, 2018, Chapter 6), leads
to
P nMkn - EkAkh t0≤ 2exp (-cH min (高 op，⅛!)	(21)
≤ 2exp {—CHnmin (t∕κu, (t∕κu)2)},
where cH is a positive constant.
On the other hand,
1n
k∆k2 — E k∆k2 = — V ∆ZZi∆ — E(∆0zizi0∆) = ∆0Γ∆,
n	nn
i=1
where Γ = n-1 Ρn==1[z-iz-i0 一 E(ZiZi0)] is a symmetric matrix. Consider a ε-net S2κ, With the
cardinality of N(2K, ε), for the set S2κ. For any ∆ ∈ S2κ, there exists a ∆j ∈ S2κ SUch that
∣∣∆ — ∆j∣∣F ≤ ε. Note that ∆ — ∆j ∈ S4κ and, from Lemma 1, we can further find ∆ι, ∆2 ∈ S2κ
such that h∆ι, ∆2 = 0 and ∆ — ∆j = ∆ι + ∆2, and it then holds that ∣∣Δi∣∣f + ∣∣∆2∣∣F ≤
√2∣∆ — ∆j∣∣f ≤ √2ε. Moreover, for a general real symmetric matrix A ∈ Rd×d, U ∈ Rd and
v ∈ Rd, supkuk2=kvk2=1 u0Av = supkuk2=1 u0Au. As a result,
∆0Γ ∆ = ∆jΓ ∆j + (∆ι + ∆2)0Γ(∆1 + ∆2 + 2∆j)
≤ max	∆0j Γ∆j + 5ε sup ∆0Γ∆,
1≤j ≤N (2K,ε)	j	∆∈S2K
where (∣∣Δi∣∣f + ∣∣∆2∣f)2 + 2(∣Δi∣f + ∣∣∆2∣F)k^∆j∣f ≤ 2(ε + √2)ε ≤ 5ε as ε ≤ 1, and this
leads to
sup ∆0Γb∆ ≤ (1 — 5ε)-1	max	∆ j Γ ∆j.
∆∈S2K	1≤j ≤N (2K,ε) j
(22)
Note that, from Lemma 1(b), log N (2K, ε) ≤ 2dM log(9∕ε), where dM = K(L + P + 1). Let
ε = 1∕10, and then 2 log(9∕ε) < 9. Combining (21) and (22) and letting t = κL∕2, we have
P{∆suρ Bkn -Ekδ∣: ≥ ~L~} ≤ 2exp{-cHn (4κ~) + 9dMI,
which, together with (19) and the fact that κL ≤ κU , implies that
P {ʌs^ ∣∣∆∣∣n ≥ αRSM∣ ≤ 2exp {—c∏n (-KL-) +9"m},
where αRSM = 1.5κU ≥ κU + κL∕2.
(23)
By a method similar to (22), we can also show that
sup E ∣∆∣2n — ∣∆∣2n ≤ (1 — 5ε)-1 max
∆∈S2K	n	n	1≤j≤N(2K,ε)
which, together with (21), leads to
E∣a j∣∣n - ∣∣δ j∣∣n,
n
P {∆X EkAkn TAkn ≥ KL} ≤ 2exp {-^(Ku ?
where t = KL∕2 and ε = 1∕10. As a result,
+ 9dM	,
P {△% kAkn ≤ αRSC} ≤2exp {~cnn (-KLU)
+ 9dM	,
where αRSC = KL — KL∕2. This, together with (23), accomplishes the proof.
□
23
Under review as a conference paper at ICLR 2021
Lemma 3. (Concentration bound for martingale) Let {ξi , 1 ≤ i ≤ n} be independent σ2 -sub-
Gaussian random variables with mean zero, and {zi, 1 ≤ i ≤ n} is another sequence of random
variables. Suppose that ξi is independent of {zi, zi-1, . . . , z1} for all 1 ≤ i ≤ n. It then holds that,
for any real numbers α, β > 0,
P "(1 X ξizi ≥ α)∩ (1 X (zi)2 ≤ e)# ≤ exp (-2σ⅛).
Proof. We can prove the lemma by a method similar to Lemma 4.2 in SimchoWitz et al. (2018). □
A.4 Classification Problems
Starting from Section 2.2, We knoW that, for each input tensor X, the intermediate scalar output after
convolution and pooling has the folloWing form
K
output = hX, X(Bk 乳 Ak) ×1 UFI) ×2 UF ×3 …×N UFN)i = hZ, Wi,
k=1
where W = PK=I Bk 乳 Ak and Z = X ×ι UFI)O ×2 UF2) ×3 …×n UFN)0, Ak ∈ Rl1 ×l2×…×lN
is the kth kernel and Bk ∈ Rp1×p2× ×pN is the corresponding kth fully-connected weight tensor.
Consider a binary classification problem. We have the binary label output y ∈ {0, 1} with
p(y|Z) = (1 + exp[z, Wi)) -11+eXPZZAW i) y = eXP {yhZ, Wi-lOg[1 + eXP(hZ，Wi)]}.
Suppose we have samples {Zi, yi}in=1, we use the negative log-likelihood function as our loss func-
tion. It is given, up to a scaling of n-1 by
nn
Ln (W) = - — X yihZi, Wi + — X Φ(hZi, Wi),	(24)
n i=1	n i=1
where φ(z) = log(1 + ez) and its gradient and Hessian matrix is
VLn(W) = :Ln(W) = -1 Xyi Vec(Zi) + 1 X Φ0(hZi, Wi)Vec(Zi) and,	(25)
∂ vec(W)	n	n
i=1	i=1
Hn(W) = J? Ln((W)) =1 X Φ00(hZi, Wi)vec(Zi )vec(Zi)),	(26)
∂2 Vec(W)	n
with φ0(z)	=	1/(1 + e-z)	∈ (0, 1)	and φ00(z)	=	ez/(1+ ez)2 =	1/(e-z	+2+ ez) ∈ (0, 0.25)
[because e-z + ez ≥ 2]. Since Hn(W) is a positive semi-definite matrix, our loss function in (24)
is convex.
Suppose W is the minimizer to the loss function
^ . __________________
W ∈ arg min Ln(W),
W∈SbK∩B(R)
where SK = {Pk=1 Bk ㊈ Ak : Ak ∈ Rl1×l2×…×lN and Bk ∈ Rp1 ×p2×…×pN}, and B(R) is a
Frobenius ball of some fixed radius R centered at the underlying true parameter.
Similar to Fan et al. (2019), we need to make two additional assumptions to guarantee the locally
strong convexity condition.
Assumption 4 (Classification). Apart from Assumptions 2-3, we additionally assume that
(C1) {Vec(Xi)}in=1 are i.i.d gaussian vectors with mean zero and variance Σ, where Σ ≤ CxI.
(C2) the Hessian matrix at the underlying true parameter W* is positive definite and there exists
some κo > κu > 0, such that H(W*) = E(Hn(W*)) ≥ κo ∙ I, where KU = CxCu;
(C3) ∣∣W*kF ≥ ɑʌ/dM for some ConStant a, where dM = K (P + L + 1).
24
Under review as a conference paper at ICLR 2021
Notice that we can relax (C1) into Assumption 1, but it will require more techincal details. Also,
{vec(Xi)}in=1 can be sub-gaussian random vectors instead of gaussian random vectors.
Denote dM = K(P + L + 1).
Theorem 3 (Classification: CNN). Suppose that Assumptions 2&3 and Assumption 4 hold and
n & dM. Then, for some δ > 0,
∣∣W - w*∣∣f
with probability 1 - 4 exp {-0.25cn + 9dM} - 2 exp {-cγ dM - cδ}, where δ = Op(1), c and cγ
are some positive constants.
Denote dTMU = QjN=+11 Rj + PiN=1 liRi + RN+1P and dCMP = RN+1 + R(PiN=1 li + P).
Corollary 2 (Classification: Compressed CNN). Let (Wc, dM ) be (WcTU , dTMU) for Tucker decom-
position, or (WcCP, dCMP) for CP decomposition. Suppose that Assumptions in Theorem 3 hold and
n & cNdM. Then, for some δ > 0,
.■—■	,
∣∣W — W*∣∣F
≤ 2√κU ( ∕3cndM
—Ki 卜V n
+
with probability 1 - 4 exp {-0.25cn	+ 3cNdM} - 2 exp {-cγ dM - cδ}, where δ = Op(1), c and
cγ are some positive constants, and cN is defined as in Theorem 2.
We consider a binary classification problem as a simple illustration. In fact, the analysis frame-
work can be easily extended to a multiclass classification problem. Here, we consider a M -class
classification problem.
Because we need our intermediate output after convolution and pooling to be a vector of length M,
instead of a scalar, we need to introduce one additional dimension to the fully-connected weight
tensor. Hence, we introduce another subscript m to represent the class label. And the set of fully-
connected weights is represented as {Bk,m}1≤k≤K,1≤m≤M where each Bk,m is of size p1 × p2 ×
…X Pn .
Then, for each input tensor X, our intermediate output is a vector of size M, where the mth entry is
represented by
outputm = hZ, Wmi,
where Wm = PkK=1(Bk,m 0 Ak) is an N-th order tensor of size lipi × I2p2 ×∙∙∙× INPN.
For a M -class classification problem, we have the vector label output y ∈ (0, 1)M. Essentially, each
entry of y comes from a different binary classification problem, with M problems in total. We can
model it as
(	IZ)= (	1	Y-ym ( exp(hZ, Wmi) Ym
p(yml )= V+exp(hZ,Wmi)J	V+exp(hZ,Wmi)J
= exp {ymhZ, Wmi - log [1 + exp(hZ, Wmi)]}
Ifwe stack {Wm}mM=1 into a tensor Wstack, which is an N + 1-order tensor of size l1P1 × l2P2 ×
…× INPn × M. And We further introduce some natural basis vectors {em, ∈ RM}M=i. It can be
shown that
hZi, Wmi = hZi ◦ em, Wstacki,
'-------------------}
{z^
Zi
m
where ◦ is the outer product.
We can then recast this model into one With nM samples {Zim, ymi : 1 ≤ k ≤ K, 1 ≤ m ≤ M}.
The corresponding loss function is
Mn	Mn
Ln (Wstack) = - nM X X yihZm, Wstacki + nM X X (KhZml WstackiX
m=1 i=1	m=1 i=1
25
Under review as a conference paper at ICLR 2021
We can now use the techniques in Theorem 3 to show the following corollaries for multiclass clas-
sification problem.
Denote dMMC = K(MP + L + 1).
Corollary 3 (Multiclass Classification: CNN). Under similar assumptions as in Theorem 3, suppose
that n & dMMC. Then, for some δ > 0,
■—■	., ..
kWstack- W：tackkF
with probability 1 - 4 exp -0.25cn + 9dMMC - 2 exp -cγ dMMC - cδ , where δ = Op(1), c and
cγ are some positive constants.
Denote dMMC-TU = QjN=+11 Rj +PiN=1 liRi + RN+1PM and dMMC-CP = RN+1 + R(PiN=1 li +
PM).
Corollary 4 (Multiclass Classification: Compressed CNN). Let (Wcstack, dMMC) be
(Wcstack,TU, dMMC-TU) for Tucker decomposition, or (Wcstack,CP , dMMC-CP) for CP decom-
position. Suppose that Assumptions in Theorem 3 hold and n & cN dmMulti. Then, for some
δ > 0,
kWstack- W'CkkF ≤ 2^ (^^ +
κ1	n
with probability 1 - 4 exp -0.25cn + 3cN dMMC - 2exp -cγ dMMC - cδ , where δ = Op (1), c
and cγ are some positive constants, and cN is defined as in Theorem 2.
Proofof Theorem 3. Denote the sets SK = {PK=ι Bk 0 Ak : Ak ∈ Rl1×l2×…×lN and Bk ∈
Rp1 ×p2×…×pN} and SK = {W ∈ SK : ∣∣W∣∣f = 1}. We further denote ∆ = Vec(W - W ),
where W* is the underlying true parameter and W, W* ∈ SK, and define the first-order Taylor
error
En(∆) = Ln(W)- Ln(W*) -hVLn(W*), △〉.
Suppose Wc is the minimizer for the loss function, i.e.,
^ . ___________
W = argminLn(W).
W∈SbK
Denote ∆b = vec(Wc - W*). We then have
Ln(W) -Ln(W*) ≤ 0, which can be rearranged into En(∆) ≤ - DVLn(W*), △).
一 ~	m -	^	____V
Then, for some W between W and W*,
1 ∆ 0Hn (W)∆ ≤ ∣DvLn(W*), ∆E∣,
which leads to
k∆ kF SUp ∆0Hn(W )∆ ≤ 2k∆ kF SUp KVLn(W*),∆i∣.
∆∈S2K	∆∈S2K
(27)
From Lemma 4 and Lemma 5, when n & dM , we obtain that, for some δ > 0,
kWc - W*kF
with probability 1 - 4exp {-0.25cn + 9dM } - 2exp {-cγ dM - cδ}.
□
26
Under review as a conference paper at ICLR 2021
Now we proof several lemmas to be used in Theorem 3. For simplicity of notation, denote Xi =
VeC(Xi) and Zi = vec(Zi). It holds that Zi = UGxi, for 1 ≤ i ≤ n.
Lemma 4 (Deviation bound). Under Assumption 4(C3), suppose that n & d^, then, for some
δ > 0,
P
s SUP
(∆∈S2κ
KVLn(W*),∆>∣ ≥ 0.5√κu
m) } ≤ 2 eχp {-Cγ dM },
where δ = OP (1), dM = K (P + L + 1), KU = CXCU and CY is some positive constant.
Proof. Let n=(Zi, W*〉，and from (25),
1 n
hVLn(W*), △〉= — E[φ0(ηi) - yi]hzi, △),
n i=1
and we can observe that
E{[φ0(ηi) -yi](zi,∆>} = E{E[φ0(ηi) -y∣z](zi,∆>} = 0 = E[φ0(ηi) -yi]E[(zi,∆>].
It implies (i) EhVLn(W*), △〉= 0, (ii) the independence between φ0(ηi) - yi and hzi, △). And
from Lemma 6, the independence leads to k[φ0(ηi) - yi]{zi, ∆>∣∣ψι ≤ ∣∣φ0(ηi) - yi∣∣ψ2 ll(zi, ∆)kψ2.
Denote KU = CχCu. For any fixed ∆ such that ∣∣∆∣∣2 = 1,
IKZi,∆ikψ2 = khwi,∑1∕2UG∆>kψ2 ≤√KU.
This, together with ∣∣φ0(ηi) — yi∣∣ψ2 ≤ 0.25 in Lemma 7, gives US
k[φ'(ηi)-yi]hzi,∆i∣∣M ≤ 0.25√KU.
Then, we can use the Beinstein-type inequality, namely Corollary 5.17 in (Vershynin, 2010) to derive
that, for any fixed ∆ with unit /2-norm,
P {∣hVLn(W*),∆i∣ ≥ t} = p[1 XX [φ' (ηi )-yi]hzi, ∆i∣ ≥ t]
Si=I	I ʃ	(28)
.4 4t	16t2∖↑
-cn min	,---- > .
∖√KU KU ) J
≤ 2 exp
Consider a ε-net S2κ, with the cardinality of N(2K, ε), for the set S2κ. For any ∆ ∈ $2K, there
exists a Aj ∈ S2κ such that ∣∣∆ 一 Aj ∣∣p ≤ ε. Note that ∆ 一 Aj ∈ S4κ and, from Lemma 1(a),
we can further find ∆1, A2 ∈ S2κ such that hʌi, A2)= 0 and A — Aj = ∆1 + A2. It then
holds that I Ai IF + ∣∣A2∣∣F ≤ √⅜A — A j∣F ≤ √2ε since ∣∣A — A j ∣∣∣ = ∣∣A1∣∣ + ∣∣A21∣∣. As
a result,
IhVLn(W*), Ai∣ = IhVLn(W*), Aji ∣ + IhVLn(W*),A0∣ + IhVLn(W*), A2i ∣
≤ max	IhVLn(W*), Aji ∣ + √2ε sup IhVLn(W*), Ai ∣,
1≤j≤N (2κ,ε)	∆∈S2K
which leads to
△% ∣hVLn(W*),A>∣ ≤ (I- √2ε 厂ZmxKJhVLn(W*), A j i 1 .
Note that, from Lemma 1(b), log N (2K, ε) ≤ 2dM log(9∕ε), where dM = K (P + L + 1). Let
ε = (2√2)-i and then 2log(9∕ε) < 7. With (28), we can show that
P ʃ sup IhVLn(W*), Ai ∣ ≥ 2t∖ ≤ 2 exp --cn min ( ~y=, ɪθ^ [+7”从].
l∆∈S2K	J	I	∖√κU KU J	J
Take t = 0.25√κU(,dM∕n + ,δ∕n) where δ = Op(1), and there exists some Y such that
^d/n/n ≤ Y holds. We can finally show that
P
sup
(∆∈S2K
IhVLn(W*), Ai I ≥ 0.5√KU
≤ 2 exp {-Cγ dM — cδ},
where CY is some positive constant related to γ.
□
27
Under review as a conference paper at ICLR 2021
Lemma 5 (LRSC). Suppose that n & dM, under Assumptions 4, there exists some constant R > 0,
1,1 . 「	AlT — G	,	■ IlA IT A IT* Il / C
such that for any W ∈ S2κ satisfying ∣∣W 一 W ∣∣f ≤ R,
inf
∆∈S2K
κ
∆0Hn(W)∆ ≥ -21
holds with probability
1 — 4exp {—0.25cn + 9"m},
where κe1 = κ1 一 κU. And κ1 is defined in Lemma 8.
Proof. We divide this proof into two parts.
1.	RSC of Ln(W) atW = W*.
We first show that, for all ∆ ∈ S2K, the following holds with probability at least 1 —
2exp {—0.25c(kl∕ku)2n + 9"m},
∆0Hn(W*)∆ ≥κe,
where eκ = κ0 — κL > 0.
Let ηi = hZ, W*i and denote Z = /φ"(ηi)Z and We can see that
1n
Hn(W*) = — TeieiO and, H (W*)= EHJW*).
n
i=1
Denote xi = vec(Xi). Here, for simplicity, We assume {xi}in=1 to be independent gaussian vectors
With mean zero and covariance matrix Σ, Where cxI ≤ Σ ≤ CxI for some 0 < cx < Cx . We Will
also use the notation of ∆ for its vectorized version, vec(∆), and We consider ∆ With unit l2-norm.
Since ∣∣(∆, Zi ikψ2 = ∣∣(∆, pφ"(ηi)UG £1/2 Wiikψ2 ≤ 0.25√κU, where Wi is a standard gaussian
vector and κU = Cx Cu, We can shoW that
∣h∆, zeii2 — E h∆, zei2 ∣ψ1 ≤2∣h∆,zeii2∣ψ1 ≤ 4∣h∆, zeii∣2ψ2 ≤κU,
where the first inequality comes from Remark 5.18 in (Vershynin, 2010) and second inequality
comes from Lemma 5.14 in (Vershynin, 2010).
And hence, by the Beinstein-type inequality in Corollary 5.17 in (Vershynin, 2010), for any fixed ∆
such that ∣∆∣2 = 1, we have
P{∣∆0(Hn(W*) — H(W*))∆∣≥ t}
"{(△
i=1
,eii2- e[(δ, e)2]}
≥t
≤ 2 exp —cn min
P
With similar covering number argument as presented in Lemma 2 in our paper, we can show that,
p[ SUP ∣∆0(Hn(W*) — H (W* ))∆∣ ≥ 2t) ≤ 2exp J-Cnmin (-ɪ, ɪ-ʌʌ + 9"m),
∆∈S2K	κU κ2U
where dM = K(P + L + 1).
Llet t = 0.5κU. By Assumption 4(C1), we can obtain that, when n & dM,
P inf ∆0Hn(W*)∆ ≤ eκ ≤ 2 exp {—0.25cn + 9dM} ,
∆∈S2K
where eκ = κ0 — κU > 0.
2.	LRSC ofLn(W) aroundW*.
Define the event A = {|(W*, Zi)| > τsuP∆∈S2κ](△, zii∣} and construct the functions
1n
hn(W) = — fφ0(hZ, Wi)IAzizi0 and, h(W) = Ehn(W),
n
i=1
28
Under review as a conference paper at ICLR 2021
where τ is some positive constant to be selected according to Lemma 8. Since the difference between
hn(∙) and Hn(∙) is the indicator function, it holds that Hn(∙) ≥ hn(∙).
We will finish the proof of LRSC in two steps. Firstly, we show that, with high probability,
hn(W*) is positive definite on the restricted set S2κ. Secondly, We bound the difference between
∆0hn(W)∆ and ∆0hn(W*)∆, and hence show that hn(W) is locally positive definite around
W*. This naturally leads to the LRSC of Ln(W) around W*.
From Lemma 8, we can select T, such that h(W*) ≥ κιI. Following similar arguments as in
the first part, we can show that for all ∆ ∈ S2K, the following holds with probability at least
1 - 2 exp {-0.25cn + 9dM},
∆0hn(W*)∆ ≥ κe1,	(29)
where eκ1 = κ1 - κU > 0.
In the meanwhile, for any W ∈ SbK such that kW - W* kF ≤ R, where R can be specified later to
satisfy some conditions,
n	n
∣hn(W) - hn(W*)∣ = — X Φ'0(hZi, Wi)IAzizi0— — X φ00(hZi, W*i)lAZizi0
n i=1	n i=1
1n
≤ - X∣φ00(hzi, Wi)- φ00(hzi, W*i)IIAzizi0
n i=1
n
=-X ∣φ000 (hζi, Wi)hzi, W - W*i IIAzizi0,
n i=1
where W lies between W and W*, and φ000 (ζ) = ez(1 - ez)/(1 + ez)3. Given the event A holds,
choose R < τ, we can lower bound the term,
IhZi,Wil ≥ IhZi,W*i∣-	sup	IhZi,∆i∣ ≤ (τ - R) sup IhZi,∆i∣.
∆∈Sb2K,k∆kF≤R	∆∈S2K
Notice that, for all z ∈ R, the third order derivative of the function φ(z) is upper bounded as
Iφ000(z)I ≤ 1/IzI. This relationship helps us further bound the term,
∣Φ000 (hZi, Wi)hZi, W - W*i∣ ≤
^""- '≤ ≤ — ~~^j ~^"^ ≤ ~ ʌ ^^^ ^ ~~ ~ 1	^^^^
I	I	IhZi, Wi∣	(τ - R)SUP∆∈S2K IhZi, ∆i∣
RsuP∆∈S2κ IhZi, ∆iI	R
≤ --------------------:-----=------.
(T - R) sup∆∈S2K IhZ, δ>i	T - R
Hence, we can show that
p( SUp I∆0[hn(W) - hn(W*)]∆I ≥ t] ≤ PI -R- SUp 1 X ∆0zizi0∆ ≥ t∖.
∆∈S2K	T - R ∆∈S2K n i=1
By setting t = grsmR∕(t - R), where grsm = 3κu/2, we can use the equation (16) in Lemma 2
to obtain, as long as n & dM,
P {∆suLA[hn(W)- hn (W*)Q ≥	≤ 2exp {-cH n + 9dM}.
By rearranging terms, this is equivalent to
P <	inf	∆0hn(W)∆	≤ sup ∆0hn(W*)∆	— αRSMA ∖	≤ 2exp {-chn	+ 9"m}.
I	δ∈s2K	∆∈S2K	t - R I
denoted as the event B1
If we define the event B2 = {inf∆∈S2K ∆0hn(W*)∆ ≤ κe1} and denote its complementary event
by B2c, and from (29), we know that P(B2) ≤ 2exp {-0.25cn + 9dM}. It can be seen that
P ([ inf ∆0hn(W)∆ ≤ eι- αRsM∣1 ∩ Bc) ≤ P(B1 ∩ Bc) ≤ P(Bi)
∆∈S2K	T - R
29
Under review as a conference paper at ICLR 2021
So, if We choose R to be sufficiently small, such that grsmR∕(t - R) ≤ Kι∕2, it holds that,
P {△% △%(W)A ≤ κ21} ≤ P ({∆* △%(W)A ≤ 而-
OR-MR} ∩ Bc) +P(B2).
This, together with Hn(∙) ≥ hn(∙), leads us to conclude that, when n & dM, there exists some
R > 0, such that for any W ∈ S2κ satisfying ∣∣W - W ∣∣f ≤ R,

∆∈inSf2KA0Hn(W)A
≥ K1,
holds with probability
1 - 2 exp {-0.25cH n + 9dM} - 2exp {-0.25cn + 9dM} .
We accomplished our proof of the LRSC of Ln (W) around W*.	□
Lemma 6. For two sub-gaussian random variables, X and Y, when E(XY ) = EXEY, i.e. X is
independent to Y , it holds that
∣XY∣ψ1 ≤∣X∣ψ2∣Y∣ψ2.
Lemma 7.
∣φ0(ηi) - yi∣ψ2 ≤ 0.25.
Proof. Firstly, we observe that
E (exp{λ [φ0(ηi) - yi]}∣zi) = exp{λφ0(ηi)} exp{-φ(ηi)} + {λ [φ0(ηi) - 1]} exp{ηi - φ(ηi)}
= exp{λφ0(ηi) - φ(ηi)}[1 + exp{ηi - λ}]
= exp{φ(ηi - λ) - φ(ηi) + λφ0(ηi)}
= exp{0.5λ2φ00(η*)} ≤ exp{0.125λ2}.
It then holds that E (exp{λ [φ0(ηi) - yi]}) = E{E (exp{λ [φ0(ηi) - yi] }|zi)} ≤ exp{0.125λ2},
and this implies that ∣φ0(ηi) - yi ∣ψ2 ≤ 0.25.
□
Lemma 8. Under Assumption 4, there exists a universal constant τ > 0 such that h(W*) ≥ κ1I,
where κ1 is a positive constant.
Proof. We first show that for any p0 ∈ (0, 1), there exists a constant τ such that
P(|hW*,Zii| >τ sup |hA,zii|) ≥p0.
∆∈S2K
We would separately show that
P(∣hW*, Ziil >cιPdM) ≥ p0+1 and, P( SUp ∣(∆, zi)| ≤ CQPM) ≥ p0+1,
'------------{----------}	2	∆∈S2K	2
denoted by event Di	∣	∙sz	/
denoted by event D2
for some positive constants c1 and c2. Using the relationship P(D1 ∩ D2) = P (D1) + P (D2) -
P (D1c ∪D2c) ≥ P(D1) +P(D2) - 1, it follows naturally that
P(lhW*, Ziil >τ SUp ∣h∆, ziil) ≥ p0+1 + p0+1 - 1= PO,	(30)
∆∈S2K	2	2
where τ = c1 ∕c2 .
Since xi is a gaussian vector with mean zero and covariance Σ, zi = UG0 xi is a zero-mean gaussian
vector with covariance given by UG0 ΣUG, and hW*, Zii = hvec(W*), zii also follows a normal
distribution with mean zero and variance (also its sub-Gaussian norm) upper bounded by κU ∣W* ∣2F,
where κU = Cx Cu .
30
Under review as a conference paper at ICLR 2021
Since from Assumption 4(C2), k W* kF ≥ Oidm, We can take ci to be sufficiently small such that
P(D1)
(IhW*, Ziil
∖κu kW*kF
≥ P(∣χ∣ > κc1α) ≥
P0 + 1
2
Where x is a gaussian variable With variance upper bounded by 1.
Then, We can also observe that, for any fixed ∆ ∈ S2K, h∆, zii is a gaussian variable With zero
mean and variance upper bounded by κU. We can use the concentration inequality for gaussian
random variable to establish that
t2
P (lh∆, ziil ≥ t) ≤ 2exp(—-),
κU
for all t ∈ R. We can further use the union bound to shoW that
P ( sup ∣h∆, zii∣ ≥ t) ≤ 2exp(--——+7dM).
∆∈S2K	κU
Let t = c2 √dM for some positive constant c2 > √7κu. We can choose c2 large enough such that
P( sup Ih∆, ziil ≥ C2pM ≤ 1 -p0 .
∆∈S2K	2
The probability at (30) is hence shoWn.
NoW, We Will take a look at the matrix h(W*) and shoW that it is positive definite. Same as in
Lemma 5, We denote the event A = {lhW*, Ziil > τ sup∆∈S lh∆, ziil}, and its complement by
Ac, then
∆0h(W*)∆ = 1E {XX φ00(hZi, Wi)lA∆0zizi0∆}
=1E {XXφ00(hZi, Wi)∆0zizi0∆} - 1E {XXφ00(hZi, Wi)IAC∆0zizi0∆}
=∆0H(W*)∆ - 1E {XXφ00(hZi, Wi)IAC∆0zizi0∆}
(i)
≥ κ0
1
nt
E Xn [φ00(hZi,Wi)]2h∆,zii4	tuuE Xn IAC
(ii)
≥ κ0 -
√3κU∕1	ʌ
(1 - PO),
—
Where (i) folloWs from Assumption 4(C1), And since h∆, zii is a gaussian variable With mean zero
and variance bounded by κU, its fourth moment is bounded by3κU. Also, by φ00(z) ∈ (0, 0.25) for
all z ∈ R, (ii) can be shoWn.
Here, we can takepo to be small enough such that 0.25√3κu (1 -po) ≤ 0.5κ0 holds. It follows that
∆0h(W*)∆ ≥ κ1,
with κι = 0.5κ0. We hence accomplished the proof of the lemma.	□
A.5 Additional Experiments
A.5.1 More experiments for theoretical results
We first provide some additional implementation details for Section 4.1 in the paper. We consider
two types of inputs for verification of the sample complexity in Theorem 1. For the time-dependent
inputs, we generate a sequence of vectors {xi} with size Rd from a stationary VAR(1) process,
where d = 245 or 192. The VAR(1) process has a transition matrix with spectral norm less than
31
Under review as a conference paper at ICLR 2021
Figure 7: Additional experiments for efficient number of kernels.
1 to ensure stationarity, and a white noise sequence which follows multivariate standard normal
distribution. The first 50 simulated data points in the sequence are discarded to alleviate the influence
of initialization, and the length of the resulting sequence is equal to the training sample size plus the
testing sample size, i.e. n + 200. We then reshape {xi } into a sequence of tensors of shape (7, 5, 7)
or (8, 8, 3), which are the time-dependent inputs for experiments under S1/S2 or S3/S4.
Next, we conduct extra experiments for efficient number of kernels for a CP block design. This study
uses 32 × 32 inputs with 16 channels, and we set stride to 1 and pooling sizes to (5, 5). We generate
the orthonormal factor matrices {H(j), 1 ≤ j ≤ 4}, where H(1) is of size R8×R, H(2) is of size
R8×R, H(3) is of size R16×R and H(4) is of
If we denote the orthonormal column vectors
size RK×R where R = 8 and K ∈ {8, 16, 24, 32}.
of H (j) by h(rj) where 1 ≤ r ≤ R and 1 ≤ j ≤ 4,
the stacked kernel tensor A can then be generated as A = PrR=1 h(r1) ◦ h(r2) ◦ h(r3) ◦ h(r4) . And it
can be seen that A has a CP rank of R = 8. We split the stacked kernel tensor A along the kernel
dimension to obtain {Ak , 1 ≤ k ≤ K} and generate the corresponding fully-connected weight
tensors {Bk, 1 ≤ k ≤ K} with standard normal entries. The parameter tensor W is hence obtained,
and we further normalize it to have unit Frobenius norm to ensure the comparability of estimation
errors between different Ks. The block structure in Figure 7(1) is employed to train the network,
and it is equivalent to the bottleneck structure with a CP decomposition on A; see Kossaifi et al.
(2020b). We can see that as K increases, there is more redundancy in the network parameters. Fifty
training sets are generated for each training size, and we stop training when the target function drops
by less than 10-5. From Figure 7(2), the estimation error increases as K is larger, and the difference
is more pronounced when training size is small.
A.5.2 Ablation studies on more block designs
In this section, we present the results from the studies of K/R ratios on two popular networks
structures, namely, ResNeXt (Xie et al., 2017) and Shufflenet v2 (Ma et al., 2018). To start with, we
will show the bottleneck block structure and its respective K/R ratio in ResNeXt and Shufflenet.
In Figure 8, we represent the bottleneck block structure of ResNeXt consisting of 3 layers: a 1 × 1
convolution layer, a 3 × 3 group convolution layer followed by another 1 × 1 convolution layer.
During group convolution phase, there are g parallel paths, each has a bottleneck size of r, which
∣3×3 Gconv,
group = g
Figure 8: Equivalent ResNext building blocks. Both (1) and (2) represent a block of ResNeXt with
cardinality g, With R = r ∙ g. Here, the expansion ratio t = K/R, and takes values of 1,2 and 4.
32
Under review as a conference paper at ICLR 2021
I Chamlel SPIit ∣
IChamlel ShUffIel
Bottleneck
∕∣∑^ K	► ʌ
1 × 1conv
3 × 3 DWconv
……R →∣
Figure 9: A basic block of Shufflenet v2, with its bottleneck structure depict explicitly. Here, the
expansion ratio t = K/R, and takes values of 1,2 and 4.
Table 3: Test accuracy(%) of ResNeXt and ShUfflenet on SVNH.
t	ResNeXt	#FLOPs	#Params	Shufflenet	#FLOPs	#Params
1	96.20	0.05GMac	1.45M	95.93	0.05GMac	1.33M
2	96.25	0.07GMac	2.13M	96.08	0.08GMac	1.67M
4	96.20	0.14GMac	4.44M	96.03	0.16GMac	3.84M
is also the rank of the output channel in each individual path; see also Figure 1 in Xie et al. (2017).
Since the output from each path is aggregated via summation, the rank of the output channel of the
entire bottleneck block is equal to R = r ∙ g. Subsequently, we can define the expansion ratio of this
block to be t = K/R.
In Figure 9, we represent the bottleneck block structure of Shufflenet. It consists of 3 layers: a
1 × 1 convolution layer, a 3 × 3 depthwise convolution layer followed by another 1 × 1 convolution
layer. So, the only difference between this block structure and that in ResNet (He et al., 2016) is
that the full 3 × 3 convolution is replaced by the depthwise separable convolution. It follows that the
expansion ratio is t = K/R.
We conducted the experiments on the SVNH dataset and take t = 1, 2, 4, since t = 2 or t = 4 in
particular, is commonly used in practice. We followed the design of ResNeXt-50 (see Table 1 in Xie
et al. (2017)) and Shufflenet v2-50 (see Appendix Table 2 in Ma et al. (2018)), only be removing
the three convolution block in ”conv5” to avoid overfitting. And we set R = 24, 58, 116, 232 as the
bottleneck rank for ”conv1”-”conv4”, respectively. And K = t ∙ R is taken according to different
values of t. The implementations are similar to before. Following the practice in He et al. (2016),
we adopt batch normalization(BN) (Ioffe & Szegedy, 2015) right after each convolution and before
the ReLU activation. For ResNeXt, the cardinality of the group convolution is set to be 32, and we
initialize the weights as in He et al. (2015). We use stochastic gradient descent with weight decay
10-4, momentum 0.9 and mini-batch size 128. The learning rate starts from 0.1 and is divided by 10
for every 100 epochs. We stop training after 350 epochs, since the training accuracy hardly changes.
The training accuracy is approximately 96%. We set seeds 1-5 and report the worst case scenario of
the test accuracy in Table 3.
We can see that, when t = 1, the test accuracy is comparable to when t = 2 or 4, and the number
of parameters are reduced by a lot. In fact, the ratio of number of parameters at a single bottleneck
block for t = 1, 2 and 4 is roughly 1 : 2 : 4, for both networks. This implies that the number
of parameters will increase dramatically when we have a deeper CNN, for instance, ResNeXt-101.
Hence, it is always recommended to keep t = 1 to achieve more parameter efficiency without
sacrificing the test accuracy.
33