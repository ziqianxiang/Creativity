Under review as a conference paper at ICLR 2021
Putting Theory to Work:
From Learning Bounds to Meta-Learning
Algorithms
Anonymous authors
Paper under double-blind review
Ab stract
Most of existing deep learning models rely on excessive amounts of labeled train-
ing data in order to achieve state-of-the-art results, even though these data can be
hard or costly to get in practice. One attractive alternative is to learn with little
supervision, commonly referred to as few-shot learning (FSL), and, in particular,
meta-learning that learns to learn with few data from related tasks. Despite the
practical success of meta-learning, many of its algorithmic solutions proposed in
the literature are based on sound intuitions, but lack a solid theoretical analysis of
the expected performance on the test task. In this paper, we review the recent ad-
vances in meta-learning theory and show how they can be used in practice both to
better understand the behavior of popular meta-learning algorithms and to improve
their generalization capacity. This latter is achieved by integrating the theoretical
assumptions ensuring efficient meta-learning in the form of regularization terms
into several popular meta-learning algorithms for which we provide a large study
of their behavior on classic few-shot classification benchmarks. To the best of our
knowledge, this is the first contribution that puts the most recent learning bounds
of meta-learning theory into practice for the task of few-shot classification.
1	Introduction
Since the very seeding of the machine learning field, its algorithmic advances were inevitably fol-
lowed or preceded by the accompanying theoretical analyses establishing the conditions required for
the corresponding algorithms to learn well. Such a synergy between theory and practice is reflected
in numerous concepts and learning strategies that took their origins in the statistical learning theory:
for instance, the famous regularized risk minimization approach is directly related to the minimiza-
tion of the complexity of the hypothesis space, as suggested by the generalization bounds established
for supervised learning (Vapnik, 1992), while most of the adversarial algorithms in transfer learn-
ing (e.g., DANN from (Ganin & Lempitsky, 2015)) follow the theoretical insights provided by the
seminal theory of its domain (Ben-David et al., 2010).
Even though many machine learning methods now enjoy a solid theoretical justification, some more
recent advances in the field are still in their preliminary state which requires the hypotheses put
forward by the theoretical studies to be implemented and verified in practice. One such notable
example is the emerging field of meta-learning, also called learning to learn (LTL), where the goal
is to produce a model on data coming from a set of (meta-train) source tasks to use it as a starting
point for learning successfully a new previously unseen (meta-test) target task with little supervision.
This kind of approach comes in particularly handy when training deep learning models as their
performance crucially depends on the amount of training data that can be difficult and/or expensive
to get in some applications. Several theoretical studies (Baxter, 2000; Pentina & Lampert, 2014;
Maurer et al., 2016; Amit & Meir, 2018; Yin et al., 2020)1 provided probabilistic meta-learning
bounds that require the amount of data in the meta-train source task and the number of meta-train
tasks to tend to infinity for efficient meta-learning. While capturing the underlying general intuition,
these bounds do not suggest that all the source data is useful in such learning setup due to the
1We omit other works for meta-learning via online convex optimization (Finn et al., 2019; Balcan et al.,
2019; Khodak et al., 2019; Denevi et al., 2019) as they concern a different learning setup.
1
Under review as a conference paper at ICLR 2021
additive relationship between the two terms mentioned above. To tackle this drawback, two very
recent studies (Du et al., 2020; Tripuraneni et al., 2020) aimed at finding deterministic assumptions
that lead to faster learning rates allowing meta-learning algorithms to benefit from all the source
data. Contrary to probabilistic bounds that have been used to derive novel learning strategies for
meta-learning algorithms (Amit & Meir, 2018; Yin et al., 2020), there was no attempt to verify the
validity of the assumptions leading to the fastest known learning rates in practice or to enforce them
through an appropriate optimization procedure.
In this paper, we bridge the meta-learning theory with practice by harvesting the theoretical results
from Tripuraneni et al. (2020) and Du et al. (2020), and by showing how they can be implemented
algorithmically and integrated, when needed, to popular existing meta-learning algorithms used for
few-shot classification (FSC). This latter task consists in classifying new data having seen only few
training examples, and represents one of the most prominent examples where meta-learning has
shown to be highly efficient. More precisely, our contributions are three-fold:
1.	We identify two common assumptions from the theoretical works on meta-learning and
show how they can be verified and forced via a novel regularization scheme.
2.	We investigate whether these assumptions are satisfied for popular meta-learning algo-
rithms and observe that some of them naturally satisfy them, while others do not.
3.	With the proposed regularization strategy, we show that enforcing the assumptions to be
valid in practice leads to better generalization of the considered algorithms.
The rest of the paper is organized as follows. After presenting preliminary knowledge on the meta-
learning problem in Section 2, we detail the existing meta-learning theoretical results with their cor-
responding assumptions and show how they can be enforced via a general regularization technique in
Section 3. Then, we provide an experimental evaluation of several popular few-shot learning (FSL)
methods in Section 4 and highlight the different advantages brought by the proposed regularization
in practice. Finally, we conclude and outline future research perspectives in Section 5.
2	Preliminary Knowledge
We start by formally defining the meta-learning problem following the model described in Du et al.
(2020). To this end, we assume having access to T source tasks characterized by their respective
data generating distributions {μt}T=ι supported over the joint input-output space X X Y with X ⊆
Rd and Y ⊆ R. We further assume that these distributions are observed only through finite size
samples of size n1 grouped into matrices Xt = (Xt,1, . . . , Xt,n1 ) ∈ Rn1 ×d and vectors of outputs
yt = (yt,1,...,yt,n1) ∈Rn1, ∀t∈ [[T]] := {1,...,T}.
Given this set of tasks, our goal is to learn a shared representation φ belonging to a certain class of
functions Φ := {φ | φ : X → V, V ⊆ Rk} and linear predictors Wt ∈ Rk, ∀t ∈ [[T]] grouped in a
matrix W ∈ RT ×k . More formally, this is done by solving the following optimization problem:
1 T n1
φ, W = arg min',kɪɪ'(yt,i, hWt, φ(xt,i)i),
φ∈Φ,W∈RT×k 2Tn1 t=1 i=1
(1)
where ` : Y × Y → R+ is a loss function. Once such a representation is learned, we want to apply
it to a new previously unseen target task observed through a pair (XT +1 ∈ Rn2×d, yT+1 ∈ Rn2)
containing n samples generated by the distribution μτ +1. We expect that a linear classifier W
learned on top of the obtained representation leads to a low true risk over the whole distribution
μτ +ι. More precisely, We first use φ to solve the following problem:
n2
1
WT +1 = argmin —〉/(yτ +ι,i, hw,φ(xτ +ι,i)i).
w∈Rk n2 i=1
Then, We define the true target risk of the learned linear classifier WT +ι as:
^
^
L(φ, WT +1) = E(x,y)〜“τ+1 ['(y, hwT +1,φ(X)i)]
and want it to be small and as close as possible to the ideal true risk L(φ*, WT +J where
Vt ∈ [[T + 1]] and (x, y)〜μt,
y = hw*,φ*(x)i + ε, ε ~N(0,σ1 2).
(2)
2
Under review as a conference paper at ICLR 2021
Equivalently, most of the works found in the literature seek to upper-bound the excess risk defined
as ER(φ, W τ +ι) := L(φ, W T+ι) — L(φ , WT +J With quantities involved in the learning process.
Remark 1 We note that many popular meta-learning algorithms used for FSL do not follow exactly
the approach described above. However, we believe that the exact way of how this is done algo-
rithmically (with or without the support set, with or without learning episodes) does not change the
statistical challenge of it which is to learn a model that can provably generalize with little supervi-
sion. Supervised learning theory tells us that generalization in this case is poor (not enough target
data and it is difficult to rely on data coming from different probability distributions), while the theo-
retical works we built upon suggest that source data may contribute in improving the generalization
of the learned model alongside the target data if the assumptions described below are satisfied.
3	From Theory to Practice
In this section, We highlight main theoretical contributions that provably ensure the success of meta-
learning in improving the performance on the previously unseen target task With the increasing
number of source tasks and the amount of data available for them. We then concentrate our atten-
tion on the most recent theoretical advances leading to the fastest learning rates and shoW hoW the
assumptions used to obtain them can be forced in practice through a novel regularization strategy.
3.1	When does Meta-learning Provably Work?
One requirement for meta-learning to succeed in FSC is that a representation learned on meta-train
data should be useful for learning a good predictor on the meta-test data set. This is reflected by
bounding the excess target risk by a quantity that involves the number of samples in both meta-train
and meta-test samples and the number of available meta-train tasks.
To this end, first studies in the context of meta-learning relied on probabilistic assumption (Baxter,
2000; Pentina & Lampert, 2014; Maurer et al., 2016; Amit & Meir, 2018; Yin et al., 2020) stating
that meta-train and meta-test tasks distributions are all sampled i.i.d. from the same random distri-
bution. This assumption, hoWever, is considered unrealistic as in FSL source and target tasks’ data
are often given by different draWs (Without replacement) from the same dataset. In this setup, the
above-mentioned Works obtained the bounds having the folloWing form:
ER(Φ, Wτ +ι) ≤ O
This guarantee implies that not only the number of source data, but also the number of tasks should
be large in order to draW the second term to 0. An improvement Was then proposed by Du et al.
(2020) and Tripuraneni et al. (2020) that obtained the bounds on the excess risk behaving as
O
+
k
and O
kd
n1 T
+----
n2
respectively, where k《d is the dimensionality of the learned representation and O(∙) hides loga-
rithmic factors. Both these results shoW that all the source and target samples are useful in minimiz-
ing the excess risk: in the FSL regime where target data is scarce, all source data helps to learn well.
From a set of assumptions made by the authors in both of these works , we note the following two:
Assumption 1. The matrix of optimal predictors W* should cover all the directions in Rk evenly.
More formally, this can be stated as
Rσ (W*) = σ^ = O(1),
σk(W*)
(3)
where σ%(∙) denotes the ith singular value of W*. As pointed out by the authors, such an assumption
can be seen as a measure of diversity between the source tasks that are expected to be complementary
to each other in order to provide a meaningful representation for a previously unseen target task.
3
Under review as a conference paper at ICLR 2021
Figure 1: Illustration of the example from Section 3.2 with ε = 0.02.
Assumption 2. The norm of the optimal predictors W should not increase with the number of tasks
seen during meta-training2. This assumption says that the classification margin of linear predictors
should remain constant thus avoiding over- or under-specialization to the seen tasks.
While being highly insightful, the authors did not provide any experimental evidence suggesting that
verifying these assumptions in practice helps to learn more efficiently in the considered learning
setting. To bridge this gap, we propose to use a general regularization scheme that allows us to
enforce these assumptions when learning the matrix of predictors in several popular meta-learning
algorithms.
3.2	Putting Theory to Work
As the assumptions mentioned above are stated for the optimal predictors that are inherently linked
to the data generating process, one may wonder what happens when these latter do not satisfy them.
To this end, we aim to answer the following question:
--——.
Given W* such that Rσ (W*)》1, can we learn W with Rσ (W) ≈ 1 while
solving the underlying classification problems equally well?
It turns out that we can construct an example illustrated in Fig. 1 for which the answer to this
question is positive. To this end, let us consider a binary classification problem over X ⊆ R3 with
labels Y = {-1, 1} and two source tasks generated for k, ε ∈ ]0, 1], as follows:
1.	μι is uniform over {1 一 kε, k, 1} X {1} ∪ {1 + kε, k, —1} X { —1};
2.	μ2 is uniform over {1 + kε, k, k-1} X {1} ∪ { —1 + kε,k, 1++k} X { -1}.
We now define the optimal representation and two optimal predictors for each distribution as the
solution to Eq. 1 over the two data generating distributions and Φ = {φ∣ φ(x) = Φtx, Φ ∈ R3×2}:
2
φ*, W* = arg min X, E '(y, <Wi,Φ(x)i),
φ∈φ,W∈R2×2 i = ι (x,y)^μi
(4)
One solution to this problem can be given as follows:
W*
ε
where φ* projects the data generated by μi to a two-dimensional space by discarding its third dimen-
sion and the linear predictors satisfy the data generating process from Eq. 2 with ε = 0. One can
verify that in this case W* have singular values equal to √2 and √2ε, so that the ratio Rσ (W*) = ɪ:
when ε → 0, the optimal predictors make the ratio arbitrary large thus violating Assumption 1.
2While not stated as a separate assumption, in Du et al. (2020) assume it to derive the Assumption 1 men-
tioned above. See p.5 and the discussion after Assumption 4.3 in their pre-print.
4
Under review as a conference paper at ICLR 2021
Let us now consider a different problem where we want to solve Eq. 4 with a constraint that forces
linear predictors to satisfy Assumption 1:
2
φb, Wc = arg min X E	'(y, hwi,φ(x)i),	s.t. Rσ(W) ≈ 1.	(5)
Φ∈φ,w∈R2×2 i=ι(X,yhμa
Its solution is different and is given by
Φb
00
1
-ε
Similarly to Φ*, Φ PrCjects to a two-dimensional space by discarding the first dimension of the data
generated by μi. The learned predictors in this case also satisfy Eq. 2 with ε = 0, but contrary to
W*, Rσ (W) = ∖∕2+ε2+ε√⅜≡- tends to 1 when ε → 0.
2+ε -ε ε +4
Several remarks are in order here. First, it shows that even when W* does not satisfy Assumption 1
in the space induced by φ*, it may still be possible to learn anew representation space φ such that the
optimal predictors in this space will satisfy Assumption 1. This can be done either by considering
the constrained problem from Eq. 5, or by using a more common strategy that consists in adding
Rσ(W) directly as a regularization term
7
φ, W = arg min
φ∈Φ,W∈RT ×k
2Tn1
T n1
ΣΣ'(yt,i, hwt, φ(xt,i)i) + λιRσ(W).
(6)
1
Below, we explain how to implement this idea in practice for popular meta-learning algorithms.
Ensuring assumption 1. We propose to compute singular values of W during the meta-training
stage and follow its evolution during the learning episodes. In practice, this can be done by per-
forming the Singular Value Decomposition (SVD) on W ∈ RT ×k with a computational cost of
O(T k2 ) floating-point operations (flop). However, as T is typically quite large, we propose a more
computationally efficient solution that is to take into account only the last batch of N predictors
(with N T) grouped in the matrix WN ∈ RN×k that capture the latest dynamics in the learning
process. We further note that σi (WN WN> ) = σi2 (WN), ∀i ∈ [[N]] implying that we can calculate
the SVD of WN W>N (orW>NWN fork ≤ N) and retrieve the singular values from it afterwards.
We now want to verify whether the optimal linear predictors wt cover all directions in the embedding
space by tracking the evolution of the ratio of singular values Rσ(WN) during the training process.
For the sake of conciseness, we use Rσ instead ofRσ(WN) thereafter. According to the theory, we
expect Rσ to decrease during training thus improving the generalization of the learned predictors
and preparing them for the target task. When we want to enforce such a behavior in practice, we
propose to use Rσ as a regularization term in the training loss of popular meta-learning algorithms.
Alternatively, as the smallest singular value σN(WN) can be close to 0 and lead to numerical errors,
we propose to replace the ratio of the vector of singular values by its entropy as follows:
N
Hσ(WN) =—ɪ2 softmax(σ(Wn))i ∙ logsoftmax(σ(WN))i,
i=1
where Softmax(∙)i is the ith output of the softmax function. As with Rσ, we write H instead of
Hσ (WN) from now on. Since uniform distribution has the highest entropy, regularizing with Rσ
or —Hσ leads to a better coverage of Rk by ensuring a nearly identical importance regardless of
the direction. We refer the reader to the Supplementary materials for the derivations ensuring the
existence of the subgradients for these terms.
Ensuring assumption 2. In addition to the full coverage of the embedding space by the linear
predictors, the meta-learning theory assumes that the norm of the linear predictors does not increase
with the number of tasks seen during meta-training, i.e., kwk2 = O(1) or, equivalently, kWk2F =
O(T ). If this assumption does not hold in practice, we propose to regularize the norm of linear
predictors during training or directly normalize the obtained linear predictors W = ^^^^.
5
Under review as a conference paper at ICLR 2021
The final meta-training loss with the theory-inspired regularization terms is given as:
1 T n1
φ∈φ WinRT ×k 2τm∑ Σ'(yt,i, hwt, φ(xt,i)i) + λ1Rσ (WN ) + λ kWN kF，
,	1 t=1 i=1
(7)
and depending on the considered algorithm, We can replace Rσ by -H and/or replace Wt by Wt
instead of regularizing with kWN k2F. In what follows, we consider λ1 = λ2 = 1 and we refer the
reader to the Supplementary materials for more details and experiments With other values.
To the best of our knoWledge, such regularization terms based on insights from the advances in meta-
learning theory have never been used in the literature before. We also further use the basic quantities
involved in the proposed regularization terms as indicators of Whether a given meta-learning algo-
rithm naturally satisfies the assumptions ensuring an efficient meta-learning in practice or not.
3.3 Related work
BeloW, We discuss several related studies aiming at improving the general understanding of meta-
learning, and mention other regularization terms specifically designed for meta-learning.
Understanding meta-learning While a complete theory for meta-learning is still lacking, several
recent Works aimed to shed light on phenomena commonly observed in meta-learning by evaluat-
ing different intuitive heuristics. For instance, Raghu et al. (2020) investigated Whether the popular
gradient-based Maml algorithm relies on rapid learning With significant changes in the representa-
tions When deployed on target task, or due to feature reuse Where the learned representation remains
almost intact. They establish that the latter factor is dominant and propose a neW variation of Maml
that freezes all but task-specific layers of the neural netWork When learning neW tasks. In another
study (Goldblum et al., 2020) the authors explain the success of meta-learning approaches by their
capability to either cluster classes more tightly in feature space (task-specific adaptation approach),
or to search for meta-parameters that lie close in Weight space to many task-specific minima (full
fine-tuning approach). Finally, the effect of the number of shots on the classification accuracy Was
studied theoretically and illustrated empirically in Cao et al. (2020) for the popular metric-based
ProtoNet algorithm. Our paper is complementary to all other Works mentioned above as it inves-
tigates a neW aspect of meta-learning that has never been studied before, While folloWing a sound
theory. Also, We provide a more complete experimental evaluation as the three different approaches
of meta-learning (based on gradient, metric or transfer learning), separately presented in Raghu et al.
(2020), Cao et al. (2020) and Goldblum et al. (2020), are noW compared together.
Other regularization strategies Regularization is a common tool to reduce model complexity dur-
ing learning for better generalization, and the variations of its tWo most famous instances given by
Weight decay (Krogh & Hertz, 1992) and dropout (Srivastava et al., 2014) are commonly used as a
basis in meta-learning literature as Well. In general, regularization in meta-learning is applied to the
Weights of the Whole neural netWork (Balaji et al., 2018; Yin et al., 2020), the predictions (Jamal &
Qi, 2019; Goldblum et al., 2020) or is introduced via a prior hypothesis biased regularized empirical
risk minimization (Pentina & Lampert, 2014; Kuzborskij & Orabona, 2017; Denevi et al., 2018a;b;
2019). Our proposal is different from all the approaches mentioned above for the folloWing reasons.
First, We do not regularize the Whole Weight matrix learned by the neural netWork but the linear
predictors of its last layer contrary to What Was done in the methods of the first group, and, more
specifically, the famous Weight decay approach (Krogh & Hertz, 1992). The purpose of the regu-
larization in our case is also completely different: Weight decay is used to improve generalization
through sparsity in order to avoid overfitting, While our goal is to keep the classification margin
unchanged during the training to avoid over-/under-specialization to some source tasks. Similarly,
spectral normalization proposed by Miyato et al. (2018) to satisfy the Lipschitz constraint in GANs
through dividing W values by σmax(W) does not affect the ratio betWeen σmax(W) and σmin(W)
and serves a completely different purpose. Second, We regularize the singular values (entropy or ra-
tio) of the matrix of linear predictors instead of the predictions, as done by the methods of the second
group (e.g., using the theoretic-information quantities in Jamal & Qi (2019) and Yin et al. (2020)).
Finally, the Works of the last group are related to the online setting With convex loss functions only,
and, similarly to the algorithms from the second group, do not specifically target the spectral proper-
ties of the learned predictors. Last, but not least, our proposal is built upon the most recent advances
in the meta-learning field leading to faster learning rates contrary to previous Works.
6
Under review as a conference paper at ICLR 2021
4	Practical Results
In this section, we use extensive experimental evaluations to answer the following two questions:
Q1) Do popular meta-learning methods naturally satisfy the learning bounds assumptions?
Q2) Does ensuring these assumptions help to (meta-)learn more efficiently?
For Q1, we run the original implementations of popular meta-learning methods to see what is their
natural behavior. For Q2, we study the impact of forcing them to closely follow the theoretical setup.
4.1	Experimental setup
Datasets & Baselines We consider few-shot image classification problem on three benchmark
datasets, namely: 1) Omniglot (Lake et al., 2015) consisting of 1,623 classes with 20 images/class of
size 28 × 28; 2) miniImageNet (Ravi & Larochelle, 2017) consisting of 100 classes with 600 images
of size 84 × 84 per class and 3) tieredImageNet (Ren et al., 2018) consisting of 779,165 images
divided into 608 classes. For each dataset, we follow the commonly adopted experimental protocol
used in Finn et al. (2017) and Chen et al. (2019) and use a four-layer convolution backbone (Conv-
4) with 64 filters as done by Chen et al. (2019). On Omniglot, we perform 20-way classification
with 1 shot and 5 shots, while on miniImageNet and tieredImageNet we perform 5-way classifica-
tion with 1 shot and 5 shots. Finally, we evaluate four FSL methods: two popular meta-learning
strategies, namely, Maml (Finn et al., 2017), a gradient-based method, and Prototypical Networks
(ProtoNet) (Snell et al., 2017), a metric-based approach; two popular transfer learning baselines,
termed as Baseline and Baseline++ (Ravi & Larochelle, 2017; Gidaris & Komodakis, 2018;
Chen et al., 2019). Even though these baselines are trained with the standard supervised learning
framework, such a training can also be seen as learning a single task in the LTL framework.
Implementation details Enforcing Assumptions 1 and 2 for MAML is straightforward as it closely
follows the LTL framework of episodic training. For each task, the model learns a batch of linear
predictors and we can directly take them as WN to compute its SVD. Since the linear predictors
are the weights of our model and change slowly, regularizing the norm kWN kF and the ratio of
singular values Rσ does not cause instabilities during training. Meanwhile, metric-based methods
do not use linear predictors but compute a similarity between features. In the case of ProtoNet,
the similarity is computed with respect to class prototypes (i.e. the mean features of the images of
each class). Since they act as linear predictors, a first idea would be to regularize the norm and ratio
of singular values of the prototypes. Unfortunately, this latter strategy hinders the convergence of
the network and leads to numerical instabilities. Most likely because prototypes are computed from
image features which suffer from rapid changes across batches. Consequently, we regularize the
entropy of singular values Hσ instead of the ratio Rσ to avoid instabilities during training to ensure
Assumption 1 and We normalize the prototypes to ensure Assumption 2 by replacing Wt with Wt in
Eq. 7. For transfer learning methods Baseline and Baseline++, the last layer of the network is
discarded and linear predictors are learned during meta-testing. Thus, we only regularize the norm
kWNkF of predictors learned during the finetuning phase of meta-testing. Similarly to Maml, we
compute Rσ with the last layer of the network during training and fine-tuning phase.
Remark 2 We choose well-established meta-learning algorithms for our comparison, but the pro-
posed regularization can be integrated similarly into their recent variations (Park & Oliva, 2019;
Lee et al., 2019) (see Supplementary materials for results obtained with the method of Park & Oliva
(2019)). Finally, using models that do not rely on linear predictors is also possible but might be more
difficult as it would require upstream work to understand which part of the model acts as predictors
(as done for PROTONET in this paper) and how to compute and track the desired quantities.
4.2	Insights
Q1 - Verifying the assumptions According to theory, kWn∣∣f and Rσ should remain constant
or converge toward a constant value when monitoring the last N tasks. From Fig. 2(a), we can see
that for MAML (Fig. 2(a) top), both ∣WN ∣F and Rσ increase with the number of tasks seen during
training, whereas ProtoNet (Fig. 2(a) bottom) naturally learns the prototypes with a good cover-
age of the embedding space, and minimizes their norm. This behavior is rather peculiar as neither
7
Under review as a conference paper at ICLR 2021
(a)
MAML
MAML+reg
——MAML ——MAML+reg
----Proto ------ Proto+norm
--- MAML ------ MAML+reg
(b)
Proto
Proto+norm
0.9
----Proto ------ Proto+norm
0«
›0∙7
,06
tieredlmageNet
Omniglot
----Baseline
---- Baseline + reg
Training epochs
----Baseline++ ------------ tieredlmageNet
----Base∣i∏e+++reg --------Omniglot
----Baseline++
----Base∣i∏e+++reg
tieredlmageNet
Omniglot
Training epochs


Figure 2: (a) Evolution of kWN kF (left), Rσ (middle) and validation accuracy (right) when train-
ing of MAML (top) and PROTONET (bottom) on miniImageNet (1 shot for MAML, 5 shots for
PROTONET). (b) Evolution of Rσ (left) and validation accuracy (right) when training BASELINE
(top) and BASELINE++ (bottom) on Omniglot (dashed lines) and tieredImageNet (solid lines). All
training curves were averaged over 4 different random seeds. For MAML, kWN kF and Rσ increase
during training and violate Assumptions 1-2. ProtoNet prototypes naturally cover the embedding
space, while minimizing their norms. Rσ converges during training on both datasets for BASE-
line++ (similarly to ProtoNet) whereas it diverges for Baseline on tieredImageNet. With our
regularization, kWN kF and Rσ are constant during training in accordance with theory.
of the two methods specifically controls the theoretical quantities of interest, and still, ProtoNet
manages to do it implicitly. As for the transfer learning baselines (Fig. 2(b) top and bottom), we
expect them to learn features that cover the embedding space with Rσ rapidly converging towards a
constant value. As can be seen in Fig. 2(b), similarly to ProtoNet, Baseline++ naturally learns
linear predictors that cover the embedding space. As for Baseline, it learns a good coverage for
Omniglot dataset, but fails to do so for the more complicated tieredImageNet dataset. The observed
behavior of these different methods leads to a conclusion that some meta-learning algorithms are
inherently more explorative of the embedding space.
Q2 - Ensuring the assumptions Armed with our regularization terms, We now aim to force the
considered algorithms to verify the assumptions when it is not naturally done. In particular, for
MAML we regularize both kWN kF and Rσ in order to keep them constant throughout the training.
Similarly, we regularize Rσ during the training of BASELINE and BASELINE++, and both kWN kF
and Rσ during the finetuning phase of meta-testing. For PROTONET, we enforce a normalization of
8
Under review as a conference paper at ICLR 2021
Dataset	Episodes	Maml	ProtoNet	Baseline	Baseline++
Omniglot	20-way 1-shot	+3.95*	+0.33*	-13.2*	-7.29*
	20-way 5-shot	+ 1.17*	+0.01	+0.66*	-2.24*
miniImageNet	5-way 1-shot	+ 1.23*	+0.76*	+1.52*	+0.39
	5-way 5-shot	+ 1.96*	+2.03*	+1.66*	-0.13
tieredImageNet	5-way 1-shot	+ 1.42*	+2.10*	+5.43*	+0.28
	5-way 5-shot	+2.66*	+0.23	+1.92*	-0.72
Table 1: Accuracy gap (in p.p.) of the considered algorithms when using the regularization (or
normalization in the case of ProtoNet) enforcing the theoretical assumptions. All accuracy results
are averaged over 2400 test episodes and 4 different seeds. Statistically significant results (out of
confidence intervals) are reported with *. Exact performances are on par with those found in the
literature and are reported in the Supplementary materials.
the prototypes. According to our results for Q1, regularizing the singular values of the prototypes
through the entropy Hσ is not necessary.3 Based on the obtained results, we can make the following
conclusions. First, from Fig. 2(a) (left, middle) and Fig. 2(b) (left), we note that for all methods con-
sidered, our proposed methodology used to enforce the theoretical assumptions works as expected,
and leads to a desired behavior during the learning process. This means that the differences in terms
of results presented in Table 1 are explained fully by this particular addition to the optimized ob-
jective function. Second, from the shape of the accuracy curves provided in Fig. 2(a) (right) and
the accuracy gaps when enforcing the assumptions given in Table 1, we can see that respecting the
assumptions leads to several significant improvements related to different aspects of learning. On
the one hand, we observe that the final validation accuracy improves significantly in all benchmarks
for meta-learning methods and in most of experiments for Baseline (except for Omniglot, where
Baseline already learns to regularize its linear predictors). In accordance with the theory, we at-
tribute the improvements to the fact that we fully utilize the training data which leads to a tighter
bound on the excess target risk and, consequently, to a better generalization performance. On the
other hand, we also note that our regularization reduces the sample complexity of learning the tar-
get task, as indicated by the faster increase of the validation accuracy from the very beginning of
the meta-training. Roughly speaking, less meta-training data is necessary to achieve a performance
comparable to that obtained without the proposed regularization using more tasks. Finally, we note
that Baseline++ and ProtoNet methods naturally satisfy some assumptions: both learn diverse
linear predictors by design, while Baseline++ also normalizes the weights of its linear predictors.
Thus, these methods do not benefit from additional regularization as explained before.
5	Conclusion
In this paper, we studied the validity of the theoretical assumptions made in recent papers applied to
popular meta-learning algorithms and proposed practical ways of enforcing them.
On the one hand, we showed that depending on the problem and algorithm, some models can natu-
rally fulfill the theoretical conditions during training. Some algorithms offer a better covering of the
embedding space than others. On the other hand, when the conditions are not verified, learning with
our proposed regularization terms allows to learn faster and improve the generalization capabilities
of meta-learning methods. The theoretical framework studied in this paper explains the observed
performance gain. Notice that no specific hyperparameter tuning was performed as we rather aim
at showing the effect of ensuring learning bounds assumptions than comparing performance of the
methods. Absolute accuracy results are detailed in the Supplementary materials.
While this paper proposes an initial approach to bridging the gap between theory and practice in
meta-learning, some questions remain open on the inner workings of these algorithms. In particular,
being able to take better advantage of the particularities of the training tasks during meta-training
could help improve the effectiveness of these approaches. Self-supervised meta-learning and multi-
ple target tasks prediction are also important future perspectives for the application of meta-learning.
3The effect of entropic regularization on ProtoNet is detailed in the Supplementary materials.
9
Under review as a conference paper at ICLR 2021
References
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended pac-bayes theory. In
International Conference on Machine Learning, 2018.
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. MetaReg: Towards Domain Gener-
alization using Meta-Regularization. In Advances in Neural Information Processing Systems, pp.
998-1008,2018.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-
based meta-learning. In International Conference on Machine Learning, 2019.
Jonathan Baxter. A Model of Inductive Bias Learning. Journal of Artificial Intelligence Research,
12:149-198, 2000.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine Learning, 79(1-2):151-175,
2010.
Tianshi Cao, Marc T. Law, and Sanja Fidler. A theoretical analysis of the number of shots in few-shot
learning. In ICLR, 2020.
Wei-Yu Chen, Yu-Chiang Frank Wang, Yen-Cheng Liu, Zsolt Kira, and Jia-Bin Huang. A closer
look at few-shot classification. In ICLR, 2019.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-
to-learn with statistical guarantees. In Conference on Uncertainty in Artificial Intelligence, pp.
457-466, 2018a.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around
a common mean. In Advances in Neural Information Processing Systems, pp. 10169-10179,
2018b.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. In International Conference on Machine
Learning, 2019.
Simon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-Shot Learning via Learning
the Representation, Provably. In arXiv:2002.09434, 2020.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-Agnostic Meta-Learning for Fast Adapta-
tion of Deep Networks. In International Conference on Machine Learning, 2017.
Chelsea Finn, Aravind Rajeswaran, Sham M. Kakade, and Sergey Levine. Online meta-learning. In
International Conference on Machine Learning, 2019.
Yaroslav Ganin and Victor Lempitsky. Unsupervised Domain Adaptation by Backpropagation. In
International Conference on Machine Learning, pp. 1180-1189, 2015.
Spyros Gidaris and Nikos Komodakis. Dynamic Few-Shot Visual Learning Without Forgetting. In
CVPR, pp. 4367-4375, 2018.
Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and Tom Goldstein.
Unraveling Meta-Learning: Understanding Feature Representations for Few-Shot Tasks. In In-
ternational Conference on Machine Learning, 2020.
Muhammad Abdullah Jamal and Guo-Jun Qi. Task Agnostic Meta-Learning for Few-Shot Learning.
In CVPR, 2019.
Mikhail Khodak, Maria-Florina Balcan, and Ameet S. Talwalkar. Adaptive gradient-based meta-
learning methods. In Advances in Neural Information Processing Systems, 2019.
Anders Krogh and John A. Hertz. A simple weight decay can improve generalization. In Advances
in Neural Information Processing Systems, 1992.
10
Under review as a conference paper at ICLR 2021
Ilja Kuzborskij and Francesco Orabona. Fast rates by transferring from auxiliary hypotheses. Ma-
chine Learning, 106(2):171-195, 2017.
Brenden M. Lake, Ruslan Salakhutdinov, and Joshua B. Tenenbaum. Human-level concept learning
through probabilistic program induction. Science, 350(6266):1332-1338, 2015.
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with
differentiable convex optimization. In CVPR, pp. 10657-10665, 2019.
Adrian S. Lewis and Hristo S. Sendov. Nonsmooth Analysis of Singular Values. Part I: The-
ory. Set-Valued Analysis, 13(3):213-241, September 2005. ISSN 1572-932X. doi: 10.1007/
s11228-004-7197-7. URLhttps://doi.org/10.1007/s11228-004-7197-7.
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes. The benefit of multitask
representation learning. Journal of Machine Learning Research, 17:81:1-81:32, 2016.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In ICLR, 2018.
Eunbyung Park and Junier B. Oliva. Meta-curvature. In Hanna M. Wallach, Hugo Larochelle,
Alina Beygelzimer, Florence d,Alche-Buc, Emily B. Fox, and Roman Garnett (eds.), NeurIPS,
pp. 3309-3319, 2019.
Anastasia Pentina and Christoph H. Lampert. A pac-bayesian bound for lifelong learning. In Inter-
national Conference on Machine Learning, 2014.
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?
Towards understanding the effectiveness of MAML. In ICLR, 2020.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum,
Hugo Larochelle, and Richard S. Zemel. Meta-Learning for Semi-Supervised Few-Shot Classifi-
cation. In ICLR, 2018.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet large scale visual recognition challenge. International Journal of Computer Vision, 115
(3):211-252, 2015.
Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical Networks for Few-shot Learning. In
Advances in Neural Information Processing Systems, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15(56):1929-1958, 2014.
Nilesh Tripuraneni, Chi Jin, and Michael I. Jordan. Provable Meta-Learning of Linear Representa-
tions. In arXiv:2002.11684, 2020.
Vladimir Vapnik. Principles of risk minimization for learning theory. In Advances in Neural Infor-
mation Processing Systems, 1992.
Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, and Chelsea Finn. Meta-learning
without memorization. In ICLR, 2020.
11