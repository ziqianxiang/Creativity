Under review as a conference paper at ICLR 2021
Guarantees for Tuning the Step Size using a
Learning-to-Learn Approach
Anonymous authors
Paper under double-blind review
Ab stract
Learning-to-learn—using optimization algorithms to learn a new optimizer—has
successfully trained efficient optimizers in practice. This approach relies on meta-
gradient descent on a meta-objective based on the trajectory that the optimizer
generates. However, there were few theoretical guarantees on how to avoid meta-
gradient explosion/vanishing problems, or how to train an optimizer with good
generalization performance. In this paper we study the learning-to-learn ap-
proach on a simple problem of tuning the step size for quadratic loss. Our results
show that although there is a way to design the meta-objective so that the meta-
gradient remain polynomially bounded, computing the meta-gradient directly us-
ing backpropagation leads to numerical issues that look similar to gradient explo-
sion/vanishing problems. We also characterize when it is necessary to compute
the meta-objective on a separate validation set instead of the original training set.
Finally, we verify our results empirically and show that a similar phenomenon ap-
pears even for more complicated learned optimizers parametrized by neural net-
works.
1 Introduction
Choosing the right optimization algorithm and related hyper-parameters is important for training a
deep neural network. Recently, a series of works (e.g., Andrychowicz et al. (2016); Wichrowska
et al. (2017)) proposed to use learning algorithms to find a better optimizer. These papers use
a learning-to-learn approach: they design a class of possible optimizers (often parametrized by
a neural network), and then optimize the parameters of the optimizer (later referred to as meta-
parameters) to achieve better performance. We refer to the optimization of the optimizer as the
meta optimization problem, and the application of the learned optimizer as the inner optimization
problem. The learning-to-learn approach solves the meta optimization problem by defining a meta-
objective function based on the trajectory that the inner-optimizer generates, and then using back-
propagation to compute the meta-gradient (Franceschi et al., 2017).
Although the learning-to-learn approach has shown empirical success, there are very few theoreti-
cal guarantees for learned optimizers. In particular, since the optimization for meta-parameters is
usually a nonconvex problem, does it have bad local optimal solutions? Current ways of optimizing
meta-parameters rely on unrolling the trajectory of the inner-optimizer, which is very expensive and
often lead to exploding/vanishing gradient problems. Is there a way to alleviate these problems?
Can we have a provable way of designing meta-objective to make sure that the inner optimizers can
achieve good generalization performance?
In this paper we answer some of these problems in a simple setting, where we use the learning-
to-learn approach to tune the step size of the standard gradient descent/stochastic gradient descent
algorithm. We will see that even in this simple setting, many of the challenges still remain and we
can get better learned optimizers by choosing the right meta-objective function. Though our results
are proved only in the simple setting, we empirically verify the results using complicated learned
optimizers with neural network parametrizations.
1
Under review as a conference paper at ICLR 2021
1.1	Challenges of learning-to-learn approach and our results
Metz et al. (2019) highlighted several challenges in the meta-optimization for learning-to-learn ap-
proach. First, they observed that the optimal parameters for the learned optimizer (or even just the
step size for gradient descent) can depend on the number of training steps t of the inner-optimization
problem, which is also observed by Wu et al. (2018). Ge et al. (2019) theoretically proved this in a
least-squares setting. Because of this, one needs to ensure that the inner training has enough number
of steps (similar to the number of steps that it would take when we apply the learned optimizer).
However, when the number of steps is large, the meta-gradient can often explode or vanish, which
makes it difficult to solve the meta-optimization problem.
Our first result shows that this is still true in the case of tuning step size for gradient descent on a
simple quadratic objective. In this setting, we show that there is a unique local and global minimizer
for the step size, and we also give a simple way to get rid of the gradient explosion/vanishing
problem.
Theorem 1 (Informal). For tuning the step size of gradient descent on a quadratic objective, if the
meta-objective is the loss of the last iteration, then the meta-gradient can explode/vanish. If the
meta-objeCtive is the log of the loss of the last iteration, then the meta-gradient is polynomially
bounded. Further doing meta-gradient descent with a meta step size of 1∕√k (where k is the
number of meta-gradient steps) provably converges to the optimal step size for the inner-optimizer.
Surprisingly, even though taking the log of the objective solves the gradient explosion/vanishing
problem, one cannot simply implement such an algorithm using auto-differentiation tools such as
those used in TensorFlow (Abadi et al., 2016). The reason is that even though the meta-gradient is
polynomially bounded, if we compute the meta-gradient using the standard back-propagation algo-
rithm, the meta-gradient will be the ratio of two exponentially large/small numbers, which causes
numerical issues. Detailed discussion for the first result appears in Section 3 (Theorem 3 and Theo-
rem 4).
The generalization performance of the learned optimizer is another challenge. If one just tries to
optimize the performance of the learned optimizer on the training set (we refer to this as the train-
by-train approach), then the learned optimizer might overfit. Metz et al. (2019) proposed to use a
train-by-validation approach instead, where the meta-objective is defined to be the performance of
the learned optimizer on a separate validation set.
Our second result considers a simple least squares setting where y =(w* ,x)+ ξ and ξ 〜N(0, σ2).
We show that when the number of samples is small and the noise is large, it is important to use
train-by-validation; while when the number of samples is much larger train-by-train can also learn a
good optimizer.
Theorem 2 (Informal). For a simple least squares problem in d dimensions, if the number of samples
n is a constant fraction ofd (e.g., d∕2), and the samples have large noise, then the train-by-train ap-
proach performs much worse than train-by-validation. On the other hand, when number of samples
n is large, train-by-train can get close to error dσ2∕n, which is optimal.
We discuss the details in Section 4 (Theorem 5 and Theorem 6). In Section 5 we show that
such observations also hold empirically for more complicated learned optimizers—an optimizer
parametrized by neural network.
1.2	Related work
Learning-to-learn for supervised learning Hochreiter et al. (2001) introduced the application of
gradient descent method to meta-learning. The idea of using a neural network to parametrize an
optimizer started in Andrychowicz et al. (2016), which used an LSTM to directly learn the update
rule. Before that, the idea of using optimization to tune parameters for optimzers also appeared
in Maclaurin et al. (2015). Later, Li & Malik (2016); Bello et al. (2017) applied techniques from
reinforcement learning to learn an optimizer. Wichrowska et al. (2017) used a hierarchical RNN as
the optimizer. Metz et al. (2019) adopted a small MLP as the optimizer and used dynamic weighting
of two gradient estimators to stabilize and speedup the meta-training process.
2
Under review as a conference paper at ICLR 2021
Learning-to-learn in other settings Ravi & Larochelle (2016) used LSTM as a meta-learner to
learn the update rule for training neural networks in the few-shot learning setting, Wang et al. (2016)
learned an RL algorithm by another meta-learning RL algorithm, and Duan et al. (2016) learned a
general-purpose RNN that can adapt to different RL tasks.
Gradient-based meta-learning Finn et al. (2017) proposed Model-Agnostic Meta-Learning
(MAML) where they parameterize the update rule for network parameters and learn a shared ini-
tialization for the optimizer using the tasks sampled from some distribution. Subsequent works
generalized or improved MAML, e.g., Rusu et al. (2018) learned a low-dimensional latent repre-
sentation for gradient-based meta-learning, and Li et al. (2017) enabled the concurrent learning of
learning rate and update direction. Chen et al. (2020) studied a model with an optimization solver
stacked on another neural component. They computed Rademacher complexity of the model, but
didn’t give any optimization guarantee or study train-by-train versus train-by-validation.
Learning assisted algorithms design Similar ideas can also be extended to develop a meta-
algorithm selecting an algorithm from a family of parametrized algorithms. Gupta & Roughgarden
(2017) first modeled the algorithm-selection process as a statistical learning problem and bounded
the number of tasks it takes to tune a step size for gradient descent. However, they didn’t consider
the meta-optimization problem. Based on Gupta & Roughgarden (2017), people have developed
and analyzed the meta-algorithms in many problems (Balcan et al., 2016; 2018a;c;b; Denevi et al.,
2018; Alabi et al., 2019; Denevi et al., 2019)
Tuning step size/step size schedule for SGD Shamir & Zhang (2013) showed that SGD with
polynomial step size scheduling can almost match the minimax rate in convex non-smooth settings,
which was later tightened by Harvey et al. (2018) for standard step size scheduling. Assuming that
the horizon T is known to the algorithm, the information-theoretically optimal bound in convex non-
smooth setting was later achieved by Jain et al. (2019) which used another step size schedule, and
Ge et al. (2019) showed that exponentially decaying step size scheduling can achieve near optimal
rate for least squares regression. There are also a line of works that investigate methods which adapt
a vector of step sizes (SUtton, 1992; Schraudolph, 1999; Kearney et al., 2018; Gunther et al., 2019;
Jacobsen et al., 2019).
2	Preliminaries
In this section, we first introduce some notations, then formulate the learning-to-learn framework.
2.1	Notations
For any integer n, we use [n] to denote {1, 2, ∙∙∙ ,n}. We use ∣∣∙k to denote the '2 norm for a vector
and the spectral norm for a matrix. We use〈•, •)to denote the inner product of two vectors. For
a symmetric matrix A ∈ Rd×d, We denote its eigenvalues as λι(A) ≥ ∙∙∙ ≥ λd(A). We denote
the d-dimensional identity matrix as Id . We also denote the identity matrix simply as I when the
dimension is clear from the context. We use O(∙), Ω(∙), Θ(∙) to hide constant factor dependencies.
We use poly(∙) to represent a polynomial on the relevant parameters with constant degree. We say
an event happens with high probability if it happens with probability 1 - c for small constant c.
2.2	Learning-to-learn framework
We consider the learning-to-learn approach applied to training a distribution of learning tasks. Each
task is specified by a tuple (D, Strain, Svalid, `). Here D is a distribution of samples in X × Y , where
X is the domain for the sample and Y is the domain for the label/value. The sets Strain and Svalid
are samples generated independently from D, which serve as the training and validation set (the
validation set is optional). The learning task looks to find a parameter w ∈ W that minimizes the
loss function `(w, x, y) : W × X × Y → R, which gives the loss of the parameter w for sample
(x, y). The training loss for this task is f (W) := ∣s1q E(X y)∈stιιι.n '(w, x, y), while the population
loss is f (w) := E(χ,y)〜D['(w,x,y)].
3
Under review as a conference paper at ICLR 2021
The goal of inner-optimization is to minimize the population loss f (w). For the learned op-
timizer, We consider it as an update rule u(∙) on weight w. The update rule is a parame-
terized function that maps the weight at step τ and its history to the step τ + 1 : wτ+1 =
U(Wτ, Vf (wτ), Vf (wτ-ι), .…;θ). In most parts of this paper, we consider the update rule U
as gradient descent mapping with step size as the trainable parameter (here θ = η which is
the step size for gradient descent). That is, Uη (w) = w - ηVf(w) for gradient descent and
Un (W) = w 一 ηVw'(w, x, y) for stochastic gradient descent where (x, y) is a sample randomly
chosen from the training set Strain.
In the outer (meta) level, we consider a distribution T of tasks. For each task P 〜 T, we can define
a meta-loss function ∆(θ, P). The meta-loss function measures the performance of the optimizer
on this learning task. The meta objective, for example, can be chosen as the target training loss f at
the last iteration (train-by-train), or the loss on the validation set (train-by-validation).
The training loss for the meta-level is the average of the meta-loss across m different specific tasks
Pi, P2,..., Pm, that is, F(θ)=+ Pm=I ∆(θ, Pk). The population loss for the meta-level is the
expectation over all the possible specific tasks F(θ) = EP〜T[∆(θ, P)].
In order to train an optimizer by gradient descent, we need to compute the gradient of meta-objective
F in terms of meta parameters θ. The meta parameter is updated once after applying the optimizer
on the inner objective t times to generate the trajectory w0, w1, ..., wt. The meta-gradient is then
computed by unrolling the optimization process and back-propagating through the t applications of
the optimizer. As we will see later, this unroll procedure is costly and can introduce meta-gradient
explosion/vanishing problems.
3	Alleviating gradient explosion/vanishing problems
First we consider the meta-gradient explosion/vanishing problem. More precisely, we say the meta-
gradient explodes/vanishes if it is exponentially large/small with respect to the number of steps t of
the inner-optimizer.
In this section, we consider a very simple instance of the learning-to-learn approach, where the
distribution T only contains a single task P, and the task also just defines a single loss function f1.
Therefore, in this section F(η) = F(η) = ∆(η, P). We will simplify notation and only use F(η).
The inner task P is a simple quadratic problem, where the starting point is fixed at w0, and the loss
function is f (w) = ɪw>Hw for some fixed positive definite matrix H. Without loss of general-
ity, assume w0 has unit `2 norm. Suppose the eigenvalue decomposition of H is Pid=1 λi Ui Ui> .
Throughout this section we assume L = λ1 (H) and α = λd(H) are the largest and smallest eigen-
values of H with L > α. For each i ∈ [d], let ci be hw0, Uii and let cmin = min(|c1|, |cd|). We
assume Cmin > 0 for simplicity. Note that if w° is uniformly sampled from the unit sphere, with
high probability Cmin is at least Ω(1∕√d); if H is XX> with X ∈ Rd×2d as a random Gaussian
matrix, with constant probability, both a and L 一 α are at least Ω(d).
Let {wτ,η} be the GD sequence running on f(w) starting from w0 with step size η. We consider
several ways of defining meta-objective, including using the loss of the last point directly, or using
the log of this value. We first show that although choosing F(η) = f (wt,n) does not have any bad
local optimal solution, it has the gradient explosion/vanishing problem. We use F0(η) to denote the
derivative of F in η.
Theorem 3.	Let the meta objective be F(η) = f (wt,n) = 1 w^>,ηHwt,n. We know F(η) is a strictly
convex function in η with an unique minimizer. However, for any step size η < 2/L, ∣F0(η)∣ ≤
t Pd=ι c2λ2∣1 一 ηλi∣2t-i; for any step size η > 2/L, ∣F0(η)∣ ≥ c2L2t(ηL 一 1)2t-i 一 L2t.
Note that in Theorem 3, when η < 2/L, |F0(η)∣ is exponentially small because |1 一 ηλ∕ < 1 for
all i ∈ [d]; when η > 2/L, |F0(η)∣ is exponentially large because ηL 一 1 > 1. Intuitively, gradient
1In the notation of Section 2, one can think that D contains a single point (0, 0) and the loss function
f(w) = `(w, 0, 0).
4
Under review as a conference paper at ICLR 2021
explosion/vanishing happens because the meta-loss function becomes too small or too large. A
natural idea to fix the problem is to take the log of the meta-loss function to reduce its range. We
show that this indeed works. More precisely, if We choose F(η) = 1 log f (wt,n), then We have
Theorem 4.	Let the meta objective be F(η) = 1 log f (wt,n). We know F(η) hasa Umquemmimizer
η* and F0(η) = O (c2-OLL-O)) for all η ≥ °∙ Let {ηk} be the GD sequence running on F with
meta step size μk = 1∕√k. Suppose the starting step size no ≤ M. Given any 1/L > e > 0, there
exists k0 = MM- poly (^1- ,L, 1, L-a) Such that forall k ≥ k0, |nk — n*| ≤ e.
For convenience, in the above algorithmic result, we reset η to zero once η goes negative. Note
that although we show the gradient is bounded and there is a unique optimizer, the problem of
optimizing η is still not convex because the meta-gradient is not monotone. We use ideas from
quasi-convex optimization to show that meta-gradient descent can find the unique optimal step size
for this problem.
Surprisingly, even though we showed that the meta-gradient is bounded, it cannot be effectively
computed by doing back-propagation due to numerical issues. More precisely:
Corollary 1. Ifwe choose the meta-objective as F(η) = 1 log f (wt,η), when computing the meta-
gradient using back-propagation, there are intermediate results that are exponentially large/small
in number of inner-steps t.
Indeed, in Section 5 we empirically verify that standard auto-differentiation tools can still fail in this
setting. This suggests that one should be more careful about using standard back-propagation in the
learning-to-learn approach. The proofs of the results in this section are deferred into Appendix A.
4	Train-by-train vs. train-by-validation
Next we consider the generalization ability of simple optimizers. In this section we consider
a simple family of least squares problems. Let T be a distribution of tasks where every task
(D(w*), Strain, SValid,') is determined by a parameter w* ∈ Rd which is chosen uniformly at ran-
dom on the unit sphere. For each individual task, (x, y)〜D(w*) is generated by first choosing
X 〜N(0, Id) and then computing y = (w*, x)+ ξ where ξ 〜N(0, σ2) with σ ≥ 1. The loss
function '(w, x, y) is just the squared loss '(w, x,y) = 2(y — hw, x))2. That is, the tasks are just
standard least-squares problems with ground-truth equal to w* and noise level σ2 .
For the meta-loss function, we consider two different settings. In the train-by-train setting, the
training set Strain contains n independent samples, and the meta-loss function is chosen to be the
training loss. That is, in each task P , we first choose w* uniformly at random, then generate
(x1, y1), ..., (xn, yn) as the training set Strain. The meta-loss function ∆T bT (n) (η, P) is defined
to be
1n
δTbT(n)(n, P) = 2n y^(yi - hwt,η, xii)? ∙
i=1
Here wt,η is the result of running t iterations of gradient descent starting from point 0 with step size
η. Note we truncate a sequence and declare the meta loss is high once the weight norm exceeds
certain threshold. We can safely do this because we assume the ground truth weight w* has unit
norm, so if the weight norm of our model is too high, it means the inner training has diverged and
the step size is too large. Specifically, ifat the τ-th step, kwτ,η k ≥ 40σ, we freeze the training on this
task and set wτ0 ,η = 40σu for all τ ≤ τ0 ≤ t, for some arbitrary vector u with unit norm. Setting
the weight to a large vector is just one way to declare the loss is high; we choose this particular way
for some proof convenience.
As before, the empirical meta objective in train-by-train setting is the average of the meta-loss across
m different specific tasks P1, P2, ..., Pm, that is,
m
1
FTbT (n)(n) = —〉J δTbT (n)(n,Pk )∙	⑴
m k=1
5
Under review as a conference paper at ICLR 2021
In the train-by-validation setting, the specific tasks are generated by sampling n1 training samples
and n2 validation samples for each task, and the meta-loss function is chosen to be the valida-
tion loss. That is, in each specific task P, We first choose w* uniformly at random, then generate
(x1,y1), ..., (xn1,yn1) as the training set Strain and (x01,y10), ..., (x0n2,yn02) as the validation set Svalid.
The meta-loss function ∆TbV (n1,n2) (η, P) is defined to be
1	n2
δTbV (nι,n2)(η, P) = 2n £(y0 - hwt,η ,xii)2.
2	i=1
Here again wt,η is the result of running t iterations of the gradient descent on the training set starting
from point 0, and We use the same truncation as before. The empirical meta objective is defined as
m
1
FTbV (nι ,n2)(η) = —〉,XbV (ni,n2)(n,Pk ),	⑵
m k=1
Where each Pk is independently sampled according to the described procedure.
We first shoW that When the number of samples is small (in particular n < d) and the noise is a large
enough constant, train-by-train can be much Worse than train-by-validation, even When n1 +n2 =n
(the total number of samples used in train-by-validation is the same as train-by-train)
Theorem 5. Let FT bT (n)(η) and FTbV(n1,n2) (η) be as defined in Equation (1) and Equation (2)
respectively. Assume n,n1,n2 ∈ [d/4, 3d/4]. Assume noise level σ is a large constant c1. Assume
unroll length t ≥ c2, number of training tasks m ≥ c3 log(mt) and dimension d ≥ c4 log(mt) for
certain constants c2, c3, c4. With high probability in the sampling of training tasks, we have
η train = θ(1) and Eilwt,η 嘉l, - w*||2 =回1方2,
for all ηt*rain ∈ argminη≥0FTbT(n)(η);
ηValid = θ(1∕t)andE∣∣wt,ηValid- w*l∣2 = l∣w*k2 -ω(I)
for all ηv*alid ∈ arg minη≥0 FTbV(n1,n2) (η). In both equations the expectation is taken over new
tasks.
Note that in this case, the number of samples n is smaller than d, so the least square problem is under-
determined and the optimal training loss Would go to 0 (there is alWays a Way to simultaneously
satisfy all n equations). This is exactly What train-by-train Would do—it Will choose a large constant
learning rate Which guarantees the optimizer converges exponentially to the empirical risk minimizer
(ERM). HoWever, When the noise is large making the training loss go to 0 Will overfit to the noise
and hurt the generalization performance. Train-by-validation on the other hand Will choose a smaller
learning rate Which alloWs it to leverage the information in the training samples Without overfitting
to noise. Theorem 5 is proved in Appendix B. We also prove similar results for SGD in Appendix D
We emphasize that neural netWorks are often over-parameterized, Which corresponds to the case
When d > n. Indeed Liu & Belkin (2018) shoWed that variants of stochastic gradient descent can
converge to the empirical risk minimizer With exponential rate in this case. Therefore in order to
train neural netWorks, it is better to use train-by-validation. On the other hand, We shoW When the
number of samples is large (n d), train-by-train can also perform Well.
Theorem 6. Let FTbT(n)(η) be as defined in Equation 1. Assume noise level is a constant c1. Given
any 1 > e > 0, assume training set size n ≥ Cd log(嗤), unroll length t ≥ c2 log(*), number
oftraining tasks m ≥ C4n2 log( td) and dimension d ≥ c4 for certain constants c, c2, c3, c4. With
high probability in the sampling of training tasks, we have
El∣wtηtVan - w*l∣2 ≤ (1 + e)^n^,
for all ηt*rain ∈ arg minη≥0 FTbT(n)(η), where the expectation is taken over new tasks.
Therefore if the learning-to-learn approach is applied to a traditional optimization problem that is not
over-parameterized, train-by-train can Work Well. In this case, the empirical risk minimizer (ERM)
already has good generalization performance, and train-by-train optimizes the convergence toWards
the ERM. We defer the proof of Theorem 6 into Appendix C.
6
Under review as a conference paper at ICLR 2021
5 Experiments
Optimizing step size for quadratic objective We first validate the results in Section 3. We fixed
a 20-dimensional quadratic objective as the inner problem and vary the number of inner steps t
and initial value η0 . We compute the meta-gradient directly using a formula which we derive in
Appendix A. In this way, we avoid the computation of exponentially small/large intermediate terms.
We use the algorithm suggested in Theorem 4, except we choose the meta-step size tobe 1/(100√k)
as the constants in the theorem were not optimized.
An example training curve of η for t = 80 and η0 = 0.1 is shown in Figure 1, and we can see that
η converges quickly within 300 steps. Similar convergence also holds for larger t or much larger
initial η0 . In contrast, we also implemented the meta-training with Tensorflow, where the code
was adapted from the previous work of Wichrowska et al. (2017). Experiments show that in many
settings (especially with large t and large η0) the implementation does not converge. In Figure 1,
under the TensorFlow implementation, the step size is stuck at the initial value throughout the meta
training because the meta gradient explodes and gives NaN value.
In Figure 2, we verify the observation from Metz et al. (2019) that the optimal step size depends on
inner training length.
0.4
5 0.2
0
OUrS
Tensorflow
1 3
3 O
。 S
O
r -BLU-IdO
100	200	300
Meta steps
0.029 ---------'--------'-------'--------
0	50	100	150	200
Inner training length t
0
Figure 1:	Training η (t = 80, η0 = 0.1)
Figure 2:	Optimal η* for different t
Train-by-train vs. train-by-validation, synthetic data Here we validate our theoretical results
in Section 4 using the least-squares model defined there. We fix the input dimension d to be 1000.
In the first experiment, we fix the size of the data (n = 500 for train-by-train, n1 = n2 = 250
for train-by-validation). Under different noise levels, We find the optimal η* by a grid search on
its meta-objective for train-by-train and train-by-validation settings respectively. We then use the
optimal η* found in each of these two settings to test on 10 new least-squares problem. The mean
RMSE, as Well as its range over the 10 test cases, are shoWn in Figure 3. We can see that for all
of these cases, the train-by-train model overfits easily, while the train-by-validation model performs
much better and does not overfit. Also, when the noise becomes larger, the difference between these
two settings becomes more significant.
Figure 3: Training and testing RMSE for different σ values (500 samples)
In the next experiment, we fix σ = 1 and change the sample size. For train-by-validation, we always
split the samples evenly into training and validation set. From Figure 4, we can see that the gap
between these two settings is decreasing as we use more data, as expected by Theorem 6.
7
Under review as a conference paper at ICLR 2021
0
2
e 1.5
远
LLI 1
S
W
CC 0.5
0	2500 5000 7500 10000
Sample size
壬壬
bτT
2
Q 1.5
ω
φ
LLl 1
(f)
W
H 0.5
0	2500 5000 7500 10000
Sample size
壬壬
bτt
0
Figure 4:	Training and testing RMSE for different samples sizes (σ = 1)
Train-by-train vs. train-by-validation, MLP optimizer on MNIST Finally we consider a more
complicated multi-layer perceptron (MLP) optimizer on MNIST data set. We use the same MLP
optimizer as in Metz et al. (2019), details of this optimizer is discussed in Appendix F. As the
inner problem, we use a two-layer fully-connected network of 100 and 20 hidden units with ReLU
activations. The inner objective is the classic 10-class cross entropy loss, and we use mini-batches
of 32 samples at inner training. In all the following experiments, we use SGD as a baseline with step
size tuned by grid search against validation loss.
To see whether the comparison between train-by-train and train-by-validation behave similarly to
our theoretical results, we consider different number of samples and different levels of label noise.
For each optimizer, we run 5 independent tests and collect training accuracy and test accuracy for
evaluation. The plots show the mean of the 5 tests. We didn’t show the measure of the spread because
the results of these 5 tests are so close to each other, such that the range or standard deviation marks
will not be readable in the plots.
First, consider optimizing the MNIST dataset with small number of samples. In this case, the train-
by-train setting uses 1,000 samples (denoted as “TbT1000”), and we use another 1,000 samples as
the validation set for the train-by-validation case (denoted as “TbV1000+1000”). To be fair to train-
by-train we also consider TbT2000 where the train-by-train algorithm has access to 2000 data points.
Figure 5 shows the results—all the models have training accuracy close to 1, but both TbT1000 and
TbT2000 overfits the data significantly, whereas TbV1000+1000 performs well.
1
题95
£
2J
O
O
<
0.85
0.8 L
0
------SGD
------TbT1000
------TbV1000+1000
------TbT2000
Figure 5:	Training and testing accuracy for different models (1000 samples, no noise)
To show that when the noise is higher, the advantage of train-by-validation increases, we keep the
same sample size and consider a “noisier” version of MNIST, where we randomly change the label
ofa sample with probability 0.2 (the new label is chosen uniformly at random, including the original
label). The results are shown in Figure 6. We can see that both train-by-train models, as well as SGD,
overfit easily with training accuracy close to 1 and their test performances are low. The train-by-
validation model performs much better.
Finally we run experiments on the complete MNIST data set (without label noise). For the train-by-
validation setting, we split the data set to 50,000 training samples and 10,000 validation samples.
As shown in Figure 7, in this case train-by-train and train-by-validation performs similarly (in fact
both are slightly weaker than the tuned SGD baseline). This shows that when the sample size is
sufficiently large, train-by-train can get comparable results as train-by-validation.
8
Under review as a conference paper at ICLR 2021
Figure 6: Training and testing accuracy for different models (1000 samples, 20% noise)
Steps	x104
Figure 7: Training and testing accuracy for different models (all samples, no noise)
Steps	x104
6 Conclusion and Future Works
In this paper, we have proved optimization and generalization guarantees for tuning the step size
for quadratic loss. From the optimization perspective, we considered a simple task whose objective
is a quadratic function. We proved that the meta-gradient can explode/vanish if the meta-objective
is simply the loss of the last iteration; we then showed that the log-transformed meta-objective has
polynomially bounded meta-gradient and can be successfully optimized. To study the generalization
issues, we considered the least squares problem. We showed that when the number of samples is
small and the noise is large, train-by-validation approach generalizes better than train-by-train; while
when the number of samples is large, train-by-train can also work well. Although our theoretical
results are proved for quadratic loss, this simple setting already yields interesting phenomenons
and requires non-trivial techniques to analyze. We have also verified our theoretical results on an
optimizer parameterized by neural networks and MNIST dataset.
Since this is a very first work studying the learning to learn approach, there are many potential future
works. One immediate future work is to extend the result for least squares to log-transformed meta
objective (as in Section 3). This is probably doable because compositing the log function with the
current meta-objective should not change its minimizer. For the least squares problem, we only
studied the generalization properties of the optimal step size under the meta-objective, it’s unclear if
meta-gradient descent can converge to such an optimal step size. We believe our techniques for the
meta-optimization of the simple quadratic objective function (Section 3) can also be useful in this
analysis. More broadly, we are also interested in analyzing more complicated optimizers on more
complicated tasks.
9
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, MatthieU
Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorflow: A system for large-
scale machine learning. In 12th USENIX Symposium on Operating Systems Design and Imple-
mentation (OSD116),pp. 265-283, 2016.
Daniel Alabi, Adam Tauman Kalai, Katrina Ligett, Cameron Musco, Christos Tzamos, and
Ellen Vitercik. Learning to prune: Speeding up repeated computations. arXiv preprint
arXiv:1904.11875, 2019.
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew W Hoffman, David Pfau, Tom Schaul,
Brendan Shillingford, and Nando De Freitas. Learning to learn by gradient descent by gradient
descent. In Advances in neural information processing systems, pp. 3981-3989, 2016.
Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin White. Learning-theoretic
foundations of algorithm configuration for combinatorial partitioning problems. arXiv preprint
arXiv:1611.04535, 2016.
Maria-Florina Balcan, Travis Dick, Tuomas Sandholm, and Ellen Vitercik. Learning to branch.
arXiv preprint arXiv:1803.10150, 2018a.
Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm design,
online learning, and private optimization. In 2018 IEEE 59th Annual Symposium on Foundations
of Computer Science (FOCS), pp. 603-614. IEEE, 2018b.
Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. A general theory of sample complex-
ity for multi-item profit maximization. In Proceedings of the 2018 ACM Conference on Economics
and Computation, pp. 173-174, 2018c.
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc V Le. Neural optimizer search with reinforce-
ment learning. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 459-468. JMLR. org, 2017.
Xinshi Chen, Yufei Zhang, Christoph Reisinger, and Le Song. Understanding deep architectures
with reasoning layer. arXiv preprint arXiv:2006.13401, 2020.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. arXiv preprint arXiv:1803.08089, 2018.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn
stochastic gradient descent with biased regularization. arXiv preprint arXiv:1903.10399, 2019.
Yan Duan, John Schulman, Xi Chen, Peter L Bartlett, Ilya Sutskever, and Pieter Abbeel. rl2 : Fast
reinforcement learning via slow reinforcement learning. arXiv preprint arXiv:1611.02779, 2016.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In Proceedings of the 34th International Conference on Machine Learning-
Volume 70, pp. 1126-1135. JMLR. org, 2017.
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse
gradient-based hyperparameter optimization. arXiv preprint arXiv:1703.01785, 2017.
Rong Ge, Sham M Kakade, Rahul Kidambi, and Praneeth Netrapalli. The step decay schedule: A
near optimal, geometrically decaying learning rate procedure for least squares. In Advances in
Neural Information Processing Systems, pp. 14951-14962, 2019.
Johannes Gunther, Alex Kearney, Nadia M Ady, Michael R Dawson, and Patrick M Pilarski. Meta-
learning for predictive knowledge architectures: A case study using tidbd on a sensor-rich robotic
arm. In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent
Systems, pp. 1967-1969, 2019.
Rishi Gupta and Tim Roughgarden. A pac approach to application-specific algorithm selection.
SIAM Journal on Computing, 46(3):992-1017, 2017.
10
Under review as a conference paper at ICLR 2021
Nicholas JA Harvey, Christopher Liaw, Yaniv Plan, and Sikander Randhawa. Tight analyses for
non-smooth stochastic gradient descent. arXiv preprint arXiv:1812.05217, 2018.
Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent.
In International Conference on Artificial Neural Networks, pp. 87-94. Springer, 2001.
Andrew Jacobsen, Matthew Schlegel, Cameron Linke, Thomas Degris, Adam White, and Martha
White. Meta-descent for online, continual prediction. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 33, pp. 3943-3950, 2019.
Prateek Jain, Dheeraj Nagaraj, and Praneeth Netrapalli. Making the last iterate of sgd information
theoretically optimal. arXiv preprint arXiv:1904.12443, 2019.
William B Johnson and Joram Lindenstrauss. Extensions of lipschitz mappings into a hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
Alex Kearney, Vivek Veeriah, Jaden B Travnik, Richard S Sutton, and Patrick M Pilarski.
Tidbd: Adapting temporal-difference step-sizes through stochastic meta-descent. arXiv preprint
arXiv:1804.03334, 2018.
Ke Li and Jitendra Malik. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.
Zhenguo Li, Fengwei Zhou, Fei Chen, and Hang Li. Meta-sgd: Learning to learn quickly for few-
shot learning. arXiv preprint arXiv:1707.09835, 2017.
Chaoyue Liu and Mikhail Belkin. Accelerating sgd with momentum for over-parameterized learn-
ing. arXiv preprint arXiv:1810.13395, 2018.
Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimiza-
tion through reversible learning. In International Conference on Machine Learning, pp. 2113-
2122, 2015.
Luke Metz, Niru Maheswaranathan, Jeremy Nixon, Daniel Freeman, and Jascha Sohl-Dickstein.
Understanding and correcting pathologies in the training of learned optimizers. In International
Conference on Machine Learning, pp. 4556-4565, 2019.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. 2016.
Andrei A Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osin-
dero, and Raia Hadsell. Meta-learning with latent embedding optimization. arXiv preprint
arXiv:1807.05960, 2018.
Nicol N Schraudolph. Local gain adaptation in stochastic gradient descent. 1999.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Conver-
gence results and optimal averaging schemes. In International conference on machine learning,
pp. 71-79, 2013.
Richard S Sutton. Adapting bias by gradient descent: An incremental version of delta-bar-delta. In
AAAI, pp. 171-176. San Jose, CA, 1992.
Roman Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv preprint
arXiv:1011.3027, 2010.
Roman Vershynin. High-dimensional probability: An introduction with applications in data science,
volume 47. Cambridge university press, 2018.
Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
Charles Blundell, Dharshan Kumaran, and Matt Botvinick. Learning to reinforcement learn.
arXiv preprint arXiv:1611.05763, 2016.
Olga Wichrowska, Niru Maheswaranathan, Matthew W Hoffman, Sergio Gomez Colmenarejo,
Misha Denil, Nando de Freitas, and Jascha Sohl-Dickstein. Learned optimizers that scale and
generalize. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 3751-3760. JMLR. org, 2017.
Yuhuai Wu, Mengye Ren, Renjie Liao, and Roger Grosse. Understanding short-horizon bias in
stochastic meta-optimization. arXiv preprint arXiv:1803.02021, 2018.
11
Under review as a conference paper at ICLR 2021
In the appendix, we first give the missing proofs for the theorems in the main paper. Later in
Appendix F we give details for the experiments.
Notations: Besides the notations defined in Section 2, we define more notations that will be used
in the proofs.
For a matrix X ∈ Rn×d With n ≤ d, We denote its singular values as σι(X) ≥ •…≥ σn(X).
For a positive semi-definite matrix A ∈ Rd×d, we denote u>Au as kuk2A . For a matrix X ∈ Rd×n,
let ProjX ∈ Rd×d be the projection matrix onto the column span of X. That means, ProjX = SS>,
Where the columns of S form an orthonormal basis for the column span of X.
For any event E, we use 1 {E} to denote its indicator function: 1 {E} equals 1 when E holds and
equals 0 otherwise. We use E to denote the complementary event of E.
A	Proofs for Section 3 — alleviating gradient
explosion/vanishing problem for quadratic objective
in this section, we prove the results in Section 3. Recall the meta learning problem as follows:
The inner task is a fixed quadratic problem, where the starting point is fixed at w0 , and the loss
function is f (W) = 2w>Hw for some fixed positive definite matrix H ∈ Rd×d. Suppose the
eigenvalue decomposition of H is Pid=1 λiuiui>. in this section, we assume L = λ1 (H) and α =
λd(H) are the largest and smallest eigenvalues of H with L > α. We assume the starting point w0
has unit `2 norm. For each i ∈ [d], let ci be hw0, uii and let cmin = min(|c1 |, |cd|). We assume
cmin > 0 for simplicity, which is satisfied if w0 is chosen randomly from the unit sphere.
Let {wτ,η} be the GD sequence running on f(w) starting from w0 with step size η. For the meta-
objective, we consider using the loss of the last point directly, or using the log of this value. in
Section A.1, we first show that although choosing F(η) = f (wt,η) does not have any bad local
optimal solution, it has the gradient explosion/vanishing problem (Theorem 3). Then, in Section A.2,
we show choosing F (η) = t log f (wt,n) leads to polynomially bounded meta-gradient and further
show meta-gradient descent converges to the optimal step size (Theorem 4). Although the meta-
gradient is polynomially bounded, if we simply use back-propogation to compute the meta-gradient,
the intermediate results can still be exponentially large/small (Corollary 1). This is also proved in
Section A.2.
A. 1 Meta-gradient vanishing/explosion
in this section, we show although choosing F(η) = f (wt,η) does not have any bad local optimal
solution, it has the meta-gradient explosion/vanishing problem. Recall Theorem 3 as follows.
Theorem 3.	Let the meta objective be F(η) = f (wt,η) = 1 w>ηHwt,η. We know F(η) is a strictly
convex function in η with an unique minimizer. However, for any step size η < 2/L, ∣F0(η)∣ ≤
t Pd=ι c2λ2∣1 - ηλi∣2t-1; for any step size η > 2/L, ∣F0(η)∣ ≥ c2L2-t(ηL - 1)2t-1 - L2t.
intuitively, if we write wt,η in the basis of the eigen-decomposition of H, then each coordinate
evolve exponentially in t. The gradient of the standard objective is therefore also exponential in t.
Proof of Theorem 3. According to the gradient descent iterations, we have
wt,η = wt-i,η — η^f (wt-i,η) = wt-i,η - ηHwt-i,η = (I - ηH)wt-i,η = (I - ηH)tw0.
Therefore, F(η) := f (wt,η) = 2w> (I - ηH)2tHw0. Taking the derivative of F(η),
d
F0(η) = -tw0(I - ηH)2t-1H2w0 = -t X c2λ2(i - ηλi)2t-1,
i=1
12
Under review as a conference paper at ICLR 2021
where ci = hw0, uii . Taking the second derivative of F (η),
d
F00(η) =t(2t - 1)w0>(I-ηH)2t-2H3w0 =t(2t- 1)Xci2λi3(1 - ηλi)2t-2.
i=1
Since L > α, we have F00(η) > 0 for any η. That means F(η) is a strictly convex function in η with
a unique minimizer.
For any fixed η ∈ (0,2/L) We know |1 - ηλ∕ < 1 for all i ∈ [d]. We have
d
∣F0(η)∣ ≤ tXc2λ2∣ι-ηλi∣2t-1.
i=1
For any fixed η ∈ (2/L, ∞), we know ηL - 1 > 1. We have
户 0(η)
= - tc12L2(1 - ηL)2t-1 - t	X	ci2λi2(1 - ηλi)2t-1 - t X	ci2λi2(1-ηλi)2t-1
i=i：(i—ηλi )≤0	i=i：(i—ηλi)>o
d
≥tc2L2(ηL - 1)2t-1 - tX c2λ2 ≥ tc1L2(ηL - 1)2t-1 - L2t,
i=1
where the last inequality uses Pid=1 ci2 = 1.
A.2 Alleviating meta-gradient vanishing/explosion
We prove when the the meta objective is chosen as ɪ log f (wt,η), the meta-gradient is polynomially
bounded. Furthermore, we show meta-gradient descent can converge to the optimal step size within
polynomial iterations. Recall Theorem 4 as follows.
Theorem 4.	Let the meta objective be F(η) = 1 log f (wt,η). We know F(η) hasa Umquemmimizer
η* and F0(η) = O (c2-OLL-O)) for all η ≥ 0. Let {ηk} be the GD sequence running on F with
meta step size μk = 1∕√k. Suppose the starting step size no ≤ M. Given any 1/L > e > 0, there
exists k0 = MM- poly (^1- ,L, O, L-—) Such that forall k ≥ k0, |nk — n*| ≤ e.
When we take the log of the function value, the derivative of the function value with respect to η
becomes much more stable. We will first show some structural result on F(n) - it has a UnqiUe
minimizer and the gradient is polynomially bounded. Further the gradient is only close to 0 when
the point η is close to the unique minimizer. Then using such structural result we prove that meta-
gradient descent converges.
Proof of Theorem 4. The proof consists of three claims. In the first claim, we show that F has
a unique minimizer and the minus meta derivative always points to the minimizer. In the second
claim, we show that F has bounded derivative. In the last claim, we show that for any n that is
outside the ^-neighborhood of n*, | F0 (n) | is lower bounded. Finally, we combine these three claims
to finish the proof.
Claim 1. The meta objective F has only one stationary point that is also its unique minimizer n*.
*0	*	0
For any n ∈ [0, n*), F 0(n) < 0 and for any n ∈ (n*, ∞), F 0(n) > 0. Furthermore, we know
n*∈ [1∕L,1∕α].
I-V T	. . 1 1	CR ♦	i' 11
We can compute the derivative of F in n as follows,
Fo( ) = -2w>(I - nH)2t-1H2wo = -2Pd=Ic2λ2(1 - nλi)2t-1
w>(I- nH)2tHwo	—	Pd=1 c2λi(i - nλi)2t
(3)
It’s not hard to verify that the denominator Pid=1 ci2λi (1 - nλi)2t is always positive. Denote the
numerator -2 Pid=1 ci2λi2(1 - nλi)2t-1 as g(n). Since g0 (n) > 0 for any n ∈ [0, ∞), we know g(n)
13
Under review as a conference paper at ICLR 2021
is strictly increasing in η. Combing with the fact that g(0) < 0 and g(∞) > 0, we know there is a
unique point (denoted as η*) where g(η*) = 0 and g(η) < 0 for all η ∈ [0, η*) and g(η) > 0 for all
η ∈ (η , ∞). Since the denominator in F0(η) is always positive and the numerator equals g(η), We
know there is a unique point η* where F0(η*) = 0 and F0(η) < 0 for all η ∈ [0, η*) and F0(η) > 0
for all η ∈ (η*, ∞). It s clear that η* is the mmimizer of F.
Also, it S not hard to verify that for any η ∈ [0,1/L), F0(η) < 0 and for any η ∈ (1∕a, ∞),
F0(η) > 0. This implies that η* ∈ [1/L, 1∕α].
Claim 2. For any η ∈ [0, ∞), we have
4L3
IF (η)1 ≤ F----万-----7 := Dmax.
c2minα(L - α)
For any η ∈ [0, α+L], We have 11 - ηλi∣ ≤ 1 - ηα for all i. Dividing the numerator and denominator
in F0(η) by (1 - ηa)2t, we have
IFO(M= 2 P=1蒜(1-λ 产 T ≤ 2 P=Iciλ ≤ 2(α + L) Pd=I c2λ2 ≤	4L3
I W 1	Cda	+ Pd-I	c2λi( 1-λi)2t -	cdα(1 - ηα)	-	Cda(L	- α)	一	cddα(L	- α),
where the second last inequality uses η ≤ 0++l .
Similarly for any η ∈ (o+^l, ∞), We have 11 - ηλi∣ ≤ ηL - 1 for all i. Dividing the numerator and
denominator in F0(η) by (ηL - 1)2t, we have
F OZ) = 2 £3	⅛-⅜(n≡λi 产」≤ 2 Pd=IC2 λ2	≤ 2(α + L)	Pd=IC2 λ2	≤	4L3
⑺—c1L + Pd=2 Cλi(nL-i)2t - c1L(ηL - 1) -	c2L(L - a)	— c2L(L - a)
where the last inequality uses η ≥ α++L.
Overall, we know for any η ≥ 0,
∣F0(η)∣ ≤ -4L^ max (ɪ, ɪ) ≤ 2 4;-V ∙
L - α	∖cda CIL)	Cmina(L - a)
一 .	_	_ .	_，	-.，一	一一	一^	n	r .	J-r	-
Claim 3.	Given	M ≥ 2/a and 1/L	> e > 0, forany η ∈	[0, η*	— e] ∪	[η*	+ e, M], we	have
∣F0(η)∣ ≥ min
2 2eCdda3	2eCιL2
I L , (ML - 1)2
Dmin(M).
If η ∈ [0, η* - e] and η ≤ α+L, We have
FOs) = -2Pd=ιC2λ2(1- ηλi)2tτ = -2Pd=I C2λ2(1 - ηλi)2t-1 - P：=i，2入2(1 - η*λi)2tτ
Pd=I C2λi(1 - ηλi)2t	Pd=I C2λi(1 - ηλi)2t
where the second equality holds because Pd=I C2λ2(1 - η*λi)2t-1 = 0. For the numerator, we
have
dd
X C2λ2(1 - ηλi)2tτ - X C2λ2(1 - η*λi)2tT ≥cda2 ((1 - ηa)2tT - (1 - η*a)2tT)
i=1	i=1
≥Cdda2 ((1 — ηa)2t-1 — (1 — ηa — ea)2t-1);
for the denominator, we have
dd
XCi2λi(1-ηλi)2t ≤ XCi2λi (1 - ηa)2t,
i=1	i=1
14
Under review as a conference paper at ICLR 2021
where the second inequality holds because |1 - ηλ∕ ≤ 1 - ηα for all i. Overall, We have when
η ∈ [0, η* — e] and η ≤ α⅛，
∣F^0(η)∣ ≥2Ca ((1 - ηα)2t-1 - (I - ηα - ea)2tτ)
1	"I-	(Pd=j2λi) (1 - ηa)2t
〉	2ecda3	〉2ecda3
(pd=ι ciλi) (1 - ηa) L
where the last inequality holds because (1 - ηα) ≤ 1 and Pidci2λi ≤ L.
Similarly, if η ∈ [0, η* - e] and η ≥ 0++l , We have
∣户0()| ≥2CL ((1 - ηL)2tτ - (I - ηL - eL)2tτ)
I η ∣^	(P3c2λi)(1-ηL)2t
_2c2L2 ((ηL + eL - 1)2t-1 - (ηL - 1)2t-1)
(P3c2λ)(ηL - 1)2t
2ec12L3	2ec21α2L2
≥----------1---------- ≥ ---1----
Pid=1ci2λi (ηL - 1)2	(L-α)2
where the last inequality holds because η ≤ η* - e ≤ 1/a and Pd c2λi ≤ L.
If η ∈ [η* + e, ∞) and η ≤ α++L, we have
∣foz J、2CQa((1 - ηα + ea)2t-1 - (1 - ηa)2t-1)
IF (η)∣ ≥2	(P3c2λi) (1-ηa)2t
2ec2 α3
≥ —d—,
一L ,
If η ∈ [η* + e, ∞) and η ≥ α++L, we have
∣F0( )∣ ≥2c2L2 ((1 -ηL + ηe)2t-1 - (1 -ηL)2t-1)
∣ η ∣^	(Pd=IC2%)(1-ηL)2t
2ec12L3	2ec12 L2
≥1≥1
^(Pd=ι c2λi) (ηL -1)2 ^ (MML -1)2'
where the last inequality uses the assumption that η ≤ MM.
With the above three claims, we are ready to prove the optimization result. By Claim 1, we know
F0(η) < 0 for any η ∈ [0, η*) and F0(η) > 0 for any η ∈ (η*, ∞). So the opposite gradient descent
always points to the minimizer.
D D	D	D2f
Since μk = 1∕√k, when k ≥ k1 := —max we know μk ≤ ∙^~. By Claim 2, we know |F0(η) | ≤
DmaX for all η ≥ 0, which implies ∣μk F 0(η) | ≤ e forall k ≥ k∖. That means, meta gradient descent
will never overshoot the minimizer by more than e when k ≥ k1. In other words, after k1 meta
iterations, once η enters the e-neighborhood of η*, it will never leave this neighborhood.
We also know that at meta iteration k∖, we have nk` ≤ maχ(1∕α + Dmax, M) ：= MM. Here,
1/a + DmaX comes from the case that the eta starts from the left of η* and overshoot to the right of
η by Dmax. Since η* ∈ [1∕L, 1∕α], we have 切加-η*∣ ≤ max(1∕α, 1/a + DmaX - 1∕L,M -
1∕L) := R. By Claim 3, we know that ∣F0(η)∣ ≥ Dmin(MM) for any η ∈ [0, η* - e] ∪ [η* + e, MM].
Choosing some k2 satisfying Pk2Lkγ 1∕√k ≥ DR-, we know for any k ≥ k2, ∣ηk - η*∣ ≤ e.
15
Under review as a conference paper at ICLR 2021
Plugging in all the bounds for Dmin , Dmax from Claim 3 and Claim 2, we know there exists k1 =
表poly(cmin,L,1, L-a),k2 = MM6poly(c⅛,l, 1, L--α) satisfying these conditions.	口
Next, we show although the meta-gradient is polynomailly bounded, the intermediate results can
still vanish or explode if we use back-propogation to compute the meta-gradient.
Corollary 1. Ifwe choose the meta-objeCtive as F(η) = 1 log f (wt,n), when CompUting the meta-
gradient using back-propagation, there are intermediate results that are exponentially large/small
in number of inner-steps t.
Proof of Corollary 1. This is done by direct calculation. Ifwe use back-propagation to compute the
derivative of t log(f (wt,η)), We need to first compute fWttη 1 log(f (wt,η)) that equals f(W±).
Same as the analysis in Theorem 3, we can show f(W=)is exponentially large when η < 2/L and
is exponentially small When η > 2/L.
B Proofs of train-by-train v.s. train-by-validation (GD)
In this section, we show when the number of samples is small and when the noise level is a large
constant, train-by-train overfits to the noise in training tasks while train-by-validation generalizes
well. We separately prove the results for train-by-train and train-by-validation in Theorem 7 and
Theorem 8, respectively. Then, Theorem 5 is simply a combination of Theorem 7 and Theorem 8.
Recall that in the train-by-train setting, each task P contains a training set Strain with n samples. The
2
inner objective is defined as f (W) = 2n E(X y)三$也瓜(〈w, Xi- y) . Let {wτ,η} be the GD sequence
running on f(w) from initialization 0 (with truncation). The meta-loss on task P is defined as the
2
inner objective of the last point, ∆τvτ(„)(n, P) = f(wt,η) = 21n ∑(x,y)∈strain (hwt,η,x) - y) . The
empirical meta objective FT bT (n)(η) is the average of the meta-loss across m different tasks. We
show that under FTbT(n)(η), the optimal step size is a constant and the learned weight is far from
ground truth w* on new tasks. We prove Theorem 7 in Section B.2.
Theorem 7. Let the meta objeCtive FTbT(n)(η) be as defined in Equation 1 with n ∈ [d/4, 3d/4].
Assume noise level σ is a large Constant c1. Assume unroll length t ≥ c2, number of training tasks
m ≥ c3 log(mt) and dimension d ≥ c4 log(m) for Certain Constants c2, c3, c4. With probability at
least 0.99 in the sampling of the training tasks, we have
η Jain = θ⑴ and Ellwt,n ian - w*f = ^1)σ2,
for all ηt*rain ∈ arg minη≥0 FTbT(n)(η), where the expeCtation is taken over new tasks.
In Theorem 7, Ω(1) is an absolute constant independent with σ. Intuitively, the reason that train-by-
train performs badly in this setting is because there is a way to set the step size to a constant such
that gradient descent converges very quickly to the empirical risk minimizer, therefore making the
train-by-train objective very small. However, when the noise is large and the number of samples is
smaller than the dimension, the empirical risk minimizer (ERM) overfits to the noise and is not the
best solution.
In the train-by-validation setting, each task P contains a training set Strain with n1 sam-
ples and a validation set with n2 samples. The inner objective is defined as f(w) =
2⅛ P(X y)∈straiII (〈w, Xi - y)2. Let {wτ,η} be the GD sequence running on f(w) from initial-
ization 0 (with truncation). For each task P, the meta-loss ∆TbV (n1,n2) (η, P) is defined as the
loss of the last point wt,η evaluated on the validation set Svalid. That is, ∆TbV (n1,n2) (η, P) =
2n2 P(x,y)∈svalid (hwt,η,xi - y)2. The empirical meta objective FTbV(n1,n2)(η) is the average of
the meta-loss across m different tasks P1, P2, ..., Pm. We show that under FTbV(n1,n2)(η), the op-
timal step size is Θ(1∕t) and the learned weight is better than initialization 0 by a constant on new
tasks. Theorem 8 is proved in Section B.3.
Theorem 8. Let the meta objeCtive FTbV(n1,n2) (η) be as defined in Equation 2 with n1, n2 ∈
[d/4, 3d/4]. Assume noise level σ is a large Constant c1. Assume unroll length t ≥ c2, number of
16
Under review as a conference paper at ICLR 2021
training tasks m ≥ c3 and dimension d ≥ c4 log(t) for certain constants c2 , c3 , c4 . With probability
at least 0.99 in the sampling of training tasks, we have
ηValid = θ(1lZt) andE∣∣wt,ηValid- w*∣∣2 = l∣w*k2 - a(1)
for all ηvValid ∈ arg minη≥0 FT bV (n1,n2)(η), where the expectation is taken over new tasks.
Intuitively, train-by-validation is optimizing the right objective. As long as the meta-training prob-
lem has good generalization performance (that is, good performance on a few tasks implies good
performance on the distribution of tasks), then train-by-validation should be able to choose the op-
timal learning rate. The step size of Θ(1∕t) here serves as regularization similar to early-stopping,
which allows gradient descent algorithm to achieve better error on test data.
Notations We define more quantities that are useful in the analysis. In the train by train setting,
given a task Pk := (D(WkV), St(rkai)n, `). The training set St(rkai)n contains n samples {xi(k), yi(k)}in=1 with
yi(k) = DWkV, xi(k)E + ξi(k).
Let Xtrkin be an n X d matrix with its i-th row as (x(k))>. Let Hrkn := n(Xtrkin)>Xtrkn be the
covariance matrix of the inputs in St(rkai)n . Let ξt(rkai)n be an n-dimensional column vector with its i-th
entry equal to ξi(k).
Since n ≤ d, with probability 1, we know Xt(rkai)n is full row rank. Therefore, Xt(rkai)n has pseudo-
inverse (Xtrki；)* SUch that Xtrkin(Xtrki；户 = In. It,s not hard to verify that there exists Wtrain =
Wt(rkai)n, xi(k)	for every (xi(k), yi(k)) ∈ St(rkai)n
Proj(X(k))>Wk + (Xtrkin)tξtrkin such that y(k)
Here, Proj(X(k))> is the projection matrix onto the column span of (Xt(rkai)n)>. We also denote
Proj(X(k))>WkV as (Wt(rkai)n)V. We use Bt(,kη) to denote (I - (I - ηHt(rkai)n)t). Let Wt(,kη) be the weight
obtained by running GD on St(rkai)n with step size η (with truncation).
With the above notations, it’s not hard to verify that for task Pk, the inner objective f (W)
2 11W - Wtraiin 11 (k) . The meta-loss on task Pk is just δTbT (n) (η, Pk ) = 2 1wt,η- Wt(rkai)n ∣∣2 (k) .
Htrain	Htrain
In the train-by-validation setting, each task Pk contains a training set St(rkai)n with n1 sam-
ples and a validation set Sv(akl)id with n2 samples. Similar as above, for the training set St(rkai)n ,
we can define ξt(rkai)n,Xt(rkai)n,Ht(rkai)n,Wt(rkai)n,Bt(,kη),Wt(,kη); for the validation set Sv(akl)id, we can define
ξVklid, Xvk)d, Hvk)d, w(klid∙ With these notations, the inner objective is f(w) = 2 ∣∣w - *匐|吟
and the meta-loss is ∆τbv5皿)(小？k) = 1 ∣∣wt,η — w(3∣∣ (k).
Hv(ali)d
We also use these notations without index k to refer to the quantities defined
on task P. In the proofs, we ignore the subsripts on n, n1, n2 and simply write
∆TbT (η, Pk), ∆Tbv (η, Pk), FTbT, FTbv, FTbT, FTbv.
B.1	Overall Proof Strategy
In this section (and the next), we follow similar proof strategies that consists of three steps.
Step 1: First, we show for both train-by-train and train-by-validation, there is a good step size that
achieves small empirical meta-objective (however the step sizes and the empirical meta-objective
they achieve are different in the two settings). This does not necessarily mean that the actual optimal
step size is exactly the good step size that we propose, but it gives an upperbound on the empirical
meta-objective for the optimal step size.
17
Under review as a conference paper at ICLR 2021
Step 2: Second, we define a threshold step size such that for any step size larger than it, the
empirical meta-objective must be higher than what was achieved at the good step size in Step 1.
This immediately implies that the optimal step size cannot exceed this threshold step size.
Step 3: Third, we show the meta-learning problem has good generalization performance, that
is, if a learning rate η performs well on the training tasks, it must also perform well on the task
distribution, and vice versa. Thanks to Step 1 and Step 2, we know the optimal step size cannot
exceed certain threshold and then only need to prove generalization result within this range. The
generalization result is not surprising as we only have a single trainable parameter η, however we
also emphasize that this is non-trivial as we will not restrict the step size η tobe small enough that the
algorithms do not diverge. Instead we use a truncation to alleviate the diverging problem (this allows
us to run the algorithm on distribution of data whose largest possible learning rate is unknown).
Combing Step 1, 2, 3, we know the population meta-objective has to be small at the optimal step
size. Finally, we show that as long as the population meta-objective is small, the performance of the
algorithms satisfy what we stated in Theorem 5. The last step is easier for the train-by-validation
setting, because its meta-objective is exactly the correct measure that we are looking at; for the train-
by-train setting we instead look at the property of empirical risk minimizer (ERM), and show that
anything close to the ERM is going to behave similarly.
B.2	Train-by-train (GD)
Recall Theorem 7 as follows.
Theorem 7. Let the meta objective FT bT (n) (η) be as defined in Equation 1 with n ∈ [d/4, 3d/4].
Assume noise level σ is a large constant c1. Assume unroll length t ≥ c2, number of training tasks
m ≥ c3 log(mt) and dimension d ≥ c4 log(m) for certain constants c2, c3, c4. With probability at
least 0.99 in the sampling of the training tasks, we have
ηLn = θ⑴ andE∣∣wt,η鼠“ -w*∣∣2 = a(1)b2,
for all ηin ∈ argminη≥o FTbT(n)(η), where the expectation is taken over new tasks.
According to the data distribution, we know Xtrain is an n × d random matrix with each entry
i.i.d. sampled from standard Gaussian distribution. In the following lemma, we show that the
covariance matrix Hrain is approximately isotropic when d/4 ≤ n ≤ 3d/4. Specifically, We show
√√d ≤ σi(Xtrain) ≤ √Ld and L ≤ λi (Htrain) ≤ L for all i ∈ [n] with L = 100. We use letter
L to denote the upper bound of kHtrain k to emphasize that this bounds the smoothness of the inner
objective. Throughout this section, we use letter L to denote constant 100. The proof of Lemma 1
follows from random matrix theory. We defer its proof into Section B.2.4.
Lemma 1. Let X ∈ Rn×d be a random matrix with each entry i.i.d. sampled from standard
Gaussian distribution. Let H := 1/nX >X. Assume n = Cd with C ∈ [ 4, 3 ]. Then, with probability
at least 1 一 exp(-Ω(d)), there exists constant L = 100 such that
√d
√L
≤ σi(X) ≤ √Ld and ɪ ≤ λ%(H) ≤ L,
for all i ∈ [n].
In this section, we always assume the size of each training set is within [d/4, 3d/4] so Lemma 1
holds. Since kHtrain k is upper bounded by L with high probability, we know the GD sequence
converges to wtrain for η ∈ [0, 1/L]. In Lemma 2, we prove that the empirical meta objective FTbT
monotonically decreases as η increases until 1/L. Also, we show FTbT is exponentially small in t
at step size 1/L. This serves as step 1 in Section B.1. The proof is deferred into Section B.2.1.
Lemma 2. With probability at least 1 — m exp(一Ω(d)), FTbT(η) is monotonically decreasing in
[0, 1/L] and
FTbT (1/L) ≤ 2L2σ2
18
Under review as a conference paper at ICLR 2021
When the step size is larger than 1/L, the GD sequence can diverge, which incurs a high loss in meta
objective. Later in Definition 1, We define a step size η such that the GD sequence gets truncated
with descent probability for any step size that is larger than η. In Lemma 3, we show with high
probability, the empirical meta objective is high for all η > η. This serves as step 2 in the proof
strategy described in Section B.1. The proof is deferred into Section B.2.2.
Lemma 3. With probability at least 1 一 exp(一Ω(m)),
σ2
FTbT ㈤ ≥ 10L8，
for all η > η.
By Lemma 2 and Lemma 3, we know the optimal step size must lie in [1 /L,η]. We can also show
1/L <η < 3L, so ηtrain is a constant. To relate the empirical loss at ntrain to the population loss. We
prove a generalization result for step sizes within [1 /L,η]. This serves as step 3 in Section B.1. The
proof is deferred into Section B.2.3.
Lemma 4. Suppose σ is a large constant c1. Assume t ≥ c2 , d ≥ c4 for certain constants c2 , c4 .
With probability at least 1 — m exp(-Ω(d)) — O(t + m) exp(-Ω(m)),
σ2
IFTbT(η) - FTbT(η)∣ ≤ L3,
for all η ∈ [1/L, η],
Combining the above lemmas, we know the population meta objective FTbT is small at n%n, which
means wt,η篙II is close to the ERM solution. Since the ERM solution overfits to the noise in training
tasks, we know ∣∣ wt,η*aiιι - w1∣ has to be large. We present the proof of Theorem 7 as follows.
Proof of Theorem 7. We assume σ is a large constant in this proof. According to Lemma 2, we
know with probability at least 1 - m exp(一Ω(d)), FTbT(η) is monotonically decreasing in [0,1/L]
and FTbT(1/L) ≤ 2L2σ2(1 - 1/L2)t. This implies that the optimal step size 优疝 ≥ 1/L and
FTbT (ηtrain) ≤ 2L2σ2 (1-1/L2)t. ByLemma3, we know with probability at least 1-exp(-Ω(m)),
2
FTbT (η) ≥ 10L8 for all η > η, where η is defined in Definition 1. As long as t ≥ c2 for certain
constant c2, we know 1L > 2L2σ2(1 - 1/L2)t, which then implies that the optimal step size ηtrain
lies in [1 /L,η]. According to Lemma 6, we know η ∈ (1/L, 3L). Therefore 瑞出口 is a constant.
According to Lemma 4, we know with probability at least 1 - mexp(一Ω(d)) - O(t +
2
m)exp(-Ω(m)), ∣FTbT(η) - FTbT(η)∣ ≤ L3, for all η ∈ [1/L, η]. As long as t is larger than
2
some constant, we have FTbT(n；ain) ≤ L3 ∙ Combing with the generalization result, we have
FTbT(ηtrain) ≤ 2σ2. Next, we show that under a small population loss, E ∣∣wt,η意II - w*∣∣2 has to
be large.
Let Ei be the event that √d/√L ≤ σ%(Xtrain) ≤ √Ld and 1/L ≤ λ%(Htrain) ≤ L for all i ∈ [n] and
√dσ,4 ≤ kξtrain∣∣ ≤ √dσ. We have
21	2
E llwt,η温-WtrainIlHtrain ≥LE llwt<1in - wtrainll 1 {E1}
12
≥	丁(E ||wt，n 温-Wtrain-(Xtrain 户 ξtrain 1 { {El})
L
12
≥	丁(EII(Xtrain户ξtrain 1 1 {Ei}- E ||wt,瑞就-Wtrain 八 ɪ {El}).
L
Since E ∣∣wt,η温一Wtrain 11 Htram ≤ 4σ2, this then implies
√4σ2	2σ
LT3 = ~L'
Conditioning on Ei, we can lower bound ∣∣(Xtrain)*ξtrain∣∣ by -√^. According to Lemma 1 and
Lemma 45, we know Pr[Eι] ≥ 1 - exp(-Ω(d)). As long as d is at least certain constant, we have
19
Under review as a conference paper at ICLR 2021
Pr[Eι ] ≥ 0.9. This then implies Ell(Xtrain) Gain ∣∣ 1 {Eι} ≥ 49√L .Therefore, We have
E llwt,η温
9σ
-Wtrainll 1 {E1} ≥4。海
2σ 9σ	2σ	σ
^L = 4L - ^L = 4L
Where the first equality uses L = 100. Then, We have
2
E llwtFin - w*11 ≥ E llwt,η%n - /ainU 1 {EI} ≥ (E llwt,η温-Wtrainll 1 {EI}) ≥ 16^2，
Where the first inequality holds because for any Strain , wt*rain is the projection of w* on the subspace
of Strain and wt,η*ain is also in this subspace. Taking a union bound for all the bad events, we know
this result holds With probability at least 0.99 as long as σ is a large constant c1 and t ≥ c2, m ≥
c3 log(mt) and d ≥ c4 log(m) for certain constants c2, c3, c4.
B.2.1	B EHAVIOR OF FTbT FOR η ∈ [0, 1/L]
In this section, we prove the empirical meta objective FTbT is monotonically decreasing in [0, 1/L].
Furthermore, we show FTbT (1/L) is exponentially small in t.
Lemma 2. With probability at least 1 - m exp(一Ω(d)), FTbT(η) is monotonically decreasing in
[0, 1/L] and
Fτbτ (1/L) ≤ 2L2σ2
t
Proof of Lemma 2. For each k ∈ [m], let Ek be the event that √d/√L ≤ σi (Xtrain) ≤ √Ld and
1/L ≤ λi(Htrain) ≤ L for all i ∈ [n] and JdQ/4 ≤ |瓜疝|| ≤ √dσ. Here, L is constant 100 from
Lemma 1. According to Lemma 1 and Lemma 45, we know for each k ∈ [m], Ek happens with
probability at least 1 - exp(一Ω(d)). Taking a union bound over all k ∈ [m], we know ∩k∈[m]Ek
holds with probability at least 1 - m exp(一Ω(d)). From now on, we assume ∩k∈[m]Ek holds.
Let,s first consider each individual loss function ∆τbτ(η,Pk). Let {wTk)} be the GD sequence
without truncation. We have
w(k) - w⑹=w⑹ -w(k) - TlH(k) (w(k) - w⑹)
wτ,η - wtrain =wτ -1,η - wtrain - ηHtrain(wτ -1,η - wtrain)
=(I - ηHtrkin)(WT-ι,η - w(ki)n) = -(I - ηH(kn)tw(kn.
FOrany η ∈ [0,1/L], we have lltυTk,)ll ≤ lwknll Tl(w(2)* + (Xtrkin)dinH ≤ 2√Lσ for any
τ . Therefore, llwt(,kη) ll never exceeds the norm threshold and never gets truncated.
Noticing that δTbT(η, Pk) = 2(Wtkn - w(ki)n)>Htrkin(W(k) - wt(ki)n), we have
δTbT(η, Pk) = 1(w(2)>Htrkin(I- ηHtrkin )2twta)∏.
Taking the derivative of ∆TbT (η, Pk) in η, we have
∂η ∆TbT (η,Pk) = -t(wtra)n)>(Htrkin)2(I - ηH(k^t-1w^n.
Conditioning on Ek , we know 1/L ≤ λi (Ht(rkai)n) ≤ L for all i ∈ [n] and Ht(rkai)n is full rank in the
row span of XtrkiI. Therefore, we know 品∆TbT(η, Pk) < 0 for all η ∈ [0,1/L). Here, we assume
llwt(rkai)n ll >0, which happens with probability 1.
Overall, we know that conditioning on ∩k∈[m]Ek, every ∆TbT (η, Pk) is strictly decreasing for η ∈
[0,1/L]. Since FTbT(η)：= + Pm=I Δtbt(η, Pk), We know FTbT(η) is strictly decreasing when
η ∈ [0, 1/L].
20
Under review as a conference paper at ICLR 2021
At step size η = 1/L, we have
δTbT (η,Pk) =2(W(a；In)>现3(I- nHrkn产w(k)n
≤ 1 L (1 - ⅛) IlW(Ml ≤ 2L2σ2 (1 - L),
where We upper bound ∣∣w(k)n∣∣ by 4Lσ2 at the last step. Therefore, We have FTbT(1/L) ≤
2L2σ2(1 - L12)t.	□
_ λ λ	∙a	/ -
B.2.2	LOWER BOUNDING FTbT FOR η ∈ (η, ∞)
In this section, we prove that the empirical meta objective is lower bounded by Ω(σ2) with high
probability for η ∈ (η, ∞). Step size η is defined such that there is a descent probability of diverging
for any step size larger than η. Then, we show the contribution from these truncated sequence will
be enough to provide an Ω(σ2) lower bound for FTbT. The proof of Lemma 3 is given at the end of
this section.
Lemma 3. With probability at least 1 一 exp(一Ω(m)),
σ2
FTbT(n)≥ 10L8，
for all η > η.
We define η as the smallest step size such that the contribution from the truncated sequence in the
population meta objective exceeds certain threshold. The precise definition is as follows.
Definition 1. Given a training task P, let Ei be the event that √d∕√L ≤ σ%(Xtrain) ≤ √Ld and
1/L ≤ λi(Htrain) ≤ L for all i ∈ [n] and √dσ∕4 ≤ ∣∣ξtrain∣∣ ≤ √dσ. Let E2(η) be the ^vent that the
GD Sequence is truncated with step size η. Define η as follows,
η = inf {η ≥ 0 EI kwt,η - W train ∣Htrain 1 {E1 ∩ E2 ㈤} ≥ L6 }.
In the next lemma, we prove that for any fixed training set, 1 {Ei ∩E2(η0)} ≥
1 {Ei ∩E2(η)} for any W ≥ η. This immediately implies that Pr[E1 ∩ E2(η)] and
E1 ∣∣wt,η — WtrainIlHtrain 1 {Ei ∩ E2(η)} is non-decreasing in η.
Basically we need to show, conditioning on Ei, if a GD sequence gets truncated at step size η, it
must be also truncated for larger step sizes. Let {Wτ0 ,η} be the GD sequence without truncation. We
only need to show that for any τ, if ∣Wτ0 ,η ∣ exceeds the norm threshold, ∣Wτ0,η0 ∣ must also exceed
the norm threshold for any η0 ≥ η. This is easy to prove if τ is odd because in this case ∣Wτ0 ,η ∣ is
always non-decreasing in η. The case when τ is even is trickier because there indeed exists certain
range ofη such that ∣Wτ0 ,η ∣ is decreasing in η. We manage to prove that this problematic case cannot
happen when ||w；m ∣∣ is at least 4√Lσ. The full proof of Lemma 5 is deferred into Section B.2.4.
Lemma 5. Fixing a task P, let Ei and E2(η) be as defined in Definition 1. We have
1{E1 ∩E2(η0)} ≥ l{Ei ∩E2(η)},
for any η0 ≥ η.
In the next Lemma, we prove that η must lie within (1/L, 3L). We prove this by showing that the GD
sequence never gets truncated for η ∈ [0, 2∕L] and almost always gets truncated for η ∈ [2.5L, ∞).
The proof is deferred into Section B.2.4.
Lemma 6. Let η be as defined in Definition 1. Suppose σ isa large constant ci. Assume t ≥ c?,d ≥
c4 for some constants c2 , c4 . We have
1/L < η < 3L.
21
Under review as a conference paper at ICLR 2021
Now, we are ready to give the proof of Lemma 3.
Proof of Lemma 3. Let Ei and E2(η) be as defined in Definition 1. For the simplicity of the proof,
We assume Eɪ ∣∣wt,η - WtrainIlHtraiII ɪ {Ei ∩ E2(η)} ≥ Lθ. We will discuss the proof for the other
case at the end, which is very similar.
Conditioning on Ei, we know ɪ ∣∣wt,η — WtrainllHtraiII ≤ 18L2σ2. Therefore, we know Pr[E1 ∩
E2 (η)] ≥ i8L8. For each task Pk, define EIk) and E2k)(η) as the corresponding events on train-
ing set Strkin. By HOeffding's inequality, we know with probability at least 1 - exp(-Ω(m)),
m
m X1nE(k) ∩E2k)(研 ≥ 20L8.
k=i
By Lemma 5, we know 1 ∣E(k) ∩ E2k)(η)} ≥ 1 ∣E(k) ∩ E2k)(η)} for any η ≥ η. Then, we can
lower bound FTbT for any η > η as follows,
mm
1	1	(k)	(k) 2	1	1	(k)	(k) 2	(k)	(k)
FTbT⑺=m T 2 ^wt,η - wtrain^H(k) ≥嬴 T 2 ||% - wtrain ||础)"Ei ∩ E2 (叫
k=i	train	k=i	train
m
≥2σ2 m Xι{E(k) ∩E2k)(η)}
k=i
m2
≥2σ2 亟 X1nE(k) ∩E2k)(η)o ≥ 10L8,
k=i
where the second inequality lower bounds the loss for one task by 2σ2 when the sequence gets
truncated.
We have assumed E2 ∣∣wt,η - WtrainIlHtraiII 1 {Ei ∩E2(η)} ≥ L26 in the proof. Now, we show the
proof also works when Eɪ ∣∣wt,η - WtrainllHtraiII ɪ {Ei ∩E2(η)} < L26 with slight changes. Accord-
ing to the definition and Lemma 5, we know Eɪ ∣∣wt,η - WtrainllHtrahI ɪ {Ei ∩ E2(η)} > L26 for all
η > η. At each training set Strain, we can define 1 {Ei ∩ E2(η0)} as limη→η+ 1 {Ei ∩ E2(η)} . We
also have Pr[Ei ∩ E2(η0)] ≥ ɪj^. The remaining proof is the same as before as we substitute
ι{Ei ∩E2(η)} by ι{Ei ∩E2(n0)}.	□
B.2.3 Generalization for η ∈ [1∕L,η]
in this section, we show empirical meta objective FTbT is point-wise close to population meta
objective FTbT for all η ∈ [1∕L,η].
Lemma 4. Suppose σ is a large constant ci. Assume t ≥ c2 , d ≥ c4 for certain constants c2, c4.
With probability at least 1 — m exp(-Ω(d)) — O(t + m) exp(-Ω(m)),
σ2
IFTbT(η) - FTbT(η)∣ ≤ L,
for all η ∈ [1∕L, η],
T . < ∙	. ∙	r∙ . <	τ^S	.	T-ɪ	Γ∙	f` 1	1 .1	.	. i`
in this section, we first show FTbT concentrates on FTbT for any fixed η and then construct -net for
R	1 τι	Γ∙	_ Γ-ι / 7-	~1	5 T ♦	.1	Γ∙ Γ∙ T	Λ ..1	1
FTbT and FTbT for η ∈ [1∕L, η]. We give the proof of Lemma 4 at the end.
We first show that for a fixed η, FT bT (η) is close to FT bT (η) with high probability. We prove the
meta-loss on each task ∆TbT (η, Pk) is O(1)-subexponential. Then we apply Bernstein’s inequality
to get the result. The proof is deferred into Section B.2.4. We will assume σ is a large constant and
t ≥ c2,d ≥ c4 for some constants c2, c4 so that Lemma 6 holds and η is a constant.
Lemma 7. Suppose σ is a constant. For any fixed η and any 1 >	> 0, with probability at least
1	— exp(-Ω(e2m)),
IFTbT(η) - FTbT(η)∣ ≤ e.
22
Under review as a conference paper at ICLR 2021
Next, We construct an e-net for FTbT. By the definition of η, We know for any η ≤ η, the contribution
from truncated sequences in FTbT (η) is small. We can show the contribution from the un-truncated
sequences is O(t)-lipschitz.
Lemma 8. Suppose σ is a large constant c1. Assume t ≥ c2, d ≥ c4 for some constant c2, c4. There
exists an 坐--net N ⊂ [1/L, η] for FTbT With |N| = O(t). That means,forany η ∈ [1∕L,η],
IFTbT(η) — FTbT(η0)∣ ≤
11σ2
R,
for η0 = arg minη00∈N,η00≤η(η - η00).
Proof of Lemma 8. Let Ei and E2(η) be as defined in Definition 1. For the simplicity of the proof,
we assume E1 ∣∣wt,n - WtrainIlHtraiII ɪ {Ei ∩ E2(η)} ≤ fθ. We will discuss the proof for the other
case at the end, Which is very similar.
We can divide E1 ∣∣wt,n - WtrainllHtram as follows,
E2 kwt,n - WtrainllHtrain
=E2 kwt,n - WtrainIlHtrain 1 {E1 ∩E2(η)} + E2 llwt,n - WtrainIlHtrain 1 {E1 ∩E^2(η)}
+ E2 llWt,n - WtrainkHkain ɪ {El} ∙
We will construct an e-net for the first term and show the other two terms are small. Let’s first con-
12
sider the third term. Since 2 ∣∣Wt,n - WtrainIlHtram is O(1)-SUbexPOnentialand Pr[Eι] ≤ exp(-Ω(d)),
we have E1 ∣∣Wt,n - WtraInIlHtraiII ɪ {Ei} = O(1) exp(一Ω(d)). Choosing d to be at least certain con-
stant, we know 2 ||Wt,n - WtraInkHtolin 1 {E1 } ≤ σ2∕L4.
Then we upper bound the second term. Since E1 ∣∣Wt,n — WtrainkHtrailI ɪ {Ei ∩E2(η)} ≤ L26 and
2	∣∣Wt,n - WtrainkHtraiII ≥ 2σ2 When Wt,η diverges, we know Pr[E1 ∩ E2(η)] ≤ 击.Then, we can
upper bound the second term as follows,
E 2 llWt,n - WtrainkHtrain ɪ {E1 ∩ E^2(η)} ≤ 18L2。2 元6 =
Next, we show the first term 1 ∣∣Wt,n — WtrainkHtraiII ɪ {E1 ∩ E2(η)} has desirable Lipschitz condition.
According to Lemma 5, we know 1 {E1 ∩E2(η)} ≥ 1 {E1 ∩ E2(η)} for any η ≤ η. Therefore,
conditioning on E1 ∩ E2(η), we know Wt,η never gets truncated for any η ≤ η. This means Wt,η =
Bt,ηWtrain With Bt,η = (I - (I - ηHtrain)t). We Can compute the derivative of1 ∣∣Wt,η - WtrainkHtrain
as follows,
∂1	2	t1
∂η 2 Il wt,η - WtrainkHtrain =〈tHrain(I - ηHtrain)	Wtrain, Htrain(Wt,η - Wtrain
Since ∣∣Wt,ηk = k(I - (I - ηHtrai∏)t)Wtrai∏k ≤ 4√Lσ and kWtraink ≤ 2√Lσ,
we have k(I - ηHtrain)tWtraink ≤ 6√Lσ. We can bound ∣∣(I - ηHtrain)t-1 Wtrain∣∣ with
k(I - ηHtrain)t Wtrain k + kWtraink by bounding the expanding directions using k(I - ηHtrain)tWtraink
and bounding the shrinking directions using kWtrain k . Therefore, we can bound the derivative as
follows,
∂η2 kWtw - WtrainkHtain
≤ tL X 8√Lσ X 6L√Lσ = 48L3σ2t.
Suppose σ is a constant, we know E1 ∣∣Wt,η - WtrainkHtrailI ɪ {E1 ∩ E2(η)} is O(t)-lipschitz. There-
2
fore, there exists an	ɪɪ-net N for	E1	∣∣Wt,η - WtrainkHtrain	ɪ {E1	∩	E2(η)}	with size	O(t).	That
means, for any η ∈ [1∕L, η],
1	2	1	2	σ2
E2 ku⅛,η - WtrainkHtrain ɪ {E1 ∩ E2(η)} - Eg I*" - WtrainkHtrain ɪ {E1 ∩ E2(η)} ≤ L
23
Under review as a conference paper at ICLR 2021
for η0 = arg minη00∈N,η00≤η(η - η00). Note we construct the -net in a particular way such that η0 is
chosen as the largest step size in N that is at most η.
Combing with the upper bounds on the second term and the third term, We have for any η ∈ [1/L, η],
IFTbT(η) - FTbT(nO)I ≤
11σ2
for η0 = arg minη00∈N,η00≤η(η - η00).
In the above analysis, we have assumed E1 ∣∣wt,η - WtrainI∣Htr. 1 {Eι ∩E2(η)} ≤ l6. The
proof can be easily generalized to the other case.	We can define	1 {Eι ∩E2(η0)}	as
limη→η- 1 {Eι ∩E2(η)} . Then the proof works as	long	as we	substitute	1 {Eι ∩E2(η)}	by
1 {Eι ∩ E2 (η0)} . We will also add η into the e-net.	□
In order to prove FTbT is close to FTbT point-wise in	[1/L,	η], we	still need	to construct an e-net
for the empirical meta objective FTbT .
Lemma 9. Suppose σ is a large constant c1. Assume t ≥ c2, d ≥ c4 for certain constants c2, c4.
σ2
With probability at least 1 — mexp(-Ω(d)), there exists an 耳-net N0 ⊂ [1∕L,η] for FTbT With
∣N∣ = O(t + m). That means, for any η ∈ [1∕L, η],
0
IFTbT (η) - FTbT (η )I ≤
σ2
L4,
for η0 = arg minη00∈N0,η00≤η(η - η00).
Proof of Lemma 9. For each k ∈ [m], let Eι,k be the event that √d∕√L ≤ σ% (Xtrkin) ≤ √Ld and
1/L ≤ λi(Htrkin) ≤ L for all i ∈ [n] and √dσ∕4 ≤ Iktrai"∣ ≤ √dσ. According to Lemma 1 and
Lemma 45, we know with probability at least 1 - m exp(一Ω(d)), Eι,k,s holdfor all k ∈ [m]. From
now on, we assume all these events hold.
Recall that the empirical meta objective as follows,
m
1
FTbT(η) .二 一〉J ∆TbT(η,Pk).
m k=1
For any k ∈ [m], let %,k be the smallest step size such that w(,k) gets truncated. If ηc,k > η, by
similar argument as in Lemma 8, we know Δtbt(η, Pk) is O(t)-Lipschitz in [1/L, η] as long as σ
is a constant. If ηc,k ≤ η, by Lemma 5 we know w(k gets truncated for any η ≥ ηc,k. This then
implies that Δtbt (η,Pk) isa constant function for η ∈ [ηc,k, η]. We can also show that Δtbt (η, Pk)
is O(t)-Lipschitz in [1∕L, ηc,k). There might be a discontinuity in function value at ηc,k, so we need
to add ηc,k into the e-net.
Overall, we know there exists an
η ∈ [1/L, η],
That means, for any
for η0 = arg minη00∈N0,η00≤η(η - η00).
□
τ-,∙	11	Λ ∙ T	r T	C 1 ɪ	∕∖ .	,1,∙A	♦	∙	.	∙	1	.
Finally, we combine Lemma 7, Lemma 8 and Lemma 9 to prove that FTbT is point-wise close to
FTbT for η ∈ [1/L,n].
Proof of Lemma 4. We assume σ as a constant in this proof. By Lemma 7, we know with probability
at least 1 - exp(一Ω(e2m)), IFTbT(η) - FTbT(η)∣ ≤ e for any fixed η. By Lemma 8, we know
there exists an ILj2-net N for FTbT with size O(t). By Lemma 9, we know with probability at least
2
1 - m exp(-Ω(d)), there exists an L1 -net N0 for FTbT with size O(t + m). According to the proofs
24
Under review as a conference paper at ICLR 2021
11 2
of Lemma 8 and Lemma 9, it S not hard to verify that N ∪ N0 is still an IL^ -net for FTbT and FTbT.
That means, for any η ∈ [1∕L,η], We have
00
|FTbT (η) - FTbT(η )|, |FT bT (η) - FTbT (η )| ≤
11σ2
R,
for η0 = arg minη00∈N∪N0,η00≤η(η - η00).
Taking a union bound over N ∪ N0, we have with probability at least 1 - O(t + m) exp(-Ω(m)),
IFTbT (η) - FTbT (η)∣ ≤ L4
for all η ∈ N ∪ N0 .
Overall, we know with probability at least 1 - mexp(-Ω(d)) - O(t + m) exp(-Ω(m)), for all
η ∈ [1/L,n],
IFTbT(η) - FTbT(η)∣
0	0	00
≤∣FTbT(η) - FTbT(η )1 + IFTbT(η) - FTbT(η )1 + IFTbT(η) - FTbT(η )1
<23σ2 V σ2
where η0 = arg minη00∈N∪N0,η00≤η(η - η00). We use the fact that L = 100 in the last inequality.
B.2.4 Proofs of Technical Lemmas
Proof of Lemma 1. Recall that Xtrain is an n× d matix with n = cd where c ∈ [1/4, 3/4]. According
to Lemma 48, with probability at least 1 - 2 exp(-t2/2), we have
d— — √cd — t ≤ σi(Xtrain) ≤ √d + √cd +1,
for all i ∈ [n].
Since Htrain = 1∕nX>ainXtrain, We know λi(Htrain) = 1∕nσ2(Xtrai∏). Since C ∈ [4, 4], we have
Cd(√d + √cd)2 ≤ 100 - c0 and Cd(√d - √cd)2 ≥ 忐 + C, for some constant C. Therefore, we
know with probability at least 1 - exp(-Ω(d)),
100 ≤ λi (Htrain) ≤ 100,
for all i ∈ [n].
Similarly, since there exists constant C0 such that √d + √∑d ≤ (10 - c00)√d and √d - √∑d ≥
(1/10 + C00)√d, we know with probability at least 1 - exp(-Ω(d)),
10√d ≤ σi(Xtrain) ≤ 10√d,
for all i ∈ [n]. Choosing L = 100 finishes the proof.
Proof ofLemma 5. We prove that for any training set Strain, 1 {Eι ∩ E2 (η0)} ≥ 1 {Eι ∩ E2 (η0)} for
any η0 > η. This is trivially true if E1 is false on Strain . Therefore, we focus on the case when E1 holds
for Strain. Suppose ηC is the smallest step size such that the GD sequence gets truncated. Let {wτ0 ,ηc}
be the GD sequence without truncation. There must exists τ ≤ t such that IlwT,ηc∣∣ ≥ 4√Lσ∙ We
only need to prove that ∣∣wT,η ∣∣ ≥ 4√Lσ for any η ≥ ηc. We prove this by showing the derivative
of IIwτ0 ,η II2 in η is non-negative assuming∣∣wT,η∣∣2 ≥ 4√Lσ.
Recall the recursion of wτ0 as wτ0	= wtrain - (I - ηHtrain)τ wtrain. If τ is an odd number, it’s
τ,η τ,η
clear that 磊 ∣∣wT,η ∣∣ is non-negative at any η ≥ 0. From now on, we assume T is an even number.
Actually in this case,磊 ∣∣wT,η ∣∣2 Can be negative for some η. However, we can prove the derivative
must be non-negative assuming ∣∣wT,η∣∣2 ≥ 4√Lσ∙
25
Under review as a conference paper at ICLR 2021
Suppose the eigenvalue decomposition of Htrain is Pn=ι λiu%u> with λι ≥ •…λn. Denote Ci as
hwtrain, Uii. Let λj be the smallest eigenvalue such that(1 - ηλj) ≤ -1. This implies λi ≤ 2/n for
any i ≥ j + 1. We can write down wτ0 ,η 2 as follows
jn
UwT,n『=X(1-(i-ηλi)t)2c2 + X (ι-(i-ηλi)t)2C
i=1	i=j+1
j
≤ X (1 - (1 - ηλi)t) c2 + Ilwtraink2 ∙
i=1
Since E1 holds, we know kwtrain k2 ≤ 4Lσ2. Combining with UUwτ0,η UU2 ≥ 16Lσ2, we have
Pij=1 (1 - (1 - ηλi)t)2 ci2 ≥ 12Lσ2. We can lower bound the derivative as follows,
∂ kwτ,η k2
jn
X 2tλi(1 - ηλi)t 1 (1 - (1 - ηλi)t) c2 + X 2tλi(1 - ηλi)t 1 (1 - (1 - ηλi)t) c2
i=1	i=j+1
jn
≥2t Xλi(1-ηλi)tτ(1-(1-ηλi)t) Ci - 2t2 X Ci
i=1	η i=j+1
j
≥2t)： λi(1 — ηλi)t 1 (1 — (1 — ηλi)t) ci — 2t X 8Lσ2∕η.
i=1
Then, we only need to show that Pij=1 λi (1 - ηλi)t-1 (1 - (1 - ηλi)t) ci2 is larger than 8Lσ2 /η.
We have
X λi(I - ηλi)t-1 (1 - (1 - ηλi)t) c2 = X λi 1(- (1η-i),λ .)t (1 - (1 - ηλi)t)2 C
i=1	i=1	η i
X λ (nλi - 1)t-1
=i (ηλi-1)t-1
(1 - (1 - ηλi)t)2 C
X 1.(ηλ -1)t 1
=i (ηλi - 1)t - 1 ηλi - 1
(1 -(I- ηλ0t)2C
j1
≥ £ - (1 - (1 - ηλi)t) ci2 ≥ 12Lσ2∕η > 8Lσ2∕η.
i=1 η

Proof of Lemma 6. Similar as the analysis in Lemma 2, conditioning on E1, we know the GD
sequence never exceeds the norm threshold for any η ∈ [0, 2∕L]. This then implies
E2 kwt,η - wtrainkHtrain ɪ {E1 ∩ EiS) } = 0,
for all η ∈ [0, 2∕L].
Let {wτ0 ,η} be the GD sequence without truncation. For any step size η ∈ [2.5L, ∞], conditioning
on E1 , we have
llw0,ηll ≥ (InlL -I)t - 1) Ilwtraink ≥ (1.5t - 1)
where the last inequality holds as long as σ ≥ 5√L,t ≥ ci for some constant ci. Therefore, We
know when n ∈ [2.5L, ∞), 1 {Eι ∩Ei(n)} = 1 {Eι}. Then, we have for any n ≥ 2.5L,
E2 kwt,η - wtrainkHtrain
1 {E1 ∩Ei(n)} ≥21L (4√Lσ - 2√Lσ)i Pr[Eι] ≥ 2σi Pr[Eι] ≥
σi
L3,
26
Under review as a conference paper at ICLR 2021
where the last inequality uses Pr[Eι] ≥ 1 - exp(-Ω(d)) and assume d ≥ c4 for some constant c4.
Overall, we know E1 ∣∣wt,η - WtrainllHtraiII ɪ {Eι ∩ E2(η)} equals zero for all η ∈ [0,2/L] and is at
2
least L3 for all η ∈ [2.5L, ∞). By definition, we know η ∈ (1/L, 3L).	□
Proof of Lemma 7. Recall that FTbT(η) ：= ml Pm=I ∆τbτ(η,Pk). We prove that each
∆TbT(η, Pk) is O(1)-subexponential. We can further write ∆TbT (η, Pk) as follows,
∆τbτ (η,pk)=1 M)- Wk -(Xtrkin)喘"∣H(k)
≤2 llw(kη)- wk∣l IIHtrkinll+2n llξtrai)nll+ M)-W"I (√n 懵"I) (√n Mnl∣).
We can write ^Htrkinl as 02^(√nXtrkn). According to Lemma 47, we know σmaχ(Xtrkin)-
Eσmaχ(Xtrkin) is O(I)-SUbgaUSsian, which implies that σmaχ(√1nXtrkin) - Eσmaχ(√1nXtrkin) is
O(1∕√d)-subgaussian. Since Eσmaχ(√nXtrkin) is a constant, we know σmaχ(√1nXtrkin) is O(1)-
subgaussian and σmax(√nXtrkin) is O(1)-subexponential. Similarly,
we know both
(√1n Mi)J) (√1n [就)』)
are O(1)-subexponential.
21n llξ(-ki)∏f and
Suppose σ is a constant, we know llWt(,kη) - Wkk ll is upper bounded by a constant. Then, we
know ∆T bT (η, Pk) is O(1)-subexponential. Therefore, FT bT (η) is the average of m i.i.d. O(1)-
subexponential random variables. By standard concentration inequality, we know for any 1 > > 0,
with probability at least 1 - exp(-Ω(e2m)),
I FTbT(η) - Fτbτ(η)∣ ≤ e.
□
B.3 Train-by-validation (GD)
In this section, we show that the optimal step size under FTbV is Θ(1∕t). Furthermore, we show un-
der this optimal step size, GD sequence makes constant progress towards the ground truth. Precisely,
we prove the following theorem.
Theorem 8. Let the meta objective FTbV (n1,n2) (η) be as defined in Equation 2 with n1,n2 ∈
[d∕4, 3d∕4]. Assume noise level σ is a large constant c1. Assume unroll length t ≥ c2, number of
training tasks m ≥ c3 and dimension d ≥ c4 log(t) for certain constants c2, c3, c4. With probability
at least 0.99 in the sampling of training tasks, we have
ηValid = θ(1∕t)andEllwt,ηValid- w*∣∣2 = l∣w*k2 - ω(1)
for all ηvValid ∈ arg minη≥0 FT bV (n1,n2)(η), where the expectation is taken over new tasks.
In this section, we still use L to denote constant 100. We start from analyzing the behavior of the
population meta-objective FTbV for step sizes within [0, 1∕L]. We show the optimal step size within
this range is Θ(1∕t) and GD sequence moves towards WV under the optimal step size. This serves
as step 1 in Section B.1 We defer the proof of Lemma 10 into Section B.3.1.
Lemma 10. Suppose noise level σ is a large enough constant c1. Assume unroll length t ≥ c2 and
dimension d ≥ c4 for some constants c2, c4. There exist η1, η2, η3 = Θ(1∕t) with η1 < η2 < η3
such that
FTbV(η2) ≤ 2 llw*k2 -10C + σ^
FTbV(η) ≥ 2 l∣w*k2 - 10C + 号,∀η ∈ [0,ηι] ∪ 呐,1/L]
where C is a positive constant.
27
Under review as a conference paper at ICLR 2021
-1 '	1 , ,IFI	♦	i' T-1	. .Λ 1 1	CR	,1 Cll ♦	1 ∙
To relate the behavior of FTbV to the behavior of FTbV , we prove the following generalization
result for step sizes in [0, 1/L]. This serves as step 3 in Section B.1. The proof is deferred into
Section B.3.2.
Lemma 11. For any 1 > > 0, assume d ≥ c4 log(1/) for some constant c4. With probability at
least 1 一 O(1∕e) exp(-Ω(e2m)),
lFTbV(η) 一 FTbV ⑺I ≤ 3
for all η ∈ [0, 1/L].
In Lemma 12, we show the empirical meta objective FTbV is high for all step size larger than 1/L,
which then implies 九加 ∈ [0,1/L]. This serves as step 2 in Section B.1. We prove this lemma in
Section B.3.3.
Lemma 12. Suppose σ is a large constant. Assume t ≥ c2, d ≥ c4 log(t) for some constants c2, c4.
With probability at least 1 — exp(-Ω(m)),
FTbV(η) ≥C0σ2 + 2σ2,
for all η ≥ 1/L, where C0 is a positive constant independent with σ.
Combining Lemma 10, Lemma 11 and Lemma 12, we give the proof of Theorem 8.
Proof of Theorem 8. According to Lemma 10, we know as long as d and t are larger than certain
constants, there exists ηι, η2,η3 = Θ(1∕t) with ηι < η < η3 such that
FTbv(η2) ≤ 2 l∣w"∣2 - 10C + σ2∕2
FTbv(η) ≥ 1 ∣∣w* k2 - 160C + σ2∕2,∀η ∈ [0,ηι] ∪ 呐,1/L],
for some positive constant C.
Choosing = min(1, C∕10) in Lemma 11, we know as long as d is larger than certain constant,
with probability at least 1 - exp(-Ω(m)),
IFrbV (η) - FTbv (η)∣ ≤ c∕10,
for all η ∈ [0, 1∕L].
Therefore,
FTbv(W ≤ 2 l∣w* ∣2 - 10C + σ2∕2
17
FTbv(η) ≥ 2 Ilw Il - 10C + σ ∕2,∀η ∈ [0,ηι] ∪ 哂,1∕L]∙
By Lemma 12, we know as long as t ≥ c2, d ≥ c4 log(t) for some constants c2, c4, with probability
at least 1 - exp(-Ω(m)),
FTbV(η) ≥ C0σ2 + 2σ2,
for all η ≥ 1∕L. As long as σ ≥ 1∕√C7, We have FTbV(η) ≥ 1 + 2σ2 for all η ≥ 1∕L. Combining
withFTbV(η2) ≤ 2 ∣∣w*∣∣2-180C+σ2∕2,weknow宿加 ∈ [0,1∕L]. Furthermore, sinceFTbV(η) ≥
2 l∣w*∣2 - 170C + σ2∕2,∀η ∈ [0,ηι] ∪ [η3,1∕L],wehaveηι ≤ 宿加 ≤ η.
Recall that ηι, η3 = Θ(1∕t), we know ^^^=Θ(1∕t). At the optimal step size, we have
17
FTbV (ηvalid) ≤ FTbV (ηvalid) + C/10 ≤ FTbV(η2) + C/10 ≤ 2 Ilw Il - J0C + σ ∕2∙
SinCe FTbV(ηaIid) = E2 ∣∣wt,η%id - w*∣∣2 + σ2∕2, we have
E^wt,η:alid-w*∣∣2≤lw*『-7C.
Choosing m to be at least certain constant, this holds with probability at least 0.99.
28
Under review as a conference paper at ICLR 2021
B.3.1 B EHAVIOR OF FTbV FOR η ∈ [0, 1/L]
In this section, we study the behavior of FTbV when η ∈ [0, 1/L]. We prove the following Lemma.
Lemma 10. Suppose noise level σ is a large enough constant c1. Assume unroll length t ≥ c2 and
dimension d ≥ c4 for some constants c2,c4. There exist η1,η2,η3 = Θ(1∕t) with ηι < η < η3
such that
FTbV(η2) ≤ 2 llw*k2 - 10C + 工
FTbV(η) ≥ 2 llw*k2 - 10C + -2-,∀η ∈ [0,ηι] ∪ [η3,1/L]
where C is a positive constant.
It,s not hard to verify that FTbV (η) = E1/2 ∣wt,η - w*∣∣2 +σ2∕2. For convenience, denote Q(η):=
1/2 ∣∣wt,η 一 w*∣2. In order to prove Lemma 10, we only need to show that EQ(小) ≤ 2 iiw* k2 -
190C and EQ(η) ≥ 2 ∣∣w*∣2 一 余C for all η ∈ [0, ηι] ∪ [η3,1/L]. In Lemma 13, We first show that
this happens with high probability over the sampling of tasks.
Lemma 13. Suppose noise level σ is a large enough constant c1. Assume unroll length t ≥ c2 for
certain constant c2. Then, with probability at least 1 — exp(一Ω(d)) over the SamPlingoftaSkS,there
exists η1,η2,η3 = Θ(1∕t) with ηι < η < η3 such that
Q(η2) :=2 kwt,η2 — w*k2 ≤ 1 kw*k2- C
Qm) := 1 kwt,η 一 w*k2 ≥ 2 kw*k2 一 C2,∀η ∈ [0,ηι] ∪ [η3,1/L]
where C is a positive constant.
Since we are in the small step size regime, we know the GD sequence converges with high proba-
bility and will not be truncated. For now, let's assume wt,η = Bt,ηWtrain + Bt,η (Xtrain)*ξtrain, where
Bt,η = I 一 (I 一 ηHtrain)t. We have
Qm)=I IIBt
=2 ∣Bt,
,η wtrain
,η wtrain
+ Bt,η(Xtrain)*ξtrain — w* ||
一 w*k2 + 2 ||Bt,n (Xtrain 户 ξtrain∣∣
+ <Bt,η Wtrain 一 W*,Bt,η (Xtrain)* ξtrain)
2 ∣∣w*k2 + 2 kBt,ηWtraink2 + 2 lIBt,η(Xtrain)*ξtrain11	一(6mWtrain,w*i
+ Bt,η Wtrain 一 W , Bt,η (Xtrain) ξtrain	.
In Lemma 14, we show that with high probability the crossing term
(Bt,n Wtrain — W*, Bt,η (Xtrain) * Strain〉is negligible for all η ∈ [0,1/L]. By HOeffding,s in-
equality, we know the crossing term is small for any fixed η. Constructing an -net for the crossing
term in η, we can take a union bound and show it,s small for all η ∈ [0, 1/L]. We defer the proof of
Lemma 14 to Section B.3.4.
Lemma 14. Assume σ is a constant. For any 1 >	> 0, we know with probability at least
1 一 O(1∕e) expjΩ(c2d)),
Bt,η Wt*rain 一 W* , Bt,η (Xtrain ) ξtrain 〉 ≤ ,
for all η ∈ [0, 1/L].
Denote
G(η) := 2 ∣W*k2 + 1 ∣Bt,η Wtraink2 + 2 I∣Bt,η (Xtrain )*ξtrain∣2 TBt,ηWtrain, W* i ∙
Choosing = C/4 in Lemma 14, we only need to show G(η2) ≤ ∣W* ∣2 一 5C/4 and G(η) ≥
∣W*∣2 一 C/4 for all η ∈ [0,η1] ∪ [η3, 1/L].
29
Under review as a conference paper at ICLR 2021
We first show that there exists η = Θ(1∕t) such that G(η2) ≤ 2 ∣∣w* k2 - 5C/4 for some constant
C. It,s not hard to show that 2 ∣∣Bt,ηWtrain∣∣2 + 1 ∣∣Bt,η(Xtrain)tξtrain∣∣ = O(η2t2σ2). In Lemma 15,
We show that the improvement hBt,ηWtrain, w*i = Ω(ηt) is linear in η. Therefore there exists η =
Θ(1∕t) such that G(η2) ≤ 1 ∣∣w*∣2 - 5C/4 for some constant C. We defer the proof of Lemma 15
to Section B.3.4.
Lemma 15. For any fixed η ∈ [0, L∕t] with probability at least 1 — exp(-Ω(d)),
hBt,η Wtrain,
w*〉〉L
i ≥ 16L
To lower bound G(η) for small η, we notice
G(η)〉1 kw*k2 - hBt,ηWtrain,w*i .
We can show that hBt,ηWtrain,w*i = O(ηt). Therefore, there exists ηι = Θ(1∕t) such that
hBt,ηWt*rain, W*i ≤ C/4 for all η ∈ [0, η1].
To lower bound G(η) for large η, we lower bound G(η) using the noise square term,
12
G(n)〉2 ∣∣Bt,η(Xtrain) ξtrain∣∣ .
We show that with high probability ∣∣Bt,η(Xtrain)tξtrain∣∣2 = Ω(σ2) for all η ∈ [log(2)L∕t, 1∕L].
Therefore, as long as σ is larger than some constant, there exists η3 = Θ(1∕t) such that G(η)〉
1 ∣∣W*k2 forallη ∈ [η3,1∕L].
Combing Lemma 14 and Lemma 15, we give a complete proof for Lemma 13.
Proof of Lemma 13. Recall that
Q(η) =1 kBt,ηWtrain- w*∣2 + 1 ∣∣Bt,η(Xtrain)tξtrain∣∣2
+ Bt,ηWt*rain - W*, Bt,η(Xtrain)tξtrain
=G(η) + Bt,η Wt*rain - W*, Bt,η (Xtrain)t ξtrain
We first show that with probability at least 1 - exp(-Ω(d)), there exist ηι, η2,η3 = Θ(1∕t) with
η1 < η2 < η3 such that G(η2) ≤ 1∕2 ∣W* ∣2 - 5C∕4 and G(η) 〉 1∕2 ∣W* ∣2 - C∕4 for all
η ∈ [0,η1] ∪ [η3, 1∕L].
According to Lemma 1, we know with probability at least 1 - exp(-Ω(d)), √d∕√L ≤ σi (Xtrain) ≤
√Ld and 1∕L ≤ λi(Htrain) ≤ L for all i ∈ [n] with L = 100.
Upper bounding G(η2): We can expand G(η) as follows:
G(n) : = 2 kBt,ηWtrain- w*∣2 + 2 ∣∣Bt,η(Xtrain)tξtrain∣∣
=2 llW*k2 + 2 kBt,ηWtraink2 + 2 HBt，n(Xtrain)tξtrain∣∣ -hBt,ηWtrain,W*i ∙
Recall that Bt,η = I - (I - ηHtrain)t, for any vector W in the span of Htrain,
∣∣Bt,ηWk = ∣∣ (I - (I - ηHtrain)t) w∣∣ ≤ Lnt ∣∣W∣∣ .
According to Lemma 45, we know with probability at least 1 - exp(-Ω(d)), ∣ξtrain∣∣ ≤ √dσ.
Therefore, we have
2 l∣Bt,ηWtrain∣2 + 2 ∣∣Bt,η(Xtrain)tξtrain∣∣2 ≤ L2n2t2∕2 + L3n2t2σ2∕2 ≤ L3n2t2σ2,
where the second inequality uses σ, L 〉 1. According to Lemma 15, for any fixed η ∈ [0, L∕t], with
probability at least 1 - exp(-Ω(d)), hBt,ηWtrain,W*i〉16L. Therefore,
G(n) ≤ 1 IIw"2 + L3n2t2b2 - τnt- ≤ 1 |依*『-备,
2	16L	2	32L
30
Under review as a conference paper at ICLR 2021
where the second inequality holds as long as η ≤ 32^2=. Choosing η := 32L4σ2t, We have
G(η2) ≤ 1 kw*k2 - 1024L5σ2 = 2 "A 5C
where C = 819 2Zσ2 ∙ Note C is a constant as σ, L are constants.
Lower bounding G(η) for η ∈ [0,ηι] : Now, we prove that there exists ηι = Θ(1∕t) with
ηι < η2 such that for any η ∈ [0,ηι], G(η) ≥ ɪ ∣∣w*∣∣2 - C. Recall that
G(η) =1 kw*k2 + 1 kBt,η Wtraink2 + 2 ∖∖Bt,n (Xtrain)&ain『-hBt,η Wtrain,w*i .
≥1 ι∣w* k2 - hBt,η wtrai∏,w*i.
Since ∣hBt,ηWtrain,w*i∣ ≤ Lηt, we know for any η ∈ [0,ηι],
G㈤ ≥ 2 l∣w*k2 - Lηιt.
Choosing ηι =缶,we have for any η ∈ [0, ”1],
G((I) ≥ 1 kw*k2- C.
Lower bounding G(η) for η ∈ [η3, 1/L]:
η3 > η2 such that for all η ∈ [η3 , 1/L],
Now, we prove that there exists ”3 = Θ(1∕t) with
G(η) ≥
2 kw*k2- C.
Recall that
G(”)= 1 kBt,η wtrain
-w*∣∣2 + 2 ∣lBt,η (Xtrain) *ξtrain 11
≥ 2 IIBt,η (Xtrain)*ξtrainIl	.
According to Lemma 45, we know with probability at least 1 - exp(-Ω(d)), 2√√2 ≤ ∣∣ξtrain∣∣.
Therefore,
l∣Bt,η (Xtrain户ξtrain1∣ ≥
2
σ
1-
σ2
8L ≥ 32L,
where the last inequality assumes ” ≥ log(2)L/t. As long as t ≥ log(2)L2 , we have log(2)L/t ≤
1/L. Choosing ”3 = log(2)L/t, we know for all ” ∈ [”3, 1/L],
G(n) ≥ l ||Bt,n(Xtrain)*ξtrain ∣∣	≥ 7Γ7j- .
2	64L
Note that 2 ∣∣w*∣2 = 1/2. Therefore, as long as σ ≥ 8√L, we have
G(η) ≥ 1 kw*k2
for all ” ∈ [”3, 1/L].
Overall, we have shown that there exist ”1, n2, ”3 = Θ(1∕t) with ”1 < ”2 < n3 such that G(η2) ≤
1/2 ∣∣w*∣2 - 5C/4 and G(”)≥ 1/2 ∣∣w*∣2 - C/4 for all ” ∈ [0, m] ∪ [”3,1/L]. Recall that
Q(”) = G(”) +〈Bt,nWtrain - w*,Bt,η(Xtrain)tStrain). Choosing E = C/4 in Lemma 14, we know
with probability at least 1 - exp(-Ω(d)), ∖(Bt,ηw1^∏ - w*,Bt,η(Xtrain)tξtrain)∣ ≤ C/4 for all
” ∈ [0,1/L]. Therefore, we know Q(”2) ≤ 1/2 ∣∣w*∣2 - C and Q(”)≥ 1/2 ∣∣w*∣2 - C/2 for all
” ∈ [0,”1] ∪ [”3, 1/L].
Next, we give the proof of Lemma 10.
31
Under review as a conference paper at ICLR 2021
Proof of Lemma 10. Recall that FTbV(η) = E1/2 ∣∣wt,η - w*∣∣2 + σ2. For convenience, denote
Q(η) := 1/2 ∣∣wt,η - w*∣2. In order to prove Lemma 10, we only need to show that EQ(η2) ≤
2 llw* l∣2 - 190C and EQ(η) ≥ 1 ∣∣w* ∣∣2 - 10C for all η ∈ [0, ηι] ∪ [η3,1/L].
According to Lemma 13, as long as σ is a large enough constant c1 and t is at least certain constant
c2, with probability at least 1 - exp(-Ω(d)) over the sampling of Strain, there exists ηι, η2,η3 =
Θ(1∕t) with ηι < η2 < η3 such that
Q(η2) :=1/2 kwt,n2 - w*k2 ≤ 2 ∣∣w*k2- C
Q(η) := 1/2 kwt,n - w*k2 ≥ 2 kw*k2 - C2, ∀η ∈ [0,ηι] ∪ [η3,1/L]
where C is a positive constant. Call this event E . Suppose the probability that E happens is 1 - δ .
We can write EQ(η) as follows,
EQ(η) = E[Q(η)∣E ] Pr[E ]+ E[Q(η)l司 PM∙
According to the algorithm, We know ∣∣wt,n ∣∣ is always bounded by 4√Lσ. Therefore, Q(η):
1/2 ∣∣wt,n — w*∣∣2 ≤ 13Lσ2. When η = η2, we have
EQ(η2) ≤ ɑ ∣w*∣2 - C) (1 - δ) + 13Lσ2δ
=1 ∣w*∣2 - δ - C +(C + 13Lσ2)δ
≤ 1 ∣∣w*∣∣2-丝，
-2 11	11	10 ,
where the last inequality assumes δ ≤ 10c+*0Lσ2.
When η ∈ [0, η1] ∪ [η3, 1/L], we have
EQ(η2) ≥	2 ∣w*∣2		-C) (1 - δ) - 13Lσ2δ
1 =— 2	∣∣w*	∣2-	2 - (1 - δ)-2 - 13Lσ2δ
1 ≥- -2	∣∣w*	∣2-	C E -(1/2 + 13Lb2)S
1	∣∣w*	∣2-	6C
≥ —			元,
-2			
C
5C+130Lσ2 .
where the last inequality holds as long as δ ≤
According to Lemma 13, we know δ ≤ exp(-Ω(d)). Therefore, the conditions for δ can be satisfied
as long as d is larger than certain constant.
B.3.2	GENERALIZATION FOR η ∈ [0, 1/L]
In this section, we show FTbV is point-wise close to FTbV for all η ∈ [0, 1/L]. Recall Lemma 11 as
follows.
Lemma 11. For any 1 > > 0, assume d ≥ c4 log(1/) for some constant c4. With probability at
least 1 — O(1/e) exp(-Ω(e2m)),
IFTbV(η) - FTbV(η)∣ ≤ 3
for all η ∈ [0, 1/L].
In order to prove Lemma 11, let’s first show that for a fixed η with high probability FTbV (η) is close
to FTbV (η). Similar as in Lemma 7, we show each ∆TbV (η, Pk) is O(1)-subexponential. We defer
its proof to Section B.3.4.
32
Under review as a conference paper at ICLR 2021
Lemma 16. Suppose σ is a constant. For any fixed η ∈ [0, 1/L] and any 1 > > 0, with probability
at least 1 一 exp(-Ω(e2m)),
回bv(η) - FTbV(η)∣ ≤ e.
Next, we show that there exists an -net for FTbV with size O(1/). By -net, we mean there exists
a finite set Ne of step size such that ∣FTbv(η) 一 FTbV(η0)∣ ≤ E for any η ∈ [0,1/L] and η0 ∈
argmi□η∈Ne ∣η — η0∣. We defer the proof of Lemma 17 to Section B.3.4.
Lemma 17. Suppose σ is a constant. For any 1 > E > 0, assume d ≥ c4 log(1/E) for constant c4.
There exists an E-net Ne for FTbV with |Ne| = O(1/E). That means, for any η ∈ [0, 1/L],
∣FTbv(η) - FTbV(η0)∣ ≤ e,
for η0 ∈ argmi□η∈Ne ∣η — η0∣.
Next, we show that with high probability, there also exists an E-net for FTbV with size O(1/E).
Lemma 18. Suppose σ is a constant. For any 1 > E > 0, assume d ≥ c4 log(1/E) for constant c4.
With probability at least 1 — exp(—Ω(e2m)), there exists an e-net N for FTbV With |Ne| = O(1∕e).
That means, for any η ∈ [0, 1/L],
0
|FT bV (η) — FTbV (η )| ≤ E,
for η0 ∈ argminη∈M ∣η 一 η0∣.
Combing Lemma 16, Lemma 17 and Lemma 18, now we give the proof of Lemma 11.
Proof of Lemma 11. The proof is very similar as in Lemma 4. By Lemma 16, we know with
probability at least 1 — exp(—Ω(e2m)), IFrbV(η) — FTbV(η)∣ ≤ E for any fixed η. By Lemma 17
and Lemma 18, we know as long as d = Ω(log(1∕e)), with probability at least 1 — exp(—Ω(e2m)),
there exists E-net Ne and Ne0 for FTbV and FTbV respectively. Here, both of Ne and Ne0 have size
O(1/E). According to the proofs of Lemma 17 and Lemma 18, it’s not hard to verify that Ne ∪ Ne0
is still an E-net for FTbV and FTbV. That means, for any η ∈ [0, 1/L], we have
IFTbV(η) - FTbV(ηO)I, IFTbV(η) - FTbV(n0)| ≤E,
for η0 ∈ arg minη∈N∪N0 Iη — η0I.
Taking a union bound over Ne ∪ N[, We have with probability at least 1 — O(1∕e) exp(—Ω(E2m)),
IFTbV(η) - FTbV(η)∣ ≤ E
for any η ∈ Ne ∪ Ne0 .
Overall, we know with probability at least 1 — O(1∕e) exp(—Ω(E2m)), for all η ∈ [0,1/L],
IFTbV(η) — FTbV (η)I
≤lFTbV(η) - FTbV(η )| + IFTbV(η) - FTbV(η )| + IFTbV(η) - FTbV(η )|
≤3E,
where η0 ∈ arg minη∈N ∪N0 Iη — η0I. Changing E to E0/3 finishes the proof.
B.3.3	LOWER BOUNDING FTbV FOR η ∈ [1/L, ∞)
In this section, we prove FTbV is large for any step size η ≥ 1/L. Therefore, the optimal step size
η*alid must be smaller than FrbV.
Lemma 12. Suppose σ is a large constant. Assume t ≥ c2, d ≥ c4 log(t) for some constants c2, c4.
With probability at least 1 — exp(—Ω(m)),
FTbV(η) ≥C0σ2 + 2σ2,
for all η ≥ 1/L, where C0 is a positive constant independent with σ.
33
Under review as a conference paper at ICLR 2021
When the step size is very large (larger than 3L), we know the GD sequence gets truncated with
high probability, which immediately implies the loss is high. The proof of Lemma 19 is deferred
into Section B.3.4.
Lemma 19. Assume t ≥ c2 , d ≥ c4 for some constants c2 , c4 . With probability at least
1 — exp(一Ω(m)),
FTbV (η) ≥ σ ,
for all η ∈ [3L, ∞)
The case for step size within [1/L, 3L] requires more efforts. We give the proof of Lemma 20 in this
section later.
Lemma 20. Suppose σ is a large constant. Assume t ≥ c2, d ≥ c4 log(t) for some constants c2, c4.
With probability at least 1 一 exp(-Ω(m)),
户TbV(η) ≥C4σ2 + 2σ2,
for all η ∈ [1/L, 3L], where C4 is a positive constant independent with σ.
With the above two lemmas, Lemma 12 is just a combination of them.
Proof of Lemma 12. The result follows by taking a union bound and choosing C0 = min(C4, 1/2).
In the remaining of this section, we give the proof of Lemma 20. When the step size is between 1/L
and 3L, if the GD sequence has a reasonable probability of diverging, we can still show the loss is
high similar as before. If not, we need to show the GD sequence overfits the noise in the training
set, which incurs a high loss.
Recall that the noise term is roughly 1 ∣∣(I - (I - ηHtrain)t)(Xtrain)^ξtrain112. When η ∈ [1/L, 3L],
the eigenvalues of I - ηHtrain in Strain subspace can be negative. If all the non-zero n eigenvalues
of Htrain have the same value, there exists a step size such that the eigenvalues of I - ηHtrain in
subspace Strain is -1. If t is even, the eigenvalues of I - (I - ηHtrain)t in Strain subspace are zero,
which means GD sequence does not catch any noise in Strain .
Notice that the above problematic case cannot happen when the eigenvalues of Htrain are spread out.
Basically, when there are two different eigenvalues, there won’t exist any large η that can cancel
both directions at the same time. In Lemma 21, we show with constant probability, the eigenvalues
of Htrain are indeed spread out. The proof is deferred into Section B.3.4.
Lemma 21. Let the top n eigenvalues of H才m be λι ≥ ∙∙∙ ≥ λn. Assume dimension d ≥ c4 for
certain constant c4. There exist positive constants μ, μ0, μ00 such that with probability at least μ,
λμ0n - λn—μ0n+1 ≥ μ ∙
Next, we utilize this variance in eigenvalues to prove that the GD sequence has to learn a constant
fraction of the noise in training set.
Lemma 22. Suppose noise level σ is a large enough constant c1. Assume unroll length t ≥ c2 and
dimension d ≥ c4 for some constants c2, c4. Then, with probability at least C1
kBt,ηWtrain - w*kHtrain ≥ C2σ2,
for all η ∈ [1/L, 3L], where C1, C2 are positive constants.
Proof of Lemma 22. Let Ei be the event that √d/√L ≤ σ% (Xtrain) ≤ √Ld and 1/L ≤ λ% (Htrain) ≤
L for all i ∈ [n] and JdQ/4 ≤ ∣∣ξtrain∣∣ ≤ √dσ. Let E3 be the event that √d∕√L ≤ σi(Xvalid) ≤
√Ld and 1/L ≤ λi(Hvalid) ≤ L for all i ∈ [n] and √dσ∕4 ≤ kξvalidk ≤ √dσ. According to
Lemma 1 and Lemma 45, We know both Ei and E3 hold with probability at least 1 - exp(-Ω(d)).
Let the top n eigenvalues of Htrain be λι ≥ ∙∙∙ ≥ λn. According to Lemma 21, assuming d is larger
than certain constant, we know there exist positive constants μι,μ2, μ3 such that with probability at
least μ1, λμ2n - λn-μ2n+l ≥ μ3. Call this event E2 .
34
Under review as a conference paper at ICLR 2021
Let Si and S? be the span of the bottom and top μ2n eigenvectors of Htrain respectively. Ac-
cording to Lemma 45, we know kξtrain k ≥ √4d σ with probability at least 1 - exp(-Ω(d)).
Let Pi ∈ Rn×n be a rank-μ2n projection matrix such that the column span of (Xtrain)*Pι
is Si. By Johnson-Lindenstrauss Lemma, We know with probability at least 1 - exp(-Ω(d)),
IlPrOjPl ξtrai∏∣∣ ≥ ʌɪ2 ∣∣ξtrain∣∣ ∙ Taking a union bound, with probability at least 1 - exp(-Ω(d)),
IlPrOjPl ξtrain∣∣ ≥ vμ2dσ. Similarly, we can define P? for the S? subspace and show with probability
at least 1 - exp(-Ω(d)), ∣∣Projp2 ξtrain∣∣ ≥ μf2dσσ. Call the intersection of both events as E4, which
happens with with probability at least 1 - exp(-Ω(d)).
Taking a union bound, we know Ei ∩ E2 ∩E3 ∩ E4 holds with probability at least μι∕2 as long as d
is larger than certain constant. Through the proof, we assume Ei ∩ E? ∩ E3 ∩ E4 holds.
Let,s first lower bound ∣∣Bt,ηWtrain - WtrainIl as follows,
IIBt,ηWtrain- Wtraink = ∣∣Bt,η (Wtrain + (Xtrain),ξtrain) - Wtrain∣∣
≥ (UBt，n (Wtrain + (xtrain)^ξtrain) ∣∣ - 1)
Recall that we define Si and S? as the span of the bottom and top μ2n eigenvectors of Htrain re-
spectively. We rely on Si to lower bound ∣∣Wt,η - w*∣ when η is small and rely on S? when η is
large.
Case 1: Let σmS1in (Bt,η) be the smallest singular value of Bt,η within Si subspace. If
ηλn-μ2n+i ≤ 2 - μ3∕(2L), we have
σmS1in(Bt,η ) ≥ min
where the second inequality assumes t ≥ max(L2, 2L∕μ3) log 2. Then, we have
llWt,n - W*k ≥ (σSl1in(Bt,η) (∣∣ProjSi (Xtrain)*ξtrain∣∣ - 1) - 1)
√μσ
≥ -----；=,
_ 32√L
where the second inequality uses ∣∣ Projpi ξtrain∣∣ ≥ ^μ2dσ and the last inequality assumes σ ≥ 4√μL.
Case 2： If ηλn-μ2n+i > 2 - μ∣∕(2L), we have 历2” ≥ 2 + μ∣∕(2L) since λμ2n -入”…n+i ≥
μ3 and η ≥ 1∕L. Let σS2n(Bt,η) be the smallest singular value of Bt,” within S? subspace. We
have
where the last inequality
kWt，n - W*k ≥ S.
Therefore, we have
%(Bt，n) ≥ (( + Il) - 1) ≥ 2,
assumes t ≥ 4L∕μ∣. Then, similar as in Case 1, we can also prove
∣∣Bt,ηWtrain- W*kHtrain
IIBt,ηWtrain - WtrainkHtrain ≥ L ∣∣Bt,ηWtrain - Wtraink	≥ 1024L? ,
for all η ∈ [1∕L, 3L]. We denote Ci := μi∕2 and C?=[。,力?.	□
Before we present the proof of Lemma 20, we still need a technical lemma that shows the noise in
Svalid concentrates at its mean. The proof of Lemma 23 is deferred into Section B.3.4.
Lemma 23. Suppose σ is constant. For any 1 >	> 0, with probability at least 1 -
O(t∕E)exp(-Ω(E2d)), λn(Hvalid) ≥ 1∕L and
kWt，n - wvalidIlHvalid ≥ IWt，n - w*∣Hvalid + (1 - e)σ?,
for all η ∈ [1∕L, 3L].
35
Under review as a conference paper at ICLR 2021
Combing the above lemmas, we give the proof of Lemma 20.
Proof of Lemma 20. According to Lemma 23, we know given 1 > > 0, with probability at least
1 — O(t∕E) exp(—ω(e2d)), λn(Hvalid) ≥ 1/L and kwt,η — wvalid11^加 ≥ kwt,η — w* kHvaiid + (1 —
)σ2 for all η ∈ [1∕L, 3L]. Call this event E1 . Suppose Pr[E1] ≥ 1 — δ∕2, where δ will be specifies
later. For each training set St(rkai)n, we also define E1(k) . By concentration, we know with probability
at least 1 — exp(—C(62m)), 1/mPm=I 1 {E(k)} ≥ 1 — δ.
According to Lemma 22, we know there exist constants C1 , C2 such that with probability at least
C1,
∣∣Bt,ηwtrain — w*kHtiaia ≥ C2σ2 for all η ∈ [1/L, 3L]. Call this event E2. For each training set
Strkin, We also define e2" By concentration, We know with probability at least 1 — exp(—C(m)),
1/mP乙 1 {E2k)} ≥ Cι∕2.
1 -	.	♦	一 J/ TCrl	1	F	1 ^τS	/	∖ CIl
For any step size η ∈ [1/L, 3L], we can lower bound FTbV(η) as follows,
1
FTbV (η)=—
m
(k)
wt,η
— wv(akl)id III2Hv(akli)d
(k)
wt,η
YdLvkid 1{E*}
1
≥ 一
m
1
≥ 一
m
(k)
wt,η
(k)
wt,η
—wfe∣∣ ɪ nE(k)o+2(1—e)(1—δ)σ2
Hvalid	2
1
≥ 一
m
-同IH ɪ nE(k) ∩E2k)o+2(1—e)(1—δ)σ2.
As long as δ ≤ C1∕4, we know m Pm=I 1 {E(k) ∩ E2k)} ≥ Cι∕4∙ Let E3(η) be the event that w(k
gets truncated with step size η. We have
1
m
(k)
wt,η
1
m
(k)
wt,η
m
+ɪ X 2 M)-w』Lɪ nE(k) ∩嘤 ∩E3k)0
k=1
If ml PLI l{E(fc) ∩ E2k) ∩ E3fc)} ≥ C1/8, we have
mm
ɪ X 1||w(k)-w*||	dE(fc)	∩E2k)0	≥ ɪ X 1I|w(k)-w*||	ME(fc) ∩E(k)	∩^3fc)0
m 2 ∣ t,η k ∣Hvalid	1	2	m 2 ∣ t,η k ∣Hvalid	1	2	3
k=1	k=1
C1	9σ2	9C1 σ2
≥ ^8^ × ^^2^ =	16 .
Here, we lower bound ∣∣wt(,kη) — wk* ∣∣	by 9σ2 when the sequence gets truncated.
If ml P乙 1 {E(fc) ∩ E2k) ∩ E3fc)} < Cl/8, we know mm P乙 1 {e* ∩ E(k) ∩ E(k)) ≥ C1/8.
Then, we have
ɪ X 1 M)-W*II：	1 {E(k) ∩ E2k0 ≥ɪ X 2 IMwtram- w*II：	1 {E(k) ∩ E2k) ∩ E3k)0
m 2	Hvalid	m 2	Hvalid
k=l	k=l
Cl	C2σ2	ClC2σ2
≥ ^8^ ×	2	= -16-
36
Under review as a conference paper at ICLR 2021
Letting C3 = min(噜,CIC2), We then have
FTbV(η) ≥ C3σ2 + 2(I - E)(I - δ)σ2 ≥ C； + 2σ2,
where the last inequality chooses δ = E = C3/2. In order for Pr[E1] ≥ 1 - δ∕2, we only need
d ≥ c4 log(t) for some constant c4. Replacing C3/2 by C4 finishes the proof.
B.3.4 Proofs of Technical Lemmas
Proof of Lemma 14. We first show that for a fixed η ∈ [0, 1/L], the crossing term
∣(Bt,ηWtrain — w*, Bt,η(Xtrain)^Strain)∣ is small with high probability. We can write down the Cross-
ing term as follows:
*
,η wtrain -
W*,Bt,η (Xtrain)'ξtrain> = <[(Xtrain)']>Bt,η (Bt,η wtrain -
Noticing that ξtrai∏ is independent with [(Xtrain户]>Bt,η(Bt,ηWtrain - w*), We will use Hoeffding's
inequality to bound ∣(Bt,ηWtrain - w*, Bt,η(Xtrain)^Strain)∣. According to Lemma 1, we know with
probability at least 1 - exp(-Ω(d)), √d∕√L ≤ σi(Xtrain) ≤ √Ld and 1/L ≤ λi(Htrain) ≤ L for
all i ∈ [n] with L = 100. Since η ≤ 1/L, we know kBt,η k = kI - (I - ηHtrain)t k ≤ 1. Therefore,
we have
∣∣[(Xtrain)t]>Bt,η(Bt,ηw*ain - w*)∣∣ ≤ 2√J,
for any η ∈ [0, 1/L]. Then, for any E > 0, by Hoeffding’s inequality, with probability at least
1 — exp(-Ω(e2d)),
∣(Bt,η Wtrain - W*, Bt,η (Xtrain),Strain)∣ ≤ E.
Next, we construct an E-net on η and show the crossing term is small for all η ∈ [0, 1/L]. Let
g(η) := Bt,η Wtrain - W , Bt,η (Xtrain) Strain .
We compute the derivative of g(η) as follows:
g0 (η) = tHtrain(I - ηHtrain)t-1
Wtrain, Bt,η(Xtrain),Strain
+ Bt,η Wtrain - W , tHtrain(I - ηHtrain)	(Xtrain) Strain
By Lemma 45, we know with probability at least 1 - exp(-Ω(d)), kξtraink ≤ √dσ. Therefore,
∣g0(η)∣ ≤ L1∙5t(1 - L)tτ σ + 2L1∙5t(1 - L) j σ = 3L1.5t(1 - L广 σ.
We can control ∣g0(η) | in different regimes:
•	For η ∈ [0,占],we have ∣g0(η)∣ ≤ 3L1.5tσ.
•	Given any 1 ≤ i ≤ log t - 1, for any η ∈ (当,(i+1)L], we have ∣g0(η)∣ ≤ 3LeFσ
•	For any η ∈ (Lt-gt, 1/L], we have ∣g0(η)∣ ≤ 3L1-5σ.
Fix any E > 0, we know there exists an E-net Ne with size
1	L log t-1 3L1.5tσ	1
|Ne1=E	t-1 X F- + (L
i=0
Logt) 3L1.5σ
—
1 3 3eL2.5tσ	r- 1	〜1、
≤ Eh-I- + 3√Lσ =O(E)
such that for any η ∈ [0,1/L], there exists η0 ∈ Ne with ∣g(η) - g(η0)| ≤ e. Note that L = 100 and
σ is a constant. Taking a union bound over N and all the other bad events, we have with probability
at least 1 - exp(-Ω(d)) - O(1/e) exp(-Ω(e2d)), for all η ∈ [0,1/L],
∣∣Bt,η Wt*rain - W*, Bt,η (Xtrain), Strain ∣∣ ≤ E + E = 2E.
37
Under review as a conference paper at ICLR 2021
As long as 1 > c > 0, this happens with probability at least 1 - O(1∕e) exp(-Ω(e2d)). Replacing
by 0 /2 finishes the proof.
Proof of Lemma 15. According to Lemma 1, We know with probability at least 1 - exp(-Ω(d)),
1/L ≤ λi (Htrain) ≤ L for all i ∈ [n] with L = 100. Wecan lower bound hBt,ηWtrain, w*i as follows,
hBt,ηwtrain, W i =〈(I - (I - ηHtrain) ) wtrain, Wtrain)
≥λmin (I - (I - ηHtrain)t) ||说疝『
≥ (1 - exp (-L)) kwtkink2 .
By Johnson-Lindenstrauss lemma (Lemma 49), we know with probability at least 1 -
2 exp(-c2d/4),
kwtraink ≥ 2(1-e)kw*k = 1(1-e)∙
Then, we know with probability at least 1 - 2 exp(-ce2d∕4) - exp(-Ω(d)),
hBt,ηwrain,w*i ≥ (1 -exP (-η∣) ) kwtkink2
≥(1-exp O )4(i2
Since ex ≤ 1 — X + x2∕2 for any X ≤ 0, we know exp(-ηt∕L) ≤ 1 — ηt∕L + η2t2/(2L2). For any
η ≤ L∕t, we have exp(-ηt∕L) ≤ 1 -ηt∕(2L). Then with probability at least 1 - 2exp(-ce2d∕4)-
exp(-Ω(d)),
hBt,nwtrain, w*i ≥ —界
4	2L
≥ 4
—16L,
where the second inequality holds by choosing = 1∕4.

Proof of Lemma 16. Recall that
m
1
FTbV(η) := 一〉J δTbV(η, Pk)
m k=1
For each individual loss function ∆TbV (η, Pk), we have
∆τbv(η,Pk)=1 Mk)-W* - (χV3)tξV3∣∣;(k)
2H
valid
=1 ∣∣w(k) - w*∣∣2	+ ɪ 2 + ∕w(k) - W* 1(X(k))>
=2 l|Wt，n	W ∣∣H(k) + 2n	+ \Wt，n W , n(XVaIid)
valid
25Lσ2	1
≤------- + —
-2	2n
We can write ∣∣HV3∣∣ as σ2Lx(√nXVk)d). ACCOrding to Lemma 47, we know σmaχ(χV3) -
Eσmaχ(XVk)d) is O(1)-SUbgaUSsian, which implies that σmaχ(√nXvkjld) - Eσmaχ(√nXVk)d) is
O(1∕√d)-subgaussian. Since Eσmaχ(√nXVk)d) is a constant, we know σmaχ(√nXVk)d) is O(1)-
subgaussian and σ2ιax( √nXVk)d) is O(1)-subexponential. Similarly, we know both 贵 ∣∣ξVklid∣∣ and
(√n ∣∣XVkd∣∣) (√n ∣∣ξVklid∣∣)
are O(1)-subexponential. This further implies that ∆TbV (η, Pk) is
38
Under review as a conference paper at ICLR 2021
八 / T ∖ F	. ∙	1 EI	i'	∙A	∙	. Λ	i' ∙ ∙	1	/ -I ∖	1	. ∙	1	1	♦
O(1)-subexponential. Therefore, FTbV is the average of m i.i.d. O(1)-subexponential random vari-
ables. By standard concentration inequality, we know for any 1 > > 0, with probability at least
1 — exp(-Ω(e2m)),
IFTbV(η) - FTbV(η)∣ ≤ e.
Proof of Lemma 17. Recall that
FTbV(η) =E2 ∣∣wt,η - w*∣∣2 + σ2∕2∙
We only need to construct an e-net for E1 ∣∣wt,η — w*∣∣2. Let E be the event that √d∕√L ≤
σi (Xtrain) ≤ √Ld and 1/L ≤ λi (Htrain) ≤ L for all i ∈ [n] and ∣∣ξtrai∏k ≤ √dσ. We have
E 1 kwt,η - w*k2 = E 2 kwt,η - w*k2lE Pr[E] + E 2 kwt,η - w*k2E Pr[E]
We first construct an e-net for E [l ∣∣wt,η - w*∣∣2 |e] Pr[E]. Let Q(η) := 1 ∣∣wt,η - w*∣∣2. Fix a
training set Strain under which event E holds. We show that Q(η) has desirable lipschitz property.
The derivative of Q(η) can be computed as follows,
Q0(η) = <tHtrain(I 一 ηHtrain)t-1Wtrain, Wt,η - W*) ∙
Conditioning on E , we have
IQ0(η)l = o(i)t(i - η)t-1.
L
Therefore, we have
∂ηE 2 kwt,η- w*k2∣E Pr[E]=。⑴Μ - ⅛一
Similar as in Lemma 14, for any e > 0, we know there exists an e-net N with size O(1∕e) such that
for any η ∈ [0, 1∕L],
E 1 kwt,η- w*k2 |E Pr[E] — E 1 llwt,η0 - w*k2 |E Pr[E] ≤ e
for 叶 ∈ argminη∈N"η - η0∣.
Suppose the probability of E is δ. We have
E
2 ∣wt,η- w*k2∣E[ Pr[E] ≤
25Lσ2
------δ ≤ e,
2 一 ,
where the last inequality assumes δ ≤ ?512. According to Lemma 1 and Lemma 45, We know
δ := Pr闾 ≤ exp(-Ω(d)). Therefore, given any e > 0, there exists constant c4 such that δ ≤ 25^
as long as d ≥ c4 log(1∕e).
Overall, for any e > 0, as long as d = Ω(log(1∕e)), there exists Ne with size O(1∕e) such that for
any η ∈ [0,1/L], ∣F⅛v(η) — FTbV(η0)∣ ≤ 3e for η0 ∈ argminη∈Ne ∣η — η0∣. Changing e to e0∕3
finishes the proof.
Proof of Lemma 18. For each k ∈ [m], let Ek be the event that √d∕ √L ≤ σi(Xtrkin) ≤ √Ld for
any i ∈ [n] and ξt(rkai)n ≤ √dσ. Then, we can write the empirical meta objective as follows,
1m	1m
FTbV ⑹=m X ATTgPd + m X XbTgPk) ι¾.
k=1
k=1
39
Under review as a conference paper at ICLR 2021
Similar as Lemma 17, we will show that the first term has desirable Lipschitz property and the
second term is small. Now, let's focus on the first term ml Pm=I ∆τbτ(η, Pk)IEk. Recall that
ATbTg pk)=2 Mk)-w(klid∣∣ (k)
2	Hvalid
=1	wtrki)n - w* - (Xvk)d)tξVklidllH(k).
Hvalid
Computing the derivative of ∆TbT (η, Pk ) in terms ofη, we have
d ʌ (rn D D — D rr(k) ( τ gH(k) ∖t-1"<k) H(k) Λ ,,(k)	?”* (T(k)、t Mk)、\
∂η ZbT (n，Pk ) = 'tHtrain(I - ηHtrain) Wtrain，Hvalid [wt,η - W - (XvaIid) ξvalid J /
Conditioning on Ek , we can bound the derivative,
∂XbT(η,pk) = O(1)t (1 - LY IM，』+(√d MMl) (√d 微Ml)).
Therefore, we have
mX ∂η*(η,pk)iEk = O(ι)t (1 - L)	mX(M)d|1 + (以|Xvk)d∣l)(左阳口).
SimiIar as in Lemma 16, We know boh ∣lHvkd∣l and (√ l∣χvk)dl∣) (√d l∣ξvklid∣D are
O(1)-SUbexPonentiaL Therefore, we know with probability at least 1 - exp(-Ω(m)),
m1 P乙(llHvk)d( + (√ l∣Xvkdl∣) (√ l∣ξ(klid∣∣)) = O(1). ThiS further shows that with Proba-
bility at least 1 - exp(-Ω(m)),
m X 齐TbTa)IEk=O(I)t(1-ɪ)t ι.
k=1
Similar as in Lemma 14, we can show that for any > 0, there exists an -net with size O(1/) for
m1 Pm=I δTbT (η, pk ) IEk .
Next, we show that the second term * Pm=I Δtbt(η, Pk)IEk is small with high probability. Ac-
cording to the Proof in Lemma 16, we know
∆TbT (η, Pk) = O(1)
Therefore, there exists constant C such that
~m X δTbT5, Pk) ι⅛ ≤ Cm X (∣∣Hv辅+d ^vklidn + (√d nXvk)jd (√d 宿川。)IEk.
It’s not hard to verify that (∣∣Hvk)d∣∣ + d ∣∣ξ(kid∣∣2 + (√d ∣∣Xvk)d∣∣) (√d ∣∣ξv3∣∣)) IEk is O(I)-
∣∣∣Hv(akli)d ∣∣∣
subexponential. Suppose the expectation of
+ d ∣∣ξvklid∣∣	+ (√ ∣∣Xvk)d∣∣) (√ ∣∣ξvkliid∣∣)
is μ, which is a constant. Suppose the probability of Ek be δ. We know the expectation of
IEk is μδ due to independence.
By
standard concentration inequality, for any 1 > e > 0, with probability at least 1 - exp(-Ω(e2m)),
IEk ≤ Cμδ+Ce ≤ (C +1)e,
where the second inequality assumes δ ≤ e∕(Cμ). By Lemma 1 and Lemma 45, we know δ ≤
exp(-Ω(d)). Therefore, as long as d ≥ c4 log(1∕e) for some constant c4, we have δ ≤ e∕(Cμ).
40
Under review as a conference paper at ICLR 2021
Overall, We know that as long as d ≥ c4 log(1∕e), with probability at least 1 - exp(-Ω(e2m)),
there exists N0 with |N0| = O(1/) such that for any η ∈ [0, 1/L],
IFTbV ⑺-FTbV (η )| ≤ (2C + 3)e,
for η0 ∈ arg mi□η∈Ne ∣η - η0∣. Changing E to e0∕(2C + 3) finishes the proof.	□
Proof of Lemma 19. Let Ei be the event that √d/√L ≤ σi (Xtrain) ≤ √Ld and 1/L ≤ λ% (Htrain) ≤
L for all i ∈ [n] and JdQ/4 ≤ ∣∣ξtrain∣∣ ≤ √dσ. Let E2 be the event that √d∕√L ≤ σi(Xvalid) ≤
√Ld and 1/L ≤ λi(Hvalid) ≤ L for all i ∈ [n] and √dσ∕4 ≤ kξvalidk ≤ √dσ. According to
Lemma 1 and Lemma 45, we know both Ei and E? hold with probability at least 1 - exp(-Ω(d)).
Assuming d ≥ c4 for certain constant c4, we know Pr[E1 ∩ E2] ≥ 2/3. Also define E1(k) and E2(k) on
each training set S(kn. By concentration, we know with probability at least 1 - exp(-Ω(m)),
m
ɪXi{e*∩e∕o ≥ 1.
k=i
It’s easy to verify that conditioning on Ei, the GD sequence always exceeds the norm threshold and
gets truncated for η ≥ 3L as long as t is larger than certain constant. We can lower bound FTbV for
any η ≥ 3L as follows,
m
1
FTbV (η)=一
m
k=i
1m
≥-
m
k=i
(k)	(k)	2
wt,η - wvalidH(k)
Hvalid
w(k - w(alidll H(k) 1 {EI ∩E2} ≥ 2σ2 2 = σ2,
Hvalid	2
where the last inequality lower bounds
(k)
wt,η
(k)
- wvalid
by 2σ2 when wt(,kη) gets truncated.
□
Proof of Lemma 21. We first show that with constant probability in Xtrain, the variance of the
eigenvalues of Htrain is lower bounded by a constant. Let λ be 1/n Pn=i λ%. Specifically, we show
1/n Pn=i λ - ʌ2 is lower bounded by a constant.
Let’s first compute the variance of the eigenvalues in expectation. Let the i-th row of Xtrain be xi> .
We have,
E [ʌ2] = ~2 E	(tr ("trainXtrain))	= E
1n	1
=n4 EEkxik4 + n4 ∑ Ekxik2kxjk2
i=i	i≤i6=j ≤n
=二(nd(d + 2) + n(n - 1)d2) = ∖ + ^.
n4	n2	n3
Similarly, we compute E 1/n Pin=i λi2 as follows,
1n
n X λ2
i=i
E
nE [tr (XtrainXtrainXtrainXtrain)]
1n	1
nXEkxik4 + n X Ehxi,xji2
i=i	i≤i6=j≤n
ʌ (nd(d + 2) + n(n - 1)d) = ' + — + ɪ
n3	n2 n n2
Therefore, we have
i=i
d d	2d	d 4
n + n2-忑 ≥ n ≥ 3
41
Under review as a conference paper at ICLR 2021
where the first inequality assumes n ≥ 2 and the last inequality uses n ≤ 苧.Since n ≥ 1 d, We
know n ≥ 2 as long asd ≥ 8.
Let E be the event that √d∕√L ≤ σi (Xtrain) ≤ √Ld and 1/L ≤ λi (Htrain) ≤ L for i ∈ [n] with
L = 100. According to Lemma 1, we know E happens with probability at least 1 - eχp(-Ω(d)).
Let 1 {E} be the indicator function for event E. Next we show that E[l∕nPn=ι(λi - λ)2l {E}] is
also lower bounded.
It,s clear that E [λ2l {E}] is upper bounded by E [λ2]. In order to lower bound
E [ 1 Pn=ι λ2l {E}] , we first show that E [ 1 Pn=ι λ2l {£}] is small. We can decompose
E [ 1 Pn=1 λ2l {«?}] into two parts,
1n
n X λ2ι {e}
E
1n
=E — £121 {£andλι ≤ L}
i=1
1n
+ E — £入21 {λι > L}
i=1
The first term can be bounded by L2 Pr[£]. Since Pr[£] ≤ exp(-Ω(d)), we know the first term
is at most 1/6 as long as d is larger than certain constant. The second term can be bounded by
E [λ21 {λι > L}] . According to Lemma 48, we know Pr[λι ≥ L +1] ≤ exp(-Ω(dt)). Then, it,s
not hard to verify that E {λ1 > L}] = O(1∕d) that is bounded by 1/6 as long as d is larger
than certain constant. Overall, we know E [n Pn=1 λ21 {E}] ≥ E [ 1 pn=1 λ2] - 1/3. Combing
with the upper bounds on E [λ2l {E}], we have E [ 1 Pn=1(λi - λ)2l {E}] ≥ 1.
Since conditioning on E, λi is bounded by L for all i ∈	[n]. In order to make
E [1 Pn=1(λi - λ)2l {E}] lower bounded by one, there must exist positive constants μ1, μ2 such
that with probability at least μ1, E holds and n Pn=1(λi - λ)2 ≥ μ2.
Since 1 Pn=1(λi-λ)2 ≥ μ2 and λ% ≤ L for all i ∈ [n], we know there exists a subset of eigenvalues
S ⊂ {λi}n with size μ3n such that ∣λi - λ∣ ≥ μ4 for all λ% ∈ S, where μ3, μ4 are both positive
constants.
If at least half of eigenvalues in S are larger than λ, we know at least μ翌n number of eigenval-
ues are smaller than λ. Otherwise, the expectation of the eigenvalues will be larger than λ, which
contradicts the definition of λ. Similarly, if at least half of eigenvalues in S are smaller than λ,
we know at least μμn number of eigenvalues are larger than λ. Denote μ5 := μ3μ. We know
λμ5n - λn—μ5n+1 ≥ μ4.	□
Proof of Lemma 23. Let E1 be the event that √d/ √L ≤ σi (Xtrain) ≤ √Ld and 1/L ≤ λi (Hrain) ≤
L for all i ∈ [n] and Vdo/4 ≤ ∣∣ξtrai∩k ≤ √dσ. Let E3 be the event that Vd/VL ≤ σi(Xvalid) ≤
√Ld and 1/L ≤ λi(Hvalid) ≤ L for all i ∈ [n] and Vdo/4 ≤ ∣ξvalidk ≤ √dσ. According to
Lemma 1 and Lemma 45, we know both E1 and E3 hold with probability at least 1 - exp(-Ω(d)).
In this proof, we assume both properties hold and take a union bound at the end.
We can lower bound ∣wt,η - wvalid ∣2H	as follows,
Ilwt,η - WVaIidIIHvaIid = llwt,η - w* - (XValid),ξvalid 11 凡加
≥ kwt,η - w*kHvalid + n ∣ξvalid∣∣2 - 2 Kwt,η - w*, Hvalid(Xvalid)*ξvalid) | .
For the second term, by Lemma 45, we know for any 1 >	> 0, with probability at least 1 -
exp(-Ω(e2d)),
—kξvalid∣∣2 ≥ (1 - e)σ2.
n
We can write down the third term as [(Xvalid)*]>Hvalid(wt,η 一 w*),ξvalid). Suppose σ is a constant,
we know ∣∣[(Xvalid)*]>Hvalid(wt,η - w*)∣∣ = O(1/yd). Therefore, for a fixed η ∈ [1/L, 3L], We
have with probability at least 1 - exp(-Ω(e2d)),
Kwt,η - w , HVaIid(XValid),ξvalid)
≤ .
42
Under review as a conference paper at ICLR 2021
To prove this crossing term is small for all η ∈ [1/L, 3L], we need to construct an -net for the
crossing term. Similar as in Lemma 9, we can show there exists an -net for the crossing term with
size O(t/). Taking a union bound over this -net, we are able to show with probability at least
1 — O(t∕e) exp(-Ω(e2d)),
Hvalid(Xvalid),ξvalid) ∣
≤ ,
for all η ∈ [1/L, 3L].
Overall, We have with probability at least 1 — O(t∕e) exp(—Ω(e2d)),
kwt,η — WvalidlIHvaIid ≥ kwt,η — w* kHvalid + n kξvalidk - 2 ∣<wt,η — w*, Hvalid(Xvalid),ξvalid) ∣
≥ kwt,η — w*kHvalid + (I-E)σ2 — 2e ≥ (I- 3e)σ2,
for all η ∈ [1/L, 3L], where the last inequality uses σ ≥ 1. The proof finishes as we change 3 to
E0.
C Proofs of train-by-train with large number of samples (GD)
In this section, we give the proof of Theorem 6. We show when the size of each training set n
and the the number of training tasks m are large enough, train-by-train also performs well. Recall
Theorem 6 as follows.
Theorem 6. Let FT bT (n) (η) be as defined in Equation 1. Assume noise level is a constant c1. Given
any 1 > e > 0, assume training set size n ≥ cd log(嗤),unroll length t ≥ c? log( Id), number
2
of training tasks m ≥ 盘声 log(勺芳)and dimension d ≥ c4 for certain constants c, c2, c3, c4. With
high probability in the sampling of training tasks, we have
Ellwt,ηiαn — w*ll2 ≤ (1 + e)^n^,
for all ηtrain ∈ argminη≥o FTbT(n)(η), where the expectation is taken over new tasks.
In the proof, we use the same notations defined in Section B. On each training task P, in Lemma 24
we show the meta-loss can be decomposed into two terms:
△TbT(n，P) = 2 llwt,η — WtrainkHtrain + 2n ll (In-ProjXtrain)ξtrainll ,
where Wtrain = wt + (Xtrain)tξtrain∙ Recall that Xtrain is a n X d matrix with its i-th row as x>. The
pseudo-inverse (Xtrain )^ has dimension d X n satisfying XtJainXtrain = Id Here, ProjXtraIn ∈ Rn×n is
a projection matrix onto the column span of Xtrain.
In Lemma 24, we show with a constant step size, the first term in ∆TbT (η, P) is exponentially small.
The second term is basically the projection of the noise on the orthogonal subspace of the data span.
We show this term concentrates well on its mean. This lemma servers as step 1 in Section B.1. The
proof of Lemma 24 is deferred into Section C.1.
Lemma 24. Assume n ≥ 40d. Given any 1 > e > 0, with probability at least 1 — m exp(一Ω(n))—
exp(—Ω(e4md∕n)),
1	n — d E2dσ2
FTbT(2/3) ≤ 20(1 - 3)σ +丁。+w
In the next lemma, we show the empirical meta objective is large when η exceeds certain threshold.
We define this threshold η such that for any step size larger than η the GD sequence has reasonable
probability being truncated. In the proof, we rely on the truncated sequences to argue the meta-
objective must be high. The precise definition of η is in Definition 2. This lemma serves as step 2 in
Section B.1. We leave the proof of Lemma 25 into Section C.2.
Lemma 25. Let ^^ be as defined in Definition 2 with 1 > E > 0. Assume n ≥ cd, t ≥ c2 , d ≥ c4 for
some constants c, c2,c4. With probability at least 1 — exp(—Ω(e4md2∕n2)),
FTbT (η) ≥
E2dσ2	n — d 2 E2dσ2
8n + 2n σ 20n
for all η > η.
43
Under review as a conference paper at ICLR 2021
1 ʌ T	C/	1 T	Ck	1	1	,	11	1	7^S	/	∖	∙	1	.1
By Lemma 24 and Lemma 25, we know when t is reasonably large, FTbT (η) is larger than
FTbT (2/3) for all step sizes η > rη. This means the optimal step Size η must lie m [0,η]. In
Lemma 26, we show a generalization result for η ∈ [0, η]. This serves as step 3 in Section B.1.
We prove this lemma in Section C.3.
Lemma 26. Let η be as defined in Definition 2 with 1 > > 0. Suppose σ is a constant. Assume n ≥
C log( Id )d,t ≥ c2,d ≥ c4 for some constants c, c2,c4. With probability at least 1—m exp(-Ω(n))-
O(^end + m) exp(—Ω(me4d2∕n2)),
IFTbT (η) — Fτbτ(η)∣ ≤
17e2dσ2
n
for all η ∈ [0, η],
Combining Lemma 24, Lemma 25 and Lemma 26, we present the proof of Theorem 6 as follows.
Proof of Theorem 6. According to Lemma 24, assuming n ≥ 40d, given any 1/2 >	> 0,
with probability at least 1 — m exp(-Ω(n)) — exp(-Ω(c4md∕n)), FTbT(2/3) ≤ 20(1 — 3)2tσ2 +
n-dσ2 + ∖0n . As long as t ≥ c2 log( Jn) for certain constant c2, We have
FTbT(2/3) ≤
n — d 2	72 dσ2
2n 0 + 100n
Let η be as defined in Definition 2 with the same e. According to Lemma 25, as long as n ≥ cd,t ≥
c2,d ≥ c4 with probability at least 1 — exp(—Ω(e4md2∕n2)),
2dσ2	n — d 2	2dσ2 n — d 2	7.52dσ2
FTbT (η) ≥------1------σ2-------=------σ2 H---------
(η) - 8n + 2n	20n	2n	+ 100n
for all η > η. We have FTbT(η) > FrbT(2/3) for all η ≥ η. This implies that ηtrain is within [0, η]
and FTbT(η%n) ≤ FTbT(2/3) ≤ nnσ2 + 7ιθ0n .
By Lemma 26, assuming σ is a constant and assuming n ≥ C log( -nd)d for some constant c, we have
with probability at least 1 — mexp(—Ω(n)) — O(Itnd + m)exp(—C(mE4d2/n2)),
IFTbT (η) — FTbT(η)∣ ≤
17e2dσ2
n
for all η ∈ [0,η]. This then implies
分 ^	17e2dσ2	n — d 2	24e2 dσ2
FTbTStrain) ≤ FTbTStrain) +---------≤ F-σ +----------------.
n	2n	n
By the analysis in Lemma 24, we have
FTbT(Inlain) =E2 Mg
=E2 Uwt<1in
21	2
-Wtrain UHain + E2n U(In-PrOjXtrain)ξtrain U
-WtrainllH	+ 丁 σ2.
Htrain	2n
Therefore, We know E2 ∣∣wt,n裔II 一 Wtrain∣∣H .	≤	24-y2. Next, We show this implies
E ∣∣wt,η*a,n — w*∣∣2 is small.
Let E be the event that 1 — ≤ λi(Htrain) ≤ 1 + for all i ∈ [d]. According to Lemma 27, we know
Pr[Ε] ≥ 1 — exp(—Ω(e2n)) as long as n ≥ 10d/e2. Then, we can decompose E ∣∣wt,η意II 一 w*∣∣2
as follows,
E ∣∣wt,η温—w* ∣∣2 = E M,η温 — w* ∣∣2 1 ⑹ + E M,η温 — w* ∣∣2 ɪ ⑻.
Let’s first show the second term is small. Due to the truncation in our algorithm, we know
∣∣wt,η温 — w* ∣∣2 ≤ 412σ2, which then implies E ∣∣wt,η温—w* ∣∣2 1 {£} ≤ 412σ2 exp(—Ω(e2n)).
As long as n ≥ & log(-nd) for some constant c, we haveE ∣∣wt,η嘉ɪɪ — w*∣∣21 {£} ≤ -dn2.
44
Under review as a conference paper at ICLR 2021
We can upper bound the first term by Young’s inequality,
EUwt,η温-w*l∣2 ɪ {E} ≤ (I + N)EIlwt,η温-wtrain∣∣2 ɪ {E} + (I + E)Ekwtrain- w*∣∣2 { {E} .
Conditioning on E, We have ||皿,需加一wtrain∣∣Htrain ≥ (1 - E) ∣∣wt,η*ain - wtrain∣∣2 which implies
llwt,η温-wtrain∣∣2 ≤ (1 + 2e) ∣∣wt,璃ain - wtrain11HtraiII as long as E ≤ 1/2. Similarly, we also have
IIwtrain - w*∣∣2 ≤ (1 + 2e) ∣∣wtrain - w*kHtrain. Then, We have
E∣∣wt,η 温-w*∣∣2 ”}
≤(1 + E)(1 + 2e)E ∣∣wt,η温-wtrain∣∣Htodn 1 {E} + (1 + E)(1 + 2e)E kwtrain - w* 嗫^ 1 {E}
≤(5 + I)E Uwt,η温-wtrain ∣∣Htrain + (1 + 5E)Ekwtrain- w* kHtaln
≤(5 + 1)48E2dσ2 +(1 + 5e)Q ≤ (1 + 293e)处.
En	n	n
Overall, we have E ∣∣wt,η*ain — w* ∣∣2 ≤ (1 + 293e)dσ2 + ^dσ2 = (1 + 294e)誓.Combining all
the conditions, we know this holds with probability at least 0.99 as long as σ is a constant c1,
n ≥ Cd log(嗤),t ≥ c2 log(Id),m ≥ C4⅛ log(誓),d ≥。4 for some constants c,c2,c3,c4. We
finish the proof by choosing E = E0/294.
…一一	√∖	，c\
C.1 UPPER BOUNDING FT bT (2/3)
In this section, we show there exists a step size that achieves small empirical meta objective. On
each training task P, we show the meta-loss can be decomposed into two terms:
1n	2
δTbT(n，P) =2n〉： (hwt,η - wtrain, Xii- (ξi - Xi Xtrainξtrain))
=2 kwt,η - wtrain k Htrain + % U (In - ProjXtrain)ξtrain 11，
where wtrain = w* + (Xtrain)tξtrain∙ In Lemma 24, We show with a constant step size, the first term is
exponentially small and the second term concentrates on its mean.
Lemma 24. Assume n ≥ 40d. Given any 1 > e > 0, with probability at least 1 — m exp(-Ω(n))一
exp(-Ω(E4md∕n)),
1	n - d	E2dσ2
FTbT(2/3) ≤ 20(I- 3) σ +kσ +k
Before we go to the proof of Lemma 24, let’s first show the covariance matrix Htrain is very close to
identity when n is much larger than d. The proof follows from the concentration of singular values
of random Gaussian matrix (Lemma 48). We leave the proof into Section C.4.
Lemma 27. Given 1 > e > 0, assume n ≥ 10d∕E2. With probability at least 1 — exp(-Ω(E2n)),
(1 - E) √n ≤ σi (Xtrain) ≤ (1 + E) √n and 1 - E ≤ λi (Htrain) ≤ 1 + E,
for all i ∈ [d].
Now, we are ready to present the proof of Lemma 24.
Proof of Lemma 24. Let’s first look at one training set Strain, in which yi = hw*, Xii + ξi for each
sample. Recall the meta-loss as
1n
δTbT(η, P) =诟 E (hwt,η,xii - hw*,xii - ξi) .
n i=1
45
Under review as a conference paper at ICLR 2021
Recall that Xtrain is an n × d matrix with its i-th row as xi>. With probability 1, we know Xtrain is full
column rank. Denote the pseudo-inverse of Xtrain as XtJain ∈ Rd×n that satisfies XtJainXtrain = Id
and XtrainXtJain = ProjXtrain, where ProjXtrain ∈ Rn×n is a projection matrix onto the column span of
Xtrain.
Let wtrain be w* + Xtrainξtrain, where ξtrain is an n-dimenSiOnal vector with its i-th entry as ξi. We
have,
∆TbT (η, P)
t,η - wtrain, xii -	ξi - xi Xtrainξtrain
i=1
2 kwt,η — WtrainkHtrain
+ 2n Il (In - PrOjXtrain)&aM『-
1n
—X : (wt,η - wtrain, xiξi - XixJXtrainξtrain) ∙
n i=1
We first show the crossing term is actually zero. We have,
- wtrain, xiξi -
xixi>XtJrainξtrain
1
n
nn
wt,η	- wtrain ,	xiξi -	xixi Xtrainξtrain
i=1	i=1
- wtrain , Xtrainξtrain - Xt>rainXtrainXtJrainξtrainE
- wtrain ,
rain
—
where the second last equality holds because XtrainXtJrain = ProjX .
We can define wt(rkai)n as wk* + (Xt(rkai)n)Jξt(rkai)n for every training set St(rkai)n. Then, we have
mm
FTbT(η)	WEn- w(kn	+ — X 2- ll(In - ProjX(k)
m	n	train
2
We first prove that the second term concentrates on its mean. We can concatenate m noise vectors
ξ(kil into a single noise vector &疝 with dimension nm. We can also construct a data matrix Xtrain ∈
Rnm×dm that consists of Xt(rkai)n as diagonal blocks. Then the second term can be written as
1
2
(Inm - PrOjXtrain) ξtrain
2
According to Lemma 45, with probability at least 1 - exp(-Ω(e4md2∕n)),
σ.
By Johnson-Lindenstrauss Lemma (Lemma 49), we know with probability at least 1 -
exp(-Ω(e4md)),
√= ∣∣Proj/nξtrain∣∣ ≥ (1 - -2)√md口
nm train	mn nm
Therefore, we have ∣∣ √nmξtrain∣l ≤ (1 +
l∣ξrain∣∣ ≥ (1 - e2) ʌ/n (1 - -n- )σ∙
l	l2
nd)σ2 and l √nmPrOjXttainftrain|	≥ (1 - 2-2) nσ2.
Overall, We know with probability at least 1 - exp(-Ω(e4md∕n)),
2 || √nm (Inm -PrOjXtraj瓦加
2 n-
d 2 5-2 dσ 2
≤ 2n ° +	2n
Now, we show the first term in meta objective is small when we choose a right step size. According
to Lemma 27, we know as long as n ≥ 40d, with probability at least 1 - exp(-Ω(n)), √n∕2 ≤
46
Under review as a conference paper at ICLR 2021
σi (Xtrkin) ≤ 3√n∕2 and 1/2 ≤ λi(Ht(kn) ≤ 3/2, for all i ∈ [d]. According to Lemma 45, we know
with probability at least 1 - exp(-Ω(n)), 忖小| ≤ 2√nσ. Taking a union bound on m tasks, We
know all these events hold with probability at least 1 - m exp(-Ω(n)).
For each k ∈ [m], we have IwtrkinI ≤ 1 + √2n 2√nσ ≤ 5σ. It,s easy to verify that for any step size at
most 2/3, the GD sequence will not be truncated since we choose the threshold norm as 40σ. Then,
for any step size η ≤ 2/3, we have
m
ɪ X 1 Ilw⑹-w⑹ Il
m E 2 ll t,η trainllHtrkn
m2
「 X 2 ||(I-ηH(±Ml∣∣H (k)
m k=1 2	Htrain
≤3(1 - η产25σ2 ≤ 20(1 - 1)2tσ2,
where the last inequality chooses η as 2/3.
Overall, we know with probability at least 1 - m exp(-Ω(n)) - exp(-Ω(c4 *md∕n)),
1	n-d 52dσ2
FTbT(2/3) ≤ 20(I - 3尸%2 + 2n σ2 +-----------2n~.
We finish the proof by changing 522 by (e0)2∕20.

C.2 LOWER BOUNDING FTbT FOR η ∈ (η, ∞)
In this section, we show the empirical meta objective is large when the step size exceeds certain
threshold. Recall Lemma 25 as follows.
Lemma 25. Let η be as defined in Definition 2 with 1 > e > 0. Assume n ≥ cd, t ≥ c2,d ≥ c4 for
some constants c, c2,c4. With probability at least 1 — exp(-Ω(e4md2∕n2)),
2dσ2	n - d 2
ETbT⑺ ≥ F + kσ
e2dσ2
20n
—
for all η > η.
Roughly speaking, we define η such that for any step size larger than η the GD sequence has a
reasonable probability being truncated. The definition is very similar as η in Definition 1.
Definition 2. Given a training task P, let Ei be the event that √n∕2 ≤ σ%(χtrain) ≤ 3√n∕2 and
1/2 ≤ λi(Htrain) ≤ 3∕2 for all i ∈ [d] and √nσ∕2 ≤ ∣∣ξtrain ∣∣ ≤ 2√nσ. Let E2(η) be the event that
the GD Sequence is truncated with SteP size η. Given 1 > e > 0, define η as follows,
η
inf ir] ≥ 0E1 kwt,η - w train ∣Htrain 1 {E1 ∩ E2 (η)} ≥
e2dσ2 ]
n
Similar as in Lemma 5, We show 1 {Ei ∩ E2(η0)} ≥ 1 {Ei ∩ E2 (η)} for any η0 ≥ η. This means
conditioning on E1 , if a GD sequence gets truncated with step size η, it has to be truncated with any
step size η0 ≥ η. The proof is deferred into Section C.4.
Lemma 28. Fixing a training set Stan, let Ei and E (η) be as defined in Definition 2. We have
1{E1 ∩E2(η0)} ≥ 1{E1 ∩E2(η)},
for any η0 ≥ η.
Next, we show η does exist and is a constant. Similar as in Lemma 6, we show that the GD sequence
almost never diverges when η is small and diverges with high probability when η is large. The proof
is left in Section C.4.
Lemma 29. Let η be as defined in Definition 2. Suppose σ is a constant. Assume n ≥ cd, t ≥
c2 , d ≥ c4 for some constants c, c2 , c4 . We have
4
3 < η < 6.
47
Under review as a conference paper at ICLR 2021
Next, We show the empirical loss is large for any η larger than η. The proof is very similar as the
proof of Lemma 3.
Proof of Lemma 25. By Lemma 29, we know η is a constant as long as n ≥ cd,t ≥ c2, d ≥ c4
for some constants c, c2, c4. Let Ei and E2(η) be as defined in Definition 2. For the simplicity of the
proof, we assume E 11 ∣∣wt,η - WtrainIlHtraiII 1 {Ei ∩ E2(η)} ≥ € ^σ . The other case can be resolved
using same techniques in Lemma 3
Conditioning on Ei, we know 1 ∣∣wt,η — WtrainIlHtrahI ≤ 3452σ2. Therefore, we know Pr[E1 ∩
E1 (η)] ≥ 3445dn. For each task k, define E(k) and E1k)(η) as the corresponding events on training
set Strain. By Hoeffding,s inequality, we know with probability at least 1 - exp(-Ω(e4md2∕n2)),
m∙ XX 1{E(k) ∩E2k)(η)0 ≥ 4⅛.
k=i
By Lemma 28, we know 1 1E(k) ∩ E1k) (η): ≥ InE(k) ∩ E1k) (η): for any η ≥ ^^.
Recall that
mm
1	(k)	(k) 2	1	1	2
FTbT ⑺=m	wt,η - wtrain 11 H(k) + ^X 2n||(1- - ProjX^n )
k=i	train	k=i
We can lower bound the first term for any η > ^^ as follows,
m
1
FTbT (η) = 一
m
k=i
(k)	(k)	2
Wt,η - WtrainH(k)
Htrain
1m
≥-
m
k=i
352σ2 1
≥------
4m
WjkinL (k)i
352σ2 1
≥-----
4m
m
Xι{E(k) ∩E2k)(η)}
k=i
X ι{E(k)∩E2k)(η)} ≥ 乎
k=i
where the second inequality lower bounds the loss for one task by 352σ2 when the sequence gets
truncated.
For the second term, according to the analysis in Lemma 24, with probability at least 1 -
exp(-Ω(e4md∕n)),
1 m 1	2
mX M(In-PrOjXtrkMainll	≥
k=i
n - d 2	2dσ2
2n 0 - 20n
Overall, with probability at least 1 - exp(-Ω(e4md2∕n2)),
FTbT (η) ≥
2dσ2 n-d 2 2 dσ2
8n + 2n σ	20n
for all η > η.

C.3 Generalization for η ∈ [0, η]
Combing Lemma 24 and Lemma 25, it,s not hard to see that the optimal step size 优疝 lies in [0, η].
In this section, we show a generalization result for step sizes in [0, η]. The proof of Lemma 26 is
given at the end of this section.
Lemma 26. Let ^^ be as defined in Definition 2 with 1 > e > 0. Suppose Q isa constant. Assume n ≥
C log(专 )d, t ≥ c1,d ≥ c4 for some constants c, c2,c4. With probability at least 1-m exp(-Ω(n))-
O(段 + m) exp(-Ω(me4d2∕n2)),
IFTbT(η) - FTbT(η)l ≤
17e2dσ2
n
for all η ∈ [0, η],
48
Under review as a conference paper at ICLR 2021
In Lemma 30, we show FTbT concentrates on FTbT at any fixed step size. The proof is almost the
same as Lemma 7. We omit its proof.
Lemma 30. Suppose σ is a constant. For any fixed η and any 1 >	> 0, with probability at least
1 — exp(-Ω(e2m)),
I-FTbT(η) - FTbT(η)∣ ≤ e.
Next, We construct an e-net for FTbT in [0, η]. The proof is very similar as in Lemma 8. We defer
its proof into Section C.4.
Lemma 31. Let η be as defined in Definition 2 with 1 > e > 0. Assume the conditions in Lemma 29
hold. Assume n ≥ Clog(六)d for some constant c. There exists an 法 aσ -net N ⊂ [0, η] for FTbT
with |N| = O(着).That means, for any η ∈ [0, η],
IFTbT(η) - FTbT(η0)∣ ≤
8e2dσ2
n
for η0 = arg minη00∈N,η00≤η(η - η00).
We also construct an e-net for the empirical meta objective. The proof is very similar as in Lemma 9.
We leave its proof into Section C.4.
Lemma 32. Let η be as defined in Definition 2 with 1 > e > 0. Assume the conditions in Lemma 29
hold. Assume n ≥ 40d. With probability at least 1 一 mexp(一Ω(n)), there exists an Tσ--net
N ⊂ [0, η] for FTbT with ∣N0∣ = O(% + m). That means, for any η ∈ [0, η],
0
IFT bT (η) - FTbT(η )I ≤
e2 dσ2
n
for η0 = arg minη00∈N0,η00≤η(η - η00).
Combing the above three lemmas, We give the proof of Lemma 26.
Proof of Lemma 26. We assume σ as a constant in this proof. By Lemma 30, We knoW With
probability at least 1 — exp(-Ω(me4d2/n2)), IFrbT(η) — FTbT(η)∣ ≤ ^¾σ2 for any fixed η. By
Lemma 31, we know as long as n ≥ Clog(六)d for some constant c, there exists an ^nσ2-net N
for FTbT with size O(辑).By Lemma 32, we know with probability at least 1 - mexp(一Ω(n)),
there exists an W ；： -net N0 for FTbT with size O(舞 + m). It’s not hard to verify that N ∪ N0 is
still an 8w fσ -net for FTbV and FTbV. That means, for any η ∈ [0, η], we have
00
IFTbT(η) - FTbT(η )I, IFTbT (η) - FTbT(η )I ≤
8e2dσ2
n
for η0 = arg minη00∈N∪N0,η00≤η(η - η00).
Taking a union bound over N ∪ N0, we have with probability at least 1 - O(禺 +
m) exp(-Ω(me4d2∕n2)),
l-FTbT (η) - FTbT (η)∣ ≤ ~~~
for all η ∈ N∪ N0.
Overall, we know with probability at least 1 - m exp(-Ω(n)) - O(舞 + m) exp(-Ω(me4d2∕n2)),
for all η ∈ [0, η],
IFT bT (η) - FTbT(η)I
0	0	00
≤IFT bT (η) - FTbT(η )I + IFTbT (η) - FTbT(η )I + IFT bT (η ) - FTbT(η )I
17e2 dσ2
≤-------,
n
where η0 = arg minη00∈N∪N0,η00≤η(η - η00).
49
Under review as a conference paper at ICLR 2021
C.4 Proofs of Technical Lemmas
Proof of Lemma 27. According to Lemma 48, we know with probability at least 1 - 2 exp(-t2/2),
√n - λ∕d - t ≤ σi (Xtrain) ≤ √∕n + ʌ/d + t
for all i ∈ [d]. Since d ≤ 奈,We have √n - √√0 - t ≤ σi(Xtrain) ≤ √n + √√0 + t. Choosing
t = (ɪ - √o)e√n, We have with probability at least 1 - exp(-Ω(e2n)),
(I - 3)√n ≤ σi(Xtrain) ≤ (I + 3)√n.
Since λi(Htrain) = 1∕nσ2(Xtrain), we have 1 -E ≤ λi(Htrain) ≤ 1 + e.	口
Proof of Lemma 28. The proof is almost the same as in Lemma 5. We omit the details here.
Basically, in Lemma 5, the only property we rely onis that the norm threshold is larger than 2 kwtraink
conditioning on E1 . Conditioning on E1 , we know kwtrain k ≤ 5σ. Recall that the norm threshold is
still set as 40σ. So this property is preserved and the previous proof works.	口
Proof of Lemma 29. The proof is very similar as in Lemma 6. Conditioning on E1 , we know
kHtrain k ≤ 3/2 and kwtrain k ≤ 5σ. So the GD sequence never exceeds the norm threshold 40σ for
any η ≤ 4/3. That means,
E1 kwt,η - wtrain IlHtraln ɪ {E1 ∩ E2 ㈤} = 0
for all η ≤ 4/3.
To lower bound the loss for large step size, we need to first lower bound k wtrain k. Recall that
Wtrain = w* + (Xtrain)tξtrain. Conditioning on E1, we know kξtraink ≤ 2√nσ and σd(Xtrain) ≥ √n∕2,
which implies Il(Xtrain)t∣∣ ≤ 2∕√n. By Johnson-Lindenstrauss Lemma (Lemma 49), we have
∣∣Projxtrainξtrain∣∣ ≤ 2ʌ/d/n ∣∣ξtraink with probability at least 1 - exp(-Ω(d)). Call this event E3.
Conditioning on E1 ∩ E3 , we have
II (Xtrain)tξtrain∣∣ ≤ 2√nσ √n 2ʌ/ n ≤ 6∖∣ ^σ,
which is smaller than 1/2 as long as n ≥ 122dσ2. Note that we assume σ is a constant. This then
implies kwtraink ≥ 1/2.
Let {wτ0 ,η} be the GD sequence without truncation. For any step size η ∈ [6, ∞], conditioning on
E1 ∩ E3 , we have
||w0,n|| ≥ ((6 X 2 - 1)t - 1) Ilwtrainll ≥ (2t - 1) 2 ≥ 40σ,
where the last inequality holds as long as t ≥ c2 for some constant c2. Therefore, we know when
η ∈ [6, ∞), 1 {Eι ∩E2 (η)} = 1 {Eι ∩ E3}. Assuming n ≥ 40d, we know Ei holds with probability
at least 1 - exp(-Ω(n)). Then, we have for any η ≥ 6,
E2 ∣∣wt,η - WtrainkHtrain ɪ {E1 ∩ E2(n)} ≥4 (40σ - 5σ)? Pr[E1 ∩ E3] ≥ —n—,
where the last inequality assumes n ≥ c, d ≥ c4 for some constant c, c4.
Overall, we know Eɪ ∣∣wt,η 一 WtrainkHtrailI ɪ {Ei ∩ E2(η)} equals zero for all η ∈ [0,4/3] and is at
least e2nσ2 for all η ∈ [6, ∞). By definition, we know η ∈ (4/3,6).	口
Proof of Lemma 31. By Lemma 29, we know η is a constant. The proof is very similar as in
Lemma 8. Let Ei and E (η) be as defined in Definition 2. For the simplicity of the proof, we assume
E2 ∣∣wt,η - WtraInkHtraiII ɪ {Ei ∩ E2(η)} ≤ e T . The other case can be resolved using techniques
in the proof of Lemma 8.
50
Under review as a conference paper at ICLR 2021
Recall the population meta objective
1	2 n-d 2
FTbT(η) = E2 llWt,n - WtramkHtrain +	2n-σ .
Therefore, we only need to construct an -net for the first term.
We can divide E1 ∣∣wt,η - WtrainIlHtram as follows,
E2 kwt,η - WtrainkHtrain
=E2 kwt,η - WtrainkHtrain 1 {E1 ∩E2(η)} + E2 kwt,η - WtrainkHtrain 1 {E1 ∩E2㈤}
+ E2 kwt,η - WtrainkHkain ɪ {EM ∙
We will construct an -net for the first term and show the other two terms are small. Let’s
first consider the third term. Assuming n ≥ 40d, We know Pr[Eι] ≤ exp(-Ω(n)).
Since 1 ∣∣Wt,η - WtrainkHtraiII is O(1)-subexponential, by Cauchy-Schwarz inequality, we have
E1 ∣∣Wt,η - WtrainkHtrailI ɪ {1^1}=。⑴ exp(-Ω(n)). Choosing n ≥ Clog(n/(Ed)) for some con-
stant C, we know 2 ∣∣Wt,η - WtrainkHkain 1{E1} ≤ ⅛2∙
Then we upper bound the second term. Since E 2 ∣∣Wt,η 一 WtrainkHtrailI ɪ {E1 ∩E2(η)} ≤ ^⅛σ2 and
1 ∣∣Wt,η - WtrainkHkain ≥ 3⅛2 When Wt^ diverges, we know Pr[Ei ∩ E2(η)] ≤ 4⅛. Then, we can
upper bound the second term as follows,
1	2	3 × 452σ2 4E2d	6E2 dσ2
E 2 kWt,n - WtrainkHtrain 1 {E1 ∩ & ㈤} ≤ —4-砺 ≤
Next, similar as in Lemma 8, we can show the first term 1 ∣∣Wt,η - WtrainkHtrain ɪ {E1 ∩ E2(η)} is
O(t)-lipschitz. Therefore, there exists an —『-net N for E 1 ∣∣Wt,η - WtrainkHtrain ɪ {E1 ∩ E2(η)}
with size O(%).That means, for any η ∈ [0, η],
E2 llWt,n - WtrainkHteain 1 {E1 ∩ E2㈤}
-E2 llWt,n0 - WtrainkHkain
E2dσ2
1{E1 ∩E2(η)} ≤-------
n
for η0 = arg minη00∈N,η00≤η(η - η00).
Combing with the upper bounds on the second term and the third term, we have for any η ∈ [0, η],
IFTbT(η) - FTbT(ηO)I ≤
8e2dσ2
n
for η0 = arg minη00∈N,η00≤η(η - η00).

Proof of Lemma 32. By Lemma 29, we know η is a constant. For each k ∈ [m], let Eι,k be
the event that √n∕2 ≤ σi(Xtrkn) ≤ 3√n∕2 and 1/2 ≤ λi(Htrkn) ≤ 3/2 for all i ∈ [d] and
√nσ∕2 ≤ ∣∣ξtrai)1∣∣ ≤ 2√nσ. Assuming n ≥ 40d, by Lemma 27, we know with probability at least
1 — m exp(-Ω(n)), Eι,k,s hold for all k ∈ [m].
Then, similar as in Lemma 9, there exists an e2^2 -net N0 with ∣N0∣ = O(% + m) for FTbT. That
means, for any η ∈ [0, η],
IFrbT(η) - FTbT(η0)∣ ≤ —n—
for η0 = arg minη00∈N0,η00≤η(η - η00).

51
Under review as a conference paper at ICLR 2021
D Proofs of train-by-train v.s. train-by-validation (SGD)
Previously, we have shown that train-by-validation generalizes better than train-by-train when the
tasks are trained by GD and when the number of samples is small. In this section, we show a similar
phenomenon also appears in the SGD setting.
In the train-by-train setting, each task P contains a training set Strain = {(xi, yi)}in=1. The
inner objective is defined as f(w) = * P(x y)∈straiII (8X 一 y)2. Let {wτ,η} be the SGD
sequence running on f(w) from initialization 0 (without truncation). That means, wτ,η =
wT—i,n - nvf (WT—ι,η), Where Vf (WT—i,n) = (<wτ—ι,η, Xi(T―1))- Iyi(T—1)) Xi(T—1). Here in-
dex i(τ - 1) is independently and uniformly sampled from [n]. We denote the SGD noise as
O
ʌ O
nτ-ι,η := Vf (wτ-ι,η) - Vf (w「-ι,η). The meta-loss on task P is defined as follows,
∆τbT(n)(η,P) = EsGD^(wt,η) = ESGDX	(hWt,η,X- y)2 ,
(x,y)∈Strain
where the expectation is taken over the SGD noise. Note wt,η depends on the SGD noise along the
trajectory. Then, the empirical meta objective FT bT (n)(η) is the average of the meta-loss across m
different specific tasks
m
1
FTbT(n)(η) = —〉, δTbT(n)(η,Pk).
m k=1
(4)
In order to control the SGD noise in expectation, we restrict the feasible set of step sizes into O(1/d).
We show within this range, the optimal step Size under FTbT(n) is Ω(1∕d) and the learned weight is
far from ground truth w* on new tasks. We prove Theorem 9 in Section D.1.
Theorem 9. Let the meta objective FT bT (n) be as defined in Equation 4 with n ∈ [d/4, 3d/4].
Suppose σ is a constant. Assume unroll length t ≥ c2d and dimension d ≥ c4 log(m) for certain
constants c2,c4. Then, with probability at least 0.99 in the sampling of training tasks Pi, •…，Pm
and test task P,
n train = C(Ud) and E SGD ∣∣wt,η *ain - w*||2 =回抗),
for all ntrain ∈ arg mm0≤η≤ 1 FTbT(n) (n), where L = 100 and wt,η*αin IS trained by running SGD
on test task P.
In the train-by-validation setting, each task P contains a training set Strain with n1 sam-
ples and a validation set with n2 samples. The inner objective is defined as f(W) =
2n7 P(x y)∈strai 11(hw,xi - y)2. Let {wT,η} be the SGD sequence running on f(w) from ini-
tialization 0 (with the same truncation defined in Section 4). For each task P, the meta-loss
∆TbV (n1,n2) (n, P) is defined as
∆TbV(n1,n2)(n,p) = ESGD2n~	^X	(hwt,η, Xi - y)2 .
2 (x,y)∈Svalid
The empirical meta objective FTbV(n1,n2)(n) is the average of the meta-loss across m different tasks
P1 , P2 , ..., Pm ,
m
1
FTbV (nι ,n2)(n) =m，ΔTbV (niggP )∙	⑸
m k=1
In order to bound the SGD noise with high probability, we restrict the feasible set of the step sizes
into O( &2 ιig2 &). Within this range, we prove the optimal step size under FTbV⑺八敢)is Θ(1∕t) and
the learned weight is better than initialization 0 by a constant on new tasks. Theorem 10 is proved
in Section D.2.
Theorem 10. Let the meta objective FTbV(n1,n2) be as defined in Equation 5 with n1, n2 ∈
[d/4, 3d/4]. Assume noise level σ is a large constant c1. Assume unroll length t ≥ c2d2 log2 (d),
52
Under review as a conference paper at ICLR 2021
number of training tasks m ≥ c3 and dimension d ≥ c4 for certain constants c2, c3, c4. There exists
constant c5 such that with probability at least 0.99 in the sampling of training tasks, we have
ηValid = ®(1/t) andE∣∣wt,ηValid — w*∣∣2 = l∣w*k2 - a(1)
V
for all ηValid ∈ argmιno≤η≤	1	FTbV(n n2)(η), where the expectation Is taken over the new
一c— C5 d2 log2(d)	，
tasks and SGD noise.
Notations: In the following proofs, we use the same set of notations defined in Appendix B.
We use EP〜T to denote the expectation over the sampling of tasks and use ESGD to denote the
expectation over the SGD noise. We use E to denote EP〜TEsgd. Same as in Appendix B, We use
letter L to denote constant 100, which upper bounds lHtrainl with high probability.
D.1	Train-by-train (SGD)
Recall Theorem 9 as folloWs.
Theorem 9. Let the meta objective FTbT(n) be as defined in Equation 4 with n ∈ [d/4, 3d/4].
Suppose σ is a constant. Assume unroll length t ≥ c2d and dimension d ≥ c4 log(m) for certain
constants c2,c4. Then, with probability at least 0.99 in the sampling of training tasks Pi, •…，Pm
and test task P,
ηtrain = Q(1/d) and ESGD ||wt,%VaM- wlF = ω(*,
for all ηVrain ∈ arg mm°≤η≤ ∖ FTbT(n) (η), where L = 100 and wt,η7加 is trained by running SGD
on test task P.
In order to prove Theorem 9, we first show that ηtain is Ω(1∕d) in Lemma 33. The proof is similar
as in the GD setting. As long as η = O(1/d), the SGD noise is dominated by the full gradient.
Then, we can show that ∆τbτ(η, P) is roughly (1 一 Θ(1)η)t, which implies that ηVa⅛ = Ω(1∕d).
We leave the proof of Lemma 33 into Section D.1.1.
Lemma 33. Assume t ≥ c2d with certain Constant c2. With probability at least 1 — m exp(一Ω(d))
in the sampling of m training tasks,
总山"≥ 6L5d,
forallηVi" ∈ argmin0≤η≤马 FTbT⑺.
Let P = (D(wV), Strain, `) be an independently sampled test task with |Strain| = n ∈ [d/4, 3d/4].
For any step size η ∈ [6^, 2^], let wt,η be the weight obtained by running SGD on f(w) for t
steps. Next, we show ESGD ∣∣wt,η — WVk = Ω(σ2) with high probability in the sampling of P.
Lemma 34. Suppose σ is a constant. Assume unroll length t ≥ c2d for some constant c2. With
probability at least 1 — exp(一Ω(d)) in the sampling oftest task P,
ESGD kwt,η - w*k2 ≥ ʌr,
128L
for all η ∈ [ 6Ll5d, £3% ], where wt,η is obtained by running SGD on task P for t iterations.
With Lemma Lemma 33 and Lemma 34, the proof of Theorem 9 is straightforward.
Proof of Theorem 9. Combing Lemma 33 and Lemma 34, we know as long as σ is a constant,
t ≥ c2d,d ≥ c4 log(m), with probability at least 0.99, n^n = Ω(1∕d) and ESGD ∣∣wt,ηvin 一 wt∣∣ =
a。2), forallηtrain ∈ argmino≤η≤ 1 FTbT(η).	□
2L3 d
D.1.1 Detailed Proofs
Proof of Lemma 33. The proof is very similar to the proof of Lemma 2 except that we need to
bound the SGD noise term. For each k ∈ [m], let Ek be the event that √d∕√L ≤ σi(Xtrain) ≤
53
Under review as a conference paper at ICLR 2021
√Ld and 1/L ≤ 入乂耳向口)≤ L for all i ∈ [n] and √dσ∕4 ≤ |归廿疝|| ≤ √dσ. According
to Lemma 1and Lemma 45, we know for each k ∈ [m], Ek happens with probability at least
1 - exp(-Ω(d)). Taking a union bound over all k ∈ [m], We know ∩k∈[m]Ek holds with probability
at least 1 - m exp(-Ω(d)). From now on, we assume ∩k∈[m]Ek holds.
For each k ∈ [m], we have
∆TbT(η, Pk) := 2ESGD l∣w(,kη) - wtai)∏∣∣H(k).
Htrain
Since 1/L ≤ λi(Ht(rkai)n) ≤ Land (wt(,kη) -wt(rkai)n) is in the span of Ht(rkai)n,we have
1 F ∣L ,(k)	. ,(k) ∣∣2 ‹ ʌ L	p ∖	∣ LiP	∣,Xk)	2”(k) ∣∣2
2L ESGD ∣∣wt,η - Wtrain∣∣ ≤ δTbT (n，Pk )	≤ 2 ESGD	∣∣wt,η	- Wtrain∣∣	.
Recall the updates of stochastic gradient descent,
Wt(,kη) - Wt(rkai)n = (I - ηHt(rkai)n)(Wt(-k)1,η -Wt(rkai)n) -ηnt(k-)1,η.
Therefore,
ESGD ∣∣w(,kη) -Wtrai)n∣∣2 lw(-)ι,η TI(I- nHtrkn)(W(-)ι,η - w(ai)n)∣∣2+η2ESGD M”,η∣∣2 lw(-)ι,η
We know for any η ≤ 1/L,
(I -2«L)	∣∣w(k)	- w(k)∣∣2 ≤∣∣(I-MH(k))(w(k),	- w(k))∣∣2 ≤ (I	- η)∣∣w(k)	- w(k)∣∣2
(I	2ηL)	∣∣ wt-1,η Wtrain∣∣ ≤	∣∣(I	ηHtrain )(wt-1,η Wtrain)∣∣ ≤ (I L) ∣∣wtT,η Wtrain∣∣ .
The noise can be bounded as follows,
η2ESGD ∣∣n(-)ι,η∣∣ |W(-)i,n
=η2ESGD 卜(31产>1)屈-晨-Wtrai)n)- Htrkin(W(-I- Wtrki)n)∣∣ 尾-晨
≤η2ESGD ∣∣xi(t-1)x>t-I)(W(-)i,η - Wtrai'n)∣∣ |W(-)i,n
≤η2im(t-ax1) ∣∣xi(t-1) ∣∣2 ∣∣∣Wt(-k)1,η - Wt(rkai)n ∣∣∣2H(k) .
Since ∣∣Xtraink ≤ √L√d, we immediately know maxi(t-i) ∣∣xi(t-i) ∣∣ ≤ √L√d. Therefore, we can
bound the noise as follows,
η2ESGD ∣∣n(-)ι,η∣∣2 1W(-)i,n ≤η2imaχ) ∣∣xi(t-i)∣∣2 m-l - YinHHg)
≤L2η2d ∣∣∣Wt(-k)1,η - Wt(rkai)n ∣∣∣2 .
As long as η ≤ 2^, we have
∣ (k)	(k) ∣2	∣ (k)	(k) ∣2	(k)	η ∣ (k)	(k) ∣2
(I -ηL) "11m-	Wtrain∣∣	≤ ESGD	"^ -	w%∏∣∣	初-1内	≤ (I-五)	^-1^	- w%π∣∣	.
This further implies
(I - ηL)t kWtrain||2 ≤ ESGD mk) - Wtrai)J∣ ≤ (I- 2^ )t 忖廿皿『∙
Let n := 2L13d, we have
δTbT(n，Pk ) ≤ 5(1 - a1-4 J )t kWtraink2
2	4L d
54
Under review as a conference paper at ICLR 2021
Let ηι := 6Li5d, for all η ∈ [0, ηι] We have
δTbTSPQ ≥ 57(I - 4J). y Ilwtraink2 ∙
2L	6L4d
As long as t ≥ c2d for certain constant c2 , We knoW
2L(I- 6Ld)t kwtrαink2 > L2(1 - 4Ld)t kwtraink2.
As this holds for all k ∈ [m] and FTbT = 1/m Pm=I ∆τbτ(η, Pk), we know the optimal step size
ηtrain is Within [6L15d, ^d].	□
We rely the following technical lemma to prove Lemma 34.
Lemma 35. Suppose σ is a constant. Given any > 0, with probability at least 1 -
O(1∕e) exp(-Ω(e2d)),
Bt,η wtrain - w , Bt,η (Xtrain) ξtrain	≤ ,
for all η ∈ [0, 2L13d].
Proof of Lemma 35. By Lemma 1, with probability at least 1 一 exp(一Ω(d)), √d∕√L ≤
σi (Xtrain) ≤ √Ld and 1/L ≤ λi (Htrain) ≤ L for all i ∈ [n]. There-
fore ∣∣[(Xtrain)^]>Bt,η(Bt,ηw%n 一 w*)∣∣ ≤ 2√L∕√d. Notice that ξtrain is independent with
[(Xtrain)^]>Bt,η (Bt,ηwtrain —w*). By Hoeffding's inequality, with probability at least 1 一
exp(-Ω(e2d)),
K[(Xtrain户], Bt,η (Bt,η wtrain - w*), ξtrain) | ≤ C.
Next, We construct an c-net for η and show the crossing term is small for all η ∈ [0, 2^]. For
simplicity, denote g(η) :=〈Bt〃w*ain — w*B,η(Xtrain)Strain〉. Taking the derivative of g(η), We
have
g (n) =t(Htrain (I — ηHtrain)	wtrain, Bt,η (Xtrain),ξtrain)
+ t Bt,η wt
rain
一 w , Htrain (I 一 ηHtrain )	(Xtrain ) ξtrain 〉
According to Lemma 45, We know with probability at least 1 — exp(—Ω(d)), kξtraink ≤ √dσ.
Therefore, the derivative g0 (η) can be bounded as follows,
∣g0(η)l = o(i)t(i - n )t-1
L
Similar as in Lemma 14, there exists an c-net Ne with size O(1∕e) such that for any n ∈ [0, 3L3d],
there exists W ∈ Ne with |g(n)—g(n0)∣ ≤ c. Taking a union bound over Ne, we have with probability
at least 1 — O(1∕e) exp(—Ω(e2d)), for every n ∈ Ne,
|〈Bt,n wtrain — w*,Bt,η (Xtrain)HtraiJ | ≤ C.
which implies for every n ∈ [0, 3L3d].
KBt,ηwtrain - w , Bt,η (Xtrain),ξtrain)l ≤ 2c.
Changing c to c0∕2 finishes the proof.	□
Proof of Lemma 34. According to Lemma 1 and Lemma 45, we know with probability at least
1 — exp(—Ω(d)), √d∕√L ≤ σi(Xtrain) ≤ √Ld and 1∕L ≤ λi(Htrain) ≤ L for all i ∈ [n] and
√dσ∕4 ≤ ∣∣ξtrai∏k ≤ √dσ. We assume these properties hold in the proof and take a union bound at
the end.
Recall that ESGD kwt,η - w*k2 can be lower bounded as follows,
ESGD kwt,η - w*k2 =ESGD
t-1
Bt,η (wtrain + (Xtrain),ξtrain) - W ^X(I - WHtrain)t 1 Tnτ,η - w*
τ =0
2
≥ ∣∣Bt,η(wtrain + (Xtrain),ξtrain) - w*||
≥ ∣∣Bt,η (Xtrain)tξtrain∣∣2 + 2 (Btn w%n - w*, Bt,η (Xtrain) Strain)
55
Under review as a conference paper at ICLR 2021
For any η ∈ [6^, 2L^], we can lower bound the first term as follows,
IIBt,η (Xtrain) tξtrain∣F ≥ (1 - exp (-L)) 16L
≥ (1-exp (-6Lt6d)) T6L
σ2
≥ 64L,
where the last inequality holds as long as t ≥ c2d for certain constant c2.
2
Choosing E = ^^ in Lemma 35, We know with probability at least 1 - exp(-Ω(d)),
∣<Bt,η Wtrain- w* , Bt,η (Xtrain) ,ξtrain) I ≤ Xξ7y,
256L
for all η ∈ [0, 2L⅛d].
2 σ2
Overall, we have ESGD ∣∣wt,η - w*k ≥ ⅛L∙ Taking a union bound over all the bad events, we
know this happens with probability at least 1 - exp(-Ω(d)).

D.2 Train-b y-validation (SGD)
Recall Theorem 10 as follows.
Theorem 10. Let the meta objective FTbV (n1,n2) be as defined in Equation 5 with n1 , n2 ∈
[d/4, 3d/4]. Assume noise level σ is a large constant c1. Assume unroll length t ≥ c2d2 log2(d),
number of training tasks m ≥ c3 and dimension d ≥ c4 for certain constants c2, c3, c4. There exists
constant c5 such that with probability at least 0.99 in the sampling of training tasks, we have
ηValid = θ(1∕t)andE∣∣wt,ηValid- w*∣∣2 = l∣w*k2 - ω(I)
for all ηValid ∈ argmmo≤η≤	ι	FTbV(n n2)(η), where the expectation Is taken over the new
一c— C5 d2 log2(d)	，
tasks and SGD noise.
To prove Theorem 10, we first study the behavior of the population meta objective FTbV . That is,
Fτbv(η) ：= EP~t∆τbv(η, P) =EP~丁Esgd2 ∣∣wt,η -
=EP~丁ESGD2 ∣∣wt,η -
wV-
w*k2+σ2.
We show that the optimal step size for the population meta objective FTbV is Θ(1∕t) and
EP~丁ESGD ∣∣wt,η - w*∣∣2 = IlWVIl2 - Ω(1) under the optimal step size.
Lemma 36. Suppose σ is a large constant c1 . Assume t ≥ c2d2 log2 (d), d ≥ c4 for some constants
c2, c4. There exist η1,η2, η3 = Θ(1∕t) with < η < η3 and constant c5 such that
FTbV(η2) ≤ 2 ι∣w*∣2 -10C+σ2-
1	6 σ2	1
FTbV ㈤ ≥ 2 kw k - 10 C +T，∀η ∈ [0，ni] ∪[η3，c5 d2 log2(d)]
where C is a positive constant.
T	1 .	1 . . < 1 <	i' T—1	,∙A	1	1 ∙	1 . i'	7^S	,T^I
In order to relate the behavior of FTbV to FTbV, we show a generalization result from FTbV to FTbV
for η ∈ [0, C5d2 log2(d∕e)].
Lemma 37. For any 1 > E > 0, assume σ is a constant and d ≥ c4 log(1/E) for some constant c4.
There exists constant c5 such that with probability at least 1 — O(1∕e) exp(-Ω(e2m)),
∣FTbV(η) - FTbV(η)∣ ≤ e,
forallη ∈ [0, C5d2 l0g2(d∕e)].
56
Under review as a conference paper at ICLR 2021
Combining Lemma 36 and Lemma 37, we give the proof of Theorem 10.
Proof of Theorem 10. The proof is almost the same as in the GD setting (Theorem 8). We omit the
details here.
D.2.1 BEHAVIOR OF FTbV FOR η ∈ [0, c" 1og2 d]
In this section, we give the proof of Lemma 36. Recall the lemma as follows,
Lemma 36. Suppose σ is a large constant c1. Assume t ≥ c2d2 log2 (d), d ≥ c4 for some constants
c2, c4. There exist η1,η2,η3 = Θ(1∕t) with m < η < η3 and constant c5 such that
FTbV(η2) ≤ 2 llw*k2 - 10C + 亍
1	6	σ2	1
FTbV ㈤ ≥ 2 kw k - 10 C +T，∀η ∈ [0，ni] ∪[η3，c5 d2 log2(d)]
where C is a positive constant.
Recall that FTbV(η) = EP〜TESGD1/2 ∣∣wt,η - w*k2 + σ2∕2. Denote Q(η) :=
ESGD1/2 ∣∣wt,η 一 w*∣2. Recall that We truncate the SGD sequence once the weight norm
exceeds 4√Lσ. Due to the truncation, the expectation of 1/2 ∣wt,η — w*∣2 over SGD noise is very
tricky to analyze.
Instead, we define an auxiliary sequence {wτ0 ,η} that is obtained by running SGD on task P without
truncation and we first study Q0(η) := 1/2ESGD ∣∣wt,η 一 w*∣∣2. In Lemma 38, we show that with
high probability in the sampling of task P, the minimizer of Q0(η) is Θ(1/". The proof is very
similar as the proof of Lemma 13 except that we need to bound the SGD noise at step size η2. We
defer the proof into Section D.2.3.
Lemma 38. Given a task P, let {wτ0,η} be the weight obtained by running SGD on task P without
truncation. Choose σ as a large constant ci. Assume unroll length t ≥ c2 d for some constant c2.
With probability at least 1 — exp(-Ω(d)) over the sampling of task P, Vd/VL ≤ σi(Xtrain) ≤
Lda and 1/L ≤ λ%(Htrain) ≤ L for all i ∈ [n] and JdQ/4 ≤ ∣∣ξtraink ≤ √dσ and there exists
ηι, η2,η3 = Θ(1,t) with ηι < η2 < η3 SUCh that
Q0(η2) := 1/2ESGD ∣∣w0,η2 - w*∣∣2 ≤ 1 kw*k2 - C
Q0(η) := 1/2ESGD ∣∣w0,η - w*∣∣2 ≥ 1 ∣w*k2 - C2,∀η ∈ [0,ηι] ∪ 味，1/L]
where C is a positive constant.
To relate the behavior of Q0(η) defined on {wτ0 ,η} to the behavior of Q(η) defined on {wτ,η}.
We show when the step size is small enough, the SGD sequence gets truncated with very small
probability so that sequence {wτ,η} almost always coincides with sequence {wτ0,η}. The proof of
Lemma 39 is deferred into Section D.2.3.
Lemma 39. Given a task P, assume Vd/VL ≤ Qi(Xtrain) ≤ Lda and 1/L ≤ λi(H3说)≤ L for
all i ∈ [n] and ∖∕~dσ/4 ≤ ∣∣ξtrain k ≤ √dσ. Given any e > 0, suppose η ≤	42 32⑷4 for some
constant c5, we have
IQ(η) - Q0(η)l ≤ e∙
Combining Lemma 38 and Lemma 39, we give the proof of lemma 36.
Proof of Lemma 36. Recall that we define Q(η) := 1/2ESGD ∣∣wt,η - w*k2 and Q0(η)=
1/2ESGD ∣∣wt0,η —w* ∣∣2. Here, {wT η} is a SGD sequence running on task P without truncation.
According to Lemma 38, with probability at least 1 - exp(-Ω(d)) over the sampling of task P,
√d√L ≤ Qi(Xtrain) ≤ √L and 1/L ≤ λi(Htrain) ≤ L for all i ∈ [n] and √dσ∣A ≤ ∣∣ξtrai∏k ≤
57
Under review as a conference paper at ICLR 2021
√dσ and there exists η1,η2,η3 = Θ(1∕t) with ηι < η < η3 such that
Q0(η2) ≤ 1 kw*k2-C
Q0(η) ≥ 2 l∣w*k2 - C,∀η ∈ [0,ηι] ∪ [η3,1∕L]
where C is a positive constant. Call this event E . Suppose the probability that E happens is 1 - δ .
We can write EP〜TQ(η) as follows,
EP 〜T Q(η) = EP 〜T [Q(η)∣E ] Pr[E ]+ EP 〜T [Q(η)国 Pr[£].
According to the algorithm, We know ∣∣wt,η ∣∣ is always bounded by 4√Lσ. Therefore, Q(η):=
1/2 ∣∣wt,η 一 w*k2 ≤ 13Lσ2. By Lemma 39, we know conditioning on E, ∣Q(η) — Q0(η)∣ ≤ E
for any η ≤ 气心 匕；？5/0. AS long as t ≥ c2d2 log2(d∕e) for certain constant c2, we know η3 ≤
1
c5d2 log2(d∕e).
When η = η2 , we have
EP〜TQ(η2) ≤ (Q0(η2) + e) (1 一 δ) + 13Lσ2δ
≤ (1 ∣∣w*k2 — C + E)(1 — δ) + 13Lσ2δ
≤ 1 kw*k2- C +13Lσ2δ + E ≤ 1 ∣∣w*k2
where the last inequality assumes δ ≤ 2622 and E ≤ 品.
1
When η ∈ [0, η1] ∪ [η3,
c5d2 log2(d∕e)
], we have
9C
^10 ,
—
EP〜TQ(η2) ≥ (Q0(η) — e)(1 — δ) 一 13Lσ2δ
C
2
≥ (2 ∣w*k2
E (1 一 δ) 一
13Lσ2 δ
≥ 2 ∣w*k2
一 13Lσ2δ 一
E ≥ 2 kw*k2
6C
^10 ,
—
—
—
C
2
—
δ
2
—
where the last inequality holds as long as δ ≤ 28^2 and E ≤ 20.
According to Lemma 38, we know δ ≤ exp(-Ω(d)). Therefore, the conditions for δ can be satisfied
as long as d is larger than certain constant. The condition on E can be satisfied as long as η ≤
C5d2 lθg2(d) for some COnStant c5 .	D
D.2.2 Generalization for η ∈ [0,	1 2 A]
c5 d2 log2 d
in this section, we prove Lemma 37 by showing that FTbV (η) is point-wise close to FTbV (η) for all
η ∈ [0, c5d2 io；2(d/e)]. Recall Lemma 37 as follows.
Lemma 37. For any 1 > E > 0, assume σ is a constant andd ≥ c4 log(1∕E) for some constant c4.
There exists constant c5 such that with probability at least 1 — O(1∕e) exp(一Ω(E2m)),
∣Fτbv(η) — Fτbv(η)∣ ≤ e,
for a η ∈ [0, c5d2 i0g2(d∕e)].
T	1	T	Cr	f' .	1	.1	. i'	C 1	∙ .1	1 ∙	1	FF ∙ 1 ∙ .	7^S	/	∖ ∙	1	.
in order to prove Lemma 37, we first show that for afixedη with high probabilityFτbv (η) is close to
Fτbv (η). Similar as in Lemma 16, we can still show that each ∆τbv (η, P) is O(1)-subexponential.
The proof is deferred into Section D.2.3.
Lemma 40. Suppose σ is a constant. Given any 1 > E > 0, for any fixed η with probability at least
1 — exp(—Ω(E2m)),
IFTbV(η) — FTbV(η)∣ ≤ e.
58
Under review as a conference paper at ICLR 2021
Next, we show that there exists an -net for FTbV with size O(1/). By -net, we mean there exists a
finite set Ne of step sizes such that ∣FTbv (η) - FTbV (η0)∖ ≤ E for any η and η0 ∈ argminη∈Ne ∣η -
η’∖. The proof is very similar as in Lemma 17. We defer the proof of Lemma 41 into Section D.2.3.
Lemma 41. Suppose σ is a constant. For any 1 > E > 0, assume d ≥ c4 log(1/E) for some c4.
There exists constant c5 and an E-net Ne ⊂ [0,切?匕；25/0] for FTbV With ∖Ne∖ = O(1∕e). That
means for any η ∈ [0, C5d i0g2(d∕e) ],
∖FT bV (η) - FTbV (η0)∖ ≤ E,
forη0∈ arg minη∈N ∖η - η0∖.
Next, we show that with high probability, there also exists an E-net for FTbV with size O(1/E). The
proof is very similar as the proof of Lemma 18. We defer the proof into Section D.2.3.
Lemma 42. Suppose σ is a constant. For any 1 > E > 0, assume d ≥ c4 log(1/E) for some c4. With
probability at least 1 — exp(-Ω(e2m)), there exists constant c5 and an E-net Ne ⊂ [0, ©◎ klg2(d∕e)]
for Fτbv with ∖Ne∖ = O(1∕e). ThatmeanS,forany η ∈ [0, c式? h0g2(d∕e)],
0
∖FT bV (η) - FTbV (η )∖ ≤ E,
forη0∈ arg minη∈N ∖η - η0∖.
Combing Lemma 40, Lemma 41 and Lemma 42, now we give the proof of Lemma 37.
Proof of Lemma 37. The proof is almost the same as the proof of Lemma 11. We omit the details
here.
D.2.3 Proofs of Technical Lemmas
In Lemma 43, we show when the step size is small, the expected SGD noise square is well bounded.
The proof follows from the analysis in Lemma 33.
Lemma 43. Let {wτ0,η} be an SGD sequence running on task P without truncation. Let n0τ,η be the
SGD noise at WTm.Assume √d∕√L ≤ σi(Xtrain) ≤ √L√σ for all i ∈ [n] and ∣∣ξtrain∣∣ ≤ √dσ,
Suppose η ∈ [0, 2L13d], we have
ESGDn0τ,η2 ≤ 4L3σ2d
for all τ ≤ t.
Proof of Lemma 43. Similar as the analysis in Lemma 33, for η ≤ 方皆,We have
ESGD hn0τ,η2 ∖wτ0-1,ηi ≤L2d wτ0 -1,η - wtrain 2 .
and
ESGD IIwT-I η - Wtrainll ≤ (I - T^)T 1 kWtrain『≤ IIwtrain + (xtrain)^ξtrain || ≤ 4Lσ2 .
2L
Therefore, We have
ESGD IIn0T,η II2 ≤ L2dESGD IIwT0,η - wtrain II2 ≤ 4L3σ2d.
Proof of Lemma 38. We can expand Q0(η) as folloWs,
Q,(η) :=2ESGD l∣w0,η - w*『
1 II	t-1
=5ESGD Bt,ηwtrain + Bt,η (xtrain)^ξtrain - η ɪ2(I - ηHtTain)t ' TnT,η - w*
I	T=0
2
2
2 ∣Bt,ηwtrain - w*k2 + £ l∣Bt,η(Xtrain)tξtrain|l	+ -2ESGD X(I -声廿皿尸"nT,η
IT=0
+ Bt,η wt*rain - w*, Bt,η (Xtrain
59
Under review as a conference paper at ICLR 2021
Denote
G(η) := 2 kBt,ηwtain
1	2	η2	t-1
-w*『+ 2 ∣∣Bt,η (Xtrain )^ ξtrain 11 +~2 ESGD Iy^(I - ηHtrain)t 1 T nT,η
2
We first show that with probability at least 1 - exp(-Ω(d)), there exist η1, η2,η3 = Θ(1∕t) with
η1 < η2 < η3 such that G(η2) ≤ 1/2 ∣∣w*k2 - 5C/4 and G(η) ≥ 1/2 ∣∣w*k2 - C/4 for all
η ∈ [0,η1] ∪ [η3, 1/L].
According to Lemma 1, we know with probability at least 1 - exp(-Ω(d)), √d∕√L ≤ σi (Xtrain) ≤
√L√d and 1/L ≤ λi (Htrain) ≤ L for all i ∈ [n]. According to Lemma 45, We know with probability
at least 1 — exp(-Ω(d)), √dσ∕4 ≤ kξtraink ≤ √dσ.
Upper bounding G(η2): We can expand G(η) as follows:
2
G(η) : = 2 kBt,ηWtrain
-w"∣2 + 2 ∣∣Bt,η(Xtrain)*ξtrain∣∣ + ɪESGD X(I - ηHtrain)t~1~τn0τ,η
1	1	1	2 η2	∣ t-1
2 ∣∣w*∣2 + 2 kBt,η Wtraink2 + 2 HBt,η (Xtrain)'ξtrain ∣∣ + ɪ ESGD £(I - ηHtrain)t-1-τ nT,η
∣τ=0
- hBt,η Wtrain, W i .
Same as in Lemma 13, we know 2 ∣∣Bt,ηWtraink2 + 2 ∣∣Bt,η(Xtrain)tξtrain∣∣2 ≤ L3n2t2σ2. For the
SGD noise, by Lemma 43 we know ESGD ∣∣nT,η∣∣2 ≤ 4L3σ2d for all T ≤ t as long as η ≤ ɪ^.
Therefore,
2
ηr Esgd
t-1
X(I - ηHtrain)t-1-τ n0τ,η
τ=0
≤ η22 XEsgd ∣∣nT,η∣∣2 ≤ 2L3η2σ2dt ≤ 2L3n2σ2t2,
τ=0
where the last inequality assumes t ≥ d. According to Lemma 15, for any fixed η ∈ [0, L/t], with
probability at least 1 - exp(-Ω(d)) over Xtrain,
hBt,η Wtrain,w*i
ηt
≥ ----
- 16L
Therefore, for any step size η ≤ 2l^ ,
G(η) ≤ 1 kw*k2 + 3L3n2b2t2 - τηt- ≤ 1 ∣∣w*k2 - n^j-,
2	16L	2	32L
where the second inequality holds as long as η ≤ 96L4σ2t. Choosing η2 := 96L4σ2t that is smaller
than 2L13d assuming t ≥ d. Then, we have
G(η2) ≤ 2 ∣∣w*k2 —4~^,
where constant C = 3072L5σ2.
Lower bounding G(η) for η ∈ [0,η1 ] : Now, we prove that there exists η1 = Θ(1∕t) with
η1 < η2 such that for any η ∈ [0,η1], G(η) ≥ ɪ ∣∣w*∣2 - C. Recall that
1	1	1	2 η2	∣ t-1
G(η) =2 ∣∣w*∣2 + 2 kBt,η Wtraink2 + 2 HBt,η (Xtrain)tξtrain ∣∣ + ɪ ESGD E(I - ηHtrain)t-1-τ nT,η
∣τ=0
2
- hBt,η Wtrain, W i .
≥ 1 llW*k2 - hBt,ηWtrain,W*i ∙
Same as in Lemma 13, by choosing η1 = 缶,we have for any η ∈ [0, η1 ],
G(η) ≥ 2 llW*k2 - ɪ.
60
Under review as a conference paper at ICLR 2021
t-1	∣2
X(I-ηHtrain)t-1-τn0τ,η∣∣
τ=0	∣
Lower bounding G(η) for η ∈ [η3,1/L]: Now, We prove that there exists η3 = Θ(1∕t) with
η3 > η2 such that for all η ∈ [η3, 1/L],
G(η) ≥ 2 l∣w*k2 - 4.
Recall that
G(η) =2 kBt,ηWtrain- w*k2 + 2 ∣∣Bt,η(Xtrain)*ξtrain∣∣ + ^2ESGD
≥2 ∣∣Bt,η (Xtrain),ξtrain∣∣ .
Same as in Lemma 13, by choosing η3 = log(2)L∕t, as long as σ ≥ 8√L, we have
G(η) ≥ 1 kw*k2
for all η ∈ [η3, 1/L]. Note η3 ≤ 1/L as long as t ≥ log(2)L2 .
Overall, we have shown that there exist ηι, n2,n3 = Θ(1∕t) with ηι < η < η3 such that G(η2) ≤
1/2 ∣∣w*k2 — 5C∕4 and G(η) ≥ 1/2 ∣∣w*∣2 — C/4 for all η ∈ [0,ηι] ∪ [η3,1∕L]. Recall that
Q0(η) = G(η) + (Bt,ηWtrain - W*, Bt,η(Xtrain)*Strain). Choosing e = C/4 in Lemma 14, we know
with probability at least 1 — exp(-Ω(d)), |沼,〃Wtrain- w*,Bt,η(Xtrain)Strain)∣ ≤ C/4 for all
η ∈ [0, 1/L]. Therefore, we know Q0(η2) ≤ 1/2 ∣W* ∣2 - C and Q0(η) ≥ 1/2 ∣W* ∣2 - C/2 for all
η ∈ [0,η1] ∪ [η3, 1/L].
In order to prove Lemma 39, we first construct a super-martingale to show that as long as task P
is well behaved, with high probability in SGD noise, the weight norm along the trajectory never
exceeds 4√Lσ.
Lemma 44. Assume Vd/VL ≤ σi(Xtrain) ≤ √Ld and 1/L ≤ λ%(Htrain) ≤ L for all i ∈ [n] and
yj~dσ/4 ≤ ∣∣ξtrain ∣∣ ≤ √dσ. Given any 1 > δ > 0, suppose η ≤ ©炉 口®® for some constant c5,
with probability at least 1 - δ in the SGD noise,
∣∣wT,η∣∣ < 4√Lσ
for all τ ≤ t.
Proof of Lemma 44. According to the proofs of Lemma 43, as long as η ≤ ɪ^, we have
ESGD [∣∣wt,η - wtrain∣∣ ∣w0-1,/ ≤ (I- 2L) ∣∣w0-1,η - wtrain∣∣ .
Since log is a concave function, by Jenson’s inequality, we know
ESGD [log ∣∣w0,η - Wtrain∣∣2 lw0-1,η]
≤ log ESGD ∣∣Wt0,η-Wtrain∣∣	|Wt-1,/ ≤ log ∣∣ Wt-1,η	- Wtrain ∣∣	+	log(1	-	~2L).
Defining Gt = log ∣∣Wt,η 一 Wtrain∣∣2 一 t log(1 一 号),we know Gt is a super-martingale. Next, we
bound the martingale differences.
We can bound |Gt - ESGD[Gt∣W0-ι,η]| as follows,
|Gt - ESGD[Gt∣W0-m ]∣≤ max log ( ∣∣ (I-")(WtTn-Wtram )-ηntτ,η∣∣2 1
nt-1,η,nt-1,η	∣∣(I - ηHtrain)(Wt0-1,η -Wtrain) -ηn0t0-1,η∣∣
We can expand ∣∣(I - ηHtrain)(Wt0-1,η - Wtrain) - ηn0t-1,η∣∣2 as follows,
∣∣(I - η Htrain)(Wt0 -1,η - Wtrain) - ηn0t-1,η ∣∣
= ∣∣(I - η Htrain)(Wt0 -1,η - Wtrain)∣∣ - 2η n0t-1,η, (I - ηHtrain)(Wt0-1,η - Wtrain)) + η ∣∣n0t-1,η ∣∣
61
Under review as a conference paper at ICLR 2021
We can bound the norm of the noise as follows,
n0t-1,η = xi(t-1)xi(t-1) (wt0-1,η - wtrain) - Htrain(wt0-1,η - wtrain)
≤ xi(t-1)xi(t-1) (wt0-1,η - wtrain) + Htrain(wt0-1,η - wtrain)
≤ (Ld + L) wt0-1,η - wtrain ≤ 2Ld wt0
-1,η - wtrain ,
where the second inequality uses ||xi(t-i)∣∣ ≤ √Ld. Therefore, We have
2η n0t-1,η, (I - ηHtrain)(wt0-1,η - wtrain)	≤ 4Lηd	wt0
-1,η - wtrain | ,
η2 llnt-1,ηl∣2 ≤ 4L2η2d2 ∣lw0-1,η - wtrai∏l∣2 .
This further implies,
IGt- ESGD[GtIw0-i,η]|
≤ log l ll(I - ηHtrai∏)(wt-i,η - wtrai∏)『+ (4L∏d + 4L2η2d2) llw0-1,η - wtrain l !
ll(I - ηHtrai∏)(wt0-1,η - wtrai∏)ll2 - 4Lηd llwt0
-1,η - wtrai∏ l
8Lηd + 4L2η2d2	2 2 2
≤ log 1 + ∩~~57——≤j-6L	≤ 16Lnd + 8L2η2d ,
(1 - 2Lη - 4Lηd)
where the seco∏d i∏equality uses ll(I - ηHtrai∏)(wt0-1,η - wtrai∏)ll2	≥	(1	-
2Lη) llwt0-1,η - wtrai∏ ll2 . The last i∏equality assumes η ≤ l2Ld a∏d uses ∏umerical i∏equal-
ity log(1 + x) ≤ x. Assumi∏g η ≤ 1/(Ld), we further have IGt - ESGD[GtIwt0-1,η]I ≤ L2ηd.
By Azuma's inequality, we know with probability at least 1 - δ∕t,
Gt ≤ Go + L2 V2tnd log(t∕δ).
Plugging in Gt =lθgllwt,η - wtrainH2-t lθg(1- 2L) and Go = log kwo - wtrain∣∣2 = log Ilwtraink2 ,
we have
log llwt,η - WtrainH2 ≤ log Ilwtraink2 + t lθg(1 —著)+ L2√2tndlθg(t∕δ)
,	2L
≤ log Ilwtraink2 - ɪt + L2√2tnd log(t∕δ).
2L
This implies,
；,n - wtrainll2 ≤kwtraink2 exp 卜(一2t + L2√2log(t∕δ)d√t
=Ilwtraink2 exp (O(d2 log2(d∕δ))n)
≤ IwtrainI2 exp (2∕3) ,
where the second inequality assumes n ≤ %召京⑷台)for Some constant c5. Furthermore, since
IIwtraink ≤ (1 + √L)σ,wehave llw0,ηll ≤ (1 + e1/3) Ilwtraink < 4√Lσ.
Overall, We know as long as n ≤ *2^5®, with probability at least 1 - δ∕t, ∏w0,η n ≤ 4√Lσ.
Since this analysis also applies to any τ ≤ t, we know for any τ, with probability at least 1 - δ∕t,
HwT,ηll < 4√Lσ. Taking a union bound over T ≤ t, We have with probability at least 1 - δ,
HwT,ηH < 4√Lσ for all T ≤ t.	□
Proof of Lemma 39. Let E be the event that HwT,/l < 4√Lσ for all T ≤ t. We first show that
ESGD ∣∣wt,η - w* ∣∣2 is close to ESGD llw0,η - w* ll2 ɪ {E}. It’s not hard to verify that
ESGD ∣∣wt,η - w*∣∣2 = ESGD Hw0,η - w*ll2 1 {E} + Ilu - w*∣∣2 Pr[同，
62
Under review as a conference paper at ICLR 2021
where u is a fixed vector with norm 4√Lσ. By Lemma 44, We know Pr[£] ≤ e/(25Lσ2) as long as
η ≤ c5d2 io；2(d/e)for some constant c5. Therefore, we have
IEsgd ∣∣wt,η - w*∣∣2 - ESGD ∣∣wt,η - w*∣∣2 1 {E}∣ ≤ J
Next, we show that ESGD ∣∣w0,η 一 w* ∣∣21 {E} is close to ESGD ∣∣wt,η 一 w*∣∣2. For any 1 ≤ T ≤ t,
let Eτ be the event that ∣∣wT,η∣∣ ≥ 4√Lσ and ∣∣wT0 4 ∣∣ < 4√Lσ for all T0 < τ. Basically ET means
the weight norm exceeds the threshold at step T for the first time. It,s easy to see that ∪T=1Eτ = £.
Therefore, we have
ESGD ∣∣w0,η - w*∣∣2 = ESGD ∣∣w0,η - w*∣∣2 1 {E} + XXESGD ∣∣w0,η - w*∣∣2 1 {&}.
T=1
Conditioning on ET, We know ∣∣ wT-ι,η ∣∣ < 4√Lσ. Since we assume √d ≤ σi (Xtrain) ≤ √L√d for
all i ∈ [n] and ξtrai∏ ≤ √dσ, we know ∣∣wtrai∏k ≤ 2√Lσ. Therefore, we have ∣∣wT-ι,η - Wtrain∣∣ ≤
6√Lσ. Recall the SGD updates,
wT,η - wtrain = (I - ηHtrain)(wT-1,η - wtrain) - ηnT -1,η .
For the noise term, we have η ∣n0T-1,η ∣ ≤ 2ηLd ∣wT0 -1,η - wtrain∣ that is at most ∣wT0 -1,η - wtrain∣
assuming η ≤ 2Ld. Therefore, we have ∣∣wT,η - Wtrain∣∣ ≤ 2 ∣∣wT-ι,η — Wtrain∣∣ ≤ 12√Lσ. Note
that event ET is independent with the SGD noises after step T. Therefore, according to the previous
analysis, we know as long as η ≤ ^^,
ESGD ∣Wt,η - Wtrain ∣	|ET ≤ ∣WT,η - Wtrain ∣	≤ 2L σ .
Then, we can bound ESGD ∣∣Wt0
,η -
W* ∣∣2 |ETi as follows,
ESGD
=ESGD
≤ESGD
∣∣Wt0,η - W*∣∣2 |ET
∣Wt,η - Wtrain + Wtrain - W ∣	|ET
∣∣Wt0,η	- Wtrain∣∣2	|ET	+	2ESGD	∣∣Wt0,η	-	Wtrain∣∣	|ET	∣Wtrain - W*∣ + ∣Wtrain - W*∣2
≤2L2σ2 + 2 ∙ 2Lσ ∙ 3√Lσ + 9Lσ2 ≤ 3L2σ2.
Therefore, we have
tt
XESGDIKn- w*∣∣2 l{Eτ} = XEsgd [∣∣w0,η - w*∣∣2 印 P*]
T=1	T=1
t
≤3L2σ2 X Pr[Eτ] = 3L2σ2 Pr[E] ≤ 3L2σ2e.
T=1
This then implies that ∣Esgd ∣∣wt,n - w*∣∣2 - ESGD ∣∣w0,n - w*∣∣2 1 {E}∣ ≤ 3L2σ2j.
Finally, we have
∣∣∣ESGD ∣Wt,η - W*∣2 - ESGD ∣∣Wt0,η - W* ∣∣2∣∣∣
≤ Esgd ∣∣wt,n - w*∣∣2 - ESGD ∣∣w0,n - w*∣∣2 1 {E}∣ + ∣Esgd ∣∣w0,n - w*∣∣2 - ESGD ∣∣w0,n - w*∣∣2 1 {E}∣
≤ (3L2σ2 + 1) j
as long as η ≤ * 电壮左).Therefore, ∣Q(η) - Q0(η)∣ ≤ (3L2σ2 + 1)j/2. Choosing j0 =
(3L2σ2+i) finishes the proof.	口
63
Under review as a conference paper at ICLR 2021
Proof of Lemma 40. Recall that
mm
FTbV (η) := —X δTbV (n，P) = X ESGD 2 ∣∣w(k) - w(kιid∣∣ (k)
mm	H
k=1	k=1	valid
Similar as in Lemma 11, we can show 2 ∣∣w(k) - W(Rd∣∣ (廿 is O⑴-SUbexPonential,whichimplies
Hvalid
ESGD2 ∣∣w(k) - w(klid∣∣ (fc) is。⑴-SUbexPonentiaL Therefore, FTbV(η) is the average of m i.i.d.
Hv(ali)d
O(1)-sUbexponential random variables. By standard concentration ineqUality, we know for any
1 > e > 0, with probability at least 1 - eχp(-Ω(e2m)),
IFTbV(η) - FTbV(η)∣ ≤ e.

Proof of Lemma 41. Recall that
FTbV(η) =EP〜TEsgd2 ∣∣wt,η - w*k2 + σ2∕2
We only need to construct an e-net for EP〜TESGD2 ∣∣wt,η - w*k2. Let E be the event that
√d∕√L ≤ σi(Xtrain) ≤ √Ld and 1/L ≤ λi(H↑xain) ≤ L for all i ∈ [n] and √dσ∕4 ≤ ∣∣ξtrai∏k ≤
√dσ We have
EP〜TEsgd2 ∣∣wt,η - w*∣∣2
12	12
=Ep〜T 2ESGD ∣∣wt,η - w k |E Pr[E] + EP〜T 2ESGD ∣∣wt,η - W k |E Pr[同
According to Lemma 39, we know conditioning on E,
2ESGD kwt,η - w*∣∣2 - 2ESGD ∣∣w0,η - w*∣f ≤ e,
as long as η ≤	匕；？®/0. Note {wT,η} is the SGD sequence without truncation.
For the second term, we have
EP〜T 1 Esgd ∣wt,η - w*『|E Pr[E] ≤ 13Lσ2 Pr[E] ≤ e,
where the last inequality assumes Pr[£] ≤ ɪɪ^ ∙ According to Lemma 1 and Lemma 45, we
know Pr[E] ≤ exp(-Ω(d)). Therefore, given any e > 0, we have Pr[同 ≤ ɪɪ^ as long as
d ≥ c4 log(1∕e) for some constant c4.
Then, we only need to construct an e-net for EP〜T
in Lemma 33, it’s not hard to prove
[l Esgd ∣∣w0,η 一 w*∣∣2 |e] Pr[E ]. By the analysis
∂~Ep〜T [2Esgd ∣∣w0,η - w*∣∣2 |e] Pr[E] = O(1)t(1 - 2L)t-1,
η	2	2L
for all n ∈ [0, c5d2 b0g2(d∕e)]. Similar as in Lemma 14, for any e > 0, we know there exists an e-net
Ne with size O(1∕e) such that for any η ∈ [0,W 1og2(d∕e)],
Ep〜T 1 EsGD∣∣w0,η - w*∣∣2∣E Pr[E] - EP〜T ∣Esgd ∣∣wt,η, - w*∣∣2∣E Pr[E] ≤ e
for n0 ∈ argminη∈N"η - η0∣.
Combing with the bounds on 12 ESGDIlWt,η - w*∣∣2 1 {E}- 1ESGD ∣∣w0,η - w[f { {E}|and
Ep〜T h 1 Esgd ∣∣wt,η - w*k2 |3] Pr[E], we have for any η ∈ [0, c5d2 log2(d/e)],
FTbV (η) - FTbV (η0) ≤ 4e
64
Under review as a conference paper at ICLR 2021
for η0 ∈ arg mi□η∈ Ne ∣η - η0∣. We finish the proof by replacing 4e by e0.	□
Proof of Lemma 42. The proof is very similar as the proof of Lemma 18. The only difference is that
we need to first relate the SGD sequence with truncation to the SGD sequence without truncation
and then bound the Lipschitzness on the SGD sequence without truncation (as we did in Lemma 41).
We omit the details here.	□
E	Tools
E.1 Norm of random vectors
We use the following lemma to bound the noise in least squares model.
Lemma 45 (Theorem 3.1.1 in Vershynin (2018)). Let X = (X1,X2, ∙∙∙ , Xn) ∈ Rn be a random
vector with each entry independently sampled from N (0, 1). Then
Pr[∣kxk - √n∣ ≥ t] ≤ 2exp(-t2∕C2),
where C is an absolute constant.
E.2 Singular values of Gaussian matrices
Given a random Gaussian matrix, in expectation its smallest and largest singular value can be
bounded as follows.
Lemma 46 (Theorem 5.32 in Vershynin (2010)). Let A be an N × n matrix whose entries are
independent standard normal random variables. Then
√N-√n ≤ Esmin(A) ≤ Esmax (A) ≤ √N + √n
Lemma 47 shows a lipchitz function over i.i.d. Gaussian variables concentrate well on its mean.
We use this lemma to argue for any fixed step size, the empirical meta objective concentrates on the
population meta objective.
Lemma 47 (Proposition 5.34 in Vershynin (2010)). Let f be a real valued Lipschitz function on
Rn with Lipschitz constant K. Let X be the standard normal random vector in Rn . Then for every
t ≥ 0 one has
t2
Pr[f (X) - Ef(X) ≥ t] ≤ exp(-5记).
2K2
The following lemma shows a tall random Gaussian matrix is well-conditioned with high probability.
The proof follows from Lemma 46 and Lemma 47. We use Lemma 48 to show the covariance matrix
is well conditioned in the least squares model.
Lemma 48 (Corollary 5.35 in Vershynin (2010)). Let A be an N × n matrix whose entries are
independent standard normal random variables. Then for every t ≥ 0 with probability at least
1 - 2 exp(-t2∕2) one has
√n - √n - t ≤ smin (A) ≤ smax(A) ≤ √N + √n + t
E.3 Johnson-Lindenstrauss lemma
We also use Johnson-Lindenstrauss Lemma in some of the lemmas. Johnson-Lindenstrauss Lemma
tells us the projection of a fixed vector on a random subspace concentrates well as long as the
subspace is reasonably large.
Lemma 49 (Johnson & Lindenstrauss (1984)). Let P be a projection in Rd onto a random n-
dimensional subspace uniformly distributed in Gd,n . Let z ∈ Rd be a fixed point and > 0, then
with probability at least 1 - 2 exp(-c2n),
(1-e) Jd kzk≤kPzk≤ (1 + E)	kzk .
65
Under review as a conference paper at ICLR 2021
F	Experiment details
We describe the detailed settings of our experiments in Section F.1 and give more experimental
results in Section F.2.
F.1 Experiment settings
Optimizing step size for quadratic objective In this experiment, we meta-train a learning rate
for gradient descent on a fixed quadratic objective. Our goal is to show that the autograd mod-
ule in popular deep learning softwares, such as Tensorflow, can have numerical issues when using
the log-transformed meta objective. Therefore, we first implement the meta-training process with
Tensorflow to see the results. We then re-implement the meta-training using the hand-derived meta-
gradient (see Eqn 3) to compare the result.
A general setting for both implementations is as follows. The inner problem is fixed as a 20-
dimensional quadratic objective as described in Section 3, and we use the log-transformed meta
objective for training. The positive semi-definite matrix H is generated by first sampling a 20 × 20
matrix X with all entries drawn from the standard normal distribution and then setting H = XTX.
The initial point w0 is drawn from standard normal as well. Note that we use the same quadratic
problem (i.e., the same H and w0) throughout the meta-training. We do 1000 meta-training iter-
ations, and collect results for different settings of the initial learning rate η0 and the unroll length
t.
We first implement the meta-training code with Tensorflow. Our code is adapted from Wichrowska
et al. (2017) 2. We use their global learning rate optimizer and specify the problem set to have
only one quadratic objective instance. We implemented the quadratic objective class ourselves (the
”MyQuadratic” class). We also turned off multiple advanced features in the original code, such as
attention and second derivatives, by assigning their flags as false. This ensures that the experiments
have exactly the same settings as we described. The meta-training learning rate is set to be 0.001,
which is of similar scale as our next experiment. We also try RMSProp as the meta optimizer, which
alleviates some of the numerical issues as it renormalizes the gradient, but our experiments show
that even RMSProp is still much worse than our implementation.
We then implement the meta-training by hand to show the accurate training results that avoid nu-
merical issues. Specifically, we compute the meta-gradient using Eq (3), where we also scaled the
numerator and denominator as described in Claim 2 to avoid numerical issues. We use the algorithm
suggested in Theorem 4, except We choose the meta-step size to be 1/(100√k) as the constants in
Theorem 4 were not optimized.
Train-by-train vs. train-by-validation, synthetic data In this experiment, We find the optimal
learning rate η* for least-squares problems trained in train-by-train and train-by-validation settings
and then see hoW the learning rate Works on neW tasks.
Specifically, We generate 300 different 1000-dimensional least-squares tasks With noise as defined
in Section 4 for inner-training and then use the meta-objectives defined in Eq (1) and (2) to find the
optimal learning rate. The inner-training number of steps t is set as 40. We try different sample sizes
and different noise levels for comparison. Subsequently, in order to test how the two η* (for train-
by-train and train-by-validation respectively) Work, We use them on 10 test tasks (the same setting
as the inner-training problem) and compute training and testing root mean squared error (RMSE).
Note that since we only need the final optimal η* found under the two meta-objective settings (re-
gardless of how we find it), we do not need to actually do the meta-training. Instead, we do a grid
search on the interval [10-6, 1], which is divided log-linearly to 25 candidate points. For both the
train-by-train and train-by-validation settings, we average the meta-objectives over the 300 inner
problems and see which η minimizes this averaged meta-objective.
Train-by-train vs. train-by-validation, MLP optimizer on MNIST To observe the trade-off
between train-by-train and train-by-validation in a broader and more realistic case, we also do ex-
2Their open source code is available at https://github.com/tensorflow/models/tree/
master/research/learned_optimizer
66
Under review as a conference paper at ICLR 2021
periments to meta-train an MLP optimizer as in Metz et al. (2019) to solve the MNIST classification
problem. We use part of their code 3 to integrate with our code in the first experiment, and we use
exactly the same default setting as theirs, which is summarized below.
The MLP optimizer is a trainable optimizer that works on each parameter separately. When doing
inner-training, for each parameter, we first compute some statistics of that parameter (explained be-
low), which are combined into a feature vector, and then feed that feature vector to a Muti-Layer
Perceptron (MLP) with ReLU activations, which outputs two scalars, the update direction and mag-
nitude. The update is computed as the direction times the exponential of the magnitude. The feature
vector is 31-dimensional, which includes gradient, parameter value, first-order moving averages
(5-dim), second-order moving averages (5-dim), normalized gradient (5-dim), reciprocal of square
root second-order moving averages (5-dim) and a step embedding (9-dim). All moving averages
are computed using 5 different decay rates (0.5, 0.9, 0.99, 0.999, 0.9999), and the step embedding
is tanh distortion of the current number of steps divided by 9 different scales (3, 10, 30, 100, 300,
1000, 3000, 10000, 300000). After expanding the 31-dimensional feature vector for each parameter,
we also normalize the set of vectors dimension-wise across all the parameters to have mean 0 and
standard deviation 1 (except for the step embedding part). More details can be found in their original
paper and original implementation.
The inner-training problem is defined as using a two-layer fully connected network (i.e., another
“MLP”) with ReLU activations to solve the classic MNIST 10-class classification problem. We use
a very small network for computational efficiency, and the two layers have 100 and 20 neurons. We
fix the cross-entropy loss as the inner-objective and use mini-batches of 32 samples when inner-
training.
When we meta-train the MLP optimizer, we use exactly the same process as fixed in experiments
by Wichrowska et al. (2017). We use 100 different inner problems by shuffling the 10 classes and
also sampling a new subset of data if we do not use the complete MNIST data set. We run each of
the problems with three inner-training trajectories starting with different initialization. Each inner-
training trajectory is divided into a certain number of unrolled segments, where we compute the
meta-objective and update the meta-optimizer after each segment. The number of unrolled segments
in each trajectory is sampled from 10 + Exp(30), and the length of each segment is sampled from
50 + Exp(100), where Exp(∙) denotes the exponential distribution. Note that the meta-objeCtive
computed after each segment is defined as the average of all the inner-objectives (evaluated on the
train/validation set for train-by-train/train-by-val) within that segment for a better convergence. We
also do not need to log-transform the inner-objective this time because the cross entropy loss has a
log operator itself. The meta-training, i.e. training the parameters of the MLP in the MLP optimzier,
is completed using a classic RMSProp optimizer with meta learning rate 0.01.
For each settings of sample sizes and noise levels, we train two MLP optimizer: one for train-by-
train, and one for train-by-validation. When we test the learned MLP optimizer, we use similar
settings as the inner-training problem, and we run the trajectories longer for full convergence (4000
steps for small data sets; 40000 steps for the complete data set). We run 5 independent tests and
collect training accuracy and test accuracy for evaluation. The plots show the mean of the 5 tests.
We have also tuned a SGD optimizer (with the same mini-batch size) by doing a grid-search of the
learning rate as baseline.
F.2 Additional results
Optimizing step size for quadratic objective We try experiments for the same settings of the
initial η0 and inner training length t for all of three implementations (our hand-derived GD version,
Tensorflow GD version and the Tensorflow RMSProp version). We do 1000 meta-training steps for
all the experiments.
For both Tensorflow versions, we always see infinite meta-objectives ifη0 is large ort is large, whose
meta-gradient is usually treated as zero, so the training get stuck and never converge. Even for the
case that both η0 and t is small, it still has very large meta-objectives (the scale of a few hundreds),
and that is why we also try RMSProp, which should be more robust against the gradient scales. Our
3Their code is available at https://github.com/google- research/google-research/
tree/master/task_specific_learned_opt
67
Under review as a conference paper at ICLR 2021
Table 1: Whether the implementation converges for different t (fixed no = 0.1)
t	10	20	40	80
Ours	"X	X	X	X
Tensorflow GD	X	×	×	×
Tensorflow RMSProp	X	X	×	×
Table 2: Whether the implementation converges for different no (fixed t = 40)
no	0.001	0.01	0.1	1
Ours	X	X	X	X
Tensorflow GD	×	×	×	×
Tensorflow RMSProp	X	X	×	×
hand-derived version, however, does not have the numerical issues and can always converge to the
optimal n*. The detailed convergence is summarized in Tab 1 and Tab 2. Note that the optimal n* is
usually around 0.03 under our settings.
Train-by-train vs. train-by-validation, MLP optimizer on MNIST We also do additional ex-
periments on training an MLP optimizer on the MNIST classification problem. We first try using all
samples under the 20% noised setting. The results are shown in Fig 8. The train-by-train setting can
perform well if we have a large data set, but since there is also noise in the data, the train-by-train
model still overfits and is slightly worse than the train-by-validation model.
------SGD
------TbT60000
------TbV50000+10000
0	0.5	1	1.5	2	2.5	3	3.5	4	0
Steps	×104
(lsəl) Aoffl,Jnoo<
5 8 5 7 5
80706
Ooo
0.5	1	1.5	2	2.5	3	3.5	4
Steps	x104

Figure 8: Training and testing accuracy for different models (all samples, 20% noise)
We then try an intermediate sample size 12000. The results are shown in Fig 9 (no noise) and Fig
10 (20% noise). We can see that as the theory predicts, as the amount of data increases (from 1000
samples to 12000 samples and then to 60000 samples) the gap between train-by-train and train-by-
validation decreases. Also, when we condition on the same number of samples, having additional
label noise always makes train-by-train model much worse compared to train-by-validation.
68
Under review as a conference paper at ICLR 2021
Figure 9: Training and testing accuracy for different models (12000 samples, no noise)
Figure 10: Training and testing accuracy for different models (12000 samples, 20% noise)
69