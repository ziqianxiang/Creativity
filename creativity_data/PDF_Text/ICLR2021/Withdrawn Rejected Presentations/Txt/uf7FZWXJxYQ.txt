Under review as a conference paper at ICLR 2021
Communication Efficient Primal-Dual Algo-
rithm for Nonconvex Nonsmooth Distributed
Optimization
Anonymous authors
Paper under double-blind review
Ab stract
Decentralized optimization problems frequently appear in the large scale machine
learning problems. However, few works work on the difficult nonconvex nonsmooth
case. In this paper, we propose a decentralized primal-dual algorithm to solve
this type of problem in a decentralized manner and the proposed algorithm can
achieve an O(1/2) iteration complexity to attain an -solution, which is the
well-known lower iteration complexity bound for nonconvex optimization. To
our knowledge, it is the first algorithm achieving this rate under a nonconvex,
nonsmooth decentralized setting. Furthermore, to reduce communication overhead,
we also modifying our algorithm by compressing the vectors exchanged between
agents. The iteration complexity of the algorithm with compression is still O(1/2).
Besides, we apply the proposed algorithm to solve nonconvex linear regression
problem and train deep learning model, both of which demonstrate the efficiency
and efficacy of the proposed algorithm.
1 Introduction
Decentralized machine learning problems have attracted many attentions as the growing size of the
data. Usually, the problem will be solved on a network. We define represent the network by an
undirected graph G = (V, E), where V is a node set and E is the set of edges. We denote |V | = N
and |E | = M. Then, the problem is formulated as follows:
min f (X) = -1 X fi(Xi)
xN
i=1
s.t. Xi = Xj , ∀(i, j) ∈ E
where fi : Rn → R is a non-convex but smooth function,Xi ∈ Rn and X ∈ RNn =
(XT, xt, ∙∙∙ , XN)T. Moreover, to embed some prior knowledge or tackle over-fitting problem,
some regularization terms will come into the formulation. Besides, when considering deep neural
network compression problem(He et al., 2017), some nonsmooth term will also be formulated in the
objective function. Then, the problem becomes:
1N
min	M Tfi(Xi) + hi(xi)
X1,∙∙∙ ,XN N Ad
i=1
s.t. xi =xj, ∀(i, j)	∈ E.
(1)
where hi can be a non-smooth function(e.g. L1 regularization or indicator function of some constraint
on x), and here we assume hi is a convex function. In general, we can use different regularization in
the different nodes, however, because of the consensus constraint xi = xj , we can combine all the hi
to the first node, then without loss of generality we use h1 (x) = h(x) and the rest of hi are zeros.
Because of intolerable computation complexity for higher-order algorithms, first-order methods are
popular for solving large-scale problems. In general, to solve problem 1, there are two different types
of first-order methods. One is the gradient descent based method, where each node performs some
gradient steps and then average xi with its neighbors. Some works change the gradient steps by
1
Under review as a conference paper at ICLR 2021
adding momentum (Yu et al., 2019) or adaptive learning rate (Nazari et al., 2019) and some works
change the average step other average schemes of xi (e.g. weighted average (Tang et al., 2019)). The
other kind of method is the primal-dual based method, where dual variables(y) are introduced into the
algorithm and using primal-dual type methods to solve the saddle points of the resulting Lagrangian
function. The primal-dual methods consist of two parts, the primal update, and the dual update. The
primal step is to minimize the Lagrangian function by a local update of primal variables and the dual
step is to perform a dual ascent by using the consensus residual. The primal-dual method is usually
more efficient than the gradient-based method, which is well-studied in convex cases (Chang et al.,
2014) and nonconvex smooth cases (Hong et al., 2017). Also in Hong & Luo (2017), they prove
that the ADMM algorithm, which is a primal-dual method, converges for consensus problem with a
nonsmooth term in the centralized setting, where a central node controls the consensus, and achieve
the O(1/2) iteration complexity. However, it is still unknown whether we can use the primal-dual
method to solve problem 1 with the decentralized setting. More importantly, to our knowledge it is
still unclear whether there exists a decentralized first-order optimization algorithm solving problem
1 that can achieve the O(1/2) iteration complexity, which is the well-known lower bound for the
iteration complexity for solving nonconvex optimization problems using first-order method (Carmon
et al., 2019).
Furthermore, the success of large models such as deep neural networks, communication among nodes
becomes an important factor influencing the speed of the optimization algorithms. A popular strategy
to reduce communication complexity is to compress the vectors exchanged by neighbor nodes. Many
compression functions are used in these scenarios such as quantization functions and sparsification
functions. Therefore, another important problem is whether we can design a communication efficient
algorithm for solving problem 1 with low communication complexity.
In this paper, we give affirmative answers to both of the above two problems.
Concretely speaking, we proposed a smoothed proximal-primal dual algorithm for solving problem 1
with a nonconvex nonsmooth setting. The algorithm can achieve an -solution of problem 1 within
O(1/2) iteration complexity, which, to our knowledge, is the first algorithm achieving the lower
iteration complexity bound for nonconvex nonsmooth optimization. Furthermore, to reduce the
communication cost, we use a compressor when nodes communicate with each other. We prove that
our algorithm with compression of information exchanged between neighbor nodes can also achieve
an O(1/2) iteration complexity and an O(1/2) communication complexity if the compression is in
sufficient high accuracy.
2	Related Work
Distributed optimization methods have been studied for many years. For convex cases, many
algorithms are solving distributed optimization problems, including the distributed subgradient
method (Nedic & Ozdaglar, 2009), consensus ADMM methods (Chang et al., 2014; Shi et al., 2014).
Recently, nonconvex distributed optimization problems have attracted more attention.
Yuan et al. (2016) extends the DGD algorithm to the nonconvex smooth case and the iteration
complexity is O(1/4). Lian et al. (2017) gives the convergence analysis of a gradient-based algorithm
under the nonconvex but smooth setting and the iteration complexity is O(1/3). Di Lorenzo
& Scutari (2016) proposes a gradient-based algorithm for solving problems with the nonconvex
nonsmooth setting, they show the result will converge in the infinite time, but they don’t give the
iteration complexity of the algorithm.
On the other hand, it is well-known in convex cases, primal-dual algorithms are usually more efficient
than gradient-based methods(Lan et al., 2020; Scaman et al., 2018). Though primal-dual methods are
well-studied for convex problems, the convergence of primal-dual algorithms for nonconvex cases
is still unclear in many problems. Recently, some papers analyze the primal-dual algorithms for
nonconvex cases. Hong et al. (2017) give the primal-dual algorithm and show the convergence under
the nonconvex setting with the optimal order. For nonconvex, nonsmooth settings, Hong & Luo
(2017) analyze the ADMM algorithm for a special type of graph with one a center point. They prove
that the iteration complexity is also O(1/2). To our knowledge, there is no paper in the literature
achieving the optimal iteration complexity O(1/2) iteration complexity for nonconvex, nonsmooth
decentralized optimization.
2
Under review as a conference paper at ICLR 2021
Later, to reduce the overhead of communication, Tang et al. (2018) introduce the compression
functions into decentralized gradient descent, and still give the iteration complexities at order O(-3),
but the algorithm only works for carefully designed compressors. Tang et al. (2019) and Koloskova
et al. (2019) give the algorithms working for more general compression functions. But still the
iteration complexity is O(-3). In the primal-dual setting, to our best knowledge, there is not any
research that gives the algorithm or convergence results.
Different from previous work, we propose a primal-dual method that can achieve the optimal
rate under the nonconvex nonsmooth setting. Besides, by adding the compression function in the
communication, we reduce the communication overhead and show it will not hurt the convergence
speed.
3	Algorithm
To solve the problem 1, we first reformulate it into a linearly constrained problem with equality
constraints.
We define a matrix W ∈ RM×N. The kth row of W represents the kth edge of graph. If (i, j) is the
kth edge in the graph, we set Wk,i = 1, Wk,j = -1, and the rest entries in the kth row are zeros.
Then, We rewrite the constraint as Ax = 0, where A = W 0 In. The problem 1 will become
min f(x) + h(x)
x
s.t.Ax = 0.
Then, the Lagrange function of the problem is defined as follows:
L(x, y) = f(x) + h(x) + yTAx.
We need to solve the saddle points of this Lagrange function. To solve the Lagrange function, people
usually add (variant) augmented terms to give more convexity. However, the inclusion of augmented
terms in the Lagrange function still can not guarantee the theoretical convergence of algorithms for
nonconvex-nonsmooth problems. Moreover, the augmented terms in the Lagrange function will result
in more communication costs when exchanging information. Inspired by Zhang & Luo (2020), we
use a proximal framework instead. In any iteration, we include a proximal term to the Lagrange
function centered at an auxiliary sequence, which is an exponentially weighted sequence zt of the
primal iterates. Then we consider the following strongly convex counterpart of problem 1:
min max K(x, y, z) + h(x),	(2)
x,z y
where K(x, y, Z) = f(x) + yτAx + P ∣∣x 一 z∣∣2.
The Algorithm 1 shows the steps to solve the problem 2. In each iteration, first, we update xt by
proximal gradient method. Then, we communicate the new xt+1 with the neighbors. After that,
we update y by gradient ascent with step size α and update z by gradient descent with step size β.
By calculating VχK(x, y, Z) = Vf (x) + ATy + p(x 一 z), it can be shown that in the iteration of
updating X only ATy are needed. So we do not need to store y directly, we store μ = ATy instead.
On the other hand variable y is in RMn, which is not easy to divide into N blocks, but μ is in RNn.
Thus, we use μ, instead of y. As all x, μ, Z can be divided into N blocks, and we give the algorithm.
Algorithm 1 Distributed Primal-Dual Algorithm
Select c > 0, α > 0, 0 < β ≤ 1, and P ≥ 0;
Initialize x0, X0 = x0, μ0 and z0;
for t = 0,1, 2,…，T do
xt+1 = argminxi (hVxiK (xt,yt,zt) ,Xi - Xti + hi (xi) + 21c∣∣Xi - xt∣∣2);
Send xit+1 to N (i) and receive xtj+1 from j ∈ N (i);
μt+1 = μt + α (diχt+1 - Pj∈N(i) xj+1);
zt+1 = Zt + β (xt+1-zt);
end for
To reduce the communication overhead, we add a compression function at each iteration when
communicating with its neighbors. Then we give the final algorithm in Algorithm 2.
3
Under review as a conference paper at ICLR 2021
Algorithm 2 Communication Efficient Distributed Primal-Dual Algorithm
Select c > 0, α > 0, 0 < β ≤ 1, P ≥ 0 and compression function Q (∙);
Initialize x0, X0 = x0, μ0 and z0;
for t = 0,1, 2,…，T do
xt+1 = argminxi (hVχiK (xt,yt, zt) ,xi - Xti + hi (xi) + 2cIIxi- xt∣∣* 1 2 3);
Send Q (x；+1 - xt) to N (i) and receive Q (xj+1 — xj) from j ∈ N (i);
xt+1 = xi- Q (xt+1 - xi)；
xj+1 = xi - Q (xj+1 - xj), for j ∈ N (i);
μt+1 = μt + α (dixi+1 - Pj∈N (i) xj+1);
zt+1 = zt + β (xt+1-zt);
end for
4	Theoretical Analysis
In this section, we present the convergence result of the Algorithm 2.
4.1 The stationary solution of problem 1
First, we give the definition of stationary point and the approximate stationary points of problem 1.
We say that x is a (first-order) stationary point of problem 1 if there exists a y such that
0 ∈ ∂h(x) + Vf (x) + ATy
Ax = 0
We then define the -stationary point as follows:
Definition 1. (x,y) is an E-stationary point if √= IlAxIl ≤ E and there exists V, such that V ∈
Vf (x) + ∂h(x) + ATy and √N∣∣ν∣ ≤ e.
Remark 1. Definition 1 is a sufficient condition for the E-solution in Hong et al. (2017); Tang et al.
(2019).
4.2 Assumptions
Next, we state our assumptions used in the theoretical analysis. We first give some assumptions for
the function f and the regularization function h.
Assumption 1. For the function f and h, we assume:
1. There exists f, f (x) + h (x) ≥ f holds for all x ∈ RNn.
2. Function fi is a differential function with Lipschitz continuous gradient, i.e., for all i, we
have
IVfi (x) -Vfi(x0)I ≤LIx-x0I, ∀x, x0 ∈Rn.
3. hi is a convex function.
We then give the assumption for the compression function.
Assumption 2. Compression function Q (∙) satisfies thefollowing inequality:
IQ (x) - xI ≤ (1 - δ) IxI for some δ > 0, and ∀x ∈ Rn
For the communication network, We also need to assume that it is connected so that the information
can be sent through different nodes.
Assumption 3. The graph G is connected.
These assumptions are standard in the literature.
4
Under review as a conference paper at ICLR 2021
4.3	The main theoretical result
Under the above assumptions, we have the following main theoretical result:
Theorem 1. Suppose the parameters C ≤ 上,P > —L/N, and α, β are sufficiently small. Then it
holds thatforany T > 0, there exists S ∈ {0,1,…,T — 1} such that (xs+1, ys) is a B/VT-solution,
where B = O (L J(Φ (x0, y0, z0) 一 f) max{4c, 8, Pe}), where & is the Lipschitz constant of
K.
Remark 2. The values of α and β are related to N, p, δ, c, L and connectivity of the graph. The
upper bound of α and β can be calculated by some inequalities which are defined in the proof of the
theorem in the Appendix.
Remark 3. Using the result in the Theorem 1, to achieve E-stationary point, O(*) iterations are
needed, which is the well-known lower bound of iteration complexity for the nonconvex case.
Corollary 1. Suppose we choose the parameters C = -N, P = 2L/N, α
Lδ2
24λι(5δ2+9∕4(1-δ)2))
and β = 17α+4L(2+√3)κ, where K is a constant related to the connection of the graph. For any
T > 0, there exists S ∈ {0,1, ∙∙∙ ,T — 1}, such that (xs+1,ys) is a Bι∕√T-solution, where
Bi = O (J® (x0,y0,z0)- f) (√L + √l )户F2+1)), where γ is a
connection of graph, .
constant related to
Remark 4. For the smooth case where h(∙) = 0, Y is just the spectral gap ofthe Laplacian matrix
defined as the division of the largest eigenvalue with the smallest non-zero eigenvalue. For nonsmooth
case where h(∙) = 0, Y will also depend on the node corresponding to xi. In thefollowing table, we
calculate the γ for some graph in smooth case and nonsmooth case respectively as follows:
Graph Structure	Smooth	Nonsmooth
Ring(10 nodes)	3.9021	-10.4721-
Grid(3 × 3)	6	33.9973
Complete(10 nodes)	1	10
Star(10 nodes)	10	10
Table 1: Different Y under different settings.
4.4	Proof sketch of Theorem 1
To prove Theorem 1, we will give some important lemmas and the full proof can be found in the
Appendix. The idea of the proof is to construct a “proximal-primal-dual ” potential function, which
is bounded below and decreases along the iteration sequence and uses it to show the convergence on
xt, Axt and zt. Remember the function K(xt,y t, zt) is defined as:
f(xt) + yτ Axt + P ∣∣xt — ztk2,
which contains the primal information. We first need to define the dual function and the proximal
function. We let
d (y, z) = min K (x,y, z) + h (x) ,
x∈RnN
x (y, z) = arg min K (x, y, z) + h (x) ,
x∈RnN
M (Z)=	„min n(f(x) + h (X) + P Ilx - zk2),
x∈RnN,Ax=0	2
x* (z) = arg min	(f (x) + h (x) + PIlx — z∣2).
x∈RnN, Ax=0	2
Then we construct the potential function as follows:
Φ(x,y, z) = K (x, y, z) + h (x) — 2d (y, z) + 2M (z) ,
5
Under review as a conference paper at ICLR 2021
which is a linear combination of the primal function, dual function and the proximal function. It
is not hard to show that the function Φ(χ, y, Z) is bounded below by f. Therefore, in the rest, We
hope to show that Φ(xt, yt, zt) decreases sufficiently after any iteration and hence prove that the
optimization residuals can go to zero. We first have the following basic estimate:
Lemma 1. Under Assumptions above, for any t > 0, it always holds that
Φ (χt,yt,zt) - Φ (χt+1,yt+1,zt+1)
≥ 1-k ∣∣xt+1 - xtk2 - α (AXt+1)T Axt+1 + 泰 ∣∣zt - zt+1k2 + 2α (AXt+1)T Ax (yt, Zt)
-α2^λ1 kAXt+1k2 + p (zt+1 - Zt)T (zt+1 + Zt- 2x (yt+1,zt+1))
σ4
+ 2p (zt+1 - Zt)T (Zt- x* (Zt)) -pL∣zt - zt+1k2,
where λι is the largest eigenvalue of AT A, σ4 is a Constant related to P and L, and L is a Constant
related to the Lipschitz constant of Z.
Lemma 1 gives a basic estimate of the change of the potential function during the iteration. According
to Lemma 1, we need to bound the negative terms that appear in the basic estimate. The framework
of the algorithm is based on the proximal algorithm. Therefore, the negative terms come from the
error of estimating x* (Zt) in any iteration. The error consists of two parts: the dual error and the
compression error. The following two lemmas show that we can bound the two error terms by the
optimization residuals. The first error bound is a dual bound.
Lemma 2. For all y ∈ RMn and all Z ∈ RNn, it holds that
∣x(y, Z) - x*(Z)∣ ≤ σ1∣Ax(y,Z)∣,
where σ1 related to the p, L and the graph property on the first node.
The remaining part is to deal with the compression and we bound compression error by the difference
on xt and xt+1, which we give in the following:
Lemma 3. With the definition of X and X in Algorithm 2, the following equality always holds:
T	2T
X kχt- xtk2 ≤ ¼-lX kχt-χT∣2.
t=1	t=1
Then we can prove the Theorem 1, using the error bound in lemma 2 and lemma 3.
5	Experimental Results
In this section, we will give the experimental results for the algorithm. We compare our algorithm with
a gradient descent based algorithm (Koloskova et al., 2019), and Prox_PDA (Hong et al., 2017) on
two tasks. First, we implement the algorithms to optimize the nonconvex linear regression objective,
and then we implement the algorithm to train a neural network.
5.1	Nonconvex Linear Regression Case
To show the efficacy of our algorithm, we start with a simple function. We use f (x) = 2 ∣∣AiX-bi∣2 +
2
pn=ι χ2++ι, which is formulated from linear regression with a regularization. In the experiment we
randomly generate matrices Ai and vectors bi . The network structure used in the experiment is a ring
with 10 nodes. With 10 times repeat, we give the following results.
In the figure 1, we only optimize with function f, and give the result on the consensus of xi(i.e.
PN=IkXi - X∣∣) and the norm of gradient of PN=I fi(x), where X = 芸 PN=I Xi. For the full
precision version, the primal-dual methods can converge much faster than the gradient descent
method, and can converge to more accurate solution. Besides, our algorithm converge a little bit
faster than Prox_PDA.
6
Under review as a conference paper at ICLR 2021
Figure 1: Results on Smooth and Full Precision Settings.
Figure 2: Results on Smooth and Compressed Settings.
On the other hand, when taking consideration of compression, we choose a commonly use compres-
sion function top_K sparsification function. We select K as 5 and give the experimental results of
different algorithms in Fig. 2. As it is shown in Fig. 2, the primal-dual method can have a better
solution than the gradient based method. Besides, as Prox_PDA has the augmented terms which will
be affected by the compression, it converges slower than ours.
Figure 3: Results on Nonsmooth Settings.
7
Under review as a conference paper at ICLR 2021
Then, we add non-smooth term into objective, we use h(x) = ∞I(kxk ≥ 1). For the gradient-based
method and Prox_PDA, we simply extend the method by adding a projection after each update of x.
We still calculate the consensus of x. To estimate the solution , wedoa gradient descent on X with
gradient of PN=I fi (χ) and then project it into unit ball. We denote the result of gradient projection
as X and use the ∣∣X - Xk to estimate solution. We show the results for the nonconvex nonsmooth
objective in Fig. 3. In the Fig. 3, the upper line is the results with full precision communication, and
the bottom line is the results with compressed communication. From the results, it can be seen that
although we only use projection in one node, our algorithm performs better than gradient descent and
Prox_PDA.
5.2	Neural Network Case
In this section, we will give the result on training ResNet-18(He et al. (2016)) on the dataset
CIFAR10(Krizhevsky et al. (2009)). Still, we use the top_K sparsification function as a compression
function, and we set K as 10% × n. We use a ring with 10 nodes as the communication network
structure. Besides, we use the same learning rate in all algorithms. Then the results are given in
Fig. 4 and Fig. 5. Fig. 4 shows the results with full precision communication, it can be seen that
because we use the large proximal term, we will converge slower than SGD and Prox_PDA at the
beginning iterations, but finally our algorithm becomes faster and can get a better solution than those
two methods in 200 epochs. Besides, in Fig. 5, we show the result with compressed communication.
Because Prox_PDA fails to converge, we do not draw the curve of Prox_PDA. Although we converge
a little bit slower than SGD because of the inexact of dual variables in the initial phase, we can get
higher accuracy compared to the SGD.
Figure 4: Results on CIFAR10 with Full Precision Communication.
Figure 5: Results on CIFAR10 with Compressed Communication.
6	Conclusion
In this paper, we proposed a primal-dual based algorithm to solve distributed optimization problems.
To reduce the communication overhead we use compression function during the communication.
We show that under the nonconvex nonsmooth case the algorithm can converge to the -stationary
point with O( ±) iterations, which is a well-known lower bound for nonconvex optimization. The
experimental results on nonconvex linear regression and the deep neural network show the efficacy of
the proposed algorithm.
8
Under review as a conference paper at ICLR 2021
References
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points i. Mathematical Programming, pp. 1-50, 2019.
Tsung-Hui Chang, Mingyi Hong, and Xiangfeng Wang. Multi-agent distributed optimization via
inexact consensus admm. IEEE Transactions on Signal Processing, 63(2):482-497, 2014.
Paolo Di Lorenzo and Gesualdo Scutari. Next: In-network nonconvex optimization. IEEE Transac-
tions on Signal and Information Processing over Networks, 2(2):120-136, 2016.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European conference on computer vision, pp. 630-645. Springer, 2016.
Yihui He, Xiangyu Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks.
In Proceedings of the IEEE International Conference on Computer Vision, pp. 1389-1397, 2017.
Mingyi Hong and Zhi-Quan Luo. On the linear convergence of the alternating direction method of
multipliers. Mathematical Programming, 162(1-2):165-199, 2017.
Mingyi Hong, Davood Hajinezhad, and Ming-Min Zhao. Prox-pda: The proximal primal-dual
algorithm for fast distributed nonconvex optimization and learning over networks. In International
Conference on Machine Learning, pp. 1529-1538, 2017.
Anastasia Koloskova, Sebastian U Stich, and Martin Jaggi. Decentralized stochastic optimization
and gossip algorithms with compressed communication. arXiv preprint arXiv:1902.00340, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Guanghui Lan, Soomin Lee, and Yi Zhou. Communication-efficient algorithms for decentralized and
stochastic optimization. Mathematical Programming, 180(1):237-284, 2020.
Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. Can decentralized
algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic
gradient descent. In Advances in Neural Information Processing Systems, pp. 5330-5340, 2017.
Parvin Nazari, Davoud Ataee Tarzanagh, and George Michailidis. Dadam: A consensus-based
distributed adaptive gradient method for online optimization. arXiv preprint arXiv:1901.09109,
2019.
Angelia Nedic and Asuman Ozdaglar. Distributed subgradient methods for multi-agent optimization.
IEEE Transactions on Automatic Control, 54(1):48-61, 2009.
Kevin Scaman, Francis Bach, SebaStien Bubeck, LaUrent MaSSOUlia and Yin Tat Lee. Optimal
algorithms for non-smooth distributed optimization in networks. In Advances in Neural Information
Processing Systems, pp. 2740-2749, 2018.
Wei Shi, Qing Ling, Kun Yuan, Gang Wu, and Wotao Yin. On the linear convergence of the admm in
decentralized consensus optimization. IEEE Transactions on Signal Processing, 62(7):1750-1761,
2014.
Hanlin Tang, Shaoduo Gan, Ce Zhang, Tong Zhang, and Ji Liu. Communication compression for
decentralized training. In Advances in Neural Information Processing Systems, pp. 7652-7662,
2018.
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce Zhang, Tong Zhang, and Ji Liu. Deepsqueeze
: Parallel stochastic gradient descent with double-pass error-compensated compression. arXiv
preprint arXiv:1907.07346, 2019.
Hao Yu, Rong Jin, and Sen Yang. On the linear speedup analysis of communication efficient
momentum sgd for distributed non-convex optimization. arXiv preprint arXiv:1905.03817, 2019.
Kun Yuan, Qing Ling, and Wotao Yin. On the convergence of decentralized gradient descent. SIAM
Journal on Optimization, 26(3):1835-1854, 2016.
Jiawei Zhang and Zhi-Quan Luo. A proximal alternating direction method of multiplier for linearly
constrained nonconvex minimization. SIAM Journal on Optimization, 30(3):2272-2302, 2020.
9