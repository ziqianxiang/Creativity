Under review as a conference paper at ICLR 2021
Neural Time-Dependent Partial Differential
Equation
Anonymous authors
Paper under double-blind review
Ab stract
Partial differential equations (PDEs) play a crucial role in studying a vast number
of problems in science and engineering. Numerically solving nonlinear and/or high-
dimensional PDEs is frequently a challenging task. Inspired by the traditional finite
difference and finite elements methods and emerging advancements in machine
learning, we propose a sequence-to-sequence learning (Seq2Seq) framework called
Neural-PDE, which allows one to automatically learn governing rules of any time-
dependent PDE system from existing data by using a bidirectional LSTM encoder,
and predict the solutions in next n time steps. One critical feature of our proposed
framework is that the Neural-PDE is able to simultaneously learn and simulate
all variables of interest in a PDE system. We test the Neural-PDE by a range
of examples, from one-dimensional PDEs to a multi-dimensional and nonlinear
complex fluids model. The results show that the Neural-PDE is capable of learning
the initial conditions, boundary conditions and differential operators defining
the initial-boundary-value problem of a PDE system without the knowledge of
the specific form of the PDE system. In our experiments, the Neural-PDE can
efficiently extract the dynamics within 20 epochs training and produce accurate
predictions. Furthermore, unlike the traditional machine learning approaches for
learning PDEs, such as CNN and MLP, which require great quantity of parameters
for model precision, the Neural-PDE shares parameters among all time steps, and
thus considerably reduces computational complexity and leads to a fast learning
algorithm.
1	Introduction
The research of time-dependent partial differential equations (PDEs) is regarded as one of the most
important disciplines in applied mathematics. PDEs appear ubiquitously in a broad spectrum of
fields including physics, biology, chemistry, and finance, to name a few. Despite their fundamental
importance, most PDEs can not be solved analytically and have to rely on numerical solving methods.
Developing efficient and accurate numerical schemes for solving PDEs, therefore, has been an active
research area over the past few decades (Courant et al., 1967; Osher & Sethian, 1988; LeVeque;
Cockburn et al., 2012; Thomas, 2013; Johnson, 2012). Still, devising stable and accurate schemes with
acceptable computational cost is a difficult task, especially when nonlinear and(or) high-dimensional
PDEs are considered. Additionally, PDE models emerged from science and engineering disciplines
usually require huge empirical data for model calibration and validation, and determining the multi-
dimensional parameters in such a PDE system poses another challenge (Peng et al., 2020).
Deep learning is considered to be the state-of-the-art tool in classification and prediction of nonlinear
inputs, such as image, text, and speech (Litjens et al., 2017; Devlin et al., 2018; LeCun et al., 1998;
Krizhevsky et al., 2012; Hinton et al., 2012). Recently, considerable efforts have been made to employ
deep learning tools in designing data-driven methods for solving PDEs (Han et al., 2018; Long et al.,
2018; Sirignano & Spiliopoulos, 2018; Raissi et al., 2019). Most of these approaches are based on
fully-connected neural networks (FCNNs), convolutional neural networks(CNNs) and multilayer
perceptron (MLP). These neural network structures usually require an increment of the layers to
improve the predictive accuracy (Raissi et al., 2019), and subsequently lead to a more complicated
model due to the additional parameters. Recurrent neural networks (RNNs) are one type of neural
network architectures. RNNs predict the next time step value by using the input data from the current
1
Under review as a conference paper at ICLR 2021
and previous states and share parameters across all inputs. This idea (Sherstinsky, 2020) of using
current and previous step states to calculate the state at the next time step is not unique to RNNs. In
fact, it is ubiquitously used in numerical PDEs. Almost all time-stepping numerical methods applied
to solve time-dependent PDEs, such as Euler’s, Crank-Nicolson, high-order Taylor and its variance
Runge-Kutta (Ascher et al., 1997) time-stepping methods, update numerical solution by utilizing
solution from previous steps.
This motivates us to think what would happen if we replace the previous step data in the neural
network with numerical solution data to PDE supported on grids. It is possible that the neural network
behaves like a time-stepping method, for example, forward Euler’s method yields the numerical
solution at a new time point as the current state output (Chen et al., 2018). Since the numerical
solution on each of the grid point (for finite difference) or grid cell (for finite element) computed
at a set of contiguous time points can be treated as neural network input in the form of one time
sequence of data, the deep learning framework can be trained to predict any time-dependent PDEs
from the time series data supported on some grids if the bidirectional structure is applied (Huang et al.,
2015; Schuster & Paliwal, 1997). In other words, the supervised training process can be regarded
as a practice of the deep learning framework to learn the numerical solution from the input data, by
learning the coefficients on neural network layers.
Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) is a neural network built upon
RNNs. Unlike vanilla RNNs, which suffer from losing long term information and high probability of
gradient vanishing or exploding, LSTM has a specifically designed memory cell with a set of new
gates such as input gate and forget gate. Equipped with these new gates which control the time to
preserve and pass the information, LSTM is capable of learning long term dependencies without the
danger of having gradient vanishing or exploding. In the past two decades, LSTM has been widely
used in the field of natural language processing (NLP), such as machine translation, dialogue systems,
question answering systems (Lipton et al., 2015).
Inspired by numerical PDE schemes and LSTM neural network, we propose a new deep learning
framework, denoted as Neural-PDE. It simulates multi-dimensional governing laws, represented by
time-dependent PDEs, from time series data generated on some grids and predicts the next n time
steps data. The Neural-PDE is capable of intelligently processing related data from all spatial grids by
using the bidirectional (Schuster & Paliwal, 1997) neural network, and thus guarantees the accuracy
of the numerical solution and the feasibility in learning any time-dependent PDEs. The detailed
structures of the Neural-PDE and data normalization are introduced in Section 3.
The rest of the paper is organized as follows. Section 2 briefly reviews finite difference method for
solving PDEs. Section 3 contains detailed description of designing the Neural-PDE. In Section 4
and Appendix A of the paper, we apply the Neural-PDE to solve four different PDEs, including
the 1-dimensional(1D) wave equation, the 2-dimensional(2D) heat equation, and two systems of
PDEs: the invicid Burgers’ equations and a coupled Navier Stokes-Cahn Hilliard equations, which
widely appear in multiscale modeling of complex fluid systems. We demonstrate the robustness of
the Neural-PDE, which achieves convergence within 20 epochs with an admissible mean squared
error, even when we add Gaussian noise in the input data.
2	Preliminaries
2.1	Time Dependent Partial Differential Equations
A time-dependent partial differential equation is an equation of the form:
∂u
Ut = f (xι,…，u, ∂χ-,
∂u	∂ 2u
∂xn ∂x1 ∂x1
∂2u
∂x1∂xn
∂ nu
∂xι …∂xn
(2.1.1)
where u = u(t, x1, ..., xn) is known, xi ∈ R are spatial variables, and the operator f maps R 7→ R.
For example, consider the parabolic heat equation: ut = α2∆u, where u represents the temperature
and f is the Laplacian operator ∆. Eq. (2.1.1) can be solved by finite difference methods, which is
briefly reviewed below for the self-completeness of the paper.
2
Under review as a conference paper at ICLR 2021
2.2	Finite Difference Method
Consider using a finite difference method (FDM) to solve a two-dimensional second-order PDE of
the form:
Ut= f(χ,y,Uχ,Uy,Uχχ,Uyy), (χ,y) ∈ Ω ⊂ R2, t ∈ R+ ∪{0} ,	(221)
with some proper boundary conditions. Let Ω be Ω = [x。，xb] X [y。，yb], and
uin,j = u(xi , yj , tn )	(2.2.2)
where tn = nδt, 0 ≤ n ≤ N, and δt = NT for t ∈ [0, T], and some large integer N. Xi = iδx, 0 ≤
i ≤ Nx, δx = XaN-Xb for x ∈ [x。, Xb]. yj = jδy, 0 ≤ j ≤ Ny, δy = yN-yb for y ∈ [y。, yb]. Nx
and Ny are integers.
The central difference method approximates the spatial derivatives as follows (Thomas, 2013):			
UX (xi , yj , t) =	二 2δx(Ui+1j	- Ui-1,j) + O(δx2) ,	(2.2.3)
Uy(xi, yj, t) =	= 21y (Uij+1	- Ui,j-1) + O(δy2) ,	(2.2.4)
UXX (xi, yj , t) =	二 δX2(Ui+1j	- 2Ui,j + Ui-1,j) + O(δx2 ) ,	(2.2.5)
Uyy (xi, yj, t) =	=δy2 (Uij+1	- 2Ui,j + Ui,j-1) + O(δy2) .	(2.2.6)
To this end, the explicit time-stepping scheme to update next step solution un+1 is given by:
Unj ≈ Un+1 = Unj + δtf (Xi, yj, Unj, Unj-1, Unj+1, Ui+ι,j, Un-ι,j) ,	(2.2.7)
≡ F(Xi, yj,δx, δy, δt, Unj, Unj-1, Unj+1, Un+ιj, Un-ιj) ,	(2.2.8)
Apparently, the finite difference method (2.2.7) for up-
dating un+1 on a grid point relies on the previous time
steps, solutions, supported on the grid point and its neigh-
bours. The scheme (2.2.7) updates 硬，1 using four points
of Un values (see Figure 1). Similarly, the finite element
method (FEM) approximates the new solution by calculat-
ing the corresponded mesh cell coefficient (see Appendix),
which is updated by its related nearby coefficients on the
mesh. From this perspective, one may regard the numeri-
cal schemes for solving time-dependent PDEs as methods
catching the information from neighbourhood data of in-
terest.
3	Proposed Method
Figure 1: updating scheme for central
difference method
3.1	Mathematical Motivation
Recurrent neural network including LSTM is an artificial
neural network structure of the form (Lipton et al., 2015):
ht = σ(Whxxt + Whhht-I + bh) ≡ σ0,(xt, ht-1) ≡ σb(x0, x1, x2,…,xt) ,	(3.1.1)
where xt ∈ Rd is the input data of the tth state and ht-1 ∈ Rh denotes the processed value in its
previous state by the hidden layers. The output yt of the current state is updated by the current state
value ht:
yt =σ(Whyht+by)	(3.1.2)
≡ σc(ht) ≡ σd(x0, x1, x2,…，Xt) .	(3.1.3)
Here WhX ∈ Rh×d, Whh ∈ Rh×h, Why ∈ Rh×h are the matrix of weights, vectors bh, by ∈ Rh
are the coefficients of bias, and σ, o。，σb, σc, σd are corresponded activation and mapping functions.
3
Under review as a conference paper at ICLR 2021
With proper design of input and forget gate, LSTM can effectively yield a better control over
the gradient flow and better preserve useful information from long-range dependencies (Graves &
Schmidhuber, 2005).
Now consider a temporally continuous vector function u ∈ Rn given by an ordinary differential
equation with the form:
du(t)
-~dt- = g(Ui)).
(3.1.4)
Let un = u(t = nδt), a forward Euler’s method for solving u can be easily derived from the Taylor’s
theorem which gives the following first-order accurate approximation of the time derivative:
dun
dt
un+1 - un
δt
+ O(δt) .
(3.1.5)
Then we have:
-=g(u)  -----→ un+1 = Un + δt g(un) + O(δt2)
dt
→ un+1 = f1(un) = f1 ◦ f1。…f1(u0)	(3.1.6)
'------{z------}
n
Here Un ≈ u(nδt) is the numerical approximation and f1 ≡ un + δt g(un) : Rn → Rn. Combining
equations (3.1.1) and (3.1.6) one may notice that the residual networks, recurrent neural network and
also LSTM networks can be regarded as a numerical scheme for solving time-dependent differential
equations if more layers are added and smaller time steps are taken. (Chen et al., 2018)
Canonical structure for such recurrent neural network usually calculate the current state value by its
previous time step value ht-1 and current state input xt. Similarly, in numerical PDEs, the next step
data at a grid point is updated from the previous (and current) values on its nearby grid points (see
Eq. 2.2.7).
Thus, what if we replace the temporal input ht-1 and xt with spatial information? A simple sketch
of the upwinding method for a 1d example of u(x, t):
ut + νux = 0	(3.1.7)
will be:
un+1 = Un -Vit (Un -un-1)+o(δx, δt) → un+1 = f2(*, Un)	(3.i.8)
δx
≡ fθ(fη(xi, hi-1(u))) = fθ,η (Un,说,…，U-1,U) = v"	(3.1.9)
Xi = un, hi-1(U) = σ(U-1, hi-2(U)) ≡ fη(如,说,端，…，U-1).	(3.1.1O)
Here let vn+1 be the prediction of un+1 processed by neural network. We replace the temporal
previous state ht-1 with spacial grid value hi-1 and input the numerical solution Un ≈ u(iδx, nδt)
as current state value, which indicates the neural network could be seen as a forward Euler method
for equation 3.1.7 (LU et al., 2018). Function f2 ≡ Un 一 V卷(Un 一 Un-1) : R → R and the function
fθ represents the dynamics of the hidden layers in decoder with parameters θ, and fη specifies the
dynamics of the LSTM layer (Hochreiter & Schmidhuber, 1997; Graves & Schmidhuber, 2005)
in encoder withe parameters η. The function fθ,η simulates the dynamics of the Neural-PDE with
paramaters θ and η . By applying Bidirectional neural network, all grid data are transferred and it
enables LSTM to simulate the PDEs as :
vn+1 = fθ(fη(hi+1 (U),Un, hi-1(U)))	(3.1.11)
hi+1(U) ≡ fη (Un+1,Un+2,Un+3,…愁)	(3.1.12)
For a time-dependent PDE, if we map all our grid data into an input matrix which contains the
information of δx, δt, then the neural network would regress such coefficients as constants and will
learn and filter the physical rules from all the k mesh grids data as:
v"+1 = fθ,η(站,说,弱,…，&n)	(3.1.13)
The LSTM neural network is designed to overcome the vanishing gradient issue through hidden
layers, therefore we use such recurrent structure to increase the stability of the numerical approach in
deep learning. The highly nonlinear function fθ,η simulates the dynamics of updating rules for Uin+1,
which works in a way similar to a finite difference method (section 2.2) or a finite element method.
4
Under review as a conference paper at ICLR 2021
Figure 3: Neural-PDE
3.2	Neural-PDE
In particular, We use the bidirectional LSTM (Hochreiter &
SChmidhuber, 1997; Graves & Schmidhuber, 2005) to better
retain the state information from data on grid points which
are neighbourhoods in the mesh but far away in input matrix.
The right frame of Figure 3 shows the overall de-
sign of the Neural-PDE. Denote the time series data
at collocation points as QN, QN,…，QN with QN =
[u0, Ui, ∙ ∙ ∙ , UN] at ith point. The superscript represents
different time points. The Neural-PDE takes the past
states {aN, QN ,…，QN} of all collocation points, and out-
puts the predicted future states {bM, bM,…，bM}, where
bM = [vN+1, VN+2,…,VN+m] is the Neural-PDE predic-
tion for the ith collocation point at time points from N + 1
to N + M. The data from time point 0 to N are the training
data set.
The Neural-PDE is an encoder-decoder style sequence
model that first maps the input data to a low dimensional
latent space that
hi = LSTM(Qi)㊉ LSTM(Qi),	(3.2.1)
where ㊉ denotes concatenation and hi is the latent embed-
ding of point Qi under the environment.
2d PDE data at time t
P
10
5
0
-5
-10
5
m0,留,u2,…,或]
Figure 2: An example of maping 2d
data matrix into 1d vector where k =
Nx × Ny and Nx and Ny are the num-
bers of grid points on x and y, respec-
tively.
One then decoder, another bi-lstm with a dense layer:
Vi = (LSTM(hi)㊉ LsTM(hi)) ∙ W,
(3.2.2)
where W is the learnable weight matrix in the dense layer.
During training process, mean squared error (MSE) loss L is used as we typically don’t know the
specific form of the PDE.
N+M k
L = X X l∣Ut - VtIl2 ,	(3.2.3)
t=N +1 i=1
3.3	Data Initialization and Grid Point Reshape
In order to feed data into our sequence model framework, we map the PDE solution data onto a
K × N matrix, where K ∈ Z+ is the dimension of the grid points and N ∈ Z+ is the length of
the time series data on each grid point. There is no regularization for the input order of the grid
points data in the matrix because of the bi-directional structure of the Neural-PDE. For example, a 2d
5
Under review as a conference paper at ICLR 2021
	1d Wave	2d Heat	2d Burgers’	Fluid System
MSE	7.4444E - 5	7.0741E - 6	1.4018E - 5	6.1631E - 7
Table 1: Neural-PDE shows very small test MSE on 4 different PDEs.
	Allen-Cahn	Burgers
PINN	-7.0E - 3-	6.7E - 4
Neura-PDE	-2.9E - 5-	2.4E - 5
Table 2: Neural-PDE outperforms baseline in test MSE on 1d Allen-Cahn and Burgers equations.
heat equation at some time t is reshaped into a 1d vector (See Fig. 2). Then the matrix is formed
accordingly.
For a n-dimensional time-dependent partial differential equation with K collocation points, the input
and output data for t ∈ (0, T) will be of the form:
aN 一
.
.
.
A(K,N )= aN
.
.
.
aNK
^bM 一
.
.
.
B(K,M) = bM
U0 u1 …
.	..
...
..	.
UO U' …
.	..
...
..	.
UK UK …
vN+1 vN+2
..
..
..
vN+1 VN+2
Un …UN
Un …UN
N+m
v`
N+M
vK
(3.3.1)
(3.3.2)
N+1
K
vKN+2
N+m
K
v
v
Here N = T and each row ' represents the time series data at the 'th mesh grid, and M is the time
length of the predicted data.
By adding Bidirectional LSTM encoder in the Neural-PDE, it will automatically extract the informa-
tion from the time series data as:
B(K, M) = PDESolVer(A(K, N)) = PDESolVer(a0N, af,…aN,…，aNN)	(3.3.3)
4	Computer Experiments
Since the Neural-PDE is a sequence to sequence learning framework which allows one to predict
within any time period by the given data. One may test the Neural-PDE using different permutations
of training and predicting time periods for its efficiency, robustness and accuracy. In the following
examples, the whole dataset is randomly splitted in 80% for traning and 20% for testing. We will
predict the next tp ∈ [31 × δt, 40 × δt] PDE solution by using its previous ttr ∈ [0, 30 × δt] data as:
B(K, 10) = PDESolVer(A(K, 30))	(4.0.1)
Table 1 summaries the experimental results of the Neural-PDE model on 4 different PDEs, which
achieve extremely small MSES from 〜10-5 to 〜10-7. Table 2 shows the comparison results of
our proposed Neural-PDE with the state-of-the-art method Physically Informed Artificial Neural
Networks (PINN) (Raissi et al., 2019) on two PDEs (1d Allen-Cahn and 1d Burgers’ equation).
Neural-PDE is able to outperform PINN while having much less parameters, where PINN contains 4
hidden layers with 200 neurons per layer and Neural-PDE only consists of 3 layers (2 bi-lstm with 20
neurons per layer and 1 dense output layer with 10 neurons).
6
Under review as a conference paper at ICLR 2021
(a) Exact u(t = 1)
Figure 4: Neural-PDE shows ideal prediction on Burgers’ equation.
Example: Inviscid Burgers’ Equation
Inviscid Burgers’ equation is a classical nonlinear PDE in fluid dynamics. In this example, we
consider an invicid Burgers’ equation which has the following hyperbolic form:
∂u ∂u ∂u ∂v ∂u ∂u
而 + u∂x + v∂y =0, ∂t+ u∂x + v∂y =0
Ω= [0,1] X [0,1],t ∈ [0,1],
(4.0.2)
(4.0.3)
and with initial and boundary conditions:
u(0.25 ≤x≤ 0.75, 0.25 ≤y≤ 0.75,t=0) =0.9	(4.0.4)
v(0.25 ≤x≤ 0.75, 0.25 ≤y≤ 0.75,t=0) =0.5	(4.0.5)
u(0, y, t) = u(1, y, t) = v(x, 0, t) = v(x, 1, t) = 0	(4.0.6)
The invicid Burgers’ equation is hard to deal with in numerical PDEs due to the discontinuities (shock
waves) in the solutions. We use a upwinding finite difference scheme to create the training data and
put the velocity u, v in to the input matrix. Let δx = δy = 10-2, δt = 10-3, our empirical results
(see Figure 4) show that the Neural-PDE is able to learn the shock waves, boundary conditions and
the rules of the equation, and predict u and v simultaneously with an overall MSE of 1.4018 X 10-5.
The heat maps of exact solution and predicted solution are shown in Figure 5.
Example： MULTISCALE Modeling： Coupled CAHN-HILLIARD-NAVIER-STOKES System
Finally, let's consider the following 2d Cahn-Hilliard-Navier-Stokes system widely used for model-
ing complex fluids：
Ut + U ∙ Vu = - Vp + V∆u 一 φVμ ,	(4.0.7)
φt + V ∙ (uφ) = M∆μ ,	(4.0.8)
μ = λ(-∆φ + ɪ(φ2 - 1))	(4.0.9)
η2
V∙ u = 0	(4.0.10)
In this complicated example we will use the following initial condition：
φ(x, y, 0) = (ɪ - 50 tanh(fι — 0.1)) + (ɪ - 50 tanh(f2 — 0.1)), I.C.	(4.0.11)
fι = √(x + 0.12)2 + (y)2, f2 = √(x - 0.12)2 + (y)2	(4.0.12)
with x ∈ [-0.5, 0.5], y∈ [-0.5, 0.5], t∈ [0, 1], M = 0.1, ν= 0.01	(4.0.13)
This fluid system can be derived by the energetic variational approach (Forster, 2013). The complex
fluids system has the following features： the micro-structures such as the molecular configurations,
the interaction between different scales and the competition between multi-phase fluids (Hyon et al.,
2010). Here U is the velocity and φ(x, y,t) ∈ [0, 1] denotes the volume fraction of one fluid phase.
M is the diffusion coefficient and μ is the chemical potential of φ. Equation (4.0.10) indicates the
incompressibility of the fluid. Solving such pDE system is notorious because of its high nonlinearity
and multi-physical and coupled features. one may use the decoupled projection method (Guermond
et al., 2006) to numerically solve it efficiently or an implicit method which however is computationally
7
Under review as a conference paper at ICLR 2021
(a) Exact Test Dataset (b) Predicted Test Dataset
Figure 5: Neural-PDE shows ideal prediction
on 2d Burgers Equation.
Figure 6: Neural-PDE shows ideal prediction
on Fluid System.
Figure 7: Predicted data by Neural-PDE (first row) and the exact data (second row) of volume fraction
φ (column 1-3) and pressure p (column 4-6). The graphs of each columns 1-3 and 4-6 represent the
time states of t1, t2, t3, where 0 ≤ t1 < t2 < t3 ≤ 1.
expensive. Another challenge of deep learning in solving a system like this is how to process the data
to improve the learning efficiency when the input matrix consists of variables such as φ ∈ [0, 1] with
large magnitude value and variable of very small values such as P 〜 10-5 . For the Neural-PDE to
better extract and learn the physical features of variables in different scales, we normalized the p data
with a sigmoid function. Set δt = 5 × 10-4, and here the training dataset is generated by a FEM
solver FreeFem++ (Hecht, 2012) using a Crank-Nicolson finite element scheme. Our Neural-PDE
prediction shows that the physical features of p and φ have been successfully captured with an overall
MSE: 6.1631 × 10-7 (see Figure 7).
5 Conclusions
In this paper, we proposed a novel sequence recurrent deep learning framework: Neural-PDE, which is
capable of intelligently filtering and learning solutions of time-dependent PDEs. One key innovation
of our method is that the time marching method from the numerical PDEs is applied in the deep
learning framework, and the neural network is trained to explore the accurate numerical solutions for
prediction.
The state-of-the-art researches have shown the promising power of deep learning in solving high-
dimensional nonlinear problems in engineering, biology and finance with efficiency in computation
and accuracy in prediction. However, there are still unresolved issues in applying deep learning in
PDEs. For instance, the stability and convergence of the numerical algorithms have been rigorously
studied by applied mathematicians. Due to the high nonlinearity of the neural network system and
the curse of dimensionality (Hutzenthaler et al., 2019), theorems guiding stability and convergence of
solutions predicted by the neural network are to be revealed.
Lastly, it would be helpful and interesting if one can theoretically characterize a numerical scheme
from the neural network coefficients and learn the forms or mechanics from the scheme and prediction.
We leave these questions for further study.
8
Under review as a conference paper at ICLR 2021
References
Uri M Ascher, Steven J Ruuth, and Raymond J Spiteri. Implicit-explicit runge-kutta methods for
time-dependent partial differential equations. Applied Numerical Mathematics, 25(2-3):151-167,
1997.
Fischer Black and Myron Scholes. The pricing of options and corporate liabilities. Journal of political
economy, 81(3):637-654, 1973.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pp. 6571-6583,
2018.
Bernardo Cockburn, George E Karniadakis, and Chi-Wang Shu. Discontinuous Galerkin methods:
theory, computation and applications, volume 11. Springer Science & Business Media, 2012.
Richard Courant, Kurt Friedrichs, and Hans Lewy. On the partial difference equations of mathematical
physics. IBM journal of Research and Development, 11(2):215-234, 1967.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Johannes Forster. Mathematical modeling of complex fluids. Master’s, University of Wurzburg, 2013.
Alex Graves and Jurgen Schmidhuber. Framewise phoneme classification with bidirectional lstm and
other neural network architectures. Neural networks, 18(5-6):602-610, 2005.
Jean-Luc Guermond, Peter Minev, and Jie Shen. An overview of projection methods for incompress-
ible flows. Computer methods in applied mechanics and engineering, 195(44-47):6011-6045,
2006.
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. Proceedings of the National Academy of Sciences, 115:8505 - 8510, 2018.
F. Hecht. New development in freefem++. J. Numer. Math., 20(3-4):251-265, 2012. ISSN 1570-2820.
URL https://freefem.org/.
Geoffrey Hinton, Li Deng, Dong Yu, George E Dahl, Abdel-rahman Mohamed, Navdeep Jaitly,
Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N Sainath, et al. Deep neural networks
for acoustic modeling in speech recognition: The shared views of four research groups. IEEE
Signal processing magazine, 29(6):82-97, 2012.
Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735-1780, 1997.
Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. arXiv
preprint arXiv:1508.01991, 2015.
Martin Hutzenthaler, Arnulf Jentzen, Thomas Kruse, and Tuan Anh Nguyen. A proof that rectified
deep neural networks overcome the curse of dimensionality in the numerical approximation of
semilinear heat equations, 2019.
Yunkyong Hyon, Chun Liu, et al. Energetic variational approach in complex fluids: maximum
dissipation principle. Discrete & Continuous Dynamical Systems-A, 26(4):1291, 2010.
Claes Johnson. Numerical solution of partial differential equations by the finite element method.
Courier Corporation, 2012.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolu-
tional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
9
Under review as a conference paper at ICLR 2021
Randall J LeVeque. Numerical methods for conservation laws, volume 3. Springer.
Zachary C Lipton, John Berkowitz, and Charles Elkan. A critical review of recurrent neural networks
for sequence learning. arXiv preprint arXiv:1506.00019, 2015.
Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Arnaud Arindra Adiyoso Setio, Francesco
Ciompi, Mohsen Ghafoorian, Jeroen AWm Van Der Laak, Bram Van Ginneken, and Clara I Sanchez.
A survey on deep learning in medical image analysis. Medical image analysis, 42:60-88, 2017.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
International Conference on Machine Learning, pp. 3208-3216, 2018.
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong. Beyond finite layer neural netWorks:
Bridging deep architectures and numerical differential equations. In International Conference on
Machine Learning, pp. 3276-3285, 2018.
Tomas Mikolov, Stefan Kombrink, LUkaS Burget, Jan Cernocky, and SanjeeV Khudanpur. Extensions
of recurrent neural netWork language model. In 2011 IEEE international conference on acoustics,
speech and signal processing (ICASSP), pp. 5528-5531. IEEE, 2011.
Stanley Osher and James A Sethian. Fronts propagating With curvature-dependent speed: algorithms
based on hamilton-jacobi formulations. Journal of computational physics, 79(1):12-49, 1988.
Grace CY Peng, Mark Alber, Adrian Buganza Tepole, William R Cannon, Suvranu De, Savador
Dura-Bernal, Krishna Garikipati, George Karniadakis, William W Lytton, Paris Perdikaris, et al.
Multiscale modeling meets machine learning: What can We learn? Archives of Computational
Methods in Engineering, pp. 1-21, 2020.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural netWorks: A
deep learning frameWork for solving forWard and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686-707, 2019.
Mike Schuster and Kuldip K PaliWal. Bidirectional recurrent neural netWorks. IEEE transactions on
Signal Processing, 45(11):2673-2681, 1997.
Alex Sherstinsky. Fundamentals of recurrent neural netWork (rnn) and long short-term memory (lstm)
netWork. Physica D: Nonlinear Phenomena, 404:132306, 2020.
Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial
differential equations. Journal of computational physics, 375:1339-1364, 2018.
James William Thomas. Numerical partial differential equations: finite difference methods, vol-
ume 22. Springer Science & Business Media, 2013.
10
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Finite Element Method
Finite element method (FEM) is a powerful numerical method in solving PDEs. Consider a 1D wave
equation of u(x, t):
	Utt — VlIuxx = f, X ∈ [a,b] ≡ Ω ⊂ R,	t ∈ R+ ∪ {0} ,	(A.1.1) ux (a, t) = ux (b, t) = 0 .	(A.1.2)
The function u is approximated by a function uh :
	N u(x, t) ≈ uh (x, t) =	ai (t)ψi (x)	(A.1.3) i=1
(A.1.4)
where ψi ∈ V , is the basis functions of some FEM space V , and ain denotes the coefficients. N
denotes the degrees of freedom.
Multiply the equation with an arbitrary test function ψj and integral over the whole domain we have:
	/ uttψj dx + v2 / VuVψj dx = / fψj dx	(A.1.5) (A.1.6)
and approximate u(x, t) by uh:
	X ' ∂i(t Z ψiψj dx +v2 X ai(t) Z VψiVψj dx = Z fψj dx ,	(A.1.7) i	12_{	}	i	∣ω	7	} ∣ω^^ Mi,j	Ai,j	b ≡ MT att + v2ATa = b .	(A.1.8)
Here M is the mass matrix and A is the stiffness matrix, a = (a1, .., aN)t is a N × 1 vector of the
coefficients at time t. The central difference method for time discretization indicates that (Johnson,
2012):
	an+1 = 2an — an-1 +M-1(b — v2AT an) ,	(A.1.9) N un+1 ≈ uhn+1 = X ain+1ψi(x) .	(A.1.10) i
A.2	Long Short-Term Memory
Long Short-Term Memory Networks (LSTM) (Hochreiter & Schmidhuber, 1997; Graves & Schmid-
huber, 2005) are a class of artificial recurrent neural network (RNN) architecture that is commonly
used for processing sequence data and can overcome the gradient vanishing issue in RNN. Similar to
most RNNs (MikoloV et al., 2011), LSTM takes a sequence {xι, x2,…,xt} as input and learns
hidden vectors {hi, h2,…，ht} for each corresponding input. In order to better retain long distance
information, LSTM cells are specifically designed to update the hidden Vectors. The computation
process of the forward pass for each LSTM cell is defined as follows:
it = σ(Wi(x)xt + Wi(h)ht-1 +Wi(c)ct-1 +bi) ,
ft = σ(W(fx)xt + Wf(h)ht-1 +Wf(c)ct-1 +bf) ,
ct =ftct-1+ittanh(Wc(x)xt+W(ch)ht-1+bc) ,
ot = σ(W(ox)xt +Wo(h)ht-1 + Wo(c)ct + bo),
ht = ot tanh(ct) ,
where σ is the logistic sigmoid function, Ws are weight matrices, bs are bias vectors, and subscripts
i, f, o and c denote the input gate, forget gate, output gate and cell vectors respectively, all of which
have the same size as hidden vector h.
This LSTM structure is used in the paper to simulate the numerical solutions of partial differential
equations.
11
Under review as a conference paper at ICLR 2021
A.3 Examples
A.3.1 Wave Equation
Consider the 1d wave equation:
utt = cuxx, x∈ [0, 1], t∈ [0, 2] ,	(A.3.1)
u(x, 0) = sin(4πx)	(A.3.2)
u(0, t) = u(1, t)	(A.3.3)
LetC= 16∏2 and use the analytical solution given by the characteristics for the training and testing
data:
u(x, t)
g(sin(4πx + t) + sin(4πx — t))
(A.3.4)
O741B52963O741B52963
1223445677B99。Ilu3
dəis EF
(a) Exact Test Dataset
- 三
07142128354249566370“b491980512192633
mil
⅛K*U--L
-1.0
°∙
-0.6
-0.4
-0.2
b-o.o
(b) Predicted Test Dataset
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
Model Loss
		train 	Cv 	
0	20	40	60	80	1∞
epoch
(c) Training Metrics
Figure 8: δx = 1 × 10-2, δt = 1 × 10-3, MSE: 7.4444 × 10-5, the whole time period length is
nt = 2000 and the mesh grid size is 101, the test dataset size is 14 and thus the total discrete testing
time period is 140, figure (a) and figure(b) are the heat map for the exact test data and our predicted
test data. Figire(c) shows both training and cross-validation errors of Neural-PDE convergent within
10 epochs.
(b) t = 1.994
(a) t = 1.991
Figure 9: We selected the final states for computation and the results indicate that Neural-PDE is
robust in capturing the physical laws of wave equation and predicting the sequence time period.
(c) t = 1.997
(d) t = 2
A.3.2 Heat Equation
The heat equation describes how the motion or diffusion of a heat flow evolves over time. The
Black-Scholes model (Black & Scholes, 1973) is also developed based on the physical laws behind
the heat equation. Rather than the 1D case that maps the data into a matrix (??) with its original
spatial locations, the high dimensional PDEs grids are mapped into matrix without regularization
of the position, and the experimental results show that Neural-PDE is able to capture the valuable
features regardless of the order of the mesh grids in the matrix. Let’s start with a 2D heat equation as
follows:
ut = uxx + uyy ,
u(x,y,0)= 00..19,, iofth(xer-wis1e)+(y-1) < 0.25
Ω = [0, 2] X [0, 2], t ∈ [0,0.15]
(A.3.5)
(A.3.6)
(A.3.7)
12
Under review as a conference paper at ICLR 2021
Figure 10: δx = 0.02, δy = 0.02, δt = 10-4, MSE: 7.0741 × 10-6, the size of the test data is 10
and the test time period is 140.
2.00
0.00
0.0
Exact
1.75
1.50
1.25
A 1.00
0.75
0.50
0.25
0.5
1.5
1.0
x
2.0
0.48
2.00
0.42
0.36
0.30
0.24
0.18
0.12
0.06
0.00
0.00
0.0
Predicted
1.75
1.50
1.25
A 1.00
0.75
0.50
0.25
0.5
1.5
1.0
x
2.0
0.48
2.00
0.42
0.36
0.30
0.24
0.18
0.12
0.06
0.00
0.00
0.0
Error
1.75
1.50
1.25
A 1.00
0.75
0.50
0.25
0.48
0.42
0.36
0.30
0.24
0.18
0.12
0.06
0.5
1.5
2.0
1.0
x
0.00
(a)
(b)
(c)
Figure 11: figure (a) is the exact solution u(x, y, t = 0.15) at the final state and figure (b) is the
model’s prediction. Figure (c) is the error map.
13