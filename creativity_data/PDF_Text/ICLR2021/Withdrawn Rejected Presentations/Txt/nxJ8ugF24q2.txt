Under review as a conference paper at ICLR 2021
Learning Disconnected Manifolds: Avoiding
The No GAN’s Land By Latent Rejection
Anonymous authors
Paper under double-blind review
Ab stract
Standard formulations of GANs, where a continuous function deforms a connected
latent space, have been shown to be misspecified when fitting disconnected mani-
folds. In particular, when covering different classes of images, the generator will
necessarily sample some low quality images in between the modes. Rather than
modify the learning procedure, a line of works aims at improving the sampling
quality from trained generators. Thus, it is now common to introduce a rejection
step within the generation procedure. Building on this, we propose to train an
additional network and transform the latent space via an adversarial learning of
importance weights. This idea has several advantages: 1) it provides a way to
inject disconnectedness on any GAN architecture, 2) since the rejection happens
in the latent space, it avoids going through both the generator and the discrimina-
tor saving computation time, 3) this importance weights formulation provides a
principled way to reduce the Wasserstein’s distance to the target distribution. We
demonstrate the effectiveness of our method on different datasets, both synthetic
and high dimensional.
1	Introduction
GANs (Goodfellow et al., 2014) are an effective way to learn complex and high-dimensional distribu-
tions, leading to state-of-the-art models for image synthesis in both unconditional (Karras et al., 2019)
and conditional settings (Brock et al., 2019). However, it is well-known that a single generator with
a unimodal latent variable cannot recover a distribution composed of disconnected sub-manifolds
(Khayatkhoei et al., 2018). This leads to a common problem for practitioners: the necessary existence
of very-low quality samples when covering different modes. This is formalized by Tanielian et al.
(2020) which refers to this area as the no GAN’s land and provides impossibility theorems on the
learning of disconnected manifolds with standard formulations of GANs. Fitting a disconnected target
distribution requires an additional mechanism inserting disconnectedness in the modeled distribution.
A first solution is to add some expressivity to the model: Khayatkhoei et al. (2018) propose to train a
mixture of generators while Gurumurthy et al. (2017) make use of a multi-modal latent distribution.
A second solution is to improve the quality of a trained generative model by avoiding its poorest
samples (Tao et al., 2018; Azadi et al., 2019; Turner et al., 2019; Grover et al., 2019; Tanaka, 2019).
This second line of research relies heavily on a variety of Monte-Carlo algorithms, such as Rejection
Sampling or the Metropolis-Hastings. These methods aim at sampling from a target distribution, while
having only access to samples generated from a proposal distribution. This idea was successfully
applied to GANs, using the previously learned generative distribution as a proposal distribution.
However, one of the main drawback is that Monte-Carlo algorithms only guarantee to sample from
the target distribution under strong assumptions. First, we need access to the density ratios between
the proposal and target distributions or equivalently to a perfect discriminator (Azadi et al., 2019).
Second, the support of the proposal distribution must fully cover the one of the target distribution,
which means no mode collapse. This is known to be very demanding in high dimension since
the intersection of supports between the proposal and target distribution is likely to be negligible
(Arjovsky and Bottou, 2017, Lemma 3). In this setting, an optimal discriminator would give null
acceptance probabilities for almost any generated points, leading to a lower performance.
To tackle the aforementioned issue, we propose a novel method aiming at reducing the Wasserstein
distance between the previously trained generative model and the target distribution. This is done via
1
Under review as a conference paper at ICLR 2021
the adversarial training of a third network that learns importance weights in the latent space. The goal
is to learn the redistribution of mass of the modeled distribution that best fits the target distribution.
To better understand our approach, we first consider a simple 2D motivational example where the real
data lies on four disconnected manifolds. To approximate this, the generator splits the latent space
into four distinct areas and maps data points located in the frontiers, areas in orange in Figure 1b, out
of the true manifold (see Figure 1a). Our method consequently aims at learning latent importance
weights that can identify these frontiers and simply avoid them. This is highlighted in Figure 1d
where the importance weighter has identified these four frontiers. When sampling from the new latent
distribution, we can now perfectly fit the mixture of four gaussians (see Figure 1c).
(a) WGAN: real samples in green and fake ones in
blue.
(b) Latent space: heatmap of the distance between a
generated sample and its nearest real sample.
(c) WGAN with latent rejection sampling: real
samples in green and fake ones in blue.
Figure 1: Learning disconnected manifolds leads to the apparition of an area in the latent space
generating points outside the target manifold. With the use of the importance weighter, one can avoid
this specific area and better fit the target distribution.
(d) Latent space: heatmap of the learned importance
weights. The blue frontiers have zero weights.
Our contributions are the following:
•	We discuss works improving the sampling quality of GANs and identify their limitations.
•	We propose a novel approach that directly modifies the latent space distribution. It provides
a principled way to reduce the Wasserstein distance to the target distribution.
•	We thorougly compare our method with a large set of previous approaches on a variety of
datasets and distributions. We empirically show that our solution significantly reduces the
computational cost of inference while demonstrating an equal efficiency.
Notation. Before moving to the related work section, we shortly present notation needed in the paper.
The goal of the generator is to generate data points that are “similar” to samples collected from some
target probability measure μ?. The measure μ? is defined on a potentially high dimensional space RD,
equipped With the euclidean norm，』.To approach μ?, We use a parametric family of generative
distribution where each distribution is the push-forward measure of a latent distribution Z and a
continuous function modeled by a neural netWork. In most of all practical applications, the random
variable Z defined on a loW dimensional space Rd is either a multivariate Gaussian distribution
or uniform distribution. The generator is a parameterized class of functions from Rd to RD , say
G = {Gθ : θ ∈ Θ}, Where Θ ⊆ Rp is the set of parameters describing the model. Each function
Gθ takes input from Z and outputs “fake” observations with distribution μθ = Gθ ]Z. On the other
hand, the discriminator is described by a family of functions from RD to R, say D = {Dα : α ∈ Λ },
Λ ⊆ RQ, where each Da. Finally, for any given distribution μ, we note Sμ its support.
2
Under review as a conference paper at ICLR 2021
2	Related Work
2.1	Disconnected manifold learning: how to train and evaluate GANs
Goodfellow et al. (2014) already stated that when training vanilla GANs, the generator could
ignore modes of the target distribution: this is the mode collapse. A significant step towards
understanding this phenomenon was made by Arjovsky and Bottou (2017) who explained that the
standard formulation of GANs leads to vanishing or unstable gradients. The authors proposed the
Wasserstein GANs (WGANs) architecture (Arjovsky et al., 2017) where, in particular, discriminative
functions are restricted to the class of 1-Lipschitz functions. WGANs aim at solving:
SUP inf EX〜由 Dα(X) -EZ〜ZDa(Gθ(Z)))	(1)
α∈A θ∈Θ
The broader drawback of standard GANs is that, since any modeled distribution is the Push-forward
of a unimodal distribution by a continuous transformation, it consequently has a connected suPPort.
This means that when the generator covers multiPle disconnected modes of the target distribution, it
necessarily generates samPles out of the real data manifold (Khayatkhoei et al., 2018). Consequently,
any thorough evaluation of GANs should assess simultaneously both the quality and the variety of the
generated samPles. Sajjadi et al. (2018) argue that a single-digit metric such as the IncePtion Score
(Salimans et al., 2016) or the Frechet IncePtion distance (Heusel et al., 2017) is thus not adequate to
comPare generative models. To solve this issue, the authors ProPose a Precision/Recall metric that
aims at measuring both the mode dropping and the mode inventing.
In the Improved PreciSion/Recall (Kynkaanniemi et al., 2019), the precision refers to the portion of
generated Points that belongs to the target manifold, while the recall measures how much of the target
distribution can be re-constructed by the model distribution. Building on this metric, Tanielian et al.
(2020) highlighted the trade-off property of GANs deriving upper-bounds on the precision of standard
GANs. To solve this problem, a common direction of research consists in over-parameterizing the
generative model. Khayatkhoei et al. (2018) enforce diversity by using a mixture of generators while
Gurumurthy et al. (2017) suggest that a mixture of Gaussians in the latent space is efficient to learn
diverse and limited data.
2.2	Improving the quality of trained generators
To better fit disconnected manifolds with standard GANs architectures, another line of research
consists in inserting disconnectedness into a previously learned generative distribution μ0. Tanielian
et al. (2020) proposed an heuristic to remove the no GAN’s land (i.e. samples mapped out of the true
manifold): rejecting data points with a high Jacobian Frobenius norm. Another possibility would be
to use one of the different Monte-Carlo methods (Robert and Casella, 2013) and apply it to GANs.
Building up on the well-known inference theory, Azadi et al. (2019) suggests the use of rejection
sampling to improve the quality of the proposal distribution μθ. One can compute density ratios
using either a classifier trained from scratch or the discriminator obtained at the end of the training.
Consequently, in this Discriminator Rejection Sampling (DRS), any generated data point X 〜μθ is
accepted with the following acceptance probability Pa :
Pa(X)
μ√x)
M μθ (x)
L ” ~	μ?(X)
where M = max ---,
X∈ι⅛θ μθ (χ)
(2)
where μ? and μ° here refers to the density functions. Similarly, Turner et al. (2019) use the same
density ratios and derive MH-GAN, an adaptation of the Metropolis-Hasting algorithm (Hastings,
1970), that improves the sampling from μθ. Finally, Grover et al. (2019) use these density ratios
r as importance weights and define an importance resampled generative model whose density is
now defined by μ^θ 尺 μθ X r(x). In order to perform discrete sampling from μ^θ, authors rely on the
Sampling-Importance-Resampling (SIR) algorithm (Rubin, 1988; Liu and Chen, 1998). This defines
a new distribution μ^θ sir:
μSIR(Xi) = I®	where Xi,..., Xn 〜μn.
∑ r(Xj)
j=1
Note that these algorithms rely on the same density ratios and an acceptance-rejection scheme. In
Rejection Sampling, the acceptance rate is uncontrollable but sampling from μ? is assured. With SIR
and MH, the acceptance rate is controllable but sampling from μ? is no longer guaranteed.
3
Under review as a conference paper at ICLR 2021
3	Adversarial Learning of Latent Importance weights
3.1	Our approach
Similar to previous works, our method consists in improving the performance of a given generative
model, post-training. Given a trained WGANs (Gθ ,Dα), we now propose to learn importance
weights in the latent space. To do so, we use a feed-forward neural network from Rd to R+,
say Ω = {Wφ : φ ∈ Φ}. The neural network Wφ is trained using an adversarial process with the
discriminator Dα, whilst keeping the weights of Gθ frozen. We now want to solve the following:
sup inf EX~μ? Da (X) - E Z 〜Z (W φ (Z) × Da (G θ (Z))))	(3)
α∈A φ∈Φ	?
Note that our formulation can also be plugged on top of many different objective functions. Interest-
ingly, the use of the predictor Wφ defines a new latent space distribution whose density Y is defined
by 久 z) ɑ wφ (Z) X γ(Z). Consequently, the newly defined modeled distribution μθ is defined as the
pushforward μθ = Gθ ]γ. The proposed method can be seen as minimizing the Wasserstein distance
to the target distribution, over an increased class of generative distributions. The network Wφ thus
learns how to redistribute the mass of μθ such that μθ is closer to μ? in terms Wasserstein distance.
However, as in the field of counterfactual estimation, a naive optimization of importance weights by
gradient descent can lead to trivial solutions. First, if for example, the Wasserstein critic Da outputs
negative values for any generated samples, the network Wφ could simply learn to avoid the dataset
and output 0 everywhere. To avoid this issue, we follow Swaminathan and Joachims (2015c) and
scale the output of the discriminator such that the reWard is always positive. A second problem
comes from the fact that equation 3 can now be minimized not only by putting large importance
weights Wφ (Z) on the examples with high likelihoods Da (G(Z)), but also by maximizing the sum of
the weights: this is the propensity overfitting (Swaminathan and Joachims, 2015a). To stabilize the
optimisation process, we consequently introduce two important regularization techniques:
Self-normalization. Similarly to Swaminathan and Joachims (2015a), we advocate the use of a
normalization of the importance weights. To be more precise, we enforce the expectation of the
importance weights to be close 1 by adding a penalty term. By doing so, we prohibit the propensity
overfitting since the sum of the importance weights in the batch is bounded.
Soft-Clipping To avoid cases where small areas of Z have really high Wφ (Z) values, which would
lead to mode collapse, we enforce a soft-clipping on the weights (Bottou et al., 2013; Grover et al.,
2019). Note that this constraint on Wφ (Z) could also be implemented with a bounded activation
function on the final layer, such as a re-scaled sigmoid or tanh activation.
Finally, we thus get the following objective function:
sup EZ〜Z Wφ(Z)(Da(GΘ(Z))) - v) -λι (EZ〜ZWφ(Z) - 1)2-λ2EZ〜Zmax(0,(Wφ(Z) -m))2, (4)
φ∈Φ	X-----------V----------} X-----------V--------} X---------------V------------}
discriminator reward	self-normalization	soft-clipping
where V = minZ〜Z Da(G(Z)). λι, λ2, and m are hyper-parameters (values displayed in Appendix).
3.2	Sampling from the new distribution
As mentionned above, the scale and variance of the learned importance weights are actively controlled,
as it is done in counterfactual estimation (Bottou et al., 2013; Swaminathan and Joachims, 2015b;
Faury et al., 2020). Doing so, we explicitly control the acceptance rates Pa (Z) of the rejection
sampling algorithm performed on Y since for any given Z 〜Z, we have:
P a (Z ) =	and E Z P ° (Z )= Z -ɪ(ɪ γ(Z) dZ = Z Yz) dZ =-,
mY(Z)	Rd mY(Z)	Rd m m
where m is the maximum output of the importance weighter as defined in equation 4. We define as
Latent Rejection Sampling (latentRS), the method that performs the Rejection Sampling algorithm on
top of the learned importance weights. Since the exact sampling of the distribution YiS now tractable
with a rejection sampling algorithm, we do not need to implement neither the Metropolis-Hasting nor
the Sampling Importance Resampling algorithm.
4
Under review as a conference paper at ICLR 2021
(a) WGAN-GP:
EMD = 0.12 ± 0.01
(b) WGAN-GP with DRS:
EMD = 0.36 ± 0.01.
(c) WGAN-GP with LRS:
EMD = 0.03 ± 0.01.
Figure 2: In this synthetic experiment, the target distribution has two modes slightly shifted from the
generated distribution. When using an optimal discriminator, DRS only selects the intersection of
the two supports, thus removing important data. Our method achieves a much better fit and does not
suppress any mode. The EMD metric confirms the interest of the method in this specific case.
Inspired from the literature on latent space optimal transport (Salimans et al., 2018; Agustsson
et al., 2019; Tanaka, 2019), we also propose a second method where we perform gradient ascent
in the latent space. To be more precise, for any given sample in the latent space, we follow the
path maximizing the learned importance weights. This method is denoted latent Gradient Ascent
(latentGA). In high-dimension, similarly to Tanaka (2019, Algorithm 2), gradients are projected to
restrict z on the training support. Note that the learning rate and the number of updates used for this
method are hyper-parameter that need to be tuned.
3.3	Advantages of the proposed approach
We now discuss, in detail, the flaws of previous Monte-Carlo based approaches:
1)	Computational cost. By using sampling algorithms in the latent space, we avoid going through
both the generator and the discriminator, leading to a significant computational speed-up. This is of
particular interest when dealing with high-dimensional spaces since we do not need to pass through
deep CNNs generator and discriminator (Brock et al., 2019).
2)	Existence of density functions. Every Monte-Carlo based methods assume that both μ? and μθ
are probability distributions with associated density functions. However, in high dimension, the
hypothesis that data tend to lie near a low dimensional manifold (Fefferman et al., 2016) is now
commonly accepted. Besides, it is often the case that GANs be defined as the push-forward from
much lower dimensional space, that is d << D. In that case, neither μ? nor μ° have density function
in RD . Note that our method based on Wasserstein distance does not require this assumption.
3)	Covering of the support Sμ? . First, Monte-Carlo methods are well-known to suffer from the
curse of dimensionality (Mengersen et al., 1996; Robert and Casella, 2013). Besides, in the context of
GANs, Arjovsky and Bottou (2017, Theorem 2.2) have shown that the intersection Sμ? TSμθ is likely
to be a negligible set under μθ. In this specific case, the density ratios would evaluate close to 0 almost
everywhere on Sμθ , increasing the time complexity. More generally, Monte-Carlo based methods
tend to avoid any area within Sμθ \Sμ? which could lead to a deteriorated sampling quality. To better
illustrate this phenomenon, we represent in Figure 2a a synthetic experiment, where Sμθ does not
recover Sμ? (by slightly shifting the mean of two modes after training the WGAN). In this setting,
we clearly see in Figure 2b that Monte-carlo based methods worsen the WGAN. when Sμ? 6⊂ Sμθ ,
density ratios focus on local information and lead to non-optimal solutions. On the opposite, our
method suggests to learn the optimal re-weighting of mass within the support Sμθ . Interestingly, on
this synthetic dataset, it significantly reduces the Wasserstein distance to μ?, see Figure 2c.
4)	Non-optimal discriminators. Knowing that optimal discriminators would lead to non-optimal
objectives (very low acceptance probabilities), previous approaches made sure that their obtained
classifier is sufficiently far from the optimal classifier (Section 3.2 in (Azadi et al., 2019)). Authors
have thus come up with heuristics to approximate density ratios: for example Azadi et al. (2019) fine
tune a regularized discriminator, Grover et al. (2019) use a pre-trained neural network on ImageNet
classification and only fine-tune the final layers for the binary classification task. In our method, on
the contrary, we are still looking for the discriminator maximizing the Integral Probability Metric
(Muller, 1997) in equation 3, linked to optimal transport.
5
Under review as a conference paper at ICLR 2021
4	Experiments
In the following section, we illustrate the efficiency of the proposed methods, latentRS and latentGA
on synthetic datasets. Then, we compare their performances with previous works on image datasets.
On this image generation tasks, we empirically stress that both latentRS or latentGA methods slightly
surpass density ratios based methods while significantly reducing the time complexity.
4.1	Evaluation metrics
To measure performances of GANs when dealing with low dimensional applications - as with
synthetic datasets - we equip our space with the standard Euclidean distance. However, for high
dimensional applications such as image generation, Brock et al. (2019); Kynkaanniemi et al. (2019)
have shown that embedding images into a feature space with a pre-trained convolutional classifier
provides more semantic information. In this setting, we consequently use the euclidean distance
between the images’ embeddings from a classifier. For a pair of images (a, b), we define the distance
d(a, b) as d(a,b) = kφ(a) - φ (b)k2 where φ is a pre-softmax layer of a supervised classifier, trained
specifically on each dataset. Doing so, they will more easily separate images sampled from the target
distribution μ? from the ones sampled by the distribution μθ.
We compare the performance of the different methods with a panel of evaluation metrics. To begin
with, we use the Improved Precision/Recall (Improved PR) metric (Kynkaanniemi et al., 2019), a
more robust version of the Precision/Recall metric which was first applied to the context of GANs
by Sajjadi et al. (2018). The Improved PR metric is based on a non-parametric estimation of the
support of both generated and real distributions using k-Nearest Neighbors. Besides, we also report
two well-known metrics: the Earth Mover’s Distance (EMD), the discrete version of the Wasserstein
distance, and the average Hausdhorff distance (Hausd). EMD is a distance between probability
distributions while Hausd. focuses on support estimations. These two measures are particularly
interesting in GANs since one can compute them with collections of discrete points. Let (x1, . . . ,xn)
and (y1, . . . ,yn) be two collections ofn data points and S be the set of permutations of [1, n], then:
EMD(X, Y) = min ∑ ∣∣Xi,yσ∙J∣ and average Hausd(X, Y) = ∑ ∑ min ∣∣Xi,yj k
σ∈S i=1	i	nxi∈Xyj∈Y
Besides, we argue that the Wasserstein distance should be a metric of reference when evaluating
WGANs since it is directly linked to their objective function. Finally, for completeness, we report
FID Heusel et al. (2017).
4.2	Mixture of Gaussians
Further experiments were ran on synthetic datasets with mixtures of 2D Gaussians, with either 4, 9,
16 or 25 components. When dealing with 2D mixtures of Gaussians, we used MLP with 4 hidden
layers of 30 nodes for the generator, the discriminator and the importance weighter. As expected in
this setting, a standard WGAN-GP combined with a connected latent space (i.e. multivariate normal
or uniform), necessarily generates samples in-between two modes. Both Figure 1 and Figure 2a have
stressed how the importance weighter can truncate latent space areas that are mapped outside the
real data manifold and improve the EMD metric. More figures and details of the different evaluation
metrics are given in Appendix.
4.3	Image datasets
MNIST, F-MNIST and Stacked MNIST. We further study the efficiency of the proposed methods
on three image datasets: MNIST (LeCun et al., 1998), FashionMNIST (F-MNIST) (Xiao et al.,
2017), and Stacked MNIST (Metz et al., 2016) a highly disconnected datasets with 1,000 classes.
For MNIST, F-MNIST and Stacked MNIST, we follow Khayatkhoei et al. (2018) and use a standard
CNN architecture composed of a sequence of 3x3 convolution layer, relu activation with nearest
neighbor upsampling. To exhibit the efficiency of the proposed methods in different settings, we use
hinge loss with gradient penalty (Hinge-GP) (Miyato et al., 2018) on MNIST and F-MNIST, and a
Wasserstein loss with gradient penalty (Gulrajani et al., 2017) on Stacked Mnist. For the importance
weighter Wφ, We use an MLP architecture with fully-connected layers and relu activation. Wφ has 4
hidden layers, each having its width four times larger than the dimension of the latent space.
6
Under review as a conference paper at ICLR 2021
MNIST	Prec. (↑)	Rec. (↑)	EMD Q)	HaUsd (1)	FID (；)	Inference
Hinge-GP	87.4±0.9	94.6±0.4	24.9±0.3	21.9±0.2	53.6±7.2	09
HGP: SIR-GAN	89.4±0.5	94.3±0.5	24.1±0.1	21.2±o.2	35.9±3.1	52.0
HGP: DOT	89.5±0.6	94.0±0.3	24.8±0.2	21.7±0.2	43.3±3.4	43
HGP: IatentRS (?)	88.9±0.4	94.7±o.7	24.2±0.3	21.4±o.2	37.3±3.2	2.0
HGPTatentGA (?)	91.4±1.0	92.9±0.4	23.5±o.ι	19.8±0.2	38.2±3.8	18
F-MNIST
Hinge-GP	86.4±0.6	86.8±o.6	68.6±0.4	58.4±o.3	598.9±55.5	0.9
HGP: SIR-GAN	86.8±o.4	87.4±o.5	68.9±o.8	57.4±o.3	558.9±58.7	52.0
HGP: DOT	88.2±o.6	85.8±o.5	70.4±o.7	57.0±o.2	797.5±84.9	43
HGPTatentRS (?)	86.8±o.8	87.5±o.9	67.6±o.6	58.6±o.5	438.3±50.2	2.1
HGPTatentGA (?)	88.4±o.7	86.8±0.7	67.0±o.9	56.4±0.2	475.5±58.5	18
Stacked MNIST							
WGAN-GP	88.7±o.8	87.3±0.5	81.1±o.2	75.7±0.2	594.2±8.2	0.9
WGP: SIR-GAN	89.9±o.5	88.4±o.6	80.3±o.2	75.1±0.1	528.1±13.4	52.0
WGP: DOT	90.8±o.5	90.2±o.7	79.4±0.i	74.1±0.2	484.0±8.8	43
WGP: latentRS (?)	90.4±0.4	86.6±o.5	80.3±0.1	75.3±0.1	568.5±9.1	1.9
WGPTatentGA (?)	90.3±o.7	87.2±0.5	80.1±o.2	74.9±0.2	530.1±6.6	18
Table 1: On the image generation task, latentGA seems the best performer and latentRS matches
sato with a significantly reduced inference cost (by an order of 20). ± is 97% confidence interval.
Inference refers to the time needed to compute one image on a NVIDIA V100 GPU.

omosi守色紊?印烝加日
Figure 3: Gradient ascent on latent importance weights (latentGA): the quality is gradually improved
as we move to higher importance weights. Each image is generated only for the purpose of visualiza-
tion, and one can run this gradient ascent directly in the latent space with the importance weighter.
For exhaustivity, we compare latentRS and latentGA with previous works leveraging density ratios.
In particular, we implemented a wide set of post-processing methods for GANs: DRS (Azadi et al.,
2019), MH-GAN (Turner et al., 2019), SIR-GAN (Grover et al., 2019) and DOT (Tanaka, 2019).
Similarly to Azadi et al. (2019), we take the discriminator at the end of the adversarial training,
fine-tune it with the binary cross-entropy loss and select the best model in terms of EMD. During
fine-tuning, we keep the gradient penalty or spectral normalization, otherwise the classifier easily
separates real from generated data, which leads to a degraded performance, as shown in Figure 2a.
Following Azadi et al. (2019); Grover et al. (2019), we do not include explicit mechanism to calibrate
the classifier. To the extent of our knowledge, we are the first to empirically compare such a wide
variety of Monte-Carlo methods on different datasets and metrics.
The main results of this comparison are shown in Table 1 (see Appendix for more details). We see
that, except for Stacked MNIST, both of our methods outperform every other method on precision,
av. Hausdhorff and the EMD metric. Interestingly, latentGA seems to be the strongest one. In
Figure 3, we show how samples evolve when performing latent gradient ascent on the importance
weights. As expected, as importance weights are increased, the quality of the generated images
significantly improves. Besides, a strong contribution of the paper also resides in notably speeding-up
the inference procedure. As shown in Table 1, the inference time of a given data point is 25 times
faster with latentRS than with SIR-GAN.
7
Under review as a conference paper at ICLR 2021
Figure 4: Ranking CelebA faces with the trained importance weighter. For each line, we compare the
worst-5% vs best-5% of the generated samples. We see a significant difference in terms of quality.
CelebA is a large-scale dataset of faces covering a variety of poses. We train the models at 64x64
resolution. Following recent studies (Brock et al., 2019), the discriminator is trained with the hinge
loss and spectral normalization (Miyato et al., 2018). For the generator network, residual connections
(He et al., 2016) are used alongside self-modulation layers (Chen et al., 2019). The importance
weighter is a simple 4 hidden-layer MLP with a width 10 times larger than the latent space dimension.
In this one-class high-dimensional dataset, the importance weighter still managed to learn some
meaningful features. First, Figure 3 higlights a subtle improvement of the generated images when
performing latentGA. Second, when ranking generated images with the importance weighter and
comparing the top-5% vs worst-5% in Figure 4, we observe some differences in quality. However, on
a broader scale, the importance weighter does not bring a clear improvement on neither the EMD nor
the Hausdhorff metric. Interestingly, this is also the case for any of the different rejection methods
(see Appendix for details).
We argue that in this one-class generation task, post-processing the generated samples is not as
efficient as in a multi-modal setting (e.g. MNIST, FMNIST, Stacked MNIST). Intuitively, it is a
much easier task to remove generated samples that are out of the target manifold than to discriminate
between samples that already share similarities with training samples. It further stresses that this
family of methods is useful if one needs to insert disconectedness in the modeled distribution.
However, when the target distribution is a single-class distribution with a connected support, their
efficiency decrease. To illustrate this, we added in Appendix a figure highlighting samples generated
by a trained WGAN on Celeba 64x64, ranked by the discriminator. We observe that on these images,
the discriminator does not correlate well with human judgement prohibiting the importance weighter
to learn a meaningful signal.
5 Conclusion
In this paper, we provide insights on improving the learning of disconnected manifolds with GANs.
Given the existence of the no GAN’s land, latent space areas mapping outside the target manifold, we
provide two methods to truncate them. Contrary to previous works focusing on learning density ratios
in the output space, both of our methods are based on training adversarially a neural network learning
importance weights in the latent space. On the task of image generation, both of the proposed methods
were shown to be empirically efficient while significantly reducing the inference time (latentRS by an
order of 20), when compared to density ratio based methods. This paper has specifically stressed the
efficiency of post-training methods when dealing with highly disconnected target distributions.
However, when dealing with single-class connected distributions or class-conditioning generative
models, the efficiency of such methods is not clear. We argue that one of the reason is that, once the
generator maps correctly inside the target manifold, it is a much harder task to discriminate between
realistic and fake samples. A potential future work would therefore be to investigate how can we help
the discriminator better classify among the set of generated images.
8
Under review as a conference paper at ICLR 2021
References
Eirikur Agustsson, Alexander Sage, Radu Timofte, and Luc Van Gool. Optimal transport maps for
distribution preserving operations on latent spaces of generative models. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,
2019.
M. Arjovsky and L. Bottou. Towards principled methods for training generative adversarial networks.
In International Conference on Learning Representations, 2017.
M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In D. Precup
and Y.W. Teh, editors, Proceedings of the 34th International Conference on Machine Learning,
volume 70, pages 214-223. PMLR, 2017.
Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus Odena. Discrimi-
nator rejection sampling. In International Conference on Learning Representations, 2019.
L6on Bottou, Jonas Peters, Joaquin Quinonero Candela, Denis Xavier Charles, Max Chickering,
Elon Portugaly, Dipankar Ray, Patrice Y Simard, and Ed Snelson. Counterfactual Reasoning and
Learning Systems: the example of Computational Advertising. Journal of Machine Learning
Research, 14(1):3207-3260, 2013.
A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high fidelity natural image
synthesis. In International Conference on Learning Representations, 2019.
Ting Chen, Mario Lucic, Neil Houlsby, and Sylvain Gelly. On self modulation for generative
adversarial networks. In International Conference on Learning Representations, 2019. URL
https://openreview.net/forum?id=Hkl5aoR5tm.
L. Faury, U. Tanielian, E. Dohmatob, E. Smirnova, and F. Vasile. Distributionally robust counterfactual
risk minimization. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 3850-
3857. AAAI Press, 2020.
C. Fefferman, S. Mitter, and H. Narayanan. Testing the manifold hypothesis. Journal of the American
Mathematical Society, 29:983-1049, 2016.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
J. Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N.D. Lawrence,
and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems 27, pages
2672-2680. Curran Associates, Inc., 2014.
Aditya Grover, Jiaming Song, Ashish Kapoor, Kenneth Tran, Alekh Agarwal, Eric J Horvitz, and
Stefano Ermon. Bias correction of learned generative models using likelihood-free importance
weighting. In Advances in Neural Information Processing Systems, pages 11056-11068, 2019.
I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A.C. Courville. Improved training of Wasser-
stein GANs. In I. Guyon, U. von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,
and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 5767-5777.
Curran Associates, Inc., 2017.
Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu. Deligan: Generative
adversarial networks for diverse and limited data. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 166-174, 2017.
W Keith Hastings. Monte carlo sampling methods using markov chains and their applications. 1970.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pages 770-778, 2016.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two
time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information
Processing Systems, pages 6626-6637, 2017.
9
Under review as a conference paper at ICLR 2021
T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pages 4401-4410, 2019.
Mahyar Khayatkhoei, Maneesh K Singh, and Ahmed Elgammal. Disconnected manifold learning for
generative adversarial networks. In Advances in Neural Information Processing Systems, pages
7343-7353, 2018.
T. Kynkaanniemi, T. Karras, S. Laine, J. Lehtinen, and T. Aila. Improved precision and recall metric
for assessing generative models. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch6 Buc,
E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems 32, pages
3927-3936. Curran Associates, Inc., 2019.
Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. In Proceedings of the IEEE, pages 2278-2324, 1998.
Jun S Liu and Rong Chen. Sequential monte carlo methods for dynamic systems. Journal of the
American statistical association, 93(443):1032-1044, 1998.
Kerrie L Mengersen, Richard L Tweedie, et al. Rates of convergence of the hastings and metropolis
algorithms. The annals of Statistics, 24(1):101-121, 1996.
L. Metz, B. Poole, D. Pfau, and J. Sohl-Dickstein. Unrolled generative adversarial networks.
arXiv.1611.02163, 2016.
T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial
networks. In International Conference on Learning Representations, 2018.
A. Muller. Integral probability metrics and their generating classes of functions. Advances inApplied
Probability, 29:429-443, 1997.
Christian Robert and George Casella. Monte Carlo statistical methods. Springer Science & Business
Media, 2013.
Donald B Rubin. Using the sir algorithm to simulate posterior distributions. Bayesian statistics, 3:
395-402, 1988.
M.S.M. Sajjadi, O. Bachem, M. Lucic, O. Bousquet, and S. Gelly. Assessing generative models via
precision and recall. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and
R. Garnett, editors, Advances in Neural Information Processing Systems 31, pages 5228-5237.
Curran Associates, Inc., 2018.
T.	Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen. Improved techniques
for training GANs. In D.D. Lee, M. Sugiyama, U. von Luxburg, I. Guyon, and R. Garnett, editors,
Advances in Neural Information Processing Systems 29, pages 2234-2242. Curran Associates, Inc.,
2016.
Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving gans using optimal
transport. In International Conference on Learning Representations, 2018.
A. Swaminathan and T. Joachims. The self-normalized estimator for counterfactual learning. In
C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural
Information Processing Systems 28, pages 3231-3239. Curran Associates, Inc., 2015a.
Adith Swaminathan and Thorsten Joachims. Batch Learning from Logged Bandit Feedback through
Counterfactual Risk Minimization. Journal of Machine Learning Research, 16(1):1731-1755,
2015b.
Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged
bandit feedback. volume 37 of Proceedings of Machine Learning Research, pages 814-823, Lille,
France, 07-09 Jul 2015c. PMLR.
Akinori Tanaka. Discriminator optimal transport. In Advances in Neural Information Processing
Systems, pages 6813-6823, 2019.
10
Under review as a conference paper at ICLR 2021
U.	Tanielian, T. Issenhuth, E. Dohmatob, and J. Mary. Learning disconnected manifolds: a no gan’s
land. In International Conference on Machine Learning, 2020.
Chenyang Tao, Liqun Chen, Ricardo Henao, Jianfeng Feng, and Lawrence Carin Duke. Chi-square
generative adversarial network. In International conference on machine learning, pages 4887-4896,
2018.
Ryan Turner, Jane Hung, Eric Frank, Yunus Saatchi, and Jason Yosinski. Metropolis-hastings
generative adversarial networks. In International Conference on Machine Learning, pages 6345-
6353, 2019.
H.	Xiao, K. Rasul, and R. Vollgraf. Fashion-MNIST: A novel image dataset for benchmarking
machine learning algorithms. arXiv:1708.07747, 2017.
11
Under review as a conference paper at ICLR 2021
A Evaluation details
Precision recall metric. For the precision-recall metric, we use the algorithm from Khayatkhoei
et al. (2018). Namely, when comparing the set of real data points (x1, ...,xn) with the set of fake data
points (y1,...,yn):
A point xi has a recall r(xi) = 1 if there exists yj, such that kxi - yj k ≤ kyj - y j (k)k, where yj (k) is
the k-nearest neighbor of n. Finally, the recall is the average of individual recall: 1 ∑i r(Xi).
A point yi has a precision p(yi) = 1 if there exists xj, such that kyi -xjk ≤ kxj -xj(k)k, where xj (k)
is the k-nearest neighbor of n. Finally, the precision is the average of individual precision: ɪ ∑i-P(Xi).
Images’ embeddings. As mentioned earlier, for images we use the distance between embeddings of
images in a neural network trained specifically for classification on this dataset. For Stacked Mnist,
we use a MNIST classifier on each output channel and simply stack the three embedding vectors.
Parameters. For all datasets, we use k = 3 (3rd nearest neighbor). For MNIST, F-MNIST and
Stacked MNIST, we use a set ofn = 2048 points. For CelebA, we use a set of n = 1024 points. This
is also valid for the other metrics used: EMD, Av. Hausd. For FID on CelebA, we use the standard
protocol evaluation with Inception Net and 50k data points.
B	Hyper-parameters.
SIR: Model selection: we fine-tune with a binary cross-entropy loss the discriminator from the end
of the adversarial training and select the best model in terms of EMD.
We use then use Sampling-Importance-Resampling algorithm, with sets of n=40 points.
DRS: Model selection: we fine-tune with a binary cross-entropy loss the discriminator from the end
of the adversarial training and select the best model in terms of EMD.
We use the standard Rejection Sampling algorithm, without artificially increasing the acceptance
rate such as Azadi et al. (2019). We use regularized discriminator (with gradient penalty or spectral
normalization), which avoids the acceptance rate falling to almost zero.
MH-GAN: Model selection: we fine-tune with a binary cross-entropy loss the discriminator from the
end of the adversarial training and select the best model in terms of EMD. We use the independance
Metropolis-Hastings algorithm with Markov Chains of40 points, and select the last point.
DOT: Model selection: we fine-tune with the dual wasserstein loss the discriminator from the end
of the adversarial training and select the best model in terms of EMD. We then perform a projected
gradient descent as described in Tanaka (2019) with SGD, with Nsteps = 10 and ε = 0.01.
LRS: For MNIST, F-MNIST and Stacked MNIST, we use the same hyper-parameters: λ1 = 10,
λ = 2 and m = 3. Wφ is a standard MLP with 4 hidden layers, each having 600 nodes (6x dimension
of latent space), and relu activation. The output layer is 1-dimensional and with a relu activation.
For CelebA, we use: λ1 = 50, λ2 = 2 and m = 3. Wφ is a standard MLP with 4 hidden layers,
each having 1280 nodes (10x dimension of latent space), and relu activation. The output layer is
1-dimensional and with a relu activation.
For the adversarial training of importance weights, we use the discriminator from the end of the
standard adversarial training (generator vs discriminator). We then alternate between 1 step of Wφ
and 1 update of Dα.
LGA: We use the same neural network than in LRS. The hyper-parameters for this method are similar
to DOT: number of steps of gradient ascent Nsteps and learning rate ε. We choose Nsteps = 10 and
ε = 0.1.
12
Under review as a conference paper at ICLR 2021
C Visualization and results for synthetic datasets
(a) WGAN for the mixture of 9
Gaussians.
(b) WGAN for the mixture of 16
Gaussians.
(c) WGAN for the mixture of 25
Gaussians.
(d) Heatmap in the latent space of
the distance between a generated
(e) Heatmap in the latent space of
the distance between a generated
(f) Heatmap in the latent space of
the distance between a generated
(g)	Heatmap of the importance
weights in the latent space for the
mixture of 9 Gaussians.
(h)	Heatmap of the importance
weights in the latent space for the
mixture of 16 Gaussians.
(i)	Heatmap of the importance
weights in the latent space for the
mixture of 25 Gaussians.
(j)	WGAN with latentRS for the
mixture of 9 Gaussians.
(k)	WGAN with latentRS for the
mixture of 16 Gaussians.
∙∙	∙ ∙	∙⅜*∙**0 ∙* ♦ .♦ .
∙ ∙ ⅜t∙∙≡ ■ M ⅜j2 *•»* c*s∙ z ∙ ∙*λ⅛
(l)	WGAN with latentRS for the
mixture of 25 Gaussians.
Figure 5:	Synthetic examples on mixtures of Gaussians with respectively (per column) 9, 16 and 25
components. Real samples in green and generated points in blue.
13
Under review as a conference paper at ICLR 2021
Mixture of 4 Gaussians	Prec.	Rec.	EMD	Hausd.
WGAN-GP	95.0±0.5	78.5±1.1	0.50±0.05	0.85±0.05
WGAN-GP with latentRS (?)	99.2±0.1	70.8±2.2	0.37±o.07	0.10±0.02
Mixture of 9 Gaussians
Hinge-GP	86.3±o.3	76.2±o.8	O.45±o.07	O.57±o.07
HGPTatentRS (?)		91.O±o.3	73.0±1.4	O.4O±o.06	O.23±o.04
Mixture of 16 Gaussians
Hinge-GP	7O.6±o.3	68.2±o.8	1.10±0.07	O.85±o.O2
HGPTatentRS (?)		74.O±o.3	69.5±1.4	1 .OO±o.O4	O.54±o.O4
Mixture of 25 Gaussians
Hinge-GP	63.4±o.3	64.6±o.8	1.35±o.O8	1.1O±o.O7
HGPTatentRS (?)		7O.6±o.3	67.0±1.4	1.25±o.O5	O.8±o.O4
Table 2: Performance of the latentRS method on the estimation of 2D mixtures of Gaussians with
respectively 4, 9, 16 or 25 components. Each mode has a variance σ2 = 0.025 is separated by a
distance d = 5. As expected the latentRS method removes data points that are off the target manifold
and therefore significantly improves the Hausdhorff metric.
D Comparisons with concurrent methods on synthetic and
real-world datasets
* ∙7rr>Yr4*学
*-∙⅛⅛.∙∙y.
:•;・∙z." ∙ ∙
■*->■,+.:TF-
L*.*
二∙ ∙ ∙∙ ∙
■⅛-∙ ，FS
*∙ :<>.: A?.?...>
∙∙*∙∙.βj¼
-;'• ““ ∙∙,.∙∙ .∙
.--Hi:
∙∙l∙ ∙ ∙ ∙»
J ∙ ∙ : ∙
一—
" ∙ .∙∙ .∙
4•¥•*„
(a) WGAN-GP.
(e) WGAN-GP.
(b) WGAN-GP with
SIR.
(f) WGAN-GP with
SIR.
(c) WGAN-GP with
DOT.
f、
ɑ)
(g) WGAN-GP With
DOT.
(d) WGAN-GP With
LRS.
(h) WGAN-GP With
LRS.
Figure 6:	Comparisons of concurrent methods on synthetic datasets: mixtures of 25 Gaussians and
on 2D SWiss Roll. Experimental setting from Tanaka (2019). Real samples in green and generated
points in blue.
14
Under review as a conference paper at ICLR 2021
Mixture of 25 Gaussians	Prec.	Rec.	-EMD
WGAN-GP	24.7±0.5	90.4±1.0	0.044±0.001
WGAN-GP with DRS	86.9±0.6	84.0±0.5	0.038±0.002
WGAN-GP with SIR	84.3±0.8	90.0±o.7	0.041±0.001
WGAN-GP with DOT	72.7±1.0	76.3±o.8	O.O35±0.002
WGAN-GP With latentRS (?)	36.2±0.8	88.5±0.9	0.036±0.001
Swiss Roll
WGAN-GP	77.9±0.5	90.3±0.7	O.030±o.002
WGAN-GP: DRS	97.7±o.2	94.4±o.5	0.036±0.004
WGAN-GP: SIR	97.3±0.6	93.9±o.7	0.037±0.003
WGAN-GP: DOT	92.9±o.6	77.7±o.5	0.029±0.003
WGAN-GPTatentRS (?)	84.3±o.8	90.1±o.8	O.O25±0.002
Table 3: Comparison of latentRS with concurrent methods on two synthetic datasets in the same
setting as Tanaka (2019). Our method enables a consistent gain in EMD, surpassing other methods
on Swiss Roll and slightly behind DOT on Mixture of 25 Gaussians.
MNIST	Prec. (↑)	Rec.(↑)	EMD (；)	av. HaUsd ⑷	FID Q)
Hinge-GP	87.4±0.9	94.6±0.4	24.9±0.3	21.9±0.2	53.6±7.2
HGP: DRS	89.0±o.5	94.4±0.5	24.3±0.3	21.2±0.2	41.3±8.2
HGP: SIR-GAN	89.4±o.5	94.3±o.5	24.1±0.1	21.2±0.2	35.9±3.i
HGP: MH-GAN	89.4±o.6	94.5±o.3	24.4±0.3	21.1±0.1	43.2±5.7
HGP: DOT	89.5±o.6	94.0±o.3	24.8±0.2	21.7±0.2	43.3±3.4
HGP: latentRS (?)	88.9±o.4	94.7±o.7	24.2±0.3	21.4±0.2	37.3±3.2
HGPTatentGA (?)	91.4±ι.o	92.9±0.4	23.5±o.ι	19.8±0.2	45.2±3.8
F-MNIST
Hinge-GP	86.4±0.6	86.8±o.6	68.6±0.4	58.4±0.3	598.9±55.5
HGP: DRS	86.7±o.7	87.4±o.5	68.7±o.7	57.7±0.2	546.8±37.3
HGP: SIR-GAN	86.8±o.4	87.4±o.5	68.9±o.8	57.4±0.3	558.9±58.7
HGP: MH-GAN	87.8±o.5	87.0±o.4	68.8±o.6	57.6±0.2	555.3±42.4
HGP: DOT	88.2±o.6	85.8±o.5	70.4±o.7	57.0±0.2	797.5±84.9
HGP: latentRS (?)	86.8±0.8	87.5±o.9	67.6±o.6	58.6±0.5	438.3±50.2
HGPTatentGA (?)	88.4±o.7	86.8±o.7	67.0±o.9	56.4±0.2	475.5±58.5
Stacked MNIST						
WGAN-GP	88.7±o.8	87.3±o.5	81.1±o.2	75.7±0.2	594.2±8.2
WGP - DRS	89.9±0.5	88.5±o.6	80.2±o.i	75.1±0.1	527.2±9.2
WGP: SIR-GAN	89.9±o.5	88.4±o.6	80.3±o.2	75.1±0.1	528.1±13.4
WGP: MH-GAN	90.3±o.6	88.9±o.5	80.2±o.i	75.1±0.1	527.2±13.4
WGP: DOT	90.8±o.5	90.2±o.7	79.4±o.ι	74.1±0.2	484.0±8.8
WGP: latentRS (?)	90.4±o.4	86.6±o.5	80.3±0.1	75.3±0.1	568.5±9.i
WGPTatentGA (?)	90.3±o.7	87.2±0.5	80.1±o.2	74.9±0.2	530.1±6.6
CelebA 64x64	—					
Hinge-SN	88.2±o.6	66.7±0.8	37.7±o.2	34.5±0.2	17.8±o.i
HSN: DRS	88.1±o.7	66.5±i.2	38.3±o.7	34.5±0.2	19.1±o.2
HSN: SIR	87.9±o.6	66.9±i.3	38.1±o.2	34.4±0.2	19.2±o.2
HSN: MH	88.1±o.8	67.3±i.3	38.1±o.3	34.4±0.3	19.2±o.2
HSN: DOT	88.0±o.8	67.4±i.5	37.4±o.3	34.3±0.3	18.0±o.i
HSN: latentRS (?)	88.0±o.6	68.1±i.5	37.3±o.3	34.1±0.3	18.0±0.1
HSNTatentGA (?)	88.1±o.6	66.8±1.1	37.4±0.2	34.3±0.2	18.1±0.2
Table 4: Comparing latentRS and latentGA with previous works (DRS, MH-GAN, SIR-GAN, DOT)
on the image generation task. Interestingly, latentGA seems the best performer and latentRS matches
sato with a significantly reduced inference cost (by an order of 20). ± is 97% confidence interval.
Inference refers to the time needed to compute one image on a NVIDIA V100 GPU. ± is 97%
confidence interval.
15
Under review as a conference paper at ICLR 2021
(a)	Generated samples on the Celeba dataset 64x64
ordered by the discriminator (left to right, top to
bottom).
(b)	Generated samples on the Celeba dataset
64x64 ordered by the importance weighter (left to
right, top to bottom).
(c)	Generated samples on the Celeba dataset 64x64
ordered by the discriminator (left to right, top to
bottom).
(d)	Generated samples on the Celeba dataset
64x64 ordered by the importance weighter (left to
right, top to bottom).
Figure 7:	We observe that the ranking with the discriminator does not perfectly correlate with human
judgment, prohibiting the importance weighter to learn meaningful features. Consequently, the target
distribution is not better approximated.
16