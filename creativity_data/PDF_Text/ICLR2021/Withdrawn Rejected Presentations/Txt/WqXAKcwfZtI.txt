Under review as a conference paper at ICLR 2021
f -DOMAIN-ADVERSARIAL LEARNING: THEORY AND
Algorithms for Unsupervised Domain Adapta-
tion with Neural Networks
Anonymous authors
Paper under double-blind review
Ab stract
The problem of unsupervised domain adaptation arises in a variety of practical ap-
plications where the distribution of the training samples differs from those used at
test time. The existing theory of domain adaptation derived generalization bounds
based on divergence measures that are hard to optimize in practice. This has
led to a large disconnect between theory and state-of-the-art methods. In this pa-
per, we propose a novel domain-adversarial framework that introduces new theory
for domain adaptation and leads to practical learning algorithms with neural net-
works. In particular, we derive a novel generalization bound that utilizes a new
measure of discrepancy between distributions based on a variational characteri-
zation of f -divergences. We show that our bound recovers the theoretical results
from Ben-David et al. (2010a) as a special case with a particular choice of di-
vergence, and also supports divergences typically used in practice. We derive a
general algorithm for domain-adversarial learning for the complete family of f-
divergences. We provide empirical results for several f -divergences and show that
some, not considered previously in domain-adversarial learning, achieve state-of-
the-art results in practice. We provide empirical insights into how choosing a
particular divergence affects the transfer performance on real-world datasets. By
further recognizing the optimization problem as a Stackelberg game, we utilize
the latest optimizers from the game optimization literature, achieving additional
performance boosts in our training algorithm. We show that our f -domain adver-
sarial framework achieves state-of-the-art results on the challenging Office-31 and
Office-Home datasets without extra hyperparameters.
1 Introduction
The ability to learn new concepts and skills from general-purpose
data and transfer them to similar scenarios is critical in many mod-
ern applications. For example, it is often the case that the learner
has access to only a small (unlabeled) subset of data on its domain
of interest, but has access to a larger labeled dataset (for the same
task) in a domain that is similar to the target domain. If the gap
between these two domains is not considerable, we may expect
to train a model by using the labeled and unlabeled data, and to
generalize well to the target dataset. This scenario is called unsu-
pervised domain adaptation, and it is the focus of this paper.
The paramount importance of domain adaptation (DA) has led to
remarkable advances in the field. From a theoretical point of view,
the seminal works of Ben-David et al. (2007; 2010a;b); Mansour
et al. (2009) provided generalization bounds for unsupervised DA
Figure 1: Domain Adaptation. A
learner is trained on abundant la-
beled data and is expected to per-
form well in the target domain
(marked as +). Decision bound-
aries correspond to a 2-layers neu-
ral net trained using f -DAL.
based on discrepancy measures that are a reduction of the Total Variation (TV). More recently, Zhang
et al. (2019) took one step further and proposed the Margin Disparity Discrepancy (MDD) with the
aim of closing the gap between theory and algorithms. Their notion of discrepancy is tailored to
margin losses and builds on the observation of only taking a single supremum over the class set
1
Under review as a conference paper at ICLR 2021
to make optimization easier. Moreover, theories based on weighted combination of hypotheses for
multiple source DA have also been developed (Hoffman et al., 2018a).
From an algorithmic perspective, specifically in the context of neural networks, Ganin & Lempit-
sky (2015); Ganin et al. (2016) proposed the idea of learning domain-invariant representations as
a two-player zero-sum game. This approach led to a plethora of methods including state-of-the-art
approaches such as Shu et al. (2018); Long et al. (2018); Hoffman et al. (2018b); Zhang et al. (2019).
While these methods were explained with insights from the theory of Ben-David et al. (2010a), and
more recently through MDD (Zhang et al., 2019), in deep neural networks, both the H∆H diver-
gence (Ben-David et al., 2010a) and MDD are hard to optimize, and ad-hoc objectives have been
introduced to minimize the divergence between source and target distributions in a representation
space. This has led to a disconnect between theory and the current SoTA methods. Specifically,
approaches that follow Ganin et al. (2016) minimize a Jensen Shanon (JS) divergence, while the
practical objective of MDD can be interpreted as minimizing a γ-weighted JS divergence. From
the optimization perspective, the game-optimization nature of the problem has been ignored, and
these min-max objectives are usually optimized using Gradient Descent Ascent (GDA) (referred to
as Gradient Descent (GD) with the Gradient Reversal Layer (GRL)). Paradoxically, the last iterate
of GDA is known to not converge to a Nash equilibrium even in simple bilinear games (Nemirovsky
& Yudin, 1983).
The aim of this paper is to provide a novel perspective on the domain-adversarial problem by de-
riving theory that generalizes previous seminal works and translates into a new general framework
that supports the complete family of f -divergences and is practical for modern neural networks. In
particular, we introduce a novel measure of discrepancy between distributions and derive its corre-
sponding learning bounds. Our notion of discrepancy is based on a variational characterization of
f -divergences and includes both previous theoretical results (i.e. based on reductions of the TV)
and practical results (i.e. based on JS). We empirically show that any f -divergence can be used
to learn invariant representations. Most importantly, we show that several divergences that were
not considered previously in domain-adversarial learning achieve SoTA results in practice. From
an optimization point of view, we observe that under mild conditions, the optimal solution of our
framework is a Stackelberg equilibrium. This allows us to plug-and-play the latest optimizers from
the recent min-max optimization literature within our framework.
We also discuss practical considerations in deep networks, and compare how learning invariant
representations for different choices of divergence affects the transfer performance on real-world
datasets. We further discuss the practical gains (for popular f -divergences) that can be achieved by
introducing more advanced optimizers. We will release code upon acceptance.
2 Preliminaries
In this paper, we focus on the unsupervised domain adaptation scenario. During training, we assume
that the learner has access to a source dataset ofns labeled examples S = {(xis, yis)}in=s1, and a target
dataset of nt unlabeled examples T = {(xit)}in=t 1, where the source inputs xis are sampled i.i.d. from
a distribution Ps (source distribution) over the input space X and the target inputs xti are sampled
i.i.d. from a distribution Pt (target distribution) over X. Usually, in the case of binary classification,
we have Y = {0, 1} and in the multiclass classification scenario, Y = {1, ..., k}. When X or Y
cannot be inferred from the context or other assumptions are required, we will mention it explicitly.
We denote a labeling function by f : X → Y , and the source and target labeling functions by
fs and ft , respectively. The task of unsupervised domain adaptation is to find a hypothesis func-
tion h : X → Y that generalizes to the target dataset T (i.e., to make as few errors as possi-
ble by comparing with the ground truth label ft (xit)). The risk of a hypothesis h w.r.t. the la-
beling function f, using a loss function ` : Y × Y → R+ under distribution D is defined as:
RD(h, f) ：= Ex〜D['(h(x), f (x))]. We also assume that' satisfies the triangle inequality. For sim-
plicity of notation, we define RS (h) := RP,(h, fs) and RT (h) := Rpt (h, f) where the indices S
and T refer to the source and target domains, respectively. We additionally use RS, RT to refer to
the empirical risks over the source dataset S and the target dataset T.
2
Under review as a conference paper at ICLR 2021
Divergence	φ(x)	Φ*(t)	φ0(1)	g(X)
Kullback-Leibler (KL)	x log x	exp(t - 1)	1	X
Reverse KL (KL-rev)	- log x	-1 - log(-t)	-1	- exp X
Jensen-Shannon (JS)	-(X + 1) log 1++x + X log X	- log(2 - et)	0	log T-T-2-T
Pearson χ2	(x-1)2	t2/4 + t	0	1+exp(-x) X
Total Variation (TV)	2|x - 1l	1-1∕2≤t≤1∕2	[-1/2, 1/2]	2 tanh X
Ellrc 1 f T	. 1	■	. C . ■	1 1	C	T ， 1 τ`/ 7 ∖	/7	∖
Table 1: Popular f -divergences, their conjugate functions and choices of g. We take l(a, b) = g(bargmax a).
2.1 COMPARING SOURCE AND TARGET DOMAINS WITH f -DIVERGENCES
A key component of domain adaptation is the study of the discrepancy between the source and
target distributions. This differentiates transductive approaches and more generally transfer learning
from traditional supervised learning methods. In our work, we derive generalization bounds that
capture the entire family of f -divergences. We define new discrepancies between source and target
distributions, based on the variational characterization of popular choices of f -divergences. These
new discrepancies play a fundamental role in our work.
Definition 1 (f -divergence, Csiszar (1967); Ali & Silvey (1966)). Let Ps and Pt two distributions
functions with densities ps and pt, respectively. Let ps and pt be absolute continuous with respect
to a base measure dx. Let φ : R+ → R be a convex, lower semi-continuous function that satisfies
φ(1) = 0. The f -divergence Dφ is defined as:
Dφ(Ps||ptP) = ZPP(X φ(管)
dx.
(2.1)
Variational characterization of f -divergences: Nguyen et al. (2010) derive a general variational
method that estimates f -divergences from samples by turning the estimation problem into variational
optimization. They show that any f -divergence can be written as (see details in Appendix A.2):
Dφ(Ps∖∖Pt) ≥ sup Ex〜Ps[T(x)] — Ex〜Pt[φ*(T(x))]	(2.2)
T∈T
where φ* is the (Fenchel) conjugate function of φ : R+ → R defined as φ* (y) := supx∈R十{xy 一
φ(χ)}, and T : X → dom φ*. The equality holds if T is the set of all measurable functions. Many
popular divergences that are heavily used in machine learning and information theory are special
cases of f -divergences. We summarize them and their conjugate function in Table 1. For simplicity,
we assume in the following that X ⊆ Rn and each density (i.e ps and pt) is absolutely continuous.
3 Domain Adaptation Theory
Domain adaptation approaches generally build upon the idea of bounding the gap between the source
and target domains’ error functions in terms of the discrepancy between their probability distribu-
tions. Measuring the similarity between the distributions Ps and Pt is thus critical in the derivation
of generalization bounds and/or the design of algorithms. We remind the reader of the seminal work
of Ben-David et al. (2010a) that bounds the risk of any binary classifier in the hypothesis class H
with the following theorem:
Theorem 1. If '(x, y) = ∖h(x) — y∖ and H is a class offunCtionS,thenfor any h ∈ H we have:
RT(h) ≤ RS(h) + DTV(PkP)+min{Ex〜PS[∖ft(x) — fs(x)∖],Ex〜Ptp[∖ft(x) — fs(χ)∖]}. (3.1)
Here DTV(PSkPt) := SuPT∈t ∖Ex〜ps[T(x)] — Ex〜pJT(x)]∖ is the TV and T is the set of measur-
able functions. TV is an f -divergence such that φ(x) = ∖x — 1∖ in Definition 1. For any function
φ(x) ≥ ∖x — 1∖, one can replace DTV(PskPt) in eq. 3.1 with Dφ(PskPt). Theorem 1 thus bounds
a classifiers target error in terms of the source error, the divergence between the two domains, and
the dissimilarity of the labeling functions. Unfortunately, DTV(PskPt) cannot be estimated from
finite samples of arbitrary distributions (Kifer et al., 2004). It is also a very loose upper bound as it
involves the supremum over all measurable functions and does not account for the hypothesis class.
3
Under review as a conference paper at ICLR 2021
3.1	MEASURING DISCREPANCY WITH f -DIVERGENCES
This section introduces a new discrepancy that aims to solve the two aforementioned problems,
namely (1) estimation of the divergence from finite samples of arbitrary distributions (Lemma 2)
and (2) restriction of the discrepancy to the set including the hypothesis class H. (Defs. 2 and 3).
We show in Sec. 3.2 how these allows to extend the bounds studied in Ben-David et al. (2010a).
Definition 2 (DH discrepancy). Let φ* be the Fenchel conjugate ofa convex, lower semi-continuous
function φ that satisfies φ(1) = 0, and let T be a set of measurable functions such that T =
{'(h(x), h0(x)) : h,h0 ∈ H}. We define the discrepancy between PS and Pt as:
DH(PsIlPt)：= SUp |Ex〜PS['(h(x),h0(x))] - Ex〜Pt[φ*('(h(x),h0(x)))∣.	(3.2)
h,h0∈H
The DφH discrepancy can be interpreted as a lower bound estimator of a general class of f-
divergences (Lemma 1). Therefore, for any hypothesis class H and choice of φ, DφH is never larger
than its corresponding f -divergence. We show in Lemma 2 that its computation can be bounded in
terms of finite examples. Finally, we recover the H∆H divergence (Ben-David et al., 2010a) if we
consider φ*(t) = t and '(h(x), h0(x)) = 1[h(x) = h0(x)], which corresponds to the TV.
Definition 3 (Dhφ,H discrepancy). Suppose the same conditions as above, the discrepancy between
two distributions Ps and Pt is defined by:
Dh H(PS||Pt)：= SUp |Ex〜Ps['(h(x),h0(x))] - Ex〜Pt[φ"('(h(x),h0(x)))∣.	(3.3)
,	h0∈H
Taking the supremum of D? H over h ∈ H, we obtain dH，and thus Dφ H(Ps||Pt) ≤ DH(Ps∣∣Pt).
This bound will be useful when deriving practical algorithms.
Lemma 1 (lower bound). For any two functions h,h0 in H, we have:
IRS(h,h0) - RT*"(h,h0)∣ ≤ DDΦ,h(PSIIPt) ≤ DH(PSIIPt) ≤ Dφ(PsIIPt).	(3.4)
Lemma 1 is fundamental in the derivation of divergence-based generalization bounds for DA. Specif-
ically, it bounds the gap between the source and target domains’ error functions in terms of the
discrepancy between their distributions using f -divergences. We now show that the Dhφ,H can be
estimated from finite samples.
Lemma 2. Suppose ' : Y × Y → [0,1], φ* L-Lipschitz, and [0,1] ⊂ dom φ*. Let S and T be
two empirical distributions corresponding to datasets containing n datapoints sampled i.i.d. from
Ps and Pt, respectively. Let us note R the Rademacher complexity ofa given class of functions, and
' ◦ H := {x → '(h(x), h0(x)) : h,h0 ∈ H}. ∀δ ∈ (0,1), we have with probability ofat least 1 — δ:
IDΦ,h(AI阳)-DΦH(S∖∖T)I≤ 2Rps('。冗)+ 2LRP,(' oH) + 2,(-logδ)∕(2n).	(3.5)
In Lemma 2, we show that the empirical Dφh,H converges to the true Dhφ,H discrepancy. It can
then be estimated using a set of finite samples from the two distributions. The gap is bounded by
the complexity of the hypothesis class and the number of examples (n). This result will be also
important in the derivation of Theorem 3.
3.2	Domain Adaptation: Generalization B ounds
We now provide a novel generalization bound to estimate the error of a classifier in the target domain
using the proposed Dhφ,H divergence and results from the previous section. We also provide a gener-
alization Rademacher complexity bound for a binary classifier based on the estimation of the Dφh,H
from finite samples. We show that our bound generalizes previous existing results in Appendix D.1.
Theorem 2 (generalization bound). Suppose ' : Y × Y → [0,1] ⊂ dom φ* and that '(a, b) ≤
'(a, C) + '(c, b) for any a,b,c ∈ Y. Denote λ* := RS (h*) + RT (h*), and let h* be the ideal joint
hypothesis. We have:
RT(h) ≤ RS(h)+ DhH(PsIIPt) + λ*.	(3.6)
4
Under review as a conference paper at ICLR 2021
The three terms in this upper bound share similarity with the bounds proposed by Ben-David et al.
(2010a) and more recently by Zhang et al. (2019). The main difference lies in the discrepancy
being used to compare the two marginal distributions. In the case of Ben-David et al. (2010a),
they use the H∆H divergence (a reduction of the TV), and in Zhang et al. (2019), they use the
MDD. In our case, we use a reduction of a lower bound estimator of a variational characterization
of the general f -divergences. This generalizes the TV (and thus Ben-David et al. (2010a)) and also
includes popular divergences typically used in practice (see Appendix D). Intuitively, the first term
in the bound accounts for the source error, the second corresponds to the discrepancy between the
marginal distributions, and the third measures the ideal joint hypothesis (λ*). If H is expressive
enough and the labeling functions are similar, this last term could be reduced to a small value. The
ideal joint hypothesis incorporates the notion of adaptability: when the optimal hypothesis performs
poorly in either domain, we cannot expect successful adaptation.
Theorem 3 (generalization bound with Rademacher complexity). Let ` : Y × Y → [0, 1] and
φ* be L-Lipschitz. Let S and T be two empirical distributions (i.e. datasets containing n data points
SamPledi.i.d.from PS and Pt, respectively). Denote λφ := RS(h*) + RT(h*). ∀δ ∈ (0,1), we have
with probability of at least 1 - δ:
RT(h) ≤ RS(h)+ DΦ,h(s||T) + λ
+ 6RS(' ◦ H) + 2(1 + L)Rt(' ◦ H) + 5,(-log δ)∕(2n).	(3.7)
In Theorem 3, we show the computation of our generalization bound for a binary classifier in terms
of the Rademacher complexity of the class H. We see that under the assumption of an ideal joint
hypothesis, λφ, the generalization error can be reduced by jointly minimizing the risk in the source
domain, the discrepancy between the two distributions, and regularizing the model to limit the com-
plexity of the hypothesis class. We take all these into account when deriving practical algorithms in
the next sections.
4 f-Domain Adversarial Learning (f-DAL)
We now use the theory presented in the previous sections to derive a
novel generalized domain adversarial learning framework. The key
idea of domain-adversarial training is to simultaneously minimize
the source error and align the two distributions in a representation
space Z. Specifically, We let a hypothesis h be the composition of
^ ^ ^ ʌ ʌ
h = h◦ g (i.e let H := {h◦ g : h ∈ H,g ∈ G} with H another func-
tion class) where g : X → Z. This can be interpreted as a mapping
that pushes forward the two densities PS and Pt to a representation
ʌ ʌ
space Z where a classifier h ∈ H operates. Consequently, we refer
to PZ := g#PS and Pz := g#Pt as the push-forwards of the source
and target domain densities, respectively. Figure 2 illustrates the
f -DAL framework.


Clearly from Theorem 2, for adaptation to be possible in the repre-
sentation space Z, there has tobe an h ∈ H, such that the ideal joint
risk λ* is negligible. This condition is necessary even if Pz = Pz. In
other words, we need the difference between Psz and Ptz to be small,
and the ideal joint risk λ* to be negligible. These are both sufficient
and necessary conditions. We refer the reader to Ben-David et al.
(2010b) for details on the impossibility theorems for DA. Conse-
quently, we state the following:
Assumption 1. There ISa g ∈ G and h* ∈ H, such that the
ideal joint risk (λ*) is negligible. We also assume that the class-
conditional distributions 1 between source and target are similar.
A
Figure 2: f-DAL framework.
We interpret h : X → Y as
the composition of two nets h =
h ◦ g, where g : X → Z and h is
a classifier that operates in a rep-
resentation space Z . inspired by
the theory, we let h0 be another
net of the same topology than
h. This is intuitively interpreted
as a per-category domain classi-
fier. Our framework is different
from domain-adversarial frame-
works that follows from (Ganin
et al., 2016) since they use a
global domain-classifier or dis-
criminator.
While these assumptions may seem restrictive, they are ubiquitous in modern DA methods, including
SoTA methods i.e Ganin et al. (2016); Long et al. (2018); Hoffman et al. (2018b); Zhang et al. (2019)
1Also referred to as the coovariate-shift assumption (Shimodaira, 2000). The source and target domains
only differ in their marginals according to the input space.
5
Under review as a conference paper at ICLR 2021
(sometimes not explicitly mentioned). Moreover, neural networks are generally known to be able to
ʌ
learn rich and powerful representations, and in practical scenarios, g and h are both neural networks.
From Theorem 2 and Assumption 1, the target risk RT (h) can be optimized by jointly minimizing
the error in the source domain and the discrepancy between the two distributions. Letting y :=
fs(x), an optimization objective can be clearly written as:
m∈iH Ez 〜Pz ['(h(z),y)] + Dφ ,h (PzIIpz).	(4.1)
Here, ` is a surrogate loss function used to minimize the empirical risk in the source domain.
Nonetheless, it does not have to be the binary classification loss (i.e it can be the cross-entropy
loss). Under some assumptions (Proposition 1) and the use of Lemma 1, the minimization problem
in equation 4.1 can be upper bounded (hence replaced) by the following min-max objective2:
min max Ez〜pz ['(h(z), y)] + Ez〜pz ['(h0(z), h(z))] - Ez〜Pz [(φ* ◦ ')(h0(z), h(z))]	(4.2)
h∈H h 0∈H	S	、----s--------------------{z------------------------}
ds,t
where we refer to the difference between the last two terms as ds,t . We now formalize this result.
ʌ ʌ ʌ
Proposition 1. Suppose ⅛,t takes the form shown in equation 4.2 With '(h0(z), h(z)) → dom φ*
pz (z)
and thatfor any h ∈ H, there exists h ∈ H s.t. '(h0(z), h(z)) = φ (pζ(z)) forany Z ∈ supp(pZ(Z)),
with φ0 the derivative of φ. The optimal ⅛,t isDφ(Ps∖∖Pf) (i.e maxRo∈H ⅛,t = Dφ(Ps∖∖Ptz)).
If we let the feature extractor g ∈ G be the one that minimizes both the source error and the discrep-
ancy term, equation 4.2 can be rewritten as:
ʌ ʌ ʌ ʌ ʌ ʌ ʌ
,min max Ex〜Ps ['(h ◦ g, y)] + Ex~Ps ['(h ◦ g, h ◦ g)] - Ex~pt[(φ* ◦ ')(h ◦ g, h ◦ g)]. (4.3)
h∈Hg∈G h-∈H
ʌ
The choice of ` is “somewhat arbitrary” as stated for GANs in Nowozin et al. (2016). For the
ʌ
multiclass scenario, we let `(a, b) = g(bargmaxa), where argmax a is the index of the largest element
ʌ
of vector a. For the binary case, we define '(_, b) = g(b). This implies that we choose the domain
of ' to be Rk X Rk with k categories for the multi-class scenario and R for binary classification.
Intuitively, h0 is an auxiliary per-category domain classifier. Note that this is different from Ganin
et al. (2016) where there is a unique domain classifier or discriminator. For the choice of g, we
follow Nowozin et al. (2016) and choose it to be a monotonically increasing function when possible.
ʌ
We summarize our choices of ` for different f -divergences in Table 1. We also show other choices
ʌ
of ` including generalizations of previous methods in Appendix D.
γ-weighted JS divergence. If we relax the need for φ(1) = 0 in Proposition 1, the new objective
ʌ
only shifts by a constant, e.g, maxh,∈H ⅛,t = Dφ(Psz∣∣Ptz) + φ(1) with φ(x) := φ(x) - φ(1).
By Lemma 4 (Appendix D), We can then rescale φ*, and φ will change accordingly. This allows to
include the practical objective from Zhang et al. (2019) as part of our framework (i.e. γ-weighted
JS, see Appendix D). While these can be done for the general family of divergences, we do not
pursue this line of research in practice as it requires additional hyperparameter tuning of γ.
4.1	OPTIMALITY IN f -DAL
The main objective of our framework (i.e equation 4.3) is a minimax optimization problem and our
desired (optimal) solution is under mild assumptions a Stackelberg equilibrium. This key obser-
vation allows us to incorporate in our framework the latest optimizers from the game-optimization
literature. We now formalize and prove this concept. Based on this, we propose to use the extra-
gradient algorithm and its aggressive version within our framework. We show the effectiveness
through a toy example (Figure 3, Appendix E) and also empirically in our large scale experiments.
Stackelberg equilibria in f -DAL. We show the existence of Stackelberg equilibria in f -Domain
Adversarial Learning (f -DAL). Let G and H be a class of functions defined by a fixed parametric
functional (i.e neural networks with fixed architecture), and define ω1 such that is a vector composed
2Indeed, under these assumptions, ds,t can be seen as an upper bound for the Dhφ,H discrepancy.
6
Under review as a conference paper at ICLR 2021
of the parameters of the feature extractor g and the source classifier h. Similarly, let ω2 be the pa-
rameters of the auxiliary classifier h0, and Ω1 and Ω2 denote their separate domains. Equation (4.2)
can be rewritten as:
min max V (ω1, ω2).	(4.4)
ωι ∈Ωι ω2 ∈Ω2
In general, V is nonconvex in ω1 and nonconcave in ω2, and for the min-max game in equation 4.4,
Nash equilibria may not exist as in (Farnia & Ozdaglar, 2020). A Stackelberg equilibrium is more
general than Nash equilibrium (see definition in Definition 5) and reflects the sequential nature of
our zero-sum game equation 4.4. We now show that the optimal solution of f -DAL is a Stackelberg
equilibrium. Such an equilibrium is a stationary point under the assumption that V (ω1, ∙) is (locally)
strongly concave in ω2 (Evtushenko, 1974), and we can then use gradient algorithms to search for
such a desirable solution. In the following theorem, we use the explicit form of push-forward to
emphasize the dependence on the feature extractor g, rather than psz , ptz.
Theorem 4 (Stackelberg equilibrium, informal). Suppose ds,t takes the form shown in equa-
tion 4.2, and assume that (a) There exists an optimal g* ∈ G that maps both the source and the
target distribution to the same distribution. (b) There exists an optimal classifier that yields the
ground truth in a neighborhood. (c) For any g ∈ G and h ∈ H, there exists h0 that achieves
ʌ ʌ ʌ
'(h0(Z),h(z)) = φ((g#pS)(Z)/(g#pt)(z)). Then the objective of f -adversarial learning has a
*
Stackelberg equilibrium at (h*, g*, h0 ).
ʌ ʌ ʌ
When '(h, h0) = '(h0) (e.g., in a binary classification scenario or in
Ganin et al. (2016)), the Stackelberg equilibrium can be shown to be
a Nash equilibrium (see Theorem 6).
Extra-gradient algorithms. We have shown that the optimal solution
in f -DAL is a Stackelberg equilibrium which is more general than a
Nash equilibrium. For convergence to a Nash equilibrium, the sim-
plest method is GDA. However, the last iterate of GDA does not con-
verge in the bilinear case (e.g. Nemirovsky & Yudin, 1983). To accel-
erate and stabilize the convergence, the extra-gradient (EG) method
was proposed in (Korpelevich, 1976) . It was recently shown (Zhang
et al., 2020; Hsieh et al., 2020) that having an aggressive extra-step
is even more stable than vanilla EG, and is more suitable for conver-
gence to Stackelberg equilibria (Zhang et al., 2020). With the aim
of quantifying whether exploiting Theorem 4 leads to practical gains,
we follow those works, and therefore let the extrapolation step to be
Figure 3: Comparison of GDA
vs AExG in a toy task with JS
as divergence. G is the class
of quadratic functions and H
is linear. AExG can accelerate
the convergence to the optimal
solution. (Appendix E)
larger. We refer to this algorithm as Aggressive Extra-Gradient (AExG). We illustrate this in a
simple example (Appendix E), whose convergence/trajectories are shown in Figure 3 and we will
explore AExG further in the experimental section.
5	Experimental Results
We present experimental results of our framework in practical scenarios. In these scenarios, the
learner is a neural network and the input domain is the set of natural images. Specifically, we aim
to answer the following questions: (1) How does choosing a particular divergence affect the do-
main adaptation performance among different datasets? (2) Is there a better universal notion of
f -divergence that achieves significant performance gains across different datasets and thus helps
generalization? (3) Are there considerable practical gains by exploiting the fact that the optimal
solution of f -DAL is a Stackelberg Equilibrium? (4) How does our theoretical framework compare
in practice to existing SoTA methods? We also compare in Figure 4 the difference in interpreta-
tion of the auxiliary classifier of f -DAL vs Ganin et al. (2016). The comparison shows significant
performance boost for the same divergence i.e DANN vs f -DAL (JS)
Datasets. In our experiments we use two main datasets. We use (1) the Office-31 dataset (Saenko
et al., 2010) which contains 4,652 images and 31 categories, collected from three distinct domains:
Amazon (A), Webcam (W) and DSLR (D). We also use (2) the Office-Home dataset (Venkateswara
et al., 2017). This is a more complex dataset containing 15,500 images from four different domains:
Artistic images, Clip Art, Product images, and Real-world images. In each of our experiments, we
report the average over 3 different seeds.
7
Under review as a conference paper at ICLR 2021
Method	A→W	D→W	W→D	A→D	D→A	W→A	Avg
ResNet-50 (He et al., 2016)	68.4±0.2	96.7±0.1	99.3±0.1	68.9±0.2	62.5±0.3	60.7±0.3	76.1
DANN (Ganin et al., 2016)	82.0±0.4	96.9±0.2	99.1±0.1	79.7±0.4	68.2±0.4	67.4±0.5	82.2
ADDA (Tzeng et al., 2017)	86.2±0.5	96.2±0.3	98.4±0.3	77.8±0.3	69.5±0.4	68.9±0.5	82.9
JAN (Long et al., 2017)	85.4±0.3	97.4±0.2	99.8±0.2	84.7±0.3	68.6±0.3	70.0±0.4	84.3
GTA (Sankaranarayanan et al., 2018)	89.5±0.5	97.9±0.3	99.8±0.4	87.7±0.5	72.8±0.3	71.4±0.4	86.5
MCD (Saito et al., 2018)	88.6±0.2	98.5±0.1	100.0±.0	92.2±0.2	69.5±0.1	69.7±0.3	86.5
CDAN Long et al. (2018)	94.1±0.1	98.6±0.1	100.0±.0	92.9±0.2	71.0±0.3	69.3±0.3	87.7
MDD (Zhang et al., 2019)	94.5±0.3	98.4±0.1	100.0±.0	93.5±0.2	74.6±0.3	72.2±0.1	88.9
Ours (Pearson χ2, SGD)	95.4 ±0.7	98.4 ± 0.2	100.0±.0	93.8 ±0.4	73.5 ±1.1	74.2 ±0.5	89.2
Ours (Pearson χ2, AExG)	95.3 ±0.1	98.8 ± 0.3	100.0±.0	94.2 ±0.6	73.3 ±0.3	75.3 ±0.2	89.5
Table 2: Comparison vs previous unsupervised domain adaptation approaches on the Office-31 benchmark.
Accuracy represented in (%) with average and standard deviation. Impressively, our approach achieves SoTA
results without the need of additional techniques (i.e CDAN) or additional hyperparameters (i.e MDD∕γ-JS).
Implementation Details: We implement our algorithm in PyTorch. We use ResNet-50 (He et al.,
ʌ
2016) pretrained on ImageNet (Deng et al., 2009) as the feature extractor. The main classifier (h)
ʌ
and auxiliary classifier (h0) are both 2 layers neural nets with Leaky-Relu activation functions. We
ʌ ʌ
use spectral normalization (SN) as in Miyato et al. (2018) only for these two (i.e h and h0 ). We
did not see any transfer improvement by using it, neither by using Leaky-Relu activation functions
instead of Relu. The reason for this was to avoid gradient issues and instabilities during training for
some divergences (i.e KL, TV) in the first epochs. For simplicity, and fair comparison with previous
work, we perform simultaneous updates using the GRL. We also use the GRL warm-up strategy.
This is standard in most DA frameworks and follows from Eq. (14) in Ganin & Lempitsky (2015).
For optimization, we use 1) mini-batch (32) SGD (or GDA) with the Nesterov momentum (0.9). 2)
For experiments using AExG, we take inspiration from Gidel et al. (2019) and implement a version
of the ExtraGradient with momentum (0.9). For the aggressive step, we use a multiplier [10, 1] with
a polynomial decay rate with power= 0.5 for the first 10K iterations. In all cases, the learning rate
of the classifiers is set 10 times larger than the one of the feature extractor (0.01) whose value is
adjusted according to Ganin et al. (2016), which is standard practice. We will release source code.
Comparing f -divergences. We
first compare the performance of f-
divergences on Office-31. Specifi-
cally, we evaluate the performance of
the model on the six combinations
of transfer tasks with different diver-
gences. The optimizer is SGD with
Nesterov Momentum. All hyperpa-
rameters are kept constant for all di-
vergences. As shown in Figure 4, the
Pearson χ2 achieves the best over-
all result among all the transfer tasks
on this benchmark. This divergence
was never used before to learn in-
variant representations in the context
of DA. Interestingly, a similar trend
was observed for GANs in Nowozin
et al. (2016). This observation is
also reminiscent of histogram-based
Figure 4: Transfer performance of a model trained using f -DAL
for different choices of divergences and different transfer tasks on
the Office-31 benchmark. Baseline is ResNet-50 w/o f -DAL. We
additionally show the performance of DANN (Table 2). When
compared with f-DAL (JS), We see a significant performance
boost. This is in line with our theory which suggests the use of
(visual) bag of words representations a per-category domain classifier VS a discriminator.
that were shown to work better with χ2 distances than with '2 and '1 distances for image and text
classification tasks (i.e. (Li et al., 2013) and references therein). The KL divergence performs well
in some transfer tasks but significantly worse than the rest in others (i.e D → A). The reason might
be that, unlike the JS, TV and Pearson χ2 divergences which are lower and upper bounded by finite
values, the KL divergence can grow exponentially and tend to +∞ even when the densities PS and
Pt are nonzero (Nielsen & Nock, 2013). The lack of upper bound of the KL divergence might lead
to numerical instability of the optimizer and explain inconsistency of performance.
What do We get by the “extra” gradient? We now compare the use of the AExG vs GDA method
with Nesterov Momentum. The main idea is to evaluate whether the characterization of the optimal
solution off-DAL as a Stackelberg Equilibrium leads to practical gains by exploiting more suitable
8
Under review as a conference paper at ICLR 2021
A-»D 2W D-»A t»W W->A W-»D AVG
■ TV JS	Pearson
so
89
SGDNeetew ββ
---ExtraSGDAesreeIve
2	4 β 8 Iol2	14	16	18	20 87
Figure 5: left) Relative Improvement (%) AExG Vs GDA(SGD) for different choices of divergences and transfer
tasks on the Office-31 benchmark. Overall, we observe gains in performance among all divergences. right)
Transfer curves for Pearson χ2 on the task A→W on the Office-31 benchmark (# Iter vs Acc). We can see
AExG converges faster and also obtains slightly better results. This is inline with the insights obtained from
the theoretical results presented in Sec 4.1 and Appendix 5.
Method	Ar→Cl	Ar→Pr	Ar→Rw	Cl→Ar	Cl→Pr	Cl→Rw	Pr→Ar	Pr→Cl	Pr→Rw	Rw→Ar	Rw→Cl	Rw→Pr	Avg
ResNet-50 (He et al., 2016)	34.9	50.0	58.0	37.4	41.9	46.2	38.5	31.2	60.4	53.9	41.2	59.9	46.1
DANN (Ganin et al., 2016)	45.6	59.3	70.1	47.0	58.5	60.9	46.1	43.7	68.5	63.2	51.8	76.8	57.6
JAN (Long et al., 2017)	45.9	61.2	68.9	50.4	59.7	61.0	45.8	43.4	70.3	63.9	52.4	76.8	58.3
CDAN (Long et al., 2018)	50.7	70.6	76.0	57.6	70.0	70.0	57.4	50.9	77.3	70.9	56.7	81.6	65.8
MDD (Zhang et al., 2019)	54.9	73.7	77.8	60.0	71.4	71.8	61.2	53.6	78.1	72.5	60.2	82.3	68.1
Ours (Pearson χ2, SGD) Ours (Pearson χ2, AExG)	54.7	69.4	77.8	61.0	72.6	72.2	60.8	53.4	80.0	73.3	60.6	83.8	68.3
	55.2	70.2	78.6	60.9	73.2	72.8	61.3	53.6	80.5	73.7	61.0	83.6	68.7
Table 3: Accuracy (%) on Office-Home for unsupervised DA. Impressively, our approach achieves SoTA with-
out additional techniques (i.e. CDAN), or additional hyperparameters (i.e. MDD).
optimizers (Section 4.1). In both cases, we use a momentum coefficient (0.9). The experiments are
performed on Office-31 and hyperparameters are kept constant for all divergences. In Figure 5, we
observe that using AExG significantly improves the performance in some transfer tasks for some di-
vergences (i.e JS in A → W). Overall, we also observe gains in performance among all divergences.
Figure 5 illustrates the transfer curves of the AExG vs GDA with Nesterov Momentum for the task
A→ W. For this divergence and pair of datasets, AExG converges faster and also obtains slightly
better accuracy. This is in line with the insights obtained from the theoretical results presented in
Section 4.1 and Appendix 5. If computation is not an issue we encourage the use of AExG. That
said, f -DAL achieves comparable performance with GDA in terms of accuracy (see Tables 2 and 3).
A look to state-of-the-art arena. We also compare our best method (i.e Pearson χ2 + AExG and
Pearson χ2 + GDA) with current SoTA unsupervised DA methods. We use this in a leader-board
like fashion. For fair comparison, we use the same network architecture (i.e Resnet-50), training
strategy and set of hyperparameters from Long et al. (2018); Zhang et al. (2019) from where we took
the baselines results. Our approach achieves SoTA results on Office-31 and Office-Home Datasets.
Remark on SoTA Comparison: We compare with SoTAs that rely on adversarial training since
this is the focus of our work. Therefore, methods such as Kang et al. (2019) are not included as
they rely on additional techniques that neither our method nor the proposed baselines use and could
be added to improve the performance further. Our goal is to propose a unifying framework that
connects the theory used to explain DANN (Ganin et al., 2016) (and similar SoTA algorithms), and
the algorithms themselves. The new theory results in a new adversarial framework (Sec 4), which
impressively outperforms previous SoTA (Tables 2 and 3). This follows from connecting theory
and algorithms and a proper interpretation of the former. Our results can be further improved with
additional tuning or techniques (i.e CDAN) since most of SoTAs either follow from Ganin et al.
(2016) or are part of our framework (i.e MDD= γ-JS). This problem is deferred to future work.
6	Conclusions
We have provided a novel perspective on the domain-adversarial problem by deriving new theory
and learning algorithms that support the complete family of f -divergences, and that are practical
for modern neural networks. We further recognize the learning objective of our framework as a
Stackelberg game, borrowing the latest optimizers from the game-optimization literature, achieving
additional performance boosts. We show through large-scale experiments that any f -divergence can
be used to minimize the discrepancy between source and target domains in a representation space.
We also show that some divergences, not considered previously in domain-adversarial learning,
achieve SoTA results in practice, reducing the need for additional techniques and hyperparameter
tuning as required by previous methods.
9
Under review as a conference paper at ICLR 2021
References
Syed Mumtaz Ali and Samuel D Silvey. A general class of coefficients of divergence of one distri-
bution from another. Journal of the Royal Statistical Society: Series B (Methodological), 28(1):
131-142,1966.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010a.
Shai Ben-David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain adap-
tation. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and
Statistics, pp. 129-136, 2010b.
Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Imre Csiszar. Information-type measures of difference of probability distributions and indirect ob-
servation. studia scientiarum Mathematicarum Hungarica, 2:229-318, 1967.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-
erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,
pp. 248-255. IEEE, 2009.
Yurii Gavrilovich Evtushenko. Some local properties of minimax problems. Zhurnal Vychislitel’noi
Matematiki i Matematicheskoi Fiziki, 14(3):669-679, 1974.
Farzan Farnia and Asuman Ozdaglar. GANs may have no Nash equilibria. In International Confer-
ence on Machine Learning, 2020.
Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In
International conference on machine learning, pp. 1180-1189. PMLR, 2015.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Gauthier Gidel, Hugo Berard, Gaetan Vignoud, Pascal Vincent, and Simon Lacoste-Julien. A varia-
tional inequality perspective on generative adversarial networks. In ICLR, 2019.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-source
adaptation. In Advances in Neural Information Processing Systems, pp. 8246-8256, 2018a.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International
conference on machine learning, pp. 1989-1998. PMLR, 2018b.
Yu-Guan Hsieh, Franck Iutzeler, Jerome Malick, and Panayotis Mertikopoulos. Explore aggres-
sively, update conservatively: Stochastic extragradient methods with variable stepsize scaling.
arXiv preprint arXiv:2003.10162, 2020.
Ferenc Huszar. How (not) to train your generative model: Scheduled sampling, likelihood, adver-
sary? arXiv preprint arXiv:1511.05101, 2015.
Guoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network
for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 4893-4902, 2019.
10
Under review as a conference paper at ICLR 2021
Daniel Kifer, Shai Ben-David, and Johannes Gehrke. Detecting change in data streams. In VLDB,
volume 4, pp. 180-191. Toronto, Canada, 2004.
GM Korpelevich. The extragradient method for finding saddle points and other problems. Matecon,
12:747-756, 1976.
Ping Li, Gennady Samorodnitsk, and John Hopcroft. Sign cauchy projections and chi-square kernel.
In Advances in Neural Information Processing Systems, pp. 2571-2579, 2013.
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael I. Jordan. Deep transfer learning with joint
adaptation networks, 2017.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
Yishay Mansour, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning
bounds and algorithms. In Proceedings of The 22nd Annual Conference on Learning The-
ory (COLT 2009), Montreal, Canada, 2009. URL http://www.cs.nyu.edu/~mohri/
postscript/nadap.pdf.
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization
for generative adversarial networks. In International Conference on Learning Representations,
2018.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning.
MIT press, 2018.
Arkadii Semenovich Nemirovsky and David Borisovich Yudin. Problem complexity and method
efficiency in optimization. Wiley-Interscience series in discrete mathematics, 1983.
XuanLong Nguyen, Martin J Wainwright, and Michael I Jordan. Estimating divergence functionals
and the likelihood ratio by convex risk minimization. IEEE Transactions on Information Theory,
56(11):5847-5861, 2010.
Frank Nielsen and Richard Nock. On the chi square and higher-order chi distances for approximating
f-divergences. arXiv preprint arXiv:1309.3029, 2013.
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers
using variational divergence minimization. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon,
and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 271-279. Cur-
ran Associates, Inc., 2016.
IeVgen Redko, Emilie Morvant, Amaury Habrard, Marc Sebban, and Younes Bennani. Advances in
Domain Adaptation Theory. Elsevier, 2019.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new
domains. In European conference on computer vision, pp. 213-226. Springer, 2010.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier dis-
crepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 3723-3732, 2018.
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to
adapt: Aligning domains using generative adversarial networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 8503-8512, 2018.
Igal Sason and Sergio Verdu. f -divergence inequalities. IEEE Transactions on Information Theory,
62(11):5973-6006, 2016.
Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the log-
likelihood function. Journal of statistical planning and inference, 90(2):227-244, 2000.
Rui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A dirt-t approach to unsupervised domain
adaptation. In International Conference on Learning Representations, 2018.
11
Under review as a conference paper at ICLR 2021
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain
adaptation, 2017.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In (IEEE) Conference on Computer Vision
and Pattern Recognition (CVPR), 2017.
Mei Wang and Weihong Deng. Deep visual domain adaptation: A survey. Neurocomputing, 312:
135-153,2018.
Guojun Zhang, Pascal Poupart, and Yaoliang Yu. Optimality and stability in non-convex smooth
games. In arxiv: 2002.11875, 2020.
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan. Bridging theory and algorithm
for domain adaptation. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of
the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine
Learning Research, pp. 7404-7413, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/zhang19i.html.
12
Under review as a conference paper at ICLR 2021
A Related Work
A. 1 Domain adaptation
We briefly but clearly positioned our approach w.r.t. related work mentioned in the paper. We refer
the reader to Redko et al. (2019) and Wang & Deng (2018) for a comprehensive survey.
A.2 Divergences between probability measures
As explained above, the difference term between source and target domains is important in bounding
the target loss. We now provide more details about the H∆H-divergence and f -divergences that are
used to compare both domains.
H∆H-divergence The H-divergence is a restriction of total variation. For binary classification,
define I(h) := {x ∈ X : h(x) = 1}, then the H-divergence between two measures μ and V given
the hypothesis class H is (Ben-David et al., 2010a):
dH(μ, V)=2 sup ∣μ(I(h)) - V(I(h))|.	(A.1)
h∈H
Define H∆H := {h ㊉ h0 : h, h0 ∈ H}(㊉：XOR), then d,H∆H(μ, V) can be used to bound the
difference between the source and target errors. H∆H divergence has been extended to general loss
functions (Mansour et al., 2009) and marginal disparity discrepancy (Zhang et al., 2019).
f -divergence Given two measures μ and V with μ《V (μ absolute continuous wrt V), the f -
divergence Dφ(μ∣∣V) is defined as (Csiszar, 1967; Ali & Silvey,1966):
dφ(μ Il V) = Z φ d-j~d dV,	(AZ
where dμ∕du is known as the Radon-Nikodym derivative (e.g. Billingsley, 2008). Assume φ is
convex and lower semi-continuous, then from the Fenchel-Moreau theorem, φ** = φ, with φ*
known as the Fenchel conjugate of φ:
φ*(y) = sup hx, yi — φ(x),	(A.3)
x∈dom φ
which is convex since it is a supremum of an affine function. In order for x to take the supremum,
it is necessary and sufficient that y ∈ ∂φ(x) using the stationarity condition. Therefore, with
equation A.2 and equation A.3, Dφ(μ ∣∣ V) can be written as:
Dφ(μ Il V) = SUP EX〜μ [T(X )] — EZ〜V [φ* (T (Z))],	(A.4)
T∈T
where T = {T : T is a measurable function and T : X → dom φ*}. In practice we restrict T to a
subset as in Definition 2. For different choices of φ see Table 4.
Nguyen et al. (2010) derive a general variational method to estimate f -divergences given only sam-
ples. Nowozin et al. (2016) extend their method from merely estimating a divergence for a fixed
model to estimating model parameters. While our method builds on this variational formulation, we
use it in the context of domain adaptation.
B	Proofs
In this section, we provide the proofs for the different theorems and lemmas:
Theorem 1. If '(x, y) = |h(x) — y| and H is a class offunCtions,thenfor any h ∈ H we have:
Rr(h) ≤ RS(h) + DTV(PskP) +min{Eχ〜PS[|ft(x) — fs(x)∣],Ex〜Ptp[|ft(x) — fs(x)∣]}. (3.1)
Proof. Rewriting the target loss we have:
RT (h) = RT (h) — RS (h,ft) + RS (h,ft) — RS (h) + RS (h),
≤ RS(h) + |RS(h) — RS(h,ft)l + ∣RT(h) — RS(h,ft)l
13
Under review as a conference paper at ICLR 2021
Divergence	φ(x)	φ*(t)	φ0(1)	g(X)
MDD	X log 1++γχ + Y log 1+1γχ	- log(1 - et)/Y	log 1++γ	log X
Kullback-Leibler (KL)	x log x	exp(t - 1)	1	X
Reverse KL (KL-rev)	- log x	-1 - log(-t)	-1	- exp X
Jensen-Shannon (JS)	-(X + 1) log 1++x + x log x	- log(2 - et)	0	log rη~27~~V 1+exp(-x)
Pearson χ2	(X - 1)2	t2 /4 + t	0	X
Squared Hellinger (SH)	(√X -1)2	t 1 — t	0	1 - exp X
γ-Weighted Pearson χ2	(YX T)2/Y	(t2/4 + t)/Y	0	X
Neynman χ2	(1 — x)2 x	2 - 2√1-t	0	1 - exp X
γ-Weighted total variation	药 |YX - 1|	(t/Y) 1-1∕2≤t≤1∕2	[-1/2, 1/2]	2 tanhX
Total Variation (TV)	2|x - 1l	1-1∕2≤t≤1∕2	[-1/2, 1/2]	2 tanhX
mil 4-n»	-1 f τ	. ι	■	. c∙	. ■	ι 1	c∙	τ ， 1 τ` / 7 ∖	/7	∖
Table 4: Popular f -divergences, their conjugate functions and choices of g. We take l(a, b) = g(bargmax a).
where:
|RS(h) - RS(h,ft)l = R(h,fs) - RS(h,ft)l
=|Ex〜Ps[∣h(χ)- ft(χ)∣-∣h(χ)- fs(χ)∣]∣
≤ Ex〜Ps[∣ft(χ)- fs (x)|]
and:
|RT(h) - RS(h,ft)l = |Rt(h,ft) - RS(h,ft)l ≤ / ∣Pt(x) -Ps(x)∣∙ |h(x) - ft(x)∣dx
-1)Ps(x)∣dx = Dφ(Ps∣∣Pt)
with φ(x) = |x - 1| which represents the total divergence.
□
Lemma 1 (lower bound). For any two functions h,h0 in H, we have:
|RS(h,h0) - RT*"(h,h0)∣ ≤ Dh,H(PS||Pt) ≤ DH(PS||Pt) ≤ Dφ(Ps||Pt).	(3.4)
Proof.
DH(PsIIPt) = sup Dh H(PsIIPt) ≥ Dh H(PsIIPt)	(B.1)
h∈H ,	,
=sup IEx〜Ps['(h(x),h0(x))] - Ex〜p∕φ"('(h(x),h0(x)))]I	(B.2)
h0∈H
≥ IEx〜Ps['(h(x),h0(x))] - Ex〜Mφ*('(h(x),h0(x)))]I	(B.3)
=irS (h,h0) - W(h,hN	(B.4)
For the rightmost inequality in equation 3.4, it is well-known that f -divergence Dh is nonnegative
(e.g. Sason & VerdU, 2016), and thus
Dh(PSlIE) = sup IEx〜PST(x) - Ex〜Ptφ*(τ(x))I.
T ∈T
(B.5)
Restricting T to T as in Definition 2 We obtain Dh(PSkPt) ≥ DH(PSIIPt).	□
Lemma 2. Suppose ' : Y × Y → [0,1], φ* L-Lipschitz, and [0,1] ⊂ dom φ*. Let S and T be
two empirical diStributionS correSponding to dataSetS containing n datapointS Sampled i.i.d. from
Ps and Pt, respectively. Let us note R the Rademacher complexity of a given class of functions, and
' ◦ H := {x → '(h(x), h0(x)) : h,h0 ∈ H}. ∀δ ∈ (0,1), we have with probability ofat least 1 — δ:
IDIh(PsIIPt) - DΦ,h(SIIT)I ≤ 2RpS(' ◦ H) + 2LRPi(' ◦ H) + 2,(-logδ)∕(2n).	(3.5)
14
Under review as a conference paper at ICLR 2021
Proof. For reference, we refer the reader to Chapter 3 of Mohri et al. (2018). Using the notations of
R and R that represent the true and empirical risks, we have:
Dφ丸(PsIIPt)- Dφ丸(SIrT)=SUP {∣RS(h,h')- RTe(h,h∖∖}	(B.6)
,	,	h0三H
-sup {∣RS(h, h0) - RT*"(h, h0)∣}
h0∈H
≤ sup ∣∣RS(h,h0)- RT*"(h,h,)∣ - ∣RS(h,h0)- R."(h,h0)∣∣
h0∈H
≤ sup ∣RS(h, h') - Rφ*oe(h, h') - RS(h, h') + RT*"(h, h')∣
h0∈H
=sup ∣RS(h, h') - RS(h, h0)∣ + ∣RΦ*oe(h, h 0) - RT*oe(h, h')∣
h0∈H
≤ 2Rr('o∏) + ∖1^γl + 2Rr(Φ* ◦ 4°H) + ∖1^γl
2n	2n
Where: ∣RS(h, h') - RS(h, h ')∣ ≤ 2Rps (` o H) + ʌ/ɪo2nɪ (Theorem 3.3 of Mohri et al. (2018)).
Similarly, by Talagrand,s lemma (Lemma 5.7 and Definition 3.2 of Mohri et al. (2018)) we have:
Rp"φ* o ` o H) ≤ LRPt(` o H), with φ* o ` o H := {x 1 φ('(h(x), h'(x))) : h, h' ∈ H}. 口
Theorem 2 (generalization bound). Suppose ` : Y × Y → [0,1] U dom φ* and that '(α, b) ≤
'(α, c) + '(c, b) for any α,b,c ∈ Y. Denote λ* := RS (h*) + RT (h*), and let h* be the ideal joint
hypothesis. We have:
RT(h) ≤ RS(h)+ Dh,%(Ps∣∣Pf) + λ*.	(3.6)
Proof. We first introduce the following lemma for our proof:
Lemma 3. For any function φ that satisfies φ(1) = 0 we have φ*(t) ≥ t where φ* is the Fenchel
conjugate of φ.
Proof. From the definition of Fenchel conjugate, φ*(t) = supx∈dom @(xt - φ(x)) ≥ t - φ(1)=
t.	口
With the triangle inequality of `, we can write:
Rr (h,ft) ≤ RT (h, h*)+ RT (h*,ft)	(B.7)
=RT(h, h*) + RT(h*,ft) - RS(h, h*) + RS(h, h*)	(B.8)
≤ RT*oe(h, h*) - RS(h, h*) + RS(h, h*) + RT(h*,ft)	(Lemma 3)	(B.9)
≤ ∣RT*oe(h, h*) - RS(h, h*)∣ + RS(h, h*) + RT(h*,ft)	(B.10)
≤ Dh,丸(Ps∣∣R) + RS(h, h*) + RT(h*,ft)	(Lemma 1)	(B.11)
≤ Dh,丸(Ps∣∣R) + RS(h,fs)+ RS(h*,fs)+ RT(h*,".	(B.12)
,^^^^^-^^^^^^{l^^^^^^―^^^^^^}
λ*
□
Theorem 3 (generalization bound with Rademacher complexity). Let ` : Y × Y → [0,1] and
φ* be L-Lipschitz. Let S and T be two empirical distributions (i.e. datasets containing n data points
sampled i.i.d.from PS and Pt, respectively). Denote λφ := RS(h*) + RT(h*). ∀δ ∈ (0,1), we have
with probability of at least 1 一 δ:
RT(h) ≤ RS(h)+ D仁(S∣∣T) + 其
+ 6Rs(` oH) + 2(1 + DRT(Q oH)+ 5√(-log δ)∕(2n).	(3.7)
15
Under review as a conference paper at ICLR 2021
Proof. We show in the following that:
RT(h) ≤ RS(h)+ Dh,H(S||T) + λφ	(B.13)
+ 6Rs (' ◦H)+ 2(1 + L)Rt(' ◦H) + 5,(-log δ)∕(2n).	(B.14)
This follows from Theorem 2 where: RT(h) ≤ RS(h) + Dh h(Ps∖∖Pi) + RS(h*) + RT(h*). We
also have: |RD(h) 一 RD(h)| ≤ 2Rd('◦H) + J¾l (Theorem of 3.3 Mohri et al. (2018)). From
Lemma2,Dφ,H (Ps ||Pt) ≤ 2Rps (' ◦H) + 2LRpt (' oH) + 2 ʌ/lθgl. Plugging in and rearranging
gives the desired results.	□
ʌ ʌ ʌ
Proposition 1. Suppose ⅛,t takes the form Shown in equation 4.2 With '(h0(z), h(z)) → dom φ*
and thatfor any h ∈ H, there exists h0 ∈ H s.t. '(h'(z), h(z)) = φ0(ps(z)) forany Z ∈ SuPP(Pz(Z)),
with φ0 the derivative of φ. The optimal ⅛,t isDφ(Psz||PZ) (i.e maxhγ∈H ⅛,t = Dφ(PS∖∖Ptz)).
Proof. We first rewrite from the definition of ds,t in equation 4.2:
ds,t
Z
[Pz(z)'(h (z), h(z)) — Pz(Z)(Φ* ◦ ')(h (z), h(z))]dz
∕pZ(z)黯"h'(z),h(z))-(φ* ◦ ')(h'(z), h(z)) dz.
(B.15)
(B.16)
(B.17)
Maximizing w.r.t h0 and assuming H is unconstrained We have: Pz(Z) ∈ (∂φ*)(%(h0(z), h(z)) for
any Z ∈ suPP(Ptz). From the definition of Fenchel conjugate we have:
X ∈ ∂φ*(t) ^⇒ φ(x) + φ*(t) = Xt ^⇒ φ0(x) = t.
ʌ ʌ ʌ ʌ
Plugging x = pZ(z)∕pz(z) and t = '(h0(z), h(z)) we obtain '(h0(z), h(z)) = φ0(pZ(z)/Pz(Z)).
Hence, from the definition of f -divergences (Definition 1) and its variational characterization
(eq. 2.2), we write:
maH ds,t=Dφ(p"Ptz )∙
(B.18)
□
C THE EXISTENCE OF THE STACKELBERG/NASH EQUILIBRIUM IN f -DAL
In this appendix, we formally define the Nash/Stackelberg equilibrium and show that they exist in
our f -DAL framework under mild assumptions.
Definition 4 (Stackelberg equilibrium). A StaCkeIberg equilibrium (ωj,ωg) ∈ Ω1 X Ω2 of the
min-max game satisfies ∀(ω1,ω2) ∈ Ωι × Ω2, V(ωj,ω2) ≤ V(ωj,ωg) ≤ maXω2∈Ω2 V(ω1,ω2)∙
Definition 5 (Nash equilibrium). A Nash equilibrium (ωj,ωg) ∈ Ωι × Ω2 of the min-max game
equation 4.4 is defined such that ∀(ω1,ω2) ∈ Ω1 × Ω2, V(ω"ω2) ≤ V(ωj,ωg) ≤ V(ω1,ω^)∙
Theorem 5 (Stackelberg equilibrium). Suppose ds,t takes the form shown in equation 4.2, and
assume that (a) There is an optimal feature extractor g* ∈ G that maps both the source and the
target distribution to the same distribution, i.e. g*#PS = g*#Pt. (b) There is an optimal classi-
ʌ ʌ ʌ
fier s.t. h* ◦ g* = fs is the ground truth, and h ◦ g = fs for any (h, g) in a neighborhood of
ʌ ʌ ʌ
(h*, g*). (c) For any g ∈ G and h ∈ H, there exists h0 s.t. for any z ∈ suPP(g#Ps), one has
ʌ ʌ ʌ
'(h0(z), h(z)) = 00((g#Ps)(z)/(g#Pt)(z)). Then the objective of f-adversarial learning has a
**
Stackelberg equilibrium at (h*, g*, h0 ), where ∀z ∈ suPP(g*#Ps), `(h* (z), h0 (z)) = φ0(1).
Proof. At g* and h*, we have
ʌ ʌ
ʌ ʌ ʌ
ʌ ʌ ʌ
ds,t(h*,g*, h0) = Ez 〜g*#Ps '(h *(z), h0(z)) - φ*('(h*(z), h0(z)))∙
(C.1)
16
Under review as a conference paper at ICLR 2021
ʌ ʌ ʌ
Maximizing over '(h*(z), h0(z)) yields:
*,
'(h (z), h0 (z)) = Φ (1), Vz ∈ supp(g #Ps).	(C.2)
*
In other words, ds,t(h*, g*, h0) ≤ ds,t(h*, g*, h0 ) for any h0 ∈ H. Now let us prove that
maxho∈H^ ds,t(h,g, h0) ≥ ds,t(h*,g*,h0 ) = φ(1) = 0. This is because from our assumptions
ʌ ʌ
and Lemma 1, maxh,∈r ʤt(h, g, h0) = D®(g#Ps kg#Pt) ≥ 0.
So far, we have shown that
*
ds,t(h*,g*,h0) ≤ ds,t(h*,g*,h0 ) ≤ max ds,t(h,g, h0)	(C.3)
h 0∈H^
ʌ ʌ ʌ
for any h, h0 ∈ H and g ∈ G. Also, (h*, g*) is an optimal pair for the source loss, namely:
ʌ ʌ
Rs(h*,g*) ≤ Rs(h,g),	(C.4)
for any h ∈ H and g ∈ G. Combining equation C.3 and equation C.8, We claim that (h*,g*,h0*) is
a Stackelberg equilibrium.	□
Theorem 6 (Nash equilibrium). With the same assumptions as in Theorem 5 and assume also that
ʌ ʌ ʌ
' only depends on the second argument, i.e., '(h,h0) = '(h0). Then the objective of f -adversarial
*
learning has a Nash equilibrium at (h*, g*, h0 ) where
[*
supp(g#ps), '(h0 (z)) = φ (1).	(C.5)
g∈G
ʌ
Proof. The proof is in parallel to the proof of Theorem 5 except that at h0* ,
*
ds,t(g, h0*) = Ez〜Pz'(h0 (z)) - Ez〜pzφ* ◦ '(h0*(z))
= φ0(1) - φ*(φ0(1))	(C.6)
is a constant in terms of g, and thus We have:
ds,t(g, h0*) ≥ ds,t(g*, h0*) ≥ ds,t(g*, h0),	(C.7)
*
for any g ∈ G and h0* ∈ H. Combining With equation C.8 We conclude that (h*, g*, h0 ) is a Nash
ʌ
equilibrium of ds,t. Also, (h*, g*) is an optimal pair for the source loss, namely:
ʌ ʌ
Rs(h*,g*) ≤ Rs(h,g),	(C.8)
*
for any h ∈ H andg ∈ G. Combining equation C.7, We claim that (h*, g*, h0 ) is a Nash equilibrium
of the objective in f -DAL.	□
D Connection to previous frameworks
In this appendix We shoW that f -DAL encompasses previous frameWorks on domain adaptation,
including H∆H-divergence, DANN (Ganin et al., 2016) and MDD (Zhang et al., 2019).
D.1 H∆H-DIVERGENCE
We noW shoW that Theorem 2 generalizes the bound proposed in Ben-David et al. (2010a). Let
the pair {φ(x), φ*(t)} = {2 |x - 1∣,t} for t ∈ [0,1], such that Dφ H = DhVH and suph∈H DhVH =
DTV = 2d∏∆H, with d∏∆H defined in Ben-David et al. (2010a) (see also equation A.1). Theorem 2
gives us that RT(h) ≤ RS(h) + 2d∏∆H + λ*, recovering Theorem 2 of Ben-David et al. (2010a).
17
Under review as a conference paper at ICLR 2021
D.2 DANN formulation and JS divergence
The DANN formulation by Ganin & Lempitsky (2015) can also be incorporated in our framework
if one takes '(a, b) = log b and φ*(t) = - log(1 - et). Effectively, this formulation ignores the
contribution of the source classifier and experimentally we saw it had inferior performance compared
ʌ
to using `(a, b) = g(bargmax a ).
The original idea of domain adversarial training was introduced by Ganin et al. (2016) where the au-
thors defined the following surrogate function to measure the discrepancy between the two domains:
ds,t ：= Exs〜ps[logh0(g(xs))]+ Ext〜pt[log(1 - h0(g(xt)))].	(D.1)
In this context, h0 was defined to be a domain classifier, that is h0 : Z → {0,1} with 0 and 1
corresponding to the source and target domain pseudo-labels. The following proposition shows
that under the assumption of an optimal domain classifier h0, ⅛,t achieves JS-divergence (UP to a
constant shift), which upper bounds the DJhS,H .
Proposition 2. Suppose ds,t follows the form of eq. D.1 and h is the optimal domain classifier which
is unconstrained, then max/, ⅛,t = DJS(S||T) — 2log2.
Proof. From the definition, we have:
ds,t(h0,g)= P pz(z)log h0(z) + Pz(Z) log(1- h0(z))dz.
Z
(D.2)
By taking derivatives and finding the optimal h*(z), We get: h*(z)=.低彳⑶
ʌ
By plugging h*(z) into equation D.1, rearranging, and using the definition of the Jensen-Shanon
(JS) divergence, we get the desired result.	□
It is worth noting that the additional negative constant -2 log 2 does not affect the optimization.
D.3 MDD FORMULATION AND γ-WEIGHTED JS DIVERGENCE
Now let us demonstrate how our f -DAL framework incorporates MDD naturally. Suppose φ*(t) =
-1 log(1 - et) and '(h(z), h0(z)) = log h0(z)argmaxh(z). We retrieve the following result as in
Zhang et al. (2019):
Proposition 3 (Zhang et al. (2019)). Suppose ds,t takes the form of MDD, i.e,
γds,t =YEz〜PS log h0(z)a- 力⑶ + Ez〜Pt h(Z) ∙log(1 - hO(Z)。…Q	(D.3)
With unconstrained function class H, the optimal ds,t satisfies:
maxγds,t = (γ+ 1)JSγ(pszkptz) +γlogγ - (γ+ 1) log(γ + 1),	(D.4)
h0
where JSY(pZ∣∣pZ) is Y-weighted Jensen-Shannon divergence (HuSzar, 2015; Nowozin et al., 2016):
JSY (p Z kP Z) = ɪ KL(P Z k γpz+pz) + -4τKL(P Z k γpz+pz).	(D.5)
Y+1	Y+1 Y+1	Y+1
We remark that when Y = 1, JSY(PzkPz) is the original Jensen-Shannon divergence. One should
also note the the additional negative constant Y log Y - (Y + 1) log(Y + 1), which attributes to the
negativity of MDD, does not affect the optimization.
φ*(t) = - Y log(1 - et) can be considered by rescaling the φ* for the usual JS divergence (see
Table 4). In general we can rescale φ* for any f -divergence with the following lemma:
Lemma 4 (Boyd & Vandenberghe (2004)). For any λ > 0, the Fenchel conjugate of λφ is
(λφ)*(t) = λφ*(t∕λ), with dom(λφ)* = λdomφ*.
18
Under review as a conference paper at ICLR 2021
E A toy example for the training dynamics
Suppose that the source dataset S and target dataset T contain one sample each and are formulated:
S = {(0.5, 1)} and T = {0.55}. Let the feature extractor to be quadratic functions, and we choose
linear predictors, i.e.:
g(x) = wιx2 + x, h(x) = w2x, h0(x) = σ(w3x).	(E.1)
Let us consider the regression task with the JS divergence used to compare S and T.
min maxEx〜Ps(fs(x) - h(g(x)))2 + Ex〜Pslogh0(g(x))+
w1 ,w2 ∈R w3∈R
+ Ex〜Ptlog(I- h0(g(x))).
ʌ
If We consider that g(0.5) = g(0.55), h(g(0.5)) = 1 and w3 = 0, then the optimal solution
(w1,w2,w3) satisfies the assumption in Theorem 5. We plot the trajectories of GDA and AExG in
Figure 3 and shoW that AExG can accelerate the convergence to the optimal solution.
19