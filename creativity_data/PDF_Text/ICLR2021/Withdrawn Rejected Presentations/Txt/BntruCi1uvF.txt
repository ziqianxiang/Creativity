Under review as a conference paper at ICLR 2021
Truly Deterministic Policy Optimization
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we present a policy gradient method that avoids exploratory noise
injection and performs policy search over the deterministic landscape. By avoiding
noise injection all sources of estimation variance can be eliminated in systems with
deterministic dynamics (up to the initial state distribution). Since deterministic
policy regularization is impossible using traditional non-metric measures such as
the KL divergence, we derive a Wasserstein-based quadratic model for our purposes.
We state conditions on the system model under which it is possible to establish
a monotonic policy improvement guarantee, propose a surrogate function for
policy gradient estimation, and show that it is possible to compute exact advantage
estimates if both the state transition model and the policy are deterministic. Finally,
we describe two novel robotic control environments—one with non-local rewards
in the frequency domain and the other with a long horizon (8000 time-steps)—
for which our policy gradient method (TDPO) significantly outperforms existing
methods (PPO, TRPO, DDPG, and TD3).
Policy Gradient (PG) methods can be broadly characterized by three defining elements: the policy
gradient estimator, the regularization measures, and the exploration profile. For gradient estimation,
episodic (Williams, 1992), importance-sampling-based (Schulman et al., 2015a), and deterministic
(Silver et al., 2014) gradients are some of the most common estimation oracles. As for regularization
measures, either an Euclidean distance within the parameter space (Williams, 1992; Silver et al.,
2014; Lillicrap et al., 2015), or dimensionally consistent non-metric measures (Schulman et al.,
2015a; Kakade & Langford, 2002; Schulman et al., 2017; Kakade, 2002; Wu et al., 2017) have been
frequently adapted. Common exploration profiles include Gaussian (Schulman et al., 2015a) and
stochastic processes (Lillicrap et al., 2015). These elements form the basis of many model-free and
stochastic policy optimization methods successfully capable of learning high-dimensional policy
parameters.
Both stochastic and deterministic policy search can be useful in applications. A stochastic policy
has the effect of smoothing or filtering the policy landscape, which is desirable for optimization.
Searching through stochastic policies has enabled the effective control of challenging environments
under a general framework (Schulman et al., 2015a; 2017). The same method could either learn
robotic movements or play basic games (1) with minimal domain-specific knowledge, (2) regardless
of function approximation classes, and (3) with less human intervention (ignoring reward engineering
and hyper-parameter tuning) (Duan et al., 2016). Using stochasticity for exploration, although it
imposes approximations and variance, has provided a robust way to actively search for higher rewards.
Despite many successes, there are practical environments which remain challenging for current policy
gradient methods. For example, non-local rewards (e.g., those defined in the frequency domain),
long time horizons, and naturally-resonant environments all occur in realistic robotic systems (Kuo
& Golnaraghi, 2002; Meirovitch, 1975; Preumont & Seto, 2008) but can present issues for policy
gradient search.
To tackle challenging environments such as these, this paper considers policy gradient methods
based on deterministic policies and deterministic gradient estimates, which could offer advantages by
allowing the estimation of global reward gradients on long horizons without the need to inject noise
into the system for exploration. To facilitate a dimensionally consistent and low-variance deterministic
policy search, a compatible policy gradient estimator and a metric measure for regularization should
be employed. For gradient estimation we focus on Vine estimators (Schulman et al., 2015a), which
can be easily applied to deterministic policies. As a metric measure we use the Wasserstein distance,
which can measure meaningful distances between deterministic policy functions that have non-
1
Under review as a conference paper at ICLR 2021
overlapping supports (in contrast to the Kullback-Liebler (KL) divergence and the Total Variation
(TV) distance).
The Wasserstein metric has seen substantial recent application in a variety of machine-learning
domains, such as the successful stable learning of generative adversarial models (Arjovsky et al.,
2017). Theoretically, this metric has been studied in the context of Lipschitz-continuous Markov
decision processes in reinforcement learning (Hinderer, 2005; Ferns et al., 2012). Pirotta et al. (2015)
defined a policy gradient method using the Wasserestein distance by relying on Lipschitz continuity
assumptions with respect to the policy gradient itself. Furthermore, for Lipschitz-continuous Markov
decision processes, Asadi et al. (2018) and Rachelson & Lagoudakis (2010) used the Wasserstein
distance to derive model-based value-iteration and policy-iteration methods, respectively. On a
more practical note, Pacchiano et al. (2019) utilized Wasserstein regularization for behavior-guided
stochastic policy optimization. Moreover, Abdullah et al. (2019) has proposed another robust
stochastic policy gradient formulation. Estimating the Wasserstein distance for general distributions
is more complicated than typical KL-divergences (Villani, 2008). This fact constitutes and emphasizes
the contributions of Abdullah et al. (2019) and Pacchiano et al. (2019). However, for our deterministic
observation-conditional policies, closed-form computation of Wasserstein distances is possible
without any approximation.
Existing deterministic policy gradient methods (e.g., DDPG and TD3) use deterministic policies
(Silver et al., 2014; Lillicrap et al., 2015; Fujimoto et al., 2018), meaning that they learn a deterministic
policy function from states to actions. However, such methods still use stochastic search (i.e., they
add stochastic noise to their deterministic actions to force exploration during policy search). In
contrast, we will be interested in a method which not only uses deterministic policies, but also uses
deterministic search (i.e., without constant stochastic noise injection). We call this new method truly
deterministic policy optimization (TDPO) and it may have lower estimation variances and better
scalability to long horizons, as we will show in numerical examples.
Scalability to long horizons is one of the most challenging aspects for policy gradient methods that
use stochastic search. This issue is sometimes referred to as the curse of horizon in reinforcement
learning (Liu et al., 2018). General worst-case analyses suggests that the sample complexity of
reinforcement learning is exponential with respect to the horizon length (Kakade et al., 2003; Kearns
et al., 2000; 2002). Deriving polynomial lower-bounds for the sample complexity of reinforcement
learning methods is still an open problem (Jiang & Agarwal, 2018). Lower-bounding the sample
complexity of reinforcement learning for long horizons under different settings and simplifying
assumptions has been a topic of theoretical research (Dann & Brunskill, 2015; Wang et al., 2020).
Some recent work has examined the scalability of importance sampling gradient estimators to long
horizons in terms of both theoretical and practical estimator variances (Liu et al., 2018; Kallus &
Uehara, 2019; 2020). All in all, long horizons are challenging for all reinforcement learning methods,
especially the ones suffering from excessive estimation variance due to the use of stochastic policies
for exploration, and our truly deterministic method may have advantages in this respect.
In this paper we focus on continuous-domain robotic environments with reset capability to previously
visited states. The main contributions of this work are: (1) we introduce a Deterministic Vine
(DeVine) policy gradient estimator which avoids constant exploratory noise injection; (2) we derive a
novel deterministically-compatible surrogate function and provide monotonic payoff improvement
guarantees; (3) we show how to use the DeVine policy gradient estimator with the Wasserstein-based
surrogate in a practical algorithm (TDPO: Truly Deterministic Policy Optimization); (4) we illustrate
the robustness of the TDPO policy search process in robotic control environments with non-local
rewards, long horizons, and/or resonant frequencies.
1 Background
MDP preliminaries. An infinite-horizon discounted Markov decision process (MDP) is specified
by (S, A,P,R, μ, γ), where S is the state space, A is the action space, P : S × A → ∆(S)
is the transition dynamics, R : S × A → [0, Rmax] is the reward function, γ ∈ [0, 1) is the
discount factor, and μ(s) is the initial state distribution of interest (where ∆(F) denotes the set of all
probability distributions over F, otherwise known as the Credal set of F). The transition dynamics
P is defined as an operator which produces a distribution over the state space for the next state
s0 〜P(s, a). The transition dynamics can be easily generalized to take distributions of states or
2
Under review as a conference paper at ICLR 2021
actions as input (i.e., by having P defined as P : ∆(S) × A → ∆(S) or P : S × ∆(A) → ∆(S)).
We may abuse the notation and replace δs and δa by s and a, where δs and δa are the deterministic
distributions concentrated at the state s and action a, respectively. A policy π : S → ∆(A) specifies
a distribution over actions for each state, and induces trajectories from a given starting state s as
follows: si = s, aι 〜π(sι), ri = R(sι, aι), s2 〜P(s2,a2), a2 〜∏(s2), etc. We will denote
trajectories as state-action tuples τ = (s1, a1, s2, a2, . . .). One can generalize the dynamics (1)
by using a policy instead of an action distribution P(μs,π) := Es〜μs [E0〜冗⑶[P(s, a)]], and (2)
by introducing the t-step transition dynamics recursively as Pt(μs,π) := P(PtT(μs,π), π) with
P0(μs,π) := μs, where μs is a distribution over S. The visitation frequency can then be defined as
ρμ := (1 - Y) P∞=ι Yt-1Pt-1(μ, π). Table 2 of the appendix summarizes all MDP notation.
The value function of π is defined as V π (s) := E[Pt∞=i γt-irt | si = s; π]. Similarly, one can
define Qπ(s, a) by conditioning on the first action. The advantage function can then be defined as
their difference (i.e. Aπ(s, a) := Qπ(s, a) - V π (s)). Generally, one can define the advantage/value
of one policy with respect to another using An(s, π0) := E[Qπ(s, a) - Vπ(S) | a 〜 π0(∙∣s)] and
Qπ(s, π0) := E[Qπ(s, a) | a 〜π0(∙∣s)]. Finally, the payoff of a policy η∏ := E [V π(s); S 〜μ] is the
average value over the initial states distribution of the MDP.
Probabilistic and mathematical notations. Sometimes we refer to f (x)g(x)dx integrals as
hf, gix Hilbert space inner products. Assuming that ζ and ν are two probabilistic densities,
the Kulback-Liebler (KL) divergence is DKL(Z∣ν) := hZ(x),log(V(X))ix, the Total-Variation
(TV) distance is TV(Z, V) =: ɪh∣Z(χ) - ν(x)∣, 1)x, and the Wasserstein distance is W(Z, V)=
infγ∈Γ(ζ,ν) hkx - yk, Y(x, y)ix,y where Γ(ζ, ν) is the set of couplings for ζ and ν. We de-
fine Lip(f (x,y); x) := SuPx ∣∣Vxf (x,y)k2 and assume the existence of LiP(Qn(s, a); a) and
k LiP(VsQn(s, a); a)∣2 constants. Under this notation, the Rubinstein-Kantrovich (RK) duality
states that the ∣hZ(χ) - V(χ),f (χ)ix∣ ≤ W(Z,ν) ∙ Lip(f; x) bound is tight for all f. For brevity, we
may abuse the notation and denote SuPs W(∏1(∙∣s),∏2(∙∣s)) with W(∏1,∏2) (and similarly for other
measures). For parameterized policies, we define Vnf(π) := Vθf(π) where π is parameterized by
the vector θ. Table 1 of the appendix summarizes all these mathematical definitions.
Policy gradient preliminaries. The advantage decomposition lemma provides insight into the
relationship between payoff improvements and advantages (Kakade & Langford, 2002). That is,
η∏2 - η∏ι = ɪɪ ∙ Es〜p；? [An1(s, π2)].	⑴
We will denote the current and the candidate next policy as πi and π2, respectively. Taking derivatives
of both sides with respect to π2 at πi yields
V∏2η∏2 = ± hV∏2ρμ2(∙),An1 (∙,∏i)i + hρμι(∙), V∏2An1(∙,∏2)i .	(2)
Since ∏ι does not have any advantage over itself (i.e., An1 (∙, ∏ι) = 0), the first term is zero. Thus,
the Policy Gradient (PG) theorem is derived as
v∏2 η∏2∣	=	∙ Es 〜p；1 [Vn2 AnI(S,π2 )]|	.	⑶
n2 =n1	1 - Y	；	n2 =n1
For policy iteration with function approximation, we assume π2 and πi to be parameterized by θ2
and θi, respectively. One can view the PG theorem as a Taylor expansion of the payoff at θi.
Conservative Policy Iteration (CPI) (Kakade & Langford, 2002) was one of the early dimensionally
consistent methods with a surrogate of the form L∏, (∏2) = η∏ + ɪ-Y ∙ Es〜p∏ι [An1 (s, ∏2)]-
C TV2(∏1, ∏2). The C coefficient guarantees non-decreasing payoffs. However, CPI is limited to
linear function approximation classes due to the update rule ∏new — (1 一 α)∏∩ld + α∏0. This lead to
the design of the Trust Region Policy Optimization (TRPO) (Schulman et al., 2015a) algorithm.
TRPO exchanged the bounded squared TV distance with the KL divergence by lower bounding it using
the Pinsker inequality. This made TRPO closer to the Natural Policy Gradient algorithm(Kakade,
2002), and for Gaussian policies the modified terms had similar Taylor expansions within small trust
3
Under review as a conference paper at ICLR 2021
regions. Confined trust regions are a stable way of making large updates and avoiding pessimistic
coefficients. For gradient estimates, TRPO used Importance Sampling (IS) with a baseline shift:
vθ2Es〜ρμι[Aπ1 (s,π2)]∣	= vs s~ρ∏ι	QnI(S，a)π2(a∣s) I .	⑷
lθ2=θ1	a〜∏ι(∙∣s)L	n1(a|s)」∣θ2=θ1
While empirical E[Aπ1 (S, π2)] and E[Qπ1 (S, π2)] estimates yield identical variances in principle, the
importance sampling estimator in (4) imposes larger variances. Later, Proximal Policy Optimization
(PPO) (Schulman et al., 2015b) proposed utilizing the Generalized Advantage Estimation (GAE)
method for variance reduction and incorporated first-order smoothing like ADAM (Kingma & Ba,
2014). GAE employed Temporal-Difference (TD) learning (Bhatnagar et al., 2009) for variance
reduction. Although TD-learning was not theoretically guaranteed to converge and could add bias, it
improved the estimation accuracy.
As an alternative to importance sampling, deterministic policy gradient estimators were also utilized
in an actor-critic fashion. Deep Deterministic Policy Gradient (DDPG) (Lillicrap et al., 2015)
generalized deterministic gradients by employing Approximate Dynamic Programming (ADP) (Mnih
et al., 2015) for variance reduction. Twin Delayed Deterministic Policy Gradient (TD3) (Fujimoto
et al., 2018) improved DDPG’s approximation to build an even better policy optimization method.
Although both methods used deterministic policies, they still performed stochastic search by adding
stochastic noise to the deterministic policies to force exploration.
Other lines of stochastic policy optimization were later proposed. Wu et al. (2017) used a Kronecker-
factored approximation for curvatures. Haarnoja et al. (2018) proposed a maximum entropy actor-
critic method for stochastic policy optimization.
2 Monotonic Policy Improvement Guarantee
We use the Wasserstein metric because it allows the effective measurement of distances between
probability distributions or functions with non-overlapping support, such as deterministic policies,
unlike the KL divergence or TV distance which are either unbounded or maximal in this case. The
physical transition model’s smoothness enables the use of the Wasserstein distance to regularize
deterministic policies. Therefore, we make the following two assumptions about the transition model:
W(P(μ, ∏ι), P(μ, ∏2)) ≤ Ln ∙ W(∏1,∏2),	(5)
W(P(μι, ∏), P(μ2,∏)) ≤ Lμ ∙ W(μι, μ2).	(6)
Also, We make the dynamics stability assumption SuP Pk=ι Lk-M Qt=1+ι Liinι∏ < ∞, With
the definitions of the new constants and further discussion of the implications deferred to Section A.5
of the appendix Where We also discuss Assumptions 5 and 6 and the existence of LiP(Qn (S, a); a).
The advantage decomposition lemma can be reWritten as
η∏2 = η∏ι + 占∙ Es〜ρμι [An1 (s, ∏2)] + 占∙ hP∏2 - ρ∏1, An1 (∙,"	(7)
The hρμ2 — ρμ1,An1 (∙, ∏2)i term has zero gradient at ∏ = ∏ι, which qualifies it to be crudely
called “the second-order term”. The theory behind loWer-bounding this second-order term With all
derivations is left to the appendix. Next, We present the theoretical bottom line.
2.1 The Monotonic Payoff Improvement Guarantee
Combining the results of Inequality (30) and Theorems A.5 and A.4 of the appendix leads us to define
the regularization terms and coefficients:
2 ∙ LiP(Qn1 (s, a); a) ∙ Y ∙ L∏
CI= SuP-----而---加-----Γ~∖---
S (I- Y)(I- YLμ)
k LiP(VsQn1 (s, a); a)k2 ∙ γ ∙ L∏
SuP	(1-γ)(1-γLμ)
LWG(π1,π2; S) := W(π2 (a|S), π1 (a|S))
vs0W
∏2 (a∣s0) + ∏ι(a∣s)
2
×
∏2(a∣s) + ∏1 (a∣s0)ʌ I
2	儿=S 2
(8)
4
Under review as a conference paper at ICLR 2021
This gives us the novel lower bound for payoff improvement:
L∏1p(∏2) = η∏ι + 1—Es〜p7[Aπ1(s,∏2)] - Cl ∙ sup Lwg(∏i,∏2; S)
-C2 ∙ sup W(∏2(a∣s),∏1(a∣s))2 .	(9)
s
We have the inequalities ηπ2 ≥ Lsπu1p(π2) and Lsπu1p(π1) = ηπ1 . This facilitates the application of
Theorem 2.1 as an instance of Minorization-Maximization algorithms (Hunter & Lange, 2004).
Theorem 2.1. Successive maximization of Lsup yields non-decreasing policy payoffs.
Proof. With π2 = arg maxπ Lsπu1p(π), we have Lsπu1p(π2) ≥ Lsπu1p(π1). Thus,
ηπ2 ≥ Lsπu1p(π2) and ηπ1 = Lsπu1p(π1) =⇒ ηπ2 -ηπ1 ≥Lsπu1p(π2)-Lsπu1p(π1) ≥0.	(10)
□
3 Surrogate Optimization and Practical Algorithm
Successive optimization of Lsπup(π2) generates non-decreasing payoffs. However, it is impractical
due to the large number of constraints and statistical estimation of maximums. To mitigate this, we
take a similar approach to TRPO and optimize for the surrogate
L∏1 (∏2) = η∏ι + 十∙ Es〜ρμι [Aπι (s, ∏2)] - Cl ∙ Es〜ρμι LWG(∏1,∏2; S)
-C2 ∙ Es〜ρX W(n2(a|s),n1(a|s))2 .	(II)
Although first order stochastic optimization methods can be directly applied to the surrogate defined
in Equation (11), second order methods can be more efficient. Since LWG (π1, π2; S) is the geometric
mean of two functions of quadratic order, it is also of quadratic order. However, LWG (π1, π2; S) may
not be twice continuously differentiable. For this, we lower bound LWG (π1, π2; S) further using the
AM-GM inequality and replace it with
Lg2 (∏1,∏2; S) := Vs，W (n2(a|s0) + ni(a|s) ,n2(a|s) + πι(als0) M 2	(12)
2 2	s0=s2
to form a quadratic regularization term with a definable Hessian matrix (see Section A.8 of the
appendix for detailed derivations). This induces our final surrogate which is used for second order
optimization:
LnI (π2)	=	η∏ι	+	1—ʒ;	∙ Es〜ρμι	[Aπi(s,π2)] - CI	∙	Es〜ρπ	LG2 (π1,π2;	S)
-C2 ∙ Es〜ρμι W(n2(a|s),ni(a|s))2 .
(13)
The coefficients C1 and C2 are dynamics-dependent. For simplicity, we used constant coefficients
and a trust region. This yields the Truly Deterministic Policy Optimization (TDPO) as given in
Algorithm 1. See the appendix for practical notes on the choice of C1 and C2 . Alternatively, one
could adopt processes similar to Schulman et al. (2015a) where the IS-based advantage estimator used
a line search for proper step size selection, or the adaptive penalty coefficient setting in Schulman
et al. (2017). We plan to consider such approaches in future work.
3.1	On the interpretation of the surrogate function
For deterministic policies, the squared Wasserstein distance W(∏2(a∣s), ∏1(a∣s))2 degenerates to
the Euclidean distance over the action space. Any policy defines a sensitivity matrix at a given
state S, which is the Jacobian matrix of the policy output with respect to S. The policy sensitivity
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Truly Deterministic Policy Optimization (TDPO)
Require: The squared Wasserestein regularization coefficient C20 , The secondary regularization
coefficient ratio C10 /C20, and a trust region radius δmax.
Require: Initial policy π0 .
Require: Advantage estimator and sample collector oracle Aπ.
1:	for k = 1, 2, . . . do
2:	Collect trajectories and construct the advantage estimator oracle Aπk .
3:	Compute the policy gradient g at θk : g J Vθ0 Ank (∏0) ∣∏,=∏k
4:	Construct a surrogate Hessian vector product oracle V → H ∙ V such that for θ0 = θk + δθ,
C0	1
Es~ρμ% W(∏ (a∣s),∏k(a∣s))2 + C1 Es~0μ% Lg。(∏ ,∏k； S) =2δθ Hδθ + h.o.t., (14)
where h.o.t. denotes higher order terms in δθ.
5:	Find the optimal update direction δθ* = HTg using the Conjugate Gradient algorithm.
6:	Determine the best step size a* within the trust region:
C0
α* = arg max gτ(αδθ*)----2 (αδθ*)TH(αδθ*)
α2
s.t.	1(α*δθ*)TH(α*δθ*) ≤ δ21aχ	(15)
7:	Update the policy parameters: θk+1 J θk + α*δθ*.
8:	end for
term LG2 (π1, π2; S) is essentially the squared Euclidean distance over the action-to-observation
Jacobian matrix elements. In other words, our surrogate prefers to step in directions where the
action-to-observation sensitivity is preserved within updates.
Although our surrogate uses a metric distance instead of the traditional non-metric measures for regu-
larization, we do not consider this sole replacement a major contribution. The squared Wasserestein
distance and the KL divergence of two identically-scaled Gaussian distributions are the same up
to a constant (i.e., DKL(N(mι,σ)∣∣N(m2,σ)) = W(N(m1, σ),N(m2,σ))2∕2σ2). On the other
hand, our surrogate’s compatibility with deterministic policies makes it a valuable asset for our
policy gradient algorithm; both W(∏2(a∣s), ∏ι(a|s))2 and Lg。(∏1,∏2; S) can be evaluated for two
deterministic policies π1 and π2 numerically without resorting to any approximations to overcome
singularities.
4	Model-Free Estimation of Policy Gradient
The DeVine advantage estimator is formally defined in Algorithm 2. Unlike DDPG and TD3, the
DeVine estimator allows our method to perform deterministic search by not consistently injecting
noise in actions for exploration. Essentially, DeVine rolls out a trajectory and computes the values of
each state. Since the transition dynamics and the policy are deterministic, these values are exact. Then,
it picks a perturbation state St according to the visitation frequencies using importance sampling. A
state-reset to St is made, a σ-perturbed action is applied for a single time-step, followed by π1 policy.
This exactly produces Qπ1 (St, at + σ). Then, Aπ1 (St, at + σ) can be computed by subtracting the
value baseline. Finally, Aπ1 (St, at) = 0 and Aπ1 (St, at + σ) define a two-point linear Aπ1 (St, a)
model with respect to the action. Parallelization can be used to have as many states of the first roll-out
included in the estimator as desired. The parameter σ acts as an exploration parameter and a finite
difference to establish derivatives. While σ ' 0 can produce exact gradients, larger σ can build
stabler interpolations.
Under deterministic dynamics and policies, if the DeVine oracle samples each dimension at each
time-step exactly once then in the limit of small σ it can produce exact advantages, as stated in
Theorem 4.1, whose proof is deferred to the appendix.
Theorem 4.1. Assume a finite horizon MDP with both deterministic transition dynamics P and
initial distribution μ, with maximal horizon length of H. Define K = H ∙ dim(A), a uniform V, and
6
Under review as a conference paper at ICLR 2021
Algorithm 2 Deterministic Vine (DeVine) Policy Advantage Estimator
Require: The number of parallel workers K
Require: A policy π, an exploration policy q, discrete time-step distribution ν(t), initial state
distribution μ(s), and the discount factor γ.
1:	Sample an initial state s° from μ, and then roll out a trajectory T = (s0, a0, si, aι, ∙∙∙) using π.
2:	for k = 1,2,…，K do
3:	Sample the integer number t = tk from ν.
4:	Compute the value V π1 (st) = Pi∞=t γt-iR(si, ai).
5:	Reset the initial state to st, sample the first action a according to q(∙∣st), and use ∏ for the
rest of the trajectory. This will create T0 = (st, at, s；+、, at+i, ∙ ∙ ∙).
6:	Compute the value Qπ1 (st, a0t) = Pi∞=t γt-iR(s0i, a0i).
7:	Compute the advantage Aπ1 (st, a0t) = Qπ(st, a0t) - Vπ(st).
8:	end for
An /	、	1 K^ dim(A) ∙ Ytk
9:	Define A1 (π2):= K X	Vitk)
(n2 (S) - atk 产(%一atk )
(atk - atk )T(atk - atk )
• A"" ,atk).
10:	Return Aπ1 (∏2) and V∏2 Aπ1 (∏2) as unbiased estimators for Es〜Pn [Aπ1 (s, ∏2)] and PG, re-
spectively.
q(s; σ) = π1(s) + σej in Algorithm 2 with ej being the jth basis element for A. If the (j, ik) pairs
are sampled to exactly cover {1, • • • , dim(A)} × {1, • • • , H}, then we have
σli→m0Vπ2Aπ1(π2)π2=π1 = Vπ2 ηπ2 π2=π1
(16)
Theorem 4.1 provides a guarantee for recovering the exact policy gradient if the initial state distri-
bution was deterministic and all time-steps of the trajectory were used to branch vine trajectories.
Although this theorem sets the stage for computing a fully deterministic gradient, stochastic ap-
proximation can be used in Algorithm 2 by randomly sampling a small set of states for advantage
estimation. In other words, Theorem 4.1 would use νto deterministically sample all trajectory states,
whereas this is not a practical requirement for Algorithm 2 and the gradients are still unbiased if a
random set of vine branches is used.
The DeVine estimator can be advantageous in at least two scenarios. First, in the case of rewards that
cannot be decomposed into summations of immediate rewards. For example, overshoot penalizations
or frequency-based rewards as used in robotic systems are non-local. DeVine can be robust to
non-local rewards as it is insensitive to whether the rewards were applied immediately or after a long
period. Second, DeVine can be an appropriate choice for systems that are sensitive to the injection of
noise, such as high-bandwidth robots with natural resonant frequencies. In such cases, using white
(or colored) noise for exploration can excite these resonant frequencies and cause instability, making
learning difficult. DeVine avoids the need for constant noise injection.
5	Experiments
The next two subsections show challenging robotic control tasks including frequency-based non-local
rewards, long horizons, and sensitivity to resonant frequencies. See Section A.11 of the appendix
for a comparison on traditional gym environments. TDPO works similar or slightly worse on these
traditional gym environments as they seem to be well-suited for stochastic exploration.
5.1	An Environment with Non-Local Rewards 1
The first environment that we consider is a simple pendulum. The transition function is standard—the
states are joint angle and joint velocity, and the action is joint torque. The reward function is non-
standard—rather than define a local reward in the time domain with the goal of making the pendulum
1Non-local rewards are reward functions of the entire trajectory whose payoffs cannot be decomposed into
the sum of terms such as η = Pt ft (st, at), where functions ft only depend on nearby states and actions. An
example non-local reward is one that depends on the Fourier transform of the complete trajectory signal.
7
Under review as a conference paper at ICLR 2021
PPO
/∖∕∖∕∖Z^z^^
0	5	10
Time (s)
tD
占 0.4-
ω
≤ 0.2 -
E
< o.o 4
•- Desired Offset
Desired
Frequency
& Amplitude
i ff- Desired Offset
Desired
Γ∙<- Frequency
:I & Amplitude
Frequency (Hz)
0	5	10
Frequency (Hz)
*e- Desired Offset
Desired
Γ∙<- Frequency
:I & Amplitude
0	5	10
Frequency (Hz)
*e- Desired Offset
Desired
Γ∙<- Frequency
:I & Amplitude
0	5	10
Frequency (Hz)
Φe- Desired Offset
Desired
Frequency
& Amplitude
5	10
Frequency (Hz)
0	5
Figure 1: Results for the simple pendulum with non-local rewards. Upper panel: training curves
with empirical discounted payoffs. Lower panels: trajectories in both the time domain and frequency
domain, showing target values of oscillation frequency, amplitude, and offset.
stand upright (for example), we define a non-local reward in the frequency domain with the goal
of making the pendulum oscillate with a desired frequency and amplitude about a desired offset.
In particular, we compute this non-local reward by taking the Fourier transform of the joint angle
signal over the entire trajectory and by penalizing differences between the resulting power spectrum
and a desired power spectrum. We apply this non-local reward at the last time step of the trajectory.
Implementation details and similar results for more pendulum variants are left to the appendix.
Figure 1 shows training curves for TDPO (our method) as compared to TRPO, PPO, DDPG, and
TD3. These results were averaged over 25 experiments in which the desired oscillation frequency was
1.7 Hz (different from the pendulum’s natural frequency of 0.5 Hz), the desired oscillation amplitude
was 0.28 rad, and the desired offset was 0.52 rad. Figure 1 also shows trajectories obtained by the
best agents from each method. TDPO (our method) was able to learn high-reward behavior and to
achieve the desired frequency, amplitude, and offset. TRPO was able to learn the correct offset but
did not produce any oscillatory behavior. TD3 also learned the correct offset, but could not produce
desirable oscillations. PPO and DDPG failed to learn any desired behavior.
5.2 An Environment with Long Horizon and Resonant Frequencies2 * * * &
The second environment that we consider is a single leg from a quadruped robot (Park et al., 2017).
This leg has two joints, a “hip” and a “knee,” about which it is possible to exert torques. The hip is
attached to a slider that confines motion to a vertical line above flat ground. We assume the leg is
dropped from some height above the ground and the task is to recover from this drop and to stand
upright at rest after impact. States given to the agent are the angle and velocity of each joint (slider
position and velocity are hidden), and actions are the joint torques. The reward function penalizes
difference from an upright posture, slipping or chattering at the contact between the foot and the
2Resonant frequencies are a concept from control theory. In the frequency domain, signals of certain
frequencies are excited more than others when applied to a system. This is captured by the frequency-domain
transfer function of the system, which may have a peak of magnitude greater than one. The resonant frequency
is the frequency at which the frequency-domain transfer function has the highest amplitude. Common examples
of systems with a resonant frequency include the undamped pendulum, which oscillates at its natural frequency,
and RLC circuits which have characteristic frequencies at which they are most excitable. See Chapter 8 of Kuo
& Golnaraghi (2002) for more information.
8
Under review as a conference paper at ICLR 2021
JJoAed US5
0 0 5 0 5
-2-
' - - (E N) S3nbjo
(S∕paι ωω-⅛uo-ω> - -≤ 卜
0.2	0.3
Time (s)
0.2	0.3
Time (s)
0.2	0.3
Time (s)
0.2	0.3
Time (s)
0.2	0.3
Time (s)
Figure 2: Results for the leg environment with a long horizon and resonant frequencies due to ground
compliance. Upper panel: training curves with empirical discounted payoffs. Lower panel: partial
trajectories, restricted to times shortly before and after impact with the ground. Note the oscillations
at about 100 Hz that appear just after the impact at 0.2 s—these oscillations are evidence of a resonant
frequency.
ground, non-zero joint velocities, and steady-state joint torque deviations. We use MuJoCo for
simulation (Todorov et al., 2012), with high-fidelity models of ground compliance, motor nonlinearity,
and joint friction. The control loop rate is 4000 Hz and the rollout length is 2 s, resulting in a horizon
of 8000 steps. Implementation details are left to the appendix.
Figure 2 shows training curves for TDPO (our method) as compared to TRPO, PPO, DDPG and TD3.
These results were averaged over 75 experiments. A discount factor of γ = 0.99975 was chosen
for all methods, where (1 - γ)-1 is half the trajectory length. Similarly, the GAE factors for PPO
and TRPO were scaled up to 0.99875 and 0.9995, respectively, in proportion to the trajectory length.
Figure 2 also shows trajectories obtained by the best agents from each method. TDPO (our method)
was able to learn high-reward behavior. TRPO, PPO, DDPG, and TD3 were not.
We hypothesize that the reason for this difference in performance is that TDPO better handles the
combination of two challenges presented by the leg environment—an unusually long time horizon
(8000 steps) and the existence of a resonant frequency that results from compliance between the foot
and the ground (note the oscillations at a frequency of about 100 Hz that appear in the trajectories
after impact). Both high-speed control loops and resonance due to ground compliance are common
features of real-world legged robots to which TDPO seems to be more resilient.
6 Discussion
This article proposed a deterministic policy gradient method (TDPO: Truly Deterministic Policy Op-
timization) based on the use of a deterministic Vine (DeVine) gradient estimator and the Wasserstein
metric. We proved monotonic payoff guarantees for our method, and proposed a novel surrogate
for policy optimization. We showed numerical evidence for superior performance with non-local
rewards defined in the frequency domain and a realistic long-horizon resonant environment. This
method enables applications of policy gradient to customize frequency response characteristics of
agents. Future work should include the addition of a regularization coefficient adaptation process for
the C1 and C2 parameters in the TDPO algorithm.
9
Under review as a conference paper at ICLR 2021
References
Mohammed Amin Abdullah, Hang Ren, Haitham Bou Ammar, Vladimir Milenkovic, Rui Luo,
Mingtian Zhang, and Jun Wang. Wasserstein robust reinforcement learning. arXiv preprint
arXiv:1907.13196, 2019.
Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Kavosh Asadi, Dipendra Misra, and Michael L Littman. Lipschitz continuity in model-based
reinforcement learning. arXiv preprint arXiv:1804.07193, 2018.
Shalabh Bhatnagar, Doina Precup, David Silver, Richard S Sutton, Hamid R Maei, and Csaba
Szepesvari. Convergent temporal-difference learning with arbitrary smooth function approximation.
In Advances in neural information processing systems, pp. 1204-1212, 2009.
Christoph Dann and Emma Brunskill. Sample complexity of episodic fixed-horizon reinforcement
learning. In Advances in Neural Information Processing Systems, pp. 2818-2826, 2015.
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, Yuhuai Wu, and Peter Zhokhov. Openai baselines. https:
//github.com/openai/baselines, 2017.
Yan Duan, Xi Chen, Rein Houthooft, John Schulman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continuous control. In International Conference on Machine Learning,
pp. 1329-1338, 2016.
Norman Ferns, Prakash Panangaden, and Doina Precup. Metrics for markov decision processes with
infinite state spaces. arXiv preprint arXiv:1207.1386, 2012.
Scott Fujimoto, Herke Van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. arXiv preprint arXiv:1802.09477, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In Proceedings of the thirteenth international conference on artificial intelligence and
statistics, pp. 249-256, 2010.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.
Ashley Hill, Antonin Raffin, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto, Rene Traore,
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias Plappert, Alec Radford,
John Schulman, Szymon Sidor, and Yuhuai Wu. Stable baselines. https://github.com/
hill-a/stable-baselines, 2018.
Karl Hinderer. Lipschitz continuity of value functions in markovian decision processes. Mathematical
Methods of Operations Research, 62(1):3-22, 2005.
David R Hunter and Kenneth Lange. A tutorial on mm algorithms. The American Statistician, 58(1):
30-37, 2004.
Nan Jiang and Alekh Agarwal. Open problem: The dependence of sample complexity lower bounds
on planning horizon. In Conference On Learning Theory, pp. 3395-3398, 2018.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
ICML, volume 2, pp. 267-274, 2002.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Sham Machandranath Kakade et al. On the sample complexity of reinforcement learning. PhD thesis,
University of London London, England, 2003.
10
Under review as a conference paper at ICLR 2021
Nathan Kallus and Masatoshi Uehara. Efficiently breaking the curse of horizon in off-policy evaluation
with double reinforcement learning. arXiv, pp. arXiv-1909, 2019.
Nathan Kallus and Masatoshi Uehara. Statistically efficient off-policy policy gradients. arXiv preprint
arXiv:2002.04014, 2020.
Michael Kearns, Yishay Mansour, and Andrew Y Ng. A sparse sampling algorithm for near-optimal
planning in large markov decision processes. Machine learning, 49(2-3):193-208, 2002.
Michael J Kearns, Yishay Mansour, and Andrew Y Ng. Approximate planning in large pomdps
via reusable trajectories. In Advances in Neural Information Processing Systems, pp. 1001-1007,
2000.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Benjamin C. Kuo and Farid Golnaraghi. Automatic Control Systems. John Wiley & Sons, Inc., USA,
8th edition, 2002. ISBN 0471134767.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-
horizon off-policy estimation. In Advances in Neural Information Processing Systems, pp. 5356-
5366, 2018.
Leonard Meirovitch. Elements of vibration analysis. McGraw-Hill Science, Engineering & Mathe-
matics, 1975.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Aldo Pacchiano, Jack Parker-Holder, Yunhao Tang, Anna Choromanska, Krzysztof Choromanski,
and Michael Jordan. Learning to score behaviors for guided policy optimization. arXiv preprint
arXiv:1906.04349, 2019.
Hae-Won Park, Patrick M Wensing, and Sangbae Kim. High-speed bounding with the mit cheetah 2:
Control design and experiments. The International Journal of Robotics Research, 36(2):167-192,
2017.
Matteo Pirotta, Marcello Restelli, and Luca Bascetta. Policy gradient in lipschitz markov decision
processes. Machine Learning, 100(2-3):255-283, 2015.
Andre Preumont and KazUto Seto. Active control of structures. John Wiley & Sons, 2008.
Emmanuel Rachelson and Michail G. Lagoudakis. On the locality of action domination in sequential
decision making. In 11th International Symposium on Artificial Intelligence and Mathematics
(ISIAM 2010), pp. 1-8, Fort Lauderdale, US, 2010. URL https://oatao.univ-toulouse.
fr/17977/.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International conference on machine learning, pp. 1889-1897, 2015a.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438,
2015b.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller.
Deterministic policy gradient algorithms. In ICML, 2014.
11
Under review as a conference paper at ICLR 2021
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012IEEE/RSJ International Conference on Intelligent Robots and Systems,pp. 5026-5033.
IEEE, 2012.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media,
2008.
Ruosong Wang, Simon S Du, Lin F Yang, and Sham M Kakade. Is long horizon reinforcement learning
more difficult than short horizon reinforcement learning? arXiv preprint arXiv:2005.00527, 2020.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine learning, 8(3-4):229-256, 1992.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing systems, pp. 5279-5288, 2017.
12
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Tables of Notation
The same mathematical definitions and notations used in the paper were re-introduced and summarized
in two tables; Table 1 describes the mathematical functions and operators used throughout the paper,
and Table 2 describes the notations needed to define the Markov Decision Process (MDP). The tables
consist of two columns; one showing or defining the notation, and the other includes the name in
which the same notation was called in the paper.
Name	Mathematical Definition or Description
Value function	Vπ(S) := i--Est〜ρμ [R(St,at)] Y at 〜π(st) =E[P∞=ι Yt-1R(st,at)∣sι = s, at 〜∏(st),st+ι 〜P(st,at)].
Q-Value function	Qn(S, a)：= R(S, a)+ Y ∙ Eso〜P(s,a)[Vπ(SO)]	
Advantage function	Aπ(s, a) := Qπ(s, a) — Vπ(s).
Advantage function	Aπ(S,π0):= Ea50(s)[Aπ(S,α)].	
Arbitrary functions	f and g are arbitrary functions used next.
Arbitrary distributions	ν and ζ are arbitrary probability distributions used next.
Hilbert inner product	hf,giχ ：= ∕f(x)g(x)dx	
Kulback-Liebler (KL) divergence	DKL(Z∣ν) ：= hζ(x),log(黑)ix = R Z(x)log(船)dx
Total Variation (TV) distance	TV(Z, V) := 2h∣Z(x) — ν(x)∣, 1ix = 2 R |Z(x) — V(x)∣dx.
Coupling set	Γ(Z, ν) is the set of couplings for Z and ν.
Wasserstein distance	W (z,v) = inf γ∈r(ζ,ν)hkx — yk,γ(χ,y)ix,y.	
Policy Wasserstein dis- tance	W(∏1,∏2) ：= sups∈s W(∏1(∙∣s),∏2(∙∣s)).
Lipschitz Constant	Lipf(X,y); χ) ：= SUpx kVχf(χ,y)l∣2.	
Rubinstein-Kantrovich (RK) duality	KZ(x) — V(x),f(x)ix∣ ≤ W(ζ,ν) ∙ Lip(f; x).
Table 1: The mathematical notations used throughout the paper.
13
Under review as a conference paper at ICLR 2021
Mathematical Notation Name and Description
S	This is the state space of the MDP.
A	This is the action space of the MDP.
	Y		This is the discount factor of the MDP.
R : S×A→R	This is the reward function of the MDP.
	μ		This is the initial state distribution of the MDP over the state space.
∆	∆(F) is the set of all probability distributions over the arbitrary set F (otherwise known as the Credal set of F).
π	In general, π denotes the policy of the MDP. However, the output argument type could vary in the text. See the next lines.
∏ : S → ∆(A)	Given a state S ∈ S, ∏(s) and ∏(∙∣s) denote the action distribution suggested by the policy π. In other words, a 〜π(s) and a 〜π(∙∣s).
∏det : S → A	For a deterministic policy πdet, the unique action a suggested by the policy given the state s can be denoted by π(s) specially. In other words, a = πdet(s).
Π	Π is the set of all policies (i.e., ∀π : π ∈ Π).
P	In general, P denotes the transition dynamics model of the MDP. However, the input argument types could vary throughout the text. See the next lines for more clarification.
P : S×A→ ∆(S)	Given a particular state s and action a, P(s, a) will be the next state distribution of the transition dynamics (i.e. s0 〜P(s, a) where s0 denotes the next state after applying s, a to the transition P).
P : ∆(S) ×A→ ∆(S)	This is a generalization of the transition dynamics to accept state distributions as input. In other words, P", a) := Es 〜νs [P (s, a)].
P : SX ∆(A) → ∆(S)	This is a generalization of the transition dynamics to accept action distributions as input. In other words, P(s, Va):= Ea〜νa [P(s, a)].
P : ∆(S) × Π→ ∆(S)	This is a generalization of the transition dynamics to accept a state distribution and a policy as input. Given an arbitrary state distribution νs and a policy π, and P(νs , π) will be the next state distribution given that the state is sampled from νs and the action is sampled from the π(s) distribution. In other words, we have P(νs, ∏) := E s~νs [P (s, a)]. a 〜π(s)
Pt : ∆(S) × Π → ∆(S)	This is the t-step transition dynamics generalization. Given an arbitrary state distribution νs and a policy π and non-negative in- teger t, one can define Pt recursively as P0(νs, π) := νs and Pt(νs,π) := P(PtT(Vs,π),π).
ρμ	The discounted visitation frequency pμ is a distribution over S, and can be defined as pμ := (1 一 Y) P∞=0 γtPt(μ, ∏).
Table 2: The MDP notations used throughout the paper.
14
Under review as a conference paper at ICLR 2021
A.2 BOUNDING W(Pt(μ, ∏ι), Pt(μ, ∏2))
To review, the dynamical smoothness assumptions were
W(P(μ,∏1),P(μ,∏2)) ≤ L∏ ∙ W(∏1,∏2),
W(P(μ1,∏),P(μ2,∏)) ≤ Lμ ∙ W(μ1,μ2).
The following lemma states that these two assumptions are equivalent to a more concise assumption.
This will be used to bound the t-step visitation distance and prove Lemma A.2.
Lemma A.1. Assumptions (5) and (6) are equivalent to having
W(P(μ1,∏1), P(μ2,∏2)) ≤ Lμ ∙ W(μ1,μ2) + Ln ∙ W(∏1,∏2).	(17)
Proof. To prove the (5), (6) ⇒ (17) direction, the triangle inequality for the Wasserstein distance
gives
W(P(μ1,∏1), P(μ2,∏2)) ≤ W(P(μ1,∏1),P(μ2,∏1)) + W(P(μ2,∏1), P(μ2,∏2))	(18)
and using (5), (6), and (18) then implies
W(P(μ1,∏1), P(μ2,∏2)) ≤ Lμ ∙ W(μ1,μ2) + Ln ∙ W(∏1,∏2).	(19)
The other direction is trivial.	□
Lemma A.2. Under Assumptions (5) and (6) we have the bound
W (Pt (μ,π1), Pt(μ, π2)) ≤ Ln ∙ (1 + Lμ + …+ LjI) ∙ W (π1, π2),	QO)
where Pt(μ, π) denotes the state distribution after running the MDPfor t time-steps with the initial
state distribution μ and policy π.
Proof. For t = 1, the lemma is equivalent to Assumption (5). This paves the way for the lemma to
be proved using induction. The hypothesis is
W(PtT(μ, ∏ι), Pt-1(μ, ∏2)) ≤ Ln ∙ (1 + Lμ + …+ LtU ∙ W(∏1,∏2),	(21)
and for the induction step we write
W (Pt(μ,∏ι),Pt (μ,∏2)) = W (P(PtT(μ,∏1),∏1),P(Pt-1(μ,∏2),∏2)).	(22)
Using Assumption (17), which according to Lemma A.1 is equivalent to Assumptions (5) and (6), we
can combine (21) and (22) into
W(Pt(μ, ∏ι), Pt(μ, ∏2)) ≤ Ln ∙ W(∏1, ∏2) + Lμ ∙ W(Pt-1(μ, ∏ι), Pt-1(μ, ∏2)).	(23)
Thus, by applying the induction Hypothesis (21), we have
W(Pt(μ,∏ι), Pt(μ, ∏2)) ≤ Ln ∙ W(∏1, ∏2) + Lμ ∙ Ln ∙ (1 + L“ + …+ L「) ∙ W(∏1,∏2), (24)
which can be simplified into the lemma statement (i.e., Inequality (20)).	□
A.3 Bounding W(PT ,阵2)
Lemma A.2 suggests making the γLμ < 1 assumption and paves the way for Theorem A.4. The
γLμ < 1 assumption is overly restrictive and unnecessary, but makes the rest of the proof easier to
follow. This assumption can be relaxed by a general transition dynamics stability assumption which
is discussed in more detail later at section A.5.3, and an equivalent YLμ < 1 assumption is introduced
to replace γL* < 1.
First, we need to introduce Lemma A.3 first, which will be used in the proof of Theorem A.4.
Lemma A.3. The Wasserstein distance between linear combinations of distributions can be bounded
as W(β ∙ μι + (1 - β) ∙ νι,β ∙ μ2 + (1 - β) ∙ V2) ≤ β ∙ W(μ1,μ2) + (1 - β) ∙ W(ν1,ν2).
Proof. Plugging Y = β ∙ Y(μ1,μ2) + (1 - β)∙ γ3 ,ν2) in the Wasserstein definition yields the result. □
15
Under review as a conference paper at ICLR 2021
Theorem A.4. Assuming (5), (6), and γL* < 1, we have the inequality
W隔柿 ) ≤ E ∙ W(π1,π2).	(25)
Proof. Using Lemma A.3 and the definition of pμ, We can write
∞
W(ρ∏1 ,ρ∏2 ) ≤ (1-γ) X γt∙ W(Pt(μ,∏1), Pt(μ,∏2)).	(26)
t=0
Using Lemma A.2, we can take another step to relax the inequality (26) and write
W(ρμι ,ρμ2) ≤ Ln(I -LY)W1∏1,π2) X((γLμ)t - γt).	(27)
(μ )	t=0
Due to the γLμ < 1 assumption, the right-hand summation in (27) is convergent. This leads Us to
W (ρμι ,ρ∏2) ≤
Ln(1 - Y)W(∏1,∏2)(	1)
(Lμ - 1)	1 - YLμ	1 - Y
Inequality (28) can then be simplified to give the result.
(28)
□
A.4 Steps to Bound the Second-order Term
The RK duality yields the following bound:
lhρμ2-ρμ1 ,Aπ1 (∙,∏2)isl≤ W (ρμι ,ρ∏2) ∙ sup kVsAπ1 (s,∏2)k2.	(29)
s
To facilitate the further application of the RK duality, any advantage can be rewritten as the following
inner product: Aπ1 (s,∏2) = h∏2(a∣s) - ∏ι (a|s), Qπ1 (s, a)ia∙ Taking derivatives of both sides with
respect to the state variable and applying the triangle inequality produces the bound
SUp ∣∣VsAπ1 (s,∏2)k2 ≤ SUp khVs(∏2(a∣s) - ∏1(a∣s)),Qπ1(s,a)iJ2
ss
+ SUp kh∏2(a∣s) - ∏ι(a∣s), VsQπ1 (s, a)ia∣2.	(30)
s
The second term of the RHS in (30) is compatible with the RK duality. However, the form of the first
term does not warrant an easy application of RK. For this, we introduce Theorem A.5.
Theorem A.5. Assuming the existence of Lip(Qπ1 (s, a); a), we have the bound
〈Vs(n2(a|s) - ni(a|s)),Qni (s, a)〉a
2
≤ 2 ∙ Lip(Qπ1 (s,a); a) ∙ VsoW(π2(als0) + π1(als)
(31)
∏2(a∣s) + ∏ι(a∣bs0)ʌ I
2	儿=s 2.
Proof. By definition, we have
IKVs(n2(a|s) -πl(als)),Qnι(s,a)>J2
∂S(j) (n2(a|s) - n1(a|s)),Qni(s,a)Ej .	(32)
For better insight, we will write the derivative using finite differences:
D ∂jj (n2(a|s)-n1(a|s)), Qn1(s, a))”
=lim ɪ ((∏2(a∣s +	δs ∙ e1)	-∏ι(a∣s + δs	∙ e7-)	),Qπ1	(s,a))
δs→0 δs	a
-<(π2(a∣s)	-πι(a∣s)	), Qπ1(s, a)〉a	.	(33)
16
Under review as a conference paper at ICLR 2021
We can rearrange the finite difference terms to get
D ∂S"(j) (n2⑷S)-π1⑷S)), QnI(S, a)E”
= Jim0 δs <(∏2(a∣s + δs ∙ ej)	+∏ι(a∣s)
), Qπ1 (S, a) a
+∏ι(a∣s + δs ∙ ej)	),Qπ1 (s,a))@ .	(34)
Equivalently, we can divide and multiply the inner products by a factor of 2, to make the inner product
arguments resemble mixture distributions:
(∂S(j)(π2 (IaIS) - n1(a|s)),Qn1 (s,a)}a
=lim2 [Dn2(a|S + δs ∙ ej) + n1(a|s) ,Q∏ι(s,a)E
δs→0 δS	2	a
-D n2(a|S)+ π1 ⑷S + δs ∙ ej),Q∏ι (S,a)E ].	(35)
2a
The RK duality can now be used to bound this difference as
D ∂S(j) (n2(a|s) - π1(WS)),Qπ1(S,a)Ea
(36)
≤ lim -2
δs→0 δ S
jW (n2(a|s + δs ∙ ej) + n1(a|s)
1	2
n2(a|s) + π1(alS + δs ∙ ej)) ∙ Lip(Q∏ι (s,a); a)
which can be simplified as
∂
∖∂Sj) (π2(a|S)-
π1 (a|s)), Qπ1 (s,
a)Ea
≤ 2 ∙ Lip(Q∏ι (S, a); a) ∙ &W Π"" +	S)
∂ s	2
∏2(a∣s) + ∏ι(a∣s0)
2
(37)
Combining Inequality (37) and Equation (32), We obtain the bound in the theorem.	□
A.5 The Lipschitz Continuity and Transition Stability Assumptions
There are three key groups of assumptions made in the derivation of our policy improvement loWer
bound. First is the existence of Qπ -function Lipschitz constants. Second is the transition dynamics
Lipschitz-continuity assumptions. Finally, We make an assumption about the stability of the transition
dynamics. Next, We Will discuss the meaning and the necessity of these assumptions in the same
order.
A.5.1 ON THE EXISTENCE OF THE LIP(Qπ, a) CONSTANT
The Lip(Qπ, a) constant may be undefined When either the reWard function or the transition dynamics
are discontinuous. Examples of knoWn environments With undefined Lip(Qπ, a) constants include
those With grazing contacts Which define a discontinuous transition dynamics. In practice, even
for environments that do not satisfy Lipschitz continuity assumptions, there are mitigating factors;
practical Qπ functions are reasonably narroW-bounded in a small trust region neighborhood, and
since We use non-vanishing exploration scales and trust regions, a bounded interpolation slope can
still model the Q-function variation effectively. We should also note that a slightly stronger version of
this assumption is frequently used in the context of Lipschitz MDPs (Pirotta et al., 2015; Rachelson
& Lagoudakis, 2010; Asadi et al., 2018). In practice, We have not found this to be a substantial
limitation.
A.5.2 The Transition Dynamics Lipschitz Continuity Assumption
Assumptions 5 and 6 of the main paper essentially represent the Lipschitz continuity assumptions of
the transition dynamics With respect to actions and states, respectively. If the transition dynamics
17
Under review as a conference paper at ICLR 2021
and the policy are deterministic, then these assumptions are exactly equivalent to the Lipschitz
continuity assumptions. Assumptions 5 and 6 only generalize the Lipschitz continuity assumptions in
a distributional sense.
The necessity of these assumptions is a consequence of using metric measures for bounding errors.
Traditional non-metric bounds force the use of full-support stochastic policies where all actions have
non-zero probabilities (e.g., for the KL-divergence of two policies to be defined, TRPO needs to
operate on full-support policies such as the Gaussian policies). In those analyses, since all policies
share the same support, the next state distribution automatically becomes smooth and Lipschitz
continuous with respect to the policy measure even if the transition dynamics was not originally
smooth with respect to its input actions. However, metric measures are also defined for policies of
non-overlapping support. To be able to provide closeness bounds for future state visitations of two
similar policies with non-overlapping support, it becomes necessary to assume that close-enough
actions or states must be yielding close-enough next states. In fact, this is a very common assumption
in the framework of Lipschitz MDPs (See Section 2.2 of Rachelson & Lagoudakis (2010), Section 3
of Asadi et al. (2018), and Assumption 1 of Pirotta et al. (2015)).
A.5.3 The Transition Dynamics Stability Assumption
Before moving to relax the γLμ < 1 assumption, We will make a few definitions and restate
the previous lemmas and theorems under these definitions. We define Lμ1,μ2,∏ to be the infi-
mum non-negative value that makes the equation W(P(μι,∏), P(μ2,∏)) = Lμ1,μ2,∏W(μ1,μ2)
hold. Similarly, Lμ1,μ2,∏ is defined as the infimum non-negative value that makes the equation
W(P(μ,∏ι), P(μ,π)) = Lμ,∏1,∏2 W(∏1(∙∣μ),∏2(∙∣μ)) hold. For notation brevity, we will also de-
note LPt (μ,∏ι ),Pt (μ,∏2 ),∏2 and LPt (μ,∏1),∏1,∏2 by Lμ,∏ι,∏2 and Lμ,∏ι,∏2 , respectively.
Under these definitions, Lemma A.1 evolves into
W(P(μι, ∏ι), P(μ2,∏2)) ≤ Lμ1,μ2,∏ W(μι, μ2) + Lμι,∏ι∏ W(∏ι, ∏2).	(38)
We can apply a time-point recursion to this lemma and have
W(P(Pt(μ,∏1),∏1), P(Pt(μ,∏2),∏2))
≤ LPt(μ,∏ι ),∏1,∏2W (∏1,∏2) + Lpt(μ,∏ι ),pt(μ,∏2),∏2 W (Pt (μ, ∏ι), Pt (μ, ∏2))	(39)
, which can be notationally simplified to
W(Pt(μ,∏l), Pt(μ,∏2)) ≤ Lμt,∏1)∏2 W(∏1,∏2)+ Lμt-1)∏2 W(Pt-l(μ,∏1),PtT(μ,∏2)).	(40)
These modifications lead Lemma A.2 to be updated accordingly into
W(Pt(μ,∏l), Pt(μ,∏2)) ≤ CK小皿∙ W(∏1,∏2)
, where we have
t	k-1
Cl∏1,∏2 := X Lμt-k,)∏2 Y Lμ-i,∏2.
k=1	i=1
By a simple change of variables, we can have the equivalent definition of
t
t-1
π1 ,π2
：=∑Lμk-11,∏2 ∏ Lμ,∏1,∏2.
k=1	i=k+1
(41)
(42)
(43)
Now, we would replace the γLμ < 1 assumption with the following assumption.
The Transition Dynamics Stability Assumption: A transition dynamics P is called stable if and
only if the induced {L *∏1,∏2}t≥0 and {L Mn 1,∏2}t≥0 sequences satisfy
CL :=	SUp F(Sμ,∏ι,∏2
μ,∏ι ,∏2,t
t-1
SUp ELμk-11,∏2 ∏ L几,∏2
μ,π1 ,π2,tk = 1	i=k + 1
< ∞.
(44)
t
To help understand which {Lμ,∏1,∏2 }t≥0 and {Lμ,πι,∏2 }t≥0 sequences can satisfy this assumption,
we will provide some examples:
18
Under review as a conference paper at ICLR 2021
•	The ∀t : Lμ,∏1,∏2 = ci > 1 and ∀t : L(t∏ι,∏2 = c2 sequences violate the dynamics stability
assumption.
•	The ∀t : Lf∏ι,∏2 ≤ 1 and ∀t : Lf∏ι,∏2 = O(表)sequences satisfy the dynamics stability
assumption.
•	SuPt Lμ,∏1,∏2 < 1 guarantees the dynamics stability assumption.
•	∀t ≥ to : Lf∏ι,∏2 < 1 guarantees the dynamics stability assumption no matter (1) how big
to is (as long as it is finite), or (2) how big the members of the finite set {Lf∏ι,∏2 |t < to}
are.
If the dynamics stability assumption holds with a constant Cl, one can define a Lμ constant such
that CL = Ln P∞=o(γLμ)t. Then, we can replace all the Lμ instances in the rest of the proof with
the corresponding L* constant, and the results will remain the same without any change of format.
The L(t,∏ι,∏2 and Lμt,∏ι,∏2 constants can be thought as tighter versions of Lμ and L∏, but with
dependency on ∏ι, ∏2, μ and the time-point of application. Having γLμ < 1 is a sufficient yet
unnecessary condition for this dynamics stability assumption to hold. Vaguely speaking, Lμ is an
expansion rate for the state distribution distance; it tells you how much a divergence in the state
distribution will expand after a single application of the transition dynamics. Having effective
expansion rates that are larger than one throughout an infinite horizon trajectory is a sign of the
system instability; some change in the initial state’s distribution could cause the observations to
diverge exponentially. While controlling unstable systems is an important and practical challenge,
none of the existing reinforcement learning methods is capable of learning effective policies on
such environments. Roughly speaking, having the dynamics stability assumption guarantees that the
expansion rates cannot be consistently larger than one for infinite time steps.
A.6 CHOICE OF C1 AND C2
Since the TDPO algorithm operates using the metric Wasserstein distance, thinking about how
normalizing actions and rewards affect the corresponding optimization objective builds insight into
how to set these coefficients properly. Say we use the same dynamics, only the new actions are scaled
up by a factor of β and the rewards are scaled up by a factor of α:
anew = β ∙ aold rnew = α ∙ rold∙	(45)
If the policy function approximation class remained the same, the policy gradient would be scaled
by a factor of β∣ (i.e., danew = α∣ ∙ Iaold). Therefore, one can easily show that the corresponding new
regularization coefficient and trust region sizes can be obtained by
Cnew = O- ∙ Cold	(46)
β2
and
snɪ=β ∙ δomax.	(47)
We used equal regularization coefficients (i.e., C1 = C- = C), and the process to choose them can
be summarized as follows: (1) Define C = 3600 ∙ α ∙ β-2, δmaχ = β∕6θ0 and σq = β∕60 (where
σq is the action disturbance parameter used for DeVine), (2) using prior knowledge or by trial and
error determine appropriate action and reward normalization coefficients. The reward normalization
coefficient α was sought to be approximately the average per-step discounted reward difference of a
null policy and an optimal policy. We used a reward scaling value of α = 5 and an action scaling
value of β = 5 for the non-locally rewarded pendulum and β = 1.5 for the long-horizon legged
robot. Both environments had a per-step discounted reward of approximately -5 for a null policy
and non-positive rewards, justifying the choice of α.
A.7 Proof of Theorem 4.1
We restate Theorem 4.1 below for reference and now prove it.
19
Under review as a conference paper at ICLR 2021
Theorem 4.1. Assume a finite horizon MDP with both deterministic transition dynamics P and
initial distribution μ, with maximal horizon length of H. Define K = H ∙ dim(A), a uniform V, and
q(s; σ) = π1(s) + σej in Algorithm 2 with ej being the jth basis element for A. If the (j, tk) pairs
are sampled to exactly cover {1, . . . , dim(A)} × {1, . . . , H}, then we have
σi→0 Vn2 Aπ1 (π2)l∏2=∏ι = Vn2 ηπ2 l∏2=∏ι
(48)
Proof. According to the advantage decomposition lemma, we have
V.2 η∏2k=∏1 =占 Esf [V∏2 Aπ1 (S,∏2 )]l∏2=∏ι .	(49)
Due to the fact that the transition dynamics, policies π1 and π2, and initial state distribution are all
deterministic, we can simplify Equation (49) to
H-1
v∏2 η∏2 l∏2=∏ι = X Yt∙v∏2 AnI(St,0l∏2=∏ι,	(5O)
t=0
where St is the state after applying the policy π1 for t time-steps. We can use the chain rule to write
Vπ2Aπ1(St,π2)llπ2=π1 =Vπ2Aπ1(St,at)llat=π2(st)
2 1	π2=π1
dim(A)	∂
= X v∏2 atj) ∙	(j) AnI(St,at"at=∏2(st).	(51)
j=1	∂at	π2=π1
To recap, Equations (50), (50), and (51) can be summarized as
H-1	dim(A)	∂
V∏2ηn2ln2=n1 = X Yt X V∏2aj) ∙ EAnI(St,at)lɑt=∏2(st).	(52)
2	1	t=0	j=1	∂at	n2=n1
Under the assumption that the (j, t) pairs are sampled to exactly cover {1, . . . , dim(A)}×{1, . . . , H},
we can simplify the DeVine oracle to
H-1 dim(A)
An1(∏2) = K X X
t=0 j=1
dim(A) • Yt
一	V⑴
(π2 (St) 一 π1(Sty)T(q(st； j,σ 一 π1(St))
(q(st； j,σ) 一 πι(StXT (q(St； j,σ) — ni(St X
• An1 (St, q(St; j, σ)) .	(53)
From the q definition, we have q(St;j, σ) -π1(St) = σej and (q(St; j, σ) -π1(St))T(q(St; j, σ) -
π1(St)) = σ2. Since V is uniform (i.e., ν(t) = H) and K = H ∙ dim(A), We Can take the policy
gradient of Equation (53) and simplify it into
H-1 dim(A)
Vn2An1(π2)lln2=n1=X X
t=0 j=1
Yt∙Vn2(∏2(St)-∏1(St))Tej∙ AnI(St,，t；j,σ))
Since, An1 (St, π1(St)) = 0, We can Write
lim AKl(St,q(St； j,σ))=① AnI(St,∏ι(St) + σej) — AnI(St,∏ι(St))
σ→0	σ	σ→0	σ
∂
=∂0FA I(St,at)lat =ni(st).
Also, by the definition of the gradient, We can Write
Vn2 (π2(St) — π1(St))Tej = Vn2at(j).
Combining Equations (55) and (56), and applying them to Equation (54), yields
H-1 dim(A)	∂
Iim v∏2 Ani (π2)ln2=n1 = X X Yt ∙ v∏2 atj) ∙ -7") AnI(St,at)lat=n2(st).
σ→0	2 l	t=0 j=1	∂at	n2=nl
(54)
(55)
(56)
(57)
Finally, the theorem can be obtained by comparing Equations (57) and (52).
□
20
Under review as a conference paper at ICLR 2021
A.8 Quadratic Modeling of Policy Sensitivity Regularization
First, we will build insight into the nature of the
Lwg(∏i,∏2; S) =W(∏2(a∣s),∏1(a∣s))×
term. It is fairly obvious that
▽ s，W
∏2(a∣s0) + ∏ι(a∣s)
∏2(a∣s) + ∏ι(a∣s0) M
2	儿=s 2
W (n2(a|s),ni(a|s))|n2=ni = 0.
(58)
(59)
2
If π2 = π1, then the two distributions "式"®)+"1'"1" and "乂"®+"1("®) will be the same no matter
what s0 is. In other words,
∏ι = ∏2 ⇒ ∀s0 ： w(π⅛2+∏处)
∏2(a∣s) + ∏ι(a∣s0) λ = o
(60)
2
This means that
▽s0W
∏2 (a∣s0) + ∏ι(a∣s)
∏2(a∣s) + ∏ι(a∣s0)) । U
0.
(61)
π2=π1
2
The Taylor expansion of the squared Wasserestein distance can be written as
W (∏2(a∣s),∏1(a∣s))2∣θ2=θι +δθ = 2 δθT H2δθ + h.o.t..	(62)
Considering (60) and similar to the previous point, one can write the following Taylor expansion
▽s，W(W) + ni(a|S),π2(als) + πi(als0))|	U2l	= δθTHιδθ + h.o.t.. (63)
∖	2	2	l ls0=su2lθ2=θ1+δθ
According to above, LWG is the geometric mean of two functions of quadratic order. Although this
makes LWG of quadratic order (i.e., limδθ→0 LWWG(濡)= α2 holds for any constant α), this does
not guarantee that LWG is twice continuously differentiable w.r.t. the policy parameters, and may
not have a defined Hessian matrix (e.g., f(x1, x2) = |x1x2 | is of quadratic order, yet is not twice
differentiable). To avoid this issue, we compromise on the local model. Using the AM-GM inequality
and for any arbitrary positive α, one can bound the LW G term into two quadratic terms:
LwG(π1,π2;S) ≤ 2 (⅛W (n2(a|s),n1(a|s))2+
α2 ▽ W(n2(a|s0)+ ni(a|s)
∏2 (a|s) + ∏1(a∣s0)
2
(64)
Therefore, by defining
LG (∏1,∏2; S)= ▽ W (W) + —),π2(als)+ πi(als0) )|	2,	(65)
U	2	2	ls0=sU2
C1 := C10~, and C2 := (C2 + 枭)the new surrogate will have the twice-differentiable form
L∏ι (π2) =1-^ ∙ Es〜ρμι [Aπ1 (s, π2)]
一 CI ∙ Es〜ρμι LG2 (π1,π2; S)
—C2 ∙ Es〜ρμι W(n2(a|s),ni(a|S))2 .
(66)
C10 and C20 will be the corresponding regularization coefficients for the surrogate defined in (13). Due
to the arbitrary α used in bounding, no constrain governs the C10 and C20 coefficients. Therefore, C10
and C20 can be chosen without constraining each other.
21
Under review as a conference paper at ICLR 2021
A.9 Implementation Details for the Environment with Non-local Rewards
We used the stable-baselines implementation (Hill et al., 2018), which has the same structure as the
original OpenAI baselines (Dhariwal et al., 2017) implementation. We used the “ppo1” variant since
no hardware acceleration was necessary for automatic differentiation and MPI parallelization was
practically efficient. TDPO, TRPO, and PPO used the same function approximation architecture with
two hidden layers, 64 units in each layer, and the tanh activation. TRPO, PPO, DDPG, and TD3 used
their default hyper-parameter settings. We used Xavier initialization (Glorot & Bengio, 2010) for
TDPO, and multiplied the outputs of the network by a factor of 0.001 so that the initial actions were
small and nearly zero. We also confirmed that TDPO does not decrease in performance when using
an identical network initialization to PPO/TRPO. TD3’s baseline implementation was amended to
support MPI parallelization just like TRPO, PPO, and DDPG. To produce the results for DDPG and
TD3, we used hyperparameter optimization both with and without the tanh final activation function
that is common for DDPG and TD3 (this causes the difference in initial payoff in the figures). However,
under no conditions were DDPG and TD3 able to solve these environments effectively, suggesting
that the deterministic search used by TDPO is operating in a qualitatively different way than the
stochastic policy optimization used by DDPG and TD3. Note that we made a thorough attempt
to compare DDPG and TD3 fairly, including trying different initializations, different final layer
scalings/activations, different network architectures, and performing hyperparameter optimization.
Mini-batch selection was unnecessary for TDPO since optimization for samples generated by DeVine
was fully tractable. The confidence intervals in all figures were generated using 1000 samples of the
statistics of interest.
For designing the environment, we used Dhariwal et al. (2017)’s pendulum dynamics and relaxed the
torque thresholds to be as large as 40 N m. The environment also had the same episode length of 200
time-steps. We used the reward function described by the following equations:
R(St,at) = Cr ∙ R(T) ∙ 1{t = 200}
R(τ) = RFreq(τ) + ROffset (τ) + RAmp(τ)
fmax
RFreq(T)=0.1 ∙	X θ+d(f)2 - 1
f =fmin
ROffset (T )
θ(f = 0) θ
200	- θTarget Offset
1	199
200 ∑ θt - θTarget Offset
—
RAmp (T ) = hpie
Cewise I "λ	- 1
θTarget Amp.
(67)
where
•	θ is the pendulum angle signal in the time domain.
•	Θ is the magnitude of the Fourier transform of θ.
•	Θ+ is the same as Θ only for the positive frequency components.
•	ΘAC is the normalized oscillatory spectrum of Θ:
C	Pθ+Tθ +
θac = -200-.
•	hpiecewise is a piece-wise linear error penalization function:
hpiecewise(x) = -X ∙ 1{x ≥ 0} + 10-4x ∙ 1{-x ≥ 0}.
(68)
(69)
•	θs+td is the standardized positive amplitudes vector:
θ+
θstd
θ+
θ Θ+T θ+ + 10-6
(70)
22
Under review as a conference paper at ICLR 2021
•	CR = 1.3 × 104 is a reward normalization coefficient, and was chosen to yield approximately
the same payoff as a null policy would yield in the typical pendulum environment of Dhariwal
et al. (2017).
•	θTarget Offset is the target offset, θTarget Amp. is the target amplitude, and [fmin, fmax] is the
target frequency range of the environment.
All methods used 48 parallel workers. The machines used Xeon E5-2690-v3 processors and 256 GB
of memory. Each experiment was repeated 25 times for each method, and each run was given 6 hours
or 500 million samples to finish.
A.10 Implementation Details for the Environment with Long Horizon and
Resonant Frequencies
For the robotic leg, we used exactly the same algorithms with the same parameters as described in
Section A.9 above.
We used the reward function described by the following equations:
with
R = Rposture + Rvelocity + Rfoot offset + Rfoot height + Rground force + Rknee height + Ron-air torques (71)
Rposture =	-1 ×	θknee + 2	+	θhip + 4	
Rvelocity =	二-0.08 X ∣jωkneel + Zhip。				
Rfoot offset =	-10×	Ijxfoot| ∙ 1{zknee < 0.2}			
Rground force =	-1 ×	fz - mg I	1{fz < mg}		• ItoUchdown]
Rfoot height =	二-1 X [ ∣zfoot I ∙ ItoUchdown]				
Rknee height =	-15 ×	[I Zknee - Zknee | ∙ ItoUchdown]			
Ron-air torques =	-10-4	X [(Tlknee + Thip)，(I- ItoUchdown)]			
(72)
where
•	θknee and θhip are the knee and hip angles in radians, respectively.
•	ωknee and ωhip are the knee and hip angular velocities in radians per second, respectively.
•	xfoot and zfoot are the horizontal and vertical foot offsets in meters from the desired standing
point on the ground, respectively.
•	xknee and zknee are the horizontal and vertical knee offsets in meters from the desired standing
point on the ground, respectively.
•	fz is the vertical ground reaction force on the robot in Newtons.
•	m is the robot weight in kilograms (i.e., m = 0.76 kg).
•	g is the gravitational acceleration in meters per second squared.
•	1touchdown is the indicator function of whether the robot has ever touched the ground.
•	zktanregeet is a target knee height of 0.1 m.
•	τknee and τhip are the knee and hip torques in Newton meters, respectively.
All methods used 72 full trajectories between each policy update, and each run was given 16 hours
of wall time, which corresponded to almost 500 million samples. This experiment was repeated 75
times for each method. The empirical mean of the discounted payoff values were reported without
any performance or seed filtration. The same hardware as the non-local rewards experiments (i.e.,
Xeon E5-2690-v3 processors and 256 GB of memory) was used.
23
Under review as a conference paper at ICLR 2021
A.11 Gym Suite Benchmarks
While it is clear that our deterministic policy gradient performs well on the new control environments
we consider, one may naturally wonder about its performance on existing RL control benchmarks.
We ran our method on a suite of Gym environments and include four representative examples in
Figure 3. Broadly speaking, our method (TDPO) performs slightly worse on average than others,
but occasionally performs much better as seen in the Swimmer-v3 environment. We speculate that
these gym environments are reasonably robust to injected noise, and this may mean that stochastic
policy gradients can more rapidly and efficiently explore the policy space, or there may be other
algorithmic enhancements that are needed for fully deterministic policy gradients in these cases. The
experiments granted each method 72 parallel MPI workers for about 144 million steps (i.e., 2 million
sequential steps), and the returns were averaged over 100 different seeds for each method. Since the
computational cost of running both DDPG and TD3 were high, we only included TD3 since it was
shown to outperform DDPG in earlier benchmarks.
InvertedDoubIePenduIum-V2	Swimmer-v3
HalfCheetah-v3
Figure 3: Results for the gym suite benchmarks.
Reacher-v2
A.12 Running Time Comparison
Figure 4 depicts a comparison of each method’s running time per million steps. These plots show the
combination of both the simulation (i.e., environment sampling) and the optimization (i.e., computing
the policy gradient and running the conjugate gradient solver) time. It is clear that our method (TDPO)
is generally faster than the other algorithms. This is mainly due to the computational efficiency of the
DeVine gradient estimator, which summarizes two full trajectories in a single state-action-advantage
tuple which can significantly reduce the optimization time. That being said, these relative comparisons
could vary to a large extent (1) under different processor architectures, (2) with more (or less) efficient
implementations, or (3) when running environments whose simulation time constitutes a significantly
larger (or smaller) portion of the total running time.
A.13 Other Swinging Pendulum Variants
Multiple variants of the pendulum with non-local rewards were used, each with different frequency
targets and the same reward structure. Table 3 summarizes the target characteristics of each variant.
24
Under review as a conference paper at ICLR 2021
Robotic Leg
300
200
100
InvertedDoublePendulum-VZ
HalfCheetah-v3
mJ IaJ
0
Swimmer-v3
Reacher-v2
0
tn
I200
I 150
息100
S
ŋ 50
I 200-	I 200-
150-	150-
-	100 -	ioo -
:***H ：U«*H ：***1
0
0
Qo Qo Qo 妗	Qo Qo Qo 妗	&	& Qo 妗
Figure 4: Training time comparison in different environments. The lower the bar, the faster the
method. The vertical axis shows the time in seconds needed to consume one million state-action pairs
for training. Each environment was shown separately in a different subplot.
The main variant was shown in the paper. Figures 5, 6, 7, 8, 9, 10, 11, and 12 show similar results
for the second to ninth variants. To focus on our method’s ability to solve all these variants efficiently,
we only show the performance of our method (TDPO) on all variants in Figure 13. Overall, we found
TRPO, PPO, DDPG, and TD3 to occasionally find the correct offset. They either excited the natural
or the maximum (not the desired) frequency of the pendulum, but they were not able to drive the
desired frequency and amplitude. TDPO was able to achieve the desired oscillations (and thus high
rewards) in all variants.
Pendulum Variant	Desired Frequency	Desired Offset	Desired Amplitude
Main	1.7-2 Hz	-0.524 rad-	0.28 rad
Second	0.5.-0.7 Hz	1.571rad	1.11 rad
Third	2.5-3 Hz	0.524 rad	0.28 rad
Fourth	2.-2.4 Hz	0.785 rad	0.28 rad
Fifth	2.-2.4 Hz	1.571rad	0.74 rad
Sixth	2-2.4 Hz	0.524 rad	0.28 rad
Seventh	2.-2.4 Hz	1.047 rad	0.28 rad
Eighth	2-2.4 Hz	0.785 rad	0.74 rad
Ninth	2.-2.4 Hz	1.309 rad	0.28 rad
Table 3: The target oscillation characteristics used to define different pendulum swinging environ-
ments.
A.14 Notes on How to Implement TDPO
In short, our method (TDPO) is structured in the same way TRPO was structured; both TDPO and
TRPO use policy gradient estimation, and a conjugate-gradient solver utilizing a Hessian-vector
product machinery. On the other hand, there are some algorithmic differences that distinguish TDPO
from TRPO. First of all, TRPO uses line-search heuristics to adaptively find the update scale; no
such heuristics are applied in TDPO. Second, TDPO uses the DeVine advantage estimator, which
requires storing and reloading pseudo-random generator states. Finally, the Hessian-vector product
machinery used in TDPO computes Wasserstein-vector products, which is slightly different from
those used in TRPO. The hyper-parameter settings and notes on how to choose them were discussed
25
Under review as a conference paper at ICLR 2021
12 3 4
Oooo
TTT
OAeds
O	IOOM
400M	500M
200M	300M
Sample Count
O 5	10 O 5	10
Frequency (Hz)	Frequency (Hz)
O 5	10
Frequency (Hz)
DDPG
O 5	10
Time (s)
⅛t≤- Desired Offset
jjd""''"''∙ Desired
I； Frequency
Amplitude
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
fcOAedus5
Figure 5: Results for the second variant of the simple pendulum with non-local rewards. Upper panel:
training curves with empirical discounted payoffs. Lower panels: trajectories in both the time domain
and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
1
-
2 3
O O
1 1
04
1
-
O
IOOM
400M
500M
200M	300M
Sample Count
PPO
TDPO
TRPO
-0.8-
P
tTJ
5 0.5 -
(U
> 0.2 -
<
0.0-
5	10
Time (s)
A∕V∖^^××
O 5	10
Time (s)
i fe- Desired Offset
O 5	10
Time (s)
"S
占 0.4-
ω
≤ 0.2 -
a.
E
< 0.0 - 1
6	5 ib
Frequency (Hz)
Desired
Frequency
& Amplitude
Desired Offset
Desired
Frequency
SeAmpIitude
*e- Desired Offset
Desired
r-∙<- Frequency
I I & Amplitude
IJLJJ
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
DDPG
TD3
5	10 O
Time (s)
5	10
Time (s)
Desired Offset
Desired
r-∙<- Frequency
i I & Amplitude
O 5	10
Frequency (Hz)
申 J Desired Offset
Desired
r-∙<- Frequency
i I & Amplitude
O 5	10
Frequency (Hz)
Figure 6:	Results for the third variant of the simple pendulum with non-local rewards. Upper panel:
training curves with empirical discounted payoffs. Lower panels: trajectories in both the time domain
and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
26
Under review as a conference paper at ICLR 2021
200M	300M	400M	500M
Sample Count
PPO	DDPG	TD3
O	IOOM
TDPO	TRPO
5	10 O
Time (s)
5	10 O
Time (s)
5	10 O 5	10 O
Time (s)	Time (s)
5	10
Time (s)
⅞o-s-
⅛0∙5-
jθ-2-
E
< 0.0-
⅜⅛- Desired Offset
Desired
Frequency
& Amplitude
	re— Desired Offset Desired b Frequency j-∣	& Amplitude
⅜e- Desired Offset
Desired
Frequency
& Amplitude
Λ*- Desired Offset
Desired
Frequency
& Amplitude
ι	k— Desired Offset Desired 一Frequency ∣^^j	& Amplitude
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
Figure 7:	Results for the fourth variant of the simple pendulum with non-local rewards. Upper panel:
training curves with empirical discounted payoffs. Lower panels: trajectories in both the time domain
and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
-IO1-
-IO2 -
-ιo3-
-IO4 -
O
IOOM
400M	500M
fcOAedus5
200M	300M
Sample Count
TDPO	TRPO	PPO	DDPG	TD3
9 20 ^
lŋ
ɪ ι.o-
6
U
<
o.o-
O
5	10 O
Time (s)
5	10 O
Time (s)
5
Time (s)
10 O 5	10 O
Time (s)
5	10
Time (s)
Desired Offset
Desired Offset
Desired Offset
i ι∙5-
∣ ι∙θ-
∣0∙5-
E
<o.o-,	,	,	,	,	,	,	,	,	,	,	,	,	,	,
O	5	10	O	5	10	O	5	10	O	5	10	O	5	10
Frequency (Hz)	Frequency (Hz)	Frequency (Hz)	Frequency (Hz)	Frequency (Hz)
Desired
Frequency
& Amplitude
	re— Desired Offset 口- Desired ! !	Frequency I I	& Amplitude
口- Desired
!! Frequency
I I & Amplitude
口- Desired
! !	Frequency
I I	& Amplitude
	re— Desired Offset 口- Desired ! !	Frequency I I	& Amplitude
Figure 8:	Results for the fifth variant of the simple pendulum with non-local rewards. Upper panel:
training curves with empirical discounted payoffs. Lower panels: trajectories in both the time domain
and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
27
Under review as a conference paper at ICLR 2021
0
IOOM	200M	300M	400M	500M
Sample Count
"S
占 0.4-
¾
≤ 0.2 -
d
E
< 0.0-
i ~ Desired Offset
Desired
Γ∣<- Frequency
j I & Amplitude
Ld
w÷- Desired Offset
Desired
Γ∣<- Frequency
j I & Amplitude
w÷- Desired Offset
Desired
Γ∣<- Frequency
j I & Amplitude
⅛e- Desired Offset
Desired
Frequency
& Amplitude
ΠHΠ⅛
0	5	10
Frequency (Hz)
0	5	10
Frequency (Hz)
0	5	10
Frequency (Hz)
0	5	10
Frequency (Hz)
0	5	10
Frequency (Hz)
Figure 9:	Results for the sixth variant of the simple pendulum with non-local rewards. Upper panel:
training curves with empirical discounted payoffs. Lower panels: trajectories in both the time domain
and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
fcOAedus5
-IO2 -
-103-
-IO4 -
0	IOOM	200M	300M	400M	500M
Sample Count
TDPO	TRPO	PPO	DDPG	TD3
ŋ 1.0-
fθ
Φ
⊂ 0.0-
<
I	I	I	I	I	I	I	I	I	I	I	I	I	I	I
O	5	10	O	5	10	O	5	10	O	5	10	O	5	10
Time (s)	Time (s)	Time (s)	Time (s)	Time (s)
Desired Offset
Desired
Frequency
∣]L & Amplitude
Desired Offset
Desired
Frequency
∣]L & Amplitude
Desired Offset
Desired
Frequency
∣]L & Amplitude
⅛≤- Desired Offset
Desired
Frequency
& Amplitudes
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
Figure 10:	Results for the seventh variant of the simple pendulum with non-local rewards. Upper
panel: training curves with empirical discounted payoffs. Lower panels: trajectories in both the time
domain and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
28
Under review as a conference paper at ICLR 2021
O 12 3 4
O Oooo
1 Illl
- - - - -
OAeds
-IO5
O
IOOM
400M	500M
200M	300M
Sample Count
ŋ 1.0-
fθ
ω
目 o.o-
<
TDPO
TRPO
PPO	DDPG	TD3
O 5	10 O 5	10 O
Time (s)	Time (s)
5	10 O
Time (s)
5	10 O 5	10
Time (s)	Time (s)
⅞0-8-
[θ∙5-
tθ2-
E
< 0.0-
O 5	10
Frequency (Hz)
•— Desired Offset
11 Desired
：j Frequency
& Amplitude
*— Desired Offset
j I Desired
：j Frequency
! :	& Amplitude
Desired Offset
卜
j I Desired
：j Frequency
! j & Amplitude
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
Desired Offset
j I Desired
：j Frequency
! j & Amplitude
∙≤- Desired Offset
Desired
Frequency
SeAmpIitude
Frequency (Hz)
Figure 11:	Results for the eighth variant of the simple pendulum with non-local rewards. Upper
panel: training curves with empirical discounted payoffs. Lower panels: trajectories in both the time
domain and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
O
IOOM
2
O
1
OAeds
3 4
O O
T T
200M	300M	400M
Sample Count
O 5	10 O 5	10
Frequency (Hz)	Frequency (Hz)
⅛e- Desired Offset
Desired
Frequency
& Amplitude
O 5	10
Frequency (Hz)
⅛e-- Desired Offset
Desired Offset
Desired
Frequency
& Amplitude
Desired
Frequency
& AmPIitU 货
O 5	10
Frequency (Hz)
O 5	10
Frequency (Hz)
Figure 12:	Results for the ninth variant of the simple pendulum with non-local rewards. Upper panel:
training curves with empirical discounted payoffs. Lower panels: trajectories in both the time domain
and frequency domain, showing target values of oscillation frequency, amplitude, and offset.
29
Under review as a conference paper at ICLR 2021
in Sections A.9, A.6, and A.10. We will describe how to implement TDPO, and focus on the subtle
differences between TDPO and TRPO next.
As for the state-reset capability, our algorithm does not require access to a reset function to arbitrary
states. Instead, we only require to be able to start from the prior trajectory’s initial state. Many
environments, including the Gym environments, instantiate their own pseudo-random generators
and only utilize that pseudo-random generator for all randomized operations. This facilitates a
straightforward implementation of the DeVine oracle; in such environments, implementing an
arbitrary state-reset functionality is unnecessary, and only reloading the pseudo-random generator to
its configuration prior to the trajectory would suffice. In other words, the DeVine oracle can store the
initial configuration of the pseudo-random generator before asking for a trajectory reset, and then start
sampling. Once the main trajectory is finished, the pseudo-random generator can be reloaded, thus
producing the same initial state upon a reset request. Other time-step states can then be recovered by
applying the same preceeding action sequence.
To optimize the quadratic surrogate, the conjugate gradient solver was used. Implementing the
conjugate gradient algorithm is fairly straightforward, and is already included in many common
automatic differentiation libraries. The conjugate gradient solver is perfect for situations where (1)
the Hessian matrix is larger than can efficiently be stored in the memory, and (2) the Hessian matrix
includes many nearly identical eigenvalues. Both of these conditions apply well for TDPO, as well as
for TRPO. Instead of requiring the full Hessian matrix to be stored, the conjugate gradient solver
only requires a Hessian-vector product machinery v → Hv, which must be specifically implemented
for TDPO. Our surrogate function can be viewed as
C0
L(δθ) = gT δθ + 2δθ δθT Hδθ
where the Hessian matrix can be defined as
C0
H = H2 + OH H1,
C2
H1 := V2,Es〜ρμk Lg2 (∏0,∏k; S),
H2 ：= V2oEs〜ρμk W(π0(a∣s),∏k(a∣s))2 .	(73)
In order to construct a Hessian-vector product machinery v → Hv, one can design an automatic-
differentiation procedure that returns the Hessian-vector product. Many automatic-differentiation
packages already include functionalities that can provide a Hessian-vector product machinery of a
given scalar loss function without computing the Hessian matrix. This can be used to implement the
Hessian-vector product machinery in a straightforward manner; one only needs to provide the scalar
quadratic terms of our surrogate, and would obtain the Hessian-vector product machinery in return.
On the other hand, this may not be the most computationally efficient approach, as our problem
exhibits a more specific structure. Alternatively, one can implement a more elaborate and specifically
designed Hessian-vector product machinery by following these three steps:
•	Compute the Wasserstein-vector product v → H2v according to Algorithm 3.
•	Compute the Sensitivity-vector product v → H1v according to Algorithm 4.
•	Return the weighted sum of H1v and H2v as the final Hessian-vector product Hv.
One may also need to add a conjugate gradient damping to the conjugate gradient solver (i.e., return
βv + Hv for some small β as opposed to returning Hv purely), which is also done in the TRPO
method. This may be important when the number of policy parameters is much larger than the sample
size. Setting β = 0 may yield poor numerical stability if H had small eigenvalues, and setting large
β will cause the conjugate gradient optimizer to mimic the gradient descent optimizer by making
updates in the same direction as the gradient. The optimal conjugate gradient damping may depend
on the problem and other hyper-parameters such as the sample size. However, it can easily be picked
to be a small value that ensures numerical stability.
Once the conjugate gradient solver returned the optimal update direction H-1g, it must be scaled
down by a factor of C2 (i.e., δθ* = H-1g∕C2). If δθ* satisfied the trust region criterion (i.e.,
30
Under review as a conference paper at ICLR 2021
1 δθ*τHδθ* ≤ δ2mχ), then one can make the parameter update (i.e., θ∩ew = θoid + δθ*) and proceed
to the next iteration. Otherwise, the proposed update δθ* must be scaled down further, namely by
α, such that the trust region condition would be satisfied (i.e., 2(αδθ*)TH(αδθ*) = δ2mχ) before
making the update θ∩ew = θold + αδθ*.
Algorithm 3 Wasserstein-Vector-Product Machinery
Require: Current Policy π1 with parameters θ1 .
Require: The vector v with the same dimensions as θ1.
Require: An observation s.
1: Compute the action for the observation s with |A| elements.
一 π⑴(s1
a| A∣× 1 :=	.
∏(IAI)(S)
(74)
This vector should be capable of propagating gradients back to the policy parameters when used
in automatic differentiation software.
2: Define t to have be a constant vector with the same shape as a. It could be populated with any
values such as all ones.
3: Define the scalar a := aτt.
4: Using back-propagation, find the gradient
IAI	IAI
Nea = X tiVθai = X ti [dai	…	∂θaΘ?] .	(75)
i=1	i=1
5: Compute the following dot-product:
IAI ∂a	IAI
"θa,vi = (X ti ∙ ∂θi) ∙ vι +------+ (X ti ∙
i=1	* 1 2 3 4	i=1
∂ai
dθ∣Θ∣
) ∙ v∣θ∣.
(76)
6:	Using automatic differentiation, take the gradient w.r.t. the t vector.
∂a1	∂a1
∂θ1 ∙ v1 +	+ ∂θ^ ∙ v∣θ∣
ae,v ：= Nthyea,vi =	.
∂α∣A∣	.	. ∂α∣A∣
.^θτ ∙ vι +	+ ∂θ∣Θ∣ ∙ v∣θ∣
-∂aι
∂θ1
da∣ A∣
~∂iΓ
daɪ ~
∂θ^
.V (77)
∂a∣A∣
dθ∣Θ∣ ,
7:	Compute the dot product h<⅛,v, a).
8:	Using back-propagation, take the gradient w.r.t. θ, and return it as the gain-vector-product.
T
-dɑɪ
∂θ1
dɑɪ
∂θ^∣
-dɑɪ
∂θ1
▽i hɑi,v, a)
dα∣ A∣
~∂θΓ
∂α∣A∣
∂θ^.
dα∣A∣
~∂θΓ
dɑɪ ~
∂θ^
.
. V
.
∂a∣A∣
∂θ^J
(78)
31
Under review as a conference paper at ICLR 2021
Algorithm 4 Sensitivity-Vector-Product Machinery
Require: Current Policy ∏ι with parameters θι.
Require: The vector v with the same dimensions as θι.
Require: An observation s.
1:	Compute the action to observation Jacobian matrix
∂π(I) (s)
∂s1
∂π⑴(s)
∂s(ISI)
j∣A∣×∣S∣ :：
(79)
∂π(l A|) (s)
∂s(I)
∂π(l A|) (s)
∂s∣S∣
This can either be done using finite-differences in the observation using
∂∏(i)(s)	n(i)(s + ds ∙ ej) — n(i)(s)
∂s(G —	ds
(80)
(which may be a bit numerically inaccurate), or using automatic differentiation. In any case, this
matrix should be a parameter tensor capable of propagating gradients back to the parameters
when used in automatic differentiation software.
2:	Define J to be the vectorized (i.e., reshaped into a column) J matrix, with ∖AS| = |A| × |S|
rows and one column.
3:	Define t to have be a constant vector with the same shape as J. It could be populated with any
values such as all ones.
4:	Define the scalar Jt := JTt.
5:	Using back-propagation, find the gradient
∣a∣ ∣s∣	∣a∣ ∣s∣
vθJt = XX ti,jvθJij = XX ti,j [⅞θiτ ∙ ∙ ∙ !¾].	(81)
i=1j=1	i=1j=1
6:	Compute the following dot-product.
∣a∣ ∣s∣	∂ J.	∣a∣ ∣s∣	∂ J.
hvθ JtNV = (XX ti,j ∙ ~θj~ ) X v1 +-----------+ (XX ti,j ∙ ∂θhj ) ×v IΘ ।	(82)
i=1 j = 1	1	i=1 j = 1	∣ θ I
7:	Using automatic differentiation, take the gradient w.r.t. the t vector.
∂J1,1	I	I ∂J1,1
~∂ΘΓ ∙ vι +	+ 明面∙ v IΘ I
(VθJ)v := VthVθJt,v)
dj∣A∣,∣s∣
~^1-
∙ vι + ∙ ∙ ∙ +
dj∣A∣,∣s∣
dθ∣Θ∣
v IΘ I
r ∂jQ，1)	∂jQ，1)i
-- ... --
∂Θ1	∂Θ∣Θ∣
.	. V
..
∂J (∣A∣,∣S∣)	∂J (∣A∣,∣S∣)
- - . ♦ ♦ - -
L	∂Θ1	∂Θ∣Θ∣ 」
(83)
8:	Reshape (Vθ J)v into a column vector and name it Jθ,v.
9:	Compute the dot product Jθ,v, J).
10:	Using back-propagation, take the gradient w.r.t. θ, and return it as the gain-vector-product.
	Γ ∂J(1,D 	 . ∂Θ1	∂j(1,d η • 			:	 dθ∣Θ∣	T	「∂J (1,D 	 . ∂Θ1	∂J (1,D 1 • 			:	 dθ∣Θ∣
V7 / T	T∖ Vθ Jθ,v, J V =	. . .	. . .		. . .	.	V .
	∂J (∣A∣,∣S∣)	∂J (∣A∣,∣S∣)		∂J (∣A∣,∣S∣)	∂J (∣a∣,∣s∣)
		 . L ∂Θ1	• 			:	 dθ∣Θ∣	-			 . L ∂Θ1	• 			：	 dθ∣Θ∣	-
(84)
32
Under review as a conference paper at ICLR 2021
Main Variant
Second Variant
Third Variant
/wwwwwwwwʌ
∕vwwwwwwvwwvvww
Fourth Variant
Sixth Variant
0	2	4	6	8	10
Frequency (Hz)
(a)
∕wwwwwwvvwv≡
0	2	4	6	8	10
Time (s)
Desired Offset
1r∣*一, DeSired
i !	Frequency
! i	& Amplitude
0	2	4	6	8	10
Frequency (Hz)
(b)
Eighth Variant
Ninth Variant
Seventh Variant
Figure 13: Time and frequency domain trajectories for our method (TDPO) on multiple variants of
the simple pendulum with non-local rewards. (a) The high-reward trajectories for the first group
of variants, (b) the high-reward trajectories for the second group of variants, (c) the high-reward
trajectories for the third group of variants. Target values of oscillation frequency, amplitude, and
offset were annotated in the frequency domain plots.
O 2	4	6	8	10
Frequency (Hz)
(c)
1	i*	Desired Offset Desired Frequency 迎*	& AmPIitlJde
33