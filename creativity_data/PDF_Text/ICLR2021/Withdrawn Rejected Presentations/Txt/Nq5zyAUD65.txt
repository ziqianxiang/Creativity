Under review as a conference paper at ICLR 2021
Smooth Activations and Reproducibility in
Deep Networks
Anonymous authors
Paper under double-blind review
Ab stract
Deep networks are gradually penetrating almost every domain in our lives due to
their amazing success. However, with substantive performance accuracy improve-
ments comes the price of irreproducibility. Two identical models, trained on the
exact same training dataset may exhibit large differences in predictions on indi-
vidual examples even when average accuracy is similar, especially when trained
on highly distributed parallel systems. The popular Rectified Linear Unit (ReLU)
activation has been key to recent success of deep networks. We demonstrate, how-
ever, that ReLU is also a catalyzer to irreproducibility in deep networks. We show
that not only can activations smoother than ReLU provide better accuracy, but they
can also provide better accuracy-reproducibility tradeoffs. We propose a new fam-
ily of activations; Smooth ReLU (SmeLU), designed to give such better tradeoffs,
while also keeping the mathematical expression simple, and thus implementation
cheap. SmeLU is monotonic, mimics ReLU, while providing continuous gradi-
ents, yielding better reproducibility. We generalize SmeLU to give even more flex-
ibility and then demonstrate that SmeLU and its generalized form are special cases
of a more general methodology of REctified Smooth Continuous Unit (RESCU)
activations. Empirical results demonstrate the superior accuracy-reproducibility
tradeoffs with smooth activations, SmeLU in particular.
1	Introduction
Recent developments in deep learning leave no question about the advantages of deep networks
over classical methods, which relied heavily on linear convex optimization solutions. With their as-
tonishing unprecedented success, deep models are providing solutions to a continuously increasing
number of domains in our lives. These solutions, however, while much more accurate than their con-
vex counterparts, are usually irreproducible in the predictions they provide. While average accuracy
of deep models on some validation dataset is usually much higher than that of linear convex models,
predictions on individual examples of two models, that were trained to be identical, may diverge
substantially, exhibiting Prediction Differences that may be as high as non-negligible fractions of
the actual predictions (see, e.g., Chen et al. (2020); Dusenberry et al. (2020)). Deep networks ex-
press (only) what they learned. Like humans, they may establish different beliefs as function of the
order in which they had seen training data (Achille et al., 2017; Bengio et al., 2009). Due to the huge
amounts of data required to train such models, enforcing determinism (Nagarajan et al., 2018) may
not be an option. Deep networks may be trained on highly distributed, parallelized systems. Thus
two supposedly identical models, with the same architecture, parameters, training algorithm and
training hyper-parameters that are trained on the same training dataset, even if they are initialized
identically, will exhibit some randomness in the order in which they see the training set and apply
updates. Due to the highly non-convex objective, such models may converge to different optima,
which may exhibit equal average objective, but can provide very different predictions to individual
examples. Irreproducibility in deep models is not the classical type of epistemic uncertainty, widely
studied in the literature, nor is it overfitting. It differs from these phenomena in several ways: It does
not diminish with more training examples like classical epistemic uncertainty, and it does not cause
degradation to test accuracy by overfitting unseen data to the training examples.
While irreproducibilty may be acceptable for some applications, it can be very detrimental in appli-
cations, such as medical ones, where two different diagnoses to the same symptoms may be unac-
ceptable. Furthermore, in online and/or re-enforcement systems, which rely on their predictions to
1
Under review as a conference paper at ICLR 2021
determine actions, that in turn, determine the remaining training examples, even small initial irre-
producibility can cause large divergence of models that are supposed to be identical. One example is
sponsored advertisement online Click-Through-Rate (CTR) prediction (McMahan et al., 2013). The
effect of irreproducibility in CTR prediction can go far beyond changing the predicted CTR of an
example, as it may affect actions that take place downstream in a complex system. Reproducibility
is a problem even if one trains only a single model, as it may be impossible to determine whether the
trained model provides acceptable solutions for applications that cannot tolerate unacceptable ones.
A major factor to the unprecedented success of deep networks in recent years has been the Rec-
tified Linear Unit (ReLU) activation, (Nair & Hinton, 2010). ReLU nonlinearity together with
back-propagation give simple updates, accompanied with superior accuracy. ReLU thus became
the undisputed activation used in deep learning. However, is ReLU really the best to use? While it
gives better optima than those achieved with simple convex models, it imposes an extremely non-
convex objective surface with many such optima. The direction of a gradient update with a gradient
based optimizer is determined by the specific example which generates the update. Thus the order of
seeing examples or applying updates can determine which optimum is reached. Many such optima,
as imposed by ReLU, thus provide a recipe for irreproducibility. In recent years, different works
started challenging the dominance of ReLU, exploring alternatives. Overviews of various activations
were reported in Nwankpa et al. (2018); Pedamonti (2018). Variations on ReLU were studied in Jin
et al. (2015). Activations like SoftPlus (Zheng et al., 2015), Exponential Linear Unit (ELU) (Clevert
et al., 2015), Scaled Exponential Linear Unit (SELU) (Klambauer et al., 2017; Sakketou & Ampazis,
2019; Wang et al., 2017), or Continuously differentiable Exponential Linear Unit (CELU) (Barron,
2017) were proposed, as well as the Gaussian Error Linear Unit (GELU) (Hendrycks & Gimpel,
2016). Specifically, the Swish activation (Ramachandran et al., 2017) (that can approximate GELU)
was found through automated search to achieve superior accuracy to ReLU. Further activations with
similarity to GELU were proposed recently; Mish (Misra, 2019), and TanhExp (Liu & Di, 2020).
Unlike ReLU, many of these activations are smooth with continuous gradient. Good properties of
smooth activations were studied as early as Mhaskar (1997) (see also Du (2019); Lokhande et al.
(2020)). These series of papers started suggesting that smooth activations, if configured properly,
may be superior to ReLU in accuracy. Recent work by Xie et al. (2020), that was done subsequently
to our work, and was inspired from our results that we report in this paper, (Lin & Shamir, 2019),
demonstrated also the advantage of smooth activations for adversarial training.
Our Contributions: In this paper, we first demonstrate the advantages of smooth activations to
reproducibility in deep networks. We show that not only can smooth activations improve accuracy
of deep networks, they can also achieve superior tradeoffs between reproducibility and accuracy, by
attaining a lower average Prediction Difference (PD) for the same or better accuracy.
Smooth activations, like Swish, GELU, Mish, and TanhExp, all have a very similar non-monotonic
form, that does not provide a clear stop region (with strict 0; not only approaching 0), and slope 1
region. While these activations approximate the mathematical form of ReLU, they lack these prop-
erties of ReLU. All these activations including SoftPlus also require more expensive mathematical
expressions, involving exponents, and, in some, logarithms, or even numerically computed values
(e.g., GELU). This can make deployment harder, especially with certain simplified hardware that
supports only a limited number of operations, as well as can slow down training due to the heavier
computations. Unlike ReLU, which can be transformed into Leaky ReLU, the smooth activations
described cannot be easily transformed into more general forms. In this work, we propose Smooth
ReLU (SmeLU), which is mathematically simple and based only on linear and quadratic expressions.
It can be more easily deployed with limited hardware, and can provide faster training when hard-
ware is limited. SmeLU provides a clearly defined 0 activation region, as well as a slope 1 region, is
monotonic, and is also extendable to a leaky or more general form. SmeLU gives the good proper-
ties of smooth activations providing better reproducibility as well as better accuracy-reproducibility
tradeoffs. Its generalized form allows even further accuracy improvements. The methodology to
construct SmeLU is shown to be even more general, allowing for more complex smooth activations,
which are all clustered under the category of Rectified Smooth Continuous Units (RESCUs).
Related Work: Ensembles (Dietterich, 2000) have been used to reduce uncertainty (Lakshmi-
narayanan et al., 2017). They are useful also for reducing irreproducibility. However, they make
models more complex, and can trade off accuracy in favor of reproducibility if one attempts to keep
constant computation costs (which require reducing capacity of each component of the ensemble).
Compression of deep networks into smaller networks that attempt to describe the same information
2
Under review as a conference paper at ICLR 2021
is the emerging area of distillation (Hinton et al., 2015). Predictions of a strong teacher train a
weaker student model. The student is then deployed. This approach is very common if there are
ample training resources, but deployment is limited, as for mobile network devices. Co-distillation,
proposed by Anil et al. (2018) (see also Zhang et al. (2018)), took advantage of distillation to address
irreproducibility. Instead of unidirectional transfer of knowledge, several models distill information
between each other. They attempt to converge to the same solution. The method requires more
training resources to co-train models, but deployment only requires a single model. A somewhat op-
posite approach; Anti-Distillation, to address irreproducibility was proposed by Shamir & Coviello
(2020), embracing ensembles, with an additional loss that forces their components away from one
another. Each component is forced to capture a (more) different part of the objective space, and as
a whole, the predictions of the ensemble are more reproducible. To the best of our knowledge, all
previously reported techniques to address irreproducibility in deep networks required some form of
ensembles. In this work, we leverage smooth activations, and do not require ensembles.
Outline: Section 2 proposes several PD metrics, which we use to measure irreproducibility. Next,
we overview our setup and smooth activations in Section 3, and describe SmeLU and its generaliza-
tion in Section 4. Experimental results are shown in Section 5.
2	Prediction Difference
Average individual per-example Prediction Difference (PD) over a set of models that are configured,
trained, and supposed to be identical over some validation dataset can be defined in various ways.
We refer to Shamir & Coviello (2020) for a more detailed discussion. We opt to the same definitions
they used, where we describe the classification case, in which we measure the PD using some Lp
norm on the actual label predictions. Following their notation, denote the number of models by M,
and the number of validation examples by N. Let Pn,m be the distribution over labels predicted by
model m for example n. (Pnm(' is the probability predicted for label'.) Let Pn 4 Pm PnmJM
be the expected distribution over labels over all M models. Then, the pth norm PD, ∆p , is given by
1N1M
δP = N X ∙ M X kPn,m
- Pnp
-1 X ∙ -1X
N乙M乙
n=1	m=1
1/p
X ∣Pn,m(') - Pn(')∣p
`
(1)
With practical large scale systems, with very high training costs, we can use M = 2, where for
binary labels, ∆ι = 1/N ∙ Pn ∣Pn,ι(1) - Pn,2(l)∣, where 1 is the positive label. As in Shamir &
Coviello (2020), we can consider relative PD, ∆r1, normalizing the innermost summand in (1) by
Pn('), which for binary problems can be tweaked into ∆r, normalizing by 2(1) instead. PD, ∆L,
can be computed only on the observed true label, by replacing the innermost sum on ` in (1) by
∣Pn,m('true) - Pn('true)∖ for the true label, normalizing by Pn('true). Its computation, however,
requires knowledge of the true label. Finally, in classification, one can use Hamming PD; ∆H,
specifying the average fraction of labels predicted differently between model pairs.
3	Smooth Activations
We consider standard Neural Networks. In addition to numerical features, inputs can also be embed-
ding vectors representing unmeasurable features, learned together with weights and biases of hidden
layers. Networks can be deep or shallow (a few or even a single hidden layer). The output of the ma-
trix multiplication in any of the networks’ hidden layers is activated by a nonlinear activation. Our
results apply in a standard setup, where training passes over a training dataset for multiple epochs,
allowing example revisits. We also consider the online setting, where training consists of a single
pass over the data. Inference is applied to unseen examples before updates. Normally, updates are
applied in mini-batches. Some form of normalization may be applied in the hidden layers to limit
signal range, guarding from vanishing or exploding gradients, and providing some scale invariance.
Without normalization, activations can start drifting to become larger. As we show, smooth activa-
tions are controlled by parameters that dictate their behavior. Inputs that scale up generate an effect
of scaling down widths of the activation regions. As we will observe, slower smoother gradient
changes, manifested as wider input regions for the same output change, are more reproducible. With
such drifts, they gradually appear to be narrower, slowly losing reproducibility benefits. Normaliza-
tions, as weight normalization (Salimans & Kingma, 2016), layer normalization (Ba et al., 2016),
3
Under review as a conference paper at ICLR 2021
or batch normalization (Ioffe & Szegedy, 2015) can be considered. In this work, however, we use
a slightly different form of weight normalization, where some Lp norm normalizes all matrix link
weights incoming to an output neuron of a layer. We specifically normalize the L2 norm of the
incoming matrix weights into the neuron to some norm v without centering around the mean.
Let W' be the weight matrix incoming into layer ' from layer ' - 1, use Wj to denote the jth row
of W'. Then, the pre-aCtiVation neuron vector of layer ' is given by a' = W' ∙ f (aj-1), where f (∙)
denotes the activation non-linearity (except for the input layer ` = 0, where it is the identity). Then,
with L2 weight normalization, we replace wj by Wj 4 V ∙ wj/k wj ∣∣2. Norms, other than L2, as well
as other normalization regimes can also be used. As we demonstrate below, for smooth activations,
the norm v can trade off with the activation parameters, tuning the accuracy/reproducibility tradeoffs,
but not changing the good effects of smooth activations. As demonstrated in Appendix A, lack of
normalization with nonlinearity in the form of clipping values can diminish the benefits of smooth
activations. Note that recent work by Liu et al. (2020) combined normalization with activations.
As described, several forms of smooth activations have been recently proposed in the literature.
They all attempt to mimic the shape of ReLU but with a smoother function. They also try to avoid
similarity to the smooth Sigmoid, σ(x) = 1/[1 + exp(-x)], which was considered several decades
ago, but had limited success due to diminishing gradients. Many of them can be parameterized by
one parameter or more. While some are smooth for all parameter values, others, like SELU, are only
smooth with specific values of the parameters. Let β be the main activation parameter (SELU will
also have another parameter λ). Let erf(∙) denote the standard error function and Φ(∙) the standard
normal CDF. Then, activation functions for some smooth activations are given by
x,	if x > 0
ySELU	=	λ	∙	I βex — β, if X ≤ 0
USoftPlus	=	1	∙ log [1 + exp (β ∙ x)]
β
ySwish	=	x	∙	σ (β ∙ x)
Ugelu	=	X	∙	； ∙ [l + erf (β ∙ x∕√2)] = X ∙ Φ(β ∙ x)
yMish = x ∙ tanh [log (1 + exp(β ∙ x))]
yTanhExp = X ∙ tanh (e"") ∙
(2)
(3)
(4)
(5)
(6)
(7)
ELU is SELU with λ = 1, and CELU is a differentially continuous variant of SELU, shown in Ap-
pendix C. We generalized SoftPlus, GELU, Mish, and TanhExp by adding the β parameter. GELU
can be approximated by Swish with GELUe(x) ≈ xσ(∖∕8∕∏βx)∙ Fig. 1 shows the different activa-
tions and their gradients for different values of the parameter β . (More detailed figures are shown in
Appendix C.) All the activations described thus far share a region in which the activation function
limits changes in the signal with a gradient close to 0 (on the left), and a region in which the gradient
approaches 1 on the right. The width of the middle transition region is a function of the parameter β,
where it is wider with smaller β and narrower with greater β. For β → ∞, the activation resembles
ReLU, and for β → 0, it becomes closer to linear.
Figure 1: Smooth activations (top) and their gradients (bottom) for different β parameter values.
4
Under review as a conference paper at ICLR 2021
The discontinuous gradient of the ReLU partitions the parameter space into regions, each of which
has its own local optimum. Many of the local optima may be identical in the overall total objective,
but not on how they predict for individual examples. Randomness in training leads the model into
one of the regions, and locks it in a region of some local optima. Specifically, sudden jumps in
the gradient, like those of ReLU, may lock some hidden units at some state, requiring other units
to compensate and move towards their nearest optima. If this happens early in training (see, e.g.,
Achille et al. (2017)), it determines the trajectory of the optimization based on the examples seen
and the updates applied. Different example and update orders thus lead to different trajectories.
Smoother activations give a training model less opportunities to diverge. Wider transition regions
allow gradients to change more slowly, and units in the network to gradually adjust to updates
and changes. Wider transition regions are closer in shape to more reproducible linear functions.
However, a good activation must also retain good accuracy. The Sigmoid was unable to do that.
Unfortunately, while reproducibility mostly tends to improve with widening the transition region
of the activation, accuracy moves mostly in the opposite direction. As we lower β and widen the
region, we generally trade accuracy for reproducibility. This, however, is not the full picture. While
moving towards ReLU improves accuracy on datasets we observed, the optimal point is not at the
ReLU point of β → ∞ but for some finite β, after which accuracy degrades. This shows that
properly designed smooth activations for the specific dataset can be actually superior to ReLU in
both accuracy and reproducibility. They also provide a knob to trade off between the two. Similar
behavior is also observed in the opposite direction, where the best reproducibility for some datasets
may be achieved for some β > 0, worsening for smaller values. The overall tradeoffs are dataset,
model architecture and model configuration dependent.
4 SmeLU to the RESCU
Swish, GELU, Mish, and TanhExp, all have a non-monotonic region on the left of the transition
region, neighboring the signal suppression region. This may be vulnerable to input irregularities,
possibly less interpretable, and perhaps suboptimal. SoftPlus is monotonic, but the wider the tran-
sition region, the farther it is from the origin, degrading its accuracy. In addition, for all these
activations, there are no clean stop and pass regions like in ReLU, except for very large β . Asymp-
totically, for β → ∞, all tend to look like ReLU, but for wider transition regions, which are good
for reproducibility, they do not reach the ReLU extremes on the left and right. Furthermore, all
the activations considered require complex mathematical functions, such as exponents. Such func-
tions require more complex hardware implementations, that could be very costly in huge scale sys-
tems. The computationally heavy functions could also slow down training and inference. We now
show a very simple smooth ReLU activation, SmeLU, that benefits from smoothness, monotonicity,
clean stop and pass regions, a very simple mathematical implementation, and comparable or better
accuracy-reproducibility tradeoffs. We show that the same design is extendible to a generalization
of this activation as well as to more sophisticated extensions.
SmeLU: In some literature, SoftPlus is referred to as Smoothed ReLU. Recent work (Bresler &
Nagaraj, 2020) also considered smoothing ReLU using initial coefficients of its Fourier series. ReLU
is piecewise continuous linear, but with a gradient discontinuity. As shown in Chen & Ho (2018);
Montufar et al. (2014), the concept of piecewise linear can be extended to multiple pieces. We
take a further step here and construct a piecewise continuous activation but also with a continuous
gradient, by simply enforcing these conditions. The general idea is to define the activation as a
piecewise linear and quadratic (resembling Huber loss (Huber, 1992) on one side). On both sides,
we match ReLU and its gradients, and we fit a quadratic middle region between. We define β to be
the half-width of a symmetric transition region around x = 0. Note that this definition of SmeLU
uses a parameter β that is reciprocal to the β in Swish and other activations like it in the sense that
larger β here gives a wider transition region, and a smaller β a narrower one. To the left of the
transition region, y = 0. To its right, y = x. Then, we enforce the gradients to be continuous by
(dy/dX)|x=-β = 0,	(dy/dX)lχ = β = 1.	⑻
Applying the conditions gives the SmeLU activation
(	0; x ≤ —β
ySmeLU = <	(x+；" ；	∣X∣ ≤ β	⑼
I	x; X ≥ β∙
5
Under review as a conference paper at ICLR 2021
Fig. 1 (also Fig. 12) shows SmeLU and its continuous gradient as function of β. It matches ReLU on
both sides, giving a gentle middle transition region, wider with greater β and narrower with smaller
β. SmeLU is a convolution of a ReLU with a box of magnitude 1 /(2β) in [—β, β]. Its gradient is a
hard Sigmoid. As with other smooth activations, we observe that with greater β, reproducibility is
improved possibly at the expense of accuracy. Optimal accuracy on datasets we experimented with
is for some β > 0. For β → 0, SmeLU becomes ReLU. Fig. 2 demonstrates the surface of the
output of a network with two inputs as function of the values of these inputs, with ReLU, SmeLU
with different β, and no activation (top). Surfaces are obtained with weight normalization. The
bottom row shows SmeLU with β = 0.5 for different L2 weight norms. Examples of effects of
normalization, clipping, and other activations on this surface are shown in Figs. 5-7 in Appendix A.
Fig. 5 demonstrates the importance of normalization with clipping activations. Fig. 6 shows similar
surfaces of other smooth activations. Fig. 7 shows effects of (other) normalizations. In Fig 2, we
observe that ReLU exhibits multiple valleys (that lead to multiple optima in higher dimensions).
Smoothing the activation by increasing β smoothes these valleys. Thus with a smooth activation,
there are fewer optima that the model can find while training, each spanning a larger region around
it. However, activations that are too smooth resemble linear models with worse accuracy.
The parameter of SmeLU (as well as of other smooth activations) interacts with the normalization
applied, as demonstrated in the second row of Fig. 2. Smoothing of the surface can be achieved by
either tuning β or the norm of the normalization applied. This, however, is only possible with smooth
activations. Tuning weight norm with ReLU does not smoothen the surface. Thus the smooth
activation allows for smoother surfaces whose smoothness (and good reproducibility behavior) can
be tuned by either the parameters of the activation or by normalization. For consistency, one can
keep the normalization to a norm ofv = 1, and study the tradeoff as function ofβ, or apply SmeLU
with no normalization (which may affect accuracy and reproducibility) and no clipping.
An approach that may improve the overall PD/accuracy tradeoff is to use wider values ofβ for layers
closer to the inputs and narrower β values for layers closer to the output. Reproducibility seems to
be dominated by rich parameter sets, and applying wider β for layers that control such sets could
improve it. Then, layers closer to the output can focus on better prediction accuracy. Alternatively,
one can learn the values of β for the whole model, per layer, or even for individual units, as part of
the network training. Some insights on how this can be done are given in Appendix B.
Figure 2: Three dimensional surfaces showing network outputs as function of 2-dimensional inputs
in [-6, 6], with 5 hidden layers of dimensions [256, 128, 64, 32, 16] activated by ReLU, SmeLU with
different β parameters, and no activation (linear). Layers apply weight normalization, and there is no
clipping of large activations. Matrices of all hidden layers are equal for all figures and are randomly
drawn from a standard normal N (0, 1) distribution. Top: Different activations and SmeLU β values.
Bottom: SmeLU with β = 0.5 for different L2 weight norms.
Generalized SmeLU: The constraints in (8) can be generalized to a wider family of activations,
including ones such as a leaky SmeLU, by allowing different gradients g- 6= 0 to the left and
g+ > g- to the right of the quadratic transition region, and allowing for an asymmetric transition
region, and for a bias shift. The conditions are then
(dy∕dχ)∖x=-α = g-,	(dy/dX)|x=e = g+, y(X = -α) = t	(IO)
6
Under review as a conference paper at ICLR 2021
The five parameters {α, β, g-, g+, t} define a generalized SmeLU. Typically (but not necessarily),
α, β > 0, and t ≤ 0 possibly to allow the activation to cross through the origin. For a leaky
activation, g- > 0, but we can also have a negative slope on the left (where then we relax the
monotonicity requirement). Enforcing (10) and continuity of the function at x = -α and x = β,
we come to the definition of the generalized SmeLU activation
{g-x + t + g-α;	X ≤ —α
ax2 + bx + c;	-α ≤ x ≤ β
g+x + t + α+βg- + α-βg+; X ≥ β
where
=g+ — g-	b =	αg+ + Bg-	= t a2(g+ + g-") + 2αβg-
a =	2(α + β),	b = α + β ,	C =+	2(α + β)
The fifth pair of graphs in Fig. 1 (also Fig. 12) shows examples of generalized SmeLU with t = 0
and asymmetric transition regions, including leaky versions with g- > 0 and also versions with
g- ≤ 0. SmeLU is a special case of the generalized version, with g- = 0, g+ = 1, α = β, and
t = 0. Some additional special cases are reviewed in Appendix D.
(11)
(12)
The hyper-parameters of generalized SmeLU can be learned in training of a deep network together
with the model parameters, either for the whole model, per layer, and even per neuron, as noted
for SmeLU and discussed in Appendix B. Experiments demonstrate that for some datasets, layers
closer to inputs may learn a negative g- , implying that they learn some sparsification of the input,
for better model accuracy. Layers closer to the output learned, in similar cases, slopes closer to 0.
Further Generalization - RESCU: The general idea to derive the generalized SmeLU can be ex-
tended in various ways. Generally, we define REctified Smooth Continuous Unit (RESCU) as any
function that consists of multiple smooth pieces, and is continuously differentiable. At any point,
α, y(α-) = y(α+), as well as (dy/dx)|a_ = (dy∕dx)∣α+. We can extend the generalized SmeLU
by additional linear pieces and quadratic pieces connecting between any pair of linear pieces with
different slopes. For example, the non-monotonicity of Swish (and alike) can be achieved by adding
a concave piece to the left of the convex quadratic piece. At the point the gradient becomes 0 it can
be joined by a linear piece to its left with 0 gradient.
Alternatively, the same concept can be used by joining pieces of more complex mathematical func-
tions, ensuring smoothness at the connection points between the pieces. This makes activations like
Swish special cases of RESCU with a single piece. SELU with β = 1 is another example with two
pieces. Another example is the REctified Sigmoid Continuous Unit, which consists of a Sigmoid on
the left, but linear on the right
ySig-RESCU =f2β N Y) ； x ≤ β
X; X ≥ β
(13)
We can apply the concept with polynomials of higher degree as well.
5	Experiments
Criteo: We evaluated SmeLU and Swish on the benchmark Criteo Display Advertising Challenge
dataset1. The dataset provides a binary classification task with 45M examples, which are split to
37M training examples, and validation and test sets of 4M examples each. Each example has
13 integer features and 26 categorical features. Models have 3 fully connected hidden layers of
dimension 2572, 1454, 1596, respectively. Each categorical feature Xk is hashed into Nk buckets.
If Nk < 110 the hash bin is encoded as a one-hot vector, otherwise it is embedded as a vector
of dimension dk. Values of Nk and dk are taken from Ovadia et al. (2019) (see Appendix E). Each
model is trained for one epoch using the Adam optimizer. Unlike Ovadia et al. (2019), input features
are not fed into a batch-norm layer. Integer features are log-square-transformed. We trained models
with ReLU, SmeLU and Swish activations. Each experiment consisted of 40 independent runs. For
each run, models are initialized to different random values, and a different random shuffle is applied
to the data. Training mini-batches of 1024 examples were used. Table 1 shows AUC and PDs; ∆1
1Criteo data in https://www.kaggle.com/c/criteo-display-ad-challenge
7
Under review as a conference paper at ICLR 2021
Table 1: Smooth activations on Criteo: AUC, and PDs; ∆1, ∆r1.
Model	AUC	AUC Stdev	∆1	∆1 stdev	∆r1 %	∆r1 stdev %
ReLU	0.781	0.0144	0.053	0.033	36.3	19.2
SmeLU β = 1	0.783	0.0116	0.044	0.032	30.5	17.8
β=1.5	0.785	0.0095	0.037	0.028	28.6	15.0
β=2	0.786	0.0068	0.032	0.021	25.2	11.8
β=2.5	0.787	0.0012	0.029	0.004	22.5	4.5
β=3	0.787	0.0012	0.029	0.005	23.5	5.7
β=4	0.787	0.0012	0.029	0.004	22.8	4.9
Swish β = 0.25	0.765	0.0208	0.088	0.006	45.1	29.4
β=0.5	0.774	0.0197	0.078	0.025	45.3	25.1
β=1	0.766	0.0211	0.087	0.009	45.3	26.5
β=2	0.757	0.0208	0.078	0.028	40.0	29.1
β=4	0.760	0.0170	0.076	0.022	39.5	27.1
(absolute) and ∆r1 (relative), on the test data. ReLU exhibits huge PDs of over 36%. While only
slightly improving AUC, around 40% decrease in both PD measurements as well as substantial order
of magnitude reductions in AUC and PD standard deviations, are observed with SmeLU. Tables 2
and 3 in Appendix E show that similar less extreme improvements are attained when training data
is not shuffled, or models are identically initialized. Baseline ReLU PDs are lower but still over
20%. SmeLU still improves PD (about 12% without shuffling) and AUC/PD standard deviations
over ReLU. With Swish, on this dataset, we were unable to obtain comparable AUC and PD values.
Figure 3: Relative PD; ∆r1, expressed in [%] of the positive label prediction as function of loss
change [%] from a ReLU baseline for different activations on real validation data. The X-axis is:
left: logarithmic cross entorpy loss, right: ranking loss.
Rank Loss Change % vs. ReLU
Real Data: We tested various smooth activations on a large private dataset for ad Click-Through-
Rate (CTR) prediction for sponsored advertisement. We constructed a simple 6 layer fully connected
network with dimensions [1024, 512, 256, 128, 64, 16] with 5 informative features that are used as
learned embedding vectors of different dimensions, providing a total of 240 inputs. Models are
identically initialized, trained with data-shuffling on hundreds of billions of examples, using online
mini-batch, single pass over the data, optimizing with the Adagrad optimizer (Duchi et al., 2011).
Resources limit the quantity of models trained, so we use M = 2. Inference is applied to examples
prior to training on them, and progressive validation (Blum et al., 1999), as commonly used for
this problem (McMahan et al., 2013), is applied to measure validation logarithmic and ranking
losses on predictions. Fig. 3 reports relative PD; ∆r1, (as defined for binary labels) as function of
both average validation logarithmic (cross-entropy) loss (left) and ranking loss (right). Losses are
expressed as percentage of the baseline ReLU model. Relative PD is expressed as percentage of the
actual positive label prediction. On the graph, moving left implies better accuracy, and moving down
8
Under review as a conference paper at ICLR 2021
implies better reproducibility. For smooth activations, the different points are the result of different
β . Moving down at least before reaching an optimum implies larger β for SmeLU and smaller β for
the other activations. In such a large scale system, we observe major relative PD values of over 10%
with ReLU (12% when models initialized equally, and 13% when initialized differently). Smooth
activations are substantially better on PD and better on the tradeoffs than ReLU. Among themselves,
the tradeoffs are more comparable, where SmeLU, GELU and Swish appear to be slightly better on
this data than Mish and Tanhexp. SoftPlus appears to push towards better PD, but at the expense
of accuracy with unacceptable trade-offs (in such a system even a fraction of percent can make
a difference). Continuously differential SELU appears inferior in accuracy and PD. SmeLU also
seems to be stronger on PD, whereas Swish on accuracy. Results shown use weight normalization
with norm v = 1 without clipping. We observe similar relative behavior without normalization (no
clipping). However, PD’s of smooth activations increase, but are still better than the ReLU baseline.
Figure 4: PD ∆1 as function of error rate on MNIST dataset for different activations with different
β parameters. Left: with Adagrad optimizer, right: with an SGD optimizer.
Benchmark MNIST Dataset: We tested PD and accuracy on the MNIST dataset (LeCun, 1998).
Models were implemented with Tensorflow Keras. Networks were simple 2 layer networks,
with layers of width 1200. We regularized with dropout (input 0.2, layers 0.5) and image aug-
mentation in training (starting from epoch 2, two dimensional independent random shifts by
{-3, -2, -1, 0, 1, 2, 3} pixels take place with probability 0.5.) Hidden weights were identically
initialized with the default Keras uniform random, and biases to 0. For AdaGrad, we used learning
rate 0.02, accumulator initialization 0.1, and 150 epochs without weight normalization. For SGD,
we used learning rate 0.01, momentum 0.9, and 50 epochs. Batch size is 32. For each experiment,
we trained M = 12 models. Fig. 4 shows PD ∆1 as function of error rate for both configura-
tions, AdaGrad (left) and SGD (right). Graphs for other PD metrics, with similar behavior, are
in Appendix F. PD values may not be as large as they are in large scale data, but we still see PD
change substantially relative to its lowest values (over 10% for AdaGrad, over 30% for SGD). As
in large scale, ReLU’s PD is much worse than those achievable by smooth activations, and among
themselves, smooth activations are comparable. With SGD, however, Swish appears inferior on PD.
6	Conclusions
We studied irreproducibility in deep networks, showing that the popular ReLU activation is a major
factor in exacerbating prediction differences. Smooth activations allow substantial improvement,
giving better accuracy-reproducibility tradeoffs. We introduced SmeLU, and its generalizations,
specifically demonstrating that superior accuracy-reproducibility tradeoffs are achievable even with
very simple inexpensive to implement activations, whose performance on both accuracy and PD
do not fall from and can be better than those of more complex expensive to implement, and non-
monotonic, smooth activations. We empircially demonstrated these tradeoffs, showing that proper
smooth activations are superior to ReLU. For a specific dataset, model architecture, and model
configuration, one smooth activation may be better than the other. We have shown, however, that
the much simpler SmeLU can be comparable or superior to the more complex activations.
9
Under review as a conference paper at ICLR 2021
References
Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep neural
networks. arXiv preprint arXiv:1711.08856, 2017.
Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geoffrey E
Hinton. Large scale distributed neural network training through online distillation. arXiv preprint
arXiv:1804.03235, 2018.
Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
Jonathan T Barron. Continuously differentiable exponential linear units. arXiv, pp. arXiv-1704,
2017.
Yoshua Bengio, Jerome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In
Proceedings ofthe 26th annual international conference on machine learning, pp. 41T8, 2009.
Avrim Blum, Adam Kalai, and John Langford. Beating the hold-out: Bounds for k-fold and progres-
sive cross-validation. In Proceedings of the twelfth annual conference on Computational learning
theory,pp. 203-208, 1999.
Guy Bresler and Dheeraj Nagaraj. A corrective view of neural networks: Representation, memo-
rization and learning. In Jacob Abernethy and Shivani Agarwal (eds.), COLT 2020, volume 125
of Proceedings of Machine Learning Research, pp. 848-901. PMLR, 09-12 Jul 2020.
Zhe Chen, Yuyan Wang, Dong Lin, Derek Cheng, Lichan Hong, Ed Chi, and Claire Cui. Beyond
point estimate: Inferring ensemble prediction variation from neuron activation strength in recom-
mender systems. arXiv preprint arXiv:2008.07032, 2020.
Zhi Chen and Pin-han Ho. Deep global-connected net with the generalized multi-piecewise relu
activation in deep learning. arXiv preprint arXiv:1807.03116, 2018.
DjOrk-Ame Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network
learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.
T. G. Dietterich. Ensemble methods in machine learning. Lecture Notes in Computer Science, pp.
1-15, 2000.
Simon Du. Gradient Descent for Non-convex Problems in Modern Machine Learning. PhD thesis,
Carnegie Mellon University, 2019.
J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic
optimization. Journal of Machine Learning Research, 12:2121-2159, Feb. 2011.
Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas Kemp, Jeremy Nixon, Ghassen Jerfel,
Katherine Heller, and Andrew M Dai. Analyzing the role of model uncertainty for electronic
health records. In Proceedings of the ACM Conference on Health, Inference, and Learning, pp.
204-213, 2020.
Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint
arXiv:1606.08415, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pp. 492-
518. Springer, 1992.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by
reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015.
Xiaojie Jin, Chunyan Xu, Jiashi Feng, Yunchao Wei, Junjun Xiong, and Shuicheng Yan. Deep
learning with s-shaped rectified linear activation units. arXiv preprint arXiv:1512.07030, 2015.
10
Under review as a conference paper at ICLR 2021
Gunter Klambauer, Thomas Unterthiner, Andreas Mayr, and SePP Hochreiter. Self-normalizing
neural networks. In Advances in neural information processing Systems, pp. 971-980, 2017.
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. SimPle and scalable Predic-
tive uncertainty estimation using deep ensembles. In Advances in neural information processing
systems, pp. 6402-6413, 2017.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Dong Lin and Gil I. Shamir. Private communication to Quoc Le. Unpublished, Aug. 2019.
Hanxiao Liu, Andrew Brock, Karen Simonyan, and Quoc V Le. Evolving normalization-activation
layers. arXiv preprint arXiv:2004.02967, 2020.
Xinyu Liu and Xiaoguang Di. Tanhexp: A smooth activation function with high convergence speed
for lightweight neural networks. arXiv preprint arXiv:2003.09855, 2020.
Vishnu Suresh Lokhande, Songwong Tasneeyapant, Abhay Venkatesh, Sathya N Ravi, and Vikas
Singh. Generating accurate pseudo-labels in semi-supervised learning and avoiding overconfident
predictions via hermite polynomial activations. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 11435-11443, 2020.
H Brendan McMahan, Gary Holt, David Sculley, Michael Young, Dietmar Ebner, Julian Grady, Lan
Nie, Todd Phillips, Eugene Davydov, Daniel Golovin, et al. Ad click prediction: a view from
the trenches. In Proceedings of the 19th ACM SIGKDD international conference on Knowledge
discovery and data mining, pp. 1222-1230, 2013.
HN Mhaskar. On smooth activation functions. In Mathematics of Neural Networks, pp. 275-279.
Springer, 1997.
Diganta Misra. Mish: A self regularized non-monotonic neural activation function. arXiv preprint
arXiv:1908.08681, 2019.
Guido F Montufar, Razvan Pascanu, Kyunghyun Cho, and Yoshua Bengio. On the number of linear
regions of deep neural networks. In Advances in neural information processing systems, pp.
2924-2932, 2014.
Prabhat Nagarajan, Garrett Warnell, and Peter Stone. Deterministic implementations for repro-
ducibility in deep reinforcement learning. arXiv preprint arXiv:1809.05676, 2018.
Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines.
In ICML, 2010.
Chigozie Nwankpa, Winifred Ijomah, Anthony Gachagan, and Stephen Marshall. Activation
functions: Comparison of trends in practice and research for deep learning. arXiv preprint
arXiv:1811.03378, 2018.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Process-
ing Systems, pp. 13991-14002, 2019.
Dabal Pedamonti. Comparison of non-linear activation functions for deep neural networks on mnist
classification task. arXiv preprint arXiv:1804.02763, 2018.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv
preprint arXiv:1710.05941, 2017.
Flora Sakketou and Nicholas Ampazis. On the invariance of the selu activation function on algorithm
and hyperparameter selection in neural network recommenders. In IFIP International Conference
on Artificial Intelligence Applications and Innovations, pp. 673-685. Springer, 2019.
Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate
training of deep neural networks. In Advances in neural information processing systems, pp. 901-
909, 2016.
11
Under review as a conference paper at ICLR 2021
Gil I Shamir and Lorenzo Coviello. Anti-distillation: Improving reproducibility of deep networks.
Preprint, 2020.
Tianyang Wang, Zhengrui Qin, and Michelle Zhu. An elu network with total variation for image
denoising. In International Conference on Neural Information Processing, pp. 227-237. Springer,
2017.
Cihang Xie, Mingxing Tan, Boqing Gong, Alan Yuille, and Quoc V Le. Smooth adversarial training.
arXiv preprint arXiv:2006.14536, 2020.
Ying Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4320-
4328, 2018.
Hao Zheng, Zhanlei Yang, Wenju Liu, Jizhong Liang, and Yanpeng Li. Improving deep neural net-
works using softplus units. In 2015 International Joint Conference on Neural Networks (IJCNN),
pp.1U.IEEE, 2015.
A Smooth Activations and Normalization Induced S urfaces
In this appendix, we demonstrate effects of smooth activations and normalization on the objective
surfaces, enhancing on the description in Sections 3-4, and on the illustrations of Fig. 2. Fig. 5 ex-
pands on Fig. 2, showing objective surfaces for different activations, parameters and normalizations.
It shows the transition from a non-smooth ReLU to a linear activation through different parameters
of SmeLU. As we increase β, the objective surface becomes smoother. It also demonstrates the
importance of normalization, especially when activations are clipped to some cap. The first row
duplicates that of Fig. 2 for completeness. The second row shows the transition of a weight normal-
ized clipped activation from ReLU to linear through widening SmeLUs. With weight normalization
there are slight deviations from the top row, but the behavior appears rather similar. The third row
shows the same transition without weight normalization and clipping. In order to be as smooth, a
larger β must be used than in the first row. This result is in line with the experiments reported in
Section 5 on real data, where PD values increased in all cases with the same β parameters if weight
normalization was not applied. Furthermore, the objective variance is now much larger than in the
first row. When clipping is added, as shown in the last row, the effect of the smoothness disappears,
where numerous peaks appear in all activations. With large β, the effectiveness of the activation
altogether also appears to diminish. This happens even with the linear activation. As observed in the
second row, weight normalization diminishes the effect of clipping.
Fig. 6	shows surfaces for Swish, SoftPlus, GELU and SELU with weight normalization, without
clipping. For this simple two dimensional input example, the smooth activations appear rather sim-
ilar to the surfaces of SmeLU. As mentioned earlier, the smoothness is reversed as a function of
β from SmeLU, with the larger β values being less smooth. Since GELU can be approximated by
Swish with a larger parameter, for the same β, Swish is smoother. Very similar surfaces are observed
for Mish and Tanhexp. SELU is smooth for β = 1, but is clearly not smooth for other values of β.
This is observed in Fig. 6 for β = 10.
Fig. 7	demonstrates the effect of normalization (weight and layer) onno activation and on ReLU, and
the effects of layer normalization (Ba et al., 2016) on SmeLU. Unlike weight normalization, layer
normalization changes the shape of the surface from the shape obtained when there is no normaliza-
tion. While the type of norm changes the shape of the surface obtained, unlike smooth activations,
the norm value v has no effect on the shape of the surface observed for either no activation or ReLU.
(It does affect the magnitude of the objective, though.) For smooth activations, on the other hand, as
observed in Fig. 2, the magnitude v of the norm does affect the smoothness of the surface also with
layer normalization, as demonstrated for SmeLU on the bottom row, with smoother objective as the
norm is smaller.
B	Learning SmeLU Parameters
As we observed, β in smooth activations roughly serves as a tuning knob between accuracy and
reproducibility. However, the optimal accuracy is attained away from the end point (0 for SmeLU,
12
Under review as a conference paper at ICLR 2021
Figure 5: Three dimensional surfaces showing network outputs as function of 2-dimensional inputs
in [-6, 6], with 5 hidden layers of dimensions [256, 128, 64, 32, 16] with no activation (linear), and
activated by ReLU, and SmeLU with different β parameters. Matrices of all hidden layers are
equal for all figures and are randomly drawn from a standard normal N (0, 1) distribution. Top:
ReLU, SmeLU different β and linear, no clipping of pre-activation values with L2 norm 1 weight
normalization. Second Row: weight normalization with activation clipping to [-6, 6]. Third Row:
no clipping, no weight normalization. Bottom Row: clipping, no weight normalization.
Figure 6: Three dimensional surfaces with identical configuration to Fig. 5 for Swish, Softplus,
GELU, and SELU, with different β parameters, all with weight normalization and no clipping. Top:
Swish and Softplus. Bottom: GELU and SELU.
13
Under review as a conference paper at ICLR 2021
Figure 7: Three dimensional surfaces with identical configuration to Fig. 5 for no activation, ReLU
and Smelu (β = 0.5) with weight and layer normalizations, with no clipping. Top: Linear with
different weight and layer norms. Middle: ReLU with different weight and layer norms. Bottom:
SmeLU with different layer norms.
∞ for Swish and other smooth activations). Similarly, there are no guarantees that the wider is the
activation the more reproducible it is, at the other extreme. It is expected that the optimal points will
be properties of both the actual dataset and the model architecture. For the generalized SmeLU and
wider RESCU generalization, more parameters should be tuned. While tuning can optimize various
tasks, we focus here on tuning for accuracy and reproducibility.
The parameter β or the parameters of the generalized SmeLU (α, β, g-, g+, t) can be trained and
learned as part of the training process of a deep network. Learning can optimize accuracy, repro-
ducibility or any other objective. Training can learn a single parameter set for the whole network, a
single parameter set per layer, or a single parameter set per neuron. Depending on the task and num-
ber of updates, learning rates can be tuned for each of the learned hyper-parameters. Initialization
can initialize to known good parameters. For the generalized SmeLU, we can initialize values to
those of a reasonable SmeLU, e.g., with g- = 0, g+ = 1, t = 0, and α = β = 0.5. Empirically, we
observe, when optimizing for accuracy, that each layer learns a different set of parameters. Specifi-
cally, we observed that layers closer to the input tend to learn a negative g-. The first hidden layer
that is connected to the input (especially in systems where the input consists of learned embeddings)
feeds input that is usually more symmetric around the origin, while layers farther from the input see
biased inputs. With the learned g-, it appears that the layer closest to the input attempts to learn
sparsification, forcing its inputs either to an extreme or to the origin.
Learning hyper-parameters for accuracy can be done simultaneously with training the parameters of
the networks. Alternatively, this can be done in one training session. Then, the hyper-parameters
can be fixed, and used for training the parameters. The procedure can be further applied to learning
functional forms of the activation as done by Ramachandran et al. (2017). One can apply such a
methodology on a RESCU activation, where the pieces are learned under the constraints of contin-
uous function and gradient between the pieces.
Optimization of the parameters can combine an objective for both accuracy and reproducibility, as a
mean or weighted sum of both objectives. However, this must be done carefully, as these objectives
may be conflicting. Optimizing the activation for reproducibility may be more involved, as gradient
methods with the objective loss used for training neural networks tend to try to improve the model’s
accuracy. There are several options for training the activation parameters to improve reproducibility.
We can include reproducibility optimization of the parameters in training the full network, or train
14
Under review as a conference paper at ICLR 2021
the model offline while learning the parameters that are better for reproducibility first, and then re-
peat training to learn the model itself using these parameters. In either case, ensembles can be used as
a proxy for reproducibility, where we impose a loss that minimizes the prediction difference (either
in probability or on log-odds score) between components of the ensemble. This approach essentially
applies co-distillation (Anil et al., 2018) but for the purpose of learning the hyper-parameters that
improve reproducibility instead of training the actual model. Co-trained ensemble may not mimic
the full effect causing divergence of models as they may have less randomness in the order of exam-
ples they see and updates they apply, but empirical results show that even with that components of
ensemble tend to diverge from one another, even when initialized identically, and especially when
initialized differently. Unlike co-distillation, to learn the hyper-parameters for better reproducibility,
the objective loss in training the ensemble is applied only on the parameters, but the similarity loss
is applied only on the hyper-parameters.
There are disadvantages to learning the hyper-parameters while training. This is because of po-
tentially conflicting objectives to accuracy and reproducibility. Furthermore, in many cases, the
preferred deployed model should not be an ensemble. We can use an ensemble to optimize the
hyper-parameters for reproducibility offline, and then train and deploy a model that retrains with the
reproducibility optimized activation hyper-parameters.
If we deploy an ensemble, training with identically initialized components may be valuable for opti-
mizing activation hyper-parameters for better reproducibility, but will be suboptimal for deployment,
at least for better reproducibility. As shown by Shamir & Coviello (2020), ensembles are advanta-
geous for reproducibility by leveraging and encouraging the differences between their components,
so that as a whole, the ensemble captures the objective space better. Initializing components iden-
tically for optimizing the hyper-parameters defeats this idea, and thus it is more desirable to apply
such initialization only when learning the activation hyper-parameters, but then retrain the models
with these parameters with different initializations of the components, for better deployed repro-
ducibility.
C Graphs of Different Smooth Activations
In this appendix, we provide activation function and gradient curves for the different (smooth) acti-
vations discussed in this paper as function of their parameter. This set of curves completes the ones
shown in Fig. 1. Fig 8 shows curves for the different activations with β = 1. Figs. 9, 10, 11 and
12 show SELU and CELU, SoftPlus, Swish, GELU, Mish, Tanhexp, SmeLU, generalized SmeLU,
and Sigmoid-RESCU, respectively. SELU is shown to be smooth only for β = 1. The equations for
CELU are given by
x,	if x ≥ 0
yCELU = β β •卜XP (X) — 1), if x < 0.
(14)
We observe the close similarity between Swish, GELU, Mish, and Tanhexp, and the relation between
Sigmoid-RESCU and SmeLU. Subsequently to our work, and inspired by SmeLU, (Lin & Shamir,
2019), a different SmoothRelu activation was given in Xie et al. (2020),
ySmoothRelu
if x ≥ 0,
if x < 0.
(15)
This activation is also a RESCU activation. Graphs for this activation are included in Fig. 12. As
observed, this activation does not achieve full slope 1, unless β 1, in which case, it already
approaches ReLU. Thus it may lack the smoothness tuning flexibilities of SmeLU.
D Generalized S meLU - Reformulations and Some Special Cases
In this appendix we show some special cases of the generalized SmeLU defined in (11), and also
address the asymptotic relation between SmeLU and SoftPlus.
D.1 Asymmetric SmeLU
SmeLU is a special case of the generalized SmeLU. The definition of the generalized SmeLU in
(11) can be used to obtain a version of SmeLU that is asymmetric around the origin. Like SmeLU,
15
Under review as a conference paper at ICLR 2021
Figure 8: Different activations and their gradients for β = 1.
Figure 9: SELU (top) and CELU (bottom) activations and gradients with different β parameter
values.
16
Under review as a conference paper at ICLR 2021
Figure 10: SoftPlus activations and gradients with different β parameter values.
this activation can guarantee 0 output on the left, and slope 1 on the right, but a transition region in
[-α, β], where α 6= β,
yAsymmetric-SmeLU =
0;
(x+α)2 .
x+α+β);
X +	2 ;
x ≤ -α
-α ≤ x ≤ β
x ≥ β.
(16)
Setting α = β gives SmeLU.
D.2 Leaky SmeLU
Setting g- > 0, but g+ = 1 > g-, α = β and t = 0, gives the SmeLU version of a Leaky ReLU,
referred to as Leaky SmeLU, and given by
g- ∙(X + β);	X ≤ -β
yLeaky-SmeLU =	-g- ∙ x2 + ++-~ ∙ x + 4 (I + 3g-)；	|x| ≤ β	(17)
[X + g- ∙ β；	X ≥ β.
Apart for the middle region, this activation resembles Leaky ReLU.
D.3 Shifted SmeLU
The constraint used to generate the generalized SmeLU constrains the g- slope region to end at the
point X = -α, y = t. Constraints can be specified differently, where this region ends at the origin,
and then shifted horizontally and/or vertically. Vertical shift is given by t, whereas we can define the
activation z(X) = y(X - s) for a horizontal shift by s.
D.4 Origin Crossing SmeLU
SmeLU as defined in (9) does not cross the origin. We can similarly define a version of generalized
SmeLU that is constrained to cross the origin instead of a point (-α, t). We can apply the general-
ized SmeLU with the desired α, β and gradients, and then shift it vertically to cross the origin (or
we can solve directly with the new constraints). A zero crossing version keeps the sign of the input
on one hand, but does not suppress negative values to 0. The form is given by
yzero-cross-SmeLU
g- ∙ X +
g+ -g- x2 +
2(α+β)X +
g+ ∙ X +
α2 (g--g+).
2(α+β)	;
αg+ +βg-
α+β	X;
β2 (g--g+).
2(α+β);
X ≤ -α
-α ≤ X ≤ β
X ≥ β.
(18)
17
Under review as a conference paper at ICLR 2021
Figure 11: Swish, GELU, Mish, and TanhExp activations and gradients with different β parameter
values.
18
Under review as a conference paper at ICLR 2021
Figure 12: SmeLU, Generalized SmeLU, Sigmoid-RESCU, and SmoothRelu activations and gradi-
ents with different α, β and gradient parameter values.
19
Under review as a conference paper at ICLR 2021
D.5 Asymptotic Relation of SmeLU and SoftPlus
The gradient of the SmeLU function is a hard Sigmoid. This already shows a connection to SoftPlus,
whose gradient is a Sigmoid. To further demonstrate the relation between SmeLU and SoftPlus, we
express SoftPlus using a reciprocal parameter γ to the original definition in (3),
ysoftPius = Yln(1 + exp(x∕γ)).	(19)
Asymptotically, for x → -∞, we have y → 0. For x	0, y ≈ x. For |x|	1, we can approximate
the SoftPlus using Taylor series expansion by
ysoftpius ≈ γ ln2 + 7τ + z-.	QO)
2	8γ
Assigning the SmeLU parameter β = 2γ, we obtain for |x|	1,
ySoftPlUs(X) - ySmeLU(X) ≈ Y ln2 - = = g(ln2 -A ) = gln - ∙	(21)
4	2	2	4e
Thus the middle region around X = 0 of SoftPlus is a vertical shift by β∕4 ln(4∕e) of the middle
region of SmeLU with a factor of 2 wider parameter, but the extremes on both sides meet those of
SmeLU. Therefore, SoftPlus opens wider, away from the origin, making it even smoother for the
same parameter, on one hand, but also potentially less accurate on the other, due to the distance of
the middle region from the origin.
E Experiments on the Criteo Dataset
We describe the set up of the experiments on the Criteo benchmark dataset and provide additional
results.
Model architecture: Models have 3 fully connected hidden layers of dimension 2572, 1454, 1596
respectively. Each categorical feature Xk is hashed into Nk buckets. If Nk < 110 the hash bin is
encoded as a one-hot vector, otherwise it is embedded as a vector of dimension dk. Values of Nk
and dk are taken from Ovadia et al. (2019) and are the following (where dk = 0 means that the hash
bin is encoded as a one-hot vector):
Nk = [1373, 2148, 4847, 9781, 396, 28, 3591, 2798, 14, 7403, 2511, 5598, 9501,
46, 4753, 4056, 23, 3828, 5856, 12, 4226, 23, 61, 3098, 494, 5087]	(22)
dk = [3,9,29,11,17,0,14,4,0,12,19,24,29,0,13,25,0,8,29,0,22,0,0,31,0,29] (23)
Unlike the set up in Ovadia et al. (2019), input features are not fed into a batch-norm layer, while
integer features are log-square-transformed.
Additional results: Table 2 shows results for ReLU, SmeLU and Swish when the training data is not
shuffled. As a consequence, all models visit the data in the same order of batches, but are initialized
differently. (Note, however, that due to randomness in the platform, within a batch, there could still
be some randomness in the order examples are being processed.) Table 3 shows results for ReLU,
SmeLU and Swish when all models have the same initialization of the hidden layer weights. In both
cases, models exhibit irreproducibility. Without shuffling, as a result of initialization (and random
effect in the platform that applies the updates). With identical initialization, irreproducibility is due
to the training data shuffling. In both case, SmeLU improves ∆1 and ∆r1 by about 12%, and exhibits
a factor of 4 reduction of the standard deviations of both AUC and ∆1.
F	Additional results on MNIST
In this appendix, we show matching graphs to those in Fig. 4 for the other PD metrics; ∆2, ∆1L, and
∆H, in Figs 13, 14, and 15, respectively. The behavior of the different activations shown in these
figures for all the metrics is very similar to the behavior w.r.t. ∆1.
20
Under review as a conference paper at ICLR 2021
Table 2: Smooth activationS on Criteo: AUC, ∆1, ∆r1. Training data iS not Shuffled.
AUC	∆ι	∆ι ∆	∆
Model	AUC Stdev	Stdev % Stdev
ReLU
SmeLU
Swish
11223400124
===========
βββββββββββ
0.786	0.0068	0.028	0.014	21.5	8.3
0.787	0.0008	0.025	0.003	19.3	3.4
0.787	0.0007	0.025	0.003	19.0	2.9
0.787	0.0011	0.025	0.004	19.1	3.7
0.786	0.0011	0.025	0.003	19.2	2.8
0.787	0.0011	0.025	0.003	18.4	2.8
0.787	0.0007	0.025	0.003	20.5	4.6
0.769	0.0203	0.087	0.018	47.2	28.3
0.765	0.0216	0.087	0.006	44.3	32.8
0.768	0.0215	0.085	0.012	46.2	30.1
0.756	0.0180	0.070	0.030	37.4	29.9
0.764	0.0180	0.082	0.010	46.6	23.1
Table 3: Smooth activations on Criteo: AUC, ∆1, ∆r1. Models are identically initialized.
AUC ∆ι	∆ι	∆	∆
Model	AUC Stdev	Stdev % Stdev
ReLU
SmeLU
Swish
11223400124
===========
βββββββββββ
0.787	0.0023	0.031	0.007	25.3	7.2
0.787	0.0004	0.028	0.004	22.2	5.0
0.787	0.0004	0.029	0.004	23.5	5.8
0.787	0.0003	0.029	0.004	23.6	6.4
0.787	0.0005	0.028	0.004	22.5	5.6
0.787	0.0004	0.027	0.004	22.4	5.6
0.787	0.0003	0.027	0.004	22.4	5.7
0.767	0.0210	0.088	0.012	47.5	28.3
0.770	0.0207	0.083	0.015	45.9	27.5
0.769	0.0204	0.084	0.013	46.7	26.1
0.763	0.0204	0.085	0.012	43.4	27.9
0.762	0.0181	0.083	0.014	45.3	24.3
Error Rate %
Figure 13: PD ∆2 aS function of error rate on MNIST dataSet for different activationS with different
β parameterS. Left: with Adagrad optimizer, right: with an SGD optimizer.
21
Under review as a conference paper at ICLR 2021
<①ouəjət:ɑUO-Ip一 P ①」d
0.0072
0.0070
0.0068
0.0066
0.0064
0.0062
0.0060
0.0058
0.8	0.9	1.0	1.1	1.2
Error Rate %
• ReLU
SmeLU
Swish
1.0	1.2	1.4	1.6	1.8
Error Rate %
Figure 14: PD ∆1L as function of error rate on MNIST dataset for different activations with different
β parameters. Left: with Adagrad optimizer, right: with an SGD optimizer.
HV ①。U ①」①⅛:QUO-WPSd
0.0028
0.8	0.9	1.0	1.1	1.2
Error Rate %
1.0	1.2	1.4	1.6	1.8
Error Rate %
Figure 15: PD ∆H as function of error rate on MNIST dataset for different activations with different
β parameters. Left: with Adagrad optimizer, right: with an SGD optimizer.
22