Under review as a conference paper at ICLR 2021
Deep Learning is Composite Kernel Learning
Anonymous authors
Paper under double-blind review
Abstract
Recent works have connected deep learning and kernel methods. In this paper,
we show that architectural choices such as convolutional layers with pooling, skip
connections, make deep learning a composite kernel learning method, where the
kernel is a (architecture dependent) composition of base kernels: even before
training, standard deep networks have in-built structural properties that ensure their
success. In particular, we build on the recently developed ‘neural path’ framework1
that characterises the role of gates/masks in fully connected deep networks with
ReLU activations.
1	Introduction
The success of deep learning is attributed to feature learning. The conventional view is that feature
learning happens in the hidden layers of a deep network: in the initial layers simple low level features
are learnt, and sophisticated high level features are learnt as one proceeds in depth. In this viewpoint,
the penultimate layer output is the final hidden feature and the final layer learns a linear model with
these hidden features. While this interpretation of feature learning is intuitive, beyond the first couple
of layers it is hard make any meaningful interpretation of what happens in the intermediate layers.
Recent works Jacot et al. (2018); Arora et al. (2019); Cao and Gu (2019) have provided a kernel
learning interpretation for deep learning by showing that in the limit of infinite width deep learning
becomes kernel learning. These works are based on neural tangents, wherein, the gradient of the
network output with respect to the network parameters known as the neural tangent features (NTFs)
are considered as the features. Arora et al. (2019) show that at randomised initialisation of weights,
the kernel matrix associated with the NTFs, known as the neural tangent kernel (NTK) converges to a
deterministic matrix and that optimisation and generalisation of infinite width deep neural networks is
characterised by this deterministic kernel matrix. Cao and Gu (2019) provided generalisation bounds
in terms of the NTK matrix. Arora et al. (2019) also proposed a pure-kernel method based on CNTK
(NTK of convolutional neural networks, i.e., CNNs) which significantly outperformed the previous
state-of-the-art kernel methods. The NTK either as an interpretation or as a method in itself has been
very successful. Nevertheless it has some open issues namely i) non-interpretable: the kernel is
the inner product of gradients and has no physical interpretation, ii) no feature learning: the NTFs
are random and fixed during training and iii) performance gap: finite width CNN outperforms the
infinite width CNTK, i.e., NTK does not fully explain the success of deep learning.
Recently, Lakshminarayanan and Singh (2020) developed a neural path (NP) framework to provide a
kernel interpretation for deep learning that addresses the open issues in the current NTK framework.
Here, DNNs with ReLU activations are considered, and the gates (on/off state of ReLU) are encoded
in the so called neural path feature (NPF) and the weights in the network in the so called neural path
value (NPV). The key findings can be broken into the following steps.
Step 1: The NPFs and NPV are decoupled. Gates are treated as masks, which are held in a separate
feature network and applied to the main network called the value network. This enables one to study
the various kinds of gates (i.e., NPFs), such as random gates (of a randomly initialised network),
semi-learnt gates (sampled at an intermediate epoch during training), and learnt gates (sampled from
a fully trained network). This addresses the feature learning issue.
Step 2: When the gates/masks are decoupled and applied externally it follows that NTK = const ×
NPK, at random initialisation of weights. For a pair of input examples, NPK is a similarity measure
1Introduced for the first time in the work of Lakshminarayanan and Singh (2020).
1
Under review as a conference paper at ICLR 2021
that depends on the size of the sub-network formed by the gates that are active simultaneously for
examples. This addresses the interpretability issue.
Step 3: CNTK performs better than random gates/masks and gates/masks from fully trained networks
perform better than CNTK. This explains the performance gap between CNN and CNTK. It was
also observed (on standard datasets) that when learnt gates/masks are used, the weights of the value
network can be reset and re-trained from scratch without significant loss of performance.
1.1	Contributions in this work
We attribute the success of deep learning to the following two key ingredients: (i) a composite kernel
with gates as fundamental building blocks and (ii) allowing the gates to learn/adapt during training.
Formally, we extend the NP framework of Lakshminarayanan and Singh (2020) as explained below.
• Composite Kernel: The NPK matrix has a composite structure (architecture dependent).
1.	Fully-Connected networks: Hfc is the Hadamard product of the input data Gram matrix,
and the kernel matrices corresponding to the binary gating features of the individual layers.
2.	Residual networks (ResNets) with skip connections: Hres assumes a sum of products form.
In particular, consider a ResNet with (b + 2) blocks and b skip connections. Within this
ResNet there are i =1,...,2b possible dense networks, and then Hres = Pi2b CiHifc,
where Ci > 0 are positive constants based on normalisation layers.
3.	Convolutional neural networks (CNN) with pooling: Hcnn is rotation invariant.
• Gate Learning: We show that learnt gates perform better than random gates. Starting with the
setup of Lakshminarayanan and Singh (2020), we build combinatorially many models by,
1.	permuting the order of the layers when we apply them as external masks,
2.	having two types of modes based on input provided to the value network namely i) ‘standard’:
input is the actual image and ii) ‘all-ones’: input is a tensor with all entries as ‘1’.
We observe in our experiments that the performance is robust to such combinatorial variations.
Message: This work along with that of Lakshminarayanan and Singh (2020) provides a paradigm
shift in understanding deep learning. Here, gates play a central role. Each gate is related to a hyper-
plane, and gates together form layer level binary features whose kernels are the base kernels. Laying
out these binary features depth-wise gives rise to a product of the base kernels. The skip connections
gives a ‘sum of product’ structure, and convolution with pooling gives rotation invariance.
Organisation: Section 2 describes the network architectures namely fully-connected, convolutional
and residual, which we take up for theoretical analysis. Section 3 extends the neural path framework
to CNN and ResNet. Section 4 explains the composite kernel. Section 5 connects the NTK and NPK
for CNN and ResNet. Section 6 consists of numerical experiments.
2Architectures:Fully Connected, Convolutional and Residual
In this section, we present the three architectures that we take up for theoretical analysis. These are
i) fully connected (FC or FC-DNN), ii) convolutional (CNN) and iii) residual (ResNets). In what
follows, [n] is the set {1,...,n}, and the dataset is given by (xs,ys)sn=1 ∈ Rdin × R.
Fully Connected: We consider fully connected networks with width ‘w’ and depth ‘d’.
CNN: We consider a 1-dimensional convolutional neural network with circular convolutions (see
Table 2), with dcv convolutional layers (l =1,. .., dcv), followed by a global-average/max-pooling
layer (l = dcv +1) and dfc (l = dcv +2,...,dcv + dfc +1) FC layers. The convolutional window
size is wcv < din, the number of filters per convolutional layer is w, and the width of the FC is also w.
Definition 2.1 (Circular Convolution). For x ∈ Rdin, i ∈ [din] and r ∈{0,...,din - 1}, define :
(i)	i ㊉ r = i + r, for i + r ≤ din and i ㊉ r = i + r - dn for i + r > dn
(ii)	rot(x, r)(i) = x(i ㊉ r),i ∈ [din].
2
Under review as a conference paper at ICLR 2021
Input Layer Pre-activation	Zχ,Θ (∙, O) :	qx,Θ (iout,l)	= X =	Piin Θ(iin, iout, l) ∙ Zχ,Θ (iin, I- 1)
Gating Value	:	Gx,Θ (iout, l)	=1{qχ,θ (iout,l)>0}
Hidden Unit Output	:	zx,Θ (iout,l)	=	qx,Θ (iout, l) ∙ Gx,Θ (iout, l)
Final Output	0θ (χ)	=	Eiin Θ(iin, iout ,d) ∙ Zχ,Θ (iin ,d - 1)
Table 1: Information flow in a FC-DNN with ReLU. Here, ‘q’s are pre-activation inputs, ‘z’s are output of the
hidden layers, ‘G’s are the gating values. l ∈ [d - 1] is the index of the layer, iout and iin are indices of nodes in
the current and previous layer respectively.
skip1
skipb
χ e Rdm 7 Block。Ll Block]	・・山Bk⅜	BloCkb+ι% )θ(∕)
skip1 : ignored/removed	skipb : retained/connected
χ e Ran 7 Block。Ll Block]	j BIoCkb"帮不严官Bk)Ckb+1% ⅛(x)
Blocki : retained/connected	BlOCkb : ignored/removed
Figure 1: ResNet Architecture is shown in the top. Process of obtaining a sub-FC-DNN by ignoring skip
(retaining block) or retaining skip (ignoring block) is shown in the bottom.
(iii)	qχ,Θ (ifout, i out, I) = Pi Cv ,i in θ(i cv, i in, i out, I) ∙ zx,Θ (ifout ㊉ (i cv - 1), i in, l - I), where i in /i out are
the indices (taking values in [w]) of the input/output filters. icv denotes the indices of the convolutional
window (taking values in [wcv]) between input and output filters iin and iout. ifout denotes the indices
(taking values in [din], the dimension of input features) of individual nodes in a given output filter.
Definition 2.2 (Pooling). Let Gpxo,ol(ifout,iout,dcv + 1) denote the pooling mask, then we have
zx,Θ (i out, d CV + I) =>: zx,Θ (ifout, i out, d CV ) ∙ GX,Θ (ifout, i out, d CV + 1),
ifout
where in the Case of global-average-pooling Gpoθ o (ifout, i out ,d cv + 1)=看,∀i out ∈ [w],ifout ∈ [d in ],
and in the Case of max-pooling, for a given iout ∈ [w], Gpxo,ol (imax,iout,dCv + 1) = 1 where
imax = arg maxifout zx,Θ (ifout,iout,dCv), and Gx,Θ (ifout,iout,dCv + 1) = 0, ∀ifout 6= imax.
ResNet: We consider ResNets with ‘(b + 2)’ blocks and ‘b’ skip connections between the blocks
(Figure 1). Each block is a FC-DNN of depth ‘dblk’ and width ‘w’. Here, γipre,γipost,i ∈ [b] are
normalisation variables.
Definition 2.3 (Sub FC-DNNs). Let 2[b] denote the power set of [b] and let J∈2[b] denote any
subset of [b]. Define the‘Jth’ sub-FC-DNN of the ResNet to be the fully ConneCted network obtained
by ignoring/removing (see Figure 1) the skip ConneCtions skipj , ∀j ∈J (see Figure 1).
3 Neural Path Framework
In this section, we extend the neural path framework developed by LS2020, to CNN and ResNet
architectures described in the previous section. The neural path framework exploits the gating property
of ReLU activation, which can be thought of as gate/mask that blocks/allows its pre-activation input
depending on its 0/1 state ( 0 if pre-activation is negative and 1 if pre-activation is positive). The key
idea here is to break a DNN (with ReLU) into paths, and express its output as a summation of the
contribution of the paths. The contribution of a path is the product of the signal in its input node, the
weights in the path and the gates in the path. For a DNN with P paths, for an input x ∈ Rdin , the
gating information is encoded in a novel neural path feature (NPF), φx,Θ ∈ RP and a novel neural
path value (NPV), vΘ ∈ RP encodes the weights. The output of the DNN is then the inner product
of the NPFs and NPVs, i.e., yθ (XS) = hφxs,θ ,vθ〉(ProPosition 3.4).
Definition 3.1. A path starts from an input node, passes through weights, hidden nodes, and normal-
isation Constants and ends at the output node.
Proposition 3.1. The total number of paths in FC-DNN, CNN and ResNet are respeCtively given by
PfC = dinw(dT), PCnn = din(WCVw)dCVw(dfC-1) and Pres = din ∙ Pb=O (b)w(i+2)dblk-1.
3
Under review as a conference paper at ICLR 2021
Notation 3.1 (Index Maps). The ranges of index maps Ilf, Ilcv, Il are [din], [wcv] and [w] respec-
tively. The index maps are used to identify the nodes through which a path p passes. Further, let
IJ (p): [Pres] → 2[b] specify the indices of the skip connections ignored in path p. Also, we follow
the convention that weights and gating values of layers corresponding to blocks skipped are 1.
Definition 3.2 (Path Activity). The product of the gating values in a path p is its ‘activity’ denoted
by AΘ (x, p). We define:
(a)	AΘ(x,p)=Πld-1Gx,Θ(Il(p),l), for FC-DNN and ResNet.
(b)	Aθ(x,p) = ∏d=+1Gχ,θ(If(p),Il(p),l) ∙∏d:+cd+ +1Gx,θ(Il(P)DforCNN
In CNN, the pooling layer is accounted by letting G = Gpool for l = dcv +1.
cnn
Definition 3.3 (Bundle Paths of Sharing Weights). Let PCnn = Pd^, and {B1,..., BP削} be a
^ cnn
collection of sets such that ∀ij ∈ [Pd cnn ], i = J we have Bi ∩ Bj = 0 and UP= 1 Bi = [Pcnn ]. Further,
if paths p,p0 ∈ Bi, then Ilcv(p) = Ilcv(p0), ∀l =1,...,dcv and Il(p) = Il(p0), ∀l =0,...,dcv.
Proposition 3.2. There are exactly din paths in a bundle.
Definition 3.4 (Normalisation Factor). Define Γ( J) = Π∕jre ∙ ∏苫os
Weight sharing is shown in the the cartoon in Figure 2, which shows a CNN with din =3, w =1,
wcv =2, dcv =3, dfc =0. Here, the red coloured paths all share the same weights Θ(1, 1, 1,l),l =
1, 2, 3 and the blue coloured paths all share the same weights given by Θ(2, 1, 1,l),l =1, 2, 3.
θ(i, ι,ι,i) θ(ι, 1,1,2) Θ(i, 1,1,3) GAP	Θ(i,ι,ι,i) θ(i,i,i,2)	Θ(i,ι,ι,3) max —pool	θ(ι., ι,ι,i) Θ(i,ι,ι,2) Θ(lll3) max — pool
Θ(2,1,1.1)	θ(271,1,2)	Θ(2.L 1,3)	Θ(2.1.1.1)	Θ(2,l.l,2)	θ(2,l,l,3)	θ(2.1.1.1)	Θ(2,1,1,2)	θ(2.1.1.3)
Figure 2: Shows weight sharing and rotational symmetry of internal variables and the output after pooling in a
CNN. Left most cartoon uses a GAP layer, and the other two cartoons use max-pooling. Circles are nodes and
the 1/0 in the nodes indicate the gating. Pre-activations/node output are shown in brown/purple.
Definition 3.5 (Neural Path Value). The product of the weights and normalisation factors in a path p
is its ‘value’. The value of a path bundle is the value of any path in that bundle. The path/bundle
values are denoted by vθ (P)/vθ (Bp) and are defined as follows:
(a)vΘ(p)=Πld=1Θ(Il-1(p),Il(p),l).
(b)	vθ (Bp) = ∏d=V ι Θ(ic (p), Ii-ι(p), Ii (p),l) ∙ Πd:+d++1Θ(Ii-ι(p),工I (p),l) ,for any P ∈ Bp.
(c)	vΘ (P) = πd=ιθ(Ii-I (P), I (P), I) ∙ γ(IJ(P)).
The neural path value is defined as vθ = (vθ (p),p ∈ [Pfc ]) ∈ RPfc,vθ = (vθ (Bp), P ∈ [Iy cnn ]) ∈
RP cnn, and vΘ =∆ (vΘ(P),P∈ [Pres]) ∈ RP res for FC-DNN, CNN and ResNet respectively.
Proposition 3.3 (Rotational Invariance). Internal variables in the convolutional layers are circularly
symmetric, i.e.,for r ∈ {0,..., d% — 1} it follows that (i) Zrot(χ,r),θ (ifout, ∙, ∙) = Zχ,θ (ifout ㊉ r, ∙, ∙),
(ii) qrot(x,r) ,Θ (ifout, ∙, ∙) = qx,Θ (ifout ㊉ r, ∙, ∙) and (iii) Grot(X,r),Θ (ifout, ∙, ∙) = G x,Θ (ifout ㊉ r, ∙,)
Definition 3.6. The neural path feature (NPF) corresponding to a path P is given by
=f
(a)	φx,Θ(P) = x(I (P))AΘ(xs, P) for FC-DNN and ResNet.
(b)	φχ,θ(P) = Pp∈B° X(If (P))AΘ(x,P)for cnn
The NPF is defined as φχ,θ = (Φx,θ(p),P ∈ [Pfc]) ∈ RPfc, Φx,θ = (Φx,θ(Bp), P ∈ [P5cnn]) ∈ RPPcnn,
and φx,Θ == (φx,Θ(P),P∈ [Pres]) ∈ RP res for FC-DNN, CNN and ResNet respectively.
Proposition 3.4 (Output=hNPF,NPVi). The output of the network can be written as an inner product
OftheNPFandNPV i.e., ^θ(x) = hφx,θ,vθ).
4
Under review as a conference paper at ICLR 2021
4 Neural Path Kernel: Composite Kernel Based on sub-networks
In this section, we will discuss the properties of neural path kernel (NPK) associated with the NPFs
defined in Section 3. Recall that a co-ordinate of NPF can be non-zero only if the corresponding path
is active. Consequently, the NPK for a pair of input examples is a similarity measure that depends
on the number of paths that are active for both examples. Such common active paths are captured
in a quantity denoted by Λ (Definition 4.2). The number of active paths are in turn dependent on
the number of active gates in each layer, a fact that endows the NPK with a hierarchical/composite
structure. Gates are the basic building blocks, and the gates in a layer for a w-dimensional binary
feature whose kernels are the base kernels. When the layers are laid out depth-wise, we obtain a
product of the base kernels. When skip connections are added, we obtain a sum of products of base
kernels. And presence of convolution with pooling provides rotational invariance.
Definition 4.1. Define the NPK matrix to be HΘ =∆ Φ>ΦΘ, where ΦΘ =(φx1,Θ,...,φx ,Θ) ∈
RP ×n is the NPF matrix.
Definition 4.2. Define ΛΘ(i, x, xs0) =∆ |{p ∈ [P]: I0(p) = i, AΘ(xs,p)=AΘ(xs0 ,p)=1}| to be
total number of ‘active’ paths for both xs and xs0 that pass through input node i.
Definition 4.3 (Layer-wise Kernel). Let Gx,θ (∙,l) ∈ Rw be W-dimensional feature of the gating
values in layer l for input x ∈ Rdin . Define layer-wise kernels:
Hlyθ(S,sO) = hGxs ,θ (∙, l)Gxso ,θ (∙, l)i
Lemma 4.1 (Product Kernel). Let Hfc denote the NPK of a FC-DNN, and for D ∈ Rdin ×din be a
diagonal matrix with strictly positive entries, and u, u0 ∈ Rdin let hu, u0iD = Pidin D(i)u(i)u0(i).
HΘc (s, s) = hxs, xs0 iΛ(∙,Xs ,xs0) = hxs ,xSiπ-l-1 HlΘ (s, s0)
Lemma 4.2 (Sum of Product Kernel). Let Hres be the NPK of the ResNet, and HJ be the NPK of
the sub-FC-DNN within the ResNet obtained by ignoring those skip connections in the set J . Then,
HΘres = X HΘJ
J ∈2[b]
Lemma 4.3 (Rotational Invariant Kernel). Let Hcnv denote the NPK of a CNN, then
din -1	din -1
HΘ (s，s ) =〉: hxs, rot(xs0, r)iΛ(∙,xs ,rot(xso ,r)) =〉: hrot(xs, rr), χs，\Λ(∙,rot(xs ,r),xs/)
r=0	r=0
5 Main Theoretical Result
In this section, we proceed with the final step in extending the neural path theory to CNN and ResNet.
As with LS2020, we first describe the deep gated network (DGN) setup that decouples the NPFs and
NPV, and follow it up with the main result that connects the NPK and the NTK in the DGN setting.
Feature Network zXe (∙, 0) = Xf qfx,θf(iout,l) = hΘf(∙,iout,l),zX,θf(∙,l - 1)i zX,θf (iout,l)=qxf ,θf (i out, l) ∙ Gfx,θf(iout,l) f (x) = hΘf(∙,iout, d), Zx,θf (∙, d - 1)i	Value Network Zx ,ΘDGN (∙, 0) = XV qV ,ΘDGN (iout, l) = hθv {,'out, l), zX,Θv (∙, l - 1)i Zx,θdgn (iout，I) = qX,θdgn (iout,l) ∙ GX,θf (iout, I) yθDGN (x) = hΘv (∙,iout ,d),zx,θDGN (∙, d - 1)i	I
HardReLU: GX® 伉仇，l) = 1{qf.(",1)>0} or SOft-ReLU: GX® 伉仇，l)=/χp(-β.f eιf(，,i))	
Figure 3: Shows a deep gated network (DGN). The soft-ReLU enables gradient flow into the feature network.
DGN set up was introduced by LS2020 to analytically characterise the role played by the gates in
a ‘standalone’ manner. The DGN has two networks namely the feature network parameterised by
Θf ∈ Rdfnet which holds the NPFs (i.e., the gating information) and a value network parameterised by
Θv ∈ Rdnvet which holds the NPV. The combined parameterisation is denoted by ΘDGN = (Θf, Θv) ∈
Rdnet+dvet. Thus the learning problem in the DGN is yθDGN (x) = hφx,θf, vθv).
5
Under review as a conference paper at ICLR 2021
Definition 5.1. The DGN has 4 regimes namely decoupled learning (DL), fixed learnt (FL), fixed
random-dependent initialisation (FR-DI) and fixed random-independent initialisation (FR-II). In all
the regimes yθDGN is the output, and ΘV is always initialised at random and is trainable. However, the
regimes differ based on i) trainability of Θf, ii) initialisation Θf as described below.
DL	:	Θf is trainable, and Θf and	Θv0 are random and statistically independent, β>0.
FL	:	Θf is non-trainable, and Θf	is pre-trained; Θv is statistically independent of Θf .
FR-II	:	Θf is non-trainable, and Θf	and Θv are random and statistically independent.
FR-DI	:	Θf is non-trainable, and Θf	= Θv0.
DGN Regimes: The flexibility in a DGN is that a) Θf can be trainable/non-trainable and b) Θf0 can
be random or pre-trained using yθf as the output (Definition 5.1). By using the DGN setup We can
study the role of gates by comparing (a) learnable (DL) vs fixed gates (FL, FR-DI, FR-II), (b) random
(FR-DI, FR-II) vs learnt gates (FL) and (c) dependent (FR-DI) vs independent initialisations (FR-II).
In the DL regime ‘soft-ReLU’ is chosen to enable gradient floW through the feature netWork.
Proposition 5.1. Let KΘDGN be the NTK matrix of the DGN, then KΘDGN = Kv DGN + Kf DGN, with
Overall NTK	KθDGN (Sg0) = hΨxs, ΘDGN , Ψx,0,θdgni, Where ψχ,ΘDGN = ▽ ®DGN 0θDGN (x) ∈ Rdnet
Feature NTK	KvDGN (S, S ) = h∖ψχs ΘDGN , ψX o ®DGN), Where ψ. 0DGN = ^ Θv yΘDGN (X) ∈ R 'net
Value NTK	KΘdGN (s, S0) = hΨX S,ΘDGN , ΨXJ ,ΘDGN ∖' Where ΨX,ΘDGN = N Θ 0θDGN (x) ∈ Rdnet
Remark: There are tWo separate NTKs, each one corresponding to feature and value netWorks
respectively. In the case of fixed regimes, Kf =0.
Theorem 5.1. (i) Θv0 is statistically independent of Θf (ii) Θv0 are i.i.d symmetric Bernoulli over
{—σ, 十σ}. Let σf = √e and σcv = √c^ for FC and convolutional layers. As W → ∞, we have:
(ii) Kv DGN → βfcHΘf, βfc = dσf2c(d-1) for FC-DNN,
(ii)	K QDGN → βcvHθf, βcv = di2 (d cv σ 2(d cv-1)σ2dfc + dfc σ 2d cv σffc-1)) for CNN with GAP,
(iii)	Kv DGN → PJ ∈2[b] βrJs HJf, βrJs =(|J| + 2)dblkσf2c (|J|+2)dblk-1 Γ(J)2 for ResNet.
•	βfc,βcv,βrs : The simplest of all is βfc = dσf2c(d-1), Where d is due the fact that there are d Weights
in a path and in the exponent of σfc, factor (d — 1) arises because the gradient of a particular Weight
is product of all the Weights in the path excluding the said Weight itself, and the factor of 2 is due to
the fact that NTK is an inner product of tWo gradients. βcv is similar to βfc With separate bookkeeping
for the convolutional and FC layers, and 今 is due to the GAP layer. In /旧 the βfc for all the
sub-FC-DNNs Within the ResNet are scaled bny the corresponding normalisation factors and summed.
•	Decoupling In a DNN With ReLU (and FR-DI regime of DGN), NPV and NPF are not statistically
independent at initialisation, i.e., Theorem 5.1 does not hold. HoWever, the current state-of-the-art
analysis Jacot et al. (2018); Arora et al. (2019); Cao and Gu (2019) is in the infinite width (w → ∞)
regime, wherein, the change in activations during training is only of the order , which goes to
0 as w →∞. Hence, though assumption in Theorem 5.1 may not hold exactly, it is not a strong
assumption to fix the NPFs for the purpose of analysis. Once the NPFs are fixed, it only natural to
statistically decouple the NPV from fixed NPFs (Theorem 5.1 hold in FR-II, FL and DL regimes).
•	Gates are Key: In simple terms, Theorem 5.1 says that if the gates/masks are known, then the
weights are expendable, a fact which we also verify in our extensive experiments.
6	Numerical Experiments
We now show via experiments that gates indeed play a central role in deep learning. For this we use
the DGN setup (Figure 4) to create models in the 4 regimes namely DL, FL, FR-II and FR-DI. In
6
Under review as a conference paper at ICLR 2021
each of the 4 regimes, we create combinatorially many models via a) permutation of the layers when
the copied from the feature to the value network, and b) setting the input to the value network to 1 (in
training and testing), i.e., a tensor with all its entries to be 1. We observe that in all the 4 regimes, the
models are robust to the combinatorial variations.
Setup: Datasets are MNIST and CIFAR-10. For CIFAR-10, we use Figure 4 with 3 × 3 windows
and 128 filters in each layer. For MNIST, we use FC instead of the convolutional layers. All the
FC-DNNS and CNNS are trained With 'Adam' [10] (step-size = 3 ∙ 10-4 , batch size = 32). A ReSNet
called DavidNet [12] was trained with SGD ( step-size = 0.5, batch size = 256). We use β = 10.
Reporting of Statistics: The results are summarised in Figure 4. For FC-DNN and CNN, in each
of the 4 regimes, We train 48 = 2(xv = x/xv = 1) × 24(layer permutations) models. Each of these
models are trained to almost 100% accuracy and the test performance is taken to be the best obtained
in a given run. Each of the 48 models is run only once. For the ResNet, We train only tWo model
for each of the 4 regimes ( Without permuting the layers, but With image as Well as ‘all-ones’ input
variation) and here each mode is run 5 times.
• Result Discussion: Recall that in regimes FR-II and FR-DI the gates are fixed and random, and
only Θv are trained. In DL regime, both Θf and Θv are trained, and FL regime Θf is pre-trained
and fixed, and only Θv is trained. In the folloWing discussion, We compare the performance of the
models in various regimes, along With the performance of CNTK of Arora et al. (2019) (77.43%
in CIFAR-10) and the performance of standard DNN With ReLU. The main observations are listed
beloW (those by Lakshminarayanan and Singh (2020) are also revisited for the sake of completeness).
1.	Decoupling: There is no performance difference betWeen FR-II and FR-DI.Further, decoupled
learning of gates (DL) performs significantly better than fixed random gates (FR), and the gap betWeen
standard DNN With ReLU and DL is less than 3%. This marginal performance loss seems to be
Worthy trade off for fundamental insights of Theorem 5.1 under the decoupling assumption.
2.	Recovery: The fixed learnt regime (FL) shoWs that using the gates of a pre-trained ReLU
netWork, performance can be recovered by training the NPV. Also, by interpreting the input dependent
component of a model to be the features and the input independent component to be the Weights, it
makes sense to look at the gates/NPFs as the hidden features and NPV as the Weights.
3.	Random Gates: FR-II does perform Well in all the experiments (note that for a 10-class problem,
a random classifier Would achieve only 10% test accuracy). Given the observation that the gates are
the true features, and the fact that is no learning in the gates in the fixed regime, and the performance
of fixed random gates can be purely attributed to the in-built structure.
4.	Gate Learning: We group the models into three sets Where S1 = { ReLU, FL , DL}, S2 = {
FR} and S3 = { CNTK }, and explain the difference in performance due to gate learning. S2 and S3
have no gate learning. HoWever, S3 due to its infinite Width has better averaging resulting in a Well
formed kernel and hence performs better than S2 Which is a finite Width. Thus, the difference betWeen
S2 and S3 can be attributed to finite versus infinite Width. Both S1 and S2 are finite Width, and hence,
conventional feature learning happens in both S1 and S2, but, S1 With gate learning is better (77.5%
or above in CIFAR-10) than S2 (67% in CIFAR-10) With no gate learning. Thus neither finite Width,
nor the conventional feature learning explain the difference betWeen S1 and S2. Thus, ‘gate learning’
discriminates the regimes S1,S2 and S3 better than the conventional feature learning vieW.
Figure 4: Cfi , Civ ,i ∈ [4] are the convolutional layers, Which are folloWed by global-average-pooling (GAP)
layer then by a dense layer (Df/Dv), and a softmax layer to produce the final logits.
7
Under review as a conference paper at ICLR 2021
(Left) Standard ReLU DNN and (Right) DGN with Fixed Learnt Gates from model on the left with j1 ,j2 ,j3 ,j4 =4, 3, 2, 1 and xv = rand - tensor
For each model, input is shown first and then starting from the first layer, the first 2 filters of each of the 4 layers are shown.
Figure 5: Hidden layer outputs for a fixed random input to the value network of DGN with permuted gating.
5.	Permutation and Input Invariance: The performance (in all the 4 regimes) is robust to ‘all-ones’
inputs. Note that in the ‘all-ones’ case, the input information affects the models only via the gates.
Here, all the entries of the input Gram matrix are identical, and the NPK depends only on Λ, which is
the measure of sub-network active simultaneously for the various input pairs. The performance (in
all the 4 regimes) is also robust to permutation of the layers. This can be attributed to the product
Πl(d-1)Hll,yr of the layer level base kernels being order invariant.
6.	Visualisation: Figure 5 compares the hidden layer outputs of a standard DNN with ReLU with 4
layers, and that of a DGN which copies the gates from the standard DNN, but, reverses the gating
masks when applying to the value network. Also, the value network of the DGN was provided with a
fixed random input (as shown in Figure 5). Both the models achieved about 80% test accuracy, an
otherwise surprising outcome, yet, as per the theory developed in this paper, a random input to the
value network should not have much effect on performance, and this experiment confirms the same.
7 Related and Future Work
Our paper extended the work of Lakshminarayanan and Singh (2020) to CNN and ResNet. Further,
we pointed out the composite nature of the underlying kernel. Experiments with permuted masks and
constant inputs are also significant and novel evidences, which to our knowledge are first of their
kind in literature. Gated linearity was studied recently by Fiat et al. (2019), however, they considered
only single layered gated networks.Jacot et al. (2018); Arora et al. (2019); Cao and Gu (2019); Jacot
et al. (2019); Du et al. (2018) have all used the NTK framework to understand questions related to
optimisation and/or generalisation in DNNs. We now discuss the future work below.
1.	Base Kernel: At randomised initialisation, for each l,
啖0 (s,s0)
W
is the fraction of gates that
are simultaneously active for input examples s, s0, which in the limit of infinite width is equal to
(1 - angle(zχs (∙,l), ZxS0 (∙,l))) (Xie et al., 2017). Further, due to the property of ReLU to pass only
positive components, we conjecture that the pairwise angle between input examples measured at the
hidden layer outputs is a decreasing function of depth and as l → ∞, l*(W s---› 1, ∀s, s0 ∈ [n].
We reserve a formal statement on the behaviour of Hll,yr 0 for the future.
2.	Multiple Kernel Learning (GGnen and AIPaydin, 2011; Bach et al., 2004; Sonnenburg et al., 2006;
Cortes et al., 2009) is the name given to a class of methods that learn a linear or non-linear combination
of one or many base kernels. For instance, Cortes et al. (2009) consider polynomial combinations of
base kernels, which also has a ‘sum of products’ form. Our experiments do indicate that the learning
in the gates (and hence the underlying base kernels) has a significant impact. Understanding Kf
(Proposition 5.1) might be a way to establish the extent and nature of kernel learning in deep learning.
It is also interesting to check if in ResNet the kernels of its sub-FC-DNNs are combined optimally.
8 Conclusion
We attributed the success of deep learning to the following two key ingredients: (i) a composite
kernel with gates as fundamental building blocks and (ii) allowing the gates to learn/adapt during
training. We justified our claims theoretically as well as experimentally. This work along with that of
Lakshminarayanan and Singh (2020) provides a paradigm shift in understanding deep learning. Here,
gates play a central role. Each gate is related to a hyper-plane, and gates together form layer level
binary features whose kernels are the base kernels. Laying out these binary features depth-wise gives
rise to a product of the base kernels. The skip connections gives a ‘sum of product’ structure, and
convolution with pooling gives rotation invariance. The learning in the gates further enhances the
generalisation capabilities of the models.
8
Under review as a conference paper at ICLR 2021
References
[1]	Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang.
On exact computation with an infinitely wide neural net. In Advances in Neural Information
Processing Systems, pages 8139-8148, 2019.
[2]	Francis R Bach, Gert RG Lanckriet, and Michael I Jordan. Multiple kernel learning, conic
duality, and the smo algorithm. In Proceedings of the twenty-first international conference on
Machine learning, page 6, 2004.
[3]	Yuan Cao and Quanquan Gu. Generalization bounds of stochastic gradient descent for wide
and deep neural networks. In Advances in Neural Information Processing Systems, pages
10835-10845, 2019.
[4]	Corinna Cortes, Mehryar Mohri, and Afshin Rostamizadeh. Learning non-linear combinations
of kernels. In Advances in neural information processing systems, pages 396-404, 2009.
[5]	Simon S Du, Jason D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds
global minima of deep neural networks. arXiv preprint, arXiv:1811.03804, 2018.
[6]	Jonathan Fiat, Eran Malach, and Shai Shalev-Shwartz. Decoupling gating from linearity. CoRR,
abs/1906.05032, 2019. URL http://arxiv.org/abs/1906.05032.
[7]	Mehmet Gonen and Ethem AlPaydin. Multiple kernel learning algorithms. The Journal of
Machine Learning Research, 12:2211-2268, 2011.
[8]	Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and
generalization in neural networks. In Advances in neural information processing systems, pages
8571-8580, 2018.
[9]	Arthur Jacot, Franck Gabriel, and Clement Hongler. Freeze and chaos for dnns: an ntk view
of batch normalization, checkerboard and boundary effects. arXiv preprint arXiv:1907.05715,
2019.
[10]	Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2014.
[11]	Chandrashekar Lakshminarayanan and Amit Vikram Singh. Neural path features and neural
path kernel : Understanding the role of gates in deep learning. arXiv preprint, arXiv:2006.10529
(Accepted in NeurIPS 2020), 2020. URL https://arxiv.org/abs/2006.10529.
[12]	David C. Page. Davidnet. URL https://github.com/davidcpage/
cifar10-fast.
[13]	Soren Sonnenburg, Gunnar Ratsch, Christin Schafer, and Bernhard Scholkopf. Large scale
multiple kernel learning. Journal of Machine Learning Research, 7(Jul):1531-1565, 2006.
[14]	Bo Xie, Yingyu Liang, and Le Song. Diverse neural network learns true target functions. In
Artificial Intelligence and Statistics, pages 1216-1224, 2017.
9