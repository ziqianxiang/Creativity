Under review as a conference paper at ICLR 2021
Dual-Tree Wavelet Packet CNNs for Image
Classification
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we target an important issue of deep convolutional neural networks
(CNNs) - the lack of a mathematical understanding of their properties. We present
an explicit formalism that is motivated by the similarities between trained CNN
kernels and oriented Gabor filters for addressing this problem. The core idea is
to constrain the behavior of convolutional layers by splitting them into a succes-
sion of wavelet packet decompositions, which are modulated by freely-trained
mixture weights. We evaluate our approach with three variants of wavelet decom-
positions with the AlexNet architecture for image classification as an example.
The first variant relies on the separable wavelet packet transform while the other
two implement the 2D dual-tree real and complex wavelet packet transforms, tak-
ing advantage of their feature extraction properties such as directional selectivity
and shift invariance. Our experiments show that we achieve the accuracy rate of
standard AlexNet, but with a significantly lower number of parameters, and an
interpretation of the network that is grounded in mathematical theory.
1	Introduction
Deep convolutional neural networks (CNNs) have dramatically improved state-of-the-art perfor-
mances in many domains such as speech recognition, visual object recognition or object detection
(LeCun et al., 2015). However, they are very resource-intensive and a full mathematical understand-
ing of their properties remains a challenging issue.
On the other hand, in the field of signal processing, wavelet and multi-resolution analysis are built
upon a well-established mathematical framework. They have proven to be very efficient in tasks such
as signal compression and denoising (Mallat, 2009). Moreover, wavelet filters have been widely
used as feature extractors for signal, image and texture classification (Laine & Fan, 1993; Pittner &
Kamarthi, 1999; Yen, 2000; Huang & Aviyente, 2008).
While both fields rely on filters to achieve their goals, the two approaches are radically different. In
wavelet analysis, filters are specifically designed to meet very restrictive conditions, whereas CNNs
use freely trained filters, without any prior assumption on their behavior. Nevertheless, in many
computer vision tasks, CNNs tend to learn parameters that are pretty similar to oriented Gabor
filters in the first layer (Boureau et al., 2010; Yosinski et al., 2014). This phenomenon suggests that
early layers extract general features such as edges or basic shapes, which are independent from the
task at hand.
Proposed approach In order to improve our understanding of CNNs, we propose to constrain their
behavior by replacing freely-trained filters by a series of discrete wavelet packet decompositions
modulated by mixture weights. We therefore introduce prior assumptions to guide learning and
reduce the number of trainable parameters in convolution layers, while retaining predictive power.
The main goal of our work is to describe and interpret the observed behavior of CNNs with a
sparse model, taking advantage of the feature extraction properties of wavelet packet transforms.
By increasing control over the network, we pave the way for future applications in which theoretical
guarantees are critical.
1
Under review as a conference paper at ICLR 2021
In this paper we describe our wavelet packet CNN architectures with a mathematical formulation
and introduce an algorithm to visualize the resulting filters. As a proof of concept, we based our
experiments on AlexNet (Krizhevsky et al., 2012). Our choice was driven by the large kernels in its
first layer and convolution operations performed with a downsampling factor of 4. This allows to
perform two levels of wavelet decomposition without any additional transformation, and facilitates
visual comparison with our own custom filters. Note however that most CNNs trained on natural
image datasets exhibit the same oscillating patterns. We therefore believe that our work could be
extended to other architectures with a few adaptations.
Related work In a similar spirit, a few attempts to combine the two research fields have been made
in recent years. Wavelet scattering networks (Bruna & Mallat, 2013) compute CNN-like cascading
wavelet convolutions to get translation-invariant image representations that are stable to deforma-
tion and preserve high-frequency information. They were later adapted to the discrete case using
complex oriented wavelet frames (Singh & Kingsbury, 2017). While these networks are designed
from scratch and are totally deterministic, other approaches enhance existing networks with wavelet
filter preprocessing or embedding. The goal is either to improve classification performance without
increasing the network complexity (Chang & Morgan, 2014; Williams & Li, 2016; Fujieda et al.,
2017; Williams & Li, 2018; Lu et al., 2018; Luan et al., 2018), or to replace freely-trained layers
by more constrained structures implementing spectral filtering. Such models include Gabor filters
in parallel to regular trainable weight kernels (Sarwar et al., 2017), wavelet scattering coefficients
as the input of a CNN (Oyallon et al., 2018), or linear combinations of discrete cosine transforms
(Ulicny et al., 2019).
Our approach falls into this second category, although our design is based upon a different CNN
architecture, i.e., AlexNet. To our knowledge we are the first to introduce the dual-tree wavelet
packet transform (DT-CWPT) (Bayram & Selesnick, 2008) in such context. Like the filters used in
the above papers, wavelet packet transforms are well-localized in the frequency domain and share a
subsampling factor over the output feature maps. A major advantage with our approach is sparsity: a
single vector (called conjugate mirror filter, or CMF) is sufficient to characterize the whole process.
Moreover, like Gabor filters, DT-CWPT extracts oriented and shift-invariant features, but achieves
this goal with minimal redundancy, while providing an efficient decomposition algorithm based on
separable filter banks. Regarding the discrete cosine transform, its complexity is similar to DT-
CWPT but lacks orientation properties. Our models therefore provide a sparser description of the
observed behavior of convolutional layers. This is a step toward a more complete description of
CNNs by using a small number of arbitrary parameters.
2	Background
Notations In this paper, d-dimensional tensors are written with straight bold capital letters: Z ∈
RAι× ×Ad, where Ai denotes the Size of Z along its i-th dimension； the shape of Z is denoted
hZi = A1 . . . Ad > . 2D matrices are written in italic: U ∈ RA×B and 1D vectors in bold
lower-case letters: z ∈ RA. For the sake of legibility, indices are written between square brackets.
The convolution between two matrices U ∈ RA×B and V ∈ RA0 ×B0 is defined, for allm∈
{0..A + A0 - 2} and n ∈ {0..B + B0 - 2}, by (U * V )[m, n] = £ν U [m - i, n - j ] ∙
V [i, j]. Since some indices are negative or bigger than the matrix size, U and V must be ex-
tended beyond their limits, either by setting all outside values to zero, or by using a periodic or
symmetric pattern. Practical implications of this choice will not be discussed in this paper.
For any U ∈ Ra×b, U denotes the flipped matrix: U [m, n] = U [A 一 (m + 1), B 一 (n +1)].
The upsampling and downsampling operators are respectively denoted ↑ and，For any α ∈ N*,
(U ↑ α)[m, n] = U 片,n] if both m and n are divisible by α (= 0 otherwise), and (U ]
α) [m, n] = U [αm, αn]. Finally, for any scalar z ∈ R, we denote z + U = zJ + U, where
J ∈ RA×B denotes the matrix of ones.
Discrete wavelet packet transform (WPT) This is a brief overview on the WPT algorithm (Mal-
lat, 2009), written as a sequence of matrix convolutions. An illustration of the transform is given in
Appendix A.7.
2
Under review as a conference paper at ICLR 2021
We will implicitly build a discrete orthogonal basis in which any matrix X ∈ RN ×N can be de-
composed. The basis is made of oriented 2D waveforms with high frequency resolution, which is
an interesting property for feature extraction. Considering a pair of conjugate mirror filters (CMFs)
h and g ∈ Rμ, We build a separable 2D filter bank, made of one low-pass filter G(O) = h ∙ h> and
three high-pass filters G⑴=h ∙ g>, G⑵=g ∙ h> and G⑶=g ∙ g>.
We start the decomposition with D(00) = X . Let us assume that for a given j ∈ N, the feature
maps of wavelet packet coefficients at scale j, denoted D(jk), have already been computed for any
k ∈ 0 . . 4j - 1 . Then we compute the wavelet packet coefficients at the coarser scale j + 1 by
decomposing each feature map D(jk) into four smaller submatrices:
∀l ∈ {0.. 3} , Dj++l) = (Djk) * G(I)) J 2 .
(1)
At each scale j > 0, the set of 4j matrices
D(jk) is a representation ofX from which the original
signal can be reconstructed. Figure 1 illustrates the WPT resulting filters with j = 2.
Dual-tree complex wavelet packet transform (DT-CWPT) WPT has interesting properties such
as sparse signal representation and vertical / horizontal feature discrimination. However, it suffers
from a lack of shift invariance and a poor directional selectivity. To overcome this, Kingsbury (2001)
designed a discrete wavelet transform in which input images are efficiently decomposed in a tight
frame of complex oriented waveforms with limited redundancy. It was generalized to the wavelet
packet framework by Bayram & Selesnick (2008).
In a nutshell, let us assume that we have decomposed an input matrix X into four WPT represen-
tations D(ak,)j , D(bk,)j , D(ck,j)	, D(dk,)j
This is achieved by applying expression (1) with
four suitable filter banks G(al) ,	G(bl) , G(cl) , G(dl)
Then we can compute the following
complex wavelet packet coefficients E%(k) and E-(k), for each k ∈ {0.. 4j - 1}:
Ej%(k)	I	-I	D(ak,)j	I I	D(ck, j)
Ej-(k)	I I	D(dk,)j	I	-I	D(bk,)j
(2)
For a given scale j > 0, the set of (2 ∙ 4j) complex matrices {e%(k), E—(k)} constitutes a re-
dundant representation of X from which the original signal can be reconstructed. DT-CWPT is
oriented, and nearly shift invariant if we consider the modulus of complex coefficients. Figure 1
illustrates the DT-CWPT resulting filters with j = 2.
Dual-tree real wavelet packet transform (DT-RWPT) By computing only the real part of the
above coefficients, we get a representation of X in a real tight frame. Like above, DT-RWPT is an
oriented transform, but does not possess the shift invariance property. This may have consequences
on its predictive power, as will be seen later.
Link with Gabor filters Such as presented above, the wavelet packet transforms compute a full
decomposition in what Mallat (2009) calls a pseudo-local cosine basis. The resulting filters have
identical window size with a varying number of oscillations within these windows (see Figure 1).
Therefore, such wavelet packets share similarities with Gabor filters. However, they offer a compet-
itive advantage: the decomposition is performed efficiently using one or few separable filter banks,
which are fully characterized by a single one-dimensional vector.
Convolutional layers Let P denote the number of samples (batch size), K (resp. L) the number
of input (resp. output) channels, (M, N) the size of input feature maps and (μ, V) the kernel size.
A 2D convolutional layer with fixed parameters s, d, q ∈ N* (stride, dilation factor and number of
groups, respectively), weight W ∈ R(K/q)xLx*xv and bias b ∈ RL, transforms any4D input tensor
3
Under review as a conference paper at ICLR 2021
□E3ES
耀至昌胴[UHMES
MgS□J≡[gHEΓ
■物网■£目回日
之ZN■口厚昌M
ES□B^B≡^
©
Figure 1: a -b Respectively, WPT and DT-CWPT filters for j = 2, computed with Q-shift or-
thogonal CMFs of length 10 (Kingsbury, 2003). The matrices have been cropped to 11 × 11. b
displays 32 complex filters, alternatively represented by their real and imaginary parts. 1 and 2
are the filters computing Ej%(k) and Ej-(k), respectively. c Convolution kernels Walex [0, k] in
AlexNet’s first layer. We used a model from the Torchvision package, pretrained on ImageNet.
X ∈ RP×K×M×N into an output tensor Y ∈ RP×L×M0×N0, such that
K/q-1
Y [p, l] = b[l]+ X (X[p, ko(l) + k] * (WKTJ ↑d))[ s,	⑶
k=0
where ko(l) = [lq/L」∙ K/q denotes the first input channel influencing the l-th output. Note that in
expression (3), Y [p, l], X [p, k0(l) + k] and W [k, l] are 2D matrices while b[l] is a scalar.
Definition 1. We denote Csqd(W, b) the operator computing (3): Y = C(q)(W, b) ∙ X.
In this paper, we focus on AlexNet’s first convolutional layer, which can be represented as a con-
volution operator C4(1,)1 (W, b), with W ∈ R3×64×11×11 and b ∈ R64. The kernels W [0, k] after
training with ImageNet are displayed Figure 1. We can notice oriented oscillating patterns similar
to wavelet packet filters.
3	Proposed models
We now introduce several network architectures that are built on standard AlexNet, in which the first
11 × 11 convolutional layer is replaced by a succession of WPT or DT-(RC)WPT decompositions
modulated by mixture weights. Each network takes as input a 4D tensor X∈RP×3×224×224,i.e.,a
set of P images with three input channels (RGB images).
3.1	WPT module
This module computes two successive WPT decompositions (1) for every input channel. Each
step j ∈ {0, 1} is implemented as a strided convolution operator Cs(q, jd) (Wj, 0) (see Definition 1),
where Wj contains the fixed low- and high-pass filters. In this configuration, each input channel is
convolved with its own set of filters.
More precisely, We have (s = 2), (d = 1) and (qj = Kj), where Kj = (3 ∙ 4j) denotes the number
of input channels. Wj ∈ R1×(4Kj)×μ×μ is SUCh that for all k ∈ {0 ..Kj 一 1} and l ∈ {0.. 3},
Wj [0, 4k + l] = G(l). The output, denoted D ∈ RP×48×N0×N0, is such that
D = C(II)(Wι, 0) ∙ (C(31(Wo, 0) ∙ X).	(4)
Once this is done, we need to modulate the importance of each wavelet packet. Moreover, the
number of output channels must be equal to 64 as in standard AlexNet, and every output channel
4
Under review as a conference paper at ICLR 2021
must be influenced by each input RGB channel. This is achieved with a 1 × 1 convolutional layer
(Lin et al., 2014; Szegedy et al., 2015) placed after the WPT decomposition. Note that this approach
was also chosen by Ulicny et al. (2019) in what they call a harmonic block.
Therefore, the final output, denoted Ywpt ∈ RP ×64×56×56 , is such that
YwPt = C(11 (Wmix, bmix) ∙ D ,	(5)
where Wmix ∈ R48×64×1×1 and bmix ∈ R64 are freely trained. A schematic representation of the
WPT module can be found in Figure 2-2 . The orange (“FB”, a.k.a., filter bank) and green (“Conv”)
layers compute expressions (4) and (5), respectively.
Number of trainable parameters A WPT module has |Wmix| + |bmix| = 3, 136 trainable pa-
rameters, versus 23, 296 for the first convolutional layer in a standard AlexNet.
3.2	DT-(RC)WPT MODULES
In DT-RWPT and DT-CWPT modules, respectively two and four suitable filter banks are used on
each input channel. The outputs, denoted ER ∈ RP×96×N0×N0 and EC ∈ RP×192×N0×N0, have the
following structure. For any sample p ∈ {0 . . P - 1},
Re E% [p]
Ernl= /ReE%[p]} = (Da[p] - Dd[p]∖ E 加=Re＞刎
R[p]=(Re E-[p]J = [Da[p] + Dd[p/	C[pl = Im E%[p]
Im E- [p]
where Da, Db, Dc and Dd ∈ RP×48×N0×N0 are computed similarly to (4).
/Da[p] - Dd[p]∖
Da [p] + Dd[p]
Dc[p] + Db[p],
Dc [p] - Db [p]
(6)
Expression (6) is a tensor formulation of (2), where the real and imaginary parts of the complex
coefficients are stored separately. As for WPT computed in (4), both DT-RWPT and DT-CWPT can
be expressed as a succession of CNN-style convolution operators. This requires a few technicalities
that are provided in Appendix A.4.
Again, we placed a 1 × 1 convolutional layer after the wavelet packet decompositions. The final
outputs, denoted Ydt-RwPt and Ydt-CwPt ∈ RP×64×N0×N0, are such that
Ydt-RwPt
Cf 1(wmix, b0mix) ∙ Er;	Ydt-CwPt
C(11(Wmix, bmix) ∙ EC ,
(7)
where W0mix ∈ R96×64×1×1, W0m0 ix ∈ R192×64×1×1, b0mix and b0m0 ix ∈ R64 are freely trained. A
schematic representation of both modules can be found in Figure 2-(3)④.The blue (“干” and “士”)
and green (“Conv”) layers compute expressions (6) and (7), respectively.
Number of trainable parameters DT-RWPT and DT-CWPT modules have 6, 208 and 12, 352
trainable parameters, respectively (23, 296 in a standard AlexNet). We will see in Section 5 how
these numbers can be further decreased without degrading the performance of the network.
3.3	Kernel visualization
WPT and DT-(RC)WPT modules are designed as a succession of multi-channel convolutional lay-
ers. The following proposition states that such cascading layers can be expressed as a single CNN-
style convolution operator. It provides an explicit formulation of the resulting hyperparameters 一
i.e., stride, dilation factor and number of groups describing input-output channel connections - and
weight tensor. It takes advantage of the well-known result that two successive convolutions can be
written as another convolution with a wider kernel.
Let CsI)(W, b), denote an initial convolution operator, with W ∈ RK×L×μ×ν. We consider a
second operator C(q)(V, a), with V ∈ R(L/])乂工0乂口乂/7 and a ∈ RL0 (we assume that both L and L0
are divisible by q).
5
Under review as a conference paper at ICLR 2021
56, 56, 48
56, 56, 48
56, 56, 192
56, 56, 96
Conv (1, 1)
56, 56, 64
56, 56, 64
56, 56, 64
Figure 2: 1 First layer of AlexNet; 2 WPT module; 3 DT-RWPT module; 4 DT-CWPT module.
Only the green layers (Conv) have trainable parameters. The numbers between each layer indicate
the height, width and depth (number of channels) of the current feature map tensor.
Proposition 1. The composition of two strided CNN-style convolution operators such as introduced
by Definition 1 can be written as another strided CNN-style convolution operator:
Ct(,q1)(V, a) ◦ Cs(,1)1(W, b) =Cs(0q,0)d0(W0,b0),	(8)
with s0 = st (stride), d0 = 1 (dilation) and q0 = 1 (number of groups). Moreover, the resulting
weight W0 is computed using a dilated CNN-style convolution operator:
W0 = Cswwdw (V, 0) ∙ W ,	(9)
with (sw	= 1),	(dw	=	s)	and (qw =	q).	We also have	b0	= a +	f(V,	b),	where f is such that
f(V, 0) = 0. An explicit formulation of f is given in the Appendix A.6.
Remark. Proposition 1 is valid only if the matrices are infinitely extended beyond their limits -
we either get an infinite sequence with finite support (zero padding), a N -periodic signal (periodic
padding) or a 2N -periodic signal (symmetric padding). In practice, the amount of padding at each
layer must be carefully chosen to avoid distortion effects at the edges of feature maps, as done in
our implementation.
Proposition 1, whose proof is given in Appendix A.6, shows that the wavelet packet modules com-
pute
Ywpt= C411(Wwpt, bmix) ∙ X ；	(10)
Ydt-Rwpt = c41 1(Wdt-Rwpt, b'mix) ∙ X ；	Ydt-Cwpt = c41 1(Wdt-Cwpt, b0mmix) ∙ X ,	(11)
where Wwpt, Wdt-Rwpt, Wdt-Cwpt ∈ R3×64×(3μ-2)×(3μ-2) are obtained from (9). As a reminder,
μ denotes the size of the CMF. By means of comparison, AlexNet,s first layer computes
Yalex = C411(Walex, balex) ∙ X ,	(12)
where Walex ∈ R3×64×11×11 and balex ∈ R64. Therefore, all modules are represented as convolu-
tion operators with stride s = 4, mapping 3 input channels to 64 output channels. Regarding kernel
size, it is bigger than 11 as long as μ ≥ 5; However, most energy is concentrated in a region the size
of which is similar to AlexNet kernels.
A visualization of these kernels after training is given in Figure 3. Training details are provided in
Section 4. For the sake of visual comparison, we only displayed center patches of size 11 × 11 from
the original matrices - it turns out that in all our models, between 97% and 99% of their energy (i.e.,
the squared L2-norm) is concentrated in these cropped regions. We point out that computing the
resulting kernels is used for analysis purpose, but should not be involved in the training process.
6
Under review as a conference paper at ICLR 2021
N□国□sHraE
□*c陋系□S3K
s□口川■的星■
Figure 3: From left to right: {Wwpt [0, k]}, {Wdt-Rwpt [0, k]}, {Wdt-Cwpt [0, k]}k∈{0..63} after
training with ImageNet ILSVRC2012. All modules are implemented with a Q-Shift filter (μ = 10).
Whereas the WPT module mainly extracts horizontal and vertical features, many more orientations
arise from the dual-tree modules. We can also notice the low-pass filters, which appear as color
blobs. The resemblance with AlexNet kernels (see Figure 1) is prominent.
4	Experiments
4.1	Implementation details
Wavelet filters For the experiments we used a PyTorch / CUDA implementation. Our wavelet
packet modules were designed with a Q-shift orthogonal filter of length 10 (Kingsbury, 2003), which
approximately meets the half-sample-shift condition required for the dual-tree transforms. For the
sake of setting consistency, we also used this filter for conventional WPT.
Datasets Our models were trained and evaluated on ImageNet ILSVRC2012 dataset (Russakovsky
et al., 2015). Since the online evaluation server is no longer available, we set aside 100, 000 images
from the training set - 100 per class - in order to create a validation set. We used this subset to
compute accuracy rate along the training phase. As for the validation set provided by ImageNet, we
turned it into a test set on which our trained models were evaluated.
To assess generalization performance of our models, we also finetuned them on PASCAL VOC 2012
(Everingham et al., 2015) and COCO 2014 (Lin et al., 2015) datasets, on the multilabel classification
task. For this we initialized the networks with the parameters previously obtained with ImageNet and
replaced the last fully-connected layer by a layer containing the desired number of outputs. Since
again we didn’t have access to the ground truth for the “official” test sets, we split each validation
set in two roughly equal parts. We then used the first part for validation and the second for testing.
Training details For each dataset, the models were trained on a single GPU. The training pro-
cedure was inspired from many ILSVRC papers (Krizhevsky et al., 2012; Simonyan & Zisserman,
2015; Szegedy et al., 2015; He et al., 2016). More precisely, it was carried out by optimizing the
cross-entropy loss with stochastic gradient descent. For this we fed the network with random batches
of 256 images, until we reached 100 cycles through the whole training set (100 epochs, i.e., 461.4K
iterations for ImageNet). The momentum was set to 0.9 and weight decay to 5 ∙ 10-4. As for the
learning rate, it was initially set to 10-2, and then decreased by a factor of 10 every 25 epochs.
To reduce overfitting, we followed the data augmentation procedure used in Inception networks
(Szegedy et al., 2015). The images are first normalized to a specified mean and standard deviation
for each RGB channel. Then they are randomly flipped and cropped from 8% to 100% of their
original sizes, with a random aspect ratio varying from 3 to 3, before being resized to 224 X 224
using a bilinear interpolation.
Model evaluation The test phase was carried out following Krizhevsky et al. (2012). Namely,
predictions are made over 10 patches extracted from each input image, and the softmax output
vectors are then averaged to get the overall prediction. We used top-1-5 accuracy rates (ImageNet)
and average precision (multilabel tasks) (Everingham et al., 2015) as evaluation metrics.
7
Under review as a conference paper at ICLR 2021
Figure 4: Evolution of the loss function (left) and top-1 validation error (right) during the first 65
training epochs. Validation has been performed by simply resizing the smaller edge of each image
to 224 and extracting a single patch of size 224 × 224 at the center.
Table 1: Error rates on our custom validation and test sets.
Model	Nb params	Top-1 (test)	ImageNet Top-5 (test)	Top-5 (val)	VOC Average	COCO error (test)
WPT AIeXNet	3.1K	45.5%	22.9%	20.2%	25.6%	46.0%
DT-RWPT AlexNet	6.2K	43.2%	20.7%	18.2%	23.6%	44.1%
DT-CWPT AlexNet	12.4K	42.3%	20.2%	17.5%	23.1%	43.4%
Standard AlexNet	23.3K	42.2%	20.0%	17.5%	22.9%	43.4%
Frozen AlexNet	None	51.1%	27.8%	24.8%	31.2%	51.3%
Comparison with existing models Our models were compared with a standard AlexNet that we
trained according to the same procedure. In addition, we wanted to isolate the contribution of
wavelet packet modules to the global predictive power. To achieve that we trained an AlexNet
in which the first 11 × 11 convolutional layer was frozen to its initial parameters.
4.2	Results and discussion
The evolution of the loss function and validation error along training with ImageNet is shown in
Figure 4. Similar graphs can be found in Appendix A.3 for VOC and COCO datasets. Besides,
classification performance of our trained models are displayed in Table 1. For multilabel tasks, we
have defined the average error by E = 1 - Π ∈ [0, 1], where Π denotes the average precision.
The DT-CWPT models almost reach standard AlexNet’s accuracy, with twice less parameters in
the first layer. While WPT and DT-RWPT models achieve lower performances, they are still much
higher than the frozen version of AlexNet. This result suggests that the predictive power is partly
accountable to the wavelet packet modules themselves, and not entirely to the following layers.
Besides, the good results we obtained on multilabel classification tasks assert the generalizability of
our models.
By looking at Figure 3, it is easy to explain why conventional WPT has the lowest accuracy of all our
models. We identified two main reasons. (1) The filter design makes it impossible to extract oriented
features that are neither horizontal nor vertical. Instead, it yields checkerboard patterns, which
are useful to catch the remaining information and ensure perfect reconstruction, but fail to process
geometric image features like ridges and edges, as pointed out by Selesnick et al. (2005). (2) There
is no medium-frequency feature extractor like in AlexNet kernels - black-and-white patches side
by side. Such patterns can however be found in dual-tree kernels. If we consider the real and
imaginary parts of the complex low-pass filters separately, we actually get one low-pass and one
oriented band-pass filter - see Figure 1.
Regarding the DT-RWPT model, it performs better than WPT but does not reach the accuracy of
DT-CWPT or standard AlexNet, despite generating oriented filters. Intuitively, DT-CWPT is twice
8
Under review as a conference paper at ICLR 2021
more redundant than DT-RWPT, and thus is more likely to extract relevant features for image clas-
sification. We propose here a more specific interpretation. In Section 2 we mentioned the shift
invariance property of DT-CWPT, that neither WPT nor DT-RWPT possess. By applying a slight
shift to an image, great disturbances can indeed be observed in the matrices of real coefficients
(Selesnick et al., 2005). Important features extracted from one image can thus disappear from the
other. On the other hand however, the modulus of complex wavelet packet coefficients is nearly shift
invariant, meaning that their value is smoothly transferred toward the neighboring pixels in the shift
direction. Therefore, any loss of information in their real part is recovered in their imaginary part.
The DT-CWPT module is capable to extract similar features from two shifted images, which could
explain its superior performances. We tested the robustness of our models with respect to small
shifts and obtained results that support our hypothesis. More details can be found in Appendix A.1.
The source code used for our experiments will be published shortly after paper acceptance. We will
also provide notebooks for the sake of replicability.
5	Conclusion and future work
Following an epoch of frantic race toward classification performance, research is more and more
focused on understanding learning mechanisms in CNNs. In this perspective, we proposed an ar-
chitecture in which standard convolutional layers are replaced by dual-tree wavelet packet feature
extractors. Our experiments show that such networks can compete with conventional models, pro-
viding a sparse description of their behavior.
The DT-CWPT module contains twice less trainable parameters than standard AlexNet’s first layer.
Future research could be held to further increase the sparsity of our models. (1) Some filters gen-
erally extract less information than others, in that the corresponding feature map’s energy is much
lower. By discarding these feature maps before computing 1 × 1 convolutions, we could reduce the
number of parameters by the same amount. More details about energy distribution is given in Ap-
pendix A.2. (2) Feature map combinations may not be equally worth. We could constrain the 1 × 1
convolutional layer by preventing some scales or orientations from wiring together, or by separating
the real and imaginary coefficients.
So far we trained our models with a predefined CMF, but other filters may provide better extraction
properties due to a higher frequency localization or number of vanishing moments. One way to
address this question could be to let the network learn the optimal filter with a proper regularizer in
the loss function. This will be addressed in future work.
We tested our framework on the first layer of AlexNet, because introducing wavelet packet trans-
form into it is quite straightforward. Nevertheless, the phenomenon of oscillating patterns does not
restrict to this particular model, nor is it specific to the sole task of image classification. However,
extending our models to a wider range of architectures must be handled carefully to match the de-
sired hyperparameters. This will be tackled in future work, in which we will also benchmark our
results with other wavelet CNN approaches.
References
Ilker Bayram and Ivan W. Selesnick. On the dual-tree complex wavelet packet and M-band trans-
forms. IEEE Transactions on Signal Processing, 2008.
Y-Lan Boureau, Francis Bach, Yann LeCun, and Jean Ponce. Learning mid-level features for recog-
nition. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition,
pp. 2559-2566, 2010.
Joan BrUna and Stephane Mallat. Invariant Scattering Convolution Networks. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 35(8):1872-1886, 2013.
Shuo-Yiin Chang and Nelson Morgan. Robust CNN-based speech recognition with Gabor filter
kernels. In Fifteenth Annual Conference of the International Speech Communication Association,
2014.
9
Under review as a conference paper at ICLR 2021
Mark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and
Andrew Zisserman. The Pascal Visual Object Classes Challenge: A Retrospective. International
JournalofComPuterVision,111(1):98-136, 2015. ISSN 1573-1405.
Shin Fujieda, Kohei Takayama, and Toshiya Hachisuka. Wavelet Convolutional Neural Networks
for Texture Classification. 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE ComPuter Society Conference on ComPuter Vision and Pattern
Recognition, 2016.
Ke Huang and Selin Aviyente. Wavelet feature selection for image classification. IEEE Transactions
on Image Processing, 2008.
Nick Kingsbury. Complex wavelets for shift invariant analysis and filtering of signals. APPlied and
comPutational harmonic analysis, 10(3):234-253, 2001.
Nick Kingsbury. Design of Q-shift complex wavelets for image processing using frequency domain
energy minimization. In Proceedings International Conference on Image Processing, volume 1,
pp. I-1013, 2003.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in Neural Information Processing Systems, pp. 1097-1105,
2012.
Andrew Laine and Jian Fan. Texture Classification by Wavelet Packet Signatures. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 1993.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521(7553):436-444,
2015.
Min Lin, Qiang Chen, and Shuicheng Yan. Network In Network. arXiv:1312.4400 [cs], 2014.
Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro
Perona, Deva Ramanan, C. LaWrence Zitnick, and Piotr Dollar. Microsoft COCO: Common
Objects in Context. arXiv:1405.0312 [cs], 2015.
Hongya Lu, Haifeng Wang, Qianqian Zhang, Daehan Won, and Sang Won Yoon. A Dual-Tree
Complex Wavelet Transform Based Convolutional Neural NetWork for Human Thyroid Medical
Image Segmentation. In IEEE International Conference on Healthcare Informatics (ICHI), pp.
191-198, 2018.
Shangzhen Luan, Chen Chen, Baochang Zhang, Jungong Han, and Jianzhuang Liu. Gabor Convo-
lutional NetWorks. IEEE Transactions on Image Processing, 27(9):4357-4366, 2018.
Stephane Mallat. A Wavelet Tour ofSignal Processing : The Sparse Way. Elsevier/Academic Press,
2009.
Edouard Oyallon, Eugene Belilovsky, Sergey Zagoruyko, and Michal Valko. Compressing the Input
for CNNs With the First-Order Scattering Transform. In Proceedings of the European Conference
on Computer Vision (ECCV), pp. 301-316, 2018.
Stefan Pittner and Sagar V. Kamarthi. Feature extraction from Wavelet coefficients for pattern recog-
nition tasks. IEEE Transactions on pattern analysis and machine intelligence, 21(1):83-88, 1999.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision,
115(3):211-252, 2015.
Syed Shakib SarWar, Priyadarshini Panda, and Kaushik Roy. Gabor filter assisted energy efficient
fast learning Convolutional Neural NetWorks. In 2017 IEEE/ACM International Symposium on
Low Power Electronics and Design (ISLPED), pp. 1-6, 2017.
10
Under review as a conference paper at ICLR 2021
Ivan W. Selesnick, Richard Baraniuk, and Nick Kingsbury. The dual-tree complex wavelet trans-
form. IEEE Signal Processing Magazine, 22(6):123-151, 2005.
Karen Simonyan and Andrew Zisserman. Very Deep Convolutional Networks for Large-Scale Image
Recognition. In International Conference on Learning Representations, 2015.
Amarjot Singh and Nick Kingsbury. Dual-Tree wavelet scattering network with parametric log
transformation for object classification. In ICASSP, IEEE International Conference on Acoustics,
Speech and Signal Processing - Proceedings, 2017.
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-
mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In
Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recog-
nition, 2015.
Matej Ulicny, Vladimir A. Krylov, and Rozenn Dahyot. Harmonic Networks for Image Classifica-
tion. In British Machine Vision Conference, pp. 202, 2019.
Travis Williams and Robert Li. Advanced Image Classification Using Wavelets and Convolutional
Neural Networks. In 2016 15th IEEE International Conference on Machine Learning and Appli-
cations (ICMLA), pp. 233-239, 2016.
Travis Williams and Robert Li. Wavelet Pooling for Convolutional Neural Networks. In Interna-
tional Conference on Learning Representations, 2018.
Gary G. Yen. Wavelet packet feature extraction for vibration monitoring. IEEE Transactions on
Industrial Electronics, 2000.
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How transferable are features in deep
neural networks? In Advances in Neural Information Processing Systems, pp. 3320-3328, 2014.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Robustness of our models with respect to small shifts
To assess the robustness of our models with respect to small shifts, we compared the network outputs
between a reference image and eight shifted versions along each axis, over our custom ImageNet test
set (50, 000 images). To do so, the Kullback-Leibler divergence is computed between each pair of
softmax activation vectors (∈ R1000) after forward-propagation through the network. An illustration
of the results can be found in Figure 5.
Figure 5: Shift-robustness of our models, compared to standard AlexNet. The Kullback-Leibler di-
vergence is computed between output vectors and averaged over the whole dataset (50, 000 images).
When the input image is shifted by 4 pixels, the output of the first layer is strictly shifted by one
pixel. The first layer is therefore invariant to a 4-pixel shift. Consequently, any divergence between
outputs should be due either to edge effects or to the action of deeper layers. Likewise, when the
shift is equal to 8, the invariance applies to the two first layers. However, when the shift is not a
multiple of 4, we can observe bigger discrepancies which depend on the chosen model.
The sensitivity to small shifts seems to increase with the network’s predictive power. On the other
hand, we observe higher discrepancies for WPT AlexNet and DT-RWPT AlexNet, compared to DT-
CWPT AlexNet. This is in agreement with our hypothesis that the nearly shift-invariance property
of DT-CWPT is - to some extent - conserved across the whole network, and therefore brings a
competitive advantage regarding predictive power. However, DT-CWPT AlexNet fails to reach the
shift-robustness of standard AlexNet, suggesting that further improvements could be brought to our
models (see Section 5).
A.2 Energy distribution over feature maps
Figure 6 displays the mean energies of the 30 feature maps of high-frequency DT-CWPT coeffi-
cients, computed over our custom ImageNet test set. As we can see, the energy distribution is very
unbalanced over the different filters.
A.3 Training and validation curves for VOC and COCO datasets
The evolution of the loss function and validation error along training for multilabel tasks is shown
in Figure 7. The graphs share similarities with Figure 4. These experiments suggest that DT-CWPT
AlexNet has good generalization performance on other recognition tasks.
We can notice the erratic aspect of the validation curves during the 25 first epochs. This may be due
to a poor learning rate initial setting. After decreasing this parameter, the validation errors become
more stable.
12
Under review as a conference paper at ICLR 2021
12000
IOOOO
A5」3U3 dEE 3,ln⅛3J UQ① W
Figure 6: Mean energies of the 30 feature maps of high-frequency DT-CWPT coefficients, computed
over our custom ImageNet test set.
Figure 7: Evolution of the loss function (left) and average error (right) over VOC and COCO vali-
dation sets (top and bottom, respectively).
13
Under review as a conference paper at ICLR 2021
A.4 IMPLEMENTATION OF THE DT-(RC)WPT MODULES
In this section we show that the dual-tree transforms can be written as a succession of CNN-style
convolution operators. We focus on DT-CWPT but it can be easily adapted to DT-RWPT.
The first step is to duplicate each input image four times (one for each filter bank). Given an input
tensor X ∈ RP ×3×224×224, this operation can be written as a CNN-style 1 × 1 convolution operator:
Xo = C(11 (VduPι, O) ∙ X,	(13)
where X0 ∈ RP×12×224×224 and Vdupl = I I I I ∈ R3×12×1×1, with I ∈ R3×3×1×1 such
that I[k, l, 0, 0] = 1ifk=l;
0 otherwise.
Then each level of filter bank decomposition can be summarized into a single CNN-style convolution
operator Csqjd即j, O), With (S = 2), (d = 1) and (qj = Kj), where Kj = (3 ∙ 4j+1)) denotes
the number of input channels. For any j ∈ {0, 1}, the weight tensor Wj ∈ R1×(4Kj)×μ×μ has the
following structure:
Wj[0]
Mj [叭
Wb,j [0]
Wc,j[0]
Wd,j[0]
(14)
where Wa,j, Wb,j, Wc,j and Wd,j are built similarly to the WPT module, using filter banks G(al) ,
G(bl) , G(cl) and G(dl) , respectively. By denoting D the output of this stage, we get
D = C(48)(W1, 0) ∙ (c21 2)(Wo, 0) ∙ Xo) .	(15)
Remark. Note that we have, for all sample p ∈ {0 . . P - 1},
D[p]
/Da[p]∖
Db[p]
Dc[p]
Dd[p]
(16)
Finally, expression (6) is expressed as a CNN-style 1 × 1 convolution operator:
EC = C(11 (Vcombine, O) ∙ D ,	(17)
where Vcombine ∈ R192×192×1×1 has the following structure:
	/I	O	O	-I\	
Vcombine =	O I	I O	I O	O I	,	(18)
	O	-I	I	O	
with
I ∈ R48×48×1×1 such that I[k, l, 0, 0]
1 if k = l;
0 otherwise;
• O ∈ R48×48×1×1 such that O[k, l, 0, 0] = 0forallk, l ∈ {0. . 47}.
Note that the two last dimensions of Wcombine have size 1 × 1; the “convolution” kernels are thus
reduced to singleton matrices.
Therefore, expressions (13), (15) and (17) provide a description of DT-CWPT as a succession of
CNN-style convolution operators.
14
Under review as a conference paper at ICLR 2021
A.5 An algorithm to compute the resulting weight and bias
Proposition 1 provides an iterative algorithm to compute the weight and bias resulting from a suc-
cession of CNN-style convolution operators. A special care must be paid to initialization. The
number of groups in the first convolution operator must indeed be equal to 1. In order to meet
this requirement, we introduce an identity operator C1(1, )1(I, 0), where I ∈ RK×K×1×1 is defined by
I[k,l,0,0]= 1ifk=l;	.
0 otherwise.
Proposition 2 (Identity operator). Let K, L ∈ N* and t, q ∈ N* such that both K and L are
divisible by q. Forall V ∈ R(K®×L×μ×ν and all a ∈ RL,
C(q)(V, a) = (c(q)(V, a)。或 1(I, 0)) .	(19)
Proof. It can be easily proven that for all X ∈ RP×k×n×n, C(11(I, 0) ∙ X = X.
□
Therefore Proposition 1 can be used on expression (19) in order to initialize the algorithm. The
details are given in Algorithm 1.
Algorithm 1 Composition of convolution operators
Require: K = L0 {number of input channels}
Require: {(L1, t1, g1, W1, b1), . . . , (LR, tR, qr, WR, bR)} {list of output channels, strides,
groups, weights and bias}
Ensure: ∀r ∈ {1 . . R}, both Lr-1 and Lr are divisible by qr
Lr μr Vr) and |br| = Lr
Lr-1
qr
Ensure: ∀r ∈ {1 . . R}, hWri
W — I ∈ Rk×k×1×1 {identity weight}
b J 0 {initial bias}
S J 1 {initial stride}
for r ∈ {1 . . R} do
WJC1(q,rs)(Wr, 0) ∙ W {resulting weight}
bJbr+ DPb(lqqrr)/Lrcb, SlWrE)l∈{0..L } {resulting bias}
s J s × tr {new stride}
end for
W J W {flip weight tensor along its 2 last dimensions}
return (s, W, b) {resulting stride, weight and bias}
A.6 Proof of Proposition 1
Proof. Let X ∈ RP×K×N×N denote an input tensor. Let Y ∈ RP×L×M×M and Y0 ∈
RP×L0×M0×M0 denote the outputs of the convolution operators, such that
Y = CsI) (W, b) ∙ X and Y0 = (c(q)(V, a) oCf,11(W, b)) ∙ X
=C(q)(V, a)(c(1)(W, b) ∙ X)	QO)
=C(q)(V, a) ∙ Y .
Let p ∈ {0 . . P - 1}. By using (3) and (20), we have, for all l0 ∈ {0 . . L0 - 1},
Y0[p,l0]= a[l0]+X(γ[p, lLq ∙ L + l]* VKTi) J t,	(21)
l=0	q
15
Under review as a conference paper at ICLR 2021
and, for all l ∈ {0 . . L - 1},
K-1
Y[p, l] = b[l]+ X(X[p, k] * WKTJ) J S .	(22)
k=0
The next steps require the two following lemmas:
Lemma 1. For all 2D matrices U, V , and all b ∈ R,
(b + U) * V = (b ∙ X V[m,n])+(U * V) .	(23)
m, n
Lemma 2. For all 2D matrices U, V, and all integers s,t ∈ N*,
((UJs)*V)Jt=(U*(V↑s))J(st) ,	(24)
where, as a reminder, (V ↑ s) denotes the s-dilated matrix.
Then, by plugging (22) into (21) and by using Lemma 1, we get
L/q-1
Y0[p, l0] = a[l0] + X
l=0
b0[l0]
-}------------------------{
• L + l ∙ XV[l, l0, m, n])
(25)
L/q-1
+X
l=0
K-1
X X[p, k] *W k,
k=0
l0q
L0
J S * V[l,l0]
Jt;
• l+l
and finally, by reversing the 2 sums and using Lemma 2, we get
/
∖
{
K-1
Y0[p, l0] = b0[l0] + X
k=0
L/q-1
X[p, k] * X W k,
l=0
W0[k,l0]
ʌ
l0q
L0
• L + l * V[l,l0] ↑ S
q
J (St) ;
(26)
∖
/
K-1	________
b0[l0] + X (X[p,k] * W0[k,l0]) J (St).
k=0
Hence
Y0=C((s1t)),1(W0, b0) •X	(27)
for all X, which proves (8).
By applying the definition of a CNN-style convolution operator (3) to the definition of W0 [k, l0] in
expression(26), We get
W = c(qS (V, 0) • W ,	(28)
Which proves (9).
Note that the expression of b0 [l0] defined in (25) can be reWritten in a more concise Way:
b0[l0] = a[l0] + DPb(lq0)q/L0cb, Sl0VE ,	(29)
Where
•	Pγ(q)b	γ∈{0..q-1}
denotes a partition of even-size slices of b. For any γ ∈ {0 . . q- 1},
P(q)b = b [2K ：(”K -1
γ	qq
(30)
16
Under review as a conference paper at ICLR 2021
• SlW ∈ RK/q denotes the vector such that for any k ∈ {0 . . K/q - 1},
(SlW)[k] =XW[k, l, m, n] .	(31)
m,n
Let’s denote f(V, b) ∈ RL0 such that f(V, b)[l0] = DPb(lq0)q/L0c b, Sl0 WE. Then we get f(V, 0) =
0, as stated in Proposition 1.
□
Remark. Operators Pγ(q) and Sl have the advantage of being efficiently computed with libraries such
as PyTorch or even NumPy.
A.7 ILLUSTRATION OF WPT AND DT-CWPT
Figures 8 and 9 illustrate WPT and DT-CWPT, respectively. We chose an image from our custom
ImageNet test set (one channel only), resized and cropped to 224 × 224 pixels. We can notice
that specific orientations tend to be selected by specific filters, especially in the dual-tree transform.
Moreover, some filters seem to extract features of higher energy than others.
17
Under review as a conference paper at ICLR 2021
Figure 8: Left: original image from ImageNet2012 (j = 0). Middle: WPT with j = 1. Right:
WPT with j = 2. At each step j , the feature maps of wavelet packet coefficients D(jk) are further
decomposed into four smaller submatrices D(j4+k1+l), for l ∈ {0 . . 3}, according to (1). To avoid
overexposure, scaling coefficients D(j0) have been set to 0 (top-left images).
Figure 9: Modulus of the complex DT-CWPT coefficients, computed with j = 2: Ej%(k) (left) and
Ej-(k) (right), for k ∈ {0 . . 15}. Numbering follows the order of Figure 8. To avoid overexposure,
scaling coefficients Ej%(0) and Ej-(0) have been set to 0 (top-left images).
18