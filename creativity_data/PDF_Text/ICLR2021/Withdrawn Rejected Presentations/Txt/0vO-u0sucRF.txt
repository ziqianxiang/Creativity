Under review as a conference paper at ICLR 2021
Information Theoretic Meta Learning with
Gaussian Processes
Anonymous authors
Paper under double-blind review
Ab stract
We formulate meta learning using information theoretic concepts such as mutual
information and the information bottleneck. The idea is to learn a stochastic rep-
resentation or encoding of the task description, given by a training or support set,
that is highly informative about predicting the validation set. By making use of
variational approximations to the mutual information, we derive a general and
tractable framework for meta learning. We particularly develop new memory-
based meta learning algorithms based on Gaussian processes and derive exten-
sions that combine memory and gradient-based meta learning. We demonstrate
our method on few-shot regression and classification by using standard bench-
marks such as Omniglot, mini-Imagenet and Augmented Omniglot.
1	Introduction
Meta learning (Ravi & Larochelle, 2017; Vinyals et al., 2016; Edwards & Storkey, 2017; Finn et al.,
2017; Lacoste et al., 2019; Nichol et al., 2018) and few-shot learning (Li et al., 2006; Lake et al.,
2011) aim to derive data efficient learning algorithms that can rapidly adapt to new tasks. Such
systems require training deep neural networks from a set of tasks drawn from a common distribution,
where each task is described by a small amount of experience, typically divided into a training or
support set and a validation set. By sharing information across tasks the neural network can learn to
rapidly adapt to new tasks and generalize from few examples at test time.
Several few-shot learning algorithms use memory-based (Vinyals et al., 2016; Ravi & Larochelle,
2017) or gradient-based procedures (Finn et al., 2017; Nichol et al., 2018), with the gradient-based
model agnostic meta learning algorithm (MAML) by Finn et al. (2017) being very influential in
the literature. Despite the success of specific schemes, one fundamental issue in meta learning is
concerned with deriving unified principles that can allow to relate different approaches and invent
new schemes. While there exist probabilistic interpretations of existing methods, such as the ap-
proximate Bayesian inference approach (Grant et al., 2018; Finn et al., 2018; Yoon et al., 2018) and
the related conditional probability modelling approach (Garnelo et al., 2018; Gordon et al., 2019),
meta learning still lacks of a general and tractable learning principle that can help to get a better
understanding of existing algorithms and derive new methods.
To this end, the main contribution of this paper is to introduce an information theoretic view of meta
learning, by utilizing tools such as the mutual information and the information bottleneck (Cover &
Thomas, 2006; Tishby et al., 1999). Given that each task consists of a support or training set and a
target or validation set, we consider the information bottleneck principle, introduced by Tishby et al.
(1999), which can learn a stochastic encoding of the support set that is highly informative about pre-
dicting the validation set. Such stochastic encoding is optimized through the difference between two
mutual informations, so that the encoding compresses the training set into a representation that can
predict well the validation set. By exploiting recent variational approximations to the information
bottleneck (Alemi et al., 2017; Chalk et al., 2016; Achille & Soatto, 2016) that make use of varia-
tional lower bounds on the mutual information (Barber & Agakov, 2003), we derive a general and
tractable framework for meta learning. Such framework can allow us to re-interpret gradient-based
algorithms, such as MAML, and also derive new methods.
Based on the variational information bottleneck (VIB) framework (Alemi et al., 2017; Chalk et al.,
2016; Achille & Soatto, 2016), we introduce a new memory-based algorithm for supervised few-
shot learning (right panel in Figure 1) based on Gaussian processes (Rasmussen & Williams, 2006)
1
Under review as a conference paper at ICLR 2021
Figure 1: (left) Meta learning with the information bottleneck. Zi 〜qw(Zi∣Dt) is the encoder We wish to
optimize to compress the task training set Dit (minimize the mutual information I(Zi, Dit)) and predict well the
validation set Div (maximize I (Zi , Div )); see Section 2. (right) Specialization to supervised few-shot learning
where for each task we have input-output data. Gradient-based algorithms, such as MAML, and Gaussian
process memory-based methods (proposed in this paper) are instances of the framework; see Section 2.2 and 3.
and deep neural kernels (Wilson et al., 2016) that offers a kernel-based Bayesian view of a memory
system. With Gaussian processes, the underlying encoding takes the form of a non-parametric func-
tion that follows a stochastic process amortized by the training set. Further, we show that VIB gives
rise to gradient-based meta learning methods, such as MAML, when combined with parametric
encodings corresponding to model parameters or weights, and based on this we derive a stochastic
MAML algorithm. In an additional scheme, we show that our framework can naturally allow for
combinations of memory and gradient-based meta learning by constructing suitable encodings, and
we derive such an algorithm that combines Gaussian processes with MAML. We demonstrate our
methods on few-shot regression and classification by using standard benchmarks such as Omniglot,
mini-Imagenet and Augmented Omniglot.
2	Meta Learning with Information B ottleneck
Suppose we wish to learn from a distribution of tasks. During training for each task we observe a
pair consisted of a task description represented by the support or training set Dt and task validation
represented by the target or validation set Dv . At test time only Dt will be given and the learning
algorithm should rapidly adapt to form predictions on Dv or on further test data.
We wish to formulate meta learning using information theoretic concepts such as mutual information
and the information bottleneck (Tishby et al., 1999). The idea is to learn a stochastic representation
or encoding of the task description Dt that is highly informative about predicting Dv. We introduce a
random variable, Z, associated with this encoding drawn from a distribution qw (Z |Dt) parametrized
by w. Given this encoding the full joint distribution is written as
qw(Dv, Dt, Z) =qw(Z|Dt)p(Dv,Dt),	(1)
where p(Dv, Dt) denotes the unknown data distribution over Dt and Dv. In equation 1 and through-
out the paper we use the convention that the full joint as well as any marginal or conditional that
depends on Z is denoted by qw (emphasizing the dependence on the parametrized encoder), while
corresponding quantities over data Dt , Dv are denoted by p. Eg. from the above we can express a
Z -dependent marginal such as, qw(Z, Dv) = R qw(Z|Dt)p(Dv, Dt)dDt.
To tune w we would like to maximize the mutual information between Z and the target set Dv , de-
noted by I(Z, Dv). A trivial way to obtain a maximally informative representation is to set Z = Dt,
which does not provide a useful representation. Thus, the information bottleneck (IB) principle
(Tishby et al., 1999) adds a model complexity penalty to the maximization of I(Z, Dv) which pro-
motes an encoding Z that is highly compressive of Dt, i.e. for which I(Z, Dt) is minimized. This
leads to the IB objective:
LIB(w) = I(Z, Dv) - βI(Z, Dt),	(2)
where β ≥ 0 is a hyperparameter. Nevertheless, in order to use IB for meta learning we need
to approximate the mutual information terms I(Z, Dv) and I(Z, Dt), which are both intractable
since they depend on the unknown data distribution p(Dv , Dt). To overcome this, we will consider
variational approximations by following similar arguments to the variational IB approach (Alemi
2
Under review as a conference paper at ICLR 2021
et al., 2017) that was introduced for supervised learning of a single task, which allows us to express
a tractable lower bound on LIB (w) by lower bounding I(Z, Dv) and upper bounding I(Z, Dt).
2.1	Variational Information Bottleneck (VIB) for Meta Learning
To construct a bound F ≤ LIB (w) we need first to lower bound I(Z, Dv), which is written as
I(Z,Dv) =KL[qw(Z,Dv)||qw(Z)p(Dv)] =Eqw (Z,Dv)
1 qw (Dv |Z)
log Rvr
(3)
where KL denotes the KUllback-Leibler divergence and qw (DV |Z) = R R qZ(DDp)DDD∖tDdDdDv is
intractable since we do not know the analytic form of the data distribution p(Dv , Dt). To lower
boUnd I(Z, Dv), we follow Barber & Agakov (2003) (see Appendix A.1) by introdUcing a decoder
model pθ(Dv |Z) to approximate the intractable qw(Dv |Z) where θ are additional parameters1,
I(Z,Dv) ≥Eqw(ZDv)
1 Pθ (Dv |Z)]
log Kr J
Eqw (ZDv)[log Pθ (Dv |Z)] + H(Dv),	(4)
where the entropy H(Dv) is jUst a constant that does not depend on the tUnable parameters (θ, w).
FUrthermore, to deal with the second intractable mUtUal information I(Z, Dt) so that to maintain an
lower boUnd on LIB(w) we need to Upper boUnd this term. Note that
I(Z, Dt) = Eqw(ZD)
1	qw (Z|Dt)]
log----77^—
qw(Z)
where qw(Z) = qw(Z|Dt)p(Dt)dDt is intractable since, e.g. it involves the Unknown data dis-
tribUtion p(Dt). By working similarly as before, we can approximate qw (Z) by a tractable prior
model distribUtion pθ (Z) which leads to the following Upper boUnd on the mUtUal information,
"7 C八 V 而	q	qw(ZIDt)
I(Z，D ) ≤ Eqw (Z,Dt) [log pθ (Z)
(5)
Then, by combining the two boUnds we obtain the overall boUnd, F(θ, w) + H(Dv ) ≤ LIB (w):
F(θ,w) = Eqw(ZDv) [logPθ(Dv |Z)] - βEqw(z,Dt) log
qw(Z|Dt)
pθ(Z)
where the constant H(Dv ) is dropped from the objective fUnction. Given a set of task pairs
{Dt, DV}b=1, where each (Dt, DV)〜p(Dv, Dt), during meta-training the objective function for
learning (θ, W) reduces to the maximization of the empirical average, 1b Ei Fi(θ, w), where each
Fi(θ, w) is an unbiased estimate of F(θ, w) (see Appendix A.2) and is given by
Fi(w,θ)= Eqw(Zi∣Dt) [logPθ(DVIZi)] - βKL[qw(Zi∣D∙t)I∣Pθ(Zi)].	(6)
The meta-training procedure is carried out in different episodes where at each step we receive a
minibatch of task pairs and perform a stochastic gradient maximization step.
The objective in equation 6 is similar to variational inference objectives for meta learning (Ravi
& Beatson, 2019). In particular, it can be viewed as an evidence lower bound (ELBO) on the
validation set log marginal likelihood, log pθ(DivIZi)pθ(Zi)dZi, with the differences: (i) there
is the hyperparameter β in front of the KL term and (ii) the variational distribution qw(ZiIDit) in
equation 6 is more restricted than in standard variational inference, since qw(ZiIDit) now acts as a
stochastic bottleneck that encodes the support set Dit (i.e. it is amortized by Dit) and via the term
Eqw (Zi |D) [logpθ(DivIZi)] it is optimized to reconstruct the validation set.
2.2	Information Theoretic View of MAML-type Methods
MAML (Finn et al., 2017) is a special case of our framework. To see this, suppose that the task
encoding variable Zi for the i-th task coincides with a vector of some task-specific model param-
eters or neural network weights ψi, so that pθ(Div IZi) ≡ p(Div Iψi) and pθ(Zi) reduces to a prior
1The lower bounds are valid even when the parameters w of the encoder qw(Z|Dt) and θ of the decoder
pθ (Dv |Z) (and prior pθ (Z)) have shared components, e.g. are parameters of the same neural architecture.
3
Under review as a conference paper at ICLR 2021
pθ(ψi) over these parameters. The MAML approach (Finn et al., 2017) tries to find a shared initial
parameter value θ so that few gradient steps based on the support set objective, logp(Dt∣θ), lead
to a task-specific parameter value ψi with good generalization on the validation set. MAML esti-
mates the task parameters by ψi = θ + ∆(θ, Dit), where ∆(θ, Dit) denotes the inner loop adaptation
steps which for a single gradient step is just ρVθ logp(Dt∣θ) and where P is a step size. By setting
β = 0 and by using a deterministic Dirac measure encoder, δψi,θ+∆(θ,Dt), the VIB objective from
equation 6 reduces to the standard MAML objective, Fi(θ) = logP(Dv ∣θ + ∆(θ, Dtt)).
We can construct a generalization of MAML by making the encoder stochastic, e.g. qθ,s (ψi |Dt)=
N(ψi∣θ + ∆(θ, Dt), SI) where S is a scalar variance parameter. Then, the objective becomes
Fi(θ,s) = EN(e∣o,i) [logP(Dv ∣θ + ∆(θ, Dt) + √S6)] - βKL [qθ,s(ψiDt)∣∣Pθ(ψi)],
where we reparametrized the expectation suggesting the use of the reparametrization trick (Kingma
& Welling, 2013) for stochastic optimization of the meta parameters (θ, S). In the experiments
we will investigate an instance of the above where Pθ (ψi) follows the hierarchical Gaussian form
N(ψi∣θ, sI), which views each task-specific parameter ψi as a randomized version of θ and with
S being the same variance used by the encoder. For such prior the KL divergence term reduces to
一 2is ∣∣∆(θ, Dt)||2, which penalizes large values of the inner adaptation steps and small values of s.
2.3	VIB for Supervised Meta Learning
Here, we explain how to adapt the VIB principle to supervised meta learning. Suppose the learning
problem involves few-shot supervised learning where for each task we wish to predict outputs or
labels given corresponding inputs. Let us denote the task support set as Dt = (Yt, Xt), where
Y t = {yjt }jn=t 1 and Xt = {xtj }jn=t 1 denote the output and input observations. Similarly, we write
Dv = (Y v , Xv ) for the validation set. During meta-testing, for any novel task we observe the sup-
port set Dl = (Y^t, X=t) together with the test inputs Xv and the goal is to predict the test outputs
Yv. This suggests that we can construct a task encoder distribution of the form qw (Z|Yt, X) that
depends on the training outputs Yt and generally on all inputs X = (Xt, Xv).2 We would like to
train this encoder so that Z becomes highly predictive about the validation outputs Y v and simulta-
neously compressive about Y t . Then, a suitable VIB objective can be based on approximating the
input-conditioned information bottleneck, I(Z, Yv |X) - βI(Z, Yt|X) i.e. where both I(Z, Yv |X)
and I(Z, Y t |X ) are conditional mutual informations (see Appendix A.3). By following similar ar-
guments as those in Section 2.1, we can lower bound this objective and finally approximate it by an
unbiased empirical average, b Pb=I Fei(θ, w), where
Fi(θ,w) = Eqw(Zi∣γt,Xi) [logPθ(YV∣Zi,Xi)] -βKL [qw(ZiKt,Xi)MZXi)],	⑺
and wherepθ(YiV∣X∕, Zi) andpθ(Z/Xi) are the decoder and prior model distributions introduced
when applying the variational approximation. A detailed derivation can be found in Appendix A.3.
Equation 7 provides the general form of the VIB objective suitable for supervised meta learning. The
supervised version of MAML is expressed as a special case by following the arguments of Section
2.2. In Section 3, we particularize the above by combining it with a Gaussian process model.
3	Supervised Meta Learning with Gaus sian Processes
In this section we introduce VIB-based meta learning algorithms using Gaussian processes (GPs),
which are suitable for few-shot supervised learning. In Section 3.1 we introduce a memory-based
system, while in Section 3.2 we further combine it with gradient-based meta learning.
3.1	A Gaussian Process Memory-based Method
To use the VIB framework for few-shot supervised learning, as described in Section 2.3, for each
i-th task we need to specify the encoding variable Zi together with the encoder qw(Zi|Yit, Xi),
the decoder over the validation outputs pθ(YV|Zi, Xi) and the prior model pθ(Zi∣Xi). Here, we
construct these quantities by using a GP model (Rasmussen & Williams, 2006).
2Dependence on all inputs can allow to explain both transductive and non-transductive settings for meta
learning (Bronskill et al., 2020; Finn et al., 2017; Nichol et al., 2018) as special cases; see Appendix B.
4
Under review as a conference paper at ICLR 2021
We denote the unknown task-specific function that solves the i-th task by fi (x) and we assume
that a priori (before observing any task data) this function is a draw from a GP, i.e. f (x) 〜
GP(0, kθ(x, x0)), where kθ denotes the kernel function. Without loss of generality we shall use
a deep kernel function where fi (x) is a linear function of a deep neural network feature vector
φ(χ; θ) with task-specific Gaussian weights, i.e. fi(x) = φ(χ; θ)τθθut,θθut 〜N(θθut∣0, σfI).
Such function can be equivalently viewed as a GP sample: fi(x)〜 GP(0, kθ(x, χ0)), k§(x, χ0)=
σf2φ(x; θ)>φ(x0; θ). In this functional space view the task-specific output weights θiout have been
marginalized out and we are left with the feature vector parameters θ shared across tasks.3
Suppose now that we observe the task data, i.e. the support Dit = (Yit, Xit) and validation Div =
(Yiv, Xiv) sets, so that we can evaluate the task function on all task inputs Xi = (Xit, Xiv). Let
fiv,j ≡ f (xiv,j ) denote the function value at the validation input xiv,j, associated with output yiv,j,
while the vector of all such values is denoted by fiv = {fiv,j}jn=v 1. Similarly, the vector of function
values at the training inputs Xit is fit . In the VIB framework, we specify the task encoding variable
Zi to be the full set of function values, Zi ≡ (fiv, fit), and we further choose the prior model for this
encoding to be the GP prior, pθ (Z/Xi) ≡ p(f, ft∣Xi), where
p(fv, ft∣Xi)= p(fvft,Xi)× p(fitXt),	(8)
=N(fv ∣KVtKt]-1fit, Kv - KVt[Kt]T[KVt]>) ×N "O, Kt).	(9)
Here, Kit , Kiv are square kernel matrices of size nt × nt and nv × nv on the training inputs Xit and
validation inputs Xiv , while Kivt is the nv × nt cross kernel matrix between these two sets of inputs.
Note that the encoding is non-parametric since its size grows with the number of task data points.
To continue with the specification of the VIB objective, the second quantity we need to set is the
decoder model pθ(Yiv |fiv, fit, Xi) which is chosen to be a standard GP likelihood. Specifically, for
i.i.d. observations Yiv given fiv becomes independent from fit and Xi and the likelihood factor-
v
izes across data points, i.e. p(Yiv|fiv) = jn=1 p(yiv,j |fiv,j), where each p(yiv,j |fiv,j) is a standard
likelihood model, such as Gaussian density p(yiv,j|fiv,j) = N (yiv,j |fiv,j, σ2) suitable for standard re-
gression problems or categorical/softmax likelihood for few-shot classification; see Appendix C.4.
Finally, we specify the encoder distribution qw(Zi|Yit, Xi) ≡ q(fiv, fit|Yit, Xi) as follows,
q(fiv, fit|Yit, Xi) = p(fiv|fit, Xi)q(fit|Dit),	(10)
where p(fiv|fit|Xi) is the same conditional GP prior from equation 8, while q(fit|Dit) is a encoder
of the training set that takes the form of a Gaussian distribution specified based on a very general
amortization procedure, see Appendix C.1. Equation 10 shares a similar structure with a standard
posterior Gaussian process where we first observe the training set, then we compute the (approxi-
mate) posterior distribution q(fit|Dit), and finally we extrapolate/predict the validation set function
values at inputs Xiv based on the conditional GP prior p(fiv|fit, Xi). The above assumptions yield
(see Appendix C.2) the following VIB objective for a single task,
Xn=IEqfvj)[logp(yVjfVj)] - βKL [q(fit∣Dt)∣∣p(ft∣Xt)] ,	(11)
where q (fiv,j ) = R p(fiv,j |fit, xiv,j , Xit)q(fit|Dit)dfit is a univariate Gaussian over an individual vali-
dation function value fiv,j . Here, q (fiv,j ) depends on the training set and the single validation input
xiv,j , so that from the training set and the corresponding function values fit we extrapolate (through
the univariate conditional GP p(fiv,j |fit, xiv,j , Xit)) to the input xiv,j in order to predict its value fiv,j .
3.2 Combination with Gradient-based Meta Learning
In this section, we combine the GP memory-based meta learning method with a gradient-based
approach, such as MAML (Finn et al., 2017). Based on the VIB principle we need to specify
an encoding that will allow us to combine a memory-based with a gradient-based approach. As
discussed in Section 2.2, gradient-based meta learning is associated with a parametric encoding
that corresponds to a fixed-size task model parameter ψi. In contrast, as seen in Section 3.1, a
memory system is associated with a function-space or non-parametric encoding that consists of the
3The kernel variance parameter σf2 (if learnable) is also considered to be part of the full set of parameters θ.
5
Under review as a conference paper at ICLR 2021
function values (fit , fiv) of all task input points. Therefore, a way to combine these techniques is to
concatenate the encodings, i.e. Zi ≡ (ψi , fit , fiv ). Here, ψi are the task-specific parameters of the
GP kernel function kψi (x, x0) (and possibly of the likelihood p(y |f)), which in our implementation
are the parameters of the feature vector φ(x; ψi). Intuitively, a combination of the GP memory-
based method with MAML will try to apply a short inner adaptation loop in order to adjust an
initial feature vector φ(x; θ) to obtain a final φ(x; ψi) that can better solve the task. For the overall
encoding (ψi, fit, fiv), the general form of the encoder distribution takes the form
q(ft, fv,ψi∣Yit,Xi)= p(fv ∣fit,ψi,Xi)q(fit∣ψi,Dt)q(ψi∣Dt),
where p(f ∣fit, ψi,Xi) is the conditional GP prior and q(f1t∖ψi, Dt) is the amortized encoder (See
Appendix C.1), where we have emphasized their dependence on ψi. The VIB objective becomes
-nv	一
Eq(Ψi∣Dt) XEq(fivjlΨi) [logp(yv,j∖fV,j)]-βKL[q(fit∖Ψi,Dt)∖∖p(fit∖Ψi,Xt)] -βKL[q(ψi∖Dt)∖∖p(ψi)].
j=1
In practise, we can relax this objective and use different hyperparameters βf and βψ in front of
the two KL terms. This is convenient as we would prefer to set βψ = 0 and use a deterministic
MAML w.r.t. ψi rather than a stochastic MAML; see Section 2.2. This simplification avoids the
need to specify a prior p(ψi) over the task-specific neural network parameters and at the same time
it reduces the encoder q(ψi ∖Dit ) to a Dirac delta, which simplifies the objective as follows,
nv
Ej=IEq(fv”,)[logp(yV,j∖fVj)] - βfKL [q(fit∖ψi,Dt)∖∖p(ft∖ψi,Xt)],
where ψi = θ + ∆(θ, Dit). The inner loop adaptation term can be defined by an objective function
on the support set Dit. In our case a suitable objective is the VIB for the GP memory-based method
obtained by setting the validation set equal to the training set, i.e. Div = Dit in equation 11, which
t
gives pj=ι Eq(ft,j∣θ)[logp3,j∖fit,j)] - βfKL [q(ft∖θ, Dt)Mfit∖θ,χit)].
4	Related Work
In this work, the VIB principle was used to formulate meta learning. VIB has been used before for
different purposes, such as for regularization of single-task supervised learning (Alemi et al., 2017),
sparse coding (Chalk et al., 2016), re-interpretation of β-VAEs and Dropout (Burgess et al., 2018;
Achille & Soatto, 2016) and for compression of deep neural networks (Dai et al., 2018).
A meta learning method that connects with the information bottleneck was recently proposed by Hu
et al. (2020). There the information bottleneck was used to analyze the generalization of a variational
Bayesian inference objective suitable for transductive supervised few-shot learning. Note that the
information bottleneck derived in Hu et al. (2020) (theorem 1) is not the same as the information
bottleneck objective used here (i.e. the objective in equation 7 for the supervised learning case),
since they differ in the second term. In addition, our framework expresses a general information
bottleneck principle for meta learning applicable beyond transductive supervised learning.
Given the probabilistic nature of our framework, we can relate it to other probabilistic or Bayesian
approaches and particularly with those that: (i) probabilistically re-interpret and extend gradient-
based methods (Grant et al., 2018; Finn et al., 2018; Yoon et al., 2018; Nguyen et al., 2019;
Gordon et al., 2019; Chen et al., 2019) and (ii) derive amortized conditional probability mod-
els (Garnelo et al., 2018; Gordon et al., 2019). The underlying learning principle in both (i)-
(ii) is to construct and maximize a predictive distribution (or conditional marginal likelihood)
of the validation points given the training points, which, e.g. in supervised few-shot learning is
written as pθ(YV∖XV,Xt,Yt) = Rp(YV∖XV,ψi)pθ(ψi∖Xt,Yt)dψi = Pp*却Xft. Here,
pθ(ψi∖Xt, Y t) is a posterior distribution over the task parameters ψi, after observing the training
points, and θ is a meta parameter which for simplicity we assume to be found by point estimation.
However, this objective is very hard to rigorously approximate. Unlike the marginal likelihood on
all task outputs pθ(YV, Yt∖XV, Xt) for which we can easily compute a lower bound, there is no
tractable lower bound on the predictive conditional pθ (YV ∖XV, Xt, Yt).4 This inherent difficulty
4To obtain such a bound We either need to have access to the intractable posterior pθ(ψi∣Xt,Yt), or to
upper bound the marginal likelihood on the training points pθ (Yt |Xt), which is very hard.
6
Under review as a conference paper at ICLR 2021
with computing the predictive distribution has led to several approximations, i.e. methods of cat-
egory (i) above, ranging from MAP, Laplace, variational inference procedures (without rigorous
bounds on the predictive conditional) and Stein variational gradient descent (Grant et al., 2018; Finn
et al., 2018; Yoon et al., 2018; Nguyen et al., 2019; Gordon et al., 2019; Chen et al., 2019). The con-
ditional probability modelling approaches (Garnelo et al., 2018; Gordon et al., 2019) try to directly
model pθ (YV |XV ,Xt, Yt) without considering this as an approximation to an initial joint Bayesian
model. Our VIB framework differs significantly from the predictive distribution principle since VIB
has an information theoretic motivation and it rigorously bounds an information bottleneck objec-
tive. VIB is also a fully tractable objective, avoiding the need to choose a particular approximate
inference method and allowing us to rather focus on setting up the encoding procedure, as we did in
the GP example in Section 3.
Regarding related works of GPs in meta learning, the ALPaCA method (Harrison et al., 2018) ap-
plied GPs and Bayesian linear regression to standard regression tasks with Gaussian likelihoods,
while Tossou et al. (2019) used kernel-based methods (from a regularization rather than Bayesian
perspective) again in standard regression. Closer to us, Patacchiola et al. (2019) and Snell & Zemel
(2020) used GPs to perform few-shot classification together with deep neural kernels. However, our
usage of GPs is different from these latter approaches, e.g. our general encoder amortization strategy
can potentially deal with arbitrary likelihood functions and task output observations, while Patacchi-
ola et al. (2019) assumes a Gaussian likelihood for the binary class labels and Snell & Zemel (2020)
consider the Polya-Gamma augmentation, which is very tailored to classification problems.
5	Experiments
To evaluate the proposed algorithms, we consider a standard set of meta-learning benchmarks which
include sinusoid regression and few-shot classification. As a baseline for comparison we consider
MAML (Finn et al., 2017) and use exactly the same neural architecture for all methods and bench-
marks as in Finn et al. (2017). The new methods we implemented are the following: (i) Stochastic
MAML (S. MAML), as defined in Section 2.2, where the difference with MAML is the injected
task-specific noise added to the outer loop update rule, together with the regularization term that
comes from the VIB objective; see Section 2.2. The scalar noise parameter s is learned alongside
the others. (ii) The memory-based GP method (referred as GP) trained by the VIB objective, as
defined in Section 3, where the kernel feature vector φ(x; θ) is obtained from the last hidden layer of
the same neural architecture as used in MAML. We use cosine and linear kernels (see Appendix D).
For few-shot classification we report results separately for both kernels, while for sinusoid regression
we only consider the linear kernel (the cosine kernel has similar performance). (iii) A combination
of a memory-based and gradient-based approach referred to as GP+MAML where a MAML meta
update rule is applied to the feature vector parameters θ; see Section 3.2.
Sinusoid regression. We first evaluate the method on a sinusoid regression as described by Finn
et al. (2017) following their protocol. We train GP using VIB and evaluate its K-shot mean squared
error performance. From Table 1, we observe that GP significantly outperforms MAML especially
as K grows. Similarly, Figure 3 in Appendix illustrates this few-shot predictive ability of the GP,
where posterior GP uncertainty reduces as K grows. Finally, from Figure 2 (left), we observe
that GP drastically improves when more data is available and is more data efficient than MAML
achieving near-optimal performance for K = 5. See Appendix D for more results and details.
Table 1: Few-shot sinusoid regression re-
sults. We report the results (mean values and
95% confidence intervals after 10 repeats) of
all methods (GP, GP+MAML, MAML and S.
MAML) for K = 5, 10, 20. GP-based meth-
ods use linear kernel.
Model	K=5	K=10	K=20
MAML	0.280±0.013 S. MAML 0.317±0.34 GP	0.02±0.014 GP+MAML 0.058±0.054		0.096±0.005 0.116±0.012 0.002±0.001 0.002±0.001	0.043±0.003 0.054±0.004 0.001±0.001 0.002±0.002
Few-shot classification. The second domain is a standard few-shot meta-learning benchmark based
on three datasets: Omniglot (Lake et al., 2011), mini-Imagenet (Ravi & Larochelle, 2017) and Aug-
mented Omniglot (Flennerhag et al., 2018). For both Omniglot and mini-Imagenet we follow the
experimental protocol proposed by Finn et al. (2017). For Augmented Omniglot, following Flen-
nerhag et al. (2018); Chen et al. (2019), during meta-testing both MAML and S. MAML perform
7
Under review as a conference paper at ICLR 2021
(a)
(c) meta-trained with N = 5, K = 5
(d)
Figure 2: (a) Sinusoid regression with GP and MAML in meta-testing as the number of shots K (x-axis)
increases. On y-axis, we report the MSE. For MAML, we report the performance with different number of
inner loop steps, i.e. just SGD steps since we do meta-testing, specified in the legend. (b)-(c) Meta-testing
classification accuracy (y-axis) on mini-Imagenet, where each system has been meta-trained with either N =
5, K = 1 or N = 5, K = 5, as the number K of observed examples per class (while always N = 5) grows
from 1 to 20, e.g. for K = 20 each system sees N × K = 100 support examples. For MAML we show
the performance for different inner loop sizes, which in meta-testing is just SGD updates, where each SGD
step uses a random mini-batch of size 10 data points from the support set of size N × K. (d) Similarly to
(b)-(c) for Augmented Omniglot, where instead of growing K we increase the amount of data augmentation
in a pre-specified/fixed N = 20, K = 15 initial support set. This means in each SGD update of MAML or
predictive density GP update we sample a mini-batch from the fixed N × K support set, we apply random
transformations in this mini-batch and then we use it to perform the actual update. The mini-batch size was 20
and based on this data augmentation process we grow the amount of data (x-axis) processed by each method
from 20 up to 2000. Finally to create all plots we average performance under 10 repeats, where in each repeat
the systems are meta-trained from scratch and then are evaluated in a large number of meta-testing tasks.
	Omniglot 5-way	mini-Imagenet 5-way	Augmented Omniglot 20-way
Model	K = 1 shot	K = 5 shot	K = 1 shot	K = 5 shot	K = 15 shot
Stochastic MAML GP (linear) GP (cos) GP+MAML (linear) GP+MAML (cos)	98.944±0.031%	99.745±0.017%	48.139±0.311%	64.179±0.781%	77.3±1.123% 99.036±0.034%	99.776±0.019%	48.097±0.213%	64.474±0.235%	80.73±0.776% 99.059±0.030%	99.774±0.016%	47.934±0.293%	63.660±0.262%	80.77±0.804% 99.027±0.025%	99.771±0.019%	48.509±0.346%	64.760±0.226%	81.33±0.488% 99.051±0.028%	99.783±0.019%	48.075±0.445%	64.506±0.178%	*81.79±0.671%
MAML (Finn et al., 2017)	98.7±0.4%	99.9±0.1%	48.7±1.84%	63.11±0.92%	76.67±0.663% (OUr)
Table 2: Classification test accuracy on Omniglot, mini-Imagenet and Augmented Omniglot. For all methods,
mean performances with 95% confidence intervals are reported after repeating the experiments 10 times and
each experiments performs meta-testing in 1000 tasks. Best performance is with bold, while * indicates statis-
tically significant better performance than MAML in Augmented Omniglot. For Augmented Omniglot we run
MAML (see Appendix for the hyperparameters) since this dataset was not included in Finn et al. (2017).
100 steps of adaptation (resulting in 2000 data points seen by the model where each step processes
a minibatch of size 20 points), while they are meta-trained by applying 20 adaptation steps (i.e.
400 training points seen per task). Both GP methods are meta-trained by memorizing the full
N ×K = 20×15 = 300 support points without further data augmentation, while during meta-testing
we allow the GP methods to see up to 2000 points. See Appendix D for more details.
8
Under review as a conference paper at ICLR 2021
	Omniglot 5-way		mini-Imagenet 5-way		Augmented Omniglot 20-way
Model	K = 1 shot	K = 5 shot	K = 1 shot	K = 5 shot	K = 15 shot
Stochastic MAML	0.031±0.001%	0.008±0.001	1.27±0.008%	0.925±0.013%	0.673±0.025%
GP (linear)	0.036±0.002%	0.012±0.001%	1.267±0.008%	0.904±0.005%	0.676±0.034%
GP (cos)	0.045±0.001%	0.019±0.001%	1.262±0.006%	0.921±0.006%	0.662±0.027%
GP+MAML (linear)	0.036±0.003%	0.010±0.001%	*1.246±0.007%	*0.900±0.009%	0.671±0.024%
GP+MAML (cos)	0.045±0.001%	0.019±0.001%	1.274±0.009%	0.902±0.005%	*0.616±0.027%
MAML (our)	0.032±0.001%	0.008±0.001%	1.279±0.006%	0.926±0.011%	0.694±0.02%
Table 3: Classification negative log likelihood (NLL) test performance on Omniglot, mini-Imagenet and Aug-
mented Omniglot. To obtain these scores for MAML we re-run MAML since only classification accuracy is
reported in Finn et al. (2017). Again * indicates statistically significant better performance than MAML.
Classification accuracy performance for all methods are given in Table 2, while the corresponding
negative log likelihood (NLL) scores are given in Table 3. We observe GP-based architectures out-
perform MAML and S. MAML in more complex scenarios such as Augmented Omniglot. From the
NLL scores that depend on how well the predicted class probabilities are calibrated, we can observe
that the GP methods perform significantly better in all cases where uncertainty matters i.e. on mini-
Imagenet and Augmented Omniglot. On top of the described standard implementations of GP and
GP+MAML, we implement ones with additional previously used architectures and tricks to improve
results (see Appendx D.4). We observe that GP+MAML generally performs better than GP. Further,
we notice that S. MAML has similar performance to MAML. One reason why S. MAML does not
always outperform MAML could be related to hyperparameters and to the higher variance of the
gradients caused by the reparametrization trick used to maximize the VIB objective in equation 7.
This could imply that training a stochastic architecture could require longer training time, an issue
deserving further investigation.
We also found that the qualitative behaviour of GP and GP+MAML are quite different as shown in
Figure 4 in the Appendix D.2. In Appendix D.3 we provide ablative analysis for the impact of β
on the performance of our architecture where the main result is that a large range of β values gives
similar performance in practice.
Data efficiency in meta-testing of gradient-based vs GP-based meta learning. Having performed
an expensive meta-training phase, the ultimate goal of a meta-learning system is to be deployed in
practice and solve new tasks. In a real meta-testing scenario it is more likely the system to operate
in a regime, where the training data for a given task is coming sequentially in mini-batches, and the
system should continuously update itself. With this in mind, it is interesting to study the effect of
K shots (or more generally the effect of the amount of processed data) in meta-testing performance.
In Figure 2, we carry out an ablation study by comparing MAML and GP in sinusoid regression,
mini-Imagenet and Augmented Omniglot by varying either K, for sinusoid regression and mini-
Imagenet, or the amount of data augmentation, for Augmented Omniglot. We found that GP can
be much more data efficient than MAML. This is because the GP predictive updates are based on
Bayesian updating of sufficient statistics (for the linear kernels used here) which are similarly to
Bayesian linear regression, as described in detail in Appendix C.3. Such updates are exchangeable
(i.e. data order does not matter on the final prediction) and do not depend on learning rates. In
contrast, a gradient-based method such as MAML can be less data efficient since stochastic gradient
updates depend on the learning rate, mini-batch size, data order and the size of the inner loop.
6 Conclusion
We introduced an information theoretic framework for meta learning by using a variational approxi-
mation (Alemi et al., 2017; Chalk et al., 2016; Achille & Soatto, 2016) to the information bottleneck
(Tishby et al., 1999). We derived a novel memory-based meta learning method with GPs, a stochas-
tic MAML and a combination of memory and gradient-based meta learning.
While we have demonstrated our method in few-shot regression and classification, we believe that
the scope of the information bottleneck for meta learning is much broader. For instance, an interest-
ing topic for future research is to consider applications in reinforcement learning.
9
Under review as a conference paper at ICLR 2021
References
Alessandro Achille and Stefano Soatto. Information dropout: Learning optimal representations
through noisy computation, 2016.
Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy. Deep variational information bottleneck.
In ICLR, 2017.
David Barber and Felix Agakov. The im algorithm: A variational approach to information maxi-
mization. In Proceedings of the 16th International Conference on Neural Information Processing
Systems, NIPS'03,pp. 201-208, Cambridge, MA, USA, 2003. MIT Press.
John Bronskill, Jonathan Gordon, James Requeima, Sebastian Nowozin, and Richard E. Turner.
Tasknorm: Rethinking batch normalization for meta-learning, 2020.
Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in β-vae, 2018.
Matthew Chalk, Olivier Marre, and Gasper Tkacik. Relevant sparse codes with variational infor-
mation bottleneck. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett (eds.),
Advances in Neural Information Processing Systems 29, pp. 1957-1965. Curran Associates, Inc.,
2016.
Yutian Chen, Abram L. Friesen, Feryal Behbahani, Arnaud Doucet, David Budden, Matthew W.
Hoffman, and Nando de Freitas. Modular meta-learning with shrinkage, 2019.
Thomas M. Cover and Joy A. Thomas. Elements of Information Theory (Wiley Series in Telecom-
munications and Signal Processing). Wiley-Interscience, USA, 2006. ISBN 0471241954.
Bin Dai, Chen Zhu, Baining Guo, and David P. Wipf. Compressing neural networks using the vari-
ational information bottleneck. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, ICML 2018, Stockholmsmassan, Stock-
holm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pp.
1143-1152. PMLR, 2018.
Harrison Edwards and Amos Storkey. Towards a neural statistician. In 5th International Conference
on Learning Representations (ICLR 2017), 4 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp.
1126-1135, International Convention Centre, Sydney, Australia, 06-11 Aug 2017. PMLR.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in
Neural Information Processing Systems 31, pp. 9516-9527. Curran Associates, Inc., 2018.
Sebastian Flennerhag, Pablo G. Moreno, Neil D. Lawrence, and Andreas Damianou. Transferring
knowledge across learning processes, 2018.
Sebastian Flennerhag, Andrei A. Rusu, Razvan Pascanu, Francesco Visin, Hujun Yin, and Raia
Hadsell. Meta-learning with warped gradient descent, 2020.
Marta Garnelo, Dan Rosenbaum, Christopher Maddison, Tiago Ramalho, David Saxton, Murray
Shanahan, Yee Whye Teh, Danilo Rezende, and S. M. Ali Eslami. Conditional neural processes.
VolUme 80 of Proceedings of Machine Learning Research, pp. 1704-1713, Stockholmsmassan,
Stockholm Sweden, 10-15 Jul 2018. PMLR.
Jonathan Gordon, John Bronskill, Matthias Bauer, Sebastian Nowozin, and Richard Turner. Meta-
learning probabilistic inference for prediction. In International Conference on Learning Repre-
sentations, 2019.
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths. Recasting gradient-
based meta-learning as hierarchical bayes. In International Conference on Learning Representa-
tions, 2018.
10
Under review as a conference paper at ICLR 2021
James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online
bayesian regression, 2018.
James Hensman, Max Zwiessele, and Neil D. Lawrence. Tilted variational bayes. In Proceedings
of the Seventeenth International Conference on Artificial Intelligence and Statistics, AISTATS
2014, Reykjavik, Iceland, April 22-25, 2014, volume 33 of JMLR Workshop and Conference
Proceedings,pp. 356-364.JMLR.org, 2014.
Xu Hu, Pablo Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, and Neil Lawrence. Empirical
bayes transductive meta-learning with synthetic gradients. ICLR, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Alexandre Lacoste, Boris Oreshkin, Wonchang Chung, Thomas Boquet, Negar Rostamzadeh, and
David Krueger. Uncertainty in multitask transfer learning, 2019.
Brenden M. Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua B. Tenenbaum. One shot learning
of simple visual concepts. Cognitive Science, 33, 2011.
Fei-Fei Li, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Trans.
Pattern Anal. Mach. Intell., 28(4):594-611, 2006.
Cuong Nguyen, Thanh-Toan Do, and Gustavo Carneiro. Uncertainty in model-agnostic meta-
learning using variational inference, 2019.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms, 2018.
Manfred Opper and Cedric Archambeau. The variational gaussian approximation revisited. Neural
Comput., 21(3):786-792, March 2009. ISSN 0899-7667.
Massimiliano Patacchiola, Jack Turner, Elliot J. Crowley, Michael O’Boyle, and Amos Storkey.
Deep kernel transfer in gaussian processes for few-shot learning, 2019.
Carl Edward Rasmussen and Christopher KI Williams. Gaussian Processes for Machine Learning.
MIT Press, 2006.
Sachin Ravi and Alex Beatson. Amortized bayesian meta-learning. In 7th International Confer-
ence on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenRe-
view.net, 2019. URL https://openreview.net/forum?id=rkgpy3C5tX.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In ICLR, 2017.
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B. Tenenbaum,
Hugo Larochelle, and Richard S. Zemel. Meta-learning for semi-supervised few-shot classifica-
tion, 2018.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization, 2019.
Jake Snell and Richard ZemeL Bayesian few-shot classification with one-vs-each Polya-Gamma
augmented Gaussian processes, 2020.
Naftali Tishby, Fernando C. Pereira, and William Bialek. The information bottleneck method. In
Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing, pp.
368-377, 1999.
Prudencio Tossou, Basile Dura, Francois Laviolette, Mario Marchand, and Alexandre Lacoste.
Adaptive deep kernel learning, 2019.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Match-
ing networks for one shot learning. In D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp. 3630-3638. Curran
Associates, Inc., 2016.
11
Under review as a conference paper at ICLR 2021
Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, and Eric P Xing. Deep kernel learning.
In Artificial Intelligence and Statistics, pp. 370-378, 2016.
Jaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin Ahn.
Bayesian model-agnostic meta-learning. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 7332-7342. Curran Associates, Inc., 2018.
12
Under review as a conference paper at ICLR 2021
Here, we provide additional details regarding our method. Appendix A describes fully the derivation
of the variational information bottleneck (VIB) objective for meta learning. Appendix B explains
how the transductive and non-transductive settings, often used in few-shot image classification, can
be explained as a particular cases of the VIB framework under suitably defined encodings. Appendix
C provides full details regarding the GP meta learning method proposed in the main paper. Finally,
Appendix D discusses experimental settings, and it presents additional experimental results and
ablation studies.
A Further details about VIB in meta learning
A. 1 B ounds on the mutual information
Here, we review the standard variational bounds on the mutual information from Barber & Agakov
(2003). Recall the definition of the mutual information,
I(x,y)= / q(x,y)log ：；)：；)dxdy = / q(x,y) log q；* dxdy.
By introducing p(x|y) that approximates q(x|y) we get
I(X 曾) 一 Ii(J(X ?/)	Iom	p(^∣y)q(方Iy)	dxd?,	一 (	∩fx 曾)log∙	p(©y) dxd?J	I (((?J)KL [∩(x|?,) ||r)(xl?，)]d?，
1 (x,y) 一 J q(X, y)	ιog	p(χ∣y)q(χ)	CIlXCIly	- J	q(x,y)	log q(χ) CIlXCIly	' J	q(y)JVL[q(x|y川p(x|y)]ay,
which shows that
I(χ,y) ≥ /q(χ,y)logp;(Xy)dxdy,
(12)
since q(y)KL[q(x|y)||p(x|y)]dy is non negative. An upper bound is obtained similarly. Suppose
p(x) approximates q(x) then
Zp(x)q(x|y)	q(x|y)
q(x,y)log p(χ)q(χ) dxdy = J q(x,y)log p⑺ dxdy - KL[q(x)∣∣p(x)]dy,
which shows that
I(X,y) ≤ ∕qEy)Iog	dxdy.
(13)
A.2 The general VIB meta learning case
Consider the general case, where we work with the unconditional mutual information and we wish
to approximate the IB: I(Z, Dv) - βI(Z, Dt). Recall that the joint distribution is written as
qw(Dv, Dt, Z) =qw(Z|Dt)p(Dv,Dt),
from which we can express any marginal or conditional. In particular observe that
qw(Z,Dv) =	qw(Z|Dt)p(Dv, Dt)dDt.
If we have a function f(Z, Dv) and we wish to approximate the expectation,
qw(Z,Dv)f(Z,Dv)dZdDv =	qw(Z|Dt)p(Dv, Dt)f(Z, Dv)dZdDvdDt,
(14)
(15)
then given that We sample a task pair (Dv, Dt)〜P(Dv, Dt) We can obtain the following unbiased
estimate of this expectation,
qw(Z|Dit)f(Z, Div)dZ.	(16)
We are going to make use of equation 15 and equation 16 in the derivation beloW.
13
Under review as a conference paper at ICLR 2021
To compute the variational approximation to IB, we need to lower bound I(Z, Dv ) as
I(Z,Dv)=
≥Z
=Z
qw(Z，Dv )l * *og qw(⅜D⅜ = ∕qw(Z，DV * * * )log qwD华 dZdDv
qw (Z, Dv) log pθCV [ dZdDv,	by using equation 12
qw (Z, Dv )log Pθ (DvIZ)dZdDv + H(DV),
where the entropy H(Dv ) is just a constant.
Subsequently, we upper bound I(Z, Dt) as follows,
I(Z,Dt) = Z qw(Z, Dt)log * DDtt八 dZdDt = Z qw (ZIDt)p(Dt)log qw(Z IDt) dZdDt,
qw (Z)p(Dt)	qw (Z)
≤	qw(ZIDt)p(Dt)log
qw(ZIDt)
pθ(Z)
dZdDt	by using equation 13
Then we obtain the overall loss, F(θ, w) ≤ LIB(w):
F(θ,w) =	qw(Z,Dv)logpθ(DvIZ)dZdDv -β	qw(ZIDt)p(Dt) log
qw(ZIDt)
pθ(Z)
dZdDt,
where we dropped the constant entropic term H(Dv ). Therefore, given a set of task pairs
{Dt, Dv}b=ι, where each (D：, DV)〜P(DlV, Dt), the objective function for learning (θ,w) be-
comes the empirical average, ɪ Pb=I Fi(θ, w), where
Fi(W⑼=/ qw(ZiIDt)IogPθ(DV ∣Zi)dZi — β / qw(Zi∣Dt)log ^wZZD^dZi,	(17)
where for the first term we made use of equation 15 and equation 16 with f(Dv, Z) = log pθ (Dv IZ).
A.3 The supervised meta learning VIB case
For the supervised meta learning case the joint density can be written as
qw(Dv, Dt, Z) =qw(ZIYt,Xt,Xv)p(Yt,YvIXt,Xv)p(Xv,Xt),
= qw(ZIYt,X)p(Yt,YvIX)p(X),	(18)
where X = (Xt, Xv) and the encoding distribution qw(ZIY t, X) could depend on all inputs
X but only on the training outputs Y t . The derivation of the VIB objective is similar with the
general case with the difference that now we approximate the conditional information bottleneck
I(Z,YvIX) - βI(Z, Y tIX) where we condition on the inputs X. In other words, both I(Z, Y v IX)
and I(Z, Y t IX) are conditional mutual informations, i.e. they have the form
I(Z，y|x) = /q(x) ]/q(z，y|x) log q(ZiX)yjχ)x) dzdy]dx = Zq(Z，y，x) log q(qfey⅛) dzdydx.
We can lower bound I(Z, Yv IX), as follows,
Zp(X) ]/ qw (Z,YVX )log qw：Z(XY(VYXX) dZdYV kX
=[P(X) / qw (Z,YV ∣X) log qw CV j：) dZdYVdX where qw (Z∣X) cancels
p
≥ ∕p(X) / qw(Z,ΥV |X)log pθ(YγVZXX, dZYVdX	by using equation 12
=/ qw (Z,YV ,X ,log pθ(γVZXX, dZdYV dX
qw(Z, Y V, X) log pθ(Y VIZ, X)dZdY VdX -	p(YV, X) logp(YV IX)dYVdX	(19)
14
Under review as a conference paper at ICLR 2021
Note that - p(Y v, X) log p(Y v |X)dY vdX is just a constant that does not depend on tunable
parameters. Also
qw(Z,Yv,X) =	qw(Z|Yt,X)p(Yt,Yv|X)p(X)dYt,	(20)
so that if We have a task sample (Yit, Yiv,Xi)〜P(Yt, YVX)p(X) an unbiased estimate of the
expectation R qw(Z, Y v, X) log pθ (Y v|Z, X)dZdY vdX is given by
/ qw (Z ∣Yit,Xi)log Pθ (Yv ∣Z,Xi)dZ.
(21)
We upper bound I(Z, Yt|X) as folloWs,
Z P(X) [/ qw (Z，Y tχ )log HYYXX)
dZdYt dX
/P(X) /qw(Z,Yt∣X)log qwq(ZZ∣XX)dZdYt
≤	P(X)	qw(Z, Y t|X) log
qw (Z∣Yt,X)
Pθ (Z|X)
qw(Z|Yt, X)P(Yt, X) log
dX,
dZdY tdX,
qw (Z |Y t ,X)
Pθ (Z|X)
dZdY tdX,
Where P(Yt |X) cancels
by using equation 13
Then We obtain the overall objective,
F (θ, w)
/ qw (Z, YV ,X )log pθ (YV |Z,X )dZdYV dX
-β	qw(Z|Yt,X)P(Yt,X)log
qw (Z∣Yt,X)
Pθ (Z|X)
dZdY tdX,
Where We dropped the constant term. Therefore, given a set of task pairs the objective becomes the
empirical average, 1 Pb=I Fi(θ, w), where
Fi(θ,w) = / qw(Z∣Yit,Xi)logPθ(YV ∣Z,Xi)dZ - β / qw(Z∣Yit,Xi)log 忆照Xi) dZ,
(22)
where we made use of equation 21.
A.4 Connection with variational inference
As mentioned in the main paper, the VIB for meta learning (where we consider for simplicity the
general case from A.2) is similar to applying approximate variational inference to a certain joint
model over the validation set,
Pθ (Dv |Z )Pθ (Z),
where Pθ (DV |Z) is the decoder model, Pθ(Z) a prior model over the latent variables and where the
corresponding marginal likelihood is
p(Dv ) = ∕pθ (Dv |Z )Pθ (Z)dZ.
We can lower bound the log marginal likelihood with a variational distribution qw(Z|Dt) that de-
pends on the training set Dt,
Fβ=1(w, θ)
/ qw(Z∣Dt)logPθ(Dv ∣Z)dZ - / qw(Z∣Dt)log
qw(Z|Dt)
Pθ(Z)
dZ,
(23)
which corresponds to the VIB objective with β = 1.
15
Under review as a conference paper at ICLR 2021
B	Transductive and non-transductive meta learning
Here, we discuss how the transductive and non-transductive settings that appear in few-shot image
classification (Bronskill et al., 2020; Finn et al., 2017; Nichol et al., 2018), due to the use of batch-
normalization, can be interpreted under our VIB framework by defining suitable encodings. We
shall use MAML as an example, but the discussion is more generally relevant.
The transductive case occurs when the concatenated support and validation/test inputs X =
(Xt , Xv ) of a single task (we ignore the task index i to keep the notation uncluttered) are used
to compute batch-norm statistics (possibly at different stages) shared by all validation/test points,
when predicting those points. For MAML this implies a deterministic parametric encoding, i.e.
common to all individual validation inputs xjv ∈ Xv, obtained by a sequence of two steps: (i) Ob-
tain first the task-specific parameter ψ in the usual way by the support loss, i.e. ψ = θ + ∆(θ, Dt).
If batch-normalization is used here, then the statistics are computed only by Xt . (ii) Compute the
validation loss by applying batch-normalization on Xv or the union X = Xt ∪ Xv (the union seems
to be a better choice, but not used often in practice for computational reasons, e.g. Finn et al. (2017);
Nichol et al. (2018) prefer to use only Xv ). In both cases, the underlying encoder is parametric
over the final effective task parameter ψ = BN(ψ, X), where BN denotes the final batch-norm
operation that outputs a parameter vector, that predicts all validation points and it is a deterministic
delta measure.
In contrast, the non-transductive setting occurs when each individual validation input xjv is concate-
nated with the support inputs Xt to form the sets xjv ∪ Xt, j = 1, . . . , nv. Then, each set xjv ∪ Xt is
used to compute point-specific batch-norm statistics when predicting the corresponding validation
output yjv . Under the VIB framework this corresponds to a non-parametric encoding, which grows
with the size of the validation set. The first deterministic step of this encoder is the same (i) above
from the transductive case but the second step differs in the sense that now we get a validation point-
tt
specific task parameter ψj = BN (ψ, xjv ∪Xt) by computing the statistics using the set xjv ∪Xt. For
MAML, this encoding becomes, Z ≡ {ψej}jn=v 1, and the encoder distribution is a product of delta
measures. i.e. P({ψj}n= 1|Yt,X) ≡ Qn= 1 δψj,BN(θ+∆(θ,Dt),χv∪Xt).
Finally, note that under the VIB perspective it does not make much sense to meta-train transductively
and meta-test non-transductively and via versa, since this changes the encoding. That is, whatever
we do in meta-training we should do the same in meta-testing.
C Further details about the Gaussian process method
For simplicity next we ignore the task index i to keep the notation uncluttered, and write for example
fit as ft and etc.
C.1 AMORTIZATION OF THE GP ENCODER q(f t|Dt)
A suitable choice of q(f t|Dt) is to set it equal to the exact posterior distribution over ft given
the training set, i.e. p(ft∣Dt) 8 Qn=ιp(yjlft)N(ft∣0, Kt). Interestingly, such a setting does not
require to introduce any extra variational parameters w and it will depend only on the model param-
eters θ that appear in the kernel function and possibly also in the likelihood. For standard regression
problems where the likelihood is Gaussian, i.e. p(yjt |fjt) = N (yjt |fjt, σ2), the exact posterior has an
analytic form given by
p(ft|Dt) =N(ft|Kt(Kt + σ2I)-1Yt, Kt - Kt(Kt + σ2I)-1Kt)	(24)
and thus we can set q(ft|Dt) = p(f t|Dt). For all other cases where the likelihood is not Gaussian
we need to construct an amortized encoding distribution by approximating each non-Gaussian like-
lihood term p(yjt |fjt), with a Gaussian term similarly to how we often parametrize variational Bayes
or Expectation-Propagation Gaussian approximation to a GP model (Hensman et al., 2014; Opper
& Archambeau, 2009; Rasmussen & Williams, 2006) i.e.
p(yjt|fjt) ≈ N(mtj|fjt,stj),
16
Under review as a conference paper at ICLR 2021
where mj ≡ mw (yj, Xj) ∈ R and Sj ≡ Sw (Xj) ∈ R+ are neural network amortized functions that
depend on tunable parameters w and receive as input an individual data point (yjt , xtj ) associated
with the latent variable fjt . We made the simplification that the output point might only influence
the real-valued mean mw (yjt, Xtj ), while the variance Sw(Xtj ) can depend only on the input. Based
on the above the amortized encoder is a fully dependent multivariate Gaussian distribution having
the form
q(ft|Dt) =N(ft|Kt(Kt+St)-1mt,Kt - Kt(Kt + St)-1Kt),	(25)
where St is a diagonal covariance matrix with the vector (St1, . . . , Stnt) in the diagonal and mt is
the vector of values (mt1, . . . , mtnt). This allows to re-write the VIB objective in equation 11 in the
following computationally more convenient form (see Appendix C.2 next):
nv	nt
XEq(fv)[logp(yvIfv)] - βXEq(ft)[logN(mj∖fj,sj)] + βlogN(mt∣0,Kt + St),	(26)
j=1	j=1
where each marginal Gaussian distribution q(fj) when Xj is either from the validation or the training
set (or any other further test set) is computed by the same expression, q(fj) = N (fj |ktj (Kt +
St)-1mt,kj - ktj(Kt + St)-1ktj>), where ktj ≡ k(Xj , Xt ) is the nt dimensional row vector of
kernel values between Xj and the training inputs Xt and kj ≡ k(Xj , Xj ).
Classification. Here, we discuss how the above general amortization procedure can be particularized
to classification problems, which is the standard application in few-shot learning. For notational
simplicity we focus on binary classification, while multi-class classification is fully covered later in
Appendix C.4.
Suppose a meta learning problem where each task is a binary classification problem where the binary
class labels are encoded in {-1, 1}. To apply the method we simply need to specify the form of the
amortized mean function mw (yjt, Xtj) (recall that Sw (Xtj) is independent from the output yjt), which
is chosen to be
m(yjt , Xtj ) = yjt × mew (Xtj ),
where me w (Xtj ) is a real-valued function given by the neural network. Notice that the dependence
on the output label yjt ∈ {-1, 1} simply changes the sign of mew(Xtj). This latter function acts as
a discriminative function that should tend towards positive values for data from the positive class
and negative values for data from the negative class, while the product yjt × me w (Xtj ) should tend
towards positive values. This amortization of the mean function is invariant to class re-labeling, i.e.
ifwe swap the roles of the two labels {-1, 1} the amortization remains valid and it does not require
any change. The multi-class classification case can be dealt with similarly, by introducing as many
latent functions as classes, as discussed fully in Appendix C.4.
C.2 Derivation of the VIB bound
The VIB objective for a single task from Eq. equation 22 in the main paper is computed as follows
v
XEq(fv)[logp(yv屏)]-β∕p(fv∣ft,XV,χt)q(ft∣Dt)logp(fVf,XV,Xt;p(ftXt)dffdff
v
v
X Eq(fv )[log p(yv 屏)]-β∕ q(f t∣Dt)log pfX)) ,
v
nv
X Eq(fjv)[log p(yjv|fjv)] - βKL q(ft|Dt)||p(ft|Xt) ,
j=1
dft
(27)
where q(fjv) = p(fjv |ff, Xjv, Xf)q(ff|Df)dff is a marginal Gaussian over an individual validation
function value fjv, as also explained in the main paper. Specifically, q(fjv) depends on the training set
(Y f,Xf) and the single validation input Xjv, so intuitively from the training set and the corresponding
function values ff we extrapolate (through the conditional GP p(fjv |ff, Xjv ,Xf)) to the input Xjv in
order to predict its function value fjv .
17
Under review as a conference paper at ICLR 2021
Given the specific amortization of q(f t|Dt):
Qjn=t1N(mtj|stj)N(ft|0,Kt)
q(ft|Dt) = ʌ——N(mt∣∩ M+ S八------------=N(ft∣κt(κt + St)Tmt,Kt - Kt(Kt + St)TKt),
N(m |0, K + S )
(28)
the VIB objective, by using the middle part of equation 28, can be written in the following form,
nv	nt
X Eq(号)[log p(yj Ifv )] — β X Eq(ft)[log N(mjfj, sj)]+ β log N (mt∣0, Kt + St),
j=1	j=1
which is convenient from computational and programming point of view. Specifically, to compute
this we need to perform a single Cholesky decomposition of Kt + St which scales as O((nt)3), i.e.
cubically w.r.t. the size of the support set nt. This is fine for small support sets (which is the standard
case in few-shot learning) but it can become too expensive when nt becomes very large. However,
given that the kernel has the linear form kθ (x, x0) = φ(x; θ)>φ(x0; θ) (ignoring any kernel variance
σf2 for notational simplicity), where φ(xi; θ) is M -dimensional and given that M nt, we can
also carry out the computations based on a Cholesky of a matrix of size M × M . This is because
Kt = ΦtΦt>, where Φt is an nt × M matrix storing as rows the features vectors on the support
inputs Xt, and therefore we can apply the standard matrix inversion and determinant lemmas for the
matrix ΦtΦt> + St when computing logN(mt|0, Kt + St). Such O(M3) computations also gives
us the quantities q(fjv ) and q(fjt), as also explained next.
C.3 Data efficient GP meta-testing prediction with constant memory
Once we have trained the GP meta learning system we can consider meta-testing where a new fresh
task is provided having a support set D；=(匕,X1) based on which We predict at any arbitrary val-
idation/test input x；. This requires to compute quantities (such as the mean value E[y；]) associated
with the predictive density
q(y；) =	p(y；|f；)p(f；|f；t,x；,X；t)q(f；t|D；t)df；df；t=	p(y；|f；)q(f；)df；
where q (f； ) is an univariate Gaussian given by
q(f；) =N(f；|kt；(Kt+St)-1mt,k； — kt；(Kt + St)-1kt；>),
k； = φ>Φt, Kt = ΦtΦt>, k； = φ>φ*, φ; = φ(x*; θ).
Here, Φt is an nt； × M matrix storing as rows the features vectors on the support inputs X；t . Note
that if we wish to evaluate q(y；) at certain value of y；, and given that the likelihood p(y； |f；) is
not the standard Gaussian, we can use 1-D Gaussian quadrature or Monte Carlo by sampling from
q(f；).
An interesting property of the above predictive density is that when the support set D；t can grow
incrementally, e.g. individual data points or mini-batches are added sequentially, the predictive den-
sity can be implemented with constant memory without requiring to explicit memorize the points in
the support. The reason behind this that the feature parameters θ remain constant at meta-test time
and the kernel function is linear, so we can apply standard tricks to update sufficient statistics as in
Bayesian linear regression.
More precisely, what we need to show is that we can sequentially update the mean and variance of
q(f；) with constant memory. q(f；) can be written as
q(f*) = N(f*∣φ>Φt>(ΦtΦt> + St)Tmt, φ>(I — Φt>(ΦtΦt> + St)-1Φt) φ*)
=N(f*∣φ>(Φt>[St]Tφt+1)-1Φt>[St]-1mt, φ>(Φt>[St]-1Φt +1)-1 φ*)	(29)
where we applied the matrix inversion lemma backwards to write I — Φt>(ΦtΦt> + St)-1Φt =
(Φt>[St]-1Φt+I)-1 and also used that Φt> (ΦtΦt> +St)-1 = Φt>(ΦtΦt>[St]-1+I)-1[St]-1 =
18
Under review as a conference paper at ICLR 2021
(Φt>[St]-1Φt +I)-1Φt>[St]-1 (based on the identity (AB + I)-1A = A(BA + I)-1). Now ob-
serve that the M -dimensional vector bt = Φt> [St]-1 mt
Pn= ι Φ(xj； θ) mmj can grow incremen-
t
tally without memorizing the feature vectors φ(χj; θ) based on the recursion bt - bt + φ(χj; θ) mɪj
(with the initialization bt = 0) as individual data points (similarly for mini-batches) are added
in the support set: Dt -Dt ∪ (xj,yj). Similarly, the M X M matrix At = Φt>[St]-1Φt =
Pn= ι s⅛Φ(xj； θ)φ(xj； θ)> can also be computed recursively with constant O(M2) memory.
Finally, note that the above constant memory during meta-testing can only be implemented when the
feature vector θ remain fixed, which means that it is not applicable for the GP+MAML combination.
C.4 Multi-class classification
For multi-class classification meta learning problems we need to introduce as many latent functions
as classes. For instance, when the number of classes for each task is N we will need N latent
functions fn (x) which all are independent draws from the same GP. The marginal GP prior on the
training and validation function values for a certain task factorizes as
N
Yp(fnv|fnt,Xv,Xt)p(fnt|Xt).
n=1
We assume a factorized encoding distribution of the form,
N
Yp(fnv|fnt,Xv,Xt)q(fnt|Dt),
n=1
where each
q(fnt |Dt) = N(fnt |Kt(Kt + St)-1mtn, Kt - Kt(Kt + St)-1Kt).
Here, mtn = Ynt ◦ me t and Ynt is a vector obtaining the value 1 for each data point xtj that belongs to
class n and -1 otherwise. Note that the encoding distributions share the covariance matrix and they
only have different mean vectors. The representation of mtn makes the full encoding distribution
permutation invariant to the values of the class labels. Since also we are using a shared (i.e. indepen-
dent of class labels) amortized functions mew(x) and sw(x) the terms (St, me t) are common to all N
factors. This allows to compute the VIB objective very efficiently (in way that is fully scalable w.r.t.
the number of classes N) by requiring only a single Cholesky decomposition of the matrix Kt + St .
Specifically, by working similarly to C.2 we obtain the VIB objective per single task,
nv
N nt
Eq({fnv,j}nN=1)[log p(yjv|{fnv,j}nN=1)] -β	Eq(fnt,j)[log N (mtn,j|fnt,j, stj)]
j=1
N
+ β X log N (mn∣0, Kt + St),
n=1
n=1 j=1
where q({fnv,j }nN=1) = QnN=1 q(fnv,j ) and each univariate Gaussian q(fnv,j ) is given by the same
expression as provided in C.3. The last two terms of the bound (i.e. the ones multiplied by the
hyperparameter β) are clearly analytically computed, while the first term involves an expectation of
a log softmax since the likelihood is
N	efnv,j
p(yv = nl{fvo,j }n0=ι) = =N—Jv-.
n0=1 e n ,j
To evaluate this expectation we apply first the reparametrization trick to move all tunable parameters
of q({fnv,j }nN=1) inside the log-likelihood (so that we get a new expectation under a product of N
univariate standard normals) and then we apply Monte Carlo by drawing 200 samples.
Finally, note that to compute the predictive density we need to evaluate,
qg= Eq({fn,*}N=ι) [P(y*l{fn,*}N=i)],
19
Under review as a conference paper at ICLR 2021
which again is done by applying Monte Carlo by drawing 200 samples from q({fn,^}N=ι). This
is precisely how the negative log-likelihood test performance was computed for the GP models in
Table 3. To decide the classification label based on the maximum class predictive probability (in
order to compute e.g. accuracy scores), we take advantage of the fact that all N univariate predic-
tive Gaussians q(fn,*) have the same variance but different means, thus the predicted class can be
equivalently obtained by taking the argmax of the means of these N distributions.
C.5 Specific GP implementation and amortization for few-shot classification
For all few-shot multi-class classification experiments in order to implement the GP and GP+MAML
methods we need to specify the feature vector φ(x; θ) and the amortized variational functions mew(x)
and sw(x). The feature vector is specified to have exactly the same neural architecture used in pre-
vious works for all datasets, Omniglot, mini-Imagenent and Augmented Omniglot; see for example
Chen et al. (2019) for details on these architectures for all these three datasets. Note that when
computing the GP kernel function the feature vector φ(x; θ) is also augmented with the value 1 to
automatically account for a bias term in the kernel function.
Regarding the two amortized variational functions needed to obtain the encoder, we consider a
shared (with the GP functions) representation by adding two heads to the same feature vector
φ(x; θ): the first head corresponds to a linear output function mew(x) and the second applies at
the end the softplus activation sw(x) = log(1 + exp(a(x))) (since sw(x) represents variance) where
the pre-activation a(x) is obtained by a linear function of the feature vector. For numerical sta-
bility we also apply a final clipping by bounding these functions so that me w (x) ∈ [-20, 20] and
sw(x) ∈ [0.001, 20]. The bound -20 and 20 are almost never realized during optimization, so they
are not so crucial, in contrast the lower bound 0.001 on sw(x) is rather crucial regarding numerical
stability since it ensures that the minimum eigenvalue of the matrix Kt + St (i.e. the matrix we need
to decompose using Cholesky) is bounded below by 0.001.
D	Further experimental details and results
D.1 Experimental settings and hyperparameters
The memory-based GP method (referred to in the Tables as GP) is trained by the VIB objective,
as defined in Section 3, where the kernel feature vector φ(x; θ) is obtained by the last hidden layer
of the same neural architecture as used in MAML. Based on this M -dimensional feature vector
φ(χ; θ) ∈ RM We consider two kernel functions: the standard linear kernel function kθ(x, χ0)=
Mφ(χ; θ)>φ(χ0; θ) (where the kernel variance σf is fixed to 1/M) and the cosine kernel kθ(x, χ0)=
φ(x∙>θ) φ(X ;^	ThQaQ IrQmQIa ClrQ hcpH in flip pvŋprimpnte
∣∣φ(x∙θ)∣∣∣∣φ(x'∙θ)∣∣. TheSe kernels are used in the expeiuments.
The Omniglot dataset consists of20 instances of 1623 characters from 50 different alphabets. Each
instance was drawn by a different person. It is augmented by creating new characters which are
rotations of each of the existing characters by 0, 90, 180 or 270 degrees. In Omniglot experiment,
MAML, Stochastic MAML and GP+MAML run by applying one adaptation step for both meta-
training and meta-testing. The mini-Imagenet involves 64 training classes, 12 validation and 24
tests classes. Following previous work on mini-Imagenet we meta-train Stochastic MAML with
5 adaptation steps, while 10 steps are used for meta-testing. GP+MAML uses 5 steps for both
meta-training and meta-testing. For both Omniglot and mini-Imagenet we follow the experimental
protocol proposed by Finn et al. (2017), which involves fast learning of N = 5-way classification
with K = 1 or K = 5 shots. The problem of N -way classification is set up as follows: select N
unseen classes, provide the model with K different instances per class, and evaluate the model’s
ability to classify new instances within the N classes.
The Augmented Omniglot benchmark is a modified version of Omniglot which necessitates long-
horizon adaptation and it is often considered as many-shots problem (Chen et al., 2019). For each
alphabet, 20 characters are sampled to define a N = 20-class classification problem with K = 15
shots. Further, both train and validation images are randomly augmented, by applying transforma-
tions, which makes it even more challenging. Following Flennerhag et al. (2018); Chen et al. (2019),
during meta-testing both MAML and Stochastic MAML perform 100 steps of adaptation (resulting
in 2000 data points seen in total by the model where each step processes a minibatch of size 20
20
Under review as a conference paper at ICLR 2021
points), while they are meta-trained by applying 20 adaptation steps (i.e. 400 training points seen
per task). Both GP methods are meta-trained by memorizing the full N × K = 20 × 15 = 300 sup-
port points without further data augmentation, while during meta-testing we allow the GP methods
to see up to 2000 points. For all methods we perform a hyperparameter search using the train and
validation subsets of all three benchmarks as detailed below.
For sinusoid regression, Omniglot and mini-Imagenet, meta-training of all methods consisted of
60000 iterations or episodes, where in each episode a learning update is performed based on a mini-
batch of tasks. For the sinusoid regression the meta batch-size was 5 tasks and for each task we
have K = 10 examples. For N -way, K-shot classification in Omniglot and mini-Imagenet (with
N = 5, and K = 1, 5 as mentioned in Table 2 in the main paper), the meta batch was 32 tasks
in Omniglot and 4 tasks in mini-Imagenet. Also for these two datasets Stochastic MAML uses
one gradient step (in the inner loop) for both meta-training and meta-testing in Omniglot, while
in mini-Imagenet it uses 5 and 10 steps respectively, i.e. exactly as MAML by Finn et al. (2017)
was applied in these datasets. As mentioned in the main paper GP+MAML considers one gradient
adaptation step in Omniglot and 5 in mini-Imagenet (for both meta-training and meta-testing). The
neural architectures of all these experiments is the same as in Finn et al. (2017). Specifically, for
the Omniglot the architecture is from Vinyals et al. (2016), which has 4 modules with a 3 × 3
convolutions and 64 filters, followed by batch normalization, a ReLU nonlinearity, and 2 × 2 max-
pooling. The Omniglot images are downsampled to 28 × 28, so the dimensionality of the last hidden
layer is 64. For Omniglot, strided convolutions are used instead of max-pooling. For the GP methods
this constructs 65-dimensional feature vector φ(x; θ) (64 plus one for the bias term which is included
in φ(x; θ)). For mini-Imagenet, the network uses 32 filters per layer and the final layer the feature
vector feature is obtained by flattening so that finally the feature vector φ(x; θ) is 801-dimensional.
For Augmented Omniglot, the meta-training of all methods consisted of 5000 iterations. The Aug-
mented Omniglot dataset is a modified version of Omniglot which, for gradient-based methods
like MAML, necessitates long-horizon adaptation and it is often considered as many-shots problem
(Chen et al., 2019). For each alphabet, 20 characters are sampled to define a 20-class classification
problem with K = 15 data points per class. Furthermore, both train and test images are randomly
augmented, by applying transformations. Following Flennerhag et al. (2018); Chen et al. (2019), we
use a 4-layer convnet and during meta-testing MAML and Stochastic MAML performs 100 steps
of adaptation (resulting in 2000 data points where each step processes a minibatch of size 20 data-
points, i.e. per class), while they are meta-trained by applying 20 adaptation steps. GP+MAML uses
5 adaptation steps for both meta training and meta testing.
Additional hyperparameters details are included in Tables 4, 5 and 6. The ranges for the hyperpa-
rameters’ search are:
• Outer learning rate α:
[0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.002, 0.0025,
0.005, 0.0075, 0.01, 0.025, 0.05, 0.075, 0.1]
• Inner learning rate ρ:
[0.0001, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.025, 0.05, 0.1, 0.2, 0.5, 1.0, 1.5, 2.0]
• Bottleneck coefficient β :
[1e - 07, 1e - 06, 1e - 05, 5e - 05, 0.0001, 0.00025, 0.0005,
0.00075, 0.001, 0.0025, 0.005, 0.0075, 0.01, 0.1, 0.5, 1.0]
D.2 Additional experimental results
Table 7 provides detailed few-shot sinusoid regression results, while Figure 3 illustrates how the GP
model, after having been meta-trained on the sinusoid regression tasks, predicts a test task as the
number K of the training examples increases.
We also found that the qualitative behaviour of GP and GP+MAML are quite different. In Figure
4, we report the performance of the GP and GP+MAML on mini-Imagenet tasks as a function of
21
Under review as a conference paper at ICLR 2021
Parameter Stochastic MAML
Omniglot
K=1 K=5
Outer l.r. α	0.005	0.001
Inner l.r. ρ	1.5	1.5
Bottleneck β	10-7	10-6
mini-Imagenet
K=1 K=5
Outer l.r. α	0.0005	0.0005
Inner l.r. ρ	0.05	0.1
Bottleneck β	10-5	10-4
Augmented Omniglot
Outer l.r. α	0.01
Inner l.r. ρ	0.5
Bottleneck β	10-6
Sinusoid regression
Outer l.r. α	0.0005
Inner l.r. ρ	0.1
Bottleneck β	10-3
Table 4:	Hyperparameters for Stochastic MAML.
Parameter	MAML
Augmented Omniglot
Outer l.r. α	0.01
InnerLr. P	0.5
Sinusoid regression
Outer l.r. α	0.002
InnerLr. P	0.005
Table 5:	Hyperparameters for MAML in sinusoid regression and Augmented Omniglot.
22
Under review as a conference paper at ICLR 2021
Parameter	GP (cos)	GP (linear)	GP + MAML (cos)	GP + MAML (linear)
Omniglot			
	K=1	K=5	K = 1 K = 5 K = 1	K = 5	K=1 K=5
Outer l.r. α	0.001	0.0025	0.0025 0.005	0.00075	0.005	0.0025	0.001
Inner l.r. ρ	--	-	-	0.0005	0.002	0.002	0.0005
Bottleneck β	0.0005	0.0005	0.0001 0.0001	0.0005	0.0005	0.0001	0.00005
mini-Imagenet			
	K=1	K=5	K = 1 K = 5 K = 1	K = 5	K=1 K=5
Outer l.r. α	0.00075	0.0005	0.0025 0.00075	0.001	0.0005	0.001	0.00075
Inner l.r. ρ	--	-	-	0.0005	0.0005	0.002	0.002
Bottleneck β	0.005	0.005	0.005	0.005	0.001	0.0005	0.0001	0.0005
Augmented Omniglot			
Outer l.r. α	0.0025	0.05	0.005	0.005
Inner l.r. ρ	-	-	0.001	0.0005
Bottleneck β	0.0005	0.00075	0.00025	0.00001
Sinusoid regression			
Outer l.r. α	0.00075	-	0.002	-
Inner l.r. ρ	-	-	0.0001	-
Bottleneck β	1.0	-	0.5	-
Table 6: Hyperparameters for the GP methods.
23
Under review as a conference paper at ICLR 2021
Model	K=5	K=10	K=20
MAML (our)			
grads steps 1	0.600±0.662	0.359±0.015	0.228±0.018
grads steps 5	0.311±0.013	0.12±0.006	0.06±0.004
grads steps 10	0.280±0.013	0.096±0.005	0.043±0.003
Stochastic MAML			
grads steps 1	0.662±0.04	0.382±0.021	0.244±0.014
grads steps 5	0.352±0.035	0.141±0.014	0.073±0.006
grads steps 10	0.317±0.34	0.116±0.012	0.054±0.004
GP	0.02±0.014	0.002±0.001	0.001±0.001
GP+MAML	0.058±0.054	0.002±0.001	0.002±0.002
MAML (Finn etal., 2017) 0.35		-	-
Table 7: Detailed few-shot sinusoid regression results. We report the results of the GP model for K = 5, 10, 20
and for MAML assuming different gradient adaptation steps (including also a result reported in the original
MAML paper by Finn et al. (2017)).
K=1
K=2
K=3
UOA3uaαu-s
Uo-0UnJ əu-s
Uo-0UnJ əu-s
050505050
q---q---q
Ioooooooi
-- --
Figure 3: The red curve represents a ground truth sinusoid function whereas the light blue one is the mean
prediction of a GP. As K increases, the uncertainty (shown by the shaded area) of the GP decreases and its
mean predictions become closer to the ground truth. Given already K = 4 points, the mean prediction matches
well the sinusoid function.
mini-Imagenet (K = 1)
mini-Imagenet (K = 5)
Inner step
GP, β = 0.005 GP+ MAML, β = 0.0001
Figure 4: Qualitative difference between GP and GP+MAML on mini-Imagenet as a function of inner loop
steps. Because there is no inner loop for the GP, we simply report it as a reference to the GP+MAML.
24
Under review as a conference paper at ICLR 2021
inner steps executed at test time by GP+MAML. Note, that GP actually has only one step executed
over all the test-time data in once, because there is no inner loop for it. Intuitively, we would expect
GP+MAML to have similar performance to GP at the beginning of inner loop, but this is not what
is happening. Instead, it starts quite low and peaks at exactly the number of inner steps used during
meta-training and then the performance starts to deteriorate.
D.3 Ablative studies
Bottleneck cost β ablation To understand the impact of β on learning we provide the ablation
analysis in Figure 5 on sinusoid regression and two few-shot classification tasks. For a regression
task, Figure 5 (a), we observe that having a large β = 1.0 is beneficial and provides the best mean
squared error (MSE). For few-shot classification tasks Figures 5 (b)-(f), we observe that there is a
large range of β values in between [10-7, 0.01] providing similar classification accuracy. Interest-
ingly, for values of β close to 1, where for β = 1 the VIB corresponds to an ELBO, the performance
deteriorates significantly.
(a) Sinusoid regression (K = 1)
(b) Omniglot (K = 1)
(c) Omniglot (K = 5)
(d) mini-Imagenet (K = 1)
7 6 5 4 3 2
- a a ■ a -
Oooooo
>US3UU<
le-6	le-3	1
β
(e) mini-Imagenet (K = 5)
le-6	le-3
β
(f) Augmented Omniglot
Figure 5: Ablation analysis for different β values on sinusoid regression and few shot classification tasks. For
sinsusoid regression we report mean squared error (MSE): lower is better. For few-shot classification tasks, we
report the accuracy: higher is better.
D.4 Improved versions for the GP-based methods
For GP-based methods we consider additional popular tricks used in the community to improve
performance.
For Augmented Omniglot dataset (Flennerhag et al., 2018), for the network architecture we add 4
additional layers similar to Flennerhag et al. (2020), where each layer is added after a convolutional
layer. These additional layers are simple convolutions as considered in Flennerhag et al. (2020).
In addition, we add a batch normalization. Simply using more layers in GP methods significantly
improves the performance on this dataset. Note, that the results are achieved without a special warp-
grad architecture as in Flennerhag et al. (2020). In Table 8 we show the results with and without
batch normalization (BN).
In addition, we run additional experiments on mini-ImageNet (Ravi & Larochelle, 2017) and tiered-
ImageNet (Ren et al., 2018) with using pre-trained embeddings from LEO (Rusu et al., 2019) paper
as features. The embeddings are taken from their GitHub repository. On top of these features, we
construct an MLP with two hidden layers and ReLU activations where each layer has 128 hidden
dimensions. This defines the feature vector φ(x; θ) which is used by the GP methods in all the
experiments. Moreover, we apply dropout to the LEO embeddings before passing them to the MLP.
The results are given in Table 9. The hyperparameters are tuned in the same way as for the previous
experiments. Their parameters are given in Table 10. Notice that the GP methods are comparable
25
Under review as a conference paper at ICLR 2021
Augmented Omniglot 20-way
Model
GP (linear) GP (cos) GP (linear), BN GP (cos), BN	86.29±0.84% 85.6±0.76% 86.62±1.3% *86.7±0.9%
GP+MAML (linear) GP+MAML (cos) GP+MAML (linear), BN GP+MAML (cos), BN	84.59±0.5% 84.23±1.0% 85.94±0.7% *86.17±1.04%
Warp-Leap Flennerhag et al. (2020)	83.6±1.9%
Table 8: Classification test accuracy performance on Augmented Omniglot for improved architectures. The
GP based methods use 4 additional simple convolutional layers where each of these layers is added after each
network layer, similar to Flennerhag et al. (2020). We also report results with and without batch normalization.
	mini-ImageNet 5-way		tiered-ImageNet 5-way	
Model	K=1	K = 5	K=1	K=5
GP (linear)	60.4±0.28%	77.1±0.08%	64.16±0.16%	81.6±0.09%
GP (cos)	60.8±0.16%	76.8±0.08%	65.3±0.16%	81.3±0.22%
GP + MAML (linear)	60.5±0.5%	77.08±0.13%	64.5±0.22%	81.6±0.16%
GP + MAML (cos)	60.8±0.19%	77.1±0.21%	65.6±0.2%	81.5±0.14%
LEO Rusu et al. (2019)	61.76±0.08%	77.59±0.12%	66.33±0.05%	81.44±0.09%
Table 9: Additional results on mini-ImageNet and tiered-ImageNet. We re-use LEO Rusu et al. (2019) embed-
dings from their GitHub code and apply a 2-layers MLP with ReLU activations and 128 hidden dimensions to
construct the feature vector.
with LEO, with no significant difference. We would like to point out that LEO uses a set of different
tricks: dropout, label smoothing, `2 regularization and orthogonality penalty (Rusu et al., 2019),
while we have only considered dropout.
GP (cos)			GP (linear)		GP+MAML (cos)		GP+MAML (linear)	
	K=1	K=5	K=1	K=5	K=1	K=5	K=1	K=5
mini-ImageNet								
Outer l.r. α	5 * 10-5	2.5 * 10-5	0.00025	7.5 * 10-5	5 * 10-5	7.5 * 10-5	2.5* 10-5	0.0001
Inner l.r. ρ	-	-	-	-	0.001	0.001	0.001	0.001
Bottleneck β	0.0025	10-6	0.00075	10-6	0.0001	0.0001	0.0001	10-6
Dropout p	0.1	0.1	0.1	0.2	0.1	0.2	0.1	0.2
tiered-ImagetNet								
Outer l.r. α	7.5 * 10-5	0.0001	7.5 * 10-5	0.00025	7.5* 10-5	7.5 * 10-5	0.0001	0.0001
Inner l.r. ρ	-	-	-	-	0.001	0.001	0.001	0.001
Bottleneck β	10-5	10-5	5* 10-7	10-6	10-8	10-8	10-7	10-7
Dropout p	0.2	0.1	0.1	0.2	0.2	0.1	0.2	0.2
Table 10: Hyperparameters for the additional results on mini-ImageNet and tiered-ImageNet. For dropout rate,
we considered values: 0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7.
26