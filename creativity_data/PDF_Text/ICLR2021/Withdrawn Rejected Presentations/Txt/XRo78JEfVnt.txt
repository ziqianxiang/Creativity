Under review as a conference paper at ICLR 2021
Optimizing Over All Sequences of
Orthogonal Polynomials
Anonymous authors
Paper under double-blind review
Ab stract
Every length-(n + 1) sequence of orthogonal polynomials is uniquely represented
by two length-(n + 1) sequences of coefficients α and β. We make this rep-
resentation learnable by gradient-based methods. Orthogonal polynomial opera-
tions may be automatically differentiated, but this uses O(n2) memory and is very
slow in practice. By exploiting reversibility, we derive a differentiation algorithm
which uses O(n) memory and is much faster in practice. Using this algorithm,
fixed polynomial transforms (e.g. discrete cosine transforms) can be replaced by
learnable layers. These are more expressive, but they retain the computational
efficiency and analytic tractability of orthogonal polynomials.
As another application, we present an algorithm for approximating the minimal
value f (w*) of a general nonconvex objective f, without finding the minimizer
w*. It follows a scheme recently proposed by Lasserre (2020), whose core al-
gorithmic problem is to find the sequence of polynomials orthogonal to a given
probability distribution. Despite the general intractability of this problem, we ob-
serve encouraging initial results on some test cases.
1 Introduction
Sequences P0,P1,...,Pn of orthogonal polynomials - such as the Chebyshev, Laguerre, and Her-
mite sequences - are fundamental in many areas of scientific computing. Their core operation is
evaluation (or transformation): given a polynomial f, represented as coefficients c0 , . . . , cn in the
basis p0, . . . ,pn, compute f(xi) at n + 1 distinct points x0, . . . , xn1. The inverse operation is inter-
polation: given the points xi and corresponding outputs yi , compute the coefficients of the unique
f satisfying f (Xi) = y》Computing the Jacobian determinant ∣∂f (x)/dc| is useful in applications
such as normalizing flows (Rezende & Mohamed, 2015).
This work enables gradient-based optimization over the set of orthogonal polynomial sequences.
These can be written recursively in terms of scalar sequences α and β, the latter positive:
p-1 (x) = 0; p0 (x)	= 1;	pj+1 (x)	= (x - αj)pj (x)	- βjpj-1 (x)	for αj,	βj	s.t.	βj	> 0	(1)
The correspondence between the orthogonal polynomial sequences p and coefficients (α, β) is exact.
(A more formal statement is given in Section 2.) We make (α, β) a learnable representation. We
derive the gradients, with respect to (α, β), of the evaluation and interpolation operations. Compared
to automatic differentiations, our algorithms use O(n) (rather than O(n2)) memory, and are much
faster in practice. We explore two applications of this new optimization capability.
Learned polynomial transforms. Currently, orthogonal polynomials transforms are manually cho-
sen based on intuitions about their suitability. Informally, Chebyshev polynomials resemble cosines;
Laguerre polynomials are similar to cosines multiplied by exponentials; Hermite polynomials are
roughly cosines divided by Gaussians (Valiant, 2016). These sequences are orthogonal with respect
to different distributions μ over their inputs x; see Figure 1. Their corresponding transforms are
generalized by our learnable AnyPT layer, which has fast algorithms for its forward, backward,
inverse, and log-determinant passes. By unobtrusively replacing the DCT and IDCT within JPEG
compression, we obtain better tradeoffs between visual quality and compression. It may be possible
to gently improve many signal processing pipelines in this manner.
1Zero-indexing vectors of length n + 1 is a standard convention for polynomial transforms.
1
Under review as a conference paper at ICLR 2021
Transform	Xi	αk	β =	二 Y2	μ
Cosine-III	cos( n+ι)(i + 1)	0	βo	=π,βι = 2 ,βk = 4	2 ∙ Beta( 1, 2) — 1-
Legendre	(Bogaert, 2014)	0	βo	= 2,βk = k2∕(4k2 - 1)	Unif(-1, 1)
Hermite	(Press etal., 1992)	0	βo	=√π, βk = k/2	Normal(0,1)
Laguerre	(Press etal., 1992)	2k + 1	βo	=1,βk = k2	Exp(1)
Figure 1:	Parameters of classic orthonormal polynomial transforms, up to diagonal scaling. Cosine-
III (short for the Discrete Cosine Transform, type III) is formed from the Chebyshev polynomials.
The evaluation points xi are taken to be the roots ofpn+1. They may not have a closed form, but can
be calculted by the cited algorithms. Coefficient sequences may be obtained from Gautschi (2005);
Leibon et al. (2008) and Chapter 4.5 of Press et al. (1992). μ is the probability distribution which
renders the polynomial sequence μ-orthogonal.
Minimal values of optimization problems. We present an algorithm, called Mop, for approximat-
ing the minimal value f (w*) of a continuous function f, without finding its minimizer w*. In recent
work, Lasserre (2020) reduced this difficult problem to the following one: given access to a distri-
bution μ over R, find the sequence of orthogonal polynomials (represented as coefficients ɑ and β)
which are orthogonal with respect to μ. Our approach to solve the latter problem uses stochastic
gradient descent.
Our contributions are: (1) memory-efficient algorithms, with fast CUDA implementations, for com-
puting the vector-Jacobian products of evaluation and interpolation, (2) the learnable AnyPT layer,
which demonstrates reduced error when applied to image compression, and (3) Mop, an iterative
algorithm for estimating the minimal value f * of a continuous function f .
2 Preliminaries
The following background material is found in references on orthogonal polynomials (Gautschi,
2004; 2005; Ismail et al., 2005) and numerical linear algebra (Higham, 2002).
Measures and moments. Let μ be a measure over R. It is positive if μ(B) > 0 for all nonempty
open sets B. Its moments are mi = / xidμ(x) for i ≥ 0. These define a linear functional L(p),
over polynomials p, via L(Xk) = mk. Positive measures μ, moment sequences m whose Hankel
matrices are positive definite, and positive linear functionals (satisfying L(p) > 0 for all nonnegative
polynomials p) uniquely correspond to one another.
Orthogonal polynomials. A measure μ defines an inner product hpi,pji = ∕pi(x)pj(x)dμ(x)
over polynomials. Pi and Pj are μ-orthogonal if hp% ,pj = 0. A sequence [po ,pι,...,Pn] = P of
polynomials is μ-orthogonal if Pi and Pj are μ-orthogonal when i = j. A polynomial is monic if its
coefficient on Xn is 1. A polynomial q is orthonormal if its norm ||qi|| =，hqi, q/ = 1.
Three-term recurrence. For every positive μ, the sequence of μ-orthogonal monic polynomials
satisfies the three-term recurrence (1) for some α and β. An orthonormal polynomial sequence
q0, q1, . . . satisfies the following three-term recurrence:
q-1(X) = 0; q0(X) = γ0-1	γj+1qj+1(X) = (X - αj)qj (X) - γjqj-1(X) for γj > 0	(2)
The orthonormal polynomial sequence q defined by (α, Y) corresponds to the monic polynomial
sequence P defined by (α, β) where Yi = √βi for i > 0. The Jacobi matrix, truncated to order n,
organizes the coefficients (α, β) in the following n × n tridiagonal matrix:
	αo	√β1	0	0	0	
	√β1	α1	√β2	0	0	
Jn =	0 .	√β2 .	α2 .	√β3 .	0 .	⑶
	. . 0	. . ...	. . 0	. . √βn-?	. . αn-1	
2
Under review as a conference paper at ICLR 2021
Operation	Inputs	Output	O(n2)-time Algorithm	O(n log2 n)-time Algorithm
Evaluation	x, α, β, C	Vc	(Smith,1965)	(Potts, 2003)
Interpolation	x,α,β,y	V-1y	Dual in Higham (1988)-	
Derivative Evaluation	x, α, β, c	V0c	(Smith, 1965)	
Transpose Multiply	x, α, β, c	VTc	Appendix	(Driscoll et al., 1997)
Transpose Solving	x,α,β,y	V-Ty	Primal in Higham (1988)	(Bostan etal., 2010)
Determinant	x, a, β	det(V)	Formula in Sec. 2.1	(GOhberg&Olshevsky,1994)
Eigenvalues	α, β	λi(Jn)		(Coakley & Rokhlin, 2013)-
Figure 2:	Algorithms for orthogonal polynomials. The O(n2) algorithms can be parallelized. In
particular, a version of Clenshaw’s algorithm for evaluation takes O(n) parallel time (Barrio, 2000).
Interpolate (the dual in Higham (1988)) takes O(n) parallel time, since the inner loop over j can
be run by O(n) threads, each operating on three entries of c. O(n log2 n)-time algorithms for
interpolation and derivative evaluation are not plainly written in the literature, to our knowledge.
Favard’s theorem.2 Given any sequence of monic polynomials p defined by α and β in the re-
currence 1, define L via L(p0) = β0 and L(pi) = 0 for i > 0. Then L is positive, and thereby
corresponds to a positive measure μ for which P are μ-orthogonal.
Normalization. The norm ofa degree-k monic polynomial is ||pk||2 = Qjk=0 βj. Neither the three-
term recurrence (1) nor the Jacobi matrix involve β0. This is because βo = ∣∣po∣∣2 = / 1dμ(x) is
the normalizing constant of μ. By fixing βo = 1, We restrict attention to probability distributions.
Vandermonde systems. Evaluation and interpolation can be viewed as matrix-vector multiplication
and linear system solving, respectively, with a polynomial Vandermonde matrix V :
V c = y where Vi,j = pj(xi)	(indexed from zero)	(4)
V generalizes the Vandermonde matrix from monomials to orthogonal polynomials. Its determinant
is the same as the Vandermonde matrix: det(V ) = Q1≤i<j≤n(xj - xi) (Barnett, 1975). V is
invertible when the xi are distinct. The derivative Vandermonde matrix has entries Vi0,j = p0j (xi).
Numerical stability. The Evaluate algorithm in Figure 3 (i.e. Clenshaw’s algorithm) is known to
be numerically stable (Smoktunowicz, 2002). By contrast, Interpolate can be unstable for moder-
ate n (Gohberg & Olshevsky, 1997; Higham, 1990). Fortunately, various mitigations have been
developed for this issue. Furthermore, the instability in Interpolate can be resolved entirely using
program transformation techniques. We do not encounter numerical instability in our present work,
but discuss this issue further in the appendix.
3	Gradient-Based Optimization over Orthogonal Polynomials
Efficient reverse-mode differentiation (backpropagation) amounts to implementation of the vector-
Jacobian product (VJP). Here is an example of our notation for the evaluation operation, i.e. com-
puting y = Vc. Let Vχy =煞 be the Jacobian matrix of y with respect to x. The VJP operation is
(x, V) → VTVχy. In reverse mode, X = yτVxy is the application at (x, y), where y was computed
similarly. α, β, and c are similarly defined for the other arguments α, β, and c.
VJPs may be computed automatically from a forward computation graph. However, this stores
intermediate computations of the forward pass, which requires memory scaling with depth. Evaluate
and Interpolate have depth O(n), so O(n2) memory would be used. Furthermore, they involve
sparse updates in nested loops, which are faster in plain CPU/GPU code.
We manually derive the VJPs in the appendix. By exploiting reversibility, we reduce their memory
requirement to O(n). The complete algorithms are displayed in Figure 4. As illustrated in Figure 5,
they are substantially faster than automatic differentions, even when memory is not a concern.
2This is also by Shohat, Stone, etc., so it is also called the spectral theorem of orthogonal polynomials.
3
Under review as a conference paper at ICLR 2021
procedure Evaluate(x, α, β , c)
u = [cn, . . . , cn]
v = [0, . . . , 0]
for k ∈ [n - 1, . . . , 0] do
τ=u
U =(X — αk) ∙ U
-βk+1 v + ck
v=τ
return U
Figure 3: Algorithms for polynomial
evaluation and interpolation. All the
inputs are vectors in Rn+1. Both algo-
rithms return a vector in Rn+1. Eval-
uate is the Clenshaw algorithm. Inter-
polate is the dual algorithm of Higham
(i988). DiVDiffS(x,y) computes the
first row of the table of divided differ-
ences of y with respect to x; see the
appendix for its definition.
procedure Interpolate(x, α, β, y)
δ = DivDiffs(x, y)
return ChangeBasis(x, α, β, δ)
procedure DivDiffs(x, y) (naive)
δ=y
for k ∈ [0, . . . , n - 1] do
∆ = δk
for j ∈ [k + 1, . . . , n] do
τ = δj
δj = (δj - △"(Xj - xj-k-1)
∆=τ
return δ
procedure ChangeBasis(x, α, β, δ)
c=δ
cn-1 = (α0 - xn-1 )cn
for k ∈ [n - 2, . . . , 0] do
ck += (α0 - xk)ck+1 + β1ck+2
for j ∈ [1, . . . , n - 2 - k] do
ck+j = (αj - xk )ck+j+1 + βj+1 ck+j+2
cn-1 += (αn-k-1 - xk)cn
return c
procedure Evaluate(x, α, β, u, v, U)
X = y ◦ (V 0c)
C= V T y
v = 0
α = β = [0,..., 0]
for k ∈ [0, . . . , n - 1] do
W = βk+ι(-u + (x - ɑk) ∙ V
+ck )
αk = -UTv
βk+l = -UT w
U, v = v, w
T = U
U = V + U ∙ (x — αk)
V = -τ ∙ βk+1
return X, α, β, c
Figure 4:	These algorithms compute
the vector-Jacobian products of Eval-
uate and Interpolate. The arguments
u, v ofEvaluate are the corresponding
values computed during the forward
pass, and U is an alias of c.
procedure Interpolate(x, α, β, y, c, C)
yc = V-Tcc
xc = -yc ◦ (V0c)
αc , βc = [0, . . . , 0]
for k ∈ [0, . . . , n - 2] do
cn-1 =+ - (αn-k-1 - xk)cn
tαn-k-1 + Cn-I ∙ cn
Cn =Cn—1 ∙ (αn-k—1 - xk)
for j ∈ [n - 2 - k, . . . , 1] do
ck+j =+ - (αj - xk )ck+j+1 - βj+1ck+j+2
αj + ck+j ∙ ck+j+1
∕¾ + 1 + ck+j ∙ ck+j+2
ck+j+2 = ck+j ∙ lβj+1
Ck+j+1 = Ck+j ∙ (αj - Xk)
ck += - (α0 - xk)ck+1 - β1ck+2
a0 = Ck ∙ ck+1
β∖ + Ck ∙ ck+2
Ck+1 = Ck ∙ (αo - Xk)
Ck+2 = Ck ∙ β1
cn—1 += - (α0 - Xn—1)cn
αo = Cn-1 ∙ cn
return XC, αC, βC, yC
Figure 5:	Runtime comparisons of standard (JAX) and cus-
tom (CUDA) implementations. The latter is orders of magni-
tudes faster than the former, for both the forward and back-
ward passes. Presently, algorithms involving sparse updates
within nested loops are an edge case for compilers. We expect
this situation to improve, but our present investigation would
not have been possible without custom implementation.
4
Under review as a conference paper at ICLR 2021
Figure 6:	Learning both AnyPT and quantization tables
(red) achieves lower loss than learning just the tables (blue).
The difference is minimal at lower values of λ, where the
target PSNRs are roughly 35-40. This is the regime where
the standard JPEG tables are designed to operate. However,
as λ increases (and the target PSNR decreases), AnyPT can
express a substantially different transform.
4	Learned Polynomial Transforms
The differentiable AnyPT layer encapsulates the Evaluate and Interpolate algorithms. Its parameters
are the evaluation points x ∈ Rn+1 and the coefficients α, β ∈ Rn+1. Its forward pass is Evaluate
and its inverse pass is Interpolate. It is invertible when the xi are distinct. Its Jacobian is V , whose
log-determinant is calculated by an algorithm from Section 2.
4.1	Learned JPEG
Perhaps the most widely-used orthogonal polynomial transform is the discrete cosine transform
(DCT) within JPEG image compression (Wallace, 1992). At a high level, JPEG converts RGB chan-
nels to one luma (brightness channel) and two chroma (color) channels, and operates on each channel
separately. It splits the image into 8x8 patches and applies the 2D DCT, which is equivalent to the
DCT applied to each column, followed by the DCT applied to each row. The transformed patches
are quantized (rounded to the nearest integer) after pointwise division by an 8x8 quantization table,
in which larger values correspond to less visually significant components. The human visual system
is less sensitive to high-frequency stimuli. Since the DCT expresses the patch as a combination of
different-frequency components, the standard tables can readily discard high-frequency information.
Finally, the sequence of integers is losslessly compressed with an entropy-based method.
The quantization tables are learnable parameters of JPEG. They can be learned by proxy objectives
(Fung & Parker, 1995) or zero-order methods (Hopkins et al., 2018). By replacing rounding with
a smooth approximation, JPEG becomes differentiable (Shin & Song, 2017); thus, the quantization
tables can be learned with first-order methods (Luo et al., 2020). We investigate the additional benefit
of replacing the DCT by AnyPT. To do so, we use a simple objective which (loosely) captures the
tradeoff between visual distortion and compression rate, whose relative importance is set by λ > 0.
To measure the former, we use PSNR, which is a normalized log-squared error. To measure the latter,
itis typical to jointly train a measure of entropy, which can be used for the lossless compression step.
Since we don’t aim to replace this step, we simply use a linear penalty for higher-value coefficients.
We evaluate AnyPT on the CLIC 2020 dataset, from the eponymous image compression challenge
(Toderici et al., 2020). As shown in Figure 6, replacing AnyPT with the DCT consistently reduces
error. These improvements are modest, but there are other advantages to keeping the compression
pipeline mostly intact. The first is interpretability: the explanation of JPEG quantizing visually
unimportant components remains valid. Another is ease of training: there are only 24 additional
parameters to be trained on a multi-gigabyte dataset.
5	Minimal Values of General Optimization Problems
5.1	Background
Let W ⊆ RN be a compact set and let f : W 7→ R be a continuous function which (for simplicity)
attains its minimum over W. f * = minw∈w f (W) is that minimum value. Approximating f * is NP-
hard in general, but may be possible for some practically relevant f. The minimal value f * should
not be confused with the minimizer w* ∈ RN achieving f(w*) = f*. In general, it is unclear if
knowing f* helps obtain w* . Nonetheless, knowing f* could be practically useful for debugging.
For example, if a model is trained to have parameters W, and its loss f (W) is much larger than f *,
then blame lies with the model’s training, rather than its raw expressiveness.
5
Under review as a conference paper at ICLR 2021
τr-<∙	r irʌl , i' X^lΛ i'	1	∙ 1	, 1	1	1	∙ 1 El
Figure 7: Plots of Σ for classical orthogonal polynomials. These are
formed from B = 1024 samples and the correct α,β,μ listed in Figure
1, so Σ should be close to identity. The Chebyshev and Legendre poly-
nomials behave as desired. The Hermite and Laguerre polynomials suffer
in the bottom-right entries involving higher-degree polynomials. This is
because Hermite and Laguerre μ have unbounded support, along which
high-degree polynomials quickly diverge. The Chebyshev and Legendre
μ are supported on [-1,1]. f should ideally be bounded.
Lasserre (2020) recently proposed the following approximation scheme for f *. Suppose P is some
probability distribution on W which (for simplicity) is absolutely continuous to Lebesgue measure.
We call it a prior since it is ideally concentrated around w*. Then μ(X) = ρ(f-1(X)) is the
distribution of f (w), where f-1(X) = {w ∈ W : f(w) ∈ X} is the preimage of f. Let α and
β be the (unique) recurrence coefficients of the sequence of polynomials orthogonal with respect to
P. Let λ be the minimum eigenvalue of Jn (or, equivalently, the smallest root of pn+1 ). Lasserre
(2020) shows that, as n → ∞, λ → f* from above.Laurent & Slot (2020) prove this convergence
has rate O(log2 n/n2) if W satisfies a mild geometric condition.
To implement this scheme, Lasserre (2020) observes that the smallest eigenvalue of Jn coincides
with the smallest generalized eigenvalue of two n× n matrices formed from the moments Ef(w)k =
Exk. In particular, λ is the largest value satisfying [Exi+j+1]in,j=0 λ[Exi+j]in,j=0. In principle,
these moments can be computed if P can be sampled and f can be evaluated. However, unless f
has special structure, the computation is challenging for large n and N . In particular, the sample
complexity of estimating the moments (i.e. the number of evaluations of f) is exponential in n.
5.2	Our Approach
The core problem in this scheme is: given sampling access to the distribution μ, find the unique
sequence q* of μ-orthonormal polynomials, defined by the coefficients (α*,β*). By definition,
these satisfy I = Ex〜μ [ q*(χ)q*(χ)]i,j. This can be rephrased in terms of mean and covariance
using q0(x) = 1 and, via Favard’s theorem, E qi*(x) = 0 for i > 0. Let r = [q1(x), . . . , qn(x)] be
the rest of q(x), and let σ = E r. We want to minimize (over α and β) some loss L(σ, Σ) between 0
and σ, and between I and Σ = E(r - σ)(r - σ)T. σ and Σ are unknown, but may be estimated from
a batch of data. Consider the (regularized) estimators σ = E r and Σ = B-I E(r — σ)(r 一 σ)T + δI,
where δ > 0 and E is the batch mean. When L is convex, L(σ, Σ) ≤ E L(σ, Σ), and the latter can
be iteratively minimized. This yields the Mop algorithm (Figure 8), so-named because it reveals the
“floor” f* , or at least abbreviates Minimal value of Optimization Problem.
Implementation. Mop is readily implemented using NEvaluate (Figure 12 in the appendix). Let
xo,...,xn ∈ R have be drawn iid from μ, let V be formed from these points, and let e% be the ith co-
ordinate vector. Then Vei = [qi(xk)]k andEn+ɪ T ((Vei)。(Vej)) = E n+1 Pkn=0 qi(xk)qj (xk) =
Eqi(x)qj (X). Thus, σ and Σ are obtained by calling NEvaluate, with a batch of x, on eι,..., e2
Choice of L. An obvious choice for L is the squaredFrobenius norm Lp(σ, Σ) = ∣∣σ∣∣2 + ||I一 ∑∣∣2.
In our experience, we encountered more success with Lkl (σ, ∑) = ∣∣σ∣∣2 +tr Σ-log det Σ-(n+1),
which is the Kullback-Leibler divergence between a standard normal distribution and N(σ, Σ). One
possible explanation is that the sampling distributions of the estimators are normal. Another is a low
Jensen gap in E Lkl(Σ) = tr(Σ) + Ei Elogλi(Σ).
Stochastic optimization. Our approach extends beyond deterministic objectives f to stochastic
objectives which predominate machine learning (Srebro & Tewari, 2010). In this setting, D is a
distribution over examples z — for example, input-output pairs in supervised learning. fz (w) is
typically the loss of weights W on example z. The stochastic objective F(W) = Ez〜Dfz(w) is
handled in Mop by combining the sampling of Z 〜D and W 〜P in μ. In other terms, μ is
the posterior loss (or negative log-likelihood), over the data distribution D, of a Bayesian neural
network with prior P over its parameters W.
6
Under review as a conference paper at ICLR 2021
procedure Mop(f, D, ρ)
procedure μ
Z〜D
W〜P
x = fz(w)
return x
α,β = arg mi□α β E L(σ, Σ)
where σ = IL [qι(x),..., qn(x)]
-U q 一 , .	....... ■、、_
and ∑ H E [(qi(x) - σi)(qj(X)- σi)]%=ι
λ = smallest eigenvalue of Jn
return λ
Figure 8: Mop iteratively draws a batch from
μ, forms the estimates σ and Σ, and descends
on their loss. fz (w) is the loss of parame-
ters w on the example z. ρ is the prior over
w and D is the data distribution of z. μ is
a distribution over R. Larger n trades more
computation for better approximation. The
expectation Ex is over X ∈ R drawn iid μ.
E is the empirical expectation formed from a
batch. Recall Jn is defined in (3).
Limitations and failure modes. As mentioned, approximating f * is a computationally intractable
problem. Furthermore, Mop is a statistical query algorithm (Kearns, 1998): it uses gradient esti-
mates computed from batches of data, and does not directly manipulate individual examples. It is
therefore subject to stronger (information-theoretic) lower bounds than the previously-mentioned
NP-hardness of minimal value approximation (Reyzin, 2020). Accordingly, there are different ways
in which Mop can fail or demand exorbitant resources. It is possible to encounter local minima or
saddle points of L with respect to α and β . The approximation of f * by λ occurs asymptotically
as n grows, so a large n may be required for some f . Larger n, in turn, demand more samples
of f. λ → f* from above for Jn defined by the optimal (α*, β*); this is not necessarily true for
SUboPtimaI (α,β) obtained by an inexact algorithm. Furthermore, Lasserre S result does not take
finite-sample approximation of moments (or in our setting, covariances) into consideration.
5.3	Basic Empirical Evaluation
As a preliminary test, we apply the algorithm to f with known f*. Some of these problems are easy,
and Mop is able to solve them. One of them is impossible to solve, so Mop fails. Details of these
experiments are given in the appendix.
1.	Recovering classical orthogonal polynomials. Mop defines μ in terms of f and ρ. Before doing
that, let us simply take the μ listed in Figure 1, and see if minimizing L(σ, Σ) recovers the listed
coefficients (α*,β*). As depicted in Figure 9, success depends on the choice of L and μ. In all
cases, LKL achieved lower error than LF . The Chebyshev and Legendre coefficents were easier to
recover than the Hermite and Laguerre ones. This agrees with the intuitions in Figure 7.
2.	Test polynomials. Lasserre’s scheme has been run on 4 standard test polynomials (Lasserre,
2020). These are bivariate (N = 2) and have minimum value f* = 0 on W = [-1, 1]N. We
normalized the polynomials to take values in [0, 1]. To distinguish the algorithm’s behavior from
trivially returning 0, we added constants (either 0.2, 0.4, 0.6, or 0.8), shifting the f*. Figure 10
shows LKL reasonably approximates f*, whereas the LF does not. Due to the normalization, our
results are quantitatively different than those previously reported, but are qualitatively the same: the
Matyas polynomial is the easiest, and the camel polynomial is the hardest (Laurent & Slot, 2020).
Note that λ produced midway during optimization are neither upper nor lower bounds of f*; it is
important to completely optimize α and β .
3.	Learning halfspaces / noisy parities. Let X be uniform on the hypercube {-1, 1}N. For some
noise rate η > 0, y is the parity of X with probability 1 - η, and is negated with probability η. Let
f* = arg minw∈RN -Ehw(X)y be the (negated) correlation of the best possible (smooth) halfspace
hw (X) = tanh(wT X). Using a statistical query algorithm, it is impossible to distinguish f* from
zero, by the reduction of Kalai et al. (2008) to learning noisy parities (Blum et al., 1994). For
N = 16 and η = 0 we attempt to use Mop to approximate f*. For varying values ofn, we run Mop
samples (x, y). For distinguishment, We run it on (x, y) where y are random signs. In Figure 11, We
see that Mop does not distinguish these distributions, regardless of n.
7
Under review as a conference paper at ICLR 2021
1
0.50
0.10
0.05
0.01
ιιβ*- βι∣∕∣ιβ*ιι
r......... LagUerre
L -，一 -	~ ~~	∙-^---^
'S‰n⅛√*>kκ
Hermite
c	Chebyshev
Legendre
0	1000	2000
T
Figure 10: Mop on toy polynomials. Solid
lines are true f *, thick dashed lines are
Mop using LKL, and dotted lines use LF .
Mop using LKL approximates the f *, but
only after optimization is complete.
0.500 λ
500	1000
Figure 11: Mop fails to
distinguish parity data
(colored lines) from
noise (gray lines), no
matter the value of n.
Figure 9: Mop recover-
ing β* for different Clas-
sical polynomials. Thick
dashed lines use LKL .
Dotted lines use LF.
6	Related Work
Reversibility underpins low-memory, reverse-mode automatic differentiation (Gomez et al., 2017).
Besides our manual derivation, there are other ways to obtain our VJPs, though they would require
comparable effort, and may not be as practical. If an algorithm is written in a reversible programming
language — which is not a trivial rewriting — then its VJPs can be computed with low memory
overhead (Liu & Zhao, 2020). Forward mode differentiation uses O(n) memory, but generally
requires a factor O(n) more computation, and needs software support to intermix with reverse mode.
Learned image compression algorithms usually replace traditional compression pipelines with neu-
ral networks (Jiang, 1999). They tend to achieve excellent compression results at the expense of
computational complexity. Even as machine learning hardware becomes faster, there will likely be
a role for fast, simple compression algorithms. This is amply demonstrated by the most powerful
GPU ever released, which adds five dedicated cores for JPEG decoding (Lisiecki et al., 2020).
Orthogonal polynomial transforms are subsumed by recently-developed representations of struc-
tured linear maps, such as tridiagonal factorizations of low-displacement rank operators (Thomas
et al., 2018), and butterfly factorizations culminating in the Kaleidoscope hierarchy (Dao et al.,
2019; 2020). It is appropriate to think of unstructured matrices, Kaleidoscope, AnyPT, and fixed
polynomial transforms as varying tradeoffs between expressive power and tractability.
Our results partially extend to the complex domain. The Fast Fourier Transform, perhaps the most
well-known orthogonal polynomial transform, involves the monomials (α = β = 0) of the roots of
unity xi ∈ C. This can be handled by our algorithms, and indeed the original version of Interpolate
(Bjθrck & Pereyra, 1970). The SzegO polynomials, which are orthogonal with respect to a Hermitian
inner product on the unit circle, need a variant of Interpolate (Bella et al., 2007).
The problem of finding the orthogonal polynomials (as α, β) which match the given distribution (as
moments m) was first studied by Chebyshev. The map m 7→ (α, β) is ill-conditioned (Gautschi,
1967), so it is preferable to begin with “modified” moments (Gautschi, 2004).
7	Conclusions and Future Work
This work enables gradient-based optimization over sequences of orthogonal polynomials, and takes
an initial foray into such optimization problems. Based on the algorithm of Bella et al. (2009), our
core techniques might extend from polynomial Vandermonde matrices to quasiseparable matrices.
With sufficient computational resources, Mop could be applied larger problems, especially modern
neural networks whose minima are unknown. It may be possible to develop satisfactory theory for
Mop: it has a well-defined optimum (α*, β*) which achieves a known (true) loss of 0, is simultane-
ously understood via Lassere’s moment-based approach.
8
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,
Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian
Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefow-
icz, LUkasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mane, Rajat Monga,
Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner,
Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer-
nanda Viegas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu,
and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous
systems, 2015. URL https://github.com/tensorflow/tensorflow/blob/
e61bc26e00f48db9abaf165f343f1f44a10227a9/tensorflow/python/ops/
linalg_grad.py#L539. Gradient for MatrixSolve.
Stephen Barnett. Some applications of the comrade matrix. International Journal of Control, 21(5):
849-855,1975.
Roberto Barrio. Parallel algorithms to evaluate orthogonal polynomial series. SIAM Journal on
Scientific Computing, 21(6):2225-2239, 2000.
Atilim GUnes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Re-
search, 18(1):5595-5637, 2017.
Tom Bella, Yuli Eidelman, Israel Gohberg, Israel Koltracht, and Vadim Olshevsky. A bjorck-
pereyra-type algorithm for szego-vandermonde matrices based on properties of unitary hessen-
berg matrices. Linear algebra and its applications, 420(2-3):634-647, 2007.
Tom Bella, Yuli Eidelman, Israel Gohberg, and Vadim Olshevsky. Computations with quasiseparable
polynomials and matrices. Theoretical Computer Science, 409(2):158-179, 2008.
Tom Bella, Yuli Eidelman, Israel Gohberg, Israel Koltracht, and Vadim Olshevsky. A fast bjorck-
pereyra-type algorithm for solving hessenberg-quasiseparable-vandermonde systems. SIAM Jour-
nal on Matrix Analysis and Applications, 31(2):790-815, 2009.
Jean-Paul Berrut and Lloyd N Trefethen. Barycentric lagrange interpolation. SIAM review, 46(3):
501-517, 2004.
Ake Bjorck and Victor Pereyra. Solution of vandermonde systems of equations. Mathematics of
computation, 24(112):893-903, 1970.
Avrim Blum, Merrick Furst, Jeffrey Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich.
Weakly learning dnf and characterizing statistical query learning using fourier analysis. In Pro-
ceedings of the twenty-sixth annual ACM symposium on Theory of computing, pp. 253-262, 1994.
Ignace Bogaert. Iteration-free computation of gauss-legendre quadrature nodes and weights. SIAM
Journal on Scientific Computing, 36(3):A1008-A1026, 2014.
Alin Bostan, Bruno Salvy, and Eric Schost. Fast conversion algorithms for orthogonal polynomials.
Linear Algebra and its Applications, 432(1):249-258, 2010.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, and Skye Wanderman-Milne. JAX: composable transformations of Python+NumPy
programs, 2018. URL http://github.com/google/jax.
D Calvetti and L Reichel. Fast inversion of vandermonde-like matrices involving orthogonal poly-
nomials. BIT Numerical Mathematics, 33(3):473-484, 1993.
Ed S Coakley and Vladimir Rokhlin. A fast divide-and-conquer algorithm for computing the spectra
of real symmetric tridiagonal matrices. Applied and Computational Harmonic Analysis, 34(3):
379-414, 2013.
9
Under review as a conference paper at ICLR 2021
Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher Re. Learning fast algorithms for
linear transforms using butterfly factorizations. volume 97 of Proceedings of Machine Learning
Research,pp. 1517-1527, Long Beach, California, USA, 09-15 JUn 2019. PMLR. URL http:
//proceedings.mlr.press/v97/dao19a.html.
Tri Dao, Nimit Sohoni, Albert GU, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri
Rudra, and Christopher Re. Kaleidoscope: An efficient, learnable representation for all structured
linear maps. In International Conference on Learning Representations, 2020. URL https:
//openreview.net/forum?id=BkgrBgSYDS.
James R Driscoll, Dennis M Healy Jr, and Daniel N Rockmore. Fast discrete polynomial transforms
with applications to data analysis for distance transitive graphs. SIAM Journal on Computing, 26
(4):1066-1099, 1997.
Hei Tao Fung and Kevin J Parker. Design of image-adaptive quantization tables for jpeg. Journal of
Electronic Imaging, 4(2):144-151, 1995.
Walter Gautschi. Computational aspects of three-term recurrence relations. SIAM review, 9(1):
24-82, 1967.
Walter Gautschi. How (un) stable are vandermonde systems. Asymptotic and computational analy-
sis, 124:193-210, 1990.
Walter Gautschi. Orthogonal polynomials. Oxford university press Oxford, 2004.
Walter Gautschi. Orthogonal polynomials (in matlab). Journal of computational and applied math-
ematics, 178(1-2):215-234, 2005.
Israel Gohberg and Vadim Olshevsky. Fast algorithms with preprocessing for matrix-vector multi-
plication problems. Journal of Complexity, 10(4):411-427, 1994.
Israel Gohberg and Vadim Olshevsky. The fast generalized parker-traub algorithm for inversion of
vandermonde and related matrices. Journal of Complexity, 13(2):208-234, 1997.
Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual net-
work: Backpropagation without storing activations. In Advances in neural information processing
systems, pp. 2214-2224, 2017.
Nicholas J Higham. Error analysis of the bjorck-pereyra algorithms for solving vandermonde SyS-
tems. Numerische Mathematik, 50(5):613-632, 1987.
Nicholas J Higham. Fast solution of vandermonde-like systems involving orthogonal polynomials.
IMA Journal of Numerical Analysis, 8(4):473-486, 1988.
Nicholas J Higham. Stability analysis of algorithms for solving confluent vandermonde-like sys-
tems. SIAM Journal on Matrix Analysis and Applications, 11(1):23-41, 1990.
Nicholas J Higham. Iterative refinement enhances the stability ofqr factorization methods for solving
linear equations. BIT Numerical Mathematics, 31(3):447-468, 1991.
Nicholas J Higham. Accuracy and stability of numerical algorithms. SIAM, 2002.
Max Hopkins, Michael Mitzenmacher, and Sebastian Wagner-Carena. Simulated annealing for jpeg
quantization. In 2018 Data Compression Conference, pp. 412-412. IEEE, 2018.
Mourad Ismail, Mourad EH Ismail, and Walter van Assche. Classical and quantum orthogonal
polynomials in one variable, volume 13. Cambridge university press, 2005.
J Jiang. Image compression with neural networks-a survey. Signal processing: image Communica-
tion, 14(9):737-760, 1999.
Thomas Kailath and Vadim Olshevsky. Displacement-structure approach to polynomial vander-
monde and related matrices. Linear Algebra and Its Applications, 261(1-3):49-90, 1997.
10
Under review as a conference paper at ICLR 2021
Adam Tauman Kalai, Adam R Klivans, Yishay Mansour, and Rocco A Servedio. Agnostically
learning halfspaces. SIAMJournal on Computing, 37(6):1777-1805, 2008.
Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM
(JACM), 45(6):983-1006, 1998.
Jean B Lasserre. Connecting optimization with spectral analysis of tri-diagonal matrices. Mathe-
matical Programming, pp. 1-15, 2020.
Monique Laurent and Lucas Slot. Near-optimal analysis of univariate moment bounds for polyno-
mial optimization. arXiv preprint arXiv:2001.11289, 2020.
Gregory Leibon, Daniel N Rockmore, Wooram Park, Robert Taintor, and Gregory S Chirikjian. A
fast hermite transform. Theoretical computer science, 409(2):211-228, 2008.
Janusz Lisiecki, Michal Szolucha, Joaquin Anton Guirao, and Maitreyi Roy.
Loading data fast with dali and the new hardware jpeg decoder in nvidia
a100 gpus, 2020.	URL https://developer.nvidia.com/blog/
loading-data-fast-with-dali-and-new-jpeg-decoder-in-a100/.
Jin-Guo Liu and Taine Zhao. Differentiate everything with a reversible programming language.
arXiv preprint arXiv:2003.04617, 2020.
Xiyang Luo, Hossein Talebi, Feng Yang, Michael Elad, and Peyman Milanfar. The rate-distortion-
accuracy tradeoff: Jpeg case study. arXiv preprint arXiv:2008.00605, 2020.
Daniel Potts. Fast algorithms for discrete polynomial transforms on arbitrary grids. Linear algebra
and its applications, 366:353-370, 2003.
William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical
Recipes in C (2nd Ed.): The Art of Scientific Computing. Cambridge University Press, USA,
1992. ISBN 0521431085.
Thomas W Reps and Louis B Rall. Computational divided differencing and divided-difference
arithmetics. Higher-order and symbolic computation, 16(1-2):93-149, 2003.
Lev Reyzin. Statistical queries and statistical algorithms: Foundations and applications. arXiv
preprint arXiv:2004.00557, 2020.
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In Interna-
tional Conference on Machine Learning, pp. 1530-1538, 2015.
Richard Shin and Dawn Song. Jpeg-resistant adversarial images. In NIPS 2017 Workshop on Ma-
chine Learning and Computer Security, volume 1, 2017.
Francis J Smith. An algorithm for summing orthogonal polynomial series and their derivatives with
applications to curve-fitting and interpolation. Mathematics of Computation, 19(89):33-36, 1965.
Alicja Smoktunowicz. Backward stability of clenshaw’s algorithm. BIT Numerical Mathematics,
42(3):600-610, 2002.
Nathan Srebro and Ambuj Tewari. Stochastic optimization for machine learning. ICML Tutorial,
2010.
Anna Thomas, Albert Gu, Tri Dao, Atri Rudra, and Christopher Re. Learning compressed trans-
forms with low displacement rank. In Advances in neural information processing systems, pp.
9052-9060, 2018.
George Toderici, Wenzhe Shi, Radu Timofte, Lucas Theis, Johannes Balle, Eirikur Agustsson,
Nick Johnston, and Fabian Mentzer. Workshop and challenge on learned image compression
(clic2020), 2020. URL http://www.compression.cc.
Paul Valiant. Three perspectives on orthogonal polynomials. FOCS 2016 Workshop: (Some)
Orthogonal Polynomials and their Applications to TCS, 2016. URL http://www.cs.
columbia.edu/~ccanonne/Workshop-focs2016/.
Gregory K Wallace. The jpeg still picture compression standard. IEEE transactions on consumer
electronics, 38(1):xviii-xxxiv, 1992.
11
Under review as a conference paper at ICLR 2021
procedure NEvaluate(x, α, γ , c)
u = [cn, . . . , cn]
v = [0, . . . , 0]
for k ∈ [n - 1, . . . , 0] do
τ=u
((X - αk"γk+1) ∙
v=τ
-Yk+2V + Ck
μ = u∕γo
return μ
Figure 12: Variants of Evaluate and
Interpolate for orthonormal polyno-
mial sequences. As in Interpolate,
DiVDiffs is the first row of the table
of divided differences.
procedure NInterpolate(x, α, γ, y)
δ = DivDiffs(x,y)
return NChangeBasis(x, α, γ, δ)
procedure NChangeBasis(x, α, γ, δ)
c=δ
cn-1 =+ (α0 - xn-1)cn
cn = γ1 cn
for k ∈ [n - 2, . . . , 0] do
ck += (α0 - xk)ck+1 + γ1 ck+2
for j ∈ [1, . . . , n - 2 - k] do
ck+j = γj ck+j + (αj - xk )ck+j+1
+γj+1ck+j+2
cn-1 = γn-k-1cn-1 + (αn-k-1 - xk )cn
cn = γn-k cn
σ = γo ∙ c
return σ
u
u
procedure VTMultiply(x, α, β , c)
q = [0,. . . ,0]
p= [1,...,1]
y = [0,. . . ,0]
for i ∈ [0, . . . , n] do
for j ∈ [0, . . . , n] do
y = Pj ∙cj
if i < n then
τ = pj
pj = (xj -αi)pj -βiqj
qj = τ
return y
procedure NVTMultiply(x, α, γ, c)
q= [0,...,0]
p = [γ0 , . . . , γ0 ]
y = [0,...,0]
for i ∈ [0, . . . , n] do
for j ∈ [0, . . . , n] do
y = Pj ∙ Cj
if i < n then
τ = Pj
Pj = ((Xj - ai)/Yi+i)Pj- γγ=ιqj
qj = τ
return y
Figure 13: Transpose Vandermonde multiplication for monic orthogonal (left) and orthonormal
(right) polynomial sequences. These use O(n2) time using O(n) space. They are used in the
following algorithms for VJPs.
A	Appendix
A. 1 Numerical Stability of Interpolate
The divided differences of y at X are the following 11 (n + 1)(n + 2) recursively-defined values:
y[χi] = y	y[χo,χι]	=	y[x1] - y[x°]	y[χj,…，χk]	=	y[xj+1,…，Xk] -	y[xj,…，xk-1]	»)
X1 - X0	Xk - Xj
DivDiffs(X, y) = [y[X0], y[X0, X1], . . . , y[X0, . . . , Xn]] ∈ Fn+1 is called the first row of the table of
divided differences. They are also known as the coefficients of the Newton interpolant of the data
(X, y) (Berrut & Trefethen, 2004). Interpolate starts by computing these coefficients δ . Then, it
changes the basis of these coefficients to the specified orthogonal polynomial sequence. The blame
for its instability lies with the first step: naive calculation of divided differences, according to the
definition (5), can lead to severe numerical error.
This error can be ameliorated in some simple ways. The most appropriate way depends on the
amount of control over the input data. If X can be chosen arbitrarily, then complex nodes can
be more stable (Gautschi, 1990). In particular, consider setting Xk = e-2πizk where z is low-
discrepancy sequence, such as the van der Corput sequence. Intuitively, such sequences will keep
12
Under review as a conference paper at ICLR 2021
denominators xi -xj close to uniform. If x is real, then stability strongly depends on the ordering of
x0, . . . , xn (Gohberg & Olshevsky, 1997; Higham, 1990). Choosing x randomly, as is typical when
training a neural network, is a poor choice. If x is in increasing order, then divided differencing is
(relatively) stable (Higham, 1987). If some points are nonnegative, then the Leja ordering explicitly
maximizes the relevant denominators xi - xj (Higham, 1990). If all these options are exhausted,
iterative refinement can eliminate moderate amounts of error (Higham, 1991).
The aforementioned fixes may work in specific scenarios, but they do not satisfactorily solve the
numerical instability of divided differencing. Fortunately, there is a principled, general solution to
this problem. Divided differencing, like differentiation, is a composable function transformation
(Reps & Rall, 2003): given a function f, the function x 7→ f [x0 , . . . , xn] computing its divided
difference can be automatically derived. Existing software packages for machine learning support
the addition of such transformations (Bradbury et al., 2018).
Thus, the Interpolate algorithm of Higham (1988)’s algorithm is both simple and worthwhile in
the long term. The Parker algorithm, which computes V -1 and then dense-multiplies V -1y, is
known to be more numerically stable (Gohberg & Olshevsky, 1997; Calvetti & Reichel, 1993).
However, it requires O(n2) memory. Specialized Gaussian elimination with partial pivoting (Kailath
& Olshevsky, 1997) takes O(n2) time and O(n) memory. However, its implementation is involved,
and is asymptotically slower than Fourier-based methods. Some of the cited O(n log2 n) algorithms
in Figure 2 may also be numerically unstable; see, for example, Remark 4.2 in Bella et al. (2008).
A.2 Vector-Jacobian Products for Evaluate and Interpolate
The following algorithms have been numerically checked against finite differencing. We use stan-
dard “overbar” notation for adjoints (Baydin et al., 2017). Let ` be the final scalar loss produced in
the entire forward pass. The adjoint of each intermediate variable Ui is Ui =需.
A.2.1 Evaluate
By linearity, C = yτVcVc = (yτV)t = Vty. By similarly elementary operations:
X= yT h dVc ik = hX yi X 1(i = k)Pj (Xi )cj i k = hyk X Pj (Xk)CjL= y ◦ (V 0c)
k	ij	j
Now we derive the VJPs with respect to α and β . Here is Evaluate written with indexed notation for
U, which distinguishes the intermediate values.
procedure Evaluate(X, α, β , c)
U(n) = [cn, . . . , cn]
U(n+1) = [0, . . . , 0]
for k ∈ [n - 1, . . . , 0] do
u(k) = (x — ak) ∙ u(k+1) — βk+ιu(k+2) + Ck
return U(0)
The adjoints are computed via standard reverse accumulation. Also, exploiting reversibility, prior
values U(k+2) can be computed from later values U(k+1).
U(k+1) =U(k) ∙ (x - ak)
U(k+2) =U(k) ∙ (-βk+l)
αck =Ukk ∙ (-u(k+1))
βk+ι =U(k) ∙ (-u(k+2k)
u(k+2k =-^-(-u(k) + (x - αk) ∙ u(k+1k + Ck)
βk+1
Performing these computations, in the reverse order of Evaluate, yields the VJP for α and β .
13
Under review as a conference paper at ICLR 2021
procedure EvalUate(χ, α, β, u(0), u(1), U(O),U(I))
a = β = [0, ..., 0]
for k ∈ [0, . . . , n - 1] do
u(k+2) = β1-^(—u(k) + (x — αk) ∙ u(k+1) + Ck)
αk = —U(k) ∙ u(k+1)
βk+i = —U(k) ∙ u(k+2)
U(k+1) = U(k) ∙ (X — ak)
U(k+2) = Ulk) ∙ (—βk+l)
return α, β
Note that v = 0 since it is computed during the forward pass, but is not part of the output ofEvaluate.
The full VJP for Evaluate, given in Figure 4, includes the computation of X and c, and elides the
indexing of intermediate values.
A.2.2 Interpolate
It is known that CTNyC is the solution y to VTy = C (Abadi et al., 2015). Also, CT VvC = -ycT.
By the chain rule, VxC = 需◦ ∂V. By the definition of V, d-Vj = 1(i = k)pj(Xk). Therefore,
similarly to the previous derivation for evaluation:
TTdV
X = C VxC = —yC ◦--
dXk k
X —yCiCj1(i = k)p0j(Xi)	= X —yCkCjp0j(Xk)
i,j	j
-y ◦ [X Cjpj (χk)] = -y ◦ (v 0c)
jk
For αC and βC, we write ChangeBasis in the indexed notation of Higham (1988).
procedure ChangeBasis(X, α, β, δ)
C(n) = δ
C(nn--11) = C(nn-)1 + (α0 — Xn-1)C(nn)
C(n-1) = C(n)
Cn	= Cn
for k ∈ [n — 2, . . . , 0] do
C(kk) = C(kk+1) + (α0 — Xk)C(kk++11) +β1C(kk++21)
for j ∈ [1, . . . , n — 2 — k] do
C(k)	=	C(k+1) +	(α	—	X )C(k+1)	+
Ck+j	=	Ck+j +	(αj	—	Xk)Ck+j+1	+
βj+1C(kk++j1+)2
C(nk-)1 = C(nk-+11) + (αn-k-1 — Xk)C(nk+1)
(k)	(k+1)
Cn = Cn
return C(0)
14
Under review as a conference paper at ICLR 2021
We derive the reverse-mode adjoints in the usual manner.
c(k) = c(k + 1)
Cn = Cn
Jk), = C(k+I) + (α - 1 - Xk )c(k + 1)
cn-1 = cn-1 + (ɑn-k-1	xk )cn
c(k) = c(k+1) + (α- x X以)c(k+1) + β-I-∣c(k+1)
ck+j = ck+j + (αj Xk )ck+j + 1 + βj + 1ck+j+2
Ckk) = ckk+1) + («0 X χk)ckk+1I) + β1ck%I)
c(n-1) = c(n)
Cn	= Cn
c(n-1) = c(n)-, + (α∩ X X	-l )c(n)
cn-1	= cn-1 + («0	Xn-I)Cn
或k+1)=木)
C(k+1) Cn-1	(k) =cn-1
C(k+1) Cn	=Cn-1 ∙ («n-k-1 X Xk)
αn-k-1	± C(k)i ∙ C(k+1) =Cn-1 Cn
(k+1) Ck+j	=C(k). k+j
(k+1) Ck+j+1	=ck+j ∙ («j x Xk)
C(k+1) Ck+j+2	(k) =Ck+j ∙ βj+1
	± (k)	(k+1)
«j	=Ck+j ∙ %+j + 1
	± (k)	(k+1)
βj+1	=Ck+j . %+j+2
(k+1) Ck	(k) =ck
(k+1) Ck+1	=Ckk) ∙ («0 X Xk)
C(k+1) Ck+2	=Ckk) ∙ β1
«0	± Ckk) ∙ c3)
β1	± Ckk) ∙ c"
娘	=C(n-1) =Cn
(n)	(n-1)
Cn-I	=CCn-1
螳	=Cn∏I) ∙ («0 X Xn-1)
«0	± C(n-1) ∙ C(n) =Cn-1	Cn
Observe that the reverse accumulation involves values c(k) for k > 0. As with Evaluate, we can
compute C(k+1) from C(k).
C(k) = C(k+1)
Cn = Cn
c(k) = c(k+1) + («	. 1 X X，)c(k+1)
cn-1 = cn-1 + (αn-k-1	Xk )Cn
c(k) = c(k+1) + (α∙-X以)c(k+1) + β-I-∣c(k+1)
Ck+j = ck+j + («j	Xk )ck+j+1 + βj + 1ck+j+2
Ckk) = ckk+1) + («0 X Xk)Ck+1I) + β1ck%I)
C(n-1) = C(n)
Cn	= Cn
Cn-II)=Cn-I+(«oX Xn-I)Cnn)
^⇒	C(k+1) = C(k) Cn	= Cn
^⇒	C(k+1) = C(k) X («	X X )C(k+1) Cn-1 = Cn-1	(«n-k-1	Xk )Cn
^⇒	C(k+1) = C(k) X (« X X )C(k+1) X β C(k+1) Ck+j = Ck+j X («j X Xk)Ck+j+1 X βj+1Ck+j+2
^⇒	Ckk+1) = Ckk) X («0 X Xk)Ck+1I) X 员43I)
^⇒	C(n) = C(n-1) Cn	= Cn
^⇒	(n)	(n-1)	(n) Cn-1 = Cn-1	X («0 X Xn-1)Cn
Combining the adjoint equations with the reversed computation of C(k), we obtain the VJP with
respect to α and β. The full algorithm in Figure 4 elides no-ops and the indexed notation.
15
Under review as a conference paper at ICLR 2021
procedure ChangeBasis(x, α, β, δ, c(0),c(0))
α,β = [0,..., 0]
for k ∈ [0,...,n — 2] do
√k+1) = √k)
Cn	= Cn
C(k+1) — C(k)
Cn-1	= Cn-1
十 (k)
αn-k-1 = Cn-I
cnk+1)=Cnk-1 .
-(αn-k-1 - xk)cn +)
. c(k+1)
Cn
. (an-k-1 — Xk)
(k+1) — (k)
Cn-1	= Cn-1
for j ∈ [n — 2 — k,..., 1] do
C(k+1) = C(k) - (α- - Xk)C(k+1) - β-I iC(k+1)
Ck+j = Ck+j (Uj xk )Ck+j+1 βj + 1Ck+j+2
-十 -(k)	(k+1)
αj = ck+j • Ck+j+1
C 十 K(k)	∕k+1)
βj + 1 = ck+j • Ck+j+2
c(k+D
ck+j+2
√k+i)
ck+j+1
√k+1) 一
ck+j
Ckk+1)=
a0 i Xk
+ 十-(k)
β1 = Ck
*D =
=ck+j • βj+1
=ck+j . (aj - xk)
(k)
=ck+j
Ckk) - (Q0 - xk)Ck+11) - β1ck%I)
,(k+1)
'k+1
,(k+1)
k+2
√k+1) + √k)
ck⅛1 = Ck
齐(k+1)十 F(k)
ck⅛2 = Ck .
(n) = (n-1)
Cn	- Cn
(n) = c(n-1)
n-1 = cn-1	-
Q0=Cn-II) . Cnn)
• (α0 - Xk)
• βι
(ɑ0 - Xn-I)Cnn)
(n)	(n-1)
Cn ——Cn
Cg) = ⅛-LI) . (Q0 - Xn-1)
(n) = (n-1)
n-1 = cn-1
return Q, β
• c
• c

A.2.3 VECTOR-JACOBIAN PRODUCT IN α AND Y FOR NEVALUATE
In the notation of Higham (1990), the three-term recurrence is:
p-1(x) = 0； po(x) = 1； Pj+1(x) = θj(X - βj)pj (x) - YjPj-I(X)
Their θj,βj, and Yj correspond to our 1∕γj+1,07∙/γj+1, and Yj/γj+1, respectively. Under their
normalization po(x) = 1, suppose the solution of their algorithm is C. Then the solution under
Po (x) = Y-1, as in the orthonormal polynomials, is γo . C. Following the template of the previous
VJP derivations, here is NEvaluate written in indexed notation.
procedure NEvaluate(x, α,Y,c)
Un) = [Cn,...,Cn]
for k ∈ [n — 1,..., 0] do
u(k) = ((x - ak)∕Yk+1) . u(k+1)- γk+2u(k+2) + Ck
μ = u(0)/Y0
return μ
16
Under review as a conference paper at ICLR 2021
The adjoints and U are derived as before.
u(0) =μ∕γo
U(o) =μ ∙ Yo
Y(O) = - μ ∙U(O)“
U(k+1) ± U(k) ∙ (x - ɑk)∕γk+1
U(k+2) =U(k) ∙ (-3)
Yk+2
αk =u(k) ∙ (-u(k+1^Yk+1)
7k+1 =U(k) ∙ (-(x - αk)u(k+1)∕γ2+1 - u(k+2)∕γk+2)
μk+2 =u(k) ∙ (Yk + 1u(k+丝γk+2)
u(k+2) = γk+2(-u(k) + ((x - Qk)∕Yk+1) ∙ u(k+1) + Ck)
Yk+1
Computing these quantities in reverse order yields the VJP algorithm.
procedure NEVaIUate(x, α, γ, μ, u(1),μ, U(1))
α = Y = [0,..., 0]
u(0)= μ ∙ γ0
U(0)= μ∕γo
Yo = -μ ∙ U(O)∕γ2
for k ∈ [0,..., n — 1] do
u(k+2) = Yk+2(-u(k) + ((X - ak)∕γk+ι) ∙ u(k+1) + Ck)
Yk+1
ak = -U(k) ∙ (u(k+1)∕γk+ι)
7k+ι = - U(k) ∙ ((x - ak)u(k+1)∕γ2+ι + u(k+2)/Yk+2)
μk+2 = u(k) ∙ (7k+1u(k+2)/72+2)
U(k+1) + u(k) ∙ (x - αk)∕γk+1
U(k+2) = U(k) ∙ (-γk+1)
' Yk+2'
return α, B
The final algorithm elides no-ops and indexing, and is given below.
17
Under review as a conference paper at ICLR 2021
procedure NEvalUate(x, α, γ, μ, v, μ)
V = 0
α = β = [0,..., 0]
U = μ ∙ Yo
U -
Yo
for k ∈ [0, . . . , n - 1] do
αk = -UT v∕γk+ι
7k+ι = - UT(X - αk) ∙ v/Y2+i
if k < n - 1 then
W = γk+2 (-u + x-αk ∙ V + Ck)
γk+1	γk+1	k
7k + 1 = - UTw/Yk+2
Yk+2 = UTW ∙ Yk + 1/Y2+2
U, V = V, w
T = U
U = V + U ∙ (x - α∙k)∕γk+l
V = -τ ∙ γk+1
γk+2
return α, β
A.3 Experiment Details
Op Benchmarks (FigUre 5). The comparison implementation is written in JAX (BradbUry et al.,
2018), with for loops specially expressed as strUctUred control flow. Python’s timeit is Used to
perform timing, taking the best of 4 rUns, each having 2 repetitions. A batch size of 32 is Used.
Learnable JPEG. Adagrad is Used as the optimizer. Learning rates of 0.2 and 2.0 performed best
with and withoUt AnyPT, respectively. The standard train/test split of CLIC is Used. The batch size
is 1.
Mop Experiment 1. n = 8, B = 4096, and δ = 0. The initialization is αi = 0 and βi = 1/2. βo,
βn, and αn are not in Jn , and so are not measUred as part of the relative error. The error of α is not
plotted becaUse it follows the same pattern.
Mop Experiment 2. n = 8, B = 8192, and δ = 0. α was initialized with a Glorot normal and β by
random U(0, 1). The optimizer is RMSprop with learning rate 0.05 and gradient clipping.
Mop Experiment 3. n = 8, B = 4096. VarioUs optimizers (inclUding SGD and RMSprop) were
attempted, and none changed the (lack of) resUlts.
18