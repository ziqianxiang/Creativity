Under review as a conference paper at ICLR 2021
Azimuthal Rotational Equivariance in Spheri-
cal CNNs
Anonymous authors
Paper under double-blind review
Ab stract
In this work, We analyze linear operators from L2(S2) → L2 (S2) which are
equivariant to azimuthal rotations, that is, rotations around the z-axis. Several
high-performing neural networks defined on the sphere are equivariant to az-
imuthal rotations, but not to full SO(3) rotations. Our main result is to show
that a linear operator acting on band-limited functions on the sphere is equivariant
to azimuthal rotations if and only if it can be realized as a block-diagonal matrix
acting on the spherical harmonic expansion coefficients of its input. Further, we
show that such an operation can be interpreted as a convolution, or equivalently, a
correlation in the spatial domain. Our theoretical findings are backed up with ex-
perimental results demonstrating that a state-of-the-art pipeline can be improved
by making it equivariant to azimuthal rotations.
1	Introduction
Signals defined on the surface ofa sphere arise frequently in applications. For example, omnidirec-
tional cameras are increasingly being utilized in applications that benefit from a large field of view,
such as visual odometry and mapping (Matsuki et al., 2018). Spherical signals may also arise from
other forms of measurements, such as magnetoencephalography images of brain activity or from
planetary data like surface temperature or wind speed measurements (Mudigonda et al., 2017).
Convolutional neural networks (CNNs) (LeCun et al., 1998) have achieved remarkable success in the
analysis of images and other data that is naturally arranged in a planar, rectangular grid, but applying
them to spherical data has proven more challenging. The reason for this is that it is impossible to
map the sphere to a flat surface without introducing distortions. For instance, in the equirectangular
projection of spherical images, objects are increasingly stretched out as they approach the poles.
To remedy this, several alternatives to the 2D convolution specifically designed for functions on the
sphere have been proposed in the recent literature. Some of these run a 2D CNN on an equirectan-
gular projection of the spherical signal, but modify the kernel (Su & Grauman, 2019), the sampling
pattern of the kernel (Coors et al., 2018), or run the CNN on projections to different tangent planes
in order to minimize the effect of the distortion (Eder et al., 2020). Other approaches discretize the
sphere, where the vertices are intepreted as nodes in a graph, allowing them to apply the machinery
of Laplacian-based graph convolutions (Defferrard et al., 2016; 2019) to the problem.
One of the main properties that make regular CNNs so successful is their equivariance to transla-
tions. In fact, it is known from abstract algebra that a convolution is deeply tied to the concept
of equivariance (Kondor & Trivedi, 2018). This insight was exploited by Esteves et al. (2018) and
Cohen et al. (2018), where they define network layers equivariant to SO(3q rotations. This is accom-
plished by transforming the input using the spherical Fourier transform (SFT) and then performing
convolutions in the spectral domain. These networks have the appealing property of resting firmly
on the abstract algebraic underpinnings of convolution.
However, some spherical data comes equipped with a natural orientation. For instance, imagery
captured from a self-driving vehicle can be aligned with the gravity direction using information
from an inertial measurement unit (IMU). Brain scans and planetary data also come with a preferred
orientation, and it has been shown that employing convolutions that utilize this information (and thus
are not SO(3q equivariant), can lead to improved performance in classification and segmentation
tasks (Jiang et al., 2019). Instead, what is required in these applications is a convolution which
1
Under review as a conference paper at ICLR 2021
is equivariant to SOp2q transformations, i.e., azimuthal rotations, of the input data, but where the
requirement of full SOp3q equivariance may be unnecessarily restrictive.
In this paper, we follow the line of work that examines spherical convolutions in the spectral domain
(Kondor et al., 2018; Esteves et al., 2018; Cohen et al., 2018), and consider the problem of how to
design SOp2q equivariant convolutions. Specifically, we make the following contributions:
•	Our main contribution is theoretical: We perform a complete characterization of all possible
azimuthal-rotation equivariant linear operators T : L2(S2) → L2(S2) of band-limited
functions. We show that these can be parametrized naturally as block diagonal matrices of
complex numbers that act on the spherical harmonic expansion coefficients of their input.
•	In addition to the Fourier space characterization, we also show how these operations can
be interpreted as correlations between the input signal and filters in the spatial domain.
•	We demonstrate that the correlations are natural generalizations of the convolutions pro-
posed by Kondor et al. (2018) and Esteves et al. (2018), and how these, as well as those of
Jiang et al. (2019) can be formulated as special cases of our framework by parametrizing
the elements of the block-diagonal matrices corresponding to the operator T.
•	Through experiments on two datasets, we show that by employing azimuthal-rotation
equivariant correlations, we can recreate a state-of-the-art neural network architecture
(Jiang et al., 2019) in our framework, allowing it to benefit from improved equivariance.
The paper is organized as follows. Sec. 2 gives some preliminaries and Sec. 3 introduces the notion
of equivariance. Our main theoretical results on azimuthal equivariance and azimuthal correlations
are given in Sec. 4 and Sec. 5, respectively. Sec. 6 provides an in-depth summary of related work
and finally, we present experimental results and comparisons in Section 7.
2	Preliminaries
A point ω on the unit sphere S2 can be parametrized by spherical coordinates ωpθ, φq “
pcos φ sin θ, sin φsinθ, cos θq, where θ is the polar angle measured down from the z-axis and φ
the azimuthal angle measured from the x-axis (see Fig. 2). The angle θ varies between 0 and π,
while φ varies between 0 and 2π.
The Hilbert space L2pS2q consists of all functions f : S2 → C which are square integrable on the
unit sphere, and comes equipped with the inner product
xf, hy “
f fhdω “ 广 J
Js2	Jφ=0Jθ=0
fpθ, φqhpθ, φq sin θdθdφ.
The spherical harmonics area set of functions on the sphere that form an orthogonal basis ofL2pS2q.
This means that a function f P L2pS2q can be represented in the orthogonal basis via
8l
f(θ,φ) = ∑ ∑ fimYlmpθ,φq,
l = 0 m = -l
(1)
where Ylm is the spherical harmonic of degree l and order m, and the fim P C are the (spherical)
Fourier coefficients. Note that We always have ´l ≤ m ≤ l. Here,
Ym(θ,φq = (-Dm、尸` i(l´ Imq
4π(l ` mq!
Plm(cos θqeimφ,
(2)
where Plm is the associated Legendre function of degree l and order m. For the exact definition of
Legendre functions, see Driscoll & Healy (1994). For our purposes, it will be important to know
that for fixed m, θπ=0 Pkm(cos θqPlm (cos θq sin θdθ = 0 whenever k ‰ l. Similarly, as the spherical
harmonics form a complete orthogonal basis, we have xYlm, Ylm1 1y = δl,l1 δm,m1 . Given f, the
Fourier coefficients flm can be computed as flm = xf, Ylmy = S2 f Ylm dω.
3	Equivariance and Linear Operators
With the notation in place, we are now in a position to turn our attention towards equivariance.
We will start by looking at the group of translations over reals, which will be insightful for further
analysis. Then, we will turn to functions on S2 and the group of rotations SO(3q.
2
Under review as a conference paper at ICLR 2021
3.1	Translations
For f P L2 pRnq and t P Rn, the translation of f by t is the function Λtf pxq given by Λtfpxq “
f (x — tq. Clearly, ∣∣Λtf∣∣2 “ ||f ∣∣2 for all f P L2(Rn). So, Λt is a bounded and linear operator
fromL2pRnq to L2pRnq.
We say that a linear operator T is equivariant with respect to translations if, for all t P Rn , we have
ΛtT “ TΛt,	(3)
which means that the linear operator T commutes with translation Λt .
For h P L2(Rn), let Th be the operator performing convolution of f P L2(Rn) with h, defined by
the function
Thf(X) =(h * f)(χ)= f h(y)f(χ — y)dy, Vx P Rn.
Rn
This operator is equivariant with respect to translations. Indeed, for t P Rn,
(4)
ThΛtf(x) “
Rn
h(y )f (x — y — t)dy “ (h * f)(x — t) “ Thf(x — t) “ ΛtThf(x).
(5)
It turns out that Eq. (4) is the most general form of a translation equivariant operator for functions
on Rn. IfT is a translation equivariant operator, then there exists a function h such that Tf “ h * f
for all f, provided one is allowed to pick h from the (larger) function space that also includes
distributions. This is a classical result in functional analysis, see Hormander (1960) for a proof.
3.2	Rotations
Now, let us consider a real-valued function f defined on S2 . Let R P SO(3) be a rotation. Then
We define Λr as the operator that rotates the graph of the function f : S2 → R by R. Specifically,
ΛRf(ω) = f (R').
An operator T is equivariant With respect to rotations, if for all R P SO(3), We have
TΛR = ΛRT,
i.e., ifit commutes With the rotation operator.
The characterization of all rotation equivariant operators is Well-knoWn in the literature. In Dai &
Xu (2013), it is shoWn that a linear, bounded operator T : L2 (S2) → L2 (S2) is equivariant to
rotations if and only if there exists a sequence of numbers tμι}, l = 0,1,..., such that
(Tf )lm = μiflm
(6)
is fulfilled for all Fourier coefficients fim indexed by l and m of an arbitrary function f P L2 (S2).
Such linear operators T can also be interpretated as convolutions. In Driscoll & Healy (1994),
spherical convolution is defined as
(h * f )(ω) = [	h(Rη)f(RTω)dR.
RPSOp3q
(7)
4π
Ph * fqlm = 2πV (2∑'1)
Here, ω is any point on the sphere and η the north pole. The measure is dR = sin θ dθ dφ dψ in
terms of Euler angle coordinates of a rotation. It is also proven that the Fourier transform of the
convolution is given by
- 8
EhlOflm.	(8)
l“0
Note that only the coefficients With order m = 0 for h are present in the formula. Filters, Which only
have terms of order m = 0 in their Fourier expansion, are knoWn as zonal filters and consequently
they are constant for fixed θ When varying φ. Also note that the convolution fulfills (6), so it is
indeed a rotation equivariant operation. Conversely, it is also clear from (6) that every bounded
linear operator T can be associated With such a filter h.
3
Under review as a conference paper at ICLR 2021
4 Azimuthal-Rotation Equivariant Linear Operators
At last, we are ready to perform our analysis of azimuthal-rotation equivariant linear operators. Let
T : L2pS2q → L2 pS2q be a linear operator acting on functions on the unit sphere. What conditions
must be imposed on T in order for it to have equivariance to rotations around the z-axis? Formally
we should have, for all such azimuthal rotations Rφ P SOp3q, that TΛRφ “ ΛRφT.
Consider the action of T on an arbitrary function f P L2 (S2). Let fim be its expansion coefficients
according to (1). Applying T to this function gives, due to the linearity ofT,
8n
Tf(θ,φ) = ∑ ∑ fimTYιm(θ,φq.	(9)
l“0 m = -l
The action of T is thus fully determined by its action on the spherical harmonic basis functions. In
particular, it will be equivariant if and only if the spherical harmonic basis functions are equivariant.
Let glm (θ, φq = T Ylm(θ, φq be the action ofT on Ylm. Since glm P L2(S2q, we can expand it,
8 l1
gim(θ,φq = ∑ ∑ ^mYlm(θ,φq.	(10)
l1“0 m1 = —11
The requirement that T be equivariant then gives, for all azimuthal angles φ0,
T ［匕m(θ, φ ´ Φ0)S= glm(θ, φ ´ Φθ).	(11)
The left hand side becomes, after using the definition of the spherical harmonic Ylm in (2) and
moving the constant phase e´imφo outside T and using the definition of glm and its expansion (10):
8 l1
T ［匕m(θ,φ ´ Φ0)S= ∑ ∑ glm e´imφ0 YK (θ,φq.	(12)
l1=0m1=—l1
The right hand side of (11) is simply glm(θ, φ — φo) = ∑8=0 SmI=TI g'me´im1φoYlm(θ, φ).
So the azimuthal equivariance condition (11) requires that the above two expressions are equal, i.e.,
8	l1	8 l1
∑ ∑ 嬷 1 e´imφ0 YK (θ,φq= ∑ ∑ 媚 1 e´im1φ0 Ylm1 (θ,φq.	(13)
l1=0 m1=—l1	l1=0 m1=—l1
Now, two functions in L2(S2q are identical if and only if their coefficients in the basis are identical.
This gives us the constraint glmme´imφo = gllme´im1φo. Thus, for any non-zero coefficient g∣mm,
we must have that m = m1. The expansion (10) of glm must therefore have the form
8
gim(θ,φq= ∑ ^mYlm(θ,φq.	(14)
l1=|m|
The inner sum has vanished, since the terms are only non-zero for m = m1, and the sum starts
at l1 = |m| since no terms exist in (10) for l1 V |m|. The coefficients ^m thus parametrize the
azimuthal rotationally equivariant transform T. To summarize, we have proven the following.
Proposition 1. A linear, bounded operator T : L2 (S2q → L2 (S2q is equivariant to azimuthal
rotations ifand only if there exists a Sequence ofnumbers {μlm} such that
8
(Tf )lm = ∑ μ∣mfl1m	(15)
l1=|m|
isfulfilledfor all Fourier coefficients flm indexed by l,m ofan arbitrary function f P L2(S2).
Note that T maps a spherical harmonic Ylm to a linear combination of spherical harmonics with
the same order m. Thus, another way of stating the proposition is that a bounded linear operator
4
Under review as a conference paper at ICLR 2021
∕Γ2∕Γ1∕Γ1%∕p^∕l∕i形
Figure 1: Left: The matrix representation of a general linear SOp2q equivariant operator T acting on
the spherical harmonic expansion coefficients of a function f P L2 pS2q. Note that the coefficients
have been grouped by their order (superscript). Right: The matrix representation of an SOp3q
equivariant linear operator. The matrix elements shown in the same color are identical, since all
coefficients of the same degree are multiplied by the same number.
T : L2 (S2 q → L2pS2q is equivariant to azimuthal rotations if and only if the subspaces spanned by
the spherical harmonics of a fixed degree form invariant subspaces under the action of T.
This result has a natural interpretation if we restrict ourselves to spherical harmonics of some max-
imum degree L, i.e., when we are working with band-limited signals. This occurs commonly in
practice, e.g., with a signal obtained through an equiangular sampling on the sphere. If the expan-
sion coefficients are listed in a vector and grouped by m-values, then the matrix representation of T
will be a block diagonal matrix (cf. Fig. 1).
The SO(3q equivariant convolution as defined by Driscoll and Healy, cf. (7), with the transform
formula given in (8) is a special case of the above proposition where the matrix T is diagonal and
where the coefficients for all spherical harmonics of the same degree are identical, cf. Fig. 1.
5 AZIMUTHAL CONVOLUTIONS AND CORRELATIONS ON S2
Translation and rotation equivariant operations can be realized as convolutions, or equivalently, cor-
relations. Previously we characterized azimuthal-rotation equivariant linear operators in the Fourier
domain (Proposition 1). A natural question arises: Can we interpret these operators as convolutions
or correlations? Below, we show that these operators can be interpreted in terms ofa correlation.
Let h, f : S2 → R. Then, a first attempt at a definition of correlation is (h ‹ f q(φq “
∖ωps2 h(Rφ1ω~qf (ω)dω. However, such a correlation is only defined on S1, since it is a func-
tion of only the azimuthal angle φ. Instead, we extend the domain to S2 by considering a filter
parametrized by the polar angle θ, i.e., h “ hθ . So, by varying θ, we get a different filter and
consequently a different correlation response.
Definition 1. Let hθ , f : S2 → R. Then, we define the azimuthal correlation as
(hθ ‹ f q(θ, φq “
Let fim and hRm be the Fourier expansion coefficients of f and hθ, Further, We will expand each
coefficient hm in the associated Legendre basis,
8
hθm “ ∑ hklmPm (cos θ).	(16)
k“|m|
Now we can show that every azimuthal-rotation equivariant linear operator can be expressed as a
correlation. See Fig. 2 for an illustration.
Proposition 2. For functions hθ , f in L2(S2q, the transform of the correlation is given by
8------------ ∞
phθ < f qlm =(T)m](2；` ?)； ´ m ∑ Mllmflm
l1“|m|
(17)
where f and hθ are expanded in Fourier and Legendre series according to (16).
5
Under review as a conference paper at ICLR 2021
φ1	φ2
(a)	(b)
Figure 2: Correlation interpretation of an SOp2q equivariant transformation. To the left, four points
are shown, positioned at two azimuths φ1 and φ2, and polar angles θ1 and θ2 . To compute the
response at a point pθ, φq, the polar-dependent filter hθ is rotated around the z-axis by φ, as shown
in (b), before being multiplied with the input signal f and integrated over the sphere. This is similar
to regular correlation in R2, except that changing θ does not translate the filter, but rather morphs
the filter into a new one. The operator shown is the partial derivative B{Bφ.
The proof is straight-forward by substituting the expansions and simplifying, see Appendix.
6	Comparison of Our Results With the Literature
Our work is closely related to, and inspired by, recent theoretical developments on spherical CNNs
and SOp3q equivariance, including Cohen et al. (2018); Esteves et al. (2018); Kondor & Trivedi
(2018); Defferrard et al. (2019). See also Esteves (2020) for a more comprehensive review. However,
these works do not consider the case of purely azimuthal equivariance, and the characterization we
present in Proposition 1 is, to our knowledge, new. Our second result, Proposition 2 which shows
that azimuthal equivariant operators can be expressed as correlations is by no means a surprise, but
it does not follow from, for instance, the general framework of Kondor & Trivedi (2018).
Another closely related work is that of Jiang et al. (2019). They present an efficient and powerful
framework for analyzing spherical images with state-of-the-art performance. It is based on learn-
ing filters that are linear combinations differential operators, or more specifically,磊,Bφ and the
LaPlacian V2. The resulting filters are not SO(3) equivariant, but they are indeed linear and az-
imuthal equivariant operators (and thus azimuthal correlations, cf. Proposition 2) and consequently
a special case of our framework. However, the discretization of the sphere into an icosahedral mesh
at lower resolutions breaks the equivariance in Jiang et al. (2019). This hurts the generalization
performance as confirmed by our experiments. This limitation can be alleviated by applying fully
azimuthal-rotation equivariant operations.
7	Experiments
In this section we implement neural networks based on the presented framework. First, we verify
that a correlation layer L2 (S2) → L2 (S2) designed according to Proposition 1 is indeed equiv-
ariant to azimuthal rotations. Then, as in Cohen et al. (2018); Jiang et al. (2019), we perform
experiments on the Omni-MNIST and ModelNet40 (Wu et al., 2015) datasets, where the task is to
perform classification of spherical images and 3D shape models, respectively. We demonstrate that
the state-of-the-art architecture of Jiang et al. (2019) can be recreated in our framework, and that
the classification results of both approaches on unrotated data are virtually identical, but with our
approach showing increased generalization performance to unseen SO(2) rotations on the test set of
ModelNet40.
6
Under review as a conference paper at ICLR 2021
(a)	(b)
Figure 3: (a) The equivariance error ∆ as a function of the number of consecutive correlation and
ReLU layers in the network Φ. (b) Accuracy during training of the network with general block-
diagonal correlation filters. The accuracy is plotted both for the original test set, as well as for a
rotated version of the test set, where each image has been randomly rotated about the z-axis.
7.1	Equivariance error
To evaluate the equivariance error in our network layers, we follow the approach of Co-
hen et al. (2018). We create prototype networks Φ by composing randomly initialized
differential type correlation layers and ReLU. We sample n “ 500 azimuthal rotations
Ri and n signals fi with 12 channels each. Next we compute the equivariance error
∆ “ 1 Xn=I std(ΛRiΦ(fi) ´ Φ(ΛRifi))/std(Φ(fi)) which should be Zero for a perfectly equiv-
ariant network. The obtained equivariance errors are presented in Fig. 3a. The low order of the error
and that the error does not increase much with the number of layers indicates that the correlations
are aZimuthally equivariant.
7.2	Digit classification on Omni-MNIST
The Omni-MNIST dataset is obtained by projecting the images of the MNIST handwritten digit
dataset onto the surface of a sphere. The resulting dataset consists of 60,000 spherical images for
training, and 10,000 for testing. As in Jiang et al. (2019), the images are projected onto the north
pole, and then rotated to the equator.
We run the method of Jiang et al. (2019) as a reference, using their implementation, and also recreate
their architecture in our SFT-based framework. The network consists of a stack of residual blocks.
Each block contains 1 X 1 convolutions that mix feature maps, as well as spherical convolutional
layers that process each feature map in parallel by forming a linear combination of the image, its
horiZontal and vertical derivatives, and its Laplacian. Note that since all of these operations are
SO(2q equivariant, they can each be represented by a block-diagonal matrix acting on the spherical
harmonic expansion coefficients of the feature maps. Batch-normaliZation and ReLU are also per-
formed within the residual blocks. Please refer to the paper of Jiang et al. (2019) for more details
about the network architecture.
Downsampling is performed in the spectral domain, where we simply reduce the maximum degree
L of included SFT coefficients by a factor of two, rounding up.
Both our implementation and the original implementation by Jiang et al. (2019) are trained with
stochastic gradient descent, using the Adam optimiZer and a batch siZe of 16. The initial learning
rate is 0.01, and is decreased by a factor of two every ten epochs. For each epoch during training, we
compute the accuracy on the test set, using both the original version, and a randomly aZimuthally
rotated version of the test set. The results are shown in Fig. 4a. The training is also run on an
unrotated and a randomly aZimuthally rotated version of the test set.
7
Under review as a conference paper at ICLR 2021
Comparison of Jiang's PDO implementation
our SFT-based implementation
99∙99∙
100.0
100
(s) >ME3MMS ⅛WH
---- Ours, training set rotated, test set rotated
— Ours, training set non-rotated, test set rotated
---- Oursl training set πoπ-rotated, test set πoπ-ratated
---- Jiang, training set rotated, test set rotated
---- Jiang, trainig set non-rotated, test set rotated
---- Jiang, training set πoπ-ratated, test set πoπ-ratated
Comparison of j∣angls PDO implementation
and our SFT-based implementation
(*) Aue.Jnuue≡91
20
40	60
Epoch
80
ιoo
5 0 5 0
9999

o
(a) Omni-MNIST	(b) Modelnet40
Figure 4: Accuracy during training of the discretized sphere implementation of Jiang et al. (2019)
and our SFT-based implementation. The accuracy is plotted both for the original test set, as well as
for a rotated version of the test set, where each image has been randomly rotated about the z-axis.
7.3 3D shape classification on ModelNet40
ModelNet40 (Wu et al., 2015) is a dataset containing CAD models of 40 different object classes.
We run our experiments on the aligned version of the dataset provided by Sedaghat et al. (2017)
and follow Cohen et al. (2018) by creating spherical images by ray casting the CAD models onto an
enclosing sphere. Each model is represented by six channels - the ray length, the sine of the surface
angle and the cosine of the surface angle, as well as the same three properties obtained from ray
casting the convex hull of the model.
We compare to the implementation by Jiang et al. (2019). We recreate their network architecture (c.f.
Sec. 7.2) and train using stochastic gradient descent with Adam and a batch size of 16. The learning
rate is set to 5 ∙ lθ´3 and decreased by a factor of 0.7 every 25 epochs. Again We evaluate and
train the model both on the aligned ModelNet40 dataset as well as a version of the dataset with each
CAD model rotated by a random azimuthal rotation. We train for 100 epochs and shoW the results
in Fig. 4b. Of special note is the case With a non-rotated training set and a rotated test set, Where We
see that our netWork manages to generalize better than the one by Jiang et al. (2019). The reason
for the loW degree of generalization from non-rotated to rotated data in the baseline model might
be the hierarchical nature of the icosahedral grid and doWn-sampling used by Jiang et al. (2019),
Where a grid point on the sphere Will influence the output more if it belongs to a loWer level in the
hierarchy. If an important point in the input is rotated to a grid point in a higher hierarchy level, the
output might change drastically. This problem is not present in our design since We doWnsample in
the frequency domain and therefore the information from every grid point is treated equally.
Additionally, We created another netWork using the same architecture as before, but With general
block-diagonal correlation filters (instead of learning a linear combination of differential filters).
The performance results of this netWork are shoWn in Fig. 3b. The accuracy of the general netWork
is comparable to the accuracy of the netWorks With differential feature maps in Fig. 4b, but it should
be noted that the number of trainable parameters is much larger.
8 Conclusions
In this paper, We have examined hoW to design convolutions Which operate on spherical data and
are equivariant to SOp2q rotations. Specifically, We have performed a complete characterization of
bounded, linear operators from L2(S2) → L2(S2) which exhibit SO(2) equivariance. We showed
that, for band-limited signals, these can be realized as block-diagonal matrices in the spectral do-
main, and also demonstrated how these operators may be interpreted as correlations in the spatial
domain. Using this framework, we implemented an existing state-of-the-art pipeline, which in our
framework showed better generalization performance to SOp2q rotations not seen during training.
8
Under review as a conference paper at ICLR 2021
References
Taco S Cohen, Mario Geiger, Jonas Kohler, and Max Welling. Spherical CNNs. In Int Conf. on
Learning Representations, 2018.
Benjamin Coors, Alexandru Paul Condurache, and Andreas Geiger. SphereNet: Learning spherical
representations for detection and classification in omnidirectional images. In European Conf. on
Computer Vision, 2018.
Feng Dai and Yuan Xu. Approximation Theory and Harmonic Analysis on Spheres and Balls.
Springer, 2013.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
2016.
Michael Defferrard, Martino Milani, Frederick Gusset, and Nathanael Perraudin. DeepSphere: a
graph-based spherical CNN. In Int. Conf. on Learning Representations, 2019.
James R. Driscoll and Dennis M. Healy. Computing fourier transforms and convolutions on the
2-sphere. Advances in Applied Mathematics, 15:202-250, 1994.
Marc Eder, Mykhailo Shvets, John Lim, and Jan-Michael Frahm. Tangent images for mitigating
spherical distortion. In Conf. Computer Vision and Pattern Recognition, 2020.
Carlos Esteves. Theoretical aspects of group equivariant neural networks. arXiv preprint
arXiv:2004.05154, 2020.
Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning
SO(3) equivariant representations with spherical CNNs. In European Conf. on Computer Vision,
2018.
Lars Hormander. Estimates for translation invariant operators in Lp spaces. Acta Math., 104(1-2):
93-140, 1960.
Chiyu Jiang, Jingwei Huang, Karthik Kashinath, Philip Marcus, Matthias Niessner, et al. Spherical
CNNs on unstructured grids. In Int. Conf. on Learning Representations, 2019.
Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In Int. Conf. on Machine Learning, 2018.
Risi Kondor, Zhen Lin, and Shubhendu Trivedi. Clebsch-Gordan nets: A fully fourier space spher-
ical convolutional neural network. In Advances in Neural Information Processing Systems, 2018.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Hidenobu Matsuki, Lukas von Stumberg, Vladyslav Usenko, Jorg Stuckler, and Daniel Cremers.
Omnidirectional dso: Direct sparse odometry with fisheye cameras. IEEE Robotics and Automa-
tion Letters, 3(4):3693-3700, 2018.
Mayur Mudigonda, Sookyung Kim, Ankur Mahesh, Samira Kahou, Karthik Kashinath, Dean
Williams, Vincen Michalski, Travis O’Brien, and Mr Prabhat. Segmenting and tracking extreme
climate events using neural networks. In Deep Learning for Physical Sciences (DLPS) Workshop,
held with NIPS Conference, 2017.
Nima Sedaghat, Mohammadreza Zolfaghari, Ehsan Amiri, and Thomas Brox. Orientation-boosted
voxel nets for 3D object recognition. In British Machine Vision Conf., 2017.
Yu-Chuan Su and Kristen Grauman. Kernel transformer networks for compact spherical convolu-
tion. In Conf. Computer Vision and Pattern Recognition, 2019.
Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Linguang Zhang, Xiaoou Tang, and Jianxiong
Xiao. 3D ShapeNets: A deep representation for volumetric shapes. In Conf. Computer Vision and
Pattern Recognition, 2015.
9
Under review as a conference paper at ICLR 2021
Appendix A: Proof of proposition 2
Proof. From the definition of correlation we obtain
π 2π
(hθ < f )(θ,φ) = J	/θ (RJω)f(ω)dω =I I	hθ (θ1 ,φ1)f(θ1,φ + φ1) Sin θ'dθ'dφ'.
ωP	“一	(18)
Substituting in the Fourier expansions of f and h, and employing the orthogonality of the spherical
harmonics, we get
8	l1
(hθ < f)(θ,φ)= ∑ ∑ hθ，m，fl'm'eim'φ.	(19)
〃 “0 m'“-l'
Now, employing the expansion (16) of Mm into the basis of the associated Legendre functions, We
find
8	l1	8
(hθ < f)(θ,φ)= £ £	£ hki'm'Pm'(cosθ)fi'm,eim'φ∙	(20)
l1“0 m1“-l1 k“|m1 |
By reindexing this last sum, we can instead write it in the form
8l 8
(hθ < f)(θ,Φ) “ £ £	£ hιi'mfi'mPm(cosθ)eimφ =
l“0 m“-l l1“|m|
“£ £	(Tqm{(214：(1)+”;q! £ hll'mfl'm	YFHM
i“0 m“-i	i1“|m|
(21)
(22)
where, in the last equality, we have used the definition (2) of the spherical harmonics. Note that
the last expression is a spherical harmonic expansion, and the expression in the brackets is the
corresponding expansion coefficient, given by
m m / 1 λm I 4n(l + mq!	8 }	£
(hθ < fqlm = (T)弋(21 + 1)(1 — m)! 〃三 hll'mfl
(23)
This proves the proposition.
□
10