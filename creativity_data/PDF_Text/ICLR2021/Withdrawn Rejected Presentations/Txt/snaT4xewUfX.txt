Under review as a conference paper at ICLR 2021
Variational Inference for Diffusion modu-
lated Cox Processes
Anonymous authors
Paper under double-blind review
Ab stract
This paper proposes a stochastic variational inference (SVI) method for comput-
ing an approximate posterior path measure of a Cox process. These processes are
widely used in natural and physical sciences, engineering and operations research,
and represent a non-trivial model of a wide array of phenomena. In our work, we
model the stochastic intensity as the solution of a diffusion stochastic differential
equation (SDE), and our objective is to infer the posterior, or smoothing, mea-
sure over the paths given Poisson process realizations. We first derive a system of
stochastic partial differential equations (SPDE) for the pathwise smoothing pos-
terior density function, a non-trivial result, since the standard solution of SPDEs
typically involves an Ito stochastic integral, which is not defined pathwise. Next,
we propose an SVI approach to approximating the solution of the system. We
parametrize the class of approximate smoothing posteriors using a neural net-
work, derive a lower bound on the evidence of the observed point process sample-
path, and optimize the lower bound using stochastic gradient descent (SGD). We
demonstrate the efficacy of our method on both synthetic and real-world prob-
lems, and demonstrate the advantage of the neural network solution over standard
numerical solvers.
1	Introduction
Cox processes (Cox, 1955; Cox & Isham, 1980), also known as doubly-stochastic Poisson processes,
are a class of stochastic point processes wherein the point intensity is itself stochastic and, condi-
tional on a realization of the intensity process, the number of points in any subset of space is Poisson
distributed. These processes are widely used in the natural and physical sciences, engineering and
operations research, and form useful models of a wide array of phenomena.
We model the intensity by a diffusion process that is the solution of a stochastic differential equa-
tion (SDE). This is a standard assumption across a range of applications (Susemihl et al., 2011;
Kutschireiter et al., 2020). The measure induced by the solution of the SDE serves as a prior mea-
sure over sample paths, and our objective is to infer a posterior measure over the paths of the under-
lying intensity process, given realizations of the Poisson point process observations over a fixed time
horizon. This type of inference problem has been studied in the Bayesian filtering literature (Schup-
pen, 1977; Bain & Crisan, 2008; Sarkka, 2013), where it is of particular interest to infer the state of
the intensity process at any past time given all count observations till the present time instant (the
resulting posterior is called the smoothing posterior measure).
In a seminal paper, Snyder (1972) derived a stochastic partial differential equation (SPDE) describ-
ing the dynamics of the corresponding posterior density for Cox processes. The solution of this
smoothing SPDE requires the computation of an Ito stochastic integral with respect to the counting
process. It has long been recognized (Clark, 1978; Davis, 1981; 1982) that for stochastic smoothing
(and filtering) theory to be useful in practice, it should be possible to compute smoothing posteriors
conditioned on a single observed sample path. However, Ito integrals are not defined pathwise and
deriving a pathwise smoothing density is remarkably hard. 30 years after Synder’s original work El-
liott & Malcolm (2005) derived a pathwise smoothing SPDE in the form of a coupled system of
forward and backward pathwise SPDEs. Nonetheless, solving the system of pathwise SPDEs, or
sampling from the corresponding SDE, is still challenging and intractable in general. It is well
known, for example, that numerical techniques for solving these SPDEs, such as the finite element
1
Under review as a conference paper at ICLR 2021
method (FEM), suffers from the curse of dimensionality (Han et al., 2018). Therefore, it is of con-
siderable interest to find more efficient methods to solve the smoothing SPDE.
We take a variational inference approach to computing an approximate smoothing posterior measure.
Variational representations of Bayesian posteriors in stochastic filtering and smoothing theory have
been developed in considerable generality; see (Mitter & Newton, 2003) for a rigorous treatment.
There are a number of papers that consider the computation of an approximate posterior distribution
over the paths of the underlying intensity process that is observed with additive Gaussian noise (Ar-
chambeau et al., 2007; 2008; Cseke et al., 2013; Susemihl et al., 2011; Sutter et al., 2016). Susemihl
et al. (2011) studied Bayesian filtering of Gaussian processes by deriving a differential equation
characterizing the evolution of the mean-square error (MSE) in estimating the underlying Gaussian
process. On the other hand, Sutter et al. (2016) compute a variational approximation to the smooth-
ing posterior density when the underlying diffusion intensity is observed with additive Brownian
noise. They choose their variational family to be a class of SDEs with an analytically computable
marginal density. This setting is considerably different from our setting, where the observed process
is a point process. Nonetheless, Sutter et al. (2016) provides methodological motivation for our cur-
rent study. In the context of the computation of approximate smoothing/filtering posteriors for point
process observations, Harel et al. (2015) developed an analytically tractable approximation to the
filtering posterior distribution of a diffusion modulated marked point processes under specific mod-
eling assumptions suited for a neural encoding/decoding problem. In general, however, analytical
tractability cannot be assured without restrictive assumptions.
We present a stochastic variational inference (SVI) (Hoffman et al., 2013) method for computing
a variational approximation to the smoothing posterior density. Our approach fixes an approxi-
mating family of path measures to those induced by a class of parametrized SPDEs. In particular,
we parametrize the drift function of the approximating SPDEs by a neural network with input and
output variables matching the theoretical smoothing SPDE. Thereafter, using standard stochastic
analysis tools we compute a tractable lower bound to the evidence of observing a sample path of
count observations, the so-called evidence lower bound (ELBO). A sample average approximation
(SAA) to the ELBO is further computed by simulating sample paths from the stochastic differen-
tial equation (SDE) corresponding to the approximating SPDE. Finally, by maximizing the ELBO,
the neural network is trained using stochastic gradient descent (SGD) utilizing multiple batches of
sample paths of count observations. Note that each sample path of the count observations entails
the simulation of a separate SDE. We note that there are many problems in the natural and physical
sciences, engineering and operations research where multiple paths of a point process (over a finite
time horizon) may be obtained. For instance, we present an example in Section 5 modeling the
demand for bikes rented during a 24 hour, one day time period in a bike-sharing platform, where the
underlying driving intensity is subject to stochastic variations, and demand information is collected
over multiple days.
In contrast to the variational algorithm developed in Sutter et al. (2016), where the variational lower
bound must be re-optimized for new sample paths of the observation process, our variational method
is more general and our approximation to the smoothing posterior can be used as a map for another
(unobserved) sample path of count observations. Our computational approach can also be straight-
forwardly adapted to solve the problem of interest in Sutter et al. (2016).
In the subsequent sections, we describe our problem and method in detail and demonstrate the util-
ity of our method with the help of numerical experiments. In particular, we show how the choice
of approximating family enables us to use the trained neural network and in turn, the variational
Bayesian smoothing posterior (VBSP), to compute smoothing SPDE in almost (3/4)th of the com-
putational time required to compute the original smoothing SPDE using FEM. Moreover, we also
efficiently generate Monte Carlo samples from the learned VBSP and use them for inference on the
bike-sharing dataset, whereas FEM failed to compute either VBSP or the true smoothing density for
the given time-space discretization.
2	Problem Description
Let Nt be a Cox process with unknown stochastic intensity {zt ∈ R+ , t ∈ [0, T]}. We use Nt0,t to
denote a sample path realization of Nt restricted to the interval [t0, t], and use Nt to denote Nt -N0;
recall that N0 = 0 by definition. As noted before, a Cox process conditioned on the intensity is a
2
Under review as a conference paper at ICLR 2021
Poisson process. Therefore, given a realized sample path {zt , t ∈ [0, T]} of the intensity, and for
any 0 ≤ t0 < t ≤ T, the marginal likelihood of observing Nt - Nt0 ∈ N counts in (t0, t] is
t	Nt-Nt0 e- Rtt0 zsds
Nt- Nt，〜L(Nt- N, = Nt- N∣{zs}to<s≤t) := I Zsds	^5,	⑴
t0	(Nt - Nt0)!
where L denotes the Poisson likelihood. Rather than directly modeling the intensity z, we will
bring a little more flexibility to our setting, and assume that zt is a deterministic transformation of
an another stochastic process xt through a known mapping h : Rd 7→ R+: is zt = h(xt). Note that
the non-negative range of h ensures that the Poisson intensity zt = h(xt) is non-negative. Unless
xt ∈ R+, the mapping h cannot be an identity function. We use the term intensity process to refer
to either zt or xt.
We model the intensity process {xt ∈ Rd, ∀t ∈ [0, T]} with the following SDE,
dxt = b(xt)dt + σ(xt)dBt, ∀t ≤ T and x0 = 0,
(2)
where b : Rd → Rd is the drift function, σ(∙) : Rd → Rd×d is the diffusion coefficient, and Bt is the
d-dimensional Brownian motion (or Wiener process). We assume that there exists a strong solution
to the SDE above (OkSendal, 2013, Chapter 5). Moreover, we assume that b(∙), h(∙), and σ(∙) are
fixed by the modeler apriori, and we are interested in inferring the unknown intensity process with
their fixed definitions. Incorporating them will obscure our main contribution, and we leave it for
future work.
The model of the count observations above forms a diffusion modulated Cox process. Diffusion
modulated Cox processes are widely used to model the arrival process in various service systems
such as call centers, hospitals, airports etc. (Zhang et al., 2014; Wang et al., 2020). Zhang & Kou
(2010) use a Gaussian process modulated Cox-process to infer proteins’ conformation, in particular,
they model the arrival rates of the photons collected from a laser excited protein molecule as a
Gaussian process. Schnoerr et al. (2016) model spatio-temporal stochastic systems from systems
biology and epidemiology using Cox process where intensity is modelled with diffusions.
As stated in the introduction, we seek to infer the smoothing posterior measure over the un-
known intensity process {xt, t ∈ [0, T]} using the count observations upto time T . Follow-
ing terminology from the Bayesian filtering theory (Sarkka, 2013), We use smoothing to refer to
inferring the unobserved intensity process at any past time given the observations upto the cur-
rent time. Mathematically, the smoothing posterior is defined using the conditional expectation
of the form E[f(xt)|s(Nu, u ∈ [0, T])], where s(Nu, u ∈ [0, T]) is the smallest sigma alge-
bra (or filtration) generated by the Cox process {Nt} from time 0 to T . For brevity we write
E[f(xt)|s(Nu, u ∈ [0, T])] as E[f (xt)|N0,T]. Interested readers may refer to Kutschireiter et al.
(2020) for more details on non-linear filtering theory.
We now provide a formal derivation of the smoothing posterior using Bayes’ theorem (Bain &
Crisan (2008); Elliott & Malcolm (2005)). Observe the conditional expectation satisfies
E[f(xt)|N0,T]
Et[Λ°,τ f(xt)∣No,τ ]
E*[Λo,t ∣No,τ ]
(3)
for any measurable function f (∙) and Λs,t := LNs：))for any 0 ≤ s < t ≤ T, where Lt is the
unit intensity Poisson likelihood and EtH denotes the expectation with respect to LL Note that Lt
does not depend on the stochastic intensity process x and forms a reference measure. The marginal
smoothing posterior density is defined as
pt(x|N0,T) := P(xt ∈ dx|N0,T),
(4)
which can be formally obtained from equation 3 by setting f(xt) = I{A} (xt) for any A ∈ Rd,
where I{A} (y) is an indicator function that equals 1 when y ∈ A, otherwise 0. Now, define the
unnormalized filtering density function qt(x) as the function satisfying
P(xt ∈ dx|N0,t)
q^t(x)dx
RRd qt(ξ)dξ
(5)
3
Under review as a conference paper at ICLR 2021
and also define Vt(X)= E* [Λt,τ∣No,τ]. Then, it can be shown (Elliott & Malcolm (2005)) that for
any measurable function f,
叫Λo,τ f (xt)∣No,τ ] = JRdf (ξ)qt(ξ)Vt(ξ)dξ	(6)
EtA,T 1N0,T]	RRd qt⑹Vt⑹dξ
Next, recalling that h(∙) is the mapping to ensure the intensity process is positive, define the function
Ψt for a given sample path of count observations (i.e., pathwise) as
Ψt := Ψ(h(x), t, Nt) = exp [(1 - h(x))t + Nt log h(x)] , ∀x ∈ Rd.
Following Elliott & Malcolm (2005, Theorem 4) one may use Ψt to derive a coupled system of
pathwise SPDES that characterize qt(x) and Vt(x). In particular, they show that qt = Ψ-1qt is a
solution to the following SPDE
∂tqt(x) = Ψ-1L"Ψtqt(x)],∀t ≤ T,qo(x) = δχo(x),	⑺
where L is the adjoint of L[F(x)] = 2 £切 ai,j(x)∂χiχj F(x) + Pi bi(x)∂χiF(x), which is the
infinitesimal generator of the prior process for any twice-differentiable, continuous, and bounded
function F : Rd 7→ R and a(x) = σ(x)σ(x)T, and δx0 (x) is the Dirac delta distribution at x0.
Moreover, they also show that vt(x) = ΨtVt(χ) satisfies the following backward parabolic equation
∂tVt(x) = -ΨtL[Ψt-1Vt(x)],
(8)
with terminal condition VT (x) = ΨT (x).
Now it follows from equation 6 that using the solution of these two SPDEs, the marginal smoothing
posterior density for any t ∈ [0, T] satisfies
N	qt(ξ)vt(ξ)dξ	/9
Pt(XIN0,T) = ~7∙ 7TT(C\iC .	(9)
Rd qt (ξ)Vt (ξ)dξ
Using the SPDEs in equation 7, and 8, together with 9, it can be shown that the marginal smoothing
posterior densitypt(XIN0,T) satisfies its own SPDE: for any t ∈ [0, T],
∂tpt(x∣No,τ) = - X ∂χi h {(a(x)[Vlog(Ψ-1vt(x))])i + bi(x)}Pt(x∣No,τ)]
i
+ 2 X Mx)ai,j (X)Pt(XIN0,T)	(IO)
i,j
and Pt(XIN0,T) = δx0 (X) with X0 = 0. We present a detailed derivation in Appendix A.1. Corre-
sponding to this SPDE, there exists a smoothing posterior SDE, defined as
dxt = {a(Xt)[Vlog(Ψ-1Vt(Xt))]+ b(Xt)} dt + σ(Xt)dBBt and X。= 0,	(11)
where {Xt} is a modification of the process {xt} such that BBt is independent of the Cox process
Nt (and thus Bt).
Observe that the entire sample path of the count observations N0,T is summarized through the path-
wise function Ψt and Vt together in the drift term of this SDE. Also note that the diffusion coefficient
of the smoothing posterior SDE is precisely the same as that of the prior SDE. The computation of
the drift term in the smoothing posterior SDE requires the solving equation 8 for Vt (X) which, in
turn, makes the posterior computation challenging and computationally intractable in general. Con-
sequently, the computation of the marginal posterior density (and hence the path measure). There-
fore, we propose a variational inference-based method to compute an approximation to the solution
of the smoothing posterior SPDE, by computing an approximate solution to the smoothing posterior
SDE in equation 11.
3	Variational Bayes for approximating the smoothing density
Observe that the posterior path measure is the minimizer of the following variational optimization
problem (Mitter & Newton (2003, Proposition 2.1)),
max
Π(x0,T)∈P(C)
KL(ΠkΠ0) +
dΠ(x0,T) log L(N0,T Ih(x0,T))
(12)
4
Under review as a conference paper at ICLR 2021
where P(C) is the space of all absolutely continuous measures with respect to Π0, the measure
induced by a solution of the intensity SDE (equation 2) on the space C [0, T] of continuous func-
tions with support [0, T], and KL denotes the Kullback-Leibler divergence between two absolutely
continuous measures.Note that P(C) also contains Π0.
Solving this optimization problem over all measures in P(C) is intractable. Therefore, we choose
the subset of absolutely continuous measures Qb ⊂ P (C) induced by solutions of the following
SDE:
dxt =b(xt, Nt,t)dt + σ(xt)dBt, for t ≤ T and x0 = 0,	(13)
where b(∙, ∙, ∙) : Rd X N X [0, T] → Rd is the drift function. We term this space of measures Qb
as the Variationalfamily. The measures in Qb are absolutely continuous with respect to ∏o as they
have the same diffusion coefficient, therefore Qb ⊂ P (C). This choice of the variational family is
not arbitrary, but rather motivated by the smoothing posterior SDE derived in equation 11, where
the diffusion coefficient is σ(∙) and the drift coefficient has an intractable form (that depends on the
prior drift b(∙) and diffusion coefficient σ(∙), and Nt through Ψt and vt(∙)). Notice that the choice
of drift function spans the space of measures in the variational family Qb
Since, Qb ⊂ P (C), it follows from equation 12 that
max
Π(x0,T)∈P(C)
-KL(ΠkΠ0) +
dΠ(x0,T) log L(N0,T |h(x0,T))
≥ max
—Q∈Qb
-KL(QkΠ0) +
dQ(x0,T) log L(N0,T |h(x0,T))
(14)
The right hand side above is known as the evidence lower bound (ELBO). The corresponding ELBO
maximization problem to compute the optimal Q ∈ Qb (for a given sample path No,τ) is simply
Q*(∙∣No,τ) = arg maxEQ {log L(No,t∣xo,τ)} — KL(Qkno)∙	(15)
Q∈Qb
Note that absolutely continuous measures on path space correspond to changes in the drift function,
for a fixed diffusion coefficient (else, the measures are singular). As a consequence of Girsanov’s
theorem (Oksendal, 2013, Theorem 8.6.8) (see Appendix A.2 for the proof) we have,
KL(QkΠ0)
1T
2EQ /
∣∣σ-1(xt)(b(xt) — b(xt,Nt,t))k2dt
(16)
where recall b(∙, ∙) is the drift of the prior SDE defined in equation 2 and b(∙, ∙, ∙) is the drift of the
variational SDE. Substituting this into equation 15 yields
Q*(∙∣No,τ) = arg max EQ 卜g L(No,t ∣xo,τ) — 1 Z ∣∣σ-1 (xt)[b(xt,t) — b(xt ,Nt,t)]∣∣2dt}.
b	(17)
We denote Q*(∙∣No,τ) as the variational Bayesian smoothing posterior (VBSP) path measure .
Next, we lay down the details of the SVI algorithm to solve the above optimization problem to
compute the VBSP.
4	Stochastic Variational Inference of the VBSP
It is evident from the ELBO in equation 17 and the definition of the variational family Qb that com-
puting the VBSP measure entails the computation of the unknown drift function b(∙, ∙, ∙) in equa-
tion 13. We further restrict the family of measures Qb by assuming the drift functions belong to
a class of parametrized, smooth functions. A feasible way to model this class of drift functions is
through a neural network. However, it is possible to use simpler approximation function classes
as done in, for example, Sutter et al. (2016), who fixed b to ensure that the marginal distributions
of the variational smoothing SDE belong to a specific exponential family of distributions. We note
5
Under review as a conference paper at ICLR 2021
that in choosing a parametrized class, we must still ensure that the resulting drift functions are Lip-
schitz continuous and satisfy sufficient regularity so that a solution to the SDE equation 13 exists.
Furthermore, restricting the drift functions in this way entails a further restriction of the class of (ap-
proximating) path measures. An open question is here is how much of a loss this entails (in terms
of the Kullback-Leibler divergence from the ‘true’ posterior path measure in this instance).
To fix the idea, We assume that b(∙, ∙, ∙) lies in a general class of functions parametrized by θ.
Henceforth, we write the drift coefficient as b(∙, ∙, ∙, θ) to make its dependence on θ explicit. We
use stochastic gradient descent (SGD) to maximize the ELBO, Which requires the computation of
stochastic gradients of the ELBO with respect to θ. To compute the gradients, we generate sample
paths of xθ, the solution of the variational SDE equation 13 for a given θ, using a first-order Euler-
Maruyama integration of the SDE. We do this for convenience, though higher order approximations
could be used. Specifically, we partition the time interval [0, T] in K equal sub-intervals of length
∆ = T/K, {t0, t1, . . . tK}, where t0 = 0 and tK = T and then generate the sequence of {xtθ }iK=1
using the following recursive equation and initial condition xtθ = 0:
XIti= xθi-ι +b(xθi-ι ,Nti-1 ,ti-i,θ)∆ + ∆σ(χθ-i)Zi, ∀ i ∈ {1, 2,...K}	(18)
where {Zi}iK=1 is the sequence of K independent and identically distributed (i.i.d.) d-dimensional
standard Gaussian random vectors.
We generate M independent sample paths of the discrete-time process in equation 18 denoted as
{xIθi,m}, for m ∈ {1, 2, . . . M}, to compute a sample average approximation (SAA) of the ELBO
over the partition {t0, t1, . . . tK} as
E\LBO
1M
-1X
M乙
m=1
K-1
X
i=0
logL(Nti,ti+ι ∣h(xθi,m)∆) - 1 kσ(xθi,m)-1[b(xθim,ti) - b(xθim,Nti,ti,θ)]k2∆
1M
-1X
M乙
m=1
K-1
X
i=0
Nti,ti+ι log(h(xθim)∆) - h(xθD∆ - 1 kσ(xθi,m)-1[b(xθi,m,ti)- b(xθ,m,Nti,ti,θ)]k2∆
(19)
+ C,
where we used the definition of the Poisson likelihood L from equation 1 and C is a constant
independent of θ. Now, to compute gradients of ELBO with respect to θ, observe that the gradient
operator can be exchanged with the expectation, since the only source of randomness in each sample
path of xIθ,m are K i.i.d. Gaussian random vectors {Zim}iK=1, which are independent of θ. Notice
too that this is a pathwise analog of the reparametrization trick, and has been used recently to learn
deep latent models (Tzen & Raginsky, 2019; Li et al., 2020). Also, note that thus far the ELBO is
defined for a single sample path of the count observations N0,T . However, as noted before, in our
method we will also take a sample average of ELBO over multiple batches of sample paths of count
observations at each epoch of the training algorithm.
5	Numerical Experiments
We present three experiments demonstrating the efficacy and utility of our proposed SVI method.
First, we consider a setting when the underlying stochastic intensity process is 1-dimensional. We
compare the SVI approximation with the ‘true’ smoothing posterior density computed using the
solution of the forward and backward SPDEs, defined in equation 7 and equation 8. We solve the
SPDEs using a standard finite element method (FEM) solver (Skeel & Berzins, 1990, Matlab solvers
for 1-D PDEs). In a second experiment, we demonstrate the performance of our algorithm on a sub-
set of a Bike-sharing dataset obtained from the UCI machine learning repository (Fanaee-T & Gama,
2013). In this experiment we estimate a smoothing posterior density for the observed counts of the
demand for bikes in a 24 hour period, assuming that the demand process is well-modeled by a Cox
process. In our third and final experiment, we apply our method to compute an approximation to a 4-
dimensional smoothing posterior density. We note that despite being low dimensional, the standard
FEM solver does not scale to this setting, while our method can be straightforwardly adapted.
6
Under review as a conference paper at ICLR 2021
5.1	Variational approximation of univariate Smoothing posterior density
As defined in Section 2, we are interested in learning the posterior measure over an unknown process
{xt ∈ R} where the intensity process {zt} satisfies zt = h(xt). We set b(x) = -x and σ(x) = 1
in the prior SDE as defined in equation 2. Furthermore, motivated from the mathematical structure
of the true smoothing SDE equation 11, we fix our variational family Q to be the class of measures
induced by solutions to the class of SDEs in equation 13 with drift and diffusion coefficient set as
b(x,t, Nt) = -ψt + V(x,t,Nτ - Nt； θ) - x, σ(x) = 1,
where Ψ0t is the derivative of Ψt with respect to x. Here V(x, t, NT - Nt; θ) is modeled using a
neural network with 2 hidden layers whose parameters we call θ (see Appendix A.3 for more details
on the architecture).
5.1.1	S imulated dataset
We generate sample paths of the count observation N0,T from a non-homogeneous Poisson process,
where the intensity process {zt0 } is the solution of the following ordinary differential equation,
dzt = 20(2 — t) exp(-0.85(2 — t)2), and z0 = 0. We fix the map h(a) = 5 * exp(-.08 * (a — 5)2)
in this experiment. To train the neural network V, we use 150 samples paths of the count observation
between time interval [0, 2] and optimize ELBO in equation 19 using Adam (Kingma & Ba, 2014).
To demonstrate the efficacy of our approach, we first generate 20 test sample paths of count obser-
vations and compute the true smoothing posterior density (defined in equation 9) using the solution
of the forward and backward SPDEs (defined in equation 7 and equation 8 respectively). We do this
using the FEM method. Then for the same test observations we compute the VBSP density by FEM
using the trained drift coefficient of VBSP SDE (see equation 10 and 11).
Comparative results are presented in Figure 1. We clearly see from the the top row plot that these
are very similar to each other, with our variational approximation capturing the sharp rises in density
with high fidelity. Note from the first two plots in the bottom row that as the ELBO decreases the
gap between the the true and VB smoothing posterior reduces too. Moreover, the time required to
compute the smoothing density using the forward and backward SPDEs on the test data is about 2.2
seconds which is approximately fifty percent higher than the trained VBSP density, which required
1.6 seconds (on an 3.1 GHz Intel i5 CPU). This is due to the fact that we are required to solve one
SPDE in the latter case instead of two in the former case.
Notice that the learnt drift of the smoothing SDE equation 13 is a map which can be used with
any sample path of count observations to generate Monte Carlo samples from the an approximate
smoothing posterior density. In contrast, it is challenging to sample from the true smoothing SDE
as it involves computing the solution of the system of SPDEs in equation 7 and 8. Furthermore, this
solution must be recomputed for each new sample path of count observations.
5.1.2	Bike Sharing Dataset
In this experiment, we compute the VBSP density for the hourly counts of demand in a bike-sharing
system. The experimental setting remains unaltered, albeit the diffusion coefficient is set to σ(x) =
1.1, to capture the increased variability in counts of bike demand than the variability in simulated
counts in the previous experiment. Notice that fixing the map h is a modeling question, and we
consider mappings of the form h(x) = a exp(-b(x - c)2) parametrized by a, b, and c. We take a
simple empirical Bayes heuristic to fix their values after observing the count observations, based on
the fact that h(x)∆ is the mean of the Poisson counts in the interval ∆. We set a in such a way that
h(x)∆ equals the maximum of the median observed count. For the Bike-sharing data we re-scaled
the problem to interval [0, 2] and fixed ∆ = 0.083 and thus choose 90/0.083 ' 1050 as a. The
choice of b and c depends on the diffusion coefficient σ(x) and x0, as the SDE should be able to
explore the relevant domain of h to appropriately model the actual count observations. Thus, after
looking at the count data, we chose h(x) = 1050 * exp(-0.001 * (y - 50)2).
The empirical results are summarized in Figure 2. We note here that the FEM approach (our imple-
mentation) to compute the VBSP density was numerically unstable and failed; this may be attributed
7
Under review as a conference paper at ICLR 2021
SIUno。
50
Figure 1: Variational vs. True smoothing posterior density on the 1-D simulated dataset. The top
row compares the variational approximation with the true smoothing density. The bottom-left plot
shows the ELBO as a function of epochs of the training algorithm. We also compute the L1 distance
between the VB and true smoothing density over 20 sample paths of both training and test count
observations as the training progresses and plot the 10th, 50th and 90th quantile in the bottom-
middle figure. The bottom-right plot depicts that 95% of the test count observations are within the
5th and 95th quantile of the simulated counts, where counts are simulated using the learned VBSP
SDE for that test sample path of count observations.
to the concentrated nature of the VBSP, as visible in the plots. It is evident from the last plot in Fig-
ure 2 that for the current h, VBSP density captures the trend in the real count observations well,
however we anticipate that a different choice of the map h can better model the count observations.
Note that in a smoothing problem, the prior intensity process (specifically the drift coefficient b(∙),
diffusion coefficient σ(∙) and the map h(∙)) are sourced from an expert, and the objective is to update
the modeler’s beliefs with count observations to compute the smoothing posterior density. In many
settings, the functions b(∙),σ(∙) and h(∙) are known only UP to some unknown parameters. It is
fairly straightforward to combine the parameters of h and b with the neural network parameters
θ, and learn them all in a data-driven manner. We choose not to do this to keep the discussion
simple. Learning the parameters of σ presents a slightly greater challenge since the path measures
for different settings ofσ are singular. This is a crucial difference from the finite dimensional setting,
where the Lebesgue measure is a common reference. We leave solving this problem for future work.
5.2	Variational approximation of multivariate smoothing posterior density
We demonstrate our method on a 4-dimensional smoothing problem. In this case, we fix the map
h(a) = 25 * exp(-.08 * ∣∣a - 5∣∣2), where ∣∣ ∙ k is the L-2 norm, and a ∈ R4. We also choose
the prior density to be induced by an SDE defined in equation 2 with b(x) = -x and σ(x) = I,
where x ∈ Rd and I is a d × d identity matrix. Furthermore, we choose our variational family to
be a family of SDE as defined in equation 13, with drift and diffusion coefficients, b(x, t, Nt) =
-守 + Vd(x,t,Nτ - Nt； θ) - X and σ(x) = I.
To train the neural network V , we use 150 samples paths of the count observation between time
interval [0, 2] and optimize the ELBO defined in equation 19 using Adam (Kingma & Ba, 2014).
We plot the empirical results in Figure 3.
8
Under review as a conference paper at ICLR 2021
Figure 2: VBSP smoothing posterior density for Bike Sharing Data. The left plot shows the
ELBO as the training progresses. The second and third plots depict the empirical VBSP density
computed on a test sample path of count observations, using 1000 simulated sample paths of the
learnt variational smoothing SDE. In the right plot, we use the same test sample path of count
observation, compute the VBSP using the trained drift and demonstrate that most count observations
(of the test sample path) ( 70%) lie within the 97.5th and 2.5th quantile of the simulated counts.
0M
Figure 3: Multivariate VB smoothing posterior density: The first two rows of Figure 3 show the
marginals of the (empirical) VBSP density using the trained drift coefficient for a given test sample
path of count observations. The bottom-left figure plots the ELBO value as a function of the number
of training epochs. Then, for a test sample path of count observations, the right hand plot shows that
more than 95% of these observations are within the 95% confidence interval of the simulated count
observations generated from the trained 4-dimensional VBSP density computed for that test path.
20	30	40	50
Epochs
References
Cedric Archambeau, Dan Cornford, Manfred Opper, and John Shawe-Taylor. GaUSSian process
approximations of stochastic differential equations. Journal of machine learning research, 1:
1-16, 2007.
Cedric Archambeau, Manfred Opper, Yuan Shen, Dan Cornford, and John S Shawe-Taylor. Varia-
tional inference for diffusion processes. In Advances in Neural Information Processing Systems,
pp. 17-24, 2008.
Alan Bain and Dan Crisan. Fundamentals of stochastic filtering, volume 60. Springer Science &
Business Media, 2008.
9
Under review as a conference paper at ICLR 2021
JMC Clark. The design of robust approximations to the stochastic differential equations of nonlinear
filtering. Communication systems and random process theory, 25:721-734, 1978.
David R Cox. Some statistical methods connected with series of events. Journal of the Royal
Statistical Society: Series B (Methodological), 17(2):129-157, 1955.
David Roxbee Cox and Valerie Isham. Point processes, volume 12. CRC Press, 1980.
Botond Cseke, Manfred Opper, and Guido Sanguinetti. Approximate inference in latent gaussian-
markov models from continuous time observations. In Advances in Neural Information Process-
ing Systems, pp. 971-979, 2013.
Mark HA Davis. A pathwise solution of the equations of nonlinear filtering. Theory of Probability
& Its Applications, 27(1):167-175, 1982.
MHA Davis. Pathwise non-linear filtering. In Stochastic Systems: The Mathematics of Filtering and
Identification and Applications, pp. 505-528. Springer, 1981.
Robert J. Elliott and W. Paul Malcolm. General smoothing formulas for Markov-modulated poisson
observations. IEEE Transactions on Automatic Control, 50(8):1123-1134, 2005. ISSN 00189286.
doi: 10.1109/TAC.2005.852565.
Hadi Fanaee-T and Joao Gama. Event labeling combining ensemble detectors and background
knowledge. Progress in Artificial Intelligence, pp. 1-15, 2013. ISSN 2192-6352. doi: 10.1007/
s13748-013-0040-3. URL [WebLink].
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510,
2018.
Yuval Harel, Ron Meir, and Manfred Opper. A tractable approximation to optimal point process
filtering: Application to neural encoding. Advances in Neural Information Processing Systems,
2015-January:1603-1611, 2015. ISSN 10495258.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-
ence. The Journal of Machine Learning Research, 14(1):1303-1347, 2013.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Anna Kutschireiter, Simone Carlo Surace, and Jean-Pascal Pfister. The hitchhiker’s guide to
nonlinear filtering. Journal of Mathematical Psychology, 94:102307, February 2020. doi:
10.1016/j.jmp.2019.102307. URL https://doi.org/10.1016/j.jmp.2019.102307.
Xuechen Li, Ting-Kam Leonard Wong, Ricky TQ Chen, and David Duvenaud. Scalable gradients
for stochastic differential equations. arXiv preprint arXiv:2001.01328, 2020.
Sanjoy K Mitter and Nigel J Newton. A variational approach to nonlinear estimation. SIAM journal
on control and optimization, 42(5):1813-1833, 2003.
Bernt Oksendal. Stochastic differential equations: an introduction with applications. Springer
Science & Business Media, 2013.
Simo Sarkka. Bayesian Filtering and Smoothing. Institute of Mathematical Statistics Textbooks.
Cambridge University Press, 2013. doi: 10.1017/CBO9781139344203.
David Schnoerr, Ramon Grima, and Guido Sanguinetti. Cox process representation and inference
for stochastic reaction-diffusion processes. Nature Communications, 7(1), May 2016. doi: 10.
1038/ncomms11729. URL https://doi.org/10.1038/ncomms11729.
J. H. Van Schuppen. Filtering, prediction and smoothing for counting process observations, a
martingale approach. SIAM Journal on Applied Mathematics, 32(3):552-570, May 1977. doi:
10.1137/0132045. URL https://doi.org/10.1137/0132045.
10
Under review as a conference paper at ICLR 2021
Robert D. Skeel and Martin Berzins. A method for the spatial discretization of parabolic equations
in one space variable. SIAM Journal on Scientific and Statistical Computing, 11(1):1-32, January
1990. doi: 10.1137/0911001. URL https://doi.org/10.1137/0911001.
D Snyder. Smoothing for doubly stochastic poisson processes. IEEE Transactions on Information
Theory, 18(5):558-562, 1972.
Alex Susemihl, Ron Meir, and Manfred Opper. Analytical results for the error in filtering of Gaus-
sian processes. Advances in Neural Information Processing Systems 24: 25th Annual Conference
on Neural Information Processing Systems 2011, NIPS 2011, pp. 1-9, 2011.
Tobias Sutter, Arnab Ganguly, and Heinz Koeppl. A variational approach to path estimation and
parameter inference of hidden diffusion processes. Journal of Machine Learning Research, 17:
1-37, 2016. ISSN 15337928.
Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian
models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019.
Ruixin Wang, Prateek Jaiswal, and Harsha Honnappa. Estimating stochastic poisson intensities
using deep latent models. arXiv preprint arXiv:2007.06037, 2020.
Tingting Zhang and S. C. Kou. Nonparametric inference of doubly stochastic poisson process data
via the kernel method. The Annals of Applied Statistics, 4(4):1913-1941, December 2010. doi:
10.1214/10-aoas352. URL https://doi.org/10.1214/10-aoas352.
Xiaowei Zhang, L Jeff Hong, and Jiheng Zhang. Scaling and modeling of call center arrivals. In
Proceedings of the Winter Simulation Conference 2014, pp. 476-485. IEEE, 2014.
11
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Derivation of Smoothing SPDE
According to Theorem * [sic] in Elliott & Malcolm (2005), Kt :=
surely constant for t ≤ T. Using this result it follows that
dtpS,t =Ktqt(X)dtvt(X) + Ktvt(X)dtqt(X)
=Ktqt(X)dtvt(x) + Ktvt(X) Ψ-1L*[Ψt
( Rd qt(ξ)vt(ξ)dξ)-1 is almost
Ps,t	]
Ktvt(X)I
—3必〃…(x)]+Vt (χ)[ψ-1L*[ψt 禺]
vt(x)
— MLM(x)]+ ¼(x) L*[
Vt(X)
VtM
(20)
where Vt(X) = Ψt-1vt(X) and PS,t = pt(X|N0,T) are introduced for brevity. Now observe that
Vt(X)L*
i,j
Vt(x)	2)： dXiXj (ai,j (X)
Vt(X) 2 X dXiXj (ai,j (X)
i,j
+
i
-	dXi	bi(X)
i
dχi Vt(xWi(x)ps,t
Vt(X)
-	[(∂Xi bi (X)pS,t + bi (X)∂XipS,t)] .
i
(21)
Consider the summand in the first term of equation 21 and observe that
dX X
XiXj
Vt(X)
dX
Xi
Vt(X)∂Xj (ai,j (X)pS,t) - ∂Xj Vt(X)ai,j (X)pS,t
Vt2(x)
'θjXyps^
_Vt(x)dXiXj- (ai,j(x)ps,t) — ∂xiVt(x)∂x3- (ai,j(x)ps,t)
=	V^
+ 2%匚()X) [dXjVt(X) (ai,j(X)PS,t)] — V21X)dXi (dXjVt(X)ai,j(X)PS,t)
Now the second term in equation 21 can be expressed as
Vt(X)dXiXj
Vt(X)
∂XiXj (ai,j (X)pS,t ) -
∂Xi Vt(X)∂Xj (ai,j (X)pS,t)
Vt(X)
'θjXyps^
+ 2dVV()X) [dXj Vt(X) (ai,j (X)PS,t)] - VIx) dXi [dXj Vt (Xlaij (X)PS,t]
(22)
Substituting the above expression in equation 21, we have
Vt(x)L*
Ps,t
-V(XV
2∂xi Vt (x)
V2(x)
2 XndXi Xj (ai,j (X)Pst)-
∂Xi Vt (X)∂Xj (ai,j (X)pS,t)
Vt(X)
[dXjVt(X) (ai,j(X)PS,t)[ - V1X)dXi [dXj匕(X)ai,j(X)PS,t] O
a% Vt (x)bi(x)ps,J
Vt(X)
-	[(∂Xibi(X)pS,t + bi(X)∂XipS,t)] .
i
(23)
+
+
i
Now consider the first term in the RHS of equation 20
— 品L[Vt(x)] = — I X αi⅛XFdXiXj Vt(X) — X bi⅛dXiVt(x).	(24)
t(X)	2 i,j t(X)	i	t(X)
12
Under review as a conference paper at ICLR 2021
Now substituting equation 23 and equation 24 into equation 20, we obtain
∂
∂tpS,t
pS,t
-品LM(x)] + ¼(x) [l*[容]
Vt(x)
-2 X
i,j
ai,j (X)PS,t
Vt(x)
dXiXj ¼(x) - X biVXSt dXiVt(X) + 1 X {dXiXj (ai,j (X)PS,t)
i t	i,j
dXiVt(X)dXj (ai,j(X)PS,t) + 2∂x"Vt(x)
Vt(X)
dXiVt(X)bi(X)PS,t
V2 (X)
dXj Vt (X) (ai,j (X)PS,t)
-V (x) dXi [dXj Vt(X)ai,j (X)pS,t] O
Vt(X)
-	[(dXibi(X)PS,t + bi(X)dXiPS,t)]
i
2 X dxiXj (ai,j (X)PS,t) - X∂Xi
∖bi(x)ps,t] - 2 Xn
∂xi xj Vt (X)
i,j
i,j
Vt(X)
[ai,j (X)pS,t]
dXiVt(X)dXj (ai,j (X)PS,t)	2dXi Vt (X)dXj Vt(X)
----------------------------------Z--------
Vt(X)
V2(χ)
[(ai,j(X)pS,t)]
+ VIx) [dXj 匕(X)dXi(ai,j (X)PS,t) + dXiXj Vt(X)ai,j (X)PS,t] O
2 X dXiXj (ai,j (X)PS,t ) - XdXi [bi (X)PS,t]
i,j
1
——
2
X ∂Xi [∂Xj log Vt(X)[ai,j(X)PS,t] +
i,j
∂xiVt(X)∂xj (ai,j(X)pS,t)
Vt(X)
—
+
i
+
i
i
—
d"iVt(V)；j匕(X) [(ai,j(X)PS,t)] + V1X) [dXiXjVt(X)Wj(X)PS,t] O
Since, dXiXj Vt(X) = dXj Xi Vt (X) therefore
ItPS,t = 1 X dXiXj (Iaij(X)PS,t) - XdXi [bi(X)Ps,t]
i
i,j
2 X{∂X,
i [dXj log Vt(X)[ai,j (X)PS,t] + dXj [dXi log Vt(X)[ai,j (X)PS,t]]
—
2 X	dXiXj	(ai,j (X)PS,t) - X	dXi	[bi(x)PS,t]	- X {dXi	[dXj	log	Vt (x)[ai,j	(X)PS,t]]	O
i,j
i,j
1 X dXiXj (ai,j(X)PS,t) - X dXi {[(a(x)[Vlog Vt(X)Di + bi(x)]ps,t}.
(25)
i,j
i
i
A.2 KL-divergence between a member of variational family and prior SDE
We derive a pathwise expression for KL(QkΠ0) for a given count observation path N0,T.
Theorem A.1. Define U(Xt,t,Nt; θ) := σ(Xt)-1 (b(Xt,t) - E(Xt,t, Nt； θ)) and suppose that U
satisfies a strong Novikov’s condition:
E exp
ku(Xt,t, Nt; θ)k2dt < +∞ ∀θ, φ.
Then,
KL(Qkn0) = Eq 2 /
T
kU(Xt,t, Nt; θ)k2ds .
(26)
13
Under review as a conference paper at ICLR 2021
Proof. Given samples path of count observation N0,T , using the definition ofu and under Novikov’s
condition, using Girsanov’s theorem (Oksendal, 2013, Theorem 8.6.8), we have
dQ
d∏o
exp (- Z u(xt,t,Nt; θ)dBs - 2 Z ∣∣u(xt,t,Nt; θ)∣∣2ds
and
Bt ：= Z u(xt,t,Nt; θ)ds + B(t)
0
is a Brownian motion w.r.t. Q. Furthermore, we also have
dxt = b(xt, t)dt + σ(xt)dBt.
(27)
(28)
The following expression is obtained by substituting for d∏Q:
EQ log
dQ(x0:T)
dΠ0 (x0:T)
-EQ
u(xt,t, Nt； θ)dBs + 1 ∣u(xt,t,Nt; θ)∣∣2) ds .
(29)
Now, applying B(t) := Rt u(xt,t, Nt； θ)ds + Bt in Equation equation 29, We have
Z u(xt, t, Nt; θ)dBs
0
EQ
Z0T
"Z0T
/	， Λ T ∕Λ∖ Γ τr∖	/	， Λ T C∖ 7 1
u(xt, t, Nt； θ)[dBs - u(xt, t, Nt； θ)ds]
u(xt,t, Nt； θ)dBS - [T ∣u(xt, t, Nt； θ)∣∣2ds
0
(30)
Substituting equation 30 into equation 29 yields
KL(Qkn0 )= EQ [log ^QgT))
ku(xt, t, Nt； θ)k2ds
and thus concludes the proof.
(31)
□
14
Under review as a conference paper at ICLR 2021
A.3 Neural network architecture
For all the experiments, we use the neural network architecture depicted in Figure A.3, with ReLU
activation functions between fully-connected hidden layers.
Figure 4: Neural network architecture used in the numerical experiments.
A.4 Comparing computational time required to numerically compute VBSP
and True smoothing density using FEM
Epochs	1	2	3	4	5	6	7	8	9	10	11	12	13	14	15	16
VBSP	1.62	1.44	1.62	1.6	1.59	1.63	1.59	1.61	1.61	1.62	1.6	1.65	1.65	1.61	1.61	T.73
True SD	2.07	孱	2.22	2.17	2.16	2.16	2.23	2.12	2.12	2.15	2.12	2.17	2.14	2.11	2.14	ɪɪɪ
Epochs	18	19	20	21	22	23	24	25	26	27	28	29	30	31	32	-33-
VBSP	1.7	1.72	1.83	1.9	1.86	1.84	1.81	1.85	1.75	1.78	1.87	1.9	1.95	~∑l-	2.06	T.8r
True SD	2.09	2.18	2.25	2.37	2.22	2.18	2.22	2.23	2.21	2.24	2.33	2.29	2.36	2.39	2.41	~∏7
Epochs	35	36	37	38	39	40	41	42	43	44	45	46	47	48	49	^30^^
VBSP	1.83	1.81	1.85	1.89	1.87	1.84	1.85	1.86	1.92	1.91	1.92	1.88	1.84	1.91	1.91	T.82
True SD	2.14	2.14	2.15	2.18	2.25	2.17	2.14	2.12	2.2	2.15	2.23	2.17	2.17	2.28	2.25	~Σ7T
17
1.74
2.15
34
1.84
2.12
Table 1: Comparing computational time (in sec) required to compute 1-D VBSP and true smoothing
density using FEM. The time reported here are median over 20 test sample paths of count observa-
tions at each epoch
15