Under review as a conference paper at ICLR 2021
Near-Optimal Regret Bounds for Model-Free
RL in Non-Stationary Episodic MDPs
Anonymous authors
Paper under double-blind review
Ab stract
We consider model-free reinforcement learning (RL) in non-stationary Markov
decision processes (MDPs). Both the reward functions and the state transition
distributions are allowed to vary over time, either gradually or abruptly, as long
as their cumulative variation magnitude does not exceed certain budgets. We pro-
pose an algorithm, named Restarted Q-Learning with Upper Confidence Bounds
(RestartQ-UCB), for this setting, which adopts a simple restarting strategy and
an extra optimism term. Our algorithm outperforms the state-of-the-art (model-
based) solution in terms of dynamic regret. Specifically, RestartQ-UCB with
111	2
Freedman-type bonus terms achieves a dynamic regret of O(S3 A3∆3 HT3),
where S and A are the numbers of states and actions, respectively, ∆ > 0 is
the variation budget, H is the number of steps per episode, and T is the total num-
ber of steps. We further show that our algorithm is near-optimal by establishing an
information-theoretical lower bound of Ω(S 1 A3 ∆ 1 H2 T3), which to the best of
our knowledge is the first impossibility result in non-stationary RL in general.
1	Introduction
Reinforcement learning (RL) studies the class of problems where an agent maximizes its cumulative
reward through sequential interaction with an unknown but fixed environment, usually modeled by
a Markov Decision Process (MDP). At each time step, the agent takes an action, receives a random
reward drawn from a reward function, and then the environment transitions to a new state according
to an unknown transition kernel. In classical RL problems, the transition kernel and the reward
functions are assumed to be time-invariant. This stationary model, however, cannot capture the
phenomenon that in many real-world decision-making problems, the environment, including both
the transition dynamics and the reward functions, is inherently evolving over time. Non-stationarity
exists in a wide range of applications, including online advertisement auctions (Cai et al., 2017; Lu
et al., 2019), dynamic pricing (Board, 2008; Chawla et al., 2016), traffic management (Chen et al.,
2020), healthcare operations (Shortreed et al., 2011), and inventory control (Agrawal & Jia, 2019).
Among the many intriguing applications, we specifically emphasize two research areas that can
significantly benefit from progress on non-stationary RL, yet their connections have been largely
overlooked in the literature. The first one is sequential transfer in RL (Tirinzoni et al., 2020) or
multi-task RL Brunskill & Li (2013). In this setting, the agent encounters a sequence of tasks
over time with different system dynamics and reward functions, and seeks to bootstrap learning
by transferring knowledge from previously-solved tasks. The second one is multi-agent reinforce-
ment learning (MARL) (Littman, 1994), where a set of agents collaborate or compete in a shared
environment. In MARL, since the transition and reward functions of the agents are coupled, the en-
vironment is non-stationary from each agent’s own perspective, especially when the agents learn and
update policies simultaneously. A more detailed discussion on how non-stationary RL can benefit
sequential transfer, multi-task, and multi-agent RL is given in Appendix A.
Learning in a non-stationary MDP is highly non-trivial due to the following challenges. The first
one is the exploration vs. exploitation challenge inherited from standard (stationary) RL. An agent
needs to explore the uncertain environment efficiently while maximizing its rewards along the way.
Classical solutions in stationary RL oftentimes leverage the “optimism in the face of uncertain”
principle that adopts an upper confidence bound to guide exploration. These bounds can be either an
optimistic estimate of the state transition distributions in model-based solutions (Jaksch et al., 2010),
1
Under review as a conference paper at ICLR 2021
Setting	Algorithm	Regret	Model-free?	Comment
Undis- counted	Jaksch et al. (2010) Gajane et al. (2018) Ortner et al. (2019) Cheung et al. (2020) Lower bound	Oe(S A 1 L 1 D T 1) O(S 2 A1L 1 D 2 T 1) Oe(S A 1 ∆ 1 D T 1) O(S2A 1 ∆ 1 D T4) Ω(S 1 A 1 ∆ 1 D 2 T 1)	X X X X	only abrupt changes only abrupt changes requires local budgets does not require ∆
Episodic	Domingues et al. (2020) ReStartQ-UCB* Lower bound*	O(S A 1 ∆ 1 H4T 1) O(S 1 A 1 ∆ 1 H T 1) Ω(S 1 A 1 ∆ 1 H 2 T 1)	X ✓	also metric spaces
Table 1: Dynamic regret comparisons for RL in non-stationary MDPs. S and A are the numbers of
states and actions, L is the number of abrupt changes, D is the maximum diameter, H is the number
of steps per episode, and T is the total number of steps. Gray cells denote results from this paper.
or an optimistic estimate of the Q-values in the model-free ones (Jin et al., 2018; Zhang et al., 2020).
An additional challenge in non-stationary RL is the trade-off between remembering and forgetting.
Since the system dynamics vary from one episode to another, all the information collected from
previous interactions are essentially out-of-date and biased. In fact, it has been shown that a standard
RL algorithm might incur a linear regret if the non-stationarity is not handled properly (Ortner et al.,
2019). On the other hand, the agent does need to maintain a sufficient amount of information from
history for future decision making, and learning what to remember becomes a further challenge.
In this paper, we introduce an algorithm, named Restarted Q-Learning with Upper Confi-
dence Bounds (RestartQ-UCB), to address the aforementioned challenges in non-stationary RL.
Our algorithm utilizes an extra optimism term for exploration, in addition to the standard
Hoeffding/Bernstein-based bonus in the upper confidence bound, to counteract the non-stationarity
of the MDP. This additional bonus term guarantees that our optimistic Q-value is still an upper bound
of the optimal Q?-value even when the environment changes. To address the second challenge, we
adopt a simple but effective restarting strategy that resets the memory of the agent according to a
calculated schedule. Similar strategies have also been considered in non-stationary bandits (Besbes
et al., 2014) and non-stationary RL in the un-discounted setting (Jaksch et al., 2010; Ortner et al.,
2019). The restarting strategy ensures that our algorithm only refers to the most up-to-date experi-
ence for decision-making. A further advantage of our algorithm is that RestartQ-UCB is model-free.
Compared with model-based solutions, our model-free algorithm is more time- and space-efficient,
flexible to use, and more compatible with the design of modern deep RL architectures.
Related Work. Dynamic regret of non-stationary RL has been mostly studied using model-based
solutions. Jaksch et al. (2010) consider the setting where the MDP is allowed to change abruptly L
times, and achieve a regret of O(SA 1 L 1 DT2), where D is the maximum diameter of the MDP. A
sliding window approach is proposed in Gajane et al. (2018) under the same setting. Ortner et al.
(2019) generalize the previous setting by allowing the MDP to vary either abruptly or gradually at
every step, subject to a total variation budget of ∆. Cheung et al. (2020) consider the same setting
and develop a sliding window algorithm with confidence widening. The authors also introduce a
Bandit-over-RL technique that adaptively tunes the algorithm without knowing the variation bud-
get. In a setting most similar to ours, Domingues et al. (2020) investigate non-stationary RL in the
episodic setting. They propose a kernel-based approach when the state-action set forms a metric
11	42
space, and their results can be reduced to an O(SA2 ∆ 1 H 1 T 1) regret in the tabular case. Fei et al.
(2020) also consider the episodic setting, but they assume stationary transition kernels and adver-
sarial (subject to some smoothness assumptions) full-information rewards. The authors propose two
* Connections to stationary RL: Results in Table 1 hold for ∆ > 0. To derive an upper bound for ∆ = 0,
we only need a simple modification in the proof of Theorem 3 by setting the number of epochs to be 1. This
leads to an upper bound of O(H，SAT), which matches the results given in Zhang et al. (2020). A similar
modification in the proof of Theorem 4 results in a lower bound of Ω(H√SAT) when ∆ = 0.
2
Under review as a conference paper at ICLR 2021
policy optimization algorithms, which are also the only model-free solutions that we are aware of
in non-stationary RL. In contrast, we allow both the transition kernel and the reward function to
change over time, and deal with bandit-feedback, which makes the setting in Fei et al. (2020) not
directly comparable. Table 1 compares our regret bounds with existing results that tackle the same
setting as ours. Interested readers are referred to Padakandla (2020) for a comprehensive survey on
RL in non-stationary environments. We would also like to mention another related line of research
that studies online/adversarial MDPs (Yu & Mannor, 2009; Neu et al., 2010; Arora et al., 2012;
Yadkori et al., 2013; Dick et al., 2014; Wang et al., 2018; Lykouris et al., 2019; Jin et al., 2019), but
they mostly only allow variations in reward functions, and use static regret as performance metric.
Finally, RL with low switching cost (Bai et al., 2019) also shares a similar spirit as our restarting
strategy since it also periodically forgets previous experiences. However, such algorithms do not
address the non-stationarity of the environment explicitly, and it is non-trivial to analyze its dynamic
regret in terms of the variation budget.
Non-stationarity has also been considered in bandit problems. Under different non-stationary multi-
armed bandit (MAB) settings, various methods have been proposed, including decaying memory
and sliding windows (Garivier & Moulines, 2011; Keskin & Zeevi, 2017), as well as restart-based
strategies (Auer et al., 2002; Besbes et al., 2014; Allesiardo et al., 2017). These methods largely
inspired later research in non-stationary RL. A more recent line of work developed methods that do
not require prior knowledge of the variation budget (Karnin & Anava, 2016; Cheung et al., 2019a) or
the number of abrupt changes (Auer et al., 2019). Other related settings considered in the literature
include Markovian bandits (Tekin & Liu, 2010; Ma, 2018), non-stationary contextual bandits (Luo
et al., 2018; Chen et al., 2019), linear bandits (Cheung et al., 2019b; Zhao et al., 2020), continuous-
armed bandits (Mao et al., 2020), and bandits with slowly changing rewards (Besbes et al., 2019).
Contributions. First, we propose RestartQ-UCB, the first model-free RL algorithm in the general
setting of non-stationary MDPs, where both the transition kernel and reward functions are allowed
to vary over time. Second, we provide dynamic regret analysis for RestartQ-UCB, and show that
it outperforms even the model-based state-of-the-art solution. Third, we establish the first lower
bounds in non-stationary RL, which suggest that our algorithm is optimal in all parameter depen-
dences except for an H3 factor, where H is the episode length.
In the main text of this paper, we will present and analyze a simpler version of RestartQ-UCB with
a Hoeffding-style bonus term. Replacing the Hoeffding term with a Freedman-style one will lead to
a tighter regret bound, but the analysis is more involved. For clarity of presentation, we defer the
exposition and analysis of the Freedman-based algorithm to the appendices. All missing proofs in
the paper can also be found in the appendices.
2	Preliminaries
Model: We consider an episodic RL setting where an agent interacts with a non-stationary MDP
for M episodes, with each episode containing H steps. We use a pair of integers (m, h) as a time
index to denote the h-th step of the m-th episode. The environment can be denoted by a tuple
(S , A, H, P, r), where S is the finite set of states with |S | = S, A is the finite set of actions with
|A| = A, H is the number of steps in one episode, P = {Phm }m∈[M],h∈[H] is the set of transition
kernels, and r = {rhm}m∈[M],h∈[H] is the set of mean reward functions. Specifically, when the
agent takes action ahm ∈ A in state shm ∈ S at the time (m, h), it will receive a random reward
Rhm(shm, ahm) ∈ [0, 1] with expected value rhm(shm, ahm), and the environment transitions to a next
state sh+i following the distribution Pm (∙ | Sm,a*. Itis worth emphasizing that the transition
kernel and the mean reward function depend both on m and h, and hence the environment is non-
stationary over time. The episode ends when sHm+1 is reached. We further denote T = MH as the
total number of steps.
A deterministic policy π : [M] × [H] × S → A is a mapping from the time index and state space
to the action space, and we let πhm (s) denote the action chosen in state s at time (m, h). Define
Vhm,π : S → R to be the value function under policy π at time (m, h), i.e.,
H
Vrn(S) =f E X Tm (sh0,∏m (Sh书 | Sh = S ,Sh0+1 〜Pm (∙ | s〃,a〃).
h0=h
3
Under review as a conference paper at ICLR 2021
Accordingly, the state-action value function Qhm,π : S × A → R is defined as:
H
Qhm,π(s,a) d=ef rhm(s, a) +E X rhm0 (sh0, πhm0 (sh0)) | sh = s,ah = a
h0=h+1
def
For simplicity of notation, We let PmVh+ι(s, a) = E§o〜Pm(∙∣s,a) [Vh+ι(s0)]. Then, the Bellman
equation gives Vhm,π (s) = Qhm,π (s, πhm(s)) and Qhm,π (s, a) = (rhm + Phm Vhm+,1π)(s, a), and we also
have VHm+,π1(s) = 0, ∀s ∈ S by definition. Since the state space, the action space, and the length
of each episode are all finite, there alWays exists an optimal policy π? that gives the optimal value
Vhm,?(s) d=ef Vhm,π? (s) = supπ Vhm,π(s), ∀s ∈ S, m ∈ [M], h ∈ [H]. From the Bellman optimality
equation, We have Vhm,?(s) = maxa∈A Qhm,?(s, a), Where Qhm,?(s, a) d=ef (rhm + PhmVhm+,1?)(s, a),
andVHm+,?1(s) = 0,∀s ∈ S.
Dynamic Regret: The agent aims to maximize the cumulative expected reWard over the entire M
episodes, by adopting some policy π. We measure the optimality of the policy π in terms of its
dynamic regret (Cheung et al., 2020; Domingues et al., 2020), Which compares the agent’s policy
With the optimal policy of each individual episode in the hindsight:
M
Rg M) = X (Vm,* (Sm)- vm,π W)),
m=1
Where the initial state S1m of each episode is chosen by an adversary (and more specifically, by an
oblivious adversary (Zhang et al., 2020)). Dynamic regret is a stronger measure than the standard
(static) regret, Which only considers the single policy that is optimal over all episodes combined.
Variation: We measure the non-stationarity of the MDP in terms of its variation in the mean reWard
function and transition kernels:
M-1 H	M-1 H
∆r 驾 X X sup ∣rm(s,a)-rm+1(s,a)∣, ∆p =f X X SUplIPm(∙ | s,a) -Pm+1(∙ | s,a)||「
m=1 h=1 s,a	m=1 h=1 s,a
where 小% is the L1-norm. Note that our definition of variation only imposes restrictions on the
summation of non-stationarity across tWo different episodes, and does not put any restriction on the
difference between two consecutive steps in the same episode; that is, Pm(∙ | s,a) and Pm+ι(∙ | s,a)
are allowed to be arbitrarily different. We further let ∆ = ∆r + ∆p, and assume ∆ > 0.
3 Algorithm: RestartQ-UCB
We present our algorithm Restarted Q-Learning with Hoeffding Upper Confidence Bounds
(RestartQ-UCB Hoeffding) in Algorithm 1. Replacing the Hoeffding-style upper confidence bound
in Algorithm 1 with a Freedman-style one will lead to a tighter regret bound, but for clarity of
exposition, the latter version will be deferred to Algorithm 2 in Appendix C.
RestartQ-UCB breaks the M episodes into D epochs, with each epoch containing K = d M ]
episodes (except for the last epoch which possibly has less than K episodes). The optimal value
of D (and hence K) will be specified later in our analysis. RestartQ-UCB periodically restarts
a Q-learning algorithm with UCB exploration at the beginning of each epoch, thereby addressing
the non-stationarity of the environment. For each d ∈ [D], define ∆(rd) to be the variation of the
mean reward function within epoch d. By definition, we have PdD=1 ∆(rd) ≤ ∆r . Further, for each
d ∈ [D] and h ∈ [H], define ∆(rd,h) to be the variation of the mean reward at step h in epoch d, i.e.,
∆(rd,h) d=ef Pmmi=n({dd-K1,)MK}+-11 sups,a rhm(S,a) -rhm+1(S,a) . It also holds that PhH=1 ∆(rd,h) = ∆(rd) by
definition. Define ∆(pd) and ∆(pd,h) analogously.
Since our algorithm essentially invokes the same procedure for every epoch, in the following, we
focus our analysis on what happens inside one epoch only (and without loss of generality, we focus
on epoch 1, which contains episodes 1, 2, . . . , K). At the end of our analysis, we will merge the
results across all epochs.
4
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Algorithm 1: RestartQ-UCB (Hoeffding)
for epoch d — 1 to D do
Initialize: Vh(S) - H 一 h + 1, Qh(s, a) — H 一 h +1,Nh(s,a) . 0, Nh(s, a) - 0,
rh(s, a) - 0, vh(s, a) - 0, for all (s, a, h) ∈ S ×A× [H];
for episode k — (d — 1)K + 1 to min{dK, M} do
observe s1k ;
for step h — 1 to H do
Take action ah J arg max。Qh(Sh, a), receive Rh(Sh, ah), and observe sh+C
Irh(Sh ah) - Irh(Sh ah) + Rh(Sh ah), Gh(Sh ah) — Vh(Sh ah) + Vh+1(Sh+1);
Nh(Sh ah) J Nh(Sh, ah) + 1, Nh(Sh ah) 一 Nh(Sh ah) + 1;
if Nh (Skh, akh) ∈ L // Reaching the end of the stage
then
J q Nh(HiaIh)ι+q Nh(Sh,a.)ι,以J δ+H △£；
Qh(Sh, ah) J min {NNh¾⅛⅛ + Nh(⅛⅛ + bh + 2b∆, QMSlh ah)};	(*)
h h, h	h h, h
Vh (Skh) J maxa Qh(Skh, a);
Nrh(Skh, akh) J 0, Irh(Skh, akh) J 0, vrh(Skh, akh) J 0;
For each triple (S, a, h) ∈ S × A × [H], we divide the visitations (within epoch 1) to the triple
into multiple Stages, where the length of the stages increases exponentially at a rate of (1 + H).
Specifically, let eι = H, and e%+ι = b(1 + H )ej, i ≥ 1 denote the lengths of the stages. Further,
let the partial sums L d=ef {Pij=1 ei | j = 1, 2, 3, . . . } denote the set of the ending times of the stages.
We remark that the stages are defined for each individual triple (S, a, h), and for different triples the
starting and ending times of their stages do not necessarily align in time.
Recall that the time index (k, h) represents the h-th step of the k-th episode. At each step (k, h), we
take the optimal action with respect to the optimistic Qh(S, a) value (Line 6 in Algorithm 1), which
is designed as an optimistic estimate of the optimal Qkh,?(S, a) value of the corresponding episode.
For each triple (S, a, h), we update the optimistic Qh (S, a) value at the end of each stage, using
samples only from this latest stage that is about to end (Line 12 in Algorithm 1). The optimism in
Qh(S, a) comes from two bonus terms bkh and b∆, where bkh is a standard Hoeffding-based optimism
that is commonly used in upper confidence bounds (Jin et al., 2018; Zhang et al., 2020), and b∆ is
the extra optimism (Cheung et al., 2020) that we need to take into account the non-stationarity of the
environment. The definition of b∆ requires knowledge of the local variation budget in each epoch,
or at least an upper bound of it. The same assumption has also been made in Ortner et al. (2019).
Fortunately, in our method, we can show (in Theorem 2) that if we simply replace Equation (*) in
Algorithm 1 with the following update rule:
Qh(Sh, ah) j min {ah)+NBl+bh, Qh(Sh, ah)}	⑴
then we can achieve the same regret bound without the assumption on the local variation budget.
We set ∣ def log (2), where δ is the failure probability.
4 Analysis
In this section, we present our main result—a dynamic regret analysis of the RestartQ-UCB algo-
rithm. Our first result on RestartQ-UCB with Hoeffding-style bonus terms is summarized in the
following theorem. The complete proofs of its supporting lemmas are given in Appendix B.
Theorem 1. (Hoeffding) For T = Ω(SA∆H2), and for any δ ∈ (0,1), with probability at least
1 一 δ, the dynamic regret of RestartQ-UCB with Hoeffding bonuses (Algorithm 1) is bounded by
111	52
O(S3 A 3 ∆ 3 H3 T3), where OG) hidespoly-logarιthmιcfactors of T and 1∕δ.
5
Under review as a conference paper at ICLR 2021
Recall that we focus our analysis on epoch 1, with episode indices ranging from 1 to K. We start
with the following technical lemma, stating that for any triple (s, a, h), the difference of their optimal
Q-values at two different episodes 1 ≤ k1<k2 ≤ K is bounded by the variation of this epoch.
Lemma 1. For any triple (s, a, h) and any 1 ≤ k1 < k2 ≤ K, it holds that |Qkh1,?(s, a) -
Qkh2,?(s,a)| ≤ ∆(r1) +H∆(p1).
We now define a few notations to facilitate the analysis. Denote by skh and akh respectively the state
and action taken at step h of episode k. Let Nk (s, a), Nk (s, a), Qh(s, a) and Vh (S) denote, respec-
tively, the values of Nh(s, a), Nh (s, a), Qh (s, a) and Vh(s) at the beginning of the k-th episode in
Algorithm 1. Further, for the triple (skh, akh, h), let nkh be the total number of episodes that this triple
has been visited prior to the current stage, and let lhk,i denote the index of the episode that this triple
was visited the i-th time among the total n£ times. Similarly, let nkh denote the number of visits to
the triple (Sh ah，h) in the stage right before the current stage, and let Ikk 分 be the i-th episode among
the nk episodes right before the current stage. For simplicity, We use Ii and li to denote Ih 分 and lhl 讨
and n to denote nh, when h and k are clear from the context. We also use rk(s, a) and Vh(s, a) to
denote the values of rlh(Skh, akh) and vlh(Skh, akh) when updating the Qh (Skh, akh) value in Line 12 of
Algorithm 1.
The following lemma states that the optimistic Q-value Qkh (S, a) is an upper bound of the optimal
Q-value Qkh,?(S, a) with high probability. Note that we only need to show that the event holds with
probability 1 - Poly(K, H)δ, because we can replace δ with δ∕poly(K, H) in the end to get the
desired high probability bound without affecting the polynomial part of the regret bound.
Lemma 2. (Hoeffding) For δ ∈ (0, 1), with probability at least 1- 2KHδ, it holds that Qkh,?(S, a) ≤
Qkh+1(S,a) ≤ Qkh(S, a), ∀(S, a, h, k) ∈ S × A × [H] × [K].
We now proceed to analyze the dynamic regret in one epoch, and at the very end of this section, we
will see how to combine the dynamic regret over all the epochs to prove Theorem 1. The following
analysis will be conditioned on the successful event of Lemma 2.
The dynamic regret of Algorithm 1 in epoch d = 1 can hence be expressed as
KK
R⑷(π, K) = X M* (Sh) - Vk,π (Sh)) ≤ X (VIh(Sh) - Vh,π (Sh)).	⑵
k=1	k=1
From the update rules of the value functions in Algorithm 1, we have
Vh(Sh) <1 [nh	=	0]H +	r'h也	ah)	+	Vh(Sh，ah)	+ bh	+ 2b∆
h (h) < L h j	Nk(Sh,ah)	Nk(Sh,ah)	b b
=Mnh = 0]H + NU + n X Vh+ ι(Sh+ι) + bh + 2b∆.
For ease of exposition, we define the following notations:
δhh =def Vhh(Shh) - Vhh,?(Shh),	ζhh d=ef Vhh(Shh) - Vhh,π(Shh).	(3)
We further define rk (Sh,ah) def	- rk (Sh，说).Then by the Hoeffding,s inequality, it holds
h h h	Nh (sh,ah)	h h h
with high probability that
1 n ▼
,ah) < n Erhi(Slh心 +
n i=1
ι- rk (Sb ah) < bh+b∆.	(4)
6
Under review as a conference paper at ICLR 2021
By the Bellman equation Vhk,π(skh) = Qkh,π(skh, π(skh)) = rhk(skh,akh) +PhkVhk+,π1(skh,akh),wehave
1 n	„
1
Zh ≤1 [nh = 0] H + n E Vh+ 1(sh+1) + bh + 2b∆ + 稼&,。勃 - PhVk+：/，ah)
i=1
In „
1
≤1 [nh	=	0]H + % EPhiVl+1 (sh,	ah)	-	PhVhQ(Sh	ah	+ 3bhh	+ 3b∆	(5)
n i=1
1 n
=l[nh = 0]H + n X (Phi
i=1
、
1 n
Vh+ι(sh, ah) + n X Ph (vh+ι - Vh+?卜sh, ah)
i=1
-------------J X-----------------------------------J
1 n
+ n X Ph (Vh+?
i=1
①
Vhk+,π1 (skh, akh) +3bkh + 3b∆,
(6)
—
^Z
②
—
^Z
③
where (5) is by the Azuma-Hoeffding inequality and by (4). In the following, we bound each term
in (6) separately. First, by Holder,s inequality, We have
1 n
① ≤ R∆p1)(H -h) ≤ b∆.
i=1
(7)
Let ej denote a standard basis vector of proper dimensions that has a 1 at the j -th entry and 0s at the
others, in the form of (0, . . . , 0, 1, 0, . . . , 0). Recall the definition of δhk in (3), and We have
n
n
n
1n	1
②=n χ δhi+ι+⅛
i=1
i=1
、
1
ah) = n Σδli+1 + ξk+ι.	(8)
i=1
J
Finally, recalling the definition of ζhk in (3), We have that
In	In
③=n X (Vh+I(sh+1) - *(sh+ι)) + n X (p" esh+1)(Vh+?
i=1
i=1
、
^z^^~
φkh+1
i=1
(9)
Where inequality (9) is by Lemma 1. Combining (6), (7), (8), and (9) leads to
1
Zh ≤ l[nh = 0] H + n E δh+ι + ξh+ι + Zk+1 - δh+ι + φ" + 研 + 5b∆.
i=1
(10)
To find an upper bound of PkK=1 Zhk, We proceed to upper bound each term on the RHS of (10) sep-
arately. First, notice that PK=I 1 [nh = 0] ≤ SAH, because each fixed triple (s, a, h) contributes
at most 1 to PK=II [nh = 0]. The second term in (10) can be upper bounded by the following
lemma:
Lemma 3. PK=ι nk Pn=ι δ^ ≤ (1 + H) £屋 δ^.
n
—
—
J
—
7
Under review as a conference paper at ICLR 2021
Combining (10) and Lemma 3, we now have that
K	1K	K
X Zk ≤sah 2 + H X δk+ι + X @+i + ζh+ι+Φh+ι+球 + 5b∆)
k=1	k=1	k=1
KK
≤SAH2 + (1 + H) X Zh+1 + X(ξk-
k=1	k = 1 X 
+ 1 + φh+1 + 3bh + 5b∆),	(II)
^^{^"
Λkh+1
where in (11) we have used the fact that δhk+1 ≤ ζhk+1, which in turn is due to the optimality that
Vhk,?(skh) ≥ Vhk,π(skh). Notice that we have ζhk on the LHS of (11) and ζhk+1 on the RHS. By
iterating (11) over h = H, H - 1, . . . , 1, we conclude that
K	HK	1
X ζ1k ≤ O SAH3 + XX(1 + ⅛)h-1Λh+ι ) .	(12)
WeboUnd PH=I PK=ι(1 + H尸A" in the proposition below. Its proof relies on a series of
lemmas in Appendix B that Upper boUnd each term in Akh+1 separately.
Proposition 1. With probability at least 1 - (KH + 2)δ, it holds that
HKI	,	、
X X(1 + -i)h-1Ah+1 ≤ O NSAKH5 + KH△声 + KH4。).
h=1 k=1
Now we are ready to prove Theorem 1.
Proof. (of Theorem 1) By (2) and (12), and by replacing δ with KH+2 in Proposition 1, We know
that the dynamic regret in epoch d = 1 can be Upper boUnded with probability at least 1 - δ by:
R(d)(π, K) ≤ Oe ZAH3 + √SAKH5 + KH△[I) + KH2∆∖I)),
and this holds for every epoch d ∈ [D]. Suppose T = Ω(SA∆H2); summing UP the dynamic
regret over all the D epochs gives US an upper bound of O(D√SAKH5 + PD=I KH∆Jrd) +
PD=I KH2∆Pd)). Recall the definition that PD=I ∆d ≤ ∆, PD=I ∆pd ≤ △), △ = ∆ + △),
and that K = Θ(DTH). By setting D = S- 1 A- 1 △ 2H-2T 1, the dynamic regret over the entire T
1,1.1	5	2
steps is bounded by R(∏, M) ≤ O(S 1 A 1 △ 1 H 1 T 1), which completes the proof.	□
Algorithm 1 relies on the assumption that the local budgets b∆ are known a priori, which hardly
holds in practice. In the following theorem, we will show that this assumption can be safely removed
without affecting the regret bound. The only modification to the algorithm is to replace the Q-value
update rule in Equation (*) of Algorithm 1 with the new update rule in Equation (1).
Theorem 2. (Hoeffding, no local budgets) For T = Ω(SA^H2), and for any δ ∈ (0,1), with
probability at least 1 - δ, the dynamic regret of RestartQ-UCB with Hoeffding bonuses and no
knowledge oflocal budgets is bounded by O(S 1 A 1 △ 1 H 5 T 2), where O(∙) hides poly-logarithmic
factors of T and 1∕δ.
To understand why this simple modification works, notice that in (*) we are adding exactly the same
value 2b∆ to the upper confidence bounds of all (s, a) pairs in the same epoch. Subtracting the
same value from all optimistic Q-values simultaneously should not change the choice of actions in
future steps. The only difference is that the new “optimistic” Qkh (s, a) values would no longer be
strict upper bounds of the optimal Qkh,?(s, a) anymore, but only an “upper bound” subject to some
error term of the order b∆ . This further requires a slightly different analysis on how this error term
propagates over time, which is presented as a variant of Lemma 2 as follows.
8
Under review as a conference paper at ICLR 2021
Lemma 4. (Hoeffding, no local budgets) Suppose we have no knowledge of the local variation
budgets and replace the update rule (*) in Algorithm 1 with Equation (1). For δ ∈ (0,1), with
probability at least 1 - 2KHδ, it holds that Qkh,? (s, a) - 2(H - h + 1)b∆ ≤ Qkh+1 (s, a) ≤
Qkh(s, a), ∀(s, a, h, k) ∈ S × A × [H] × [K].
Remark 1. The easy removal of the local budget assumption is non-trivial in the design of the
algorithm, and does not exist in the non-stationary RL literature with restarts. In fact, it has been
shown in a concurrent work (Zhou et al., 2020) that removing this assumption would lead to a much
worse regret bound (cf. Corollary 2 and Corollary 3 therein).
Replacing the Hoeffding-based upper confidence bound with a Freedman-style one will lead to a
tighter regret bound, summarized in Theorem 3 below. The proof of the theorem follows a simi-
lar procedure as in the proof of Theorem 1, and is given in Appendix D. It relies on a reference-
advantage decomposition technique for variance reduction as coined in Zhang et al. (2020). The in-
tuition is to first learn a reference value function Vref that serves as a roughly accurate estimate of the
optimal value function V?. The goal of learning the optimal value function V? = Vref + (V* - Vref)
can hence be decomposed into estimating two terms Vref and V * - Vref, each of which can be ac-
curately estimated due to the reduced variance. For ease of exposition, we proceed again with the
assumption that the local variation budgets are known. The reader should bear in mind that this
assumption can be easily removed using a similar technique as in Theorem 2.
Theorem 3. (Freedman) For T greater than some polynomial of S, A, ∆ and H, and for any δ ∈
(0, 1), with probability at least 1 - δ, the dynamic regret of RestartQ-UCB with Freedman bonuses
(Algorithm 2) is bounded by O(S 3 A 3 ∆ 3 HT 2), where O(∙) hides poly-logarithmic factors.
5 Lower Bounds
In this section, we provide information-theoretical lower bounds of the dynamic regret to character-
ize the best achievable performance of any algorithm for solving non-stationary MDPs.
Theorem 4. For any algorithm, there exists an episodic non-stationary MDP such that the dynamic
1	1	1	22
regret of the algorithm is at least Ω(S 3 A 3 ∆ 3 H 3 T 3).
Proof sketch. The proof of our lower bound relies on the construction of a “hard instance” of non-
stationary MDPs. The instance we construct is essentially a switching-MDP: an MDP with piece-
wise constant dynamics on each segment of the horizon, and its dynamics experience an abrupt
change at the beginning of each new segment. More specifically, we divide the horizon T into
L segments1, where each segment has To = [T_| steps and contains Mo = [M_| episodes, each
episode having a length of H. Within each such segment, the system dynamics of the MDP do
not vary, and we construct the dynamics for each segment in a way such that the instance is a hard
instance of stationary MDPs on its own. The MDP within each segment is essentially similar to the
hard instances constructed in stationary RL problems (Osband & Van Roy, 2016; Jin et al., 2018).
Between two consecutive segments, the dynamics of the MDP change abruptly, and we let the dy-
namics vary in a way such that no information learned from previous interactions with the MDP can
be used in the new segment. In this sense, the agent needs to learn a new hard stationary MDP in
each segment. Finally, optimizing the value of L and the variation magnitude between consecutive
segments (subject to the constraints of the total variation budget) leads to our lower bound. □
A useful side result of our proof is the following lower bound for non-stationary RL in the un-
discounted setting, which is the same setting as studied in Cheung et al. (2020), Gajane et al. (2018)
and Ortner et al. (2019).
Proposition 2. Consider a reinforcement learning problem in un-discounted non-stationary MDPs
with horizon length T, total variation budget ∆, and maximum MDP diameter D (Cheung et al.,
2020). For any learning algorithm, there exists a non-stationary MDP such that the dynamic regret
of the algorithm is at least Ω(S 3 A 3 ∆ 3 D 2 T 3).
1The definition of segments is irrelevant to, and should not be confused with, the notion of epochs we
previously defined.
9
Under review as a conference paper at ICLR 2021
References
David Abel, Yuu Jinnai, Sophie Yue Guo, George Konidaris, and Michael Littman. Policy and value
transfer in lifelong reinforcement learning. In International Conference on Machine Learning,
pp. 20-29, 2018.
Shipra Agrawal and Randy Jia. Learning in structured MDPs with convex cost functions: Improved
regret bounds for inventory management. In ACM Conference on Economics and Computation,
pp. 743-744, 2019.
Robin Allesiardo, Raphael Feraud, and Odalric-Ambrym Maillard. The non-stationary stochastic
multi-armed bandit problem. International Journal of Data Science and Analytics, 3(4):267-283,
2017.
Raman Arora, Ofer Dekel, and Ambuj Tewari. Deterministic MDPs with adversarial rewards and
bandit feedback. In Conference on Uncertainty in Artificial Intelligence, pp. 93-101, 2012.
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multi-
armed bandit problem. SIAM Journal on Computing, 32(1):48-77, 2002.
Peter Auer, Pratik Gajane, and Ronald Ortner. Adaptively tracking the best bandit arm with an
unknown number of distribution changes. In Conference on Learning Theory, pp. 138-158, 2019.
Mohammad Gheshlaghi Azar, Ian Osband, and Remi Munos. Minimax regret bounds for reinforce-
ment learning. In International Conference on Machine Learning, pp. 263-272, 2017.
Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient q-learning with low
switching cost. In Advances in Neural Information Processing Systems, pp. 8004-8013, 2019.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-
stationary rewards. In Advances in Neural Information Processing Systems, pp. 199-207, 2014.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Optimal exploration-exploitation in a multi-armed
bandit problem with non-stationary rewards. Stochastic Systems, 9(4):319-337, 2019.
Simon Board. Durable-goods monopoly with varying demand. The Review of Economic Studies, 75
(2):391-413, 2008.
Michael Bowling and Manuela Veloso. Rational and convergent learning in stochastic games. In
International Joint Conference on Artificial Intelligence, volume 17, pp. 1021-1026. Lawrence
Erlbaum Associates Ltd, 2001.
Emma Brunskill and Lihong Li. Sample complexity of multi-task reinforcement learning. In Un-
certainty in Artificial Intelligence, pp. 122, 2013.
Lucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent rein-
forcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications
and Reviews), 38(2):156-172, 2008.
Han Cai, Kan Ren, Weinan Zhang, Kleanthis Malialis, Jun Wang, Yong Yu, and Defeng Guo. Real-
time bidding by reinforcement learning in display advertising. In International Conference on
Web Search and Data Mining, pp. 661-670, 2017.
Shuchi Chawla, Nikhil R Devanur, Anna R Karlin, and Balasubramanian Sivan. Simple pricing
schemes for consumers with evolving values. In ACM-SIAM Symposium on Discrete Algorithms,
pp. 1476-1490, 2016.
Chacha Chen, Hua Wei, Nan Xu, Guanjie Zheng, Ming Yang, Yuanhao Xiong, Kai Xu, and Zhenhui
Li. Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic
signal control. In AAAI Conference on Artificial Intelligence, pp. 3414-3421, 2020.
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei. A new algorithm for non-stationary
contextual bandits: Efficient, optimal, and parameter-free. arXiv preprint arXiv:1902.00980,
2019.
10
Under review as a conference paper at ICLR 2021
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Hedging the drift: Learning to optimize
under non-stationarity. arXiv preprint arXiv:1903.01461, 2019a.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Learning to optimize under non-
stationarity. In International Conference on Artificial Intelligence and Statistics, pp. 1079-1087,
2019b.
Wang Chi Cheung, David Simchi-Levi, and Ruihao Zhu. Reinforcement learning for non-stationary
Markov decision processes: The blessing of (more) optimism. arXiv preprint arXiv:2006.14389,
2020.
Trevor Davis, Neil Burch, and Michael Bowling. Using response functions to measure strategy
strength. In AAAI Conference on Artificial Intelligence, 2014.
Travis Dick, Andras Gyorgy, and Csaba Szepesvari. Online learning in Markov decision processes
with changing cost sequences. In International Conference on Machine Learning, pp. 512-520,
2014.
Omar DarWiche Domingues, Pierre Menard, Matteo Pirotta, Emilie Kaufmann, and Michal Valko. A
kernel-based approach to non-stationary reinforcement learning in metric spaces. arXiv preprint
arXiv:2007.05078, 2020.
Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie. Dynamic regret of policy optimization
in non-stationary environments. arXiv preprint arXiv:2007.00148, 2020.
David A Freedman. On tail probabilities for martingales. The Annals of Probability, pp. 100-118,
1975.
Pratik Gajane, Ronald Ortner, and Peter Auer. A sliding-WindoW algorithm for Markov decision
processes With arbitrarily changing reWards and transitions. arXiv preprint arXiv:1805.10066,
2018.
Aurelien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit
problems. In International Conference on Algorithmic Learning Theory, pp. 174-188, 2011.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11:1563-1600, 2010.
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael I Jordan. Is Q-learning provably effi-
cient? In Advances in Neural Information Processing Systems, pp. 4863-4873, 2018.
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu. Learning adversarial MDPs
with bandit feedback and unknown transition. arXiv preprint arXiv:1912.01192, 2019.
Christos Kaplanis, Murray Shanahan, and Claudia Clopath. Continual reinforcement learning with
complex synapses. In International Conference on Machine Learning, pp. 2497-2506, 2018.
Zohar S Karnin and Oren Anava. Multi-armed bandits: Competing with optimal sequences. In
Advances in Neural Information Processing Systems, pp. 199-207, 2016.
N Bora Keskin and Assaf Zeevi. Chasing demand: Learning and earning in a changing environment.
Mathematics of Operations Research, 42(2):277-307, 2017.
Chung-Wei Lee, Haipeng Luo, Chen-Yu Wei, and Mengxiao Zhang. Linear last-iterate convergence
for matrix games and stochastic games. arXiv preprint arXiv:2006.09517, 2020.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
International Conference on Machine Learning, pp. 157-163. 1994.
Junwei Lu, Chaoqi Yang, Xiaofeng Gao, Liubin Wang, Changcheng Li, and Guihai Chen. Rein-
forcement learning with sequential information clustering in real-time bidding. In International
Conference on Information and Knowledge Management, pp. 1633-1641, 2019.
Haipeng Luo, Chen-Yu Wei, Alekh Agarwal, and John Langford. Efficient contextual bandits in
non-stationary worlds. In Conference On Learning Theory, pp. 1739-1776, 2018.
11
Under review as a conference paper at ICLR 2021
Thodoris Lykouris, Max Simchowitz, Aleksandrs Slivkins, and Wen Sun. Corruption robust explo-
ration in episodic reinforcement learning. arXiv preprint arXiv:1911.08689, 2019.
Will Ma. Improvements and generalizations of stochastic knapsack and Markovian bandits approx-
imation algorithms. Mathematics ofOperations Research, 43(3):789-812, 2018.
Weichao Mao, Kaiqing Zhang, Qiaomin Xie, and Tamer Bayar. POLY-HOOT: Monte-Carlo plan-
ning in continuous space MDPs with non-asymptotic analysis. arXiv preprint arXiv:2006.04672,
2020.
GergeIy Neu, Andras Antos, Andras Gyorgy, and Csaba Szepesvari. Online Markov decision pro-
cesses under bandit feedback. In Advances in Neural Information Processing Systems, pp. 1804-
1812, 2010.
Ronald Ortner, Pratik Gajane, and Peter Auer. Variational regret bounds for reinforcement learning.
In Uncertainty in Artificial Intelligence, pp. 81-90, 2019.
Ian Osband and Benjamin Van Roy. On lower bounds for regret in reinforcement learning. arXiv
preprint arXiv:1608.02732, 2016.
Sindhu Padakandla. A survey of reinforcement learning algorithms for dynamically varying envi-
ronments. arXiv preprint arXiv:2005.10619, 2020.
Susan M Shortreed, Eric Laber, Daniel J Lizotte, T Scott Stroup, Joelle Pineau, and Susan A Mur-
phy. Informing sequential clinical decision-making through reinforcement learning: An empirical
study. Machine learning, 84(1-2):109-136, 2011.
Yanchao Sun, Xiangyu Yin, and Furong Huang. Temple: Learning template of transitions for sample
efficient multi-task RL. arXiv preprint arXiv:2002.06659, 2020.
Cem Tekin and Mingyan Liu. Online algorithms for the multi-armed bandit problem with Marko-
vian rewards. In 48th Annual Allerton Conference on Communication, Control, and Computing
(Allerton), pp. 1675-1682, 2010.
Andrea Tirinzoni, Riccardo Poiani, and Marcello Restelli. Sequential transfer in reinforcement
learning with a generative model. arXiv preprint arXiv:2007.00722, 2020.
Jingkang Wang, Yang Liu, and Bo Li. Reinforcement learning with perturbed rewards. arXiv
preprint arXiv:1810.01032, 2018.
Yasin Abbasi Yadkori, Peter L Bartlett, Varun Kanade, Yevgeny Seldin, and Csaba Szepesvari.
Online learning in Markov decision processes with adversarially chosen transition probability
distributions. In Advances in Neural Information Processing Systems, pp. 2508-2516, 2013.
Jia Yuan Yu and Shie Mannor. Online learning in Markov decision processes with arbitrarily chang-
ing rewards and transitions. In International Conference on Game Theory for Networks, pp.
314-322. IEEE, 2009.
Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via
reference-advantage decomposition. arXiv preprint arXiv:2004.10019, 2020.
Peng Zhao, Lijun Zhang, Yuan Jiang, and Zhi-Hua Zhou. A simple approach for non-stationary
linear bandits. In International Conference on Artificial Intelligence and Statistics, volume 2020,
2020.
Huozhi Zhou, Jinglin Chen, Lav R Varshney, and Ashish Jagmohan. Nonstationary reinforcement
learning with linear function approximation. arXiv preprint arXiv:2010.04244, 2020.
12
Under review as a conference paper at ICLR 2021
A	Applications to Sequential Transfer, Multi-Task, and
Multi-Agent RL
One area that could benefit from non-stationary RL is sequential transfer in RL (Tirinzoni et al.,
2020) or multi-task RL (Brunskill & Li, 2013), which itself is conceptually related to contin-
ual RL (Kaplanis et al., 2018) and life-long RL (Abel et al., 2018). In the setting of sequential
transfer/multi-task RL, the agent encounters a sequence of tasks over time with different system
dynamics, and seeks to bootstrap learning by transferring knowledge from previously-solved tasks.
Typical solutions in this area (Brunskill & Li, 2013; Tirinzoni et al., 2020; Sun et al., 2020) need
to assume that there are finitely many candidate tasks, and every task should be sufficiently different
from the others2. Only under this assumption can the agent quickly identify the current task it is
operating on, by essentially comparing the system dynamics it observes with the dynamics it has
memorized for each candidate task. After identifying the current task with high confidence, the
agent then invokes the policy that it learned through previous interactions with this specific task.
This transfer learning paradigm in turn causes another problem—it “cold switches” between poli-
cies that are most likely very different, which might lead to unstable and inconsistent behaviors of
the agent over time. Fortunately, non-stationary RL can help alleviate both the finite-task assump-
tion and the cold-switching problem. First, non-stationary RL algorithms do not need the candidate
tasks to be sufficiently different in order to correctly identify each of them, because the algorithm
itself can tolerate some variations in the task environment. There will also be no need to assume the
finiteness of the candidate task set anymore, and the candidate tasks can be drawn from a continuous
space. Second, since we are running the same non-stationary RL algorithm for a series of tasks,
it improves its policy gradually over time, instead of cold-switching to a completely independent
policy for each task. This could largely help with the unstable behavior issues.
Multi-agent reinforcement learning (MARL) (Littman, 1994) studies the problem where a set of
agents collaborate or compete in a shared environment. In MARL, since the transition and re-
ward functions of the agents are coupled, the environment is non-stationary from each agent’s own
perspective, especially when the agents learn and update their policies simultaneously. The non-
stationarity in MARL is a setting where non-stationary RL can play a role. As advocated earlier in
Bowling & Veloso (2001); Busoniu et al. (2008), a good MARL algorithm should be both rational
and convergent, where the former means that the algorithm converges to its opponent’s best response
if its opponent converges to a stationary policy, and the latter means that if all agents use the same
algorithm, the algorithm converges to a stationary policy. As such, a non-stationary RL algorithm
can be viewed as a rational MARL algorithm, thanks to its dynamic regret guarantees, although
its convergence property in MARL settings is still worth further investigation. In fact, developing
algorithms that are both rational and convergent in general MARL settings is still relatively open. In
addition, non-stationary RL algorithms also apply to the MARL setting to achieve low regret against
slowly-changing opponents (see (Lee et al., 2020, Sec. 5.2) for the setting) but we consider a more
challenging measure of dynamic regret (as opposed to the static regret in Lee et al. (2020)). Finally,
dynamic regret is also pertinent to the notion of exploitability of strategies in two-player zero-sum
games (Davis et al., 2014).
B Proofs of the Technical Lemmas
B.1 Proof of Lemma 1
Proof. In fact, in the following, we will prove a stronger statement: Qkh1,?(s, a) - Qkh2,?(s, a) ≤
PhH0=h ∆(r1,h)0 +H PhH0=h ∆(p1,h) 0, which implies the statement of the lemma because PhH0=h ∆(r1,h)0 ≤
∆(r1) and PhH0=h ∆(p1,h) 0 ≤ ∆(p1) by definition. Our proof relies on backward induction on h. First,
2Needless to say, this assumption itself also to some extent contradicts the primary motivation of transfer
learning. After all, we only want to transfer knowledge among tasks that are essentially similar to each other.
13
Under review as a conference paper at ICLR 2021
the statement holds for h = H because for any (s, a), by definition
k2 -1
QkH1,?(s,a) - QkH2,?(s,a) = rHk1 (s, a) - rHk2 (s, a) ≤ X rHk+1(s, a) - rHk (s, a)
k=k1
K-1
≤ X rHk+1(s, a) - rHk (s, a) ≤ ∆(r1,H) ,	(13)
k=1
where we have used the triangle inequality. Now suppose the statement holds for h + 1; by the
Bellman optimality equation,
Qkh1,?(s,a)-Qkh2,?(s,a)
=Phk1Vhk+1,1?(s,a)-Phk2Vhk+2,1?(s,a)+rhk1(s,a)-rhk2(s,a)
≤PfH+1(s, a) - PhVhk+1?(s, a) + ∆⅛)	(14)
=X Phk1 (s0 | s,a)Vh+l?⑺-X Phk (s0 | s,a)**(s0) + △*
s0∈S	s0∈S
=X (PkI(S0 I s,a)Qh+1(s0,nk+?(s0)) - Pk (s0 | s,a)Qh+i(s0,∏k+?(『)))+△*,	(15)
s0∈S
where inequality (14) holds due to a similar reasoning as in (13), and in (15) πk1,? and πk2,? denote
the optimal policy in episode k1 and k2, respectively. Then by our induction hypothesis on h + 1,
for any s0 ∈ S,
HH
Qh+i(s0,∏M(s0)) ≤Qh+i(s0,∏k+i(s0))+ X A% + H X Hho
h0=h+1	h0=h+1
HH
≤Qh+i(s0,∏k+1(s0))+ X A% + H X Hh0,	(16)
h0=h+1	h0=h+1
where inequality (16) is due to the optimality of the policy πk2,? in episode k2 over πk1,?. Then,
Qkh1,?(s,a) -Qkh2,?(s,a)
HH
≤ X (Pf1(s0I s,a) - Phk2 (s' I s,a))Qh+?(s0,nh+?(s0))+ X △* + H X *,h，
s0∈S	h0=h	h0=h+1
HH
≤M(∙∣s,a)-Ph (∙∣s,a)∣∣ι∣∣Qh+1 (∙,∏h+1 (∙))L+ X A%+ H X △*，	(17)
∞ h0=h	h0=h+1
HH
≤∆1h (H -h)+ X A%+ H X △* 0	(18)
h0=h	h0=h+1
HH
≤ X △(r1,h)0 + H X △(p1,h)0,
h0=h	h0=h
where (17) is by Holder,s inequality, and (18) is by the definition of △)? and by the definition of
optimal Q-values that Qkh2+,?1(s, a) ≤ H - h, ∀(s, a) ∈ S × A. Repeating a similar process gives us
Qh2,*(s, a) - Qh1,?(s, a) ≤ PH=h A% + H PH=h 41h ,.This COmPletes OUrProof.	口
B.2 Proof of Lemma 2
Proof. It should be Clear from the way we uPdate Qh(s, a) that Qkh(s, a) is monotoniCally deCreasing
in k. We now Prove Qkh,?(s, a) ≤ Qkh+1(s, a) for all s, a, h, k by induCtion on k. First, it holds for
k = 1 by our initialization of Qh (s, a). For k ≥ 2, now suPPose Qjh,?(s, a) ≤ Qjh+1 (s, a) ≤
14
Under review as a conference paper at ICLR 2021
Qjh (s, a) for all s, a, h and 1 ≤ j ≤ k. For a fixed triple (s, a, h), we consider the following two
cases.
Case 1:	Qh(s, a) is updated in episode k. Then with probability at least 1 - 2δ
Qk+i(s, a) = TSM + VhGa) + bh + 2b∆ Qh ( , ) NkGa) Nh(S,a) h	b		
≥ Y + nX Vh+? (shi+ι)+产 n=1	∣ + rn + 2b∆	(19)
n ≥「n X收3+r n=1	+ 2b∆	(20)
n =-+ n X(Qhi，?(s，a)-rhi(s,a n=1	0)+rn^ +2以	(21)
≥Qh,*(s,a) + b∆.		(22)
_ 一. ................................ 一 .一 „r	'i.	、 一兀上，二 、一	，
Inequality (19) is by the induction hypothesis that Qlhi+1(slhi+1, a) ≥ Qlhi+,?1(slhi+1, a), ∀a ∈ A,
一一	一.二，二、	^j-^,τ. .	一. _____ 一一 一 _	__	___ .
and hence Vhli+1(slhi+1) ≥ Vhli+,?1 (slhi+1). Inequality (20) follows from the Azuma-Hoeffding in-
equality. (21) uses the Bellman optimality equation. Inequality (22) is by the Hoeffding’s in-
equality that n (Pn=I rh(s,a) - rh(s,a)) ≤ PI With high probability, and by Lemma 1
that Qlhi,?(s, a) + b∆ ≥ Qkh,?(s, a). According to the monotonicity of Qkh (s, a), we know that
Qkh,?(s, a) ≤ Qkh+1 (s, a) ≤ Qkh (s, a). In fact, We have proved the stronger statement Qkh+1 (s, a) ≥
Qkh,?(s, a) + b∆ that Will be useful in Case 2 beloW.
Case 2:	Qh (s, a) is not updated in episode k. Then there are tWo possibilities:
1.	If Qh (s, a) has never been updated from episode 1 to episode k: It is easy to see that
Qh+1(s, a) = Qh(s, a)=…=Qh(s, a) = H - h + 1 ≥ Q^(s, a) holds.
2.	If Qh (s, a) has been updated at least once from episode 1 to episode k: Let j be the
index of the latest episode that Qh (s, a) Was updated. Then, from our induction hypothesis
and Case 1, We knoW that Qjh+1 (s, a) ≥ Qjh,?(s, a) + b∆. Since Qh (s, a) has not been
updated from episode j + 1 to episode k, We know that Qh+1(s, a) = Qh(S,a) = .… =
Qjh+1 (s, a) ≥ Qjh,?(s, a) + b∆ ≥ Qkh,?(s, a), Where the last inequality holds because of
Lemma 1.
A union bound over all time steps completes our proof.
□
B.3 Proof of Lemma 3
Proof. It holds that
KAnkyk	KKl	nh	K	KInh
X⅛Xδhk+iι = XXɪδj+1X 1 [% = j] = Xδj+1X:X 1 [% = j]. (23)
k=1	h n=1	k=1 j=1	h	n=1	j=1	k=1	h n=1
For a fixed episode j, notice that Pn=I ɪ[ŋk n = j] ≤ 1, and that Pn=I ɪ[ŋk n = j] = 1 happens
if and only if (skh, akh) = (sjh, ajh) and (j, h) lies in the previous stage of (k, h) With respect to the
triple (Sh ah, h). Let K =ef {k ∈ [K] : Pn=I 1[* = j] = 1}; then, We knoW that every element
k ∈ K has the same value of nh, i.e., there exists an integer Nj > 0, such that nh = Nj, ∀k ∈ K.
Further, by our definition of the stages, we know that |K| ≤ (1 + H)Nj, because the current stage
15
9T
1=“【=豆
+ 木HX +，田X八)0 5	舱Lq(f + τ)ZZ
X H
iυψ SPIolI ??'? 一 [ isvəj m ^ιjιqυqojd 以丛 ∙9 Kiuuiaq
□	∙joojd Qqj səjəɪdmoɔ Nq PUB & JoJ spunoq Qqj Suτuτqmo□
/ % ʌ	τ<c uis	∖	I=寸 ι=q
mv"书(ISmRRzʌBJ O =标L〃(f + I)RR
5Λ∏q əM tAjTjBnbQUT ZllBMqɔs Mqone。叫 1 Kq 的削阳
⅛jojajaqι	> τ-r(f + T)> ⅛P8 aM*(υ⅛)m=LH
moj∏ 7⅛(f + T^ = [% =的q SqNeS) =(M；s)] Tt qK = (BS)CnI > < > ɪ
joj MoID[ əm 'ə配IS snoɪʌəjd Qqj UBqI jə^uoɪ səunj (ɪ + ɪ) st q⅛js qo∏5 əɔuɪs / sb pəjouəp
iC jo punoq jəddn UB PUU mou sn jəɪ ζ(υ ⅛) pəXU b joj y = (υ is)aι "'飞 网 1 MOlD[ UQqj əM
∙(<⅛⅛)mτ<-f^ = (p⅛)mpuf? *[⅛ = (a/s)/'(") =(a'；s)] ɪ = (<⅛⅛)∞3J3qM
出 ʌ	τ<.f R飞
，孝(SS)吗—〃(与+ ι)RRzΛfm=
I=寸 % ʌ	T<.f d's
氏=(工叫S)尔，(*)=(:叫S)] 口21y[_〃(3+I)RR少g=
X _J L
(M为/ʌ /、弓一 (L、弓
7 "h piτ-√γ + ɪ)ʌ > ^≡τ-√γ + ɪ)ʌ
'[ < 田 əɔuɪs ζ⅛] ə q poxg bjoj -ɪ < f |"?a(f +1)]=τ+⅛ PUB
H=" JBq1 uopraɪjəp mo ∏egX 彳(SV田X +( jVHX)θ > ((EVH +(T)V)O IwK WW
> v⅛gτ-^(f + IymM胃M W卬 ∞s OJ Xseo ST 其'Wq JO uopraɪjəp。卬 Aq 4sj虹'foθ-tJ
【="【=豆
,((3V汨X + SNmHXVS2。> (v⅛S + &£)L〃(牛+[)RR
X H
iυψ ∂^υrq 1 ^ιjιqυqojd W如 ,$ Kiuuiaq
snmməɪ JO sənəs B uτ KHiBiBtfos 弋V uIUU旭 qo∏o punoq ∏tm əm 4Sutmo∏oj Qqj Ul
T NoilISOdOHd HO JOOHJ i? H
□ 'τ⅛τ^Z(f + τ) > VJbma 芈 TmaJOJOom 0卬 S0W【duiooM) pro?(⑦ WuwquTOD
τj	T=- Hu1=寸
M)	γ+ τ>[f=4¾]ιZfZ
L	我LX
JBqj MOID[ əm iC^jəʌə joj RQpaqjj ə配IS snoɪʌəjd əgi UBql jə^uoɪ səunj (早 +1)ISOuI jb st
【COZ mɔi JB jədŋd əɔuəjəjuoɔ B SB mətaəj jəpun
Under review as a conference paper at ICLR 2021
Proof. We have that
HK
XX(1 + fh-1φh+ι
h=1 k=1
H K	n
XX(1+HkIT n Xm
h=1 k=1	i=1
H K	n
X X(1 + 万)h-1 n X (Phk - e*ι) (Vh+? -Vhk+?1 + *I-Vhk+。(sh, ah)
h=1 k=1	i=1
HK	HK
≤ XX(1 + NhT2b∆ + XX(1 + NhT (Ph - esh+J (Vh+*1 - Vh+;) (sh, ah),
h=1 k=1	h=1 k=1
where the last inequality follows from Lemma 1 and the definition of b∆ . From the proof of
Lemma 5, we know that the first term can be bounded as
HK
XX(1+春)h-12b∆ ≤ O(KH△'I) + KH2∆P1)).
h=1 k=1
Further, the second term is bounded by the Azuma-Hoeffding inequality as
H K	1
XX(1 + H尸(P/k - e*ι) (Vh+*1 - Vhk+。(sh,ah) ≤ O(√KH3l).
h=1 k=1
Combining the two terms completes the proof.	□
Lemma 7. With probability at least - (KH +)δ, it holds that
H K	ι
XX(H
+ HyTξh+ι ≤ O(√SAKH3ι + KH2∆P1)).
h=1 k=1
Proof. We have that
HK
XX(1 + H)h-1ξk+ι
h=1 k=1
HK
=xχ(i+H)
h=1 k=1
HK
=xχ(i+H)
h=1 k=1
HK 1	1 J.
≤O(KH 2 ∆1))+XX(1 + H 尸 n X (Phi
h=1 k=1	i=1
Vh+?) (sh,
(25)
-	- 一	.	- -	-	--二，7一 7一、	--7.∙X∙,7 _ 7一、_	_	-	--	-
where the last step is by the fact that Vhli+1(skh, akh) ≥ Vhli+,?1(skh, akh) from Lemma 2, and then by
Holder's inequality and the triangle inequality. The following proof is analogous to the proof of
Lemma 15 in Zhang et al. (2020). For completeness we reproduce it here. We have
HK
XX(i+H)
h=1 k=1
HKK
XXX(H+
h=1 k=1 j=1
1	1 nh
H)h-1 凝 Xl [% =j] (Ph -e*ι) (Vh+1 - Vh+1)(Sh,ah)
h i=1
HKK	ι	Inh
XXX(1 + HyT谈 Xl [% = j] (Ph -e*ι) (Vh+1 - jι) (Sh,ah),
h=1 k=1 j=1	nh i=1
(26)
17
Under review as a conference paper at ICLR 2021
where (26) holds because 攻i(s£, ah) = j if and only if j is in the previous stage of k and (Sh ah)=
(Sh, ah). For simplicity of notations, we define θkk+ι = (1 + H)h-1 PK=I nj PnhI ɪ ∣⅛,i = k].
Then we further have (note that we have swapped the notation ofj and k)
HK
(26)=XXθhk+1 Phk - eskh+1	Vhk+1 - Vhk+,?1 (Skh,akh).
h=1 k=1
For (k, h) ∈ [K] × [H], let xkh denote the number of occurrences of the triple (Skh, akh, h) in the
current stage. Define θhk+1 =f (1 + ⅛)h-1b(I+ H)xhc ≤ 3. Define K = {(k, h) : θ%ι = θ% J,
+	H	xh	+	+
and K =f {(k, h) ∈ [K] × [H] ： θhk+ι = θk+ι}. Then, we have that
HK
(26) = XXθk+ι (Ph-esh+)W+ι- Vh+?J(Sh, ah)
h=1 k=1
HK
+ XX(θh+ι — 以ι) (Ph-esh+)①h+ι- Vk+*J (sh,ah).
h=1 k=1
Since θh+ι is independent of shh+ι, by the Azuma-Hoeffding inequality, it holds with probability at
least 1 - δ that
H K
XX 碉+1 (Phh -esh+ι)(Vh+ι-Vh+1) (sh,ah) ≤ O(√KH3∣).	(27)
h=1 h=1
It is easy to see that if k is in a stage that is before the second last stage of the triple
(Shh, ahh, h), then (k, h) ∈ K. For a triple (S, a, h), define Kh⊥(S, a) d=ef {k ∈ [K] :
k is in the second last stage of the triple (S, a, h), (Shh, ahh) = (S, a)}. We have that
HK
XX(θh+ι-碉+1) (Ph -esh+1)(Vh+1 - Vh+*J (sh,ah)
h=1 h=1
=X X l[(sh,ah) = (s,a)] (Θh+1 -以ι) (Ph-e,h+) ^山1-Vh+*J(s,a)
s,a,h h<h,h)∈K
=X (θh+ι(s,a)	-	θh+ι(s,a))	X	(Ph-esh+1)	(vh+ι	-	Vh+；)	(s,a),	(28)
s,a,h	h∈Kh⊥ (s,a)
where for a fixed triple (S, a, h), we have defined θh+1(S, a) =def θhh+1, for any k ∈ Kh⊥(S, a).
Note that θh+1(S, a) is well-defined, because θhh1+1 = θhh2+1, ∀k1, k2 ∈ Kh⊥(S, a). Similarly, let
θh+ι(s, a) =f 京h+ι for any k ∈ K⊥(s,a), and θh+ι(s, a) is also well-defined. By the Azuma-
Hoeffding inequality and a union bound, it holds with probability at least 1 - KHδ that
(28) ≤ X O ZH2∣K⊥(s,a)∣∣)
s,a,h
=X O QH2NK+1(s,a)∣)
s,a,h
≤O (SSAH3ι Xh NK+1(s,a))	(29)
≤O (√SAKH3ι)	(30)
where NK+Xs, a) is defined to be the total number of visitations to the triple (s, a, h) over the
entire K episodes. (29) is by the Cauchy-Schwartz inequality. (30) holds because by the way stages
are defined, for each triple (S, a, h), the length of its last two stages is at most an O(1/H) fraction
of the total number of visitations.
Combining (25), (27) and (30) completes the proof.
□
18
Under review as a conference paper at ICLR 2021
B.5 Proof of Lemma 4
Proof. This proof follows a similar structure as the proof of Lemma 2. It should be clear from the
way we update Qh(s, a) that Qkh(s, a) is monotonically decreasing in k. We now prove Qkh,?(s, a) -
2(H - h + 1)b∆ ≤ Qkh+1(s, a) for all s, a, h, k by induction on k. First, it holds for k = 1 by our
initialization of Qh(s, a). For k ≥ 2, now suppose Qjh,?(s, a) - 2(H - h + 1)b∆ ≤ Qjh+1 (s, a) ≤
Qjh (s, a) for all s, a, h and 1 ≤ j ≤ k. For a fixed triple (s, a, h), we consider the following two
cases.
Case 1:	Qh(s, a) is updated in episode k. Then with probability at least 1 - 2δ
Qk+i(s,a)=半© + 十) + bh
h	Nh (s,a)	Nh (s,a)
≥『%丁)+n XVl+?(Sh+1) -2(H—可必+m~ι+rn	(31)
i=1
n	I—
≥r⅛M + 或 XPhM+ι(s, a) +Jl - 2(H - h)b∆	(32)
nn	n
i=1
n	I—
=-h÷-----+ ι X (q%?(s, a) — rh (s,a)) + ∖∕l — 2(H — h)b∆	(33)
n n i=1	n
≥QT(s, a) — b∆ - 2(H — h)b∆.	(34)
Inequality (31) is by the induction hypothesis that Qlhi+1(Slhi+1, a) ≥ Qlhi+,?1(Slhi+1, a) - 2(H -
一二，二、	r.『
h)b∆, ∀a ∈ A, and hence Vhli+1(Slhi+1) ≥ Vhl+i,?1(Slhi+1) - 2(H - h)b∆. Inequality (32) follows from
the Azuma-Hoeffding inequality. (33) uses the Bellman optimality equation. Inequality (34) is by
the Hoeffding's inequality that n (Pn=I rli (s, a) — rh(s, a)) ≤ Pn with high probability, and by
1
Lemma 1 that Qlhi,?(S, a) ≥ Qkh,?(S, a) — b∆. According to the monotonicity of Qkh(S, a), we know
that Qkh,?(S, a) — 2(H — h + 1)b∆ ≤ Qkh+1(S, a) ≤ Qkh(S, a). In fact, we have proved the stronger
statement Qkh+1(S, a) ≥ Qkh,?(S, a) — b∆ — 2(H — h)b∆ that will be useful in Case 2 below.
Case 2:	Qh(S, a) is not updated in episode k. Then there are two possibilities:
1.	If Qh(S, a) has never been updated from episode 1 to episode k: It is easy to see that
Qh+1 (s, a) = Qh(s, a) = ∙∙∙ = Qh(s, a) = H - h +1 ≥ Q”(s, a) — 2(H — h + 1)b∆
holds.
2.	If Qh(S, a) has been updated at least once from episode 1 to episode k: Let j be the index
of the latest episode that Qh(S, a) was updated. Then, from our induction hypothesis and
Case 1, we know that Qjh+1(S, a) ≥ Qjh,?(S, a) — b∆ — 2(H — h)b∆. Since Qh(S, a) has
not been updated from episode j + 1 to episode k, we know that Qkh+1(S, a) = Qkh(S, a) =
…=Qh+1(s, a) ≥ Qh,*(s, a) - b∆ - 2(H - h)b∆ ≥ Qh,*(s, a) - 2(H - h + 1)b∆,
where the last inequality holds because of Lemma 1.
A union bound over all time steps completes our proof.	□
B.6 Proof Sketch of Theorem 2
Proof sketch. We only sketch the difference with respect to the proof of Theorem 1 in the main text.
The reader should have no difficulty recovering the complete proof by following the same routine as
Theorem 1. Specifically, it suffices to investigate the steps that are involved with Lemma 2.
The dynamic regret of the new algorithm in epoch d = 1 now can be expressed as
KK
R(d)(π, K) = X (Vj* (Sk) - VJn(Sk)) ≤ X (VIk (Sk) - Vk'π (sh)) + 2KHb∆,	(35)
k=1	k=1
19
Under review as a conference paper at ICLR 2021
where we applied the results of Lemma 4 instead of Lemma 2. The reader should bear in mind that
from the new update rules of the value functions, we now have
Vk(Sh) ≤ l[nh =0]H + rh(sh, ah) + Vhh(Ssh ah) + bh,
Lh 」Nhk(S 3 ah	Nk(Sh,as)	s,
(36)
where the RHS no longer has the additional bonus term b∆. If we define ζhk, ξhk+1, and φkh+1 in the
same way as before, the author can easily verify that all the derivations until Equation (12) still holds,
although the value of Λkh+1 should be re-defined as Λkh+1 d=ef ξhk+1 + φkh+1 + 3bkh + 3b∆ due to the
new upper bound in (36) that is independent of b∆ . Proposition 1 also follows analogously though
some additional attention should be paid to the proof of Lemma 7 where the results of Lemma 2
have been utilized. Finally, we obtain the dynamic regret upper bound in epoch d= 1 as follows:
R(d)(π, K) ≤ O ZAH3 + √SAKH5 + KH△[I) + KH2∆∖I)) + 2KHb上,
where the additional term 2KHb∆ comes from (35). From our definition of b∆, we can easily see
that 2KHb∆ ≤ O(KH△9) + KH2∆P1)). Therefore, We can conclude that the dynamic regret
upper bound in one epoch remains the same order, which leaves the dynamic regret over the entire
horizon also unchanged.	□
C Algorithm: RestartQ-UCB (Freedman)
The algorithm Restarted Q-Learning With Freedman Upper Confidence Bounds (RestartQ-UCB
Freedman) is presented in Algorithm 2. For ease of exposition, we use r, μ, V, σ, μref, σref, n, and
n to denote 予h(s,a), μs(Sh,a) Vh(Sh,αh), σs(sh,ah), μh(ss,ah), σref(sh,as), Nh(Sk,as)，
and Nh(Skh, akh) respectively, when the values of (S, a, h, k) are clear from the context.
Compared with Algorithm 1, there are two major improvements in Algorithm 2. The first one is to
replace the HOeffding-based bonus term bhh with a tighter term bh The latter term takes into account
the second moment information of the random variables, which allows sharper tail bounds that rely
on second moments to come into use (in our case, the Freedman’s inequality). The second improve-
ment is a variance reduction technique, or more specifically, the reference-advantage decomposition
as coined in Zhang et al. (2020). The intuition is to first learn a reference value function V ref that
serves as a roughly accurate estimate of the optimal value function V ? . The goal of learning the
optimal value function V? = Vref + (V* - Vref) can hence be decomposed into estimating two terms
Vref and V * - Vref. The reference value Vref is a fixed term, and can be accurately estimated using
a large number of samples (in Algorithm 2, we estimate Vref only when we have cSAH6 ι samples
for a large constant c). The advantage term V* - Vref can also be accurately estimated due to the
reduced variance.
D	Proof of Theorem 3
Similar to the proof of Theorem 1, we start with the dynamic regret in one epoch, and then extend to
all epochs in the end. The proof follows the same routine as in the proof of Theorem 1. Given that
a rigorous analysis on the Freedman-based bonus with variance reduction is present in Zhang et al.
(2020), one should not find it difficult to extend our Hoeffding-based algorithm to Algorithm 2.
Therefore, rather than providing a complete proof of Theorem 3, in the following, we sketch the
differences and highlight the additional analysis needed that is not covered by the proof of Theorem 1
and Zhang et al. (2020).
To facilitate the analysis, first recall a few notations Nh, TNhk,Qkh(s, a),Vh(s), nkh Jhi, nh [ i,li
and li that we have defined in Section 4. In addition, when (h, k) is clear from the context, we drop
the time indices and simply use μ, σ, μref, σref to denote their corresponding values in the computa-
tion of the Qh(Skh, akh) value in Line 15 of Algorithm 2.
We start with the following lemma, which is an analogue of Lemma 2 but requires a more careful
treatment of variations accumulated in μref and μ%. It states that the optimistic Qh (s, a) is an upper
bound of the optimal Qkh,? (S, a) with high probability.
20
Under review as a conference paper at ICLR 2021
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
Algorithm 2: RestartQ-UCB (Freedman)
for epoch d — 1 to D do
Initialize: Vh(S) - H 一 h + 1, Qh(s, a) — H 一 h + 1, Nh(s, a) . 0, Nh(s, a) - 0,
rh(s,a) - 0,μh(s, a) - 0,Vh(s,a) - 0,σh(s,a) - 0,μhef(s,a) - 0,°/(£,0)一
0, Vref(S) - H, for all (s,a,h) ∈S×A× [H ];
for episode k — (d — 1)K + 1 to min{dK, M} do
observe S1k ;
for step h — 1 to H do
Take action ahh J arg maxa Qh(Sh a), receive Rh(Sh, ah)，and observe shh+i；
r — r+RMSih ah), v - v+Vh+ι(Sh+J;
μ J μ + Vh+1(Sh+1) — Vh+ I(Sh+1),σ J σ + (Vh+1(Sh+1) — Vh+ I(Sh+1))2;
μref J μref + Vh++1 (Sh+ι),σref J σref+ M+1 (Sh+ι))2;
n J n +1,n J n +1;
if n ∈ L // Reaching the end of the stage
then
bhJ qH ι+q1ι, b∆J &")+H Nd)；
b∣ J2q∕ σref∕n-(μref/n)2 I 2q∕ 万 ∕n-(μ√n)2 / 15/Hι I Hι I Hι3/4 ∣ Hι3/4 ʌ ∣ FT
bh J 2V-----n------ι+2V--------n----ι+5( ~n+后+~n^+~^w )+√ 无ι;
Qh(Sh, ah)J min n n+n+bh+2b∆, n+μn-+nn+2bh+4b∆,Qh(Sh,ah)};
Vh (Skh) J maxa Qh(Skh, a);
NhIsh, ah), Irh(Sh ah), Gh(Sh ah), μh(Sh ah), bh(Sh, ah)J 0;
if Pa Nh(Sh,a) = Ω(SAH 6ι)// Learn the reference value
then
Vhref(Skh) J Vh(Skh);
Lemma 8. (Freedman) For δ ∈ (0,1), with probability at least 1 —2KHδ, it holds that Qj*(S,a) ≤
Qkh+1(S,a) ≤ Qkh(S, a), ∀(S, a, h, k) ∈ S × A × [H] × [K].
Proof. It should be clear from the way we update Qh(S, a) that Qkh(S, a) is monotonically decreasing
in k. We now prove Qkh,?(S, a) ≤ Qkh+1(S, a) for all S, a, h, k by induction on k. First, it holds for
k = 1 by our initialization of Qh (S, a). For k ≥ 2, now suppose Qjh,?(S, a) ≤ Qjh(S, a) for all S, a, h
and 1 ≤ j ≤ k. For a fixed triple (S, a, h), we consider the following two cases.
Case 1:	Qh(S, a) is updated in episode k. Notice that it suffices to analyze the case where Qh(S, a)
is updated using bh, because the other case of bh would be exactly the same as in Lemma 2. With
probability at least 1 — δ,
Qk+1(S, a)= A) + fa) + *a1 + 2bh + 4b∆
h '' ' Nh (S,a) Nh (S,a) Nh(S,a)	-h δ
rrh (S, a)	1
----3------+ -
nr	n
J
i=1
}
χ1
[n
+ n X [(V⅛1(Sh+1) - Vr+,1i (Shi+1)) — (Phi Vh+1 - Phi c+1i) (S,a)]
i=1
|-------------------------------------------------------}
^z
χ2
1
+—
n
J
n	In
1
E Phi Vr+'1i + V∑ (Phi 片+1 - Phi Vr+,1i) (S, a) +2bh + 4b∆	(37)
i=1
i=1
{z
χ3
}
21
Under review as a conference paper at ICLR 2021
In the following, we will bound each term in (37) separately. First, we have that
n
X3 + 2b∆ = - X (Phi Vfi - PhVh+'1i) (s, a) + b∆
i=1
n n
-n X (Phi Vfi- Ph Vfi) (s, a) + b∆
i=1
In	In	In
+ - X PhVh+1i(s, a) - n X PhVf(s, a) + n X PhM+ι(s, a)
i=1	i=1	i=1
-n „
≥n EPhiVh+ι(s,a),
i=1
(38)
(39)
(40)
(41)
where (38)≥ 0 and (39)≥ 0 by Holder,s inequality and the definition of 5δ. In (40), We have that
1 Pi=IPh Vh+1i (s, a) - 1 Pi=IPhVhe+li(s, a) ≥ 0, because Vhf(S) is non-increasing in k.
Following a similar procedure as in Lemma 10, Lemma 12, and Lemma 13 in Zhang et al. (2020),
we can further bound ∣χι | and ∣χ21 as follows:
|X1|
|X2|
5Hι3	2√ι	2Hι
-3---+ TFT + —
n 4	T n	n
__ 3	_	__
5Hι4	2√ι	2Hι
---+-- + ~Z~- + -----
3
n 4	T n n
(42)
(43)
where νref def 吟 -(嚓) and V = ⅞ - (n)2. These are the steps where Freedman,s inequal-
ity Freedman (1975) come into use, and we omit these steps since they are essentially the same as
the derivations in Zhang et al. (2020). We can see from (42), (43), and the definition of bh that
lχιl + Iχ21≤ bh.
Substituting the results on χ1, χ2 and χ3 back to (37), it holds that with probability at least - δ,
Qh+1(s, a) = ~~n，-]+χι+χ2+χ3+2bh+4b∆
≥ rɪ + - X PhiVf+ι(s, a)+ bh + 2b∆	(44)
nn
i=1
≥迎⑷十-XPhiV⅛(s,a)+ bh + 2b∆	(45)
nn
i=1
=rɪ) + N X	(s, a) - rh (s, a)) + 琮 + 2b∆
i=1
- nh	h
≥~ Qh, Qh, (s,a) +2b∆ ≥ Qh, (s,a) + b∆,	(46)
n i=1
where in (44) we used (41), (42), (43), and the definition of bh in Algorithm 2. (45) is by the
hh	hh
induction hypothesis that Qlhi+1(Slhi+1, a) ≥ Qlhi+,?1(Slhi+1, a), ∀a ∈ A,- ≤ lVi ≤ k. The second to last
inequality holds due to the Hofdding,s inequality that f (Pn=1 r%(s, a) - rh(s, a)) ≤ pɪ ≤ bh
with high probability. Finally, the last inequality follows from Lemma 1.
According to the monotonicity of Qkh (S, a), we can conclude from (46) that Qkh,?(S, a) ≤
Qkh+1 (S, a) ≤ Qkh(S, a). In fact, we have proved the stronger statement Qkh+1 (S, a) ≥ Qkh,?(S, a) +
b∆ that will be useful in Case 2 below.
Case 2:	Qh(S, a) is not updated in episode k. Then, there are two possibilities:
22
Under review as a conference paper at ICLR 2021
1.	If Qh (s, a) has never been updated from episode 1 to episode k: It is easy to see that
Qh+1(s, a) = Qh(s, a)=…=Qh(s, a) = H - h + 1 ≥ Q^(s, a) holds.
2.	If Qh (s, a) has been updated at least once from episode 1 to episode k: Let j be the
index of the latest episode that Qh(s, a) was updated. Then, from our induction hypothesis
and Case 1, we know that Qjh+1(s, a) ≥ Qjh,?(s, a) + b∆. Since Qh(s, a) has not been
updated from episode j + 1 to episode k, We know that Qh+1(s, a) = Qh(S,a) = .… =
Qjh+1(s, a) ≥ Qjh,?(s, a) + b∆ ≥ Qkh,?(s, a), where the last inequality holds because of
Lemma 1.
A union bound over all time steps completes our proof.	□
Conditional on the successful event of Lemma 8, the dynamic regret of Algorithm 2 in epoch d = 1
can hence be expressed as
KK
R⑷(π, K) = X (VIk,* (Sk) - V" (Sk)) ≤ X (VIk(Sk) - V" (sk)) .	(47)
k=1	k=1
From the update rules of the value functions in Algorithm 2, we have
Vh(Sh) ≤ l[nh = 0]H + rh(sh,ah) + μhfk + μh +2bh + 4b∆
n	n n
=Mnh=0]H+取?确+nX Vh+ιi(Sh+ι)+nX(*1M+1)-琢+十距+/+或+4b∆.
n	n i=1	n i=1
If we again define ζhk d=ef Vhk (Skh) - Vhk,π (Skh), we can follow a similar routine as in the proof of
Theorem 1 (details can be found in Zhang et al. (2020)) and obtain
K	HK	1
X ζ1k ≤ O SAH3 + XX(1 + 1)h-1Λh+ι ,
k=1	h=1 k=1
where Λh+ι = Ψk+ι + ξh+ι + Φh+ι + 4bh + 8b∆ with the following definitions:
Ψh+1 =」X (Ph Vh+1i - Phk Vh+'K+)(Sh, ah),
nh i=1
ξh+ι =f	X (Pk- es J (Vh+1 - V⅛Wh, ah),
nh i=1	h+1
Φh+1 = (Ph - esh+1)(Vh+? - Vh+D 淄,ah).
An upper bound on the first four terms in Λkh+1 is derived in the proof of Lemma 7 in Zhang et al.
(2020) (There is an extra term of ^11l in our defnition of bh compared to theirs, but it does not
affect the leading term in the upper bound). By further recalling the definition of b∆, we can obtain
the following lemma.
Lemma 9. (Lemma 7 in Zhang et al. (2020)) With probability at least (1 - O(H2T4δ)), it holds
that
H K
XX(1+春)h-1Λh+1 = O (√SAH2Tl+H√Tllog(T)+S2A3H8T41+KH∆r1) +KH2∆p1)).
h=1 k=1
Combined with (47) and the definition of ζhk, we obtain the dynamic regret bound in a single epoch:
R(d)(π,K) = O (√SAH2Tι + H√Tllog(T) + S2A3 H8T 11 + KH△「+ KH2∆^1)) , ∀d ∈ [D].
23
Under review as a conference paper at ICLR 2021
Finally, suppose T is greater than a polynomial of S, A, ∆ and H, VSAH2Tb would be the leading
term of the dynamic regret in a single epoch. In this case, summing up the dynamic regret over all
the D epochs gives Us an upper bound of O (D√SAH2T + PD=I KH△，+ PD=I KH2∆Pd)).
Recall that PD=I ∆d ≤ △, PD=I ∆d) ≤ ∆p, △ = △ + ∆p, and that K = Θ(DH). By setting
D = S- 3 A- 1 △ 2 T1, the dynamic regret over the entire T steps is bounded by
111	2
R(∏,M) ≤ O (SιA3△ 3HT3).
This completes the proof of Theorem 3.
E	Proof of Theorem 4
The proof of our lower bound relies on the construction of a “hard instance” of non-stationary
MDPs. The instance we construct is essentially a switching-MDP: an MDP with piecewise constant
dynamics on each segment of the horizon, and its dynamics experience an abrupt change at the
beginning of each new segment. More specifically, we divide the horizon T into L segments3, where
each segment has T = [L_| steps and contains Mo = [M_| episodes, each episode having a length
of H. Within each such segment, the system dynamics of the MDP do not vary, and we construct
the dynamics for each segment in a way such that the instance is a hard instance of stationary MDPs
on its own. The MDP within each segment is essentially similar to the hard instances constructed
in stationary RL problems (Osband & Van Roy, 2016; Jin et al., 2018). Between two consecutive
segments, the dynamics of the MDP change abruptly, and we let the dynamics vary in a way such that
no information learned from previous interactions with the MDP can be used in the new segment. In
this sense, the agent needs to learn a new hard stationary MDP in each segment. Finally, optimizing
the value of L and the variation magnitude between consecutive segments (subject to the constraints
of the total variation budget) leads to our lower bound.
We start with a simplified episodic setting where the transition kernels and reward functions are held
constant within each episode, i.e., Pm = … =Ph = ... PH and r『 = … = 咪 =...r∏, ∀m ∈
[M]. This is a popular but less challenging episodic setting, and its stationary counterpart has been
studied in Azar et al. (2017). We further require that when the environment varies due to the non-
stationarity, all steps in one episode should vary simultaneously in the same way. This simplified
setting is easier to analyze, and its analysis conveniently leads to a lower bound for the un-discounted
setting as a side result along the way. Later We will show how the analysis can be naturally extended
to the more general setting we introduced in Section 2, using techniques that have also been utilized
in Jin et al. (2018). For simplicity of notations, we temporarily drop the h indices and use Pm and
rm to denote the transition kernel and reward function whenever there is no ambiguity.
Figure 1:	The “JAO MDP” constructed in Jaksch et al. (2010). Dashed lines denote transitions
related to the good action a?.
Consider a two-state MDP as depicted in Figure 1. This MDP was initially proposed in Jaksch
et al. (2010) as a hard instance of stationary MDPs, and following Jin et al. (2018) we will refer
to this construction as the “JAO MDP”. This MDP has 2 states S = {s0,s∣} and SA actions A =
{1,2,..., SA}. The reward does not depend on actions: state Sl always gives reward 1 whatever
action is taken, and state So always gives reward 0. Any action taken at state Sl takes the agent to
state SO with probability δ, and to state s∣ with probability 1 - δ. At state s。，for all but a single
3The definition of segments is irrelevant to, and should not be confused with, the notion of epochs we
previously defined.
24
Under review as a conference paper at ICLR 2021
“good” action a?, the agent is taken to state Sl with probability δ, and for the good action a?, the
agent is taken to state Sl with probability δ + ε for some 0 < ε < δ. The exact values of δ and
ε will be chosen later. Note that this is not an MDP with S states and A actions as We desire, but
the extension to an MDP with S states and A actions is routine (JakSCh et al., 2010), and is hence
omitted here.
Figure 2:	A chain with H copies of JAO MDPs correlated in time. At the end of an episode, the
state should deterministically transition from any state in the last copy to the So state in the first copy
of the chain, the arrows of which are not shown in the figure. Also, the s∣ state in the first copy is
actually never reached and hence is redundant.
To apply the JAO MDP to the simplified episodic setting, we “concatenate” H copies of exactly
the same JAO MDP into a chain as depicted in Figure 2, denoting the H steps in an episode. The
initial state of this MDP is the SO state in the first copy of the chain, and after each episode the
state is “reset” to the initial state. In the following, we first show that the constructed MDP is a
hard instance of stationary MDPs, without worrying about the evolution of the system dynamics.
The techniques that we will be using are essentially the same as in the proofs of the lower bound
in the multi-armed bandit problem (Auer et al., 2002) or the reinforcement learning problem in the
un-discounted setting (Jaksch et al., 2010).
The good action a? is chosen uniformly at random from the action space A, and we use E*[∙] to
denote the expectation with respect to the random choice of a?. We write EaH for the expectation
conditioned on action a being the good action a?. Finally, we use EUnifH to denote the expectation
when there is no good action in the MDP, i.e., every action in A takes the agent from state SO to s∣
with probability δ. Define the probability notations P*(∙), Pa(∙), and Punif(∙) analogously.
Consider running a reinforcement learning algorithm on the constructed MDP for T0 steps, where
T0 = M0H. It has been shown in Auer et al. (2002) and Jaksch et al. (2010) that it is sufficient
to consider deterministic policies. Therefore, we assume that the algorithm maps deterministically
from a sequence of observations to an action at at time t. Define the random variables N, NO and
N? to be the total number of visits to state Sl, the total number of visits to s。，and the total number
of times that a? is taken at state SO, respectively. Let St denote the state observed at time t, and
at the action taken at time t. When there is no chance of ambiguity, we sometimes also use Shm to
denote the state at step h of episode m, which should be interpreted as the state St observed at time
t = (m - 1) × H + h. The notation ahm is used analogously. Since SO is assumed to be the initial
state, we have that
T0	M0 H
Ea [N ] = X Pa(St = S)= X X Pa (Sm = SJ
t=1	m=1 h=2
M0 H
=XX (Pa(Sm-I= So) ∙ Pa (Sm = SJ Sm-I= So) + Pa(Sm-I = S) ∙ Pa (Sm = SJ Sm-I = SJ)
m=1 h=2
M0 H
=XX(δPa(Sm-ι =So,am = a?) + (δ + ε)Pa(Sm-ι = s。，amm = a?) + (1- δ)Pa(Sm-ι = S))
m=1 h=2
≤δEa[No - N?] + (δ + ε)Ea[N?] + (1 - δ)E0[N],
25
Under review as a conference paper at ICLR 2021
and rearranging the last inequality gives Us Ea [N] ≤ Ea [N0 - N?] + (1 + ε)Ea [N?].
For this proof only, define the random variable W (T0) to be the total reward of the algorithm over
the horizon T0, and define G(T0) to be the (static) regret with respect to the optimal policy. Since
for any algorithm, the probability of staying in state So under Pa (∙) is no larger than under Punif(∙),
it follows that
ε
Ea [W (To)] ≤ Ea [N ] ≤ Ea [No - N?] + Q + K )EaN?]
δ
KK
=Ea [No] + 锣a [N?] ≤ EUnif [No] + 锣a N?]
δδ
K
=To - Eunif[N] + 大Ea[No ]∙	(48)
δ
Let Tm denote the first step that the state transits from state s° to Sl in the m-th episode, then
M0	H
Eunif[N ]= XX
PUnif(Tm = h)Eunif[N | τm
m=1 h=1
h]=XM0XH(1-δ)h-1δEunif[N ] Tm = h]
m=1 h=1
〉X Xn 对h-1 万H - h X ( H	1(1 - δ)H
⅛(1 - δ) δ "^ = 7⅛U - 2δ + ^^
To	Mo
≥^2^ - ^2δ^.
(49)
Since the algorithm is a deterministic mapping from the observation sequence to an action, the
random variable No? is also a function of the observations up to time T. In addition, since the
immediate reward only depends on the current state, No? can further be considered as a function of
just the state sequence up to T. Therefore, the following lemma from Jaksch et al. (2010), which in
turn was adapted from Lemma A.1 in Auer et al. (2002), also applies in our setting.
Lemma 10. (Lemma 13 in Jaksch et al. (2010)) For any finite Constant B, let f : {so, s∣}T0+1 →
[0, B] be anyfunction defined on the state sequence S ∈ {so, s∣}T0+1. Then, for any 0 < δ ≤ 2, any
0 < K ≤ 1 - 2δ, and any a ∈ A, it holds that
Ea[f (S)] ≤ Eunif[f (S)] + ɪ ∙ √δ^ J2EUnif [N?] ∙
Since No? itself is a function from the state sequence to [0, To], we can apply Lemma 10 and arrive
at
Ea [N?] ≤ Eunif [N?] + T2o ∙√ P2Eunif [N?].	(50)
From (49), We have that PSAI EUnif [N?] = To - EUnif [N] ≤ T + M. By the Cauchy-Schwarz
inequality, we further have that PSAI p2Eunif [N?] ≤ JSA(TO + MM0). Therefore, from (50), we
obtain	___________
X Ea[N?] ≤TO+M+To ∙√δ rSA(To+ M .
a=1
ogether with (48) and (49), it holds that
SA
E*[W(To)] ≤SA XEa[W(To)]
a=1
To Mo ε 1	T T0 Mo To ε /	Mo Λ
≤τ + lδ	+ δSl[τ	+ lδ	+τ	.者VSA(TO +	丁)).	(51)
E.1 Un-discounted Setting
Let us now momentarily deviate from the episodic setting and consider the un-discounted setting
(with Mo = 1). This is the case of the JAO MDP in Figure 1 where there is not reset. We could
26
Under review as a conference paper at ICLR 2021
calculate the stationary distribution and find that the optimal average reward for the JAO MDP is
2δ+εε. It is also easy to calculate that the diameter of the JAO MDP is D = δ. Therefore, the
expected (static) regret with respect to the randomness of a* can be lower bounded by
E*[G(To)]=2⅞εεT0- E?[W(To)]
、εTo	D	εD(To + D) ε2ToD√D. /— ι
≥4δ+iε -工 2SA 2√丁(MT0 +
By assuming To ≥ DSA (which in turn suggests D ≤ y TD) and setting ε = CJTlD for C = 40,
we further have that
E? [G(To)] ≥
CD
2SATo
≥ 3 -3- C - C2 - 1— ) √SAToD = -ɪ √SAToD.
一120	200 v 0	1600 V 0
It is easy to verify that our choice of δ and ε satisfies our assumption that 0 < ε < δ. So far, We
have recovered the (static) regret lower bound of Ω(√SATqD) in the un-discounted setting, which
was originally proved in Jaksch et al. (2010).
Based on this result, let us now incorporate the non-stationarity of the MDP and derive a lower
bound for the dynamic regret R(T). Recall that we are constructing the non-stationary environment
as a SWitChing-MDp For each segment of length Tq, the environment is held constant, and the
regret lower bound for each segment is Ω(√SATqD). At the beginning of each new segment,
we uniformly sample a new action a* at random from the action space A to be the good action
for the new segment. In this case, the learning algorithm cannot use the information it learned
during its previous interactions with the environment, even if it knows the switching structure of the
environment. Therefore, the algorithm needs to learn a new (static) MDP in each segment, which
leads to a dynamic regret lower bound of Ω(L√SAT0D) = Ω(^SATLD), where let us recall
that L is the number of segments. Every time the good action a* varies, it will cause a variation
of magnitude 2ε in the transition kernel. The constraint of the overall variation budget requires that
2εL = 2200 JTSD L ≤ ∆, which in turn requires L ≤ 4∆ 3 T3 D 1 S- 3 A- 1. Finally, by assigning
the largest possible value to L subject to the variation budget, we obtain a dynamic regret lower
bound of Ω (S 1 A 1 ∆ 1 D 1 T2 ). This completes the proof of Proposition 2.
E.2 Episodic Settings
Now let us go back to our simplified episodic setting, as depicted in Figure 2. One major difference
with the previous un-discounted setting is that we might not have time to mix between So and Sl
in H steps. (Note that we only need to reach the stationary distribution over the (s°, Sl) pair in
each step h, rather than the stationary distribution over the entire MDP. In fact, the latter case is
never possible because the entire MDP is not aperiodic.) It can be shown that the optimal policy on
this MDP has a mixing time of Θ (1) (Jin et 京.,2018), and hence we can choose δ to be slightly
larger than Θ(H) to guarantee sufficient time to mix. All the analysis up to inequality (51) carries
over to the episodic setting, and essentially we can set δ to be Θ (H^) to get a (static) regret lower
bound of Ω(√SATqH) in each segment. Another difference with the previous setting lies in the
usage of the variation budget. Since we require that all the steps in the same episode should vary
simultaneously, it now takes a variation budget of 2εH each time we switch to a new action a*
at the beginning of a new segment. Therefore, the overall variation budget now puts a constraint
of 2εHL ≤ O(∆) on the magnitude of each switch. Again, by choosing ε = Θ (JTSH) and
optimizing over possible values of L subject to the budget constraint, we obtain a dynamic regret
lower bound of Ω (S 1 A 1 ∆ 1 H 1 T 1 ) in the simplified episodic setting.
Finally, we consider the standard episodic setting as introduced in Section 2. In this setting, we
essentially will be concatenating H distinct JAO MDPs, each with an independent good action a*,
27
Under review as a conference paper at ICLR 2021
into a chain like Figure 2. The transition kernels in these JAO MDPs are also allowed to vary
asynchronously in each step h, although our construction of the lower bound does not make use of
this property. As argued similarly in Jin et al. (2018), the number of observations for each specific
JAO MDP is only T0/H, instead of T0. Therefore, we can assign a slightly larger value to ε and the
learning algorithm would still not be able to identify the good action given the fewer observations.
Setting δ = Θ (H) and ε = Θ ( JlAɔ leads to a (static) regret lower bound of Ω(H√SAT0) in the
stationary RL problem. Again, the transition kernels in all the H JAO MDPs vary simultaneously
at the beginning of each new segment. By optimizing L subject to the overall budget constraint
2εHL ≤ O(∆), We obtain a dynamic regret lower bound of Ω (S3 A 3 ∆ 3 H 2 T 3 ) in the episodic
setting. This completes our proof of Theorem 4.
28