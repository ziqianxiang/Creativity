Under review as a conference paper at ICLR 2021
Mitigating bias in calibration error estimation
Anonymous authors
Paper under double-blind review
Ab stract
Building reliable machine learning systems requires that we correctly understand
their level of confidence. Calibration focuses on measuring the degree of accuracy
in a model’s confidence and most research in calibration focuses on techniques
to improve an empirical estimate of calibration error, ECEBIN. Using simulation,
we show that ECEBIN can systematically underestimate or overestimate the true
calibration error depending on the nature of model miscalibration, the size of the
evaluation data set, and the number of bins. Critically, ECEBIN is more strongly
biased for perfectly calibrated models. We propose a simple alternative calibration
error metric, ECESWEEP, in which the number of bins is chosen to be as large as
possible while preserving monotonicity in the calibration function. Evaluating
our measure on distributions fit to neural network confidence scores on CIFAR-
10, CIFAR-100, and ImageNet, we show that ECESWEEP produces a less biased
estimator of calibration error and therefore should be used by any researcher
wishing to evaluate the calibration of models trained on similar datasets.
1	Introduction
Machine learning models are increasingly de-
ployed in high-stakes settings like self-driving cars
(Caesar et al., 2020; Geiger et al., 2013; Sun et al.,
2020) and medical diagnoses (Esteva et al., 2017;
2019; Gulshan et al., 2016) where a model’s ability
to recognize when it is likely to be incorrect is crit-
ical. Unfortunately, such models often fail in unex-
pected and poorly understood ways, hindering our
ability to interpret and trust such systems (Azulay
& Weiss, 2018; Biggio & Roli, 2018; Hendrycks &
Dietterich, 2019; Recht et al., 2019; Szegedy et al.,
2013). To address these issues, calibration is used
to ensure that a machine learning model produces
confidence scores that reflect the model’s ground
truth likelihood of being correct (Platt et al., 1999;
Zadrozny & Elkan, 2001; 2002).
To obtain an estimate of the calibration error, or
Figure 1: Bias in ECEBIN for perfectly cali-
brated models. Simulated data from a perfectly
calibrated model with confidence scores mod-
eled to ResNet-110 CIFAR-10 output (He et al.,
2016; Kangsepp, 2019). We show a reliability
diagram for a sample of size n = 200 and the
distribution of ECEBIN scores computed across
106 independent simulations. Even though the
model is perfectly calibrated, ECEBIN systemati-
cally predicts large calibration errors.
ECE1, the standard procedure (Guo et al., 2017; Naeini et al., 2015) partitions the model confidence
scores into bins and compares the model’s predicted accuracy to its empirical accuracy within each
bin. We refer to this specific metric as ECEBIN . Although recent work has pointed out that ECEBIN
is sensitive to implementation hyperparameters (Kumar et al., 2019; Nixon et al., 2019), measuring
the statistical bias in ECEBIN , or the difference between the expected ECEBIN and the true calibration
error (TCE), has remained largely unaddressed.
In this paper, we address this problem by developing techniques to measure bias in existing calibration
metrics. We use simulation to create a setting where the TCE can be computed analytically and
thus the bias can be estimated directly. As Figure 1 highlights, we find empirically that ECEBIN has
non-negligible statistical bias and systematically predicts large errors for perfectly calibrated models.
1Naeini et al. (2015) introduce ECE as an acronym for Expected Calibration Error. However, ECE is
not a proper expectation whereas the true calibration error is computed under an expectation. To resolve this
confusion, we prefer to read ECE as Estimated Calibration Error.
1
Under review as a conference paper at ICLR 2021
resnet110_c10
resnet110_SD_c10
resnet_wide32_c 10
densenet40_c10
resnet15 2_imgnet
EC∈sweep
eceb∣n
ECESWEEP
ECEb∣n
EC E$weep
ECEb∣n
ECESWEEP
ECEbin
Hill
lllllll	
	
EC∈sweep
ECEb∣n
ιιιιιιιιι
][≡[∏
resnet110_c100
resnet110_SD_c100
resnet_wide3 2_c 100
densenet40_c100
densenet161_imgnet
ec≡sweep	P∏∏∏∏∏∏∏∏Γ
ec∈bin	M8⅛
EC∈sweep
ECEb∣n
HZI
EC∈sweep
ECEb∣n
ECESWEEP
ECEBlN
ECEsweep
ECEb∣n

Figure 2: Bias affects which recalibration algorithm is preferred. For ten models, we report
which recalibration method is determined to be superior based either on ECEBIN or ECEsweep. The
wide bar indicates the superior method using entire validation set (mean of X instances); narrow bars
each use a random sample of 10% of the original validation set. Recalibration methods tested are
histogram binning, temperature scaling, and isotonic regression.
Motivated by monotonicity in true calibration curves arising from trained models, we develop a
simple alternate for measuring calibration error, the monotonic sweep calibration error (ECEsweep),
which chooses the largest number possible while maintaining monotonicity in the approximation to
the calibration curve. Our results suggest that ECEsweep is less biased than the standard ECEBIN and
can thus more reliably estimate calibration error.
Does the use of an improved ECE measure affect which recalibration method is preferred? In Figure
2, we examine this question using 10 pre-trained models, and compare the standard ECE measure,
ECEbin with 15 equal-width-spaced bins, to our ECEsweep. With large dataset sizes for recalibration
and evaluation 2, we find that ECEBIN produces a different selection of the preferred recalibration
method on 30% of the models. (we use histogram binning (Zadrozny & Elkan, 2001), temperature
scaling (Guo et al., 2017), and isotonic regression (Zadrozny & Elkan, 2002) as the recalibration
techniques.) when we reduce the size of the validation and evaluation by 10% and recalibrate with
these smaller sets, ECEBIN produces a different selection on 22% of the cases ( with 10 bins, we see
disagreement on 27% of the cases). Thus, the use of our improved ECE measure has significant
implications not only for estimation of calibration error but for improving calibration with methods
like temperature scaling.
2	Background
Consider a binary classification setup with input X ∈
X, target output Y = {0, 1}, and we have a model
f : X → [0, 1] whose output represents a confidence
score that the true label Y is 1.
True calibration error (TCE). we define true cal-
ibration error as the difference between a model’s
predicted confidence and the true likelihood of being
correct under the `p norm:
1
TCE(f) = (EX [|f(X) - Eγ[Y[f (X)]∣p])p . (1)
The TCE is dictated by two independent features of
a model: (1) the distribution of confidence scores
Figure 3: Curves controlling true calibra-
tion error. Our ability to measure calibration
is contingent on both the confidence score
distribution (e.g., f(X)〜Beta(2.8,0.05))
and the true calibration curve (e.g., EY [Y |
f(X) = c] = c2.
f (x)〜F over which the outer expectation is computed, and (2) the true calibration curve EY [Y |
f(X)], which governs the relationship between the confidence score f(x) and the empirical accuracy
(see Figure 3 for illustration).
In our experiments, we measure calibration error using the `2 norm because it increases the sensitivity
of the error metric to extremely poorly calibrated predictions, which tend to be more harmful in
applications. In addition, the mean squared prediction error of the classifier, or Brier score (Brier,
1950), can be decomposed into terms corresponding to the squared `2 calibration error and the
variance of the model’s correctness likelihood (Kuleshov & Liang, 2015; Kumar et al., 2019).
2we use standard validation sets of size 5, 000 examples for CIFAR-10/100 and 25, 000 examples for
ImageNet and evaluation sets of size 10, 000 for CIFAR-10/100 and 25, 000 for ImageNet.
2
Under review as a conference paper at ICLR 2021
2.1	Estimates of calibration error
To estimate the TCE of a model f, assume we are given a dataset containing n samples, {xi, yi}in=1.
We can approximate TCE by replacing the outer expectation in Equation 1 by the sample average and
replacing the inner expectation with an average over a set of instances with similar f (x) values:
ECENEIGH (f) = (1 Pn=If(Xi)- ∣NNi∣ Pj∈Ni yj| V,	⑵
where Ni is instance i’s set of neighbors in model confidence output space.
Label-binned calibration error (ECELB). The label-binned calibration error uses binning to define
Ni and estimate the model’s empirical accuracy E[Y |f (X)]. Specifically, the instances are partitioned
into b bins, where Bk denotes the set of all instances in bin k, allowing us to express Equation 2 in
terms of the binned neighborhood:
1
ECELB (f) = (1 Pk=I Pi∈Bk |f (Xi) - yk|p)P where yk = ∣Bk∣ Pj∈Bk yj.	⑶
Binning is commonly implemented using either equal width binning (Guo et al., 2017; Naeini et al.,
2015), which creates bins by dividing the model confidence domain [0, 1] into equal sized intervals, or
equal mass binning (Nixon et al., 2019), which creates bins by dividing the n samples into partitions
with an equal number of instances.
Binned calibration error (ECEBIN). In contrast to ECELB , which operates on the original instances
but uses binning to estimate empirical accuracy, ECEBIN collapses all instances in a bin into a single
instance and compares the per-bin empirical accuracy to the per-bin confidence score, weighted by
the per-bin number of instances. Given b bins, where Bk is the set of instances in bin k, and letting
fk and yk be the per-bin average confidence score and label, ECEBIN is defined under the 'p norm as
1
ECEbin (f )=(Pk=i 粤 Ifk- yk∣p) p	(4)
Importantly, ECEBIN always underestimates ECELB, ECELB(f) ≥ ECEBIN(f), which follows by
applying Jensen’s inequality on each inner term k ∈ {1, 2, . . . , b} in Eqs. 3 and 4:
WJ Pi∈Bk |f (Xi)- Yk lp ≥ ∣Pi∈Bk fk - Yk ∣P .	(5)
3	Measuring bias through simulation
We focus on bias rather than variance because the variance can be estimated from a finite set of samples
through resampling techniques whereas the bias is an unknown quantity that reflects systematic error.
We also found empirically that the variance seems relatively insensitive to the estimation technique
and number of bins (see Appendix B). The bias of a calibration error estimator, ECEA for some
estimation algorithm A, is the difference between the estimator’s expected value with respect to the
data distribution and the TCE:
BiasA = E[ECEA] - TCE.	(6)
If we assume a particular confidence score distribution F and true calibration curve
T(X) = EY [Y | f(X) = c] (see Figure 3 for examples), we can compute the TCE by numerically
evaluating the integral implicit in the outer expected value of Equation 1.
We then compute a sample estimate of the bias as follows. First, we generate n samples {f (Xi), yi}in=1
such that f (Xi)〜 F and EY [Y | f (X) = c] := T(c), and compute the ECE on the sample. We
repeat this process for m simulated datasets and compute the sample estimate of bias as the difference
between the average ECE and the TCE:3
BdSA(n) = ml Pi=ι ECEa - TCE.	(7)
Bias in ECEBIN for varying hyperparameters. Using simulation, we next investigate the bias
in ECEBIN as a function of the number of samples n and the number of bins. We compute
ECEBIN with equal width binning and we assume parametric curves for f(X) and EY [Y |
f(X) = c] that are fit to the ResNet-110 CIFAR-10 model output (see Section 5.1 for de-
tails on how we compute fits). Kumar et al. (2019) Proposition 3.3 shows that any binned
VerSion of CaIibration error systematically underestimates TCE in the limit of infinite data.
3In the remainder of the text, we will use the term "bias" to refer to this sample estimate of bias.
3
Under review as a conference paper at ICLR 2021
However, for a finite number of samples n, Figure
4 shows that ECEBIN can either overestimate or un-
derestimate TCE and that increasing the number
of bins does not always lead to better estimates of
TCE. Intuitively, as the number of bins explodes,
each example lies in its own bin, and the prediction
error is computed with respect to a target of 0 or 1.
Moreover, regardless of binning scheme, there ex-
ists a bin number for each sample size that results
in the lowest estimation bias and this optimal bin
count grows with the sample size. In Appendix
B, we show the variance associated with this ex-
periment, and also include results for alternative
calibration metrics.
BiaS in EqUal Width ECEBIN
2	-4.34	-4.52	-4.65	-4.72 -4.78 -4.82
4	-3.28	-3.71	-4.02	-4.21 -4.34 -4.42
2 8 * 16	-1.43	-2.14	-2.69	-3.04 -3.26 -3.40
	0.62	-0.37	-1.12	-1.67 -2.01 -2.24
32	2.66	1.50	0.52	-0.26 -0.83 -1.22
64	4.54	3.32	2.14	1.13 0.30 -0.30
200	400	800	1600 3200 6400
# Samples
Figure 4: ECEBIN with equal width binning can
overestimate TCE and the optimal number of
bins depends on number of samples.
4	Monotonic calibration metrics
Though Section 3 shows that there exists an optimal number of bins for which ECEBIN has the lowest
bias, unfortunately, this number depends on the binning technique, the number of samples, the
confidence score distribution, and the true calibration curve. This observation motivates us to seek a
method for adaptively choosing the number of bins.
Monotonicity in the true calibration curve implies that a model’s expected accuracy should always
increase as the model’s confidence increases. Although such a requirement seems reasonable for
most any statistical model, it is not obvious how to prove why or when a “reasonable” model would
attain such a property. We offer a rationale for why it should be expected of machine learning models
trained with a maximum likelihood objective, e.g., cross-entropy or logistic loss (Murphy, 2012).
Namely, from ROC (receiver operating characteristic) analysis of maximum likelihood models, an
under-appreciated observation of ROC curves is that a model trained to maximize the likelihood
ratio must have a convex ROC curve in the limit of infinite data (Green et al. (1966), Section 2.3).
The slope of the ROC curve is related to the calibration curve, and a convex ROC curve implies a
monotonically increasing calibration curve (the converse is also true) (Chen et al., 2018; Gneiting &
Vogel, 2018).
In practice, several potential confounds may lead to measuring a non-monontonic calibration curve.
First, finite data size effects may lead to fluctuations in the true positive or false positive rates, but do
not reflect the behavior of the underlying model. Second, deviations in the domain statistics between
cross-validated splits in the data may lead to unbounded behavior; however, we assume that such
domain shifts are negligible as cross-validated splits are presumed to be selected i.i.d..4 Given that
deviations from non-monotonic calibration curves are considered artificial, we posit that any method
that is trying to assess the TCE of an underlying model may freely assume monotonicity in the true
calibration curve. Note that this proposition already guides the entire field of re-calibration to require
that re-calibration methods only consider monotonic functions (Platt et al., 1999; Wu et al., 2012;
Zadrozny & Elkan, 2002).
Monotonic sweep calibration error (ECESWEEP). Accordingly, we leverage the underlying mono-
tonicity in the true calibration and propose the monotonic sweep calibration error, a metric that choose
the largest number of bins possible such that it and all smaller bin sizes preserve monotonicity in the
bin heights yk.
ECESWEEP= maxb	((Pk'=1	⅛l	Ifk	-	yk∣p) p	St	yι	≤ y2 …≤	yb,	∀b0	≤ b).
We can compute the monotonic sweep calibration error by starting with b = 2 bins (since b = 1 is
guaranteed to be a monotonic binning) and gradually increasing the number of bins until we either
4Note that a third potential reason for a non-monotonic calibration curve is that a classifier could be trained
with a non-likelihood-based statistical criteria, e.g. moment matching. However, the lack of monotonic behavior
in the calibration curve of such a model may actually be a sign that the model is not reasonable or admissible
model for consideration on a given task (Chen et al., 2018; Pesce et al., 2010).
4
Under review as a conference paper at ICLR 2021
ECEbin (%)
resnetllθ clθ
—
re sn et 11O_S D_c 10
resn et_wide32_c 10
densenet40 ClO
—
resnerIlO clOO
—
resn et 110_S D_cl OO
resn et_wide32_c 1OO
densenet40 ClOO
—
resn et 152Jmgnet
densenetl61Jmgnet
Figure 5: Maximum likelihood fits to empirical datasets illustrate large skew in their density
distribution and calibration function. For each dataset, (a) confidence distributions were fit with
a two-parameter beta distribution and (b) calibration curves were fit via generalized linear models
across multiple model families, with the best model selected via the Akaike information criterion
(details in Appendix A). Across models, the dataset source systematically affects both curves. (c)
We plot the overall quality of the fits by computing the ECEBIN on the original data vs. the ECEBIN
averaged over 1000 simulated trials. Curves well-fit to the data should lie close to the identity line.
reach a non-monotonic binning, in which case we return the last b that corresponded to a monotonic
binning, or until every sample belongs to its own bin (b = n).
Algorithm 1: Monotonic Sweep Calibration Error
ι for b — 2 to n do
2	Compute ECEBIN bin heights (yk) with b bins;
3	if Binning is not monotonic then
4	b = b-1 ;
5	break;
6	return ECEBIN computed with b bins
5	Results
5.1	Parametric fits capture calibration curve and score distribution
TCE is analytically computable when we assume parametric forms for the confidence distribution and
the true calibration curve. To what extent can parametric forms capture the diversity and complexity
of real world data? In many applications, only sample-based approximations to these functions are
available. In order to estimate ECEBIN bias in real-world data, we develop parametric models of
empirical logit datasets that enable direct measurement of TCE.
We consider 10 logit datasets (including those studied in Guo et al. (2017)), arising from training
four different neural model families (ResNet, ResNet-SD, Wide-ResNet, and DenseNet) on three
different image datasets (CIFAR-10/100 and ImageNet) (Deng et al., 2009; He et al., 2016; Huang
et al., 2016; 2017; Krizhevsky, 2009; LeCun et al., 1998; Zagoruyko & Komodakis, 2016). For each
dataset (Figure 5), we compute confidence scores by applying softmax and top-1 selection to logits
from KangsePP (2019).
Computing TCE directly for real-world data via Equation 1 is infeasible because of the expectation
across X. Instead, we model the distribution of the scores f (X) directly with a two-Parameter
beta distribution, which we fit using maximum likelihood estimation (note that in many cases, the
confidence scores are heavily skewed). Calibration functions are comPuted by fitting multiPle (binary)
generalized linear models (GLM) to the calibration data. From these candidate models, we select the
model of best fit using the Akaike Information Criteria (AIC). The models considered include logit,
log, and "logfliP" (log(1 - x)) link and transformation functions, uP to first order in the transformed
domain, resulting in monotonic calibration functions. See APPendix A for additional details.
We find that the Parametric forms for the calibration curve and distribution of scores are well caPtured
by these simPle GLM and Beta models. Figure 5(a,b) shows the resulting fits, with Parameters
summarized in APPendix A. We observe significant skew in the score distribution which, as discussed
5
Under review as a conference paper at ICLR 2021
CIFAR-IO
CIFAR-100
■ EM
■ EW
■ EMsweep
■ EWsweep
■ KDE
200	800	3,200	12,800
Num samples (log scale)
□ EMdebias	--A- ResNeJSD
-k— ResNet	-∙- Wide ResNet
ɪ-
二*二隼:二产丰T
.—      .-    
200—'—800― 3,200	12,800
Num samples (log scale)
♦…DenseNet
Bias=O
Figure 6: EMsweep is less biased than alternative calibration metrics. We plot bias versus
number of samples n for calibration metrics on simulated data drawn from the CIFAR-10, CIFAR-
100, and ImageNet fits (Section 5.1). The dataset the model was trained on has a greater influence on
bias than the model architecture. Metrics based on equal mass binning consistently outperform equal
width binning. Exploiting monotonicity in the EMsweep metric helps the most at small sample sizes.
in Section 5.2, poses a challenge to measuring calibration error with equal-width bins. We find that
the dataset has more influence on the fits than the neural model, with ImageNet models the least
skewed and CIFAR-10 the most (correlating with model accuracy). Figure 4 5(c) indicates that
ECEbin scores computes on simulated data from the fits closely match ECEBIN scores computed on
the real data, as witnessed by the fact that the points lie near the line of equality.
5.2	Simulation results on distributions fit to CIFAR-10/100, and ImageNet
Using the resulting parametric fits from Section 5.1, we evaluate ECEBIN and ECESWEEP using both
equal mass binning and equal width binning and compare these values to the analytically computed
tCe. In addition, we include a comparison to the recently proposed debiased estimator, ECEDEBias,
using equal mass binning (Kumar et al., 2019) and a smoothed Kernel Density Estimation (KDE)
method for estimating calibration error (Zhang et al., 2020). We abbreviate each method as follows:
•	EW: ECEbin using equal width binning and 15 bins (Guo et al., 2017; Naeini et al., 2015),
•	EM: ECEBiN using equal mass binning and 15 bins (Nixon et al., 2019),
•	EMdebias: Debiased calibration metric using equal mass binning (Kumar et al., 2019)
•	KDE: KDE calibration metric (Zhang et al., 2020)
•	EWsweep: ECESWEEP using equal width binning, and
•	EMsweep: ECESWEEP using equal mass binning.
We choose 15 bins for ECEBiN and ECEDEBiAS, following the standard set by Guo et al. (2017). For a
comparison of different choices of fixed number of bins, Appendix B includes an analysis of the bias
and variance of ECEBiN , ECESWEEP, and ECEDEBiAS across different bin numbers and sample sizes
for the curves corresponding to CiFAR-10 ResNet-110, CiFAR-100 Wide ResNet-32 and imageNet
ResNet-152 models. We see that the optimal number of bins varies with the number of samples, and
using a different fixed number of bins introduces bias at varying sample sizes.
Figure 6 plots the bias (estimated using m = 1,000 simulations) versus the sample size n for the
best-fit curves for each neural network model trained on the CiFAR-10, CiFAR-100, and imageNet
datasets. An unbiased estimator would have Bias = 0, which we highlight visually in green. We
6
Under review as a conference paper at ICLR 2021
find that the dataset the model was trained on has more influence on the calibration metric behavior
than the model architecture, which may be unsurprising given that Section 5.1 shows that the dataset
heavily influences both the distribution of confidence scores and the true calibration curve.
Equal width versus equal mass binning: Overall, metrics that employ equal mass binning show less
bias than those with equal width binning. Surprisingly, for EW and EWsweep on CIFAR-10, as well
as EW on CIFAR-100, we see increasing absolute bias as the number of samples increases across all
model architectures tested. We propose a possible explanation for this phenomenon. As Figure 5 (a)
shows, models trained on CIFAR-10 and CIFAR-100 have highly skewed confidence distributions
and, as a result, equal width binning places the majority of the instances in the top bin. As we increase
the number of samples, we increase the likelihood that we generate a sample that populates one of the
lower bins, which, due to their low sample density, may have a poorer average estimate of the TCE.
ECESWEEP versus alternative metrics: Our experiments show that ECESWEEPwith equal mass binning
has either similar or less bias than alternative calibration error metrics, and at low sample sizes, the
ECESWEEP method is consistently less biased than all other metrics. However, the ECESWEEP does not
show improvements over other metrics when combined with equal width binning, and we do not
recommend using the combination in practice.
KDE estimator. Compared to all calibration metrics we evaluate, the KDE estimator has much higher
bias across the CIFAR-10, CIFAR-100, and ImageNet simulations. Our results suggest that the
heuristic used to choose the kernel bandwidth and the specific ‘triweight’ kernel worked well for the
one synthetic example evaluated in (Zhang et al., 2020), but fails to generalize to the more realistic
synthetic examples we study. Specifically, Zhang et al. (2020) assumes a Gaussian distribution for
P(X|Y ) and a logistic confidence score distribution, which result in notably different qualitative
shapes than the logit distributions we obtain from models trained on CIFAR-10/100 or ImageNet (see
Figure 5(a, b) or the reliability diagrams and confidence score distributions from KangsePP (2019)).
Debiased estimator. The debiased estimator (Kumar et al., 2019) uses a jackknife technique to
estimate the Per-bin bias in the standard ECEBIN, and subtracts off this bias to achieve a better
binned estimate of the calibration error. However, unlike ECESWEEP , the debiased estimator still has a
hyPerParameter b that controls the number of bins. On the CIFAR-10/100, and ImageNet simulations
in Figure 6, the debiased estimator with 15 bins is more comPetitive to equal mass ECESWEEP than any
other estimation method we test, but the equal mass ECESWEEP method still outPerforms the debiased
estimator for low samPle sizes. In APPendix B, we also show that the equal mass debiased estimator
has higher variance than the equal mass ECESWEEP (excePt when b ≤ 4, when all estimators have high
bias).
5.3	Bias versus true calibration error
True Calibration Error (%)
Figure 7: Bias in calibration estimation increases as TCE decreases. We Plot average ECE
(%) for EW (left) and EMsweep (right) versus the TCE (%), with varying samPle size and score
distributions. The estimator bias is systematically worse for better calibrated models, and the effect is
more egregious with fewer samPles. At n = 200 samPles, dePending on the score distribution, an EW
estimate of 12% could either corresPond to 5% or 8% TCE. The EMsweep metric is able to mitigate
the bias and ambiguity in calibration error estimation to a certain extent.
True Calibration Error (%)
7
Under review as a conference paper at ICLR 2021
We next evaluate the estimation bias of the baseline EW and our best estimator from the previous
section, EMsweep, as we systematically vary the TCE. We are interested in the low calibration
error regime because a goal of many recalibration algorithms is to reduce the calibration error of
the model to 0%. Figure 7 shows the average estimated calibration error for EW and EMsweep
versus the TCE. The average calibration error is computed across m = 1,000 simulated datasets,
and we include results for two sample sizes, n = 200 and n = 5,000, and two score distributions,
f(x)〜Uniform(0,1) and f(x)〜Beta(1.1,0.1), the beta distribution fit to the CIFAR-100 Wide
ResNet_32. To control the TCE, we assume EY [Y | f(X) = c] = cd and vary d ∈ [1, 10]. When
d = 1 the true calibration curve is EY [Y | f(X) = c] = c, which means the model’s predicted
confidence score is exactly equal to its empirical accuracy and thus the TCE is 0%. As we increase d,
we move the true calibration curve farther away from the perfect calibration curve, which increases
the TCE of the model.
The bias in the calibration error estimation can be seen visually as the difference between the ECE
and the TCE. Perfect estimation (0 bias) corresponds to the y = x line. Bias is highest when the
model is perfectly calibrated (TCE is 0%) and generally decreases as TCE increases. Using a larger
sample size of n = 5, 000 reduces the bias, but when the model is perfectly calibrated, the ECEBIN
can still be off by 2%. The EMsweep metric significantly reduces this bias.
In practice, we do not know the distribution of scores F, the true calibration curve EY [Y | f(X) = c],
or the TCE. So, given a measurement of calibration error, how much bias can we expect? If we
measure an ECEBIN of 20%, we expect it to be fairly close to the TCE. However, if we see an ECEBIN
of 2%, it is possible that the model may actually be perfectly calibrated! We further explore this
dilemma in the next section.
5.4	How well can we detect miscalibration?
Consider the situation where we have a model whose TCE
is unknown and we wish to test the hypothesis that the
model is miscalibrated, i.e., TCE > 0. Our ability to de-
tect miscalibration depends on the TCE, the sample size
(n), and the method for estimating calibration error. We
conduct a simulation with f (x)〜Beta(1,1) and true cal-
ibration curve from the family EY [Y | f(X) = c] = cd,
where d is varied to obtain a range of TCE. Allowing for a
type I error rate of .05 (also known as the false-alarm rate,
or the rate of mistakenly claiming miscalibration when
a model is perfectly calibrated), we obtain type II error
rates (also known as the miss rate, or the rate of failing to
detect a miscalibration). Figure 8 shows the type II error
rate as a function of TCE and n for EMsweep and EW.
Our results indicate that EMsweep obtains a significantly
lower failure rate than EW, particularly for under 10,000
samples. More generally, we note limitations with both
methods: to detect a miscalibration of 2%, over 10,000
samples are needed; and if one has under 500 samples, the
miscalibration must be greater than 10% to be detected
reliably.
2	4	6	8	10	12
TCE (%)
Figure 8: Probability of failing to de-
tect miscalibration (type II error, or
miss rate), plotted as a function of TCE
for various sample sizes (n), with type
I error rate fixed at 0.05. EMsweep
(dashed lines) obtains a significantly
lower failure rate than EW (solid lines).
6	Discussion
Much research in model calibration has focused on recalibrating models, i.e., transforming f(x) to
f0(x) (Platt et al., 1999; Zadrozny & Elkan, 2001; 2002). We focus on estimating calibration error,
because without a good estimate of TCE, there is little point in trying to recalibrate models. What
implications do our results have on choosing and evaluating recalibration algorithms?
One possibility is that bias affects all recalibration methods in the same way, which would imply
that we should still be able to select recalibration method A over method B with a biased estimator.
However, our results show that the distribution of confidence scores significantly impacts the bias
in calibration error estimation, even when the sample size is fixed. Since recalibration methods are
inherently designed to modify the confidence score distribution, we cannot assume that bias will
affect all methods in the same way.
8
Under review as a conference paper at ICLR 2021
Another possibility is that the bias is small compared to the calibration error differences we would
measure. However, our results suggest that this is not true. Even when the number of evaluation
samples is high, n = 5,000, Figure 7 shows that it is entirely possible that we might measure an
ECEBIN of 2% when the model is in fact perfectly calibrated. Moreover, Figure 2 shows that the
preference of recalibration algorithm can change depending on whether ECESWEEP or ECEBIN is used
to measure calibration error, implying that bias might meaningfully affect the conclusions of previous
studies of calibration error such as those in Guo et al. (2017).
Several authors attempt a different approach to recalibration: improving model calibration during
training. For instance, Mukhoti et al. (2020) trains a model with a batch size of 128 across multiple
types of losses including maximum mean calibration error (Kumar et al., 2018) and Brier loss (Brier,
1950) which explicitly tries to minimize a calibration loss using 128 examples at a time. However,
our results suggest that training a model with naive estimates of calibration error as an objective
using a batch size < O(1000) is a potentially flawed endeavor, particularly because the distribution of
scores from the model is changing throughout training, and any potential measure of calibration may
be more affected by the distribution of scores (as opposed to the calibration curve).
7	Related Work
Sensitivity of ECEBIN to hyperparameters. Several works have pointed out that ECEBIN is sensitive
to implementation details. Kumar et al. (2019) show that ECEBIN increases with number of bins while
Nixon et al. (2019) find that ECEBIN scores are sensitive to several hyperparameters, including `p
norm, number of bins, and binning technique. In addition, Nixon et al. (2019) find that ECEBIN with
equal mass binning produces more stable rankings of recalibration algorithms, which is consistent
with our conclusion that equal mass ECEBIN is a less biased estimator of TCE. However, in contrast
to prior work, we study the sensitivity of the bias in ECEBIN to implementation hyperparameters.
Metrics for calibration error estimation. Gupta et al. (2020) propose a calibration error metric
inspired by the Kolmogorov-Smirnov (KS) statistical test that estimates the maximum difference
between the cumulative probability distributions P(f(X)) and P(Y | f(X)). The KS is similar to
the maximum calibration error (MCE) (Naeini et al., 2015) in that it computes a worst-case deviation
between confidence and accuracy, but the KS is computed on the CDF, while the MCE uses binning
and is computed on the PDF. In contrast, our work focuses on measuring the average difference
between confidence and accuracy. As mentioned in Guo et al. (2017), both the worst case and average
difference are useful measures but may be applicable under different circumstances.
8	Conclusions and Future Work
If we are to rely on the predictions from machine learning models in high stakes situations like
autonomous vehicles, content moderation, and medicine, we must be able to detect when these
predictions are likely to be incorrect. Given that the default confidence scores produced by machine
learning models do not necessarily correspond to the model’s empirical accuracy, recalibration
is necessary in order to produce reliable and consistent output. However, it is impossible for a
recalibration algorithm to achieve perfect calibration if we cannot measure calibration accurately.
Our results show that the statistical bias in current calibration error estimators grows as we approach
perfect calibration, but this bias can be mitigated by using monotonic estimation techniques. We
conclude with some directions for future work:
Simulation as an evaluation tool. We have shown that simulation is a powerful technique for evalu-
ating calibration error estimators. However, especially for small sample sizes, the ECESWEEP method
does not completely eliminate estimation bias, and we only evaluated a finite set of distributions
arising from image classification datasets and models. We hope future work can use simulation as an
effective tool for developing both new calibration metrics and new recalibration techniques, and that
evaluations can be extended to a more diverse set of datasets and models.
Distribution shifts. Since models deployed in real world application will necessarily make pre-
dictions on out-of distribution examples and since we would like these predictions to be calibrated
(Ovadia et al., 2019), it is important that we are able to accurately measure and improve calibration
error on non-iid data. However, in such situations, we do not necessarily expect the calibration curve
to be monotonic. Thus, exploring how distribution shifts can modify the shape of the true calibration
curve is an important future direction.
9
Under review as a conference paper at ICLR 2021
References
Aharon Azulay and Yair Weiss. Why do deep convolutional networks generalize so poorly to small
image transformations? arXiv preprint arXiv:1805.12177, 2018.
Battista Biggio and Fabio Roli. Wild patterns: Ten years after the rise of adversarial machine learning.
Pattern Recognition, 2018. https://arxiv.org/abs/1712.03141.
Glenn W Brier. Verification of forecasts expressed in terms of probability. Monthly weather review,
78(1):1-3,1950.
Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush
Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for
autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 11621-11631, 2020.
Weijie Chen, Berkman Sahiner, Frank Samuelson, Aria Pezeshk, and Nicholas Petrick. Calibration
of medical diagnostic classifier scores to the probability of disease. Statistical methods in medical
research, 27(5):1394-1409, 2018.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical
Image Database. In CVPR09, 2009.
Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and
Sebastian Thrun. Dermatologist-level classification of skin cancer with deep neural networks.
nature, 542(7639):115-118, 2017.
Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark DePristo,
Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff Dean. A guide to deep
learning in healthcare. Nature medicine, 25(1):24-29, 2019.
Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti
dataset. The International Journal of Robotics Research, 32(11):1231-1237, 2013.
Andrew Gelman, John B. Carlin, Hal S. Stern, and Donald B. Rubin. Bayesian Data Analysis.
Chapman and Hall/CRC, 2nd ed. edition, 2004.
Tilmann Gneiting and Peter Vogel. Receiver operating characteristic (roc) curves. arXiv preprint
arXiv:1809.04808, 2018.
David Marvin Green, John A Swets, et al. Signal detection theory and psychophysics, volume 1.
Wiley New York, 1966.
Varun Gulshan, Lily Peng, Marc Coram, Martin C Stumpe, Derek Wu, Arunachalam Narayanaswamy,
Subhashini Venugopalan, Kasumi Widner, Tom Madams, Jorge Cuadros, et al. Development
and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus
photographs. Jama, 316(22):2402-2410, 2016.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. International Conference on Machine Learning (ICML), 2017. URL https://arxi
v.org/abs/1706.04599.
Kartik Gupta, Amir Rahimi, Thalaiyasingam Ajanthan, Thomas Mensink, Cristian Sminchisescu, and
Richard Hartley. Calibration of neural networks using splines. arXiv preprint arXiv:2006.12800,
2020.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Computer Vision and Pattern Recognition (CVPR), 2016. URL https://arxi
v.org/abs/1512.03385.
Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common
corruptions and perturbations. In International Conference on Learning Representations (ICLR),
2019. https://arxiv.org/abs/1807.01697.
Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with
stochastic depth. In European conference on computer vision, pp. 646-661. Springer, 2016.
10
Under review as a conference paper at ICLR 2021
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
A. Krizhevsky. Learning multiple layers of features from tiny images. 2009.
Volodymyr Kuleshov and Percy S Liang. Calibrated structured prediction. In Neural Information
Processing Systems (NeurIPS), 2015. URL http://papers.nips.cc/paper/5658-ca
librated-structured-prediction.pdf.
Ananya Kumar, Percy S Liang, and Tengyu Ma. Verified uncertainty calibration. In Neural Informa-
tion Processing Systems (NeurIPS), 2019. URL https://arxiv.org/abs/1909.10155.
Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks
from kernel mean embeddings. In International Conference on Machine Learning (ICML), pp.
2805-2814, 2018.
Markus Kangsepp. Nn_calibration. https://github.com/markus93/NN_calibration,
2019.
Yann LeCun, Leon Bottou, YoshUa Bengio, and Patrick Haffner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Jishnu Mukhoti, Viveka Kulharia, Amartya Sanyal, Stuart Golodetz, Philip HS Torr, and Puneet K
Dokania. Calibrating deep neural networks using focal loss. arXiv preprint arXiv:2002.09437,
2020.
Kevin P Murphy. Machine learning: a probabilistic perspective. MIT press, 2012.
Mahdi Pakdaman Naeini, Gregory F Cooper, and Milos Hauskrecht. Obtaining well calibrated
probabilities using bayesian binning. In AAAI Conference on Artificial Intelligence. NIH Public
Access, 2015. URL https://www.ncbi.nlm.nih.gov/pmc/articles/PMC44100
90/.
Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring
calibration in deep learning. In CVPR Workshops, pp. 38-41, 2019.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing
Systems, pp. 13991-14002, 2019.
Lorenzo L Pesce, Charles E Metz, and Kevin S Berbaum. On the convexity of roc curves estimated
from radiological test results. Academic radiology, 17(8):960-968, 2010.
John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized
likelihood methods. Advances in large margin classifiers, 10(3):61-74, 1999.
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalize to imagenet? In International Conference on Machine Learning, pp. 5389-5400, 2019.
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James
Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous
driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, pp. 2446-2454, 2020.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow,
and Rob Fergus. Intriguing properties of neural networks. In International Conference on Learning
Representations (ICLR), 2013. http://arxiv.org/abs/1312.6199.
Yuan Wu, Xiaoqian Jiang, Jihoon Kim, and Lucila Ohno-Machado. I-spline smoothing for calibrating
predictive models. AMIA Summits on Translational Science Proceedings, 2012:39, 2012.
Bianca Zadrozny and Charles Elkan. Obtaining calibrated probability estimates from decision trees
and naive bayesian classifiers. In Icml, volume 1, pp. 609-616. Citeseer, 2001.
11
Under review as a conference paper at ICLR 2021
Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass proba-
bility estimates. In ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, 2002.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,
2016.
Jize Zhang, Bhavya Kailkhura, and T Han. Mix-n-match: Ensemble and compositional methods for
uncertainty calibration in deep learning. arXiv preprint arXiv:2003.07329, 2020.
12
Under review as a conference paper at ICLR 2021
A	Maximum-likelihood fits
Table 1 provides parameters fit to the top-scores obtained for each of 10 empirical datasets by
maximum likelihood estimation.
Table 1: Parameters of best fit for distribution functions investigated in Section 5.1.
	α	ʌ β
resnet110_c10	2.7752	0.0478
resnet110_SD_c10	2.1714	0.0394
resnet_wide32_c10	2.3806	0.0379
densenet40_c10	1.9824	0.0397
resnet110_c100	1.1823	0.1081
resnet110_SD_c100	1.1233	0.1147
resnet_wide32_c100	1.0611	0.0650
densenet40_c100	1.0805	0.0808
resnet152_imgnet	1.1359	0.2069
densenet161_imgnet	1.1928	0.2206
d Fl	♦ ʌ _	Γ∕~. CCCl :一 「C LCI	.	1	-I	♦	1	C ♦
Global optimιa α ∈ [0,200], β ∈ [0, 50] are approximately computed using a recursively-refining
brute-force search until both parameters are established to within an absolute tolerance of 1e-5.
Each step in the recursion contracts a linear sampling grid (N = 11) by a factor of γ = .5 centered
on the previously established optimal parameter, subject to the constraints α, β > 0. Experiments
confirmed that the computed optima were robust to the hyperparameters N, γ.
arg min - ln
χα-1(1 — Xi)β-1
Γ(α)Γ(β)
Γ(α+β)
(8)
Table 2 provides parameters fit to calibration functions. For each sample image xi in the image
dataset, define si = f (xi) to be the score (the output of the top-scoring logit after softmax) and
yi ∈ {0, 1} to be the classification (yi = 1 when the top-scoring logit correctly classified image
xi) for the sample image. The loss for the binary generalized linear model (GLM) across different
combinations of link functions g(y) and transform functions t(s) was optimized via the standard loss
(Gelman et al. (2004)):
arg min X -lnpiyi(1 -pi)1-yi, pi = g-1(b0 + b1t(si))	(9)
b0,b1	i
For each dataset, the GLM of best fit was selected via the Akaike Information Criteria using the
likelihood at the optimized parameter values.
13
Under review as a conference paper at ICLR 2021
Table 2: Parameters of best fit for calibrations functions investigated in Section 5.1.
dataset_name	glm_name	AIC	b0	b1	dataset_name	glm_name	AIC	b0	b1
resnet110_c10	logflip_logflip_b0_b1	2779.22	-0.24	0.30	resnet110_SD_c100	logit_logit_b0_b1	7873.61	-0.88	0.49
	logit_logflip_b0_b1	2790.40	-0.55	-0.38		logflip_logflip_b0_b1	7878.19	-0.09	0.35
	logit_logflip_b1	2827.51		-0.31		logflip_logflip_b1	7932.28		0.38
	logit_logit_b0_b1	2840.70	-0.38	0.36		logit_logflip_b0_b1	7944.61	-1.04	-0.52
	logit_logit_b1	2900.02		0.30		logit_logit_b1	8315.51		0.32
	logflip_logflip_b1	2932.09		0.34		log_log_b0_b1	8437.82	-0.11	2.18
	log_log_b0_b1	3221.72	-0.06	2.53		logit_logflip_b1	8510.36		-0.30
	logit_logit_b0	3799.63	1.99			log_log_b1	9988.07		3.30
	logflip_logflip_b0	3811.98	-2.13			logit_logit_b0	10803.27	0.80	
	log_log_b0	3829.05	-0.13			log_log_b0	10810.90	-0.37	
	logit_logflip_b0	3868.40	1.95			logflip_logflip_b0	10823.15	-1.16	
	log_log_b1	4281.78		4.75		logit_logflip_b0	10834.48	0.78	
resnet110_SD_c10	logit_logflip_b0_b1	2498.98	-0.27	-0.35	resnet_wide32_c100	logflip_logflip_b0_b1	7183.93	-0.13	0.21
	logit_logit_b1	2502.52		0.30		logit_logit_b0_b1	7219.14	-0.98	0.33
	logit_logflip_b1	2508.70		-0.30		logflip_logflip_b1	7233.51		0.25
	logit_logit_b0_b1	2538.41	-0.26	0.33		logit_logflip_b0_b1	7297.00	-1.06	-0.34
	logflip_logflip_b0_b1	2550.29	-0.36	0.27		logit_logit_b1	7626.21		0.19
	logflip_logflip_b1	2572.85		0.35		log_log_b0_b1	7650.97	-0.24	2.51
	log_log_b0_b1	2594.91	-0.08	1.98		logit_logflip_b1	7795.28		-0.17
	log_log_b0	3137.19	-0.19			logflip_logflip_b0	8977.39	-0.98	
	logflip_logflip_b0	3150.42	-1.80			logit_logflip_b0	8987.38	0.49	
	logit_logit_b0	3175.58	1.58			log_log_b0	9000.24	-0.49	
	logit_logflip_b0	3179.67	1.56			logit_logit_b0	9009.51	0.49	
	log_log_b1	3697.37		3.77		log_log_b1	11911.51		5.48
resnet_wide32_c10	logit_logit_b1	2483.34		0.26	densenet40_c100	logit_logit_b0_b1	8158.28	-0.97	0.34
	logflip_logflip_b0_b1	2487.69	-0.47	0.22		logflip_logflip_b0_b1	8229.43	-0.12	0.22
	logit_logit_b0_b1	2511.39	-0.13	0.28		logit_logflip_b0_b1	8267.77	-1.08	-0.35
	logit_logflip_b0_b1	2558.45	-0.26	-0.28		logflip_logflip_b1	8368.86		0.25
	logit_logflip_b1	2586.47		-0.25		logit_logit_b1	8783.50		0.19
	log_log_b0_b1	2647.03	-0.12	1.87		log_log_b0_b1	8832.20	-0.25	2.26
	logflip_logflip_b1	2713.17		0.30		logit_logflip_b1	8918.57		-0.18
	log_log_b0	2981.24	-0.21			logit_logit_b0	10138.24	0.47	
	logflip_logflip_b0	2983.05	-1.70			logit_logflip_b0	10182.61	0.45	
	logit_logit_b0	2989.90	1.49			logflip_logflip_b0	10242.15	-0.94	
	logit_logflip_b0	3055.55	1.45			log_log_b0	10261.01	-0.50	
	log_log_b1	4582.09		4.61		log_log_b1	13322.10		5.25
densenet40_c10	logit_logflip_b1	2910.62		-0.26	resnet152_imgnet	logflip_logflip_b0_b1	18729.85	-0.12	0.58
	logit_logit_b0_b1	2961.31	-0.40	0.31		logit_logit_b0_b1	18783.22	-0.29	0.65
	logit_logflip_b0_b1	3000.23	-0.38	-0.31		log_log_b0_b1	18785.44	-0.03	1.32
	logflip_logflip_b0_b1	3001.78	-0.31	0.24		logflip_logflip_b1	18872.14		0.65
	logit_logit_b1	3021.54		0.25		logit_logit_b1	19074.37		0.57
	logflip_logflip_b1	3027.78		0.31		logit_logflip_b0_b1	19095.40	-0.82	-0.79
	log_log_b0_b1	3153.38	-0.12	2.04		log_log_b1	19840.25		1.53
	log_log_b0	3531.22	-0.22			logit_logflip_b1	20062.10		-0.50
	logflip_logflip_b0	3589.11	-1.60			logflip_logflip_b0	26935.09	-1.41	
	logit_logit_b0	3601.85	1.37			log_log_b0	26968.50	-0.28	
	logit_logflip_b0	3679.95	1.30			logit_logflip_b0	27012.77	1.12	
	log_log_b1	4735.18		4.27		logit_logit_b0	27084.11	1.11	
resnet110_c100	logflip_logflip_b0_b1	8181.97	-0.11	0.28	densenet161_imgnet	log_log_b0_b1	18202.41	-0.03	1.27
	logit_logit_b0_b1	8206.19	-0.88	0.39		logit_logit_b0_b1	18460.70	-0.25	0.68
	logflip_logflip_b1	8301.28		0.31		logflip_logflip_b1	18521.48		0.67
	logit_logflip_b0_b1	8371.53	-1.01	-0.40		logflip_logflip_b0_b1	18534.07	-0.10	0.61
	logit_logit_b1	8732.11		0.25		logit_logit_b1	18822.25		0.60
	log_log_b0_b1	8918.21	-0.16	2.35		logit_logflip_b0_b1	18913.25	-0.77	-0.80
	logit_logflip_b1	8926.99		-0.23		log_log_b1	19493.85		1.44
	logit_logflip_b0	10903.83	0.74			logit_logflip_b1	19562.58		-0.54
	logit_logit_b0	10943.95	0.72			logit_logflip_b0	26426.38	1.19	
	logflip_logflip_b0	10964.91	-1.12			logflip_logflip_b0	26445.91	-1.46	
	log_log_b0	11002.20	-0.40			logit_logit_b0	26519.76	1.18	
	log_log_b1	11850.89		4.26		log_log_b0	26662.65	-0.27	
14
Under review as a conference paper at ICLR 2021
Table 3: ECE reported in Figure 5(c).
	ECE2 (%)	<ECE2> (%, simulated)
resnet110_c10	6.67	8.42
resnet110_SD_c10	6.54	8.79
resnet_wide32_c10	6.09	8.44
densenet40_c10	6.70	8.09
resnet110_c100	20.26	18.87
resnet110_SD_c100	17.44	15.78
resnet_wide32_c100	20.40	17.53
densenet40_c100	23.12	19.69
resnet152_imgnet	6.85	9.26
densenet161_imgnet	6.15	6.87
B Bias and variance in calibration metrics
B.1	Bias
We evaluate bias for various calibration metrics using both equal-width and equal-mass binning as
we vary both the sample size n and the number of bins b. These plots should be seen as an alternative
visualization to 6 where we additionally compare to different choices for the fixed number of bins b.
Since the ECESWEEP metrics adaptively choose a different number of bins for each sample size, we
display the bin number for this metric as -1.
We find that ECEBIN can overestimate the true calibration error and there exists an optimal number of
bins that produces the least biased estimator that changes with the number of samples n. Additionally,
equal mass binning generally results in a less biased metric than equal width binning.
CIFAR-10 ResNet-110. Figure 9 assume parametric curves forp(f(x)) and EY [Y | f(X) = c] that
we obtain from maximum-likelihood fits to CIFAR-10 ResNet-110 model output.
CIFAR-100 Wide ResNet-32. Figure 10 assume parametric curves for p(f (x)) and EY [Y | f(X) =
c] that we obtain from maximum-likelihood fits to CIFAR-100 Wide ResNet-32 model output.
ImageNet ResNet-152. Figure 11 assume parametric curves for p(f (x)) and EY [Y | f(X) = c]
that we obtain from maximum-likelihood fits to ImageNet ResNet-152 model output.
15
Under review as a conference paper at ICLR 2021
BIaS In EqCal WIdth ECEBlN
2 -4.34	-4.52	-4.65	-4.72	-4.78	-4.82
4 -3.28	-3.71	-4.02	-4.21	-4.34	-4.42
BlaS In EqUal MaSS ECEBlN
2 -2.58	-2.59	-2.56	-2.50	-2.50	-2.53
4 -0.70	-0.75	-0.73	-0.66	-0.66	-0.69
816
sus⅛
-1.43	-2.14	-2.69	-3.04	-3.26	-3.40
0.62	-0.37	-1.12	-1.67	-2.01	-2.24
816
sus⅛
0.16
-0.13 -0.19 -0.18 -0.20 -0.24
1.17	0.42	0.14
0.01 -0.07 -0.14
32 2.66
1.50 0.52 -0.26 -0.83 -1.22
32 2.82	1.36 0.66 0.32	0.13 -0.01
64 4.54 3.32	2.14	1.13	0.30 -0.30
200	400	800 1600 3200 6400
# Samples
Bias in Equal Width ECEd∈bias
2
816
sus⅛
32
64
-4.90	-4.96	-4.93	-4.87	-4.86	-4.86
-4.62	-4.60	-4.53	-4.50	-4.50	-4.51
-3.30	-3.48	-3.54	-3.54	-3.54	-3.55
-1.58	-2.15	-2.36	-2.45	-2.47	-2.50
0.35	-0.56	-1.11	-1.41	-1.56	-1.65
2.28	1.14	0.19	-0.41	-0.79	-1.00
64 5.50 3.01 1.56 0.83 0.41 0.17
200 400	800 1600 3200 6400
# Samples
Bias in Equal Mass ECEdeb∣as
2
816
sus⅛
32
64
-3.00 -2.79 -2.66 -2.55 -2.53 -2.54
-1.34 -1.05
-0.97 -0.66
-0.89 -0.59
-0.96 -0.55
-1.40 -0.55
-0.88 -0.74
-0.45 -0.31
-0.37 -0.24
-0.31 -0.17
-0.29 -0.12
-0.69 -0.70
-0.27 -0.28
-0.19 -0.20
-0.11 -0.13
-0.07 -0.08
SU击*
200	400	800 1600 3200 6400
# Samples
BiaS in EqUal Width ECESWEEP
-3.52 -3.96 -4.24 -4.39 -4.50 -4.55
200	400	800^^1600^^3200^^^6400
# Samples
200	400	800 1600 3200 6400
# Samples
BiaS in EqUal MaSS ECESWEEP
-1 0.05	-0.16	-0.17	-0.14	-0.15	-0.19
200	400	800	1600	3200	6400
# Samples
4
Figure 9: Bias for various calibration metrics assuming curves fit to CIFAR-10 ResNet-110
output. We plot bias for various calibration metrics using both equal-width binning (left column)
and equal-mass binning (right column) as we vary both the sample size n and the number of bins b.
B.2	Variance
We also compute the variance for various calibration metrics using both equal-width and equal-mass
binning as we vary both the sample size n and the number of bins b. As expected, the variance
decreases with number of samples, but, unlike the bias, there is no clear dependence on the number
of bins.
CIFAR-10 ResNet-110. Figure 12 assume parametric curves for p(f (x)) and EY [Y | f(X) = c]
that we obtain from maximum-likelihood fits to CIFAR-10 ResNet-110 model output.
CIFAR-100 Wide ResNet-32. Figure 13 assume parametric curves forp(f(x)) and EY [Y | f(X) =
c] that we obtain from maximum-likelihood fits to CIFAR-100 Wide ResNet-32 model output.
ImageNet ResNet-152. Figure 14 assume parametric curves for p(f (x)) and EY [Y | f(X) = c]
that we obtain from maximum-likelihood fits to ImageNet ResNet-152 model output.
16
Under review as a conference paper at ICLR 2021
YqCal WId色fC包N
2 -6.29	-6.43	-6.49	-6.54	-6.55	-6.54
4 -5.12	-5.48	-5.66	-5.79	-5.83	-5.84
BlaS In EqUal MaSS ECEBlN
2 -2.07 -2.06 -2.04 -2.05 -2.04 -2.02
4 -1.02
-1.16 -1.20 -1.25
-1.25 -1.23
816
sus⅛
-3.15	-3.86	-4.28	-4.50	-4.61	-4.65
-0.83	-1.91	-2.69	-3.12	-3.34	-3.44
816
sus⅛
-0.06 -0.49 -0.68 -0.78 -0.81
-0.82
32
64
816
sus⅛
32
64
1.59
3.83
0.19 -0.98
-1.74 -2.15 -2.38
2.31
0.86 -0.26 -1.01 -1.43
200	400	800 1600 3200 6400
# Samples
Bias in Equal Width ECEd∈bias
-6.81	-6.69	-6.61	-6.60	-6.58	-6.55
-6.14	-6.00	-5.93	-5.92	-5.89	-5.87
-4.80	-4.86	-4.79	-4.76	-4.73	-4.71
-3.03	-3.49	-3.64	-3.62	-3.59	-3.56
-0.81	-1.89	-2.49	-2.66	-2.63	-2.62
1.41	-0.05	-1.20	-1.74	-1.89	-1.90
sus*
200	400	800 1600 3200 6400
# Samples
三inf qual Widtj占CESWEEP
-5.80	-6.10	-6.09	-6.00	-5.83	-5.58
200	400	800	1600	3200	6400
# Samples
1.30 0.51
0.08 -0.14 -0.23 -0.26
Bias in Equal Mass ECEd∈bias
-2.42 -2.23 -2.12 -2.09 -2.06 -2.03
816
sus⅛
32
64
-1.59 -1.44
-1.12 -1.02
-0.69 -0.49
-0.45 -0.27
-0.47 -0.23
-1.34 -1.32
-0.94 -0.91
-0.42 -0.39
-0.20 -0.16
-0.13 -0.09
-1.28 -1.25
-0.88 -0.85
-0.35 -0.32
-0.12 -0.10
-0.05 -0.03
200	400	800 1600 3200 6400
# Samples
BiaS in EqUal MaSS ECESWEEP
-1 -0.49	-0.58	-0.58	-0.56	-0.46	-0.38
200	400	800	1600	3200	6400
# Samples
2
2
4
Figure 10:	Bias for various calibration metrics assuming curves fit to CIFAR-100 Wide ResNet-
32 output. We plot bias for various calibration metrics using both equal-width binning (left column)
and equal-mass binning (right column) as we vary both the sample size n and the number of bins b.
17
Under review as a conference paper at ICLR 2021
Bias in Equal Width ECEb∣n
-1.35 -1.72 -1.89 -1.96 -2.00 -2.03
0.23 -0.65 -1.09 -1.31 -1.40 -1.47
816
sus⅛
816
sus⅛
sus*
2.69	1.05
0.11 -0.39 -0.63 -0.77
5.98
3.23
1.53
0.57
0.07
-0.20
6.22
3.50 1.77 0.83 0.31
200	400	800 1600 3200 6400
# Samples
Bias in Equal Width ECEd∈bias
3.61
1.92
0.95
-2.82 -2.40 -2.21 -2.12 -2.08 -2.07
-2.53 -2.00 -1.74 -1.63 -1.56 -1.55
-1.91 -1.44 -1.11
-0.65 -1.09 -0.71
2.20 -0.22 -0.49
6.96
2.56 0.33
-1.00 -0.94 -0.92
-0.59 -0.52 -0.49
-0.38 -0.29 -0.26
-0.24 -0.19 -0.15
200	400	800 1600 3200 6400
# Samples
BlaS In EqUal Wldth ECESWEEP
-0.44 -0.80 -0.88 -0.86 -0.80 -0.72
200	400	800	1600 3200 6400
# Samples
32 7.06
# Samples
Bias in Equal Mass ECEd∈bias
816
sus⅛
64
-2.29 -1.79
-1.83 -1.37
-1.33 -0.79
-1.10 -0.65
-1.15 -0.68
-1.33 -0.90
-1.54 -1.41
-1.15 -1.05
-0.55 -0.44
-0.33 -0.23
-0.29 -0.17
-0.34 -0.16
-1.32 -1.29
-0.98 -0.95
-0.36 -0.35
-0.14 -0.12
-0.08 -0.06
-0.08 -0.05
200	400	800 1600 3200 6400
# Samples
BIaS In EqUal MaSS ECESWEEP
-1 0.73	0.36	0.17	0.01	0.01 -0.03
200	400	800	1600 3200 6400
# Samples
2
2 4
2 4
2
3
2
3
Figure 11:	Bias for various calibration metrics assuming curves fit to ImageNet ResNet-152
output. We plot bias for various calibration metrics using both equal-width binning (left column)
and equal-mass binning (right column) as we vary both the sample size n and the number of bins b.
18
6T
•q smq
jo nqumu θψ PUe U əzts əɪdures θψ ψoq AreA əM Se (umnɪoɔ	Suτuuτq SSeul-gnbə PUe (umnɪoɔ
jjəɪ) Suramq qjpτM-pnba q)oq Smsn sɔnjəuɪ uoμBiq∏B3 SnopreA IoJ əou叩队八 joɪd əʌv QndJno θɪɪ
-8NSeH OrHvHlɔ o∣ 肉 səʌjnɔ SuiiunssB sɔɪjpiu uopυjqιjυ□ sπoijba joj 93uυiJB4Λ :^ɪ əɪngɪj
S9∣dιues #	S9∣dιues #
00V9 00乙£ 009T	008 OOb 00 乙	00V9 00乙£ 0091	008 OOb 00 乙
090	ɛso	9T,τ	69'T	0£3	91W	* 1-5		OSO	£90	S80	0乙T	9L1	T9-3	⅛ 1-5	
d33∕v∖s3j3 sse∣Λ∣ Ienbm uι əɔueijeʌ^								d33∕v∖s3j3 IllP!M IRnbm ui θDueμeΛ∕v							
00179	00Z£	sθ∣dωes # 0091 0Q8		OOV	OOZ			00179	OOZE	sθ∣dωes # 0091 Qgg		OOV	OOZ		
090	WO	OZl	iz8T	GZ/乙	£8力	179		1790	060	Lzτ	9LT	8£乙	6££	179	
090	£80	6TT	gzɪ	LVZ	T6T	乙£		290	880	9乙T	178T	6丁乙	ςς,ε	在	
090	乙80	əɪɪ	TLT	L£Z	εsε	9T	⅛	090	1780	T乙T	乙8 1：	。9.乙	ozɛ	9T	⅛
650	280	əɪɪ	69T	££■乙	9εε	8	OJ 5' S	1750	PLO	60T	591	6£乙	9W	8	OJ 5' S
9S,0	8Z0	oɪɪ	T9 T	0乙乙	TT£	P		TVO	ZSO	£80	q乙T	96«	乙8.乙	P	
ςi7θ	£90	£80	9乙T	ɛzɪ	PPZ	Z		2£0	9170	590	660	9。T	90N	Z	
s∀ιa3α3j3 sse∣∕∖∣ ∣enbg uι əɔuejjeʌʌ								Sviaaa333 LaP州 Ienbm u				! θDueμeΛΛ			
00179	00Z£	sθ∣dωes # 0091 008		OOV	OOZ			00179	OOZE	sθ∣dωes # 0091 008		OOV	OOZ		
090	280	izɪɪ	G9 1	T乙.乙	80T	179		650	8Z0	90T	£。T	£6«	6LZ	179	
650	280	gɪɪ	G9 1	0乙乙	ɛθɛ	乙£		650	080	60T	8。1	86«	“.乙	在	
650	乙80	gɪɪ	99T	乙乙乙	90T	9T	⅛	850	8Z0	ZOT	LVT	96«	£93	9T	⅛
							OJ ɔ'								5 ɔ'
650	T8,0	gɪɪ	99T	q乙乙	80T	8	S	乙GO	TZO	660	6£T	6LT	£。N	8	S
9S,0	8Z0	6oτ		LYZ	£63	P		OizO	950	8Z0	TrT	6。T	OON	P	
ςi7θ	£90	£80	9乙T	TLT	8£.乙	Z		2£0	GTO	290	T60	T乙T	TLT	Z	
Nia333 sse∣∕∖∣ Ienbm uι əɔuejjeʌʌ	NlgmDm i∙ΠP!M Ienbm uι əɔuejjeʌʌ
ɪ COC mɔl JB ɪəded əɔuəɪəjuoɔ e Se mətaəj ιapu∩
OC
^q smq Jo joquιnu
əqj Plre u əzts ə[dɪLreS əqj qjoq Xjbλ əm sb (uuɪnɪoɔ jq§u) Suramq SSeUI-[enbə Plre (uuɪnɪoɔ jjəɪ)
SuτuuτqψpτM-jπnbθψoqSmsn sɔnjəmuoμBiqτjBθ sπotjbλ joj 93ubtjbλΛ joɪdəʌv QndJno Z£-NNSeH
əpɪʌ^ OoI-HVtnO。114 səʌjnɔ SuiiunssB sɔɪjjəiu uopυjqιjυ□ sπoijba joj 93uυiJB4Λ :gɪ əɪngɪj
Oot79	00乙£	S9∣dιues # 009T	008		OOfr	00乙			Q0t79	00乙£	S9∣dιues # 009T	008		OOfr	00乙		
V90	880	。乙.工	τ8-τ	t7t7-3		* τ-5		T90	ZLQ	860	TbT	LLl	T9-3	⅛ 1-5	
d33∕v∖s3j3 sse∣Λ∣ Ienbm uι əɔueijeʌ^								d33∕v∖s3j3 IllP!M IRnbm ui θDueμeΛ∕v							
00179	00Z£	sθ∣dωes # 0091 0Q8		OOV	OOZ			00179	OOZE	sθ∣dωes # 0091 Qgg		OOV	OOZ		
390	£80	9乙T	P8T	692	6乙b	179		T90	£80	9乙T	εsτ	Gb乙	9τε	179	
390	£80	MT	08«	£q.乙	6ZT	乙£		650	580	£乙T	6LT	乙17.乙	oɛɛ	在	
390	£80	ρz τ	08«	8。.乙	Z9T	9T	⅛	ZSO	080	LVT	乙ZT	6£.乙	“£	9T	⅛
T90	580	£乙T	LLT	TVZ	8GW	8	OJ 5' S	εso	ɛzo	9oτ	izςτ	91.乙	δτε	8	OJ 5' S
090	£80	OZl	PLT	9£*乙	GOW	P		£170	590	G60	6ετ	68«	力ZN	P	
650	T8,0	LVT	69T	乙£.乙	乙££	Z		17170	090	880	Z乙T	ɛzɪ	6。乙	Z	
s∀ιa3α3j3 sse∣∕∖∣ ∣enbg uι əɔuejjeʌʌ								Sviaaa333 LaP州 Ienbm u				! θDueμeΛΛ			
00179	00Z£	sθ∣dωes # 0091 008		OOV	OOZ			00179	OOZE	sθ∣dωes # 0091 008		OOV	OOZ		
T90	980	T乙T	TLT	T£■乙	ɛɪɛ	179		650	280	ɛɪɪ	GG I	96«	“.乙	179	
390	980	£乙T	PLT	。£乙	9乙W	乙£		850	280	izɪɪ	8G I	TO,乙	乙〃乙	在	
390	980	£乙T	9LT	8£*乙	sɛɛ	9T	⅛	ZSO	8Z0	ZVl	ssɪ	903	6LZ	9T	⅛
							OJ ɔ'								5 ɔ'
T90	580	£乙T	9L1	9£*乙	δi7ε	8	S	εso	乙ZO	WT	LVT	96«	P9Z	8	S
650	£80	6TT	ɛzɪ	££■乙	9εε	P		£170	590	1760	gɛɪ	08«	LVZ	P	
850	T8,0	LVT	89T	Ob乙	LZZ	Z		17170	090	880	9乙T	QLT	TbN	Z	
Nia333 sse∣∕∖∣ ∣enbg uι θDueμeΛΛ								ni93D≡ M4P!ΛΛ IQnbm Ui				θDueμeΛΛ			
ɪ COC mɔl JB ɪəded əɔuəɪəjuoɔ e Se mətaəj ιapu∩
IZ
,q suιq
jo nqumu 叫］PUe U əzis 0［dures 叫］φoq Aiba əM Se (uuɪnɪoɔ jq§n) 8uιuuιq SSeUI-［enbo PUe (uuɪnɪoɔ
jjəɪ) Suiumq qjpiM-pnbo qjoq §uisn sɔnjəuɪ uopniqipo sπoπbλ ioj qoubπbλΛ joɪd əʌv QndlIW 苔［
-jə^sə^ jaχ［9SBmι oj jy səʌjɪiɔ SuiiunssB sɔujəuɪ uoιjBiqι∣β3 snouυA λoj əɔumjnʌʌ :什［ojπSij
S9∣dιues #
Oot79	00乙£	009T	008	00。	00乙
GGO	9Δ0	sot	£G"	or乙	383
用MSmDm ssr∣λ∣ IRnbm 3 ədur.RAP
⅛ Bms
r
S9∣dιues #
00V9 00乙£ 009T	008	00t7	OO 乙
GGO ££ 0	96 0	631 WT TZ3
⅛ Bms
r
用MSmDm IIIP!M IRnbm 3 θ3URμRAp
sθ∣dωes #
00179 OOZE 0091 008	0017 OOZ
sθ∣dωes #
00179 OOZE 0091 008	0017 OOZ
ZSO	乙80	17乙T	GO,乙		29G
ZSO	080	lzɪɪ	8LT	ɛɪɛ	£817
950	ZZO	θɪɪ	99T	TV乙	91力
GGo	9Z0	LQl	851	GV乙	08£
ESO	IzZZO	ɛθɪ	OST	9乙乙	817£
GGo	ZZO	LQl	1751	6乙乙	TVW
Φ9
s∀ιa3α3j3 sse∣∕∖∣ ∣enbg uι əɔuejjeʌ/v
sθ∣dωes #
00179 00乙£ 0091	008	00。	002
v
ZE
⅛ Bins
I
9 8
650	T60	8i7,τ	τvz	zςτ	0217
850	580	iz3,τ	ZYZ	ςzτ	T6.17
950	T8,0	gɪɪ	6LT	9乙W	5817
GGO	8Z0	6oτ	£9«	i7zτ	9乙b
OGo	890	G60	sɛɪ	乙乙乙	9GW
2170	ZSO	080	LVT	zzɪ	983
179
V
ZE
⅛-ns
9τ8
svιa≡a333 qιpι∕v∖ Ienbm uι əɔuejjeʌʌ
sθ∣dωes #
00179 00乙£ 0091	008	00。	002
Nia333 sse∣∕∖∣ ∣enbg uι θDueμeΛΛ
v
ZE
Φ9
⅛ Bins
I
9 8
ɛso 2£0
i7ς 0 ςz o
i7ς 0 ςz o
i7ς 0 ςz o
6170 £90
τi7θ 950
£6 0 Z1乙 T
860 ɛɛɪ
ɪθ,ɪ 6£I
ɪθ'ɪ 0i7T
T6 0 Z1乙 T
8Z0 乙 TT
89T	6乙.乙
εs,τ τvz
06τ 9f 乙
£6 1 179,2
τsτ OGN
6GI 6乙.乙
179
ZE
⅛ Bms
9τ8
ni93D≡ q4P!ΛΛ IQnbm uι əɔuejjeʌʌ
V
ɪ COC mɔl JB ɪəded əɔuəɪəjuoɔ e Se mətaəj ιapu∩
Under review as a conference paper at ICLR 2021
C What number of bins does equal mass ECESWEEP choose
(a) Uncalibrated model.
Figure 15: Bins chosen by equal mass ECESWEEP method. We plot equal mass ECEBIN % versus
number of bins for various sample sizes n. We highlight the TCE with a horizontal dashed line and
show the average number of bins chosen by the ECESWEEP method for different sample sizes with
vertical dashed lines. When the model is uncalibrated (left) ECESWEEP chooses a bin number that
is close to optimal. However, for perfectly calibrated models (right), the optimal number of bins is
small (<=4), and ECESWEEP does not do a good job of selecting a good bin number. The incorrect
bin selection may partially explain why ECESWEEP still has some bias for perfectly calibrated models.
However, we note that any binning-based technique that always outputs a positive number will never
be completely unbiased for perfectly calibrated models.
(b) Perfectly calibrated model.
For Figure 15, the uncalibrated plot assumes EY [Y | f (X) = c] = logistic(10 * C - 5) while the
calibrated plot assumes EY [Y | f (X) = c] = c. Both experiments assume f (x)〜Uniform(0,1).
22
Under review as a conference paper at ICLR 2021
D Differences in ew_ece_bin vs. em_ece_sweep
In Table 4 we compare the values for ECEBIN reported in Guo et al. (2017) Table 1 against our
computation of the same quantities using 15 equal-width bins, but using the logits reported in
Kangsepp (2019). We report both absolute and relative differences between these two quantities. The
table has rows sorted according to the ECEBIN obtained in Guo et al. (2017).
Table 4: Comparison of ECE values (and associated rank orderings) computed using ew_ece_bin vs
em_ece_sweep from uncalibrated logits.
Uncalibrated ECE(%)	x=ew_bin	y=em_sweep	x-y	100(x-y)/x
resnet110_SD_c10	4.11(0)	4.10(0)	0.01	0.24
resnet_wide32_c10	4.51(1)	4.48(1)	0.03	0.66
resnet110_c10	4.75(2)	4.75(2)	-0.00	-0.00
densenet40_c10	5.50(3)	5.49(3)	0.01	0.13
densenet161_imgnet	5.72(4)	5.72(4)	-0.00	-0.00
resnet152_imgnet	6.54(5)	6.54(5)	0.00	0.00
resnet110_SD_c100	15.86(6)	15.83(6)	0.03	0.18
resnet110_c100	18.48(7)	18.48(7)	0.00	0.00
resnet_wide32_c100	18.78(8)	18.78(8)	-0.00	-0.00
densenet40_c100	21.16(9)	21.16(9)	0.00	0.00
Table 5: Comparison of ECE values (and associated rank orderings) computed using ew_ece_bin
vs em_ece_sweep from logits calibrated using temperature scaling Guo et al. (2017). Red indicates
differences in the sorted order of each entry in the column.
Temp. scaling(%)	x=ew_bin	y=em_sweep	x-y	100(x-y)/x
resnet110_SD_c10	0.56(0)	0.36(1)	0.19	34.99
resnet_wide32_c10	0.78(1)	0.21(0)	0.57	72.65
densenet40_c100	0.90(2)	0.72(2)	0.18	20.45
densenet40_c10	0.95(3)	0.90(3)	0.05	4.98
resnet110_c10	1.13(4)	0.91(4)	0.23	19.90
resnet110_SD_c100	1.21(5)	0.98(5)	0.23	18.96
resnet_wide32_c100	1.47(6)	1.31(6)	0.16	10.82
densenet161_imgnet	1.94(7)	1.88(8)	0.06	3.01
resnet152_imgnet	2.08(8)	2.13(9)	-0.05	-2.59
resnet110_c100	2.38(9)	1.87(7)	0.51	21.38
23
Under review as a conference paper at ICLR 2021
Table 6: Comparison of ECE values (and associated rank orderings) computed using ew_ece_bin vs
em_ece_sweep from logits calibrated using isotonic regression Zadrozny & Elkan (2002).
Isotonic regression(%)	x=ew_bin	y=em_sweep	x-y	100(x-y)/x
resnet110_SD_c10	1.03(0)	0.70(0)	0.33	32.15
resnet_wide32_c10	1.19(1)	0.77(1)	0.42	35.41
resnet110_c10	1.47(2)	0.93(2)	0.55	37.14
densenet40_c10	1.68(3)	1.61(3)	0.08	4.66
densenet161_imgnet	4.64(4)	4.64(4)	-0.00	-0.00
resnet110_SD_c100	4.89(5)	4.86(5)	0.04	0.81
densenet40_c100	5.01(6)	4.95(6)	0.06	1.14
resnet152_imgnet	5.15(7)	5.11(7)	0.04	0.74
resnet_wide32_c100	5.76(8)	5.64(8)	0.12	2.15
resnet110_c100	6.19(9)	6.05(9)	0.15	2.40
Table 7: Comparison of ECE values (and associated rank orderings) computed using ew_ece_bin
vs em_ece_sweep from logits calibrated using histogram binning Zadrozny & Elkan (2001). Red
indicates differences in the sorted order of each entry in the column.
Histogram binning(%)	x=ew_bin	y=em_sweep	x-y	100(x-y)/x
resnet_wide32_c10	0.56(0)	0.56(0)	0.00	0.00
resnet110_SD_c10	0.62(1)	0.57(1)	0.05	7.61
resnet152_imgnet	0.78(2)	0.76(4)	0.02	2.14
densenet161_imgnet	0.80(3)	0.64(2)	0.16	19.86
densenet40_c100	0.81(4)	0.80(5)	0.01	1.02
resnet110_c10	0.84(5)	0.67(3)	0.17	20.67
resnet110_c100	0.91(6)	0.91(6)	0.00	0.00
densenet40_c10	1.27(7)	1.27(7)	-0.00	-0.00
resnet_wide32_c100	1.45(8)	1.45(8)	0.00	0.00
resnet110_SD_c100	1.58(9)	1.55(9)	0.03	1.82
24