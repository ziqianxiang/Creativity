Under review as a conference paper at ICLR 2021
Unified Principles For Multi-Source Transfer
Learning Under Label Shifts
Anonymous authors
Paper under double-blind review
Ab stract
We study the label shift problem in multi-source transfer learning and derive new
generic principles. Our proposed framework unifies the principles of conditional
feature alignment, label distribution ratio estimation and domain relation weights
estimation. Based on inspired practical principles, we provide unified practical
framework for three multi-source label shift transfer scenarios: learning with lim-
ited target data, unsupervised domain adaptation and label partial unsupervised
domain adaptation. We evaluate the proposed method on these scenarios by ex-
tensive experiments and show that our proposed algorithm can significantly out-
perform the baselines.
1	Introduction
Transfer learning (Pan & Yang, 2009) is based on the motivation that learning a new task is easier
after having learned several similar tasks. By learning the inductive bias from a set of related source
domains (S1, . . . , ST) and then leveraging the shared knowledge upon learning the target domain
T, the prediction performance can be significantly improved. Based on this, transfer learning arises
in deep learning applications such as computer vision (Zhang et al., 2019; Tan et al., 2018; Hoffman
et al., 2018b), natural language processing (Ruder et al., 2019; Houlsby et al., 2019) and biomedical
engineering (Raghu et al., 2019; Lundervold & Lundervold, 2019; Zhang & An, 2017).
To ensure a reliable transfer, it is critical to understand the theoretical assumptions between the
domains. One implicit assumption in most transfer learning algorithms is that the label proportions
remain unchanged across different domains (Du Plessis & Sugiyama, 2014) (i.e., S(y) = T (y)).
However, in many real-world applications, the label distributions can vary markedly (i.e. label
shift) (Wen et al., 2014; Lipton et al., 2018; Li et al., 2019b), in which existing approaches cannot
guarantee a small target generalization error, which is recently proved by Combes et al. (2020).
Moreover, transfer learning becomes more challenging when transferring knowledge from multiple
sources to build a model for the target domain, as this requires an effective selection and leveraging
the most useful source domains when label shift occurs. This is not only theoretically interesting
but also commonly encountered in real-world applications. For example, in medical diagnostics, the
disease distribution changes over countries (Liu et al., 2004; Geiss et al., 2014). Considering the task
of diagnosing a disease in a country without sufficient data, how can we leverage the information
from different countries with abundant data to help the diagnosing? Obviously, naively combining
all the sources and applying one-to-one single source transfer learning algorithm can lead to unde-
sired results, as it can include low quality or even untrusted data from certain sources, which can
severely influence the performance.
In this paper, we study the label shift problem in multi-source transfer learning where St(y) 6= T (y).
We propose unified principles that are applicable for three common transfer scenarios: unsupervised
Domain Adaptation (DA) (Ben-David et al., 2010), limited target labels (Mansour et al., 2020) and
partial unsupervised DA with supp(T (y)) ⊆ supp(St(y)) (Cao et al., 2018), where prior works
generally treated them as separate scenario. It should be noted that this work deals with target shift
without assuming that semantic conditional distributions are identical (i.e., St(x|y) 6= T (x|y)),
which is more realistic for real-world problems. Our contributions in this paper are two-folds:
(I)	We propose to use Wasserstein distance (Arjovsky et al., 2017) to develop a new target gen-
eralization risk upper bound (Theorem 1), which reveals the importance of label distribution ratio
1
Under review as a conference paper at ICLR 2021
estimation and provides a principled guideline to learn the domain relation coefficients. Moreover,
we provide a theoretical analysis in the context of representation learning (Theorem 2), which guides
to learn a feature function that minimizes the conditional Wasserstein distance as well as controls
the weighted source risk. We further reveal the relations in the aforementioned three scenarios lie in
the different assumptions for estimating label distribution ratio.
(II)	Inspired by the theoretical results, we propose Wasserstein Aggregation Domain Network
(WADN) for handling label-shift in multi-source transfer learning. We evaluate our algorithm on
three benchmark datasets, and the results show that our algorithm can significantly outperform state-
of-the-art principled approaches.
2	Related Work
Multi-Source Transfer Learning Theories have been investigated in the previous literature with
different principles to aggregate source domains. In the popular unsupervised DA, (Zhao et al., 2018;
Peng et al., 2019; Wen et al., 2020; Li et al., 2018b) adopted H-divergence (Ben-David et al., 2007),
discrepancy (Mansour et al., 2009) and Wasserstein distance (Arjovsky et al., 2017) of marginal dis-
tribution d(St(x), T (x)) to estimate domain relations and dynamically leveraged different domains.
These algorithms generally consists source risk, domain discrepancy and an un-observable term η,
the optimal risk on all the domains, which are ignored in these approaches. However, as Combes
et al. (2020) pointed out, ignoring the influence of η will be problematic when label distributions
between source and target domains are significantly different. Therefore it is necessary to take η into
consideration by using a small amount of labelled data is available for the target domain (Wen et al.,
2020). Following this line, very recent works (Konstantinov & Lampert, 2019; Wang et al., 2019a;
Mansour et al., 2020) started to consider measure the divergence between two domains given label
information for the target domain by using Y -discrepancy (Mohri & Medina, 2012). However, we
empirically showed these methods are still unable to handle label shift.
Label-Shift Label-Shift (Zhang et al., 2013; Gong et al., 2016) is a common phenomena in the
transfer learning with S(y) 6= T(y) and generally ignored by the previous multi-source transfer
learning practice. Several theoretical principled approaches have been proposed such as (Azizzade-
nesheli et al., 2019; Garg et al., 2020). In addition, (Combes et al., 2020; Wu et al., 2019) analyzed
the generalized label shift problem in the one-to-one single unsupervised DA problem but did not
provide guidelines of levering different sources to ensure a reliable transfer, which is more chal-
lenging. (Redko et al., 2019) proposed optimal transport strategy for the multiple unsupervised DA
under label shift by assuming identical semantic conditional distribution. However they did not con-
sider representation learning in conjunction with their framework and did not design neural network
based approaches. Different from these, we analyzed our problem in the context of representation
learning and propose an efficient and principled strategies. Moreover, our theoretical results high-
lights the importance of label shift problem in a variety of multi-source transfer problem. While the
aforementioned work generally focus on the unsupervised DA problem, without considering unified
rules for different scenarios (e.g. partial multi-source DA).
3	Theoretical Insights: Transfer Risk Upper B ound
We assume a scoring hypothesis defined on the input space X and output space Y with h : X × Y →
R, is K-Lipschitz w.r.t. the feature x (given the same label), i.e. for ∀y, kh(x1, y) - h(x2, y)k2 ≤
Kkx1 - x2k2, and the loss function ` : R × R → R+ is positive, L-Lipschitz and upper bounded
by Lmaχ. We denote the expected risk w.r.t distribution D: RD(h) = E(x,y)〜D'(h(χ, y)) and its
empirical counterpart (w.r.t. D) RD(h) = E(X y)∈D^ '(h(x, y)). We adopted Wasserstem-1 distance
(Arjovsky et al., 2017) as a metric to measure the similarity of the domains. Compared with other
divergences, Wasserstein distance has been theoretically proved tighter than TV distance (Gong
et al., 2016) or Jensen-Shnannon divergence (Combes et al., 2020).
Based on previous work, the label shift is generally handled by label-distribution ratio weighted loss:
Ra(h) = E(XV)〜Sα(y)'(h(x,y)) with α(y) = T(y)/S(y). We also denote α^t as its empirical
counterpart, estimated from samples. Besides, to measure the task relations, we define a simplex
λ with λ[t] ≥ 0, PtT=1 λ[t] = 1 as the task relation coefficient vector by assigning high weight to
2
Under review as a conference paper at ICLR 2021
the similar task. Then we first present Theorem 1, which proposed theoretical insights about how to
combine source domains through properly estimating λ.
Theorem 1.	Let {St = {(xi,yi)}NSt }T=1 and T = {(xi, yi)}NT1, respectively be T source and
target i.i.d. samples. For ∀h ∈ H with H the hypothesis family and ∀λ, with high probability
≥ 1 - 4δ, the target risk can be upper bounded by:
Rτ(h) ≤ X λ[t]RSt (h)+ LKX λ[t]Ey~T(y)Wι(T(x∣Y = y)kSt(x∣Y = y))+ Lmaχd∞pt X 苧^ogNδ
t	t	t=1 βt	2N
+ LmaX sup ∣∣αt - αtk2 + Comp (Ns、，…，Nst ,N ,δ),
t
where N = PtT=1 NSt and βt = NSt /N and ds∞up = maxt∈[1,T],y∈[1,Y] αt (y) the maximum true
label distribution ratio value. Wι(∙∣∙) is the Wasserstein-I distance with L-distance as cost func-
tion. Comp(NS1 , . . . , NST , NT , δ) is a function that decreases with larger NS1, . . . , NT, given a
fixed δ and hypothesis family H. (See Appendix Efor details)
Remarks (1) In the first two terms, the relation coefficient λ is controlled by αt-weighted loss
RSt(h) and conditional Wasserstein distance Ey〜t@)Wi(T(x|Y = y)∣St(x∣Y = y)). Tomin-
imize the upper bound, we need to assign a higher λ[t] to the source t with a smaller weighted
prediction loss and a smaller weighted semantic conditional Wasserstein distance. Intuitively, we
tend to leverage the source task which is semantic similar to the target and easier to learn. (2) If
each source have equal observations with βt = 1, then the third term will become ∣λ∣2, a L2 norm
regularization, which can be viewed as an encouragement of uniformly leveraging all the sources.
Combing these three terms, we need to consider the trade-off between assigning a higher λ[t] to
the source t that has a smaller weighted prediction loss and conditional Wasserstein distance, and
assigning balanced λ[t] for avoiding concentrating on only one source. (3) ∣∣<^t -而、indicates
the gap between ground-truth and empirical label ratio. Therefore if We can estimate a good 0⅛,
these terms can be small. In the practice, If target labels are available, α can be computed from
the observed data and α → αt. If target labels are absent (unsupervised DA), we need to design
methods and to properly estimate α (Sec. 4). (4) COmP(NSI,..., NST, NT, δ) is a function that
reflects the convergence behavior, which decreases with larger observation numbers. If we fix H, δ,
N and NT, this term can be viewed as a constant.
Insights in Representation Learning Apart from Theorem 1, we propose a novel theoretical
analysis in the context of representation learning, which motivates practical guidelines in the deep
learning regime. We define a stochastic feature function g and we denote its conditional distribution
w.r.t. latent variable Z (induced by g) as S(z|Y = y) = x g(z|x)S(x|Y = y)dx. Then we have:
Theorem 2.	We assume the settings of loss, the hypothesis are the same with Theorem 1. We further
denote the stochastic feature learning function g : X → Z, and the hypothesis h : Z × Y → R.
Then ∀λ, the target risk is upper bounded by:
RT(h,g) ≤ Xλ[t]RSt(h,g) + LKXλ[t]Ey"(y)W1(St(z∣Y = y)∣T(z∣Y = y)),
where RT(h, g) = E(x,y)〜T(x,y)Ez〜g(z∣x)'(h(z, y)).
Theorem 2	reveal that to control the upper bound, we need to learn g that minimizes the weighted
conditional Wasserstein distance and learn (g, h) that minimizes the weighted source risk.
Comparison with previous Theorems. Our theory proposed an alternative prospective to under-
stand transfer learning. The first term is α-weighted loss. And it will recover the typical source loss
minimization if there is no label shift with αt(y) ≡ 1 (Li et al., 2019a; Peng et al., 2019; Zhao et al.,
2018; Wen et al., 2020). Beside, minimizing the conditional Wasserstein distances has been shown
to be advantageous, compared with W1 (St(z)∣T(z)) (Long et al., 2018). Moreover, Theorem 2 ex-
plicitly proposed the theoretical insights about the representation learning function g, which remains
elusive for previous multi-source transfer theories such as (Wang et al., 2019a; Mansour et al., 2020;
Konstantinov & Lampert, 2019; Li et al., 2019a; Peng et al., 2019).
3
Under review as a conference paper at ICLR 2021
Table 1: Practical principles under different scenarios
Transfer Scenarios	I Learn (g, h) ∣ Estimate at ∣	I Estimate λ
Multi-Source Transfer with Limited Target Data	I T(y)∕St(y)	
Unsupervised Multi-Source DA	l Sec. 4.1 I	l Sec. 4.2 I	I Sec. 4.3
Partial Unsupervised Multi-Source DA	I	I	
4 Unified Practical Framework in Deep Learning
The theoretical results in Section 3 motivate general principles to folloW When designing multi-
source transfer learning algorithms. We summarize those principles in the folloWing rules.
(I)	Learn a g that minimizes the Weighted conditional Wasserstein distance as Well as learn (g, h)
that minimizes the αt-weighted source risk (Sec. 4.1).
(II)	Properly estimate the label distribution ratio (^t (Sec. 4.2).
(III)	Balance the trade-off between assigning a higher λ[t] to the source t that has a smaller weighted
prediction loss and conditional Wasserstein distance, and assigning balanced λ[t]. (Sec. 4.3).
We instantiate these rules with a unified practical framework for solving multi-source transfer learn-
ing problems, as shown in Tab 1. We would like to point out that our original theoretical result is
based on setting with the available target labels. The propoSed algorithm Can be applied to unSuper-
viSed SCenarioS under additional aSSumptionS.
4.1	Guidelines in the representation learning
Motivated by Theorem 2, given a fixed label ratio estimation α^t and fixed λ, We should find a
representation function g : X → Z and a hypothesis function h : Z × Y → R such that:
m3[ λ[t]RSt(h,g) + Co A λ[t]Ey^T(y)Wι(,St(z∣Y = y)kT(z∣Y = y))
(1)
Explicit Conditional Loss When target label information is available, one can expliCitly solve
the conditional optimal transport problem with g and h for a given Y = y . However, due to the
high computational complexity in solving T × |Y | optimal transport problems, the original form is
practically intractable. To address this issue, we propose to approximate the conditional distribution
on latent space Z as Gaussian distribution with identical Covariance matrix such that St(z|Y =
y) ≈N (cy, Σ) and T(z∣Y = y) ≈N (Cy, Σ). Thenwe have W^ι(<St(z∣Y = y)∣∣T(z∣Y = y)) ≤
kCty - Cy k2 (see Appendix G for details). Intuitively, the approximation term is equivalent to the
well known feature mean matChing (Sugiyama & Kawanabe, 2012), which computes the feature
centroid of each class (on latent space Z) and aligns them by minimizing their L2 distance.
Implicit Conditional Loss When target label information is not available (e.g. unsupervised
DA and partial DA), the explicit matching approach can adopt pseudo-label predicted by the hy-
pothesis h as a surrogate of the true target label. However, in the early stage of the learn-
ing process, the pseudo-labels can be unreliable, which can lead to an inaccurate estimate of
W1 (S(z|Y = y)kT(z|Y = y)). To address this, the following Lemma indicates that estimat-
ing the conditional Wasserstein distance is equivalent to estimating the Wasserstein adversarial loss
weighted by the label-distribution ratio.
Lemma 1. The weighted Conditional WaSSerStein diStanCe Can be impliCitly expreSSed aS:
Eλ[t]Ey 〜T (y)Wι(St(ZIY = y)kT(ZIY = y))=
t
where at(z)
max): λ[t∏Ez~St(z) αt (Z) dt (Z) ―Ez~T(z) dt(z)],
d1,…，dT
1{(z,y)^St}at(Y = y), and dι,...,dτ : Z → R+ are the I-LipSChitz domain
discriminators (Ganin et al., 2016; Arjovsky et al., 2017).
Lemma 1 reveals that instead of using pseudo-labels to estimate the weighted conditional Wasser-
stein distance, one can train T domain discriminators with weighted Wasserstein adversarial loss,
4
Under review as a conference paper at ICLR 2021
which does not require the pseudo-label of each target sample during the matching. On the other
hand, α can be obtained from c^t, which will be elaborated in Sec. 3.2.
In practice, we adopt a hybrid approach by linearly combining the explicit and implicit matching
strategies for all the scenarios, in which empirical results show its effectiveness.
4.2	Estimate label distribution ratio Ct
Multi-Source Transfer with target labels When the target labels are available, Ct can be directly
estimated from the data without any assumption and Ct → Ct can be proved from asymptotic
statistics.
Unsupervised Multi-Source DA In this scenario, it is impossible to estimate a good Ct with-
out imposing any additional assumptions. Following (Zhang et al., 2013; Lipton et al., 2018;
Azizzadenesheli et al., 2019; Combes et al., 2020), we assume that the conditional distributions
are aligned between the target and source domains (i.e., St(z|y) = T (z|y)). Then, we denote
St(y), T(y) as the predicted t-source/target label distribution through the hypothesis h, and also
define C^ɪ [y, k] = S>t[argmaxyoh(z,y0) = y, Y = k] is the t-source prediction confusion ma-
trix . We can demonstrate that if the conditional distribution is aligned, We have T(y) = Tat (y),
with Tat (Y = y) = PY=I Cst [y, k]Ct(k) the constructed target prediction distribution from the
t-source information. (See Appendix I for the proof). Then We can estimate Ct through matching
these two distributions by minimizing DKL(T(y)kT^t (y)), which is equivalent to:
|Y|	|Y|	|Y|
min	- X T(y)log(X Cst[y, k]Ct(k))	s.t ∀y	∈ Y ,Ct(y)	≥ 0,	X Ct(y)St(y)	= 1 ⑵
t	y=1	k=1	y=1
in the aforementioned part, we have assumed the conditional distribution is aligned, which is a
feasible requirement in our algorithm, since the goal of g exactly aims at gradually achieving this.
In the experiments, we iteratively estimate Ct and learn g.
Unsupervised Multi-Source Partial DA When supp(T (y)) ⊆ supp(St(y)), Ct is sparse due to
the non-overlapped classes. Accordingly, in addition to the assumption of St(z|y) = T (z|y) as
in unsupervised DA, we also impose such prior knowledge by adding a regularizer IlCtkI to the
objective of Eq. (2) to induce sparsity in (^t (See Appendix J for more details).
in training the neural network, since the non-overlapped classes will be automatically assigned with
a small or zero Ct, (g, h) will be less affected by the classes with small Ct. Our empirical results
effectively validate its capability in detecting non-overlapping classes and show significant improve-
ments over other baselines.
4.3	ES TiMATE TASK RELATioN CoEFFiCiENT λ
Inspired by Theorem 1, given fixed (^t and (g, h), we estimate λ through optimizing the derived
upper bound.
uT
min Xλ[t]R^St(h,g) + CoXλ[t]Ey^T(y)Wι(∙T(z∣Y = y)∣∣S(z∣Y = y)) + A Xλβp-
t	t	t=1	t (3)
T
s.t ∀t, λ[t] ≥ 0, X λ[t] = 1
t=1
a
In practice, Rst(h,g) is the weighted empirical prediction error and Ey〜T(y)Wi(T(z|Y =
y)∣S(z∣Y = y)) is approximated by the dynamic feature centroid distance Py T(y)∣Cy 一 Cy ∣∣2
(See Appendix L for details). Thus, solving λ is a standard convex optimization problem.
4.4 Algorithm Description
Based on the aforementioned components, we present the description of WADN (Algorithm 1) in
the unsupervised scenarios (UDA and Partial DA), which iteratively updates (g, h), Ct, and λ. When
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Wasserstein Aggregation Domain Network (unsupervised scenarios, one iteration)
Require: Labeled source samples Si,.
. . , ST, Target samples T
Ensure: Label distribution ratio α^t, task relation simplex λ. Feature Function g, Classifier h, Do-
main critic function d1, . . . , dT, class centroid for source Cty and target Cy (∀t = [1, T], y ∈ Y).
1:
2:
3:
4:
5
6
7
8
9
10:
11:
12:
13:
14:
15:
. . . DNN Parameter Training Stage (fixed αt and λ) / / /
for mini-batch of samples (xs1,ys1)~ Si, ..., (XST, YST) ~ ST, (XT) ~ T do
Predict target pseudo-label NT = argmaXyh(g(xT), y)
Compute source confusion matrix for each batch (un-normalized)
CSt = #[argmaXy0 h(Z, yO) = y, Y = k] (t = 1, . . . , T)
Compute the batched class centroid for source Cty and target Cy.
Moving Average for update source/target class centroid: (i = 0.7)
Update Source class centroid Cty = i ×Cty+(1-i)×Cty
Update Target class centroid Cy = i × Cy + (1 - i) × Cy
Updating g, h, di, . . . , dT (SGD and Gradient Reversal), by solving:
min ,max E,λ[t]RSXhM+eCoE ,λ[t]Ey-∙T(y)kCy -
g,h d1 ,...,dT	t	t
S--------------{--------------}
Classification Loss
V----------------------{-----------
Explicit Conditional Loss
Cyk2
+ (1- e)Co Etλ[t][Ez~St(z)at(z)d(z) - E^^z)]
、	、z	,
≡{^^^^^^^^^≡
Implicit Conditional Loss
end for
...Estimation α^t and λ / / /
Compute the global(normalized) source confusion matrix
CSt = * * * Stlargmaxy0 h(z, yO = y, Y = k] (t = 1, . . . , T)
Solve αt (denoted as {α0t}tT=i) by (Sec. 4.2 Unsupervised DA or Partial UDA).
Update αt by moving average: αt = i × αt + (1 - i) × α0t
Compute the weighted loss and weighted centroid distance, then solve λ (denoted as λ0) from
Sec. 4.3. And updating λ by moving average: λ = i × λ + (1 - i) × λ0
updating λ and αt, we used package CVXPY to optimize the two standard convex losses after each
training epoch, then we updating them by using the moving average. As for WADN under target
label information, We did not require pseudo-label and directly compute at, shown in Appendix L.
5	Experiments
In this section, we compare proposed approaches with several baselines for the popular tasks. For
all the scenarios, the following baselines are evaluated: (I) Source method applied only labelled
source data to train the model. (II) DANN (Ganin et al., 2016). We follow the protocol of Wen et al.
(2020) to merge all the source dataset as a global source domain. (III) MDAN (Zhao et al., 2018);
(IV) MDMN (Li et al., 2018b); (V) M3SDA (Peng et al., 2019) adopted maximizing classifier dis-
crepancy (Saito et al., 2018) and (VI) DARN (Wen et al., 2020). For the conventional multi-source
transfer and partial unsupervised multi-source DA, we additionally compare specific baselines. All
the baselines are re-implemented in the same network structure for fair comparisons. The detailed
network structures, hyper-parameter settings, training details are put in Appendix M.
We evaluate the performance on three different datasets: (I) Amazon Review. (Blitzer et al., 2007) It
contains four domains (Books, DVD, Electronics, and Kitchen) with positive and negative product
reviews. We follow the common data pre-processing strategies as (Chen et al. (2012)) to form a
5000-dimensional bag-of-words feature. Note that the label distribution in the original dataset is
uniform. To enhance the benefits of the proposed approach, we create a label distribution drifted
task by randomly dropping 50% negative reviews of all the sources while keeping the target identical.
(show in Fig.3 (a)). (II) Digits. It consists four digits recognition datasets including MNIST, USPS
(Hull, 1994), SVHN (Netzer et al., 2011) and Synth (Ganin et al., 2016). We also create a slight
label distribution drift for the sources by randomly dropping 50% samples on digits 5-9 and keep
6
Under review as a conference paper at ICLR 2021
target identical. (showed in Fig.(3)(b)). (III) Office-Home Dataset (Venkateswara et al., 2017).
It contains 65 classes for four different domains: Art, Clipart, Product and Real-World. We used
the ResNet50 (He et al., 2016) pretrained from the ImageNet in PyTorch as the base network for
feature learning and put a MLP for the classification. The label distributions in these four domains
are different and we did not manually create a label drift (showed in Fig.3 (c)).
5.1	Unsupervised Multi-Source DA
In the unsupervised multi-source DA, we evaluate the proposed approach on all the three datasets.
We use a similar hyper-parameter selection strategy as in DANN (Ganin et al., 2016). All reported
results are averaged from five runs. The detailed experimental settings are illustrated in Appendix
M. The empirical results are illustrated in Tab. 7, 2 and 3. Since we did not change the target label
distribution throughout the whole experiments, then we still use the target accuracy as the metric.
We report the means and standard deviations for each approach. The best approaches based on a
two-sided Wilcoxon signed-rank test (significance level p = 0.05) are shown in bold.
Table 2: Unsupervised DA: Accuracy (%) on the Source-Shifted Digits.
Target	MNIST	SVHN	SYNTH	USPS	Average
Source	84.93±i.50	67.14±i.40	78.11±1.31	86.02±1.12	79.05
DANN	86.99±i.53	69.56±2,6	78.73±1.30	86.81±1.74	80.52
MDAN	87.86±2,4	69.13±i.56	79.77±1.69	86.50±1.59	80.81
MDMN	87.31±i.88	69.84±i.59	80.27±0.88	86.61±1.41	81.00
M3SDA	87∙22±i.70	68.89±i.93	80.01±1.77	86.39±1.68	80.87
DARN	86.98±i.29	68.59±i.79	80.68±0.61	86.85±1.78	80.78
WADN	89.07±o.72	71.66±o.77	82.06±0.89	90.07±1.10	83.22
Full TAR	98.70±o.i5	85.20±o.09	95.10±0.14	96.64±0.13	93.91
Table 3: Unsupervised DA: Accuracy (%) on Office-Home
Target	Art	Clipart	Product	Real-World	Average
Source	49.25±0.6o	46.89±0.6i	66.54±1.72	73.64±0.91	59.08
DANN	50.32±0.32	50.11±ι.i6	68.18±1.27	73.71±1.63	60.58
MDAN	67.93±0.36	66.61±i.32	79.24±1.52	81.82±0.65	73.90
MDMN	68.38±0.58	67.42±0.53	82.49±0.56	83.32±1.93	75.28
M3SDA	63.77±i.07	62.30±0.44	75.85±1.24	79.92±0.60	70.46
DARN	69.89±0.42	68.61±o.5o	83.37±0.62	84.29±0.46	76.54
WADN	73.78±i143	70.18±0.54	86.32±0.38	87.28±0.87	79.39
Full TAR	76.17±o.i6	79.37±o.22	90.60±0.24	87.65±0.18	83.45
The empirical results reveal a significantly better performance (≈ 3%) on different datasets. For
understanding the working principles of WADN, we evaluate the performance under different levels
of source label shift in Amazon Review dataset (Fig.1(a)). The results show strong practical benefits
for WADN during a gradual larger label shift. In addition, we visualize the task relations in digits
(Fig.1(b)) and demonstrate a non-uniform λ, which highlights the importance of properly choosing
the most related source rather than simply merging all the data. E.g. when the target domain is
SVHN, WADN mainly leverages the information from SYNTH, since they are more semantically
similar and MNIST does not help too much for SVHN (observed by Ganin et al. (2016)). The
additional analysis and results can be found in Appendix O.
5.2	Multi-Source Transfer Learning with Limited Target Samples
We adopt Amazon Review and Digits in the multi-source transfer learning with limited target sam-
ples, which have been widely used. In the experiments, we still use shifted sources. We randomly
sample only 10% labeled samples (w.r.t. target dataset in unsupervised DA) as training set and the
rest 90% samples as the unseen target test set. (See Appendix M for details). We adopt the same
7
Under review as a conference paper at ICLR 2021
lωN≡NH>S
HINASSdSn
0.21
0.25
0.16
MNIST SVHN SYNTH USPS
b
Figure 1: (a) Unsupervised DA with Amazon Review dataset. Accuracy under different levels of
shifted sources (higher dropping rate means larger label shift). The results are averaged on all target
domains. See the results for each task in Fig. (12). (b) Visualization of λ in unsupervised DA, each
row corresponds to one target domain. (c) Transfer learning with limited target labels in USPS.
The performance of WADN is consistently better under different target samples (smaller portion
indicates fewer target samples).
hyper-parameters and training strategies with unsupervised DA. We specifically add a recent base-
line RLUS (Konstantinov & Lampert, 2019) and MME (Saito et al., 2019), which also considered
transfer learning with the labeled target.
Table 4: Multi-source Transfer: Accuracy (%) on Source-Shifted Amazon Review
Target	Books	DVD	Electronics	Kitchen	Average
Source + Tar	72.59±i.89	73.02±i.84	81.59±1.58	77.03±1.73	76.06
DANN	67.35±2.28	66.33±2.42	78.03±1.72	74.31±1.71	71.50
MDAN	68.70±2.99	69.30±2.2i	78.78±2.21	74.07±1.89	72.71-
M3SDA	69.28±i.78	67.40±o.46	76.28±0.81	76.50±1.19	72.36
DARN	68.57±i.35	68.77±i.81	80.19±1.66	77.51±1.20	73.76
RLUS	71.83±ι.7i	69.64±2.39	81.98±1.04	78.69±1.15	75.54
MME	69.66±0.58	71.36±o.96	78.88±1.51	76.64±1.73	74.14
WADN	74.83±0.84	75.05±o.62	84.23±0.58	81.53±0.90	78.91
Full TAR	84.10±0.13	83.68±0.i2	86.11±0.32	88.72±0.14	86.65
Table 5: Multi-source Transfer: Accuracy (%) on the Source-Shifted Digits
Target	MNIST	SVHN	SYNTH	USPS	Average
Source + Tar	79.63±i.74	56.48±i.90	69.64±i.38	86.29±i.56	73.01
DANN	86.77±i.30	69.13±i.09	78.82±i.35	86.54±i.03	80.32
MDAN	86.93±i.05	68∙25±i.53	79.80±i.i7	86.23±i.4i	80.30
M3SDA	85.88±2.06	68.84±i.05	76.29±0.95	87.15±i.i0	79.54
DARN	86.58±i.46	68.86±i.30	80.47±0.67	86.80±0.89	80.68-
RLUS	87.61±i.08	70∙50±0.94	79.52±i.30	86.70±i.i3	81.08-
MME	87.24±0.95	65.20±i.35	80.31±0.60	87.88±0.76	80.16
WADN	88.32±i.i7	70.64±i.02	81.53±i.ii	90.53±0.7i	82.75
Full TAR	98.70±0.i5	85.20±0.09	95.10±0.i4	96.64±0.i3	93.91
The results are reported in Tabs. 4, 5, which also indicates strong empirical benefits. To show the
effectiveness of WADN, We select various portions of labelled samples (1% 〜10%) on the target.
The results in Fig.1(c) on USPS dataset shows a consistently better than the baselines, even in the
feW target samples.
8
Under review as a conference paper at ICLR 2021
5.3	Partial Unsupervised Multi-Source DA
In this scenario, we adopt the Office-Home dataset to evaluate our approach, as it contains large
(65) classes. We do not change the source domains and we randomly choose 35 classes from the
target. We evaluate all the baselines on the same selected classes and repeat 5 times. All reported
results are averaged from 3 different sub-class selections (15 runs in total), showing in Tab.6. (See
Appendix M for details.) We additionally compare PADA (Cao et al., 2018) approach by merging
all the sources and use one-to-one partial DA algorithm. We adopt the same hyper-parameters and
training strategies with unsupervised DA scenario.
Table 6: Unsupervised Partial DA: Accuracy (%) on Office-Home (#Source: 65, #Target: 35)
Target	Art	Clipart	Product	Real-World	Average
Source	50.56±1.42	49.79±1.14	68.10±1.33	78.24±0.76	61.67
DANN	53.86±2.23	52.71±2.20	7L25±2.44	76.92±1.21	63.69
MDAN	67.56±1.39	65.38±1.30	81.49±i.92	83.44±1.01	74.47
MDMN	68.13±1.08	65.27±1.93	81.33±i.29	84.00±0.64	74.68~
M3SDA	65.10±1.97	61.80±1.99	76.19±2.44	79.14±1.51	70.56
DARN	71.53±0.63	69.31±1.08	82.87±i.56	84.76±0.57	77.12
PADA	74.37±0.84	69.64±0.80	83.45±ι.i3	85.64±0.39	78.28
WADN	80.06±0.93	75.90±1.06	89∙55±0.72	90.40±0.39	83.98
The reported results are also significantly better than the current multi-source DA or one-to-one
partial DA approach, which verifies the benefits of WADN: properly estimating αt and assigning
proper λ for each source.
# Selected Classes (P)
(a)
(b)
Figure 2: Analysis on Partial DA of target Product. (a) Performance (mean ± std) of different
selected classes on the target; (b) We select 15 classes and visualize estimated ɑt (the bar plot). The
”X” along the x-axis represents the index of dropped 50 classes. The red curves are the true label
distribution ratio. See Appendix P for additional results and analysis.
Besides, we change the number of selected classes (Fig 2(a)), the proposed WADN still indicates
consistent better results by a large margin, which indicates the importance of considering αt and
λ. In contrast, DANN shows unstable results in less selected classes. (See Appendix P for details)
Beside, WADN shows a good estimation of the label distribution ratio (Fig 2(b)) and has correctly
detected the non-overlapping classes, which indicates its good explainability.
6 Conclusion
In this paper, we proposed a new theoretical principled algorithm WADN (Wasserstein Aggregation
Domain Network) to solve the multi-source transfer learning problem under target shift. WADN
provides a unified solution for various deep multi-source transfer scenarios: learning with limited
target data, unsupervised DA, and partial unsupervised DA. We evaluate the proposed method by
extensive experiments and show its strong empirical results.
9
Under review as a conference paper at ICLR 2021
References
Kei Akuzawa, Yusuke Iwasawa, and Yutaka Matsuo. Adversarial invariant feature learning with ac-
curacy constraint for domain generalization. In Joint European Conference on Machine Learning
and Knowledge Discovery in Databases, pp. 315-331. Springer, 2019.
Martin Arjovsky, SoUmith Chintala, and Leon Bottou. Wasserstein gan. arXiv preprint
arXiv:1701.07875, 2017.
Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learn-
ing for domain adaptation under label shifts. In International Conference on Learning Represen-
tations, 2019. URL https://openreview.net/forum?id=rJl0r3R9KX.
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa. Metareg: Towards domain gener-
alization using meta-regularization. In Advances in Neural Information Processing Systems, pp.
998-1008, 2018.
Yogesh Balaji, Rama Chellappa, and Soheil Feizi. Normalized wasserstein distance for mixture
distributions with applications in adversarial learning and domain adaptation. arXiv preprint
arXiv:1902.00415, 2019.
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations
for domain adaptation. In Advances in neural information processing systems, pp. 137-144, 2007.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. Machine learning, 79(1-2):151-175,
2010.
John Blitzer, Mark Dredze, and Fernando Pereira. Biographies, bollywood, boom-boxes and
blenders: Domain adaptation for sentiment classification. In Proceedings of the 45th annual
meeting of the association of computational linguistics, pp. 440-447, 2007.
Silvia Bucci, Antonio D’Innocente, and Tatiana Tommasi. Tackling partial domain adaptation with
self-supervision. In International Conference on Image Analysis and Processing, pp. 70-81.
Springer, 2019.
Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation.
In Proceedings of the European Conference on Computer Vision (ECCV), pp. 135-150, 2018.
Zhangjie Cao, Kaichao You, Mingsheng Long, Jianmin Wang, and Qiang Yang. Learning to transfer
examples for partial domain adaptation. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 2985-2994, 2019.
Minmin Chen, Zhixiang Xu, Kilian Q Weinberger, and Fei Sha. Marginalized denoising autoen-
coders for domain adaptation. In Proceedings of the 29th International Coference on Interna-
tional Conference on Machine Learning, pp. 1627-1634, 2012.
Xilun Chen, Ahmed Hassan Awadallah, Hany Hassan, Wei Wang, and Claire Cardie. Multi-source
cross-lingual model transfer: Learning what to share. In Proceedings of the 57th Annual Meeting
of the Association for Computational Linguistics, 2019.
Zhihong Chen, Chao Chen, Zhaowei Cheng, Ke Fang, and Xinyu Jin. Selective transfer with rein-
forced transfer network for partial domain adaptation. In AAAI Conference on Artificial Intelli-
gence, 2020.
Stergios Christodoulidis, Marios Anthimopoulos, Lukas Ebner, Andreas Christe, and Stavroula
Mougiakakou. Multisource transfer learning with convolutional neural networks for lung pat-
tern analysis. IEEE journal of biomedical and health informatics, 21(1):76-84, 2016.
Remi Tachet des Combes, Han Zhao, Yu-Xiang Wang, and Geoff Gordon. Domain adaptation with
conditional distribution matching and generalized label shift. arXiv preprint arXiv:2003.04475,
2020.
10
Under review as a conference paper at ICLR 2021
Marthinus Christoffel Du Plessis and Masashi Sugiyama. Semi-supervised learning of class balance
under class-prior change by distribution matching. Neural Networks, 50:110-119, 2014.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In ICML, 2017.
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Francois
Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural net-
works. The Journal of Machine Learning Research, 17(1):2096-2030, 2016.
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary C Lipton. A unified view of label
shift estimation. arXiv preprint arXiv:2003.07554, 2020.
Linda S Geiss, Jing Wang, Yiling J Cheng, Theodore J Thompson, Lawrence Barker, Yanfeng Li,
Ann L Albright, and Edward W Gregg. Prevalence and incidence trends for diagnosed diabetes
among adults aged 20 to 79 years, united states, 1980-2012. Jama, 312(12):1218-1226, 2014.
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and Bernhard
Scholkopf. Domain adaptation with conditional transferable components. In International Con-
ference on machine learning, pp. 2839-2848, 2016.
Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-
proved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.
5767-5777, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Judy Hoffman, Brian Kulis, Trevor Darrell, and Kate Saenko. Discovering latent domains for multi-
source domain adaptation. In European Conference on Computer Vision, pp. 702-715. Springer,
2012.
Judy Hoffman, Mehryar Mohri, and Ningshan Zhang. Algorithms and theory for multiple-source
adaptation. In Advances in Neural Information Processing Systems, pp. 8246-8256, 2018a.
Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros,
and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International
conference on machine learning, pp. 1989-1998. PMLR, 2018b.
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-
drea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.
arXiv preprint arXiv:1902.00751, 2019.
Jonathan J. Hull. A database for handwritten text recognition research. IEEE Transactions on
pattern analysis and machine intelligence, 16(5):550-554, 1994.
Maximilian Ilse, Jakub M Tomczak, Christos Louizos, and Max Welling. Diva: Domain invariant
variational autoencoders. arXiv preprint arXiv:1905.10427, 2019.
Nikola Konstantinov and Christoph Lampert. Robust learning from untrusted sources. In Interna-
tional Conference on Machine Learning, pp. 3488-3498, 2019.
Joshua Lee, Prasanna Sattigeri, and Gregory Wornell. Learning new tricks from old dogs: Multi-
source transfer learning from pre-trained networks. In Advances in Neural Information Processing
Systems, pp. 4370-4380, 2019.
Jingmei Li, Weifei Wu, Di Xue, and Peng Gao. Multi-source deep transfer neural network algorithm.
Sensors, 19(18):3992, 2019a.
Ya Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and Dacheng Tao.
Deep domain generalization via conditional invariant adversarial networks. In Proceedings of the
European Conference on Computer Vision (ECCV), pp. 624-639, 2018a.
11
Under review as a conference paper at ICLR 2021
Yitong Li, David E Carlson, et al. Extracting relationships by multi-domain matching. In Advances
in Neural Information Processing Systems,pp. 6798-6809, 2018b.
Yitong Li, Michael Murias, Samantha Major, Geraldine Dawson, and David Carlson. On target shift
in adversarial domain adaptation. In The 22nd International Conference on Artificial Intelligence
and Statistics, pp. 616-625, 2019b.
Chuang Lin, Sicheng Zhao, Lei Meng, and Tat-Seng Chua. Multi-source domain adaptation for
visual sentiment classification. arXiv preprint arXiv:2001.03886, 2020.
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift
with black box predictors. In International Conference on Machine Learning, pp. 3122-3130,
2018.
Jing Liu, Yuling Hong, Ralph B D’Agostino Sr, Zhaosu Wu, Wei Wang, Jiayi Sun, Peter WF Wilson,
William B Kannel, and Dong Zhao. Predictive value for the chinese population of the framingham
chd risk assessment tool compared with the chinese multi-provincial cohort study. Jama, 291(21):
2591-2599, 2004.
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial
domain adaptation. In Advances in Neural Information Processing Systems, pp. 1640-1650, 2018.
Alexander Selvikvag LUndervold and Arvid LUndervoid. An overview of deep learning in medical
imaging focusing on mri. ZeitSchriftfiirMediziniSche PhySik, 29(2):102-127, 2019.
Yishay MansoUr, Mehryar Mohri, and Afshin Rostamizadeh. Domain adaptation: Learning boUnds
and algorithms. arXiv preprint arXiv:0902.3430, 2009.
Yishay MansoUr, Mehryar Mohri, Ananda Theertha SUresh, and Ke WU. A theory of mUltiple-soUrce
adaptation with limited target labeled data. arXiv preprint arXiv:2007.09762, 2020.
Mehryar Mohri and Andres MUnoz Medina. New analysis and algorithm for learning with drift-
ing distribUtions. In International Conference on Algorithmic Learning Theory, pp. 124-138.
Springer, 2012.
Saeid Motiian, Marco Piccirilli, Donald A. Adjeroh, and Gianfranco Doretto. Unified deep sUper-
vised domain adaptation and generalization. In The IEEE International Conference on Computer
Vision (ICCV), Oct 2017.
YUval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo WU, and Andrew Y Ng. Reading
digits in natUral images with UnsUpervised featUre learning. 2011.
XUanLong NgUyen, Martin J Wainwright, Michael I Jordan, et al. On sUrrogate loss fUnctions and
f-divergences. The Annals of Statistics, 37(2):876-904, 2009.
Sinno Jialin Pan and Qiang Yang. A sUrvey on transfer learning. IEEE Transactions on knowledge
and data engineering, 22(10):1345-1359, 2009.
Zhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. MUlti-adversarial domain adap-
tation. arXiv preprint arXiv:1809.02176, 2018.
Xingchao Peng, QinxUn Bai, Xide Xia, ZijUn HUang, Kate Saenko, and Bo Wang. Moment matching
for mUlti-soUrce domain adaptation. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 1406-1415, 2019.
Maithra RaghU, ChiyUan Zhang, Jon Kleinberg, and Samy Bengio. TransfUsion: Understanding
transfer learning for medical imaging. In Advances in neural information processing systems, pp.
3347-3357, 2019.
Ievgen Redko, Nicolas Courty, Remi Flamary, and Devis Tuia. Optimal transport for multi-source
domain adaptation Under target shift. In Kamalika ChaUdhUri and Masashi SUgiyama (eds.),
Proceedings of Machine Learning Research, volume 89 of Proceedings of Machine Learning Re-
search, pp. 849-858. PMLR, 16-18 Apr 2019. URL http://proceedings.mlr.press/
v89/redko19a.html.
12
Under review as a conference paper at ICLR 2021
Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint
arXiv:1706.05098, 2017.
Sebastian Ruder, Matthew E Peters, Swabha Swayamdipta, and Thomas Wolf. Transfer learning
in natural language processing. In Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Tutorials, pp. 15-18, 2019.
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new
domains. In European conference on computer vision, pp. 213-226. Springer, 2010.
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier dis-
crepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 3723-3732, 2018.
Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko. Semi-supervised
domain adaptation via minimax entropy. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 8050-8058, 2019.
Swami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to
adapt: Aligning domains using generative adversarial networks. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 8503-8512, 2018.
Shai Shalev-Shwartz and Shai Ben-David. Understanding machine learning: From theory to algo-
rithms. Cambridge university press, 2014.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in neural information processing systems, pp. 4077-4087, 2017.
Petar Stojanov, Mingming Gong, Jaime G Carbonell, and Kun Zhang. Data-driven approach to
multiple-source domain adaptation. Proceedings of machine learning research, 89:3487, 2019.
Masashi Sugiyama and Motoaki Kawanabe. Machine learning in non-stationary environments:
Introduction to covariate shift adaptation. MIT press, 2012.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.
Learning to compare: Relation network for few-shot learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pp. 1199-1208, 2018.
Ben Tan, Erheng Zhong, Evan Wei Xiang, and Qiang Yang. Multi-transfer: Transfer learning with
multiple views and multiple sources. In Proceedings of the 2013 SIAM International Conference
on Data Mining, pp. 243-251. SIAM, 2013.
Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang Liu. A survey
on deep transfer learning. In International conference on artificial neural networks, pp. 270-279.
Springer, 2018.
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep
hashing network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 5018-5027, 2017.
Boyu Wang, Jorge Mendez, Mingbo Cai, and Eric Eaton. Transfer learning via minimizing the
performance gap between domains. In Advances in Neural Information Processing Systems, pp.
10645-10655, 2019a.
Haotian Wang, Wenjing Yang, Zhipeng Lin, and Yue Yu. Tmda: Task-specific multi-source domain
adaptation via clustering embedded adversarial training. In 2019 IEEE International Conference
on Data Mining (ICDM), pp. 1372-1377. IEEE, 2019b.
Jonathan Weed, Francis Bach, et al. Sharp asymptotic and finite-sample rates of convergence of
empirical measures in wasserstein distance. Bernoulli, 25(4A):2620-2648, 2019.
Pengfei Wei, Ramon Sagarna, Yiping Ke, Yew-Soon Ong, and Chi-Keong Goh. Source-target sim-
ilarity modelings for multi-source transfer gaussian process regression. In International Confer-
ence on Machine Learning, pp. 3722-3731, 2017.
13
Under review as a conference paper at ICLR 2021
Junfeng Wen, Chun-Nam Yu, and Russell Greiner. Robust learning under uncertain test distribu-
tions: Relating covariate shift to model misspecification. In International Conference on Machine
Learning,pp. 631-639, 2014.
Junfeng Wen, Russell Greiner, and Dale Schuurmans. Domain aggregation networks for multi-
source domain adaptation. Proceedings of the 37th International Conference on Machine Learn-
ing, 2020.
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton. Domain adaptation with
asymmetrically-relaxed distribution alignment. In International Conference on Machine Learn-
ing, pp. 6872-6881, 2019.
Yi Yao and Gianfranco Doretto. Boosting for transfer learning with multiple sources. In 2010
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pp. 1855-
1862. IEEE, 2010.
Jing Zhang, Zewei Ding, Wanqing Li, and Philip Ogunbona. Importance weighted adversarial nets
for partial domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 8156-8164, 2018.
Jing Zhang, Wanqing Li, Philip Ogunbona, and Dong Xu. Recent advances in transfer learning
for cross-dataset visual recognition: A problem-oriented perspective. ACM Computing Surveys
(CSUR), 52(1):1-38, 2019.
KUn Zhang, Bernhard Scholkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under
target and conditional shift. In International Conference on Machine Learning, pp. 819-827,
2013.
YiNan Zhang and MingQiang An. Deep learning-and transfer learning-based super resolution re-
construction from single medical image. Journal of healthcare engineering, 2017.
Yu Zhang and Qiang Yang. A survey on multi-task learning. arXiv preprint arXiv:1707.08114,
2017.
Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships in multi-task
learning. arXiv preprint arXiv:1203.3536, 2012.
Han Zhao, Shanghang Zhang, Guanhang Wu, Jose MF Moura, Joao P Costeira, and Geoffrey J
Gordon. Adversarial multiple source domain adaptation. In Advances in neural information
processing systems, pp. 8559-8570, 2018.
Sicheng Zhao, Guangzhi Wang, Shanghang Zhang, Yang Gu, Yaxian Li, Zhichao Song, Pengfei
Xu, Runbo Hu, Hua Chai, and Kurt Keutzer. Multi-source distilling domain adaptation. arXiv
preprint arXiv:1911.11554, 2019.
Sicheng Zhao, Bo Li, Pengfei Xu, and Kurt Keutzer. Multi-source domain adaptation in the deep
learning era: A systematic survey. arXiv preprint arXiv:2002.12169, 2020.
Yongchun Zhu, Fuzhen Zhuang, and Deqing Wang. Aligning domain-specific distribution and clas-
sifier for cross-domain classification from multiple sources. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volume 33, pp. 5989-5996, 2019.
14
Under review as a conference paper at ICLR 2021
A Additional Empirical Results
Table 7: Unsupervised DA: Accuracy (%) on Source-Shifted Amazon Review.
Target	Books	DVD	Electronics	Kitchen	Average
Source	68.15±i.37	69.51±o.74	82.09±0.88	75.30±1.29	73.81
DANN	65.59±i.35	67.23±0.7i	80.49±1.11	74.71±1.53	72.00
MDAN	68.77±2.3i	67.81±2.46	80.96±0.77	75.67±1.96	73.30
MDMN	70.56±i.05	69.64±o.73	82.71±0.71	77.05±0.78	74.99
M3SDA	69.09±i.26	68.67±i.37	81.34±0.66	76.10±1.47	73.79
DARN	71.21±1.16	68.68±1.12	81.51±0.81	77.71±1.09	74.78
WADN	73.72±o.63	79∙64±o.34	84.64±0.48	83.73±0.50	80.43
Full TAR	84.10±o.i3	83.68±o.i2	86.11±0.32	88.72±0.14	86.65
B	Additional Related Work
Multi-source transfer learning Practice has been proposed from various prospective. The key
idea is to estimate the importance of different sources and then select the most related ones, to mit-
igate the influence of negative transfer. In the multi-source unsupervised DA, (Sankaranarayanan
et al., 2018; Balaji et al., 2019; Pei et al., 2018; Zhao et al., 2019; Zhu et al., 2019; Zhao et al.,
2020; 2019; Stojanov et al., 2019; Li et al., 2019b; Wang et al., 2019b; Lin et al., 2020) proposed
different practical strategies in the classification, regression and semantic segmentation problems.
In the presence of target labels, Hoffman et al. (2012); Tan et al. (2013); Wei et al. (2017); Yao
& Doretto (2010); Konstantinov & Lampert (2019) used generalized linear model to learn the tar-
get. Christodoulidis et al. (2016); Li et al. (2019a); Chen et al. (2019) focused on deep learning
approaches and Lee et al. (2019) proposed an ad-hoc strategy to combine to sources in the few-shot
target domains. These ideas are generally data-driven approaches and do not analyze the why the
proposed practice can control the generalization error.
Label-Partial Transfer Learning Label-Partial can be viewed as a special case of the label-shift.
1 Most existing works focus on one-to-one partial transfer learning (Zhang et al., 2018; Chen et al.,
2020; Bucci et al., 2019; Cao et al., 2019) by adopting the re-weighting training approach without
a formal understanding. In our paper, we first rigorously analyzed this common practice and adopt
the label distribution ratio as its weights, which provides a principled approach in this scenario.
B.1 Other scenarios related to Multi-Source Transfer Learning
Domain Generalization The domain generalization (DG) resembles multi-source transfer but
aims at different goals. A common setting in DG is to learn multiple source but directly predict
on the unseen target domain. The conventional DG approaches generally learn a distribution in-
variant features (Balaji et al., 2018; Saenko et al., 2010; Motiian et al., 2017; Ilse et al., 2019) or
conditional distribution invariant features (Li et al., 2018a; Akuzawa et al., 2019). However, our the-
oretical results reveal that in the presence of label shift (i.e αt (y) 6= 1) and outlier tasks then learning
conditional or marginal invariant features can not guarantee a small target risk. Our theoretical result
enables a formal understanding about the inherent difficulty in DG problems.
Few-Shot Learning The few-shot learning (Finn et al., 2017; Snell et al., 2017; Sung et al., 2018)
can be viewed as a very specific scenario of multi-source transfer learning. We would like to point
out the differences between the few-shot learning and our paper. (1) Few-shot learning generally
involves a very large set of source domains T 1 and each domain consists a modest number
of observations NSt . In our paper, we are interested in the a modest number of source domains T
but each source domain including a sufficient large number of observations (NSt	1). (2) In the
target domain, the few-shot setting generally used K-samples (K is very small) for each class for the
fine-tuning. We would like to point out this setting generally violates our theoretical assumption. In
1Since supp(T (y)) ⊆ supp(St (y)) then we naturally have T(y) 6= St (y).
15
Under review as a conference paper at ICLR 2021
our paper, we assume the target data is i.i.d. sampled from D(X, y). It is equivalently viewed that we
first i.i.d. sample y 〜 D(y), then i.i.d. sample X 〜 D(x∣y). Generally the D(y) is non-uniform,
thus few-shot setting are generally not applicable for our theoretical assumptions.
Multi-Task Learning The goal of multi-task learning (Zhang & Yang, 2017) aims to improve the
prediction performance of all the tasks. In our paper, we aim at controlling the prediction risk of a
specified target domain. We also notice some practical techniques are common such as the shared
parameter (Zhang & Yeung, 2012), shared representation (Ruder, 2017), etc.
C Additional Figures related to the main paper
(a) Amazon
*10 2 O 2 02 1 n
■ I I I I I II I
Vooo O O Oo O n
XaUgbaJH Λ3ugbalΛ3uanbalXucs
Label distribution: MnlSt
口∏∏∏口口□□
Label dlstHbutlon： USPS
(b) Digits
ruenbellɪ,AaUenbaIlɪ,Xuco
Label Ostifeution: Art
nι∣Mll 喻 ∣⅛⅛⅛⅛ 如偏 IllIu.
IliiiiilhlilllliiiiillliiililiililiLhiihiIiiil
Labeldi8tf⅛ut⅛n: RBalWorid
lBlilllb»liilluiillWiii.llitalliltliiiili
0	10	20	30	«	50	60
(c) Office-Home
Figure 3: Label distribution visualization. (a) One example in Amazon Review dataset with sources:
Book, Dvd, Electronic and target: Kitchen. We randomly drop 50% of the negative reviews in all the
sources while keeping target label distribution unchanged. (b) One example in Digits dataset with
Sources: MNIST, USPS, SVHN and Target Synth. We randomly drop 50% data on digits 5-9 in all
sources while keeping target label distribution unchanged. (c) Office-Home dataset. The original
label distribution is non-uniform. See Appendix M for details.
D Table of Notation
Table 8: Table of Notations
RD (h) = E(x,y)~D '(h(x,y))
RD (h) = N Pi=1 '(h(χi,yi))
a and (^t
RS(h) = N Pi=1 α(yi)'(h(Xi,yi))
S(z|y) = Rx g(z|x)S(x|Y = y)dx
W1(St(z|y)kT (z|y))
Expected Risk on distribution D w.r.t. hypothesis h
Empirical Risk on observed data {(xi, yi)}iN=1 that are i.i.d. sampled
from D.
True and empirical label distribution ratio α(y) = T (y)/S(y)
Empirical Weighted Risk on observed data {(xi, yi)}iN=1.
Conditional distribution w.r.t. latent variable Z that induced by feature
learning function g .
Conditional Wasserstein distance on the latent space Z
E Proof of Theorem 1
Proof idea Theorem 1 consists three steps in the proof:
Lemma 2. If the prediction loss is assumed as L-Lipschitz and the hypothesis is K -Lipschitz w.r.t.
the feature x (given the same label), i.e. for ∀Y = y, kh(x1, y) - h(x2, y)k2 ≤ Kkx1- x2k2. Then
the target risk can be upper bounded by:
RT(h) ≤ Xλ[t]RSt(h) + LKXλ[t]Ey~τ(y)Wi(T(XIY = y)ks(XIY = y))	(4)
tt
Proof. The target risk can be expressed as:
RT (h(x,y)) = E(x,y)~T '(h(x,y)) = Ey~T (y)Ex~T (x| y)'(h(x,y ))
16
Under review as a conference paper at ICLR 2021
By denoting α(y) = T(y), then We have:
Ey〜T(y)Ey〜T(x∣y)'(h(x, y)) = Ey〜S(y)a(y)Ex〜T(x∣y)'(h(x, y))
Then we aim to upper bound Ex〜T(x∣y)'(h(x, y)). For any fixed y,
Ex〜T(x∣y)'(h(x,y)) - Ex〜S(x∣y)'(h(χ,y)) ≤ | /	'(h(χ,y))d(T(x|y) - S(χ∣y))∣
x∈X
Then according to the Kantorovich-Rubinstein duality, for any distribution coupling γ ∈
Π(T(χ∣y), S(x|y)), then we have:
=inf | /	'(h(xp,y)) - '(h(xq,y))dγ(xp,xq)|
≤ inf /	l'(h(χp,y))- '(h(xq ,y))∣dγ(xp,xq)
≤ Linf/	∣h(xp,y)) - h(xq,y)∣dγ(xp,xq)
≤ LK inf	kxp - xqk2dγ(xp,xq)
=LKW1(T(x|Y =y)kS(x|Y =y))
The first inequality is obvious; and the second inequality comes from the assumption that ` is L-
Lipschitz; the third inequality comes from the hypothesis is K-Lipschitz w.r.t. the feature x (given
the same label), i.e. for ∀Y = y, kh(x1, y) - h(x2, y)k2 ≤ Kkx1 - x2 k2.
Then we have:
RT(h) ≤ Ey〜s(y)α(y)[Ex〜S(x∣y)'(h(x,y)) + LKWI(T(x∣y)∣∣S(x∣y))]
=E(x,y)〜S α(y)'(h(χ, y)) + LK Ey 〜T (y)Wι (T(XIY = y)ks (XIY = y))
=Ra (h) + LK Ey 〜T (y)Wι(T(XIY = y)ks (XIY = y))
Supposing each source St we assign the weight λ[t] and label distribution ratio ɑt(y) = S(y), then
by combining this T source target pair, we have:
rt(h) ≤ Xλ[t]Rat(h) + LKXλ[t]Ey〜T(y)Wι(T(XIY = y)kSt(XIY = y))
tt
□
Then we will prove Theorem 1 from this result, we will derive the non-asymptotic bound, estimated
from the finite sample observations. Supposing the empirical label ratio value is c^t, then for any
simplex λ we can prove the high-probability bound.
E.1 Bounding the empirical and expected prediction risk
Proof. We first bound the first term, which can be upper bounded as:
s
；up |£ λ[t]Rαt (h)-£ λ[t]RSt(h)∣≤ SuP |£ λ[t]Rαt (h) - fλ[t]Rst(h)∣ + sup |£ λ[t]Rαt(h)- fλ[t]RSt(h)∣
ht	t	ht	t	ht
t
{z^^^^^~
(II)
t
{^^^^^^~
(I)
✓
|
|
Bounding term (I) According to the McDiarmid inequality, each item changes at most
12λ[Nat(y)' ∣. Thenwe have:
NSt
P ((I) - E(I) ≥ t) ≤ exp(
-2t2
∑T=1 β4N λ2[t]αt(y)2'2
)
δ
17
Under review as a conference paper at ICLR 2021
By substituting δ, at high probability 1 - δ we have:
⑴ ≤ E(I)+ Lmaxd^uXX λβt2 r l0g(≡
Where Lmax = suph∈H `(h) and N = PtT=1 NSt the total source observations and βt =
NSt the frequency ratio of each source. And d∞p = maxt=ι,…,td∞(T(y)∣∣S(y)) =
maxt=1,...,T maxy∈[1,Y] αt(y), the maximum true label shift value (constant).
Bounding E sup(I), the expectation term can be upper bounded as the form of Rademacher Com-
plexity:
T1
E(I) ≤ 2EσEST SUpEλ[t]	E	TN (αt(y)'(h(χt,yt))
h t=1	(χt,yt)∈St
≤ 2Xλ[t]EσEST SUp	X	TN (αt(y)'(h(χt,yt))
h
t	(xt,yt)∈St
≤ 2supEσES, sup	X	ɪ [αt(y)'(h(xt,yt))]
t	t h	T N
(xt,yt)∈St
= suρ2Rt(', H) = 2R(', H)
t
Where R(', H)= supt Rt(' H)= supt SUph〜H ESt,σ P(xt,yt)∈St T1N [αt(y)'(h(xt,yt))],rePre-
sents the Rademacher complexity w.r.t. the prediction loss `, hypothesis h and true label distribution
ratio αt .
Therefore with high probability 1 - δ, we have:
S
Up | ∑ λ[t]Rαt(h) - ∑ λ[t]Rαt(h)l ≤ R(', h) + Lmaχd∞p
X牛r
log(1∕δ)
2N
∖
Bounding Term (II) For all the hypothesis h, we have:
1 NSt
I X λ[t]R^St (h) - X λ[t]R^St (h)l = I X λ[t]N- X(α(y(i)) - α(y(i)))'(h)∣
t	t	t	NSt i
1	|Y|
=X λ[t]N-1 X(α(Y = y) - α(Y = y))'(Y = y)∣
t	NSt y
Where '(Y = y) = PNSt '(h(χi, yi = y)), represents the cumulative error, conditioned on a given
label Y = y . According to the Holder inequality, we have:
1	|Y|	1
∑λ[t]7r I ∑(αt(Y = y) - at (Y = y))'(Y = y)∣ ≤ ]Tλ[t]-	一曲口2忸丫 = y)k2
t	NSt y	t	NSt
≤	Lmax X λ[t]kat - at k2
t
≤	Lmax SUp ∣∣at - atk2
t
Therefore, ∀h ∈ H, with high probability 1 - δ we have:
T	I----
X λ[t照t(h) ≤ X λ"h)+2R(',h)+Lmaxd∞ptX 与 d、+Lmax SUp gik2
18
Under review as a conference paper at ICLR 2021
E.2 Bounding empirical Wasserstein Distance
Then we need to derive the sample complexity of the empirical and true distributions, which can be
decomposed as the following two parts. For any t, we have:
. . . ..	. . . , ʌ . , . .. ʌ , ..
Ey〜T(y)Wi(T(XIY = y)kSt(XIY = y)) - Ey〜T(y)wi(T(XIY = y)kSt(XIY = y))
≤ Ey〜T(y)W1(T(XIY = y)kSt(XIY = y)) - Ey〜T(y)W1(T(XIY = J)∣St(XIY = y))
'--------------------------------------V--------------------------------------}
(I)
,ʌ ., . .. ʌ , . .	, ʌ .,	. .. ʌ , ..
+ Ey〜T(y)Wi(T(XIY = y)kSt(XIY = y)) - Ey〜T(y)wi(T(XIY = y)kSt(XIY = y))
'-----------------------------------V--------------------------------------}
(II)
Bounding (I) We have:
Ey 〜T (y)Wι(T(XIY = y)kSt(XIY = J))- Ey 〜T(y)Wi(T(XIY = y)kSt(XIY = y))
=XT(y) "i(T(xIY = y)kSt(XIY = y)) - WI(T(XIY = y)kSt3Y = y))
y
≤ I	T(y)I sup
yy
(Wi(T(χIY = y)kSt(χIY = y)) - WI(T(XIY = y)∣∣St(χIY = y))
=SUp (WI(T(XIY = y)kSt(χIY = y)) - WI(T(XIY = y)k<St(χIY = y))
-	. .. ʌ , . . , ʌ , . .. ʌ ., ..
≤ sup [Wι(St(χIY = y)k<St(χIY = y)) + WI(St(XIY = y)kT(χIY = y))
y
,ʌ . , .......... . . , ʌ . , . .. ʌ
+ WI(T(XIY = y)kT(χIY = y)) - WI(T(XIY = y)∣∣St(χIY = y))]
, . . .. ʌ , . . , ʌ ., ........................ ..
=sup WI(St(XIY = y)kSt(χIY = y)) + WI(T(XIY = y)kT(χIY = y))
y
The first inequality holds because of the Holder inequality. As for the second inequality, we use the
triangle inequality of Wasserstein distance. W1(PkQ) ≤ W1(PkP1) +W1(P1kP2) +W1(P2kQ).
According to the convergence behavior of Wasserstein distance (Weed et al., 2019), with high prob-
ability ≥ 1 - 2δ we have:
WI(St(XIY = y)kSt(χIY = y)) + WI(T(XIY = y)kT(χIY = y)) ≤ κ(δ,NSt,NT)
Where k(δ, Nst ,NT) = Ct,y (NSt)-st,y+Cy (NT T-S+/1 log( 2 MqNyt+qN), Where NSt
is the number of Y = y in source t and NTy is the number of Y = y in target distribution. Ct,y, Cy
st,y > 2, sy > 2 are positive constant in the concentration inequality. This indicates the convergence
behavior betWeen empirical and true Wasserstein distance.
If we adopt the union bound (over all the labels) by setting δ  δ∕IYI, then with high probability
≥ 1 - 2δ, We have:
SUpWι(S(χIY = y)kS(χIY = y)) + WI(T(XIY = y)∣∣T(χIY = y)) ≤ κ(δ,NSt,NT)
y
where κ(δ,NSt,NT) = Ct,y(NSt)-st，y + Cy(NT尸® + √⅛Wl)(q⅛ + 产)
Again by adopting the union bound (over all the tasks) by setting δ  δ∕T, with high probability
≥ 1 - 2δ, we have:
fλ[t]Ey 〜T (y) Wι(T (x|Y = y)kS(x∣Y = y))-£ λ[t]Ey 〜T (y)Wι(T(X∣Y = y)∣∣S(x∣Y = y)) ≤ Sup κ(δ, NSyt ,NT)
tt	t
Where κ(δ, NSyt, NTy) = Ct,y(NSyt)-st，y
+ Cy(NTy )-sy
+ Vz2log( 2TδYl)(qNf + qNy).
19
Under review as a conference paper at ICLR 2021
Bounding (II) We can bound the second term:
,ʌ . .	. .. ʌ , . . , ʌ . , . .. ʌ , ..
Ey~τ(y)Wι(T(XIY = y)kSt(XIY = y)) - Ey~T(y)Wι(T(XIY = y)kSt(XIY = y))
≤ SUp WI(T(XIY = y)k<St(χ∣Y = y))|£T(y) - T(y)∣
≤ Cm axI X T (y) -T(y)I
y
t 1 τl	d∙	TTT- /zɪ-/ I，广	∖ I I A/ I，广	∖ ∖ ♦	∙ . ∙	1 1	1 1	.	EI
Where Cmt ax = sUpy W1(T(XIY = y)kS(XIY = y)) is a positive and bounded constant. Then we
need to bound I y T(y) - T (y)I, by adopting MicDiarmid’s inequality, we have at high probability
1 - δ:
IET(y) -T(y)I ≤ ETI∑T(y) -Ty)I +
yy
/log(1∕δ)
2N 2Nt
2Eσ ET X σT(y) +JiNT
Then we bound EσET Ey σT(y). WeUse the properties of Rademacher complexity [Lemma 26.11,
(Shalev-Shwartz & Ben-David, 2014)] and notice that T(y) is a probability simplex, then We have:
EσET X σT(y) ≤ Siog(IYJ
yT
2io	2 ilt∕∖ +/、I，/ 2 log(2∣Y∣) l	/log(1∕δ)
ThenWehaVe iEyT⑻ — T(y)I ≤ N	NTYI) + N OgNT)
Then using the union bound and denoting δ J δ∕T, with high probability ≥ 1 - δ and for any
simplex λ, we have:
Eλ[t]Ey~T(y)Wι(T(XIY =沙川&3丫 = y)) ≤ Eλ[t]Ey~T(y)WI(T(XIY = y)kSt(XIY = y))
tt
i-+;
where Cmax = sUpt Cmt ax .
Combining together, we can derive the PAC-Learning bound, which is estimated from the finite
samples (with high probability 1 - 4δ):
X
λtRSt (h)+ LH ∑ λtEy~T(y)W1(T(XY
=y)kS(XY = y)) + Lmaχd∞pt X W JIoglNδ)
+次化h)+Lmax SUpkαt - αt k2+SUp κ(δ, NSt, NT+Cmax(JONTIYI)+v⅛F)
Then we denote COmP(NSI,..., NT, δ) = 2TR(', h) + supt κ(δ, NS七,NT) + Cmax( J210NmYIr +
JIOgN∕δ)) as the convergence rate function that decreases with larger Ns、,..., NT. Bedsides,
R(', h) = supt Rt (', H) is the re-weighted Rademacher complexity. Given a fixed hypothesis with
finite VC dimension2, it can be proved R(', h) = minNsι ,...,Nst O( JN^) i.e (Shalev-Shwartz &
Ben-David, 2014).	t	口
2If the hypothesis is the neural network, the Rademacher complexity can still be bounded analogously.
20
Under review as a conference paper at ICLR 2021
F Proof of Theorem 2
We first recall the stochastic feature representation g such that g : X → Z and scoring hypothesis h
h : Z × Y → R and the prediction loss ` with ` : R → R. 3
Proof. The marginal distribution and conditional distribution w.r.t. latent variable Z that are induced
by g, which can be reformulated as:
S(z) =	g (z |x)S (x)dx	S(z|y) =	g(z|x)S(x|Y = y)dx
xx
In the multi-class classification problem, we additionally define the following distributions:
μk (Z) = S (Y = k,z) = S (Y = k)S (z|Y = k)
πk(z) =T(Y=k,z) =T(Y=k)T(z|Y=k)
Based on (Nguyen et al., 2009) and g(z|x) is a stochastic representation learning function, the loss
conditioned a fixed point (x, y) w.r.t. h and g is Ez〜g(z∣χ)'(h(z, y)). Then taking the expectation
over the S (x, y) we have: 4
RS(h, g) = E(x,y)〜S(x,y)Ez〜g(z|x)'(h(z, y)
|Y|
XS(y
k=1
|Y|
XS(y
k=1
|Y|
XS(y
k=1
k) J S(x|Y = k) J g(z∣x)'(h(z,y = k))dzdx
k)	[ S(x|Y = k)g(z∣x)dx]'(h(z, y = k))dz
k)	S(z|Y = k)`(h(z, y = k))dz
z
|Y|
= X S(z, Y = k)`(h(z, y = k))dz
k=1 z
|Y|
=X J μk (z)'(h(z,y = k))dz
Intuitively, the expected loss w.r.t. the joint distribution S can be decomposed as the expected loss on
the label distribution S(y) (weighted by the labels) and conditional distribution S(∙∣y) (real valued
conditional loss).
Then the expected risk on the S and T can be expressed as:
|Y|
RS(h,g) = X J '(h(z,y = k))μk(z)dz
|Y|
RT (h, g) = X	`(h(z, y = k))πk (z)dz
3 Note this definition is different from the conventional binary classification with binary output, and it is
more suitable in the multi-classification scenario and cross entropy loss (Hoffman et al., 2018a). For example,
if we define l = — log(∙) and h(z, y) ∈ (0,1) as a scalar score output. Then '(h(z, y)) can be viewed as the
cross-entropy loss for the neural-network.
4An alternative understanding is based on the Markov chain. In this case it is a DAG with Y <S(ylx)
X → Z, X S—→ Y → S <— Z <— X. (S is the output of the scoring function). Then the ex-
pected loss over the all random variable can be equivalently written as P(x, y, z, s) `(s) d(x, y, z, s) =
R P(x)P(y∣x)P(z∣x)P(s∣z, y)'(s) = R P(x,y)P(z∣x)P(s∣z,y)'(s)d(x,y)d(z)d(s). Since the scoring S
is determined by h(x, y), then P(s|y, z) = 1. According to the definition we have P(z|x) = g(z|x),
P(x, y) = S(x, y), then the loss can be finally expressed as ES(x,y)Eg(z|x)'(h(z, y))
21
Under review as a conference paper at ICLR 2021
By denoting α(y) = Ty), We have the α-weighted loss:
RSα(h,g)=T(Y
1)	`(h(z, y
z
1))S(z|Y =1)+T(Y =2)
`(h(z, y
z
2))S(z|Y =2)
+ ∙∙∙ + T(Y
k)
z
`(h(z, y = k))S(z|Y = k)dz
Then we have:
RT (h, g) - RSα (h, g) ≤
XT(Y= k)
`(h(z, y
k))d|S(z|Y = k) -T(z|Y = k)|
Under the same assumption, we have the loss function `(h(z, Y = k)) is KL-Lipschitz w.r.t. the
CostlH∣2 (given a fixed k). Therefore by adopting the same proof strategy (Kantorovich-RUbinstein
duality) in Lemma 2, we have
≤ KLT(Y = 1)Wι(S(z|Y = 1)∣IT(z|Y = 1)) + …+ KLT(Y = k)Wι(S(z|Y = k)∣∣T(z|Y = k))
=KLEy 〜T (y)Wι(S(ZIY = y)kT(ZIY = y))
Therefore, we have:
Rτ(h,g) ≤ RS(h,g)+ LKEy〜T(y)Wi(S(ZIY = y)kT(ZIY = y))
Based on the aforementioned resUlt, we have ∀t = 1, . . . , T and denote S = St and α(y) = αt (y) =
T (y)/St(y):
λ[t]Rτ(h,g) ≤ λ[t]RSt(h,g) + LKλ[t]Ey〜T(y)W1(St(z∣Y = y)kT(z∣Y = y))
SUmming over t = 1, . . . , T, we have:
TT
Rτ(h,g) ≤ Xλ[t]RSt(h,g) + LKXλ[t]Ey〜T(y)Wi(St(ZIY = y)kT(ZIY = y))
t=1	t=1
□
G APPROXIMATION W1 DISTANCE
According to Jensen ineqUality, we have
Wι(S^t(ZIY = y)kT(ZIY = y)) ≤ √[W2(S(ZIY = y)kT(Z∖Y = y))]2
Supposing St(ZIY = y) ≈ N(Cy, Σ) and ^T(z∖Y = y) ≈ N(Cy, Σ), then we have:
[W2(St(ZIY = y)∣∣T(ZIY = y)]2 = kCy - Cyk2 + Trace(2Σ - 2(ΣΣ)1/2) = ∣Cy - Cy∣2
We would like to point out that assuming the identical covariance matrix is more computationally
efficient during the matching. This is advantageous and reasonable in the deep learning regime:
we adopted the mini-batch (ranging from 20-128) for the neural network parameter optimization,
in each mini-batch the samples of each class are small, then we compute the empirical covari-
ance/variance matrix will be surely biased to the ground truth variance and induce a much higher
complexity to optimize. By the contrary, the empirical mean is unbiased and computationally ef-
ficient, we can simply use the moving the moving average to efficiently update the estimated mean
value (with a unbiased estimator). The empirical results verify the effectiveness of this idea.
H Proof of Lemma 1
For each source St, by introducing the duality of Wasserstein-1 distance, for y ∈ Y, we have:
WI(St(ZIy)IlT(ZIy)) = Sup Ez〜St(z∣y)d(Z) - Ez〜T(z∣y)d(Z)
kdkL≤1
= sup X St(ZIy)d(Z) - X T(ZIy)d(Z)
kdkL≤1 z	z
=τ≡1τ SUp T7yτXSt(Z,y)d(Z) - XT(Z,y)d(Z)
T(y) kdkL≤1 St(y) z	z
22
Under review as a conference paper at ICLR 2021
Then by defining αt(z) = 1{(z,y)〜&} J(Y=1) = 1{(zy)^st}αt(Y = y), We can See for each pair
observation (z,y) sampled from the same distribution, then at(Z = Z) = αt(Y = y). Then we
have:
T (y)W1(St(z|y)kT (z|y)) = sup {	αt(y)St(z, y)d(z) -	T (z, y)d(z)}
y	y kdkL≤1 z	z
=sup	c^t(z)St(z)d(z) —	T (z)d(z)
kdkL≤1 z	z
=sup Ez〜St(z)Ct(z)d(z) — Ez〜丁(z)d(z)
kdkL≤1
We propose a simple example to understand Ct: supposing three samples in St = {(zι,Y =
1), (Z2,Y = 1),(Z3,Y = 0)} then Ct(zι) = αt(z2) = αt(1) and αt(z3) = ct(0). Therefore,
the conditional term is equivalent to the label-weighted Wasserstein adversarial learning. We plug
in each source domain as weight λ[t] and domain discriminator as dt, we finally have Lemma 1.
I Derive the label ratio loss
We suppose the representation learning aims at matching the conditional distribution such that
T(z|y) ≈ St(z∣y),∀t, then we suppose the predicted target distribution as T(y). By simplifying
the notation, we define f(z) = argmaxyh(z, y) the most possible prediction label output, then we
have:
YY
T(y) = X T(f (Z) = y∣Y = k)T(Y = k) = X St(f(z) = y|Y = k)T (y = k
k=1	k=1
Y
=£St(f (Z)= y,γ = k)αt(k) = Tat (y)
i=1
The first equality comes from the definition of target label prediction distribution, T(y) =
ET (z)1{f (Z) =y} = T(f(Z) =y) = PkY=1 T(f(Z) =y,Y = k) = PkY=1 T(f(Z) =y|Y=
k)T (Y = k).
The second equality T(f(Z) = y|Y = k) = St(f(Z) = y|Y = k) holds since ∀t, T(Z|y) ≈
St(Z|y), then for the shared hypothesis f, we have T(f(Z) = y|Y = k) = St(f (Z) = y|Y = k).
The term St(f(Z) = y, Y = k) is the (expected) source prediction confusion matrix, and we denote
its empirical (observed) version as St (f(Z) = y, Y = k).
Based on this idea, in practice we want to find a Ct to match the two predicted distribution T and
Tat. If we adopt the KL-divergence as the metric, we have:
min DKL(TkTit) = min Ey 〜T IOg(JT (y)J = min -Ey 〜T l°g(T^t (y))
^t	^t	∕αt(y)	^t
Y
=min -ET(y)log(ESt(f(z) = y, Y = k)Ct(k))
t y	k=1
We should notice the nature constraints of label ratio: {αt(y) ≥ 0, Ey αt(y)St(y) = 1}. Based
on this principle, we proposed the optimization problem to estimate each label ratio. We adopt its
empirical counterpart, the empirical confusion matrix C^t [y,k] = St[f (z) = y, Y = k], then the
optimization loss can be expressed as:
|Y|	|Y|
min	- XT(y)lOg(X Cst[y,k]αt(k))
at	y=1	k=1
s.t. ∀y ∈ Y,Ct(y) ≥ 0, XCt(y)St(y) = 1
y
23
Under review as a conference paper at ICLR 2021
Classifier
St
f
晦V
Feature Extractor
Critic
Figure 4: Network Structure of Proposed Approach. It consists three losses: the weighted Classifi-
cation losses; the centroid matching for explicit conditional matching; the weighted adversarial loss
for implicit conditional matching, showed in Eq. (6)
—牖V
v> Ccentroid
J Label Partial Multi-source unsupervised DA
The key difference between multi-conventional and partial unsupervised DA is the estimation step
of αt. In fact, We only add a sparse constraint for estimating each at：
|Y|
|Y|
min
^t
-ET0Iog(E CSt [y,k]αt(k)) + C2katkι
y=1
k=1
(5)
s.t. ∀y ∈ Y,at(y) ≥ 0,	Xat(y)St(y) = 1
y
Where C2 is the hyper-parameter to control the level of target label sparsity, to estimate the target
label distribution. In the paper, We denote C2 = 0.1.
K Explicit and Implicit conditional learning
Inspired by Theorem 2, we need to learn the function g : X → Z and h : Z × Y → R to minimize:
min E 入口就：(h, g) + C0» λ[t]Ei

ʌ
,. ..... .,
W(y)Wi(St(ZIY = y)kT(z|Y = y))
This can be equivalently expressed as:
imin ∑ λ[t]R⅛t (h,g) + eCo£ λ[t]Ey^t(y) Wι(,St(z∣Y = y)kT(z∣Y = y))
+ (1 -C)CO X λ[t]Ey〜T(y)Wι(St(ZIY = y)kT(ZIY = y))
t
Due to the explicit and implicit approximation of conditional distance, we then optimize an alterna-
tive form:
min max
g,h d1 ,...,dT
∑λ[t]jR∣; (h,g) + eCo fλ[t]Ey〜"y)kCy - Cyk2
t
、------------{-------------}
Classification Loss
t
、----------------{----------
Explicit Conditional Loss
+ (1 -C)CO X λ[t][Ez〜St(Zy
t
、
t	(6)
a (Z)d(Z)- Ez〜T(Z)d(Z)]
{^^^^^^^^^—
Implicit Conditional Loss
}
}
Where
24
Under review as a conference paper at ICLR 2021
•	Cy = E(Zt,yt)〜St 1{yt=y}Zt the centroid of label Y = y in source St.
•	Cy = P(zt yp)〜T 1{yp=y}zt the centroid of pseudo-label Y = yp in target St. (If it is the
unsupervised DA scenarios).
•	αt(z) = 1{(z,y)〜St}&t(Y = y), namely if each pair observation (z, y) from the distribu-
tion, then at(Z = Z) = αt(Y = y).
•	di,…，dτ are domain discriminator (or critic function) restricted within 1-Lipschitz func-
tion.
•	∈ [0, 1] is the adjustment parameter in the trade-off of explicit and implicit learning.
Based on the equivalence form, our approach proposed a theoretical principled way to
tuning its weights. In the paper, we assume = 0.5.
•	T(y ) empirical target label distribution. (In the unsupervised DA scenarios, we approxi-
mate it by predicted target label distribution T(y).)
Gradient Penalty In order to enforce the Lipschitz property of the statistic critic function, we
adopt the gradient penalty term (Gulrajani et al., 2017). More concretely, given two samples Zs 〜
St(Z) and Zt 〜T(Z) We generate an interpolated sample Zint = ξzs + (1 - ξ)zt With ξ 〜Unif[0,1].
Then we add a gradient penalty ∣∣Vd(Zint)k2 as a regularization term to control the Lipschitz property
w.r.t. the discriminator dι,.∙∙ ,dτ.
L	Algorithm Descriptions
We propose a detailed pipeline of the proposed algorithm in the following, shown in Algorithm 2
and 3. As for updating λ and αt, we iteratively solve the convex optimization problem after each
training epoch and updating them by using the moving average technique.
For solving the λ and αt, we notice that frequently updating these two parameters in the mini-batch
level will lead to an instability result during the training. 5 As a consequence, we compute the
accumulated confusion matrix, weighted prediction risk, and conditional Wasserstein distance for
the whole training epoch and then solve the optimization problem. We use CVXPY to optimize the
two standard convex losses. 6
Comparison with different time and memory complexity. We discuss the time and memory
complexity of our approach.
Time complexity: In computing each batch we need to compute T re-weighted loss, T domain
adversarial loss and T explicit conditional loss. Then our computational complexity is still (O)(T )
during the mini-batch training, which is comparable with recent SOTA such as MDAN and DARN.
In addition, after each training epoch we need to estimate αt and λ, which can have time complexity
O(T |Y |) with each epoch. (If we adopt SGD to solve these two convex problems). Therefore, the
our proposed algorithm is time complexity O(T |Y|). The extra Y term in time complexity is due to
the approach of label shift in the designed algorithm.
Memory Complexity: Our proposed approach requires O(T) domain discriminator and O(T |Y|)
class-feature centroids. By the contrary, MDAN and DARN require O(T ) domain discriminator
and M3SDA and MDMN require O(T2 ) domain discriminators. Since our class-feature centroids
are defined in the latent space (Z), then the memory complexity of the class-feature centroids can be
much smaller than domain discriminators.
5In the label distribution shift scenarios, the mini-batch datasets are highly labeled imbalanced. If we
evaluate αt over the mini-batch, it can be computationally expensive and unstable.
6The optimization problem w.r.t. αt and λ is not large scale, then using the standard convex solver is fast
and accurate.
25
Under review as a conference paper at ICLR 2021
Algorithm 2 Wasserstein Aggregation Domain Network (unsupervised scenarios, one iteration)
ɪʌ * Till	IA	A ' 1 '	1	∕r-
Require: Labeled source samples S1, . . . , ST, Target samples T
Ensure: Label distribution ratio α and task relation simplex λ. Feature Learner g, Classifier h,
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
Statistic critic function d1, . . . , dT, class centroid for source Cty and target Cy (∀t = [1, T], y ∈
Y).
. . . DNN Parameter Training Stage (fixed αt and λ) / / /
for mini-batch of samples (xsι, ysι)〜Si, ..., (XST, YST)〜ST, (XT)〜T do
Predict target pseudo-label NT = argmaxyh(g(χT), y)
Compute source confusion matrix for each batch (un-normalized)
CSt = #[argmaxy0 h(Z, yO) = y, Y = k] (t = 1, . . . , T)
Compute the batched class centroid for source Cty and target Cy.
Moving Average for update source/target class centroid: (We set 1 = 0.7)
Source class centroid update	Cty = 1 ×Cty+(1-1)×Cty
Target class centroid update Cy = 1 × Cy + (1 - 1) × Cy
Updating g, h, d1, . . . , dT (SGD and Gradient Reversal), based on Eq.(6)
end for
...Estimation α^t and λ / / /
Compute the global(normalized) source confusion matrix
一	ʌ Γ	、，	，、	--..	.	一
CSt = StiargmaXy0 h(z, yO = y, Y = k] (t = 1, . . . , T)
13:	Solve αt (denoted as {α0t}tT=1) by Equation (2) (Or Eq.(5)) in the partial scenario).
14:	Update αt by moving average: αt = 1 × αt + (1 - 1) × α0t
15:	Compute the weighted loss and weighted centroid distance, then solve λ (denoted as λ0) from
Sec. 2.3.
16:	Updating λ by moving average: λ = 0.8 × λ + 0.2 × λ0
Algorithm 3 Wasserstein Aggregation Domain Network (Limited Target Data, one iteration)
ɪʌ	*	Till	1 A	A rɪ-i	1	/4- T 1	1	1 ∙ i`.	. ∙
Require: Labeled source samples S1, . . . , ST, Target samples T, Label shift ratio αt
Ensure: Task relation simplex λ. Feature Learner g, Classifier h, Statistic critic function
d1, . . . , dT, class centroid for source Cty and target Cy (∀t = i1, T], y ∈ Y).
1:	. . . DNN Parameter Training Stage (fixed λ) / / /
2:	fθr mini-batch of samples (xsi , y$i)〜Si, ..., (XST, YST)〜ST, (XT)〜T do
3:	Compute the batched class centroid for source Cty and target Cy.
4:	Moving Average for update source/target class centroid: (We set i = 0.7)
5:	Source class centroid update	Cty = i ×Cty+(1-i)×Cty
6:	Target class centroid update Cy = i × Cy + (1 - i) × Cy
7:	Updating g, h, di, . . . , dT (SGD and Gradient Reversal), based on Eq.(6).
8:	end for
9:	. . . Estimation λ / / /
10:	Solve λ by Sec. 2.3. (denoted as λ0)
11:	Updating λ by moving average: λ = i × λ + (1 - i) × λ0
M Dataset Description and Experimental Details
M.1 Amazon Review Dataset
We used the amazon review dataset (Blitzer et al., 2007). It contains four domains (Books, DVD,
Electronics, and Kitchen) with positive (label ”1”) and negative product reviews (label ”0”). The
data size is 6465 (Books), 5586 (DVD), 7681 (Electronics), and 7945 (Kitchen). We follow the
common data pre-processing strategies Chen et al. (2012): use the bag-of-words (BOW) features
then extract the top-5000 frequent unigram and bigrams of all the reviews.
We also noticed the original data-set are label balanced D(y = 0) = D(y = 1). To enhance the
benefits of the proposed approach, we create a new dataset with label distribution drift. Specifically,
in the experimental settings, we randomly drop 50% data with label ”0” (negative reviews) for all
the source data while keeping the target identical, showing in Fig (5).
26
Under review as a conference paper at ICLR 2021
We choose the MLP model with
•	feature representation function g: [5000, 1000] units
•	Task prediction and domain discriminator function [1000, 500, 100] units,
We choose the dropout rate as 0.7 in the hidden and input layers. The hyper-parameters are chosen
based on cross-validation. The neural network is trained for 50 epochs and the mini-batch size is 20
per domain. The optimizer is Adadelta with a learning rate of 0.5.
Experimental Setting We use the amazon Review dataset for two transfer learning scenarios (lim-
ited target labels and unsupervised DA). We first randomly select 2K samples for each domain. Then
we create a drifted distribution of each source, making each source ≈ 1500 and target sample still
2K.
In the unsupervised DA, we use these labeled source tasks and unlabelled target task, which aims to
predict the labels on the target domain.
In the conventional transfer learning, we random sample only 10% dataset (≈ 200 samples) as the
target training set and the rest 90% samples as the target test set.
We select C0 = 0.01 and C1 = 1 for these two transfer scenarios. In both practical settings, we set
the maximum training epoch as 50.
Figure 5: Amazon Review dataset (a) Original Label Training Distribution; (b) Label-Shifted dis-
tribution with sources tasks: Book, Dvd, Electronic, and target task Kitchen. We randomly drop
50% of the negative reviews for all the source distribution while keeping the target label distribution
unchanged.
M.2 Digit Recognition
We follow the same settings of Ganin et al. (2016) and we use four-digit recognition datasets in the
experiments MNIST, USPS, SVHN, and Synth. MNIST and USPS are the standard digits recogni-
tion task. Street View House Number (SVHN) Ganin et al. (2016) is the digit recognition dataset
from house numbers in Google Street View Images. Synthetic Digits (Synth) Ganin et al. (2016) is
a synthetic dataset that by various transforming SVHN dataset.
We also visualize the label distribution in these four datasets. The original datasets show an almost
uniform label distribution on the MNIST as well as Synth, (showing in Fig. 7 (a)). In our paper,
we generate a label distribution drift on the source datasets for each multi-source transfer learning.
Concretely, we drop 50% of the data on digits 5-9 of all the sources while we keep the target label
distribution unchanged. (Fig. 7 (b) illustrated one example with sources: Mnist, USPS, SVHN, and
Target Synth. We drop the labels only on the sources.)
MNIST and USPS images are resized to 32 × 32 and represented as 3-channel color images to
match the shape of the other three datasets. Each domain has its own given training and test sets
when downloaded. Their respective training sample sizes are 60000, 7219, 73257, 479400, and the
respective test sample sizes are 10000, 2017, 26032, 9553.
27
Under review as a conference paper at ICLR 2021
The model structure is shown in Fig. 6. There is no dropout and the hyperparameters are chosen
based on cross-validation. It is trained for 60 epochs and the mini-batch size is 128 per domain. The
optimizer is Adadelta with a learning rate of 1.0. We adopted γ = 0.5 for MDAN and γ = 0.1 for
DARN in the baseline (Wen et al., 2020).
Experimental Setting We use the Digits dataset for two transfer learning scenarios (limited target
labels and unsupervised DA). Notice the USPS data has only 7219 samples and the digits dataset
is relatively simple. We first randomly select 7K samples for each domain. We create a drifted
distribution of each source, making each source ≈ 5300, and the target sample still 7K.
In the unsupervised DA, we use these labeled source tasks and unlabelled target task, which aims to
predict the labels on the target domain.
In the transfer learning with limited data, we random sample only 10% dataset (≈ 700 samples) as
the target training set and the rest 90% samples as the target test set.
We select C0 = 0.01 and C1 as the maximum prediction loss C1 = maxt Rαt (h) as the hyper-
parameters across these two scenarios. The maximum training epoch is 60.
1.	Feature extractor: with 3 convolution layers.
’layer1’: ’conv’: [3, 3, 64], ’relu’: [], ’maxpool’: [2, 2, 0],
’layer2’: ’conv’: [3, 3, 128], ’relu’: [], ’maxpool’: [2, 2, 0],
’layer3’: ’conv’: [3, 3, 256], ’relu’: [], ’maxpool’: [2, 2, 0],
2.	Task prediction: with 3 fully connected layers.
’layer1‘：‘fc'： [*,512],'actfn': ‘relu’,
’layer2‘：‘fc'： [512, 100], ,act-fn': ‘relu’,
’layer3’： ’fc’： [100, 10],
3.	Domain Discriminator： with 2 fully connected layers.
reverse-gradient ()
'Iayer1': 'fc': [*,256],'actfn': ‘relu’,
’layer2’： ’fc’： [256, 1],
Figure 6:	Neural Network Structure in the digits recognition (Ganin et al., 2016)
Label distribution: Mmst
Λ3u"nbE
∙2Q
O O
Λ3unbE
∙n∏∏∏∏∏∏∏∏
Label distribution: USPS
「∏ r^ι, 口 ʃi □ ɪɪri r∣ rι
~~~~~~ Label distribution： SVHN ~~~~~
l~l 口 ∏. ∏ XL 口 ΓLΓ~∣ 口 口
..Label	distribution: Synth.
□000000000
Label distribution: Mmst
/ɪQ
Uoo
Λ3unbaH
Q
O
Λ3unbaH
Figure 7:	One example in Digits dataset with Sources: MNIST, USPS, SVHN and Target Synth.
We randomly drop 50% data on digits 5-9 in all sources while keeping target label distribution
unchanged.
M.3 Office-Home dataset
To show the dataset in the complex scenarios, we use the challenging Office-Home dataset
(Venkateswara et al., 2017). It contains images of 65 objects such as a spoon, sink, mug, and
28
Under review as a conference paper at ICLR 2021
pen from four different domains: Art (paintings, sketches, and/or artistic depictions), Clipart (cli-
part images), Product (images without background), and Real-World (regular images captured with
a camera). One of the four datasets is chosen as an unlabelled target domain and the other three
datasets are used as labeled source domains.
The dataset size is 2427 (Art), 4365 (Clipart), 4439 (Product), 4357 (Real-World). We follow the
same training/test procedure as (Wen et al., 2020). We additionally visualize the label distribution
D(y) in four domains in Fig.9, which illustrated the inherent different label distributions. We did
not re-sample the source label distribution to uniform distribution in the data pre-processing step.
All the baselines are evaluated under the same setting.
We use the ResNet50 (He et al., 2016) pretrained from the ImageNet in PyTorch as the base network
for feature learning and put an MLP with the network structure shown in Fig. 10.
Experimental Settings We use the original Office-Home dataset for two transfer learning scenar-
ios (unsupervised DA and label-partial unsupervised DA). We use SGD optimizer with learning rate
0.005, momentum 0.9 and Weight-decay value 1e-3. It is trained for 100 epochs and the mini-batch
size is 32 per domain. As for the baselines, MDAN use γ = 1.0 while DARN use γ = 0.5. We select
C0 = 0.01 and C1 as the maximum prediction loss C1 = maxt Rαt (h) as the hyper-parameters
across these tWo scenarios.
In the multi-source unsupervised partial DA, We randomly select 35 classes from the target (by
repeating 3 samplings), then at each sampling We run 5 times. The final result is based on these
3 × 5 = 15 repetitions.
Alann-Clock
Figure 8: Samples Images From Office-Home dataset (Venkateswara et al., 2017), which consists
four domains with non-uniform label distribution.
A	Label distribution: Art
I0OOO 佩在际温」尚川洲曲[]LMflfl[lJlnJIMUfl口口
υ υυυ	Label distribution: Clipart
ξ
2 0.025	... ................... π π π
10 ooo 娥。⅛L1MJ⅛iU⅛lLlflWIMLI溯加WIfl
υ υυυ	Label distribution: Product
ξ
I o OoolIhlMhIlIIIhIlnIlLIIilLIIIhLlm 加 LIlLlllIllUil
U UUU	Label distribution: ReaIWorId
ξ
φ 0.025 π π ππ
二(X)M删I岫端机螂端岫唧源邮佩林
0	10	20	30	40	50	60
Figure 9: Label distribution of Office-Home Dataset
N Analysis on the Pseudo-Labels
29
Under review as a conference paper at ICLR 2021
1.	Feature extractor: ResNet50 (He et al., 2016),
2.	Task prediction: with 3 fully connected layers.
’layerl‘： ‘fc'： [*,256],'batch_normalization'，'act_fn'： ‘Leaky_relu,,
'layer2': ‘fc'： [256, 256], 'batch-normalization', ‘act_fn': 'Leaky-relu',
‘layer3‘： ‘fc‘： [256, 65],
3.	Domain Discriminator： with 3 fully connected layers.
reverse-gradient ()
’layer1’: ‘fc’: [*,256],’batch_normaIization’,‘actfn’: ‘Leaky_relu’,
’layer2’: ‘fc’: [256, 256], ‘batch_normalization’, ‘act_fn‘: ‘Leaky-relu’,
‘layer3‘: ‘fc‘: [256, 1], ‘Sigmoid‘,
Figure 10: Neural Network Structure in the Office-Home
O	20	40
Epocħ
(a) Amazon Review: target
DVDs
0.70
0.65
0.60
0.50
0.45
0.40
(b) Digits: target SVHN
O	20	40	60
Epocħ
(c) Digits: target Synth
Figure 11: Evolution of accuracy w.r.t. the predicted target pseudo-labels in different tasks in unsu-
pervised DA.
O Analysis in Unsupervised DA
O. 1 Ablation Study: Different Dropping Rate
To show the effectiveness of our proposed approach, we change the drop rate of the source domains,
showing in Fig.(12). We observe that in task Book, DVD, Electronic, and Kitchen, the results are
significantly better under a large label-shift. In the initialization with almost no label shift, the
state-of-the-art DARN illustrates a slightly better (< 1%) result.
O.2 Additional Analysis on Amazon Dataset
We present two additional results to illustrate the working principles of WADN, showing in Fig.
(13) and (14).
We visualize the evolution of λ between DARN and WADN, which both used theoretical principled
approach to estimate λ. We observe that in the source shifted data, DARN shows an inconsistent
estimator of λ Fig. (13). This is different from the observation of Wen et al. (2020). We think it may
in the conditional and label distribution shift problem, using RS(h(z)) + Discrepancy(S(z), T(z))
to update λ is unstable. In contrast, WADN illustrates a relative consistent estimator of λ under the
source shifted data.
In addition, WARN gradually and correctly estimates the unbalanced source data and assign higher
wights αt for label y = 0 (first row of Fig.(14)). These principles in WADN jointly promote
significantly better results.
30
Under review as a conference paper at ICLR 2021
Dropped %
(b) Target: DVD
(a) Target: Book
(c) Target: Electronics
(d) Target: Kitchen
Figure 12: Different label drift levels on Amazon Dataset. Larger dropping rate means higher label
shift.
(b) WADN
(a) DARN
Figure 13:	Source Shifted Amazon Dataset. Evolution of λ during the training. B=Books, D=DVD,
E=Electronics, K=Kitchen.
O.3 Additional Analysis on Digits Dataset
We show the evolution of at on WADN, which verifies the correctness of our proposed principle.
Since We drop digits 5-9 in the source domains, the results in Fig. (15) illustrate a higher αt on these
digits.
P	Partial multi-source Unsupervised DA
From Fig. (16), WADN is consistently better than other baselines, given different selected classes.
31
Under review as a conference paper at ICLR 2021
Target B∞k	Target: DVD
O	10	20	30	40	50	0	10	20	30	40	50
(a) Target: Book	(b) Target: DVD
Target: Electronics	Target: Kitchen
0	10	20	30	40	50	0	10	20	30	40	50
(c) Target: Electronics	(d) Target: Kitchen
Figure 14:	Amazon Dataset. WADN approach: evolution of α during the training. Darker indicates
higher Value. Since we drop y = 0 in the sources, then the true αt (0) > 1 will be assigned with
higher value.
32
Under review as a conference paper at ICLR 2021
Target MNIST
NH>S ecdφ

Target SVHN
lωN≡e⅛φ
SdSn BMdW
(a) Target: MNIST
NH>S eedæ
SdSn e⅛w
(c) Target: Synth
(d) Target: USPS
Figure 15: Digits Dataset. WADN approach: evolution of α^t during the training. Darker indicates
higher value. Since we drop digits 5 - 9 on source domain, therefore, αt (y), y ∈ [5, 9] will be
assigned with a relative higher value.
Besides, when fewer classes are selected, the accuracy in DANN, PADA, and DARN is not drasti-
cally dropping but maintaining a relatively stable result. We think the following possible reasons:
•	The reported performances are based on the average of different selected sub-classes
rather than one sub-class selection. From the statistical perspective, if we take a close
look at the variance, the results in DANN are much more unstable (higher std) inducing
by the different samplings. Therefore, the conventional domain adversarial training is im-
proper for handling the partial transfer since it is not reliable and negative transfer still
occurs.
•	In multi-source DA, it is equally important to detect the non-overlapping classes and find
the most similar sources. Comparing the baselines that only focus on one or two principles
shows the importance of unified principles in multi-source partial DA.
•	We also observe that in the Real-World dataset, the DANN improves the performance by
a relatively large value. This is due to the inherent difficultly of the learning task itself. In
fact, the Real-World domain illustrates a much higher performance compared with other
domains. According to the Fano lower bound, a task with smaller classes is generally easy
to learn. It is possible the vanilla approach showed improvement but still with a much
higher variance.
Fig (17), (18) showed the estimated (^t with different selected classes. The results validate the
correctness of WADN in estimating the label distribution ratio.
33
Under review as a conference paper at ICLR 2021
ACC
(a) Target: Art
(b) Target: Clipart
(c) Target: Real-World
Figure 16: Multi-source Label Partial DA: Performance with different target selected classes.
34
Under review as a conference paper at ICLR 2021
X X XX XXXXXXXX X XXXXXXXXXXXXXXX X XXXXXXXXX XXXXXX X XXXX
(a) Target: Art
A⅛A∏ ,λa ∏ u/
X X X XX XXXXXXXX X xxxxxxxxxxxxxxx
(½B⅛<
XXXXXXXXX XXXXXX X XXXX
(b) Target: Clipart
XXXXXXXXX XXXXXX X XXXX

X X X XX XXXXXXXX X XXXXXXXXXXXXXXX X
(c) Target: Product
X X X XX XXXXXXXX X XXXXXXXXXXXXXXX X XXXXXXXXX XXXXXX X XXXX
(d) Target: Real-World
Figure 17: We select 15 classes and visualize estimated α^t (the bar plot). The "X" along the x-axis
represents the index of dropped 50 classes. The red curves are the ground-truth label distribution
ratio.
35
Under review as a conference paper at ICLR 2021
X XX XXXX X XXXX X X XX X X X XXX XX X
(a) Target: Art
(dn⅛<
1
X XX
X XX XXXX X XXXX X X
(b) Target: Clipart
XX
XX X X X XXX
(½BMdfV

MfLllJil
XXX XX
X XX XX X XX XXXX X XXXX X X
LλAA∕
XX X X X
(c) Target: Product

X
XX XX X XX XXXX X XXXX X X XX X X X XXX XX X
(d) Target: Real-World
Figure 18: We select 35 classes and visualize estimated c^t (the bar plot). The "X" along the x-axis
represents the index of dropped 30 classes. The red curves are the ground-truth label distribution
ratio.
36