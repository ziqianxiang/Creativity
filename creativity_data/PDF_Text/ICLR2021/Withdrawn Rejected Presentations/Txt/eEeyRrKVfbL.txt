Under review as a conference paper at ICLR 2021
Balancing training time vs. performance with
Bayesian Early Pruning
Anonymous authors
Paper under double-blind review
Ab stract
Pruning is an approach to alleviate overparameterization of deep neural network
(DNN) by zeroing out or pruning DNN elements with little to no efficacy at a given
task. In contrast to related works that do pruning before or after training, this paper
presents a novel method to perform early pruning of DNN elements (e.g., neurons
or convolutional filters) during the training process while preserving performance
upon convergence. To achieve this, we model the future efficacy of DNN elements
in a Bayesian manner conditioned upon efficacy data collected during the training
and prune DNN elements which are predicted to have low efficacy after training
completion. Empirical evaluations show that the proposed Bayesian early pruning
improves the computational efficiency of DNN training. Using our approach we
are able to achieve a 48.6% faster training time for ResNet-50 on ImageNet to
achieve a validation accuracy of 72.5%.
1	Introduction
Deep neural networks (DNNs) are known to be overparameterized (Allen-Zhu et al., 2019) as they
usually have more learnable parameters than needed for a given learning task. So, a trained DNN
contains many ineffectual parameters that can be safely pruned or zeroed out with little/no effect on its
predictive accuracy. Pruning (LeCun et al., 1989) is an approach to alleviating overparameterization
of a DNN by identifying and removing its ineffectual parameters while preserving its predictive
accuracy on the validation/test dataset. Pruning is typically applied to the DNN after training to
speed up testing-time evaluation. For standard image classification tasks with MNIST, CIFAR-10,
and ImageNet datasets, it can reduce the number of learnable parameters by up to 50% or more while
maintaining test accuracy (Han et al., 2015; Li et al., 2017; Molchanov et al., 2017).
In particular, the overparameterization of a DNN also leads to considerable training time being
wasted on those DNN elements (e.g., connection weights, neurons, or convolutional filters) which are
eventually ineffectual after training and can thus be safely pruned. Our work in this paper considers
early pruning of such DNN elements by identifying and removing them throughout the training
process instead of after training.1 As a result, this can significantly reduce the time incurred by the
training process without compromising the final test accuracy (upon convergence) much.
Recent work (Section 5) in foresight pruning (Lee et al., 2019; Wang et al., 2020) show that pruning
heuristics applied at initialization work well to prune connection weights without significantly
degrading performance. In contrast to these work, we prune throughout the training procedure, which
improves performance after convergence of DNNs, albeit with somewhat longer training times.
In this work, we pose early pruning as a constrained optimization problem (Section 3.1). A key
challenge in the optimization is accurately modeling the future efficacy of DNN elements. We achieve
this through the use of multi-output Gaussian process which models the belief of future efficacy
conditioned upon efficacy measurements collected during training (Section 3.2). Although the posed
optimization problem is NP-hard, we derive an efficient Bayesian early pruning (BEP) approximation
algorithm, which appropriately balances the inherent training time vs. performance tradeoff in pruning
prior to convergence (Section 3.3). Our algorithm relies on a measure of network element efficacy,
termed saliency (LeCun et al., 1989). The development of saliency functions is an active area of
research with no clear optimal choice. To accomodate this, our algorithm is agnostic, and therefore
1In contrast, foresight pruning (Wang et al., 2020) removes DNN elements prior to the training process.
1
Under review as a conference paper at ICLR 2021
flexible, to changes in saliency function. We use BEP to prune neurons and convolutional filters to
achieve practical speedup during training (Section 4).2 Our approach also compares favorably to
state-of-the-art works such as SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), and momentum
based dynamic sparse reparameterization (Dettmers & Zettlemoyer, 2019).
2	Pruning
Consider a dataset of D training examples X = {x1, . . . , xD}, Y = {y1, . . . , yD} and a neural
network Nvt parameterized by a vector of M pruneable network elements (e.g. weight parameters,
neurons, or convolutional filters) vt , [vta]a=1,...,M, where vt represent the network elements after
t iterations of stochastic gradient descent (SGD) for t = 1, . . . , T. Let L(X , Y; Nvt) be the loss
function for the neural network Nvt . Pruning aims at refining the network elements vt given some
sparsity budget B and preserving the accuracy of the neural network after convergence (i.e., NvT),
which can be stated as a constrained optimization problem (Molchanov et al., 2017):
minm∈{0,1}M |L(X, Y; NmvT) - L(X,Y;NvT)| s.t. ||m||0 ≤ B	(1)
where is the Hadamard product and m is a pruning mask. Note that we abuse the Hadamard
product for notation simplicity: for a = 1, .., M, ma × vTa corresponds to pruning vTa if ma = 0,
and keeping vTa otherwise. Pruning a network element refers to zeroing the network element or the
weight parameters which compute the network element. Any weight parameters which reference the
output of the pruned network element are also zeroed since the element outputs a constant 0.
The above optimization problem is difficult due to the NP-hardness of combinatorial optimization.
This leads to the approach of using saliency function s which measures efficacy of network elements
at minimizing the loss function. A network element with small saliency can be pruned since it’s
not salient in minimizing the loss function. Consequently, pruning can be done by maximizing the
saliency of the network elements given the sparsity budget B :
maxm∈{0,1}M PaM=1 mas(a; X, Y,NvT, L) s.t. ||m||0 ≤ B	(2)
where s(a; X, Y,NvT , L) measures the saliency of vTa at minimizing L after convergence through
T iterations of SGD. The above optimization problem can be efficienctly solved by selecting the B
most salient network elements in vT .
The construction of the saliency function has been discussed in many existing works: Some ap-
proaches derived the saliency function from first-order (LeCun et al., 1989; Molchanov et al., 2017)
and second-order (Hassibi & Stork, 1992; Wang et al., 2020) Taylor series approximations of L.
Other common saliency functions include L1 (Li et al., 2017) or L2 (Wen et al., 2016) norm of the
network element weights, as well as mean activation (Polyak & Wolf, 2015). In this work, we use
a first-order Taylor series approximation saliency function defined for neurons and convolutional
filters3 (Molchanov et al., 2017), however our approach remains flexible to arbitrary choice of saliency
function on a plug-n-play basis.
3	Bayesian Early Pruning
3.1	Problem statement
As has been mentioned before, existing pruning works based on the saliency function are typically
done after the training convergence (i.e., (2)) to speed up the testing-time evaluation, which waste
considerable time on training these network elements which will eventually be pruned. To resolve
this issue, We extend the pruning problem definition (2) along the temporal dimension, allowing
network elements to be pruned during the training process consisting of T iterations of SGD.
2Popular deep learning libaries do not accelerate sparse matrix operations over dense matrix operations.
Thus, pruning network connections cannot be easily capitalized upon with performance improvements. It is
also unclear whether moderately sparse matrix operations (i.e., operations on matrices generated by connection
pruning) can be significantly accelerated on massively parallel architectures such as GPUs (see Yang et al.
(2018) Fig. 7). See Section 5 in BUlUc & Gilbert (2008) for challenges in parallel sparse matrix multiplication.
3Implementation details of this saliency function can be found in Appendix A.1.
2
Under review as a conference paper at ICLR 2021
Let sta , s(a; X, Y,Nvt , L) be a random variable which denotes the saliency of network element vta
after t iterations of SGD, St，[sa]a=ι,...,M for t = 1,...,T, and，个小，[st]t=τι,…,τ2 be a vector
of saliency of all the network elements between iterations τ1 and τ2. Our early pruning algorithm is
designed with the goal of maximizing the saliency of the unpruned elements after iteration T, yet
allowing for pruning at each iteration t given some computational budget Bt,c for t = 1, . . . , T:
ρτ(mτ-1, Bτ,c, Bs)，maxm『mτ ∙ ST (3a)
s.t.	∣∣mτ||o ≤ Bs (3b) mτ ≤ mτ -1 (3c) Bτ,c ≥ 0(3d)
Pt(mt-ι,Bt,c,Bs)，maXmtEp(st+i|Si：t)[ρt+ι(mt,Bt,c-||mt||o,Bs)] (4a)
s.t. mt ≤ mt-1 (4b)
where Bs is the trained network sparsity budget, Si：t is a vector of observed values for si：t, m0
is an M-dimensional 1's vector, and mt ≤ mt-ι represents an element-wise comparison between
mt and mt-1: mta ≤ mta-1 for a = 1, . . . , M. At each iteration t, the saliency St is observed and
mt ∈ {0, 1}M in ρt represents a pruning decision performed to maximize the expectectation of ρt+1
conditioned upon saliency measurements S1:t collected up to and including iteration t. This recursive
structure terminates with base case ρT where the saliency of the unpruned elements is maximized
after T iterations of training.
In the above early pruning formulation4, constraints (3c) and (4b) ensure pruning is performed in
a practical manner whereby once a network element is pruned, it can no longer be recovered in
a later training iteration. We define a trained network sparsity budget, Bs (3b), which may differ
significantly from initial network size ||m0||0 (e.g., in the case where the network is trained on
GPUs, but deployed on resource constrained edge or mobile devices). We also constrain a total
computational effort budget Bt,c which is reduced per training iteration t by the number of unpruned
network elements ||mt||0. We constrain BT,c ≥ 0 (3d) to ensure training completion within the
specified computational budget. Here we assume that a more sparse pruning mask mt corresponds
to lower computational effort during training iteration t due to updating fewer network elements.
Finally, (3a) maximizes the saliency with a pruning mask mT constrained by a sparsity budget Bs
(3b). Our early pruning formulation balances the saliency of network elements after convergence
against the total computational effort to train such network (i.e., mT ∙ ST vs. PT=ι |mt ∣∣o). ThiS
appropriately captures the balancing act of training-time early pruning whereby the computational
effort is saved by early pruning network elements while preserving the saliency of the remaining
network elements after convergence.
3.2	Modeling the Saliency with Multi- Output Gaussian Process
To solve the above early pruning problem, we need to model the beliefp(S1:T) of the saliency for
computing the predictive belief p(st+i：T |Si：t) of the future saliency in (4a). At the first glance, one
may consider to decompose the belief: p(S1:T) , QaM=1 p(S1a:T) and model the saliency S1a:T ,
[sta]t=1,...,T of each network element independently. Such independent models, however, ignore the
co-adaptation and co-evolution of the network elements which have been shown to be a common
occurrence in DNN (Hinton et al., 2012; Srivastava et al., 2014; Wang et al., 2020). Also, modeling
the correlations between the saliency of different network elements explicitly is non-trivial since
considerable feature engineering is needed for representing diverse network elements such as neurons,
connections, or convolutional filters.
To resolve such issues, we use multi-output Gaussian process (MOGP) to jointly model the belief
p(S1:T) of all saliency measurements. To be specific, we assume that the saliency sta of the a-th
network element at iteration t is a linear mixture5 of Q independent latent functions {uq(t)}qQ=1:
Sa，PQ=ι Yauq(t). As shown in (AIVarez & Lawrence, 2011), if each Uq(t) is an independent
GP with prior zero mean and covariance kq(t, t0), then the resulting distribution overp(S1:T) is a
multivariate Gaussian distribution with prior zero mean and covariance determined by the mixing
4In contrast to PruneTrain (Lym et al., 2019), our problem definition balances training time vs. performance
under an additional constraint on the trained network size (3b). We discuss this further in Section 5.
5Among the various types of MOGPS (see AIVarez & Lawrence (2011) for a detailed review.), we choose
this linear model such that the correlations between sta and sta00 can be computed analytically.
3
Under review as a conference paper at ICLR 2021
weights: cov[sta, sta00] = PqQ=1 γqaγqa0kq(t, t0). This explicit covariance between sta and sta00 helps to
exploit the co-evolution and co-adaptation of network elements within the neural networks.
To capture the horizontal asymptote trend of s1a , . . . , saT as visualized in Appendix A.2, we turn to a
kernel used for modeling decaying exponential curves known as the “exponential kernel” (Swersky
et al., 2014) and set kq(t, t0)，(+：；* where aq and βq are hyperparameters of MOGP and can
be learned via maximum likelihood estimation (Alvarez & Lawrence, 2011). Then, given a vector of
observed saliency Si：t, the MOGP regression model can provide a Gaussian predictive distribution
for any future saliency st/. Thus, the predictive mean 仙：,∣ i：t，旧k，，| Si：t] of the saliency s% and the
predictive (co)variance σ,a[t，cov^a，, sa； | Si：t] between the saliency Sa and sa can be computed
analytically, as detailed in Appendix A.3.
3.3	Early pruning algorithm
Solving the above optimizing problem (3) and (4) is difficult due to the interplay between
[mt，]t，=t,…,t, [Bto,c]t0=t,…,t, and mτ ∙ ST. Instead, We consider a simplification of the above
problem by only considering solutions of the form mT-1 = mT-2 = . . . = mt which yields6:
pt(mt-ι, Bt,c, Bs)，maxmt Ep(ST|Si：t) M(mt,Bt,c - (T - t)∣mt∣∣o,Bs)]	(5)
This approach allows us to lift (3d) from (3), to which we add a Lagrange multiplier and achieve:
pt(mt-ι, Bt,c, Bs)，maxmt Ep(ST|Si：t) [Pt(mt,Bs)] + λ (Bt,c - (T - t)∣mt∣∣o)	⑹
for t = 1,...,T - 1 and PT is defined as PT without constraint (3d). Consequently, such a PT
can be solved in a greedy manner as in (2). Afterwards, we will omit Bt,c as a parameter of
Pt as it no longer constrains the solution of PT. Note that the presence of an additive penalty
in a maximization problem is due to the constraint BT,c ≥ 0 ⇔ -BT,c ≤ 0 which is typically
expected prior to Lagrangian reformulation. The above optimization problem remains NP-hard as
Ep(ST同:t) [Pt(mt, Bs)] is submodular in mt (see Appendix B). Although greedy approximations
exist for submodular optimization, their running time of O(||mt-1 ||20) remains far too slow due to
the large number of network elements in DNNs. Fortunately, it can be significantly simplified by
exploiting the following lemma (its proof is in Appendix C.):
Lemma 1. Let e(i) be an M -dimensional one-hot vectors with the i-th element be 1. ∀ 1 ≤
a, b ≤ M; m ∈ {0,1}M s.t. m ∧ (e(a) ∨ e(b)) = 0. Given a vector of observed Saliency Si：t, if
μT|i：t ≥ μT|i：t andμT|i：t ≥0，then
Ep(ST|Si：t) [PT(m ∨ e(b) )] - Ep(ST|Si：t) [PT(m ∨ e(a))] ≤ μT|1:t φ(νM) + θ φ(νM)
Where θ , ^Tai：t+ σ‰,- 2σTb∣ιt
CDF and PDF, respectively.
V，μT|i:t — μT|i:t，and Φ and φ are Standard normal
Here, '∨' and ‘八' represent bitwise OR and AND operations, respectively. The bitwise OR operation
is used to denote the incluSion of e(a) or e(b) in mt . Due to the strong tail decay7 of φ and Φ,
Lemma 1 indicates at moSt marginal possible improvement provided by opting for mt = m ∨ e(b)
as opposed to mt = m ∨ e(a) given μT«：t ≥ μT∣ι∙t∙
Lemma 1 admits the following approach to optimize pt： starting with mt = 0m, We consider the
inclusion of network elements in mt by the descending order of {μT∣ i：t}M=i which can be computed
analytically using MOGP. A network element denoted by e(a) is included in mt if it improves
the objective in (5). The algorithm terminates once the highest not-yet-included element does not
improve the objective function as a consequence of the penalty term outweighing the improvement in
Ep(ST|Si：t) [pt]. The remaining excluded elements are then pruned.
Following the algorithm sketch above, we define the utility of network element vta with respect
to candidate pruning mask mt≤mt-ι which measures the improvement in Ep(ST|Si：t)[PT] as a
6We omit (4b) as it is automatically satisfied due to our simplification.
7Note as μT口土 ≥ μT∣lt, Φ(∙) ≤ 0.5 and experiences tail decay proportional to μT中土 一 μT口土.
4
Under review as a conference paper at ICLR 2021
consequence of inclusion of e(a) in mt :
∆(a, mt, Si：t,Bs)，Ep(ST|Si：t) [ρτ(e(a) ∨ mt, Bs)- pτ(mt, Bs)].	⑺
We can now take a Lagrangian approach to pruning decisions during iteration t by balancing the
utility of network element vta against the change of the penalty (i.e., λ(T - t)) in Algorithm 1. Due
to the relatively expensive cost of performing early pruning, we chose to early prune every Tstep
iterations of SGD. Typically Tstep was chosen to correspond to 10-20 epochs of training. To compute
∆(∙) We sampled fromp(sτ|Si：t) and used a greedy selection algorithm per sample as in (2). During
implementation, we also enforced an additional hard constraint ||mt||0 ≥ Bs which we believe is
desirable for practicality reasons. We used a fixed value of B1,c = ||m0||0T0 + Bs(T - T0) in all
our experiments.
Algorithm 1 Bayesian Early Pruning		
Require: N, v1, T0, Tstep, T, B1,c, Bs, λ		. DNN N , Lagrangian penalty λ
1	Si：To — train(NVι, T0)	. Train for T0 iterations to create seed dataset.
2	BTo,c — B1,c - T0 dim(VI)	. Track computational effort expenditure.
3	for k J 0,..., T-TO; t J To + kTstep do Tstep	. Early prune every Tstep iterations from T0 .
4	μT|1:t, σT|1:t J MOGP(SI:t)	. Train and perform inference.
5	ST J argsort(-μτ|i：t)	. Sort descending.
6	:	mt J 0dim(vt )	. Initial pruning mask.
7	:	for a J s1T , . . . , sdTim(vt ) do :	if Bt,c - (T -t)||mt||0 > 0 then	. Consider each netWork element.
8		. Remaining Bt,c budget can support training vta .
9	:	mt = mt ∨ e(a)	
10	else if ∆(a, mt,鼠:t,Bs) ≥ λ(T -	t) then . Balance utility against change of penalty.
11	:	mt = mt ∨ e(a)	
12	:	else	
13	:	break	
14	:	prune(vt, mt)	. dim(vt) is reduced here.
15	:	Bt+Tstep,c J Bt,c - Tstep||mt||0 :	St+1:t+Tstep J train(Nvt , Tstep)	
16		. Continue training With pruned netWork.
17	: return N	
4	Experiments and Discussion
We evaluate our modeling approach as Well as our BEP algorithm on the CIFAR-10, CIFAR-
100 (Krizhevsky, 2009), and ImageNet (Deng et al., 2009) datasets. For CIFAR-10/CIFAR-100 We
used a benchmark Convolutional Neural NetWork (CNN) With 4 convolutional layers, and 1 dense
layer.8 For ImageNet We validated on the ResNet-50 architecture (He et al., 2016a).
Due to the cubic time complexity of MOGPs, We used a variational approximation (Hensman et al.,
2015). In all of our models, We used 60 variational inducing points per latent function. We used
GPFloW library (MattheWs et al., 2017) to build our models.
4.1	Modeling evaluation
A key assertion in our approach is the importance of capturing co-adaptation and co-evolution effects
in netWork elements. To verify our MOGP approach captures these effects, We compare MOGP
vs. GP belief modeling Where GP assumes independence in saliency measurements across netWork
elements (i.e., p(s1:T) , QaM=1 p(s1a:T)).
A dataset of saliency measurements of convolutional filters and neurons Was constructed by instru-
menting the training process of our 5-layer CNN on the CIFAR-10/CIFAR-100 dataset. Keras (Chollet,
2015) Was used to train this model over 150 epochs.9
8Code available at https://github.com/keras-team/keras/blob/master/examples/
cifar10_cnn.py
9Complete experimental setup details found in Appendix G.2.
5
Under review as a conference paper at ICLR 2021
O 1∞ 200 300 400
Time
0 100 200 300 400
Time
0	1∞ 200 300 400
Time
Figure 1: Visualization of qualitative differences between GP and MOGP prediction. Top: GP,
Bottom: 18-MOGP. Dataset is separated into training (green) and validation (blue). Posterior belief
of the saliency is visualized as predictive mean (red line), and 95% confidence interval (error bar).
Table 1: Comparing log likelihood (standard error) of test data for independent GPs (GP) vs. MOGP
with n latent functions (n-MOGP) on collected saliency measurements from CIFAR-100 training.
Measurements are given as a multiple of -104 (lower is better). MOGP outperforms GP, particularly
on the small dataset. Results are averaged over 20 runs. Extremely large values are due to the GP
model being unable to fit the data.
	Small dataset			Medium dataset			Large dataset		
	Lyr 1	Lyr2	Lyr 3	Lyr 1	Lyr2	Lyr 3	Lyr 1	Lyr2	Lyr 3
GP	0.75(0.06)	5.7(5.7)e4	5.6(5.6)e4	0.64(0.04)	0.70(0.04)	2.13(0.05)	3.4(3.4)e3	0.31(0.02)	1.06(0.02)
4-MOGP	0.79(0.05)	0.98(0.12)	3.13(0.10)	0.44(0.04)	0.60(0.10)	2.29(0.06)	0.12(0.01)	0.24(0.03)	1.07(0.03)
8-MOGP	0.65(0.05)	0.89(0.11)	3.00(0.09)	0.38(0.04)	0.60(0.10)	2.20(0.06)	0.10(0.01)	0.18(0.01)	1.02(0.03)
18-MOGP	0.62(0.05)	0.84(0.11)	2.93(0.10)	0.36(0.03)	0.56(0.10)	2.22(0.07)	0.09(0.01)	0.18(0.01)	1.01(0.03)
32-MOGP	0.65(0.05)	0.85(0.09)	2.89(0.10)	0.36(0.03)	0.59(0.i0)	2.16(0.06)	0.09(0.02)	0.18(0.01)	1.00(0.03)
We trained belief models with small (t = [0, 26] epochs), medium (t = [0, 40] epochs), and large
(t = [0, 75] epochs) training dataset of saliency measurements. For GPs, a separate model was
trained per network element (convolutional filter, or neuron). For MOGP, all network elements in
a single layer10 shared one MOGP model. We evaluated these models using log likelihood of the
remainder of the saliency measurements. We present the performance of the models in Table 1 for
CIFAR-100.11 Our MOGP approach better captures the saliency of network elements than a GP
approach. Furthermore, using additional latent functions improves MOGP modeling with diminishing
returns. We visualize the qualitative differences between GP and MOGP prediction in Figure 1. We
observe that MOGP is able to capture the long term trend of saliency curves with significantly less
data than GP.
4.2	Small-scale experiments
We applied the early pruning algorithm on the aforementioned architecture, and training regimen.
We investigated the behavior of the penalty parameter, λ. We observed that the penalty parameter
was difficult to tune properly, either being too aggressive at pruning, or too passive. To rectify this
issue, we used a feedback loop to determine the penalty at iteration t, λt dynamically. Dynamic
penalty scaling12 uses feedback from earlier pruning iterations to increase or decrease the iteration
penalty at time t: λt = λ [(1∕λ)∧ ((T - t)||mt∣∣o∕Bt,c - 1)]. The dynamic penalty is increased if
the anticipated compute required to complete training, (T - t)||mt||0 begins to exceed the amount of
compute budget remaining, Bt,c. In such case, a higher penalty is needed to satisfy the computational
budget constraint as per (6). We compare dynamic penalty scaling, and penalty without scaling in
Fig. 2 using T0 = 20 epochs, Tstep = 10 epochs for the first convolutional layer of our CNN. Going
forward, we use dynamic penalty scaling in our experiments.
10In our observations, jointly modeling the belief of multiple layers’ saliency measurements using MOGP
yielded no measurable improvement in log-likelihood.
11ForCIFAR-10 see Appendix G.1.
12Further details can be found in Appendix D.
6
Under review as a conference paper at ICLR 2021
Static penalty
-Δ-
垩
壬
Dynamic penalty scaling
0 5 0 5 0
3 2 2 1 1
6u-uEal S-E 殳-euo4o>uou
Bs = 19, λ=le-09
Bs = 19, λ=le-04
Bs = 19, λ=le-02
Bs = 9.入=Ie-O9
Bs = 9.入=Ie-O4
Bs = 9, λ=le-02
30
(g)<g>lg>(g)<g><g>(g)(g)<g>lg>(g)(g


~ψ 一鱼鱼鱼已鱼至垩至

ca⅛≡⅛B-s-se⅛e⅛se
÷-÷÷-÷-+-÷-÷-+÷-+-÷÷-+-+
0	20	40	60	80	100	120	140	0	20	40	60	80	100	120	140
Epochs	Epochs
Figure 2: Comparing dynamic penalty scaling vs. static on pruning a 32-convolutional filter layer in
a CNN. Dynamic penalty scaling encourages gradual pruning across a wide variety of settings of λ.
Table 2: Performance (standard error) against percentage of neurons/filters pruned per layer with
varying λ for tested algorithms.
	CIFAR-10					CIFAR-100		
	70%	80%	90%	95%	70%	80%	90%	95%
DSR	74.1(0.2)%	65.3(0.4)%	52.8(0.5)%	22.1(5.0)%	37.9(0.7)%	29.7(0.1)%	17.5(0.3)%	4.4(1.4)%
SNIP	75.4(4.7)%	67.7(0.7)%	50.8(0.8)%	29.4(4.9)%	22.9(9.0)%	15.7(6.1)%	9.9(3.7)%	2.2(1.2)%
GraSP	74.6(0.6)%	66.5(0.9)%	50.7(0.6)%	32.9(1.0)%	28.4(7.0)%	22.6(5.4)%	13.9(3.2)%	1.0(0.0)%
BEP 1e-2	75.9(0.3)%	69.7(0.4)%	54.8(1.0)%	18.9(5.4)%	40.6(0.2)%	32.2(0.6)%	19.1(0.5)%	7.1(1.6)%
BEP 1e-4	75.4(1.7)%	70.5(3.2)%	55.7(0.9)%	36.1(1.1)%	41.3(0.3)%	32.4(0.3)%	19.7(0.8)%	8.5(0.8)%
BEP 1e-7	76.0(0.1)%	70.6(0.2)%	56.2(0.4)%	30.4(5.1)%	40.6(0.2)%	33.0(0.5)%	19.5(0.5)%	6.6(1.5)%
We compare our work with SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), and momentum-based
dynamic sparse remaparameterization (DSR) (Dettmers & Zettlemoyer, 2019). To compare against
DSR, we instantiate a smaller network of the size BEP yields after training has completed as it is
a prune and regrow method. The SNIP and GraSP approaches are extended to neurons/filters by
averaging the saliencies of the constituent weight parameters. We experimented with various degrees
of sparsity, using BEP to prune a portion of filters/neurons of each layer.13 We present the results in
Table 2. Our approach better preserves performance at equivalent sparsity. A lower penalty yields
higher performing results showing λ serves well at balancing performance vs. computational budget.
We investigate the robustness of BEP and MOGP hyperparameters. We vary the number of MOGP
variational inducing points, MOGP latent functions, and Tstep and observe the performance of BEP
1e-4 on CIFAR-10/CIFAR-100 at 80%, 90%, and 95% sparsity. We present these results in Table 3.
We observe that in general, all hyperparameters are robust to changes. Mild degradation is observed
in the extremal hyperparameter settings.
4.3	Speeding up ResNet training on ImageNet
Our chief goal in this work is to speed up training of large-scale DNNs such as ResNet (He et al.,
2016a;b) on the ImageNet dataset. Pruning ResNet requires a careful definition of network element
saliency to allow pruning of all layers. ResNet contains long sequences of residual units with
matching number of input/output channels. The inputs of residual units are connected with shortcut
connections (i.e., through addition) to the output of the residual unit.14 Due to shortcut connections,
this structure requires that within a sequence of residual units, the number of inputs/output channels
of all residual units must match exactly. This requires group pruning of residual unit channels for
13In our observations saliency measurements don’t well capture network element efficacy when comparing
across layers. Thus pruning whole networks using network element saliency yields poor performing networks
with bottlenecks. This limitation of saliency functions is well known (See Molchanov et al. (2017) Appendix
A.1 and A.2; Wang et al. (2020) Section 3 last paragraph). Development of saliency functions which overcome
this shortcoming while remaining performant is a difficult open problem outside the scope of this work.
14Precise details of the ResNet architecture may be found in He et al. (2016a) Section 3.
7
Under review as a conference paper at ICLR 2021
Table 3: Ablation study showing performance (standard error) vs. varying early pruning hyperpa-
rameters: MOGP variational inducing points (Ind. pnts.), MOGP latent functions (Lat. func.), Tstep.
Default setting for hyperparameters are 60, 1.0×, and 10 respectively. Outside of the higest sparsity
setting, 95%, all hyperparameters are robust to changes, with mild degradation observed in the
extremal settings.
Il I	CIFAR-10	CIFAR-100
Il I 80%	90%	95%	80%	90%	95%
Ind. pnts.	26 40 60 90	69.6(0.4)%	56.8(0.2)%	29.7(4.9)%	31.7(0.6)%	19.2(0.5)%	8.3(0.7)% 70.9(0.l)%	55.6(0.6)%	30.5(5.1)%	32.3(0.7)%	19.6(0.3)%	6.6(1.4)% 70.5(3.2)%	55.7(0.9)%	36.1(1.1)%	32.4(0.3)%	19.7(0.8)%	8.5(0.8)% 70.4(0.3)%	55.1(0.7)%	35.5(1.9)%	32.6(0.4)%	18.5(0.6)%	8.7(0.3)%
Lat. func.	0.25× 0.50× 1.0× 2.0×	70.4(0.4)% 55.6(0.8)% 35.8(0.2)%^^32.6(0.3)% 16.3(3.8)% 7.4(1.8)% 70.0(0.2)%	56.9(0.4)%	34.5(0.6)%	32.1(0.5)%	18.9(0.7)%	7.0(1.5)% 70.5(3.2)%	55.7(0.9)%	36.1(1.1)%	32.4(0.3)%	19.7(0.8)%	8.5(0.8)% 69.8(0.3)%	55.7(0.7)%	34.8(0.5)%	32.0(0.4)%	20.8(0.2)%	7.7(0.4)%
Tstep	2 5 10 20	69.2(0.5)% 54.7(0.6)% 29.4(5.0)%^^32.1(0.2)% 20.0(0.3)% 4.3(1.5)% 70.3(0.2)%	55.6(0.5)%	31.6(5.4)%	32.7(0.4)%	19.4(0.4)%	5.2(1.8)% 70.5(3.2)%	55.7(0.9)%	36.1(1.1)%	32.4(0.3)%	19.7(0.4)%	8.5(0.8)% 70.3(0.2)%	56.2(0.1)%	29.8(5.0)%	32.8(0.5)%	19.6(0.4)%	6.8(1.5)%
a sequence of residual units, where group pruning an output channel of a residual unit sequence
requires pruning it from the inputs/outputs of all residual units within the sequence.15
We trained ResNet-50 with BEP as well as SNIP and GraSP.16 We group pruned less aggressively
as residual unit channels feed into a large number of residual units, thus making aggressive pruning
likely to degrade performance. We ran BEP iterations at t = [15, 20, 25, 35, 45, 55, 75] epochs. We
trained for 100 epochs on 4× Nvidia Geforce GTX 1080 Ti GPUs. More experimental details found
in Appendix G.2. We present our results in Table 4. We achieve higher performance than related
techniques, albeit at longer wall time. Our approach captures the training time vs. performance
tradeoff present in DNNs, unlike competing approaches.
5	Related Work
Pruning and related techniques. Initial works in DNN pruning center around saliency based
pruning after training including Skeletonization (Mozer & Smolensky, 1988), Optimal Brain Dam-
age and followup work (Hassibi & Stork, 1992; LeCun et al., 1989) as well as sensitivity based
pruning (Karnin, 1990). In recent years, saliency functions been adapted to pruning neurons or
convolutional filters. Li et al. (2017) define a saliency function on convolutional filters by using the
L1 norm. Molchanov et al. (2017) propose using a first-order Taylor-series approximation on the
objective function as a saliency measure. Dong et al. (2017) propose layer wise pruning of weight
parameters using a Hessian based saliency measure. Several variants of pruning after training exist.
Han et al. (2015) propose iterative pruning where pruning is performed in stages alternating with
fine tune training. Guo et al. (2016) suggest dynamic network surgery, where pruning is performed
on-the-fly during evaluation time. Li et al. (2017) and He et al. propose reinforcement learning for
pruning decisions. A comprehensive overview may be found in Gale et al. (2019).
Knowledge distillation (Hinton et al., 2015; Lu et al., 2017; Tung & Mori, 2019; Yim et al., 2017) aim
to transfer the capabilities of a trained network into a smaller network. Weight sharing (Nowlan &
Hinton, 1992; Ullrich et al., 2017) and low rank matrix factorization (Denton et al., 2014; Jaderberg
et al., 2014) aim to compress the parameterization of neural networks. Network quantization (Cour-
bariaux et al., 2015; Hubara et al., 2017; Micikevicius et al., 2018) use lower fidelity representation
of network elements (e.g. 16-bit) to speed up training and evaluation. Although speedup during
training is achievable through network quantization, this technique requires hardware support and only
15We formally define saliency on residual unit sequences in Appendix G.3
16We omit comparison to DSR due to differing underlying deep learning library which makes walltime
comparisons inaccurate.
8
Under review as a conference paper at ICLR 2021
Table 4: BEP vs. SNIP and Grasp on ResNet-50 on ImageNet dataset. We vary percentage of residual
unit sequence channels (Seq) and filters pruned (Lyr). ‘Train’ refers to wall time during network
training, ‘Prune’ refers to pruning modeling/inference overhead. Benchmark wall time for ResNet-50
is 55h on our hardware. Benchmark performance is 75.7% for unpruned ResNet-50.
	Seq 30%, Lyr 60%				Seq 60%, Lyr 90%				Seq 30%, Lyr 98%			
	Top-1	Top-5	Train Prune		Top-1	Top-5	Train Prune		Top-1	Top-5	Train	Prune
SNIP	72.0%	90.6%	27.9h	0.7h	62.0%	83.8%	15.8h	0.7h	50.9%	74.2%	21.7h	0.7h
GraSP	72.1%	90.6%	27.1h	2.7h	61.6%	83.6%	16.5h	2.7h	52.2%	75.4%	21.7h	2.7h
BEP 1e-1	72.2%	90.6%	31.6h	2.9h	62.0%	83.8%	22.8h	1.5h	53.7%	76.8%	27.8h	2.5h
BEP 1e-4	72.5%	91.0%	34.8h	2.2h	62.3%	84.5%	22.0h	1.8h	53.5%	76.6%	27.8h	2.6h
provides coarse granularity in trading off computational effort vs. performance. Current GPUs only
extend native support to 16-bit floating point operations. Furthermore, our approach is orthogonal to
quantization allowing the techniques to be combined for further speedup.
Initialization time or training time pruning. Frankle & Carbin (2019) show that a randomly
initialized DNN contains a small subnetwork, which if trained by itself, yields equivalent performance
to the original network. SNIP (Lee et al., 2019) and GraSP (Wang et al., 2020) propose pruning
connection weights prior to the training process through a first order and second order saliency
function respectively. Sparse Evolutionary Training (Mocanu et al., 2018) propose initializing
networks with sparse topology prior to training. Narang et al. (2017) consider connection weight
pruning during training for recurrent neural networks using a heuristic approach.
Dynamic sparse reparameterization considers pruning and regrowing parameter weights during the
training process (Bellec et al., 2018; Dettmers & Zettlemoyer, 2019; Mostafa & Wang, 2019). Dai et al.
(2019) propose a grow and prune approach to learning network architecture and connection layout.
We differ from existing work as our focus is on speeding up neural network training, meanwhile other
works in training time pruning aim to achieve sparse network layouts. To the best of our knowledge,
except for small speedups presented in (Dettmers & Zettlemoyer, 2019), the above works do not
demonstrate speedup during training time using popular deep learning libraries run on modern GPUs.
PruneTrain (Lym et al., 2019) also proposes pruning filters during training to achieve speedup while
minimizing degradation to performance with periodic pruning iterations. In contrast to our approach,
PruneTrain does not allow specification of the desired network size after training. A specified network
size may be useful if training for resource constrained devices such as mobile phones or edge devices.
We compare with PruneTrain under the early pruning problem definition in Appendix E.
6	Conclusion
This paper presents a novel efficient algorithm to perform pruning of DNN elements such as neurons,
or convolutional layers during the training process. To achieve early pruning before the training
converges while preserving the performance of the DNN upon convergence, a Bayesian model (i.e.,
MOGP) is used to predict the saliency of DNN elements in the future (unseen) training iterations by
exploiting the exponentially decaying behavior of the saliency and the correlations between saliency
of different network elements. Then, we exploit a property (Lemma 1) of the objective function and
propose an efficient Bayesian early pruning algorithm. Empirical evaluations on benchmark datasets
show that our algorithm performs favorably to related works for pruning convolutional filters and
neurons. Our approach remains flexible to changes in saliency function, and appropriately balances
the training time vs. performance tradeoff in training DNNs. We are able to train an early pruned
ResNet-50 model achieving a 48.6% speedup (37h vs. 55h) while maintaining a validation accuracy
of 72.5%.
9
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overparameterized
neural networks, going beyond two layers. In Proc. NeurIPS, pp. 6155-6166, 2019.
Mauricio A. Alvarez and Neil D. Lawrence. Computationally efficient convolved multiple output
Gaussian processes. JMLR, 12(1):1459-1500, 2011.
Karl Johan Astrom, Tore Hagglund, Chang C Hang, and Weng K Ho. Automatic tuning and adaptation
for pid controllers-a survey. Control Engineering Practice, 1(4):699-714, 1993.
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert A. Legenstein. Deep rewiring:
Training very sparse deep networks. In Proc. ICLR, 2018.
Richard E Bellman. Adaptive control processes: a guided tour. Princeton university press, 2015.
Aydin Buluc and John R. Gilbert. Challenges and advances in parallel sparse matrix-matrix multipli-
cation. In Proc. ICCP, pp. 503-510, 2008.
FranCOiS Chollet, 2015. URL https://github.com/fchollet/keras.
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Binaryconnect: Training deep neural
networks with binary weights during propagations. arXiv:1511.00363, 2015.
Xiaoliang Dai, Hongxu Yin, and Niraj K. Jha. Nest: A neural network synthesis tool based on a
grow-and-prune paradigm. IEEE Trans. Computers, 68(10):1487-1497, 2019.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. ImageNet: A large-scale
hierarchical image database. In Proc. CVPR, pp. 248-255, 2009.
Emily L. Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun, and Rob Fergus. Exploiting linear
structure within convolutional networks for efficient evaluation. In Proc. NeurIPS, pp. 1269-1277,
2014.
Tim Dettmers and Luke Zettlemoyer. Sparse networks from scratch: Faster training without losing
performance. arXiv:1907.04840, 2019.
Xin Dong, Shangyu Chen, and Sinno Jialin Pan. Learning to prune deep neural networks via
layer-wise optimal brain surgeon. In Proc. NeurIPS, pp. 4857-4867, 2017.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. In Proc. ICLR, 2019.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks.
arXiv:1902.09574, 2019.
Yiwen Guo, Anbang Yao, and Yurong Chen. Dynamic network surgery for efficient DNNs. In Proc.
NeurIPS, pp. 1379-1387, 2016.
Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for
efficient neural networks. In Proc. NeurIPS, pp. 1135-1143, 2015.
Babak Hassibi and David G. Stork. Second order derivatives for network pruning: Optimal brain
surgeon. In Proc. NeurIPS, pp. 164-171, 1992.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proc. CVPR, pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In Proc. ECCV, pp. 4432-4440, 2016b.
Yihui He, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han. AMC: Automl for model
compression and acceleration on mobile devices. In Proc. ECCV, pp. 815-832.
James Hensman, Alexander Matthews, and Zoubin Ghahramani. Scalable variational Gaussian
process classification. In Proc. AISTATS, pp. 351-360, 2015.
10
Under review as a conference paper at ICLR 2021
Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov.
Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580,
2012.
Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.
arXiv:1503.02531, 2015.
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. Quantized
neural networks: Training neural networks with low precision weights and activations. JMLR, 18
(1):6869-6898, 2017.
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks
with low rank expansions. In Proc. BMVC, 2014.
Ehud D. Karnin. A simple procedure for pruning back-propagation trained neural networks. IEEE
Trans. Neural Networks, 1(2):239-242, 1990.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proc. ICLR,
2015.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis, Univ.
Toronto, 2009.
Yann LeCun, John S. Denker, and Sara A. Solla. Optimal brain damage. In Proc. NeurIPS, pp.
598-605, 1989.
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H. S. Torr. Snip: Single-shot network pruning
based on connection sensitivity. In Proc. ICLR, 2019.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for
efficient convnets. In Proc. ICLR, 2017.
Liang Lu, Michelle Guo, and Steve Renals. Knowledge distillation for small-footprint highway
networks. In Proc. ICASSP, pp. 4820-4824, 2017.
Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and Mattan Erez.
Prunetrain: fast neural network training by dynamic sparse model reconfiguration. In Proc. SC, pp.
36:1-36:13, 2019.
Alexander Matthews, Mark van der Wilk, Tom Nickson, Keisuke Fujii, Alexis Boukouvalas, Pablo
Leon-Villagra, ZoUbin Ghahramani, and James Hensman. GPflow: A Gaussian process library
using tensorflow. JMLR, 18(1):1-6, 2017.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory F. Diamos, Erich Elsen, David Garcia,
Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed
precision training. In Proc. ICLR, 2018.
Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artificial neural networks with adaptive sparse connectivity
inspired by network science. Nature, 9(1):1-12, 2018.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz. Pruning convolutional
neural networks for resource efficient inference. In Proc. ICLR, 2017.
Hesham Mostafa and Xin Wang. Parameter efficient training of deep convolutional neural networks
by dynamic sparse reparameterization. In Proc. ICML, pp. 4646-4655, 2019.
Michael Mozer and Paul Smolensky. Skeletonization: A technique for trimming the fat from a
network via relevance assessment. In Proc. NeurIPS, pp. 107-115, 1988.
Saralees Nadarajah and Samuel Kotz. Exact distribution of the max/min of two gaussian random
variables. Trans. VLSI, 16(2):210-212, 2008.
Sharan Narang, Greg Diamos, Shubho Sengupta, and Erich Elsen. Exploring sparsity in recurrent
neural networks. In Proc. ICLR, 2017.
11
Under review as a conference paper at ICLR 2021
Steven J. Nowlan and Geoffrey E. Hinton. Simplifying neural networks by soft weight-sharing.
Neural Computation, 4(4):473-493, 1992.
Adam Polyak and Lior Wolf. Channel-level acceleration of deep face representations. IEEE Access,
3:2163-2175, 2015.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. JMLR, 15(1):1929-1958,
2014.
Kevin Swersky, Jasper Snoek, and Ryan Prescott Adams. Freeze-thaw Bayesian optimization.
arXiv:1406.3896, 2014.
Frederick Tung and Greg Mori. Similarity-preserving knowledge distillation. In Proc. ICCV, pp.
1365-1374, 2019.
Karen Ullrich, Edward Meeds, and Max Welling. Soft weight-sharing for neural network compression.
In Proc. ICLR, 2017.
Chaoqi Wang, Guodong Zhang, and Roger B. Grosse. Picking winning tickets before training by
preserving gradient flow. In Proc. ICLR, 2020.
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in
deep neural networks. In Proc. NeurIPS, pp. 2074-2082, 2016.
Carl Yang, Aydin Buluc, and John D. Owens. Design principles for sparse matrix multiplication on
the GPU. In Proc. Euro-Par, pp. 672-687, 2018.
Junho Yim, Donggyu Joo, Ji-Hoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast
optimization, network minimization and transfer learning. In Proc. CVPR, pp. 7130-7138, 2017.
12
Under review as a conference paper at ICLR 2021
Mle> Λ3u*-es
____
Mle> Λ3u*-≈
Mle> Λ3u*-≈
Figure 3: Convolutional filter saliency over 150 epochs of SGD on CIFAR-10.
A Modeling details
A.1	Saliency function
In this work, we use a first order Taylor-series saliency function proposed by Molchanov et al. (2017).
Our design (Section 3) remains flexible to allow usage of arbitrary saliency functions in a plug-n-play
basis. We partition a DNN of L layers, where each layer ' contains e` convolutional filters, into
a sequence of convolutional filters [z'/c=1', '；'. Each filter z`e : RC'-1 ×w'-1×H'-1 → RW' ×H'
,c」'=1,…,L	,c
can be considered as one network element in VT and z',c(P'-1) , R(W',c * P'-1 + b',c) where
W',c ∈ RC'× O'× O0, b`,c are kernel weights and bias.with receptive field θ` X O', '*’ represents
the convolution operation, R is the activation function, P'-ι represents the output of z`-i ，
[z'-1'c0]c0=1'...'C'-1 with P0 corresponding to an input xd ∈ X, and W', H' are width and height
dimensions of layer ' for ' = 1,...,L. Let Nz'：z&0，z` ◦,..., ◦z' denote a partial neural network
of layers [',..., '∖i≤'≤'≤l. The Taylor-series saliency function on the convolutional filter z',c
denoted as s([`, c]) is defined17:
s([' c]) , ɪ XX _1_ WXH' dL(P'Xd),yd;Nz'+1:ZL)P(Xd)	⑻
s([',c]) , D 乙 W' X H' 工	dP(χd)	P',c,j .	(8)
d=1	j=1	''c'j
where P('Xd) is the output of the partial neural network Nz1:z' with xd as the input and
[P'X'cd'j]j=1'...'W'×H' interprets the output of the c-th filter in vectorized form. This function uses the
first-order Taylor-series approximation of L to approximate the change in loss if z''c was changed to a
constant 0 function. Using the above saliency definition, pruning filter z''c corresponds to collectively
zeroing W''c, b''c as well as weight parameters18 [W'+1'c0'{:':'c}]c0=1'...'C'+1 of z'+1 which utilize
the output of zl'c. This definition can be extended to elements (e.g. neurons) which output scalars by
setting W' = H' = 1.
A.2 On the choice of the “exponential kernel”
We justify our choice of the exponential kernel as a modeling mechanism by presenting visualizations
of saliency measurements collected during training, and comparing these to samples drawn from the
exponential kernel kq(t, t0)，(方十丁十乔,as shown in Figs. 3-4. Both the saliency and the function
samples exhibit exponentially decaying behavior, which makes the exponential kernel a strong fit for
modeling saliency evolution over time.
Furthermore we note that the exponential kernel was used to great effect in Swersky et al. (2014)
with respect to modeling loss curves as a function of epochs. Loss curves also exhibit asymptotic
behavior, similar to saliency measurement curves, thus providing evidence for the exponential kernel
being an apt fit for our task.
A.3 Predictive distribution of the saliency
Let the prior covariance matrix be Kτm，[cov[sa, sa0]]a,a==1,, ,M for any 1 ≤ τι ≤ τ2 ≤ T.
Given a vector of observed saliency Si：t, the MOGP regression model can provide a Gaus-
17For brevity, we omit parameters X , Y, Nz1 :zL , L.
18Here We use {} to distinguish indexing into a tensor from indexing into the sequence of tensors [W'+ι *].
13
Under review as a conference paper at ICLR 2021
(a) α = β = 0.5
(b) α = β = 1.0
Figure 4: Function samples drawn from the exponential kernel.
(c) α = β = 1.5
Sian predictive distribution p(st，|Si：t) = N3t，|i：t, Kt，|i：t) for any future Saliency st，with the
following posterior mean vector and covariance matrix: μt∕∣Lt ，K[t，t]K-1Si：t, Kt，|i：t ，
Kt，：t，一 K[t0t]K-1K>0t] where K%]，[covS，, sa]]<a=a-i=1，.t.,M. Then, the a-th element 仙：，"花
of μt∕∣Lt is the predictive mean of the saliency Sa. And the [a, a0]-th element of K[t，t] denoted as
σ,a[t is the predictive (CO)Variance between the saliency s? and sf，.
B SUBMODULARITY OF E[pτ]
In (6), the problem of choosing m from {0, 1}M can be considered as selecting a subset A of
indexes from {1, . . . , M} such that mta = 1 for a ∈ A, and mta = 0 otherwise. Therefore,
P(m) , Ep(ST同：t)[PT(m, Bs)] Can be considered as a set function which we will show to be
submodular. To keep notation consistency, we will remain using P(m) instead of representing it as a
function of the index subset A.
Lemma 2 (Submodularity). Let m0, m00 ∈ {0, 1}M, and e(a) be arbitrary M-dimensional one
hot vector with 1 ≤ a ≤ M. We have P(m0 ∨ e(a)) - P(m0) ≥ P (m00 ∨ e(a)) - P(m00) for any
m0 ≤ m00, m0 ∧ e(a) = 0, and m00 ∧ e(a) = 0.
Proof. According to (3),
Ep(ST包:t)[PT(m, Bs)]=	Ep(ST包:t)	max	[mτ	∙	sτ,	s.t. ∣∣mτ||o ≤	Bs,mτ	≤	m]
mT
Let α(m)，argmaXmT [mτ ∙ ST, s.t. ∣∣mτ∣∣o ≤ Bs, mτ≤m] return the optimized mask mτ
given any m, Λm , min(α(m) sT) be the minimal saliency of the network elements selected at
iteration T for P (m). Then, we have
P(m ∨ e(a)) = Ep(ST包:t) [pτ(m ∨ e(a), Bs)]
=Ep(ST | Si：t) [ρT (m, BS ) — λe + maχ(sτ, λe )]
The second equality is due to the fact that the network element vTa would only replace the lowest
included element in mT in order to maximize the objective. Then,
P(m ∨ e(a)) - P(m)
=Ep(ST同:t) [Pτ(m,Bs) — Am + max(sT, Am)] — Ep(ST同：t) [Pt(m,Bs)]
=Ep(ST|si：t) [—Am + maχ(sτ, Am)]
=Ep(ST同:t) [max(sT — Am, 0)]	(9)
Given m0 ≤ m00, we have Am， ≤ Am，， since mT ≤ m in α(m0) is a tighter constraint than that in
α(m00). Consequently, we can get sta — Am， ≥ sta — Am，，, and thus
[P (m0 ∨ e(a)) — P(m0)] ≥ [P (m00 ∨ e(a)) — P(m00)] .
□
14
Under review as a conference paper at ICLR 2021
C PROOF OF LEMMA 1
We restate Lemma 1 for clarity.
Lemma 1. Let e(i) be an M -dimensional one-hot vectors with the i-th element be 1. ∀ 1 ≤
a, b ≤ M; m ∈ {0,1}M s.t. m ∧ (e(a) ∨ e(b)) = 0. Given a vector of observed Saliency Si::t, if
μT|i：t ≥ μT|i：t andμT|i：t ≥0，then
Ep(ST |Si：t) [PT(m ∨ e(b) 力-Ep(ST |Si：t) [PT(m ∨ e(叫] ≤ μT|1：t φ(V/。)+ θ φ(V物
where θ，《σTa1f + σT∣1.t - 2σaτ∖∖t , V，μT∣1.t 一 μT∣1.t , and Φ and φ are Standard normal
CDF and PDF, respectively.
To prove this Lemma, we prove the following first:
Lemma 3. Ep(ST|Si：t) [pt(m ∨ e(b))] - Ep(ST|Si：t) [pt(m ∨ e(a))] ≤ EmaX(ST -ST, 0)].
Proof. Due to (9), we have
Ep(ST恒Lt) [pt(m ∨ e(b))i - Ep(ST|Si：t) [pt(m ∨ e(a))i
= P(m ∨ e(b)) - P(m) - (P(m ∨ e(a)) - P (m))
=Ep(ST同:t) [maχ(sτ - Am, O)] - Ep(ST同：t) [maχ(sτ - Am, O)]
=Ep(ST|Si：t)[max(sT - Am, O) - max(sT - Am, 0)]	(10)
=Ep(ST|Si：t) [maχ(sT -	sT,	Am	- sT)	- maχ(0,	Am - ST)]	(II)
≤ Ep(ST|Si：t) [maχ(sT -	sT,	0)]	(12)
The equality (11) is achieved by adding Am - saT in each term of the two max functions in (10). The
inequality (12) can be proved by considering the following two cases:
If Am - saT ≥ O, then
maχ(sT - sT , Am - sT ) - maχ(O, Am - sT )
= maχ(sT - sT , Am - sT ) - (Am - sT )
= maχ(sT - sT - (Am - sT ), O)
≤ maχ(sbT - saT, O) .
If Am - saT < O, then
maχ(sT - sT , Am - sT ) - maχ(O, Am - sT )
= maχ(sT - sT , Am - sT )
≤ maχ(sbT - saT, O) .
□
Next we utilize a well known bound regarding the maximum of two Gaussian random variables
(Nadarajah & Kotz, 2008), which we restate:
Lemma 4. Let sa,sb be Gaussian random variables with means μa,μb and Standard devia-
tions σa, σb, then E[maχ(sa, sb)] ≤ μaΦ(μ -μ ) + μbΦ(μ -μ ) + θφ(μ -μ ) where θ ，
,[σb]2 + [σa]2 — 2CoV(Sb, Sa) and Φ, φ are standard normal CDF and PDF respectively.
15
Under review as a conference paper at ICLR 2021
Then,
Ep(ST|Si：t)[maX(ST -ST, 0)]
=Ep(ST|Si：t)[maX(ST,sT)] - Ep(ST|Si：t) [sT]
≤ (μT∣ι-,t + μT∣rt)Φ( ^--^t) + θφ(	) - 〃*‘
= "T∣rtΦ("T11：t -"T11：t) + θΦ(μ⅛T^) + 〃T|i：t (φ (“Tl1：t -”T11：t! - l)
J b ɪ ∕μT|1：t - μT|1：t\ l 八」∕μT|1：t - μT|1:t\
≤ μT utφ(	——)+ θφ( —―L-)
ba
The first inequaltiy follows from Lemma 4. The second inequaltiy is due to Φ( T|1:t & T|1:t) ≤ 1
andμT|i：t ≥ 0.
D Dynamic penalty scaling as a feedback loop
We designed a feedback loop to automatically determine λt during early pruning. A proportional
feedback loop can be defined as follows19:
λt , λ + Kp × e(t)	(13)
where Kp ≥ 0 is a proportional constant which modulates λt according to a signed measure of error
e(∙) at time t. Note that λt ≥ λ as e(t) ≥ 0, and the opposite occurs if e(t) ≤ 0, which allows the
error to serve as feedback to determine λt. Implicitly, λt asserts some control over e(t + 1), and thus
closing the feedback loop.
Traditional PID approaches to determine Kp do not work in our case as λ may vary over several
orders of magnitude. Consequently, a natural choice for Kp is λ itself which preserves the same order
of magnitude between Kp and λ:
λt = λ + λ × e(t) = λ(1 + e(t)).
(14)
Here we make two decisions to adapt the above to our task. First, as λ is likely to be extremely small,
we use exponentiation, as opposed to multiplication. Secondly as λ ≤ 1 in practice, we use 1 - e(t)
as an exponent:
λt = λ∧[1-e(t)] = λ [(1∕λ)∧e(t)].
The above derivation is complete with our definition of e(t):
e(t) , (T - t)||mt||0/Bt,c - 1.
(15)
(16)
The above determines error by the discrepancy between the anticipated compute required to complete
training (T - t)||mt||0, vs. the remaining budget Bt,c with e(t) = 0 if the two are equal. This is a
natural measure of feedback for λ as we expect the two to be equal if λ is serving well to early prune
the network.
E Comparis on with PruneTrain
We compare with PruneTrain (Lym et al., 2019) in Table 5. PruneTrain uses an orthogonal technique
of dynamically increasing the minibatch size to achieve further wall time improvements. This prevents
accurate wall time comparisons between BEP and PruneTrain. To compare with PruneTrain which
19This approach is inspired from Proportional-Integral-Derivative (PID) controllers (Bellman, 2015), see
AStrom et al. (1993) for an introductory survey.
16
Under review as a conference paper at ICLR 2021
Table 5: Comparing BEP with PruneTrain on ResNet-50 on ImageNet dataset. PruneTrain uses a
stronger ResNet-50 baseline with Top-1 76.2% performance. BEP uses a 75.7% baseline. ‘Train’
refers to wall time during network training, ‘Prune’ refers to pruning modeling/inference overhead.
Benchmark wall time for ResNet-50 is 55h on our hardware. Comparison performed using 47%
of ResNet-50 baseline inference FLOPs. Train FLOPs refers to proportion of ResNet-50 baseline
training FLOPs used to train the network.
	47% Inference FLOPs			
	Top-1 (∆) Top-5	Train Prune Train FLOPs		
PruneTrain	74.3% (-1.9%)	-	-	-	60%
BEP 1e-1	74.1% (-1.6%) 91.8%	35.5h	3.9h	55.2%
BEP 1e-4	73.9% (-1.8%) 91.7%	36.5h	3.2h	56.0%
Table 6: Notations used elsewhere in the paper.
Notation	Definition
M T Sa	Total number of network elements in Neural Network. Total iterations of SGD in training procedure. Random variable representing the saliency measurement of network element a at time t.
St sTi：T2 si:t mt Bs Bt,c M：0|i：t 0 啷:t	Sequence of random variables [s，]a=i,…,m Sequence of random variables [st]t=ι …,t. The realization of random variable s、：t. Pruning mask at time t Trained network sparsity budget. Computational effort budget at time t. E[s；o | Si：t] cov[sao,Sta | si：t]
does not constrain the trained network size (3b), we train ResNet-50 under equivalent inference cost
as a network trained by PruneTrain. To compute train/inference cost (FLOPs) for a convolutional
layer, we used a formula defined in Molchanov et al. (2017) A.1:
FLOPs , 2HW(CinK2+1)Cout
(17)
where H, W, Cin are input height, width, and channels respectively, K is the convolutional kernel
size, and Cout is the number of output channels of the layer.
Under equivalent inference cost, BEP 1e-1 outperforms PruneTrain in Top-1 performance. We also
find that BEP 1e-1 and BEP 1e-4 consumes fewer training FLOPs when compared to baseline.
It should be noted that PruneTrain does not provide a mechanism to constrain the trained network
size, thus it is unclear how to utilize it in order to solve the early pruning problem (3), (4).
F Table of notations
We list a table of notations used elsewhere in the paper in Table 6.
17
Under review as a conference paper at ICLR 2021
Table 7: Comparing log likelihood (standard error) of test data for Independent GPs (GP) vs. MOGP
with n latent functions (n-MOGP) on collected saliency measurements from CIFAR-10 training.
	Small dataset				Medium dataset						Large dataset			
	Lyr 1	Lyr2		Lyr 3	Lyr 1		Lyr2	Lyr 3			Lyr 1	Lyr2		Lyr 3
GP	1.19(0.5)	1.08(0.06)		1.07(1.07)e5	0.96	(0.04)	0.93(0.03)	2.47(0.04)		0.49	(0.01)	0.48(0.01)	1.33	(0.02)
4-MOGP	1.15(0.05)	0.89	(0.06)	2.44(0.05)	0.91	(0.02)	0.80(0.03)	2.20	(0.03)	0.38(0.02)		0.39(0.02)	1.25	(0.02)
8-MOGP	1.09(0.04)	0.86	(0.05)	2.38(0.04)	0.84	(0.03)	0.78(0.03)	2.16	(0.03)	0.32	(0.01)	0.35(0.02)	1.20	(0.02)
18-MOGP	0.97(0.04)	0.80	(0.05)	2.33(0.04)	0.89	(0.03)	0.76(0.03)	2.13	(0.03)	0.31	(0.01)	0.35(0.02)	1.20	(0.02)
32-MOGP	0.96(0.06)	0.81	(0.06)	2.32(0.04)	0.79	（0.03）	0.74(0.03)	2.13	(0.03)	0.31	(0.01)	0.34(0.02)	1.20	(0.02)
G More Experimental Results and Experimental Details
G. 1 GP VS. MOGP LOG-LIKELIHOOD ON CIFAR-10 DATASET
Table 7 presents the results of the experiment in Section 4.1 for the CIFAR-10 dataset.
G.2 Experimental details
To train our CIFAR-10 and CIFAR-100 models we used an Adam optimizer (Kingma & Ba, 2015)
with an initial learning rate of 0.001. The learning rate used an exponential decay of k = 0.985, and a
batch size of32 was used. Training was paused three times evenly spaced per epoch. During this pause,
we collected saliency measurements using 40% of the training dataset. This instrumentation subset
was randomly select from the training dataset at initialization, and remained constant throughout
the training procedure. We performed data preprocessing of saliency evaluations into a standardized
[0, 10] range.20 We used (8) to measure saliency of neurons/convolutional filters. For the convolutional
layers we used 12 latent MOGP functions. For the dense layer we used 4 latent MOGP functions.
For our ResNet-50 model we used an SGD with Momentum optimizer with an initial learning rate of
0.1. The learning rate was divided by ten at t = [30, 60, 80] epochs. We collected saliency data every
5 iterations of SGD, and averaged them into buckets corresponding to 625 iterations of SGD to form
our dataset. We used a minimum of 4 latent functions per MOGP, however this was dynamically
increased if the model couldn’t fit the data up to a maximum of 15.
We sampled 10K points from our MOGP model to estimate ∆(∙) for CIFAR-10/CIFAR-100. For
ResNet we sampled 15K points. We repeated experiments 5 times for reporting accuracy on CIFAR-
10/CIFAR-100.
G.3 Pruning on ResNet
ResNet architecture is composed of a sequence of residual units: z`，F (P'-ι) + P'-ι, where P'-ι
is the output of the previous residual unit z`-i and '+' denotes elementwise addition. Internally, F
is typically implemented as three stacked convolutional layers: F(P'-ι)，M ◦ z`2 ◦ z`i] (P'-ι)
where z`1 , z`2 , z`3 are convolutional layers. Within this setting we consider convolutional filter
pruning. Although z`1 , z`2 may be pruned using the procedure described earlier. Pruning z`3 requires
a different procedure. Due to the direct addition of P'-ι to F(P'-ι), the output dimensions of Z'-ι
and z`3 must match exactly. Thus a ResNet architecture consists of sequences of residual units of
length B with matching input/output dimensions: Z，[Z']'=ι,…,b, s.t. dim(Pι) = dim(P2)=
...=dim(PB). We propose group pruning of layers [z'3]'=1,…,b where filters are removed from
allz'3 in a residual unit sequence in tandem. We define s([Z,c])，PB=I s(['3,c]), where s(∙) is
defined for convolutional layers as in (8). To prune the channel c from ζ, we prune it from each layer
in [z'3]'=1,…,b . Typically we pruned sequence channels less aggressively than convolutional filters
as these channels feed into several convolutional layers.
20Generally, saliency evaluations are relatively small (≤ 0.01), which leads to poor fitting models or positive
log-likelihood. Precise details of our data preprocessing is in Appendix G.4.
18
Under review as a conference paper at ICLR 2021
G.4 Data preprocessing
We followed the same data preprocessing procedure for both our small scale and ImageNet ex-
Periments. To standardize the Saliency measurements for a training dataset Si：t in our mod-
eling experiments we clip them between 0 and an upper bound computed as follows: ub ,
Percentile(Si：t, 95) X 1.3. This procedure removes outliers. We used 1.3 as a multiplier, as this
uPPer bound is used to transform test dataset as well, which may have higher saliency evaluations.
After clipping the training data, we perform a trend check for each element va by fitting a Linear
Regression model to the data S?#. For Sa：t with an increasing trend (i.e., the linear regression model
has positive slope) we perform the transformation S?：t = Ub — Sa.；. The reasoning behind this is that
the exponential kernel strongly prefers decaying curves. After this preprocessing, we scale up the
saliency measurements to a [0,10] range: Si：t = Si：t × 10. We found that without scaling to larger
values, log-likelihood of our models demonstrated extremely high positive values due to small values
of unscaled saliency measurements.
We transform the test data in our modeling experiments St+ι.τ with the same procedure using the
same ub and per-element va regression models as computed by the training data. We measure
log-likelihood after this transformation for both the test dataset in our small scale experiments.
During the BEP Algorithm, the same steps are followed, however we inverse the trend check
transformation (Sa.=Ub — S?：t) on the predicted MOGP distribution of ST prior to sampling for
estimation of △(•).
19