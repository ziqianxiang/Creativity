Under review as a conference paper at ICLR 2021
Semantic Inference Network for Few-shot
Streaming Label Learning
Anonymous authors
Paper under double-blind review
Ab stract
Streaming label learning aims to model newly emerged labels for multi-label clas-
sification systems, which requires plenty of new label data for training. However,
in changing environments, only a small amount of new label data can practically be
collected. In this work, we formulate and study few-shot streaming label learning
(FSLL), which models emerging new labels with only a few annotated examples
by utilizing the knowledge learned from past labels. We propose a meta-learning
framework, Semantic Inference Network (SIN), which can learn and infer the
semantic correlation between new labels and past labels to adapt FSLL tasks from
a few examples effectively. SIN leverages label semantic representation to reg-
ularize the output space and acquires label-wise meta-knowledge based on the
gradient-based meta-learning. Moreover, SIN incorporates a novel label decision
module with a meta-threshold loss to find the optimal confidence thresholds for
each new label. Theoretically, we demonstrate that the proposed semantic inference
mechanism could constrain the complexity of hypotheses space to reduce the risk of
overfitting and achieve better generalizability. Experimentally, extensive empirical
results and ablation studies illustrate the superior performance of SIN over the prior
state-of-the-art methods on FSLL.
1	Introduction
In many real-world systems, with more in-depth exploration and understanding of data, the number of
associated labels gradually increases (Raghavan & Hafez, 2000; Krempl et al., 2014; Zhou, 2016). For
instance, when a photo is posted to Facebook or Twitter, the photo will be continuously tagged with
new labels by users who are browsing, and the classification system needs to be updated accurately
according to the incoming new labels (Dietterich, 2017). As for previous classification systems, on
the one hand, independent learning for only new labels would ignore the correlated information from
past labels. On the other hand, integrating past labels with new labels to retrain the whole system each
time would be prohibitively computationally expensive. Overall, the problem of learning emerging
new labels is referred to as streaming label learning (You et al., 2016; Wang et al., 2020b).
Exploring the relationship between new labels and past labels is crucial for modelling newly-arrived
labels. The intuition is formulated in the pioneering work of You et al. (2016), which assumes that
the new label vectors are a linear combination of past label vectors and that the new classifier can
inherit the linear combination to obtain the relationship across labels without retraining the historical
classifier. Wang et al. (2020b) proposed a DNN-based framework to learn label dependencies and
distill knowledge from the historical model, which models new labels more accurately. In these
streaming label learning tasks, the newly labeled data is assumed to be sufficiently available, which
could be easily violated in changing environments.
In practical applications, collecting large number of emerging new labels is often prohibitively
expensive and time-consuming. In such a situation where only a few examples associated with new
labels are available, both training from scratch and fine-tuning on the small dataset are likely to cause
severe overfitting, leading to generalization issues in streaming label learning approaches. Although
few-shot learning methods (Li Fei-Fei et al., 2006; Snell et al., 2017; Finn et al., 2017; Xing et al.,
2019) are designed to accommodate limited labeled data, existing methods mainly focus on single-
label examples. It is nontrivial for few-shot learning to adapt to the multiple newly-arrived labels,
since the output space (the number of possible label sets) exponentially grows with the increasing
1
Under review as a conference paper at ICLR 2021
number of category labels. Moreover, the label correlation between past labels and new labels ignored
by traditional few-shot learning approaches would be crucial for modeling the emerging new labels.
We formulate the above real-world learning problem as Few-shot Streaming Label Learning (FSLL),
where new labels associated with a few examples are arrived on the fly, to learn a model for the
newly-arrived labels with the help of the knowledge learned from past labels. Our motivation
stems from the challenges of modeling the new labels in FSLL: overwhelming output space and
low-data problem. Inspired by human infant learning, in which linguistic information can help
infants recognize new concepts from very few objects (Jackendoff, 1987; Smith, 2003; Smith &
Gasser, 2005), we hypothesize that in the context of FSLL, semantic representation from text can be
a powerful source of information to distinguish new labels. In a word embedding space, the semantic
representation naturally brings correlation across labels, e.g., the label “cake” is semantically close to
the label “food”. The semantic correlation across labels could help to structure and regularize the
overwhelming output space of FSLL. Moreover, we will leverage the correlation from past labels to
new labels and acquire knowledge from historical models to alleviate the low-data problem.
In this paper, we propose a meta-learning framework, Semantic Inference Network (SIN), to solve
FSLL problem with only a few examples. Unlike using learnable parameters as output layer weights
in the previous neural networks, SIN utilizes the fixed semantic representation of labels (learned
from the unsupervised text corpora), which empowers the network to map features into a semantic
space. We propose a semantic inference mechanism that can extract semantic features and exploit
the correlation between new labels and past labels, to learn more disentangled representations.
Furthermore, we propose a label decision module with a novel meta-threshold loss function to find
the optimal confidence thresholds for each new label. SIN employs a gradient-based meta-learning
paradigm to learn label-wise meta-knowledge while fast adapting semantic inference parameters to
distinguish multiple new labels within a few optimization updates.
To verify the effectiveness and the performance of SIN, we conduct both theoretical and experimental
analyses. Theoretically, we demonstrate that SIN leverages semantic knowledge and meta knowledge
to constrain the complexity of the hypotheses space and reduce the risk of overfitting of FSLL, bring-
ing better generalizability of our proposed model. Experimentally, extensive empirical evaluations
show that our model achieves state-of-the-art performance on FSLL, and ablation studies validate the
effectiveness of each module. The anonymous code and model are available here1.
2	Related Work
Streaming Label Learning. As new labels usually emerge from open and dynamic environments (Di-
etterich, 2017), streaming label learning is proposed to facilitate the learning system with the capability
of modeling new labels effectively (You et al., 2016; Wang et al., 2020b). Recent studies are presented
to explore and exploit the relationship between past labels and new labels, so that the historical data
can be further utilized. SLL (You et al., 2016) learns a mapping from past label vectors to new label
vectors and then assume the new classifier can inherit the mapping relationship as a regularization.
Constrained by the hypothesis, new classifiers can improve the performance. DSLL (Wang et al.,
2020b) is a DNN-based framework to learn deep relationships across labels with a label smoothing
technique. Moreover, DSLL has the ability to distill the feature-level knowledge from a past-label
classifier to a new-label classifier and outperforms other methods on modeling emerging new labels.
Nevertheless, current streaming label learning approaches assume new labels assigned with a large
number of examples, which could be easily violated in changing environments. Consequently, they
can not handle new labels only associated with a few examples, which will cause severe overfitting.
Few-shot Learning. Machine learning achieves impressive breakthroughs in data-intensive applica-
tions, but it often fails when the data set is small. Few-shot learning (Thrun., 1998; Li Fei-Fei et al.,
2006; Chen et al., 2019) is proposed to solve this problem. Current works could be categorized as
metric-based and gradient-based approaches. Metric-based methods (Vinyals et al., 2016; Snell et al.,
2017; Sung et al., 2018; Chen et al., 2020) train a model to embed examples into a metric space where
examples with the same label are gathered closely, and examples with different labels are spread far
away. For instance, Prototypical Network (Snell et al., 2017) can first train an average value of the
features belonging to the same label in the metric space as the prototype and then perform the nearest
1https://github.com/ICLR-FSLL/SIN
2
Under review as a conference paper at ICLR 2021
neighbor classification. AM3 (Xing et al., 2019) introduces semantic representation to adapt the pro-
totype and achieves state-of-the-art performance on few-shot learning. Gradient-based methods (Finn
et al., 2017; Gidaris & Komodakis, 2018; Antoniou et al., 2019) aim at training parameters of models
that can be well adapted to novel tasks with only a few optimization updates. Finn et al. (2017)
proposes a model-agnostic meta-learning (MAML) framework, and many follow-up works built on
top of MAML (Finn et al., 2018). Reptile (Nichol et al., 2018) simply replaces the second-order
gradient information with the first-order gradient computation of MAML. MAML++ (Antoniou et al.,
2019) further improve the generalized performance and stabilize the system. ATAML (Jiang et al.,
2018) is designed to encourage task-agnostic representation learning with attention mechanisms.
LEO (Rusu et al., 2019) applies pre-trained representations on a low-dimensional latent space instead
of the original high-dimensional parameter space, which achieves better classification performance.
However, existing few-shot learning methods mainly focus on single-label examples, which is
nontrivial to adapt to the multiple newly-arrived labels for solving FSLL problem. For metric-based
methods, a testing example can only be allocated to a single label through the maximum similarity
(the nearest neighbor), which limits to handle the incoming new labels. For gradient-based methods,
although they can be transformed to predict multiple new labels by converting 1-hot to n-hot output,
the challenges brought by the exponential-sized output space would severely restrict the performance
on FSLL. Moreover, the label correlation between past labels and new labels is ignored, while label
semantic correlation served as the meta-information can be crucial when the available data is scarce.
3	Methodology
In this section, after giving the mathematical definition of few-shot streaming label learning (FSLL),
we give technical details of the proposed Semantic Inference Network (SIN) to handle the newly-
emerged labels associated with only a few examples.
3.1	Problem Formulation
First, we formulate the problem of FSLL. Assume D={(x1,y1),...,(xn,yn)} is an initial training data
set, where xi∈X is a real vector representing an input feature (example) and yi∈Y0⊆{0,1}m×1 is the
corresponding output label vector, where m is the number of past labels, i∈{1,...,n}. Moreover, if the
j-th label is associated to the example xi, yij =1; otherwise, yij =0. By observing D, a proper learning
model can be derived for the m past labels. In a streaming fashion, new labels will continuously
emerge on a few examples. For simplicity, we denote ynew=[ym+1,ym+2,...,ym+k]T ∈Rk as the
new k-labels vector for an example x, where k≥1. A new label is associated with only Ns examples
(usually Ns<5). Few-shot streaming label learning aims to derive a learning model for the emerging
new labels using only a few labeled examples.
3.2	Semantic Inference Network
We propose Semantic Inference Network (SIN), designed for FSLL, to accommodate emerging
new labels using only a few examples. Figure 1 illustrates the overall learning framework of SIN.
Different from previous neural networks using learnable parameters as output layer weights, we
utilize fixed semantic embedding vectors of labels, which empowers the network to map features
into a semantic space. Specifically, Wpast and Wknew represent the matrices composed of past-label
semantic embeddings and new-label semantic embeddings (learned from unsupervised large text
corpora), respectively. In streaming label learning fashion (You et al., 2016; Wang et al., 2020b), as a
preliminary, a feature extractor F with output matrix Wpast is obtained by observing past-label data.
A feature z produced by F would be close to the corresponding labels in the semantic space. To model
the newly-arrived labels using only a few examples, we design a semantic inference mechanism to
recognize and distinguish the new labels by leveraging the label-wise meta-knowledge. The semantic
inference mechanism contains three different levels: 1) feature-level, utilizing proximity between
features and labels’ semantic representation; 2) label-level, exploiting semantic correlation between
new labels and past labels; 3) attention-level, transferring the weighted probabilistic prediction
from past labels to new labels. These three levels are convexly combined by learnable coefficients.
Moreover, a novel label decision module φ(∙) is proposed with a meta-threshold loss to find the
optimal thresholds value for each label by leveraging feature content.
3
Under review as a conference paper at ICLR 2021
F
Feature
Extractor
Semantic Representation of
Past Labels
dog
cat
car
∖
Preliminiary stage
Meta-learning stage
WPas
Meta-threshold
Module
Few-shot ExamplesX
Label Semantic
Representation
Iiction Logits
[OOP] W/
Semantic Representation
of New Labels
New lables
ʌ Semantic
/ 'Attention
Z
λ

Semantic
Inference
Mechanism
WjL
Figure 1: Semantic Inference Network (SIN) for Few-shot Streaming Label Learning. In the
preliminary stage, we learn a semantic-aware feature extractor to build a semantic embedding space.
In the meta-learning stage, we propose the semantic inference mechanism to exploit the correlation
between past labels and new labels. A meta-threshold module generates optimal threshold values for
each label. SIN is optimized by meta-learning to incorporate label-wise meta-knowledge.
3.2.1	Semantic Inference Mechanism
A key challenge of few-shot streaming label learning lies in the low-data problem causing overfitting
and generalization issues. We propose a semantic inference mechanism to explore the correlation of
features and labels in semantic space and acquire label-wise meta-knowledge, which could alleviate
the model’s overfitting and improve generalization. The semantic inference mechanism contains
three different levels: feature-level, label-level, and attention-level, described in detail as follows.
Assume Wpast=[w1past,...,wpmast]∈Rd×m is the matrix composed of m past-label semantic vectors,
and Wknew=[w1new,...,wknew]∈Rd×k is the matrix composed of k new-label semantic vectors, where
d is the dimension of semantic vectors. Note that the past label set Lpast and the new label set Lnew
are disjoint. As the preliminary, we have a past-label classifier which consists of a feature extractor
F and the output matrix Wpast trained on D. A feature z∈R1×d is produced by F, i.e., z=F(x).
Feature-level. Since we have obtained features embedded into the semantic space, the output
feature would naturally be closer to those with similar semantic meanings. In that case, we propose
to infer new labels’ prediction by feature-level proximity between z and new-label semantic vectors
Wknew . Moreover, to learn the label-specific knowledge, we build a multi-layer perceptron WZ to
obtain transformation from z to new labels. The feature-level semantic inference If takes the form:
If (Z)= WZ(Z) Wk
f ( ) kWz(z)k2 Wnew
(1)
where WZ(Z) denotes a nonlinear transformation for z, and ∣∣∙k2 denotes 12-norm which could
eliminate the influence of the absolute magnitudes of semantic features and improve the robustness.
Label-level. Intuitively, we find that utilizing the semantic correlation between new labels and past
labels as the meta-knowledge can help model to adapt new labels using only a few examples. For
instance, a feature is considered to be associated with “car” in the past label space, and if we know
the correlation between “car” and a new label “truck”, then this can help us to classify the feature
correctly in the new task. Generally, we use WpTast wjnew to represent the correlation of the j-th new
label in terms of m past labels. In order to learn the deep correlation between new labels and past
labels, we define an explicit label-level semantic inference mechanism Ic as the following form:
Ic(Z)= ZWpast∖ WiWnew ,	(2)
∣Z Wpast ∣2
where WI ∈Rm×d is a learnable matrix trained on different labels to learn the correlation between
new and past labels. Although WI is a linear operation, the l2-norm can offer a non-linear operation.
Since the number of past labels may be different between training and testing (see section 3.3),
l2-norm could also limit the adverse effects of absolute magnitudes fluctuations of ZWpast.
Attention-level. Moreover, we design an attention-level semantic inference to transfer the proba-
bilistic predictions on past labels to new labels. Assume that yj is the probabilistic prediction for
4
Under review as a conference paper at ICLR 2021
j-th label and yjWpast is the corresponding probabilistic weighted semantic vector. By defining
the convex combination of the past-label semantic embeddings: e(x)=Pj=ιyjWpast=Wpasty=
Wpastsigmoid(F(x)Wpast), we could treat e(x) as a region in the semantic space. Thus, the
closer new label embedding Wjnew near the region e(x), the higher probability of cos(e(x),Wjnew)
will be, where we use the cosine function to represent similarity. In that case, we can generate a
probabilistic prediction of x on the corresponding new labels through this simple inference, i.e.,
cos(e(x),Wknew)=cos(e(x), W1new,...,Wknew ). To further improve the model’s learning ability, we
design the attention-level semantic inference mechanism based on the above basic idea. We con-
sider an attentional head a(z,Wpast) with z to compute the query and past-label semantic vectors
Wpast used for keys and values. Next, we build a nonlinear network WA to learn and figure out
the relationship between the attention head a(z,Wpast) and new label semantic vectors Wnew. The
attention-level semantic inference Ia can be formed as:
Ia(Z)=WA ( UZ,Wpas?" )wMew .	⑶
ka(z,Wp
ast )k2 new
The implementation details of a(z,Wpast) can be found in Appendix A.2. Finally, we establish the
semantic inference mechanism I as a convex combination of the above three levels in (1)-(3):
I(z)=I(F(x))=γfIf(z)+γcIc(z)+γaIa(z),	(4)
where γf, γc, and γa are the learnable module coefficients. The ablation study in Section 5.3 verifies
the effectiveness of the semantic inference mechanism I and its three levels, respectively.
3.2.2	Meta-threshold
Different from traditional few-shot learning, which outputs only one label for an example, FSLL
requires recognize and predict multiple newly-emerged labels. Hence, the label decision part is to
determine which labels should appear in the prediction results from the list of predictive probability.
Most existing methods apply simple heuristics for label decision, e.g., a threshold of 0.5 is used to
all labels to generate the prediction decision. However, these methods ignore feature content when
making the decision. We propose a novel meta-threshold module φ(∙) to find the optimal threshold
values for each label by leveraging feature content. We define φ(∙) as a multi-layer perceptron which
outputs a vector of label confidence threshold λ∈Rk for making the decision: 1(Ij(z)>λj), where
1event denotes the indicator function for event, ∀j∈{1,...,k}. To learn λ for each label, we propose
a meta-threshold loss:
k
lthresh(ynew ,I(z),λ)=χiog(cosh(sigmoid(Ij (z)-λj )-yjnew)),	(5)
j=1
x -x
where Cosh(X) = e-+e-. To independently learn the meta-thresholds of each label, We fix parameters
of the semantic inference mechanism and optimize only for the parameters in φ(∙). Empirical studies
indicate that meta-threshold with the sequential training improves the classification accuracy of FSLL.
3.3 Training Procedure
In order to learn the semantic inference network for few-shot streaming label learning, we use a
training set D with m past labels as the sole input. First, we can obtain a past-label classifier (i.e., the
feature extractor F as well as the output matrix Wpast) by minimizing a cross-entropy loss. Second,
we employ a gradient-based meta-learning training (Finn et al., 2017; Antoniou et al., 2019) for the
learnable parameters θ of SIN model (feature extractor F is frozen). To learn the label-wise meta
knowledge, in each batch Ti , we randomly extract k simulative new labels out from m past labels,
and we treat them in the same way to simulate the newly-emerged labels in the testing procedure.
Specifically, we sample Ns associated training examples per simulative new label (typically Ns<5)
as the support set S . The parameters are adapted to label-specific parameters θ0 by applying a step
of gradient descent on S . Then, we sample other Nq associated examples per simulative new label
as the query set Q. The parameters θ are optimized by back-propagating in order to reduce errors
produced by θ0 on Q. We iteratively train the model on different batches of simulative new labels. The
meta objective is minimized to optimize the initial parameters θ, which contains the label-wise meta
knowledge. More implementation details of the training procedure are provided in Appendix A.3.
5
Under review as a conference paper at ICLR 2021
Figure 2: Features are projected to semantic space.
Figure 3: SIN solves the FSLL problem.
4 Generalization Analysis
In this section, we analyze the generalization of our learning model based on error decomposition in
machine learning (Bottou & Bousquet, 2008; Bottou et al., 2018). Our analysis demonstrates that
SIN leverages semantic knowledge and meta-knowledge to reduce the size of the hypotheses space
and provide a good initialization for modeling new labels, bringing better generalizability of our
proposed model (illustrated in Figure 2 and Figure 3).
For notational simplicity, let P(x,ynew) be the ground-truth joint probability distribution of example
X and label vector ynew, and h* be the optimal hypothesis from X to ynew. Assume that the number
N of the training examples for new labels is small. To discover h*, the FSLL model determines a
hypotheses space H containing a family of hypotheses h’s by fitting the dataset. The performance of
h is measured by a loss function '(h(x),ynew) defined over the prediction ynew=h(x) and the real
ynew. Our benchmark is the optimal hypothesis h* that minimizes the expected risk:
R
(h)=
'(h(x),ynew
)dP (x,y new )=E['(h(χ),y new)]],
(6)
that is, h*(x)=argminynewE['(ynew,ynew)|x]]. Since P(x,ynew) is unknown, We define the
empirical risk (by averaging the losses on the training set of N examples),
Rn (h) = -N X'(h(x)y new),	(7)
i=1
to approximate R(h). The learning model aims to find the hypothesis hN =argminh∈H RN (h) that
minimizes the empirical risk. As h* is unlike to exist in the space H, we define h*H=argminh∈HR(h)
to be the best approximation for h in H. For simplicity, we assume that h*, h*H, and hN are unique.
The excessive error can be decomposed as (Bottou & Bousquet, 2008; Wang et al., 2020a):
E[[R(hN)-R(h*)]]=E[[R(h*H)-R(h*)]]+E[[R(hN)-R(h*H)]]=Eapp+Eest,	(8)
where the expectation is regarding the random selection of the N training examples. As illustrated in
Figure 3, the approximation error Eapp measures how close the hypothesis in H can approximate the
optimal hypothesis h*, and the estimation error Eest measures the effect of minimizing the empirical
risk RN (h) instead of the expected risk R(h) within H. In FSLL, since the number of annotated
examples of new labels is very limited, the empirical risk RN (h) may be far away from being a good
approximation of the expected risk R(h), leading to the overfitting of empirical risk minimizer hN.
To solve the above issue in FSLL, SIN leverages semantic knowledge and meta-knowledge (as
described in Section 3), and we demonstrate that these can help reduce the estimation error Eest
to improve the model’s generalizability through the following two aspects. (1) SIN employs label
semantic embeddings as the output layer weights Wnew , which can project features into a structured
semantic space. As shown in Figure 2, a feature z0i is mapped into the semantic space based on w 1new
and wnew as Zi, and W is the basis of the orthocomplement space. The semantic space essentially
serves as the prior knowledge for the new labels. SIN uses the prior semantic knowledge to constrain
H to a smaller hypothesis space H via the prior semantic knowledge. As shown in Figure 3, the gray
area is excluded for optimization as it is considered to be unlikely to contain the best h*H according to
semantic knowledge. For smaller H, the empirical risk RN (h) can be more approximate the expected
risk R(h), and the risk of overfitting is reduced. (2) Since we employ a meta-learning paradigm to
learn from different labels, a good initialization with label-generic information is saved in the model
as meta-knowledge. SIN leverages the meta-knowledge to obtain a good initial hypothesis h as a
start to search the best h*H in H. As shown in Figure 3, the gray start generated by meta-knowledge
is closer to h*H . Hence, Eest could be reduced by taking fewer optimization iterations with a few
examples of new labels. Based on the above analysis, we give the following theorem.
6
Under review as a conference paper at ICLR 2021
Table 1: Few-shot streaming label classification accuracy (F1 score) on Delicious with 95% confidence intervals. k-way Ns-shot denotes k new labels with Ns tagged examples per label for training.				
Method	5-way 1-shot	5-way 5-shot	10-way 1-shot	10-way 5-shot
Streaming label learning baselines				
SLL (You et al., 2016)	17.59±1.22%	29.27±0.67%	7.84±1.59%	17.53±0.86%
DSLL (Wang et al., 2020b)	24.42±1.84%	37.63±0.96%	11.22±1.47%	25.68±0.82%
Few-shot learning baselines				
MAML (Finn et al., 2017)	30.78±1.82%	40.76±0.95%	15.77±0.52%	27.69±0.37%
ProtoNet (Snell et al., 2017)	36.09±0.78%	42.53±0.64%	25.42±0.71%	31.26±0.59%
Reptile (Nichol et al., 2018)	29.09±0.13%	40.14±0.16%	19.53±0.21%	28.15±0.35%
ATAML (Jiang et al., 2018)	35.53±0.82%	43.51±0.75%	20.82±0.67%	30.59±0.59%
MAML++ (Antoniou et al., 2019)	41.21±0.64%	44.51±0.55%	23.91±0.57%	29.33±0.47%
LEO (Rusu et al., 2019)	42.52±0.16%	45.07±0.11%	30.95±0.14%	33.01±0.08%
Semantic-augment baselines				
DSLL (+semantic embeddings)	29.15±0.85%	40.26±0.25%	15.63±0.74%	26.94 ±0.51%
MAML (+semantic embeddings)	37.94±1.27%	44.03±0.62%	23.51±0.91%	32.56±0.84%
AM3 (Xing et al., 2019)	44.67±0.61%	47.44±0.47%	32.09±0.53%	34.73±0.43%
SIN (ours)	46.17±0∙35%	49.19±0.27%	35.06±0.31%	38.35±0.18%
Theorem 1. Assume the loss function ` is bounded and C-Lipschitz. Let h be a hypothesis in terms
of k new labels using formulation hN =argminh∈H RN (h) over a set of N training examples. Then
with probability at least 1-δ, we have
R(hN)- inf R(h)≤CRN(H)+O
h∈H

where RN(H)=E[suph∈HNEiEih(Xi)]∣ is the Rademacher complexity of H, and Ei is a random
Rademacher variable: prob(i=-1)=prob(i=1)=1/2.
Refer to Appendix B for the proof. Theorem 1 relates the generalization error of SIN to the
Rademacher complexity of the hypotheses space H. The smaller the hypotheses space H, the more
generalizable the result is. Therefore, it turns out that SIN has the better generalizability by employing
semantic knowledge and meta-knowledge to reduce the size of the hypotheses space.
5 Experiments
In this section, we conduct extensive experiments to evaluate the proposed model, SIN, in dealing
with new labels under data scarcity. Experimental results show that SIN outperforms the state of the
art of three different types of baselines: streaming label learning methods, few-shot learning methods,
semantic-augment methods. Ablation studies verify the effectiveness of each module in SIN.
5.1	Experimental Setup
We use two widely used real-world multi-label datasets: Delicious (Tsoumakas et al., 2008) and
Mir-Flickr (Huiskes & Lew, 2008) from text and image domains, respectively. Delicious has 12,920
training examples and 3,185 testing examples with 983 labels. Mir-Flickr has 20,000 training image
features and 5,000 testing image features with 23 labels. We randomly choose 60% labels as past
labels, 20% labels as new labels and the rest of labels as validation. Models are trained on the training
set with past labels, and evaluated on the testing set with unseen new labels using only a few labeled
examples. We use GloVe (Pennington et al., 2014) to generate the word vectors for the category
labels as the semantic embeddings. The GloVe model is trained with large unsupervised text corpora.
More details about the experimental setup can be found in Appendix C.
Baselines. We compare SIN with three families of methods. The first is streaming label learning
methods: SLL (You et al., 2016) and DSLL (Wang et al., 2020b). The second fold is few-shot learning
methods such as: MAML (Finn et al., 2017), ProtoNet (Snell et al., 2017), Reptile (Nichol et al.,
2018), ATAML (Jiang et al., 2018), MAML++ (Antoniou et al., 2019), LEO (Rusu et al., 2019). Note
7
Under review as a conference paper at ICLR 2021
Figure 4: Performance comparison on Mir-Flickr
Student	5-way 1-shot	5-way 5-shot
SIN\l2	42.35±1.91%	44.47±1.26%
SIN∖φ	45.25±0.83%	48.47±0.35%
SIN\I	39.12±1.17%	45.93±0.62%
SIN\If	45.01±0.47%	47.93±0.34%
SIN\Ic	45.23±0.28%	48.51±0.19%
SIN\Ia	44.83±0.31%	47.64±0.21%
SIN	46.17±0∙35%	49.19±0.27%
Table 2: Ablation study of components
that we improve and enable few-shot learning methods to handle the scenario of multiple new labels.
To evaluate the effect of semantic embeddings more comprehensively and fairly, in the third fold
baselines, we extend DSLL and MAML with the semantic representation of labels and compare with
AM3 (Xing et al., 2019). AM3 also leverages semantic information of labels and achieves the the
current state of the art among few-shot learning methods. For more fairness, all methods use the same
feature extractor (same as SIN’s) as the backbone of models. Details of baselines in Appendix C.3.
5.2	Results
The few-shot streaming label classification performance for SIN and other baselines are shown in
Table 1 and Figure 4. We evaluate different numbers of new labels (-way) with different numbers of
examples (-shot) on Delicious and MIR-Flickr measured by Micro F1-score and AUC (Wu & Zhou,
2017). First, the results show that SIN significantly outperforms other methods and has state-of-the-
art performance on FSLL tasks. This indicates that the semantic knowledge and meta-knowledge
leveraged by SIN boost the few-shot streaming label learning very effectively. Second, traditional
streaming label learning and few-shot learning baselines (e.g., DSLL and MAML) get a significant
performance improvement when embedded semantic information, and AM3 also leverages the textual
features to improve classification accuracy. The results demonstrate the importance of semantic
representation for FSLL. However, these semantic-augment baselines ignore the correlation between
past labels and new labels. Third, it is worth noting that streaming label learning baselines perform
worse than current few-shot learning baselines when examples are scarce, but they could recover
quickly as examples increase. More detailed experimental results are given in Appendix D.
5.3	Ablation Study
To assess the effects of the proposed components in SIN, we perform the ablation study in Table 2.
We first evaluate the impact of l2-norm in the semantic inference. If SIN removes the l2 normalization
(denoted as SIN\l2), the performance of the model will be degraded. Since the l2 -norm could limit
the adverse effects of absolute value fluctuations in semantic space, then the semantic similarity can
be better represented. SIN∖φ denotes that the model uses a fixed global threshold for all labels, whose
result shows the positive impact of the proposed meta-threshold module. The semantic inference
mechanism is the key to our model. If we remove it from SIN (denoted as SIN\I), SIN would be re-
duced to a MAML algorithm with semantic embedding and produce a large performance degradation.
The results confirm that the semantic inference mechanism can help boost the performance of FSLL.
Specifically, we also separately assess the effects of the three different levels in the semantic inference
mechanism (SIN\If, SIN\Ic, and SIN\Ia denote SIN without feature-level, correlation-level, and
attention-level semantic inference, respectively). The results demonstrate effectiveness of each level.
6 Conclusion
This paper proposes a meta-learning framework, Semantic Inference Network (SIN), for the few-shot
streaming label learning, which can effectively model new labels with only a few labeled examples.
SIN exploits the semantic correlation between past labels and new labels and acquires label-wise meta-
knowledge. Moreover, SIN incorporating a label decision module can find the optimal confidence
threshold for each new label. Theoretical analysis proves that SIN can leverage semantic knowledge
and meta-knowledge to reduce the size of the hypotheses space, resulting in better generalizability.
Experiments show that SIN significantly outperforms state of the art on few-shot streaming label
classification, and ablation studies indicate the effectiveness of the proposed components in SIN.
8
Under review as a conference paper at ICLR 2021
References
Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. In International
Conference on Learning Representations (ICLR), 2019.
Leon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In Advances in Neural
Information Processing Systems (NeurIPS), pp.161-168. 2008.
Leon Bottou, Frank E. Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM Review, 60(2):223-311, 2018.
Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look
at few-shot classification. In International Conference on Learning Representations (ICLR), 2019.
Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell. A new meta-baseline for
few-shot learning. ArXiv, abs/2003.04390, 2020.
Thomas G. Dietterich. Steps toward robust artificial intelligence. AI Magazine, 38:3-24, 2017.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning (ICML), volume 70, pp.
1126-1135, 2017.
Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic model-agnostic meta-learning. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 9537-9548, 2018.
Sara Van De Geer. Applications of empirical process theory. Journal of the Royal Statistical Society.
Series D (The Statistician), 51:416-417, 01 2002.
Spyros Gidaris and Nikos Komodakis. Dynamic few-shot visual learning without forgetting. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.
Mark J. Huiskes and Michael S. Lew. The mir flickr retrieval evaluation. In ACM International
Conference on Multimedia Information Retrieval, 2008.
Ray Jackendoff. On beyond zebra: The relation of linguistic and visual information. Cognition, 26
(2):89 - 114, 1987.
Xiang Jiang, Mohammad Havaei, Gabriel Chartrand, Hassan Chouaib, Thomas Vincent, Andrew
Jesson, Nicolas Chapados, and Stan Matwin. On the importance of attention in meta-learning for
few-shot text classification. ArXiv, abs/1806.00852, 2018.
Georg KremPL Indre Zliobaite, Dariusz Brzezinski, Eyke Hullermeier, Mark Last, Vincent Lemaire,
Tino Noack, Ammar Shaker, Sonja Sievi, Myra Spiliopoulou, and Jerzy Stefanowski. Open
challenges for data stream mining research. SIGKDD Explor. Newsl., 16(1):1-10, 2014.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: Rademacher Averages, pp.
89-121. Springer Berlin Heidelberg, 1991.
Li Fei-Fei, R. Fergus, and P. Perona. One-shot learning of object categories. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 28(4):594-611, 2006.
Andreas Maurer. A vector-contraction inequality for rademacher complexities. In Algorithmic
Learning Theory, pp. 3-17, 2016.
Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. ArXiv,
abs/1803.02999, 2018.
Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word
representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language
Processing (EMNLP), pp. 1532-1543, 2014.
David Pollard. Empirical processes: Theory and applications. NSF-CBMS Regional Conference
Series in Probability and Statistics, 2:i-86, 1990.
9
Under review as a conference paper at ICLR 2021
Vijay Raghavan and Alaaeldin Hafez. Dynamic data mining. In Intelligent Problem Solving.
Methodologies andApproaches, pp. 220-229. Springer, 2000.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization. In International Conference
on Learning Representations (ICLR), 2019.
Claude Sammut and Geoffrey I. Webb (eds.). Encyclopedia of Machine Learning: McDiarmid’s
Inequality, pp. 651-652. 2010.
Linda Smith and Michael Gasser. The development of embodied cognition: Six lessons from babies.
Artificial Life, 11(1-2):13-30, 2005.
Linda B. Smith. Learning to recognize objects. Psychological Science, 14(3):244-250, 2003. PMID:
12741748.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems (NeurIPS), pp. 4077-4087. 2017.
Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, and Timothy M. Hospedales.
Learning to compare: Relation network for few-shot learning. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018.
S. Thrun. Lifelong learning algorithms. In Learning to Learn, pp. 181-209, 1998.
Grigorios Tsoumakas, Ioannis Katakis, and Ioannis P. Vlahavas. Effective and efficient multilabel
classification in domains with large number of labels. In ECML/PKDD 2008 Workshop on Mining
Multidimensional Data (MMD), 2008.
V. N. Vapnik. An overview of statistical learning theory. IEEE Transactions on Neural Networks, 10
(5):988-999, 1999.
Vladimir N. Vapnik. Statistical Learning Theory. Wiley-Interscience, 1998.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching
networks for one shot learning. In Advances in Neural Information Processing Systems (NeurIPS),
pp. 3630-3638. 2016.
Yaqing Wang, Quanming Yao, James T. Kwok, and Lionel M. Ni. Generalizing from a few examples:
A survey on few-shot learning. ACM Comput. Surv., 53(3), June 2020a.
Zhen Wang, Liu Liu, and Dacheng Tao. Deep streaming label learning. In International Conference
on Machine Learning (ICML), pp. 378-387. 2020b.
Xi-Zhu Wu and Zhi-Hua Zhou. A unified view of multi-label performance measures. In International
Conference on Machine Learning (ICML), volume 70, pp. 3780-3788, 2017.
Chen Xing, Negar Rostamzadeh, Boris Oreshkin, and Pedro O O. Pinheiro. Adaptive cross-modal few-
shot learning. In Advances in Neural Information Processing Systems (NeurIPS), pp. 4847-4857.
2019.
Shan You, Chang Xu, Yunhe Wang, Chao Xu, and Dacheng Tao. Streaming Label Learning for
Modeling Labels on the Fly. arXiv preprint arXiv:1604.05449, 2016.
M. Zhang and Z. Zhou. A review on multi-label learning algorithms. IEEE Transactions on
Knowledge and Data Engineering, 26(8):1819-1837, 2014.
Pengchuan Zhang, Qiang Liu, Dengyong Zhou, Tao Xu, and Xiaodong He. On the discrimination-
generalization tradeoff in GANs. In International Conference on Learning Representations (ICLR),
2018.
Zhi-Hua Zhou. Learnware: On the future of machine learning. Front. Comput. Sci., 10(4):589-590,
2016.
10
Under review as a conference paper at ICLR 2021
Appendixes
Appendix A supplements the content of motivation, attention-level semantic inference and training
procedure. Appendix B details the proof of Theorem 1. Appendix C presents the implementation
details and definition of evaluation metrics. Appendix D presents additional empirical evaluation for
few-shot streaming label learning and further analyses.
A Method
A.1 Motivation
We illustrate an instance of FSLL, as shown in Figure 5, when a photo is posted to Facebook or
Twitter, the photo may be continuously tagged with new labels by users who are browsing, and the
classification system needs to be updated accurately according to the new labels (Dietterich, 2017).
We hypothesize that in the context of few-shot streaming label learning, semantic representation
from the text can be a powerful source of information to distinguish new labels. As illustrated in
Figure 6, the semantic representation naturally brings correlation across labels, e.g., the label “dog”
is semantically close to the label “Labrador” in the word embedding space. The semantic correlation
across labels would help to structure and regularize the overwhelming output space of FSLL.
/ k
past labels：
'Alice','dog','cat'
emerging new labels:
'grass',
'woman,,
'Labrador',
'shirt,,
Figure 5: A photo is being tagged with new labels
P PaSt label
QneW label
OShirt
0 woman
「,Alice	0 grass
/,cat
D dog
Q Labrador
Figure 6: Labels in semantic space
A.2 Attention-level Semantic Inference
We design an attention-level semantic inference to transfer the probabilistic predictions on past labels
to new labels. A basic idea of this mechanism is using the output score of past labels to compute
a weighted combination of new labels embeddings in the semantic space. Given example x and
past-label classifier, We can get a label prediction vector y= [y1 ,...,ym], where yj is the probabilistic
prediction for the j-th label. Consequently, yj Wpast is the probabilistic weighted word vector for the
j-th past label. More formally, by defining the convex combination of the past-label word embeddings:
m
e(x) = Σ2 yj Wpast= Wpast y = Wpast Sigmoid(F (X)Wpast),
j=1
In that case, we can generate a probabilistic prediction of x on the corresponding new labels through
this simple inference, i.e., cos(e(x),Wknew)=cos(e(x), W1new,...,Wknew ). However, the simple
inference mentioned above has not taken the full utilization of label-specific knowledge. In order to
improve the learning ability of the model, we further design the attention-level semantic inference
mechanism based on the above basic idea. We consider an attentional head a(z,Wpast) with z to
compute the query and base-label vectors used for keys and values, which takes the form: *
sigmoid ( q⑶ W past)
a(z，Wpast) = η	/ (N)W~ʒɪWpast ,
11 Sigmoid (q(Z)Wp	J∣∣1
(9)
11
Under review as a conference paper at ICLR 2021
Algorithm 1 Semantic Inference Network: Meta-learning Training		
Require: Training set D Require: Learning rates α, β		
1	: Train feature extractor F on D	
2	: Initialize θ	. Initialize all parameters
3	: while not done do	
4	Sample batch of new labels tasks T 〜D	. Sample tasks for meta-training
5	:	Let (STi, QTi)=Ti	. Get support set and query set
6	: for all Ti do	
7	θi = θ-αVθ L(ST *)	. Compute temporary parameters
8	: end for	
9	Update θ-θ - βVθ PT L(QTi Hi)	. Update network parameters
10	: end while	
where q(z) = ZWq,Wq ∈Rd×d is a learnable matrix, k∙k 1 is lι-norm, and T is a temperature. Different
from the traditional attention mechanism, we use the sigmoid function to compute the probability,
rather than softmax function, causing that the sum of the probabilities is not 1. Hence, we perform
l1 -norm to regularize different new-label tasks.
A.3 Training Procedure
In order to learn the semantic inference network for few-shot streaming label learning, we use as the
sole input a training set D of m past labels.
First, we can obtain a past-label classifier (i.e., the feature extractor F as well as the output matrix
Wpast) by minimizing a cross-entropy loss of the following form:
m
ICE (y,y)=-X[yjiog(yj)+(i-yj)iog(i-yj)].
j=1
Second, we employ a gradient-based meta-learning training (Finn et al., 2017; Antoniou et al., 2019)
for the learnable parameters θ of the semantic inference mechanism and the meta-threshold model (the
feature extractor is frozen during this stage). The meta-learning training is described in Algorithm 1.
To learn the label-wise meta-knowledge, in each batch Ti , we randomly extract k simulative new
labels out from m past labels, and we treat them in the same way to simulate the unseen new labels in
the testing procedure. Specifically, we sample Ns associated training examples per simulative new
label (typically Ns<5) as the support set S. The parameters are adapted to label-specific parameters
θ0 by applying a step of gradient descent on S :
θi = θ-αVθ L(S*),	(10)
where α is a learning rate, L generates the loss. Then, we sample other Nq associated examples per
simulative new label as the query set Q. The parameters θ are optimized by back-propagating in
order to reduce errors on the query set Q:
θ一θ-βVθ X L(QWi),	(11)
Ti ~D
where β is a learning rate. We iteratively train the model on different batches of simulative new
labels. The meta objective (11) is minimized to optimize the initial parameters θ, which contains the
label-wise meta-knowledge.
B Proof of Theorem 1
In the section, we shall provide proof of Theorem 1. Our proof demonstrates that the smaller the
hypotheses space H, the more generalizable the result is. It turns out that the proposed SIN has
better generalizability by employing semantic knowledge and meta-knowledge to reduce the size
12
Under review as a conference paper at ICLR 2021
of the hypotheses space. In few-shot streaming label learning (FSLL), newly emerged labels are
associated with only a few examples. Given N examples with k new labels, SIN learns a hypothesis
h by minimizing the empirical risk,
1N
RN (h) = NN E'(h(χi),ynew)=EN['(h(χ),ynew)]],	(12)
N i=1
where '(∙) denotes the loss function; ynew =h(x) is the prediction for y. The learning model aims to
find the hypothesis
hN=argminRN(h)
h∈H
that minimizes the empirical risk (12). The goal of the proof is to show that the excess risk R(hN) -
RN(hN) is related to the size of the hypotheses space H, where R(h) is the expected risk, defined as:
R(h) = /'(h(x),ynew)dP(x,ynew)=E['(h(x),ynewH = x 晟。®)]"X'(h(X)Er)[.
i=1	(13)
This can be achieved by the uniform concentration bounds developed in empirical process (Pollard,
1990; Geer, 2002) and statistical learning theory (Vapnik, 1998; 1999; Zhang et al., 2018). In
particular, the size of hypotheses space H can be characterized by the Rademacher complexity of H,
defined as
R(H)= E
(xi,i)
卜 H 得 XX ei h(XiJ
(14)
where i is a random Rademacher variable: prob(i=-1)=prob(i=1)=1/2.
By using McDiarmid’s inequality (Sammut & Webb, 2010), the excess risk R(hN)-RN(hN) can
be bounded by the expected supremum deviation of empirical risks,
R(hN)-RN(hN)≤sup{R(h)-RN(h)}
h∈H
= SuP {E['(h(x), ynew N-EN['(h(x),ynew )J}
h∈H
=huH((Xi小)"NX队h"w] -&X仲(Xi),ynew))
,g((x1,y1new),...,(xN,ynNew)).
Since the decomposable loss function '(h(xi),ynew)=Pjm+k+ι'(hj(x),yj) are bounded, changing
any (xi,ynew) would induce a perturbation of g((xι,yψew),..., (xn,yNew)) at most O(N). Then
2k2
by applying McDiarmid S inequality, the sum of squared perturbations is bounded by 2N, and thus
the excess risk is bounded by a term related to the expectation of g((x1,y1new), ..., (xN,ynNew)), i.e.,
the expected supremum deviation. Therefore, we have established that with probability at least - δ,
R(hN)-RN(hN)≤(xi,yEinew)[[g((x1,y1new),...,(xN,ynNew))]]+O

13
Under review as a conference paper at ICLR 2021
Next, we bound the expected supremum deviation by Rademacher complexity (Ledoux & Talagrand,
1991; Maurer, 2016). We have
E	[[g((x1,y1new),..., (xN,ynNew))]]
(xi,yinew)
=E JsUp {E['(h(x),ynew )J-EN['(h(x), ynew )J}1
(xi,yinew) h∈H
=(χi,Enew) "hUH((Xi,Enew) "NX,j -事X))〔
≤ (Xm )"hUH 代 X '(h(e),enew)- N X Sl))[
(xei,yeinew)	i=1	i=1
",EnewJhUH (⅛ X q ('(h(e),enew)-'(Synew)))]
(xei,yeinew ),i	=
≤…晨,k」hUH代X))〔
十(XiL 晨,enew)JhUH ( N X Mh(Xjynew))[
≤2∕wJuH (鼻 X q'(Synew))[
≤C E	sup⅛Xeih(xi)i ：=CR(H),
(Xi,i) h∈HN i
which establishes Theorem 1. Note that the first inequality employs Jensen inequality. The second
inequality is based on the convexity of the supremum function. The third inequality is based on (13).
The last inequality under the assumption that the loss ` is bounded and C-Lipschitz.
C Experimental Settings
C.1 Implementation Details
Pytorch2 is used to implement the proposed algorithm and conduct all the experiments. All the
computations are performed on a 64-Bit Linux workstation with 10-core Intel Core CPU i7-6850K
3.60GHz processor, 256 GB memory, and 4 Nvidia GTX 1080 Ti GPUs. For the training stage, we
use Adam optimizer with a fixed learning rate of 0.001, weight decay 10-6. We train models 100
epochs, where each epoch contains 200 tasks randomly sampled from training set Dtrain. For the
testing stage, we test models on 200 novel tasks randomly sampled from Dtest to get average results.
The semantic embedding model, GloVe (Pennington et al., 2014), generates 300-dimension word
vectors for the category labels3. For the architecture of SIN, the feature extractor F has 3 layers of
fully connected layers, and the meta-learner I consists of different networks with several connected
layers. The temperature hyperparameter in Equation (9) is set in the range [0.1, 10], and dropout rate
in the networks is set in the range [0.1, 0.5]. We also provide the source code for reference4.
2https://pytorch.org/
3https://nlp.stanford.edu/projects/glove/
4https://github.com/ICLR-FSLL/SIN
14
Under review as a conference paper at ICLR 2021
C.2 Evaluation Metrics and Settings
Given a query set QT sampled from test dataset Dtest denoted by QT ={(x1,y1),..., (xq,yq)},
where xi∈X ⊆Rds ×1 is a real vector representing an input feature (example) and yi ∈Y ⊆{0, 1}k×1
is the corresponding output new label vector (i∈{1,...,N}). Moreover, yij=1 if the j-th label is
assigned to the instance Xi and yj =0 otherwise. For notational simplicity, We use 匕+ (匕-)to
denote the index set of associated (non-associated) labels of yk Formally, Y++ = {j∣yj = 1} and
Yi- ={j∣yj =0}. With respect to j-th column of label matrix, Yj = {i∣yj = 1} denotes the index
set of associated examples of the j-th label and Yj" = {i∣yj =0} denotes the set of non-associated
examples similarly. We use ∣∙∣ to represent the cardinality of a set. For an k-way Ns-shot setting, each
task is sampled with k labels, and each label includes Ns support examples and 13 query examples.
Note that due to the label distribution of multi-label data (Zhang & Zhou, 2014), several labels may
correspond to more examples than other labels, however, the comparison on different methods are
fair because of the same setting.
Table 3: Definitions of streaming label learning performance measures.
micro-F1	2	2pm=i Pn=1 yj hi micro-F1 (H)=-	-	-	-	 mn	mn j=1 i=1 yij + j=1 i=1 hij	F-measure averaging on the prediction matrix.
AUC	micro-AUC (F) =	ISmlCro I	 (Pn=ι ∣Yi+ l)∙(P"Y∏) Smicro = {(a,b,i,j)l(a,b)∈γ∙+ ×γ∙- , fi(Xa)≥fj (Xb))	AUC averaging on prediction matrix. Smicro is the set of cor- rect quadruples.
Table 3 summarizes two popular multi-label evaluation metrics used in this paper, which can be
divided into a bipartition-based metric, i.e., micro-F1, and a ranking-based metric, i.e., AUC (Wu &
Zhou, 2017). We assume that H:Rd→{0,1}m is the FSLL classifier and predicts which labels an
example is associated. H can be decomposed as {h1,...,hm} and hj(xi) represents the prediction
of yj. The results of H can be evaluated by bipartition-based metrics. F:Rd→Rm is the FSLL
predictor whose predicted value could be regarded as the confidence of association. F={f1,...,fm}
and fj (xi) denotes the predicted value of yij, which can be evaluated by ranking-based metrics. H
can be induced from F by thresholding techniques t(∙). For example, hj(Xi) = 1{fj(xi) >t(x%)},
where we use 1{event} to denote the indicator function for event. In the experiment, we simply use
0.5 as the threshold for the output of all models. The evaluation metrics implementation is based on
scikit-learn tools5.
C.3 Baselines
SLL (You et al., 2016) is a streaming label learning method that learns a mapping from past label
vectors to new label vectors and assumes the new classifier can inherit the mapping relationship as a
regularization. Constrained by the hypothesis, new classifiers can improve performance. DSLL (Wang
et al., 2020b) is a DNN-based framework to learn deep relationships across labels with label smoothing
techniques. Moreover, DSLL has the ability to distill the feature-level knowledge from a past-label
classifier to a new-label classifier and outperforms other methods on streaming label learning.
MAML (Finn et al., 2017) is a groundbreaking model-agnostic meta-learning framework for few-shot
learning, which learns a good initialization for new tasks. Reptile (Nichol et al., 2018) proposes
a shortest descent method to further improve efficiency and performance. ATAML (Jiang et al.,
2018) introduces attention mechanism into meta-learning to learn task-agnostic representation.
MAML++ (Antoniou et al., 2019) is a state-of-the-art meta-learning framework for the few-shot
learning problem, which employs multi-step loss optimization to improve generalization performance.
LEO (Rusu et al., 2019) applies pre-trained representations on a low-dimensional latent space instead
of the original high-dimensional parameter space, which achieves better classification performance.
Prototypical Network (Snell et al., 2017) can first train an average value of the features belonging to
the same label in the metric space as the prototype and then perform the nearest neighbor classification.
5 https://scikit- learn.org/stable/
15
Under review as a conference paper at ICLR 2021
Table 4: Few-shot streaming label classification accuracy (AUC) on Delicious with 95% confidence intervals. k-way Ns-shot denotes k new labels with Ns tagged examples per label for training.				
Method	5-way 1-shot	5-way 5-shot	10-way 1-shot	10-way 5-shot
Streaming label learning baselines				
SLL (You et al., 2016)	60.11±1.81%	67.83±1.37%	61.10±1.53%	70.59±1.16%
DSLL (Wang et al., 2020b)	63.59±2.36%	70.91±1.31%	62.70±2.07%	72.87±1.38%
Few-shot learning baselines				
MAML (Finn et al., 2017)	68.24±2.02%	73.82±1.35%	68.17±1.01%	74.42±0.74%
ProtoNet (Snell et al., 2017)	65.96±1.53%	71.87±1.23%	69.79±1.45%	73.71±1.21%
Reptile (Nichol et al., 2018)	66.65±0.53%	70.39±0.61%	68.54±0.49%	73.79±0.55%
ATAML (Jiang et al., 2018)	70.14±1.54%	72.50±1.42%	68.68±1.20%	74.60±1.04%
MAML++ (Antoniou et al., 2019)	72.50±1.31%	74.09±1.13%	71.38±1.26%	73.85±0.92%
LEO (Rusu et al., 2019)	72.39±0.34%	74.87±0.26%	69.43±0.27%	74.10±0.18%
Semantic-augment baselines				
DSLL (+semantic embeddings)	67.35±1.75%	73.13±0.63%	68.38±1.39%	72.99 ±1.12%
MAML (+semantic embeddings)	71.43±1.91%	74.30±1.28%	70.03±1.54%	74.59±1.32%
AM3 (Xing et al., 2019)	72.58±1.31%	74.95±0.68%	72.25±1.13%	74.43±0.74%
SIN (ours)	75∙38±0.61%	76.41±0.51%	73.90±0.71%	76.13±0.47%
Figure 7: Performance comparison on Mir-Flickr. k-way Ns-shot denotes k new labels with Ns
tagged examples per label for training.
AM3 (Xing et al., 2019) introduces semantic representation to adapt the prototype and achieves
state-of-the-art performance on few-shot learning.
D	Additional Results and Analyses
D. 1 Additional Results
Table 4 and Figure 7 show the average performance of the few-shot streaming label classification
for SIN and baselines on Delicious and Mir-Flickr, respectively. The results consistently show
that our model outperforms the baselines on AUC and F1-score metrics for both 1-shot and 5-shot,
5-way and 10-way. Please note that Mir-Flickr only has 5 new labels in the testing dataset. SIN can
extract semantic features and exploit the correlation between novel labels and base labels as prior
16
Under review as a conference paper at ICLR 2021
knowledge; therefore, it can achieve better results in dealing with the problem of few-shot streaming
label learning.
D.2 Additional Analyses
Influence of semantic correlation. As shown in Section 5 and Appendix D.1, it can be confirmed
that label semantic correlation is a key for few-shot streaming label learning. We leverage label
semantic correlation in both of semantic-aware feature extractor F and semantic inference mecha-
nism I. Exploiting label correlation can facilitate FSLL process to cope with the challenge of the
overwhelming size of output space. For instance, if an image has been annotated with label whale,
the probability of the image being associated with labels ocean and seaweed would be high, and the
image is unlikely to be newly labeled as grassland and lion. Moreover, semantic embedding (learned
from large unsupervised text corpora) can serve as prior knowledge and context to supplement the
label correlation. The proposed SIN incorporates the label correlation not only from the learning
process but also from label semantic embedding, which has achieved great success in few-shot
streaming label learning. Table 2 shows that if we remove the semantic inference I (denoted as
SIN\I), SIN will produce a significant performance degradation.
Influence of l2-norm. Table 2 demonstrates that if remove the l2 normalization (denoted as SIN\l2-
norm), the performance of the model will be degraded. l2-norm is a technique that is often used to
provide regularities for deep neural networks. However, in our meta-leaner, l2-norm plays a more
critical role. We employ l2-norm in three different levels of semantic inference. In feature-level
inference (Equation (1)), l2 -norm is used to eliminate the influence of the absolute magnitudes
of semantic features and improve the robustness. In label-level inference (Equation (2)), l2-norm
not only offers nonlinear operation for feature transformation but also limits the adverse effects of
absolute magnitudes fluctuations of zWbase . Since the number of past labels is different between
training and testing (described in Section 3.3), by using l2-norm, the absolute value of the feature
transformation in training and testing can be kept consistent, which is important for the convergence
of the model. In attention-level inference (Equation (3)), l2-norm provides the regularity of the
attention value.
17