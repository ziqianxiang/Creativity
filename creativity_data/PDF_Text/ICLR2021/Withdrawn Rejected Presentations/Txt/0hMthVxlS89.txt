Under review as a conference paper at ICLR 2021
Convergence Proof for Actor-Critic Methods
Applied to PPO and RUDDER
Anonymous authors
Paper under double-blind review
Ab stract
We prove under commonly used assumptions the convergence of actor-critic re-
inforcement learning algorithms, which simultaneously learn a policy function,
the actor, and a value function, the critic. Both functions can be deep neural net-
works of arbitrary complexity. Our framework allows showing convergence of the
well known Proximal Policy Optimization (PPO) and of the recently introduced
RUDDER. For the convergence proof we employ recently introduced techniques
from the two time-scale stochastic approximation theory. Our results are valid
for actor-critic methods that use episodic samples and that have a policy that be-
comes more greedy during learning. Previous convergence proofs assume linear
function approximation, cannot treat episodic examples, or do not consider that
policies become greedy. The latter is relevant since optimal policies are typically
deterministic.
1	Introduction
In reinforcement learning, popular methods like Proximal Policy Optimization (PPO) (Schulman
et al., 2018) lack convergence proofs. Convergence proofs for these methods are challenging, since
they use deep neural networks, episodes as samples, policies that become greedy, and previous
policies for trust region methods. For Q-learning, convergence to an optimal policy has been proven
in Watkins & Dayan (1992); Bertsekas & Tsitsiklis (1996) as well as for TD(λ) in Dayan (1992).
Convergence of SARSA to an optimal policy has been established for policies that become greedy,
like “greedy in the limit with infinite exploration” (GLIE) or “restricted rank-based randomized”
(RRR) (Singh et al., 2000). Policy gradient methods converge to a local optimum, since the “policy
gradient theorem” (Sutton & Barto, 2018, Chapter 13.2) shows that they form a stochastic gradient of
the objective. Stochastic gradients converge according to the stochastic approximation theory to an
optimum (Robbins & Monro, 1951; Kushner & Clark, 1978; Tsitsiklis, 1994; Borkar & Meyn, 2000;
Kushner & Yin, 2003; Borkar, 2008; Bhatnagar et al., 2013). Temporal difference (TD) convergences
to a local optimum with smooth function approximation like by neural networks (Maei et al., 2009).
Also Deep Q-Networks (DQNs) (Mnih et al., 2013; 2015) use a single neural network, therefore can
be shown to converge, as done in Fan et al. (2020). However it is assumed that every training set of
reward-state transitions is drawn iid and that a global minimum of the Q-function on the training set
is provided.
We prove the convergence of general actor-critic reinforcement learning algorithms (Sutton & Barto,
2018, Chapter 13.5). Recently, actor-critic methods have had a considerable success, e.g. at defeating
humans in the game Dota 2 (OpenAI et al., 2019) and in mastering the game of Starcraft II (Vinyals
et al., 2019). Actor-critic algorithms simultaneously learn a policy function, the actor, and a critic
function that estimates values, action-values, advantages, or redistributed rewards. The critic is
responsible for credit assignment, that is, which action or state-action pair was responsible for
receiving a reward. Using this credit assignment, a policy function is updated to increase the return.
Actor-critic algorithms are typically policy gradient methods, but can also be reward redistribution
methods like RUDDER (Arjona-Medina et al., 2019) or “backpropagation through a model” (Munro,
1987; Robinson, 1989; Robinson & Fallside, 1989; Bakker, 2007). Actor-critic algorithms have been
only proven to converge for simple settings like for the neural networks that are linear (Konda &
Tsitsiklis, 2000; Konda & Borkar, 1999; Xu et al., 2019; Yang et al., 2019; Liu et al., 2019). In
contrast to these convergence proofs, in our setting both functions can be deep neural networks of
arbitrary complexity, though they should not share weights.
1
Under review as a conference paper at ICLR 2021
The main contribution of this paper is to provide a convergence proof for general actor-critic reinforce-
ment learning algorithms. We apply this convergence proof to two concrete actor-critic methods. First,
we establish convergence of a practical variant of Proximal Policy Optimization (PPO) (Schulman
et al., 2018). PPO is an actor-critic on-policy gradient method with trust region penalties to ensure a
small policy gap (Schulman et al., 2015). Secondly, we prove convergence of the recently introduced
RUDDER (Arjona-Medina et al., 2019). RUDDER targets the problem of sparse and delayed rewards
by reward redistribution which directly and efficiently assigns reward to relevant state-action pairs.
Thus, RUDDER dramatically speeds up learning for sparse and delayed rewards. In RUDDER, the
critic is the reward redistributing network, which is typically an LSTM.
The main proof techniques are recent developments from the two time-scale stochastic approximation
theory (Borkar, 2008). The recent addition to the theory is the introduction of controlled Markov
processes (Karmakar & Bhatnagar, 2017), which can treat policies that become more greedy and trust
region methods that use previous policies. The two time-scale stochastic approximation framework
has been applied previously to show convergence of actor-critic algorithms (Konda & Tsitsiklis, 2000;
Konda & Borkar, 1999) and, more recently, of Linear Quadratic Regulator (LQR) problems (Xu
et al., 2019) and off-policy TD learning (Yang et al., 2019). However, only tabular cases or linear
function approximations have been considered. In a recent work, convergence was shown for variants
of PPO and Trust Region Policy Optimization (TRPO) equipped with neural networks (Liu et al.,
2019). However, again the neural networks were only linear, the policy was energy-based, and the
Kullback-Leibler term of the trust-region method was modified.
We aim at generalizing these proofs to learning settings which use deep neural networks, use episodes,
use policies that become greedy, and use trust region methods. Therefore, the idea of stationary
distributions on state-action pairs does not apply (Konda & Tsitsiklis, 2000; Konda & Borkar, 1999)
and we have to enrich the framework by a controlled Markov process which describes how the policy
becomes more greedy and how to use previous policies. While we are developing a framework to
ensure convergence, it does not imply convergence to an optimal policy. Such proofs are in general
difficult for methods that use deep neural networks, since locally stable attractors may not correspond
to optimal policies (Mazumdar et al., 2019; Jin et al., 2019; Lin et al., 2019). However, convergence
to a locally optimal policy can be proven for linear approximation to Q-values (Sutton et al., 2000;
Konda & Tsitsiklis, 2003). Our main contributions to the convergence proof are, that we:
•	use a Markov control in the two time-scale stochastic approximation framework,
•	use episodes as samples instead of transitions,
•	allow policies to become greedy,
•	allow objectives that use previous policies (trust region methods).
In the next section, the main theorem is provided, which shows local convergence of actor-critic
methods. Next, we formulate the results for PPO and RUDDER as corollaries. The third section
gives a roadmap for the corresponding proofs, thereby introducing the precise framework and the
results from stochastic approximation theory (Borkar, 2008; Karmakar & Bhatnagar, 2017). Finally,
we discuss the technical assumptions and details for the proofs.
2	The Main Results
2.1	Abstract setting and Main Theorem
Preliminaries We consider a finite MDP defined by the 4-tuple P = (S, A, R, p) (we assume a
discount factor γ = 1) where the state space S and the action space A are sets of finite states s and
actions a and R the set of rewards r which are bounded. Let us denote by |A| and |S| the corresponding
cardinalities and KR > 0 an upper bound on the absolute values of the rewards. For a given time step
t, the random variables for state, action, and reward are St, At and Rt+1 = R(St, At), respectively.
Furthermore, P has transition-reward distributions p(St+1 = s0, Rt+1 = r | St = s, At = a).
By π we denote an associated Markov-policy. The (undiscounted) return of a sequence of length
T at time t is Gt = PkT=-0t Rt+k+1. As usual, the action-value function for a given policy π is
qπ(s,a) = En [Gt | St = s,At = a]. The goal is to find the optimal policy π* = argmax∏ En [Go].
We assume that the states s are time-aware (time t can be extracted from each state) in order to
guarantee stationary optimal policies.
2
Under review as a conference paper at ICLR 2021
The abstract actor-critic setting is assumed to have two loss functions: Lh for the policy and Lg for
the critic. Additionally we have the following building blocks:
•	We consider two classes of parameters, denoted by ω ∈ Rm and θ ∈ Rk. Moreover, z denotes
an additional controlled Markov process with values in a compact metric space that may allow e.g.
to force the policy to get more greedy and for treating trust region methods which rely on previous
policies (it may be used for other purposes as well, e.g. Markovian sampling). z will be defined in
a similar abstract way as done in Karmakar & Bhatnagar (2017) to make the setting as general as
possible. We defer the technical details to Section 3.1.
•	The first loss Lh(θ, ω, z) is minimized with respect to θ in order to find an optimal policy.
This is achieved by updating a sufficiently smooth policy π(θ, z), that can be controlled by z.
We will discuss in Section 3.2, how π can be constructed in specific situations. Next we con-
sider two optional possibilities, how Lh (θ, ω, z) may be defined: it may equal the expectation
(i) ET〜∏(θ,z) [φ(τ, θ, ω, z)] or (ii) ET〜∏ [φ(π(.; θ, z), τ, θ, ω, z)] where the expectations are taken
over whole episodes τ = (s0, a0, . . . , sT, aT) (sequences) that are generated via (i) π(θ, z) or (ii) a
behavioral policy ∏, respectively. It will be clear from the context, which of these two possibilities
we are using. The function φ can be interpreted as a per sample loss for a sufficiently smooth neural
network, that tries to find the optimal policy, evaluated only on a single trajectory τ . The detailed
smoothness assumptions on Lh that need to be imposed are discussed in Section 3.2. The gradient of
Lh(θ, ω, z) will be denoted by h(θ, ω, z).
•	The second loss is given by Lg(θ, ω, z) = ET〜∏(θ,z) [Φ(g(τ; ω, z), τ, θ, ω, z)] and is minimized
with respect to ω in order to find an optimal critic function g(τ; ω, z). The functions g and Φ should
again be sufficiently smooth, such that Lg(θ, ω, z) satisfies (L1)-(L3) from Section 3.2. Φ can be
seen as the per sample loss for the critic g. The gradient of Lg will be denoted by f.
•	Since the expectations cannot be computed analytically, we do not have the exact gradients
h(θ, ω, z) and f(θ, ω, z). Therefore, the expectations are approximated by sampling sequences τ and
computing the average gradient on the sampled sequences. In our case, the stochastic approximations
h and f of the gradients h and f respectively, are created by randomly inserting only one sample
trajectory τ, i.e. we are dealing with online stochastic gradients. A formal description of the sampling
process can be found in Section 3.2. Our losses are then minimized using online stochastic gradient
descent (SGD) with learning rates a(n) and b(n), where the integer n ≥ 0 denotes the timestep of
our iteration.
For a given n, let us now state the discussed building blocks in a more compact and formal way:
Lh(θn, ωn, Zn) = ET〜π(θn,Zn) [φ(τ, θn, ωn, zn)] ,	(1)
h(θn, ωn, zn) =
ET ~∏(θn,Zn) [Vθn log ∏(θn, Zn) φ(τ, θn, 3", Zn) + V©. φ(τ, θn 3n N,)],
where the first possibility for the policy loss Lh and its gradient h is listed. h is computed by the
Policy Gradient Theorem, which can be found e.g. in Sutton & Barto (2018, Chapter 13.2). Next we
discuss the expressions for the second possibility for Lh (when sampling via a behavioral policy ∏):
Lh(θn, 3n, Zn) = ET 〜∏ [φ(π(.; θn, Zn) ,τ, θn, 3n, zn)] ,	(2)
h(θn,3n, Zn) = ET〜∏ [Vθnφ(π(.; θn, zn),τ, θn, 3n, zn)] ∙
(3)
The expressions for our second loss Lg and its gradient f are as follows:
Lg (θn, 3n, zn) = ET 〜n(θn,z.) [φ(g(τ; 3n, Zn) ,τ, θn, 3n, zn )] ,	(4)
f (θn, 3n, zn) = ET ~π(θn∕n) [V ω. φ(g(τ; 3n, Zn) ,τ, θn, 3n, zn)],
and finally, the iterative algorithm that optimizes the losses by online SGD, is given by:


θn+1 = θn - a(n) h(θn, 3n, zn) , 3n+1 = 3n - b(n) f(θn, 3n, zn) .
(5)
Main Theorem. Our main result will guarantee local convergence for (5). To this end we fix a
starting point (θ0, 30) and determine an associated neighborhood V0 × U0 which can be constructed
by the loss assumptions (L1)-(L3) given in Section 3.2. The iterates (5) will always stay in V0 X U
3
Under review as a conference paper at ICLR 2021
by these assumptions. Furthermore, let us denote the loss functions that result after considering the
“limit” of the control sequence zn → z by Lh(θ, ω) (and similarly Lg(θ, ω)). Again we refer to
Section 3.1 for a precise account on this informal description. Moreover, we denote a local minimum
of Lg(θ, ∙) by λ(θ), whereas θ*(θ0, ω0)) should indicate a local minimum of Lh(∙, λ(∙)) in V0 X U0.
Also here we refer to Section 3.2 for a precise discussion. We can now state our main theorem:
Theorem 1. Fix a starting point (θ0, ω0). Determine the associated neighborhood V0 × U0 as
in Section 3.2. Assume learning rates like (A4) for the time-scales a(n) and b(n) mentioned in
Section 3.1. Also take the loss assumptions in Section 3.2 for granted.
Then Eq. (5) converges to a local minimum (θ*(θ°, ω°), λ(θ*(θ°, ω°))) ofthe associated losses (i)
Eq. (1) or (ii) Eq. (2) andEq. (3): (θn, ωn) → (θ*(θ°, ω°),λ(θ*(θo, ω°))) a.s. as n → ∞.
2.2	Convergence Proof for PPO.
The main theorem is applied to prove convergence of PPO. Our PPO variant uses deep neural
networks, softmax outputs for the policy network, regularization, trust region, or exploration terms.
Regularization can be entropy, weight decay, or a trust region penalty like the Kullback-Leibler
divergence. All functions are assumed to be sufficiently smooth, i.e. at least three times continuously
differentiable and bounded wrt. the parameters. The losses should satisfy (L1)-(L3) from Section 3.2.
The PPO alorithm aims at minimizing the following losses:
Lh(θn, ωn, Zn) = ET〜π(θn,Zn) [― G0 + (z2)n P(T, θn, Zn)] ,
(6)
LgTD(θn,ωn,Zn) = ET〜∏(θn,Zn)
T
XWD(t))2
t=0
(7)
LMC(θn, ωn, Zn ) =ET〜π(θn,Zn)
2 X(Gt — qπ (St, at ωn)^
(8)
1
2
θn+1 = θn - a(n) h(θn , ωn , Zn ) ,
7 / ∖ P / Λ	∖
ωn+1 = ωn - b(n) f(θn, ωn, Zn) .
(9)
Let us now briefly describe the terms in Eq. (6)-(9):
•	qπ(st, at； ω) is a function that approximates the Q-value qπ(st, at).
•	δτD(t) = R(st, at) + qπ(st+1,αt+1; ωn-ι) - qπ(st, at; ωnj) is the temporal difference error.
•	The exact gradients h(θn, ωn, Zn) (from the Policy Gradient Theorem assuming causality and
subtracting a baseline (Sutton & Barto, 2018, Chapter 13.2)), fTD(θn, ωn, Zn) and fMC(θn, ωn, Zn)
of the respective losses can be found in Sections A.1 and A.3 in the appendix.
•	Zn = ((z1)n, (z2)n, (z1)n-1, (z2)n-1, θn-1, ωn-1) denotes an additional controlled Markov
process with values in compact sets. The controlled Markov process is essential to define the trust
region term of PPO which uses previous values of θ and z1. Here, z1 ∈ [1, β] increases from 1 to
β > 1 and z2 ∈ [0, (z2)0] decreases from (z2)0 > 1 to 0. z1 controls the amount of greediness and
z2 the regularization. Details can be found in Section 3.2.
•	π(θn, Zn) is a softmax policy that depends on (z1)n to make it more greedy. We will introduce it
precisely in Section 3.2, especially Eq. (15) there. ∏ is learned using q and updated in every time-step.
•	P(τ, θn , Zn ) includes the trust region term of PPO and may also include regulariza-
tion terms like weight decay or entropy regularization. For example, P(τ, θn, Zn) =
KLe(π(θn-ι, (zι)n-ι), ∏(θn, (zι)n)), Where KLe(p, q) = KL(p, q) withPi = (Pi + €)/(1 + ke).
The next corollary states that the above described PPO algorithms (TD and MC versions) converge.
Corollary 1 (Convergence PPO). Fix a starting point (θ0, ω0). Determine the associated neighbor-
hood V0 × U0 as in Section 3.2. Assume learning rates like (A4) for the time-scales a(n) and b(n)
mentioned in Section 3.1. Also take the loss assumptions in Section 3.2 for granted.
Using the same notation as in Theorem 1, the PPO algorithm Eq. (9) converges to a local min-
imum (θ*(θo, ω0), λ(θ*(θ°, ω°))) ofthe associated losses Eq. (6) and either Eq. (7) or Eq. (8):
(θn, ωn) → (θ*(θo, ωo), λ(θ*(θo, ωo))) a.s. as n → ∞.
Proof. We apply Theorem 1 since all its assumption are fulfilled.
4
□
Under review as a conference paper at ICLR 2021
2.3	Convergence Proof for RUDDER.
The main theorem is applied to prove convergence of RUDDER, which excels for tasks with sparse
and delayed rewards (Arjona-Medina et al., 2018). Again, we assume enough smoothness for all
functions, i.e. they are at least three times continuously differentiable wrt. the parameters and bounded.
The losses should satisfy (LI)-(L3) from Section 3.2. We formulate the RUDDER algorithm as a
minimization problem of square losses Lh(θn, ωn, zn) and Lg(θn, ωn, zn):
Lh =ET〜∏ 2 X (Rt+l(τ; 3n) - q(st,at; θn)) +(Z2)n Pθ(τ, θn, Zn) ,	(10)
1
2
Lg = ET 〜∏(θn,Zn)
2
Rt+1 - g(τ; ωn)	+ (z2)n ρω(τ, θn, zn) .
(11)


θn+1 = θn - a(n) h(θn, ωn, zn) , ωn+1 = ωn - b(n) f(θn, ωn, zn) .
(12)
Let us now briefly describe the terms in Eq. (10)-(12):
•	q(s, a; θn) is a function parametrized by θn that approximates the Q-value q(s, a). Note that the
policy loss Lh implicitly depends on the policy ∏ via q.
•	The expressions for h(θn, ωn, zn) and f(θn, ωn, zn) can be found in Section A.2 in the appendix.
•	R is the original MDP reward.
•	R(τ ; ωn ) is the redistributed reward based on the return decomposition of g with parameter vector
ωn . For a state-action sequence τ the realization of its redistributed reward R is computed from
T
g(τ ; ωn ) and the realization of return variable	t=0 Rt+1 . In practice, g is an LSTM-network.
•	ρθ(τ, θn, Zn) is a regularization term for learning the Q-value approximation q.
•	ρω(τ, θn, zn) is a regularization term for learning the reward redistribution function g.
•	∏ is a behavioral policy that does not depend on the parameters.
•	Zn = ((z1)n, (z2)n, (z1)n-1, (z2)n-1, θn-1, ωn-1) denotes an additional Markov process, where
we use the same construction as in the PPO setting. Details can again be found in Section 3.2.
•	∏(θn, Zn) is a softmax policy applied to (zι)n q (see Eq. (15) for a precise introduction). It
depends on (z1 )n > 1 which makes it more greedy and θn is updated in every time-step.
The next corollary states that the RUDDER algorithm converges.
Corollary 2 (Convergence RUDDER). Fix a starting point (θ0, ω0). Determine the associated
neighborhood V0 × U0 as in Section 3.2. Assume learning rates like (A4) for the time-scales a(n)
and b(n) mentioned in Section 3.1. Also take the loss assumptions in Section 3.2 for granted.
Using the same notation as in Theorem 1, the RUDDER algorithm Eq. (12) converges to a local
minimum (θ*(θ0, ωo), λ(θ*(θo, ω°))) of the associated losses Eq. (10) and Eq. (11): (θn,, ωn) →
(θ*(θo, ωo), λ(θ*(θo, ωo))) a.s. as n → ∞.
Proof. We apply Theorem 1 since all its assumptions are fulfilled.
□
3	Assumptions and Proof of Theorem 1
This section aims at presenting the theoretical framework from Karmakar & Bhatnagar (2017) and
Borkar (2008) that we want to apply to prove Theorem 1. We formulate the convergence result
Theorem 2 and the assumptions (A1)-(A7) that we need to ensure in order to get there. Then we
discuss how it can be applied to our setting.
3.1	The Stochastic Approximation Theory: Borkar and Karmakar & B hatnagar.
For this section we use the formulations in Heusel et al. (2017); Karmakar & Bhatnagar (2017).
Stochastic approximation algorithms are iterative procedures to find stationary points (minimum,
maximum, saddle point) of functions when only noisy observations are provided. We use two
time-scale stochastic approximation algorithms, i.e. two coupled iterations moving at different speeds.
Convergence of these interwoven iterates can be ensured by assuming that one step size is considerably
smaller than the other. The slower iterate is assumed to be slow enough to allow the fast iterate to
converge while simultaneously being perturbed by the slower. The perturbations of the slower should
5
Under review as a conference paper at ICLR 2021
be small enough to ensure convergence of the faster. The iterates map at time step n ≥ 0 the fast
variable ωn ∈ Rk and the slow variable θn ∈ Rm to their new values:
θn+1 =	θn	-	a(n)	(h(θn, ωn, zn)	+	(m1)n)	,	(13)
ωn+1 =	ωn	-	b(n)	(f(θn, ωn, zn)	+	(m2)n)	,	(14)
where:
•	h(.) ∈ Rm and f(.) ∈ Rk are mappings for Eq. (13) and Eq. (14), respectively.
•	a(n) and b(n) are step sizes for Eq. (13) and Eq. (14), respectively.
•	(m1)n and (m2)n are Martingale difference sequences for Eq. (13) and Eq. (14), respectively.
•	zn denotes the common Markov control process for Eq. (13) and Eq. (14).
We assume that all the random variables are defined on a common probability space (Ω, A, P) with
associated sigma algebra A and probability measure P . Let us continue with an informal summary of
the assumptions needed to ensure convergence of (13)-(14). More precise technical details can be
found in Section A.4 in the appendix and in Karmakar & Bhatnagar (2017).
(A1) Assumptions on the controlled Markov processes: zn takes values in a compact metric space
S . It is controlled by the iterate sequences θn and ωn and additionally by a random process an taking
values in a compact metric space W. The dynamics wrt. n are specified by a transition kernel. Be
aware that this control setting can in general be different than the already introduced MDP setting.
(A2) Assumptions on the update functions: f, and h are jointly continuous as well as Lipschitz in
their first two arguments, and uniformly w.r.t. the third.
(A3) Assumptions on the additive noise: For i = 1, 2 the (mi)n are martingale difference sequences
with bounded second moments.
(A4) Assumptions on the learning rates: Informally, the sums of the positive a(n) and b(n) diverge,
while their squared sums converge. a(n) goes to zero faster than b(n).
(A5) Assumptions on the transition kernels: The transition kernels of zn are continuous.
(A6) Assumptions on the associated ODEs: We consider occupation measures which intuitively
give for the controlled Markov process the probability or density to observe a particular state-action
pair from S × W for given θ and ω and a given control. A precise definition of these occupation
measures can be found e.g. on page 68 of Borkar (2008) or page 5 in Karmakar & Bhatnagar (2017).
We need the following assumptions:
•	We assume that there exists only one such ergodic occupation measure for zn on S × W, denoted
by Γθ,ω. A main reason for assuming uniqueness is that it enables us to deal with ODEs instead of
differential inclusions. Moreover, set f(θ, ω) = f(θ, ω, z) Γθ,ω (dz, W).
•	For θ ∈ Rm, the ODE ω(t) = f(θ, ω(t)) has a unique asymptotically stable equilibrium λ(θ)
with attractor set Bθ such that λ : Rm → Rk is a Lipschitz map with global Lipschitz constant.
•	The Lyapunov function V (θ, .) associated to λ(θ) is continuously differentiable.
•	Next define h(θ) = ∕h(θ,λ(θ), z) Γθ,λ(θ)(dz, W). TheODE θ(t) = h(θ(t)) has a global
attractor set A.
•	For all θ, with probability 1, ωn for n ≥ 1 belongs to a compact subset Qθ of Bθ “eventually”.
This assumption is an adapted version of (A6)’ of Karmakar & Bhatnagar (2017) to avoid too many
technicalities (e.g. Karmakar & Bhatnagar (2017) uses a different control for each iterate).
(A7) Assumption of bounded iterates: The iterates θn and ωn are uniformly bounded almost surely.
Convergence for Eq. (13)-(14) is given by Theorem 1 in Karmakar & Bhatnagar (2017):
Theorem 2 (Karmakar & Bhatnagar). Under the assumptions (A1)-(A7), the iterates Eq. (13) and
Eq. (14) converge: (θn, ωn) → ∪θ*∈a(Θ*, λ(θ*)) a.s. as n → ∞ .
3.2	Application to Proof of Main Result
Next we describe how Theorem 2 yields Theorem 1 by discussing the validity of (A1)-(A7). We
additionally mention details about their concrete realization in the context of PPO and RUDDER. We
conclude by a discussion on how we can allow our policies to become sufficiently greedy over time.
(A1) Controlled Markov process for the abstract setting: For Eq. (1) - (3) we assume to have a
controlled process that fulfills the previously discussed requirements for (A1).
6
Under review as a conference paper at ICLR 2021
(A1) Controlled Markov process for PPO and RUDDER: In our applications to RUDDER
and PPO, however, the Markov control will have a much simpler form: zn mainly consists of
real sequences which obey the Markov property. Also we do not have any additional control
in these situations. More concretely: zn = ((z1)n, (z2)n, (z1)n-1, (z2)n-1, θn-1,ωn-1) with
(z1)n ∈ [1, β] for some β > 1, and (z2)n ∈ [0, (z2)0] for some (z2)0 > 1. (z1)n can be defined
by (z1)0 = 1 and (z1)n+1 = (1 - ∣)(zι)n + 1. It consists of the partial sums of a geometric
series converging to β > 1. For (z2)n we can use any sequence satisfying the Markov Property and
converging to zero, e.g. (z2)0 = C and (z2)n+ι = α(z2)n with α < 1 or (z2)n+1 =嵩六α with
1 < α. zn then is a time-homogeneous Markov process with unique invariant measure, cf. Section
A.6 in the appendix.
Let us now describe the meaning of this process for RUDDER and PPO: The component (z1)n is
used as a slope parameter for the softmax policy and goes to a large value β to make the policy greedy.
The softmax policy is introduced in the following rather abstract way: For a sufficiently smooth
function (deep neural network) ψ(s; θ) = (ψ1(s; θ),..., ψ⑷(s; θ)), a softmax policy ∏(θ, zι) is
defined as
π(ai | s; θ, z1 )
exp(zι ψi(s; θ))
Pj exp(zι ψj(s; θ))
(15)
For RUDDER and PPO We use ψ(s; θ) = q(s; θn) with qi(s; θn) approximating qπ(s, ai). The
component (z2)n is used to weight an additional term in the objective and goes to zero over time. We
require (z1)n-1 and θn-1 for the trust-region term ρ, for which we have to reconstruct the old policy.
Further details, especially concerning β, can be found in Section 3.2 and Section A.7 in the appendix.
(A3) Martingale Difference Property and the Probabilistic Setting. Here we describe the sam-
pling process more formally:
•	The baseline probability space is given by Ω = [0,1], P = μ and A = B([0,1]), with μ denoting
the Lebesgue measure and B([0, 1]) the Borel σ-algebra on [0, 1].
•	Next we introduce the set of all trajectories obtained by following ∏ as Ω∏	=
C	/	∖ I ∙	1	,	r∙<	A	,	1 1	1 Ii r∖r
{τ = (s, a)o：T∣τ is chosen wrt. π, So = so, Ao = a。} . Its power set serves as related σ-algebra A∏.
T
•	We endow Aπ	with a probability measure:	Pπ(τ)	=	t=1 p(st	|	st-1,	at-1)	π(at	|	st),	which
computes the probability of choosing a sequence T with starting point (so, a。). An can be ordered
according to the magnitude of the values of its events on Pπ. We denote this ordering by ≤.
•	We define S∏ : Ω → Ω∏ as S∏ : x → argmaxT∈ω7γ {Pη≤τ Pn(η) ≤ x} . ThiS map is well
defined and measurable and it describes how to get one sample from a multinomial distribution with
probabilities P∏(τ), where T ∈ Ω∏.
•	Now we are in the position to describe the sampling process. As mentioned already in the beginning,
we use an online update, i.e. we introduce functions h and f , where h approximates h by using one
sample trajectory instead of the expectation, the same goes for f. More formally, for f we define
f(θn, ωn, Zn) ： [0, 1] → Rk as x → S∏(θn,zn) (x) = τ → Vωn Φ(g(τ ； 3n, Zn),T, θn, 3加 Zn).
L♦ 11	FC , F	, ∙ 1	/ ∖	T~ / Λ	∖ 7 / Λ	∖ 1
• Finally we can define the martingale errors as (m1)n+1 = h(θn, ωn, Zn) - h(θn, ωn, Zn) and
(m2 )n+1 = f(θn, ωn, Zn) - f(θn, ωn, Zn).
Further details (regarding the sampling process and the bounds for the second moments) can be found
in Sections A.5 and A.6 in the appendix.
(A2) and (A6) Smoothness for h and f and Stability of ODEs via Assumptions on Losses. We
make the following assumptions on the loss functions of Eq. (1) (or Eq. (2)) and Eq. (3):
(	L1) π, g, Φ and φ all have compact support and are at least three times continuously differentiable
wrt. their parameters θ and ω.
(L2) For each fixed θ all critical points of Lg (θ, ω) are isolated local minima and there are only
finitely many. The local minima {λi(θ)}ik=(θ1) of Lg(θ, .) can be expressed locally as at least twice
continuously differentiable functions with associated domains of definitions {Vλi(θ)}ik=(θ1).
(L3) Locally in Vλi(θ), Lh (θ, λi(θ)) has only one local minimum.
7
Under review as a conference paper at ICLR 2021
Some remarks concerning these assumptions are in order:
•	Comment on (L1): The parameter space of networks can be assumed to be bounded in practice.
•	Comment on (L2): For each starting point (θ0, ω0) we can find a neighborhood Uθ0 (ω0) that
connects ω° with a local minimum λi(θ0) of Lg (θo, ∙), so that Ue0 (ω0) contains no further critical
points, e.g. a small neighborhood around the steepest descent path on the loss surface of Lg (θo, ∙)
starting at ω0 . Next we apply the implicit function theorem (IFT) to find a neighborhood V0
around θ0, such that λi(θ) is twice continuously differentiable there. The IFT can be applied to
f (θ, ∙) = VωLg(θ, ∙) = 0, since the associated Hessian is positive definite and thus invertible. It can
even be shown that it is twice continuously differentiable, using analytic versions of the IFT.
•	Comment on (L3): In a similar vein, for each θ ∈ V0 we can construct neighborhoods Uθ(ω0)
around ω0 with λi (θ) as unique associated local minimum (we may have to shrink V0). Define
∪θ∈V0 ({θ} × Uθ (ω0)) = V0 × U0.
•	Comment on compatibility with global setting: By using a suitable regularization (e.g. weight
decay) for the networks, we can assume that the algorithm Eq. (13) and Eq. (14) always stays in
V0 × U0. This heuristically justifies that for (θ0, ω0) we localize to V0 × U0.
•	Comment on drawbacks of assumptions: A completely rigorous justification of this argument would
require a more thorough analysis of SGD, which would of course be a very interesting future research
direction. For SGD with one time scale, a result in this direction can be found in Metrikopoulos et al.
(2020). It would be interesting to extend it to two timescales.
• Comment on requirements for critical points: It is also a widely accepted (but yet unproven)
conjecture that the probability of ending in a poor local minimum is very small for sufficiently
large networks, see e.g. Choromanska et al. (2015); Kawaguchi (2016); Kawaguchi et al. (2017);
Kawaguchi & Bengio (2019); Kawaguchi et al. (2019). Thus, we can ensure that Eq. (13) and Eq. (14)
really converges to a useful quantity (a high quality local minimum), if our networks are large enough.
Using these smoothness assumptions, it is not hard to ensure the required properties in (A2) and (A6)
by relating the analysis of the loss surfaces of Lg and Lh to a stability analysis of the corresponding
gradient systems. Further technical details can be found in Section A.6 in the appendix.
(A5) and (A7) Transition Kernel and Bounded Iterates. The transition kernel is continuous (c.f.
Section A.6 in the appendix). Boundedness of θn and ωn is achieved by weight decay terms in
practice.
Proof of Theorem 1.
Proof. In the previous paragraphs we discussed how the assumptions of Theorem 2 can be fulfilled.
. □
Finite Greediness is Sufficient to Converge to the Optimal Policy. Regularization terms are
weighted by (z2)n which converges to zero, therefore the optimal policies are the same as without the
regularization. There exists an optimal policy ∏* that is deterministic according to Proposition 4.4.3
in Puterman (2005). We want to ensure via a parameter (z1)n that the policy becomes more greedy
during learning. If the policy is not greedy enough, estimates of the action-value or the advantage
function may misguide the algorithm and the optimal policy is not found. For example, huge negative
rewards if not executing the optimal actions may avoid convergence to the optimal policy if the policy
is not greedy enough. (z1)n directly enters the policy according to Eq. (15). We show that we can
estimate how large (z1)n must become in order to ensure that Q-value and policy gradient methods
converge to an optimal policy, if it is the local minimum of the loss function (we cannot ensure this).
For policy gradients, the optimal actions receive always the largest gradient and the policy converges
to the optimal policy. The required greediness will be measured by the parameter β > 1. In practical
applications we know that β exists but do not know its value, since it depends on characteristics of
the task and the optimal Q-values. For a more formal treatment c.f. Section A.7 in the appendix,
especially Lemma 2.
Conclusions and Outlook We showed local convergence of an abstract actor-critic setting and
applied it to a version of PPO and RUDDER under practical assumptions. We intend to apply our
results to similar practically relevant settings, e.g. the PPO algorithm discussed in Schulman et al.
(2018). A further future direction is to guarantee convergence to an optimal policy. It would also be
interesting to relax some of the required assumptions on the loss functions (e.g. by extending the
techniques in Metrikopoulos et al. (2020) to two timescales) or elaborate on convergence rates.
8
Under review as a conference paper at ICLR 2021
References
P. A. Absil and K. Kurdyka. On the stable equilibrium points of gradient systems. Systems & Control
Letters, 55(7):573-577, 2006.
J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter, and S. Hochreiter.
Rudder: Return decomposition for delayed rewards. ArXiv, 1806.07857, 2018.
J. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter, and S. Hochreiter.
Rudder: Return decomposition for delayed rewards. In Advances in Neural Information Processing
Systems 33, 2019. ArXiv 1806.07857.
B. Bakker. Reinforcement learning by backpropagation through an lstm model/critic. In IEEE
International Symposium on Approximate Dynamic Programming and Reinforcement Learning,
pp. 127-134, 2007. doi: 10.1109/ADPRL.2007.368179.
D. P. Bertsekas and J. N. Tsitsiklis. Neuro-dynamic programming. Athena Scientific, Belmont, MA,
1996.
S. Bhatnagar, H. L. Prasad, and L. A. Prashanth. Stochastic Recursive Algorithms for Optimization.
Lecture Notes in Control and Information Sciences. Springer-Verlag London, 2013. ISBN 978-1-
4471-4284-3. doi: 10.1007/978-1-4471-4285-0.
V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint, volume 48 of Texts and
Readings in Mathematics. Springer, 2008. ISBN 978-93-86279-38-5.
V. S. Borkar and S. P. Meyn. The O.D.E. method for convergence of stochastic approximation and
reinforcement learning. SIAM Journal on Control and Optimization, 38(2):447-469, 2000. doi:
10.1137/S0363012997331639.
G. Casella and R. L. Berger. Statistical Inference. Wadsworth and Brooks/Cole, 2002.
A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun. The loss surfaces of multilayer
networks. Proceedings of the Eighteenth International Conference on Artificial Intelligence and
Statistics, pp. 192-204, 2015.
P. Dayan. The convergence of TD(λ) for general λ. Machine Learning, 8:341, 1992.
J.	Fan, Z. Wang, Y. Xie, and Z. Yang. A theoretical analysis of deep q-learning. CoRR, abs/1901.00137,
2020.
M. Hairer. Ergodic properties of markov processes. Lecture notes, 2018.
M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, G. Klambauer, and S. Hochreiter. GANs trained
by a two time-scale update rule converge to a Nash equilibrium. In I. Guyon, U. v. Luxburg,
S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 30, pp. 6626-6637. Curran Associates, Inc., 2017. preprint
arXiv:1706.08500.
C. Jin, P. Netrapalli, and M. I. Jordan. Minmax optimization: Stable limit points of gradient descent
ascent are locally optimal. ArXiv, 1902.00618, 2019.
P. Karmakar and S. Bhatnagar. Two time-scale stochastic approximation with controlled Markov
noise and off-policy temporal-difference learning. Mathematics of Operations Research, 2017.
doi: 10.1287/moor.2017.0855.
K. Kawaguchi. Deep learning without poor local minima. In D. D. Lee, M. Sugiyama, U. V. Luxburg,
I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems 29, pp.
586-594, 2016.
K. Kawaguchi and Y. Bengio. Depth with nonlinearity creates no bad local minima in ResNets.
Neural Networks, 118:167-174, 2019.
K.	Kawaguchi, L. P. Kaelbling, and Y. Bengio. Generalization in deep learning. arXiv, 1710.05468,
2017.
9
Under review as a conference paper at ICLR 2021
K. Kawaguchi, J. Huang, and L. P. Kaelbling. Effect of depth and width on local minima in deep
learning. Neural Computation, 31(6):1462-1498, 2019.
V.	R. Konda and V. S. Borkar. Actor-critic-type learning algorithms for Markov decision processes.
SIAM J. Control Optim., 38(1):94-123, 1999. doi: 10.1137/S036301299731669X.
V.	R. Konda and J. N. Tsitsiklis. Actor-critic algorithms. Advances in neural information processing
systems, pp. 1008-1014, 2000.
V.	R. Konda and J. N. Tsitsiklis. On actor-critic algorithms. SIAM J. CONTROL OPTIM., 42(4):
1143-1166, 2003. doi: 10.1137/S0363012901385691.
H. J. Kushner and D. S. Clark. Stochastic Approximation Methods for Constrained and Unconstrained
Systems. Applied Mathematical Sciences. Springer-Verlag New York, 1978. ISBN 978-0-387-
90341-5. doi: 10.1007/978-1-4684-9352-8.
H. J. Kushner and G. G. Yin. Stochastic Approximation and Recursive Algorithms and Applications.
Stochastic Modelling and Applied Probability. Springer New York, 2003. ISBN 9780387008943.
T. Lin, C. Jin, and M. I. Jordan. On gradient descent ascent for nonconvex-concave minimax problems.
ArXiv, 1906.00331, 2019.
B. Liu, Q. Cai, Z. Yang, and Z. Wang. Neural proximal/trust region policy optimization attains
globally optimal policy. In Advances in Neural Information Processing Systems 33, volume ArXiv
1906.10306, 2019.
H. R. Maei, C. Szepesvari, S. Bhatnagar, D. Precup, D. Silver, and R. S. Sutton. Convergent temporal-
difference learning with arbitrary smooth function approximation. In Y. Bengio, D. Schuurmans,
J. D. Lafferty, C. K. I. Williams, and A. Culotta (eds.), Advances in Neural Information Processing
Systems 22, pp. 1204-1212. Curran Associates, Inc., 2009.
E. V. Mazumdar, M. I. Jordan, and S. S. Sastry. On finding local Nash equilibria (and only local Nash
equilibria) in zero-sum games. ArXiv, 1901.00838, 2019.
P.	Metrikopoulos, N. Hallak, A. Kavis, and V. Cevher. On the almost sure convergence of stochastic
gradient descent in non-convex problems. ArXiv, 2006.11144, 2020.
V.	Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra, and M. A. Riedmiller.
Playing Atari with deep reinforcement learning. ArXiv, 1312.5602, 2013.
V.	Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Ried-
miller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,
D. Kumaran, D. Wierstra, S. Legg, , and D. Hassabis. Human-level control through deep reinforce-
ment learning. Nature, 518(7540):529-533, 2015. doi: 10.1038/nature14236.
P. W. Munro. A dual back-propagation scheme for scalar reinforcement learning. In Proceedings of
the Ninth Annual Conference of the Cognitive Science Society, Seattle, WA, pp. 165-176, 1987.
OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. Debiak, C. Dennison, D. Farhi, Q. Fischer,
S. Hashme, C. Hesse, R. Jozefowicz, S. Gray, C. Olsson, J. Pachocki, M. Petrov, H. P. deOliveira
Pinto, J. Raiman, T. Salimans, J. Schlatter, J. Schneider, S. Sidor, I. Sutskever, J. Tang, F. Wolski,
and S. Zhang. Dota 2 with large scale deep reinforcement learning. arXiv, 1912.06680, 2019.
M. L. Puterman. Markov Decision Processes. John Wiley & Sons, Inc., 2nd edition, 2005. ISBN
978-0-471-72782-8.
H. Robbins and S. Monro. A stochastic approximation method. Ann. Math. Stat., 22(3):400-407,
1951. doi: 10.1214/aoms/1177729586.
A. J. Robinson. Dynamic Error Propagation Networks. PhD thesis, Trinity Hall and Cambridge
University Engineering Department, 1989.
T. Robinson and F. Fallside. Dynamic reinforcement driven error propagation networks with applica-
tion to game playing. In Proceedings of the 11th Conference of the Cognitive Science Society, Ann
Arbor, pp. 836-843, 1989.
10
Under review as a conference paper at ICLR 2021
J. Schulman, S. Levine, P. Moritz, M. I. Jordan, and P. Abbeel. Trust region policy optimization. ArXiv,
1502.05477, 2015. 31st International Conference on Machine Learning (ICML), Proceedings of
Machine Learning Research, volume 37.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. ArXiv, 1707.06347, 2018.
S. Singh, T. Jaakkola, M. Littman, and C. Szepesvdri. Convergence results for single-step on-
policy reinforcement-learning algorithms. Machine Learning, 38:287-308, 2000. doi: 10.1θ23∕A:
1007678930559.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge,
MA, 2 edition, 2018.
R. S. Sutton, D. McAllester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement
learning with function approximation. Advances in neural information processing systems, pp.
1057-1063, 2000.
J. N. Tsitsiklis. Asynchronous stochastic approximation and q-learning. Machine Learning, 16(3):
185-202, 1994. doi: 10.1023/A:1022689125041.
O. Vinyals, I. Babuschkin, W. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. Choi, R. Powell,
T. Ewalds, P. Georgiev, J. Oh, D. Horgan, M. Kroiss, I. Danihelka, A. Huang, L. Sifre, T. Cai,
J. Agapiou, M. Jaderberg, and D. Silver. Grandmaster level in StarCraft II using multi-agent
reinforcement learning. Nature, 575(7782):350-354, 2019. doi: 10.1038/s41586-019-1724-z.
C. J. C. H. Watkins and P. Dayan. Q-Learning. Machine Learning, 8:279-292, 1992.
T. Xu, S. Zou, and Y Liang. Two time-scale off-policy td learning: Non-asymptotic analysis over
Markovian samples. In Advances in Neural Information Processing Systems 32, pp. 10633-10643,
2019.
Z. Yang, Y. Chen, M. Hong, and Z. Wang. Provably global convergence of actor-critic: A case
for linear quadratic regulator with ergodic cost. In Advances in Neural Information Processing
Systems 32, pp. 8351-8363, 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Further Details on PPO
Here we describe the minimization problem for the PPO setup in a more detailed way by including
the exact expression for the gradients of the respective loss functions:
Lh (θn, ωn, Zn)	=	ET 〜π(θn,Zn)	[ ―	GO	+	(z2)n ρ(τ,	θn,	Zn)] ,	(16)
T
h(θn, ωn, Zn) =ET 〜∏(θn,zn) - £^8 log ∏(°t | St； θn, Zn) (^" (stgt； ωn - V"(st； 3冗))
t=0
T
+ (z2)n EjVen log∏(at | St; θn
t=0
,Zn) P(T, θn, Zn) + (z2)n Ren ρ(τ, θn, Zn),
n
Lg	(θn, ωn, Zn) =
ET ~∏(θn,Zn)
fTD(θn, ωn, Zn) =
ET~∏(θn ,Zn)
Lg	(θn, ωn, Zn) = Et^π(θn,zn)
fMC(θn, ωn, Zn) = ET〜π(θn,Zn)
T
2 XWD(t))2 ,
2 t=0
T
-	X δTD(t) Vωn qπ (st,at; ωn),
t=0
-	qπ (St, at; ωn) )
-	qπ(st,at; ωn) Vωn^π(st,at; ωn)
θn+1 = θn - a(n) h(θn, ωn, Zn) , ωn+1 = ωn - b(n) f(θn, ωn, Zn) ,
(17)
(18)
(19)
(20)


A.2 Further details on RUDDER
In a similar vein we present the minimization problem of RUDDER in more detail:
Lh (θn, ωn, Zn)
ET〜∏
1 X (Rt+l(τ; ωn) - q(st,at; θn)) + (z2)n Pθ(τ, θn, Zn)
(21)
h(θn, ωn, Zn)	= ET 〜∏	-X (	Rt+1 (t;	ωn) -	q(St,	at;	θn)	) Vθq(st, at; θn)	(22)
t=0
+ (z2)n VθPθ(τ, θn, Zn)
Lg (θn, ω
n, Zn) = Et〜∏(θn,Zn)
f(θn,ωn,Zn)
=ET ~∏(θn,Zn)
+ (z2 )n VωPω (τ, θn , Zn )
θn+1 = θn - a(n) h(θn, ωn, Zn) ,	ωn+1
1 - g(τ; ωn)	+ (z2 )n Pω (τ, θn, Zn)	(23)
- g(τ; ωn) Vωg(τ; ωn)	(24)
= ωn - b(n) f(θn , ωn , Zn ) ,	(25)
A.3 Causality and Reward-To-Go
This section is meant to provide the reader with more details concerning the causality assumption that
leads to the formula for h in Eq. (16) for PPO. We can derive a formulation of the policy gradient
with reward-to-go. For ease of notation, instead of using Pπ (τ) as in previous sections, we here
12
Under review as a conference paper at ICLR 2021
denote the probability of state-action sequence τ = τ0,T = (s0, s0, s1, a1, . . . , sT, aT) with policy π
as
T
p(τ) = p(s0) π(a0 | s0)	p(st | st-1, at-1) π(at | st)	(26)
t=1
TT
= p(s0)	p(st | st-1 , at-1)	π(at | st) .
t=1	t=0
The probability of state-action sequence τ0,t = (s0, s0, s1, a1, . . . , st, at) with policy π is
t
p(τ0,t) = p(s0) π(a0 | s0)	p(sk | sk-1, ak-1) π(ak | sk)	(27)
k=1
tt
= p(s0 )	p(sk | sk-1, ak-1)	π(ak | sk) .
k=1	k=0
The probability of state-action sequence τt+1,T = (st+1, at+1, . . . , sT , aT) with policy π given
(st, at) is
T
p(τt+1,T | st, at) =	p(sk | sk-1, ak-1) π(ak | sk)	(28)
k=t+1
TT
p(sk | sk-1, ak-1)	π(ak | sk) .
k=t+1	k=t+1
The expectation of PtT=0 Rt+1 is
T
Eπ X Rt+1
t=0
T
XEπ [Rt+1] .
t=0
(29)
With Rt+1 〜p(rt+ι | st, at), the random variable Rt+ι depends only on (st, at). We define the
expected reward Ert+1 [Rt+1 | st, at] as a function r(st, at) of (st, at):
r(st, at)	:=	Ert+1	[Rt+1 | st, at]	=	p(rt+1	|	st, at)	rt+1	.	(30)
rt+1
Causality. We assume that the reward Rt+ι = R(st, at)〜p(rt+ι | st, at) only depends on the past
but not on the future. The state-action pair (st, at) is determined by the past and not by the future.
Relevant is only how likely we observe (st, at) and not what we do afterwards.
Causality is derived from the Markov property of the MDP and means:
ET〜∏ [Rt+ι] = Eτo,t〜∏ [Rt+ι] .	(31)
That is
Ei∏ [Rt+ι] =EE ∑∑...∑ ∑p(τ) r(st, at)	(32)
TT
=XXXX ... XX Yp(sι I si-i,ai-i) Y∏(aι I 扪(”，出)
tt
= XXXX...XXYp(slIsl-1,al-1)Yπ(alIsl)r(st,at)
TT
XXXX ... XX Y p(slIsl-1,al-1) Y π(al I sl)
st+1 at+1 st+2 at+2	sT aT l=t+1	l=t+1
tt
= XXXX...XXYp(slIsl-1,al-1)Yπ(alIsl)r(st,at)
=ET0,t〜π [Rt+1].
13
Under review as a conference paper at ICLR 2021
Policy Gradient Theorem. We now assume that the policy π is parametrized by θ, that is, π(at |
st) = π(at | st; θ). We need the gradient with respect to θ of Qtb=a π(at | st):
b	bb
Vθ Y ∏(at |	st; θ)	= X Y	π(at	|	st; θ)	Vθπ(as	|	ss; θ)	(33)
t=a	s=a t=a,t6=s
Y / I X X vθπ(as 1 Ss； θ)
=∏π(at | st； θ)E Π(as | ss; θ)
t=a	s=a s s
bb
=	π(at | st; θ)	Vθ log π(as | ss; θ) .
t=a	s=a
It follows that
t
VθEπ [Rt+1] = Eπ X Vθ log π(as | ss; θ) Rt+1
(34)
We only have to consider the reward to go. Since a0 does not depend on π, we have VθEπ [R1] = 0.
Therefore
VθEπ
T
X Rt+1
t=0
Tt
T
X VθEπ [Rt+1]
t=0
(35)
We can express this by Q-values.
T
We have finally:
ΣΣVθ log π(ak | sk; θ) Rt+1
t=1 k=1
TT
ΣΣVθ log π(ak | sk; θ) Rt+1
k=1 t=k
T
k=1
T
Vθ log∏(ak | sk； θ) ERt+1
t=k
Vθ log ∏(ak | sk； θ) Gk .
k=1
E∏	Vθ log∏(ak I sk； θ) Gk
k=1
T
=EEn [Vθ log∏(ak | sk； θ) Gk]
k=1
T
XE
k=1
T
XE
k=1
，丁0,卜〜π [vθ log π(ak | sk； θ) ETk+ι,τ〜π [Gk | sk,ak]]
:T0,k〜∏ [Vθ log∏(ak | sk； θ) qπ(sk,ak)]
ET〜π
T
Vθ log ∏(ak | sk； θ) qπ(sk,ak).
k=1
T
VθEπ X Rt+1
t=0
ET〜π
T
£ Vθ log ∏(ak | sk； θ) qπ (sk,ak).
k=1
(36)
(37)
T
14
Under review as a conference paper at ICLR 2021
A.4 Precise statement of Assumptions
Here we provide a precise formulation of the assumptions from Karmakar & Bhatnagar (2017). The
formulation we use here is mostly taken from Heusel et al. (2017):
(A1) Assumptions on the controlled Markov processes: The controlled Markov process z takes
values in a compact metric space S. It is controlled by the iterate sequences θn } and ωn and
furthermore zn by a random process an taking values in a compact metric space W. For B Borel in
S the Zn dynamics for n ≥ 0 is determined by a transition kernel p:
P(zn+1 ∈ B∣Z1, al, θι, ωι,l 6 n)
/ p(dz∣Zn, an,
B
θn, ωn ).
(A2) Assumptions on the update functions: h : Rm+k × S(1) → Rm is jointly continuous as well
as Lipschitz in its first two arguments, and uniformly w.r.t. the third. This means that for all z ∈ S:
kh(θ, ω, z) - h(θ0,w0,z)k6 L(1)(kθ-θ0k+kω-ω0k).
Similarly for f, where the Lipschitz constant is L(2).
(A3) Assumptions on the additive noise: For i = 1, 2, {(mi)n} are martingale difference sequences
with bounded second moments. More precisely, (mi)n are martingale difference sequences w.r.t.
increasing σ-fields
Fn = σ(θl, ωl, (m1)l, (m2)l, zl, l 6 n),
satisfying E k(mi)nk2 | Fn 6 Bi forn ≥ 0 and a given constants Bi .
(A4) Assumptions on the learning rates:
a(n) = ∞, a2(n) < ∞,
X b(n) = ∞,	X b2(n) < ∞,
and a(n) = o(b(n)). Furthermore, a(n), b(n), n ≥ 0 are non-increasing.
(A5) Assumptions on the transition kernels: The state-action map
S X W X Rm+k 3(z, a, θ, ω) → p(dy | z, a, θ, ω)
is continuous (the topology on the spaces of probability measures is induced by weak convergence).
(A6) Assumptions on the associated ODEs: We consider occupation measures which intuitively
give for the controlled Markov process the probability or density to observe a particular state-action
pair from S X W for given θ and ω and a given control. A precise definition of these occupation
measures can be found e.g. on page 68 of Borkar (2008) or page 5 in Karmakar & Bhatnagar (2017).
We have following assumptions:
•	We assume that there exists only one such ergodic occupation measure for zn on S X W, denoted
by Γθ,ω. A main reason for assuming uniqueness is that it enables us to deal with ODEs instead of
differential inclusions. Moreover, set f(θ, ω) = f(θ, ω, z) Γθ,ω (dz, W).
•	We assume that for θ ∈ Rm, the ODE ω(t) = f(θ, ω(t)) has a unique asymptotically stable
equilibrium λ(θ) with attractor set Bθ such that λ : Rm → Rk is a Lipschitz map with global
Lipschitz constant.
•	The Lyapunov function V (θ, .) associated to λ(θ) is continuously differentiable.
•	Next define h(θ) = ʃ h(θ, λ(θ), z) Γθ,λ(θ)(dz, W). We assume that the ODE θ(t) = h(θ(t))
has a global attractor set A.
•	For all θ, with probability 1, ωn for n ≥ 1 belongs to a compact subset Qθ of Bθ “eventually”.
This assumption is an adapted version of (A6)’ of Karmakar & Bhatnagar (2017), to avoid too many
technicalities (e.g. in Karmakar & Bhatnagar (2017) two controls are used, which we avoid here to
not overload notation).
(A7) Assumption of bounded iterates: supn kθn k < ∞ and supn kωn k < ∞ a.s.
15
Under review as a conference paper at ICLR 2021
A.5 Further Details concerning the Sampling Process
Let us formulate the construction of the sampling process in more detail: We introduced the function
Sπ in the main paper as follows:
S∏ : Ω → Ω∏, x → argmax < ^^Pn (η) ≤ x > .
T∈ω∏	(η≤τ
Now Sπ basically divides the interval [0, 1] into finitely many disjoint subintervals, such that the i-th
subinterval Ii maps to the i-th element Ti ∈ Ω∏, and additionally the length of Ii is given by P∏ (Ti).
Sπ is measurable, because the pre-image of any element of the sigma-algebra Aπ wrt. Sπ is just a
finite union of subintervals of [0, 1], which is clearly contained in the Borel-algebra. Basically Sπ just
describes how to get one sample from a multinomial distribution with (finitely many) probabilities
P∏(τ), where T ∈ Ω∏. Compare With inverse transform sampling, e.g. Theorem 2.1.10. In Casella &
Berger (2002) and applications thereof. For the reader’s convenience let’s briefly recall this important
concept here in a formal way:
Lemma 1 (Inverse transform sampling). Let X have continuous cumulative distribution FX (x) and
define the random variable Y as Y = FX(X). Then Y is uniformly distributed on (0, 1).
A.6 Further Details for Proof of Theorem 1
Here we provide further technical details needed to ensure the assumptions stated before to prove our
main theorem Theorem 1.
(A1) Assumptions on the controlled Markov processes: Let us start by discussing more details
for controlled processes that appear in the PPO and RUDDER setting. Let us focus on the process
related to(z1)n: Let β > 1 and let the real sequence zn be defined by (z1)1 = 1 and (z1)n+1 =
(I - 1 )(z1)n + LThe zn'sare nothing more but the partial sums of a geometric series converging to
β.
The sequence (z1)n can also be interpreted as a time-homogeneous Markov process (z1)n with
transition probabilities given by
p (z,y) = δ(i-1 )z+ι,	(38)
where δ denotes the Dirac measure, and with the compact interval [1, β] as its range. We use the
standard notation for discrete time Markov processes, described in detail e.g. in Hairer (2018). Its
unique invariant measure is clearly δβ . So integrating wrt. this invariant measure will in our case just
correspond to taking the limit (z1)n → β.
(A2) h and f are Lipschitz: By the mean value theorem it is enough to show that the derivatives wrt.
θ and ω are bounded uniformly wrt. z. We only show details for f, since for h similar considerations
apply. By the explicit formula for Lg, we see that f(θ, ω, z) can be written as:
T
E ∏P(% I st-i,at-i)∏(at | St, θ, z)VωΦ(g(τ; ω, z),τ, θ, ω, z).
s1,..	,sT t=1
a1,...,aT
The claim can now be readily deduced from the assumptions (L1)-(L3).
(A3) Martingale difference property and estimates: From the results in the main paper on the
probabilistic setting, (m1)n+1 and (m2)n+1 can easily be seen to be martingale difference sequences
wrt. their filtrations Fn . Indeed, the sigma algebras created by ωn and θn already describe Aπθn , and
thus:
一一	、	._	一一，一	、，一 r	一。 ~ ，一	」	.
E[(mi )n+1∣Fn] = E[f(θn, 34,Zn)∣Fn] - E[f(θn, 34,Zn)] = 0.
It remains to show that E[|(mi)n+1|2|Fn] ≤ Bi for i = 1, 2. This, however, is also clear, since all
the involved expressions are bounded uniformly again by the assumptions (L1)-(L3) on the losses
(e.g. one can observe this by writing down the involved expressions explicitly as indicated in the
previous point (A2) ).
(A4) Assumptions on the learning rates: These standard assumptions are taken for granted.
(A5) Transition kernels: The continuity of the transition kernels is clear from Eq. (38) (continuity
is wrt. to the weak topology in the space of probability measures. So in our case, this again boils
down to using continuity of the test functions).
16
Under review as a conference paper at ICLR 2021
(A6) Stability properties of the ODEs:
•	By the explanations for (A1) we mentioned that integrating wrt. the ergodic occupation measure in
our case corresponds to taking the limit zn → z (since our Markov processes can be interpreted as
sequences). Thus f (θ, ω) = f (θ, ω, z). In the sequel we will also use the following abbreviations:
f(θ, ω) = f(θ, ω, z), h(θ, ω) = h(θ, ω, z), etc.. Now consider the ODE
ω(t) = f(θ, ω(t)),	(39)
where θ is fixed. Eq. (39) can be seen as a gradient system for the function Lg . By standard results on
gradient systems (cf. e.g. Section 4 in Absil & Kurdyka (2006) for a nice summary), which guarantee
equivalence between strict local minima of the loss function and asymptotically stable points of the
associated gradient system, We can use the assumptions (LI)-(L3) and the remarks thereafter from
the main paper to ensure that there exists a unique asymptotically stable equilibrium λ(θ) of Eq. (39).
• The fact that λ(θ) is smooth enough can be deduced by the Implicit Function Theorem as discussed
in the main paper.
•	For Eq. (39) Lg(θ, ω) - Lg (θ, λ(θ)) can be taken as associated Lyapunov function Vθ (ω), and
thus Vθ(ω) clearly is differentiable Wrt. ω for any θ.
•	The sloW ODE θ(t) = h(θ(t), λ(θ(t)) also has a unique asymptotically stable fixed point, Which
again is guaranteed by our assumptions and the standard results on gradient systems.
(A7) Assumption of bounded iterates: This folloWs from the assumptions on the loss functions.
A.7 Finite Greediness is sufficient to converge to the optimal policy
Here We provide details on hoW the optimal policy can be deduced using only a finite parameter
β > 1. The Q-values for policy π are:
T
q (st, at) = Eπ	Rτ+1 | st, at
τ=t
T-1	T	T
p(sτ+1 | sτ, aτ)	π(aτ | sτ)	Rτ+1 .
st,..,sT τ=t	τ=t	τ=t
at,...,aT
The optimal policy π* is known to be deterministic
(QT=I ∏*3 I St) ∈ {0,1}). LetUS
assume that
the optimal policy is also unique. Then we are going to show the following result:
Lemma 2. For imaχ = arg maxi qπ* (s, αi) and vπ*(s) = maxi qπ*(s,ai). We define
0 < e < min (vπ* (S) - qπ* (s,ai)),
s,i6=imax
We assume a function ψ(S, αi) that defines the actual policy π via
i i	eχp(βψ(S,αi))
π(a 1 s; β) = Pj exp(βψ(s,aj)).
(40)
(41)
We assume that the function ψ already identified the optimal actions, which will occur during learning
at some time point when the policy is getting more greedy:
0 < δ < min (ψ(S, αimax) - ψ(S, αi)) .
s,i6=imax
Hence,
lim π(ai ∣ s; β) = π* (ai ∣ s).
β→∞
(42)
(43)
We assume that
β > max	~ɪ' Tog(2T (∣A∣- 1)图T ∣A∣T(T +1) KR) /δ }	(44)
Then we can make the statement for all S:
∀j,j6=i : qπ(S, αi) > qπ(S, αj) ⇒ i = imax ,	(45)
therefore the Q-values qπ (S, αi) determine the optimal policy as the action with the largest Q-value
can be chosen.
More importantly, β is large enough to allow Q-value based methods and policy gradients converge
to the optimal policy if it is the local minimum of the loss functions. For Q-value based methods
the optimal action can be determined if the optimal policy is the minimum of the loss functions. For
policy gradients the optimal action receives always the largest gradient and the policy converges to
the optimal policy.
17
Under review as a conference paper at ICLR 2021
Proof. We already discussed that the optimal policy π* is known to be deterministic
(QTI π* (at | St) ∈ {0,1}). Let us assume that the optimal policy is also unique. Since
we have
π(aimax | s; β)
π(ai I s∙β) = exp(e (MS,aD -以S,αimax/
( j β) = Pj exp(βψ(s,αj) - ψ(s,aimaχ)) ,	( 6)
1	1
1 + Pjj=imax exp(β ψ(s,aj) - ψ(s,aimax)) > 1 + (∣A∣ - 1) exp(- β δ)
'	(47)
1-
and for i = imax
π(ai ∣ s; β)
辿-1) exp(- βδ)
1 + (∣A∣ — 1) exp(-β δ)
> 1 - (∣A∣ - 1) exp(- β δ)
exp(β (ψ(s,ai) - ψ(s,aimax)))	<
1 + Pj,j=imax exp(βψ(s,aj) - ψ(s,aimax))	exp
(48)
For QT=I π*(at ∣ St) = 1, we have
T
Yπ(at ∣ St) > (1 - (IAI- 1) exp(- β δ))τ > 1 - T (IAI - 1) exp(- β δ),	(49)
t=1
where in the last step we used that (∣A∣ - 1) exp(-βδ) < 1 by definition of β in (44) so that an
application of Bernoulli,s inequality is justified. For QT=I π*(at ∣ St) = 0, we have
τ
Y π(at I St) < exp(- β δ) .	(50)
t=1
Therefore
τ	τ
Y π*(at I St) - Yπ(at I St) < T (∣A∣ - 1) exp(- β δ) .	(51)
t=1	t=1
Using Eq. (51) and the definition of β in Eq. (44) we get:
I qπ (S,ai) - If (S,ai) ∣	(52)
T	T	TT
E山⑶∣	St-1, at-1)	∏	π*(at	∣	St)- ∏ π(at	∣	St)	)〉~?Rt+i
sι,--,sτ t=1	∖t=1	t=1	) t=0
ɑɪ ,...,aτ
<
T
E ∏p(st ∣ St-1, at-1)
sɪ,..,ST	t=1
aɪ ,...,aτ
TT
Y π*(at I St) - Y π(at I St)
t=1	t=1
(T +1) Kr
<
TT
X	Y π*(at∣ St) - Y π(at ∣ St)(T + 1) KR
si,..,st	t=1	t=1
aɪ ,...,aτ
<
ISTIAIT 2 ∣S∣T∣ AI T I(T +1) KR (T + 1) KR = 〃2 ∙
Now from the condition that qπ (s, ai) > qπ(s, aj) for all j = i we can conclude that
qπ* (s, aj) — qπ* (s, ai) < (qπ(s, aj) + e/2) — (qπ(s,ai) — e/2) < e (53)
for all j = i. Thus for j = i it follows that j = imax and consequently i = imax.
□
18