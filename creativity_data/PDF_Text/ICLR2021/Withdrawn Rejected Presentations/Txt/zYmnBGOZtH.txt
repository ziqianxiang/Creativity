Under review as a conference paper at ICLR 2021
An information-theoretic framework for
learning models of instance-independent la-
BEL NOISE
Anonymous authors
Paper under double-blind review
Ab stract
Given a dataset D with label noise, how do we learn its underlying noise model?
If we assume that the label noise is instance-independent, then the noise model
can be represented by a noise transition matrix QD . Recent work has shown that
even without further information about any instances with correct labels, or further
assumptions on the distribution of the label noise, it is still possible to estimate
QD while simultaneously learning a classifier from D. However, this presupposes
that a good estimate of QD requires an accurate classifier. In this paper, we show
that high classification accuracy is actually not required for estimating QD well.
We shall introduce an information-theoretic-based framework for estimating QD
solely from D (without additional information or assumptions). At the heart of our
framework is a discriminator that predicts whether an input dataset has maximum
Shannon entropy, which shall be used on multiple new datasets DD synthesized from
D via the insertion of additional label noise. We prove that our estimator for QD
is statistically consistent, in terms of dataset size, and the number of intermediate
datasets DD synthesized from D As a concrete realization of our framework, we
shall incorporate local intrinsic dimensionality (LID) into the discriminator, and
we show experimentally that with our LID-based discriminator, the estimation
error for QD can be significantly reduced. We achieved average Kullback-Leibler
(KL) loss reduction from 0.27 to 0.17 for 40% anchor-like samples removal when
evaluated on CIFAR10 with symmetric noise. Although no clean subset of D is
required for our framework to work, we show that our framework can also take
advantage of clean data to improve upon existing estimation methods.
1	Introduction
Real-world datasets are inherently noisy. Although there are numerous existing methods for learning
classifiers in the presence of label noise (e.g. Han et al. (2018); Hendrycks et al. (2018); Natarajan
et al. (2013); Tanaka et al. (2018)), there is still a gap between empirical success and theoretical
understanding of conditions required for these methods to work. For instance-independent label
noise, all methods with theoretical performance guarantees require a good estimation of the noise
transition matrix as akey indispensable step (Cheng et al., 2017; Jindal et al., 2016; Patrini et al., 2017;
Thekumparampil et al., 2018; Xia et al., 2019). Recall that for any dataset D with label noise, we can
associate to it a noise transition matrix QD, whose entries are conditional probabilities p(y|z) that a
randomly selected instance of D has the given label y, under the condition that its correct label is z .
Many algorithms for estimating QD either require that a small clean subset Dclean of D is provided
(Liu & Tao, 2015; Scott, 2015), or assume that the noise model is a mixture model (Ramaswamy
et al., 2016; Yu et al., 2018), where at least some anchor points are known for every component. Here,
“anchor points” refer to datapoints belonging to exactly one component of the mixture model almost
surely (cf. Vandermeulen et al. (2019)), while “clean” refers to instances with correct labels.
Recently, it was shown that the knowledge of anchor points or Dclean is not required for estimating
QD. The proposed approach, known as T-Revision (Xia et al., 2019), learns a classifier from D
and simultaneously identifies anchor-like instances in D, which are used iteratively to estimate QD ,
which in turn is used to improve the classifier. Hence for T-Revision, a good estimation for QD is
inextricably tied to learning a classifier with high classification accuracy. In this paper, we propose a
1
Under review as a conference paper at ICLR 2021
framework for estimating QD solely from D, without requiring anchor points, a clean subset, or even
anchor-like instances. In particular, we show that high classification accuracy is not required for a
good estimation of QD . Our framework is able to robustly estimate QD at all noise levels, even in
extreme scenarios where anchor points are removed from D, or where D is imbalanced.
Our key starting point is that Shannon entropy and other related information-theoretic concepts
can be defined analogously for datasets with label noise. Suppose we have a discriminator Φ that
takes any dataset D0 as its input, and gives a binary output that predicts whether D0 has maximum
entropy. Given D, a dataset with label noise, we shall synthesize multiple new datasets D by inserting
additional label noise into D, using different noise levels for different label classes. Intuitively, the
more label noise that D initially has, the lower the minimum amount of additional label noise we
need to insert into D to reach near-maximum entropy. We show that among those datasets D that
are predicted by Φ to have maximum entropy, their associated levels of additional label noise can be
used to compute a single estimate for QD . Our estimator is statistically consistent: We prove that by
repeating this method, the average of the estimates would converge to the true QD .
As a concrete realization of this idea, we shall construct Φ using the notion of Local Intrinsic
Dimensionality (LID) (Houle, 2013; 2017a;b). Intuitively, the LID computed at a feature vector v
is an approximation of the dimension of a smooth manifold containing v that would “best” fit the
distribution D in the vicinity of v. LID plays a fundamental role in an important 2018 breakthrough
in noise detection (Ma et al., 2018c), wherein it was empirically shown that sequences of LID scores
could be used to distinguish clean datasets from datasets with label noise. Roughly speaking, the
training data for Φ consists of LID sequences that correspond to multiple datasets synthesized from
D. In particular, we show that Φ can be trained without needing any clean data. Since we are
optimizing the predictive accuracy of Φ, rather than optimizing the classification accuracy for D, we
also do not require state-of-the-art architectures. For example, in our experiments on the CIFAR-10
dataset (Krizhevsky et al., 2009), we found that LID sequences generated by training on shallow
“vanilla” convolutional neural networks (CNNs), were sufficient for training Φ.
Our contributions are summarized as follows:
•	We introduce an information-theoretic-based framework for estimating the noise transition
matrix of any dataset D with instance-independent label noise. We do not make any assumptions
on the structure of the noise transition matrix.
•	We prove that our noise transition matrix estimator is consistent. This is the first-ever estimator
that is proven to be consistent without needing to optimize classification accuracy. Notably, our
consistency proof does not require anchor points, a clean subset, or any anchor-like instances.
•	We construct an LID-based discriminator Φ and show experimentally that training a shallow
CNN to generate LID sequences is sufficient for obtaining high predictive accuracy for Φ. Using
our LID-based discriminator Φ, our proposed estimator outperforms the state-of-the-art methods,
especially in the case when anchor-like instances are removed from D.
•	Given access to a clean subset Dclean , we show that our method can be used to further improve
existing competitive estimation methods.
2	Proposed Information-theoretic Framework
Our framework hinges on a simple yet crucial observation: Datasets with different label noise levels
have different entropies. Although the entropy of any given dataset D is (initially) unknown to us,
we do know, crucially, that a complete uniformly random relabeling of D would yield a new dataset
with maximum entropy (which we call “baseline datasets”), and we can easily generate multiple
such datasets. We could also use partial relabelings to generate a spectrum of new datasets whose
entropies range from the entropy of D, to the maximum possible entropy. We call them "a-increment
datasets”, where α is a parameter that we control. The minimum value αmin for α, such that an
α-increment dataset reaches maximum entropy, would depend on the original entropy of D. See
Fig. 1 for a visualization of the spectrum of entropies for α-increment datasets and baseline datasets.
Our main idea is to train a discriminator Φ that recognizes datasets with maximum entropy, and
then use Φ to determine this minimum value αmin . Once this value is estimated, we are then able
to estimate QD . Specific realizations of our framework correspond to specific designs for Φ. An
2
Under review as a conference paper at ICLR 2021
Figure 1: A visualization of the entropy maximization process. We illustrate the noise transition
matrices of DILNs as heat maps. The intact CIFAR-10 dataset is used as the underlying clean dataset,
and four noise models are considered (symmetric 20%/50%/80% noise and pairwise 45% noise),
which are shown here as four pairs of columns. The first four rows depict heat maps for α-increment
DILNs, where α = (a, . . . , a) for four values a = 0.0, 0.6, 0.854, 0.877. The last row depicts heat
maps for baseline DILNs, which have expected maximum entropy. Note that the minimum value amin
for a, such that a discriminator would find α-increment DILNs “indistinguishable” from baseline
DILNs, would depend on the base noise model that the α-increment DILN is derived from. From
this figure, we are able to infer, for example, that for symmetric noise models, amin ≈ 0.877 for base
noise level 50%, while in contrast, amin ≈ 0.854 for base noise level 80%. In general, different noise
levels correspond to different minimum values for α.
illustration of our framework using LID-based discriminators is given in Fig. 2; details on LID-based
discriminators can be found in Section 3, and will be further elaborated in the appendix.
Throughout this paper, given any discrete random variables X, Y , we shall writepX(x) andpX|Y (x|y)
to mean Pr(X = x) and Pr(X = x|Y = y) respectively. We assume that the reader is familiar with
the basics of information theory; see Cover & Thomas (2012) for an excellent introduction.
2.1	Entropy of datasets with label noise
Given D a dataset with instance-independent label noise (DILN), let A be its set of all label classes,
and let Y (resp. Z) be the given (resp. correct) label of a randomly selected instance X of D.1 For
convenience, we say that D is a DILN with noise model (Y |Z; A). The noise transition matrix of D
is a matrix QD whose (i,j)-th entry is qiD,j := pY |Z (j, i). We shall define the entropy of D by
H(D) :=-XpZ(i)XqiD,jlogqiD,j.
Notice that H (D) is precisely the conditional entropy of Y given Z. (We use the convention that
0 log 0 = 0.) Hence, it is easy to prove that 0 ≤ H(D) ≤ log |A|. In particular, H(D) = 0 if and
only if every pair of instances of D in the same class have the same given labels. Note also that D
has maximum entropy log |A| if and only if every entry of QD equals 帚(i.e. the given labels of D
are completely noisy). Thus, H(D) could be interpreted as a measure of the label noise level of D.
1Every datapoint of D is a pair (x, y), where x is an instance, and y is its given label, which may differ
from the correct label z associated to x. Note that Z is a function of X , and Y is a random function of Z . By
instance-independent label noise, we mean that Pr(Y = y|Z = z, X = x) = Pr(Y = y|Z = z). A more
detailed treatment of DILNs can be found in Appendix A. In particular, a DILN includes its noise model.
3
Under review as a conference paper at ICLR 2021
airplane
automobile
bird
cat
deer
ΞB9BBR
■iD 值3
■ W
■■■序喝城
A dataset with unknown label
noise. By injecting label noise to
it, we synthesize different new
datasets and generate LID
sequences from them. Here We
used CIFAR-Wfor visualization.
The final estimate QD ∣s
computed as the average of
all intermediate estimates.
∖⅞k1l …⅞fc,⅛∕
Each dotted box represents a discriminator trained on a collection of LID sequences,
obtained via 3 random seeds. LID sequences generated from each random seed are
depicted as a plot. The three plots shown represent collectively the training data for a single
discriminator. Each well-trained discriminator produces a single intermediate estimate.
Figure 2: An overview of the proposed information-theoretic framework, using LID-based discrimi-
nators as a concrete example. Given an input dataset D with instance-independent label noise, several
collections of new datasets are synthesized from D. These synthesized datasets are “derived” via
different partial relabelings (and hence have different entropies). The synthesized datasets in each
collection are further processed to create training data for a single discriminator. In this illustration,
LID sequences from the synthesized datasets become the training data for each discriminator. Every
discriminator generates a single intermediate estimate for the noise transition matrix QD . The final
estimate is computed as the mean of all intermediate estimates.
A derived DILN of D shall mean a DILN D0 with noise model (Y0|Z; A) for some Y0 independent
of Z, such that both D, D0 have the same underlying set of instances, given in the same sequential
order. For example, D0 could be “derived” from D by inserting additional instance-independent label
noise, in which case D0 can be interpreted as a partial relabeling of D. For convenience, we say that
D0 is a Y 0-derived DILN of D.
2.2	SYNTHESIS OF NEW DATASETS FROM D
Let D be a DILN with noise model (Y |Z; A). Without loss of generality, assume A = {1, . . . , k},
assume that D has N instances, and write QD = [qi,j]1≤i,j≤k. The correct labels for the instances
are fixed and unknown to us, hence all entries of QD are fixed constants with unknown values. Our
goal is to estimate QD . As alluded to earlier, we shall be synthesizing two types of datasets from D .
The first type is What We call a baseline dataset, described as follows: Let D be a random dataset
obtained from D by replacing the given label of each instance of D by a label chosen uniformly at
random from A. Hence D is a randomDILN with expected entropy log k (i.e. maximum entropy),
which we shall denote by DmaX ：= E[D]. The noise transition matrix QD = [Qi,j] ι≤i,j≤k of D is a
random matrix whose entries Qij = 1 (1 + Eij) are random variables, where each “error” Ei,j is a
random variable with mean 0. Any observed D is called a baseline DILN of D.
The second type is what we call an α-increment dataset, where α = (α1, . . . , αk) is a vector whose
entries satisfy 0 ≤ αi ≤ 1 for all i. Let Dα be obtained from D as follows: For each 1 ≤ i ≤ k,
select uniformly at random αi × 100% of the instances with given label i, and reassign each selected
given label to one of the remaining k - 1 classes, chosen uniformly at random. Hence Dα is a random
DILN, and its noise transition matrix QDα = [Q0i0,j]1≤i,j≤k is a random matrix whose entries
Q0i0,j = qi,j (1 - αj) + X qi,tαt⅛ (1 + Ei0j) (for all 1 ≤ i,j ≤ k)	(1)
1≤t≤k
t6=j
are random variables, where each “error” Ei00,j is a random variable with mean 0. Any observed Dα
is called an α-increment DILN of D.
4
Under review as a conference paper at ICLR 2021
2.3	Underlying Intuition for distinguishing baseline DILNs
Suppose we have a discriminator Φ that is able to predict whether an input DILN is a baseline DILN
of D. We could try, as input to Φ, an α-increment DILN D0 of D for different values of α. For any
given α, we could try, as input to Φ, multiple observed values of the random DILN Dα . Intuitively,
the values of α for which “most” of the observed values for Dα are predicted by Φ to be baseline
DILNs, give non-trivial information about QD . In this subsection, we explain the underlying intuition
for how Φ can be trained, without requiring any knowledge of the correct labels Z.
Let Do be a baseline DILN, and let Di,..., d` be α-increment DILNs for α = α(1),..., α('),
respectively. Intuitively, if a(1),..., α(') cover a sufficiently wide range of vectors, then most of
the DILNs among Di,..., d`, would have entropies that are not near maximum entropy and hence
would (in principle) be distinguishable from baseline DILNs. Ideally, we would want to train a
discriminator Φ using baseline DILNs as “positive” data, and α-increment DILNs as “unlabeled”
data, via some positive-unlabeled learning algorithm. However, having entire datasets as training
data (i.e. where each DILN is a single datapoint for training Φ) may not necessarily be a good
representation for the training data. Hence, we introduce the notion of a separable random function
g, where effectively, we shall use g to generate the training data for Φ (by applying g on the DILN).
Our definition for the “separability” of g relies on (a suitable analog of) the asymptotic equipartition
property (AEP) from information theory, which is a key ingredient in proving Shannon’s channel
coding theorem. This AEP can be interpreted as a rigorous formulation of the idea that a “typical”
sequence of observed values for r i.i.d. random variables would belong to a tiny fraction of all
possible sequences of observed values, when r is sufficiently large. This notion of “typicality” can be
explained with the example of flipping biased coins. Suppose we have two coins, with probabilities 0.1
and 0.6 respectively, for getting heads. Toss each coin a total of r times, and record the corresponding
sequence of outcomes. Now repeat the process multiple times to get multiple sequences, each of
length r. If r is sufficiently large, then with high probability, a randomly selected sequence for the
first (resp. second) coin would have heads for ≈ 0.1 (resp. ≈ 0.6) of the outcomes; such sequences
form only a vanishingly small fraction of all possible sequences. Hence, if we repeatedly generate
these sequences, then with high probability, we would be able to distinguish our two coins.
We define g to be an Rd-valued random function. If g(D0) is invoked r times, then we get a randomly
generated sequence of length r, where each entry is a vector in Rd ; this resulting output sequence
shall be treated as a single datapoint in the “positive” class, for training Φ. Analogously, for each
1 ≤ i ≤ `, we shall invoke g(Di) a total of r times, and treat the resulting output sequence (of length
r) to be a single “unlabeled” datapoint for training Φ. We repeat this process to generate our training
data for Φ. If r is sufficiently large, then with high probability, each output sequence (datapoint for
Φ) would be a typical sequence, whose statistics is based on the input DILN. Informally, we define g
to be “separable” if distinct DILNs have distinguishable typical sequences for sufficiently large r.
2.4	DISCRIMINATORS FOR D AND ESTIMATORS FOR QD
We now formalize our intuition presented in Section 2.3. Let D[D] be the set of all derived DILNs of
D, and let U[D] be the set of all possible underlying datasets for D[D] (i.e. we throw away information
about the associated noise models). For notational ease, a DILN D0 could be an element of either
D[D] or U[D]. Every D0 in U[D] is uniquely determined by its sequence of given labels (yi, . . . , yN),
which we call the labeling of D0. We think of (yi, . . . , yN) as a relabeling of D, generated from
some (possibly unknown) noise model (Y0|Z; A). Hence, U[D] is a finite set of size kN. Formally,
a random derived DILN of D is a discrete random variable V : D[D] → U[D], defined on some
distribution on D[D]. We shall also define a map fmatrix with domain D[D], given by D0 7→ QD0 .
A discriminator for D is a prediction model Φ that takes any D0 in U[D] as its input and gives a score
Φ(D0) in [0, 1], which could be interpreted as the likelihood that D0 is a baseline DILN of D. We say
that D0 is predicted “positive” if Φ(D0) ≥ 0.5, and predicted “negative” otherwise. Let Φ+ denote
the subset of U[D] on which Φ predicts positive.
What is a “typical” value of fmatrix (D)? Notice that all kN possible relabelings of D could occur as the
labeling of a randomly generated baseline DILN of D, so the set of possible outcomes for fmatrix(D)
is the entire set of all possible noise transition matrices. Intuitively, we know for example that the
5
Under review as a conference paper at ICLR 2021
identity matrix is a “non-typical" value for fmatrix (D), even though its occurrence is possible. We
shall adapt the notion of typical sets from information theory to capture this intuition of “typicality”.
Definition 2.1. Given V a random derived DILN of D, let V1, V2, . . . be an infinite sequence of i.i.d.
random derived DILNs of D, with the same distribution as V .
(i)	For any > 0 and integer n ≥ 1, the n-fold ε-typical set of V is defined to be the set Λ(εn)(V )
consisting of all sequences (D1, . . . , Dn) ∈ U[D]n of observed values of V1, . . . , Vn, with
the property
E[H(V)] — ε ≤ 1 X H(Dt) ≤ E[H(V)]+ ε.	(2)
1≤t≤n
(ii)	Consider an arbitrary function g : U[D] → Rd. Note that each g(Vi) is an Rd-valued random
variable. For any > 0 and integer n ≥ 1, the n-fold ε-typical set of g(V) is defined to be
the set Λεn)(g(V)) consisting of all sequences (uι,..., Un) ∈ Rd X …X Rd = Rdn of
observed vectors of g(V1), . . . , g(Vn), with the property
E[g(V)]-ε1d ≤ 1 X Ut ≤ E[g(V)] + ε1d.
1≤t≤n
Remark 2.2. Given a (non-random) derived DILN D0 ofD, and an Rd-valued random function g, we
could treat g(D0 ) equivalently as a composition of a random derived DILN of D with an Rd-valued
(non-random) function. Hence for any > 0 and integer n ≥ 1, in view of Definition 2.1(ii), the
notion of an n-fold ε-typical set of g(D0) is well-defined.
Definition 2.3. Let D0 ∈ U[D], and let g be an Rd-valued random function on U[D]. We say that g
is D0-separable if for every ε > 0, δ > 0, and every D1 ∈ U[D] satisfying |H(D0) - H(D1)| > δ,
there exists some sufficiently large n such that Λ(εn) (g(D0)) and Λ(εn) (g(D1)) are disjoint typical
sets. We say that g is separable if g is D0-separable for all D0 ∈ U[D].
Definition 2.4. Let β > 0, let D0 ∈ U[D], and let g be a D0-separable Rd-valued random function
on U[D]. We say that a discriminator Φ for D is trained n-fold on (D0, g) with threshold β, if
Φ+ = {D0 ∈ U[D] ： Λβn(g(Do)) ∩ Λ^(g(D0)) = 0}
Definition 2.5. An α-sequence for D is a (finite or infinite) sequence (α(1), α(2), . . . ) of distinct vec-
tors in [0, k-1 )k that satisfies α(i) ≤ α(j) (coordinate-wise inequality) for all i < j. An α-sequence
is called valid if it is a (possibly finite) subsequence of an infinite α-sequence (α(1), α(2), . . . ) whose
set of elements {α(i)}∞=1 is a dense subset of [0, k-1 ]k.
Our estimator for QD relies on the existence of a separable random function g on U[D]. Once we find
such a g, we can then train multiple discriminators Φ using multiple randomly generated baseline
DILNs of D, to get multiple intermediate estimates for QD . We use each discriminator Φ to find a
suitable α ∈ [0, 1]k such that Φ gives a high score for a “typical” α-increment DILN of D. We shall
then use this value α to compute an intermediate estimate for QD . The average of these intermediate
estimates is our final estimate QD for QD; see Algorithm 1. More details are found in Appendix B.
Theorem 2.6. Let QD be the final averaged output matriXfrom Algorithm 1, which takes as its inputs
integers r, m, n, ` ≥ 1, a threshold β > 0, a separable Rd-valued random function on U[D], and a
valid α-sequence Ω = (α(1),..., α(`)) for D. Then QD converges in probability to QD as r → ∞,
m → ∞, n → ∞, ` → ∞, and N → ∞.
Informally, the input integers r, m, n, ` can be interpreted as follows: ` is the length of the input
α-sequence; m is the number of baseline DILNs generated; r is the length of the sequences that each
discriminator Φ is trained on; and n is the number of observed values of Dα (for some optimal α
contained in the input α-sequence) that are predicted positive by Φ.
Theorem 2.6 tells us that our estimator (i.e. Algorithm 1) is consistent; see Corollary B.16 in the
appendix for a more refined statement. Roughly speaking, our proof of Theorem 2.6 involves a
careful iterated use of typical sequences, and requires an analog of joint AEP for DILNs, as well as a
notion of “transverse entropy”, which has no corresponding analog in the usual notion of entropy for
random variables (see Appendix B.1). Although our consistency result requires the limit N → ∞
(recall that N is the number of instances in D), a careful analysis of our proof reveals that for fixed N
(with r → ∞, m → ∞, n → ∞, ` → ∞), we have an explicit upper bound on the estimation error
of our estimator; see Corollary B.15. In the next section, we shall introduce a suitable candidate for g.
6
Under review as a conference paper at ICLR 2021
Algorithm 1 An overview for the general framework for estimating QD
Require: integers r,m,n,' ≥ 1.
Require: threshold β > 0, g: a separable Rd-valued random function on U[D]
Require: Ω = (α(1),..., α(')) ⊆ [0,1]k a valid a-sequence for D.
1:	Initialize empty list L.
2:	for ς = 1 . . . m do
3:	Generate observed value D = D0ς).
4:	for s = 1 . . . ` do
5:	Generate n independent observed values Dα(s) = Ds(ς,1), Ds(ς,2), . . . , Ds(ς,n) .
6:	Let Φς be a discriminator trained r-fold on (D0(ς), g) with threshold β.
7:	Compute s0 := min{s : 1 ≤ s ≤ `, there exists 1 ≤ t ≤ n such that Ds(ς,t) ∈ Φς+}.
8:	if s0 exists (i.e. s0 is well-defined) then
9:	for t = 1 . . . n do
10:	Generate observed values for random variables Ei0,j , Ei0,0j (for all 1 ≤ i, j ≤ k)
#[Note: E0,j, E0j are defined in QD, QDa / respectively.]
11:	Solve system of linear equations QD = QDa ' (in the k2 variables qi,j for 1 ≤ i,j ≤ k)
#[Note: We substitute the generated observed values for E0,j, E00j into QD = QDa z.]
#[Note: Unique solution to linear system exists almost surely; more details in Appendix B.2.]
12:	if Unique solution to linear system exists then
13:	QlG J [qi,j]1≤i,j≤k, where {qi,j = ^i,j}i,j is the unique solution to the linear system.
(ς)
14:	Insert matrix Qt into list L
15:	return mean of matrices in L (This is our estimate QD for QD.)
3	Realization of framework using LID-based discriminators
A key challenge for realizing our framework is the construction of good discriminators. This requires
a suitable separable random function g. The underlying intuition for g we should have is that we
want g to “separate” datasets with different noise levels. Appendix B.6 elaborates on this intuition.
With this intuition in mind, we propose to use Local Intrinsic Dimensionality (LID) scores (Houle,
2013; 2017a;b). The LID score is used in several applications (Amsaleg et al., 2017; Von BrUnken
et al., 2015; Schubert & Gertz, 2017), and it plays a fundamental role in a 2018 breakthrough in noise
detection: It is possible to determine whether a dataset is clean or has label noise, by considering
LID sequences (Ma et al., 2018b), which are sequences of LID scores; cf. Amsaleg et al. (2015). In
particular, it is possible for LID sequences to detect adversarial noise (Ma et al., 2018a).
LID scores are assigned to every training epoch. As observed in Ma et al. (2018b), the LID score
of a model has an initial phase: It would start “high” and then generally decrease to a “low” value.
Subsequently, its behavior depends on the amount of label noise in the dataset. In the absence of
label noise, the LID score would remain low. If instead there is “significant” label noise, then the
LID score would rise (after its initial decrease). Thus, the presence of label noise in a dataset could in
principle be detected by any sharp increase in the LID score during the training phase. In this paper,
we use LID sequences as a proxy for measuring the entropy of the underlying dataset trained on.
Consider a neural network N. Suppose D0 ∈ U[D], and let x1, . . . , xN be the enumeration of all
instances of D0 . As we train our neural network on D0 , we shall keep track of how the feature vectors
of randomly selected instances evolve over the training epochs. Given an input instance x, the feature
vector of x shall mean the output vector of the last hidden layer of N , given the input x; we shall
denote the feature vector of X in epoch j by ωj(x), and we shall define Ωj := {ωj(Xi)}ι≤i≤N.
Let s, s0 ≥ 1 be fixed integers. The LID score of a single instance x in epoch j is defined by
LIDj (x; D0) := - (S X (log ri(x) - log %(x))),
1≤i≤s
where ri(x) is the Euclidean distance between ωj(x) and its i-th nearest neighbor in Ωj. The LID
score ofD0 in epoch j, denoted by LIDj (D0), is the mean LID scores ofs0 randomly selected instances
7
Under review as a conference paper at ICLR 2021
in epoch j. If training is done over L epochs, then LID(D0) := (LID1 (D0), . . . , LIDL (D0)) is the
LID sequence of D0. We then define the random function gLID : U[D] → RL by D0 7→ LID(D0).
An LID-based discriminator is a discriminator Φ trained on LID sequences as its training data. For
every synthesized baseline DILN D0 of D, we shall invoke gLID (D0) a total ofr times, which yields r
LID sequences that shall be considered “positive”. For each α ∈ [0, 1]k and each α-increment DILN
D00 of D, we similarly invoke gLID(D00) a total of r times, which yields r LID sequences that shall be
considered “unlabeled”. Hence, we can generate training data for Φ consisting of “positive” samples
and “unlabeled” samples. We could then use any positive-unlabeled learning algorithm to train Φ.
CIFAR-10 AP Removal	0%	Sym-20% 40%	70%	0%	Sym-50% 40%	70%	0%	Sym-80% 40%	70%	Sym (averaged)		
										0%	40%	70%
S-model	0.0225	0.0427	0.1381	0.4886	0.4014	0.3177	1.5650	1.3957	1.2215	0.6920	0.6133	0.5591
Forward	0.0865	0.2116	0.4999	0.0873	0.2490	0.2735	0.1617	0.1720	0.5589	0.1118	0.2109	0.4441
T-Revision	0.0869	0.2896	0.6206	0.1459	0.2425	0.3159	0.2303	0.2765	0.2267	0.1544	0.2695	0.3877
ours-1	0.0573	0.2171	0.2907	0.0268	0.1802	0.4927	0.1355	0.1022	0.1742	0.0732	0.1665	0.3192
MPEIA	0.8898	1.1730	1.4105	0.3322	0.5176	0.6085	0.0187	0.0494	0.0909	0.4136	0.5800	0.7033
ours-2	0.7133	0.8999	1.0454	0.2752	0.3987	0.4585	0.0121	0.0365	0.1225	0.3335	0.4450	0.5421
GLC	0.1966	0.3922	1.0582	0.1444	0.2783	0.8438	0.0598	0.0907	0.2365	0.1336	0.2537	0.7128
ours-3	0.1397	0.3282	1.1580	0.0992	0.2037	0.9991	0.0864	0.1012	0.2991	0.1084	0.2110	0.8187
Table 1: Forward KL loss comparisons for symmetric noise matrix estimations with CIFAR-10 as the
underlying clean dataset. “AP” means anchor point. We removed anchor-like data up to 70% in the
same manner described in (Xia et al., 2019). An average loss reduction is achieved from 0.27 to 0.17
for 40% anchor-like instances removal when comparing with baselines not using clean samples. We
also improved MPEIA and GLC by using their estimates as priors. Smaller loss values are bold-faced.
CIFAR-10 90% removal	Pairwise-20%			Pairwise-45%			Pairwise-80%			Pairwise (averaged)		
	no	hardest	easiest	no	hardest	easiest	no	hardest	easiest	no	hardest	easiest
S-model	0.5156	0.5568	0.5019	1.2443	1.2016	1.2104	2.6906	2.5212	2.6777	1.4835	1.4265	1.4633
Forward	0.0901	0.0982	0.1128	1.5657	1.0621	1.8904	8.2493	8.7039	9.1303	3.3017	3.2881	3.7112
T-Revision	0.0723	0.0870	0.1356	0.9283	0.6337	0.7656	5.7647	5.7378	5.2833	2.2551	2.1528	2.0615
ours-1	0.3644	0.5334	0.4093	0.8332	0.7364	0.7770	2.6957	1.8657	2.0011	1.2978	1.0452	1.0625
MPEIA	0.5854	0.5741	0.6139	0.5477	0.5814	0.5868	0.6006	0.6123	0.6507	0.5779	0.5893	0.6171
ours-2	0.3881	0.5499	0.4344	0.5528	0.5505	0.5751	0.6771	0.4924	0.4738	0.5393	0.5309	0.4944
GLC	0.2637	0.2545	0.2591	0.2752	0.2896	0.2666	0.2928	0.2588	0.2653	0.2772	0.2676	0.2637
ours-3	0.1967	0.3754	0.1867	0.3544	0.3294	0.3037	0.2015	0.2751	0.2169	0.2509	0.3266	0.2358
Table 2: Forward KL loss comparisons for pairwise noise transition matrix estimations with CIFAR-
10 as the underlying clean dataset. In the table, “no” means no sample removal, “hardest” and “easiest”
mean a 90% random sample removal from the hardest class, cat, and the easiest class, frog. We
constantly perform the best for averaged noise levels. Smaller loss values are bold-faced.
4	Experiments
Framework implementation details. Let α(1),..., α(') be a sequence of vectors in [0,1]k. Let
D0 be a baseline DILN of D, and for each 1 ≤ s ≤ `, let Ds be an α(s)-increment DILN of D. If
Do, Di,..., d` are synthesized using a common random seed ς, then We say that {Do, Di,..., D'}
is a seed collection with seed ς and α-sequence (α(1),..., α(')). In our experiments, we used a fixed
α-sequence Ω, where each α = (αι,..., ak) in Ω satisfies ai ≤ 0.886 for all i for CIFAR-10 and
αi ≤ 0.916 for all i for Clothing1M. We trained each discriminator Φ on the LID sequences obtained
from three different seed collections (called a “triple”), where two of them are used for training and
the third is used for validation. Our LID-based discriminator Φ is trained using positive-unlabeled
bagging (Elkan & Noto, 2008; Mordelet & Vert, 2014), with decision trees as our sub-routine. We
used 1000 decision trees. For each derived DILN, we generated 50 LID sequences to be used as
training data. Once trained, our discriminator Φ assigns a score to each input DILN D0 based on
voting: Again, 50 LID sequences are generated for D0 . Each LID sequence is predicted either positive
or negative by Φ, and the total number of positive votes, divided by 50, is the final score assigned
to D0. After training, if the validation recall is τ ≥ 0.9, then the discriminator Φ would be further
fine-tuned. Details on fine-tuning can be found in Appendix C.2.2.
Datasets. We did experiments on CIFAR-10 (Krizhevsky et al., 2009) and Clothing1M (Xiao et al.,
2015). CIFAR-10 has 50, 000 training and 10, 000 test images over 10 classes. We manually added
8
Under review as a conference paper at ICLR 2021
two types of instance-independent label noise: symmetric and pairwise, following the label flip
settings in (Han et al., 2018). For symmetric noise, we used noise levels 20%, 50% and 80%, while
for pairwise asymmetric noise, we used noise levels 20%, 45% and 80%. Clothing1M (Xiao et al.,
2015) has around 1 million clothing images of 14 classes. The paper (Xiao et al., 2015) also provides
a noisy subset, whose corresponding QD has been manually verified exactly. We estimate its QD
based on this subset for real-life label noise scenario and refer this subset as “Clothing1M subset”.
Methods. We compared our method with baselines: (i) S-model (Goldberger & Ben-Reuven, 2016),
which concatenates a neural network (NN) with an extra softmax layer; (ii) Forward (Patrini et al.,
2017), which trains an NN and uses anchor-like instances to estimate QD ; (iii) T-Revision (Xia
et al., 2019), which finetunes QD concurrently with the training of its classifier; (iv) MPEIA (Yu
et al., 2018), which estimates mixture proportion by a fraction of Dclean, for QD; and (v) Gold
Loss Correction (GLC) (Hendrycks et al., 2018), which trains an NN on the noisy data. Then the
trained NN computes softmax outputs of Dclean for QD . S-model, Forward and T-Revision do not
require Dclean while MPEIA and GLC randomly selects 0.5% Dclean from the whole dataset (in our
paper). NN structure, training losses and training epochs can be found in Table ?? and Table 6 (in the
appendix) for CIFAR-10 and Clothing1M, respectively. We used the same training hyper-parameters
and data augmentation settings as given in Patrini et al. (2017), except T-Revision, which follows the
settings in Xia et al. (2019) for respective datasets. Ours-1 took a similar approach as GLC without
Dclean to obtain prior (we call this prior “avg prior”). We first split the dataset into a 90% training
set and a 10% validation set randomly. We then train an NN to compute probability vectors of the
whole noisy dataset. The probability vectors of the samples with label i from the epoch with the best
validation accuracy is averaged as the ith row of prior. Ours-2 (resp. ours-3) used MPEIA’s (resp.
GLC’s) estimates as priors2. All values reported are averaged over at least 5 estimates.
Experimental Results. We row-normalized all estimates from the baselines then evaluated them
using (forward) KUllback-Leibler (KL) loss3 For CIFAR-10's both symmetric and pairwise cases,
even for 70% anchor-point removal or imbalanced class ratios, our method has the lowest losses for
averaged noise levels, compared to all the baselines, and made improvements when MPEIA and GLC
are used as our priors. For Clothing1M, ours-1 has the lowest KL loss, 0.4903, slightly better than
T-Revision with a KL loss of 0.5262. S-model ranked the last. Among all methods that used 0.5%
clean samples, ours-3 achieved the lowest loss, 0.5311, while its prior GLC has a loss of 0.5957.
Methods	S-model	Forward	T-Revision	ours-1	MPEIA	ours-2	GLC	ours-3
Forward KL loss	2.1189	1.1098	0.5262	0.4903	1.7408	1.8344	0.5957	0.5311
Table 3: Forward KL loss comparisons for Clothing1M subset. Ours-1 has the lowest forward KL
loss among all baseline models. When 0.5% Dclean is used, we (ours-3) improved GLC.
5	Concluding Remarks
This paper focuses on datasets with instance-independent label noise (DILNs), and tackles the
problem of estimating the noise transition matrix QD of a DILN D. Our main algorithm is the first-
ever estimator for QD that is proven to be consistent without needing to optimize the classification
accuracy of a classifier trained on D. Notably, we do not require clean data or anchor-like instances,
and we do not make any assumptions on the structure of QD . Thus, a key “takeaway insight” is that
QD could be accurately estimated in a wide range of scenarios, including possibly for classification
tasks that are “inherently still difficult” to get high classification accuracies even without label noise.
Our consistent estimator is based on a new information-theoretic framework, in which we introduce
the notion of entropy for DILNs. A key step in our approach is the training of discriminators to
predict whether an input DILN has maximum entropy. Our proof of consistency relies crucially on
the notion of “typicality” and the asymptotic equipartition property from information theory.
2Both MPEIA and GLC inherently require Dclean. Since our method does not leverage clean data, it would not
be fair to directly evaluate our method against them. Instead, ours-2 and ours-3 are intended to show that MPEIA
and GLC can be enhanced with our method, without having to do further clean data annotation/augmentation.
3If QD = [qi,j]ι≤i,j≤k (resp. QD = [qi,j]ι≤i,j≤k) is the estimated (resp. true) noise transition matrix for
D, then the corresponding (forward) KL loss is defined to be Pk=ι PZ(i) Pk=I qi,j log (qi,j∕qi,j).
9
Under review as a conference paper at ICLR 2021
References
LaUrent Amsaleg, Oussama Chelly, Teddy Furon, StePhane Girard, Michael E Houle, Ken-ichi
Kawarabayashi, and Michael Nett. Estimating local intrinsic dimensionality. In Proceedings of
the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, PP.
29-38, 2015.
Laurent Amsaleg, James Bailey, Dominique Barbe, Sarah Erfani, Michael E Houle, Vinh Nguyen,
and Milos Radovanovic. The vulnerability of learning to adversarial perturbation increases with
intrinsic dimensionality. In 2017 IEEE Workshop on Information Forensics and Security (WIFS),
pp. 1-6. IEEE, 2017.
Jiacheng Cheng, Tongliang Liu, Kotagiri Ramamohanarao, and Dacheng Tao. Learning with bounded
instance-and label-dependent label noise. stat, 1050:12, 2017.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Charles Elkan and Keith Noto. Learning classifiers from only positive and unlabeled data. In
Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data
mining, pp. 213-220, 2008.
Marylou Gabrie, Andre Manoel, Clement Luneau, jean barbier, Nicolas Macris, Florent
Krzakala, and Lenka Zdeborova. Entropy and mutual information in models of deep
neural networks. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-
Bianchi, and R. Garnett (eds.), Advances in Neural Information Processing Systems 31,
pp. 1821-1831. Curran Associates, Inc., 2018. URL http://papers.nips.cc/paper/
7453-entropy-and-mutual-information-in-models-of-deep-neural-networks.
pdf.
Jacob Goldberger and Ehud Ben-Reuven. Training deep neural-networks using a noise adaptation
layer. 2016.
Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi
Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. In
Advances in neural information processing systems, pp. 8527-8537, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,
pp. 770-778, 2016.
Dan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train
deep networks on labels corrupted by severe noise. In Advances in neural information processing
systems, pp. 10456-10465, 2018.
M.	E. Houle. Dimensionality, discriminability, density and distance distributions. In 2013 IEEE
13th International Conference on Data Mining Workshops, pp. 468-473, Dec 2013. doi: 10.1109/
ICDMW.2013.139.
Michael E. Houle. Local intrinsic dimensionality i: An extreme-value-theoretic foundation for
similarity applications. In Christian Beecks, Felix Borutta, Peer Kroger, and Thomas Seidl (eds.),
Similarity Search and Applications, pp. 64-79, Cham, 2017a. Springer International Publishing.
Michael E. Houle. Local intrinsic dimensionality ii: Multivariate analysis and distributional support.
In Christian Beecks, Felix Borutta, Peer Kroger, and Thomas Seidl (eds.), Similarity Search and
Applications, pp. 80-95, Cham, 2017b. Springer International Publishing.
Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learning deep networks from noisy labels with
dropout regularization. In 2016 IEEE 16th International Conference on Data Mining (ICDM), pp.
967-972. IEEE, 2016.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Tongliang Liu and Dacheng Tao. Classification with noisy labels by importance reweighting. IEEE
Transactions on pattern analysis and machine intelligence, 38(3):447-461, 2015.
10
Under review as a conference paper at ICLR 2021
Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi N. R. Wijewickrema, Michael E. Houle,
Grant Schoenebeck, Dawn Song, and James Bailey. Characterizing adversarial subspaces using
local intrinsic dimensionality. In ICLR 2018, 2018a. URL http://arxiv.org/abs/1801.
02613.
Xingjun Ma, Yisen Wang, Michael E. Houle, Shuo Zhou, Sarah Erfani, Shutao Xia, Sudanthi
Wijewickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In Jennifer Dy
and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
volume 80 of Proceedings of Machine Learning Research, pp. 3355-3364, Stockholmsmassan,
Stockholm Sweden, 10-15 JUl 2018b. PMLR. URL http://Proceedings .mlr.ρress/
v80/ma18d.html.
Xingjun Ma, Yisen Wang, Michael E Houle, Shuo Zhou, Sarah Erfani, Shutao Xia, Sudanthi Wijew-
ickrema, and James Bailey. Dimensionality-driven learning with noisy labels. In International
Conference on Machine Learning, pp. 3355-3364, 2018c.
Fantine Mordelet and J-P Vert. A bagging svm to learn from positive and unlabeled examples. Pattern
Recognition Letters, 37:201-209, 2014.
Nagarajan Natarajan, Inderjit S Dhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learning with
noisy labels. In Advances in neural information processing systems, pp. 1196-1204, 2013.
Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making
deep neural networks robust to label noise: A loss correction approach. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 1944-1952, 2017.
Michael D. Perlman. Jensen’s inequality for a convex vector-valued function on an infinite-
dimensional space. Journal of Multivariate Analysis, 4(1):52 - 65, 1974. ISSN 0047-259X.
doi: https://doi.org/10.1016/0047-259X(74)90005-0. URL http://www.sciencedirect.
com/science/article/pii/0047259X74900050.
Harish Ramaswamy, Clayton Scott, and Ambuj Tewari. Mixture proportion estimation via kernel
embeddings of distributions. In International Conference on Machine Learning, pp. 2052-2060,
2016.
Andrew Michael Saxe, Yamini Bansal, Joel Dapello, Madhu Advani, Artemy Kolchinsky, Bren-
dan Daniel Tracey, and David Daniel Cox. On the information bottleneck theory of deep
learning. In International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=ry_WPG-A-.
Erich Schubert and Michael Gertz. Intrinsic t-stochastic neighbor embedding for visualization and
outlier detection. In International Conference on Similarity Search and Applications, pp. 188-203.
Springer, 2017.
Clayton Scott. A rate of convergence for mixture proportion estimation, with application to learning
from noisy labels. In Artificial Intelligence and Statistics, pp. 838-846, 2015.
Ravid Shwartz-Ziv and Naftali Tishby. Opening the Black Box of Deep Neural Networks via
Information. arXiv e-prints, art. arXiv:1703.00810, March 2017.
Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiyoharu Aizawa. Joint optimization framework
for learning with noisy labels. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pp. 5552-5560, 2018.
Kiran K Thekumparampil, Ashish Khetan, Zinan Lin, and Sewoong Oh. Robustness of conditional
gans to noisy labels. In Advances in neural information processing systems, pp. 10271-10282,
2018.
N.	Tishby and N. Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE
Information Theory Workshop (ITW), pp. 1-5, 2015.
Robert A Vandermeulen, Clayton D Scott, et al. An operator theoretic approach to nonparametric
mixture models. The Annals of Statistics, 47(5):2704-2733, 2019.
11
Under review as a conference paper at ICLR 2021
Jonathan Von Brunken, Michael E Houle, and Arthur Zimek. Intrinsic dimensional outlier detection
in high-dimensional data. NII Technical Reports, 2015(3):1-12, 2015.
Xiaobo Xia, Tongliang Liu, Nannan Wang, Bo Han, Chen Gong, Gang Niu, and Masashi Sugiyama.
Are anchor points really indispensable in label-noise learning? In Advances in Neural Information
Processing Systems, pp. 6835-6846, 2019.
Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy
labeled data for image classification. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 2691-2699, 2015.
Xiyu Yu, Tongliang Liu, Mingming Gong, Kayhan Batmanghelich, and Dacheng Tao. An efficient
and provable approach for mixture proportion estimation using linear independence assumption. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4480-4489,
2018.
12
Under review as a conference paper at ICLR 2021
Appendix
This appendix is organized as follows:
•	Section A gives a detailed treatment of datasets with instance-independent label noise (DILNs).
•	Section B proves the consistency of our proposed estimator.
•	Section C provides all the implementation details of our experiments.
•	Section D describes how our work relates to the information bottleneck theory for deep learning.
A A rigorous formalism for DILNs
A dataset D is a set consisting of N (“instance”, “given-label”) pairs. If we enumerate these pairs by
(x1, y1), (x2, y2), . . . , (xN, yN), then D is the set of pairs {(xi, yi)}1≤i≤N. Across all disciplines
that deal with datasets, there is an implicit assumption that the instances of a dataset are sampled from
some “true” data distribution. Formally, each instance x is an observed value of some random variable
Xtrue. For classification tasks, given an instance Xtrue = x, it is assumed that there is a uniquely
determined correct label yx associated to this instance x. Hence, there is a function f true that assigns
each instance x to its correct label yx . We assume that f true is completely deterministic and does
not involve any randomness. If Ztrue is a random variable representing the correct label of a random
instance Xtrue, then Ztrue = f true (Xtrue). Because we are only given the datapoints of D, we typically
consider the restriction of the domain of f true to the instances of D; we denote this restricted function
by ftDrue. In the absence of label noise, the dataset becomes precisely D = {(xi, yxi)}1≤i≤N. For such
“noise-free” datasets D, the goal of learning a classifier from D is to obtain a good approximation
ftrue for the function ftD).4
When instance-independent label noise is added to the dataset, there is a random function fnoise that
is applied to the correct labels yx of all instances x. This random function fnoise depends only on
the input label z, and does not depend on which instance x this label is for. Similar to the case of
ftrue, because we are only given the datapoints of D, we again typically consider the restriction of
the domain of fnoise to the instances of D; we denote this restricted function by fnDoise . Assuming that
the set of all possible labels is A = {1, . . . , k}, this implies that fnDoise can be decomposed into k
random variables Y1 , . . . , Yk , where each Yi is a discrete random variable taking on values in A. The
set of probabilities {pYi (1), . . . ,pYi (k)} would completely determine the distribution of Yi. Thus,
fnDoise is a function on A given by the map i 7→ Yi . This is a random function that is completely
determined by the set of all k2 probabilities {pYi (j))}1≤i,j≤k. The k-by-k matrix Q = [qi,j]1≤i,j≤k
whose (i,j)-th entry equals pYi (j) is precisely the noise transition matrix that we consider in our
paper, which we have denoted by QD . Consequently, to “learn” a model of this instance-independent
label noise is to obtain a good approximation fnoise for fDise，which is exactly the same as finding a
good approximation QD to the noise transition matrix QD .
Our goal for this paper is to estimate the noise transition matrix QD from a given dataset D. Thus, one
of our main assertions for this paper, that “high classification accuracy is not required for estimating
QD well"，can be interpreted as the assertion that We can find a good approximation fnoise for fDise,
even if we are unable to find a good approximation for ftDrue, or a good approximation for fnDoise ◦ ftDrue.
Notice that for existing methods that “learn in the presence of label noise”, their underlying goal is to
find a good approximation for either ftDrue or the composite map fnDoise ◦ ftDrue . In contrast, our goal is
to find a good approximation for fnDoise.
Given D a dataset with instance-independent label noise (DILN), let X be a random variable
representing an instance ofD selected uniformly at random (notice that X 6= Xtrue), let Z := ftDrue(X),
and let Y := fnDoise(Z). By definition, fnDoise is completely determined by the conditional distribution
of Y given Z, and the set of all possible labels A; in particular, fnDoise does not depend on X or ftDrue.
This explains why the noise model associated to D is denoted by (Y |Z; A). In our paper, we have 4
4Of course, the purpose for learning a classifier from D is to learn a good representation for ftrue, and we
can only do so given the dataset D. If the underlying distribution of D is not a “good representation” of the
distribution of Xtrue, then any approximation to ftDrue, no matter how accurate, will not approximate f true well.
Henceforth, we assume that D has a “good underlying representation” for the distribution of Xtrue.
13
Under review as a conference paper at ICLR 2021
defined a DILN to be a set. It is the set D = {(xi , yi)}1≤i≤N, which also has an associated noise
model (Y |Z; A).
Strictly speaking, a DILN should be defined as a triple (D, Y |Z, A), since we also include the
information about the noise model (Y |Z; A) as part of the definition of the DILN. However, for the
purpose of this paper, we abuse notation (slightly) and assume that a DILN D includes its associated
noise model. To avoid ambiguity, the set consisting of all (“instance”, “given-label”) pairs (xi , yi)
shall be called the underlying dataset of D, and each such (xi , yi ) pair shall be called a datapoint
of D. Of course, we are implicitly assuming that the instances x1, . . . , xN are sampled from some
“true” distribution (i.e. the distribution of Xtrue), but we do not need any information involving Xtrue
beyond this implicit assumption.
B	PROOF OF CONSISTENCY OF PROPOSED ESTIMATOR FOR QD
The goal for this section is to prove Theorem 2.6, i.e. that our proposed estimator for QD is consistent;
see Theorem B.14 for a precise (equivalent) formulation of Theorem 2.6. Our consistency proof is
essentially an iterated application of suitable analogs of the asymptotic equipartition property (AEP)
theorem from information theory. In particular, we will prove a joint AEP theorem for DILNs; see
Theorem B.3. The proof of Theorem B.14 requires some preparation, so we shall first prove several
related results in Sections B.1-B.3, before We present Theorem B.14 in Section B.4.
Throughout, let D be a DILN with noise model (Y |Z; A). Without loss of generality, assume that
A = {1, . . . , k} satisfies k ≥ 2, assume that D has N instances, and Write QD = [qi,j]1≤i,j≤k .
If {Xn}∞=ι is any sequence of random variables, then we write "Xn -→ μ“ to mean that "Xn
converges in probability to μ”. Here, μ could be a scalar or a random variable.
B.1 Joint asymptotic equipartition property for DILNs
Recall that in Definition 2.1, we introduced the notion of "n-fold ε-typical sets” for random derived
DILNs. We will also need to define jointly typical sets for random derived DILNs. This involves
what we shall call "transverse entropy”, which has no corresponding analog in the usual notion of
entropy for random variables.
Definition B.1. Let D0 and D00 be derived DILNs of D. The transverse entropy of D0 and D00 is
H(D0 ∧ D00) := - XpZ(i) X |qiD,j0 - qiD,j00| log |qiD,j0 - qiD,j00|.
(Recall: We use the convention that 0 log 0 = 0.)
Definition B.2. Let V and W be random derived DILNs of D. Let V1, V2, . . . (resp. W1, W2, . . . )
be an infinite sequence of i.i.d. random derived DILNs of D with the same distribution as V (resp.
W). For any > 0 and integer n ≥ 1, the n-fold jointly ε-typical set of V and W is defined to be
the set Λεn)(V, W) consisting of all sequences ((Dj,..., Dn), (D∖ ..., Dn)) ∈ U[D]n X U[D]n of
observed values of V1, . . . , Vn, W1, . . . , Wn, with the following properties:
1n
E[H(V)] — ε ≤ - EH(D0) ≤ E[H(V)] + ε;	(3)
n t=1
-n
E[H(W)] - ε ≤ — E H(D00) ≤ E[H(W)] + ε;	(4)
n t=1
-n
E[H(V ∧ W)] - ε ≤ — E H(D0 ∧ D：) ≤ E[H(V ∧ W)] + ε.	(5)
n t=1
In particular, notice that the restriction of Λ(εn)(V, W) ⊆ U[D]n × U[D]n to the first (resp. second)
U[D]n component is a subset of the n-fold ε-typical set of V (resp. W).
Theorem B.3 (cf. Cover & Thomas (2012, Thm. 7.6.1)). Let V and W be random derived DILNs
of D. Let V1, V2, . . . (resp. W1, W2, . . . ) be an infinite sequence of i.i.d. random derived DILNs of
14
Under review as a conference paper at ICLR 2021
D with the same distribution as V (resp. W), and suppose that D10 , D20 , . . . (resp. D100, D200, . . . ) is a
corresponding sequence of observed values. Then for any ε > 0,
n→∞ Pr (((D1,..., Dn ),(D10,..., D00)) ∈ Λεn (V,W)) =L
Proof. Consider an arbitrary ((DJ,..., DnJ,①{,..., Dn)) ∈ A：n)(V, W). By the weak law of
large numbers, nn Pn=I H(Dt) converges in probability to E[H(V)]. Hence, given any ε > 0, there
exists an integer n1 ≥ 1 such that for all integers n > n1,
n
Pr(∣-(XH(D0))- E[H(V)]∣ ≥ ε) ≤ 彳	⑹
t=1
By a similar argument, we also infer that given any ε > 0, there exists an integer n2 ≥ - such that
for all integers n > n2,
∣- n	∣	ε
Pr(∣- (XH(Dt) — E[H(W)]∣ ≥ε) ≤ ɪ,	⑺
t=1
and there exists an integer -3 ≥ - such that for all integers - > -3,
∣- n	∣	ε
Pr(∣- (XH(Dt ∧D00)) - E[H(V ∧ W)]∣ ≥ ε) ≤ -.	⑻
- t=1
Therefore, for all integers ≥ max{1, 2, 3}, the probability of the union of the events in (6), (7)
and (8) must be at most ε, which proves our assertion.	□
For the rest of this subsection, let D0 ∈ U[D], and let g be a D0-separable Rd-valued random function
on U[D]. Recall that for β > 0, a discriminator Φ for D is said to be trained r-fold on (D0, g) with
threshold β, if the set of positive predictions for Φ is
Φ+ = {D0 ∈ U[D] ： Λβr)(g(Do)) ∩ Λ(r)(g(D0)) = 0}.
Definition B.4. Let δ0 > 0, β > 0, let r ≥ - be an integer, and suppose Φ is a discriminator for D
that is trained r-fold on (D0, g) with threshold β. We say that Φ is δ0 -sufficient if every D0 ∈ U[D]
satisfying H(D0 ∧ D0) < δ0 is predicted positive, i.e. D0 ∈ Φ+.
Lemma B.5. For every β, δ > 0, there is a sufficiently large integer rβ,δ ≥ - such that for all
integers - ≥ rβ,δ, if Φ is a discriminator for D that is trained --fold on (D0, g) with threshold β,
then the following implication holds:
D0 ∈ Φ+ =⇒ |H(D0) - H(D0)| ≤ δ.
Proof. Consider any D1 ∈ U[D] that satisfies |H(D0) - H(D1)| > δ. Since g is D0-separable, it
follows from Definition 2.3 that there is an integer 吟、such that Λ,)(g(D0)) ∩ Λ,)(g(Dι)) = 0
for all integers - ≥ rβD,1δ . For each β and δ, define
rβ,δ := max{rβD,1δ ∈ Z : D1 ∈ U[D], |H(D0) - H(D1)| > δ}.
In particular, rβ,δ is well-defined, since U[D] is finite (of size kN).
Now, for any - ≥ rβ,δ, suppose that Φ is trained --fold on (D0, g) with threshold β. By Definition
2.4, this means that Φ+ = {D0 ∈ U[D] : Λ(βn)(g(D0)) ∩ Λ(βn) (g(D0)) 6= 0}. By the definition of rβ,δ,
we thus have Λ(βn)(g(D0)) ∩ Λ(βn) (g(D0)) = 0 for all D0 satisfying |H(D0) - H (D0)| > δ, which
then proves the assertion.	□
Lemma B.6. Let 0 < - ≤ -, and let h : [0, k - -] → R be a function given by
h(x) = k-1 (1 — k-1 )log(1 — k-ɪ) + 1 (1 + x)log(1 + x).	(9)
Then h(x) is strictly increasing, and h(-) > 0. Moreover, if ζ := min{pZ (i) : - ≤ i ≤ k} > 0, and
if H (D) ≥ log k 一 Zh(ε) ,then |qD- — ɪ | ≤ ε forall 1 ≤ i,j ≤ k.
15
Under review as a conference paper at ICLR 2021
Proof. First of all, we check that the derivative of h(x) is
h0(x) = -k hlog(1 - k-ι)- Iog(I+X)i = -1 [ log( ⅛F)_，
which satisfies h0(x) > 0 for all 0 < x < k - 1. (Recall that k ≥ 2 by assumption.) Thus, h(x) is
strictly increasing on the closed interval [0, k - 1]. In particular, ε > 0 implies that h(ε) > h(0) = 0.
Henceforth, assume ζ > 0, and suppose on the contrary that there exists some 1 ≤ i0, j0 ≤ k such
that ∣qD,j0 - 11 > ε. Let qD,j = 1 (1 + εj) for all 1 ≤ j ≤ k, and assume without loss of generality
that qD,jo = 1 (1 + εjo) for some 1 εj0 > ε. This implies that
-X qD,j logqD,j = - X 1 (I + ε)log [k(I + ε)]
1≤j≤k	1≤j≤k
j6=j0	j6=j0
= X 1(I + εj)logk - X 1(I + εj)log(1 + εj).
1≤j≤k	1≤j≤k
j 6=j0	j6=j0
Since Pjk=1 εj = 0, and since the map x 7→ -x log x is concave, it follows from Jensen’s inequality
(see Cover & Thomas (2012, Thm. 2.6.2)) that
-E qD,j logqD,j ≤ E 1(I+εj)logk - ⅛1 (1 - j)log (I- j). (IO)
1≤j≤k	1≤j≤k
j 6=j0	j6=j0
Note also that
-qD,j0 log qD,j0 = - k (I + εj0 )log h 1 (1 + εj0 )i
=1(1 + εj0) logk - I(I + εj0) Iog(I + εj0).	(II)
Summing (10) and (11) gives us
-X qD,j log qD,j ≤ 1 h X(1 + ε )log ki - k-1(1 - j) log(1 - j)- 1(1 + j )log(1 + j)
1≤j≤k	1≤j≤k
= log k - k-1 (1 - j )log(1 - j ) - 1 (1+ εjο )log(1+ εjο )
= log k - h(εj0 ).	(12)
Note that qD,jo = 1 + 1 εj0 ≤ 1 implies εj0 ≤ k - 1, and recall that 1 εj0 > ε by assumption, hence
h(εj0) > h(ε) > 0. It then follows from (12) that
-	qiD0,jlogqiD0,j ≤ logk - h(εj0) < logk - h(ε) < logk
1≤j≤k
Note also that for all 1 ≤ i ≤ k satisfying i 6= i0, Jensen’s inequality yields
-	qiD,j log qiD,j ≤ log k.
1≤j≤k
Thus,
kk
H(D) = -XpZ(i)XqiD,jlogqiD,j < logk - ζh(ε)	(13)
Since (13) contradicts the condition that H(D) ≥ log k - ζh(ε), we conclude that no such i0,j0
exist, therefore H(D) ≥ log k - Zh(ε) implies |qD- - 11 ≤ ε for all 1 ≤ i,j ≤ k.	□
16
Under review as a conference paper at ICLR 2021
B.2 Joint asymptotic equipartition property for DILN matrices
Let Matk×k([0, 1]) be the set of all row-stochastic k-by-k matrices. For convenience, a random matrix
shall henceforth mean a Matk×k ([0, 1])-valued random variable, i.e. we omit the qualifier “row-
stochastic” from “random row-stochastic matrix”. Let 圉 denote the matrix in Matk×k([0,1])
whose k2 entries are all equal to 1. We shall also use ∣∣∙]∣ to denote a norm on Matk×k([0, l]). All
subsequent results still hold for any norm on Matk×k ([0, 1]); only certain constants in continuity
arguments would change with a different norm. For concreteness, we shall work with the matrix
1-norm, i.e. k[ci,j]1≤i,j≤k k := max1≤j≤k Pik=1 |ci,j|.
Proposition B.7. Let 0 < ε ≤ 1, and suppose that D(1), . . . , D(m) are derived DILNs of D. If
Z := min{pz(i) : 1 ≤ i ≤ k} > 0, andif 煮 Pm=I H(D(t)) ≥ log k 一 Zh(k) (where h(x) is defined
as in (9)), then
m
IK m X QD(t))-[ k ]k×k∣∣≤ J	(14)
t=1
Proof. Define the function σZ : Matk×k ([0, 1]) → R by
kk
[ci,j]1≤i,j ≤k 7→ 一	pZ (i)	ci,j log ci,j .
Note that H(D0) = σZ (QD0) for any derived DILN D0 of D. Note also that σZ is a concave function,
so by generalized Jensen’s inequality (see Perlman (1974)),
mm
log k 一 ζh( ε)≤ m X H(D(t))= m XσZ (QD(t)) ≤ σZ
t=1
t=1
m
(m X QD(t))∙
t=1
Consequently, writing QD(t) = qi(,tj)1≤i,j≤k for each 1 ≤ t ≤ m, it follows from Lemma B.6 that
for all 1 ≤ i,j ≤ k, therefore (14) follows from the definition of the matrix 1-norm.	□
Next, we shall define several Matk×k ([0, 1])-valued functions. For every α = (α1, . . . , αk ) ∈ [0, 1]k ,
define the function fiαncrement : Matk×k ([0, 1]) × Matk×k ([0, 1]) → Matk×k ([0, 1]) by
([qi,j ]1≤i,j≤k, [εi,j' ]ι≤i,j≤k)→	[qi,j'(I-	αj)	+ X	qi,tatkki(I +	εi,j )]	1≤ij≤k	(15)
1≤t≤k
t6=j
Define the random matrix Eα00 := [Ei0,0j]1≤i,j≤k, where Ei0,0j is the random variable as defined in (1).
Notice that by definition, fiαncrement (QD, Eα00) = QDα .
Next, let fsαolve : Matk×k([0, 1]) × Matk×k([0, 1]) → Matk×k([0, 1]) be the function that is uniquely
determined by the map fiαncrement(Q, E), E 7→ Q. Note that to compute this map fsαolve, we would
need to solve a system of linear equations: Specifically, if Q = [qi,j]1≤i,j≤k and E = [εi,j]1≤i,j≤k,
and if fiαncrement (Q, E) is the given matrix [ci,j]1≤i,j≤k , then Q can be computed by solving the
system of k2 linear equations in the k2 variables {qi,j}1≤i,j≤k, given as follows:
ci,j = qi,j (I 一 αj ) + E qi,t at^k-⅛ (I + εi,j ) (for 1 ≤ i,j ≤ k).
1≤t≤k
t6=j
In general, for any given matrix [ci,j]1≤i,j≤k, if we sample E from the distribution of Eα00, then this
system of linear equations has a unique solution almost surely. Consequently, fsαolve([ci,j]1≤i,j≤k, Eα00)
is well-defined almost surely.
Lemma B.8. Let ε > 0, let α ∈ [0, 1]k, and let n ≥ 1 be an integer.
17
Under review as a conference paper at ICLR 2021
•	Let E100, E200, . . . be an infinite sequence of i.i.d. random matrices with the same distribution as
Eα00, and suppose that E100 = E1, E200 = E2 , . . . is a corresponding sequence of observed values.
•	Let Dα(1), Dα(2), . . . be an infinite sequence of i.i.d. random derived DILNs of D with the same
distribution as Dα, and suppose that Dα(1) = D1, Dα(2) = D2 , . . . is a corresponding sequence
of observed values.
For every integer i ≥ 1, define QEi := fiαncrement(QD,Ei). Then,
∣1n	∣
nl→∞ pr(∣∣ n X(QDi-QEi)卜ε) = 1.
i=1
Proof. By the weak law of large numbers, and using the definitions of QDi and QEi, we have
n Pn=I QDi -→ E[QDa], and 1 Pn=ι QEi -→ E[QDa], hence the assertion follows.
□
Theorem B.9. Let n, - ≥ 1 be integers, and let (α(1), . . . , α(m)) be a sequence of - vectors in
[0,1]k. Assume that Z := minι≤i≤k PZ(i) > 0. Let ε > 0, and define δ := 1 Zh(品)> 0 (where
h(x) is defined in (9)).
•	Let D0(1) , . . . , D0(m) ∈ U[D] such that
m
logk - mm XH(Dj))I ≤ δ∙
j=1
(16)
•	For every 1 ≤ j ≤ m, let E100(j), E200(j), . . . be an infinite sequence of i.i.d. random matrices
E1(j) , E200(j) = E2(j) , . . . is a
with the same distribution as Eα00(j), and suppose that E100(j)
corresponding sequence of observed values.
•	Let (D1(1), . . . , Dn(1)) ∈ Λ(εn) (Dα(1) ), . . . , (D1(m), . . . , Dn(m)) ∈ Λ(εn) (Dα(m) ) be m sequences
such that for all 1 ≤ i ≤ n,
m
∖-X(H (Dj))-H (Dj))) I ≤ δ∙
j=1
(17)
Forevery 1 ≤ i ≤ n, 1 ≤ j ≤ -, define (QD(j):= fSOlVe(QD(j), Ey)) .Then,
i0
m
lim Pr (∣∣ (一 X
n→∞	-
j=1	i=1
n
X QDij))-QD ll <ε) = L
(18)
Proof. First of all, note that (16) yields
m
Ilogk - - X H (Dj) )∣≤δ ≤ 2δ = ζh(枭),
thus it follows from Proposition B.7 that ∣∣ml Pj=I Qd(3)- [k]fc×fc∣∣ ≤ ∣. By (16) and (17), we
infer that ∣ log k - mm Pj=I H(Dj))I ≤ 2δ for all 1 ≤ i ≤ n. So by similarly applying Proposition
B.7, Wegetll m pmm=ι QD(j) — [ k ]fc×fc∣∣ ≤
Thus, by triangle inequality, ∣∣ m1 Pj=I (QD
ε for all 1 ≤ i ≤ n.
(j ) - QD(j) ∣ ≤ ε for all 1 ≤ i ≤ n, which implies
∣1m 1n	∣
∣∣ - X n X (QD(j)- QDij)) ∣∣ ≤ ε
j=1 i=1
Also, by Lemma B.8, we infer that
(19)
∣1m 1n
nl→∞Pr (∣∣- X nX (QDLQEy
< ε = 1,
(20)
1
n
18
Under review as a conference paper at ICLR 2021
where QE(j) := fiαn(cjr)ement(QD,Ei(j)). Consequently, it follows from (19) and (20) that
i
mn
nl→∞ Pr(Il m X n X(QDOj)-QE(j))∣∣ <2ε) = 1.	(21)
m j=1 n i=1
α(j)	(j)	α(j)	α(j)	(j)	(j)
Note that by definition, fsαolve(QE(j),Ei ) = fsαolve(fiαncrement(QD,Ei ),Ei ) = QD. Therefore,
i
by applying the multilinear function fSθljζ(1, E(j)) to each term in (21), We get (18) as desired. □
B.3 ESTIMATION OF QD VIA TYPICAL SETS
Define the random matrix E0 := [Ei0,j]1≤i,j≤k, Where Ei0,j is the random variable as defined in
Section 2.2. Given any observed value E = [εi,j]ι≤i,j≤k for E0, we shall write E + 1 to denote the
matrix [εi,j + k] 1≤i,j≤k.
Lemma B.10. Let ε > 0, and let n ≥ 1 be an integer.
•	Let E10 , E20 , . . . be an infinite sequence of i.i.d. random matrices with the same distribution as
E0, and suppose that E10 = E10, E20 = E20, . . . is a corresponding sequence of observed values.
•	Let Di, D2,..._be an infinite sequence of i.i.d._ random derived DILNof D with the same
distribution as D, and suppose that Di = Di, D2 = D2,... is a corresponding sequence of
observed values.
Then,
n
nl→∞ Pr (II n X(QDi-(Ei+1 川 <ε)=1∙
i=i
Proof. By the weak law of large numbers, and using the definitions of QDi and Ei + ɪ, we have
n Pn=I QDi -→ E[QD], and 1 Pn=I(Ei + 1) —→ E[QD], hence the assertion follows.	□
By assumption, our dataset D_has N instances and k label classes. We shall define the gap of D to
be gap(D) := log k - E[H(D)]. Notice that by the definition D, this gap(D) depends only on the
values of N and k.
Lemma B.11. 0 ≤ gap(D) ≤ log k, and limN →∞ gap(D) = 0.
Proof. By Jensen,s inequality (see Cover & Thomas (2012, Thm. 2.6.2)), we have E[H(D)] ≤
H(E[D]) = log k, hence gap(D) ≥ 0. Note that gap(D) ≤ log k, since H(D0) ≥ 0 for all DILNS
D0. Finally, the limit limN→∞ gap(D) = 0 is a direct consequence of the weak law of large
numbers.	□
The following theorem is an extension of Theorem B.9 that takes into account predictions from a
discriminator for D. This extension involves the gap ofD.
Theorem B.12. Let n, m, r ≥ 1 be integers, let (α(i), . . . , α(m)) be a sequence ofm vectors in
[0, 1]k, and assume that ζ := min{pZ (i) : 1 ≤ i ≤ k} > 0. Let β, δ, ε, ε0 > 0 be scalars satisfying
gap(D) < δ ≤ 2ζ log k, 0 < ε ≤ δ — gap(D), and ε0 := 2k ∙ h-1(2δ) > 0, where h(χ) is defined
in (9). Also, let g be a separable Rd-valued random function on U[D].
•	Let Ei0 , . . . , En0 be a sequence of i.i.d. random matrices with the same distribution as E0, and
suppose that Ei0 = Ei0, . . . , En0 = En0 is a corresponding sequence of observed values.
•	For every 1 ≤ j ≤ m, let Ei00(j), E200(j), . . . , En00(j) be a sequence of i.i.d. random matrices with
the same distribution as Eα00(j), and suppose Ei00(j) = Ei00(j), E200(j) = E200(j), . . . , En00(j) = En00(j)
is a corresponding sequence of observed values.
•	Let(D01,..., D0m)) ∈ Λm (D), andfor every 1 ≤ j ≤ m, suppose that Φj is a discriminator
for D that is trained r-fold on (D0(j), g) with threshold β.
•	For every 1 ≤ j ≤ m, suppose that (Di(j), . . . , Dn(j)) ∈ Λ(εn0 ) (Dα(j) ) ∩ (Φj+)n.
19
Under review as a conference paper at ICLR 2021
For every integer 1 ≤ i ≤ n, define Qj) := fSθlV)e(E0 + 1, EFj). Then there exists a sufficiently
large integer rβ,δ such that for all r ≥ rβ,δ,
lim Pr
n→∞
j=1	i=1
- QD < 2ε0 = 1.
(22)
Proof. The proof, although seemingly complicated, actually follows essentially from unraveling the
relevant definitions. First of all, notice that
E[H(D)] - ε ≥ E[H(D)] - δ + gap(D) =log k - δ,
which implies that for any (D01),..., DOml) ∈ Λim)(D), We have (by the definition of Λ^m)(D))
that
m
I log k - mm X H(D0j))∣ ≤ δ.	(23)
For each 1 ≤ j ≤ m, it follows from Lemma B.5 that there exists some sufficiently large integer
rβ(j,)δ ≥ 1 such that for all r ≥ rβ(j,)δ,
D0 ∈ Φj+ =⇒ |H(D0(j)) - H(D0)| ≤ δ.	(24)
Let rβ,δ := max{rβ(j,)δ : 1 ≤ j ≤ m}, and henceforth assume that every discriminator Φjis trained
r-fold on (D0(j), g) with threshold β, for some r ≥ rβ,δ. For all 1 ≤ j ≤ m and 1 ≤ i ≤ n, note that
Di(j) ∈ Φj+ by definition, hence (24) implies that |H(D0(j)) - H(Di(j))| ≤ δ, so in particular,
m
∣m X(H(Dj))-H(Dj))) ∣ ≤ δ∙	(25)
j=1
By definition, ε0 = 2k ∙ h-1(2δ). This means that δ = 2Zh(蔡).In particular, recall from Lemma
B.6 that h(x) is a strictly increasing (and hence bijective) function with domain [0, k - 1], note that
h(k - 1) = log k, and note that 2δ ≤ log k by assumption, so the inverse h-1(2δ) is well-defined.
Then for every 1 ≤ j ≤ k, it follows from (23), (25), and Theorem B.9 that
lim Pr
n→∞
- QD < ε0) = 1.
(26)
Note that by Lemma B.10, we have
mn
nl→∞Pr (II m X n X(QDOj)-(E0+1 ))∣∣ <ε0) = 1.
m j=1 n i=1
Next, apply the multilinear function /a&—, E00(j)) to each term in (27); this yields
mn
Um Pr (II ɪ X 1 X fιVe(QD ⑺,EFj) - fSθlVe(E0 + 1, Ei00jj)) II < ε0) = L
n→∞ m n	D0
m j=1 n i=1
(27)
(28)
Note that Qj) := f,O,lVe(E0 + 1, E00(j)) by definition, thus it follows from (26) and (28) that
mn
nl→∞pr (H( m X 而 X Qj)) - QD 卜 2ε')=L	(29)
j=1 i=1
□
20
Under review as a conference paper at ICLR 2021
B.4 Proof of Theorem 2.6
Consider a valid α-sequence Ω = (α(1),..., α(`)) for D. By definition, this means Ω is a subsequence
of an infinite sequence (α(1),α(1),...) of distinct vectors in [0, k-1 )k (recall that k is the number of
label classes), such that {α(i)}∞=ι is a dense subset of [0, k-1 ]k.
Let ε > 0. For any fixed integer n ≥ 1, observe that
Iim WnIDa(G ∩ Wn(D)=Wn(D).
'→∞
This implies that
∣Λεn)(Dɑ('))∩ Λεn)(D)∣
lim J--------/ 、 _--------L = 1.
'→∞	∣Λεn)(D)∣
In contrast, for any fixed integer ' ≥ 1, if E[H(Da('))] = E[H(D)], then
lim lim A，n)(Da(')) ∩ A，n)(D) = 0.
ε→0 n→∞
(30)
This implies that
lim lim
ε→0 n→∞
∣Λεn)(Da(')) ∩ Λεn)(D)∣
∣Λεn)(D)∣
0.
(31)
LemmaB.13. Let β,ε > 0, let D0 ∈ Λ!1)(D), i.e. D0 is an ε-typical baseline DILN of D, and
let g be a D0-separable Rd-valued random function on U[D]. Let r ≥ 1 be an integer, let Φ be
a discriminator for D that is trained r-fold on (D0, g) with threshold β, and suppose that Φ is
δ0-sufficient for some δ0 > 0. Let Ω = (a(1),α(2),...) be a valid a-sequencefor D. Then there
exists some sufficiently large integer 'φ ≥ 1 such thatfor all integers ' ≥ 'φ, a randomly generated
ε -typical α(')-increment DILN of D has non-zero probability of being predicted positive by Φ, i.e.
Pr (Λεrn(g(Do)) ∩ Λr(g①))=0∣D0 ∈ A：%。”))> 0.	(32)
Proof. Since Φ is δ0-sufficient, We infer that (32) is true if there exists some D0 ∈ A：1) (Da(')) such
that H (Do ∧ D0) < δ0. Clearly, H (Do ∧D0) = 0 < δ0,so it suffices to show that Do ∈ A：1)(Da(')).
From (30), we conclude that Do ∈ A：1)(Da(')) is true if ' is sufficiently large.	□
Finally, we prove an equivalent (and rather long) reformulation of Theorem 2.6 from the main paper.
Theorem B.14. Let n,m,r,' ≥ 1 be integers, let Ω = (α(1),α(2),...) be a valid a-sequence for
D, and assume that ζ := min{pZ (i) : 1 ≤ i ≤ k} > 0. Let β, δ, ε, ε0 > 0 be real scalars satisfying
gap(D) < δ ≤ 2ζ log k, 0 < ε ≤ δ 一 gap(D), and ε0 := 2k ∙ h-1(2δ) > 0, where h(x) is defined
in (9). Also, let g be a separable Rd-valued random function on U[D].
•	Let D(1),..., D(m) be a sequence ofi.i.d. random derivedDILNs of D with the same distribution
as D, and suppose we have observed values D(I) =D01),...,D(m) = D0m).
•	For every 1 ≤ s ≤ ` and 1 ≤ j ≤ m, let Dα(j()s),1, Dα(j()s),2, . . . , Dα(j()s),n be a sequence of i.i.d.
random derived DILNs ofD with the same distribution as Dα(s), and suppose we have the
observed values Dα(j()s),1 = Ds(j,1),Dα(j()s),2 = Ds(j,2), . . .,Dα(j()s),n = Ds(j,n).
For every 1 ≤ j ≤ m, suppose that Φj is a discriminator for D that is trained r-fold on (Do(j), g)
with threshold β, and suppose that every discriminator Φj is δ0-sufficientfor some δ0 > 0. Then there
exists some sufficiently large integer '丁3 ≥ 1 (which depends on r and δ0), such thatfor all integers
'0 ≥ 'r,δo, a randomly generated ε-typical α('0)-increment DILN of D has non-zero probability of
being predicted positive by Φj for all 1 ≤ j ≤ m. Assume that the given integer ` is sufficiently large,
i.e. ' ≥ 'r,δo. For every 1 ≤ j ≤ m, define
s(bje)st := min{s : 1 ≤ s ≤ `, there exists some 1 ≤ i ≤ n such that Ds(j,i) ∈ Φj+}.	(33)
Let 1 ≤ jι < j2 < •…< jm0 ≤ m be all indices jt such that Sbjts)t = -∞. (By default, we define
min 0 = -∞. Note that m0 ≤ m.)
21
Under review as a conference paper at ICLR 2021
•	For every 1 ≤ j ≤ m, let E10(j), . . . , En0(j) be a sequence of i.i.d. random matrices with the
same distribution as E0, and suppose that we have the sequence of observed values E10(j) =
0(j)	0(j)	0(j)
1	,..., n = n .
• For every 1 ≤ j ≤ m, let E100(j) , E200(j) , . . . , En00(j) be a sequence of i.i.d. random matrices
with the same distribution as Eα00(s0), where s0 := s(bje)st, and suppose we have the sequence of
observed values E100(j) = E100(j) , E200(j) = E200(j) , . . . , En00(j) = En00(j).
For every 1 ≤ i ≤ n and 1 ≤ j ≤ m, define Qj) := fO(Ve(E0 + 1, E00(j)). Then there exist a
sufficiently large integer rβ,δ ≥ 1 (depending on β and δ), a corresponding sufficiently large integer
'rβ,δ ,δo ≥ 1 (depending on rβ,δ and δ0), such thatfor a fixed r = rβ,δ, andfor all' ≥ '",§ e,
0
mι→m∞ [nlim∞ Pr (IH XG XQjt)))- QD∣ < 2ε0) ] = L	(34)
Proof. First of all, for each 1 ≤ j ≤ m, Lemma B.13 says that there is some sufficiently large integer
'φj such that for all' ≥ 'φj, a randomly generated ε-typical α(')-increment DILN of D has non-zero
probability to be in Φ+. Thus, We could set 'r := max{'φj : 1 ≤ j ≤ m}.
For each 1 ≤ t ≤ m0, define α(bjets)t := α(s0), where s0 = s(bjets)t . Observe that if
(Dj)) I,…,DSj n) ∈ W) (Dajt) ) ∩ (Φ+ )n,	(35)
best ,	best ,	ε αbest	t
and if (Dj1,..., D0jm0)) ∈ A!m’^D), then Theorem B.12 yields
m0	n
nl→∞ Pr (||(菽 X n X Qjt))-QD 卜 2ε')=L	(36)
t=1 i=1
By definition, each Ds(j(jtt)) ,i (for 1 ≤ i ≤ n) is already contained in Φj+, hence for (35) to be true, We
only need to check that
(Djj))) J,..., Dj) ,n) ∈ Λε 1(Dajt)).
best ,	best ,	ε	αbest
NoW, for any 1 ≤ t ≤ m0, it folloWs from Theorem B.3 that there exists some sufficiently large
integer n0 ≥ 1 such that for all integers n ≥ n0, We have
Pr (((DO11,..., D0n)), (Djtj,..∙, Djt,n)) ∈ "(D，Djt)) > 1-ε0.	(37)
Consequently, if m0 ≥ n0, then
0
nlim∞ Pr (IH X (1 XQjt))) -QD 卜 2ε)>1 - ε0	(38)
Finally, the choice of `r implies that Pr(s(bje)st 6= -∞) > 0, hence m0 → ∞ as m → ∞, therefore
(39) is true.	□
Note that the matrix
Q D ；
Which appears in (39) is precisely the output matrix of Algorithm 1. This gives the folloWing tWo
corollaries.
Corollary B.15. Let δ0 > 0. Assume that the initial conditions (on r,m,n,', Ω,β,g) given in
Algorithm 1 are satisfied. Assume k ≥ 2, assume that ζ := min{pZ (i) : 1 ≤ i ≤ k} > 0, and
assume that the discriminator Φj is δ0-sufficient for all 1 ≤ j ≤ m. If r (depending on β) is
sufficiently large, and if ` (depending on r and δ0) is sufficiently large, then the final output matrix
QD from Algorithm 1 satisfies
lim lim Pr (∣∣<Qd - Qd∣∣ < 4k ∙ h-1 (2 . gap(D))) = 1.	(39)
m→∞ n→∞	ζ
22
Under review as a conference paper at ICLR 2021
Proof. Following the notation of Theorem B.14, note that 2ε0 = 4k ∙ h-1(2δ) > 0 (where h(x) is
a strictly increasing function defined in (9)), and note also that δ > gap(D), hence the assertion
follows from Theorem B.14 by choosing δ arbitrarily close to gap(D).	□
Corollary B.16. Let δ0 > 0. Assume that the initial conditions (on r,m,n,', Ω,β,g) given in
Algorithm 1 are satisfied. Assume k ≥ 2, and assume that the discriminator Φj is δ 0 -sufficient for
all 1 ≤ j ≤ m. Also, assume that for every possible label i ∈ A = {1, . . . , k}, there is at least one
instance ofD with i as its correct label. If r (depending on β) is sufficiently large, and if ` (depending
on r and δ0) is sufficiently large, then the final output matrix QD from Algorithm 1 converges in
probability to QD as m → ∞, n → ∞, and N → ∞. (Here, N is the number of instances in D.)
Proof. This is immediate from Corollary B.15 and Lemma B.11.	□
B.5	Precise reformulation of proposed algorithm
In the previous few subsections, we have introduced new notation and terminology, so that we are
able to give a precise statement of our consistency result (Theorem B.14). Correspondingly, using
our new notation and terminology, we shall also give, in this subsection, an equivalent reformulation
of Algorithm 1 from the main paper.
Algorithm 2 A precise formulation of algorithm to estimate QD
Require: integers r,m,n,' ≥ 1.
Require: threshold β > 0, and g a separable Rd-valued random function on U[D].
Require: Ω = (α(1),..., a(`)) ⊆ [0,1]k a valid α-sequence with ' vectors.
1:	Initialize empty list L.
2:	for j = 1 . . . m do
3:	Generate observed value D = D0j).
4:	for s = 1 . . . ` do
5:	Generate n independent observed values Dα(s) = Ds(j,1), Ds(j,2), . . . , Ds(j,n) .
6:	Let Φj be a discriminator trained r-fold on (D0(j), g) with threshold β.
7:	Compute s0 := min{s : 1 ≤ s ≤ `, there exists 1 ≤ i ≤ n such that Ds(j,i) ∈ Φj+}.
#[Note: s0 equals SjL in Theorem B.14. By default, min 0 = -∞.]
8:	ifs0 6= -∞ then best
9:	for i = 1 . . . n do
10:	Generate observed values for random matrices E0 = Ei0(j) and Eα00(s0) = Ei00(j).
11:	COmPUte Qy)= fαje(E0(j) + k, Ei00(j))
#[Note: Computation of Q(j) involves solving a system of k2 linear equations in k2 variables.]
#[Note: Unique solution to linear system exists almost surely.]
12:	if unique solution exists in computation of Q(j) then
13:	Insert matrix Q(j) into list L.
14:	return mean of matrices in L (This is our estimate QD for QD .)
B.6	How to check for separable random functions ?
Roughly speaking, Fig. 3 captures the intuition on how to check experimentally whether a candidate
Rd-VaIUed random function g on U[D] is approximately separable. Suppose Dɑγ,..., Dat is a
sequence of α-increment datasets for distinct vectors α = αι,..., a`, i.e. corresponding to different
noise levels. Since g is a random function, we can repeatedly call g(Dαi ) to get a sequence of
vectors in Rd, say of length r (i.e. each of the r entries is a vector). As elaborated in Section 2.3,
these sequences of length r, which correspond to different values αi, are the datapoints for training
discriminators; each sequence is a single datapoint for training a discriminator Φ. For our random
function g to be separable, the datapoints (sequences) generated using a particular value for α should,
with high probability, be distinguishable from datapoints (sequences) generated using a different
value for α. Using Fig. 3 as an example, notice that for each considered noise rate a (corresponding
23
Under review as a conference paper at ICLR 2021
to α = (a, . . . , a)), we have plotted a coordinate-wise minimum-to-maximum range of the entries for
sequences generated using α = (a, . . . , a), where these sequences associated to Dα are repeatedly
generated using multiple random seeds. We call this coordinate-wise minimum-to-maximum range a
“band”. Now, notice that different bands, corresponding to different values for α, are already ”visually
separable” in our given plot. When we train a discriminator Φ on such sequences (datapoints), those
sequences in the red band are treated as ”positive” datapoints, while those sequences in the blue
bands (of various blue hues) are treated as ”unlabeled” datapoints. The goal for Φ is to identify which
blue band is ”most indistinguishable” from the red band, and then use the α value corresponding to
the identified blue band to infer a single intermediate estimate of the noise transition matrix. For this
idea to work, the blue bands (for different values of α) should themselves be distinguishable (i.e.
”separable”) from each other. By ”distinguishable”, we mean that a machine learning model (we
used decision trees in our experiments) is able to distinguish (i.e. ”separate”) different blue bands.
So informally, a candidate random function g is “separable” if sequences of g(Dα ) are (with high
probability) distinguishable for different values of α. Therefore, we have experimentally verified (see
Fig. 3) that g = gLID is a suitable function that is empirically (approximately) separable.
B.7	Striking connections to Shannon’ s coding theorem
The significance of our proposed information-theoretic framework is not our newly introduced
information-theoretic notion H(D) of “the entropy of a DILN” per se, but rather the idea of how
this notion H(D) is used to define typical sets of random derived DILNs of D (see Definition 2.1),
and correspondingly, how the idea of typical sets is used to define our notion of separable random
functions (see Definition 2.3).
Generally speaking, H(D) is a compact notation that is very useful for us to define typical sets (of
random derived DILNs of D) in Definition 2.1). This is very similar to the scenario of defining typical
sets of random variables, in the context of the asymptotic equipartition property (AEP) theorem;
see, e.g. Chapter 3.1 in Cover & Thomas (2012). Notice that the AEP theorem (Thm 3.1.1 in
Cover & Thomas (2012)) is essentially a direct consequence of the weak law of large numbers. Its
precise theorem statement, usually formulated in terms of the entropy of a random variable, could
equivalently be formulated without any mention of, or without any interpretation involving, the
notion of information entropy. Similarly, the notion of “typical sets” of random variables makes sense
in the general context of probability theory, without necessarily needing any information-theoretic
interpretation.
Intuitively, we should still think of “typical sets” of a random derived DILN of D as a “typical”
sequence of DILNs, where “typical” has a precise meaning in terms of our notion of the entropy of
DILNs. Just like how typical sets (of random variables) are crucial for proving Shannon’s coding
theorem, our notion of typical sets of random derived DILNs of D are crucial for proving our
consistency theorem. Just like how we should think of channel coding in terms of the entropy of
random variables, we should also analogously think of label noise estimation in terms of the entropy
of DILNs.
As We have initially described in Section 2.3, and subsequently detailed in Appendix B.1-B.4, the
notions of “typical sets” and (joint) AEP are crucial for proving our main consistency result Theorem
2.6. In fact, Theorem 2.6 can be interpreted as an ”inverse” analog of (one direction of) Shannon’s
channel coding theorem. Perhaps, the separable random function g, required as input to our estimator
(Algorithm 1), is analogous to a (random) error-correction code in channel coding.
The idea of “separability” for random functions could serve as a guide for the design of future
estimators for QD . It is plausible (likely) that the convergence rate for our consistent estimator may
depend on the choice of this separable random function g. Hence, building on the parallelism betWeen
separable functions and error-correction codes, We pose the folloWing question: Could We obtain
more efficient estimators for QD by designing “better” separable functions for DILNs, analogous to
hoW more efficient error-correction codes Were designed to further improve communication over a
noisy channel?
We are excited by the interesting questions that naturally arise from this parallelism, especially
concerning the design of neW and ”better” estimators for noise transition matrices and more general
noise models, independent of classification accuracy.
24
Under review as a conference paper at ICLR 2021
C Implementation details
Algorithm 3 provided below describes in detail an explicit implementation of our proposed framework
to estimate QD , based on the use of LID-based discriminators. Broadly, there are three stages in our
implementation, and correspondingly, we organize the rest of this section into the following three
subsections:
•	Section C.1: Gathering of LID sequences.
•	Section C.2: Training of LID-based discriminators.
•	Section C.3: Final computation to estimate QD .
Algorithm 3 Implementation details to estimate QD .
Require: D, a dataset with instance-independent label noise.
Require: Σ, a collection of random seeds, with ∣Σ∣ ≥ 10.
Require: A valid α-sequence Ω =(α⑴,...，α(')).
#	[Stage 1: Gathering of LID sequences]
1:	for each random seed ς in Σ do
2:	for each a§ in Ω do
3:	Generate αs-increment DILN Ds.
4:	Generate LID sequences for Ds from a neural network.
#[Stage 2: Training of LID-based discriminators]
5:	Consolidate all generated LID sequences (from all random seeds) for initial training of LID-based
discriminators.
6:	for each triple (i.e. three seed collections) do
7:	Train a discriminator Φ on triple to produce a recall τ and a vote sequence.
8:	Initialize a fine-tuning list F containing all the triples with recall τ ≥ 0.9.
9:	Refine list F.
10:	Initialize an empty list Q.
11:	for each triple in F do
12:	Fine-tune discriminator trained on the triple.
13:	If the discriminator is well-trained (i.e. able to produce required recalls), then add it to Q.
14:	if —Q— < 2 then
15:	Add at least 1 new random seed to Σ . Go to line 1.
16:	else
#[Stage 3: Final computation to estimate QD]
17:	Get a prior.
18:	for each well-trained discriminator in Q do
19:	Compute an intermediate estimate QD).
20:	for each recall do
21:	Compute the mean of intermediate estimates QD) with the same recall.
22:	Compute the mean QD of all means of intermediate estimates.
23:	return QD (This is our final estimate for QD.)
C.1 Gathering of LID sequences
In our experiments, we synthesized α-increment DILNs of D, using only “uniform” vectors α, i.e.
every vector α(s) in our α-sequence can be expressed as α(s) = a§ ∙ 1k for some 0 ≤ a§ ≤ 1. (Here,
1k denotes the all-ones vector in Rk.) It should be noted that our choice to use only “uniform” vectors
for α(s) is an implementation simplification, stemming from our limited computational resources.
Ideally, with no computational constraints, having an α-sequence containing “non-uniform” vectors
α(s) (i.e. whose entries are distinct) could potentially improve the final estimate, albeit with a lot
more trials.
For CIFAR-10, a fixed α-sequence Ω = (α(1),..., α(`)) is used throughout our experiments (i.e.
Ω is fixed, as we consider various random seeds). We restricted every α ∈ Ω to be of the form
25
Under review as a conference paper at ICLR 2021
α = (a, . . . , a), where the values of a used are given as follows:
0%, 30%, 83%, 84%, 85%, 85.4%, 85.8%, 86%, 86.2%, 86.4%, 87.6%, 87.7%,
87.8%, 87.9%, 88.0%, 88.1%, 88.2%, 88.3%, 88.4%, 88.5%, 88.6%
For Clothing1M, we added extra α: 90.9%, 91.1%, 91.3%, 91.4%, 91.5%, 91.6%. Notice that these
values for a are not uniformly spaced apart; rather, they are concentrated in the range [83%, 88.6%]
(for Clothing1M, [83%, 91.6%]). This is because a high value of a is typically required for the
corresponding α-increment DILN of D to have near-maximum entropy, unless D already has a
significantly high noise level, e.g. > 80%. (We define the noise level of D to be the probability that a
randomly selected instance of D has a given label that differs from the correct label.) To accurately
estimate the noise level of D, we recommend choosing a finer α-sequence, if computational resources
allow for it. However, as a compromise, we have opted to use the above set of values for a in our
experiments.
C.1.1 Training of neural networks
To generate LID sequences for the CIFAR-10 dataset (Krizhevsky et al., 2009), the following 12-layer
convolutional neural network (CNN) is used 5:
•	Two convolutional layers with 64 out channels with filter size of 3-by-3 and padding of size 1,
each followed by a ReLU activation function. Max pooling of size of 2-by-2 is applied after the
last ReLU activation function.
•	Two convolutional layers with 128 out channels with filter size of 3 by 3 and padding of size 1,
each followed by a ReLU activation function. Max pooling of size of 2-by-2 is applied after the
last ReLU activation function.
•	Two convolutional layers with 196 out channels with filter size of 3 by 3 and padding of size 1,
each followed by a ReLU activation function. Max pooling of size of 2-by-2 is applied after the
last ReLU activation function.
•	Output vectors are flattened.
•	Fully connected layer with 1024 (hidden) units, which is followed by a ReLU activation function.
•	Fully connected layer with 10 output units, whose weights are set to be non-trainable (i.e.
frozen).
The above model architectures are used when the dataset size is “relatively” big. When the size is
small, for example, less than 40, 000 samples6, the LID sequences generated could be identical even
if the level of label noise present in the DILNs are not similar. For instance, in Fig. 3, the given
datasets in both plots are clean, but the top plot corresponds to the intact CIFAR-10 dataset with
50, 000 samples, while the bottom plot corresponds to a subset of the CIFAR-10 dataset with 70%
anchor-like instances removed, i.e., with a total of 15, 000 samples.
For noise levels of 80% and 85%, the LID sequences in the top plot is still “visually separable” except
from epoch 9 to epoch 17. In contrast, the LID sequences on the bottom plot for the same noise
levels (80% and 85%) overlap with each other across almost all training epochs, until they become
stagnant on the 23rd epoch. We posit that such significant overlaps could vitiate the effectiveness of a
discriminator Φ in distinguishing baseline DILNs from non-baseline DILNs, when Φ is trained on
such overlapping LID sequences.
Therefore, to further differentiate the LID sequences, we used the trials described as follows: (i)
increase the number of hidden units in the last fully connected (fc) layer, (ii) increase the number of
5Although this CNN is trained for the CIFAR-10 classification task, achieving high classification accuracy
is not our goal. Instead, our goal is to gather LID sequences from the training phase. For example, given a
dataset with symmetric noise rate 50%, when 80% uniform α-vector is used, the best test accuracy is around
25%. While training baseline DILNs for LID sequences, the test accuracy is around 10%, which is equivalent to
“random guessing” (since the labels in baseline DILNs are by definition chosen uniformly at random). Despite
the seemingly low test accuracies, these LID sequences are sufficient for use as training data for the discriminator
Φ.
6In one of our trials, as part of our comparison with MPEIA, we used the CIFAR-10 dataset with 90%
samples removed from one class. To gather LID sequences for the resulting smaller subset of CIFAR-10, we
used the same neural network model architecture as in the case when the dataset is intact.
26
Under review as a conference paper at ICLR 2021
convolutional layers, (iii) reduce the training batch size, (iv) fix the weight of the last fc layer, (v) do
not use softmax and batchnorm; and (vi) do not use random crop during training. See Table 4 for
adjustments made when a smaller subset of the original dataset is used. Note that we did not continue
using 8192 hidden units in the last fc layer for CIFAR-10 with 70% anchor point removal. This is
because if the batch size is small or if the number of hidden units in the last fc layer is increased,
then the training time would increase significantly. Therefore, 4096 number of hidden units was used
instead.
For Clothing1M, we used ImageNet-Pretrained ResNet-18 (PyTorch version). An fc layer with 512
hidden units is added to it (2 fc layers in total) and the last fc layer is fixed without updating weights
during training. We refer this model as “ResNet19”.
Dataset Amount of anchor-like instances removed	CIFAR-10		
	0%	40%	70%
Number of convolutional layers	6	6	6
Number of hidden units in the last fc layer	1024	8192	4096
Batch size	128	128	32
Table 4: A summary of the main differences while training CNN to generate LID sequences for
CIFAR-10, when different amounts of anchor-like instances are removed.
For training, after normalization, random horizontal flip with probability 0.5 is applied for CIFAR-10
as part of data augmentation. The initial learning rate is 0.01, which is reduced to 0.001 at the 40th
epoch for CIFAR-10. We used stochastic gradient descent (SGD) with a momentum of 0.9 and a
weight decay of 10-4. Training is stopped at the 45th epoch for CIFAR-10.
For the Clothing1M subset, images are first resized to 256X256. During training, images are randomly
cropped to size 224X224 then random horizontal flip is applied with probability 0.5. During test
time, images are first resized to 256X256, then centrally cropped to 224X224. For normalization,
we used mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225] for the 3-channel RGB images.
The initial learning rate is 0.001, which is reduced to 0.0001 at the 5th epoch. We used stochastic
gradient descent (SGD) with a momentum of 0.9 and a weight decay of 0.05. The model is trained
for 6 epochs to gather LID sequences, with batch size 32.
In our experiments for all datasets, our computation of LID sequences follows the same process as
given in (Ma et al., 2018a), with the following exceptions:
1.	To remove unwanted randomness in any LID sequences computed, we re-implemented
their code in PyTorch. This ensures that any random function we invoke is completely
determined by the selected random seed. The unwanted randomness in the original Keras
implementation includes (although not limited to) randomized weight initializations and
non-deterministic cuDNN sub-routines.
2.	The weights in the last fully-connected (fc) layer of the neural network are fixed throughout
training, so that all weights updated during backpropagation are used in the computation of
LID sequences. (Recall that LID sequences are computed using the output vectors from the
last hidden layer.)
3.	In (Ma et al., 2018a), 1280 samples are randomly selected from the whole dataset throughout
the training epochs to compute one LID sequence. In contrast, we generated 50 random sets
of 1280 sample indices to compute 50 LID sequences for Φ. At each epoch, the LID scores
are computed from the fixed samples.
C.1.2 Comparison of neural networks used for CIFAR- 1 0 and Clothing 1 M
To ensure a fair comparison, we chose the same backbone neural network architecture across all
baseline methods, and we used a smaller architecture for our method. As far as possible, we tried
to use the same number of training epochs across all baseline methods. However, the different
methods are rather distinct, and employ various techniques for their estimation of QD , such as the
augmentation of the neural network architecture (e.g., one extra “distinguished” softmax layer for
27
Under review as a conference paper at ICLR 2021
S-model), a schedule using multiple loss functions for training (e.g., T-Revision uses 3 loss functions
in total for training). For a more comprehensive overview of the subtle differences across the various
methods, please see Tables 5 and 6, which explicitly specify the neural network architecture used, the
usage of neural network, the type of loss functions, and the number of training epochs for different
stages (including the computation of priors, if any).
CIFAR-10	(analogous) prior NN structure	(analogous) prior NN no. of training epochs (default loss: crossentropy)	main method NN structure	main method NN no. of training epochs (default loss: crossentropy)
S-model	ResNet18 (to initialize matrix)	20	ResNet18 + softmax layer	100
Forward	na	na	ResNet18	120
T-Revision	ResNet18 (to initialize matrix)	20	ResNet18 (with “slack variable” component)	200 (reweight loss) + 100 (revision loss)
ours-1	ResNet18 (to obtain prior)	20	12-layer CNN (6 conv layers, to gather LID sequences)	45
MPEIA	ResNet18 (ImageNet pretrained)	na	MPEIA doesn’t use NN in its main method.	
ours-2	We used MPEIA’s estimate as our prior.		12-layer CNN (6 conv layers, to gather LID sequences)	45
GLC	na	na	ResNet18	120
ours-3	We used GLC’s estimate as our prior.		12-layer CNN (6 conv layers, to gather LID sequences)	45
Table 5: Neural network (and its usage), training losses and training epochs of all the baselines for
CIFAR-10. Some methods, S-model, T-Revision, ours-1, ours-2 and ours-3, require “prior model” to
initialize noise transition matrices for formal estimation. While some method, T-Revision, trains NN
with special losses in its main method.
Clothing1M subset	(analogous) prior NN (ImageNet pretrained)	(analogous) prior NN no. of training epochs (default loss: crossentropy)	main method NN (ImageNet pretrained)	main method NN no. of training epochs (default loss: crossentropy)
S-model	ResNet50 (to initialize matrix)	10	ResNet50 + softmax layer	10
Forward	na		ResNet50	10
T-Revision	ResNet50 (to initialize matrix)	10	ResNet50 (with “slack variable” component)	10 (reweight loss) + 10 (revision loss)
ours-1	ResNet19 (to obtain prior)	10	ResNet19 (to gather LID sequences)	6
MPEIA	ResNet50 (to obtain features)	10	MPEIA doesn't use NN in its main method.	
ours-2	We used MPEIA’s estimate as our prior.		ResNet19 (to gather LID sequences)	6
GLC =	na		ResNet50	10
ours-3	We used GLC’s estimate as our prior.		ResNet19 (to gather LID sequences)	6
Table 6: Neural network (and its usage), training losses and training epochs of all the baselines for
Clothing1M subset. Respective prior models and losses are summarized.
C.1.3 LID plots
Fig. 3 is provided to show that LID sequences are effective in distinguishing datasets with different
entropies. We used 4 clean datasets as the underlying dataset D: CIFAR-10 intact (top plot in Fig.
28
Under review as a conference paper at ICLR 2021
3), CIFAR-10 with 70% anchor-like data removal (bottom plot in Fig. 3). For every α-increment
DILN (shown in blue) or baseline DILN (shown in red) of D, we generated 5 such DILNs. The
noise transition matrices are of symmetric form and the noise levels inserted are 0%, 60%, 80%, 85%
and 87.8% (colored from the lightest blue to the darkest). We computed 50 LID sequences for each
DILN, which corresponds to 50 datapoints in each epoch. To visualize the 50 sequences of a DILN,
only the maximum and the minimum scores are displayed for each epoch. For CIFAR-10, this forms
a band with 2 lines over the total 45 epochs. The same random seed is used for all the plots.
C.2 Training of LID-based discriminators
Each LID-based discriminator Φ is trained using positive-unlabeled bagging (Elkan & Noto, 2008;
Mordelet & Vert, 2014), with decision trees used as our sub-routine. We used 1, 000 trees and the
number of unlabeled samples to draw and train each base estimator is 50.
C.2.1 Initial training
The LID sequences from α-increment DILNs are treated as unlabeled samples while those from
baseline DILNs are treated as positive. For CIFAR-10, although the maximum noise level as injected
into D is 88.6% to synthesize a derived DILN of D, we used the LID sequences from as up to 88.3%
for initial training. It is recommended to reserve some LID sequences associated to the high value
as during the initial training. The reason is that it could be hard for a discriminator to get a good
recall as the positive samples (LID sequences from the baseline DILN) and large number of negative
samples (LID sequences from DILNs with as > 0.883) are very similar. However, for Clothing1M,
we did not reserve LID sequences with high α values, for the purpose of training ease. Recall that
α-increment DILNs and a baseline DILN from the same common random seed is collectively called
a “seed collection”. Any three different seed collections is called a “triple”. The discriminators would
be trained on the LID sequences from all possible combinations of triples. In our experiments, we
used at least 10 random seeds for each matrix in CIFAR-10 and 20 random seeds for Clothing1M.
We do not use any augmentation or normalization of the LID sequences when training Φ.
A trained discriminator Φ predicts whether an input LID sequence is similar to the LID sequences
generated from baseline DILNs. If Φ assigns a score of ≥ 0.5 to some LID sequence, then this LID
sequence is predicted positive and one vote is assigned to its respective α(s). If the score is instead
< 0.5, then 0 vote is assigned. The vote sum of α(s) represents the number of LID sequences a
discriminator “considers” to be baseline-LID-sequences-alike. The vote sum sequence of all the α(s)
in the α-sequence is called a “vote sequence”. A trained discriminator produces a recall τ and a vote
sequence.
C.2.2 Finetuning
After the initial training, all the triples come with their recalls and vote sequences. We only select
those with recall above 0.9 and put them in a list F for further fine-tuning, which indicates a possibly
well-trained discriminator and similar label distributions in both DILN D0 (the baseline dataset) and
Ds (synthesized from D by injecting noise αs).
Triples with recall above 0.9 are added into a list F and we would further refine the list for better
QD estimation. For CIFAR-10, if the total number of non-zero votes of all the triples in F before
noise level 0.85 is above 4 (for Clothing1M, the noise level threshold is 0.86, since Clothing1M has
wider range of α values), we use triples with low recall ≤ 0.92 and abandon the remaining. Else,
we use triples high recall ≥ 0.98. Given that (for CIFAR-10 only), if the number of non-zero votes
from noise levels of 0.85 to 0.86 is more than 4, only recall 0.98 is used. The intuition is that if the
presence of the number of non-zero votes before noise level 0.85 (or as ≤ 0.85) is not negligible (we
say ≥ 4 in this paper), QD could potentially possess high noise level, as “significant” number of LID
sequences from as ≤ 0.85 can be recognized as similar to the baseline’s by Φ. Experiments show
that when D has high noise level, triples with low recall (≤ 0.92) can give better estimate while if D
has low noise level, then high recall (≥ 0.98) is preferred. Therefore, based on recall above 0.9, we
further narrow down the range of recalls of the triples to do fine-tuning. Let the new range of recalls
be R.
29
Under review as a conference paper at ICLR 2021
We then re-consolidate the LID sequences with the new range of α-sequence according to R to
finetune Φ. Re-consolidation of LID sequences is necessary, because different sets of recalls require
different α-sequences to estimate QD . The higher the recall, the smaller the range of possible vectors
we are allowed to use in the α-sequence. If an inappropriate α-sequence is used, then the matrix QD
could have illegal entries (i.e. with values < 0 or > 1). During the initial training, for CIFAR-10,
LID sequences are consolidataed with a(s) ≤ 0.883, then we select triples with recall above 0.9. For
Clothing1M, the maximum α used for initial training has noise rate 91.6%. Now with refinement,
the range of recall is narrower, the respective range of LID sequences would have to be adjusted for
later fine-tuning.
With the re-consolidated LID sequences, we can fine-tune Φ for the same triple, T, for five times.
Let the new recall during fine-tuning be τ0 and the old recall in the initial training be τ . If τ0 = τ for
at least 3 times, we can stop fine-tuning. Else, we abandon this triple. And now the discriminator
for T is considered well-trained, which comes together with τ0 and the new vote sequence. In the
new vote sequence, the a§ with the highest number of vote sum is denoted as top-voted a*. T0 and
top-voted a* would be used for QD estimation.
For each D, there shall be at least 2 discriminators well-trained. Otherwise, more random seeds are
required to synthesize DILNs ofD, generate LIDs and train Φ.
C.3 FINAL COMPUTATION TO ESTIMATE QD
C.3.1 Priors
To learn QD, a prior, P, is used to help define the structure of QD. For CIFAR-10, we used ResNet-
18 (He et al., 2016) to train on 90% of the given noisy dataset for 20 epochs and validate on the
10% instances. For Clothing1M, we used ImageNet pretrained ResNet18 with an extra fc layer (512
hidden units) to train and validate it for 10 epochs. For respective datasets, the optimization procedure
used are the same as given in (Patrini et al., 2017). The probabilities of the training instances from
the softmax layer are recorded. The probabilities from the epoch with the highest validation accuracy
is used to compute a prior. Let Υ(yn = i)j be the j-th output from the neural network’s softmax
layer for some sample with label i, and let the set of samples with label i be Si, i.e.
Pij = ISI nX∈Xi γ(yn)j
C.3.2 Further simplifications in modeling
Consider an arbitrary α ∈ [0,1]k. Recall that when We equate the random matrices QD = QDa, We
get the following equation for each entry:
1 (1 + E0j) = qij (1 - α*)+ X qi,tα* ɪ (1 + E0j) (for all 1 ≤ i,j ≤ k)	(40)
1≤t≤k
t6=j
To simplify computations in our estimation, for each row, we estimate only one entry qi,j and the
remaining entries shall be proportional to 1 - qi,j according to a prior. This reduces the estimation
“burden” from k2 entries to k. The coordinate (i,j) is determined by the maximum entry’s location of
the i-th row in the prior: j = argmaxm pi,m . The entry qi,t is substituted with
Pjk(I-%j).
Next, We estimate Eij and Ei,j. For D, let the binary random variable Xn be 1 if the n-th sample
has been assigned a wrong label. The total number of instances in D is N. Hence, Xn is a Bernoulli
random variable with parameter k-1 and the noise level in D is PN=I XNn ≈ k-1 + ED, where
ED is a random variable that approximates the relabeling process. By the central limit theorem, we
infer that PN=I XNn is approximately normal with mean k-1 and variance N-2. Let Z be a random
variable following standard normal distribution. Normalizing, we get:
30
Under review as a conference paper at ICLR 2021
Let the recall from the well-trained discriminator be T0, where T0 = Pr(-θ ≤ Z ≤ θ), or 1-τ =
P(z ≤ -θ). From the standard normal distribution table, θ can be found when τ0 is defined.
Therefore ED = -θ JNk. We make a simplification that 1 Eij ≈ ED for all (i,j). Both random
variables Ei0,0j and Ei0,j are assumed to follow multivariate normal distributions, with unknown
parameters to us. To further simplify the estimation, we model both random variables with a common
random variable “E”. This simplification has been shown reasonable by experiments:
and
Ei0,j = -(k - 1)E
Ei00,j = (k-2)E
Suppose R = {r1, . . . , rm}. For each 1 ≤ s ≤ m, let Is denote the set of all estimates with
associated recall rs. Therefore, our final estimate for QD is:
m
QD=X
s
Ei")
∣isl
D Relation to the information bottleneck theory for deep
LEARNING
The momentous work on the Information Bottleneck (IB) theory for deep learning (Tishby &
Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017) has spurred much interest and discussion; see (Gabrie
et al., 2018; Saxe et al., 2018). A key aspect proposed by the IB theory is that the training of a deep
neural network (DNN) consists of two distinct phases: An “expansion” phase where the mutual
information between layers increase, and a “compression” phase, where the mutual information
between layers decrease. The notion of local intrinsic dimensionality (LID), which we used as an
essential ingredient for a concrete realization of our proposed framework, was in part motivated by
this IB theory. It was reported in (Ma et al., 2018c) that for their experiments on training DNNs, the
LID scores of the training epochs also exhibits a similar two-phase phenomenon: An initial decrease
in LID score, then either an increase in LID score or a stagnant LID score, depending on whether
there is label noise or not, respectively. As part of our experiments, we synthesized multiple datasets
with varying entropy, and generated multiple LID sequences for each synthesized dataset. Although
our focus is on the estimation of the noise transition matrices of datasets with instance-independent
label noise, a by-product of our work is that we observed a very wide spectrum of behavior for LID
sequences, some of which the two-phase phenomenon is not obvious. Notice that in Fig. 3, we have
included multiple plots of LID sequences at various entropies, which may be of independent interest
to other researchers (especially those directly working on the IB theory). A caveat is that LID scores
are distinct from the mutual information between layers, but LID scores could still be interpreted as a
measure of model complexity, which presumably have close connections to the generalizability of
deep learning.
31
Under review as a conference paper at ICLR 2021
The given dataset D: CIFAR-IO intact
noise rate 85% max ----noise rate 85% min noise rate 87.80% max
The given dataset D: CIFAR-IO with 70% of
anchor-like instances removed
noise rate 0% max
noise rate 0% min
noise rate 60% max
noise rate 60% min
noise rate 80% max
noise rate 80% min
noise rate 85% max
noise rate 87.80% min
noise rate 85% min
baseline max
noise rate 87.80% max
baseline min
noise rate 0% max
noise rate 0% min
noise rate 60% max
noise rate 60% min
noise rate 80% max
noise rate 80% min
noise rate 87.80% min baseline max
baseline min
Figure 3: LID sequences visualization for clean CIFAR-10 intact as D (top plot) and clean CIFAR-10
with 70% of anchor-like instances removed as D (bottom plot). The x-axis represents epochs, and
the y-axis represents individual LID scores (at each epoch). Notice that for each considered noise
level a (corresponding to α = (a, . . . , a)), we have a coordinate-wise minimum-to-maximum range
of the entries for sequences generated using α = (a, . . . , a), where these sequences associated to Dα
are repeatedly generated using multiple random seeds. We call this coordinate-wise minimum-to-
maximum range a ”band”. Blue bands represent LID sequences associated to α-increment DILNs
with noise levels 0%, 60%, 80%, 85% and 87.8% (from lightest blue to darkest blue). Red bands
represent LID sequences associated to baseline DILNs.
32