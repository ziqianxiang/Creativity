Under review as a conference paper at ICLR 2021
The unreasonable effectiveness of the class-
reversed sampling in tail sample memoriza-
TION
Anonymous authors
Paper under double-blind review
Ab stract
Long-tailed visual recognition poses significant challenges to traditional machine
learning and emerging deep networks due to its inherent class imbalance. A com-
mon belief is that tail classes with few samples cannot exhibit enough regularity
for pattern extraction. What makes things worse, the limited cardinality may lead
to low exposure of tail classes in the training stage. Re-sampling methods, espe-
cially those who naively enlarge the exposure frequency, eventually fail with head
classes under-represented and tail classes overfitted.
Arguing that long-tailed learning involves a trade-off between head class pattern
extraction and tail class memorizing, we first empirically identify the regularity of
classes under long-tailed distributions and find that regularity of the same training
samples will be sharply decreased with the reduction of class cardinality. Moti-
vated by the recent success of a series works on the memorization-generalization
mechanism, we propose a simple yet effective training strategy by switching from
instance-balanced sampling to class-reversed sampling to memorize tail classes
without seriously damaging the representation of head classes. Closely after-
wards, we give the theoretical generalization error upper bound to prove that class-
reversed sampling is better than instance-balanced sampling during the last train-
ing stage. In our experiments, the proposed method can reach the state-of-the-art
performance more efficiently than current methods, on several datasets. Further
experiments also validate the superior performance of the proposed sampling strat-
egy, implying that the long-tailed learning trade-off could be effectively tackled
only in the memorization stage with a small learning rate and over-exposure of
tail samples.
1	Introduction
With the prosperity of deep learning research field, visual recognition has witnessed the prominence
of powerful representation learning approaches and high-quality, large-scale datasets, e.g., ImageNet
ILSVRC ( Russakovsky et al. (2015)) and Places ( Zhou et al. (2018)). These datasets are usually
carefully balanced, exhibiting roughly uniform distributions of class labels. However, visual phe-
nomena in real world tend to have skewed data distributions with long-tailed characteristics (Dong
et al. (2017); Xiang & Ding (2020)), consisting of a few majority classes (head classes) and a large
number of minority classes (tail classes). When dealing with such long-tailed data, many standard
approaches fail to work well due to the extreme class imbalance trouble, leading to a significant drop
in accuracy for tail classes.
In this paper, we propose to investigate long-tailed visual recognition from a memorization-
generalization point of view. A recent work of Feldman (Feldman (2020)) introduced a new the-
oretical explanation suggesting that memorization is necessary for achieving close-to-optimal gen-
eralization when the data distribution is long-tailed, since useful samples from tail classes can be
statistically indistinguishable from the useless one. To verify the impact of cardinality on each class
during training, we firstly visualize the cumulative learned events and forgetting events of each
sample (see Section A.4) and find that the long-tail challenge is essentially a trade-off between the
representation of high-regularity head classes and generalization to low-regularity tail classes.
1
Under review as a conference paper at ICLR 2021
Conventional re-sampling methods, which tend to naively change the exposure frequency of each
class to generate class-balanced data ( Han et al. (2005); Buda et al. (2018)), are generally thought
to be flawed since model may suffer from head classes being under-represented and tail classes
overfitted (Zhang et al. (2019); Ye et al. (2020)). One recent study ( Kang et al. (2020)) has sug-
gested that the learning of feature representations and classifiers should be completely decoupled
and different training stages need different data samplers. They thus train the feature extractor with
instance-balanced sampling first and then use class-balanced sampling to re-train the classifier with
a fixed backbone. This approach is intuitive, and has proven empirically successful. However, it is
not without limitations: For instance, training the feature extractor with only instance-balanced sam-
pling means head classes will always dominate the training procedure, leading under-representation
for tail classes ( Zhang et al. (2019); Zhou et al. (2020)). On the other hand, the re-trained classifier
may be sub-optimal if the feature representation is sub-optimal (see Section A.6).
To challenge the Decouple (Kang et al. (2020)) hypothesis, we propose an end-to-end method by
properly combining different samplers to overcome these limitations of complete decoupling. Our
method is motivated by the memorization-generalization phenomenon in deep learning (Jiang et al.
(2020)), which indicates that rare and low-regularity samples could be able to be learned based on
the internal representations learned from strongest-domain regularities first. We argue that properly
shifting learning focus from high-regularity head classes to low-regularity tail classes is unreason-
ably effective and we provide intuitive insight into solving long-tailed problems. To support this, we
explore a joint training strategy which performs well without decoupling.
Specifically, we employ the standard training procedure with cross-entropy loss and instance-
balanced sampler w.r.t. the original data distribution to ensure the learning of universal visual pat-
terns. We only switch from instance-balanced sampler to class-reversed sampler for the last several
epochs of training, when tail classes tend to be over-exposed. In earlier training, when head classes
dominate the training data, the patterns and structures discovered in the regular examples are utilized
to build a generalizable representation. In later training phase, the memorization of tail classes will
not seriously disrupt the learned representation as the learning rate is much smaller than in earlier
stages. Such a training strategy can simultaneously boost the representation and classification to-
wards long-tailed distributions, avoiding the risk of excessive dependence on the feature extractor
(see Section A.6).
We conduct extensive experiments across four benchmark long-tailed datasets, CIFAR10-LT,
CIFAR100-LT, iNaturalist 2018 and ImageNet-LT, to evaluate the effectiveness of our proposed
method. With such a simple training strategy, we obtain compare or better results and more efficient
compared with previous state-of-the-art methods.
To summarize, the main contributions of this paper are as follows:
•	We empirically identify that the low regularity of tail classes is the primary hurdle for
learning an accurate model for long-tailed distributions and appropriately memorizing them
is essential for better generalization across all classes.
•	We propose a simple yet effective switching strategy to handle the trade-off between high-
regularity head classes and low-regularity tail classes, and give the theoretical general-
ization error bound proving that class-reversed sampling is better than instance-balanced
sampling during the last training stage.
•	We investigate the effectiveness and efficiency of the proposed method through extensive
experimentation and demonstrate that the tackling the long tail trade-off problem could only
cost a few training epochs with a small learning rate and over-exposure of tail samples.
2	Related Work
2.1	Long-tailed visual recognition
Here we mainly introduce the re-sampling methods, other types of long-tailed recognition methods
can be found in Section A.2.
Re-sampling strategies. Re-sampling strategies can be divided into two classical types: over-
sampling the minority classes by repeatedly adding augmented images ( Drummond et al. (2003);
2
Under review as a conference paper at ICLR 2021
Han et al. (2005); Buda et al. (2018) ); or under-sampling the majority classes by removing sev-
eral images ( Japkowicz & Stephen (2002); He & Garcia (2009)). All these re-sampling methods
tend to provide a more balanced data distribution during training to solve the long-tailed problem.
However, sometimes, over-sampling may cause over-fitting towards minority classes, while under-
sampling may weaken the representation ability of networks.
Different from these, we do not artificially generate class-balanced batches or losses; instead, we
simply emphasize the memorization of low-regularity tail class samples by only switching from the
instance-balanced sampler to class-reversed sampler during the standard training procedure.
2.2	Memorization-generalization mechanism in deep learning
Memorization was once considered a failure of deep networks since it implies a lack of general-
ization. However, the view that memorization is harmful may be a misunderstanding towards deep
learning. Zhang et al. (2017) were the first to demonstrate that standard deep learning algorithms
can achieve high training accuracy even on large and randomly labeled datasets, generating a large
wave of research interest in the topic of generalization for deep learning. Toneva et al. (2019) in-
troduced the “forgetting event” to describe the learning dynamics of neural networks, where some
instances flip flop between “learned” and “forgotten” states during training. In order to analyze how
individual instances are treated by a model on the memorization-generalization continuum, Jiang
et al. (2020) proposed the C-score to measure the consistency of a sample with respect to the rest of
the training set. They found that samples having lower C-scores are learned more slowly, indicating
the need for a stage-wise learning rate schedule during training.
A recent work of Feldman (2020) proposed a new theoretical explanation for the benefits of mem-
orization. In their abstract model, algorithm can only get the frequency of a subpopulation through
the empirical frequency of its representatives, thus it can only avoid the risk of missing subpop-
ulations with significant frequency by memorizing examples. Further, Feldman & Zhang (2020)
introduced the influence estimation to validate the necessity of memorizing useful examples for
achieving close-to-optimal generalization error.
Different from them, we first empirically identity the correlation between cardinality and regularity
under long-tailed distributions and propose a simple yet effective method to boost the performance
of long-tailed visual recognition via memorizing low-regularity tail class samples.
3	Method
Long-tailed visual recognition follows a long-tailed distribution over classes, leading the model to
exhibit under-fitting on tail classes and over-fitting to head classes. Since increasing the exposure of
tail classes may lead to over-fitting while under-sampling head classes may weaken the representa-
tion ability of networks, the trade-off between the representation of head classes and generalization
towards tail classes becomes the main dilemma in long-tailed problem. Here we propose to resolve
this dilemma by only switching the standard instance-balanced sampler to a class-reversed sampler
during the last training procedure, in order to learn low-regularity samples (tail classes) without
seriously disrupting the representation of the strongest domain regularities (head classes) first.
3.1	Problem Setup and Notations
Let fθ(∙) denote a feature extractor implemented by a CNN model with parameter θ, We get the
class prediction through y = arg max g(fθ(x)), where X is the input image and g(∙) is a classifier
function. Given a training set D = {xi, yi}, i ∈ {1, ..., n} with C classes, let nj denote the number
of samples for class j and n = PiC=1 ni be the total number of samples. Without loss of generality,
we assume that the classes are sorted by cardinality in decreasing order, i.e., if i < j, then ni ≥ nj.
For most sampling strategies, the probability pj of sampling a data point from class j is given by:
pj
q
n
PC n,
i=1 ni
(1)
with different values of q arise for different sampling strategies. The sampling of each data can be
capsuled into the following two steps: 1) Randomly sample a class according to pj; 2) Uniformly
3
Under review as a conference paper at ICLR 2021
pick up a sample from class j . Sampling strategies that correspond to q = 1, q = 0, and q = -1 are
introduced as below:
Instance-balanced sampling (IB). This is the most common and standard way of sampling data,
where each sample of the training dataset is sampled only once with equal probability in a training
epoch. For instance-balanced sampling, the probability pjIB is given by Equation 1 with q = 1, i.e.,
a sample from class j will be sampled proportionally to the cardinality nj of the class.
Class-balanced sampling (CB). To alleviate the extreme data imbalance during training, class-
balanced sampling is proposed to artificially generate class-balanced data. The probability pjCB is
given by Equation 1 with q = 0, e.g., pjCB = 1/C. In this scenario, the probability of each class j
being selected is equal, independent to its cardinality nj .
Class-reversed sampling (CR). Zhou et al. (2020) firstly propose the reversed sampler to re-
balance feature representation and particularly improve the classification accuracy on tail classes.
Here we integrate pjCR into Equation 1 with q = -1. For class-reversed sampling, a data point from
class j will be sampled proportionally to the reciprocal of its cardinality nj , i.e., the more samples
in a class, the smaller sampling possibility that class has.
The sampling weights of IB, CB and CR are visualized in Figure 1(a).
3.2	Switching Data Samplers during Training
Switching is proposed to shift the learning focus from head classes to tail classes by simply switching
the IB sampler to CR sampler at some epoch during training. Before the switching happens, the
uniform IB sampler retains the characteristics of original distributions and almost the high-regularity
samples from head classes are learned, the patterns and structures discovered in those head class
samples can be used to build a generalizable representation. In later stages, the memorization of
tail class samples will not seriously disrupt the learned representation as the learning rate is much
smaller than the earlier stages.
Concretely, the number of total training epochs is denoted as T and the learning rate milestones are
denoted as [m1, ..., mn], where m1 < ... < mn ≤ T. Let γ ∈ (0, 1) becomes the multiplicative
factor, learning rate will be decayed by γ once the epoch reaches one of the learning rate milestones
during training. When training procedure reaches the mn + S epoch, we switch IB sampling to CR
sampling and continuing training, where S is the hyper-parameter in our method indicating when to
switch. The details of our switching strategy is shown in Algorithm 1.
Our method is simple and clean, which only switches the data sampler from IB to CR during train-
ing, without changing any structure of the original network or artificially generating class-balanced
training batches or losses.
Algorithm 1 Hard Switching From IB to CR
Input: D: Training Set; M : Mini-batch size;
Input: T: Total epochs; [m1, ..., mn]: Learning rate decay milestones;	S: Switch epoch;
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
n-1
initial PCR = PCjn-I ；
for t ∈ [1, T] do =
B = {};
if t ≤ mn + S then
Randomly sample M samples from D as B.
else
for i ∈ [1, M] do
Sample one class j from the D according to pjCR.
Uniformly pick up a sample {xi , yi} from class j without replacement.
B = {B; {xi, yi}}.
end for
end if
Optimize network by SGD based on B .
end for
4
Under review as a conference paper at ICLR 2021
3.3	Theoretical Analysis
In this section, we answer the following three questions: (1) what is the objective function of the
optimization, (2) what is the generalization error upper bound of different sampling strategies, and
(3) why CR is better than IB during the last training stage with small learning rate.
Objective function. Let Lji (θ) denote the standard training error on the ith sample of class j:
Lji (θ) = `(fθ (xji) , yji) ,	(2)
where ` is the loss function, e.g., cross-entropy loss.
For the standard training process with IB sampling, where each sample of the training dataset is
sampled with equal probability, the object function over the total training set D is given as follows:
1n
Ls(θ) = - ∑Li(θ)+ R(θ),	(3)
where R(θ) is the regular terms.
Now considering a more general scene, where sampling a data containing two steps: 1) Randomly
chooses one class according to pj ; 2) Uniformly pick up one sample from its nj samples, we have:
C nj
L(θ)= XX pjLji(θ) + R(θ)∙	(4)
j=1 i=1 j
Generalization error bound. Now we give the generalization theory of such a objective func-
tion. Let Θ be the family function of our learned neural network, we define Rn(Θ) as the standard
Rademacher complexity (Bartlett & Mendelson (2002)) of the set {(x, y) 7→ `(f (x; θ), y) : θ ∈ Θ}:
-n
<n(Θ) = ED,ξ SUP- ξΓ(fθ (fθ (Xji) ,yji) ,	(5)
θ∈Θ n i=1
where ξ1, . . . , ξn are in-dependent uniform random variables taking values in {-1,1} (i.e.
Rademacher variables).
Let M denote the least upper bound on the difference of individual loss values:
l'(fθ(x), y) - ' (fθ (x0), y0)∣ ≤ M for all θ ∈ Θ. For the standard training process with Ls(θ),
for any δ > 0, with probability at least - - δ over the training set D, the following error bound holds
for all θ ∈ Θ (Kawaguchi & Lu (2020)):
Esx,y)['(fθ (x),y)] ≤ Ls(θ)+2Rn(Θ)+ M Jln2pδ) .	⑹
For the general objective function Lθ, for any δ> 0, with probability at least - - δ over the training
set D, we have the following error bound for all θ ∈ Θ (the proof is given in Section A.1):
E(x,y)['(fθ (x),y)] ≤ L(θ) + 2<n(Θ) - Qn(Θ; p,p) + M
ln(l∕δ)
2
(7)
where Qn(Θ;p,n)	=	ED 卜nfθ∈θ PC=I Pn= ι 倍-1) '(fθ (Xji) ,yji)卜
Theorem 1. With a small size of Θ (the last training stage with small learning rate) and a bounded
M, the upper bound on the expected error for CR sampling is strictly lower than IB sampling if
Qn(Θ; p, n) + Ls — L > 0 or if Ls - L > 0 (the proof is given in Section A.1).
4	Experiments
To investigate the effectiveness and the efficiency of our proposed approach, we conduct several
qualitative and quantitative experiments on imbalance controlled datasets like long-tailed CIFAR-
10 and CIFAR-100 as well as large-scale real-world long-tailed dataset, i.e. iNaturalist 2018 and
ImageNet-LT, as described in Section A.3.
5
Under review as a conference paper at ICLR 2021
4.1	Comparison with the state-of-the-art on long-tailed datasets
In this section, we compare the performance of the proposed scheme to other recent works that re-
port state-of-the-art results on four common long-tailed benchmarks: Long-tailed CIFAR-10, Long-
tailed CIFAR-100, iNaturalist2018 and ImageNet-LT.
Table 1: Top-1 accuracy ofResNet-32 on long-tailed CIFAR-10 and CIFAR-100. Rows with f denote
results directly copied from ZhoU et al. (2020). * denotes results reproduced with the authors code.
Dataset	∣ Long-tailed CIFAR-10 ∣ Long-tailed CIFAR-100
Imbalance ratio	100	50	10	100	50	10
Cross Entropyf	70.4	74.8	86.7	38.3	43.9	55.7
Cross Entropy*	73.1	77.9	86.4	40.7	44.9	57.2
Focalf (Lin et al. (2017))	70.4	76.7	86.7	38.4	44.3	55.8
CE-DRWf (Cao et al. (2019))	76.3	80.0	87.6	41.5	45.3	58.1
CE-DRSf (Cao et al. (2019))	75.6	79.8	87.4	41.6	45.5	58.1
CB-Focalf (Cui et al. (2019))	74.6	79.3	87.1	39.6	45.2	58.0
LDAM-DRWf (Cao et al. (2019))	77.0	81.0	88.2	42.0	46.6	58.7
Decouple-cRT* (Kang et al. (2020))	73.8	80.7	86.7	40.1	46.4	57.7
Decouple-LWS* (Kang et al. (2020))	73.5	77.5	86.1	40.2	45.7	58.1
BBNf (Zhou et al. (2020))	79.8	82.2	88.3	42.6	47.0	59.1
Ours (S = 1)	79.7	82.9	88.4	44.7	49.5	59.5
Table 2: Top-1 accuracy of ResNet-50 on iNat-
uralist 2018. Rows with fdenote results directly
copied from their original paper. We present re-
sults when training for 90 / 200 epochs.
Dataset	iNaturalist 2018
Cross Entropyf	57.2
CE-DRWf	63.7
CE-DRSf	63.6
CB-Focalf	61.1
LDAM-DRWf	68.0
Decouple-cRTf	65.2 / 67.6
Decouple-LWSf	65.9 / 69.5
BBNf	66.3 / 69.6
Ours (S =1/S = 1)	66.7 / 70.4
Ours (S = 10/ S = 40)	66.8 / 70.0
Table 3: Top-1 accuracy of on large-scale long-
tailed datasets ImageNet-LT for different back-
bone architectures. Rows with fdenote results di-
rectly copied from Kang et al. (2020). * denotes
results reproduced with the authors code.
Method	ResNet-50	ResNext-50
Cross Entropyf	41.6	44.4
OLTRf (Liu et al. (2019))	-	37.7
Decouple-cRTf	47.3	49.5
Decouple-LWSf	47.7	49.9
BBN*	45.9	47.1
Ours (S = 1)	47.9	49.2
Ours (S = 10)	48.2	49.3
Long-tailed CIFAR. We conduct extensive experiments on long-tailed CIFAR datasets with three
different imbalanced ratios: 10, 50 and 100. Table 1 reports the accuracy of various methods. For
CIFAR-10 series, our method achieves comparable or better results comparing other complicated
methods. When working on CIFAR-100 series, our method achieves the best results across all
imbalance ratios, compared with the two-stage fine-tuning strategies (i.e., CE-DRW/CE-DRS) and
also previous state-of-the-arts (i.e., Decouple (Kang et al. (2020)) and BBN (Zhou et al. (2020))).
Especially for long-tailed CIFAR-100 with imbalanced ratio 100 (the most extreme imbalance case),
we get 44.7% accuracy which is 2.1% higher than BBN (Zhou et al. (2020)).
iNaturalist 2018. We further evaluate our methods on the iNaturalist 2018 dataset. Similar to
Kang et al. (2020) and Zhou et al. (2020), we present results training after 90 and 200 epochs.
From the Table 2 we can see, we surpass other complicated methods even with a plain ResNet-50
model. When S =1, where total training epochs are 10 epoch less than Decouple, we get 1.5% gains
compared with the totally decouple training strategy cRT. We can achieve further improvements with
the same training epochs as Decouple (see S = 10 and S = 40).
6
Under review as a conference paper at ICLR 2021
ImageNet-LT. Table 3 presents results for ImageNet-LT. We present results for two backbones:
ResNet-50 and more powerful ResNext-50. The results of BBN are conducted using the author’s
open-sourced codebase. From the table we see that our simple method, without bells and whis-
tles outperform the current state-of-the-art for ResNet-50, about 0.9% higher than Decouple and
2.1% higher than BBN. With more powerful backbone, our method performs slightly worser than
Decouple, while still surpasses the more complicated architecture BBN about 2.2%.
Fine-grained analysis To better validate our assumption that memorizing low-regularity samples
with small learning rate can avoid seriously damage the representation of high-regularity samples,
we further report accuracy on three splits of the set of classes: Many-shot (more than 100 images),
Medium-ShotQ0〜100 images) and Few-shot (less than 20 images).
As shown in Table 4 and Table 5, standard training process (see Cross Entropy) always perform best
on Many-shot since the head class samples dominate the training batch all the time. Meanwhile, our
method can improve the performance of tail classes by a large margin due to the CR sampling in the
last training stage. It is worth to note that while greatly boosting the recognition of tail classes, our
switching method only sightly damage the performance of head classes (compared with Decouple
and BBN), indicating that memorizing tail class samples with small learning rate can better handle
the trade-off between high-regularity head classes and low-regularity tail classes.
Table 4: Comprehensive results on the most skewed long-tailed CIFAR-100 (imbalance ratio: 100).
Method	Many-shot	Medium-shot	Few-shot	All
Cross Entropy	68.2	39.7	9.9	40.7
Decouple-cRT	58.1	40.3	18.0	40.1
Decouple-LWS	59.5	40.7	17.4	40.2
BBN	54.5	51.0	16.7	42.6
Ours (S = 1)	57.1	48.1	26.5	44.7
Table 5: Comprehensive results on ImageNet-LT with different backbone networks (ResNet-50,
ReSNext-50).__________________________________________________________________
Method	Many	ResNet-50		All	ResNext-50			All
		Medium	Few		Many	Medium	Few	
Cross Entropy	64.0	33.8	5.8	41.6	65.9	37.5	7.7	44.4
Decouple-cRT	58.8	44.0	26.1	47.3	61.8	46.2	27.4	49.5
Decouple-LWS	57.1	45.2	29.3	47.7	60.2	47.2	30.3	49.9
BBN	56.2	46.6	14.1	45.9	58.1	47.2	15.6	47.1
Ours (S = 1)	53.5	45.2	41.8	47.9	55.2	46.7	39.0	49.2
Ours (S = 10)	54.5	45.0	41.9	48.2	56.5	46.1	39.4	49.3
4.2	Ablation Study
To find the optimal setting of S, which is the hyper-parameter controlling when to switch, we in-
vestigate the S value and corresponding results are shown in Table 6. Interestingly, our method
achieves comparable results despite different values of S, indicating S is not dataset/distribution
dependent or sensitive. This is consistent with our motivation: memorization of tail classes will
not seriously disrupt the learned representation with smaller learning rate. Thus, once there is a CR
sampling during the small learning rate training, model could jointly fine-tune both feature extractor
and classifier to achieve better generalization, regardless of the specific value of S.
We also investigate the progressively switching in Table 7. For the first CB then CR setting, the
results are almost the same as only CR, showing switching strategy is robust to the samplers used in
earlier stages. However, first CR then CB will lead a great drop in accuracy, indicating memorizing
low-regularity samples of tail classes should happen in the last training stage.
7
Under review as a conference paper at ICLR 2021
Table 6:	Determining of the optimal S on
long-tailed CIFAR-10 (imbalance ratio: 50)
and CIFAR-100 (imbalance ratio: 50).
S CIFAR-10 CIFAR-100
Table 7:	Determining of switching strategies on
long-tailed CIFAR-10 (imbalance ratio: 50).
First	Switching
Sampler	Sampler	Accuracy
0	82.6	49.7	CR	82.9
1	82.9	49.5	IB	CB for 5 epochs, then CR	82.7
5	82.9	49.3	CR for 5 epochs, then CB	79.2
10	82.8	49.1		
50	82.6	49.1		
-InStanCe-balanced Sampling
Class-balanced Sampling
-class-reversed Sampling
1.0
0.8
0.6
0.4
0.2
0.0
class index
(a) samling weights
0	25	50	75	100 125 150 175 200
training epoch
(b) test accuracy
(a) standard
Figure 2: Learning speed of examples of 4 selected classes with SGD using stage-wise constant
learning rate. Left: standard training procedure. Right: our switching training procedure.
Figure 1: Sampling weights and test performance of three methods with different sampling strategies
on long-tailed CIFAR-10 with imbalance ratio 50. Left: instance-balanced, class-balanced and class-
reversed sampling. Right: standard cross entropy, BBN and our switching.
(b) ours
4.3	Learning Speed
In order to further validate our method could learn long-tailed distributions more efficiently, we
plot the test accuracy per epoch of three methods with different sampling strategies in Figure 1(b).
Compared with using IB only, switching to CR can immediately improve the performance by a large
margin. Meanwhile, although BBN could achieve comparable performance with ours, it converges
more slowly since it optimizes two branches of feature extractor in turn during training.
Intuitively, a training example from head classes should be learned quickly since it is consistent with
many others and the gradient steps for all consistent examples should be well aligned. As Jiang et al.
(2020) indicates that strong regularities in a data set are not only better learned at asymptote leading
to better generalization performance but are also learned sooner in the time course of training, we
8
Under review as a conference paper at ICLR 2021
conjecture that head class samples will be learned sooner than tail class samples and plot average
proportion correct as a function of training epoch for each class to validate it.
As shown in Figure 2, all examples are learned during training, where jumps in the graph correspond
to points at which the learning rate is reduced. However, interestingly, the tail class samples are
learned most slowly and head class samples are learned most quickly. Indeed, learning speed of
each class is monotonically related to its cardinality. An interesting observation from the learning
speed plot in Figure 2 is that the stage-wise learning rate decay has a greater impact for tail class
examples. To explore this phenomenon further, we trained models with constant learning rates of
0.1, 0.02, 0.01, and 0.001, results are shown in Figure 6 and Figure 7 in Appendix. In addition,
our switching strategy can significantly accelerate the learning of tail classes based on the internal
representations with a slight damage to head classes, indicating the dilemma of trade-off between
head class pattern extraction and tail class memorizing in long-tailed learning.
5	Discussion
To further reveal the mechanism of the proposed method, more empirical studies on the proposed
method are investigated. Our findings and take-home messages are as follows:
1.	Why effective? As shown in Figure 3 and Figure 4, reductions on class size lead to
both lower regularity and lower exposure in the training stage (more evidences in Sec-
tion A.4). According to Jiang et al. (2020), a training stage with a relatively small learning
rate may benefit for low-regularity sample memorization. Therefore, the intuitive combi-
nation of such sampling stage and certain frequency increasing re-sampling strategy for
low-regularity classes naturally comes up.
However, evidences in Table 9 show the clear superiority of the CR sampler over other sam-
pling method in the latter stage, implying a mild frequency increasing for under-represented
class may not reach the balanced tradeoff. Given that class regularities share most corre-
lation with class number, CR sampling is the near optimal choice when class regularities
are agnostic. In fact, Table 10 shows that performance of class-reversed sampling and
class-regularity-reversed1 sampling differs slightly.
2.	Need decoupling? Decouple proposed by Kang et al. (2020) emphasizes the advantage of
classifier re-training for long tail learning, which indeed exists in the standard training pro-
cedure. However, evidences in Section A.6 illustrates that the quality of the classification
is inherently restricted by the previously learned representations. When tail samples are
better represented, the decoupling between representation learning and classifier learning
seems futile (more evidences in Table 12).
3.	Why efficiency? The efficiency of the proposed method is obviously contributed by its
parsimony, i.e. without any extra training stage like Kang et al. (2020) or extra sophisticated
structure like Zhou et al. (2020). According to Figure 1, the testing accuracy reaches
maximum with minimum epochs after the sampler switching.
Moreover interestingly, when extra switching to another sampler like CB is added 5 epochs
afterwards (see Table 7), the long-tailed classification performance drops significantly. In
this case, as long as the representation learning at large learning rates ends, our method
with switching strategy will end shortly afterwards while maintaining reasonable accuracy.
6	Conclusion
In this paper, we challenge the hypothesis by Kang et al. (2020) that the learning of feature repre-
sentation and classifier should be completely decoupled in long-tailed visual recognition problem
settings. Instead, we propose to switch the original instance-balanced sampling to class-reversed
sampling in mini-batch stochastic gradient descent at the last few training epochs for memorizing
tail samples. The presented approach exhibits unreasonable effectiveness and efficiency, leading to
the proposition that the decoupling paradigm seems futile when tail samples are better represented.
Further empirical findings show the inevitability to deal the trade-off between head class represent-
ing and tail class memorizing in the memorization stage with small learning rate.
1a data point from class j will be sampled proportionally to its irregularity rj, shown in Table 10.
9
Under review as a conference paper at ICLR 2021
References
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463-482, 2002.
Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, NIPS, pp. 6240-6249,
2017.
Mateusz Buda, Atsuto Maki, and Maciej A. Mazurowski. A systematic study of the class imbalance
problem in convolutional neural networks. Neural Networks, 106:249-259, 2018.
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and TengyU Ma. Learning imbalanced
datasets with label-distribution-aware margin loss. In Advances in Neural Information Processing
Systems 32: Annual Conference on Neural Information Processing Systems (NeurIPS), pp. 1565-
1576, 2019.
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie. Class-balanced loss based on
effective number of samples. In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR, pp. 9268-9277, 2019.
Qi Dong, Shaogang Gong, and Xiatian Zhu. Class rectification hard mining for imbalanced deep
learning. In IEEE International Conference on Computer Vision, ICCV, pp. 1869-1878, 2017.
Chris Drummond, Robert C Holte, et al. C4. 5, class imbalance, and cost sensitivity: why under-
sampling beats over-sampling. In Workshop on learning from imbalanced datasets II, volume 11,
pp. 1-8. Citeseer, 2003.
Vitaly Feldman. Does learning require memorization? a short tale about a long tail. In Konstantin
Makarychev, Yury Makarychev, Madhur Tulsiani, Gautam Kamath, and Julia Chuzhoy (eds.),
Proccedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing, STOC, pp.
954-959, 2020.
Vitaly Feldman and Chiyuan Zhang. What neural networks memorize and why: Discovering the
long tail via influence estimation. CoRR, abs/2008.03703, 2020.
Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, LUkasz Wesolowski, AaPo Kyrola,
Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ima-
genet in 1 hour. CoRR, abs/1706.02677, 2017.
Hui Han, Wenyuan Wang, and Binghuan Mao. Borderline-smote: A new over-samPling method in
imbalanced data sets learning. In Advances in Intelligent Computing, International Conference
on Intelligent Computing, ICIC Proceedings, Part I, volume 3644 of Lecture Notes in Computer
Science, PP. 878-887, 2005.
Haibo He and Edwardo A. Garcia. Learning from imbalanced data. IEEE Trans. Knowl. Data Eng.,
21(9):1263-1284, 2009.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. DeeP residual learning for image recog-
nition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR, PP. 770-
778, 2016.
Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deeP rePresentation for im-
balanced classification. In 2016 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR, PP. 5375-5384, 2016.
Muhammad Abdullah Jamal, Matthew Brown, Ming-Hsuan Yang, Liqiang Wang, and Boqing Gong.
Rethinking class-balanced methods for long-tailed visual recognition from a domain adaPtation
PersPective. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR,
PP. 7607-7616, 2020.
Nathalie JaPkowicz and Shaju StePhen. The class imbalance Problem: A systematic study. Intell.
Data Anal., 6(5):429-449, 2002.
10
Under review as a conference paper at ICLR 2021
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C. Mozer. Exploring the memorization-
generalization continuum in deep learning. CoRR, abs/2002.03206, 2020.
Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis. Decoupling representation and classifier for long-tailed recognition. In 8th Interna-
tional Conference on Learning Representations, ICLR, 2020.
Kenji Kawaguchi and Haihao Lu. Ordered SGD: A new stochastic optimization framework for
empirical risk minimization. In The 23rd International Conference on Artificial Intelligence and
Statistics, AISTATS, volume 108, pp. 669-679, 2020.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning.
CoRR, abs/1710.05468, 2017.
TsUng-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense
object detection. In IEEE International Conference on Computer Vision, ICCV, pp. 2999-3007,
2017.
Jialun Liu, Yifan Sun, Chuchu Han, Zhaopeng Dou, and Wenhui Li. Deep representation learn-
ing on long-tailed data: A learnable embedding augmentation perspective. In 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition, CVPR, pp. 2967-2976, 2020.
Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X. Yu. Large-
scale long-tailed recognition in an open world. In IEEE Conference on Computer Vision and
Pattern Recognition, CVPR, pp. 2537-2546, 2019.
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of Machine Learning.
Adaptive computation and machine learning. MIT Press, 2012.
Wanli Ouyang, Xiaogang Wang, Cong Zhang, and Xiaokang Yang. Factors in finetuning deep model
for object detection with long-tail distribution. In 2016 IEEE Conference on Computer Vision and
Pattern Recognition, CVPR, pp. 864-873, 2016.
Junran Peng, Xingyuan Bu, Ming Sun, Zhaoxiang Zhang, Tieniu Tan, and Junjie Yan. Large-scale
object detection in the wild from imbalanced multi-labels. In 2020 IEEE/CVF Conference on
Computer Vision and Pattern Recognition, CVPR, pp. 9706-9715, 2020.
Ashish Rastogi. McDiarmid’s Inequality. Springer US, 2011.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei
Li. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis., 115(3):211-252, 2015.
Mariya Toneva, Alessandro Sordoni, Remi Tachet des Combes, Adam Trischler, Yoshua Bengio,
and Geoffrey J. Gordon. An empirical study of example forgetting during deep neural network
learning. In 7th International Conference on Learning Representations, ICLR, 2019.
Yu-Xiong Wang, Deva Ramanan, and Martial Hebert. Learning to model the tail. In Advances in
Neural Information Processing Systems 30: Annual Conference on Neural Information Process-
ing Systems (NeurIPS), pp. 7029-7039, 2017.
Tong Wu, Qingqiu Huang, Ziwei Liu, Yu Wang, and Dahua Lin. Distribution-balanced loss for
multi-label classification in long-tailed datasets. CoRR, abs/2007.09654, 2020.
Liuyu Xiang and Guiguang Ding. Learning from multiple experts: Self-paced knowledge distillation
for long-tailed classification. CoRR, abs/2001.01536, 2020.
Han-Jia Ye, Hong-You Chen, De-Chuan Zhan, and Wei-Lun Chao. Identifying and compensating
for feature deviation in imbalanced deep learning. CoRR, abs/2001.01385, 2020.
Xi Yin, Xiang Yu, Kihyuk Sohn, Xiaoming Liu, and Manmohan Chandraker. Feature transfer learn-
ing for face recognition with under-represented data. In IEEE Conference on Computer Vision
and Pattern Recognition, CVPR, pp. 5704-5713, 2019.
11
Under review as a conference paper at ICLR 2021
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization. In 5th International Conference on Learning
Representations, ICLR, 2017.
Junjie Zhang, Lingqiao Liu, Peng Wang, and Chunhua Shen. To balance or not to balance: An em-
barrassingly simple approach for learning with long-tailed distributions. CoRR, abs/1912.04486,
2019.
Yaoyao Zhong, Weihong Deng, Mei Wang, Jiani Hu, Jianteng Peng, Xunqiang Tao, and Yaohai
Huang. Unequal-training for deep face recognition with long-tailed noisy data. In IEEE Confer-
ence on Computer Vision and Pattern Recognition, CVPR, pp. 7812-7821, 2019.
Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10
million image database for scene recognition. IEEE Trans. Pattern Anal. Mach. Intell., 40(6):
1452-1464, 2018.
Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. BBN: bilateral-branch network with
cumulative learning for long-tailed visual recognition. In IEEE Conference on Computer Vision
and Pattern Recognition, CVPR, pp. 9716-9725, 2020.
A Appendix
A. 1 Missing Proofs and Derivations in Section 3.3
Proof of the general upper bound
Given a long-tailed dataset D sampled from the main dataset S, we define:
Φ(D) = supE(χ,y)['(fθ(x),y)] - L(θ;D).	(8)
θ∈Θ
To apply McDiarmid’s inequality (Rastogi (2011)) to provide the upper bound on Φ(D), we first
show that Φ(D) satisfies the remaining condition of McDiarmid’s inequality. Let D and D0 be two
datasets differing by exactly one point of an arbitrary index i0, i.e., Di = Di0 for all i 6= i0 and
Di0 6= Di0 . Then, the upper bound on Φ (D0) - Φ(D) is given as follows:
Φ (D0) - Φ(D) ≤ sup L(θ; D) - L (θ; D0)
θ∈Θ
=SUp(X X nj L(θ; D)- XX nj L (θ; Dy)
θ	∈Θ j∈C i∈nj j	j∈C i∈nj j	(9)
≤	SUP " ILjiO (θ; D)- LjiO (θ; D0)∖
θ∈Θ nj
≤	pj M
nj
Therefore, ∖Φ(D) — Φ (D0)∖ ≤ jM since We also have Φ(D) — Φ (D0) ≤ jM. Thus, according
to McDiarmid’s inequality, for any δ > 0, with probability at least 1 - δ we have:
Therefore,
ED[Φ(D)]
Φ(D) ≤ ED [Φ(D)] + UX n2 rnT M.
(10)
ED Sup E(x,y) [` (fθ (x), y)] - Ls(θ; D) + Ls(θ; D) - L(θ; D)
θ∈Θ
≤ ED Sup E(x,y) [` (fθ (x) ,y)] - Ls (θ; D) - Qn (Θ; p,n)	(11)
θ∈Θ
1n
≤ Eξ,D,D0 sup n £ ξi (' (fθ (Xi) ,yi) - ' (fθ (Xi),为))一 Qn(Θ; p, n)
≤2Rn (Θ) - Qn (Θ; p, n).
12
Under review as a conference paper at ICLR 2021
where
Qn (Θ; p, n)
C nj
Ed	θnΘXX	nj -1 ` (fθ(xi) , yi)
∈	nn
j=1 i=1	j
(12)
Therefore, for any δ > 0, with probability at least 1 - δ we have:
Φ(D) ≤ 2<n(Θ)-Qn(Θ;p,n) + M
ln(1∕δ)
2
(13)
Substituting Equation 13 into Equation 8 we have:
E(x,y)['(fθ(x),y)] ≤ L(θ; D) + 2<n(Θ) - Qn(Θ;p,n) + M
ln(1∕δ)
2
(14)
Proof of the Theorem 1
We assume that:
1.	The range of Θ is narrow due to the learning rate is small and the network has converged
in the previous training process, so <n(Θ) → 0 as n → ∞, which has been shown to be
satisfied for various models and sets Θ (Bartlett & Mendelson (2002); Mohri et al. (2012);
Kawaguchi et al. (2017); Bartlett et al. (2017)).
2.	Without loss of generality, the classes are sorted by cardinality in decreasing order, thus
sampling weight pj of each class j is ordered. p1 is weight of the class with most samples
and pc is the weight of the class with least samples.
3.	Li ≤ Lj if i < j . This is an empirical conclusion that average loss of tail class samples
are always higher than its of head class samples with a model trained by instance-balanced
sampling.
Now let's compare the value of Esx。)['(fθ(x),y)] and E(χ,y) ['(fθ(x), y)]. Since both M JInynδ)
and M j~q L
will disappear as n → ∞, the core is to discuss the Qn (Θ; p, n).
We first consider the situation which only exchange the sampling rate for class 1 and class c under
the instance-balanced sampling (p1 > pc). Here we have:
C	nj
Xj=1Xi=1
n1	nc
X(竺-1)Lii + X(pi - 1)Lci
ni n	nc	n
i=i	i	i=i c
(Pc 1	p1	1 ∖~τ~
nι(-----)L1 + nc(----)Lc
n1 n
nc	n
(15)
(pc - P1)L1 + (pi - Pc)Lc
(pc - PI)(LI - Lc) > 0
Therefore, for high probability We can hold that Qn (Θ; p,n) > 0 if We only exchange the sampling
weight of the class 1 and class c. Naturally, Qn(Θ;p, n) > 0 will always hold if we exchange the
sampling Weight of class i and class j (i < j), Which is exactly hoW class-reversed sampling Works.
Now let’s promote our conclusion to more general situations, what will happen if we just change the
sampling weight of one class instead of exchanging? Let’s increase the pc from pc top0c, then every
13
Under review as a conference paper at ICLR 2021
Pj will change to Pj = Pj 1-pc due to the constraint PC=I Pj = 1, now We have:
C nj
X X (nj-1)
-n)ncLc+ɔ nj ⅛
 
—
1	—	CT
-)ncL； -T
n
j=1
Pc - Pc
1 - Pc
Pj Lj
—
C-1 0
≥ (Pc -Pc)Lc - E P~~PCPjLc-I
j=1 1 - Pc
≥ (Pc- Pc)L - Pc-^L-T X Pj
1 - Pc	j=1
≥ (Pc -Pc)Lc - ^C—— Lc-I(I -Pc)
c	1 - Pc
≥ (Pc - Pc)(Lc - Lc-I) ≥ 0
(16)
Therefore, for high probability we can hold that Qn (Θ; P, n) > 0 ifwe increase the sampling weight
of the last class, and we can extend it to any tail class similarly.
To sum up, we can draw the conclusion that Qn (Θ; P, n) > 0 holds if the sampling weight of tail
classes is increased. Thus, with n → ∞ and M is bounded, the upper bound on the expected error of
class-reversed sampling is strictly lower than that for instance-balanced sampling if Qn (Θ; P, n) +
Ls - L > 0 or if Ls - L > 0. Based on the empirical experience that Ls and L will always be very
close after training (no matter using only IB or only CR, the final training loss will always be small),
Ls-L → 0 holds after the complete training, thus Qn (Θ; p, n)+Ls-L > 0 ^⇒ Qn (Θ; p, n) > 0
holds.
A.2 Other related work in Long-tailed Learning
Re-weighting losses. Re-weighting methods usually allocate different weights for training samples
of each class to re-balance data distribution ( Huang et al. (2016); ?); Cao et al. (2019); Wu et al.
(2020)). Cui et al. (2019) assigns weights to each class based on the effective numbers of samples
instead of the proportional frequency. Further, Jamal et al. (2020) utilizes both effective numbers
( Cui et al. (2019)) and conditional weights to augment the classic class-balanced learning by ex-
plicitly estimating the differences between the class-conditioned distributions with a meta-learning
approach.
Transfer learning. Transfer learning methods ( Wang et al. (2017); Zhong et al. (2019); Yin et al.
(2019)) learn general patterns from head classes with abundant samples and transfer feature repre-
sentations to help to recognize tail class samples. Recently, Liu et al. (2020) constructs each feature
into a “feature cloud” to recover the intra-class diversity of tail classes. Xiang & Ding (2020)
splits the entire dataset into several cardinality-adjacent subsets and acquires knowledge from mul-
tiple models trained on those subsets. However, transfer learning methods are usually complicated,
which may be hard to applied to real-world scenarios.
Two-stage fine-tuning. Various methods (Ouyang et al. (2016); Cao et al. (2019); Liu et al. (2019);
Peng et al. (2020)) are proposed to modify re-balancing for further improvements in long-tailed
recognition. These methods usually separate training process into two single stages. In general,
they train the networks with instance-balanced sampling in the first stage and exploit re-sampling
or re-weighting methods at the second stage to fine-tune the network. More radically, Kang et al.
(2020) re-train the classifier from scratch in a class-aware manner in the second stage with backbone
fixed. Our work indicates that we can effectively learn the long-tailed distribution with only proper
sampling strategies combination.
14
Under review as a conference paper at ICLR 2021
A.3 Experiment Details
A.3.1 Datasets
Long-tailed CIFAR-10 and CIFAR-100. Both CIFAR-10 and CIFAR-100 contains 60,000 images,
with 50, 000 for training and 10,000 for validation with category number of 10 and 100, respectively.
For fair comparisons, we use the long-tailed versions of CIFAR datasets as the same as those used
in Zhou et al. (2020) with controllable degrees of data imbalance. Imbalance factor β is utilized
to describe the severity of the long tail problem with the number of training samples for the most
frequent class and the least frequent class, e.g., β = nmax. We use β as 10, 50, and 100 in our
nmin
experiments.
iNaturalist 2018. The iNaturalist species classification dataset is a large-scale real-world, naturally
long-tailed dataset, suffering from extremely imbalanced label distributions. We choose the 2018
version in our experiments, which consists of 437,513 images from 8,142 categories. Note that,
besides the extreme imbalance, the iNaturalist datasets also face the fine-grained problem. For fair
comparisons, we utilize the official splits of training and validation images.
ImageNet-LT. ImageNet-LT is artificially truncated from their balanced versions so that the labels
of the training set follow a long-tailed distribution. ImageNet-LT has 1000 classes and the number
of images per class ranges from 1280 to 5 images. Note that the validation set is balanced of 1000
classes.
A.3.2 Implementation details
Implementations details on CIFAR. We adopt the plaining ResNet-32 (He et al. (2016)) as our
model in all experiments. Standard mini-batch stochastic gradient descent (SGD) with momentum
of 0.9, weight decay of 2 × 10-4 is utilized to optimize the whole network. We train all the models
on one single NVIDIA 2080Ti GPU for 200 epochs with batch size of 64. The initial learning rate
is set to 0.1 and the first five epochs is trained with the linear warm-up learning rate schedule (Goyal
et al. (2017)). The learning rate is decayed at the 100th by 0.1. S is set to 1, which means we switch
the instance-balanced sampling to class-reversed sampling at the 101st epoch.
Implementations details on iNaturalist. For fair comparisons, we utilize the plaining ResNet-
50 (He et al. (2016)) as our backbone network in all experiments. We train all the models on eight
NVIDIA 2080Ti GPUs with batch size of 512 for 90 epochs and 200 epochs, respectively. The
initial learning rate is set to 0.05 and decayed by 0.1 at the 60th and 80th epoch for 90, 120th and
160th for 200. The batch size is 512 and S is set to 1, which is similar to experiments on CIFAR.
For fair comparison with Decouple (Kang et al. (2020)), we also set the S as 10 and 40 respectively,
which means to switch sampler and train it for additional 10 epochs after the same standard training
procedure have done, with total number of training epochs as 100 epochs and 210 epochs.
Implementations details on ImageNet-LT. We adopt ResNet-50 and ResNext-50 as our backbone
to analyze the effectiveness of our method. The initial learning rate is set to 0.2 and decayed by 0.1
at the 60th and 80th epoch for total 90 epochs. The batch size is 256 and S is set to 1, which is
similar to experiments on CIFAR. For fair comparison with Decouple (Kang et al. (2020)), we also
set the S as 10, which means to switch sampler and train it for additional 10 epochs after the same
standard training procedure have done, with total number of training epochs as 100 epochs.
A.3.3 comparison methods
In experiments, we compare our method with four groups of methods:
Baseline methods. We employ plaining training with cross-entropy loss and focal loss (Lin et al.
(2017)) as our baselines.
Re-weighting methods. For re-weighting methods, we compare with the CB-Focal (Cui et al.
(2019)) and LDAM (Cao et al. (2019)), where effective numbers or margin-based generalization are
utilized to alleviate the extreme data imbalance during training.
Two-stage fine-tuning strategies. To prove the effectiveness of our switching strategy, we compare
it with the two-stage fine-tuning strategies proposed in Cao et al. (2019). Networks are trained with
cross-entropy (CE) on imbalanced data first, and then are trained with class re-balancing strategy in
15
Under review as a conference paper at ICLR 2021
the second stage. CE-DRW and CE-DRS refer to the two-stage baselines using re-weighting and re-
sampling at the second stage. We also compare with Decouple (Kang et al. (2020)), which trains the
network with instance-balanced sampling and uses class-balanced sampling to re-train the classifier
in the second stage with backbone fixed.
State-of-the-art methods. For state-of-the-art methods, we compare with the recently proposed
BBN( Zhou et al. (2020)) which achieves good classification accuracy on long-tailed datasets. BBN
also utilizes class-reversed sampling to re-balance the feature extractor but it has a more complicated
model structure, neglecting the proper combination of different data samplers itself.
A.4 Regularity
To investigate the memorization-generalization continuum in deep learning towards long-tailed dis-
tributions, we introduce the cumulative learned events and forgetting events (Toneva et al. (2019))
as follows:
Cumulative learned events. For sample {xi, yi}, y1 2t = arg max g(yi |xi； θt) is the predicted label
for sample Xi obtained after t epochs of SGD optimization. Let acct = 1yt=yi be a binary variable
indicating whether the sample is correctly classified at time epoch t, the cumulative learned events
events at epoch t are defined as follows:
t
Lit = X accin.	(17)
n=1
Forgetting events.: Let f orit = 1acct-1=1 acct=0, the forgetting events of one sample {xi, yi} at
acci = ,acci =
epoch t are defined as follows:
t
Fit = X forin.	(18)
n=1
Based on these notations, we intuitively give the metric to describe the regularity of one sample,
which means with higher cumulative learned events as well as lower forgetting events, the higher
regularity it will be and vice versa.
For Long-tailed CIFAR-10 with imbalance ratio 50, we run the ResNet-32 for 10 runs and plot the
averaged cumulative learned events and forgetting events of each sample grouped by its class, as
shown in Figure 3. We surprisingly find that the clustering degree of samples is almost proportional
to its cardinality. To explore this phenomenon further, we plot the same events of same samples
when learning the standard CIFAR-10 with class-balanced distributions in Figure 4. Compared
with the same samples under class-balanced distributions, long-tailed distribution samples show
different properties of each class: higher degree of clustering of head classes (cls0, cls1) and lower
degree of clustering of tail classes (cls6, cls7, cls8, cls9). Differences between them indicate that
the cardinality of one class can significantly affect the regularity itself during training: regularity of
the same training samples will be sharply decreased with the reduction of class cardinality, which
is easy to understand: the more samples one class have, the higher regular it will be.
In order to analysis this phenomenon, we propose a novel metric to quantize the regularity of each
class. For class j containing nj samples, regularity event of each sample {xi, yi} can be denoted
by its cumulative learned events and forgetting events as ri,j = {LiT , FiT }, which is a point on the
two-dimensional plane. There are three sub-procedures to calculate the regularity of each class j:
1) Let {(LiT, FiT)|1 ≤ i ≤ nj } = [LF] denote the regularity set of class j, we calculate the
covariance matrix as follows:
E[(L - E[L])(L - E[L])]	E[(L - E[L])(F - E[F])]
Cj = E[(F T - E[F])(L - E[L])] E[(F - E[F])(F - E[F])]
where E is the expectation.
2) calculate the F -norm of Cj:
∈ R2×2,	(19)
||Cj||F
22
X X |cmn|2.
m=1 n=1
(20)
16
Under review as a conference paper at ICLR 2021
3)	normalize the F -norm by its cardinality:
Ij = ||Cj||F/nj.	(21)
This metric essentially indicates the deviation of each class, so we name it Irregularity.
As shown in Table 8, the Irregularity is almost proportionally to the reciprocal of its cardinality,
which is consistent with our visual perception. To further validate the correlation between regularity
and its cardinality, we exploit the Pearson correlation coefficient. Let I = {Ii|1 ≤ i ≤ C} be the
regularity set of all classes and N = {ni|1 ≤ i ≤ C} be the cardinality of all classes, we calculate
the Pearson correlation coefficient as follows:
P
P NI-PNPI
(22)
VZ(P N2- (PN2 )(P I2 -交)
As Table 8 shows, the Pearson coefficient is -0.6112, indicating cardinality and regularity are signif-
icantly negatively correlated.
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
(a) cls0 (5000 samples)
(b) cls1 (3237 samples) (c) cls2 (2096 samples) (d) cls3 (1357 samples)
(e) cls4 (878 samples)
(f) cls5 (568 samples) (g) cls6 (368 samples) (h) cls7 (238 samples)
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
SAUΦ>Φ 611君6」0JjO -IeqEnU
number Ofcumulative learned events
number of cumulative learned
200
events
(i) cls8 (154 samples) (j) cls9 (100 samples)
Figure 3: Cumulative learned events and forgetting events of each sample of Long-tailed CIFAR-10
with imbalance ratio 50. The regularity of one class will be higher with more samples gathering in
the lower right corner of the picture.
A.5 Combinations of Sampling S trategies
In order to find the optimal sampler combination before and after switching, we conduct compre-
hensive experiments on long-tailed CIFAR-10 (imbalance ratio: 50) with combinations of differ-
ent data samplers used in different stages. As shown in Table 9, our strategy, which switches
instance-balanced sampling to class-reversed sampling in the small learning rate stage, achieves
the best performance across all experimental settings. We draw the same conclusion with Decouple
that instance-balanced sampling gives the most generalizable representations, for using instance-
balanced in the first stage always performs better than other results. In addition, switching to class-
reversed sampling can always bring a significant improvement no matter what samplers used in the
17
Under review as a conference paper at ICLR 2021
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
(a) cls0 (5000 samples)
(b) cls1 (3237 samples) (c) cls2 (2096 samples) (d) cls3 (1357 samples)
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
number Ofcumulative learned events
(e) cls4 (878 samples)
(f) cls5 (568 samples) (g) cls6 (368 samples) (h) cls7 (238 samples)
number Ofcumulative learned events
number Ofcumulative learned events
(i) cls8 (154 samples) (j) cls9 (100 samples)
Figure 4: Cumulative learned events and forgetting events of each sample of standard CIFAR-10.
Samples here are in one-to-one correspondence with samples in Figure 3.
Table 8: Quantitative results of the regularity of each class on long-tailed CIFAR-10 (imbalance
ratio: 50). All indexes are calculated based on the cumulative learned events and forgetting events.
Class	Cardinality	F-norm	Irregularity (F-norm / Cardinality)	Pearson
0	5000	85.7341	0.0171	
1	3237	44.3574	0.0137	
2	2096	296.3582	0.1414	
3	1357	397.6501	0.2930	
4	878	267.5524	0.3047	
5	568	447.2937	0.7875	-0.6112
6	368	253.5138	0.6889	
7	238	355.5270	1.4938	
8	154	279.5059	1.8150	
9	100	363.4100	3.6341	
first stage, except class-reversed sampling on the long-tailed CIFAR-100 with imbalance ratio 100
and 50 (see the last row in Table 9). We conjecture this is because class-reversed sampling can not
learn the general representations on such extreme imbalanced data, since it mainly samples from
the tail classes with low cardinality. Without generalizable representation and seeing samples from
other classes, network can not generalize well across all classes.
As shown in Table 10, the performance of sampling weights based on cardinality and regularity of
each class differs slightly, indicating the class-reversed sampling is the near optimal choice when
class regularities are agnostic.
18
Under review as a conference paper at ICLR 2021
Table 9: Comprehensive results on long-tailed CIFAR-10 (imbalance ratio: 50) with combinations
of different data samplers used in different stages.
Sampling Strategy Combination ∣ Long-tailed CIFAR-10 ∣ Long-tailed CIFAR-100
Imbalance ratio		100	50	10	100	50	10
IB =	⇒ IB	73.1	77.9	86.4	40.7	44.9	57.2
IB =	CB	77.1	81.9	87.9	44.2	48.7	59.2
IB =	CR	79.7	82.9	88.4	44.7	49.5	59.5
CB =	⇒ IB	66.5	73.8	86.7	33.3	37.1	55.0
CB =	⇒ CB	73.0	78.5	87.3	36.2	40.3	56.9
CB =	⇒ CR	74.8	80.7	87.9	38.6	42.4	57.8
CR =	⇒ IB	63.8	74.8	85.2	24.7	28.7	51.3
CR =	⇒ CB	64.2	72.7	86.3	24.7	29.2	52.5
CR =	⇒ CR	68.4	76.3	86.8	22.9	28.0	53.2
Table 10: Sampling weights in the switching stage based on cardinality and irregularity respectively
on long-tailed CIFAR-10 (imbalance ratio: 50).
Sampling Weight	Test accuracy
cardinality	82.9
irregularity	82.8
A.6 Do We Need Decoupling?
To further compare our method with Decouple, we investigate the factors of fixing feature extractor
and re-training classifier towards learning long-tailed distributions, which are adopted in Decouple.
From the results shown in Table 11, the following observations can be made:
•	Joint training is better. Training the backbone and the classifier jointly always performs bet-
ter than fixing the backbone. This phenomenon indicates that although instance-balanced
sampling gives the most generalizable representations, it is not good enough. Fine-tuning
the backbone with low-regularity tail class samples in the small learning rate stage can
significantly improve its representation ability across tail classes.
•	Re-training is matter when no switching. When training with the switching strategy, re-
sults with re-training or without re-training the classifier are much similar (see rows with
CB, CR as the switching sampler). However, interestingly, re-training the classifier can
bring improvements in the standard training procedure (see rows with IB as the switching
sampler). We speculate that model trained by uniform instance-balanced sampling would
have a strong bias towards tail classes in both backbone and classifier. Re-training classifier
based on the learned general representations can alleviate it.
•	Switching and joint training are complementary. We compare the results of only switching
to only joint training, finding that while switching samplers and joint training can bring
improvements respectively, their combination can improve the performance further. Fine-
tuning with class-balanced or class-reversed distributions can boost the generalization abil-
ity further.
Further, we valid the quality of features learned by standard training procedure and our switching
training procedure in Table 12, just like Decouple. Although a slightly lower with IB, re-training
based on our feature can bring a significant improvements compare with stand features. These
results also indicate a disadvantage of Decouple: performance of re-training classifier is depend on
the performance of feature extractor. Once the feature representation is sub-optimal, the re-trained
classifier is sub-optimal.
To validate our method could reach a better balance under bias-variance trade-off, we calculate the
total error of each method in Table 13. The higher accuracy as well as lower total error indicate our
switching performs better in the challenging trade-off compared with other methods.
19
Under review as a conference paper at ICLR 2021
Table 11: Comparisons between Decouple learning paradigm and our learning paradigm on long-
tailed CIFAR-10 (imbalance ratio: 50), where Decouple indicates fixing the backbone and re-train
the classifier from scratch while We continue tojoint train both of them.
First Sampler S Switching Sampler Joint Training Re-training Classifier Test accuracy
76.3
		IB	X		77.9
				X	77.7
			X	X	78.9
					81.9
IB	1	CB	X		81.9
				X	81.8
			X	X	81.9
					81.4
			X		82.9
		CR		X	81.6
			X	X	82.4
Table 12: Feature quality of Decouple learning paradigm and our switching learning paradigm on
long-tailed CIFAR-10 (imbalance ratio: 50). We firstly train the model with standard and switching
procedure respectively, then re-train the classifier with different data samplers with backbone fixed.
Feature	Re-training	Test accuracy
	IB	77.7
Standard	CB	80.7
	CR	82.2
	IB	77.0
Our	CB	81.4
	CR	82.6
Table 13: Total error (bias2+variance) of different methods on the test set of long-tailed CIFAR-10
(imbalance ratio: 50)._____________________________________________________________________
Method	Test Accuracy ↑	Bias2 J	Variance J	Total Error J
Cross Entropy (IB only)	0.779	0.049	0.168	0.217
Cross Entropy (CR only)	0.763	0.056	0.183	0.239
Decouple-cRT	0.807	0.037	0.148	0.185
BBN	0.822	0.032	0.146	0.178
Ours (S = 1)	0.829	0.029	0.142	0.171
A.7 Learning Rate Scheduling
In Section 4.3 we visualize the learning speed of different classes to reveal the effectiveness of
stage-wise constant learning rate scheduler during training. The observations lead to an interesting
hypothesis for explaining why we should mainly memorizing tail class samples in the small learning
rate stage. We provide more details here.
Figure 5 shows the learning speed of4 selected classes with SGD using stage-wise constant learning
rate scheduling. This is the same as Figure 2, replicated here for easy comparison. In Figure 6
we show the learning speeds of 4 selected classes trained with SGD using constant learning rate
scheduling with the standard training procedure. The 4 panels show the results of different values
of constant learning rate used in training. It is observed that faster convergence are achieved with
smaller learning rate (see 0.1, 0.02 and 0.01). While the learning rate is so small, e.g., 0.001, the
learning speed of each class is significantly slowed down.
In Figure 7 we show the learning speeds of of our switching training procedure trained with SGD
using constant learning rate scheduling. Similar to Figure 6, proper small learning rate could ac-
celerate the convergence, with higher and more stable accuracy. It it worth noting that switching to
20
Under review as a conference paper at ICLR 2021
class-reversed sampler always improves the accuracy of tail classes, but will damage the representa-
tive ability of head classes to some extent. Stage-wise constant learning bring the smallest damage
to the head class representations, showing the necessity of building generalization representations
first. Quantitative results of both standard training and switching training are shown in Table 14.
Here we manage to explain why class-reversed sampler is effective. The reason that switching
to class-reversed sampler performs well is that it delayed the learning of low-regularity samples
(tail classes samples) to later small learning rate stages. In the first stage, when almost head class
samples are learned, the patterns and structures discovered in those high-regularity samples can be
used to build a generalizable representation. In later stage, network is able to learn or memorize low-
regularity samples of tail classes based on the representations from a clean subset of high-regularity
samples. In addition, learning or memorizing tail class samples will not seriously disrupt the learned
representation as the learning rate is much smaller than the earlier stages. In contrast, standard
learning procedure without switching could not focus on the tail class samples since the extreme data
imbalance, leading under-representation for tail classes, while SGD with (small) constant learning
rate learns the examples across all classes fairly quickly, which can not learn the generalizable
representation from high-regularity samples of head classes before.
(a) standard
Figure 5: Learning speed of examples of 4 selected class with SGD using stage-wise constant learn-
ing rate. Left: standard training procedure. Right: our switching training procedure.
(b) ours
Table 14: Test performance of models trained with various learning rate schedulers on long-tailed
CIFAR-10 (imbalance ratio: 50).________________________________________________________
Standard			Our		
Optimizer	Learning Rate	Test Accuracy	Optimizer	Learning Rate	Test Accuracy
SGD	Stage-wise	77.9	SGD	Stage-wise	82.9
SGD	0.1	75.5	SGD	0.1	77.7
SGD	0.02	76.1	SGD	0.02	78.3
SGD	0.01	75.0	SGD	0.01	77.7
SGD	0.001	65.9	SGD	0.001	66.1
21
Under review as a conference paper at ICLR 2021
80604020
⅛ω 6u⊂ra⅛co AUB_n<_>。B
(a) lr = 0.1
(b) lr = 0.02
(c) lr = 0.01
(d) lr = 0.001
cisO (5000 samples)
:lsl (3237 samples)
cls7 (238 samples)
cls9 (100 samples)
150
training epoch
cisO (5000 samples)
clsl (3237 samples)
cls7 (238 samples)
cls9 (100 samples)
Figure 6: Learning speed of examples of 4 selected classes with SGD using constant learning rate
with standard training strategy. The 4 different learning rates correspond to the constants used in the
stage-wise scheduler.
----cisO (5000 samples)
----clsl (3237 samples)
----cls7 (238 samples)
----cls9 (100 samples)
0	25	50	75	100 125 150 175 200
training epoch
(a) lr = 0.1
cisO (5000 samples)
clsl (3237 samples)
cls7 (238 samples)
cls9 (100 samples)
0	25	50	75	100 125 150 175 200
training epoch
Ooooo
0 8 6 4 2
¾s 6U-U-B 匕 UO Aue-Inuue
10(604020
¾S 5u⊂rabUO
Γ Γ	  elsŋ	(5000	samples}
ʃ*	  clsl	(3237	samples)
∖J}	  cls7	(238	samples)
----cls9 (100 samples)
0	25	50	75	100 125 150 175 200
training epoch
(b) lr = 0.02
10(604020
-l-ləs 6u⊂一 p」-uo ABJrrn P
training epoch
O □
2
(c) lr = 0.01
(d) lr = 0.001
Figure 7: Learning speed of examples of 4 selected classes with SGD using constant learning rate
with our training strategy. The 4 different learning rates correspond to the constants used in the
stage-wise scheduler.
22