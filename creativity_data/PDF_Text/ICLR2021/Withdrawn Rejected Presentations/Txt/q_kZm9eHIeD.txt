Under review as a conference paper at ICLR 2021
Entropic	Risk-Sensitive	Reinforcement
Learning: A Meta Regret Framework with
Function Approximation
Anonymous authors
Paper under double-blind review
Ab stract
We study risk-sensitive reinforcement learning with the entropic risk measure and
function approximation. We consider the finite-horizon episodic MDP setting,
and propose a meta algorithm based on value iteration. We then derive two
algorithms for linear and general function approximation, namely RSVI.L and
RSVI.G, respectively, as special instances of the meta algorithm. We illustrate that
the success of RSVI.L depends crucially on carefully designed feature mapping
and regularization that adapt to risk sensitivity. In addition, both RSVI.L and
RSVI.G maintain risk-sensitive optimism that facilitates efficient exploration. On
the analytic side, we provide regret analysis for the algorithms by developing
a meta analytic framework, at the core of which is a risk-sensitive optimism
condition. We show that any instance of the meta algorithm that satisfies the
condition yields a meta regret bound. We further verify the condition for RSVI.L
and RSVI.G under respective function approximation settings to obtain concrete
regret bounds that scale sublinearly in the number of episodes.
1 Introduction
Risk is one of the most important considerations in decision making, so should it be in reinforcement
learning (RL). As a prominent paradigm in RL that performs learning while accounting for risk, risk-
sensitive RL explicitly models risk of decisions via certain risk measures and optimizes for rewards
simultaneously. It is poised to play an essential role in application domains where accounting for
risk in decision making is crucial. A partial list of such domains includes autonomous driving
(Buehler et al., 2009; Thrun, 2010), behavior modeling (Niv et al., 2012; Shen et al., 2014), real-
time strategy games (Berner et al., 2019; Vinyals et al., 2019) and robotic surgery (Fagogenis et al.,
2019; Shademan et al., 2016).
In this paper, we study risk-sensitive RL through the lens of function approximation, which is an
important apparatus for scaling up and accelerating RL algorithms in applications of high dimension.
We focus on risk-sensitive RL with the entropic risk measure, a classical framework established by
the seminal work of Howard & Matheson (1972). Informally, for a fixed risk parameter β 6= 0, our
goal is to maximize the objective
Ve = 1log{EeβR }.	(1)
β
The definition of Vβ will be made formal later in (2). The objective (1) admits a Taylor expansion
Ve = E[R] + βVar(R) + O(β2). Comparing (1) with the risk-neutral objective V = E[R] studied
in the standard RL setting, we see that β > 0 induces a risk-seeking objective and β < 0 induces a
risk-averse one. Therefore, the formulation with the entropic risk measure in (1) accounts for both
risk-seeking and risk-averse modes of decision making, whereas most others are restricted to the
risk-averse setting (Fu et al., 2018). It can also be seen that Ve tends to the risk-neutral V as β → 0.
Existing works on function approximation for RL have mostly focused on the risk-neutral setting
and heavily exploits the linearity of risk-neutral objective V in both transition dynamics (implicitly
captured by the expectation) and the reward R, which is clearly not available in the risk-sensitive
objective (1). It is also well known that even in the risk-neutral setting, improperly implemented
function approximation could result in errors that scale exponentially in the size of the state space.
1
Under review as a conference paper at ICLR 2021
Combined with nonlinearity of the risk-sensitive objective (1), it compounds the difficulties of
implementing function approximation in risk-sensitive RL with provable guarantees.
This work provides a principled solution to function approximation in risk-sensitive RL by
overcoming the above difficulties. Under the finite-horizon MDP setting, we propose a meta
algorithm based on value iteration, and from that we derive two concrete algorithms for linear and
general function approximation, which we name RSVI.L and RSVI.G, respectively. By modeling
a shifted exponential transformation of estimated value functions, RSVI.L and RSVI.G cater to the
nonlinearity of the risk-sensitive objective (1) and adapt to both risk-seeking and risk-averse settings.
Moreover, both RSVI.L and RSVI.G maintain risk-sensitive optimism in the face of uncertainty
for effective exploration. In particular, RSVI.L exploits a synergistic relationship between feature
mapping and regularization in a risk-sensitive fashion. The resulting structure of RSVI.L makes
it more efficient in runtime and memory than RSVI.G under linear function approximation, while
RSVI.G is more general and allows for function approximation beyond the linear setting.
Furthermore, we develop a meta regret analytic framework and identify a risk-sensitive optimism
condition that serves as the core component of the framework. Under the optimism condition, we
prove a meta regret bound incurred by any instance of the meta algorithm, regardless of function
approximation settings. Furthermore, we show that both RSVI.L and RSVI.G satisfy the optimism
condition under the respective function approximation and achieve regret that scales sublinearly in
the number of episodes. The meta framework therefore helps us disentangle the analysis associated
with function approximation from the generic analysis, shedding light on the role of function
approximation in regret guarantees. We hope that our meta framework will motivate and benefit
future studies of function approximation in risk-sensitive RL.
Our contributions. We may summarize the contributions of the present paper as follows:
•	we study function approximation in risk-sensitive RL with the entropic risk measure; we
provide a meta algorithm, from which we derive two concrete algorithms for linear and
general function approximation, respectively; the concrete algorithms are both shown to
adapt to all levels of risk sensitivity and maintain risk-sensitive optimism over the learning
process;
•	we develop a meta regret analytic framework and identify a risk-sensitive optimism
condition, under which we prove a meta regret bound for the meta algorithm; furthermore,
by showing that the optimism condition holds for both concrete algorithms, we establish
regret bounds for them under linear and general function approximation, respectively.
Notations. For a positive integer n, we let [n] := {1, 2, . . . , n}. For a number u 6= 0, we define
sign(u) = 1 if u > 0 and -1 if u < 0. For two non-negative sequences {ai} and {bi}, we write
ai . bi if there exists a universal constant C > 0 such that ai ≤ Cbi for all i, and write ai bi if
ai . bi and b . a%. We use O(∙) to denote O(∙) while hiding logarithmic factors. For any ε > 0
and set X, We let Nε(X, k Tl) be the ε-net of the set X with respect to the norm k ∙ k. We let ∆(X)
be the set of probability distributions supported on X. For any vector U ∈ Rn and symmetric and
positive definite matrix Γ ∈ Rn×n, Weletkukr ：= Vu>Γu. We denote by In the n X n identity
matrix.
2	Related work
Initiated by the seminal work of Howard & Matheson (1972), risk-sensitive control/RL with the
entropic risk measure has been studied in a vast body of literature (Bauerle & Rieder, 2014;
Borkar, 2001; 2002; 2010; Borkar & Meyn, 2002; Cavazos-Cadena & Herndndez-Herndndez,
2011; Coraluppi & Marcus, 1999; Di Masi & Stettner, 1999; 2000; 2007; Fleming & McEneaney,
1995; Herndndez-Herndndez & Marcus, 1996; Ja§kiewicz, 2007; Marcus et al., 1997; Mihatsch
& Neuneier, 2002; Osogami, 2012; Patek, 2001; Shen et al., 2013; 2014; Whittle, 1990). Yet,
this line of works either assumes known transition kernels or focuses on asymptotic behaviors of
the problem/algorithms, and finite-sample/time results with unknown transitions have rarely been
investigated.
2
Under review as a conference paper at ICLR 2021
The most relevant work to ours is perhaps Fei et al. (2020), who consider the same problem as ours
under the tabular setting. They propose two algorithms based on value iteration and Q-learning.
They prove regret bounds for their algorithms, which are then certified to be nearly optimal by a
lower bound. However, their algorithms and analysis are restricted to the tabular setting. Compared
to Fei et al. (2020), our paper provides a novel and unified framework of algorithms and analysis for
function approximation. We study linear and general function approximation as two instances of the
framework, both of which subsume the tabular setting.
We also briefly discuss existing works on function approximation with regret analysis, which so far
have focused on the risk-neutral setting. The works of Cai et al. (2019); Jin et al. (2019); Wang
et al. (2019); Yang & Wang (2019); Zhou et al. (2020) study linear function approximation, while
Ayoub et al. (2020); Wang et al. (2020) investigate general function approximation. In addition, all
these works prove O(K 1/2)-regret for their algorithms, although dependence on other parameters
varies in settings. As we have argued in the previous section, the nonlinear objective (1) makes
algorithm design and regret analysis for function approximation much more challenging in risk-
sensitive settings than in the standard risk-neutral one.
3	Problem formulation
3.1	EPISODIC MDP
An episodic MDP is parameterized by a tuple (K, H, S, A, {Ph}h∈[H] , {rh}h∈[H] ), where K is the
number of episodes, H is the number of steps in each episode, S is the state space, A is the action
space, Ph : S × A → ∆(S) is the transition kernel at step h, and rh : S × A → [0, 1] is the reward
function at step h. We assume that {Ph} are unknown. For simplicity we also assume that {rh}
are known and deterministic, as is done in existing works such as Yang & Wang (2019); Zhou et al.
(2020).
We interact with the episodic MDP as follows. In the beginning of each episode k ∈ [K], the
environment chooses an arbitrary initial state s1k ∈ S. Then in each step h ∈ [H], we take an action
akh ∈ A, receives a reward rh(skh, akh) and transitions to the next state skh+1 ∈ S sampled from
Ph(∙ | sh, ah). Once We reach sH+1, the current episode terminates and We advance to the next
episode unless k = K.
3.2	Value functions, Bellman equations and regret
We assume that β is fixed prior to the learning process, and for notational simplicity We omit it from
quantities to be introduced subsequently. In risk-sensitive RL With the entropic risk measure, We
aim to find a policy π = {πh : S → A} so as to maximize the value function given by
Vn (S) = 1log {E exP
H
β	rh0 (sh0, πh0 (sh0))
h0=h
sh = s ,
(2)
for all (h, s) ∈ [H] × S. Under some mild regularity conditions, there exists a greedy policy
π* = {∏h} which gives the optimal value Vhr* (s) = SuPn V∏ (s) for all (h, S) ∈ [H] × S (Bauerle
& Rieder, 2014). In addition to the value function, another key notion is the action-value function
defined as
Qn (s,α) := 1log {e exp
H
β rh0 (Sh0, ah0)
h0=h
Sh = S, ah = a
(3)
,
for all (h, S, a) ∈ [H] × S × A.
The action-value function Qhπ is closely associated with the value function Vhπ via the so-called
Bellman equation:
Qn (s, a = Th(S, a) + β log {Es0~Ph(∙∣ s,a) [exP (β ∙ Vh+1(s0))]},
Vhπ(S)=Qhπ(S,πh(S)),	VHπ+1(S) = 0,
(4)
3
Under review as a conference paper at ICLR 2021
which holds for all (h, s, a) ∈ [H] × S × A. Note that the identity of Qπh in (4) is a result of simple
calculation based on (2) and (3). Similarly, the Bellman optimality equation is given by
Qh(s, a) = rh(s, a) + 1 log {Es，〜Ph(∙∣ s,a) [exp (β ∙ Vh+ι(s'))]} ,	(5)
β
Vh=(S) = max Qh(S, a),	VH+ι(S) =0,
a∈A
again for all (h, s, a) ∈ [H] ×S × A. In the above, We use the shorthand Qh(∙, ∙) := Qh (∙, ∙) for
all h ∈ [H] and Vh=(∙) is similarly defined. The identity Vh=(∙) = max。*/ Qh(∙, a) implies that the
optimal π= is the greedy policy With respect to the optimal action-value function {Q=h}h∈[H] .
During the learning process, the policy πk in each episode k may be different from the optimal π = .
We quantify this difference over all K episodes through the notion of regret, defined as
Regret(K) := X hV1=(S1k) - V1πk (S1k)i .	(6)
k∈[K]
Since V1=(S) ≥ V1π(S) for any π and S ∈ S by definition, regret also characterizes the suboptimality
of {πk } relative to the optimal π = .
3.3 Function approximation
In this paper, We focus on linear and general function approximation. We consider the folloWing
form of linear function approximation, Which assumes that each transition kernel admits a linear
form.
Assumption 1 We assume that the MDP is equipped with a knownfeaturefUnction ψ : S×A×S →
Rd such that for any h ∈ [H ], there exists a vector θh ∈ Rd with ∣∣θh∣∣2 ≤ √d and the transition
kernel is given by
Ph(S0 | S,a) = ψ(S, a, S0)>θh
for any (S, a, S0) ∈ S × A × S. We also assume that
Il ZS ψ(s, a, s0) ∙ (eβ∙V (SO)- 1)ds0∣∣2 ≤ √d ∙ ∣eβv - 1∣,
for any (s, a) ∈ S ×A and V : S → [0, v] where V ≥ 0.
This form of linear function approximation is also studied in the Work of Ayoub et al. (2020); Cai
et al. (2019); Zhou et al. (2020), Whose setting is equivalent to ours When β → 0. The setting of
Assumption 1 may be reduced to the tabular setting in Which ψ(S, a, S0) is a canonical basis vector
in Rd With d = |S|2 |A|, i.e., the (S, a, S0)-th entry of ψ(S, a, S0) is equal to one and the other
entries are equal to zero. It also subsumes various settings of function approximation including
linear combinations of base models (Modi et al., 2020) and the matrix bandit setting (Yang & Wang,
2019); We refer readers to (Zhou et al., 2020) for more details on the generality of Assumption 1.
For general function approximation, We make the folloWing assumption.
Assumption 2 We assume that we have access to a function set1 P such that the transition kernel
Ph ∈ Pfor all h ∈ [H].
This setting is also considered in Ayoub et al. (2020). Under Assumption 2, We may measure the
complexity of function sets using the notion of the so-called eluder dimension (Russo & Van Roy,
2014), Which We define and discuss in Appendix A.
Note that although We focus on function approximation of transition kernels, a similar approach can
be taken to apply function approximation to reWard functions and our regret guarantees presented
beloW Would still hold, as argued in Yang & Wang (2019).
1Throughout the paper, We use function class and function set interchangeably.
4
Under review as a conference paper at ICLR 2021
4 Algorithms
We first present the Meta algorithm for Risk-Sensitive Value Iteration (MetaRSVI) in Algorithm
1, which is a high-level framework including key features of algorithms based on value iteration
(Bradtke & Barto, 1996; Osband et al., 2014; Jin et al., 2019). It mainly consists of a value estimation
step and policy execution step. In the value estimation step, the algorithm estimates the optimal
{Qh} by its iterates {Qh} based on historical data. Since We focus on greedy policies, in Line 5
the estimated value function Vhk(') is SimPIy taken as the maximum among {Qh(∙, a0)}aθ∈A. The
primary machinery of value estimation, knoWn as Risk-Sensitive Temporal Difference or RSTD, is
presented in an abstract Way in Line 4; this is because its concrete form Would depend on function
approximation of the underlying MDP and algorithmic implementation. In the policy execution
step, the algorithm uses the policy learned so far (represented by {Qkh}) to collect data for subsequent
learning stages. We remark that Algorithm 1 is flexible and general enough to alloW for any function
approximation. In the remaining of this section, We derive tWo special instances of Algorithm 1 for
linear and general function approximation, respectively, by providing concrete implementation of
RSTD.
Algorithm 1 MetaRSVI
Input: risk parameter β, number of episodes K
1
2
3
4
5
6
7
8
9
10
11
12
for episode k = 1, . . . , K do
VH+ι (∙) 一 0
for step h = H, H - 1, . . . , 1 do
Qh(∙, ∙) 一 RSTD(C,h,β,{VT+ι}τ∈[k])
Vh(•) — maXa0∈A Qh(∙,a0)
end for
Receive the initial state s1k from the environment
for step h = 1, 2, . . . , H do
Take action ah J argmax。,∈/ Qjh(Sh a0)
Receive the reWard rh(skh, akh) and the next state
end for
end for
value estimation
. policy execution
We introduce Risk-Sensitive Value Iteration for Linear function approximation, or RSVI.L, in
Algorithm 2 under the setting of Assumption 1. This algorithm is inspired by RSVI proposed in
Fei et al. (2020) that specializes in the tabular setting. It replaces the abstract function RSTD in
Algorithm 1 With a concrete implementation for the linear function approximation. In Line 4 the
iterate whk can be interpreted as the solution of the folloWing least-squares problem:
Wk — argmin X [eβVh+1 ⑸+I)- 1- w>φh(sh,ah)]2 + λkwk2,	⑺
w∈Rd τ∈[k-1]
where the surrogate feature mappings {φh(∙, ∙)}τ∈[k-i] are constructed in Line 5 and λ ≥ 0 is the
regularization parameter to be set by users. The above regression problem essentially computes an
estimate of θh, the parameter of the transition kernel Ph, by taking advantage of the linear form
Es，〜p.(∙∣ s,a) [eβ'vh+1(s)] in the Bellman equation (4). Note that the regression targets are set as a
shifted exponential transformation of estimated value functions, i.e., {eβ,VhT+1(Sh+1) - 1}. Similar
construction is applied to the surrogate features φh(∙, ∙) in Line 5. Mechanically, such design ensures
that when Vh+ι(∙) = 0, we would have φh(∙, ∙) = 0 and therefore by definition Qh(∙, ∙) = rh(∙, ∙);
this is a similar behavior exhibited by risk-neutral algorithms for linear function approximation, e.g.,
Algorithm 1 in Cai et al. (2019) whose surrogate features are given by replacing eβ∙vhk+1(∙) 一 1 in
Line 5 with Vh+ι (∙). To update Qh(∙, ∙) in Line 7, we use the quantity
k ( ) := ∫min{eβ(H-h), <φh(∙, )wQ + 1+bh(∙, ∙)},	ifβ >0,
qh,L( , ) = {max{e"h), ", ∙),wQ + 1- bh(∙, ∙)}, if β< 0.
(8)
Informally, With bkh taking the role of bonus, qhk,L can be seen as an “optimistic” and risk-adaptive
estimate for the expected value of eβ^vh+1 (∙) under the (unknown) transition kernel Ph. The term
5
Under review as a conference paper at ICLR 2021
Algorithm 2 RSVI.L
Input: risk parameter β, number of episodes K, bonus multiplier yl, regularization parameter λ
1:	Run Algorithm 1 with RSTD() therein overloaded by the following subroutine:
2:	procedure RSTD(k, h, β, {Vhτ+1}τ ∈[k], γL, λ)
3：	Λh 一 Pτ ∈[k-1] φτh(sτh,aτh)φτh(sτh,aτh)> + λId
4:	Wk -Ah)-1 PT∈[k-i] Φh(sh, ah) ∙ (eβ,Vh+1 (Sh+1)- 1)
5:	Φh(∙, ∙) — RS ψ(∙, ∙, s0) ∙ (eβ,Vh+1 (SO)- 1)ds0
6:	bh(∙,∙) - yl∙ [Φh(∙, ∙)>(Λh)Tφh(∙,w2
7:	return Qh(∙, ∙) - rh(∙, ∙) + β log(qk^(∙, ∙)), where qk∕(∙, ∙) is defined in (8)
8:	end procedure
<φh(∙, ∙),wk) + 1 serves as a correction of the shifted quantity eβ^Vhk+1(^) - 1 in both φhh(∙, ∙) and
whk , so as to align the structure of Line 7 to that of the Bellman equation (4). A proper definition of
bonus bkh (with γL therein formally given in Theorem 2 below) and the truncation at eβ(H-h) would
put qh L within [1, eβ(H-h)] entrywise. This ensures that the estimate Qh(∙, ∙) is on the same scale
as the optimal Qh(∙, ∙) ∈ [0,H - h + 1].
We note that there is a significant difference between Algorithm 2 and RSVI introduced in Fei et al.
(2020). Since RSVI is designed only for the tabular setting, it suffices to set λ = 0 and update the
entries in Qkh whose corresponding state-action pairs have been visited in the history. In the linear
setting, however, the entire Qkh is updated at once and it’s imperative to prevent the singularity of
the covariance matrix Λkh. This boils down to having a proper choice of the regularization parameter
λ. If λ is too small, Λkh would be nearly singular; if λ is too large, the spectrum of Λkh would
be dominated by λ for prohibitively many episodes with the algorithm making little progress in
learning. Intuitively, since kΦh(∙, j∣∣2 a ∣eβH - 1|, an ideal λ should depend on β and be on
the order of (eβH - 1)2, so that the spectrums of the matrices Φh(∙, ∙)Φh(∙, ∙)> and λId are close.
Indeed, this intuition provides guidance to our choice of λ (see Theorem 2). Our Algorithm 2 hence
demonstrates a synergistic relationship between the surrogate features and regularization in a risk-
sensitive fashion. This is in great contrast with existing algorithms for linear function approximation
in the risk-neutral setting, such as Cai et al. (2019); Zhou et al. (2020), in which the design of
surrogate features and choice of regularization are decoupled.
For general function approximation under Assumption 2, we present Risk-Sensitive Value Iteration
for General function approximation, abbreviated as RSVI.G, in Algorithm 3 of Appendix B. Despite
their apparent difference, Algorithms 2 and 3 both implement the principle of Risk-Sensitive
Optimism in the Face of Uncertainty (RS-OFU) (Fei et al., 2020), by adding bonus/maximizing
over the confidence set when β > 0, and subtracting bonus/minimizing over the confidence set
when β < 0. Such mechanism encourages the algorithms to explore actions that may have rarely
been taken due to low estimated Q-values, while accounting for risk sensitivity at the same time.2
5 Main results
In this section, we present regret guarantees for our algorithms via a meta regret framework. We
identify a risk-sensitive optimism condition, which certifies a certain form of optimism of an
algorithm. Under the condition, we first provide a meta regret bound for Algorithm 1. We then
instantiate the meta regret bound for Algorithms 2 and 3 under Assumptions 1 and 2, respectively.
5.1	Meta regret bound
Recall the iterates {Qkh} in Algorithm 1. For each tuple (k, h, s, a) ∈ [K] × [H] × S × A, we
define Qh(s, a) := rh(s, a) + β log{Es0〜p.(.∣ s,a)ee，Vh+1(S0)}. It can be seen that {Qhh} are the
2We also discuss computational aspects of our algorithms; see Appendix B.
6
Under review as a conference paper at ICLR 2021
ideal counterparts of {Qkh} that could be constructed if the transition kernels {Ph} were known. We
set forth the following condition, which is the central component of our meta regret framework.
Condition 1 For all (k, h, s, a) ∈ [K] × [H] × S × A, we have Qkh (s, a) ∈ [0, H - h + 1], and
there exist some quantities mkh (s, a) > 0, g ≥ 1 and universal constant c > 0 such that
心	-k	e∣β∣H - 1	卜
0 ≤ QhGa)- Qh (s,a) ≤ C ∙	∙ g ∙ mh(s,a).
lβ |
Since {Qh} are informally “optimistic” estimates of the ideal {Q。}, the difference Qh(s,a)-
-^k
Qh(s, a) in Condition 1 may be thought of as the level of optimism maintained by the algorithm
for state-action pair (s, a) in step h of episode k, with its upper bound depending on risk sensitivity
through the factor。|：；-1. Therefore, We say that Condition 1 is arisk-sensitive optimism condition.
In the upper bound of the condition, the actual values of g and mkh (s, a) may depend on function
approximation and implementation of the abstract function RSTD. Let us recall that {(skh, akh)} are
the state-action pairs visited by Algorithm 1, and we are ready to state the meta regret bound.
Theorem 1 Let M := g k∈[K] h∈[H] min{1, mkh(skh, akh)}, where g and {mkh} are as given in
Condition 1. On the event of Condition 1, for any δ ∈ (0, 1], with probability at least 1 - δ the regret
of Algorithm 1 satisfies
Regret(K) . e~~---elβlHHM + elβlH2 PKH3 log(1∕δ).
|e |
The proof is given in Appendix D. Even though the actual form of M depends on specific function
approximation, the derivation of Theorem 1 only requires the structure of Algorithm 1, which is
agnostic of function approximation. In the above bound, the first term can be interpreted as the total
optimism maintained by Algorithm 1, and is in fact a direct consequence of Condition 1. The second
term can be seen as the total drift of iterates {Vhk} from the value functions {Vhπk}, which is the
result of a martingale analysis. The factor elβlH shared by both terms is due to a local linearization
of the nonlinear objective (2) as well as a standard backward induction analysis of H-horizon MDPs.
Soon we will show that M = O(K 1/2) under both linear and general function approximation, so
the first term in Theorem 1 would dominate in the regret bound. Similar to M, the exponential
factor e|：H-1 also comes into the bound from Condition 1. It has been shown as a distinctive
feature of risk-sensitive RL algorithms that represents a tradeoff between risk sensitivity and sample
complexity; see Fei et al. (2020) for a detailed discussion on this point.
5.2	Regret bound for linear function approximation
We now present a regret bound for Algorithm 2 induced by Theorem 1. Let
YL = CY∣eβH - 11Pdlog(2dKH∕δ),	(9)
where cγ > 0 is an appropriate universal constant. We have the following result.
Theorem 2 Let γL of (9) and λ = (eβH - 1)2 be input to Algorithm 2, and M be as defined in
Theorem 1. Under Assumption 1, for any δ ∈ (0, 1], with probability at least 1 - δ, Condition 1
holdsfor Algorithm 2 so Ihat M . [d2KH2 log2(2dKH∕δ)]1/2. Therefore, Theorem 1 implies that
the regret of Algorithm 2 satisfies
Regret(K) < "H- 1 elβlH2 Jd2KH2 log2(2dKH∕δ).
lβ |
The proof is given in Appendix E. One may obtain a regret bound for the tabular setting by taking
d = |S|2 |A| in Theorem 2, and the resulting bound can be seen to nearly match that of Fei et al.
(2020, Theorem 1), except for the polynomial dependency on |S | and |A|.3 The bound obtained
3By inspecting the proof of Fei et al. (2020, Theorem 1), we see that they apply the bound
(1∕∣β∣)(exp(∣β∣ H) — 1) exp(∣β∣ H2) ≤ (1∕∣β∣)(exp(C ∣β∣ H2) — 1) for some universal constant C > 0.
7
Under review as a conference paper at ICLR 2021
from specializing Theorem 2 to the tabular setting is also nearly optimal for small ∣β∣ (with respect
to ∣β∣, K and H) in view of the lower bound
e∣β∣H∕2 - 1	,________
E [Regret(K)] & -	- √K log K	(10)
lβ |
given by Fei et al. (2020, Theorem 3). In addition, as β → 0, the setting of risk-sensitive RL tends
to that of standard risk-neutral RL. We have the following corollary as a precise characterization of
Theorem 2 in that regime.
Corollary 1 Under the setting of Theorem 2 and when β → 0, with probability at least 1 - δ, the
regret of Algorithm 2 satisfies
Regret (K) < Jd2 KH4 log2(2dKH∕δ).
The proof is given in Appendix F. Corollary 1 matches the standard result in the risk-neutral setting,
e.g. Cai et al. (2019, Theorem 3.1), up to logarithmic factors.
5.3	Regret bound for general function approximation
To present the regret guarantee for general function approximation, we need to set a few additional
notations. Recall the function set P from Assumption 2. For any P ∈ P, (s, a) ∈ S × A and
V : S → [0, H], we define the function set
Z := {zp : P ∈ P}, where ZP(s, a, V):= sign(β) / P(s0 | s,a) ∙ (-β.V(s0) - 1)ds0,	(11)
For any P,P0 ∈ P, we define ∣∣P - P0k∞,1 ：= suP(s,a)∈s×A ∣∣P(∙ | s,a) - P0(∙ | s,a)kι. WeIet
dE := dimE(Z, ∣-βH — 1|/K)
be the (∣-βH - 1∣∕K)-eluder dimension of function set Z,4 and
Z= log (H ∙ Nι∕κ(P, k∙ k∞,ι)∕δ) + √log(4K2H∕δ).
In Algorithm 3, we set
YG = ιo∣-βH - 1|√Z.
We now state our result for Algorithm 3, which is another instantiation of Theorem 1.
(12)
(13)
Theorem 3 Let γG of (13) be input to Algorithm 3 and M be as defined in Theorem 1. Under
Assumption 2, for any δ ∈ (0, 1], with probability at least 1 - δ, Condition 1 holds for Algorithm
3 so that M < H min{dE, K} + dEKH2ζ. Therefore, Theorem 1 implies that the regret of
Algorithm 3 satisfies
Regret (K) < -	-1 -lβlH2 (H min{dE, K} + 山石 KH 2Z).
|e |
The proof is given in Appendix G. The term H min{dE , K} above also appears in the regret
bound for the multi-arm bandit problem with general function approximation (Russo & Van Roy,
2014), which is a special case of our finite-horizon episodic MDP setting. When K is sufficiently
large, we have Hmin{dE, K} < ddKKH2Z and therefore Theorem 3 yields Regret(K) =
e|：H—1 -lelH2O(√dEKH2). In case that the transition kernels in P take the linear form as
in Assumption 1, we have dχ < dlog K, and log(N1∕κ(P, ∣ ∙ ∣∞,ι)) < dlog K so that
Z < dlog(KH∕δ). Then for sufficiently large K, the bound in Theorem 3 matches that in Theorem
2 up to a logarithmic factor. On the other hand, under the linear setting of Assumption 1, Algorithm
2 may be more efficient in runtime and memory than Algorithm 3, as discussed in Appendix B.
4Recall that the definition of the eluder dimension is formally given in Appendix A.
8
Under review as a conference paper at ICLR 2021
References
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin F. Yang. Model-based
reinforcement learning with value-targeted regression. arXiv preprint arXiv:2006.01107, 2020.
Nicole Bauerle and Ulrich Rieder. More risk-sensitive Markov decision processes. Mathematics of
Operations Research, 39(1):105-120, 2014.
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, PrzemySIaW Debiak, Christy
Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large
scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.
Vivek S. Borkar. A sensitivity formula for risk-sensitive cost and the actor-critic algorithm. Systems
& Control Letters, 44(5):339-346, 2001.
Vivek S. Borkar. Q-learning for risk-sensitive control. Mathematics of Operations Research, 27(2):
294-311, 2002.
Vivek S. Borkar. Learning algorithms for risk-sensitive control. In Proceedings of the 19th
International Symposium on Mathematical Theory of Networks and Systems-MTNS, pp. 55-60,
2010.
Vivek S. Borkar and Sean P. Meyn. Risk-sensitive optimal control for Markov decision processes
With monotone cost. Mathematics of Operations Research, 27(1):192-209, 2002.
Steven J. Bradtke and AndreW G. Barto. Linear least-squares algorithms for temporal difference
learning. Machine Learning, 22(1-3):33-57, 1996.
Martin Buehler, Karl Iagnemma, and Sanjiv Singh. The DARPA urban challenge: autonomous
vehicles in city traffic, volume 56. springer, 2009.
Qi Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang. Provably efficient exploration in policy
optimization. arXiv preprint arXiv:1912.05830, 2019.
Rolando Cavazos-Cadena and Daniel Herndndez-Herndndez. Discounted approximations for risk-
sensitive average criteria in Markov decision chains With finite state space. Mathematics of
Operations Research, 36(1):133-146, 2011.
Stefano P. Coraluppi and Steven I. Marcus. Risk-sensitive, minimax, and mixed risk-
neutral/minimax control of Markov decision processes. In Stochastic Analysis, Control,
Optimization and Applications, pp. 21-40. Springer, 1999.
Giovanni B. Di Masi and Lukasz Stettner. Risk-sensitive control of discrete-time Markov processes
With infinite horizon. SIAM Journal on Control and Optimization, 38(1):61-78, 1999.
Giovanni B. Di Masi and Lukasz Stettner. Infinite horizon risk sensitive control of discrete time
Markov processes With small risk. Systems & Control Letters, 40(1):15-20, 2000.
Giovanni B. Di Masi and Eukasz Stettner. Infinite horizon risk sensitive control of discrete time
Markov processes under minorization property. SIAM Journal on Control and Optimization, 46
(1):231-252, 2007.
Georgios Fagogenis, Margherita Mencattelli, Zurab Machaidze, Benoit Rosa, Karl Price, F Wu,
V Weixler, Mossab Saeed, John E Mayer, and Pierre E Dupont. Autonomous robotic intracardiac
catheter navigation using haptic vision. Science robotics, 4(29), 2019.
Yingjie Fei, Zhuoran Yang, Yudong Chen, Zhaoran Wang, and Qiaomin Xie. Risk-
sensitive reinforcement learning: Near-optimal risk-sample tradeoff in regret. arXiv preprint
arXiv:2006.13827, 2020.
Wendell H Fleming and William M McEneaney. Risk-sensitive control on an infinite time horizon.
SIAM Journal on Control and Optimization, 33(6):1881-1915, 1995.
Michael Fu et al. Risk-sensitive reinforcement learning: A constrained optimization vieWpoint.
arXiv preprint arXiv:1810.09126, 2018.
9
Under review as a conference paper at ICLR 2021
Daniel Hemdndez-Hemdndez and Steven I. Marcus. Risk sensitive control of Markov processes in
countable state space. Systems & Control Letters, 29(3):147-155, 1996.
Ronald A. Howard and James E. Matheson. Risk-sensitive Markov decision processes. Management
Science, 18(7):356-369, 1972.
Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement
learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010.
Anna JaSkiewicz. Average optimality for risk-sensitive control with general state space. The Annals
of Applied Probability, 17(2):654-675, 2007.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I. Jordan. Provably efficient reinforcement
learning with linear function approximation. arXiv preprint arXiv:1907.05388, 2019.
Steven I. Marcus, Emmanual Ferndndez-Gaucherand, Daniel Herndndez-Hernandez, Stefano
Coraluppi, and Pedram Fard. Risk sensitive Markov decision processes. In Systems and Control
in the Twenty-first Century, pp. 263-279. Springer, 1997.
Oliver Mihatsch and Ralph Neuneier. Risk-sensitive reinforcement learning. Machine Learning, 49
(2-3):267-290, 2002.
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. Sample complexity of reinforcement
learning using linearly combined model ensembles. In International Conference on Artificial
Intelligence and Statistics, pp. 2010-2020, 2020.
Yael Niv, Jeffrey A. Edlund, Peter Dayan, and John P. O’Doherty. Neural prediction errors reveal
a risk-sensitive reinforcement-learning process in the human brain. Journal of Neuroscience, 32
(2):551-562, 2012.
Ian Osband, Benjamin Van Roy, and Zheng Wen. Generalization and exploration via randomized
value functions. arXiv preprint arXiv:1402.0635, 2014.
Takayuki Osogami. Robustness and risk-sensitivity in Markov decision processes. In Advances in
Neural Information Processing Systems, pp. 233-241, 2012.
Stephen D. Patek. On terminating Markov decision processes with a risk-averse objective function.
Automatica, 37(9):1379-1386, 2001.
Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of
Operations Research, 39(4):1221-1243, 2014.
Azad Shademan, Ryan S. Decker, Justin D. Opfermann, Simon Leonard, Axel Krieger, and Peter
C. W. Kim. Supervised autonomous robotic soft tissue surgery. Science translational medicine, 8
(337):337ra64-337ra64, 2016.
Yun Shen, Wilhelm Stannat, and Klaus Obermayer. Risk-sensitive Markov control processes. SIAM
Journal on Control and Optimization, 51(5):3652-3672, 2013.
Yun Shen, Michael J. Tobia, Tobias Sommer, and Klaus Obermayer. Risk-sensitive reinforcement
learning. Neural Computation, 26(7):1298-1328, 2014.
Sebastian Thrun. Toward robotic cars. Communications of the ACM, 53(4):99-106, 2010.
Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki, Michael Mathieu, Andrew Dudzik,
Junyoung Chung, David H. Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al.
Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):
350-354, 2019.
Ruosong Wang, Ruslan Salakhutdinov, and Lin F. Yang. Provably efficient reinforcement learning
with general value function approximation. arXiv preprint arXiv:2005.10804, 2020.
Yining Wang, Ruosong Wang, Simon S. Du, and Akshay Krishnamurthy. Optimism in
reinforcement learning with generalized linear function approximation. arXiv preprint
arXiv:1912.04136, 2019.
10
Under review as a conference paper at ICLR 2021
Peter Whittle. Risk-sensitive Optimal Control, volume 20. Wiley New York, 1990.
Lin F. Yang and Mengdi Wang. Reinforcement learning in feature space: Matrix bandit, kernels,
and regret bound. arXiv preprint arXiv:1905.10389, 2019.
Dongruo Zhou, Jiafan He, and Quanquan Gu. Provably efficient reinforcement learning for
discounted mdps with feature mapping. arXiv preprint arXiv:2006.13165, 2020.
A	Eluder dimension
To introduce the eluder dimension, we need to set forth the concept of ε-independence.
Definition 1 For any ε > 0 and function set G whose elements are in the domain X , we say that an
x ∈ X is ε-dependent on the set of elements Xn := {x1, x2, . . . , xn} ⊂ X with respect to G if any
pair of functions g, g0 ∈ G satisfying i∈[n] (g(xi) - g0(xi))2 ≤ ε2 also satisfies g(x) - g0 (x) ≤ ε.
We say that x is ε-independent of Xn with respect to G if x is not ε-dependent on Xn with respect to
G.
Hence, ε-independence characterizes a notion of dissimilarity of a point x to the elements in subset
Xn of function set G . Now we are ready to formally define the eluder dimension, which quantifies
the length of the longest possible chain of dissimilar elements in a function set.
Definition 2 For any ε > 0 and function set G whose elements are in the domain X, the ε-eluder
dimension dimE (G, ε) is defined as the length d0 of the longest sequence of elements in X such that,
for some ε0 ≥ ε, every element is ε0-independent of its predecessors.
The eluder dimension extends the concept of dimension in linear spaces and generalizes to non-
linear function spaces. It is also related to the notions of Kolmogorov dimension and VC dimension.
We refer readers to Russo & Van Roy (2014) for further details on the eluder dimension and its
advantages compared to other complexity measures.
B More on algorithms
We present details of Algorithm 3 that we have omitted from the main text. For each (k, h) ∈
[K] × [H], transition kernels P, P0 ∈ P and estimated value functions {Vhτ+1}τ∈[k-1], we define
the squared error
Γh(P,P0) := X (ZS P(s0 | sh,ah)eβ∙Vh+1(SO)ds0- ZS P0(s0 | Sh,ah)eβ∙vh+1(SO)ds j . (14)
In Algorithm 3, Line 3 computes an estimate Phk of the transition kernel Ph , by solving a least-
squares problem similar to (7). Line 4 then constructs a confidence set around the estimate Phk .
This step is reminiscent of UCRL (Jaksch et al., 2010), where confidence sets are used to enforce
the so-called Optimism in the Face of Uncertainty (OFU) principle for efficient exploration. Line 5
updates Qkh by solving an optimization problem over the confidence set Phk with the definition
k ( ):= (maxp∈pk Rs P(s0 | ∙, ∙)eβ.Vh+1(SO)ds0, if β > 0,
q",G ， ɪminp∈pk RS P(s0 | ∙, ∙)eβ.Vh+1(SO)ds0, if β < 0.
(15)
It is worth remarking that both Lines 3 and 4 implicitly operate with the shifted exponential
transformation of estimated value functions: replacing eβ'vh+1(') therein by eβ∙Vh+1G) - 1 would
not result in any difference for the algorithm.
Computational aspects. We briefly discuss computational aspects of Algorithms 2 and 3. It is
not hard to see that the time and space complexities for Algorithm 2 are polynomial in key model
parameters d, K and H. For Algorithm 3, itis unclear how the complexities scale under Assumption
2, where the structure of P is unknown. Nevertheless, under Assumption 1 in which the transition
11
Under review as a conference paper at ICLR 2021
Algorithm 3 RSVLG
Input: risk parameter β, number of episodes K, confidence width yg, function set P
1:	Run Algorithm 1 with RSTD() therein overloaded by the following subroutine:
2:	procedure RSTD(k, h, β, {Vhτ+1}τ ∈[k], γG, P)
3:	Ph — argminp∈p PT=1(eβ∙vhτ+1(sh+1) - RS P(s0 | sh, ah)eβ'vh+1(s)ds0)2
4:	Ph 一 {P ∈ P : Γh(P, Ph) ≤ YG}, where Γh(∙, ∙) is defined in (14)
5:	return Qh(∙, ∙) - rh(∙, •) + ⅛ bg(qhG(∙, ∙)), where qkG(∙, ∙) is defined in (15)
6:	end procedure
kernels admit a linear form, Algorithm 3 also attains polynomial time and space complexities with
respect to key model parameters. The actual runtime and memory consumption of Algorithm 3 may
be higher than those of Algorithm 2 though, since the construction of confidence sets in Algorithm
3 requires solving linear programs which could be computationally cumbersome.
C Preliminaries to proofs
We fix a tuple (k, h, s, a) ∈ [K] × [H] × S × A and a policy π. For Algorithm 1 (which subsumes
both Algorithms 2 and 3), define
q2 = qk,2(S, a):= Es0〜Ph(∙∣ s,a)eβ∙Vhk+1(s),	(16)
and
q3 = qk,3(S, a) = Es0〜Ph(∙∣ s,a)eβ∙vh+1(s ).	(17)
In the above definitions, note that q2 and q3 depend on (k, h, s, a); we suppress such dependency for
notational simplicity. We have the following bounds on q2 and q3 .
Lemma 1 We have q2, q3 ∈ [min{1, eβ(H-h)}, max{1, eβ(H-h)}].
Proof. The result for q2 and q3 can be seen if We recall their definitions and the fact that eβ.v(∙) ∈
[min{1, eβ(H-h)}, max{1, eβ(H-h)}] for any V : S → [0, H - h].
For any q0 > 0, define
G1(q0) := 1iog{q0} - 1iog{q2},
β	；	(18)
G2 :=下 log{q2} - β log{q3}.
Since q2, q3 > 0 by definition, G1(q0) and G2 are well-defined. By the Bellman equation (4), for
any π we have
Qh(s,a) = rh(s, a) + 1 log {Es，〜p”. ∣ s,a)eβ,Vhπ+1(s0)}.
We restate Condition 1 in the following.
Condition 2 (Restatement of Condition 1) Let G1(q0) be as defined in (18). For all (k, h, S, a) ∈
[K] × [H] × S × A, assume Qkh(S, a) ∈ [0, H] in Algorithm 1, then there exist some quantities g ≥ 1,
mkh = mkh(S, a) > 0, q1 = qhk,1(S, a) ≥ min{1, eβ(H-h)} and universal constant c1 > 0 such that
Gι(qk,ι(s, a)) = Qh(s, a) - ɪ log{qk,2(s, a)}, and
k	e∣β∣H - 1	k
0 ≤ Gi(qh,i(s,a)) ≤ ci-----∣β∣----g ∙ mh.
It can be seen that Gι(qh i(s, a)) = Qh(s, a) 一 Qh(s, a), where Qh is defined in Section 5. Under
Condition 2, it holds that
(Qh - Qh)(S, a)=后 log{qι}-后 l0g{q3} = Gi + g2,	(19)
ββ
12
Under review as a conference paper at ICLR 2021
by the construction of Qkh in the algorithms, where we have let G1 := G1 (q1). Condition 2 has
unspecified quantities q1, {mkh} and g. The condition, along with those quantities therein, will be
verified in Lemmas 6 and 11 under Assumptions 1 and 2, respectively.
For now let us focus on G2, and we need the following simple result to control it.
Fact 1 Consider x, y, b ∈ R such that x ≥ y.
(a)	if y ≥ go for some go > 0, then log(x) - log(y) ≤ g (X - y)；
(b)	Assume further that y ≥ 0. If b ≥ 0 and x ≤ u for some u > 0, then ebx - eby ≤
bebu(x - y); ifb < 0, then eby - ebx ≤ (-b)(x - y).
Proof. The results follow from Lipschitz continuity of the functions x 7→ log(x) and x 7→ ebx.
We next control G2 , whose proof is agnostic of function approximation.
Lemma 2 For each (k, h, s, a) ∈ [K] × [H] × S × A, ifVhk+1(s0) ≥ Vhπ+1(s0)for all s0 ∈ S, then
we have
0 ≤ G2 ≤ eβH ∙ EsO〜Ph(∙ | s,a)[Vhk+1(s0) - Vh∏+l(s0)].
Proof. Case β > 0. The assumption Vhk+1(s0) ≥ Vhπ+1(s0) for all s0 ∈ S implies that q2 ≥ q3 (by
the definitions of q2 and q3 in (16) and (17)) and therefore G2 ≥ 0 by the definition (18). We also
have
G2 ≤ β (q2 - q3)
≤ eβHEsO〜Ph(∙∣s,a)[Vk+l(s0)- V∏+1(s0)],
where the first step holds by Fact 1(a) (with go = 1, x = q2, and y = q3) and the fact that q2 ≥
q3 ≥ 1 (implied by Lemma 1), and the second step holds by Fact 1(b) (with b = β, x = Vhk+1(s),
and y = Vhπ+1(s)) and H ≥ Vhk+1(s) ≥ Vhπ+1(s) ≥0.
Case β < 0. The assumption Vhk+1(s0) ≥ Vhπ+1(s0) for all s0 ∈ S implies that q2 ≤ q3 and therefore
G2 ≥ 0 due to its definition (18). We also have
G2 = 7~ΞV (log{q3} - log{q2})
(-β)
e-βH
≤ Fβy(q3- q2)
≤ eβH Es，〜Ph(∙∣s,a)[Vk+l(s0)- *l(s0)],
where the second step holds by Fact 1(a) (with go = eβH, x = q3, and y = q2) and the fact that
q3 ≥ q2 ≥ eβH (suggested by Lemma 1), and the third step holds by Fact 1(b) (with b = β,
x = Vhk+1(s), and y = Vhπ+1(s)) and Vhk+1(s) ≥ Vhπ+1(s) ≥ 0.
With the help of Lemma (2), we can show the “optimism” of Qkh in the following sense.
Lemma 3 Suppose (19) holds with G1 ≥ 0. We have Qkh (s, a) ≥ Qπh (s, a) for all (k, h, s, a) ∈
[K] × [H] × S × A.
Proof. For the purpose of the proof, We set QH+1(s, a) = QH+ι(s, a) = 0 for all (s, a) ∈ S ×A.
We fix a tuple (k, s, a) ∈ [K] × S × A and use strong induction on h. The base case for h = H + 1
is satisfied since (QkH+1 - QπH+1)(s, a) = 0 for k ∈ [K] by definition. NoW We fix an h ∈ [H] and
assume that 0 ≤ (Qkh+1 - Qhπ+1)(s, a). By the induction assumption We have
Vhk+1(s) = mOaxQkh+1(s,a0) ≥ mOaxQhπ+1(s,a0) ≥ Vhπ+1(s).	(20)
a ∈A	a ∈A
Applying (20) to Lemma 2 yields G2 ≥ 0. Since G1 ≥ 0 by assumption, it folloWs that (Qkh -
Qhπ )(s, a) ≥ 0 by (19). The induction is completed and so is the proof.
Lemma 3 implies an immediate but important corollary.
13
Under review as a conference paper at ICLR 2021
Lemma 4 Suppose (19) holds with G1 ≥ 0. We have Vhk (s) ≥ Vhπ (s) for all (k, h, s) ∈ [K] ×
[H] × S.
Proof. The result follows from Lemma 3 and Equation (20).
We now have all the keys to proving the meta regret bound.
D Proof of Theorem 1
We work on the event of Condition 2 (which is a restatement of Condition 1), where g and
{mkh} are defined. This means (19) also holds. Define δhk := Vhk(skh) - Vhπk (skh), and ζhk+1 :=
Es0~Ph(∙∣ sh,ah)[vhk+ι(s0) - Vh+1 (SO)] - δh+ι. Let {mh} be as defined in Condition 2. For any
(k, h) ∈ [K ] × [H ], we have
δhk = (Qkh - Qhπk)(Skh, akh)
≤ min{H, (Qkh - Qπhk)(Skh, akh)}	(21)
e∣β∣H _ ι	k
≤ min 1H, ci ∙ "ɪ- ∙ g ∙ m",欧)} + elβlH ∙ Es，~Ph(. | sh,ah)[*ι(s0) — VnH(S0)]
e∣β∣H _ ι	k
≤ ci ∙	,∙ g ∙ min{1,加总3 碌) + elβH ∙ Ey~p,(∙ | sh,ah) W+ι(s0) — V∏+i(s0)]
Iel
(22)
e∣β∣H _ 1
=ci •	,∙ g ∙ min{1,帚^,破)} + elβH倒+i + Zk+i).	(23)
Iel
In the above equation, the first step holds by the construction of Algorithm 1 and the definition of
V∏k in (4); the second step is due to the fact that Qh(∙, ∙) ≤ H and Qn (∙, ∙) ≥ 0; the third step
holds by (19) combined with Condition 2 and Lemma 2; the fourth step holds since ci , g ≥ 1 and
e1：；-i ≥ H; the last step follows from the definitions of δjfk and Zh+].
k
Recalling from Algorithm 1 and the Bellman equation (4) that VHk +i(S) = VHπ+i(S) = 0, as well
as noting the fact that δhk+i + Zhk+i ≥ 0 implied by Lemma 4, we can continue by expanding the
recursion in Equation (23) and get
e∣β∣H _ 1
δk ≤ X eβHhZh+i + ci ∙	∙ X elβlH(h-i)g ∙ miη{1,mh(sh,aQ}
h∈[H]	h∈[H]
(24)
Therefore, we have
Regret(K) = X [(Vi*- Viπk)(sk)i ≤ X δh
k∈[K]	k∈[K]
≤ elβlH2 X X Zhi + ci ∙ eɪɪ ∙ e|e|H2 ∙ g X X mm{1,mh(sh,ah)},
k∈[K] h∈[H]	k∈[K] h∈[H]
(25)
where the second step holds by Lemma 4 with π therein set to the optimal policy, and in the last step
we have applied (24) along with the Holder inequality.
We proceed to control the first term in Equation (25). Since the construction of Vhk is independent
of the new observation Skh in episode k, we have that {Zhk+i} is a martingale difference sequence
satisfying Zhk ≤ 2H for all (k, h) ∈ [K] × [H]. By the Azuma-Hoeffding inequality, we have for
any t > 0,
P I X X Zh+i ≥ tj ≤ exp
k∈[K] h∈[H]
(—2T⅛ ).
14
Under review as a conference paper at ICLR 2021
Hence, with probability 1 - δ∕2, there holds
X X Zh+1 ≤ P2H2T ∙ log(2∕δ).	(26)
k∈[K] h∈[H]
Finally, plugging (26) into (25) yields
Regret(K) ≤ elβlH2 P2H2T ∙ log(2∕δ) + c「e ；	1 ∙elβlH2 ∙g X X min{1,mh(SS.
k∈[K] h∈[H]
We then rescale δ properly and finish the proof of Theorem 1.
To obtain regret bounds for Algorithms 2 and 3, it remains to only specify quantities g and
P∈[K] Ph∈[H] mkh(skh, akh)in Condition 2. The next result prepares us for the quest.
Lemma 5 Let q2 = qhk (s, a) be as defined in (16). If for each (k, h, s, a) ∈ [K] × [H] × S × A,
there exists some quantities g ≥ 1, mkh = mkh(s, a) > 0, q1 = qhk,1(s, a) ≥ min{1, eβ(H-h)}
and universal constant ci ≥ 1 SUCh that Gι(qkk ι(s, a)) = Qh(s, a) — β log{qkk 2(s, a)}, and 0 ≤
qi — q2 ≤ ci ∣eβH — 1∣ g ∙ mh for β > 0 and 0 ≤ q2 — qi ≤ ci ∣eβH — 1∣ g ∙ mh for β < 0, then
Condition 2 holds with the aforementioned c1 , g, mkh and q1 .
Proof. Case β > 0. By the definition of Gi in (19), the assumption 0 ≤ qi — q2 implies that
Gi ≥ 0. Moreover, Lemma 1 and Fact 1(a) (with g0 = 1, x = qi and y = q2) together imply
Gi ≤ β(qi - q2).
Invoking the upper bound on qi — q2 for β > 0 in the assumption completes the proof for the case.
Case β < 0. By the definition of Gi in 18, the assumption 0 ≤ q2 — qi implies that Gi ≥ 0.
Furthermore, by Lemma 1 and Fact 1(a) (with g0 = eβH, x = q2 and y = qi), we further have
Gi = γ-η( (IOg{q2} — log{qi})
(—β)
e-βH
≤lβf (q2 — qi).
Invoking the upper bound on q2 — qi and the fact that ∣eβH — 1∣ = 1 — eβH for β < 0 completes
the proof for the case.
E Proof of Theorem 2
First of all, it can be seen that Line 7 in Algorithm 2 and the initial condition VHk+i(s, a) = 0 in
Algorithm 1 ensure that Qkh(s, a) ∈ [0, H — h + 1] for all (k, h, s, a) ∈ [K] × [H] × S × A. We let
λ = (eβH — 1)2 and γL set as in (9) for Algorithm 2. We define
φkh(s, a), whk	+ 1 +	bkh(s,	a),	ifβ >	0,
φkh(s, a), whk	+ 1 —	bkh(s,	a),	ifβ <	0;
min{eβ(H-h), qi+}, if β > 0,
max{eβ(H-h), qi+}, if β < 0.
(27)
(28)
Indeed, qi defined above is equivalent to qhk,L defined in (8). It can also be verified that Gi(qi) =
Qh(s, a) — β log{q2}, where Gi(∙) is defined in (18) and q2 is defined in (16). We have the following
result which shows that Algorithm 2 satisfies Condition 2 (a restatement of Condition 1) with high
probability.
15
Under review as a conference paper at ICLR 2021
Lemma 6 Under Assumption 1, for any δ ∈ (0, 1], with probability at least 1 - δ, Condition 2 holds
for Algorithm 2 with ci ≥ 1, g = Pd log(2dKH∕δ). mh(s, a) = Jφh (s, a)>(Λh)-1φh(s, a) and
q1as defined in (28).
Proof. Let us fix a tuple (k, h, s, a) ∈ [K] × [H] × S × A. Then, we have
Φh(s,a)τwh + 1 = φh(s,a)>(A皆T	X。1屈&) ∙/-)-1] +1	(29)
τ ∈[k-1]
by Line 4 of Algorithm 2, and
Es0〜Ph(∙ | s,a)eβ,Vh+1(s0) = Z ψ(s, a, s')τθh(eβ,Vh+1(s0) - 1)ds0 + 1
= φkh(s, a)τθh + 1
= φkh(s,a)τ(Akh)-1Akhθh+ 1
=Φh(s,a)τ(Ah)-i	X φh(sh,ah)φh(sh,ah)>θh + λ ∙ θh +1
τ ∈[k-1]
=Φh(s,a)τ(Ah)-i	X φh(sh,ah) ∙ Es，〜ph(∙ ∣ sh,ah叔丹+1(/)-1] + λ ∙ Oh +1,
τ ∈[k-1]
(30)
where the first step follows from Assumption 1, the second step holds by Line 5 of Algorithm 2,
the third step holds since Akh is positive definite by construction, the fourth step holds by Line 3 of
Algorithm 2, and the last step holds since
φh(s, a)>θh = / ψ(s, a, s0)τθh ∙ (e"∙Vh+1(S ) - 1)ds0 = EsO〜Ph(∙ | s,a)[eβ∙Vh+1(S ) - 1]
for τ ∈ [K], which is due to Assumption 1. Now we consider the cases β > 0 and β < 0 separately.
Case β > 0. Recall q1+ defined in (27) and q2 in (16). To control G1 , we can compute
q1+ - q2 - bkh(s, a)
=∣φh(s,a)τwk + 1 - Es0〜ph(∙∣ s,a)eβ∙Vh+1(S )|
=Φh(s,a)τ(Ah)-i	X φh(sh,ah) ∙ SVh+1(sh+1)- Es，〜Ph(∙ ∣ sh^WVh+1”)
|	τ ∈[k-1]
X--------------------------------------{-----------------------------------------}
S1
-λ ∙ φh(s,a)τ(Ah)Tθh ≤ |Si| + 国,	(31)
V---------{----------}
S2
where the first step holds by the definitions of q1+ and q2, and the second step is implied by (29) and
(30). We control each of S1 and S2. For S1, we have
|Si| ≤ I X Φh(sh,ah) ∙ (eβ∙Vh+1(sh+1) -Es，〜Ph(∙ | sh'S，，')"	kΦh(s,明(Ah)-I
τ∈[k-1]	(Λh)
by the Cauchy-Schwarz inequality. On the event of Lemma 8, we further have
|Si| ≤ c∣eβH - 1| Pdlog(2dKH∕δ) ∙ kφh(s,a)Mh)-I
for some universal constant c > 0. Now for S2, we have
|S2| ≤ λ ∙ kφh(s, a)k(Ah)τ ∙ kθhk(Ah)τ
16
Under review as a conference paper at ICLR 2021
≤ C∙kΦh(s,a)k(Λh)-1∙kθhk2
≤ √λd ∙ llφh(s, a)k(Λh)-1,
where the first step holds by the CaUchy-SchWarz inequality, the second step holds since Ah 占 λ ∙ Id,
and the last step holds by Assumption 1 that ∣∣θh∣∣2 ≤ √d. Plugging the bounds on Si and S2 into
(31), and using the fact that λ = (eβH - 1)2 and the definition of bkh, we have
∣Φh(s, a)>wk + 1 — Es0〜Ph(∙ | s,a)eβ,Vh+11 ≤ bhh(s, a).
We choose cγ = c + 1 in the definition of bkh(s, a) in Line 6 of Algorithm 2, and we have
0 ≤ q+ - q2 ≤ 2cγ ∙ ∣eβH - 1∣ √dlog(2dKH∕δ) ∙ kΦh(s,a)k(Λh)-ι .	(32)
l{z}	|-------7--------- 1-------{--------'
c1	g	mkh (s,a)
Since (32) implies q1+ ≥ q2 and Lemma 1 implies q2 ≤ eβ(H-h), we can infer that q1 ≥ q2 from the
definition of q1 = min{eβ(H-h), q1+} in (28). Then we have 0 ≤ q1 - q2 ≤ q1+ - q2. By Lemma 1,
we also have q1 ≥ q2 ≥ 1.
Case β < 0. Similar to the case ofβ > 0, we have
∣q2- q+ - bh(s,a)∣ ≤ c ∙ ∣eβH -11 Pdιog(2dKH/δ) ∙ ∣∣φh(s, a)k(Λh)τ.
If we choose cγ = c + 1 in the definition of bkh(s, a) in Line 6 of Algorithm 2, the above equation
implies
0 ≤ q2 - q+ ≤ 2cγ ∙ ∣eβH - 1∣ Pdlog(2,KH∕δ),∙ ∣φh(s,叫|可『.	(33)
c1	g	m^h(s,a)
Therefore, using the same reasoning as in the case of β > 0, we have 0 ≤ q2 - q1 ≤ q2 - q1+ . Also,
by the definition of q1 in (28), we have q1 ≥ eβ(H-h).
The proof is completed by invoking Lemma 5 and recalling the identity lφkh(s, a)l(Λk)-1 =
q∕φh(s,a)>(Ah)Tφh(s,a).	□
Next, we give a bound for the quantity Pk∈[K] Ph∈[H] min{1, mkh(skh, akh)}.
Lemma 7 Under Assumption 1, let {mkh(s, a)} be as defined in Lemma 6 and we have
XX
min{1, mh(sh, ah)} ≤ √2dKH21,
k∈[K] h∈[H]
where ι = log(2dK∕δ).
Proof. We have
min{1,mkh(skh,akh)}
k∈[K] h∈[H]
≤ x √K E
h∈[H]	k∈[K]
min{1,φkh(skh,akh)>(Akh)-1φkh(skh,akh)}
≤ H√2dKι.
where the first step holds by the Cauchy-Schwarz inequality, and the last step holds by Lemma 10.
Recall the definition of M from Theorem 1, and now its upper bound can be determined by
combining Lemmas 6 and 7. Therefore, the proof of the theorem is completed.
17
Under review as a conference paper at ICLR 2021
E.1 Auxiliary lemmas
We first present a concentration result.
Lemma 8 Let λ = (eβH - 1)2 in Algorithm 2. There exists a universal constant c > 0 such that
for any δ ∈ (0, 1] and (k, h) ∈ [K] × [H], with probability 1 - δ, we have
Il X φh(sh,ah)∙(eβ-Es，〜Ph(∙∣sh,ah)eβ,Vh+1(S0)“	≤ CleeH _ 11 ,dlog(2dKH∕δ)
τ ∈[k-1]	(Λh)
Proof. The proof is adapted from that of Cai et al. (2019, Lemma D.1) by using the fact that for any
function V : S → [0, H], we have
sup	leβ∙V(S，，)
(s,a,s00)∈S×A×S '
-ES，〜Ph(∙∣ s,a)e"∙V(S )| ≤ IeeH - 1| ,
and that Assumption 1 implies
sup Il φ φ(s, a, s0) ∙ (eβV(SO)- 1)ds0∣∣ ≤ √d IeeH - 1∣ .
(S,a)∈S×A	S	2

The next few lemmas can help control the sum of the terms {φkh(skh,akh)>(Λkh)-1φkh(skh,akh)}.
Lemma 9 (Cai et al. (2019, Lemma D.3)) Let {φj}j≥1 be a sequence in Rd. Let Λ0 ∈ Rd×d be a
positive-definite matrix and Λt := Λ0 +	j∈[t-1] φjφj>. Then for any t ∈ Z>0, we have
X min{1,φj>Λj-1φj} ≤ 2log
j∈[t]
detHt+1)
det(Λι)
Lemma 10 Let λ = (eeH - 1)2 in Algorithm 2. For any h ∈ [H], we have
X min{1,φkh(skh,akh)>(Λkh)-1φkh(skh,akh)}≤2dι,
k∈[K]
where ι = log(2dK∕δ).
Proof. By construction of Algorithm 2, we may define Λ0h := λId so we have
Λkh = Λ0h +	φτh(sτh,aτh)φτh(sτh,aτh)>.
τ ∈[k-1]
Since kΦh(s[, ah)∣∣2 ≤ √d IeeH _ 1∣ for all (k, h) ∈ [K] × [H] as implied by Assumption 1, We
have for any h ∈ [H] that
ΛhK+1 =	φkh(skh,akh)φkh(skh,akh)>+λId (dK|eeH - 1|2 + λ)Id.
k∈[K]
Given λ = IIeeH - 1II2, We have for any h ∈ [H] that
log
detSK+1)
det(Ah)
≤ dlog
dK ∣eβH - 1|2 + λ
λ
≤ d log[dK + 1]
≤ dι.
We noW apply Lemma 9 to get
X min{1,φkh(skh,akh)>(Λkh)-1φkh(skh,akh)}≤2log
k∈[K]
≤ 2dι,
as desired.
det。"
det(Ah)

18
Under review as a conference paper at ICLR 2021
F Proof of Corollary 1
The result follows from Theorem 2, as well as the fact that limβ→0 e|：HT = H and
limβ→0 elβlH2 = 1.
G Proof of Theorem 3
First of all, it can be seen that Line 5 in Algorithm 3 and the initial condition VHk+1(s, a) = 0 in
Algorithm 1 ensure that Qkh(s, a) ∈ [0, H — h + 1] for all (k, h, s, a) ∈ [K] × [H] × S × A. We fix
a tuple (k, h, s, a) ∈ [K] × [H] × S × A. Recall that γG is as defined in (13). For Algorithm 3, we
let
k	maxP∈pk Rs P(s0 | s,a)eβ,Vh+ι(s')ds0, if β > 0,
q1	qh,1 S, a	ɪminp∈pk Rs P(s0 | s,a)e?Vh+1(SO)ds0, if β < 0,
(34)
which is equivalent to qhk,G(s, a) defined in (15). Recall the definitions of zp and Z in (11). We have
the result below, which verifies Condition 2 (a restatement of Condition 1) for Algorithm 3 under
the general function approximation.
Lemma 11 Under Assumption 2, for any δ ∈ (0, 1] the following holds with probability at least
1 — δ. For all (k, h, s, a) ∈ [K ] X [H ] ×S×A, Condition 2 holdsforAlgorithm 3 with ci = 1, g = 1,
mh(s, a) = ∣eβj1-ι∣ [maxp∈pk ZP(s, a, Vh+i) 一 minp∈pk ZP(s, a, Vh+i)] and qi as defined in
(34).
Proof. It is not hard to see that by the definition of q1 in (34), we have
q1 ∈ [min{1, eβ(H-h)}, max{1, eβ(H-h)}].
Recall the definitions of q2 in (16) and Gι(∙) in (18)； it holds that G1(q1) = Qh — ɪ log{q2}.On
the event of Lemma 14, we have Ph ∈ Phk .
Case β > 0. By definitions of q1 and q2 and the fact that Ph ∈ Phk, we have q1 ≥ q2. We can also
derive
q1 — q2 = max
P ∈Phk S
≤ max
P ∈Phk S
= max
P ∈Phk S
P(s0
P(s0
P(s0
s, a)eβ.Vh+1(s )ds0 — / Ph(s | s, a)eβ.Vh+1(s )ds0
s,a)eβ∙Vhk+ι(s')ds0 — min Z P (s0 | s,a)eβ∙Vh+ι(s0)ds0
P ∈Phk S
s,a)(eβ∙Vh+1(SO) — 1)ds0 — min P P(s0 | s,a)(eβ∙Vh+1(SO) — 1)ds0
P ∈Phk S
|
|
|
=∣eβH — 1∣ g ∙ mh(s,a),
where the second step holds since Ph ∈ Phk, and the third step holds since S P(s0 | s, a)ds0 = 1.
Case β < 0. By definitions of q1 and q2 and the fact that Ph ∈ Phk , we may deduce that q1 ≤ q2 .
Also, we have
q2 — qι = P Ph(s0 | s,a)eβ.Vh+1(sθ)ds0 — min P P(s0 | s,a)eβ.Vh+1(SO)ds0
S	P ∈Phk S
≤ max/ P(s0 | s,a)eβ,Vh+1(sθ)ds0 — min / P(s0 | s,a)eβ∙Vh+1(sθ)ds0
—
min / P(s0 | s,a)(-eβ.Vhk+1(so))ds0 — ( — 1) max / P(s0 | s,a)(-eβ∙Vh+1(SO))ds0
=max / P(s0 | s, a)(1 — eβ^vh+1(s ))ds0 — m4 / P(s0 | s, a)(1 — eβ^vh+1(s ))ds0
=IeeH - 1∣ g ∙ mh(s,a),
19
Under review as a conference paper at ICLR 2021
where the second step holds since Ph ∈ Phk, and the fourth step holds since S P (s0 | s, a)ds0 = 1.
Finally, invoking Lemma 5 completes the proof.
The following lemma controls Pk∈[K] Ph∈[H] min{1, mkh(skh, akh)}.
Lemma 12 Let d := dimE (Z, eβH - 1 /K). Under Assumption 2, for any δ ∈ (0, 1], let
{mkh(s, a)} be as defined in Lemma 11 and ζ be as defined in (12), then with probability at least
1 - δ, we have
XX
min{1, mh(sh, ah)} ≤ 2Hmin{d, K} + 20HPdKζ.
k∈[K] h∈[H]
Proof. Since min {1, mh(sh, ah)} ≤ mh(sh,ah) for all (k,h) ∈ [K] X [H]. For each fixed
h ∈ [H], by Lemma 15, we have
X min{ι,mh(sh,ah)} ≤ ∣eβH1-1∣ IjeeH — 1| + IeeH — 1| ∙ min{d, K}+5ργ2dκi
k∈[K]	e
≤ ∣eβHl- 1| h2 leβH — 1I ∙ min{d, K} + 5Pγ2dκi
.	.	5	L—
=2 ∙ min{d,K} + VjH	γ2dK
∣eβH — 1|
≤ 2 ∙ min{d, K} + 20 JdK [log (M/k(P J ∙ ∣∣∞,ι) ∙ H∕δ) + Plog(4K2H∕δ)],
where the last step holds since γG = γ where γis given in Lemma 14. Summing both sides of the
above equations over h ∈ [H] results in the desired bound.
Recall the definition of M from Theorem 1, and now its upper bound can be determined by
combining Lemmas 11 and 12. Therefore, the proof of the theorem is completed.
G.1 Auxiliary lemmas
Let Z be a set of [0, D]-valued functions for some number D > 0. We define {(Xτ, Yr)}τ∈[t] be
a series of random variables such that each XT is in the domain of the elements of function set Z,
and each Yτ ∈ R. Let F = {Fτ}τ≥1be a set of filtrations such that for all τ ≥ 1, the random
variables {X1, Y1, . . . , Xτ-1, Yτ-1, Xτ} is Fτ-1-measurable. Furthermore, we assume there exists
a function z* ∈ Z such that E[YT | FT-ι] = z*(Xτ). For any ε > 0, we denote by Nε(Z, ∣HI∞) the
ε-covering number of Z with respect to the supremum norm ∣∣z1 一 z2∣∣∞ = SuPx ∣zι(x) — z2(x)∣.
We define
^t := argmin X (z(Xτ) 一 Yr)2,
z∈Z T ∈[t]
and for γ≥ 0, let
Zgy= ∖z∈Z： X(z(Xτ)一Zt(XT))2≤γ2∖.
[	T ∈[t]
We record a concentration result.
Lemma 13 Suppose that for any τ ≥ 1, the random variable YT 一 z*(XT) is conditionally σ-sub-
Gaussian given filtration FT-1. Let
Y2(δ, ε) := 8σ2 log (N^ε(Z, ∣∙ ∣∣∞)∕δ) + 4εt (D + Pσ2 log(4t(t + 1)∕δ)).
Then for any ε > 0 and δ∈ (0, 1], with probability at least 1 一 δwe have z* ∈ Zt(γt(δ, ε)).
20
Under review as a conference paper at ICLR 2021
Proof. The proof can be adapted from that of Russo & Van Roy (2014, Proposition 6).
In the following, we use the shorthand γ := γG, where γG is defined in (13) and used in Line 4 of
Algorithm 3.
Lemma 14 For any δ ∈ (0, 1] and
Y Y = 10 IeeH - 1∣2 [log (Nι∕κ (P, k∙k∞,ι) ∙ HM + ,log(4K2H∕δ)卜
then for all (k, h) ∈ [K] × [H] we have Ph ∈ Phk with probability at least 1 - δ.
Proof. We first note that for any (k, h) ∈ [K] × [H], Line 3 in Algorithm 3 can be equivalently
written as
Ph — argmin X ((eβ∙Vh+ι(sh+ι) - 1) - P P(s0 | sh,ah) ∙ (eβ∙Vh+ι(s0) - 1)ds0)2,
P∈P τ∈[k-1]	S
since S P(s0 | sτh, aτh)ds0 = 1. Recall the definition of zP in (11). For any (k, h) ∈ [K] × [H],
We set Z = Z, Yk = eβ∙Vh+1(Sh+1) - 1, Xk = (Sh, a£, Vjfk+1) and z* = ZPh. Then, We have that
Yτ - z*(Xτ) is conditionally (∣eβH - 1∣)-SUb-GaUssian for T ∈ [k - 1] given a properly defined
filtration. By the definition of Phk, We have Zk(γ) = {zP : P ∈ Phk}. By definition of γ, We have
γ ≥ γk-1(δ∕H, ∣eβH - 1∣∕K)
for all k ∈ [K], where γt(∙, ∙) is as defined in Lemma 13. By Lemma 13 with D = σ = ∣eβH - 11
and ε = ∣eβH - 1∣ ∕K, With probability at least 1 - δ∕H and for all k ∈ [K], We have
z* ∈ Zk(γk-1(δ∕H, ∣eβH -1∣∕K)) ⊂ Zk(γ),
thUs implying Ph ∈ Phk . Applying the Union boUnd over h ∈ [H], we have that Ph ∈ Phk with
probability at least 1 - δ. Now we show that Nε(Z, k ∙ ∣∣∞) ≤ Nε∕∖eβH-ι∣(P, k ∙ k∞,ι) for any
ε > 0. Let V := {V : S → [0, H]}. For any P, P0 ∈ P and their corresponding zP, zP0 ∈ Z, we
can compUte
∣∣zp - Zp0 k∞ = sup I [ P(s0 ] s,a)(eβ∙V(SO)- 1)ds0 — P P0(s0 | s,a)(eβ"(SO)- 1)ds0
(s,a,V)∈S×A×V ∣ S	S
≤ IeeH - 1∣ ∙ sup I |P(s0 | s, a) — P0(s0 | s, a)| ds0
(S,a)∈S ×A S
= ∣eβH - 1∣∙∣P - P0k∞,ι,
as desired.

We have the following resUlt on the elUder dimension.
Lemma 15 Let Z = Z and d = dimE (Z, ∣eβH — 1∣ /K). For any K ≥ 1, β ∈ R and Y > 0, we
have
^X	sup	∣z(xk) — z0(xk)| ≤ IeeH — 11 + IeeH _ 1∣ ∙ min{d, K} + 4pγ2dK.
k∈[K] z,zO∈Zk(γ)
Proof. This resUlt is an adaptation of RUsso & Van Roy (2014, Lemma 5) with C = ∣eβH - 1∣
therein.
21