Under review as a conference paper at ICLR 2021
PURE: An Uncertainty-aware Recommendation
Framework for Maximizing Expected Poste-
rior Utility of Platform
Anonymous authors
Paper under double-blind review
Ab stract
Commercial recommendation can be regarded as an interactive process between
the recommendation platform and its target users. One crucial problem for the
platform is how to make full use of its advantages so as to maximize its utility,
i.e., the commercial benefits from recommendation. In this paper, we propose
a novel recommendation framework which effectively utilizes the information of
user uncertainty over different item dimensions1 and explicitly takes into consid-
eration the impact of display policy on user in order to achieve maximal expected
posterior utility for the platform. We formulate the problem of deriving optimal
policy to achieve maximal expected posterior utility as a constrained non-convex
optimization problem and further propose an ADMM-based solution to derive an
approximately optimal policy. Extensive experiments are conducted over data col-
lected from a real-world recommendation platform and demonstrate the effective-
ness of the proposed framework. Besides, we also adopt the proposed framework
to conduct experiments with an intent to reveal how the platform achieves its com-
mercial benefits. The results suggest that the platform should cater to the user’s
preference for item dimensions that the user prefers, while for item dimensions
where the user is with high uncertainty, the platform can achieve more commer-
cial benefits by recommending items with high utilities.
1	Introduction
Commercial recommendation systems have been widely applied among prevalent content distribu-
tion platforms such as YouTube, TikTok, Amazon and Taobao. During the interactive process on the
recommendation platform, the users may find contents of their interests and avoid the information
overload problem with the help of recommendation services. Meanwhile, the platform may gain
commercial benefits from user behaviors on the platform such as clicks and purchases. As the plat-
form may serve millions of users and can determine which contents to be recommended, it naturally
has some advantages over individual user. Therefore, it would be crucial for the platform to make
full use of its advantages in order to maximize the commercial benefits.
One typical advantage of the platform is its information advantage, i.e., they may collect plenty of
information over users and items for conducting better recommendation. Typical state-of-the-art
recommendation systems (Covington et al., 2016; Guo et al., 2017; Ren et al., 2019; Zhou et al.,
2019) always take these information into consideration including user profiles, item features and
historical interactions between users and recommended items. It is worth noting that information
over item features is always directly incorporated into the recommendation models without consid-
ering that the user may be with different levels of uncertainty over different item dimensions (which
can be regarded as different hidden attributes describing different high-order features of the item).
For instance, when buying a new coat on the platform, a user may be sure that the logistics is very
fast as she (he) has bought clothes from the same online store before (i.e., the user is with low un-
certainty over the logistics). But she (he) may be uncertain about the quality of the coat since it is
of the brand that she (he) does not know much about (i.e., the user is with high uncertainty over the
quality). Thus, it would be crucial for the platform to figure out whether it is possible to leverage the
user uncertainty over different item dimensions to maximize the platform utility, and if yes, how?
1 Item dimensions: Typical state-of-the-art solutions for recommendation systems always encode each item
as an embedding. The item dimensions refer to different dimensions of the item embedding, which can be
explained as different high-order features.
1
Under review as a conference paper at ICLR 2021
Actually, with consideration of the user uncertainty over different item dimensions, we would show
that more commercial benefits can be gained from the item dimensions with higher uncertainty.
Another advantage of the platform is that it owns the capacity of determining which items to display
for the users and thus may affect the users’ behaviors. It has been proved by lots of works (Kamenica
& Gentzkow, 2011; Immorlica et al., 2019; Abdollahpouri & Mansoury, 2020) that the display
signal itself would highly affect users’ behaviors, and affected behaviors would apparently result in
different benefits for the platform. Regarding the recommendation as a game between the platform
and the users, it is possible for the platform to achieve more commercial benefits from the game
by taking a proper display (recommendation) policy. However, though there are works to explore
the impact of recommendation policies, it is still not well-studied in recommendation area how to
explicitly model and exploit the impact of the display policy over users.
In this paper, we propose an uncertainty-aware expected Posterior Utility maximization framework
for REcommendation platforms (denoted as PURE in short). We take both the two previously men-
tioned factors, i.e., user uncertainty over different item dimensions and influence of display policy
over the user, into account and introduce a generic utility function which can be flexibly adjusted for
different real-world scenarios. Then, we formulate the problem of maximizing expected posterior
utility for the platform as a constrained non-convex optimization problem, and correspondingly pro-
pose a solution based on Alternating Direction Method of Multipliers (ADMM, Boyd et al. (2011))
to derive the approximately optimal policy. To verify the effectiveness of the proposed framework,
extensive experiments are conducted over data collected from a real-world recommendation plat-
form. Furthermore, we also provide practical insights derived from carefully designed experiments
and empirically reveal how the platform utilizes its information advantage to achieve more commer-
cial benefits, which may help to better understand and conduct commercial recommendation.
2	Related Work
Existing state-of-the-art recommendation systems (Zhou et al., 2018; Pi et al., 2019; Qu et al., 2016)
mainly try to make full use of the information advantage of the platform. These works take these
information into consideration including user profiles, item features, contextual information and
historical interactions between users and recommended items. Typically, some works (Qu et al.,
2016; Zhou et al., 2018; Li et al., 2019) focus on how to achieve better feature interactions or
conduct better user interest modeling, while some works (Ren et al., 2019; Pi et al., 2019) may
pay more attention to utilizing extremely long sequential interactive information. However, most of
them ignore the existence of user uncertainty over different item dimensions, which might be crucial
to conduct better commercial recommendation.
In the research area to explore the display influence to the information receiver, Bayesian Persuasion
(Kamenica & Gentzkow, 2011) is one of the most crucial works, which theoretically proves that the
information sender may benefit from displaying proper information to the receiver. Some works
(Immorlica et al., 2019; Mansour et al., 2016) follow this idea and strive to incentivize exploration
via information asymmetry in scenarios such as recommendation. In another research direction that
try to develop Reinforcement Learning (RL) based solutions for recommendation scenarios, a series
of works (Dulac-Arnold et al., 2015; Zhao et al., 2018; Chen et al., 2019) model the recommendation
process as a Markov Decision Process (MDP) and maximize the long-term reward via utilizing
learned sequential patterns, which can also be regarded as taking the display (recommendation)
influence into consideration to some extent.
3	Methodology
3.1	Optimal Policy for Maximizing Platform’ s Expected Posterior Utility
From the perspective of the platform, the optimal recommendation policy is the one with maximal
expected utility (i.e., maximal expected commercial benefits). As mentioned before, the influence
of display policy over users can not be ignored as it would highly affect the commercial benefits
of the platform. In this paper, taking the impact of display policy on users into consideration, we
formulate the platform’s optimal policy πu for user u over a given item set I as follows.
πu = argmax	∏iUu(i∣display; ∏), s.t., ∀i ∈ I, ∏i ≥ 0 and	πi = 1,	(1)
π	i∈I	i∈I
where Uu(i|display; π) is the posterior utility of recommending item i to user u with consideration
of the influence of display policy π. With this formulation, the remaining problem is how to model
2
Under review as a conference paper at ICLR 2021
the posterior utility properly. In the following, we illustrate two reasonable assumptions in detail,
which make it possible to model the posterior utility with consideration of the user uncertainty over
different item dimensions as well as the influence of display policy over user.
As discussed before, it would be crucial to explicitly consider the user uncertainty over different item
dimensions to conduct recommendation. For a given user, we assume that the representation for an
item is sampled from a multivariate Gaussian distribution and adopt the variances to describe the
user uncertainty over different item dimensions, which is formulated as the following assumption.
Assumption 1 (Assumption of uncertainty (correlation) over different item dimensions). Fora user
u, the representation of item i is sampled from a n-dimension multivariate Gaussian distribution
N(μ如i, ∑u,i), i.e., the probability density function ofthe representation is:
pu,i(x)
ɪ e- 1 (χ-μu,i)T ςu i(x—μu,i)
P(2∏)n∣∑u,i∣	,
(2)
where X ∈ Rn, μ"i and ∑u,i denote the mean vector and the Covariance matrix respectively.
The covariance matrix can be decomposed as Σu,i = Du,iCu,iDu,i, where Du,i is the diagonal
standard deviation matrix and Cu,i is the correlation matrix (see Barnard et al. (2000) for more infor-
mation). Thus, the covariance matrix can depict the user uncertainty over different item dimensions
(with diagonal standard deviation matrix Du,i) as well as the correlation between different item di-
mensions (with correlation matrix Cu,i). Note that We provide a practical method to gain μ如i and
∑u,i in Section 4.1 while any other reasonable approach to get μ%i and ∑u,i can be applied.
From the perspective of users, they may try to understand the display policy of the platform from
the interactive process. When an item i is recommended to a user u, the user may consider the
corresponding display probability, which could influence his behavior. One reasonable assumption
is that the probability of displaying item i from the perspective of user u is linear to the similarity
between the item representation x and the representations of historical recommended items. Without
loss of generality, we formulate this assumption as follows.
Assumption 2 (Assumption of the influence of the display policy over user). Given display policy
π and item representation x, the probability of recommending item i to user u from the user’s
perspective is:
pu,i(display|x; π) = Φ(avuT x + b),
(3)
where display denotes the event of displaying (recommending) the corresponding item to the tar-
2
get user, a and b are scale hyper-parameters, a > 0, Φ(z) = f ∞ √2∏e-ɪ dx and Vu 二
Ei~π(μu,i) = ɪ2i∈I πiμu,i.
Note that Φ is the cumulative distribution function of standard normal distribution, which is widely
adopted to map the input into range [0, 1] in many models such as probit model, and vu takes the
expected value of item representations w.r.t. the display policy π over item set I. Thus, with a > 0,
higher similarity (calculated by inner product) between the current item representation x and the
expected representation of display items would lead to higher likelihood, which is reasonable as
discussed before.
So far, we have presented two crucial assumptions adopted in our framework. In the following, we
introduce the utility function and derive the formula of posterior utility. To ease the illustration,
we denote the utility function of recommending item i to user u given sampled item representation
x as f(wu, x), where wu is the embedding of user side typically and f (wu, x) depicts the utility
(benefits) that the platform can gain from recommending item i to user u with item representation x.
For instance, when we regard the click trough rate (CTR) as the platform’s utility, one simple way
is to model f as the inner product of wu and x where wu can be regarded as the user preference
vector of user u. Note that the function f can be flexibly adjusted to fit different requirements of
different scenarios. For example, when we try to maximize the gross merchandise volume (GMV) in
a recommendation scenario, f can be defined as the product of the corresponding CTR, conversion
rate (CVR) and the item price.
Now we can combine the two assumptions and the utility function f to derive the formula of poste-
rior utility. By adopting the Bayes’ theorem and the law of total probability, we present the formula
of posterior utility of recommending item i to user u as follows.
Uu (i|display; π) =	f(wu, x)pu,i(x|display; π) dx
Rn
(4)
3
Under review as a conference paper at ICLR 2021
f(wu, x)pu,i (x)
Rn
Pu,i(display∣x; ∏)
Pu,i(display; π)
dx
(5)
Equation 5 provides a way to model the posterior utility of the platform taking into account both
the user uncertainty over different item dimensions and the influence of display policy over user.
However, it is still challenging to derive the optimal policy πu for given user u as the right part of
Equation 5 is not a close-form expression (which makes it intractable to calculate the exact value of
the posterior utility).
3.2	Posterior Utility Derivation for Linear and Non-linear Utility Functions
In this section, we present how to compute (estimate) the value of posterior utility for both linear
and non-linear utility functions. To derive an effective calculation formula of Uu(i|display; π) for
the case with linear utility function, we first introduce a lemma in the following.
0 0	b02	0
Lemma 1. R-+∞∞(c0x + d0)Φ(a0x + b0)N(x|0,1) dx = √====== e 2a02+2 + d0Φ( √]bd),∀a0,b0,
c0, d0 ∈ R.
The detailed proof of Lemma 1 is provided in Appendix A.1 due to page limits. Lemma 1 reveals
the fact that the definite integral R-+∞∞ (c0x + d0)Φ(a0x + b0)N(x|0, 1) dx can be computed directly,
which is a crucial property to derive an effective calculation formula of Uu(i|display; π) for the
case with linear utility function.
By utilizing Lemma 1, the following corollary shows that pu,i(display; π) can be straightforwardly
calculated if the covariance matrix is positive definite.
Corollary 1. pui(display; π) = Φ( / avu μu,i+b	), if ∑ui is positive definite.
1+a2vuTΣu,ivu
Due to page limits, the detailed proof of Corollary 1 is also presented in Appendix A.2. With
Lemma 1 and Corollary 1, we can derive the calculation formula of Uu(i|display; π) if the utility
function is linear, which is illustrated in detail as follows.
Corollary 2. Uu(i|display; π)
T
Wu μu,i+
avT ∑u,iWu
√2π(a2vT ∑u,iVu + 1)Φ(
iff(wu, x) = wuT x and Σu,i is positive definite.
avT μu,i + b
ʌ/ɑ2VT Zu,i vu+1
(aVT Nu,i+b)2
2(a2vT Nu,ivu+I)
)
e
Proof. If Σu,i is positive definite, Σu-,1i is also positive definite. Then we can conduct Cholesky
decomposition: Σu-,1i = ATu,iAu,i, where Au,i is an upper triangular matrix with real and positive
diagonal entries. Thus, we have:
f(wu,x)pu,
Rn
i(x)pu,i(display|x; π) dx
(6)
/
Rn
avuT x
wuT (Au-,1iz
Rn
+ b)N(x∣μu,i, Σu,i) dx
+ μu,i)φ(αvT(A-,iZ + μu,i) + b)N(zl0, I) dz
WTμu,iΦ(	,avT% + b ) +	,	avT%Wu	— e 2(aaV⅛UiX)+i)
√a2vT Σu,iVu + 1	√2π(a2vT Σu,iVu + 1)
(7)
(8)
(9)
Equation 8 and Equation 9 are derived from Proposition 3 and Proposition 5 respectively. Due to
page limits, Proposition 3 and Proposition 5 and their detailed proofs are presented in Appendix.
Combing Corollary 1 and Equation 9, we have:
Uu(i|display; π)
———ɪ-----------ʌ f f(wu, X)Pu,i(x.)pu,i(display∖x; π) dx
pu,i (display; π) Rn
(10)
T
Wu μu,i +
avTΣu,iWu____________ -
,2π(a2vT £u”u + 1>( √o⅞ ∑u,i+b +J ɛ
a vu Σu,i vu +1
(aVT Nu,i + b)2
2(a2vT Nu,ivu+1)
(11)
4
Under review as a conference paper at ICLR 2021
□
Corollary 2 reveals that the posterior utility can be effectively calculated (since it is with a close-
form expression and thus can avoid the intractable calculation in Equation 5) when Σu,i is positive
definite and the utility function f is linear, which makes it possible to be effectively utilized in the
real-world scenarios.
However, when the utility function f is non-linear, it might be challenging or even impossible to
calculate the exact value of the posterior utility. To estimate the posterior utility when f is non-
linear, we leverage the importance sampling technique to approximate the posterior utility. Combing
Assumption 2, Equation 5 and Corollary 1, we have:
Uu(i|display; π) =	f (wu, x)pu,i(x)l(x; π) dx,
Rn
(12)
Φ(avT x+b)
φ( avT μu,i+b )
√a2vT Zu,ivu+1
where l(x; π)
Thus, computing Uu(i|display; π) can be regarded as calcu-
lating the expected value of f(wu, X) w.r.t. the target probability density function q(X = x) =
pu,i(x)l(x; π). Since pu,i (x) is the probability density function of a multivariate Gaussian distri-
bution from which one can easily conduct sampling, we can adopt pu,i (x) as the sampler density
and as a result, given a sample x, the corresponding importance weight is l(x; π). Thus, given a
sample set X which consists of m samples drawn i.i.d. according to distribution N(μ%i, ∑u,i), We
can approximate the posterior utility as:
1
Uu(i∣display; π) = P------lχ∏) Tf (wu, x)l(x; ∏).
(13)
In this section, we derive an effective calculation formula for posterior utility in the case with linear
utility function and provide a sampling-based method to estimate posterior utility in the case with
non-linear utility function. Note that the sampling-based method can also be applied to estimate the
posterior utility for the case with linear utility function. Combining the results in this section (i.e.,
Equation 11 and Equation 13) and the formulation of optimal policy (as shown in Equation 1), we
notice that the problem of deriving the optimal policy for maximizing platform’s expected posterior
utility, which is formulated in Equation 1, is turned into a constrained non-convex optimization
problem even for the case with linear utility function. To solve the problem, an ADMM-based
solution is proposed in the next section.
3.3	An ADMM-based S olution for Deriving Approximately Optimal Policy
As presented in the previous section, finding the optimal policy to maximize the expected posterior
utility of the platform is formulated as a constrained non-convex optimization problem. Noticing that
the ADMM technique (Boyd et al., 2011) has been successfully adopted as an effective method to
find approximate solution for constrained non-convex optimization problem (see Leng et al. (2017)
for more details), we develop an ADMM-based solution to solve the aforementioned constrained
non-convex optimization problem.
Denoting gu,ι(∏) = - £记口 ∏iUu(i∣displαy; π), we rewrite the constrained non-convex OPtimiza-
tion problem (which is formulated in Equation 1) as follows.
argmin gu,I(π), s.t., ∀i ∈ I, πi ≥ 0 and	πi = 1.
By introducing an auxiliary parameter vector π0 , the problem can be reformulated as:
argmin gu,I(π0), s.t., π0 = π, ∀i ∈ I, πi ≥ 0 and	πi = 1.
(14)
(15)
i∈I
π
Note that the target function gu,I(π0) and the target policy π is linked via the first constrain, i.e.,
π0 = π . We regard the last two constrains as hard constrains which are required to be satisfied at
any optimization step. And the augmented Lagrangian w.r.t. to the first constrain is:
Lρ(∏, ∏0, λ) = gu,ι(∏0) + λτ(π0 - ∏) + p∣∣∏0 - ∏∣∣2,	(16)
5
Under review as a conference paper at ICLR 2021
where λ is the Lagrangian multipliers and ρ > 0 is a hyper-parameter. To achieve an approximate
solution, the ADMM method consists of the following three steps for each iteration:
π(k+1) := argmin Lρ(π, π0(k), λ(k))	(17)
π
π0(k+1) := argmin Lρ(π(k+1), π0, λ(k))	(18)
π0
λ(k+1) := λ(k) + ρ(π0(k+1) - π(k+1))	(19)
We can observe that the last two steps do not involve optimization w.r.t. π. Thus, the third step
can be simply achieved by value updating while the second step can be approximately solved by
adopting gradient-based solutions such as stochastic gradient decent. The optimization problem
w.r.t. the first step can be reformulated as:
argmin Lρ(∏, π0(k), λ(k)) = argmin λ(k)T (π0(k) — π) + P ∣∣π0(k) — π∣∣2	(20)
π	π2
=argmin ρ ∣∣π0(k) + λ----∏∣∣2,	(21)
π2	ρ
s.t., ∀i ∈ I, πi ≥ 0 and	πi = 1,	(22)
i∈I
which is a convex quadratic programming problem and can be solved by convex quadratic program-
ming solutions such as active set methods (Wong, 2011). It is worth noting that the optimization
problem w.r.t. the first step can be regarded as finding the nearest point from a given convex set
(which satisfy the two constrains) to a given point (i.e., π0(k) + λρk)).
By iteratively conducting the aforementioned three optimization steps until convergence (say, after
k0 steps), we can acquire an approximately optimal policy π(k0) for maximizing expected posterior
utility of the platform, which is demonstrated to be great enough in the experiment section.
4	Experiments
In this section, we present the experiment setup and the experiment results in detail. First, we present
the detailed setup of the experiments including a practical way to gain the mean vector and covari-
ance matrix for given pair of user and item, which makes it possible to take the user uncertainty over
different item dimensions into consideration. Second, we conduct experiments to verify the effec-
tiveness of the proposed technical solutions involved in our framework, i.e., the effectiveness of the
ADMM-based solution for maximizing platform expected posterior utility and the sampling-based
technique for posterior utility estimation. Third, to verify the superior of the policy derived from
the proposed framework, we conduct experiments and present the results of comparison between
our policy and other two heuristic policies. Fourth, to answer the crucial question that how the
platform maximizes its commercial benefits, we analyse the relation between the learned policy and
two significant factors, the user preference and uncertainty over given item dimension, and provide
practical insights to better understand and conduct commercial recommendation.
4.1	Experiment Setup
To conduct experiments, we first collect click log data of about 1 million users and 5 million items
from the GUESS U LIKE scenario of the Taobao Application. To encode the users and the items,
we adopt prevalent Embedding&MLP paradigm (He et al., 2017; Guo et al., 2017) and encode the
information of the user and the item sides respectively (Xu et al., 2019). By training on specific task
(e.g, click through rate prediction task), we can gain the embedding of the user side as well as that
of the item side. Without loss of generality, we consider the following two model architectures: i)
the model output is the inner product of the user and the item embeddings; ii) the model output is
achieved by applying a non-linear function over the user and the item embeddings. Note that these
two architectures correspond to the cases with linear and non-linear utility functions respectively.
For each pair of user group (divided according to age and gender) and item category, the clicked item
set is extracted and the mean vector and the covariance matrix of the corresponding item embeddings
are calculated. For a given pair of user and item, the mean vector and the covariance matrix can be
gained by adopting those of the pair of the corresponding user group and item category.
6
Under review as a conference paper at ICLR 2021
Without loss of generality, we empirically set the value of hyper-parameters a and b to ensure that
most of the values of pu,i (display|x; π) lie between 0.001 and 0.1 (noticing that other reasonable
ranges also lead to similar experimental conclusions). Since Φ is a function in the form of integral
which might be intractable to calculate, we adopt an effective approximation (Waissi & Rossin,
1996) to ease the calculation. To avoid learning a highly unbalanced policy (with probability 1 for
one item while with 0 for others) due to unbalanced utilities, we also incorporate an entropy-based
regularization term into our model. Specifically, instead of directly adopting the optimization step
as shown in Equation 18, we add an extra term to smooth the learned policy and the corresponding
optimization step is:
∏0(k+1) := argmin Lρ(∏(k+1∖ ∏0, λ(k)) + η X(∏i - ∣1∣ )2,	(23)
π0	i∈I	|I|
where |I| denotes the number of items in item set I, η is the parameter to control the smooth level
of the learned policy and is set to 10.0 empirically in the experiments. Besides, the optimizer we
adopted for gradient decent is the Adam Optimizer (Kingma & Ba, 2014) and the value of ρ is set
as 1.0 empirically, which is proved to be effective in the following experiments.
4.2	Effectiveness of the Proposed Technical Solutions
Figure 1: Expected posterior utility w.r.t. optimization iterations for three cases
non-linear case with estimated
optimization iterations
optimization curves for user 1
0	20	40	60	80 IQO
optimization iterations
optimization curves for user 2
= 0.26
⅛
∙≡ 0.24
ω
0.0.22
P
(υ
¥ 0.20
d
0.18
6	20	40	60	80	100
OPtlmIZatIon CUrVeS for USer 3
---- with exact value
1 sample per estimation
---- 10 samples per estimation
---- 100 samples per estimation
— 1000 samples per estimation
optimization iterations
Figure 2: Optimization curves w.r.t. different numbers of samples per estimation
In this section, we present the experiment results to demonstrate the effectiveness of the two pro-
posed technical solutions absorbed into our framework, i.e., the effectiveness of the ADMM-based
solution for maximizing expected posterior utility and the importance sampling technique for pos-
terior utility estimation.
To demonstrate the effectiveness of the proposed ADMM-based solution for maximizing expected
posterior utility, we record the optimization curves (i.e., expected posterior utility w.r.t. the number
of optimization iterations) and present the results in Figure 1. According to Section 3.2, there are
three cases to be considered: i) linear utility function with exact posterior utility (calculated by
adopting Equation 11); ii) linear utility function with estimated posterior utility; iii) non-linear utility
function with estimated posterior utility. We randomly sample 3 users and 100 items and utilize the
proposed ADMM-based optimization method to learn the approximately optimal policy to maximize
the expected posterior utility of the platform. As shown in Figure 1, the expected posterior utility
first increases and then converges to its maximum with the increase of the number of optimization
iterations. These results demonstrate that the proposed ADMM-based solution can effectively learn
a policy to maximize the expected posterior utility. Besides, comparing the optimization curves of
the three cases, we can observe that the variances of the expected posterior utility in the cases with
7
Under review as a conference paper at ICLR 2021
estimated posterior utility Uu(i|display; π) (the last two cases) are higher than that in the case with
exact calculated posterior utility Uu(i|display; π) (the first case). The reason is that extra random
noise is induced by importance sampling when we conduct posterior utility estimation and as a
result, expected posterior utility is with higher variance in the case with estimated posterior utility.
In order to verify the effectiveness of the adopted importance sampling technique for posterior es-
timation, we record the optimization curves w.r.t. different numbers of samples for estimating pos-
terior utility for three randomly sampled users and present the results in Figure 2. The linear utility
function is adopted to conduct the experiments which makes it possible to make comparison be-
tween the results of utilizing estimated posterior utility (estimated according to Equation 13) and
exact posterior utility (calculated according to Equation 11). From Figure 2 we can observe that
even in the case with 1 sample per estimation, the tendency of the optimization curve is similar to
that of the case with exact posterior utility, which verifies that the adopted sampling-based tech-
nique for posterior utility estimation is effective. Besides, when varying the number of samples per
estimation from 1 to 1000, we observe that the optimization curves become more and more stable.
It is because that larger number of samples per estimation leads to more accurate approximation
of posterior utility, and thus results in more stable optimization curve. More details including the
Root Mean Square Error (RMSE) between the estimated values and the exact values of the posterior
utilities for each sampling setting are provided in Appendix A.4 due to page limits.
4.3	Experimental Comparison with Heuristic Policies
In this section, we present the experiment details and the corresponding results to verify the effec-
tiveness of the policy derived from the proposed framework. To the best of our knowledge, the
presented framework is the first complete work that proposes to maximize the expected posterior
utility (which explicitly takes the impact of display policy into consideration) and develops a cor-
responding solution to derive the approximately optimal policy. Thus, there may lack solutions for
conducting straightforward comparison and we intent to verify the effectiveness of the learned pol-
icy by comparing it with other heuristic policies. One simple policy for comparison is the random
policy that recommends each item with same probability. Thus, the random policy can be regarded
as an uniform probability distribution over the candidate items. We also incorporate a heuristic prior
policy for comparison where the recommendation probability is higher for item with higher prior
utility2. Specifically, the recommendation probability of item i is given by
pop
πi
Uu(i)
e T
7ζ Uuj)
∑j∈ιe T
(24)
where τ is the temperature parameter (Hinton et al., 2015) to control the smooth level of the heuristic
prior policy. For conducting fair comparison, we adjust the value ofτ to ensure that the smooth levels
of the heuristic prior policy and our learned policy are comparable.
Table 1: Expected posterior utilities w.r.t. different policies
I random policy		prior policy	our policy
linear case	0.1593	0.1922	0.2171
non-linear case	0.1609	0.1936	0.2156
We randomly sample 1000 users, calculate the mean of expected posterior utilities for each policy
and present the results in Table 1. As shown in Table 1, the policy derived from the proposed
framework achieves the highest expected posterior utility compared to the random and the heuristic
prior policies in both linear and non-linear cases, which demonstrates the superior of the proposed
framework.
4.4	How Does the Platform Achieve Its Commercial Benefits
In this section, we intent to adopt the proposed framework to reveal how the platform achieves its
commercial benefits taking both the user preference and the user uncertainty over different item
dimensions into account. In the experiments, the utility function is realized by applying the sigmoid
2Similar to the derivation of posterior utility, we can derive that i) for linear case, Uu(i) = WTμu,i; ii) for
non-linear case, Uu (i) = RRn f (wu , x)pu,i (x) dx.
8
Under review as a conference paper at ICLR 2021
influence of user preference
-1.0 -0.5	0.0	0.5	1.0	1.5
the mean of user preference of dimension 0
0.250
0.249
R 0.248
1 0.247-
to
ω 0.246
0.245
0.244
the mean of item variance of dimension 0
InflUenCe Of Item VananCe
Pearson's r: 0.9931
Figure 3: Influence of user preference and item variance
activation function over the inner product of item representation and user representation (i.e., wu).
Each element of wu can be explained as the user preference over the corresponding item dimension.
For a randomly sampled user, we denote the mean vector of the user preference vectors over items as
W and denote its first element as Wo, whose value indicates the user's preference of item dimension
0. We vary the value of Wo and record the learned expected value (w.r.t. the learned policy) of the
elements with dimension 0 of the mean vectors, which is denoted as v0 . As shown in the left part
of Figure 3, with the increase of Wo, the value of vo increases approximately linearly (noting that
the Pearson correlation coefficient is 0.9995 which indicates strong linear correlation). According
to its definition, vo can reflect to what extent the policy has adjusted to fit item dimension 0 (more
details about how the policy adjust to different levels of user preference or uncertainty are provided
in Appendix A.5). Thus, the result suggests that the policy would tend to adjust more for dimensions
with higher value. In other words, the platform can achieve more commercial benefits from the item
dimension with stronger user preference. We also vary the mean of item variance of dimension
0, which indicates the user uncertainty over item dimension 0, and record vo as presented in the
right part of Figure 3. By similar analysis, the result indicates that the platform can achieve more
commercial benefits from the item dimension that the user is with higher uncertainty.
Therefore, the results in Figure 3 indicate that commercial benefits mainly come from the item di-
mensions with either strong user preference or high user uncertainty when we take the impact of
display policy on user into consideration. Note that this result may contribute to better understand
how commercial benefits are achieved and better conduct commercial recommendation. For in-
stance, from the perspective of the platform, one effective way to improve its commercial benefits
is to better balance the user experience and its commercial actions, i.e., for item dimensions where
the user is with high preference, the platform should cater to his preference, while for flexible item
dimensions where the user is with high uncertainty, the platform may achieve more benefits by
recommending items with high platform utilities.
5	Conclusion
In the paper, we propose a novel recommendation framework and take into account the user uncer-
tainty over different item dimensions and the influence of display policy over user, both of which
could highly affect the commercial benefits of the platform. We derive the calculation formula of
the posterior utility in the case with linear utility function and provide a sampling-based method to
estimate the posterior utility in the case with non-linear utility function. Based on these works, we
formulate the problem of deriving optimal policy for maximizing the expected posterior utility of the
platform as a constrained non-convex optimization problem and further propose an ADMM-based
solution to derive an approximately optimal policy. To demonstrate the effectiveness of the pro-
posed technical solutions absorbed into our framework, extensive experiments are conducted over
data collected from a real-world recommendation scenario. Furthermore, we also provide some
practical insights about how to achieve commercial benefits for the platform taking both the user
preference and the user uncertainty over different item dimensions into consideration, which might
contribute to better understand and conduct commercial recommendation.
References
Himan Abdollahpouri and Masoud Mansoury. Multi-sided exposure bias in recommendation. arXiv
preprint arXiv:2006.15772, 2020.
9
Under review as a conference paper at ICLR 2021
John Barnard, Robert McCulloch, and Xiao-Li Meng. Modeling covariance matrices in terms of
standard deviations and correlations, with application to shrinkage. Statistica Sinica, pp. 1281-
1311, 2000.
Stephen Boyd, Neal Parikh, and Eric Chu. Distributed optimization and statistical learning via the
alternating direction method of multipliers. Now Publishers Inc, 2011.
Haokun Chen, Xinyi Dai, Han Cai, Weinan Zhang, Xuejian Wang, Ruiming Tang, Yuzhou Zhang,
and Yong Yu. Large-scale interactive recommendation with tree-structured policy gradient. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pp. 3312-3320, 2019.
Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recommendations.
In Proceedings of the 10th ACM conference on recommender systems, pp. 191-198, 2016.
Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy Lillicrap,
Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and Ben Coppin. Deep rein-
forcement learning in large discrete action spaces. arXiv preprint arXiv:1512.07679, 2015.
Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. Deepfm: a factorization-
machine based neural network for ctr prediction. arXiv preprint arXiv:1703.04247, 2017.
Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural col-
laborative filtering. In Proceedings of the 26th international conference on world wide web, pp.
173-182, 2017.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv
preprint arXiv:1503.02531, 2015.
Nicole Immorlica, Jieming Mao, Aleksandrs Slivkins, and Zhiwei Steven Wu. Bayesian exploration
with heterogeneous agents. In The World Wide Web Conference, pp. 751-761, 2019.
Emir Kamenica and Matthew Gentzkow. Bayesian persuasion. American Economic Review, 101
(6):2590-2615, 2011.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Cong Leng, Hao Li, Shenghuo Zhu, and Rong Jin. Extremely low bit neural network: Squeeze the
last bit out with admm. arXiv preprint arXiv:1707.09870, 2017.
Chao Li, Zhiyuan Liu, Mengmeng Wu, Yuchi Xu, Huan Zhao, Pipei Huang, Guoliang Kang, Qiwei
Chen, Wei Li, and Dik Lun Lee. Multi-interest network with dynamic routing for recommen-
dation at tmall. In Proceedings of the 28th ACM International Conference on Information and
Knowledge Management, pp. 2615-2623, 2019.
Yishay Mansour, Aleksandrs Slivkins, Vasilis Syrgkanis, and Zhiwei Steven Wu. Bayesian explo-
ration: Incentivizing exploration in bayesian games. arXiv preprint arXiv:1602.07570, 2016.
Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu, and Kun Gai. Practice on long sequential user
behavior modeling for click-through rate prediction. In Proceedings of the 25th ACM SIGKDD
International Conference on Knowledge Discovery & Data Mining, pp. 2671-2679, 2019.
Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. Product-based
neural networks for user response prediction. In 2016 IEEE 16th International Conference on
Data Mining (ICDM), pp. 1149-1154. IEEE, 2016.
Kan Ren, Jiarui Qin, Yuchen Fang, Weinan Zhang, Lei Zheng, Weijie Bian, Guorui Zhou, Jian Xu,
Yong Yu, Xiaoqiang Zhu, et al. Lifelong sequential modeling with personalized memorization
for user response prediction. In Proceedings of the 42nd International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 565-574, 2019.
Gary R Waissi and Donald F Rossin. A sigmoid approximation of the standard normal integral.
Applied Mathematics and Computation, 77(1):91-95, 1996.
Elizabeth Lai Sum Wong. Active-set methods for quadratic programming. PhD thesis, UC San
Diego, 2011.
10
Under review as a conference paper at ICLR 2021
Chen Xu, Quan Li, Junfeng Ge, Jinyang Gao, Xiaoyong Yang, Changhua Pei, Fei Sun, Jian Wu,
Hanxiao Sun, and Wenwu Ou. Privileged features distillation at taobao recommendations. arXiv
preprint arXiv:1907.05171, 2019.
Xiangyu Zhao, Long Xia, Liang Zhang, Zhuoye Ding, Dawei Yin, and Jiliang Tang. Deep rein-
forcement learning for page-wise recommendations. In Proceedings of the 12th ACM Conference
on Recommender Systems,pp. 95-103, 2018.
Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin,
Han Li, and Kun Gai. Deep interest network for click-through rate prediction. In Proceedings of
the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp.
1059-1068, 2018.
Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu, and Kun Gai.
Deep interest evolution network for click-through rate prediction. In Proceedings of the AAAI
conference on artificial intelligence, volume 33, pp. 5941-5948, 2019.
A Appendix
A.1 Proof of Lemma 1
To prove Lemma 1, we first prove the following two propositions.
Proposition 1. R—∞∞ Φ(a0x + b0)N(x∣μo, σ2) dx
Φ( J；++；2)，∀a0,b0,μo,σ0 ∈ R.
Proof. By introducing two independent variables X and Y which satisfy X ~ N(μo,σ0) and
Y 〜N(0,1), we have
Φ Φ(a0x + b0)N(x∣μo,σ0) dx = EX~n(μ0,σ0)(Φ(a0X + Hy)
-—∞
(25)
=P(Y < a0X + b0)
=P(Y-a0X < b0)
p( Y — a0X + a0μo	b0 + a0μ0 )
p1 + a02σ0	/I + a02σ2
=φ( a0μo + b0 )
/I + a02σ2
(26)
(27)
(28)
(29)
Note that Equation 26 is gained by the definition of function Φ, i.e., Φ(z) is the probability
of sampling from the standard normal distribution with the sampled value less than z. Since
X 〜N(μo,σo) and Y 〜N(0,1), We have
Y—a0X+a0μo
λ∕1+a02 σ2
~ N(0,1). Thus, Equation 29 can
be straightforwardly derived by definition of Φ, which concludes the proof.
□
Proposition 2. Denoting the error function by erf which satisfies erf (z) = √∏ Rz e—12 dt, we
-x2
have: f ∞ Xe"ɪ erf (a0x + b0) dx =
b02
2a02+1, ∀a0, E ∈ R.
2a0
H e
2

Proof. For case with a0 = 0, we have:
∕∞	-x2	0
xe 2 erf (a0x + b0) dx = xe
-χ2	,,,，、、.尸 T	-,，，、，
2 erf (b ) dx + Xe 2 erf (b ) dx
0
-Z∞xe
0
0
-x2	/八	∞ -x2	/八
2 erf (b ) dx +	xe 2 erf (b ) dx
0
2a0
/ e
Ja02 + 2
b02
2a02+1
(30)
(31)
(32)
(33)

11
Under review as a conference paper at ICLR 2021
For case with a0 6= 0, we have:
Z∞	2	2	∞	∞	2 2a0
XeFerf (a0x + b0) dx = -eF erf(a0x + b0)l	-	-eF -=e~^a x+b ) dx
∞	-∞	-∞	π
	(34)
Z	2a e-(a02+1 )x2-2a0b0x-b02 dχ	(35)
∞∞ 2±e-(a02+2)(x+ a020⅛)2+≡-b02 dx -∞ π	(36)
∞ °z√ a02b02 √2	C	C /	√e - -b e-(a02+1 )z2 dz	(37)
2a0 篝1 -b02 √πe +2	#+1	(38)
2α!	-	b02	
—	e 2a02+1	(39)
a02 + 2
Note that Equation 34 is derived by applying Newton-Leibniz formula.
∞ -x2
Thus, combining the above two cases, We have: f∞ Xe -ɪ erf (ax +
2a0
b02
亍e 2a02+ι, ∀a0, b0 ∈ R.
2
b0) dx
□
With Proposition 1 and Proposition 2, now we prove Lemma 1 as follows.
Proof of Lemma 1. By definition, Φ(z)
1+ef( √)
. Thus, we have:
2
Z	(c0x + d0)Φ(a0x + b0)N(x|0, 1) dx
-∞
(40)
Z+∞	1
∞ 2√2π(C'x)(1 + erf(
a0x + b0
a0c0
-，	---e
，2n(a02 + 1)
b02
2a02 + 2 + d0φ(
√2
b0
χ x _ X2
))e 丁
Z+∞
d0Φ(a0x + b0)N(x|0, 1) dx	(41)
√Γ+^002
(42)

)
The last equation is derived by utilizing Proposition 1 and Proposition 2, which completes the proof.
□
A.2 Proof of Corollary 1
In order to prove Corollary 1, we first prove the following two propositions.
Proposition 3. If Σu,i is positive definite, then JRn h(x)N(x∣μu,i, ∑u,i) dx = JRn h(A-,1 Z +
μu,i)N(z|0, I) dz, where Au,i is the upper triangular matrix gained by Cholesky decomposition
over Σu,i, i.e., Σu,i = Au,iAu,i
Proof. If Σu,i is positive definite, Σu-,1i is also positive definite. Then we can conduct Cholesky
decomposition:
Σu-,1i = ATu,iAu,i ,
where Au,i is an upper triangular matrix with real and positive diagonal entries.
Thus, by adopting integration by substitution, we have:
/ h(x)N(x∣μu,i, ∑u,i) dx
Rn
(43)
(44)
12
Under review as a conference paper at ICLR 2021
h(x)
Rn
1
P(2∏)nl∑u,i∣
e- 1 (x-μu,i)T ςu,⅛ (χ-μu,i) dχ
h h(A-iAu,i(x — μu,i) + μu,i)	J—— e-2(Au,i(X-“u,i))TAu,i(X-“u,i) |4“,小 dχ
Rn	(2π)n
…Z+μu,i) p⅛e-1 ZTz
dz
h(Au-,1iz
Rn
+ μu,i )N(z∣0, I) dz
(45)
(46)
(47)
(48)
Thus, if Σu,i is positive definite and Au,i is the upper triangular matrix gained by Cholesky
decomposition over Σ-,i, then We have RRn h(x)N(x∣μ%i, ∑u,i) dx = RRn h(A-,iZ +
μu,i)N(z|0, I) dz, which concludes the proof.	口
Proposition 4. RRn Φ(vTX + a0)N(x|0, I) dx = Φ( / a0	), ∀v ∈ Rn and a0 ∈ R.
R	1+vT v
Proof. Denoting Vi = a0 + Pn=i+ι Vj Xj, we have:
/
Rn
x + a0)N(x|0, I) dx
Z+∞
∞
Z+∞	n
Φ(a0 + y? VjXj)N(xi|0, 1) dxι …N(Xn|0,1) dXn
Z+∞
∞
Z+∞
Φ(v1X1 + VI)N(Xl|0, 1) dxι …N(Xn|0,1) dXn
∞
Z+∞
∞
Z+∞
∞
Z +∞Φ(
-∞
Z +∞Φ(
-∞
Z+∞
∞
Z +∞Φ(
-∞
Z+∞
∞
Z +∞Φ(
-∞
/ v1 2 )N(X2∣0, 1) dX2 …N(Xn|0, 1) dXn
1 + V12
v2：2 + v22 )N(x2∣0, 1) dX2 …N(Xn|0, 1) dXn
1 + V12
V
/"+%2 : )N(x3∣0, 1) dX3 …N(Xn |0, 1) dXn
√1+^⅛
V2 、一 —、一 一 「、一
/	2	=^)N (x3∣0, 1) dX3 …N (Xn|0, 1) dXn
√1+ v2 + V2
=Φ( /	：)
√1 + VT V
(49)
(50)
(51)
(52)
(53)
(54)
(55)
(56)
Equation 52 and Equation 54 are derived from Lemma 1 (or Proposition 1). Similarly, Equation 56
is achieved by applying Lemma 1 (or Proposition 1) iteratively, which completes the proof. 口
With Proposition 3 and Proposition 4, now we can prove Corollary 1 as follows.
Proof of Corollary 1. If Σu,i is positive definite, Σu-,1i is also positive definite. Then we can conduct
Cholesky decomposition: Σu-,1i = ATu,iAu,i, where Au,i is an upper triangular matrix with real and
positive diagonal entries. Thus, we have:
pu,i(display; π) =	pu,i(display|X; π)pu,i(X) dX
Rn
=/ Φ(a(VTx) + b)N(X∣μu,i, ∑u,i) dX
Rn
=/ Φ(。端A-iZ + aVTμu,i +b)N(z|0,1)dz
Rn
(57)
(58)
(59)
13
Under review as a conference paper at ICLR 2021
φ( ,avU μ%i + b )
(P1 + a2vTΣu*u J
(60)
Equation 59 and Equation 60 are derived from Proposition 3 and Proposition 4 respectively, which
concludes the proof.	□
A.3 Proposition Adopted for proving Corollary 2
Proposition 5. RRn (uTx + b0)Φ(vT x + a0)N(x|0, I) dx
b0Φ( / a0	), ∀u, V ∈ Rn and a0, b0 ∈ R.
“√vT v+1'	,	,
_.t_.	------a_____
_____V U_______e 2(vTv+i)	+
√2π(vτ v+1)
Proof. Denoting Ui = b0 + En=分+1Uj Xj and Vi = a0 + En=分+1 Vj Xj, We have:
/
Rn
(UTX + b0)Φ(VT X + a0)N(X|0, I) dX
(61)
Z+∞
∞
Z+∞	n	n
(b0+	UjXj)Φ(a0+
∞	j=1	j=1
VjXj)N(xi|0,1) dxι …N(xn|0,1) dxn
Z+∞
∞
Z+∞
∞
(u1x1 + U 1)Φ(v1x1 + V1)N(x1∣0,1) dx1 …N(xn∣0,1) dxn
(62)
(63)
Z+∞
∞
Z +∞(
-∞
v2
v1u1	e-2(V2⅛)
√2π(v2 + 1)
+ UlΦ( P［若))N(X2 |0, 1) dX2 …N(Xn|0, 1) dXn
1	(64)
Z+∞
∞
Z +∞(
-∞
V1 U1
-，	—e
√2π(v2 + 1)
(V2x2+v2)2
2(v2 + 1)
+ (u2X2 + U2)Φ(v2x2 + v2 ))N(x2∣0,1) dx2
1 + V12
…N(Xn|0, 1) dXn
(65)
Z+∞	Z +∞
∞	-∞
V1U1
(v3x3+v3)2
e 2(v2+v2 + 1)
V2U2
_ (v3x3+v3)2
e 2(v2+v2+1) +
+
(U3X3 + U3)Φ( v3x3 + v3= ))N(X3∣0, 1) dX3 …N(Xn |0, 1) dXn
√1+ V2 + V2
(66)
Z+∞	Z +∞
∞	-∞
V1U1 + V2U2
P2π(v2 + v2 + 1)
_ (V3x3+V3)2
e 2(v2+v2+1) + (u3χ3 + U3)Φ(
V3X3 + 心3
p1+ v2 + v2
N (X3∣0, 1) dx3 …N (Xn|0, 1) dXn
VT U	— e- 2(vT0V+i) + b0Φ(	aO )
p2π(vτ V + 1)	√vt v + 1
(67)
(68)
Equation 64 is derived from Lemma 1. Applying Lemma 1 and conducting definite integration over
the exponential function, we can derive Equation 66. Similar calculation step can be conducted
iteratively to derive Equation 68, which completes the proof.	□
A.4 Additional Experiments to Verify the Effectiveness of the Importance
Sampling Based Posterior Utility Estimation
Table 2: Statistics of posterior utilities w.r.t. samples per estimation
I 	L	exact value	I	samples per estimation			
		I 1	10 I	100	1000
RMSE of posterior utilities	∣	0.0000	^∣~0.0369~	0.0125 I	0.0038	0.0011-
mean of expected posterior utilities ∣	0.2681	^∣~0.2660~	0.2677~∣^	0.2681	0.2681
variance of expected posterior utilities ∣	3.4917e-9	I 3.5331e-5	3.0590e-6 ∣	2.9973e-7	4.3271e-8
14
Under review as a conference paper at ICLR 2021
As described in Section 3.2, importance sampling technique can be adopted to estimate the value of
posterior utility. However, it is apparent that the number of samples for each posterior utility estima-
tion would highly affect the results. Extra experiments are conducted to verify the effectiveness of
the importance sampling based posterior utility estimation as shown in Table 2. When we vary the
number of samples per estimation from 1 to 1000, the Root Mean Square Error (RMSE) between
the estimated values and the exact values of the posterior utilities decreases from 0.0369 to 0.0011,
which indicates that larger number of samples per estimation leads to more accurate approximation.
We also utilize the proposed ADMM-based solution to maximize the expected posterior utility and
record the mean and variance of the expected posterior utilities of the last 50 optimization iterations
for each sampling setting. For the case with the number of samples per estimation set to 100 (or
1000), the variance of the expected posterior utilities is very small while the learned final value of
expected posterior utility is around 0.2681, which is similar to the result of optimization with exact
value of posterior utility and thus verifies the effectiveness of the proposed importance sampling
based posterior utility estimation.
A.5 Additional Experiments to Illustrate How the Policy Adjust w.r.t.
Different Levels of User Preference or Uncertainty
71	72	，3	，4	，5	，6 h ，8	，9	710 ʃll ∣12	'13	/14	715	，16
Wq = —1.0
W0= - 0.5 -
W0 = 0.0
Wq = 0.5 -
W0 = 1.0
IV0 = 1.5-
0.0116
0.0113
0.0111
0.0108
0.0106
0.0104
0.0207
0.0203
0.0198
0.0192
0.0186
0.0182
0.0163
0.0158
0.0154
0.0150
0.0146
0.0143
0.0208
0.0203
0.0197
0.0192
0.0186
0.0182
0.0186
0.0182
0.0178
0.0176
0.0173
0.0169
0.0187
0.0185
0.0182
0.0181
0.0178
0.0175
0.0093
0.0095
0.0098
0.0099
0.0102
0.0102
0.0101
0.0104
0.0106
0.0107
0.0108
0.0110
0.0155
0.0051
0.0135
0.0053
0.0069
0.0163
0.0158
0.0159
0.0160
0.0164
0.0166
0.0053
0.0138
0.0055
0.0072
0.0170
0.0056
0.0057
0.0059
0.0061
0.0141
0.0144
0.0146
0.0148
0.0058
0.0075
0.0175
0.0060 0.0077
0.0062 0.0080
0.0065 0.0082
0.0185
0.0192
0.0203
10.020
0.018
0.016
0.014
0.012
0.010
0.008
0.006
recommenda-on Probab 三 ty
Figure 4: Learned policy w.r.t. different levels of user preference
0.005
0.010
0.015
0.020
0.025
i,ι i,2	i,3
“4	15	-6	"7 f,B 19	-10	"11	”12	-13	『14	-15	-16
Figure 5: Learned policy w.r.t. different levels of user uncertainty
0.018
不
0.016 O
0.014 I
&
0.012 o
n
P
0.010 °
a
σ
0.008]
We also provide analysis about how different levels of user preference would affect the final learned
policy over items. For a randomly sampled user, We vary W0 from -1.0 to 1.5 and keep the values
of other dimensions unchanged. We also randomly pick 16 items from candidate item set, sort these
items according to the value of dimension 0 of the item mean vector and denote the sorted items
as iι, i2,∙∙∙,ii6. For each case (with specific value for W0), we record the final learned policy
(recommendation probabilities) over the picked items and present the heat map in Figure 4 Where
darker color represents larger recommendation probability. As shown in Figure 4, with the increase
of the value of W0, the learned recommendation probabilities of items with large value of dimension
0 (e.g., i15 and i16) increase while those with small value decrease. Thus, to maximize the platform’s
commercial benefits, the platform would tend to recommend items that match user’s preference over
item dimension with high user preference. Similar results can be derived when we analyse how the
learned policy adjust to different levels of user uncertainty over item dimensions. Thus, the proposed
framework reveals that the policy would adjust more for item dimensions with higher preference or
uncertainty so as to maximize the commercial benefits of the platform.
15