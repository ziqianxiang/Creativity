Under review as a conference paper at ICLR 2020
Revisiting the stability of stochastic gradi-
ent descent: a tightness analysis
Anonymous authors
Paper under double-blind review
Ab stract
The technique of algorithmic stability has been used to capture the generalization
power of several learning models, especially those trained with stochastic gradient
descent (SGD). This paper investigates the tightness of the algorithmic stability
bounds for SGD given by Hardt et al. (2016). We show that the analysis of Hardt
et al. (2016) is tight for convex objective functions, but loose for non-convex
objective functions. In the non-convex case we provide a tighter upper bound
on the stability (and hence generalization error), and provide evidence that it is
asymptotically tight up to a constant factor.
However, deep neural networks trained with SGD exhibit much better stability and
generalization in practice than what is suggested by these (tight) bounds, namely,
linear or exponential degradation with time for SGD with constant step size. We
aim towards characterizing deep learning loss functions with good generalization
guarantees, despite training using SGD with constant step size.
In this vein, we propose the notion of a Hessian Contractive (HC) region, which
quantifies the contractivity of regions containing local minima in the neural network
loss landscape. We provide empirical evidence that several loss functions exhibit
HC characteristics, and provide theoretical evidence that the known tight SGD
stability bounds for convex and non-convex loss functions can be circumvented
by HC loss functions, thus partially explaining the generalization of deep neural
networks.
1	Introduction
Stochastic gradient descent (SGD) has gained great popularity in solving machine learning optimiza-
tion problems (Kingma & Ba, 2014; Johnson & Zhang, 2013). SGD leverages the finite-sum structure
of the objective function, avoids the expensive computation of exact gradients, and thus provides a
feasible and efficient optimization solution in large-scale settings (Bottou, 2012). The convergence
and the optimality of SGD have been thoroughly studied (Ge et al., 2015; Rakhlin et al., 2012; Reddi
et al., 2018; Zhou & Gu, 2019; Carmon et al., 2019a;b; Shamir & Zhang, 2013) .
In recent years, new research questions have been raised regarding SGD’s impact on a model’s
generalization power. The seminal work (Hardt et al., 2016) tackled the problem using the algorithmic
stability of SGD, i.e., the progressive sensitivity of the trained model w.r.t. the replacement of a single
(test) datum in the training set. The stability-based analysis of the generalization gap allows one
to bypass classical model capacity theorems (Vapnik, 1998; Koltchinskii & Panchenko, 2000) or
weight-based complexity theorems (Neyshabur et al., 2017; Bartlett et al., 2017; Arora et al., 2018).
This framework also provides theoretical insights into many phenomena observed in practice, e.g., the
“train faster, generalize better” phenomenon, the power of regularization techniques such as weight
decay (Krogh & Hertz, 1992), Dropout (Srivastava et al., 2014), and gradient clipping. Other works
have applied the stability analysis to more sophisticated settings such as Stochastic Gradient Langevin
Dynamics and momentum SGD (Mou et al., 2018; Chaudhari et al., 2019; Chen et al., 2018).
Despite the promises of this stability-based analysis, it remains open whether this framework can
explain the strong generalization performance of deep neural networks in practice. Existing theoretical
upper bounds of the stability (and thus, generalization) (Hardt et al., 2016) are ideal for strongly
convex loss functions: the upper bound remains constant even as the number of training iterations
increases. However, the same bound deteriorates significantly when we relax to more general and
realistic settings. In particular, for convex (but not strongly convex) and non-convex loss functions, if
1
Under review as a conference paper at ICLR 2020
SGD has constant step size, then the upper bound grows linearly and exponentially with the number
of training iterations. This bound fails to match the superior generalization performance of deep
neural networks, and leads to the following question:
Question 1: Can we find a better stability upper bound for convex or non-convex loss functions?
In this paper, we first address the question above and investigate the tightness of the algorithmic
stability analysis for stochastic gradient methods (SGM) proposed by (Hardt et al., 2016).
R1. We show in Theorem 1 that the analysis in (Hardt et al., 2016) is tight for convex and smooth
objective functions; in other words, there is a convex loss function whose stability grows linearly
with the number of training iterations, with constant step size (αt = α) in SGD.
R2. We show that in Theorem 2 that for linear models, the analysis in the convex case can be
tightened to show that stab does not increase with t.
R3. In Theorem 3 we show that the analysis in (Hardt et al., 2016) for decreasing step size (αt =
O(1/t)) is loose for non-convex objective functions by providing a tighter upper bound on the stability
(and hence generalization error).
R4. The bound on the stability of SGD by (Hardt et al., 2016) is achieved by bounding the divergence
at time t, defined as δt := E||wt - wt0 ||, where wt is the model trained on data set S and wt0 is the
model trained on a data set S0 that differs from S in exactly one sample. In Theorem 4 we provide
evidence that our new upper bound in the non-convex case is tight, by showing a non-convex loss
function whose divergence matches the upper bound for our divergence.
R5. Although it is not derived formally, the techniques in (Hardt et al., 2016) can be employed
to show an exponential upper bound for non-convex loss functions minimized using SGD with
constant-size step. In Theorem 5, we give evidence that this abysmal upper bound is likely tight for
non-convex loss functions, by exhibiting a non-convex loss function for which the divergence δt
increases exponentially.
Thus the only functions whose stability provably does not increase with the number of iterations
when a constant step-size during SGD is employed, are strongly convex functions. However, a) it
has been empirically observed that for deep neural network loss, near the local minima, the Hessians
are usually low rank (Chaudhari et al., 2017; Yao et al., 2019), and b) neural networks trained with
constant step-size SGD do generalize well in practice (Lin & Jegelka, 2018; Huang et al., 2017;
Smith et al., 2017). Combined with our lower bounds on convex and non-convex functions, we seem
to hit an obstacle on the way to explaining generalization using the stability framework.
Question 2: What is it that makes constant-step SGD on deep learning loss function generalize well?
Realizing the limitation of the current state of stability analysis, we investigate whether a stronger-
than-convex, but weaker-than-strongly-convex assumption of the loss function can be made, at least
near local minima. If we can show algorithmic stability near local minima, we can still show the
stability using similar argument as (Du et al., 2019; Allen-Zhu et al., 2019).
Aiming towards a characterization of loss functions exhibiting good stability, we propose a new
condition for loss near local minima. This condition, called Hessian contractive, is slightly stronger
than a general convex condition, but considerably weaker than strongly convex. Formally, the Hessian
contractive condition stipulates that near any local minima, (1) the function is convex; and (2) a data
dependent Hessian is positive definite in the gradient direction. Theoretically, we show that such a
condition is sufficient to guarantee a constant stability bound for SGD (constant step size) near the
local minima, while allowing the Hessian to be low rank. We also provide examples showing Hessian
Contractive is a reasonable condition for several loss functions. Empirically, we verify the Hessian
Contractive condition near a local minima of the loss while training deep neural networks. We sample
points from a neighborhood of current iterates by adding Gaussian noise and verify the HC condition
locally by Hessian product approximation. Summarizing our second set of contributions:
R6. In Observation 1 we show that the family of widely used (convex) linear model loss functions
will satisfy the Hessian Contractive condition. One typical example of such linear model loss is the
regression loss function. These observation suggests that Hessian Contractive is a condition satisfied
by (potentially many) machine learning loss functions.
2
Under review as a conference paper at ICLR 2020
R7. In Theorem 6 we show that the Hessian Contractive condition will localize SGD iterates in a
neighborhood of minima, which implies a constant stability bound for SGD near the local minima.
Table 1: Current landscape of stability bounds. [H] indicates results in (Hardt et al., 2016), and *
indicates results in this paper. Bounds without [H] or * are trivial. β is the smoothness parameter.
SGD SteP Size		COnStant at = a∕β			αt = a/ (βt		COnStant at	
Loss function	Strongly Con- vex	Convex	Non-Convex	Hessian Contrac- tive
Upper Bound	。⑴[H]	O(aT∕n) [H]	O (Tι+a ∕n) [H] O (Ta∕n1+a)*	Theorem 6
Lower Bound	Ω(1)	Ω(aT∕n)*	Open, evidence*	Ω(1)	
1.1	Related Works
Stability and generalization. The stability framework suggests that a stable machine learning al-
gorithm results in models with good generalization performance (Kearns & Ron, 1999; Bousquet
& Elisseeff, 2002; Elisseeff et al., 2005; Shalev-Shwartz et al., 2010; Devroye & Wagner, 1979a;b;
Rogers & Wagner, 1978). It serves as a mechanism for provable learnability when uniform conver-
gence fails (Shalev-Shwartz et al., 2010; Nagarajan & Kolter, 2019). The concept of uniform stability
was introduced in order to derive high probability bounds on the generalization error (Bousquet &
Elisseeff, 2002). Uniform stability describes the worst case change in the loss of a model trained on
an algorithm when a single data point in the dataset is replaced. In (Hardt et al., 2016), a uniform
stability analysis for iterative algorithms is proposed to analyze SGD, generalizing the one-shot
version in (Bousquet & Elisseeff, 2002). Algorithmic uniform stability is widely used in analyzing the
generalization performance of SGD (Mou et al., 2018; Feldman & Vondrak, 2019; Chen et al., 2018).
The worst case leave-one-out type bounds also closely connect uniform stability with differential
private learning (Feldman et al., 2018; 2020; Dwork et al., 2006; Wu et al., 2017b), where the uniform
stability can lead to provable privacy guarantee. While the upper bounds of algorithmic stability
of SGD have been extensively studied, the tightness of those bounds remains open. In addition to
uniform stability, an average stability of the SGD is studied in Kuzborskij & Lampert (2018) where
the authors provide data-dependent upper bounds on stability1. In this work, we report for the first
time lower bounds on the uniform stability of SGD. Our tightness analysis suggests necessity of
additional assumptions for analyzing the generalization of SGD on deep learning.
Geometry of local minima. The geometry of local minima plays an important role in the generaliza-
tion performance of deep neural networks (Hochreiter & Schmidhuber, 1995; Wu et al., 2017a). The
flat minima, i.e., minima whose Hessians have a large portion of zero-valued eigenvalues, are believed
to attain better generalization (Keskar et al., 2016; Li et al., 2018). In (Chaudhari et al., 2019), the
authors construct a local entropy-based objective function which converges to a solution with good
generalization in a flat region, where “flatness” means that the Hessian matrix has a large portion of
nearly-zero eigenvalues. However, these observations have not been supported theoretically. In this
paper, we propose the Hessian contractive condition that is slight stronger than flat minima. Such
condition suggests that the minima is sharp only in the gradient direction while remains flat in other
directions, which unifies the geometrical interpretation of flat minima and uniform stability analysis.
2	Preliminaries
In this section we introduce the notion of uniform stability and establish our notations. We first
introduce the quantities Empirical and Population Risk and Generalization Gap. Given an unknown
distribution D on labeled sample space Z = X × {-1, +1}, let S = {z1, ..., zn} denote a set of
n samples zi = (xi, yi) drawn i.i.d. from D. Let w ∈ Rd be the parameter(s) of a model that
tries to predict y given x, and let f be a loss function where f(w; z) denotes the loss of the model
with parameter W on sample z. Let f (w; S) denote the empirical risk f (w; S) = Ez〜S[f (w; z)]=
1While it is an interesting open problem to get data-dependent lower bounds by lower bounding the average
stability, we construct lower bounds on the worst-case stability. Thus our lower bounds are general and not
data-dependent.
3
Under review as a conference paper at ICLR 2020
n1 ∑n=ι f (w； Zi) with corresponding population risk Ez〜D [f (w; z)]. The generalization error of
the model with parameter w is defined as the difference between the empirical and population risks:
|Ez〜D[f(w; z)] - Ez〜S[f(w; z)]|.
Next we introduce the Stochastic Gradient Descent (SGD) method. We follow the setting of (Hardt
et al., 2016), and starting with some initialization w0 ∈ Rd, consider the following SGD update step:
wt+1 = Wt - αtVw f(w; Zit),
where it is drawn from [n] := {1, 2, ∙∙∙ , n} uniformly and independently in each round. The analysis
of SGD requires the following crucial properties of the loss function f(., z) at any fixed point z,
viewed solely as a function of the parameter w:
Definition 1 (L-Lipschitz). A function f(w) is L-Lipschitz if ∀u, v ∈ Rd: |f (u) -f(v)| ≤ Lku - vk.
Definition 2 (β-smooth). A function f(w) is β-smooth if ∀u, v ∈ Rd: |Vf (u) -Vf(v)| ≤ βku - vk.
Definition 3 (γ-strongly convex). A function f(w) is γ strongly convex if ∀u, v ∈ Rd:
f(u) > f(v) + Vf (v)>[u - v] + 2 ku - vk2.
Algorithmic Stability Next we define the key concept of algorithmic stability, which was intro-
duced by (Bousquet & Elisseeff, 2002) and adopted by (Hardt et al., 2016). Informally, an algorithm
is stable if its output only varies slightly when we change a single sample in the input dataset. When
this stability is uniform over all datasets differing at a single point, this leads to an upper bound on
the generalization gap. More formally:
Definition 4. Two sets of samples S, S0 are twin datasets if they differ at a single entry, i.e., S =
{z1, ...zi, ..., zn } and S0 = {z1, ..., zi0, ..., zn }.
Consider a possibly randomized algorithm A that given a sample S of size n outputs a parameter
A(S). Define the algorithmic stability parameter of A by:
εstab(A,n) := inf{ε : sup EA |f(A(S); z) - f(A(S0);z)| ≤ ε}.
z∈Z,S,S0
Here EA denote expectation over the random coins of A. Also, for such an algorithm, one can define
its expected generalization error as:
GE(A,n) := Es,A[Ez 〜D [f (A(S); z)] - Ez 〜S [f (A(S); z)]].
Stability and generalization: It was proved in (Hardt et al., 2016) that GE(A, n) ≤ εstab(A, n).
Furthermore, the authors observed that an L-Lipschitz condition on the loss function f enforces a
uniform upper bound: supz |f(w; z) - f (w0; z)| ≤ Lkw - w0k. This implies that for Lipschitz loss,
the algorithmic stability εstab(A, n) (and hence the generalization error GE(A, n)) can be bounded
by obtaining bounds on kw - w0 k.
Let wT and wT0 be the parameters obtained by running SGD starting on twin datasets S and S0 ,
respectively. Throughout this paper we will focus on the divergence quantity δT := EA||wT - wT0 ||.
While (Hardt et al., 2016) reports upper bounds on δT with different types of loss functions, e.g.,
convex and non-convex loss functions, we investigate the tightness of those bounds.
3	Tightness of existing bounds
In this section we report our main results. We first consider the convex case with constant step
size, where we prove 1) that the existing bounds stating EStab a t are tight, and 2) for linear models,
the analysis can be tightened to show that stab does not increase with t. Then we move on to the
non-convex case, where a) for decreasing step size we improve the existing upper bound, and give
evidence that our new upper bound is tight, and b) for constant step size we give loss functions whose
divergence δt increases exponentially with t.
4
Under review as a conference paper at ICLR 2020
3.1	Convex Case
In this section we analyze the stability of SGD when loss function is convex and smooth. We begin
with a construction which shows that Theorem 3.8 in (Hardt et al., 2016) is tight. Our lower bound
analysis will require the following quadratic function:
f (w; Z) = gw>Aw — yx>w
(1)
where A is a d × d matrix. In the construction of lower bounds, we carefully choose A and S so that
the single data point replaced in twin data set will cause the instability of SGD.
Theorem 1. Let wt, wt0 be the outputs of SGD on twin datasets S, S0 respectively. Let ∆t = wt — wt0
and αt be the step size of SGD. There exists a function f which is convex, β-smooth, and L-Lipschitz
on the domain of w, and twin datasets S, S0 such that:
Ek∆τk ≥ L XT a αt, and £“ab ≥ L XT ,叫.	⑵
3n	t=1	3n	t=1
The convex upper bound in Theorem 3.8 in (Hardt et al., 2016) states that k∆T∣∣ ≤ L Pn= αt, which
implies that the divergence increases throughout training. The lower bound in Theorem 1 suggests
the tightness of the upper bound. However, in practice, such phenomenon is not commonly observed,
i.e., for a family of convex but not-strongly-convex loss functions, the generalization performance
does not deteriorate as the number of training iterations increases. This motivates us to investigate
a weaker condition which still can enforce an O (1) stability, without strong convexity. In the next
theorem, we restrict ourselves to a family of linear model loss functions and show that the divergence
will not increase unboundedly during training.
We shall need the following definition of a ξ-self correlated data set. Essentially, a self-correlated
dataset requires an average linear dependence of each x. Recall that the i’th sample zi = (xi, yi).
Definition 5. A set S = {zι,…，Zn} is ξ-self correlated if ∀j ∈ [n],1 En=ι(x>xi)2 ≥ ξ > 0∙
Assuming that ∀j ∈ [n], ∣Xjk ≥ r for some r > 0, definition 5 implies that S is at least r2-self
correlated. Thus the above condition holds for all datasets S not containing the zero-feature vector. In
our next theorem, we leverage on the ξ-self correlated condition to prove a non-accumulate uniform
stability bound for SGD on a loss function of Linear Models. We characterize a linear model by
rewriting the loss function f (w; Z) in terms of fy (w>χ) where fy (∙) is a scalar function depending
only on the inner product of the model parameter w and the input feature x.
Theorem 2. Suppose a loss function f (w, Z) is oftheform f (w, S) = ɪ En=I fyj (w>Xj), where
fy(w>x) satisfies (1) ∣fy(∙)| ≤ L ,(2) 0 < Y ≤ fy0(∙) ≤ β, (3) ∣x∣ ≤ R and (4) S, S0 are ξ-self
correlated, twin datasets. Let wt and wt0 be the outputs of SGD on S and S0 after t steps, respectively.
Let the divergence ∆t := Wt — Wt and α ≤ ɪ be the step size ofSGD. Then, Ek∆T∣∣ ≤ ξγR.
Remark 1. In (Hardt et al.,2016), an O (Lθ stability bound is derived on a loss function f(w,Zi)
which is StrOngly convex, i.e., V2f (w) * YI. In practice one can incorporate a StrOngly convex
regularizer to impose strong convexity, often resulting in improved generalization performance in
practice (Shalev-Shwartz et al., 2010; Bousquet & Elisseeff, 2002). The ξ-self correlated condition
allows SGD to maintain a uniformly upper-bounded divergence guarantee for a family of widely
used models for arbitrary long training without using strongly convex regularizer. The theorem
suggests that if the dataset S is reasonably simple, e.g., every xi lies in a low dimensional subspace,
the divergence of SGD is comparable with a strongly convex loss function. This analysis suggests
an alternative condition other can strong convexity can empower SGD an O(1) stability which is
data-dependent. This motivates us to go beyond linear model and seek for a generalized condition
in Theorem 2. In section 4.1, we propose the Hessian Contractive condition for more general loss
function driven by the observation on the linear model.
Example: Linear and logistic regression. Linear regression minimizes the quadratic loss on w:
f (w, S) = 2nn Pxχ∈.∈s(χ>w — yj)2. Note that the Hessian of an individual linear regression loss term
is xj xj> which is not strongly convex since it has rank 1. One cannot apply the strongly convex bound,
5
Under review as a conference paper at ICLR 2020
and the bound for convex suggests stability will increase linearly. However, one can rewrite the loss
function as fy(w>χ) where fy(∙) = 1. Hence Thm. 2 can be applied to give a non-accumulative
bound on SGD’s stability. A similar result can be derived for the logistic regression loss.
3.2	Non-Convex Case
In this section, we construct a non-convex loss function to analyze the tightness of the divergence
bound in (Hardt et al., 2016). We first focus on the case of decreasing step size.
Theorem 3.	There exists a function f which is non-convex and β-smooth, twin datasets S, S0 and
constants a s.t. the following holds: if SGD is run using step size ɑt = 0 Q= for 1 ≤ t < T, and
wt, wt0 are the outputs of SGD on S and S0, respectively, and ∆t := wt - wt0, the divergence of SGD
after T rounds (T > n) satisfies: Ek∆τ∣∣ ≥ 3T+a.
Comparison to the bound in Theorem 3.12(Hardt et al., 2016) In (Hardt et al., 2016), an assump-
tion is made on the non-convex loss function, namely that f(w, z) < 1. We remark that our function
f used in proving the lower bound above does not obey this assumption. Thus for very large T, our
lower bound may exceed the upper bound in (Hardt et al., 2016), and in general is incomparable due
to the lack of this assumption.
Next one observes that in the range T 1+a ≤ n, the upper bound in (Hardt et al., 2016), namely
O(Ta /n1+a ) is larger than 1, weakening its importance, especially because of the assumption
f(z, w) < 1, and the fact that when a is small, and one is interested in training faster, smaller values
of T in the above range are important. Our divergence lower bound motivates an investigation into the
possible tightness of the analysis leading to the upper bound in (Hardt et al., 2016). In the following
theorem we prove a tighter upper bound for this range of T: it does not assume f(z, w) < 1, and is
a
non-trivial in the range when T 1+α ≤ n.
Theorem 4.	Assume f is β-smooth and L-lipschitz. Running T (T > n) iterations of SGD on
f (w; S) with SteP size a= = *, the stability ofSGD satisfies: εstab ≤ 2L2T .
Dividing our bound with the one in Theorem 3.12 in Hardt et al. (2016), we obtain the ratio
Ω (Tn+a ). This factor is less than 1 (and so we improve the upper bound) exactly when T 1+a ≤ n.
Note that this is potentially a large range as a is a small and positive constant. We remark that our
tight bound is for permutation SGD. The bound for SGD (see Appendix Theorem 4b) using sampling
without replacement has an additional log(n) factor (which we conjecture can be removed), which
nevertheless is also a polynomial improvement over the known bounds.
4 Deep Learning and Limitations of Bounds
While Theorem 4 improves existing upper bounds on stability by an polynomial factor, it still can not
explain the generalization performance of deep learning model. In particular, the analysis relies on a
decreasing step size while in training deep learning model, constant instead of a decreasing step size
is a common choice. In the next result, we adopt the same construction as in Theorem 3 but with a
constant learning rate to show that, unfortunately, SGD may have an exponential divergence rate for
general β smooth function.
Theorem 5.	Let wt, wt0 be the outputs of SGD on twin datasets S, S0, and let ∆t := wt - wt0.
There exists a function f which is non-convex and β -smooth, twin sets S, S0 and constants a, γ such
that the divergence of SGD after T rounds (T > n) using constant step size α = o_9⅛β satisfies:
E∣∆τ∣ ≥ n⅛eaT/2.
The above theorem implies that the generalization gap may deteriorate at an exponential rate if SGD
is run with a constant step size for a non-convex function. However, in Fig. 1a we observe that
the distance between the weights of two deep models trained on twin datasets (in other words, the
divergence δT stabilizes after ≈ 20 epochs of training. Fig. 1a suggests that the generalization gap for
deep models may remain constant after sufficiently many training iterations. However, the analysis
of (Hardt et al., 2016) suggests that local minima of the loss landscape must be in strongly convex
6
Under review as a conference paper at ICLR 2020
regions in order to achieve this sort of stability. Indeed, strongly convex Hessians have been proven
to be contractive, leading to the stability of SGD (Ge et al., 2015). On the other hand, it is known
that the Hessians of the loss functions of common deep learning architectures have a large fraction
of eigenvalues which are close to zero (Yao et al., 2019; Chaudhari et al., 2017). This phenomenon
requires further investigation to close the gap between theory and practice. Next, we formalize a
condition requiring that local minima of deep learning loss landscapes lie in contractive regions,
which aids in explaining why the generalization gap stabilizes. We then provide empirical evidence
that this property does indeed hold.
4.1	Geometry of Local Minima
In Theorems 1 and 5, we show that in general, divergence (and hence stability) may deteriorate at a
linear or exponential rate with the number of training iterations when the local minima are not in
strongly convex regions. We introduce the following property to aid in understanding the stability of
the training curve of SGD.
Definition 6. [Hessian Contractive] For a given Set S = {zι,…zn}, a local minimum w* is in a
(σ, γ)-Hessian contractive region if ∀w, w0 with max(∣∣w0 — w*k, ∣∣w — w*k) ≤ σ and Z ∈ S,
▽wf (w,z)>Hw0Vwf (w,z) ≥ Y∣∣Vwf (w,z)k2	(3)
where Hw = VWf(W0； S) = 1 En=I VW f(w',Zj). When (σ, Y)-Hessian contractive holdsfor all
σ > 0 and local minima w*, we say f(w; S) is globally γ-Hessian contractive.
Hessian contractivity describes a region where the stochastic gradient will lie in the range of a positive
semi-definite Hessian matrix. Such a condition prevents the iterates of SGD from escaping from a
local minima in the sense that all gradient descent steps will be pulled back to the minima by the
power of Hessian, which mimics the structure of a strongly convex loss function but with a weaker
assumption. To show that this class is nonempty, we observe the (not strongly convex) linear models
loss functions in Theorem 2 satisfy Hessian contractive condition globally.
Observation 1. Suppose loss function f(w; S) conditions in Theorem 2, i.e., f(w, S) =
n Pj=Ifyj(w>Xj) satisfies ⑴ |fy(∙)∣ ≤ L ,⑵ Y ≤ fy0(∙) ≤ β, (3) ∣χ∣ ≤ R and (4) S is
ξ-self correlated, we have f (w; S) is globally R Hessian Contractive.
Dependence on dataset The Hessian Contractive condition states that the gradient of loss function
evaluated on z can be regulated by Hessian evaluated on the whole dataset, which is a generalization
of ξ-self correlated set condition. In the linear model case, the power of Hessian Contractive
condition depends on the the average correlation between data points which indicates the effect of
data complexity in generalization. We postulate that the value of Y in Hessian Contractive condition
can be one complexity measurement of the data, which is believed to affect generalization power
(Kawaguchi et al., 2017).
Next, we leverage on the Hessian contractive condition to show that once SGD hits a stationary
point around w* in a (σ, Y)-Hessian contractive region, it will be localized near the minima. The
localization of SGD implies that the divergence curve will be stable eventually.
Theorem 6. Given a set S and a β-smooth and L-Lipschitz loss funtion f, suppose SGD with a
2
fixed step size a ≤ ^^ reaches point wo which is close to a local minimum w* in a (σ, Y)-Hessian
contractive region: ∣wo — w*∣ ≤ σ with large enough radius so that σ > 12L, then we have ∀T ≥ 1,
∣wT — w* ∣ ≤ σ.
Theorem 6 states that once the iterate w of SGD encounters a point in a (σ, Y)-Hessian contractive
region with large enough radius σ, w will remain in this region. In terms of the stability of SGD,
assuming SGD will bring wt into one of the local minima quickly (Ge et al., 2015; Allen-Zhu et al.,
2019), the divergence ∣wt — wt0 ∣ will be uniformly bounded by the difference between two local
minima w*, w*0 of loss functions f(w; S) and f(w0; S0) plus the σ-radius of the Hessian contractive
region. The above suggests that a weaker condition than strong convexity is sufficient to explain the
observation in Fig. 1a.
4.2	Empirical Support for Hessian Contractivity
In order to explain the stabilization of the generalization gap observed when training deep learning
models via SGD, we hypothesize that local minima of deep loss landscapes found by SGD lie in Hes-
7
Under review as a conference paper at ICLR 2020
(a)	(b)
Figure 1: (a) As in (Hardt et al., 2016), we measure the normalized Euclidean distance between the parameters
of two identical deep models (ResNet18) trained on twin datasets (CIFAR10 with a random image removed).
We plot the mean over 35 twin datasets (variance shown). The distance first increases but stabilizes after ≈ 20
train epochs. The “all” curve is the distance between the full model parameter vectors; convi is the distance
between first convolutional layer weights in the i-th “block” of ResNet18. (b) Approximate normalized Hessian
contractivity. For both neural networks, the “mean” contractivity value at each epoch is averaged over 100
perturbations and 5 different copies of the network trained on CIFAR10. Similarly for “mean(min)”, except the
min of the 100 approximations after perturbing is taken. Both quantities for both networks provide evidence that
SGD converges to an increasingly contractive solution.
sian contractive regions. We propose to empirically support this hypothesis by locally approximating
the LHS of Eq. (6)-normalized by ∣∣Vw f (W, Z )k2 -during the training of a model, and showing that
this quantity steadily increases as training progresses. This indicates that SGD leads the training into
regions of increasing contractivity.
We now explain our approximation: For a model w*, We generate many random samples W from
the unit sphere around w*. For each W, We locally estimate a stochastic version of Hessian-gradient
product to get the approximation
Vwf (W, z)>H(Z)Vwf (W,	z) ≈	1[Vwf (W,	z)	- Vwf (W	-	ηVwf (W,	Z),	Z)]>Nwf (W, Z)	(4)
where the gradients on the RHS approximation are taken with respect to a random minibatch of
samples. For each perturbation W we normalize the corresponding approximation by ∣∣Vwf (W, Z)∣2,
and then take the mean and minimum over all of these quantities.
For our experiments, we used the well-known CIFAR10 dataset (Krizhevsky et al., 2009) and two
popular deep architectures: ResNet18, and DenseNet121 (Huang et al., 2017; Lin & Jegelka, 2018).
To match the settings in the theorems of this paper as much as possible and avoid any confounding
factors, we avoided standard regularization (e.g. weight decay), data augmentation, and adaptive
learning rates. We used SGD with a constant learning rate of 0.01 and momentum of 0.9. The
models were trained for 200 epochs and batch size of 128. We trained five copies of each network
and checkpointed the models every 8 epochs. For each checkpoint, we generated 100 random
perturbations from the unit sphere around the checkpoint weights. We used η = 0.001 and batch size
128 for the approximation in Eq. (4). We averaged over these 100 approximations and then further
took the average over the five networks. The results of these experiments are shown in Fig. 1b, and
indicate a clear increase in Hessian contractivity as training progresses.
5 Conclusion
In this paper, we studied the stability bound of SGD with regard to different types of loss functions.
We proved a better upper bound and proved the tightness of various bounds. These tightness results
suggest that existing stability bounds may not suffice in answering the generalization problem of deep
neural nets. We propose a new Hessian contractive condition that is slighly stronger than convex,
but with O(1) stability bound. We provide empirical evidence to support the hypothesis that deep
learning loss function is Hessian contractive near local minima.
8
Under review as a conference paper at ICLR 2020
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via over-
parameterization. In International Conference on Machine Learning, pp. 242-252. PMLR, 2019.
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi Zhang. Stronger generalization bounds for
deep nets via a compression approach. In Proceedings of the 35th International Conference on
Machine Learning, ICML, 2018.
Peter L Bartlett, Dylan J Foster, and Matus J Telgarsky. Spectrally-normalized margin bounds for
neural networks. In Advances in Neural Information Processing Systems, pp. 6240-6249, 2017.
Leon Bottou. Stochastic gradient descent tricks. In Neural networks: Tricks ofthe trade, pp. 421-436.
Springer, 2012.
Olivier Bousquet and Andre Elisseeff. Stability and generalization. Journal of machine learning
research, 2(Mar):499-526, 2002.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points i. Mathematical Programming, pp. 1-50, 2019a.
Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Lower bounds for finding stationary
points ii: First-order methods. Mathematical Programming, pp. 1-50, 2019b.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. In ICLR, 2017.
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs,
Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent
into wide valleys. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124018,
2019.
Yuansi Chen, Chi Jin, and Bin Yu. Stability and convergence trade-off of iterative optimization
algorithms. arXiv preprint arXiv:1804.01619, 2018.
Luc Devroye and T Wagner. Distribution-free performance bounds with the resubstitution error
estimate (corresp.). IEEE Transactions on Information Theory, 25(2):208-210, 1979a.
Luc Devroye and Terry Wagner. Distribution-free inequalities for the deleted and holdout error
estimates. IEEE Transactions on Information Theory, 25(2):202-207, 1979b.
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai. Gradient descent finds global
minima of deep neural networks. In International Conference on Machine Learning, pp. 1675-
1685, 2019.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of cryptography conference, pp. 265-284. Springer, 2006.
Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil. Stability of randomized learning
algorithms. Journal of Machine Learning Research, 6(Jan):55-79, 2005.
Vitaly Feldman and Jan Vondrak. High probability generalization bounds for uniformly stable
algorithms with nearly optimal rate. arXiv preprint arXiv:1902.10710, 2019.
Vitaly Feldman, Ilya Mironov, Kunal Talwar, and Abhradeep Thakurta. Privacy amplification by
iteration. In 2018 IEEE 59th Annual Symposium on Foundations of Computer Science (FOCS), pp.
521-532. IEEE, 2018.
Vitaly Feldman, Tomer Koren, and Kunal Talwar. Private stochastic convex optimization: optimal
rates in linear time. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of
Computing, pp. 439-449, 2020.
Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic
gradient for tensor decomposition. In Conference on Learning Theory, pp. 797-842, 2015.
9
Under review as a conference paper at ICLR 2020
Moritz Hardt, Ben Recht, and Yoram Singer. Train faster, generalize better: Stability of stochastic
gradient descent. In International Conference on Machine Learning, pp. 1225-1234, 2016.
Sepp Hochreiter and Jurgen Schmidhuber. Simplifying neural nets by discovering flat minima. In
Advances in neural information processing systems, pp. 529-536, 1995.
Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected
convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition, pp. 4700-4708, 2017.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in neural information processing systems, pp. 315-323, 2013.
Kenji Kawaguchi, Leslie Pack Kaelbling, and Yoshua Bengio. Generalization in deep learning. arXiv
preprint arXiv:1710.05468, 2017.
Michael Kearns and Dana Ron. Algorithmic stability and sanity-check bounds for leave-one-out
cross-validation. Neural computation, 11(6):1427-1453, 1999.
Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter
Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv
preprint arXiv:1609.04836, 2016.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.
Vladimir Koltchinskii and Dmitriy Panchenko. Rademacher processes and bounding the risk of
function learning. In High dimensional probability II, pp. 443-457. Springer, 2000.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.
Anders Krogh and John A Hertz. A simple weight decay can improve generalization. In Advances in
neural information processing systems, pp. 950-957, 1992.
Ilja Kuzborskij and Christoph Lampert. Data-dependent stability of stochastic gradient descent. In
International Conference on Machine Learning, pp. 2815-2824. PMLR, 2018.
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein. Visualizing the loss landscape
of neural nets. In Advances in Neural Information Processing Systems, pp. 6389-6399, 2018.
Hongzhou Lin and Stefanie Jegelka. Resnet with one-neuron hidden layers is a universal approximator.
In Advances in Neural Information Processing Systems, pp. 6172-6181, 2018.
Wenlong Mou, Liwei Wang, Xiyu Zhai, and Kai Zheng. Generalization bounds of sgld for non-
convex learning: Two theoretical viewpoints. volume 75 of Proceedings of Machine Learning
Research, pp. 605-638. PMLR, 06-09 Jul 2018. URL http://proceedings.mlr.press/
v75/mou18a.html.
Vaishnavh Nagarajan and J Zico Kolter. Uniform convergence may be unable to explain generalization
in deep learning. In Advances in Neural Information Processing Systems, pp. 11615-11626, 2019.
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro. Exploring general-
ization in deep learning. In Advances in Neural Information Processing Systems, pp. 5947-5956,
2017.
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan. Making gradient descent optimal for
strongly convex stochastic optimization. In Proceedings of the 29th International Coference on
International Conference on Machine Learning, pp. 1571-1578. Omnipress, 2012.
Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of adam and beyond. In ICLR,
2018.
William H Rogers and Terry J Wagner. A finite sample distribution-free performance bound for local
discrimination rules. The Annals of Statistics, pp. 506-514, 1978.
10
Under review as a conference paper at ICLR 2020
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan. Learnability, stability
and uniform convergence. The Journal of Machine Learning Research, 11:2635-2670, 2010.
Ohad Shamir and Tong Zhang. Stochastic gradient descent for non-smooth optimization: Convergence
results and optimal averaging schemes. In International Conference on Machine Learning, pp.
71-79, 2013.
Samuel L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc V Le. Don’t decay the learning rate,
increase the batch size. arXiv preprint arXiv:1711.00489, 2017.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Vladimir N Vapnik. Statistical learning theory. A Wiley-Interscience Publication, 1998.
Lei Wu, Zhanxing Zhu, et al. Towards understanding generalization of deep learning: Perspective of
loss landscapes. arXiv preprint arXiv:1706.10239, 2017a.
Xi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri, Somesh Jha, and Jeffrey Naughton. Bolt-on
differential privacy for scalable stochastic gradient descent-based analytics. In Proceedings of the
2017 ACM International Conference on Management of Data, pp. 1307-1322, 2017b.
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael Mahoney. Pyhessian: Neural networks
through the lens of the hessian. arXiv preprint arXiv:1912.07145, 2019.
Dongruo Zhou and Quanquan Gu. Lower bounds for smooth nonconvex finite-sum optimization.
arXiv preprint arXiv:1901.11224, 2019.
A Proofs
Lemma 1 (Dynamics of divergence). Let f (w; x) = 1 w> Aw - yx. Suppose [xi - xi]∕∣∣xi - Xik
is an eigenvector of A where A[xi - x0i] = λxx0 [xi - x0i]. Let ∆t be wt - wt0, αt ≤ λxx0 be the step
size of SGD and ∆0 = 0. Suppose one runs SGD on f(w, S) and f(w, S0) where S, S0 are twin
datasets and x0i>xj = 0, xi>xj = 0, ∀j 6= i, the dynamics of∆t are given by:
Ell∆t+1|| = (I - αtλxxO)EllδM +—Ilxi- XiIl	(5)
n
Proof: In case the different entry zi, zi is not picked, the gradient difference of f(w; z) and f(w; z0)
is
Vf (w,z) - Vf (w0, z0) = A[w - w0]
and in case different entry zi , zi is picked,
Vf (w, z) - Vf (w0, z0) = A[w - w0] + [xi - x0i]
Since ∆0 = 0, one can inductively show ∆t = θt [xi - x0i] where θt > 0. Since SGD selects zt = zt0
with probability 1 - * and a different entry with probability ɪ We have the following dynamic:
∆t+1
(I - αtA)[wt - wt0]
(I - αtA)[wt - wt0] + αt [xi - x0i]
with prob. 1 - 1
with prob 1∕n.
E∣∆t+1 k =E [∣∆t+1k∣Index i is not selected] P [Index i is not selected]
+ E [l∆t+1 l|Index i is selected] P[Index i is selected]
=(I-)k(I - A)[wt	-	wt]k	+ ■—k(I	- A)[wt	- Wt]	+	αt[xi	- xi]k
nn
=(1-)(1 - αt λxx0 )θtkxi - Xi k +-[1 - αtλxx0 θt + αt]kxi - Xiil
nn
=(1-°』中同十 α kxi-xik
11
Under review as a conference paper at ICLR 2020
□
Lemma 2 (Lower bound on divergence). Let ∆t be wt - wt0, αt be the step size of SGD and ∆0 = 0.
Suppose [xi - x0i]/kxi - x0ik is an eigenvector of A where A[xi - x0i] = λxx0 [xi - x0i]. Running
SGD on f(w, S), we have:
k 0k T-1 T-1
Ek∆τk ≥ kxi- Xik X Y at(1- ατ…
n
t=1 τ =t+1
Proof: By Lemma 1 we have
Ek∆τk = Ek(I - ατ-iA)∆τ —i + αT-1 [g - χi]k
n
=(1 — ατ —1 λχχ,)E∣∣∆τ -1 k + αT-1 kxi - Xik
n
1 T-1 T-1
= k[xi- χi]k n ɪ2 at ɪɪ (1 - ατλχχθ)
n t=1	τ =t+1
□
Theorem 1.	Let wt, wt0 be the outputs of SGD on twin datasets S, S0 respectively, ∆t be wt - wt0 and
αt be the step size of SGD. There exists a function f which is convex and β-smooth, L-Lipschitz on
domain of wt, wt0 and twin datasets S, S0 such that the divergence of the two SGD outputs satisfies:
Ek∆τk ≥ L XT a at, and e3 ≥ L XT , a	⑹
2n	t=1	2n	t=1
Proof We set Xi = v, yi = 0.5, X0i = -v, yi0 = 0.5. The proof is constructive. Let f(w, z) =
2w>Aw - yx. The function is β smooth and convex by setting β = ∣∣Ak and u>Au ≥ 0 for all
u. We set S \ {zi } = S0 \ {zi0} to lie in the range of A, where A is a PSD matrix which is not
full-rank. We further set Xi , X0i to lie in the null space of A so that Av = 0. The lower bound
k∆kT follows from Lemma 2 from the fact that ∆0 = 0. Since w>Aw = w0>Aw0, εstab =
SUpz E|f (wT, Z)- f(wT, Z) | ≥ E∣v>[wT - wT] | ≥ n1 PT=I at.
The last part we show for any sequence wt generated from stochastic gradient descent step using
data set S will have a constant gradient thus the Lipshitzness will follow. Let U be the range of
A (so any u ∈ U satisfy uAu > 0. Pick γ so that u>Au ≥ γ > 0 holds for all u ∈ U. For all u,
IlWt+ι”k ≤ k(I — atA)w>u + az>uk ≤ (1 — atγ)∣∣w>Uk + a. This implies ∣∣w>Uk ≤ Y. Now We
boundgradient ∣Vfw(wt,z)k = ∣∣Awt-Xk ≤ ∣v>Aw∣ + ∣v>yx∣ + ∣u>Aw∣ + ∣u>yx∣ ≤ 1 + Y. This
implies that the function is L-Lipschitz with L = 1 + Y. By setting β = 2γ the proof follows. □
Theorem 2.	Suppose loss function f (w,z) oftheform f (w, S) = ɪ £；=[ fyj (w>Xj) and fy (w>x)
satisfies (1) |fy(∙)∣ ≤ L, (2) 0 < γ ≤ fy0(∙) ≤ β, (3) ∣∣x∣ ≤ R and (4) , S0 are ξ-self correlated.
Let wt, Wt be the outputs of SGD on twin datasets S, S0, ∆t := Wt - Wt and α ≤ ɪ is the step size
of SGD. Then,
4LR
Ekδt k ≤ 可
Proof: For simplicity we omit the dependence of f on yj so that fyj (W>Xj ) = f(W, Zj ). Note that
the gradient of the loss function is Pfyj (WIXj) = fj§ (WIXj )xj and the Hessian is V2 fyj (WIXj)=
fy00 (Wt>Xj )Xj Xj>. The stochastic gradient step of fyj (Wt>Xj ) is
Wt+1 = Wt - atfy0j (WtIXj)Xj.
The dynamics of the divergence can be described as:
Ei：t+iid+ik=Ei：tk n X ∆t - atfy Nj(W>x ) - fy j (WrXj)]叼
j6=i	(7)
1°t - αt[fy i (WJXi)Xi- fy, (Wt>Xi)Xi])k
12
Under review as a conference paper at ICLR 2020
Note that [fy0 j yj (wt>xj) - fy0j (wt0>xj)]xj can be rewritten as fy00j (wtθj xj)xjxj>∆t where wθj
(1 - θj)wt + θj wt0 , 0 < θj < 1. Similarly we can also rewrite fy0i (wt>xi)xi - fy00 (wt0>xi)x0i as
fy0i (wt>xi)xi - fy0i0 (wt0>xi)x0i
=1 {fy i (WJxi)Xi - fy i(W0> Xi)Xi}
+1 {fyo IWIXO)Xi - fy； (Wrxi)Xi}
+ 1 {fy i (w0>χi)+fy； (w>χi)}xi
-	1 {fy0 (w>xi) + fy0(W0>xi)}Xi
=	1 fyi ⑶"“)""d +1 fyi (Wtjxl0)XlixliTdt
+	1 {fy i (WOTxi)+fy；(W>xi)}Xi
-	1 {fy0 (WTxi) + fy0(W0Txi)}Xi
Thus equation equation 7 can be written as
Ert+1∣A+1k = Ei：tk(I - αt Xfyj(Wθi>xi)xixT)∆t
n j=1
+ αt {fy i (WtTxi)+fy i (WTxi) }xi
- at{fy0 (WTxi) + fy0 (W0Txi)}xik
By the ξ-self correlated assumption and letting H = * Pxχ∈.6s xjxj and H0 = * Pxχ∈,Es，xjx；, We
have:
Ek∆t+ιk ≤ Ek(I- αtγ{H + H0})∆tk + αLkxik + αLkxik
2	nn
≤ (1 - αtξγ)Ek∆tk + 2αtLR.
2n
(9)
The first inequality follows from the fact that f00(∙) ≥ Y and xjxjS are all PSD. The second inequality
follows from the fact that ∆t ∈ Span{xι,.., xn}. Fix ɑt = ɑ and the theorem follows.	□
Our next two lemmas are used in the proof of Theorem 3.
Lemma 3. Suppose xt° ≥ 0, xt+ι = (1 + o^a^ )xt + y, we have xτ ≥ y( T )α if a > 0 is a
sufficiently small constant.
Proof: In the proof we use following inequality:
eα ≤ 1 + a— ≤ e oa99
一 0.99 一
13
Under review as a conference paper at ICLR 2020
where a > 0 is a sufficiently small constant.
xT
TT
x y Y(i+
t=t0+1 s=t+1
T
099S) + x0 Y (I+ 099s)
t=t0+1
≥ X I exp (a X S；
t=t0+1	s=t+1
T
≥ X I exp (alog(T∕t))
t=t0+1
(10)
T1
≥ yTa X -f-
t1+a
≥y
□
Lemma 4. There exists a function f which is non-convex and β -smooth, twin datasets S, S0 and
constant a > 0 such that thefollowing holds: ifSGD is run using step size at = 0 嬴t for 1 ≤ t <T,
and wt, wt0 are the outputs of SGD on S and S0, respectively, and ∆t := wt - wt0, then:
∀1 ≤ t0 ≤ T,
EgTkd=0]≥ 2n
(11)
Proof: Consider the function f in Equation 1, and choose A to have positive and negative eigenvalues.
We set the minimum eigenvalue of A equal to -β and all other eigenvalues with absolute value at
most β. We select twin datasets for such A as follows. We set all elements in S \ {xi} = S0 \ {x0i}
to lie in the column space of A. Also, ∀j 6= i, choose xj such that xj>Axj > 0, and choose any yj
between 0 and 1.
Let v be such that v>Av = -β and kvk = 1. Finally, let xi = v,yi = 0.5, x0i = -v,yi0 = 0.5.
In this setting, one observes that the divergence ∆t follows the following dynamic:
∆t+1
(I - αtA)∆t
with prob. 1 - 1
(I - atA)∆t + α2t [xi - xi]	with prob 1∕n.
We first observe that ∆t := wt - wt0 is of the form vθt, where θt > 0. This can be shown using
induction. Let T be the first time that xi, Xi are picked, We have △「+ι = Ot [xi - χi] = va「. The
iterative step of ∆t+1 and ∆t implies that ∆t+1 = vθt+1 where θt+1 = (1 + αtβ)θt with probability
(1 - 1) and θt+ι = (1 + αtβ)θt + at with probability 1 .
The above construction then yields:
Ei：t+i gt+ikd=o] =ELj (1 - n) k(I- atAAk+n k(I- atA)^t+atvk
kv kE1:t
(1 + atβ)θt H—((1 + atβ)θt + at)
n
(12)
kvkEi：t h[(1+atβ)θt] + ^nt i
(1 + 0⅛t )EιMk∆tk∣∆t0=0] + a kvk
0.99t	n
Now apply Lemma 3, with Xt = E[∣∣∆t∣∣∣∆to = 0] and y =。。/：.This gives Us that XT ≥
0.99βn (T∕t0)a = 0⅛ (T∕t0)a, since ||v|| = 1.
Finally, the claimed bound follows by setting the minimum eigenvalue β =瑞9
□
14
Under review as a conference paper at ICLR 2020
Theorem 3. There exists a function f which is non-convex and β-smooth, twin datasets S, S0 and
constants a s.t. the following holds: if SGD is run using step size ɑt = 0 Q= for 1 ≤ t < T, and
wt, wt0 are the outputs of SGD on S and S0, respectively, and ∆t := wt - wt0, the divergence of SGD
after T rounds (T > n) satisfies:
a
Ekδtk ≥ k	(13)
Proof: The proof is based on Theorem 4 plus the idea of a “burn-in” period. We have:
Ek∆τ Il= E[kwt - w0k∣∆n = 0]P[∆n =0]+ E[kwt - wtk∣∆n = 0]P[∆n = 3
≥ E[kwt - w0 k^n = 0]P@n = 0]
Ta
3n+a kxi- Xik
n+a kxi - Xik
(14)
≥
□
Lemma 5. (Hardt et al., 2016) Assume f is β-smooth and L-lipschitz. Let wt, wt0 be outputs of
SGD on twin datasets S, S0 respectively after t iterations and let ∆t := [wt - wt0] and δt = Ek∆tk.
Running SGD on f (w; S) with step size α==% satisfies the following conditions:
•	The SGD update rule is a (1 + αtβ)-expander and 2αtL-bounded.
•	E[k∆tk∣∆t-1] ≤ (1 + αtδ) k∆t-ik + 2atL.
•	e[HδT Hdk =0] ≤ ( tkT-1 )a 2L.
Theorem 4 ( Permutation). Assume f is β-smooth and L-lipschitz. Running T (T > n) iterations of
SGD on f (w; S) with SteP size a==%,the stability ofSGD satisfies:
2LTa	2L2Ta
Ek∆τk≤ F,εstab ≤ -1+T	(15)
n +a	n +a
Proof. Let H = t represents the event that the first time the SGD pick the different entry is at time t:
Ek∆T k
E[k∆τk∣H ≤ n]P[H ≤ n] + E[∣∣∆τk∣H > n]P[H > n]
^^^^^{^^^^^≡
0(permutation)
≤
≤
*
≤
≤
1n
-EE[k∆τ k∣H = t]
t=1
1 n	T	a	2L
n ɪ-/ ∖ t	J	n
n t=1	t	n
2LTa	n	1
n2 Jt=ι ta t
2LT a
n1+a
(16)
The inequality (*) derived by applying Lemma 5.
□
Lemma 6. Let wt, wt0 be outputs of SGD on twin datasets S, S0 respectively after t iterations and
let ∆t := wt - wt0. Suppose that tk = ctk-1. Then the following conditions hold:
• P∆tk-1 = Od=0] ≤ nɪ!.
• P∆tk-ι = o∣∆tk = 0] ≤ C (ι + tnk).
• E[k∆τk∆tk =0] ≤ E[k∆τk∣∆tk.ι = 0]C (1 + n) + (tk-1 )a2L
15
Under review as a conference paper at ICLR 2020
Proof. In the proof we will use the following inequality with r ≥ 1:
U ≤ (1-1)r≤ ɪ
n	n n + r
i):
	PAtk-1 = 0∣Atk =0] = PAtk-IA = *⅛ =0] P[Atk = 0] ,	…	1 - (1 - 1∕n)tk-tk-1	八 =(1 - 1∕n)tk-1	J L <7 —	(17) 1 - (1 - 1∕n)tk n ≤ (1 - 1∕n)tk-1 ≤ —— n + tk-1
ii):	P[∆tk-1 = 0∣ Atk =0] = PAtk £ A⅛ =0] P[Atk = 0] =P[∆tk-1 = 0] =P[∆tk =0] _ 1 - (1 - 1∕n)tk-1 -I-(I- 1/n)tk	(18) 1	n— ≤	n+tk-1 —	1 - n-tk n ≤ F (1+tk) tk	n = 1(1 +tk) c	n
iii):
By applying i) and ii) in the decomposition of E[∆τ∣Atfc = 0] we have
E[∣∣AtMA" =0] ≤ E[∣∣∆τ∣∣∣∆t一 = 0]P[∆Ι-ι = 0∣∆tfc = 0]
+ E[k∆τ k∣∆"- = 0]P[∆tk-ι = 0∣∆tk = 0]
≤ EgT g 一=0]铝1+?)
T
+J尸
2L
n + tk-1
(19)
C (1 + F )E[k∆τ k∣∆i-
=0]
T
+( Q )a
2L
n + tk-1
where the last inequality uses the fact that E[∣∣∆t∣∣∣∆tfc = 0] ≤ (τ^)a 斗.	□
Theorem4b (Uniformly Independent) Assume f is β-smooth and L-lipschitz. Running T (T > n)
iterations of SGD on f (w; S) with step size at =防,the stability of SGD satisfies:
T a	T a
EkATIl ≤ 16log(n)Ln1+-； εstab ≤ 16log(n)L2n—	(20)
Proof. We first decompose AT as follows by selecting tk = n:
EllAT Il= E[∣∣At IlIAtk = 0]P[Atfc = 0] + E[∣∣At IlIAtk = 0]P[Atfc = 0]
'------------{-------------} '-------------{------------}	(21)
Term 1 ‹ 2⅛a (Lemma 5)	Term 2 ‹ 11L lo里n)Ta
一 n1 + a	_	nl+ a
16
Under review as a conference paper at ICLR 2020
Term 1 is easily bounded by applying Lemma 5 with ɑt = ɪ. To bound Term 2, plug in
P[∆tk = 0] = 1 一 (1 一 1∕n)tk ≤ tk and recursively apply point (iii) from Lemma 6 by setting
ti+1 = cti. We get:
E[k∆τ k∣∆tk=0]P[∆tk=0]
≤
≤
≤
≤
k1	k1
2LtkX( T )a -⅛ Y (1 + A 卢
n n i=1 ti n + ti τ=i+1	n tτ+1
k-1	k-1
2L X(T)an⅛eχp( X ⅛1)
nn	n
i=1 i	i	τ =i+1
2nLexp (c-1)X(T)a n⅛
2cLT a	c	k-1 ti1-a
^expj工〒
i=1
(22)
≤
2L log(n)T a ca
c
≤
ni+a 而 expJ
11 log(n)LT a
n1+a
In the second and third inequality we use the fact that 1 + x ≤ exp(x) and ti+1 = cti to get
Qk-1+1(1 + tτn+1) ≤ exp(PT=1+ι tτn+1) ≤ exp (占).The 山St inequality is derived by picking
C = 4 .	□
Theorem 5. Let wt, wt0 be the outputs of SGD on twin datasets S, S0, and let ∆t := wt - wt0. There
exists a function f which is non-convex and β -smooth, twin sets S, S0 and constants a, γ such that
the divergence ofSGD after T rounds (T > n) using constant SteP size ɑ = o_99Y satisfies:
Ea k ≥ n⅛ …
(23)
Proof: The proof is similar to Theorem 3. Since ∆t ∈ Span{xi - x0i}, we have:
Ek∆t+ιk ≥ (1 — 1)(1 + αtβ)E∣∣∆tk + αtIE- Xik
n
n
Suppose to is the hitting time when k∆tok > 0 and k∆t0-ik = 0 ,k∆τk ≥ kxi-xik ea(T-t0)/2
Ek∆τ k = E[kwt - wtk∣∆ι = 0]P[∆ι = 0] + E[kwt - wt k∣∆1 = 0]P[∆1 = 0]
≥ E[kwt - wtkA = 0]p[δi = 0]
=1( kxi - Xik eaT/2)
nn
_ kxi - Xik .aT/2
——	2 e .
n2
(24)
□
Theorem 6. Given a set S and a β-smooth and L-LiPschitz loss funtion f, suPPose SGD with a
2
fixed step size a ≤ ^^ reaches point wo which is close to a local minimum W in a (σ, Y)-Hessian
contractive region: kw0 一 w*k ≤ Q with large enough radius so that Q > 1γL, then we have ∀T ≥ 1,
kWT — W* k ≤ Q.
17
Under review as a conference paper at ICLR 2020
Proof: LetHwt,w*,θ = n Pzj∈S VW f(θwt + (1 - θ)wο,Zj) We derive that:
kwt+1 - w0k2
=kwt - w0 - αVwf(w, z)k2
=kwt - w0k2 + α2kVwf(ws,z)k2 - 2αVwf(wt,z)>[wt - w0]
=kwt - w0k2 + α2kVwf(ws,z)k2 - 2αVwf(w0, S)>[wt - w0]	(25)
-	2α[Vwf(wt, z) - Vwf(wt, S)]>[wt - w0]
-	2α[Vwf(wt, S) - Vwf(w0, S)]>[wt - w0]
≤kwt - w0k2 + α2L2 - 2α[wt - w0]>Hwt,w*,θ[wt - w0] + 6αLσ
Due to the fact that wt - w0 ∈ Span{Vwf(w0, z), ..., Vwf(wt-1, z)}, we have
kwt - w0k2 + α2L2 - 2α[wt - w0]>Hwt,w*,θ[wt - w0] + 6αLσ
≤(1 - 2αγ)kwt - w0 k2 + α2L2 + 6αLσ
(26)
This implies kwt - w0k ≤ σ for all t.
□
B Empirical Support for Theorem 6
In order to empirically support Theorem 6, we ran the follow experiment: We started with a fully-
trained model w*, and generated random perturbations W = w* + V where V 〜N(0, σ2). We then
trained each W until convergence W*, and computed the normalized distance ∣∣W* — w*k∕∣∣w*k.To
match Theorem 6, this distance should be no more than 2∣∣W* — w* k∕∣∣w* k, i.e., the trained weights
W* after perturbing stay close to w*, as in the conclusion of Theorem 6.
For the above experiment, we first trained both ResNet18 and Densenet121 for 200 epochs, with
constant learning rate of 0.01, a momentum of 0.9, and batch size of 128. We generated 20 Gaussian
perturbations W of the trained model weights w* and trained each for 50 more epochs, resulting in
weights W*. We used standard deviation σ ∈ {0.01,0.005} for the Gaussian perturbations. We then
computed the normalized differences as explained in the previous paragraph. The results are shown in
Table 2. Inspecting the table shows that the distances between the trained weights after perturbing are
quite close to the distances before further training, with low variance. This indicates that the weights
after training the perturbed model are well within the desired range as specified in Theorem 6.
			kW - w*k∕kw*k	kW*- w*k∕kw*k
ResNet	σ	=0.01	0.549±0.00009	0.557±0.0003
	σ =	二 0.005	0.274±0.00005	0.277±0.007
DenseNet	σ	=0.01	0.123±0.00003	0.126±0.0004
	σ =	二 0.005	0.062±0.00001	0.065±0.003
Table 2: Pertubed SGD Results
18