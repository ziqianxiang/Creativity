Under review as a conference paper at ICLR 2021
Robust Memory Augmentation by Constrained
Latent Imagination
Anonymous authors
Paper under double-blind review
Ab stract
The latent dynamics model summarizes an agent’s high dimensional experiences
in a compact way. While learning from imagined trajectories by the latent model is
confirmed to have great potential to facilitate behavior learning, the lack of memory
diversity limits generalization capability. Inspired by a neuroscience experiment
of “forming artificial memories during sleep”, we propose a robust memory aug-
mentation method with Constrained Latent ImaginatiON (CLION) under a novel
actor-critic framework, which aims to speed up the learning of the optimal policy
with virtual episodic. Various experiments on high-dimensional visual control
tasks with arbitrary image uncertainty demonstrate that CLION outperforms ex-
isting approaches in terms of data-efficiency, robustness to uncertainty, and final
performance.
1	Introduction
Model-based Reinforcement Learning (MBRL) approaches benefit from the knowledge of a model,
allowing them to summarize the agent’s past experiences and foresight future outcomes into a
compact latent space. For complex high-dimensional visual domain tasks, the latent dynamic model
has shown great potential to enable efficient long-sighted behavior learning (Hafner et al. (2019b),
Hafner et al. (2019a)). Recurrent Stochastic State Model (RSSM) (Hafner et al. (2019b)) used in the
PlaNet (Hafner et al. (2019b)) and Dreamer (Hafner et al. (2019a)) is the state-of-the-art solution
for building latent dynamics in MBRL, in which there are both a stochastic path and a deterministic
path to summarize the experiences. The historical information, which we called memory, is mainly
embedded through the deterministic path to reliably remember the experiences for multiple time
steps. However, such deterministic embedding is a trade-off between the model prediction error
and its efficiency to generate useful trajectories. In prior works, imagined trajectories branch off
from the states of sequence sampled from the agent’s past experiences with a same deterministic
memory embedding. The efficiency for generating useful trajectories is limited by the interaction
between the agent and the environment, which leads to the lack of policy robustness to the noise on
the deterministic memory vector caused by observation uncertainty and model error.
Inspired by a neuroscience experiment in inducing sleeping mice’s artificial memories of a specific
reward situation (Gundersen (2015)), which shows for the first time that artificial memories can be
implanted into the brains of sleeping animals, we investigate the integration of memory augmentation
techniques with model-based reinforcement learning to enable agents learning behaviors from
trajectories they have never really experienced. As shown in Figure 1, performing augmentation can
improve the data-efficiency significantly without any interaction because every augmented memory
vector corresponds to a state transition sequence. We propose a robust memory augmentation
approach by Constrained Latent ImaginatiON (CLION) that aims to improve the policy robustness
by effectively enhancing memory diversity. Different arbitrary transforms are implanted on the
deterministic hidden states of the latent trajectories sampled from the dataset. Every augmented state
is chosen as the initial states of the imagined trajectories, which enable the agent to learn how to
obtain more rewards from the trajectories it has not experienced before. The optimal policy is learned
with the memory augmented latent trajectories by a novel actor-critic algorithm, in which the state
values optimize Bellman consistency for imagined rewards, and the policy maximizes the state values
by propagating their analytic gradients back through the dynamics.
1
Under review as a conference paper at ICLR 2021
Figure 1: Memory augmentation in latent space.
In this paper, we aim to improve the policy robustness by effectively enlarge the model’s latent space
with the enhancement of agent’s memory diversity by performing augmentation on the deterministic
memory vectors. Our main contributions are:
(1)	We improve the latent model used in Dreamer and PlaNet by adding two constraints to guide
the augmentation in latent space to meet a basic condition that the latent states near the embedded
states from real data should also be mapped to real physical states.
(2)	We proposed a latent trajectory augmentation framework to improve the policy robustness to the
latent noise and data-efficiency, giving the confidence of every augmented trajectory simultaneously
expected to filter out the augmented data in the area with poor model prediction and make full use of
the reliable augmented trajectories.
2	Constrained latent stochastic state model
2.1	Recurrent stochastic state model
Learning the optimal policy by latent imagination requires an accurate dynamics model without
loss of generality. RSSM can facilitate long-term predictions and enable the agent to imagine
thousands of trajectories in parallel. It represents both the historical memory and the dynamic
uncertainty by a recurrent neural network. Denote the sequence of observations o = {o0, o1,..., oτ},
actions a = {a0, a1,..., aτ } and rewards r = {r0, r1,... ,rT }, with corresponding stochastic latent
states sa = {s0, s1, . . . , sT} and memories h = {h0, h1, . . . , hT}. The RSSM consists of four key
components: (1) the representation model p(st|ht, ot), which encodes observations and actions to
create continuous vector-valued latent states with Markovian transitions and embeds the long-term
historical information ht recurrently; (2) the transition model q(st|st-1, at-1, ht-1), which is built
by a recurrent neural network, predicts future latent states with historical memory and actions; (3) the
observation model q(ot|ht, st), which is used to provide a reconstruction learning signal; and (4) The
reward model q(rt|st, ht) predicts the rewards given to the latent states.
The components of the RSSM are optimized jointly to increase the variational lower bound (Jordan
et al. (1999)) of log p(oa, ra | ua), which includes reconstruction terms for observations and rewards and
a KL regularizer. The expectation is taken under the dataset and representation model,
Jm = Ep(E (JO + JR + JD)J
JOt = logq(ot|st,ht)	JRt = logq(rt|st,ht)
JDt = KL [p(st|st-1,at-1,ot) kq(st|st-1,at-1, ht-1)]
(1)
Here, we implement the transition model as a recurrent neural network, the representation model as a
convolutional neural network (CNN), the observation model as a transposed convolutional neural
network , and the reward model as a dense network.
2.2	Constrained RSSM for memory augmentation
The RSSM embeds the long-term memory by the recurrent neural network and outperforms the non-
recurrent network in many model-based tasks. However, the memory embedded in the deterministic
hidden state results in the lack of memory diversity, which limits the generality of the latent dynamics
model. Memory augmentation is an emerging technology in neuroscience whereby either virtual or
2
Under review as a conference paper at ICLR 2021
real memories can be implanted into an agent’s brain. One benefit is to enable the agent to remember
things they may not otherwise be able to experience.
Q
C
CNNI / CNNI
Embedding
Constraint
Reconstruction
7^t) I ~P(TtlSf，瓦)
Stochastic Node
Deterministic Node
Memory
Augmentation
Figure 2: Constrained RSSM framework. CNN, GRU and FC represent a convolutional neural
network, a GRU-cell, and a fully-connected layer, respectively.
I ~p(θf∣sf,⅛f)
Decoding
Constraint
However, the augmented memory obtained by intuitively adding random transforms to the memory
vector could lead to mismatching between augmented trajectories and real physical states. As shown
in Figure 3a, if we add an arbitrary noise to the memory embedding ht, the augmented latent state
(h0t, s0t) in RSSM results in low-fidelity reconstructed images that couldn’t be mapped to real existed
physical states. Such a phenomenon suggests that RSSM is sensitive to the memory noise. It will
damage the policy because both the model prediction error during the imagination and the observation
uncertainty in the testing process will introduce noise on the memory vector. In an attempt to ensure
(a) RSSM (without constraints)
Figure 3: Reconstruction with augmented memory. The first row is the original image input, the
second row is the image reconstructed by (h0t, s0t), and the third row is reconstruction error.
the latent model is robust to the added memory noise, we propose a Constrained RSSM (CRSSM)
framework for memory augmentation with two constraints: (1) embedding constraint, which ensures
the embedding consistency between the original input ot and transformed input o0t (we perform (-5,
5) degree random rotation on the original image ot ), and (2) decoding constraint, which ensures the
consistency between augmented latent states and the real physical states. As shown in Figure 2, o0t
is an arbitrary transformed image from the original image input ot , et is the encoding feature of
original input ot, e0t is the encoding feature of augmented input o0t, ξh is a noise that obeys N (0, σh)
distribution, h[ is the augmented memory which adds the noise ξh to the hidden state ht, and Ot
is the reconstructed image from (h0t , st). The CRSSM is to minimize JM in equation 1 with two
constraints:
min JM
s.t.||e0t - et ||2 ≤ δ1	(2)
||Ot - Ot∣∣2 ≤ δ
As demonstrated in Figure 3b, our proposed CRSSM generates high-fidelity reconstructions, which
suggests the enriched dynamics representation is capable of allowing agents to learn behaviors by
latent imaginations with leveraging diverse memory. Note that we leave margin δ1 and δ2 between
3
Under review as a conference paper at ICLR 2021
the augmented branch’s output and the non-augmented branch’s output instead of requiring them to
be entirely consistent. It can be seen from Figure 3b that the reconstructed graph is still different
from the ground truth in detail. For instance, in the third column, the robot’s leg which close to the
ground is straight in the ground truth, while in the reconstructed graph of the latent augmentation, the
leg is bent. In summary, the two constraints used in CRSSM aims to guide the augmentation in latent
space to meet a basic condition that the latent states near the embedded states from real data should
also be mapped to real physical states.
3 Memory augmentation and constrained latent imagination
3.1	Latent trajectory augmentation with diverse memory
By leveraging CRSSM, we propose a Constrained Latent ImaginatiON (CLION) framework that
aims to enable the agent to learn the policy in a larger latent space to improve the policy robustness
by enlarging the capacity and diversity of the deterministic part that deployed to store memory. We
add several zero-mean noises ξh 〜 N(0, σ2) on the memory vector ht in the replay buffer to extend
the agent’s memory which includes diverse embedded historical information. Therefore, the agent
could learn behaviors with the trajectories in a larger latent space, rather than just the agent’s real
experiences. The latent trajectories are imagined from both original states and the augmented states
by the learned transition model. The CLION framework can be summarized as equation 3, for the
i-th augmented trajectory, the augmentation process contains two key steps: (1) start point resampling
with diverse memory and (2) trajectory imagination (model-based rollout).
Resample start point:
hi 〜N(M σ2),	st 〜N(mt, σ2)
Trajectory imagine:
ht,si 〜p(ht,st∣hi-i,
st-1,ai-1),	at~ π(ht,si)
(3)
In order to further mitigate the model error’s impact, we also consider the model’s reliability of
the augmented trajectories by adding a trajectory evaluator. We evaluate every imagined trajectory
predicted by the model by an evaluator and use the trajectories with different weights which depend
on its reliability. In the area where the model has reliable predict accuracy, the augmented data should
be able to improve the policy robustness and data efficiency significantly and we only filtered out the
augmented data in the area with poor model prediction.
Trajectory evaluation is widely used in imitation learning, the Generative Adversarial Imitation
Learning (GAIL) Ho & Ermon (2016); Torabi et al. (2018) utilize a GAN-like framework to learn the
expert policy. In GAIL, a discriminator is used to judge whether the trajectory is generated by an
expert. Similarly, we use a trajectory evaluator to discriminate between real sequence and overshoot
sequence predicted by the model as well as possible. In every iteration, we sample a number of
state-action sequences generated by real interactions from the dataset as real sequences Seqreal =
Concat {st, ht, at, st+1, ht+1}. Then we use the latent model to predict the overshoot trajectories
from the same start points as the fake sequences Seqfake = Concat {st, ht, at, st+ι, ht+ι} to train
the evaluator. The evaluator aims to output the probability that the input is the real trajectory, that is
equal to maximize the following objective function
Jeva = EVA(Seqreal) - EVA(Seqfake)
(4)
3.2	Policy optimization with memory augmented trajectories
The policy π(s;夕)is optimized by imagined trajectories {sτ, a「, IrT}T=H from original data and
augmented data predicted by the transition model. We estimate every state’s value in the imagined
trajectory to trade off the difficulty of long-horizon back-propagation and efficiency of gradient
utilization. That means a single trajectory {sτ, aτ, rτ}tτ+=Ht generate many sub-trajectories with
different rollout steps. As shown in Figure 4, every sub-trajectory is used to update the actor and critic
with different reliable weight w(sτ) given by the trajectory evaluator. The reliable weight w(sτ) is
4
Under review as a conference paper at ICLR 2021
given by the trajectory evaluator and is normalized in batch.
W(sτ) = EVA(sτ, aτ, Sτ +1),	sT 〜(D,Daug)
wsiτ =
W (sT)
pN=0 PT=H W (sτ)
(5)
Figure 4: The CLION framework
In an attempt to trade off bias and variance, we calculate the V (sT) by TD-λ method (Srinivas et al.
(2018)) similar to Dreamer, which is an exponentially-weighted average of the estimates for different
k to make the estimation robust to different predict horizon.
H-1
Vλ (sT) =. (1-λ) X λn-1VNn (sT) + λH-1VNH (sT)	(6)
n=1
VN (ST)= Eqθ,q中(X Yn-τrn + Yh-Tvψ (Sh))	With h = min(τ + k,t + H)	(7)
where the VNK estimates rewards beyond k steps with the learned value model.
The critic is updated to regress the targets around Which We stop the gradient as typical (Srinivas et al.
(2018)). The reliable Weight is used in the critic updating, Which is called Weighted regression of
target value. The objective for the critic vψ (ST), in turn, is to regress the target value
min E
ψ
vψ(ST)-Vλ(ST)k
))
(8)
Different from DDPG (Lillicrap et al. (2015)) and SAC (Haarnoja et al. (2018)) Which only max-
imize immediate Q-values, We leverage gradients through transitions and by update the policy by
backpropagation through the value model.
The object of the actor with policy ∏(s;夕)is to maximize
N	t+H
Jactor = E Ew(ST)VI®)	(9)
i=0 T =t
Since all steps are implemented as neural networks, we analytically compute VφJactor by stochastic
backpropagation. We use reparameterization for continuous actions and latent states and straight-
through gradients for discrete actions. Note that prevous works like DrQ that just uses image-level
augmented data as a regularizer for Q-learning, CLION directly learns the policy by back-propagating
through both augmented trajectories and original trajectories.
5
Under review as a conference paper at ICLR 2021
3.3	Theoretical analyses of CLION framework
Based on Janner’s work (Janner et al. (2019)), we hope to give the upper bound of the value
function estimation error after introducing the latent data augmentation mechanism and provide
theoretical inspiration for further study of model-based reinforcement learning with augmentation.
The gap between true returns and model returns can be expressed in terms of two error quantities
of the model: generalization error m due to sampling, and distribution shift π due to the updated
policy encountering states not seen during model training. As the model is trained with supervised
learning, sample error can be quantified by standard PAC generalization bounds, which bound the
difference in expected loss and empirical loss by a constant with high probability. We denote this
model generalization error as Em = maxt Es〜n口产[Dtv (P (s0, r | s, a) ∣∣pθ (s0, r | s, a))] and the
policy distribution shift between iterations maxs DTV (πkπD) ≤ π by the maximum total-variation
distance. If we can instead approximate the model error on the distribution of the current policy
π, which we denote as E0m , and approximate the E0m with a linear function of the policy divergence
yields: ^mo (e∏ ) ≈ Em + En d^m0 , the upper bound of returns estimation error of k-branched model
dπ
rollout is illustrated in Proposition 3.1.
Proposition 3.1. (Janner et al. (2019)) Under the k-branched rollout method, using model error under
the updated policy em/ ≥ maxt Es〜n口产[Dtv (P (s0 | s, a) ||p (s0 | s, a))], We have
k+1E	kE	k
ηbranch [∏] - η[∏] ≤ 2rmax γ_∏ + +¾ + I— (Em，)	(10)
(1 - γ)2	(1 - γ)	1 - γ
Where the η [π] denotes the returns of the policy in the true MDP, ηbranch [π] denotes the returns of
the policy by k steps rollout based on our model . Such a statement guarantees that, as long as We
optimize such an upper bound, We can guarantee improvement on the true MDP.
Next, we analyze the CLION framework based on Proposition 3.1. The p(s0 | s, a) in the CLION
frameWork is equal to w(s)Pθ (s0 | s, a). In the CLION frameWork the reliable Weight w(s) aims to
approximate the important sampling factor between the data from real environment and the imagined
data, and it is the most significant difference between CLION and Dreamer. We assumed that w(s) is
bounded by
P (S0 | s,a)
Pθ (s0 | s, a)
≤ Ew
(11)
Thus, the total variance distance between the Pθ (s0 | s, a) and P (s0 | s, a) can be derived as equa-
tion 17 (see Appendix A.1).
DTV (P (s' | s,a) ||P(S0 | s,a))
j
(s,a,s，)
|P (s0 | s, a) - w(s)Pθ (s0 | s, a)| ≤ Ew
(12)
Since the Em could be bounded by Ew, the returns estimation error upper bound of CLION could be
given by Theorem 3.1.
Theorem 3.1 . Under the CLION framework, if the model error under the updated policy is bounded
by Em，≥ maxt Es〜∏D,t
by maxt {|w(s) - P (s0
[Dtv (p (s0 | s, a) ∣∣P(s0 | s, a))] and the reweighting coefficient is bounded
| s, a) /P (s0 | s, a)|} ≤ Ew, the returns estimation error upper bound is
ηbranch [π] - η[π] ≤ 2rmax
Yk + 1E∏	YkE∏	k
Dɪ + D + L
(13)
where Ew = Ew + En ⅛w0 . More specifically, for sufficiently low Ew we have
dπ
k* = argmin	[ (Y +	E∏2	+ γ En	+	(Ew)]	> 0	(14)
k	(1-	γ)2	(1- γ)	1- γ w
This insight suggests that when E0w is sufficiently low, the nonzero-length model rollout helps decrease
the upper bound of returns estimation error. Ew indicates how well does the evaluator could classify
the real transition data and the generated transition data by model. The lower the Ew the smaller the
returns estimation error ηbranch [π] - η[π].
6
Under review as a conference paper at ICLR 2021
4 Experiments
4.1	Effectiveness testing
The CLION framework is implemented as Algorithm 1 (see Appendix A.2.1). We use a single
Nvidia K80 GPU for each training run. We use the same hyperparameters of Dreamer across all
continuous control tasks. We compare with both the SOTA of model-based methods and model-free
methods. Considering that the Dreamer algorithm provides a very mature model-based RL training
framework, to eliminate the interference of other engineering factors, we mainly choose Dreamer
as the model-based baseline to test the effectiveness of the CLION framework. As for model-free
methods, Drq (Kostrikov et al. (2020)) uses data regularized Q-learning method to learn the policy
and shows a significant advantage in data efficiency compared to CURL (Laskin et al. (2020b)). We
choose the Drq (Kostrikov et al. (2020)) as the model-free baseline.
In an attempt to test the agent’s ability to handle environmental uncertainty, the experiments are
implemented in DMControl tasks (Tassa et al. (2018)) with observation uncertainty by adding a
rotation (-5°, +5°) to the input from the Mujoco environment, and test the performance of CLION
together with the Dreamer and the Drq. The effectiveness of the CLION framework to tackle the
input uncertainty is demonstrated as Figure 5 and Table 1.
Walker-walk
---dreamer
CLION
Cartpole-Swingup
Hopper-stand
le6
0.5
Steps
Finger-spin
0.00	0.25	0.50 0.75
Steps le6
Cheetah-run
O O
O 5
5 2
u」n」ΦCT2ω><
0.00	0.25	0.50	0.75
Steps le6
u」ns」ΦCT2ω><
0.00	0.25	0.50	0.75
Steps le6
Figure 5:	CLION exceeds at visual control tasks that testing with environmental uncertainty.
We also test the robustness of Drq. Since Drq’s input is a stack of three sequential images (3
continuous images), we implement 2 different experiments: 1) random rotate the 3 images by the
same angle and 2) randomly rotate the 3 images by different angles. All the rotate angles are sampled
from (-5,5) degree. The results show that under the first type of uncertainty Drq almost failed, and
under the second type of uncertainty,as illustrated in , Drq’s performance decreased significantly as
shown in Figure 6.
Entt∙lc,62c,><
Walker-Walk
1000
0.0	0.2	0.4	0.6	0.8	1.0
Steps	le6
Cartpole-Swingup
800
Ooo
Ooo
6 4 2
En∙l62><
0.0	0.2	0.4	0.6	0.8	1.0
Steps	le6
Entt∙lc,62c,><
Walker-Run
IOOO
0.0	0.2	0.4	0.6	0.8	1.0
Steps	le6
Figure 6:	Drq’s performance decreased due to the observation uncertainty
In all tasks for CLION and Dreamer, the only observations are third-person camera images of size
(64, 64, 3) pixels. All the hyperparameters are shown in Table 3 (see Appendix A.2.2).
7
Under review as a conference paper at ICLR 2021
Table 1: Performance comparison with image uncertainty
		CLION			Dreamer	
	105K	505K	105K	505K
Walker-Walk	527.1 ± 166	914.9 ± 61	372.5 ± 19-	876.3 ± 48
Walker-Run	214.6 ± 72	529.3 ± 107	116.1 ± 11	464.2 ± 65
Hopper-Stand	92.3 ± 166	752.7 ± 150	85.6 ± 52	389.7 ± 162
Cartpole-Swingup	384.5 ± 58	736.0 ± 35	303.3 ± 43	423.2 ± 66
Cheetah-Run	165.3 ± 24	631.5 ± 42	245.9 ± 132	526.3 ± 241
Finger-Spin	522.5 ± 133	685.7 ± 61	283.5 ± 138	374.9 ± 178
Table 2: Performance comparison without external image uncertainty
500k step scores	Drq	CLION	Dreamer
Walker Walk	921 ± 45	959 ± 22	897 ± 49
Walker Run	459 ± 139	536 ± 25	482 ± 68
Walker stand	942 ± 22	979 ± 28	966 ± 35
Pendulum Swingup	548 ± 287	724 ± 104	682 ± 203
Cheetah Run	660 ± 96	631 ± 103	570 ± 253
100k step scores			
Walker Walk	612 ± 164	630 ± 113	277 ± 12
Walker Run	185 ± 148	211 ± 162	161 ± 112
Walker stand	843± 251	728 ± 46	494 ± 128
Pendulum Swingup	86 ± 83	182 ± 128	43 ± 34
Cheetah Run	344 ± 67	328 ± 48	235 ± 137
In order to show the data efficiency of CLION, compared to Dreamer and Drq (Kostrikov et al. (2020),
we test the performance of CLION in DMControl environments without image uncertainty with 10
random seeds. As illustrated in Table 2, CLION still outperforms Drq and Dreamer in these tasks.
Overall, the experimental results suggest that CLION outperforms Dreamer and Drq in terms of data-
efficiency, robustness to uncertainty, and final performance, which proves that our proposed memory
augmentation method can facilitate the agent to learn better behavior with limited experiences.
4.2 Ablation Experiments
We implemented three groups of ablation experiments which are tested in 3 DMContol tasks with
observation uncertainty: a) two constraints for RSSM with latent trajectory augmentation only. As
Waiker-Waik
IOOO
Oooo
Ooo
6 4 2
---Qnfy adding constraint
Only adding EVA
---Augmentation w/o EVA
——CUON
Dou bled-d reamer
0	1	2	3	4	5
Steps	P
Walker-Run
0	1	2	3	4	5
Steps	P
0	1	2	3	4	5
Steps	P
Figure 7:	Contribution of each component in CLION to the performance improvement
illustrated in Figure 7, the CLION performs better than Dreamer but are insufficient compared with
CLION’s advantages. For instance, at the 100k of Walker-walk, its performance is 26.6% better than
Dreamer, but CLION is 41.6% better than Dreamer; b) trajectory evaluator only. The experimental
results show that its performance is basically similar to Dreamer; c) memory augmentation with
the constraints for RSSM but without the trajectory evaluator. Although it performs better than
the Dreamer in Walker-Walk, it has lower performance in Hopper-Stand due to lack of reliable
guarantee for the augmented data. We also compared CLION with the method that utilizes two times
8
Under review as a conference paper at ICLR 2021
batch size in Dreamer (Doubled-Dreamer), as shown in Figure 7, the ClION also outperforms the
Doubled-Dreamer. Overall, the latent trajectory augmentation with constraints, and the trajectory
evaluator are critical components for CLION, and they complement each other.
5	Related works
Model-based policy optimization: Prior works can be divided into the following categories, includ-
ing Dyna-like algorithms (Sutton et al. (2012)), value expansion, adaptive programming (Bertsekas
et al. (1995)), and sampling-based planning. Dyna-like algorithms alternate between model learning
from environmental interaction, data generation, and policy improvement by model-free methods,
e.g., ME-TRPO (Kurutach et al. (2018)). Value expansion algorithm utilizes the dynamic model
to improve the estimation accuracy of the cumulative return (Feinberg et al. (2018)). The adaptive
dynamic programming is built upon the use of the analytic gradient of state value backpropagation
through the dynamic transition. Typical algorithms in this category include Dreamer (Hafner et al.
(2019a)), PILCO (Deisenroth & Rasmussen (2011)), and SVG (Heess et al. (2015)). The idea of
sampling-based planning is to choose the best action by a large number of samples, and regard it
as the objective for policy network. Representative algorithms include the cross-entropy method
(Bekker (2012)) in continuous space, which is used in PlaNet (Hafner et al. (2018)) and PETS (Chua
et al. (2018)), and MCTS (Silver et al. (2017)) in discrete space. To our best knowledge, the Dreamer
algorithm (Hafner et al. (2019a)) is the most successful model-based algorithm for image tasks.
Latent world model: In MBRL for high dimension input tasks, the latent world model offers a
flexible way to represent key information of the observations with lower dimension and more accurate
forward prediction. World Models (Ha and Schmidhuber, 2018) learn latent dynamics in a two-stage
process: representation learning and latent dynamic learning. However, it lacks the coordination of
the two processes. PlaNet (Hafner et al. (2019b)) proposes a recurrent stochastic state model (RSSM),
which learns them jointly and learn the optimal policy by latent online planning. Dreamer utilizes the
RSSM to learn behavior by long-term imagination. Dreaming (Okada & Taniguchi (2020)) develops
a non-reconstruction based latent model to solve target vanishing problems. To our best knowledge,
the latent recurrent stochastic state model (RSSM) used in Planet and Dreamer is the most popular
method to represent the image state dynamics (Hafner et al. (2019b)).
Data augmentation for reinforcement learning: Data augmentation has been investigated in the
context of RL to improve generalization and data efficiency. There are many prior works that suggest
the potential conclusion that data augmentation is helpful for data-efficient reinforcement learning
from image. CURL (Laskin et al. (2020b)) utilizes data augmentations for learning representations
in the RL setting which minimize the contrastive loss between an image and its augmented version
using the MoCo (He et al. (2020)) mechanism. RAD (Laskin et al. (2020a)) attempts to directly
train the RL objective on multiple augmented views of the observations without any auxiliary loss,
thereby ensuring consistencies on the augmented views implicitly. Both CURL and RAD aim to get
the representation of the input image to avoid overfitting. Cocurrent but independent to them, Drq
(Kostrikov et al. (2020)) aims to solve the distribution mismatch problem in off-policy RL, which
utilized random cropping and regularized Q-functions in conjunction.
6	Conclusion
We present a robust memory augmentation method with constrained latent imagination (CLION),
which aims to improve the policy robustness by effectively enlarge the model’s latent space with
the enhancement of agent’s memory diversity. For this, we propose a constrained RSSM framework
to embed the experiential knowledge into the latent world model with memory robustness. CLION
optimizes a parametric policy with the augmented trajectories by propagating analytic gradients of
multi-step values back through learned latent dynamics. CLION exceeds previous methods in data-
efficiency, robustness to uncertainty, and final performance on a variety of challenging continuous
control tasks with image inputs. The combination with advanced contrastive representation learning
proposed by RAD, CURL, and dreaming for more extensive applications will be investigated in
future.
9
Under review as a conference paper at ICLR 2021
References
James Bekker. Applying the cross-entropy method in multi-objective optimisation of dynamic
stochastic systems. PhD thesis, Stellenbosch: Stellenbosch University, 2012.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic
programming and optimal control, volume 1. Athena scientific Belmont, MA, 1995.
Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement
learning in a handful of trials using probabilistic dynamics models. In Proceedings of the 32nd
International Conference on Neural information Processing Systems, pp. 4759-4770, 2018.
Marc Deisenroth and Carl E Rasmussen. Pilco: A model-based and data-efficient approach to policy
search. In Proceedings of the 28th International Conference on machine learning (ICML-11), pp.
465-472, 2011.
Vladimir Feinberg, Alvin Wan, Ion Stoica, Michael I Jordan, Joseph E Gonzalez, and Sergey Levine.
Model-based value estimation for efficient model-free reinforcement learning. arXiv preprint
arXiv:1803.00101, 2018.
Brigitta Gundersen. Forming artificial memories during sleep. Nature neuroscience, 18(4):483-483,
2015.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maxi-
mum entropy deep reinforcement learning with a stochastic actor. arXiv preprint arXiv:1801.01290,
2018.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. arXiv preprint arXiv:1811.04551,
2018.
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning
behaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019a.
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha, Honglak Lee, and James
Davidson. Learning latent dynamics for planning from pixels. In International Conference on
Machine Learning, pp. 2555-2565, 2019b.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 9729-9738, 2020.
Nicolas Heess, Gregory Wayne, David Silver, Timothy Lillicrap, Tom Erez, and Yuval Tassa. Learning
continuous control policies by stochastic value gradients. In Advances in Neural Information
Processing Systems, pp. 2944-2952, 2015.
Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in neural
information processing systems, pp. 4565-4573, 2016.
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Model-based
policy optimization. In Advances in Neural Information Processing Systems, pp. 12519-12530,
2019.
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning, 37(2):183-233, 1999.
Ilya Kostrikov, Denis Yarats, and Rob Fergus. Image augmentation is all you need: Regularizing
deep reinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.
Thanard Kurutach, Ignasi Clavera, Yan Duan, Aviv Tamar, and Pieter Abbeel. Model-ensemble
trust-region policy optimization. arXiv preprint arXiv:1802.10592, 2018.
Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas.
Reinforcement learning with augmented data. arXiv preprint arXiv:2004.14990, 2020a.
10
Under review as a conference paper at ICLR 2021
Michael Laskin, Aravind Srinivas, and Pieter Abbeel. Curl: Contrastive unsupervised representations
for reinforcement learning. arXiv preprint arXiv:2003.06417, 2020b.
Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv
preprint arXiv:1509.02971, 2015.
Masashi Okada and Tadahiro Taniguchi. Dreaming: Model-based reinforcement learning by latent
imagination without reconstruction. arXiv preprint arXiv:2007.14535, 2020.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without
human knowledge. nature, 550(7676):354-359, 2017.
Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn. Universal planning
networks. arXiv preprint arXiv:1804.00645, 2018.
Richard S Sutton, Csaba Szepesvari, Alborz Geramifard, and Michael P Bowling. Dyna-style planning
with linear function approximation and prioritized sweeping. arXiv preprint arXiv:1206.3285,
2012.
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de Las Casas, David Budden,
Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et al. Deepmind control suite. arXiv preprint
arXiv:1801.00690, 2018.
Faraz Torabi, Garrett Warnell, and Peter Stone. Generative adversarial imitation from observation.
arXiv preprint arXiv:1807.06158, 2018.
A Appendix
A. 1 Proof for the returns estimation error upper bound
Lemma A.1.1 (Returns bound, branched rollout (Janner et al. (2019))). Assume we run a branched
rollout of length Before the branch ("pre" branch),w e assume that the dynamics distributions
are bounded as maxt Es〜Pt(S)DKL (Ppre (s0, a | S) Ilppre (s0, a | S)) ≤ Eme and after the branch as
maxt Es〜Pt(S)DKL (PpoSt (s0, a | S) IlppoSt (s0, a | S)) ≤ EmSt. Likewise, the policy divergence is
bounded pre- and post- branch by Epπre and Epπost , repsectively. Then the K-step returns are bounded
as:
k+1	k	k	1
∣ηi - 磔 |	≤ 25x ɪ-2	(Epre	+ Enre) + -(Epost	+ Enost)	+ -^6∏re	+ IEnost
(1 - γ)2	1	- γ	1 - γ 1	- γ
(15)
Next we prove the Theorem 3.1 metioned in Section 4.3 by using Lemma A.1.1. Assume that the
reliable weight w(S) is bounded by
P (S0 | s,a)
Pθ (s0 | S, a)
≤ Ew
(16)
Thus, the total variance distance between the pθ (S0 | S, a) andp (S0 | S, a) can be derived as
DTV (P (S0 | S,a) IIp(S0 | s, a))
/
(s,a,s0)
Ip (S0 I S, a) - w(S)pθ (S0 I S, a)I
(17)
11
Under review as a conference paper at ICLR 2021
when p (s0 | s, a) - w(s)pθ (s0 | s, a) > 0, we have
(s,
=Z(s,
≤Z
(s,
=Z
(s,
a,s0
a,s0
p (s0 |
)
p (s0 |
)
p (s0 |
a,s0)
s, a)
s, a)
s, a)
- w(s)pθ (s0 | s, a)
- w(s)pθ (s0 | s, a)
p (s0 | s, a)	0
Pθ (s0 | s,a) - ew) pθ (S 1 s,a)
(18)
wPθ (s0 | s, a) = w
a,s0)
—
when P (s0 | s, a) - w(s)Pθ (s0 | s, a) ≤ 0, we have
,a,s0
P (s0 | s, a) - w(s)Pθ (s0 | s, a)
)
w(s)Pθ (s0 | s, a) - P (s0 | s, a)
)
(19)
≤
(s
Z(s
,a,s0
P (s0 | s, a)	0	0
Cw + P (s0 | S a) I P (S | s,a) — P (S | s,a) = Cw
Therefore the m could be bounded by w . Since the m could be bounded by
w.
The choice of reference quantity is a branched rollout which executes the old policy πD under the
true dynamics until the branch point, then executes the new policy π under the true dynamics for k
steps. We denote the returns under this scheme as ηπD,π. We can split the returns as follows:
η[π] - ηbranch
η[π] - ηπD,π + ηπD,π
'-----{-----} S-----
L1
- ηbranch
-- /
{z^^^^^^^
L2
(20)
We can bound both terms L1 and L2 using Lemma A.1.1. L1 accounts for the error from executing
the old policy instead of the current policy. This term only suffers from error before the branch begins,
and we can use Lemma A.1.1 Cpπre ≤ Cπ and all other errors set to 0. This implies:
γk+1	γk
∣η[∏] — ηπD,π | ≤ 2rmaχ  -GCn +  -Cn	(21)
(1 - γ)2	1 - γ
L2 incorporates model error under the new policy incurred after the branch. Again we use Lemma
A.1.1, setting Cpmost ≤ Cm and all other errors set to 0. This implies:
kk
∣η[∏] — η D, | ≤ 2rmaχ 1 - Y CmJ ≤ 2rmaχ 1 - Y Cwo	(22)
Adding L1 and L2 together, the returns estimation error upper bound can be derived as
η[π] ≥ ηbranch [π] — 2rmax
^ Yk + 1Cn	YkCn	k
- Dɪ + D + L
(23)
Where Cw = Cw + Cn ddw-.
A.2 Algorithm flowchart and HYPER PARAMETERS
A.2. 1 Algorithm flowchart
The CLION frameWork is implemented as Algorithm 1, Which mainly contains 4 key processes:
(1) learning the constrained recurrent stochastic state model, (2) memory augmentation and latent
imagination, (3) behavior learning, and (4) environment interaction.
12
Under review as a conference paper at ICLR 2021
Algorithm 1: CLION (Robust memory augmentation by constrained latent imagination)
Initialize dataset D with S random seed episodes.
Initialize neural network parameters θ,夕 and ψ randomly.
while not convergence do
for update step c = 1..C do
// Learning the constrained RSSM
Draw B data sequences {(at, ot, rt)}k+L 〜D;
Compute model state St 〜pθ (St | st-ι, at-ι,ot);
Update θ by minθ {Jm + βι∣∣et - etl∣2 + β2||0t - 0t∣∣2}
// Memory augmentation and latent imagination
Sample start point of the augment trajectories
hAug 〜N(hτ, σ2),	SAug 〜N(mτ, σ22)
Imagine trajectories on both original and augmented data {(Sτ, aτ)}tτ+=Ht from each St
Compute the reweighting score W(Sτ) = EVA(Sτ, aτ, Sτ+1) for every imagined
trajectory and normalize the score in the mini batch
Predict rewards E (qθ (rτ | Sτ)) and values vψ (Sτ)
Compute value estimates Vλ (Sτ)
// Behavior learning
Update φ — φ + αVφ PTtH W(ST)Vλ (ST)
L Update Ψ 一 Ψ - αVψ PT=H w⅛τ) ∣∣vψ (ST) - Vλ &)k2
/ / Environment interaction
01 J env.reset ()
for time step t = 1..T do
Compute St 〜pθ (St | St-ι,at-ι, ot) from history.
Compute at 〜qφ (at | St) With the action model.
Add exploration noise to action.
_ rt, 0t+1 - env.step (at)
Add experience to dataset D J D ∪
(ot,at,rt)tT=1 .
Table 3: Hyper parameters table
Name	Symbol	Value
Collect interval	C	100
Interact interval	T	1000
Batch size	B	50
Sequence length	L	15
Imagination horizon	H	50
Learning rate for model	α0	6e - 4
Learning rate for critic	α1	8e - 5
Learning rate for actor	α2	8e - 5
Penalty coefficient	β1	0.01
Penalty coefficient	β2	0.5
13
Under review as a conference paper at ICLR 2021
A.2.2 HYPER PARAMETERS
The main parameters of the CLION algorithm are listed in Table 3. Here, we discuss some tricks
used in the implement. We scale down gradient norms that exceed 100 and clip the KL-divergence in
equation 1 below 3 free nats as in Dreamer and PlaNet. We compute the vλ targets with γ = 0.99
and λ = 0.95. The dataset is initialized with S = 5 episodes collected using random actions. Similar
to Dreamer, we fix the action repeat to 2 for all environments.
14