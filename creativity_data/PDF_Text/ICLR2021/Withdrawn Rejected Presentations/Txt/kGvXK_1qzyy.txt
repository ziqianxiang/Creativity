Under review as a conference paper at ICLR 2021
Drift Detection in Episodic Data:
Detect When Your Agent Starts Faltering
Anonymous authors
Paper under double-blind review
Ab stract
Detection of deterioration of agent performance in dynamic environments is chal-
lenging due to the non-i.i.d nature of the observed performance. We consider an
episodic framework, where the objective is to detect when an agent begins to falter.
We devise a hypothesis testing procedure for non-i.i.d rewards, which is optimal
under certain conditions. To apply the procedure sequentially in an online man-
ner, we also suggest a novel Bootstrap mechanism for False Alarm Rate control
(BFAR). We demonstrate our procedure in problems where the rewards are not
independent, nor identically-distributed, nor normally-distributed. The statistical
power of the new testing procedure is shown to outperform alternative tests - often
by orders of magnitude - for a variety of environment modifications (which cause
deterioration in agent performance). Our detection method is entirely external to
the agent, and in particular does not require model-based learning. Furthermore,
it can be applied to detect changes or drifts in any episodic signal.
1	Introduction
Reinforcement learning (RL) algorithms have recently demonstrated impressive success in a variety
of sequential decision-making problems (Badia et al., 2020; Hessel et al., 2018). While most RL
works focus on the maximization of rewards under various conditions, a key issue in real-world RL
tasks is the safety and reliability of the system (Dulac-Arnold et al., 2019; Chan et al., 2020), arising
in both offline and online settings.
In offline settings, comparing the agent performance in different environments is important for
generalization (e.g., in sim-to-real and transfer learning). The comparison may indicate the difficulty
of the problem or help to select the right learning algorithms. Uncertainty estimation, which could
help to address this challenge, is currently considered a hard problem in RL, in particular for model-
free methods (Yu et al., 2020).
In online settings, where a fixed, already-trained agent runs continuously, its performance may be
affected (gradually or abruptly) by changes in the controlled system or its surroundings, or when
reaching new states beyond the ones explored during the training. Some works address the robust-
ness of the agent to such changes (Lecarpentier & Rachelson, 2019; Lee et al., 2020). However,
noticing the changes may be equally important, as it allows us to fall back into manual control, send
the agent to re-train, guide diagnosis, or even bring the agent to halt. This is particularly critical in
real-world problems such as health care and autonomous driving (Zhao et al., 2019), where agents
are required to be fixed and stable: interventions in the policy are often limited or forbidden (Mat-
sushima et al., 2020), but any performance degradation should be detected as soon as possible.
Many sequential statistical tests exist for detection of mean degradation in a random process. How-
ever, common methods (Page, 1954; Lan, 1994; Harel et al., 2014) assume independent and iden-
tically distributed (i.i.d) samples, while in RL the feedback from the environment is usually both
highly correlated over consecutive time-steps, and varies over the life-time of the task (Korenkevych
et al., 2019). This is demonstrated in Fig. 1.
A possible solution is to apply statistical tests to large blocks of time-steps assumed to be i.i.d (Dit-
zler et al., 2015). Since many RL applications consist of repeating episodes, such a blocks-partition
can be applied in a natural way (Colas et al., 2019). However, this approach requires complete
episodes for change detection, while a faster response is often required. Furthermore, naively ap-
1
Under review as a conference paper at ICLR 2021
plying a statistical test on the accumulated feedback (e.g., sum of rewards) from complete episodes,
ignores the dependencies within the episodes and may miss vital information, leading to highly
sub-optimal tests.
In this work, we devise an optimal test for detection of degradation of the rewards in an episodic RL
task (or in any other episodic signal), based on the covariance structure within the episodes. Even in
absence of the assumptions that guarantee its optimality, the test is still asymptotically superior to
the common approach of comparing the mean (Colas et al., 2019). The test can detect changes and
drifts in both the offline and the online settings defined above. In addition, for the online settings, we
suggest a novel Bootstrap mechanism to control the False Alarm Rate (BFAR) through adjustment
of test thresholds in sequential tests of episodic signals. The suggested procedures rely on the ability
to estimate the correlations within the episodes, e.g., through a ”reference dataset” of episodes.
Since the test is applied directly to the rewards, it is model-free in the following senses: the underly-
ing process is not assumed to be known, to be Markov, or to be observable at all (as opposed to other
works, e.g., Banerjee et al. (2016)), and we require no knowledge about the process or the running
policy. Furthermore, as the rewards are simply referred to as episodic time-series, the test can be
similarly applied to detect changes in any episodic signal.
We demonstrate the new procedures in the environments of Pendulum (OpenAI), HalfCheetah and
Humanoid (MuJoCo; Todorov et al., 2012). BFAR is shown to successfully control the false alarm
rate. The covariance-based degradation-test detects degradation faster and more often than three
alternative tests - in certain cases by orders of magnitude.
Section 3 formulates the offline setup (individual tests) and the online setup (sequential tests). Sec-
tion 4 introduces the model of an episodic signal, and derives an optimal test for degradation in such
a signal. Section 5 shows how to adjust the test for online settings and control the false alarm rate.
Section 6 describes the experiments, Section 7 discusses related works and Section 8 summarizes.
To the best of our knowledge, we are the first to exploit the covariance between rewards in post-
training phase to test for changes in RL-based systems. The contributions of this paper are (i) a
new framework for model-free statistical tests on episodic (non-i.i.d) data with trusted reference-
episodes; (ii) an optimal test (under certain conditions) for degradation in episodic data; and (iii) a
novel bootstrap mechanism that controls the false alarm rate of sequential tests on episodic data.
2	Preliminaries
Reinforcement learning and episodic framework: A Reinforcement Learning (RL) problem is
usually modeled as a decision process, where a learning agent has to repeatedly make decisions
that affect its future states and rewards. The process is often organized as a finite sequence of time-
steps (an episode) that repeats multiple times in different variants, e.g., with different initial states.
Common examples are board and video games (Brockman et al., 2016), as well as more realistic
problems such as repeating drives in autonomous driving tasks.
Once the agent is fixed (which is the case in the scope of this work), the rewards of the decision
process essentially reduce to a (decision-free) random process {Xt}tn=1, which can be defined by its
PDF (f{Xt }n : Rn → [0, ∞)). {Xt} usually depend on each other: even in the popular Markov
Decision Process (Bellman, 1957), where the dependence goes only a single step back, long-term
correlations may still carry information if the states are not observable by the agent.
Hypothesis tests: Consider a parametric probability function P(X∣θ) describing a random Pro-
cess, and consider two different hypotheses H0 , HA determining the value (simple hypothesis) or
allowed values (complex hypothesis) of θ. When designing a test to decide between the hypotheses,
the basic metrics for the test efficacy are its significance P (not reject H0|H0) = 1 - α and its power
P (reject H0|HA) = β. A statistical hypothesis test with significance 1 - α and power β is said to
be optimal if any test With as high significance 1 - α ≥ 1 - α has smaller power β ≤ β.
The likelihood of the hypothesis H : θ ∈ Θ given data X is defined as L(H|X) = supθ∈θp(X∣θ).
According to Neyman-Pearson Lemma (Neyman et al., 1933), a threshold-test on the likelihood ratio
LR(H0, HA|X) = L(H0|X)/L(HA|X) is optimal. In a threshold-test, the threshold is uniquely
determined by the desired significance level α, though is often difficult to calculate given α.
2
Under review as a conference paper at ICLR 2021
In many practical applications, a hypothesis test is repeatedly applied as the data change or grow, a
procedure known as a sequential test. If the null hypothesis H0 is true, and any individual hypothesis
test falsely rejects H0 with some probability α, then the probability that at least one of the multiple
tests will reject H0 is α0 > α, termed family-wise type-I error (or false alarm rate when associated
with frequency). See Appendix K for more details about hypothesis testing and sequential tests in
particular.
Common approaches for sequential tests, such as CUSUM (Page, 1954; Ryan, 2011) and α-spending
functions (Lan, 1994; Pocock, 1977), usually require strong assumptions such as independence or
normality, as further discussed in Appendix F.
3	Problem Setup
In this work, We consider two setups where detecting performance deterioration is important - Se-
quential degradation-tests and individual degradation-tests. The individual tests, in addition to their
importance in (offline) settings such as sim-to-real and transfer learning, are used in this work as
building-blocks for the (online) sequential tests.
Both setups assume a fixed agent that was previously trained, and aim to detect whenever the agent
performance begins to deteriorate, e.g., due to environment changes. The ability to notice such
changes is essential in many real-world problems, as explained in Section 1.
Setup 1 (Individual degradation-test). We consider a fixed trained agent (policy must be fixed
but is not necessarily optimal), whose rewards in an episodic environment (with episodes of length
T ) were previously recorded for multiple episodes (the reference dataset). The agent runs in a new
environment for n time-steps (both n < T and n ≥ T are valid). The goal is to decide whether
the rewards in the new environment are smaller than the original environment or not. If the new
environment is identical, the probability of a false alarm must not exceed α.
Setup 2 (Sequential degradation-test). As in Setup 1, we consider a fixed trained agent with
recorded reference data of multiple episodes. This time the agent keeps running in the same envi-
ronment, and at a certain point in time its rewards begin to deteriorate, e.g., due to changes in the
environment. The goal is to alert to the degradation as soon as possible. As long as the environment
has not changed, the probability of a false alarm must not exceed α0 during a run of h episodes.
Note that while in this work the setups focus on degradation, they can be easily modified to look for
any change (as positive changes may also indicate the need for further training, for example).
4	Optimization of Individual Degradation-Tests
To tackle the problem of Setup 1, we first define the properties of an episodic signal and the general
assumptions regarding its degradation.
Definition 4.1 (T -long episodic signal). Let n, T ∈ N, and write n = KT + τ0 (for non-negative
integers K, τ0 with τ0 ≤ T). A sequence of real-valued random variables {Xt}tn=1 is a T -long
episodic signal, if its joint probability density function can be written as
K-1
f{Xt}n=ι (x1, ...,xn) = ɪɪ f{Xt}T=ι (XkT +1,…，xkT+T) ∙ f{Xt}：=i (XKT +1,…，xKT+τO ) ⑴
k=0
(where an empty product is defined as 1). Wefurther denote μo := E[(Xι,…，Xt )>] ∈ RT, ∑0 :=
Cov((X1,...,XT)>,(X1,...,XT)) ∈ RT×T.
Note that the episodic signal consists of i.i.d episodes, but is not assumed to be independent or
identically-distributed within the episodes. For simplicity we focus on one-dimensional episodic
signals, although a generalization to multidimensional signals is straight-forward (see Appendix G).
In the analysis below we assume that both μo and ∑0 are known. In practice, this can be achieved
either through detailed domain knowledge, or by estimation from the recorded reference dataset of
Setup 1, assuming it satisfies Eq.(1). The estimation errors decrease as O(1/√N) with the number
N of reference episodes, and are distributed according to the Central Limit Theorem (for means)
3
Under review as a conference paper at ICLR 2021
Figure 1: Parameters of an episodic signal of the rewards in HalfCheetah environment, estimated over N =
10000 episodes of T = 1000 time-steps: (a) distribution of rewards per time-step; (b) variance per time-step;
(c) correlation(t1, t2) vs. t2 - t1. The estimations were done in resolution of 25 time-steps, i.e., every episode
was split into 40 intervals of 25 consecutive rewards, and each sample is the average over an interval.
[HalfCheetah] Reference episodes
and Wishart distribution (K. V. Mardia & Bibby, 1979) (for covariance). While in this work we
use up to N = 10000 reference episodes, Appendix E shows that N = 300 reference episodes are
sufficient for reasonable results in HalfCheetah, for example. Note that correlations estimation has
been already discussed in several other RL works (Alt et al., 2019).
Fig. 1 demonstrates the estimation of mean and covariance parameters for a trained agent in the
environment of HalfCheetah, from a reference dataset of N = 10000 episodes. This also demon-
strates the non-trivial correlations structure in the environment. According to Fig. 1b, the variance
in the rewards varies and does not seem to reach stationarity within the scope of an episode. Fig. 1c
shows the autocorrelation function ACF (t2 - t1) = corr(t1, t2) for different reference times t1. It
is evident that the correlations last for hundreds of time-steps, and depend on the time t1 rather than
merely on the time-difference t2 - t1. This means that the autocorrelation function is not expressive
enough for the actual correlations structure.
Once the per-episode parameters μo ∈ RT, ∑0 ∈ RT×T are known, the expectations and covariance
matrix of the whole signal μ ∈ Rn, Σ ∈ Rn×n can be derived directly: μ consists of periodic
repetitions of μo, and Σ consists of copies of ∑0 as T X T blocks along its diagonal. For both
parameters, the last repetition is cropped if n is not an integer multiplication of T. In other words,
by taking advantage of the episodic setup, we can treat the temporal univariate non-i.i.d signal as a
multivariate signal with easily-measured mean and covariance - even if the signal is measured in the
middle of an episode.
The degradation in the signal X = {Xt}tn=1 is defined through the difference between two hypothe-
ses. The null hypothesis H states that X is a T-long episodic signal with expectations μo ∈ RT
and invertible covariance matrix ∑0 ∈ RT×t. Our first alternative hypothesis - uniform degradation
一 states that X is a T-long episodic signal with the same covariance ∑0 but smaller expectations:
∃e ≥ €0, ∀1 ≤ t ≤ T : (μ)t = (μo)t 一 e. Note that this hypothesis is complex (e ≥ e0), where
0 can be tuned according to the minimal degradation magnitude of interest. In fact, Theorem 4.1
shows that the optimal corresponding test is independent of the choice of e0.
Theorem 4.1 (Optimal test for uniform degradation). Define the uniform-degradation weighted-
mean Sunif(X) := W ∙ X, where W := 1> ∙ Σ-1 ∈ Rn (and 1 is the all-1 vector). Ifthe distribution
ofX is multivariate normal, then a threshold-test on sunif is optimal.
Proof Sketch. According to Neyman-Pearson Lemma (Neyman et al., 1933), a threshold-test on the
likelihood-ratio (LR) between H0 and HA is optimal. Since HA is complex, the LR is a minimum
over e ∈ [e0, ∞). Lemma 1 shows that ∃s0 : sunif ≥ s0 ⇒ e = e0 and sunif ≤ s0 ⇒ e = e(sunif).
The rest of the proof in Appendix J substitutes e in both domains of sunif to prove monotony of the
LR in Sunif, from which we can conclude monotony in Sunif over all R.	□
Following Theorem 4.1, we define the Uniform Degradation Test (UDT) to be a threshold-test on
sunif, i.e., ”declare a degradation if Sunif < K” for a pre-defined κ.
4
Under review as a conference paper at ICLR 2021
Recall that optimality of a test is defined in Section 2 as having maximal power given significance
level. To achieve the significance α required in Setup 1, we apply a bootstrap mechanism that
randomly samples episodes from the reference dataset and calculates the corresponding statistic
(e.g., sunif). This yields a bootstrap-estimate of the distribution of the statistic under H0, and the
α-quantile of the estimated distribution is chosen as the test-threshold (κ = qα(sunif |H0)).
Note that Theorem 4.1 relies on multivariate normality assumption, which is often too strong for
real-world applications. Theorem 4.2 guarantees that if we remove the normality assumption, it
is still beneficial to look into the episodes instead of considering them as atomic blocks; that is,
UDT is still asymptotically better than a test on the simple mean ssimp = Ptn=1 Xt /n. Note that
”asymptotic” refers to the signal length n → ∞ (while T remains constant), and is translated in the
sequential setup into a ”very long lookback-horizon h” (rather than very long running time).
Theorem 4.2 (Asymptotic power of UDT). Denote the length of the signal n = K ∙ T, assume a
uniform degradation of size √K, and let two threshold-tests Tsimp on Ssimp and UDT on Sunif be
tuned to have significance α. Then
limK→∞P (Tsimp rejects H0 HA = Φ qα0 +
p⅛)
≤ Φ (qα + e Jl>∑-1l) = limk→∞P (UDTrejects %∣Ha)
(2)
where Φ is the CDF of the standard normal distribution, and qα0 is its α-quantile.
Proof Sketch. Since the episodes of the signal are i.i.d, both Ssimp and Sunif are asymptotically
normal according to the Central Limit Theorem. The means and variances of both statistics are
calculated in Lemma 2. Calculation of the variance of Sunif relies on writing Sunif as a sum of
linear transformations of X (Sunif = Pin=1(∑-1)iX), and using the relation between ∑ and ∑0.
Appendix J shows that the inequality between the resulted powers is equivalent to a matrix-form of
the means-inequality, and proves it by applying CaUchy-SchWarz inequality to ∑-"21 and ∑1∕21.
□
Motivated by Theorem 4.2, we define G2 := (1 ς0 T(1 .01) to be the asymptotic power gain of
UDT, quantify it, and show that it increases with the heterogeneity of the spectrum of ∑0.
Proposition 4.1 (Asymptotic power gain). G2 = 1 + PiT,j=1 wij(λi - λj)2, where {λi}iT=1 are the
eigenvalues of ∑0 and {wij }iT,j=1 are positive weights.
Proof Sketch. The result can be calculated after diagonalization of ∑0, and the weights {wij } cor-
respond to the diagonalizing matrix. See Appendix J for more details.	□
əuuəjəj①」— P-IPMΦ,J UpOlU
Figure 2: Rewards degradation of a fixed
agent in HalfCheetah following changes in
gravity, mass, and control-cost, over N =
5000 episodes per scenario.
So far we assumed a uniform degradation. In the con-
text of RL, such a model may refer to changes in constant
costs or action costs, as well as certain environment dy-
namics whose change influences various states in a sim-
ilar way. Fig. 2 demonstrates the empiric degradation in
the rewards of a fixed agent in HalfCheetah, following
changes in gravity, mass and control-cost. It seems that
some modifications indeed cause a quite uniform degra-
dation, while in others the degradation is mostly restricted
to certain ranges of time.
To model effects that are less uniform in time we suggest
a partial degradation hypothesis, where some (unknown)
entries of μo are reduced by e > 0, and others do not
change. The number m = P ∙ T of the reduced entries is
defined by a parameter p ∈ (0, 1).
5
Under review as a conference paper at ICLR 2021
This time, calculation of the optimal test-statistic through the LR yields a minimum over mT pos-
sible subsets of decreased entries, which is computationally heavy. However, Theorem 4.3 shows
that if we optimize for small values of (where optimality is indeed most valuable), a near-optimal
statistic is spart, which is the sum of the m = P ∙ T smallest time-steps of (X - μ) after a Σ-1-
transformation (see formal definition in Definition I.11). We define the Partial Degradation Test
(PDT) to be a threshold-test on spart with a parameter p.
Theorem 4.3 (Near-optimal test for uniform degradation). Assume that X is multivariate normal,
and let Pα be the maximal power of a hypothesis test with significance 1 - α. The power of a
threshold-test on spart with significance 1 - α is Pα - O().
Proof Sketch. The expression that is minimized is a sum of two terms. One term is the sum of a
subset of entries of Σ-1(X - μ), which is minimized by simply taking the lowest entries (UP to
the constraint of consistency across episodes, which requires us to sum the rewards per time-step in
advance). In Appendix J we bound the second term and its effects on the modified statistic and on
the modified test-threshold. We show that the resulted decrease of rejection probability is O(e). □
5	B ootstrap for False Alarm Rate Control (BFAR)
For Setup 2, we suggest a sequential testing procedure: run an individual degradation-test every
d steps (i.e., F = T/d test-points per episode), and return once any individual test declares a
degradation. The tests can run according to Section 4, applied on the h recent episodes. Multiple
tests may be applied every test-point, e.g., with varying test-statistics {s} or lookback-horizons {h}.
This procedure, as implemented for the experiments of Section 6, is described in Fig. 3.
Setup 2 limits the probability ofa false alarm to α0 in a run of h episodes. To satisfy this condition,
we set a uniform threshold κ on the p-values of the individual tests (i.e., declare once a test returns
p-val < κ). The threshold is determined using a Bootstrap mechanism for False Alarm control
(BFAR, Algorithm 1).
While bootstrap methods for false alarm control are quite popular, they often rely on the data samples
being i.i.d (Kharitonov et al., 2015; Abhishek & Mannor, 2017), which is crucial for the re-sampling
to reliably mimic the source of the signal. To address the non-i.i.d signal, we take advantage of the
episodic framework and sample whole episodes. We then use the re-sampled sequence to simulate
tests on sub-sequences where the first and last episodes may be incomplete, as described below.
This allows simulation of sequences of various lengths (including non-integer number of episodes)
without assuming independence, normality, or identical distributions within the episodes.
BFAR samples hmax + h episodes (where
hmax is the maximal lookback-horizon)
from reference data of N episodes, to sim-
ulate sequential data Y . Then individual
tests are simulated for any test-point along
h episodes, starting after hmax episodes.
The minimal p-value determines whether
a detection would occur in Y . The whole
procedure repeats B times, creating a boot-
strap estimate of the distribution of the min-
imal p-value along h episodes. We choose
the tests threshold to be the α0 -quantile of
this distribution, such that α0 of the boot-
strap simulations would raise a false alarm.
Note that the statistic for the tests is given
to BFAR as an input, making its choice in-
dependent of BFAR. Additional details and
time complexity are discussed in Appen-
dices H,L.
Algorithm 1: BFAR: Bootstrap for FAR control
Input: reference dataset x ∈ RN×T ; statistic
functions {s}; lookback-horizons {h1, ..., hmax};
test length h ∈ N; B ∈ N; α0 ∈ (0, 1);
Output: test threshold for individual tests;
Initialize P = (1, ..., 1) ∈ [0, 1]B;
for b in 1:B do
Initialize Y ∈ R(hmax+h)T ;
for k in 0:(hmax +h-1) do
Sample j uniformly from (1,…,N);
Y[kT + 1 : kT + T] — (xji,…,XjT);
for t in test-points do
for h in lookback-horizons and s in
statistic functions do
y — Y [t — hT : t];
p J individuaLteSt_pvalue(y vs. x; S)
P[b] J min(P[b],p);
Return quantileα0 (P);
6
Under review as a conference paper at ICLR 2021
SeqUential test for a SCenario H
RePeat for M=IOo different CIata-blocks 0f H;
AmaX episodes of a random block of H0	A block of N episodes of H
___________________A____________________ _______________________人_____________________
( Time d between	ʌ
Figure 3: A summary of the sequential degradation-test procedure described in Section 6.1.
6	Experiments
6.1	Methodology
We run experiments in standard Reinforcement Learning environments as described below. For
every environment, we use a PyTorch implementation (Kostrikov, 2018) of the standard A2C al-
gorithm (Mnih et al., 2016) to train an agent. We let the trained agent run in the environment for
N0 episodes and record its rewards, considered the trusted reference data. We then define several
scenarios, and let the agent run for M × N episodes in each scenario (divided later into M = 100
blocks of N episodes). One scenario is named H0 and is identical to the reference run (up to initial-
state randomization). The other scenarios are defined per environment, and present environmental
changes expected to harm the agent’s rewards. The agent is not trained to adapt to these changes,
and the goal is to test how long it takes for a degradation-test to detect its degradation.
Individual degradation-tests of length n (Setup 1) are applied for every scenario over the first n
time-steps of each block. Sequential degradation-tests (Setup 2) are applied sequentially on the
episodes of each block. Since the agent is assumed to run continuously as the environment changes
from H0 to an alternative scenario, each block is preceded by a random sample of H0 episodes, as
demonstrated in Fig. 3.
BFAR adjusts the tests thresholds to have a false alarm with probability α0 = 5% per h = N
episodes (where N is the data-block size). Two lookback-horizons h1 , h2 are chosen for every
environment. The rewards are downsampled by factor d before applying the tests, intended to reduce
the parameters estimation error and the running time of the tests. Table 1 summarizes the setup of
the various environments.
The experimented degradation-tests are a threshold-test on the simple Mean; CUSUM (Ryan,
2011); Hotelling (Hotelling, 1931); UDT and PDT (with p = 0.9) from Section 4; and a Mixed
Degradation Test (MDT) that runs Mean, Hotelling and PDT in parallel - applying all three in
every test-point (as permitted in Algorithm 1). Further implementation details are discussed in Ap-
pendix D.
6.2	Results
We run the tests in the environments of Pendulum (OpenAI), where the goal is to keep a one-
dimensional pendulum pointing upwards; HalfCheetah (Todorov et al., 2012), where the goal is for
Table 1: Environments parameters
(episode length (T), reference episodes (N0), test blocks (M), episodes per block (N),
sequential test length (h), lookback horizons (h1, h2), tests per episode (F = T /d))
Environment	T	No	M	N (= h)	hi, h2	F = T/d
PendUlum-v0	200	3000	^tgg^	30	3,30	20
HalfCheetah-v3	1000	10000	^tgg^	-50-	5,50	40
Humanoid	200	5000	^W^	30	3,30	10
7
Under review as a conference paper at ICLR 2021
Pendulum	HaIfCheetah	Humanoid
CCOStllO ccost3OO len090	Ienl20	nαιse05 HO ccostl30 gravιty090 gravιtyl20 ccostllO len090 lenl20
Scenario
Figure 4: Bottom: percent of sequential tests (among M = 100 runs with different seeds) that ended with
degradation detection, for various degradation-tests (corresponding to different colors), in a sample of scenarios
in Pendulum, HalfCheetah and Humanoid. Top: the distribution of time until detection - for the runs that ended
with detection. High detection rates usually go along with short detection times.
a two-dimensional cheetah to run as fast as possible; and Humanoid, where the goal is for a person
to walk without falling. In each environment we define the scenario ccostx of control cost increased
to x% of its original value, in addition to scenarios of changed dynamics as specified in Appendix D.
In all the environments the rewards are clearly not independent, identically-distributed or normally-
distributed (see Fig. 1 for example). Yet the false alarm rates are close to α0 = 5% per h episodes
in all the tests, as demonstrated in Fig. 4 for HalfCheetah, for example. These results for the H0
scenarios indicate that BFAR tunes the thresholds properly in spite of the complexity of the data.
Note that BFAR never observed the data of scenario H0 , but only the reference data.
In most of the non-H0 scenarios, our tests prove to be more powerful than the standard tests, often
by extreme margins. For example, increased control cost in all the environments and additive noise
in Pendulum are all 100%-detected by the suggested tests, usually within few episodes (Fig. 4);
whereas Mean, CUSUM and Hotelling have very poor detection rates. Mean did not detect degra-
dation in Pendulum even after the control cost increased from 110% to 300%(!).
Note that we run the tests with two lookback-horizons in parallel, as allowed by BFAR. This proves
useful: with +30% control cost in HalfCheetah, for example, the short lookback-horizon allows fast
detection of degradation; but with merely +10%, the long horizon is necessary to notice the slight
degradation over a large number of episodes. This is demonstrated in Fig. 11 in Appendix C.
The covariance-based tests reduce the weights of the highly-varying (and presumably noisier) time-
steps. In HalfCheetah they turn out to be in the later parts of the episode. As a result, in certain
scenarios, Mean (which ignores the different variances), CUSUM and Hotelling (which exploit them
only in a heuristic way) do better in individual degradation-tests of 100 samples (out of T = 1000)
than they do in one or even 10 full episodes. This does not occur in UDT and PDT. Essentially, we
see that ignoring the noise variability leads to violation of the principle that more data are better.
In Pendulum the ratio between variance of different steps may reach 5 orders of magnitude. This
phenomenon increases the potential power of the covariance-based tests. For example, when the pole
is shortened, negative changes in the highly-weighted time-steps are detected even when the mean
of the whole signal increases. This feature allows us to detect slight changes in the environment
before they develop into larger changes and cause damage.
On the other hand, a challenging situation arises when certain rewards decrease but the highly-
weighted ones slightly increase (as in longer Pendulum’s pole), which strongly violates the assump-
tions of Section 4. UDT is doomed to falter in such scenarios. PDT proves somewhat robust to this
phenomenon since it is capable of focusing on a subset of time-steps, as demonstrated in increased
gravity in HalfCheetah (see Fig. 4). However, it cannot overcome the extreme weights differences
in Pendulum. The one test that demonstrated robustness to all the experimented scenarios, including
modified Pendulum’s length and mass, is MDT. MDT combines Mean, Hotelling and PDT and does
not fall far behind any of the three, in any of the scenarios. Hence, it presents excellent results in
some scenarios and reasonable results in the others.
Detailed experiments results are available in Appendix C.
8
Under review as a conference paper at ICLR 2021
7	Related Work
Training in non-stationary environments has been widely researched, in particular in the frameworks
of MAB (Mukherjee & Maillard, 2019; Garivier & Moulines, 2011; Besbes et al., 2014; Lykouris
et al., 2020; Alatur et al., 2020; Gupta et al., 2019; Jun et al., 2018), model-based RL (Lecarpentier
& Rachelson, 2019; Lee et al., 2020) and general multi-agent environments (Hernandez-Leal et al.,
2019). Banerjee et al. (2016) explicitly detect changes in the environment and modify the policy
accordingly, but assume that the environment is Markov, fully-observable, and its transition model
is known - three assumptions that We avoid and that do not hold in many real-world problems. Safe
exploration during training in RL was addressed by Garcia & Fernandez (2015); Chow et al. (2018);
Junges et al. (2016); Cheng et al. (2019); Alshiekh (2017). Note that our work refers to changes
beyond the scope of the training phase: it addresses the stage where the agent is fixed and required
not to train further, in particular not in an online manner. Robust algorithms may prevent degradation
in the first place, but when they fail - or when their assumptions are not met - a model-free monitor
with minimal assumptions (as the one suggested in this work) is crucial.
Sequential tests were addressed by many over the years. Common approaches rely on strong as-
sumptions such as samples independence (Page, 1954; Ryan, 2011) and normality (Pocock, 1977;
O’Brien & Fleming, 1979). Generalizations exist for certain private cases (Lu & Jr., 2001; Xie &
Siegmund, 2011), sometimes at cost of alternative assumptions such as known change-size (Lund
et al., 2007). Samples independence is usually assumed also in recent works based on numeric
approaches (Kharitonov et al., 2015; Abhishek & Mannor, 2017; Harel et al., 2014), and is often
justified by consolidating many data samples (e.g., an episode) together as a single sample (Colas
et al., 2019). Ditzler et al. (2015) wrote that ”change detection is typically carried out by inspect-
ing i.i.d features extracted from the incoming data stream, e.g., the sample mean”. Certain works
address monitoring of cyclic signals (Zhou et al., 2005), but to the best of our knowledge, we are
the first to devise an optimal test for mean change in temporal non-i.i.d signals, and bootstrap-based
false alarm control for such non-i.i.d signals.
Our work can be seen in part as converting a univariate temporal episodic signal into a T -
dimensional multivariate signal (with incomplete observations in mid-episodes). Many works ad-
dressed the problem of changepoint detection in multivariate variables, e.g., using histograms com-
parison (Boracchi et al., 2018), Hotelling statistic (Hotelling, 1931), and K-L distance (Kuncheva,
2013). Hotelling in particular looks for changed mean under unchanged covariance, similarly to our
work. However, it is not derived optimally for mean change detection, and it also inherently ignores
the sign of change. Our test is optimal under similar conditions to Hotelling test, is further proved
to be robust to the normality assumption, and is shown to perform better in a variety of experiments.
We are not aware of any other work that derives an optimal test to either the uniform degradation or
the partial degradation complex hypotheses.
8	Summary
We introduce a novel approach that is optimal (under certain conditions) for detection of changes
in episodic signals, exploiting the correlations structure as measured in a reference dataset. In en-
vironments of classic control (Pendulum) and MuJoCo (HalfCheetah, Humanoid), the suggested
statistical tests detected degradation faster than alternatives, often by orders of magnitude. Certain
conditions, such as combination of positive and negative changes in very heterogeneous signals,
may cause instability in some of the suggested tests; however, this is shown to be solved by running
the new test in parallel to standard tests - with only a small loss of test power.
We also introduce BFAR, a bootstrap mechanism that adjusts tests thresholds according to the de-
sired false alarm rate in sequential tests. The mechanism empirically succeeded in providing valid
thresholds for various tests in all the environments, in spite of the non-i.i.d data.
The suggested approach may contribute to development of more reliable RL-based systems. Future
research may: consider different hypotheses, such as a permitted small degradation (instead of H0)
or a mix of degradation and improvement (instead of HA); suggest additional stabilizing mecha-
nisms for covariance-based tests; exploit other metrics than rewards for tests on model-based RL
systems; and apply comparative tests of episodic signals beyond the scope of drifts detection.
9
Under review as a conference paper at ICLR 2021
References
Vineet Abhishek and Shie Mannor. A nonparametric sequential test for online randomized exper-
iments. Proceedings of the 26th International Conference on World Wide Web Companion, pp.
610-6, 2017.
Pragnya Alatur, Kfir Y. Levy, and Andreas Krause. Multi-player bandits: The adversarial case.
JMLR, 2020.
Mohammed Alshiekh. Safe reinforcement learning via shielding. Logic in Computer Science, 2017.
Bastian Alt, Adrian Sosic, and Heinz Koeppl. Correlation priors for reinforcement learning.
NeurIPS, 2019.
Samaneh Aminikhanghahi and D. Cook. A survey of methods for time series change point detection.
Knowledge and Information Systems, 51:339-367, 2016.
Adria Puigdomenech Badia et al. Agent57: Outperforming the atari human benchmark. ICML,
2020.
Taposh Banerjee, Miao Liu, and Jonathan How. Quickest change detection approach to optimal
control in markov decision processes with model changes, 09 2016.
Richard Bellman. A markovian decision process. Indiana Univ. Math. J., 6:679-684, 1957. ISSN
0022-2518.
Donald A. Berry and Bert Fristedt. Bandit problems. Springer Netherlands, 1985. doi: 10.1007/
978-94-015-3711-7.
Omar Besbes, Yonatan Gur, and Assaf Zeevi. Stochastic multi-armed-bandit problem with non-
stationary rewards. Advances in Neural Information Processing Systems (NIPS), 27, 2014.
Giacomo Boracchi, Diego Carrera, Cristiano Cervellera, and Danilo Maccio. Quanttree: His-
tograms for change detection in multivariate data streams. Proceedings of Machine Learning
Research, 80:639-648, 10-15 Jul 2018. URL http://proceedings.mlr.press/v80/
boracchi18a.html.
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
D. Brook et al. An approach to the probability distribution of cusum run length. Biometrika, 59(3):
539-549, 1972.
Tom Bylander. Lecture notes: Reinforcement learning. http://www.cs.utsa.edu/
~bylander∕cs62 4 3∕reinfOrcement-learning.pdf.
Stephanie C.Y. Chan et al. Measuring the reliability of reinforcement learning algorithms. ICLR,
2020.
James Chen. Conditional value at risk (cvar). https://www.investopedia.com/terms/
c/conditional_value_at_risk.asp, 2020.
Richard Cheng et al. End-to-end safe reinforcement learning through barrier functions for safety-
critical continuous control tasks. AAAI Conference on Artificial Intelligence, 2019.
Yinlam Chow et al. A lyapunov-based approach to safe reinforcement learning. NIPS, 2018.
Cedric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer. A hitchhiker’s guide to statistical compar-
isons of reinforcement learning algorithms, 2019.
Bin Dai, Shilin Ding, and Grace Wahba. Multivariate bernoulli distribution. Bernoulli, 19(4):
1465-1483, 09 2013. doi: 10.3150/12-BEJSP10. URL https://doi.org/10.3150/
12-BEJSP10.
10
Under review as a conference paper at ICLR 2021
David A. Dickey and Wayne A. Fuller. Distribution of the estimators for autoregressive time se-
ries with a unit root. Journal of the American Statistical Association, 74(366a):427-431, 1979.
doi: 10.1080/01621459.1979.10482531. URL https://doi.org/10.1080/01621459.
1979.10482531.
Gregory Ditzler, Robi Polikar, and Cesare Alippi. Learning in nonstationary environments: A sur-
vey. IEEE Computational Intelligence Magazine, 2015.
Gabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement
learning, 2019.
Bradley Efron. Second thoughts on the bootstrap. Statist. Sci., 18(2):135-140, 05 2003. doi:
10.1214/ss/1063994968. URL https://doi.org/10.1214/ss/1063994968.
Ari Freedman.	Convergence theorem for finite markov chains.
https://math.uchicago.edu/ may/REU2017/REUPapers/Freedman.pdf, 2017.
Javier Garcia and Fernando Fernandez. A comprehensive survey on safe reinforcement learning.
JMLR, 2015.
AUrelien Garivier and Eric Moulines. On upper-confidence bound policies for switching bandit
problems. International Conference on Algorithmic Learning Theory, pp. 174-188, 10 2011. doi:
10.1007/978-3-642-24412-4_16.
Megan Goldman. Lecture notes in stat c141:	The bonferroni correction.
https://www.stat.berkeley.edu/ mgoldman/Section0402.pdf, 2008.
Anupam Gupta, Tomer Koren, and Kunal Talwar. Better algorithms for stochastic bandits with
adversarial corruptions. Proceedings of Machine Learning Research, 2019.
Maayan Harel, Koby Crammer, Ran El-Yaniv, and Shie Mannor. Concept drift detection through
resampling. International Conference on Machine Learning, pp. II-1009-II-1017, 2014.
Peter Henderson et al. Deep reinforcement learning that matters. AAAI, 2017.
Pablo Hernandez-Leal, Michael Kaisers, Tim Baarslag, and Enrique Munoz de Cote. A survey of
learning in multiagent environments: Dealing with non-stationarity, 2019.
Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
Horgan, Bilal Piot, Mohammad Azar, and David Silver. Rainbow: Combining improvements in
deep reinforcement learning. AAAI, 2018.
Harold Hotelling. The generalization of student’s ratio. Ann. Math. Statist., 2(3):360-378,
08 1931. doi: 10.1214/aoms/1177732979. URL https://doi.org/10.1214/aoms/
1177732979.
Mark E. Irwin. Lecture notes: Convergence in distribution and central limit theorem. http:
//www2.stat.duke.edu/~sayan/230/2017/Section53.pdf, 2006.
Kwang-Sung Jun et al. Adversarial attacks on stochastic bandits. NeurIPS, 2018.
Sebastian Junges et al. Safety-constrained reinforcement learning for mdps. International Confer-
ence on Tools and Algorithms for the Construction and Analysis of Systems, 2016.
J. T. Kent K. V. Mardia and J. M. Bibby. Multivariate analysis. Academic Press, 1979.
Eugene Kharitonov, Aleksandr Vorobev, Craig Macdonald, Pavel Serdyukov, and Iadh Ounis. Se-
quential testing for early stopping of online experiments. Proceedings of the 38th International
ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 473-482,
2015. doi: 10.1145/2766462.2767729. URL https://doi.org/10.1145/2766462.
2767729.
Dmytro Korenkevych, A. Rupam Mahmood, Gautham Vasan, and James Bergstra. Autoregressive
policies for continuous control deep reinforcement learning, 2019.
11
Under review as a conference paper at ICLR 2021
Ilya Kostrikov. Pytorch implementations of reinforcement learning algorithms. https://
github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail, 2018.
Dirk P. Kroese, T. Brereton, T. Taimre, and Z. Botev. Why the monte carlo method is so important
today. Wiley Interdisciplinary Reviews: Computational Statistics, 6:386-392, 2014.
L. I. Kuncheva. Change detection in streaming multivariate data using likelihood detectors. IEEE
Transactions on Knowledge and Data Engineering, 25(5):1175-1180, 2013. doi: 10.1109/TKDE.
2011.226.
David L. Demets K. K. Gordon Lan. Interim analysis: The alpha spending function approach.
Statistics in Medicine, 13:1341-52, 1994.
Erwan Lecarpentier and Emmanuel Rachelson. Non-stationary markov decision processes: a worst-
case approach using model-based reinforcement learning. NeurIPS 2019, abs/1904.10090, 2019.
URL http://arxiv.org/abs/1904.10090.
Kimin Lee et al. Context-aware dynamics model for generalization in model-based rl. ICML, 2020.
Chao-Wen Lu and Marion R. Reynolds Jr. Cusum charts for monitoring an autocorrelated process.
Journal of Quality Technology, 33(3):316-334, 2001. doi: 10.1080/00224065.2001.11980082.
URL https://doi.org/10.1080/00224065.2001.11980082.
Robert Lund, Xiaolan L. Wang, Qi Qi Lu, Jaxk Reeves, Colin Gallagher, and Yang Feng. Change-
point Detection in Periodic and Autocorrelated Time Series. Journal of Climate, 20(20):5178-
5190, 10 2007. ISSN 0894-8755. doi: 10.1175/JCLI4291.1. URL https://doi.org/10.
1175/JCLI4291.1.
Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Bandits with adversarial scaling. ICML,
2020.
Shie Mannor.	Why does reinforcement learning not work (for
you)?	https://rlrl.net.technion.ac.il/2020/01/27/
why- does- reinforcement- learning- not- work- for- you/, 2019.
MathWorks. Conditional value-at-risk (cvar). https://www.mathworks.com/
discovery/conditional- value- at- risk.html.
Tatsuya Matsushima, Hiroki Furuta, Y. Matsuo, Ofir Nachum, and Shixiang Gu. Deployment-
efficient reinforcement learning via model-based offline optimization. ArXiv, abs/2006.03647,
2020.
Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim
Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement
learning. Proceedings of Machine Learning Research, 48:1928-1937, 20-22 Jun 2016.
MuJoCo. Halfcheetah-v2. https://gym.openai.com/envs/HalfCheetah-v2/.
Subhojyoti Mukherjee and Odalric-Ambrym Maillard. Distribution-dependent and time-uniform
bounds for piecewise i.i.d bandits. arXiv preprint arXiv:1905.13159, 2019.
Susan A Murphy, Mark J van der Laan, and James M Robins. Marginal mean models for dynamic
regimes. Journal of the American Statistical Association, 2001.
Ofir Nachum, Michael Ahn, Hugo Ponte, Shixiang (Shane) Gu, and Vikash Kumar. Multi-agent
manipulation via locomotion using hierarchical sim2real. PMLR, 100:110-121, 30 Oct-01 Nov
2020. URL http://proceedings.mlr.press/v100/nachum20a.html.
NCSS. Cumulative sum (cusum) charts. https://ncss-wpengine.netdna-ssl.com/
wp-content/themes/ncss/pdf/Procedures/NCSS/CUSUM_Charts.pdf.
Jerzy Neyman, Egon Sharpe Pearson, and Karl Pearson. On the problem of the most efficient tests
of statistical hypotheses. Philosophical Transactions of the Royal Society of London, 1933. doi:
10.1098/rsta.1933.0009.
12
Under review as a conference paper at ICLR 2021
Peter C. O’Brien and Thomas R. Fleming. A multiple testing procedure for clinical trials. Biomet-
rics,35(3):549-556, 1979. ISSN 0006341X, 15410420. URL http://www.jstor.org/
stable/2530245.
PennState College of Science. Lecture notes in stat 509: Alpha spending function approach.
https://online.stat.psu.edu/stat509/node/81/.
OpenAI. Pendulum-v0. https://gym.openai.com/envs/Pendulum-v0/.
E. S. Page. Continuous Inspection Schemes. Biometrika, 41(1-2):100-115, 06 1954. ISSN 0006-
3444. doi: 10.1093/biomet/41.1-2.100. URL https://doi.org/10.1093/biomet/41.
1-2.100.
Fabio Pardo, Arash Tavakoli, Vitaly Levdik, and Petar Kormushev. Time limits in reinforcement
learning. CoRR, abs/1712.00378, 2017. URL http://arxiv.org/abs/1712.00378.
V. V. Petrov. Sums of Independent Random Variables. Nauka, 1972.
S. J. Pocock. Group sequential methods in the design and analysis of clinical trials. Biometrika, 64
(2):191-199, 08 1977. ISSN 0006-3444. doi: 10.1093/biomet/64.2.191. URL https://doi.
org/10.1093/biomet/64.2.191.
R. Tyrrell Rockafellar and Stanislav Uryasev. Optimization of conditional value-at-risk. Journal of
Risk, 2:21-41, 2000. doi: 10.21314/JOR.2000.038.
Thomas P. Ryan. Statistical Methods for Quality Improvement. Wiley; 3rd Edition, 2011.
E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. 2012
IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026-5033, 2012.
A. Wald. Sequential tests of statistical hypotheses. Annals of Mathematical Statistics, 16(2):117-
186, 06 1945. doi: 10.1214/aoms/1177731118. URL https://doi.org/10.1214/aoms/
1177731118.
James Westgard, Torgny Groth, T Aronsson, and C Verdier. Combined shewhart-cusum control
chart for improved quality control in clinical chemistry. Clinical chemistry, 23:1881-7, 11 1977.
doi: 10.1093/clinchem/23.10.1881.
S. S. Wilks. The large-sample distribution of the likelihood ratio for testing composite hypotheses.
Ann. Math. Statist., 9(1):60-62, 03 1938. doi: 10.1214/aoms/1177732360. URL https://
doi.org/10.1214/aoms/1177732360.
S. M. Williams et al. Quality control: an application of the cusum. BMJ: British medical journal,
304.6838:1359, 1992.
Yao Xie and David Siegmund. Weak change-point detection using temporal correlation, 2011.
E. Yashchin. On the analysis and design of cusum-shewhart control schemes. IBM Journal of
Research and Development, 29(4):377-391, 1985.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn,
and Tengyu Ma. Mopo: Model-based offline policy optimization, 2020.
Xingyu Zhao et al. Assessing the safety and reliability of autonomous vehicles from road testing.
ISSRE, 2019.
Shiyu Zhou, Nong Jin, and Jionghua (Judy) Jin. Cycle-based signal monitoring using a directionally
variant multivariate control chart system. IIE Transactions, 37(11):971-982, 2005. doi: 10.1080/
07408170590925553. URL https://doi.org/10.1080/07408170590925553.
13