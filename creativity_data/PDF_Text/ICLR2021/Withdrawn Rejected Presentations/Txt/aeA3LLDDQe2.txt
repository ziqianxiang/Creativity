Under review as a conference paper at ICLR 2021
Fairness guarantee in analysis of incomplete
DATA
Anonymous authors
Paper under double-blind review
Ab stract
Missing data are prevalent and present daunting challenges in real data analysis.
While there is a growing body of literature on fairness in analysis of fully observed
data, there has been little work on investigating fairness in analysis of incomplete
data when the goal is to develop a fair algorithm in the complete data domain
where there are no missing values. In practice, a popular analytical approach for
dealing with missing data is to use only the set of complete cases, i.e., observations
with all features fully observed, as a representation of complete data in learning.
However, depending on the missing data mechanism, the complete case domain
and the complete data domain may have different data distributions and a fair
algorithm in the complete case domain may show disproportionate bias towards
some marginalized groups in the complete data domain. To fill this significant gap,
we studying the problem of estimating fairness in the complete data domain for
a model trained using observed data and evaluated in the complete case domain.
We provide upper and lower bounds on the fairness estimation error and conduct
numerical experiments to assess our theoretical results. Our work provides the
first known results on fairness guarantee in analysis of incomplete data.
1 Introduction
Mounting evidence (Gianfrancesco et al., 2018; Buolamwini & Gebru, 2018; Kiritchenko & Mo-
hammad, 2018; Trewin et al., 2019; Wang et al., 2019) has suggested that powerful machine learning
algorithms can be unfair and lead to disproportionately unfavorable treatment towards marginalized
groups. In recent years, there has been a growing body of research on addressing unfairness and bias
of machine learning algorithms (Chouldechova & Roth, 2020).
Meanwhile, missing data are ubiquitous and present daunting challenges in real data analysis. Par-
ticularly, missing data, if not adequately handled, would lead to biased estimation and improper
statistical inference (Little & Rubin, 2019). As such, analysis of incomplete data has been an active
research area. More recently, there is also a growing recognition that missing data may have dele-
terious impact on algorithmic fairness. For example, in medicine, bias caused by missing values in
electronic health records is identified as a significant factor contributing to unfairness of machine
learning (ML) algorithms used in medicine that may exacerbate health care disparities (Rajkomar
et al., 2018; Gianfrancesco et al., 2018). However, there has been little to no reported research on
fairness in analysis of incomplete data.
In the presence of missing values, one popular approach for training ML models is to use only
the set of complete cases, i.e., observations with all features observed, as a representation to the
complete data in learning, discarding the other incomplete cases/observations. However, in practice,
the ultimate goal, oftentimes, is to apply the trained models to the complete data domain where there
are no missing values. Depending on the missing data mechanism, the complete case domain and the
complete data domain may have different data distributions. A fair algorithm in the complete case
domain may show disproportionate bias towards marginalized groups in the complete data domain.
Our work addresses this significant gap when using complete cases as representations in learning.
1
Under review as a conference paper at ICLR 2021
1.1	Related works
The growing body of literature on algorithmic fairness has been primarily focused on two types of
fairness defintions, group fairness and individual fairness (Chouldechova & Roth, 2020). Group
fairness emphasizes that members from different groups (e.g. gender, race etc.) should be treated
similarly, while individual fairness pays more attention to treatment similarity between any of two
similar individuals. In this work we investigate group fairness in analysis of incomplete data. Fair-
ness notions are acknowledged to be case-specific, in a sense that it is not possible to achieve fairness
under various definitions (Friedler et al., 2016). In binary classification problems, demographic (or
statistical) parity (Calders & Verwer, 2010; Dwork et al., 2012) is a fairness notions that has been
mostly studied. It states that the predicted outcome should be independent with sensitive attributes.
However demographic parity can cause severe harm to the prediction performance when true re-
sponse is dependent with sensitive attributes. As an alternative, disparate mistreatment (Zafar et al.,
2017) states that misclassification level (regarding e.g. overall accuracy, false negative rate, false
discovery rate etc.) should be similar between two sensitive groups. Similarly, Hardt et al. (2016)
proposes equalized odds, which requires both false positive rate (FPR) and false negative rate (FNR)
to be the same between two groups. In regression setting, fairness notion is usually associated with
parity of loss between two groups (Agarwal et al., 2019; Oneto et al., 2019; Donini et al., 2018).
In this paper we adopt accuracy parity gap as fairness notions in learning tasks including classifica-
tion and regression. In fact, our results can be generalized into other fairness notions such as equal
opportunity and prediction error parity with respect to mean square error.
Recently, Schumann et al. (2019) investigated fairness cross different domains and provided an upper
bound of fairness on one desired domain (target domain) given that on another domain (source
domain). MartInez-PlUmed et al. (2019) investigated the impact of missing data on algorithmic
fairness through numerical experiments. To the best of our knowledge, this is the only work on
algorithmic fairness in the presence of missing data.
1.2	Our contributions
There are a nUmber of fUndamental differences between oUr work and SchUmann et al. (2019).
Their work does not deal with missing data and associated challenges and does not consider the
techniqUe of re-weighting for domain adaptation. In addition, they provided only Upper boUnds on
transferring fairness. The main contribUtions of oUr work are as follows: (1) the first theoretical
analysis of fairness gUarantee in analysis of incomplete data which provides both theoretical Upper
and lower boUnds; (2) the first stUdy on the effect of re-weighting in analysis of fairness across two
data domains indUced by missing data and the impact of the missing data mechanism; (3) evalUation
of the transferred fairness (from the complete case domain to the complete data domain) throUgh
extensive nUmerical experiments Using both synthetic and real data. OUr work represents significant
advances over the existing work by Martlnez-Plumed et al. (2019) in studying fairness guarantee for
analysis of incomplete data.
2	Problem Formulation
2.1	Preliminaries on Missing data
Suppose that the data set of interest contains n observations. If there were no missing values, each
observation/case zi := {xi, yi} ∈ X × Y (i = 1, . . . , n) consists of predictors xi ∈ X and label (re-
sponse variable) yi ∈ Y. We denote the complete data matrix by Z, whose ith row is zi . Since some
entries of Z are missing, define the indicator for observing zij or not as rij = 1{zij is observed}.
Denote the corresponding indicator matrix by R. Let z(1)i denote the components of zi that are
observed for observation i, and z(0)i denote the components of zi that are missing for observation
i. For example, consider the case when there are two predictors and one response. If only zi1 is
observed, then z(1)i = zi1, z(0)i = (zi2, zi3). We then define the observed data Zobs as the collec-
tion of the observed components from all n observations, z(1)i, i = 1, . . . , n and the missing data
Zmis as the collection of all the missing components, z(0)i , i = 1, . . . , n .
There are three primary missing data mechanisms, namely, missing completely at random (MCAR),
missing at random (MAR) and missing not at random (MNAR) (Little & Rubin, 2019). Data are said
2
Under review as a conference paper at ICLR 2021
to be missing completely at random (MCAR) if the distribution of R is independent of Z. Data are
said to be missing at random (MAR) if the distribution of R depends on Z only through its observed
components, i.e., R ⊥ Zmis|Zobs. Data are said to be missing not at random (MNAR) if the
distribution of R depends on the missing components of Z. We will investigate fairness guarantee
under all three mechanisms.
In the presence of missing values, observation i is said to be a complete case if it is fully observed
(i.e. z(1)i = zi). Let Ri := 1{zi is fully observed} denote the indicator of complete cases. We can
then define two different data domains (distributions). The complete data domain, denoted by DT,
is the distribution of zi in the joint space X × Y. The complete case domain, denoted by DS, is the
distribution of observations that have all variables fully observed: zi |Ri = 1 .
Under the MCAR mechanism, the distribution of the complete case in DS is the same as the dis-
tribution of the complete data in DT . However under MAR or MNAR, the data distributions in
DS and DT can be different. For example, if missingness depends on gender and other features
associated with gender, then females may have a substantially higher proportion of missing values
and the feature distribution in females in DS may be very different from that in DT . As a result, an
algorithm that is trained and shown to be fair in DS may not be fair when evaluated in DT , noting
that in practice we typically are more interested in the fairness guarantee in DT .
2.2	Fairnes s notions and estimand
We consider learning tasks that use features x ∈ X to predict response y ∈ Y. Each observation also
has a binary sensitive attribute A ∈ {0, 1}. We are interested in assessing fairness of a prediction
model g : X → Y, which is learned from the observed data Zobs, in complete data domain. Let
Ea(g) := ETa |g(x) - y(x)| denote the prediction error, where Ta represents that the expectation
is taken with respect to domain DTa, the (conditional) distribution of sensitive group A = a. We
define a general fairness measurement to develope our theoretical results, which is applicable in
various learning tasks including regression and classification.
Definition 1 (Accuracy Parity Gap). Given an arbitrary complete data domain DT and predictor
g, the accuracy parity gap of g is ∆T (g) = |E0(g) - E1(g)|, where subscript T indicates that the
fairness estimand is defined in the complete data domain DT.
This definition has close connections with various fairness notions proposed in the literature. In
binary classification task where the response is binary: y ∈ {0, 1}, the notion accuracy parity has
been used in Zafar et al. (2017), Friedler et al. (2016) and Zhao et al. (2019b), which requires that the
prediction accuracy between two sensitive groups to be equal: P(g(x) = y|A = 0) = P(g(x) =
y |A = 1). Accuracy parity gap in this case is the absolute value of difference between above two
quantities |P(g(x) = y|A = 0) - P(g(x) = y|A = 1)|. For regression tasks where the response
y takes continuous value, fairness constraints on loss difference between two groups are adopted in
Donini et al. (2018), Oneto et al. (2019) and Agarwal et al. (2019). Accuracy parity gap under such
setting can be regarded as the difference of the mean absolute error (MAE) loss.
2.3	Fairnes s estimator
Since missing values can cause bias in an incomplete data set, fairness in the complete data domain
is typically of primary interest. However, due to missing values, fairness of the prediction model
g is assessed using only the complete cases. Fairness estimation, using the finite sum over the
complete cases to approximate ∆T (g) can be biased because of the domain difference. To mitigate
such estimation bias, one useful approach is to assign weight ω(zi) to observation zi and calculate
the weighted sum over the complete cases to estimate ∆T (g). Specifically, we define the weighted
empirical risk (prediction error) using the complete cases Eba(g, ω) := Pn-I(A∙=a)R Pn=ι I(Ai =
a)Riω(zi)∖g(xi) - yi∖, where a ∈ {0,1}. Here we assume there is at least one complete case
observed for each sensitive group (Pin=1 I(Ai = a)Ri ≥ 1). Then the proposed fairness estimator
is defined as follows.
Definition 2 (Fairness estimator from complete cases). Suppose the weights assigned to complete
cases are given by ω. Then the fairness estimator for predictor g in the complete data domain is
3
Under review as a conference paper at ICLR 2021
∆b S (g, ω) =
E0(g, ω) - E1(g, ω), where subscript S indicates that the estimator is obtained from
the data from the complete case domain DS.
^
^
In practice, a popular choice of ω is the normalized inverse of the propensity score (a.k.a, the proba-
bility of being a complete case). Let π(zi) := PT (Ri = 1|zi) denote the true propensity score (PS)
model. Corollary 1 presents the results when ω(zi) = [∏(zi)Es {1∕∏(z)}]-1, where S represents
that the expectation is taken with respect to DS . In practice, we typically do not know the true
propensity scores or the true distribution of complete cases, so we need to estimate the propensity
scores and use the empirical distribution of complete cases. Various statistical and machine learning
models can be used to estimate the propensity scores, such as logistic regression, random forest,
support vector machines and boosting.
3	Main results
In this section we provide theoretical analysis for the proposed fairness estimator ∆S(g, ω) in the
complete data domain DT . Some of the techniques in the proofs are inspired from the analysis of
learning guarantee in domain adaptation in Cortes et al. (2010). Throughout, we assume the weights
are normalized: ES ω(z) = 1 and are bounded away from 0 and infinity. In the regression setting,
it is commonly assumed in the domain adaptation literature that y takes value inside interval [b1, b2]
for some real number b1 , b2 .
Our theoretical results can be generalized to a wide range of other fairness notions. For binary
classification, all the five measurements of disparate mistreatment (accuracy, false positive rate,
false negative rate, false mission rate and false discover rate) mentioned in Section 2 of Zafar et al.
(2017) can be adopted to obtain our main results. Notably, false negative rate parity is also known as
Equal Opportunity proposed in Hardt et al. (2016). For regression, itis straightforward to generalize
our results to the fairness notation defined as the difference of Lp loss with 1 ≤ p < ∞.
3.1	An upper bound
Let B = supzω(z)< + ∞ denote the upper bound of the weights, Daω = ESa ω(z)2 denote the
second moment of weights, ωDSa denote the distribution whose probability density function at z
equals to ω(z)fSa (z) with fSa be the probability density function in DSa. We further let na denote
the number of complete cases observed in group a ∈ {0, 1}. Theorem 1 below provides an upper
bound of the fairness estimation error.
Theorem 1. Assume that g is from a hypothesis class H with VC dimension d (pseudo dimension if
in regression setting) and y ∈ [0, 1]. Assume Daω ≤ na/8 for both groups. Then, for any δ > 0, with
probability at least 1 - δ, the following inequality holds:
∣∆τ(g)-∆ S (g,ω)∣≤ X dTV (Dτa∣∣ωDsa) + ^D Cd",D ⑼ +
a∈{0,1}	na	na
(1)
where dtv (Dτa ∣∣ωDsa) denote the total variation distance between DTa and ωDsa and that
log (d+1)(8e) + + d log 2Daω in classification
log 4 (8e)d + 3d log √Dω	in regression
Remark 1. If y ∈ [b1, b2], the upper bound in Theorem 1 would be multiplied by b2 - b1, with
requirement that Daω (b2 - b1) ≤ na/8.
Remark 2. Daω is always upper bounded by B2. In particular, for MCAR mechanism, Daω = 1.
Cd(na,Daω,δ)
Since the fairness estimand is defined based on prediction error, there is some similarity between the
upper bound in Theorem 1 and the upper bounds for learning error in the domain adaptation literature
Ben-David et al. (2010); Redko et al. (2017). The upper bound is obtained by the triangle inequality
and the detailed analysis of generalization error. In the first term of the upper bound, ωDSa is
an approximation to the complete data domain in sensitive group a, and dτv(Dτa ∣∣ωDsa) can be
4
Under review as a conference paper at ICLR 2021
viewed as the approximation error. It follows that a less accurate approximation of the complete
data domain would lead to a looser upper bound on fairness estimation error. In the second and third
terms, Cd(na, Daω, δ) is proportional to logna. Since the third term is dominated by the second
term, their sum is of order O (log nmin/nmin) 2 With nm[n representing the sample size in the minority
group. It also follows that for a fixed total sample size n, the upper bound increases with sample
imbalance betWeen tWo groups defined by A. In addition, the missing data mechanism impacts
the second moment of estimated Weights Daω ∈ [1, B2]. If a missing data mechanism leads to a
larger second moment of the Weights, the upper bound for the fairness estimation error Would be
looser. Furthermore, When the Weights ω are defined using the true propensity scores, We call the
resulting ω0(zi) := (π(zi)Es(1∕π(z)))-1 as the true weights and the first term in the upper bound
in Theorem 1 Would vanish in Which case We have the folloWing result.
Corollary 1. If ω(zi) = ω0(zi), then ∆S (g, ω) is consistent for estimating ∆T (g).
There are several implications from Corollary 1. Under the MCAR mechanism, setting ω(zi) = 1
would yield a consistent (unweighted) estimator. Under MAR and MNAR, since we typically do
not know the true propensity scores ∏(zi), we replace ∏(zi) with its estimate ∏(zi) using a work-
ing model for π(zi) which is subject to mis-specification. When the number of complete cases
in both groups are sufficiently large, the first term in the upper bound would often be the domi-
nant term. Then, ∏(zi) from a correctly-specified propensity score model would lead to a smaller
dψV(Dτa l∣ωDsa) and hence a tighter bound. On the other hand, an incorrectly-specified propensity
score model could lead to a larger dTγ(Dτa ∣∣ωDsa) compared with the unweighted estimator.
3.2 A lower bound
Define σ2(g,ω) := Varsa(ω∣g - y|) and it can be shown that σ2(g,ω) ≤ B2∕4. We present the
result on a lower bound for the fairness estimation error in the following theorem.
Theorem 2. Ifthe weight ω(zi) is set to be ω0(zi) and that B2/σ^(g,ω) ≤ min{n0,nι}. Then
with probability at least 诵4行,thefoUowing hold:
12∖S σ0^j 嚏") ≥ ∣(E0(g) -Eι(g)) - (bo(g,ω) -Eι(g,ω))∣ ≥ ɪ S σ^也 十 嚏山
n0	n1	24	n0	n1
2 2 ⑵
Ifabove holds and that ∆S(g, ω) ≥ 13 ∖∣σ人；：：0 + %(不；亘,we have:
∣∆τ(g) - ∆S(g,ω)∣ ≥ ɪS30I + 也应
24	n0	n1
If ∆s(g,ω) ≤ 72,σ2(gωO) + σ2(n,ωO), We have:
∣∆τ⑹-∆s(g,ω)∣ ≥ ɪS哽 j 嗖g回
72	n0	n1
The proof involves detailed analysis of the truncation probability for the fairness difference. We also
have the following corollary describing the lower bound of fairness when g is a consistent estimator
for y and ∣ω(zi) - ω°(Zi)| = Op((n0 + nι)-1/2).
Corollary 2. If the weights satisfy ∣ω(zi) 一 ω0(zi)∣ = Op((no + nι)-1/2) and g is consistent
for y (i.e. limn g(x) = y(x)), then for any positive δ, there exists N0 and N1 such that whenever
no > No and nι > N∖, with probability at least 缶(-12)2 — δ
∣ (Eo⑷-EI⑷)-(EO(g, ω)- E-(g,⑼)∣ ≥ 215Sσ0(noω0)+σ⅛ω^	⑶
Remark 3. Ify is bounded in [b- , b2] in regression instead of the unit interval, then Theorem 2 and
Corollary 2 still hold.
5
Under review as a conference paper at ICLR 2021
Remark 4. Ifthe weight ω is based on the estimated ∏(zi) from a correctly specified propensity
score model, then the optimal convergence rate for ∣ω(zi) 一 ω0(zi)∣ is Op ((n0 + nι)-1/2). Com-
paring results from (1) and (3), the upper bound is of order O((log nmin/nmin)1/2) while the lower
bound is of order O((nmin)-1/2).
Our theoretical results have additional important implications related to the missing data mecha-
nisms. First, as long as sample size is sufficiently large as required, Theorem 1 would always hold
for all mechanisms. Under the MCAR mechanism, the true propensity score is a constant and hence
can be regarded as known, and the results from Theorem 2 hold for the unweighted estimator. Un-
der the MAR mechanism, the true propensity score is generally unknown. If the correctly specified
propensity score model is fitted and the estimated propensity scores converge to the true values at
the rate of Op((n0 +n1)-1/2), the results in Corollary 2 would hold. Under the MNAR mechanism,
the propensity score model depends on missing values, so it cannot be estimated without making
additional modeling assumptions. If the propensity score model is mis-specified under MAR or
MNAR, the results in Theorem 2 are not applicable.
4	Numerical Experiments
In this section we empirically evaluate the bias of fairness estimation in both synthetic and real data
sets. Lower bound in Theorem 2 is justified and several factors that influence the fairness estimation
are also investigated. Recall that our goal is to estimate fairness ∆T (g) defined in the complete data
domain, while the estimator ∆S (g, ω) is obtained from the complete case domain.
4.1	Synthetic data
In our simulation studies, we consider a regression task with 10 predictors and a binary sensitive
attribute A ∈ {0, 1}. With sample size n , the predictors are generated from gaussian distribution:
Xiji㈣ N(1 一 2Ai,0.52) (i = 1,..., n and j = 1, . . . , 10). We generate the response yi =
(x>β)2 + Ei where β = (0.1,0.1,0.1,0.1,0.1,1,1,1,1,1)> and E = (6ι,..., 6n)τ 〜N(0, In).
Missing values are generated from the last 5 features using different propensity score models under
MAR. After generating missing values, we use complete cases to fit a random forest model g for
predicting y and investigate the bias of estimated fairness ∆S(g, ω) for g.
Justification of convergence rate Missing values are generated based on the following propen-
Sity score model logit(π(zi)) = -3 + 5 P：=i sin(3xj), where ∏(zi) = P(Ri = 1∣Zi) and
logit(p) = log i-p. Of note, since Ri depends on only the fully observed features, the missing
data mechanism is MAR. In this experiment, we use the true weights ω(z) = ω0(z) to calculate
the fairness estimation bias. Figure 1-(a) shows that the lower bound from Theorem 2 is smaller
than the mean of fairness estimation bias. Meanwhile, it sometimes lies inside 90% band of fairness
estimation bias, implying the lower bound is not disproportionately loose. In addition, the slope of
fairness difference is smaller than that of lower bound, indicating the actual convergence rate might
be o(nm-i1n/2) under this experiment setting.
Effect of sample imbalance We fix the total sample size and alter the sample imbalance between
two groups. We generate missing values using MAR mechanism logit(∏(zi)) = -1 + ɪ P5=ι Xij.
To estimate the fairness, we adopt logistic regression for propensity score estimation, which is
correctly-specified. The resulting fairness estimation bias is shown in Figure 1-(b), in which we
set sample imbalance ratio, which is the number of complete data in group Ai = 0 divided by that
in group Ai = 1, from 1 to 9. For a fixed n, increasing sample imbalance will lead to larger bias
iδt(g) - δs(g, ω) |. This result supports that sample imbalance could harm the fairness estimation.
Effect of different weights We compare the performance of fairness estimator among 7 estima-
tors with different weights: ω(zi) = 1 (i.e., unweighted estimator), the true weights ω(zi) = ω0(zi)
and 5 weights estimated via ω(zi) = ωo(zi) := Pn=ι Ri/ (∏(zi) Pn=ι {Ri∕∏(zi)}) where ∏(zj
is obtained from logistic regression(both correctly and incorrectly specified), random forest (RF)
6
Under review as a conference paper at ICLR 2021
estimator, support vector machine (SVM) as well as extreme gradient boosting (XGB) estimators.
Missing values are generated from MAR mechanism logit(π(zi)) = 3 -1 j Xj We alter
the complete data’s sample size so that both group has the same number of complete cases (in ex-
pectation). In particualr we use {xi3j} to fit the first logistic regression model, which is correctly
specified. In the second model, we use {|xij |}, which leads to an incorrectly specified model. From
Figure 1-(c), using true inverse probability weights or correctly specified model (logistic regres-
sion) lead to smaller gap between fairness in complete data and complete case domains. At the
same time, incorrectly-specified propensity score model and non-parametric propensity score mod-
els could result m larger ∣∆τ(g) - ∆s(g, ω)∣. In this case, unweighted estimator has the largest
fairness estimation bias.
Effect of sensitive groups’ domains We add an additional parameter M in the distribution of
designs: xij i.i.d. drawn from N(1 - 2MAi, 0.52). Then we generate missing values using MAR
mechanism logit(π(zi)) = 2 - 4Ai. We use a correctly-specified logistic regression model to
estimate propensity score model. Fairness estimation bias with different M are shown in Figure
1-(d). From the figure, with increasing M, the distance between two sensitive group’s domain
increases. At the same time the fairness estimation bias also increases. This implies that it can be
harder to guarantee fairness in complete data domain when two sensitive groups’ distributions are
more different.
(a) Justification of bounds
(b) Effect of sample imbalance
(c) Effect of PS estimator
(d) Effect of sensitive group domains
Figure 1: Experiments on synthetic data. In (a), 90% confidence band and mean value of fairness
estimation bias are calculated over 200 repeats. Each curve in (b) (c) (d) plots the mean fairness
estimation bias over 50 repeats.
4.2	Real datasets
In analysis of each real dataset, we artificially generate missing values under the three miss-
ing data mechanisms, train a random forest prediction model g for the outcome of interest
using the set of complete cases, and then evaluate the fairness estimation error of g . For
the estimated fairness ∆S(g, ω), We compare multiple options of ω, a) ω(zi) = 1 (i.e., un-
7
Under review as a conference paper at ICLR 2021
Specification of ω
Unweighted True Logistic RF SVM XGB
COMPAS	MCAR	1.27 ± 0.81	1.30 ± 1.01	1.32 ± 1.04	1.33 ± 0.92	2.02 ± 1.81	1.41 ± 1.08
(×10-2)	MAR	3.12 ± 2.05	1.91 ± 1.61	2.03 ± 1.69	2.16 ± 1.71	8.05 ± 5.36	3.44 ± 2.40
	MNAR	3.92 ± 1.90	2.78 ± 1.73	3.82 ± 1.90	3.84 ± 2.11	3.81 ± 2.03	3.51 ± 2.07
ADNI	MCAR	3.00 ± 2.63	2.96 ± 2.47	2.45 ± 2.09	2.70 ± 2.27	3.09 ± 2.55	3.06 ± 2.46
(×10-3)	MAR	3.24 ± 2.71	2.41 ± 1.67	2.73 ± 2.37	2.71 ± 1.72	3.00 ± 2.63	3.22 ± 2.68
	MNAR	3.13 ± 2.48	2.13 ± 1.82	2.61 ± 2.02	2.35 ± 1.88	3.16 ± 2.61	3.07 ± 1.55
EFlTTʌ*	∙ r∙ ∙	. ∙	. ∙	I ʌ / ∖	公/	∖ I	∙ . 1 1∙ i`i`	.	. ∙	C	1	♦♦
Table 1: Bias in fairness estimation ∣∆τ(g) - ∆s(g, ω)∣ With different options for ω and missing
data mechanisms in analysis of the COMPAS and ADNI datasets. Mean ± SD over 50 repeats.
Weighted), b) the true inverse probability Weights ω(zi) = ω0(zi), and c) ω(zi) = ωb0(zi) =
pn=ι Ri/ (∏(zi) Pn=ι {Ri∕Π(zi)}) Where ∏(zi) is obtained from logistic regression, RF, SVM
and XGB. We compute the fairness estimation bias ∣∆τ(g) - ∆s(g, ω)∣, where ∆τ(g) is approx-
imated using the complete data in the original real dataset before missing values are generated.
This procedure is repeated 50 times for each dataset. Of note, in all real data analyses, the logistic
regression model is the correctly specified model for π(zi).
COMPAS recidivism dataset COMPAS (Correctional Offender Management Profiling for Alter-
native Sanctions) (Northpointe, 2010) is a risk assessment instrument developed by Northpointe Inc.
A set of questions is completed by defendants to be used in prediction of “Risk of Recidivism.” The
dataset analyzed in this Work contains records of defendants from BroWard County from 2013 and
2014. Prior Work has demonstrated bias of COMPAS toWards certain groups of defendants (e.g.
race, gender and age) (AngWin et al., 2016). In our analysis, gender is treated as the sensitive at-
tribute and nine features are used to predict tWo-year recidivism (defined by arrest Within 2 years)
(Rudin et al., 2018). We generate missing values for the last feature and the outcome variable un-
der three missing mechanisms: MCAR, logit(π(zi)) = 0.5; MAR, logit(π(zi)) = 3 + Pj5 * * 8=1 xij;
MNAR, logit(π(zi)) = 2y + 2 Qj9=6 xij. As shoWn in Table 1, all options of ω lead to compara-
ble results under MCAR, noting that all of them are valid under MCAR. Under MAR and MNAR,
hoWever, the estimator using the true Weights is most accurate and folloWed by logistic regression,
noting that logistic regression is the correctly specified model for π(zi) in this data analysis.
ADNI gene expression data We analyze a dataset from Alzheimer’s Disease Neuroimaging Ini-
tiative Which contains gene expression and clinical data for 649 patients. In our analysis, We
use the top 1000 transcriptomic features With highest positive correlation With gender, the sen-
sitive feature. The outcome variable is the VBM right hippocampal volume, ranging betWeen
[0.4, 0.6]. Missing values are generated for the last 900 features under the three missing data
mechanisms: MCAR, logit(π(z∙i)) = 0.5; MAR, logit(π(z∙i)) = 2 —击 P：=i Xij; MNAR,
logit(π(zi)) = 2 - 25 pi501θι Xij. As shown in Table 1, the main findings are consistent with
those from the analysis of COMPAS dataset.
5 Discussions
In this paper, we provide both an upper and lower bounds on fairness estimation error in the com-
plete data space and evaluate the theoretical properties in extensive numerical experiments. Our
work provides the first known results on fairness guarantee when learning with incomplete data as
representations. We expect the work to offer insight into this area of research. Indeed this area
offers fertile ground for future research. For example, many methods have been developed for han-
dling missing data including imputation. It is of potential interest to investigate the impact of these
methods on algorithmic fairness and refine these methods for the purpose of developing fair ML
algorithms using the imputed data.
8
Under review as a conference paper at ICLR 2021
References
Alekh Agarwal, Miroslav Dudik, and ZhiWei Steven Wu. Fair regression: Quantitative definitions
and reduction-based algorithms. arXiv Preprint arXiv:1905.12843, 2019.
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchne. Machine bias. ProPublica. Retrieved
from	https:〃www.propublica.org/article/ how-we-analyzed-the-compas-recidivism-algorithm,
2016.
Martin Anthony and Peter L Bartlett. NeUral network Iearning: TheOretiCal foundations. cambridge
university press, 2009.
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wort-
man Vaughan. A theory of learning from different domains. MaChine Iearning, 79(1-2):151-175,
2010.
Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data decisions and theoretical implications when
adversarially learning fair representations. arXiv preprint arXiv:1707.00075, 2017.
Victor Bouvier, Philippe Very, Clement Chastagnol, Myriam Tami, and CeIine Hudelot. Robust do-
main adaptation: Representations, weights and inductive bias. arXiv preprint arXiv:2006.13629,
2020.
Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in commer-
cial gender classification. In COnferenCe on fairness, accountability and transparency, pp. 77-91,
2018.
Toon Calders and Sicco Verwer. Three naive bayes approaches for discrimination-free classification.
Data Mining and Knowledge Discovery, 21(2):277-292, 2010.
Alexandra Chouldechova and Aaron Roth. The frontiers of fairness in machine learning. arXiv
preprintarXiv:1810.08810, 2018.
Alexandra Chouldechova and Aaron Roth. A snapshot of the frontiers of fairness in machine learn-
ing. COmmUniCatiOnS of the ACM, 63(5):82-89, 2020.
Sam Corbett-Davies, Emma Pierson, Avi Feller, Sharad Goel, and Aziz Huq. Algorithmic deci-
sion making and the cost of fairness. In PrOCeedingS of the 23rd ACM SIGKDD InternatiOnal
COnferenCe on Knowledge DiSCOVery and Data Mining, pp. 797-806, 2017.
Corinna Cortes, Yishay Mansour, and Mehryar Mohri. Learning bounds for importance weighting.
In AdVanCeS in neural information processing systems, pp. 442T50, 2010.
Shai Ben David, Tyler Lu, Teresa Luu, and David Pal. Impossibility theorems for domain adaptation.
In Proceedings of the Thirteenth InternatiOnal COnferenCe on ArtifiCiaintelligence and StatiStics,
pp. 129-136, 2010.
Michele Donini, Luca Oneto, Shai Ben-David, John S Shawe-Taylor, and Massimiliano Pontil.
Empirical risk minimization under fairness constraints. In AdVanCeS in NeUraI InfOrmatiOn
PrOCeSSing Systems, pp. 2791-2801, 2018.
Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness
through awareness. In PrOCeedingS of the 3rd innovations in theoretical COmpUter SCienCe
conference, pp. 214-226, 2012.
Michael Feldman, Sorelle A Friedler, John Moeller, Carlos Scheidegger, and Suresh Venkatasubra-
manian. Certifying and removing disparate impact. In proceedings of the 21th ACM SIGKDD
international COnference on knowledge discovery and data mining, pp. 259-268, 2015.
Sorelle A Friedler, Carlos Scheidegger, and Suresh Venkatasubramanian. On the (im) possibility of
fairness. arXiv preprint arXiv:1609.07236, 2016.
Milena A Gianfrancesco, Suzanne Tamang, Jinoos Yazdany, and Gabriela Schmajuk. Potential bi-
ases in machine learning algorithms using electronic health record data. JAMA internal medicine,
178(11):1544-1547, 2018.
9
Under review as a conference paper at ICLR 2021
Paula Gordaliza, Eustasio Del Barrio, Gamboa Fabrice, and Jean-Michel Loubes. Obtaining fairness
using optimal transport theory. In IntemationaI Conference on Machine Learning, pp. 2357-2365,
2019.
Nina Grgic-Hlaca, Muhammad Bilal Zafar, Krishna P Gummadi, and Adrian Weller. The case for
process fairness in learning: Feature selection for fair decision making. In NIPS SymPosiUm on
Machine Learning and the Law, volume 1, pp. 2, 2016.
Moritz Hardt, Eric Price, and Nati Srebro. Equality of opportunity in supervised learning. In
AdvanceS in neural information PrOceSSing systems, pp. 3315-3323, 2016.
David Haussler. Sphere packing numbers for subsets of the boolean n-cube with bounded vapnik-
chervonenkis dimension. J. Comb. Theory, Ser. A, 69(2):217-232, 1995.
Toshihiro Kamishima, Shotaro Akaho, and Jun Sakuma. Fairness-aware learning through regular-
ization approach. In 2011 IEEE 11th International COnference on Data Mining WOrkShops, pp.
643-650. IEEE, 2011.
Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and Jun Sakuma. Fairness-aware classifier
with prejudice remover regularizer. In Joint EUrOPean COnference on Machine Learning and
Knowledge DiScOvery in Databases, pp. 35-50. Springer, 2012.
Svetlana Kiritchenko and Saif M Mohammad. Examining gender and race bias in two hundred
sentiment analysis systems. arXiv PrePrint arXiv:1805.04508, 2018.
Jon Kleinberg, Jens Ludwig, Sendhil Mullainathan, and Ashesh Rambachan. Algorithmic fairness.
In Aea PaPerS and proceedings, volume 108, pp. 22-27, 2018.
Matt J Kusner, Joshua Loftus, Chris Russell, and Ricardo Silva. Counterfactual fairness. In
AdVanceS in neural information PrOceSSing systems, pp. 4066T076, 2017.
Roderick JA Little and Donald B Rubin. StatiSticaI analysis With missing data, volume 793. John
Wiley & Sons, 2019.
Fernando Martlnez-Plumed, CeSar Ferri, David Nieves, and Jose Hernandez-Orallo. Fairness and
missing values. arXiv PrePrint arXiv:1905.12728, 2019.
Northpointe. Compas risk & need assessment system: Selected questions posed by inquiring agen-
cies. 2010.
Luca Oneto, Michele Donini, and Massimiliano Pontil. General fair empirical risk minimization.
arXiv PrePrint arXiv:1901.10080, 2019.
Dana Pessach and Erez Shmueli. Algorithmic fairness. arXiv PrePrint arXiv:2001.09784, 2020.
Geoff Pleiss, Manish Raghavan, Felix Wu, Jon Kleinberg, and Kilian Q Weinberger. On fairness
and calibration. In AdVanceS in NeuraI InfOrmatiOn PrOceSSing Systems, pp. 5680-5689, 2017.
Alvin Rajkomar, Michaela Hardt, Michael D Howell, Greg Corrado, and Marshall H Chin. Ensuring
fairness in machine learning to advance health equity. AnnaIS of internal medicine, 169(12):866-
872, 2018.
Ievgen Redko, Amaury Habrard, and Marc Sebban. Theoretical analysis of domain adaptation with
optimal transport. In Joint EUrOPean COnference on Machine Learning and Knowledge DiScOVery
in Databases, pp. 737-753. Springer, 2017.
Andrew Slavin Ross, Michael C Hughes, and Finale Doshi-Velez. Right for the right reasons: Train-
ing differentiable models by constraining their explanations. arXiv PrePrint arXiv:1703.03717,
2017.
Cynthia Rudin, Caroline Wang, and Beau Coker. The age of secrecy and unfairness in recidivism
prediction. arXiv PrePrint arXiv:1811.00731, 2018.
Candice Schumann, Xuezhi Wang, Alex Beutel, Jilin Chen, Hai Qian, and Ed H Chi. Transfer of
machine learning fairness across domains. arXiv PrePrint arXiv:1906.09688, 2019.
10
Under review as a conference paper at ICLR 2021
Shari Trewin, Sara Basson, Michael Muller, Stacy Branham, Jutta Treviranus, Daniel Gruen, Daniel
Hebert, Natalia Lyckowski, and Erich Manser. Considerations for ai fairness for people with
disabilities. AI Matters, 5(3):40-63, 2019.
Sahil Verma and Julia Rubin. Fairness definitions explained. In 2018 IEEE/ACM International
Workshop on SoftWare Faimess (FairWare), pp. 1-7. IEEE, 2018.
Christina Wadsworth, Francesca Vera, and Chris Piech. Achieving fairness through adversarial
learning: an application to recidivism prediction. arXiv PrePrint arXiv:1807.00199, 2018.
Tianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei Chang, and Vicente Ordonez. Balanced datasets
are not enough: Estimating and mitigating gender bias in deep image representations. In
ProceedingS of the IEEE International Conference on ComPUter ViSion, pp. 5310-5319, 2019.
Muhammad Bilal Zafar, Isabel Valera, Manuel Gomez Rodriguez, and Krishna P Gummadi. Fair-
ness beyond disparate treatment & disparate impact: Learning classification Without disparate
mistreatment. In ProceedingS of the 26th international conference on world Wide web, pp. 1171-
1180, 2017.
Rich Zemel, Yu Wu, Kevin Swersky, Toni Pitassi, and Cynthia Dwork. Learning fair representations.
In International COnferenCe on MaChine Learning, pp. 325-333, 2013.
Han Zhao and Geoff Gordon. Inherent tradeoffs in learning fair representations. In Advances in
neural information PrOCeSSing systems, pp. 15675-15685, 2019.
Han Zhao, Remi Tachet des Combes, Kun Zhang, and Geoffrey J Gordon. On learning invariant
representation for domain adaptation. arXiv PrePrint arXiv:1901.09453, 2019a.
Han Zhao, Amanda Coston, Tameem Adel, and Geoffrey J Gordon. Conditional learning of fair
representations. arXiv PrePrint arXiv:1910.07162, 2019b.
11
Under review as a conference paper at ICLR 2021
A Proof of Theorem 1
We begin by stating and proving the following theorem, which will play an important role in the
proof of Theorem 1. Suppose we have N observations {zi }iN=1 drawn from an arbitrary domain DS
and let
1
Eb(g, ω) :
PN=1 Ri
N
ERiω(zi)∣g(xi) - yi|
i=1
Furthermore we define Dω = ESω(z)2, in this section we use ES to represent that the expectation
is taken with respect to this arbitrary domain DS. Then we have the following result:
Theorem 3. Let H be a hypothesis set with VC-dimension (pseudo dimension in regression setting)
d. Let g be an arbitrary prediction model from hypothesis set H. If Dω ≤ N/8, then, for any δ > 0,
with probability at least 1 - δ, the following bound holds:
品，，、i b 、匕 ∕i28DωCd(N,Dω,δ)	16Cd(N,Dω,δ)B
IEsω(Z)Ig(X) - y(X)I-E(g,ω)∣ ≤ V------------N--------+--------3N---------	⑷
log (d+1)(8e) + + d log 2Nω in classification
log δ (8e)d + 3d log √NDω	in regression
Cd(N,Dω,δ) =
Proof. The proof follows a standard approach in deriving generalization error bound related to VC-
dimension (or pseudo-dimension). Recall that an observation z = {X,y }, we begin by letting
fg(z) := ω(z)Ig(X) - y(X)I and σ2 = Eω 2. Then since g ∈ H, we let F denote the set of fg.
In the rest of the proof, we simply ignore subscription g and let f(X) := ω(z)Ig(X) -y (X)I. This
is possible since the analysis holds for arbitrary g ∈ H, i.e. f ∈ F. We simplify the notation by
defining
Eb(g, ω) = PN f(z)
and
ES ω(z) Ig(X) - y(X)I = Pf(z)
We further let DN denote the training data {(Xi,yi)}iN=1. We also consider a set of “ghost” sample
D0N = {(X0i,yi0)}iN=1 with size N. In reality we do not have access to them but we will make use of
them to prove the theorem. We let fN be the maximizer of IPf - PN f I in F. Similar to PN , we
can define P0N for the ghost samples. Firstly notice that
I (IPfN - PN fN I > t) I (IPfN - P0N fN I < t/2) ≤ I (IP0N fN - PN fN I > t/2)
Taking expecation with respect to the ghost sample yields
I (IPfN - PN fN I > t) PDN0 (IPfN - P0N fN I < t/2) ≤ PD0N (IP0N fN - PN fN I > t/2)
Chebyshev’s inequality gives
PDN (IPfN - PNfN1 > t/2) ≤ 4VNtfN] = N2 ≤ 4DT
where the last inequality is given by the fact that Dω ≥ Eω 2 = σ2 (see detailed argument in Cortes
et al. (2010)). This in turn gives
I (IPfN - PNfN I > t
≤ PDN0 (IP0NfN-PNfNI > t/2)
when t ≥ J8Nω, We have
P sup IPf - PNfI ≥ t ≤ 2P sup IP0N f - PNfI ≥ t/2
We further define F|N = {(f (X1,y1) , . . . , f (XN,yN)) I f ∈ H}. Then
P sup	IP0N f	-	PNfI	≥ t/2	= P sup	IP0N f	-	PNfI	≥	t/2
f∈ ∈F	f	∖∈ ∈F∣2N
(5)
12
Under review as a conference paper at ICLR 2021
Let N1 (t/8, F, 2N ) denote the uniform covering number defined as
N1(t∕8, F, 2N) = max N (t/8, F∣2n,dι)
DN ,D0N	|
(6)
where N t/8, F|2N, d1 is the t/8-covering number of set F|2N with respect to L1 distance. Now
define G ⊆ F|2N as a t/8-cover of F|2N with |G| ≤ N1(t/8, F, 2N). Then
P sup |P0N f - PNf| ≥ t/2 ≤ P sup |P0N g - PNg| ≥ t/4
∖f ∈F∣2N	)	∖g∈G	)
≤ N1(t/8,F, 2N) sUpP (|P0N g - PNg| ≥ t/4)	(7)
g∈G
≤ 2N1(t/8,F, 2N) sUpP (|PN g - Pg| ≥ t/8)
g∈G
Then according to classical bound of covering number (Theorem 1 in Haussler (1995)):
(16e)d
The part remained is analysis of the probability supg∈G P (|PN g - Pg| ≥ t/8). By Bernstein’s
inequality
supP (|PN g - Pg| ≥ t/8) ≤ exp
g∈G
Combining results above yields
-Nt2/128
Dω + tB/24
(8)
P (SUp |Pf - PNf | ≥ t) < 4e(d + 1) (16e) exp
-Nt2/128
Dω + tB/24
≤ 4e(d+1)(16er8DNω! exp
-Nt2/128
Dω + tB/24
Let δ = 4e(d + 1) (lbe^^^ə exp
-Nt2∕128
Dω+tB/24
. Simplify the equation gives
where
Nt2- 16Cd(N,Dω,δ)B
t - 128DωCd(N, Dω, δ) = 0
Cd(N,Dω,δ …包「+ d log^Nω
This equation has non-negative solution
tδ
16Cd(N,Dω,δ)B + J(竽Cd(N,Dω,δ)B)2 + 512NDωCd(N, Dω, δ)
2N
3
≤

16Cd(N, Dω,δ)B
3N	+
128DωCd(N, Dω, δ)
N
Thus we have
1 - δ ≤ P "Pf - PN f∣≤ tδ! ≤ P "Pf - PNf∣≤ ^N^ + ∕HDωcN≡
As an alternative result, we can make use of the Bennett’s inequality instead of Bernstein’s inequal-
ity, which results in
P sUp |Pf - PNf | ≤
f∈F
I 2BCd(N, Dω,δ)
y log(1 + B∕Dω)N
≥1-δ
13
Under review as a conference paper at ICLR 2021
Now we consider the regression case, in which we assume H has pseudo dimension d. Notice that
all the results above hold before equation (7). Now by Theorem 12.2 in Anthony & Bartlett (2009),
we have that
16Ne d
N (t/8, F|2Ν,dl) ≤ N (t/8, F|2N,d∞) ≤ ( td )
when N ≥ d/2. Combining with (8) yields
P sup |Pf - PNf | ≥ t < 2
≤2
16Νe )deχɪɔ ^ -NP /128 )
16Ne / N V - -Nt2/128 ʌ
~ N布)exp Dd+ + tB/24)
Let δ=2(i6Ne q8Nω) eχp( DN+z%)and 加6优
CdND3 ,δ) = log 2( 8e) +32d log √⅛
Following exactly the same procedure as above, we can have
1-δ ≤ P S sup IPf - PN f∣ ≤ tδ! ≤ P S sup IPf - PN f∣ ≤ 空*-+ 产DW-
f∈F	f∈F	3N	N
□
Now we are ready to prove Theorem 1:
Proof. From Theorem 3, notice that for both groups a ∈ {0, 1}:
Ea(g) - ESaω(x)(g(x) - y(x)) = ESa(ω0(x) - ω(x)) Ig(x) - y(x)I	(9)
By triangle inequality, combining (9) and (4) yields that with at least probability 1 - 2δ:
E0(g) - E1 (g) - Eb0(g, ω) - Eb1(g, ω)
≤ X	ESa(ω0(x)	- ω(x)) Ig(x) -	y(x)I	+	ESaω(x) Ig(x) -y(x)I	-	Eba(g,	ω)
a∈{0,1}
≤ X IESa(ωο(x)-ω(x)) ∣g(x) - y(x)∣ I + /128D3Cd(na, DG +≡d(na'D3,δ)B
a∈{0,1}	a	na	3na
(10)
Notice that by definition of total variation distance:
dTV(DTaIIωbDSa) ≥ IIIESa (ω0(x) - ω(x)) Ig(x) - y(x)I III
Finally substituting δ to δ/2 yields the result.
□
B Proof of Theorem 2
Proof. Since we have ω = ω0, we only use ω in the proof for the sake of simplicity. The proof is in-
spired by the technique used in Theorem 9 in Cortes et al. (2010). Assume (x, y) = {(xi, yi)}in=01+n1
is the complete cases, in which n0 of the data belongs to sensitive group A = 0. Let
1 n0	1 n1
φg (x,y) = E0(g,ω)-E1(g,ω) = 一Eω(xo,i,yo,i)∣g(x0,i)-yo,i1F ∑ω(x1,i,y1,i)Ig(x1,i)-y1,iI
0 i=1	1 i=1
14
Under review as a conference paper at ICLR 2021
With g ∈ H. Obviously When true Weights are adopted, We have Eφg(x, y) = E0 (g) - E1(g).
Without loss of generality, we assume no < n`. We let σi2 := VarSi(ω∣g - y|). Furthermore let
σ2 = σ02 + (n0∕n1)σ12. NoW consider U :
E%(x，y)-%(x，y). Notice that
σ
n⅛ σ0 + n⅛ σ2
EZ2
σ2
1
no
(11)
Meanwhile we can split the expectation into
EU 21∣u ∣∈[0,1∕(k√n0)) + EU 21∣u ∣∈[i∕(k√no),u∕√no) + EU 21∣u ∣∈[u∕√ne,+∞)
which is upper bounded by
1	U2 —,.—	,…	.，/, .-、、	-3一
k2no + n^P(u∕√n0 > |U| > 1∕(k√nO)) + EU 1∣u∣∈[u∕√ne,+∞)
Combined with (11) yields
P(U/√no > |U| > V(k√nO)) ≥ k2u2— u0EU21∣u∣∈[u∕√ne,+∞)
Now notice that n°EU21∣u∣∈[u∕√no,+∞) can be written as
(12)
noEU21∣u∣∈[u∕√no,+∞) = / P hno|U|21|u|>√u= > ti dt
u2P |U
u2
oP
Z+∞
2
+∞
P |U| >
工]dt
n0
(13)
P |U| >
2
The probability in the last line can be upper bounded by
d-∖ dt
n0
σ2t
1 n0
|(n-£"(xo,i,yo,i)|g(xo,i) - yo,i∣ - ESeIω(x,y)∣g(x) - y|)|
o i=1
yι,i)lg(xι,i) - yi,i| - ESIω(χ^Ig(X) - y|)| > 2
≤ exp--------------J=
∖ 8σ0+4∕3Bσ Jn
+ exp -
(nι∕no)σ21
+“3Bσq!
no
σ
> 2
where the second inequality is given by Bernstein’s inequality. We now state that
(ni∕no)σ2t
8σ12 + 4∕3B σ
JT ≥ 8 + 4∕3√t
n0
t
To see this, consider two cases σ > σ1 and σ ≤ σ1. If σ > σ1 then
(ni∕no)σ2t
8σ2+4∕3Bσqto
σ2t
t
≥ 8σ2 +4∕3Bσqn ≥ 8 + 4∕3√l
where the second inequality is given by the assumption B2∕σ2 ≤ n0. If σ ≤ σ1
(nι∕no)σ2t
8* + 43B(JqO
t((ni/nO)σ2 +。2) ≥
8σ12
nt0
tσ2
8σ12 + 4∕3Bσ12
~∏L ≥ 8 + 4∕3√t
n0
Similarly------σ2t
≥ 8+4∕3√t.
Notice that √t ≥ 1∕k, take k = 24, We have
P |U| >
用 ≤2exp (-8+
≤ 2 exp
t
t
15
Under review as a conference paper at ICLR 2021
Plug into (13) gives that
noEU21∣u∣∈[u∕√no,+∞) ≤ 2u2 exp
when u = 12, above is smaller than 0.29. In this case, (12) yields
P(|U| > 1/(24√⅛)) > P(u∕√⅛ > |U| > 1/(24√⅛))
Finally we have the observation
575	0.29 _ 7 _ 7 ( 1 Y
576u2 -^W = 10u2 = 10 V12J
(14)
P(|U| > 1/(24√⅛)) = P
- E1(g) - Eb0 (g,
∖ G /	∖
ω) - E1(g, ω)
which completes the proof of first argument. To see the second argument, it can be proven that when
∆b (g ω) ≥ 13 ∕σ0(g,M + σ1(g,ω') > U +1/24 ∕σ2(g,M + σ1(g,ω')
S ,	2	n0	n1	2	n0	n1
the interval
-∆b S (g, ω)
[∆s(g,ω) - u/√n0, ∆s(g,ω)+ u/vw] will never intersect with the interval
-1/(24Vn0), -∆s(g,ω) + 1/(24Vn0)]. Hence we have
P [∣∆τ(g) - ∆ S (g,ω)∣ > ɪʌ ∕σ0⅛)+ σ1⅛
24	n0	n1
> P(u/vn0 > |U| > 1/(24Vn0))
The third argument can be proved in a similar way: when ∆S(g, ω) ≤ 击 J&廿) + %片), We
have that
∆T(g) -	∆b S(g,	ω)∣∣	≥	∣∣	E0 (g) -	E1 (g)	-	Eb0(g,	ω)	- Eb1(g,	ω)	∣∣	-	2∆b S(g,ω)
≥ ɪ σ0(g,ω) + σ2S,M
≥ 72 y	+
n0
n1
□
C Proof of Corollary 2
Proof. When the propensity score model is not accurate, by Theorem 2 we have
I(EO(G- Ei(g))- (E0(g, ω)- Eι(g, M) ∣ ≥ 24Sσ0(n0ωO)+σ⅛ω^	(15)
with probability at least 击(=)2. Now recall the convergence assumption of estimated propensity
score model: ∣ω - ωo∣ = Op((no + nι)-1/2). We have that Dla - D%。= ESaω2 - ESaω2 =
Op((n0 + n1)-1/2) for both groups a ∈ {0, 1}. This yields
S σ2(g,ω)+ σ⅛ω2 = S σ2(g,ω0)+ σ‰j0Γ + Op(n-1∕2 + n-1∕2)	(质)
n0	n1	n0	n1	0	1
Meanwhile, since g is consistent, we have
∣Esaω(x)(g(x) - y(x)) - Eα(g)∣ = ∣Esa(ω - ωο)∣g(x) - y(x)∣∣ = Op((no + nι)-1/2)	(17)
16
Under review as a conference paper at ICLR 2021
for both group.
E0(g) - E1(g) - Eb0(g, ω) - Eb1(g, ω)
≥	ES0ω(x)(g(x) - y(x)) - ES1 ω(x)(g(x) - y(x)) - Eb0(g, ω) - Eb1(g, ω)
-	ES0 ω(x)(g(x) - y(x)) - ES1ω(x)(g(x) - y(x)) - E0(g) - E1(g)
≥ 24不σ0g31 十 σ1(g,ω T(ESOω(χ)(g(x) -y(X)) -ESIω(X)(g(X) -y(X))) - (&⑼-EI(G)I
where the last inequality holds with probability at least ɪ^^. Combining equation (16) and (17), We
know that for arbitrary δ, there exists N0 and N1 such that whenever n0> N0 and n1 > N1, with
probability at least 1 - δ,
III(ES0 ω(X)(g(X) - y(X)) - ES1ω(X)(g(X) - y(X))) - (E0(g, ω) - E1(g, ω))III
σ2(g,ω0) + σ2(g,ω0)
no	nι
Thus with probability at least ɪ^ - δ,
∣(Eo(g) -Eι(g))- (E0(g,ω) -E1(g,ω))∣ ≥ 25 yo (n0ωo)+σ1 (n：o)
which yields the desired argument.
□
17