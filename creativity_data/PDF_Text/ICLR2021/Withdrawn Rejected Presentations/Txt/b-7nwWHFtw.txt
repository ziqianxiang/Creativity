Under review as a conference paper at ICLR 2021
Privacy-preserving Learning via Deep Net
Pruning
Anonymous authors
Paper under double-blind review
Abstract
Neural network pruning has demonstrated its success in significantly im-
proving the computational efficiency of deep models while only introducing
a small reduction on final accuracy. In this paper, we explore an extra
bonus of neural network pruning in terms of enhancing privacy. Specifi-
cally, we show a novel connection between magnitude-based pruning and
adding differentially private noise to intermediate layers under the over-
parameterized regime. To the best of our knowledge, this is the first work
that bridges pruning with the theory of differential privacy. The paper also
presents experimental results by running the model inversion attack on two
benchmark datasets, which supports the theoretical finding.
1	Introduction
Data privacy has become one of the top concerns in the application of deep neural networks,
since there has been an increasing demand to train deep models on private data sets. For
example, hospitals are now training their automated diagnosis systems on private patients’
data Litjens et al. (2016); Lakhani & Sundaram (2017); De Fauw et al. (2018); and adver-
tisement providers are collecting users’ online tra jectories to optimize their learning-based
recommendation algorithm Covington et al. (2016); Ying et al. (2018). These private data,
however, are usually subject to the regulations such as California Consumer Privacy Act
(CCPA) Legislature (2018), Health Insurance Portability and Accountability Act (HIPAA)
Act (1996), and General Data Protection Regulation (GDPR) of European Union.
Differential privacy (DP) Dwork et al. (2006b); Dwork (2009); Dwork & Roth (2014) has
emerged, during the past few years, as a strong standard to provide theoretical privacy
guarantees for algorithms on aggregate databases. The core idea of achieving differential
privacy is to add controlled noise to the output of a deterministic function, such that the
output cannot be used to infer much about any single individual in the database. Recent
years have seen an increasing number of applications that adapt differential privacy mecha-
nisms to address privacy concerns in deep learning Shokri & Shmatikov (2015); Abadi et al.
(2016); Phan et al. (2016); McMahan et al. (2018).
Neural network pruning (or pruning in short), a seemingly orthogonal field to privacy, has
also been the sub ject of a great amount of work in recent years. Pruning aims to reduce the
number of model parameters, such that the compressed model can be applied even under
the memory constraints of the edge-devices. Various pruning techniques have succeeded in
significantly compressing models with little or no loss of accuracy Han et al. (2015; 2016a);
Li et al. (2016); Ding et al. (2018); Evci et al. (2019); Tanaka et al. (2020). However, the
ma jority of existing literature only demonstrate the benefits of pruning in terms of energy
saving and inference speedup, while in this work, we investigate another interesting bonus
of pruning - preserving data privacy.
Our investigation is mainly inspired by the observation that neural network pruning makes
the inversion from hidden-layers harder, as the percentage of remained weight decreases (see
Figure 1). Motivated by this empirical observation, we build under the over-paramterized
regime of deep learning theory, and show an interesting connection between neural network
pruning and adding differentially private noise to intermediate layers. We believe this con-
1
Under review as a conference paper at ICLR 2021
Figure 1: Visualization of inverted CIFAR-10 Krizhevsky (2009) inputs from the third
bottleneck of ResNet-18 He et al. (2016) using the inversion algorithm in Section 5.2. We
prune the network with different k’s, the fraction of remained weights. Inverted images from
networks with fewer weights are visually more different from the original image.
nection may have important practical implications since the pruned model only incurs small
accuracy loss, and we leave that as future work.
We list our contributions as follow:
•	We explore the benefits of pruned neural networks in terms of preserving data
privacy. To the best of our knowledge, this is the first step towards drawing a
theoretical connection between neural network pruning and differential privacy.
•	To build the connection between pruning and adding differentially private noise to
intermediate layers, we generalize the famous anti-concentration inequality due to
Carbery and Wright Carbery & Wright (2001). This generalization might find more
applications in the theoretical analysis of neural network pruning in the future.
•	We provide empirical results in support of our theoretical finding. Specifically, we
demonstrate on two benchmark datasets that pruned neural networks are more
secure in the sense that running the model inversion attack becomes harder.
Roadmap. The rest of this paper is organized as follow. Section 2 covers existing literature
in different privacy, neural network pruning, and over-parameterized deep learning theory.
Section 3 provides theoretical preliminaries and Section 4 presents our main theoretical
result. Section 5 shows empirical results on MNIST and CIFAR-10 benchmarks that are in
support of our theoretical findings. We conclude this work in Section 6.
2	Related work
Neural network pruning Traditional deep neural network models are computationally
expensive and memory intensive, which hinders their deployment in applications with limited
memory resources or strict latency requirements. Many progress has been made to perform
model compression in deep networks, including low-rank factorization Sainath et al. (2013);
Lebedev et al. (2015), network pruning LeCun et al. (1990); Srinivas & Babu (2015); Han
et al. (2016b); Li et al. (2017), and knowledge distillation Hinton et al. (2015); Chen et al.
(2017). Among them, neural network pruning has been widely adopted because it is able
to reduce model sizes by up to one order of magnitude without significant accuracy loss.
The idea of network pruning dates back to the Optimal Brain Damage in 1990s LeCun
et al. (1990). Recently, it has been shown that removing the weights with low magnitude
can also achieve a highly compressed model Han et al. (2016b), which is referred to as
‘magnitude-based pruning’.
Differential privacy The concept of e-differential privacy WaS originally introduced by
Dwork, McSherry, Nissim and Smith Dwork et al. (2006b). Later, it was generalized to a
relaxation of (e, δ)-differential privacy DWork et al. (2006a); DWork (2009); DWork & Roth
(2014). Differential privacy haS been SucceSSfully applied to many problemS. For more
detailed SurveyS of the applicationS of differential privacy, We refer the readerS to DWork
(2008; 2011). Applying differential privacy techniqueS in deep learning iS an intereSting but
non-trivial taSk. PreviouS reSearch have cuStomized differential privacy for different learning
taSkS and SettingS Shokri & Shmatikov (2015); Abadi et al. (2016); Phan et al. (2016).
Although there are exiSting StudieS about applying differential privacy in neural netWork,
but there iS little exploration on preSenting differential privacy uSing prune netWork yet.
2
Under review as a conference paper at ICLR 2021
To the best of our knowledge, this paper is the first work that shows a connection between
differential privacy and pruned neural network.
Over-parameterized deep learning theory Recently, there is super long line of
work focusing showing the convergence of deep neural network training under over-
parameterization regime Li & Liang (2018); Du et al. (2019); Allen-Zhu et al. (2019b;c;a);
Arora et al. (2019a;b); Song & Yang (2019); Oymak & Soltanolkotabi (2020); Brand et al.
(2020). The theory suggested as long as the neural network is sufficiently wide, i.e.,
m ≥ poly(n, d, 1∕δ, L) then running (stochastic) gradient descent algorithm is able to find
the global minimum, where n is the number of input data points, d is the dimension of data,
δ is the minimum `2 distances between all pairs and L is the number of layers in neural
network.
However, unlike the above classical deep learning convergence theory, this work explored
the over-parameterized theory in a very different perspective, e.g. privacy. Our result is
not an optimization result which indicating neural network can learn a set of input data
points in certain sense, however our result is suggesting neural network can be private in
the differential privacy sense.
3	Backgrounds
Notations. For a positive integer n, We use [n] to denote set {1, 2, •一，n}. For vector
x ∈ Rn, we use kxk1 to denote Pn |xi |, kxk2 to denote (Pn x2)1/2, kxk∞ to denote
maxi∈[n] |xi|. We use N(μ, σ2) to denote random Gaussian distribution. For a matrix A,
We use kAk to denote its spectral norm.
This section presents some backgrounds before theoretically establishing the equivalence be-
tWeen magnitude-based pruning and adding differentially private noise in Section 4. Section
3.1 revisits the notion of (edp, δdp)-differential privacy. Section 3.2 describes the magnitude
pruning algorithm.
3.1	Differential privacy
The classical definition of differential privacy is shoWn as folloW:
Definition 3.1 ((edp, δdp)-differential privacy DWork et al. (2006a)). For a randomized
function h(x), we say h(x) is (edp, δdp)-differential privacy if for all S ⊆ Range(h) and for
all x, y with kx - yk1 ≤ 1 we have
Pr[h(x) ∈ S] ≤ exp&p) ∙ Pr[h(y) ∈ S] + δdp.
Definition 3.1 says that, if there are tWo otherWise identical records x and y, one With
privacy-sensitive information in it, and one Without it, and We normalize them such that
kx - y k1 ≤ 1. Differential Privacy ensures that the probability that a statistical query
Will produce a given result is nearly the same Whether it’s conducted on the first or second
record. Parameters (edp, δdp) are called the privacy budget, and smaller edp and δdp provide
a better differential privacy protection. One can think of a setting Where both parameters
are 0, then the chance of telling Whether a query result is from x or from y is no better than
a random guessing.
A standard strategy to achieve differential privacy is by adding noise to the the original
data x or the function output h(x). In order to analyze it, We need the folloWing definition:
Definition 3.2 (Global Sensitivity DWork et al. (2006b)). Let f : Rn → Rd, define GSp (f),
the `p global sensitivity of f ,forallx, y with kx - yk1 ≤ 1 as
GSp(f) = sup kf (x) - f(y)kp.
x,y∈Rn
The global sensitivity of a function measures hoW ‘sensitive’ the function is to slight changes
in input. The noise needed for differential privacy guarantee is then calibrated using some
3
Under review as a conference paper at ICLR 2021
well-known mechanisms, e.g., Laplace or Gaussian Dwork & Roth (2014), and the amount of
noise (the standard deviation of the noise distribution) is proportional to the sensitivity, but
inversely proportional to the privacy budget edp. That is to say, for a given function with
fixed global sensitivity, a larger amount of noise is required to guarantee a better differential
privacy (one with a smaller budget edp).
3.2	Magnitude-based pruning
Magnitude-based pruning Han et al. (2016a) compresses a neural network by removing
connections with smallest-magnitude weights (usually determined by using a threshold, say
α). The pruning procedure starts with a trained (dense) network f (Wι, •一,Wl), where L
is its depth, and Wl ,l ∈ [L] is the weight of its i-th layer. For each layer Wl , it sets the
weights with magnitudes smaller than a to zero:
(Wl)i,j — (Wl)i,j ∙ 1∣(Wι)i,j∣>a,∀i,j
We then run model update after the pruning step.
4Mainresult
We start by formulating the connection between pruning and adding differentially private
noise. We define the following notions to describe the closeness between a randomized
function and a given function, which can be either randomized or deterministic.
Definition 4.1 ((eap, δap)-close). For a pair of functions g : Rd → Rm and h : Rd → Rm,
and a fixed input x, we say g(x) is (e, δ)-close to h(x) if and only if,
Pr	kg(X)- h(X)k2 ≤ e
g,h
≥ 1 - δ.
(eap , δap )-closeness basically requires that, `2 distance between two functions’ output with
a given input is small enough (kf - gk2 := ( |f (X) - g(X)|2dX)1/2). When applying the
definition to deep neural network, we view m as the width of deep neural network (e.g. the
number of neurons).
Let φ(t) denote the activation function. In this work, we focus on ReLU case where φ(t) =
max{t, 0}. Without loss of generality, our techniques can be generalized to other activation
functions. We present our main theoretical result in Theorem 4.2. The main message of our
theorem is that, as long as the neural network is sufficiently wide, pruning neural network
has a similar effect to adding differentially private noise.
Theorem 4.2 (Main result, informal of Theorem E.1). Let σa = O(edpδdp∕(m2)). For
a fully connected neural network (each layer can be viewed as f(X) = φ(AX + b)), where
kXk2 =1 and X ∈ Rd≥ . Applying magnitude-based pruning on the weight A ∈ Rm×d (where
each Ai,j 〜N(0, σ2)) gives Us A ∈ R .
If m = Ω(poly(1∕eap, log(1∕δap), log(1∕δdp))), then there exists a function h(x) satisfying
two properties :
1.	h(X) is (edp,δdp)-differential privacy on input X;
2.	h(X) is (eap, δap)-close to g(X) = φ(AX + b).
In the above theorem, we denote d as the input data dimension. φ is the activation function,
e.g., φ(z) = max{z, 0}. In this work, we focus on one hidden layer neural network.1
1 We would like to emphasize that one hidden layer is not just a toy example, but a natural
and standard situation to study theory, see Zhong et al. (2017b;a); Li & Yuan (2017); Li & Liang
(2018); Du et al. (2019); Song & Yang (2019); Brand et al. (2020); Bubeck et al. (2020) for example.
Usually, if a proof holds for one-hidden layer, generalizing it to multiple layers is straightforward
Allen-Zhu et al. (2019b;c)
4
Under review as a conference paper at ICLR 2021
Regarding the two properties of h(x), property 1 requires h(x) to provide (edp, δdp)-
differential privacy, and property 2 requires that h(x) is similar to magnitude-based pruning
with the predefined (eap,δap)-close notation.
m×d
Proof Sketch Let A ∈ Rm×d denote the weight matrix after magnitude-based pruning,
and A = A - A ∈ Rm×d. We define a noise vector e ∈ Rm as the follow:
e = Lap(1, σ)m ◦ (Ax).
The main proof can be split into two parts in correspondence to property 1 and 2 respec-
tively: Claim 4.3 and Claim 4.4.
Claim 4.3. Let h(x)=f(x)+e ∈ Rm, we can show that h(x) is
(edp ,δdp) - differential privacy.
Claim 4.4. For sufficiently large m, we have
Pr h √m ke - Axk2 ≥ eap] ≤ δap .
Proof sketch of Claim 4.3 To prove Claim 4.3, we anchor from the definition of differ-
ential privacy. Recall Def. 3.1,(edp,δdp)-differential privacy requires that for any inputs x
and y with kx - yk ≤ 1,
Pr[h(x) ∈ S] ≤ exp(edp) ∙ Pr[h(y) ∈ S] + δdp.
h
To be more specific, we use the fact that the noise e is sampled from the Laplace distribution,
and try to bound the ratio
Ph(h(x) = t ∈ S)
Ph(h(y) = t ∈ S)
where p(∙) denotes the probability density function. To bound the above ratio: first we need
to derive and upper-bound the global sensitivity (see Appendix D) of a single-layer neural
network.
Recall the well-known anti-concentration result by Carbery & Wright (2001).
Lemma 4.5 (Carbery and Wright Carbery & Wright (2001)). Let p : Rd → R denote a
degree-k polynomial with d variables. There is a universal constant C>0 such that
Pr
X 〜N (0,id)
[|p(x)| ≤ δPVar[p(x)f∣
≤ C ∙ 61/k.
Another contribution in this work is that we extend the anti-concentration result Carbery
& Wright (2001) to a more general setting, which has not been explored in literature. We
state our generalization as follows2
Lemma 4.6 (An variation of Carbery & Wright (2001), Anti-concentration of sum of trun-
cated Gaussians). Let xι,•一,Xn be n i.i.d. zero-mean Gaussian random variables N(0,1).
Let p : Rn → R denote a degree-1 polynomial defined as
n
P(xι,…，Xn) = Eaixi.
Let f denote a truncation function where f (x) = x if |x| ≤ a, and f (x) = 0 if |x| > a. Then
we have
Pr
X〜N (0,Id)
[|p(f (x))| ≤ min{a,0.1} ∙ δ ∙ ∣∣α∣,
≥ C ∙ δ.
Once the densities are bounded, integrating p(∙) yields the requirement of differential privacy,
thus complete the proof of part 1.
2 for more details of the proof, we refer the readers to Appendix C
5
Under review as a conference paper at ICLR 2021
Proof sketch of Claim 4.4 To prove Claim 4.4, We firstly define Zi = ei — (Ax)i. Then
we apply the concentration theorem (see Appendix B) to show that for any kxk2 = 1 and
x ∈ Rd ,
m
Pr [τ X(Zi- E[zi])| ≥ e2p] ≤ δap,
m i=1
Which completes the proof of Claim 4.4.
In certain, the proof is mainly a sophisticated combination of the folloWing concentration
inequalities
•	Lemm B.1 shoWs the concentration of the `2 norm of a random truncated Gaussian
vector
•	Lemma B.2 shoWs the concentration of matrix vector multiplication.
•	Lemm B.3 bounds the inner product betWeen a random Guassian vector and a fixed
vector
•	Lemma B.4 bounds inner product betWeen tWo random Guassian vectors
•	Lemma B.5 shoWs the concentration of folded Gaussian random vectors.
5	Experiments
This section presents experimental results in support of our theoretical finding in Section
4 by ansWering this question: does the pruned model preserve better privacy than
the dense one? We describe experimental setups in Section 5.1 and the evaluation for
privacy in Section 5.2. We summarize results in Section 5.3.
5.1	Experimental setup
Datasets and model architectures. We have conducted image classification experi-
ments on MNIST LeCun et al. (2010) and CIFAR-10 Krizhevsky (2009) benchmarks.
For netWork architectures, We have used LeNet-5 LeCun et al. (1998) for MNIST, and
ResNet-18 He et al. (2016) for CIFAR-10, With PyTorch Paszke et al. (2019) as the experi-
ment platform. We have used SGD Qian (1999) With learning rate 0.05 and momentum 0.9
for both models, and train LeNet-5 for 20 epochs and ResNet-18 for 150 epochs. All models
are trained on 8 NVIDIA GeForce RTX 2080 Ti GPUs With batch size 256.
Pruning algorithm. We have employed the iterative pruning technique, Which repeatedly
prunes and retrains the netWork over n rounds: in each round, We prune p fraction of the
Weights that survive the previous round and retrain for t epochs. We use k to denote the
fraction of Weights remained in the final sparse model, thus We have k =(1- p)n . In our
experiments, We set p = 20% and t = 5, and vary n ∈ [15] to get pruned netWorks With
different k’s.
5.2	Test of privacy leakage as a model inversion attack
We have used the attack-based evaluation to investigate Whether pruning could preserve
more privacy (i.e., suffer less “privacy leakage” under the attack). We have adopted the
model inversion attack Mahendran & Vedaldi (2015) to shoW the privacy leakage of a given
l-layer neural network f (Wι, ∙∙∙ ,Wi), where Wi, i ∈ [l] is the weight of its i-th layer.
Let us conceptualize the mapping from the f’s input to the output of its i-th intermediate as
a representation function Φ : Rd → Rm . The model inversion attack captures the potential
privacy leakage of Φ when applied on some input x:
Given the representation Φ(x), and the weights in the public mapping Φ, the attacker’s goal
is to find the preimage of Φ(x), namely
x* = arg min L(Φ(χ), Φ(z)) + λR(z)
z∈Rd
6
Under review as a conference paper at ICLR 2021
k(%)	100.0	80.0	64.0	51.2	41.0	32.8	26.2	21.0
MNIST	99.0	98.9	99.0	99.0	99.0	99.0	99.0	98.9
CIFAR-10	94.1	93.8	93.6	93.4	93.0	92.5	91.9	90.7
k(%)	16.8	13.4	10.7	8.6	6.9	5.5	4.5	3.5
MNIST	98.8	98.8	98.7	98.4	98.1	98.0	97.5	97.5
CIFAR-10	89.8	88.2	86.3	83.3	75.9	72.4	61.9	10.0
Table 1: Test accuracy (%) achieved on MNIST
network. k is the fraction of weights remained.
and
CIFAR-10 when
pruning a dense
16.8%	10.7%
98.8	98.7
k≡100.0%	64.0%
Oriainal Acc=99.0	99.0
41.0%	26.2%
99.0	99.0
4.5%
97.5
>uoo Eoo E zoL 8ll
(a) MNIST
Vloo

(b) CIFAR-10
Figure 2: Recovered MNIST digits (a) and CIFAR-10 samples (b) by running the model
inversion attack in Section 5.2 on LeNet-5 and ResNet-18 models trained with pruning. We
test different layers (different rows) and fractions of remained weights (k, different columns).
The naming of layers is explained in Section 5.3.
where the loss function L is defined as L(a, a0)= ka - a0 k2 . λ>0 is the regularization
parameter, and the regularization function R in our case is the total variation of a 2D signal:
R(a) = Pi,j ((ai+1,j - ai,j)2 + (ai,j+1 - ai,j)2)1/2.
5.3	Results
We have evaluated the model inversion attack on different representation function Φ's by
7
Under review as a conference paper at ICLR 2021
1.	Pruning the neural network with different k’s, the fraction of remained weights,
where k =(1- 20%)n, n ∈ [15]
2.	Selecting different intermediate layers to invert. Specifically, for LeNet-5, we run the
attack on all 5 layers, namely {‘Conv1’, ‘Conv2’, ‘FC1’, ‘FC2’, ‘FC3’}. For ResNet-
18, we pick 5 layers {‘Conv2-1’, ‘Conv2-2’, ‘Conv3-1’, ‘Conv3-2’, ‘Conv4-1’}, where
‘Convi-j ’ stands for the j-th convolutional group of the i-th convolutional block.
Now we report that our experimental results strongly suggest that magnitude-based pruning
preserves better privacy (suffer less leakage) than its dense counterpart.
Accuracy of network pruning. Table 1 shows the accuracy results of MNIST and
CIFAR-10 tasks for the network pruning approach whose networks have different k percent-
ages of remaining weights. The task on MNIST achieves the same level of accuracy as the
network model without pruning when k, the fraction of remained weights ≥ 10%. Its accu-
racy gradually decreases as more weights get removed. The test with CIFAR-10 maintains
the same accuracy as or better than the model without pruning when k ≥ 40%, and then
gradually decreases as k decreses.
Note that the model accuracy achieved with the pruning algorithm in our experiments may
be lower than that of the SOTA model with the same sparsity. However, for answering the
question if network pruning preserves privacy, our experiments can be viewed as conservative
results.
Sparser networks suffer less privacy leakage. Figure 2 visualized the inverted samples
under different choices of layers and fractions of remained weights. Each column of Figure 2
suggests the increasing difficulty of inverting deeper layers.
A more important observation is that, for all layers, we consistently observe that the privacy
leakage gradually decreases as k increases: for deep layers (e.g. ‘FC2’ for LeNet-5 and
‘Conv4-1’ for ResNet-18), though the inverted image from the dense model (i.e. k = 100%)
may look quite identical to the original image without pruning, it is no longer true when
k ≤ 90%). This agrees with Theorem 4.2, which suggests that under the over-parameterized
regime, pruning yields a similar effect to adding differentially private noise and thus preserves
better privacy.
6	Conclusions
This paper has presented a theoretical result to show that, if a fully-connected layer of
a neural network is wide enough, there is a connection between magnitude-based neural
network pruning and adding differentially private noise to the model’s intermediate outputs.
Empirical results on two benchmark datasets support our theoretical findings.
These results have strong practical implications for two reasons. First, since neural network
pruning has the property that the fraction of removed weights can be quite high (e.g. > 90%)
without reducing inference accuracy, it strongly suggests that network pruning can be an
effective method to achieve differential privacy without any or much reduction of accuracy.
Second, although the result is for a single layer of a neural network, it is quite natural in
a distributed or federated learning system to use a particular layer to communicate among
multiple sites.
Several questions remain open. First, Theorem 4.2 is only for a single-layer fully connected
network, and it would be interesting if one can extend it to multi-layer settings and also
convolutional neural networks. Second, our theoretical finding is based on the worst case
analysis, which means in most cases, m can be much smaller. How to efficiently determine
m for different settings requires more investigation. Finally, in order to use network pruning
as a mechanism to preserve privacy in a practical distributed or federated learning system,
one needs to consider many design details including which layers to prune, whether or not
to prune layers with the same sparsity, where the work of pruning should be performed, and
how to coordinate among multiple sites.
8
Under review as a conference paper at ICLR 2021
References
Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal
Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the
2016 ACM SIGSAC Conference on Computer and Communications Security, pp. 308-
318, 2016.
Accountability Act. Health insurance portability and accountability act of 1996. Public law,
104:191, 1996.
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang. Learning and generalization in overpa-
rameterized neural networks, going beyond two layers. In Advances in neural information
processing systems (NeurIPS), pp. 6155-6166, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In ICML. https://arxiv.org/pdf/1811.03962.pdf, 2019b.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent
neural networks. In NeurIPS. https://arxiv.org/pdf/1810.12065.pdf, 2019c.
Richard Aoun, Marwa Banna, and Pierre Youssef. Matrix poincare inequalities and concen-
tration. In arXiv preprint. https://arxiv.org/1910.13797.pdf, 2019.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. In
International Conference on Machine Learning (ICML), pp. 322-332. https://arxiv.
org/pdf/1901.08584.pdf, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong
Wang. On exact computation with an infinitely wide neural net. arXiv preprint
arXiv:1904.11955, 2019b.
Lavrentin M Arutyunyan and Egor D Kosov. Deviation of polynomials from their expecta-
tions and isoperimetry. Bernoulli, 24(3):2043-2063, 2018.
Sergei Bernstein. On a modification of chebyshev’s inequality and of the error formula of
laplace. Ann. Sci. Inst. Sav. Ukraine, Sect. Math, 1(4):38-49, 1924.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over-
parametrized) neural networks in near-linear time. arXiv preprint arXiv:2006.11648,
2020.
SebaStien Bubeck, Ronen Eldan, Yin Tat Lee, and Dan Mikulincer. Network size and weights
size for memorization with two-layers neural networks. arXiv preprint arXiv:2006.02855,
2020.
Anthony Carbery and James Wright. Distributional and Lq norm inequalities for polyno-
mials over convex bodies in Rn. Mathematical research letters, 8(3):233-248, 2001.
Guobin Chen, Wongun Choi, Xiang Yu, Tony Han, and Manmohan Chandraker. Learning
efficient object detection models with knowledge distillation. In NIPS, 2017.
Herman Chernoff. A measure of asymptotic efficiency for tests of a hypothesis based on the
sum of observations. The Annals of Mathematical Statistics, pp. 493-507, 1952.
Kevin P Costello, Terence Tao, and Van Vu. Random symmetric matrices are almost surely
nonsingular. Duke Mathematical Journal, 135(2):395-413, 2006.
Paul Covington, Jay Adams, and Emre Sargin. Deep neural networks for youtube recom-
mendations. In Proceedings of the 10th ACM conference on recommender systems, 2016.
Jeffrey De Fauw, Joseph R Ledsam, Bernardino Romera-Paredes, Stanislav Nikolov, Nenad
Tomasev, Sam Blackwell, Harry Askham, Xavier Glorot, Brendan O’Donoghue, Daniel
Visentin, et al. Clinically applicable deep learning for diagnosis and referral in retinal
disease. Nature medicine, 24(9):1342, 2018.
9
Under review as a conference paper at ICLR 2021
Xiaohan Ding, Guiguang Ding, Jungong Han, and Sheng Tang. Auto-balanced filter pruning
for efficient convolutional neural networks. In AAAI, volume 3, pp. 7, 2018.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. In ICLR. https://arxiv.org/pdf/1810.
02054.pdf, 2019.
Cynthia Dwork. Differential privacy: A survey of results. In International conference on
theory and applications of models of computation, pp. 1-19. Springer, 2008.
Cynthia Dwork. The differential privacy frontier. In Theory of Cryptography Conference,
pp. 496-502. Springer, 2009.
Cynthia Dwork. A firm foundation for private data analysis. Communications of the ACM,
54(1):86-95, 2011.
Cynthia Dwork and Aaron Roth. The algorithmic foundations of differential privacy. Foun-
dations and Trends® in Theoretical Computer Science, 9(3-4):211-407, 2014.
Cynthia Dwork, Krishnaram Kenthapadi, Frank McSherry, Ilya Mironov, and Moni Naor.
Our data, ourselves: Privacy via distributed noise generation. In Annual International
Conference on the Theory and Applications of Cryptographic Techniques, pp. 486-503.
Springer, 2006a.
Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to
sensitivity in private data analysis. In Theory of cryptography conference, pp. 265-284.
Springer, 2006b.
Paul Erdos. On a lemma of littlewood and offord. Bulletin of the American Mathematical
Society, 51(12):898-902, 1945.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the
lottery: Making all tickets winners. arXiv preprint arXiv:1911.11134, 2019.
Ankit Garg, Yin-Tat Lee, Zhao Song, and Nikhil Srivastava. A matrix expander chernoff
bound. In STOC. https://arxiv.org/pdf/1704.03864, 2018.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neu-
ral networks with pruning, trained quantization and huffman coding. arXiv preprint
arXiv:1510.00149, 2015.
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A Horowitz, and
William J Dally. Eie: efficient inference engine on compressed deep neural network. In
ISCA, 2016a.
Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding. In ICLR. https://
arxiv.org/1510.00149.pdf, 2016b.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.
In arXiv preprint. https://arxiv.org/pdf/1503.02531.pdf, 2015.
Wassily Hoeffding. Probability inequalities for sums of bounded random variables. Journal
of the American Statistical Association, 58(301):13-30, 1963.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report,
Citeseer, 2009.
Rasmus Kyng and Zhao Song. A matrix chernoff bound for strongly rayleigh distributions
and spectral sparsifiers from a few random spanning trees. In FOCS. https://arxiv.
org/pdf/1810.08345, 2018.
10
Under review as a conference paper at ICLR 2021
Paras Lakhani and Baskaran Sundaram. Deep learning at chest radiography: automated
classification of pulmonary tuberculosis by using convolutional neural networks. Radiology,
284(2):574-582, 2017.
Beatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by
model selection. Annals of Statistics, pp. 1302-1338, 2000.
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.
Speeding-up convolutional neural networks using fine-tuned cp-decomposition. In ICLR.
https://arxiv.org/pdf/1412.6553.pdf, 2015.
Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In NIPS, 1990.
Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. In ATT
Labs, volume 2. http://yann.lecun.com/exdb/mnist, 2010.
California State Legislature. California consumer privacy act (ccpa). https://oag.ca.gov/
privacy/ccpa, 2018.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters
for efficient convnets. arXiv preprint arXiv:1608.08710, 2016.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters
for efficient convnets. In ICLR. https://arxiv.org/pdf/1608.08710.pdf, 2017.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochas-
tic gradient descent on structured data. In Advances in Neural Information Processing
Systems, pp. 8157-8166, 2018.
Yuanzhi Li and Yang Yuan. Convergence analysis of two-layer neural networks with ReLU
activation. In Advances in neural information processing systems (NIPS), pp. 597-607.
https://arxiv.org/pdf/1705.09886.pdf, 2017.
Geert Litjens, Clara I Sanchez, Nadya Timofeeva, Meyke Hermsen, Iris Nagtegaal, Iringo
Kovacs, Christina Hulsbergen-Van De Kaa, Peter Bult, Bram Van Ginneken, and Jeroen
Van Der Laak. Deep learning as a tool for increased accuracy and efficiency of histopatho-
logical diagnosis. Scientific reports, 6:26286, 2016.
John Edensor Littlewood and Albert Cyril Offord. On the number of real roots of a random
algebraic equation (iii). Rec. Math. [Mat. Sbornik] N.S., 12(3):277-286, 1943.
Aravindh Mahendran and Andrea Vedaldi. Understanding deep image representations by
inverting them. In CVPR. https://arxiv.org/pdf/1412.0035.pdf, 2015.
H Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially
private recurrent language models. In International Conference on Learning Representa-
tions, 2018.
Raghu Meka, Oanh Nguyen, and Van Vu. Anti-concentration for polynomials of independent
random variables. In Theory Of Computing. https://arxiv.org/pdf/1507.00829, 2017.
Assaf Naor, Shravas Rao, and Oded Regev. Concentration of markov chains with bounded
moments. In arXiv preprint. https://arxiv.org/pdf/1906.07260.pdf, 2019.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global
convergence guarantees for training shallow neural networks. In IEEE Journal on Selected
Areas in Information Theory. https://arxiv.org/pdf/1902.04674.pdf, 2020.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,
Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imper-
ative style, high-performance deep learning library. In NeurIPS, pp. 8024-8035, 2019.
11
Under review as a conference paper at ICLR 2021
NhatHai Phan, Yue Wang, Xintao Wu, and Dejing Dou. Differential privacy preservation
for deep auto-encoders: an application of human behavior prediction. In AAAI, 2016.
Ning Qian. On the momentum term in gradient descent learning algorithms. Neural net-
works, 12(1):145-151, 1999.
Alexander Razborov and Emanuele Viola. Real advantage. ACM Trans. Comput. Theory,
5(4), November 2013.
Tara N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana Ramabhad-
ran. Low-rank matrix factorization for deep neural network training with high-dimensional
output targets. In IEEE international conference on acoustics, speech and signal process-
ing, pp. 6655-6659, 2013.
Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the
22nd ACM SIGSAC conference on computer and communications security, pp. 1310-1321.
ACM, 2015.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff
bound. In arXiv preprint. https://arxiv.org/pdf/1906.03593.pdf, 2019.
Zhao Song, Xi Yang, and Ruizhe Zhang. Joint anti-concentration for random gaussian
polynomials. In Manuscript, 2020.
Suraj Srinivas and R Venkatesh Babu. Data-free parameter pruning for deep neural net-
works. In BMVC. https://arxiv/pdf/1507.06149.pdf, 2015.
Hidenori Tanaka, Daniel Kunin, Daniel LK Yamins, and Surya Ganguli. Pruning neu-
ral networks without any data by iteratively conserving synaptic flow. arXiv preprint
arXiv:2006.05467, 2020.
Joel A Tropp. An introduction to matrix concentration inequalities. Foundations and
Trends® in Machine Learning, 8(1-2):1-230, 2015.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In
KDD, 2018.
Kai Zhong, Zhao Song, and Inderjit S Dhillon. Learning non-overlapping convolutional
neural networks with multiple kernels. arXiv preprint arXiv:1711.03440, 2017a.
Kai Zhong, Zhao Song, Prateek Jain, Peter L. Bartlett, and Inderjit S. Dhillon. Recovery
guarantees for one-hidden-layer neural networks. In ICML. https://arxiv.org/pdf/
1706.03175.pdf, 2017b.
Michael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning
for model compression. In ICLR (Workshop), 2017.
12