Under review as a conference paper at ICLR 2021
Temperature Regret Matching for Imperfect-
Information Games
Anonymous authors
Paper under double-blind review
Ab stract
Counterfactual regret minimization (CFR) methods are effective for solving two
player zero-sum extensive games with imperfect information. Regret matching
(RM) plays a crucial role in CFR and its variants to approach Nash equilibrium.
In this paper, we present Temperature Regret Matching (TRM), a novel RM algo-
rithm that adopts a different strategy. Also, we consider not only the opponent’s
strategy under the current strategy but also the opponent’s strategies of the several
last iterations for updating the external regret of each iteration. Furthermore, we
theoretically demonstrate that the update of TRM converges to Nash Equilibrium.
Competitive results in imperfect-information games have verified its effectiveness
and efficiency.
1	Introduction
Games in extensive form provide a mathematical framework for modeling the sequential decision-
making problems with imperfect information. We focus on solving poker games, a common bench-
mark for two-player zero-sum imperfect-information games. For these games, the goal is to find a
Nash Equilibrium (NE) (Nash, 1950).
The most popular algorithms to solve this problem are variants of counterfactual regret minimiza-
tion (CFR) (Zinkevich et al., 2007; Lanctot et al., 2009; Gibson et al., 2012). In pariticular, the
development of CFR+ (Tammelin, 2014; Tammelin et al., 2015) and LCFR Brown & Sandholm
(2019) provides stronger baselines than vanilla CFR. CFR+ was used to solve heads-up limit Texas
hold’em (Bowling et al., 2015) and heads-up no-limit Texas hold’em (HUNL) (Moravcik et al.,
2017; Brown & Sandholm, 2018). It helps agents defeat poker professionals. LCFR accelerates the
learning process and outperforms CFR+ in HUNL subgames and 5-card Goofspiel. These immedi-
ate counterfactual regrets are defined by the counterfactual rewards and can be iteratively minimized
by regret matching (RM) (Blackwell, 1956; Hart & Mascolell, 2000; Abernethy et al., 2011) or
AdaHedge (Freund & Schapire, 1997; De Rooij et al., 2014).
In this work, we first focus on RM in CFR. We prove that any strategies which are inversely pos-
itively to the external regret can help the CFR algorithm converge to an ε-equilibrium (Nesterov,
2005). Furthermore, we prove that any combinatorial regret matching methods converge to an ε-
equilibrium. Based on the above conclusion, we propose Temperature Regret Matching (TRM),
which is a method to change the matching weight of external regret according to the number of
iterations.
Secondly, we consider that the strategy at this iteration must be able to win the last iteration because
regret matching methods increase the action weight to obtain the expected return. Inspired by Opti-
mistic CFR variants (Syrgkanis et al., 2015; Brown & Sandholm, 2019) which the last iteration is
counted twice when calculating the strategy can lead to substantially faster convergence , we con-
sider not only the opponent’s strategy under the current strategy but also the opponent’s strategy of
the last iteration for the external regret of each iteration. Also, we give the reason for this regret
matching method.
To summarize, the main contributions of this work are listed bellow in three-fold:
1
Under review as a conference paper at ICLR 2021
•	We prove that any strategies which are inversely positively to the external regret can help
the CFR algorithm converge to an ε-equilibrium. Furthermore, we prove that any combi-
natorial regret matching methods converge to an ε-equilibrium.
•	We propose a new regret matching method Temperature Regret Matching (TRM) which
adopts a different strategy to obtain the output strategies, which helps the average strategy
converges faster.
•	We consider not only the opponent’s strategy under the current strategy but also the oppo-
nent’s strategy of the last moment for the external regret of each iteration, which further
improves the learning efficiency.
We empirically evaluate TRM in the vanilla CFR (Zinkevich et al., 2007), CFR+ (Tammelin, 2014)
, LCFR (Brown & Sandholm, 2019), and Optimistic CFR on the standard benchmarks Leduc
Hold’em. Extensive experimental analyses and comparisons demonstrate the effectiveness of Tem-
perature Regret Minimization.
2	Notations and Perliminaries
In this section, we first introduce the notations and definitions of imperfect-information games in
extensive form. Then we introduce Best Response (BR) and Nash Equilibrium (NE). Also, we
introduce Regret Matching (RM), CFR , CFR+, Linear CFR, and Optimistic CFR.
2.1	Notations
Extensive games compactly model the decision-making problems with sequential interactions a-
mong multiple agents. In extension form games, there is a finitset of players: P . An extension form
game can be represented by a game tree H of histories, where a history h is a sequence of actions
in the past. A player function P assigns a member of P ∪ c to each non-terminal history, where c is
the chance. A(h) is the actions available at a history and P(h) is the player who acts at this history.
If P(h) = c then chance determines the action taken after history h. If a player take an action a at
history h and reach an new history h0, We represent this as h ∙ a = h0. Z ⊆ H are terminal histories
in which no actions are available. For each player i ∈ P, there is a payoff function ui : Z → R. 4i
is the range of payoffs reachable by player i. Furthermore, 4i = maxz∈Z ui(z) - minz∈Z ui(z)
and 4 = maxi 4i. If P = 1, 2 and u1 + u2 = 0, the game is two-player zero-sum.
In imperfect-information games, imperfect information is denoted by information set(infoset) Ii for
each player i ∈ P. All states h ∈ Ii are indistinguishable to i. Let A = maxh |A(h)|. A strategy of
player i is a function σi for player i in infoset I . The probability of a particular action a is denoted
by σi(I, a). Since all histories in an information set belonging to player i are indistinguishable, the
strategies in each of them must be identical. For all h ∈ I, σi(h) = σi(I) and σi(h, a) = σi(I, a).
We define σi to be a probability vector for player i over all available strategies in the game. A
strategy profile σ = {σ∕,σi ∈ Σi, i ∈ P} is a collection of strategies for all players. ∑i is the set
of all possible strategies for player i. The strategy of all players other than player i is presented as
σ-i. up(σi, σ-i) is the expected payoff for i if player i plays according to σi and the other players
play according to σ-i .
Let πσ (h) be the probability of history h occurring if players choose actions according to σ. We
can decompose πσ (h) = Πi∈N{c}πσ (h) into each player’s contribution to this probability. Hence,
πiσ (h) is the probability that if player i plays according to σ if all players other than i, and chance
always chose actions leading to h. Let π-σi(h) be the product of all players’ contribution (including
chance) except player i. In this paper we focus on perfect-recall games. Therefore, for i = P (I) we
define πi(I) = πi(h) for h ∈ I.
2.2	Best Response and Nash Equilibrium
A best response to σi is a strategy BR(σi) satisfies that
ui(σi, B R(σi)) = max ui(σi, σ-0 i).
σ-0 i
(1)
2
Under review as a conference paper at ICLR 2021
Nash equilibrium is a strategy profile where everyone plays a best response. A Nash equilibrium is
a strategy profile σ satisfies:
u1 (σ) ≥ max u1 (σ10 , σ2)	u2 (σ) ≥ max u2(σ1 , σ20 ).	(2)
σ10 ∈Σ1	σ20 ∈Σ2
An approximation of a Nash equilibrium or -Nash equilibrium in a two-player extensive game is a
strategy profile σ satisfies:
u1 (σ) + ≥ max u1 (σ10 , σ2)	u2 (σ) + ≥ max u2(σ1, σ20 ).	(3)
σ10 ∈Σ1	σ20 ∈Σ2
The -NE in an extensive game can be efficiently computed by regret minimization. The exploitabil-
ity of a strategy (σ1, σ2) can be interpreted as the approximation error to the Nash equilibrium. The
exploitability is defined as
max u1(σ0,1, σ2) + maxu2(σ1, σ0,2).	(4)
σ0,1	σ0,2
2.3	Regret Matching(RM) and Counterfactual Regret Minimization (CFR)
Counterfactual regret minimization (CFR) is an equilibrium finding algorithm for extensive games
that minimizes regret in each infoset (Zinkevich et al., 2007). Regret matching (RM) is the most
popular option (Blackwell, 1956). Regret is an online learning concept that has triggered many
powerful learning algorithms. These learning algorithms minimize some kinds of regrets, known
as regret minimization algorithms such as regret matching and AdaHedge. To define this concept,
we first consider repeatedly playing an extensive game. Let Σa be the set of valid actions. At each
iteration, the player selects an action at and get a reward CFR makes frequent use of counterfactual
value v, which is the expected utility of an infoset if a player i tries to reach it. Given a strategy
profile σ and a strategy σ, the counterfactual value vσ (I) for an infoset I and the counterfactual
value of a special action a in this infoset are defined as:
vσ (I) = X π-σi(h) X(πσ(h, z)ui(z)) ,	(5)
h∈I	z∈Z
vσ(I, a)= £ I π-i(h) J2(πσ(h ∙ a, Z)Ui(Z)) I .	⑹
h∈I	z∈Z
We define σt as the strategy profile used on iteration t. The regret rt (I, a) for action a in infoaet I
at iteration T is :	rt(I, a) =vσt(I,a) -vσt(I).	(7)
The whole regret RT in T iterations is:
(8)
Let RT+ (I, a) = max{RT (I, a), 0}. In RM, player i selects actions a ∈ A(I) on each iteration T
according to probabilities:
σT (I, a) =
Pα0∈A^RT-(I,a0) , Pa0 RT (I, a0) > 0
∣A(i)∣,	otherwise.
(9)
According to RM in infoset I, the regret on interation T satisfies RT ≤ 4，|A(I)|√T (Zinkevich
et al., 2007). The time average strategy σT(I) = EPbn(I)；)(I) converges to an e-NE. CFR+ is
a variant of CFR with the two small changes. First, CFR+ set all negative regret to 0. Formally,
CFR+ defines the regret-like value Q as QT (I, a) = max{QT T1 (I, a) + rt (I, a), 0} and uses it
as RTT (I, a). CFR+ uses strategy on iteration T according to Regret Matching+ (RM+) rather than
RM. Second, CFR+ uses a weighted average strategy σ0T(I)= "三？(1)；)(I) rather than using
a average strategy as CFR. Linear CFR is a varient of CFR which multiplies the external regret by
tT1 on iteration T. It means the iteration t regret has a weight T22TT in iteration T. Optimistic CFR
counts the regret and uses a modified regret RmT od = PtT=1 rt(I, a) + 2rT (I, a) rather than RT.
3
Under review as a conference paper at ICLR 2021
Figure 1: From SWaP regret to external regret.
3 Temperature Regret Matching
In this section, We ProPose TemPerature Regret Matching (TRM), an efficient RM method that
adoPts a different strategy to obtain the outPut strategies using a temPerature Perliminary. In all Past
variants of CFR, each iteration strategy is given by regret matching using external regret. We discuss
using different RM methods in CFR When determining strategies. We first theoretically demonstrate
that different regret matching methods converge to an ε-equilibrium. We give our regret matching
methods.
σT (I, a) =
(R+(I,a))β
Pα0∈A(i)(R+(1,a0))β ,
,
Pa0 RT+ (I, a0) > 0,
otherwise;
(10)
in Which β > 0 is a hyPer-Parameter. This regret matching method can converge to an ε-
equilibrium (Cesa-Bianchi & Lugosi, 2006). Furthermore, When We set different β for different iter-
ation regret matching, it can also converge to an ε-equilibrium. We demonstrate that different regret
matching methods (in Which We can change beta during calculating outPut strategies) can converge
to an ε-equilibrium. This Part folloWs that if there is a no-external-regret algorithm, then there is a
no-sWaP-regret algorithm. Our hoPe is that We can change β randomly during calculating strategies.
Let n denote the number of actions. Suppose there are n * k different no-external-regret algorithms
(M1 = (m11, m12, ..., m1k), M2 = (m21, m22, ..., m2k), ..., Mn = (mn1, mn2, ..., mnk)) With d-
ifferent RM parameter β . In Which Mj is the set of algorithm for action j . The master algorithm
M (shoWn as Fig. 1) is: 1) Receive distributions q1t , ..., qnt over actions from M1, M2, ..., Mn. 2)
Compute and outpur a consensus distribution pt. 3) Receive a counterfactual vector vt from the ad-
versary. 4) Give algorithm mjh the Counterfactual vector pt(jh) ∙ vt. Let δ : A → A be a switching
function. The time-average expected value of the master algorithm M is :
Tnk
T XXXpt(ih)vt(i).	(11)
t=1 i=1 h=1
The time-average expected value under a switching function δ : A → A is :
Tnk
T XXXpt(ih) ∙ vt(δ(i)).	(12)
t=1 i=1 h=1
We need to prove that when t → ∞, Eqn.(11) and Eqn.(12) should be equivalent. Fora set algorithm
Mj, actions are chosen according to its recommended distributions qj1h, ..., qjTh and the expected
values are p1(jh) ∙ c1,…,pτ(jh) ∙ cτ. For an algorithm mjh, the expected values arePT(jh) ∙ VT.
4
Under review as a conference paper at ICLR 2021
Algorithm Mj receives its time-average value is:
Tnk
T XXXqjh(i) (pt(jh)v(i),	(13)
t=1 i=1 h=1
Because mjh is a no-regret algorithm, the perceived value of each fixed action a ∈ A should be
smaller than:
Tk
T XX pt(jh)vt(a) + Rjh,	(14)
t=1 h=1
as T → ∞, Rj → 0. Now fix a switching function δ. According to Eqn.(13) and Eqn.(14), we can
get:
Tnnk	Tnk	nk
TXXXXqjh(i)pt(jh)vt(i) ≤ TXXXpt(jh)vt(δ(j)) + XXRjh.
(15)
t=1 i=1 j=1 h=1
t=1 j=1 h=1
j=1 h=1
nk
As t → ∞, P P Rjh should be close to 0, the right part of Eqn.( 15) should be equal to Eqn.( 12).
j=1 h=1
So we can choose the splitting of the value vector vt among the no-external-regret algorithms mjh .
The pt for each action i ∈ A and each iteration t should be choosen as:
nk
pt (ih) = XXqjth(i)pt(jh).
j=1 h=1
(16)
It means we can first ask m1 algorithm for a recommended strategy σt then use the regret RT ask
m2 algorithm for a recommended strategy. When an action with a higher regret, we need to choose
it with higher probability. Due to β > 0, we give our temperature regret matching algorithm:
σT (I, a) =
(R+(I,a))α+γt
Pa0∈A(i)(R+(I,a0))α+γt ,
wπ,
Pa0RT+(I,a0)>0
otherwise.
(17)
Here α, γ are two hyperparameters α should be close to 1 and γt should be close to 0. We give a
temperature parameter γ to RM as a way to increase noise. When we update the strategies using
TRM using T iterations, the average of the hyper-parameter β = α - T2γ should equal to 1, which
can be compared with RM under the same settings. This setting can help agents have a certain
chance of producing more effective strategies.
4 A new regret updating method
In this part, we give our novel regret updating method. Optimistic CFR (Brown & Sandholm, 2019)
counts the regret and uses a modified regret RTmod = PtT=1 rt (I, a) + 2rT (I, a) rather than RT
and leads to faster converge. Linear CFR CalclUate the t iteration regret has a weight τ22t^τ after T
iterations. However, the regret rt gained in each iteration t is obtained through the virtual games
between the current iteration of strategy σit, σ-t i of player i and his/her opponent player -i. In
this section, we consider not only the strategy of our opponent in the current iteration but also the
strategy of our opponent in different previous iterations. When player i0s own strategy σit fights with
all previous iterations of opponent strategies σ-1 i, σ-2 i, ..., σ-t i, player i gets t regrets in the current
iterations, which is somewhat similar to LCFR’s weight oft for the current regret. However, saving
all past strategies will consume too many storage resources, and calculating t regret values will also
consume too many computing resources. To reduce the computational storage complexity, we save
the strategies of both sides of the last iteration of the game and update the regret values with the
opponent strategies of this iteration and the last iteration. We redefine two regret in iteration t for
σt σt-1
player i, the regret ri i, -i (Ii, a) for player i in iteration t using σit and the opponent player -i
using σ-t-i 1 is:
t t-1	t t-1	t t-1
riσi,σ-i (Ii,a)=viσi,σ-i (Ii, a) - viσi,σ-i (Ii).
(18)
5
Under review as a conference paper at ICLR 2021
σt σt
The regret σitri i -i (Ii, a) for player i in iteration t using σit and the opponent player -i using σ-t i
is:
riσi,σ-i(Ii,a) =viσi,σ-i(Ii,a) -viσi,σ-i(Ii).	(19)
The regret in iteration riσi (Ii , a) is:
t	t t-1	t t
riσi(Ii,a) =riσi,σ-i (Ii,a)+riσi,σ-i(Ii,a).	(20)
This way of calculating regret value has two advantages. First, in the traditional way of updating
regret, player i increases the weight for actions with large counterfactual values to ensure that the
expected return of the current strategy against the previous iteration of opponent strategy continues
to increase, simultaneously, player -i reduces the expected return of the current player in the same
way. Expressed mathematically as follows:
ui(σti,σt--i1) ≥ui(σti,σt-i) ≥ui(σti-1,σt-i).	(21)
When we save the strategies of the last iteration and updating the regret using last two strategies, the
expected return of player i in t, t + 1th interation satisfies that:
ui(σti,σt--i1) ≥ui(σti,σt-i) ≥ui(σti-1,σt-i),	(22)
ui(σt+1 , σt-1) ≥ ui(σt+1, σt+1) ≥ ui(σt-1, σt+1).	(23)
Second, when the CFR+/Linear CFR algorithm iterates many times, according to experience, each
iteration of its output strategy is close to the Nash equilibrium. We consider playing against several
opponents close to the Nash equilibrium, which can help the algorithm converge faster. An overall
description of our updating and regret matching is shown in Algorithm 1.
Algorithm 1: Temperature Regret Matching and the new regret updating method.
ι for t= 1 to T do
2	viσt(Ii) = P	πiσt (h, z)π-σti(z)ui(z).
h∈Ii ,hwz,z∈Z
t t-1	t t-1	t t-1
3	riσi,σ-i (Ii,a)=viσi,σ-i (Ii,a)-viσi,σ-i (Ii).
tt	tt	tt
4	riσi,σ-i(Ii,a)=viσi,σ-i(Ii,a)-viσi,σ-i(Ii).
5	Rit(Ii, a) = Rit-1(Ii, a) + riσi,σ-i (Ii, a) + riσi,σ-i (Ii, a).
+γt, Pa0 RT(1,6 > 0,
otherwise.
7	St(Ii,a) = St-1(Ii, a) + πiσt (Ii)σit(Ii, a).
8	end
9	σiT(Iia) = Pa0∈S(];ST)(Ii,a0).
(	(R+(I,a))α+γt
6	στ(I,a) = <	Pa0∈A(ii)(R+(I,a0))α
[	∣A⅛,
5	Experiments
To verify the effectiveness of our proposed TRM algorithm and regret matching method, we evaluate
their performances on some games such as Rock-Paper-Scissors, a modified form of Rock-Paper-
Scissors in which the winner receives two points and the loser loses two points when either player
chooses Scissors, Leduc Hold’em, and Big Leduc Hold’em against existing CFR variants. Leduc
Hold’em a two-players IIG of poker introduced in Southey et al. (2005). In Leduc Hold’em, there
is a deck of six cards that includes two suites, each with three ranks. The cards are often denoted
by king, queen and jack. The game has a total of two rounds. Each round has a maximum of two
raises per round. Each player gets a private card in the first round and the opponent’s card is hidden.
In the second round, another card is dealt with as a community card, and the information about this
card is open to both players. If a player’s private card is paired with the community card, that player
wins the game; otherwise, the player with the highest private card wins the game. Both players bet
6
Under review as a conference paper at ICLR 2021
O 2000	4000	6000	8000 IOOOO	O 2000	4000	6000	8000 IOOOO
(a) RoCk-PaPer-SCissors	(b) A modified form of Rock-Paper-Scissors
Figure 2: The left and right shows distances (log10) between the strategies and Nash equilibriumin
the two games when β ∈ {0.01, 0.1, 1(RM), 10, a0.01, a0.1, a1, a10}. The results are reported by
using in 10000 itearations. a0 represents a value β ∈ (0, 2a) that is randomly set in each iteration.
one chip into the pot before the cards are dealt. Moreover, a betting round follows at the end of each
dealing round. There are four kinds of actions: fold (End this game, the other gets all the pot.), call
(Increase his/her bet until both players have the same chips.), check (Do not action.), and bet (Add
some chips to the pot.) in Leduc Hold’em. In Leduc Hold’em, the player may wager any amount
of chips up to a maximum of that player’s remaining stack. There is also no limit on the number of
raises or bets in each betting round. The Big Leduc Hold’em has the same rules as Leduc Hold’em
and uses a deck of twenty-four cards with twelve ranks. In addition to the larger size of the state
space, BigLeduc allows a maximum of six instead of two raises per round.
5.1	Ablation studies
We take Rock-Paper-Scissors and the modified form of Rock-Paper-Scissors as the experimental en-
vironments for our ablation studies. We empirically evaluate our algorithm TRM against vanilla RM.
We verify that when we set different β for different iterations regret matching, it can also converge to
an ε-equilibrium. We set β to different values, i.e., β ∈ {0.01, 0.1, 1(RM), 10, a0.01 , a0.1, a1, a10}
to test the sensitivity of the algorithm. ax represents a value ∈ (0, 2x) that is randomly set in each
iteration. We measure the distance between the strategies and Nash equilibrium ((1, 3, 3) in Rock-
Paper-Scissors and (2, 2, 5) in the modified form Rock-Paper-Scissors) of these algorithms. Fig-
ure 2 (left) shows the performance of different β in Rock-Paper-Scissors and Figure 2 (right) shows
the performance of different β in the modified form Rock-Paper-Scissors. These results validate that
RM converges to an ε-equilibrium setting different β for different iterations.
5.2	Poker Games
In this section, we empirically evaluate our algorithm against existing CFR variants. Since we do
not know what Nash equilibrium strategies are, we measure the exploitability of these algorithms.
Experiments are conducted on variants of two common benchmarks in imperfect-information game
solving: Leduc Hold’em and Big Leduc Hold’em.
We run each algorithm on each Leduc Hold’em game for 65536 iteartions in Leduc Hold’em and
1024 iteartions in Big Leduc Hold’em. We empirically compare TRM-(CFR, CFR+, LCFR) with
existing methods vanilla CFR (Zinkevich et al., 2007), CFR+ (Tammelin, 2014), and LCFR (Brown
& Sandholm, 2019). Figure 3 (left) shows the performance of different α, γ in Leduc Hold’em and
Figure 3 (right) shows the performance of different α, γ in Big Leduc Hold’em. We can see that the
performance of TRM-CFR has a similar performance to CFR on Leduc Hold’em. This is because
in the experiments of CFR on Leduc Hold’em, there are a large portion of histories with n average.
The performance of TRM-(CFR+, LCFR) which a1.005,0.095 is better than CFR+ and LCFR on both
Leduc games.
7
Under review as a conference paper at ICLR 2021
0.01 j~~I------1--------1--------1--------1-------1-
64	256	1024	4096	16384	65536
(a) LeduC Hold'em
(b) Big Leduc Hold'em
Figure 3: The left and right shows the exploitability (log10) of these algorithms when β ∈
{1, a0.99-1.01, a0.995-1.005, a1.005-0.995, a1.01-0.09}. The exploitability (y-axis) are reported in T
itearations (x-axis, T = 65536 in Leduc and T = 1024 in Big Leduc). ax,y represents a value that
is set as β = X + t(y-x) in iteration t.
Figure 4: The left and right shows the exploitability (log10) of TRM and RTRM in CFR, CFR+ and
LCFR. The exploitability (y-axis) are reported in T itearations (x-axis, T = 65536 in Leduc and
T = 1024 in Big Leduc).
We use the best-performance parameter a1.005,0.095 in TRM to measure whether TRM and the new
regret updating method (RTRM) can cooperate to enhance the performance further. We use the
network in deep CFR (DCFR) (Brown et al., 2019) to save the startegies for players i, -i in iteration
t- 1 to avoid consuming too many storage resources. Figure 4 (left) shows the performance of TRM
and RTRM in Leduc Hold'em and Figure 4 (right) shows the performance in Big Leduc Hold'em.
Compared to TRM, TRM with the new regret updating method algorithm learns faster.
6	Conclusion
In this work, we propose TRM, a novel regret matching method for CFR to solve imperfect informa-
tion games and theoretically demonstrate that the update of TRM converges to Nash Equilibrium.
TRM provides a mechanism to CFR methods to dynamically change the matching weight of exter-
nal regret according to the number of iterations. Experimental results in some IIGs demonstrate that
TRM is beneficial for CFR and its variants to accelerate convergence speed.
8
Under review as a conference paper at ICLR 2021
References
Jacob Abernethy, Peter L Bartlett, and Elad Hazan. Blackwell approachability and no-regret learning
are equivalent. In Conference on Learning Theory, volume 19, pp. 27-46, 2011.
David Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathe-
matics, 6(1):1-8, 1956.
Michael Bowling, Neil Burch, Michael Johanson, and Oskari Tammelin. Heads-up limit hold’em
poker is solved. Science, 347(6218):145-149, 2015.
Noam Brown and Tuomas Sandholm. Superhuman ai for heads-up no-limit poker: Libratus beats
top professionals. Science, 359(6374):418-424, 2018.
Noam Brown and Tuomas Sandholm. Solving imperfect-information games via discounted regret
minimization. In AAAI Conference on Artificial Intelligence, pp. 1829-1836, 2019.
Noam Brown, Adam Lerer, Sam Gross, and Tuomas Sandholm. Deep counterfactual regret mini-
mization. In International Conference on Machine Learning, pp. 793-802, 2019.
NicoIo Cesa-Bianchi and Gabor Lugosi. Prediction, learning, and games. 2006.
Steven De Rooij, Tim Van Erven, Peter Grunwald, and Wouter M Koolen. Follow the leader if you
can, hedge if you must. Journal of Machine Learning Research, 15(1):1281-1316, 2014.
Yoav Freund and Robert E Schapire. A decision-theoretic generalization of on-line learning and an
application to boosting. volume 55, pp. 119-139, 1997.
Richard G Gibson, Marc Lanctot, Neil Burch, Duane Szafron, and Michael Bowling. Generalized
sampling and variance in counterfactual regret minimization. In AAAI Conference on Artificial
Intelligence, pp. 1355-1361, 2012.
Sergiu Hart and Andreu Mascolell. A simple adaptive procedure leading to correlated equilibrium.
Econometrica, 68(5):1127-1150, 2000.
Marc Lanctot, Kevin Waugh, Martin Zinkevich, and Michael Bowling. Monte carlo sampling for
regret minimization in extensive games. In Neural Information Processing Systems, pp. 1078-
1086, 2009.
Matej Moravcik, Martin Schmid, Neil Burch, Viliam Lisy, DUstin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. Deepstack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508-513, 2017.
Jr John F Nash. Equilibrium points in n-person games. Proceedings of the National Academy of
Sciences of the United States of America, 36(1):48-49, 1950.
Yu Nesterov. Excessive gap technique in nonsmooth convex minimization. Siam Journal on Opti-
mization, 16(1):235-249, 2005.
Finnegan Southey, Michael Bowling, Bryce Larson, Carmelo Piccione, Neil Burch, Darse Billings,
and Chris Rayner. Bayes’ bluff: opponent modelling in poker. In Uncertainty in Artificial Intelli-
gence, pp. 550-558, 2005.
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert E Schapire. Fast convergence of regu-
larized learning in games. In Neural Information Processing Systems, pp. 2989-2997, 2015.
Oskari Tammelin. Solving large imperfect information games using cfr+. arXiv preprint arX-
iv:1407.5042, 2014.
Oskari Tammelin, Neil Burch, Michael Johanson, and Michael Bowling. Solving heads-up limit
texas hold’em. In International Joint Conferences on Artificial Intelligence, pp. 645-652, 2015.
Martin Zinkevich, Michael Johanson, Michael Bowling, and Carmelo Piccione. Regret minimization
in games with incomplete information. In Neural Information Processing Systems, pp. 1729-
1736, 2007.
9