Under review as a conference paper at ICLR 2021
A Probabilistic Approach to Constrained Deep
Clustering.
Anonymous authors
Paper under double-blind review
Ab stract
Clustering with constraints has gained significant attention in the field of semi-
supervised machine learning as it can leverage partial prior information on a grow-
ing amount of unlabelled data. Following recent advances in deep generative mod-
els, we derive a novel probabilistic approach to constrained clustering that can
be trained efficiently in the framework of stochastic gradient variational Bayes.
In contrast to existing approaches, our model (CVaDE) uncovers the underlying
distribution of the data conditioned on prior clustering preferences, expressed as
pairwise constraints. The inclusion of such constraints allows the user to guide
the clustering process towards a desirable partition of the data by indicating which
samples should or should not belong to the same class. We provide extensive
experiments to demonstrate that CVaDE shows superior clustering performances
and robustness compared to state-of-the-art deep constrained clustering methods
in a variety of data sets. We further demonstrate the usefulness of our approach
on challenging real-world medical applications and face image generation.
1	Introduction
The ever-growing amount of data and the time cost associated with its labeling has made clustering
a relevant task in the field of machine learning. Yet, in many cases, a fully unsupervised clustering
algorithm might naturally find a solution which is not consistent with the domain knowledge (Basu
et al., 2008). In medicine, for example, clustering could be driven by unwanted bias, such as the type
of machine used to record the data, rather than more informative features. Moreover, practitioners
often have access to prior information about the types of clusters that are sought, and a principled
method to guide the algorithm towards a desirable configuration is then needed. Constrained clus-
tering, therefore has a long history in machine learning as it enforces desirable clustering properties
by incorporating domain knowledge, in the form of constraints, into the clustering objective.
Following recent advances in deep clustering, constrained clustering algorithms have been recently
used in combination with deep neural networks (DNN) to favor a better representation of high-
dimensional data sets. The methods proposed so far mainly extend some of the most widely used
deep clustering algorithms, such as DEC (Xie et al., 2016), to include a variety of loss functions
that force the clustering process to be consistent with the given constraints (Ren et al., 2019; Shukla
et al., 2018; Zhang et al., 2019b). Although they perform well, none of the above methods model the
data generative process. As a result, they can neither uncover the underlying structure of the data,
nor control the strength of the clustering preferences, nor generate new samples (Min et al., 2018).
To address the above issues, we propose a novel probabilistic approach to constrained clustering,
the Constrained Variational Deep Embedding (CVaDE), that uncovers the underlying data distribu-
tion conditioned on domain knowledge, expressed in the form of pairwise constraints. Our method
extends previous work in unsupervised variational deep clustering (Jiang et al., 2017; Dilokthanakul
et al., 2016) to incorporate clustering preferences as Bayesian prior probabilities with varying de-
grees of uncertainty. This allows systematical reasoning about parameter uncertainty (Zhang et al.,
2019a), thereby enabling the ability to perform Bayesian model validation, outlier detection and
data generation. By integrating prior information in the generative process of the data, our model
can guide the clustering process towards the configuration sought by the practitioners.
Our main contributions are as follows: (i) We propose a constrained clustering method (CVaDE)
to incorporate given clustering preferences, with varying degrees of certainty, within the Variational
1
Under review as a conference paper at ICLR 2021
Auto-Encoder (VAE) framework. (ii) We provide a thorough empirical assessment of our model. In
particular, we show that (a) a small fraction of prior information remarkably increases the perfor-
mance of CVaDE compared to unsupervised variational clustering methods, (b) our model shows
superior clustering performance compared to state-of-the-art deep constrained clustering models on
a wide range of data sets and, (c) our model proves to be robust against noise as it can easily incor-
porate the uncertainty of the given constraints. (iii) We show that our model can drive the clustering
performance towards different desirable configurations, depending on the constraints used, and it
successfully generates new samples on challenging real-world image data.
2	Theoretical Background & Related Work
Constrained Clustering. A constrained clustering problem differs from the classical clustering
scenario as the user has access to some pre-existing knowledge about the desired partition of the data.
The constraints are usually expressed as pairwise constraints (Wagstaff & Cardie, 2000), consisting
of must-links and cannot-links, which indicate whether two samples are believed to belong to the
same cluster or to different clusters. Such pairwise relations contain less information than the labels
used in classification tasks but are usually easier to obtain. Traditional clustering methods have been
then extended to enforce pairwise constraints (Lange et al., 2005). COP-KMEANS (Wagstaff et al.,
2001) and MPCK-mean (Bilenko et al., 2004) adapted the well-known K-means algorithm, while
several methods proposed a constrained version of the Gaussian Mixture Models (Shental et al.,
2003; Law et al., 2004; 2005). Among them, penalized probabilistic clustering (PPC, Lu & Leen
(2004)) is most related to our work as it expresses the pairwise constraints as Bayesian priors over
the assignment of data points to clusters. However, PPC, as well as the previous models, shows poor
performance and high computational complexity on high-dimensional and large-scale data sets.
Deep Constrained Clustering. To overcome the limitations of the above models, constrained clus-
tering algorithms have been lately used in combination with DNNs. Hsu & Kira (2015) train a DNN
to minimize the Kullback-Leibler (KL) divergence between similar pairs of samples, while Chen
(2015) performs semi-supervised maximum margin clustering of the learned features on a DNN.
More recently, many extensions of the widely used DEC model (Xie et al., 2016) have been pro-
posed to include a variety of loss functions to enforce pairwise constraints. Among them, SDEC
(Ren et al., 2019) includes a distance loss function that forces the data points with a must-link to
be close in the latent space and vice-versa. C-IDEC (Zhang et al., 2019b), uses, instead, a KL
divergence loss, extending the work of Shukla et al. (2018). Other works have focused on discrim-
inative clustering methods by self-generating pairwise constraints from either Siamese networks or
KNN graphs (Smieja et al., 2020) (Fogel et al., 2019). As none of the approaches proposed so far
is based on generative models, the above methods fail to uncover the underlying data distribution.
Additionally, DEC-based architectures rely on heavy pretraining of the autoencoder, resulting in no
theoretical guarantee that the learned latent space is indeed suitable for clustering (Min et al., 2018).
VAE-based deep clustering. Many models have been proposed in the literature to perform unsuper-
vised clustering through deep generative models (Li et al., 2019; Yang et al., 2019; Manduchi et al.,
2019; Kopf et al., 2019). Among them, the Variational Deep Embedding (VaDE, Jiang et al. (2017))
and the Gaussian Mixture Variational Autoencoder (GMM-VAE, Dilokthanakul et al. (2016)) pro-
pose a variant of the VAE (Kingma & Welling (2014); Rezende et al. (2014)) in which the prior is a
Gaussian Mixture distribution. With this assumption, they construct an inference model that can be
directly optimised in the framework of stochastic gradient variational Bayes. However, variational
deep clustering methods, such as the VaDE, cannot incorporate domain knowledge and clustering
preferences. Even though a semi-supervised version on the VAE has been proposed by Kingma et al.
(2014), the latter cannot be naturally applied to clustering. For this reason, we aim at extending the
above methods to incorporate clustering preferences in the form of constraints, modeled as Bayesian
priors, to guide the clustering process towards a desirable configuration.
3	Constrained Variational Deep Embedding
In the following section, we propose a novel constrained clustering model (CVaDE) to incorporate
clustering preferences, with varying degree of certainty, in a VAE-based deep clustering setting. In
particular, we use the VaDE (Jiang et al., 2017) generative assumptions of the data, conditioned on
2
Under review as a conference paper at ICLR 2021
the domain knowledge. We then illustrate how our model can be trained efficiently in the framework
of stochastic gradient variational Bayes by optimizing the Conditional Variational Lower Bound.
Additionally, we define concrete prior formulations to incorporate our preferences, with a focus on
pairwise constraints.
3.1	The Generative Assumptions
Let us consider a data set X = {xi}iN=1 consisting of N samples with xi ∈ RM that we wish to
cluster into K groups according to some prior information encoded as G. For example, we may
know a priori that certain samples should be clustered together with different degree of certainty.
Hence G encodes both our prior knowledge on the data set and the degree of confidence.
We assume the data is generated from a random process consisting of three steps. First, the cluster
assignments c = {ci}iN=1, with ci ∈ {1, . . . , K}, are sampled from a distribution conditioned on the
prior information, C 〜P(CG). Next, for each cluster assignment Ci, a continuous latent embedding,
zi ∈ RD, is sampled from a Gaussian distribution, whose mean and variance depend on the selected
cluster ci . Finally, the sample xi is generated from a distribution conditioned on zi . Given ci , the
generative process can be summarized as:
Zi 〜p(zi∣ci)= N(zi∣μ°i,σ2iI)	(1)
Xi 〜pθ (Xi∣Zi)
∫N(Xi∣μxi, σ2iI) with [μχi, σ2i] = f(zi； θ) if Xi is real-valued
IBer(μxi) with 以国工=f (zi； θ) if xi is binary
(2)
where μci and σ2i are mean and variance of the Gaussian distribution corresponding to cluster ci in
the latent space and the function f(z； θ) is a neural network, called decoder, parametrized by θ.
Without prior information, that is when p(c∣G) = P(C) = QiP(Ci) = Qi Cat(Ci∣π), the cluster
assignments are independent and identical distributed as they follow a categorical distribution with
mixing parameters π. In that case, the generative assumptions described above are equal to those
of Jiang et al. (2017) and the parameters of the model can be learned using the unsupervised VaDE
method (see Appendix C). In the following, we explore the case when P(C|G) 6= P(C).
3.2	Conditional Variational Lower Bound
Given the data generative assumptions illustrated in Sec. 3.1, the objective is to infer the parameters
π, μc, σ2 and θ which better explain the data X given prior information on the cluster assignments
G. We achieve this by maximizing the marginal log-likelihood conditioned on G, that is:
log P(X|G) = log
X P(X, Z, C|G),
(3)
where Z = {zi}iN=1 is the collection of the latent embeddings corresponding to the data set X. The
conditional joint probability is derived from Eq 1/2 and can be factorized as:
N
p(X, Z, c|G) = Pθ(X∣Z)p(Z∣c)p(c∣G) = P(CG) YPθ(xi∣Zi)p(zi∣ci).	(4)
i=1
Since the conditional log-likelihood is intractable, we derive a lower bound of the log marginal
conditional probability of the data, which we call Conditional ELBO (C-ELBO, LC):
r “IC *	1	P(X, Z,c∣G)
LC(XIG)= Eqφ(ZaX) log qφ(z,c∣X)
(5)
Similarly to Jiang et al. (2017) and Dilokthanakul et al. (2016), we employ the following amortized
mean-field variational distribution:
N
qφ (Z, c|X) = qφ(Z∣X)p(c∣Z) = Y qφ(zi∣Xi)p(ci∣zi)
i=1
with P(CiIZi)=PSZi∣p(p(k),⑹
where qφ(zi∣Xi) is a Gaussian distribution with mean μ(xi) and variance σ2(xi)I which are the
outputs of a neural network, called encoder, parametrized by φ and P(ci = k) is denoted as P(k)
for simplicity. It is important to note that, in this formulation, the variational distribution does not
depend on G. This approximation is used to retain a mean-field variational distribution if the cluster
assignments, conditioned on the prior information, are not independent (Sec 3.4.1), that is when
P(cIG) 6= Qi P(ciIG).
3
Under review as a conference paper at ICLR 2021
3.3	Role of the prior information
To highlight how the prior information G influences the clustering objective, we rewrite Eq. 5 as:
Lc(X∣G)= Eqφ(z∣χ) [logpθ(X|Z)] - DκL(qφ(Z, c∣X)kp(Z, c∣G)).	⑺
The first term is called the reconstruction term, similarly to the VAE. The second term, on the other
hand, is the Kullback-Leibler (KL) divergence between the variational posterior and the Constrained
Gaussian Mixture prior. By maximizing the C-ELBO, the variational posterior mimics the true con-
ditional probability of the latent embeddings and the cluster assignments. This results in enforcing
the latent embeddings to follow a Gaussian mixture which agrees on the clustering preferences.
Using Eq. 6, the Conditional ELBO can be further factorized as:
LC(XG)=Ep(c|z)[log p(c∣G)]+ Eqφ(z∣x)[log Pθ (X∣Z)]	⑻
+ Eqφ(Z∣X)p(c∣Z)[lθgP(ZIc)] - Eqφ(Z,c∣X)[log qφ(Z, c|X)],
where the last three terms are not affected by G and they can be rewritten using the SGVB estimator
and the reparameterization trick (Kingma & Welling, 2014) to be trained efficiently using stochastic
gradient descent (see Appendix B). The first term, on the other hand, is investigated in Sec 3.4.
3.4	Conditional Prior Probability
We incorporate our clustering preference through the conditional probability p(c|G). In particular,
we construct the conditional prior probability to be:
P(CG) = PYCigi(c))= τy‰ Y πCigi(C),	⑼
Σc ∏j πCj gj (C)	ω ⑺ JYL
where π = {πk}kK=1 are the weights assoCiated to eaCh Cluster, Ciis the Cluster assignment of sample
Xi, Ω(π) is the normalization factor and gi(c) is a weighting function that assumes large values if
Ci agrees with our belief with respeCt to C and low values otherwise.
3.4.1	Pairwise Constraints
We hereby foCus on expressing the Conditional prior distribution in the Context of pairwise Con-
strained Clustering and we do so by adapting the work of Lu & Leen (2004) in our variational
framework. The weighting funCtion gi(C) is then defined as:
gi(c) = Y exp (Wi,j δciCj) ,	(10)
j6=i
where δ is the KroneCker δ-funCtion and W ∈ RN×N is a symmetriC matrix Containing the pairwise
preferenCes and ConfidenCe. In partiCular, Wi,j = 0 if we have no prior information on samples xi
and xj , Wi,j > 0 if there is a must-link Constraint (the two samples should be Clustered together) and
Wi,j < 0 if there is a cannot-link Constraint (the two samples should not be Clustered together). The
value |Wi,j | ∈ [0, ∞) refleCts the degree of Certainty in the Constraint. For example, if Wi,j -→ -∞
then xi and xj must be assigned to different Clusters otherwise P(C|G) -→ 0 (hard Constraint). On
the other hand, smaller values indiCate a soft preference as they admit some degree of freedom in the
model. An heuristiC to seleCt |Wi,j | is presented in SeC 4. Interestingly, the probability P(C|G) with
pairwise Constraints Can be seen as the posterior of the superparamagnetiC Clustering method (Blatt
et al., 1996), with loss funCtion given by a fully ConneCted Potts model (Wu, 1982). Differently
from our method, Blatt et al. (1996) Cluster the data aCCording to the pairwise Correlation funCtions
Ep(CG)δcicj that are estimated with MCMC methods.
Finally, we inCorporate the Conditional prior distribution in Eq. 8. The first term Can be written as:
Ep(c∣Z) [log P(c|G)] = Ep(c∣Z) logʒ(nʌ Y πCi Y exP (Wij δCiCj)	(II)
() i	j=i
=-logΩ(π) + EEp(CiiZi) log∏Ci + E Ep(Ci∣Zi)Ep(cj∣zj)Wi,jδciCj	(12)
i	i,j 6=i
=- logΩ(π) + £ EP(Ci = k∣Zi)log∏Ci + E EP(Ci = k∣Zi)ρ(Cj = k∣Zj)Wij.	(13)
i k	i,j6=i k
4
Under review as a conference paper at ICLR 2021
Maximizing Eq. 13 w.r.t. π poses computational problems due to the normalization factor Ω(π).
Crude approximations are investigated in (Basu et al., 2008), however we choose to fix the parameter
πk = 1/K to make z uniformly distributed in the latent space, as in previous works (Dilokthanakul
et al., 2016). By doing so, the normalization factor can be treated as a constant. The analysis of
different approaches to learn the weights π is left for future work. The C-ELBO with a pairwise
constrained prior can then be optimized using Monte Carlo sampling and stochastic gradient descent.
3.4.2	Further possible Constraints
Given the flexibility of our general framework, different types of constraints can be included in the
formulation of the weighting functions gi(c). In particular, we can perform semi-supervised learning
by setting the prior information as:
gi(c) = g(ci) = exp (Wi,k) with W ∈ RN×K	(14)
where Wi,k indicates whether the sample xi should be assigned to cluster k.
Additionally, one could also include triple-constraints by modifying the weighting function to be:
gi(c)= Y exp (Wi,j,kδciCjck) with W ∈ RN×N×N symmetric.	(15)
j,k6=i
where Wi,j,k = 0 ifwe do not have any prior information, Wi,j,k > 0 indicates that the samples xi,
xj and xk should be clustered together and Wi,j,k < 0 if they should belong to different clusters.
The analysis of these different constraints formulation is outside the scope of our work but they may
represent interesting directions for future work.
4	Experiments
In the following, we provide a thorough empirical assessment of our proposed method (CVaDE)
with pairwise constraints using a wide range of data sets. First, we evaluate our model’s performance
compared to both unsupervised variational deep clustering methods and state-of-the-art constrained
clustering methods. Then, we present extensive evidence of the ability of our model to handle noisy
constraint information. We additionally perform experiments on a medical application to prove that
our model can reach different desirable partitions of the data, depending on the constraints used,
even with real-world, noisy data. Finally, we show that CVaDE successfully generates new data,
using the learned generative process of Sec 3.1, on a challenging face image data set.
Baselines & implementation details. As baselines, we include the traditional constrained K-
means (MPCK-means, Bilenko et al. (2004)) and two recent deep constrained clustering methods
based on DEC (SDEC, Ren et al. (2019), and C-IDEC, Zhang et al. (2019b)) as they achieve state-
of-the-art performance in constrained clustering. We also compare our model to the unsupervised
variational deep clustering method VaDE (Jiang et al., 2017). To implement our model, we were
careful in maintaining a fair comparison with the baselines. In particular, we adopted the same en-
coder and decoder feed-forward architecture among all methods with four layers of 500, 500, 2000,
D units respectively, where D = 10 unless stated otherwise. The VAE is pretrained for 10 epochs
while the DEC-based baselines need a more complex layer-wise pretraining of the autoencoder
which involves 50 epochs of pretraining for each layer and 100 epochs of pretraining as finetuning.
The pairwise constraints are chosen randomly within the training set by sampling two data points
and assigning a must-link if they have the same label and a cannot-link otherwise. Unless stated
otherwise, the values of |Wi,j | are set to 104 for all data sets, and N pairwise constraints are used
for both our model and the constrained clustering baselines, where N is the number of samples of
the considered data set. Note that N pairwise constraints can be obtained by using only √N labeled
data points. To allow for fast iteration we simplify the last term of Eq. 13 by allowing the search of
pairwise constraints to be performed only inside the considered batch. We observed empirically that,
with a batch size of 1024, this approximation does not affect the clustering performance. Further
details on the other hyper-parameters setting can be found in the Appendix E.1.
Constrained clustering. We test the clustering performance of our model compared with the base-
lines on four different data sets: MNIST (LeCun et al., 2010), Fashion MNIST (Xiao et al., 2017),
5
Under review as a conference paper at ICLR 2021
Reuters (Xie et al., 2016) and HHAR (Stisen et al., 2015) (see Appendix A). Note that we pre-
processed the Reuters data by computing the tf-idf features on the 2000 most frequent words on a
random subset of 10000 documents and by selecting 4 root categories (Xie et al., 2016). Accuracy
and Normalized Mutual Information (NMI) are used as evaluation metrics. The results are shown in
Table 1. We observe that our model reaches state-of-the-art performance in the Fashion MNIST and
Reuters data sets. On MNIST and HHAR data sets, on the other hand, it has comparable clustering
performance with C-IDEC, however it is generally more stable accross different data sets.
Table 1: Clustering performances of CVaDE compared with baselines. All methods use N pair-
wise constraints (√N labels) except the unsupervised VaDE. Means and standard deviations are
computed across 10 runs with different random model initialization and pre-training weights.
	MNIST		FASHION		REUTERS		HHAR	
	Acc	NMI	ACC	NMI	Acc	NMI	Acc	NMI
VaDE	85.9 ±5.7	81.9 ±2.2	59.3 ±2.3	59.8 ±ι.9	76.0 ±o.7	50.2 ±0.9	75.3 ±5.9	70.5 ±4.0
MPCK	58.8 ±0.0	53.3 ±0.1	57.5 ±o.o	59.8 ±ι.9	60.7 ±0.5	50.2 ±0.9	58.1 ±o.2	70.5 ±4.0
SDEC	87.9 ±0.2	86.8 ±0.3	59.7 ±2.2	63.0 ±1.2	77.3 ±3.9	60.0 ±2.9	67.0 ±7.o	71.5 ±2.0
C-IDEC	97.9 ±0.1	94.3 ±0.3	84.5 ±ι.7	76.0 ±1.6	92.6 ±4.ι	79.1 ±4.1	93.4 ±0.5	86.3 ±1.1
CVaDE	98.0 ±0.1	94.4 ±0.2	86.1 ±o.2	77.6 ±0.3	95.3 ±0.5	82.6 ±1.4	93.5 ±0.8	86.5 ±1.2
Constrained clustering with noisy labels. In real-world applications it is often the case that
the additional information comes from different sources with different noise levels. As an exam-
ple, pairwise annotations could be obtained by both very experienced domain experts and by less-
experienced users. Hence, the ability to integrate constraints with different degrees of certainty into
the clustering algorithm is of significant practical importance. In this experiment, we consider the
case in which the given pairwise constraints have three different noise levels, q ∈ {0.1, 0.2, 0.3},
where q determines the fraction of pairwise constraints with flipped signs (that is, when a must-
link is turned into a cannot-link and vice-versa). In Fig. 1 we show the clustering performance
of our model compared to the strongest baseline derived from the previous section, C-IDEC. For
all data sets, we decrease the value of the pairwise confidence of our method using the heuristic
∣Wi,j | = α log (1--q) with α = 3500. Also, We use grid search to choose the hyper-parameters of
C-IDEC for the different noise levels (in particular we set the penalty weight to 0.1, 0.005, and 0.001
respectively). CVaDE clearly achieves better performance in terms of accuracy and NMI on all three
noise levels for all data sets. In particular, the higher the noise level the greater the performance dif-
ference. We can conclude that our model is more robust than its main competitor. Additionally,
C-IDEC cannot model different noise levels within the same data set, while our model can easily
include different source of information with different degree of uncertainty.
Heart Echo. We evaluate the effectiveness of our model in a real-world application by using in-
fant heart echo cardiogram videos. The data set consists of 305 infant echo cardiogram videos from
65 patient visits obtained from a large University Children’s Hospital. Each visit may consist of
videos taken from several different angles (called view), denoted by [LA, KAKL, KAPAP, KAAP,
4CV]. We cropped the videos by isolating the cone of the echo cardiogram, we resized them to
64 × 64 pixels and split them into individual frames obtaining a total of N = 20000 images. We
focused on investigating two constrained clustering problems using this data set. First, we cluster
the echo video frames by view. This is a relevant task, as in many datasets, heart echo videos are not
explicitly labeled (Zhang et al., 2018). Then, we cluster the echo video frames by infant maturity
at birth, following the WHO definition of premature birth categories (”Preterm”). We believe that
these two clustering tasks demonstrate that our model admits a degree of control in choosing the
underlying structure of the learned clusters. As the experiments demonstrate, by providing different
pairwise constraints, it is possible to guide the clustering process towards a preferred configuration,
depending on what the practitioners are seeking in the data.
For both experiments, we compare the performance of our method, CVaDE, with the unsupervised
VaDE method and the C-IDEC. Additionally, we include a variant of both our method and VaDE in
6
Under review as a conference paper at ICLR 2021
Figure 1: Accuracy and NMI clustering performance on four different datasets with noisy labels.
which we use convolutional layers (CNN-CVaDE and CNN-VaDE), for details on the implementa-
tion we refer to the Appendix E.2. The results are shown in Table 2. The CVaDE model outperforms
both baselines by a significant margin in both accuracy and NMI in all clustering experiments. This
demonstrates that the addition of domain knowledge is particularly effective for medical purposes.
Additionally, we observe that C-IDEC performs poorly on real-world noisy data. We believe this is
due to the heavy pretraining of the autoencoder, required by DEC-based methods, as it does not al-
ways enforce that the learned latent space is suitable for clustering. Additionally, we investigate the
relationship between the number of constraints and clustering performance for the view detection
task. We observed that with only 5000 pairwise constraints, which could be obtained with less than
80 labels, our model achieves results comparable to those of Table 2 (see Appendix D.3 for details).
Table 2: Constrained clustering results using heart echo cardiogram data with fully connected layers
on the left and convolutional layers on the right. Means and standard deviations are computed across
10 runs with different random model initialization and pre-training weights.
Clustering	I Metric	C-IDEC	VaDE	CVaDE	I CNN-VaDE	CNN-CVaDE
View	ACC NMI	69.5 ±8.4 47.8 ±15.5	33.4 ±2.0 11.9 ±2.6	81.0 ±2.5 60.9±4.0	46.3 ±8.o 24.2 ±6.5	91.4±2.7 80.1±4.7
Preterm	Acc NMI	64.9 ±5.8 11.0 ±9.9	40.9±4.6 6.2 ±1.6	73.4 ±1.3 26.0 ±2.6	40.7±4.9 9.9±5.7	74.4 ±2.3 34.7 ±3.8
Face Image Generation We evaluate the generative capabilities of our model using the UTKFace
dataset (Zhang et al., 2017). This dataset contains over 20000 images of male and female faces,
aged from 1 to 118 years old, with multiple ethnicities represented. We use a convolutional network
for the VAE (the implementation details are described in the Appendix E.3). As a first task, we
cluster the data using the gender prior information, in the form of 2N pairwise constraints, which
requires labels for 0.7% of the data set. Fig. 2a/2b shows the PCA decomposition of the embedding
space of both CVaDE and the unsupervised VaDE method. As a second task, we select a sub-sample
of individuals between 18 and 50 years of age (approx. 11000 samples), and cluster by ethnicity
(White, Black, Indian, Asian) using 2N pairwise constraints, Fig. 2c/2d. We believe that one possi-
ble reason for biases in machine learning models currently is due to under-representation of various
ethnic groups in training data sets. The constrained CVaDE method could be used to identify such
imbalances, while requiring only a relatively small number of labeled training samples. This would
allow easy identification of such instances of bias in training data sets at a relatively low cost. Fur-
thermore, the ability to generate samples from these detected clusters potentially allows automatic
data set augmentation. With the inclusion of domain knowledge, we observe a neat division of the
7
Under review as a conference paper at ICLR 2021
(a) CVaDE: Acc 0.89	(b) VaDE: Acc 0.51
(c) CVaDE: Acc 0.85
(d) VaDE: Acc 0.57
Figure 2: PCA decomposition of test set examples in the embedded space and generative samples
using CVaDE and VaDE for (a)-(b) Gender, (c)-(d) Ethnicity. In this configuration, CVaDE obtains
a NMI of 0.52 (gender) and 0.4 (ethnicity) while VaDE obtains a NMI close to 0 for both tasks.
selected clusters in the embedding space in both tasks, conversely the unsupervised approach is not
able to distinguish any feature of interest.
Finally, using the multivariate Gaussian distributions of each cluster in the learned embedded space,
we test the generative capabilities of CVaDE by first recovering the mean face of each cluster, and
then generating several more faces from each cluster. Figure 3 shows these generated samples. As
can be observed, the ethnicities present in the data set are represented well by the mean face. Fur-
thermore, the sampled faces all correspond to the respective cluster, and have a good amount of
variation. The quality of generated samples could be improved by using higher resolution training
samples or different CNN architectures.
Mean	Samples
redneG yticinhtE
Figure 3: Mean face and sampled faces for each learned cluster, top two rows corresponding to
gender, bottom rows to ethnicity
5	Conclusion
In this work, we present a novel constrained deep clustering method, CVaDE, that incorporates clus-
tering preferences in the form of pairwise constraints, with varying degrees of certainty. In contrast
to existing constrained deep clustering approaches, CVaDE uncovers the underlying distribution of
the data, resulting in the ability to generate new samples, perform Bayesian model validation and
outlier detection. With the integration of domain knowledge, we show that our model can drive the
clustering algorithm towards the partitions of the data sought by the practitioners, achieving state-
of-the-art constrained clustering performance in real-world and complex data sets. Additionally,
our model proves to be robust to noisy constraints as it can efficiently include uncertainty into the
clustering preferences. As a result, the proposed model can be applied to a variety of applications
where the difficulty of obtaining labelled data prevents the use of fully supervised algorithms.
8
Under review as a conference paper at ICLR 2021
References
S. Basu, I. Davidson, and K. Wagstaff. Constrained clustering: Advances in algorithms, theory, and
applications. 2008.
Mikhail Bilenko, S. Basu, and R. Mooney. Integrating constraints and metric learning in semi-
supervised clustering. In ICML ’04, 2004.
Marcelo Blatt, Shai Wiseman, and Eytan Domany. Superparamagnetic clustering of data. Phys. Rev.
Lett., 76:3251-3254, Apr 1996. doi: 10.1103∕PhysRevLett.76.3251. URL https://link.
aps.org/doi/10.1103/PhysRevLett.76.3251.
G. Chen. Deep transductive semi-supervised maximum margin clustering. ArXiv, abs/1501.06237,
2015.
Nat Dilokthanakul, Pedro A. M. Mediano, Marta Garnelo, Matthew C. H. Lee, Hugh Salimbeni,
Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture
variational autoencoders. ArXiv, abs/1611.02648, 2016.
S.	Fogel, Hadar Averbuch-Elor, D. Cohen-Or, and J. Goldberger. Clustering-driven deep embedding
with pairwise constraints. IEEE computer graphics and applications, 39 4:16-27, 2019.
Yen-Chang Hsu and Z. Kira. Neural network-based clustering using pairwise constraints. ArXiv,
abs/1511.06321, 2015.
Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, and Hanning Zhou. Variational deep
embedding: An unsupervised and generative approach to clustering. In IJCAI, 2017.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann
LeCun (eds.), 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB,
Canada, April 14-16, 2014, Conference Track Proceedings, 2014. URL http://arxiv.org/
abs/1312.6114.
Diederik P. Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and M. Welling. Semi-supervised
learning with deep generative models. In NIPS, 2014.
A. Kopf, Vincent Fortuin, Vignesh Ram Somnath, and M. Claassen. Mixture-of-experts varia-
tional autoencoder for clustering and generating from similarity-based representations. ArXiv,
abs/1910.07763, 2019.
T.	Lange, Martin H. C. Law, Anil K. Jain, and J. Buhmann. Learning with constrained and unlabelled
data. 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition
(CVPRg, 1:731-738 vol. 1, 2005.
Martin H. C. Law, A. Topchy, and Anil K. Jain. Clustering with soft and group constraints. In
SSPR/SPR, 2004.
Martin H. C. Law, A. Topchy, and Anil K. Jain. Model-based clustering with probabilistic con-
straints. In SDM, 2005.
Yann LeCun, Corinna Cortes, and CJ Burges. Mnist handwritten digit database. ATT Labs [Online].
Available: http://yann.lecun.com/exdb/mnist, 2, 2010.
David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li. Rcv1: A new benchmark collection for
text categorization research. J. Mach. Learn. Res., 5:361-397, December 2004. ISSN 1532-4435.
Xiaopeng Li, Zhourong Chen, Leonard K. M. Poon, and N. L. Zhang. Learning latent superstruc-
tures in variational autoencoders for deep multidimensional clustering. In ICLR, 2019.
Z. Lu and T. Leen. Semi-supervised learning with penalized probabilistic clustering. In NIPS, 2004.
Laura Manduchi, M. Huser, G. Ratsch, and Vincent Fortuin. Variational psom: Deep probabilistic
clustering with self-organizing maps. ArXiv, abs/1910.01590, 2019.
9
Under review as a conference paper at ICLR 2021
Erxue Min, Xifeng Guo, Q. Liu, G. Zhang, Jianjing Cui, and Jun Long. A survey of clustering
with deep learning: From the perspective of network architecture. IEEEAccess, 6:39501-39514,
2018.
Yazhou Ren, Kangrong Hu, Xinyi Dai, Lili Pan, Steven C. H. Hoi, and Zenglin Xu. Semi-supervised
deep embedded clustering. Neurocomputing, 325:121-130, 2019.
Danilo Jimenez Rezende, S. Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-
imate inference in deep generative models. In ICML, 2014.
N. Shental, Aharon Bar-Hillel, T. Hertz, and D. Weinshall. Computing gaussian mixture models
with em using equivalence constraints. In NIPS, 2003.
Ankita Shukla, Gullal Singh Cheema, and Saket Anand. Semi-supervised clustering with neural
networks. arXiv: Learning, 2018.
M. Smieja, LUkasz Struski, and Mario A. T. Figueiredo. A classification-based approach to semi-
supervised clustering with pairwise constraints. Neural networks : the official journal of the
International Neural Network Society, 127:193-203, 2020.
Allan Stisen, Henrik Blunck, Sourav Bhattacharya, Thor Siiger Prentow, Mikkel Baun Kjærgaard,
Anind Dey, Tobias Sonne, and Mads M0ller Jensen. Smart devices are different: Assessing and
mitigatingmobile sensing heterogeneities for activity recognition. In Proceedings of the 13th
ACM Conference on Embedded Networked Sensor Systems, SenSys ’15, pp. 127-140, New York,
NY, USA, 2015. Association for Computing Machinery. ISBN 9781450336314. doi: 10.1145/
2809695.2809718. URL https://doi.org/10.1145/2809695.2809718.
K.	Wagstaff and Claire Cardie. Clustering with instance-level constraints. In AAAI/IAAI, 2000.
K.	Wagstaff, Claire Cardie, S. Rogers, and S. SChrOdL Constrained k-means clustering with back-
ground knowledge. In ICML, 2001.
F. Y. Wu. The potts model. Rev. Mod. Phys., 54:235-268, Jan 1982. doi: 10.1103/RevModPhys.54.
235. URL https://link.aps.org/doi/10.1103/RevModPhys.54.235.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmark-
ing machine learning algorithms, 2017.
Junyuan Xie, Ross Girshick, and Ali Farhadi. Unsupervised deep embedding for clustering analysis.
volume 48 of Proceedings of Machine Learning Research, pp. 478-487, New York, New York,
USA, 20-22 Jun 2016. PMLR. URL http://proceedings.mlr.press/v48/xieb16.
html.
L.	Yang, N. Cheung, J. Li, and Jun Fang. Deep clustering by gaussian mixture variational autoen-
coders with graph embedding. 2019 IEEE/CVF International Conference on Computer Vision
(ICCV), pp. 6439-6448, 2019.
C. Zhang, Judith Butepage, H. Kjellstrom, and S. Mandt. Advances in variational inference. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 41:2008-2026, 2019a.
H. Zhang, S. Basu, and I. Davidson. A framework for deep constrained clustering - algorithms and
advances. In ECML/PKDD, 2019b.
Jeffrey Zhang, Sravani Gajjala, Pulkit Agrawal, Geoffrey Tison, Laura Hallock, Lauren Beussink,
Mats Lassen, Eugene Fan, Mandar Aras, ChaRandle Jordan, Kirsten Fleischmann, Michelle
Melisko, Atif Qasim, Sanjiv Shah, Ruzena Bajcsy, and Rahul Deo. Fully automated echocar-
diogram interpretation in clinical practice: Feasibility and diagnostic accuracy. Circulation, 138:
1623-1635, 10 2018. doi: 10.1161/CIRCULATIONAHA.118.034338.
Zhifei Zhang, Yang Song, and H. Qi. Age progression/regression by conditional adversarial au-
toencoder. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
4352-4360, 2017.
10
Under review as a conference paper at ICLR 2021
Appendix
A Data sets
The data sets used in the experiments are the followings:
•	MNIST: It consists of 70000 handwritten digits. The images are centered and of size 28
by 28 pixels. We reshaped each image to a 784- dimensional vector (LeCun et al., 2010).
•	Fashion MNIST: A data set of Zalando’s article images consisting of a training set of
60,000 examples and a test set of 10,000 examples (Xiao et al., 2017).
•	HHAR: The Heterogeneity Human Activity Recognition (HHAR) dataset contains 10299
sensor records from smart phones and smart watches. All samples are partitioned into 6
categories of human activities and each sample is of 561 dimensions (Stisen et al., 2015).
•	Reuters: It contains 810000 English news stories (Lewis et al., 2004). Following the work
of Xie et al. (2016), we used 4 root categories: corporate/industrial, government/social,
markets, and economics as labels and discarded all documents with multiple labels, which
results in a 685071-article dataset. We computed tf-idf features on the 2000 most frequent
words to represent all articles. A random subset of 10000 documents is then sampled.
•	Newborn echo cardiograms: The dataset consists of 305 infant echo cardiogram videos
from 65 patient visits obtained from a large children hospital. Each visit may consist of
videos taken from several different angles, denoted by [LA, KAKL, KAPAP, KAAP, 4CV].
We cropped the videos by isolating the cone of the echo cardiogram, we resized them to
64x64 pixels and split them into individual frames obtaining a total of N = 20000 images.
•	UTKFace: This dataset contains over 20000 images of male and female face of individuals
from 1 to 118 years old, with multiple ethnicities represented (Zhang et al., 2017).
B Conditional ELBO Derivations
In this section, we provide the detailed derivation of the Conditional ELBO with pairwise constraints
and we describe how it can be trained efficiently with Monte Carlo sampling. Specifically, the C-
ELBO, LC(X|G), is the upper bound of the marginal log-likelihood conditioned on G:
logP(XG) = log X Xp(X, Z, c|G) ≥ Eqφ(z,c∣χ) log “子。CG) = LC(XG).	(16)
Z c	qφ (Z, c|X)
By plugging in the marginal log-likelihood of Eq. 3 and the variational distribution of Eq. 6, the
C-ELBO can be rewritten as:
Lc(X∣G) = Eqφ(z,c∣χ)[logPθ(X∣Z)p(Z∣c)p(c∣G)] - Eqφ(z,c∣χ)[log q@(Z, c|X)]	(17)
=Eqφ(Z∣X)[log pθ (XIZ)]+ Eqφ(Z∣X)p(c∣Z)[log P(ZIc)]	(
+ Ep(c∣Z)[logP(CIG)] - Eqφ(Z∣X)[log qφ(ZIX)] - Ep(c∣Z)[logP(CIZ)]
where we used the fact that the variational distribution can be factorized as qφ(Z, cIX) =
qφ(ZIX)P(cIZ).
Given that qφ(ZIX)P(cIZ) = Qi qφ(ziIxi)P(ciIzi) and using Eq 13, we can further factorize Eq 18:
N
LC(XIG)= X [Eqφ (Zi∣Xi) [log Pθ (XiIZi )] + Eqφ(Zi∣X)p(c∣Zi)[log P(Zi心)]
i=1
+ Ep(CilZi) log ∏Ci + E Ep(Ci∣Zi)Ep(Cj∣Zj )Wi,jδciCj	(19)
i,j6=i
-Eqφ(Zi ∣Xi)[log q(ZiIXi)] - Ep(CilZi) [log q(CiIZi)]]
11
Under review as a conference paper at ICLR 2021
As p(Ci∣Zi) is discrete, Ep©%) H = kP(p(c = k∣Zi)[∙], the above equation can then be written as:
N
LC(XIG)= X [Eqφ (Zi∣Xi) [log Pθ (XiIZi)] + Eqφ(Zi∣Xi) [X P(Ci = k|Zi)IOg P(ZiICi=川
i=1	k
+	P(ci	=	kIZi)logπci	+	P(ci	=	kIZi)P(cj	=	kIZj)Wi,j	(20)
i k	i,j6=i k
— Eqφ(Zi∣Xi) [log q(zi∣Xi)] - Xp(Ci = k∣Zi)[logp(Ci = k∣Zi)]]
k
Using the SGVB estimator, we can approximate the above equation as:
NL
LC(XG) = X L X [logPΘ(XiIZil)) + XP(Ci= k|Z(I))IogP(Z(I)ICi= k)
i=1	l=1	k
+ XXP(Ci = kIZi(l)) log πCi + X X P(Ci = kIZi)P(Cj = kIZj)Wi,j	(21)
i k	i,j 6=i k
- log qφ(Zi(l) IXi) - X P(Ci = kIZi(l))logP(Ci = kIZi(l))i,
k
where L is the number of Monte Carlo samples in the SGVB estimator and it is set to L = 1 in all
experiments.
C Variational Deep Embedding
The Variational Deep Embedding method, VaDE (Jiang et al., 2017), assumes an observed sample
Xi is generated by the following generative proCess:
Ci 〜Cat(1∕K)
Zi 〜P(ZiIci) = N(ZiIμci, σ2iI)
Xi 〜 P(XiIZi)
σX.I) with [μχi, σ2.] = f (zi； θ) if Xi is real-valued
with μxi = f (Zi； θ) if Xi is binary
(22)
(23)
where K is the predefined number of clusters μc, σ2 are mean and variance of the Gaussian distri-
bution Corresponding to Cluster c in the latent spaCe and the funCtion f(z； θ) is a neural network,
called decoder, parametrized by θ, similarly to a VAE. To infer both the parameters of the Gaussian
mixture model and the decoder, the VaDE maximises the likelihood of the data X, that is:
log P(X) =log JZ XP(X, z, c) ≥ Eq(ZaX) log px⅛c)
LELBO
The variational distribution is chosen to be:
qφ(Z, cIX) =	qφ(Zi,ciIXi) =	qφ(ZiIXi)P(ciIZi)
ii
with
P(ciIZi)
P(ZiICi)P(Ci)
Pk P(ZiIk)P(k)
(24)
where q°(ZiIXi) is a Gaussian distribution with mean μ(xi) and variance σ2(xi)I which are the
outputs of a neural network, Called encoder, parametrized by φ.
The ELBO can be then formulated as:
LELBO(X)= Eqφ(Z∣X) [logPθ(X0] — DKLlqφ(Z, cIX)∣∣P(Z, c)).	(25)
D Further Experiments
D.1 Comparison with C-IDEC
In Table 3, we present the quantitative results in term of Adjusted Rand Index (ARI) and the per-
centage of satisfied constraints (SC) of both our model, CVaDE, and the strongest baseline, C-IDEC
with N constraints.
12
Under review as a conference paper at ICLR 2021
Table 3: Clustering performances of CVaDE compared with C-IDEC. All methods use N pairwise
constraints (√N labels) except the unsupervised VaDE. Means and standard deviations are com-
puted across 5 runs with different random model initialization and pre-training weights.
MNIST FASHION REUTERS HHAR
ARI SC C-IDEC 95.1 ±0.5 100 CVaDE 95.7 ±0.2 100	ARI SC 70.5 ±2.3	98 75.5 ±0.4 100	ARI SC 88.6 ±o.8 100 90.3 ±0.8 100	ARI SC 86.4 ±o.8 100 87.4 ±1.0 100
D.2 Noisy Labels
In Tables 4/5/6/7, we present the quantitative results in term of Accuracy and Normalized Mutual In-
formation of both our model, CVaDE, and the strongest baseline, C-IDEC with N noisy constraints
(presented visually in Fig. 1). In particular, the results are computed for q ∈ {0.1, 0.2, 0.3}, where q
determines the fraction of pairwise constraints with flipped signs (that is, when a must-link is turned
into a cannot-link and vice-versa).
Table 4: Clustering performances with noisy labels averaged over 10 runs on MNIST.
Noise level	Metric	C-IDEC	CVaDE
q=0.1	Accuracy NMI	0.960 ± 0.001 0.904 ± 0.003	0.974 ± 0.001 0.930 ± 0.002
q=0.2	Accuracy NMI	0.939 ± 0.003 0.863 ± 0.006	0.964 ± 0.003 0.911 ± 0.004
q = 0.3	Accuracy NMI	0.905 ± 0.010 0.808 ± 0.016	0.944 ± 0.006 0.874 ± 0.009
Table 5: Clustering performances with noisy labels averaged over 10 runs on fMNIST.
Noise level	Metric	C-IDEC	CVaDE
q=0.1	Accuracy NMI	0.800 ± 0.027 0.717 ± 0.021	0.843 ± 0.005 0.757 ± 0.004
q=0.2	Accuracy NMI	0.749 ± 0.037 0.667 ± 0.024	0.816 ± 0.004 0.731 ± 0.003
q = 0.3	Accuracy NMI	0.688 ± 0.031 0.601 ± 0.024	0.727 ± 0.030 0.656 ± 0.021
Table 6: Clustering performances with noisy labels averaged over 10 runs on Reuters.
Noise level	Metric	C-IDEC	CVaDE
q = 0.1	Accuracy NMI	0.923 ± 0.010 0.748 ± 0.020	0.942 ± 0.003 0.794 ± 0.009
q = 0.2	Accuracy NMI	0.786 ± 0.018 0.557 ± 0.016	0.917 ± 0.005 0.727 ± 0.013
q = 0.3	Accuracy NMI	0.706 ± 0.010 0.455 ± 0.017	0.850 ± 0.019 0.594 ± 0.030
13
Under review as a conference paper at ICLR 2021
Table 7: Clustering performances with noisy labels averaged over 10 runs on HHAR.
Noise level	Metric	C-IDEC	CVaDE
q = 0.1	Accuracy NMI	0.893 ± 0.016 0.797 ± 0.018	0.920 ± 0.009 0.844 ± 0.014
q=0.2	Accuracy NMI	0.788 ± 0.037 0.671 ± 0.040	0.906 ± 0.016 0.818 ± 0.019
q=0.3	Accuracy NMI	0.597 ± 0.034 0.493 ± 0.041	0.841 ± 0.024 0.720 ± 0.023
D.3 Heart Echo
PCA Decompostion In Figure 4 we present a PCA decomposition of the embedded space learned
by both the CVaDE and VaDE baseline, from the experiment presented in Section 4. It can be
observed that the CVaDE model, using N constraints, is able to learn an embedded space which
clusters by view much more effectively.
Figure 4: Left: PCA decomposition of CVaDE learned embedded space with representative samples
from the dataset, right: baseline VaDE embedded space
Impact of Constraints In order to try to understand how the number of constraints provided
impacts the performance of CVaDE, we performed an experiment using the heart echo dataset where
progressively more and more constraints were provided to the model during training. Figure 5
demonstrates that the clustering performance of CVaDE improves as more pairwise constraints are
provided. This trend continues until 5000 pairwise constraints are provided (requiring 0.35% of the
dataset to be labeled), at which point the performance reaches a plateau.
E	Implementation Details
E.1 Hyper-parameters setting
In Table 8 we specify the hyper-parameters setting of both our model, CVaDE, and the unsuper-
vised baseline, VaDE. Given the semi-supervised setting, we did not focus in fine-tuning the hyper-
parameters but rather we chose standard configurations for all data sets. We observed that our model
is robust against changes in the hyper-parameters, except for the batch size. The latter requires a
high value, as we simplify the last term of Eq. 13 by allowing the search of pairwise constraints to
be performed only inside the considered batch. For the VaDE, we used the same hyper-parameters
setting used in their paper (Jiang et al., 2017).
14
Under review as a conference paper at ICLR 2021
1987654321-
00αα Ciaaaa
-N」0e,ln8q
Figure 5: Plot demonstrating that CVaDE’s performance improves as more constraints are given. At
5000 pairwise constraints the performance equals that of N = 20000 (the total number of samples)
Table 8: Hyperparameters setting of both our model, CVaDE, and the unsupervised VaDE.
	CVaDE				VaDE			
	MNIST	FASHION	REUTERS	HHAR	MNIST	FASHION	REUTERS	HHAR
Batch size	1024	1024	1024	1024	128	128	128	128
Epochs	1500	1000	1000	1000	1000	1000	100	500
Learning rate	0.001	0.001	0.001	0.0001	0.002	0.002	0.002	0.002
Decay	10-5	10-4	10-4	10-4	10, 0.9	10, 0.9	5, 0.5	10, 0.9
E.2 Heart Echo
In addition to the model described in Section 4, we also used a VGG-like convolutional neural
network. This model is implemented in Tensorflow, using two VGG blocks (using a 3 × 3 kernel
size) of 32 and 64 filters for the encoder, followed by a single fully-connected layer reducing down
to an embedding of dimension 10. The decoder has a symmetric architecture.
The VAE is pretrained for 10 epochs, following which our model is trained for 300 epochs using a
learning rate of 0.001 (with an exponential decay of 0.00001), and a batch size of 1024. Refer to the
accompanying code for further details.
E.3 Face Image Generation
The face image generation experiments using the UTK Face dataset described in Section 4 were
carried out using VGG-like convolutional neural networks implemented in Tensorflow. In particular,
15
Under review as a conference paper at ICLR 2021
the input image size of64 × 64 × 3 allowed two VGG blocks (using a 3 × 3 kernel size) of64 and 128
filters for the encoder, followed by a single fully-connected layer reducing down to an embedding
of dimension 50. The decoder has a symmetric architecture.
The VAE is pretrained for 100 epochs, following which our model is trained for 1000 epochs using a
learning rate of 0.001 (with a decay of 0.00001), and a batch size of 1024. Refer to the accompanying
code for further details.
16