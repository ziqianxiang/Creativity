Under review as a conference paper at ICLR 2021
HAMILTONIAN Q-LEARNING: LEVERAGING
Importance-sampling for Data Efficient RL
Anonymous authors
Paper under double-blind review
Ab stract
Model-free reinforcement learning (RL), in particular Q-learning is widely used
to learn optimal policies for a variety of planning and control problems. However,
when the underlying state-transition dynamics are stochastic and high-dimensional,
Q-learning requires a large amount of data and incurs a prohibitively high compu-
tational cost. In this paper, we introduce Hamiltonian Q-Learning, a data efficient
modification of the Q-learning approach, which adopts an importance-sampling
based technique for computing the Q function. To exploit stochastic structure of
the state-transition dynamics, we employ Hamiltonian Monte Carlo to update Q
function estimates by approximating the expected future rewards using Q values
associated with a subset of next states. Further, to exploit the latent low-rank struc-
ture of the dynamic system, Hamiltonian Q-Learning uses a matrix completion
algorithm to reconstruct the updated Q function from Q value updates over a much
smaller subset of state-action pairs. By providing an efficient way to apply Q-
learning in stochastic, high-dimensional problems, the proposed approach broadens
the scope of RL algorithms for real-world applications, including classical control
tasks and environmental monitoring.
1	Introduction
In recent years, reinforcement learning (Sutton & Barto, 2018) have achieved remarkable success
with sequential decision making tasks especially in complex, uncertain environments. RL algorithms
have been widely applied to a variety of real world problems, such as resource allocation (Mao et al.,
2016), chemical process optimization (Zhou et al., 2017), automatic control (Duan et al., 2016), and
robotics (Kober et al., 2013). Existing RL techniques often offer satisfactory performance only when
it is allowed to explore the environment long enough and generating a large amount of data in the
process (Mnih et al., 2015; Kamthe & Deisenroth, 2018; Yang et al., 2020a). This can be prohibitively
expensive and thereby limits the use of RL for complex decision support problems.
Q-Learning (Watkins, 1989; Watkins & Dayan, 1992) is a model-free RL framework that captures
the salient features of sequential decision making, where an agent, after observing current state of
the environment, chooses an action and receives a reward. The action chosen by the agent is based
on a policy defined by the state-action value function, also called the Q function. Performance of
such policies strongly depends on the accessibility of a sufficiently large data set covering the space
spanned by the state-action pairs. In particular, for high-dimensional problems, existing model-free
RL methods using random sampling techniques leads to poor performance and high computational
cost. To overcome this challenge, in this paper we propose an intelligent sampling technique that
exploits the inherent structures of the underlying space related to the dynamics of the system.
It has been observed that formulating planning and control tasks in a variety of dynamical systems
such as video games (Atari games), classical control problems (simple pendulum, cart pole and
double integrator) and adaptive sampling (ocean sampling, environmental monitoring) as Q-Learning
problems leads to low-rank structures in the Q matrix (Ong, 2015; Yang et al., 2020b; Shah et al.,
2020). Since these systems naturally consist of a large number of states, efficient exploitation of
low rank structure of the Q matrix can potentially lead to significant reduction in computational
complexity and improved performance. However, when the state space is high-dimensional and
further, the state transition is probabilistic, high computational complexity associated with calculating
the expected Q values of next states renders existing Q-Learning methods impractical.
1
Under review as a conference paper at ICLR 2021
A potential solution for this problem lies in approximating the expectation of Q values of next states
with the sample mean of Q values over a subset of next states. A natural way to select a subset of next
states is by drawing IID samples from the transition probability distribution. However, this straight
forward approach becomes challenging when the state transition probability distribution is high-
dimensional and is known only up to a constant. We address this problem by using Hamilton Monte
Carlo (HMC) to sample next states; HMC draws samples by integrating a Hamiltonian dynamics
governed by the transition probability (Neal et al., 2011). We improve the data efficiency further by
using matrix completion methods to exploit the low rank structure of a Q matrix.
Related Work
Data efficient Reinforcement Learning: The last decade has witnessed a growing interest in
improving data efficiency in RL methods by exploiting emergent global structures from underlying
system dynamics. Deisenroth & Rasmussen (2011); Pan & Theodorou (2014); Kamthe & Deisenroth
(2018); Buckman et al. (2018) have proposed model-based RL methods that improve data efficiency
by explicitly incorporating prior knowledge about state transition dynamics of the underlying sys-
tem. Dearden et al. (1998); Koppel et al. (2018); Jeong et al. (2017) propose Baysean methods to
approximate the Q function. Ong (2015) and Yang et al. (2020b) consider a model-free RL approach
that exploit structures of state-action value function. The work by Ong (2015) decomposes the Q
matrix into a low-rank and sparse matrix model and uses matrix completion methods (Candes & Plan,
2010; Wen et al., 2012; Chen & Chi, 2018) to improve data efficiency. A more recent work by Yang
et al. (2020b) has shown that incorporation of low rank matrix completion methods for recovering Q
matrix from a small subset of Q values can improve learning of optimal policies. At each time step
the agent chooses a subset of state-action pairs and update their Q value according to the Bellman
optimally equation that considers a discounted average between reward and expectation of the Q
values of next states. Shah et al. (2020) extends this work by proposing a novel matrix estimation
method and providing theoretical guarantees for the convergence to a -optimal Q function. On the
other hand, entropy regularization (Ahmed et al., 2019; Yang et al., 2019; Smirnova & Dohmatob,
2020), by penalizing excessive randomness in the conditional distribution of actions for a given state,
provides an alternative means to implicitly exploit the underlying low-dimensional structure of the
value function. Lee et al. (2019) proposes an approach that samples a whole episode and then updates
values in a recursive, backward manner.
Contribution
The main contribution of this work is three-fold. First, we introduce a modified Q-learning framework,
called Hamiltonian Q-learning, which uses HMC sampling for efficient computation of Q values.
This innovation, by proposing to sample Q values from the region with the dominant contribution
to the expectation of discounted reward, provides a data-efficient approach for using Q-learning in
real-world problems with high-dimensional state space and probabilistic state transition. Furthermore,
integration of this sampling approach with matrix-completion enables us to update Q values for only
a small subset of state-action pairs and thereafter reconstruct the complete Q matrix. Second, we
provide theoretical guarantees that the error between the optimal Q function and the Q function
obtained by updating Q values using HMC sampling can be made arbitrarily small. This result also
holds when only a handful of Q values are updated using HMC and the rest are estimated using matrix
completion. Further, we provide theoretical guarantee that the sampling complexity of our algorithm
matches the mini-max sampling complexity proposed by Tsybakov (2008). Finally, we demonstrate
the effectiveness of Hamiltonian Q-learning by applying it to a cart-pole stabilization problem and
an adaptive ocean sampling problem. Our results also indicate that our proposed approach becomes
more effective with increase in state space dimension.
2	Preliminary Concepts
In this section, we provide a brief background on Q-Learning, HMC sampling and matrix completion,
as well as introduce the mathematical notations. In this paper, |Z| denotes the cardinality of a set Z.
Moreover, R represent the real line and AT denotes the transpose of matrix A.
2.1	Q-LEARNING
Markov Decision Process (MDP) is a mathematical formulation that captures salient features of
sequential decision making (Bertsekas, 1995). In particular, a finite MDP is defined by the tuple
2
Under review as a conference paper at ICLR 2021
(S, A, P, r, γ), where S is the finite set of system states, A is the finite set of actions, P : S × A× S →
[0, 1] is the transition probability kernel, r : S × A → R is a bounded reward function, and γ ∈ [0, 1)
is a discounting factor. Without loss of generality, states s ∈ S and actions a ∈ A can be assumed to
be Ds-dimensional and Da-dimensional real vectors, respectively. Moreover, by letting si denote the
ith element of a state vector, we define the range of state space in terms of the following intervals
[di-, di+] such that si ∈ [di-, di+] ∀i ∈ {1, . . . , Ds}. At each time t ∈ {1, . . . , T} over the decision
making horizon, an agent observes the state of the environment st ∈ S and takes an action at
according to some policy π which maximizes the discounted cumulative reward. Once this action
has been executed, the agent receives a reward r(st , at) from the environment and the state of
the environment changes to st+ι according to the transition probability kernel P (∙∣st, at). The Q
function, which represents the expected discounted reward for taking a specific action at the current
time and following the policy thereafter, is defined as a mapping from the space of state-action pairs
to the real line, i.e. Q : S × A → R. Then, by letting Qt represent the Q matrix at time t, i.e. the
tabulation of Q function over all possible state-action pairs associated with the finite MDP, we can
express the Q value iteration over time steps as
Qt+1 (st, at) = X P (s|st, at) r(st, at) + γ max Qt(s, a) .
s∈S	a
(1)
Under this update rule, the Q function converges to its unique optimal value Q* (Melo, 2001). But
computing the sum (1) over all possible next states is computationally expensive in certain problems;
in these cases taking the summation over a subset of the next states provides an efficient alternative
for updating the Q values.
2.2	Hamiltonian Monte Carlo
Hamiltonian Monte Carlo is a sampling approach for drawing samples from probability distributions
known up to a constant. It offers faster convergence than Markov Chain Monte Carlo (MCMC)
sampling (Neal et al., 2011; Betancourt; Betancourt et al., 2017; Neklyudov et al., 2020). To draw
samples from a smooth target distribution P(s), which is defined on the Euclidean space and assumed
to be known up to a constant, HMC extends the target distribution to a joint distribution over the
target variable s (viewed as position within the HMC context) and an auxiliary variable v (viewed as
momentum within the HMC context). We define the Hamiltonian of the system as
H(s, v) = - log P (s, v) = -logP(s) - logP(v|s) = U(s) + K(v, s),
where U(S)，- log P(S) and K(v, S)，- log P(v|s) = 2VTMTv represent the potential and
kinetic energy, respectively, and M is a suitable choice of the mass matrix.
HMC sampling method consists of the following three steps - (i) a new momentum variable v
is drawn from a fixed probability distribution, typically a multivariate Gaussian; (ii) then a new
proposal (S0, v0) is obtained by generating a trajectory that starts from (S, v) and obeys Hamiltonian
dynamics, i.e. S = ∂∂H ,v = - ∂∂S; and (iii) finally this new proposal is accepted with probability
min {1, exp (H(s, v) — H(s0, -v0))} following the Metropolis-Hastings acceptance/rejection rule.
2.3	LOW-RANK STRUCTURE IN Q-LEARNING AND MATRIX COMPLETION
Prior work (Johns & Mahadevan, 2007; Geist & Pietquin, 2013; Ong, 2015; Shah et al., 2020) on
value function approximation based approaches for RL has implicitly assumed that the state-action
value functions are low-dimensional and used various basis functions to represent them, e.g. CMAC,
radial basis function, etc. This can be attributed to the fact that the underlying state transition and
reward function are often endowed with some structure. More recently, Yang et al. (2020b) provide
empirical guarantees that the Q-matrices for benchmark Atari games and classical control tasks
exhibit low-rank structure.
Therefore, using matrix completion techniques (Xu et al., 2013; Chen & Chi, 2018) to recover
Q ∈ RlSl×lAl from a small number of observed Q values constitutes a viable approach towards
improving data efficiency. As low-rank matrix structures can be recovered by constraining the nuclear
norm, the Q matrix can be reconstructed from its observed values (Q) by solving
Q = arg min IlQl∣* subject to Jω(Q) = Jω(Q),	(2)
. _ ....
Q∈RlSl×lAl
where ∣∣ ∙ k* denotes the nuclear norm (i.e., the sum of its singular values), Ω is the observed set of
elements, and Jω is the observation operator, i.e. Jω(x) = X if X ∈ Ω and zero otherwise.
3
Under review as a conference paper at ICLR 2021
3 HAMILTONIAN Q-LEARNING
A large class of real world sequential decision making problems - for example, board/video games,
control of robots’ movement, and portfolio optimization - involves high-dimensional state spaces and
often has large number of distinct states along each individual dimension. As using a Q-Learning
based approach to train RL-agents for these problems typically requires tens to hundreds of millions
of samples (Mnih et al., 2015; Silver et al., 2017), there is a strong need for data efficient algorithms
for Q-Learning. In addition, state transition in such systems is often probabilistic in nature; even when
the underlying dynamics of the system is inherently deterministic; presence of external disturbances
and parameter variations/uncertainties lead to probabilistic state transitions.
Learning an optimal Q* function through value iteration methods requires updating Q values of
state-action pairs using a sum of the reward and a discounted expectation of Q values associated with
next states. In this work, we assume the reward to be a deterministic function of state-action pairs.
However, when the reward is stochastic, these results can be extended by replacing the reward with
its expectation. Subsequently, we can express (1) as
Qt+1(st,at) = r(st, at) +γE max Qt (s, a) ,	(3)
where E denotes the expectation over the discrete probability measure P. When the underlying state
space is high-dimensional and has large number of states, obtaining a more accurate estimate of
the expectation is computationally very expensive. The complexity increases quadratically with the
number of states and linearly with number of actions, rendering the existing algorithms impractical.
In this work, we propose a solution to this issue by introducing an importance-sampling based method
to approximate the aforementioned expectation from a sample mean of Q values over a subset of next
states. A natural way to sample a subset from the set of all possible next states is to draw identically
and independently distributed (IID) samples from the transition probability distribution P(∙∣ st ,at).
However, when the transition probability distribution is high-dimensional and known only up to a
constant, drawing IID samples incurs a very high computation cost.
		
		
		
Psuoɪsuəmɪɑlɪ
|s - sMode|
一P(S)
Psuoɪsuəmɪɑ-
|s - sMode|
ds ----------- P(s)ds
PeU2suθlulα∞
|s - sMode|
★ HMC Samples
Figure 1: The first row illustrates that, as the dimension of the space increases, the relative volume inside
a partition compared to the volume outside of the partition decreases. When the dimension increases from 1
through 3, the relative volume of red partition decreases as 1/3, 1/9 and 1/27, respectively. The second row
illustrates that the HMC samples are concentrated in the region that maximizes probability mass. Here P(s),
sMode and ds represent probability density, mode of the distribution and volume, respectively.
3.1	Data efficiency through HMC sampling
A number of importance-sampling methods (Liu, 1996; Betancourt) have been developed for estimat-
ing the expectation of a function by drawing samples from the region with the dominant contribution
to the expectation. HMC is one such importance-sampling method that draws samples from the typi-
cal set, i.e., the region that maximizes probability mass, which provides the dominated contribution
to the expectation. As shown in the second row of Figure 1, most of the samples in a limited pool
of HMC samples indeed concentrate around the region with high probability mass. Since the decay
in Q function is significantly smaller compared to the typical exponential or power law decays in
transition probability function, HMC provides a better approximation for the expectation of the Q
value of the next states (Yang et al., 2020b; Shah et al., 2020). Then by letting Ht denote the set of
4
Under review as a conference paper at ICLR 2021
HMC samples drawn at time step t, we update the Q values as:
Qt+1(st,at) = r(st,at) + J T maxQt(s,a).
|Ht| s∈Ht a
(4)
HMC for a smooth truncated target distribution: Recall that region of states is a subset of a
Euclidean space given as s ∈ [d1-, d1+] × . . . × [d-D , d+D ] ⊂ RDs. Thus the main challenge to using
HMC sampling is to define a smooth continuous target distribution P(s|st, at) which is defined on
RDs with a sharp decay at the boundary of the region of states (Yi & Doshi-Velez, 2017; Chevallier
et al., 2018). In this work, we generate the target distribution by first defining the transition probability
kernel from the conditional probability distribution defined on RDs and then multiplying it with a
smooth cut-off function.
We first consider a probability distribution P(∙∣st,at) : RDs → R such that the following holds
P(s|st, at)
s+ε
H /	P(s∣st, at)ds
s-ε
for some arbitrarily small ε > 0. Then the target distribution can be defined as
P(s|st, at)
Ds
P(s|st, at) Y
i=1
1	1
1 + exp(-κ(d+ — Si)) 1 + exp(一κ(si — d-))
(5)
(6)
Note that there exists a large κ > 0 such that if s ∈ [d1-, d1+] × . . . × [d-D , d+D ] then P(s|st, at) H
P(s∣st, at) and P (s|st, at) ≈ 0 otherwise. Let μ(st, at), Σ(st, at) be the mean and covariance of the
transition probability kernel. In this paper we consider transition probability kernels of the form
P(s∣st, at) H exp (— 1(s — μ(st, at))T∑-1(st, at)(s — μ(st, at))) .	(7)
Then from (5) the corresponding mapping can be given as a multivariate Gaussian P(s|st, at) =
N(μ (st,at), Σ(st,at)). ThUS from (6) it follows that the target distribution is
Ds	1	1
P(slst,at) = N(μ (st,at), `et,at))ɪɪ —---------7-++----i	i~~-j-ττ	(8)
i=1 1 + exp(—κ(d+ — Si)) 1 + exp(一κ(si — d- ))
Choice of potential energy, kinetic energy and mass matrix: Recall that the target distribution
P(S|St, at) is defined over the Euclidean space RDs . For brevity of notation we drop the explicit
dependence on (St, at) and denote the target distribution as P(S). As explained in Section 2.2
we choose the potential energy U(S) = — log (P(S)). We consider an Euclidean metric M that
induces the distance between S S as d(s, s) = (s — s)tM(G — s). Then we define Ms ∈ RDs ×Ds
as a diagonal scaling matrix and Mr ∈ RDs ×Ds as a rotation matrix in dimension Ds. With
this we can define M as M = MrMs MMsTMrT. Thus, any metric M that defines an Euclidean
structure on the target variable space induces an inverse structure on the momentum variable space as
d(vS, vS) = (vS—vS)TM-1(vS—vS). This generates a natural family of multivariate Guassian distributions
such that P(v|s) = N(0, M) leading to the kinetic energy K(v, s) = — log P(v|s) = 2vτMTv
where M-1 is the covariance of the target distribution.
3.2	Q-LEARNING WITH HMC AND MATRIX COMPLETION
In this work we consider problems with a high-dimensional state space and large number of distinct
states along individual dimensions. Although these problems admit a large Q matrix, we can exploit
low rank structure of the Q matrix to further improve the data efficiency.
At each time step t we randomly sample a subset Ωt of state-action pairs (each state-action pair is
sampled independently with some probability p) and update the Q function for state-action pairs in
Ωt. Let Qt+1 be the updated Q matrix at time t. Then from (4) we have
Qt+1(st,at)= r(st,at)+	X maxQt(S,a),	∀(st,at) ∈ Ct∙	⑼
|Ht| s∈Ht a
Then we recover the complete matrix Qt+1 by using the method given in (2). Thus we have
Qt+1 =〜arg min	kQt+1k* subjectto J「t (Qt+1) = J0t (Qt+1) .	(10)
Qt+1∈RlSl×lAl	'	'	'	'
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Hamiltonian Q-Learning
Inputs: Discount factor γ; Range of state space; Time horizon T ;
Initialization: Randomly initialize Q0
for t = 1 to T do
Step 1: Randomly sample a subset of state-action pairs Ωt
Step 2: HMC sampling phase - Sample a set of next states Ht according to the target distribu-
tion defined in (6)
Step 3: Update phase - For all (st, at) ∈ Ωt
Yf-∖-Y
Q +1(st, at) = r(st, at) + ∣⅛ Es∈Ht max。Qt(s, a)
Step 4: Matrix Completion phase
Qt+1 = argminQt+ι∈R∣s∣×∣A∣kQt+1k* subject to Jω4 (Qt+1) = Jω4 (Qt+1)
end for
Similar to the approach used by Yang et al. (2020b), we approximate the rank of the Q matrix as the
minimum number of singular values that are needed to capture 99% of its nuclear norm.
3.3	Convergence, B oundedness and Sampling Complexity
In this section we provide the main theoretical results of this paper. First, we formally introduce the
following regularity assumptions:
(A1) The state space S ⊆ RDs and the action space A ⊆ RDa are compact subsets.
(A2) The reward function is bounded, i.e., r(s, a) ∈ [Rmin, Rmax] for all (s, a) ∈ S × A.
(A3) The optimal value function Q* is C -Lipschitz, i.e.
∣Q*(s,a) - Q*(s0, a0)∣ ≤ C(||s - s0∣∣f + ||a - a0∣∣F)
where || ∙ ||f is the Frobenius norm (which is same as the Euclidean norm for vectors).
We provide theoretical guarantees that Hamiltonian Q-Learning converges to an -optimal Q function
with O (eDs+Da+2) number of samples. This matches the mini-max lower bound Ω (eDs+Da+2)
proposed in Tsybakov (2008). First we define a family of -optimal Q functions as follows.
Definition 1 (-optimal Q functions). Let Q* be the unique fixed point of the Bellman optimality
equation given as (T Q)(s0, a0) = s∈S P(s|s0, a0) (r(s0, a0) + γ max。 Q(s, a)) ∀(s0, a0) ∈ S × A
where T denotes the Bellman operator. Then, under update rule (3), the Q function almost surely
converges to the optimal Q*. We define -optimal Q functions as the family of functions Q such that
kQ0 - Q*k∞ ≤ whenever Q0 ∈ Q.
As IIQ0 - Q* k∞ = max(s,a)∈s×A ∣∣Q0(s, a) - Q*(s, a)k, any e-optimal Q function is element wise
-optimal. Our next result shows that under HMC sampling rule given in Step 3 of the Hamiltonian
Q-Learning algorithm (Algorithm 1), the Q function converges to the family of e-optimal Q functions.
Theorem 1 (Convergence of Q function under HMC). Let T be an optimality operator under
HMCgiven as (TQ)(s0, a0) = r(s0, a0) + 讶 Σ2s∈h max。Q(s, a), ∀(s0, a0) ∈ S × A, where H is
a subset of next states sampled using HMC from the target distribution given in (6). Then, under
update rule (4) and for any given e ≥ 0, there exists nH , t0 > 0 such that IQt - Q* I∞ ≤ e ∀t ≥ t0.
Refer Appendix A.1 for proof of this theorem. The next theorem shows that the Q matrix estimated
via a suitable matrix completion technique lies in the e-neighborhood of the corresponding Q function
obtained via exhaustive sampling.
Theorem 2 (Bounded Error under HMC with Matrix Completion). Let QtE+1(st, at) =
r(st, at) + γ s∈S P(s|st, at) max。 QtE (s, a), ∀(st, at) ∈ S × A be the update rule under ex-
haustive sampling, and Qt be the Q function updated according to Hamiltonian Q-Learning (9)-(10).
Then, for any given e ≥ 0, there exists n∏ = minr ∣Hτ∣,t0 > 0, Such that ∣∣Qt 一 QE ∣∣∞ ≤ e Vt ≥ t0.
Please refer Appendix A.2 for proof of this theorem. Finally we provide guarantees on sampling
complexity of Hamiltonian Q-Learning algorithm.
Theorem 3. (Sampling complexity of Hamiltonian Q-Learning) Let Ds, D。 be the dimension
of state space and action space, respectively. Consider the Hamiltonian Q-Learning algorithm
presented in Algorithm 1. Then, under a suitable matrix completion method, the Q function convergea
to the family of e-optimal Q functions with Oe e-(Ds +Da +2) number of samples.
6
Under review as a conference paper at ICLR 2021
Proof of Theorem 3 is given in Appendix B.
4 Experiments
4.1	Empirical Evaluation for Cart-pole
Experimental setup: By letting θ, θ denote the angle and angular velocity of the pole and x, X
denote the position and velocity of the cart, the 4-dimensional state vector for the cart-pole system
can be defined as S = (θ, θ,χ,χ). After defining the range of state space as θ ∈ [-∏∕2,∏∕2],
θ ∈ [-3.0,3.0], x ∈ [-2.4,2.4] and X ∈ [-3.5, 3.5], we define the range of the scalar action as
a ∈ [-10, 10]. Then each state space dimension is discretized into 5 distinct values and the action
space into 10 distinct values. This leads to a Q matrix of size 625 × 10. To capture parameter
uncertainties and external disturbances, we assume that the probabilistic state transition is governed
by a multivariate Gaussian with zero mean and covariance Σ = diag[0.143, 0.990, 0.635, 1.346]. To
drive the pole to an upright position, we define the reward function as r(s, a) = cos4 (15θ) (Yang
et al., 2020b). After initializing the Q matrix using randomly chosen values from [0, 1], we sample
state-action pairs independently with probability p = 0.5 at each iteration. Additional experimental
details and results are provided in Appendix C.
Results: As it is difficult to visualize a heat map for a 4-dimensional state space, we show results for
the first two dimensions θ, θ with fixed x, X. The color In each cell of the heat maps shown In Figures
2(a), 2(b) and 2(c) indicates the value of optimal action associated with that state. These figures
illustrate that the policy heat map for Hamiltonian Q-Learning is closer to the policy heat map for
Q-Learning with exhaustive sampling. The two curves in Figure 2(d), that show the Frobenius norm
of the difference between the learned Q function and the optimal Q* , illustrate that Hamiltonian
Q-Learning achieves better convergence than Q-Learning with IID sampling. We also show that the
sampling efficiency of any Q-Learning algorithm can be significantly improved by incorporating
Hamiltonian Q-Learning. We illustrate this by incorporating Hamiltonian Q-Learning with vanilla
Q-Learning, DQN, Dueling DQN and DDPG. Figure 3 shows how Frobenius norm of the error
between Q function and the optimal Q* varies with increase in the number of samples. Red solid
curves correspond to the case with exhaustive sampling and black dotted curves correspond to the
case with Hamiltonian Q-Learning. These results illustrate that Hamiltonian Q-Learning converges
to an optimal Q function with significantly smaller number of samples than exhaustive sampling.
(a) Exhaustive Sampling (b) HMC Sampling	(c) IID Sampling
Figure 2: Figure 2(a), 2(b) and 2(c) show policy heat maps for Q-Learning with exhaustive sampling, Hamil-
tonian Q-Learning and Q-Learning with IID sampling, respectively. Figure 2(d) provides a comparison for
convergence of Q function with Hamiltonian Q-Learning and Q-Learning with IID sampling.
(d) Q Function Convergence
0.0
∙0∙8∙64∙2
L
*0—0=
0.0
0.5
1.0
Number of samples le8
0.0	0.5	1.0
Number of samples le8
0.0	0.5	1.0	0.0	0.5	1.0
Number of samples le8	Number of samples le8
Figure 3: Mean square error Vs number of samples of Q function With exhaustive sampling and Hamiltonian
Q-Learning for vanila Q-Learning, DQN, Dueling DQN and DDPG.
4.2	Empirical Evaluation for Acrob ot (i.e., Double Pendulum)
Experimental setup: By letting θι, θ`,θ2, θ? denote the angle of the first pole, angular velocity
of the first pole, angle of the second pole and angular velocity of the second pole, respectively, the
4-dimensional state vector for the acrobot can be defined as s = (θ1, θ1, θ2, θ2).After defining the
7
Under review as a conference paper at ICLR 2021
range of state space as θι ∈ [-∏, ∏], θι ∈ [-3.0, 3.0], θ2 ∈ [-∏, ∏] and θ2 ∈ [-3.0, 3.0], We define
the range of the scalar action as a ∈ [-10, 10]. Then each state space dimension is discretized into 5
distinct values and the action space into 10 distinct values. This leads to a Q matrix of size 625 × 10.
Furthermore, We assume that the probabilistic state transition is governed by a multivariate Gaussian
With zero mean and covariance Σ = diag[0.143, 0.990, 0.635, 1.346]. FolloWing Sutton & Barto
(2018), We define an appropriate reWard function for stabilizing the acrobot to the upright position.
After initializing the Q matrix using randomly chosen values from [0, 1], We sample state-action pairs
independently With probability p = 0.5 at each iteration.
Results: Figure 4 illustrates hoW Frobenius norm of the error betWeen Q function and the optimal
Q* varies with the number of samples. Red solid curves correspond to the case with exhaustive
sampling and black dotted curves correspond to the case With Hamiltonian Q-Learning. These results
show that for the same level of error Hamiltonain Q-Learning requires a significantly smaller number
of samples compared to exhaustive sampling.
Figure 4: Mean square error vs number of samples of Q function with exhaustive sampling and Hamiltonian
Q-Learning for vanila Q-Learning, DQN, Dueling DQN and DDPG.
4.3	Application to Ocean Sampling
Ocean sampling plays a major role in a variety of science and engineering problems, ranging from
modeling marine ecosystems to predicting global climate. Here, we consider the problem of using an
under water glider to obtain measurements ofa scalar field (e.g., temperature, salinity or concentration
of a certain zooplankton) and illustrate how the use of Hamiltonian Q-Learning in planning the glider
trajectory can lead to measurements that minimize the uncertainty associated with the field.
States, actions and state transition: By assuming that the glider’s motion is restricted to an
horizontal plane (Refael & Degani, 2019), we let x, y and θ denote its center of mass position and
heading angle, respectively. Then we can define the 6-dimensional state vector for this system as s =
(x, y, X, y, θ, θ) and the action a as a scalar control input to the glider. Also, to accommodate dynamic
perturbations due to the ocean current, other external disturbances and parameter uncertainties, we
assume that the probabilistic state transition is governed by a multivariate Gaussian.
Reward: As ocean fields often exhibit temporal and spatial correlations (Leonard et al., 2007),
this work focuses on spatially correlated scalar fields. Following the approach of Leonard et al.
(2007), we define ocean statistic correlation between two positions q = (x, y) and q0 = (x0, y0)
as B(q, q0) = exp(-∣∣q - q0∣∣2∕σ2), where σ is the spatial decorrelation scale. The goal of the
task is to take measurements that reduce the uncertainty associated with the field. Now we assume
that the glider takes N measurements at positions {q1, . . . , qN}. Then covariance of the collected
data set can be given by a N × N matrix W such that its ith row and the jth column element
is: Wij = ηδij + B(qi, qj), where δij is the Dirac delta and η is the variance of the uniform and
uncorrelated measurement noise. Then using objective analysis data assimilation scheme (Kalnay,
2003; Bennett, 2005), the total reduction of uncertainty of the field after the measurements at positions
Q = {q1, . . . , qN } can be expressed as
N
U= X X B(q,qi)Wi-j1B(qj,q),	(11)
q∈Q i,j=1
By substituting the formulas from (Kalnay, 2003; Bennett, 2005) into (11), this formulation can be
generalized to Gaussian measurement noise.
Recall that the task objective is to guide the glider to take measurements at multiple locations/positions
which maximize the reduction in uncertainty associated with the scalar field. Therefore the reward
assigned to each state-action pair (s, a) is designed to reflect the amount of uncertainty that can
8
Under review as a conference paper at ICLR 2021
be reduced by taking a measurement at the position corresponding to the state and at the positions
that correspond to the set of maximally probable next states, i.e., arg maxs0 P(s0|s, a). Then, by
letting Zs = {s} ∪ {∪a∈A{arg maxs0 P(s0|s, a)}} denote the set of current state s and the maximally
probable next states for all possible actions, the reward function associated with reducing uncertainty
can be given as
ru(s,a) = X X B(q, qi)Wi-j1B(qj, q).
q∈Q i,j∈Zs
Without loss of generality, we assume that the glider is deployed from q = (0, 0) and retrieving the
glider incurs a cost depending on its position. To promote trajectories that do not incur a high cost for
glider retrieval, we define the following reward function
rc(s, a) = -qT Cq
where C = CT ≥ 0. Then we introduce the total reward that aims to reduce uncertainty of the scalar
field while penalizing the movements away from the origin, and define it as
r(s, a) = ru(s,a) + rc(s,a) = -λqT Cq +	B(q,qi)Wi-j1B(qj,q),
q∈Q i,j∈Zs
where λ > 0 is a trade-off parameter that maintains a balance between these two objectives.
Experimental setup We define the range of state and action space as χ,y ∈ [-10,10], X,y ∈
[-25, 25], θ ∈ [-π, π], θ ∈ [-3, 3], and a ∈ [-1, 1], respectively and then discretizing each state
dimension into 5 distinct values and the action space into 5 distinct values, we have a Q matrix of size
15625 × 5. Also, we assume that the state transition kernel is given by a multivariate Gaussian with
zero mean and covariance Σ = diag[11.111, 69.444, 11.111, 69.444, 0.143, 0.990]. After initializing
the Q matrix using randomly chosen values from [0, 1], we sample state-action pairs independently
with probability p = 0.5 at each iteration. Also, we assume σ = 2.5, λ = 0.1, C = diag[1, 0].
Additional experimental details and results are provided in Appendix D.
(a) Exhaustive Sampling.
(b) HMC Sampling.
(c) IID Sampling.
Time (t)
(d) Q-function Error.
Figure 5: Figure 5(a), 5(b) and 5(c) show policy heat maps for Q-Learning with exhaustive sampling, Hamil-
tonian Q-Learning and Q-Learning with IID sampling respectively. Figure 5(d) provides a comparison for
convergence of Q function with Hamiltonian Q-Learning and Q-Learning with IID sampling.
Results Figures 5(a), 5(b) and 5(c) show the policy heat map over first two dimensions x, y with
fixed X,y, θ and θ. The color of each cell indicates the value of optimal action associated With
that state. These figures illustrate that the difference between policy heat maps associated with
Hamiltonian Q-Learning and Q-Learning With exhaustive sampling is smaller than the difference
betWeen policy heat maps associated With Q-Learning With IID sampling and Q-Learning With
exhaustive sampling. The tWo curves in Figure 5(d), that shoW the Frobenius norm of the difference
between the learned Q function and the optimal Q*, illustrate that Hamiltonian Q-Learning achieves
better convergence than Q-Learning With IID sampling. A comparison betWeen results of the ocean
sampling problem and the cart-pole stabilization problem indicates that Hamiltonian Q-Learning
provides increasingly better performance with increase in state space dimension.
5 Discussion and Conclusion
Here we have introduced Hamiltonian Q-Learning which utilizes HMC sampling with matrix
completion methods to improve data efficiency. We show, both theoretically and empirically, that
the proposed approach can learn very accurate estimates of the optimal Q function with much fewer
data points. We also demonstrate that Hamiltonian Q-Learning performs significantly better than
Q-Learning with IID sampling when the underlying state space dimension is large. By building
upon this aspect, future works will investigate how importance-sampling based methods can improve
data-efficiency in multi-agent Q-learning with agents coupled through both action and reward.
9
Under review as a conference paper at ICLR 2021
References
Zafarali Ahmed, Nicolas Le Roux, Mohammad Norouzi, and Dale Schuurmans. Understanding the
impact of entropy on policy optimization. In International Conference on Machine Learning, pp.
151-160. PMLR, 2019.
Andrew F Bennett. Inverse modeling of the ocean and atmosphere. Cambridge University Press,
2005.
Dimitri P Bertsekas. Dynamic Programming and Optimal Control, volume 1. Athena Scientific
Belmont, MA, 1995.
Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo.
Michael Betancourt, Simon Byrne, Sam Livingstone, Mark Girolami, et al. The geometric foundations
of Hamiltonian Monte Carlo. Bernoulli, 23(4A):2257-2298, 2017.
Jacob Buckman, Danijar Hafner, George Tucker, Eugene Brevdo, and Honglak Lee. Sample-
efficient reinforcement learning with stochastic ensemble value expansion. In Advances in Neural
Information Processing Systems, pp. 8224-8234, 2018.
Emmanuel J Candes and Yaniv Plan. Matrix completion with noise. Proceedings of the IEEE, 98(6):
925-936, 2010.
Yudong Chen and Yuejie Chi. Harnessing structures in big data via guaranteed low-rank matrix
estimation: Recent theory and fast algorithms via convex and nonconvex optimization. IEEE
Signal Processing Magazine, 35(4):14-31, 2018.
AUgUstin Chevallier, Sylvain Pion, and Frederic Cazals. Hamiltonian Monte Carlo with boundary
reflections, and application to polytope volume calculations. 2018.
Richard Dearden, Nir Friedman, and StUart RUssell. Bayesian q-learning. In Aaai/iaai, pp. 761-768,
1998.
Marc Deisenroth and Carl E RasmUssen. Pilco: A model-based and data-efficient approach to policy
search. In International Conference on Machine Learning (ICML), pp. 465-472, 2011.
Yan DUan, Xi Chen, Rein HoUthooft, John SchUlman, and Pieter Abbeel. Benchmarking deep
reinforcement learning for continUoUs control. In International Conference on Machine Learning
(ICML), pp. 1329-1338, 2016.
Jianqing Fan, Bai Jiang, and Qiang SUn. Hoeffding’s lemma for markov chains and its applications
to statistical learning. arXiv preprint arXiv:1802.00211, 2018.
MatthieU Geist and Olivier PietqUin. Algorithmic sUrvey of parametric valUe fUnction approximation.
IEEE Transactions on Neural Networks and Learning Systems, 24(6):845-867, 2013.
SUsan Holmes, Simon RUbinstein-Salzedo, and Christof Seiler. CUrvatUre and concentration of
Hamiltonian Monte Carlo in high dimensions. arXiv:1407.1114, 2014.
Heejin Jeong, Clark Zhang, George J Pappas, and Daniel D Lee. AssUmed density filtering q-learning.
arXiv preprint arXiv:1712.03333, 2017.
Jeff Johns and Sridhar Mahadevan. ConstrUcting basis fUnctions from directed graphs for valUe
fUnction approximation. In Proceedings of the 24th international conference on Machine learning,
pp. 385-392, 2007.
EUgenia Kalnay. Atmospheric modeling, data assimilation and predictability. Cambridge University
Press, 2003.
Sanket Kamthe and Marc Deisenroth. Data-efficient reinforcement learning with probabilistic
model predictive control. In International Conference on Artificial Intelligence and Statistics, pp.
1701-1710. PMLR, 2018.
10
Under review as a conference paper at ICLR 2021
Jens Kober, J Andrew Bagnell, and Jan Peters. Reinforcement learning in robotics: A survey. The
International Journal ofRobotics Research, 32(11):1238-1274, 2013.
Alec Koppel, Ekaterina Tolstaya, Ethan Stump, and Alejandro Ribeiro. Nonparametric stochastic
compositional gradient descent for q-learning in continuous markov decision problems. arXiv
preprint arXiv:1804.07323, 2018.
Su Young Lee, Choi Sungik, and Sae-Young Chung. Sample-efficient deep reinforcement learning via
episodic backward update. In Advances in Neural Information Processing Systems, pp. 2112-2121,
2019.
Naomi E. Leonard, Derek A. Paley, Francois Lekien, Rodolphe Sepulchre, David M. Fratantoni, and
Russ E. Davis. Collective motion, sensor networks, and ocean sampling. Proceedings of the IEEE,
95(1):48-74, 2007.
Jun S Liu. Metropolized independent sampling with comparisons to rejection sampling and impor-
tance sampling. Statistics and Computing, 6(2):113-119, 1996.
TWU Madhushani, DH Sanjeeva Maithripala, and Jordan M Berg. Feedback regularization and
geometric pid control for trajectory tracking of mechanical systems: Hoop robots on an inclined
plane. In 2017 American Control Conference (ACC), pp. 3938-3943. IEEE, 2017.
DHS Maithripala, TWU Madhushani, and JM Berg. A Geometric PID Control Framework for
Mechanical Systems. arXiv:1610.04395, 2016.
Hongzi Mao, Mohammad Alizadeh, Ishai Menache, and Srikanth Kandula. Resource management
with deep reinforcement learning. In Proceedings of the 15th ACM Workshop on Hot Topics in
Networks, pp. 50-56, 2016.
Rowan McAllister and Carl Edward Rasmussen. Data-efficient reinforcement learning in continuous
state-action Gaussian-POMDPs. In Advances in Neural Information Processing Systems, pp.
2040-2049, 2017.
Francisco S Melo. Convergence of Q-learning: A simple proof. Institute Of Systems and Robotics,
Tech. Rep, pp. 1-4, 2001.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,
Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control
through deep reinforcement learning. Nature, 518(7540):529-533, 2015.
Radford M Neal et al. MCMC using Hamiltonian dynamics. Handbook of Markov Chain Monte
Carlo, 2(11):2, 2011.
Kirill Neklyudov, Max Welling, Evgenii Egorov, and Dmitry Vetrov. Involutive MCMC: A Unifying
Framework. arXiv:2006.16653, 2020.
Hao Yi Ong. Value function approximation via low-rank models. arXiv:1509.00061, 2015.
Yunpeng Pan and Evangelos Theodorou. Probabilistic differential dynamic programming. In
Advances in Neural Information Processing Systems, pp. 1907-1915, 2014.
Gilad Refael and Amir Degani. A single-actuated swimming robot: Design, modelling, and experi-
ments. Journal of Intelligent & Robotic Systems, 94(2):471-489, 2019.
Devavrat Shah, Dogyoon Song, Zhi Xu, and Yuzhe Yang. Sample efficient reinforcement learning
via low-rank matrix estimation. arXiv:2006.06135, 2020.
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez,
Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of Go
without human knowledge. Nature, 550(7676):354-359, 2017.
Elena Smirnova and Elvis Dohmatob. On the convergence of smooth regularized approximate value
iteration schemes. Advances in Neural Information Processing Systems, 33, 2020.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
11
Under review as a conference paper at ICLR 2021
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008.
Christopher JCH Watkins. Learning from delayed rewards. PhD thesis, King’s College, Cambridge,
Cambridge, UK, 1989.
Christopher JCH Watkins and Peter Dayan. Q-learning. Machine learning, 8(3-4):279-292, 1992.
Zaiwen Wen, Wotao Yin, and Yin Zhang. Solving a low-rank factorization model for matrix
completion by a nonlinear successive over-relaxation algorithm. Mathematical Programming
Computation, 4(4):333-361, 2012.
Yangyang Xu, Ruru Hao, Wotao Yin, and Zhixun Su. Parallel matrix factorization for low-rank tensor
completion. arXiv:1312.1254, 2013.
Wenhao Yang, Xiang Li, and Zhihua Zhang. A regularized approach to sparse optimal policy in
reinforcement learning. In Advances in Neural Information Processing Systems, pp. 5940-5950,
2019.
Yuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan Zhang, Jie Tan, and Vikas Sindhwani. Data
efficient reinforcement learning for legged robots. In Conference on Robot Learning, pp. 1-10.
PMLR, 2020a.
Yuzhe Yang, Guo Zhang, Zhi Xu, and Dina Katabi. Harnessing structures for value-based planning
and reinforcement learning. In International Conference on Learning Representations (ICLR),
2020b.
Kexin Yi and Finale Doshi-Velez. Roll-back Hamiltonian Monte Carlo. arXiv:1709.02855, 2017.
Zhenpeng Zhou, Xiaocheng Li, and Richard N Zare. Optimizing chemical reactions with deep
reinforcement learning. ACS Central Science, 3(12):1337-1344, 2017.
A Convergence and B oundedness Results
We proceed to prove theorem by stating convergence properties for HMC as follows. In the initial
sampling stage, starting from the initial position Markov chain converges towards to the typical
set. In the next stage Markov chain quickly traverse the typical set and improves the estimate by
removing the bias. In the last stage Markov chain refine the exploration of typical the typical set
provide improved estimates. The number of samples taken during the last stage is referred as effective
sample size.
A.1 Proof of Theorem 1
Theorem 1. Let T be an optimality operator under HMC given as (TQ)(s0, a0) = r(s0, a0) +
而 ∑s∈h max。Q(s, a), ∀(s0, a0) ∈ S × A, where H is a subset ofnext states sampled using HMC
from the target distribution given in (6). Then, under update rule (4) and for any given ≥ 0, there
exists n∏, t0 > 0 such that ∣∣Qt 一 Q*k∞ ≤ E ∀t ≥ t0.
Proof of Theorem 1. Let Qt(s, a) = n^ max。Qt(s, a), ∀(s, a) ∈ S ×A. Here We consider n∏ to
be the effective number of samples. Let EPQt , VarP Qt be the expectation and covariance of Qt
With respect to the target distribution. From Central Limit Theorem for HMC We have
Qt 〜N (EPQt, ʌ/VarPQt
nH
Since Q function does not decay fast We provide a proof for the case Where Qt is C-Lipschitz. From
Theorem 6.5 in (Holmes et al., 2014) We have that, there exists a c0 > 0 such that
||Qt - EPQt|| ≤ co.	(12)
12
Under review as a conference paper at ICLR 2021
Recall that Bellman optimality operator T is a contraction mapping. Thus from triangle inequality
we have
I I TQ1 - TQ2 I I	≤ max I I r(s0, a0) + —j— X max Qι(s,α)
Il	ll∞ s0,a0 I I	|H1| z—a a
S ∈S
O	..	J
-r(S , α ) - IZlJ I m. max Q2 (S, α)
H21 s⅛ a
≤ maχl I ∏γτ y^maxQι(s,α) - -ɪ VmaxQ2(s,a)
s''a, 1 * * * S l∣Hι∣ WSa	∣H21 s⅛ α
Let ∣H11 = ∣H21 = nχ. Then using triangle inequality we have
I Itqi - TQ2 I I oo ≤ max J [ I I Qi - EP Qi I I + I I Ql - EP q2 ∖ ∖ ] + max J I I EP Qi- EP Ql ∖ ∖
Since Q function almost surely converge under exhaustive sampling we have
max J I I Ep Qi - EP Q2 I I ≤ J I I Qi - Q2 I L	(⑶
From equation 12 and equation 13 we have after t time steps
I IτQi- TQ21 L ≤ 2co + J I I Qi - Q2 I L
Let Rmax and Rmin be the maximum and minimum reward values. Then we have that
I I Qi - Q2 I I ≤ 1	RmaX - Rmin .
Thus for any E ≥ by choosing a J such there exists a t0 such that ∀t ≥ t0
IlQt- Q*∣∣∞ ≤ C
This concludes the proof of Theorem 1. □
A.2 PROOF OF THEOREM 2
Theorem 2. Let Qtε+1(st, at) = r(st,at) + J PSeS P(SISt ,at )max° QtE (s,α), ∀(st,at) ∈S×A
be the update rule under exhaustive sampling, and Qt be the Q function updated according to
Hamiltonian Q-Learning, i.e. by (9)-(10). Then, for any given E ≥ 0, there exists nγ,t' > 0, such
that ∣∣Qt — QE∣∣∞ ≤ E∀t ≥ t0.
Proof of Theorem 2. Note that at each time step we attempt to recover the matrix QE, i.e., Q
function time time t under exhaustive sampling though a matrix completion method starting from Qt,
which is the Q updated function at time t using Hamiltonian Q-Learning. From Theorem 4 in (Chen
& Chi, 2018) we have that ∀t ≥ t0 there exists some constant δ > 0 such that when the updated Q
function a Qt satisfy
i i Qt-QE L ≤C
where C is some positive constant then reconstructed (completed) matrix Qt satiesfies
I I Qt - Qe I L ≤ δ I Qt - QE I L	(14)
for some δ > 0. This implies that when the initial matrix used for matrix completion is sufficiently
close to the matrix we are trying to recover matrix completion iterations converge to a global optimum.
From the result of Theorem 1 we have for any given E ≥ 0, there exists n^ ,t0 > 0 such that ∀t ≥ t0
I I Qt - q* I I ≤ E	(15)
Recall that under the update equation Q∣+i(st, at) = r(st, at)+J Ps∈s max。QE (s, a), ∀(st,at) ∈
S ×A we have that QE almost surely converge to the optimal Q*. Thus there exists a t* such that
∀t ≥ t*
I I QE - q* I I ≤ E
Let t^ = max{t*, t0}. Then from triangle inequality we have that
I I Qt - QE I I ≤ I I Qt - q* I I + I I qe - q* I I ≤ 2e.
13
Under review as a conference paper at ICLR 2021
Thus from equation 14 we have that
Qt - QtE∞ ≤2δ
This concludes the proof of Theorem 2. □
B S ampling Complexity
In this section we provide theoretical results on sampling complexity of Hamiltonian Q-Learning. For
brevity of notation we define MQ(s) = maxa Q(s, a). Note that we have the following regularity
conditions on the MDP studied in this paper.
Regularity Conditions
1.	Spaces S and A (state space and action space) are compact subsets of RDs and RDa
respectively.
2.	All the rewards are bounded such that r(s, a) ∈ [Rmin, Rmax], for all (s, a) ∈ S × A.
3.	The optimal Q* is C-LiPschitz such that
∣Q*(s,a) — Q*(s0, a0)∣ ≤ C(l∣s — s0∣∣F + ||a - a0∣∣F)
Now we prove some useful lemmas for proving sampling complexity of Hamiltonian Q-Learning
Lemma 1. For some constant c1, if
max |S|2, |A|2 |S||A|DsDa
iωJ ≥ cL
log (Ds + Da)
with ∣∣∣∣Qbt(s, a) — Q*(s, a)∣∣∣∣ ≤ then there exists a constant c2 such that
∣∣∣∣∣∣Qt(s, a) — Q*(s, a)∣∣∣∣∣∣	≤ c2
Proof of Lemma 1. Recall that in order to complete a low rank matrix using matrix estimation
methods, the matrix can not be sparse. This condition can be formulated using the notion of
incoherence. Let Q be a matrix of rank rQ with the singular value decomposition Q = UΣV T . Let
TQ be the orthogonal projection of Q ∈ RlSl×lAl to its column space. Then incoherence parameter
of φ(Q) can be give as
φ(Q) = max n|S| max ||Tue∕∣F, 1A1 max ||Tue∕∣FO
I IrQ ι≤i≤∣s∣	IrQ ι≤i≤∣A∣	J
where ei are the standard basis vectors. Recall that Qt is the matrix generated in matrix completion
phase from Q. From Theorem 4 in Chen & Chi (2018) we have that for some constant C1 if a fraction
of p elements are observed from the matrix such that
C C φ2rQDsDa
P ≥	1 lθg(Ds + Da)
where φt is the coherence parameter of Qt then with probability at least 1 — C2 (Ds + Da)-1 for
some constant C2 with ∣∣∣∣Qbt(s, a) — Q*(s, a)∣∣∣∣	≤ there exists a constant c2 such that
Note that p ≈
∣Ωt∣
TS∏AT.
∣∣∣∣Qt(s, a) — Q*(s, a)∣∣∣∣	≤ c2
Further we have for some constant c3
φ2rQ DsDa
log (Ds + Da)二
Thus it follows that for some constant c1 if
max |S|2, |A|2 DsDa
c3 -lθg(Ds + Da)~
max |S|2, |A|2 |S||A|DsDa
|"| = c1	log(Ds + Da)
14
Under review as a conference paper at ICLR 2021
with ∣ ∣ Qt(S,α) - Q* (S,a) ∣ L ≤ e then there exists a constant c2 such that
1 I Qt(S,a) - Q*(S,a) 1 I	≤ c2e
This concludes the proof of Lemma 1. □
Lemma 2. Let 1 一 ξ be the spectral gap of Markov chain under Hamiltonian sampling where
ξ ∈ [0,1]. Let ∆R = Rmax — Rmin be the maximum reward gap. Then ∀(s0, a0) ∈ S × A we have
that
Ib 0 0、C*( 0 八 I Y2	1+ ξ 2 ARmaxV1	⑶
IQ(S,a)-Q (S,a) i ≤ 不δr +M口同(1-7) log(δ)
with at least probability 1 一 δ.
Proof of Lemma 2. Let Q(s0, a0) = γ(s0, a0) + 力 Ps∈H maxa, Q(s, a). Recall that MQ(s)
max。Q(s, a). Then we have that Q(s0, a0) = γ(s0, a0) + 讶 Ps∈H MQ(s). Then it follows that
IQ(S0,a') - Q*(s',a')∣ =卜3,a') + |H| X MQ(S)- r(s0,a') - 7EPMQ*(S) I
jɪ XMQ(Si)- YEPMQ*(s)∣
IHI	IHI
1 |H| XMQ(Si)-∣H XMQ*(Si)I
IHI
+ ∣ ɪ XMQ*(Si) - YEPMQ*(s) I
(16)
Recall that all the rewards are bounded such that r(s, a) ∈ [Rmin, Rmax], for all (s, a) ∈ S ×A.
Thus for all s, a we have that MQ(S) ≤ I-YRmax. Let ∆R = RmaX 一 Rmin. Then we have that
, IHI	IHI∣	2
i |H| X MQ(Si)-而 X MQ*(Si)1 ≤ 1-γ δr.
(17)
Let ξ ∈ [0,1] be a constant such that 1 - ξ is the spectral gap of the Markov chain under Hamiltonian
sampling. Then from Fan et al. (2018) we have that
ɪ XMQ*(Si) - EPMQ*(s) ≥ 秒)≤ exp (-
Let δ = exp 一
1-ξ |HM2
ι+ξ wax
.Then we have that
1 - ξ ∖H∖02 ”
1+ ξ 2Rmaχ
“=∕H⅛!iFl.
Thus we see that
,1 IHI
I 同 NMQ*(Si)- EPMQ*(s)
1+ ξ 2 ARmaxV1 (2
口W∖{—) logC
(18)
with at least probability 1 - δ. Thus it follows from equations equation 16, equation 17 and equation 18
that
IQ(S0,a0) - Q*(S0,a0) I ≤ ɪ∆R + /占高(YRmX^(；
I 1-7	1 -ξ |H| ∖ 1 -7 √ V
with at least probability 1 - δ. This concludes the proof of Lemma 2. □
Lemma 3. For all (s, a) ∈ S × A we have that
|Qt(S,a) - Q*(s,a)〔 ≤ 2c17⅛x
1-
with probability at least 1 - δ
15
Under review as a conference paper at ICLR 2021
Proof of Lemma 3. From Lemma 2 and Shah et al. (2020) We have that for all (s, a) ∈ Ωt
IQt(s,a) - Q*(s,a)∣ ≤ J∆R + ʌ/ɪ+1⅛ (cRmxy
1 - γ	1 - ξ IHt I	1 - γ
with probability at least 1 - T. Thus we have that
IQt(S, a) - Q*(s，a)1 ≤ ci ⅛δr + ci S 1+ξ ∣⅛ (T⅛
with probability at least 1 - T. Fro all 1 ≤ t ≤ T letting
1+ ξ 2	∕2∣Ωt∣T∖
HtI二口 N log(—)
2 log (芈
(19)
2 log (叩
We obtain
CY2 R ≥, /1+ ξ 2
1 - γ max ≥ V 1 - ξ |Ht|
2
log
2”
δ
Thus We have,
∣Qt(s,a) — Q*(s,a)∣ ≤ 2cι⅞Rmax
1-γ
with probability at least 1 - δ. Recall that ∀(s, a) ∈ S ×A we have MQ(s, a) ≤ γ--R. Thus this
also proves that
IQt(S, a) - Q* (S, a) I ≤ 2ciY|QtI(S, a) - Q* (s, a) I
This concludes the proof of Lemma 3. 口
Now we proceed to prove the main theorem for sampling complexity as follows.
Theorem 3. Let Ds, Da be the dimension of state space and action space respectively. Consider the
Hamiltonian Q-Learning algorithm presented in Algorithm 1. Under a suitable matrix completion
method sampling complexity of the algorithm, Q function converge to the family of -optimal Q
functions with O -(Ds+Da+2) number of samples.
Proof of Theorem 3. Note that sample complexity of Hamiltonian Q-Learning can be given as
T
X ∣ΩtIIHtI ≤ Te∣Ωτe IIHTe I
t=i
log ( YRmax )
Let βt be the discretization parameter at time t and Te = -1,1-Y)V. Then from Lemmas 1, 2 and 3
log(2YCT)
it follows that
X IQIIHtI = O( ^Ds+1Da+2)
This concludes the proof of Theorem 3. 口
C Additional Experimental Details and Results for Cart-pole
Let θ, θ be the angle and angular velocity of the pole, respectively. Let x, X be the position and linear
velocity of the cart, respectively. Let a be the control force applied to the cart. Then, by defining m,
M , l and g as the mass of the pole, mass of the cart, length of the pole and gravitational acceleration,
respectively, the dynamics of cart-pole system can be expressed as
∙∙
θ
g sin θ - a+m+Min θ cos θ
4 _ m cos2 θ
l 3	m+M
a + ml
X =-------
(θ2 Sin θ — θ cos θ
(20)
m+M
16
Under review as a conference paper at ICLR 2021
State space of cart-pole system is 4-dimensional (Ds = 4) and any state s ∈ S is given by s =
(θ, θ, x, x). We define the range of state space as θ ∈ [-pi/2, π∕2], θ ∈ [-3.0, 3.0], x ∈ [-2.4, 2.4]
and X ∈ [一3.5, 3.5]. We consider action space to be a 1-dimensional (Da = 1)space such that
a ∈ [-10, 10]. We discretize each dimension in state space into 5 values and action space into 10
values. This forms a Q matrix of dimensions 625 × 10.
Although the differential equations (20) governing the dynamics of the pendulum on a cart system
are deterministic, uncertainty of the parameters and external disturbances to the system causes the
cart pole to deviate from the defined dynamics leading to a stochastic state transition. Following the
conventional approach we model these parameter uncertainties and external disturbances using a
multivariate Gaussian perturbation (Maithripala et al., 2016; Madhushani et al., 2017; McAllister
& Rasmussen, 2017). Here we consider the co-variance of the Gaussian perturbation to be Σ =
diag[0.143, 0.990, 0.635, 1.346].
Let st = (θt ,θt, xt ,xt) and at be the state and the action at time t. Then the state transition
probability kernel and corresponding target distribution can be given using (7) and (8), respectively,
with mean μ(st, at) = (θt + θtτ,θt + θtτ, xt + Xtτ, xt + xtT), where θt, xt can be obtained from
(20) by substituting θt , θt , at , and co-variance Σ(st, at) = Σ.
Our simulation results use the following value for the system parameters - m = 0.1kg, M = 1kg,
l = 0.5m and g = 9.8ms-2. We take 100 HMC samples during the update phase. We use trajectory
length L = 100 and step size δl = 0.02. We randomly initialize the Q matrix using values between 0
and 1. We provide additional comparison heat maps for first two dimensions θ, θ With fixed x, x.
(a) Exhaustive Sampling
-1.57 -0.78^^æθ0^78^^1.57
θ
(b) HMC Sampling	(c) IID Sampling
10.0
7.5
5.0
2.5
0.0
-2.5
-5.0
-7.5
-10.0
Figure 6:	Figure 6(a), 6(b) and 6(c) show policy heat maps for Q-Learning with exhaustive sampling, Hamilto-
nian Q-Learning and Q-Learning with IID sampling, respectively.
Further, We provide additional comparison heat maps for last two dimensions x, X with fixed θ,θ.
-2.4
-1.2
0.0
×
1.2	2.4
(a) Exhaustive Sampling
(b) HMC Sampling
-2.4	-1.2	0.0	1.2	2.4
×
(c) IID Sampling
10.0
7.5
5.0
2.5
0.0
-2.5
-5.0
-7.5
-10.0
Figure 7:	Figure 7(a), 7(b) and 7(c) show policy heat maps for Q-Learning with exhaustive sampling, Hamilto-
nian Q-Learning and Q-Learning with IID sampling, respectively.
17
Under review as a conference paper at ICLR 2021
-2.4
-1.2
1.2	2.4
(a) Exhaustive Sampling
(b) HMC Sampling
(c) IID Sampling
10.0
7.5
5.0
2.5
0.0
-2.5
-5.0
-7.5
-10.0
Figure 8: Figure 8(a), 8(b) and 8(c) show policy heat maps for Q-Learning with exhaustive sampling, Hamilto-
nian Q-Learning and Q-Learning with IID sampling, respectively.
D	Additional Details for Ocean Sampling Application
Glider dynamics By assuming that the glider’s motion is restricted to an horizontal plane (Refael
& Degani, 2019), we let x, y and θ denote its center of mass position and heading angle, respectively.
Then We can define the 6-dimensional state vector for this system as S = (x, y, x ,y, θ, θ) and the
action a as a scalar control input to the glider. Also, to accommodate dynamic perturbations due
to the ocean current, other external disturbances and parameter uncertainties, We assume that the
probabilistic state transition is governed by a multivariate Gaussian. We consider that the motion of
the glider is restricted to a horizontal plane. Let x, y and θ be the coordinates of the center of mass of
glider and heading angle respectively. By introducing q = [x y θ]T, the dynamics of the glider
can be expressed as
Mq = RFf + Fb + T
Where
	m		CoS θ	- Sin θ	0
M = diag	m	; R=	Sin θ	CoS θ	0
	Iin + Iout		0	0	1
	αf θ2sgn(θ) sin(β + ψ)		「X]		0
Ff =	αf θ2 cos(β + ψ)	;Fb = -αb√X2 + y2 y ;	τ=	0
	0	0		-μf sgn(θ)θ2 -Zina_
αb = 2 Cb db πr
+ rL CoS ψ) ; μf = αf( + + r CoS ψ);
αf=1 PCfdf L /2+(2)
αb = 0.005;	αf = 0.062;	μf = 0.0074; σ = 2.5.
Our simulation results use system parameter values from Table 1. We define the range of state and
action space as x,y ∈ [-10,10], X,y ∈ [-25, 25], θ ∈ [-∏,∏], θ ∈ [-3, 3], and a ∈ [—1,1],
respectively and then discretizing each state dimension into 5 distinct values and the action
space into 5 distinct values, We have a Q matrix of size 15625 × 5. Also, We assume that
the state transition kernel is given by a multivariate Gaussian With zero mean and covariance
Σ = diag[11.111, 69.444, 11.111, 69.444, 0.143, 0.990]. After initializing the Q matrix using ran-
domly chosen values from [0, 1], We sample state-action pairs independently With probability p = 0.5
at each iteration. Also, We assume σ = 2.5, λ = 0.1, C = diag[1, 0]. We take 100 HMC samples
during the update phase. We use trajectory length L = 100 and step size δl = 0.02.
Additional experimental results : We provide additional comparison heat maps for first tWo
ι ∙	∙	∙. 1 r∙ ι ∙ ∙ n A
dimensions x, y with fixed X ,y ,θ,θ.
18
Under review as a conference paper at ICLR 2021
Table 1: Notations
Notations	Description	Value
m	total mass	1.03 kg
Iin	inner body moment of inertia	0.5 kg m2
Iout	outer body moment of inertia	0.174 kg m2
r	radius	0.08 m
L	length of the flap	0.09 m
df	submersible depth of the flap	0.044 m
db	submersible depth of the body	0.02 m
β	angular location of the flap	30o
ψ	maximum open angle of the flap	20o
Cf	drag coefficient of the flap	2
Cb	drag coefficient of the body	2
ρ	water density	1027 kg m3
rl.00
11.00
0.75
0.50
0.25
-(0.00
(a) Exhaustive Sampling
-0.25
-0.50
-0.75
-1.00
-10.0 -5.0	0.0	5.0	10.0
×
0.75	10∙0
0.50
5.0
0.25
0∙00 X -0.0
(b) HMC Sampling
-10.0 -5.0	0.0	5.0	10.0
×
1.00
0.75
0.50
0.25
0.00
-0.25
-0.50
-0.75
-1.00
(c) IID Sampling
Figure 9: Figure 9(a), 9(b) and 9(c) show policy heat maps for Q-Learning with exhaustive sampling, Hamilto-
nian Q-Learning and Q-Learning with IID sampling, respectively.
(b) HMC Sampling
(a) Exhaustive Sampling
.1.00
Hθ,75
Hθ,5θ
Hθ,25
・ 0.00
-0.25
-0.50
-0.75
-1.00
-10.0 -5.0
0.0
×
(c) IID Sampling
1.00
0.75
0.50
0.25
0.00
-0.25
-0.50
-0.75
-1.00
Figure 10:	Figure 10(a), 10(b) and 10(c) show policy heat maps for Q-Learning with exhaustive sampling,
Hamiltonian Q-Learning and Q-Learning with IID sampling, respectively.
(a) Exhaustive Sampling
(b) HMC Sampling
.1.00
Hθ,75
Hθ,5θ
Hθ,25
・ 0.00
-0.25
-0.50
-0.75
-1.00
-10.0 -5.0	0.0	5.0	10.0
(c) IID Sampling
1.00
0.75
0.50
0.25
0.00
-0.25
-0.50
-0.75
-1.00
Figure 11:	Figure 11(a), 11(b) and 11(c) show policy heat maps for Q-Learning with exhaustive sampling,
Hamiltonian Q-Learning and Q-Learning with IID sampling, respectively.
E	APPLICATION OF HMC SAMPLING FOR Q-LEARNING: FURTHER DETAILS
In this section we provide a detailed explanation on drawing HMC samples from a given state
transition probability distribution. Let (St,at) be the current state action pair. Let μ(st,at), Σ(st,at)
be the mean and covariance of the transition probability kernel. In order to draw HMC samples
19
Under review as a conference paper at ICLR 2021
we are required to define the corresponding potential energy and kinetic energy of the system. Let
P(s|st, at) be the smooth target state transition distribution.
Potential energy, kinetic energy and mass: In this work we consider P(s|st, at) to be a truncated
multivariate Gaussian as given in equation 8. Thus potential energy can be explicitly given as,
U(S) = TOg(P(S)) = I(S- μ)τς-1 (S- μ) - 2 log ((2n)Ds detN”
Ds
-X log 1 + exp(-κ(di+ - Si)) +log 1 + exp(-κ(Si - di-))
i=1
where, μ and Σ correspond to the mean and variance of the transition probability kernel. In the
context of HMC S is referred to as the position variable. Then we chose kinetic energy can be given
as
K(V) = — log(P (v∣s)) = VTM M-1v = IvT Σv.
where v is the momentum variable andM	= Σ-1 corresponds to the mass/inertia matrix associated
with the Hamiltonian.
Hamiltonian Dynamics: As the Hamiltonian is the sum of the kinetic and the potential energy, i.e.
H(S, v) = U(S) + K(v), the Hamiltonian dynamics can be expressed as
S
∂K
∂v
Σv
and
∂U
=— = —Σ 1(s — μ) + K [S(-κ(d+ — S)) — S(—κ(s — d ))],
V =一
where S(ξι,…，ξ0s) = [S(ξι), ∙ ∙ ∙ ,S(ξns)] denotes element wise sigmoid function of the
vector ξ. We initialize HMC sampling by drawing a random sample S from the transition probability
distribution and a new momentum variable v from the multivariate Gaussian N (0, Σ-1). We integrate
the Hamiltonian dynamics for L steps with step size ∆l to generate the trajectory from (S, v) to
(S0 , v0 ). To ensure that the Hamiltonian is conserved along the trajectory, we use a volume preserving
symplectic integrator, in particular a leapfrog integrator which uses the following update rule to go
from step l to l + ∆l:
vι+∆i =	vι	— 0.5∆l	—()I ,	si+δi =	Sl + ∆lΣvι+ δi,	vι+∆i	= vι —	0.5∆l	—J)I
2	∂S l	2	∂S l+∆l
Acceptance of the new proposal: Then, following the Metropolis-Hastings acceptance/rejection
rule, this new proposal is accepted with probability
exp(H (s,v)) 1
mini1, exp(H (s0, —v0))》
Updating Q function using HMC samples: Let Ht be the set of next states obtained via HMC
sampling, i.e., position variables from the accepted set of proposals. Then we update Q(St, at) using
equation 9.
20