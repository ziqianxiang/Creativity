Under review as a conference paper at ICLR 2021
Deep Ensembles with Hierarchical Diversity
Pruning
Anonymous authors
Paper under double-blind review
Ab stract
Diverse deep ensembles hold the potential for improving accuracy and robustness
of deep learning models. Both pairwise and non-pairwise ensemble diversity met-
rics have been proposed over the past two decades. However, it is also challenging
to find the right metrics that can effectively prune those deep ensembles with insuf-
ficient ensemble diversity, thus failing to deliver effective ensemble accuracy. In
this paper, we first compare six popular diversity metrics in the literature, coined
as Q metrics, including both pairwise and non-pairwise representatives. We an-
alyze their inherent limitations in capturing the negative correlation of ensemble
member models, and thus inefficient in identifying and pruning low quality ensem-
bles. We next present six HQ ensemble diversity metrics by extending the existing
Q-metrics with three novel optimizations: (1) We introduce the concept of focal
model and separately measure the ensemble diversity among the deep ensembles
of the same team size with the concept of focal model, aiming to better capture
the negative correlations of member models of an ensemble. (2) We introduce
six HQ-diversity metrics to optimize the corresponding Q-metrics respectively in
terms of measuring negative correlation among member models of an ensemble
using its ensemble diversity score. (3) We introduce a two phase hierarchical
pruning method to effectively identify and prune those deep ensembles with high
HQ diversity scores, aiming to increase the lower and upper bounds on ensem-
ble accuracy for the selected ensembles. By combining these three optimizations,
deep ensembles selected based on our hierarchical diversity pruning approach sig-
nificantly outperforms those selected by the corresponding Q-metrics. Compre-
hensive experimental evaluation over several benchmark datasets shows that our
HQ-metrics can effectively select high diversity deep ensembles by pruning out
those ensembles with insufficient diversity, and successfully increase the lower
bound (worst case) accuracy of the selected deep ensembles, compared to those
selected using the state-of-the-art Q-metrics.
1	Introduction
Deep ensembles with sufficient ensemble diversity hold potential of improving both accuracy and
robustness of ensembles with their combined wisdom. The improvement can be measured by three
criteria: (i) the average ensemble accuracy of the selected ensemble teams, (ii) the percentage of
selected ensembles that exceed the highest accuracy of individual member models; (iii) the lower
bound (worst case) and the upper bound (best case) accuracy of the selected ensembles. The higher
these three measures, the higher quality of the ensemble teams. Ensemble learning can be broadly
classified into two categories: (1) learning the ensemble of diverse models via diversity optimized
joint-training, coined as the ensemble training approach, such as boosting (Schapire, 1999); and (2)
learning to compose an ensemble of base models from a pool of existing pre-trained models through
ensemble teaming based on ensemble diversity metrics (Partridge & Krzanowski, 1997; Liu et al.,
2019; McHugh, 2012; Skalak, 1996), coined as the ensemble consensus approach. This paper is
focused on improving the state-of-the-art results in the second category.
Related Work and Problem Statement. Ensemble diversity metrics are by design to capture the
degree of negative correlation among the member models of an ensemble team (Brown et al., 2005;
Liu et al., 2019; Kuncheva & Whitaker, 2003) such that the high diversity indicates high nega-
tive correlation among member models of an ensemble. Three orthogonal and yet complimentary
1
Under review as a conference paper at ICLR 2021
threads of efforts have been engaged in ensemble learning: (1) developing mechanisms to produce
diverse base neural network models, (2) developing diversity metrics to select ensembles with high
ensemble diversity from the candidate ensembles over the base model pool, and (3) developing con-
sensus voting methods. The most popular consensus voting methods include the simple averaging,
the weighted averaging, the majority voting, the plurality voting (Ju et al., 2017), and the learn to
rank (Burges et al., 2005). For the base model selection, early efforts have been devoted to train-
ing diverse weak models to form a strong ensemble on a learning task, such as bagging (Breiman,
1996), boosting (Schapire, 1999), or different ways of selecting features, e.g., random forests (Tin
Kam Ho, 1995). Several recent studies also produce diverse base models by varying the training
hyper-parameters, such as snapshot ensemble (Huang et al., 2017), which utilizes the cyclic learn-
ing rates (Smith, 2015; Wu et al., 2019) to converge the single DNN model at different epochs to
obtain the snapshots as the ensemble member models. Alternative method is to construct the pool of
base models by using pre-trained models with different neural network backbones (Wu et al., 2020;
Liu et al., 2019; Wei et al., 2020; Chow et al., 2019a). The research efforts on diversity metrics have
proposed both pairwise and non-pairwise ensemble diversity measures (Fort et al., 2019; Wu et al.,
2020; Liu et al., 2019), among which the three representative pairwise metrics are Cohen’s Kappa
(CK) (McHugh, 2012), Q Statistics (QS) (Yule, 1900), Binary Disagreement (BD) (Skalak, 1996),
and the three representative non-pairwise diversity metrics are Fleiss’ Kappa (FK) (Fleiss et al.,
2013), Kohavi-Wolpert Variance (KW) (Kohavi & Wolpert, 1996; Kuncheva & Whitaker, 2003) and
Generalized Diversity (GD) (Partridge & Krzanowski, 1997). These diversity metrics are widely
used in several recent studies (Fort et al., 2019; Liu et al., 2019; Wu et al., 2020). Some early study
has shown that these diversity metrics are correlated with respect to ensemble accuracy and diversity
in the context of traditional machine learning models (Kuncheva & Whitaker, 2003). However, few
studies to date have provided in-depth comparative critique on the effectiveness of these diversity
metrics in pruning those low quality deep ensembles from the candidate ensembles due to their high
negative correlation.
Scope and Contributions. In this paper, we focus on the problem of defining ensemble diversity
metrics that can select diverse ensemble teams with high ensemble accuracy. We first investigate the
six representative ensemble diversity metrics, coined as Q metrics. We identify and analyze their
inherent limitations in capturing the negative correlation among the member models of an ensemble,
and why pruning out those deep ensembles with low Q-diversity may not always guarantee to im-
prove the ensemble accuracy. To address the inherent problems of Q metrics, we extend the existing
six Q metrics with three optimizations: (1) We introduce the concept of the focal model and argue
that one way to better capture the negative correlations among member models of an ensemble is to
compute diversity scores for ensembles of fixed size based on the focal model. (2) We introduce the
six HQ diversity metrics to optimize the six Q-diversity metrics respectively. (3) We develop a HQ-
based hierarchical pruning method, consisting of two stage pruning: the α filter and the K-Means
filter. By combining these optimizations, the deep ensembles selected by our HQ-metrics can sig-
nificantly outperform those deep ensembles selected by the corresponding Q metrics, showing that
the HQ metrics based hierarchical pruning approach is efficient in identification and removal of low
diversity deep ensembles. Comprehensive experiments are conducted on three benchmark datasets:
CIFAR-10 (Krizhevsky & Hinton, 2009), ImageNet (Russakovsky et al., 2015), and Cora (Lu &
Getoor, 2003). The results show that our hierarchical diversity pruning approach outperforms their
corresponding Q-metrics in terms of the lower bound and the upper bound of ensemble accuracy
over the deep ensembles selected, exhibiting the effectiveness of our HQ approach in pruning low
diversity deep ensembles.
2	Hierarchical Pruning with Diversity Metrics
Existing studies on consensus based ensemble learning (Huang et al., 2017; Krizhevsky et al., 2012;
Zoph & Le, 2016) generate the base model pool through two channels: (i) deep neural network
training using different network structures or different configurations of hyperparameters (Breiman,
1996; Schapire, 1999; Zoph & Le, 2016; Hinton et al., 2015; Wu et al., 2018; 2019) and (ii) selecting
the top performing pre-trained models from open-source projects (e.g., GitHub) and public model
zoos (Jia et al., 2014; ONNX Developers, 2020; GTModelZoo Developers, 2020). Hence, an im-
portant technical challenge for deep ensemble learning is to define diversity metrics for producing
high quality ensemble teaming strategies, aiming to boost the ensemble accuracy. Given that the
number of possible ensemble teams increases exponentially with a small pool of base models, de-
2
Under review as a conference paper at ICLR 2021
veloping proper ensemble diversity metrics is critical for effective pruning of deep ensembles with
insufficient diversity. Consider a pool of M base models for a learning task on a given dataset D,
denoted by BMSet(D)= {F1, ..., FM}. Let EnsSet denote the set of all possible ensemble teams
that are composed from BM Set(D), with the ensemble team size S varying from 2 to M. We have
a total of PSM=2 (MM) ensembles, i.e., ∣EnsSet∣ = (MM) + (MM) + ... + (M) = 2M - (1 + M).
The cardinality of the set of possible ensembles EnsSet grows exponentially with M, the number
of base models. For example, M = 3, we have |EnsSet| = 4. When M becomes larger, such as
M = 5, 10, 20, |EnsSet| = 26, 1013, 1048555. Hence, as M increases, it is non-trivial to construct
a set of high-accuracy ensemble teams (GEnsSet), from the candidate set (EnsSet) of all possible
ensembles that are composed from BMSet(D).
Consider a pool of M = 10 base models for ImageNet, in which the highest performing base model
is 78.25%, the lowest performing base model is 56.63%, and the average accuracy of these 10 base
models is 71.60% (see Table 5 in Appendix Section F). For a pool of 10 base models, there will
be a total of 1013 (210 - (10 + 1)) different ensembles with team size ranging from 2 to 10. The
performance of these ensembles vary sharply, from 61.39% (lower bound) to 80.77% (upper bound).
Randomly selecting an ensemble team from these 1013 teams in EnsSet(ImageNet) may lead to
a non-trivial probability of selecting a team with the ensemble accuracy lower than the average
member model accuracy of 71.60% over the 10 base models. Clearly, an efficient ensemble diversity
metric should be able to prune out those ensemble teams with insufficient ensemble diversity and
thus low ensemble accuracy, increasing (i) the average ensemble accuracy of the selected ensemble
teams, (ii) the percentage of selected ensembles that exceed the highest accuracy of individual
member models (i.e., 78.25% for the 10 base DNN models on ImageNet); and (iii) the lower bound
(worst case) and the upper bound (best case) accuracy of the selected ensembles. A number of
ensemble diversity metrics have been proposed to address this challenging problem. In this section,
we first provide a comparative study of the six state-of-the-art Q-diversity metrics and analyze their
inherent limitations in identifying and pruning out low diversity ensembles. Then we introduce our
proposed HQ-diversity metrics and analyze the effectiveness of our HQ based hierarchical diversity
approach in pruning low quality ensembles.
2.1	Q-diversity metrics and their Limitations
We outline the key notations for the six Q-diversity metrics in Table 1: three pairwise diversity met-
rics: Cohen’s Kappa (CK) (McHugh, 2012), Q Statistics (QS) (Yule, 1900) and Binary Disagreement
(BD) (Skalak, 1996), and three non-pairwise diversity metrics: Fleiss’ Kappa (Fleiss et al., 2013)
(FK), Kohavi-Wolpert Variance (KW) (Kohavi & Wolpert, 1996; Kuncheva & Whitaker, 2003) and
Generalized Diversity (GD) (Partridge & Krzanowski, 1997). The arrow column ↑ | J specifies the
relationship between the Q-value and the ensemble diversity. The ↑ represents positive relationship
of the Q-value and the ensemble diversity, that is a high Q-value refers to high ensemble diversity.
The J indicates the negative relationship, that is the low Q-value corresponds to high ensemble di-
versity. To facilitate the comparison of the six Q-diversity metrics such that the low Q-value refers to
high ensemble diversity for all six Q-metrics, we apply (1 - Q-value) when calculating Q-diversity
score with BD, KW and GD . We refer readers to Appendix (Section C) for the formal definitions
of the six Q-diversity metrics.
Table 1: The six Q-diversity metrics
Category	Name	Notation	↑u	Mean Threshold		
				CIFAR-10	ImageNet	Cora
Pairwise	Cohen,s Kappa	-CK-		-0.562-	-0.500-	0.468
	Q Statistics	-QS-		-0.515-	-0.697-	0.373
	Binary Disagreement	-BD-	~τ~	-0.661-	-0686-	0.617
Non-pawise	FleiSs, Kappa	-FK-		-0.561-	-0.499-	0.461
	Kohavi-Wolpert variance	-KW-		-0.868-	-0.878-	0.851
	Generalized Diversity-	GD	~τ~	0.476	0.665	0.592
Given a Q-diversity metric, we calculate the diversity score for each ensemble team in the ensemble
set (EnsSet) using a set of negative samples (NegSampSet) on which one or more models in
the ensemble make prediction errors. The low Q-score indicates sufficient diversity among member
models of an ensemble. Upon the completion of Q-diversity score computation for all ensembles in
EnsSet, the diversity threshold based pruning is employed to remove those ensembles with insuf-
ficient diversity among ensemble member models. Either a pre-defined Q-diversity threshold or a
3
Under review as a conference paper at ICLR 2021
mean threshold by taking the average value of all Q-diversity scores calculated for all candidate deep
ensembles in EnsSet. The mean threshold tends to work better in general than a manually defined
threshold. Once a mean threshold is obtained, those ensembles in EnsSet with their Q diversity
scores below the threshold will be selected and placed into the diverse ensemble set GEnsSet, and
the remaining ensembles are those with their Q scores higher than the threshold and thus will be
pruned out. The pseudo code of the algorithm is included in Appendix (Algorithm 1). The last three
columns of Table 1 show the mean threshold for all six Q-diversity metrics calculated on the set of
1013 candidate deep ensembles for the three benchmark datasets used in this study. We make two
observations. First, different Q-diversity metrics capture the ensemble diversity from different per-
spectives with different diversity measurement principles, resulting in different Q-scores. Second,
each Q-metric, say CK, is used to compare ensembles based on their Q-CK scores. Hence, even
though the Q-KW metric has relatively high KW-specific Q scores for all ensemble teams, it can
select the diverse ensembles based on the mean KW-threshold, in a similar manner as any of the
other five Q metrics.
(a) All, (Pruning 427 out of 1013)	(b) All, (Pruning 517 out of 1013)	(c) S=4,(Pruning 103 out of 210)
Figure 1:	Pruning with Q diversity with mean threshold (CIFAR-10))
Limitations of Q Metrics. Figure 1a and 1b show Q-KW and Q-GD metrics and their relationship
with ensemble accuracy for all 1013 deep ensembles on CIFAR-10 respectively. Each dot represents
a deep ensemble team with team sizes color-coded by the color diagram on the right. The vertical
red dashed line represents the Q-KW and Q-GD mean thresholds of 0.868 and 0.476 respectively.
The horizontal red and black dashed lines represent the maximum single model accuracy 96.68%
and the average accuracy 94.25% of the 10 base models respectively. We use these two accuracy
bounds as important references to quantify the quality of the deep ensembles selected using a Q
metric with its mean threshold. Those deep ensembles on the left of the red vertical dash line are
selected and added into GEnsSet given that their Q-scores are below the mean threshold (e.g., Q-
KW or Q-GD). The ensembles on the right of this red vertical dash line are pruned out because their
Q diversity scores exceed the mean threshold. Compare Figure 1a and 1b, it is visually clear that
both Q metrics can select a sufficient number of good quality ensemble teams while at the same time,
both Q metrics with mean threshold pruning will miss a large portion of teams with high ensemble
accuracy, indicating the inherent limitations of both Q metrics and the mean threshold pruning with
respect to capturing the concrete ensemble diversity in terms of low negative correlation among
member models of an ensemble.
To better understand the inherent problems with the Q-diversity metrics, we performed another set
of experiments by measuring the Q-GD metric over ensemble teams of fixed size S on CIFAR-10.
Figure 1c shows a visualization of the results using the Q-GD scores computed over ensembles
of size S = 4 with mean threshold indicated by the vertical red dashed line, showing a visually
sharper trend in terms of the relationship between ensemble diversity and ensemble accuracy when
comparing the selected ensemble teams (red dots) with those ensembles (black dots) on the right
of the red vertical threshold line. However, relying on separating the diversity computation and
comparison over ensemble teams of the same size alone may not be sufficient, because Figure 1c
shows that (i) some selected ensemble teams have low accuracy, affecting all three ensemble quality
measures (recall Section 2, page 3), and (ii) a fair number of ensemble teams with high ensemble
accuracy (black dots on the top right side) are still missed. Similar observations are also found
for other five Q-diversity metrics. We conclude our analysis with three arguments: (1) The Q-
diversity metrics may not accurately capture the degree of negative correlation among the member
models of an ensemble even when its ensemble Q-diversity score is below the mean threshold. (2)
Comparing ensembles of different team size S using their Q scores may not be a fair measure of
their true ensemble diversity in terms of the degree of negative correlation among member models
4
Under review as a conference paper at ICLR 2021
of an ensemble. However, relying on ensembles of the same team size S alone is still insufficient.
(3) Mean threshold is not a good Q-diversity pruning method in terms of capturing the intrinsic
relationship between ensemble diversity and ensemble accuracy. This motivates us to propose the
HQ diversity metrics with two phase pruning using learning algorithms.
2.2 HQ-diversity Metrics and their Two-Phase Pruning
The design of the six HQ metrics is to enhance the six existing popular Q-metrics with three opti-
mizations. First, we argue that comparing ensembles of the same team size in terms of their diversity
scores can better capture the intrinsic relationship between ensemble diversity and ensemble accu-
racy. Second, to further improve the comparison of ensembles of the same size S in terms of their
ensemble diversity in the context of negative correlation, we introduce the concept of focal model
to obtain the set of negative samples for computing the diversity scores of ensembles by taking each
member model in turn as the focal model. This is motivated by adversarial robustness with ensemble
defense (Chow et al., 2019b; Wei & Liu, 2020), which composes robust ensemble teams for each
attack target model. The concept of focal model allows us to capture the ensemble diversity of a
team by utilizing the focal model and its negative samples, and then obtain a unified HQ score by
taking an average of the S focal model based diversity measurements for each ensemble team of size
S. These two optimizations enable HQ scores to more accurately capture the ensemble diversity and
its relationship with the ensemble accuracy. Finally, we employ the third optimization, which uti-
lizes the two-phase HQ score based filtering process using the α filter and then the K-means filter
to select the set of high quality ensembles.
(d) S=3, (Pruning 39 out of 120)	(e) S=4, (Pruning 137 out of 208)	(f) S=5, (Pruning 232 out of 252)
Figure 2:	Ensemble teams of size S = 3, 4, 5 on CIFAR-10: top three figures for HQ-KW (α) and
bottom three figures for HQ-GD (α)
HQ (α): HQ metrics with α filter. We observe that if an ensemble team of size S has large HQ
score (say [F5, F6]), indicating insufficient ensemble diversity, then all the ensemble teams that have
larger size than S and contain all the member models of this ensemble team (e.g., [F5 , F6 , F7],
[F0, F5 , F6], [F0, F5 , F6 , F7], [F5 , F6 , F7, F8]) tend to have insufficient ensemble diversity (i.e.,
larger HQ score) as well. This motivates us to design a hierarchical pruning algorithm, coined
as α filter. Concretely, we start with the set of ensembles of smallest team size, say S = 2,
| EnsSet| = (MM) = M(M - 1) candidate ensembles. For M = 10 We will have 90 teams of
size 2. Given a HQ metric, we first sort the ensembles of small size S, say S = 2, by their HQ
scores in decreasing order, and then choose the top β (percentage) of ensembles of size S with large
HQ value as our pruning targets at team size S . We recommend a conservative approach by using
a small β (e.g., β = 5%, 10%). We first preemptively prune out the β(%) of the ensembles with
large HQ scores and then prune all those ensembles that are super-sets of these β(%) of ensembles.
Imagining a hierarchical structure with all teams of size 2 on the top, and each layer we add one
5
Under review as a conference paper at ICLR 2021
additional model to the teams such that all teams of size S + 1 are placed in the next tier. The bot-
tom tier will be one ensemble team of size M. For each of the β(%) of ensembles of size 2 that are
pruned out, this α filter algorithm will cut off the whole branch of ensemble teams that are supersets
of this removed ensemble team. Due to space constraint, we include the Algorithm 2 to compute
HQ metrics and the α filter algorithm in Appendix: section D and section E respectively.
Figure 2 shows the visualization of applying α filter on two HQ metrics: HQ-KW and HQ-GD. The
black dots denote the ensemble teams pruned out by using the α filter and the red dots are the en-
sembles selected after HQ metric with α pruning. We highlight two interesting observations. First,
the α filter can effectively prune those ensembles with large HQ values (representing insufficient en-
semble diversity). Compared Q-GD in Figure 1c with HQ-GD (α) in Figure 2e (both with S = 4),
HQ-GD (α) can significantly improve the quality of selected ensemble teams while effectively prun-
ing out most of the low accuracy ensembles. Second, both HQ-GD (α) and HQ-KW (α) diversity
metrics display high correlation of measured ensemble diversity with the ensemble accuracy: low
HQ scores correspond to high ensemble accuracy. Similar observations are found consistently for
all HQ diversity metrics.
HQ (α + K) metrics: HQ metrics with α filter followed by K-means filter. In our two-phase
HQ diversity pruning approach, we introduce K-means filter to correct as much as possible the
remaining errors in high quality ensemble team selection. Recall Figure 2a and 2d for ensemble
teams of size S = 3, it is visually clear that the α filter is less effective in pruning out some ensemble
teams of low accuracy, compared to teams of larger sizes, S = 4, 5 in Figure 2(b)(c)(e)(f). We
introduce the second phase filtering by using a customized K-means clustering algorithm with K =
2 and two strategically chosen initial centroids: top left and bottom right (marked in the red and black
unfilled circles respectively), aiming to learn two clusters of ensembles: (1) the cluster of ensembles
with low HQ score and high ensemble accuracy, and (2) the cluster of ensembles with low accuracy
and relatively larger HQ score. The clustering results are indicated by the two solid circles: the
pink one for cluster (1) and the light grey one for cluster (2). The two phase filtering powered HQ
(α+K) metrics can effectively remove those ensembles with low accuracy and insufficient diversity
(i.e., higher HQ values), further improving the three ensemble accuracy measures (recall Section 2,
page 3) compared to the HQ (α) metrics, increasing the lower bound accuracy and improving the
worst-case ensemble selection quality. Figure 3 provides a visualization for ensemble teams of size
S = 3 using three HQ (α + K) metrics: HQ-CK, HQ-KW and HQ-GD. The red dots and black dots
show the two clusters produced by K-means, and the red vertical dashed line indicates the filtering
threshold produced by the K-Means filter, which chooses the smallest HQ value from the cluster of
low accuracy ensembles as the HQ-specific pruning threshold. By using HQ with two phase α + K
filters, we can further fine tune the quality of ensemble selection by removing those ensembles with
relatively low ensemble accuracy, effectively boosting the lower bound of ensemble accuracy for all
the ensemble teams selected by HQ (α + K) metrics, compared to either HQ (α) or Q metrics.
(a) HQ-CK (α + K),θ=0.139
(b) HQ-KW (α + K),θ=0.261
(c) HQ-GD (α + K),θ=0.375
Figure 3: Three HQ metrics with two phase (α + K) filters for the team size S = 3 (CIFAR-10)
3 Experimental Evaluation
Extensive experiments on three benchmark datasets (CIFAR-10, ImageNet, and Cora), with a total
of 10 base models for each dataset, are conducted to evaluate our hierarchical diversity pruning
methods. All the experiments were conducted on an Intel Xeon E5-1620 server with Nvidia GeForce
GTX 1080Ti GPU on Ubuntu 16.04. Readers may refer to Appendix (section F) for further details
on the base models used in this study and their accuracy results.
CIFAR-10 Table 2 shows the experimental comparison of the ensemble teams selected by Q met-
rics with mean threshold, HQ (α) metrics and HQ (α + K) metrics for CIFAR-10. For the se-
6
Under review as a conference paper at ICLR 2021
lected ensembles, we show their ensemble accuracy range (%) in the 4th column. The 5th column
#(%)(Acc>96.68% (max)) shows the number and percentage of the ensembles selected, which have
ensemble accuracy higher than the highest (max) single model accuracy of 96.68% over the M = 10
CIFAR-10 base models. The last column shows the number of selected ensembles with ensemble
accuracy over 96.70%, exceeding the best 96.68% single base model accuracy. We highlight three
interesting observations. First, compare to Q metrics, our HQ (α) metrics significantly reduce the
number of candidate ensembles in #EnsSet (from 1013 to 230~281) and improve the quality of
selected ensembles. For example, with the α filter, HQ-BD, HQ-KW and HQ-GD can improve the
ensemble accuracy lower bound from 93.56% to 93.88%, while HQ-CK, HQ-QS, HQ-BD, HQ-FK
and HQ-KW all improve the accuracy upper bound from 96.72% or 96.74% to 97.01% or 97.15%.
Second, the two phase filtering HQ (α + K) metrics further improved the quality of selected ensem-
bles compared to both Q-metrics and HQ (α) metrics, e.g., increasing the lower bound of ensemble
accuracy from 93.56%~94.27% to 94.46%~95.45%. Furthermore, 42.22% (38 out of 90) of the
ensembles selected by HQ-GD (α + K) have the ensemble accuracy above 96.70%, showing that
with random picking of an ensemble from the selected set (GEnsSet), HQ-GD has higher than
42% probability to choose an ensemble team with accuracy better than the max accuracy of the 10
single base models for CIFAR-10, compared to 17.93% by HQ-GD (α) and 7.26% by Q-GD. This
further demonstrates the effectiveness of our HQ (α + K) metrics.
Table 2: Comparing Q, HQ (α), HQ (α + K) metrics on CIFAR-10
Methods	#EnSSet	#GEnSSet	Ensemble Acc Range (%)	#(%)(ACC > 96.68% (max))	#(Acc >= 96.70%)
Baseline	-1013-	-1013-	93.56-97.15	-66 (6.52%)-	56
Q-CK	-1013-	544	93.56-96.72	1(0.18%)-	1
Q-QS 一	-1013-	516	93.56-96.74	-4 (0.78%)-	3
Q-BD	-1013-	550	93.56-96.72	-2 (0.36%)-	1
Q-FK	-1013-	541	93.56-96.72	1(0.18%)-	1
Q-KW	-1013-	586	94.27-96.74	-5 (0.85%)-	1
Q-GD	-1013-	496	93.56-97.15	-36 (7.26%)-	32
HQ-CK (α)	-230-	209	93.56-97.01	-18(8.61%)-	18
HQ-QS (α)	-235-	212	94.01-97.01	-9 (4.25%)-	9
HQ-BD (α)	-279-	249	93.88-97.15	43 (17.27%)	43
HQ-FK (α)	-261-	235	93.56-97.01	25 (10.64%)	25
HQ-KW (α)	-279-	249	93.88-97.15	43 (17.27%)	43
HQ-GD (α)	-281-	251	93.88-97.15	45 (17.93%)	44
HQ-CK (α + K)	-209-	53	95.04-97.01	-6(11.32%)-	6
HQ-QS(α + K)	-212-	31	95.45-96.73	-2 (6.45%)-	2
HQ-BD (α + K)	-249-	76	95.23-97.15	26 (34.21%)	26
HQ-FK (α + K)	-235-	50	94.46-97.01	-5 (10.00%)-	5
HQ-KW (α + K)	-249-	74	95.23-97.15	27 (36.49%)	27
HQ-GD (α + K)	251	90	94.72-97.15~~	38 (42∙22%T-	38
ImageNet Table 3 shows the same set of experiments on ImageNet. We make three observations.
(1) For ImageNet, many ensembles generated by HQ metrics can achieve higher ensemble accuracy,
better than the max single base model accuracy of 78.25% by the member model F5 (Table 5 in
Appendix Section F), even without having F5 as a member model of the ensemble teams. For ex-
ample, with α +K, HQ-BD and HQ-GD both have 19 ensemble teams that offer ensemble accuracy
higher than the max single model accuracy of 78.25% by the member model F5, and yet do not
have F5 as the member model of their ensemble teams. (2) Similar to CIFAR-10, many ensembles
with low accuracy and insufficient HQ diversity are effectively pruned out by using our HQ (α)
metrics. Compared to Q-metrics, our HQ (α) metrics effectively increase the accuracy lower bound
of all selected ensembles from 61.39% to 68.99%, significant improvement over Q metrics. (3) The
HQ (α + K) metrics further boost the lower bound ensemble accuracy over the corresponding HQ
(α) metrics, with the lower bound (worst case) accuracy of 76.16%~78.35%, significantly higher
than Q metrics (61.39%~70.79%). Three HQ (α + K) metrics (HQ-CK, HQ-QS, HQ-FK) achieve
100% of the selected ensembles with over 78.25% accuracy (the max single base model accuracy
on ImageNet), while HQ-BD has over 90.91%, HQ-KW and HQ-GD have over 87.10% of the se-
lected ensembles with their ensemble accuracy over the best single base model accuracy (78.25%).
Clearly, the average accuracy of the selected ensembles by HQ (α + K) metrics is much higher than
that by using Q-diversity metrics.
7
Under review as a conference paper at ICLR 2021
Table 3: Comparing Q, HQ (α), and HQ (α + K) metrics on ImageNet
Methods	#EnSSet	#GEnSSet	Ensemble ACC Range (%)	#(%)(ACC > 78.25% (max))	#(ACC >= 79.50%)
Baseline	-10T3-	-10T3-	61.39 〜80.77	753 (74.33%)	343
Q-CK	-10T3-	555	61.39 〜80.50	338 (60.90%)	92
QQS	-10T3-	483	61.39 〜80.54	296 (61.28%)	96
Q-BD	-10T3-	554	61.39 〜80.54	349 (63.00%)	107
Q-FK	-10T3-	553	61.39 〜80.50	336 (60.76%)	91
Q-KW	-10T3-	647	68.72〜80.56	473 (73.11%)	188
Q-GD	1013	530	70.79〜80.60~~	394 (74.34%)~	170
HQ-CK (α)	-221-	201	68.99〜80.42	94 (46.77%)	16
HQ-QS (α)	-245-	221	70.80〜80.70	157 (71.04%)	70
HQ-BD (α)	-283-	253	69.21 〜80.77	198 (78.26%)	110
HQ-FK (α)	-221-	201	68.99〜80.42	94 (46.77%)	16
HQ-KW (α)	-283-	253	69.21 〜80.77	198 (78.26%)	110
HQ-GD (α)	287	257	69.21 〜80.77~~	203 (78.99%)~	115
HQ-CK (α + K)	-201-	4	78.28〜79.64	4 (100.00%)	1
HQ-QS (α + K)	-221-	16	78.35〜80.54	16 (100.00%)	9
HQ-BD (α + K)	-253-	55	77.18 〜80.77	50 (90.91%)	36
HQ-FK (α + K)	-201-	4	78.28〜79.64	4 (100.00%)	1
HQ-KW (α + K)	-253-	65	76.16 〜80.77	57 (87.69%)	44
HQ-GD (α + K)	257	62	76.16 〜80.77~~	54 (87.10%~~	40
Ensemble Accuracy Distribution. We further investigate the ensemble accuracy distribution for the
ensemble teams selected by Q, HQ (α) and HQ (α + K) metrics. Figure 4 shows the visualization
of the results. For CIFAR-10, we compare the ensemble teams selected by Q-GD (yellow triangles),
HQ-GD (α) (blue dots), and HQ-GD (α + K) (red circles). It is visually clear that HQ-GD (α + K)
diversity metric can effectively prune out more low accuracy ensembles with insufficient HQ scores
compared to Q-GD and HQ-GD (α), although it still suffers from a few low accuracy ensembles,
which dragged the improvement on the ensemble accuracy lower bound of 94.72% on CIFAR-10.
For ImageNet, Figure 4b and 4c show that both HQ-BD (α + K) and HQ-GD (α + K) have the
best performance with most of the selected ensembles on the top left (red circles), indicating high
ensemble accuracy and high lower bound on the ensemble accuracy of all selected teams.
(a) CIFAR-10,GD
Figure 4: Ensemble Accuracy Distribution on CIFAR-10 and ImageNet
HQ-GD (α)
HQ-GD (a + K)
0.2	0.4
Generalized Diversity (HQ)
(c) ImageNet,GD








4 Conclusion
We have presented a two-phase hierarchical ensemble diversity pruning approach for high quality
ensemble selection. This paper makes three original contributions. First, we identify and analyze
the inherent limitations of existing six ensemble diversity metrics, coined as Q-metrics. Second, we
address the limitations of Q-metrics by introducing the six HQ diversity metrics respectively. Third,
we develop a two phase HQ-based hierarchical pruning method with α filter followed by K-means
filter. By combining these optimizations, the deep ensembles selected by our HQ (α + K) metrics
can significantly outperform the deep ensembles selected by the corresponding Q metrics, showing
that the HQ metrics based hierarchical pruning approach is efficient in identification and removal
of low quality deep ensembles. Comprehensive experiments conducted on benchmark datasets of
CIFAR-10 and ImageNet show that our hierarchical diversity pruning approach outperforms the
corresponding Q-metrics in terms of the lower bound (worst case) and the upper bound (best
case) of ensemble accuracy over the deep ensembles selected, in addition to the average ensemble
accuracy of the selected ensemble teams, and the percentage of selected ensembles that exceed
the highest accuracy of the member models in the base model pool.
8
Under review as a conference paper at ICLR 2021
References
Leo Breiman. Bagging predictors. Machine learning, 24(2):123-140, 1996.
Gavin Brown, Jeremy Wyatt, Rachel Harris, and Xin Yao. Diversity creation methods: A survey
and categorisation. Information Fusion, 6:5-20, 03 2005. doi: 10.1016/j.inffus.2004.04.004.
Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg
Hullender. Learning to rank using gradient descent. In Proceedings of the 22nd International
Conference on Machine Learning, ICML ’05, pp. 89-96, New York, NY, USA, 2005. Associ-
ation for Computing Machinery. ISBN 1595931805. doi: 10.1145/1102351.1102363. URL
https://doi.org/10.1145/1102351.1102363.
K.	Chow, W. Wei, Y. Wu, and L. Liu. Denoising and verification cross-layer ensemble against
black-box adversarial attacks. In 2019 IEEE International Conference on Big Data (Big Data),
pp. 1282-1291, 2019a.
K.	Chow, W. Wei, Y. Wu, and L. Liu. Denoising and verification cross-layer ensemble against
black-box adversarial attacks. In 2019 IEEE International Conference on Big Data (Big Data),
pp. 1282-1291, 2019b.
Joseph L Fleiss, Bruce Levin, and Myunghee Cho Paik. Statistical methods for rates and propor-
tions. john wiley & sons, 2013.
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep Ensembles: A Loss Landscape Per-
spective. arXiv e-prints, art. arXiv:1912.02757, December 2019.
GTModelZoo Developers. Gtmodelzoo. https://github.com/git-disl/GTModelZoo,
2020. [Online; accessed 01-Apr-2020].
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.
Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E. Hopcroft, and Kilian Q. Weinberger.
Snapshot ensembles: Train 1, get M for free. CoRR, abs/1704.00109, 2017. URL http://
arxiv.org/abs/1704.00109.
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Ser-
gio Guadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embed-
ding. In Proceedings of the 22Nd ACM International Conference on Multimedia, MM ’14, pp.
675-678, New York, NY, USA, 2014. ACM. ISBN 978-1-4503-3063-3. doi: 10.1145/2647868.
2654889. URL http://doi.acm.org/10.1145/2647868.2654889.
Cheng Ju, Aurelien BibaUL and Mark J. van der Laan. The relative performance of ensemble meth-
ods with deep convolutional neural networks for image classification, 2017.
Ron Kohavi and David Wolpert. Bias plus variance decomposition for zero-one loss functions. In
Proceedings of the Thirteenth International Conference on International Conference on Machine
Learning, ICML’96, pp. 275-283, San Francisco, CA, USA, 1996. Morgan Kaufmann Publishers
Inc. ISBN 1558604197.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep
convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q.
Weinberger (eds.), Advances in Neural Information Processing Systems 25, pp. 1097-
1105. Curran Associates, Inc., 2012. URL http://papers.nips.cc/paper/
4824-imagenet-classification-with-deep-convolutional-neural-networks.
pdf.
Ludmila I. Kuncheva and Christopher J. Whitaker. Measures of diversity in classifier ensembles
and their relationship with the ensemble accuracy. Mach. Learn., 51(2):181-207, May 2003.
ISSN 0885-6125. doi: 10.1023/A:1022859003006. URL https://doi.org/10.1023/A:
1022859003006.
9
Under review as a conference paper at ICLR 2021
L.	Liu, W. Wei, K. Chow, M. Loper, E. Gursoy, S. Truex, and Y. Wu. Deep neural network ensembles
against deception: Ensemble diversity, accuracy and robustness. In 2019 IEEE 16th International
Conference on Mobile Ad Hoc and Sensor Systems (MASS),pp. 274-282, 2019.
Qing Lu and Lise Getoor. Link-based classification. In Proceedings of the Twentieth International
Conference on International Conference on Machine Learning, ICML’03, pp. 496-503. AAAI
Press, 2003. ISBN 1577351894.
Mary L McHugh. Interrater reliability: the kappa statistic. Biochemia medica, 22(3):276—282,
2012. ISSN 1330-0962. URL https://europepmc.org/articles/PMC3900052.
ONNX Developers. Onnx model zoo. https://github.com/onnx/models, 2020. [Online;
accessed 01-Apr-2020].
D. Partridge and W. Krzanowski. Software diversity: practical statistics for its measurement
and exploitation. Information and Software Technology, 39(10):707 - 717, 1997. ISSN
0950-5849. doi: https://doi.org/10.1016/S0950-5849(97)00023-2. URL http://www.
sciencedirect.com/science/article/pii/S0950584997000232.
RemigljUs PaUlaviciUs and Julius Zilmskas. Analysis of different norms and corresponding Iipschitz
constants for global optimization. Ukio Technologinis ir Ekonominis Vystymas, 12(4):301-306,
2006. doi: 10.1080/13928619.2006.9637758. URL https://www.tandfonline.com/
doi/abs/10.1080/13928619.2006.9637758.
Olga RUssakovsky, Jia Deng, Hao SU, Jonathan KraUse, Sanjeev Satheesh, Sean Ma, Zhiheng
HUang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale VisUal Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3):211-252, 2015. doi: 10.1007/s11263-015-0816-y.
Robert E Schapire. A brief introdUction to boosting. In Ijcai, volUme 99, pp. 1401-1406, 1999.
David B. Skalak. The soUrces of increased accUracy for two proposed boosting algorithms. In In
Proc. American Association for Arti Intelligence, AAAI-96, Integrating Multiple Learned Models
Workshop, pp. 120-125, 1996.
Leslie N. Smith. Cyclical Learning Rates for Training NeUral Networks. arXiv e-prints, art.
arXiv:1506.01186, JUne 2015.
Tin Kam Ho. Random decision forests. In Proceedings of 3rd International Conference on Docu-
ment Analysis and Recognition, volUme 1, pp. 278-282 vol.1, 1995.
KAGAN TUMER and JOYDEEP GHOSH. Error correlation and error redUction in ensemble clas-
sifiers. Connection Science, 8(3-4):385-404, 1996. doi: 10.1080/095400996116839. URL
https://doi.org/10.1080/095400996116839.
W. Wei, L. LiU, M. Loper, K. Chow, E. GUrsoy, S. TrUex, and Y. WU. Cross-layer strategic en-
semble defense against adversarial examples. In 2020 International Conference on Computing,
Networking and Communications (ICNC), pp. 456-460, 2020.
Wenqi Wei and Ling LiU. RobUst Deep Learning Ensemble against Deception. arXiv e-prints, art.
arXiv:2009.06589, September 2020.
Y. WU, L. LiU, C. PU, W. Cao, S. Sahin, W. Wei, and Q. Zhang. A comparative measUrement stUdy
of deep learning as a service framework. IEEE Transactions on Services Computing, pp. 1-1,
2019. ISSN 1939-1374. doi: 10.1109/TSC.2019.2928551.
Yanzhao WU, , Wenqi Cao, Semih Sahin, and Ling LiU. Experimental Characterizations and Anal-
ysis of Deep Learning Frameworks. In 2018 IEEE 38th International Conference on Big Data,
December 2018.
Yanzhao WU, Ling LiU, JUhyUn Bae, Ka-Ho Chow, ArUn Iyengar, Calton PU, Wenqi Wei, Lei YU, and
Qi Zhang. Demystifying learning rate policies for high accUracy training of deep neUral networks,
2019.
10
Under review as a conference paper at ICLR 2021
Yanzhao Wu, Ka-Ho Chow, Wenqi Wei, Zhongwei Xie, and Ling Liu. Boosting ensemble accuracy
by revisiting ensemble diversity metrics. Technical report, Georgia Institute of Technology, 2020.
GU Yule. “on the association of attributes in statistics”philosophical transactions of the royal society.
SeriesA, 194:257-319, 1900.
Barret Zoph and Quoc V. Le. Neural architecture search with reinforcement learning. CoRR,
abs/1611.01578, 2016. URL http://arxiv.org/abs/1611.01578.
A Diversity by Uncorrelated Error
Deep neural network ensembles use multiple (say M > 1) deep neural networks to form a committee
(team) to collaborate and combine the predictions of individual member models to make the final
prediction. A consensus method will be used to combine the individual predictions, such as majority
voting, plurality voting, or model averaging (the average of prediction vectors).
A deep neural network classifier is typically trained to minimize a cross-entropy loss and output a
probability vector to approximate a posteriori probability densities for the corresponding class. For
a given input X, the ith element in the output probability vector of model Fk can be modeled as:
fik(X) = p(Ci|X) + ik(X), where p(Ci|X) is the posteriori probability distribution of the ith class (Ci)
for the input X, and ik(X) is the error associated with this output. For making the Bayes optimum
decision, X will be predicted as class Ci if p(Ci|X) > p(Cj|X), ∀j 6= i. Therefore, the Bayes optimum
boundary locates at all points x* such thatp(c∕x*) = P(Cj|x*) whereP(Cj|x*) = maxι=ip(cι∣x).
Given the neural network model will output fik(X) instead of p(Ci|X), the decision boundary of the
model, X, may vary from the optimum boundary by an offset o = X - x*. (TUMER & GHOSH,
dσ2
1996) shows that the added error beyond Bayes error is Eadd = -of- where d is the difference
between the derivatives of the two posteriors and σo2 is the variance of the boundary offset o, σo2 =
2σ2k /d2. Combining the predictions of S models with model averaging (avg), the ith element in
i
the combined probability vector gives an approximation to p(c∕x) as ffv9(x) = 1 PS=I fk (x)=
P(CiIX) + «(x), where Wi(X) = 1 e，(x). We can calculate the variance of Wi with
SS	S	S
σ2 = S XX Covd(X)V(X)) = s2 X σ2k + s2 XX Covd(X)片(X))
k=1 l=1	k=1	k=1 l6=k
where Cov() represents the covariance. With Cov(a, b) = Corr(a, b)σaσb, we can replace the covari-
ance with correlation Corr() and derive
SS
σoi =点 ∑σok + 或 EECorr(Wik (X), Wli (X))σk σl
k=1	k=1 l6=k
Let δi denote the average correlation factor among these models, we have
1S
δi =』ΣΣCorr(Wik (X), Wli (X))
(	- ) k=1 l6=k
Assuming the common variance σ2i = σ2k holds for every model Fk , with δi we have
i
2	1 2 S-1	2
σ⅞ = Sσ" + -^δiσ6i
With the variance of the ensemble decision boundary offset σo2avg
GHOSH, 1996), we have
σei+2σej given in (TUMER &
σ2avg = ʒoθ (σ2i + (S - 1)δiσ2i + σ2j + (S — 1)δj σlj )
dS	j	j
11
Under review as a conference paper at ICLR 2021
Assume that the error between classes are i.i.d., that is σ2i = σ2j . With σ2i = σ2k (the previous
2σ2k
assumption) and σθ =	given in (TUMER & GHOSH,1996), We have
σθavg = dps (2σ2 + (S - I)di (δi + δj))
σ2	—2%	(δi + δ)	2σ2k	(δ + δj)
σoavg = dS(1 + (S - 1)^^) = dS(I + (S -1)^^)
σθavg = -S (I + (S - 1) 'i + 'j )
To extend the above formula to include all classes, given δ = PiC=1 Pi δi , Where Pi is the prior
probability of class ci and C is the total number of classes. Assuming the prior probability Pi of
class ci is uniformly distributed, We have
σθavg = σ2(1 + (S - 1)δ) = σθ(1 +(S- 1)δ))
So We can derive the added error for the ensemble prediction Eaadvdg as
dσθ 1 + (S - 1)δ)	1 + (S - 1)δ)
(7Γ∖-------3-------) = Eadd(------3-------)
Therefore, the ideal scenario is When all members in an ensemble team of size S are diverse. They
can learn and predict With uncorrelated errors (negative correlation), i.e., δ = 0. Then a simple
model averaging method can significantly reduce the overall prediction error by S. MeanWhile, the
Worst scenario happens When error of individual model are highly correlated With δ = 1, such as all
S models are perfect duplicates, the error of the ensemble is identical to the initial errors Without any
improvement. In general, the correlation δ lies betWeen 0 and 1, and therefore, it is alWays beneficial
to use ensemble to reduce the prediction errors.
B	Ensemble Robustness
Let g(x) = fc(x) - fj (x), Where c = argmax1≤i≤C fi(x) is the predicted class label and j 6=
c. Assume g(x) is LiPschitz continuous with LiPschitz constant Lq, according to (PaUlaviciUs &
Zilinskas, 2006), we have
|g(x) - g(y)| ≤ Ljq||x - y||p
where Lq = maxχ∣∣Vg(x)∣∣q, P + 1 = 1 and 1 ≤ p,q ≤∞.
Let x = x0 + δ and y = x0, we have
Ig(XO+δ) - g(xo)1 ≤ Lq |M||p
which can be rearranged as
g(χo) - Lq ι∣δ∣∣p ≤ g(χo + δ) ≤ g(xo) + Lq ∣∣δ∣∣p
When g(x0 + δ) = 0, the Predicted class label will change. However, g(x0 + δ) is lower bounded
by g(xo) - Lq ∣∣δ∣∣p ≤ g(xo + δ). If 0 ≤ g(xo) - Lq ∣∣δ∣∣p, we have g(χo + δ) ≥ 0 to ensure that the
Prediction result will not change with the small change δ on the inPut x0 . This leads to
g(χo) - Lq ι∣δ∣∣p ≥ o ⇒ι∣δ∣∣p ≤ 嚓
That is
12
Under review as a conference paper at ICLR 2021
I∣δ∣∣p≤
fc(x0) - fj (χo)
Lq
To ensure the classification result will not change, that is argmax1≤i≤C fi (x0 + δ) = c, we use the
minimum of the bound on δ over j 6= c, that is
l∣δ∣∣p ≤ minj=Cfc(x0)-fj(XO)
Lq
which indicates that as long as ∣∣δ∣∣p is small enough to fulfill the above bound, the classifier decision
will never be changed, which marks the robustness of this classifier. The robustness bound (R) can
be denoted as
R = min N fC (X0)- fj (XlO)
=L   Imbnj = c	-
Ljq
.	fc(xo) - fj(xo)
Tmn-C maXχ∣∖V (fc(x) - fj (x))∣∣q
For a model Fk, we have its upper bound
Rk - min=c _fC (χ0)-fjk(χ0)_
m maXχ∣∣V(fk(x) - fj(x))||q
Let gjk (X) - fCk (X) - fjk (X), we have
Rk - min W _____UjS________
3 CmaXx||V(gj(x))||q
Given S models, combining their predictions with model averaging (avg), we have the ith element
in the combined probability vector as f：vg(x) — 1 PS=I fk (x) corresponding to the robustness
bound
Ravg
.	favg(X0)- ∕Tg(x0)	=	.	gavg(χo)
mmj=cmαxx∣∣V(favg(x) - fjaVg(X))||q	mmj=cmaxx∣∣V(gavg(x))||q
Assume the minimum of the robustness bound can be achieved with the prediction result c and j for
each model including the ensemble Favg, that is
Rk -	gk(x0)
maxx||V(gk(x))||q
and
Mg	gjΓ(xo)
R -	-------------------
maxx∣∣V(gavg (x))||q
where gjvvg(x) — si PS=I gjk(x). The following property always holds that ∃1 ≤ k ≤ S, Rk ≤
Ravg , indicating that the ensemble can improve the robustness bound.
We prove the property by contradiction. First, we assume ∀1 ≤ k ≤ S, Rk > Ravg , that is
gj (χo)	g；vg(χo)
maxx∣∣V(gk(x))||q〉maxx∣∣V(gjvg(x))||q
So we have
gjk(X0)(maXx||V(gjavg(X))||q) > gjavg(X0)(maXx||V(gjk(X))||q)
13
Under review as a conference paper at ICLR 2021
For each k ∈ {1, ..., S}, we have the above inequality. To add them all, we have
SS
X gj (x0)(maxxllV(gαvg (X))IIq ) > X gjvg (XOxmaxx||V(gk(X))IIq )
k=1	k=1
That is
SS
(maXxIIV(gjavg(X))IIq) Xgjk(X0) > gjavg(X0) X(maXxIIV(gjk(X))IIq)
k=1	k=1
Given gjv(x) = 1 PS=I gj(x), We have
S	S	SS
(maXx∣∣V(X gj (X))IIq) S X gj(xo) >S X gj(xo) X(maxχ ∣∣V(gj (X))IIq)
j=1	j=1	j=1	j=1
Therefore, We have
SS
(maXxIIV(Xgjj(X))IIq) > X(maXxIIV(gjj(X))IIq)
j=1	j=1
According to the triangle inequality, We have
S	SS
maXxIIV(Xgjj(X))IIq ≤ maXx(X IIV(gjj(X))IIq) ≤ X(maXxIIV(gjj(X))IIq)
j=1	j=1	j=1
Which contradicts With our derived inequality. Therefore, the previous assumption does not hold.
We shoW that ∃1 ≤ k ≤ S, Rj ≤ Ravg , demonstrating that the robustness of a member model can
be further improved With ensemble.
Furthermore, for a model Fj, if its robustness bound Rj Was not obtained With j. We have ∃i 6=
j, i,j = c, Rk =-gJχ0)H ≤--jχ0)H . The above claim still holds as long as each
m m m ,	maXχ∣∣V(gk(x))||q — rnaXχ∣∣V(gk(x))||q	。
model makes the same prediction c.
C	Algorithms for Computing Q-diversity Metrics
We have covered six state-of-the-art diversity metrics (coined in this paper as Q-diversity metrics).
In the literature, different studies Will use one of these diversity metrics to select models and analyze
the prediction results. HoWever, there are feW studies to provide guidelines for choosing them or to
compare and evaluate these diversity metrics in terms of pruning out loW diversity ensembles.
In general, diversity metrics can be classified into tWo major categories based on hoW the fault inde-
pendence and uncorrelated errors are computed using a set of negative samples. They are pairWise
metrics and non-pairWise metrics. We beloW describe six representative diversity metrics considered
in our study: Cohen’s Kappa, Q Statistics and Binary Disagreement for pairWise, and Fleiss’ Kappa,
Kohavi-Wolpert Variance and Generalized Diversity for non-pairWise.
Given a pool of M base models, all trained on the same dataset, one approach to create negative
samples is to get the negative samples from the validation set of each model and then randomly
select a subset of negative samples from the union of all M subsets of negative examples. Let
X = {x1, x2, ..., xN} be the randomly selected N labeled negative examples on the training dataset.
Given a base model Fi and a negative sample X, the output of Fi on X is a vector of binary values,
denoted as ωi = [ωi,1, ωi,2, ..., ωi,N]T, and ωi,j = 1 ifFi predicts xj correctly, otherWise, ωi,j = 0.
14
Under review as a conference paper at ICLR 2021
Pairwise Diversity Metrics For pairwise diversity metrics, they are calculated based on a pair
of classifiers. Table 4 shows the relationship between a pair of classifiers Fi , Fj . For a labeled
sample xk, four different types of prediction results emerge, such as both Fi and Fj make correct or
wrong predictions and either Fi or Fj makes correct predictions. Correspondingly, we can count the
number of samples in the four different types, that is Nab, which represents the number of elements
xk ∈ X, such that ωi,k = a and ωj,k = b.
Table 4: The relationship between a pair of classifiers
Fj correct (1)	Fj wrong (0)
Fi correct (1)	N11	N10
Fi Wrong (0)	N01	N00
N = N00 + N01 + N10 + N11
i.	Cohen’s Kappa (CK): The Cohen’s Kappa measures the diversity between the two classifiers
Fi, Fj from the perspective of agreement (McHugh, 2012; Kuncheva & Whitaker, 2003). A lower
Cohen’s kappa value implies lower agreement and higher diversity. Formula 1 shows the definition
of the Cohen’s kappa (κij) between the two classifiers Fi , Fj . The value for the Cohen’s Kappa
ranges from -1 to 1 with 0 representing the amount of agreement of random chance. (McHugh,
2012)
2(N11N00 - N01N10)
Kij = (N11 + N10)(N01 + N00) + (N11 + N01)(N10 + N00)
(1)
ii.	Q Statistics (QS): The Q statistics (Yule, 1900) is defined as QSij in Formula 2 between two
models Fi, Fj. QSij varies between -1 and 1. When the models Fi, Fj are statistically independent,
the expected QSij is 0. If the two models tend to recognize the same object similarly, QSij will
have positive value. While two diverse models, recognizing the same object differently, will render
a small or negative QSij value.
QSij
N11N00 - N01N10
N11N 0。+ N 01N10
(2)
iii.	Binary Disagreement (BD): The binary disagreement (Skalak, 1996; Kuncheva & Whitaker,
2003) is the ratio between (i) the number of samples on which one model is correct while the other
is wrong to (ii) the total number of samples predicted by the two models Fi , Fj as Formula 3 shows.
N01 + N10
N11 + N10 + N01 + N00
(3)
For an ensemble team of S models, as recommended by (Kuncheva & Whitaker, 2003), we calculate
the averaged metric value over all pair of classifiers as Formula 4 shows, where Q represents a pair-
wise diversity metric.
2
S(S - 1)
Q
S-1 S
X X Qij
i=1 j=i+1
(4)
Non-pairwise Diversity Metrics Numerous non-pairwise diversity metrics are widely used for a
team of over 2 models. To compare with pairwise diversity metrics, we focus on three representative
non-pairwise diversity metrics.
In an ensemble team of S classifiers, we use l(xk) to denote the number of classifiers that correctly
recognize xk, i.e., l(xk) = PiS=1 ωik.
iv.	Fleiss’ Kappa (FK): Similar to Cohen’s Kappa, the Fleiss’ Kappa (Fleiss et al., 2013) also
measures the diversity from the perspective of agreement. But it is directly calculated from a team
of more than 2 models as Formula 5 shows, where P is the average classification accuracy for the
15
Under review as a conference paper at ICLR 2021
ensemble team and κ is not obtained by simply averaging the Cohen’s kappa (κij).
1NS
P= NS XX ωi,k
k=1 i=1
=1 _ 1 PN=I i(χk)(s - I(Xk)
K =	N (S - 1)p(1 - P)
(5)
v.	Kohavi-Wolpert Variance (KW): Kohavi-Wolpert Variance is derived by (Kuncheva &
Whitaker, 2003) to measure the variability of the predicted class label for the sample χ with the
team of models F1, F2, ..., FS as Formula 6 shows. Higher value of KW variance indicates higher
model diversity of the team.
1N
KW = Nq2 El(xk)(S - I(Xk))	⑹
NS2
k=1
vi.	Generalized Diversity (GD): The generalized diversity has been proposed by (Partridge &
Krzanowski, 1997) as Formula 7 shows. Y is a random variable, representing the proportion of clas-
Sifiers (out of S) that fail to recognize a random sample x. The probability of Y = S is denoted as
Pi, i.e., the probability ofi (out ofS) classifiers recognizing a randomly chosen sample X incorrectly.
P(1) represented the expected probability of one randomly picked model failing while P(2) denotes
the expected probability of both two randomly picked models failing. GD varies between 0 and
1. The maximum diversity (1) occurs when the failure of one model is accompanied by the correct
recognition by the other model for two randomly picked models, that is P(2) = 0. When both two
randomly picked models fail, we have P(1) = P(2), corresponding to the minimum diversity, 0.
Si
P(I) = X Spi
i=1
P(2) = X U Pi
i=1
GD = 1 - p(2)
P⑴
(7)
Algorithm 1 shows the sketch of the process of using a threshold-based filter. The diversity threshold
calculation function is denoted as Θ, such as the mean function. First, we calculate the diversity
measurements for all ensemble teams. Then based on the diversity threshold θ(Q) (Line 10), we
can prune out the teams with low diversity (qi ≥ θ(Q)) and place the remaining high diversity
ensembles into GEnsSet (Line 11~15). With a proper threshold, θ, the threshold-based pruning
can efficiently prune out low-diversity deep ensembles.
D	The Algorithm for Computing HQ-diversity Metrics
Unlike the Q-diversity metrics, HQ-diversity metrics calculate the diversity among the ensembles of
the same size with a focal model. Algorithm 2 shows the skeleton of calculating the HQ diversity
metrics for all the candidate ensembles in EnsSet. For each team size S (Line 6~29), We follow two
general steps to calculate the HQ diversity scores for each ensemble. First, each model in the base
model pool will serve as the focal model. For the specific focal model Ff ocal, let EnsSet(Ffocal, S)
denote all candidate ensembles of size S, each containing the member model Ff ocal . We first
compute the Q-diversity score for each ensemble in EnsSet(Ffocal , S) with the negative samples
drawn from the focal model Ffocal and store them in D(Q, S, Ffocai) (Line 10~10). Then, in
order to make them comparable across different focal models, we scale D(Q, S, Ffocal) into [0, 1]
and store them into D(Q, S, Ffocal,Ti) for each ensemble Ti (Line 15~18). Second, for each
candidate ensemble ⑵)of size S, we perform a weighted average of the scaled diversity scores
D(Q, S, Ffocal = Ti[j],Ti) associated with each of its member model Ti[j] to obtain the unified
16
Under review as a conference paper at ICLR 2021
Algorithm 1 Threshold-based Q-diversity Pruning
1: 2:	procedure THRESHOLD-BASED-PRUNING(N egSampSet, Q, Θ, EnsSet) Input: N egSampSet: negative samples; Q the diversity metric; Θ: the diversity threshold calculation function; EnsSet: the set of ensemble teams to be considered;
3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17:	Output: GEnsSet: the set of good ensemble teams. Initialize GEnsSet = {}, D = {} for i = 1 to |EnsSet| do . calculate the diversity metric Q for Ti ∈ EnsSet qi = DiversityM etric(Q, Ti , NegSampSet) D.append(qi)	. Store qi in the diversity measures D end for θ(Q) = Θ(D)	. Calculate the diversity threshold for i = 1 to |E nsS et| do if qi < θ(Q) then GEnsSet.add(Ti)	. add qualified Ti end if end for return GEnsSet end procedure
Algorithm 2 HQ Diversity Metric Calculation
1: 2: 3: 4: 5: 6: 7: 8: 9:	procedure GETHQ(N egSampSet, Q, E nsS et) Input: N egSampSet: negative samples for each model; Q the diversity metric; EnsSet: the set of ensemble teams to be considered; Output: HQ: the set ofHQ diversity measurements Initialize D(Q) = {}, D(Q) = {} Initialize HQ = {}	. A map ofHQ diversity metrics and teams for S = 2 to M - 1 do for f ocal = 0 to M - 1 do Obtain E nsS et(Ff ocal , S) with candidate teams of size S and containing Ff ocal. Initialize D(Q, S, Ffocal) = [ ]
10:	for i = 1 to |E nsS et(Ff ocal , S)| do
11: 12:	. calculate the diversity metric Q for Ti ∈ E nsS et(Ff ocal , S) qi = DiversityM etric(Q, Ti , N eg S ampS et(Ff ocal))
13:	D(Q, S, Ffocal).append(qi)	. add the acci into D(Q, S, Ffocal)
14:	end for
15:	for i = 1 to |E nsS et(Ff ocal , S)| do
16:	. scale the diversity metrics for ensemble teams of the same size
17:	D(Q, S, Ffocal, Ti) = max(D(QS,Ffo：i);QminfD(Q)S ,Ff℃ai ))	. Scale to [0, 1]
18:	end for
19:	end for
20:	Obtain EnsSet(S) with candidate teams of size S
21:	for i = 1 to |E nsS et(S)| do
22:	Initialize tmpD = {}
23:	for j = 0 to 田 | do
24:	tmpD.append(D(Q, S, Ffocal = Ti[j],Ti))
25:	end for
26:	w = MemberModelAccuracyRank(Ti )	. Obtain the weights for combining tmpD
27:	HQ(Ti) = W eightedAverage(w, tmpD)
28:	end for
29:	end for
30:	return HQ
31:	end procedure
HQ score. The weight is calculated with the corresponding rank of accuracy of the member model
(Ti [j]) in the ensemble (Ti), i.e., the member model with higher accuracy will have higher weight
(Line21 ~28).
17
Under review as a conference paper at ICLR 2021
E THE ALGORITHM FOR THE α FILTER
To construct a deep ensemble teams of diverse models, we start with building the ensembles of a
smaller size, such as S = 2 with (MM) = M (M -1) candidates. For a larger size, such as S * = S +1,
we then extend these candidate ensembles of size S by adding another member model from the base
model pool. This way of constructing deep ensembles enables us to efficiently form high quality
deep ensembles step by step and strategically prune out low diversity ensembles.
Intuitively, an ensemble team of a larger size S = 3, such as [F5 , F6 , F7] containing a subset of
models with lower ensemble diversity (i.e., higher correlation), such as [F5 , F6], then the other
teams with size S = 3, such as [F5 , F7, F9], which may have higher diversity than [F5 , F6 , F7],
so we can preemptively prune out [F5 , F6] for S = 2 to avoid calculating the diversity scores for
ensembles with S > 2 containing [F5 , F6] as Figure 5 shows.
Figure 5: α Filter
Algorithm 3 α Filter
1: 2:	procedure α-FILTER(N egSampSet, Q, β, EnsSet) Input: N egSampSet: negative samples; Q the diversity metric; β: the percentage of number of ensemble teams to be pruned out each step; EnsSet: the set of ensemble teams to be considered;
3: 4: 5: 6:	Output: GE nsS et(Q): the set of good ensemble teams pruned by α-pruning with diversity metric Q. Initialize GEnsSet(Q) = {}, D = {} Initialize pruneSet = {}	. To prune out. for S = 2 to M do
7: 8: 9:	Initialize tmpGEnsSet(Q, S) = {}. Construct ensembles from EnsSet of size S into EnsSet(S) for i = 1 to |EnsSet(S)| do
10:	if Ti contains any group of models in pruneSet then
11:	continue	. Prune out this branch
12:	else
13:	qi = DiversityM etric(Q, Ti , N egSampSet)
14:	D.append(qi)
15:	tmpGEnsSet(Q, S).append(Ti)
16:	end if
17:	end for
18:	n = β * 1tmpGEnsSet(Q, S)|
19:	sort Ti ∈ tmpGE nsS et(Q, S) by qi
20:	remove n teams of lowest diversity from tmpGEnsSet(Q, S) and add them in to pruneSet
21:	GEnsSet(Q)∪ = tmpGEnsSet(Q, S)
22:	end for
23:	return GE nsS et(Q)
24:	end procedure
Therefore, with this property we can effectively prune out low-diversity deep ensembles. Algo-
rithm 3 presents a skeleton of the pseudo code, describing this pruning process. NegSampSet
contains the set of negative samples for calculating the diversity metric Q. β marks the percentage
of the teams to be further pruned out for a fixed team size. By default, we set β = 10%. EnsSet
contains the set of ensemble teams to be considered. For each team size, we omit all the teams
that contain any group of models in pruneSet. For the remaining teams, we measure their diver-
sity scores and ordered them based on the diversity score pi . Then we remove β of the remaining
teams with the lowest diversity and add them into pruneSet for further pruning. This algorithm can
significantly avoid exploring unpromising branches in searching for high-quality ensembles.
18
Under review as a conference paper at ICLR 2021
F The Base Model Pools for Three Benchmark Datasets
We evaluate the proposed hierarchical diversity pruning methods using three benchmark datasets,
CIFAR-10, ImageNet, and Cora. The specification of these datasets and the base model pools for
each of the datasets are included in this section as Table 5 shows. We use 10 base models in this
study for each dataset, primarily collected from GTModelZoo (GTModelZoo Developers, 2020).
Table 5: Base Model Pools for Three Benchmark Datasets
Dataset		CIFAR-10			ImageNet		Cora	
	10,000 testing samples		50,000 testing samples		1,000 testing samples	
Number	Models	Accuracy (%)	Models	Accuracy (%)	Models	Accuracy (%)
0	DenseNet190	96.68	AlexNet	56.63	GCN	8170
1	DenseNet100	95.46	DenseNet	77.15	GAT	8280
2	ResNeXt	96.23	EfficientNet-B0	75.80	SGC	8170
3	WRN	96.21	ResNeXt50~~	77.40	-ARMA	82T0
4	VGG19 一	93.34	Inception3	77.25	-APPNP~~	8220
5	ResNet20	91.73	ResNet152	78.25	-APPNPI^^	83:80
6	ResNet32	92.63	ResNet18	69.64	-APPNP2^^	8870
7	ResNet44	93.10	SqueezeNet	58.00	SplineCNN	8890
8	ResNet56	93.39	VGG16	71.63	SPIineCNNI	8830
9	ResNet110	93.68	VGG19-BN~~	74.22	SPIineCNN2	88:50
-MIn-	ResNet20	91.73	AlexNet	56.63	GCN/SGC 一	81.70 —
-AVG-		94.25			71.60			84.87	
MAX	DenseNet190	96.68	ResNet152	78.25	SPHneCNN	88.90
Table 6: Q-Metrics with α filter
Dataset	Methods	#EnsSet	#GEnsSet	Ensemble Acc Range (%)	#(%)(ACC > max)
CIFAR-10	Q-CK	-270-	242	93.56〜96.64	0
	Q-QS	-269-	242	93.56〜97.01	-4(1.65%)-
	Q-BD	-241-	241	93.56〜96.64	0
	-Q-FK	-266-	238	93.56〜96.64	0
	Q-KW	-268-	241	93.56〜96.64	0
	Q-GD	-236-	213	93.56〜97.01	16(7.51%)
ImageNet	Q-CK	-260-	235	61.39 〜80.01	56 (26.83%)
	Q-QS	-253-	229	61.39 〜80.01	64 (27.95%)
	Q-BD	-277-	249	61.39 〜79.84	67 (26.91%)
	-Q-FK	-260-	235	61.39 〜80.01	56 (23.83%)
	Q-KW	-277-	249	61.39 〜79.84	67 (26.91%)
	Q-GD	-237-	215	68.99〜80.29	104 (48.37%)
Cora	Q-CK	-2∏-	185	82.10 〜89.50	9 (4.86)
	Q-QS	-214-	187	82.10 〜89.50	-9(4.81%)-
	Q-BD	-215-	188	82.10 〜89.50	-9 (4.79%)-
	-Q-FK	-2∏-	185	82.10 〜89.50	-9 (4.86%)-
	Q-KW	-215-	188	82.10 〜89.50	-9 (4.79%)-
	Q-GD	228	208	82.10 〜89.50~	12(5.77%)~~
G	THE α FILTER ON Q DIVERSITY METRICS
We also applied the α filter with six Q-diversity metrics for pruning out low-diversity ensembles.
Figure 6 shows the experimental results on the Q-GD metric on CIFAR-10, where Figure 6a, 6b
and 6c present all the candidate ensembles of size 3, 4 and 5 respectively, and the relationship of
the Q-diversity metric GD and ensemble accuracy. The black dots mark the ensembles that pruned
out by the α filter while the red ones represent the remaining ensembles. Even though, the α filter
can significantly filter many low-diversity ensembles, we still miss a fair number of ensembles with
high ensemble accuracy. There are two primary reasons behind this observation: (1) The Q-diversity
metrics fail to precisely capture the diversity of ensembles, therefore, when pruning out a low Q-
diversity branch, such as in Figure 6a with S = 3, some ensembles of a larger size with high diversity
(with low Q-GD values) many also be pruned out in Figure 6b with S = 4. (2) A few ensembles
19
Under review as a conference paper at ICLR 2021
with high ensemble accuracy have low diversity, demonstrating that Q-diversity metrics may not
be effectively correlated to ensemble accuracy. We further perform a comprehensive evaluation as
Table 6 shows on three datasets. Due to the above inherent problems with Q metrics, the α filter on
our HQ metrics achieved much better performance than Q metrics, when comparing Table 6 with
Table 2, 3, 7.
(a) S=3, (Pruning 39 out of 120)
(b) S=4, (Pruning 139 out of 210)
(c) S=5, (Pruning 232 out of 252)
Figure 6: α filter on Q diversity with different team size S (CIFAR-10, Q- GD))
H Experimental Evaluation on Cora Dataset
We also evaluate our methods on a popular graph dataset, Cora. The same set of experimental
results are shown on Table 7. We found similar observations as CIFAR-10 and ImageNet. First,
the α filter with HQ metrics works much better than Q metrics. HQ metrics can capture more high
accuracy (≥ 89%) ensembles (14〜18) than 6〜17 by Q metrics With the mean threshold. Second,
the combined hierarchical pruning method of the α filter and K-Means filter on HQ metrics (α + K)
can significantly improve the ensemble accuracy lower bound from 82.10% to 86.70%〜87.80% as
Well as the probability of high accuracy ensembles among the selected ones.
Table 7: Comparing Q, HQ (α), and HQ (α + K) metrics on Cora
Methods	#EnSSet	#GEnSSet	Ensemble ACC Range (%)	#(%)(ACC > 88.90% (max))	#(ACC >= 89.00%)
Baseline	-10T3-	-10T3-	82.10 〜89.50	-38 (3.75%)-	22
Q-CK	-10T3-	594	85.00〜89.50	-21 (3.54%)-	12
Q-QS	-10T3-	596	85.40〜89.50	-23 (3.86%)-	14
Q-BD	-10T3-	602	85.00〜89.50	-22 (3.65%)-	13
Q-FK	-10T3-	583	85.00〜89.50	-21 (3.60%)-	12
Q-KW	-10T3-	647	86.10 〜89.10	-13(2.10%)-	6
Q-GD	-10T3-	583	85.40〜89.50	-32 (5.49%)-	17
HQ-CK (α)	-234-	212	82.10 〜89.50	-21 (9.91%)-	15
HQ-QS (α)	-256-	232	82.50〜89.50	24 (10.34%)	18
HQ-BD (α)	-241-	218	82.10 〜89.50	-20 (9.17%)-	14
HQ-FK (α)	-234-	212	82.10 〜89.50	-21 (9.91%)-	15
-HQ-KW (α)	-241-	218	82.10 〜89.50	-20 (9.17%)-	14
HQ-GD (α)	-239-	216	82.10 〜89.50	-20 (9.26%)-	14
HQ-CK (α + K)	-212-	22	86.70〜89.50	-5 (22.73%)-	4
HQ-QS (α + K)	-232-	67	87.80〜89.00	-8(11.94%)-	5
HQ-BD (α + K)	-218-	26	86.70〜89.50	-5 (19.23%)-	4
HQ-FK (α + K)	-212-	24	86.70〜89.50	-5 (20.83%)-	4
HQ-KW (α + K)	-218-	30	86.70〜89.50	-4(13.33%)-	3
HQ-GD (α + K)	216	24	86.70〜89.50~~	5 (20.83%)~~	4
20