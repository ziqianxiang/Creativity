Under review as a conference paper at ICLR 2021
Information distance for neural network
FUNCTIONS
Anonymous authors
Paper under double-blind review
Ab stract
We provide a practical distance measure in the space of functions parameterized
by neural networks. It is based on the classical information distance, and we
propose to replace the uncomputable Kolmogorov complexity with information
measured by codelength of prequential coding. We also provide a method for di-
rectly estimating the expectation of such codelength with limited examples. Em-
pirically, we show that information distance is invariant with respect to different
parameterization of the neural networks. We also verify that information distance
can faithfully reflect similarities of neural network functions. Finally, we applied
information distance to investigate the relationship between neural network mod-
els, and demonstrate the connection between information distance and multiple
characteristics and behaviors of neural networks.
1	Introduction
Deep neural networks can be trained to represent complex functions that describe sophisticated
input-output relationships, such as image classification and machine translation. Because the func-
tions are highly non-linear and are parameterized in high-dimensional spaces, there is relatively
little understanding of the functions represented by deep neural networks. One could interpret deep
models by linear approximations (Ribeiro et al., 2016), or from the perspective of piece-wise linear
functions, such as in (Arora et al., 2018).
If the space of functions representable by neural networks admits a distance measure, then it would
be a useful tool to help analyze and gain insight about neural networks. A major difficulty is the
vast number of possibilities of parameterizing a function, which makes it difficult to characterize the
similarity given two networks. Measuring similarity in the parameter space is straightforward but is
restricted to networks with the same structure. Measuring similarity at the output is also restricted
to networks trained on the same task. Similarity of representations produced by intermediate layers
of networks is proved to be more reliable and consistent (Kornblith et al., 2019), but is not invariant
to linear transformations and can fail in some situations, as shown in our experiments.
In this paper, we provide a distance measure of functions based on information distance (Bennett
et al., 1998), which is independent of the parameterization of the neural network. This also removes
the arbitrariness of choosing “where” to measure the similarity in a neural network. Information
distance has mostly been used in data mining (Cilibrasi & Vitanyi, 2007; Zhang et al., 2007). Intu-
itively, information distance measures how much information is needed to transform one function to
the other. We rely on prequential coding to estimate this quantity. Prequential coding can efficiently
encode neural networks and datasets (Blier & Ollivier, 2018). If we regard prequential coding as
a compression algorithm for neural networks, then the codelength can give an upper bound of the
information quantity in a model.
We propose a method for calculating an approximated version of information distance with prequen-
tial coding for arbitrary networks. In this method, we use KL-divergence in prequential training and
coding, which allow us to directly estimate the expected codelength without any sampling process.
Then we perform experiments to demonstrate that this information distance is invariant to the pa-
rameterization of the network while also being faithful to the intrinsic similarity of models. Using
information distance, we are able to sketch a rough view into the space of deep neural networks and
uncover the relationship between datasets and models. We also found that information distance can
1
Under review as a conference paper at ICLR 2021
help us understand regularization techniques, measure the diversity of models, and predict a model’s
ability to generalize.
2	Methodology
Information distance measures the difference between two objects by information quantity. The
information distance between two functions fA and fB can be defined as (Bennett et al., 1998):
d(fA,fB) = max{K(fA|fB),K(fB|fA)}
(1)
This definition makes use of Kolmogorov complexity: K(fB|fA) is the length of the shortest pro-
gram that transforms fA into fB, and information distance d is the larger length of either direction.
(Note that this is not the only way to define information distance with Kolmogorov complexity,
however we settle with this definition for its simplicity.) Intuitively, this is the minimum number of
bits we need to encode fB with the help of fA , or how much information is needed to know fB if
fA is already known.
Given two functions fA : X → Y and fB : X → Y 0 defined on the same input space X , each
parameterized by a neural network with weights θA and θB , we want to estimate the information
distance between fA and fB. The estimation of Kolmogorov complexity term is done by calculating
the codelength of prequential coding, so what we get is an upper bound of d, which we denote by
dp (p for prequential coding).
2.1	ESTIMATING K(fB|fA) WITH PREQUENTIAL CODING
To send fB to someone who already knows fA, we generate predictions yi from fB using input xi
sampled from X. Assume that {xi } is known, we can use prequential coding to send labels {yi}. If
we send enough labels, the receiver can use {xi, yi} to train a model to recover fB.
If fA and fB have something in common, i.e. K(fB|fA) < K(fB), then with the help of fA we
can reduce the codelength used to transmit fB. A convenient way of doing so is to use θA as the
initial model in prequential coding. The codelength of k samples is:
k
Lpreq(yi：k|xi：k) ：= - ElOgpθi(yiM：i,yi：i-i)	⑵
i=1
where θi is the parameter of the model trained on {xi：i—i, yi：i—i}, and θι = Θa. With sufficient
large k, the function parameterized by θk would converge to fb .
If both fA and fB are classification models, we can sample y from the output distribution of fB . In
this case, the codelength (2) not only transmits fB, but also k specific samples we draw from fB .
The information contained in these specific samples is Pik=1 lOg pθB (yi|xi). Because we only care
about estimating K(fB |fA), using the “bits-back protocol” (Hinton & van Camp, 1993) the infor-
mation of samples can be subtracted from the codelength, resulting in an estimation of K(fB |fA)
as Lk(fB |fA):
kk
Lk (∕b |/a) = - Elog pθ (yiM：i,yi：i-i) + £log pθb (y∕χi)	(3)
i=1	i=1
In practice, We want to use k sufficiently large such that f@^ can converge to fp, for example by the
criterion
Eχ[DκL(fB(x)llfθk(x))] ≤ E	(4)
However, empirically we found that this often means a large k is needed, which can make the
estimation using (3) unfeasible when the number of available x is small. Also the exact value of (3)
depends on the specific samples used, introducing variance into the estimation.
2
Under review as a conference paper at ICLR 2021
2.2	THE PRACTICAL INFORMATION DISTANCE dp
We propose to directly estimate the expectation of Lk(fB|fA), which turns out to be much more ef-
ficient in the number of examples x, by leveraging infinite y samples. The expectation of codelength
Ey1:k [Lk] over all possible samples y1:k from fB is:
kk
EyLk ~fB (xι")[Lk(fB IfA)] = - E EyLilOg Pθ (yi |x1：i, yi:i-1) + E EyilOg PΘb 3 E)
i=1	i=1
kk
≥ - EEyilog EyLi-IPθ(yi|xi:i, yi:i-1) + EEyilogPΘb 3肩)
i=1	i=1
k
=X DKL(fB (Xi)I|Eyi：i-i f^i (Xi))	(5)
i=1
k
≈ X DκL(fB (xi)IIfθi (Xi)) =: L0 (fB IfA)	(6)
i=1
In (5), Eyi：i—i fθ (xi) represents an infinite ensemble of models θi estimated from all possible sam-
ples yi：i—i. We replace this ensemble with a single model & that is directly trained on all the
samples. Gi is trained using KL-divergence as objective, which is equivalent to training with infinite
samples (see Appendix A for details from (5) to (6)).
The expected codelength E[Lk] is related, via (6), to the KL-divergence between the output distri-
butions of fp and fa. Another interpretation of the above estimation is, We finetune model Θa with
an increasing number of outputs generated by θB, and aggregate the KL-divergence between the
two models along the way. The more information fA shares with fB, the faster the KL-divergence
decreases, resulting in a lower estimation of K(fB IfA).
Now dp is the approximation of (1) we propose in this paper:
dp(fA,fB) ∧=max{L0(fAIfB),L0(fBIfA)}	(7)
2.3	PROPERTIES OF dp
The information distance d in (1) applied to functions defines a metric on the space of functions.
Now we check if dp satisfy the axioms of a metric:
1.	dp(fA, fB) = 0 ⇔ fA = fB : fA = fB if and only if they always produce the same
predictions, which is equivalent to dp(fA, fB ) = 0
2.	dp(fA,fB) = dp(fB, fA): by definition.
3.	dp(fA, fB) ≤ dp(fA, fC) + dp(fC, fB): whether dp keeps this property ofd depends on
the efficiency of prequential coding, which in turn depends on model optimization.
Another important property of the information distance d is the invariancy with respect to the pa-
rameterization of function f . We found that dp is also largely invariant to parameterization of the
functions. dp can be used to compare models trained differently, having different structures, or even
trained on different tasks. The only condition is that both models should have sufficient expressibil-
ity to allow approximation of each other.
There is also a connection between Lk(fB IfA) and the information transfer measure LIT (Zhang
et al., 2020):
LkT(θn) = Lθ0eq(yi：k|xi：k) - Lθreq(y±k|xi：k)
as n → ∞, θn → Θb, and when yi ~ f§ (xi), We have
E[Lkτ(θn)] =ELAq(yi：k|xi：k)] - ELreq(yi：k|xi：k)]
=E[Lk(fBIfA)]	(8)
3
Under review as a conference paper at ICLR 2021
2.4	Data-dependency and equivalent Interpretations of data
In machine learning, we often only care about the output of a model on the data distribution of the
task. Neural network models are trained on input data from a specific domain, for example, image
classification models take natural images in RGB format as valid input. It would be meaningless to
discuss the behavior of such a model on non-RGB images. This is an important distinction between
a neural network function and a function in the mathematical sense.
This motivates us to take a data-dependent formulation of distance measure. In this paper, we limit
our discussion to distribution-dependent information distance:
dp(fA,fB) =max{K(fA0 |fB),K(fB0 |fA)}	(9)
where
fA0 = argminK(f|fB),	fB0 = argminK(f|fA)	(10)
f∈FA	f∈FB
are equivalencies of fA and fB in the below function family ( can be A or B):
F□ = {f|Ex〜D[DκL(f (x)llf□(x))] ≤ 4	(11)
F is a set containing all the functions producing outputs almost indistinguishable from f, in
the expected sense over x drawn from data distribution D. Because they produce almost identical
outputs for X 〜D, we call them equivalent interpretations of data in D. Intuitively, this means
that instead of transmitting fB, we can transmit fB0 , which is equivalent to fB on D, if fB0 can be
transmitted in fewer bits.
A quick note on why data-dependency here in the context of neural network models does not break
the definition of information distance: if f is a neural network trained on dataset D, then f is fully
determined by the information in D plus a random seed (which is of negligible information).
By introducing data-dependency, it enables us to approximate Kolmogorov complexity by coding
samples drawn from data distribution D, in other words we can use the training set for coding.
3	Empirical study
The proposed information distance dp relies on an estimation of Kolmogorov complexity of neural
networks with prequential codelength, which unfortunately does not have known theoretical guaran-
tees. Therefore we validate the performance ofdp mainly with empirical results. We use experiments
to show the advantages of dp and in what situations are dp useful.
3.1	Experiment setup
All experiments in this section are performed using ResNet-56 models (He et al., 2016) on Tiny-
ImageNet1, a 200-class image classification dataset. To make the codelength estimation more reli-
able, we try to achieve lower codelength in prequential coding by optimizing the model estimation
process in sequential coding. We performed a hyper-parameter search to select optimal hyper-
parameters that result in the lowest codelength. Unless otherwise stated, we use k = 10000 in
experiments throughout this paper, which we found in most cases allow the difference between the
coding model and the reference model Eχ[DkL(∕b (x) ∣∣f^ (x))] to converge.
3.2	Invariances of information distance
A prominent advantage of information distance is independent of parameterization. Neural networks
like multi-layer perceptrons can have a very large number of different configurations (units, weights)
that correspond to the same function. Because there is no “canonical” way of parameterizing neural
networks, comparing the input-output functions represented by different neural networks can be
difficult by merely looking at the network parameters.
There exist many metrics for measuring the similarity or distance between neural networks. But
because neural networks are so versatile, similar neural networks can look similar under some metric
1https://tiny-imagenet.herokuapp.com
4
Under review as a conference paper at ICLR 2021
2000
—■ Param L2 -∙∙∙ Rep CKA
Param cosine	Rep EMD ,
-♦- Param EMD	OutkL
RePL2
...Rep cosi,ne ∙''
Param L2 /―•■• Rep CKA
Param cosine / •—∙ Rep EMD
Param EMD / OutKL
Rep L2 /
Rep.ς≈ineZ-
Scaling
Neuron swapping
2000
1500
1500
1000
500
500
Architecture
Initialization
Adversarial perturbation
2000
2000
1500
1500
言
1000
500
--∙ Rep CKA
--- Rep EMD
--OutKL/
--Param L2
---Param cosine
--- Param EMD
RePL2
Rep cosine
⅛ɪooo
Param L2 /
Param Codne
Param EXlD
-ɔ∖-Rep12Z.______
∖Reo cosine
0-l->--------1--------1--------1-------1--------r-i
0.5 LO 1.5	2.0	2.5	3.0
Coefficient
-Rep CKA
■ ■ Rep EMD
--Out KL
OutKL
2000
1500
500
500
transfer learning
0.4
Scale
⅛ɪooo
P.O 0.2	0.4	0.6
Coefficient
.... RePCKA
■--- Rep EMD
1000
0
from SCratCh


Figure 1: Distance dp changes with respect to changes in the parameterization of the networks. We
measure the distance between a pair of ResNet-56 models θA and θB, trained on two subsets of
Tiny-ImageNet, respectively. We modify the configuration of θA while keeping θB the same. Mod-
ifications include: scaling, where we multiply the weights of the network by a coefficient c, neuron
swapping, where we randomly permute a fraction c of the units within a layer, perturbation, where
Gaussian random noise of zero mean and standard deviation c is added to the weights of the network,
and adversarial perturbation, where a vector of standard deviation c is added to the weights of the
network to maximize the change of second-to-last layer representations. We also include training θA
with different initializations and using different network architectures. Some commonly used dis-
tance measures are used as baselines. These include measures in three different spaces: parameter
space distances, which measure distance using metrics on the parameter matrices, including plain
L2 distance, cosine similarity, and Earth Mover Distance (EMD) (Monge, 1781; Rubner et al., 1998)
that computes the cost to align neurons. Representation space distances measure distance on the
second-to-last layer representation vectors, including plain L2, cosine, the EMD cost of aligning
feature dimensions, and Linear Centered Kernel Alignment (CKA) (Kornblith et al., 2019) which is
based on pairwise sample similarity and outperforms previously proposed similarity measures. For
output space distance, we use the KL-divergence between the probability distribution generated by
the final softmax layer. The baseline measures include both common and straightforward measures
and more sophisticated measures like EMD and CKA that possess invariance properties. Underlined
labels on the x-axis denote the distances measured between unmodified θA and θB . Curves for each
measure are individually scaled (only scaled, not shifted) to ease viewing on the same graph. Cosine
similarity and CKA value are within [0,1] and is inversely correlated with distance.
while very dissimilar by other metrics. There lacks a universal definition of similarity, which is
precisely the problem we try to solve with dp .
To empirically examine the invariancy of dp , we evaluated dp under different re-parameterizations
of a neural network. We also include a number of distance metrics as baselines for comparison.
Descriptions of test scenarios, baselines, and the results are shown in Figure 1. Table 1 summarizes
the observed invariancy of distance measures with a quantitative measure.
Results indicates that dp is relatively stable under different kinds of re-parameterization of the net-
work and is the most invariant overall. Other distance measures all exhibit strong dependency on cer-
tain kind of parameterization or is inapplicable for some parameterizations. For re-parameterizations
that does not change or only minimally change the function f (scaling, neuron swapping, initializa-
tion, architecture), dp also exhibit minimal change. For adding perturbations to the network, infor-
mation distance only starts to increases when the perturbation is large enough. This is because only
large noise will start to “wipe out” information in the network. dp is also robust to small adversarial
perturbations, while also showing that adversarial perturbations destroy information in the network
faster than random noise.
5
Under review as a conference paper at ICLR 2021
Table 1: Measuring invariancy of distance measures. Invariancy is measured by the relative change
of distance in each test scenario (averaged over all data points). Lower value means more invariant.
Oracle refers to an ideal distance measure. N/A means the method cannot be used in that test.
Space	Method	Scaling	Neuron swapping	Perturbation	Adversarial perturbation	Initialization	Architecture	Mean
Parameter	L2	0.51	0.16	0.55	0.14	0.62	n/a	0.39
	Cosine	0.04	0.19	0.16	0.02	0.88	n/a	0.26
	EMD	2.39	0.03	0.76	0.01	0.04	n/a	0.65
Representaiton	L2	0.02	0.55	0.69	0.15	0.43	n/a	0.37
	Cosine	0.00	0.30	0.08	0.04	0.18	n/a	0.12
	EMD	0.02	0.00	0.60	0.15	0.36	0.08	0.20
	Linear CKA	0.00	0.00	0.12	0.33	0.10	0.01	0.10
Output	DKL	0.02	0.01	1.38	3.59	0.37	0.08	0.91
Other	dp	0.04	0.00	0.11	0.14	0.07	0.03	0.06
	Oracle	~0	0	>0,≪1	>0, ≪1	~0	~0	-
In Initialization experiments, we initialize θA with a network pre-trained on CIFAR-10 (Krizhevsky
& Hinton, 2009). dp increase 7% compared to random initialization, because θA carries over some
information from CIFAR-10, making θA and θB slightly less similar. In Architecture experiments, if
θA uses a different architecture than θB (which is ResNet-56), we also observe increase in dp . The
more different the models are (in terms of number of layers) form ResNet-56, the distance dp is also
slightly higher. This indicates that while dp is largely invariant to model parameterizations, it is also
consistent with intuitive similarities of models. This is not observed with EMD and CKA distances.
3.3	Is information distance faithful?
Having seen the invariance properties of dp , we next check if dp is faithful to reflect true model
distance. Empirically, we verify if dp is aligned with intuitive indicators of model distance. We
experiment with two settings: examining model interpolation and the model training progress. These
can serve as a sanity-check that dp does reflect differences in the information of models.
In model interpolation we use a straightforward way to manipulate the distance between models: we
use two ResNet-56 models θA and θB trained on Tiny-Imagenet, and perform linear interpolation in
parameter space to get model θ = (1 - c) θA + c θB. As c change from 0 to 1, the model θ goes
smoothly from θA to θB . We apply dp to measure the distance from the interpolated model θ to θB
for different c. The results are shown in Figure 2.2
Different task, same initialization
Same task, different initialization
Method
Trend
Identifi-
cation
Param EMD	X	X
Rep EMD	✓	✓
Rep CKA	✓	✓
Out DKL	✓	X
dp	✓	✓
Figure 2: Distance from the interpolation model θ to θB . Left: θA and θB are trained on different
but related tasks (first 100-class and last 100-class of Tiny-ImageNet) from the same initialization.
Middle: θA and θB are trained on the same task but from different initializations. Right: whether the
measure’s trend is correct, and whether it can be used to identify different interpolation coefficients.
As we interpolate two functions fA and fB in the parameter space, if θA and θB are parameterized
similarly, we observe dp(f, fB) to monotonically decrease as f gets closer to fB (Figure 2 left). At
the beginning when c is small, increase in c introduces more “fresh information” about fB, thus dp
2To avoid clutter in graphs, we did not include L2 and cosine measures in Figure 2 and 3, as they fail basic
invariancy tests in Section 3.2. Full results are given in Appendix B.2.
6
Under review as a conference paper at ICLR 2021
decreases faster than later in interpolation. On the other hand, ifwe interpolate two functions that are
parameterized differently, because linear mixing ofθA and θB in parameter space leads to a degraded
network, the distance would first increase then decrease, indicating a loss of information middle in
the interpolation. Overall, the general trend of dp agrees with advanced similarity measures like
representation EMD and CKA in this scenario.
In the experiment training progress, we use dp to examine the training progress of a model. Starting
from θ0, we train the model n epochs to converge and measure the distance from the i-th epoch
model θi to the initial model θ0 as well as to the final model θn . Results are shown in Figure 3.
Training process： distance to start
1750-
1500-
1250-
1000-
750-
500-
250-
0-
1750-
1500-
1250-
, 1000-
750-
500-
250-
0-
-- Param EMD	—Out KL
∙∙-∙ Rep CKA	- dp
Training process: distance to end
epoch
Method	Trend	Identifi- cation
Param EMD	✓	✓
Rep EMD	X	X
Rep CKA	X	X
Out DKL	X	X
dp	✓	✓
Figure 3: Distance from the i-th epoch model θi to the initial model θ0 (left) and the final model θ14
(middle). Right: whether the measure has monotonic trend with respect to the training progress, and
whether it can be used to identify models from different epochs.
With the training progress after each epoch, dp(fi , f0) steadily decreases and dp(fi , fn) increases.
Change in distance is faster in earlier epochs because most of the learning happens in the first few
epochs. In this scenario, dp is monotonic with training and correctly depicts the training progress.
Both representation EMD and CKA fail to show the dynamics of training beyond the first epoch.
To summarize, parameter space distances fail when function similarity does not correspond to pa-
rameter value similarities, and representation space distances can be too noisy to be reliable when
similarity is high. Only information distance dp remains faithful in both scenarios.
4	Application
To illustrate the utility of a universal function distance, we provide a few scenarios where we use dp
for understanding and making predictions.
4.1	Sketching the geometry of data and model space
A distance measure can help us understand the relationship between datasets and between models.
Datasets and models usually live in very high-dimensional spaces, which makes it hard to directly
perform a comparison. Instead, we can use dp to get the information distance between datasets
and models. In computer vision there is a myriad of datasets and model structures, and we use the
Visual Task Adaptation Benchmark (VTAB) (Zhai et al., 2019) as a collection of vision datasets.
On each dataset, a model is trained to represent the input-output function of the task. Then we use
dp to measure pairwise distances between these functions. To help to visualize the relationship, we
use Isometric Mapping (Tenenbaum et al., 2000), a manifold learning algorithm to generate three-
dimensional embeddings for each function. The distance of points in three-dimensional space is
optimized to keep the original structure.
Distances can tell a lot about the relationship between models. In the nine large datasets of VTAB,
datasets cluster largely according to the three categories proposed in VTAB (natural, specialized, and
structured). CIFAR-100 is very different from any other datasets, but is relatively closer to satellite
image datasets than to artificial shape datasets. SVHN (Netzer et al., 2011) is close to 2D shape
datasets. The four small datasets are evenly distributed in space: no pair of them is very similar.
In terms of model architecture, ResNet variants are relatively similar, while AlexNet (Krizhevsky,
2014) and VGG (Simonyan & Zisserman, 2014) are farther away. VGG with batch normalization
7
Under review as a conference paper at ICLR 2021
Figure 4: Visualizing distances between datasets and models in three-dimensional space. Top-left:
large datasets in VTAB. Top-right: small datasets in VTAB. Bottom: various model architectures
trained on ImageNet. The numbers on colored lines are pairwise distances.
is closer to ResNet than without. ResNet-50, ResNeXt-50 (Xie et al., 2017) and WideResNet-50
(Zagoruyko & Komodakis, 2016) are closest as they are indeed very similar.
4.2	Understanding regularizations
Regularization techniques like L2 regularization can bias the learned neural networks toward less
complex functions, while for techniques like dropout (Srivastava et al., 2014) and self-distillation
(Furlanello et al., 2018), the regularization effect may be less straightforward to explain. We can use
dp to examine the (information) complexity of a network f, by measuring its distance dp (f, 0) to an
empty function.
Weight decay	Self-distillation	Label smoothing
Dropout
Figure 5: Distance to an empty function dp(f, 0) for models with different kinds of regularization
and varying strength. From left to right: Weight decay (L2 regularization), self-distillation, label
smoothing, dropout. For dropout, a different base model without batch normalization is used.
From Figure 5, we observe that all the listed techniques result in a reduction of dp(f, 0), which
means that the information complexity of the model function f is reduced. For weight decay, infor-
mation complexity only starts to decrease after the regularization coefficient is larger than a thresh-
old. Self-distillation has a similar effect to regularization, with the number of distillation iterations
controlling regularization strength. This agrees with the theoretical analysis in (Mobahi et al., 2020).
Label smoothing and dropout also result in simpler models, highlighting their regularization effect.
8
Under review as a conference paper at ICLR 2021
Figure 6: Relation between ensemble perfor-
mance and model diversity given by dp(f1, f2).
ι.o-
0.9-
0.8-
0.7-
0.6-
0.5-
0.4-
0.3-
3000	3500	4000	4500
• Generalization gap
arch=LeNet
•	lr=0.0002 -
• a arch=resnet20
bs=128
lr=0.02
• Opt=SGD
• default
∙bs=8
data-aug=on
Distance
Figure 7: Relation between generalization gap
and model complexity dp (f, 0).
4.3	Ensembles and model diversity
Distance can be used as an indicator of model diversity: the larger dp between models, the more
diverse the models are. Ensembling is a common technique to use the consensus of multiple models
to deliver superior performance than a single model. We speculate that a larger model diversity will
result in more performance gain from ensembles.
To verify this connection, we train a number of models on Tiny-Imagenet, all to the same perfor-
mance on the validation set, but with different initializations and different subsets of the training
set. Then we choose models in pairs to measure their ensemble performance as well as the dis-
tance dp(f1 , f2) between them. The result is given in Figure 6, and we found a clear correlation
between dp and ensemble performance, and the relationship is about linear. This also indicates that
dp captures model diversity.
4.4	Predicting generalization
Finally, dp is also linked with model generalization. Generalization of neural networks is heavily
affected by hyper-parameters and optimization. There have been several works aiming to find the
relationship between generalization performance and properties of the network, but it turns out that
predicting generalization gap can be a challenging task (Jiang et al., 2019; 2020).
We perform a small-scale experiment to illustrate the connection between information distance and
generalization gap. We train a number of models with different hyper-parameters (batch size, learn-
ing rate, optimizer, etc.) all to the same loss on the training set, and then measure the distance to a
random model by dp(f, 0). In Figure 7, we observe that the information complexity of the model is
also linked with generalization gap, which also turns out to be a roughly linear relationship. Models
that generalize better are farther away from a random model than less performing models.
5 Discussion and conclusion
The proposed distance dp is based on information distance defined with Komolgorov complexity K.
We do not attempt to give a good estimation of K, but instead relying on the efficiency of prequential
coding, we empirically illustrate that dp share the invariance properties of information distance, and
reflects the similarity relationships of functions parameterized by neural networks. We also found
that dp is linked with behaviors of models, making it a potential tool for analyzing neural networks.
The most notable difference between dp and other similarity metrics is universality. Theoretically
rooted in information distance, dp is independent of parameterization and widely applicable in sit-
uations involving different tasks and models. However, dp ’s utilization of prequential coding also
introduces limitations that it might not work in situations where prequential coding fails, for exam-
ple, when f cannot be efficiently approximated by neural networks.
dp could introduce a potential scale-free, or even parameterization-free geometry of space spanned
by neural models. Optimization with manifold descent by dp could also remove the dependency on
parameterization, thus avoiding ill-posed conditions in some parameterizations (Dinh et al., 2017).
9
Under review as a conference paper at ICLR 2021
References
Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep neural
networks with rectified linear units. In 6th International Conference on Learning Representations,
ICLR 2018, 2018.
Charles Beattie, Joel Z. Leibo, Denis TePlyashin, Tom Ward, Marcus Wainwright, Heinrich Kuttler,
Andrew Lefrancq, Simon Green, Victor Valdes, Amir Sadik, Julian Schrittwieser, Keith Ander-
son, Sarah York, Max Cant, Adam Cain, Adrian Bolton, StePhen Gaffney, Helen King, Demis
Hassabis, Shane Legg, and Stig Petersen. DeePmind lab. CoRR, abs/1612.03801, 2016.
C. H. Bennett, P. Gacs, Ming Li, P. M. B. Vitanyi, and W. H. Zurek. Information distance. IEEE
Transactions on Information Theory, 44(4):1407-1423, 1998.
Leonard Blier and Yann Ollivier. The description length of deep learning models. In Samy Bengio,
Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolo Cesa-Bianchi, and Roman Garnett
(eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural
Information Processing Systems 2018, NeurIPS 2018, pp. 2220-2230, 2018.
Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Bench-
mark and state of the art. Proceedings of the IEEE, 105(10):1865-1883, 2017.
Rudi Cilibrasi and Paul M. B. Vitanyi. The google similarity distance. IEEE Trans. Knowl. Data
Eng., 19(3):370-383, 2007.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribing textures in the wild. In 2014 IEEE Conference on Computer Vision and Pattern Recog-
nition, CVPR 2014, pp. 3606-3613, 2014.
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio. Sharp minima can generalize
for deep nets. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International
Conference on Machine Learning, ICML 2017, volume 70 of Proceedings of Machine Learning
Research, pp. 1019-1028, 2017.
Tommaso Furlanello, Zachary Chase Lipton, Michael Tschannen, Laurent Itti, and Anima Anand-
kumar. Born-again neural networks. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of
the 35th International Conference on Machine Learning, ICML 2018, volume 80 of Proceedings
of Machine Learning Research, pp. 1602-1611, 2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset
and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl.
Earth Obs. Remote. Sens., 12(7):2217-2226, 2019.
Geoffrey E. Hinton and Drew van Camp. Keeping the neural networks simple by minimizing the
description length of the weights. In Lenny Pitt (ed.), Proceedings of the Sixth Annual ACM
Conference on Computational Learning Theory, COLT 1993, pp. 5-13, 1993.
Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger. Densely connected
convolutional networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2017, pp. 2261-2269, 2017.
Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid Ashraf, William J Dally, and Kurt
Keutzer. Squeezenet: Alexnet-level accuracy with 50x fewer parameters and < 0.5 mb model
size. arXiv preprint arXiv:1602.07360, 2016.
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio. Predicting the generalization
gap in deep networks with margin distributions. In 7th International Conference on Learning
Representations, ICLR 2019, 2019.
10
Under review as a conference paper at ICLR 2021
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. Fantastic
generalization measures and where to find them. In 8th International Conference on Learning
Representations, ICLR 2020, 2020.
Simon Kornblith, Mohammad Norouzi, Honglak Lee, and Geoffrey E. Hinton. Similarity of neural
network representations revisited. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Pro-
ceedings of the 36th International Conference on Machine Learning, ICML 2019, volume 97 of
Proceedings ofMachine Learning Research, pp. 3519-3529, 2019.
Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint
arXiv:1404.5997, 2014.
Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Tech-
nical report, 2009.
Yann LeCun, FU Jie Huang, and Leon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. In 2004 IEEE Computer Society Conference on Computer Vision
and Pattern Recognition (CVPR 2004), pp. 97-104, 2004.
Fei-Fei Li, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE Trans.
Pattern Anal. Mach. Intell., 28(4):594-611, 2006.
Xuhong Li, Yves Grandvalet, Remi Flamary, Nicolas Courty, and Dejing Dou. Representation
transfer by optimal transport. arXiv preprint arXiv:2007.06737, 2020.
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dsprites: Disentanglement
testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017.
Hossein Mobahi, Mehrdad Farajtabar, and Peter L Bartlett. Self-distillation amplifies regularization
in hilbert space. arXiv preprint arXiv:2002.05715, 2020.
Gaspard Monge. Memoire sur la theorie des deblais et des remblais. Histoire de YAcademie Royale
des Sciences de Paris, 1781.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng. Reading
digits in natural images with unsupervised feature learning. NIPS Workshop on Deep Learning
and Unsupervised Feature Learning, 2011.
Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large num-
ber of classes. In Sixth Indian Conference on Computer Vision, Graphics & Image Processing,
ICVGIP 2008, pp. 722-729, 2008.
Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In 2012
IEEE Conference on Computer Vision and Pattern Recognition, 2012, pp. 3498-3505, 2012.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should I trust you?”： Explaining
the predictions of any classifier. In Balaji Krishnapuram, Mohak Shah, Alexander J. Smola,
Charu C. Aggarwal, Dou Shen, and Rajeev Rastogi (eds.), Proceedings of the 22nd ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, 2016, pp. 1135-1144, 2016.
Yossi Rubner, Carlo Tomasi, and Leonidas J. Guibas. A metric for distributions with applications
to image databases. In Proceedings of the Sixth International Conference on Computer Vision
(ICCV-98), pp. 59-66, 1998.
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.
ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115(3)：211-252, 2015.
Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.
Mobilenetv2： Inverted residuals and linear bottlenecks. In 2018 IEEE Conference on Computer
Vision and Pattern Recognition, CVPR 2018, pp. 4510-4520, 2018.
11
Under review as a conference paper at ICLR 2021
Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image
recognition. arXiv preprint arXiv:1409.1556, 2014.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research ,15(56):1929-1958,2014.
Joshua B Tenenbaum, Vin De Silva, and John C Langford. A global geometric framework for
nonlinear dimensionality reduction. science, 290(5500):2319-2323, 2000.
Saining Xie, Ross Girshick, Piotr Dollar, ZhUoWen Tu, and Kaiming He. Aggregated residual trans-
formations for deep neural networks. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pp. 1492-1500, 2017.
Sergey Zagoruyko and Nikos Komodakis. Wide residual netWorks. In Richard C. Wilson, EdWin R.
Hancock, and William A. P. Smith (eds.), Proceedings of the British Machine Vision Conference
2016, BMVC 2016, 2016.
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario
Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A
large-scale study of representation learning With the visual task adaptation benchmark. arXiv
preprint arXiv:1910.04867, 2019.
Xian Zhang, Yu Hao, Xiaoyan Zhu, Ming Li, and David R. Cheriton. Information distance from
a question to an ansWer. In Pavel Berkhin, Rich Caruana, and Xindong Wu (eds.), Proceedings
of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,
2007, pp. 874-883, 2007.
Xiao Zhang, Xingjian Li, Dejing Dou, and Ji Wu. Measuring information transfer in neural net-
Works. arXiv preprint arXiv:2009.07624, 2020.
12
Under review as a conference paper at ICLR 2021
A	TECHNICAL DETAILS IN CALCULATING dp
In equations (5)-(6), We used 危 in place of the ensemble model Eyi:— f^. (Xi)
k
EyLk ~fB (xi:k )[Lk (fB IfA)] ≥ X DκL(fB (Xi)||Eyi:i-i f^i (Xi))	(12)
i=1
Vk^	E Ey1 . 1 f^ (Xi) ∖
=EDκL(fB(Xi)IIfi(Xi))+ Ey-fB(Xi) log	yif-1(θi)) '
i=1	∖	f θi (Xli))
(13)
Gi is trained with objective function ckl：
i
cKL(θ) = XDKL(fB(Xj)IIfθ(Xj))	(14)
j=1
i
= X[Hc (fB (Xj), fθ (Xj)) - H(fB(Xj))]	(15)
j=1
ii
=EyLi〜fB(xi:i) X Hc(Uj, fθ(Xj) - X H(fB (Xj))	(16)
i
=Eyi-.i^fB(xi:i)cCE(θ)- X H(fB (Xj))	(17)
j=1
where cCE is the cross-entropy objective function used to train θi . Hc stands for cross-entropy.
In other words, Gi is trained with the average loss used in training θi (the entropy term in (17) does
not depend on θ and has no effect in training). Therefore 危 should mimic the behavior of the
infinite ensemble Eyι-i-ι f^. (Xi) reasonably well and make the second term in (13) small.
Using (6) instead of (3) to estimate codelength not only makes estimations independent of the sam-
pling process, but also requires fewer input examples X. This is because we are making the most use
of each X by essentially drawing infinite y samples from each f (X). Generally speaking, to estimate
dp, one first needs to sample some input X from D (based on data-dependency introduced in Section
2.4). Usually D is unknown, but we have a dataset S containing samples from D so that we can use
examples from S instead. When the size of S is small, there may not be enough examples to train
fA to converge to fB by (3). We found that this is often the case for small datasets, for example in
Section 4.1. Even when S is large, we can save computation time by using a smaller sample size k.
In Section 2.4 we introduced data dependency, where we study function restricted to the data distri-
bution. We can quickly see that if f represents a model trained on data distribution D, then
K(fIf0, D) = K(f0If, D) = 0
where f0 means f restricted to D. Because f is fully determined by f0 . And
K(fB0 IfA0,D) =K(fBIfA,D)
follows from above. We can study K(fB0 IfA0 , D) by coding examples from D. This requires the
functions to be compared (fB and fA) trained on the same kind of input. This is a reasonable re-
striction because it is unlikely one would be concerned about the distance between functions defined
on different input spaces.
13
Under review as a conference paper at ICLR 2021
B Experiment details
B.1	Invariance experiments
We provide details for re-parametreizations used in invariance experiments:
•	Scaling: the weights in layer i is multiplied by c, and the weights in layer i - 1 multiplied
by 1/c. For relu networks, this keeps the output of network unchanged.
•	Neuron swapping: We randomly permute c∙(total number of neurons) neurons in layer i.
We also correspondingly permute the input of layer i + 1 so that the network output is
unchanged.
•	Perturbation: we add Gaussian noise of zero mean and standard deviation c to each indi-
vidual weight of the network.
•	Adversarial perturbation: we add a vector v of standard deviation c to the weight vector
of the network, and optimize the vector to maximize deviations of second-to-last layer
representations. i.e. maxv:std(v)=cEx [(fθr(x) - fθr+v (x))2]
•	Initialization: we experiment with random initialization and initialize with a pre-trained
network on another dataset (CIFAR-10).
•	Architecture: we use ResNet architecture with different width and depth. ResNet-56-s
refers to ResNet-56 with half the width in each layer.
Next we list the baseline distance (or similarity) measures and describe how to calculate them for
network fA and fB :
Parameter space distances: we denote the i-th layer weight matrix of network fA as wAi . For L2 and
cosine measures, we first flatten and concatenate all weight matrices wAi and biases of the network
into a long vector wAall (excluding parameters in batch normalization layers, because some statistic
variable in them can be large and dominate the norm of vector w).
•	L2: dl2 = ||wAall -wBall||2.
•	Cosine: dcosine = WT ∙ wBl/(l∣wA"l∣2∣∣wB"l∣2).
•	EMD: we use “Optimal Transport of Neurons” in (Li et al., 2020). The distance matrix M
is taken to be the pairwise L2 distance between weights of each neuron
Mmn = llwAm∙ - WBn」|2
The EMD distance is the optimal transport cost of matching neurons from one network to
neurons in the other network
demd = min hP, MiF
P ∈∏(μ,ν)	F
where P is the optimal transport plan.
Representation space distances: we sample xi from data distribution D and denote the output repre-
sentation vector of the second-to-last layer of model fA by fAi .
•	L2: dl2 = 1 Pk=I ||fAi - fBi ||2.
•	Cosine: dcosine = 1 Pk=1[fAi ∙ fBi∕(∣∣fAi∣∣2∣∣fBi∣∣2)].
•	EMD: same as in “parameter space distances”, except that the distance matrix M is taken
to be the pairwise L2 distance between the activation vector of each neuron
k
Mmn = (X(fAim - fBin)2)1/2
i=1
•	Linear CKA: we use implementation provided by (Kornblith et al., 2019):
llfB∙T fA∙llF
dcka
llfA∙T fA∙∖∖p llfB∙T fB∙∖∖p
where ∕a. isaa matrix whose k rows are vectors ∕ai,…，fAk.
14
Under review as a conference paper at ICLR 2021
Output space distances: we use the output distributions of fA and fB .
•	KL-divergence: Ex[DKL(fA(x)||fB(x))]
B.2 More results of Section 3.3
Figure 8 and 9 shows the results in model interpolation experiments and training progress experi-
ments, for all distance measures studied in this paper. We also show that if each method gives the
correct trend, and whether it can be used to identify different models.
Different task, same initialization
Same task, different initialization
2000-
1500-
1000-
500-
0-
Method
Trend
Identifi-
cation
Param L2	X	X
Param cosine	X	X
Param EMD	X	X
Rep L2	✓	✓
Rep cosine	✓	X
Rep EMD	✓	✓
Rep CKA	✓	✓
Out DKL	✓	X
dp	✓	✓
Figure 8:	Full results of Figure 2: Distance from the interpolation model θ to θB .
Training process: distance to start
1750-
1500-
一二 J⅛ra<τVEMD-∙-'t7≡rt∙-PSi¾1吊 ^t64加
RePCKA	Rep L2
Training process: distance to end
epoch
Method
Trend
Identifi-
cation
Param L2	✓	✓
Param cosine	✓	✓
Param EMD	✓	✓
Rep L2	X	X
Rep cosine	X	X
Rep EMD	X	X
Rep CKA	X	X
Out DKL	X	X
dp	✓	✓
Figure 9:	Full results of Figure 3: Distance from the i-th epoch model θi to initial model θ0 and final
model θ14.
B.3	Geometry experiments
From the 19 datasets included in VTAB (Zhai et al., 2019), we were able to download 13 datasets
for using in this work. Because the dataset size vary greatly among the 13 datasets, we divide them
into two groups: larger datasets (size > 10000), which include:
•	cifar100: CIFAR-100 (Krizhevsky & Hinton, 2009)
•	svhn: SVHN (Netzer et al., 2011)
•	eurosat: EuroSAT (Helber et al., 2019)
•	resisc45: Resisc45 (Cheng et al., 2017)
•	dsprites-position: dSprites/location (Matthey et al., 2017)
•	dsprites-orientation: dSprites/orientation
•	Smallnorb_azimuth: SmanNORB/azimuth (LeCUn et al., 2004)
•	smallnorb-elevation: SmallNORB/elevation
•	dmlab: DMLab (Beattie et al., 2016)
and smaller datasets (size < 10000), which include:
15
Under review as a conference paper at ICLR 2021
•	caltech101: Caltech101 (Li et al., 2006)
•	dtd: DTD (Cimpoi et al., 2014)
•	oxfordflowers102: FloWers102 (Nilsback & Zisserman, 2008)
•	oxfordΛiit-pet: Pets (Parkhi et al., 2012)
For larger datasets, We use k = 10000 as With other experiments. For smaller datasets, We use
k = 2000. Model is ResNet-56 trained from scratch.
In model geometry experiments, We use the folloWing models trained on ImageNet, as provided by
torchvision3:
•	resnet18: ResNet-18 (He et al., 2016)
•	resnet34: ResNet-34
•	resnet50: ResNet-50
•	vgg11: VGG-11 Without batch normalization (Simonyan & Zisserman, 2014)
•	vgg11-bn: VGG-11 with batch normalization
•	alexnet: AlexNet (Krizhevsky, 2014)
•	resnext50: ResNeXt-50-32x4d (Xie et al., 2017)
•	wide-resnet50: WideResNet-50-2 (ZagorUyko & Komodakis, 2016)
•	densenet121: Densenet-121 (Huang et al., 2017)
•	squeezenet: SqueezeNet 1.1 (Iandola et al., 2016)
•	mobilenet: MobileNet V2 (Sandler et al., 2018)
Codelength is calculated on the training set of ILSVRC 2012 (Russakovsky et al., 2015).
B.4	Ensemble experiments
We train multiple ResNet-56 models with 2 different random initializations and half of the examples
sampled form the TinyImageNet training set. This means the training examples seen by each model
can overlap from 0% to 100%. We then select two models out of this collection and ensemble the
two models. Ensemble performance and distance between the two models measured by dp (fA , fB)
is given in Table 2. We list pairs with same or different initialization, and with different overlap in
training examples. Generally speaking, the less the training examples overlaps the larger the distance
between the models. Different initialization can also makes the models more dissimilar. Note that
from Figure 6, we see that distance dp(fA, fB) is correlated with ensemble performance regardless
of whether diversity comes from difference in training examples or difference in initializations.
Table 2: Details of configurations in ensemble experiments
Initialization seed	Training set overlap	Ensemble loss	Ensemble accuracy	dp(fA,fB)
same	0%	2.197	47.22	2502
same	25%	2.211	46.59	2234
same	50%	2.229	46.40	1946
same	75%	2.229	45.93	1569
different	0%	2.192	47.09	2957
different	25%	2.201	47.09	2481
different	50%	2.214	47.06	2195
different	75%	2.235	45.99	1772
different	100%	2.257	45.75	1436
3https://github.com/pytorch/vision
16
Under review as a conference paper at ICLR 2021
B.5	Generalization experiments
We run experiments on CIFAR-10 with different hyperparameters and model configurations, and in
all configurations we train the model to cross-entropy loss of 0.1 on training set. Then we measure
the generalization gap as the loss on testing set minus the loss on training set. Starting from a default
configuration (which is the same hyperparameters we use in other experiments in this paper), each
time we modify one of the hyperparameters. Results are listed in Table 3. In terms of studying
generalization gap, our experiments is far less thorough than in (Jiang et al., 2020), but here we
would like to illustrate the connection between dp with generalization gap under different experiment
settings, without spending too many machine hours.
Table 3: Details of configurations in generalization experiments.
Configuration	Architecture	Optimizer	Batch size	Learning rate	Data augmentation	Generalization gap	dp(f, 0)
default	ResNet-56	Adam	32	0.002	off	0.32	4605
lr=0.0002	ResNet-56	Adam	32	0.0002	off	0.80	3045
lr=0.02	ResNet-56	Adam	32	0.02	off	0.77	3322
bs=8	ResNet-56	Adam	8	0.002	off	0.50	3908
bs=128	ResNet-56	Adam	128	0.002	off	0.69	3220
opt=SGD	ResNet-56	SGD	32	0.2	off	0.68	3344
arch=resnet20	ResNet-20	Adam	32	0.002	off	0.79	3348
arch=LeNet	LeNet-5	Adam	32	0.002	off	0.84	2873
data_aug=on	ResNet-56	Adam	32	0.002	on	0.62	3615
17