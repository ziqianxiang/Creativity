Under review as a conference paper at ICLR 2021
Temporal Difference Uncertainties
as a Signal for Exploration
Anonymous authors
Paper under double-blind review
Ab stract
An effective approach to exploration in reinforcement learning is to rely on an
agent’s uncertainty over the optimal policy, which can yield near-optimal explo-
ration strategies in tabular settings. However, in non-tabular settings that involve
function approximators, obtaining accurate uncertainty estimates is almost as chal-
lenging as the exploration problem itself. In this paper, we highlight that value
estimates are easily biased and temporally inconsistent. In light of this, we propose
a novel method for estimating uncertainty over the value function that relies on
inducing a distribution over temporal difference errors. This exploration signal
controls for state-action transitions so as to isolate uncertainty in value that is due
to uncertainty over the agent’s parameters. Because our measure of uncertainty con-
ditions on state-action transitions, we cannot act on this measure directly. Instead,
we incorporate it as an intrinsic reward and treat exploration as a separate learning
problem, induced by the agent’s temporal difference uncertainties. We introduce a
distinct exploration policy that learns to collect data with high estimated uncertainty,
which gives rise to a “curriculum” that smoothly changes throughout learning and
vanishes in the limit of perfect value estimates. We evaluate our method on hard-
exploration tasks, including Deep Sea and Atari 2600 environments and find that
our proposed form of exploration facilitates efficient exploration.
1	Introduction
Striking the right balance between exploration and exploitation is fundamental to the reinforcement
learning problem. A common approach is to derive exploration from the policy being learned.
Dithering strategies, such as -greedy exploration, render a reward-maximising policy stochastic
around its reward maximising behaviour (Williams & Peng, 1991). Other methods encourage higher
entropy in the policy (Ziebart et al., 2008), introduce an intrinsic reward (Singh et al., 2005), or drive
exploration by sampling from the agent’s belief over the MDP (Strens, 2000).
While greedy or entropy-maximising policies cannot facilitate temporally extended exploration
(Osband et al., 2013; 2016a), the efficacy of intrinsic rewards depends crucially on how they relate
to the extrinsic reward that comes from the environment (Burda et al., 2018a). Typically, intrinsic
rewards for exploration provide a bonus for visiting novel states (e.g Bellemare et al., 2016) or
visiting states where the agent cannot predict future transitions (e.g Pathak et al., 2017; Burda et al.,
2018a). Such approaches can facilitate learning an optimal policy, but they can also fail entirely in
large environments as they prioritise novelty over rewards (Burda et al., 2018b).
Methods based on the agent’s uncertainty over the optimal policy explicitly trade off exploration and
exploitation (Kearns & Singh, 2002). Posterior Sampling for Reinforcement Learning (PSRL; Strens,
2000; Osband et al., 2013) is one such approach, which models a distribution over Markov Decision
Processes (MDPs). While PSRL is near-optimal in tabular settings (Osband et al., 2013; 2016b), it
cannot be easily scaled to complex problems that require function approximators. Prior work has
attempted to overcome this by instead directly estimating the agent’s uncertainty over the policy’s
value function (Osband et al., 2016a; Moerland et al., 2017; Osband et al., 2019; O’Donoghue et al.,
2018; Janz et al., 2019). While these approaches can scale posterior sampling to complex problems
and nonlinear function approximators, estimating uncertainty over value functions introduces issues
that can cause a bias in the posterior distribution (Janz et al., 2019).
1
Under review as a conference paper at ICLR 2021
In response to these challenges, we introduce Temporal Difference Uncertainties (TDU), which de-
rives an intrinsic reward from the agent’s uncertainty over the value function. Concretely, TDU relies
on the Bootstrapped DQN (Osband et al., 2016a) and separates exploration and reward-maximising
behaviour into two separate policies that bootstrap from a shared replay buffer. This separation
allows us to derive an exploration signal for the exploratory policy from estimates of uncertainty of
the reward-maximising policy. Thus, TDU encourages exploration to collect data with high model
uncertainty over reward-maximising behaviour, which is made possible by treating exploration as a
separate learning problem. In contrast to prior works that directly estimate value function uncertainty,
we estimate uncertainty over temporal difference (TD) errors. By conditioning on observed state-
action transitions, TDU controls for environment uncertainty and provides an exploration signal only
insofar as there is model uncertainty. We demonstrate that TDU can facilitate efficient exploration in
challenging exploration problems such as Deep Sea and Montezuma’s Revenge.
2	Estimating Value Function Uncertainty is Hard
We begin by highlighting that estimating uncertainty over the value function can suffer from bias that
is very hard to overcome With typical approaches (see also Janz et al., 2019). Our analysis shoWs that
biased estimates arise because uncertainty estimates require an integration over unknoWn future state
visitations. This requires tremendous model capacity and is in general infeasible. Our results shoW
that We cannot escape a bias in general, but We can take steps to mitigate it by conditioning on an
observed trajectory. Doing so removes some uncertainty over future state-visitations and We shoW in
Section 3 that it can result in a substantially smaller bias.
We consider a Markov Decision Process (S, A, P, R, γ) for some given state space (S), action space
(A), transition dynamics (P), reWard function (R) and discount factor (γ). For a given (deterministic)
policy π : S 7→ A, the action value function is defined as the expected cumulative reWard under the
policy starting from state s With action a:
∞
Qn (s, a) := En X γ rt+1
t=0
s0 = s, a0 = a
Er〜R(s,a) [r + YQn(s0, ∏(s0))] ,	(1)
s0~P(s,a)
where t index time and the expectation Eπ is with respect to realised rewards r sampled under the
policy π; the right-hand side characterises Q recursively under the Bellman equation. The action-
value function Qπ is estimated under a function approximator Qθ parameterised by θ. Uncertainty
over Qπ is expressed by placing a distribution over the parameters of the function approximator, p(θ).
We overload notation slightly and write p(θ) to denote the probability density function pθ over a
random variable θ. Further, We denote by θ 〜p(θ) a random sample θ from the distribution defined
bypθ. Methods that rely on posterior sampling under function approximators assume that the induced
distribution, p(Qθ), is an accurate estimate of the agent’s uncertainty over its value function, p(Qπ),
so that sampling Qθ 〜p(Qθ) is approximately equivalent to sampling from Qn 〜P(Qn).
For this to hold, the moments ofp(Qθ) at each state-action pair (s, a) must correspond to the expected
moments in future states. In particular, moments of p(Qn) must satisfy a Bellman Equation akin to
Eq. 1 (O’Donoghue et al., 2018). We focus on the mean (E) and variance (V):
Eθ[Qθ(s,a)] = Eθ [Er,s0 [r + γQθ(s0, π(s0))]] ,	(2)
Vθ [Qθ(s, a)] = Vθ [Er,s0 [r + γQθ(s0, π(s0))]].	(3)
If Eθ [Qθ] and	Vθ [Qθ] fail to satisfy these conditions, the estimates of E[Qn]	and V[Qn]	are	biased,
causing a bias	in exploration under posterior sampling from p(Qθ). Formally,	the agent’s	uncertainty
over p(Q) implies uncertainty over the MDP (Strens, 2000). Given a belief over the MDP, i.e., a
distribution P(M), we can associate each M 〜P(M) with a distinct value function QM. Lemma 1
beloW shoWs that, for p(θ) to be interpreted as representing some p(M) by push-forWard to p(Qθ),
the induced moments must match under the Bellman Equation.
Lemma 1. If Eθ[Qθ] and Vθ[Qθ] fail to satisfy Eqs. 2 and 3, respectively, they are biased estimators
of EM QnM and VM QnM for any choice ofP(M).
2
Under review as a conference paper at ICLR 2021
All proofs are deferred to Appendix B. Lemma 1 highlights why estimating uncertainty over value
functions is so challenging; while the left-hand sides of Eqs. 2 and 3 are stochastic in θ only, the
right-hand sides depend on marginalising over the MDP. This requires the function approximator to
generalise to unseen future trajectories. Lemma 1 is therefore a statement about scale; the harder it is
to generalise, the more likely we are to observe a bias—even in deterministic environments.
This requirement of “strong generalisation” poses a particular problem for neural networks that tend
to interpolate over the training data (e.g. Li et al., 2020; Liu et al., 2020; Belkin et al., 2019), but the
issue is more general. In particular, we show that factorising the posterior p(θ) will typically cause
estimation bias for all but tabular MDPs. This is problematic because it is often computationally
infeasible to maintain a full posterior; previous work either maintains a full posterior over the final
layer of the function approximator (Osband et al., 2016a; O’Donoghue et al., 2018; Janz et al., 2019)
or maintains a diagonal posterior over all parameters (Fortunato et al., 2018; Plappert et al., 2018)
of the neural network. Either method limits how expressive the function approximator can be with
respect to future states, thereby causing an estimation bias. To establish this formally, let Qθ := W◦。仇
where θ = (wι,...,Wn ,Hι,...,"v), with W ∈ Rn a linear projection and φ : S×A→ Rn a feature
extractor with parameters H ∈ Rv.
Proposition 1. If the number of state-action pairs where Eθ [Qθ (s, a)] 6= Eθ[Qθ(s0, a0)] is greater
than n, where W ∈ Rn, then Eθ[Qθ] and Vθ[Qθ] are biased estimators of EM QπM and VM QπM
for any choice of p(M).
This result is a consequence of the feature extractor ψ mapping into a co-domain that is larger than the
space spanned by W; a bias results from having more unique state-action representations ψ(s, a) than
degrees of freedom in W. The implication is that function approximators under factorised posteriors
cannot generalise uncertainty estimates across states (a similar observation in tabular settings was
made by Janz et al., 2019)—they can only produce temporally consistent uncertainty estimates if
they have the capacity to memorise point-wise uncertainty estimates for each (s, a), which defeats
the purpose of a function approximator. This is a statement about the structure of p(θ) and holds for
any estimation method. Thus, common approaches to uncertainty estimation with neural networks
generally fail to provide unbiased uncertainty estimates over the value function in non-trivial MDPs.
Proposition 1 shows that to accurately capture value function uncertainty, we need a full posterior over
parameters, which is often infeasible. It also underscores that the main issue is the dependence on
future state visitation. This motivates Temporal Difference Uncertainties as an estimate of uncertainty
conditioned on observed state-action transitions.
3	Temporal Difference Uncertainties
While Proposition 1 states that we cannot remove this bias unless we are willing to maintain a
full posterior p(θ), we can construct uncertainty estimates that control for uncertainty over future
state-action transition. In this paper, we propose to estimate uncertainty over a full transition
τ := (s, a, r, s0) to isolate uncertainty due to p(θ). Fixing a transition, we induce a conditional
distribution p(δ | τ) over Temporal Difference (TD) errors, δ(θ, τ) := γQθ(s0, π(s0)) +r - Qθ(s, a),
that we characterise by its mean and variance:
Eδ[δ | τ] =Eθ[δ(θ,τ) | τ] and	Vδ[δ | τ] =Vθ[δ(θ,τ) | τ].	(4)
Estimators over TD-errors is akin to first-difference estimators of uncertainty over the action-value.
They can therefore exhibit smaller bias if that bias is temporally consistent. To illustrate, for
simplicity assume that Eθ[Qθ] consistently over/under-estimates EM QπM by an amount b ∈ R. The
corresponding bias in Eθ[δ(θ, τ) | τ] is given by Bias(Eθ [δ(θ, τ) | τ]) = Bias(γEθ[Qθ(s0, π(s0))] +
r - Eθ[Qθ(s, a)]) = (Y - 1)b. This bias is close to 0 for typical values of Y—notably for Y = 1,
Eθ [δ(θ, τ) | τ] is unbiased. More generally, unless the bias is constant over time as in the above
example, we cannot fully remove the bias when constructing an estimator over a quantity that relies
on Qθ . However, as the above example shows, by conditioning on a state-action transition, we can
make it significantly smaller. We formalise this logic in the following result.
3
Under review as a conference paper at ICLR 2021
Proposition 2. For any τ := (s, a, r, s0) and any p(M), given p(θ), define the following ratios:
ρ = Bias (Eθ[Qθ(s0, π(s0))]) / Bias (Eθ[Qθ(s,a)])	(5)
φ = Bias(Eθ[Qθ(s0,π(s0))2]) / Bias (Eθ [Qθ(s,a)2])	(6)
K = Bias (Eθ[Qθ(s0, ∏(s0))Qθ(s, a)]) /Bias(Eθ [Qθ(s, a)2])	(7)
α=EM[QπM(s0,π(s0)) / EM [QπM (s, a) .	(8)
If P ∈	(0, 2∕γ),	then	Eδ [δ	|	T]	has lower bias than	Eθ [Qθ(s,a)].	Moreover, if P = 1∕γ,	then
Eδ[δ | τ] is unbiased. Additionally, there exists ρ ≈ 1, φ ≈ 1, κ ≈ 1, α ≈ 1 such that Vθ [δ(θ, τ) | τ]
have less bias than Vθ [Qθ (s, a)]. In particular, if P = φ = κ = α = 1, then
| Bias(Vθ [δ(θ,τ) | T ])| = ∣(γ - 1)2 Bias(Vθ [Qθ (s,a)])∣ < IBias(Vθ [Qθ (s,a)])∣.	(9)
Further, P = 1∕γ, K = 1∕γ, φ = 1∕γ2, then Vθ [δ(θ, τ) ∣ τ] is unbiasedfor any a.
The first part of Proposition 2 generalises the example above to cases where the bias b varies across
action-state transitions. It is worth noting that the required “smoothness” on the bias is not very
stringent: the bias of Eθ[Qθ] (s0, π(s0)) can be twice as large as that of Eθ[Qθ] (s, a) and Eδ[δ I τ]
can still produce a less biased estimate. Importantly, it must have the same sign, and so Proposition 2
requires temporal consistency. To establish a similar claim for Vδ[δ I τ], we need a bit more structure.
The ratios P, φ, and K capture temporal consistency in the bias, while α relates to the temporal
consistency of the underlying estimand. Proposition 2 establishes that if these ratios are close to unity,
then Vθ[δ(θ, τ) I τ] will have less bias. For most transitions, it is reasonable to assume that this holds
true. In some MDPs, large changes in the reward can cause these requirements to break. Because
Proposition 2 only establishes sufficiency, violating this requirement does not necessarily mean that
Vδ[δ I τ] has greater bias than Vθ[Qθ(s, a)]. Finally, it is worth noting that these are statements about
a given transition τ. In most state-action transitions, the requirements in Proposition 2 will hold, in
which case Eδ[δ I τ] and Vδ[δ I τ] exhibit less overall bias. We provide direct empirical support that
Proposition 2 holds in practice through careful ceteris paribus comparisons in Section 5.1.
To obtain a concrete signal for exploration, we follow O’Donoghue et al. (2018) and derive an
exploration signal from the variance Vθ[δ(θ, τ)Iτ]. Because p(δ I τ) is defined per transition, it
cannot be used as-is for posterior sampling. Therefore, we incorporate TDU as a signal for exploration
via an intrinsic reward. To obtain an exploration signal that is on approximately the same scale as the
extrinsic reward, We use the standard deviation σ(τ) := a∕Vθ [δ(θ,τ) ∣ τ] to define an augmented
reward function
~ , _ ,, . .	. ,
R(T) := R((s, a) ∈ T) + β σ(τ),	(10)
where β ∈ [0, ∞) is a hyper-parameter that determines the emphasis on exploration. Another
appealing property of σ is that it naturally decays as the agent converges on a solution (as model
uncertainty diminishes); TDU defines a distinct MDP (S, A, P, R, γ) under Eq. 10 that converges
on the true MDP in the limit of no model uncertainty. For a given policy π and distribution p(Qθ),
there exists an exploration policy μ that collects transitions over which p(Qθ) exhibits maximal
uncertainty, as measured by σ. In hard exploration problems, the exploration policy μ can behave
fundamentally differently from ∏. To capture such distinct exploration behaviour, we treat μ as a
separate exploration policy that we train to maximise the augmented reward R, along-side training a
policy π that maximises the extrinsic reward R. This gives rise to a natural separation of exploitation
and exploration in the form of a cooperative multi-agent game, where the exploration policy is tasked
with finding experiences where the agent is uncertain of its value estimate for the greedy policy π .
As π is trained on this data, we expect uncertainty to vanish (up to noise). As this happens, the
exploration policy μ is incentivised to find new experiences with higher estimated uncertainty. This
induces a particular pattern where exploration will reinforce experiences until the agent’s uncertainty
vanishes, at which point the exploration policy expands its state visitation further. This process
can allow TDU to overcome estimation bias in the posterior—since it is in effect exploiting it—in
contrast to previous methods that do not maintain a distinct exploration policy. We demonstrate this
empirically both on Montezuma’s Revenge and on Deep Sea (Osband et al., 2020).
4
Under review as a conference paper at ICLR 2021
4	Implementing TDU with B ootstrapping
The distribution over TD-errors that underlies TDU can be estimated using standard techniques for
probability density estimation. In this paper, we leverage the statistical bootstrap as it is both easy to
implement and provides a robust approximation without requiring distributional assumptions. TDU
is easy to implement under the statistical bootstrap—it requires only a few lines of extra code. It
can be implemented with value-based as well as actor-critic algorithms (we provide generic pseudo
code in Appendix A); in this paper, we focus on Q-learning. Q-learning alternates between policy
evaluation (Eq. 1) and policy improvement under a greedy policy πθ(s) = arg max a Qθ(s, a). Deep
Q-learning (Mnih et al., 2015) learns Qθ by minimising its TD-error by stochastic gradient descent
on transitions sampled from a replay buffer. Unless otherwise stated, in practice we adopt a common
approach of evaluating the action taken by the learned network through a target network with separate
parameters that are updated periodically (Van Hasselt et al., 2016).
Our implementation starts from the bootstrapped DQN (Osband et al., 2016a), which maintains a
set of K function approximators Q = {Qθk }kK=1, each parameterised by θk and regressed towards
a unique target function using bootstrapped sampling of data from a shared replay memory. The
Bootstrapped DQN derives a policy πθ by sampling θ uniformly from Q at the start of each episode.
We provide an overview of the Bootstrapped DQN in Algorithm 1 for reference. To implement TDU
in this setting, we make a change to the loss function (Algorithm 2, changes highlighted in green).
First, we estimate the TDU signal σ using bootstrapped value estimation. We estimate σ through
observed TD-errors {δk }kK=1 incurred by the ensemble Q on a given transition:
σ(τ) ≈
1K
K-I](δ(θk,τ) - δ(τ))2,
(11)
∖
where S = YQ0 + r - Q, with X := K PK=I Xi and Q0 := Q(s0, π(s0)). An important assumption
underpinning the bootstrapped estimation is that of stochastic optimism (Osband et al., 2016b), which
requires the distribution over Q to be approximately as wide as the true distribution over value
estimates. If not, uncertainty over Q can collapse, which would cause σ to also collapse. To prevent
this, Q can be endowed with a prior (Osband et al., 2018) that maintains diversity in the ensemble by
defining each value function as Qθk + λPk, λ ∈ [0, ∞), where Pk is a random prior function.
Rather than feeding this exploration signal back into the value functions in Q, which would create
a positive feedback loop (uncertainty begets higher reward, which begets higher uncertainty ad-
infinitum), we introduce a separate ensemble of exploration value functions Q = {Qρk }N11 that we
train over the augmented reward (Eqs. 10 and 11). We derive an exploration policy μ^ by sampling
exploration parameters θ uniformly from Q, as in the standard bootstrapped DQN.
In summary, our implementation of TDU maintains K + N value functions. The first K defines a
standard Bootstrapped DQN. From these, we derive an exploration signal σ, which we use to train the
last N value functions. At the start of each episode, we proceed as in the standard Bootstrapped DQN
and randomly sample a parameterisation θ from Q ∪Q that we act under for the duration of the episode.
All value functions are trained by bootstrapping from a single shared replay memory (Algorithm 1);
see Appendix A for a complete JAX (Bradbury et al., 2018) implementation. Consequently, we
execute the (extrinsic) reward-maximising policy ∏θ~q with probability k∕(k+n) and the exploration
policy μ9~Q with probability N/(k+n). While π visits states around current reward-maximising
behaviour, μ searches for data with high model uncertainty. While each population Q and Q can
be seen as performing Bayesian inference, it is not immediately clear that the full agent admits a
Bayesian interpretation. We leave this question for future work.
There are several equally valid implementations of TDU (see Appendix A for generic implementations
for value-based learning and policy-gradient methods). In our case, it would be equally valid to
define only a single exploration policy (i.e. N = 1) and specify the probability of sampling this
policy. While this can result in faster learning, a potential drawback is that it restricts the exploratory
behaviour that μ can exhibit at any given time. Using a full bootstrapped ensemble for the exploration
policy leverages the behavioural diversity of bootstrapping.
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Bootstrapped DQN with TDU Algorithm 2 Bootstrapped TD-loss with TDU.
Require: M, L: MDP to solve, TDU loss
Require: β, K, N, ρ: hyper-parameters
1:	Initialise B : replay buffer
2:	Initialise K + N value functions, Q ∪Q
3:	while not done do
4:	Observe S and choose Qk 〜Q ∪Q
5:	while episode not done do
6:	Take action a = arg max ^ Qk(s, ^)
7:	Sample mask m, m% 〜Bin(n =1,p =P)
8:	Enqueue transition (s, a, r, s0, m) to B
9:	Optimise L({θk}K, {θik}N,γ, β, D〜B)
10:	end while
11:	end while
Require: {θk}f, {石k }N: parameters
Require: γ, β , D: hyper-parameters, data
1:	Initialise ' - 0
2:	for s, a, r, s0, m ∈ D do
3:	T J (s, a, r, S, Y)
4:	ComPUte {δi}K=ι = {δ(θi,τ)}K=ι
5:	Compute σ from {δk}K=ι (Eq. 11)
6:	Update T by r — r + β σ
7:	ComPUte {&}N=κ+ι = {δ(θ,τ)}N=κ+ι
8:	' J ' + PK=I miδ2 + PN=I mκ+j%
9:	end for
10:	return: ` / (2(N + K)| D |)
5	Empirical Evaluation
5.1	Behaviour Suite
Bsuite (Osband et al., 2020) was introduced as a benchmark for characterising core capabilities of
RL agents. We focus on a Deep Sea, which is explicitly designed to test for deep exploration. It
is a challenging exploration problem where only one out of 2N policies yields any positive reward.
Performance is compared on instances of the environment with grid sizes N ∈ {10, 12, . . . , 50},
with an overall “score” that is the percentage of N for which average regret goes to below 0.9 faster
than 2N. The stochastic version generates a ‘bad’ transition with probability 1/N. This is a relatively
high degree of uncertainty since the agent cannot recover from a bad transition in an episode.
For all experiments, we use a standard MLP with Q-learning, off-policy replay and a separate target
network. See Appendix D for details and TDU results on the full suite. We compare TDU on Deep
Sea to a battery of exploration methods, broadly divided into methods that facilitate exploration by
(a) sampling from a posterior (Bootstrapped DQN, Noisy Nets (Fortunato et al., 2018), Successor
Uncertainties (Janz et al., 2019)) or (b) use an intrinsic reward (Random Network Distillation (RND;
Burda et al., 2018b), CTS (Bellemare et al., 2016), and Q-Explore (QEX; Simmons-Edler et al.,
2019)). We report best scores obtained from a hyper-parameter sweep for each method. Overall,
performance varies substantially between methods; only TDU performs (near-)optimally on both
the deterministic and stochastic version. Methods that rely on posterior sampling do well on the
deterministic version, but suffer a substantial drop in performance on the stochastic version. As
the stochastic version serves to increase the complexity of modelling future state visitation, this
is clear evidence that these methods suffer from the estimation bias identified in Section 2. We
could not make Q-explore and NoisyNets perform well in the default Bsuite setup, while Successor
Uncertainties suffers a catastrophic loss of performance on the stochastic version of DeepSea.
Examining TDU, we find that it facilitates exploration while retaining overall performance except
on Mountain Car where β > 0 hurts performance (Appendix D). For Deep Sea (Figure 2), prior
functions are instrumental, even for large exploration bonuses (β 0). However, for a given prior
strength, TDU does better than the BDQN (β = 0). In the stochastic version of Deep Sea, BDQN
suffers a significant loss of performance (Figure 2). As this is a ceteris paribus comparison, this
performance difference can be directly attributed to an estimation bias in the BDQN that TDU
circumvents through its intrinsic reward. That TDU is able to facilitate efficient exploration despite
environment stochasticity demonstrates that it can correct for such estimation errors.
Finally, we verify Proposition 2 experimentally. We compare TDU to versions that estimate uncer-
tainty directly over Q (full analysis in Appendix D.2). We compare TDU to (a) a version where σ is
defined as standard deviation over Q and (b) where σ(Q) is used as an upper confidence bound in the
policy instead of as an intrinsic reward (Figure 2). Neither matches TDU’s performance across Bsuite
an in particular on Deep Sea. Being ceteris paribus comparisons, this demonstrates that estimating
uncertainty over TD-errors provides a stronger signal for exploration, as per Proposition 2.
6
Under review as a conference paper at ICLR 2021
Deep Sea Deterministic
Figure 1: Deep Sea Benchmark. QEX, CTS, and RND use intrinsic rewards; BDQN, SU, and NNS
use posterior sampling (Section 5.1). Posterior sampling does well on the deterministic version, but
struggles on the stochastic version, suggesting an estimation bias (Section 2). Only TDU performs
(near-)optimally on both the deterministic and the stochastic version of Deep Sea.
Deep Sea Stochastic
Figure 2: Deep Sea results. All models solve the deterministic version for prior scale λ = 3 (dashed
line). TDU also solves it for λ = 1. Left: introducing stochasticity substantially deteriorates baseline
performance; including TDU (β > 0) recovers close to full performance. Center left: effect of
varying λ, TDU benefits from diversity in Q estimates. Center right: effect of removing prior (λ=0).
Increasing β improves exploration, but does not reach full performance. Right: Qa replaces σ(δ) with
σ(Q), Qb acts by argmaxa(Q + σ(Q))(s, a). Estimating uncertainty over Q fails to match TDU.
5.2	Atari
Proposition 1 shows that estimation bias is particularly likely in complex environments that require
neural networks to generalise across states. In recent years, such domains have seen significant
improvements from running on distributed training platforms that can process large amounts of
experience obtained through agent parallelism. It is thus important to develop exploration algorithms
that scale gracefully and can leverage the benefits of distributed training. Therefore, we evaluate
whether TDU can have a positive impact when combined with the Recurrent Replay Distributed
DQN (R2D2) (Kapturowski et al., 2018), which achieves state-of-the-art results on the Atari2600
suite by carefully combining a set of key components: a recurrent state, experience replay, off-policy
value learning and distributed training.
As a baseline we implemented a distributed version of the bootstrapped DQN with additive prior
functions. We present full implementation details, hyper-parameter choices, and results on all
games in Appendix E. For our main results, we run each agent on 8 seeds for 20 billion steps. We
focus on games that are well-known to pose challenging exploration problems (Machado et al.,
2018): montezuma_revenge, pitfall, private_eye, solaris, venture, gravitar,
and tennis. Following standard practice, Figure 3 reports Human Normalized Score (HNS),
HNS = Amancore-RRndmmcore , as an aggregate result across exploration games as well as results on
montezuma_revenge and tennis, which are both known to be particularly hard exploration
games (Machado et al., 2018).
7
Under review as a conference paper at ICLR 2021
Environment steps 1e10	Environment steps 1e10	Environment steps 1e10
Figure 3: Atari results with distributed training. We compare TDU with and without additive prior
functions to R2D2 and Bootstrapped R2D2 (B-R2D2). Left: Results for montezuma_revenge.
Center: Results for tennis. Right: Mean HNS for the hard exploration games in the Atari2600
suite (including tennis). Shading depicts standard deviation over 8 seeds.
Generally, we find that TDU facilitates exploration substantially, improving the mean HNS score
across exploration games by 30% compared to baselines (right panel, Figure 3). An ANOVA
analysis yields a statistically significant difference between TDU and non-TDU methods, control-
ling for game (F = 8.17, p = 0.0045). Notably, TDU achieves significantly higher returns on
montezuma_revenge and is the only agent that consistently achieves the maximal return on
tennis. We report all per-game results in Appendix E.4. We observe no significant gains from
including prior functions with TDU and find that bootstrapping alone produces relatively marginal
gains. Beyond exploration games, TDU can match or improve upon the baseline, but exhibits sensi-
tivity to TDU hyper-parameters (β, number of explorers (N); see Appendix E.3 for details). This
finding is in line with observations made by (PUigdomenech Badia et al., 2020); combining TDU
with online hyper-parameter adaptation (Schaul et al., 2019; Xu et al., 2018; Zahavy et al., 2020) are
exciting avenUes for fUtUre research. See Appendix E for fUrther comparisons.
In Table 1, we compare TDU to recently proposed state-of-the-art exploration methods. While
comparisons mUst be made with care dUe to different training regimes, compUtational bUdgets, and
architectUres, we note a general trend that no method is Uniformly sUperior. Methods that are good
on extremely sparse exploration games (montezuma_ revenge and pitfall!) tend to do
poorly on games with dense rewards and vice versa. TDU is generally among the top 2 algorithms
in all cases except on montezuma_revenge and pitfall!, state-based exploration is needed
to achieve sUfficient coverage of the MDP. TDU generally oUtperforms both Pixel-CNN (Ostrovski
et al., 2017), CTS, and RND. TDU is the only algorithm to achieve sUper-hUman performance on
solaris and achieves the highest score of all baselines considered on venture.
6	Related Work
Bayesian approaches to exploration typically Use Uncertainty as the mechanism for balancing ex-
ploitation and exploration (Strens, 2000). A popUlar instance of this form of exploration is the PILCO
algorithm (Deisenroth & RasmUssen, 2011). While we rely on the bootstrapped DQN (Osband et al.,
2016a) in this paper, several other Uncertainty estimation techniqUes have been proposed, sUch as by
placing a parameterised distribUtion over model parameters (FortUnato et al., 2018; Plappert et al.,
2018) or by modeling a distribUtion over both the valUe and the retUrns (Moerland et al., 2017), Using
Bayesian linear regression on the valUe fUnction (Azizzadenesheli et al., 2018; Janz et al., 2019), or
by modelling the variance over valUe estimates as a Bellman operation (O’DonoghUe et al., 2018).
The Underlying exploration mechanism in these works is posterior sampling from the agent’s cUrrent
beliefs (Thompson, 1933; Dearden et al., 1998); oUr work sUggests that estimating this posterior is
significantly more challenging that previoUsly thoUght.
An alternative to posterior sampling is to facilitate exploration via learning by introdUcing an
intrinsic reward fUnction. PrevioUs works typically formUlate intrinsic rewards in terms of state
8
Under review as a conference paper at ICLR 2021
Table 1: Atari benchmark on exploration gamesJOstroVski et al. (2017), ^Bellemare et al. (2016),
◊Burda et al. (2018b), ?Choi et al. (2018), §Puigdomenech Badia et al. (2020), +With prior functions.
Algorithm	Gravitar	Montezuma’s Revenge	Pitfall!	Private Eye	Solaris	Venture
Avg. Human	3,351	4,753	6,464	69,571	12,327	1,188
R2D2	15,680	2,061	0.0	5,322.7	3,787.2	1,970.7
DQN- PiXeICNNt	859.1	2,514	0.0	15,806.5	5,501.5	1,356.3
DQN-CTSt	498.3	3,706	0.0	8,358.7	82.2	—
RND°	3,906	10,070	-3	8,666	3,282	1,859
CoEx?	—	11,618	—	11,000	—	1,916
NGU§	14,100	10,400	8,400	100,000	4,900	1,700
TDU-R2D2	13,000	5,233	0	40,544	14,712	2,000
TDU-R2D2+	10,916	2,833	0	61,168	15,230	1,977
visitation (Lopes et al., 2012; Bellemare et al., 2016; Puigdomenech Badia et al., 2020), state novelty
(Schmidhuber, 1991; Oudeyer & Kaplan, 2009; Pathak et al., 2017), or state predictability (Florensa
et al., 2017; Burda et al., 2018b; Gregor et al., 2016; Hausman et al., 2018). Most of these works
rely on properties of the state space to drive exploration while ignoring rewards. While this can be
effective in sparse reward settings (e.g. Burda et al., 2018b; Puigdomenech Badia et al., 2020), it can
also lead to arbitrarily bad exploration (see analysis in Osband et al., 2019).
A smaller body of work uses statistics derived from observed rewards (Nachum et al., 2016) or
TD-errors to design intrinsic reward functions; our work is particularly related to the latter. Tokic
(2010) proposes an extension of -greedy exploration, where the TD-error modulates to be higher in
states with higher TD-error. Gehring & Precup (2013) use the mean absolute TD-error, accumulated
over time, to measure controllability of a state and reward the agent for visiting states with low mean
absolute TD-error. In contrast to our work, this method integrates the TD-error over time to obtain
a measure of irreducibility. Simmons-Edler et al. (2019) propose to use two Q-networks, where
one is trained on data collected under both networks and the other obtains an intrinsic reward equal
to the absolute TD-error of the first network on a given transition. In contrast to our work, this
method does not have a probabilistic interpretation and thus does not control for uncertainty over
the environment. TD-errors have also been used in White et al. (2015), where surprise is defined in
terms of the moving average of the TD-error over the full variance of the TD-error. Kumaraswamy
et al. (2018) rely on least-squares TD-errors to derive a context-dependent upper-confidence bound
for directed exploration. Finally, using the TD-error as an exploration signal is related to the notion
of “learnability” or curiosity as a signal for exploration, which is often modelled in terms of the
prediction error in a dynamics model (e.g. Schmidhuber, 1991; Oudeyer et al., 2007; Gordon &
Ahissar, 2011; Pathak et al., 2017).
7	Conclusion
We present Temporal Difference Uncertainties (TDU), a method for estimating uncertainty over an
agent’s value function. Obtaining well-calibrated uncertainty estimates under function approximation
is non-trivial and we show that popular approaches, while in principle valid, can fail to accurately
represent uncertainty over the value function because they must represent an unknown future.
This motivates TDU as an estimate of uncertainty conditioned on observed state-action transitions,
so that the only source of uncertainty for a given transition is due to uncertainty over the agent’s
parameters. This gives rise to an intrinsic reward that encodes the agent’s model uncertainty, and we
capitalise on this signal by introducing a distinct exploration policy. This policy is incentivised to col-
lect data over which the agent has high model uncertainty and we highlight how this separation gives
rise to a form of cooperative multi-agent game. We demonstrate empirically that TDU can facilitate
efficient exploration in hard exploration games such as Deep Sea and Montezuma’s Revenge.
9
Under review as a conference paper at ICLR 2021
References
Azizzadenesheli, K., Brunskill, E., and Anandkumar, A. Efficient exploration through Bayesian deep
Q-Networks. arXiv preprint arXiv:1802.04412, 2018.
Belkin, M., Hsu, D., and Xu, J. Two models of double descent for weak features. arXiv preprint
arXiv:1903.07571, 2019.
Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying
count-based exploration and intrinsic motivation. In Advances in Neural Information Processing
Systems, 2016.
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M. J., Leary, C., Maclaurin, D., and Wanderman-
Milne, S. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.
Burda, Y., Edwards, H., Pathak, D., Storkey, A. J., Darrell, T., and Efros, A. A. Large-scale study of
curiosity-driven learning. CoRR, abs/1808.04355, 2018a.
Burda, Y., Edwards, H., Storkey, A. J., and Klimov, O. Exploration by random network distillation.
arXiv preprint arXiv:1810.12894, 2018b.
Choi, J., Guo, Y., Moczulski, M., Oh, J., Wu, N., Norouzi, M., and Lee, H. Contingency-aware
exploration in reinforcement learning. arXiv preprint arXiv:1811.01483, 2018.
Dearden, R., Friedman, N., and Russell, S. Bayesian Q-learning. In Association for the Advancement
of Artificial Intelligence, 1998.
Deisenroth, M. and Rasmussen, C. E. Pilco: A model-based and data-efficient approach to policy
search. In International Conference on Machine Learning, 2011.
Florensa, C., Duan, Y., and Abbeel, P. Stochastic neural networks for hierarchical reinforcement
learning. In International Conference on Learning Representations, 2017.
Fortunato, M., Azar, M. G., Piot, B., Menick, J., Osband, I., Graves, A., Mnih, V., Munos, R.,
Hassabis, D., Pietquin, O., et al. Noisy networks for exploration. In International Conference on
Learning Representations, 2018.
Gehring, C. and Precup, D. Smart exploration in reinforcement learning using absolute temporal
difference errors. In Proceedings of the International Conference on Autonomous Agents and
Multi-Agent Systems, 2013.
Gordon, G. and Ahissar, E. Reinforcement active learning hierarchical loops. In International Joint
Conference on Neural Networks, pp. 3008-3015, 2011.
Gregor, K., Rezende, D. J., and Wierstra, D. Variational intrinsic control. CoRR, abs/1611.07507,
2016.
Guez, A., Viola, F., Weber, T., Buesing, L., Kapturowski, S., Precup, D., Silver, D., and Heess, N.
Value-driven hindsight modelling. In Advances in Neural Information Processing Systems, 2020.
Hausman, K., Springenberg, J. T., Wang, Z., Heess, N., and Riedmiller, M. Learning an embedding
space for transferable robot skills. In International Conference on Learning Representations, 2018.
Janz, D., Hron, J., Herndndez-Lobato, J. M., Hofmann, K., and Tschiatschek, S. Successor Un-
certainties: exploration and uncertainty in temporal difference learning. In Advances in Neural
Information Processing Systems, 2019.
Kapturowski, S., Ostrovski, G., Quan, J., Munos, R., and Dabney, W. Recurrent experience replay
in distributed reinforcement learning. In International Conference on Learning Representations,
2018.
Kearns, M. and Singh, S. Near-optimal reinforcement learning in polynomial time. Machine learning,
49(2-3):209-232, 2002.
Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference
on Learning Representations, 2015.
10
Under review as a conference paper at ICLR 2021
Kumaraswamy, R., Schlegel, M., White, A., and White, M. Context-dependent upper-confidence
bounds for directed exploration. In Advances in Neural Information Processing Systems, pp.
4779-4789, 2018.
Li, Y., Gimeno, F., Kohli, P., and Vinyals, O. Strong generalization and efficiency in neural programs.
arXiv preprint arXiv:2007.03629, 2020.
Liu, C., Zhu, L., and Belkin, M. Toward a theory of optimization for over-parameterized systems of
non-linear equations: the lessons of deep learning. arXiv preprint arXiv:2003.00307, 2020.
Lopes, M., Lang, T., Toussaint, M., and Oudeyer, P.-Y. Exploration in model-based reinforcement
learning by empirically estimating learning progress. In Advances in Neural Information Processing
Systems, 2012.
Machado, M. C., Bellemare, M. G., Talvitie, E., Veness, J., Hausknecht, M., and Bowling, M.
Revisiting the arcade learning environment: Evaluation protocols and open problems for general
agents. Journal of Artificial Intelligence Research, 61:523-562, 2018.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., Graves, A., Ried-
miller, M., Fidjeland, A. K., Ostrovski, G., et al. Human-level control through deep reinforcement
learning. Nature, 518(7540):529-533, 2015.
Moerland, T. M., Broekens, J., and Jonker, C. M. Efficient exploration with double uncertain value
networks. In Advances in Neural Information Processing Systems, 2017.
Nachum, O., Norouzi, M., and Schuurmans, D. Improving policy gradient by exploring under-
appreciated rewards. In International Conference on Learning Representations, 2016.
Osband, I., Russo, D., and Van Roy, B. (more) efficient reinforcement learning via posterior sampling.
In Advances in Neural Information Processing Systems, 2013.
Osband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped DQN. In
Advances in Neural Information Processing Systems, 2016a.
Osband, I., Van Roy, B., and Wen, Z. Generalization and exploration via randomized value functions.
In International Conference on Machine Learning, 2016b.
Osband, I., Aslanides, J., and Cassirer, A. Randomized prior functions for deep reinforcement
learning. In Advances in Neural Information Processing Systems, 2018.
Osband, I., Van Roy, B., Russo, D. J., and Wen, Z. Deep exploration via randomized value functions.
Journal of Machine Learning Research, 20:1-62, 2019.
Osband, I., Doron, Y., Hessel, M., Aslanides, J., Sezener, E., Saraiva, A., McKinney, K., Lattimore,
T., Szepezvari, C., Singh, S., and Benjamin Van Roy, Richard Sutton, D. S. H. V. H. Behaviour
suite for reinforcement learning. In International Conference on Learning Representations, 2020.
Ostrovski, G., Bellemare, M. G., van den Oord, A., and Munos, R. Count-based exploration
with neural density models. In Proceedings of the 34th International Conference on Machine
Learning-Volume 70, pp. 2721-2730. JMLR. org, 2017.
Oudeyer, P.-Y. and Kaplan, F. What is intrinsic motivation? A typology of computational approaches.
Frontiers in neurorobotics, 1:6, 2009.
Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. Intrinsic motivation systems for autonomous mental
development. IEEE transactions on evolutionary computation, 11(2):265-286, 2007.
O’Donoghue, B., Osband, I., Munos, R., and Mnih, V. The uncertainty bellman equation and
exploration. In International Conference on Machine Learning, 2018.
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. Curiosity-driven exploration by self-supervised
prediction. In International Conference on Machine Learning, 2017.
Peng, J. and Williams, R. J. Incremental multi-step Q-learning. In Machine Learning Proceedings
1994, pp. 226-232. Elsevier, 1994.
11
Under review as a conference paper at ICLR 2021
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R. Y., Chen, X., Asfour, T., Abbeel, P.,
and Andrychowicz, M. Parameter space noise for exploration. In International Conference on
Learning Representations, 2018.
PUigdomenech Badia, A., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S.,
Tieleman, O., Arjovsky, M., Pritzel, A., Bolt, A., and Blundell, C. Never give up: Learning
directed exploration strategies. In International Conference on Learning Representations, 2020.
Schaul, T., Borsa, D., Ding, D., Szepesvari, D., Ostrovski, G., Dabney, W., and Osindero, S. Adapting
behaviour for learning progress. arXiv preprint arXiv:1912.06910, 2019.
Schmidhuber, J. Curious model-building control systems. In Proceedings of the International Joint
Conference on Neural Networks, 1991.
Simmons-Edler, R., Eisner, B., Mitchell, E., Seung, H. S., and Lee, D. D. Qxplore: Q-learning
exploration by maximizing temporal difference error. arXiv preprint arXiv:1906.08189, 2019.
Singh, S. P., Barto, A. G., and Chentanez, N. Intrinsically motivated reinforcement learning. In
Advances in Neural Information Processing Systems, 2005.
Strens, M. A Bayesian framework for reinforcement learning. In International Conference on
Machine Learning, 2000.
Thompson, W. R. On the likelihood that one unknown probability exceeds another in view of the
evidence of two samples. Biometrika, 25(3/4):285-294, 1933.
Tokic, M. Adaptive ε-greedy exploration in reinforcement learning based on value differences. In
Annual Conference on Artificial Intelligence, pp. 203-210. Springer, 2010.
Van Hasselt, H., Guez, A., and Silver, D. Deep reinforcement learning with double Q-learning. In
Association for the Advancement of Artificial Intelligence, 2016.
White, A. et al. Developing a predictive approach to knowledge. PhD thesis, University of Alberta,
2015.
Williams, R. J. and Peng, J. Function optimization using connectionist reinforcement learning
algorithms. Connection Science, 3(3):241-268, 1991.
Xu, T., Liu, Q., Zhao, L., and Peng, J. Learning to explore via meta-policy gradient. In International
Conference on Machine Learning, 2018.
Zahavy, T., Xu, Z., Veeriah, V., Hessel, M., Oh, J., van Hasselt, H., Silver, D., and Singh, S.
Self-tuning deep reinforcement learning. arXiv preprint arXiv:2002.12928, 2020.
Ziebart, B. D., Maas, A., Bagnell, J. A., and Dey, A. K. Maximum entropy inverse reinforcement
learning. In Association for the Advancement of Artificial Intelligence, 2008.
12
Under review as a conference paper at ICLR 2021
A Implementation and Code
In this section, we provide code for implementing TDU in a general policy-agnostic setting and
in the specific case of bootstrapped Q-learning. Algorithm 3 presents TDU in a policy-agnostic
framework. TDU can be implemented as a pre-processing step (Line 9) that augments the reward with
the exploration signal before computing the policy loss. If Algorithm 3 is used to learn a single policy,
it benefits from the TDU exploration signal but cannot learn distinct exploration policies for it. In
particular, on-policy learning does not admit such a separation. To learn a distinct exploration policy,
we can use Algorithm 3 to train the exploration policy, while the another policy is trained to maximise
extrinsic rewards only using both its own data and data from the exploration policy. In case of multiple
policies, we need a mechanism for sampling behavioural policies. In our experiments we settled on
uniform sampling; more sophisticated methods can potentially yield better performance.
In the case of value-based learning, TDU takes a special form that can be implemented efficiently as
a staggered computation of TD-errors (Algorithm 4). Concretely, we compute an estimate of the dis-
tribution of TD-errors from some given distribution over the value function parameters (Algorithm 4,
Line 3). These TD-errors are used to compute the TDU signal σ, which then modulates the reward
used to train a Q function (Algorithm 4, Line 7). Because the only quantities being computed are
TD-errors, this can be combined into a single error signal (Algorithm 4, Line 11). When implemented
under bootstrapping, Qparams denotes the ensemble Q and Qtilde_distribution_params
denotes the ensemble Q; We compute the loss as in Algorithm 2.
Finally, Algorithm 5 presents a complete JAX (Bradbury et al., 2018) implementation that can
be used along With the Bsuite (Osband et al., 2020) codebase.1 We present the corresponding
TDU agent class (Algorithm 4), Which is a modified version of the BootstrappedDqn class in
bsuite/baselines/jax/boot_dqn/agent.py and can be used by direct sWap-in.
Algorithm 3 Pseudo-code for generic TDU loss
1	def loss(transitions, pi_params, Qtilde_distribution_Params, beta):
2	# Estimate TD-error distribution.
3	td = array([td_error(p, transitions) for P in Sample(Qtilde_distribution_Params)])
4
5	# Compute critic loss .
6	td_loss = mean(0.5 * (td ** 2))
7
8	# Compute exploration bonus .
9	transitions.r_t += beta * stop_gradient(std(td, axis=1))
10
11	# Compute policy loss on transition with augmented reward.
12	pi_loss = pi_loss_fn(pi_params, transitions)
13
14	return pi_loss, td_loss
Algorithm 4 Pseudo-code for Q-learning TDU loss
1	def loss(transitions, Q_params, Qtilde_distribution_params, beta):
2	# Estimate TD-error distribution.
3	td_K = array([td_error(p, transitions) for P in SamPle(Qtilde_distribution_params)])
4
5	# Compute exploration bonus and Q-function reward.
6	transitions.reward_t += beta * stop_gradient(std(td_K, axis=1))
7	td_N = td error(Q params, transitions)
8
9	# Combine for overall TD-loss.
10	td_errors = COnCatenate((td_ex, td_in), axis=1)
11	td_loss = mean(0.5 * (td_errors) ** 2))
12	return td_loss
1Available at: https://github.com/deepmind/bsuite.
13
Under review as a conference paper at ICLR 2021
Algorithm 5 JAX implementation of TDU Agent under Bootstrapped DQN
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
#	Copyright 2020 the Temporal Difference Uncertainties as a Signal for Exploration authors. Licensed under
#	the Apache License, Version 2.0 (the “License”)； you may not use this file except in compliance with
#	the License. You may obtain a copy of the License at https://www.apache.org/IiCenSeS/LICENSE-2.0.
#	Unless required by applicable law or agreed to in writing, software distributed under the License is
#	distributed on an “AS IS” BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#	See the License for the specific language governing permissions and limitations under the License.
class TDU(bsuite.baselines.jax.boot_dqn.BootstrappedDqn):
def ___init__(self, K: int, beta: float, **kwargs: Any):
"""TDU under Bootstrapped DQN with randomized prior functions."""
super(TDU, self).___init__(**kwargs)
network, optimizer, N = kwargs[, network'], kwargs[, optimizer'], kwargs[, num_ensemble']
noise_scale, discount = kwargs[' noise_scale'], kwargs['discount']
def td(params: hk.Params, target_params: hk.Params,
transitions: SeqUenCe[jnp.ndarray]) -> jnp.ndarray:
"""TD-error with added reward noise + half-in bootstrap."""
o_tm1, a_tm1, r_t, d_t, o_t, z_t = transitions
q_tm1 = network.appl (params, o_tm1)
q_t = network. apply (target_Params, o_t)
r_t += noise_scale * z_t
return jax.vmap(rlax.q_learning)(q_tm1, a_tm1, r_t, discount * d_t, q_t)
def loss(params: SeqUenCe[hk.Params], target_Params: Sequence[hk.Params],
transitions: SeqUenCe[jnp.ndarray]) -> jnp.ndarray:
"""Q-learning loss with TDU."""
#	Compute TD-errors for first K members.
o_tm1, a_tm1, r_t, d_t, o_t, m_t, z_t = transitions
td_K = [td(params[k], target_ParamS[k],
[o_tm1, a_tm1, r_t, d_t, o_t, z_t[:, k]]) for k in rang (K)]
#	TDU signal on first K TD-errors.
r_t += beta * jax.lax.stop_gradient(jnp.std(jnp.StaCk(td_K, axis=0), axis=0))
#	Compute TD-errors on augmented reward for last K members.
td_N = [td(params[k], target_ParamS[k],
[o_tm1, a_tm1, r_t, d_t, o_t, z_t[:, k]]) for k in rang (K, N)]
return jnp.mean(m_t.T * jnp.stack(td_K + td_N) ** 2)
def update(state: TrainingState, gradient: Sequence[jnp.ndarray]) -> TrainingState:
"""Gradient update on ensemble member ."""
updates, new_opt_state = optimizer.update(gradient, state.opt_state)
new_params = optix.apply_UPdateS(State.params, updates )
return TrainingState(Params=new_Params, target_ParamS=State.target_Params,
opt_state=new_opt_state, step=state.step + 1)
@jax.jit
def sgd_step(States: Sequence[TrainingState],
transitions: Sequence[jnp.ndarray]) -> Sequence[TrainingState]:
"""Does a step of SGD for the whole ensemble over ‘transitions ‘."""
params, target_params = zip (*[ (state.params, State.target_ParamS) for state in states])
gradients = jax.grad(loss)(params, target_params, transitions)
return [update(state, gradient) for state, gradient in zip (states, gradients)]
self._sgd_step = sgd_step # patch BootDQN sgd_step with TDU sgd_step.
def update(self, timestep: dm_env.TimeStep, action: base.Action,
new_timestep: dm_env.TimeStep):
"""Update the agent : add transition to replay and periodically do SGD."""
if new_timestep.last():
self._active_head = self._ensemble[np.random.randint(0, self._num_ensemble)]
mask = np.random.binomial(1, self._mask_prob, self._num_ensemble)
noise = np.random.randn(self._num_enSembIe)
transition= [timestep.observation, action,np.float32(new_timestep.reward),
np.float3 2(new_timestep.discount), new_timestep.observation,mask, noise]
self._replay.add(transition)
if self._replay.size ‹ self._min_replay_size:
return
if self._total_steps % self._sgd_period == 0:
transitions = self._replay.sample(self._batch_size)
self._ensemble = self._sgd_step(self._ensemble, transitions)
for k, state in enumerat (self._ensemble):
if state.step % self._target_update_PeriOd == 0:
self._ensemble[k] = State._replace(target_ParamS=State.params )
14
Under review as a conference paper at ICLR 2021
B	Proofs
We begin with the proof of Lemma 1. First, we show that if Eq. 2 and Eq. 3 fail, p(θ) induce
a distribution p(Qθ) whose first two moments are biased estimators of the moments of the dis-
tribution of interest p(Qπ), for any choice of belief over the MDP, p(M). We restate it here for
convenience.
Lemma 1. If Eθ[Qθ] and Vθ[Qθ] fail to satisfy Eqs. 2 and 3, respectively, they are biased estimators
of EM QπM and VM QπM for any choice ofp(M).
Proof. Assume the contrary, that EM QπM (s, π(s)) = Eθ[Qθ(s, π(s))] for all (s, a) ∈ S × A. If
Eqs. 2 and 3 do not hold, then for any M ∈ {E, V},
MM QπM (s, π(s)) = Mθ[Qθ(s, π(s))]	(12)
=Mθ Es0 〜P(s,π(s))[r + YQθ (S0,π(SO))]	(13)
_ r 〜R(s,π(s))	_
=Es0〜P(s,π(s))[r + YMθ [Qθ (S0,π(SO))]]	(14)
r 〜R(s,π(s))
=Es0 〜P(s,π(s))r+γMMQπM(SO,π(SO))	(15)
r 〜R(s,π(s))
= MM Es
0~P(s,π(s)) r + γQπM (SO, π(SO))	(16)
_ r 〜R(s,π(s))	_
=MMQπM(S,π(S)) ,	(17)
a contradiction; conclude that MM QπM (S, π(S)) 6= Mθ[Qθ(S, π(S))]. Eqs. 13 and 17 use Eqs. 2
and 3; Eqs. 12, 13 and 15 follow by assumption; Eqs. 14 and 16 use linearity of the expectation
operator Er,s0 by virtue of M being defined over θ. As (S, a, r, SO) and p(M) are arbitrary, the
conclusion follows.
Methods that take inspiration from by PSRL but rely on neural networks typically approximate p(M)
by a parameter distribution p(θ) over the value function. Lemma 1 establishes that the induced
distribution p(Qθ) under push-forward of p(θ) must propagate the moments of the distribution p(Qθ)
consistently over the state-space to be unbiased estimate ofp(QπM), for any p(M).
With this in mind, we now turn to neural networks and their ability to estimate value function
uncertainty in MDPs. To prove our main result, we establish two intermediate results. Recall that
We define a function approximator Qθ = W ◦ φ心.,where θ = (wι,..., Wn, Hi,..., Hv); W ∈ Rn is a
linear layer and φ : S×A→ Rn is a feature extractor with parameters H ∈ Rv.
As before, let M be an MDP (S, A, P, R, γ) with discrete state and action spaces. We denote by N
the number of states and actions with Eθ[Qθ(S, a)] 6= Eθ[Qθ(SO, aO)] with N ⊂ S × A × S × A the
set of all such pairs (S, a, SO, aO). This set can be thought of as a minimal MDP—the set of states
within a larger MDP where the function approximator generates unique predictions. It arises in an
MDP through dense rewards, stochastic rewards, or irrevocable decisions, such as in Deep Sea. Our
first result is concerned with a very common approach, where H is taken to be a point estimate so that
p(θ) = p(W). This approach is often used for large neural networks, where placing a posterior over
the full network would be too costly (Osband et al., 2016a; O’Donoghue et al., 2018; Azizzadenesheli
et al., 2018; Janz et al., 2019).
Lemma 2. Let p(θ) = p(W). If N > n, with W ∈ Rn, then Eθ[Qθ] fail to satisfy the first moment
Bellman equation (Eq. 2). Further, if N > n2, then Vθ[Qθ] fail to satisfy the second moment Bellman
equation (Eq. 3).
Proof. Write the first condition of Eq. 2 as
Eθ WTφ.(S, a) = Eθ Er,s0 r + γWT φ.(SO, π(SO)) .
(18)
15
Under review as a conference paper at ICLR 2021
Using linearity of the expectation operator along with p(θ) = p(w), we have
Ew [w]T φ4(s,a) = μ(s,a) + YEw [w]T Es，[φ^(s0,π(s0))] ,	(19)
where μ(s, a) = Er〜R(s,a) [r]. Rearrange to get
μ(s,a) = Ew [w]T (φ^(s,a) - γEs0 [φ4(s0,π(s0))] ) .	(20)
By assumption Eθ[Qθ(s, a)] = Eθ[Qθ(s0, a0)], which implies φ^(s, a) = φ^(s0, ∏(s0)) by linearity
in w. Hence φ4(s,a) - γEs0 [φ^(s0,π(s0))] is non-zero and unique for each (s, a). Thus, Eq. 20
forms a system of linear equations over S × A, which can be reduced to a full-rank system over
N: μ = ΦEw [w], where μ ∈ RN stacks expected reward μ(s, a) and Φ ∈ RN×n stacks vectors
φ吧(s, a) — γEs0[φ^(s0,π(s0))] row-wise. Because Φ is full rank, if N > n, this system has no
solution. The conclusion follows for Eθ[Qθ]. If the estimator of the mean is used to estimate the
variance, then the estimator of the variance is biased. For an unbiased mean, using linearity in w,
write the condition of Eq. 3 as
Eθ h(w — Ew H)TΦ4(s, a)]	= Eθ /(w — Ew [w])TEs，[φ^(s0, ∏(s0))]]	.	(21)
Let W = (WT — Ew [w]), X = WTφ^(s, a), y = YwTEs，[φ^(s0, a0)]. Rearrange to get
Eθ x2 — y2 = Ew[(x — y)(x +y)] = 0.	(22)
Expanding terms, we find
0 = Eθ [(wt[φ^(s,a) — YEs，®(s0,a0)]])(WT[φ4(s,a) + γEs0 [φ^(s0, a0)]]^]	(23)
nn	nn
=XX
Ew [Wi Wj ] d-d+ = XX
Cov (Wi, Wj) di-dj+ .	(24)
where we define d- = φ4(s,a) — γEs0 [φ^(s0, a0)] and d+ = φ4(s,a) + YEs，[φ^(s0, a0)]. As
before, d- and d+ are non-zero by assumption of unique Q-values. Perform a change of variables
ωα(i,j) = Cov(Wi, Wj), λα(i,j) = di- dj+ to write Eq. 24 as 0 = λTω. Repeating the above process
for every state and action we have a system 0 = Λω, where 0 ∈ RN and Λ ∈ RN ×n2 are defined by
stacking vectors λ row-wise. This is a system of linear equations and if N > n2 no solution exists;
thus, the conclusion follows for Vθ [Qθ], concluding the proof.
Note that if Eθ [Qθ] is biased and used to construct the estimator Eθ [Qθ], then this estimator is
also biased; hence if N > n, p(θ) induce biased estimators Eθ[Qθ] and Vθ[Qθ] of EM QπM and
VM QπM , respectively.
Lemma 2 can be seen as a statement about linear uncertainty. While the result is not too surprising
from this point of view, it is nonetheless a frequently used approach to uncertainty estimation. We
may hope then that by placing uncertainty over the feature extractor as well, we can benefit from its
nonlinearity to obtain greater representational capacity with respect to uncertainty propagation. Such
posteriors come at a price. Placing a full posterior over a neural network is often computationally
infeasible, instead a common approach is to use a diagonal posterior, i.e. Cov(θi, θj ) = 0 (Fortunato
et al., 2018; Plappert et al., 2018). Our next result shows that any posterior of this form suffers from
the same limitations as placing a posterior only over the final layer. We establish something stronger:
any posterior of the form p(θ) = P(W)P(H) suffers from the limitations described in Lemma 2.
Lemma 3. Let p(θ) = P(W)P(H); if N > n, with W ∈ Rn ,then Eθ [Qθ ] fail to satisfy the first
moment Bellman equation (Eq. 2). Further, if N > n2, then Vθ[Qθ] fail to satisfy the second moment
Bellman equation (Eq. 3).
16
Under review as a conference paper at ICLR 2021
Proof. The proof largely proceeds as in the proof of Lemma 2. Re-write Eq. 19 as
Ew[w]T E^[φ^(s, a)] = μ(s,a) + YEwHT Es，[E^[φ^(s0,∏(s0))]] .	(25)
TA ，-	ι	r∙ ∙ i i 7 -rm Γ ( T , ι ,
Perform a change of variables φ = E^ [φ^ ] to obtain
μ(s,a) = Ew HT (Φ(s,a) — γEs0 [Φ(s0,∏(s0))] ) .	(26)
Because Eθ[Qθ(s, a)] 6= Eθ[Qθ(s0, a0)], by linearity in w we have that φ(s, a) - φ(s0, a0) is non-zero
for any (s0, a0) and hence Eq. 26 has no trivial solutions. Proceeding as in the proof of Lemma 2
obtains μ = ΦEw [w], where Φ Is analogously defined. Note that If N > n there Is no solution Ew [w]
for any admissible (full-rank) choice of Φ, and hence the conclusion follows for the first part. For the
second part, using that Eθ = Ew E^ in Eq. 24 yields
nn	nn
0 = XX Ew [WiWj] EO [d-d+] = XX Cov (wi, Wj) E® [d-d+] .	(27)
Perform a change of variables 入a(i,j) = E® [d-d+]. Again, by Eθ [Qθ(s, a)] = Eθ [Qθ(s0, a0)] We
have that λ is non-zero; proceed as before to complete the proof.
We are now ready to prove our main result. We restate it here for convenience:
Proposition 1. If the number of state-action pairs where Eθ [Qθ (s, a)] 6= Eθ[Qθ(s0, a0)] is greater
than n, where w ∈ Rn, then Eθ[Qθ] and Vθ[Qθ] are biased estimators of EM QπM and VM QπM
for any choice of p(M).
Proof. Let p(θ) be of the form p(θ) = P(W) or p(θ) = P(W)P(%. By Lemmas 2 and 3, p(θ) fail
to satisfy Eq. 2. By Lemma 1, this causes Eθ[Qθ] to be a biased estimator of EM QπM . This in
turn implies that Vθ[Qθ] is a biased estimator ofVM [QπM ]. Further, if N > n2, Vθ[Qθ] is biased
independently of Eθ [Qθ].
We now turn to analysing the bias of our proposed estimators. As before, we will build up to
Proposition 2 through a series of lemmas. For the purpose of these results, let B : S × A → R denote
the bias of Eθ [Qθ] in any tuple (s, a) ∈ S × A, so that Bias(Eθ[Qθ] (s, a)) = B(s, a).
Lemma 4. Given a transition τ := (s, a, r, s0), for anyP(M), given P(θ), if
B(s0,∏(s0))
B(s, a)
∈ (O, 2/Y)
(28)
then Eθ[δ(θ, τ) | τ] has less bias than Eθ [Qθ (s, a)].
Proof. From direct manipulation of Eθ [δ(θ, τ) | τ], we have
Eθ[δ(θ,τ) | τ] = Eθ[γQθ(s0, π(s0)) + r - Qθ(s,a)]	(29)
= γEθ[Qθ(s0, π(s0))] + r - Eθ[Qθ(s,a)]	(30)
= γEM [QπM(s0, π(s0))] +r-EM[QπM(s,a)] + γB(s0, π(s0)) - B(s, a) (31)
=EM[δπM(τ)] + γB(s0, π(s0)) -B(s,a).	(32)
Consequently, Bias(Eθ[δ(θ, τ) | τ]) = γB(s0, π(s0)) - B(s, a) and for this bias to be less
than Bias(Eθ[Qθ(s, a)]) = B(s, a), we require ∣γB(s0,π(s0)) - B(s, a)| < |B(s, a)|. Let
P = B(s0,π(s0))∕B(s,a) and write ∣(γρ - 1)B(s, a)| < |B(s, a)| from which it follows that
for this to hold true, we must have P ∈ (0,2∕γ), as to be proved.	■
17
Under review as a conference paper at ICLR 2021
We now turn to characterising the conditions under which Vθ [δ(θ, τ) | τ] enjoys a smaller bias
than Vθ[Qθ(s, a)]. Because the variance term involves squaring the TD-error, we must place some
restrictions on the expected behaviour of the Q-function to bound the bias. First, as with B, let C :
S × A → R denote the bias ofEθ Qθ2 for any tuple (s, a) ∈ S × A, so that Bias(Eθ Qθ(s, a)2 ) =
C(s, a). Similarly, let D : S × A × S → R denote the bias of Eθ[Qθ(s0, π(s0))Qθ(s, a)] for any
transition (s, a, s0) ∈ S × A × S.
Lemma 5. For any τ and any p(M), given p(θ), define relative bias ratios
= B(s0,π(s0))	C(s0,π(s0))	=D(s,a,s0)	=EM [QM(s0,∏(s0))]
P = B(s,a) ,	φ = C(s,a) ,	K = C(s,a) ,	° = EM [QM E	.	( )
There exists ρ ≈ 1, φ ≈ 1, K ≈ 1, α ≈ 1 such that Vθ [δ(θ,τ) | T] have less bias than Vθ [Qθ (s, a)].
In particular, if P = φ = κ = α = 1, then
| Bias(Vθ[δ(θ,τ) | T])| = ∣(γ - 1)2 Bias既[Qθ(s,a)])∣ < | Bias(Vθ[Qθ(s,a)])∣.	(34)
Further, if P = 1∕γ, K = 1∕γ, φ = 1∕γ2, then | Bias(Vθ[δ(θ, τ) | τ])| = 0 for any α.
Proof. We begin by characterising the bias of Vθ [Qθ (s, a)]. Write
Vθ[Qθ(s, a)] = Eθ hQ(s, a)2i - Eθ[Q(s, a)]2	(35)
=EM [qM(s,a)2i + C(s,a)- (EM [QM(s,a)] + B(s,a))2.	(36)
The squared term expands as
(EM [QM(s, a)] + B(s, a))2 = EM [QM(s, a)]2 + 2Em [QM(s, a)] B(s, a) + B(s, a)2. (37)
Let A(s, a) = EM QπM (s, a) B(s, a) and write the bias of Vθ[Qθ(s, a)] as
Bias(Vθ [Qθ (s, a)]) = C(s, a) + 2A(s, a) + B(s, a)2.	(38)
We now turn to Vθ[δ(θ, τ) | τ]. First note that the reward cancels in this expression:
δ(θ, τ) - Eθ[δ(θ, τ)] = γQθ(s0, π(s0)) - Qθ(s, a) - (γEθ[Qθ(s0, π(s0))] - Eθ[Qθ(s, a)]). (39)
Denote by xθ = γQθ(s0, π(s0)) -Qθ(s, a) with Eθ[xθ] = γEθ[Qθ(s0, π(s0))] -Eθ[Qθ(s, a)]. Write
Vθ[δ(θ,τ) | τ] =Eθ (δ(θ, τ) - Eθ [δ(θ, τ)])2	(40)
= Eθ h(xθ - Eθ[xθ])2i	(41)
=Eθ[xθ2] -Eθ[xθ]2	(42)
= Eθ h(γQθ(s0, π(s0)) - Qθ(s, a))2i - (γEθ[Qθ(s0, π(s0))] - Eθ [Qθ (s, a)])2.
(43)
Eq. 41 uses Eq. 39 and Eq. 43 substitutes back for xθ . We consider each term in the last expression
in turn. For the first term, Eθ
(γQθ(s0, π(s0)) - Qθ(s, a))2
, expanding the square yields
γ2Eθ[Qθ(s0, π(s0))2] - 2γEθ[Qθ(s0, π(s0)Qθ(s, a)] + Eθ [Qθ(s, a)2] .	(44)
18
Under review as a conference paper at ICLR 2021
From this, we obtain the bias as
Bias Eθ h(γQθ(s0, π(s0)) - Qθ(s, a))2i	= γ2C(s0, π(s0)) - 2γD(s, a, s0) + C(s, a)	(45)
(46)
We can compare this term to C(s, a) in the bias of of Vθ [Qθ (s, a)] (Eq. 38). For the bias term in
Eq. 46 to be smaller, we require | γ2φ - 2γκ + 1 C(s, a)| < |C(s, a)| from which it follows that
γ2φ - 2γκ + 1 ∈ (-1, 1). In terms of φ, this means
2kγ - 2 2k
φ ∈(	,7
(47)
If the bias term D is close to C (κ ≈ 1), this is approximately the same condition as for ρ in Lemma 4.
Generally, as κ grows large, φ must grow small and vice-versa. The gist of this requirement is that
the biases should be relatively balanced κ ≈ φ ≈ 1.
For the second term in Eq. 43, recall that Eθ[Qθ(s0, π(s0))] = EM QπM (s0, π(s0)) + B(s0, π(s0))
and Eθ[Qθ(s, a)] = EM QπM (s, a) + B(s, a). We have
(Eθ[Qθ(S0, π(SO))] - Eθ[Qθ(S, a)])2 = ((Ya - I)EM [QM(S, 叫 + (7P - I)B(S, a))2,	(48)
where α = EM QπM (s0, π(s0)) /EM QπM (s, a). This expands as
(7α-1)2EM[QπM(S, a)2+2(7α-1)(7ρ-1)EM[QπM(S,a) B(S, a) + (7ρ - 1)2B(S, a)2. (49)
NotethatfromEq.34, (Em [QM(s0,π(s0))] - EM[QM(s, a)])2 = (γα- 1)2Em [QM(s, a)]2 and
so the bias of Vθ [δ(θ, τ) | τ] can be written as
Bias(Vθ [δ (θ, τ) | τ]) = w1(φ, κ)C (S, a) + w2 (α, ρ)2A(S, a) + w3(ρ)B(S, a)2	(50)
where
w1(φ, κ) = 72φ - 27κ + 1 , w2(α,ρ) = (7α - 1)(7ρ - 1),	w3(ρ) = (7ρ - 1)2.	(51)
Note that the bias in Eq. 50 involves the same terms as the bias of Vθ[Qθ(S, a)] (Eq. 38) but
are weighted. Hence, there always exist as set of weights such that | BiaS(Vθ[δ(θ,τ) | T])| <
| Bias(Vθ[Qθ(s, a)])∣. In particular, if P = 1∕γ, κ = 1/7, φ = I/72, then Bias(Vθ[δ(θ,τ) | T])| =
0 for any α. Further, if ρ = α = κ = φ = 1, then we have that w1(φ, κ) = w2(α, ρ) = w3(ρ) =
(7 - 1)2 and so
| BiaS(Vθ [δ(θ, T)	|	T])|	=	|(7	-	1)2 BiaS(Vθ[Qθ(S,	a)])| < | BiaS(Vθ[Qθ(S,	a)])|,	(52)
as desired.
Proposition 2. For any T and any p(M), given p(θ), if P ∈ (0, 2/7), then Eδ[δ | T] has lower
bias than Eθ [Qθ (S, a)]. Additionally, there exists P ≈ 1, φ ≈ 1, κ ≈ 1, α ≈ 1 such that
Vθ [δ(θ, T) | T] have less bias than Vθ [Qθ (S, a)]. In particular, if P = φ = κ = α = 1, then
| BiaS(Vθ [δ(θ, T) | T])| = |(7 - 1)2 BiaS(Vθ[Qθ(S, a)])| < | BiaS(Vθ[Qθ(S, a)])|. Further, if
P = 1/7, κ = 1/7, φ = 1/72, then | BiaS(Vθ [δ (θ, T) | T])| = 0 for any α.
Proof. The first part follows from Lemma 4, the second part follows from Lemma 5.
19
Under review as a conference paper at ICLR 2021
C	Binary Tree MDP
In this section, we make a direct comparison between the Bootstrapped DQN and TDU on the Binary
Tree MDP introduced by Janz et al. (2019). In this MDP, the agent has two actions in every state.
One action terminates the episode with 0 reward while the other moves the agent one step further up
the tree. At the final branch, one leaf yields a reward of 1. Which action terminates the episode and
which moves the agent to the next branch is randomly chosen per branch, so that the agent must learn
an action map for each branch separately. This is a similar environment to Deep Sea, but simpler in
that an episode terminates upon taking a wrong action and the agent does not receive a small negative
reward for taking the correct action. We include the Binary Tree MDP experiment to compare the
scaling property of TDU as compared to TDU on a well-known benchmark.
We use the default Bsuite implementation2 of the bootstrapped DQN, with the default architecture
and hyper-parameters from the published baseline, reported in Table 2. The agent is composed of
a two-layer MLP with RELU activations that approximate Q(s, a) and is trained using experience
replay. In the case of the bootstrapped DQN, all ensemble members learn from a shared replay buffer
with bootstrapped data sampling, where each member Qθk is a separate MLP (no parameter sharing)
that is regressed towards separate target networks. We use Adam (Kingma & Ba, 2015) and update
target networks periodically (Table 2).
We run 5 seeds per tree-depth, for depths L ∈ {10, 20, . . . , 250} and report mean performance in
Figure 4. Our results are in line with those of Janz et al. (2019), differences are due to how many
gradient steps are taken per episode (our results are between the reported scores for the 1× and 25×
versions of the bootstrapped DQN). We observe a clear beneficial effect of including TDU, even for
small values of β . Further, we note that performance is largely monotonically increasing in β, further
demonstrating that the TDU signal is well-behaved and robust to hyper-parameter values.
We study the properties of TDU in Figure 5, which reports performance without prior functions
(λ = 0). We vary β and the number of exploration value functions N . The total number of value
functions is fixed at 20, and so varying N is equivalent to varying the degree of exploration. We note
that N has a similar effect to β, but has a slightly larger tendency to induce over-exploration for large
values of N .
2https://github.com/deepmind/bsuite/tree/master/bsuite/baselines/jax/
bootdqn.
20
Under review as a conference paper at ICLR 2021
experiment： BootDQN
beta： 0.0
3000-
2000-
1000-
0-
experiment： TDU
beta： 1.0
3000--
2000-
1000-
0 J I"..". I ,	,	,
0	50	100 150 200 250
experiment： TDU
beta： 0.1
experiment： TDU
beta： 5.0
0	50	100	150	200 250
Tree MDP problem size
Figure 4:	Performance on Binary Tree MDP. Top left: BootDQN (β = 0). Others: TDU with varying
strenghts of intrinsic reward (β > 0). Results with prior strength λ = 3. Mean performance over 5
seeds for each tree depth.
5000-
4000-
3000-
2000-
1000-
0-
experiment： TDU
beta： 0.5
numexplorers： 1
experiment： TDU
beta： 1.0
numexplorers： 1
• ∙∙
experiment： TDU
beta： 0.5
num explorers： 5
experiment： TDU
beta： 1.0
num explorers： 5
experiment： TDU
beta： 0.5
numexplorers ： 10
experiment： TDU
beta： 1.0
numexplorers ： 10
5000-
4000-
3000-
2000-
1000-
0-
0	50	100	150	200 250
0	50	100	150	200 250
Tree MDP problem size
0	50	100 150 200 250
Figure 5:	Hyper-parameter sensitivity analysis on Binary Tree MDP. Top: Sweep over number of
exploration policies (N) for β = 0.5. Top: Sweep over number of exploration value functions (N)
for β = 1. All results without prior functions (λ = 0). Mean performance over 5 seeds for each tree
depth.
21
Under review as a conference paper at ICLR 2021
Figure 6: Overall performance scores on Bsuite. Left: Effect of varying β . Right: comparison of
TDU to exploration under σ = σ(Q) as intrinsic reward (QU) or as an immediate bonus (Q+UCB).
D B ehaviour Suite
From Osband et al. (2020): “The Behaviour Suite for Reinforcement Learning (Bsuite) is a collection
of carefully-designed experiments that investigate core capabilities of a reinforcement learning agent .
The aim of the Bsuite project is to collect clear, informative and scalable problems that capture key
issues in the design of efficient and general learning algorithms and study agent behaviour through
their performance on these shared benchmarks.”
D.1 Agents and hyper-parameters
All baselines use the default Bsuite DQN implemen-
tation3. We use the default architecture and hyper-
parameters from the published baseline, reported in
Table 2, and sweep over algorithm-specific hyper-
parameters, reported in Table 3. The agent is com-
posed of a two-layer MLP with RELU activations
that approximate Q(s, a) and is trained using experi-
ence replay. In the case of the bootstrapped DQN, all
ensemble members learn from a shared replay buffer
with bootstrapped data sampling, where each member
Qθk is a separate MLP (no parameter sharing) that
is regressed towards separate target networks. We
use Adam (Kingma & Ba, 2015) and update target
networks periodically (Table 2).
Table 2: Hyper-parameters for Bsuite.
discount factor (γ)	0.99
batch size	32
num hidden layers	2
hidden layer sizes	[64, 64]
ensemble size	20
learning rate	0.001
mask prob	1.0
replay size	10000
env steps per gradient step	1
env steps per target update	4
QEX Uses two networks Qθ and Q仇 where Qθ is trained to maximise the extrinsic reward, while
Qe is trained to maximise the absolute TD-error of Qθ (SimmonS-Edler et al., 2019). In contrast to
TDU, the intrinsic reward is given as a point estimate of the TD-error for a given transition, and thus
cannot be interpreted as measuring uncertainty as such.
CTS Implements a count-based reward defined by i(s, a, H) = (N (s, a, H) + 0.01)-1/2, where
H is the history and N(s, a, H) = Pτ∈H 1(s,a)∈τ is the number of times (s, a) has appeared in
a transition τ := (s, a, r, s0). This intrinsic reward is added to the extrinsic reward to form an
augmented reward r = r + βi used to train a DQN agent (Bellemare et al., 2016).
RND Uses two auxiliary networks f and f that map a state into vectors X = fe(s) and X = f8(s),
m
x,x ∈ R . While H is a random parameter vector that is fixed throughout, H is trained to minimise
the mean squared error i(s) = ∣∣x - X∣∣. This error is simultaneously used as an intrinsic reward in the
augmented reward function r(s, a) = r(s, a) + βi(s) and is used to train a DQN agent. Following
3https://github.com/deepmind/bsuite/tree/master/bsuite/baselines/jax/
dqn.
22
Under review as a conference paper at ICLR 2021
Table 3: Hyper-parameter grid searches for Bsuite. Best values in bold.
Algorithm ∣ Hyper-parameter		Sweep set
QEX	Intrinsic reward scale (β)	{10-4, 10-3,10-2, 10-1,100, 5 ∙ 100,101,102, 103}
CTS	Intrinsic reward scale (β)	{10-4, 10-3,10-2, 10-1,5 ∙ 100,100,101, 102,103}
RND	Intrinsic reward scale (β) x-dim (m) Moving average decay (α) Normalise intrinsic reward	{10-2, 5 ∙ 10-1, 10-1,100, 5 X 100,101,102} {10, 64, 128} {0.9, 0.99, 0.999} {True, False}
BDQN	Prior scale (λ)	{0, 1, 3, 5, 10, 50, 100}
SU	Hidden size Likelihood variance (β) Prior variance (θ)	{20, 64} {10-2, 10-1, 100, 101, 102} {10-3, 10-1, 100, 101, 103}
NNS	Noise scale (β)	{10-2, 10-1, 100, 101, 102}
TDU	Prior scale (λ) Intrinsic reward scale (β)	{0,100, 3 ∙ 100} {10-3, 10-2,10-1, 100, 5 ∙ 100,101}
Burda et al. (2018b), we normalise intrinsic rewards by an exponential moving average of the mean
and the standard deviation that are being updated with batch statistics (with decay α).
BDQN Trains an ensemble Q = {Qθk}kK=1 of DQNs (Osband et al., 2016a). At the start of each
episode, one DQN is randomly chosen from which a greedy policy is derived. Data collected is placed
in a shared replay memory, and all ensemble members have some probability ρ of training on any
transition in the replay. Each ensemble member has its own target network. In addition, each DQN is
augmented With a random prior function 久,where H is a fixed parameter vector that is randomly
sampled at the start of training. Each DQN is defined by Qθk + λ%k, where λ is a hyper-parameter
regulating the scale of the prior. Note that the target network uses a distinct prior function.
SU Decomposes the DQN as Qθ(s, a) = WTψ4(s, a). The parameters H are trained to satisfy the
Success Feature identity while w is learned using Bayesian linear regression; at the start of each
episode, a new w is sampled from the posterior p(w | history) (Janz et al., 2019).4
NNS NoisyNets replace feed-forward layers Wx + b by a noisy equivalent (W + Σ W)x + (b +
Q Θ eb), where Θ is element-wise multiplication; eW 〜N(0, β) and eb〜N(0, β) are white noise of
the same size as W and b, respectively. The set (W, Σ, b, σ) are learnable parameters that are trained
on the normal TD-error, but with the noise vector re-sampled after every optimisation step. Following
Fortunato et al. (2018), sample noise separately for the target and the online network.
TDU We fix the number of explorers to 10 (half of the number of value functions in the ensemble),
which roughly corresponds to randomly sampling between a reward-maximising policy and an
exploration policy. Our experiments can be replicated by running the TDU agent implemented in
Algorithm 5 in the Bsuite GitHub repository.5
D.2 TDU Experiments
Effect of TDU Our main experiment sweeps over β to study the effect of increasing the TDU
exploration bonus, with β ∈ {0, 0.01, 0.1, 0.5, 1, 2, 3, 5}; β = 0 corresponds to default bootstrapped
DQN. We find that β reflects the exploitation-exploration trade-off: increasing β leads to better
performance on exploration tasks (see main paper) but typically leads to worse performance on
4See https://github.com/DavidJanz/successor_uncertainties_tabular.
5https://github.com/deepmind/bsuite/blob/master/bsuite/baselines/jax.
23
Under review as a conference paper at ICLR 2021
tasks that do not require further exploration beyond -greedy (Figure 6). In particular, we find that
β > 0 prevents the agent from learning on Mountain Car, but otherwise retains performance on
non-exploration tasks. Figure 7 provides an in-depth comparison per game.
Because σ is a principled measure of concentration in the distribution p(δ | s, a, r, s0), β can be
interpreted as specifying how much of the tail of the distribution the agent should care about. The
higher we set β, the greater the agent’s sensitivity to the tail-end of its uncertainty estimate. Thus,
there is no reason in general to believe that a single β should fit all environments, and recent advances
in multi-policy learning (SchaUl et al., 2019; Zahavy et al., 2020; Puigdomenech Badia et al., 2020)
suggests that a promising avenue for further research is to incorporate mechanisms that allow either
β to dynamically adapt or the sampling probability over policies. To provide concrete evidence to
that effect, we conduct an ablation study that uses bandit policy sampling below.
Effect of prior functions We study the inter-relationship between additive prior functions (Osband
et al., 2019) and TDU. We sweep over λ ∈ [0, 1, 3], where prior functions define value function
estimates by Qk = Qθk + λPk for some random network Pk . Thus, λ = 0 implies no prior function.
We find a general synergistic relationship; increasing λ improves performance (both with and without
TDU), and for a given level of λ, performance on exploration tasks improve for any β > 0. It
should be noted that these effects do no materialise as clearly in our Atari settings, where we find no
conclusive evidence to support λ > 0 under TDU.
Ablation: exploration under non-TD signals To empirically support theoretical underpinnings
of TDU (Proposition 2), we conduct an ablation study where σ is re-defined as the standard deviation
over value estimates:
σ(Q) := t
1K
K-1X Qk - Q.
(53)
In contrast to TDU, this signal does not condition on the future and consequently is likely to suffer
from a greater bias. We apply this signal both as in intrinsic reward (QU), as in TDU, and as an
UCB-style exploration bonus (Q+UCB), where σ is instead applied while acting by defining a policy
by π(∙) = arg max ° Q(∙, a) + βσ(Q; ∙, a). Note that TDU cannot be applied in this way because
the TDU exploration signal depends on r and s0 . We tune each baseline over the same set of β values
as above (incidentally, these coincide to β = 1) and report best results in Figure 6. We find that either
alternative is strictly worse than TDU. They suffer a significant drop in performance on exploration
tasks, but are also less able to handle noise and reward scaling. Because the only difference between
QU and TDU is that in TDU, σ conditions on the next state. Thus, there results are in direct support
of Proposition 2 and demonstrates that Vθ [δ | τ] is likely to have less bias than Vθ [Qθ (s, a)].
Ablation: bandit policy sampling Our main results indicate, unsurprisingly, that different envi-
ronments require different emphasis on exploration. To test this more concretely, in this experiment
we replace uniform policy sampling with the UCB1 bandit algorithm. However, in contrast to that
example, where UCB1 is used to take actions, here it is used to select a policy for the next episode.
We treat each N + K value function as an “arm” and estimate its mean reward V k ≈ Eπk [r], where
the expectation is with respect to rewards r collected under policy πk (∙) = arg max a Qk (∙, a). The
mean reward is estimated as the running average
1 n(k)
Vk (n)=西 X ri,
(54)
where n(k) is the number of environment steps for which policy πk has been used and ri are the
observed rewards under policy ∏k. Prior to an episode, we choose a policy to act under according
to: arg max k=ι,…,n+k Vk(n) + η/log n/n(k), where n is the total number of environment steps
taken so far and η is a hyper-parameter that we tune. As in the bandit example, this sampling strategy
biases selection towards policies that currently collect higher reward, but balances sampling by a
count-based exploration bonus that encourages the agent to eventually try all policies. This bandit
mechanism is very simple as our purpose is to test whether some form of adaptive sampling can
provide benefits; more sophisticated methods (e.g. Schaul et al., 2019) can yield further gains.
24
Under review as a conference paper at ICLR 2021
cartpole swingup
Figure 7: Bsuite per-task results. Results reported for different values of β with prior λ = 3. We also
report results under UCB1 policy sampling (“bandit”) for β = 1, λ = 3, η = 8.
We report full results in Figure 7; we use β = 1 and tune η ∈ {0.1, 1, 2, 4, 6, 8}. We report results
for the hyper-parameter that performed best overall, η = 8, though differences with η > 4 are
marginal. While TDU does not impact performance negatively in general, in the one case where it
does—Mountain Car—introducing a bandit to adapt exploration can largely recover performance.
The bandit yields further gains in dense reward settings, such as in Cartpole and Catch, with an
outlying exception in the bandit setting with scaled rewards.
E Atari with R2D2
E.1 Bootstrapped R2D2
We augment the R2D2 agent with an ensemble of dueling action-value heads Qi. The behavior policy
followed by the actors is an -greedy policy as before, but where the greedy action is determined
according to a single Qi for a fixed length of time (100 actor steps in all of our experiments), before
sampling a new Qi uniformly at random. The evaluation policy is also -greedy with = 0.001,
where the Q-values are averaged only over the exploiter heads.
Each trajectory inserted into the replay buffer is associated with a binary mask indicating which Qi
will be trained from this data, ensuring that the same mask is used every time the trajectory is sampled.
Priorities are computed as in R2D2, except that TD-errors are now averaged over all heads.
25
Under review as a conference paper at ICLR 2021
Instead of using reward clipping, R2D2 estimates a transformed version of the state-action value
function to make it easier to approximate for a neural network. One can define a transformed Bellman
operator given any squashing function h : R → R that is monotonically increasing and invertible.
We use the function h : R 7→ R defined by
h(z) = Sign(Z)(P∣z∣ + 1 - 1) + ez,	(55)
h-1(z) = Sign(Z) ( (p1 + 4e(lz2+1 + e)3! - l) ,	(56)
for e small. In order to compute the TD errors accurately we need to account for the transformation,
δ(θ, s, a,r, s0) := γh-1(Qθ(s0, π(s0))) + r - h-1(Qθ(s, a)).	(57)
Similarly, at evaluation time we need to apply h-1 to the output of each head before averaging.
When making use of a prior we use the form Qk = Qθk + λPk, where Pk is of the same architecture
as the Qθk network, but with the widths of all layers cut to reduce computational cost. Finally, instead
of n-step returns we utilise Q(λ) (Peng & Williams, 1994) as was done in (Guez et al., 2020). In all
variants we used the hyper-parameters listed in Table 4.
E.2 Pre-processing
We used the standard pre-process of the frames received from the Arcade Learning Environment.6
See Table 5 for details.
E.3 Hyper-parameter Selection
In the distributed setting we have three TDU-
specific hyper-parameters to tune namely: β,
N and the prior weight λ. For our main results,
we run each agent across 8 seeds for 20 billions
steps. For ablations and hyper-parameter
tuning, we ran agents across 3 seeds for 5
billion environment steps on a subset of 8
games: frostbite,gravitar, hero,
montezuma_revenge,	ms_pacman,
seaquest, space_invaders, venture.
This subset presents quite a bit of diversity
including dense-reward games as well as
three hard exploration games: gravitar,
Table 5: Atari pre-processing hyperparameters
Max episode length	30 min
Num. action repeats	4
Num. stacked frames	4
Zero discount on life loss	false
Random noops range	30
Sticky actions	false
Frames max pooled	3and4
Grayscaled/RGB	Grayscaled
Action set	Full
montezuma_revenge and venture. To minimise the computational cost, we started by
setting λ and N while maintaining β = 1. We employed a coarse grid of λ ∈ {0., 0.05, 0.1} and
N ∈ {2, 3, 5}. Figure 8 summarises the results in terms of the mean Human Normalised Scores
(HNS) across the set. We see that the performance depends on the type of games being evaluated.
Specifically, hard exploration games achieve a significantly lower score. Performance does not
significantly change with the number of explorers. The largest differences are observed for the
exploration games when N = 5. We select best performing sets of hyper parameters for TDU with
and without additive priors: (N = 2, λ = 0.1) and (N = 5, λ = 0), respectively.
We evaluate the influence of the exploration bonus strength by fixing (N = 5, λ = 0) and choosing
β ∈ {0.1, 1., 2.}. Figure 9 summarises the results. The set of dense rewards is composed of the
games in the ablation set that are not considered hard exploration games. We observe that larger
values of β help on exploration but affect performance on dense reward games. We plot jointly the
performance in mean HNS acting when averaging the Q-values for both, the exploiter heads (solid
lines) and the explorer heads (dotted lines). We can see that higher strengths for the exploration
bonus (higher β) renders the explorers “uninterested” in the extrinsic rewards, preventing them
to converge to exploitative behaviours. This effect is less strong for the hard exploration games.
6Publicly available at https://github.com/mgbellemare/Arcade-Learning-Environment.
26
Under review as a conference paper at ICLR 2021
Figure 8:	Ablation for prior scale, λ and the number of explorers, N , on the distributed setting. We
fix β = 1. Refer to the text for details on the ablation and exploration set of games.
Figure 9:	Ablation for the exploration bonus strength, β, on the distributed setting. We fix (N =
5, λ = 0). We report the mean HNS for the ensemble of exploiter (solid lines) and the ensemble of
explorers (dotted lines). All runs are average over three seeds per game. Refer to the text for details
on ablation and exploration set of games.
Figure 10 we show how this effect manifests itself on the performance on three games: gravitar,
space_invaders, and hero. This finding also applies to the evaluation performed on our
evaluation using all 57 games in the Atari suite, as shown below. We conjecture that controlling for
the strength of the exploration bonus on a per game manner would significantly improve the results.
This finding is in line with observations made by (PUigdomenech Badia et al., 2020); combining
TDU with adaptive policy sampling (Schaul et al., 2019) or online hyper-parameter tuning (Xu et al.,
2018; Zahavy et al., 2020) are exciting avenUes for fUtUre research.
27
Under review as a conference paper at ICLR 2021
Environment steps 1e9
Environment steps 1e9
Figure 10: Ablation for the exploration bonus strength, β, on the distributed setting. We fix (N =
5, λ = 0). We report the score on three different games for the ensemble of exploiter (solid lines) and
the ensemble of explorers (dotted lines). All runs are average over three seeds per game.
40000
30000
20000
10000
0
hero
0	12	3	4
Environment steps 1e9
E.4 Detailed Results: Main Experiment
In this section we provide more detailed results from our main experiment in Section 5.2. We concen-
trated our attention on the subset of games that are well-known to pose challenging exploration prob-
lems (Machado et al., 2018): montezuma_revenge, pitfall, private_eye, solaris,
venture, gravitar, and tennis. We also add a varied set of dense reward games.
Figure 11 shows the performance for each game. We can see that TDU always performs on par
or better than each of the baselines, leading to significant improvements in data efficiency and
final score in games such as montezuma_revenge, private_eye, venture, gravitar,
and tennis. Gains in exploration games can be substantial, and in montezuma_revenge,
private_eye, venture, and gravitar, TDU without prior functions achieves statistically
significant improvements. TDU with prior functions achieve statistically significant improvements on
montezuma_revenge, private_eye, and gravitar. Beyond this, both methods improve
the rate of convergence on seaquest and tennis, and achieve higher final mean score. Overall,
TDU yields benefits across both dense reward and exploration games, as summarised in Figure 12.
Note that R2D2’s performance on dense reward games is deflated due to particularly low scores on
space_invaders. Our results are in line with the original publication, where R2D2 does not
show substantial improvements until after 35 Bn steps.
E.5 Full Atari suite
In this section we report the performance on all 57 games of the Atari suite. In addition to the two
configurations used to obtain the results presented in the main text (reported in Section 5.2), in this
section we included a variant of each of them with lower exploration bonus strength of λ = 0.1.
In all figures we refer to these variants by adding an L (for lower λ) at the end of the name, e.g.
TDU-R2D2-L. In Figure 13 we report a summary of the results in terms of mean HNS and median
HNS for the suite as well as mean HNS restricted to the hard exploration games only. We show
the performance on each game in Figure 14. Reducing the value of β significantly improves the
mean HNS without strongly degrading the performance on the games that are challenging from an
exploration standpoint. The difference in performance in terms of mean HNS can be explained by
looking at a few high scoring games, for instance: assault, asterix, demon_attack and
gopher (see Figure 14). We can see that incorporating priors to TDU is not crucial for achieving
high performance in the distributed setting.
28
Under review as a conference paper at ICLR 2021
1000000
800000
600000
400000
200000
seaquest
00000
75000
50000
25000
frostbite
10000
⅛0000
10000
ms_PaCman
'60000
10000
20000
SPaCe_invaders
0夕
0.0
0.5	1.0	1.5	2.0
gravitar 1e10
0 I
0.0	0.5
1.0
hero
15000
40000
10000
30000
5000
0
0.0
0.5
1.0
Solaris
1.5	2.0 0.0	0.5
1e10
1.0
15000
10000
5000
80000
60000
40000
20000
0),0	0.5	1.0	1.5	2.0
1e10
0.0
0
1.5	2.0	0.0
2500
2000
1500
1000
500
0.5
1.0
venture
1.5	2.
1e10
0 I I I	-I
0.0	0.5	1.0	1.5	2.0
montezuma_revenge1e10
4000
2000
1.5
2.0
1e10
0
0.0
20
10
?10
0.5	1.0	1.5	2.0	0.0
1e10
0.5	1.0	1.5	2.
tennis 1e10
0.5	1.0	1.5	2.0
1e10
0.0
0.5	1.0	1.5	2.0
1e10
----TDU-R2D2-PRIOR
TDU-R2D2
R2D2
B-R2D2
0
0
Figure 11:	Performance on each game in the main experiment in Section 5.2. Shading depicts
standard deviation over 8 seeds.
Environment steps 1e10	Environment steps 1e10	Environment steps 1e10
Figure 12:	Performance across all games in the main experiment in Section 5.2. We report mean
HNS over the full set of games used in the main experiment, dense reward games, and exploration
games. Shading depicts standard deviation over 8 seeds.
29
Under review as a conference paper at ICLR 2021
Table 4: R2D2 hyperparameters.
Ensemble size
Optimizer
Learning rate
Adam epsilon
Adam beta1
Adam beta2
Adam global clip norm
Discount
Batch size
Trace length
Replay period
Burn in length
λ for RL loss
R2D2 reward transformation
Replay capacity (num of sequences)
Replay priority exponent
Importance sampling exponent
Minimum sequences to start replay
Actor update period
Target Q-network update period
Evaluation
10
Adam (Kingma & Ba, 2015)
0.0002
0.001
0.9
0.999
40
0.997
64
80
40
20
0.97	_______
Sign(X) ∙ (p|x| + 1 — 1) + 0.001 ∙ x
1e5
0.9
0.6
5000
100
400
0.001
Environment steps 1e10	Environment steps 1e10	Environment steps 1e10
Figure 13: Performance over the 57 atari games. We report mean and median HNS over the full suite,
and mean HNS over the exploration games.
30
Under review as a conference paper at ICLR 2021
asteroids
Ult
1000000
80000
500000
800000
1250000
60000
600000
1000000
40000
	
400000
750000
20000
500000
battlezone
beamrider
250
60000
100000
200
80000
40000
150
60000
20000
100
40000
1000000
250000
500000
800000
400000
600000
200000
300000
400000
150000
200000
200000
40000
30000
20000
10000
20
400000
300000
10
200000
100000
1000000
750000
500000
250000
20
80000
60000
40000
20000
bowling
85
defender
125000
100000
fishing_derby
Kostbite
100000
ice hockey
jamesbond
kangaroo
200000
14000
50000
800001
600001
400000
200001
40
25000
0
seaquest
10000
-29500
5000
■30000
time_pilot
tι Itankham
UpndOwn
1000000
400
500001
1800
400000
250000
Wizardofwor
TDU-R2D2-PRIOR
50000
TDU-R2D2-PRIOR-L
600000
40000
TDU-R2D2
400000
TDU-R2D2-L
30000
R2D2
200000
B-R2D2
Environment steps
0.0	0.2	0.4	0.6	0.8	1.0
Environment steps 1e10
200000
centipede
Crazydimber
100000
75000
50000
ong
-10
10
100000
120000
115000
110000
20000
15000
10000
5000
40000
30000
20000
12000
10000
105000
150000
100000
name_this_game
privateeye
60000
40000
20000
-28500
-29000
400000
300000
200000
100000
800000
0.4	0.6	0.8	1.0
Environment steps 1e10
yarsrevenge
0
nven^aid
30000
20000
15000
60000
40000
20000
venture
2000
robotank
surround
750000
500000
300000
0
Figure 14: Results for each individual game. Shading depicts standard deviation over 3 seeds.
31