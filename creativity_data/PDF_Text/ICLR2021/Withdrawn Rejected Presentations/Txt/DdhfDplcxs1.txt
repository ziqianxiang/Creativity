Under review as a conference paper at ICLR 2021
OT-LLP: Optimal Transport for Learning from
Label Proportions
Anonymous authors
Paper under double-blind review
Ab stract
Learning from label proportions (LLP), where the training data are arranged in form of
groups with only label proportions provided instead of the exact labels, is an impor-
tant weakly supervised learning paradigm in machine learning. Existing deep learning
based LLP methods pursue an end-to-end learning fashion and construct the loss using
Kullback-Leibler divergence, which measures the difference between the prior and poste-
rior class distributions in each bag. However, unconstrained optimization on this objective
can hardly reach a solution in accordance with the given proportions at the bag level. In
addition, concerning the probabilistic classifier, it probably results in high-entropy condi-
tional class distributions at the instance level. These issues will further degrade the perfor-
mance of instance-level classification. To address these problems, we propose to impose
the exact proportions on the classifier with a constrained optimization, and firstly apply
the optimal transport algorithm to solve LLP. With the entropic regularization, our formu-
lation allows to solve a convex programming efficiently and further arrive at an integer
solution that meets the proportion constraint strictly. More importantly, our framework
is model-agnostic, and demonstrates compelling performance improvement in extensive
experiments, when it is incorporated into other deep LLP models as a post-hoc stage.
1	Introduction
Learning from label proportions (LLP) is a weakly supervised classification problem with only the label
proportions in grouped data available. Still, training LLP aims to obtain an instance-level classifier for the
new-come inputs. Successfully resolving LLP problems greatly contribute to many real-life applications:
demographic classification (Ardehaly & Culotta, 2017), US presidential election (Sun et al., 2017), embryo
implantation prediction (Hernandez-Gonzalez et al., 2018), spam filtering (KUck & de Freitas, 2012), video
event detection (Lai et al., 2014), visual attribute modeling (Chen et al., 2014; Yu et al., 2014a), and traffic
flow prediction (Liebig et al., 2015).
On the one hand, the learnability of LLP strongly depends on the groUping of instances, as well as the
proportions distribUtion. For example, YU et al. (2014b) have stUdied the instance-level expected risk of
empirical proportions risk minimization (EPRM) algorithm for LLP, with respect to the nUmber of bags, the
bag size, and the prior label distribUtion within the bags. They have proved that LLP is learnable with the
EPRM principle and given the boUnd of expected learning risk.
On the other hand, EPRM strives for minimizing bag-level label proportions error. Normally, this goal is
achieved by minimizing KUllback-Leibler (KL) divergence between prior and posterior class distribUtions
in each bag. However, bag-level proportional information provides too insUfficient constraints to perfectly
solve LLP, becaUse too many instance-level classifiers can satisfy proportional constraints exactly. In other
words, when considering instance-level classification, LLP is ill-posed. As a conseqUence of this Underdeter-
mination, despite a nUmber of achievements have been developed to resolve LLP accUrately and efficiently,
1
Under review as a conference paper at ICLR 2021
it is still of great challenge to design an effective learning scheme to significantly improve the performance
of high dimensional instances recognition, e.g., images, merely with bag-level proportional information.
In addition, acquiring the exact labeling congruous with label proportions somehow leads to a problematic
integer programming problem (Stolpe & Morik, 2011). As a result, advanced LLP algorithms usually impose
the relaxation to achieve probabilistic labeling, which is also used as the final classifier in an alternate
labeling-and-classifying framework (Yu et al., 2013). However, in terms of the probabilistic classifier, the
aforementioned ill-posed situation may be even worse: Infinite solutions are in accordance with the bag-
level proportions. In general, suboptimal hypothesis can be probably found. In particular, in the extreme
case where proportions are equal in each bag, it will lead to a trivial solution with uniform output distribution.
In this paper, we argue that the above challenge is mainly due to the inadequate KL divergence at the
bag level, and propose to solve LLP through explicitly combing unsupervised clustering and proportional
information. To be concrete, the unsupervised learning helps to discover the data clusters specifically cor-
responding to the classes. However, naively applying clustering without supervision will easily result in a
trivial solution to assign all the instances to the same cluster. Fortunately, we can avoid this degeneration
by imposing appropriate constraints on the label distribution (Caron et al., 2018). For example, when there
is no knowledge on label distribution, we can impose discrete uniform distribution to the labels, leading to
equal clustering. Besides, in a semi-supervised learning protocol, the clustering result can be restricted with
the help of the labeled instance (Asano et al., 2019). Similarly, in LLP scenario, the constraint for clustering
can be the label proportions. Hence, our solution is to construct a constrained optimization problem by
confining the feasible solution to accurately comply the proportions in each bag. In order to tackle the weak
supervision, we cast a framework by considering the classification and pseudo labeling within one objective,
and leverage the label proportions as the constraints. To implement the proposed schema, we alternately up-
date the network parameters and the pseudo labels, where optimal transport (OT) is employed to the pseudo
labeling process and standard cross-entropy minimization is adopted to train the network parameters.
In summary, our main contributions are four-fold: (1) We propose a novel LLP framework called OT-LLP,
which leverage the exact proportional information to construct a constrained optimization, and is an essen-
tially orthogonal LLP treatment to the existing LLP algorithms; (2) We propose an alternate optimization
process to solve the optimization on both the classifier and pseudo labeling, and firstly apply the OT algo-
rithm to obtain integer solutions for the labels; (3) Our framework is model-agnostic, and we demonstrate
that it can easily fit for various deep-based LLP algorithms to further boost their performance; (4) With no
additional hyper-parameter involved, our framework achieves state-of-the-art LLP performance on several
benchmark datasets, based on the neural network pipeline with standard settings.
2	Related Work
To our knowledge, four end-to-end pipelines are recently proposed for LLP, using deep neural networks as
the backbone architectures. Specifically, Ardehaly & Culotta (2017) first propose an end-to-end LLP algo-
rithm, incorporating the KL divergence of prior and posterior proportions into an objective. Although their
approach can learn a competent instance-level classifier, it is hardly in accordance with the proportions in
training data, especially with large bag sizes (e.g., > 64). Based on guessing the individual labels of samples
(Yu et al., 2013), Dulac-Arnold et al. (2019) employ a similar instance-level cross entropy loss as the objec-
tive. Then, alternating update and convex relaxation are exploited to mitigate the intractable combinatorial
optimization. However, the inaccurate labeling problem is still intact, due to the KL divergence loss.
Recently, by introducing the adversarial learning mechanism, LLP-GAN (Liu et al., 2019) greatly improves
the method in (Ardehaly & Culotta, 2017). In detail, the discriminator is designed as a (K+1)-way classifier,
where the first K classes indicate multi-class true samples, and the (K+1)th class accounts for the generated
samples. The main insight is to implement better representation learning through the adversarial mechanism,
2
Under review as a conference paper at ICLR 2021
thus boosting the downstream discriminative task performance. Despite of the substantial improvement in
performance compared with previous methods, LLP-GAN greatly suffers from the training instability, which
inherits the characteristics from generative adversarial networks. Furthermore, subtle network structure
design and hyper-parameters selection are required, in order to obtain satisfactory results. Similarly, Tsai &
Lin (2019) introduce a consistency regularization technique in semi-supervised learning to the multi-class
LLP problem, where a KL divergence on the proportions is also employed in the loss.
The above four algorithms all build their losses (partly) on the KL divergence, and solve an unconstrained
optimization. We advocate that their final classifiers cannot fully agree with the proportions, thus deteri-
orating the classification performance at the instance level. In contrast, we address this problem with a
constrained optimization, which can exactly follow the proportion constraint.
Furthermore, enough diversity in unlabeled data demonstrates useful behavior for classification (Lu et al.,
2018). Based on the great ability of deep models, unsupervised representation learning is achievable and
appealing as the essential step towards discriminative tasks. For example, aligning the clustering result
with semantic classes can be successfully applied to image classification and segmentation (Ji et al., 2019).
Asano et al. (2019) have proposed to apply (entroPic constrained) optimal transport (OT) (Peyre et al., 2019)
to unsupervised representation learning with a self-labeling mechanism. Following their seminal framework,
in this paper, we propose an OT based LLP algorithm, and develop more detailed techniques.
3	Preliminaries
In order to clearly describe how to leverage OT for LLP, we first introduce several preliminaries for both OT
and LLP. More details are in Appendix due to the space limitation.
3.1	Optimal transport via Kantorovich relaxation
In Appendix A.1, we describe the Kantorovich formulation of coupling as an extension for the mass trans-
portation in the Monge problem (10) (see Appendix A.1). Different from that mass transportation should be
deterministic, Kantorovich relaxation instead considers a probabilistic transport, which allows mass splitting
from a source toward several targets (Villani, 2008). Note that in this way, we may get rid of the challenge
in the Monge problem, because the source measure is regarded as the atom in the Monge problem while
separable in the Kantorovich relaxation with respect to the source measure.
To achieve the mass splitting, instead of a permutation σ or a surjective map T , we need a coupling matrix
P ∈ Rn+×m, where Pij is the amount of mass flowing from xi to yj. Admissible couplings give a simpler
characterization than maps in the Monge problem as: Suppose that a ∈ Σn, b ∈ Σm, we denote U (a, b) =
nP ∈ Rn+×m | P1m = a, P|1n = bo, where Σs is a probability simplex, i.e., Σs = na ∈ Rs+ P ai = 1o,
and 1n represents the column vector of all ones with dimension n.
Remark 1 (Peyre et al. (2019)). The Kantorovich mapping is symmetric: P ∈ U (a, b) ⇔ PT ∈ U (b, a).
Similar to the Monge problem (10), let(•, •)be the FrobeniUS dot-product, the Kantorovich,s optimal ITans-
port (OT) problem is a linear programming problem given a, b ∈ Σn and a cost matrix C as
LC(a,b) = min hP, Ci =	PijCij
P∈U(a,b)	i,j
(1)
An important feature of OT (1) is that it induces a distance between two probability measures in Σn, which
are both discrete in this paper, as soon as the cost matrix satisfies the properties of a legitimate distance.
Proposition 1 (Villani (2008)). Let D ∈ R+×n be a distance on JnK = {1, 2,…,n} and P > 1. We can
define the p-Wasserstein distance on Σn as Wp(a, b) = LDp (a, b)1/p, ∀a, b ∈ Σn.
3
Under review as a conference paper at ICLR 2021
Remark2. Particularly, Wι(a, b) = minp∈u(a,b)E(χ,y)〜P [∣∣x — yk] is a legitimate distance between two
distributions, and is used Wasserstein GAN, as a weaker distance than the Jensen-Shannon (JS) Divergence
in the original GAN and Kullback-Leibler (KL) divergence in the maximizing likelihood estimation (MLE).
3.2	Entropic regularization of optimal transport
The solution of the original transport problem (1) is non-unique and tends to be sparse, i.e., arriving at certain
vertex of the polytope U(a, b). In certain scenarios, the sparsity of optimal couplings for (1) is not desirable,
so Cuturi (2013) instead employs the entropic regularization term to form a more “blurred” prediction. The
discrete entropy of a coupling matrix P is well-known as H(P) = — Pi,j Pij log(Pij).
Remark 3 (Peyre et al. (2019)). The entropic function H(∙) is 1-strongly concave, due to the negative
definite Hessian matrix: ∂2H(P) = —diag(1./P) and Pij 6 1, ∀i, j, where we flatten P to a long vector.
Note that ab| is an admissible coupling, and H(P) 6 H(a) + H(b) = H(ab ). The idea of the entropic
regularization of OT is to add —H(∙) to the original OT (1) to obtain an approximate solutions as:
LεC(a,b) = min hP, Ci — εH(P),	(2)
P∈U (a,b)
which is a constrained minimization problem ofan ε-convex function, thus has a unique optimal solution.
More importantly, the entropic constraint for OT can guarantee a more computationally efficient process to
find the solution, as a consequence of restricting the search for low cost joint probabilities within sufficient
smooth tables (Cuturi, 2013). In addition, we introduce Proposition 2 in Appendix A.2 to confirm that the
solution of entropic regularized OT (2) converges to that of the original OT (1) as ε → 0.
3.3	Learning from label proportions
In LLP problem, because the label proportions are available, we can restrict the instance-level self-labeling
procedure with these proportional information using an OT framework. Before further discussion, we fist
give the formal formulation for LLP by directly considering a multi-class problem with K classes in this
paper. With no prior knowledge, we further suppose that the training data consist of N randomly generated
disjoint bags. Consequently, the training data can be expressed as D = {(Bi, pi)}im=1, where Bi = {xi,j}jn=i 1
denotes the instances in the ith bag, and Bi ∩ Bj = 0, ∀i = j. The Pi ∈ [0,1]K and n are the known
ground-truth label proportions and the bag size of the ith bag, respectively.
4	Approach
4.1	Linking OT to LLP (OT-LLP)
In Appendix A.3, we introduce how to leverage equal clustering to learn discriminative representation on
unsupervised data and achieve classification in a self-labeling framework (Asano et al., 2019). In LLP
problem, although the class distribution is not uniform within each bag, we can easily modify the constraint
in (18) (see Appendix A.3) or the admissible couplings in (19) (see Appendix A.3) to fit in the proportional
information. Consequently, with piy as the proportion of class y in bag i, we convert (18) into
m ni K
min BCE(p, q) = —XXX
i=1 j=1 y=1
q(y |xi,j )
ni
log Pφ(y∣χi,j),
ni
St Xq(y∖χi,j) = Py ∙ ni,q(y∣∙) ∈ [0, i],∀y ∈ JKK,∀i ∈ JmK∙
j=1
(3)
4
Under review as a conference paper at ICLR 2021
Note that (3) is a constrained optimization and the labels should strictly comply with the proportion infor-
mation of each bag. Nevertheless, (3) is combinatorial in q and thus seems to be very difficult to optimize.
Fortunately, we can convert (3) to a typical OT problem, which can be solved relatively efficiently.
In order to better explain the relation between (3) and OT, we rewrite it in a matrix fashion. Formally, let
Qi = (Qjk) ∈ RK×ni, Qjk = q(k∖xi,j )/ni, and Pi = (Pj k) ∈ R?ni ,Pjk= Pφ(k∣Xi,j )/m. We further denote
Q = diag{Qi}m=ι, P = diag{Pi}m=ι, and P =(p|, p|,…，PmjI b =(叽/nι,叽加2,…，吗皿/nm)∣
Accordingly, We can define a set With the form of U(p, b) = {Q ∈ RmKxN ∖ Q1N = p, Ql1mK = b}.
Then, we can give an equivalent problem for (3) with the following OT problem:
m
min hQ, -logPi = BCE(p, q) + logYni.	(4)
Q∈U(p,b)
i=1
On the other hand, With λ → +∞, We can instead solve the entropic regularized OT problem to accelerate
the process of convergence, as Well as avoiding the non-unique sparse solution.
L"gP(P, b) = C min jl∕q, - logPi- 1 H(Q).
Q∈U (p,b)	λ
(5)
4.2	Alternating optimization
In the proposed learning framework, the network parameters φ =(夕，θ) and self-labels Q are alternately
updated. NoW, We further describe the details as folloWs.
Training the network with fixed q: Because the cross-entropy is differentiable with respect to the network
parameters φ =(夕,θ), we can directly conduct common optimizer, e.g., ADAM, on the objective in (3) by
fixing q.
Updating the labels Q with fixed φ: When the model is fixed, the label assignment matrix Q are obtained
by OT or entropic regularized OT. When performing the original OT, the solution Q* is with binary elements
of 0 or 1. However, when performing entropic regularized OT, the elements of Q* are in [0,1]. In practice,
we employ two strategies for label update: hard labeling and soft labeling. In hard labeling, we update Q as:
Qijs = 10,,
if s = arg max q(k∖xi,j)
k
otherwise
i = 1, 2,…，m.
(6)
In soft labeling, we directly use the labels obtained by entropic regularized OT. In the experimental part,
we provide the performance comparison on hard and soft labeling: Hard labeling strategy outperforms the
soft labeling in convolution network while the soft labeling is superior to hard labeling in fully-connected
network. As a results, we employ the hard labeling result to report the final performance for CIFAR-10 and
CIFAR-100, while soft labeling for MNIST, F-MNIST, and K-MNIST.
4.3	The entropic regularized OT based LLP algorithm (EROT-LLP)
In practice, we consider the LLP problem in every single bag, and perform the clustering in one bag, with the
proportions as the constraint for instances number in each cluster. In detail, we conduct the constrained OT
problem (4) with respect to Qi and Pi, with minor revision on U(P, b), i.e., U(Pi, bi), where bi = 1ni/ni.
On the other hand, we can instead solve the entropic regularized OT problem (5) with the same revision as
(7) to accelerate the training, as well as obtain non-sparse solution to perform soft labeling:
LR P(Pi，bi) = Qi∈miptb/a-logPi- 1 H(Qi).
(7)
5
Under review as a conference paper at ICLR 2021
Based on the Sinkhorn’s algorithm for the entropic regularized OT (Cuturi, 2013), we describe the entropic
regularized OT based LLP algorithm, LLP-EROT, in Algorithm 1, as a realization of OT-LLP framework.
Algorithm 1 LLP based on the entropic regularized OT (LLP-EROT)
Require: D = {(Bi, pi)}im=1, λ ∈ (0, +∞), the threshold ε > 0, and δ > ε, the initialization P∆ =
diag{Pi}m=ι = diag{lκ×nJ(niK)}m=ι, V(O) = 1八“ and bi = n1- 1出.
1:	while δ > ε do
2:	for each i ∈ JmK do
3:	Solve the entropic regularized OT problem (7) by iteratively update to obtain Qi for the assignment
of bag i, using Qli = diag{u(l)}Kλdiag{v(l)}, with Kλ = exp{λ log Pi} and
U(I) = pi.∕Kλv(I) and v(l+1) = bi.∕(Kλ)lu(I)	(8)
4:	Qi = lim Q∖. (The convergence is element-wise and proved in (Peyre et al., 2019), Remark 4.8.)
l→+∞
5:	end for
6:	Combine {Qi}im=1 as the diagonal element to obtain the block diagonal matrix Q = diag{Qi}im=1.
7:	Fixing Q, solve the following unconstrained programming (9) with respect to the network parameters
φ =(夕，θ) on the whole training data:
min
φ=(ψ,θ)
1NK
ce(pφ, q) = - NEEq(y∣χi )iog Pφ(y∣χi).
i=1 y=1
(9)
8:	δ=kP-P∆kF.
9:	P∆ = P.
10:	end while
Ensure: The final network parameters φ =(夕，θ).
5 Numerical Experiments
In order to demonstrate that our proposed OL-LLP framework is model-agnostic, in this section, we conduct
extensive numerical experiments to study the improvement of former LLP methods, when combined with
OT-LLP. The evaluation strategy is to conduct a two-phase training, where the first phase is to train with the
former models, and OT-LLP is intergrated in the second phase. Four benchmark datasets: MNIST, Fashion-
MNIST, CIFAR-10, and CIFAR-100, are used in our experiments. The comparisons are performed on two
recently proposed algorithms DLLP (Ardehaly & Culotta, 2017) and LLP-GAN (Liu et al., 2019).
5.1	Experimental setting
5.1.1	Label proportions generation
As there is no off-the-shelf LLP datasets, we first generate the bag-based LLP datasets, and obtain the
label proportions with four supervised benchmark datasets. Following the setting from Liu et al. (2019),
we construct four kinds of bags, with bag sizes of 16, 32, 64, and 128, respectively. In order to avoid the
influence of different label distributions, the bag setting is fixed across different algorithms.
6
Under review as a conference paper at ICLR 2021
5.1.2	Training setting
We choose a 5-hidden-layer fully connected network for MNIST, K-MNIST, and F-MNIST, and a conv-
based 13-layer max-pooling network for CIFAR-10. The details of network architectures are given in Ap-
pendix A.4. Meanwhile, ADAM optimizer is used with β1 =0.5 and β2 = 0.999. The initial learning rate is
1e-4 consistently for all datasets, divided by 2 for every 100 epochs. Data augmentation is employed for
CIFAR-10 and CIFAR-100 by random horizontal flip and random crop with padding the original images.
5.2	Overall accuracy
We first provide the overall accuracy of DLLP and OT-LLP on five datasets in Table 1. As introduced above,
OT-LLP means a two-stage training, where the first stage is to train the KL divergence based DLLP as the
teacher model, and the second one is to update the student model based on the proposed OT-LLP framework
(c.f. Algorithm 1). In practice, the first stage can be any other previous deep LLP algorithms (Dulac-Arnold
et al., 2019; Liu et al., 2019; Tsai & Lin, 2019), then using our model to further boost their performance.
Table 1: Test accuracy rates and standard deviations (%) on benchmark datasets with different bag sizes					
Dataset	Algorithm	16	Bag 32	Size 64	128
MNIST	DLLP OT-LLP	98.47 (0.09) 98.63 (0.03)	98.40 (0.10) 98.59 (0.04)	98.01 (0.16) 98.35 (0.06)	97.14 (0.15) 97.82 (0.09)
F-MNIST	DLLP OT-LLP	88.36 (0.29) 89.31 (0.12)	87.01 (0.23) 87.89 (0.11)	85.53 (0.28) 86.75 (0.21)	82.93 (0.21) 83.98 (0.33)
K-MNIST	DLLP OT-LLP	92.58 (0.22) 92.95 (0.15)	92.03 (0.23) 92.44 (0.23)	89.01 (0.29) 90.31 (0.21)	82.14 (0.28) 82.54 (0.35)
CIFAR-10	DLLP OT-LLP	88.78 (0.37) 90.55 (0.31)	84.29 (0.66) 88.24 (0.31)	66.65 (1.19) 76.26 (0.48)	39.14 (0.78) 44.88 (0.51)
CIFAR-100	DLLP OT-LLP	63.47 (0.61) 66.21 (0.46)	48.50 (0.66) 59.66 (0.34)	1.65 (0.19) 4.86 (0.18)	1.14 (0.18) 2.27 (0.19)
In Table 1, we observe significant improvement on DLLP by adding OT-LLP as second stage. In particular,
the advantage is more apparent for CIFAR-10 and CIFAR-100, which are two harder datasets compared with
the other three.
5.3	Combining with other method
As shown above, our model is orthogonal to previous KL-based LLP algorithms, and can be combined
with these methods by adding OT-LLP as the second phase. In this section, we further investigate the
improvement with our framework on datasets F-MNIST and CIFAR-10 when the first stage is LLP-GAN
(Liu et al., 2019), which is the currently SoTA LLP solver. The final performance is shown in Figure 1,
where the performance of LLP-GAN is obtained within 300 epochs. Similar to Table 1, our model can also
boost LLP-GAN by a large margin.
5.4	Hard-label vs. Soft-label
In our framework, we can employ two pseudo labeling strategies: hard-label and soft-label. The update
details are shown in (6). To further study the difference of these two labelings, we compare their performance
under different bag sizes on K-MNIST, CIFAR-10, and CIFAR-100. Furthermore, the performance of first
7
Under review as a conference paper at ICLR 2021
(a) F-MNIST
(b) CIFAR-10
Figure 1:	The accuracy rate (%) comparison with LLP-GAN on F-MNIST and CIFAR-10.
(c) CIFAR-100
(a) K-MNIST	(b) CIFAR-10
Figure 2:	The accuracy rate (%) convergence curves with hard and soft labelings.
stage is fixed with soft-label and hard-label for fair comparison. The results are shown in Figure 2, where
we provide the convergence curve of accuracy in the second stage for both hard and soft labelings.
From Figure 2, we can find that the performance with hard labeling is superior to that with soft labeling on
CIFAR-10 and CIFAR-100, while soft labeling outperforms hard labeling on K-MNIST. Indeed, soft labels
are more informative than hard labels. However, it may lead to unstable training process by directly using
the outputs of entropic regularized OT, thus degrading the performance with convolution networks.
6 Conclusion
In this paper, we analyze the common challenge in existing LLP approaches, and point out that the mini-
mization on the KL divergence between the prior and posterior class distributions is inadequate to comply
with the label proportional information exactly. From this perspective, we propose to solve the LLP problem
with a framework to combine instance-level classification and pseudo labeling, and alternately fulfill the
optimization on these two objectives. Compared with the former LLP solvers, the main improvement in
our method is the introduction of pseudo labeling, which converts the KL divergence based unconstrained
optimization into a constrained one, so that the resulting labeling can strictly meet the proportional infor-
mation and avoid suboptimal solutions. Thanks to OT and its entropic regularized variant, the above two
processes can be efficiently conducted with a line search optimizer (e.g., ADAM) on differentiable objective
and Sinkhorn’s algorithm for entropic regularized OT, respectively. In the experimental part, by integrating
OT-LLP as the second phase, we elaborately demonstrate that our framework can further improve the per-
formance of DLLP and LLP-GAN, thus is model-agnostic. Besides that, we empirically study the difference
of hard and soft labeling strategies in our framework, and provide suggestions for the practical usage.
8
Under review as a conference paper at ICLR 2021
References
Ehsan Mohammady Ardehaly and Aron Culotta. Co-training for demographic classification using deep
learning from label proportions. In IEEE International Conference on Data Mining Workshops, pp. 1017-
1024, 2017.
Yuki Markus Asano, Christian Rupprecht, and Andrea Vedaldi. Self-labelling via simultaneous clustering
and representation learning. arXiv preprint arXiv:1911.05371, 2019.
Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised
learning of visual features. In Proceedings of the European Conference on Computer Vision (ECCV), pp.
132-149, 2018.
Tao Chen, Felix X Yu, Jiawei Chen, Yin Cui, Yan-Ying Chen, and Shih-Fu Chang. Object-based visual
sentiment concept analysis and application. In Proceedings of the 22nd ACM international conference on
Multimedia, pp. 367-376, 2014.
Marco Cuturi. Sinkhorn distances: Lightspeed computation of optimal transport. In Advances in neural
information processing systems, pp. 2292-2300, 2013.
Gabriel Dulac-Arnold, Neil Zeghidour, Marco Cuturi, Lucas Beyer, and Jean-Philippe Vert. Deep multi-class
learning from label proportions. arXiv preprint arXiv:1905.12909, 2019.
Jeronimo Hernandez-Gonzalez, Inaki Inza, Lorena Crisol-Ortlz, Maria A Guembe, Maria J Inarra, and
Jose A Lozano. Fitting the data from embryo implantation prediction: Learning from label proportions.
Statistical Methods in Medical Research, pp. 1056-1066, 2018.
XU Ji, Joao F Henriques, and Andrea Vedaldi. Invariant information clustering for unsupervised image clas-
sification and segmentation. In Proceedings of the IEEE International Conference on Computer Vision,
pp. 9865-9874, 2019.
Hendrik Kuck and Nando de Freitas. Learning about individuals from group statistics. arXiv preprint
arXiv:1207.1393, 2012.
Kuan-Ting Lai, Felix X Yu, Ming-Syan Chen, and Shih-Fu Chang. Video event detection by inferring
temporal instance labels. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 2243-
2250, 2014.
Thomas Liebig, Marco Stolpe, and Katharina Morik. Distributed traffic flow prediction with label propor-
tions: from in-network towards high performance computation with MPI. In International Conference on
Mining Urban Data-Volume 1392, pp. 36-43. CEUR-WS. org, 2015.
Jiabin Liu, Bo Wang, Zhiquan Qi, Yingjie Tian, and Yong Shi. Learning from label proportions with gen-
erative adversarial networks. In Advances in Neural Information Processing Systems, pp. 7167-7177,
2019.
Nan Lu, Gang Niu, Aditya Krishna Menon, and Masashi Sugiyama. On the minimal supervision for training
any binary classifier from only unlabeled data. arXiv preprint arXiv:1808.10585, 2018.
Gabriel Peyre, Marco Cuturi, et al. Computational optimal transport: With applications to data science.
Foundations and Trends in Machine Learning, 11(5-6):355-607, 2019.
Marco Stolpe and Katharina Morik. Learning from label proportions by optimizing cluster model selection.
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pp. 349-
364. Springer, 2011.
9
Under review as a conference paper at ICLR 2021
Tao Sun, Dan Sheldon, and Brendan O’Connor. A probabilistic approach for learning with label proportions
applied to the US presidential election. In IEEE International Conference on Data Mining (ICDM), pp.
445-454. IEEE, 2017.
Kuen-Han Tsai and Hsuan-Tien Lin. Learning from label proportions with consistency regularization. arXiv
preprint arXiv:1910.13188, 2019.
Cedric Villani. Optimal transport: old and new, volume 338. Springer Science & Business Media, 2008.
Felix X Yu, Dong Liu, Sanjiv Kumar, Tony Jebara, and Shih-FU Chang. αSVM for learning with label
proportions. In International Conference on Machine Learning, 2013.
Felix X Yu, Liangliang Cao, Michele Merler, Noel Codella, Tao Chen, John R Smith, and Shih-Fu Chang.
Modeling attributes from category-attribute proportions. In Proceedings of the 22nd ACM international
conference on Multimedia, pp. 977-980, 2014a.
Felix X Yu, Krzysztof Choromanski, Sanjiv Kumar, Tony Jebara, and Shih-Fu Chang. On learning from
label proportions. arXiv preprint arXiv:1402.5902, 2014b.
10