Under review as a conference paper at ICLR 2021
Efficient Graph Neural Architecture Search
Anonymous authors
Paper under double-blind review
Ab stract
Recently, graph neural networks (GNN) have been demonstrated effective in vari-
ous graph-based tasks. To obtain state-of-the-art (SOTA) data-specific GNN archi-
tectures, researchers turn to the neural architecture search (NAS) methods. How-
ever, it remains to be a challenging problem to conduct efficient architecture search
for GNN. In this work, we present a novel framework for Efficient GrAph Neural
architecture search (EGAN). By designing a novel and expressive search space,
an efficient one-shot NAS method based on stochastic relaxation and natural gra-
dient is proposed. Further, to enable architecture search in large graphs, a transfer
learning paradigm is designed. Extensive experiments, including node-level and
graph-level tasks, are conducted. The results show that the proposed EGAN can
obtain SOTA data-specific architectures, and reduce the search cost by two orders
of magnitude compared to existing NAS baselines.
1	Introduction
Recent years have witnessed the success of graph neural networks (GNN) (Gori et al., 2005;
Battaglia et al., 2018) in various graph-based tasks, e.g., recommendation (Ying et al., 2018a), chem-
istry (Gilmer et al., 2017), circuit design (Zhang et al., 2019), subgraph counting (Liu et al., 2020),
and SAT generation (You et al., 2019). To adapt to different graph-based tasks, various GNN mod-
els, e.g., GCN (KiPf & Welling, 2016), GAT (VelickoVic et al., 2018), or GIN (XU et al., 2019),
have been designed in the past five years. Most existing GNN models follow a neighborhood ag-
gregation (or message passing) schema (Gilmer et al., 2017), as shown in the left Part of FigUre 1,
which is that the rePresentation ofa node in a graPh is learned by iteratiVely aggregating the featUres
of its neighbors. DesPite the broad aPPlications of GNN models, researchers haVe to take efforts
to design ProPer GNN architectUres giVen different tasks by imPosing different relational indUctiVe
biases (Battaglia et al., 2018). As Pointed oUt by Battaglia et al. (2018), the GNN architectUres can
sUPPort one form of combinatorial generalization giVen different tasks, i.e., graPhs. Then a natUral
and interesting qUestion can be asked: Can we automatically design state-of-the-art (SOTA) GNN
architectures for graph-based tasks? A straightforward solUtion is to adoPt the neUral architectUre
search (NAS) aPProaches, which haVe shown Promising resUlts in aUtomatically designing archi-
tectUres for conVolUtional neUral networks (CNN) (ZoPh & Le, 2017; Pham et al., 2018; LiU et al.,
2019a; Tan & Le, 2019; YoU et al., 2020a).
HoweVer, it is nontriVial to adoPt NAS to GNN. The first challenge is to define the search sPace.
One can design a dummy search sPace to inclUde as many as Possible the related Parameters, e.g.,
aggregation fUnctions, nUmber of layers, actiVation fUnctions, etc., on toP of the message Passing
framework (Eq. (1)), howeVer, it leads to qUite a large discrete sPace, for examPle, 315,000 Possi-
ble GNN architectUres are generated by inclUding jUst 12 tyPes of model Parameters in YoU et al.
(2020b)), which is challenging for any search algorithm. The second challenge is to design an
effectiVe and efficient search algorithm. In the literatUre, reinforcement learning (RL) based and
eVolUtionary based algorithms haVe been exPlored for GNN architectUre search (Gao et al., 2020;
ZhoU et al., 2019; Lai et al., 2020; NUnes & PaPPa, 2020). HoweVer, they are inherently comPU-
tationally exPensiVe dUe to the stand-alone training manner. In the NAS literatUre, by adoPting the
weight sharing strategy, one-shot NAS methods are orders of magnitUde more efficient than RL
based ones (Pham et al., 2018; LiU et al., 2019a; Xie et al., 2019; GUo et al., 2019). HoweVer, the
one-shot methods cannot be directly aPPlied to the aforementioned dUmmy search sPace, since it
remains Unknown how to search for some model Parameters like nUmber of layers and actiVation
fUnctions by the weight sharing strategy. Therefore, it is a challenging Problem to condUct effectiVe
and efficient architectUre search for GNN.
1
Under review as a conference paper at ICLR 2021
Figure 1: An illustrative example of a GNN model and the propsed EGAN (Best view in color). (a)
An example graph with five nodes. The gray rectangle represents the input features of each node;
(b) A typical 3-layer GNN model following the message passing neighborhood aggregation schema,
which computes the embeddings of node “2”; (c) The DAG represents the search space for a 3-layer
GNN, and αn , αl , αs represent, respectively, weight vectors for node aggregators, layer aggrega-
tors, and skip-connections in the corresponding edges. The rectangles denote the representations,
out of which three green ones represent the hidden embeddings, gray (hv0) and yellow (h5v) ones rep-
resent the input and output embeddings, respectively, and blue one (h4v) represent the set of output
embeddings of three node aggregators for the layer aggregator. (d) At the t-th epoch, an architecture
is sampled from p(Zn), p(Zs), p(Zl), whose rows Zi,j are one-hot random variable vector indicat-
ing masks multipled to edges (i, j) in the DAG. Columns of these matrices represent the operations
fromOn,Os,Ol.
[2,3) O O
ol ol
In this work, we propose a novel framework, called EGAN (Efficient GrAph Neural architecture
search), to automatically design SOTA GNN architectures. Motivated by two well-established works
(Xu et al., 2019; Garg et al., 2020) that the expressive capabilities of GNN models highly rely
on the properties of the aggregation functions, a novel search space consisting of node and layer
aggregators is designed, which can emulate many popular GNN models. Then by representing the
search space as a directed acyclic graph (DAG) (Figure 1(c)), we design a one-shot framework by
using the stochastic relaxation and natural gradient method, which can optimize the architecture
selection and model parameters in a differentiable manner. To enable architecture search in large
graphs, we further design a transfer learning paradigm, which firstly constructs a proxy graph out
of the large graph by keeping the properties, and then searches for GNN architectures in the proxy
graph, finally transfers the searched architecture to the large graph. To demonstrate the effectiveness
and efficiency of the proposed framework, we apply EGAN to various tasks, from node-level to
graph-level ones. The experimental results on ten different datasets show that EGAN can obtain
SOTA data-specific architectures for different tasks, and at the same time, reduce the search cost by
two orders of magnitude. Moreover, the transfer learning paradigm, to the best of our knowledge, is
the first framework to enable architecture search in large graphs.
Notations. Let G = (V, E) be a simple graph with node features X ∈ RN ×d, where V and E
represent the node and edge sets, respectively. N represents the number of nodes and d is the
dimension of node features. We use N (v) to represent the first-order neighbors of a node v in G,
i.e., N(v) = {u ∈ V|(v, u) ∈ E}. In the literature, we also create a new set Ne(v) is the neighbor
set including itself, i.e., N(v) = {v} ∪ {u ∈ V|(v, u) ∈ E}.
2	Related Works
GNN was first proposed in (Gori et al., 2005) and in the past five years different GNN models (Kipf
& Welling, 2θl6; Hamilton et al., 2017; VelickoVic et al., 2018; Gao et al., 2018; Battaglia et al.,
2
Under review as a conference paper at ICLR 2021
2018; Xu et al., 2019; 2018; Liu et al., 2019b; Abu-El-Haija et al., 2019; Wu et al., 2019; Zeng
et al., 2020; Zhao & Akoglu, 2020; Rong et al., 2020) have been designed, and they rely on a
neighborhood aggregation (or message passing) schema (Gilmer et al., 2017), which learns the
representation of a given node by iteratively aggregating the hidden features (“message”) of its
neighbors. Besides, in Xu et al. (2018); Chen et al. (2020), the design of residual networks (He
et al., 2016a;b) are incorporated into existing message passing GNN models. Battaglia et al. (2018)
pointed out that the GNN architectures can provide one form of combinatorial generalization for
graph-based tasks, and Xu et al. (2019); Garg et al. (2020) further show that the expressive capability
of existing GNN models is upper bounded by the well-known Weisfeiler-Lehman (WL) test. It will
be an interesting question to explore GNN architectures with better combinatorial generalization,
thus neural architecture search (NAS) can be a worthwhile approach for this consideration.
NAS (Baker et al., 2017; Zoph & Le, 2017; Elsken et al., 2018) aims to automatically find SOTA
architectures beyond human-designed ones, which have shown promising results in architecture
design for CNN and Recurrent Neural Network (RNN) (Liu et al., 2019a; Zoph et al., 2018; Tan
& Le, 2019). Existing NAS approaches can be roughly categorized into two groups according to
search methods (Bender et al., 2018): i.e. the stand-alone and one-shot ones. The former ones tend
to obtain the SOTA architecture from training thousands of architectures from scratch, including
reinforcement learning (RL) based (Baker et al., 2017; Zoph & Le, 2017) and evolutionary-based
ones (Real et al., 2019), while the latter ones tend to train a supernet containing thousands of archi-
tectures based on the weight sharing strategy, and then extract a submodel as the SOTA architecture
at the end of the search phase (Pham et al., 2018; Bender et al., 2018; Liu et al., 2019a; Xie et al.,
2019). The difference of training paradigms leads to that one-shot NAS methods tend to be orders
of magnitude more efficient than the RL based ones.
Recently, there are several works on architecture search for GNN, e.g., RL based ones (Gao et al.,
2020; Zhou et al., 2019; Lai et al., 2020), and evolutionary-based ones (Nunes & Pappa, 2020; Jiang
& Balaprakash, 2020), thus all existing works are computationally expensive. Franceschi et al.
(2019) proposes to jointly learn the edge probabilities and the parameters of GCN given a graph,
thus orthogonal to our work. Besides, Peng et al. (2020) proposes a one-shot NAS method for
GCN architectures in the human action recognition task. In Li et al. (2020), a sequential greedy
search method based on DARTS (Liu et al., 2019a) is proposed to search for GCN architectures,
however, the tasks are more focused on vision related tasks, with only one dataset in conventional
node classification task. In this work, to the best of our knowledge, for conventional node-level
and graph-level classification tasks, we are the first to design a one-shot NAS method for GNN
architecture search, which is thus, in nature, more efficient than existing NAS methods for GNN. In
Appendix A.3, we give more detailed discussions about the comparisons between EGAN and more
recent GNN models.
3	The Proposed Framework
3.1	The design of search space
As introduced in Section 2, most existing GNN architectures rely on a message passing frame-
work (Gilmer et al., 2017), which constitutes the backbone of the designed search space in this
work. To be specific, a K-layer GNN can be written as follows: the l-th layer (l = 1, ∙∙∙ ,K)
updates hv for each node v by aggregating its neighborhood as
hVl) = σ(W(l) ∙ Φn({hUlT),∀u ∈ Ne(v)})),	⑴
where h(vl) ∈ Rdl represents the hidden features of a node v learned by the l-th layer, and dl is
the corresponding dimension. W(l) is a trainable weight matrix shared by all nodes in the graph,
and σ is a non-linear activation function, e.g., a sigmoid or ReLU. Φn is the key component, i.e., a
pre-defined aggregation function, which varies across different GNN models.
Thus a dummy search space is to include as many as possible related parameters in Eq. (1). How-
ever, it leads to a very large search space, making the search process very expensive. In this work,
motivated by two well-established works (Xu et al., 2019; Garg et al., 2020), which show that the
expressive capabilities of GNN models highly rely on the properties of aggregation functions, we
propose to search for different aggregation functions by simplifying the dummy search space. For
other parameters, we do simple tuning in the re-training stage, which is also a standard practice in
3
Under review as a conference paper at ICLR 2021
existing NAS methods (Liu et al., 2019a; Xie et al., 2019). Then the first component of the proposed
search space is the node aggregators, which consists of existing GNN models. To improve the ex-
pressive capability, we add the other component, layer aggregators, to combine the outputs of node
aggregator in all layers, which have been demonstrated effective in JK-Network (Xu et al., 2018).
Then we introduce the proposed search space, as shown in Figure 1(c), in the following:
•	Node aggregators: We choose 12 node aggregators based on popular GNN models, and they are
presented in Table 7 in Appendix A.1. The node aggregator set is denoted by On.
•	Layer aggregators: We choose 3 layer aggregators as shown in Table 7 in Appendix A.1. Be-
sides, we have two more operations, IDENTITY and ZERO, related to skip-connections. Instead
of requiring skip-connections between all intermediate layers and the final layer in JK-Network, in
this work, we generalize this option by proposing to search for the existence of skip-connections
between each intermediate layer and the last layer. To connect, we choose IDENTITY, and ZERO
otherwise. The layer aggregator set is denoted by Ol and the skip operation set by Os .
To further guarantee that K-hop neighborhood can always be accessed, we add one more constraint
that the output of the node aggregator in the last layer should always be used as the input of the layer
aggregator, thus for a K-layer GNN architecture, we need to search K - 1 IDENTITY or ZERO for
the skip-connection options.
3.2	Differentiable architecture search
Following existing NAS works (Liu et al., 2019a; Xie et al., 2019), we represent the search space by
a directed acyclic graph (DAG), as shown in Figure 1(c), where nodes represent embeddings, and
edges represent operations between the two end nodes. Then the intermediate nodes are
Xj=	O i,j (Xi),	⑵
i<j
where Oi,j is the selected operation at edge (i, j). In our work, each edge corresponds to an opera-
tion in the search space, and we represent it with a distribution pα(Z), which generates the one-hot
random variable Zi,j multiplied by the operation edge Oi,j in the DAG. Then the intermediate nodes
in each child graph are represented by
Xj = Xi<j Oi,j (Xi) = Xi<j (Zij)TOi,j (Xi).	⑶
Note that in our framework, as shown in Figure 1(c), for each node aggregator, the input is from the
previous one, and for the layer aggregators, the input are from outputs of all node aggregators.
Following the setting in Zoph & Le (2017) and Gao et al. (2020), the objective of the framework is
EZ〜Pa(Z) [R(Z)] = EZ〜Pa(Z) [Lw亿)],	(4)
where R(Z) represents the reward, which is defined by training loss LW(Z) in our framework. W
represents the model parameters. In the GNN literature, node-level or graph-level classification
tasks are commonly used, thus the cross-entropy loss is chosen, leading to a differentiable function
of LW(Z). To make use of the differentiable nature of LW(Z), we design a differentiable search
method to optimize Eq. (4). To be specific, we use the Gumbel-Softmax (Maddison et al., 2017;
Xie et al., 2019; Noy et al., 2020) to relax the discrete architecture distribution to be continuous and
differentiable with the reparameterization trick:
Zkj = fαi,j (Gk,j) = exp((log αk,j + GijMKl Xn=0 exp((log + Gllj)/λ)	(5)
where Zi,j is the softened one-hot random variable for operation selection at edge (i, j), and Gik,j =
- log(- log(Uik,j)) is the k-th Gumbel random variable, Uik,j is a uniform random variable. αi,j
is the architecture parameter. λ is the temperature of the softmax, which is steadily annealed to
be close to zero (Xie et al., 2019; Noy et al., 2020). Then we can use gradient descent methods
to optimize the operation parameters and model parameters together in an end-to-end manner. The
gradients are given in Appendix A.2.
To improve the search efficiency, we further design an adaptive stochastic natural gradient method
to update the architecture parameters in an end-to-end manner following Akimoto et al. (2019). To
be specific, the update of α at the m-th iteration is given as:
am+1 = αm - PHZaL,	(6)
4
Under review as a conference paper at ICLR 2021
where P is the step size. H is the Fisher matrix, which can be computed as H = Epam [pα (Z)Pa (Z)T]
with TPa(Z) := V logPa(Zh
After the searching process terminates, we derive the final architecture by retaining the edge with the
largest weight, which is the same as existing DARTS (Liu et al., 2019a) and SNAS (Xie et al., 2019).
To make the final results more robust, the search process is executed 5 times with different random
seeds, thus 5 architectures are obtained at the end of the search phase. Then the 5 architectures are
re-trained from scratch with some hyperparameters tuning on the validation set, and the one with
the best validation accuracy is returned as the final architecture.
3.3	Transfer learning paradigm
As introduced in Hamilton et al. (2017); Jia et al. (2020), when training GNN in large graphs, in each
batch, the time and memory cost increases exponentially w.r.t. K, i.e., the number of GNN layers,
with the worst cases of O(|V |). Obviously, it is extremely expensive in large graphs for any GNN
model. The situation becomes more severe when conducting architecture search in large graphs
since we are training a supernet emulating various GNN models. Therefore, it tends to be infeasible
to directly search for architectures in large graphs. Motivated by transferring searched blocks and
cells in CNN architectures from small to large data sets in the NAS literature (Zoph et al., 2018; Tan
& Le, 2019), we propose to address the above problem by transfer learning (Pan & Yang, 2009).
The core idea of the transferable architecture search is to find a small proxy graph Gproxy (the source),
then search in the proxy graph, finally tune the searched architecture {αn, αs, αl } in the large
graph G (the target). However, in order to make the architecture transfer feasible, we need to make
the proxy graph sharing the same property distribution with the original graph (Pan & Yang, 2009).
Since the properties vary across different graphs, itis not suitable to transfer across different datasets,
like that from CIFAR-10 to ImageNet for image classification (Zoph et al., 2018). Thus, we pro-
pose to sample a smaller graph from the original one, and then apply the transfer paradigm. Many
distribution-preserving sampling schema have been proposed in an established work (Leskovec &
Faloutsos, 2006), e.g., random sampling by node or edge, or sampling by PageRank. In this work,
we adopt the Random PageRank Node (RPN) sampling method in Leskovec & Faloutsos (2006),
which is empirically demonstrated to be able to preserve the properties by sampling not less than
15% nodes from the original graph. In Section 4.2.2, the experimental results shows that this transfer
paradigm empirically works well.
Table 1: Comparisons between existing NAS methods for GNN and the proposed EGAN.
	Search space		Search Algorithm	Able to run in large graphs
	Node agg	Layer agg		
GraPhNAS (Gao et al., 2020)-	√	X	RL	X
AutO-GNN (ZhOu et al., 2019)	√	X	RL	X
POliCy-GNN (Lai et al., 2020)	√	X	RL	X
Nunes & Pappa (2020)	√	X	Evolutionary	X
Jiang & Balaprakash (2020)	√	X	Evolutionary	X
EGAN	√	√ 一	Differentiable	√
3.4	Comparisons with existing NAS methods for GNN
In this section, as shown in Table 1, we emphasize the advantages of EGAN in the following:
•	In terms of the search space, EGAN can emulate more GNN models than existing methods. More-
over, by only focusing on the “aggregation function”, the total size of the search space is smaller
than those of the previous methods, which also contributes to the efficiency improvements of EGAN.
•	In terms of the search algorithm, the one-shot nature of EGAN makes it much more efficient than
stand-alone methods, e.g. GraphNAS.
•	The transfer paradigm of EGAN makes it feasible to conduct architecture search in large graphs.
Therefore, the advantages of EGAN over existing NAS methods are evident, especially the effi-
ciency.
4	Experiments
In this section, we conduct extensive experiments to demonstrate the effectiveness and efficiency of
the proposed EGAN, including node-level and graph-level tasks.
5
Under review as a conference paper at ICLR 2021
4.1	Setup
Datasets. For node-level tasks, we have three settings: transductive, inductive, and transfer. The
task is node classification on 8 datasets, which are given in Appendix A.4.1. For graph-level tasks,
the task is whole graph classification on 2 datasets, which are given in Appendix A.5.
Baselines In general, we have two types of baselines: human-designed GNN models, and NAS
methods. Details of baselines are given in Appendix A.4.2. Note that for NAS baselines in Table 1,
we only use GraphNAS (Gao et al., 2020) and its variant using weight sharing (GraphNAS-WS).
The search spaces of Auto-GNN (Zhou et al., 2019) and Nunes & Pappa (2020) are the same as
GraphNAS, while their codes are not available. Jiang & Balaprakash (2020) is an concurrent work
when we are preparing for this submission, and we will compare with it when the codes are available.
For Policy-GNN (Lai et al., 2020), they work on searching for different numbers of layers per node
in a selected GNN base model, i.e., GCN or GAT, thus it can be an orthogonal work to EGAN.
Table 2: Performance comparisons in transductive and inductive tasks, whose evaluation metrics
are mean classification accuracy and Micro-F1, respectively. For the first five GNN models in the
table, we present the better performance of each and its JK variants, and the detailed performance
of GNN models and their JK variants are in Table 10 in Appendix A.4.5. Note that the performance
of LGCN on CS and Computer is “-” due to the OOM (Out Of Memory) problem when running the
released code in our GPUs. The results of Geom-GCN are copied from the original paper (Pei et al.,
2020), since the data split ratio is the same to our experiments.
		Transductive	Inductive
	Methods	Cora	CiteSeer	PUbMed	CS	Computer	PPI
Human- designed GNN	GCN GraphSAGE GAT GIN GeniePath LGCN Geom-GCN	0.877(0.012) 0.771(0.014) 0.878(0.004) 0.949(0.003) 0.912(0.007) 0.884(0.002) 0.765(0.005) 0.882(0.007) 0.952(0.002) 0.918(0.004) 0.873(0.009) 0.753(0.013) 0.867(0.006) 0.934(0.004) 0.918(0.005) 0.870(0.010) 0.765(0.013) 0.883(0.005) 0.950(0.003) 0.913(0.008) 0.878(0.012) 0.759(0.014) 0.882(0.004) 0.926(0.004) 0.883(0.007) 0.869(0.008) 0.754(0.022) 0.875(0.001)	-	- 0.852*	0.780*	0.901*	*	*	0.934(0.001) 0.972(0.001) 0.978(0.001) 0.964(0.003) 0.964(0.001) 0.772(0.002) *
NAS methods	Random Bayesian GraphNAS GraphNAS-WS	0.869(0.003) 0.782(0.002) 0.889(0.001) 0.939(0.001) 0.902(0.003) 0.858(0.003) 0.765(0.002) 0.884(0.001) 0.943(0.001) 0.908(0.001)	0.988(0.001) 0.990(0.001)
		0.884(0.007) 0.776(0.006) 0.890(0.002) 0.928(0.002) 0.900(0.003) 0.881(0.010) 0.761(0.016) 0.884(0.010) 0.931(0.002) 0.906(0.002)	0.970(0.013) 0.958(0.042)
IEGAN	10.900(0.001) 0.786(0.011) 0.900(0.001) 0.960(0.003) 0.920(0.003)10.991(0.000)			
4.2	Performance comparison
4.2	. 1 Performance comparisons in Transductive and Inductive Tasks
From Table 2, we can see that EGAN consis-
tently obtain better or close performance com-
pared to all baselines, which demonstrates the
effectiveness of the proposed framework. In
other words, with EGAN, we can obtain SOTA
data-specific GNN architectures. When compar-
ing EGAN with GraphNAS methods, the per-
formance gain is evident. We attribute this to
the expressive search space and the differen-
tiable search algorithm. We further visualize
the searched architectures in Figure 2, and we
can see that the searched architectures vary per
dataset. More figures can be checked in Figure 6
and 7 in Appendix A.6.3.
Figure 2: The searched architectures on
Cora and CiteSeer.
4.2.2	Performance comparisons in Transfer Tasks
For the transfer learning experiments, we use two large graphs, i.e., Reddit and Arixv. As introduced
in Section 3.3, we firstly sample two smaller graphs (15% nodes), and run EGAN in these two
smaller graphs to obtain the optimal architectures, then transfer the searched architectures to the
original graphs, finally report the test results in them. In terms of baselines, we only report the
6
Under review as a conference paper at ICLR 2021
results of those, which are able to, or reported to, run in the two large graphs, i.e., Reddit and Arxiv.
More details are given in Appendix A.4.4.
The results are shown in Table 3, from which we can see that the improvement of search cost is in two
orders of magnitude, from 3 GPU days1 to less than a minute, which demonstrates the superiority of
efficiency of the proposed transfer learning paradigm compared to direct architecture search in large
graphs. Moreover, the performance of the transferred architectures is better or close to those of the
baselines, which demonstrates the effectiveness of the transfer learning paradigm.
Table 3: Performance comparisons of the transfer task on Reddit and Arxiv, for which we use Micro-
F1 and accuracy as evaluation metrics, respectively. Note that the OOM (Out Of Memory) problem
occurs when running GraphNAS on these two datasets, thus we do not report its performance.
I	Reddit	∣					Arxiv	
	Method	Micro-F1	Search cost (GPU hours)	Method	Accuracy	Search cost (GPU hours)
Human-designed	GraPhSAGE	0.938(0.001)	-	GCN	0.717(0.003)	-
GNN	GraphSAGE-JK 0.942(0.001)		-	GraphSAGE 0.715(0.003)		-
NAS methods	Random	0.930(0.002)	72	Random	0.691(0.005)	72
	Bayesian	0.942(0.001)	72	Bayesian	0.703(0.004)	72
IEGAN		0.954(0.000)	0.021	EGAN	0.715(0.001)	0.003
4.2.3	Graph-level tasks
The results of the graph-level task
are shown in Table 4, and we can
see that the performance trending
is similar to node-level tasks. The
searched architectures are shown
in Figure 7 in Appendix A.6.3,
which also shows that they are data
specific. Note that the global pool-
ing method, or the readout func-
tion, in the whole graph learning
can also be incorporated into the
search space of EGAN, thus it can
also be learned. We leave this for
future work.
Taking into consideration the re-
sults of all tasks, the effective-
ness of the proposed EGAN can be
demonstrated.
4.3	Search efficiency
Table 4: Performance comparisons on graph classification
tasks. We show the mean test accuracy (with standard de-
viation) on these datasets. For the first five GNN models in
the table, we present the better performance of each and its
JK variants, and the detailed performance of GNN models
and their JK variants are in Table 12 in Appendix A.5.3
I Methods ∣ D&D ∣ PROTEINS
	GCN	0.733(0.043)	0.730(0.026)
	GraphSAGE	0.734(0.029)	0.734(0.041)
Human-	GAT	0.716(0.056)	0.745(0.030)
designed	GIN	0.733(0.033)	0.737(0.048)
GNN	GeniePath	0.705(0.051)	0.694(0.035)
	DiffPool	0.779(0.045)	0.738(0.040)
	SAGPOOL	0.762(0.009)	0.724(0.041)
	ASAP	0.748(0.024)	0.733(0.024)
	Random	0.742(0.043)	0.731(0.031)
NAS NAS	Bayesian	0.746(0.031)	0.676(0.041)
metos	GraphNAS	0.719(0.045)	0.725(0.031)
	GraphNAS-WS	0.758(0.044)	0.752(0.025)
IEGAN	10.779(0.034) 10.761(0.039)
In this section, we conduct some experiments to show the superiority of efficiency of EGAN over
NAS baselines. And for simplicity, we only use the four commonly used datasets in the node-level
task, i.e., Cora, CiteSeer, PubMed, and PPI.
Firstly, we record the running time of each method during the search phase, which represents the
search cost of the NAS methods. The results are given in Table 5, from which we can see that the
search cost of EGAN is two orders of magnitude smaller than those of NAS baselines.
Secondly, we show the trending of the test accuracy w.r.t. the running time of different methods
during the search phase. In each epoch, we obtain the best model currently, and report the test
accuracy after retraining it from scratch. The result of Cora is shown in Figure 3, from which we
1Note that we stop the search process after 3 days.
7
Under review as a conference paper at ICLR 2021
can observe that EGAN can obtain architectures with better performance more quickly than NAS
baselines. More figures are shown in Figure 5 in Appendix A.6.2.
Taking these results into consideration, the efficiency advantage of EGAN is significant,
which is mainly attributed to the one-shot training paradigm as introduced in Section 3.
	0.91∏ 0.90 J	I. I
Transductive task	Inductive task	⅛ 0.89 J	J rΓ~
Cora^^CiteSeer^^PUbMed	PPI	u 0.88 J	I一Γ——EGAN
Random	1,500	2,694	3,174	13,934	M 0.87^	-I	] 	 Random
Bayesian	1,631	2,895	4,384	14,543	6861		Bayesian I	 GraphNAS
GraphNAS 3,240	3,665	5,917	15,940		
EGAN	14	35	54	298		0	1	2	3	4	5 Elapse time(s) in log base 10
Table 5: The running time (s) of each method during the search phase.	Figure 3: Test accuracy w.r.t. elapsed time on Cora.
4.4	Ablation study
In this section, we present two ablation studies on EGAN.
Firstly, we show the importance of the layer aggregators in the designed search space by running
EGAN in the search space without the layer aggregators, and the results are shown in Table 6, from
which we can see that the performance consistently drops on all datasets except Computer when
removing the layer aggregators. This observation aligns with the results in JK-Network (Xu et al.,
2018) that the performance of GNN models can be improved by adding an extra layer.
Secondly, we show the influence of K, i.e., the number of layers of GNN, in the search space, for
which we conduct experiments with EGAN by varying layer K ∈ {1, 2, 3, 4, 5, 6} and show the test
accuracy in Figure 4. The results suggest that along with the increment of layers, the test accuracy
may decrease. Considering the computational resources, 3-layer architecture is a good choice for
the backbone of EGAN in our experiments.
	EGAN	
	layer aggregators(w) ∣ layer aggregators(w/o)	
Cora	0.900(0.001)	0.871(0.001)
CiteSeer	0.759(0.000)	0.751(0.000)
PubMed	0.900(0.001)	0.884(0.001)
CS	0.947(0.001)	0.937(0.001)
Computer	0.915(0.003)	0.918(0.001)
PPI	0.990(0.000)	0.974(0.000)
DD	0.779(0.034)	0.761(0.038)
Protein	0.761(0.039)	0.758(0.034)
Table 6: Performance comparisons of EGAN
using different search spaces.
Figure 4: Test accuracy w.r.t.
different K s.
5 Conclusion and Future work
In this paper, we propose EGAN, an effective and efficient framework for graph neural architecture
search. By designing a novel and expressive search space, we propose a one-shot NAS framework
by stochastic relaxation and natural gradient. Further, to enable architecture search in large graphs,
we design a transfer learning paradigm. We conduct extensive experiments, including node-level
and graph-level tasks, which demonstrates the effectiveness and efficiency of EGAN compared to
various baselines.
Based on this work, we show that NAS approaches can obtain data-specific GNN architectures,
which supports one form of combinatorial generalization for GNN models. For future work, we
will explore more aspects regarding the combinatorial generalization of GNN models beyond the
aggregation functions, like the construction of the graph, or the number of layers as done in Policy-
GNN (Lai et al., 2020), as introduced in Battaglia et al. (2018).
8
Under review as a conference paper at ICLR 2021
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional
architectures via SParsified neighborhood mixing. In ICML, pp. 21-29, 2019.
Youhei Akimoto, Shinichi Shirakawa, Nozomu Yoshinari, Kento Uchida, Shota Saito, and Kouhei
Nishida. Adaptive stochastic natural gradient method for one-shot neural architecture search. In
ICML, pp. 171-180, 2019.
Bowen Baker, Otkrist Gupta, Nikhil Naik, and Ramesh Raskar. Designing neural network architec-
tures using reinforcement learning. ICLR, 2017.
Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.
Relational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,
2018.
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc V. Le. Under-
standing and simplifying one-shot architecture search. In ICML, pp. 549-558, 2018.
James S Bergstra, Remi Bardenet, YoshUa Bengio, and Balazs KegL Algorithms for hyper-parameter
optimization. In NeurIPS, pp. 2546-2554, 2011.
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph
convolutional networks. In ICML, pp. 3730-3740, 2020.
Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal
neighbourhood aggregation for graph nets. 2020.
Paul D Dobson and Andrew J Doig. Distinguishing enzyme structures from non-enzymes without
alignments. Journal of molecular biology (JMB), 330(4):771-783, 2003.
Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.
JMLR, 2018.
Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In
ICLRW, 2019.
Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. Learning discrete structures
for graph neural networks. In ICML, pp. 1972-1982, 2019.
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional
networks. In KDD, pp. 1416-1424, 2018.
Yang Gao, Hong Yang, Peng Zhang, Chuan Zhou, and Yue Hu. Graph neural architecture search.
In IJCAI, pp. 1403-1409, 2020.
Vikas K Garg, Stefanie Jegelka, and Tommi Jaakkola. Generalization and representational limits of
graph neural networks. pp. 5204-5215, 2020.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In ICML, pp. 1263-1272, 2017.
Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.
In IJCNN, volume 2, pp. 729-734, 2005.
Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian
Sun. Single path one-shot neural architecture search with uniform sampling. arXiv preprint
arXiv:1904.00420, 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NeurIPS, pp. 1024-1034, 2017.
9
Under review as a conference paper at ICLR 2021
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In CVPR, pp. 770-778, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In ECCV., pp. 630-645, 2016b.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs, 2020.
Zhihao Jia, Sina Lin, Rex Ying, Jiaxuan You, Jure Leskovec, and Alex Aiken. Redundancy-free
computation for graph neural networks. In KDD, pp. 997-1005, 2020.
Shengli Jiang and Prasanna Balaprakash. Graph neural network architecture search for molecular
property prediction, 2020.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. ICLR, 2016.
Kwei-Herng Lai, Daochen Zha, Kaixiong Zhou, and Xia Hu. Policy-gnn: Aggregation optimization
for graph neural networks. In KDD, pp. 46-71, 2020.
JUnhyUn Lee, Inyeop Lee, and Jaewoo Kang. Self-attention graph pooling. In ICML, pp. 3734-3743,
2019.
Jure Leskovec and Christos Faloutsos. Sampling from large graphs. In KDD, pp. 631-636, 2006.
Guohao Li, Guocheng Qian, Itzel C Delgadillo, Matthias Muller, Ali Thabet, and Bernard Ghanem.
SGAS: Sequential greedy architecture search. In CVPR, pp. 1620-1630, 2020.
Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. ICLR,
2019a.
Xin Liu, Haojie Pan, Mutian He, Yangqiu Song, Xin Jiang, and Lifeng Shang. Neural subgraph
isomorphism counting. In KDD, pp. 1959-1969, 2020.
Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath:
Graph neural networks with adaptive receptive paths. In AAAI,, volume 33, pp. 4424^431,2019b.
Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous
relaxation of discrete random variables. ICLR, 2017.
Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja Giryes, and
Lihi Zelnik. Asap: Architecture search, anneal and prune. In AISTAT, pp. 493-503, 2020.
Matheus Nunes and Gisele L Pappa. Neural architecture search in graph neural networks. arXiv
preprint arXiv:2008.00077, 2020.
Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on Knowledge
andData Engineering (TKDE), 22(10):1345-1359, 2009.
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn: Geometric
graph convolutional networks. ICLR, 2020.
Wei Peng, Xiaopeng Hong, Haoyu Chen, and Guoying Zhao. Learning graph convolutional network
for skeleton-based human action recognition by neural searching. In AAAI, 2020.
Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search
via parameter sharing. In ICML, pp. 4092U101, 2018.
Ekagra Ranjan, Soumya Sanyal, and Partha P Talukdar. Asap: Adaptive structure aware pooling for
learning hierarchical graph representations. In AAAI, pp. 5470-5477, 2020.
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image
classifier architecture search. In AAAI, volume 33, pp. 4780-4789, 2019.
10
Under review as a conference paper at ICLR 2021
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In ICLR, 2020.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AImagazine, 29(3):93-93, 2008.
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural net-
works. In ICML, PP. 6105-6114, 2019.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. GraPh attention networks. ICLR, 2018.
Felix Wu, Amauri Souza, Tianyi Zhang, ChristoPher Fifty, Tao Yu, and Kilian Weinberger. Sim-
Plifying graPh convolutional networks. In International Conference on Machine Learning, PP.
6861-6871, 2019.
Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. SNAS: stochastic neural architecture search.
In ICLR, 2019.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and Stefanie
Jegelka. RePresentation learning on graPhs with jumPing knowledge networks. In ICML, PP.
5449-5458, 2018.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How Powerful are graPh neural
networks? In ICLR, 2019.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
GraPh convolutional neural networks for web-scale recommender systems. In KDD, PP. 974-983,
2018a.
Zhitao Ying, Jiaxuan You, ChristoPher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graPh rePresentation learning with differentiable Pooling. In NeurIPS, PP. 4800-4810,
2018b.
Jiaxuan You, Haoze Wu, Clark Barrett, Raghuram Ramanujan, and Jure Leskovec. G2SAT: Learning
to generate sat formulas. In NeurIPS, PP. 10553-10564, 2019.
Jiaxuan You, Jure Leskovec, Kaiming He, and Saining Xie. GraPh structure of neural networks. In
ICML, PP. 327-337, 2020a.
Jiaxuan You, Zhitao Ying, and Jure Leskovec. Design sPace for graPh neural networks. volume 33,
2020b.
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, RajgoPal Kannan, and Viktor Prasanna. GraPh-
Saint: GraPh samPling based inductive learning method. ICLR, 2020.
Guo Zhang, Hao He, and Dina Katabi. Circuit-GNN: GraPh neural networks for distributed circuit
design. In ICML, PP. 7364-7373, 2019.
Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. ICLR, 2020.
Kaixiong Zhou, Qingquan Song, Xiao Huang, and Xia Hu. Auto-GNN: Neural architecture search
of graPh neural networks, 2019.
Barret ZoPh and Quoc V Le. Neural architecture search with reinforcement learning. ICLR, 2017.
Barret ZoPh, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures
for scalable image recognition. In CVPR, PP. 8697-8710, 2018.
11
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Details of Node Aggregators
As introduced in Section 3.1, we have 12 types of node aggregators, which are based on well-
known existing GNN models: GCN Kipf & Welling (2016), GraphSAGE Hamilton et al. (2017),
GAT VelickoVic et al. (2018), GIN XU et al. (2019), and GeniePath LiU et al.(2019b). Here We
give key explanations to these node aggregators in Table 8. For more details, we refer readers to the
original papers.
Table 7: The operations We Use as node and layer aggregators for the search space of EGAN. Details
of node aggregators are giVen in Table 8.
I	Operations	
On	SAGE-SUM, SAGE-MEAN, SAGE-MAX, SAGE-LSTM, GCN, GAT,GAT-SYM, GAT-COS, GAT-LINEAR, GAT-GEN-LINEAR, GIN, GeniePath
Ol	CONCAT, MAX, LSTM
Os	IDENTITY, ZERO
Table 8: More explanations to the node aggregators in Table 7.
GNN models	Symbol in the paper	Key explanations
GCN (Kipf & Welling, 2016)	GCN	FN(V) =	P	(degree(v) ∙ degree(u))-1∕2 ∙ hU^1 u∈Nf(v)
GraphSAGE (Hamilton et al., 2017)	SAGE-MEAN, SAGE-MAX, SAGE-SUM, SAGE-LSTM	4 1	τ CE* κ	, ■	, ri I	iΓr / ∖ 1 Apply mean, max, sUm, or LSTM operation to {hu |u ∈ N (v)}.
GAT (Velickovic et al., 2018)	GAT	COmPUte attention score: eUVt = Leaky-ReLU(a∣^Wuhu ||Wvhvl )-
	GAT-SYM	esys = egat + egat euv	euv 丁 evu .	
	GAT-COS	eUVs = hWuhu, Wv hvi
	GAT-LINEAR	eUn = tanh(Wuhu + Wv hv )
	GAT-GEN-LINEAR	eUvn-lin = WG tanh(Wuhu + Wv hv )∙
GIN (XU et al., 2019)	GIN	FN (V)= MLP((I+ elτ)∙hvτ + P hU-1)∙ ∖	u∈ N(v)	)
LGCN (Gao et al., 2018)	Cnn	Use 1-D CNN as the aggregator.
GeniePath (LiU et al., 2019b)	GeniePath	Composition of GAT and LSTM-based aggregators
JK-NetWork (XU et al., 2018)		Depending on the base above GNN
A.2 GRADIENTS OF L
Here We giVe the gradients of Lw (Z) W.r.t. the hidden embeddings, model parameters, and the
architectUre parameters, Which is in the folloWing:
∂L -	= ∂xj	_ ^X	dL ZT dOm(xj )	
∂L dWΓ	∂L Zk	∂Oi,j(χi)' % i,j ∂Wk,j ,	(7)
∂L dakJ	∂L	1 =LOOTj(Xi)(δ(k0- k)-Zi,j)zk,jλar.	
For fUll deriVation, We refer readers to the appendix of Xie et al. (2019).
A.3 Discussion about recent GNN methods
Very recently there Were some neW GNN models proposed in the literatUre, e.g., MixHop (AbU-El-
Haija et al., 2019), Geom-GCN (Pei et al., 2020), GraphSaint (Zeng et al., 2020), DropEdge (Rong
et al., 2020), PariNorm (Zhao & AkoglU, 2020), PNA (Corso et al., 2020). We did not inclUde all
12
Under review as a conference paper at ICLR 2021
these works into the search space of EGAN, since they can be regarded as orthogonal works of
EGAN to the GNN literature, which means they are very likely to further increase the performance
when integrating with EGAN.
As shown in Eq. (1), the embedding of a node v in the l-th layer of a K-layer GNN is computed as:
hV = σ(Wl ∙ AGGnode({hU-1,∀U ∈ Ne(v)})).
From this computation process, we can summarize four key components of a GNN model: aggre-
gation function (AGGnode), number of layers (l), neighbors (N(v)), and hyperparameters (σ, dimen-
sion size, etc.), which decide the properties of a GNN model, e.g., the model capacity, expressive
capability, and prediction performance.
To be specific, EGAN mainly focus on the aggregation functions, which affect the expressive ca-
pability of GNN models. GraphSaint mainly focuses on neighbors selection in each layer, thus the
“neighbor explosion” problem can be addressed. Geom-GCN also focuses on neighbors selection,
which constructs a novel neighborhood set in the continuous space, thus the structural information
can be utilized. DropEdge mainly focuses on the depth of a GNN model, i.e., the number of layers,
which can alleviate the over-smoothing problem with the increasing of the number of GNN layers.
Besides the three works, there are more other works on the four key components, like MixHop (Abu-
El-Haija et al., 2019) integrating neighbors of different hops in a GNN layer, or PairNorm (Zhao &
Akoglu, 2020) working on the depth of a GNN models, and PNA (Corso et al., 2020) is a more
recent GNN model, which proposes a composition of multiple aggregation functions in each GNN
layer. Therefore, all these works can be integrated as a whole to improve each other. For example,
the DropEdge or Geom-GCN methods can further help EGAN in constructing more powerful GNN
models. With PNA, we can use our framework to help search for the combinations of multiple ag-
gregation functions in a single GNN layer, or include the PNA aggregator to our search space to
see whether it can help further enhance the final performance. This is what we mean “orthogonal”
works of EGAN. Since we mainly focus on the aggregation functions in this work, we only compare
the GNN variants with different aggregations functions.
Moreover, one purpose of this work is not to design the most powerful search space to include all
aspects, but to demonstrate that the proposed EGAN, including the search space and search method,
provides an alternative option to enhance GNN architecture search. We believe the application of
NAS to GNN has unique values, and the proposed EGAN can benefit the GNN community.
A.4 Experiment Setup of node-level tasks
A.4. 1 Datasets
Table 9: Dataset statistics of the datasets in the experiments. Here we use “CS”, “Computer”, as the
abbreviations for Coauthor CS and Amazon Computers, respectively.
	TransduCtive					Inductive	Transfer	
	Cora	CiteSeer	PubMed	CS	Computer	PPI	Reddit	Arxiv
#nodes	2,708	3,327	19,717	18,333	13,752	56,944	232,965	169,343
#edges	5,278	4,552	44,324	81,894	245,861	818,716	57,307,946	1,166,243
#features	1,433	3,703	500	6,805^^	767	121	602	128
#classes	-7~~	6	3	^T5	10	50	50	40
Transductive Setting Only a subset of nodes in one graph are used as training data, and other nodes
are used as validation and test data. For this setting, we use three benchmark dataset: Cora, CiteSeer,
PubMed. They are all citation networks, provided by (Sen et al., 2008). Each node represents a
paper, and each edge represents the citation relation between two papers. The datasets contain bag-
of-words features for each paper (node), and the task is to classify papers into different subjects
based on the citation networks.
Besides the three benchmark datasets, we use two more datasets: Coauthor CS and Amazon Com-
puters, provided by (Shchur et al., 2018). Coauthor CS is a co-authorship graph where nodes are
authors which are connected by an edge if they co-author a paper. Given paper keywords for each
13
Under review as a conference paper at ICLR 2021
author’s paper as node features, the task is to map each author to its most active field of study. Ama-
zon Computers is segments of the Amazon co-purchase graph where nodes represent goods which
are linked by an edge if these goods are frequently bought together. Node features encode product
reviews as bag-of-word feature vectors, and class labels are given by product category.
For all 5 datasets, We split the nodes in all graphs into 60%, 20%, 20% for training, validation, and
test. For the transductive task, we use the classification accuracy as the evaluation metric.
Inductive Setting In this task, we use a number of graphs as training data, and other completely
unseen graphs as validation/test data. For this setting, we use the PPI dataset, provided by (Hamilton
et al., 2017), on which the task is to classify protein functions. PPI consists of 24 graphs, with
each corresponding to a human tissue. Each node has positional gene sets, motif gene sets and
immunological signatures as features and gene ontology sets as labels. 20 graphs are used for
training, 2 graphs are used for validation and the rest for testing, respectively. For the inductive task,
we use Micro-F1 as the evaluation metric.
Transfer Setting In this task, we use two datasets, Reddit and Arxiv, which are two orders of mag-
nitude larger than Cora and CiterSeer in number of nodes, as shown in Table 9. The Reddit dataset
is provided by Hamilton et al. (2017), and the task is to predict the community to which different
Reddit posts belong. Reddit is an online discussion forum where users comment on different top-
ics. Each node represents a post, and each edge represents a link between two posts, when they
are commented by the same user. The dataset contains word vectors as node features. The graph
is constructed from Reddit posts made in the month of September 2014, and we follow the same
settings in the original paper (Hamilton et al., 2017), which uses the first 20 days for training and
the remaining days for test (with 30% used for validation).
The Arxiv dataset is constructed based on the citation network between all papers, and we use the
specific version of ogbn-arxiv, provided by a recent open graph benchmark (OGB) project (Hu
et al., 2020), where the task is to predict the 40 subject areas of Arxiv CS papers, e.g., cs.AI, cs.LG.
Each node (paper) has a 128-dimensional feature vector obtained by averaging the embedding of
words in its title and abstract. And all papers are also associated with the year that the corresponding
paper was published. The dataset is split by time. To be specific, papers published before 2017 are
used as training data, while those in 2018 and 2019 are used, respectively, as validation and test set.
For more details, we refer readers to Hu et al. (2020).
A.4.2 Compared Methods
We compare EGAN with two groups of state-of-the-art methods: human-designed GNN architec-
tures and NAS methods for GNN.
Human-designed GNNs. We use the following popular GNN architectures:
•	GCN (Kipf & Welling, 2016) proposes a sum aggregator normalized by the degrees of nodes.
•	GraphSAGE (Hamilton et al., 2017) proposes scalable graph neural network with different aggre-
gators: Mean, Sum, Max-Pool, LSTM.
•	GAT (Velickovic et al., 2018) proposes the attention aggregators, and it has different vari-
ants according to the attention functions: GAT, GAT-SYS, GAT-LINEAR, GAT-COS, GAT-
GENERALIZED-LINEAR. The detail of these attention functions are given in (Gao et al., 2020).
•	GIN (Xu et al., 2019) proposes to use Multi-layern Perceptron (MLP) as aggregators.
•	LGCN (Gao et al., 2018) proposes to automatically select topK neighbors for each node, and use
the 1-D regular convolutional operation as the aggregator.
•	GeniePath (Liu et al., 2019b) proposes a composition of attention and LSTM-style aggregators,
which can learn adaptive neighborhood for each node.
•	Geom-GCN (Pei et al., 2020) propose a geometric bi-level aggregation schema over structure-
aware neighbors in an continuous space and neighbors by adjacency matrix.
For models with variants, like different aggregators in GraphSAGE or different attention functions
in GAT, we report the best performance across the variants. Besides, we extend the idea of JK-
Network (Xu et al., 2018) in all models except for LGCN, and obtain 5 more variants: GCN-JK,
GraphSAGE-JK, GAT-JK, GIN-JK, GeniePath-JK, which add an extra layer. In the experiments, we
14
Under review as a conference paper at ICLR 2021
only report the better performance of each GNN and its JK variant, which is denoted by the original
name, as shown in Table 2, and 4, respectively.
For LGCN, we use the code released by the authors 2. For other baselines, we use the popular
open source library Pytorch Geometric (PyG) (Fey & Lenssen, 2019) 3 (Version: 1.6.0), which
implements various GNN models. For all baselines, we train it from scratch with the obtained best
hyperparameters on validation datasets, and get the test performance. We repeat this process for 5
times, and report the final mean accuracy with standard deviation.
NAS methods for GNN. We consider the following methods:
•	Random search (denoted as “Random”) is a simple baseline in NAS, which uniformly randomly
samples architectures from the search space, and keeps track of the optimal architecture during
the search process.
•	Bayesian optimization4 (denoted as “Bayesian”) (Bergstra et al., 2011) is a popular sequential
model-based global optimization method for hyper-parameter optimization, which uses tree-
structured Parzen estimator as the metric for expected improvement.
•	GraphNAS5 (Gao et al., 2020), a NAS method for searching GNN architecture, which is based on
reinforcement learning (Zoph & Le, 2017).
Random and Bayesian are searching on the designed search space of EGAN, where a GNN ar-
chitecture is sampled from the search space, and trained till convergence to obtain the validation
performance. 5000 models are sampled in total and the architecture with the best validation per-
formance is trained from scratch, and do some hyperparameters tuning on the validation dataset,
and obtain the test performance. For GraphNAS, we set the epoch of training the RL-based con-
troller to 5000, and in each epoch, a GNN architecture is sampled, and trained for enough epochs
(600 〜1000 depending on datasets), update the parameters of RL-based controller. In the end, We
sample 10 architectures and collect the top 5 architectures that achieve the best validation accuracy.
Then the best architecture is trained from scratch. Again, We do some hyperparameters tuning based
on the validation dataset, and report the best test performance. Note that We repeat the re-training of
the architecture for five times, and report the final mean accuracy With standard deviation.
Note that for human-designed GNN models and NAS methods, for fair comparison and good balance
betWeen efficiency and performance, We choose set the number of GNN layers to be 3, Which is an
empirically good choice in the literature (Velickovic et al., 2018; LiU et al., 2019b).
A.4.3 Implementation details of EGAN
Our experiments are running With Pytorch (version 1.6.0) on a GPU 2080Ti (memory: 12GB, cuda
version: 10.2). We implement EGAN on top of the building codes provided by PyG (version 1.6.0)
and SNAS 6. For all tasks, We run the search process for 5 times With different random seeds,
and retrieve top-1 architecture each time. By collecting the best architecture out of the 5 top-1
architectures on validation datasets, We repeat 5 times the process in re-training the best one, fine-
tuning hyperparameters on validation data, and reporting the test performance. Again, the final mean
accuracy With standard deviations are reported.
In the training stage, We set the search epoch to 600 for all datasets except PPI (150), and the learning
rate to 0.005, L2 norm to 0.0005, dropout rate to 0.5. In the fine-tuning stage, each architecture is
trained from scratch With 600 epochs.
A.4.4 Baselines in Transfer learning
On Reddit, We folloW the same settings of GraphSAGE in the original paper (Hamilton et al., 2017),
except that We use a 3-layer GNN as backbone to search, While GraphSAGE use 2-layer. Since the
performance of Reddit is only reported in GraphSAGE (Hamilton et al., 2017) and JK-NetWork (Xu
2https://github.com/HongyangGao/LGCN
3https://github.com/rusty1s/PytOrCh-geometric
4https://github.com/hyperopt/hyperopt
5https://github.com/GraphNAS/GraphNAS
6https://github.com/SNAS-Series/SNAS-Series
15
Under review as a conference paper at ICLR 2021
et al., 2018), we use them as human-designed architectures. Note that the authors of JK-Network did
not release their implementation, thus we use the implementation by the PyG framework. To keep it
consistent, we also use the implementation for GraphSAGE by the PyG framework. For Arxiv, we
follow the same setting in OGB project (Hu et al., 2020), where only two human-designed architec-
tures, GCN and GraphSAGE, are tested, thus we use these two as human-designed architectures.
For NAS baselines, we use the same NAS approaches in transductive and inductive tasks: Random,
Bayesian, GraphNAS, and GraphNAS-WS, and run them directly in the original graphs, i.e., Reddit
and Arxiv. However, since the code of GraphNAS and GraphNAS-WS crashed due to out of memory
error, we only report the performance of Random and Bayesian. Besides, we report the search cost
in terms of GPU hours to compare the efficiency of different methods.
A.4.5 Performance comparisons of GNN models and JK variants
The detailed performance of GNN baselines and their JK variants in Section 4.2.1 are in Table 10.
Table 10: Performance comparisons between the five GNN models and their JK variants. For sim-
plicity, we report the better one of each in Table 2.
	TranSdUCtive	Inductive
	Cora	CiteSeer	PubMed	CS	Computer ∣ PPI
GCN	0.876(0.010) 0.766(0.020) 0.871(0.003) 0.935(0.005) 0.912(0.007)	0.933(0.002)
GCN-JK	0.877(0.012) 0.771(0.014) 0.878(0.004) 0.949 (0.003) 0.910(0.006)	0.934(0.001)
GraphSAGE	0.874(0.002) 0.760(0.009) 0.878(0.004) 0.933(0.005) 0.915(0.003)	0.972(0.001)
GraphSAGE-JK	0.884(0.002) 0.765(0.005) 0.882(0.007) 0.952(0.002) 0.918(0.004)	0.972(0.001)
GAT	0.872(0.016) 0.752(0.014) 0.857(0.007) 0.929(0.003) 0.917(0.006)	0.978(0.001)
GAT-JK	0.873(0.009) 0.753(0.013) 0.867(0.006) 0.934(0.004) 0.918(0.005)	0.978(0.001)
GIN	0.860(0.008) 0.734(0.014) 0.880(0.005) 0.921(0.005) 0.867(0.041)	0.959(0.005)
GIN-JK	0.870(0.010) 0.765(0.013) 0.883(0.005) 0.950(0.003) 0.913(0.008)	0.964(0.003)
GeniePath	0.867(0.012) 0.759(0.014) 0.880(0.004) 0.926(0.004) 0.861(0.008)	0.953(0.000)
GeniePath-JK	0.878(0.012) 0.759(0.012) 0.882(0.004) 0.925(0.005) 0.883(0.007)	0.964(0.000)
A.5 Experiment Setup of Graph-level tasks
A.5.1 Datasets
Table 11: The statistics of datasets in graph-level tasks.
Dataset ∣ Num. Graphs ∣ Classes ∣ Avg. NumberofNodes ∣ Avg. Number of Edges
D&D	1,178	2	384.32	715.66
PROTEINS	1,113	2	39.06	72.82
In this section, we evaluate EGAN on graph classification task two datasets: D&D and PROTEINS
datasets, provided in Dobson & Doig (2003). These two datasets are both the protein graphs. In
D&D dataset, nodes represent the amino acids and two nodes are connected iff the distance is less
than 6 A. In PROTEINS dataset, nodes are secondary structure elements and edges represent nodes
are in an amino acid or in a close 3D space. More information are shown in Table 11.
A.5.2 Baselines
Besides the GNN baselines in node-level tasks, we use three more methods, which use hierarchy
pooling to learn whole graph representation given the embeddings of all nodes. DiffPool (Ying
et al., 2018b), SAGPool (Lee et al., 2019) and ASAP (Ranjan et al., 2020) are latest methods based
on hierarchical pooling schema, which learn node embeddings with node aggregators and coarsen
graphs with pooling aggregators. The final graph embeddings are generated by a readout operation
based on the final coarsend graph. DiffPool learns a soft assignment matrix for each node with
any GNN methods, combine with entropy regularization and link prediction objective, so that the
16
Under review as a conference paper at ICLR 2021
coarsened graph can preserve as much information as possible. SAGPool learns node weights with
attention mechanism and keeps the top-k nodes in pooling layer. In ASAP, it learns a soft cluster
assignment matrix for each node with self-attention mechanism, and calculates the fitness score for
each cluster and select top-k clusters.
For other methods, including GNN models in node-level tasks and NAS methods used in Table 4,
to obtain the representation of a whole graph, we use the global sum pooling method at the end of
retraining the derived architecture, i.e., the whole graph representation is obtained by the summation
of the embeddings of all nodes. z = Pi∈V hi(K)., K is the number of GNN layers.
In this section, we use 10-fold cross-validation accuracy as the evaluation metric, and the implemen-
tation details are presented in A.4.3. After finding the best architecture and tuning the hyperparam-
eters, we report the mean accuracy and standard deviations on 10 folds data.
In the search stage, we set the search epoch to 150, and the learning rate to 0.01, L2 norm to 0.0005,
dropout rate to 0.5. In the re-training stage, each architecture is trained from scratch with 100
epochs.
A.5.3 Performance comparisons of GNN models and JK variants
The detailed performance of GNN baselines and their JK variants in Section 4.2.3 are in Table 12.
Table 12: Performance comparisons between the five GNN models and their JK variants. For sim-
plicity, we report the better one of each in Table 4.
		D&D	PROTEINS
GCN	0.727(0.039)	0.727(0.026)
GCN-JK	0.733(0.043)	0.730(0.026)
GraPhSAGE	0.734(0.029)	0.734(0.031)
GraphSAGE-JK	0.728(0.047)	0.730(0.035)
GAT	0.713(0.052)	0.734(0.037)
GAT-JK	0.716(0.056)	0.745(0.030)
GIN	0.732(0.030)	0.736(0.028)
GIN-JK	0.733(0.033)	0.737(0.048)
GeniePath	0.704(0.039)	0.670(0.045)
GeniePath-JK	0.705(0.051)	0.694(0.035)
A.6 More Experimental Results
Table 13: Performance comparisons of two search spaces on four benchmark datasets. We show the
mean classification accuracy (with STD).
Methods	Cora	CiteSeer	PUbMed	PPI
GraphNAS	0.884(0.007)	0.776(0.006)	0.890(0.002)	0.970(0.013)
GraPhNAS-WS		0.881(0.010)	0.761(0.016)	0.884(0.010)	0.958(0.042)
GraphNAS (OUr search space)	0.883(0.002)	0.771(0.006)	0.888(0.001)	0.989(0.001)
GraphNAS-WS (OUr search space)	0.890(0.005)	0.770(0.007)	0.894(0.001)	0.988(0.001)
A.6.1 The advantage of the proposed search space
In Section 3.1, we discuss the advantages of search space between EGAN and GraphNAS/Auto-
GNN. In this section, we conduct experiments to further show the advantages. To be specific, we
run GraphNAS over its own and EGAN’s search space, given the same time budget (20 hours), and
compare the final test accuracy of the searched architectures in Table 13. From Table 13, we can
see that despite the simplicity of the search space, EGAN can obtain better or at least close accuracy
compared to GraphNAS, which means better architectures can be obtained given the same time
budget, thus demonstrating the efficacy of the designed search space.
17
Under review as a conference paper at ICLR 2021
A.6.2 Test accuracy during the search phase
In this section, we compare the efficiency of EGAN and NAS baselines by showing the test accu-
racy w.r.t the running time, as shown in Figure 5, from which we can observe that the efficiency
improvements are in orders of magnitude, which aligns with the experiments in previous one-shot
NAS methods, like DARTS (Liu et al., 2019a).
Figure 5: Test accuracy w.r.t. elapsed time on CiteSeer, PubMed, and PPI.
A.6.3 More Searched architectures
Figure 6: The searched architectures on more datasets in node-level tasks.
Figure 7: The searched architectures on datasets in graph-level tasks.
A.7 More Hyperparameters
For all GNN baselines in node-level tasks, we use the Adam optimizer, and set learning rate lr =
0.005, dropout p = 0.5, and L2 norm to 0.0005. For other parameters, we do some tuning, and
present the best ones in Table 14.
18
Under review as a conference paper at ICLR 2021
On Reddit, the parameters for GraphSAGE and GraphSAGE-JK are as follows: lr = 0.005, dropout
p = 0.5, and L2 norm to 0.0002, K = 3, d = 64, relu, epoch=30; For EGAN, lr = 0.006, dropout
p = 0.3, and L2 norm to 0.0005.
On Arxiv, the parameters of EGAN are as follows: lr = 0.025, dropout p = 0.5, and L2 norm to
0.0005.
For all searched GNN architectures, the tuned hyperparameters are shown in Table 15.
Table 14: More implementing details of GNN baselines. Here we give hidden dimension size,
activation function, and the number of heads (GAT models). For JK-Network, we further give the
layer aggregators.
I Cora&CiteSeer I PUbMed ∣ CS ∣ Computer ∣ PPI ∣ D&D ∣ PROTEINS
GCN	64, elu	128, elu	64, relu	64, relu	256, elu	64, elu	64, elu
GraphSAGE	64, relu	128, relu	64, relu	128,relu	256, elu	64, elu	128, elu
GAT	64, relu, 8	128, relu, 8	16, relu, 8	16, relu, 8	256, relu, 8	64, elu, 4	64, elu, 4
GIN	128, relu	128, relu	128, relu	16, relu	256, relu	32, elu	32, elu
LGCN	128, relu	128, relu	-	-	256, relu	-	-
GeniePath	-256, tanh-	256, tanh	128,tanh	128, tanh	256, tanh	32, elu	-16, elu-
JK-Network	-CONCAT-	CONCAT	CONCAT	CONCAT	LSTM	CONCAT	CONCAT
Table 15: The hyperparameters obtained by hyperopt in the fine-tuning process for the searched
architectures in Figure 2, 6, and 7. For all GAT aggregators, we set the number of head to 2 for
simplicity, which empirically works well in our experiments.
I Cora I CiteSeer ∣ PubMed ∣ CS ∣ Computer ∣ PPI ∣ Reddit ∣ Arxiv ∣ D&D ∣ PROTEINS
Hidden size	32	64	256	256	64	1024	256	128	16	64
Learning rate	3.12e-3	5.937e-3	4.484-3	3.164e-3	2.111e-3	1.036e-3	5.440e-5	3.019e-3	3.867e-2	2.334e-2
L2 norm	3.08e-5	2.007e-5	1.46e-4	2.46e-4	3.31e-4	0	1.495e-5	6.979e-5	2.16e-4	5.46e-4
Activation function	elu	elu	elu	elu	elu	elu	relu	relu	elu	elu
Dropout rate	0.4	-0.7-	0.6	-06-	-0.5-	-0.5-	-04-	0.4	-03-	0.3
A.8 Comparisons with SNAS and DARTS
In this section, to further demonstrate the efficiency of EGAN compared to DARTS (Liu et al.,
2019a) and SNAS (Xie et al., 2019), we further record the trending of validation accuracy of the
supernet by running them on the same search space during the search phase. These results are
shown in Figure 8, from which we can see that EGAN can obtain larger validation accuracy more
quickly than than SNAS and DARTS, which is attributed to the usage of natural gradient in Eq. (6).
Figure 8: Validation accuracy w.r.t. elapsed time on CiteSeer, PubMed, and PPI.
19