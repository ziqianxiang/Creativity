Under review as a conference paper at ICLR 2021
StructFormer: Joint Unsupervised Induction
of Dependency and Constituency Structure
from Masked Language Modeling
Anonymous authors
Paper under double-blind review
Ab stract
There are two major classes of natural language grammars — the dependency
grammar that models one-to-one correspondences between words and the con-
stituency grammar that models the assembly of one or several corresponded
words. While previous unsupervised parsing methods mostly focus on only in-
ducing one class of grammars, we introduce a novel model, StructFormer, that can
induce dependency and constituency structure at the same time. To achieve this,
we propose a new parsing framework that can jointly generates constituency tree
and dependency graph. Then we integrate the induced dependency relations into
transformer, in a differentiable manner, through a novel dependency-constrained
self-attention mechanism. Experimental results show that our model can achieve
strong results on unsupervised constituency parsing, unsupervised dependency
parsing and masked language modeling at the same time.
1	Introduction
Human languages have a rich latent structure. This structure is multifaceted, with the two major
classes of grammar being dependency and constituency structures. There have been an exciting
breath of recent work that are targeted at learning this structure in a data-driven unsupervised fashion.
The core principle behind recent methods that induce structure from data is simple - provide an
inductive bias that is conducive for structure to emerge as a byproduct of some self-supervised
training, e.g., language modeling. To this end, a wide range of models have been proposed that are
able to successfully learn grammar structures (Shen et al., 2018a;c; Wang et al., 2019; Kim et al.,
2019b;a). However, most of these works focus on learning constituency structures alone. To the best
of our knowledge, there have been no prior model or work that is able to induce, in an unsupervised
fashion, more than one grammar structure at once.
In this paper, we make two important technical contributions. First, we introduce the a new neural
model that is able to induce dependency structures from raw data in an end-to-end unsupervised
fashion. Most of existing approaches induce dependency structures from other syntactic information
like gold POS tags (Klein & Manning, 2004; Cohen & Smith, 2009; Jiang et al., 2016). Previous
works, that have trained from words alone, often requires additional information, like pre-trained
word clustering (Spitkovsky et al., 2011), pre-trained word embedding (He et al., 2018), acoustic
cues (Pate & Goldwater, 2013), or annotated data from related languages (Cohen et al., 2011).
Second, we introduce the first neural model that is able to induce both dependency structure and
constituency structure at the same time. Specifically, our approach aims to unify latent structure
induction of different types of grammar within the same framework.
We introduce a new inductive bias that enables the Transformer models to induce a directed depen-
dency graph in a fully unsupervised manner. To avoid the need of grammar labels during training,
we use a distance-based parsing mechanism. The key idea is that it predicts a sequence of Syntactic
Distances T (Shen et al., 2018b) and a sequence of Syntactic Heights ∆ (Luo et al., 2019) to repre-
sent dependency graph and constituency trees at the same time. Examples of ∆ and T are illustrated
in Figure 1a. Based on the syntactic distances (T) and syntactic heights (∆), we provide a new
dependency-constrained self-attention layer to replace the multi-head self-attention layer in stan-
dard transformer model. More concretely, each attention head can only attend on its parent (to avoid
1
Under review as a conference paper at ICLR 2021
(a) An example of Syntactic Distances T (grey
bars) and Syntactic Heights ∆ (white bars). In this
example, like is the parent (head) of constituent
(like cats) and (I like cats).
tribution allows each token to attend on its parent. The
dependent distribution allows each token to attend on its
dependents. For example the parent of cats is like.
Cats and I are dependents of like Each attention
head will receive a different weighted sum of these re-
lations.
Figure 1: An example of our parsing mechanism and dependency-constrained self-attention mecha-
nism. The parsing network first predicts the syntactic distance T and syntactic height ∆ to represent
the latent structure of the input sentence I like cats. Then the parent and dependent relations
are computed in a differentiable manner from T and ∆.
confusion with self-attention head, we use “parent” to note “head” in dependency graph) or its de-
pendents in the predicted dependency structure, through a weighted sum of different relations shown
in Figure 1b. In this way, we replace the complete graph in the standard transformer model with a
differentiable directed dependency graph. During the process of training on a downstream task (e.g.
masked language model), the model will gradually converge to a reasonable dependency graph via
gradient descent. Thus, the parser can be trained in an unsupervised manner as a component of the
model.
Incorporating the new parsing mechanism, the dependency-constrained self-attention, and the Trans-
former architecture, we introduce a new model named StructFormer. The proposed model can per-
form unsupervised dependency and constituency parsing at the same time, and can leverage the
parsing results to achieve strong performance on masked language model tasks.
2	Related Work
Previous works on unsupervised dependency parsing are primarily based on the dependency model
with valence (DMV)(KIein & Manning, 2004) and its extension (DaUme III, 2009; Gillenwater
et al., 2010). To effectively learn the DMV model for better parsing accuracy, a variety of inductive
biases and handcrafted featUres, sUch as correlations between parameters of grammar rUles involv-
ing different part-of-speech (POS) tags, have been proposed to incorporate prior information into
learning. The most recent progress is the neUral DMV model (Jiang et al., 2016), which Uses a neU-
ral network model to predict the grammar rUle probabilities based on the distribUted representation
of POS tags. However, most previoUs UnsUpervised dependency parsing algorithms reqUire the gold
POS tags as inpUt, which are labeled by hUmans and can be potentially difficUlt (or prohibitively
expensive) to obtain for large corpora. Spitkovsky et al. (2011) proposed to overcome this problem
with UnsUpervised word clUstering that can dynamically assign tags to each word considering its
context.
UnsUpervised constitUency parsing has recently received more attention. PRPN (Shen et al., 2018a)
and ON-LSTM (Shen et al., 2018c) indUce tree strUctUre by introdUcing an indUctive bias to recUr-
rent neUral networks. PRPN proposes a parsing network to compUte the syntactic distance of all
word pairs, and a reading network Utilizes the syntactic strUctUre to attend to relevant memories.
ON-LSTM allows hidden neUrons to learn long-term or short-term information by a novel gating
mechanism and activation fUnction. In URNNG (Kim et al., 2019b), amortized variational infer-
ence was applied between a recUrrent neUral network grammar (RNNG) (Dyer et al., 2016) decoder
and a tree strUctUre inference network, which encoUrages the decoder to generate reasonable tree
strUctUres. DIORA (Drozdov et al., 2019) proposed Using inside-oUtside dynamic programming to
2
Under review as a conference paper at ICLR 2021
compose latent representations from all possible binary trees. The representations of inside and out-
side passes from the same sentences are optimized to be close to each other. Compound PCFG (Kim
et al., 2019a) achieves grammar induction by maximizing the marginal likelihood of the sentences
which are generated by a probabilistic context-free grammar (PCFG) in a corpus. Tree Transformer
(Wang et al., 2019) adds extra locality constraints to the Transformer encoder’s self-attention to en-
courage the attention heads to follow a tree structure such that each token can only attend on nearby
neighbors in lower layers and gradually extend the attention field to further tokens when climbing
to higher layers.
Though large scale pre-trained models have dominated most natural language processing tasks, some
recent work indicates that neural network models can see accurarcy gains by leveraging syntactic
information rather then ignoring it (Marcheggiani & Titov, 2017; Strubell et al., 2018). Strubell
et al. (2018) introduces syntactically-informed self-attention that force one attention head to attend
on the syntactic governor of input token. Omote et al. (2019) and Deguchi et al. (2019) argue that
dependency-informed self-attention can improve Transformer’s performance on machine transla-
tion. Kuncoro et al. (2020) shows that syntactic biases help large scale pre-trained models, like
BERT, to achieve better language understanding.
3	S yntactic Distance and Height
In this section, we first reintroduce the concepts of syntactic distance and height, then discuss their
relations in the context of StructFormer.
Algorithm 2 Converting binary constituency tree to
dependency graph
			1	function DEPENDENT(T, ∆)
Algorithm 1 Distance to binary constituency		2	if T = w then
tree			3	D U [], parent U w
1	: function CONSTITUENT(w, d)	4	else
2	:	if d = [] then	5	childl, childr U T
3	T U Leaf(w)	6	Dl, parentl U Dependent(childl, ∆)
4	:	else	7	Dr, parentr U Dependent(childr, ∆)
5	:	i U arg maxi(d)	8	D U Union(Dl, Dr)
6	:	childl U Constituent(w≤i, d<i)	9	if ∆(parentl) > ∆(parentr) then
7	:	childr U Constituent(w>i, d>i)	10	D.add(parentι — Parentr)
8	:	T U Node(childl, childr)	11	parent U parentl
9	:	return T	12	else
	—	13	D.add(parentr — Parentl)
		14	Parent U Parentr
		15	return D, Parent
3.1	S yntactic Distance
Syntactic distance is proposed in Shen et al. (2018b) to quantify the process of splitting sentences
into smaller constituents.
Definition 3.1. Let T be a constituency tree for sentence (w0, ..., wn). The height of the lowest
common ancestor for consecutive words Xi and Xi+ι is Ti. Syntactic distances T = (τo,..., τn-ι)
are defined as a sequence of n - 1 real scalars that share the same rank as (T0,…,T-ι).
In other words, each syntactic distance di is associated with a split point (i, i + 1) and specify the
relative order in which the sentence will be split into smaller components. Thus, any sequence of
n - 1 real values can unambiguously map to an unlabeled binary constituency tree with n leaves
through the Algorithm 1 (Shen et al., 2018b). As Shen et al. (2018c;a); Wang et al. (2019) pointed
out, the syntactic distance reflects the information communication between constituents. More con-
cretely, a large syntactic distance τi represents that less short-term or local information should be
communicated between (x≤i) and (x>i). While cooperating with correct inductive bias, we can
leverage this feature to build unsupervised dependency parsing models.
3
Under review as a conference paper at ICLR 2021
3.2	S yntactic Height
Syntactic height is proposed in Luo et al. (2019), where the syntactic height is used to capture the
distance to the root node in a dependency graph. A word with high syntactic height means it is close
to the root node. In this paper, to match the definition of syntactic distance, we redefine syntactic
height as:
Definition 3.2. Let D be a dependency graph for sentence (w0, ..., wn). The height of a token wi
in D is δi. The syntactic heights of D can be any sequence of n real scalars ∆ = (δo,…,δn) that
share the same rank as (δ0, ..., δn-1).
Although the syntactic height is defined based on the dependency structure, we cannot rebuild the
original dependency structure just by syntactic heights, since there is no information about whether
a token should be attached to the left side or the right side. However, given a unlabelled constituent
tree, we can convert it into a dependency graph with the help of syntactic distance. The convert-
ing process is similar to the standard process of converting constituency treebank to dependency
treebank (Gelbukh et al., 2005). Instead of using the constituent labels and POS tags to identify
the parent of each constituent, we simply assign the token with largest syntactic height as the par-
ent of each constituent. The converting algorithm is described in Algorithm 2. In Appendix A.1,
we also proposed a joint algorithm, that takes T and ∆ as inputs and output constituency tree and
dependency graph at the same time.
3.3	The relation between Syntactic Distance and Height
As discussed previously, the syntactic distance controls information communication between the two
side of the split point. The syntactic height quantifies the centrality of each token in the dependency
graph. A token with large syntactic height tend to have more long-term dependency relations to
connect different part of the sentence together. In StructFormer, we quantify the syntactic distance
and height in the same scale. Given a split point (i, i + 1) and it’s syntactic distance δi, only tokens
xj with τj > δi can have connections across the split point (i, i + 1). Thus tokens with small
syntactic height will be limited to mostly attend on near tokens.
4	StructFormer
In this section, we present the StructFormer model. Figure 2a shows the architecture of Struct-
Former, which includes a parser network and a Transformer module. The parser network predicts
T and ∆, then passes them to a set of differentiable functions to generate dependency distributions.
The Transformer module takes these distributions and the sentence as input to computes a contextual
embedding for each position. The StructFormer can be trained in an end-to-end fashion on a Masked
Language Model task. In this setting, the gradient back propagates through the relation distributions
into the parser.
4.1	Parsing Network
As shown in Figure 2b, the parsing network takes word embeddings as input and feeds them into
several convolution layers:
s	l,i = tanh (Conv (sl-1,i-W, sl-1,i-W+1, ..., sl-1,i+W))	(1)
where sl,i is the output of l-th layer at i-th position, s0,i is the input embedding of token wi, and
2W + 1 is the convolution kernel size.
Given the output of the convolution stack sN,i, we parameterize the syntactic distance T as:
τi = W1τ tanh W2τ ssN,i + bτ2 + bτ1	(2)
where δi is the contextualized distance for the i-th split point between token wi and wi+1. The
syntactic height ∆ is parameterized in a similar way:
δi = Wδ tanh (WgsN,i + bg) + bf	(3)
4
Under review as a conference paper at ICLR 2021
(a) Model Architecture
Figure 2: The Architecture of StructFormer. The parser takes shared word embeddings as input,
outputs syntactic distances T, syntactic heights ∆, and dependency distributions between tokens.
The transformer layers take word embeddings and dependency distributions as input, output contex-
tualized embeddings for input words.
(b) Parsing Network
4.2 Estimate the Dependency Distribution
Figure 3: An example of T, ∆ and respective de-
pendency graph D.
x6, thus D(x4) = x6.
Given T and ∆, we now explain how to esti-
mate the probability p(xj |xi) that the j-th token
is the parent of the i-th token. The first step is
identifying the smallest legit constituent C(xi),
that contains xi and xi is not C(xi)’s parent.
The second step is identifying the parent of the
constituent xj = Pr(C(xi)). Given the discus-
sion in section 3.2, the parent of C(xi) must be
the parent of xi . Thus, the two-stages of identi-
fying the parent of xi can be formulated as:
D(xi) = Pr(C(xi))	(4)
In StructFormer, C(xi) is represented as con-
stituent [l, r], where l is the starting index (l ≤
i) of C(xi) and r is the ending index (r ≥ i)
of C(xi). For example, in Figure 3, C(x4) =
[4, 8] and the parent of constituent Pr([4, 8]) =
In a dependency graph, xi is only connected to its parent and dependents. It means that xi don’t have
direct connection to the outside of C(xi). In other words, C(xi) = [l, r] is the smallest constituent
that satisfies:
δi < τl-1, δi < τr	(5)
where τl-1 is the first τ<i that is larger then δi while looking backward, and τr is the first τ≥i that
is larger then δi while looking forward. In the previous, δ4 = 3.5, τ3 = 4 > δ4 and τ8 = ∞ > δ4,
thus C(x4) = [4, 8]. To make this process differentiable, we define τk as a real value and δi
as a probability distribution p(δi). For the simplicity and efficiency of computation, we directly
parameterize the cumulative distribution function p(δi > τk) with sigmoid function:
p(δi > Tk) = σ((δi - Tk)∕μι)	(6)
where σ is the Sigmoid function, δi is the mean of distribution p(δi) and μ1 is a learnable temperature
term. Thus the probability that the l-th (l < i) token is inside C(xi) is equal to the probability that
δi is larger then the maximum distance T between l and i:
P(I ∈ C(Xi)) = p(δi > max(τi-i,…,Tl)) = σ((δi - max(τ,…，/一))∕μ)	(7)
Then we can compute the probability distribution for l :
pleft(l|i) = X pleft(k|i) - X pleft(k|i) =p(l ∈ C(xi)) -p(l - 1 ∈ C(xi))
k∈[1,l]	k∈[1,l-1]
= σ((δi - max(τι,…，Ti-i))∕μ) - σ((δi - max(τi-i,…，T"i))∕μ)	(8)
5
Under review as a conference paper at ICLR 2021
Similarly, we can compute the probability distribution for r:
Pright(r∣i) = σ((δi - max(τi,...,Tr-i))∕μ) - σ((δi - max(τi, ...,τr))∕μ)	(9)
The probability distribution for [l, r] = C(xi) can be computed as:
pC([l,r]|i) =	pleft(l|i)p0r,ight(r|i), olt≤heirw≤isre	(10)
The second step is to identify the parent of [l, r]. For any constituent [l, r], we choose the
j = argmaxk∈[l,r] (δk) as the parent of [l, r]. In the previous example, given constituent [4, 8],
the maximum syntactic height is δ6 = 4.5, thus Pr([4, 8]) = x6. We use softmax function to
parameterize the probability pPr(j|[l, r]):
{exp(hj/μ2	I ≤ t ≤ r
Pl≤k≤r exp(hk/μ2) ,	— —	(11)
0,	otherwise
Given probability p(j|[l, r]) and p([l, r]|i), we can compute the probability that xj is the parent of
xi:
pD(j|i) = P[l,r]pPr(j|[l,r])pC([l,r]|i), i6=j	(12)
0,	i = j
4.3 Dependency-Constrained Multi-head Self-Attention
The multi-head self-attention in transformer can be seen as a information propagation mechanism on
the complete graph G = (X, E), where the set of vertices X contains all n tokens in the sentence,
and the set of edges E contains all possible word pairs (xi, xj). StructFormer replace the complete
graph G with a soft dependency graph D = (X, A), where A is the set of n probability distribution
{pD(j|i)} that represent the probability of existing and directed edge between the dependent i and
the parent j . The reason that we called it a directed edge is that each specific head is only allow to
propagate information either from parent to dependent or from from dependent to parent. To do so,
structformer associate each attention head with a probability distribution over parent or dependent
relation.
=	eχp(WParent)	= _______eχp(WdeP)________
PParent - exp(Wparent)+exp(Wdep) ,	PdeP - exp(Wparent )+exp(Wdep)
(13)
where wparent and wdep are learnable parameters that associated with each attention head, Pparent is
the probability that this head will propagate information from parent to dependent, vice versa. The
model will learn to assign this association from the downstream task via gradient descent. Then we
can compute the probability that information can be propagated from node j to node i via this head:
Pi,j = PparentPD (j |i) + PdepPD (i|j)
(14)
However, Htut et al. (2019) pointed out that different heads tend to associate with different type of
universal dependency relations (including nsubj, obj, advmod, etc), but there is no generalist
head can that work with all different relations. To accommodate this observation, we compute a
individual probability for each head and pair of tokens (xi, xj ):
qi,j
sigmoid
(15)
where Q and K are query and key matrix in a standard transformer model and dk is the dimension
of attention head. The equation is inspired by the scaled dot-product attention in transformer. We
replace the original softmax function with sigmoid function, so qi,j became an independent proba-
bility that indicate whether the specific could work with the work (xi, xj). In the end, we propose to
replace transformer’s scaled dot-product attention with our dependency-constrained self-attention:
Attention(Qi, Kj , Vj , D) = Pi,j qi,j Vj
(16)
6
Under review as a conference paper at ICLR 2021
5	Experiments
We evaluate the proposed model on three tasks: Masked Language Modeling, Unsupervised Con-
situency Parsing and Unsupervised Dependency Parsing.
Our implementation of StructFormer is close to the original Transformer encoder (Vaswani et al.,
2017). Except that we put the layer normalization in front of each layer, similar to the T5 model
(Raffel et al., 2019). We found that this modification allows the model to converges faster. For
all experiments, we set the number of layers L = 8, the embedding size and hidden size to be
dmodel = 512, the number of self-attention heads h = 8, the feed-forward size dff = 2048,
dropout rate as 0.1, and the number of convolution layers in the parsing network as Lp = 3.
5.1	Masked Language Model
Masked Language Modeling (MLM) has been widely used as pretraining object for larger scale pre-
training models. In BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019), authors found that
MLM perplexities on held-out evaluation set have a positive correlation with the end-task perfor-
mance. We trained and evaluated our model on 2 different datasets: the Penn TreeBank (PTB) and
BLLIP. In our MLM experiments, each individual token has a independent chance to be replaced by
a mask token <mask>, except that we never replace < unk > token. The training and evaluation
object for Masked Language Model is to predict the replaced tokens. The performance of MLM is
evaluated measuring perplexity on masked words.
PTB is a standard dataset for language modeling (Mikolov et al., 2012) and unsupervised con-
stituency parsing (Shen et al., 2018c; Kim et al., 2019a). Following the setting proposed in Shen
et al. (2018c), we use Mikolov et al. (2012)’s prepossessing process, which removes all punctua-
tions, and replaces low frequency tokens with <unk>. The preprocessing results in a vocabulary
size of 10001 (including <unk>, <pad> and <mask>). For PTB, we use a 30% mask rate.
BLLIP is a large Penn Treebank-style parsed corpus of approximately 24 million sentences. We
train and evaluate StructFormer on three splits of BLLIP: BLLIP-XS (40k sentences, 1M tokens),
BLLIP-SM (200K sentences, 5M tokens), and BLLIP-MD (600K sentences, 14M tokens). They are
obtained by randomly sampling sections from BLLIP 1987-89 Corpus Release 1. All models are
tested on a shared held-out test set (20k sentences, 500k tokens). Following the settings provided
in (Hu et al., 2020), we use subword-level vocabulary extracted from the GPT-2 pre-trained model
rather than the BLLIP training corpora. For BLLIP, we use a 15% mask rate.
5.2	Unsupervised Constituency Parsing
The unsupervised constituency parsing task compares the latent tree structure induced by the model
with those annotated by human experts. We use the Alogrithm 1 to predict the constituency trees
from T predicted by StructFormer. Following the experiment settings proposed in Shen et al.
(2018c), we take the model trained on PTB dataset, and evaluate it on WSJ test set. The WSJ test
set is the section 23 of WSJ corpus, it contains 2416 human expert labeled sentences. Punctuation
is ignored during the evaluation.
5.3	Unsupervised Dependency Parsing
The unsupervised dependency parsing evaluation compares the induced dependency relations with
those in the reference dependency graph. The most common metric is Unlabeled Attachment Score
(UAS), which measures the percentage that a token is correctly attached to its parent in the reference
tree. Another widely used metric for unsupervised dependency parsing is Undirected Unlabeled
Attachment Score (UUAS) measures the percentage that the reference undirected and unlabeled
connections are recovered by the induced tree. Similar to the unsupervised constituency parsing, we
take the model trained on PTB dataset, and evaluate it on WSJ test set (section 23). For the WSJ
test set, reference dependency graphs are converted from its human annotated constituency trees.
However, there are two different sets of rules for the conversion: the Stanford dependencies and the
CoNLL dependencies. While Stanford dependencies are used as reference dependencies in previous
unsupervised parsing papers, we noticed that our model sometimes output dependency structures
that are closer to the CoNLL dependencies. Therefore, we report UAS and UUAS for both Stanford
7
Under review as a conference paper at ICLR 2021
Model	PTB	BLLIP-XS	BLLIP-SM	BLLIP-MD
Transformer	64.05	93.90	19.92	14.31
StructFormer	60.94	57.28	18.70	13.70
Table 1: Masked Language Model perplexities on BLLIP datasets of different models.
Methods	UF1
RANDOM	21.6
LBRANCH	9.0
RBRANCH	39.8
Pre-trained LMs	
BERT-large*	34.2
GPT2*	37.1
XLNet-base*	40.1
Unsupervised Parsing Models	
PRPN (Shen et al., 2018a)	37.4 (0.3)
ON-LSTM (Shen et al., 2018c)	47.7 (1.5)
Tree-T (Wang et al., 2019)	49.5
URNNG (Kim et al., 2019b)	52.4
C-PCFG (Kim et al., 2019a)	55.2
StructFormer	54.0 (0.3)
(a) Constituency Parsing Results. from Kim et al. (2020).	* results are
Table 2: The unsupervised parsing performance of different models.
Methods	UAS
w/o gold POS tags	
DMV (Klein & Manning, 2004)	35.8
E-DMV (Headden III et al., 2009)	38.2
UR-A E-DMV (Tu & Honavar, 2012)	46.1
CS* (Spitkovsky et al., 2013)	64.4*
Neural E-DMV (Jiang et al., 2016)	42.7
Gaussian DMV (He et al., 2018)	43.1 (1.2)
INP (He et al., 2018)	47.9 (1.2)
StructFormer	46.2 (0.4)
w/ gold POS tags (for reference only)	
DMV (Klein & Manning, 2004)	39.7
UR-A E-DMV (Tu & Honavar, 2012)	57.0
MaxEnc (Le & Zuidema, 2015)	65.8
Neural E-DMV (Jiang et al., 2016)	57.6
CRFAE (Cai et al., 2017)	55.7
L-NDMVt (Han et al., 2017)	63.2
(b) Dependency Parsing Results on WSJ testset. Starred
entries (*) benefit from additional punctuation-based
constraints. Daggered entries (t) benefit from larger ad-
ditional training data. Baseline results are from He et al.
(2018).
and CoNLL dependencies. Following the setting of previous papers (Jiang et al., 2016), we ignored
the punctuation during evaluation. To obtain the dependency relation from our model, we compute
the argmax for dependency distribution:
k = argmaxj6=ipD(j|i)	(17)
and assign the k-th token as the parent of i-th token.
5.4	Experimental Results
The masked language model results are shown in Table 1. StructFormer consistently outperform our
Transformer baseline. This result aligns with previous observations that linguistic informed self-
attention can help Transformers achieve stronger performance. We also observe that StructFormer
converges much faster than the standard Transformer model.
Table 2a shows that our model achieves strong results on unsupervised constituency parsing. While
the C-PCFG (Kim et al., 2019a) achieve a stronger parsing performance with its strong linguistic
constraints (e.g. a finite set of production rules), StructFormer may have border domain of appli-
cation. For example, it can replace standard transformer encoder in most of popular large-scale
pretrained language models (e.g. BERT and ReBERTa) and transformer based machine translation
models. It’s also interesting to notice that, different from Tree-T (Wang et al., 2019), we didn’t
directly use constituents to restrict the self-attention receptive field. But we eventually achieve a
stronger constituency parsing performance with same experiment setting. This result may suggest
that the dependency relations is a more suitable for grammar induction in transformer-based mod-
els. Table 3 shows that our model achieve strong accuracy while predicting Noun Phrase (NP),
Preposition Phrase (PP), Adjective Phrase (ADJP), and Adverb Phrase (ADVP).
Table 2b shows that our model achieves competitive dependency parsing performance while com-
paring to other models that do not require gold POS tags. While most of baseline models still relies
8
Under review as a conference paper at ICLR 2021
	PRPN	ON	C-PCFG	Tree-T	StructFormer
SBAR	50.0%	52.5%	56.1%	36.4%	48.7%
NP	59.2%	64.5%	74.7%	67.6%	72.1%
VP	46.7%	41.0%	41.7%	38.5%	43.0%
PP	57.2%	54.4%	68.8%	52.3%	74.1%
ADJP	44.3%	38.1%	40.4%	24.7%	51.9%
ADVP	32.8%	31.6%	52.5%	55.1%	69.5%
Table 3: Fraction of ground truth constituents that were predicted as a constituent by the models
broken down by label (i.e. label recall)
Relations MLM Constituency	Stanford	Conll
	PPL	UF1	UAS	UUAS	UAS	UUAS
parent+dep	60.9 (1.0)	54.0 (0.3)	46.2 (0.4)	61.6 (0.4)	36.2 (0.1)	56.3 (0.2)
parent	63.0 (1.2)	40.2 (3.5)	32.4 (5.6)	49.1 (5.7)	30.0 (3.7)	50.0 (5.3)
dep	63.2 (0.6)	51.8 (2.4)	15.2 (18.2)	41.6 (16.8)	20.2 (12.2)	44.7 (13.9)
Table 4: The performance of StructFormer with different combinations of attention masks.
on some kind of latent POS tags or pretrained word embeddings, StructFormer can be seen as a
easy-to-use alternative that works in an end-to-end fashion. Table 5 shows that our model recov-
ers 61.6% of undirected dependency relations. Given the strong performances on both dependency
parsing and masked language modeling, we believe that the dependency graph schema could be an
viable substitute for the complete graph schema used in standard transformer.
Since our model uses a mixture of relation probability distribution for each self-attention head, we
also studied how different combinations of relations effect the performance of our model. Table
5 shows that the model can achieve the best performance, while using both parent and dependent
relations. The model suffers more on dependency parsing, if the parent relation is removed. And if
the dependent relation is removed, the model will suffers more on the constituency parsing.
6	Conclusion
In this paper, we introduce a novel dependency and constituency joint parsing framework. Based
on the framework, we propose StructFormer, a new unsupervised parsing algorithm that does un-
supervised dependency and constituency parsing at the same time. We also introduced a novel
dependency-constrained self-attention mechanism that allows each attention head to focus on a spe-
cific mixture of dependency relations. This brings Transformers closer to modeling a directed de-
pendency graph. The experiments show premising results that StructFormer can induce meaningful
dependency and constituency structures and achieve better performance on masked language model
task. This research provides a new path to build more linguistic bias into pre-trained language model.
References
Jiong Cai, Yong Jiang, and Kewei Tu. Crf autoencoder for unsupervised dependency parsing. arXiv
preprint arXiv:1708.01018, 2017.
Shay B Cohen and Noah A Smith. Shared logistic normal distributions for soft parameter tying
in unsupervised grammar induction. In Proceedings of Human Language Technologies: The
2009 Annual Conference of the North American Chapter of the Association for Computational
Linguistics,pp. 74—82, 2009.
Shay B Cohen, Dipanjan Das, and Noah A Smith. Unsupervised structure prediction with non-
parallel multilingual guidance. In Proceedings of the 2011 Conference on Empirical Methods in
Natural Language Processing, pp. 50-61, 2011.
Hal DaUme III. UnsUPervised search-based structured prediction. In Proceedings ofthe 26th Annual
International Conference on Machine Learning, pp. 209-216, 2009.
9
Under review as a conference paper at ICLR 2021
Hiroyuki Deguchi, Akihiro Tamura, and Takashi Ninomiya. Dependency-based self-attention for
transformer nmt. In Proceedings of the International Conference on Recent Advances in Natural
Language Processing (RANLP 2019),pp. 239-246, 2019.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.
Andrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, and Andrew McCallum. Unsupervised
latent tree induction with deep inside-outside recursive auto-encoders. In Proceedings of the 2019
Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1129-1141, 2019.
Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network
grammars. In Proceedings of NAACL-HLT, pp. 199-209, 2016.
Alexander Gelbukh, Sulema Torres, and Hiram Calvo. Transforming a constituency treebank into a
dependency treebank. Procesamiento del lenguaje natural, (35):145-152, 2005.
Jennifer Gillenwater, KUzman Ganchev, Joao Graga, Fernando Pereira, and Ben Taskar. Sparsity in
dependency grammar induction. ACL 2010, pp. 194, 2010.
WenjUan Han, Yong Jiang, and Kewei TU. Dependency grammar indUction with neUral lexicalization
and big training data. arXiv preprint arXiv:1708.00801, 2017.
JUnxian He, Graham NeUbig, and Taylor Berg-Kirkpatrick. UnsUpervised learning of syntactic
strUctUre with invertible neUral projections. In Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pp. 1292-1302, 2018.
William P Headden III, Mark Johnson, and David McClosky. Improving UnsUpervised dependency
parsing with richer contexts and smoothing. In Proceedings of human language technologies:
the 2009 annual conference of the North American chapter of the association for computational
linguistics, pp. 101-109, 2009.
PhU Mon HtUt, Jason Phang, Shikha Bordia, and SamUel R Bowman. Do attention heads in bert
track syntactic dependencies? arXiv preprint arXiv:1911.12246, 2019.
Jennifer HU, Jon GaUthier, Peng Qian, Ethan Wilcox, and Roger P Levy. A systematic assessment
of syntactic generalization in neUral langUage models. arXiv preprint arXiv:2005.03692, 2020.
Yong Jiang, WenjUan Han, Kewei TU, et al. UnsUpervised neUral dependency parsing. Association
for CompUtational LingUistics (ACL), 2016.
TaeUk Kim, JihUn Choi, Daniel Edmiston, and Sang-goo Lee. Are pre-trained langUage mod-
els aware of phrases? simple bUt strong baselines for grammar indUction. arXiv preprint
arXiv:2002.00737, 2020.
Yoon Kim, Chris Dyer, and Alexander M RUsh. CompoUnd probabilistic context-free grammars for
grammar indUction. In Proceedings of the 57th Annual Meeting of the Association for Computa-
tional Linguistics, pp. 2369-2385, 2019a.
Yoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Gabor Melis. Un-
sUpervised recUrrent neUral network grammars. In Proceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers), pp. 1105-1117, 2019b.
Dan Klein and Christopher D Manning. Corpus-based induction of syntactic structure: Models of
dependency and constituency. In Proceedings of the 42nd annual meeting of the association for
computational linguistics (ACL-04), pp. 478-485, 2004.
Adhiguna Kuncoro, Lingpeng Kong, Daniel Fried, Dani Yogatama, Laura Rimell, Chris Dyer,
and Phil Blunsom. Syntactic structure distillation pretraining for bidirectional encoders. arXiv
preprint arXiv:2005.13482, 2020.
10
Under review as a conference paper at ICLR 2021
Phong Le and Willem Zuidema. Unsupervised dependency parsing: Let’s use supervised parsers.
arXiv preprint arXiv:1504.04666, 2015.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
approach. arXiv preprint arXiv:1907.11692, 2019.
Hongyin Luo, Lan Jiang, Yonatan Belinkov, and James Glass. Improving neural language models
by segmenting, attending, and predicting the future. In Proceedings of the 57th Annual Meeting
ofthe Associationfor Computational Linguistics, pp.1483-1493, 2019.
Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for
semantic role labeling. arXiv preprint arXiv:1703.04826, 2017.
Tomas Mikolov et al. Statistical language models based on neural networks. Presentation at Google,
Mountain View, 2nd April, 80:26, 2012.
Yutaro Omote, Akihiro Tamura, and Takashi Ninomiya. Dependency-based relative positional en-
coding for transformer nmt. In Proceedings of the International Conference on Recent Advances
in Natural Language Processing (RANLP 2019), pp. 854-861, 2019.
John K Pate and Sharon Goldwater. Unsupervised dependency parsing with acoustic cues. Trans-
actions of the Association for Computational Linguistics, 1:63-74, 2013.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text
transformer. arXiv preprint arXiv:1910.10683, 2019.
Yikang Shen, Zhouhan Lin, Chin-wei Huang, and Aaron Courville. Neural language modeling by
jointly learning syntax and lexicon. In International Conference on Learning Representations,
2018a.
Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, and Yoshua
Bengio. Straight to the tree: Constituency parsing with neural syntactic distance. In Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers), pp. 1171-1180, 2018b.
Yikang Shen, Shawn Tan, Alessandro Sordoni, and Aaron Courville. Ordered neurons: Integrating
tree structures into recurrent neural networks. In International Conference on Learning Repre-
sentations, 2018c.
Valentin I Spitkovsky, Hiyan Alshawi, Angel Chang, and Dan Jurafsky. Unsupervised dependency
parsing without gold part-of-speech tags. In Proceedings of the 2011 Conference on Empirical
Methods in Natural Language Processing, pp. 1281-1290, 2011.
Valentin I Spitkovsky, Daniel Jurafsky, and Hiyan Alshawi. Breaking out of local optima with count
transforms and model recombination: A study in grammar induction. 2013.
Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, and Andrew McCallum. Linguistically-
informed self-attention for semantic role labeling. arXiv preprint arXiv:1804.08199, 2018.
Kewei Tu and Vasant Honavar. Unambiguity regularization for unsupervised learning of probabilis-
tic grammars. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural
Language Processing and Computational Natural Language Learning, pp. 1324-1334, 2012.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Eukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information
processing systems, pp. 5998-6008, 2017.
Yaushian Wang, Hung-Yi Lee, and Yun-Nung Chen. Tree transformer: Integrating tree structures
into self-attention. In Proceedings of the 2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pp. 1060-1070, 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
A. 1 Joint Dependency and Constituency Parsing
Algorithm 3 The joint dependency and constituency parsing algorithm. Inputs are a sequence of
words w, syntactic distances d, syntactic heights h. Outputs are a binary constituency tree T, a
dependency graph D that is represented as a set of dependency relations, the parent of dependency
graph D, and the syntactic height of parent.
1:	function BUILDTREE(w, d, h)
2:	if d = [] and w = [w] and h = [h] then
3:	T U Leaf(w), D U [], parent U w, height U h
4:	else
5:	i U arg max(d)
6:	Tl , Dl , parentl , heightl U BuildTree(d<i, w≤i, h≤i)
7:	Tr, Dr, parentr, heightr U BuildTree(d>i, w>i, h>i)
8:	T U Node(childl U Tl, childr U Tr)
9:	D U Union(Dl, Dr)
10:	if heightl > heightr then
11:	D.add(parentι — Parentr)
12:	parent U parentl , height U heightl
13:	else
14:	D.add(parentr — Parentl)
15:	parent U parentr, height U heightr
16:	return T, D, Parent, height
12
Under review as a conference paper at ICLR 2021
B Dependency Relation Weights for Self-attention Heads

5 05 05 0505 05 05 05 O
0.660.0.0.0.660.60.0.Ci0.6
OlNml7c 9 N
(a) Dependency relation weights learnt on PTB
(b) Dependency relation weights learnt on BLLIP-SM
Figure 4: Dependency relation weights learnt on different datasets. Row i constains relation weights
for all attention heads in the i-th transformer layer. p represents the parent relation. d represents the
dependent relation. We observe a clearer preference for each attention head in the model trained on
BLLIP-SM. This probably due to BLLIP-SM has signficantly more training data. It’s also interesting
to notice that the first layer tend to focus on parent relations.
13
Under review as a conference paper at ICLR 2021
C Dependency Distribution Examples
hooker J	0.00	0.05	0.26	0.07	0.04	0.07	0.02	0.11
ls J	0.22	0.00	0.8	0.04	0.02	0.04	0.01	0.06
philosophy T	0.17	0.06	0.00	0.09	0.05	0.08	0.03	0.14
was -∖	0.11	0.03	0.20	0.00	0.05	0.09	0.03	0.15
to ]	0.07	0.02	0.12	0.06	0.00	0.12	0.04	0.21
build -J	0.09	0.03	0.16	0.07	0.07	0.00	0.05	0.25
and J	0.04	0.01	0.07	0.03	0.06	0.15	0.00	0.25
sell ]	0.10	0.03	0.18	0.08	0.06	0.12	0.04	0.00
/ 丁 苹 f-
(a)
there -∖	0.00	0.02	0.05	0.13	0.06	0.02	0.07	0.02	0.10
,s J	0.13	0.00	0.11	0.19	0.03	0.01	0.03	0.01	0.04
nothing -∖	0.14	0.09	0.00	.	0.05	0.02	0.05	0.02	0.07
rational -∖	0.14	0.05	0.10	0.00	0.08	0.03	0.09	0.03	0.13
about -∖	0.09	0.02	0.05	0.13	0.00	0.04	0.10	0.04	0.14
this -]	0.05	0.01	0.02	0.07	0.06	0.00	0.17	0.06	0.14
kind ]	0.08	0.02	0.04	0.11	0.08	0.07	0.00	0.07	0.21
of ]	0.04	0.01	0.02	0.06	0.05	0.05	0.16	0.00	0.20
action -∖	0.08	0.02	0.04	0.12	0.08	0.04	0.12	0.04	0.00
			心					心	
(b)
Figure 5: Dependency distribution examples from WSJ test set. Each row is the parent distribution
for the respective word. The sum of each distribution may not equal to 1.
14
Under review as a conference paper at ICLR 2021
D The Performance of S tructFormer with different mask rates
Mask rate	MLM PPL	Constituency UF1	Stanford		Conll	
			UAS	UUAS	UAS	UUAS
0.1	45.3 (1.2)	51.45 (2.7)	31.4(11.9)	51.2 (8.1)	32.3 (5.2)	52.4 (4.5)
0.2	50.4 (1.3)	54.0 (0.6)	37.4 (12.6)	55.6 (8.8)	33.0 (5.7)	53.5 (4.7)
0.3	60.9 (1.0)	54.0 (0.3)	46.2 (0.4)	61.6 (0.4)	36.2 (0.1)	56.3 (0.2)
0.4	76.9 (1.2)	53.5 (1.5)	34.0 (10.3)	52.0 (7.4)	29.5 (5.4)	50.6 (4.1)
0.5	100.3 (1.4)	53.2 (0.9)	36.3 (9.8)	53.6 (6.8)	30.6 (4.2)	51.3 (3.2)
Table 5: The performance of StructFormer on PTB dataset with different mask rates. Dependency
parsing is especially affected by the masks. Mask rate 0.3 provides the best and the most stable
performance.
15