Under review as a conference paper at ICLR 2021
Legendre Deep Neural Network (LDNN)
and its application for approximation of non-
LINEAR VOLTERRA-FREDHOLM-HAMMERSTEIN IN-
TEGRAL EQUATIONS
Anonymous authors
Paper under double-blind review
Ab stract
Various phenomena in biology, physics, and engineering are modeled by differen-
tial equations. These differential equations including partial differential equations
and ordinary differential equations can be converted and represented as integral
equations. In particular, Volterra-Fredholm-Hammerstein integral equations are
the main type of these integral equations and researchers are interested in inves-
tigating and solving these equations. In this paper, we propose Legendre Deep
Neural Network (LDNN) for solving nonlinear Volterra-Fredholm-Hammerstein
integral equations (V-F-H-IEs). LDNN utilizes Legendre orthogonal polynomials
as activation functions of the Deep structure. We present how LDNN can be used
to solve nonlinear V-F-H-IEs. We show using the Gaussian quadrature colloca-
tion method in combination with LDNN results in a novel numerical solution for
nonlinear V-F-H-IEs. Several examples are given to verify the performance and
accuracy of LDNN.
1 Introduction
Deep neural networks are a main and beneficial part of machine learning family which are applied in
various areas including speech processing, computer vision, natural language processing and image
processing (LeCun et al., 2015; Krizhevsky et al., 2012). Also, the approximation of the functions
is a significant branch in scientific computational and achieving success in this area is considered
by some research (Tang et al., 2019; Hanin, 2019). Solving differential equations is the other main
branch of scientific computational which neural networks and deep learning have been shown suc-
cess in this area. (LamPle & Charton, 2019; Berg & Nystrom, 2018; Raissi et al., 2019). Various
phenomena in biology, physics, finance, neuroscience and engineering are modeled by differential
equations (Courant & Hilbert, 2008; Davis, 1961). In recent years, several researchers studied the
solving differential equations via deeP learning or neural networks. differential equations consists
of ordinary differential equations, Partial differential equations and integral equations. (Sirignano &
SPilioPoulos, 2018; Lu et al., 2019; Meng et al., 2020). It is notable that the various numerical meth-
ods are aPPlied for solving differential equations. HomotoPy analysis method (HAM) (Liao, 2012)
and variational iteration method (VIM) (He & Wu, 2007) are known as analytical/semi-analytical
methods. Usually, sPectral methods (Canuto et al., 2012), Runge-Kutta methods (Hairer et al., 2006),
the finite difference methods (FDM) (Smith, 1985) and the finite element methods (FEM) (Johnson,
2012) are considered as the PoPular numerical methods. When the comPlexity of the model does not
allow us to obtain the solution exPlicitly, numerical methods are a ProPer selection for finding the
aPProximate solution for the models. Recently, some of the machine learning methods are aPPlied
for solving differential equations. Chakraverty & Mall (2017) introduced orthogonal neural net-
works which used orthogonal Polynomials in the structure of the network. Raja et al. (2019) aPPlied
meta-heuristic oPtimization algorithm to neural network for obtaining the solution of differential
equations. Moreover, other methods of machine learning such as suPPort vector machine (VaPnik,
2013) are used to aPProximate the solution of the models. Least squares suPPort vector machines
are considered in these researches (Hajimohammadi et al., 2020; Mehrkanoon & Suykens, 2015).
Baker et al. (2019) selected deeP neural networks for solving the differential equations. Pang et al.
1
Under review as a conference paper at ICLR 2021
(2019) introduced a new network to find the solution of the different equations. Han et al. (2018)
solved high-dimensional problems via deep networks. Also, Long et al. (2018) and Raissi et al.
(2019) introduced a group of the equations which solved by deep learning. Furthermore, He et al.
(2018) and Molina et al. (2019) investigated the effect of the activation function on networks.
In this paper, We concern nonlinear Volterra-Fredholm-Hammerstein integral equations (V-F-H-
IEs) and try to obtain the solution of them via deep neural network. We present a new numerical
approach of machine learning Which is a combination of deep neural netWork and Legendre collo-
cation method. This approach is useful for solving the differential equations and We applied it for
solving nonlinear V-F-H-IEs. We used Legendre collocation method to our netWork for perfect the
numerical computations and enhancement the performance the netWork.
2 Legendre Deep Neural Network (LDNN)
The main purpose of introducing LDNN is to apply it for solving differential models. Indeed, this
purpose is to expand the utilization of deep learning netWorks in the field of scientific computing,
especially the solution of differential equations. Moreover, this netWork has the advantages of solv-
ing equations by deep learning as Well as numerical methods such as collocation method used to
achieve better solution to the equations. LDNN presents a combination of a deep neural netWork
and Legendre collocation method. In fact, our netWork consists of tWo netWorks Which have con-
nected consecutive to each other. The first netWork is a feed forWard neural netWork Which has
an orthogonal Legendre layer. The second netWork includes operation nodes to create the desired
computational model. In recent decades, numerical methods especially collocation method are pop-
ular methods for solving differential equations. In the collocation method, first an approximation
of the solution is expanded by using the sum of the basic functions. The basic functions consists
of the orthogonal polynomials such as Legendre polynomials.Then this approximation is placed in
the differential equation. By considering the appropriate set of candidate points, an attempt is made
to obtain the unknoWn coefficients of the basic functions so that the solution satisfies the equation
in a set of candidate points. The first netWork is applied to creat the approximation of the solution.
This approximation can be knoWn as the scattered data interpolation problem. The second netWork
is used to obtain the desired equation so that the solution satisfies it. The structure of LDNN is
described in detail at the folloWing rest.
Consider that the first netWork has a M-layer Which defined as folloWs:
H0 = x,	x ∈ Rd,
H1 = L(W(1)H0 + b(1)),
Hi=f(W(i)Hi-1+b(i)),	2≤i≤M-1,
HM = W (M)HM-1 + b(M).
Where H0 is the input layer With d dimension. Hi , 1 ≤ i ≤ M - 1 are hidden layers,
L = [L0, L1, ...Ln]T Which Li are i-th degrees of Legendre orthogonal polynomials, H1 is an
orthogonal layer, f is the hyperbolic tangent activation function or other commonly used activa-
tion functions. W(i), i = 1,…，M are the weight parameters and b(i), 1 ≤ i ≤ M are the
bias parameters. HM is the output layer. It is notable that the second netWork is applied to obtain
the desired differential model. This aim is possible by using operation nodes including integrals,
derivatives, and etc. These nodes are applied to the output of the first network. Moreover, automatic
differentiation (AD) (Baydin et al., 2017) and Legendre Gaussian integration (Shen et al., 2011)
have been used in network computing to obtain more accurate and fast calculations. How to train
the network and set the parameters are also important points. Supervised learning method is used to
train network. The cost function for setting parameters is defined as follows:
CostFun = min(yt - yp) + min(Rm).	(1)
where yt is an exact value of the model and yp is a predicted value of the LDNN. The definition
of Rm is explained in section 3.The minimization of CostFun is obtained by performing Adam
algorithm (Kingma & Ba, 2015) and the L-BFGS method (Liu & Nocedal, 1989) on mean squared
errors of training data set.
2
Under review as a conference paper at ICLR 2021
2.1	Legendre polynomials
Legendre polynomials (Shen et al., 2011) are a main series of orthogonal polynomials which denoted
by Ln(η), are defined as:
L („) = 1 X (-1)'	(2n — 冽]	„n—2'
n(η)=2n 幺(1) 2n'!(n - ')!(n - 2')!η
(2)
Legendre polynomials are defined in [-1, 1] domain and have the recurrence formula in the follow-
ing form:
(n+ 1)Ln+1(η) = (2n+ 1)ηLn (η) - nLn-1 (η),	n ≥ 1,
L0 (η) = 1,	L1 (η) = η.	(3)
Orthogonality relation for these polynomials is as follows:
Ln (η)Lm (η)dη = γδn,m,
(4)
where δn,m is a delta Kronecker function and Y = ^^+.
The weight function of them is W(η) = 1. Some following useful properties of Legendre polyno-
mials are defined:
Ln(-η) = (-1)nLn(η),	(5)
∣Ln(η)∣≤ 1,	∀η ∈ [-1,1], n ≥ o,	(6)
Ln(±1) = (±1)n,	(7)
(2n + 1)Ln(η) = L0n+1(η) - L0n-1(η),	n ≥ 1.	(8)
3 Nonlinear VOLTERRA-Fredholm-Hammerstein integral
equations and LDNN
The general form of nonlinear Volterra-Fredholm-Hammerstein integral equations (V-F-H-iEs) is
as follows:
y(x) = g(x) + ξι / Kι(x,s)夕ι(s,y(s))ds + ξ / K2(x,s)22(s,y(s))ds, X ∈ [0,1]. (9)
where ξ1, ξ2 are fixed, g(χ), Ki(x, S) and K2(χ, S) are given functions and 夕ι(s, y(s)), φ2(s, y(s))
are nonlinear functions. The aim is to find the proper y(x). in order to use the LDNN, reformulated
Eq. (9) in the following form:
Rm = -y(x)+ g(x)+ ξι / Ki(x,s)Pi(s,y(s))ds + ξ2 / K2(x,s)^2(s,y(s))ds, X ∈ [0,1].
00
(10)
y(x) is approximated by the first network of the LDNN.
y(X) ≈ HM.	(11)
Furthermore, we applied Legendre-Gauss integration formula (Shen et al., 2011):
iN
h(X)dX =	ωjh(Xj )
(12)
where {Xj}N=o are the roots of Ln+i and ® }j=0 =(i—x2)(L2一(X.)产.Here, We should transfer
the [0, X] and [0, 1] domains into the [-1, 1] domain. it is possible by using the following transfor-
mation:
11	——s ——1, t2	2s ——1.
X
3
Under review as a conference paper at ICLR 2021
Consider
Zι(x, S) = Kι(x, s)n(s,y(s)),
Z2(x, S) = K2(x, S)22(s,y(s)).
we have
Rm = -y(X) + g(X) + ξ1 2 Z Z1(x, 2(t1 + I))dt1 + ɪ Z Z2(x, 2(t2 + I))dt2.
(13)
by using Legendre-GaUss integration formula, the below form is concluded:
X	N1	X	ξ N2	X
Rm =	-y(X)	+ g(X)	+ ξ1 2	X	ω1j ZI(X, 2(t1j	+ I)) + -2 X ω2j Z2(X, 2(t2j	+ 1)).	(14)
j=0	j=0
The second network of LDNN and its nodes makes Rm . The architecture of LDNN for solving
nonlinear V-F-H-IEs is represented in Figure 1.
Oo .∙ O ɪ
first network
second network
Figure 1: The architecture of LDNN for solving nonlinear V-F-H-IEs. The first network approxi-
mates the solution of IE y(x). This network has M-layer and feed forward neural network is the
structure of it. H1 is introduced as a orthogonal layer which consists of p neurons with {Li}ip=0
(Legendre polynomials) as activation functions. Other layers have f, hyperbolic tangent as activa-
tion functions. The second network with the nodes makes the desired model and the output of it, is
Rm (consider Eq. (14)). The outputs of LDNN are y(x) and Rm .
4 Numerical results
In order to present the accuracy and performance of the LDNN for solving nonlinear V-F-H-IEs and
justify the efficiency of the proposed method, several examples are given. The convergence behavior
of the LDNN is reported by using the following parameters:
The exact value yt, the predicted value yp and the absolute error (Error) in some points of test data
are reported in various tables. The number of the train data m1, the number of Legendre quadrature
points (N1, N2), the number of the test data m2, the structure of network M-layers, Lt2rain and
Lt2est are shown in Table 1. Lt2rain and Lt2est are calculated as follows:
Ltran = l∣yt -yp∣∣2 = XX(yt(χji(Xj))2]2,
j=1
L2est = ||yt - yp||2 = [X(yt(Xj) -yp(Xj))2] 1,	(15)
j=1
4
Under review as a conference paper at ICLR 2021
Table 1: The LDNN parameters for all the experiments. The structure of M-Layers indicates by
[d,NL⑴，NL⑵，…，NL(M-1), 1]. This network has d dimension in input layer, M- 1 hidden
layers with NL('), 2 ≤ ' ≤ M- 1, neurons in each layer and one output which approximates the
y(x). All the experiments have 4 hidden layers.
Experiment	M-Layers	m1	(N1,N2)	Ltrain	m2	Ltest
Experiment 1	[1, 10, 30, 20, 10, 1]	500	(50, -)	3.937867e-09	100	4.015095e-09
Experiment 2	[1, 10, 30, 20, 10, 1]	500	(50, 50)	7.156029e-09	100	7.537263e-09
Experiment 3	[1, 10, 30, 20, 10, 1]	500	(50, 50)	1.347132e-09	100	1.659349e-08
Experiment 4	[1, 10, 30, 20, 10, 1]	500	(50, 50)	9.182442e-09	100	1.107755e-09
Table 2:	The exact value, the predicted value and the absolute error (Error) in several test points on
[0, 1] domain for Experiment 1.
x exact value (yt = ex)	predicted value (yp)	Error
0.0	1.0	1.000000049	4.90000001e-08
0.2	1.22140276	1.221402765	4.99999997e-09
0.4	1.4918247	1.49182494	2.40000000e-07
0.6	1.8221188	1.82211831	4.90000000e-07
0.8	2.22554093	2.225540981	5.09999998e-08
1.0	2.71828183	2.71828179	4.00000002e-08
The Tensorflow package of Python version 3.7.0. is applied for writing the code of all experiments.
Adam algorithm is stoped when the number of iteration is up to 5000 and L-BFGS method is stoped
when it converges. The figures are obtained on the test data set.
4.1 Experiment 1
Suppose that we have the following model (Yousefi & Razzaghi, 2005):
y(x)=ex - 1e3x + 1+ [ y3(s)ds, X ∈ [0,1].	(16)
3	30
It has the exact solution y(x) = ex. Table 2 represents the exact value, the predicted value and the
absolute error (Error) in several test points on [0, 1] domain. 50 points of shifted Legendre quadrature
points are applied for training LDNN. The number of train data set is 500 and the number of test
data set is 100. Figure 2 shows the illustrated comparison between yt and yp.
Figure 2: Results of Experiment 1. Exact solution yt(x) = ex, predicted solution yp(x) by LDNN.
5
Under review as a conference paper at ICLR 2021
Table 3:	The exact value, the predicted value and the absolute error (Error) in several test points on
[0, 1] domain for Experiment 2.
x exact value (yt = cos(x))	predicted value (yp)	Error
0.0	1.0	1.000000059	5.90000000e-08
0.2	0.98006658	0.98006683	2.50000000e-07
0.4	0.92106099	0.92106083	1.50000000e-07
0.6	0.82533561	0.82533555	6.00000000e-08
0.8	0.69670671	0.69670670	9.99999994e-09
1.0	0.54030231	0.54030237	6.00000001e-08
4.2 Experiment 2
Suppose that we have the following model (Razzaghi & Ordokhani, 2002):
where
y(x) = 1 + sin2(x) +	K(x, s)y2 (s)ds,
0
x ∈ [0, 1].
K(x, s) = -3 sin(0x - s),
0 ≤ s ≤ x;
x ≤ s ≤ 1.
(17)
(18)
It has the exact solution y(x) = cos(x). The exact value, the predicted value and the absolute
error (Error) in several test points on [0, 1] domain are reported in Table 3. 50 points of shifted
Legendre quadrature points are applied for training LDNN. The number of train data set is 500 and
the number of test data set is 100. Figure 3 shows the illustrated comparison between yt and yp.
Figure 3: Results of Experiment 2. Exact solution yt (x) = cos(x), predicted solution yp(x) by
LDNN.
4.3 Experiment 3
Suppose that we have the following model (Babolian et al., 2007):
y(x)
g(x) +	(x - s)y2 (s)ds +	(x + s)y(s)ds,
x ∈ [0, 1].
(19)
where
g(x) = - —x6 + L4 — x2 + -x --
g ,	30	3	3	4
(20)
It has the exact solution y(x) = x2 - 2. Table 4 illustrates the exact value, the predicted value
and the absolute error (Error) in several test points on [0, 1] domain. 50 points of shifted Legendre
quadrature points are applied for training LDNN. The number of train data set is 500 and the number
of test data set is 100. Figure 4 represented the comparison between yt and yp.
6
Under review as a conference paper at ICLR 2021
Table 4: The exact value, the predicted value and the absolute error (Error) in several test points on
[0, 1] domain for Experiment 3.
x	exact value (yt = x2 - 2)	predicted value (yp )	Error
0.0	-2.0	-2.00000001	9.99999994e-09
0.2	-1.96	-1.96000049	4.90000000e-07
0.4	-1.84	-1.840000009	8.99999986e-09
0.6	-1.64	-1.64000036	3.60000000e-07
0.8	-1.36	-1.35999998	2.00000001e-08
1.0	-1.0	-0.99999999	1.00000001e-08
Figure 4: Results of Experiment 3. Exact solution yt(x) = x2 - 2, predicted solution yp(x) by
LDNN.
4.4 Experiment 4
Suppose that we have the following model (Hadizadeh & Mohamadsohi, 2005):
y(X) = -TTix4 + χ2+ + Q + [ ʒ-y2(S)ds,	X ∈ [0,1].	QI)
10	6	8	0 2x
It has	the	exact	solution y(x) = x2 + 2. The exact value, the predicted value	and	the	absolute
error (Error)	in	several test points on [0, 1] domain are reported in	Table 5. 50	points	of	shifted
Legendre quadrature points are applied for training LDNN. The number of train data set is 500 and
the number of test data set is 100. Figure 5 shows the illustrated comparison between yt and yp.
Figure 5: Results of Experiment 4. Exact solution yt(x) = χ2 + 2, predicted solution yp(x) by
LDNN.
5 Conclusion
Legendre deep neural network (LDNN) is introduced in this paper. LDNN and its application for
solving nonlinear Volterra-Fredholm-Hammerstein integral equations (V-F-H-IES) are proposed.
7
Under review as a conference paper at ICLR 2021
Table 5: The exact value, the predicted value and the absolute error (Error) in several test points on
[0, 1] domain for Experiment 4.
X	exact value (y = x2 + 2) predicted value (yp)	Error
0.0	0.5	0.50000004	4.00000000e-09
0.2	0.54	0.54000001	9.99999994e-09
0.4	0.66	0.66000002	2.00000000e-08
0.6	0.86	0.85999998	2.00000000e-08
0.8	1.14	1.13999999	9.99999994e-09
1.0	1.5	1.49999999	9.99999994e-09
LDNN includes two networks. The first network approximates the solution of a nonlinear V-F-H-IE
y(x) which has M-layers feed forward neural network structure. The first hidden layer of this has a
orthogonal layer consists of Legendre polynomials as activation functions. The last network adjusts
the output of the sooner network to fit to a desired equation form. The better performance of the
network has been obtained by using Legendre Gaussian integration and automatic differentiation.
Some experiments of nonlinear V-F-H-IEs are given to investigate the reliability and validity of
LDNN. The results show that this network is an efficient and has high accuracy.
References
Esmail Babolian, F Fattahzadeh, and E Golpar Raboky. A chebyshev approximation for solving
nonlinear integral equations of hammerstein type. Applied Mathematics and Computation, 189
(1):641-646, 2007.
Nathan Baker, Frank Alexander, Timo Bremer, Aric Hagberg, Yannis Kevrekidis, Habib Najm, Man-
ish Parashar, Abani Patra, James Sethian, Stefan Wild, et al. Workshop report on basic research
needs for scientific machine learning: Core technologies for artificial intelligence. Technical
report, USDOE Office of Science (SC), Washington, DC (United States), 2019.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. The Journal of Machine Learning Re-
search, 18(1):5595-5637, 2017.
Jens Berg and Kaj Nystrom. A unified deep artificial neural network approach to partial differential
equations in complex geometries. Neurocomputing, 317:28-41, 2018.
Claudio Canuto, M Yousuff Hussaini, Alfio Quarteroni, A Thomas Jr, et al. Spectral methods in
fluid dynamics. Springer Science & Business Media, 2012.
Snehashish Chakraverty and Susmita Mall. Artificial neural networks for engineers and scientists:
solving ordinary differential equations. CRC Press, 2017.
Richard Courant and David Hilbert. Methods of Mathematical Physics: Partial Differential Equa-
tions. John Wiley & Sons, 2008.
Harold Thayer Davis. Introduction to nonlinear differential and integral equations. US Government
Printing Office, 1961.
M Hadizadeh and M Mohamadsohi. Numerical solvability of a class of volterra-hammerstein inte-
gral equations with noncompact kernels. Journal of Applied Mathematics, 2005, 2005.
Ernst Hairer, Christian Lubich, and Michel Roche. The numerical solution of differential-algebraic
systems by Runge-Kutta methods, volume 1409. Springer, 2006.
Z Hajimohammadi, F Baharifard, and K Parand. A new numerical learning approach to solve general
falkner-skan model. Engineering with Computers, pp. 1-17, 2020.
8
Under review as a conference paper at ICLR 2021
Jiequn Han, Arnulf Jentzen, and E Weinan. Solving high-dimensional partial differential equations
using deep learning. Proceedings of the National Academy of Sciences, 115(34):8505-8510,
2018.
Boris Hanin. Universal function approximation by deep neural nets with bounded width and relu
activations. Mathematics, 7(10):992, 2019.
Ji-Huan He and Xu-Hong Wu. Variational iteration method: new development and applications.
Computers & Mathematics with Applications, 54(7-8):881-894, 2007.
Juncai He, Lin Li, Jinchao Xu, and Chunyue Zheng. Relu deep neural networks and linear finite
elements. arXiv preprint arXiv:1807.03973, 2018.
Claes Johnson. Numerical solution of partial differential equations by the finite element method.
Courier Corporation, 2012.
Diederik P Kingma and Jimmy Ba. Adam (2014), a method for stochastic optimization. In Pro-
ceedings of the 3rd International Conference on Learning Representations (ICLR), arXiv preprint
arXiv, volume 1412, 2015.
Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-
lutional neural networks. In Advances in neural information processing systems, pp. 1097-1105,
2012.
GuillaUme Lample and Francois Charton. Deep learning for symbolic mathematics. arXiv preprint
arXiv:1912.01412, 2019.
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature, 521(7553):436-444,
2015.
Shijun Liao. Homotopy analysis method in nonlinear differential equations. Springer, 2012.
Dong C Liu and Jorge Nocedal. On the limited memory bfgs method for large scale optimization.
Mathematical programming, 45(1-3):503-528, 1989.
Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net: Learning pdes from data. In
International Conference on Machine Learning, pp. 3208-3216, 2018.
Lu Lu, Xuhui Meng, Zhiping Mao, and George E Karniadakis. Deepxde: A deep learning library
for solving differential equations. arXiv preprint arXiv:1907.04502, 2019.
Siamak Mehrkanoon and Johan AK Suykens. Learning solutions to partial differential equations
using ls-svm. Neurocomputing, 159:105-116, 2015.
Xuhui Meng, Zhen Li, Dongkun Zhang, and George Em Karniadakis. Ppinn: Parareal physics-
informed neural network for time-dependent pdes. Computer Methods in Applied Mechanics and
Engineering, 370:113250, 2020.
Alejandro Molina, Patrick Schramowski, and Kristian Kersting. Pade activation units: End-to-
end learning of flexible activation functions in deep networks. In International Conference on
Learning Representations, 2019.
Guofei Pang, Lu Lu, and George Em Karniadakis. fpinns: Fractional physics-informed neural
networks. SIAM Journal on Scientific Computing, 41(4):A2603-A2626, 2019.
Maziar Raissi, Paris Perdikaris, and George E Karniadakis. Physics-informed neural networks: A
deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational Physics, 378:686-707, 2019.
Muhammad Asif Zahoor Raja, Jabran Mehmood, Zulqurnain Sabir, A Kazemi Nasab, and Muham-
mad Anwaar Manzar. Numerical solution of doubly singular nonlinear systems using neural
networks-based integrated intelligent computing. Neural Computing and Applications, 31(3):
793-812, 2019.
9
Under review as a conference paper at ICLR 2021
Mohsen Razzaghi and Yadollah Ordokhani. A rationalized haar functions method for nonlinear
fredholm-hammerstein integral equations. International journal of computer mathematics, 79(3):
333-343, 2002.
Jie Shen, Tao Tang, and Li-Lian Wang. Spectral methods: algorithms, analysis and applications,
volume 41. Springer Science & Business Media, 2011.
Justin Sirignano and Konstantinos Spiliopoulos. Dgm: A deep learning algorithm for solving partial
differential equations. Journal of computational physics, 375:1339-1364, 2018.
Gordon D Smith. Numerical solution of partial differential equations: finite difference methods.
Oxford university press, 1985.
Shanshan Tang, Bo Li, and Haijun Yu. Chebnet: Efficient and stable constructions of deep
neural networks with rectified power units using chebyshev approximations. arXiv preprint
arXiv:1911.05467, 2019.
Vladimir Vapnik. The nature of statistical learning theory. Springer science & business media,
2013.
S Yousefi and Mohsen Razzaghi. Legendre wavelets method for the nonlinear volterra-fredholm
integral equations. Mathematics and computers in simulation, 70(1):1-8, 2005.
10