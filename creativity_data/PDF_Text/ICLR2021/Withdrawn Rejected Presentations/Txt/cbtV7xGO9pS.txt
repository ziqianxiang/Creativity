Under review as a conference paper at ICLR 2021
TEAC: Integrating Trust Region and MAX
Entropy Actor Critic for Continuous Control
Anonymous authors
Paper under double-blind review
Ab stract
Trust region methods and maximum entropy methods are two state-of-the-art
branches used in reinforcement learning (RL) for the benefits of stability and ex-
ploration in continuous environments, respectively. This paper proposes to inte-
grate both branches in a unified framework, thus benefiting from both sides. We
first transform the original RL objective to a constraint optimization problem and
then proposes trust entropy actor-critic (TEAC), an off-policy algorithm to learn
stable and sufficiently explored policies for continuous states and actions. TEAC
trains the critic by minimizing the refined Bellman error and updates the actor by
minimizing KL-divergence loss derived from the closed-form solution to the La-
grangian. We prove that the policy evaluation and policy improvement in TEAC
is guaranteed to converge. We compare TEAC with 4 state-of-the-art solutions
on 6 tasks in the MuJoCo environment. The results show that TEAC with opti-
mized parameters achieves similar performance in half of the tasks and notably
improvement in the others in terms of efficiency and effectiveness.
1	Introduction
With the use of high-capacity function approximators, such as neural networks, reinforcement learn-
ing (RL) becomes practical in a wide range of real-world applications, including game playing (Mnih
et al., 2013; Silver et al., 2016) and robotic control (Levine et al., 2016; Haarnoja et al., 2018a).
However, when dealing with the environments with continuous state space or/and continuous ac-
tion space, most existing deep reinforcement learning (DRL) algorithms still suffer from unstable
learning processes and are impeded from converging to the optimal policy.
The reason for unstable training process can be traced back to the use of greedy or -greedy policy
updates in most algorithms. With the greedy update, a small error in value functions may lead
to abrupt policy changes during the learning iterations. Unfortunately, the lack of stability in the
training process makes the DRL unpractical for many real-world tasks (Peters et al., 2010; Schulman
et al., 2015; Tangkaratt et al., 2018). Therefore, many policy-based methods have been proposed
to improve the stability of policy improvement (Kakade, 2002; Peters & Schaal, 2008; Schulman
et al., 2015; 2017). Kakade (2002) proposed a natural policy gradient-based method which inspired
the design of trust region policy optimization (TRPO). The trust region, defined by a bound of
the Kullback-Leibler (KL) divergence between the new and old policy, was formally introduced in
Schulman et al. (2015) to constrain the natural gradient policy changing within the field of trust.
An alternative to enforcing a KL divergence constraint is to utilize the clipped surrogate objective,
which was used in Proximal Policy Optimization (PPO) (Schulman et al., 2017) to simplify the
objective of TRPO while maintaining similar performance. TRPO and PPO have shown significant
performance improvement on a set of benchmark tasks. However, these methods are all on-policy
methods requiring a large number of on-policy interaction with environment for each gradient step.
Besides, these methods focus more on the policy update than exploration, which is not conducive to
finding the global optimal policy.
The globally optimal behavior is known to be difficult to learn due to sparse rewards and insuf-
ficient explorations. in addition to simply maximize the expected reward, maximum entropy RL
(MERL) (Ziebart et al., 2008; Toussaint, 2009; Haarnoja et al., 2017; Levine, 2018) proposes to
extend the conventional RL objective with an additional “entropy bonus” argument, resulting in the
preferences to the policies with higher entropy. The high entropy of the policy explicitly encourages
1
Under review as a conference paper at ICLR 2021
exploration, thus improving the diverse collection of transition pairs, allowing the policy to capture
multi-modes of good policies, and preventing from premature convergence to local optima. MERL
reforms the reinforcement learning problem into a probabilistic framework to learn energy-based
policies to maintain the stochastic property and seek the global optimum. The most representative
methods in this category are soft Q-learning (SQL) (Haarnoja et al., 2017) and Soft Actor Critic
(SAC) (Haarnoja et al., 2018b;c). SQL defines a soft Bellman equation and implements it in a prac-
tical off-policy algorithm which incorporates the entropy of the policy into the reward to encourage
exploration. However, the actor network in SQL is treated as an approximate sampler, and the con-
vergence of the method depends on how well the actor network approximates the true posterior. To
address this issue, SAC extends soft Q-learning to actor-critic architecture and proves that a given
policy class can converge to the optimal policy in the maximum entropy framework. However, off-
policy DRL is difficult to stabilize in policy improvement procedure (Sutton & Barto, 1998; van
Hasselt et al., 2018; Ciosek et al., 2019) which may lead to catastrophic actions, such as ending the
episode and preventing further learning.
Several models have been proposed to benefit from considering both the trust region constraint and
the entropy constraint, such as MOTO (Akrour et al., 2016) , GAC (Tangkaratt et al., 2018), and
Trust-PCL (Nachum et al., 2018). However, MOTO and GAC cannot efficiently deal with high-
dimensional action space because they rely on second-order computation, and Trust-PCL suffers
from algorithm efficiency due to its requirement of trajectory/sub-trajectory samples to satisfy the
pathwise soft consistency.
Therefore, in this paper, we propose to further explore the research lines of unifying trust region
policy-based methods and maximum entropy methods. Specifically, we first transform the RL prob-
lem into a primal optimization problem with four additional constraints to 1) set an upper bound of
KL divergence between the new policy and the old policy to ensure the policy changes are within
the region of trust, 2) provide a lower bound of the policy entropy to prevent from a premature con-
vergence and encourage sufficient exploration, and 3) restrain the optimization problem as a Markov
Decision Process (MDP). We then leverage the Lagrangian duality to the optimization problem to
redefine the Bellman equation which is used to verify the policy evaluation and guarantee the pol-
icy improvement. Thereafter, we propose a practical trust entropy actor critic (TEAC) algorithm,
which trains the critic by minimizing the refined Bellman error and updates the actor by minimizing
KL-divergence loss derived from the closed-form solution to the Lagrangian. The update proce-
dure of the actor involves two dual variables w.r.t. the KL constraint and entropy constraint in the
Lagrangian. Based on the Lagrange dual form of the primal optimization problem, we develop
gradient-based method to regulate the dual variables regarding the optimization constraints.
The key contribution of the paper is a novel off-policy trust-entropy actor-critic (TEAC) algorithm
for continuous controls in DRL. In comparison with existing methods, the actor of TEAC updates
the policy with the information from the old policy and the exponential of the current Q function, and
the critic of TEAC updates the Q function with the new Bellman equation. Moreover, we prove that
the policy evaluation and policy improvement in trust entropy framework is guaranteed to converge.
A detailed comparison with similar work, including MOTO (Akrour et al., 2016), GAC (Tangkaratt
et al., 2018), and Trust-PCL (Nachum et al., 2018), is provided in Sec. 4 to explain that TEAC is the
most effective and most theoretically complete method. We compare TEAC with 4 state-of-the-art
solutions on the tasks in the MuJoCo environment. The results show that TEAC is comparable with
the state-of-the-art solutions regarding the stability and sufficient exploration.
2	Preliminaries
A RL problem can be modeled as a standard Markov decision process (MDP), which is represented
as a tuple hS, A, r, p, p0, γi. S and A denote the state space and the action space, respectively.
p0 (s) denotes the initial state distribution. At time t, the agent in state st selects an action at
according to the policy π(a∣s), in which the performance of the state-action pair is quantified by the
reward function r(st, at) and the next state of the agent is decided by the transition probability as
st+ι 〜p(st+ι∣st, at). The goal of the agent is to find the optimal policy π(a∣s) to maximize the
expected reward Es。,。。,... [P∞=0 Ytr(St,at)], where s° 〜po(s) and st+ι 〜p(st+ι∣st, aj Y is a
discount factor (0 < γ < 1)) which quantifies how much importance we give for future rewards.
2
Under review as a conference paper at ICLR 2021
The state-action value function Qπ (st , at) and the value function V π (st) are then defined as:
Qπ(st,at)
= Est+1 ,at+1 ,...
∞
Xγlr(st+l,at+l)	,Vπ(st) = Eat
,st+1,...
l=0
∞
γlr(st+l,at+l)
l=0
For the continuous environments, which is the focus of this paper, S and A denote finite dimensional
real valued vector spaces, s denotes the real-valued state vector, and a denotes the real-valued action
vector. The expected reward can be defined as :
J(π) = E(s,a)
~ρ∏ (s,a) [Qπ(s, a)] =
EPn (s)π(a∣s)[Qπ (s, a)],	(1)
where ρπ (s) and ρπ (s, a) denote the (discounted) state and (discounted) state-action marginals of
the trajectory distribution induced by a policy π(a∣s). 1
3	Our Method
This section explains the details and features of the TEAC framework with the focus on the mathe-
matical deductions and proofs of the guaranteed policy improvement and convergence in an actor-
critic architecture.
3.1	Primal and Dual Optimization Problem
To stabilize the training process and steer the exploration, in addition to simply maximizing the
expected reward with (-) greedy policy updates, we propose to 1) confine the KL-divergence be-
tween neighboring policies in the training procedure to avoid large-step policy updates, and 2) favor
a stochastic policy with relatively larger entropy to avoid premature convergence due to insufficient
exploration. Therefore, we define the RL problem as a primal optimization problem with additional
constraints, given as:
max∏	EPn(S)π(a∣s) [Q(s, a)],
subject to EPn(S) [KL(π(∙∣s)k∏oid(∙∣s))] ≤ τ,
EPn(s)[H(n(1S))] ≥ η,	(2)
EPn(S) ∕∏(a∣s)da = 1,
ʌ ʌ
EPn(S)∏(a∣s)p(s0∣s,a)v (S/) = EPn(SO)V (S/),
where Q(S, a) is a critic estimating the state-action value function whose parameter is learned such
that Q(s, a) ≈ Qn(s, a), π(∙∣s) is the policy distribution to be learned, ∏old(∙∣s) is the prior policy
distribution, and V (S0) is a state feature function estimating the state value function of the next state.
The term KL (∏(∙∣s)k∏old(∙∣s)) = E∏(a∣S)[log∏(a∣s) - log∏old(a∣s)] confines the KL-divergence
between the distributions of the new and old policies. The third constraint ensures that the state-
action marginal of the trajectory distribution is a proper probability density function. As the state
marginal of the trajectory distribution needs to comply with the policy π(a∣s) and the system dy-
namics p(s0∣s, a), i.e., ρ∏(s)π(a∣s)p(s0∣s, a) = ρ∏(s0), meanwhile the direct matching of the state
probabilities is not feasible in continuous state spaces, the use of V(s0) in the fourth constraint which
can be also considered as state features, helps to focus on matching the feature averages. These last
two constraints formally restrain the optimization problem within a MDP framework.
The objective is to maximize the expected reward of a policy while ensuring it satisfies the lower
bound of entropy and upper bound of distance from the previous policy. The constraint of KL-
divergence term helps to avoid the abrupt difference between the new and old policies, while the
constraint of the entropy term helps to promote the policy exploration.
The entropy constraint is crucial in our optimization problem for two reasons: 1) Prior studies show
that the use of KL-bound leads to a rapid decrease of the entropy, thus bounding the entropy helps
to lower the risk of premature convergence induced by the KL-bound; 2) Each iteration of policy
update will modify the critic Q(S, a) and the state distribution ρπ(S), thus changing the optimization
1Following Sutton et al. (2000), we use ρπ in the paper to implicate that ρπ is the stationary distribution of
states under π and independent of s0 for all policies.
3
Under review as a conference paper at ICLR 2021
da - (α + β + λ)
(4)
landscape of the policy parameters. The entropy constraint ensures the exploration in the action
space in case of evolving optimization landscapes.
The Lagrangian of this optimization problem is denoted as:
L(∏,α,β,λ,ν)=Eρ(s)∏(a∣s)[Q(s, a)] + α (τ --。缶)[KL (∏(∙∣s)k∏oid(∙∣s))])
+	β (EP(S)[H(∏(∙∣s))] — η) + λ (EP(S) / ∏(a∣s)da - 1)	(3)
+	V(EP(S)π(a∣ s)p(s0 |s,a) V (S ) - EP(SO)V (S )),
where α, β, λ, ν are the dual variables, and for the sake of brevity, we use ρ(s) to represent ρπ(s).
Eq. 3 is a super set of trust region and maximum entropy methods. That is, β = 0 leads to an
equivalent objective function as the standard trust region, while α = 0, which indicates that the
KL-divergence bound is not active, leads to a maximum entropy RL objective that SAC tries to
solve.
Take derivative of L w.r.t. π and set the derivative to zero:
∂∏L =EP(S) / (Q(s, a) — (α + β) log π(a∣s) + α log π°id(a∣s) - νV(s)+
0
Ep(S0|S,a) [νV(S )]
0
=Q(s, a) - (α + β) log∏(a∣s) + α logπ°id(a∣s) - VV(s) + ..但⑶。)[νV(s )]
- (α + β + λ)
=0.
Continuous problem domains require a practical approximation to the policy update function. We
use neural networks as function approximators to parameterize the policy and Q function. Specifi-
cally, the Q function, known as critic, is modeled as expressive neural networks Qφ(S, a), and we
follow Lillicrap et al. (2016) to build a target critic network Qφ which mitigates the challenge of
overestimation. Meanwhile, the policy, known as actor, is parameterized by ∏θ(∙∣s) as a Gaussian
with mean and covariance given by neural networks, and we also build up another neural network
∏θ(∙∣s) with the same architecture as ∏θ to enable us to facilitate policy learning by leveraging the
“old” policy within our framework.
3.2 Critic Update
Given the fact that we sample actions from the actor network as parameterized Gaussian distribution
and the value function should satisfy Bellman equation2,
V(S) = Q(s, a) - (α + β)log π(a∣s) + αlog∏old(a∣s) + Ep®，®。) [V(s )] - (α + β + λ) (5)
The last constant term in Eq.5 can be ignored as it does not affect the Bellman iteration when
neural networks are used to approximate the value function. Therefore, the Bellman equation can
be redefined in our framework.
According to Eq.5, we could compute the value of a fixed policy π. Starting from any function
Q : S × A → R, we define our modified Bellman backup operator.
Definition 3.1 Bellman Equation. A modified Bellman backup operator Tπ is defined as
TnQ(s, a)，r + γEp(S0∣S,a)[V(s0)],	(6)
where
V(s) = E∏(a∣S) [Q(s, a) - (α + β)logπ(a∣s) + α logπoid(a∣s)]	(7)
is the trust entropy state-value function in our framework.
2	00
2we could utilize any form of state features V (s0). Thus, νV (s0) can be seen as another form of state
features. Therefore, ν can be arbitrary
4
Under review as a conference paper at ICLR 2021
In the sequel, Q(s, a) stands for the state-action value function obtained by iteratively applying
the modified Bellman backup operator in Eq.6 and Eq.7, which is the trust entropy Q-value in our
framework. Meanwhile, the policy evaluation can be verified accordingly.
Lemma 1 (Trust Entropy Policy Evaluation). Let Qk+1 = T πQk, the sequence Qk will converge to
the trust entropy Q-value ofπ as k → ∞ when considering a mapping Q0 : S × A → R with |A| <
∞ and the Bellman backup operator Tπ.
Proof. See Appendix A.1.
The learning of Q function can utilize off-policy methods to acquire high sample efficiency. Hence,
the parameters can be trained by minimizing the squared Bellman error, given as:
LQ(φ) = E(s,a)〜D
2 (QΦ(S, a) - y)2
(8)
where D is the replay buffer, Qφ(s, a) represents the Q network (also known as critic network)
which is parameterized by φ, and y = r (s, a) + γEs0〜P [Vφ (s0)], where Vφ denotes the value
function obtained from the target critic network Qφ with Eq. 7. The inclusion of the target critic
network helps to stabilize the training. As suggested in Lillicrap et al. (2016), φ is updated via
φ J κφ +(1 - κ)φ where 0 ≤ K ≤ 1. We set K = 0.005 in the experiments. The expected squared
Bellman error is computed with samples drawn from the replay buffer using mini-batch. Thus, the
approximate gradient of the squared Bellman error LQ (φ) w.r.t. φ is:
vφLq(Φ) =VφQφ (a, S) (Qφ (s, a) - (r(s, a) + Y (Qφ (s', a0)-
(α + β) log∏θ (a∣s0) + αlogπθ(a∣s0))),
(9)
1	/IF：	,	C	,	1 ∙	FlFl♦	, ∙ 1	1 ! ∙	IFC
where θ and θ are parameters of current policy and old policy respectively, and a0 is sampled from
current policy πθ given s0.
3.3 Actor Update
Setting ν = 0 in Eq.4 in fact does not change the optimization problem. Therefore, a closed-form
solution regarding the policy is given as:
α	Q(s, a)	α + β + λ
∏(a∣s) = ∏old(a∣s)α+β exp	exp -	∣ Q
α+β	α+β
α	Q(s, a) ʌ
Y ∏old(a∣s)α+β exp	,
α+β
(10)
where exp (-α+++λ) is the normalization term of ∏(a∣s) (The detailed derivation is provided
in A.2). It should be noted that MORE (Daniel et al., 2016), MOTO (Akrour et al., 2016), GAC
(Tangkaratt et al., 2018), and Trust-PCL (Nachum et al., 2018) can also been viewed as prior work
stemming from Eq.10. However, it is infeasible to use Eq.10 to directly update the policy given the
fact that we cannot guarantee that the resulting policy remains in the same policy class when weigh-
ing the old policy with the exponential of Q function without any assumption. Different strategies
have been applied in the prior work to address this issue, and the detailed discussion is provided in
Sec. 4.
To improve the tractability of policies, as Haarnoja et al. (2018c), we require the policy is selected
from a set of policies π ∈ Π, which is a parameterized Gaussian distribution family. This is guaran-
teed by the use of the Kullback-Leibler divergence to ensure the improved policy locates in the same
policy set. Since the normalization term exp (-α+++λ) is intractable and does not contribute to
the gradient of the new policy, it can be ignored. Therefore, the policy is updated by
L∏ (θ) = Es 〜D
DKL
πθ (a|s) k
π^(a∣s) α+β exp
)))
(11)
5
Under review as a conference paper at ICLR 2021
where D is the replay buffer, ∏θ represents the parameterized policy, ∏^ represents the parameterized
old policy, and a in Q is sampled from the current policy πθ . In practice, the old policy network
equals to the policy network in the last iteration. Therefore, we could leverage another actor network
to keep the old policy by copying θ to θ after computing the loss function of the policy and before
the back propagation in each iteration (The detailed algorithm is provided in Appendix C). With the
assumption of policy being Gaussian, the policy improvement can be guaranteed in our framework.
Lemma 2 (Trust EntroPy Policy ImProvement). Given a policy π and an old policy π, define a new
policy
α
∏(a∣s) H ∏(a∣s) α十β exp
Qπ(s, a))
α + β J
∀s.
(12)
If Q is bounded,
J π(a∣s) α+β exp
(Qn (s,a)、
α+β J
da is bounded for any S (for all π, π and π),
and the policies are the parameterized Gaussian networks. Then we can obtain Qn(s, a) ≥
Qπ (s, a), ∀s, a.
Proof. See Appendix A.3.
In other words, in the policy improvement, we use the information from the old policy and exponen-
tial of the Q function induced by the current policy to derive the policy of next iteration. Because
the Q function is a non-linear function approximation parameterized by neural networks and can be
differentiated, the reparameterization trick a = fθ(ξ; s), where ξt is an input noise vector sampled
from standard normal distribution, can be applied. Then, the approximate gradient of Lπ (θ) w.r.t. θ
is given as:
.ʌ ，一、 ______ ， .. _ ， , .. ____________ . ， ._____ .,. ，一、
▽ θL∏(θ) =Vθ(α + β)log(∏θ(a∣s)) - ^α,Qφ(s, a)Vθfθ(ξ; s).	(13)
3.4 Dual Variables Update
This section explains the updates of dual variables (α and β) throughout the entire framework.
The dual function, which is derived by substituting π(a∣s) in the Lagrangian (Eq. 3) with its form in
Eq.10, is given as:
g(α, β) = ατ — βη — (α + β) ∙ Eρ(s)
α + β + λ
a + β
(14)
As exp (α+++λ) is the normalization term of ∏(a∣s), the dual function can be represented as:
g(α,β) = ατ — βη + Ep(s)[α ∙ log ∏old(a|s) + QIs, a) — (α + β) ∙ logπ(a∣s)] .	(15)
The approximate gradient of g(α, β) w.r.t. α and β are:
V ag(α) = T — log ∏θ (a|s) + log ∏^(a∣s),	(16)
入	，八	..	...
V β g(β) = —η — log ∏θ (a|s),	(17)
which enable us to find the “proper” α and β with the gradient descent method, satisfying the KL
and entropy constraints in Eq.2. The dual variable updates, along with the trust entropy Q function
updates (Sec. 3.2) and trust entropy policy updates (Sec. 3.3), constitute the main components of our
framework.
4	Connection With previous work
The most related methods to our work are MOTO (Akrour et al., 2016) , GAC (Tangkaratt et al.,
2018), and Trust-PCL (Nachum et al., 2018) as they also consider both the trust region constraint
and the entropy constraint.
MORE (Daniel et al., 2016) considers the two constraints in the domain of stochastic search opti-
mization. MOTO (Akrour et al., 2016) extends MORE to the sequential decision making domain. In
MOTO, Q function is estimated by using a quadratic surrogate function of the state and action space,
6
Under review as a conference paper at ICLR 2021
and the policy of a log-linear Gaussian form is updated according to the KL-divergence bounding
constraint and a variable lower bound of entropy determined by the policy of each iteration.
GAC (Tangkaratt et al., 2018) further extends MOTO by 1) approximating the Q function with a
truncated Taylor series expansion and parameterizing them with a deep neural network, and 2) learn-
ing log-nonlinear Gaussian form policies. In some sense, MOTO, GAC, and ours can be viewed as
solving the optimization problem with the same constraints as stated in Eq.2. After leveraging the
Lagrangian of the optimization problem, the corresponding closed form solution for the policy up-
dating is shown in Eq.10. However, Eq.10 also indicates that the new policy is derived by weighing
the old policy and the exponential of Q function (see the R.H.S of Eq.10), which may deviate the
updated policy from the expected policy distribution class. Consequently, the KL constraint will be
no longer preserved (Akrour et al., 2018). These methods differ from each other by using different
strategies to circumvent the issue. MOTO utilizes a quadratic Q function and assumes the policy is
of log-linear Gaussian form. Consequently, MOTO can update the policy in a non-parameterized
way. GAC adopts the similar strategy as MOTO to learn a non-parameterized Gaussian actor, and
then uses this actor to guide a parameterized actor with supervised learning. However, it is hard
for MOTO and GAC to deal with high-dimensional action space because they rely on second-order
computation. In comparison, we redefine the Bellman equation and guarantee the policy improve-
ment by updating policies with Eq.11. Therefore, our method resolves the challenge simply with a
more general assumption of Gaussian policy class.
Moreover, when dealing with the dual function
g(α, β) = ατ - βη+ (α + β)Eρ(s)
log
/∏oid(a∣s)α+β exp
(Q+a) )da^
(18)
where the integral term is intractable, MOTO and GAC reply on complex second-order computation
to make it tractable. In contrast, we resolve the challenge by leveraging the policy to transform
the dual function into a simpler form which can still be optimized in first-order computation, e.g.,
stochastic gradient descent method, with the policy improvement guaranteed.
Trust-PCL (Nachum et al., 2018) addresses the challenge with a different perspective by integrating
path consistency learning (PCL) (Nachum et al., 2017), which is developed in the maximum en-
tropy framework, and trust region policy optimization method. PCL, which is the base algorithm of
trust-PCL, suggests that the optimal policy and state values should satisfy pathwise soft consistency
property along any sampled trajectory, thus allowing the use of off-policy data. Consequently, the
single-step temporal consistency of state-value function in Trust-PCL is
V * (st) = Ert,st+1 [rt - (τ + λ)log π* (at | St) + λ log ∏(at+i | st+i) + YV * (st+ι)],	(19)
which is similar to our state-value function. However, our method differs from Trust-PCL in 3 ways:
1) Trust-PCL focuses on updating the state-value function while our method focuses on updating the
Q function. 2) Trust-PCL updates the policy directly with the temporal consistency squared error
while we use Eq.11. 3) As each update iteration in Trust-PCL requires trajectory/sub-trajectory sam-
ples to satisfy the pathwise soft consistency, it significantly compromises the algorithm efficiency.
In comparison, our method requires only state-action pairs for each update iteration and is capable
of finding (sub-)optimal value for every dual variable in each update iteration.
It should be noted that proximal policy optimization (PPO) (Schulman et al., 2017) can also achieve
trust region constraint and encourage unpredictably actions by adding entropy bonus to its loss
function. However, the entropy bonus in PPO is one-off effect which only considers the current
state but no future states of the agent. In comparison, our method can be considered as maximum
long-term entropy with constraint policy in trust region.
5	Experiments
We experimented to investigate the following questions: (1) In comparison with state-of-the-art
algorithms, does TEAC have a better performance in terms of sample efficiency and computational
efficiency? (2) How should we choose hyperparameters τ and η and how can these two variables
affect the performance?
7
Under review as a conference paper at ICLR 2021
14000
12000
10000
36SSΛe
0.5	1-0	1-5	2.0	2.5	3.0
timesteps	1≡δ
Em3」36eJ3>e
(b) Hopper-v3
(c) Humanoid-v3
(a) HalfCheetah-v3
0.5	1-0	1-5	2.0	2.5	3.0
timesteps	ie6
(d) Walker2d-v3
0.5	1-0	1-5	2.0	2.5	3.0
timesteps	ie6
(e) Swimmer-v3
-1000
-2000
timesteps
(f) Ant-v3
9**
一即。
-R
一β∙c
2.5	3.0
ie6
Figure 1: Performance comparisons on six MuJoCo tasks trained for 3 million timesteps. The
horizontal axis indicates number of environment steps. The vertical axis indicates the average
return. We trained three different instances of each algorithm with different random seeds, with
each instance performing an evaluation every 4,000 environment steps. The solid lines represent
the mean and the shaded regions mark the minimum and maximum returns over the three trials. We
set η as the negative of action space dimension of the task, and set τ = 0.005 for all tasks.
5.1	Setup
We experimented the continuous control tasks (HalfCheetah-v3, Hopper-v3, Humanoid-v3,
Walker2d-v3, Swimmer-v3, Ant-v3) available from the MuJoCo environment (Todorov et al., 2012).
We compared our method TEAC with 1) proximal policy optimization (PPO) (Schulman et al.,
2017), a stable and effective on-policy policy gradient algorithm; 2) SAC (Haarnoja et al., 2018c),
the state-of-the-art off-policy algorithm for learning maximum entropy policies whose temperature
is adjusted automatically; 3) Trust-PCL (Nachum et al., 2018), an off-policy method optimizing
maximum entropy RL objective with trust region; and 4) GAC (Tangkaratt et al., 2018), an off-
policy method utilizing second-order information of critic. For Trust-PCL and GAC, we used their
original implementation provided by their authors3 4 5. For PPO and SAC, we used the implemen-
tation publicly provided by OpenAI 6, and adapted SAC to its automatically adjusting version in
Haarnoja et al. (2018c). For convenience, we developed our algorithm based on spinningup version
of SAC7. The pseudo-code of our method is provided in Appendix C and the source code is available
at https://github.com/ICLR2021papersub/TEAC.
TEAC requires to specify hyperparameters τ , which represents the desired maximum KL-
divergence, and η, which specifies the desired minimum entropy, before training. As the require-
ments of stability and exploration vary in different tasks, we set η as the negative of action space
dimension of the task, and set τ = 0.005 for all tasks. The settings of effective hyperparameters are
provided in Appendix D.
3Trust-PCL code: https://github.com/tensorflow/models/ tree/master/research/pcl_rl
4GAC code: https://github.com/voot-t/guide-actor-critic
5Due to the second-order computation complexity, we only finished testing GAC on HalfCheetah-v3 and
Hopper-v3 at the time of paper submission. The experimental results show that we can achieve better perfor-
mance than GAC in much shorter running time. We will complete the comparison before the rebuttal begins.
6OpenAI spinningup code: https://github.com/openai/spinningup
7https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/sac
8
Under review as a conference paper at ICLR 2021
5.2	Results
Fig. 1 illustrates the training curve for each algorithm. In general, when τ = 0.005, TEAC has simi-
lar performance as SAC in simpler tasks which have lower dimension of actions, such as Hopper-v3,
Walker2d-v3, and Ant-v3. However, for complex tasks with higher dimension of actions, such as
Huamoid-v3, TEAC gains significant performance improvement.
We also experimented and compared different settings of τ and η for their impact on the perfor-
mance. As there are no reasonable approaches to set τ , we simply compared seven different value
levels in the tasks. The results show that the selection of proper τ value for each task can signifi-
cantly boost the performance of TEAC (see more details in Appendix B), and generally τ should be
smaller for tasks with higher complexity. For example, when we set τ = 0.001 in Humanoid-v3,
TEAC had more than 10% performance gain than that of τ = 0.005. The reason can be attributed
to the stability of the algorithm. For complex tasks, the policy needs more exploration to find the
global optima, resulting in larger update steps. Without the help of trust region constraint, the pol-
icy will explore arbitrarily in the policy space, losing its bearings and getting trapped in some bad
settle-points.
For η, as we are dealing with continuous distributions, the entropy can be negative (Abdolmaleki
et al., 2016). Hence, η should be a small value. We have investigated several heuristic approaches for
setting η provided in MORE, GAC, and SAC, but none of them can serve as a general and effective
solution (see more details in Appendix B). Thus, in our experiments, we simply set η as the negative
of action space dimension similar to SAC.
6	Conclusion
In this paper, we propose to integrate two branches of research in RL, trust region methods for better
stability and maximum entropy methods for better policy exploration during the learning, to benefit
from both sides. We first transform the original RL objective to a constraint optimization problem
with the constraints of upper bound KL-divergence to avoid the abrupt difference between the new
and old policies and lower bound entropy to promote the policy exploration. Therefore, the Bellman
equation is redefined accordingly to guide the system loss evaluation. Consequently, we introduce
TEAC, an off-policy algorithm to learn stable and sufficiently explored policies for continuous states
and actions. TEAC utilizes two Actor networks to achieve the policy improvement by leveraging the
information from the old policy and the exponential of current Q function represented in the critic
network. The results show that TEAC with optimized parameters achieves similar performance in
half of the tasks and notably improvement in the others in terms of efficiency and effectiveness.
References
Abbas Abdolmaleki, Rudolf Lioutikov, NUno Lau, Luls Paulo Reis, Jan Peters, and Gerhard NeU-
mann. Model-based relative entropy stochastic search. In Genetic and Evolutionary Computation
Conference, GECCO 2016,pp. 153-154, 2016.
Alekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation
with policy gradient methods in markov decision processes. In Jacob Abernethy and Shivani
Agarwal (eds.), Proceedings of Thirty Third Conference on Learning Theory, volume 125 of
Proceedings of Machine Learning Research, pp. 64-66. PMLR, 09-12 Jul 2020. URL http:
//proceedings.mlr.press/v125/agarwal20a.html.
Riad Akrour, Gerhard Neumann, Hany Abdulsamad, and Abbas Abdolmaleki. Model-free trajectory
optimization for reinforcement learning. In Maria-Florina Balcan and Kilian Q. Weinberger (eds.),
Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, volume 48,
pp. 2961-2970, 2016.
Riad Akrour, Abbas Abdolmaleki, Hany Abdulsamad, Jan Peters, and Gerhard Neumann. Model-
free trajectory-based policy optimization with monotonic improvement. J. Mach. Learn. Res., 19:
14:1-14:25, 2018.
Kamil Ciosek, Quan Vuong, Robert Loftin, and Katja Hofmann. Better exploration with optimistic
actor critic. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d,Alche-Buc,
9
Under review as a conference paper at ICLR 2021
Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, pp. 1785-
1796, 2019.
Christian Daniel, Gerhard Neumann, Oliver Kroemer, and Jan Peters. Hierarchical relative entropy
policy search. J. Mach. Learn. Res., 17:93:1-93:50, 2016.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer G. Dy and Andreas Krause (eds.), Proceedings of the 35th
International Conference on Machine Learning, ICML 2018, volume 80, pp. 1582-1591. PMLR,
2018.
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with
deep energy-based policies. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th
International Conference on Machine Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August
2017, volume 70, pp. 1352-1361, 2017.
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine.
Composable deep reinforcement learning for robotic manipulation. In 2018 IEEE International
Conference on Robotics and Automation, ICRA 2018, Brisbane, Australia, May 21-25, 2018, pp.
6244-6251. IEEE, 2018a.
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximum entropy deep reinforcement learning with a stochastic actor. In Jennifer G. Dy and
Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning,
ICML 2018, volume 80, pp. 1856-1865, 2018b.
Tuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithms and applications. CoRR, abs/1812.05905, 2018c.
Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems,
pp. 1531-1538, 2002.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio
and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,
2015.
Sergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review.
CoRR, abs/1805.00909, 2018.
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. End-to-end training of deep visuo-
motor policies. J. Mach. Learn. Res., 17:39:1-39:40, 2016.
Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,
David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In Yoshua
Bengio and Yann LeCun (eds.), 4th International Conference on Learning Representations, ICLR
2016, 2016.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. CoRR,
abs/1312.5602, 2013.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Bridging the gap between
value and policy based reinforcement learning. In Advances in Neural Information Processing
Systems 30: Annual Conference on Neural Information Processing Systems 2017, pp. 2775-2785,
2017.
Ofir Nachum, Mohammad Norouzi, Kelvin Xu, and Dale Schuurmans. Trust-pcl: An off-policy
trust region method for continuous control. In 6th International Conference on Learning Repre-
sentations, ICLR 2018. OpenReview.net, 2018.
Jan Peters and Stefan Schaal. Natural actor-critic. Neurocomputing, 71(7-9):1180-1190, 2008. doi:
10.1016/j.neucom.2007.11.026.
10
Under review as a conference paper at ICLR 2021
Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Proceedings
of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, 2010.
John Schulman, Sergey Levine, Pieter Abbeel, Michael I. Jordan, and Philipp Moritz. Trust region
policy optimization. In Francis R. Bach and David M. Blei (eds.), Proceedings of the 32nd
InternationaI Conference on Machine Learning, ICML 2015, volume 37,pp. 1889-1897, 2015.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. CoRR, abs/1707.06347, 2017.
David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driess-
che, Julian Schrittwieser, Ioannis Antonoglou, Vedavyas Panneershelvam, Marc Lanctot, Sander
Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy P. Lillicrap,
Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game
of go with deep neural networks and tree search. Nat., 529(7587):484-489, 2016.
Richard Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy gradient methods
for reinforcement learning with function approximation. Adv. Neural Inf. Process. Syst, 12, 02
2000.
Richard S. Sutton and Andrew G. Barto. Reinforcement learning - an introduction. Adaptive com-
putation and machine learning. MIT Press, 1998.
Voot Tangkaratt, Abbas Abdolmaleki, and Masashi Sugiyama. Guide actor-critic for continuous
control. In 6th International Conference on Learning Representations, ICLR 2018, 2018.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2012, pp.
5026-5033. IEEE, 2012.
Marc Toussaint. Robot trajectory optimization using approximate inference. In Andrea Pohoreckyj
Danyluk, Leon Bottou, and Michael L. Littman (eds.), Proceedings of the 26th Annual Interna-
tional Conference on Machine Learning, ICML 2009, volume 382 of ACM International Confer-
ence Proceeding Series, pp. 1049-1056. ACM, 2009.
Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph Mo-
dayil. Deep reinforcement learning and the deadly triad. CoRR, abs/1812.02648, 2018.
Brian D. Ziebart, Andrew L. Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse
reinforcement learning. In Dieter Fox and Carla P. Gomes (eds.), Proceedings of the Twenty-Third
AAAI Conference on Artificial Intelligence, AAAI 2008, Chicago, Illinois, USA, July 13-17, 2008,
pp. 1433-1438. AAAI Press, 2008.
11
Under review as a conference paper at ICLR 2021
Appendix
A Derivations and Proofs
A. 1 Trust Entropy Policy Evaluation
Lemma A.1 (Trust Entropy Policy Evaluation). Let Qk+1 = T πQk, the sequence Qk will con-
verge to the trust entropy Q-value of π as k → ∞ when considering a mapping Q0 : S × A →
R with |A| < ∞ and the Bellman backup operator Tπ.
Proof. We define the reward function in trust entropy framework as
r∏(s, a)，r(s, a) + (α + β) ∙ Es，〜P 叫，〜∏ [∏(∙∣s0)]] - α ∙ Es，〜P 叫，〜∏ [∏oid(∙∣s0)]] .	(20)
We rewrite the update rule as
Q(s, a)《-rπ (s, a) + γEs0〜p,a0〜π [Q(s , a )].
(21)
Following Sutton & Barto (1998), we can realize the standard convergence for policy evaluation.
A.2 Derivation of the solution of Lagrangian
By taking derivative of L w.r.t. π and setting the derivative to zero,
∂∏L =EP(S) J (Q(s, a) - (α + β)log ∏(a∣s) + αlog∏°id(a∣s) - VV(s) +
Ep(s0∣s,a)[VV(SO)])da - (α + β + λ)
0
=Q(s, a) - (α + β)logπ(a∣s) + α log ∏°id(a∣s) - νV(s) + Ep(s，|s,a)[νV(s )]
- (α + β + λ)
=0
(22)
Given the fact that we sample actions from the actor network as parameterized Gaussian distribution
and the value function should satisfy Bellman equation, ifwe set V = 0, the function can be rewritten
as
0 =Q(s, a) - (α + β)logπ(a∣s) + α log∏oid(a∣s) - (α + β + λ)	(23)
The solution of π(a∣s) is:
π(a∣s)
∏oid(a∣s)α+β ∙ exp
Q(s, a)
a + β
• exp -
α + β + λ
a + β
(24)
Here, we combine the third constraint in Eq. 2 with the solution Eq. 24:
1 =Eρ(s)
J π(a∣s)da
/ ∏oid (a | s)α+β • exp
Q(s, a)
a + β
• exp -
da
(25)
α + β + λ ∖
a + β
Given the fact that α, β, and λare constants which are independent of s and a, we can get
1
/ ∏oid (a | s)α+β • exp
Q(s, a)
a + β
• exp -
α + β + λ ∖
a + β
da
/ ∏oid (a | s)α+β • exp
Q(s, a)
a + β
da
• exp -
α + β + λ
a + β
(26)
12
Under review as a conference paper at ICLR 2021
Thus,
α + B + λ 1
exp (一 丁丁)	= EP(S)
α	Q(s, a)	J
J ∏o∣d (a | s) α+β ∙ exp I & + 尸)da
Hence, the term exp f α+++λj acts as a normalization term. Therefore,
α	Q(S, a)
∏(a∣s) Z ∏o∣d(a∣s) α+β exp ——-ɪ
α+β
A.3 Trust Entropy Policy Improvement
(27)
(28)
Lemma A.2 (Trust EntroPy Policy ImProvement). Given a policy π and an old policy πt, define a
new policy
∏(a∣s) z π(a∣s)α⅛ exp QQ (s,：)) , ∀s.
α+β
(29)
If Q is bounded, R π(a∣s) α+β exp (Qa(Sj)) da is bounded for any S (for all π, π and π),
and the policies are the parameterized Gaussian networks. Then we can obtain Qn(s, a) ≥
Qπ (S, a), ∀S, a.
Proof. With the definition of the V function, we can get:
V (S) = ET ~π,s0 = s,ao = a
∞
EYt(r(st,at) — (α + β) ∙ log∏(∙∣st) + α ∙ logΠ(∙∣St))
t=0
(30)
Here, τ = (s0, a0, s1, a1, . . . ) denotes the trajectory originating at (s, a). Using a telescoping
argument, we have:
Vπ (s) — Vπ(s) =Eτ~Π,S0 = s,ao=a
∞
X γt(r(st,at) — (α + β) ∙ logπ(∙∣St) + α ∙ log∏(∙∣st)) — Vπ(s)
t=0
∞
=ET ~∏,s0 = s,ao=aZ Yt(r(st, at) — (α + β) ∙ log∏(∙∣St) + α ∙ log∏(∙∣st)
t=0
+ Vπ(st) - Vπ(st))] - Vπ(s)
∞
=ET ~Π,S0 = s,ao=aZ Yt(r(st, at) — (α + β) ∙ log∏(∙∣st) + α ∙ log∏(∙∣st)
t=0
+γVπ(st+1)-Vπ(st))]
∞
=ET ~Π,S0 = s,ao=aZ Yt(r(st, at) — (α + β) ∙ log∏(∙∣st) + α ∙ log∏(∙∣st)
t=0
+ YE[v π (st+1)lst, at] - Vπ (St))]
∞
=ET~π,so=s,ao=a[X γt(Qπ(st,at) — V"(st) — (α + β) ∙ logπ(∙∣St) + α ∙ logπ(∙∣St))]
t=0
=1--Es'~ρ∏(s)Ea~∏(∙∣s)[γt(Qπ(s0,a) — Vπ(s0) 一 (α + β) ∙ log∏(∙∣s0) + α ∙ logπ(∙∣s0))],
(31)
where (a) rearranges terms in the summation and cancels the V(s0) term with the —V (s) outside
the summation, and (b) uses the tower property of conditional expectations and the final equality
follows from the definition of ρ∏ (s). Consider Eq.24 can be rewritten as:
α	Qπ(S,a)	α+β+λ
n(a|S) =exp (Flog n(a|S) + JeXP Γ
(32)
13
Under review as a conference paper at ICLR 2021
where exp (-α+++λ) is normalization term. Assume We follow the gradient ascent update rule
and that the distribution ρ(s) is strictly positive i.e. ρ(s) > 0 for all states s. Following the work
Agarwal et al. (2020), with the help of the gradient of the softmax policy class, we can get
X ∏(a∣s)(Q(s,a) - V(s) - (α + β) ∙ log ∏(∙∣s) + α ∙ log ∏(∙∣s)) ≥ 0,	(33)
a∈A
Then we get Vπ(s) ≥ Vπ(s), as well as Qπ(s, a) ≥ Qπ(s, a) holds for all states S and actions a.
A.4 Derivation of the dual function
To obtain the dual function, we take the solution of π(a∣s) into Eq. 3:
L(π, α,β,λ) =EP(S)∏(a∣s)[Q(s, a)] + α (T - E。®) [KL (∏(∙∣s)k∏old(∙∣s))])
+ β (EP(S)[H(∏(∙∣s))]
-η +λ
π(a∣s)da — 1
TT7'	Γ∕Λ/ _ M
=EP(S)π(a∣s)[Q(S, a)]
-(α + β)Eρ(s)π(a∣s)
Q(s, a)	α	α + β+ λ
ττβ + Flog nold(a|s) - -^+^
+ αEρ(s)∏(a∣s) [log∏old(a∣s)] + λ
π(a∣s)da — 1 I + ατ — βη
(34)
=ατ - βη - (α + β) ∙ Eρ(s)
This loss function can be rewritten as
L(α, β) =ατ - βη+ (α + β)Eρ(s)
=ατ - βη + (α + β)Eρ(s)
=g(α, β)
Meanwhile, we can rewrite Eq.24 as
α+β +λ
exp(
α+β+λ
log SXP (
α	Q(s, a)、」
log J ∏old(a∣s)α+β exp I a + 尸)da
∏old(a∣s)α+β ∙ exp (Q+a)
π(a∣s)
(35)
(36)
—
α + β + λ
a + β
With Eq.36, the loss function becomes
L(α, β) =ατ - βη+ (α + β)Eρ(s)
log
∏old(a∣s)α+β ∙ exp (Q0⅛a)
π(a∣s)
ατ - βη + (α + β)Eρ(s)
^
Flog πold(als)+ f+β - log n(a|s)
(37)
aτ - βη + EP(S)Ia ∙ log∏old(a∣s) + Q(s, a) - (α + β) ∙ logπ(a∣s)].
14
Under review as a conference paper at ICLR 2021
B HYPERPARAMETER ANALYSIS FOR τ AND η
timesteps	ie6
U」n43j<u6s3>e
0.5	1-0	1-5	2.0	2.5
timesteps	le6
(a) HalfCheetah-v3	(b) Hopper-v3
(d) Walker2d-v3
U」B3」36eJ3>e
(c) Humanoid-v3
-2000
0.5	1-0	1.5	2.0	2.5
timesteps	le6
(f) Ant-v3
0.5	1-0	1-5	2.0	2.5
timesteps	ieδ
(e) Swimmer-v3
Figure 2: Performance with different τ on six MuJoCo tasks. τ = 0.001 achieves about 8000 as
the return in Humanoid-v3, and τ = 0.005 achieves about 130 as the return in Swimmer-v3. These
two results surpass all benchmark algorithms significantly.
The impact of τ on the performance of TEAC was evaluated with τ ∈
(0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1). Fig.5 shows that tuning τ for different tasks may achieve
significant performance improvement in TEAC.
Similar to SAC (Haarnoja et al., 2018c), we set η to the negative of action space dimension in our
own experiments. Besides, there are other techniques in existing methods. MORE (Abdolmaleki
et al., 2016) changes the entropy constraint to
E - E0 ≥ γ (Eold - E0) ⇒ η = γ (Eold - E0) + E0	(38)
where E ≈ Ep(s)[H(∏θ(a|s))] denotes the expected entropy of the current policy, Eoid ≈
Ep(s)[H (∏θ(a∣s))] denotes the expected entropy of the old policy, and Eo denotes the entropy
ofa base policy N (a|0, 0.01I). GAC improves this technique to adjust η heuristically by
η = max (γ (E - E0) + E0,E0) .
(39)
We compared these three different techniques on Hopper-v3, Humanoid-v3 and Ant-v3. Fig. 3
shows that there is no outstanding difference from them. Therefore, we simply set η as the negative
of action space dimension similar to SAC.
IU a。」t>6eu<υ>e
0.5	1-0	1-5	2.0	2.5
timesteps	⅛6
500	— t«ac{MOiiE_aut«)
----teκ{GACauto)
----⅛ac(5βc-au⅛)
----tβa<XM0KE_a<*σ)
----tea⅛GACjuta)
0.5	1-0	1-5	2.0	2.5
HmestEPS	ie6
, I ɪ
luBg a6eu<υ>e
0.5	1-0	1-5	2.0	2.5
timesteps	le6
(a) Hopper-v3	(b) Humanoid-v3	(c) Ant-v3
Figure 3: Performance with different η in three Mujoco tasks with τ = 0.1.
15
Under review as a conference paper at ICLR 2021
C Algorithms
Algorithm 1 TEAC: Trust Entropy Actor Critic
Input:
Initial actor ∏θ(a|s), old actor ∏θ(a∣s),
critic Qφι and Qφ28, target critic network φι J φι, φ2 J φ2,
KL divergence bound τ, entropy bound η, learning rate ωac, ωα, ωβ,
an empty replay pool D = 0
1:	for each iteration do
2:	for each environment step do
3:	Observe state St and sample action at 〜 ∏(a∕st)
4:	Execute at, receive reward r(st, at) and next state St 〜p(s]st, at)
5:	Add transition to replay buffer D J D ∪ {(st, at, r (st, at) , s0t)}
6:	end for
7:	for each gradient step do
8:	Sample N mini-batch samples {(Si, ai, ri, S0i)}iN=1 uniformly from D
9:	Sample actions ai 〜∏θ(a|si), compute Qtar(si, ai):
Qtar(Si, ai) =min(Qφι (Si, ai),Qφ2 (Si, ai))	(4O)
10:	Compute y%, update φ by, e.g., Adam, and update φ by moving average:
Iyi = r + Y (Qtar(Si, ai) — (α + β)log∏Θ(a∣Si) + αlog∏^(a∣Si))	(41)
1N
φj J φj - ωac Vφj N ɪ2 (Qφj (Si, ai) - yi) ,j ∈ {1, 2}	(42)
φj J κφj + (I - K)φj,j ∈ {1, 2}	(43)
11:	Sample actions a 〜∏θ(a∣Si), compute Q(Si, a):
Q(Si, a) = min(Qφι (Si, a), Qφ2(Si, a))	(44)
12:	Compute loss function of θ:
1N
L(θ) = Nf ((α + β)log ∏θ (a5) — α log ∏θ(a∣Si) — Q(Si, a))	(45)
i=1
13:	Update θ using θ J θ
14:	Update θ by, e.g., Adam:
θ J θ — ωacVθL(θ)	(46)
15:	Compute loss function of dual variables α and β:
1N
L(α) = — Nza ∙ (log∏θ(a∣Si) — log∏θ(a∣Si) — T)	(47)
1N
L(β) = — N∑^β ∙ (log∏θ(a∣Si) + η)	(48)
16:	Update dual variables α and β :
α J α — ωαVαL(α), β J β — ωβVβL(β)	(49)
17:	end for
18:	end for
16
Under review as a conference paper at ICLR 2021
D Hyperparameters
Table 1 lists the effective hyperparameters of TEAC used in the experiments, of which the results
are shown in Fig. 1.
Table 1: TEAC Hyperparameters
Parameter	Value
optimizer learning rate for actor and critic learning rate for a learning rate for β discount (Y) replay buffer size number of hidden layers (all networks) number of hidden units per layer number of samples per minibatch target entropy (η) max divergence for KL (τ) nonlinearity target smoothing coefficient K target update interval gradient steps	Adam (Kingma & Ba, 2015) 1∙10-3 1∙10-4 1∙10-3 0.99 106 2 256 100 -dim(A))(e.g., -6 for HaIfCheetah-V3) 0.005 ReLU 0.005 1 	1	
E The best performance of our model
0
0.5	1-0	1-5	2.0	2.5	3.0
times tsps	比6
8000
E 6000
2000
(c) Humanoid-v3
HalfCheetah-v3	(b) Hopper-v3
(d) Walker2d-v3
(e) Swimmer-v3
Figure 4: Performance comparisons on six MuJoCo tasks. Notice that the blue line is the perfor-
mance of our model which setting different τ with respect to different tasks. In this figure, we
set τ = 0.5 for HalfCheetah-v3, τ = 0.05 for Ant-v3, τ = 0.1 for Hopper-v3, τ = 0.001 for
Humanoid-v3, τ = 0.005 for Swimmer-v3, and τ = 0.1 for Walker2d-v3.
(f) Ant-v3
8Our implementation also makes use of two Q-functions (critic networks) to mitigate positive bias in the
policy improvement step , follows Fujimoto et al. (2018) and Haarnoja et al. (2018b) .
17
Under review as a conference paper at ICLR 2021
F Additional Continuous Control Experiments with six
INSTANCES
14000
12000
⅛ 10000
国 8000
V
B 6000
I 4000
2000
0
0.5	1-0	1-5	2.0	2.5	3.0
timesteps	ie6
(a) HalfCheetah-v3
(b) Hopper-v3
0.5	1-0	1-5	2.0	2.5	3.0
timesteps	ie6
(c) Humanoid-v3
timesteps
(d) Walker2d-v3
-R"≈
-R
-9K
2.5	3.0
ie6
U」m3」36eJ3>e
timesteps	⅛6
(e) Swimmer-v3
U」B3」36eJ3>e
0.5	1-0	1.5	2.0	2.5	3.0
timesteps	ie6
(f) Ant-v3
Figure 5: Performance comparisons on six MuJoCo tasks. We trained six different instances of all
algorithms with different random seeds. In this case, for TEAC, we set η as the negative of action
space dimension of the task, and set τ = 0.005 for all tasks.
18