Under review as a conference paper at ICLR 2021
FsNet: Feature Selection Network on High-
dimensional Biological Data
Anonymous authors
Paper under double-blind review
Ab stract
Biological data including gene expression data are generally high-dimensional
and require efficient, generalizable, and scalable machine-learning methods to dis-
cover their complex nonlinear patterns. The recent advances in machine learning
can be attributed to deep neural networks (DNNs), which excel in various tasks
in terms of computer vision and natural language processing. However, standard
DNNs are not appropriate for high-dimensional datasets generated in biology be-
cause they have many parameters, which in turn require many samples. In this
paper, we propose a DNN-based, nonlinear feature selection method, called the
feature selection network (FsNet), for high-dimensional and small number of sam-
ple data. Specifically, FsNet comprises a selection layer that selects features and
a reconstruction layer that stabilizes the training. Because a large number of pa-
rameters in the selection and reconstruction layers can easily result in overfitting
under a limited number of samples, we use two tiny networks to predict the large,
virtual weight matrices of the selection and reconstruction layers. Experimental
results on several real-world, high-dimensional biological datasets demonstrate
the efficacy of the proposed method.
1	Introduction
The recent advancements in measuring devices for life sciences have resulted in the generation
of large biological datasets, which are extremely important for many medical and biological ap-
plications, including disease diagnosis, biomarker discovery, drug development, and forensics (Li
& Chen, 2014). Generally, such datasets are substantially high-dimensional (i.e., many features
with small number of samples) and contain complex nonlinear patterns. Machine learning meth-
ods, including genome-wide association studies (d > 105, n < 104) and gene selection (d > 104,
n < 103) (Marx, 2013), have been successfully applied to discover the complex patterns hidden in
high-dimensional biological and medical data. However, most nonlinear models in particular deep
neural networks (DNN) are difficult to train under these conditions because of the significantly high
number of parameters. Hence, the following questions naturally arise: 1) are all the features neces-
sary for building effective prediction models? and 2) what modifications are required in the existing
machine-learning methods to efficiently process such high-dimensional data?
The answer to the first question is to select the most relevant features, thereby requiring an ap-
propriate feature selection method (Ye et al., 2019; Ming & Ding, 2019; Liao et al., 2019). This
problem, called feature selection, consists of identifying a smaller subset (i.e., smaller than the orig-
inal dataset) that contains relevant features such that the subset retains the predictive capability of
the data/model while eliminating the redundant or irrelevant features (Yamada et al., 2014; 2018a;
Climente-Gonzalez et al., 2019). Most state-of-the-art feature selection methods are based on ei-
ther sparse-learning methods, including Lasso (Tibshirani, 1996), or kernel methods (Masaeli et al.,
2010; Yamada et al., 2018b; 2014). These shallow approaches satisfactorily work in practice for
biological data. However, sparse-learning models including Lasso are in general linear and hence
cannot capture high-dimensional biological data. Kernel-based methods can handle the nonlinearl-
ity, but it heavily depends on the choice of the kernel function. Thus, more flexibile approaches that
can train an arbitrary nonlinear transformation of features are desired.
An approach to learning such a nonlinear transformation could be based on deep autoencoders (Vin-
cent et al., 2010). However, deep autoencoders are useful for computer-vision and natural language
1
Under review as a conference paper at ICLR 2021
processing tasks, wherein a large number of training samples are available. In contrast, for high-
dimensional biological data, the curse of dimensionality prevents us from training such deep models
without overfitting. Moreover, these models focus on building useful features rather than select-
ing features from data. The training of autoencoders for feature selection results in the discrete
combinatorial optimization problem, which is difficult to train in an end-to-end manner.
To train neural networks on high-dimensional data without resulting in overfitting, several ap-
proaches were proposed. Widely used ones are based on random projection and its variants (Dahl
et al., 2013; Wojcik & KUrdzieL 2019). However, their performances significantly depend on the
random projection matrix, and their usability is limited to dimensionality reduction only. There-
fore, they cannot be applied for featUre selection. Another deep learning-based approach employs a
concrete aUtoencoder (CAE) (Balin et al., 2019), which Uses concrete random variables (Maddison
et al., 2017) to select featUres withoUt sUpervision. AlthoUgh CAE is an UnsUpervised model with
poor performance, it can be extended to incorporate a sUpervised-learning setUp. However, we ob-
served that this simple extension is not efficient becaUse the large nUmber of parameters in the first
layer of CAE can easily resUlt in overfitting Under a limited nUmber of samples.
To address these issUes, we propose a non-linear featUre selection network, called FsNet, for high-
dimensional biological data. FsNet comprises a selection layer that Uses concrete random vari-
ables (Maddison et al., 2017), which are the continUoUs variants of a one-hot vector, and a recon-
strUction layer that stabilizes the training process. The concrete random variable allows the con-
version of the discrete optimization problem into a continUoUs one, enabling the backpropagation
of gradients Using the reparameterization trick. DUring the training period, FsNet selects a few
featUres Using its selection layer while maximizing the classification accUracy and minimizing the
reconstrUction error. However, owing to the large nUmber of parameters in the selection and re-
constrUction layers, overfitting can easily occUr Under a limited nUmber of samples. Therefore, to
avoid overfitting, we propose Using two tiny networks to predict the large, virtUal weight matrices
of the selection and reconstrUction layers. ConseqUently, the size of the model is significantly re-
dUced and the network can scale high-dimensional datasets on a resoUrce-limited device/machine.
ThroUgh experiments on varioUs real-world datasets, we show that the proposed FsNet significantly
oUtperforms CAE and the sUpervised coUnterpart thereof.
Contributions: OUr contribUtions throUgh this paper are as follows.
•	We propose FsNet, an end-to-end trainable neUral network based nonlinear featUre selec-
tion, for high-dimensional data with small nUmber of samples.
•	FsNet compares favorably with the state-of-the-art nonlinear featUre selection methods for
high-dimensional data with small nUmber of samples.
•	The model size of FsNet is one to two orders magnitUde smaller than that of a standard
DNN model, inclUding CAE (Balin et al., 2019).
2	Related Work
Here, we discUss the existing shallow/deep featUre selection methods, along with their drawbacks.
Shallow, nonlinear feature selection: Maximum relevance is a simple bUt effective criterion of
nonlinear featUre selection (GUyon & Elisseeff, 2003). It Uses mUtUal information and the Hilbert-
Schmidt Independence Criterion (HSIC) to select the featUres associated with the oUtcome (Peng
et al., 2005; Song et al., 2007). It is also called sUre independence screening in the statistics commU-
nity (Fan & Lv, 2008; BalasUbramanian et al., 2013). However, becaUse it tends to select redUndant
featUres, minimUm redUndancy maximUm relevance (mRMR) featUre selection was proposed (Peng
et al., 2005). Notably, mRMR finds the sUbset of independent featUres that are maximally associ-
ated with the oUtcome by Using mUtUal information between featUres and between each featUre and
the oUtcome. Recently, a kernel-based, convex variant of mRMR was proposed, called HSIC Lasso
(Yamada et al., 2014; 2018a; Climente-Gonzalez et al., 2019). They effectively perform nonlin-
ear featUre selection on high-dimensional data, prodUcing simple models with parameters that can
be easily estimated. However, their performances are limited by the simplicity of the models and
depends on the choice of kernels.
2
Under review as a conference paper at ICLR 2021
DNNs for feature selection: DNNs are nonlinear, complex models that can address the aforemen-
tioned problems associated with kernel-based methods. They can be used for feature selection by
adding a regularization term to the loss function, orby measuring the effect ofan input feature on the
target variable (Verikas & Bacauskiene, 2002). Elaborately, an extra feature scoring layer is added
to perform element-wise multiplication on the features and score, and then they are entered as inputs
into the rest of the network (Wang et al., 2014; Lu et al., 2018). However, DNNs do not select fea-
tures during the training period, thereby resulting in a performance reduction after feature selection.
Moreover, it is generally difficult to obtain a sparse solution using a stochastic gradient. CAE (Balin
et al., 2019) addresses this problem by training an autoencoder that contains a feature selection layer
with a concrete variable, which is a continuous relaxation of a one-hot vector. Recently, another
end-to-end, supervised, feature selection method based on stochastic gates (STGs) was proposed
(Yamada et al., 2020). It uses a continuously relaxed Bernoulli variable and performs better than the
existing feature selection methods. However, these methods need to train a large number of param-
eters in the first layer, resulting in overfitting to the training data. Therefore, these approaches may
not be appropriate for DNN models with high-dimensional data and a limited number of samples.
Training DNNs on high-dimensional data: The existing DNN-based methods can easily overfit
to the high-dimensional biological data, as they suffer from the curse-of-dimensionality irrespective
of regularization constraints. The biggest drawback of DNNs is that they need to have a large
number of parameters in the first layers of the decoder and encoder. HashedNets (Chen et al.,
2015) addressed this issue by exploiting the inherent redundancy in weights to group them into
relatively fewer hash buckets and shared them with all its connections. However, the hash function
groups the weights on the basis of their initial values instead of opting for a dynamic grouping,
thereby reducing the options to arbitrarily learn weights. Diet Networks (Romero et al., 2017) used
tiny networks to predict weight matrices. However, they are limited to the multilayer perceptron
only for classification and not for feature selection. A DNN model, referred to as deep neural
pursuit (DNP) (Liu et al., 2017), selects features from high-dimensional data with a small number
of samples. It is based on changes in the average gradients with multiple dropouts by an individual
feature. However, (Liu et al., 2017) reported that the performance of DNP significantly depends on
the number of layers.
These issues render the existing approaches inefficient for processing biological data, thereby raising
the need to develop a method for efficiently extracting features from biological data.
3	Problem Formulation
Let X = (xι,…,Xn)T = (uι,…,Ud) ∈ Rn×d be the given data matrix, where X ∈ Rd
represents the sample vector with d number of features and u ∈ Rn the feature vector with n
number of samples. Let y = (yι, •…,yn)τ ∈ Rn be the target vector such that yi ∈ Y represents
the output for Xi , where Y denotes the domain of the output vector y, which is continuous for
regression problems and categorical for classification problems. In this paper, we assume that the
number of samples is significantly fewer than that of the dimensions (i.e., n d).
The final goal of this paper is to train a neural-network classifier f (∙) : Rd → Y, which simultane-
ously identifies a subset S ⊆ F = {1, 2 …d} of features of a specified size |S| = K《d, where
the subset can reproduce the remaining F\S features with minimal loss.
4	Proposed Method: FsNet
We here present the architecture and training of the proposed FsNet model for selecting nonlinear
features from high-dimensional data.
4.1	FsNet Model
We aim to build an end-to-end, trainable, compact, feature selection model. Hence, we employ a
concrete random variable (Maddison et al., 2017) to select features, and we also use the weight-
predictor models used in Diet Networks to reduce the model size (Romero et al., 2017). We build
FsNet, a simple but effective model (see Figure 1). As shown in Figure 1(A), although the selec-
tion and reconstruction layers have many connections, they are virtual layers whose weights are
3
Under review as a conference paper at ICLR 2021


Figure 1:	(A) Architecture of FsNet. (B) and (C) are the weight-predictor networks for the selection
and reconstruction layers, respectively.
predicted from significantly small networks, as shown in Figures 1(B) & (C), respectively. The
weight-predictor networks (B) and (C) are trained on the feature embeddings.
The optimization problem of FsNet is given by
nn
mΘinXLoss(yi,fθc(ENCθe(xiS)))+λXkxi-RECθr(DECθd(ENCθe(xiS)))k22,	(1)
i=1	i=1
where Loss(y, fθc) denotes the categorical cross-entropy loss (between y and fθc), ∣∣ ∙ ∣∖ the '2
norm, λ ≥ 0 the regularization parameter for the reconstruction loss, Θ all the parameters in the
model, SEL(∙) the selection layer, XS = SEL(xi), ENC(∙) the encoder network, DEC(∙) the decoder
network, and REC(∙) the reconstruction layer. The pseudocode for the training ofFsNet is provided
in Algorithm 2 in the appendix.
Selection Layer (Train): We first describe the selection layer, which is used to select important
features in an end-to-end manner. The feature selection problem is generally a combinatorial prob-
lem, but it is difficult to train in an end-to-end manner because it breaks the propagation of the
gradients. To overcome this obstacle, a concrete random variable (Maddison et al., 2017), which is
a continuous relaxation of a discrete one-hot vector, can be used for the training, as it computes the
gradients using the reparameterization trick. Specifically, selecting the k-th feature of the input x
can be expressed as x(k) = ek>x, where ek ∈ Rd denotes the one-hot vector whose k-th feature is
1 and 0 otherwise. The concrete variables for the kth neuron in the selection layer are defined as
follows:
(k) = exp((IOg δsk)+ g"τ)	k = 1 2 K
μ = Pd=ι exp ((log δSk) + gj )∕τ), = , ,...,	,
(2)
where g ∈ Rd is drawn from the Gumbel distribution. Additionally, τ denotes the temperature that
controls the extent of the relaxation, K the number of selected features, and ∆s = (δs,1, . . . , δs,d) =
(δs(1), . . . , δs(K))> ∈ RK×d, δs(k) ∈ R>K0 is the model parameter for concrete variables. Notably,
μ(k) becomes a one-hot vector when T → 0.
Using the concrete variables M = (μ(1), μ(2),..., μ(K))>, the feature selection process can be
simply written by using matrix multiplications as follows:
SEL(x) = Mx.
Because the feature selection process can be written by using matrix multiplications, it can be trained
in an end-to-end manner. However, the number of parameters in the selection layer is O(dK); it
depends on the size of the input layer d and the number of neurons in the selection layer K. Thus,
for high-dimensional data, the number of model parameters can be high, resulting in overfitting un-
der a limited number of samples n. We address both the issues by using a tiny weight-predictor
network φωs (∙) : Rb → RKo to predict the weights δs j = φωs (Φ(uj)) (see Figure MB)), where
4
Under review as a conference paper at ICLR 2021
φ(uj) ∈ Rb is the embedding representation of feature j and b ≤ n the size of the embedding rep-
resentation. Specifically, the feature embedding φ(uj) for the jth feature vector used for training
the weight-predictor networks is defined as φ(uj) = ρj νj, where denotes elementwise multi-
plication, whereas ρj and νj denote the frequencies and means of the histogram bins of feature uj ,
respectively. In this paper, we use δs,j = softmax(Wωs φ(uj)), where Wωs ∈ RK×b is the model
parameter for the tiny network. Over epochs, μ(k) will converge to a one-hot vector. Notably, the
model parameter ∆ ∈ RK ×d depends on the input dimension d. However because the model size
of the weight-predictor network depends on b d, we can significantly reduce the network model
size using the predictor network. Moreover, the tiny weight-predictor network can also be trained in
an end-to-end manner.
Selection Layer (Inference): For infer-
ence, we can replace the concrete vari-
ables with a set of feature indices. Con-
sequently, the inference becomes faster
than before, as we need not compute
tiny networks. However, if we sim-
ply use the argmax function, it tends
to select redundant features, and thus
the prediction performance can be de-
graded. Therefore, we propose the
Unique_argmax function to select non-
redundant features and then use the non-
Algorithm 1 Unique argmax function uargmax
Input: matrix A ∈ Rd+×K, with d rows and K cols
Output: selected indices S
1
2
3
4
5
6
7
S - {}
for i = 0 - K do
(x, y) - index of max value in A
S-S∪x
A.row(x) - 0
A.col(y) - 0
end for
redundant feature set for inference. The K best and unique features are selected from the estimated
M as S = uargmax(M>). Subsequently, for inference, we use xS ∈ RK as an input of the
encoder network. Although this is a heuristic approach, it works satisfactorily in practice.
Encoder Network: The goal of the encoder network ENCθe(∙) : RK → Rh is to obtain a low-
dimensional hidden representation h ∈ Rh from the output of the selection layer xS . The encoder
network is expressed as follows:
ENCθe(xS) = σ(wLe)σ(∙∙∙ wf^W^xS) ∙ ∙ ∙),	(3)
where XS = SEL(X) denotes the output of the selection layer, θe = {W''e) }L= 1 the weight matrix,
Le the number of layers in the encoder network, and σ(∙) an activation function.
Classifier Network: The classifier network fθc(∙) : Rh → Y predicts the final output from the
hidden representation h = ENCθe (XS) as follows:
fθc (h) = SOftmax(WLy)σ(…w2(y')σ(W(y')h) ∙ ∙ ∙),	(4)
where θc ={W'(y)}L=ι, and Ly denotes the number of layers in the classifier network.
Decoder Network: Generally, a decoder function is employed to reconstruct the original output.
However, in this paper, the decoder function DECθd(∙) : Rh → Rh0 computes another hidden
0
representation h ∈ Rh and defines the last reconstruction layer separately. The decoder function is
defined as follows:
DECθd (h)= σ(wLd)σ(∙∙∙ W(d)σ(W*h)…).	(5)
where h = ENCθe(xS), θd = {W''d)}L=1, and Ld denotes the number of layers in the decoder
network.
Reconstruction Layer: To reconstruct the original high-dimensional feature X, it must have O(dh0)
parameters and depend on the dimension d. Thus, in a manner similar to the selection layer, we use
a tiny network to predict the model parameters. The reconstruction layer is expressed as follows:
RECθr (he) = W (r)he,	(6)
where h = DECθd(h), θr = W(r) ∈ Rd×h0, and [W(r)>]j = ψωr(Φ(uj)) denotes the vir-
tual weights of the jth row in the reconstruction layer. The tiny network ψωr (∙) : Rb → Rh
is trained on φ(uj ) ∈ Rb to predict the weights that connect the jth row of the reconstruc-
tion layer to all the h0 neurons of the last layer of the decoder network. In this paper, we use
[W (r)>]j = tanh(Wωr φ(uj)), where Wωr ∈ Rh0×b is the model parameter for the tiny network.
5
Under review as a conference paper at ICLR 2021
Number of Epochs
Test
800	1 600 2400 3200 4000
NUmber of Epochs
0.4
Number of Epochs
(A) ALLAML
(B) CLL_SUB
Figure 2:	Comparison among FsNet, supervised CAE, and Diet Network for mean training and
testing accuracies over the epochs. For the neural-network-based approaches, we set the model
parameters to b = 10 and K = 10. (See all the experimental results in Figure 5).
(C) GLL85
Number of Epoches
(B) CLL-SUB
Figure 3:	Comparison among the proposed FsNet and existing supervised CAE approaches in terms
of the mean test reconstruction error over the epochs. (See the appendix for all the data results).
5	Empirical Evaluation
Here, we compare FsNet with several baselines using benchmark and the real metagenome dataset.
5.1	Setup
We compared FsNet with CAE (Balin et al., 2019), which is a unsupervised, neural-network-based,
feature selection method, Diet Networks (Romero et al., 2017), HSIC Lasso (Yamada et al., 2014;
2018a; Climente-Gonzalez et al., 2019), and mRMR (Peng et al., 2005). Notably, CAE and HSIC
Lasso are state-of-the-art, nonlinear feature selection methods, which are deep and shallow, respec-
tively. FsNet and CAE (Balin et al., 2019) were run on a Linux server with an Intel Xeon CPU
Xeon(R) CPU E5-2690 v4 @ 2.60 GHz processor, 256 GB RAM, and NVIDIA P100 graphics card.
HSIC Lasso (Yamada et al., 2014) and mRMR (Peng et al., 2005) were executed on a Linux server
with an Intel Xeon CPU E7-8890 v4 2.20 GHz processor and 2 TB RAM.
For FsNet and CAE, we conducted experiments on all the datasets using a fixed architecture, defined
as [d → K → 64 → 32 → 16(→ |Y|) → 32 → 64 → d], where d and |Y| are data dependent, and
K ∈ {10, 50}. Each hidden layer uses the leakyReLU activation function and dropout regularization
with a dropout rate of 0.2. We implemented FsNet in keras and used the RMSprop optimizer for all
the experiments. For the regularization parameter λ, we used λ = 1 for all the experiments. We
performed the experiments with 4000 epochs at a learning rate of η = 10-3, initial temperature
τ0 = 10, and end temperature τE = 0.01 in the annealing schedule for all the experiments.
5.2	Benchmark Dataset
We used six high-dimensional datasets from biological classification problems1. Table 4 in the
appendix lists the relevant details of these datasets. The performance was evaluated on the basis
of four parameters: classification accuracy, reconstruction error, mutual information between the
selected features, and model size. Because neither HSIC Lasso nor mRMR could directly classify
the samples, we used a support vector machine (SVM) (with a radial basis function) trained on
the selected features. As CAE is an unsupervised method, we added a softmax layer to its loss
function to ensure a fair comparison; the resulting model is henceforth referred to as supervised
1Publicly available at http://featureselection.asu.edu/datasets.php
6
Under review as a conference paper at ICLR 2021
Table 1: Comparison of the mean testing accuracy among FsNet, supervised CAE, HSIC Lasso
(HSIC), and mRMR with K = 10 and K = 50. Moreover, We report SVM and Diet Networks. *
The pyMRMR package, which is a wrapper of the original code, returns a memory error, and we
CoUld not execute the models on these datasets.
Dataset	FsNet	K CAE	= 10 HSIC	mRMR	FsNet	K CAE	= 50 HSIC	All features		
								mRMR	SVM	Diet-net
ALLAML	0.911	0.833	0.899	0.848	0.922	0.936	0.917	0.919	0.819	0.811
CLL-SUB	0.640	0.575	0.604	N/A*	0.582	0.556	0.680	N/A*	0.569	0.564
GLL85	0.874	0.884	0.831	N/A*	0.795	0.822	0.829	N/A*	0.759	0.842
GLIOMA	0.624	0.584	0.595	0.564	0.624	0.604	0.672	0.693	0.628	0.712
PrOState-GE	0.871	0.835	0.924	0.871	0.878	0.884	0.926	0.933	0.846	0.753
SMKCAN	0.695	0.680	0.660	0.620	0.641	0.667	0.684	0.668	0.699	0.665
CAE. Because RMSprop is a stochastic optimizer, all the results reported are the means of 20 runs
on random splits of the datasets.
Classification accuracy: Figure 2 compares the training and testing behaviors of FsNet and super-
vised CAE for embedding size b = 10 and number of selected features K = 10. The results across
the datasets show that FsNet can learn better than supervised CAE owing to its reduced number
of parameters. The classification performance of FsNet for 10 selected features is comparable or
superior to that of the SVM and Diet Networks for all the features across all the datasets. Similarly,
the comparable performances of the proposed FsNet for 10 selected features and Diet-Network with
all the features across the datasets illustrate that using a concrete random variable for the continuous
relaxation of the discrete feature selection objective does not significantly change the objective func-
tion. Additionally, the correlation between the testing and training accuracies of FsNet demonstrates
its generalization capability in comparison to supervised CAE, which seems to be overfitted under
such high-dimensional data with a limited number of samples.
Table 1 presents the testing accuracies of the feature selection methods for various numbers of fea-
tures selected on the six datasets. The experiments show that FsNet performs consistently better
than supervised CAE, HSIC Lasso, mRMR, and Diet Networks for K = 10. However, the perfor-
mance of neural-network-based models deteriorates when the number of features K increases. This
is because as the number of parameters increases, the training of the model becomes increasingly
difficult. Overall, FsNet tends to outperform the baselines even when the number of selected features
is small (K = 10), and this is a satisfactory property of FsNet.
The selected features are highly predictive of the target variable. However, they represent the rest
of the features in the dataset, as can be seen from the reconstruction error introduced in produc-
ing the original features from selected features (see Figure 3). FsNet achieves a more competitive
reconstruction error than supervised CAE and Diet Network on all the datasets.
Model-size comparison: The number of
parameters in the selection layer of super-
vised CAE is O(dK), whereas in FsNet, the
weight-predictor network of the selection
layer has O(bK) parameters. Similarly, the
number of parameters in the reconstruction
layer of supervised CAE is O(dh0), whereas
in FsNet, the weight-predictor network of
the reconstruction layer has O(bh0) param-
eters. The model compression ratio (CR)
for FsNet with respect to supervised CAE is
CR =
lθs | + lθr | +s
lωs | + lωr | +s
dh+h0d+s
bh+h0b+s
Table 2: Model-size comparison between supervised
CAE and FsNet 2 (in KBs) at K = 10. Because FsNet
predicts the model parameter by using a fixed-sized neu-
ral network, its model size is the same for all the datasets.
Dataset	FsNet	CAE	Compression ratio
ALLAML	108	4280	39.6
CLL-SUB	108	6748	62.5
GLL85	108	13160	121.9
GLIOMA	108	2704	25.0
PrOState_GE	108	3600	33.3
SMKCAN	108	11820	109.4
where S = ∣θe∣ + ∣θ∣ + ∣θd∣ denotes the number of parameters in the rest of the network. Thus, FsNet
has ≈ d times fewer parameters than supervised CAE.
O (d),
Table 2 lists the model sizes2 in kilobytes (KBs) for FsNet and supervised CAE. The results show
that FsNet can significantly reduce its model size according to the number of selected features (K)
and size of the feature embedding (b). FsNet compresses the model size by 25-122 folds in com-
2Model size figures are the size of the keras model on the disk.
7
Under review as a conference paper at ICLR 2021
parison to supervised CAE. This reduction in the model size of FsNet is due to the use of tiny
weight-predictor networks in the fat selection and reconstruction layers.
Minimum redundancy: The min-
imum redundancy criterion is im-
portant to measure the usefulness of
the selected features. According to
this criterion, the selected features
should have minimum dependencies
between themselves. We used the
average mutual information between
all the pairs of the selected features
1.2
O ι.o
I 0.8
O
M 0.6
0 0.4
□
N 0.2
U
<β rtrt
0.0
W
ALLAML CLL_SUB GLI_85	GLIOMA PrOStatjGE SMK_CAN
to compare the validity of the fea-
tures selected by FsNet and CAE Figure 4: Comparison in terms of the average mutual informa-
respectively The average mutual, tion between the features selected by CAE and FsNet, respec-
.
information is defined as follows: tively. The lower, the better.
I (S) = K(K-I) Ei / j>i I (Xi, Xj), where I (Xi, Xj) denotes the mutual information between
features i and j in the selected set S.
As shown in Figure 4, compared with CAE, the average mutual information between the features se-
lected by FsNet is significantly lower on all the datasets. This shows that compared with CAE, FsNet
more effectively selects the features with minimum redundancy owing to the use of Unique_argmaX
functions in the selection layer.
5.3	Application to inflammatory bowel disease
We studied a metagenome dataset
(Lloyd-Price et al., 2019), which con-
tains information regarding the gut bac-
teria of 359 healthy individuals and 958
patients with inflammatory bowel dis-
ease. Specifically, 7 547 features are
KEGG orthology accession numbers,
which represent molecular functions to
which reads from the guts of samples
guts are mapped. We included three ad-
ditional features: age, sex, and race.
We selected 10 or 50 features on this
Table 3: Classification accuracy of different methods on
the metagenome dataset on inflammatory bowel disease.
Accuracy
Method	K = 10	K = 50
FsNet	0.999 ± 0.002	0.994 ± 0.016
CAE	0.999 ± 0.002	0.983 ± 0.033
HSIC Lasso (B=10)	0.945 ± 0.003	0.962 ± 0.002
HSIC Lasso (B=20)	0.939 ± 0.004	0.959 ± 0.003
mRMR	0.941 ± 0.004	0.955 ± 0.003
0.914 ± 0.003
0.999 ± 0.002
SVM
Diet-networks
dataset using FsNet, CAE, HSIC Lasso, STG, and mRMR. For HSIC Lasso, as the number of
samples was high, We employed the block HSIC Lasso (Climente-GOnzalez et al., 2019), where B
denotes the tuning parameter of the block HSIC Lasso, and B = n is equivalent to the standard
HSIC Lasso (Yamada et al., 2014). The DNN based apporaches outperformed shallow methods.
FsNet and CAE could achieve perfect prediction accuracy with only 10 features. Moreover, the
compression ratio between FsNet and CAE is 21.41, and thus we conclude that FsNet can obtain
preferable performance with much less number of parameters for high-dimensional data. This result
indicates that DNN based methods can replace kernel methods even for high-dimensional data.
6	Conclusions
We proposed FsNet, which is an end-to-end trainable, deep learning-based, feature selection method
for high-dimensional data with a small number of samples. FsNet can select unique features by us-
ing a concrete random variable. Using weight-predictor functions and a reconstruction loss, it not
only required few parameters but also stabilized the model and made it appropriate for training
with a limited number of samples. The experiments on several high-dimensional biological datasets
demonstrated the robustness and superiority of FsNet for feature selection in the chosen settings.
Moreover, we evaluated the proposed FsNet on a real-life metagenome dataset, and FsNet outper-
formed the existing shallow models.
8
Under review as a conference paper at ICLR 2021
References
Krishnakumar Balasubramanian, Bharath K. Sriperumbudur, and Guy Lebanon. Ultrahigh dimen-
sional feature screening via RKHS embeddings. In AISTATS, 2013.
Muhammed Fatih Balin, Abubakar Abid, and James Y. Zou. Concrete autoencoders: Differentiable
feature selection and reconstruction. In ICML, 2019.
Wenlin Chen, James T. Wilson, Stephen Tyree, Kilian Q. Weinberger, and Yixin Chen. Compressing
neural networks with the hashing trick. In ICML, 2015.
Hector Climente-Gonzalez, Chloe-Agathe Azencott, Samuel Kaski, and Makoto Yamada. Block
HSIC Lasso: model-free biomarker detection for ultra-high dimensional data. Bioinformatics, 35
(14):i427-i435, 07 2019.
G. E. Dahl, J. W. Stokes, Li Deng, and Dong Yu. Large-scale malware classification using random
projections and neural networks. In ICASSP, 2013.
Jianqing Fan and Jinchi Lv. Sure independence screening for ultrahigh dimensional feature space.
JournaloftheRoyal Statistical Society: SeriesB(StatisticaIMethodoIogy), 70(5):849-911, 2008.
Isabelle Guyon and Andre Elisseeff. An introduction to variable and feature selection. Journal of
machine learning research, 3(Mar):1157-1182, 2003.
Yixue Li and Luonan Chen. Big biological data: Challenges and opportunities. Genomics, Pro-
teomics & Bioinformatics, 12(5):187-189, 2014.
Shuangli Liao, Quanxue Gao, Feiping Nie, Yang Liu, and Xiangdong Zhang. Worst-case discrimi-
native feature selection. In IJCAI, 2019.
Bo Liu, Ying Wei, Yu Zhang, and Qiang Yang. Deep neural networks for high dimension, low
sample size data. In IJCAI, 2017.
Jason Lloyd-Price, Cesar Arze, Ashwin N Ananthakrishnan, Melanie Schirmer, Julian Avila-
Pacheco, Tiffany W Poon, Elizabeth Andrews, Nadim J Ajami, Kevin S Bonham, Colin J Bris-
lawn, et al. Multi-omics of the gut microbial ecosystem in inflammatory bowel diseases. Nature,
569(7758):655-662, 2019.
Yang Young Lu, Yingying Fan, Jinchi Lv, and William Stafford Noble. DeepPINK: reproducible
feature selection in deep neural networks. In NeurIPS, 2018.
C. J. Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous relax-
ation of discrete random variables. In ICLR, 2017.
Vivien Marx. The big challenges of big data. Nature, 498(7453), 2013.
Mahdokht Masaeli, Glenn Fung, and Jennifer G. Dy. From transformation-based dimensionality
reduction to feature selection. In ICML, 2010.
Di Ming and Chris Ding. Robust flexible feature selection via exclusive L21 regularization. In
IJCAI, 2019.
Hanchuan Peng, Fuhui Long, and Chris H. Q. Ding. Feature selection based on mutual information:
Criteria of max-dependency, max-relevance, and min-redundancy. IEEE TPAMI, 27(8):1226-
1238, 2005.
Adriana Romero, Pierre Luc Carrier, et al. Diet networks: Thin parameters for fat genomics. In
ICLR, 2017.
Le Song, Alex Smola, Arthur Gretton, Karsten M Borgwardt, and Justin Bedo. Supervised feature
selection via dependence estimation. In Proceedings of the 24th international conference on
Machine learning, pp. 823-830, 2007.
Robert Tibshirani. Regression shrinkage and selection via the lasso. J. Royal Stat. Society, 58(1):
267-288, 1996.
9
Under review as a conference paper at ICLR 2021
Antanas Verikas and Marija Bacauskiene. Feature selection with neural networks. Pattern Recogni-
tion Letters, 23(11):1323-1335, 2002.
Pascal Vincent, Hugo Larochelle, et al. Stacked denoising autoencoders: Learning useful represen-
tations in a deep network with a local denoising criterion. JMLR, 11:3371-3408, 2010.
Qian Wang, Jiaxing Zhang, Sen Song, and Zheng Zhang. Attentional neural network: Feature
selection using cognitive feedback. In NIPS, 2014.
Piotr IWo Wojcik and Marcin KUrdzieL Training neural networks on high-dimensional data using
random projection. PAA, 22(3):1221-31, 2019.
Makoto Yamada, Wittawat Jitkrittum, Leonid Sigal, Eric P Xing, and Masashi Sugiyama. High-
dimensional feature selection by feature-wise kernelized lasso. Neural computation, 26(1):185-
207, 2014.
Makoto Yamada, Jiliang Tang, et al. Ultra high-dimensional nonlinear feature selection for big
biological data. IEEE TKDE, 30(7):1352-1365, 2018a.
Makoto Yamada, Yuta Umezu, Kenji Fukumizu, and Ichiro Takeuchi. Post selection inference with
kernels. In AISTATS, 2018b.
Yutaro Yamada, Ofir Lindenbaum, Sahand Negahban, and Yuval Kluger. Feature selection using
stochastic gates. ICML, 2020.
Xiucai Ye, Hongmin Li, Akira Imakura, and Tetsuya Sakurai. Distributed collaborative feature
selection based on intermediate representation. In IJCAI, 2019.
10
Under review as a conference paper at ICLR 2021
Appendix
Algorithm 2 Training of FsNet
Input: data matrix X ∈ Rn×d, output labels y ∈ {1,∙∙∙ ,L}), K target number of features, encoder
network ENCθe(∙), decoder network DECθd(∙), reconstruction function RECθr(∙), classification
network fθc (∙), weight prediction networks 中ωs (∙) & ψωs (∙), learning rate η, start temperature τo,
end temperature τE, and number of epochs E
Output: set of selected features S, model parameters Θ
1:	Initialize Θ = {ωs, θe, θd, ωr, θc}.
2:	for e ∈ {1,…，E} do
3:	Update the temperature T = t0(te/τo)fe^E
4:	(δs,1,…δs,d) — (Sωs (。(UI))…ψωs (O(Ud)))
5:	μ(k) - COnCrete(θSk), τ) using (2)
6:	M 一 (μ⑴,…，μ(K))T
7:	S — Uargmax(M >)
ENCθe (M xi) if training,
ENCθe (xS)	inference
9:	b — fθ (h) e
~ _______，-、
10:	h — DECθd (h)
11:	(θ(1),…θ(d)) — (ψωr (φ(ui)) …Sωr (Φ(Ud)))
12:	xb — RECθr (h)
13:	Define the loss L.
14:	Compute Vωr L, Vθ L, ^θd L, and Vθe L using backpropagation.
15:	Compute V^k^) L using reparameterization trick
16:	Update ωr —s ωr - ηVωr L, θ — θ - ηVθL,
θd — θd - ηvθd L, θe — θe - nVθe L, and
(k)	(k)
ωr — ωr - ηV (k) L
ωr
17:	end for
18:	return S , Θ
8:	h —
Table 4: Details of Datasets used in this paper
Dataset	Classes	Sample Size (n)	Dimensions (d)
ALLAML	2	72	7,129
CLL-SUB	3	111	11,340
GLL85	2	85	22,283
GLIOMA	4	50	4,434
PrOState-GE	2	102	5,966
SMKCAN	2	187	19,993
11
Under review as a conference paper at ICLR 2021
800	1 600 2400 3200 4000
Test
NUmber of Epochs
0.4
Number of Epochs
(A) ALLAML
A0e』n。。4 uo--o--ss-。
Train
800	1600 2400 3200
Number of Epochs
4000
Number of Epochs
(C) GLL85
Train
800	1600 2400 3200 4000
Number of Epochs
0.4
Test
800	1 600 2400 3200 4000
NUmber of Epochs
(B) CLL-SUB
Train	Test
Number of Epochs
A0£n。。4 uo--o--ss-。
0.21-----1------1-----1------1-----1
0	800	1 600 2400 3200 4000
Number of Epochs
(D) GLIOMA
Train
800	1600 2400 3200 4000
Number of Epochs
Number of Epochs
(E) ProState_GE
(F) SMKCAN
Figure 5:	CompariSon among FSNet, SuperviSed CAE, and Diet Network in termS of mean training
and teSting accuracieS over the epochS. For the neural-network-baSed approacheS, we Set the model
parameterS to b = 10 and K = 10.
Test
J- 1.05
H 1
O
o
<D
DC
0.95
0.9
0.8
0	1000	2000	3000	4000
Number of Epoches
(B)	CLL-SUB
∙1OLW ∙UOO<DH UE8≡
0	1000	2000	3000	4000
Number of Epoches
(C)	GLL85
1.2
小1
向1
S 0.9
<D
rr
U 0.8
ra
<D
≡ 0.7
0.6
0	1000	2000	3000	4000
Number of Epoches
SMK-CAN
(D) GLIOMA
(E) Prostate-GE
Figure 6:	Comparison between the proposed FsNet and existing supervised CAE approaches in
terms of the mean test reconstruction error over the epochs.
12
Under review as a conference paper at ICLR 2021
Number of Epochs
(A) K=10
Number of Epochs
(B) K=50
Figure 7: Comparison between the proposed FsNet and supervised CAE and Diet Network in terms
of the mean test classification accuracy over the epochs on metagenome dataset. The performance
of the FsNet and Diet Network is more stable than the supervised CAE.
13