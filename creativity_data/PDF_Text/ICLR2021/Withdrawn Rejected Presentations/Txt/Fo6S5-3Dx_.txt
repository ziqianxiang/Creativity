Under review as a conference paper at ICLR 2021
Deep Evolutionary Learning for Molecular
Design
Anonymous authors
Paper under double-blind review
Ab stract
In this paper, we propose a deep evolutionary learning (DEL) process that in-
tegrates fragment-based deep generative model and multi-objective evolutionary
computation for molecular design. Our approach enables (1) evolutionary op-
erations in the latent space of the generative model, rather than the structural
space, to generate novel promising molecular structures for the next evolution-
ary generation, and (2) generative model fine-tuning using newly generated high-
quality samples. Thus, DEL implements a data-model co-evolution concept which
improves both sample population and generative model learning. Experiments
on two public datasets indicate that sample population obtained by DEL ex-
hibits improved property distributions, and dominates samples generated by multi-
objective Bayesian optimization algorithms.
1	Introduction
A drug is a molecule that binds to a target (e.g. protein) to inhibit or activate specific pathways
in pathogens or host cells that cause abnormal phenotype. Drug discovery and development is a
costly and time-consuming process, which is compounded by personalized medicines development
for cancer or other complex and rare diseases. Computational drug discovery has been shown to
accelerate the whole discovery process using simulations and machine intelligence. However, chal-
lenge remains in this field by demands for a robust and unbiased feature representation theories for
molecules and their corresponding receptors, and efficient search algorithms. The rise ofAI and data
science provides us with a unique opportunity to reevaluate the problem and develop fast intelligent
search or design approaches (Gromski et al., 2019; Chen et al., 2018). These new technologies claim
differences from the traditional ones in two aspects: (1) features can be automatically learned using
embedding techniques on a large number of training samples, and (2) high-level relationships (in su-
pervised case) and complex distributions (in unsupervised case) can be captured using appropriate
deep architectures. Recently, new representation theories and architectures have been proposed in
the domain of molecular generation. There exist two new major methods to present a molecule for
a machine learning algorithm. The first method converts a molecule structure to a string, such as the
simplified molecular-input line-entry system (SMILES) string (Weininger, 1988), and adopt natural
language processing (NLP) methods for supervised or unsupervised learning. The second uses an
undirected graph to present a molecular structure and applies graph convolutional neural networks
(Duvenaud et al., 2015). Three major families of AI algorithms have been developed for novel drug
discovery: deep generative models (DGMs), reinforcement learning, and the combination of both.
As one family of major neural probabilistic models for data modelling, generative autoencoders
(e.g. variational autoencoder (VAE) (Kingma & Welling, 2014)) have been adopted to learn on
either SMILES strings (Romez-Bombarelli et al., 2018) or molecular graphs (Simonovsky & Ko-
modakis, 2018) with corresponding physical and biochemical properties for molecular generation.
By integrating the generative adversarial nets and autoencoders, adversarial autoencoders (AAEs)
have also been well applied to molecular design (Kadurin et al., 2017). The advantage of using
generative autoencoders is that, molecules, as discrete objects in our world, are mapped to the con-
tinuous latent space, whose landscape can be organized by their properties, which helps generate
new structures with preferred property values. However, critical problems remain due to imperfect
representation methods. When SMILES strings are used in VAE, the model suffers from imbalance
of tokens in embedding, generation of invalid structures, and the problem where two almost identi-
cal molecules have markedly different canonical SMILES strings. When using graph as molecular
1
Under review as a conference paper at ICLR 2021
representation in VAE, a technical difficulty is to design an effective graph decoder. In addition to
other heuristic methods, a SMILES decoder can be used to pair with a graph encoder. While DGMs
offer convenience of searching in latent space, reinforcement learning algorithms can directly search
in the molecules’ structural space by adding or deleting bounds and atoms (Zhou et al., 2019). In the
Markov decision process (MDP) for drug design, the agent is a molecular generator, the molecular
structure indicates the state, the actions are modifications to the current structure, and a simulator
(e.g. surrogate model) is often used as the environment to provide reward. Furthermore, generative
and predictive models can be integrated in MDP to form deep reinforcement learning (DRL) meth-
ods, where the generative model is trained as a policy approximation and the predictive model can
be used as a value function approximation (You et al., 2018; Popova et al., 2018). Search in the
discrete input space and inefficient learning are arguable concerns to be addressed when applying
reinforcement-learning-based solutions for compound design.
Interestingly, as an old peer of reinforcement learning for black-box optimizations, evolutionary
computation (EC) methods (Eiben & Smith, 2015) have been catching up with promising perfor-
mances in modern optimization, design and modelling problems. Besnard et al. (2012) present a
strategy for evolution of ligands along multiple properties in the structural space, where a library of
knowledge-based chemical structural transformation is used as the mutation operator. Interactions
between EC and neural networks have mainly focused on network evolution and neural surrogate
models for fitness functions. For examples, EC has been used at a large scale for neuroevolution that
leads to evolution of neural network architectures (Stanley et al., 2019); feedforward neural network
is commonly used as fitness function in EC (Mandal et al., 2019). Furthermore, it has been recently
discovered that evolutionary strategy (ES) can perform competitively with reinforcement learning in
game AI (Salimans et al., 2017). ES (Wierstra et al., 2014) and estimation of distribution algorithms
(EDA) (Hauschild & Pelikan, 2011) build parameterized search distributions over promising points
and either employ gradient information or sample from such a probabilistic model to find better
points. Both probabilistic strategies from EC can potentially be used as alternatives to Bayesian op-
timization (BO) in (continuous) black-box optimizations. A single-objective BO has been recently
applied to molecule optimization in the latent space of VAE (Romez-Bombarelli et al., 2018).
Since model learning is essentially parameter estimation from the statistical modelling perspective,
the quality of data in deep learning is crucial for model performance. Data augmentation is becoming
a new strategy in deep learning to improve the training of a model. For example, in computer
vision, transformations (such as rotation and flipping) of images are used to increase the sample
size when the original data set is insufficient (Perez & Wang, 2017; Cubuk et al., 2019; Shorten &
Khoshgoftaar, 2019). In NLP, a text dataset can be augmented using tricks such as replacing words
or phrases with their synonyms (Wei & Zou, 2019), and resorting aids from other language models
(e.g. word embedding and neural machine translations) (Sennrich et al., 2016; Wei & Zou, 2019).
Basically, these methods either increase the data by transforming existing information which can
only alleviate the limit of certain techniques (e.g. convolution), or indirectly borrow new information
from other sources (e.g. methods in NLP).
In summary, even though modern machine learning, evolutionary computation, and data science
methods have been applied to molecular design and achieved promising results, we are still chal-
lenged by three chief issues. (1) An effective representation method for compound structures, that is
encoding-decoding friendly and invariant to multimorphic forms, is still missing. (2) New ideas are
expected for effective representation and coding of discrete structures in EC. And, (3) quality of data
can be further improved as current data augmentation tricks only increase the number of samples
but does not specifically address data quality. In this paper, we propose a novel deep evolution-
ary learning (DEL) process that combines the merits of deep generative model and multi-objective
evolutionary computation together for molecular design. Specifically, our work has three major con-
tributions. (1) In our approach, latent representations of phenotypic samples in a population serve as
genotypic codes for evolutionary operations.This approach differs from traditional evolutionary al-
gorithms that search in the original space of a problem. Specifically, our framework’s DGM encoder
projects molecular structures in a population from discrete space to continuous latent space where
evolutionary operations are applied to help explore the latent representation space. Subsequently,
the DGM decoder maps the genotypic representation to the phenotypic space for generating new
molecules with desired property values. (2) In each evolutionary generation, the newly formed
population containing novel competitive molecules can be used to further fine-tune the DGM. This
approach is an innovative data augmentation strategy that enriches training data with novel high-
2
Under review as a conference paper at ICLR 2021
quality samples. The whole DEL process implements a new learning paradigm that co-evolves data
and model alternatingly through multiple evolutionary generations. (3) Our comprehensive experi-
ments demonstrate that DEL is able to produce populations of novel samples with improved values
of properties and outperforms state-of-the-art multi-objective BO algorithms (MOBO).
2	Method
The proposed deep evolutionary learning (DEL) process combines deep learning and multi-objective
EC through the latent representation space of molecules. One of the theoretical innovations of our
approach is that it demonstrates that EC methods are extendable to corresponding deep versions.
The main idea is illustrated in Figure 1a and formally presented in Algorithm 1 (see Appendix A.1).
This algorithm consists of the following steps. (a) A VAE (as molecule modeller) and a multi-
layer perceptron neural network (MLP, property predictor as regularizer) are pretrained using all
the original training data to start the first evolutionary generation, or, if not in the first generation,
using samples from the previous population. (b) Training samples (if first generation) or popula-
tion samples (otherwise) are projected to the latent space using the encoder of the VAE. (c) Based
on non-dominated ranking and crowding distances of samples with respect to multiple properties,
evolutionary operations (selection, recombination/crossover and mutation) are conducted on the la-
tent representations of the samples. (d) Given these new latent codes after evolutionary operations,
new molecule samples are generated by the decoder of the VAE. (e) Properties of these generated
samples are obtained using a simulator (e.g. RDKit (Landrum, 2006) in our experiment). (f) New
samples with good desired properties and good samples from the previous generation form the new
population. (g) Steps (b-f) are iterated for multiple generations. (h) The final population is returned.
(a) DEL.	(b) FragVAE.
Figure 1: Deep evolutionary learning process and deep generative model integrated in DEL.
The major advantages of our DEL algorithm over existing interactions between EC and neural net-
works can be explained as follows. (a) We directly evolve a collection of data rather than many
neural network structures and parameters. Data evolution tends to be more efficient than direct evo-
lution of model structures and parameters. (b) The single neural network model (i.e. DGM in DEL)
can be indirectly improved through learning on the evolved data with modern gradient-based varia-
tional learning and inference algorithms. Thus, the improvement of populations along evolutionary
generations can be viewed as an effective data augmentation strategy that includes novel and high-
quality samples for further training of the neural network. (c) The continuous latent representation
space established by the encoder of DGM can be naturally used as encoding (genotypic) space for
evolutionary computation. Thus, evolutionary operations are carried out in the latent space instead
of the discrete structural input space, allowing more efficient and smooth exploration, because the
latent space is often multimodal and can be organized by properties (regularized by the property
predictor) and evolutionary operations in this space can help the search escape from local regions
and explore new regions of interest. (d) The multi-objective operations - non-dominated sorting and
crowding distance, can help identify competitive and diverse parent samples to breed offspring. In
summary, DEL takes advantages of both multi-objective EC and probabilistic neural model learning.
The DGM, multi-objective components (non-dominated sorting and crowding distance), evolution-
ary operations, formation of new populations are discussed in details as below.
2.1	FragVAE for Fragment-based Molecular Modelling
In our DEL process, we adopted a VAE model originally for fragment-based molecular genera-
tion (Podda et al., 2020). The concept of fragment-based drug design (FBDD) was introduced in
3
Under review as a conference paper at ICLR 2021
(Shuker et al., 1996). In FBDD-based approaches, small organic molecules that bind to proximal
subsites of a protein are identified, optimized, and linked together to produce high-affinity ligands.
Wet-lab approaches for FBDD include X-ray crystallography and NMR spectroscopy. Compared
to atom-based drug design, FBDD has the following advantages (Erlanson, 2011). (1) The search
space in FBDD is much smaller (107 versus 1060). (2) Identifying a fragment with certain affinity
to the target may mean finding a pharmacophore. (3) Fragment-based synthesis could be more ef-
ficient than high-throughput screening. Majority fragmentation methods, which break a molecule
into parts, are based on synthetic accessibility. For examples, RECAP (retrosynthetic combinato-
rial analysis procedure) is a method that breaks bonds formed by chemical reactions (Lewell et al.,
1998); BRICS (breaking of retrosynthetically interesting chemical substructures) generates a more
elaborated set of fragmentation rules along synthetically accessible bonds and generates more frag-
ments than RECAP (Degen et al., 2008). BRICS is used in (Podda et al., 2020) to chop a SMILES
string into several fragments. Then, fragment embeddings are produced using Word2Vec (Mikolov
et al., 2013). Next, the sequences of fragments are modelled by a GRU-based VAE. In our work,
a multi-head feedforward neural network component for predicting values of properties is added to
the original model such that the latent representations can be regularized by properties of interest.
Additionally, we normalize the three loss terms using batch size, and allow tuning of the weights
among the loss terms. A crucial implementation bug in the original VAE model was also corrected
(see Appendix A.2). Hereafter, we name this modified VAE model for fragments as FragVAE whose
architecture is displayed in Figure 1b.
We denote the encoder parameter by φ, the decoder parameter by θ, and the property predictor
network by fψ (z) parameterized by ψ. The objective (to be minimized) of this DGM employed in
DEL is a weighted combination of three terms:
lφ,θ,ψ = -Eqφ(z∣x)[logPθ(x|h)] + βKL(qφ(z∣x)l∣Pθ(Z)) + αEqφ(z∣χ)[MSE(fψ(z), y)], (1)
where the first term is to reduce the reconstruction error, the second term is to regularize the posterior
latent distribution with a simple prior, and the third term uses mean squared error of property predic-
tion to further regularize the posterior distribution of latent codes. Previous studies unveil that VAE
can easily fail on modelling text data because of the training imbalance between the reconstruction
error (difficult to reduce once the KL divergence becomes very small) and the KL divergence (easy
to diminish to zero). Thus, proper trade-off between the reconstruction error and KL divergence
through β-VAE (Higgins et al., 2017) is vital in text generation and molecular generation (Yan et al.,
2020; Bowman et al., 2016). In practice, the value of β should be smaller than 1. To look for a
suitable value of β, we design a versatile function, called β-function, as formulated below,
β(t) = min n max {aek(I-TT),l},u},	(2)
where T represents the total number of epochs, t ∈ {1,2,…，T} indicates the current index of
epoch, k controls the incremental speed, a defines the amplitude, l and u serve as lower and upper
bounds respectively for the value of β. With different settings, a variety of curves of this function
are shown in Figure 6 (see Appendix A.7). The value of α can be set similarly in FragVAE.
2.2	Non-domination Rank and Crowding Distance
To get non-domination rank and crowding distance of a feasible solution for guiding the sample
selection (Section 2.3) and the population merging (Section 2.4), the fast non-dominated sort and
crowding comparison methods are adopted from the classic NSGA-II algorithm for multi-objective
optimization (Deb et al., 2002). The properties in molecular design are treated as objectives. In an
optimization problem with K objectives f (Z) = {fι(z),…，fκ(z)}, feasible solution zι is said to
dominate z2 (denoted by zι Y z2), if ∀k ∈ {1,…，K}: fk(zι) ≤ fk(z2) and ∃k ∈ {1,…，K}:
fk (Z1 ) < fk (Z2 ). Using this concept of domination, all feasible solutions in a collection can be
sorted to form Pareto frontiers (or fronts, ranks) F = {Fι, F2, ∙∙∙}. Samples in the same frontier
do not dominate each other. Frontier Fi dominates Fj for j > i. Thus, we define function F (Z) to
retrieve the rank (i.e. frontier index) of any feasible solution Z in the population.
The crowding distance of a feasible solution is computed as the normalized perimeter of the cuboid
formed by its immediate neighbours along all objective axes. To compute the crowding distance
of Zi, the normalized distance between its nearest neighbours above (denoted by Za) and below
(denoted by Zb) it w.r.t. the k-th objective axis is calculated using dk(zi) = f，：：!二馨；",where
fk -fk
4
Under review as a conference paper at ICLR 2021
fkmax and fkmin are respectively the maximal and minimal values of the k-th objective. Then, these
individual results are summed up to form the crowding distance of zi: d(zi) = PkK=1 dk(zi). The
concept of crowding distance measures the density of the area around a feasible solution.
Using the two concepts, partial order can be defined. We say zι Yn z? if either (1) zι Y z? (that is
F(z1) < F(z2)), or (2) F(z1) = F(z2) and d(z1) > d(z2). When two solutions have same rank,
the one with larger crowding distance is preferred because it helps maintain a diverse population.
2.3	Evolutionary Operations
The evolutionary operations include parent selection, recombination and mutation to produce new
offspring in the evolutionary process. Binary tournament selection is applied to select one out of two
randomly drawn samples from the current population. In such a selection process, let us suppose
z1 and z? are randomly taken from the population and z1 Yn z? . Sample z1 will be selected with
selection probability ps which is close to one, and z? will be selected with a small chance 1-ps. This
selection process is repeated M times to thus find M parents where M is the fixed population size.
A pair of such parents will produce two children through recombination and mutation operations.
Given two parents’ latent representation zp1 and zp?, there are two recombination options - linear
and discrete methods, to produce their new children Zi and Z?. For linear recombination, Zi =
Zpi+ri(Zp2-Zpi) and Z2 = Zpi+r2(zp2-Zpi) where ri = -d+(1+2d)αι, r2 = -d+(1+2d)α2,
d = 0.25, and αι,α? 〜Uniform(0,1). For discrete method, supposing a latent representation
vector is of length L, an integer l is randomly drawn from {1,…，L 一 1} such that Zi = [zpi[1 :
l], Zp2[l + 1 : L]] and Z? = [zp2[l : l], Zpi[l + 1 : L]]. After crossover, a new sample Zm
(m ∈ {1, ∙∙∙ , M}) will have a small mutation probability Pm (say 0.01) of getting mutation. For
Zm, a random value r is drawn from Uniform(0,1). If r < pm, then a random integer l is randomly
selected from {1, ∙∙∙ ,L} such that the l-th position of Zm is replaced with a value drawn from
standard Gaussian distribution: Zm, 〜N(0,1).
2.4	Forming New Population
Ideally we need to maintain excellent and diverse populations. After possible mutation operations,
all M genotypic coding vectors will pass through the decoder of the DGM to produce phenotypic
samples. All valid samples (supposed in setPt+i) will be kept to merge with the previous population
(denoted by Pt) to produce a new generation (denoted by Pt+i). To implement it, all samples in
Pt+i +Pt are sorted based on their non-domination ranks first and then on their crowding distances.
Finally, only the top M samples are taken from them to form the new population.
3 Experiments
The performance of DEL was investigated on the ZINC (Irwin & Shoichet, 2005) and PCBA (Wang
et al., 2016) datasets. These data were processed in the work of Podda et al. (2020). ZINC and
PCBA are respectively composed of 227,945 and 383,790 molecules with two or more fragments.
More statistics of both data can be found in (Podda et al., 2020). We comprehensively investigated
the empirical performance of FragVAE and DEL. Three properties (QED: quantitative estimation
of drug-likeness, SAS: synthetic accessibility score, and logP: water-octanol partition coefficient)
are selected as objectives in DEL. Molecules with large QED, low SAS, and small logP values are
prioritized. Incorporation of other properties (e.g. binding affinity, structure-property relationship,
and ADME) will be considered in future work. QED, a scalarization of eight molecular properties
(including logP) (Bickerton et al., 2012), is an adequate initial screening step for drug candidates. As
we dive into more specific applications, selective properties can be tailored for subsequent screening.
Thus, explicit usage of logP as one objective in DEL can help better assess lipophilicity, a key factor
in drug design for some diseases, e.g. kidney and heart problems.
3.1	Evaluation of FragVAE
As FragVAE is a significant modification of the original model used in (Podda et al., 2020), we
investigated the impact of β value to the performance of FragVAE in terms of loss function values
5
Under review as a conference paper at ICLR 2021
through Figure 7 (see Appendix A.7). Other hyperparameter values can be found in Appendix
A.3. One can see that a large β value can quickly reduce the KL loss to near zero which leads to
stagnant reductions of reconstruction error and property regression error - the notorious posterior
collapse problem (Goyal et al., 2017), because it is much easier to reduce the KL divergence than the
reconstruction error in complex sequence modelling. Using a suitable small value of β would allow
the continuous decrease of the reconstruction error and property regression error. This observation
is consistent with discoveries in language generative models (Yan et al., 2020; Bowman et al., 2016).
Table 2 (see Appendix A.6) shows the validity, novelty and diversity of 20,000 samples generated
from trained FragVAEs using standard Normal prior to sample z followed by the decoder. Results of
previous language-model-based and graph-based methods are also given for comparison. In general,
the validity is defined as the ratio of number of valid generated samples to total number of generated
samples. To clarify, the perfect validity reported in Podda et al. (2020) is actually calculated as the
ratio of valid generated SMILES strings after discarding invalid fragment sequences versus total
number of valid fragment sequences, i.e. Validity (SMILES) in Table 2. We found that this ratio is
always 1 in fragment-based models. To have a better understanding about the model, we hence com-
puted the validity of fragment sequences as the percentage of number of valid fragment sequences
to total number of generated fragment sequences, i.e. Validity (Fragments) in Table 2. The novelty
is defined as the ratio of number of generated novel valid molecules that do not exist in the training
data versus total number of generated valid samples. The diversity is calculated as the percentage
of generated unique valid samples among total number of generated valid samples. When the value
of β is very small (0.01), the posterior p(z|x) is highly different from the simple standard Normal
prior p(z). Thus, it is reasonable to see relatively low diversity in samples derived using standard
Normal distribution. However, it does not imply that FragVAE with a very small value of β is poor
at learning latent representation. In fact, previous work in β-VAE shows that small values of β tend
to encourage disentangled representations and form latent clusters (Li et al., 2020).
The property distributions of generated samples can be important indicators to the proximity of gen-
erated samples with actual samples. Figure 2 shows the distributions of QED, SAS and logP in
samples generated using standard Normal prior. Large β value can lead to the sample SAS distri-
bution appearing at the right side of the actual SAS distribution. Interestingly, the property distribu-
tions when using β = 0.01 do not resemble actual data, implying that using a very small value of
β would lead to latent representations that deviate from standard normal distributions. To summa-
rize, β ≤ 0.1 can prevent the model training from posterior collapse and can form structured latent
representation space which is useful for latent space exploration using optimization techniques.
(a) Property distributions over ZINC.
ATOMS	BONDS	RINGS
■ C F □N ≡O -other HSINGLE ■ DOUBLE ■ TRIPLE ≡Tri -Quad ■ Pent BHex
(b) Structural feature distributions over ZINC.
ATOMS
・ C F □ N -0 -Other
—PCBA
T=I
-¢=0.1
-¢=0.01
5	10	-10 -5	0	5	10
SAS	LOGP
BONDS
■ SINGLE ■ DOUBLE ■ TRIPLE
PCBA A=I β=Q.l β=QΛl
RINGS
■ Tri I Quad ■ Pent ■ Hex
PCBA F=I ¢=0.1 ^=0.01
PCBA A=I A=(H ¢=0.01

(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 2: Property and structural feature distributions over 20K randomly sampled molecules.
3.2 DEL Performance
We executed DEL processes using fixed or annealed loss trade-off weights: (1) β = 0.1 and α = 1,
(2) β = 0.01 and α = 1, (3) β = 0.1 and α = 1 in the initial training of FragVAE, then annealing
β to 0.4 and α to 4 in the second generation of DEL (denoted by β = 0.1 → 0.4, α = 1 → 4),
and similarly (4) β = 0.01 → 0.4, α = 1 → 4. Other hyperparameter values can be found in
Appendix A.4. The change of losses is shown in Figure 8. We observe that (1) all settings lead
to similar reconstruction error convergence, (2) smaller values of β tend to obtain smaller property
6
Under review as a conference paper at ICLR 2021
prediction error, (3) annealing can help reduce the KL divergence compared to the corresponding
fixed settings.
Figure 3 shows population validity, novelty and diver-
sity in different DEL processes. In this chart, validity
(SMILES) is the validity of SMILES strings in the pop-
ulation; validity (fragments) is the validity of fragment
sequences sampled using FragVAE after evolutionary
operations; novelty is the ratio of population samples
that are not in the training data against the population
size; and diversity is the ratio of unique population
samples against the population size. We observe that
almost all samples in the populations are novel; and
the population samples in the last generation are quite
diverse, ranging from 0.798 to 0.988. Table 9 lists the
increasing numbers of high-quality novel molecules
discovered along the DEL processes. In contrast to
the training molecules visualized in Figures 9 and 10
(Appendix), DEL is able to discover novel and diverse
Data	beta	alpha	Evolutionary Generation	Validity (SMILES)	Validity (Fragments)	Novelty	Diversity
ZINC	0.10	1	1	1.000	0.873	0.997	0.988
	0.10	1	5	1.000	0.961	0.999	0.921
	0.10	1	10	1.000	0.554	1.000	0.805
	0.1 to 0.4	1 to 4	1	1.000	0.957	0.999	0.986
	0.1 to 0.4	1 to 4	5	1.000	0.259	0.999	0.986
	0.1 to 0.4	1 to 4	10	1.000	0.396	0.996	0.986
	0.01	1	1	1.000	0.819	0.995	0.956
	0.01	1	5	1.000	0.945	0.998	0.958
	0.01	1	10	1.000	0.973	0.999	0.963
	0.01 to 0.4	1 to 4	1	1.000	0.917	0.996	0.911
	0.01 to 0.4	1 to 4	5	1.000	0.940	0.999	0.978
	0.01 to 0.4	1 to 4	10	1.000	0.919	1.000	0.976
PCBA	0.10	1	1	1.000	0.513	0.994	0.976
	0.10	1	5	1.000	0.787	0.999	0.981
	0.10	1	10	1.000	0.839	0.993	0.976
	0.1 to 0.4	1 to 4	1	1.000	0.527	0.980	0.935
	0.1 to 0.4	1 to 4	5	1.000	0.984	0.994	0.918
	0.1 to 0.4	1 to 4	10	1.000	0.694	0.999	0.915
	0.01	1	1	1.000	0.301	0.997	0.417
	0.01	1	5	1.000	0.912	1.000	0.655
	0.01	1	10	1.000	0.762	1.000	0.798
	0.01 to 0.4	1 to 4	1	1.000	0.849	0.998	0.633
	0.01 to 0.4	1 to 4	5	1.000	0.827	1.000	0.863
	0.01 to 0.4	1 to 4	10	1.000	0.755	1.000	0.895
Figure 3: Population validity, novelty and
diversity during DEL processes.
high-quality molecules (see Figures 11-18 in Appendix). Furthermore, the distributions of proper-
ties and structural features of samples along the evolutionary process are compared in Figure 4 (and
Figures 19-21 in Appendix). It can be seen that the process can be quite different from the actual
data distribution and is able to gradually improve the distributions of QED, SAS and logP in popula-
tion samples towards the preferred goals. Interestingly, samples randomly generated using standard
Normal prior do not clearly show this trend (Figures 22-25 in Appendix).
ATOMS	BONDS	RINGS
■ C FoN ≡0 "Other ■ SINGLE ■ DOUBLE hTRIPLE ≡Trl IQUad - Pent ■ Hex
ZINC
—DEL(I)
-DEL(5)
-DEL(F)
0.4
0.2
0.0
ZINC
—DEL(I)
-DEL(5)
-DEL(F)
0.00 0.25 0.50 0.75 1.00
QED
2	4	6	-10	-5	0	5	10
SAS	LOGP
ZINC DEL(I) DEL(5) DEL(F)
ZINC DEL(I) DEL(5) DEL(F)
-PCBA
—DEL(I)
-DEL(5)
-DEL(F)
0.00 0.25 0.50 0.75 1.00
QED
(a) Property distributions over ZINC.
(c) Property distributions over PCBA.
PCBA DEL(I) DEL(5) DEL(F)
(b) Structural feature distributions over ZINC.
ATOMS	BONDS	RINGS
■ C F □N ≡O "Other	■ SINGLE ■ DOUBLE B7HIPLE ≡Trl IQUad BPent BHex
----------------------------u
PCBA DEL(I) □EL(5) DEL(F) PCBA DEL(I) DEL(5) DEL(F)
(d) Structural feature distributions over PCBA.
Figure 4: Property & structural feature distributions of DEL populations (β=0.01 → 0.4, α=1 → 4).
3.3 Comparison with Multi-Objective Bayesian Optimization
DEL was compared with two MOBO methods: q-Pareto Efficient Global Optimization (qParEGO)
and q-Expected Hypervolume Improvement (qEHVI) (Daulton et al., 2020). These MOBO methods
were run in the latent space of FragVAE trained in the first generation of DEL using all training
samples. Hyperparameter settings of these algorithms are listed in Appendix A.5. Figure 26 shows
the hypervolumes of both algorithms and the quasi-random baseline which selects candidates from
a scrambled Sobol sequence along batches. Hypervolumes obtained using these models are plotted
in Figure 26. It shows that qParEGO and qEHVI work better with β = 0.01 than that with β = 0.1.
To qualitatively compare DEL with the MOBO algorithms, the first five Pareto fronts obtained using
DEL and last five batches obtained using qParEGO and qEHVI are visualized in Figure 5 and Figure
27 in Appendix. We can see that the solutions from qParEGO and qEHVI are behind the Pareto
fronts from DEL. To quantitatively compare DEL with qParEGO and qEHVI, we conducted non-
dominated sorting on the combination of the first Pareto front of DEL and the last six batches of
qParEGO and qEHVI and report the results in Table 1. It shows that all solutions from the DEL
Pareto front stay in the new integrated Pareto front, while almost all solutions from qParEGO and
qEHVI are behind the integrated Pareto front. Furthermore, as indicated in Table 3 in Appendix,

7
Under review as a conference paper at ICLR 2021
DEL runs more efficiently than these MOBO algorithms, even though the population size (20,000)
of DEL is much larger than the batch sizes (8) of MOBO algorithms. It has been a well-known
challenge to scale UP BO algorithms.
QED
•	DEL Front 1
•	DEL Front 2
DEL Front 3
■ DEL Front 4
•	DEL Front 5
*	qEHVI Front 1
*	qEHVI Front 2
*	qEHVI Front 3
qEHVI Front 4
qEHVI Front 5
•	qParEGO Front 1
•	qParEGO Front 2
♦	qParEGO Front 3
qParEGO Front 4
qParEGO Front 5
0.4
0-2
6
一
10
-4
2
0
~2 IogP
0.0
4
SAS
QED

⑶ ZINC.	(b) PCBA.
Figure 5: Pareto fronts of DEL and MOBO algorithms (β = 0.01 → 0.4). In the legend, the last
batch of qParEGO or qEHVI is written as Front 1.
Table 1: Non-dominated sorting of combination of DEL, qParEGO and qEHVI Pareto fronts.
			Pareto Front Size for Comparison			In New Pareto Front		
Data	DEL HyPerParametel		DEL	qParEGO	qEHVI	DEL	qParEGO	qEHVI
ZINC	β 二	=0.1 → 0.4	243	-46-	45	243 (100%)	-0(0%)-	-0(0%)-
ZINC	β =	0.01 → 0.4	200	46	40	200 (100%)	0(0%)	1 (2.5%)
PCBA	F	二0.1 ― 0.4	228	-46-	36	228 (100%)	-0(0%)-	-0(0%)-
PCBA	β =	0.01 — 0.4	183	46	43	183 (100%)	1(2.17%)	2 (4.65%)
3.4 Ablation Studies
By default, DEL uses the ProPerty Predictor for latent rePresentation regularization, fine-tunes Frag-
VAE using new PoPulation data in each generation, forms new child latent codes using the linear
crossover method, and fixes the PoPulation size to 20K. Variants were created by (1) disabling the
ProPerty Predictor, (2) disabling finetuning, (3) using the discrete crossover method, and (4) allow-
ing a much larger PoPulation size (100K). While it is difficult to comPare these variants in terms of
validity, novelty and diversity of PoPulation samPles (see Figure 28 in APPendix), it turns out that
non-dominated sorting is an informative method for comParison. Table 4, in APPendix A.6, indi-
cates that DEL with the ProPerty Predictor outPerforms the variant without it. Table 5 shows that
FragVAE finetuning in DEL can helP obtain better Pareto fronts. Table 6 imPlies that both linear and
discrete crossover oPerations behave well in DEL. Also, DEL with larger PoPulation size can form
better Pareto fronts (see Table 7). Additionally, we aPPlied non-dominated sorting to comPare the
quality of Pareto fronts obtained using different values of β on DEL, and found that the integrated
Pareto front consists of samPles relatively evenly from all settings (Table 8).
4	Conclusion
In this PaPer, we Presented our DEL framework where a fragment-based VAE is integrated such
that evolutionary exPloration is conducted in the continuous latent rePresentation sPace rather than
the discrete structural sPace. Our intensive exPeriments show that DEL is able to generate novel
PoPulations of molecules with imProved ProPerties, and outPerforms state-of-the-art multi-objective
Bayesian oPtimization algorithms. APPlications of DEL are certainly not restricted to design of
small molecules. As future work, DEL will be tested on different datasets, other design Problems,
and more sPecific aPPlications. Other tyPes of DGMs and search strategies will be exPlored to
further enhance DEL. New MOBO algorithms for latent-sPace based oPtimization need tobe studied
further to address issues such as scalability, unknown invalid domains in latent sPace, and the curse
of dimensionality. Github link of our PyTorch and BoTorch imPlementation will be available.
8
Under review as a conference paper at ICLR 2021
References
TJ. Besnard, G.F. Ruda, V. Setola, K. Abecassis, R.M. Rodriguiz, X. Huang, S. Norval, M.F. Sas-
sano, A.I. Shin, L.A. Webster, F.R.C. Simeons, L. Stojanovski, A. Prat, N.G. Seidah, D.B. Con-
stam, G.R. Bickerton, K.D. Read, W.C. Wetsel, I.H. Gilbert, B.L. Roth, and A.L. Hopkins. Auto-
mated design of ligands to PolyPharmacological profiles. Nature, 412:215-220, 20l2.
G.R. Bickerton, G.V. Paolini, J. Besnard, S. Muresan, and A.L. Hopkins. Quantifying the chemical
beauty of drugs. Nature Chemistry, 4:90-98, 2012.
S	.R. Bowman, L. Vilnis, O. Vinyals, A.M. Dai, R. Jozefowicz, and S. Bengio. Generating sentences
from a continuous sPace. In SIGNLL Conference on Computational Natural Language Learning
(CoNLL), PP. 10-21, 2016.
H. Chen, O. Engkvist, Y. Wang, M. Olivecrona, and T. Blaschke. The rise of deeP learning in drug
discovery. Drug Discovery Today, 23(6):1241-1259, 2018.
E.D. Cubuk, B. ZoPh, D. Mane, V. Vasudevan, and Q.V. Le. AutoAugment: Learning augmentation
strategies from data. In CVPR, 2019.
S. Daulton, M. Balandat, and E. Bakshy. Differentiable exPected hyPervolume imProvement for
Parallel multi-objective Bayesian oPtimization. arXiv Preprint, PP. arXiv:2006.05078, 2020.
K. Deb, A. PrataP, S. Agarwal, and T. Meyarivan. A fast and elitist multiobjective genetic algorithm:
NSGA-II. IEEE Transactions on Evolutionary Computation, 6(2):182-197, 2002.
J. Degen, C. Wegscheid-Gerlach, A. Zaliani, and M. Rarey. On the art of comPiling and using
‘drug-like’ chemical fragment sPaces. ChemMedChem, 3(10):1503-1507, 2008.
D. Duvenaud, D. Maclaurin, J. Aguilera-IParraguirre, R. Bombarell, T. Hirzel, A. AsPuru-Guzik,
and R.P. Adams. Convolutional networks on graPhs for learning molecular fingerPrints. In NIPS,
2015.
A.E. Eiben and J. Smith. From evolutionary comPutation to the evolution of things. Nature, 521
(2014):476-482, 2015.
D.A. Erlanson. Introduction to fragment-based drug discovery. Topics in Current Chemistry, 317:
1-32, 2011.
A. Goyal, A. Sordoni, M. Cote, N.R. Ke, and Y. Bengio. Z-Forcing: Training stochastic recurrent
networks. In NIPS, 2017.
P.S. Gromski, A.B. Henson, J.M. Granda, and L. Cronin. How to exPlore chemical sPace using
algorithms and automation. Nature Review Chemistry, 3:119-128, 2019.
M. Hauschild and M. Pelikan. An introduction and survey of estimation of distribution algorithms.
Swarm and Evolutionary Computation, 1(3):111-128, 2011.
I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot, M. Botvinick, S. Mohamed, and A. Lerchner.
beta-VAE: Learning basic visual concePts with a constrained variational framework. In ICLR,
2017.
J.J. Irwin and B.K. Shoichet. ZINC - a free database of commercially available comPounds for
virtual screening. Journal of Chemical Information and Modeling, 45(1):177-182, 2005.
A. Kadurin, S. Nikolenko, K. Khrabrov, A. AliPer, and A. Zhavoronkov. druGAN: An advanced
generative adversarial autoencoder model for de novo generation of new molecules with desired
molecular ProPerties in silico. Molecular Pharmaceutics, 14:3098-3104, 2017.
D.P. Kingma and M. Welling. Auto-encoding variational Bayes. In International Conference on
Learning Representations, 2014.
G. Landrum. RDKit: OPen-source cheminformatics, 2006. URL http://www.rdkit.org.
9
Under review as a conference paper at ICLR 2021
X.Q. Lewell, D.B. Judd, S.P. Watson, and M.M. Hann. RECAP - retrosynthetic combinatorial
analysis procedure: a powerful new technique for identifying privileged molecular fragments with
useful applications in combinatorial chemistry. Journal of Chemical Information and Computer
Sciences, 38(3):511-522,1998.
X. Li, I. Kiringa, T. Yeap, X. Zhu, and Y. Li. Anomaly detection based on unsupervised disentangled
representation learning in combination with manifold learning. In International Joint Conference
on Neural Networks, 2020.
S.	Mandal, T.A. Anderson, J. Gottschlich, S. Zhou, and A. Muzahid. Learning fitness functions for
genetic algorithms. arXiv Preprint, pp. arXiv:1908.08783, 2019.
T.	Mikolov, K. Chen, G.S. Corrado, and J. Dean. Efficient estimation of word representations in
vector space. In International Conference on Learning Representations, 2013.
L.	Perez and J. Wang. The effectiveness of data augmentation in image classification using deep
learning. arXiv Preprint, pp. arXiv:1712.04621, 2017.
M.	Podda, D. Bacciu, and A. Micheli. A deep generative model for fragment-based molecule gener-
ation. In International Conference on Artificial Intelligence and Statistics, pp. 2240-2250, 2020.
M. Popova, O. Isayev, and A. Tropsha. Deep reinforcement learning for de novo drug design.
Science Advances, 4:eaap7885, 2018.
R. Romez-Bombarelli, J.N. Wei, D. Duvenaud, J.M. Hernarndez-Lobato, B. Sanchez-Lengeling,
D. Sheberla, J. Aguilera-Iparraguirre, T.D. Hirzel, R.P. Adams, and A. Aspuru-Guzik. Auto-
matic chemical design using a data-driven continuous representation of molecules. ACS Central
Science, 4:268-276, 2018.
T. Salimans, J. Ho, X. Chen, S. Sidor, and I. Sutskever. Evolution strategies as a scalable alternative
to reinforcement learning. arXiv Reprint, pp. arXiv:1703.03864, 2017.
R. Sennrich, B. Haddow, and A. Birch. Improving neural machine translation models with mono-
lingual data. In ACL, 2016.
C. Shorten and T.M. Khoshgoftaar. A survey on image data augmentation for deep learning. Journal
of Big Data, 6:60, 2019.
S.B. Shuker, P.J. Hajduk, R.P. Meadows, and S.W. Fesik. Discovering high-affinity ligands for
proteins: SAR by NMR. Science, 274(5292):11531-1534, 1996.
M. Simonovsky and N. Komodakis. GraphVAE: Towards generation of small graphs using varia-
tional autoencoders. In ICANN, pp. 412-422, 2018.
K.O. Stanley, J. Clune, J. Lehman, and R. Miikkulainen. Designing neural networks through neu-
roevolution. Nature Machine Intelligence, 1:24-30, 2019.
Y. Wang, S.H. Bryant, T. Cheng, J. Wang, A. Gindulyte, B.A. Shoemaker, P.A. Thiessen, S. He, and
J. Zhang. PubChem BioAssay: 2017 update. Nucleic Acids Research, 45(D1):D955-D963, 2016.
J. Wei and K. Zou. EDA: Easy data augmentation techniques for boosting performance on text
classification tasks. In EMNLP-IJCNLP, 2019.
D. Weininger. SMILES, a chemical language and information system. 1. introduction to methodol-
ogy and encoding rules. Journal of Chemical Information and Computer Sciences, 28(1):31-36,
1988.
D. Wierstra, T. Schaul, T. Glasmachers, Y. Sun, J. Peters, and J. Schmidhuber. Natural evolution
strategies. Journal of Machine Learning Research, 15(2014):949-980, 2014.
C. Yan, S. Wang, J. Yang, T. Xu, and J. Huang. Re-balancing variational autoencoder loss for
molecule sequence generation. arXiv Preprint, pp. arXiv:1910.00698, 2020.
J. You, B. Liu, R. Ying, V. Pande, and J. Leskovec. Graph convolutional policy network for goal-
directed molecular graph generation. In NeurIPS, 2018.
Z. Zhou, S. Kearnes, L. Li, R.N. Zare, and P. Riley. Optimization of molecules vis deep reinforce-
ment learning. Scientific Reports, 9:10752, 2019.
10
Under review as a conference paper at ICLR 2021
A Appendix
A.1 DEL Algorithm
Algorithm 1: DEL Algorithm
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
Inputs: X0, Y0 : training samples and corresponding properties;
Result: population of designed molecules; learned DGM;
for each evolutionary generation do
if first generation then
X = X0 ; Y = Y0;
Train DGM Using {X, Y} ;
X, Y = sUbset(X, Y, M) ;
original training data
else
I Train DGM using {X, Y};
end
// use all training
// a subset of M
// use population to
data to learn
examples from
further train
DGM
the
DGM
end
Z = Encoder(X ) ;
// obtain
r, F = nondominated_sort(Y);
frontiers
latent representations of samples in X
// r: non-domination ranks, F : Pareto
d = ComPUte_crowding_distance( Y, F);
Z0 = EvOP(Z, r, d) ;
recombination and mutation
X0 = Decoder(Z0) ;
// evolutionary operations: selection,
Y0 = get_property(X0);
// obtain properties
// generate novel
of the generated
samples
samples
using a simulator, e.g. RDKit
X, Y = form_new_population(X, Y, X0, Y0, M);
samples from the combination of {X, Y } and
// keep the top
{X0,Y0}
M good
end
Return X, Y , DGM
A.2 Bug Correction
In the VAE implementation by Podda et al. (2020), the incorrect use of view function makes the
forward flow of information messed up among different samples in a batch.
Listing 1: Wrong use of view function in the original VAE model in (Podda et al., 2020).
_, state = self .rnn(packed , state )
#	num-layers by batch by hidden-size —> batch by hidden-layer * hidden_Size
state = state . view (batch_size, self . hidden_size * self. hidden_layers)
mean = self . rnn2mean( state ) # mean: batch by latent-size
logvar = self .rnn2logv( state ) # logv : batch by latent-size
Listing 2: Our correction.
_, state = self .rnn(packed , state )
#	num-layers by batch by hidden-size —> batch by num-layers by hidden-size
state = state . transpose (1 ,0)
state = state . flatten (start_dim = 1) # batch by hidden-layer * hidden-size
mean = self . rnn2mean( state ) # mean: batch by latent-size
logvar = self . rnn2logv ( state ) # logv : batch by Iatent_SiZe
A.3 Hyperparameter Setting for FragVAE
When not specified in the main text, the following values of hyperparameters are used in experi-
ments.
•	size of the embedding layer: 128
11
Under review as a conference paper at ICLR 2021
•	mask low-frequency fragments: Yes
•	masking frequency: 2
•	number of low frequency clusters: 5
•	window for word2vec embedding: 3
•	number of recurrent neurons per layer: 128
•	number of recurrent layers: 2
•	size of the VAE latent space: 64
•	maximum length of the sampled sequence: 10
•	batch size: 128
•	shuffle batches: Yes
•	number of epochs to train: 50
•	learning rate: 0.0005
•	dropout for the recurrent layers: 0.3
•	annealing step size for the scheduler: 4
•	annealing rate for the scheduler: 0.8
•	threshold to clip the gradient norm: 5
•	number of layers in MLP for properties: 2
•	number of hidden units in each hidden layer of MLP: 64
•	weight on property regression loss (α): 1
•	weight on KL divergence (β): {0.01, 0.1, 1}
A.4 Hyperparameter Setting for DEL
When not specified in the main text, the following values of hyperparameters are used in experi-
ments.
•	number of evolutionary generations: 10
•	popularization size: {20000 (default), 100000}
•	number of epochs in the initial training of DGM: 50
•	annealing step size for the scheduler in initial training of DGM: 4
•	annealing rate for the scheduler: 0.8
•	number of epochs in a subsequent training of DGM: 30
•	learning rate in the initial training of DGM: 0.0005
•	annealing step size for the scheduler in initial training: 2
•	probability in tournament selection: 0.95
•	crossover method: {linear (default), discrete}
•	mutation rate: 0.01
A.5 Hyperparameter Setting for MOBO
When not specified in the main text, the following values of hyperparameters are used in experi-
ments.
•	number of initial data points: 1000
•	number of batches: 30
•	batch size: 8
•	number of MC samples: 128
A.6 Additional Tables
12
Under review as a conference paper at ICLR 2021
Table 2: Performance of FragVAE in comparison with existing methods.
Model	Data	Validity (SMILES)	Validity (Fragments)	Novelty	Diversity
-ChemVAE	ZINC	0.170	-	0.980	-0.310
GrammarVAE	ZINC	0.310	-	1.000	0.108
SDVAE	ZINC	0.435	-	-	-
GraphVAE	ZINC	0.140	-	1.000	0.316
CGVAE	ZINC	1.000	-	1.000	0.998
NeVAE	ZINC	1.000	-	0.999	1.000
FragVAE (β=1.00)	ZINC	1.000	0.922	1.000	-0.961
FragVAE (β=0.10)	ZINC	1.000	0.953	0.999	0.985
FragVAE (β=0.01)	ZINC	1.000	0.655	0.997	0.809
FragVAE (β=1.00)	PCBA	1.000	0.443	1.000	-0.925
FragVAE (β=0.10)	PCBA	1.000	0.481	0.983	0.886
FragVAE (β=0.01)	PCBA	1.000		0.777		0.997	0.632
Table 3: Running time of DEL and MOBO. Since both methods involve training FragVAE, the
FragVAE training time was not counted in this table. Format: Hours:Minutes:Seconds. Note: when
β is annealed to 0.4 in DEL, α is annealed from 1 to 4. All experiments were carried out on a Dell
Precision 5820 Workstation equipped with an Intel Xeon W-2255 CPU (10C), RAM of 128GB, and
a Nvidia Quadro RTX 6000 (24GB) GPU.
Data	DEL Hyperparameter	DEL	MOBO
ZINC	β = 0.1 7 0.4	3:03:49	25:23:01
ZINC	β = 0.01 → 0.4	3:08:09	141:49:37
PCBA	β = 0.1 7 0.4	3:01:32	31:16:18
PCBA	β = 0.01 → 0.4	1:50:20	45:31:38
Table 4: Non-dominated sorting of combination of Pareto fronts from DEL with and without prop-
erty prediction (PP) component as latent space regularization.
		Pareto Front Size for Comparison		In New Pareto Front	
Data	DEL Parameter	With PP	Without PP	With PP	Without PP
ZINC PCBA	β = 0.1 β = 0.1	-235- 224	234 	108		206 (87.66%) 216 (96.43%)	-0(0%)- 0(0%)
Table 5: Non-dominated sorting of combination of Pareto fronts from DEL with and without DGM
finetuning (FT) phrase.
			Pareto Front Size for Comparison		In New Pareto Front	
Data	DEL Parameter		With FT	Without FT	With FT	Without FT
ZINC	β 二	0.1	-235-	193	235 (100%)	-070%)-
PCBA	_上	0.1	224		214		199 (88.84%)	0(0%)
Table 6: Non-dominated sorting of combination of Pareto fronts from DEL with linear or discrete
crossover operation. Note: when β is annealed to 0.4, α is annealed from 1 to 4.
		Pareto Front Size for Comparison		In New Pareto Front	
Data	DEL Parameter	Linear	Discrete	Linear	Discrete
ZINC PCBA	β = 0.01 7 0.4 β = 0.01 → 0.4	200 183	181 	195		196 (98.00%) 146 (79.78%)	173 (95.58%) 184 (94.36%)
A.7 Additional Figures
13
Under review as a conference paper at ICLR 2021
Table 7: Non-dominated sorting of combination of Pareto fronts from DEL with population sizes of
20K and 100K. Note: when β is annealed to 0.4, α is annealed from 1 to 4.
		Pareto Front Size for Comparison		In New Pareto Front	
Data	DEL Parameter	20K	100K	20K	100K
ZINC PCBA	β = 0.1 → 0.4 β = 0.1 7 0.4	243 228	3Γ6 	354		78 (32.10%) 88 (38.60%)	304 (96.20%) 334 (94.35%)
Table 8: Non-dominated sorting of combination Pareto fronts from DEL with different values of β.
Note: when β is annealed to 0.4, α is annealed from 1 to 4. Population size: 20K.
	Pareto Front Size for Comparison				In New Pareto Front			
Data	β = 0.1	β = 0.1 → 0.4	β = 0.01	β = 0.01 → 0.4	β = 0.1	β = 0.1 → 0.4	β = 0.01	β = 0.01 → 0.4
ZINC PCBA	235 224	243 228	-228- 189	200 183	195(82.98%) 169(75.45%)	186(76.54%) 183(80.26%)	170(74.56%) 122(64.55%)	143(71.50%) 111(60.66%)
Table 9: Numbers of novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1
in the 1st, 5th and final (10th) generations of DEL. Numbers of training molecules satisfying same
conditions are also given. Note: when β is annealed to 0.4, α is annealed from 1 to 4.
Data	Hyperparameter	Train	DEL(1)	DEL(5)	DEL(F)
ZINC	β=0.1	406	T	14^	40^
ZINC	β=0.1 →0.4	-	2	22	32
ZINC	β=0.01	-	6	54	115
ZINC	β=0.01 →0.4	-	3	9	20
PCBA	β=0.1	196	4	13	27
PCBA	β=0.1 →0.4	-	10	33	52
PCBA	β=0.01	-	0	8	16
PCBA	β=0.01 →0.4	-		4		9		10
Figure 6: Different of forms of the β-function. The total number of epochs is T = 100.
14
Under review as a conference paper at ICLR 2021
MSE Loss	CE Loss	MSE Loss	CE Loss
4.0
3.5
3.0
1.5
1.0
(b) PCBA.
5.0
4.5
30
25
20
15
LO
5
0
(a) ZINC.
30
25
20
15
10
5
0
0.5
0.0
Figure 7: Loss of FragVAE in initial training on ZINC and PCBA respectively.
15
Under review as a conference paper at ICLR 2021
—3 = 0-l><ι = l
β = 0.1 → OAa = 1→4
—B = OQLa=I
—β = 0.01→0.4,a = l→4
O 50 IOO 150	200	250
4.0
3.5
3.0
2.5
2.0
O 50 IOO 150	200	250
Epoch
(a) ZINC.
Sso-I 山。
^ = 0.1,α = l
^ = 0.1→0.4,a = l→4
・6 = 0.0La=I
β = 0.01→0.4,σ = l→4
。 5 Q 5 Q
3 2 2 1 1
Sso-I -Dl
Epoch
O 50 IOO 150	200	250
O 50 IOO 150	200	250
9
B
7
S 6
S
5 5
4
3
2
O 50 IOO 150	200	250
0.40
0.35
0.30
0.25
g o.2o
0.15
0.10
0.05
0.00
O 50 IOO 150	200	250
O 50 IOO 150	200	250
Epoch
Epoch
(b) PCBA.
Figure 8: Loss of of FragVAE in DEL on ZINC and PCBA respectively.
16
Under review as a conference paper at ICLR 2021
Figure 9: ZINC training molecules that satisfy properties QED≥ 0.88, SAS≤ 3, and logP≤ 1. Note:
196 molecules satisfy these conditions, but 64 are visualized.
17
Under review as a conference paper at ICLR 2021
Figure 10: PCBA training molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1.
Note: 406 molecules satisfy these conditions, but 64 are visualized.
18
Under review as a conference paper at ICLR 2021
Figure 11: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on ZINC with hyperparameter: β = 0.1, α = 1.
Figure 12: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on ZINC with hyperparameter: β = 0.1 → 0.4, α = 1 → 4.
19
Under review as a conference paper at ICLR 2021
Figure 13: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on ZINC with hyperparameter: β = 0.01, α = 1.
20
Under review as a conference paper at ICLR 2021
Figure 14: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on ZINC with hyperparameter: β = 0.01 → 0.4, α = 1 → 4.
Figure 15: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on PCBA with hyperparameter: β = 0.1, α = 1.
21
Under review as a conference paper at ICLR 2021
Figure 16: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on PCBA with hyperparameter: β = 0.1 → 0.4, α = 1 → 4.
Figure 17: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on PCBA with hyperparameter: β = 0.01, α = 1.
22
Under review as a conference paper at ICLR 2021
Figure 18: Novel molecules that satisfy properties QED ≥ 0.88, SAS ≤ 3, and logP ≤ 1 in the final
generation of DEL trained on PCBA with hyperparameter: β = 0.01 → 0.4, α = 1 → 4.
ATOMS
■ C ・F ・N ・O e Other
BONDS
■ SINGLE ■ DOUBLE ■ TRIPLE
RINGS
■ Tri HQuad BPent -Hex
ZINC
DEUl)
DEU5)
DEL(F)
0.2
0.0
ZINC
DEUl)
DEU5)
DEUF)
0.00 0.25 0.50 0.75 1.00	2
QED
4	6	8	-10	-5	0	5	10
SAS	LOGP
ZINC DEL(I) DEL(5) DEL(F)
ZINC DEL(I) DEL(5) DEL(F)
PCBA
DEUl)
2 DEL(5)
— DEL(F)
O
(a)	Property distributions over ZINC.
(b)	Structural feature distributions over ZINC.
ATOMS
■ C ・F ・N ・O e Other
BONDS
■ SINGLE ■ DOUBLE ■ TRIPLE
RINGS
■ Tri HQuad BPent -Hex
0.00 0.25 0.50 0.75 1.00
0.5
0.0
PCBA
DEUl)
DEU5)
DEL(F)
PCBA
DEUl)
DEU5)
DEUF)
QED
2.5	5.0	7.5	10.0	-10	-5 O 5	10
SAS	LOGP
20
10
PCBA DEL(I) DEL(5) DEL(F)
2
PCBA DEL(I) DEL(5) DEL(F)

O
(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 19:	Property and structural feature distributions of population samples during DEL (β = 0.1).
ATOMS
■ C ・F ・N -0 ■ Other
BONDS	RINGS
■ SINGLE ■ DOUBLE ■ TRIPLE	≡Tri HQuad -Pent -Hex
0.00 0.25 0.50 0.75 1.00
QED
r ZINC
DEUD
DEU5)
— DEL(F)
0.2
0.0
ZINC
DEUl)
DEU5)
DEUF)
2	4	6	8	-10	-5 O 5
SAS	LOGP
10
ZINC DEL(I) DEL(5) DEL(F) ZINC DEL(I) DEL(5) DEL(F)
ZINC DEL(I) DEL(5) DEL(F)

(a)	Property distributions over ZINC.
(b)	Structural feature distributions over ZINC.
ATOMS
∣C ・F nN ・O -Other
BONDS
■ SINGLE ■ DOUBLE ■ TRIPLE
RINGS
■ Tri HQuad BPent -Hex
0.00 0.25 0.50 0.75 1.00
PCBA
DEUl)
DEU5)
DEL(F)
0.2
0.0
PCBA
DEUl)
DEU5)
DEUF)
QED
2.5	5.0	7.5	10.0	-10	-5	0	5	10
SAS	LOGP
PCBA DEL(I) DEL(5) DEL(F)
(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 20:	Property and structural feature distributions of population samples during DEL (β =
0.1 → 0.4).
23
Under review as a conference paper at ICLR 2021
4
2
0
ZINC
—DEL(I)
-DEL(5)
-DEL(F)
0.4
0.2
0.00 0.25 0.50 0.75 1.00
QED
0.0
ZIMC
DELa)
-DEU5)
— DEL(F)
4 β
SAS
0.2
ZINC
—DEL(I)
-DEL(5)
-DEL(F)
0.0 ——
8 -10	-5	0	5
LOGP
10
(a)	Property distributions over ZINC.
-PCBA
—DEL(I)
-DEL(5)
-DEL(F)
0.00 0.25 0.50 0.75 1.00
QED
1.0
0.5
0.0
ATOMS
■ C F nN -0 BOther
ZINC DEL(I) DEL(5) DEL(F)
BONDS
■ SINGLE ■ DOUBLE B7HIPLE
B ■ ■ ■
Illl
ZINC DEL(I) □EL(5) DEL(F)
RINGS
■ Tri Quad ■ Pent ■ Hex
2
1
0
ZINC DEL(I) DEL(5) DEL(F)
(b)	Structural feature distributions over ZINC.
ATOMS	BONDS
■ C	F □N -O BOther
■ SINGLE ■ DOUBLE B7HIPLE
PCBA DEL(I) DEL(5) DEL(F) PCBA DEL(I) □EL(5) DEL(F)
RINGS
■ Tri Quad ■ Pent ■ Hex
PCBA DEL(I) DEL(5) DEL(F)
2


(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 21:	Property and structural feature distributions of population samples during DEL (β
0.01).
ATOMS
■ C F □N -O BOther
BONDS
■ SINGLE ■ DOUBLE B7HIPLE
0.00 0.25 0.50 0.75 1.00
QED
一ZINC
DEL(I)
DEL(5)
—DEUF)
ZINC
—DEL(I)
-DEL(5)
-DEL(F)
2	4 β 8 -10	-5	0	5	10
2
1
0
RINGS
■ Tri Quad ■ Pent ■ Hex
ZINC DEL(I) DEL(5) DEL(F)
SAS
LOGP
ZINC DEL(I) DEL(5) DEL(F)
ZINC DEL(I) □EL(5) DEL(F)

(a)	Property distributions over ZINC.
(b)	Structural feature distributions over ZINC.
BONDS
■ SINGLE ■ DOUBLE B7HIPLE
RINGS
■ Tri Quad ■ Pent ■ Hex
-PCBA
—DEL(I)
-DEL(5)
-DEL(F)
QED
1.0
0.5
0.0
0.00 0.25 0.50 0.75 1.00
ATOMS
■ C F □N -O BOther
20
0
PCBA DEL(I) DEL(5) DEL(F)
■ ■ ■ ■
Iiii
PCBA DEL(I) □EL(5) DEL(F)
Ilil
— — — ■
PCBA DEL(I) DEL(5) DEL(F)
(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 22:	Property and structural feature distributions of randomly sampled molecules using Frag-
VAE during DEL (β = 0.1).
ZINC
—DEL(I)
-DEL(5)
-DEL(F)
2	4	6	-10	-5	0	5
SAS	LOGP
10
ATOMS
■ C F □N -O BOther
BONDS
■ SINGLE ■ DOUBLE B7HIPLE
ZINC DEL(I) DEL(5) DEL(F) ZINC DEL(I) □EL(5) DEL(F)
RINGS
■ Tri Quad ■ Pent ■ Hex
ZINC DEL(I) DEL(5) DEL(F)
(a)	Property distributions over ZINC.
(b)	Structural feature distributions over ZINC.
ATOMS	BONDS
-PCBA
—DEL(I)
-DEL(5)
-DEL(F)
QED
1.0
0.5
0.0
0.00 0.25 0.50 0.75 1.00
■ C	F □N -O BOther
■ SINGLE ■ DOUBLE B7HIPLE
RINGS
■ Tri Quad ■ Pent ■ Hex
PCBA DEL(I) DEL(5) DEL(F)
(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 23:	Property and structural feature distributions of randomly sampled molecules using Frag-
VAE during DEL (β = 0.1 → 0.4).
24
Under review as a conference paper at ICLR 2021
QED
0.2
—ZINC
DELa)
DEL(5)
-DEUFJ
ZINC
DEL(1J
一血⑸
-DELfFJ
SAS
LOGP
RINGS
■ Tri QUad ■ Pent ■ Hex
BONDS
■ SINGLE - DOUBLE ■ THIPLE
ATOMS
ɪe -F HN ≡0 -Other
(a)	Property distributions over ZINC.
(b)	Structural feature distributions over ZINC.
5.0
2.5
0.0
PCBA
DEL(I)
—DEL(5)
—DEL(F)
1.0
0.5
0.0
PCBA
DEL(I)
—DEL(5)
-DEL(F)
0.4
0.2
0.00 0.25 0.50 0.75 1.∞
0.0
-PCBA
DEL(I)
—DEL(5)
-DEL(F)
QED
2.5	5.0	7.5 10.0	-10	-5	0	5	10
SAS
LOGP
RINGS
■ Tri QUad ■ Pent ■ Hex
BONDS
■ SINGLE - DOUBLE ■ THIPLE
ATOMS
ɪe -F HN ≡O -Other
(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 24:	Property and structural feature distributions of randomly sampled molecules using Frag-
VAE during DEL (β = 0.01).
ZINC
—DEL(I)
-DEL(5)
-DEL(F)
2	4	6	-10	-5	0	5	10
SAS	LOGP
ATOMS
■ C F □N -O BOther
RINGS
■ Tri Quad ■ Pent ■ Hex
ZINC DEL(I) DEL(5) DEL(F)
(a)	Property distributions over ZINC.
(b)	Structural feature distributions over ZINC.
2
0
-PCBA
—DEL(I)
-DEL(5)
-DEL(F)
0.00 0.25 0.50 0.75 1.00
QED
1.0
0.5
0.0
ATOMS
■ C F □N -O BOther
PCBA DEL(I) DEL(5) DEL(F)
BONDS
■ SINGLE ■ DOUBLE B7HIPLE
PCBA DEL(I) □EL(5) DEL(F)
RINGS
■ Tri Quad ■ Pent ■ Hex
PCBA DEL(I) DEL(5) DEL(F)

(c) Property distributions over PCBA.
(d) Structural feature distributions over PCBA.
Figure 25:	Property and structural feature distributions of randomly sampled molecules using Frag-
VAE during DEL (β = 0.01 → 0.4).
25
Under review as a conference paper at ICLR 2021
78.85
78.50
78.45
79.4
79.2
Q.8
9.&
7 7
SEn-OAJSdAH
78.6
78.4
0	5	10	15	20	25	30
0	5	10	15	20	25	30
Batch
Batch
⑶ ZINC (β = 0.1).
(b) ZINC (β = 0.01).
① EnOAJ ① dAH
190.0
189.9
189.8
189.7
189.6
189.5
Sobol
—q ParEGO
—qEHVI
25	30
0	5	10	15	20	25	30
Batch
(c) PCBA (β = 0.1).
0	5	10	15	20
Batch
(d) PCBA (β = 0.01).
Figure 26: Hypervolume along batches when running Sobol random search, qParEGO, and qEHVI.
• DEL Front 1
• DEL Front 2
DEL Front 3
• DEL Front 4
• DEL Front 5
2 4
SAS
0-2
0.0
4
6
一
10
-4
0
IogP
0.4
QED
• qEHVI Front 1
* qEHVI Front 2
.qEHVI Front 3
qEHVI Front 4
qEHVI Front 5
• qPar∈GO Front 1
♦	qParEGO Front 2
♦	qParEGO Front 3
qParEGO Front 4
qParEGO Front 5
-DEL Front 1
• DEL Front 2
DEL Front 3
• DEL Front 4
♦ DEL Front 5
•	qEHVI Front 1
. qEHVI Front 2
.qEHVI Front 3
qEHVI Front 4
qEHVI Front 5
•	qPar∈GO Front 1
♦	qParEGO Front 2
♦	qParEGO Front 3
qParEGO Front 4
qParEGO Front 5
QED
⑶ ZINC.
(b) PCBA.
Figure 27: Pareto fronts of DEL and MOBO algorithms (β = 0.1). In the legend, the last batch of
qParEGO or qEHVI is written as Front 1.
26
Under review as a conference paper at ICLR 2021
Data	Property Predictor	Finetune DGM	Crossover	Population Size	beta	alpha	Evolutionary Generation	Validity (SMILES)	Validity (Fragments)	Novelty	Diversity
ZINC	Y	Y	Iinear	20K	0.10	1	1	1.000	0.873 ―	0.997	0.988
	Y	Y	Iinear	20K	0.10	1	5	1.000	0.961 ―	0.999	0.921
	Y	Y	Iinear	20K	0.10	1	10	1.000	0.554 ―	1.000	0.805
	N	Y	Iinear	20K	0.10	1	1	1.000	0.986 ―	1.000	0.960
	N	Y	Iinear	20K	0.10	1	5	1.000	0.539 ―	1.000	0.989
	N	Y	Iinear	20K	0.10	1	10	1.000	0.971 ―	1.000	0.920
	Y	N	Iinear	20K	0.10	1	1	1.000	0.936 ―	0.999	0.977
	Y	N	Iinear	20K	0.10	1	5	1.000	0.947 ―	0.999	0.978
	Y	N	Iinear	20K	0.10	1	10	1.000	0.948 ―	0.999	0.975
	Y	Y	Iinear	20K	0.1 to 0.4	1 to 4	1	1.000	0.957 ―	0.999	0.986
	Y	Y	Iinear	20K	0.1 to 0.4	1 to 4	5	1.000	0.259 ―	0.999	0.986
	Y	Y	Iinear	20K	0.1 to 0.4	1 to 4	10	1.000	0.396 ―	0.996	0.986
	Y	Y	Iinear	100K	0.1 to 0.4	1 to 4	1	1.000	0.706 ―	0.996	0.880
	Y	-Y	linear	-100K—	0.1 to 0.4	1 to 4	5	-1.000-	0.432	1.000	0.334
	Y	-Y	linear	-100K—	0.1 to 0.4	1 to 4	10	-1.000-	0.591	0.999	0.436
	Y	Y	linear	20K	0.01	1	1	1.000	0.819 ―	0.995	0.956
	Y	Y	Iinear	20K	0.01	1	5	1.000	0.945 ―	0.998	0.958
	Y	Y	Iinear	20K	0.01	1	10	1.000	0.973 ―	0.999	0.963
	Y	Y	Iinear	20K	0.01 to 0.4	1 to 4	1	1.000	0.917 ―	0.996	0.911
	Y	Y	Iinear	20K	0.01 to 0.4	1 to 4	5	1.000	0.940 ―	0.999	0.978
	Y	-Y	linear	-20K-	0.01 to 0.4	1 to 4	10	-1.000-	0.919	1.000	0.976
	Y	-Y	discrete	-20K-	0.01 to 0.4	1 to 4	1	-1.000-	0.888	0.997	0.887
	Y	-Y	discrete	-20K-	0.01 to 0.4	1 to 4	5	-1.000-	0.974	0.995	0.934
	Y	-Y	discrete	-20K-	0.01 to 0.4	1 to 4	10	-1.000-	0.966	0.997	0.904
PCBA	Y	-Y	linear	-20K-	010	-1 ~	1	-1.000-	0.513	0.994	0.976
	Y	-Y	linear	-20K-	010	-1 ~	5	-1.000-	0.787	0.999	0.981
	Y	-Y	linear	-20K-	010	-1 ~	10	-1.000-	0.839	0.993	0.976
	N	-Y	linear	-20K-	010	-1 ~	1	-1.000-	0.467	1.000	0.987
	N	-Y	linear	-20K-	010	-1 ~	5	-1.000-	0.983	0.999	0.912
	N	Y	linear	20K	0.10	1	10	1.000	0.907	1.000	0.818
	Y	N	Iinear	20K	0.10	1	1	1.000	0.453	1.000	0.990
	Y	N	Iinear	20K	0.10	1	5	1.000	0.444	1.000	0.989
	Y	N	Iinear	20K	0.10	1	10	1.000	0.433	1.000	0.990
	Y	-Y	linear	-20K-	0.1 to 0.4	1 to 4	1	-1.000-	0.527	0.980	0.935
	Y	-Y	linear	-20K-	0.1 to 0.4	1 to 4	5	-1.000-	0.984	0.994	0.918
	Y	-Y	linear	-20K-	0.1 to 0.4	1 to 4	10	-1.000-	0.694	0.999	0.915
	Y	-Y	linear	-100K—	0.1 to 0.4	1 to 4	1	-1.000-	0.783	0.996	0.799
	Y	-Y	linear	-100K—	0.1 to 0.4	1 to 4	5	-1.000-	0.808	1.000	0.897
	Y	-Y	linear	-100K—	0.1 to 0.4	1 to 4	10	-1.000-	0.526	1.000	0.896
	Y	-Y	linear	-20K-	001	-1 ~	1	-1.000-	0.301	0.997	0.417
	Y	-Y	linear	-20K-	001	-1 ~	5	-1.000-	0.912	1.000	0.655
	Y	-Y	linear	-20K-	001	-1 ~	10	-1.000-	0.762	1.000	0.798
	Y	-Y	linear	-20K-	0.01 to 0.4	1 to 4	1	-1.000-	0.849	0.998	0.633
	Y	-Y	linear	-20K-	0.01 to 0.4	1 to 4	5	-1.000-	0.827	1.000	0.863
	Y	-Y	linear	-20K-	0.01 to 0.4	1 to 4	10	-1.000-	0.755	1.000	0.895
	Y	-Y	discrete	-20K-	0.01 to 0.4	1 to 4	1	-1.000-	0.670	0.996	0.318
	Y	-Y	discrete	-20K-	0.01 to 0.4	1 to 4	5	-1.000-	0.838	0.998	0.566
	Y	Y	discrete	20K	0.01 to 0.4	1 to 4	10	1.000	0.869 ―	0.996	0.568
Figure 28: Validity, novelty and diversity of population samples (after evolutionary operations and
before merging with previous population) in different variants of DEL.
27