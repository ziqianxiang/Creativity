Under review as a conference paper at ICLR 2021
Deep Graph Neural Networks with
Shallow Subgraph Samplers
Anonymous authors
Paper under double-blind review
Ab stract
While Graph Neural Networks (GNNs) are powerful models for learning repre-
sentations on graphs, most state-of-the-art models do not have significant accu-
racy gain beyond two to three layers. Deep GNNs fundamentally need to address:
1). expressivity challenge due to oversmoothing, and 2). computation challenge
due to neighborhood explosion. We propose a simple “deep GNN, shallow sam-
pler” design principle to improve both the GNN accuracy and efficiency — to gen-
erate representation of a target node, we use a deep GNN to pass messages only
within a shallow, localized subgraph. A properly sampled subgraph may exclude
irrelevant or even noisy nodes, and still preserve the critical neighbor features and
graph structures. The deep GNN then smooths the informative local signals to
enhance feature learning, rather than oversmoothing the global graph signals into
just “white noise”. We theoretically justify why the combination of deep GNNs
with shallow samplers yields the best learning performance. We then propose
various sampling algorithms and neural architecture extensions to achieve good
empirical results. Experiments on five large graphs show that our models achieve
significantly higher accuracy and efficiency, compared with state-of-the-art.
1	Introduction
Graph Neural Networks (GNNs) have now become the state-of-the-art models for graph mining
(Wu et al., 2020; Hamilton et al., 2017b; Zhang et al., 2019), facilitating applications such as social
recommendation (Monti et al., 2017; Ying et al., 2018; Pal et al., 2020), knowledge understanding
(Schlichtkrull et al., 2018; Park et al., 2019; Zhang et al., 2020) and drug discovery (Stokes et al.,
2020; Lo et al., 2018). With the numerous architectures proposed (Kipf & Welling, 2016; Hamilton
et al., 2017a; VeliCkovic et al., 2018), it still remains an open question how to effectively design deep
GNNs. There are two fundamental obstacles that are intrinsic to the underlying graph structure:
•	Expressivity challenge: deep GNNs tend to oversmooth (Li et al., 2018). They collapse embed-
dings of different nodes into a fixed low-dimensional subspace after repeated neighbor mixing.
•	Computation challenge: deep GNNs recursively expand the adjacent nodes along message pass-
ing edges. The neighborhood size may grow exponentially with model depth (Chen et al., 2017).
Due to oversmoothing, one of the most popular GNN architectures, Graph Convolutional Network
(GCN) (Kipf & Welling, 2016), has been theoretically proven as incapable of scaling to deep layers
(Oono & Suzuki, 2020; Rong et al., 2020; Huang et al., 2020). Remedies to overcome the GCN
limitations are two-folded. From the neural architecture perspective, researchers are actively seeking
for more expressive neighbor aggregation operations (VeIickovic et al., 2018; Hamilton et al., 2017a;
Xu et al., 2018a), or transferring design components (such as residual connection) from deep CNNs
to GNNs (Xu et al., 2018b; Li et al., 2019; Huang et al., 2018). From the data perspective, various
works (Klicpera et al., 2019a;b; Bojchevski et al., 2020) revisit classic graph analytic algorithms to
reconstruct a graph with nicer topological property. The two kinds of works can also be combined
to jointly improve the quality of message passing in deep GNNs.
All the above GNN variants take a “global” view on the input graph G (V, E) — i.e., all nodes are
considered as belonging to the same G, whose size can often be massive. To generate the node
embedding, no matter how we modify the architecture and the graph structure, a deep enough GNN
1
Under review as a conference paper at ICLR 2021
would always propagate the influence from the entire node setV into a single target node. Intuitively,
for a large graph, most nodes in V barely provide any useful information to the target nodes. We thus
regard such “global view” on G as one of the root causes for both the expressivity and computation
challenges discussed above. In this work, for the node embedding task, we take an alternative “local
view” and interpret the GNN input as V = Sv∈V V[v] and E = Sv∈V E[v]. In other words, each
target node v belongs to some small graph G[v] capturing the characteristics of only the node v. The
entire input graph G is observed as the union of all such local yet latent G[v]. Such simple global-to-
local switch of perspective enables us to address both the expressivity and computation challenges
without resorting to alternative GNN architectures or reconstructing the graph.
Present work: SHADOW-GNN. We propose a “Deep GNN, shallow sampler” design principle that
helps improve the expressive power and inference efficiency of various GNN architectures. We break
the conventional thinking that an L-layer (deep) GNN has to aggregate L-hop (faraway) neighbors.
We argue that the GNN receptive field for a target node should be shallower than the GNN depth.
In other words, an L-layer GNN should only operate on a small subgraph G[v] surrounding the
target node v, where G[v] consists of (part of) the L0-hop neighborhood. The deep vs. shallow
comparison is reflected by setting L0 < L. We name such a GNN on G[v] as a S HADOW- GNN.
We justify our design principle from two aspects. Firstly, why do we need the neighborhood to be
shallow? As a motivating example, the average number of 4-hop neighbors for the ogbn-products
graph (Hu et al., 2020) is 0.6M, corresponding to 25% of the full graph size. Blindly encoding the
0.6M node features into a single embedding vector can create the “information bottleneck” (Alon
& Yahav, 2020). The irrelevant information from the majority of the 0.6M nodes may also “dilute”
the truly useful signals from a small set of close neighbors. A simple solution to the above issues is
to manually create a shallow neighborhood by subgraph sampling. The second question regarding
SHADOW-GNN is: why do we still need deep GNNs? Using more number of layers than the number
of hops means the same pair of nodes may exchange messages with each other multiple times.
Intuitively, this helps the GNN better absorb the subgraph information. Theoretically, we prove that a
GNN deeper than the hops of the subgraph can be more powerful than the 1-dimensional Weisfeiler-
Lehman test (Shervashidze et al., 2011). A shallow GNN, on the contrary, cannot accurately learn
certain simple functions such as unweighted mean of the shallow neighborhood features. Note
that with GCN as the backbone, a shaDow-GCN still performs signal smoothing in each layer.
However, the important distinction is that a deep GCN smooths the full G regardless of the target
node, while a SHADOW-GCN constructs a customized smoothing domain G[v] for each target v .
The variance in those smoothing domains created by shaDow-GCN encourages variances in the
node embedding vectors. With such intuition, our analysis shows that shaDow-GNN does not
oversmooth. Finally, since the sizes of the shallow neighborhoods are independent of the GNN
depth, the computation challenge due to neighbor explosion is automatically addressed.
We propose various subgraph samplers for SHADOW-GNN, including the simplest k-hop sampler
and a sampler based on personalized PageRank, to improve the inference accuracy and computation
efficiency. By experiments on five standard benchmarks, our shaDow-SAGE and shaDow-GAT
models achieve significant accuracy gains compared with the original GraphSAGE and GAT models.
In the meantime, the inference cost is reduced by orders of magnitude.
2	Related Work and Preliminaries
Deep GNNs. Recently, numerous GNN models (Kipf & Welling, 2016; Defferrard et al., 2016;
Hamilton et al., 2017a; Velickovic et al., 2018; XU et al., 2018b;a) have been proposed. In general,
the input to a GNN is the graph G, and the outputs are representation vectors for each node, cap-
turing both the feature and structural information of the neighborhood. Most state-of-the-art GNNs
use shallow models (i.e., 2 to 3 layers). As first proposed by Li et al. (2018) and further elaborated
by Luan et al. (2019); Oono & Suzuki (2020); Zhao & Akoglu (2020); Huang et al. (2020), one of
the major challenges to deepen GNNs is the “oversmoothing” of node features — each layer aggre-
gation pushes the neighbor features towards similar values. Repeated aggregation over many layers
results in node features being averaged over the full graph. A deep GNN may thus generate indistin-
guishable embeddings for different nodes. Viewing oversmoothing as a limitation of the layer aggre-
gation, researchers develop alternative architectures. AS-GCN (Huang et al., 2018), DeepGCN (Li
et al., 2019) and JK-net (Xu et al., 2018b) use skip-connection across layers. MixHop (Abu-El-Haija
et al., 2019), Snowball (Luan et al., 2019) and DAGNN (Liu et al., 2020) enable multi-hop message
2
Under review as a conference paper at ICLR 2021
passing within a single layer. GraphSAGE (Hamilton et al., 2017a) and GCNII (Ming Chen et al.,
2020) encourage self-to-self message passing which effectively form an implicit skip-connection.
GIN (Xu et al., 2018a) and DeeperGCN (Li et al., 2020a) propose more expressive neighbor aggre-
gation operations. All the above focus on architectural exploration, which is a research direction
orthogonal to ours. We can construct the shaDow version of these GNNs in a plug-and-play fash-
ion. Lastly, DropEdge (Rong et al., 2020) and Bayesian-GDC (Hasanzadeh et al., 2020) propose
regularization techniques by adapting dropout (Srivastava et al., 2014) to graphs. Such techniques
are only applied during training, and so oversmoothing during inference may not be alleviated.
Learning from structural information. Another line of research is to go beyond the layer-wise
message passing and more explicitly utilize the graph structural information (Wu et al., 2019;
Klicpera et al., 2019a; Bojchevski et al., 2020; Liu et al., 2020; Frasca et al., 2020; You et al., 2019;
Li et al., 2020b). In particular, APPNP (Klicpera et al., 2019a) and PPRGo (Bojchevski et al., 2020)
utilize the personalized PageRank (Page et al., 1999) algorithm to re-define neighbor connections
— instead of propagating features along the (noisy) graph edges, any nodes of structural signifi-
cance can directly propagate to the target node. Other related methods such as GDC (Klicpera et al.,
2019b) and AM-GCN (Wang et al., 2020) reconstructs the adjacency matrix in each GNN layer to
short-cut important multi-hop neighbors. Note that all the above methods takes a global view on
G and operate the neural networks on the full graph. On the other hand, the idea of using sub-
graph samples to improve the GNN efficiency has also been explored. For example, SEAL (Zhang
& Chen, 2018) extracts local k-hop enclosing subgraphs to perform link prediction. GraphSAINT
(Zeng et al., 2020) propose random walk samplers to construct minibatches during training.
Notations. We focus on the node classification task, although our design principle can be naturally
extended to other tasks. Let G (V, E, X) be an undirected graph, with node setV, edge setE ⊆ V ×V
and node feature matrix X ∈ R* l 2 3 4Vl×d. The u-th row of X corresponds to the length-d feature of
node u. Let A be the adjacency matrix of G where Au,v = 1 if edge (u, v) ∈ E and Au,v = 0
otherwise. Denote A as the adjacency matrix after symmetric normalization (used by GCN), and
Ab as the one after random walk normalization (used by GraphSAGE). Let subscript “[u]” mark the
quantities corresponding to a small subgraph surrounding node u. For example, the subgraph itself
is G[u]. For an L-layer GNN, let superscript "(')” denote the layer-' quantities (1 ≤ ' ≤ L). Let d(')
be the number of channels for layer '; H ('-1) ∈ RlVl×d('-1) and H (') ∈ RlVl×d(') be the input and
output feature matrices. Thus, H(0) = X and d(0) = d. Further, let Y = H(L). The operation ofa
layer can be abstracted as H(') = f (H('-1), A; W(')), where W(') are the learnable weights.
3	Deep GNN, Shallow S ampler
Algorithm 1 SHADOW-GNN inference algorithm
Input: G (V, E, X); Target nodes Vt; GNN model;
Output: Node embedding matrix Y for Vt ;
1: for v ∈ Vt do
2:	Get G[v] (V[v], E[v], X[v]) by SAMPLE on G
3:	Build L-layer GNN with layer operation f
4:	y - [fL (xM,A[v])]v,:
the composition of the function f for the L layers. Operation Hv : slices the v-th row of the matrix.
We discuss in Section 3.2 how to design SAMPLE to return informative shallow neighborhood.
“Deep GNN, shallow sampler” is a design
principle to improve the expressivity and effi-
ciency of GNNs without modifying the layer
architecture. Based on this principle, we
construct shaDow-GNN by subgraph sam-
pling. shaDow-GNN uses the same sam-
pling procedure during both training and in-
ference. The inference algorithm is shown in
Algorithm 1. In line 4, we use fL to denote
Notice that the normal GNN is a special kind of shaDow-GNN. Under the normal setup, the
GNN operates on the full graph G and the L layers propagate the influence from all the neighbors
up to L hops away from the target node. Such a GNN is equivalent to a SHADOW-GNN when
SAMPLE returns the full L-hop subgraph. However, following our principle, a good SAMPLE should
encourage most of V[v] to concentrate within the L0 -hop neighborhood, where L0 < L. Figures 6, 7
in Appendix show the difference in the neighborhood composition for normal GNN and shaDow-
GNN. Section 3.1 discusses the benefits of making the subgraph shallow and the GNN deep.
3
Under review as a conference paper at ICLR 2021
3.1	Analysis on Expressivity
The shaDow-GNN design is motivated by the following:
1)	. A shallow neighborhood is sufficient for the GNN to learn a good node representation;
2)	. A shallow neighborhood is necessary to reduce the effect of noise on the GNN;
3)	. A deep GNN is necessary to be expressive on the shallow neighborhood.
Point 1 is true in many real-world scenarios, and is also justified by the γ-decaying theorem (Zhang
& Chen, 2018): to estimate various important graph metrics of a node from its L-hop neighborhood,
the estimation error decays exponentially with the number of hops L. The understanding on Point
2 is two-folded: real-world graphs are likely to include noisy nodes and edges due to erroneous
graph construction (Klicpera et al., 2019b). Additionally, even without errors, a node u may still
be regarded as “white noise” to the target node v, simply because u is far away from v and u’s
information is irrelevant. The first kind of noise may be handled by filtering out the high frequency
components of the node signals (Wu et al., 2019) or enforcing low rank constraints on the adjacency
matrix (Grover et al., 2019; Jin et al., 2020). The second kind of noise cannot be filtered as long as
the GNN aggregates the full L-hop neighborhood. In this regard, sampling provides an additional
mechanism to deal with both kinds of noises. Also, since we customize different subgraphs for
different target nodes, SAMPLE filters noise at the node level rather than the graph level. For Point
3, while theoretical understanding on the benefit of deep neural networks has been well established
for Multi-Layer Perceptrons (MLPs) (Telgarsky, 2016), such conclusion does not directly transfer
to GNNs. For shaDow-GNN, we prove a deeper model is more expressive than a shallower one.
Expressivity comparison with “deep GNN, deep sampler”. When increasing the depth of a nor-
mal GNN, the neighborhood around a target node (i.e., the full L-hop subgraph) also expands.
Eventually, a deep normal GNN on a connected G will propagate influence of all V into a single
target node. As discussed in Point 2 above, most V only act as “white noise” to a target node, thus
causing difficulties in learning regardless of the layer propagation function.
We pick the GCN architecture for case study. When the GCN depth grows, we analyze how a shallow
sampler can filter out the “white noise” in the neighborhood and help preserve target node specific
information. Following the setup of Li et al. (2018); Luan et al. (2019); Ming Chen et al. (2020);
Zhao & Akoglu (2020), we study the asymptotic behavior of repeatedly applying the neighbor ag-
L
gregation operation. i.e., we analyze how much information is preserved in M = limL→∞ ALX.
Proposition 3.1. An ∞-layer SHADOW-GCN generates the neighbor aggregation for v as:
m[v] = [e[v]]v ∙ (eT>]X[v])	⑴
where e[v] is defined by [e[v]u = P
δ[v[ (U) ](w) ； δ[v] (u) returns the degree of U in G[v] plus 1.
w∈V[v]
OVERSMOOTHING OF NORMAL GCN With large enough L, the full L-hop neighborhood be-
comes V (assuming connected G). So ∀ u, v, we have G[u] = G[v] = G, implying e[u] = e[v] and
X[u] = X[v] = X. From Proposition 3.1, the aggregation converges to a point where no feature and
little structural information of the target is preserved. The only information in m[v] is v’s degree.
Local-smoothing of shaDow-GCN With a fixed shallow subgraph, no matter how many
times we aggregate using A[v], the many layers will not include the faraway irrelevant nodes. We can
see m[v] as a linear combination of the neighbor node features X[v]. Increasing the number of layers
only pushes the coefficients of each neighbor features to the stationary values. The domain X[v] of
such linear transformation is solely determined by SAMPLE and is independent of the layer depth.
Intuitively, if SAMPLE picks non-identical subgraphs for two nodes u and v, the aggregations should
be different due to the different domains of the linear transformation. Therefore, shaDow-GCN
preserves local feature information whereas normal GCN preserves none. For structural information
in m[v] , note that e[v] is a normalized degree distribution of the subgraph around v, and e[v] v
indicates the role of the target node in the subgraph. If we let SAMPLE return the 1-hop subgraph,
then e[v] v alone already contains all the information preserved by a normal GCN, which is v’s
degree in G. Since e[v] additionally reflects the structure of v’s ego-net, a deep SHADOW-GCN with
a naive 1-hop SAMPLE preserves more structural information than a deep GCN.
4
Under review as a conference paper at ICLR 2021
Theorem 3.2.	Let m^ = φg (V) ∙ m[v] where φg is any non-zero function only depending on
the structural property of V. Let M = {m[v] | V ∈ V}. Given G, SAMPLE and some continuous
probability distribution in RlVl×f to generate X, then m^ =亦回 if V[u] = V[v], almost surely.

Corollary 3.2.1. Consider SAMPLE1 where ∀V ∈ V ,
V[v] ≤ n. Then |M| ≥
almost surely.
Corollary 3.2.2. Consider SAMPLE2 where ∀ u, V ∈ V, V[v] 6= V[u]. Then |M| = |V| almost surely.
Theorem 3.2 proves shaDow-GCN does not oversmooth, as 1). a normal GCN pushes the aggrega-
tion of same-degree nodes to the same point, while shaDow-GCN with SAMPLE2 ensures any two
nodes (even with the same degree) can have different aggregation; 2). a normal GCN wipes out all
the information in the initial node features after many times of aggregation, while shaDow-GCN
-1/2
always preserves node feature information. In particular, ifwe set φG (V) = δ[v] (V)	, a normal
GCN generates only one unique value of m, while SHADOW-GNN can still generate N for any φg.
Expressivity comparison with “shallow GNN, shallow sampler”. Most state-of-the-art GNNs
belong to the “shallow GNN, shallow sampler” category. However, we argue that a deep GNN is
still desirable even though the neighborhood is shallow. In the following, we first give two examples
showing when “shallow GNN, shallow sampler” fails to learn something that a shaDow-GNN
can. Then we prove that shaDow-GNN is strictly more powerful than 1-dimensional Weisfeiler-
Lehman test, thus more powerful than all standard “shallow GNN, shallow sampler” designs.
In Figure 1, consider the 2-hoP neighborhood (black dots) of some target
node V (blue star). In the first examPle, suPPose another target node V0 has
a slightly different 2-hoP structure as reflected by an additional edge (red
dashed line). Assume all nodes have identical features. Then any 2-layer
GNN cannot distinguish V and V0, while a 3-layer GNN can. The red edge
that distinguishes V and V0 is not having any influence on V0 until three times
of message Passing. In the second examPle, we consider a simPle task of
learning the unweighted mean of neighbor features, T = 1^\ Pu∈v∣[Xu.
V, V
Figure 1: Example
neighborhood
Suppose G[v] consists of the star node, black nodes and black edges. Surprisingly, even with such a
“regular” subgraph structure, a normal L-layer GraphSAGE cannot learn τ accurately. The reasons
are as follows. If L > 2, then normal GraphSAGE will include nodes outside V[v] , and the only
way to always exclude the impact of these nodes is by setting the weights for that layer to 0 — this
effectively reduces the number of layers to 2. Now since τ is linear, ReLU is not desirable, and should
be bypassed by shifting X with the bias parameters. The output of the neural net then simplifies to
Z = [PL=1 A'v] X[v] W'] , where L = 2 for normal GraPhS AGE. For the given A[v], there does
not exist a w` for GraPhSAGE to satisfy T = Z for any value of X[v]. On the other hand, with
SAMPLE returning G[v], SHADOW-SAGE can learn τ with any Precision ifL is allowed to grow. The
required L for SHADOW-SAGE to reach the desired Precision is determined by the mixing time of
A[v] . SHADOW-SAGE can learn the unweighted mean because the eigenvector corresPonding to
the largest eigenvalue of A[v] is a uniform vector. In fact, a deeP SHADOW-SAGE can learn the
unweighted feature mean on any neighborhood.
Theorem 3.3.	Suppose we are given any GNN following the per-layer message passing design as:
h') =芦 h'T), X f2') (h'-1), h尸),	⑵
∖	U 〜V	)
where f(') and f2f' are the layer-' update and message functions, respectively, implemented as
MLPs; U 〜v denotes U is V ,s 1-hop neighbor. Then, on the full L-hop subgraph, an L0 -layer GNN
is strictly more discriminative than an L-layer model, when L0 > L. Moreover, with injective f1
and f2, the GNN model described above (using L0 > L layers on the L-hop subgraph) is strictly
more discriminative than the 1-dimensional Weisfeiler-Lehman test.
See APPendix A for the Proof. By Theorem 3.3, given a shallow samPler, a deeP GNN is strictly
more Powerful than a shallow one. Even given a deeP samPler, we can still use a “deePer” GNN to
beat the “deeP” GNN. Other Practical concerns on deeP GNNs include the difficulty in oPtimization
and the comPutation cost. The oPtimal dePth may be determined after considering such tradeoffs.
5
Under review as a conference paper at ICLR 2021
3.2	Sampler Design
k-hop sampler. Starting from the target node v, the sampler traverses up to k hops. At a hop-` node
u, the sampler will add to its hop-(` + 1) node set either all neighbors of u, or b randomly selected
neighbors of u. The subgraph G[v] is induced from all the nodes selected by the sampler. Here depth
k and budget b are the sampling parameters. See Appendix C for more analysis.
PPR sampler. Personalized PageRank (PPR) has been recently combined with graph learning
(Klicpera et al., 2019a; Bojchevski et al., 2020; Li et al., 2020b). However, none of the existing
works have explored PPR in the context of subgraph sampling. Given a target v, our PPR sampler
proceeds as follows: 1). Compute the approximate PPR vector πv ∈ R|V| for v. 2). Select the neigh-
borhood V[v] such that for u ∈ V[v], the PPR score [πv]u is large. 3). Construct the induced subgraph
from V[v]. For step 1), even though vector πv is of length-|V|, most of the PPR values are close to
zero. So we can use a fast algorithm (Andersen et al., 2006) to compute the approximate PPR score
by traversing only the local region around v . Throughout the πv computation, most of its entries
remain as the initial value of 0. Therefore, the PPR sampler is scalable and efficient w.r.t. both time
and space complexity. For step 2), we can either select V[v] based on top-p values in πv, or based on
some threshold [πv]u > θ. Then p, θ are hyperparameters. For step 3), notice that our PPR sampler
only uses PPR scores as a node filter. The original graph structure is still preserved among V[v] , due
to the induced subgraph step. Such property differentiates our algorithm from existing ones such as
APPNP (Klicpera et al., 2019a), GDC (Klicpera et al., 2019b) and PPRGo (Bojchevski et al., 2020).
Specifically, the three related works all reconstruct the adjacency matrix by directly connecting the
target v with nodes of top PPR scores. Reconstruction promotes feature propagation from direct
neighbors at the cost of losing structural information, since all V[v] would become 1-hop away from
v. We show in Section 3.3 further connections between SHADOW-GNN and the prior arts.
Extensions. The k-hop and PPR samplers each assume shortest path distance and random walk
landing probability as important metrics for measuring neighbor relevance. Following such ratio-
nale, we can design many more sampling algorithms. For example, instead of PPR scores, we can
use Katz index (Katz, 1953) as our node filtering condition. This is another way to signify the im-
portance of shortest path distance. If the number of common neighbors is important, we can filter
V[v] by SimRank (Jeh & Widom, 2002). Apart from structural information, SAMPLE can also utilize
node features. For example, in the k-hop sampler, instead of randomly pick b neighbors, we can pick
those with highest feature similarity (e.g., by cosine similarity or heat kernel (Wang et al., 2020)).
3.3	Architectural Extension
Subgraph ensemble. From Section 3.2, each reasonable SAMPLE reveals some perspectives of the
graph, and they altogether present the full picture. We may then combine the information from
various SAMPLE. Assume a set of C candidates {SAMPLEi }, each returning G[v] i. We can either:
1). Construct a union-subgraph G% for SHADOW-GNN to operate on, where V简=UC=I (V[v]'
and Ei] = UC=I (E[v])i ,or2).Use C parallel branches of L-layer GNN, each operating on (Gv )i. So
v’s output embedding of each branch i (i.e., a vector zi) can be aggregated by attention mechanism:
C
(Z[v])i = fL	((x[v])i	,	(A[v])i)	；	Wi	= MLP (Zi)	∙	q；	yv	= X Wi	∙	Zi	⑶
i=1
where fiL is the function by the L-layer GNN at branch i; Zi is arow vector of Z[v] i corresponding
to v; q is a learnable vector; we is normalized from w by softmax. yv is the final embedding. We
can use identical GNN structure for all the C branches or even share weights among the branches.
{SAMPLEi } can also perform the same algorithm with different hyperparameters. For example, we
can let SAMPLEi be the PPR sampler with different threshold θi. A SHADOW-SAGE-ensemble
can imitate similar feature aggregation behavior as PPRGo (Bojchevski et al., 2020), while still
having the additional advantage of learning rich subgraph structural information. Note that PPRGo
generates embedding by 1-hop aggregation on the reconstructed graph: τv = Pu∈V πuhv, where
πu = [πv]u and hv = MLP (xv). We can partition V[v] = UiC=1 V[iv] such that nodes in V[iv] have
similar PPR scores approximated by πei, and πei ≤ πei+1. So τv ≈ PiC=1 ρi Pu∈V0 hu , where
6
Under review as a conference paper at ICLR 2021
ρi = πei - Pj<i πej and Vi0 = SkC=i V[kv]. Recall the discussion on learning unweighted feature mean
of any V[v] (Section 3.1). Setting θi = πei and wi = ρi, SHADOW-SAGE can approximate τv. As
our PPR sampler also preserves local structure, our model can be more expressive than PPRGo.
Sampling the reconstructed graph. Another extension is to build SHADOW-GNN on the graph
reconstructed by graph diffusion (Klicpera et al., 2019b; Frasca et al., 2020). In the simplest case, we
can apply our L-hop sampler on the reconstructed graph, and build a L0-layer GNN on the subgraph.
When L = L0, we recover GDC (Klicpera et al., 2019b). When L < L0, we have SHADOW-GDC.
4 Experiments
Setup. We perform node classification on five graphs: Flickr (Zeng et al., 2020), Reddit (Hamil-
ton et al., 2017a), Yelp (Zeng et al., 2020), ogbn-arxiv (Hu et al., 2020) and ogbn-products
(Hu et al., 2020). The graph sizes range from around 9K nodes (Flickr) to 2.5M nodes
(ogbn-products). Consistent with the original setup, we use the metric of “accuracy” for Flickr,
Reddit, ogbn-arxiv and ogbn-products, and “F1-micro score” for Yelp. See Appendix D.1.
We consider seven baselines and construct the corresponding shaDow models: GCN (Kipf &
Welling, 2016), GraPhSAGE (Hamilton et al., 2017a), GAT (Velickovic et al., 2018), JK-Net (XU
et al., 2018b), GIN (Xu et al., 2018a), SGC (Wu et al., 2019) and GraphSAINT (Zeng et al., 2020).
The first six are rePresentatives of the state-of-the-art GNN architectUres. They jointly cover varioUs
message aggregation fUnctions as well as the skiP connection design for facilitating deeP GNN
training. GraPhSAINT is the state-of-the-art minibatch training algorithm for large graPhs. The
GraPhSAINT training has been shown to work well with the GraPhSAGE and GAT architectUres. In
Table 1, for the the rows of GCN, GraPhSAGE and GAT, we train 3- and 5-layer models in the fUll
batch fashion. However, for the large graPhs, the GPU memory (15GB) is too small to store the fUll
batch. We therefore fUrther rUn exPeriments Using the GraPhSAINT minibatch training algorithm,
where “GraPhSAINT-RW” denotes the random walk samPler recommended by Zeng et al. (2020).
For all the exPeriments, we fix the nUmber of channels (i.e., the hidden dimension) for each layer as
d(`) = 256. All accUracy resUlts are measUred by five rUns withoUt fixing random seeds. Detailed
hyPerParameter tUning ProcedUre and architectUre configUration are described in APPendix D.3.
Comparison with state-of-the-art. Table 1 shows the Performance comParison of SHADOW-GNN
with state-of-the-art GNNs. We Use the following metrics: test set accUracy / F1-micro score and
inference cost. The inference cost - defined as the average amount of computation to generate pre-
diction for one test node - is purely a measure of computation complexity and is independent of hard-
ware / implementation factors such as parallelization strategy, batch processing, distributed storage,
etc. Thus, the metric reflects the feasibility of deploying each GNN on real-world workloads, where
the graph sizes and hardware specifications can be different from our setup. For shaDow-GNN,
the inference cost does not consider the sampling overhead. See Figure 2 for the sampling time
measurement and Appendix B for the equations for calculating the inference cost. For the 3-layer
and 5-layer models in Table 1, we simply stack the corresponding GNN layers. During training, we
further apply the DropEdge technique (Rong et al., 2020) to both the baseline and shaDow models.
We observe that DropEdge helps improve the baseline accuracy by alleviating oversmoothing, and
helps improves the shaDow accuracy due to its regularization effects.
Accuracy Comparing the normal GCN, GraphSAGE and GAT, shaDow-GNNs achieve sig-
nificantly higher accuracy across all the five datasets. Since we use the identical backbone GNN
architectures, the accuracy gains validate the effectiveness of our design principle of “deep GNN,
shallow sampler”. Note that that our subgraphs are truly shallow, since the the subgraphs generated
by our 2-hop and PPR samplers contain no more than 200 nodes. For 3-layer, comparing the accu-
racy of the normal GNN with the shaDow-GNN, it is clear that a shallow neighborhood contains
sufficient information to generate good node embeddings. For 5-layer accuracy comparison, we ob-
serve even higher gains - for normal GNNs, increasing from 3 layers to 5 layers sometimes results
in accuracy degradation (even with DropEdge). For shaDow-GNN, making the GNN deeper is
beneficial most of the time. These observations validates our oversmoothing and expressivity anal-
ysis in Section 3.1. Finally, the higher accuracy achieved by the PPR sampler compared with the
2-hop sampler demonstrates the importance of a good sampling algorithm.
7
Under review as a conference paper at ICLR 2021
)sm( edon rep emit ecnerefn
■ PPR ∙SAGE ■ GAr
3
Tradeoff analysis (5-layer)
100
Figure 2: Inference time after paral-
lelization on commodity hardware
)%( ycarucca tse
80
60
......................................I........................................................I
+
......I
40
10-1	100	101	102	103
Inference time per node (ms)
SHADOW
SAGE
SHADOW
GAT
■ Normal
SAGE
■ Normal
GAT
+ Flickr
X Reddit
人 arxiv
* products
Figure 3: Inference performance tradeoff. we test pre-
trained models by subgraphs of various sizes.
1 in⅛⅛xxx..!
2
1
/ A
Inference cost From Table 1, the inference cost of shaDow-GNN is often orders of magnitude
lower than that of the baselines. The high cost of the baselines reflects the “neighbor explosion”
challenge for deep GNNs. In short, for normal GNN, since we do not enforce a fixed size subgraph,
the inference computation complexity may grow exponentially with the depth of the GNN (Chen
et al., 2017). For example, the number of `-hop neighbors may be 10× the number of (` - 1)-hop
neighbors. For shaDow-GNN, it is clear that the inference cost only grows linearly with the GNN
depth. From Table 1, we observe that even when the GNN depth is only 5, the high computation
cost already makes the inference of normal GNNs practically infeasible. The results show the high
efficiency of our design. Finally, note that for GraphSAINT, its sampling is only applied during
training. The inference computation of GraphSAINT falls back to be identical as its backbone
architecture (which samples the full L-hop neighborhood). on the other hand, for SHADow-GNN,
we apply the same sampling and batching strategy, whether it is for training or for inference.
Table 1: Comparison on test accuracy / F1-micro score and inference cost tuned with DropEdge)
Method	Layers	Flickr		Reddit		Yelp		ogbn-arxiv		ogbn-products	
		Accuracy	Cost	Accuracy	Cost	F1-micro	Cost	Accuracy	Cost	Accuracy	Cost
GCN	3	0.516±0.002	2E0	0.953±0.000	6E1	0.402±0.002	2E1	0.717±0.003	1E0	0.756±0.002	5E0
	5	0.522±0.002	2E2	0.949±0.001	1E3	ooM	1E3	0.719±0.002	2E0	ooM	9E2
GraphSAGE	3	0.514±0.001	5E0	0.965±0.000	9E1	0.617±0.003	3E1	0.719±0.003	1E0	0.785±0.001	8E0
	5	0.515±0.005	3E2	0.962±0.000	2E3	ooM	3E3	0.719±0.004	3E0	ooM	2E3
GAT	3	0.507±0.003	3E1	ooM	5E2	OOM	3E2	0.720±0.001	1E0	ooM	6E1
	5	0.516±0.003	4E2	ooM	3E3	ooM	4E3	ooM	5E0	ooM	4E3
GraphSAGE	3	0.517±0.003	5E0	0.967±0.000	9E1	0.645±0.001	3E1	0.710±0.000	1E0	0.792±0.002	8E0
+ GraphSAINT-RW	5	0.520±0.003	3E2	0.967±0.001	2E3	0.639±0.000	3E3	0.701±0.002	3E0	0.796±0.002	2E3
GAT	3	0.522±0.005	3E1	0.967±0.000	5E2	0.645±0.000	3E2	0.697±0.000	1E0	0.802±0.003	6E1
+ GraphSAINT-RW	5	0.515±0.003	4E2	0.965±0.002	3E3	0.647±0.001	4E3	0.695±0.001	5E0	0.799±0.007	4E3
SHADOW-GCN	3	0.526±0.002	(1)	0.958±0.000	⑴	0.526±0.001	(1)	0.719±0.002	(1)	0.777±0.003	(1)
+PPR	5	0.527±0.002	1E0	0.958±0.000	1E0	0.527±0.001	2E0	0.721±0.003	2E0	0.784±0.003	2E0
shaDow-SAGE	3	0.529±0.001	2E0	0.966±0.000	2E0	0.649±0.000	2E0	0.716±0.001	2E0	0.799±0.001	2E0
+ 2-hop	5	0.534±0.004	3E0	0.966±0.000	3E0	0.650±0.000	3E0	0.718±0.001	3E0	0.801±0.002	3E0
shaDow-SAGE	3	0.534±0.002	2E0	0.969±0.000	2E0	0.651±0.000	2E0	0.723±0.003	3E0	0.794±0.002	2E0
+ PPR	5	0.542±0.002	3E0	0.969±0.000	3E0	0.650±0.000	3E0	0.725±0.001	3E0	0.804±0.002	3E0
shaDow-GAT	3	0.538±0.003	2E0	0.970±0.001	2E0	0.655±0.000	2E0	0.724±0.001	3E0	0.801±0.001	2E0
+ PPR	5	0.534±0.002	3E0	0.971±0.001	3E0	0.654±0.000	3E0	0.728±0.003	5E0	0.809±0.003	3E0
——S-SGC, F ——S-SGC, R ——S-SGC, A
--SGC, F - - SGC R - - SGC, A
100
——shadow 3 — shadow 5 — shadow 8 ——shadow 15
--SAINT 3	--SAINT 5 --SAINT 8	- - SAINT 15
B
O
σj
1
S
H
80
60
40
0	20	40
TA	T T
Power on A or A[v]
O
186 42 0
..........
0000
10
20
∞
1
ycarucca noitadilaV
5
9
10
20
∞
1
Epoch
Epoch
SSol UoHPP=Bʌ
O
5
8
Figure 4: Effect of increasing the model depth on the SGC and GCN architectures
8
Under review as a conference paper at ICLR 2021
Evaluation on PPR sampler. We evaluate the PPR sampler in terms of its execution time overhead
and accuracy-time tradeoff. In Figure 2, we parallelize the sampler on a 40-core Xeon CPU and
the GNN computation on a NVIDIA P100 GPU (See Appendix D.2). The sampler is lightweight:
the sampling time is lower than the GNN computation time in most cases. Also, the sampling time
per node does not necessarily grow with the full graph size. By Section 3.2, the approximate PPR
computation achieves efficiency and scalability by only traversing a local region around each target
node. To evaluate the accuracy-time tradeoff, we take the 5-layer models of Table 1 as the pretrained
models. Then for SHADOW-GNN, we vary the PPR budget p from 50 to 200 with stride 50. In
Figure 3, the inference time of shaDow-GNN has already included the PPR sampling time. Firstly,
consistent with Table 1, inference of shaDow-GNNs achieves higher accuracy than the normal
GNNs, with orders of magnitude speedup as well. In addition, based on the application requirements
(e.g., latency constraint), SHADOW-GNNs have the flexibility of adjusting the sampling size without
the need of retraining. For example, on Reddit and ogbn-arxiv, directly reducing the subgraph
size from 200 to 50 speeds up inference by 2× to 4× at the cost of less than 1% accuracy drop.
Evaluation on other architectures.
In addition to the GCN, GraphSAGE
and GAT models in Table 1, we fur-
ther compare JK-Net and GIN with
their shaDow versions in Table 2.
Similar to DropEdge, the skip connec-
tion (or “jumping knowledge”) helps
Table 2: Test accuracy (%) on other architectures
	Flickr		Reddit		ogbn-arxiv	
	Normal	SHADOW	Normal	SHADOW	Normal	SHADOW
JK (3)	49.45±0.70	53.17±0.27	96.49±0.10	96.82±0.03	71.30±0.26	72.01±0.17
JK (5)	49.40±0.83	53.28±0.26	96.40±0.13	96.85±0.06	71.66±0.53	72.26±0.24
GIN (3)	51.32±0.31	52.28±0.28	93.45±0.34	95.78±0.06	70.87±0.16	71.73±0.29
GIN (5)	50.04±0.67	52.55±0.23	75.50±0.39	95.52±0.07	69.37±0.62	71.40±0.27
accuracy improvement on deeper models. Compared with the normal JK-Net, increasing the depth
benefits shaDow-JK more. The GIN architecture theoretically does not oversmooth. However, we
observe that the GIN training is very sensitive to hyperparameter settings. We hypothesize that such
a challenge is due to the sensitivity of the sum aggregator on noisy neighbors (e.g., for GraphSAGE,
a single noisy node can hardly cause a significant perturbation on the aggregation, due to the averag-
ing over the entire neighborhood). The accuracy improvement of shaDow-GIN compared with the
normal GIN may thus be due to noise filtering by shallow sampling (see Section 3.1). The impact
of noises / irrelevant neighbors can be critical, as reflected by the 5-layer GIN accuracy on Reddit.
Effect of model depth. To validate Theorem 3.2 on the non-
oversmoothing of shaDow-GCN, we experiment on much
larger GNN depth. In the left plot of Figure 4, we run a normal
SGC and a shaDow-SGC by varying the power on the adja-
cency matrix from 1 to 40 (see Appendix D.4 for the model
details and setup). While SGC gradually collapses local infor-
mation into global “white noise”, accuracy of shaDow-SGC
Table 3: GCN test accuracy (%)
L0	SAMPLE	FliCkr	ogbn-produCts
3	PPR	52.57±0.21	77.73±0.32
	2-hop	52.10±0.23	77.94±0.39
5	PPR	52.73±0.20	78.36±0.34
	Ensemble	53.04±0.17	78.58±0.21
7	PPR	52.25 ±0.23	78.50±0.44
does not degrade. In the middle and right plots, we train the standard GCN architecture without the
SGC simplications. Since ogbn-products is too large, we compare shaDow-GCN with GCN-
SAINT. For both methods, the deeper models converge slower. Deeper models (whether they are
GNNs or other types of NNs) are generally harder to optimize. For shaDow-GCN, the validation
performance of the 15-layer model eventually catches up with the shallower ones. For GCN-SAINT,
however, there consistently exists a large performance gap between the 15-layer model and the shal-
lower ones. Both experiments clearly show that shallow sampling prevents oversmoothing. Finally,
for ogbn-products in in Table 3, we note that even through > 98% of the subgraph nodes are with
2 hops (see Figure 6), increasing the GNN depth can benefit accuracy for up to L0 = 7 layers.
Evaluation on subgraph ensemble. Benefit of ensemble is shown via the 5-layer entries in Table
3. Different samplers, such as PPR and k-hop, preserve different kinds of local information. Rather
than designing one “perfect” sampler, it may be more reasonable to ensemble a few simpler ones.
Note that ensemble is only possible for subgraph based shaDow-GNN. In Figure 8 (see Appendix
E), we show that even the simplest 1-hop sampler can benefit training after ensemble.
5 Conclusion
We have presented a design principle, “deep GNNs, shallow samplers”, enabling accurate and effi-
cient inference for a wide range of GNN architectures. We have theoretically justified the expressiv-
ity of shaDow-GNN and empirically demonstrated its benefits under various setups and metrics.
9
Under review as a conference paper at ICLR 2021
References
Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina Lerman, Hrayr
Harutyunyan, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convolutional
architectures via sparsified neighborhood mixing. arXiv preprint arXiv:1905.00067, 2019.
Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications,
2020.
Reid Andersen, Fan Chung, and Kevin Lang. Local graph partitioning using pagerank vectors. In
2006 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS'06), pp. 475-
486. IEEE, 2006.
Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol Kapoor, Martin Blais, Benedek
ROzemberczki, Michal Lukasik, and Stephan Gunnemann. Scaling graph neural networks with
approximate pagerank. In Proceedings of the 26th ACM SIGKDD International Conference on
Knowledge Discovery & Data Mining, KDD ’20, pp. 2464-2473, New York, NY, USA, 2020.
Association for Computing Machinery. doi: 10.1145/3394486.3403296.
Jianfei Chen, Jun Zhu, and Le Song. Stochastic training of graph convolutional networks with
variance reduction, 2017.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Ben Chamberlain, Michael Bronstein, and Fed-
erico Monti. SIGN: Scalable inception graph neural networks, 2020.
Aditya Grover, Aaron Zweig, and Stefano Ermon. Graphite: Iterative generative modeling of graphs.
In International Conference on Machine Learning, pp. 2434-2444, 2019.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In Advances in neural information processing systems, pp. 1024-1034, 2017a.
William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. arXiv preprint arXiv:1709.05584, 2017b.
Arman Hasanzadeh, Ehsan Hajiramezanali, Shahin Boluki, Mingyuan Zhou, Nick Duffield, Krishna
Narayanan, and Xiaoning Qian. Bayesian graph neural networks with adaptive connection sam-
pling, 2020.
Kurt Hornik, Maxwell Stinchcombe, Halbert White, et al. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359-366, 1989.
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,
and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs, 2020.
Wenbing Huang, Tong Zhang, Yu Rong, and Junzhou Huang. Adaptive sampling towards fast graph
representation learning. In Advances in neural information processing systems, pp. 4558-4567,
2018.
Wenbing Huang, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Tackling over-
smoothing for general Graph Convolutional Networks, 2020.
Glen Jeh and Jennifer Widom. Simrank: a measure of structural-context similarity. In Proceedings
of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,
pp. 538-543, 2002.
Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure
learning for robust graph neural networks. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, KDD ’20, pp. 66-74, New York, NY, USA,
2020. Association for Computing Machinery. doi: 10.1145/3394486.3403049.
10
Under review as a conference paper at ICLR 2021
Leo Katz. A new status index derived from sociometric analysis. Psychometrika, 18(1):39-43,
1953.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Johannes Klicpera, Aleksandar Bojchevski, and StePhan Gunnemann. Combining neural networks
with personalized pagerank for classification on graphs. In International Conference on Learning
Representations, 2019a.
Johannes Klicpera, Stefan WeiBenberger, and Stephan Gunnemann. Diffusion improves graph learn-
ing, 2019b.
Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. DeepGCNs: Can GCNs go as deep
as CNNs? In The IEEE International Conference on Computer Vision (ICCV), 2019.
Guohao Li, Chenxin Xiong, Ali Thabet, and Bernard Ghanem. DeeperGCN: All you need to train
deeper gcns, 2020a.
Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec. Distance encoding - design provably
more powerful gnns for structural representation learning, 2020b.
Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into Graph Convolutional Networks
for semi-supervised learning. arXiv preprint arXiv:1801.07606, 2018.
Meng Liu, Hongyang Gao, and Shuiwang Ji. Towards deeper graph neural networks. Proceedings of
the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, Aug
2020. doi: 10.1145/3394486.3403076. URL http://dx.doi.org/10.1145/3394486.
3403076.
Yu-Chen Lo, Stefano E Rensi, Wen Torng, and Russ B Altman. Machine learning in chemoinfor-
matics and drug discovery. Drug discovery today, 23(8):1538-1546, 2018.
Sitao Luan, Mingde Zhao, Xiao-Wen Chang, and Doina Precup. Break the ceiling: Stronger multi-
scale deep graph convolutional networks. In Advances in Neural Information Processing Systems
32, pp. 10945-10955. Curran Associates, Inc., 2019.
Zhewei Wei Ming Chen, Bolin Ding Zengfeng Huang, and Yaliang Li. Simple and deep graph
convolutional networks. 2020.
Federico Monti, Michael Bronstein, and Xavier Bresson. Geometric matrix completion with recur-
rent multi-graph neural networks. In Advances in Neural Information Processing Systems, pp.
3697-3707, 2017.
Kenta Oono and Taiji Suzuki. Graph Neural Networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations, 2020.
Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd. The pagerank citation ranking:
Bringing order to the web. Technical report, Stanford InfoLab, 1999.
Aditya Pal, Chantat Eksombatchai, Yitong Zhou, Bo Zhao, Charles Rosenberg, and Jure Leskovec.
PinnerSage: Multi-modal user embedding framework for recommendations at Pinterest. Pro-
ceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data
Mining, Aug 2020. doi: 10.1145/3394486.3403280.
Namyong Park, Andrey Kan, Xin Luna Dong, Tong Zhao, and Christos Faloutsos. Estimating node
importance in knowledge graphs using graph neural networks. In Proceedings of the 25th ACM
SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 596-606, 2019.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. DropEdge: Towards deep Graph
Convolutional Networks on node classification. In International Conference on Learning Repre-
sentations, 2020.
11
Under review as a conference paper at ICLR 2021
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In European Semantic Web
Conference,pp. 593-607. Springer, 2018.
Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-
wardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: a simple way to prevent neural networks from overfitting. The journal of machine
learning research, 15(1):1929-1958, 2014.
Jonathan M Stokes, Kevin Yang, Kyle Swanson, Wengong Jin, Andres Cubillos-Ruiz, Nina M
Donghia, Craig R MacNair, Shawn French, Lindsey A Carfrae, Zohar Bloom-Ackerman, et al. A
deep learning approach to antibiotic discovery. Cell, 180(4):688-702, 2020.
Matus Telgarsky. Benefits of depth in neural networks. arXiv preprint arXiv:1602.04485, 2016.
Petar VeliCkovic, GUillem CUcUrUlL Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph Attention Networks. In International Conference on Learning Representations,
2018.
Xiao Wang, Meiqi ZhU, DeyU Bo, Peng CUi, ChUan Shi, and Jian Pei. AM-GCN: Adaptive mUlti-
channel graph convolUtional networks. In Proceedings of the 26th ACM SIGKDD International
Conference on Knowledge Discovery & Data Mining, KDD ’20, pp. 1243-1253, New York,
NY, USA, 2020. Association for CompUting Machinery. ISBN 9781450379984. doi: 10.1145/
3394486.3403177.
Felix WU, Tianyi Zhang, AmaUri Holanda de SoUza Jr, Christopher Fifty, Tao YU, and Kilian Q
Weinberger. Simplifying graph convolUtional networks. arXiv preprint arXiv:1902.07153, 2019.
Zonghan WU, ShirUi Pan, Fengwen Chen, GUodong Long, Chengqi Zhang, and Philip S. YU. A
comprehensive sUrvey on graph neUral networks. IEEE Transactions on Neural Networks and
Learning Systems, 2020.
KeyUlU XU, WeihUa HU, JUre Leskovec, and Stefanie Jegelka. How powerfUl are graph neUral
networks? arXiv preprint arXiv:1810.00826, 2018a.
KeyUlU XU, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken ichi Kawarabayashi, and Stefanie
Jegelka. Representation learning on graphs with jUmping knowledge networks, 2018b.
Rex Ying, RUining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and JUre Leskovec.
Graph convolUtional neUral networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 974-
983, 2018.
JiaxUan YoU, Rex Ying, and JUre Leskovec. Position-aware graph neUral networks. arXiv preprint
arXiv:1906.04817, 2019.
Hanqing Zeng, HongkUan ZhoU, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-
SAINT: Graph sampling based indUctive learning method. In International Conference on Learn-
ing Representations, 2020.
MUhan Zhang and Yixin Chen. Link prediction based on graph neUral networks. In Advances in
Neural Information Processing Systems, pp. 5165-5175, 2018.
ShUai Zhang, Lina Yao, Aixin SUn, and Yi Tay. Deep learning based recommender system: A sUrvey
and new perspectives. ACM Computing Surveys (CSUR), 52(1):1-38, 2019.
YUyU Zhang, Xinshi Chen, YUan Yang, ArUn RamamUrthy, Bo Li, YUan Qi, and Le Song. Efficient
probabilistic logic reasoning with graph neUral networks, 2020.
Lingxiao Zhao and Leman AkoglU. PairNorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations, 2020.
12
Under review as a conference paper at ICLR 2021
A	Proofs
Proof of Proposition 3.1. The GCN model performs symmetric normalization on the adjacency ma-
trix. shaDow-GCN follows the same way to normalize the subgraph adjacency matrix as:
〜	_1	_1
A[v] = S + I)-2 • % + I) ∙ (D[v] + I)-2	(4)
where A[v] ∈ RN ×N is the binary adjacency matrix for G[v] .
A[v] is a real symmetric matrix and has the largest eigenvalue of 1. Since SAMPLE ensures the
subgraph G[v] is connected, so the multiplicity of the largest eigenvalue is 1. By Theorem 1 of
Huang et al. (2020), we can bound the eigenvalues λi by 1 = λ1 > λ2 > . . . > λN > -1.
Performing eigen-decomposition on A[v], we have
Ae[v] = E[v]ΛE[-v]1 = E[v]ΛE[Tv]	(5)
where Λ is a diagonal matrix Λi,i = λi and matrix E[v] consists of the N normalized eigenvectors.
We have:
Ae[Lv] = E[v]ΛLE[Tv]	(6)
Since ∣λ∕ < 1 When i = 1, we have limL→∞ A^ =后历产小 where ev is the eigenvector corre-
sponding to λι. It is easy to see that [e^]U α ,δ[v](u) (Huang et al., 2020). After normalization,
Γer J = / δ[v](U)一
Le[v]J U = P Pw∈V[v] δ[v](w).
It directly follows that m[v]= k⑻]@ ∙ (e^∣v] X[v]), with value of e[v] defined above.	□
ProofofTheorem 3.2. We first prove the case of m^ = m[v]. i.e., φg(V) = 1.
According to Proposition 3.1, the aggregation for each target node equals m[v] = e[v] v e[Tv]X[v] .
Let N = |V |. Let 科v] ∈ RN×1 be the vector expanded from e^, such that 弧⑻]U = 0 if u ∈ V©
and the rest of the non-zero elements of 科v] are copied from e[v] accordingly.
We define:
e = [e[v]]v ∙ ⅛] - [e[u]]u ∙ δ[U]	⑺
Suppose we only compute when two nodes u andv have non-identical sampled neighborhood. i.e.,
V[U] 6= V[v] . Then given G, SAMPLE and some distribution to generate X, there are finite number of
possible values. The reason is that G is finite and does not depend on X (where X itself can take
infinitely many values). Each of the possible E values defines a hyperplane in RN-1 by E ∙ X = 0
(where x ∈ RN). Let all such hyperplanes be H.
Now suppose we build SHADOW-GCN by performing SAMPLE on G and X. For any two v and
u with V[v] 6= V[U] , suppose their aggregations are the same: m[v] = m[U] . Then we must have
E ∙ X = 0. In other words, ∀i, X：,% must fall on one of the hyperplanes in H.
However, since X is generated from a continuous distribution in RN×f, X:,i would not fall on all of
the hyperplanes in H, almost surely. Therefore, for any v and u such that V[v] 6= V[U] , m[v] 6= m[U]
almost surely.
For a more general φG (v), since φG does not depend on X, the proof follows exactly the same steps
as above.
□
Proof of Corollary 3.2.1. Note that ∀u ∈ V, u ∈ V[U]. For any node v, there are at most n - 1 other
nodes in V with the same neighborhood as V[v] . Such n - 1 possible nodes are exactly those in V[v] .
13
Under review as a conference paper at ICLR 2021
By Theorem 3.2, ∀v ∈ V, there are at most n - 1 other nodes in V having the same aggregation as
m [v]. Equivalently, total number of possible aggregations is at least d|V| /n].	□
Proof of Corollary 3.2.2. By definition of SAMPLE2, any pair of nodes have non-identical neighbor-
hood. By Theorem 3.2, any pair of nodes have non-identical aggregation. Equivalently, all nodes
have different aggregation and |M| = |V|.	□
Proof of Theorem 3.3. Define G[Lv] as the subgraph induced from all the `-hop neighbors ofv, where
1 ≤ ` ≤ L.
We first prove that an L0-layer GNN is at least as expressive as an L-layer GNN on any L-hop
subgraph. We note that for the target node v, the only difference between these two architectures is
that an L-layer GNN exactly performs L message passing iterations to propagate node information
from at most L hops away, while an L0-layer GNN has L0 - L more message passing iterations
before performing the L message passings. Thanks to the universal approximation theorem (Hornik
et al., 1989), We can let f(') (hV'-1), Pu〜v f2') (hV'-1), hU-1))) = hV'-1),∀1 ≤ ' ≤ L- L
since MLPs can model and learn such functions. Then, the L0-layer GNN will have the same output
as the L-layer GNN.
Then, We shoW that an L0-layer GNN can learn something that an L-layer GNN cannot learn on
some G[Lv] . This can be proved exactly by the example given in Figure 1, Where the red edge is
included in the subgraph. In this G[2v0] around v0, a 2-layer GNN fails to capture the red edge, and
Will output the representation of v0 the same as When there is no red edge. In contrast, a GNN With
more than 2 layers can capture the red edge immediately after the first message passing round, Where
the tWo end nodes of the red edge receive messages from each other. And after 3 message passing
rounds, this captured additional information Will be propagated to v0, resulting in a representation
different from that learned by the 2-layer GNN.
Finally, We prove that With injective f1 and f2, the GNN model described above (using L0 > L layers
on the L-hop subgraph) is strictly more discriminative than the 1-dimensional Weisfeiler-Lehman
(1-WL) test. We first prove that no matter hoW many iterations L 1-WL runs, We can alWays run
this GNN model on the L-hop subgraph With L0 layers to get at least the same discriminative poWer
as 1-WL. According to Xu et al. (2018a), an L-layer GNN on G[Lv] is theoretically as discriminative
as 1-WL by being able to discriminate all nodes v that 1-WL With L iterations of color update can
discriminate. Suppose there are tWo node v, v0 that 1-WL gives different colors to after L iterations.
Then applying the L-layer GNN on G[Lv] and G[Lv0] Will also return h(vL) 6= h(vL0 ). Since f1 and f2 are
injective, We knoW h(vL+i) 6= h(vL0 +i) for any i > 0, Which means an L0-layer GNN (L0 > L) can
also discriminate the tWo nodes.
Then it suffices to give an example Where 1-WL alWays fails to distinguish tWo nodes no matter hoW
many iterations it runs, While the GNN model above can discriminate them With some L and L0 .
We construct a very simple example, as shoWn in Figure 5. Consider
an undirected graph With tWo connected components (CC). The first
CC is a hexagon. The second CC is a triangle. Both CC are 2-
regular. Suppose all nodes have identical attributes and edges are
unWeighted. Our goal is to discriminate a node u in the first CC
from a node v in the second CC. For 1-WL, no matter hoW many
iterations L it runs, 1-WL alWays assigns the same label to u and v.
For the GNN above, When We set L = 1 and L0 = 2, We can already
discriminate betWeen the tWo nodes.
CC1
Figure 5: Example graph
on Which shaDow-GNN is
more expressive than 1-WL
□
Remark Note that even though We discuss the (L + 1)-layer case in the above proof, increasing
the layer L0 beyond L + 1 can still be benefitial in many cases. Consider the folloWing:
CASE 1 We revisit the GraphSAGE example on Figure 1. Suppose We use a L0-layer GraphSAGE
to approximate a function τ on the subgraph G[Lv] . We consider an example τ as computing the
unWeighted mean of the subgraph node features. In this case, the error of approximating τ converges
14
Under review as a conference paper at ICLR 2021
to zero when L0 goes to infinity. The error can still be significant if L0 is not sufficiently larger than
L. The desired depth L0 is determined by the mixing time of the subgraph adjacency matrix.
CASE 2 Consider a more “powerful” GNN when f1 and f2 of Equation 2 perform injective map-
ping. Recall the following two facts:
•	On the full graph, such a GNN of L0 layers is as discriminative as the 1-WL test running
L0 iterations.
•	1-WL may take up to O(N) iterations to converge where N is the full graph size.
Now extend to the SHADOW-GNN case. Consider two target nodes u andv with their corresponding
L-hop subgraphs G[Lu] and G[Lv]. An L0-layer GNN will output different embeddings for u and v, if
u’s color assigned by L0-iteration 1-WL on G[Lu] is different from v’s color assigned by L0-iteration
1-WL on G[Lv] . The coloring of u and v can keep changing for O(N) iterations (here N is the size
of G[Lu] and G[Lv]). Thus, increasing the GNN depth on L-hop subgraphs can improve discriminative
power for up to O(N) layers.
B	Inference Complexity Calculation
Here we describe the equations to compute the “inference cost” of Table 1. Recall that inference cost
is a measure of computation complexity to generate node embeddings for a given GNN architecture.
The numbers in Table 1 shows on average, how many arithmetic operations is required to generate
the embedding for each node. For a GNN layer ', denote the number of input nodes as n(`-1) and
the number of output nodes as n(`). Denote the number of edges connecting the input and output
nodes as m(`). Recall that we use d(`) to denote the number of channels, or, the hidden dimension.
In the following, we ignore the computation cost of non-linear activation, batch-normalization and
applying bias, since their complexity is negligible compared with the other operations.
For the GCN architecture, each layer mainly performs two operations: aggregation of the neighbor
features and linear transformation by the layer weights. So the number of multiplication-addition
(MAC) operations of a layer equals:
CGCN = m(')d('-1) + n(')d('-1)d(')	(8)
Similarly, for GraphSAGE architecture, the number of MAC operations equals:
C(2F = m(')d('-1) + 2 ∙ n⑶d(J)d⑶	(9)
SAGE
where the 2 factor is due to the weight transformation on the self-features.
For GAT, suppose the number of attention heads is t. Then the layer contains t weight matrices
Wi, each of shape d('-1) X dt'). We first transform each of the n('-1) nodes by Wi. Then for
each edge (u, v) connecting the layer input u to the layer output v, we obtain its edge weight (i.e.,
a scalar) by computing the dot product between u’s, v’s transformed feature vectors and the model
attention weight vector. After obtaining the edge weight, the remaining computation is to aggregate
the n(`-1) features into the n(') nodes. The final output is obtained by concatenating the features of
different heads. The number of MAC operations equals:
CGAT =t ∙ n('T)d('T)dr + 2t ∙ m⑶，+ m⑶d⑶	(10)
=3m⑶d⑶ + n(J) d(J) d⑶	(11)
15
Under review as a conference paper at ICLR 2021
On the same graph, GCN is less expensive than GraphSAGE. GraphSAGE is in general less expen-
Sive than GAT, due to n('-1) > n('). In addition, We note that for all architectures, Cy) grows
proportionally with n(`) and m(`). For the normal GNN architecture, since we are using the full
`-hop neighborhood for each node, the value of n(`) and m(`) may grow exponentially with `. This
is the “neighbor explosion” phenonemon and is the root cause of the high inference cost of the Table
1 baselines.
For SHADOW-GNN, suppose the subgraph contains n nodes and m edges. Then n(`) = n and
m(`) = m. The inference cost of any S HAD OW- GNN is ensured to only grow linearly with the
depth of the GNN.
C Details on S amplers
Here we present more detailed discussion on the proposed k-hop and PPR samplers.
k-hop sampler. While previous works Hamilton et al. (2017a); Ying et al. (2018) have proposed
to randomly sub-sample the multi-hop neighborhood to improve GNN computation efficiency, none
of them explicitly set the GNN depth to be larger than k.
For example, GraphSAGE also uses a k-hop sampler. In their case, the sampling improves the
computation speed at the cost of accuracy drop. In our case, such a sampling process can lead to
both accuracy gain and computation efficiency improvement - This is partially due to the exclusion
of noisy neighbors by the sampler, and partially due to the compensation of the information loss by
the higher expressivity of deeper models. More importantly, in the GraphSAGE design, the sampling
depth k grows with the model depth. Thus, “neighbor explosion” is inevitable unless the sampling
budget b is set to 1 (in such a case, the sampler will likely return a path, which does not contain
much information).
Another difference between our k-hop sampler and that of GraphSAGE is that SHADOW-GNN
runs on the induced subgraph of the k-hop neighbors. Compared with the normal GraphSAGE,
a shaDow-SAGE is able to discover more complicated structural information from the subgraph
constructed by our k-hop sampler (See Theorem 3.3).
PPR sampler. In Sections 3.1 and 3.3, we have made detailed comparison between PPR-based
shaDow-GNN and related works such as Klicpera et al. (2019a;b); Bojchevski et al. (2020). Here
we would like to highlight that the approximate PPR algorithm proposed in Andersen et al. (2006)
is both scalable and efficient. The number of nodes we need to visit to obtain the approximate PPR
vector is much smaller than the graph size. In addition, each visit only involves a scalar update,
which is orders of magnitude cheaper than the cost of neighbor propagation in a GNN. Quanti-
tatively, for the three largest datasets, Reddit, Yelp and ogbn-products, for each target node on
average, the number of nodes touched by the PPR computation is comparable to the full 2-hop neigh-
borhood size, as shown in Table 4. The statistics of Table 4 also explains the empirical execution
time measurement presented in Figure 2.
Table 4: Average number of nodes touched by the approximate PPR computation
Dataset	Average 2-hop size	Average # nodes touched by PPR
Reddit	11093	27751
Yelp	2472	5575
ogbn-products	3961	5405
D Detailed Experimental S etup
D. 1 Additional Dataset Details
The statistics for the five benchmark graphs is listed in Table 5. Note that for Yelp, each node
may have multiple labels, and thus we follow the original paper (Zeng et al., 2020) to report its
16
Under review as a conference paper at ICLR 2021
F1-micro score. For all the other graphs, a node is only associated with a single label, and so we
report accuracy. Note that for Reddit and Flickr, other papers (Hamilton et al., 2017a; Zeng et al.,
2020) also report F1-micro score as the metric. However, since each node only has a single label,
“F1-micro score” is exactly the same as “accuracy”.
Table 5: Dataset statistics
Dataset	Setting	Nodes	Edges	Degree	Feature	Classes	Train / Val / Test
Flickr	Inductive	89,250	899,756	10	500	7	0.50 / 0.25 / 0.25
Reddit	Inductive	232,965	11,606,919	50	602	41	0.66 / 0.10 / 0.24
Yelp	Inductive	716,847	6,977,410	10	300	100	0.75 / 0.10 / 0.15
ogbn-arxiv	Transductive	169,343	1,166,243	7	128	40	0.54 / 0.18 / 0.29
ogbn-products	Transductive	2,449,029	61,859,140	25	100	47	0.10 / 0.02 / 0.88
D.2 Hardware Specification and Environment
We run our experiments on a DGX1 machine with Dual 20-core Intel Xeon CPUs (E5-2698 v4
@ 2.2Ghz) and eight NVIDIA Tesla P100 GPUs (15GB of HBM2 memory). The main memory is
512GB DDR4 memory. The code is written in Python 3.6.8 (where the sampling part is written
with C++ parallelized by OpenMP, and the interface between C++ and Python is via PyBind11).
We use PyTorch 1.4.0 on CUDA 10.2 with CUDNN 7.2.1 to train the model on GPU.
When measuring the execution time of Figure 2 and 3, we use the CPU (40-cores in total) to sample
subgraphs in parallel and the GPU to execute the GNN layer operations.
D.3 Hyperparameter Searching Procedure
For all the experiments, as described in the “setup” paragraph of Section 4, we set the hidden di-
mension to be always d(`) = 256. In addition, for all the GAT and SHADOW-GAT experiments, we
set the number of attention heads to be t = 4. For all the GIN and SHADOW-GIN experiments, we
use a 2-layer MLP (with hidden dimension 256) to perform the injective mapping in each GIN layer.
For all the JK-Net and shaDow-JK experiments, we use the concatenation operation to aggregate
the hidden features of each layer in the JK layer.
For all the baseline and shaDow-GNN experiments, we use Adam optimizer (Kingma & Ba, 2014).
We perform grid search on the hyperparameter space defined by:
•	Activation function: {ReLU, ELU}
•	Dropout: {0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50}
•	DropEdge: {0.00, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50}
•	Learning rate: {0.01, 0.002, 0.001, 0.0002, 0.0001, 0.00002}
The sampling hyperparameters are tuned as follows.
For the PPR sampler, we consider two versions: one based on fixed sampling budget p and the other
based on PPR score thresholding θ.
•	If with fixed budget, we then we disable the θ thresholding. We tune the budget by θ ∈
{100, 125, 150, 175, 200}.
•	If with thresholding, we set θ ∈ {0.01, 0.05}. We still have an upper bound p on the
subgraph size. So if there are q nodes in the neighborhood with PPR score larger than θ,
the final subgraph size would be max{p, q}. Such an upper bound eliminates the corner
cases which may cause hardware inefficiency due to very large q. In this case, we set p to
be either 200 or 500 depending on the graph.
For the k-hop sampler, we define the hyperparameter space as:
•	Depth k = 2
17
Under review as a conference paper at ICLR 2021
Table 6: Training configuration of s haD ow- GNN for Table 1 (PPR sampler)							
Arch.	Dataset	Layers	Learning Rate	Dropout	DropEdge	Budget p	Threshold θ
	Flickr	3	0.001	0.40	0.10	200	-
		5	0.001	0.40	0.10	200	-
	Reddit	3	0.00002	0.20	0.05	150	-
		5	0.00002	0.20	0.05	150	-
shaDow-GCN	Yelp	3	0.001	0.10	0.00	100	-
		5	0.001	0.10	0.00	100	-
	ogbn-arxiv	3	0.00002	0.25	0.10	200	-
		5	0.00002	0.25	0.10	200	-
	ogbn-products	3	0.002	0.40	0.05	150	-
		5	0.002	0.40	0.05	150	-
	Flickr	3	0.001	0.40	0.00	200	-
		5	0.001	0.40	0.00	200	-
	Reddit	3	0.0002	0.20	0.10	150	-
		5	0.0002	0.20	0.10	150	-
shaDow-SAGE	Yelp	3	0.001	0.10	0.00	200	-
		5	0.001	0.10	0.00	200	-
	ogbn-arxiv	3	0.0001	0.30	0.10	500	0.01
		5	0.00002	0.25	0.15	200	0.01
	ogbn-products	3	0.002	0.40	0.15	150	-
		5	0.002	0.40	0.15	150	-
	Flickr	3	0.001	0.40	0.00	200	-
		5	0.001	0.40	0.00	200	-
	Reddit	3	0.0001	0.20	0.00	150	-
		5	0.0001	0.20	0.00	150	-
shaDow-GAT	Yelp	3	0.001	0.10	0.00	100	-
		5	0.001	0.10	0.00	100	-
	ogbn-arxiv	3	0.0001	0.20	0.00	175	-
		5	0.0001	0.20	0.00	175	-
	ogbn-products	3	0.001	0.35	0.00	150	-
		5	0.001	0.35	0.00	150	-
Table 7: Training configuration of SHADOW-GNN for Table 1 (k-hop sampler)
Arch.	Dataset	Layers	Learning Rate	Dropout	DropEdge	Budget b	Depth k
	Flickr	3	0.001	0.40	0.00	20	2
		5	0.001	0.40	0.00	20	2
	Reddit	3	0.0001	0.20	0.00	15	2
		5	0.0001	0.20	0.00	15	2
shaDow-SAGE	Yelp	3	0.01	0.10	0.00	5	2
		5	0.01	0.10	0.00	5	2
	ogbn-arxiv	3	0.0001	0.20	0.00	10	2
		5	0.0001	0.20	0.00	10	2
	ogbn-products	3	0.002	0.35	0.00	10	2
		5	0.002	0.35	0.00	10	2
•	Budget b ∈ {5, 10, 15, 20}
The hyperparameters to reproduce the Table 1 shaDow-GNN results are listed in Tables 6 and 7.
The code will be shared with the reviewers at the end of the rebuttal period and released to public
after paper acceptance.
18
Under review as a conference paper at ICLR 2021
D.4 Setup of the Experiments on Deeper Models
SGC and shaDow-SGC. Following Wu et al. (2019), we compute the SGC model as Y =
Softmax AeKpXXW) where A = D-2 AD- 2 and A = I+A. Matrix W is the only learnable
parameter. K is the power on the adjacency matrix and we vary it as K ∈ {1, 3, 5, 10, 20, 30, 40} in
the Figure 4 experiments. For shaDow-SGC, according to Algorithm 1, we compute the embed-
ding for target v as yv = softmax Ae[Kv]X[v]W .
SGC and SHADOW-SGC are trained with the same hyperparameters (i.e., learning rate equals
0.001 and dropout equals 0.1, across all datasets). shaDow-SGC uses the same sampler as the
shaDow-GCN model in Table 1. In the legend of Figure 4, due to lack of space, we use s-SGC
to denote shaDow-SGC. We use “F”, “R” and “A” to denote the datasets of Flickr, Reddit and
ogbn-arxiv respectively.
shaDow-GCN and GCN-SAINT. For SHADOW-GCN and GCN-SAINT each, we first tune
the 5-layer version to obtain the best-performing hyperparameter settings. Then we fix such hy-
perparameters and change the number of layers to up to 15. Note that GraphSAINT also performs
subgraph sampling (during training only). As specified in the “Setup” of Section 4, we use the
random-walk sampler of GraphSAINT. We further tune the GraphSAINT sampling parameters to
optimize its convergence quality. We obverse that GraphSAINT requires a relatively large batch size
to perform well (i.e., number of subgraph nodes is around 20,000). This is also consistent with the
configuration for ogbn-products in the official GraphSAINT repository.
One more thing to notice is that in the official GraphSAINT implementation, it evaluates the valida-
tion set performance as follows:
1)	. Propagate all the GNN layers using the full graph adjacency matrix (including all the train-
ing, validation and test nodes);
2)	. Compute loss on the full graph. Use this loss as an approximation of the validation loss.
3)	. Mask out the predictions on the validation nodes and compute the validation accuracy.
Such a way of loss computation is due to the specific implementation design choices of Graph-
SAINT. In the GraphSAINT official repository, it has clarified that such a way of computing the
validation loss is not very accurate. However, since the validation loss is neither used for back-
propagation nor as the stopping criteria, such an approximate loss computation does not affect the
optimization procedure at all. On the other hand, the impact to our experiments is that for the middle
plot of Figure 4, the scale of the GraphSAINT loss curves may need to be adjusted to better reflect
the actual validation loss values. Note that such an adjustment, if performed, would equally affect
all the GraphSAINT curves for different number of layers. Therefore, we can still conclude that
increasing the number of layers have a significant negative effect on the GCN-SAINT convergence
(this conclusion can also be confirmed by the right plot of Figure 4, where the validation accuracy
of GCN-SAINT is not obtained by approximation).
E Additional Experimental Results
E.1 Is the PPR Sampler Really “Shallow” ?
The PPR sampler defines the neighborhood V[v] based on the magnitude of the PPR scores πev . Al-
though it is possible that a node far away from the target v may have a high PPR score and be selected
by SAMPLE, such a case rarely happens in practice. Most of the nodes in V[v] are concentrated within
the 2-hop neighborhood ofv. Let dist (v, u) denote the shortest path distance between nodes u and
v. Figure 6 shows the distribution of dist (v, u) for u ∈ V[v].
On the other hand, Figure 7 shows the composition of the neighborhood under the normal GNN. It
is clear that most of the nodes propagated by the normal GNN are far away from the target node.
19
Under review as a conference paper at ICLR 2021
186420
............................
0000
]v[V ni sedon fo oitaR
Figure 7: 4-hop sampler for normal 4-layer
GNN: Composition of nodes u in V[v] under var-
ious dist (u, v) = k
3 k ≥ 4
1864
...
000
]v[V ni sedon fo oitaR
Figure 6: PPR sampler for 5-layer SHADOW-
GNN: Composition of nodes u in V[v] under
various dist (u, v) = k
E.2 Example on Subgraph Ensemble
—PPR+1-hop
40
30
20
0
427
.7 .7 0.
00
ycaruccA noitadilaV
s
ch
oc
p
E
0
1
Figure 8: Subgraph ensemble: shaDow-SAGE on ogbn-arxiv
In addition to the Table 3 results on shaDow-GCN, Figure 8 further shows how subgraph ensemble
improves convergence quality with the shaDow-SAGE model. We consider a PPR sampler with
fixed threshold θ = 0.01, a 1-hop sampler (which returns the full 1-hop neighbors) and the ensemble
of these two. The 1-hop sampler incurs significant information loss since it preserves no multi-hop
neighbors. As expected, its accuracy is low. Surprisingly, we observe accuracy gain when we
ensemble the 1-hop sampler and the PPR sampler by Equation 3. As discussed, different samplers
preserve different kinds of local information. Rather than trying to design one “perfect” sampler, it
may be more reasonable to ensemble a few simpler ones.
20