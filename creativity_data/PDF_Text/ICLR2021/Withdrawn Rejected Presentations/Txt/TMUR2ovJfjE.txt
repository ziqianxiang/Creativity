Under review as a conference paper at ICLR 2021
Co-complexity:	An Extended Perspective on
Generalization Error
Anonymous authors
Paper under double-blind review
Ab stract
It is well known that the complexity of a classifier’s function space controls its gen-
eralization gap, with two important examples being VC-dimension and Rademacher
complexity (R-Complexity). We note that these traditional generalization error
bounds consider the ground truth label generating function (LGF) to be fixed. How-
ever, if we consider a scenario where the LGF has no constraints at all, then the
true generalization error can be large, irrespective of training performance, as the
values of the LGF on unseen data points can be largely independent of the values
on the training data. To account for this, in this work, we consider an extended
characterization of the problem, where the ground truth labels are generated by a
function within another function space, which we call the generator space. We
find that the generalization gap in this scenario depends on the R-Complexity of
both the classifier and the generator function spaces. Thus, we find that, even if
the R-Complexity of the classifier is low and it has a good training fit, a highly
complex generator space could worsen generalization performance, in accordance
with the no free lunch theorem. Furthermore, the characterization of a generator
space allows us to model constraints, such as invariances (translation and scale
in vision) or local smoothness. Subsequently, we propose a joint entropy-like
measure of complexity between function spaces (classifier and generator), called
co-complexity, which leads to tighter bounds on the generalization error in this
setting. Co-complexity captures the similarities between the classifier and gener-
ator spaces. It can be decomposed into an invariance co-complexity term, which
measures the extent to which the classifier respects the invariant transformations in
the generator, and a dissociation co-complexity term, which measures the ability of
the classifier to differentiate separate categories in the generator. Our major finding
is that reducing the invariance co-complexity of a classifier, while maintaining
its dissociation co-complexity, improves the training error and reduces the gener-
alization gap. Furthermore, our results, when specialized to the previous setting
where the LGF is fixed, lead to potentially tighter generalization error bounds.
Theoretical results are supported by empirical validation on the CNN architecture
and its transformation-equivariant extensions. Co-complexity showcases a new
side to the generalization abilities of classifiers and can potentially be used to
improve their design.
1	Introduction
In the context of supervised classification, a major factor for consideration is the generalization error
of the classifier, i.e., how good a classifier generalizes to test (unseen) data points. The notion of
overfitting describes the case when the test error significantly exceeds the training error. Naturally,
the objective for building a robust classifier entails the minimization of this generalization gap, to
avoid overfitting. To that end, statistical studies on generalization error (Blumer et al. (1989); Bartlett
& Mendelson (2003)) find that complexity measures on the classifier function space, F , often directly
control the generalization gap of a classifier. Two prominent examples of such measures include the
Rademacher Complexity Rm (F) (Bartlett & Mendelson (2003)) and the VC dimension (Blumer
et al. (1989)) V C(F). Both measures directly estimate the flexibility of a function space, i.e., how
likely is it for F to contain functions that can fit any random labelling over a set of data points. In this
paper, we work with Rademacher complexity and propose extensions that provide a new perspective
on generalization error.
From a statistical perspective, the generalization gap can be understood through convergence bounds
on the error function, i.e., the expected deviation of the error function on the test data compared to the
1
Under review as a conference paper at ICLR 2021
training data. Traditional generalization error bounds (Bartlett & Mendelson, 2003) state that function
complexity (i.e., Rm (F)) directly corresponds to the generalization gap. Thus, higher Rm (F)
usually leads to a greater generalization gap and slower convergence. Although the original Rm (F)
was proposed for binary classification, similar results have been shown for multi-class settings and a
larger variety of loss functions (Xu et al., 2016; Liao et al., 2018). Note that Rm(F) is over the entire
function space and thus global in nature. Local forms of Rademacher complexity, which involve
restricting the function space and lead to minimum error on the training data samples, have been
proposed (Bartlett et al., 2005; 2002). Apart from function complexity based measures, there is also
considerable work which uses an information theoretic perspective (Xu & Raginsky, 2017; Russo &
Zou, 2020; Bu et al., 2020; Haghifam et al., 2020) in treating the subject.
1.1	Why the Ground Truth Label Generating Function Matters
We define the label generating function for a classification problem as the function which generates
the true labels for all possible datapoints. Note that most generalization error bounds, including the
traditional ones, primarily are introspective in nature, i.e., they consider the size and flexibility of the
classifier’s function space F. The main direction proposed in this work is the investigation of the
unknowability of the ground truth label generating functions (LGF), using another function space
which we call the generator space.
The generalization error bounds in Bartlett & Mendelson (2003) state that the difference in test and
training performance is roughly bounded above by the Rademacher Complexity of the classifier’s
function space (i.e., Rm(F)). In other words, whatever the training error, the test error will always
be likely to be greater by an amount Rm(F) on average. We note that, in deriving the original bound,
a major assumption is that the LGF g is fixed and knowable from the data.
We now outline our main argument for taking the generator space into account. The LGF is indeed
fixed, i.e., there cannot be two different ground truth label generating functions applicable to the same
problem. However, our primary emphasis is on the fact that the true LGF will always be unknown,
i.e., for any finite training data containing data-label pairs (zi, g(zi)), we would only truly know the
output of the label generating function on the given training data samples. Only when we have infinite
training data samples, the values of LGF are known at each z ∈ Rd . In this work, we denote the
function space of all possible LGFs, within which the true LGF is contained, as the generator space.
Note that the generator space arises due to the unknowability of the LGF. We show in this work that
due to the generator space, the true generalization gap is greater than the Rademacher complexity of
the classifier, and also depends on the Rademacher complexity of the generator. Note that the size of
the generator space, which dictates its complexity, will be dependent on the amount of constraints
that the LGF has: if the LGF has no constraints at all, then generator spaces are larger, whereas if the
LGF is constrained to be smooth/invariant to many transformations, generator spaces are smaller (as
the set of functions which are smooth and invariant are also much smaller, for instance in vision).
Let us consider the case where the LGF g has no constraints at all, i.e., it is sampled from a generator
space G which contains all possible functions g : Rd -→ {-1, 1}. In this case, the function g is
expected to have no structure and behaves like a random function, and thus the expected test accuracy
of any classifier will be 50% (i.e., random chance). Therefore, even if the classifier function f ∈ F
produces a very good fit on the training data and F happens to have a low complexity measure
Rm(F), the generalization performance will still be poor, as no knowledge of the LGF values on
the unseen datapoints is available from the training samples. This is in contrast to the generalization
error bounds based on Rademacher complexity, which would estimate that the classifier should have
good generalization performance (i.e., low test error), as both Rm (F) and training error are low.
Note that although a typical training dataset extracted from this LGF may be hard to fit using a
low-complexity classifier F, there will be examples of training instances with non-zero probability
on which a low-complexity classifier can have a good fit. The takeaway from this example is that the
structure of the data (here represented using the complexity of the generator space) can additionally
dictate whether a classifier can generalize. Note that, in this scenario, the expected generalization
performance would be better if the LGF had more structure.
Figure 1 illustrates the role of both generator and classifier spaces in generalization via the two
example scenarios discussed earlier. In both examples, the same low-complexity classifier has a
good fit on the training data, thus Rm (F) is low. In example (a), the LGF has no constraints; while,
in example (b), the LGF has constraints such as local smoothness. In example (a), the classifier
2
Under review as a conference paper at ICLR 2021
Figure 1: Two examples where the same low-complexity classifier shows similar fit on training
samples, but in (a) the LGF has no constraints (large generator space) and in (b) the LGF is constrained
by smoothness and other invariances (small generator space).
clearly exhibits poor generalization performance on the test data. This agrees with our previous
argument that if the LGF has no constraints, generalization is essentially impossible. In example (b),
the classifier shows much more robust generalization performance on the test data, due to the LGF
having significantly more structure.
Furthermore, it is intuitively clear that a classifier function space F which has a high overlap with the
generator space G (and therefore its constraints) should yield good generalization performance. This
shows, that in addition to the function spaces F and G individually affecting the generalization gap,
the similarities between F and G are also important. Both of these perspectives play leading roles
in our construction of generator-and-classifier aware complexity measures and the associated novel
generalization error bounds.
1.2	Related Work
To the best of our knowledge, the approach we are proposing to study generalization performance
is novel and there is not much directly related work. Here, we describe some examples of works
which discuss relevant concepts. The no free lunch theorem proposed in Wolpert & Macready (1997)
indirectly sheds light on the behaviour of the LGFs. However, it does not incorporate ways to reduce
variability in the LGFs by considering constraints related to the classification problem. Invariance
constraints in learning algorithms were studied in Sokolic et al. (2017), where the input space was
factored into invariant transformations, similar to what we also do in this work. In doing so, the
complexity of the data was indirectly explored, based on the number of invariant transformations
present in the input space. However, the generalization bounds were derived with an assumption of
perfectly invariant classifier function spaces, which is not applicable for CNNs and their variants (as
shown in Kauderer-Abrams (2017)). In another relevant study (Jin et al. (2019)), a cover-complexity
measure of a single dataset was proposed, and the generalization error of fully connected networks
was analyzed with respect to the same. However, invariances in the dataset and the learning algorithm
were neglected, i.e., the similarities between the generator and the classifier spaces were not studied.
1.3	Key Contributions
The contributions of this work are as follows:
1.	We propose a novel complexity measure between the classifier function space F and the generator
function space G called co-complexity, which we use to derive new, more accurate global estimates
of generator-aware generalization error bounds (Theorem 3 and 4). Co-complexity considers
not only the complexity of the generator and classifier function spaces, but also the similarities
between them. Doing so allows for the a more exhaustive look into generalization error.
2.	We decompose co-complexity into two different measures of complexity, invariance co-complexity
and dissociation co-complexity, which are used to derive new generalization error bounds, in-
cluding bounds on the expected training error (Theorem 6). We find that reducing invariance
co-complexity while keeping the dissociation co-complexity unchanged, helps reduce the gener-
alization gap (low variance) while maintaining low training error (low bias). This emphasizes
3
Under review as a conference paper at ICLR 2021
the importance of having classifiers that share invariance properties with generator spaces, e.g.,
rotation-invariant CNNs (Cohen & Welling (2016b)) on MNIST-Rot (Larochelle et al. (2007)).
3.	We present empirical validation of co-complexity measures of CNN and its scale-equivariant
and rotation-equivariant counterparts (SE-CNN in Sosnovik et al. (2019), RE-CNN in Cohen &
Welling (2016b)), which explains their superior generalization ability compared to MLPs.
Our proposed error bounds are easily specialized to the case where ground truth label function is
fixed, leading to potentially tighter generalization error bounds (see Appendix A). Although our
proposed measures are global in nature, local variants can be derived via similar extensions used in
Bartlett et al. (2005).
2	Definitions
Assume that we have m number of d-dimensional i.i.d sampled instances S = [z1, z2, .., zm] drawn
from some distribution P, and another set ofm i.i.d sampled instances S0 = [z10 , z20 , .., zm0 ] also drawn
from P. Define σ = [σ1, σ2, .., σm] where σi are i.i.d random variables following the Rademacher
distribution (P r(σi = +1) = P r(σi = -1) = 0.5). F and G are two function spaces from
Rd -→ {-1, 1} and we assume that they are defined at all points in Rd. In the context of our problem,
F will be the classifier’s function space, whereas G will be the generator space.
Rademacher Complexity (Bartlett & Mendelson (2003)): First, we provide the definition of
Rademacher Complexity:
(m X 5」
Rm(F) = E
σ,S
sup
f∈F
(1)
It can be seen that Rm(F) indicates the noisy-label fitting ability of the classifier’s function space F.
Correlated Rademacher Complexity: We propose a modified form of the original Rademacher
complexity, called Correlated Rademacher Complexity, RCm(F), which is defined as follows:
Rm(F)= 2 × ,,ES,卜 F G II f"zT	⑵
We have 0 ≤ RCm(F) ≤ 1/2. We also show that RCm(F) ≤ Rm(F) (see Appendix C.1). It has been
argued that Rm(F) (and therefore RCm (F)) can be considered as ”entropy” measures of the entire
function space F (Anguita et al. (2014)). Note that, like Rm(F), RCm (F) is also eventually depends
on the noisy label fitting ability of F, but computes it via ability of the function space to assign the
same or a different label to two random points zi and zi0 (via f(zi )f (zi0)). Thus we denote it as the
correlated Rademacher complexity of F.
Co-Complexity: Now we propose various complexity measures between two separate function
spaces F and G. Similar to Rm(F) and RCm(F), these measures assess the noisy label fitting abilities
of the union of the function spaces F and G . In doing so, these measures compute a joint-entropy like
metric over the two function spaces. First, we define the Co-complexity between F and G as follows.
Rm(F, G) = 1 ×	Eq0
2 σ,S,S 0
sup
f∈F,g∈G
m
ɪ X
m
i=1
f(zi )f(zi0)g(zi )g(zi0)
(3)
Some of the properties of co-complexity are as follows:
P1 Rm(F, G) = Rm(G, F), i.e., co-complexity is symmetric.
P2 Rm (F, G) ≥ RCm (F) and Rm (F, G) ≥ RCm (G), i.e., the co-complexity between F and G is
always greater than the individual correlated Rademacher complexities of F and G .
P3 Rm(F, G) ≤ RCm(F) + RCm(G) ≤ Rm(F) + Rm(G), i.e., the co-complexity between F and G
is upper bounded by the sum of the Rademacher complexities of F and G.
P4 Rm(F, G) behaves like a joint-entropy measure ofF and G. To see this, let us define Im (F, G) =
RCm (F) + RCm (G) - Rm (F, G), called the mutual co-complexity between F and G. We later
find that Im(F, G) behaves like a mutual-information measure. This, coupled with the fact that
RCm(F) and RCm(G) can be construed as entropy measures of F and G, implies the result.
4
Under review as a conference paper at ICLR 2021
Invariance Co-Complexity: Next, we quantify some of the properties of the ground truth generator
space G in terms of its invariance transformations. We define the Invariance Classes of G as follows:
IC(G) = {τ1(.,θ1),τ2(.,θ2),...,τn(.,θn)},	(4)
where {τi(., θi)∣i = 1, 2,…，n} are functions from Rd → Rd, and θi represents the extent of the
transformation τi. Each θi ∈ R is a scalar and takes on a set of admissible values depending on the
transformation (possibly infinite). The functions are constrained such that, ∀τ ∈ IC (G), ∀g ∈ G,
g(τ (z), t) = g(z) for all data points z ∈ Rd, and for all admissible values of the transformation
parameter t. Also, note that setting θi = 0 leads to the identity function, i.e., τi(z, 0) = 1 for all
z ∈ Rd and all i.
Based on the above we define a transformation-indicator function I(z, zi), such that I(z, zi) = 1
if z = τ1(., t1) ◦ τ2(., t2) ◦ . ◦ τn(., tn)(zi), for a certain t1, t2, ...tn and otherwise I(z, zi) = 0.
Additionally, with respect to a generator space G, we also define the invariance extended set of a
datapoint zi ∈ Rd as follows:
IG (zi) = {z ∈ Rd | I(z, zi) = 1}	(5)
Note that that invariance extended set of z will always have the same ground truth label as z. Next,
we define the Invariance Co-complexity between two function spaces F and G as follows:
Rm(F, G ) = 2 × SE
2 S,S
sup
f∈F
f(zi)f(zi0)
,where Zi 〜P(Z)I(z, zi), ∀i .
(6)
Note that RIm (F, G) ≤ 1. Each data point Zi0 in S0 is contained within the invariance extended set
IG(Zi) of Zi, and is sampled according to the un-normalized distribution P(Z)I(Z, Zi) over Z ∈ Rd.
The invariance co-complexity between F and G indicates the degree to which F obeys the invariance
transformations within G. For instance, a low RIm(F, G) would indicate that f(Zi)f(Zi0) = 1 (i.e.,
f(Zi) = f (Zi0)) for most Zi and Zi0 which are related via some invariance transformation in G.
Dissociation Co-Complexity: We now define the Dissociation Co-complexity between F and G
when the corresponding datapoints in S and S0 are not related by any invariance transformation. That
is, for all Zi ∈ S and Zi0 ∈ S0, Zi0 6∈ IG (Zi), and is sampled from the distribution P (Z)(1 - I(Z, Zi))
over Z ∈ Rd . The dissociation co-complexity can then be defined as:
Rm(F, G) = 1 × E o
2	σ,S,S0
SUp (m1 X f(zi)f(zi)σj j , where Zi 〜P(Z)(I- I(z,Zi)), ∀i. (7)
Note that RDm(F, G) measures the average flexibility of F in its label assignment to any two points Zi
and Zi0 which are not related via any invariance transformation in G. Thus, a larger RDm (F, G) would
indicate that the classifier function space F is able to easily assign separate labels to datapoints which
are not related via any invariance transformation in G. We also define a variant of RDm (F, G), in
which S contains only one instance, instead of m instances, denoted as RDm,1 (F, G). RDm,1 (F, G) is
defined as follows.
Rmj(F, G) = 1 × e
2	σ,zo 〜P,S0
sup 1— X f (zo)f (z'i)σi∖ , where z'i - P(Z)(I- I (z,zo)), ∀i.
f∈F m i=1
(8)
Note that RDm(F, G) ≤ 0.5 and RDm,1(F, G) ≤ 0.5.
Rademacher Smoothness: For our final result in Theorem 6, we assume that the function space F
is Rademacher smooth w.r.t. G. We define Rademacher Smoothness as follows. If F is Rademacher
smooth w.r.t. G, then any quantity of the form Es,s，IsUPf ∈f (mm Pm=I f (Zi)f (z0 )g(Zi )g(Zi)σ,] or
Es,so [supf∈F,g∈G (m Pm=I f (Zi)f (z0)g(Zi)g(Zi)σ,], where the expectation is over datapoint per-
mutations S, S0, should lie between the two following cases : (i) S0 ∈ IG (S) (i.e., Zi0 - P(Z)(I(Z, Zi))
for all i) and (ii) S0 ∈/ IG(S) (i.e., Zi0 - P(Z)(1 - I(Z, Zi)) for all i). Then, for some non-zero value
5
Under review as a conference paper at ICLR 2021
of α, 0 ≤ α ≤ 1, the following holds:
SESO 泮m1 (Xf(zi)f(Zi[g(zi)g(Zi)σζjJ
1m
=(α) ss，sE∈is(S) sup m Xf(Zif(Z0)g(Zi)g(Zi)
S,S ,S ∈IG (S) f ∈F m i=1
+(1 -α) sso SE I (S) sup G1 X f(Z” (ZQg(ZIg(Z )σi).	⑼
s,s ,s /IG(S) f∈F ∖ ιm i=ι	)
Similarly, the same applies to Es,s，IsUPf ∈f,g∈g (m1 Pm=I f ⑵)/(Zi)g(Zi)g(Zi)σ,].
3 Theoretical Results
We now present a series of results which extend the generalization framework using generator spaces.
The proofs of all our results are available in Appendix C. For the following results, we assume that
the classifier’s function space F contains the constant function f(Z) = c ∀Z, c ∈ {-1, 1}.
We use the definitions provided in the previous section. Specifically, F is be the classifier’s
function space and G is the generator space. Given that the data labels are generated by
g ∈ G, we extend the definition of the sampled instances S by adding the output labels, i.e.,
S = [(Z1, g(Z1)), (Z2, g(Z2)), .., (Zm , g(Zm ))]. Then for f ∈ F, we denote the 0-1 loss on S by
m
ecrrS (f) = X
i=1
(1 - f(Zi)g(Zi))
2m
(10)
We also denote the generalization error over the data samples generated by distribution P as
N
errP (f)
lim
N →∞,zi 〜P
Σ
i=1
(1 - f(Zi )g(Zi))
2N
(11)
First, we describe the generalization error bound originally proposed in Bartlett & Mendelson (2003).
Theorem 1. (Bartlett & Mendelson (2003)) For 0 < δ < 1, with probability p ≥ 1 - δ, we have
errP(f) ≤ CS(f) + Rm(F) + \耳叵.	(12)
2m
The above theorem assumes that the ground truth function is completely knowable from the given
training examples, and therefore the generator space contains only a single element (Bartlett &
Mendelson (2003)). For all of our results that follow, we assume the extended case where the
generator space can contain more than one element due to the unknowability of the LGF, and
construct generalization error bounds that consider the complexity of G as well. Also note that the
following results hold for every choice of f ∈ F and g ∈ G . First we present the extended Theorem
1 when the LGF is unknowable (Rm (G) > 0).
Theorem 2.	For 0 < δ< 1, with probability p ≥ 1 - δ, we have,
errP(f) ≤ ecS(f) + Rm(F) + Rm(G) + Jlog)(I").	(13)
2m
Remark 1. This result states that the complexity of the generator space also directly contributes
towards increasing the generalization gap. In problems where the LGF is known to be heavily
constrained and structured, Rm(G) will be low, reducing the expected generalization gap of classifiers,
and vice-versa. Note that in the knowable ground truth scenario where G has a single element, we
have Rm(G) = 0, which returns the error bound in Theorem 1. This result shows that larger
complexity of G results in a greater generalization gap. We demonstrate this with neural network
generator spaces of varying complexity (see Appendix B.3). Intuitively, the additional Rm (G) term
comes from the fact that given the training data labels, there still exists a subspace of functions in G
which are potential candidates for the ground truth function.
6
Under review as a conference paper at ICLR 2021
The following result presents tightens Theorem 2 using co-complexity.
Theorem 3.	For 0 < δ < 1, with probability p ≥ 1 - δ, we have
errP (f) ≤ CS(f) + Rm (F, G) + JlogMl.	(14)
2m
Remark 2. Co-complexity measures the degree of similarity between F and G, instead of simply
adding their respective complexities, and therefore is tighter than Theorem 2 (co-complexity P3). We
also find that the generalization gap, for the case when the roles are reversed, is unchanged (see
corollary 3.1 in Appendix C). This demonstrates an inherent symmetry in the problem. Also note that
Theorem 3 leads to potentially tighter bounds when specialized to the conventional setting where
Rm(G) = 0 (Appendix A).
The following result outlines a lower bound on the generalization error, using co-complexity.
Theorem 4.	For 0 < δ< 1, with probability p ≥ 1- δ,
errP(f) ≥ CS(f) - Rm(F, G) - ∖/ɪɑgj17^).	(15)
2m
Remark 3. This result, coupled with Theorem 3, demonstrates that if one interprets the eCrrS (f) term
as the bias of the classifier, Rm(F, G) can be interpreted as the variance.
The following result addresses the joint-entropy like behaviour of Rm(F, G).
Theorem 5.	We are given the mutual complexity measure Im(F, G) as defined in co-complexity
P4. Consider another ground truth generator space G0, such thatIm(G0, G) = 0, i.e., G and G0 are
independent. Then we have,
Rm(F) ≥Im(F,G)+Im(F,G0).	(16)
Remark 4. Let H(X) represents the Shannon entropy of the random variable X and I(X; Y )
represents the mutual information between random variables X and Y . Then, it is known that
H(X) ≥ I(X; Y ) + I(X; Y 0), when Y and Y 0 are independent random variables (I(Y ; Y0) = 0).
This observation, coupled with the fact that RCm(F) can be considered as an entropy measure of the
function space (see section 2) indicates that the quantity Im(F, G) behaves like a mutual information
estimate between the function spaces F and G. Furthermore, as Rm(F, G) = RCm(F) + RCm (G) -
Im(F, G), we can see that Rm(F, G) behaves as a joint entropy measure of two function spaces.
The following result demonstrates how the co-complexity measure Rm(F, G) can be decomposed
into separate co-complexity measures.
Lemma 1. Consider function spaces F and G, such that F is Rademacher smooth w.r.t. G. Let us
define R(CD(G) as 2 × Eσ,s,so,s0∈ig(S) IsUpg∈g (m∙ ∑m=ι g(zi)g(z0)σi)]∙ Forsomenon-negative
real constant 0 ≤ α ≤ 1, we then have
Rm(F,G) ≤ αRIm(F, G) + (1 - α)(RDm(F, G) + RCm,D(G)),	(17)
where RIm(F, G) and RDm(F, G) are the invariance and dissociation co-complexity, respectively∙
Remark 5. This decomposition allows us to differentiate the impact of RIm(F, G) and RDm(F, G)
on the generalization error bound∙ Note that value of α here will be proportional to the cardinality of
the invariance transformation classes IG of G∙
Using this, we proceed to our final result, where we express the generalization error bound in Theorem
3, in terms of the invariance and dissociation co-complexities. For purposes of simplification, we
denote RIm(F, G) as RIm, RDm(F, G) as RDm and similarly for RDm,1.
Theorem 6.	Consider function spaces F and G, such that F is Rademacher smooth w∙r∙t∙ G∙ Let
errP denote the generalization error for the functions f ∈ F which showcase the best fit on the
training data samples (averaged across all g in G)∙ RCm,D (G) is defined in Lemma 1∙ For some
non-negative real constants 0 ≤ α, β ≤ 1 and 0 < δ< 1, with probability p ≥ 1 - δ, we have
errp ≤ (ι -β) Q—Rm,1) + αRm+(ι - α)(Rm+RmD(G))+，，: /ɪɪ (18)
7
Under review as a conference paper at ICLR 2021
Remark 6. The first term in the generalization error bound in (18) represents an upper bound on the
average training error of the function which best fits the training data. We find that smaller RI leads
to a smaller generalization gap (variance), while keeping the training error unchanged. However,
the same cannot be said for the dissociation co-complexity RD. Thus, for a fixed RD and RD,1,
classifier function spaces with smaller RI will lead to smaller generalization error. Also note that
when the generator space only contains a single element, RCm,D (G) = 0.
The next proposition gives interpretable bounds on invariance and dissociation co-complexity. Let
z = z1, z2, ..., zm, and z’ = z10 , ..., zm0 , where zk, zk0 ∈ Rd ∀ k. Define the well known growth
function of binary valued functions F as A = ΠF (m) = maxz |{f (zk) : k = 1, 2, ..., m, f ∈ F}|.
Define the invariance-constrained and dissociation-constrained growth functions of F w.r.t. G as:
B = πF∣g(M=	, max、uJ{f (zk),f (zk) : k = 1, 2,…,m,f ∈ F}|
z,z’,zi0 ∈IG (zi),∀i
C = πd∣g (Tm) = 0z0mGU∀i1{f (zk ),f (Zk):k = 1，2,…,m,f ∈F}l
Proposition 1. Define Rm (F, G) =,2 × Eσ,S,S0,S0∈lG (S) IsUpf ∈F (m Pm=ι f (Zi)f (zi )σi)],
where	zi0	∈	IG (zi), ∀i.	Note that	RIm	(F,	G) ≤	RIm (F,	G).	Given the definitions above, we
have Rm(F, G) ≤ qIgB ≤ q ⅛A and Rm(F, G) ≤ q ¾C ≤ q ⅛A.
Remark 7. Reducing ∏F∣ G(m) while keeping ∏F ∣ G (m) Unchangedwould result in low invariance
co-complexity without affecting dissociation co-complexity, which can’t be achieved by simply
reducing ΠF (m) (or the V C(F)). This shows that although RIm and RDm are not completely
independent, careful construction of F makes it possible to reduce RIm while maintaining RDm.
4	Experiments and Discussions
Our experiments (on MNIST and STL-10) explore the implications of Theorem 6, which states that
low RIm while maintaining RmD improves generalization while keeping training error unchanged.
As all of our proposed complexity metrics assume binary classifiers, we create subsets of these
datasets containing only two randomly chosen categories, which converts the problem into a binary
classification scenario.
4.1	Measuring Invariance Co-Complexity
As invariance co-complexity is defined w.r.t. a transformation class (IG), we compute the invariance
co-complexity (in (6)) of various networks shown in Table 1 for four different transformations. For
each transformation τ, we choose the datapoint pairs S and S 0 in (6) such that Zi0 = τ (Zi, t), where t
is the transformation parameter. The parameter t is randomly chosen as follows： 0°-180° for rotation,
0.7-1.2 for scale, 0。-90。for shear angle and 0.5-4 pixels for translation. The computed Rm values
indicate the extent to which these networks naturally allow for invariance to various transformation
types. To approximately compute RIm , we take 1000 randomly weighted networks to construct F
to compute (6). We average the results over 100 batches of data (S and S 0), each containing 1000
examples (m = 1000) from their respective datasets. We compute RIm for multi-layered perceptrons
(MLPs), CNNs and their transformation-equivariant extensions: scale-equivariant CNN (SE-CNN
in Sosnovik et al. (2019)) and rotation-equivariant CNN (RE-CNN in Cohen & Welling (2016b)).
Note that the scale-equivariant CNN and the rotation-equivariant CNN are known to outperform
the vanilla CNN on MNIST and its rotation and scale extensions (Sosnovik et al. (2019); Cohen &
Welling (2016b)). Hence, these transformation-equivariant extensions showcase better generalization
performance. One of the objectives of this experiment is to verify whether the invariance and
dissociation co-complexities of these networks indeed point to better generalization performance, as
is observed in literature. Please see Appendix D for architecture details.
In every case in Table 1, we find that CNNs have lower RIm than MLPs. As expected, we see that
SE-CNN and RE-CNN have even lower RIm . CNNs are primarily expected to be robust to translation,
due to max-pooling layers. However, even for other transformations such as rotation and shear, the
RIm of the CNN is less than that of an MLP. Please see Appendix B.2 for a more detailed analysis on
how invariance co-complexity varies with choice of the transformation parameter.
8
Under review as a conference paper at ICLR 2021
Invariance Co-Complexity (Rm)
Architecture	Rotation		Scale		Shear		Translation	
	MNIST	STL-10	MNIST	STL-10	MNIST	STL-10	MNIST	STL-10
MLP	0.424-	^0393-	0.2585	0.2545	^0409-	^0386-	^0425-	0.2565
CNN	0.3875	0.375	0.2435	0.2205	0.3885	0.3725	0.365	0.213
SE-CNN	n/a	n/a	0.1845	0.1905	^0385-	0.3535	n/a	n/a
RE-CNN	0.357	0.3455	n/a	n/a	0.3385	0.3545	n/a	n/a
Table 1: Mean invariance co-complexity values for all 4 networks over 4 transformations, rotation,
scale, shear, and translation. We approximate the computation of the invariance co-complexity in (6)
by taking 1000 randomly initialized networks in each case (for the supremum in (6)). For fairness, all
architectures share approximately the same number of parameters (around 16k).
	MLP	CNN	RE-CNN	SE-CNN
Dissociation Co-Complexity (MNIST)	0.4415	0.44	^0438	^0438
Dissociation Co-Complexity (STL-10)	0.4295	0.436	0.43	0.4375
Table 2: Mean dissociation co-complexity values for all networks. Note that CNN and its variants
maintain the dissociation co-complexity at a similar level to that of MLP.
4.2	Measuring Dissociation Co-Complexity
Smaller invariance co-complexity can be inconsequential, if the network’s ability to discriminate
between two different categories in G also suffers. Hence, we measure the dissociation co-complexity
(in (8)) to check whether lower values of RmI also affect their discriminatory potential. We compute
RDm similarly to RIm , by using 1000 randomly initialized networks (for the supremum in (8)), and
then averaging the measure for 100 batches of data (S and S0). Table 2 shows the results, where we
find that the values of RDm for all studied networks are very close to each other. Therefore, only the
invariance co-complexity of these architectures will majorly decide the generalization error bounds
in Theorem 6. This shows that, in spite of better invariance capabilities of CNNs (lower RIm) and its
transformation-equivariant extensions, those networks preserve discriminatory potential necessary
for differentiating images that belong to different categories.
5	Reflections
The proposed co-complexity measures lead to an extended perspective on generalization error by
accounting for ground truth generator spaces. The objective of introducing the generator space is to
consider the structure in the ground truth labelling functions. New error bounds are proposed which
consider various aspects of interaction between generator and classifier function spaces.
Co-complexity can be decomposed into two separate components, invariance co-complexity (RIm)
and dissociated co-complexity (RDm), which are found to measure the degree to which the classifier’s
function space obeys the invariance transformations in the data (RIm), and also the degree to which
the classifier is able to differentiate between separate categories in the data (RDm). If we interpret
Rm(F, G) as the variance of the classifier (see Theorems 3 and 4) and the training error as the bias,
we see that RIm and RDm affect the training error (bias) and generalization gap (variance) differently
(see Theorem 6). Theorem 6 outlines a clear objective for reducing variance while maintaining low
bias: reduce RIm while keeping RDm and RmD,1 unchanged. We note that monitoring RIm and RDm can
be useful for finding better learning architectures for a specific problem. Furthermore, it is also clear
that classification problems which contain more invariance constraints, RIm plays a greater role in
controlling the generalization gap (as α will be higher).
Experiments on MNIST and STL-10 reveal that the invariance co-complexity RIm for CNNs and
their transformation-equivariant extensions is always lower than MLPs, while their dissociation
co-complexities, RDm and RDm,1, are comparable to MLPs (see Appendix B.1 for experiments which
measure RmD,1). Furthermore, transformation-equivariant versions of CNN show lower RIm than the
vanilla CNN itself, while maintaining RDm, i.e., they have low variance and low bias. Appendix B.3
presents results when neural networks of varying complexity are used as generator spaces and verifies
that higher complexity generator spaces lead to a larger generalization gap.
9
Under review as a conference paper at ICLR 2021
References
D. Anguita, A. Ghio, L. Oneto, and S. Ridella. A deep connection between the VaPnik-Chervonenkis
entropy and the rademacher complexity. IEEE Transactions on Neural Networks and Learning
Systems, 25(12):2202-2211, 2014.
Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and
structural results. J. Mach. Learn. Res., 3:463-482, March 2003. ISSN 1532-4435.
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Localized rademacher complexities. In
Jyrki Kivinen and Robert H. Sloan (eds.), Computational Learning Theory, pp. 44-58. Springer,
2002.
Peter L. Bartlett, Olivier Bousquet, and Shahar Mendelson. Local rademacher complexities. Ann.
Statist., 33(4):1497-1537, 08 2005. doi: 10.1214/009053605000000282.
Anselm Blumer, A. Ehrenfeucht, David Haussler, and Manfred K. Warmuth. Learnability and the
vapnik-chervonenkis dimension. J. ACM, 36(4):929-965, October 1989. ISSN 0004-5411. doi:
10.1145/76359.76371.
Y. Bu, S. Zou, and V. V. Veeravalli. Tightening mutual information based bounds on generalization
error. IEEE Journal on Selected Areas in Information Theory, pp. 1-1, 2020.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks. In Proceedings of the
33rd International Conference on International Conference on Machine Learning - Volume 48,
ICML’16, pp. 2990-2999. JMLR.org, 2016a.
Taco S. Cohen and Max Welling. Group equivariant convolutional networks. International Conference
on Machine Learning (ICML), 2016b.
Thomas M Cover and Joy A Thomas. Elements of information theory. John Wiley & Sons, 2012.
Mahdi Haghifam, Jeffrey Negrea, Ashish Khisti, Daniel M. Roy, and Gintare Karolina Dziugaite.
Sharpened Generalization Bounds based on Conditional Mutual Information and an Application to
Noisy, Iterative Algorithms. arXiv e-prints, art. arXiv:2004.12983, April 2020.
Pengzhan Jin, Lu Lu, Yifa Tang, and George Em Karniadakis. Quantifying the generalization error
in deep learning in terms of data distribution and neural network smoothness. arXiv e-prints, art.
arXiv:1905.11427, May 2019.
Eric Kauderer-Abrams. Quantifying translation-invariance in convolutional neural networks. arXiv
e-prints, art. arXiv:1801.01450, December 2017.
Hugo Larochelle, Dumitru Erhan, Aaron Courville, James Bergstra, and Yoshua Bengio. An empirical
evaluation of deep architectures on problems with many factors of variation. In Proceedings of
the 24th International Conference on Machine Learning, ICML ’07, pp. 473-480, New York, NY,
USA, 2007. Association for Computing Machinery. ISBN 9781595937933. doi: 10.1145/1273496.
1273556.
Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, and Tomaso Poggio. A Sur-
prising Linear Relationship Predicts Test Performance in Deep Networks. arXiv e-prints, art.
arXiv:1807.09659, July 2018.
Colin McDiarmid. On the method of bounded differences. Surveys in combinatorics, 141(1):148-188,
1989.
D. Russo and J. Zou. How much does your data exploration overfit? controlling bias via information
usage. IEEE Transactions on Information Theory, 66(1):302-323, 2020.
Jure Sokolic, Raja Giryes, Guillermo Sapiro, and Miguel Rodrigues. Generalization Error of Invariant
Classifiers. In Aarti Singh and Jerry Zhu (eds.), Proceedings of the 20th International Conference
on Artificial Intelligence and Statistics, volume 54 of Proceedings of Machine Learning Research,
pp. 1094-1103, Fort Lauderdale, FL, USA, 20-22 Apr 2017. PMLR.
10
Under review as a conference paper at ICLR 2021
Ivan Sosnovik, MichaI Szmaja, and Arnold Smeulders. Scale-Equivariant Steerable Networks. arXiv
e-prints, art. arXiv:1910.11093, October 2019.
D. H. Wolpert and W. G. Macready. No free lunch theorems for optimization. Trans. Evol. Comp, 1
(1):67-82, April 1997. ISSN 1089-778X. doi: 10.1109/4235.585893.
Aolin Xu and Maxim Raginsky. Information-theoretic analysis of generalization capability of learning
algorithms. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp. 2524-2533. Curran
Associates, Inc., 2017.
C. Xu, T. Liu, D. Tao, and C. Xu. Local rademacher complexity for multi-label learning. IEEE
Transactions on Image Processing, 25(3):1495-1507, 2016.
A Appendix - Tightening Existing Results
For the following results, we primarily work in the setting where the ground truth generator space only
contains a single element, which pertains to the conventional treatment of generalization error bounds
with Rademacher complexity. Note that in this setting, we can still define the invariance classes of
G , which we will make use of. Furthermore, we primarily work with the correlated Rademacher
complexity RCm(F), which was defined in section 2. We note that RCm(F) involves two instantiations
of the dataset in S and S0. Furthermore, we also observe that the final RCm (F) depends on how each
datapoint zi in S is paired with the corresponding datapoint zi0 in S0 ,
Rm(F)=I × ,,ES,卜 F G X f(Zif(Zi)%)]	(19)
For the following results, we define the invariance-matched correlated Rademacher complexity
RCminv (F), which chooses a pairing (i, M(i)) between the points in S and S0, such that
M = arg max	|ZM0 (i) ∩ IG(Zi)|.
Mi
(20)
That is, the matching function M is chosen such that there exists the largest number of pairings
Zi , ZM0 (i) where ZM0 (i) is present the invariance extended set of Zi . Thus, the invariance-matched
correlated complexity can then be defined as
Rminv (F) = I × σ,Eso
泮 A X f (Zi)f (ZM ⑴川
(21)
where M is chosen using equation 20. Given this, we have the following theorem.
Theorem 7. Consider the classifier function space F, and the previous defined correlated
Rademacher complexity of F, RCm (F). Furthermore, let us assume that the generator space only
contains a single element, i.e., the conventional setting (Rm(G) = 0). Then we have for anyf ∈ F,
with a probability p ≥ 1 - δ, we have
errP (f) ≤ Cs(f) + Rminv (F) + JIogM).	(22)
m	2m
Proof. First, we will show that for any f ∈ F, with a probability p ≥ 1 - δ,
errp (f) ≤ e≡s(f) + Rm(F) + Jlog^/δ1.	(23)
m	2m
This result trivially follows from Theorem 3, when G contains a single element denoted by g0, as then
11
Under review as a conference paper at ICLR 2021
Rm(F, G) = 1 × E suP 1X X f (zi)f (z'i)g0(zi)g0(z'0 )σi )
2	σ,S,S0	f∈F	m i=1	i	i
=1 × E„, sup 1—Xf(zi)f(z'i)σi}
2	σ,S,S0	f∈F	m i=1	i
= RCm (F)
(24)
(25)
(26)
Note that as RCm(F) ≤ Rm(F), this is a tighter global bound that the original Rademacher bound.
Then, the result immediately follows when we recognize that changing the ordering of zi and zi0 does
not affect the value of the generalization gap in (57). Furthermore, the matching M for each sampled
S, S0 is independent of the choice of the Rademacher variables σi , and thus the subsequent steps
follow in the same way as in (67). Subsequently, we still have with a probability p ≥ 1 - δ,
errp(f) ≤ c(f) + Rminv (F) + Jlog* .	(27)
m	2m
Note that when the invariance co-complexity of F w.r.t. G is smaller than its dissociation co-
complexity, then the above is expected to be a tighter bound than (23). This is because the invariance-
aware matching function M leads to a greater number of pairings zi, zi0 where zi0 ∈ IG(zi) (thus lies
in its invariance extended set).
□
B Appendix - Additional Experiments
B.1	EXPERIMENT: MEASURING DISSOCIATION CO-COMPLEXITY (RDm,1(F, G))
In the main paper, we measure the invariance co-complexities RIm(F, G) and the dissociation co-
complexities RDm(F, G) of four different network types: Multi-layer Perceptron (MLP), Vanilla
CNN (CNN), Scale-Equivariant CNN (SE-CNN in Sosnovik et al. (2019)) and Rotation Equivariant
CNN (RE-CNN in Cohen & Welling (2016b)). In this experiment, we measure the single-sample
dissociation co-complexity RDm,1(F, G) of these networks, as RDm,1 (F, G) also affects the training
error in the generalization error bound in Theorem 6. Please note that the datasets used in our
experiments (MNIST and STL-10), have 10 categories. However, since the invariance and dissociation
co-complexities can only be computed binary classifiers, we use random subsets of the data which
contain only two categories, for computing the value of co-complexity.
Table 3 shows the results. We find that in both datasets, the RDm,1 (F, G) values of the tested networks
are very close to each other. This is consistent with the dissociation co-complexity RDm(F, G),
shown in Table 2 of the main paper. Together with those results, it is clear that only the invariance
co-complexities RIm(F, G) will mainly contribute to the generalization error of these networks, as
the dissociation co-complexities RDm,1 (F, G) and RDm(F, G) do not show any significant variation.
For example, from Table 1 and 2 of the main paper, we see that the invariance co-complexity (rotation)
of the RE-CNN (0.357) is smaller than that of the MLP (0.424), whereas the RDm,1 (F, G) (0.4492 vs.
0.4472) and RDm(F, G) (0.438 vs. 0.4415) of these two networks do not show much variation.
	MLP	CNN	SE-CNN	RE-CNN
Dissociation Co-ComPlexity (Rmm,1(F, G), MNIST)	0.4492	0.4589	0.4479	0.4472
Dissociation Co-Complexity (Rmm,1(F, G), STL-10)	0.4352	0.4465	0.4405	0.4570
Table 3: Mean dissociation co-complexity values for all networks. Note that CNN and its variants
maintain the dissociation co-complexity at a similar level to that of MLP.
12
Under review as a conference paper at ICLR 2021
B.2	Experiment: Measuring Invariance Co-complexity for different
TRANSFORMATION PARAMETER CHOICES
In addition to the experiments which estimate the overall invariance co-complexity of the various
network baselines, we also showcase how RIm depends on the exact transformation parameter t for
each case, in Fig. 3. Note that equivariant networks usually have low RIm for most values of t, except
for the RE-CNN which is tuned to be invariant to rotations of 90。，and therefore only showcases
significant drops in Rm when t is near 90。or 180。.
Figure 2: Invariance co-complexity values for the relevant networks on MNIST, when the datapoint
pairs S and S0 are separated by varying degrees of rotational, scale, shear and translational shifts.
B.3 Experiment: Testing Generalization Error Bounds with Neural Network
Generator Spaces
We consider a setting where F is the space of linear classifiers and the generator space G is the space
of relu-activated neural networks with a single hidden layer containing H hidden neurons. We have
Rm(F) T √1∕m and Rm(G) α HH∕m. Closed form expressions for co-complexity Rm(F, G)
are non-trivial to compute, but we use the property Rm (F, G) ≤ Rm (F) + Rm (G) (Theorem 2).
Theorem 2 implies that only increasing the complexity of G (by increasing H), should lead to a steady
increase in the difference between training and testing error (i.e. the generalization gap). We execute
this scenario with two variations: m = 10 and m = 100. H was chosen from {2, 4, 8, 16, 32}.
Note that the data distribution is kept fixed for all H. Therefore, in this experiment, only the label
generating function is changed.
We observe that the expected generalization gap steadily increases (ranged between 0.6%-8.9%) as
we increase the complexity of the generator class (for both m = 10 and m = 100). Furthermore,
we find a very high degree of correlation (0.982 for m = 10 and 0.976 for m = 100) between the
generalization gap trend predicted by Theorem 2 (≈ ,H∕m) and the empirically observed average
generalization gap. This shows that the complexity of the generator class also directly controls the
generalization gap, when everything else is unchanged. This result clearly highlights that when
additional information about the generator space is known, more accurate trends of generalization
error can be estimated by using both Rm(G) and Rm(F).
C Appendix - Proof of Theoretical Results
In what follows, we provide the proofs of Theorems 2 to 6. The proof of Theorem 1 of the main
paper is available in Bartlett & Mendelson (2003). We also prove, in Lemma 2, the properties of
co-complexity stated in Section 5 of the main paper. We use the definitions provided in Section 5 of
the main paper.
13
Under review as a conference paper at ICLR 2021
(H = Number of Hidden Neurons of Generator)
√77
(a)	nI = Io
(b)	m = 100
Figure 3: Plots depicting the average generalization gap (difference between test and training error)
when 2-layer neural networks of variable hidden neuron number H, are chosen as the label generating
function, and a single layer neural network is chosen as the classifier. The generalization gap trend is
plotted for two scenarios, (a) m = 10 and (b) m = 100, where m is the number of training examples.
As here Rm(F) is fixed, but Rm(G) a VH changes with H, we plot the generalization gap averages
against VH. Note that the trend is linear in nature in both cases (correlation of ≈ 0.98 for both),
showing that the generalization gap depends directly on the complexity of the generator space.
Please note that for the following proofs, a variable z which follows the Rademacher distribution
(P r(z = +1) = Pr(z = -1) = 0.5) are referred to as Rademacher variable.
C.1 PROOFS OF PROPERTIES P1-P3 OF CO-COMPLEXITY
We begin with the proof of the properties of co-complexity (P1-P3) and a property of the correlated
Rademacher complexity, which are described in Section 5 of the main paper.
Lemma 2. For any two function spaces F and G, the following statements hold.
Rm(F,G) =Rm(G,F)	(28)
Rm(F,G) ≥RCm(F)	(29)
Rm(F,G) ≥RCm(G)	(30)
Rm(F,G) ≤RCm(F)+RCm(G)	(31)
RCm (F) ≤ Rm (F)	(32)
Proof. (i) To prove the first statement, it is trivial to see that interchanging f and g in the expression
of Rm (F, G) keeps it unchanged.
Thus,Rm(F,G) =Rm(G,F).
(ii)	Let us define σi0 = g0 (zi)g0 (zi0)σi. To prove the second statement, we choose any fixed function
g0 within G and we have,
14
Under review as a conference paper at ICLR 2021
× × E
2 σ,S,S0
sup 1— X f (zi)f (ZO )g(zi)g(zi)σi
f∈F,g∈G m i=1	i	i
≥ 1 × E0, sup ( - Xgo(Zi)go(ZO)f(zi)f(z'i)σi
2	σ,S,S0 f∈F m i=1
=1 × ，E„, sup 1— X f (zi)f (zi)σ')]
2	σ,,S,S, f∈F m i=1	i i
= RCm(F).
(33)
(34)
(35)
Here, we use the fact that σiO is also a Rademacher variable.
(iii)	We can prove the third statement in the same manner, fixing fo ∈ F instead.
(iv)	The fourth statement can be proven as follows. First, we note that f(Zi)f(ZiO) and g(Zi)g(ZiO)
are variables which take the value of 1 or -1. Thus, f(Zi)f(ZiO)g(Zi)g(ZiO) can be expressed as
(1- |f(Zi)f(ZiO) - g(Zi)g(ZiO)|) = (1- f(Zi)f(ZiO) - g(Zi)g(ZiO)vi), where vi ∈ {-1, -1} depends
on f(Zi)f(ZiO) and g(Zi)g(ZiO) together, but is independent of each of them individually. We can
therefore write,
1	×	Eq,	suP	1— X f(Zi)f(Zi)g(Zi)g(Zi)bi) J
2	σ,S,S, f∈F,g∈G m i=1	i	i
=1 × Eq,	suP	1— X (I-If(Zi)f (Zi)- g(Zi)g(Zi)I) bi) J	(36)
2	σ,S,S, f∈F,g∈G	m i=1	i	i
=1 × Eq,	suP	1— X (f(Zi)f (Zi)- g(Zi)g(Zi)) Vibi ) .	(37)
2	σ,S,S, f∈F,g∈G	m i=1	i	i
Now, consider any fixed b = {bo, ..., bm} = {boo, ..., bmo }, and fix S and SO. First, let us denote
1m
fopt , gopt = arg max -∑(f (Zi) f (Zi)-g(Zi)g(Z')) Vib0	(38)
f∈F,g∈G m i=1	i	i i
Let us denote the final vi for fopt and gopt by viopt. Note that viopt here depends on fopt(Zi)fopt(ZiO)
and gopt(Zi)gopt(ZiO). Note that v1opt, v2opt, ..vmopt will be independent of each other, as b1, b2, ..bm
are independent. Also observe that v1opt, v2opt, ..vmopt will be independent of b1o, b2o, ..bmo respectively,
as they only depend on fopt and gopt. We define biO = vioptbio. We note that b1O , b2O , ..bmO will be
independent across all samples, and P r(biO = +1) = Pr(biO = -1) = 0.5, same as bi. Thus
b1O , b2O , ..bmO are i.i.d Rademacher variables like b1, b2, ..bm. We thus note,
suP	1— X(f (Zi) f (Zi) - g(Zi)g(Z'))viσ0 )
f∈F,g∈G	m i=1
m
=-X (fopt(Zi )fopt(Zi)- gopt(Zi) gopt(Ziy) Voptσ0
m i=1
1m
=m〉: (fopt(Zi Ifopt(Zi) - gopt(Zi) gopt(Ziy) σi
m i=1
≤ sup 1— X (f (Zi) f (Zi 1 - g(ZiIg(Zi))σi1,
f∈F,g∈G	m i=1
(39)
(40)
(41)
15
Under review as a conference paper at ICLR 2021
where in the final supremum σi0 are kept fixed at their original values (σi0 = vioptσi0). We then have,
1 × E 2 σ,S,S0	sup	1 — X f (Zi) f (Zi )g(a)g(Zi)C	i	
	f∈F,g∈G m i=1		J∖ g(Zi)g(ZiO)) σiO	,	
≤ 1 × -2	E σ0,S,S0	1m f 段p∈G (m X (f (Zi)f (ZO)-		(42)
RCm(F) +RCm(G).	(43)
The final result RCm (F) ≤ Rm (F) can be shown as follows. To prove this, we use the fact that
f(zi)f(zi0) = 1 - |f(zi) - f (zi0)|. We have,
Rm(F)T	×	Eq,	sup	1—X f(Zi)f(ZO)σi)	沏
m	2	σ,S,S0 f∈F m i=1	i
=1	×	Eq,	suP	[—	X(ITf(Zi)- f (zi )l)σi J	(45)
2 σ,S,S0	f∈F	m	i=1	i
=1	×	Eq,	sup	1—X(f (Zi)- f (ZO))viσi)	(46)
2	σ,S,S0 f∈F m i=1	i
Here, vi takes values in {-1, 1}, depending on whether f(Zi) or f(ZiO) is greater. Note that as σi is
a Rademacher variable, viσi is also a Rademacher variable. We also note that vi is independent of
f(Zi) and f(ZiO) individually. Thus we have,
Rm(F) = 1 × e„, sup 1—X(J(Z)-f(z'i)`)viσi\	(47)
m	2 σ,S,S0 f∈F m i=1	i
≤ 1 × Eq suP 1— X f (Zi) (σi ) )] + 1 × Eq, suP 1— X f (Z0)(-σi) )],
2	σ0,S f∈F m i=1	i	2	σ0,S0 f∈F m i=1 i i
(48)
where σiO = viσi. As -σiO is also a Rademacher variable, we finally have
Rm(F) ≤	1	× E	sup 1—Xf(Z)(σi)) + 1	× E
m	2 σ0,S	f∈F m i=1	2 σ0,S0
=Rm(F) + Rm(F)
=	2
= Rm(F).
SuF A X f(Zi)T
(49)
(50)
(51)
This completes the proofs.
□
C.2 Proof of Theorem 2
The following result proves a generalization error bound which incorporates Rm(F) and Rm(G).
Theorem 2. For 0 < δ < 1, with probability p ≥ 1 - δ, we have
errP(f) ≤ CrS(f) + Rm(F) + Rm(G) + Jlog)(I").	(52)
2m
Proof. First, we reiterate that ecrrS (f) = Pim=1 (1 - f (Zi)gtruth(Zi)) /2m, for some gtruth ∈ G,
and similarly for errP (f), where m	-→ ∞.
16
Under review as a conference paper at ICLR 2021
Then, we retrace the steps of the original proof in Bartlett & Mendelson (2003), noting that
errP (f) ≤ ecrrS (f) +	sup	(errP (f) - ecrrS (f))	(53)
f∈F,g∈G,g(S)=gtruth (S)
≤ ecrrS(f) + sup (errP (f) - ecrrS(f)).	(54)
f∈F,g∈G
Here we initially only consider functions g in G such that g(z0) = gtruth(z0), g(z1) =
gtruth (z1), ..., g(zm) = gtruth(zm). Notice, that differently from (Bartlett & Mendelson (2003)), the
supremum eventually considers the generator space G in addition to the classifier’s function space F.
We define φ(S) = supf ∈F,g∈G (errP (f) - ecrrS (f)), and notice that
SUP	∣φ(zι,Z2,...,Zi,....,Zm) - φ(zι,Z2,...,Z0,….,Zm)∣ ≤ — .	(55)
z1 ,z2,...,zm ,zi0 ∈Z	m
This leads to the application of McDiarmid’s Inequality (McDiarmid (1989)) on the concentration
bounds for φ(S), which results in the the following bound, with a probability of at least 1 - δ,

errP (f) ≤ ecrrS (f) + E sup (errP (f) - ecrrS (f))
S f∈F,g∈G
log(1∕δ)
m
Finally, we express ES [φ(S)] as follows.
E f∈FuP∈G(errP(f)-cS(f)) = E f∈FuP∈GE0[crs0(f)-cS(f)|S]
sup E
f∈F ,g∈G S0
1m
-1 X(
m
i=1
1- f(zi0)g(zi0)	1- f(zi)g(zi)
≤E
sup
S,S0 f∈F ,g∈G
m
-1 X
m
i=1
f(zi)g(zi) - f(zi0)g(zi0)
(56)
(57)
(58)
(59)
E
S
2
2
+
—
2
.
The last inequality (Jensen’s inequality (Cover & Thomas (2012)) is applicable because sup is
a convex function. We proceed by multiplying the terms with Rademacher variables σi , which
randomly take on {-1, 1} with equal probability. This keeps the expected value unchanged. Note
that E[σi] = 0. In what follows, we express f (zi)g(zi) = 1- |f(zi) - g(zi)|. This is enabled
because both have their range in {-1, 1}.
E
S,S0
sup
f∈F ,g∈G
m
-1 X
m
f(zi)g(zi) - f(zi0)g(zi0)
E
S,S0,σ
sup
f∈F ,g∈G
i=1
m
-1 X
m
i=1
))
(f (zi)g(zi) - f(zi0)g(zi0))σi
))
(60)
≤E
σ,S,S0
sup
f∈F ,g∈G
m
-1 X
m
i=1
If(Zi) - g(Zi)lσi
2
))
+E
σSS0
sup I-X
f∈F ,g∈G m i=1
lf (zi)- g(zi)lσi
))
(61)
2
2
2
Note that multiplication with the Rademacher variables does not change the value of the expectation,
as the Rademacher variable σi simply controls whether Zi and Zi0 belong to S and S0 (σi = 1), or to
S0 and S0 (σi = -1). Coupled with the fact that E[σi] = 0, we note that this keeps the expectation
in (59) unchanged. Now, note that (|f (Zi) - g(Zi)|) σi = (f(Zi) - g(Zi)) viσi. Here vi is also a
17
Under review as a conference paper at ICLR 2021
variable that takes its values in {-1, 1}. Since σi is a Rademacher variable, viσi = σi0 is also a
Rademacher variable. In what follows, we use the fact that vi is independent of fi and gi individually.
σ,S,S0
sup
f∈F,g∈G
m
-1 X
m
i=1
If(Zi) - g(Zi)lσi
2
σ0,S,S0
+
σ0,S,S0
sup
f∈F
ɪ X f (Zi)σθA1
m i=1	2 )∖
sup
g∈G
Rm(F) + Rm((G )
2
(62)
(63)
E
E
E
Similarly, We can derive the same for Eσ,s,so 卜UPf ∈f,g三g (* Pm=I |"%，-FZi)lσi) ]. Thus, finally,
we can represent the upper bound on E[φ(S)] as follows .
E[φ(S)] ≤
Rm(F) + Rm(G )	Rm(F) + Rm(G)
2	+	2
Rm(F) + Rm(G).
(64)
This leads to the final form of the generalization error bound. With probability at least 1- δ, We have
errp(f) ≤ CrS(f) + Rm(F)+ Rm(G) + JlOg(I").	(65)
m
□
C.3 Proof of Theorem 3
The folloWing statements propose the generalization error bound in terms of co-complexity.
Theorem 3.	For 0 < δ < 1, with probability p ≥ 1 - δ, we have
errP (f) ≤ CS(f) + Rm (F, G) + JIogf 0.	(66)
2m
Proof. We proceed in the same Way as in appendix C.2, until We arrive at (57). In What folloWs, We
note that We can Write f (Zi)g(Zi) - f(Zi0)g(Zi0) = (1- f (Zi)g(Zi)f (Zi0)g(Zi0))vi, for some binary
valued variable vi With range in {-1, 1}. We also multiply the original Rademacher variable σi to
(57), and derive the folloWing set of bounds.
E
S
suP (errP(f)-ecrrS(f)) ≤ SE,S
f∈F,g∈G
suP
S,Si f∈F,g∈G
E
σ,S,Si
≤E
σi,S,Si
suP
f∈F,g∈G
suP
f∈F,g∈G
SUP 仁 X
f∈F,g∈G m i=1
m
X
i=1
f(Zi)g(Zi) - f(Zi0)g(Zi0)
(1- f(Zi)g(Zi)f(Zi0)g(Zi0)) vi
m
-1X-
m
i=1
m
-1 X
m
i=1
))
(1- f (Zi)g(Zi)f (Zi0)g(Zi0)) viσi
(f(Zi)g(Zi)f(Zi0)g(Zi0))σi0
))
))
(67)
(68)
(69)
(70)
E

1
m
2
2
2
2
.
The last step folloWs from the fact that E[σi] = 0, and the assumption that σi0 = viσi, are also i.i.d
Rademacher variables. This assumption Will hold for most Well behaved classifiers, for Which less
than half of the terms in (68) are greater than zero (Which is applicable for classifiers With not too
18
Under review as a conference paper at ICLR 2021
large Rm(F)). As 2 × Eσ0,S,S0 IsUpf ∈F,g∈G ( Pm=I f (zi)g(zi)f (z'i )g(z0)σ0)] = Rm(F, G), we
have the final generalization bound as follows. With probability ≥ 1 - δ,
errP (f ) ≤ cS (f )+Rm(F，G ) + 尸兽.	(71)
□
The above theorem leads to the following corollary.
Corollary 3.1. We consider the hypothetical case where the roles are reversed, i.e., when the
generator function space G is used to fit the labels, which are now generated by a function f ∈ F.
For any function g ∈ G, for 0 < δ < 1, with probability p ≥ 1 - δ, we have
errP(g) ≤ CS(g) + Rm(F, G) + Jiogr ").	(72)
2m
Proof. This is a direct outcome of the fact that Rm (F, G) = Rm(G, F). When the data labels in S
is generated by functions in F rather than G, we will simply have with probability p ≥ 1 - δ,
I
s
errP (g) ≤ ecrrS (g) + Rm (G, F) +
=ecrrS(g)+Rm(F,G)+
log (1)
log (1)
(73)
(74)
m
m
Note that here the labels in S are generated by a certain f ∈ F. This completes the proof. □
C.4 Proof of Theorem 4
The following result outlines the lower bound on the error function.
Theorem 4.	For 0 < δ < 1, with probability p ≥ 1 - δ,
errP (f) ≥ CS(f) - Rm (F, G) - ∖JOgT⑷.	(75)
2m
Proof. First, we note that
errP (f) ≥ eCrrS (f) - E sUp (eCrrS (f) - errP (f))	(76)
S f∈F,g∈G
Next, we denote
φ(S) = sUp (errS (f) - eCrrP (f)).	(77)
f∈F,g∈G
We then proceed similarly to Theorem 2 and Theorem 3, and we can show that
E [φ(S)] ≤Rm(F,G).	(78)
Furthermore, note that
sup	∣φ(zi,Z2,…,Zi,.…,Zm) - φ(zi,Z2,…,zi,….,Zm)∣ ≤ 1- .	(79)
z1,z2,...,zn,zi0 ∈Z	m
This allows us to apply another consequence of Mcdiarmid’s inequality, which leads to the following,
with probability p ≥ 1 - δ,
19
Under review as a conference paper at ICLR 2021
/log(1∕δ)
m
(80)
errP (f) ≥ecrrS (f) - E sup (errS (f) - ecrrP (f))
S f∈F,g∈G
≥eCrs(f) - Rm(F, G) - rlogQ∕δ).	(81)
m
This completes the proof.
□
C.5 Proof of Theorem 5
Now, we outline a set of results pertaining to the joint-entropy like behaviour of Rm (F, G).
Theorem 5.	We are given the mutual complexity measure Im(F, G) as defined in co-complexity P4
(Section 5 of main paper). We consider an alternative ground truth generator space G0, such that
Im(G0, G) = 0, i.e., G and G0 are independent. Then we have,
Rm(F) ≥Im(F,G)+Im(F,G0).	(82)
Proof. To prove the above, note that it is sufficient to prove the following:
Rm(F,G)+Rm(F,G0) ≥RCm(F)+RCm(G)+RCm(G0)	(83)
We elaborate on the left hand side of the above inequality as follows
Rm(F, G)+ Rm(F, G0)= 1 ×	E «0
2	σ,S,S 0
+ 1 × σ,ES0
sup
f∈F,g∈G
sup
f∈F,g0∈G0
m1 Xf (zi)f (z'i)g(zi)g(z'i )σ):
[m1 X f (Zi)f (Zi)g'(Zi)g'(Z')σi
(84)
We introduce i.i.d Rademacher variables {0, ..., m}, by multiplying them with the terms within
Rm(F, G'). As E[i] = 0 ∀i, this maneuver does not change the value of the expectation.
Rm(F, G) + Rm(F, G') = 1 ×	E«0
2 σ,S,S 0
sup
f ∈F,g∈G
m
-1 X
m
i=1
f (Zi)f (Zi')g(Zi)g(Zi')
+1 × Eq,	SUp	\- X f (Zi )f (Zi )g'(Zi)g'(Zi)σiq)
2	σ,,S,S0 f∈F,g0∈G0	m i=1	i	i
≥ 1 × E e, SUp 1X X f (Zi)fWiHg(Zi)g(Zi) + eig'(2i)g'(Z°)) σivi )
2	σ,,S,S0 f∈F,g∈G,g0∈G0	m i=1	i	i	i
(85)
(86)
Now note that one can re-express g(Zi)g(Zi') + ig'(Zi)g'(Zi') as (1 + ig(Zi)g(Zi')g'(Zi)g'(Zi'))vi.
Here vi ∈ {-1, 1} ∀i. Note that the value of vi depends on the values of g(Zi), g(Zi'), g'(Zi), g'(Zi')
and i . However, note that with so many dependencies vi ultimately is independent of all of these
terms, individually. We have,
Rm(F, G) + Rm(F, G') ≥
1 X E
2	σ,,S,S0
1m	'
sup	—£f(Zi)f (Zi )σiVi
f∈F,g∈G,g0∈G0 m i=1
+ m1 X f (Zi)f (Zii )g(Zi)g(Zi)g'(Zi)g' (Zi )eiσivi)]
(87)
20
Under review as a conference paper at ICLR 2021
Note that the expectation is over a fixed values of σ, , S and S0 . For the next step, we recognize that
viσi = σi0 is a Rademacher variable itself in the first term to the R.H.S of the inequality above, as vi
is independent of f(zi) and f(zi0). For a fixed value of those parameters, we then consider
广
arg max	X f(zi)f(z0Wi
f∈F	m i=1
(88)
Then, we can compute a lower bound for the expression in (87) as follows,
Rm(F,G)+Rm(F,G0) ≥
1	1m
2 × ",Es 嬴 X f * (Zi)f * (ZOM
i=1
+E
σ,,
sup
S,S0 f∈F,g∈G,g0 ∈G0
N X f (zi)f (zi )g(Zi)g(z'i)g' (Zi)g' (z'i)eiσiv[]
(89)
≥ Rm(F)+1
2 σ,
sup
,S,S0 g∈G,g0∈G0
E
m1 Xg(zi)g(z'i)g'(Zi)g(zi) SσiVif *(Zi)f *(ZO力)]
i=1	(90)
Itis important to note here that the variable iσivif*(Zi)f*(ZiO) is a Rademacher variable, but however,
is not yet independent of the other terms being multiplied to it. The terms i, σi, f*(Zi) and f*(ZiO) are
all independent ofg (Zi),g (ZiO), gO(Zi)&g O(ZiO), but however, the same cannot be said of vi. Therefore,
to resolve this issue we note that the product g (Zi)g (ZiO)gO(Zi)g O(ZiO) itself can be expressed as another
function g OO(Zi, ZiO) =g (Zi)g (ZiO)gO(Zi)g O(ZiO), as an instance of another function space GOO and the
second term in (89) can then be re-expressed as,
× × E
2 σ,S,S0
sup [—X g/,(Zi,Z,i )(qσiVif *(Zi)f *(zo ))).
g00∈G00 m i=1
(91)
Note that although vi was not independent of theg(Zi),g (ZiO),g O(Zi)&gO(ZiO), it is independent of
the product g	(Zi)g	(ZiO)gO(Zi)g	O(ZiO),	and thus independent	of gOO(Zi,	ZiO).	Thus, now we can finally
express the variables {σi = (∕σiVif *(Zi)f *(zO)) |i = 1, 2,…，m} as i.i.d Rademacher variables
themselves. This leads to the lower bound as follows:
Rm(F,G)+Rm(F,GO) ≥
≥ Rm(F)+2
×E
σ0,S,S0
SUp	1— Xg(Zi)g(Zi)£(&)£(Zi) (σ0))].
g∈G,g0∈G0 m i=1
RCm(F)+Rm(G,GO)=RCm(F)+RCm(G)+RCm(GO).
(92)
(93)
The last step follows from the fact that Im(GO, G) = 0, that is, the function spaces G and GO are
independent (i.e. Rm(G, Gi) = Rm(G) + Rm(G0)). This concludes our proof.	□
We now prove a corollary to Theorem 5, where we discuss implications of Theorem 5 on the
generalization error bound in Theorem 3.
Corollary 5.1. Given a function space GO, such that Im (GO, G) = 0. Suppose that Rm (F) =
Im(F, G) + Im(F, GO) + , for some ≥ 0. We have with probability p ≥ 1 - δ,
err P (f) ≤ 折 S (f)+ Rm(G)+ Im(F, G 0)+ e + Jl°也&.	(94)
m
Proof. This is a direct consequence of the fact that Rm(F) ≥ Im(F, G) + Im(F, GO), combined
with the generalization bound in Theorem 3.	□
21
Under review as a conference paper at ICLR 2021
Now we proceed to the final set of results, where we decompose the co-complexity into two other
complexity terms, as defined in Section 5 of the main paper. First, we show how the co-complexity
measure Rm(F, G) can be decomposed into RIm(F, G) and RDm(F, G).
C.6 Proof of Theorem 6
The following results discuss the impact of dissociation co-complexity and invariance co-complexity
on generalization error bounds.
Lemma 2. Consider function spaces F and G, such that F is Rademacher smooth w.r.t. G. Let us
define RmD(G) as 2 × Eσ,s,so,s0∈ig(S) IsUpg∈g (m1 ∑m=ι g(zi)g(z0)σi)]∙ Forsomenon-negative
real constant 0 ≤ α ≤ 1, we then have
Rm(F,G) ≤ αRIm(F, G) + (1 - α)(RDm(F, G) + RCm,D(G)),	(95)
where RIm (F, G) and RDm (F, G) are the invariance co-complexity and the dissociation co-complexity,
respectively∙
Proof∙ As the functions are Rademacher smooth, we can decompose the expectation over S, S0 to
only the ones where S0 ∈ IG(S) , and the ones where S0 ∈/ IG(S). We begin from (67) as follows.
Rm (F , G) = E 0
σ,S,S0
sUp
f∈F,g∈G
m
-X
m
i=1
(f (Zi Ig(Zi)f(zi)g(Zi) σ
2
))
(96)
Then, using the Rademacher smoothness constraint, we have for a certain 0 ≤ α ≤ 1,
Rm (F, G) = α
E
σ,S,S0,S0∈IG(S)
(1 - α) E
σ,S,S0,S0∈lG (S)
αE
σ,S,S0,S0∈IG (S)
(1 - α)
σ,
sUp
f∈F,g∈G
sUp
f∈F,g∈G
m
-X
m
i=1
m
-1X
m
i=1
f (Zi)f (Zi)g(Zi)g(Zi)
2
f(Zi)f(Zi)g(Zi)g(Zi)
2
+
(97)
sup(ɪ X ffσ
sUp
,S,s0,S0∈ig (S) f ∈F ,g∈G
m
-1X
m
i=1
f(Zi)f(Zii)g(Zi)g(Zii)
(98)
E
+
2
≤ α σT∈lG (JUF A X (I- "f(Zi))” ) ]+(1 - a)(Rm(F，G) S* (G))
(99)
≤α E
S,S0,S0∈IG (S)
sUp
f∈F
m
-1 X
m
i=1
(1- f(Zi)f(Zi))
2
))
αRIm(F,G)+(1-α)(RDm(F,G)+RCm,D(G)).
+(1-α)(RDm(F,G)+RCm,D(G)) (100)
(101)
Weuse the fact that Eσ,S,S0,S0∈lG(S) IsUPf ∈F,g∈G (m1 ∑m=1 f (Zi)f""(Zi)"zig.) ] ≤ Rm(F, G) +
RCm,D(G), which follows trivially from a derivation similar to that of co-complexity P3 in Lemma 2
(Rm (F, G) ≤ RCm (F) + RCm (G)). This completes the proof.
□
Using this, we proceed to the proof of Theorem 6, where we re-express the generalization error bound
in terms of the invariance and dissociation co-complexities, including bounds on the training error.
For purposes of simplification, we denote RIm (F, G), RmD(F, G) as RDm , and RDm,1(F, G) as RDm,1.
22
Under review as a conference paper at ICLR 2021
Theorem 6.	Consider function spaces F and G, such that F is Rademacher smooth w.r.t. G and
F contains the constant function f(z) = c, ∀z, c ∈ {-1, 1} . Let errP denote the generalization
error for the functions f ∈ F which showcase the best fit on the training data samples. For some
non-negative real constants 0 ≤ α, β ≤ 1 and 0 < δ < 1, with probability p ≥ 1 - δ, we have
errp ≤(I- β) (2—Rm,1)+αRm+(I- α)(Rm+Rm,D(G))+ʌ/	gm(102)
Note: Recall that RCm,D (G) is defined in Lemma 2.
Proof. We note that errP is the generalization error for the case when only the best fitting function
fopt = argminf ∈F (ecrrS (f)) is chosen for each training data set S, and is averaged over all possible
ground truth functions g ∈ G . Note that the labels in S is subject to change, depending on the choice
of g. Thus for fixed datapoints z1, z2, .., zm ∈ S, (53) changes to
errP ≤	E	[ecrrS (fopt)] + E (errP (fopt) - ecrrS(fopt))	(103)
g∈G	g∈G
≤	E	[ecrrS (fopt)] + sup(errP (fopt) - ecrrS (fopt))	(104)
g∈G	g∈G
≤	E	[ecrrS (fopt)] + sup (errP (f) - ecrrS (f)).	(105)
g∈G	f∈F,g∈G
We now denote φ0(S) = Eg∈G [inf f ∈F (ecrrS (fopt))] + supf ∈F,g∈G (errP (f) - ecrrS (f)). Let us
also denote ψ(S) = Eg∈G [inf f ∈F (ecrrS (fopt))] and γ(S) = supf ∈F,g∈G (errP (f) - ecrrS (f)). We
then have the following for a fixed fopt .
sup	∣φ0(zi,...,Zi,....,Zm) - φ0(zι,..., zi, ...., Zm)∣
z1 ,...,zn,zi0 ∈Z
≤	sup	∣ψ(zi,…,Zi,.…,Zm) — ψ(zι,…,zi,.…,Zm)∣
z1 ,...,zn ,zi0 ∈Z
+	SuP	∣γ(zι,...,Zi,….,Zm) — γ(zι,…,zi,.…,Zm)∣
z1 ,...,zn,zi0 ∈Z
≤! + ! = A.
mmm
It is easy to show that suPz1,z2,...,zn,zi∈z ∣ψ(z1,z2,...,zi,....,zm) - ψ(z1,z2, ...,z0,….，Zm)| ≤ mm
and suPz1,z2,…,zn,z0∈z ∣γ(z1,z2,...,zi,....,zm) - γ(z1,z2, ...,z0,….,Zm)| ≤ mm. APPlyingMcdi-
armid’s inequality, we have with probability p ≥ 1 - δ,
(106)
(107)
errP ≤ E [ecrrS(fopt)] + E sup (errP (f) - ecrrS(f))
S,g∈G	S f ∈F,g∈G
+ 尸叵.(108)
m
We already have that ES supf ∈F,g∈G (errP (f) - ecrrS (f)) ≤ Rm(F,G) ≤ αRIm +(1-α)(RDm +
RCm,D (G), for a certain 0 ≤ α ≤ 1 (Lemma 2). The first term (exPected training error) can be
exPressed as,
s,E∈g [crS (fopt)] = 2 (1 - s,E∈g fuF (m Xf (zi)g(Zi)) D.	(109)
As iterated in the main statement of this theorem, we assume that the classifier function sPace is
comPlex enough to allow for an error rate ≤ 50% (chance level for binary classification) on the
training data itself. This indicates that We can safely assume that SuPf ∈f (mm- Pi f (zi)g(zi)) ≥ 0.
We then have,
23
Under review as a conference paper at ICLR 2021
sup ( — X f (Zi)g(zi) ) sup ( ɪ X f(zi)g(zi)
f∈F m i	f∈F m i
SUp 13 X f(zi)g (Zi) f(zj )g(zj)
f∈F	m2 i,j
sup ( 3 X f(zi)g(zi)f (z0)g(z0)f (z0)g(z0)f(Zj)g(Zj)
f∈F	m2 i,j
sup (m1 X f (Zi)g(zi)f (z0)g(z0)!!
(110)
(111)
(112)
where we make use of the fact that f (Z0)g(Z0)f (Z0)g(Z0) =1. Here, the datapoint Z0 is sampled from
the same distribution P . Thus we have,
S,z0Eg∈G 卜F G Xf (Zi)g(Zi)) ] ≥ U∈G 卜F G Xf (Zi)g(Zif(Z0)g(ZO)) ]. (113)
As before, we assume that the function space f is Rademacher smooth, i.e. for a certain 0 ≤ β ≤ 1,
we can express
q E r SUP(L X f (Zi )g(Zi)f (Z0)g(Z0) J =
S,z0,g∈G f∈F m i
=β E	sup I X X f (Zi) f (z0) I
s,z,s∈I(z0),g∈G f ∈F∖m 彳	刀
+ (1-β) E	sup [— X f (zi)g(zi)f (z0)g(z0) ) .	(114)
s,z0,s∈Ig(z0),g∈G f ∈F ym L
≥β S,Z0,S∈∣E …"suf (W Xf(Zi)f (Z0))#
+ (1 - β) S,z0,S∈1E5 (z0),σ 泮 A X f (zi)f (z0)σi) ].	(115)
The last step above involves the reasonable assumption that the function space is going to be on
average worse at fitting random noise labels, rather than a labelling provided by an element of the
generator space G . Finally, we note that
E	sup I X X f (zi)f (z0) ∣ =1,	(116)
s,z0,s∈Ig(z0),g∈G f ∈F ∖m 彳
simply by choosing the constant function f(Z) = c, which is assumed to be present within the
classifier,s function space F. Also, note that Es∕o,s∈ig[supf∈f (m1 Pi f (zi)f (z0)。，]=
2RDm,1(F, G). We then have
E
S,z0,g∈G
泮X f (Zi)g(zi)f (z0)g(z0)j J ≥ β + 2(1 - β) RmI(F, G).
(117)
24
Under review as a conference paper at ICLR 2021
This leads to the final upper bound for errP by replacing the terms in (108) by the above. With
probability p ≥ 1 - δ, we then have,
errp ≤(I- β) Q—Rm,1)+αRm+(I- α)(Rm+RmD(G)) +，，:。(118)
□
Finally, we provide the proof of Proposition 1, which gives interpretable bounds on invariance
co-complexity and dissociation co-complexity.
Proposition 1. Define the well known growth function of binary valued functions F as A =
ΠF (m) = maxz |{f (zk) : k = 1, 2, ..., m, f ∈ F}|. Define the invariance-constrained and
dissociation-constrained growth functions of F w.r.t. G as:
B = πF∣g (m) =	, 0maχ …Kf(Zk ),f(zk) : k = 1,2,...,m,f ∈ F}|
Z, z' ,Z0 ∈Ig (Zi ),∀i
C = πD|g(m) = 0 0max . u |{f(Zk),f (Zk) : k = 1, 2,…,m,f ∈ F}|
z,z’,zi0 ∈/ IG (zi),∀i
Define RIm0(F,G)	=/ 2 X Eσ,S,S0,S0∈lG(S) [suPf ∈F (mm PzIf (Zi) f (Zg], where Zi ∈
IG (Zi), ∀i. Note that RIm (F, G) ≤ RIm(F, G). Given the definitions above, we have RIm (F, G) ≤
qlogB ≤ qlomA and Rm(F, G) ≤ qlogC ≤ q⅞A.
Proof. First we show that RIm0 (F, G) ≤ RIm(F, G). This follows from the fact that
Rm (F, G ) = 1 × Q-E7,6 sup (-XX f (Zi)f (Z,i)σi∖	(119)
2	σ,S,S0,S0∈IG (S) f∈F m i=1
=1 × E ,、sup 1— X(1 — f (Zi)f(Z0 ))σ"	(120)
2	σ,S,S0,S0∈IG (S) f∈F m i=1
≤ 2 × SS0 S≡ Ig (SJsup G XX (1 — f(Zi)f(Z0)))[ = Rm(F, G)	(121)
S,S ,S ∈IG (S) f∈F m i=1
The proof of the main proposition immediately follows from the application of Massart’s finite class
lemma, which states that for a finite subset K of Rm, and Rademacher variables σ1, σ2, ..., σm, we
have
E [supσ∙iyi] ≤ rV72log |K|	(122)
σ1,σ2,...,σm y~∈K i
This immediately leads to the well known result for Rademacher complexity Rm(F), which states
Rm(F) ≤ r 2logπF(m).	(123)
m
The result above directly follows from the fact that the cardinality of the finite subset K is upper
bounded by the growth function ΠF (m), when K is the set of all possible function outputs from
functions f ∈ F, on any m points. In our case, as we have that
Rm (F，G ) = 2 × σ,ES0 IsuF (ɪ g 心以7 I，	(124)
we can directly apply Massart’s lemma, by the using the invariance-constrained growth function
ΠIF | G (m) to obtain the first result as follows. First, we choose the finite subset K such that it
25
Under review as a conference paper at ICLR 2021
contains all possible outputs of the function product f(zi)f(zi0) when S, S0 is chosen such that each
element in S0 lies in the invariance extended subset of each corresponding element in S. For this
K, the cardinality |K| is bounded above by the previously defined growth function ΠIF | G (m). This
immediately leads to the first result,
Rm (F, G)≤ 1?F三
m	2m
_ ∕iog∏F∙∣G(m)
V	2m	.
(125)
(126)
We can similarly show the equivalent result for dissociation co-complexity RDm (F, G), to obtain
Rm(F, G) ≤ 1S2lθg∏D∣G (m)	(127)
m	2m
iog ∏IF ∣ G(m)
=\ -F^—.	(128)
2m
Finally, we recognize that the growth functions ∏IF∣ G(m) and ∏FD ∣ G(m) are bounded above by
the cardinality of all possible function values taken in S, S0 , which in turn is bounded above by
∏F (m) × ∏F (m). This implies ∏IF∣ G(m) ≤ ∏F (m)2 and ∏FD ∣ G(m) ≤ ∏F(m)2, from which we
then have
Rm(F, G) ≤ Jlog∏D∣ G(m) ≤ r0g^≡	(129)
m	2m	m
and
Rm(F, G) ≤ Slθg∏F∣G⑺ ≤ rIog^m).	(130)
m	2m	m
This concludes the proof.	□
D	Neural Network Architecture Details
Here we present the architectures that were used in our experiments. As mentioned in section 4, four
different architectures were tested. In Table 4, we show the layer-wise details of each architecture.
Note that all architectures share a similar parameter count, and also evoke a similar dissociation
co-complexity as a result (see Table 2 and 3). Note that the input to all these architectures is an image
of size 28 × 28 (same size as in the MNIST dataset). For STL-10, we resized the input (96 × 96) to
28 × 28 and used the same architectures for consistency.
26
Under review as a conference paper at ICLR 2021
2-LayerNN (15.8k)	Standard CNN (16.1k)	Scale Equivariant CNN (15.9k)	Rotation Equivariant CNN (16k)
FC-20	Conv-15 (3x3)	ScaleConv-10(11x11,2.2:1)	P4ConvZ2-10 (3x3)
FC-2 (OUtPUt)	MaxPool(3x3)	MaxPool(3x3)	Maxpool (3x3)
	Conv-30 (3x3)	ScaleConv-22(11x11,2.2:1)	P4ConvP4-15 (3x3)
	MaxPool (3x3)	MaxPool(3x3)	Maxpool (3x3)
	Conv-40 (3x3)	ScaleConv-32(11x11,2.2:1)	P4ConvP4-19 (3x3)
	MaxPool (3x3)	Maxpool(3x3)	Maxpool (3x3x4)
	FC-20	Scale-Pool (4x1)	FC-10
	FC-2 (OUtPUt)	FC-20	FC-2(Output)
		FC-2 (OUtPUt)	
Table 4: Shown are the architectural details of all neural networks used in our experiments. Scale-
Conv represents scale-equivariant convolution and P4Conv-Z2 and P4Conv-Z4 represents rotation
equivariant convolution (as detailed in Cohen & Welling (2016a)). For the ScaleConv layers, note
that 2.2:1 represents the ratio of the maximum to the minimum scale of the filters. A total of 4 scale
pathways were chosen for those layers. FC represents the fully connected layers. Networks are
chosen such that overall they share a similar parametric count (shown within the brackets).
27