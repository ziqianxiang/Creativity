Proceedings of Machine Learning Research vol 144:1-21, 2022
Neural Point Process for Learning Spatiotemporal Event Dynamics
Zihao Zhou
UC San Diego
Xingyi Yang
National University of Singapore
Ryan Rossi
Adobe Research
Handong Zhao
Adobe Research
Rose Yu
UC San Diego
ziz244@ucsd.edu
xyang @ u.nus.edu
ryrossi @ adobe.com
hazhao @ adobe.com
roseyu@ucsd.edu
Abstract
Learning the dynamics of spatiotemporal events is a fundamental problem. Neural point processes
enhance the expressivity of point process models with deep neural networks. However, most ex-
isting methods only consider temporal dynamics without spatial modeling. We propose Deep Spa-
tiotemporal Point Process (DeepSTPP), a deep dynamics model that integrates spatiotemporal
point processes. Our method is flexible, efficient, and can accurately forecast irregularly sampled
events over space and time. The key construction of our approach is the nonparametric space-time
intensity function, governed by a latent process. The intensity function enjoys closed form inte-
gration for the density. The latent process captures the uncertainty of the event sequence. We use
amortized variational inference to infer the latent process with deep networks. Using synthetic
datasets, we validate our model can accurately learn the true intensity function. On real-world
benchmark datasets, our model demonstrates superior performance over state-of-the-art baselines.
Keywords: spatiotemporal dynamics, neural point processes, kernel density estimation
1.	Introduction
Accurate modeling of spatiotemporal event dynamics is fundamentally important for disaster re-
sponse (Veen and Schoenberg, 2008), logistic optimization (Safikhani et al., 2018) and social media
analysis (Liang et al., 2019). Compared to other sequence data such as texts or time series, spa-
tiotemporal events occur irregularly with uneven time and space intervals.
Discrete-time deep dynamics models such as recurrent neural networks (RNNs) (Hochreiter
and Schmidhuber, 1997; Chung et al., 2014) assume events to be evenly sampled. Interpolating an
irregular sampled sequence into a regular sequence can introduce significant biases (Rehfeld et al.,
2011). Furthermore, event sequences contain strong spatiotemporal dependencies. The rate of an
event depends on the preceding events, as well as the events geographically correlated to it.
Spatiotemporal point processes (STPP) (Daley and Vere-Jones, 2007; Reinhart et al., 2018)
provides the statistical framework for modeling continuous-time event dynamics. As shown in Fig-
ure 1, given the history of events sequence, STPP estimates the intensity function that is evolv-
© 2022 Z. Zhou, X. Yang, R. Rossi, H. Zhao & R. Yu.
Deep Spatiotemporal Point Process
ing in space and time. However, traditional statistical methods for estimating STPPs often re-
quire strong modeling assumptions, feature engineering, and can be computationally expensive.
O
User's trajectory
Figure 1: Illustration of learning spatiotemporal
point process. We aim to learn the space-time
intensity function given the historical event se-
quence and representative points as background.
Historical events ((s,-y, t,-ɔ) , ɪC)	Upcoming event (Sj, tɔ
O RePresentiVe points
Machine learning community is observing
a growing interest in continuous-time deep dy-
namics models that can handle irregular time
intervals. For example, Neural ODE (Chen
et al., 2018) parametrizes the hidden states in an
RNN with an ODE. Shukla and Marlin (2018)
uses a separate network to interpolates between
reference time points. Neural temporal point
process (TPP) (Mei and Eisner, 2017; Zhang
et al., 2020; Zuo et al., 2020) is an exciting
area that combines fundamental concepts from
temporal point processes with deep learning to
model continuous-time event sequences, see a
recent review on neural TPP (Shchur et al.,
2021). However, most of the existing models
only focus on temporal dynamics without con-
sidering spatial modeling.
In the real world, while time is a unidirectional process (arrow of time), space extends in multi-
ple directions. This fundamental difference from TPP makes it nontrivial to design a unified STPP
model. The naive approach to approximate the intensity function by a deep neural network would
lead to intractable integral computation for likelihood. Prior research such as Du et al. (2016) dis-
cretizes the space as “markers” and use marked TPP to classify the events. This approach cannot
produce the space-time intensity function. Okawa et al. (2019) models the spatiotemporal density
using a mixture of symmetric kernels, which ignores the unidirectional property of time. Chen et al.
(2021) proposes to model temporal intensity and spatial density separately with neural ODE, which
is computational expensive.
We propose a simple yet efficient approach to learn STPP. Our model, Deep Spatiotemporal
Point Process (DeepSTPP) marries the principles of spatiotemporal point processes with deep
learning. We take a non-parametric approach and model the space-time intensity function as mixture
of kernels. The parameters of the intensity function are governed by a latent stochastic process no
sampling which captures the uncertainty of the event sequence. The latent process is then inferred
via amortized variational inference. That is, we draw a sample from the variational distribution for
every event. We use a Transformer network to parametrize the variational distribution conditioned
on the previous events.
Compared with existing approaches, our model is non-parametric, hence does not make as-
sumptions on the parametric form of the distribution. Our approach learns the space-time intensity
function jointly without requiring separate models for time-intensity function and spatial density as
in Chen et al. (2021). Our model is probabilistic by nature and can describe various uncertainties
in the data. More importantly, our model enjoys closed form integration, making it feasible for pro-
cessing large-scale event datasets. To summarize, our work makes the following key contributions:
2
Deep Spatiotemporal Point Process
•	Deep Spatiotemporal Point Process. We propose a novel Deep Point Process model for
forecasting unevenly sampled spatiotemporal events. It integrates deep learning with spa-
tiotemporal point processes to learn continuous space-time dynamics.
•	Neural Latent Process. We model the space-time intensity function using a nonparametric
approach, governed by a latent stochastic process. We use amortized variational inference to
perform inference on the latent process conditioned on the previous events.
•	Effectiveness. We demonstrate our model using many synthetic and real-world spatiotempo-
ral event forecasting tasks, where it achieves superior performance in accuracy and efficiency.
We also derive and implement efficient algorithms for simulating STPPs.
2.	Methodology
We first introduce the background of spatiotemporal point process, and then describe our approach
to learn the underlying spatiotemporal event dynamics.
2.1.	Background on Spatiotemporal Point Process
Spatiotemporal Point Process. Spatiotemporal point process (STPP) models the number of events
N(S × (a, b)) that occurred in the Cartesian product of the spatial domain S ⊆ R2 and the time
interval (a, b]. It is characterized by a non-negative space-time intensity function given the history
Ht := {(s1, t1), . . . , (sn, tn)}tn≤t:
λ*( t) li	E[N (B(s, ∆s) X (t,t + ∆t))∣Ht ]
s,	∙ ∆s→0,∆t→0	B(s, ∆s)∆t
(1)
which is the probability of finding an event in an infinitesimal time interval (t, t + ∆t] and an
infinitesimal spatial ball S = B (s, ∆s) centered at location s.
Example 1:	Spatiotemporal Hawkes process (STH). Spatiotemporal Hawkes (or self-exciting)
process assumes every past event has an additive, positive, decaying, and spatially local influence
over future events. Such a pattern resembles neuronal firing and earthquakes. It is characterized by
the following intensity function (Reinhart et al., 2018):
λ*(s,t) := μgo(s) + E g1(t,ti)g2(s, Si) : μ > 0	(2)
i:ti <t
where g0(s) is the probability density of a distribution overS, g1 is the triggering kernel and is often
implemented as the exponential decay function, g1 (∆t) := α exp(-β∆t) : α, β > 0, and g2 (s, si)
is the density of an unimodal distribution over S centered at si .
Example 2:	Spatiotemporal Self-Correcting process (STSC). Self-correcting spatiotemporal
point process Isham and Westcott (1979) assumes that the background intensity increases with a
varying speed at different locations, and the arrival of each event reduces the intensity nearby. STSC
can model certain regular event sequences, such as an alternating home-to-work travel sequence. It
has the following intensity function:
λ*(s,t) = μ exp(go(s)lβt- X αg2(s, Si)) : α, β, μ > 0	(3)
i:ti<t
Here g0 (s) is the density of a distribution over S, and g2(s, si) is the density of an unimodal distri-
bution over S centered at location si .
3
Deep Spatiotemporal Point Process
Maximum likelihood Estimation. Given a history of n events Ht , the joint log-likelihood func-
tion of the observed events for STPP is as follows:
nt
logP(Ht) = Alogλ*(si,ti) -J J λ*(u,τ)dudτ
(4)
Here, the space-time intensity function λ*(s, t) plays a central role. Maximum likelihood estimation
seeks the optimal λ* (s, t) from data that optimizes Eqn. (4).
Predictive distribution. Denote the probability density function (PDF) for STPP as f(s, t|Ht)
which represents the conditional probability that next event will occur at location s and time t, given
the history. The PDF is closely related to the intensity function:
f (S,t|Ht) = 1 - FfH) = λ*(s,t) exP (— Rs Rn λ*(u,τ)dτdu)	⑸
where F is the cumulative distribution function (CDF), see derivations in Appendix A.1. This means
the intensity function specifies the expected number of events in a region conditional on the past.
The predicted time of the next event is the expected value of the predictive distribution for time
f? (t) in the entire spatial domain:
E[tn+1|Ht] = ∞t	f*(S,t)dSdt= ∞texp tλ*
tn S	tn
λ*(t)dSdt
Similarly, the predicted location of the next event evaluates to:
E[Sn+1|Ht]
S ∞f*(S,t)dtdS =	∞exp	tλ*(
S	tn	tn
Sλ*(S, t)dSdt
Unfortunately, Eqn. (4) is generally intractable. It requires either strong modeling assumptions
or expensive Monte Carlo sampling. We propose the Deep STPP model to simplify the learning.
2.2. Deep Spatiotemporal Point Process (DSTPP)
We propose DeepSTPP, a simple and efficient approach for learning the space-time event dynam-
ics. Our model (1) introduces a latent process to capture the uncertainty (2) parametrizes the latent
process with deep neural networks to increase model expressivity and (3) approximates the intensity
function with a set of spatial and temporal kernel functions.
Neural latent process. Given a sequence of n event, we wish to model the conditional density
of observing the next event given the history f(S, t|Ht). We introduce a latent process to capture
the uncertainty of the event history and infer the latent process with armotized variational inference.
The latent process dictates the parameters in the space-time intensity function. We sample from the
latent process using the re-parameterization trick Kingma and Welling (2013).
As shown in Figure 2, given the event sequence Ht = {(S1 , t1), . . . , (Sn, tn)}tn≤t, we encode
the entire sequence into the high-dimensional embedding. We use positional encoding to encode the
sequence order. To capture the stochasticity in the temporal dynamics, we introduce a latent process
4
DEEP Spatiotemporal Point Process
Figure 2: Design of our DeepSTPP model. For a historical event sequence, We encode it with a
transformer network and map to the latent process (zι,…，Zn). We use a decoder to generate the
parameters (wi, γi, βi) for each event i given the latent process. The estimate intensity is calculated
using kernel functions ks and kt and the decoded parameters.
z = (zι, ∙∙∙ ,Zn) for the entire sequence. We assume the latent process follows a multivariate
Gaussian at each time step:
Zi ~ qφ(zi∖Ht) = N(μ, Diag(σ))	(6)
where the mean μ and covariance Diag (σ) are the outputs of the embedding neural network. In our
implementation, we found using a Transformer Vaswani et al. (2017) with sinusoidal positional en-
coding to be beneficial. The positions to be encoded are the normalized event time instead of the in-
dex number, to account for the unequal time interval. Recently, Zuo et al. (2020) also demonstrated
that Transformer enjoys better performance for learning the intensity in temporal point processes.
Non-parametric model. We take a non-parameteric approach to model the space-time intensity
function λ*(s,t) as:
n+J
λ*(s,t∖z) = X Wiks(s, Si； Yi)kt(t,ti; βi)	(7)
i=1
Here wi(Z), γi(Z), βi (Z) are the parameters for each event that is conditioned on the latent process.
specifically, wi represents the non-negative intensity magnitude, implemented with a soft-plus ac-
tivation function. ks(∙, ∙) and kt(∙, ∙) are the spatial and temporal kernel functions, respectively. For
both kernel functions, we parametrize them as a normalized RBF kernel:
ks(S,Si) = α-1 exp - γikS - Sik , kt(t,ti) = exp - βikt - tik	(8)
where the bandwidth parameter γi controls an event’s influence over the spatial domain. The param-
eter βi is the decay rate that represents the event’s influence over time. α = S exp -γikS-Sik dS
is the normalization constant.
We use a decoder network to generate the parameters {wi , γi , βi} given Z separately, shown in
Figure 2. Each decoder is a 4-layer feed-forward network. We use a softplus activation function to
5
Deep Spatiotemporal Point Process
ensure wi and γi are positive. The decay rate βi can be any number, such that an event could have
constant or increasing triggering intensity over time.
In addition to n historical events, we also randomly sample J representative points from the
spatial domain to approximate the background intensity. This is to account for the influence from
unobserved events in the background, with varying rates at different absolution locations. The
inclusion of these representative points can approximate this background distribution.
The model design in (7) enjoys a closed form integration, which gives the conditional PDF as:
n+J w
f(s,t∣Ht,z) = λ*(s,t∣z)exp (- X βi[kt(tn, ti) - kt(t,ti)]∖	(9)
See the derivation details in Appendix A.2. DeepSTPP circumvents the integration of the intensity
function and enjoys fast inference in forecasting future events. In contrast, NSTPP Chen et al.
(2021) is relatively inefficient as its ODE solver also requires additional numerical integration.
Parameter learning. Due to the latent process, the posterior becomes intractable. Instead, we
use amortized inference by optimizing the evidence lower bound (ELBO) of the likelihood. In
particular, given event history Ht, the conditional log-likelihood of the next event is:
logp(s,t∣Ht) ≥logPθ(s,t∣Ht,z) + KL(qφ(z∣Ht)∣∣p(z))
(10)
(11)
= log λ*(s,t∣z)-
λ*(τ)dτ + KL(q||p)
tn
where φ represents the parameters of the encoder network and θ are the parameters of the decoder
network. p(z) is the prior distribution, which We assume to be Gaussian. KL(∙∣∣∙) is the Kull-
back-Leibler divergence between two distributions. We can optimize the objective function in Eqn.
(11) w.r.t. the parameters φ and θ using back-propagation.
3.	Related Work
Spatiotemporal Dynamics Learning. Modeling the spatiotemporal dynamics of a system in or-
der to forecast the future is a fundamental task in many fields. Most work on spatiotemporal
dynamics has been focused on spatiotemporal data measured at regular space-time interval, e.g.,
(Xingjian et al., 2015; Li et al., 2018; Yao et al., 2019; Fang et al., 2019; Geng et al., 2019). For dis-
crete spatiotemporal events, statistical methods include space-time point process, see (Moller and
Waagepetersen, 2003; Mohler et al., 2011). (Zhao et al., 2015) propose multi-task feature learning
whereas (Yang et al., 2018) propose RNN-based model to predict spatiotemporal check-in events.
These discrete-time models assume data are sampled evenly, thus are unsuitable for our task.
Continous Time Sequence Models. Continuous time sequence models provide an elegant ap-
proach for describing irregular sampled time series. For example, (Chen et al., 2018; Jia and Benson,
2019; Dupont et al., 2019; Gholami et al., 2019; Finlay et al., 2020; Kidger et al., 2020; Norcliffe
et al., 2021) assumes the latent dynamics are continuous and can be modeled by an ODE. But for
high-dimensional spatiotemporal processes, this approach can be computationally expensive. Che
et al. (2018); Shukla and Marlin (2018) modifies the hidden states with exponential decay. GRU-
ODE-Bayes proposed by De Brouwer et al. (2019) introduces a continuous-time version of GRU
and a Bayesian update network capable of handling sporadic observations. However, Mozer et al.
6
Deep Spatiotemporal Point Process
(2017) shows that there is no significant benefit of using continuous-time RNN for discrete event
data. Special treatment is still needed for modeling unevenly sampled events.
Deep Point Process. Point process is well-studied in statistics (Moller and Waagepetersen, 2003;
Daley and Vere-Jones, 2007; Reinhart et al., 2018). Deep point process couples deep learning with
point process and has received considerable attention. For example, neural Hawkes process applies
RNNs to approximate the temporal intensity function (Du et al., 2016; Mei and Eisner, 2017; Xiao
et al., 2017; Zhang et al., 2020), and (Zuo et al., 2020) employs Transformers. (Shang and Sun,
2019) integrates graph convolution structure. However, all existing works focus on temporal point
processes without spatial modeling. For datasets with spatial information, they discretize the space
and treat them as discrete “markers”. Okawa et al. (2019) extends Du et al. (2016) for spatiotemporal
event prediction but they only predict the density instead of the next location and time of the event.
Zhu et al. (2019) parameterizes the spatial kernel with a neural network embedding without consider
the temporal sequence. Recently, Chen et al. (2021) propose neural spatiotemporal point process
(NSTPP) which combines continuous-time neural networks with continuous-time normalizing flows
to parameterize spatiotemporal point processes. However, this approach is quite computationally
expensive, which requires evaluating the ODE solver for multiple time steps.
4.	Experiments
We evaluate DeepSTPP for spatiotemporal prediction using both synthetic and real-world data.
Baselines We compare DeepSTPP with the state-of-the-art models, including
•	Spatiotemporal Hawkes Process (MLE) (Reinhart et al., 2018): it learns a spatiotemporal para-
metric intensity function using maximum likelihood estimation, see derivation in Appendix A.3.
•	Recurrent Marked Temporal Point Process (RMTPP) (Du et al., 2016): it uses GRU to model the
temporal intensity function. We modify this model to take spatial location as marks.
•	Neural Spatiotemporal Point Process (NSTPP) Chen et al. (2021): a neural point process model
that parameterizes the spatial PDF and temporal intensity with continuous-time normalizing flows.
Specifically, we use Jump CNF as it is a better fit for Hawkes processes.
All models are implemented in PyTorch, trained using the Adam optimizer. We set the number of
representative points to be 100. The details of the implementation are deferred to the Appendix C.1.
For the baselines, we use the authors’ original repositories whenever possible.
Datasets. We simulated two types of STPPs: spatiotemporal Hawkes process (STH) and spa-
tiotemporal self-correcting process (STSC) . For both STPPs, we generate three synthetic datasets,
each with a different parameter setting, denoted as DS1, DS2, and DS3 in the tables. We also de-
rive and implement efficient algorithms for simulating STPPs based on Ogata’s thinning algorithm
Ogata (1981). We view the simulator construction as an independent contribution from this work.
The details of the simulation can be found in Appendix B. We use two real-world spatiotemporal
event datasets from NSTPP Chen et al. (2021) to benchmark the performance.
•	Earthquakes Japan: catalog earthquakes data including the location and time of all earth-
quakes in Japan from 1990 to 2020 with magnitude of at least 2.5 from the U.S. Geological
7
Deep Spatiotemporal Point Process
140	150	160	170
Figure 3: Ground-truth and learned intensity on two synthetic data. Top: ground-truth; Middle:
learned intensity by our DeepSTPP model. Bottom: learned conditional intensity by NSTPP. ‘X’s
refer to event history, where smaller ‘X’ refers to larger time difference.
75	80	85	90 time
Survey. There are in total 1,050 sequences. The number of events per sequences ranges
between 19 to 545 1.
•	COVID-19: daily county level COVID-19 cases data in New Jersey state published by The
New York Times. There are 1,650 sequences and the number of events per sequences ranges
between 7 to 305.
For both synthetic data and real-world data, we partition long event sequences into non-overlapping
subsequences according to a fixed time range T . The targets are the last event, and the input is the
rest of the events. The number of input events varies across subsequences. For each dataset, we
split each into train/val/test sets with the ratio of 8:1:1. All results are the average of 3 runs.
Table 1: Test log likelihood (LL) and Hellinger distance of distribution (HD) on synthetic data (LL higher
is better, HD lower is better). Comparison between ours and NSTPP on synthetic datasets from two type of
spatiotemporal point processes.
		Spatiotemporal Hawkes process					Spatiotemporal Self Correcting process			
	DS1		DS2	DS3		DS1		DS2	DS3	
	LL	HD	LL	HD	LL	HD	LL	HD	LL	HD	LL	HD
DeepSTPP (ours)	-3.8420	0.0033	-3.1142 0.4920	-3.6327	0.0908	-1.2248	0.2348	-1.4915 0.1813	-1.3927	0.2075
NSTPP	-5.3110	0.5341	-4.8564 0.5849	-3.7366	0.1498	-2.0759	0.5426	-2.3612 0.3933	-3.0599	0.3097
4	.1. Synthetic Experiment Results
For synthetic data, we know the ground truth intensity function. We compare our method with the
best possible estimator: maximum likelihood estimator (MLE), as well as the NSTPP model. The
MLE is learned by optimizing the log-likelihood using the BFGS algorithm. RMTPP can only learn
the temporal intensity thus is not included in this comparison.
Predictive log-likelihood. Table 1 shows the comparison of the predictive distribution for space
and time. We report Log Likelihood (LL) of f(s, t|Ht) and the Hellinger Distance (HD) between
the predictive distributions and the ground truth averaged over time.
1. The statistics differ slightly from the original paper due to updates in the data source.
8
Deep Spatiotemporal Point Process
On both the STH and STSC datasets with different parameter set-	Table 2: Estimated λ* (t) MAPE on synthetic data						
tings, DeepSTPP outperform the			STH			STSC	
baseline NSTPP in terms of LL and HD. It shows that DeepSTPP can		DS1	DS2	DS3	DS1	DS2	DS3
	DeepSTPP NSTPP	3.33 53.41	369.44 17.69	11.30 3.85	7.84 99.99	3.22 39.33	20.98 37.39
estimate the spatiotemporal intensity							
more accurately for point processes with unknown parameters.	RMTPP	263.83	729.78	0.62	45.55	21.26	37.46
	MLE	2.98	11.30	4.38	27.38	18.20	20.01
Temporal intensity estimate. Ta-
ble 2 shows the mean absolute percentage error (MAPE) between the models’ estimated temporal
intenSity and the ground truth λ? (t) over a short sampled range. On the STH datasets, since MLE
has the correct parametric form, it is the theoretical optimum. Compared to baselines, DeepSTPP
generally obtained the same or lower MAPE. It shows that joint spatiotemporal modeling also im-
prove the performance of temporal prediction.
Intensity visualization. Figure 3 visualizes
the learned space-time intensity and the ground
truth for STH and STSC, providing strong
evidence that DeepSTPP can correctly learn
the underlying dynamics of the spatiotempo-
ral events. Especially, NSTPP has difficulty in
modeling the complex dynamics of the mul-
timodal distribution such as the spatiotempo-
ral Hawkes process. NSTPP sometimes pro-
>-i≡
COVlD-NY EQ-JP STHPO STHP1 STHP2 STSCPO STSCP1 STSCP2
Figure 4: Log train time comparison on all datasets
duces overly smooth intensity surfaces, and lost
most of the details at the peak. In contrast, our
DeepSTPP can better fit the multimodal distri-
bution through the form of kernel summation and obtain more accurate intensity functions.
Computational efficiency. Figure 4 provides the run time comparison for the training between
DeepSTPP and NSTPP for 100 epochs. To ensure a fair comparison, all experiments are conducted
on 1 GTX 1080 Ti with Intel Core i7-4770 and 64 GB RAM. Our method is 100 times faster than
NSTPP in training. It is mainly because our spatiotemporal kernel formulation has a close form of
integration, which bypasses the complex and cumbersome numerical integration.
4	.2. Real-World Experiment Results
For real-world data evaluation, we re-
port the conditional spatial and tem-
poral log-likelihoods, i.e., log f * (s|t)
and log f * (t), of the final event given
the input events, respectively. The to-
tal log-likelihood, log f* (s, t), is the
summation of the two values.
Predictive performances. As our
model is probabilistic, we compare
Table 3: Test log likelihood (LL) comparison for space and
time on real-world data over 3 runs.
LL	COVID-19 NY		Earthquake JP	
	Space	Time	Space	Time
DeepSTPP	-0.1150±0.0109	2.4583±0.0008	-4.4025±0.0128	0.4173±0.0014
NSTPP	-0.0798±0.0433	2.6364±0.0111	-4.8141±0.1165	0.3192±0.0124
RMTPP	-	2∙4476±0.0039	-	0.3716±0.0077
9
Deep Spatiotemporal Point Process
against baselines models on the test predictive LL for space and time separately in Table 3. RMTPP
can only produce temporal intensity thus we only include the time likelihood. We observe that
DeepSTPP outperforms NSTPP most of the time in terms of accuracy. It takes only half of the time
to train, as shown in Figure 4. Furthermore, we see that STPP models (first three rows) achieve
higher LL compared with only modeling the time (RMTPP). It suggests the additional benefit of
joint spatiotemporal modeling to increases the time prediction ability.
Ablation study We conduct abla-
tion studies on the model design. Our
model assumes a global latent pro-
cess z that governs the parameters
{wi , βi , γi } with separate decoders.
We examine other alternative designs
experimentally. (1) Shared decoders:
We use one shared decoder to gener-
Table 4: Test LL for alternative model designs over 3 runs
(higher the better)	COVID-19NY		STH DS2	
	Space	Time	Space	Time
Shared decoders	-0.1152±0.0142	2.4581±0.0030	-2.4397±0.0170	-0.6060±0.0381
Separate processes	-0.1057±0.0140	2∙4561±0.0048	-2.4291±0.0123	-0.7022±0.0050
LSTM encoder	-0.1162±0.0102	2.4554±o.0035	-2.4331±0.0174	-0.6845±0.0252
DeepSTPP	-0.1150±0.0109	2.4583±0.0008	-2.4289±0.0102	-0.6853±0.0145
ate model parameters. Shared decoders input the sampled z to one decoder and partition its output
to generate model parameters.(2) Separate process: We assume that each of the {wi, βi, γi} follows
a separate latent process and we sample them separately. Separate processes use three sets of means
and variances to sample {wi , βi , γi } separately. (3) LSTM encoder: We replace the Transformer
encoder with a LSTM module.
As shown in Table 4, we see that (1) Shared decoders decreases the number of parameters
but reduces the performance. (2) Separate process largely increases the number of parameters but
has negligible influences in test log-likelihood. (3) LSTM encoder: changing the encoder from
Transformer to LSTM also results in slightly worse performance. Therefore, we validate the design
of DeepNSTPP: we assume all distribution parameters are governed by one single hidden stochastic
process with separate decoders and a Transformer as encoder.
5.	Conclusion
We propose a family of deep dynamics models for irregularly sampled spatiotemporal events. Our
model, Deep Spatiotemporal Point Process (DeepSTPP), integrates a principled spatiotemporal
point process with deep neural networks. We derive a tractable inference procedure by modeling the
space-time intensity function as a composition of kernel functions and a latent stochastic process.
We infer the latent process with neural networks following the variational inference procedure.
Using synthetic data from the spatiotemporal Hawkes process and self-correcting process, we show
that our model can learn the spatiotemporal intensity accurately and efficiently. We demonstrate
superior forecasting performance on many real-world benchmark spatiotemporal event datasets.
Future work include further considering the mutual-exciting structure in the intensity function, as
well as modeling multiple heterogeneous spatiotemporal processes simultaneously.
10
Deep Spatiotemporal Point Process
References
Zhengping Che, Sanjay Purushotham, Kyunghyun Cho, David Sontag, and Yan Liu. Recurrent
neural networks for multivariate time series with missing values. Scientific reports, 8(1):1-12,
2018.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differential equations. In Advances in neural information processing systems, pages 6571-6583,
2018.
Ricky TQ Chen, Brandon Amos, and Maximilian Nickel. Neural spatio-temporal point processes.
ICLR, 2021.
Junyoung Chung, Caglar Gulcehre, KyUngHyun Cho, and Yoshua Bengio. Empirical evaluation of
gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.
Daryl J Daley and David Vere-Jones. An introduction to the theory of point processes: volume II:
general theory and structure. Springer Science & Business Media, 2007.
Edward De Brouwer, Jaak Simm, Adam Arany, and Yves Moreau. Gru-ode-bayes: Continuous
modeling of sporadically-observed time series. In Advances in Neural Information Processing
Systems, pages 7379-7390, 2019.
Nan Du, Hanjun Dai, Rakshit Trivedi, Utkarsh Upadhyay, Manuel Gomez-Rodriguez, and Le Song.
Recurrent marked temporal point processes: Embedding event history to vector. In KDD, pages
1555-1564, 2016.
Emilien Dupont, Arnaud Doucet, and Yee Whye Teh. Augmented neural odes. In Advances in
Neural Information Processing Systems, pages 3140-3150, 2019.
Shen Fang, Qi Zhang, Gaofeng Meng, Shiming Xiang, and Chunhong Pan. Gstnet: Global spatial-
temporal network for traffic flow prediction. In IJCAI, pages 2286-2293, 2019.
Chris Finlay, Jorn-Henrik Jacobsen, Levon Nurbekyan, and Adam M Oberman. How to train your
neural ode. arXiv preprint arXiv:2002.02798, 2020.
Xu Geng, Yaguang Li, Leye Wang, Lingyu Zhang, Qiang Yang, Jieping Ye, and Yan Liu. Spatiotem-
poral multi-graph convolution network for ride-hailing demand forecasting. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 33, pages 3656-3663, 2019.
Amir Gholami, Kurt Keutzer, and George Biros. Anode: Unconditionally accurate memory-efficient
gradients for neural odes. arXiv preprint arXiv:1902.10298, 2019.
SePP Hochreiter and JUrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):
1735-1780, 1997.
Valerie Isham and Mark Westcott. A self-correcting Point Process. Stochastic processes and their
applications, 8(3):335-347, 1979.
Junteng Jia and Austin R Benson. Neural jumP stochastic differential equations. In NeurIPS, Pages
9847-9858, 2019.
11
Deep Spatiotemporal Point Process
Patrick Kidger, James Morrill, James Foster, and Terry Lyons. Neural controlled differential equa-
tions for irregular time series. NeurIPS, 2020.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint
arXiv:1312.6114, 2013.
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion convolutional recurrent neural net-
work: Data-driven traffic forecasting. In ICLR, 2018.
Wenwei Liang, Wei Zhang, and Xiaoling Wang. Deep sequential multi-task modeling for next
check-in time and location prediction. In International Conference on Database Systems for
Advanced Applications, pages 353-357. Springer, 2019.
Hongyuan Mei and Jason Eisner. The neural hawkes process: A neurally self-modulating multivari-
ate point process. In NeurIPS, 2017.
George O Mohler, Martin B Short, P Jeffrey Brantingham, Frederic Paik Schoenberg, and George E
Tita. Self-exciting point process modeling of crime. Journal of the American Statistical Associ-
ation, 106(493):100-108, 2011.
Jesper Moller and Rasmus Plenge Waagepetersen. Statistical inference and simulation for spatial
point processes. CRC Press, 2003.
Michael C Mozer, Denis Kazakov, and Robert V Lindsey. Discrete event, continuous time rnns.
arXiv:1710.04110, 2017.
Alexander Norcliffe, Cristian Bodnar, Ben Day, Jacob Moss, and Pietro Lio. Neural ode processes.
ICLR, 2021.
Yosihiko Ogata. On lewis’ simulation method for point processes. IEEE transactions on information
theory, 27(1):23-31, 1981.
Maya Okawa, Tomoharu Iwata, Takeshi Kurashima, Yusuke Tanaka, Hiroyuki Toda, and Naonori
Ueda. Deep mixture point processes: Spatio-temporal event prediction with rich contextual in-
formation. In KDD, pages 373-383, 2019.
Kira Rehfeld, Norbert Marwan, Jobst Heitzig, and Jurgen Kurths. Comparison of correlation anal-
ysis techniques for irregularly sampled time series. Nonlinear Processes in Geophysics, 18(3):
389-404, 2011.
Alex Reinhart et al. A review of self-exciting spatio-temporal point processes and their applications.
Statistical Science, 33(3):299-318, 2018.
Abolfazl Safikhani, Camille Kamga, Sandeep Mudigonda, Sabiheh Sadat Faghih, and Bahman
Moghimi. Spatio-temporal modeling of yellow taxi demands in new york city using general-
ized star models. International Journal of Forecasting, 2018.
Jin Shang and Mingxuan Sun. Geometric hawkes processes with graph convolutional recurrent
neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
pages 4878-4885, 2019.
12
Deep Spatiotemporal Point Process
Oleksandr Shchur, Ali Caner Turkmen, Tim JanusChoWski, and StePhan GUnnemann. Neural tem-
poral point processes: A review. arXiv preprint arXiv:2104.03528, 2021.
Satya Narayan Shukla and Benjamin Marlin. InterPolation-Prediction netWorks for irregularly sam-
Pled time series. In ICLR, 2018.
Ashish VasWani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Alejandro Veen and Frederic P Schoenberg. Estimation of space-time branching process models
in seismology using an em-type algorithm. Journal of the American Statistical Association, 103
(482):614-624, 2008.
Shuai Xiao, Mehrdad Farajtabar, Xiaojing Ye, Junchi Yan, Le Song, and Hongyuan Zha. Wasser-
stein learning of deep generative point process models. In NeurIPS, pages 3247-3257, 2017.
Shi Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo.
Convolutional lstm netWork: A machine learning approach for precipitation noWcasting. In
NeurIPS, pages 802-810, 2015.
Guolei Yang, Ying Cai, and Chandan K. Reddy. Recurrent spatio-temporal point process for check-
in time prediction. In CIKM, pages 2203-2211. ACM, 2018.
Huaxiu Yao, Xianfeng Tang, Hua Wei, Guanjie Zheng, and Zhenhui Li. Revisiting spatial-temporal
similarity: A deep learning frameWork for traffic prediction. In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volume 33, pages 5668-5675, 2019.
Qiang Zhang, Aldo Lipani, Omer Kirnap, and Emine Yilmaz. Self-attentive haWkes processes. In
International Conference on Machine Learning (ICML), 2020.
Liang Zhao, Qian Sun, Jieping Ye, Feng Chen, Chang-Tien Lu, and Naren Ramakrishnan. Multi-
task learning for spatio-temporal event forecasting. In KDD, pages 1503-1512, 2015.
Shixiang Zhu, Shuang Li, Zhigang Peng, and Yao Xie. Imitation learning of neural spatio-temporal
point processes. arXiv preprint arXiv:1906.05467, 2019.
Simiao Zuo, Haoming Jiang, Zichong Li, Tuo Zhao, and Hongyuan Zha. Transformer haWkes
process. International Conference on Machine Learning (ICML), 2020.
13
Deep Spatiotemporal Point Process
Appendix A.	Model Details
A.1. Spatiotemporal Point Process Derivation
Conditional Density. The intensity function and probability density function of STPP is related:
f(S，t|Ht)=I-*F⅛)
λ*(s,t) exp
λ*(s, t) exp
λ*(s,τ )dτds
λ* (τ)dτ
The last equation uses the relation that λ*(s, t) = λ* (t)f (s|t), according Daley and Vere-Jones
(2007) Chapter 2.3 (4). Here λ*(t) is the time intensity and f * (s|t) := f (s|t, Ht) is the spatial PDF
that the next event will be at location s given time t. According to Daley and Vere-Jones (2007)
Chapter 15.4, we can also view STPP as a type of TPP with continuous (spatial) marks,
Likelihood. Given a STPP, the log-likelihood of observing a sequence Ht = {(s1, t1), (s2, t2), ...(sn, tn)}tn≤t
is given by:
n
L(Htn) = log Yf(si,ti|Hti-1)(1- F*(s,t))
i=1
n
=X
i=1
log λ*(si, ti)- Z Z	λ*(τ)dτds
S ti-1
n
X log λ*(si, ti)-
i=1
ZSZ0tn
+ log(1 - F*(s,t))
λ*(s, τ)dτ-
λ*(s, τ)dτ
nT
=	log λ*(si, ti)-	λ*(s, τ)dτ
nn	T
=X log λ*(ti) + X log f*(Si∣ti) - / λ*(τ)dτ
Inference. With a trained STPP and a sequence of history events, we can predict the next event
timing and location using their expectations, which evaluate to
E[tn+1|Htn] =	∞t	f(s,t|Htn)dsdt=	∞texp - tλ*(τ)dτ
tn	S	tn	tn	S
= Z t exp -Z λ*(τ)dτ λ*(t)dt
tn	tn
The predicted location for the next event is:
λ*(s, t)dsdt,
(12)
E[sn+1|Htn] =	∞s λ* (s, t) exp
tn S
λ*(s, τ)dτ dsdt
∞ exp - t λ*(τ)dτ	sλ*(s, t)dsdt
(13)
14
Deep Spatiotemporal Point Process
Computational Complexity. It is worth noting that both learning and inference require condi-
tional intensity. If the conditional intensity has no analytic formula, then we need to compute
numerical integration over S . Then, evaluating the likelihood or either expectation requires at least
triple integral. Note that E[ti |Hti-1] and E[si|Hti-1] actually are sextuple integrals, but we can
memorize all λ*(s,t) from t = t— to t》ti-ι to avoid re-compute the intensities. However,
memorization leads to high space complexity. As a result, we generally want to avoid an intractable
conditional intensity in the model.
A.2. Deep Spatiotemporal Point process (DeepSTPP) Derivation
PDF Derivation
recall that
The model design of DeepSTPP enjoys a closed form formula for the PDF. First
f*(t) = λ* (t) exp
Also notice that f *(s,t) = f *(s∣t)f *(t), λ*(s,t) = f *(s∣t)λ*(t) and λ*(t)=
Therefore
f *(t)
1 - F*(t).
f*(s,t)=f*(s|t)f*(t)
f*(s | t)λ*(t)exp
λ*(τ)dτ
λ*(s, t) exp
For DeepSTPP, the spatiotemporal intensity is
λ* (s, t) =	wiexp(-βi(t - ti))ks (s - si)
i
The temporal intensity simply removes the ks (which integrates to one). The bandwidth doesn’t
matter.
λ*(t) = X wi exp(-βi(t - ti))
i
Integrate λ* (τ) yields
λ λ*(τ)dτ = — X wi exp(一βi(τ — ti)) + C
i βi
Note that deriving the exp would multiply the coefficient -βi.
The definite integral is
λ λ*(τ)dτ = — X Iw[exp(-βi(t — ti)) — exp(一βi(tn — ti))]
tn	i i
15
Deep Spatiotemporal Point Process
Then replacing the integral in the original formula yields
f*(s,t)= λ*(s,t)exp (- λ λ*(τ)dτ
tn
=λ*(s,t) exp
(X Wwi [exp(-βi(t - ti)) - exp(-βi(tn - ti))]
The temporal kernel function kt(t, ti) = exp(-βi(t - ti)), we reach the closed form formula.
Inference The expectation of the next event time is
E*[ti] =	tf * (t)dt =	tλ*(t) exp
ti-1	tn
λ* (τ)dτ dt
where the inner integral has a closed form. It requires 1D numerical integration.
Given the predicted time ι⅛, the expectation of the space can be efficiently approximated by
E*[si] ≈ E*[si∣ti] = EaTWiOkt(ti,tio)sio
i0<i
where a = £可< Wi，kt(ti, t^) is a normalize coefficient.
A.3. Spatiotemporal Hawkes Process Derivation
Spatiotemporal Hawkes process (STHP). Spatiotemporal Hawkes (or self-exciting) process is
one of the most well-known STPPs. It assumes every past event has an additive, positive, decay-
ing, and spatially local influence over future events. Such a pattern resembles neuronal firing and
earthquakes.
Spatiotemporal Hawkes is characterized by the following intensity function (Reinhart et al.,
2018):
λ*(s, t) := μgo(s) + X gι(t, ti)g2(s, Si) ： μ > 0	(14)
i:ti<t
where g0(S) is the probability density of a distribution overS, g1 is the triggering kernel and is often
implemented as the exponential decay function, g1(∆t) := a exp(-β∆t) : a, β > 0, and g2(S, Si)
is the density of an unimodal distribution over S centered at Si .
Maximum Likelihood. For spatiotemporal Hawkes process, we pre-specified the model kernels
g0(S) and g2 (S, Sj) to be Gaussian:
g0(S) ：= 2∏ Bg0|-1 exP (-I(S - %)ς-I(S - Sμ)T
g2(S, Sj)	：= 2∏Ng2|T exP (-2(S - Sj)ς-I(S - Sj)T
(15)
(16)
16
Deep Spatiotemporal Point Process
Specifically for the STHP, the second term in the STPP likelihood evaluates to
Z λ*(τ )dτ = μT + α
0
e-β(τ-u)dN(u)dτ
(0 ≤ u ≤ τ, 0 ≤ τ ≤ T) → (u ≤ τ ≤ T, 0 ≤ u ≤ T)
=μT + α
e-β(τ-u)dτdN(u)
-β(T -u) - 1 dN (u)
α
=μT - β
N
“α Xhe-
i=0
-β(T-ti) - 1i
Finally, the STHP log-likelihood is
n
N
L = Xlogλ*(%,ti)-μT + β X [e-β(TT)- 1]
i=1	i=0
This model has 11 scalar parameters: 2 for Sμ, 3 for Σg0, 3 for Σg2, α, β, and μ. We directly
estimate Sμ as the mean of {si}n, and then estimate the other 9 parameters by minimizing the
negative log-likelihood using the BFGS algorithm. T in the likelihood function is treated as tn .
Inference Based on the general formulas in Appendix A.1, and also note that for an STHP,
Z λ* (τ)dτ = Z λ* (τ)dτ - Z	λ* (τ)dτ
ti-1	0	0
=卜一α X He(Ij) - ιi
I j j=0
i-1
“ti-ι- α X[e-β(Jf) - li
β j=0
—
i-1
=μ(t - ti-ι) - α X [e-β(Ji-ι+ti-ι-tj) - e-β(JTj)]
β j=0
i-1
=μ(t - ti-ι) - α [—凤…T)-I) χ [e-β(ti-ι-tj[ and
β	j=0
/ sμg2(s, Sμ)ds = μsμ
nn
s	g1(t, ti)g2(s, si)ds =	g1 (t, ti)	sg2(s,si)ds
n
g1 (t, ti)si
i=0
n
J sλ*(s,t)ds = μsμ + Egι(t,t)si,
17
Deep Spatiotemporal Point Process
we have
Z∞
i-1
1-tj)i — μ(t — ti-ι) dt and
E[si∣Hti-ι] = I I μsμ + α
ti-1
i-1
X e-β(t-tj)
j=0
i-1
exp I e- 卜-β(t-tiT)-I) χ ,-β(ti-1-tj) — μ(t — t7)
β	j=0
dt
Both require only 1D numerical integration.
Spatiotemporal Self-Correcting process (STSCP). A lesser-known example is self-correcting
spatiotemporal point process Isham and Westcott (1979). It assumes that the background inten-
sity increases with a varying speed at different locations, and the arrival of each event reduces the
intensity nearby. The next event is likely to be in a high-intensity region with no recent events.
Spatiotemporal self-correcting process is capable of modeling some regular event sequences,
such as an alternating home-to-work travel sequence. It has the following intensity function:
λ*(s,t)= μ exp(g0(s)βt- X αg2(s, Si)) : α,β,μ> 0	(17)
i:ti<t
Here g0 (s) is the density of a distribution over S, and g2(s, si) is the density of an unimodal distri-
bution over S centered at si .
Appendix B.	Simulation Details
In this appendix, we discuss a general algorithm for simulating any STPP, and a specialized algo-
rithm for simulating an STHP. Both are based on an algorithm for simulating any TPP.
B.1.	TPP Simulation
The most widely used technique to simulate a temporal point process is Ogata’s modified thinning
algorithm, as shown in Algorithm 1 Daley and Vere-Jones (2007) It is a rejection technique; it
samples points from a stationary Poisson process whose intensity is always higher than the ground
truth intensity, and then randomly discards some samples to get back to the ground truth intensity.
The algorithm requires picking the forms of M*(t) and L*(t) such that
sup(λ*(t + ∆t), ∆t ∈ [0, L(t)]) ≤ M*(t).
In other words, M* (t) is an upper bound of the actual intensity in [t, t + L(t)]. It is noteworthy
that if M* (t) is chosen to be too high, most sampled points would be rejected and would lead to an
inefficient simulation.
18
Deep Spatiotemporal Point Process
When simulating a process with decreasing inter-event intensity, such as the Hawkes process,
M*(t) and L*(t) can be simply chosen to be λ*(t) and ∞. When simulating a process with in-
creasing inter-event intensity, such as the self-correcting process, L*(t) is often empirically chosen
to be 2∕λ*(t), since the next event is very likely to arrive before twice the mean interval length at
the beginning of the interval. M* (t) is therefore λ*(t + L*(t)).
Algorithm 1 Ogata Modified Thinning Algorithm for Simulating a TPP
1:	Input: Interval [0, T], model parameters
2:	t 什 0, Hf
3:	while true do
4:	Compute m 什 M(t|H) , l 什 L(t∣H)
5:	Draw ∆t ~ Exp(m) (exponential distribution with mean 1∕m)
6:	if t + ∆t > T then
7:	return H
8:	end if
9:	if ∆t > l then
10:	t4—t +1
11:	else
12:	t《—t + ∆t
13:	Compute λ = λ*(t)
14:	Draw U ~ Unif(0,1)
15:	if λ∕m > u then
16:	H = H ∪ t
17:	end if
18:	end if
19:	end while=0
B.2.	STPP Simulation
It has been mentioned in Section 2.1 that an STPP can be seen as attaching the locations sampled
from f * (s|t) to the events generated by a TPP. Simulating an STPP is basically adding one step to
Algorithm 1: sample a new location from f * (s|t) after retaining a new event at t.
As for a spatiotemporal self-correcting process, neither f*(s, t) nor λ* (t) has a closed form,
so the process’s spatial domain has to be discretized for simulation. λ* (t) can be approximated
by Es∈s λ*(s, t)/|S|, where S is the set of discretized coordinates. L*(t) and M*(t) are chosen
to be 2∕λ*(t) and λ*(t + L*(t)). Since f * (s|t) is proportional to λ*(s, t), sampling a location
from f * (s|t) is implemented as sampling from a multinomial distribution whose probability mass
function is the normalized λ* (s, t).
B.3.	STHP Simulation
To simulate a spatiotemporal Hawkes process with Gaussian kernel, we mainly followed an efficient
procedure proposed by Zhuang (2004), that makes use of the clustering structure of the Hawkes
process and thus does not require repeated calculations of λ* (s, t).
19
Deep Spatiotemporal Point Process
Algorithm 2 Simulating spatiotemporal Hawkes process with Gaussian kernel
1:	Generate the background events G⑼ with the intensity λ*(s,t) = μgo(s), i.e., simulate a ho-
mogenous Poisson process Pois(μ) and sample each event's location from a bivariate Gaussian
distribution N(sμ, Σ)
2:	' = 0,S = G⑷
3:	while G' = 0 do
4:	for i ∈ G' do
5:	Simulate event i's offsprings O(' with the intensity λ*(s,t) = g1(t,ti)g2(s, Si), i.e., sim-
ulate a non-homogenous stationary Poisson process Pois(g1(t, ti)) by Algorithm 1 and
sample each event’s location from a bivariate Gaussian distribution N(si , Σ)
6:	G('+1) = Si θ('),S = S ∪ G('+1),' = ' + 1
7:	end for
8:	end while
9:	return S =0
Table 5: Parameter settings for the synthetic dataset
	α		β	μ	Σg0	Σg2
ST-Hawkes	DS1	.5	1	.2	[.20;0.2]	[0.5 0; 0 0.5]
	DS2	.5	.6	.15	[5 0; 0 5]	[.1 0; 0 .1]
	DS3	.3	2	1	[1 0;0 1]	[.1 0; 0 .1]
ST-Self Correcting	DS1	.2	.2	-1 ^^	[1 0;0 1]	[0.85 0; 0 0.85]
	DS2	.3	.2	1	[.40;0.4]	[.3 0; 0 .3]
	DS3	.4	.2	1	[.25 0; 0.25]	[.20;0.2]
B.4.	Parameter Settings
For the synthetic dataset, we pre-specified both the STSCP’s and the STHP’s kernels g0 (s) and
g2(s, sj) to be Gaussian:
g0(S) := 2∏%0|-2eχp (-2(S- [0,0Dς-1(S- [0,0])T)
g2(s, Sj ) := 2∏ %2|T eχp -- 2(s - Sj)ς-I(S - Sj)T)
The STSCP is defined on S = [0, 1] × [0, 1], while the STHP is defined on S = R2. The STSCP’s
kernel functions are normalized according to their cumulative probability on S . Table 5 shows the
simulation parameters. The STSCP’s spatial domain is discretized as an 101 × 101 grid during the
simulation.
Appendix C.	Experiment Details
In this section, we include experiment configurations and some additional experiment results.
20
Deep Spatiotemporal Point Process
C.1. Model Setup Details
For a better understanding of DeepSTPP, we list out the detailed hyperparameter settings in Table 6.
We use the same set of hyperparameters across all datasets.
Name	Value	Description
Optimizer Learning rate Momentum Epoch Batch size Encoder: nlayers Encoder: nheads Encoder: dmodel Encoder: dhidden Positional Encoding Decoder: dhidden dz J β	Adam	Optimizer of the Transformer-VAE is set to Adam -	0.01(Synthetic) / 0.015(Real World) 0.9	- 200	Train the VAE for 200 epochs for 1 step prediction 128	- 3	Encoder is composed of a stack of 3 identical Transformer layers 2	Number of attention heads in each Transformer layer 128	3-tuple history is embedded to 128-dimension before fed into encoder 128	Dimension of the feed-forward network model in each Transformer layer Sinusoidal	Default encoding scheme in Vaswani et al. (2017) 128	Decoders for wi , βi , andγi are all MLPs with 2 hidden layers whose dim = 128 128	Dimension of the latent variable z as shown in Figure 2 50	Number of representative points as described in Section 2.2; 100 during 1e-3	Scale factor multiplied to the log-likelihood in VAE loss
Table 6: Hyperparameter settings for training DeepSTPP on all datasets.
21