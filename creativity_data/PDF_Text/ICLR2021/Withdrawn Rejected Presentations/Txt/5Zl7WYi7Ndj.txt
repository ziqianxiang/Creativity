Under review as a conference paper at ICLR 2021
A Unified Framework to Analyze and Design
the Nonlocal Blocks for Neural Networks
Anonymous authors
Paper under double-blind review
Abstract
The nonlocal-based blocks are designed for capturing long-range spatial-temporal
dependencies in computer vision tasks. Although having shown excellent per-
formances, they lack the mechanism to encode the rich, structured information
among elements in an image. In this paper, to theoretically analyze the property
of these nonlocal-based blocks, we provide a unified framework to interpret them,
where we view them as a graph filter generated on a fully-connected graph. When
choosing Chebyshev graph filter, a generalized formulation can be derived for
explaining the existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,
double attention block) and uses to analyze their irrationality. Furthermore, by
removing the irrationality, we propose an efficient and robust Chebyshev spectral
nonlocal block, which can be more flexibly inserted into deep neural networks
than the existing nonlocal blocks. Experimental results demonstrate the clear-cut
improvements and practical applicabilities of the proposed spectral nonlocal blocks
on image classification (Cifar-10/100, ImageNet), fine-grained image classification
(CUB-200), action recognition (UCF-101) tasks.
1	Introduction
Capturing the long-range spatial-temporal dependencies between spatial pixels or temporal frames
plays a crucial role in the computer vision tasks. Convolutional neural networks (CNNs) are inherently
limited by their convolution operators which are devoted to concern local features and relations, e.g.,
a 7 × 7 region, and are inefficient in modeling long-range dependencies. Deep CNNs model these
dependencies, which commonly refers to enlarge receptive fields, via stacking multiple convolution
operators. However, two unfavorable issues are raised in practice. Firstly, repeating convolutional
operations comes with higher computation and memory cost as well as the risk of over-fitting He &
Sun (2015). Secondly, stacking more layers cannot always increase the effective receptive fields Luo
et al. (2016), which indicates the convolutional layers may still lack the mechanism to efficiently
model these dependencies.
Feature Extracting Input Features
Our Spectral View
Figure 1: The spatial (A) and spectral (B) view of a nonlocal block. The pink dots indicate each patch
in the feature map and the “Aggregation” means calculating the weighted mean as the numerator of
Eq. (6). The dotted arrows mean “copy” and full arrows mean “feed forward”. The green bars are the
node features and the length means their strength (best view in color).
A common practice to tackle this challenge is to aggregate the feature in a non-local way with less
number of learning weights. Thus the aggregation can act on not only the k-hop neighbors but also the
long-range positions Chen et al. (2017); Wang et al. (2018); Dai et al. (2017); Zhu et al. (2019); Zhao
1
Under review as a conference paper at ICLR 2021
et al. (2017). Typically, inspired by the self-attention strategy, the Nonlocal (NL) block firstly create
a dense affinity matrix that contains the relation between every pairwise positions by generating dot
product on their feature maps and then uses this matrix as an attention map to aggregates the features
by weighted mean. Nonetheless, because the dense attention map concerns humongous amount of
useless feature pairs (e.g. the relations between background and background), the aggregation feature
map contains too much noise and increase response on the background location. Thus, filtering
discriminative features by the convolution operator on this noisy feature map is still hard, which
limit the effectiveness of the NL block and make it unstable without elaborate arrangement (e.g. the
number or the position of the added blocks).
Recently, many researchers focuses on creating a more reasonable attention map for the NL block to
enhance its performance. Chen et al. (2018) proposes the Double Attention (A2) block that firstly
gather the feature in the entire space and then distribute them back to each location. It can be seen as
using the soft-max operator to restrain the high response of the unimportant location and generate a
more reasonable attention map for the NL Block. Yue et al. (2018) proposed the Compact Generalized
Nonlocal (CGNL) block to catch cross-channel clues, which generalizes its attention map via further
considering the correlations between pairwise channels. Due to the comprehensive channel concern,
its has satisfactory performance on fine-grained classification. However, it also increases the noise of
the attention map, which further weak its stability and robustness. To enhance the stability of the
NL block, Tao et al. (2018) proposes the Nonlocal Stage (NS) module that can follow the diffusion
nature by using the Laplacian of the affinity matrix as the attention map. Although the performance
is just comparable as the NL, it can allow a more deeper connection without performance drop. Also,
the NS module contains more than two nonlocal blocks, which increases computation complexity.
To reduces the computation complexity, Huang et al. (2019) proposes lightweight nonlocal block
called Criss-Cross Attention (CC) block, which decomposes the position-wise attention of NL into
conterminously column-wise and row-wise attention.
In this paper, profited by the graph signal processing, we proposed a framework to increase the
robustness and applicability of the nonlocal block in real-world applications by reformulating the
nonlocal block based on the property of spectral graph. This framework combines the non-parameter
feature aggregation step of nonlocal with the feature filter step to better concerns the position-
wise affinity. As shown in Fig. 1, the input image is fed into the convolutional layers to extract
discriminative features such as the wing, the head, the claw and the neck. These features can be
seen as the input of the nonlocal block. Different from the spatial view which firstly aggregates
the input features by weighted mean and then uses convolutional operator to filter as in Fig. 1 A,
our spectral view constructs a fully-connected graph based on their similarity and then directly
filters the input features in a global view profited by the graph filter shown in Fig. 1 B. Moreover,
our framework can unify five existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,
double attention block) to analyze their potential mechanism with the help of Chebyshev polynomials.
Based on these analysis, we propose a novel nonlocal block called Chebyshev Spectral Nonlocal
Block (ChebySNL) that concerns the rich, structured information in an image via encoding the
graph structure. The ChabySNL guarantees the existence of the graph spectral domain and has more
mathematical guarantees to improve the robustness and accuracy. In a nutshell, our contributions are
threefold: (1) A generalized framework is proposed in this paper to theoretically analyze and design
nonlocal-based blocks in the spectral graph view; (2) We unifying well-known nonlocal-based blocks
such as NL, NS, A2, CC, CGNL and anaylize them based on the proposed framework and analyzed
their potential mechanism under graph spectral view; (3) We propose a novel nonlocal block with our
pipeline, which achieve a clear-cut improvement over existing nonlocal-based blocks in four vision
tasks.
2	Approaches
The nonlocal operator can be explained under the graph spectral domain. It can be briefly divided
into two steps: generating a fully-connected graph to model the relation between the position pairs;
converting the input features into the graph domain and learning a graph filter. In this section, we
firstly propose our framework which gives definition of the spectral view for the nonlocal operator.
Then, we unified five existing nonlocal-based operators from this spectral view. Finally, we propose a
novel nonlocal blocks based on the framework, which is more effective and robust. In this paper, we
use bold uppercase characters to denote the matrix-valued random variable and italic bold uppercase
to denote the matrix. Vectors are denoted with lowercase.
2
Under review as a conference paper at ICLR 2021
2.1	The Spectral View of Nonlocal-based blocks
Firstly, we defines the “nonlocal operator in the spectral view” F (A, Z) by merging the aggregation
step and the filter matrix W and called it “nonlocal operator” in the following paper for simplicity:
Y =X+F(X)W=X+F(A,Z).	(1)
where F (X) is the original nonlocal whose details can be seen in Appendix. A. In the spectral
view, F(A, Z) can be seen as firstly computing the affinity matrix A that defines a graph spectral
domain and then learns a filter for graph spectral features. Specifically, a fully-connected graph
G = {V, A, Z} is firstly constructed, in which V is the vertex set. Then, the node feature Z is
transformed into the graph spectral domain by the graph Fourier transformation F (Z). Finally, a
graph filter gθ is generated to enhance the feature discrimination. From this perspective, we interpret
the nonlocal operator in the spectral view as below.
Theorem 1. Given an affinity matrix A ∈ RN×N and the signal Z ∈ RN×Cs, the nonlocal operator
is the same as filtering the signal Z in the graph domain of a fully-connected weighted graph G:
F (A, Z ) = Z *G gθ = U g(Λ)U > Z	⑵
with L = DL - A = U>ΛU,
where the fully-connected graph G = (V, Z, A) has the vertex set V, node feature Z and affinity
matrix A. Λ = diag({λ1,λ2, ∙ ∙ ∙ ,λN}) and U = {uι, u2, ∙ ∙ ∙ , UN} are the eigenvalues and
eigenvectors of the graph Laplacian L, respectively. DL is the degree matrix of L. For generality,
the graph SpeCtralfilterfUnCtiOn g(Λ) can be set as a diagonal parameter matrix Ω ∈ RN× N, i.e.,
Ω = diag(ω), ω = (ω1,ω2, ∙ ∙ ∙ ,ωn).
Property 1. Theorem.1 requires the graph LaplaCian L has non-singular eigenvalues and eigenveC-
tors. Thus, the affinity matrix A should be non-negative and symmetriC.
Remark 1. Based on Theorem. 1, new nonloCal operators Can be theoretiCally designed by using
different types of graph filter suCh as the Chebyshev filter Defferrard et al. (2016a;b), the graph
wavelet filter Hammond et al. (2011) and the Cayley filter Levie et al. (2018).
The main difference between Theorem. 1 and the original spatial view of nonlocal Wang et al. (2018)
is that the former learns a graph filter to aggregate the feature under the spectral domain while the
latter aggregate the features in a non-parameter manner which cannot sufficiently suppress the noise.
Moreover, Theorem. 1 enable us to theoretically analyze existing nonlocal blocks and design novel
nonlocal blocks.
2.2	Unifying Existing Nonlocal-based Blocks
To unifying other nonlocal-based block, here we use the Chebyshev filter to express the graph filter,
i.e. using Chebyshev polynomials Defferrard et al. (2016a) to reduce the N parameters in Ω into
k (k is the order of polynomials, and k N). For simplicity, we assume that the input Z has one
channel. Then the graph filter approximated by kth-order Chebyshev polynomials is formulated as:
K-1
X
θk Tk (L )Z	(3)
k=0
where Tk(L) = 2LTk-ι(L) - Tk-2(L) With To(L) = IN, TI(L) = L. θk is the coefficient of
the kth term which can be learned Via SGD. L = 2L∕λmaχ — IN. Since L is a normalized graph
Laplacican, the maximum eigenvalue λmax = 2, which makes L = -A. Extending Eq. (3) into
multiple channels, we can get a generalized formulation of the nonlocal operator with Chebyshev
filter:
K-1
F(A, Z) = ZW1+AZW2+ X AkZWk+1,	(4)
k=2
where F(A, Z) is the nonlocal operator, Wk ∈ RCs×C1 .
Property 2. Eq. (4) requires that the graph LaplaCiCan L is a normalized LaplaCiCan whose maximum
eigenvalue satisfies λmaχ = 2. Thus, L = 2L∕λmaχ — IN = -A.
3
Under review as a conference paper at ICLR 2021
Eq. (4) gives the connection between spatial view and spectral view of the nonlocal operator, in which
the graph filter is expressed by the aggregation between the kth neighbor nodes, i.e., all nodes for
nonlocal. Thus, existing nonlocal-based structures can be theoretically analyzed by Eq. (4) in the
spectral view. Here, we elaborate 5 types of existing nonlocal-based blocks that can be unified under
certain graph structure and assumption summarized in Table. 1. More details of the proofs can be
also found in the Appendix. B.
1)	Original Nonlocal Block Wang et al. (2018): The Nonlocal (NL) Block in the spectral view is the
same as defining the graph G = (V, DM-1M, Z) and then using the second term of the Chebyshev
polynomials to approximate the generalized graph filter, where Mij = f (Xi,: , Xj,:) is the dense
attention map.
2)	Nonlocal Stage Tao et al. (2018): The Nonlocal Stage (NS) in the spectral view is the same as
defining the graph G = (V, DM-1M, Z) and then using the 1st-order Chebyshev polynomials to
approximate the graph filter with the condition W1 = W2 = -W.
3)	Double Attention Block Chen et al. (2018): The Double Attention Block in the spectral view is the
same as defining the graph G = (V, M, Z) and then using the second term of the Chebyshev polyno-
mial to approximate the graph filter, i.e F (A, Z) = MZ W, where M = σ(X Wφ)σ(X Wψ).
4)	Compact Generalized Nonlocal Block Yue et al. (2018): When grouping all channels into
one group, the CGNL in the spectral view is the same as defining a channel-awareness graph
G = (Vf, Mf, vec(Z)) and then using the second term of the Chebyshev Polynomail to approximate
the graph filter, i.e F(A, Z) = DM-1fMfvec(Z)W,whereMifj = f(vec(X)i, vec(X)j).
5)	Criss-Cross Attention Block Huang et al. (2019): The Criss-Cross Attention Block in the spectral
view is the same as defining a graph G = (V, DC-1M C M, X) with edge mask C and then using
the second term of the Chebyshev Polynomail to approximate the graph filter with node feature X .
Table 1: Summary of five exisiting nonlocal-based blocks in the spectral view.
Models	Vertex (|V|)	Edge(∣E |)	Affinity Matrix (A)	Node Feature (Z)	Filter (F(A, Z))
Unify	-	-	-	-	PK=-I Ak ZWj
NL	N	N × N-	Dm1M	一	X WZ	AZ W
A2	N	N × N-	M	X WZ	AZ W
CGNL	NCS	NCs × NCs	M f	-Vec(X WZ)	AZ W
NS	N	N × N-	Dm1M	一	X WZ	-Z W + AZ W
CC	N	N × N-	D-AM (C。Mr	X	AZ W
2.3	Proposing Novel Nonlocal Blocks
Except for unifying existing nonlocal-based blocks, the proposed spectral view can also help to
theoretically define novel nonlocal-based block.
Based on the above section, we can that existing nonlocal-based operators use the random walk
normalized (NL, NS, CC) or the non-normalized affinity matrix (A2, CGNL) whose symmetry is not
guaranteed and depends on the affinity kernel. This makes the affinity matrix against Property. 1 and
leads to the non-existence of the graph spectral domain. Thus, their robustness and flexibility are
weakened.
Moreover, all existing nonlocal-based operators only use the second term (NL, A2, CGNL, CC) or
the 1st-order approximation with sharing weight (NS) rather than the complete form of the 1st-order
approximation, which hinders their performance.
Thus, still based on the Chebyshev filter, we firstly propose a more rational nonlocal-based block
called the Chebyshev Spectral Nonlocal Block (ChebySNL) that uses a symmetry affinity matrix
with a more complete approximation:
Y = X+Fs(A,Z) = X+ZW1 +AZW2,	(5)
s.t. A = D-2 M D-1,	M =(M + M >)/2
where Fs(A, Z) is the SNL operator, W1, W2 ∈ RCs×C1, W2 are two parameter matrixes.
Remark 2. The proposed ChebySNL following other nonlocal-based blocks that uses Chebyshev
filter as the generalized graph filter but remove their irrationalities analyzed by our spectral view.
4
Under review as a conference paper at ICLR 2021
Identity Mapping
Symmetrization
Figure 2: The implementation of our SNL. A. Three feature maps φ, ψ, Z are generated by feeding
the X into three 1 × 1 convolutions. Then, the correlation matrix M is obtained by generating
affinity kernel on φ and ψ . B The second term of Eq. (5) is calculated with Z and the a normalized
symmetrization affinity matrix A. Each row of A contains a N -dimension spatial attention map (heat
maps) and z1, z2, ∙ ∙ ∙, Zn are the column vectors of Z (for simplicity, here We PiCk n = 4 where
the white squares are the central positions we visualize). C. The graph filter is approximated by
respectively feeding the 1st-order term and the 0th-order term: Z into two convolutions. Finally,
batch normalization is conducted and added with X to obtain the output Y.
-1	-1
Remark 3. The proposed ChebySNL uses a symmetric affinity matrix A = D^2 MD^2 to ensure
the existence of the real eigenvalue, which definitely satisfies the precondition of defining a graph
filter. Thus, our ChebySNL is more stable when inserted into the deep neural networks.
Remark 4. The proposed ChebySNL uses the complete form of 1st-order Chebyshev Approximation
which is a more accurate approximation of the graph filter. Thus, our ChebySNL can give the
parameters a liberal learning space with only one more parameter matrix.
The implementation details of the ChebySNL block is shown in Fig. 2. The input feature map
X ∈ RW×H×C1 is firstlyfed into three 1×1 convolutions with the weight kernels: Wφ,ψ,g ∈ RC1 ×Cs
to subtract the number of channels and then reshaped into RWH×Cs. One of the output Z ∈ RWH×Cs
is used as the transferred feature map to reduce the calculation complexity, while the other two outputs
Φ, Ψ ∈ RWH×Cs are used to get the affinity matrix A with the affinity kernel function f (∙). Then,
A is made to be symmetric and normalized as in Eq. (5). Finally, with the affinity matrix A and
the transferred feature map Z, the output of the nonlocal block can be obtained by the Eq. (5).
Specifically, the two weight matrices W1,2 ∈ RCs ×C1 are implemented by two 1×1 convolutions.
3 Experiments
In this section, we design the ablation experiments to test the robustness of nonlocal-based blocks
with different numbers, different positions, and different channels when inserted into deep models.
Then, we show performance of the proposed SNL in 4 vision tasks, including image classification
(Cifar-10/100, ImageNet), fine-grained image classification (CUB-200), action recognition (UCF-
101). More experimental results on the person re-identification tested with ILID-SVID Wang et al.
(2014), Mars Spr (2016), and Prid-2011 Hirzer et al. (2011) datasets are given in Appendix. C. All
the methods are implemented using PyTorch Paszke et al. (2019) toolbox with an Intel Core i9 CPU
and 2 Nvidia RTX 2080 Ti GPUs.
3.1	Ablation experiments on CIFAR- 1 00
Experimental Setup The ablation experiments are conducted on CIFAR-100 dataset which contains
60, 000 images of 100 classes. We use 50, 000 images as the training set and 10, 000 images as
the testing set. PreResNet56 He et al. (2016a) is used as the backbone network. Unless otherwise
specified, we set Cs = C1 /2 and add 1 nonlocal-based block right after the second residual block in
the early stage (res1). The initial learning rate 0.1 is used with the weight decay 10-4 and momentum
0.9. The learning rate is divided by 10 at 150 and 250 epochs. All the models are trained for 300
epochs. We choose the evaluation criterion of the classification accuracy: Top1 and Top5 accuracy,
5
Under review as a conference paper at ICLR 2021
which means the model prediction (the one with the highest probability) is exactly the expected label
and 5 highest probability predictions contains the expected label.
The number of channels in transferred feature space The nonlocal-based block firstly reduces the
channels of original feature map C1 into the transferred feature space Cs to reduce the computation
complexity. If Cs is too large, the feature map will contain redundant information which introduces
the noise when calculating the affinity matrix A. However, if Cs is too small, it is hard to reconstruct
the output feature map due to inadequate features. To test the robustness for the value of the Cs, we
generate three types of models with different Cs setting: “No Reduction” (Cs = C1), “Reduction
by Two Times” (Cs = C1/2), “Reduction by Four Times ” (Cs = C1/4). Table 2 shows the
experimental results of the 3 types of models with different nonlocal-based blocks. Our SNL block
outperforms other models profited by the flexibility for learning.
Moreover, from Table 2, we can see that the performances of the CGNL steeply drops when the
number of the transferred channels increases. This is because the CGNL block concerns the relations
between channels. When the number of the transferred channels increases, the relations between the
redundant channels seriously interfere with its effects. Overall, our SNL block is the most robust
for the large number of transferred channels, which generates nearly ×5 improvement than other
nonlocals (our SNL rises 1.01% in Top1 while the best of others rises 0.18% over the backbone).
Table 2: The Performances of Nonlocal-based Blocks with Different Number of Transferred Channels
on CIFAR-100
	No Reduction	Reduction by Two Times	Reduction by Four Times
Models	Topl (%) Top5 (%)	Top1(%) Top5 (%)	Top1 (%) Top5 (%)
PreResNet56	75.33↑0∙00^^93.97↑0.00	75.3350^^93.972"	TS&oRo^^93.97200
+ NL	75.29φo.o4 94.07↑o.1o	75.31即.02	92.84"13	75.50↑o.17	93.75^0.22
+ NS	75.39↑0.06	93.00^0.97	75.83↑o∙5o 93.87φo.1o	75.61↑0∙28	93.66φ0.31
+ A2	75.51↑0.18 92.90"07	75.58↑o∙25	94.27↑o∙3o	75.61↑0∙28	93.6∕0∙36
+ CGNL	74.7/0.62	93.60Jo∙37	75.75↑o∙42	93.74φo.23	75.27^0.06	93.05^0.92
+ Ours	76.34↑1.01 94.48↑0.51	76.41↑1.08 94.38↑0∙41	76.O2↑0.69 94.08↑0.11
			
The stage/position for adding the nonlocal-based blocks The nonlocal-based blocks can be added
into the different stage of the preResNet to form the Nonlocal Network. In Tao et al. (2018), the
nonlocal-based blocks are added into the early stage of the preResNet to catch the long-range relations.
Here we show the performances of adding different types of nonlocal-based blocks into the 3 stages
(the first, the second and the third stage of the preResNet). The experimental results are shown in
Table 3. We can see that the performances of the NL block is lower than the backbones when added
into the early stage. However, our proposed SNL block has averagely 1.08% improvement over the
backbone when added into the early stage, which is more than ×2 improvement over other types of
nonlocal-based blocks (0.42% for the best case).
Table 3: The Performances of Nonlocal-based Blocks Inserted into Different Position on CIFAR-100
	Stage 1	Stage 2	Stage 3
Models	Top1 (%) Top5 (%)	Top1(%) Top5 (%)	Top1 (%) Top5 (%)
PreReSNet56	75.33↑o.oo 93.97↑o.oo	75.33↑0.00^^93.97↑0.00	75.33↑0.00 93.97↑0.00^^
+ NL	75.3/0.02	92.84"13	75.64↑0.31	93.79^0.18	75.28^0.05 93.93^0.04
+ NS	75.83↑0.50	93.87φ0.10	75.74↑0.41 94.02↑0.05	75.44↑0.11	93.8610.11
+ A2	75.58↑o.25 94.27↑o.3o	75.60↑0.27	93.82φ0.15	75.2/0.12	93.65φ0.32
+ CGNL	75.75↑0.42	93.74^0.23	74.54Φ0.79	92.65^1.32	74.9010.43 92.4611.51
+ Ours	76.41↑1.08 94.38↑0∙41	76.29↑0.96 94.27↑0.30	75.68↑0.35	93.90J0.07
The number of the nonlocal-based blocks We test the robustness for adding nonlocal-based blocks
into the backbone. The results are shown in Table 4. “x3" means three blocks are added into the
stage 1, 2, and 3 respectively, and the accuracy in the brackets represent their results. We can see
that adding three proposed SNL operators into different stages of the backbone generates a larger
improvement (1.37%) than the NS operator and NL operator. This is because when adding NS and
NL into the early stage, these two models cannot well aggregate the low-level features and interfere
with the following blocks.
6
Under review as a conference paper at ICLR 2021
Table 4: Experiments for Adding Different Number of Blocks into PreResNet56 on CIFAR-100
Models	Topl (%)	Top5 (%)
PreResNet56	75.33↑0.00	93.97↑0.00
+ NL(x3f)	75.31J0.02 (74.34299)	92.84Jj3 (93.∏286)
+ NS (×3)	75.83↑0.50 (75.00233)	93.87,0.10 (93.57j0.40)
+ A2 (×3)	75.58↑0.25 (75.63↑0.33)	94.27↑0.30 (94.12↑0.15)
+ CGNL (×3)	75.75↑0.42 ( 75.96↑0.63)	93.74j0.23 (93.10287)
+ Ours (×3)	76.41↑1.08 (76.70↑1∙37)	94.38↑0.41 (93.94,0.03)
↑ The number in the bracket means the performance when adding 3 this kind of nonlocal-based blocks.
3.2	Applications on computer vision tasks
Image Classification CIFAR-10/100 and ImageNet are tested where our SNL outperforms other
types of the nonlocal-based blocks on these standard benchmarks. We use the ResNet50 He et al.
(2016b) as the backbone and insert the SNL block right before the last residual block of res4 for fair
comparison. Other settings for the CIFAR-10/100 are the same as our ablation experiments (Sec. 3.1).
For the ImageNet, the initial learning rate 0.01 is used with the weight decay 10-4 and momentum
0.9. The learning rate is divided by 31 at 61 and 81 epochs. All the models are trained for 110 epochs.
The “Floating-point operations per second” (Flops) and the “Model size” (Size) are used to compare
the computation complexity and memory consumption.
Table 5 shows the experimental results on the CIFAR10 dataset. When adding one proposed block,
the Top1 classification accuracy rises about 0.38%, which is nearly twice over other types of nonlocal-
based blocks (the best is 0.21%). As the experiments on CIFAR100 shown in Table 5, using our
proposed block brings significant improvements about 1.67% with ResNet50. While using a more
simple backbone PreResnet56 as shown in Table 5, our model can still generate 1.08% improvement
which is not marginal.
The visualization of the attention maps for two positions (“Pink” and “Orange” dots). The heatmaps
show the strength of similarity between them and other positions.
The results of ImageNet are shown in Table 6. Note that other baselines are reported with the
scores in their paper. We can see that compared with the nonlocal-based blocks, our SNL achieves a
clear-cut improvement (1.96%) with a minor increment in complexity (0.51G Flops and 2.62M of
Size compared with original 4.14G Flops and 25.56M). Moreover, our SNL is also superior to other
types of blocks such as SE block Hu et al. (2018b), CGD block He et al. (2019), GE block Hu et al.
(2018a) (0.11% higher in Top1 and 2.02M lower in size than the GE block).
Table 5: Experiments for Adding Different Types of Nonlocal-based Blocks into PreResnet56 and
ResNet50 on CIFAR-10/100 Dataset
	CIFAR-10/Resnet50				CIFAR-100/ReSnet50		CIFAR-100 / PreReSnet56	
Models	Top1 (%)	Top5 (%)	Topl (%)	Top5 (%)	Top1 (%)	Top5 (%)
Backbone	94.94↑0.00	99.87↑0∙00	76.5O↑0∙00	93.14↑0.00	75.33↑0.00	93.97↑0.00
+ NL	94.01,0.93	99.82j0.05	76.77↑0.27	93.55↑0.41	75.31,0.02	92.84,1.33
+ NS	95.15↑0.21	99.88↑0.01	77.90↑1.40	94.34↑1.20	75.83↑0.50	93.87,0.10
+ A2	94.41,0.53	99.83j0.05	77.30↑0.80	93.4O↑0.26	75.58↑0.25	94.27↑0.30
+ CGNL	94.49,0.45	99.92↑0∙05	74.88口.62	92.56,0.58	75.75↑0.42	93.74,0.23
+ Ours	95.32↑0∙38	99.94↑0.07	78.17↑1.67-	94.17↑1.03	76.41↑1.08~	94.38↑0∙39
7
Under review as a conference paper at ICLR 2021
We also visualize the output feature maps of the ResNet50 with SNL and the original ResNet50 in
Fig. 3 A. Benefited from the rich and structured information considered in SNL, the response of the
similar features between long-range spatial positions are enhanced as shown in the two mushroom,
balls, and those animals. Moreover, Fig. 3 B shows the attention maps produced by our SNL and the
original NL block where the “Pink” and “Orange” dots are the central positions and the heatmaps
represent the similarity between the central position and other positions. Compared with the original
NL block, SNL can pay more attention to the crucial parts than the original NL block profited by the
better approximation formation as discussed in Sec. 2.3.
Fine-grained Image Classification The experiments for the fine-grained classification are generated
on the Birds-200-2011 (CUB-200) dataset which contains 11, 788 images of 200 categories of
different birds. We use 5, 994 images as the training set and 5, 794 images as the testing set as Yue
et al. (2018). We use the ResNet50 model pre-trained on ImageNet as the backbone and train the
models for total 110 epochs with the initial learning rate 0.1 which is subsequently divided by 10
at 31, 61, 81 epochs. Table 7 (CUB-200) shows that our model can generate (0.59%) improvement.
Compared with the CGNL block concerning channel-wise relations, our SNL is just a bit lower in
Top1 (0.12%). That is because the dependencies between channels play an important role in the
fine-grained classification. However, these channel dependencies of CGNL can impede the practical
implementations, which needs elaborate preparations for the number of channels per block, the
number of blocks and their positions as shown in Table 2, 3, 4. Compared with the other nonlocal
block with non-channel concerned, our SNL has improvements with a large margin.
Action Recognition Experiments are conducted on the UCF-101 dataset, which contains 9, 537
videos for 101 different human actions. We use 7, 912 videos as the training set and 1, 625 videos as
the testing set. Our SNL block are tested on the UCF-101 dataset for capturing the dependence for
the temporal frames. We follow the I3D structure Hara et al. (2018) which uses k × k × k kernels to
replace the convolution operator in the residual block for learning seamless spatial-temporal feature
extractors. The weights are initialized by the pre-trained I3D model on Kinetics dataset Kay et al.
(2017). Inserting nonlocal-based blocks into the I3D can helps to capture the relations between frame
pairs with long distance and improves the feature representation. We train the models with the initial
learning rate of 0.01 which is subsequently divided by 10 each 40 epochs. The training stops at the
100 epochs. Other hyper-parameters of the experimental setup are the same as in Sec. 3.1.
Table 7 (UCF-101) shows the results on the action recognition. The network with our proposed block
can generate significant improvements (2.82%) than the I3D and outperforms all other nonlocal-based
models on the UCF-101 dataset. This shows that our proposed SNL is also effective for catching the
long-range dependencies between the temporal frames. We also conduct the experiments on UCF-101
dataset with other state-of-the-art action recognition models in Appendix. C, which also shows that
the effectiveness of our proposed ChebySNL.
Table 6: Experiments for adding different types of Table 7: Experiments for adding NLs on
nonocal-based blocks into Resnet50 on ImageNet CUB-200 and UCF-101 Datasets
Models	Top1(%)	Flops (G)	Size (M)
ResNet50	76.15↑0.00	4.14	25.56
+ CGD	76.90↑0.75	+0.01	+0.02
+ SE	77.72↑1.57	+0.10	+2.62
+ GE	78.00↑1.85	+0.10	+5.64
+ NL	76.70↑0.55	^^+0.41 ^^	+2.09
+ A2	77.00↑0.85	+0.41	+2.62
+ CGNL	77.32↑1.17	+0.41	+2.09
+ Ours	78.11↑1.96-	+0.51	+2.62
	CUB-200	UCF-101
Models	Top1(%)	Top1(%)-
Backbone'	85.43↑0.00	81.57↑0.00
+ NL	85.34φ0.09	82.88↑1.31
+ NS	85.54↑0.11	82.50↑0.93
+A2	85.91↑0.48	82.68↑1.11
+ CGNL	86.14↑0.71	83.38↑1.81
+ Ours	86.02↑0.59-	84.39↑2.82-
f The ResNet50 is used for CUB-200 as the
backbone and I3D is used for UCF-101 dataset.
4 Conclusion
In this paper, we propose a framework to design nonlocal which can captures the long-range de-
pendencies between spatial pixels (image classification) or temporal frames (video classification).
Inspired by our framework, we can interpret the existing nonlocals in the graph view, and design the
ChebySNL block, which is more robust and effective. Experimental results demonstrate the clear-cut
improvements across four vision tasks.
8
Under review as a conference paper at ICLR 2021
References
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully
connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 40(4):
834-848, 2017.
YUnPeng Chen, Yannis Kalantidis, Jianshu Li, ShUicheng Yan, and Jiashi Feng. A^ 2-nets: Double
attention networks. In Neural Information Processing Systems (NeurIPS), pp. 352-361, 2018.
Nieves Crasto, PhiliPPe WeinzaePfel, Karteek Alahari, and Cordelia Schmid. MARS: Motion-
Augmented RGB Stream for Action Recognition. In IEEE Conference on Computer Vision and
Pattern Recognition (CVPR), 2019.
Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable
convolutional networks. PP. 764-773, 2017.
Michael Defferrard, Xavier Bresson, and Pierre Vndergheynst. Convolutional neural networks on
graPhs with fast localized sPectral filtering. In Neural Information Processing Systems (NeurIPS),
PP. 3844-3852, 2016a.
Michael Defferrard, Xavier Bresson, and Pierre Vndergheynst. Convolutional neural networks on
graPhs with fast localized sPectral filtering. In Neural Information Processing Systems (NeurIPS),
PP. 3844-3852, 2016b.
Jiyang Gao and Ram Nevatia. Revisiting temPoral modeling for video-based Person reid. arXiv
preprint arXiv:1805.02104, 2018.
David K Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on graphs via spectral
graPh theory. Applied and Computational Harmonic Analysis (ACHA), 30(2):129-150, 2011.
Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history
of 2d cnns and imagenet? In IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 6546-6555, 2018.
Kaiming He and Jian Sun. Convolutional neural networks at constrained time cost. In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5353-5360, 2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual
networks. In European Conference on Computer Vision (ECCV), pp. 630-645. Springer, 2016a.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
770-778, 2016b.
Xiangyu He, Ke Cheng, Qiang Chen, Qinghao Hu, Peisong Wang, and Jian Cheng. Compact global
descriptor for neural networks. arXiv preprint arXiv:1907.09665, 2019.
Martin Hirzer, Csaba Beleznai, Peter M. Roth, and Horst Bischof. Person Re-Identification by
Descriptive and Discriminative Classification. In Proc. Scandinavian Conference on Image
Analysis (SCIA), 2011.
Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature
context in convolutional neural networks. In Neural Information Processing Systems (NeurIPS),
2018a.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In IEEE Conference on Computer
Vision and Pattern Recognition (CVPR), 2018b.
Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:
Criss-cross attention for semantic segmentation. In IEEE International Conference on Computer
Vision (ICCV), pp. 603-612, 2019.
9
Under review as a conference paper at ICLR 2021
Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,
Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset.
arXiv preprint arXiv:1705.06950, 2017.
Alexander Kozlov, Vadim Andronov, and Yana Gritsenko. Lightweight network architecture for
real-time action recognition. arXiv preprint arXiv:1905.08711, 2019.
Ron Levie, Federico Monti, Xavier Bresson, and Michael M Bronstein. Cayleynets: Graph con-
volutional neural networks with complex rational spectral filters. IEEE Transactions on Signal
Processing (TSP), 67(1):97-109, 2018.
Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive
field in deep convolutional neural networks. In Neural Information Processing Systems (NeurIPS),
pp. 4898-4906, 2016.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performance deep learning library. In Neural Information Processing Systems (NeurIPS), pp.
8024-8035, 2019.
Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d
residual networks. In IEEE International Conference on Computer Vision (ICCV), pp. 5533-5541,
2017.
MARS: A Video Benchmark for Large-Scale Person Re-identification, 2016. Springer.
Yunzhe Tao, Qi Sun, Qiang Du, and Wei Liu. Nonlocal neural networks, nonlocal diffusion and
nonlocal modeling. In Neural Information Processing Systems (NeurIPS), pp. 496-506, 2018.
Taiqing Wang, Shaogang Gong, Xiatian Zhu, and Shengjin Wang. Person re-identification by video
ranking. In European Conference on Computer Vision (ECCV), pp. 688-703. Springer, 2014.
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7794-7803, 2018.
Kaiyu Yue, Ming Sun, Yuchen Yuan, Feng Zhou, Errui Ding, and Fuxin Xu. Compact generalized
non-local network. In Neural Information Processing Systems (NeurIPS), pp. 6510-6519, 2018.
Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene
parsing network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
2881-2890, 2017.
Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable,
better results. pp. 9308-9316, 2019.
10
Under review as a conference paper at ICLR 2021
A Background
A.1 Nonlocal Block
The Nonlocal Block (NL) follows the nonlocal operator that calculates a weighted mean between the
features of each position and all possible positions as shown in Fig. 1 A. The nonlocal operator is
defined as:
F(Xi,:) =X f(Xi,:,Xj,:)g(Xj,:) /Xf(Xi,:,Xj,:)	(6)
jj
where X ∈ RN×C1 is the input feature map, i,j are the position indexes in the feature map, f (∙) is
the affinity kernel which can adopt the “Dot Product”, “Traditional Gaussian”, “Embedded Gaussian”
or other kernel metrics with a finite Frobenius norm. g(∙) is a linear embedding that is defined as:
g(Xj,:) = Xj,:WZ with WZ ∈ RC1 ×Cs. Here N is the total positions of each features and C1, Cs
are the number of channels for the input and the transferred features.
When inserting the NL block into the network structure, a linear transformation and a residual
connection are added:
Yi,: = Xi,: + F (Xi,:)W,	(7)
where W ∈ RCs ×C1 is the weight matrix.
A.2 Graph Fourier Transform & Graph Filter
The graph Fourier transform F of any function F ∈ RN on the vertices of a graph G is defined as
the expansion of F in terms of the eigenvectors of the graph Laplacian:
N
F(λι):= hF,uιi = X F (i)u↑(i)	(8)
i=1
where λ = {λ1, λ2, ..., λl, ...} and U = {u1, u2, ...ul, ...} are the eigenvalue and eigenvector of the
graph Laplacian, u； is the 1仍 column vector of U>.
The inverse graph Fourier transform FT then given by
N
F-1(λι) = X F (i)uι(i)	(9)
i=1
Based on the graph Fourier transform, the graph convolution of the input signal x with a filter gθ can
be defined as
X *g gθ = F-1(F(x) Θ F(g))	(10)
where the Hadamard product.
B Details of the Proving for the relations
We give the details of the relationship between other nonlocal operators in the spectral view discussed
in our paper as shown in Table 1. In the following proving, we assume that X ∈ RN×C, Z = g(X) =
XWZ, Mij = f(Xi, Xj ). All the normalized term uses the inverse of the degree 1/di where
di = Pj f(Xi, Xj). We also merge the output of the operators with the weight kernel W ∈ RN×C
and defines it as O for consistency. Thus the target formulations in this section are a bit different
with the definition in their own papers.
B.1 Nonlocal Block
The Nonlocal (NL) Block in the spectral view is the same as defining the graph G = (V, D-1M, Z)
and then using the second term of the Chebyshev Polynomail to approximate the graph filter.
11
Under review as a conference paper at ICLR 2021
Proof. The NL operator defined in Wang et al. (2018) can be formulated as:
Oi,: =
Pjhf (Xi,:, Xj,：)g(Xj,： )i
Pjf (Xi,：, Xj,：)
(11)
To unify it by our spectral view, we firstly define the graph G = (V, A, Z) to represent the graph
structure of the NL operator, where the affinity matrix A is calculated by:
A=DM-1M,	M=f(Xi,:,Xj,:)
Thus, each element of the affinity matrix A is:
Aij = (DMIM )ij = f≡
(12)
(13)
Based on Theorem. 1, when using Chebyshev polynomail to approximate the generalized graph filter
Ω and only choosing the second term, it becomes:
F(A, Z) = AZW
(14)
Then taking Eq. (13) into this equation, we can get the formulation of the NL operator:
Pj hf (Xi,:, Xj,:)g(Xj,:)i
…)=	Pj fX,：, Xj,：)	W
(15)
B.2 Nonlocal Stage
The Nonlocal Stage (NS) in the spectral view is the same as defining the graph G = (V, DM-1M, Z)
and then using the 1st-order Chebyshev Polynomail to approximate the graph filter with the condition
W1 = W2 = -W.
Proof. The NS operator given defined in Tao et al. (2018) can be formulated as:
Pjhf(Xi,：, Xj,)(Zj, - Zi,:)i
Oi=	Pjf (Xi,：, Xj,:)	W
(16)
Similar with the proof of NL, we can get each element of the affinity matrix A as:
Aij=(DMIM )j=Pf(XXiXXl)
(17)
The graph filter Ω on G is approximated by the Chebyshev PolynomaiL When using the 1st-order
Chebyshev Approximation, it becomes:
F(A, Z) =ZW1 - AZW2
(18)
□
When sharing the weight for W1 and W2, i.e W1 = W2 = -W, we get:
F(A, Z) =AZW-ZW
Then, taking it Z = g(X) = XWZ and Eq. (17) into this equation, it becomes:
Pj hf (Xi,：, Xj,：)Zj,：Wi
…Z )=	Pj f(Xi,：, Xjj - ZiW
(19)
(20)
12
Under review as a conference paper at ICLR 2021
Due to the fact that
Pj f(Xi,：,Xj,：)
Pjf(Xi,：,Xj,：)
1, we can get the formulation of the NS operator:
Fi(A,Z)
Pjf(Xi,:, Xj,JZj,w]
-Pj f (Xi,:, Xj,:)-
Ej f(Xi,：, Xj,：)
Pj,: f (Xi,:，Xj,∙)
Zi,:W
Pjf(Xi,:, Xj,J(Zj,: - Zi,:)]
Pj f (Xi,:, Xj,:)
(21)
B.3 Double Attention Block
The Double Attention Block in the spectral view is the same as defining the graph G = (V, M, Z)
and then using the second term of the Chebyshev Polynomail to approximate the graph filter, i.e
F(A, Z) = MZW:
Proof. The A2 operator defined in Chen et al. (2018) can be formulated as:
O = σ(θ(X))σ(φ(X)T)g(X) = fa(Xi,:,Xj,:)XW	(22)
The difference between the double A2 operator and the NL operator is only the kernel function that
calculating the affinity matrix. Thus we can use the similar proving strategy to reformulate the A2
operator into the spectral only by change the affinity matrix as:
A = M = σ(X Wφ)σ(X Wψ)
(23)
—
W
□
□
B.4 Compact Generalized Nonlocal Block
When grouping all channels into one group, the Compact Generalized Nonlocal Block in the spectral
view is the same as defining the graph G = (Vf, DM-1fMf, vec(Z)) and then using the second term
of the Chebyshev Polynomail to approximate the graph filter, i.e F(A, Z) = DM-1fMfvec(Z)W.
Note that due to the dimension of the input feature vec(Z) ∈ RNC×1 which is different with other
nonlocal operators, here we uses Mf, Af ∈ RNC×NC for clearity.
Proof. The CGNL operator defined in Yue et al. (2018) can be formulated as:
vec(O) = f (vec(X), vec(X))vec(Z)W	(24)
For simplicity, we use x to represent vec(X), thus the target becomes:
o = f(x, x)zW	(25)
Then, we define the graph G = (Vf, Af, z), where the set Vf contains each index (including position
and channel) of the vector x. The affinity matrix A is calculated by:
Af = Mf,	Mf = f(x, x)
(26)
13
Under review as a conference paper at ICLR 2021
The graph filter Ω on G is approximated by the Chebyshev polynomials. When only choosing the
second term, we can get the formulation of the CGNL operator:
F(Af, z) = AfzW = f(x, x)zW
(27)
□
B.5 Criss-Cross Attention Block
The Criss-Cross Attention Block in the spectral view is the same as defining the graph G =
(V, DC-1MCM, X) and then using the second term of the Chebyshev Polynomail to approximate
the graph filter with node feature X :
Proof. The criss-cross attention operator defined in Huang et al. (2019) can be formulated as:
X X A ɪ	Pj∈Vi f (Xi,：，Xj,：)Xj,：
Oi,: = T Aijφj,: = ~p------------f(χ X ) W
j∈Vi	j∈Vi f(Xi,:, Xj,:)
where the set Vi is collection of feature vector in V which are in the same row or column with
position u.
Then, we define the graph G = (V, A, X) to represennt the criss-cross attention operator in the
spectral view. The affinity matrix A is calculated by:
~	∙1
A = DCMCM,	M=f(Xi,Xj)
C	1	j ∈Vi
Cij =	0	else	,
We use M to represent C M, i.e. M = C M. Thus, each element of the affinity matrix M is:
Mij
Mij
j ∈Vi
else
Thus, we can get the definition of each element in the affinity matrix A:
f(Xi,Xj)
Pj
∈Vi f(Xi,Xj),
0,
j∈Vi
else
When using the Chebyshev polynomials to approximate the generalized graph filter Ω on G and
choose the second term, it becomes:
F(Ae,X) = AeXW
(28)
When taking Eq.28 into this formulation, we can get the formulation of CC operator:
Fi,：(A, X ) = (ZPVif(XX, XjX)X + X 0Xj,:)W
j∈Vefi f(Xi,:，Xj,:)	j/vi
Pj∈Vi f (Xi,:, Xj,:)Xj,:
=W
Pj∈Vi f (Xi,：，Xj,：)
(29)
(
□
14
Under review as a conference paper at ICLR 2021
C External Experiment
C.1 External Experiment on Action Recognization
We also conduct the experiments on UCF-101 dataset with other state-of-the-art action recognition
models in our supplementary materials including the P3D Qiu et al. (2017), the MARS Crasto
et al. (2019), and the VTN Kozlov et al. (2019). For Pseudo 3D Convolutional Network (P3D) and
Motion-augmented RGB Stream (MARS), our SNL block are inserted into the P3D right before the
last residual layer of the res3. For the Video Transformer Network (VTN), we replace its multi-head
self-attention blocks (paralleled-connected NL blocks) into our SNL blocks. We use the model
pre-trained on Kinetic dataset and fine-tuning on the UCF-101 dataset. Other setting such as the
learning rate and training epochs are the same as the experiment on I3D in our paper. We can see that
all the performance are improved when adding our proposed SNL model especially when training
end-to-end on the small-scale dataset. In sum, our SNL blocks have shown superior results across
three SOTAs (the VTN and MARS) in the action recognition tasks (0.30% improvement with VTN,
0.50% improvement with MARS).
Table 8: Experiments with state-of-the-art backbone
Models	Top1(%)
^^P3DQiu et al.(2017)	81.23↑0.00
P3D + Ours	82.65↑1.42
VTNKozlov et al.(2019)~~9O.06↑0.00
VTN + Ours	90.34↑0.30
MARS Crastoetal.(2019)^^92.29↑0.00
MARS + Ours	92.79↑0.50
C.2 Experiment on Video Person Re-identification
Table 9: Experiments on Video-Person Reidentification
Mars			ILID-SVID		PRID-2011		
Models	Rank1(%)	mAP(%)	Rank1(%)	mAP(%)	Rank1(%)	mAP(%)
ResNet50tp	82.30↑0∙00	75.70↑0.00	74.70↑0.00	81.60↑0.00	86.50↑0∙00	90.50↑0.00
+ NL	83.21↑0∙91	76.54↑0.84	75.30↑0∙60	83.00↑1.40	85.40"10	89.7θΦ0.80
+ SNL	83.40↑1.10	76.80↑1.10	76.30↑1.60	84.80↑3.20	88.80↑2.30	92.40↑1.90
For the backbone, we follow the strategy of Gao & Nevatia (2018) that use the pooling (RTMtp) to
fuse the spatial-temporal features. (Note that the models are totally trained on ilidsvid and prid2011
rather than fintuning the pretrained model on Mars.) We only insert the SNL block into the ResNet50
(right before the last residual block of res4) in RTMtp. Other block setting are the same as the setting
for fine-grained image classification on CUB in our paper. For all those datasets we train the model
with Adam with the initial learning rate 3e - 4, the weight decay 5e - 4. The learning rate is divided
by 200 at 400 epochs. All the models are trained for 500 epochs with the Cross Entropy and Triplet
Loss(margin = 0.3). We use rank-1 accuracy (Rank1) and men average precision (mAP) to evaluate
the performance for the models.
For the large scale dataset MarsSpr (2016) which contains 1261 pedestrians captured by at least 2
cameras. The bounding boxes are generated by the detection algorithm DPM and tracking algorithm
GMMCP, which forms 20715 person sequences. From Table. 9 (Mars), we can see that our SNL can
generate 1.10% improvement both on Rank1 and mAP than the backbone, which are both higher
than the original nonlocal block (0.91%onRank1, 0.84% on mAP).
We also generate experiments on two relatively small datasets: ILID-SVID datasets which contains
300 pedestrians captured by two cameras with 600 tracklets; PRID-2011 dataset which contains 200
pedestrians captured by two cameras with 400 tracklets. From Table. 9 (ILID-SVID), we can see that
our model can generate 1.60% and 3.20% improvement on the Rank1 and mAP respectively for the
15
Under review as a conference paper at ICLR 2021
ILID-SVID dataset. Moreover, on PRID-2011, we get a more higher improvement (2.30% on Rank1,
1.90% on mAP) as shown in Table. 9 (PRID-2011).
16