Under review as a conference paper at ICLR 2021
Adversarial Boot Camp: label free certi-
fied robustness in one epoch
Anonymous authors
Paper under double-blind review
Abstract
Machine learning models are vulnerable to adversarial attacks. One approach
to addressing this vulnerability is certification, which focuses on models
that are guaranteed to be robust for a given perturbation size. A drawback
of recent certified models is that they are stochastic: they require multiple
computationally expensive model evaluations with random noise added
to a given image. In our work, we present a deterministic certification
approach which results in a certifiably robust model. This approach is based
on an equivalence between training with a particular regularized loss, and
the expected values of Gaussian averages. We achieve certified models on
ImageNet-1k by retraining a model with this loss for one epoch without the
use of label information.
1 Introduction
Neural networks are very accurate on image classification tasks, but they are vulnerable
to adversarial perturbations, i.e. small changes to the model input leading to misclassifica-
tion (Szegedy et al., 2014). Adversarial training (Madry et al., 2018) improves robustness, at
the expense of a loss of accuracy on unperturbed images (Zhang et al., 2019). Model certifi-
cation (Lecuyer et al., 2019; Raghunathan et al., 2018; Cohen et al., 2019) is complementary
approach to adversarial training, which provides a guarantee that a model prediction is
invariant to perturbations up to a given norm.
Given an input x, the model f is certified to £2 norm r at x if it gives the same classification
on f (x + η) for all perturbation η with norm up to r,
arg max f (x + η) = arg max f (x), for all ∣∣η∣∣2 ≤ r	(1)
Cohen et al. (2019) and Salman et al. (2019) certify models by defining a “smoothed” model,
f smooth , which is the expected Gaussian average of our initial model f at a given input
example x,
fsmooth(x) ≈Eη[f(x+η)]	(2)
where the perturbation is sampled from a Gaussian, η 〜 N(0, σ2I). Cohen et al. (2019) used
a probabilistic argument to show that models defined by (2) can be certified to a given radius
by making a large number of stochastic model evaluations. Certified models can classify by
first averaging the model, (Salman et al., 2019), or by taking the mode, the most popular
classification given by the ensemble (Cohen et al., 2019).
Cohen et al. and Salman et al. approximate the model f smooth stochastically, using a
Gaussian ensemble, which consists of evaluating the base model f multiple times on the
image perturbed by noise. Like all ensemble models, these stochastic models require multiple
inferences, which is more costly than performing inference a single time. In addition, these
stochastic models require training the base model f from scratch, by exposing it to Gaussian
noise, in order to improve the accuracy of fsmooth. Salman et al. (2019) additionally expose
the model to adversarial attacks during training. In the case of certified models, there
is a trade-off between certification and accuracy: the certified models lose accuracy on
unperturbed images.
1
Under review as a conference paper at ICLR 2021
100
----baseline
一∙ stochastic (Cohen et al.)
一∙ stochastic (Salman et al.)
--deterministic (ours；
100
OooO
8 6 4 2
(IldOJj ADB,InUUB pωj≡t(L)u N0
Oooo
8 6 4 2
(ςldobXDsnuuepo≡Euq
(a) CIFAR-10 top-1 certified accuracy, σ = 0.10 (b) ImageNet top-5 certified accuracy, σ = 0.25
Figure 1: Certified accuracy as a function of £2 radius.
In this work, we present a deterministic model f smooth (x) given by (2). Unlike the stochastic
models, we do not need to train a new model from scratch with Gaussian noise data
augmentations, instead, we can fine tune an accurate baseline model for one epoch, using a
loss designed to encourage (2) to hold. The result is a certified deterministic model which
is faster to train, and faster to perform inference. In addition, the certification radius for
our model is improved, compared to previous work, on both the CIFAR-10 and ImageNet
databases, see Figure 1. Moreover, the accuracy on unperturbed images is improved on
ImageNet with a Top-5 accuracy of 86.1% for our deterministic model versus 85.3% for
Cohen et al. (2019) and 78.8% for Salman et al. (2019).
To our knowledge, this is the first deterministic Gaussian smoothing certification technique.
The main appeal of our approach is a large decrease in the number of function evaluations
for computing certifiably robust models, and a corresponding decrease in compute time at
inference. Rather than stochastically sampling many times from a Gaussian at inference
time, our method is certifiably robust with only a single model query. This greater speed in
model evaluation is demonstrated in Table 2. Moreover, the deterministic certified model
can be obtained by retraining a model for one epoch, without using labels. The speed and
flexibility of this method allows it to be used to make empirically robust models certifiably
robust. To see this, we test our method on adversarially trained models from Madry et al.
(2018), increasing the certified radius of the adversarially robust model, see Table 4.
Table 1: A comparison of robust models. Stochastic smoothing arises from methods like the ones
presented in Cohen et al. (2019) and Salman et al. (2019). Adversarial training from Madry et al.
(2018).
Model	Can be obtained from any pretrained model	Evaluation in one forward pass	Is certified?
Deterministic Smoothing (ours)	J	J	J
Stochastic Smoothing	X	X	J
Adversarial Training	X	J	X
Table 2: Average classification inference time (seconds)
Model	CIFAR-10		ImageNet-1k	
	CPU	GPU	CPU	GPU
Deterministic (ours)	0.0049	0.0080	0.0615	0.0113
Stochastic (Cohen et al., 2019)	0.0480	0.0399	0.1631	0.0932
2
Under review as a conference paper at ICLR 2021
2	Related work
The issue of adversarial vulnerability arose in the works of Szegedy et al. (2014) and
Goodfellow et al. (2015), and has spawned a vast body of research. The idea of training
models to be robust to adversarial attacks was widely popularized in Madry et al. (2018).
This method, known as adversarial training, trains a model on images corrupted by gradient-
based adversarial attacks, resulting in robust models. In terms of certification, early work
by Cheng et al. (2017) provided a method of computing maximum perturbation bounds
for neural networks, and reduced to solving a mixed integer optimization problem. Weng
et al. (2018a) introduced non-trivial robustness bounds for fully connected networks, and
provided tight robustness bounds at low computational cost. Weng et al. (2018b) proposed
a metric that has theoretical grounding based on Lipschitz continuity of the classifier model
and is scaleable to state-of-the-art ImageNet neural network classifiers. Zhang et al. (2018)
proposed a general framework to certify neural networks based on linear and quadratic
bounding techniques on the activation functions, which is more flexible than its predecessors.
Training a neural network with Gaussian noise has been shown to be equivalent to gradient
regularization (Bishop, 1995). This helps improve robustness of models; however, recent work
has used additive noise during training and evaluation for certification purposes. Lecuyer
et al. (2019) first considered adding random Gaussian noise as a certifiable defense in a
method called PixelDP. In their method, they take a known neural network architecture and
add a layer of random noise to make the model’s output random. The expected classification
is in turn more robust to adversarial perturbations. Furthermore, their defense is a certified
defense, meaning they provide a lower bound on the amount of adversarial perturbations for
which their defence will always work. In a following work, Li et al. (2018) provided a defence
with improved certified robustness. The certification guarantees given in these two papers
are loose, meaning the defended model will always be more robust than the certification
bound indicates.
In contrast, Cohen et al. (2019) provided a defence utilizing randomized Gaussian smoothing
that leads to tight robustness guarantees under the £2 norm. Moreover Cohen et al. used
Monte Carlo sampling to compute the radius in which a model’s prediction is unchanged; we
refer to this method as RandomizedSmoothing. In work building on Cohen et al., Salman
et al. (2019) developed an adversarial training framework called SmoothAdv and defined a
Lipschitz constant of averaged models. Yang et al. (2020) generalize previous randomized
smoothing methods by providing robustness guarantees in the £ 1, £2, and £∞ norms for
smoothing with several non-Gaussian distributions.
3	Deterministic Smoothing
Suppose we are given a dataset consisting of paired samples (x, y) ∈ X × Y where x is an
example with corresponding true classification y . The supervised learning approach trains
a model f : X -→ RNc which maps images to a vector whose length equals the number
of classes. Suppose f is the initial model, and let f smooth be the averaged model given by
equation (2). Cohen et al. (2019) find a Gaussian smoothed classification model f smooth
by sampling η 〜N(0,σ21) independently n times, performing n classifications, and then
computing the most popular classification. In the randomized smoothing method, the initial
model f is trained on data which is augmented with Gaussian noise to improve accuracy on
noisy images.
We take a different approach to Gaussian smoothing. Starting from an accurate pretrained
model f , we now discard the training labels, and iteratively retrain a new model, f smooth
using a quadratic loss between the model f and the new model’s predictions, with an
additional gradient regularization term. We have found that discarding the original one-hot
labels and instead using model predictions helps make the model smoother.
To be precise, our new models is a result of minimizing the loss which we call HeatSmooth-
ing,
1	2	σ2	2
E/ 2 ∣softmax (fsmooth(,)) - softmax(f(/)).+ 3 IMXfSmOoth(/)∣∣2	(3)
3
Under review as a conference paper at ICLR 2021
The smoothing achieved by the new models is illustrated schematically in Figure 5.
3.1	Related regularized losses
Gradient regularization is known to be equivalent to Gaussian smoothing (Bishop, 1995;
LeCun et al., 1998). Our deterministic smoothed model arises by training using the
HeatSmoothing loss (3), which is designed so to ensure that (2) holds for our model. Our
results is related to the early results on regularized networks (Bishop, 1995; LeCun et al.,
1998): that full gradient regularization is equivalent to Gaussian smoothing. Formally this is
stated as follows.
Theorem 1. (Bishop, 1995) Training a feed-forward neural-network model using the
quadratic (or mean-squared error) loss, with added Gaussian noise of mean 0 and vari-
ance σ2 to the inputs, is equivalent to training with
EC [Hf(x) - y『+ σ2Wf(X)『]	⑷
up to higher order terms.
The equivalence is normally used to go from models augmented with Gaussian noise to
regularized models. In our case, we use the result in the other direction: we train a regularized
model in order to produce a model which is equivalent to evaluating with noise. In practice,
this means that rather than adding noise to regularize models for certifiable robustness,
we explicitly perform a type of gradient regularization, in order to produce a model which
performs as if Gaussian noise was added. See Figure 4 in Appendix D for an illustration of
the effect of this gradient regularization.
The gradient regularization term in the HeatSmoothing loss (3), is also related to adversarial
training. Tikhonov regularization is used to produced adversarially robust models (Finlay
and Oberman, 2019). However in adversarial training, the gradient of the loss is used, rather
that the gradient of the full model. Also, our loss does not use information from the true
labels. The reason for these differences is due to the fact that we wish to have a model that
approximates the Gaussian average of our initial model f (see Appendix A). Furthermore,
minimizing the gradient-norm of the loss of the output gives us a smooth model in all
directions, rather than being robust to only adversarial perturbations.
3.2	Algorithmic Details
We have found that early on in training, the value 1 ∣∣ fsmooth(X) 一 fk(X)∣∣2 may be far
greater than the σ22 ∣∣ VCf smooth(X) ∣∣2 term. So We introduced a Softmax of the vectors in
the distance-squared term to reduce the overall magnitude of this term. We perform the
training minimization of (3) for one epoch. The pseudo-code for our neural netWork Weight
update is given by Algorithm 1 1
Note that the ∣∣ VC f smooth(x)∣∣j term in (3) requires the computation of a Jacobian matrix
norm. In high dimensions this is computationally expensive. To approximate this term, We
make use of the Johnson-Lindenstrauss lemma (Johnson and Lindenstrauss, 1984; Vempala,
2005) folloWed by the finite difference approximation from Finlay and Oberman (2019).
We are able to approximate ∣∣VCf smooth(x)，by taking the average of the product of
the Jacobian matrix and Gaussian noise vectors. Jacobian-vector products can be easily
computed via reverse mode automatic differentiation, by moving the noise vector w inside:
W ∙ (V eV (X)) = V C (W ∙ V (X))	(5)
Further computation expense is reduced by using finite-differences to approximate the norm
of the gradient. Once the finite-difference is computed, We detach this term from the
automatic differentiation computation graph, further speeding training. More details of our
implementation of these approximation techniques, and the definition of the term g which is
a regularization of the gradient, are presented in Appendix B.
4
Under review as a conference paper at ICLR 2021
Algorithm 1: HeatSmoothing Neural Network Weight Update
Input :Minibatch of input examples x( mb) = (x ⑴,...,x (Nb))
A model v set to “train” mode
Current model f set to “eval” mode
σ, standard deviation of Gaussian smoothing
κ, number of Gaussian noise replications (default= 10)
δ, finite difference step-size (default= 0.1)
Update : learning-rate according to a pre-defined scheduler.
for i ∈ {1, . . . Nb} do
Compute:fsmooth(x(i)),f(x(i)) ∈ RNc
Ji = 2 Ilfsmooth(x(i)) - f(x(i))∣∣2 ∈ R
for j ∈ {1, . . . κ} do
Generate W = √= (W1,..., WNC), W1,..., WNC ∈ N(0, 1)
Compute the normalized gradient g via (18)
Detach x(i) from the computation graph
Ji — Ji + σ2 (W ∙ fsmooth(x(i) + δg) - W ∙ f smooth(x(i)))2
end
Nb
J J Ν1b Σ Ji
i=1
end
Update the weights of v by running backpropagation on J at the current learning rate.
3.3	Theoretical details
We appeal to partial differential equations (PDE) theory for explaining the equivalence
between gradient regularization and Gaussian convolution (averaging) of the model2. The
idea is that the gradient term which appears in the loss leads to a smoothing of the new
function (model). The fact that the exact form of the smoothing corresponds to Gaussian
convolution is a mathematical result which can be interpreted probabilistically or using
techniques from analysis. Briefly, we detail the link as follows.
Einstein (1906) showed that the function value of an averaged model under Brownian motion
is related to the heat equation (a PDE); the theory of stochastic differential equations makes
this rigorous (Karatzas and Shreve, 1998). Moreover, solutions of the heat equation are given
by Gaussian convolution with the original model. Crucially, in addition solutions of the heat
equation can be interpreted as iterations of a regularized loss problem (called a variational
energy) like that of equation 3. The minimizer of this variational energy (3) satisfies an
equation which is formally equivalent to the heat equation (Gelfand et al., 2000). Thus,
taking these facts together, we see that a few steps of the minimization of the loss in (3)
yield a model which approximately satisfies the heat equation, and corresponds to a model
smoothed by Gaussian convolution. See Figure 4 for an illustration of a few steps of the
training procedure. This result is summarized in the following theorem.
Theorem 2. (Strauss, 2007) Let f be a bounded function, x ∈ Rd, and η 〜N(0, σ21).
Then the fol lowing are equivalent:
1.	Eη [f(x + η)], the expected value of Gaussian averages of f at x.
2.	ff *N(0, σ21))(x), the convolution of f with the density of the N(0, σ21) distribu-
tion evaluated at x.
1 Code and links to trained models are posted in Supplemental Materials
2 We sometimes interchange the terms Gaussian averaging and Gaussian convolution; they are
equivalent, as shown in Theorem 2.
5
Under review as a conference paper at ICLR 2021
3.	The solutions of the heat equation,
∂	σ2
∂tf (x,t) = ɪʌ xf( x,t)	⑹
at time t = 1, with initial condition f(x, 0) = f (x).
In Appendix A, we use Theorem 2 to show the equivalence of training with noise and
iteratively training (3).
To assess how well our model approximates the Gaussian average of the initial model, we
compute the certified £2 radius for averaged models introduced in Cohen et al. (2019). A
larger radius implies a better approximation of the Gaussian average of the initial model.
We compare our models with stochastically averaged models via certified accuracy. This is
the fraction of the test set which a model correctly classifies at a given radius while ignoring
abstained classifications. Throughout, we always use the same σ value for certification as for
training. In conjunction with the certification technique of Cohen et al., we also provide the
following theorem, which describes a bound based on the Lipschitz constant of a Gaussian
averaged model. We refer to this bound as the L-bound, which demonstrates the link between
Gaussian averaging and adversarial robustness.
Theorem 3 (L-bound). Suppose f smooth is the convolution (average) of f : Rd → [0, 1]Nc
with a Gaussian kernel of variance σ2I,
f smooth (X )= ( f*N(0,σ 21))(x)
Then any perturbation δ which results in a change of rank of the k-th component of f smooth (x)
must have norm bounded as fol lows:
IM ∣∣2 ≥ σ (π/2)1 / 2( f smooth (x )( k)-f smooth (x )( k +1))	⑺
where fsmooth (x)(i) is the ith largest value in the vector f smooth (x) ∈ [0, 1]Nc.
See Appendix C for proof. This bound is equally applicable to deterministic or stochastically
averaged models. In stochastically averaged models f smooth (x) is replaced by the stochastic
approximation of Eη〜N(0口2i)[f (x + η)].
3.4	Adversarial Attacks
Tb test how robust our model is to adversarial examples, We calculate the minimum £2
adversarial via our L-bound and we attack our models using the projected gradient descent
(PGD) (Kurakin et al., 2017; Madry et al., 2018) and decoupled direction and norm (DDN)
(Rony et al., 2019) methods. These attacks are chosen because there is a specific way they
can be applied to stochastically averaged models (Salman et al., 2019). In the £2 setting of
both attacks, it is standard to take the step
=S (f (X + M) ,y)
g = α IVδtL (f (x + δt),y)∣∣2
(8)
in the iterative algorithm. Here, x is an input example with corresponding true class y; Mt
denotes the adversarial perturbation at its current iteration; L denotes the cross-entropy
Loss function (or KL Divergence); ε is the maximum perturbation allowed; and α is the
step-size. In the stochastically averaged model setting, the step is given by
n
Σ S (f (x + M + ηi) ,y)
gn = α —i=--------------------------- (9)
Σ S (f (X + M + ηi) ,y)
i =1	2
where η 1,...,ηn 吧 N(0,σ21). For our deterministically averaged models, we implement
the update (8). This is because our models are deterministic, meaning there is no need to
sample noise at evaluation time. For stochastically averaged models (Cohen et al., 2019;
Salman et al., 2019), we implement the update (9).
6
Under review as a conference paper at ICLR 2021
Figure 2: Attack curves: % of images successfully attacked as a function of £2 adversarial distance.
4 Experiments & Results
We now execute our method on the ImageNet-1k dataset (Deng et al., 2009) with the
ResNet-50 model architecture. The initial model f was trained on clean images for 29 epochs
with the cross-entropy loss function. Due to a lack of computing resources, we had to modify
the training procedure (3) and Algorithm 1 to obtain our smoothed model f smooth. This
new training procedure amounts to minimizing the loss function
1	2	σ2	2
2	UsOftmaX (fsmooth(x + η)) -softmax (f (x))∣∣2 + 工 ∣∣VXfsmooth(X + η)∣∣2	(10)
for only 1 epoch of the training set using stochastic gradient descent at a fixed learning rate
of 0.01 and with σ = 0.25. This is needed because the output vectors in the ImageNet setting
are of length 1,000. Using softmax in the calculation of the £2 distance metric prevents the
metric from dominating the gradient-penalty term and the loss blowing up. Furthermore, we
add noise η 〜N(0, σ21)to half of the training images.
4.1	Comparison to Stochastic Methods via Certified Radii
We compare our results to a pretrained RandomizedSmoothing ResNet-50 model with
σ = 0.25 provided by Cohen et al. (2019). We also compare to a pretrained SmoothAdv
ResNet-50 model trained with 1 step of PGD and with a maximum perturbation of ε = 0.5
with σ = 0.25 provided by Salman et al. (2019). To assess certified accuracy, we run
the Certify algorithm from Cohen et al. (2019) with n0 = 25, n = 2, 000, σ = 0.25 for
the stochastically trained models. We realize that this may not be an optimal number
of noise samples, but it was the most our computational resources could handle. For the
HeatSmoothing model, we run the same certification algorithm but without running
SamplingunderNoise to compute ^a. For completeness, We also certify the baseline model
f0. Certification results on 5,000 ImageNet test images are presented in Figure 1b. We see
that our model is indeed comparable to the stochastic methods presented in earlier paper,
despite the fact that We only needed one training epoch. Note that CIFAR-10 results are
presented in Appendix E.
Table 3: £2 adversarial distance metrics on ImageNet-Ik
Model	L-bound		PGD		DDN	
	median	mean	median	mean	median	mean
HeatSmoothing	0.240	0.190	2.7591	2.6255	1.0664	1.2261
SmoothAdv	0.160	0.160	3.5643	3.0244	1.1537	1.2850
RandomizedSmoothing	0.200	0.180	2.6787	2.5587	1.2114	1.3412
Undefended baseline	-	-	1.0313	1.2832	0.8573	0.9864
7
Under review as a conference paper at ICLR 2021
Oooo
8 6 4 2
(T,dot)(u26m 】uuw」(ud
OOOo
8 6 4 2
(I—dot)①①」6E1U0JQ」① d
(a) top-1 classification agreement between a given (b) top-1 classification agreement between our
model and its own stochastic average prediction deterministic model and a Cohen et al. model
for varying σ.	prediction for varying σ.
Figure 3
4.2	Comparison to Stochastic Methods via Adversarial Attacks
Next, we attack our four models using PGD and DDN. For the stochastic models, we do
25 noise samples to compute the loss. We run both attacks with max £2 perturbation of
e = 4.0 until top-5 misclassification is achieved or until 20 steps are reached. Results on 1,000
ImageNet test images are presented in Table 3 and Figures 2a and 2b. We see that our model
is comparable to the stochastic models, but does not outperform them. In Figure 2a, it is
clear that the model presented in Salman et al. (2019) performs best, since this model was
trained on PGD-corrupted images. Note that CIFAR-10 results are presented in Appendix E.
4.3	Comparing Models Using Their Classifications
Recall that the goal of this work is to use deterministic methods to obtain an averaged
model equivalent to that of Cohen et al. (2019). One way of measuring the similarity is to
compare the predictions of both models. To do this, we consider a pretrained stochastically
averaged model with σ = 0.25 and our deterministic model trained using (10). We randomly
select 1, 000 ImageNet test images and assess the difference between evaluating our model
in a single forward pass vs. the stochastic method presented in Cohen et al.. In Figure 3a,
we see how often a model’s isnge forward pass top-1 prediction matches with predictions of
this model using Cohen et al.’s Predict algorithm, conditioning on the single forward pass
prediction being correct. In Figure 3b, we then see how often our deterministic model agrees
with Cohen et al.’s stochastic model’s prediction. In these plots, for stochastic prediction,
we fix the number of Gaussian replications n = 100. We see that our model’s predictions
are the same when doing a single forward pass and doing a stochastic prediction with high
probability. We also see that our model’s deterministic predictions match a high-performing
stochastically averaged model’s predictions with high probability.
4.4	Certifying Robust Models
So far, we have showed that we can take a non-robust baseline model and make it certifiably
robust by retraining for one epoch with a regularized loss (10). A natural question arises:
can we use this method to make robust models certifiably robust? To test this, we begin with
an adversarially trained model (Madry et al., 2018). This pretrained model was downloaded
from Madry’s “Robustness” GitHub repository and was trained with images corrupted by
the L2 PGD attack with maximum perturbation size e = 3.0. We certify this model by
retraining it with (10) for one epoch using stochastic gradient descent with fixed learning
rate 0.01. In Table 4, We compute the £2 certified radius from Cohen et al. (2019) for these
models using 1,000 ImageNet-1k test images with σ = 0.25. The certified radii for the model
trained with the loss function (10) are significantly higher than those of the adversarially
trained model from Madry et al. (2018).
8
Under review as a conference paper at ICLR 2021
Table 4: 22 certified radii summary statistics for robust models on ImageNet-Ik
Model	median	£2 radius mean	max.
Certified adversarially trained	0.4226	0.4193	0.6158
Adversarially trained	0.0790	0.1126	0.6158
Undefended baseline	0.0	0.1446	0.6158
5 Conclusion
Randomized smoothing is a well-known method to achieve a Gaussian average of some initial
neural network. This is desirable to guarantee that a model’s predictions are unchanged
given perturbed input data. In this work, we used a regularized loss to obtain deterministic
Gaussian averaged models. By computing £2 certified radii, We showed that our method
is comparable to previously-known stochastic methods. This is confirmed by attacking
our models, which results in adversarial distances similar to those seen with stochastically
smoothed models. We also developed a new lower bound on perturbations necessary to throw
off averaged models, and used it as a measure of model robustness. Lastly, our method is
less computationally expensive in terms of inference time (see Table 2).
9
Under review as a conference paper at ICLR 2021
References
C. M. Bishop. Training with noise is equivalent to Tikhonov regularization. Neural Computation, 7
(1):108-116, 1995. doi: 10.1162/neco.1995.7.1.108. URL https://doi.org/10.1162/neco.1995.
7.1.108.
C. Cheng, G. Nuhrenberg, and H. Ruess. Maximum resilience of artificial neural networks. In
D. D’Souza and K. N. Kumar, editors, Automated Technology for Verification and Analysis -
15th International Symposium, ATVA 2017, Pune, India, October 3-6, 2017, Proceedings, volume
10482 of Lecture Notes in Computer Science, pages 251-268. Springer, 2017. doi: 10.1007/
978-3-319-68167-2\_18. URL https://doi.org/10.1007/978-3-319-68167-2_18.
J. M. Cohen, E. Rosenfeld, and J. Z. Kolter. Certified adversarial robustness via randomized
smoothing. In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA,
volume 97 of Proceedings of Machine Learning Research, pages 1310-1320. PMLR, 2019. URL
http://proceedings.mlr.press/v97/cohen19c.html.
J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pages
248-255. Ieee, 2009.
A. Einstein. On the theory of the Brownian movement. Ann. Phys, 19(4):371-381, 1906.
C. Finlay and A. M. Oberman. Scaleable input gradient regularization for adversarial robustness.
CoRR, abs/1905.11468, 2019. URL http://arxiv.org/abs/1905.11468.
I. M. Gelfand, R. A. Silverman, et al. Calculus of Variations. Courier Corporation, 2000.
I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In
Y. Bengio and Y. LeCun, editors, 3rd International Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL
http://arxiv.org/abs/1412.6572.
W. B. Johnson and J. Lindenstrauss. Extensions of Lipschitz mappings into a Hilbert space.
Contemporary mathematics, 26(189-206):1, 1984.
I.	Karatzas and S. E. Shreve. Brownian motion. In Brownian Motion and Stochastic Calculus, pages
47-127. Springer, 1998.
A. Krizhevsky, G. Hinton, et al. Learning multiple layers of features from tiny images. 2009.
A.	Kurakin, I. J. Goodfellow, and S. Bengio. Adversarial machine learning at scale. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26,
2017, Conference Track Proceedings. OpenReview.net, 2017. URL https://openreview.net/
forum?id=BJm4T4Kgx.
Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, et al. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.
M. Lecuyer, V. Atlidakis, R. Geambasu, D. Hsu, and S. Jana. Certified robustness to adversarial
examples with differential privacy. In 2019 IEEE Symposium on Security and Privacy, SP 2019,
San Francisco, CA, USA, May 19-23, 2019, pages 656-672. IEEE, 2019. doi: 10.1109/SP.2019.
00044. URL https://doi.org/10.1109/SP.2019.00044.
B.	Li, C. Chen, W. Wang, and L. Carin. Second-order adversarial attack and certifiable robustness.
CoRR, abs/1809.03113, 2018. URL http://arxiv.org/abs/1809.03113.
A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models
resistant to adversarial attacks. In 6th International Conference on Learning Representations,
ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings.
OpenReview.net, 2018. URL https://openreview.net/forum?id=rJzIBfZAb.
A. Raghunathan, J. Steinhardt, and P. Liang. Certified defenses against adversarial examples.
In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC,
Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL
https://openreview.net/forum?id=Bys4ob- Rb.
10
Under review as a conference paper at ICLR 2021
J. Rony, L. G. Hafemann, L. S. Oliveira, I. B. Ayed, R. Sabourin, and E. Granger.
Decoupling direction and norm for efficient gradient-based L2 adversarial attacks
and defenses. In IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2019, Long Beach, CA, USA, June 16-20,	2019, pages 4322-4330. Com-
puter Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00445. URL
http://openaccess.thecvf.com/content_CVPR_2019/html/Rony_Decoupling_Direction_
and_Norm_for_Efficient_Gradient-Based_L2_Adversarial_Attacks_CVPR_2019_paper.html.
H. Salman, J. Li, I. P. Razenshteyn, P. Zhang, H. Zhang, S. Bubeck, and G. Yang. Prov-
ably robust deep learning via adversarially trained smoothed classifiers. In H. M.
Wallach, H. Larochelle, A. Beygelzimer, F. d’Alche-Buc, E. B. Fox, and R. Garnett,
editors, Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019,
Vancouver, BC, Canada, pages 11289-11300, 2019. URL http://papers.nips.cc/paper/
9307- provably- robust- deep- learning- via- adversarially- trained- smoothed- classifiers.
W. A. Strauss. Partial differential equations: An introduction. John Wiley & Sons, 2007.
C.	Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing
properties of neural networks. In Y. Bengio and Y. LeCun, editors, 2nd International Conference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference
Track Proceedings, 2014. URL http://arxiv.org/abs/1312.6199.
S.	S. Vempala. The random projection method, volume 65. American Mathematical Soc., 2005.
T.	Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, L. Daniel, D. S. Boning, and I. S. Dhillon.
Towards fast computation of certified robustness for relu networks. In J. G. Dy and A. Krause,
editors, Proceedings of the 35th International Conference on Machine Learning, ICML 2018,
Stockholmsmdssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine
Learning Research, pages 5273-5282. PMLR, 2018a. URL http://proceedings.mlr.press/v80/
weng18a.html.
T. Weng, H. Zhang, P. Chen, J. Yi, D. Su, Y. Gao, C. Hsieh, and L. Daniel. Evaluating the robustness
of neural networks: An extreme value theory approach. In 6th International Conference on Learning
Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track
Proceedings. OpenReview.net, 2018b. URL https://openreview.net/forum?id=BkUHlMZ0b.
G.	Yang, T. Duan, J. E. Hu, H. Salman, I. P. Razenshteyn, and J. Li. Randomized smoothing of all
shapes and sizes. CoRR, abs/2002.08118, 2020. URL https://arxiv.org/abs/2002.08118.
H.	Zhang, T. Weng, P. Chen, C. Hsieh, and L. Daniel. Efficient neural network robustness certifica-
tion with general activation functions. In S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems
31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8
December 2018, Montreal, Canada, pages 4944-4953, 2018. URL http://papers.nips.cc/paper/
7742-efficient-neural-network-robustness-certification-with-general-activation-functions.
H. Zhang, Y. Yu, J. Jiao, E. P. Xing, L. E. Ghaoui, and M. I. Jordan. Theoretically principled
trade-off between robustness and accuracy. arXiv preprint arXiv:1901.08573, 2019.
11
Under review as a conference paper at ICLR 2021
A Solving the heat equation by training with a regularized
loss function
Theorem 2 tells us that training a model with added Gaussian noise is equivalent to training
a model to solve the heat equation. We can discretize the heat equation (6) to obtain
(11)
for k = 0, . . . , nT - 1, where nT is the fixed number of timesteps, h = 1/nT , and f0 = f, our
initial model. Notice how, using the Euler-Lagrange equation, we can express fk+1 in (11)
as the variational problem
fk+1
argmin
v
2 / (m 2) - f k (x )∣2+hf- ∣∣v xv( x m2)p( X) dx
Rd
(12)
where ρ is the density from which our clean data comes form. Therefore, this is equivalent
to solving
fk+1 = argmin EX ∣v(x) - fk(x)∣2 + h- ∣∣Vχv(x)∣2	(13)
v2
Note that the minimizer of the objective of (12) satisfies
k hσ 2
V - fk = F、V
(14)
which matches (11) if we set fk+1 = v. In the derivation of (14), we take for granted the
fact that empirically, ρ is approximately uniform and is therefore constant. In the end, we
iteratively compute (13) and obtain models f1, . . . , fnT , setting V = fnT , our smoothed
model.
Something to take note of is that our model outputs be vectors whose length corresponds to
the total number of classes; therefore, the objective function in (13) will not be suitable for
vector-valued outputs fk(x) and V(x). We instead use the following update
fk+1 = argmin Ex 1 Kv(x) - fk(x)∣∣2 + hσ- ∣∣vxv(x)∣2
v2	2
(15)
B Approximating the gradient-norm regularization term
By the Johnson-Lindenstrauss Lemma (Johnson and Lindenstrauss, 1984; Vempala, 2005),
∣∣Vxv(x)∣2 has the following approximation,
where
and gi is given by
κ
I∣vχv(x)∣2 ≈ 工 I∣vx (wi ∙v(x))∣2
i=1
£ (
i=1
(wi ∙ V (x + δgi)) — (wi ∙ v(x))
wi = √-7 (wi 1,..., wiK V ∈ RK
wij 吧 N(0, 1)
▽ X ( Wi ∙ V ( X ))
「▽ X ( Wi ∙ v( X))Il2
0
if Vχ (wi ∙ v(x)) = 0
otherwise
(16)
(17)
(18)
g i
[

δ
)
2
In practice, we set δ = 0.1, κ = 10, and K = Nc, the total number of classes.
12
Under review as a conference paper at ICLR 2021
C Proof of Theorem 3
Proof. Suppose the loss function 0 is Lipschitz continuous with respect to model input x,
with Lipschitz constant L. Let 00 be such that if 0(x) < 0o, the model is always correct.
Then by Proposition 2.2 in Finlay and Oberman (2019), a lower bound on the minimum
magnitude of perturbation δ necessary to adversarially perturb an image x is given by
max {0o - 0(X) ,0}
Ilðl∣2 ≥------------L------------
(19)
By Lemma 1 of Appendix A in Salman et al. (2019), our averaged model
fsmooth(X) = (f *N(0,σ21)) (X)
has Lipschitz constant L = 1 Jn. Replacing L in (19) with 1 Jn and setting 00 =
fsmooth(χ)(k), 0(χ) = fsmooth(χ)(k+1) gives US the proof.	□
D Illustration of regularized training
Figure 4: Il lustration of gradient regularization (4) in the binary classification setting. The lighter
line represents classification boundary for original model with large gradients, and the darker line
represents classification boundary of the smoothed model. The symbols indicate the classification by
the original model: a single red circle is very close to many blue stars. The smoothed model has a
smoother classification boundary which flips the classification of the outlier.
Figure 5: Il lustration of performing the iterative model update (15) for 8 timesteps in the binary
classification setting. The dashed black line represents our decision boundary. The blue line represents
our current classification model. The blue stars and red circles represent our predicted classes using
the current model iteration. Consider the datapoint at x = -0.5. In the initial model f0, the
adversarial distance is ≈ 0.10. In model f5, the adversarial distance is increased to ≈ 0.35.
13
Under review as a conference paper at ICLR 2021
E Results on the CIFAR-10 dataset
Figure 6: CIFAR-10 attack curves: % of images successfully attacked as a function of £ 2 adversarial
distance.
f2 distance
(b) CIFAR-10 top-1 DDN
We test our method on the CIFAR-10 dataset (Krizhevsky et al., 2009) with the ResNet-34
model architecture. The initial model f was trained for 200 epochs with the cross-entropy loss
function. Our smoothed model v was computed by setting f0 = f and running Algorithm 1
to minimize the loss (15) with σ = 0.1 for nT = 5 timesteps at 200 epochs each timestep. The
training of our smoothed model took 5 times longer than the baseline model. We compare our
results to a ResNet-34 model trained with σ = 0.1 noisy examples as stochastically averaged
model using RandomizedSmoothing (Cohen et al., 2019). We also trained a SmoothAdv
model (Salman et al., 2019) for 4 steps of PGD with the maximum perturbation set to
ε = 0.5. To assess certified accuracy, we run the Certify algorithm from Cohen et al.
(2019) with n0 = 100, n = 10, 000, σ = 0.1 for the stochastically trained models. For the
HeatSmoothing model, we run the same certification algorithm, but without running
SamplingunderNoise to compute Ca. For completeness, We also certify the baseline model
f0. Certification plots are presented in Figure 1a. In this plot, we see that our model’s
£2 certified accuracy outperforms the stochastic models. Next, we attack our four models
using PGD and DDN. For the stochastic models, We do 100 noise samples to compute the
loss. We run both attacks with 20 steps and maximum perturbation ε = 4.0 to force top-1
misclassification. Results are presented in Table 5 and Figures 6a and 6b. In Table 5, we
see that HeatSmoothing outperforms the stochastic models in terms of robustness. The
only exception is robustness to mean PGD perturbations. This is shown in Figures 6a. Our
model performs well up to an £2 PGD perturbation of just above 1.0.
Table 5: £2 adversarial distance metrics on CIFAR-10. A larger distance implies a more robust
model.
Model	L-bound		PGD		DDN	
	median	mean	median	mean	median	mean
HeatSmoothing	0.094	0.085	0.7736	0.9023	0.5358	0.6361
SmoothAdv	0.090	0.078	0.7697	1.3241	0.4812	0.6208
RandomizedSmoothing	0.087	0.081	0.7425	1.2677	0.4546	0.5558
Undefended baseline	-	-	0.7088	0.8390	0.4911	0.5713
14