Under review as a conference paper at ICLR 2021
Decentralized Knowledge Graph
Representation Learning
Anonymous authors
Paper under double-blind review
Ab stract
Knowledge graph (KG) representation learning methods have achieved competitive
performance in many KG-oriented tasks, among which the best ones are usually
based on graph neural networks (GNNs), a powerful family of networks that learns
the representation of an entity by aggregating the features of its neighbors and itself.
However, many KG representation learning scenarios only provide the structure
information that describes the relationships among entities, causing that entities
have no input features. In this case, existing aggregation mechanisms are incapable
of inducing embeddings of unseen entities as these entities have no pre-defined fea-
tures for aggregation. In this paper, we present a decentralized KG representation
learning approach, decentRL, which encodes each entity from and only from the
embeddings of its neighbors. For optimization, we design an algorithm to distill
knowledge from the model itself such that the output embeddings can continuously
gain knowledge from the corresponding original embeddings. Extensive experi-
ments show that the proposed approach performed better than many cutting-edge
models on the entity alignment task, and achieved competitive performance on
the entity prediction task. Furthermore, under the inductive setting, it significantly
outperformed all baselines on both tasks.
1	Introduction
Knowledge graphs (KGs) support many data-driven applications (Ji et al., 2020). Recently, learning
low-dimensional representations (a.k.a. embeddings) of entities and relations in KGs has been
increasingly given attentions (Rossi et al., 2020). We find that existing models for KG representa-
tion learning share similar characteristics to those for word representation learning. For example,
TransE (Bordes et al., 2013), a well-known translational KG embedding model, interprets a triple
(e1, r, e2) as e1 + r ≈ e2, where e1, e2, r denote subject, object and their relationship, respectively,
and the boldfaces denote the corresponding embeddings. If we view e1 as a word in sentences, and
e2 as well as many other objects of e1 as the context words, then TransE and many KG embedding
models (Wang et al., 2014; Dettmers et al., 2018; Nguyen et al., 2018; Kazemi & Poole, 2018; Sun
et al., 2019), learn representations in a form simaliar to that used in Skip-gram (Mikolov et al., 2013a),
where the input representation is learned to predict the context (i.e., neighbors) representations.
Recently, many graph neural networks (GNNs) based models for KG representation learning (Wang
et al., 2018; Schlichtkrull et al., 2018; Cao et al., 2019; Wu et al., 2019; Sun et al., 2020; Vashishth
et al., 2020) have achieved state-of-the-art performance in KG-related tasks such as entity alignment
and entity prediction. Those models learn KG representations in a CBOW (continuous bag-of-words)
(Mikolov et al., 2013a) manner, in which the context entities are aggregated to predict the target. But
they also consider the representation of an entity itself when aggregating the neighborhood informa-
tion. This nature prevents those models (e.g., GCN (Kipf & Welling, 2017) and GAT (Velickovic
et al., 2018)) to be generalized to represent unseen entities. In many cases, the entities in prevalent
KG-related tasks do not have self features. This motivates us to learn entity representations from and
only from their context neighbors.
We propose a decentralized KG representation learning approach, decentRL. The key idea of
decentRL is to decentralize the semantic information of entities over only their neighbors (i.e.,
distributed context vector in CBOW (Mikolov et al., 2013b)), which can be easily implemented
by representing each entity through averaging its neighbor embeddings. In this paper, we look for
1
Under review as a conference paper at ICLR 2021
a more efficient but still simple way to realize this concept on the most popular graph attention
network (GAT) (Velickovic et al., 2018), as well as its many variants (Sun et al., 2020; Vashishth
et al., 2020). We illustrate the methodology by decentralized attention network (DAN), which is
based on the vallina GAT. DAN is able to support KG representation learning for unseen entities with
only structure information, which is essentially different from the way of using self features (e.g.,
attribute information) in the existing graph embedding models (Hamilton et al., 2017; Bojchevski &
Gunnemann, 2018; Hettige et al., 2020). Furthermore, the neighbors in DAN serve as an integrity
to give attentions, which means that DAN is more robust and more expressive compared with
conventional graph attention mechanism (Velickovic et al., 2018).
Another key problem in decentralized KG representation learning is how to estimate and optimize
the output embeddings. If we distribute the information of an entity over its neighbors, the original
embedding of this entity ei also learns how to effectively participate in the aggregations of its different
neighbors conversely. Suppose that we have obtained an output representation gi from DAN for
entity ei, we can simply estimate and optimize gi by aligning it with ei . But directly minimizing
the L1/L2 distance between gi and ei may be insufficient. Specifically, these two embeddings have
completely different roles and functions in the model, and the shared information may not reside in
the same dimensions. Therefore, maximizing the mutual information between them is a better choice.
Different from the existing works like MINE (Belghazi et al., 2018) or InfoNCE (van den Oord
et al., 2018), in this paper, we design a self knowledge distillation algorithm, called auto-distiller. It
alternately optimizes gi and its potential target ei , such that gi can automatically and continuously
distill knowledge from the original representation ei across different batches.
The main contributions of this paper are listed as follows. (1) We propose decentralized KG
representation learning, and present DAN as the prototype of graph attention mechanism under the
open-world setting. (2) We design an efficient knowledge distillation algorithm to support DAN for
generating representations of unseen entities. (3) We implement an end-to-end framework based on
DAN and auto-distiller. The experiments show that it achieved superior performance on two prevalent
KG representation learning tasks (i.e., entity alignment and entity prediction), and also significantly
outperformed those cutting-edge models under the open-world setting.
2	Background
Knowledge Graph. A KG can be viewed as a multi-relational graph, in which nodes represent
entities in the real world and edges have specific labels to represent different relationships between
entities. Formally, we define a KG as a 3-tuple G = (E, R, T), with E and R denoting the sets of
entities and relationships, respectively. T is the set of relational triples.
KG Representation Learning. Conventional models are mainly based on the idea of Skip-gram.
According to the types of their score functions, these models can be divided into three categories:
translational models (e.g., TransE (Bordes et al., 2013) and TransR (Lin et al., 2015a)), semantic
matching models (e.g., DistMult (Yang et al., 2015) and ComplEx (Trouillon et al., 2016)) and neural
models (e.g., ConvE (Dettmers et al., 2018) and RSN (Guo et al., 2019)). We refer interested readers
to the surveys (Wang et al., 2017; Ji et al., 2020) for details. Recently, GNN-based models receive
great attentions in this field, which are closely related to this paper. Specifically, R-GCN (Schlichtkrull
et al., 2018), AVR-GCN (Ye et al., 2019) and CompGCN (Vashishth et al., 2020) introduce different
relation-specific composition operations to combine neighbors and the corresponding relations before
neighbor aggregation. RDGCN (Wu et al., 2019) refactors KGs as dual relation graphs (Monti et al.,
2018) where edge labels are represented as nodes for graph convolution. All the aforementioned
GNN-based models choose GCNs and/or GATs to aggregate the neighbors of an entity, in which
an identity matrix is added to the adjacency matrix. This operation is helpful when elements have
self features, but poses a problem in learning the representations of unseen entities where no self
features are attached to them. Differently, decentRL fully relies on the neighbor context to attend to
the neighbors of each entity in linear complexity, which is efficient and easy to be deployed.
Entity Alignment. Entity alignment aims to find the potentially aligned entity pairs in two different
KGs G1 = (E1, R1, T1) and G2 = (E2, R2, T2), given a limited number of aligned pairs as training
data S ⊂ E1 × E2. Oftentimes, G1, G2 are merged to a joint KG G = (E, R, T), which enables the
models learn representations in a unified space.
2
Under review as a conference paper at ICLR 2021
Entity Prediction. Entity prediction (a.k.a. KG completion (Bordes et al., 2013)) seeks to find the
missing subject e1 or object e2, given an incomplete relation triple (?, r, e2) or (e1 , r, ?).
It is worth noting that the performance on the entity prediction task may be greatly improved by
complex deep networks, as it relies on the predictive ability rather than the embedding quality (Guo
et al., 2019). Hence, many cutting-edge models cannot obtain promising results in entity align-
ment (Guo et al., 2019; Sun et al., 2020). Differently, entity alignment directly compares the distance
of learned entity embeddings, which clearly reflects the quality of output representations. Few models
demonstrate consistently good performance on both tasks, whereas decentRL is capable of achieving
competitive, even better, performance compared with respective state-of-the-art models.
3	Decentralized Representation Learning
In the decentralized setting, the representation of an entity ei is aggregated from and only from its
neighbors Ni = {e1, e2, . . . , e|Ni| }. As it may have many neighbors that are unequally informa-
tive (Velickovic et al., 2018), involving attention mechanism is a good choice.
3.1	Graph Attention Networks
We start by introducing the Graph attention network (GAT) (Velickovic et al., 2018), which leverages
linear self attention to operate spatially close neighbors. For an entity ei , GAT aggregates the
representations of its neighbors Ni and itself into a single representation ci as follows:
ci =	aij Wej,	(1)
ej∈Ni∪{ei}
where aij is the learnable attention score from ei to ej , and W is the weight matrix. To obtain aij , a
linear attention mechanism is used here:
aij = softmax σ(aT[W1ei k W2ej]) ,	(2)
where a is a weight vector to convert the concatenation of two embeddings into a scalar attention
score, and k denotes the concatenation operation. W1 and W2 are two weight matrices. σ is the
activation function, usually being LeakyReLU (Xu et al., 2015). GAT computes the attention score
of an entity ei to its neighbors in linear complexity, which is very efficient when being applied to
large-scale graphs.
3.2	Decentralized Attention Networks
Intuitively, if ei is the embedding of an unseen entity, it is rarely useful in computing the attention
scores (as it is just a randomly initialized vector). Thus, purely relying on its neighbors may be a
good choice. Specifically, to obtain the decentralized attention scores, one may simply sum all the
attention scores from other neighbors a0ij = softmax(Pe ∈N \{e } akj). However, it would lead to
a problem that this sum only represents the attention of each neighbor to ej . In this case, a high
attention score from one neighbor ek to ej can dominate the value of a0ij , but it does not mean that ej
is more important for ei . Therefore, all neighbors should act as an integrity in giving attentions.
Towards this end, we propose decentralized attention networks (DANs). Formally, to obtain the
decentralized attention weight aij , we have to feed the attention layer with two types of input: the
neighbor context vector ni (i.e., query), and the candidate neighbor embedding ej (i.e., key and
value). Separately controlling the iterations of these two variables in a multi-layer model is evidently
inefficient. Instead, we realize this operation by involving a second-order attention mechanism. For
layer k, DAN calculates the decentralized attention score aikj as follows:
aj = Sofmax(σ(aT[Wkd"1 k Wkd，-2])),	⑶
wheredik-1,djk-2 denote the output embeddings of layer k - 1 for ei and of layer k - 2 for ej,
respectively. Ifwe regard dik-1 as the neighbor aggregation of layer k - 1 for ei, then djk-2 is exactly
the embedding of ej used in summing dik-1. In this case, aikj can represent the attention weight of
3
Under review as a conference paper at ICLR 2021
key & value
self-contained
neighbors
DAN
GAT
DenSe3
Attention
Layer
target entity at (
layer k
• target entity at
▲ layer k
key &
.k-1
Figure 1: Comparing graph attention network (GAT) with decentralized attention network (DAN).
Attention
Layer
target entity
Linear Attention Layer
neighbors
target entity
ei’s neighbor context to ej. Then, we can obtain the output of layer k by:
dik = X aikjWkdjk-2 .	(4)
ej∈Ni
It is worth noting that we perform convolutions on layer k - 2, as the score aikj is attended to
the neighbor representations in layer k - 2. This keeps the consistency and ensures the output
representations are consecutive. Also, it enhances the correlation of output in different layers, and
forms the second-order graph attention mechanism.
For the first layer of DAN, we initialize di0 and dj-1 as follows:
d0 =% X W0ej, d-1 = ej.	(5)
ej ∈Ni
Here, we simply use a mean aggregator to obtain the decentralized embedding di0 of layer 0, but
other aggregators like pooling may be employed as well. This simple mean aggregator can also be
regarded as a CBOW model with dynamic window size. For the architecture and implementation of
DAN, please refer to Appendix A.
3.3	Insight of DAN
We compare GAT with DAN in Figure 1. Although the attention layers levearged by DAN and GAT
are identical, the decentralized structure has two significant strengths:
Inductive representation learning. In GAT, the self embedding ei participates in the calculation
of attention scores (Equation 1) and the aggregation of neighbors (Equation 2). Therefore, when ei
is an open entity, its embedding is completely random-initialized. In this case, the attention scores
computed by GAT are almost meaningless. By contrast, DAN generates the embedding of ei without
requirement of its embedding throughout. Such characteristic enables DAN to induce embeddings on
unseen entities.
Robustness. When calculating the attention scores for an entity ei , GAT only takes ei as query, the
importance of other neighbors is overlooked, which may lead to biased attention computation. On the
other hand, it is generally known that most entities in KGs only has a small number of neighbors (Li
et al., 2017). Due to the lack of training examples, the embeddings of these entities are not as
informative as those with more neighbors. Therefore, they may be not capable of serving as queries
for computing attention scores, causing that GAT cannot obtain reliable attention scores in some
cases. By contrast, the queries in DAN are neighbor context vectors, which have richer semantics and
also enable DAN to compute the attention scores in an unbiased way.
Furthermore, the computational complexity of DAN is almost identical to that of GAT, except that
DAN has an additional mean aggregator. From Figure 1 and Equation 5, we can find that such an
aggregator is evidently simpler than the linear attention layer, which means that its computational
complexity (both time and space) can be almost overlooked. Therefore, DAN is an efficient model.
4
Under review as a conference paper at ICLR 2021
4	Decentralized Representation Estimation
The final output representation gi of DAN for ei can be optimized by minimizing the L1/L2 distance
between gi and ei to enable self-supervised learning. Such distance estimation pursues a precise
match at every dimension of these two embeddings, but ignores the implicit structure information
across different dimensions.
4.1	Mutual Information Maximization
As mentioned in Section 1, the original embedding ei also serves as one of neighbor embeddings in
aggregating the decentralized embeddings of its neighbors, which implies that ei itself also preserves
the latent information used to support its neighbors. Inspired by MINE (Belghazi et al., 2018),
InfoNCE (van den Oord et al., 2018), and DIM (Hjelm et al., 2019), in this paper we do not try to
optimize gi by reconstructing the original representation ei . Instead, we implicitly align them in a
way of maximizing the mutual information I(gi , ei).
Specifically, We define a learnable function f : RD 0 RO → R to estimate the mutual information
density (van den Oord et al., 2018) between gi and the copy of ei (the reason why using the copied
vector Will be explained shortly):
f(gi, ei) = exp(gTWf0 + bf),	⑹
Where D and O are the dimensions of the output and input representations, respectively. Wf, bf
are the weight matrix and bias, respectively. ^i denotes the copy of e讣 We expect that f (gi, e# is
significantly larger than f (gi, ^j) for j = i. Following InfoNCE, the objective can be written as:
b(gi, ^i)= E log (P f(gi,ei；),	(7)
Xi	Hej ∈Xi f (gi, ej V
where Xi = {e1, . . . , e|Xi|} contains |Xi| - 1 sampled negative entities plus the target entity ei.
Maximizing this objective results in maximizing a lower-bound of mutual information between gi
and ei (van den Oord et al., 2018).
4.2	Auto-Distiller
Note that in Equations (6) and (7), we actually use the copy of the original representations, which
leads to a completely different optimization process compared with existing works. Specifically,
some methods like InfoNCE or DIM jointly optimize the two input in the density function, as both
variables are the output of deep neural models requiring to be updated in back-propagation. But
in decentRL, ei is just a randomly initialized vector, and its gradient in Equation (7) may conflict
with the gradient where it is taken as an input neighbor in learning decentralized representations of
its neighbors. On the other hand, such optimization operation also prevents ei from learning the
neighborhood information at the bottom layer, and compels this variable to match gi .
To address this problem, we view ei as a teacher, and gi as a student to learn from the teacher (Tian
et al., 2020). Our aim is to let this teacher continuously gain more knowledge to teach the student,
which we call it auto-distiller. Therefore, our final objective is:
argmax E log
gi,f Xi
f(gi, ei)
ej ∈Xi f (gi, ej )
+argm axeXi Xlog (PSj),
(8)
and it has two important characteristics:
Lemma 1 (automatic distillation). Optimizing the first term of Equation 8 for entity ei naturally
contributes to optimizing the second term for the neighbors of ei, which means conventional mini-
batch training procedure can be applied.
Lemma 2 (lower-bound). The mutual information between gi and ^i is still lower-bounded in
auto-distiller.
Proof. See Appendix B.
□
5
Under review as a conference paper at ICLR 2021
Table 1: Result comparison of entity alignment on DBP15K.
Models	ZH-EN			JA-EN			FR-EN		
	Hits@1	HitS@10	MRR	Hits@1	HitS@10	MRR	Hits@1	HitS@10	MRR
AlignE (Sun et al., 2018)	0.472	0.792	0.581	0.448	0.789	0.563	0.481	0.824	0.599
RSN (Guo et al., 2019)	0.508	0.745	0.591	0.507	0.737	0.590	0.516	0.768	0.605
GAT (Velickovic et al., 2018)	0.418	0.667	0.508	0.446	0.695	0.537	0.442	0.731	0.546
AliNet (Sun et al., 2020)	0.539	0.826	0.628	0.549	0.831	0.645	0.552	0.852	0.657
decentRL	0.589	0.819	0.672	0.596	0.819	0.678	0.602	0.842	0.689
5	Experiments
We evaluated decentRL on two prevalent tasks, namely entity alignment and entity prediction, for
KG representation learning. As few existing models show state-of-the-art performance on both tasks,
we picked the state-of-the-art methods in respective tasks and compared decentRL with them. To
probe the effectiveness of decentRL, we also conducted ablation study and additional experiments.
Limited by the length, please see Appendix C for more analytic results.
5.1	Datasets
Entity Alignment Datasets. We consider the JAPE dataset DBP15K (Sun et al., 2017), which is
widely used by existing studies. It includes three entity alignment settings, each of which contains two
KGs of different languages. For example, ZH-EN indicates Chinese-English alignment on DBpedia.
Entity Prediction Datasets. We consider four datasets: FB15K, WN18, FB15K-237, and WN18RR
(Bordes et al., 2013; Dettmers et al., 2018; Toutanova & Chen, 2015). The former two have been
used as benchmarks for many years, while the latter two are the corrected versions, as FB15K and
WN18 contain a considerable amount of redundant data (Dettmers et al., 2018).
5.2	Experiment Setup
For both tasks, we initialized the original entity embeddings, relation embeddings and weight matrices
with Xavier initializer (Glorot & Bengio, 2010).
To learn cross-KG embeddings for the entity alignment task, we incorporated a contrastive loss (Sun
et al., 2020; Wang et al., 2018) to cope with aligned entity pairs S, which can be written as follows:
La = X ||gi -gj|| + X αλ- ||gi0 -gj0||+,	(9)
(i,j)∈S+	(i0,j0)∈S-
where S+, S- are the positive entity pair set and sampled negative entity pair set, respectively.
∣∣∙∣∣ denotes the L2 distance between two embeddings. a and λ are hyper-parameters. By jointly
minimizing two types of losses, decentRL is able to learn cross-KG embeddings for entity alignment.
Similarly, for entity prediction, we also need to choose a decoder to enable decentRL to predict
missing entities (Vashishth et al., 2020). We chose two simple models, TransE (Bordes et al., 2013)
and DistMult (Yang et al., 2015) for the main experiments, which are sufficient to achieve comparable
performance against the state-of-the-art.
5.3	Entity Alignment Results
Table 1 depicts the entity alignment results on the JAPE dataset. We observe that: (1) decentRL
significantly outperformed all the methods on Hits@1 and MRR, which empirically showed the
advantage of decentRL in learning high-quality representations. (2) The scores of Hits@10 of
decentRL were slightly below those of AliNet. We argue that decentRL is a purely end-to-end model,
which did not incorporate any additional data augmentation used in AliNet (Sun et al., 2020) that
may improve the Hits@10 results. Moreover, decentRL is much easier to be optimized, as it does
not need to coordinate the hyper-parameters of each part in the pipeline. Also, there is no conflict to
combine decentRL and the data augmentation algorithm for further improvement.
6
Under review as a conference paper at ICLR 2021
1.0
0.8
0.6
0.4
0.2
(a) ZH-EN	(b)JA-EN	(C) FR-EN
□ Open decentRL
□ Loss of decentRL
□ Open AliNet
□ Loss of AliNet
□ Open GAT
□ Loss of GAT
H@1	H@10	MRR	H@1	H@10	MRR	H@1	H@10	MRR
Figure 2: Open entity alignment results on DBP15K. Bars with dotted lines denote the performance
drop compared with the corresponding results in the non-open setting. The same to the following.
Table 2: Entity prediction results on FB15K-237.
Models	TransE			DistMult		
	HitS@10	MR	MRR	HitS@10	MR	MRR
Raw	0.465	357	0.294	0.419	354	0.241
+ D-GCN	0.469	351	0.299	0.497	225	0.321
+ R-GCN	0.443	325	0.281	0.499	230	0.324
+ W-GCN	0.444	1,520	0.267	0.504	229	0.324
+ CompGCN	0.515	233	0.337	0.518	200	0.338
+ decentRL	0.521	159	0.334	0.541	151	0.350
Raw denotes the original results of the decoders.
Figure 3: MRR results on open FB15K-237.
We also evaluated the graph-based models in the open-world setting. Specifically, we first split the
testing entity set into two subsets, namely known entity set and unknown entity set. Then, for those
triples in the training set (in the non-open-world setting, all triples are used in training) containing
unknown entities, we moved them to the testing triple set, which were only available during the
testing phase. We followed a classical dataset splitting used in the entity alignment task, with 20% of
entities in the original testing set are sampled as open entities. Table 6 in Appendix C.1 compares the
datasets before and after re-splitting.
The experimental results are shown in Figure 2. We can find that decentRL outperformed GAT and
AliNet (the second-best model) on all metrics. Although its performance slightly dropped compared
with that in the close setting, the results of others (especially GAT, which only uses self representation
as “query”) suffered more under this open-world setting. Overall, decentRL is capable of achieving
state-of-the-art performance on both open and conventional entity alignment tasks.
5.4	Entity Prediction Results
We also evaluated decentRL on the entity prediction task, in comparison with different GNN-based
models: D-GCN (Marcheggiani & Titov, 2017), R-GCN (Schlichtkrull et al., 2018), W-GCN (Shang
et al., 2019), and CompGCN (Vashishth et al., 2020). The results on FB15K-237 are shown in
Table 2, from which we observe that: (1) decentRL significantly outperformed all the other models
for many metrics, especially MR (mean rank). This demonstrates that decentRL can learn better
representations for both popular entities (valued by MRR metric) and long-tail entities (valued by MR
metric) (2) decentRL boosted DistMult to achieve almost state-of-the-art performance on FB15K-237.
The simpler model TransE, also gained great improvement on all metrics. The reason may be that
DAN discovered a better aggregation weights and our auto-distiller continuously refined the output
representations.
The corresponding results on open entity prediction are shown in Figure 3. We added a state-of-the-art
yet more complicated GNN-based model CompGCN + ConvE in the comparison, from which we
observe that: decentRL + DistMult outperformed all the other models under this open-world setting,
which verified its effectiveness in inductive learning with only structure data. decentRL + TransE
achieved the second-best performance, followed by CompGCN + ConvE. Overall, decentRL provided
the decoders with better representations and supported them to achieve competitive and even better
performance over the cutting-edge models.
7
Under review as a conference paper at ICLR 2021
Table 3: Entity prediction results on FB15K and WN18.
Methods	FB15K					WN18				
	Hits@1	Hits@3	Hits@10	MRR	MR	Hits@1	Hits@3	Hits@10	MRR	MR
TransE	0.297	0.578	0.749	0.463	-	0.113	0.888	0.943	0.495	-
DistMult	0.546	0.733	0.824	0.654	97	0.728	0.914	0.936	0.822	902
ComplEx	0.599	0.759	0.840	0.692	-	0.599	0.759	0.840	0.692	-
ConvE	0.558	0.723	0.831	0.657	51	0.935	0.946	0.956	0.943	374
RotatE	0.746	0.830	0.884	0.797	40	0.944	0.952	0.959	0.949	309
RSN	0.722	-	0.873	0.78	-	0.922	-	0.953	0.940	-
TuckER	0.741	0.833	0.892	0.795	-	0.949	0.955	0.958	0.953	-
decentRL + TransE	0.633	0.771	0.856	0.715	40	0.736	0.904	0.954	0.824	255
decentRL + DistMult	0.664	0.793	0.872	0.740	32	0.944	0.951	0.958	0.949	259
decentRL + ComplEx	0.745	0.847	0.901	0.804	33	0.945	0.952	0.958	0.949	251
Table 4: Entity prediction results on FB15K-237 and WN18RR.
Methods	FB15K-237					WN18RR				
	Hits@1	Hits@3	Hits@10	MRR	MR	Hits@1	Hits@3	Hits@10	MRR	MR
TransE	-	-	0.465	0.294	357	-	-	0.501	0.226	3,384
DistMult	0.155	0.263	0.419	0.241	254	0.39	0.44	0.49	0.43	5,110
ComplEx	0.158	0.275	0.428	0.247	339	0.41	0.46	0.51	0.44	5,261
ConvE	0.237	0.356	0.501	0.325	244	0.400	0.440	0.520	0.430	4,187
RotatE	0.241	0.375	0.533	0.338	177	0.428	0.492	0.571	0.476	3,340
RSN	0.202	-	0.453	0.280	-	-	-	-	-	-
TuckER	0.266	0.394	0.544	0.358	-	0.443	0.482	0.526	0.470	-
CompGCN + ConvE	0.264	0.390	0.535	0.355	197	0.443	0.494	0.546	0.479	3,533
CompGCN+ TransEt	0.242	0.367	0.510	0.332	214	-	-	-	-	-
CompGCN+ DistMultt	0.249	0.368	0.515	0.337	199	-	-	-	-	-
CompGCN + ConvEt	0.262	0.385	0.532	0.352	215	-	-	-	-	-
decentRL + TransE	0.241	0.362	0.521	0.334	159	0.290	0.420	0.505	0.369	4,710
decentRL + DistMult	0.257	0.385	0.541	0.350	151	0.433	0.481	0.542	0.470	4,613
decentRL + ComplEx	0.261	0.388	0.544	0.354	172	0.422	0.466	0.533	0.458	3,744
"t" denotes methods executed by the SoUrce Code With the Provided best Parameter settings.										
Table 5: Ablation study of entity alignment on DBP15K (average of 5 runs).
Models	ZH-EN			JA-EN			FR-EN		
	Hits@1	HitS@10	MRR	Hits@1	HitS@10	MRR	Hits@1	HitS@10	MRR
decentRL + auto-distiller	0.589	0.819	0.672	0.596	0.819	0.678	0.602	0.842	0.689
decentRL + infoNCE	0.579	0.816	0.665	0.591	0.816	0.673	0.593	0.834	0.682
decentRL + L2	0.571	0.802	0.655	0.589	0.807	0.669	0.591	0.831	0.679
centRL + auto-distiller	0.579	0.812	0.663	0.589	0.812	0.671	0.593	0.836	0.681
centRL	0.544	0.791	0.632	0.561	0.799	0.646	0.560	0.820	0.654
The detailed results on entity prediction are shown in Tables 3 and 4, respectively. For the conventional
benchmarks FB15K and WN18 that have been widely used for many years, our decentRL with only
simple decoders achieved competitive even better performance compared with those state-of-the-art
models. Furthermore, decentRL greatly improved the best results on MR, as it can more efficiently
aggregate neighbor information to learn high-quality representations for those “challenging” entities.
On the other hand, we find that the performance of decentRL on FB15K-237 and WN18RR is not as
promising as that in Table 3, although it still achieved the best Hits@10 and MR on FB15K-237. We
argue that this may be caused by the insufficient predictive ability of simple decoders (Guo et al.,
2019). However, we currently do not plan to adapt decentRL to some complex decoders like ConvE,
as such complicated architecture can largely increase the time and space complexity. For example,
CompGCN with ConvE needs to be trained at least two days on a small dataset like FB15K-237.
Overall, the performance of some simple linear KG representation learning models (i.e., TransE,
DistMult, and ComplEx) received great benefits from decentRL, and even outperformed some
cutting-edge models.
8
Under review as a conference paper at ICLR 2021
0.8
(a) ZH-EN
(b)JA-EN
(c) FR-EN
0.6
0.4
□ decentRL □ AliNet
□ decentRL □ AliNet
□ decentRL □ AliNet

Input L1	L2	L3	L4 Concat Input L1	L2	L3	L4 Concat	Input L1	L2	L3	L4 Concat
Figure 4: HitS @1 results of each layer and the concatenation. The results of AliNet are from (SUn
et al., 2020). It has no L3 and L4 scores as its best performance was achieved by a two-layer model.
5.5	Comparison with Alternative Models
To exemplify the effectiveness of each module in decentRL, we derived a series of alternative models
from decentRL and report the experimental results on entity alignment in Table 5. “centRL” denotes
the model that used DAN but self-loop was added to the adjacency matrix.
From the results, we observe that: all the models in the table achieved state-of-the-art performance on
Hits@1 metric, as DAN which leverages all neighbors as queries can better summarize the neighbor
representations of entities.
on the other hand, we also find that decentRL + auto-distiller outperformed all the other alternatives.
The centralized model centRL + auto-distiller had a performance drop compared with the decen-
tralized one. The main reason is that entities themselves in centRL also participated in their own
aggregations, which disturbed the roles of the original representations. Please see Appendix C.2 for
the corresponding results under the open entity alignment setting.
5.6	Comparison of the Output Embeddings of Each Layer
We also compared the performance of each layer in decentRL and AliNet. As shown in Figure 4,
decentRL consistently outperformed AliNet on each layer except the input layer. As mentioned
before, decentRL does not take the original representation of an entity as input, but this representation
can still gain knowledge in participating in the aggregations of its neighbors and then teach the student
(i.e., the corresponding output representation). The performance of the input layer was not as good as
that in AliNet, because the latent information in this layer may not be aligned in each dimension.
on the other hand, we also observe that concatenating the representations of each layer in decentRL
also improved the performance, with a maximum increase of 0.025 (0.023 for AliNet). Furthermore,
decentRL can gain more benefits from increasing the layer number, while the performance of AliNet
starts to drop when the layer number is larger than 2 (Sun et al., 2020).
6 Conclusion
in this paper we proposed decentralized KG representation learning, which explores a new and
straightforward way to learn representations in open-world, with only structure information. The
corresponding end-to-end framework achieved very competitive performance on both entity alignment
and entity prediction tasks.
References
Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,
abs/1607.06450, 2016.
Mohamed ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil ozair, Yoshua Bengio, R. Devon
Hjelm, and Aaron C. Courville. Mutual information neural estimation. in ICML, 2018.
Aleksandar Bojchevski and Stephan Gunnemann. Deep gaussian embedding of attributed graphs:
Unsupervised inductive learning via ranking. in ICLR, 2018.
9
Under review as a conference paper at ICLR 2021
Antoine Bordes, Nicolas Usunier, Alberto Garcia-Durdn, Jason Weston, and Oksana Yakhnenko.
Translating embeddings for modeling multi-relational data. In NIPS, 2013.
Yixin Cao, Zhiyuan Liu, Chengjiang Li, Zhiyuan Liu, Juanzi Li, and Tat-Seng Chua. Multi-channel
graph neural network for entity alignment. In ACL, 2019.
Tim Dettmers, Pasquale Minervini, Pontus Stenetorp, and Sebastian Riedel. Convolutional 2D
knowledge graph embeddings. In AAAI, 2018.
Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. In AISTATS, 2010.
Lingbing Guo, Zequn Sun, and Wei Hu. Learning to exploit long-term relational dependencies in
knowledge graphs. In ICML, 2019.
William L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In NIPS, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. In CVPR, 2016.
Bhagya Hettige, Weiqing Wang, Yuan-Fang Li, and Wray L. Buntine. Robust attribute and structure
preserving graph embedding. In PAKDD, 2020.
R. Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Philip Bachman, Adam
Trischler, and Yoshua Bengio. Learning deep representations by mutual information estimation
and maximization. In ICLR, 2019.
Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S. Yu. A survey on knowledge
graphs: Representation, acquisition and applications. CoRR, abs/2002.00388, 2020.
Seyed Mehran Kazemi and David Poole. Simple embedding for link prediction in knowledge graphs.
In NeurlIPS, Montreal, Canada, 2018.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks.
In ICLR, 2017.
Furong Li, Xin Luna Dong, Anno Langen, and Yang Li. Knowledge verification for long-tail verticals.
PVLDB, 10, 2017.
Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. Learning entity and relation
embeddings for knowledge graph completion. In AAAI, 2015a.
Diego Marcheggiani and Ivan Titov. Encoding sentences with graph convolutional networks for
semantic role labeling. In EMNLP, 2017.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representa-
tions in vector space. In ICLR Workshop, 2013a.
Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space word
representations. In NAACL-HLT, 2013b.
Federico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany, Stephan Gunnemann, and
Michael M. Bronstein. Dual-primal graph convolutional networks. CoRR, abs/1806.00770, 2018.
Dai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc Nguyen, and Dinh Q. Phung. A novel embedding
model for knowledge base completion based on convolutional neural network. In NAACL-HLT,
2018.
Andrea Rossi, Donatella Firmani, Antonio Matinata, Paolo Merialdo, and Denilson Barbosa. Knowl-
edge graph embedding for link prediction: A comparative analysis. CoRR, abs/2002.00819,
2020.
Michael Sejr Schlichtkrull, Thomas N. Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max
Welling. Modeling relational data with graph convolutional networks. In ESWC, 2018.
10
Under review as a conference paper at ICLR 2021
Chao Shang, Yun Tang, Jing Huang, Jinbo Bi, Xiaodong He, and Bowen Zhou. End-to-end structure-
aware convolutional networks for knowledge base completion. In AAAI, 2019.
Nitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.
Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning
Research, 15, 2014.
Zequn Sun, Wei Hu, and Chengkai Li. Cross-lingual entity alignment via joint attribute-preserving
embedding. In ISWC, 2017.
Zequn Sun, Wei Hu, Qingheng Zhang, and Yuzhong Qu. Bootstrapping entity alignment with
knowledge graph embedding. In IJCAI, 2018.
Zequn Sun, Chengming Wang, Wei Hu, Muhao Chen, Jian Dai, Wei Zhang, and Yuzhong Qu.
Knowledge graph alignment network with gated multi-hop neighborhood aggregation. In AAAI,
2020.
Zhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by
relational rotation in complex space. In ICLR, 2019.
Yonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive representation distillation. In ICLR,
2020.
Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text
inference. In CVSC, 2015.
Theo Trouillon, Johannes Welbl, Sebastian Riedel, Eric Gaussier, and Guillaume Bouchard. Complex
embeddings for simple link prediction. In ICML, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. CoRR, abs/1807.03748, 2018.
Shikhar Vashishth, Soumya Sanyal, Vikram Nitin, and Partha P. Talukdar. Composition-based
multi-relational graph convolutional networks. In ICLR, 2020.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.
Petar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lid, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. Knowledge graph embedding: A survey of
approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29, 2017.
Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. Knowledge graph embedding by
translating on hyperplanes. In AAAI, 2014.
Zhichun Wang, Qingsong Lv, Xiaohan Lan, and Yu Zhang. Cross-lingual knowledge graph alignment
via graph convolutional networks. In EMNLP, 2018.
Yuting Wu, Xiao Liu, Yansong Feng, Zheng Wang, Rui Yan, and Dongyan Zhao. Relation-aware
entity alignment for heterogeneous knowledge graphs. In IJCAI, pp. 5278-5284, 2019.
Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in
convolutional network. CoRR, 2015.
Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. Embedding entities and
relations for learning and inference in knowledge bases. In ICLR, 2015.
Rui Ye, Xin Li, Yujie Fang, Hongyu Zang, and Mingzhong Wang. A vectorized relational graph
convolutional network for multi-relational network alignment. In IJCAI, 2019.
11
Under review as a conference paper at ICLR 2021
A Decentralized Attention Networks
A.1 Architecture
We illustrate a four-layer decentralized attention network (DAN) in Figure 5. Following the existing
works AliNet (Sun et al., 2020), CompGCN (Vashishth et al., 2020) etc. (Ye et al., 2019; Wu et al.,
2019), we also combine the relation embeddings in the aggregation. The original entity embeddings
(i.e., g-1), relation embeddings and weight matrices are randomly initialized before training. At step
0, we initialize g0 with the original entity embeddings by mean aggregator. At step 1, g-1 and g0 are
fed into DAN. Then, we combine the hidden representations with relation embeddings (steps 2, 3).
Finally, We obtain the output of the first layer g1 (step 4). Repeating steps 1-4, We can sequentially
obtain the output of the last layers.
尸
9i
Figure 5: OvervieW of a four-layer decentralized attention netWork (DAN). Best vieWed in color.
Grey, blue, light blue and orange nodes denote original entity embeddings, decentralized entity
embeddings, hidden entity embeddings and relation embeddings, respectively. Taking the first layer
as example, as DAN requires the output embeddings of tWo previous layers as input, We randomly
initialize the original embeddings as g-1 (identical to d-1), and use an appropriate aggregator to
generate the initial decentralized embeddings g0 (identical to d0).
A.2 Implementation Details
Infrastructure. FolloWing GAT (Velickovic et al., 2018), We adopt dropout (Srivastava et al., 2014)
and layer normalization (Ba et al., 2016) for each module in DAN. To fairly compare decentRL
With other models, We do not leverage the multi-head attention mechanism (VasWani et al., 2017;
Velickovic et al., 2018) Which has not been used in other GNN-based models, although it can be
easily integrated.
Furthermore, We consider residual connections (He et al., 2016) betWeen different layers of DAN to
avoid over-smoothing, in Which We not only consider the output of the previous layer as “residual”,
but also involve the output of the mean aggregator (i.e., gi0). This can be Written as folloWs:
gik0 := gi0 + gik-1 + gik.	(10)
For simplicity, in the rest of the paper, We still use gik to denote the output at layer k.
Adaption to different tasks. For different KG representation learning tasks, We also consider
different adaptation strategies to achieve better performance.
12
Under review as a conference paper at ICLR 2021
For the entity alignment task, we follow the existing GNN-based models (Sun et al., 2020; Wang
et al., 2018) to concatenate the output representation of each layer as the final output. We formalize it
as follows:
gi = [gi1 k . . . k giK].
(11)
On the other hand, the entity prediction task prefers the prediction ability rather than the learned
representations (Guo et al., 2019). We only use the output of the last layer as the final output
representation, which allows us to choose larger batch size or hidden size to obtain better performance.
We write it as follows:
gi = giK.	(12)
To enhance the predictive ability of decoders, here we only regard the mutual information-based loss
as a kind of regularization (which is similar to MINE (Belghazi et al., 2018)), and thus we re-scale
the loss weight to 0.001.
B Automatic Knowledge Distillation
B.1 Insight
The existing works usually choose to jointly optimize two input variables in the density function f,
in which these two variables can be regarded as two different outputs of two models. For example,
InfoNCE uses an encoder to obtain the latent representations and another model to summarize those
representations to one context vector. This is similar to DeepInfoMax and DeepGraphInfo, which
also leverage two models to obtain local features and summarize global features, respectively.
However, in our case, the mutual information that we want to maximize is between the input and
output of the same model, where the input vectors are just randomly initialized raw embeddings. We
argue that jointly optimizing the original input ei with the output gi in Equation (7) may drive ei to
completely match gi .
To resolve this problem, we only use the copy of ei when estimating the mutual information density
between ei and gi. In other words, we do not update the gradient of ei in Equation (7), leading to a
natural knowledge distillation architecture.
Specifically, we separately optimize ei and gi in different training examples or batches. The first step
is corresponding to the former part of Equation (8):
argmax E log (	f(gi,e 八).	(13)
gi,f Xi	v∑ej ∈Xi f (gi ej V
Here, ei is served as a “pre-trained” teacher model to teach a “student”. Hence, the learnable
parameters are gi and f.
As aforementioned, ei needs to participate in learning the representations of its neighbors, during
which it can gain knowledge to teach its student gi. This step is achieved by the latter part in
Equation (8):
argmax X E log (P f(gj,ej) ʌ ʌ),	(14)
e ej∈Ni Xj 'Eek ∈Xj f(gj, ek ”
where our aim is to find the optimal eito maximize the mutual information between the original and
output representations of its neighbors.
B.2 The Lower-bound of Mutual Information
We do not really need to explicitly separate the training procedure into the two steps described in
Appendix B.1, which is widely used in adversarial learning. Instead, this knowledge distillation
mechanism can be automatically achieved during different mini-batches.
13
Under review as a conference paper at ICLR 2021
Specifically, if we expand Equation (13) a little bit, then we obtain:
(Ni,Θ,f)
argmax E log
Ni,Θ,f Xi
(	f(G(Ni), ^i)	ʌ
(Pej ∈Xi f (G(Ni), ^∙ V
(15)
where Ni = {ej |ej ∈ Ni} is the original neighbor embedding set for ei and Θ denotes the parameters
of our decentralized model G. As the optimal Θ for the model depends on the neighbor representation
set Ni , and the optimal density function f also relies on the output of the model, it is impossible to
search all spaces to find the best parameters. In practice, we choose to optimize a weaker lower-bound
on the mutual information I(gi, 30 (Tian et al., 2020). In this case, a relatively optimal neighbor
embedding eχ in Equation (15) is:
eX
aremax E loκ (	f"/ ei))
geχ	Xi l g (Pej∈Xi f (G(Ni), 3 )J ,
(16)
and we have:
^, ....
I(gi , t3ilex)
E log
Xi
E log
Xi
f (G({e1, . . . , ex, . . . , e∣Ni| }, ei)
Ej ∈Xi f (G({e1, . .., eX,..., e∣Ni∣}, ej)
f*(G*(NP,ei)	ʌ = bg* e∙)
e ∈xi f *(G*(N*), ej )J= (gi, i)
(17)
(18)
≤
≤
≤
^,	.. .. ..
i(gi, ei) + iog(∣Xi∣)
I(gi, ^i),
(19)
(20)
where * denotes the optimal setting for the corresponding parameters. Equations (18) and (19) are the
conclusion of InfoNCE, given that |Xi | is large enough. The above equations suggest that optimizing
eX can also lower-bound the mutual information without the requirement of other parameters being
perfectly assigned.
Consider that the entity eX may have more than one neighbor, we can optimize those cases together:
e
*
X
argmax E log
ex	ej ∈Nx Xj
(	f(G(Nj), ^j)	A
VPek∈Xj f (G(Nj), ^)
(21)
Evidently, the above equation is identical to Equation (14), which means that optimizing Equation (15)
can subsequently contribute to optimizing the original neighbor representations.
Therefore, the proposed architecture can automatically distill knowledge, in different mini-batches,
from the original representations into the output representations.
C	Further Analysis
C.1 Dataset Details
The detailed statistics of the entity alignment datasets are shown in Table 6. Although we only set
20% of entities in testing set as open entities, there are actually more than 20% of triples that were
removed from the training set.
For the details of datasets used in entity prediction, we suggest readers to refer to (Bordes et al.,
2013) and (Dettmers et al., 2018).
C.2 Ablation Study on Open Entity Alignment
We also conducted an ablation study on the open entity alignment task, as shown in Table 7. The
experimental results, in principle, are consistent to those on conventional entity alignment. The
proposed architecture (decentRL + auto-distiller) still outperformed other alternatives. By contrast,
the performance of the centralized model with auto-distiller dropped significantly, in comparison with
that it almost has identical performance with decentRL + infoNCE in Table 5. Another worth-noting
point is that the gap on Hits@10 narrowed in the open entity alignment task, which may be because
the training data were shrunk considerably due to removing the corresponding triples referred to
unseen entities.
14
Under review as a conference paper at ICLR 2021
Table 6: Statistics of entity alignment datasets.
Datasets	Original					Open	
	#Entities	#Relations	#Triples	#Train entity pairs	#Test entity pairs	#Train triples	#Test triples
ZH-EN	19,388	1,701	70,414	4,500	10,500	53,428	16,986
	19,572	1,323	95,142	4,500	10,500	72,261	22,881
JA-EN	19,814	1,299	77,214	4,500	10,500	57,585	19,629
	19,780	1,153	93,484	4,500	10,500	69,479	24,005
FR-EN	19,661	903	105,998	4,500	10,500	79,266	26732
	19,993	1,208	115,722	4,500	10,500	87,030	28692
Table 7: Ablation study on open entity alignment. Average of 5 runs.
Methods	ZH-EN			JA-EN			FR-EN		
	Hits@1	Hits@10	MRR	Hits@1	Hits@10	MRR	Hits@1	Hits@10	MRR
decentRL + auto-distiller	0.565	0.775	0.643	0.583	0.786	0.659	0.590	0.814	0.673
decentRL + infoNCE	0.557	0.775	0.637	0.574	0.785	0.652	0.583	0.811	0.666
decentRL + L2	0.552	0.770	0.632	0.574	0.782	0.650	0.581	0.806	0.664
centRL + auto-distiller	0.551	0.765	0.629	0.573	0.776	0.648	0.578	0.806	0.662
centRL	0.529	0.764	0.614	0.554	0.775	0.634	0.560	0.799	0.647
Table 8: Performance of decentRL with different dimensions. Average of 5 runs.
Hidden size	ZH-EN			JA-EN			FR-EN		
	Hits@1	Hits@10	MRR	Hits@1	Hits@10	MRR	Hits@1	Hits@10	MRR
64	0.429	0.644	0.508	0.474	0.673	0.547	0.468	0.704	0.554
128	0.511	0.726	0.590	0.541	0.745	0.617	0.535	0.773	0.623
256	0.560	0.785	0.643	0.578	0.791	0.657	0.578	0.817	0.665
512	0.589	0.819	0.672	0.596	0.819	0.678	0.602	0.842	0.689
C.3 Impact of Dimensions
We also evaluated decentRL under different settings of dimensions. The results are shown in Table 8.
With the increase of the input dimensions (i.e., embedding size), the performance of decentRL
improved quickly, with dimension = 128 achieving comparable performance with the state-of-
the-art methods (e.g., AliNet with 300 dimension) and outperforming them at dimension = 256.
Furthermore, decentRL can continually gain benefit from larger hidden sizes. Even when the
dimension was set to 512, the improvement was still significant.
15