Under review as a conference paper at ICLR 2021
Federated Learning of a Mixture of Global
and Local Models
Anonymous authors
Paper under double-blind review
Ab stract
We propose a new optimization formulation for training federated learning mod-
els. The standard formulation has the form of an empirical risk minimization
problem constructed to find a single global model trained from the private data
stored across all participating devices. In contrast, our formulation seeks an ex-
plicit trade-off between this traditional global model and the local models, which
can be learned by each device from its own private data without any communi-
cation. Further, we develop several efficient variants of SGD (with and without
partial participation and with and without variance reduction) for solving the new
formulation and prove communication complexity guarantees. Notably, our meth-
ods are similar but not identical to federated averaging / local SGD, thus shedding
some light on the essence of the elusive method. In particular, our methods do not
perform full averaging steps and instead merely take steps towards averaging. We
argue for the benefits of this new paradigm for federated learning.
1	Introduction
With the proliferation of mobile phones, wearable devices, tablets, and smart home devices comes an
increase in the volume of data captured and stored on them. This data contains a wealth of potentially
useful information to the owners of these devices, and more so if appropriate machine learning
models could be trained on the heterogeneous data stored across the network of such devices. The
traditional approach involves moving the relevant data to a data center where centralized machine
learning techniques can be efficiently applied (Dean et al., 2012; Reddi et al., 2016). However,
this approach is not without issues. First, many device users are increasingly sensitive to privacy
concerns and prefer their data to never leave their devices. Second, moving data from their place of
origin to a centralized location is very inefficient in terms of energy and time.
1.1	Federated learning
Federated learning (FL) (McMahan et al., 2016; Konecny et al., 2016b;a; McMahan et al., 2017) has
emerged as an interdisciplinary field focused on addressing these issues by training machine learning
models directly on edge devices. The currently prevalent paradigm (Li et al., 2019; Kairouz et al.,
2019) casts supervised FL as an empirical risk minimization problem of the form
n
min 1	fi(x),	(1)
x∈Rd n 乙 J -	()
i=1
where n is the number of devices participating in training, x ∈ Rd encodes the d parameters of a
global model (e.g., weights of a neural network) and
fi(x) := Eξ〜Di [f(x,ξ)]
represents the aggregate loss of model x on the local data represented by distribution Di stored on
device i. One of the defining characteristics of FL is that the data distributions Di may possess very
different properties across the devices. Hence, any potential FL method is explicitly required to be
able to work under the heterogeneous data setting.
The most popular method for solving (1) in the context of FL is the FedAvg algorithm (McMahan
et al., 2016). In its most simple form, when one does not employ partial participation, model com-
pression, or stochastic approximation, FedAvg reduces to Local Gradient Descent (LGD) (Khaled
1
Under review as a conference paper at ICLR 2021
et al., 2019; 2020), which is an extension of GD performing more than a single gradient step on
each device before aggregation. FedAvg has been shown to work well empirically, particularly
for non-convex problems, but comes with poor convergence guarantees compared to the non-local
counterparts when data are heterogeneous.
Some issues with current approaches to FL
The first motivation for our research comes from the appreciation that data heterogeneity does not
merely present challenges to the design of new provably efficient training methods for solving (1),
but also inevitably raises questions about the utility of such a global solution to individual users.
Indeed, a global model trained across all the data from all devices might be so removed from the
typical data and usage patterns experienced by an individual user as to render it virtually useless.
This issue has been observed before, and various approaches have been proposed to address it. For
instance, the MOCHA (Smith et al., 2017) framework uses a multi-task learning approach to allow
for personalization. Next, (Khodak et al., 2019) propose a generic online algorithm for gradient-
based parameter-transfer meta-learning and demonstrate improved practical performance over Fe-
dAvg (McMahan et al., 2017). Approaches based on variational inference (Corinzia & Buhmann,
2019), cyclic patterns in practical FL data sampling (Eichner et al., 2019) transfer learning (Zhao
et al., 2018) and explicit model mixing (Peterson et al., 2019) have been proposed.
The second motivation for our work is the realization that even very simple variants of FedAvg, such
as LGD, which should be easier to analyze, fail to provide theoretical improvements in communica-
tion complexity over their non-local cousins, in this case, GD (Khaled et al., 2019; 2020).1
This observation is at odds with the practical success of local methods in FL. This leads us to ask
the question: if LGD does not theoretically improve upon GD as a solver for the traditional global
problem (1), perhaps LGD should not be seen as a method for solving (1) at all. In such a case,
what problem does LGD solve? A good answer to this question would shed light on the workings
of LGD, and by analogy, on the role local steps play in more elaborate FL methods such as local
SGD (Stich, 2020; Khaled et al., 2020) and FedAvg.
2	Contributions
In our work we argue that the two motivations mentioned in the introduction point in the same
direction, i.e., we show that a single solution can be devised addressing both problems at the same
time. Our main contributions are:
New formulation of FL which seeks an implicit mixture of global and local models. We
propose a new optimization formulation of FL. Instead of learning a single global model by solving
(1), we propose to learn a mixture of the global model and the purely local models which can be
trained by each device i using its data Di only. Our formulation (see Sec. 3) lifts the problem from
Rd to Rnd, allowing each device i to learn a personalized model xi ∈ Rd. These personalized
models are encouraged to not depart too much from their mean by the inclusion of a quadratic
penalty ψ multiplied by a parameter λ ≥ 0. Admittedly, the idea of softly-enforced similarity of the
local models was already introduced in the domain of the multi-task relationship learning (Zhang
& Yeung, 2010; Liu et al., 2017; Wang et al., 2018) and distributed optimization (Lan et al., 2018;
Gorbunov et al., 2019; Zhang et al., 2015). The mixture objective we propose (see (2)) is a special
case of their setup, which justifies our approach from the modeling perspective. Note that Zhang
et al. (2015); Liu et al. (2017); Wang et al. (2018) provide efficient algorithms to solve the mixture
objective already. However, none of the mentioned papers consider the FL application, nor they
shed a light on the communication complexity of LGD algorithms, which we do in our work.
Theoretical properties of the new formulation. We study the properties of the optimal solution
of our formulation, thus developing an algorithmic-free theory. When the penalty parameter is set to
zero, then obviously, each device is allowed to train their own model without any dependence on the
data stored on other devices. Such purely local models are rarely useful. We prove that the optimal
1After our paper was completed, a lower bound on the performance of local SGD was presented that is worse
than the known minibatch SGD guarantee (Woodworth et al., 2020a), confirming that the local methods do not
outperform their non-local counterparts in the heterogeneous setup. Similarly, the benefit of local methods in
the non-heterogeneous scenario was questioned in (Woodworth et al., 2020b).
2
Under review as a conference paper at ICLR 2021
local models converge to the traditional global model characterized by (1) at the rate O(1∕λ). We
also show that the total loss evaluated at the local models is never higher than the total loss evaluated
at the global model (see Thm. 3.1). Moreover, we prove an insightful structural result for the optimal
local models: the optimal model learned by device i arises by subtracting the gradient of the loss
function stored on that device evaluated at the same point (i.e., a local model) from the average of the
optimal local models (see Thm. 3.2). As a byproduct, this theoretical result sheds new light on the
key update step in the model agnostic meta-learning (MAML) method (Finn et al., 2017), which has
a similar but subtly different structure.2 The subtle difference is that the MAML update obtains the
local model by subtracting the gradient evaluated at the global model. While MAML was originally
proposed as a heuristic, we provide rigorous theoretical guarantees.
Loopless LGD: non-uniform SGD applied to our formulation. We then propose a randomized
gradient-based method—Loopless Local Gradient Descent (L2GD)—for solving our new formula-
tion (Algorithm 1). This method is, in fact, a non-standard application of SGD to our problem, and
can be seen as an instance of SGD with non-uniform sampling applied to the problem of minimizing
the sum of two convex functions (Zhao & Zhang, 2015; Gower et al., 2019): the average loss, and
the penalty. When the loss function is selected by the randomness in our SGD method, the stochastic
gradient step can be interpreted as the execution of a single local GD step on each device. Since we
set the probability of the loss being sampled to be high, this step is typically repeated multiple times,
resulting in multiple local GD steps. In contrast to standard LGD, the number of local steps is not
fixed, but random, and follows a geometric distribution. This mechanism is similar in spirit to how
the recently proposed loopless variants of SVRG (Hofmann et al., 2015; Kovalev et al., 2020) work
in comparison with the original SVRG (Johnson & Zhang, 2013a; Xiao & Zhang, 2014). Once the
penalty is sampled by our method, the resultant SGD step can be interpreted as the execution of an
aggregation step. In contrast with standard aggregation, which performs full averaging of the local
models, our method merely takes a step towards averaging. However, the step is relatively large.
Convergence theory. By adapting the general theory from (Gower et al., 2019) to our setting,
We obtain theoretical convergence guarantees assuming that each f is L-smooth and μ-strongly
convex (see Thm. 4.2). Interestingly, by optimizing the probability of sampling the penalty (we get
p? = λ+λL), which is an indirect way of fixing the expected number oflocal steps to 1 + λ, We prove
an v2λ-L log 1 bound on the expected number of communication rounds (see Cor. 4.3). We believe
λ ^+ LJ μ	ε
that this is remarkable in several ways. By choosing λ small, we tilt our goal towards pure local
models: the number of communication rounds is tending to 0 as λ → 0. If λ → ∞, the solution our
formulation converges to is the optimal global model, and L2GD obtains the communication bound
O (L log ε), which matches the efficiency of GD.
What problem do local methods solve? Noting that L2GD is a (mildly nonstandard) version of
LGD,3 which is a key method most local methods for FL are based on, and noting that, as we show,
L2GD solves our new formulation of FL, we offer a new and surprising interpretation of the role
of local steps in FL. In particular, the role of local steps in gradient type methods, such as GD, is
not to reduce communication complexity, as is generally believed. Indeed, there is no theoretical
result supporting this claim in the key heterogeneous data regime. Instead, their role is to steer the
method towards finding a mixture of the traditional global and the purely local models. Given that
the stepsize is fixed, the more local steps are taken, the more we bias the method towards the purely
local models. Our new optimization formulation of FL formalizes this as it defines the problem that
local methods, in this case L2GD, solve. There is an added benefit here: the more we want our
formulation to be biased towards purely local models (i.e., the smaller the penalty parameter λ is),
the more local steps does L2GD take, and the better the total communication complexity of L2GD
becomes. Hence, despite a lot of research on this topic, our paper provides the first proof that a
local method (e.g., L2GD) can be better than its non-local counterpart (e.g., GD) in terms of total
communication complexity in the heterogeneous data setting. We are able to do this by noting that
local methods should better be seen as methods for solving the new FL formulation proposed here.
Generalizations: partial participation, local SGD and variance reduction. We further gener-
alize and improve our method by allowing for (i) stochastic partial participation of devices in each
communication round,(ii) subsampling on each device which means we can perform local SGD steps
2The connection of FL and multi-task meta learning is discussed in (Kairouz et al., 2019), for example.
3To be specific, L2GD is equivalent to Overlap LGD (Wang et al., 2020) with random local loop size.
3
Under review as a conference paper at ICLR 2021
instead of local GD steps, and (iii) total variance reduction mechanism to tackle the variance com-
ing from three sources: locality of the updates induced by non-uniform sampling (already present
in L2GD), partial participation and local subsampling. Due to its level of generality, this method,
which we call L2SGD++, is presented in the Appendix only, alongside the associated complexity
results. In the main body of this paper, we instead present a simplified version thereof, which we
call L2SGD+ (Algorithm 3). The convergence theory for it is presented in Thm. 5.1 and Cor. 5.2.
Heterogeneous data. All our methods and convergence results allow for fully heterogeneous data
and do not depend on any assumptions on data similarity across the devices.
Superior empirical performance. We show through ample numerical experiments that our theo-
retical predictions can be observed in practice.
3	New Formulation of FL
We now introduce our new formulation for training supervised FL models:
min	{F (x) := f(x) + λψ(x)}
x1 ,...,xn ∈Rd
nn
f(X) = 1 X fi(xi), ψ(X) = 21n X kxi - xk2,
i=1	i=1
(2)
where λ ≥ 0 is a penalty parameter, X1, . . . , Xn ∈ Rd are local models, X := (X1, X2, . . . , Xn ) ∈ Rnd
and X = 1 Pi=I Xi is the average of the local models.
Due to the assumptions on fi we will make in Sec. 3.1, F is strongly convex and hence (2) has
a unique solution, which We denote χ(λ) = (χι(λ),..., Xη(λ)) ∈ Rnd. We further let x(λ)=
1 Pn=I Xi(λ). We now comment on the rationale behind the new formulation.
Local models (λ = 0). Note that for each i, Xi (0) solves the local problem minxi∈Rd fi (Xi). That
is, Xi(0) is the local model based on data Di stored on device i only. This model can be computed
by device i without any communication whatsoever. Typically, Di is not rich enough for this local
model to be useful. In order to learn a better model, one has to take into account the date from other
clients as well. This, however, requires communication.
Mixed models (λ ∈ (0, ∞)). As λ increases, the penalty λψ(X) has an increasingly more substantial
effect, and communication is needed to ensure that the models are not too dissimilar, as otherwise
the penalty λψ(X) would be too large.
Global model (λ = ∞). Let us now look at the limit case λ → ∞. Intuitively, this limit case
should force the optimal local models to be mutually identical, while minimizing the loss f . In
particular, this limit case will solve4 min {f (x) : xι,...,Xn ∈ Rd, xι = x2 =…=Xn} , which
is equivalent to the global formulation (2). Because of this, let us define Xi(∞) for each i to be the
optimal global solution of (1), and let X(∞) := (X1(∞), . . . , Xn (∞)).
3.1	Technical preliminaries
We make the following assumption on the functions fi :
Assumption 3.1 For each i, the function f : Rd → R is L-smooth and μ-strongly convex.
For Xi, yi ∈ Rd, hXi, yii denotes the standard inner product and kXk := hXi, Xii1/2 is the stan-
dard Euclidean norm. For vectors X = (X1, . . . , Xn ) ∈ Rnd, y = (y1, . . . , yn ) ∈ Rnd we
define the standard inner product and norm via hX, yi := Pin=1hXi, yii, kXk2 := Pin=1 kXik2 .
Note that the separable structure of f implies that (Vf(x)} = 1 Nfi(Xi), i.e., Vf(x) =
1 (Vfi(Xi), Vf2(X2),..., Vfn(Xn)).
Note that Assumption 3.1 implies that f is Lf -smooth with Lf := Ln and μ广StrOngIy convex with
μf := μ. Clearly, ψ is convex by construction. It can be shown that ψ is Lψ-smooth with Lψ = 1
4If λ = ∞ and xι = x2 = •…=Xn does not hold, we have F(x) = ∞. Therefore, we can restrict
ourselves on set xι = x2 = •…=Xn without loss of generality.
4
Under review as a conference paper at ICLR 2021
(See Appendix). We can also easily see that (Vψ(χ))i = 1 (Xi - X)(See Appendix), which implies
Ψ(x) = 2 PP k(vψ(χ))ik2 = 2 kvψ(χ)k2.
i=1
3.2	Characterization of optimal solutions
Our first result describes the behavior of f (X(λ)) and ψ(X(λ)) as a function of λ.
Theorem 3.1 The function λ → ψ(X(λ)) is non-increasing, and for all λ > 0 we have
ψ(x(λ)) ≤ f(XC))-f(X(O)).	(3)
Moreover, the function λ → f (X(λ)) is non-decreasing, and for all λ ≥ 0 we have
f(X(λ)) ≤ f (X(∞)).	(4)
Ineq. (3) says that the penalty decreases to zero as λ grows, and hence the optimal local models Xi(λ)
are increasingly similar as λ grows. The second statement suggest that the loss f (X(λ)) increases
with λ, but never exceeds the optimal global loss f (X(∞)) of the standard FL formulation (1).
We now characterize the optimal local models which connect our model to the MAML frame-
work (Finn et al., 2017), as mentioned in the introduction.
Theorem 3.2 For each λ > 0 and 1 ≤ i ≤ n we have
Xi(λ) = X(λ) - 1 Vfi(Xi(λ)).	(5)
n
Further, we have E Vfi(Xi(λ)) = 0 and ψ(x(λ)) = 212 ∣∣Vf (x(λ))k .
i=1
The optimal local models (5) are obtained from the average model by subtracting a multiple of
the local gradient. Observe that the local gradients always sum up to zero at optimality. This is
obviously true for λ = ∞, but it is a bit less obvious that this holds for any λ > 0.
Next, We argue the optimal local models converge to the traditional FL solution at the rate O(1∕λ).
Theorem 3.3 Let P (z) := 1 P21 fi(z). Then, x(∞) is the unique minimizer of P and we have
∣VP(X(λ))∣2 ≤ 2L2(f (X(U - f(x(0))).	(6)
4	L2GD: Loopless Local GD
In this section we describe a new randomized method for solving the formulation (2). Our method
is a non-uniform SGD for (2) seen as a 2-sum problem, sampling either Vf or Vψ to estimate VF .
Letting 0 < p < 1, we define a stochastic gradient of F at X ∈ Rnd as follows
G(X) :
vlf(χ)	with probability 1 - P
λ*(X)	with probability P
(7)
Clearly, G(X) is an unbiased estimator of VF (X). This leads to the following method for minimizing
F, which we call L2GD: Xk+1 = Xk - αG(Xk). Plugging the formulas for Vf (X) and Vψ(X) into
(7), and writing the resulting method in a distributed manner, we arrive at Algorithm 1. In each
iteration, a coin ξ is tossed and lands 1 with probability p and 0 with probability 1 - p. If ξ = 0,
all Devices perform one local GD step (8), and if ξ = 1, Master shifts each local model towards the
average via (9). As we shall see in Sec. 4.2, our theory limits the value of the stepsize α, which has
the effect that the ratio αλ cannot exceed 1. Hence, (9) is a convex combination of Xk and Xk.
np	2
Note that Algorithm 1 is only required to communicate when a two consecutive coin tosses land a
different value (see the detailed explanation in Sec. C.1 of the appendix). Consequently, the expected
number of communication rounds in k iterations of L2GD is P(1 - P)k.
Remark 4.1 Our algorithm statements do not take the data privacy into the consideration. While
privacy is a very important aspect ofFL; in this paper, we tackle different FL challenges and thus we
ignore privacy issues. However, the proposed algorithms can be implemented in a private fashion
as well using tricks that are used in the classical FL scenario (Bonawitz et al., 2017).
5
Under review as a conference paper at ICLR 2021
Algorithm 1 L2GD: Loopless Local Gradient Descent
Input: x0 = •一 =Xn ∈ Rd, stepsize a, probability P
for k = 0, 1, . . . do
ξ = 1 with probability p and 0 with probability 1 - p
if ξ = 0 then
All Devices i = 1, . . . , n perform a local GD step:
xk+1 = Xk- n(T-p)Vfi(Xk)	(8)
else
n
Master computes the average Xk = n P Xk
i=1
Master for each i computes step towards aggregation
Xk+1 = (ι- n) Xk + αp Xk	(9)
end if
end for
4.1	The dynamics of local GD and averaging steps
Notice that the average of the local models does not change during an aggregation step. Indeed,
Xk+1 is equai to n P Xk+1 = n P [(ι - αρ) Xk + αρXk i = x .
If several averaging steps take place in a sequence, the point a = Xk in (9) remains unchanged, and
each local model Xik merely moves along the line joining the initial value of the local model at the
start of the sequence and a, with each step pushing Xik closer to the average a.
In summary, the more local GD steps are taken, the closer the local models get to the pure local
models; and the more averaging steps are taken, the closer the local models get to their average
value. The relative number of local GD vs. averaging steps is controlled by the parameter p: the
expected # oflocal GD steps is P, and the expected number ofconsecutive aggregation steps is ɪ-p.
4.2	Convergence theory
We now present our convergence result for L2GD.
Theorem 4.2 LetAssumption 3.1 hold. If α ≤ 芸,then
E IjlXk - X(，)『i ≤(1 - αμ)k 忖-，(刈『+2nασ2,
WhereL := n maχ n 舍,P o and σ2 := n2 Pn=I (i-p kvfi(Xi(X)) k2+λρ2 ∣E(X)-X(X) k2).
Let us find the parameters p, α which lead to the fastest rate, to push the error within
(θ(ε) + 2nμσ2) -neighborhood of the optimum5, i.e., to achieve
E h∣∣Xk - X(λ)∣∣2i ≤ ε ∣∣X0 - X(λ)∣∣2 + 2nασ2.	(10)
Corollary 4.3 The value p? = Lpp minimizes both the number of iterations and the expected
number of communications for achieving (10). In particular, the optimal number of iterations is
2 L+p log 1, and the optimal expected number of communications is p2pLL log 1.
μ⅛	cε	μ⅛	cε
If we choose P = p?, then Op = 2, and the aggregation rule (9) in Algorithm 1 becomes
Xk+1 = 2 (Xk + Xk)
(11)
5In Sec. 5 We propose a variance reduced algorithm which removes the (2n°σ2)-neighborhood
from Thm. 4.2. In that setting, our goal will be to achieve E xk - x(λ)2 ≤ ε x0 - x(λ)2.
6
Under review as a conference paper at ICLR 2021
°∙33
1 II
X X
6 4
ω」SPUn2 UO-SunluLUoU
°∙
1
X
2
UOAn-OS
Figure 1: Distance of solution x(λ) of (2) to	Figure 2： * CommUnication rounds to get
pure local solution χ(0) and global solution	F(：o)-F(X：) ≤ 10-5 as a function of P with
x(∞) as a function of λ. LOgiStiC regression	p* "≈ 0.09 (for L2SGD+). Logistic regres-
on a1a dataset. See Appendix for the setup. sion on a1a dataset with λ = 0.1.
while the local GD step (8) becomes χk+1 = Xk -克Vfi(χk). Notice that while our method does
not support full averaging as that is too unstable, (11) suggests that one should take a large step
towards averaging. As λ get smaller, the solution to the optimization problem (2) will increasingly
favour pure local models, i.e., xi(λ) → xi(0) := arg min fi for all i as λ → 0. Pure local mod-
els can be computed without any communication whatsoever and Cor. 4.3 confirms this intuition:
the optimal number of communication round decreases to zero as λ → 0. On the other hand, as
λ → ∞, the optimal number of communication rounds converges to 2L log ɪ, which recovers the
performance of GD for finding the globally optimal model (see Fig. 1).
In summary, we recover the communication efficiency of GD for finding the globally optimal model
as λ → ∞ (ignoring the (2nμσ ) -neighborhood). However, for other values of λ, the Communi-
cation complexity of L2GD is better and decreases to 0 as λ → 0. Hence, our communication
complexity result interpolates between the communication complexity of GD for finding the global
model and the zero communication complexity for finding the pure local models.
5 Loopless Local SGD with Variance Reduction
As we have seen in Sec. 4.2, L2GD is a specific instance of SGD, thus only converges linearly to
a neighborhood of the optimum. In this section, we resolve the mentioned issue by incorporating
control variates to the stochastic gradient (Johnson & Zhang, 2013b; Defazio et al., 2014). We
go further: we assume that each local objective has a finite-sum structure and propose an algorithm,
L2SGD+, which takes local stochastic gradient steps, while maintaining (global) linear convergence
rate. As a consequence, L2SGD+ is the first local SGD with linear convergence.6 For convenience,
we present variance reduced local GD (i.e., no local subsampling) in the Appendix.
Assumption 5.1 Assume that f has a finite-sum structure: fi(xi) = m1 Pjm=I fi,j (xi). Let fij be
convex, L-smooth while fi is μ-strongly convex (for each 1 ≤ j ≤ m, 1 ≤ i ≤ n).
5.1	Convergence theory
We are now ready to present a convergence rate of L2SGD+ (the algorithm, along with the efficient
implementation is presented in Appendix C.4).
Theorem 5.1 LetAssumption 5.1 hold and choose ɑ = n min {年+{), 4λ+μ } ∙ Then the iteration
complexity ofAlgorithm 3 is max { 4L-μm, 4λp++μ } log ɪ.
Next, we find the value of p that yields both the best iteration and communication complexity.
6We are aware that a linearly converging local SGD (with λ = ∞) can be obtained as a particular instance of
the decoupling method (Mishchenko & Richtarik, 2019). Other variance reduced local SGD algorithms (Liang
et al., 2019; Karimireddy et al., 2019; Wu et al., 2019) do not achieve linear convergence.
7
Under review as a conference paper at ICLR 2021
Corollary 5.2 Both communication and iteration complexity of L2SGD+ are minimized for p =
4λ+4L4λ+m+i)μ∙ The resulting iteration ComPIexity is(4λ + 4L + m +1)log 1, while the com-
munication ComPIexity is 4笠+44+(m+1)*(4 L + m) log 1.
Note that with λ → ∞, the communication complexity of L2SGD+ tends to(4 L + m) log ɪ,
which is communication complexity of minibatch SAGA to find the globally optimal
model (Hanzely & Richtarik, 2019). On the other hand, in the pure local setting (λ = 0), the
communication complexity becomes log ɪ - this is because the Lyapunov function involves a term
that measures the distance of local models, which requires communication to be estimated.
Remark 5.3 L2SGD+ is the simPlest local SGD method with variance reduction. In the APPendix,
we Present L2SGD++ which allows for 1) an arbitrary number of data Points Per client and ar-
bitrary local subsamPling, 2) Partial ParticiPation of clients, and 3) local SVRG-like uPdates of
control variates (thus better memory). Lastly, L2SGD++ exPloits the comPlex smoothness structure
of the local objectives, resulting in tighter rates.
6 Experiments
In this section, we numerically verify the theoretical claims from this paper. We only present a
single experiment here, all remaining ones along with the missing details about the setup are in the
Appendix. In particular, the Appendix includes two more experiments. The first one studies how p
(communication) influences the convergence of L2SGD+. The second experiment aims to examine
the effect of parameter λ on the convergence rate of L2SGD+.
We consider logistic regression problem with LibSVM data (Chang & Lin, 2011). The data were
normalized so that fi0,j is 1-smooth for each j, while the local objectives are 10-4-strongly convex.
In order to cover a range of possible scenarios, we have chosen a different number of clients for each
dataset (see the Appendix). Lastly, the stepsize was always chosen according to Thm. 5.1.
We compare three different methods: L2SGD+, L2GD with local subsampling (L2SGD in the Ap-
pendix), and L2GD with local subsampling and control variates constructed for ψ only (L2SGD2 in
the Appendix; similar to (Liang et al., 2019)). We expect L2SGD+ to converge to the global opti-
mum linearly, while both L2SGD and L2SGD2 to converge to certain neighborhood. Each method
is applied to two objectives constructed by a different split of the data among the devices. For the
homogeneous split, we randomly reshuffle the data. For heterogeneous split, we first sort the data
based on the labels and then construct the local objectives according to the current order.
Dataset： ala	Dataset： mushrooms	Dataset： g∣sette scale
~,∙,∙,∙,∙“
⅛=βE-⅛0Λ≡ 8=β-s-
O SO IW ISO 28	2S0 3W 3S0	400
Data passes
Dataset： mβdelon
⅛=βE-⅛0Λ≡ 8=β-s-
→- L2SGD+
L2SGD2
+ L2SGD
-r- Hom L2SGD-I-
-j Hom L2SGD2
—r- Hom L2SGD
O SO IW UO 28	2S0 3W 3S0	*X>
Data passes
Dataset： a Ba
1 ≡ I ≡
⅛=βE-⅛0Λ≡ 8=β-s-
Φ ，。 IW 1SΦ 28 2W 3X 3S0	«0
Data passes
Date set: PhiShIng
Figure 3: L2SGD+, vs L2SGD vs L2SGD2 with identical stepsize (details in the Appendix).
Fig. 3 demonstrates the importance of variance reduction - it ensures a fast global convergence of
L2SGD+, while the neighborhood is slightly smaller for L2SGD2 compared to L2SGD. As pre-
dicted, data heterogeneity does not affect the convergence speed of the proposed methods.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu. Katyusha: The first direct acceleration of stochastic gradient methods. In Pro-
Ceedings ofthe 49th Annual ACM SIGACT Symposium on Theory ofComputing, pp. 1200-1205.
ACM, 2017.
Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H Brendan McMahan, Sarvar
Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for privacy-
preserving machine learning. In Proceedings of the 2017 ACM SIGSAC Conference on Computer
and Communications Security, pp. 1175-1191, 2017.
Chih-Chung Chang and Chih-Jen Lin. LibSVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology (TIST), 2(3):27, 2011.
L.	Corinzia and J. M. Buhmann. Variational federated multi-task learning. arXiv preprint
arXiv:1906.06268, 2019.
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior,
Paul Tucker, Ke Yang, Quoc V Le, and et al. Large scale distributed deep networks. In Advances
in Neural Information Processing Systems, pp. 1223-1231, 2012.
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: a fast incremental gradient method
with support for non-strongly convex composite objectives. In Advances in Neural Information
Processing Systems, pp. 1646-1654, 2014.
H. Eichner, T. Koren, H. B. McMahan, N. Srebro, and K. Talwar. Semi-cyclic stochastic gradient
descent. In International Conference on Machine Learning, 2019.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation
of deep networks. In International Conference on Machine Learning, 2017.
Nidham Gazagnadou, Robert M Gower, and Joseph Salmon. Optimal mini-batch and step sizes for
SAGA. In International Conference on Machine Learning, 2019.
Eduard Gorbunov, Darina Dvinskikh, and Alexander Gasnikov. Optimal decentralized distributed
algorithms for stochastic convex optimization. arXiv preprint arXiv:1911.07363, 2019.
EdUard Gorbunov, FiliP Hanzely, and Peter Richtarik. A unified theory of sgd: Variance reduction,
sampling, quantization and coordinate descent. In The 23rd International Conference on Artificial
Intelligence and Statistics, 2020.
Robert Mansel Gower, Peter Richtarik, and Francis Bach. Stochastic quasi-gradient methods: vari-
ance reduction via Jacobian sketching. arXiv:1805.02632, 2018.
Robert Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor Shulgin, and Pe-
ter Richtarik. SGD: General analysis and improved rates. In Proceedings of the 36th In-
ternational Conference on Machine Learning, volume 97 of Proceedings of Machine Learn-
ing Research, pp. 5200-5209, Long Beach, California, USA, 09-15 Jun 2019. PMLR. URL
http://proceedings.mlr.press/v97/qian19b.html.
Filip Hanzely and Peter Richtarik. One method to rule them all: Variance reduction for data, param-
eters and many new methods. arXiv preprint arXiv:1905.11266, 2019.
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams. Variance re-
duced stochastic gradient descent with neighbors. In Advances in Neural Information Processing
Systems, pp. 2305-2313, 2015.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems 26, pp. 315-323, 2013a.
Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance
reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013b.
Peter Kairouz, H. Brendan McMahan, and et al. Advances and open problems in federated learning.
arXiv preprint arXiv:1912.04977v1, 2019.
9
Under review as a conference paper at ICLR 2021
Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and
Ananda Theertha Suresh. SCAFFOLD: stochastic controlled averaging for on-device federated
learning. arXiv preprint arXiv:1910.06378, 2019.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. First analysis of local GD on hetero-
geneous data. In NeurIPS Workshop on Federated Learning for Data Privacy and Confidentiality,
pp. 1-11,2019.
Ahmed Khaled, Konstantin Mishchenko, and Peter Richtarik. Tighter theory for local SGD on
identical and heterogeneous data. In The 23rd International Conference on Artificial Intelligence
and Statistics (AISTATS 2020), 2020.
M.	Khodak, M.-F. Balcan, and A. Talwalkar. Adaptive gradient-based meta-learning methods. In
Advances in Neural Information Processing Systems 32, 2019.
Jakub Konecny, H. Brendan McMahan, Daniel Ramage, and Peter Richtarik. Federated optimiza-
tion: distributed machine learning for on-device intelligence. arXiv:1610.02527, 2016a.
Jakub Konecny, H. Brendan McMahan, Felix Yu, Peter Richtarik, Ananda Theertha Suresh, and
Dave Bacon. Federated learning: strategies for improving communication efficiency. In NIPS
Private Multi-Party Machine Learning Workshop, 2016b.
Dmitry Kovalev, Samuel Horvath, and Peter Richtarik. Don't jump through hoops and remove
those loops: SVRG and Katyusha are better without the outer loop. In Proceedings of the 31st
International Conference on Algorithmic Learning Theory, 2020.
Guanghui Lan, Soomin Lee, and Yi Zhou. Communication-efficient algorithms for decentralized
and stochastic optimization. Mathematical Programming, pp. 1-48, 2018.
Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: challenges,
methods, and future directions. arXiv preprint arXiv:1908.07873, 2019.
Xianfeng Liang, Shuheng Shen, Jingchang Liu, Zhen Pan, Enhong Chen, and Yifei Cheng. Variance
reduced local SGD with lower communication complexity. arXiv preprint arXiv:1912.12844,
2019.
Sulin Liu, Sinno Jialin Pan, and Qirong Ho. Distributed multi-task relationship learning. In Pro-
ceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 937-946, 2017.
Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Aguera y Arcas. Federated learning
of deep networks using model averaging. arXiv preprint arXiv:1602.05629, 2016.
H Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficient learning of deep networks from decentralized data. In Proceedings of
the 20th International Conference on Artificial Intelligence and Statistics (AISTATS), 2017.
Konstantin Mishchenko and Peter Richtarik. A stochastic decoupling method for minimizing the
sum of smooth and non-smooth functions. arXiv preprint arXiv:1905.11535, 2019.
Yurii Nesterov. Introductory lectures on convex optimization: a basic course (Applied Optimization).
Kluwer Academic Publishers, 2004.
Daniel Peterson, Pallika Kanani, and Virendra J Marathe. Private federated learning with domain
adaptation. arXiv preprint arXiv:1912.06733, 2019.
Xun Qian, Zheng Qu, and Peter Richtarik. L-SVRG and L-Katyusha with arbitrary sampling. arXiv
preprint arXiv:1906.01481, 2019a.
Xun Qian, Zheng Qu, and Peter Richtarik. SAGA with arbitrary sampling. In The 36th International
Conference on Machine Learning, 2019b.
10
Under review as a conference paper at ICLR 2021
Zheng Qu and Peter Richtarik. Coordinate descent with arbitrary sampling II: Expected sep-
arable overapproximation. Optimization Methods and Software, 31(5):858-884, 2016. doi:
10.1080/10556788.2016.1190361. URL http://dx.doi.org/10.1080/10556788.
2016.1190361.
Sashank J. Reddi, Jakub Konecny, Peter Richtarik, Barnabas Poczos, and Alex Smola. AIDE: fast
and communication efficient distributed optimization. arXiv:1608.06879, 2016.
Peter Richtarik and Martin Takac. Parallel coordinate descent methods for big data optimization.
Mathematical Programming, 156(1-2):433-484, 2016.
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet S Talwalkar. Federated multi-
task learning. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-
wanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems 30, pp.
4424-4434. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/
7029-federated-multi-task-learning.pdf.
Sebastian U. Stich. Local SGD converges fast and communicates little. In International Conference
on Learning Representations, 2020.
Jianyu Wang, Hao Liang, and Gauri Joshi. Overlap local-sgd: An algorithmic approach to hide
communication delays in distributed sgd. In ICASSP 2020-2020 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP), pp. 8871-8875. IEEE, 2020.
Weiran Wang, Jialei Wang, Mladen Kolar, and Nathan Srebro. Distributed stochastic multi-task
learning with graph regularization. arXiv preprint arXiv:1802.03830, 2018.
Blake Woodworth, Kumar Kshitij Patel, and Nathan Srebro. Minibatch vs local sgd for heteroge-
neous distributed learning. arXiv preprint arXiv:2006.04735, 2020a.
Blake Woodworth, Kumar Kshitij Patel, Sebastian U Stich, Zhen Dai, Brian Bullins, H Brendan
McMahan, Ohad Shamir, and Nathan Srebro. Is local sgd better than minibatch sgd? arXiv
preprint arXiv:2002.07839, 2020b.
Zhaoxian Wu, Qing Ling, Tianyi Chen, and Georgios B Giannakis. Federated variance-
reduced stochastic gradient descent with robustness to byzantine attacks. arXiv preprint
arXiv:1912.12716, 2019.
Lin Xiao and Tong Zhang. A proximal stochastic gradient method with progressive variance reduc-
tion. SIAM Journal on Optimization, 24(4):2057-2075, 2014.
Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In
Advances in neural information processing systems, pp. 685-693, 2015.
Yu Zhang and Dit-Yan Yeung. A convex formulation for learning task relationships in multi-task
learning. Uncertainty in Artificial Intelligence, pp. 733-442, 2010.
Peilin Zhao and Tong Zhang. Stochastic optimization with importance sampling for regularized
loss minimization. In Proceedings of the 32nd International Conference on Machine Learning,
PMLR, volume 37, pp. 1-9, 2015.
Y. Zhao, M. Li, L. Lai, N. Suda, D. Civin, and V. Chandra. Federated learning with non-iid data.
arXiv preprint arXiv:1806.00582, 2018.
11
Under review as a conference paper at ICLR 2021
Appendix
Federated Learning of a Mixture of Global and Local Models
Contents
1	Introduction	1
1.1	Federated learning ...................................................... 1
2	Contributions	2
3	New Formulation of FL	4
3.1	Technical preliminaries ................................................. 4
3.2	Characterization of optimal solutions ................................... 5
4	L2GD: Loopless Local GD	5
4.1	The dynamics of local GD and averaging steps ............................ 6
4.2	Convergence theory ...................................................... 6
5	Loopless Local SGD with Variance Reduction	7
5.1	Convergence theory ...................................................... 7
6	Experiments	8
A	Possible Extensions	14
B	Experimental Setup and Further	Experiments	14
B.1	Comparison of the methods	......................................... 14
B.2	Effect ofp ............................................................. 16
B.3	Effect of λ ............................................................ 16
C	Remaining Algorithms	19
C.1 Understanding communication of L2GD ..................................... 19
C.2 L2GD and full averaging ................................................. 19
C.3 Local GD with variance reduction ........................................ 19
C.4 L2SGD+: Algorithm and the efficient implementation ...................... 21
C.5 Local SGD with variance reduction - general method....................... 21
C.6 Local stochastic algorithms ............................................. 24
D Missing Lemmas and Proofs	26
D.1 Gradient and Hessian of ψ ............................................... 26
D.2 Proof of Theorem 3.1 .................................................... 27
D.3 Proof of Theorem 3.2 .................................................... 28
12
Under review as a conference paper at ICLR 2021
D.4 Proof of Theorem 3.3 ..................................................... 28
D.5 Proof of Theorem 4.2 ..................................................... 28
D.6 Proof of Lemma D.2 ....................................................... 29
D.7 Proof of Corollary 4.3 ................................................... 29
D.8 Proof of Corollary 5.2 ................................................... 30
D.9 Proof of Theorems 5.1, C.6, and C.7 ...................................... 30
D.9.1 GJS ............................................................... 30
D.9.2 Variance reduced local SGD as special case of GJS ................. 31
D.9.3 Proof of Theorem C.6 and Theorem C.7 .............................. 31
D.9.4 Proof of Theorem 5.1 .............................................. 33
13
Under review as a conference paper at ICLR 2021
A Possible Extensions
Our analysis of L2GD can be extended to cover smooth convex and non-convex loss functions fi
(we do not explore these directions). Further, our methods can be extended to a decentralized regime
where the devices correspond to devices of a connected network, and communication is allowed
along the edges of the graph only. This can be achieved by introducing an additional randomization
over the penalty ψ. Further, our approach can be accelerated in the sense of Nesterov (Nesterov,
2004) by adapting the a variant of Katyusha (Allen-Zhu, 2017; Qian et al., 2019a) to our setting,
thus further reducing the number of communication rounds.
B Experimental Setup and Further Experiments
In all experiments in this paper, We consider a simple binary classification model - logistic regres-
sion. In particular, suppose that device i owns data matrix Ai ∈ Rm×d along with corresponding
labels bi ∈ {-1, 1}m. The local objective for client i is then given as folloWs
m
fi (X) = m X fi,j (X)	+ 2 kxk2,	where	fim+j (X)	=	Iog(I +exP((Ai) j,：x ∙	bi))	.
j=1
The rows of data matrix A were normalized to have length 4 so that each fi0,j is 1-smooth for each
j. At the same time, the local objective on each device is 10-4 strongly convex. Next, datasets are
from LibSVM (Chang & Lin, 2011).
In each case, we consider the simplest locally stochastic algorithm. In particular, each dataset is
evenly split among the clients, while the local stochastic method samples a single data point each
iteration.
We have chosen a different number of clients for each dataset - so that we cover different possible
scenarios. See Table 1 for details (it also includes sizes of the datasets). Lastly, the stepsize was
always chosen according to Thm. 5.1.
Table 1: Setup for the experiments.
Dataset	N = nm	d	n	m	μ	L	P (Sec.B.1)	λ (Sec. B.2)	P (Sec.B.3)
a1a	1 605	—	123	5	321	10—4	1	0.1	0.1	0.1
mushrooms	8 124 —	112	12	677	10-4	1	0.1	0.05	0.3
phishing	11 055 -	68	11	1 005	10—4	1	0.1	0.1	0.001
madelon	2 000 —	500	50	40	10 — 4	1	0.1	0.02	0.05
duke	44	―	7 129	4	11	10 — 4	1	0.1	0.4	0.1
gisette-Scale	6 000 —	5 000	100	60	10-4	1	0.1	0.2	0.003
a8a	22 696 —	123	8	109	10-4	1	0.1	0.1	0.1
B.1 Comparison of the methods
In our first experiment, we verify two phenomena:
•	The effect of variance reduction on the convergence speed of local methods. We compare
3 different methods: local SGD with full variance reduction (Algorithm 3), shifted local
SGD (Algorithm 7) and local SGD (Algorithm 6). Our theory predicts that a fully vari-
ance reduced algorithm converges to the global optimum linearly, while both shifted local
SGD and local SGD converge to a neighborhood of the optimum. At the same time, the
neighborhood should be smaller for shifted local SGD.
•	The claim that heterogeneity of the data does not influence the convergence rate. We con-
sider two splits of the data heterogeneous and homogeneous. For the homogeneous split,
we first randomly reshuffle the data and then construct the local objectives according to the
current order (i.e., the first client owns the first m indices, etc.). For heterogeneous split,
we first sort the data based on the labels and then construct the local objectives accordingly
(thus achieving the worst-case heterogeneity). Note that the overall objective to solve is
14
Under review as a conference paper at ICLR 2021
different in homogeneous and heterogeneous case - We thus plot relative suboptimality of
the objective (i.e., F(X0)-F(X?)) to directly compare the convergence speed.
F(x )-F (x )
In each experiment, we choose P = 0.1 and λ = 1 - such choice mean that P is very close to
optimal. The other parameters (i.e., number of clients) are provided in Table 1. Fig. 4 presents the
result.
&二2 Eqdoqns∙,>c2-∙,≈
A4d-2Ecdoqns∙,>42-id≈
IO-⑶
Dataset： ala
→- L2SGD+
—j L2SGD2
f - L2SGD
•••»••• Hom L2SGD+
―一Hom L2SGD2
—Hom L2SGD
x4=2E=doqns∙,>=2-∙,α
Dataset： mushrooms
O 50	100	150	200	250	300	350	400
Data passes
Dataset： madelon
O 50 IOO 150	200	250	300	350	400
Data passes
Dataset: phishing
O 50 IOO 150	200	250	300	350	400
Data passes
111
Al=2 EAdOqnS>通2-8α

ιo^4
O 50 IOO 150	200	250	300	350	400
Data passes
Dataset： gisette_scale
Dataset： duke
&二2 Eqdoqns∙,>c2-8≈
IOT
O 50 IOO 150	200	250	300	350	400
Data passes
x4=2E=doqns∙,>≈2-8α
O 50 IOO 150	200	250	300	350	400
Data passes
Dataset： a8a
Data passes
x4=2Ecdoqns∙,>c2-8≈
IO-1-
IO-3 -
IO-5 -
10~7
10^9
10-IL
IO-13	.
O 50 IOO 150	200	250	300	350	400
Figure 4: Variance reduced local SGD (Algorithm 3), shifted local SGD (Algorithm 7) and local
SGD (Algorithm 6) applied on LibSVM problems for both homogeneous split of data and Hetero-
geneous split of the data. Stepsize for non-variance reduced method was chosen the same as for the
analogous variance reduced method.
As expected, Figure 4 clearly demonstrates the following:
15
Under review as a conference paper at ICLR 2021
•	Full variance reduction always converges to the global optima, methods with partial vari-
ance reduction only converge to a neighborhood of the optimum.
•	Partial variance reduction (i.e., shifting the local SGD) is better than not using control
variates at all. Although the improvement in the performance is rather negligible.
•	Data heterogeneity does not affect the convergence speed of the proposed methods. There-
fore, unlike standard local SGD, mixing the local and global models does not suffer the
problems with heterogeneity.
B.2	EFFECT OF p
In the second experiment, we study the effect of p on the convergence rate of variance reduced local
SGD. Note that P immediately influences the number of communication rounds - on average, the
clients take (p-1 - 1) local steps in between two consecutive rounds of communication (aggrega-
tion).
In Section 5, we argue that, it is optimal (in terms of the convergence rate) to choose p of order
p? := Lo+λ. Figure 5 compares P = p? against other values of P and confirms its optimality (in
terms of optimizing the convergence rate).
While the slower convergence of Algorithm 3 with P < P? is expected (i.e., communicating more
frequently yields a faster convergence), slower convergence for P > P? is rather surprising; in
fact, it means that communicating less frequently yields faster convergence. This effect takes place
due to the specific structure of problem (12); it would be lost when enforcing xι = •… = Xn
(corresponding to λ = ∞).
B.3	EFFECT OF λ
In this experiment we study how different values ofλ influence the convergence rate of Algorithm 3,
given that everything else (i.e., P) is fixed. Note that for each value ofλ we geta different instance of
problem (12); thus the optimal solution is different as well. Therefore, in order to make a fair com-
parison between convergence speeds, We plot the relative suboptimality (i.e., F(Xo)-F(X?)) against
the data passes. Figure 6 presents the results.
The complexity of Algorithm 3 is7 O QLp)*) log ε as soon as λ < λ? := (Lp); otherwise the
complexity is O (-λ ) log 1. This perfectly consistent with what Figure 6 shows - the choice λ < λ?
ɪ J	∖pμ√ tj ε	J
resulted in comparable convergence speed than λ = λ? ; while the choice λ > λ? yields noticeably
worse rate than λ = λ? .
7Given that μ is small.
16
Under review as a conference paper at ICLR 2021
Dataset: ala
O 25	50	75 IOO 125	150	175	200
Data passes
x4=2E=doqns∙,>≈2-8α
Dataset: mushrooms
O 25	50	75 IOO 125	150	175	200
Data passes
Dataset： phishing
0∙
Λ4=2Edoqns>2-8
O 25	50	75 IOO 125	150	175	200
Data passes
Dataset： gisette scale
O I
Ww-
Λ4=2Edoqns>2-8
O 25	50	75 IOO 125	150	175	200
Data passes
&二2 Ecdoqns∙,>c2-8≈
Figure 5: Effect of the aggregation probability p (legend of the plots) on the convergence rate of
Algorithm 3. Choice p = p? corresponds to red dotted line with triangle marker. Parameter λ was
chosen in each case as Table 1 indicates.
17
Under review as a conference paper at ICLR 2021
Dataset: mushrooms
O 25	50	75 IOO 125	150	175	200
Data passes
Dataset： madelon
x4=2E=doqns 8>=2-8α
Dataset: ala
O 25	50	75 IOO 125	150	175	200
Data passes
Dataset: phishing
&二2 Eqdoqns∙,>c2-8≈
0∙
x4=2Edoqns>2-8α
O 25	50	75 IOO 125	150	175	200
Data passes
Dataset： gisette_scale
ιoq
O 25	50	75 IOO 125	150	175	200
Data passes
Λ4=2Ecdoqns∙,>c2-8≈
x4=2E=doqns∙,>≈2-8α
Dataset： duke
Λ4=2Ecdoqns∙,>c2-8≈
O 25	50	75 IOO 125	150	175	200	O 25	50	75 IOO 125	150	175	200
Data passes	Data passes
&二2 Ecdoqns∙,>c2-8≈
Figure 6: Effect of parameter λ (legend of the plot) on the convergence rate of Algorithm 3. The
choice λ = λ? corresponds to borwn dash-dotted line with diamond marker (the third one from the
legend). Aggregation probability p was chosen in each case as Table 1 indicates.
18
Under review as a conference paper at ICLR 2021
C Remaining Algorithms
C.1 Understanding communication of L2GD
Example C.1 In order to better understand when communication takes place in Algorithm 1, con-
sider the following possible sequence of coin tosses: 0, 0, 1, 0, 1, 1, 1, 0. The first two coin tosses
lead to two local GD steps (8) on all devices. The third coin toss lands 1, at which point all local
models Xk are communicated to the master, averaged to form Xk, and the step (9) towards averag-
ing is taken. The fourth coin toss is 0, and at this point, the master communicates the updated local
models back to the devices, which subsequently perform a single local GD step (8). Then come three
consecutive coin tosses landing 1, which means that the local models are again communicated to
the master, which performs three averaging steps (9). Finally, the eighth coin toss lands 0, which
makes the master send the updated local models back to the devices, which subsequently perform a
single local GD step.
This example illustrates that communication needs to take place whenever two consecutive coin
tosses land a different value. If 0 is followed by a 1, all devices communicate to the master, and if 1
is followed by a 0, the master communicates back to the devices. It is standard to count each pair of
communications, Device→Master and the subsequent Master→Device, as a single communication
round.
Lemma C.2 The expected number of communication rounds in k iterations of L2GD is p(1 - p)k.
C.2 L2GD and full averaging
Is a setup such that conditions of Thm. 4.2 are satisfied and the aggregation update (9) is identical
to full averaging? This is equivalent requiring 0 < p < 1 such that αλ = np. However, we have
ɑλ ≤ 2λL ≤ np, which means that full averaging is not supported by our theory.
C.3 Local GD with variance reduction
In this section, we present variance reduced local gradient descent with partial aggregation. In partic-
ular, the proposed algorithm (Algorithm 2) incorporates control variates to Algorithm 1. Therefore,
the proposed method can be seen as a special case of Algorithm 3 with m = 1. We thus only present
it for pedagogical purposes, as it might shed additional insights into our approach.
In particular, the update rule of proposed method will be Xk+1 = Xk - αgk where
k	∫p-1(λVψ(xk) 一 n-1Ψk) + n-1 Jk + n-1Ψk	with probability P
g	(i (1 — P)T(Vf(Xk) — n-1Jk) + n-1Jk + n-1Ψk with probability 1 — p .
for some control variates vectors Jk, Ψk ∈ Rnd. A quick check gives
E gk | Xk = Vf(Xk) + λVψ(Xk) =VF(Xk),
thus the direction we are taking is unbiased regardless of the value of control variates Jk , Ψk. The
goal is to make control variates Jk, Ψk correlated8 with nVf (Xk) andnλVψ(Xk). One possible so-
lution to the problem is for Jk, Ψk to track most recently observed values of nVf (∙) and nλVψ(∙),
which corresponds to the following update rule
(ψk+ι jk+i) = [ Sλvψ(Xk), Jk) with probability p
1	,	(Ψk,nVf (xk))	with probability 1 — p .
A specific, distributed implementation of the described method is presented as Algorithm 2. The
only communication between the devices takes place when the average model Xk is being computed
(with probability p), which is analogous to standard local SGD. Therefore we aim to set p rather
small.
Note that Algorithm 2 is a particular special case of SAGA with importance sampling (Qian et al.,
2019b); thus, we obtain convergence rate of the method for free. We state it as Thm. C.3.
8Specifically We aim to have Corr Jk ,nVf (xk)] → 1 and Corr [n-1Ψk, λVψ(xk)] → 1 as Xk → x?.
19
Under review as a conference paper at ICLR 2021
Algorithm 2 Variance reduced local gradient descent
Input: x0 = •一 =Xn ∈ Rd, stepsize a, probability P
Ji = .一=Jn = ψi = ∙∙∙=ψn=0 ∈ Rd
for k = 0, 1, . . . do
ξ = 1 with probability p and 0 with probability 1 - p
if ξ then
All Devices i = 1, . . . , n:
Compute Vfi(Xk)
χk+1 = Xk - α (n-1(1 - P)TVfi(Xk) - n-11-pJk + n-1Ψk)
Set Jik+1 = Vfi(Xik),Ψik+1 = Ψik
else
Master computes the average Xk = ɪ Pn= 1 Xk
Master does for all i = 1, . . . , n:
Set Xk+1 = Xk — α (nλp (Xk — Xk) — (p-1 — 1)n-1Ψk + n-1Jk)
Set Ψk+1 = λ(Xi — Xk), Jk+1 = Jk
end if
end for
Theorem C.3 Let Assumption 3.1 hold. Set α = nmin
of Algorithm 2 is
(1-p)	P
4L+μ,4λ+μ
. Then, iteration complexity
max (μ4L⅛, 4T+μ) log 1.
Proof: Clearly,
F(x) = f(x) + λψ(X) = 2 2f (x) + 2λψ(∕) I .
l-{z"} 'V/
∖ = f(x)	=ψ(x))
Note that ψ is 2λ smooth and f is 2L smooth. At the same time, F is inn strongly convex. Using
convergence theorem of SAGA with importance sampling from (Qian et al., 2019b; Gazagnadou
et al., 2019), we get
E [F(Xk) + 2Y(Jk, Ψk)] ≤ (1 — αμ)k (F(x0) + 2Υ(J0, Ψ°)),
where
n
Y(Jk, Ψk)：=今 X (∣∣Ψk - λ(g(λ) — X(X))『+ Jk - Vfi(Xi(λ))∣∣2)
i=1
and α = n min (⅛-p), 4λ⅛), as desired.
Corollary C.4 Iteration complexity of Algorithm 2 is minimized for P = 4]：：：+2*, which
yields complexity 4 (楙 + L + 2)log ɪ. The communication complexity is minimized for any
p ≤ 4λ+4L+21, in which case the total number of communication rounds to reach ε-solution is
(4λ + Dlog 1.
As a direct consequence of Corollary C.4 we see that the optimal choice of P that minimizes both
communication and number of iterations to reach ε solution of problem (17) is P
4λ:μ
4λ+4L+2μ.
Remark C.5 While both Algorithm 2 and Algorithm 3 are a special case of SAGA, the practical
version of variance reduced local SGD (presented in Section C.5) is not. In particular, we wish
to run the SVRG-like method locally in order to avoid storing the full gradient table.9 Therefore,
9SAGA does not require storing a full gradient table for problems with linear models by memorizing the
residuals. However, in full generality, SVRG-like methods are preferable.
20
Under review as a conference paper at ICLR 2021
variance reduced local SGD that will be proposed in Section C.5 is neither a special case of SAGA
nor a special case of SVRG (or a variant of SVRG). However, it is still a special case of a more
general algorithmfrom (HanzeIy & Richtdrik, 2019).
As mentioned, Algorithm 3 is a generalization of Algorithm 2 when the local subproblem is a finite
sum. Note that Algorithm 2 constructs a control variates for both local subproblem and aggregation
function ψ and constructs corresponding unbiased gradient estimator. In contrast, Algorithm 3 con-
structs extra control variates within the local subproblem in order to reduce the variance of gradient
estimator coming from the local subsampling.
C.4 L2SGD+: Algorithm and the efficient implementation
Denote 1 ∈ Rm to be vector of ones. We are now ready to state L2SGD+ as Algorithm 3.
Algorithm 3 L2SGD+: Loopless Local SGD with Variance Reduction
Input: x0 = •一=Xnn ∈ Rd, stepsize a, probability P
Ji0 = 0 ∈ Rd×m, Ψi0 = 0 ∈ Rd (for i = 1,. . . ,n)
for k = 0, 1, . . . do
ξ = 1 with probability p and 0 with probability 1 - p
if ξ = 0 then
All Devices i = 1, . . . , n:
Sample j ∈ {1, . . . , m} (uniformly at random)
gk=n(i—)(▽九(Xk)-(Jk)：j)+Jm+萼
k+1 k k
Xi = Xi - αgi
Set (Jk+1)"j = Nfij(Xk), Ψk+1 = Ψk,
(Jik+1):,l = (Jik+1):,l for all l 6=j
else
Master computes the average Xk = n Pn=ι Xk
Master does for all i = 1, . . . , n:
gk = nλp (Xk -Xk) -p-- ψk + 焉 Jk 1
Set Xik+1 = Xik - αgik
Set Ψk+1 = λ(Xk — Xk ), Jk + 1 = Jk
end if
end for
L2SGD+ only communicates when a two consecutive coin tosses land a different value, thus, on
average p(1 - p)k times per k iterations. However, L2SGD+ requires communication of control
variates JJ Ψi as well - each communication round is thus three times more expensive. In the
Appendix, we provide an implementation of L2SGD+ that does not require the communication of
Ji1, Ψi.
Here we present an efficient implementation of L2SGD+ as Algorithm 4 so that we do not have to
communicate control variates. As a consequence, Algorithm 4 needs to communicate on average
p(1 - p)k times per k iterations, while each communication consists of sending only local models
to the master and back.
C.5 Local SGD WITH variance reduction — general method
In this section, we present a fully general variance reduced local SGD. We consider a more general
instance of (2) where each local objective includes a possibly nonsmooth regularizer, which admits
a cheap evaluation of proximal operator. In particular, the objective becomes
21
Under review as a conference paper at ICLR 2021
Algorithm 4 L2SGD+: Loopless Local SGD with Variance Reduction (communication-efficient
implementation)
Input: x1 = •一 =Xn = X ∈ Rd, stepsize a, probability P
Initialize control variates Ji0 = 0 ∈ Rd×m , Ψi0 = 0 ∈ Rd (for i =
ξ-1 = 0
for k = 0, 1, . . . do
ξ k = 1 with probability p and 0 with probability 1 - p
if ξk = 0 then
All Devices i = 1, . . . , n:
if ξk-1 = 1 then
Receive Xik, c from Master
Reconstruct Xk = Xk-C using Xk, xk-c, C
Set Xk= Xk -Canm Jk 1, Jk= Jk-C, Ψk = λ(Xk-c - Xk),
end if
1, . . . , n), initial coin toss
Sample j ∈ {1, . . . , m} (uniformly at random)
gk = n(1-p) (Vf∕,j(Xk) - (Jk):,j)
k+1 k k
Xi = Xi - αgi
+ Jki + Ψk
nm n
else
Set (Jk+1)"j = f (Xk), Ψk+1 = Ψk,
(Jik+1):,l = (Jik+1):,l for all l 6=j
Master does for all i = 1, . . . , n:
if ξk-1 = 0 then
Set C = 0
Receive Xk from Device and set X = n Pn= 1 Xk, Xk = Xk
end if
Set Xk+1 =Xk - a (np(Xk - X) - p-n-1 λ(χ - X))
Set X = Xk
Set C = C + 1
end if
end for
n
n
n	mi
min
x∈Rdn
Nf Efi,j(Xi) +λ蚩EkXi-Xk2+ERi(Xi),
(12)
i=1	j=1
-------{------}
=Nfi(X)
i=1
、-------{--------}
=ψ(x)
i=1
X----{-----}
:=R(x)
^^{^^~
=f(x)
^^{^^™
=F (x)
where mi is the number of data points owned by client i and N = Pin=1 mi .
In order to squeeze a faster convergence rate from minibatch samplings, we will assume that fi0,j is
smooth with respect to a matrix Mi,j (instead of scalar L0i,j = λmaxMi,j).
Assumption C.1 Suppose that fj is MijSmooth (Mi,j ∈ Rd×d, Mi,j * 0) and μ strongly convex
for 1 ≤ j ≤ mi, 1 ≤ i ≤ n, i.e.
fi,j(y)+(y fi,j(y),X — y〉≤ fi,j(X) ≤ fi,j(y)+3fi,j(y),X — y〉+2 ky -XkMi j, VX,y ∈ Rd.
,	(13)
Furthermore, assume that Ri is convex for 1 ≤ i ≤ n.
Our method (Algotihm 5) allows for arbitrary aggregation probability (same as Algorithms 2, 3),
arbitrary sampling of clients (to model the inactive clients) and arbitrary structure/sampling of the
local objectives (i.e., arbitrary size of local datasets, arbitrary smoothness structure of each local
22
Under review as a conference paper at ICLR 2021
objective and arbitrary subsampling strategy of each client). Moreover, it allows for the SVRG-like
update rule of local control variates Jk , which requires less storage given an efficient implementa-
tion.
To be specific, each device owns a distribution Di over subsets of mi . When the aggrega-
tion is not performed (with probability 1 - p), a subset of active devices S is selected (S fol-
lows arbitrary fixed distribution D). Each of the active clients (i ∈ S) samples a subset of lo-
Cal indices Si 〜Di and observe the corresponding part of local Jacobian Gi(Xk)(：s)(where
Gi(Xk) ：= [Vf0,ι (χk), Vf0,2(χk),... Vf.0,m. (χk)). When the aggregation is performed (with prob-
ability P) We evaluate Xk and distribute it to each device; using which each device computes a cor-
responding component of λVψ(Xk). Those are the key components in constructing the unbiased
gradient estimator (without control variates).
It remains to construct control variates and unbiased gradient estimator. If the aggregation is done,
we just simply replace the last column of the gradient table. If the aggregation is not done, we have
two options - either keep replacing the columns of the Jacobian table (in such case, we obtain a
particular case of SAGA (Defazio et al., 2014)) or do LSVRG-like replacement (Hofmann et al.,
2015; Kovalev et al., 2020) (in such case, the algorithm is a particular case of GJS (Hanzely &
Richtarik, 2019), but is not a special case of neither SAGA nor LSVRG. Note that LSVRG-Iike
replacement is preferrable in practice due to a better memory efficiency (one does not need to store
the whole gradient table) for the models other than linear.
In order to keep the gradient estimate unbiased, it will be convenient to define vector pi ∈ Rmi such
that for each j ∈ {1, . . . , mi} we have P (j ∈ Si) = pi,j.
Next, to give a tight rate for any given pair of smoothness structure and sampling strategy, we use a
standard tool first proposed for the analysis of randomized coordinate descent methods (Richtarik &
Takac, 2016; Qu & Richtarik, 2016) called Expected Separable OVeraPProximation (ESO) assump-
tion. ESO provides us with smoothness parameters of the objective which “account” for the given
sampling strategy.
Assumption C.2 Suppose that there is vi ∈ Rmi such for each client we haVe:
E ]χ M2j hi,j| j ≤ X Pi,jvi,j khi,j k2, V 1 ≤ i ≤ n, ∀hi,j ∈ Rmi,j ∈ {1,...,mi}.
(14)
Lastly, denote pi to be the probability that worker i is active and 1(mi) ∈ Rmi to be the vector of
ones. The resulting algorithm is stated as Algorithm 5.
Next, Theorems C.6 and C.7 present convergence rate of Algorithm 5 (SAGA and SVRG variant,
respectively).
Theorem C.6 Suppose that Assumptions C.1 and C.2 hold. Let
α = min	min	N(1-p)piμjPi, 4λn+
j∈{1,...,mi},1≤i≤n	4vj + Nn	+”
Then the iteration complexity of Algorithm 5 (SAGA option) is
max max
j∈{1,...,mi},1≤i≤n
n
4vj N +μ
μ(1-p)Pi,j Pi
Theorem C.7 Suppose that Assumptions C.1 and C.2 hold. Let
.ʃ
α = min
N (1-P)Pi
min	—k------μ^
j∈{1,...,mi},1≤i≤n 4Pij +NnP-
Pn
1 , 4λ+μ
Then the iteration complexity of Algorithm 5 (LSVRG option) is
max max
j∈{1,...,mi},1≤i≤n
n_ 1	-1
4vj N PiJ +μP
Piμ(I-P)
23
Under review as a conference paper at ICLR 2021
Algorithm 5 L2SGD++: Loopless Local SGD with Variance Reduction and Partial Participation
Input: x01 , . . . x0n ∈ Rd, # parallel units n, each of them owns mi data points (for 1 ≤ i ≤
n), distributions Dt over subsets of {1, . . . , mi}, distribution D over subsets of {1, 2, . . . n},
aggregation probability p, stepsize α
Ji0 = 0 ∈ Rd×mi, Ψi0 = 0 ∈ Rd (for i = 1,. . . ,n)
for k = 0, 1, . . . do
ξ = 1 with probability p and 0 with probability 1 - p
if ξ = 0 then
Sample S 〜D
All Devices i ∈ S :
Sample Si 〜Di； Si ⊆ {1,...,mi} (independently on each machine)
Observe ▽月 j (Xk) for all j ∈ Si
gk = N(I-P)pi (Pj∈SiP-j1 (Vfi'j(Xk) -(Jk):j)) + 寺Jk 1(mi) + n-1ψk
xik+1 = proxαRi(xik - αgik )
For all j ∈ {1, . . . , mi } set J:k,+j 1
(VfZj (Xk) if j ∈ Si
J:k,j	otherwise
(Vfi0,j (Xik); w. p. pi
J:k,j	otherwise
Set Ψik+1 = Ψik
All Devices i 6∈ S :
gk = N Jk 1(mi + n-1Ψk
Xik+1 = proxαRi(Xik - αgik )
Set Jik+1 = Jik,Ψik+1= Ψik
else
Master computes the average Xk = 1 pn=1 Xk
Master does for all i = 1, . . . , n:
gk = p-1λ(Xk — Xk) — (p-1 — 1)n-1Ψk + N Jk 1(mi)
Set Xk+1 = PrOXaRi(Xk - agk)
Set Ψk+1 = λ(Xk — Xk), Jk+1 = Jk
end if
if SAGA
if L — SVRG
end for
Remark C.8 Algotihm 2 is a special case of Algorithm 3 which is in turn special case of Algo-
rithm 5. Similarly, Theorem 2 is a special case of Theorem 5.1 which is again special case of
Theorem C.6.
C.6 Local stochastic algorithms
In this section, We present two more algorithms - Local SGD with partial variance reduction (Al-
gorithm 7) and Local SGD without variance reduction (Algorithm 6). While Algorithm 6 uses no
control variates at all (thus is essentially Algorithm 1 where local gradient descent steps are replaced
with local SGD steps), Algorithm 7 constructs control variates forψ only, resulting in locally drifted
SGD algorithm (with the constant drift between each consecutive rounds of communication). While
we do not present the convergence rates of the methods here, we shall notice they can be easily
obtained using the framework from (Gorbunov et al., 2020).
24
Under review as a conference paper at ICLR 2021
Algorithm 6 LooPless Local SGD (L2SGD)
Input: x0 = •一 =Xn ∈ Rd, stepsize a, probability P
for k = 0, 1, . . . do
ξ = 1 with probability p and 0 with probability 1 - p
if ξ = 0 then
All Devices i = 1, . . . , n:
Sample j ∈ {1, . . . , m} (uniformly at random)
gk = ⅛ Wfij(Xk))
xik+1 = xik - αgik
else
Master computes the average Xk = ɪ Pn=ι Xk
Master does for all i = 1, . . . , n:
gk = np(Xk -Xk)
Set Xik+1 = Xik - αgik
end if
end for
Algorithm 7 Loopless Local SGD with partial variance reduction (L2SGD2)
Input: x0 = ・一=Xn ∈ Rd, stepsize a, probability P
Ψi0 = 0 ∈ Rd (for i = 1, . . . ,n)
for k = 0, 1, . . . do
ξ = 1 with probability P and 0 with probability 1 - P
if ξ = 0 then
All Devices i = 1, . . . , n:
Sample j ∈ {1, . . . , m} (uniformly at random)
gk = ⅛ Wfij(Xk)) + nψk
Xik+1 = Xik - αgik
Set Ψik+1 = Ψik
else
Master computes the average Xk = ɪ Pn=ι Xk
Master does for all i = 1, . . . , n:
gk = 2 (Xk -Xk) -，Ψk
Set Xik+1 = Xik - αgik
Set Ψk+1 = λ(Xk - Xk)
end if
end for
25
Under review as a conference paper at ICLR 2021
D Missing Lemmas and Proofs
D.1 Gradient and Hessian of ψ
Lemma D.1 Let I be the d × d identity matrix and In be n × n identity matrix. Then, we have
V2ψ(x) = 1 (In - 1 eeτ) 0 I	and
X	∕x∖∖
.
.
.
x
Vψ(x) = 1 X — X
X
∖ W/
Furthermore, Lψ = 1
Proof:
Let O the d × d zero matrix and let
Qi := [O,..., O, I, O,..., O] ∈
X---------------/ X---------/
Rd× dn
^{z
i-1
{z^
n-i
and Q := [I,..., I] ∈ Rd×dn. Note that Xi = QiX, and X = 1 Qx. So,
nn
≠(x) = 2n XuQiX-1 qxII2 = 2n XMQi-1 Q) x I I 2.
i=1	i=1
The Hessian of ψ is
v2ψ(x) = 1 X (Qi - nQ)τ (Qi - 1Q)
i=1
n
=1 X(QTQi- 1QTQ - 1 QτQi + n⅛ QτQ)
i=1
nn
1 X QTQi-1X1QT Q
i=1	i=1
nn
-1X1 QTQi + 1X n2 QTQ
n
1X QTQi- n⅛QTQ
i=1
and by plugging in for Q and Qi, we get
八1*i
n
-11
-11
n
_1
(1 :1)
—
n
-1I
1- nl-
(1
1- nl- nl- n
(1 - 1) i ∙ ∙ ∙
∖ T i
八 1 -11)
1
∖
—
	
1)1
乳I
∖ -n -n -n …	(1 - n)/
=n (In- n eeτ)乳 i∙
Notice that In - n eeτ is a circulant matrix, with eigenvalues 1 (multiplicity n - 1) and 0 (multi-
plicity 1). Since the eigenvalues of a Kronecker product of two matrices are the products of pairs of
eigenvalues of the these matrices, we have
λmax(V2ψ(x)) = λmax (1 (In - 1 ee>)脸 1) = 1 λmax (In - 1 ee>) = 1 ∙
26
Under review as a conference paper at ICLR 2021
So, l^ = n.
The gradient of ψ is given by
n
Vψ(x)
X (Qi - n Q)T(Qi- n Q)
i=1
1
n
x
n
1
n
E (QirQi - 1 QirQ - 1 QtQi + =QtQ)
x
i=1
i=1
i=1
D.2 PROOF OF THEOREM 3.1
For any λ, θ ≥ 0 we have
∕0∖
∕0∖
∕xi/n∖
∕x∕n∖
Xi
i=1
/(x(X)) + A≠(x(A)) ≤
f (x(Θ))+ θψ(x(θ)) ≤
By adding inequalities (15) and (16), we get
Xi/n
Xi/n
Xi/n
∖Xi/n)
i=1
X/n
X/n
X/n
\X/n，
/Xi/n\
/X/n\
i/n
i/n
i/n
i/n
i=1
f (x(Θ)) + λψ(x(θ))
f (x(λ)) + θψ(x(λ)).
x/n
x/n
x/n
x/n
(15)
(16)
(θ - λ)(ψ(x(λ)) - ψ(x(θ))) ≥ 0,
which means that ψ(x(λ)) is decreasing in λ. Assume λ ≥ θ. From the (16) we get
f (x(λ)) ≥ f (x(θ)) + θ(ψ(x(θ)) - ψ(x(λ))) ≥ f (x(θ)),
where the last inequality follows since θ ≥ 0 and since ψ(x(θ)) ≥ ψ(x(λ)). So, f (x(λ)) is
increasing.
1
n
1
n
1
n
1
n
n
Σ
/
n
Σ
∖
/
∖
/
∖
-
-
0
0
0
0
0
i
0
0
x
x
x
x
x
x
x
x
—
—
—
0
X
0
0
n
Σ
x
x
x
x
—
0
0
x
0
0
+
n
-
x
x
x
x
+
+
n
Σ
27
Under review as a conference paper at ICLR 2021
Notice that since ψ is a non-negative function and since x(λ) minimizes F and ψ(x(∞)) = 0, we
have
f (x(0)) ≤ f (x(λ)) ≤ f (x(λ)) + λψ(x(λ)) ≤ f (x(∞)),
which implies (3) and (4).
D.3 Proof of Theorem 3.2
The equation VF(χ(λ)) = 0 can be equivalently written as
Vfi(xi(λ)) + λ(g(λ) - X(λ)) = 0, i = 1, 2,...,n,
which is identical to (5). Averaging these identities over i, we get
n
X(X) = X(X)- 1 n X Vfi(XQ)),
i=1
which implies
n
XVfi(Xi(X)) =0.
i=1
Further, we have
nn
ψ(x(λ)) = * X kXi(λ) - X(λ)k2 = 2nλ2 X kVfi(Xi(λ))k2 = 2λ2 kVf (x(λ))k2,
i=1	i=1
as desired.
D.4 Proof of Theorem 3.3
First, observe that
2 2
IIVP(X(X))II2= 1 X Vfi(X(λ))	= 1 X Vfi(x(λ)) - 1 XVfi(Xi(λ)),
n	nn
i	ii
where the second identity is due to Theorem 3.2 which Says that 1 Pi Vfi(Xi(λ)) = 0. By applying
Jensen’s inequality and Lipschitz continuity of functions fi , we get
1	L2
IIVP(X(X))II2 ≤ — £ IIVfi(X(X))-Vfi(Xi(X))II2 ≤ 一 ]T∣∣X(λ) - Xi(X)II2 = 2L2ψ(X(λ)).
nn
ii
It remains to apply (3) and notice that P is strongly convex and thus X(∞) is indeed the unique
minimizer.
D.5 Proof of Theorem 4.2
We first show that our gradient estimator G(X) satisfies the expected smoothness property (Gower
et al., 2018; 2019).
LemmaD.2 LetL := 1 max {I-Lp, p} andσ2 :=± Pi=I (±l∣Vfi(Xi(X))∣∣2 + λ2IlXi(X)- X(X)Il2) ∙
Then for all X ∈ Rd we have the inequalities E kG(X) - G(X(X))k2 ≤ 2L (F (X) - F (X(X)))
and
E hIG(X)I2i ≤ 4L(F (X) - F (X(X))) + 2σ2.
Next, Theorem 4.2 from Lemma D.2 by applying Theorem 3.1 from (Gower et al., 2019).
28
Under review as a conference paper at ICLR 2021
D.6 Proof of Lemma D.2
We first have
E kG(X) - G(X(λ))k2
(1 - P) ∣∣ 普-∣∣2+P ∣∣λ
▽s(x)- λyψ(χ(λ))∣∣2
τ⅛ kVf(x) - Vf(x(λ))k2 + λ2 kVψ(x) - Vψ(x(λ))k2
≤	2LpDf(χ,χ(λ)) 十
=n(2-p) Df(X, x(λ))
2λ2L
ψDψ(X,X(λ))
Since Df + λDψ = DF and VF(χ(λ)) = 0, We can continue:
e[∣∣G(x)- G(x(λ))k[	≤ 2 max
2 max
n 1-Lp,λ o Df (X,x(X))
n七,PO (F(X)- F(x(λ))).
Next, note that
σ2
n
n12 X (± llVfi(xi(X))k2 + λp- kxi(λ) - x(λ)k2
i=1
1⅛ kVf(χ(λ))k2 + λ2 kVψ(χ(λ))k2
(1 - P)∣∣ ▽f≡i∣2 + P∣∣ λ▽ψx≡∣∣2
EhkG(X(λ))k2i.
Therefore, We have
E kG(X)k2
E kG(X) - G(X(λ))k2 +2E kG(X(λ))k2
as desired.
Lemma D.2+(17)
≤
4L(F (x) - F(x(λ))) + 2σ2,
D.7 Proof of Corollary 4.3
Firstly, to minimize the total number of iterations, it suffices to minimize L Which is achieved With
p? = -ɪ-. Let Us look at the communication. Fix ε > 0, choose α = ɪ and let k = 2nL log1, so
L +1^∖	2 LL	μL	ε
that
(1 - 2μL『≤ ε
The expected number of communications to achieve this goal is equal to
Commp
p(1 - p)k
≤
p
p
p
2maχ ---,λ]
p(i - P)----1μ p p j log ε
2 max{pL,(1-p)λ}
log ε.
μ
The quantity Commp is minimized by choosing any P such that PL = (1 一p)λ, i.e., for P = λ+_
p? , as desired. The optimal expected number of communications is therefore equal to
Commp* = λ2⅛⅜ log 1.
29
Under review as a conference paper at ICLR 2021
D.8 Proof of Corollary 5.2
Firstly, to minimize the total number of iterations, it suffices to solve
which is achieved with p = p?
i	n 4L0+μm 4λ+μ
min max ≤	—×—,------
(I-P)μ , pμ
____4λ+μ_____
4L0+4λ+(m+1)μ .
The expected number of communications to reach ε-solution is
Commp
p(1 - p) max
4L0+μm
(i-p)μ
log ε.
max{p(4L0+μm),(1-p)(4λ+μ)}
μ
Minimizing the above in p yield p = p?
number of communications is therefore equal to
_____4λ+μ____
4L0+4λ+(m+1)μ
, as desired. The optimal expected
Commp? = 4L0+44+(m+i)μ (4 L+m log ε.
D.9 Proof of Theorems 5.1, C.6, and C.7
Note first that Algorithm 3 is a special case of Algorithm 5, and Theorem 5.1 immediately follows
from Theorem C.6. Therefore it suffices to show Theorems C.6, and C.7. In order to do so, we will
cast Algorithm 5 as a special case of GJS from (Hanzely & Richtarik, 2019). As a consequence,
Theorem C.6 will be a special cases of Theorem 5.2 from (Hanzely & Richtarik, 2019).
D.9.1 GJS
In this section, We quickly summarize results from (Hanzely & Richtarik, 2019), which We cast
to sho convergence rate of Algorithm 3. GJS (Hanzely & Richtarik, 2019) is a method to solve
regularized empirical risk minimization objective, i.e.,
n
x∈∈Rd n X fj(X)+R(X).
x∈	j=1
(17)
Defining G(x) := [Vfι(χ),..., Vfn(χ)], we observe SG(x),UG(X) every iteration where S is
random linear projection operator and U is random linear operator which is identity on expectation.
Based on this random gradient information, GJS (Algorithm 8) constructs variance reduced gradient
estimator g and takes a proximal step in that direction.
Algorithm 8 Generalized JacSketch (GJS) (Hanzely & Richtarik, 2019)
1:	Parameters: Stepsize α > 0, random projector S and unbiased sketch U
2:	Initialization: Choose solution estimate X0 ∈ Rd and Jacobian estimate J0 ∈ Rd×n
3:	for k = 0, 1, . . . do
4:	Sample realizations of S and U, and perform sketches SG(Xk) andUG(Xk)
5:	Jk+1	= Jk - S(Jk - G(Xk))	update the Jacobian estimate
6:	gk =	1 Jke + 1U (G(Xk) 一 Jk)	e	construct the gradient estimator
7:	Xk+1	= proxαR(Xk - αgk)	perform the proximal SGD step
8:	end for
Next we quickly summarize theory of GJS.
Assumption D.1 Problem (17) has a unique minimizer x?, and f is μ-quasi strongly convex, i.e.,
f(x?) ≥ f(y) + hVf(y),x? - y + 2 ky - x?k2, ∀y ∈ Rd,	(18)
Functions fj are convex and Mj -smooth for some Mj	0, i.e.,
fj⑻ + hvfj(y),x 一 yi ≤ fj(X) ≤ fj⑻ + hvfj(y),x - yi + 2 ky 一 XkMj∙,	∀x,y ∈ Rd. (19)
30
Under review as a conference paper at ICLR 2021
Theorem D.3 (Slight simplification of Theorem 5.2 from (Hanzely & Richtarik, 2019)) LetAs-
sumption D.1 hold. Define M(X) := [M1X:,1, . . . , MnX:,n] Let B be any linear operator com-
1	'	'
muting with S, and assume Mt2 commutes with S. Define the Lyapunov function
Ψk := ∣∣xk - x?||2 + α BMt 1 (Jk - G(x*))	,	(20)
where {xk} and {Jk} are the random iterates produced by Algorithm 8 with stepsize α > 0. Suppose
that α and B are chosen so that
2aE [kUXe『]+ (I- E [S])2 BMt2X	≤ (1 - αμ) BMt 2X	(21)
and
「	-1	1	1	2	1	2
ME [∣∣UXek2] + (E [S])2 BMt2X	≤ 1 Mt2X .	(22)
for all X ∈ Rd×n. Thenfor all k ≥ 0, we have E [Ψk] ≤ (1 一 αμ)k Ψ0.
D.9.2 Variance reduced local SGD as special case of GJS
Let Ω(i,j) := j + Pi-1 mi In order to case problem (12) as a special case of 17, denote n := N +1,
f Ω(i,j) (x) ：= NN+2 fi,j(Xi) and fn ：= (N + 1)ψ. Therefore the objective (12) becomes
n
min Y(x) := n X f j (x) + R(X).	(23)
x∈RNd
j=1
Let V ∈ R T be such that Ω(i,j) = NN+2vi,j and as a consequence of (14) We have
E 1|x Mijhi,j|| ] ≤ XPi,j Ω(i,j) l∣hi,jk2 , V 1 ≤ i ≤ n, Vhij ∈ Rd, j ∈ {1,... ,mi}.
(24)
At the same time, Y is μ := n strongly convex.
D.9.3 Proof of Theorem C.6 and Theorem C.7
Let e ∈ Rd be a vector of ones and pi ∈ RN is such that pij = pi,j if j ∈ {1, . . . , mi}, otherWise
pij = 0. Given the notation, random operator U is chosen as
U X =((I- P)T Pn=I (P-Ie ((Pi)T)>)。(X:mi (Pj∈Siejej>)) w.p. (I- P)
[p-1X：,n	W.p. p
We next give two options on how to update Jacobian - first one is SAGA-like, second one is SVRG
like.
SAGA-like:	(SX):,mi = (SX):,n =	0X:,Si 0X:,n	= X:mi Pj∈Si ejej> ,			w.p. w.p.	(1 - P)Pi , (1 - P)(1 - Pi ) + P	
			w.p. w.p.	P 1-P				
SVRG-like:	(SX):,mi =	(X :mi	bi ; bi	=	1	w.p. Pi 0	w.p.	1 - Pi	w.p.	(1 - P)Pi	
		1。				w.p.	(1 - P)(1	- Pi ) + P
	(SX):,n =	X:,n	w.p.	P 1				
		0	w.p.		-P.			
31
Under review as a conference paper at ICLR 2021
We can now proceed with the proof of Theorem C.6 and Theorem C.7. As Nfi(X) - Vfi(y) ∈
Range (Mi), we must have
G(Xk) - G(x?) = MtM (G(Xk) - G(x*))	(25)
and
Jk - G(x?) = MtM (Jk - G(x?)) .	(26)
1
Due to (26), (25), inequalities (21) and (22) with choice Y = Mt 2 X become respectively:
22PTkMIY：,nk2 + 写(1-P)T XXE L1 X P-j1Mi2jY：j
i=1	j∈Si
≤ (1 - αμ)kB(Y)k2
1	2
+ (I- E [S ]) 2 B(Y)
(27)
⅛PTkMIY：,2k2+岑(1-P)T XXE 11 X P-1M⅛Y：j	+
i=1	j∈Si
1	2
(E[S])2 B(Y)	≤ 1 kYk2
(28)
Above, we have used
EkU Xek2 = E kUM1 Ye∣∣2 =PTkM! Y：,2|2+(1-P)T XX E ^p-1 X p-^ Yjl
Note that E [S(X)] = X ∙ Diag((I - P)(P ◦ p),p) where P ∈ Rn-1 such that PΩ(i,j) = Pi,j.
Using (24), setting B to be right multiplication with Diag(b) and noticing that λmaxMn = nλ it
suffices to have
22αp 1λ + (I - P)bn ≤ (1 - αμ)b2
2n2(1 - P)TP-j⅛-1VΩ(i,j) + (1 - (1 - P)Pi,j Pi)bj ≤ (1 - αμ)b2	∀j ∈ {1,... ,mi},i ≤ n
2αP-j + Pb ≤ 1
2α (1 - P)TP-j1P-1vΩ(i,j) + (1 - P)PijPibj ≤ n	∀j ∈ {1, ...,mi},i ≤ n
for SAGA case and
2aP 1λ + (1 - P)bn ≤ (1 - αμ)bn
nα (1 - P)TP-j1P-1vΩ(i,j) + (1 - (1 - P)PiPi )b2 ≤ (1 - αμ)bj	∀j ∈ {1, ...,mi},i ≤ n
2αP-1λ + Pb ≤ 1
n2 (1 - P)TP-jtP-1VΩ(i,j) + (1 - P)PiPi b2 ≤ n	∀j ∈ {1,...,mi},i ≤ n
for LSVRG case.
It remains to notice that to satisfy the SAGA case, it suffices to set b2 =六,b[,力=2 (i-p)p∙ .p∙
(for j ∈ {1,...,mi},i ≤ n and α = min{minj∈{1,..,mi},1≤i≤n 4(1-j+jpi , 4λp- }∙
To satisfy LSVRG case, it remains to set b2 = 七,bΩ(i,j) = 2 (1-P)pipi (for j ∈{1,..., mi}, i ≤
n) and α = min minj∈{1,…,mi},1≤i≤n VΩ(ij)
Pij
The last step to establish is to recall that n = N + 1, 口⑥力= NN1 vi,j and μ = n and note that
the iteration complexity is O- log 1 = αμ log 1.
32
Under review as a conference paper at ICLR 2021
D.9.4 Proof of Theorem 5.1
To obtain convergence rate of Theorem 5.1, it remains to use Theorem C.6 with pi = 1, mi = m
(∀i ≤ n), where each machine samples (when the aggregation is not performed) individual data
points with probability ml and thus Pj = ∖ (for all j ≤ N). The last remaining thing is to realize
that vj = L0 for all j ≤ N .
33