Under review as a conference paper at ICLR 2021
A Unified View on Graph Neural Networks as
Graph Signal Denoising
Anonymous authors
Paper under double-blind review
Ab stract
Graph Neural Networks (GNNs) have risen to prominence in learning representa-
tions for graph structured data. A single GNN layer typically consists of a feature
transformation and a feature aggregation operation. The former normally uses
feed-forward networks to transform features, while the latter aggregates the trans-
formed features over the graph. Numerous recent works have proposed GNN
models with different designs in the aggregation operation. In this work, we es-
tablish mathematically that the aggregation processes in a group of representative
GNN models including GCN, GAT, PPNP, and APPNP can be regarded as (ap-
proximately) solving a graph denoising problem with a smoothness assumption.
Such a unified view across GNNs not only provides a new perspective to under-
stand a variety of aggregation operations but also enables us to develop a unified
graph neural network framework Ugnn. To demonstrate its promising potential,
we instantiate a novel GNN model, Ada-Ugnn, derived from Ugnn, to handle
graphs with adaptive smoothness across nodes. Comprehensive experiments show
the effectiveness of Ada-Ugnn.
1	Introduction
Graph Neural Networks (GNNs) have shown great capacity in learning representations for graph-
structured data and thus have facilitated many down-stream tasks such as node classification (Kipf
& Welling, 2016; VelickoVic et al., 2017; Ying et al., 2018a; KlicPera et al., 2018) and graph clas-
sification (Defferrard et al., 2016; Ying et al., 2018b). As traditional deep learning models, a GNN
model is usually composed of seVeral stacking GNN layers. GiVen a graph G with N nodes, a GNN
layer typically contains a feature transformation and a feature aggregation operation as:
Feature Transformation: X0in = ftrans (Xin); Feature Aggregation: Xout = fagg(X0in; G) (1)
where Xin ∈ RN ×din and Xout ∈ RN ×dout denote the input and output features of the GNN layer
with din and dout as the corresponding dimensions, respectiVely. Note that the non-linear actiVation
is not included in Eq. (1) to ease the discussion. The feature transformation operation ftrans(∙)
transforms the input of Xin to X0in ∈ RN ×dout as its output; and the feature aggregation operation
fagg(•; G) updates the node features by aggregating the transformed node features via the graph G.
In general, different GNN models share similar feature transformations (often, a single feed-forward
layer), while adopting different designs for aggregation operation. We raise a natural question 一 is
there an intrinsic connection among these feature aggregation operations and their assumptions?
The significance of a positive answer to this question is two-fold. Firstly, it offers anew perspective
to create a uniform understanding on representative aggregation operations. Secondly, it enables
us to develop a general GNN framework that not only provides a unified view on multiple existing
representative GNN models, but also has the potential to inspire new ones. In this paper, we aim to
build the connection among feature aggregation operations of representative GNN models including
GCN (Kipf & Welling, 2016), GAT (Velickovic et al., 2017), PPNP and APPNP (Klicpera et al.,
2018). In particular, we mathematically establish that the aggregation operations in these models
can be unified as the process of exactly, and sometimes approximately, addressing a graph signal
denoising problem with Laplacian regularization (Shuman et al., 2013). This connection suggests
that these aggregation operations share a unified goal: to ensure feature smoothness of connected
nodes. With this understanding, we propose a general GNN framework, Ugnn, which not only
provides a straightforward, unified view for many existing aggregation operations, but also suggests
various promising directions to build new aggregation operations suitable for distinct applications.
To demonstrate its potential, we build an instance of Ugnn called Ada-Ugnn, which is suited for
handling varying smoothness properties across nodes, and conduct experiments to show its effec-
tiveness.
1
Under review as a conference paper at ICLR 2021
2	Representative Graph Neural Networks
In this section, we introduce notations for graphs and briefly summarize several representative GNN
models. A graph can be denoted as G = {V, E}, where V and E are its corresponding node and
edge sets. The connections in G can be represented as an adjacency matrix A ∈ RN ×N, with N the
number of nodes in the graph. The Laplacian matrix of the graph G is denoted as L. It is defined as
L = D - A, where D is a diagonal degree matrix corresponding to A. There are also normalized
versions of the Laplacian matrix such as L = I - D- 1 AD-2 or L = I - D-1A. In this work,
we sometimes adopt different Laplacians to establish connections between different GNNs and the
graph denoising problem, clarifying in the text. In this section, we generally use Xin ∈ RN ×din
and Xout ∈ RN ×dout to denote input and output features of GNN layers. Next, we describe a few
representative GNN models.
2.1	Graph Convolutional Networks (GCN)
Following Eq. (1), a single layer in GCN (Kipf & Welling, 2016) can be written as follows:
Feature Transformation: X0in = XinW; Feature Aggregation: Xout = AXin,	(2)
where W ∈ Rdin×dout is a feature transformation matrix, and A is a normalized adjacency matrix
which includes a self-loop, defined as follows:
11
A = ID 2 AID 2, with A = A + I and D = diag(ɪʃ Aι,j,∙∙∙, ʌʃ AN,j).	(3)
In practice, multiple GCN layers can be stacked, where each layer takes the output of its previous
layer as input. Non-linear activation functions are included between consecutive layers.
2.2	Graph Attention Networks (GAT)
Graph Attention Networks (GAT) adopts the same feature transformation operation as GCN in
Eq. (2). The feature aggregation operation (written node-wise) for a node i is as:
Xout [i, :] =	αijX0in [j, :],
j∈N(i)
with
exp(eij)
P exp(eik).
k∈N(i)
(4)
where N (i) = N(i) ∪ {i} denotes the neighbors (self-inclusive) of node i, and Xout[i, :] is the i-th
row of the matrix Xout, i.e. the output node features of node i. In this aggregation operation, αij
is a learnable attention score to differentiate the importance of distinct nodes in the neighborhood.
Specifically, αij is a normalized form of eij, which is modeled as:
eij = LeakyReLU ([Xin[i, :]kXin[j, ：]] a)	(5)
where [∙∣∣∙] denotes the concatenation operation and a ∈ R2d is a learnable vector. Similar to GCN,
a GAT model usually consists of multiple stacked GAT layers.
2.3	Personalized Propagation of Neural Predictions (PPNP)
Personalized Propagation of Neural Predictions (PPNP) (Klicpera et al., 2018) introduces an aggre-
gation operation based on Personalized PageRank (PPR). Specifically, the PPR matrix is defined as
α(I - (1 - α)A)-1, where α ∈ (0, 1) is a hyper-parameter. The ij-th element of the PPR matrix
specifies the influence of node i on node j . The feature transformation operation is modeled as
Multi-layer Perception (MLP). The PPNP model can be written in the form of Eq. (1) as follows:
Feature Transformation: X0in = MLP(Xin);
Feature Aggregation: Xout = α(I — (1 — α)A)-1Xin.	(6)
Unlike GCN and GAT, PPNP only consists of a single feature aggregation layer, but with a poten-
tially deep feature transformation. Since the matrix inverse in Eq. (6) is costly, Klicpera et al. (2018)
also introduces a practical, approximated version of PPNP, called APPNP, where the aggregation
operation is performed in an iterative way as:
XoU)t = (1 — α)A XoU-I) + αXin k = 1,...K,	(7)
where X(o0u)t = X0in and X(oKut) is the output of the feature aggregation operation. As proved
in Klicpera et al. (2018), X(oKut) converges to the solution obtained by PPNP, i.e., Xout in Eq. (6).
3	GNNs as graph signal denoising
In this section, we aim to establish the connections between the introduced GNN models and a graph
signal denoising problem with Laplacian regularization. We first introduce the problem.
2
Under review as a conference paper at ICLR 2021
Problem 1 (Graph Signal Denoising with Laplacian Regularization). Suppose that we are given
a noisy signal X ∈ RN×d on a graph G. The goal of the problem is to recover a clean signal
F ∈ RN×d, assumed to be smooth over G, by solving the following optimization problem:
arg min L = ∣∣F — XkF + C ∙ tr(F>LF),	(8)
F
Note that the first term guides F to be close to X, while the second term tr(F>LF) is the Laplacian
regularization that guides the smoothness of F over the graph. c > 0 is a balancing constant.
Assuming we adopt the unnormalized version of Laplacian matrix with L = D - A (the adjacency
matrix A is assumed to be binary), the second term in Eq. (8) can be written in an edge-centric way
or a node-centric way as:
edge-centric: c X kF [i,:] - F [j, ：]k2 ； node-centric： 2Cχ X kF [i, ：] - F [j, ：]k2.	⑼
(i,j)∈E	i∈V j∈N(i)
Clearly, from the edge-centric view, the regularization term measures the global smoothness of F,
which is small when connected nodes share similar features. On the other hand, we can view the term
Pj∈N(i) l∣F [i, ：] — F [j, ：] k2 as a local smoothness measure for node i as it measures the difference
between node i and all its neighbors. The regularization term can then be regarded as a summation
of local smoothness over all nodes. Note that the adjacency matrix A is assumed to be binary when
deriving Eq. (9). Similar formulations can also be derived to other types of Laplacian matrices.
In the following subsections, we demonstrate the connections between aggregation operations in
various GNN models and the graph signal denoising problem.
3.1	Connection to PPNP and APPNP
In this subsection, we establish the connection between the graph signal denoising problem (8) and
the aggregation propagations in PPNP and APPNP in Theorem 1 and Theorem 2, respectively.
Theorem 1.	When we adopt the normalized Laplacian matrix L = I -A, with A defined in Eq. (3),
the feature aggregation operation in PPNP (Eq. (6)) can be regarded as exactly solving the graph
signal denoising problem (8) with Xin as the input noisy signal and C = 1 - L
Proof. Note that the objective in Eq. (8) is convex. Hence, its closed-form solution F* to exactly
solve the graph signal denosing problem can be obtained by setting its derivative to 0 as：
∂L
—=2(F — X) + 2cLF = 0 ⇒ F* = (I + CL)TX
zɔ ∙	I--TZ TΛ⅛	1	C	1 , 1
Given L = I - A, F* can be reformulated as：
F* = (I + CL)TX = (I + C (I-A ))-1 x 1 (I-τ⅛ A )-1 X
(10)
(11)
The feature aggregation operation in Eq. (6) is equivalent to the closed-form solution in Eq. (11)
when we set α = 1/(1 + C) and X = X0in. This completes the proof.
□
Theorem 2.	When we adopt the normalized Laplacian matrix L = I - A, the feature aggrega-
tion operation in APPNP (Eq. (7)) approximately solves the graph signal denoising problem (8) by
iterative gradient descent with Xin as the input noisy signal, C = 1 - 1 and stepsize b = -2+^.
Proof. To solve the graph signal denoising problem (8), we take iterative gradient method with the
stepsize b. Specifically, the k-th step gradient descent on problem (8) is as follows：
∂L
F(k) - F(k-1) - b ∙ --(F = F(k-1)) = (1 - 2b - 2bC)F(k-1) + 2bX + 2bCAF(k-1)	(12)
∂F
where F(O) = X. When We set the stepsize b as /2C, We have the following iterative steps:
F(k) <———X +	AF(k-1),k =1,...K,	(13)
1+C 1+C
which is equivalent to the iterative aggregation operation of the APPNP model in Eq. (7) with
X = Xin and α =*.This completes the proof.	口
These two connections provide a new explanation on the hyper-parameter α in PPNP and APPNP
from the graph signal denoising perspective. Specifically, a smaller α indicates a larger C, which
means the obtained Xout is enforced to be smoother over the graph.
3.2 Connection to GCN
We draw the connection between the GCN model (Kipf & Welling, 2016) and the graph signal
denoising problem in Theorem 3.
3
Under review as a conference paper at ICLR 2021
Theorem 3.	When we adopt the normalized Laplacian matrix L = I - A, the feature aggregation
operation in GCN Eq. (2) can be regarded as solving the graph signal denoising problem (8) using
one-step gradient descent With Xin as the input noisy signal and stepsize b = 21c.
Proof. The gradient with respect to F at X is ∂F IF=X = 2cLX. Hence, one-step gradient descent
for the graph signal denoising problem (8) can be described as:
F-X-嘿
∂F
. . ~
=X — 2bcLX =(1 — 2bc)X + 2bcA X.
F=X
(14)
When stepsize b = 2c and X = Xin, We have F J AXin, which is the same as the aggregation
operation of GCN.	□
With this connection, it is easy to verify that a GCN model with multiple GCN layers can be re-
garded as solving the graph signal denoising problem multiple times with different noisy signals.
Specifically, each layer of a GCN model corresponds to a graph signal denoising problem, where
the input noisy signal is the output from the previous layer after the feature transformation of the cur-
rent layer. Note that there are earlier works (NT & Maehara, 2019; Zhao & Akoglu, 2019) drawing
connection between GCN and the optimization problem in Eq. (8), where the aggregation operation
in GCN is shown to be the first-order approximation of the exact solution.
3.3 Connection to GAT
To establish the connection between graph signal denoising and GAT (VeliCkoViC et al., 2017), in this
subsection, we adopt an unnormalized version of the Laplacian. It is defined based on the adjacency
matrix with self-loop A, i.e. L = D - A with D denoting the diagonal degree matrix of A. Then,
the denoising problem in Eq. (8) can be rewritten from a node-centric view as:
argmFn L = X kF[i,:] - X[i, ：]k2 + ∣ X C ∙ X kF[i,:] - F[j,:脏,	(15)
i∈V	i∈V	j∈N(i)
where N (i) = N(i) ∪ {i} denotes the neighbors (self-inclusive) of node i. In Eq. (15), the constant
c is shared by all nodes, which indicates that the same level of local smoothness is enforced to all
nodes. However, nodes in a real-world graph can have varied local smoothness. For nodes with low
local smoothness, we should impose a relatively smaller c, while for those nodes with higher local
smoothness, we need a larger c. Hence, instead of a unified c as in Eq. (15), we could consider a
node-dependent ci for each node i. Then, the optimization problem in Eq. (15) can be adjusted as:
argmFn L = χkF[i,：] 一X[i,：]k2 + 2XCi ∙ X kF[i,：] 一F[j,：]k2	(16)
i∈V	i∈V	j∈N(i)
We next show that the aggregation operation in GAT is closely connected to an approximate solution
of problem (16) with the help of the following theorem.
Theorem 4. With adaptive stepsize bi = 1/	(ci + cj ) for each node i, the process of taking
j∈N(i)
one step of gradient descent from X to solve problem (16) can be described as follows:
F[i,：] - X bi(Ci+Cj)X[j,：].	(17)
~一 ■,
j∈N(i)
Proof. The gradient of optimization problem in Eq. (16) with respect to F focusing on a node i can
be formulated as:
∂L
dFF7-T = 2(F [i, ：] - X [i, ：])+ E (Ci + Cj)(F [i,:] - F [j,:]) ,	(18)
,'	j∈N(i)
where cj in the second term appears since i is also in the neighborhood of j . Then, the gradient at
X is ∂FL ] I	= P (Ci + Cj) (X [i, ：] 一 X [j, ：]). Thus, taking a step of gradient descent
[i,：] lF[i，：]=X[i，：]	j∈N(i)
starting from X with stepsize b can be described as follows:
F [i, ：] — X [i, ：]	- b ∙	∂FdL	= (1 —b X	(Ci	+ Cj) )X [i, ：]+	X b	(C +	Cj)	X	[j,：]
I , ：j F[i，：]=x[i，：]	j∈N(i)	j∈N(i)
(19)
Given b = 1/	(ci +cj), Eq. (19) can be rewritten as F[i, :] J	bi(ci + cj)X[j, :], which
,~x ,	、	,~x ,、
j∈N(i)	j∈N(i)
completes the proof.
□
4
Under review as a conference paper at ICLR 2021
Eq. (17) resembles the aggregation operation of GAT in Eq. (4) ifwe treat bi(ci +cj) as the attention
score αij. Note that we have P	(ci	+	cj)	= 1/bi,	for all i ∈	V.	So,	(ci +	cj)	can be regarded
j∈N(i)
as the pre-normalized attention score and 1/bi can be regarded as the normalization constant. We
further compare bi(ci + cj) with αij by investigating the formulation of eij in Eq. (5). Eq. (5) can
be rewritten as:
eij = LeakyReLU (X0in [i, :]a1 + X0in [j, :]a2)	(20)
where a1 ∈ Rd and a2 ∈ Rd are learnable column vectors, which can be concatenated to form a in
Eq. (5). Comparing eij with (ci + cj), we find that they take a similar form. Specifically, X0in[i, :]a1
and X0in [j, :]a2 can be regarded as the approximations of ci and cj , respectively. The difference
between bi(ci + cj) and αij is that the normalization in Eq. (17) for bi(ci + cj) is achieved via
summation rather than a softmax as in Eq. (4) for αij . Note that since GAT makes the ci and
cj learnable, they also include a non-linear activation in calculating eij . By viewing the attention
mechanism in GAT from the perspective of Eq. (17), namely that ci actually indicates a notion of
local smoothness for node i, we can develop other ways to parameterize ci . For example, instead of
directly using the node features of i as an indicator of local smoothness like GAT, we can consider
the neighborhood information. In fact, we adopt this idea to design a new aggregation operation in
Section 5.
4	Ugnn: A Unified GNN Framework via Graph Signal Denoising
In the previous section, we established that the aggregation operations in PPNP, APPNP, GCN and
GAT are intimately connected to the graph signal denoising problem with (generalized) Laplacian
regularization. In particular, from this perspective, all their aggregation operations aim to ensure
feature smoothness: either a global smoothness over the graph as in PPNP, APPNP and GCN, or a
local smoothness for each node as in GAT. This understanding allows us to develop a unified feature
aggregation operation by posing the following, more general graph signal denoising problem:
Problem 2 (Generalized UGNN Graph Signal Denoising Problem).
arg min L = kF - Xk2F + r(C, F, G),	(21)
F
where r(C, F, G) denotes a flexible regularization term to enforce some prior over F.
Note that we overload the notation C here: it can function as a scalar (like a global constant in GCN),
a vector (like node-wise constants in GAT) or even a matrix (edge-wise constants) if we want to give
flexibility to each node pair. Different choices of r(∙) imply different feature aggregation operations.
Besides PPNP, APPNP, GCN and GAT, there are aggregation operations in more GNN models that
can be associated with Problem 2 with different regularization terms such as PairNorm (Zhao &
Akoglu, 2019) and DropEdge (Rong et al., 2019) (more details can be found in Appendix B). The
above mentioned regularization terms are all related to the Laplacian regularization. Other regular-
ization terms can also be adopted, which may lead to novel designs of GNN layers. For example, if
We aim to enforce that the clean signal is piece-wise linear, We can adopt r(C, F, G) = C ∙ IlLFkI
designed for trend filtering (Tibshirani et al., 2014; Wang et al., 2016).
With these discussions, We propose a unified frameWork (Ugnn) to design GNN layers from the
graph signal processing perspective as: (1) Design a graph regularization term r(C, F, G) in Prob-
lem 2 according to specific applications; (2) Feature Transformation: X0in = ftrans (Xin); and (3)
Feature Aggregation: Solving Problem 2 With X = X0in and the designed r(C, F, G). To demon-
strate the potential of Ugnn, next We introduce a neW GNN model Ada-Ugnn by instantiating
UGNN With r(C, F, G) enforcing adaptive local smoothness across nodes. Note that We introduce
Ada-Ugnn With node classification as the doWnstream task.
5	Ada-Ugnn: Adaptive Local Smoothing with Ugnn
From the graph signal denoising perspective, PPNP, APPNP, and GCN enforces global smoothness
by penalizing the difference With a constant C for all nodes. HoWever, real-World graphs may con-
sist of multiple groups of nodes Which have different behaviors in connecting to similar neighbors.
For example, Section 6.1 shoWs several graphs With varying distributions of local smoothness (as
measured by label homophily): summarily, not all nodes are highly label-homophilic, and some
nodes have considerably “noisier” neighborhoods than others. Moreover, as suggested by Wu et al.
(2019); Jin et al. (2020), adversarial attacks on graphs tend to promote such label noise in graphs
by connecting nodes from different classes and disconnecting nodes from the same class, rendering
5
Under review as a conference paper at ICLR 2021
resultant graphs with varying local smoothness across nodes. Under these scenarios, a constant C
might not be optimal and adaptive (i.e. non-constant) smoothness to different nodes is desired. As
shown in Section 3.3 by viewing GAT’s aggregation as a solution to regularized graph signal denois-
ing, GAT can be regarded as adopting an adaptive C for different nodes, which facilitates adaptive
local smoothness. However, in GAT, the graph denoising problem is solved by a single step of gra-
dient descent, which might still be suboptimal. Furthermore, when modeling the local smoothness
factor ci in Eq. (17), GAT only uses features of node i as input, which may not be optimal since by
understanding ci as local smoothness, it should be intrinsically related to the neighborhood of node
i. In this section, we adapt this notion directly into the UGNN framework by introducing a new reg-
ularization term, and develop a resulting GNN model (Ada-Ugnn) which aims to enforce adaptive
local smoothness to nodes in a different manner to GAT. We then utilize an iterative gradient descent
method to approximate the optimal solution for Problem 2 with the following regularization term:
2
r(C, F, G)
2∙χ Ci χ I F[⅛ - j
i∈V j∈N(i)	j
(22)
2
where di , dj denotes the degree of node i and j respectively, and Ci indicates the smoothness
factor of node i, which is assumed to be a fixed scalar. Note that, the above regularization term
can be regarded as a generalized version of the regularization term used in PPNP, APPNP, and
GCN. Similar to PPNP and APPNP, Ada-Ugnn only consists of a single GNN layer. However,
Ada-Ugnn assumes adaptive local smoothness. We next describe the feature transformation and
aggregation operations of Ada-Ugnn, and show how to derive the model via Ugnn.
5.1	Feature Transformation
Similar to PPNP and APPNP, we adopt MLP for the feature transformation. Specifically, for a node
classification task, the dimension of the output of the feature transformation X0in is the number of
classes in the graph.
5.2	Feature Aggregation
We use iterative gradient descent to solve Problem 2 with the regularization term in Eq. (22) The
iterative gradient descent steps are stated in the following theorem and its proof can be found at
Appendix A.1.
Theorem 5. With adaptive stepsize bi = 1/ 2 + P (Ci + Cj)/di for each node i, the iterative
j∈N(i)
gradient descent steps to solve Problem 2 with the regularization term in Eq. (22) is as follows:
F(k)[i,:] J 2bX[i,:] + bi
(Ci + Ci)
j∈N(i)
F(kτ)[j,:]
---Γ^Γ ；
k = 1,........
(23)
where F(0) [i, :] = X[i, :].
The iterative steps in Eq. (23) is guaranteed for convergence as stated in the following theorem and
its proof can be found in Appendix A.2.
Theorem 6. The iterative steps in Eq. (23) is guaranteed to converge to the optimal solution of
Problem 2 with Eq. (22) as regularization term.
Following the iterative solution in Eq. (23), we model the aggregation operation (for node i) for
Ada-Ugnn as follows:
X(oku)t[i, :]
2biX0in[i,:] +bi
(Ci+Cj)
Vj ∈N(vi)
Xou-I) j,:].
k = 1, . . . K,
(24)
J
where K is the number gradient descent iterations, Ci can be considered as a positive scalar to
control the level of “local smoothness" for node i and b can be calculated from {Cj |j ∈ N(i)} as
bi = 1/ 2 + P (Ci + Cj)/di . However, in practice, Ci is usually unknown. One possible solu-
j∈N(i)
tion is to treat Ci as hyper-parameters. Treating Ci as hyper-parameters for all nodes is impractical,
since there are, in total N of them and we do not have their prior knowledge. Thus, we model Ci as
a function of the information of the neighborhood of node i as follows:
Ci = S ∙σ (hi (h2 ({xin[j, :]j ∈N(i)}))) ,	(25)
6
Under review as a conference paper at ICLR 2021
where h2(∙) is a function to transform the neighborhood information of node i to a vector, while
hι(∙) further transforms it to a scalar. σ(∙) denotes the sigmoid function, which maps the output
scalar from hι(∙) to (0,1) and S can be treated as a hyper-parameter controlling the upper bound
of Ci. hι(∙) can be modeled as a single layer fully-connected neural network. There are different
designs for h2(∙) such as channel-wise variance or mean (Corso et al., 2020). In this paper, We
adopt channel-wise variance as the h2(∙) function. In this case, the calculation of Ci in Eq. (25) only
involves H parameters, with H denoting number of classes in the dataset. APPNP can be regarded
a special case of Ada-Ugnn, where h2(∙) is modeled as a constant function producing 1 as the
output for all nodes. For the node classification task, the representation X(oKut), which is obtained
after K iterations as in Eq. (24), is directly softmax normalized row-wise and its i-th row indicates
the discrete class distribution of node i.
6 Experiment
In this section, we evaluate how the proposed Ada-Ugnn handles graphs with varying local
smoothness. We conduct node classification experiments on natural graphs, and also evaluate the
model’s robustness under adversarial attacks. We note that our main goal in proposing/evaluating
Ada-Ugnn is to demonstrate the promise of deriving new aggregations as solutions of denoising
problems, rather than state-of-the-art performance.
6.1	Node Classification
In this section, we conduct the node classification task. We first introduce the datasets and the
experimental settings in Section 6.1.1 and then present the results in Section 6.1.2.
6.1.1	Datasets and Experimental Settings
We conduct the node classification task on 8 datasets from various domains including citation, so-
cial, co-authorship and co-purchase networks. Specifically, we use three citation networks including
Cora, Citeseer, and Pubmed (Sen et al., 2008); one social network, BlogCatalog (Huang
et al., 2017); two co-authorship networks including Coauthor-CS and Coauthor-PH (Shchur
et al., 2018); and two co-purchase networks including Amazon-Comp and Amazon Photos (Shchur
et al., 2018). Descriptions and detail statistics about these datasets can be found in Appendix C.1.
To provide a sense of the local smoothness properties of these datasets, in addition to the summary
statistics, we also illustrate the local label smoothness distributions in Appendix C.1.1: here, we
define the local label smoothness of a node as the ratio of nodes in its neighborhood that share the
same label (see formal definition in Eq. (34) in Appendix C.1.1). Notably, the variety in local label
smoothness within several real-world datasets - also observed in (Shah, 2020) 一 clearly motivates
the importance of the adaptive smoothness assumption in Ada-Ugnn. For the citation networks,
we use the standard split as provided in Kipf & Welling (2016); Yang et al. (2016). For Blog-
Catalog, we adopt the split provided in Zhao et al. (2020). For both the citation networks and
BLOGCATALOG, the experiments are run with 30 random seeds and the average results are reported.
For co-authorship and co-purchase networks, we utilize 20 labels per class for training, 30 nodes per
class for validation and the remaining nodes for test. This process is repeated 20 times, which results
in 20 different training/validation/test splits. For each split, the experiment is repeated for 20 times
with different initialization. The average results over 20 × 20 experiments are reported. We compare
our methods with the methods introduced in Section 2 including GCN, GAT and APPNP. Note that
we do not include PPNP as it is difficult to scale for most of the datasets due to the calculation of in-
verse in Eq. 6. For all methods, we tune the hyperparameters from the following options: 1) learning
rate: {0.005, 0.01, 0.05}; 2) weight decay {5e-04, 5e-05, 5e-06, 5e-07, 5e-08}; and 3) dropout
rate: {0.2, 0.5, 0.8}. For APPNP and our method we further tune the number of iterations K and the
upper bound s for ci in Eq. (25) from the following range: 1) K: {5, 10}; and s: {1, 9, 19}. Note
that we treat APPNP as a special case of our proposed method with h2(∙) = 1.
6.1.2	Performance Comparison
The performance comparison is shown in Table 1, where t-test is used to test the significance. First,
GAT outperforms GCN in most datasets. It indicates that modeling adaptive local smoothness is
helpful. Second, APPNP/Ada-Ugnn outperform GCN/GAT in most settings, suggesting that it-
erative gradient descent may offer advantages to single-step gradients, due to their better ability
to achieve a solution closer to the optimal. Third, and most notably, the proposed Ada-Ugnn
achieves consistently better performance than GCN/GAT, and outperforms or matches the state-
of-the-art APPNP across datasets. Notice that in some datasets such as Cora, Citeseer, and
7
Under review as a conference paper at ICLR 2021
Table 1: Node Classification Accuracy on Various Datasets
Dataset	GCN	GAT	APPNP	Ada-Ugnn
Cora	81.75±0.8	82.56±0.8	84.49±0.6	84.59±0.8*
Citeseer	70.13±1.0	70.77±0.8	71.97±0.6	72.05±0.5
Pub med	78.56±0.5	78.88±0.5	79.92±0.5	79.70±0.4
BlogCatalog	71.38±2.7	72.90±1.2	92.43±0.9	93.33±0.3***
Amazon-Comp	82.79±1.3	83.01±1.5	82.99±1.6	83.40±1.3***
Amazon-Photo	89.60±1.5	90.33±1.2	91.38±1.2	91.44±1.2
Coauthor-CS	91.55±0.6	90.95±0.7	91.69±0.4	92.33±0.5***
Coauthor-PH	93.23±0.7	92.86±0.7	93.84±0.5	93.92±0.6**
*, **, *** indicate the improvement over APPNP is significant at P < 0.1, 0.05 and 0.005
80
≡
*60
E
^40
UDJMJ
20
Low
High
Low
High
Low
High
Low
High
0
0
0
0
(a) Cora
(b) BlogCatalog (c) Amazon-Comp (d) Coauthor-CS
Figure 1:	Accuracy for nodes with low and high local label smoothness.
Coauthor-PH, the improvements of the proposed model compared with APPNP are not very sig-
nificant. Figure 3 in Appendix C.1.1 shows that these datasets have extremely skewed local label
smoothness distributions, with the majority of nodes having perfect, 1.0, label homophily (they are
only connected to other nodes of the same label). APPNP shines in such cases, since its assump-
tion of h2(∙) = 1 is ideal for these nodes (designating maximal local smoothness). Conversely,
our model has the challenging task of learning h2(∙) - in such skewed cases, learning h2(∙) may be
quite challenging and unfruitful. On the other hand, for datasets with higher diversity in local label
smoothness across nodes such as BlogCatalog and Amazon-Comp, the proposed Ada-Ugnn
achieves more significant improvements.
To further validate, we partition the nodes in the test set of each dataset into two groups: (1) high
smoothness: those with local label smoothness >0.5, and (2) low smoothness: those with ≤0.5, and
evaluate accuracy for APPNP and the proposed Ada-Ugnn for each group. The results for Cora,
BlogCatalog, Amazon-Comp and Coauthor-CS are presented in Figure 1 while the results
for the remaining datasets can be found in Figure 4 in Appendix C.2. Figure 1 clearly shows that
Ada-Ugnn consistently improves performance for low-smoothness nodes in most datasets, while
keeping comparable (or marginally worse) performance for high-smoothness nodes. In cases where
many nodes have low-level smoothness (like BlogCatalog or Amazon-Comp), our method can
notably improve overall performance.
6.2 Robustness Under Adversarial Attacks
Adversarial attacks on graphs tend to connect nodes from different classes and remove edges be-
tween nodes from the same class (Wu et al., 2019; Jin et al., 2020), producing graphs with varying
local label smoothness after attack (we demonstrate this in Appendix C.3). To further demonstrate
that Ada-Ugnn can handle graphs with varying local label smoothness better than alternatives, we
conduct experiments to show its robustness under adversarial attacks. Specifically, we adopt Met-
tack (Zugner & Gunnemann, 2019) to perform the attacks. Mettack produces non-targeted attacks
which aim to impair test set node classification performance by strategically adding or removing
edges from the victim graph. We utilize the attacked graphs (5%-25% perturb rate) from Jin et al.
(2020) and follow the same setting, i.e., each method is run with 10 random seeds and the average
performance is reported. These attacked graphs are generated from Cora, Citeseer and Pubmed,
respectively and only the largest connected component is retained in each graph. Furthermore, the
training, validation and test split ratio is 10/10/80%, which is different from the standard splits
we use in Section 6.1. Thus, the performances reported in this section is not directly comparable
with those in the previous section. We compare our method both with standard GNNs discussed
in Section 2 (GCN, GAT, APPNP), but also with recent state-of-the-art defense techniques against
adversarial attacks including GCN-Jaccard (Wu et al., 2019), GCN-SVD (Entezari et al., 2020), Pro-
GNN-fs and Pro-GNN (Jin et al., 2020). The detailed description of these methods can be found at
8
Under review as a conference paper at ICLR 2021
85
80
a 70
§ 65
<
⅜J60
h 55
50
0%	5% 10% 15% 20% 25%
Perturbation Rate(%)
75.0
72.5
受 70.0
I 67.5
羊 65.0
S 62.5
60.0
57.5
0%	5% 10% 15% 20% 25%
Perturbation Rate(%)
88
胪
tt80
78
76
0%	5% 10% 15% 20% 25%
Perturbation Rate(%)
(a) Cora
(b) Citeseer
(c) Pubmed
Figure 2:	Robustness under adversarial attacks (node classification accuracy).
Appendix C.4. Results under varying perturbation rates (attack intensities) are shown in Figure 2.
Again, we observe that GAT outperforms GCN, suggesting the appeal of an adaptive local smooth-
ness assumption. Here, our method (orange) substantially outperforms GCN, GAT and APPNP by
a large margin, especially in scenarios with high perturbation rate. Moreover, the proposed Ada-
Ugnn is also even more robust than several specially designed adversarial defense methods, like
GCN-Jaccard and GCN-SVD, which are based on pre-processing the adversarial attack graphs to
obtain cleaner ones, thanks to its adaptive smoothness assumption. Compared with Pro-GNN-fs,
our method performs comparably or even better in a few settings, especially when perturbation
rate is high. Furthermore, in these settings, the performance of our method is even closer to Pro-
GNN, which is the current state-of-the art adversarial defense technique. Note that, Pro-GNN-fs
and Pro-GNN involves learning cleaner adjacency matrices of the attacked graphs, and thus has
O(M) parameters (M denotes the number of edges in a graph), while our proposed model has far
less parameters. Specifically, We have O (din ∙ dout) for feature transformation and H parameters
for modelling hι(∙) with H denoting the number of labels.
7	Related Works
There are mainly two streams of work in developing GNN models, i.e, spectral-based and spatial-
based. When designing spectral-based GNNs, graph convolution (Shuman et al., 2013), defined
based on spectral theory, is utilized to design graph neural network layers together with the feature
transformation and non-linearity (Bruna et al., 2013; Henaff et al., 2015; Defferrard et al., 2016).
These designs of the spectral-based graph convolution are tightly related with graph signal process-
ing, and they can be regarded as graph filters. Low-pass graph filters can usually be adopted to
denoise graph signals (Chen et al., 2014). In fact, most algorithms discussed in our work can be
regarded as low-pass graph filters. With the emergence of GCN (Kipf & Welling, 2016), which
can be regarded as a simplified spectral-based and also a spatial-based graph convolution operator,
numerous spatial-based GNN models have since been developed (Hamilton et al., 2017; Velickovic
et al., 2017; Monti et al., 2017; Gao et al., 2018; Gilmer et al., 2017).
Graph signal denoising is to infer a cleaner graph signal given a noisy signal, and can be usually
formulated as a graph regularized optimization problem (Chen et al., 2014). Recently, several works
connect GCN with graph signal denoising with Laplacian regularization (NT & Maehara, 2019;
Zhao & Akoglu, 2019), where they found the aggregation process in GCN models can be regarded
as the first-order approximation of the optimal solution of the denoising problem. On the other hand,
GNNs are also utilized to develop novel algorithms for graph denoising (Chen et al., 2020). Unlike
these works, our paper details how a family of GNN models can be unified with a graph signal
denoising perspective, and demonstrates its promise for new architecture design.
8	Conclusion
In this paper, we show how various representative GNN models including GCN, PPNP, APPNP and
GAT can be unified mathematically as natural instances of graph denoising problems. Specifically,
the aggregation operations in these models can be regarded as exactly or approximately addressing
such denoising problems subject to Laplacian regularization. With these observations, we propose
a general framework, Ugnn, which enables the design of new GNN models from the denoising
perspective via regularizer design. As an example demonstrating the promise of this paradigm, we
instantiate the Ugnn framework with a regularizer addressing adaptive local smoothness across
nodes, a property prevalent in several real-world graphs, and proposed and evaluated a suitable new
GNN model, Ada-Ugnn.
9
Under review as a conference paper at ICLR 2021
References
Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. arXiv preprint arXiv:1312.6203, 2013.
Siheng Chen, Aliaksei Sandryhaila, Jose MF Moura, and Jelena Kovacevic. Signal denoising on
graphs via graph filtering. In 2014 IEEE Global Conference on Signal and Information Processing
(GlobalSIP), pp. 872-876. IEEE, 2014.
Siheng Chen, Yonina C Eldar, and Lingxiao Zhao. Graph unrolling networks: Interpretable neural
networks for graph signal denoising. arXiv preprint arXiv:2006.01301, 2020.
Gabriele Corso, LUca Cavalleri, Dominique Beaini, Pietro Lio, and Petar Velickovic. Principal
neighbourhood aggregation for graph nets. arXiv preprint arXiv:2004.05718, 2020.
Michael Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on
graphs with fast localized spectral filtering. In Advances in neural information processing systems,
pp. 3844-3852, 2016.
Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you
need is low (rank) defending against adversarial attacks on graphs. In Proceedings of the 13th
International Conference on Web Search and Data Mining, pp. 169-177, 2020.
Hongyang Gao, Zhengyang Wang, and Shuiwang Ji. Large-scale learnable graph convolutional
networks. In Proceedings of the 24th ACM SIGKDD, pp. 1416-1424, 2018.
Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.
In NeurIPS, pp. 1024-1034, 2017.
Mikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured
data. arXiv preprint arXiv:1506.05163, 2015.
Xiao Huang, Jundong Li, and Xia Hu. Label informed attributed network embedding. In Proceed-
ings of the Tenth ACM International Conference on Web Search and Data Mining, pp. 731-739,
2017.
Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure
learning for robust graph neural networks. arXiv preprint arXiv:2005.10203, 2020.
Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional net-
works. arXiv preprint arXiv:1609.02907, 2016.
Johannes Klicpera, Aleksandar Bojchevski, and Stephan Gunnemann. Predict then propagate:
Graph neural networks meet personalized pagerank. arXiv preprint arXiv:1810.05997, 2018.
Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodola, Jan Svoboda, and Michael M
Bronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. In
Proceedings of the IEEE Conference on CVPR, pp. 5115-5124, 2017.
Yurii Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer
Science & Business Media, 2013.
Hoang NT and Takanori Maehara. Revisiting graph neural networks: All we have is low-pass filters.
arXiv preprint arXiv:1905.09550, 2019.
Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph
convolutional networks on node classification. In International Conference on Learning Repre-
sentations, 2019.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.
Collective classification in network data. AI magazine, 29(3):93-93, 2008.
Neil Shah. Scale-free, attributed and class-assortative graph generation to facilitate introspection of
graph neural networks. KDD Mining and Learning with Graphs, 2020.
10
Under review as a conference paper at ICLR 2021
Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and StePhan Gunnemann. Pitfalls
of graph neural network evaluation. arXiv preprint arXiv:1811.05868, 2018.
David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega, and Pierre Vandergheynst. The
emerging field of signal Processing on graPhs: Extending high-dimensional data analysis to net-
works and other irregular domains. IEEE signal processing magazine, 30(3):83-98, 2013.
Ryan J Tibshirani et al. AdaPtive Piecewise Polynomial estimation via trend filtering. The Annals of
Statistics, 42(1):285-323, 2014.
Petar VeliCkovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. GraPh attention networks. arXiv preprint arXiv:1710.10903, 2017.
Yu-Xiang Wang, James SharPnack, Alexander J Smola, and Ryan J Tibshirani. Trend filtering on
graPhs. The Journal of Machine Learning Research, 17(1):3651-3691, 2016.
Huijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial
examPles on graPh data: DeeP insights into attack and defense. arXiv preprint arXiv:1903.01610,
2019.
Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-suPervised learning with
graPh embeddings. In International conference on machine learning, PP. 40-48. PMLR, 2016.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.
GraPh convolutional neural networks for web-scale recommender systems. In Proceedings of the
24th ACM SIGKDD, PP. 974-983, 2018a.
Zhitao Ying, Jiaxuan You, ChristoPher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. Hi-
erarchical graPh rePresentation learning with differentiable Pooling. In Advances in neural infor-
mation processing systems, PP. 4800-4810, 2018b.
Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. arXiv preprint
arXiv:1909.12223, 2019.
Tong Zhao, Yozen Liu, Leonardo Neves, Oliver Woodford, Meng Jiang, and Neil Shah. Data aug-
mentation for graPh neural networks. arXiv preprint arXiv:2006.06830, 2020.
Daniel Zugner and Stephan Gunnemann. Adversarial attacks on graph neural networks via meta
learning. arXiv preprint arXiv:1902.08412, 2019.
11
Under review as a conference paper at ICLR 2021
A Proofs
A.1 Proof of Theorem 5
Theorem 5. With adaptive stepsize bi = 1/ 2 +	(Ci + Cj)/di for each node vi, the
Vj ∈N(Vi)
iterative gradient descent steps to solve Problem 2 with the regularization term in Eq. (22) is as
follows:
F(k)[i,:]
2bX[i, :] + bi
(Ci +Ci)
Vj∈N(Vi)
F(k-1)[j,:]
k = 1, . . .
(26)
J
where F(0) [i, :] = X[i, :].
Proof. The gradient of the optimization problem 2 with the regularization term in Eq. (22) with
respect to F (focusing on node i) is as follows:
∂L
∂F‰^ = 2(F[i,:] - X[i,:]) + X
Vj ∈N(Vi)
C + C	F[i, ：] Fj,：
(27)
where Cj in the second term appears since node i is also in the neighborhood of node j . The iterative
gradient descent steps with adaptive stepsize bi can be formulated as follows:
F(k)[i，：]j F(I)[i，：]- bi ∙ ∂FLι
; k = 1,...
F[i,:]=F(k-1) [i,:]
(28)
With the gradient in Eq. (27), the iterative steps in Eq. (28) can be rewritten as:
F(k)[i,:] J(1- 2bi — bi	X	Ci+Cj)F(k-1)[i,:] + 2biX[i,:]
di
Vj∈N(Vi)
+ bi	(Ci + Cj)
Vj∈N(Vi)
F(k) j,：].
didj ；
k= 1,...
(29)
Given bi = 1/ 2 +	(Ci + Cj)/di , the iterative steps in Eq. (29) can be re-written as
Vj ∈N(Vi)
follows:
F(k)[i,:] J2bX[i,:]+bi	(Ci + Cj)
Vj∈N(Vi)
F(k-1)j,：],
didj
k = 1,. ..,
(30)
;
with F(0) [i, :] = X[i, :], which completes the proof.
□
A.2 Proof of Theorem 6
Theorem 6. The iterative steps in Eq. (23) is guaranteed to converge to the optimal solution of
Problem 2 with Eq. (22) as regularization term.
Proof. By taking the second derivative with respect to F[i, :], we obtain the Hessian matrix as:
∂l⅛ = 2I +	X (T )I	(31)
[i, :]	i
Vj ∈N (Vi)
which implies the LiPschitz constant of the gradient in Eq.(27) is 2+ P	(Ci+Cj). To guarantee
di
Vj∈N(Vi)
convergence, the stepsize bi for node i should be smaller than 2/ 2 + P (Ci + Cj)/di (Nes-
Vj ∈N(i)	J
terov, 2013). The stepsize we adopt in Theorem 5 is bi = 1/ 2 + P (Ci + Cj)/di , hence the
j∈N(i)
convergence is guaranteed.	□
12
Under review as a conference paper at ICLR 2021
B	Connections to PairNorm and DropEdge
PairNorm and DropEdge, which are two recently proposed GNN enhancements for developing
deeper GNN models, are corresponding to the following regularization terms:
PairNorm: X Cp ∙∣∣F[i,:] - Fj, :]k2 - X Cn ∙ ∣∣F[i,:] - F[j, ：]k2,	(32)
(i,j)∈E	(i,j)6∈E
DropEdge: X Cij ∙ ∣∣F[i,:] - F[j, :]k2, whereCij ∈ {0,1}.	(33)
(i,j)∈E
For PairNorm, C consists of Cp , Cn > 0 and the regularization term ensures connected nodes to
be similar while disconnected nodes to be dissimilar. For DropEdge, C is a sparse matrix having
the same shape as adjacency matrix. For each edge (i, j), its corresponding Cij is sampled from a
Bernoulli distribution with mean 1 - q, where q is a pre-defined dropout rate.
C Experiments
C.1 Datasets
	#Nodes	#Edges	#Labels	#Features
Cora	2708	13264	7	1433
Citeseer	3327	12431	6	3703
Pubmed	19717	108365	3	500
BlogCatalog	5196	348682	6	8189
Amazon-Comp	13381	504937	10	767
Amazon-Photo	7487	245573	8	745
Coauthor-CS	18333	182121	15	6805
Coauthor-PH	34493	530417	5	8415
Table 2: Dataset summary statistics.
In this section, we provide information of the datasets we used in the experiments as follows:
•	Citation Networks: CORA, CITESEER and PUBMED are widely adopted benchmarks of
GNN models. In these graphs, nodes represent documents and edges denote the citation
links between them. Each node is associated bag-of-words features of its corresponding
document and also a label indicating the research field of the document.
•	Blogcatalog: BLOGCATALOG is an online blogging community where bloggers can follow
each other. The BlogCatalog graph consists of blogger as nodes while their social
relations as edges. Each blogger is associated with some features generated from key words
of his/her blogs. The bloggers are labeled according to their interests.
•	Co-purchase Graph: AMAZON-COMP and AMAZON-PHOTO are co-purchase graphs,
where nodes represent items and edges indicate that two items are frequently bought to-
gether. Each item is associated with bag-of-words features extract from its corresponding
reviews. The labels of items are given by the category of them.
•	Co-authorship Graphs: COAUTHOR-CS and COAUTHOR-PH are co-authorship graphs,
where nodes are authors and edges indicating the co-authorship between authors. Each
author is associated with some features representing the keywords of his/her papers. The
label of an author indicates the his/her most active research field.
Some statistics of these graphs are shown in Table 2.
C.1.1 Local Label Smoothness of Datasets
We further present the distribution of local label smoothness in these datasets. For a node vi we
formally define the local label smoothness as follows
1{l(i) = l(j)}
j∈N(i)
ls(i) = Ni)-
(34)
13
Under review as a conference paper at ICLR 2021
(a) Cora
Local Label Smoothness
(b) Citeseer
1000-
(c) Pubmed
BOO-
600-
400-
200
0. .	...	..	...	..
0.0	0.2 0.4	0.6	0.8	1.0
Local Label Smoothness
(d)	BlogCatalog
6000-
5000-
84000-
W
13。OO-
2000-
1000-
0.0 0-2 0.4 0.6 0.6 1.0
Local Label Smoothness
4θθθf . 1000°f i ≡≡f
80∞	20000
I3000	⅝ 6000	115000
I 2000	I	I
1000 ■	2000	_5000
OLC ,	OLL，0∙L<7—3 3
0 0.0 0.2 0.4
0.6 O.B 1.0
Local Label Smoothness
0.0 0.2 0.4 0.6 O.B 1.0
Local Label Smoothness
°⅛
0.2 0.4 0.6
Local Label Smoothness
O.B 1.0
(e)	Amazon-Comp
(f)	Amazon-Photo
(g)	Coauthor-CS
(h)	Coauthor-PH
0
0
Figure 3:	Distribution of local label smoothness (homophily) on different graph datasets: note the
non-homogeneity of smoothness values.
■■ ■■ ■爨■
!___________________!
Low	High	Low	High	Low	High	Low	High
(a) Citeseer	(b) Pubmed	(c) Amazon-Photo	(d) Coauthor-PH
Figure 4:	Accuracy with low label smoothness and high label smoothness nodes. Note the consistent
improvement in low smoothness cases, enabled by adaptive local smoothing.
where l(vi) denotes the label of node vi and 1{a} is an indicator function, which takes 1 as output
only when a is true, otherwise 0. The distributions of local label smoothness for all 8 datasets are
presented in Figure 3.
C.2 Node Classification Accuracy For Nodes with Low-level and High-level
Local Label Smoothness
The performance of nodes with low local label smoothness and high local label smoothness in Cite-
seer, Pubmed, Amazon-Photo and Coauthor-PH are presented in Figure 4.
C.3 Local Smoothness Distribution of Attacked Graph
Graph adversarial attacks tend to connect nodes from different classes while disconnect nodes from
the same class, which typically leads to more diverse distributions of local smoothness level. We
present the distributions of the graphs generated by Mettack (Zugner & Gunnemann, 2019) with
different perturbation rate for Cora, Citeseer and Pubmed in Figure 5, Figure 6 and Figure 7,
respectively.
C.4 Baselines for Adversarial Defense
In this section, we list the descriptions of the defense algorithms we adopt in Section 6.2 as follows:
• GCN-Jaccard (Wu et al., 2019): GCN-Jaccard aims to pre-process a given attacked graph
by removing those edges added by the attackers. Specifically, Jaccard smilarlity is utilized
14
Under review as a conference paper at ICLR 2021
1600-
1400-
1200-
1000-
i BOO-
600-
I 黑Γ	I ɪŋŋŋf I
IOOO	800	600
l≡	"	IZ
400-
200-
0.6 0.6 1-0
Local Label Smoothness
0.0 0.2 0.4 0.6 0.B 1.0
Local Label Smoothness
0.0 0.2 0.4 0.6 O.B 1.0
Local Label Smoothness
0.0 0-2 0-4 0.6 0.6	1.0
Local Label Smoothness
(a)	0%
(b)	5%
(c)	15%
(d) 25%

0
0
0
Figure 5:	Distribution of local label smoothness on Cora with various attack perturbation rates.
1200
1000
800
600
400
200
0
0.0 0.2 0.4
10T	g	：D
4 68	∣≡∞:	∣Z
1J	⅛.	-	1≡i	L
mJ ⅛J LhJ LU
0.6 0.6 1-0
Local Label Smoothness
0.0 0.2 0.4 0.6 0.B 1.0
Lacal Label Smoothness
0.0 0.2 0.4 0.6 0.B	1.0
Local Label Smoothness
0.0 0-2 0-4 0.6 0.6	1.0
Local Label Smoothness
(a)	0%
(b)	5%
(c)	15%
(d) 25%
0
0
0
Figure 6:	Distribution of local label smoothness on Citeseer with various attack perturbation rates.
(a) 0%
10000
B000
6000
4000
0⅛ ~0.4
6000	.	5000
5∞0	.	■	4000
I Γ≡∣ LJ⅝lkl
LJ ηlxU] ɪŋ:lɪld
0.6 0.B 1.0
Local Label Smoothness
0.0 0.2 0.4 0.6 0.B 1.0
Local Label Smoothness
0.0 0.2 0.4 0.6 0.B 1.0
Local Label Smoothness
(b) 5%
(c) 15%
(d) 25%
0
0
Figure 7:	Distribution of local label smoothness on Pubmed with various attack perturbation rates.
to measure the feature similarity between connected pairs of nodes. The edges between
node pairs with low-similarity are removed by the algorithm. This pre-processed graph is
then utilized for the node classification task.
•	GCN-SVD (Entezari et al., 2020): GCN-SVD is also a pre-process method. It use SVD
to decompose the adjacency matrix of a given perturbed graph and then obtain its low-rank
approximation. The low-rank approximation is believed to be cleaner as graph adversarial
attacks are observed to be high-rank in (Entezari et al., 2020).
•	Pro-GNN (Jin et al., 2020): Pro-GNN tries to learn a cleaner graph while training the node
classification model at the same time. Specifically, it treats the adjacency as parameters,
which is optimized during the training stage. Several different constraints are enforced to
this learnable adjacency matrix, including: 1) the learned adjacency matrix should be close
to the original adjacency matrix; 2) the learned adjacency matrix should be low-rank; and
3) the learned adjacency matrix should ensure feature smoothness. Pro-GNN-fs is a variant
of Pro-GNN where the third constraint, i.e. feature smoothness, is not enforced.
C.5 Investigation on Number of Gradient Descent Steps in ADA-UGNN
In this section, we conducted experiments to check how the performance of ADA-UGNN is affected
by K. For each K, we run the experiments on standard splits of CORA, CITESEER and PUBMED
with 30 random seeds (i.e., the same setting as in Section 6.) The average performance is reported.
As shown in Figure 8, the performance increases quickly as K gets larger when K is relatively
small. After K becomes large, the performance either slowly grows or slightly fluctuates as K
further increases.
15
Under review as a conference paper at ICLR 2021
Test Accu racy(%)
(a) Cora	(b) Citeseer	(c) Pubmed
Figure 8:	ADA-UGNN performance (test accuracy) under different numbers of gradient steps (K).
16