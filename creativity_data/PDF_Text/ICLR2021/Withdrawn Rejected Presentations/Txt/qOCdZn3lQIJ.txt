Under review as a conference paper at ICLR 2021
Compressing gradients in distributed SGD by
EXPLOITING THEIR TEMPORAL CORRELATION
Anonymous authors
Paper under double-blind review
Ab stract
We propose SignXOR, a novel compression scheme that exploits temporal corre-
lation of gradients for the purpose of gradient compression. Sign-based schemes
such as Scaled-sign and SignSGD (Bernstein et al., 2018; Karimireddy et al.,
2019) compress gradients by storing only the sign of gradient entries. These meth-
ods, however, ignore temporal correlations between gradients. The equality or
non-equality of signs of gradients in two consecutive iterations can be represented
by a binary vector, which can be further compressed depending on its entropy.
By implementing a rate-distortion encoder we increase the temporal correlation
of gradients, lowering entropy and improving compression. We achieve theoreti-
cal convergence of SignXOR by employing the two-way error-feedback approach
introduced by Zheng et al. (2019). Zheng et al. (2019) show that two-way com-
pression with error-feedback achieves the same asymptotic convergence rate as
SGD, although convergence is slower by a constant factor. We strengthen their
analysis to show that the rate of convergence of two-way compression with error-
feedback asymptotically is the same as that of SGD. As a corollary we prove that
two-way SignXOR compression with error-feedback achieves the same asymp-
totic rate of convergence as SGD. We numerically evaluate our proposed method
on the CIFAR-100 and ImageNet datasets and show that SignXOR requires less
than 50% of communication traffic compared to sending sign of gradients. To the
best of our knowledge we are the first to present a gradient compression scheme
that exploits temporal correlation of gradients.
1	Introduction
Distributed optimization has become the norm for training machine learning models on large
datasets. With the need to train bigger models on ever-growing datasets, scalability of distributed
optimization has become a key focus in the research community. While an obvious solution to
growing dataset size is to increase the number of workers, the communication among workers has
proven to be a bottleneck. For popular benchmark models such as AlexNet, ResNet and BERT
the communication can account for a significant portion of the overall training time (Alistarh et al.,
2017; Seide et al., 2014; Lin et al., 2018). The BERT (“Bidirectional Encoder Representing from
Transformers”) architecture for language models (Devlin et al., 2018) comprises about 340 mil-
lion parameters. If 32-bit floating-point representation is used one gradient update from a worker
amounts to communicating around 1.3GB (340 × 106 parameters × 32 bits per parameter × 2-33 gi-
gabytes per bit ≈ 1.3GB). Frequently communicating such large payloads can easily overwhelm the
network resulting in prolonged training times. In addition, large payloads may increase other forms
of costs in distributed optimization. Novel approaches such as federated learning employ mobile
devices as worker nodes. Exchanging information with mobile devices is heavily constrained due to
communication bandwidth and budget limitations. Therefore, communication remains an important
bottleneck in distributed optimization and reducing communication is of utmost importance.
Gradient compression alleviates the communication bottleneck. The idea is to apply a compression
scheme on gradients before sending them over the network. There has been an increasing amount
of literature on gradient compression within the last few years (Seide et al., 2014; Aji & Heafield,
2017; Alistarh et al., 2017; Wen et al., 2017; Wangni et al., 2018; Wu et al., 2018; Lin et al., 2018;
Wang et al., 2018). Such compression schemes have been demonstrated to work well with dis-
tributed stochastic gradient descent (SGD) and its variants. However, SGD with arbitrary compres-
1
Under review as a conference paper at ICLR 2021
sion schemes may not converge. Karimireddy et al. (2019) give one example of non-convergence.
The recently proposed error-feedback based algorithms (Stich et al., 2018; Karimireddy et al., 2019)
circumvent the convergence issue. Error-feedback methods accumulate the compression error and
feed it back to the input of the compression scheme so that the error gets transmitted over subsequent
iterations. The dist-EF-SGD algorithm proposed by Zheng et al. (2019) applies error-feedback to
two-way compression, in which both worker-to-master and master-to-worker communications are
compressed. Theoretical guarantees provided by dist-EF-SGD are valid for all compression schemes
that fall under the definition of 'δ-approximate compressors'，also referred to as δ-compressors. The
authors prove that error-feedback with two-way compression asymptotically achieves the O (1/ √T)
convergence rate of SGD. However, the analysis by Zheng et al. (2019) suggests that dist-EF-SGD
converges slower than SGD by a constant factor.
Our contributions in this paper are as follows. We propose SignXOR, a novel compression scheme
that exploits temporal correlation of gradients. We prove that SignXOR is a δ-compressor, and
we provide convergence guarantees for SignXOR by employing dist-EF-SGD. We strengthen the
convergence bound by Zheng et al. (2019) to show that dist-EF-SGD asymptotically converges at
the same O(1∕√T) rate as SGD. Consequently, We show that the proposed method asymptoti-
cally achieves the SGD convergence rate. We empirically validate the proposed method on CIFAR-
100 and ImageNet datasets and demonstrate that the ratio between total communication budgets of
SignXOR and Scaled-sign is less than 50%.
Notation: For X ∈ Rd, x[j] denotes the jth entry of x, ∣∣χ∣∣ι denotes the 'ι-norm, and ∣∣χk denotes
the '2-norm. For vector inputs sgn(∙) function outputs the sign of the input element-wise. The index
set {1, . . . , n} is denoted by [n], and denotes elementwise multiplication.
2	Related work
The most common gradient compression schemes can be categorized into those based on sparsifica-
tion and those based on quantization. The methods based on sparsification such as Top-k, Rand-k
(Stich et al., 2018; Lin et al., 2018) and Spectral-ATOMO (Wang et al., 2018) preserve only the
most significant gradient elements, effectively reducing the quantity of information carrying gradi-
ent components. On the other hand, the methods based on quantization such as QSGD (Alistarh
et al., 2017), TernGrad (Wen et al., 2017) and SignSGD (Bernstein et al., 2018) reduce the overall
floating-point precision of the gradient. Therefore, these two classes of methods can be respectively
thought of as approaches that reduce the quantity versus the quality of the gradient. One can think
of this in analogy to image compression. For example, JPEG image compression that is based on
discrete cosine transform determines both which transform coefficients to store (the quantity) and at
what level of resolution to store those coefficients (the quality).
Sign-based compression schemes such as Scaled-sign, SignSGD and Signum (Bernstein et al., 2018)
sit at the far end of quantization-based algorithms. Such schemes quantize real values to only two
levels, +1 and -1. For example, the compressing function of Scaled-sign takes in a vector x ∈ Rd,
and the decompressing function outputs the vector (∣χ∣ι∕d) sgn(χ). This means that the com-
pressed representation needs to store only the sign of each entry x[j], along with the scaling constant
kxk 1 ∕d. In practice one can avoid the 'zero' output of Sgn by mapping it to +1 or -1. This allows
the two outcomes +1 and -1 to be represented using one bit per entry, making the size of the com-
pressed representation d + 32 bits in total (assuming 32-bit single-precision representation of the
scaling constant). As per Shannon's source coding theorem (MacKay, 2003, p. 81), the sequence of
+1 and -1 can be further compressed without any information loss if the probability of encounter-
ing +1 is different from that for -1. However, in our experiments on Scaled-sign compression we
observe that both outputs are equally likely across all iterations.
Any lossy gradient compression scheme introduces noise, also known as distortion, in addition to
the measurement noise that is already present in the stochastic gradients computed by the workers. It
is reasonable to expect the additional compression error hurts the convergence rate of the algorithm.
However, it has been empirically observed that significant compression ratios can be achieved before
observing any impact on convergence (Seide et al., 2014; Alistarh et al., 2017). One can achieve
even greater compression while keeping the convergence rate nearly the same by employing error-
feedback (Stich et al., 2018; Karimireddy et al., 2019; Zheng et al., 2019). Algorithms based on
error-feedback accumulate the compression error in past iterations and add it to the input of the
2
Under review as a conference paper at ICLR 2021
compressing function. This allows all of the gradient information to get transmitted over a sequence
of iterations, albeit with a delay. For smooth functions the gradient does not change considerably,
therefore, the delay does not significantly impact the rate of convergence.
The dist-EF-SGD algorithm proposed by Zheng et al. (2019) is based on error-feedback and of-
fers two-way compression under the master-worker topology. The dist-EF-SGD algorithm provides
convergence guarantees for δ-compressors. An operator C : Rd → Rd is a δ-compressor for all
x ∈ Rd if kC(x) - xk2 ≤ (1 - δ)kxk2 for some δ ∈ (0, 1]. The δ is a measure of the distortion due
to applying C. A good compressor will have δ close to 1 and a large compression ratio. One can
show that Scaled-sign is a d-approximate compressor. The Scaled-sign compression scheme with
dist-EF-SGD offers convergence guarantees under standard assumptions even though Scaled-sign
without error-feedback may not converge (Karimireddy et al., 2019). Zheng et al. (2019) generalize
the dist-EF-SGD algorithm to include blockwise compression, in which the gradient is partitioned
into blocks that are compressed separately. A natural partitioning method is to consider as blocks
the elements of a deep neural network such as tensors, matrices and vectors. Blockwise compression
allows better exploitation of redundancy that may be present only within a block.
Zheng et al. (2019) empirically demonstrate that Scaled-sign with dist-EF-SGD achieves almost
the same performance of SGD with respect to training loss and test accuracy. This indicates that
we can allow more distortion in the compression before observing a significant impact on training
performance. The proposed SignXOR compression is based on allowing additional distortion in the
interest of achieving a higher compression ratio.
3	Proposed algorithm: SignXOR
Our proposal is motivated by the concept of delta encoding. Delta encoding refers to techniques
that store data as the difference between successive samples, rather than directly storing the samples
themselves (Smith et al., 1997). One often encounters delta encoding in applications such as integer
compression and video compression. For example, it is more space efficient to store the first-order
differences of a digitized audio signal than to store the values of the original signal. First-order dif-
ferences have smaller magnitudes compared to the original sequence and, therefore, the differences
can be represented by a comparatively smaller number of bits. A similar approach is used in the
inter-picture prediction method in high efficiency video coding (HEVC) (Sze et al., 2014). Inter-
picture prediction makes use of temporal correlations across video frames encoding the differences
between frames. This requires less storage compared to storing each video frame. Jiang et al. (2018)
employ delta encoding in a more related application to distributed optimization. At the core of their
algorithm, delta encoding is employed to compress an increasing sequence of integers.
Our SignXOR algorithm applies delta encoding to represent temporal changes in the gradient. In
essence, SignXOR maintains a binary vector that indicates whether the sign of a gradient entry is
equal (or not equal) to the sign of the corresponding entry in the previous gradient. The equality (or
non-equality) can be represented by a binary 1 (or 0). This procedure resembles the binary XOR
operation, hence the name SignXOR. We employ a generalized version of original dist-EF-SGD by
Zheng et al. (2019) (the original dist-EF-SGD is specified in Algorithm 2 therein) to provide conver-
gence guarantees for the proposed compression scheme. The generalization is to make dist-EF-SGD
compatible with SignXOR. We outline the proposed method in Algorithm 1 and Algorithm 2.
Generalized dist-EF-SGD: Algorithm 1 presents generalized dist-EF-SGD for setup with a mas-
ter and n workers. The three main differences between the generalized and original dist-EF-SGD
versions are as follows. First, Algorithm 1 delegates compression and decompression tasks to two
separate functions encode and decode. Second, Algorithm 1 maintains a vector gk at all nodes in-
cluding the master. The vector Uk is the average gradient all workers used to update the parameter
vector wk in the last iteration. Third, the encode and decode functions each take the last gradient
gUk as the second argument. This is in contrast to the original dist-EF-SGD algorithm in which com-
pression is based only on the gradient in the current iteration. Since there are no differences related
to compression performance between the original and generalized dist-EF-SGD algorithms, they
encompass the same theoretical guarantees.
The compressing function encode takes in two inputs and outputs Gki . This output is the actual
payload sent to master over the communication channel. In addition to Gki , the ith worker also
3
Under review as a conference paper at ICLR 2021
Algorithm 1: Generalized dist-EF-SGD compatible with SignXOR
input : initial parameter vector w0; step sizes {η0, . . . , ηT-1}
initialize: let 0 be the all-zeros vector of dimension d ;
let go ∈ Rd be a vector with entries sampled uniformly from [-1,1];
on ith worker store g° ; set e0 to 0 ; set parameter vector to w0 ;
on master store gg0 ; set e0 to 0 ;
1	for k ∈ {0, . . . , T - 1} do
2	on ith worker
3	compute ^k = gk + *ebk where gk is the stochastic gradient at Wk ;
4	compute Gk = encode(gk, gk) and gk = decode(Gk,gk);
5	send Gk to master and update error ek+∖ = ηk (gk - gk);
6	receive Gk from master and compute ggk+1 = decode(Gk, ggk) ;
7	update parameter vector Wk+ι = Wk - ηkgk+ι ;
8
9
10
11
12
on master
receive Gki and compute ggki = decode(Gki, ggk) for all i ∈ [n] ;
COmPUte ^k = n Pi∈[n] gk + ηkek ;
compute Gk = encode(gk, gk) and gk+1 = decode(Gk, gk);
broadcast Gk to all workers and update ek+1 = ηk(^k - gk+ι);
computes ggki which is what the master will obtain by decompressing Gki . The vector ggki is used to
update eik+1, the compression error fed back in the next iteration. The master collects Gki from all
workers and decompresses each to obtain ggki . All workers receive the master broadcast Gk, and input
it, along with ggk, to the decode function to decompress and obtain ggk+1. Note that in the kth iteration
all nodes use the same ggk vector as the second argument in the encode and decode functions.
The encode and decode functions corresponding to SignXOR compression scheme are specified in
Algorithm 2. For the ease of explanation we consider the specific case when the master compresses
gk to obtain Gk, and the workers decompress Gk to obtain gk+1 as an approximation to gk.
Algorithm 2: SignXOR compression and decompression
input: hyperparameter 0≤ α < 4 (1 - JI- d) <1;
1	function encode(x ∈ Rd , y ∈ Rd ):
2	compute r, the fraction of +1’s in sgn(x) ;
3	compute q, the fraction of elements in x such that sgn(x[j]) = sgn(y[j])	;
4	initialize binary vector b ∈ {0, 1}d to all-zeros ;
5	for all j ∈ [d], if sgn(x[j]) = sgn(y[j]) set b[j] = 1 with probability	1	-	α ;
6	compute p, the fraction of 1’s in b ;
7	compress b with a lossless scheme and compute scalar a = kxk1/d ;
8	output G = {a, compressed representation of b};
9	function decode(G = {a ∈ R, compressed representation ofa vector b ∈ {0, 1}d}, y ∈ Rd):
io expand and decompress G to obtain a and b ;
11 output a sgn(y) Θ (2b — 1);
Compressing function: We consider the case when master calls encode with arguments X = gk and
y = ggk. Note that the scalars r, q and p are not used anywhere in Algorithm 2. These scalars help
us describe the algorithm and also become useful in our theoretical analysis. The output of encode
function is random when α 6= 0, and deterministic when α = 0. Let us first consider α = 0, in which
case b[j] = 1 if and only if sgn(gk[j]) = sgn(gk[j]). This implies that P is the fraction of entries in
gk and gk that have the same signs. This is a measure of the (positive or negative) correlation between
4
Under review as a conference paper at ICLR 2021
the two vectors. The core idea in the proposed compression scheme is to compress the binary vector
b using a lossless compression scheme. Shannon’s source coding theorem (MacKay, 2003, p. 81)
states that d i.i.d. random variables each with entropy H(p) = -p log2 p - (1 - p) log2(1 - p) can
be compressed into approximately dH(p) bits with negligible risk of information loss, as d → ∞.
In our case, while the length of the parameter vector d is well over a million for models of practical
interest, the entries of b are not necessarily i.i.d.. However, we demonstrate in Section 5 that there
exist readily available lossless compression algorithms that can compress b to very close to dH (p)
bits, the Shannon limit. Note that binary entropy H(p) is symmetric around p = 0.5, and satisfies
0 ≤ H(p) ≤ 1 with H (0.5) = 1. When p ≈ 0.5 the size of the compressed representation gets
close to d, which is same as that offered by Scaled-sign. Further compression of b is only possible
if p is away from 0.5. In our experiments presented in Section 5.2 we observe that when α = 0, p
remain close to but slightly lower than 0.5. This implies a low correlation between ^k and Uk. We
remedy this issue by making α > 0. Next we explain how making α > 0 yields compression gains
and, at the same time, induces correlation between gk and gk.
First, note that increasing α introduces distortion to b, driving p away from 0.5 and towards 0. A
lower p decreases the entropy of b, yielding a higher compression ratio realized with the lossless
compressor. Since we start with a p slightly less than 0.5, if we were to drive p towards 1 we
would first increase H(p) before starting to incur compression gains. For this reason, we design the
encoder to push p towards zero. In summary, increasing α offers rate savings in the current iteration
by adding distortion to the current gradient.
Second, we explain how added distortion induces correlation between gk and gk. Note that q is
a measure of correlation between the two vectors sgn(^k) and sgn(gk) measured prior to adding
distortion using α. We emphasize that q does not depend on the errors introduced in the current iter-
ation. Rather, q depends on α only through the past iterations. In the experimental results presented
in Section 5.2 we observe that increasing α also decreases q. This means that in a given iteration the
inputs to the encoder are already correlated. Therefore, the b vector that encodes equality (or non-
equality) of sgn(gQ and sgn®) can be compressed even without adding distortion in the current
iteration. The underlying mechanism that causes this temporal correlation in our encoder is error-
feedback. Recall that the idea behind error-feedback is to keep track of the compression error in the
last iteration and add it back to the input of the encoder in the current iteration (with correction for
the step size). Specifically, gk includes the compression error incurred in gk This feedback system
induces the temporal relation between the two vectors that we see through q. In summary, our com-
pression mechanism interacts with the error-feedback mechanism to increase temporal correlation
which we then exploit to realize further compressive gains.
The suggested upper bound for α ensures theoretical convergence of the SignXOR algorithm. We
show in Section 5 that in practice α can be increased considerably more than the suggested upper
bound before seeing an impact on the training performance.
Decompressing function: Let us now consider the case when a worker calls decode with arguments
G = Gk and y = gUk . The first argument Gk contains the scalar a and the compressed representation
of binary vector b, which can be recovered exactly as compression of b is lossless. Noting that b
is computed with gUk as the second argument to the encode function, the output of the decoder is
obtained by inverting the sign of gUk [j] whenever b[j] is 0, and by scaling the result by a. One can
compactly express the output of this operation as a sgn(gUk) (2b - 1).
Remarks: Note that we can recover the Scaled-sign compression scheme by setting α = 0 in
Algorithm 2. In comparison to SignXOR, the compressed representation of Scaled-sign stores the
sign of each entry in ^k, and the scaling constant a = IlgkkI/d. We demonstrate in Section 5.2
that r, the fraction of +l's in sgn(^Q, is approximately 0.5 across all iterations. This means that
we cannot compress further the sequence of signs, and the encoded representation of Scaled-sign
requires at least d bits. Algorithm 1 can be easily extended to accommodate blockwise compression
as was explained by Zheng et al. (2019). In blockwise compression the gradient gk is partitioned
into blocks, and the blocks are processed separately using encode and decode functions. In our
numerical experiments we employ the blockwise extension of Algorithm 1.
4 Theoretical guarantees
5
Under review as a conference paper at ICLR 2021
In this section we summarize our theoretical results on Algorithm 1 and Algorithm 2. First, we
prove that the SignXOR compression scheme presented in Algorithm 2 is a δ-compressor. Second,
We show that for any δ-compressor the generalized dist-EF-SGD scheme in Algorithm 1 converges
at the same O(1/√T) rate as SGD. Putting these two together yields the desired result.
SignXOR is a δ-compressor: We consider the general form of encode and decode functions, i.e.,
with inputs (x, y) for encode, and with inputs (G , y) for decode. Let us define the operator Cαy :
Rd → Rd where Ca(x) = asgn(y) Θ (2b — 1) with a = kχk1 and the binary vector b is such that
b = 1 with probability 1 - α if sgn(x) = sgn(y), and b = 0 otherwise. Note that a and b have the
same meaning as in Algorithm 2. The operator Cαy (x) is representative of decode(encode(x, y), y),
therefore, Cαy (x) is the SignXOR compressor. Since Cαy is a randomized operator, we show that it
is a δ-compressor in expectation as stated in Theorem 1.
Theorem 1.	There exist a δ ∈ (0, 1] such that E[kCαy (x) — xk2] ≤ (1 — δ)kxk2 for all y ∈ Rd if
2
α < 4 (1 - J1 - 1).
The proof is provided in Appendix A.1. Although the suggested upper bound for α is extremely
small for large d, we demonstrate in Section 5 that in practice α can be set quite close to 1. Next we
discuss the convergence rate of dist-EF-SGD for an arbitrary δ-compressor.
Convergence rate of dist-EF-SGD: Zheng et al. (2019) compare the convergence rates of dist-EF-
SGD and vanilla SGD in their Corollary 1. As authors note, although the convergence rates of the
two algorithms are same in O notation, the former is slower by a constant factor. The differences
between the bounds for dist-EF-SGD and vanilla SGD in Corollary 1 are as follows. In both cases,
for large T the dominant terms are those with √T in denominator. The ratio between the dominant
terms corresponding to dist-EF-SGD and vanilla SGD is 1.5, suggesting that the former is always
slower than the latter by a factor of 1.5. In our Theorem 2 we strengthen the bound for dist-EF-
SGD so that the dominant terms in the two algorithms are the same. This means that dist-EF-
SGD achieves same convergence rate as vanilla SGD in the limit as T → ∞. Our proof is for an
arbitrary δ-compressor that is not necessarily SignXOR. To this end we consider Algorithm 1 and
define C(x) = decode(encode(x, y), y). We assume that C is a δ-compressor for all y ∈ Rd, i.e.,
E[kC(x) — xk2] ≤ (1 — δ)kxk2 for some δ ∈ (0, 1]. We setup the optimization problem next.
Let F : Rd → R be a function that is lower bounded by F and has a gradient VF. We con-
sider a distributed optimization setup with a master and n workers. For some wk ∈ Rd the ith
worker calculates the stochastic gradient gki at wk . We assume that gki is an unbiased estimate of
VF(wk), and that gki has bounded variance. Specifically, we assume that gki satisfies the two prop-
erties E[gki |wk] = VF(wk) and E[kgki — VF(wk)k2] ≤ σ2. We also assume that F is L-Lipschitz
smooth, and that the gradient of F is bounded. The latter implies that E[kgki k2] ≤ G2 for some scalar
G. The master and workers generate a sequence {w0, . . . , wT } as per Algorithm 1. Convergence
results of this system is summarized in Theorem 2.
Theorem 2.	For a given w° and a step size schedule 加=-√~^(1 — 2τ⅛∕4), the convergence of
the system outlined in Algorithm 1 after T iterations is given by
E min	kVF(wk)k2
k=0,...,T -1
2L(F(w0)- f*) + σ
2√T — 1
1 2L(F (wo)— F*) + 8(1-δ)G2 (1 + 吊)
+	2T 3/4 — T1/4
+O
(1)
We defer the proof of Theorem 2 to Section A.2 in the Appendix. In comparison, the bound for
SGD has only the first term in (1). Note that the last two terms in (1) converge to zero faster than
the first term. This means that dist-EF-SGD asymptotically achieves the same convergence rate
as SGD. Since SignXOR is a δ-compressor we conclude that the proposed algorithm converges
asymptotically at the same rate as SGD.
6
Under review as a conference paper at ICLR 2021
Table 1: Details of experiments and summary of findings.
Dataset	Model	Parameters	Workers	Batch size	BX/BS
CIFAR-100	ResNet-18	12,596,388	4	32	42%
ImageNet-32	WRN-28-2	1,595,320	4	64	43%
ImageNet	ResNet-50	25,583,592	16	16	47%
5	Numerical experiments
We perform multi-GPU experiments on three datasets: CIFAR-100, ImageNet-32 and ImageNet.
The ImageNet-32 dataset (Chrabaszcz et al., 2017) is a down-sampled version of the ImageNet
dataset in which the images are of size 32 × 32 pixels. All other properties such as the number of
images and the number of classes are the same as the original ImageNet dataset. We experiment
with ImageNet-32 as an alternative to ImageNet due to the latter’s heavy consumption of hardware
resources. The additional experiments presented in Section 5.2 are performed with ImageNet-32.
We train a wide residual network classifier (Zagoruyko & Komodakis, 2016) of depth 28 and width
2 (WRN-28-2) using the ImageNet-32 dataset. The CIFAR-100 and ImageNet datasets are used to
train the ResNet-18 and ResNet-50 (He et al., 2016) models respectively. In all cases the dataset is
partitioned to equal sizes and distributed among the workers.
We implement all algorithms using the TensorFlow and Horovod (Sergeev & Balso, 2018) packages.
Since the communications component in Horovod is designed for a master-less setup, we simulate
a master-worker environment in our implementation. We choose to use the LemPel-Ziv-Markov
chain algorithm (LZMA) as the lossless scheme that compresses the binary vector b. The LZMA
compression scheme is a member of the Lempel-Ziv family of compression algorithms (MacKay,
2003, p. 119). While there exist other compression schemes in which for large d the compression
ratio converge to H(p) much faster than the Lempel-Ziv algorithms, in our experiments we employ
LZMA as its implementation is readily available in Python. We refer the reader to Appendix Sec-
tion A.3 for an comparison of H(p) and the compression ratio of LZMA. In our implementation the
encode and decode functions are not tuned to achieve best computational performance. We, there-
fore, do not compare or plot measurements with respect to time. In our experimental setup, a single
machine hosts four workers and each worker runs on a dedicated Tesla P100 16GB GPU.
We compare the performance of SignXOR to SGD and dist-EF-SGD with Scaled-sign compression.
For Scaled-sign we use the original dist-EF-SGD scheme as the error-feedback mechanism. For
SignXOR we use the generalized dist-EF-SGD outlined in Algorithm 1. Note that there are no
compression performance related differences between the original and the generalized versions of
dist-EF-SGD. In both cases we use blockwise compression, in which the gradients corresponding
to tensors, matrices and vectors are compressed and decompressed separately. Training is started
with a learning rate of 0.1, and the learning rate is multiplied iteratively by 0.1 at certain epochs.
These epochs are multiples of 8, 10 and 5 for the experiments involving the CIFAR-100, ImageNet-
32, and ImageNet datasets, respectively. We use an `2 weight regularizer in all experiments. The
regularizer is scaled by 10-3 in CIFAR-100 experiments, and by 10-4 in the other experiments.
Table 1 summarizes some additional details of the experiments. Batch size is per worker.
We define the following measurement to help us interpret our experimental results. Recall that in
Algorithm 1 Gki and Gk are the payloads that workers send and receive. Let sk and sik denote the
number of bits required to store Gk and Gki in bits. Over T iterations with SignXOR, each worker on
average sends and receives a total of BX = 1 PT-(I Pi∈[n](sk + Sk) bits. In contrast, if Scaled-sign
compression is used each worker communicates d bits in each iteration, making the total number
of bits sent and received by each worker BS = 2Td. Therefore, BX/BS is the ratio between the
total bit usage of each algorithm. We tune the hyperparameter α in Algorithm 2 so that SignXOR
achieves the same test accuracy as Scaled-sign within the same number of epochs.
7
Under review as a conference paper at ICLR 2021
Figure 1: The three columns present experimental results for CIFAR-100 (left), ImageNet-32 (mid-
dle), ImageNet (right). In each case the three rows present plots of training loss (top), test accuracy
(middle) and BX/BS (bottom). The test accuracy plots indicate top-1 test accuracy for CIFAR-100,
and top-5 for the other two. We run SignXOR experiments for multiple α values.
5.1	Main results
Figure 1 presents results from multiple runs of the experiment with different values for α. For the
CIFAR-100 dataset, the SignXOR test accuracy plot corresponding to α = 0.7 closely follows test
accuracy of Scaled-sign and SGD. The same observation is true for plots corresponding to α = 0.7
and α = 0.6 in ImageNet-32 and ImageNet experiments respectively. For these choices of α, the
plots of BX/BS indicate that in each case SignXOR achieves the top test accuracy with less than
50% of communication required by Scaled-sign. The last column in Table 1 summarizes the lowest
BX/BS that SignXOR achieves while attaining the same test accuracy as Scaled-sign by the end
of training. Figure 1 also indicates that we can make α slightly larger than the choices listed in
Table 1 to achieve an even smaller BX /BS with some reduction in the test accuracy. For example,
the ImageNet-32 experiment corresponding to α = 0.9 achieves within 0.3 test accuracy of Scaled-
sign, but only using 12% of the communications usage of Scaled-sign.
5.2	Details of SignXOR results
Figure 2 presents the evolution of r, q and p in Algorithm 2 for experiments related to ImageNet-
32 dataset. The left-sub figure in Figure 2 demonstrates that r, the fraction of +1's in sgn(^k) is
approximately 0.5 across all iterations. Consequently, the sequence of signs in sgn(gQ cannot be
further compressed. Note that this observation is true for all α. Since α = 0 recovers the Scaled-
sign compression, we conclude that the output of Scaled-sign cannot be further compressed using a
lossless compression scheme. The middle-sub figure in Figure 2 plots q, the fraction of entries in ^k
such that sgn(gk[j]) = Sgn(Uk j])∙ Note that q remains close to 0.5 when α = 0, and goes down
as α increases. Recall that q is computed prior to adding distortion using α. This implies that the
naturally present correlation between the two vectors sgn(^Q and sgn。), i.e. temporal correlation,
increases with α. The right-sub figure includes plots ofp, the fraction of 1’s in b. Note that as per
Algorithm 2 we have the relationship E[p] = q(1 - α).
8
Under review as a conference paper at ICLR 2021
Figure 2: Evolution of r, q and p in Algorithm 2 for experiments conducted using the ImageNet32
dataset. Values are collected When the master calls the encode function with arguments X = gk and
y = gk. The peaks at epochs 10 and 20 are due to the drop in the learning rate at these epochs.
6	Conclusions and future work
In this paper we propose SignXOR, a novel compression algorithm that exploits the temporal cor-
relation of consecutive gradients in SGD. The proposed algorithm builds off Scaled-sign (Bernstein
et al., 2018; Karimireddy et al., 2019) and dist-EF-SGD (Zheng et al., 2019). The SignXOR algo-
rithm combines amplitude quantization with the exploitation of temporal correlation. We accomplish
this in two stages. The first employs Scaled-sign which quantizes gradient entries into two levels,
and the second compresses by exploiting temporal correlation of quantized gradients. Itis interesting
to investigate how the second stage can be improved to accommodate more elaborate compression
mechanisms beyond a two-level quantizer, such as QSGD (Alistarh et al., 2017), Top-k (Stich et al.,
2018; Lin et al., 2018) and Spectral-ATOMO (Wang et al., 2018). Also, we conjecture that one must
be able to compress better by jointly considering the two stages. This way, one can exploit in the
second stage some of the information embedded in the gradient that may have been removed in the
first stage. We leave these two tasks as future work.
9
Under review as a conference paper at ICLR 2021
References
Alham Fikri Aji and Kenneth Heafield. Sparse communication for distributed gradient descent.
arXiv preprint arXiv:1704.05021, 2017.
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic. Qsgd:
Communication-efficient sgd via gradient quantization and encoding. In Advances in Neural
Inf Proc. Sys.,pp.1709-1720, 2017.
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar. signsgd:
Compressed optimisation for non-convex problems. July 2018.
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter. A downsampled variant of imagenet as an
alternative to the cifar datasets. arXiv preprint arXiv:1707.08819, 2017.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, Oct
2018.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770-778, 2016.
Jiawei Jiang, Fangcheng Fu, Tong Yang, and Bin Cui. Sketchml: Accelerating distributed machine
learning with data sketches. In Proc. Int. Conf. on Management of Data, pp. 1269-1284, May
2018.
Sai Praneeth Karimireddy, Quentin Rebjock, Sebastian U Stich, and Martin Jaggi. Error feedback
fixes signsgd and other gradient compression schemes. Int. Conf. Machine Learning, Jan 2019.
Yujun Lin, Song Han, Huizi Mao, Yu Wang, and William J Dally. Deep gradient compression:
Reducing the communication bandwidth for distributed training. In Int. Conf. Learning Repre-
sentations, Vancouver, May 2018.
David JC MacKay. Information theory, inference and learning algorithms. Cambridge university
press, 2003.
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu. 1-bit stochastic gradient descent and
its application to data-parallel distributed training of speech dnns. In Conf. Int. Speech Comm.
Association, Singapore, September 2014.
Alexander Sergeev and Mike Del Balso. Horovod: fast and easy distributed deep learning in Ten-
sorFlow. arXiv preprint arXiv:1802.05799, 2018.
Steven W Smith et al. The scientist and engineer’s guide to digital signal processing. 1997.
Sebastian U Stich, Jean-Baptiste Cordonnier, and Martin Jaggi. Sparsified sgd with memory. In
Advances in Neural Inf. Proc. Sys., pp. 4447-4458, 2018.
V. Sze, M. Budagavi, and G.J. Sullivan. High Efficiency Video Coding (HEVC): Algorithms and
Architectures. Springer International Publishing, 2014.
Hongyi Wang, Scott Sievert, Shengchao Liu, Zachary Charles, Dimitris Papailiopoulos, and Stephen
Wright. Atomo: Communication-efficient learning via atomic sparsification. In Advances in
Neural Inf. Proc. Sys., pp. 9850-9861, 2018.
Jianqiao Wangni, Jialei Wang, Ji Liu, and Tong Zhang. Gradient sparsification for communication-
efficient distributed optimization. In Advances in Neural Information Processing Systems, pp.
1299-1309, 2018.
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Terngrad:
Ternary gradients to reduce communication in distributed deep learning. In Advances in neural
information processing systems, pp. 1509-1519, 2017.
10
Under review as a conference paper at ICLR 2021
Jiaxiang Wu, Weidong Huang, Junzhou Huang, and Tong Zhang. Error compensated quantized sgd
and its applications to large-scale distributed optimization. Int. Conf. Machine Learning, June
2018.
Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint
arXiv:1605.07146, 2016.
Shuai Zheng, Ziyue Huang, and James Kwok. Communication-efficient distributed blockwise mo-
mentum SgdWitherror-feedback. In Advances in Neural Inf. Proc. Sys. ,pp. 11446-11456, 2019.
11
Under review as a conference paper at ICLR 2021
A Appendix
A.1 Proof of Theorem 1
Proof. We have
kCαy(x) - xk = ka sgn(y)	(2b- 1) - xk
= ka sgn(y)	(2b - 1) - a sgn(x) + a sgn(x) - xk
≤ aksgn(y)	(2b- 1) - sgn(x)k + ka sgn(x) -xk,
where the last step follows from the subadditivity of '2-norm. Squaring both sides and taking the
expectation with respect to the randomness in Cαy yields
E[kCαy (x) - xk2] ≤ E[a2ksgn(y)	(2b - 1) - sgn(x)k2] + ka sgn(x) - xk2
+ 2E[aksgn(y)	(2b- 1) - sgn(x)k]ka sgn(x) - xk	(2)
Next we bound each term in (2). In the following we use the property kxk ≤ ∣∣x∣∣ι ≤ √d∣∣x∣∣,which
shows that kxk1 is both lower and upper bounded by kxk to within a constant factor.
For the second term in (2) we have
ka Sgn(X) — xk2 = kxk 1 Sgn(X) — X
d
=kdɪ Sgn(X)	— 2(kxdk1 Sgn(X),x)+ I∣xk2
=kXi1 ∣sgn(X)k2 — 2 kd1 hsgn(X),Xi + ∣∣x∣∣2,
which can be simplified and upper bounded as
kaSgn(X) — Xk2 =呼—2呼 + ∣x∣2 = ||，『一呼 ≤ ||，『—呼=SkXk2
for s =(1 — d). The first equality is obtained by noting that ∣∣Sgn(X)k2 = d and(Sgn(X),X)=
kXk1. The inequality is due to kXk ≤ kXk1. Note that a Sgn(X) is the Scaled-sign compressor, which
is a d-approximate compressor since 0 < d ≤ 1.
For the first term in (2) we have
d
E[a2kSgn(y)	(2b— 1) —Sgn(X)k2] = a2 XE[(Sgn(y[j])(2b[j] — 1) — Sgn(X[j]))2]	(3)
j=1
= a2 4dqα
≤ 4qαkXk2,
where q is as defined in Algorithm 2. Recall that q is the fraction of elements in X such that
Sgn(Xj]) = Sgn(y[j ]). The last step is obtained by noting that a2d = kXk1 ≤ ∣∣x∣2. The sec-
ond equality is obtained with the following reasoning. The d elements in summation can be split
into three sets. In the first set we have d(1 — q) elements satisfying Sgn(X[j]) 6= Sgn(y[j]), which
will cause b[j] = 0 and Sgn(y[j])(2b[j] — 1) — Sgn(X[j]) = 0. The rest of the dq elements satisfy
Sgn(X[j]) = Sgn(y[j]) and can be further divided into two sets. Out of these dq elements, in ex-
pectation an 1 — α fraction have b = 1, again making Sgn(y[j])(2b[j] — 1) — Sgn(X[j]) = 0. Such
elements make up the second set. The third set consists of the rest of the elements. Specifically, out
of the dq, we have in expectation an α fraction with b = 0. For the elements in this third set we have
Sgn(y[j])(2b[j] — 1) 6= Sgn(X[j]). This gives us (Sgn(y[j])(2b[j] — 1) —Sgn(X[j]))2 = 4duetothe
sign mismatch.
12
Under review as a conference paper at ICLR 2021
For the last term in (2) we have
E[aksgn(y) © (2b- 1) - sgn(x)k] = aE KX(Sgn(y[j])(2b[j] — 1) - sgn(x[j]))2 j
( d	Y"
≤ (a2 ^2E[(sgn(y[j,])(2b[j] - 1) - sgn(x[j]))2]J
≤ ρ4qα∣∣xk∙
The first inequality is obtained by applying Jensen’s inequality to the concave square root function,
and the second inequality follows from (3).
Now we substitute the bounds for all terms in (2) to get
E[kcα(x) - xk2] ≤ (4qα + S + 2√4qɑ√s)kxk2 ≤ (√4α + √S)2∣∣xk2.
In the last step We remove the dependency on q as it is a function of x. The inequality is obtained
by noting that q ≤ 1 for all X and y. Finally, letting (√4α + √s)2 = 1 一 δ we find the conditions
for a so that 0 < δ ≤ 1. Note that δ trivially satisfies δ ≤ ɪ < 1 for all a ≥ 0. The lower bound of
δ is satisfied if 0 < 1 一 (√4α + √s)2 which yields the bound ɑ < 4(1 一 √s)2. This concludes the
proof.
□
A.2 Proof of Theorem 2
We start with the proof of Theorem 1 by Zheng et al. (2019) provided in their Appendix C. In the
following analysis we adhere to the same notation as in Theorem 1 to make the comparison easier.
The following equivalencies hold in their and our notation: t = k, ηt = ηk, xt = wk, M = n,
et = nk-i ek,and et,i = nk-i ek.
Proof. At the end of the first half of the proof on page 14 (Zheng et al., 2019), for ρ > 0, authors
obtain
E[F(Xt+ι)] ≤ F(Xt) — ηt(1 — Lηt2+ρ) kVF(Xt)k2 + LηMσ2 + ηηt-L2 et + MM Xet,i
2	2M	2ρ	M
i=1
Let us define λ = 1/p > 0 and C = 1 一 代.By taking total expectation and then applying Lemma
6 by Zheng et al. (2019) with μ = 0 we get
E[F(Xt+ι)] ≤ E[F(Xt)] -ηt (C- Lnt) E hkVF(“)『]+ LnM2
,IWnLIL2λ 8(1 - δ)G2 A 1 ʌ
+	2	δ2	1+ δ2).
Taking telescoping summation over T iterations and rearranging the terms give
E _nmin_ JVF (xt)k2
t=0,...,T -1
≤
F(χ∩) - F + Lσ2 PT-1 rιi2 + 4λL2(I-6G2(1 + ɪʌ PTTrI ri2 ∣
F (X0)	F* + 2M 乙t=0 〃t +	δ2	11+ δ21 乙t=0 〃t〃t-1
PT- nt (C- 等)
where we have asserted that E[F (Xe0)] = F(X0) and that E[F (XeT)] ≥ F*. Next we pick the step
size schedule n = Lc^ and rearrange terms to get
E min	kVF(Xt)k2
t=0,...,T -1
≤
普(F(X0)- F*) + M + 8cλ(1δ-δ)G2 (1 + δ2)
2√T - 1	2T - √T
(4)
13
Under review as a conference paper at ICLR 2021
In comparison to (4), the analogous bound for SGD is
E	min	kVF (χt)k2
t=0,...,T -1
2
2L(F(XO) - FJ + M
2√T - 1
(5)
For large T, the second term in (4) vanishes at a faster rate than the first term. However, we cannot
make a direct comparison between (5) and the first term in (4) when c2 6= 1. Setting an arbitrarily
large value for λ gives c2 ≈ 1, but we pay a penalty with a large cλ in second term in (4). The
solution is to make c asymptotically converge to 1 by carefully choosing a λ that grows with T. This
way c → 1 and the first term in (4) becomes directly comparable to (5). We also want the cλ in the
second term of (4) small enough so that it vanishes faster than the first term in (4).
Let λ = TS for some s > 0. We solve for S by expanding cλ and * in (4). We have cλ = λ 一 ɪ =
TS — 2. We take the following approach to expand ±. For Z ∈ R and positive integer r, from the
negative binomial series we have
(1 - z)-r
∞
Xr+pp-
p=0 p
For r = 2 and |z | < 1 we can write
(1 一 z)-2 = 1 + 2z + 3z2 +4z3 + …< 1 + 2z + O(z2).
Since 2λ =奈 < 1, we can write
< 1 + λ-1 + O (λ-2) = 1+ T-S + O (T-2s).
Substituting the result back in (4) and rearranging terms gives
E min	kVF(xt)k2
k=0,...,T -1
2
2L(F (WO) — F*) + M
2√T 一 1
+ (T-S + O (T-2S))
2T 一 TT	δ2
2L(F(WO)- F*)
2√T 一 1
1	8(1 一 δ)G2
(6)
1 + δ2
<
2
2L(F (WO) — Fj + M
2 √T - 1
+ O (T-S-1) + O (T-2s-2) + O (TST) + O (T-1).
We would like to pick an s that makes all terms in the last row go to zero in the same asymptotic
rate. The third term requires s < 1. For 0 < s < 1 the first and the third terms are the slowest to go
to zero, therefore, they determine s. Setting -S — ɪ = S 一 1 gives S = 4. One can substitute S = 4
in (6) to obtain
E min	kVF(xt)k2
k=O,...,T -1
2
2L(F (WO) — Fj + M
2√T - 1
2L(F(wo) - F*) + 8(1-δ)G2
+	2T 3/4 — T1/4^^
(1 + δ2)
+O
where we have incorporated two O 谆)into one. Changing the notation to our original yields the
result in (1), concluding the proof.
□
A.3 Performance of LZMA compression
Figure 3 compares the compression ratio of LZMA algorithm with the theoretical lower bound. We
generate a binary sequence of length 107 by sampling from the Bernoulli distribution with proba-
bility p, compress the sequence using LZMA, and plot the compression ratio versus p. The entropy
H(p) is a lower bound for the compression rate.
14
Under review as a conference paper at ICLR 2021
Figure 3: Compression performance of the LZMA algorithm on a random binary sequence.
A.4 Impact of batch size on temporal correlation
The stochastic gradient workers compute is noisy. This is because workers use only a small mini-
batch out of a large dataset. We expect the gradient to be less noisy when the size of the mini-
batch is larger, and we expect less noisy gradients to be more (positively) correlated across time.
First, note that as per Algorithm 1, gk is the sum of the stochastic gradient and the compression
error in the last iteration. Second, as noted in Algorithm 2 q is the fraction of entries such that
sgn(gk[j]) = sgn(gk[j]). Although gk and Uk are not stochastic gradients themselves, we expect
their correlation (measured with q) to be impacted by the minibatch size. As q is central to our pro-
posed compression scheme, we explore numerically how q changes with the minibatch size. Recall
that we first observed q in the central sub-figure in Figure 2. To this end we rerun the ImageNet32-
SignXOR experiments described in Section 5.1 for 4 different minibatch sizes, and for two values
of α. Figure 4 summarizes the new results plotted versus the step. The first and second rows corre-
spond to experiments with α = 0 and α = 0.7 respectively. Training is started with a learning rate
of 10-4, and the learning rate is multiplied iteratively by 0.1 at certain epochs when the loss starts
to plateau. These epochs are multiples of 10, 15, 30 and 35 for the experiments with batch sizes 16,
32, 64, 128 respectively.
We observe that for large minibatch sizes the loss decreases faster. This is because the gradients
are less noisy for large minibatches. We also observe that a larger minibatch result in a larger q.
The reason for this observation is that, as stochastic gradient gets less noisy, the stochastic gradients
within gk and gk get more (positively) correlated (recall that gk is the sum of the stochastic gradient
and the compression error). This results in an increment of q, the fraction of components in ^k and
gUk that share the sign. We also observe in Figure 4 that this increment is minimal. For α = 0.7 we
only observe a maximum difference of around 0.005 between the two q plots corresponding to batch
sizes 16 and 128. In summary, q is less responsive to the changes of the batch size.
15
Under review as a conference paper at ICLR 2021
b
0.44
0.42-
0.46-
0	2	4	0	2	4
Step	le5	Step	ɪɑʒ
S
S

3-
Figure 4: Assessing the impact of batch size on q in SignXOR. The results from four different
minibatch sizes are presented as indicated in the legends. The first and second rows correspond to
experiments with α = 0 and α = 0.7 respectively.
10
9
8-
7-
6
5-
4-
O 4	8	12
Epoch
0	4	8	12
Epoch
Figure 5:
the x-axis
This figure is same as the first two sub-figures in the third column in Figure 1 except that
runs for a larger number of epochs.
16