Under review as a conference paper at ICLR 2021
NCP-VAE: Variational Autoencoders with
Noise Contrastive Priors
Anonymous authors
Paper under double-blind review
Ab stract
Variational autoencoders (VAEs) are one of the powerful likelihood-based generative mod-
els with applications in various domains. However, they struggle to generate high-quality
images, especially when samples are obtained from the prior without any tempering. One
explanation for VAEs’ poor generative quality is the prior hole problem: the prior distribu-
tion fails to match the aggregate approximate posterior. Due to this mismatch, there exist
areas in the latent space with high density under the prior that do not correspond to any
encoded image. Samples from those areas are decoded to corrupted images. To tackle this
issue, we propose an energy-based prior defined by the product of a base prior distribution
and a reweighting factor, designed to bring the base closer to the aggregate posterior. We
train the reweighting factor by noise contrastive estimation, and we generalize it to hierar-
chical VAEs with many latent variable groups. Our experiments confirm that the proposed
noise contrastive priors improve the generative performance of state-of-the-art VAEs by a
large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.
1	Introduction
Variational autoencoders (VAEs) (Kingma & Welling, 2014; Rezende et al.,
2014) are one of the powerful likelihood-based generative models that have ap-
plications in image generation (Brock et al., 2018; Karras et al., 2019; Razavi
et al., 2019), music synthesis (Dhariwal et al., 2020), speech generation (Oord
et al., 2016; Ping et al., 2020), image captioning (Aneja et al., 2019; Deshpande
et al., 2019; Aneja et al., 2018), semi-supervised learning (Kingma et al., 2014;
Izmailov et al., 2020), and representation learning (Van Den Oord et al., 2017;
Fortuin et al., 2018). Although there has been tremendous progress in improv-
ing the expressivity of the approximate posterior, several studies have observed
that VAE priors fail to match the aggregate (approximate) posterior (Rosca
et al., 2018; Hoffman & Johnson, 2016). This phenomenon is sometimes de-
scribed as holes in the prior, referring to regions in the latent space that are not
decoded to data-like samples. Such regions often have a high density under the
prior but have a low density under the aggregate approximate posterior.
The prior hole problem is commonly tackled by increasing the flexibility of the
prior via hierarchical priors (KlUshyn et al., 2019), autoregressive models (GUlrajani et al., 2016), a mixture
of approximate posteriors (Tomczak & Welling, 2018), normalizing flows (XU et al., 2019; Chen et al.,
2016), resampled priors (Bauer & Mnih, 2019), and energy-based models (Pang et al., 2020; Vahdat et al.,
2018b;a; 2020). Among them, energy-based models (EBMs) (Du & Mordatch, 2019; Pang et al., 2020) have
shown promising results in learning expressive priors. However, they require running iterative MCMC steps
during training which is computationally expensive, especially when the energy function is represented by a
J∖7∖^
Figure 1: We propose an
EBM prior using the prod-
uct of a base prior p(z) and
a reweighting factor r(z),
designed to bring the base
prior closer to the aggre-
gate posterior q(z).

1
Under review as a conference paper at ICLR 2021
neural network. Moreover, they scale poorly to hierarchical models where an EBM is defined on each group
of latent variables.
Our key insight in this paper is that a trainable prior is brought as close as possible to the aggregate posterior
as a result of training a VAE. The mismatch between the prior and the aggregate posterior can be reduced by
simply reweighting the prior to re-adjust its likelihood in the area of mismatch with the aggregate posterior.
To represent this reweighting mechanism, we formulate the prior using an EBM that is defined by the product
of a reweighting factor and a base trainable prior as shown in Fig. 1. We represent the reweighting factor
using neural networks and the base prior using Normal distributions.
Instead of expensive MCMC sampling, We use noise contrastive estimation (NCE) (Gutmann & Hyvarinen,
2010) for training the EBM prior. We show that NCE naturally trains the reweighting factor in our prior by
learning a binary classifier to distinguish samples from a target distribution (i.e., samples from the approxi-
mate posterior) vs. samples from a noise distribution (i.e., the base trainable prior). HoWever, since NCE’s
success depends on hoW close the noise distribution is to the target distribution, We first train the VAE With
the base prior to bring it close to the aggregate posterior. And then, We train the EBM prior using NCE.
In this paper, We make the folloWing contributions: i) We propose an EBM prior termed noise contrastive
prior (NCP) Which is trained by contrasting samples from the aggregate posterior to samples from a base
prior. NCPs are learned as a post-training mechanism to replace the original prior With a more flexible prior,
Which can improve the generative performance of VAEs With any structure. ii) We also shoW hoW NCPs are
trained on hierarchical VAEs With many latent variable groups. We shoW that training hierarchical NCPs
scales easily to many groups, as they are trained for each latent variable group in parallel. iii) Finally, We
demonstrate that NCPs improve the generative quality of VAEs by a large margin across datasets.
2	Background
We first revieW VAEs, their extension to hierarchical VAEs, and the prior hole problem.
Variational Autoencoders: VAEs learn a generative distribution p(x, z) = p(z)p(x|z) Where p(z) is a prior
distribution over the latent variable z and p(x|z) is a likelihood function that generates the data x given z.
VAEs are trained by maximizing a variational loWer bound on the log-likelihood log p(x):
logP(X) ≥ EZ〜q(z∣χ)[logp(x∣z)] - KL(q(z∣x)∣∣p(z)) := LVAE(X),	(1)
where q(z∣x) is an approximate posterior and KL is the Kullback-Leibler divergence. The final training
objective is formulated by Epd(x) [LVAE(x)] Where pd(x) is the data distribution (Kingma & Welling, 2014).
Hierarchical VAEs (HVAEs): To increase the expressivity of both prior and approximate posterior, ear-
lier work adapted a hierarchical latent variable structure (Vahdat & Kautz, 2020; Kingma et al., 2016;
S0nderby et al., 2016; Gregor et al., 2016). In HVAEs, the latent variable Z is divided into K separate
groups, z = {z1, . . . , zK}. The approximate posterior and the prior distributions are then defined by
q(z|x) = QkK=1 q(zk|z<k, x) and p(z) = QkK=1p(zk|z<k). Using these, the training objective becomes:
K
LHVAE(x) := Eq(Z|x)[log p(x|z)] -	Eq(Z<k |x) [KL(q(zk|z<k, x)||p(zk|z<k))] ,	(2)
k=1
where q(z<k|x) = Qik=-11 q(zi|z<i, x) is the approximate posterior up to the (k - 1)th group1.
The Prior Hole Problem: Let q(z) , Epd(x) [q(z|x)] denote the aggregate (approximate) posterior. In
Appendix B.1, we show that maximizing Epd(x) [LVAE(x)] with respect to the prior parameters corresponds
to bringing the prior as close as possible to the aggregate posterior by minimizing KL(q(z)||p(z)) w.r.t. p(z).
Formally, the prior hole problem refers to the phenomenon that p(z) fails to match q(z).
1For k = 1, the expectation inside the summation is simplified to KL(q(z1 |x)||p(z1)).
2
Under review as a conference paper at ICLR 2021
3	Noise Contrastive Priors (NCPs)
One of the main causes of the prior hole problem is the limited expressivity of the prior that prevents it
from matching the aggregate posterior. Recently, energy-based models have shown promising results in
representing complex distributions. Motivated by their success, we introduce the noise contrastive prior
(NCP) pncp(z) = 1 r(z)p(z), where p(z) is a base prior distribution, e.g., a Normal, r(z) is a reweighting
factor, and Z = r(z)p(z)dz is the normalization constant. The function r : Rn → R+ maps the latent
variable z ∈ Rn to a positive scalar, and can be implemented using neural nets.
The reweighting factor r(z) can be trained using MCMC sampling as discussed in Appendix A. However,
MCMC requires expensive sampling iterations that scale poorly to hierarchical VAEs. To address this, we
describe a noise contrastive estimation based approach to train pNCP(z) without MCMC sampling.
3.1	Learning The Reweighting Factor with Noise Contrastive Estimation
Recall that training VAEs closes the gap between the prior and the aggregate posterior by minimizing
KL(q(z)||p(z)) with respect to prior. Assuming the base prior p(z) to be fixed, KL(q(z)||pNCP(z)) is zero
when r(z) = q(z)/p(z). However, since we do not have the density function for q(z), we cannot compute
the ratio explicitly. Instead, in this paper, we propose to estimate r(z) using noise contrastive estimation
(Gutmann & Hyvarinen, 2010), also known as the likelihood ratio trick that has been popularized in machine
learning by predictive coding (Oord et al., 2018) and generative adversarial networks (GANs) (Goodfellow
et al., 2014). Since, we can generate samples from both p(z) and q(z)2, we train a binary classifier to distin-
guish samples from q(z) and samples from the base prior p(z) by minimizing the binary cross-entropy loss:
min — EZ〜q(z)[log D(z)] - EZ〜p(z) [log(1 - D(z))]∙
(3)
Here, D : Rn → (0, 1) is a binary classifier that generates the classification prediction probabilities. Eq. (3)
is minimized when D(z) = q(z:+Zp(Z). Denoting the classifier at optimality by D*(z), We estimate the
reweighting factor r(z) = p(Z) ≈ IDDlz(Z). The appealing advantage of this estimator is that it is obtained
by simply training a binary classifier rather than using expensive MCMC sampling.
3.2	Two-stage Training for Noise Contrastive Priors
To properly learn the reweighting factor, NCE training requires the base prior distribution to be close to the
target distribution. Intuitively, if p(z) is very close to q(z) (i.e., p(z) ≈ q(z)), the optimal classifier will have
a large loss value in Eq. (3), and we will have r(z) ≈ 1. If p(z) is instead far from q(z), the binary classifier
will easily learn to distinguish samples from the two distributions and it will not learn the likelihood ratios
correctly. If p(z) is roughly close to q(z), then the binary classifier can learn the ratios.
To ensure that the base prior distribution is close to the target aggregate posterior distribution, we propose
a two-stage training algorithm. In the first stage, we train the VAE with only the base prior p(z). From
Appendix B.1, we know that at the end of training, p(z) is as close as possible to q(z). In the second stage,
we freeze the VAE model including the approximate posteriorq(z|x), the base prior p(z), and the likelihood
p(x|z), and we only train the reweighting factor r(z) using Eq. (3). The second stage can be thought of as
replacing the base distribution p(z) with a more expressive distribution of the form PNCP(Z) 8 r(z)p(z).
Hence, NCP matches the prior to the aggregate posterior q(z) by using the reweighting factors. Note that
our proposed method is generic as it only assumes that we can draw samples from q(z) and p(z), which
applies to any VAE. Our training is illustrated in Fig. 2.
2We generate samples from the aggregate posterior q(z) = Epd(x) [q(z|x)] via ancestral sampling: draw data from
the training set (x 〜Pd(X)) and then sample from Z 〜q(z∣x).
3
Under review as a conference paper at ICLR 2021
Stage 1 (VAE Training)
Stage 2(NCE Training)
Test Time Sampling
Figure 2: NCP-VAE is trained in two stages. In the first stage, we train a VAE using the original VAE
objective. In the second stage, we train the reweighting factor r(z) using noise contrastive estimation (NCE).
NCE trains a classifier to distinguish samples from the prior and samples from the aggregate posterior. Our
noise contrastive prior (NCP) is then constructed by the product of the base prior and the reweighting factor,
formed via the classifier. At test time, we sample from NCP using SIR or LD. These samples are then passed
to the decoder to generate output samples.
3.3	Test Time Sampling
To sample from a VAE with an NCP, we first generate samples from the NCP before passing them to the
decoder to generate output samples (shown in Fig. 2). We propose two methods for sampling from NCPs.
Sampling-Importance-Resampling (SIR): We first generate M samples from the base prior distribution
{z(m)}M=1 〜 p(z). We then resample one of the M proposed samples using importance weights propor-
tional to w(m) = pNCP(z(m))/p(z(m)) = r(z(m)). The benefit of this technique: both proposal generation
and the evaluation of r on the samples are done in parallel.
Langevin Dynamics (LD): Since our NCP is an EBM, we can use LD for sampling. Denoting the energy
function by E(z) = - log r(z) - log p(z), we initialize a sample z0 by drawing from p(z) and update the
sample iteratively using: zt+ι = Zt - 0.5 λVzE(z) + √λet where et 〜N(0,1) and λ is the step size. LD
is run for a finite number of iterations, and in contrast to SIR, it can be slow given its sequential form.
3.4	Generalization to Hierarchical VAEs
The state-of-the-art VAE (Vahdat & Kautz, 2020) uses a hierarchical q(z|x) and p(z). Here p(z) is cho-
sen to be a Gaussian distribution. Appendix B.2 shows that training a HVAE encourages the prior to
minimize Eq(z<k) [KL(q(zk|z<k)||p(zk|z<k))] for each conditional, where q(z<k) , Epd(x)[q(z<K|x)]
is the aggregate posterior up to the (k - 1)th group, and q(zk |z<k) , Epd(x) [q(zk|z<k, x)] is the ag-
gregate conditional for the kth group. Given this observation, we extend NCPs to hierarchical mod-
els to match each conditional in the prior with q(zk|z<k). Formally, we define hierarchical NCPs by
PNCP(Z) = -1 QK=ι r(zk ∣z<k )p(zk ∣z<k) where each factor is anEBM. PNCP(Z) resembles energy machines
with an autoregressive structure among groups (Nash & Durkan, 2019).
In the first stage, we train the HVAE with prior QkK=1P(Zk|Z<k). For the second stage, we use K binary
classifiers, each for a hierarchical group. Following Appendix C, we train each classifier via:
mDin Epd(x)q(z<k |x) - Eq(zk |z<k ,x) [log Dk (Zk, c(Z<k))] - Ep(zk |z<k ) [log(1 - Dk (Zk, c(Z<k)))] , (4)
4
Under review as a conference paper at ICLR 2021
where the outer expectation samples from groups up to the (k- 1)th group, and the inner expectations sample
from approximate posterior and base prior for the kth group, conditioned on the same z<k. The discriminator
Dk classifies samples zk while conditioning its prediction on z<k using a shared context feature c(z<k).
The NCE training in Eq. (4) is minimized When Dk(Zk,c(z<k)) = q(zk∣Zq(Zk检怖［∣z<k). Denoting the
classifier at optimality by Dk(z, c(z<k)), we obtain the reweighting factor Ir(Zk∣z<k) ≈
D 式 zk,c(ZVk))
I-Dk(Zk ,c(z<k))
in the second stage. Given our hierarchical NCP, we use ancestral sampling to sample from the prior. For
sampling from each group, we can use SIR or LD as discussed before.
The context feature c(z<k) extracts a representation from z<k. Instead of learning a new representation at
stage two, we simply use the representation that is extracted from z<k in the hierarchical prior, trained in
the first stage. Note that the binary classifiers in the second stage are trained in parallel for all groups.
4	Related Work
In this section, we review prior works related to the proposed method.
Energy-based Models (EBMs): Early work on EBMs for generative learning goes back to 1980s (Ackley
et al., 1985; Hinton et al., 1986). Prior to the modern deep learning era, most attempts for building genera-
tive models using EBMs were centered around Boltzmann machines (Hinton, 2002; Hinton et al., 2006) and
their “deep” extensions (Salakhutdinov & Hinton, 2009; Larochelle & Bengio, 2008). Although the energy
function in these models is restricted to simple bilinear functions, they have been proven effective for repre-
senting the prior in discrete VAEs (Rolfe, 2016; Vahdat et al., 2018a;b; 2020). Recently, EBMs with neural
energy functions have gained popularity for representing complex data distribution (Du & Mordatch, 2019).
Pang et al. (2020) have shown that neural EBMs can represent expressive prior distributions. However, in
this case, the prior is trained using MCMC sampling, and it has been limited to a single group of latent
variables. To eliminate MCMC sampling, NCE (Gutmann & Hyvarinen, 2010) has recently been used for
training a normalizing flow on data distributions (Gao et al., 2020). Moreover, Han et al. (2019; 2020) have
used divergence triangulation to sidesteps MCMC sampling. In contrast, we use NCE to train an EBM prior
where a noise distribution is easily available through a pre-trained VAE.
Adversarial Training: Similar to NCE, generative adversarial networks (GANs) (Goodfellow et al., 2014)
also rely on a discriminator to learn the likelihood ratio between noise and real images. However, GANs
use the discriminator to update the generator, whereas in NCE, the noise generator is fixed. In spirit similar
are recent works (Azadi et al., 2018; Turner et al., 2019; Che et al., 2020) that link GANs, defined in the
pixels space, to EBMs. We apply the likelihood ratio trick to the latent space of VAEs. The main difference:
the base prior and approximate posterior are trained with the VAE objective rather than the adversarial loss.
Adversarial loss has been used for training implicit encoders in VAEs (Makhzani et al., 2015; Mescheder
et al., 2017; Engel et al., 2018). But, they have not been linked to energy-based priors as we do explicitly.
Prior Hole Problem: Among prior works on this problem, VampPrior (Tomczak & Welling, 2018) uses a
mixture of encoders to represent the prior. However, this requires storing training data or pseudo-data to
generate samples at test time. Takahashi et al. (2019) use the likelihood ratio estimator to train a simple
prior distribution. However at test time, the aggregate posterior is used for sampling in the latent space.
Bauer & Mnih (2019) propose a reweighting factor similar to ours, but it is trained via importance sampling.
Recently, Lawson et al. (2019) introduced energy-inspired models (EIMs) that define distributions induced
by the sampling processes used by (Bauer & Mnih, 2019) as well as our SIR sampling. Although, EIMs
have the advantage of end-to-end training and can be used as either prior or decoder in VAEs, they require
multiple samples during training (up to 1K). This can make application of EIMs to deep hierarchical models
such as NVAEs very challenging as these models are often very memory intensive and are trained with a few
training samples per GPU.
5
Under review as a conference paper at ICLR 2021
q -7
Z 5
Z ɔ
a5
4 7
W /
y 〃
入，
O
a
7
O
O
C
6
g
7
Z
O
飞
G
Y
ð
夕
€
7
3
6
/
C
&
6
7
R
T
(b) CIFAR-10 (t = 0.5)
(a) MNIST (t = 1.0)
(c) CelebA 64×64 (t = 0.7)
8 7 y 247 Q
3 / I / 8 J “
(d) CelebA HQ 256×256 (t = 0.7)
Figure 3: Randomly sampled images from NCP-VAE with the temperature t for the prior.
Two-stage VAEs: VQ-VAE (Van Den Oord et al., 2017; Razavi et al., 2019) first trains an autoencoder and
then fits an autoregressive PixelCNN (Van Den Oord et al., 2016) prior to the latent variables which is slow
to sample from. Two-stage VAE (2s-VAE) (Dai & Wipf, 2018) trains a VAE on the data, and then, trains
another VAE in the latent space. Regularized autoencoders (RAE) (Ghosh et al., 2020) train an autoencoder,
and subsequently a Gaussian mixture model on latent codes. In contrast, we train the model with the original
VAE objective in the first stage, and we improve the expressivity of the prior using an EBM.
5	Experiments
Our implementation of NCP-VAE builds upon NVAE (Vahdat & Kautz, 2020), the state-of-the-art hier-
archical VAE. We examine NCP-VAE on four datasets including dynamically binarized MNIST (LeCun,
1998), CIFAR-10 (Krizhevsky et al., 2009), CelebA-64 (Liu et al., 2015) and CelebA-HQ-256 (Karras et al.,
2017). For CIFAR-10 and CelebA-64, the model has 30 groups, and for CelebA-HQ-256 it has 20 groups.
We sample from NCP-VAE using SIR with 5K proposal samples at the test (image generation) time. On
these datasets, We measure the sample quality using the FreChet Inception Distance (FID) score (Heusel
et al., 2017) with 50,000 samples, as computing the log-likelihood requires estimating the intractable nor-
malization constant. To report log-likelihood results, We train an NVAE model With a small latent space on
MNIST With 10 groups of 4 × 4 latent variables. We estimate log normalization constant in NCPs using
1000 importance Weighted samples. We intentionally limit the latent space to ensure that We can estimate
the normalization constant correctly (standard deviation of log Z estimation ≤ 0.23). Thus, on this dataset,
We report negative log-likelihood (NLL). Implementation details are provided in Appendix E.
5.1	Quantitative Results
The quantitative results are reported in Tab. 1, Tab. 2, Tab. 3, and Tab. 4. On all four datasets, our model
improves upon state-of-the-art NVAE, and it reduces the gap With GANs by a large margin. On CelebA
64, We improve NVAE from an FID of 13.48 to 5.25, comparable to GANs. On CIFAR-10, NCP-VAE
6
Under review as a conference paper at ICLR 2021
Table 2: Generative performance on CIFAR-10
Table 1: Generative performance on CelebA-64
Model	FID；	Model	FID；
NCP-VAE (ours)	5.25	NCP-VAE (ours)	24.08
NVAE (Vahdat & Kautz, 2020)	13.48	NVAE (Vahdat & Kautz, 2020)	51.71
RAE (Ghosh et al., 2020)	40.95	RAE (GhoSh et al., 2020)	74.16
2s-VAE (Dai & Wipf, 2018)	44.4	2s-VAE (Dai & WiPf, 2018)	72.9
WAE (Tolstikhin et al., 2018)	35	Perceptial AE (Zhang et al., 2020)	51.51
PerCePtial AE(Zhang et al., 2020)	13.8	EBM (DU & Mordatch, 2019)	40.58
Latent EBM (Pang et al., 2020)	37.87	Latent EBM (Pang et al., 2020)	70.15
COCO-GAN (Lin et al., 2019)	4.0	Style-GANv2 (KarraS et al., 2020)	3.26
QA-GAN (Parimala & ChannaPPayya, 2019)	6.42		Denoising Diffusion Process (Ho et al., 2020)	3.17
NVAE-Recon (Vahdat & Kautz, 2020)	1.03	NVAE-Recon (Vahdat & Kautz, 2020)	2.67
Table 3: Generative results on CelebA-HQ-256		Table 4: Likelihood results on MNIST in nats	
Model	FID；	Model	NLL；
NCP-VAE (ours)	24.79	NCP-VAE (ours)	78.10
NVAE (Vahdat & Kautz, 2020)	40.26	NVAE-small (Vahdat & Kautz, 2020)	78.67
GLOW (Kingma & DhariWaL 2018)	68.93	BIVA (Maal0e etal., 2019)	78.41
Advers. LAE (Pidhorskyi et al., 2020)-	19.21	DAVE++ (Vahdat et al., 2018b)	78.49
PGGAN (Karras et al., 2017)	8.03	IAF-VAE (Kingma et al., 2016)	79.10
NVAE-Recon (Vahdat & Kautz, 2020)	0.45	VampPrior AR dec. (Tomczak & Welling)	78.45
improves the NVAE FID of 51.71 to 24.08. On MNIST, although our latent space is much smaller, our
model outperforms previous VAEs. NVAE has reported 78.01 nats on this dataset with a larger latent space.
Reconstruction: In the last row of Tab. 1, Tab. 2, and Tab. 3, we report the FID score for the reconstructed
images of the NVAE baseline. Note how reconstruction FID is much lower than our FID, indicating that our
model is far from memorizing the training data. In Appendix G, we also provide nearest neighbours from
training data for generated samples. In Appendix F, we present our NCP applied to vanilla VAEs.
5.2	Qualitative Results
We visualize samples generated by NCP-VAE in Fig. 3 without any manual intervention. We adopt the
common practice of reducing the temperature of the base prior p(z) by scaling down the standard-deviation
of the conditional Normal distributions (Kingma & Dhariwal, 2018)3. Brock et al. (2018); Vahdat & Kautz
(2020) also observe that re-adjusting the batch-normalization (BN), given a temperature applied to the prior,
improves the generative quality. Similarly, we achieve diverse, high-quality images by re-adjusting the BN
statistics as described by Vahdat & Kautz (2020). Additional qualitative results are shown in Appendix H.
5.3	Additional Studies
We perform additional experiments to study i) how hierarchical NCPs perform as the number of latent groups
increases, ii) the impact of SIR and LD hyperparameters, and iii) what the classification loss in NCE training
conveys about p(z) and q(z). All experiments in this section are performed on CelebA-64 data.
3Lowering of the temperature is only used to obtain qualitative samples. It’s not used when computing any of the
quantitative results in Sec. 5.1.
7
Under review as a conference paper at ICLR 2021
Table 6: Effect of SIR sample size and LD iterations. Time-N is the time used to generate a batch of N images.
# SIR proposal samples	FID]	Time-1 (sec)	Time-10 (sec)	Memory (GB)	#LD iterations	FID]	Time-1 (sec)	Time-10 (sec)	Memory (GB)
5	11.75	0.34	0.42	1.96	5	14.44	3.08	3.07	1.94
50	8.58	0.40	1.21	4.30	50	12.76	27.85	28.55	1.94
500	6.76	1.25	9.43	20.53	500	8.12	276.13	260.35	1.94
5000	5.25	10.11	95.67	23.43	1000	6.98	552	561.44	1.94
Training Epoch	Binary Classifier Loss
(a)	(b)
Figure 4: (a) Classification loss for binary classifiers on latent variable groups. A larger final loss upon
training indicates that q(z) and p(z) are more similar. (b) The effective sample size vs. the final loss value
at the end of training. Higher effective sample size implies similarity of two distributions.
Number of latent variable groups: Tab. 5 shows the generative performance
of hierarchical NCP with different amounts of latent variable groups. As we
increase the number of groups, the FID score of both NVAE and our model
improves. This demonstrates the efficacy of our NCPs, even with expressive
hierarchical priors and in the presence of many groups.
SIR and LD parameters: The computationally complexity of SIR is similar
Table 5: # groups & gener-
ative performance in FID]
# groups	NVAE	NCP-VAE
6	33.18	18.68
15	14.96	5.96
30	13.48	5.25
to LD if we set the number of proposal samples in SIR equal to the number LD iterations. Tab. 6 reports
the impact of these parameters. We observe that increasing both the number of proposal samples in SIR
and the LD iterations leads to a noticeable improvement in FID score. For SIR, the proposal generation and
the evaluation of r(z) are parallelizable. Hence, as shown in Tab. 6, image generation is faster with SIR
than with LD (LD iterations are sequential). However, GPU memory usage scales with the number of SIR
proposals, but not with the number of LD iterations. Interestingly, SIR, albeit simple, performs better than
LD when using about the same compute.
Classification loss in NCE: We can draw a direct connection between the classification loss in Eq. 3 and the
similarity of p(z) and q(z). Denoting the classification loss in Eq. 3 at optimality by L, Goodfellow et al.
(2014) show that JSD(P(Z) ∣∣q(z)) = log2 - 0.5 ×L where JSD denotes the Jensen-Shannon divergence
between two distributions. Fig. 4(a) plots the classification loss (Eq. 4) for each classifier for a 15-group
NCP trained on the CelebA-64 dataset. Assume that the classifier loss at the end of training is a good
approximation of L. We observe that 8 out of 15 groups have L ≥ 0.4, indicating a good overlap between
p(z) and q(z) for those groups. To further assess the impact of the distribution match on SIR sampling, in
Fig. 4(b), we visualize the effective sample size (ESS)4 in SIR vs. L* for the same group. We observe a
strong correlation between L* and the effective sample size. SIR is more reliable on the same 8 groups that
have high classification loss. These groups are mostly at the top of the NVAE hierarchy which have been
shown to control the global structure of generated samples (see B.6 in Vahdat & Kautz (2020)).
4ESS measures reliability of SIR via 1/Pm(W(M))2, where W(m) = r(z(m))/Pmf r(z(m0)) (Owen, 2013).
8
Under review as a conference paper at ICLR 2021
5.4	Additional Comparison to Prior Art
In our previous experiments, we rely on NVAE as the VAE backbone. Although this shows that our approach
can be applied to large scale models successfully, this may make comparison against the prior art unfair as
they are often applied to smaller models. To provide a fair comparison, in this section, we apply NCP to
several commonly used small VAE models.
Comparison against RAE, 2s-VAE, and WAE: In Tab. 7 we show the generative performance of our
approach applied to the VAE architecture in RAE (Ghosh et al., 2020). Note that this VAE architecture has
only one latent variable group. The same base architecture was used in the implementation of 2s-VAE (Dai
& Wipf, 2018) and WAE (Tolstikhin et al., 2018). We borrow the training setup from Ghosh et al. (2020)
on the CelebA-64 dataset, and compare to the baselines reported in this work. We apply our NCP-VAE on
top of vanilla VAE with a Gaussian prior as well as a 10-component Gaussian mixture model (GMM) prior
that was proposed in RAEs. Our NCP-VAE improves the performance on the base VAE, improving the
FID score to 41.28 from 48.12. Additionally, when NCP is applied to the VAE with GMM prior (the RAE
model), it improves its performance from 40.95 to the FID score of 39.00.
Comparison against LARS and SNIS: In Tab. 8, we compare NCP-VAE to LARS (Bauer & Mnih, 2019)
and SNIS (Lawson et al., 2019) priors on MNIST. We implement our NCP-VAE using the VAE and energy-
function architectures that were used by Lawson et al. (2019). We closely follow the training hyperparame-
ters used by Lawson et al. (2019) as well as their approach for obtaining a lower bound on the log likelihood.
NCP-VAE obtains the negative log-likelihood (NLL) of 82.82, comparable to Lawson et al. (2019), while
outperforming LARS (Bauer & Mnih, 2019).
Table 7: Generative performance on CelebA-64
with the RAE (Ghosh et al., 2020) architecture
Table 8: Likelihood results on MNIST on sin-
gle latent group model with architecture from
LARS (Bauer & Mnih, 2019) & SNIS (Lawson
Model	FID；	et al., 2019) (results in nats)	
VAE w/ Gaussian prior	48.12	Model	NTJ T I
2s-VAE (Dai & Wipf, 2018)	49.70		NLL；
WAE (Tolstikhin et al., 2018)	42.73	VAE w/ Gaussian prior	84.82
RAE (Ghosh et al., 2020)	40.95	VAE w/ LARS prior (Bauer & Mnih, 2019)	83.03
NCP w/ Gaussian prior as base	41.28	VAE w/ SNIS prior (Lawson et al., 2019)	82.52
NCP w/ GMM prior as base	39.00	NCP-VAE	82.82
6	Conclusions
The prior hole problem is one of the main reasons for VAEs’ poor generative quality. In this paper, we tackled
this problem by introducing the noise contrastive prior (NCP), defined by the product of a reweighting factor
and a base prior. We showed how the reweighting factor is trained by contrasting samples from the aggregate
posterior with samples from the base prior. Our proposal is simple and can be applied to any VAE to increase
its prior’s expressivity. We also showed how NCP training scales to large hierarchical VAEs, as it can be done
in parallel simultaneously for all the groups. Finally, we demonstrated that NCPs improve the generative
performance of state-of-the-art NVAEs by a large margin, closing the gap to GANs.
References
David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning algorithm for Boltzmann ma-
chines. Cognitive science, 9(1):147-169,1985.
Jyoti Aneja, Aditya Deshpande, and Alexander G Schwing. Convolutional image captioning. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pp. 5561-5570, 2018.
9
Under review as a conference paper at ICLR 2021
Jyoti Aneja, Harsh Agrawal, Dhruv Batra, and Alexander Schwing. Sequential latent spaces for modeling
the intention during diverse image captioning. In Proceedings of the IEEE International Conference on
Computer Vision, pp. 4261-4270, 2019.
Samaneh Azadi, Catherine Olsson, Trevor Darrell, Ian Goodfellow, and Augustus Odena. Discriminator
rejection sampling. In International Conference on Learning Representations, 2018.
Matthias Bauer and Andriy Mnih. Resampled priors for variational autoencoders. In The 22nd International
Conference on Artificial Intelligence and Statistics, pp. 66-75. PMLR, 2019.
Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image
synthesis. arXiv preprint arXiv:1809.11096, 2018.
Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua
Bengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent
sampling. arXiv preprint arXiv:2003.06060, 2020.
Xi Chen, Diederik P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John Schulman, Ilya Sutskever,
and Pieter Abbeel. Variational lossy autoencoder. arXiv preprint arXiv:1611.02731, 2016.
Bin Dai and David Wipf. Diagnosing and enhancing vae models. In International Conference on Learning
Representations, 2018.
Aditya Deshpande, Jyoti Aneja, Liwei Wang, Alexander G Schwing, and David Forsyth. Fast, diverse and
accurate image captioning guided by part-of-speech. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 10695-10704, 2019.
Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Juke-
box: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.
Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in
Neural Information Processing Systems, pp. 3608-3618, 2019.
Jesse Engel, Matthew Hoffman, and Adam Roberts. Latent constraints: Learning to generate conditionally
from unconditional generative models. In International Conference on Learning Representations, 2018.
Vincent Fortuin, Matthias HUser, Francesco Locatello, Heiko Strathmann, and Gunnar Ratsch. Som-vae:
Interpretable discrete representation learning on time series. In International Conference on Learning
Representations, 2018.
Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow con-
trastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 7518-7528, 2020.
Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, and Bernhard Scholkopf. From varia-
tional to deterministic autoencoders. In International Conference on Learning Representations, 2020.
Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron
Courville, and Yoshua Bengio. Generative Adversarial Networks. arXiv:1406.2661, June 2014.
Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards con-
ceptual compression. In Advances In Neural Information Processing Systems, pp. 3549-3557, 2016.
Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and
Aaron Courville. PixelVAE: A latent variable model for natural images. arXiv preprint arXiv:1611.05013,
2016.
10
Under review as a conference paper at ICLR 2021
Michael Gutmann and AaPo Hyvarinen. Noise-contrastive estimation: A new estimation principle for Un-
normalized statistical models. In Proceedings of the Thirteenth International Conference on Artificial
InteUigence and Statistics, pp. 297-304, 2010.
Tian Han, Erik Nijkamp, Xiaolin Fang, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Divergence triangle
for joint training of generator model, energy-based model, and inferential model. In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recognition, pp. 8670-8679, 2019.
Tian Han, Erik Nijkamp, Linqi Zhou, Bo Pang, Song-Chun Zhu, and Ying Nian Wu. Joint training of
variational auto-encoder and latent energy-based model. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition, pp. 7978-7987, 2020.
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained
by a two time-scale update rule converge to a local nash equilibrium. In Advances in neural information
processing systems, pp. 6626-6637, 2017.
Geoffrey E Hinton. Training products of experts by minimizing contrastive divergence. Neural computation,
14(8):1771-1800, 2002.
Geoffrey E Hinton, Terrence J Sejnowski, et al. Learning and relearning in boltzmann machines. Parallel
distributed processing: Explorations in the microstructure of cognition, 1(282-317):2, 1986.
Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief nets.
Neural computation, 18(7):1527-1554, 2006.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint
arxiv:2006.11239, 2020.
Matthew D Hoffman and Matthew J Johnson. Elbo surgery: yet another way to carve up the variational
evidence lower bound. In Workshop in Advances in Approximate Bayesian Inference, NIPS, 2016.
Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference
on computer vision and pattern recognition, pp. 7132-7141, 2018.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internal covariate shift. In International Conference on Machine Learning, pp. 448-456, 2015.
Pavel Izmailov, Polina Kirichenko, Marc Finzi, and Andrew Gordon Wilson. Semi-supervised learning with
normalizing flows. In ICML, 2020.
Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved
quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017.
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial
networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp.
4401-4410, 2019.
Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training gener-
ative adversarial networks with limited data. arXiv preprint arXiv:2006.06676, 2020.
Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In The International Conference
on Learning Representations (ICLR), 2014.
11
Under review as a conference paper at ICLR 2021
Diederik P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learn-
ing with deep generative models. In Advances in Neural Information Processing Systems, pp. 3581-3589,
2014.
Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved
variational inference with inverse autoregressive flow. In Advances in Neural Information Processing
Systems, pp. 4743-4751, 2016.
Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Ben-
gio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Advances in Neural
Information Processing Systems 31, pp. 10236-10245, 2018.
Alexej Klushyn, Nutan Chen, Richard Kurle, Botond Cseke, and Patrick van der Smagt. Learning hierarchi-
cal priors in vaes. In Advances in Neural Information Processing Systems, 2019.
Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images, 2009.
Hugo Larochelle and Yoshua Bengio. Classification using discriminative restricted boltzmann machines. In
Proceedings of the 25th international conference on Machine learning, pp. 536-543, 2008.
John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with
sampler-induced distributions. In Advances in Neural Information Processing Systems, pp. 8501-8513,
2019.
Yann LeCun. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998.
Chieh Hubert Lin, Chia-Che Chang, Yu-Sheng Chen, Da-Cheng Juan, Wei Wei, and Hwann-Tzong Chen.
Coco-gan: generation by parts via conditional coordinating. In Proceedings of the IEEE International
Conference on Computer Vision, pp. 4512-4521, 2019.
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In
Proceedings of the IEEE international conference on computer vision, pp. 3730-3738, 2015.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint
arXiv:1608.03983, 2016.
Lars Maal0e, Marco Fraccaro, Valentin Lievin, and Ole Winther. BIVA: A very deep hierarchy of latent
variables for generative modeling. In Advances in neural information processing systems, pp. 6548-6558,
2019.
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan Frey. Adversarial autoen-
coders. arXiv preprint arXiv:1511.05644, 2015.
Lars Mescheder, S Nowozin, and Andreas Geiger. Adversarial variational bayes: Unifying variational au-
toencoders and generative adversarial networks. In 34th International Conference on Machine Learning
(ICML), pp. 2391-2400. PMLR, 2017.
Charlie Nash and Conor Durkan. Autoregressive energy machines. arXiv preprint arXiv:1904.05626, 2019.
Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal
Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio.
arXiv preprint arXiv:1609.03499, 2016.
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive
coding. arXiv preprint arXiv:1807.03748, 2018.
12
Under review as a conference paper at ICLR 2021
Art B. Owen. Monte Carlo theory, methods and examples. 2013.
Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space energy-based
prior model. arXiv preprint arXiv:2006.08205, 2020.
Kancharla Parimala and Sumohana Channappayya. Quality aware generative adversarial networks. In Ad-
Vances in Neural Information Processing Systems,pp. 2948-2958, 2019.
Stanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders. In Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 14104-14113,
2020.
Wei Ping, Kainan Peng, Kexin Zhao, and Zhao Song. Waveflow: A compact flow-based model for raw
audio. ICML, 2020.
Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint
arXiv:1710.05941, 2017.
Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2.
In Advances in Neural Information Processing Systems, pp. 14837-14847, 2019.
Danilo Jimenez Rezende and Fabio Viola. Taming vaes. arXiv preprint arXiv:1810.00597, 2018.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approxi-
mate inference in deep generative models. In International Conference on Machine Learning, pp. 1278-
1286, 2014.
Jason Tyler Rolfe. Discrete variational autoencoders. arXiv preprint arXiv:1609.02200, 2016.
Mihaela Rosca, Balaji Lakshminarayanan, and Shakir Mohamed. Distribution matching in variational infer-
ence. arXiv preprint arXiv:1802.06847, 2018.
Ruslan Salakhutdinov and Geoffrey Hinton. Deep boltzmann machines. In Artificial intelligence and statis-
tics, pp. 448-455, 2009.
Casper Kaae S0nderby, Tapani Raiko, Lars Maal0e, S0ren Kaae S0nderby, and Ole Winther. Ladder varia-
tional autoencoders. In Advances in neural information processing systems, pp. 3738-3746, 2016.
Hiroshi Takahashi, Tomoharu Iwata, Yuki Yamanaka, Masanori Yamada, and Satoshi Yagi. Variational
autoencoder with implicit optimal priors. In Proceedings of the AAAI Conference on Artificial Intelligence,
volume 33, pp. 5066-5073, 2019.
I Tolstikhin, O Bousquet, S Gelly, and B Scholkopf. Wasserstein auto-encoders. In International Conference
on Learning Representations (ICLR 2018). OpenReview. net, 2018.
Jakub Tomczak and Max Welling. Vae with a vampprior. In International Conference on Artificial Intelli-
gence and Statistics, pp. 1214-1223, 2018.
Ryan Turner, Jane Hung, Eric Frank, Yunus Saatchi, and Jason Yosinski. Metropolis-hastings generative
adversarial networks. In International Conference on Machine Learning, pp. 6345-6353. PMLR, 2019.
Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Neural Information
Processing Systems (NeurIPS), 2020.
Arash Vahdat, Evgeny Andriyash, and William G Macready. DVAE#: Discrete variational autoencoders
with relaxed Boltzmann priors. In Neural Information Processing Systems, 2018a.
13
Under review as a conference paper at ICLR 2021
Arash Vahdat, William G. Macready, Zhengbing Bian, Amir Khoshaman, and Evgeny Andriyash. DVAE++:
Discrete variational autoencoders with overlapping transformations. In International Conference on Ma-
chine Learning (ICML), 2018b.
Arash Vahdat, Evgeny Andriyash, and William G Macready. Undirected graphical models as approximate
posteriors. In International Conference on Machine Learning (ICML), 2020.
Aaron Van Den Oord, Nal Kalchbrenner, and Koray KavUkcUoglu. Pixel recurrent neural networks. In
Proceedings of the 33rd International Conference on International Conference on Machine Learning, pp.
1747-1756.JMLR. org, 2016.
Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in Neural
Information Processing Systems,pp. 6306-6315, 2017.
Haowen Xu, Wenxiao Chen, Jinlin Lai, Zhihan Li, Youjian Zhao, and Dan Pei. On the necessity and
effectiveness of learning the prior of variational auto-encoder. arXiv preprint arXiv:1905.13452, 2019.
Zijun Zhang, Ruixiang Zhang, Zongpeng Li, Yoshua Bengio, and Liam Paull. Perceptual generative autoen-
coders. In International Conference on Machine Learning, 2020.
14
Under review as a conference paper at ICLR 2021
A	Training Energy-based Priors using MCMC
In this section, we show how a VAE with energy-based model in its prior can be trained. Assuming that the
prior is in the formPEBM(Z) = 1 r(z)p(z), the variational bound is of the form:
Epd(x)[LVAE] = Epd(x) Eq(z|x) [logp(x|z)] - KL(q(z|x)||pEBM(z))
= Epd(x) Eq(z|x)[log p(x|z) - log q(z|x) + log r(z) + log p(z)] - log Z,
where the expectation term, similar to VAEs, can be trained using the reparameterization trick. The only
problematic term is the log-normalization constant log Z, which captures the gradient with respect to the
parameters of the prior pEBM (z). Denoting these parameters by θ, the gradient of log Z is obtained by:
d r7 1 / d(r(Z)P(Z))小 Cr(Z)P(Z) dlog(r(Z)P(Z))J〜 W	∂log(r(Z)P(Z))I ,c
∂θ log Z = ZJ —∂θ—dZ = J 1---------------------∂θ------dZ = EPEBM (Z) [----∂θ------]，⑸
where the expectation can be estimated using MCMC sampling from the EBM prior.
B Maximizing the Variational Bound from the Prior’ s Perspective
In this section, we discuss how maximizing the variational bound in VAEs from the prior’s perspective
corresponds to minimizing a KL divergence from the aggregate posterior to the prior. Note that this relation
has been explored by Hoffman & Johnson (2016); Rezende & Viola (2018); Tomczak & Welling (2018) and
we include it here for completeness.
B.1	VAE with a Single Group of Latent Variables
Denote the aggregate (approximate) posterior by q(Z) , Epd(x) [q(Z|x)]. Here, we show that maximizing
the Epd(x) [LVAE(x)] with respect to the prior parameters corresponds to learning the prior by minimizing
KL(q(Z)||P(Z)). To see this, note that the prior P(Z) only participates in the KL term in LVAE (Eq. 1). We
hence have:
arg max Epd(x) [LVAE (x)] = arg min Epd(x)[KL(q(Z|x)||P(Z))]
p(z)	p(z)
= arg min -Epd(x) [H(q(Z|x))] - Eq(z)[logP(Z)]
p(z)
= arg min -H(q(Z)) - Eq(z) [log P(Z)]
p(z)
= arg min KL(q(Z)||P(Z)),
p(z)
where H(.) denotes the entropy. Above, we replaced the expected entropy Epd(x) [H(q(Z|x))] with H(q(Z))
as the minimization is with respect to the parameters of the prior P(Z).
B.2	Hierarchical VAEs
Denote hierarchical approximate posterior and prior distributions by: q(Z|x) = QkK=1q(Zk|Z<k, x) and
P(Z) = QkK=1P(Zk|Z<k). The hierarchical VAE objective becomes:
K
LHVAE(x) = Eq(z|x)[log P(x|Z)] -	Eq(z<k|x) [KL(q(Zk |Z<k, x)||P(Zk |Z<k))] ,	(6)
k=1
15
Under review as a conference paper at ICLR 2021
where q(z<k|x) = Qik=-11 q(zi |z<i, x) is the approximate posterior up to the (k - 1)th group. Denote the
aggregate posterior up to the (K - 1)th group by q(z<k) , Epd(x) [q(z<K|x)] and the aggregate conditional
for the kth group given the previous groups q(zk|z<k) , Epd(x)[q(zk|z<k, x)].
Here, we show that maximizing Epd(x) [LHVAE(x)] with respect to the prior corresponds to learning the prior
by minimizing Eq(z<k) [KL(q(zk |z<k)||p(zk |z<k))] for each conditional:
arg max Epd (x) [LHVAE (x)] = arg min Epd(x) Eq(z<k|x) [KL(q(zk |z<k, x)||p(zk |z<k))]
p(zk |z<k)	p(zk |z<k)
= arg min -Epd (x)q(z<k |x)q(zk |z<k,x) [log p(zk |z<k)]
p(zk |z<k)
= arg min -Eq(zk,z<k) [logp(zk |z<k)]
p(zk |z<k)
= arg min -Eq(z<k) Eq(zk|z<k) [logp(zk |z<k)]
p(zk |z<k)
= arg min Eq(z<k) -H(q(zk|z<k)) - Eq(zk|z<k) [logp(zk|z<k)]
p(zk |z<k)
= arg min Eq(z<k) [KL(q(zk|z<k)||p(zk|z<k))] .	(7)
p(zk |z<k)
C Conditional NCE for Hierarchical VAEs
In this section, we describe how we derive the NCE training objective for hierarchical VAEs given in
Eq. (4). Our goal is to learn the likelihood ratio between the aggregate conditional q(zk|z<k) and the
prior p(zk|z<k). We can define the NCE objective to train the discriminator Dk(zk, z<k) that classifies zk
given samples from the previous groups z<k using:
min - Eq(zk|z<k)[log Dk(zk, z<k)] - Ep(zk|z<k)[log(1 - Dk(zk, z<k))] ∀z<k.	(8)
Dk
Since z<k is in a high dimensional space, we cannot apply the minimization ∀z<k. Instead, we sample from
z<k using the aggregate approximate posterior q(z<k) as done for the KL in a hierarchical model (Eq. (7)):
min Eq(z<k) - Eq(zk|z<k)[log Dk(zk, z<k)] -Ep(zk|z<k)[log(1 - Dk(zk, z<k))] .	(9)
Since q(z<k)q(zk|z<k) = q(zk, z<k) = Epd(x)[q(z<k|x)q(zk|z<k, x)], we have:
mDin Epd(x)q(z<k|x) - Eq(zk|z<k,x)[log Dk(zk, z<k)] - Ep(zk|z<k)[log(1 - Dk(zk, z<k))] .	(10)
Finally, instead of passing all the samples from the previous latent variables groups to D, we can pass the
context feature c(z<k) that extracts a representation from all the previous groups:
mDin Epd (x)q(z<k |x) - Eq(zk |z<k ,x) [log Dk (zk, c(z<k))] - Ep(zk |z<k) [log(1 - Dk (zk, c(z<k)))] . (11)
D NVAE Based Model and Context Feature
Context Feature: The base model NVAE (Vahdat & Kautz, 2020) is hierarchical. To encode the information
from the lower levels of the hierarchy to the higher levels, during training of the binary classifiers, we
concatenate the context feature c(z<k) to the samples from both p(z) and q(z). The context feature for each
group is the output of the residual cell of the top-down model and encodes a representation from z<k .
16
Under review as a conference paper at ICLR 2021
Squeeze &
Excitation
[~Conv 3x3~1
(si, pl)
ɪ
Batch-Norm
+ Swish
Batch-Norm
+ Swish
I ReSidUaI-Block-A ∣
(U.2⅛uwouo□ -əuuBqɔ)
UoHonpəX pəzFOI。留
Residual-Block-B
Figure 5: Residual blocks used in the binary classifier. We use s, p and C to refer to the stride parameter,
the padding parameter and the number of channels in the feature map, respectively.
Image Decoder p(x|z): The base NVAE (Vahdat & Kautz, 2020) uses a mixture of discretized logistic dis-
tributions for all the datasets but MNIST, for which it uses a Bernoulli distribution. In our model, we observe
that replacing this with a Normal distribution for the RGB image datasets leads to significant improvements
in the base model performance. This is also reflected in the gains of our approach.
E	Implementation Details
The binary classifier is composed of two types of residual blocks as in Fig. 5. The residual blocks use batch-
normalization (Ioffe & Szegedy, 2015), the Swish activation function (Ramachandran et al., 2017), and the
Squeeze-and-Excitation (SE) block (Hu et al., 2018). SE performs a squeeze operation (e.g., mean) to obtain
a single value for each channel. An excitation operation (non-linear transformation) is applied to these
values to get per-channel weights. The Residual-Block-B differs from Residual-Block-A in that it doubles
the number of channels (C → 2C), while down-sampling the other spatial dimensions. It therefore also
includes a factorized reduction with 1 × 1 convolutions along the skip-connection. The complete architecture
of the classifier is:
F Performance On Single Group VAE
To demonstrate the efficacy of our approach on any off-the-shelf VAE, we, apply our NCE based approach
to the VAE in (Ghosh et al., 2020). Note we use the vanilla VAE with Normal prior provided by the authors.
The FID for CelebA 64 improves from 48.12 to 41.28. Note that the FID for reconstruction is reported as
39.12. Single group VAEs are known to perform poorly on the task of image geeration, which is reflected in
the high FID value of reconstruction.
G Nearest Neighbors from the Training Dataset
To highlight that hierarchical NCP generates unseen samples at test time rather than memorizing the training
dataset, Figures 6-7 visualize samples from the model along with a few training images that are most similar
17
Under review as a conference paper at ICLR 2021
Conv 3x3 (s1, p1) + ReLU
J
Residual-Block-A
J
Residual-Block-A
J
Residual-Block-A
J
Residual-Block-B
J
Residual-Block-A
J
Residual-Block-A
J
Residual-Block-A
J
Residual-Block-B
J
2D average pooling
J
Linear + Sigmoid
Optimizer
Learning Rate
Batch size
Adam (Kingma & Ba, 2014)
Initialize at 1e-3, CosineAnnealing (Loshchilov & Hutter, 2016) to 1e-7
512 (MNIST, CIFAR-10), 256 (CelebA-64), 128 (CelebA HQ 256 )
Table 9: Hyper-parameters for training the binary classifiers.
to them (nearest neighbors). To get the similarity score for a pair of images, we downsample to 64 × 64,
center crop to 40 × 40 and compute the Euclidean distance. The KD-tree algorithm is used to fetch the
nearest neighbors. We note that the generated samples are quite distinct from the training images.
18
Under review as a conference paper at ICLR 2021
Query Image
Nearest neighbors from the training dataset
Figure 6: Query images (left) and their nearest neighbors from the CelebA-HQ-256 training dataset.
19
Under review as a conference paper at ICLR 2021
Query Image	Nearest neighbors from the training dataset
Figure 7: Query images (left) and their nearest neighbors from the CelebA-HQ-256 training dataset.
20
Under review as a conference paper at ICLR 2021
H Additional Qualitative Examples
Figure 8: Additional samples from CelebA-64 at t = 0.7.
21
Under review as a conference paper at ICLR 2021
Figure 9: Additional samples from CelebA-HQ-256 at t = 0.7.
22
Under review as a conference paper at ICLR 2021
Figure 10: Selected good quality samples from CelebA-HQ-256.
23
Under review as a conference paper at ICLR 2021
I Experiment on Synthetic Data
In Fig. 11 we demonstrate the efficacy of our approach on the 25-Gaussians dataset, that is generated by a
mixture of 25 two-dimensional Gaussian distributions that are arranged on a grid. The encoder and decoder
of the VAE have 4 fully connected layers with 256 hidden units, with 20 dimensional latent variables. The
discriminator has 4 fully connected layers with 256 hidden units. Note that the samples decoded from prior
p(z) Fig. 11(b)) without the NCP approach generates many points from the the low density regions in the
data distribution. These are removed by using our NCP approach (Fig. 11(c)).
Figure 11: Qualitative results on mixture of 25-Gaussians.
We use 50k samples from the true distribution to estimate the log-likelihood. Our NCP approach obtains
an average log-likelihood of -0.954 nats compared to the log-likelihood obtained by sampling from the
Gaussian prior, -2.753 nats. We use 20k Monte Carlo samples to estimate the log partition function for the
calculation of log-likelihood.
24
Under review as a conference paper at ICLR 2021
J Additional Qualitative Examples
In Fig. 12, we show additional examples of images generated by NVAE (Vahdat & Kautz, 2020) and our
NCP-VAE. We use temperature=0.7 for both. Visually corrupt images are highlighted with a red square.
Random Samples from NVAE at t = 0.7
Random Samples from NCP-VAE at t = 0.7
Figure 12: Additional samples from CelebA-64 at t = 0.7.
25