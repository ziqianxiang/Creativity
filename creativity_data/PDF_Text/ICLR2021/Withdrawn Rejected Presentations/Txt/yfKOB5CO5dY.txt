Under review as a conference paper at ICLR 2021
Localized Meta-Learning: A PAC-Bayes Anal-
ysis for Meta-Learning Beyond Global Prior
Anonymous authors
Paper under double-blind review
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
Ab stract
Meta-learning methods learn the meta-knowledge among various training tasks
and aim to promote the learning of new tasks under the task similarity assumption.
Such meta-knowledge is often represented as a fixed distribution; this, however,
may be too restrictive to capture various specific task information because the
discriminative patterns in the data may change dramatically across tasks. In this
work, we aim to equip the meta learner with the ability to model and produce
task-specific meta knowledge and, accordingly, present a localized meta-learning
framework based on the PAC-Bayes theory. In particular, we propose a Local
Coordinate Coding (LCC) based prior predictor that allows the meta learner to
generate local meta-knowledge for specific tasks adaptively. We further develop a
practical algorithm with deep neural network based on the bound. Empirical results
on real-world datasets demonstrate the efficacy of the proposed method.
1 Introduction
Recent years have seen a resurgence of interest in the
field of meta-learning, or learning-to-learn (Thrun
& Pratt, 2012), especially for empowering deep neu-
ral networks with the capability of fast adapting to
unseen tasks just as humans (Finn et al., 2017; Ravi
& Larochelle, 2017). More concretely, the neural
networks are trained from a sequence of datasets,
associated with different tasks sampled from a meta-
distribution (also called task environment (Baxter,
2000; Maurer, 2005)). The principal aim of meta
learner is to extract transferable meta-knowledge
from observed tasks and facilitate the learning of
new tasks sampled from the same meta-distribution.
The performance is measured by the generalization
ability from a finite set of observed tasks, which is
Global Meta-knowledge	Localized Meta-knowledge
Task 1 Instances
Figure 1: Illustration of the localized meta-
learning framework. Instead of using global
meta-knowledge for all tasks, we tailor the
meta-knowledge for various specific task.
Adaptation
Task 2 Instances
evaluated by learning related unseen tasks. For this reason, there has been considerable interest in
theoretical bounds on the generalization for meta-learning algorithms (Denevi et al., 2018b;a).
One typical line of work (Pentina & Lampert, 2014; Amit & Meir, 2018) uses PAC-Bayes bound to
analyze the generalization behavior of the meta learner and quantify the relation between the expected
loss on new tasks and the average loss on the observed tasks. In this setup, it formulates meta-learning
as hierarchical Bayes. For each task, the base learner produces a posterior based on the associated
task data and the prior. Each prior is a reference w.r.t. base model that is generated by the meta leaner
and must be chosen before observing task data. Accordingly, meta-knowledge is formulated as a
global distribution over all possible priors. Initially, it is called as hyperprior since it is chosen before
observing training tasks. To learn versatile meta-knowledge across tasks, the meta learner observes a
sequence of training tasks and adjusts its hyperprior into a hyperposterior distribution over the set of
priors. The prior generated by the hyperposterior is then used to solve new tasks.
Despite of its great success, such global hyperposterior is rather generic, typically not well tailored
to various specific tasks. In contrast, in many scenarios the related tasks may require task-specific
meta-knowledge. Consequently, traditional meta-knowledge may lead to sub-optimal performance
for any individual prediction task. As a motivational example, suppose we have two different
1
Under review as a conference paper at ICLR 2021
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
tasks: distinguishing motorcycle versus bicycle and distinguishing motorcycle versus car. Intuitively,
each task uses distinct discriminative patterns and thus the desired meta-knowledge is required
to extract these patterns simultaneously. It could be a challenging problem to represent it with a
global hyperposterior since the most significant patterns in the first task could be irrelevant or even
detrimental to the second task. Figure schematically illustrates this notion. Therefore, customized
meta-knowledge such that the patterns are most discriminative for a given task is urgently desired.
Can the meta-knowledge be adaptive to tasks? How can one achieve it? Intuitively, we could
implement this idea by reformulating the meta-knowledge as a maping function. Leveraging the
samples in the target task, the meta model produces tasks specific meta-knowledge.
Naturally yet interestingly, one can see quantitatively how customized prior knowledge improves
generalization capability, in light of the PAC-Bayes literature on the data distribution dependent-priors
(Catoni, 2007; Parrado-Herngndez et al., 2012; Dziugaite & Roy, 2018). Specifically, PAC-Bayes
bounds control the generalization error of Gibbs Classifiers. They usually depend on a tradeoff
between the empirical error of the posterior Q and a KL-divergence term K L(QkP), where P is the
prior. Since this KL-divergence term forms part of the generalization bound and is typically large in
standard PAC-Bayes approaches (Lever et al., 2013), the choice of posterior is constrained by the
need to minimize the KL-divergence between prior P and posterior Q. Thus, choosing an appropriate
prior for each task which is close to the related posterior could yield improved generalization bounds.
This encourages the study of data distribution-dependent priors for the PAC-Bayes analysis and gives
rise to principled approaches to localized PAC-Bayes analysis. Previous related work are mainly
discussed in Appendix A.
Inspired by this, we propose a Localized Meta-Learning (LML) framework by formulating meta-
knowledge as a conditional distribution over priors. Given task data distribution, we allow a meta
learner to adaptively generate an appropriate prior for a new task. The challenges of developing this
model are three-fold. First, the task data distribution is not explicitly given, and our only perception
for it is via the associated sample set. Second, it should be permutation invariant — the output of
model should not change under any permutation of the elements in the sample set. Third, the learned
model could be used for solving unseen tasks. To address these problems, we further develop a prior
predictor using Local Coordinate Coding (LCC)(Yu et al., 2009). In particular, if the classifier in
each task is specialized to a parametric model, e.g. deep neural network, the proposed LCC-based
prior predictor predicts base model parameters using the task sample set. The main contributions
include: (1) A localized meta-learning framework which provides a means to tighten the original
PAC-Bayes meta-learning bound (Pentina & Lampert, 2014; Amit & Meir, 2018) by minimizing
the task-complexity term by choosing data-dependent prior; (2) An LCC-based prior predictor, an
implementation of conditional hyperposterior, which generates local meta-knowledge for specific
task; (3) A practical algorithm for probabilistic deep neural networks by minimizing the bound
(though the optimization method can be applied to a large family of differentiable models); (4)
Experimental results which demonstrate improved performance over meta-learning method in this
field.
2 Preliminaries
Our prior predictor was implemented by Local Coordinate Coding (LCC). The LML framework
was inspired by PAC-Bayes theory for meta learning. In this section we briefly review the related
definitions and formulations.
2.1	Local Coordinate Coding
Definition 1. (Lipschitz Smoothness Yu et al. (2009).) A function f(x) in Rd is a (α, β)-Lipschitz
smooth w.r.t. a norm ∣∣ ∙ ∣∣ if ∣∣f (x) — f (x0)k ≤ α∣∣x — x0∣∣ and ∣∣f (x0) — f (x) — Vf (x)>(x0 — x)k ≤
βkx - x0k2.
Definition 2. (Coordinate Coding Yu et al. (2009).) A coordinate coding is a pair (γ, C), where
C ⊂ Rd is a set of anchor points(bases), and γ is a map of x ∈ Rd to [γu(x)]u∈C ∈ R|C| such that
UYu Yu(X) = 1. It induces thefollowingphysical approximation of X in Rd : X = 52u∈c Yu(X)U.
Definition 3. (Latent Manifold Yu et al. (2009).) A subset M ⊂ Rd is called a smooth manifold
with an intrinsic dimension |C| := dM if there exists a constant cM such that given any X ∈ M,
2
Under review as a conference paper at ICLR 2021
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
127
128
there exists |C| anchor points u1(x), . . . , u|C| (x) ∈ Rd so that ∀x0 ∈ M:
|C|
inf kx0 -x- X γj uj (x)k2 ≤ cMkx0 - xk22,
Y ∈RlCl	j=1
where Y = [γι,..., γ∣c∣]> are the local codings w.r.t. the anchorPoints.b
Definition 2 and 3 imply that any point in Rd can be expressed as a linear combination of a set
of anchor points. Later, we will show that a high dimensional nonlinear prior predictor can be
approximated by a simple linear function w.r.t. the coordinate coding, and the approximation quality
is ensured by the locality of such coding (each data point can be well approximated by a linear
combination of its nearby anchor points).
2.2	PAC-Bayes Regular Meta-Learning
In order to present the advances proposed in this paper, we recall some definitions in PAC-Bayes
theory for single-task learning and meta-learning (Catoni, 2007; Baxter, 2000; Pentina & Lampert,
2014; Amit & Meir, 2018). In the context of classification, we assume all tasks share the same input
space X, output space Y, space of classifiers (hypotheses) H ⊂ {h : X → Y} and loss function
` : Y × Y → [0, 1]. The meta learner observes n tasks in the form of sample sets S1, . . . , Sn. The
number of samples in task i is denoted by mi. Each observed task i consists of a set of i.i.d. samples
Si = {(xj, yj)}m=ι, which is drawn from a data distribution Si 〜DmW. Following the meta-learning
setup in (Baxter, 2000), we assume that each data distribution Di is generated i.i.d. from the same
meta distribution τ. Let h(x) be the prediction of x, the goal of each task is to find a classifier h
that minimizes the expected loss Ex〜D'(h(x), y). Since the underlying 'true' data distribution Di is
unknown, the base learner receives a finite set of samples Si and produces an “optimal” classifier
h = Ab(Si) with a learning algorithm Ab(∙) that will be used to predict the labels of unseen inputs.
PAC-Bayes theory studies the properties of randomized classifier, called Gibbs classifier. Let Q be a
posterior distribution over H. To make a prediction, the Gibbs classifier samples a classifier h ∈ H
according to Q and then predicts a label with the chosen h. The expected error under data distribution
D and empirical error on the sample set S are then given by averaging over distribution Q, namely
er(Q) = Eh〜QE(χ,y)〜D'(h(x),y) and ebr(Q) = Eh〜Qm1 Pj=I '(h(xj), yj), respectively.
In the context of meta-learning, the goal of the meta learner is to extract meta-knowledge contained in
the observed tasks that will be used as prior knowledge for learning new tasks. In each task, the prior
knowledge P is in the form of a distribution over classifiers H. The base learner produces a posterior
Q = Ab(S, P) over H based on a sample set S and a prior P . All tasks are learned through the
same learning procedure. The meta learner treats the prior P itself as a random variable and assumes
the meta-knowledge is in the form of a distribution over all possible priors. Let hyperprior P be an
initial distribution over priors, meta learner uses the observed tasks to adjust its original hyperprior P
into hyperposterior Q from the learning process. Given this, the quality of the hyperposterior Q is
measured by the expected task error of learning new tasks using priors generated from it, which is
formulated as:
er(Q)= EP ^QE(D,m)^τ,S^Dm er(Q = Ab(S,p)).	(I)
Accordingly, the empirical counterpart of the above quantity is given by:
1n
er(Q) = EP 〜Q — Eer(Q = Ab(Si,P)).	(2)
n
i=1
2.3	PAC-Bayes Regular Meta-Learning Bound with Gaussian Randomization
Based on the above definitions, Pentina & Lampert (2014) and Amit & Meir (2018) present regular
meta-learning PAC-Bayes generalization bounds w.r.t. hyperposterior Q. Notably, the proof technique
in Amit & Meir (2018) allows to incorporate different single task bounds. Consider the benefit of
Catoni’s bound (Catoni, 2007) (the minimization problem derived from the bound is a simple linear
combination of empirical risk plus a regularizer), here we instantiate a regular meta-learning bound
with Gaussian randomization based on that. To make fair comparison, we will adopt the same Catoni’s
bound to analysis the proposed LML framework later. Particularly, the classifier h is parameterized
as hw with w ∈ Rdw. The prior and posterior are a distribution over the set of all possible parameters
w. We choose both the prior P and posterior Q to be spherical Gaussians, i.e. P = N (wP, σw2 Idw )
3
Under review as a conference paper at ICLR 2021
Task 1	Task 2
Task n	Future Task∙
Task 1	Task 2
Task n	Future Task∙
"(w。, GldW) -► wζew ---► W鼠	Φv(D黑W)	^wζew	►混
Figure 2: Comparison between PAC-Bayes regular meta-learning (left) and LML (right) . In
regular meta-learning, the mean of prior wP is sampled from a global hyperposterior distribution
Q = N (wQ, σw2 Idw). In LML, wP is produced by a prior predictor Φv(Dnmew).
and Q = N(wQ , σw2 Idw ). The mean wP is a random variable distributed first according to the
hyperprior P, which we formulate as N(0, σw2 Idw), and later according to hyperposterior Q, which
we model as N(wQ, σw2 Idw). When encountering a new task i, we first sample the mean of prior
wiP from the hyperposterior N (wQ, σw2 Idw), and then use it as a basis to learn the mean of posterior
wiQ = Ab(Si, P), as shown in Figure 2(left). Then, we could derive the following PAC-Bayes
meta-learning bound.
129
130
131
132
133
134
135
136
137
138
139
140
141
142
143
144
145
146
147
148
149
150
151
152
153
154
155
Theorem 1. Consider the regular meta-learning framework, given the hyperprior P = N (0, σw2 Idw ).
Then for any hyperposterior Q, any c1, c2 > 0 and any δ ∈ (0, 1] with probability ≥ 1 - δ we have,
er(Q) ≤c1c2er(Q) +	(^^---c1c2-亍	+----c1 9	)∣∣wQ∣∣2 + 'X--c1c2-亍 ∣∣ E WQ	— wQ∣∣2
2c 2c2nm√σW 2cιnσ2	2c?nmiσW WP
i=1	i=1
+ X —c1c2 2 (1 + log 2n) +——c12 log 2,	(3)
c2nmiσW2 2	δ c1nσW2 δ
i=1
where c； =「-" and c2 = 二-2.To get a better understanding, Wefurthersimplif the notation
and obtain that
er(Q) ≤c1c2er(Q)	+ (X Ccc 2	+ T)∣wQ∣2	+ X . c1c2 2	k E WQ	— WQ∣2
1 2	2c2nmiσW2 2c1 nσW2 2c2nmiσW2	WP i
i ^1	i ^1	{^^^^^}
task-complexity
+ const(δ, n, mi, σW, c1, c2).	(4)
See Appendix D.4 for the proof. Notice that the expected task generalization error is bounded by the
empirical multi-task error plus two complexity terms which measures the environment-complexity
and the task-complexity, respectively.
3 PAC-Bayes Localized Meta-learning
3.1	Motivation and Overall Framework
Our motivation stems from a core challenge in PAC-Bayes meta-learning bound in (4), wherein
the task-complexity term Pn=1 2：2：味2。2 11 EwQ - wQk2, which measures the closeness between
the mean of posterior and the mean of global hyperposterior for each task, is typically vital to the
generalization bound. Finding the tightest possible bound generally depends on minimizing this
n	c01 c02 EwiQ
term. It is obvious that the optimal wQ is En=I 2。？…次∙ This solution for global hyperposterior is
required to satisfy the task similarity assumption that the optimal posteriors for each task are close
together and lie within a small subset of the model space. Under this circumstance, there exists a
global hyperposterior from which a good prior for any individual task is reachable. However, if the
optimal posteriors for each task are not related or even mutually exclusive, i.e., one optimal posterior
has a negative effect on another task, the global hyperposterior may impede the learning of some
tasks. Moreover, this complexity term could be inevitably large and incur large generalization error.
Note that wQ is the mean of hyperposterior Q and this complexity term naturally indicates the
divergence between the mean of prior wiP sampled from the hyperposterior Q and the mean of
posterior wiQ in each task. Therefore, we propose to adaptively choose the mean of prior wiP
according to task i. It is obvious that the complexity term vanishes if we set wiP = wiQ , but the prior
Pi in each task has to be chosen independently of the sample set Si . Fortunately, the PAC-Bayes
4
Under review as a conference paper at ICLR 2021
156
157
158
159
160
161
162
163
164
165
166
167
168
169
170
171
172
173
174
175
176
177
178
179
180
181
182
183
184
185
186
187
188
189
190
191
theorem allows us to choose prior upon the data distribution Di . Therefore, we propose a prior
predictor Φ : Dm → wP which receives task data distribution Dm and outputs the mean of prior
wP . In this way, the generated priors could focus locally on those regions of model parameters that
are of particular interest in solving specific tasks.
Particularly, the prior predictor is parameterized as Φv with v ∈ Rdv . We assume v to be a random
variable distributed first according to the hyperprior P, which we reformulate as N(0, σv2 Idv), and
later according to hyperposterior Q, which we reformulate as N (vQ, σv2 Idv). Given a new task i, we
first sample v from hyperposterior N (vQ, σv2 Idv) and estimate the mean of prior wiP by leveraging
prior predictor wiP = Φv(Dim). Then, the base learner utilizes the sample set Si and the prior
Pi = N (wiP , σw2 Idw) to produce a mean posterior wiQ = Ab(Si, Pi), as shown in Figure 2(right).
To make wP close to wQ in each task, what properties are the prior predictor is expected to exhibit?
Importantly, it is required to (i) uncover the tight relationship between the sample set and model
parameters. Intuitively, features and parameters yield similar local and global structures in their
respective spaces in the classification problem. Features in the same category tend to be spatially
clustered together while maintaining the separation between different classes. Take linear classifiers
as an example, let wk be the parameters w.r.t. category k, the separability between classes is
implemented as X ∙ Wk, which also explicitly encourages intra-class compactness. A reasonable
choice of wk is to maximize the inner product distance with the input features in the same category
and minimize the distance with the input features of the non-belonging categories. Besides, the prior
predictor should be (ii) category-agnostic since it will be used continuously as new tasks and hence
new categories become available. Lastly, it should be (iii) invariant under permutations of its inputs.
3.2	LCC-Based Prior Predictor
There exists many implementations, such as set transformer (Lee et al., 2018), relation network (Rusu
et al., 2019), task2vec(Achille et al., 2019), that satisfy the above conditions. We follow the idea of
nearest class mean classifier (Mensink et al., 2013), which represents class parameter by averaging
its feature embeddings. This idea has been explored in transductive few-shot learning problems (Snell
et al., 2017; Qiao et al., 2018). Snell et al. (2017) learn a metric space across tasks such that when
represented in this embedding, prototype (centroid) of each class can be used for label prediction
in the new task. Qiao et al. (2018) directly predict the classifier weights using the activations by
exploiting the close relationship between the parameters and the activations in a neural network
associated with the same category. In summary, the classification problem of each task is transformed
as a generic metric learning problem which is shared across tasks. Once this mapping has been
learned on observed tasks, due to the structure-preserving property, it could be easily generalized to
new tasks. Formally, consider each task as a K-class classification problem, and the parameter of the
classifier in task i denoted as Wi = [Wi[1], . . . , Wi[k], . . . , Wi[K]], the prior predictor for class k can
be defined as:
WP [k]=φv(Dmik ) = , E mik ml- X Φv(Xj ),	(5)
Sik~Dik mik Xj ∈Sik
where φv (∙) : Rd → Rdw is the feature embedding function, mik is the number of samples belonging
to category k, Sik and Dik are the sample set and data distribution for category k in task i. We call
this function the expected prior predictor. Since data distribution Dik is considered unknown and our
only insight as to Dik is through the sample set Sik, we approximate the expected prior predictor by
its empirical counterpart. Note that if the prior predictor is relatively stable to perturbations of the
sample set, then the generated prior could still reflect the underlying task data distribution, rather
than the data, resulting in a generalization bound that still holds perhaps with smaller probability
(Dziugaite & Roy, 2018). Formally, the empirical prior predictor is defined as:
WP[k] = ΦV(Sik) = — X φv(xj).	(6)
mik
ik xj∈Sik
Although We can implement the embedding function φv(∙) with a multilayer perceptron (MLP), both
input X ∈ Rd and model parameter W ∈ Rdw are high-dimensional, making the empirical prior
predictor Φv(∙) difficult to learn. Inspired by the local coordinate coding method, if the anchor points
are sufficiently localized, the embedding function φv(xj) can be approximated by a linear function
w.r.t. a set of codings, [γu (xj)]u∈C. Accordingly, we propose an LCC-based prior predictor, which
5
Under review as a conference paper at ICLR 2021
192
193
194
195
196
197
198
199
200
201
202
203
204
205
206
207
208
209
210
211
212
213
214
is defined as:
W P [k] = φ V(Sik) = ~ XX Yu(Xj )φv (u),	⑺
mik
xj ∈Sik u∈C
where φv(u) ∈ Rdw is the embedding of the corresponding anchor point u ∈ C. As such,
the parameters of LCC-based prior predictor w.r.t. category k can be represented as vk =
[φvk (u1), φvk (u2), . . . , φvk (u|C| )]. Lemma 1 illustrates the approximation error between empirical
prior predictor and LCC-based prior predictor.
Lemma 1. (Empirical Prior Predictor Approximation) Given the definition of w^P [k] and WP [k]
in Eq. (6) and Eq. (7), let (γ, C) be an arbitrary coordinate coding on Rd and φv(∙) be an
(α, β )-Lipschitz smooth function. We have for all X ∈ Rd
kwP [k] - WP [k]k ≤ Oɑ,β (Y,C)	⑻
Where Oα,β (Y,C) = m1k Pxj ∈Sik (αkxj - xj k + β Pu∈C kxj-uk2) and xj = Pu∈c Yu(Xj )u.
See Appendix D.1 for the proof. Lemma 1 shows that a good LCC-based prior predictor should make
x close to its physical approximation X and should be localized. The complexity of LCC coding
scheme depends on the number of anchor points |C |. We follow the optimization method in Yu et al.
(2009) to find the coordinate coding (Y, C), which is presented in Appendix B.
3.3	PAC-Bayes Localized Meta-Learning Bound with Gaussian Randomization
In order to derive a PAC-Bayes generalization bound for localized meta-learning, we first bound the
approximation error between expected prior predictor and LCC-based prior predictor.
Lemma 2. Given the definition of WP and WP in Eq. (5) and (7), let X be a compact set with radius
R, i.e., ∀x, x0 ∈ X, kx 一 x0k ≤ R. For any δ ∈ (0,1] with probability ≥ 1 一 δ, we have
kWP 一W P k2 ≤ X( √α⅛(I+r 2log( 1))+Oɑβ (γ, C)).
See Appendix D.2 for the proof. Lemma 2 shows that the approximation error between expected
prior predictor and LCC-based prior predictor depends on (i) the concentration of prior predictor
and (ii) the quality of LCC coding scheme. The first term implies the number of samples for each
category should be larger for better approximation. This is consistent with the results of estimating
the center of mass (Cristianini & Shawe-Taylor, 2004). Based on Lemma 2, using the same Catoni’s
bound. we have the following PAC-Bayes LML bound.
Theorem 2. Consider the localized meta-learning framework. Given the hyperprior P =
N(0, σv2 Idv), then for any hyperposterior Q, any c1, c2 > 0 and any δ ∈ (0, 1] with probabil-
ity ≥ 1 一 δ we have,
er(Q) ≤c1c2er(Q) + (X 9 c1c2 2 + T)kvQk2 + X	c1c2 2 kEwQ - ΦVQ (Si)k2
2c2nmiσv2	2c1nσv2	c2 nmi σw2 v i v
i=1	i=1
+ X CnSσw (σw X (√αRik(1 + r llog(4n))+ Oa，e (γ,C ))2+dwK( σv)2)
n
+X
i=1
c01 c02	4n
c2nmiσW log τ +
(9)
where c； =「-" and C?=
and obtain that
n
er(Q) ≤c1c2<sr(Q) + (X
i=1
ι-e-c2. To get a better understanding, wefurther simplify the notation
O c1c2 2 + ∕⅛)kvQk2 + X	c1c2 2 IIEwQ - ΦVQ (Si)k2
2c2nmiσv2	2c1nσv2	c2 nmi σw2 v i v
i=1	、	{	/
task-complexity
2ccσvlog 2，
+ const(α, β, R,δ, n, mi, σV, σw, c1, c2).	(10)
See appendix D.3 for the proof. Similarly to the regular meta-learning bound in Theorem 1, the
expected task error er(Q) is bounded by the empirical task error e⅛(Q) plus the task-complexity and
environment-complexity terms. The main innovation here is to exploit the potential to choose the
mean of prior WP adaptively, based on task data S. Intuitively, if the selection of the LCC-based
prior predictor is appropriate, it will narrow the divergence between the mean of prior WiP sampled
6
Under review as a conference paper at ICLR 2021
215
216
217
218
219
220
221
222
223
224
225
226
227
228
229
230
231
232
233
234
235
236
237
238
239
240
241
242
243
244
245
246
247
248
249
250
251
252
from the hyperposterior Q and the mean of posterior wiQ in each task. Therefore, the bound can be
tighter than the ones in the regular meta-learning (Pentina & Lampert, 2014; Amit & Meir, 2018).
Our empirical study in Section 4 will illustrate that the algorithm derived from this bound can
reduce task-complexity and thus achieve better performance than the methods derived from regular
meta-learning bounds.
When one is choosing the number of anchor points |C |, there is a balance between accuracy and
simplicity of prior predictor. As we increase |C |, it will essentially increase the expressive power of
Φv(∙) and reduce the task-complexity term ∣∣EwQ 一 ΦVQ(S)k2. However, atthe same time, it will
v
increase the enviornment-complexity term ∣vQ∣2 and make the bound loose. If we set |C| to 1, it
degenerates to the regular meta-learning framework.
3.4 Localized Meta-Learning Algorithm
Since the bound in (9) holds uniformly w.r.t. Q, the guarantees of Theorem 2 also hold for the resulting
learned hyperposterior Q = N(vQ, σv2 Idv), so the mean of prior wP sampled from the learned
hyperposterior work well for future tasks. The PAC-Bayes localized meta-learning bound in (9) can
be compactly written as Pn=I Ed(Qi = Ab(Si, P)) + αι∣vQ∣2 + Pn=I m∣∣EWQ - ΦvQ(Si)∣2,
where a1,α2 > 0 are hyperparameters. For task i, the learning algorithm Ab(∙) can be formulated as
w? = arg min Eeri (Qi = N(WQ,σWId )). To make fair comparison and guarantee the benefit of
i	wiQ v	i	w
the proposed LML is not from using an improved optimization method, we follow the same learning
algorithm in (Amit & Meir, 2018). Specifically, we jointly optimize the parameters of LCC-based
prior predictor v and the parameters of classifiers in each task w1, w2, . . . , wn, which is formulated
as
nn
arg min	XE,eri(wi) + αι∣vQ∣2 + X — ∣∣EwQ 一 ΦVQ(Si)∣2.	(11)
v,w1 ,...,wn v	mi v
i=1	i=1
We can optimize v and w via mini-batch SGD. The details of the localized meta-learning algorithm
is given in Appendix F. The expectation over Gaussian distribution and its gradient can be efficiently
estimated by using the re-parameterization trick Kingma & Welling (2014); Rezende et al. (2014).
For example, to sample W from the posterior Q = N(wQ, σWIdw), we first draw ξ 〜 N(0,Idw)
and then apply the deterministic function wQ + ξ σ, where is an element-wise multiplication.
4	Experiments
Datasets and Setup. We use CIFAR-100 and Caltech-256 in our experiments. CIFAR-100
Krizhevsky (2009) contains 60,000 images from 100 fine-grained categories and 20 coarse-level
categories. As in Zhou et al. (2018), we use 64, 16, and 20 classes for meta-training, meta-validation,
and meta-testing, respectively. Caltech-256 has 30,607 color images from 256 classes Griffin et al.
(2007). Similarly, we split the dataset into 150, 56 and 50 classes for meta-training, meta-validation,
and meta-testing. We consider 5-way classification problem. Each task is generated by randomly
sampling 5 categories and each category contains 50 samples. The base model uses the convolutional
architecture in Finn et al. (2017), which consists of 4 convolutional layers, each with 32 filters and a
fully-connected layer mapping to the number of classes on top. High dimensional data often lies on
some low dimensional manifolds. We utilize an auto-encoder to extract the semantic information of
image data and then construct the LCC scheme based on the embeddings. The parameters of prior
predictor and base model are random perturbations in the form of Gaussian distribution.
We design two different meta-learning environment settings to validate the efficacy of the proposed
method. The first one uses the pre-trained base model as an initialization, which utilizes all the
meta-training classes (64-class classification in CIFAR-100 case) to train the feature extractor. The
second one uses the random initialization. We compare the proposed LML method with ML-PL
method Pentina & Lampert (2014), ML-AM method Amit & Meir (2018) and ML-A which is
derived from Theorem 1. In all these methods, we use their main theorems about the generalization
upper bound to derive the objective of the algorithm. To ensure a fair comparison, all approaches
adopt the same network architecture and pre-trained feature extractor (more details can be found in
Appendix E).
7
Under review as a conference paper at ICLR 2021
MreMM,ra7B76747270
⅝vz Eo >wε3wu4
9876543210
7777777777
⅝vz Eo >wε3wu4
m∞7b767∙7270Hmmm
⅝vz Eo >wε3wu4
253
254
255
256
257
258
259
260
261
262
263
264
265
266
267
268
269
270
271
272
273
274
275
276
277
278
279
280
281
282
283
284
285
286
287
(a) With pre-trained feature extractor	(b) Without pre-trained feature extractor
Figure 3:	Average test accuracy of learning a new task for varied numbers of training tasks (|C| = 64).
Results. In Figure 3, we demonstrate the average test error of learning a new task based on the
number of training tasks, together with the standard deviation, in different settings (with or without
a pre-trained feature extractor). It is obvious that the performance continually increases as we
increase the number of training tasks for all the methods. This is consistent with the generalization
bounds that the complexity term converges to zero if large numbers of tasks are observed. ML-A
consistently outperforms ML-PL and ML-AM since the single-task bound used in Theorem 1(ML-A)
converges at the rate of O(ml) while the bounds w.r.t. ML-PL and ML-AM converge at the rate
of O(√=). This demonstrates the importance of using tight generalization bound. Moreover, our
proposed LML significantly outperforms the baselines, which validates the effectiveness of the
proposed LCC-based prior predictor. This confirms that LCC-based prior predictor is a more suitable
representation for meta-knowledge than the traditional global hyperposterior in ML-A, ML-AM,
and ML-PL. Finally, we observe that if the pre-trained feature extractor is provided, all of these
methods do better than meta-training with random initialization. This is because the pre-trained
feature extractor can be regarded as a data-dependent hyperpior. It is closer to the hyperposteior than
the randomly initialized hyperprior. Therefore, it is able to reduce the environment complexity term
and improves the generalization performance.
m∞7b767∙7270
⅝vz Eo >wε3wu4
Caltech-256
3	5	7	9	11
Number of Tasks
VWEVαħv>Ξ
CIFAr-IOO
3	5	7	9	11
Number of Tasks
Figure 4:	(a) The impacat of the number of anchor points |C | in LCC. (b) The divergence value
(normalized) between the mean generated prior wP and the mean of learned posterior wQ .
In Figure 4(b), we show the divergence between the mean of generated prior wP from meta model
and the mean of learned posterior wQ for LML and ML-A. This further validates the effectiveness of
the LCC-based prior predictor which could narrow down the divergence term and thus tighten the
bound. In Figure 4(a), we vary the number of anchor points |C | in LCC scheme from 4 to 256, the
optimal value is around 64 in both datasets. This indicates that LML is sensitive to the number of
anchor points |C |, which further affects the quality of LCC-based prior predictor and the performance
of LML.
5 Conclusion
This work contributes a novel localized meta-learning framework from both the theoretical and
computational perspectives. In order to tailor meta-knowledge to various individual task, we formulate
meta model as a mapping function that leverages the samples in target set and produces task specific
meta-knowledge as a prior. Quantitatively, this idea essentially provides a means to theoretically
tighten the PAC-Bayes meta-learning generalization bound. We propose a LCC-based prior predictor
to output localized meta-knowledge by using task information and further develop a practical
algorithm with deep neural networks by minimizing the generalization bound. An interesting
topic for future work would be to explore other principles to construct the prior predictor and apply
the localized meta-learning framework to more realistic scenarios where tasks are sampled non-i.i.d.
from an environment. Another challenging problem is to extend our techniques to derive localized
meta-learning algorithms for regression and reinforcement learning problems.
8
Under review as a conference paper at ICLR 2021
288
289
290
291
292
293
294
295
296
297
298
299
300
301
302
303
304
305
306
307
308
309
310
311
312
313
314
315
316
317
318
319
320
321
322
323
324
325
326
327
328
329
330
331
332
333
References
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C
Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In
Proceedings ofthe IEEE International Conference on Computer Vision, pp. 6430-6439, 2019.
Ron Amit and Ron Meir. Meta-learning by adjusting priors based on extended PAC-Bayes theory. In
International Conference on Machine Learning, pp. 205-214, 2018.
Maria-Florina Balcan, Mikhail Khodak, and Ameet Talwalkar. Provable guarantees for gradient-based
meta-learning. In Proceedings of the 36th International Conference on Machine Learning, ICML
2019, 9-15 June 2019, Long Beach, California, USA, pp. 424-433, 2019.
Jonathan Baxter. A model of inductive bias learning. Journal of Artificial Intelligence Research, 12:
149-198, 2000.
O Catoni. PAC-Bayesian supervised classification: The thermodynamics of statistical learning.
institute of mathematical statistics lecture notes—monograph series 56. IMS, Beachwood, OH.
MR2483528, 2007.
Nello Cristianini and John Shawe-Taylor. Kernel methods for pattern analysis, volume 173. Cam-
bridge University Press Cambridge, 2004.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Incremental learning-to-
learn with statistical guarantees. In Proceedings of the Thirty-Fourth Conference on Uncertainty
in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pp. 457-466,
2018a.
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil. Learning to learn around a
common mean. In Advances in Neural Information Processing Systems, pp. 10169-10179, 2018b.
Giulia Denevi, Carlo Ciliberto, Riccardo Grazzi, and Massimiliano Pontil. Learning-to-learn stochas-
tic gradient descent with biased regularization. In Proceedings of the 36th International Conference
on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 1566-1575,
2019.
Gintare Karolina Dziugaite and Daniel M Roy. Data-dependent PAC-Bayes priors via differential
privacy. In Advances in Neural Information Processing Systems, pp. 8430-8441, 2018.
Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of
deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pp. 1126-1135. JMLR. org, 2017.
Tomer Galanti, Lior Wolf, and Tamir Hazan. A theoretical framework for deep transfer learning.
Information and Inference: A Journal of the IMA, 5(2):159-209, 2016.
Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007.
Benjamin Guedj. A primer on pac-bayesian learning. arXiv preprint arXiv:1901.05353, 2019.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International
Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings, 2015.
Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014,
Conference Track Proceedings, 2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer,
2009.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R Kosiorek, Seungjin Choi, and Yee Whye Teh. Set
transformer: A framework for attention-based permutation-invariant neural networks. arXiv
preprint arXiv:1810.00825, 2018.
9
Under review as a conference paper at ICLR 2021
334
335
336
337
338
339
340
341
342
343
344
345
346
347
348
349
350
351
352
353
354
355
356
357
358
359
360
361
362
363
364
365
366
367
368
369
370
371
372
373
374
375
376
377
378
379
380
GUy Lever, Frangois Laviolette, and John Shawe-Taylor. Tighter PAC-Bayes bounds through
distribution-dependent priors. Theoretical Computer Science, 473:4-28, 2013.
Andreas Maurer. Algorithmic stability and meta-learning. Journal of Machine Learning Research, 6
(Jun):967-994, 2005.
Thomas Mensink, Jakob Verbeek, Florent Perronnin, and Gabriela Csurka. Distance-based image
classification: Generalizing to new classes at near-zero cost. IEEE transactions on pattern analysis
and machine intelligence, 35(11):2624-2637, 2013.
Emilio Parrado-Herndndez, Amiran Ambroladze, John Shawe-Taylor, and Shiliang Sun. PAC-Bayes
bounds with data dependent priors. Journal of Machine Learning Research, 13(Dec):3507-3531,
2012.
Anastasia Pentina and Christoph Lampert. A PAC-Bayesian bound for lifelong learning. In Interna-
tional Conference on Machine Learning, pp. 991-999, 2014.
Siyuan Qiao, Chenxi Liu, Wei Shen, and Alan L. Yuille. Few-shot image recognition by predicting pa-
rameters from activations. In 2018 IEEE Conference on Computer Vision and Pattern Recognition,
CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pp. 7229-7238, 2018.
Sachin Ravi and Hugo Larochelle. Optimization as a model for few-shot learning. In 5th Interna-
tional Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings, 2017.
Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and
approximate inference in deep generative models. In Proceedings of the 31th International
Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, pp. 1278-1286,
2014.
Omar Rivasplata, Csaba Szepesvari, John S Shawe-Taylor, Emilio Parrado-Hernandez, and Shiliang
Sun. PAC-Bayes bounds for stable algorithms with instance-dependent priors. In Advances in
Neural Information Processing Systems, pp. 9214-9224, 2018.
Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan Pascanu, Simon Osindero,
and Raia Hadsell. Meta-learning with latent embedding optimization. In 7th International
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,
2019.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. In
Advances in Neural Information Processing Systems, pp. 4077-4087, 2017.
Sebastian Thrun and Lorien Pratt. Learning to learn. Springer Science & Business Media, 2012.
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one
shot learning. In Advances in neural information processing systems, pp. 3630-3638, 2016.
Risto Vuorio, Shao-Hua Sun, Hexiang Hu, and Joseph J Lim. Toward multimodal model-agnostic
meta-learning. arXiv preprint arXiv:1812.07172, 2018.
Xin Wang, Fisher Yu, Ruth Wang, Trevor Darrell, and Joseph E Gonzalez. Tafe-net: Task-aware
feature embeddings for low shot learning. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pp. 1831-1840, 2019.
Kai Yu, Tong Zhang, and Yihong Gong. Nonlinear learning using local coordinate coding. In
Advances in neural information processing systems, pp. 2223-2231, 2009.
Fengwei Zhou, Bin Wu, and Zhenguo Li. Deep meta-learning: Learning to learn in the concept space.
arXiv preprint arXiv:1802.03596, 2018.
Luisa M. Zintgraf, Kyriacos Shiarlis, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast
context adaptation via meta-learning. In Proceedings of the 36th International Conference on
Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, pp. 7693-7702,
2019.
10
Under review as a conference paper at ICLR 2021
381
382
383
384
385
386
387
388
389
390
391
392
393
394
395
396
397
398
399
400
401
402
403
404
405
406
407
408
409
410
411
412
413
414
415
416
417
418
419
420
421
Supplementary Materials for Localized Meta-
Learning: A PAC-Bayes Analysis for Meta-Learning
Beyond Global Prior
This supplementary document contains the discussion of previous work, the technical proofs of
theoretical results and details of experiments. It is structured as follows: Appendix A gives a detailed
discussion of previous work. Appendix B presents the optimization method for LCC. Appendix C
presents notations for prior predictor. Appendix D gives the proofs of the main results. Appendix
D.1 and D.2 show the approximation error between LCC-based prior predictor and empirical prior
predictor, expected prior predictor, respectively. They are used in the proof of Theorem 2. Next, in
Appendix D.3 and D.4 we show the PAC-Bayes generalization bound of localized meta-learning
in Theorem 2 and also provides the PAC-Bayes generalization bound of regular meta-learning in
Theorem 1. Details of experiments and more empirical results are presented in Appendix E. Finally,
we summarize the localized meta-learning algorithm in Appendix F.
A Related Work
Meta-Learning. Meta-learning literature commonly considers the empirical task error by directly
optimizing a loss of meta learner across tasks in the training data. Recently, this has been successfully
applied in a variety of models for few-shot learning Ravi & Larochelle (2017); Snell et al. (2017);
Finn et al. (2017); Vinyals et al. (2016). Although Vuorio et al. (2018); Rusu et al. (2019); Zintgraf
et al. (2019); Wang et al. (2019) consider task adaptation when using meta-knowledge for specific
tasks, all of them are not based on generalization error bounds, which is the in the same spirit as
our work. Meta-learning in the online setting has regained attention recently Denevi et al. (2018b;a;
2019); Balcan et al. (2019), in which online-to-batch conversion results could imply generalization
bounds. Galanti et al. (2016) analyzes transfer learning in neural networks with PAC-Bayes tools.
Most related to our work are Pentina & Lampert (2014); Amit & Meir (2018), which provide a
PAC-Bayes generalization bound for meta-learning framework. In contrast, neither work provides a
principled way to derive localized meta-knowledge for specific tasks.
Localized PAC-Bayes Learning. There has been a prosperous line of research for learning priors
to improve the PAC-Bayes bounds Catoni (2007); GUedj (2019). Parrado-Herngndez et al. (2012)
showed that priors can be learned by splitting the available training data into two parts, one for
learning the prior, one for learning the posterior. Lever et al. (2013) bounded the KL divergence by a
term independent of data distribution and derived an expression for the overall optimal prior, i.e. the
prior distribution resulting in the smallest bound value. Recently, Rivasplata et al. (2018) bounded
the KL divergence by investigating the stability of the hypothesis. Dziugaite & Roy (2018) optimized
the prior term in a differentially private way. In summary, theses methods construct some quantities
that reflect the underlying data distribution, rather than the sample set, and then choose the prior P
based on these quantities. These works, however, are only applicable for single-task problem and
could not transfer knowledge across tasks in meta-learning setting.
B Optimization of LCC
We minimize the inequality in (8) to obtain a set of anchor points. As with Yu et al. (2009), we
simplify the localization error term by assuming X = x, and then We optimize the following objective
function:
n
argmCn XX akxj - Xj Il2 + β X kxj - uk2	s.t.∀x,	X Yu(X) = 1,	(12)
γ, i=1 xj ∈Si	u∈C	u∈C
where X = Pu∈c γu(x)u. In practice, we update C and Y by alternately optimizing a LASSO
problem and a least-square regression problem, respectively.
C Notations
Let φv(∙) : Rd → Rdw be the feature embedding function. m%k denotes the number of samples
belonging to category k. Sik and Dik are the sample set and data distribution for category k in task i,
11
Under review as a conference paper at ICLR 2021
respectively. Then, the expected prior predictor w.r.t. class k in task i is defined as:
WP [k] = φv(Dmkik ) = SikEDmik *:Xikkg).
The empirical prior predictor w.r.t. class k in task i is defined as:
WP[k] = ΦV(Sik) = — X Φv(xj).
mik
ik xj∈Sik
The LCC-based prior predictor w.r.t. class k in task i is defined as:
WP[k] = φV(Sik) = m1- XX Yu(Xj)φv(U).
ik xj ∈Sik u∈C
422
423
424
425
426
427
428
429
430
431
D Theoretical Results
D.1 Proof of Lemma 1
This lemma bounds the error between the empirical prior predictor WP [k] and the LCC-based prior
predictor W P [k].
Lemma 1 Given the definition of WP [k] and WP [k] in Eq. (6) and Eq. (7), let (γ, C) be an arbitrary
coordinate coding on Rdx and φ be an (α, β)-Lipschitz smooth function. We have for all x ∈ Rdx
kW P [k] - W P [k]k≤ ɪ X (α∣∣Xj-Xj k + β X kXj-uk2) = Oα,β (γ,C),	(13)
mik
xj ∈Sik	u∈C
where xj = Pu∈c Yu(Xj)u.
Proof. Let Xj- = 52u∈c Yu(Xj)u. Wehave
..ʌ , ` 一, ..
kΦV(Sik) - ΦV(Sik)k2
=— X kφv (Xj ) - X Yu(Xj )φv(u)k2
ik xj∈Sik	u∈C
≤^^- X (kφv (Xj ) - φv (Xj )k2 + k X Yu(Xj )(φv (U)- φv (Xj )k2)
mik
xj ∈Sik	u∈C
= -1- X (kφvM ) - φv (Xj )k2 + k X Yu(Xj )(φv(u) - φv ( X Yu(Xj )u)) - Vφv (Xj )(u - Xj )k2)
m
ik xj∈Sik	u∈C	u∈C
≤ — X (kφvX ) - φv(Xj )k2 + X IYu(Xj )lk(φV(U)- φv ( X Yu (Xj )u)) - Vφv (Xj )(u - Xj )k2)
mik
ik xj∈Sik	u∈C	u∈C
≤ ɪ X (akXj - Xjk2 + β X kXj - uk2) = Oα,β (Y,C)
mik
xj ∈Sik	u∈C
In the above derivation, the first inequality holds by the triangle inequality. The second equality
holds since Pu∈C Yu(Xj) = 1for all Xj. The last inequality uses the assumption of (α, β)-Lipschitz
smoothness of Φv(∙). This implies the desired bound.
This lemma demonstrates that the quality of LCC approximation is bounded by two terms: the
first term ∣∣Xj- - Xj ∣∣2 indicates X should be close to its physical approximation X, the second term
∣∣Xj - Uk implies that the coding should be localized. According to the Manifold Coding Theorem in
Yu et al. (2009), if the data points X lie on a compact smooth manifold M. Then given any > 0,
there exists anchor points C ⊂ M and coding Y such that
---X (αkXj - Xjk2 + β X llXj - uk2)≤ [αcM + (I + 5PdM)β]e2.	(14)
mik
xj ∈Sik	u∈C
It shows that the approximation error of local coordinate coding depends on the intrinsic dimension
of the manifold instead of the dimension of input.
12
Under review as a conference paper at ICLR 2021
432
433
434
435
436
437
438
439
440
441
D.2 Proof of Lemma 2
In order to proof Lemma 2, we first introduce a relevant theorem.
Theorem 3. (Vector-valued extension of McDiarmid’s inequality Rivasplata et al. (2018)) Let
X1,...,Xm ∈ X be independent random variables, and f : Xm → Rdw be a vector-valued
mapping function. If, for all i ∈ {1, . . . , m}, and for all x1, . . . , xm, x0i ∈ X, the function f satisfies
sup kf(x1:i-1, xi, xi+1:m) - f(x1:i-1,x0i,xi+1:m)k ≤ ci	(15)
xi ,x0i
Then Ekf(XI：m) — E[f (Xi：m)]k ≤ VPm=I c2∙ FOr any δ ∈ (0,1) with probability ≥ 1 一 δ we
have
kf(X1:m)—E[f(X1:m)]k ≤
um
tuXci2 +
i=1
JPf2 log(δ).
(16)
The above theorem indicates that bounded differences in norm implies the concentration of f(X1:m)
around its mean in norm, i.e., kf(X1:m) — E[f(X1:m)]k is small with high probability.
Then, We bound the error between expected prior predictor WP and the empirical prior predictor W P.
Lemma 3. Given the definition of wP [k] and WP [k] in (5) and (6), let X be a compact set with
radius R, i.e., ∀x, x0 ∈ X, kx — x0k ≤ R. For any δ ∈ (0, 1] with probability ≥ 1 — δ, we have
IiwP[k] 一WP[k]k ≤ α~~(1+J；Iogw)).	(17)
mik	2 δ
Proof. According to the definition of Φ V (∙) in (6), for all points xι,..., Xj-ι, Xj+ι,..., Xmk, Xj in
the sample set Sik , we have
SUP kφ v(χ1j-1, Xj, xj + 1：mk ) 一 φ v(χ1j-1, χj, xj + 1：mk )k
xi ,x0i
ɪ sup kφv(xj) — φv(xj)k ≤ ɪ sup α∣∣Xj 一 XjIl ≤ ^R,
mik xj ,x0j	mik xj ,x0j	mik
(18)
where R denotes the domain of X, say R= supx IXI. The first inequality follows from the Lipschitz
smoothness condition of Φv(∙) and the second inequality follows by the definition of domain X.
Utilizing Theorem 3, for any δ ∈ (0, 1] with probability ≥ 1 一 δ we have
kwP[k] — WP[k]k = ∣Φv(Sik) — E[ΦV(Sik)]k ≤ 工(1 + ∖∕ilog⅛).	(19)
√mik	V 2 δ
This implies the bound.	□
Lemma 3 shows that the bounded difference of function Φv(∙) implies its concentration, which can
be further used to bound the differences between empirical prior predictor WP [k] and expected prior
predictor WiP [k]. Now, we bound the error between expected prior predictor WiP and the LCC-based
prior predictor WP.
Lemma 2 Given the definition of WP and WP in (5) and (7), let X be a compact set with radius R,
i.e., ∀x, x0 ∈ X, ∣x — x0∣ ≤ R. For any δ ∈ (0,1] with probability ≥ 1 — δ, we have
kWP 一W P k2 ≤ X (√⅛ (1+r ∣log(1))+Oαβ (γ,C)!.	QO)
13
Under review as a conference paper at ICLR 2021
442
443
444
445
446
447
448
449
450
451
Proof According to the definition of WP, W P and W P, we have
kwp - W P k2
K
=X kwP [k]-w P [k]k2
k=1
K
=X kE[Φv(Sik)] - Φv(Sik)+Φv(Sik) - ΦV(Sik)k2
k=1
K
=X (kE[ΦV(Sik)] - φV(Sik)k2 + kΦV(Sik) - φV(Sik)k2 + 2(E[Φv(Sik)] — φV(Sik))>(Φv(Sik) — φV(Sik)))
k=1
K
≤ X (kE[ΦV(Sik)] - ΦV(Sik)k2 + kΦV(Sik) - ΦV(Sik)k2 + 2kE[ΦV(Sik)] - ΦV(Sik)kkΦV(Sik) - ΦV(Sik)k).
k=1	(21)
Substitute Lemma 3 and Lemma 1 into the above inequality, we can derive
PSik 〜Dmk {kwP - W P k2 ≤ X( √= (1 + r 2 log( I)) + Oα,β (γ,C))≥ 1 - δ∙ (22)
This gives the assertion.
Lemma 2 shows that the approximation error between expected prior predictor and LCC-based prior
predictor depends on the number of samples in each category and the quality of the LCC coding
scheme.
D.3 Proof of Theorem 2
Theorem 3 Let Q be the posterior of base learner Q = N (WQ, σw2 Idw ) and P be the prior
N(ΦV (S), σWIdw). The mean of prior is produced by the LCC-based prior predictor ΦV(S) in
Eq. (7) and its parameter v is sampled from the hyperposterior of meta learner Q = N(vQ, σV2 Idv).
Given the hyperprior P = N(0, σV2 Idv), then for any hyperposterior Q, any c1, c2 > 0 and any
δ ∈ (0, 1] with probability ≥ 1 - δ we have,
er(Q) ≤c1c2eT(Q) + (X ? Cccc 2 +	)kvQk2 + X 口丁I∣EwQ - ΦVQS)k2
i=1 2c2nmiσV2	2c1nσV2	i=1 c2nmiσw2 V i
where c01
n
+X
i=1
n
+X
i=1
——CL
c1c2	(ɪX
ccnmiσW I σW k=c
1 + r 2 log( ɪɔ) + Oα,β (Y, C)) + dwK( σ^ )2
c01 c02	4n	c01	2
C2ngσw log 丁 + 西拓c log δ,
(23)
1-e-cι
and c02 =
n
er(Q) ≤c1ccer(Q) + (E
i=1
C2
1-e-c2 .
c01c02
We can simplify the notation and obtain that
2c2nmiσV2
+ 2⅛ )kvQk2+ X C2⅛ kE wQ - φ VQ (Si)k2
+ const(α, β,R, δ, n, mi).	(24)
Proof Our proof contains two steps. First, we bound the error within observed tasks due to observing
a limited number of samples. Then we bound the error on the task environment level due to observing
a finite number of tasks. Both of the two steps utilize Catoni’s classical PAC-Bayes bound Catoni
(2007) to measure the error. We give here a general statement of the Catoni’s classical PAC-Bayes
bound.
Theorem 4. (Classical PAC-Bayes bound, general notations) Let X be a sample space and X be
some distribution over X, and letF be a hypotheses space of functions over X. Define a loss function
g(f, X) : F × X → [0, 1], and let X1G , {X1, . . . , XG} be a sequence of G independent random
14
Under review as a conference paper at ICLR 2021
variables distributed according to X. Let π be some prior distribution over F (which must not
depend on the samples X1, . . . , XG). For any δ ∈ (0, 1], the following bounds holds uniformly for
all posterior distribution ρ over F (even sample dependent),
PXG 〜x (XE E g(f,X) ≤ r⅛c "ɪ X E g(f,Xg) + κL(*; + log 1 # , ∀ρ)
1 i.i.d (X~Xf ~ρ	1 — e c G Z—f f ~ρ	G X C	J
≥ 1 - δ.	(25)
452
453
454
455
456
457
458
459
460
461
462
463
464
465
466
467
468
469
470
First step We utilize Theorem 4 to bound the generalization error in each of the observed tasks.
Let i ∈ 1, . . . , n be the index of task. For task i, we substitute the following definition into the
Catoni’s PAC-Bayes Bound. Specifically, Xg , (xij, yij), K , mi denote the samples and X , Di
denotes the data distribution. We instantiate the hypotheses with a hierarchical model f , (v, w),
where V ∈ Rdv and W ∈ Rdw are the parameters of meta learner (prior predictor) Φv(∙) and
base learner h(∙) respectively. The loss function only considers the base learner, which is defined
as g(f,X) ，'(hw(x),y). The prior over model parameter is represented as ∏，(P, P)，
(N (0, σv2 Idv ),N(WP, σw2 Idw)), a Gaussian distribution (hyperprior of meta learner) centered at 0
and a Gaussian distribution (prior of base learner) centered at WP, respectively. We set the posterior
to ρ , (Q, Q) , (N(vQ, σv2 Idv ), N (WQ, σw2 Idw )), a Gaussian distribution (hyperposterior of
meta learner) centered at vQ and a Gaussian distribution (posterior of base learner) centered at WQ .
According to Theorem 4, the generalization bound holds for any posterior distribution including
the one generated in our localized meta-learning framework. Specifically, we first sample v from
hyperposterior N (vQ, σv2 Idv) and estimate WP by leveraging expected prior predictor WP = Φv(D).
The base learner algorithm Ab(S, P) utilizes the sample set S and the prior P = N(WP, σw2 Idw)
to produce a posterior Q = Ab(S, P) = N (WQ, σw2 Idw). Then we sample base learner parameter
w from posterior N(wQ, σWIdw) and compute the incurred loss '(hw(x), y). On the whole, meta-
learning algorithm Am(S1, . . . , Sn, P) observes a series of tasks S1, . . . , Sn and adjusts its hyperprior
P = N(vP,σv2Idv) into hyperposterior Q = Am(S1, . . . ,Sn,P) = N (vQ, σv2 Idv).
The KL divergence term between prior π and posterior ρ is computed as follows:
KL(ρkπ)
E log PfI =	E	E	log N(VQ,σVIdv)N(WQ,σWIdw)
f ~p	π(f)	v~N(VQ,σv Idv)w~N(WQ,σw Idw)	N(0,σV Idv )n(wp ,σWIdw)
E	log N (VQ,σv Idv)
VZN (vQ,σv Idv )	N(O,σV Idv )
2⅛ N"2 + V 〜N ah Idv) 2σi k
v~N (vQ,σv Idv )w~N (WQQw Idw)
wQ - wP k2.
I	N(wQ,σWIdw )
g N(wp,σWIdw)
(26)
In our localized meta-learning framework, in order to make K L(Q||P) small, the center of prior
distribution wP is generated by the expected prior predictor wP = ΦV(D). However, the data
distribution D is considered unknown and our only insight as to Dik is through the sample set
Sik. In this work, we approximate the expected prior predictor ΦV(D) with the LCC-based prior
predictor WP = Φv(S). Denote the term E	21τ- kwQ - wPk2 by E21τ- kwQ - wPk2
VZN (VQ,σv2 Idv) 2σw	V 2σw
for convenience, we have
E 2⅛k
σW
wQ - wPk2 =E-ɪ-kwQ - WP + WP - wPk2
V 2σ2
W
+
E
E
=E 亲 [k
σW
≤e yr [k
V 2σW2
WQ - WPk2 + kwP - wpk2 + 2(wQ - WP)>(wP - wp)]
WQ - WPk2 + kWP - WPk2 +2kwQ - WPkkWP - WPk]
≤~1~EkwQ - Φν(S)k2 + -1rEkWP - wpk2.
σ2 V	σ2 V
WW
(27)
15
Under review as a conference paper at ICLR 2021
Since WP = ΦV(Si) = [ΦV(Si1),…，ΦV(Sik),…，ΦV(SiK)], we have
K
EkWQ - ΦV(Si)k2
V
EE ∣∣wQ[k] - φ V(Sik)k2
-2(EwQ[k])>(ΦVQ(Sik)) + kΦVQ(Sik)k2 + V[kΦv(Sik)k]
=E	kEwQ[k]- ΦvQ(Sik)k2
k=1
= kEwQ - ΦvQ(Si)k2 + dwKσ
V
dV	2
+ |C| σV
V2,
(28)
where V[∣∣ΦV(Sik)k] denotes the variance of ∣∣ΦV(Sik)k∙ The last equality uses the fact that dy =
V
|C|dW. Combining Lemma 2, for any δ0 ∈ (0, 1] with probability ≥ 1 - δ0 we have
E 与 kwQ — wpk2
V 2σW2	i
1	σ 1K
≤ 鬲 kEWi- φ vq (Si) k +dw K σw) + 鬲 X
1 + r 2 log( δ )) + Oα,β (Y,C)!
(29)
Then, according to Theorem 4, we obtain that for any
(χ,y)〜DiV〜N(VQ ,σ2Idv )w〜N(WQ ,σWIdw )
号> 0
'(hw (χ),y)
mi
≤	c2	∙ ɪ X
― 1 - e-c2 mi 乙
j=1
V〜N(VQ ,σ2Idv )w〜N(WQ ,σwIdw )
`(hw (xj), yj)
+ (I-') ∙ mi (2⅛ kvQk2 + V 〜N (vE,σ2 Idv) 2⅛ kwQ -WP k2 +log W) , ∀Q}≥
for all observed tasks i = 1,...,n. Define δ0 = δ2i and combine inequality (29), We obtain
1- δi,
(30)
PSi ~Dmi
(χ,y)〜DiV〜N(VQ,σvIdv )w〜N(WQ ,σwIdw )
'(hw (x),y)
mi
≤ c ∙ ɪ X
― 1 - e-c2	mi 乙
j=1
V〜N(VQ,σ2Idv )w~N(WQ,σwIdw )
'(hw (Xj ), yj )
+ (1 -
2⅛ kv
Qk2 + σw kE WQ - φ vQ (Si)k2+log δi+ dw K(σv )2
+ɪ
σ2
w
Σ
k=1
1
e-c2 )mi
K
(I + r 2 log( δ )) + Oα,β (Y, C)) ), ∀Q} ≥ 1 - δi,
(31)
Using the notations in Section 3, the above bound can be simplified as
V
V

E
E
E
E
E
E
E
E
E
E
i	E	er(Ab(Si, Pi))
[v~N(vQbvIdv ),WP =φv(D),pi=N(WP,σwIdw )
≤
:一	E	e^r (Ab(Si,Pi))
-c2 V〜N(VQ,σvIdv ),wP=φv(D),Pi=N(WP,σwIdw)
1-e
+ ∩	-eʒ- (τ^2^IIvQk2 +	2IIEwQ - φvq(Si)k2 +log -ξ- + dWK(—v)2
(1 - e-c2 )mi	2σV2	σw2 V i	δi	σw
+⅛^ X( √⅛(I+r 1log( δi))+O* (γ, C))!, ∀Q) ≥ 1 - δi.
(32)
Second step Next we bound the error due to observing a limited number of tasks from the environment.
We reuse Theorem 4 with the following substitutions. The samples are (Di, mi, Si), i = 1, . . . , n,
16
Under review as a conference paper at ICLR 2021
where (Di, mi) are sampled from the same meta distribution T and Si 〜Dmi. The hyposthe-
sis is parameterized as Φv(D) with meta learner parameter V. The loss function is g(f, X) ,
E
E
'(hw(x),y), where WQ = Ab(Si,Pi). Let π，N(0,σ2Idv) be the prior
(χ,y)〜Dw〜N(WQ ,σWIdw )
over meta learner parameter, the following holds for any δ0 > 0,
P(Dmi …一……{(D,E)5S%v 〜N (vE,σ22 Idv)W 〜N (wE,σw Idw )(x层 D,W(X)M
c1	1 n
≤ 1 一 e-c1 • n Σ1V〜N(vQ,σ2Idv )w〜N(wQ,σwIdw )(χ,y)〜Di ( W(x),y)
+ ∩---I-C- (y⅛IIvQk2 + log ”) ,∀Q] ≥ 1 - δ0.
(1 — e-c1)n 2σV2	δ0
Using the term in Section 3, the above bound can be simplified as
P(Dmi YfSi 〜Dm ,i=1,…,n{ er(Q)
(33)
(34)
1n
≤ —c1-----V	E	er(Ab(Si,Pi))
1 - e-c1	n i=1 V~N(vQ,σvIdv ),wp =Φv(D),Pi=N(WP,σwIdw )
+ ∩--二、(X⅛ kνQk2 +log ?) , ∀Q] ≥ 1 - δ0,
(1 — e-c1)n 2σV2	δ0
Finally, by employing the union bound, we could bound the probability of the intersection of the
events in (32) and (34) For any δ > 0, set δo，2 and δi，* for i = 1,...,n,we have
P(Dmi )fSi 〜Dmi ,i=1,…,n{ er(Q)
≤_________c1c2_________
_(1 — e-cι)(1 — e-c2)
n
+• 1 X
1 — e-c1 n
i=1
1K
+ 4r X
σW k⅛
1
1n
• - X
n i=1
1
E	er(Ab(Si,Pi))
v~N(VQ,σV Idv ),wP=φv(D),Pi=N(WP ,σw Idw)
(1 - e-c2)mi
1
1+
212 IlvQk2 + ~2^ kEWQ - φVQ (Si) k2 +log-J-
2
llog(4n ))+Oa，e (γ,C)	+ dwK(σv )2
十 (1 — e-c1 )n 12σ2
VQk2 + log δ^ ,∀q} ≥ 1 - δ.
(35)
We can further simplify the notation and obtain that
P(Dmi )fSi 〜Dmi,i=1,…,n{ er(Q) ≤ c1c2er(Q)
+(X 产F +	)kvQk2 + X'1JkEwQ - ΦVQ(Si)k2
i=1 2c2nmiσV2	2c1nσV2	i=1 c2nmiσw2 V i
+const(α, β, R, δ, n, mi), ∀Q	≥ 1 - δ,
47i where c； =「-" and c2 =「-2.This completes the proof.
(36)
472 D.4 Proof of Theorem 1
Theorem 2 Let Q be the posterior of base learner Q = N(WQ, σw2 Idw ) and P be the prior
N(WP, σw2 Idw ). The mean of prior is sampled from the hyperposterior of meta learner Q =
N(WQ, σw2 Idw ). Given the hyperprior P = N(0, σw2 Idw ), then for any hyperposterior Q, any
17
Under review as a conference paper at ICLR 2021
c1, c2 > 0 and any δ ∈ (0, 1] with probability ≥ 1 - δ we have,
er(Q) ≤i2er(Q) + (X2C2⅛2⅛ + 2Ccσw)kwQk2+ X 2C2≤⅛k5WQ-WQk2
473
where c01
n	c01c02	1	2n
+X c2nmσw (2+log 万)+
c1	1	2
------2 log I,
c1nσw2	δ
(37)
ι-e-cι and c2 = ι-c-
Proof Instead of generating the mean of prior with a prior predictor, the vanilla meta-learning frame-
work directly produces the mean of prior WP by sampling from hyperposterior Q = N (WQ,σw2 Idw ).
Then the base learner algorithm Ab(S, P) utilizes the sample set S and the prior P = N(WP,σw2 Idw)
to produce a posterior Q = Ab(S, P) = N (WQ, σw2 Idw). Similarly with the two-steps proof in
Theorem 2, we first get an intra-task bound by using Theorem 4. For any δi > 0, we have
Pq∙ Dmil EE	E	'(hw(x),y)
i〜i [(x,yhDiWP〜N(wQ,σWIdw )w〜N(wQ,σWIdw )
c 1 mi
≤  ---—2- . — X、	E	E	'(hw(Xj),yj)
1 - e-c2 mi j=1WP〜N(WQ,σwIdw )w~N(WQ,σwIdw)
+ -------^――----(CL kwQk2	+	E --^kwQ	— WP Il2 + log ɪ ), ∀Q >	≥ 1 — δi,
(I	— e-c2 )	∙ mi ∖2σw	WP~N(wQ,σwIdw) 2σw	δi J J
(38)
The term E	7⅛ k WQ — WP k2 can be simplified as
WP〜N(WQ,σwIdw )2σw" i	i
E	-ɪ ∣∣wq — WP ∣∣2
WP ~N(WQ,σw Idw ) 2σW
=Jr ( E kWQk2 - 2( E wQ)>wq + ∣wq∣2 + V [∣wP∣∣])
2σW2	WP	WP	WiP
2⅛ (kWPWQ - wQk2 + R
(39)
where V [kwiP k] denotes the variance of kwiP k. Then we get an inter-task bound. For any δ0 > 0,
WiP
we have
P(Dmi )~τ,si~Dmi,i=1,...,η{(D,mE)〜TSJDmWP 〜N (WQ,σw Idw "N (WE,σw Idw )(x,yElDi(X),"'
1n
≤ —c1--------V E	E	E	'(hW (x),y)
1 — e-c1 ni=1 WP JN (WQ,σw2 Idw )WJN (WQ,σw2 Idw )(x,y)JDi
+ ∩—1-cιx	(ʒ-ɪIIwQk2 + log!) ,∀Q∖ ≥ 1 -δ0.
(1 — e-c1)n 2σW2	δ0
For any δ > 0, set δο，2 and δi，金 for i = 1,...,η. Using the union bound, We finally get
P(Dimi)Jτ,SiJDimi,i=1,...,n er(Q)
cc	1 n
≤ ---------詈-----------∙ - V	E	er(Ab(Si,Ρi))
(1 — e-c1)(1 — e-c2) ni 1vJN(vQ,σv2Idv),WP=Φv(D),Pi=N(WP,σw2 Idw)
(40)
1
(1 — e-c2) ∙ mi
k E wiQ —
WP
+1-1-1
1n
-- X
ni=1
(41)
18
Under review as a conference paper at ICLR 2021
Similarly, we can further simplify the notation and obtain that
P(Dmi )~τ,Si~Dm ,i=1,…,n{er(Q) ≤ c1c2er(Q)
+(X 二 + 2⅛ )kwQk2+X ； kwEP wQ-wQk2
474
475
476
477
478
479
480
481
482
483
484
485
486
487
488
489
490
491
492
493
494
495
496
497
498
499
500
501
502
503
504
505
506
507
508
509
510
511
+const(δ,n,mi), ∀Q ≥ 1 - δ,
where c； =「-" and c2 =「-2.This completes the proof.
(42)
E Details of Experiments
While the theorems consider bounded-loss, we use an unbounded loss in our experiments, we can
have theoretical guarantees on a variation of the loss which is clipped to [0; 1]. Besides, in practice
the loss function is almost always smaller than one.
E.1 Data Preparation
We used the 5-way 50-shot classification setups, where each task instance involves classifying
images from 5 different categories sampled randomly from one of the meta-sets. We did not employ
any data augmentation or feature averaging during meta-training, or any other data apart from the
corresponding training and validation meta-sets.
E.2 Network Architechture
Auto-Encoder for LCC For CIFAR100, the encoder is 7 layers with 16-32-64-64-128-128-256
channels. Each convolutional layer is followed by a LeakyReLU activation and a batch normalization
layer. The 1st, 3rd and 5th layer have stride 1 and kernel size (3, 3). The 2nd, 4th and 6th layer have
stride 2 and kernel size (4, 4). The 7th layer has stride 1 and kernel size (4, 4). The decoder is the
same as encoder except that the layers are in reverse order. The input is resized to 32 × 32. For
Caltech-256, the encoder is 5 layers with 32-64-128-256-256 channels. Each convolutional layer is
followed by a LeakyReLU activation and a batch normalization layer. The first 4 layers have stride 2
and kernel size (4, 4). The last layer has stride 1 and kernel size (6, 6). The decoder is the same as
encoder except that the layers are in reverse order. The input is resized to 96 × 96.
Base Model The network architecture used for the classification task is a small CNN with 4 con-
volutional layers, each with 32 filters, and a linear output layer, similar to Finn et al. (2017). Each
convolutional layer is followed by a Batch Normalization layer, a Leaky ReLU layer, and a max-
pooling layer. For CIFAR100, the input is resized to 32 × 32. For Caltech-256, the input is resized to
96 × 96.
E.3 Optimization
Auto-Encoder for LCC As optimizer we used AdamKingma & Ba (2015) with β1 = 0.9 and
β2 = 0.999. The initial learning rate is 1 × 10-4. The number of epochs is 100. The batch size is
512.
LCC Training We alternatively train the coefficients and bases of LCC with Adam with β1 = 0.9
and β2 = 0.999. In specifics, for both datasets, we alternatively update the coefficients for 60 times
and then update the bases for 60 times. The number of training epochs is 3.The number of bases is
64. The batch size is 256.
Pre-Training of Feature Extractor We use a 64-way classification in CIFAR-100 and 150-way
classification in Caltech-256 to pre-train the feature embedding only on the meta-training dataset. For
both CIFAR100 and Caltech-256, an L2 regularization term of 5e-4 was used. We used the Adam
optimizer. The initial learning rate is 1 × 10-3, β1 is 0.9 and β2 is 0.999. The number of epochs is
50. The batch size is 512.
19
Under review as a conference paper at ICLR 2021
512
513
514
515
516
517
518
519
520
521
522
523
524
525
526
527
528
529
530
531
532
533
534
535
536
537
538
539
Meta-Training We use the cross-entropy loss as in Amit & Meir (2018). Although this is inconsistent
with the bounded loss setting in our theoretical framework, we can still have a guarantee on a variation
of the loss which is clipped to [0, 1]. In practice, the loss is almost always smaller than one. For
CIFAR100 and Caltech-256, the number of epochs of meta-training phase is 12; the number of epochs
of meta-testing phase is 40. The batch size is 32 for both datasets. As optimizer we used Adam with
β1 = 0.9 and β2 = 0.999. In the setting with a pre-trained base model, the learning rate is 1 × 10-5
for convolutional layers and 5 × 10-4 for the linear output layer. In the setting without a pre-trained
base model, the learning rate is 1 × 10-3 for convolutional layers and 5 × 10-3 for the linear output
layer. The confidence parameter is chosen to be δ = 0.1. The variance hyper-parameters for prior
predictor and base model are σw = σv = 0.01. The hyperparameters α1, α2 in LML and ML-A are
set to 0.01.
E.4 More Experimental Results
We also compare with two typical meta-learning few-shot learning methods: MAML (Finn et al.,
2017) and MatchingNet (Vinyals et al., 2016). Both two methods use the Adam optimizer with initial
learning rate 0.0001. In the meta-training phase, we randomly split the samples of each class into
support set (5 samples) and query set (45 samples). The number of epochs is 100. For MAML, the
learning rate of inner update is 0.01.
m∞70∞50m
⅝vz Eo AUeJnUaV
(a)	With pre-trained feature extractor
(b)	Without pre-trained feature extractor
Figure 5: Average test accuracy of learning a new task for varied numbers of training tasks (|C| = 64).
In Figure 5, we demonstrate the average test error of learning a new task based on the number of
training tasks, together with the standard deviation, in different settings (with or without a pre-trained
feature extractor). We can find that all PAC-Bayes baselines outperform MAML and MatchingNet.
Note that MAML and MatchingNet adopt the episodic training paradigm to solve the few-shot
learning problem. The meta-training process requires millions of tasks and each task contains limited
samples, which is not the case in our experiments. Scarce tasks in meta-training leads to severely
meta-overfitting. In our method, the learned prior serves both as an initialization of base model and
as a regularizer which restricts the solution space in a soft manner while allowing variation based on
specific task data. It yields a model with smaller error than its unbiased counterpart when applied to a
similar task.
F Pseudo Code
20
Under review as a conference paper at ICLR 2021
Algorithm 1 Localized Meta-Learning (LML) algorithm
Input: Data sets of observed tasks: Si,..., Sn.
Output: Learned prior predictor Φ parameterized by v.
Initialize v ∈ Rdv and wi ∈ Rdw for i = 1 . . . , n.
Construct LCC scheme (γ, C) from the whole training data by optimizing Eq. (12).
while not converged do
for each task i ∈ {1, . . . , n} do
Sample a random mini-batch from the data Si0 ⊂ Si .
Approximate Eeri(Wi) using Si.
v
end for
Compute the objective in (11), i.e. J — Pn=IEeri(Wi) + αιkvQk2 + Pn=I 詈∣∣EwQ -
= v	= mi v
Φ vQ (Si) IR
Evaluate the gradient of J w.r.t. {v, W1, . . . , Wn} using backpropagation.
Take an optimization step.
end while
21