Under review as a conference paper at ICLR 2021
Graph Neural Network Acceleration via
Matrix Dimension Reduction
Anonymous authors
Paper under double-blind review
Abstract
Graph Neural Networks (GNNs) have become the de facto method for ma-
chine learning on graph data (e.g., social networks, protein structures, code
ASTs), but they require significant time and resource to train. One alter-
native method is Graph Neural Tangent Kernel (GNTK), a kernel method
that corresponds to infinitely wide multi-layer GNNs. GNTK’s parameters
can be solved directly in a single step, avoiding time-consuming gradient de-
scent. Today, GNTK is the state-of-the-art method to achieve high training
speed without compromising accuracy. Unfortunately, solving for the ker-
nel and searching for parameters can still take hours to days on real-world
graphs. The current computation of GNTK has running time O(N4), where
N is the number of nodes in the graph. This prevents GNTK from scal-
ing to datasets that contain large graphs. Theoretically, we present two
techniques to speed up GNTK training while preserving the generalization
error: (1) We use a novel matrix decoupling method to reduce matrix di-
mensions during the kernel solving. This allows us to reduce the dominated
computation bottleneck term from O(N 4) to O(N3). (2) We apply sketch-
ing to further reduce the bottleneck term to o(Nω), where ω ≈ 2.373 is the
exponent of current matrix multiplication. Experimentally, we demonstrate
that our approaches speed up kernel learning by up to 19× on real-world
benchmark datasets.
1	Introduction
Graph Neural Networks (GNNs) have quickly become the de facto method for machine learn-
ing on graph data. GNNs have delivered ground-breaking results in many important areas
of AI, including social networking Yang et al. (2020a), bio-informatics Zitnik & Leskovec
(2017); Yue et al. (2020), recommendation systems Ying et al. (2018), and autonomous driv-
ing Weng et al. (2020); Yang et al. (2020b). However, efficient GNNs training has become a
major challenge with the relentless increase in the complexity of GNN models and dataset
sizes, both in terms of the number of graphs in a dataset and the sizes of the graphs.
Recently, a new direction for fast GNN training is to use Graph Neural Tangent Kernel
(GNTK). Solving for the kernel and searching for the parameters in GNTK is equivalent to
using gradient descent to train an infinitely wide multi-layer GNN. GNTK is significantly
faster than iterative gradient descent optimization because solving the parameters in GNTK
is just a single-step kernel learning process. In addition, GNTK allows GNN training to scale
with GNN model sizes because the training time grows only linearly with the complexity
of GNN models. However, GNTK training can still take hours to days on typical GNN
datasets today.
Our key observation is that, during the process of solving parameters in GNTK, most of
the training time and resource is spent on multiplications of large matrices. Let N be the
maximum number of nodes in the graphs, these matrices can have sizes as large as N2 × N2!
This means a single matrix multiplication takes at least N4 time, and it prevents GNTK
from scaling to larger graphs. Thus, in order to speed up GNTK training, we need to reduce
matrix dimensions.
1
Under review as a conference paper at ICLR 2021
Our Contributions. We present two techniques to speed up GNTK: (1) We use a novel
matrix decoupling method to reduce matrix dimensions during the training without harming
the calculation results. This reduces the dominated computation bottleneck term from
O(N4) to O(N3). (2) We propose a sketching method to further reduce the bottleneck term
to o(Nω), where ω ≈ 2.373 is the exponent of current matrix multiplication.
We provide theoretical results that the resulting randomized GNTK still has a good gener-
alization bound. In experiments, we evaluate our method on standard graph classification
benchmarks. Our method improves GNTK training time by up to 19× while maintaining
the same level of accuracy.
2	Background
Notations. For a positive integer n, We define [n] := {1, 2, •一，n}. For two integers a ≤ b,
We define [a, b] := {a, a + 1,…，b}, and (a, b) := {a + 1,…，b — 1}. Similarly we define
[a, b) and (a, b]. For a full rank square matrix A, we use A-1 to denote its true inverse. We
define the big O notation such that f(n) = O(g(n)) means there exists n0 ∈ N+ and M ∈ R
such that f (n) ≤ M ∙ g(n) for all n ≥ n°. For a matrix A, we use IlAll or JAg to denote
its spectral norm. We use IlAIlF to denote its Frobenius norm. We use AT to denote the
transpose of A. For a matrix A and a vector x, we define IlxIlA := VxτAx. We use φ to
denote the ReLU activation function, i.e. φ(z) = max{z, 0}. For a function f : R → R, we
use f f to denote the derivative of f.
Graph neural network (GNN). A GNN has L levels of Aggregate operations, each
followed by a Combine operation. A Combine operation has R fully-connected layers with
output dimension m, and uses ReLU as non-linearity. In the end, the GNN has a ReadOut
operation that corresponds to the pooling operation of normal neural networks.
Consider a graph G = (V, E) with |V | = N. Each node u ∈ V has a feature vector hu ∈ Rd.
In GNN we will use vectors h(l,r) such that l denotes the number of levels, and r denotes
the number of hidden layers. The size is h(u1,0) ∈ Rd , and h(ul,0) ∈ Rm for all l ∈ [2 : L]. We
define the initial vector h(u0,R) = hu ∈ Rd, ∀u ∈ U .
For any l ∈ [L], the Aggregate operation aggregates the information from last level:
h(i⑼ := C . V	h(i-i，R)
U	: = Cu	v/v	∙
v ∈N (u)∪{u}
where Cu ∈ R is a scaling parameter, and N(u) is the set of neighbor no des of u. The
Combine operation then uses R fully-connected layers with ReLU activation: ∀r ∈ [R],
hul,r) := (Cφ∕m)1/2 ∙ φ(W((Kr ∙ hul,rτ)) ∈ Rm,
where Cφ ∈ R is a scaling parameter, W (1,1) ∈ Rm×d, and W (l,r) ∈ Rm×m for all (l, r) ∈
[L] × [R]\{(1, 1)}. We let W := {W ((,r)}(∈[L],r∈[R] . Finally, the output of the GNN on graph
G = (V, E) is computed by a ReadOut operation:
fgnn(W,G) := V huL，R)∈ Rm
u∈V
For more details see Appendix Section B.1.
Neural tangent kernel. We briefly review the neural tangent kernel definition. Let φ
denote the non-linear activation function, e.g. φ(z) = max{z, 0} is the ReLU activation
function.
Definition 2.1 (Neural tangent kernel Jacot et al. (2018)). For any input two data points
x, z ∈ Rd, we define the kernel mapping Kntk : Rd × Rd → R
Kntk
(x, z) :=
Jw 〜N (0,id)
φf(wτx)φf( WTz)XT Zdw
2
Under review as a conference paper at ICLR 2021
where N(0, Id) is a d-dimensional multivariate Gaussian distribution, and φ is the derivative
of activation function φ. If φ is ReLU, then φz(t) = 1 if t ≥ 0 and φz(t) = 0 if t < 0. Given
x1,x2,…，Xn ∈ Rd, We define kernel matrix K ∈ Rn×n as follows: Kij = K(xi,xj).
The lower bound on smallest eigenvalue of neural tangent kernel matrix K (say λ = λmin (K),
see Du et al. (2019b); Arora et al. (2019a;b); Song & Yang (2019); Brand et al. (2020)) and
separability of input data points {x1,x2, ∙∙∙ , Xn} (δ, see Li & Liang (2018); Allen-Zhu et al.
(2019a;b)) play a crucial role in deep learning theory. Due to Oymak & Soltanolkotabi
(2020), λ is at least Ω(δ∕n2) which unifies the two lines of research. The above work shows
that as long as the neural network is sufficiently wide m ≥ poly(n,d, 1∕δ, 1∕λ) (where n is
the number of input data points, d is the dimension of each data) , running (S)GD type
algorithm is able to minimize the training loss to zero.
Kernel regression and equivalence. Kernel method or Kernel regression is a fundamen-
tal tool in machine learning Avron et al. (2017; 2019); Scholkopf & Smola (2018). Recently,
it has been shown that training an infinite-width neural network is equivalent to kernel
regression Arora et al. (2019a). Further, the equivalence even holds for regularized neural
network and kernel ridge regression Lee et al. (2020).
Let,s consider the following neural network, minw 2∣∣Y 一 fnn(W,X)∣∣2. Training this neu-
ral network is equivalent to solving the following neural tangent kernel ridge regression
problem: mine 1 l∣Y — fntk(β,X)∣2∙ Note that fntk(β,x) = Φ(x)τβ ∈ R and fntk(β,X)=
[fntk(β, xι),…,fntk(β, Xn)]τ ∈ Rn are the test data predictors. Here, Φ is the feature map
corresponding to the neural tangent kernel (NTK):
dfnn(W,x) dfnn(W,z)∖]
Kntk(X,z) = W K ∂W ， ∂W /
where x,z ∈ Rd are any input data, and Wr i'i.d. N(0, I), r = 1, ∙∙∙ ,m.
3 Our GNTK formulation
We show our GNTK formulation in this section. Our formulation builds upon the GNTK
formulas of Du et al. (2019a). The descriptions in this section is presented in a simplified
way. See Section B.2 and B.3 for more details.
3.1	Exact GNTK formulas
We consider a GNN with L Aggregate operations and L Combine operations, and each
Combine operation has R fully-connected layers. Let G = (U, E) and H = (V, F) be two
graphs with |U| = N and |V| = N'. We use AG and AH to denote the adjacency matrix
of G and H. We give the recursive formula to compute the kernel value Kgntk(G, H) ∈ R
induced by this GNN, which is defined as
Kgntk(GH )：=	W	Kd⅛WG)
W 〜N (o,ι)	∂W
∂fgnn(W,H)
∂W
where N(0, I) is a multivariate Gaussian distribution.
Recall that the GNN uses scaling factors cu for each node u ∈ G. We define CG to be
the diagonal matrix such that (CG)u = cu for any u ∈ U. Similarly we define CH . We
will use intermediate matrices K%r)(G,H) ∈ RN×nN for each Q ∈ [0 : L] and r ∈ [0 : R],
where l denotes the level of Aggregate and Combine operations, and r denotes the level
of fully-connected layers inside a Combine operation.
Initially we define K(O，R)(G, H) ∈ RN×n' as follows: ∀υ ∈ U,v ∈ V,
[K(01R)(GH)]u,v := (hu,hv).
where hu , hv ∈ Rd are the input features of u and v.
Next we recursively define K%r)(G, H) for l ∈ [L] and r ∈ [R].
3
Under review as a conference paper at ICLR 2021
Reference	Time
Du et al. (2019a)	O(n2) ∙ (Tmat(N, N,d) + L ∙ N4 + LR ∙ N2)
Thm. 4.1 and 5.1	O(n2) ∙ (Tmat(N, N,d) + L ∙ Tmat (N, N, b) + LRNr
Table 1: When L = O(1) and R = O(1), the dominate term in previous work is O(N4). We
improve it to Tmat (N, N, b).
Exact Aggregate. The Aggregate operation gives the following formula:
[KC,0)(G,H)]u,v := CuCv	E E	[KCTR)(G,H必.
a∈N (u)∪{u} b∈N (v)∪{v}
In the experiments the above equation is computed using Kronecker product:
Vec(KCQGH)) ：= ((CGAG)乳(CHAH)) ∙ vec(KdR)(G,H)).	(1)
The dominating term of the final running time does not come from Combine and ReadOut
operations, thus we defer their details into Section B.2 in Appendix.
We briefly review the running time in previous work.
Theorem 3.1 (Running time of Du et al. (2019a), simplified version of Theorem D.2).
Consider a GNN with L Aggregate operations and L Combine operations, and each
Combine operation has R fully-connected layers. We compute the kernel matrix using n
graphs {Gi = (Vi, Ei)}in=1 with |Vi | ≤ N. Let d ∈ N+ be the dimension of the feature vectors.
The total running time is O(n2) ∙ (Tmat(N, N, d) + L ∙ N4 + LR ∙ N2).
When using GNN, we usually use constant number of operations and fully-connected layers,
i.e., L = O(1), R = O(1), and we have d = o(N), while the size of the graphs can grow
arbitrarily large. Thus it is easy to see that the dominated term in the above running time
is N4, the major contribution of this work is to reduce it to o(Nω), where ω ≈ 2.373 is the
exponent of current matrix multiplication.
3.2	Approximate GNTK formulas
We follow the notations of previous section. Now the goal is to compute an approximate
version of the kernel value K(G, H) ∈ R such that Kgntk(G, H) ≈ Kgntk(G, H). We will use
intermediate matrices KWr)(G,H) ∈ RN×n' for each Q ∈ [0 : L] and r ∈ [0 : R]. In the
approximate version we add two random Gaussian matrices SG ∈ Rb×N and SH ∈ Rb'×NN
in the Aggregate operation, where b ≤ N and b ≤ N N.
Approximate Aggregate operation. In the approximate version, we add two sketch-
ing matrices SG ∈ Rb × N and SH ∈ RbN × N N:
K C,0)(G, H) ：= CGAG ∙ (S】SG) ∙ K (J ㈤(G, H) ∙ (SH SH) ∙ AH CH.	⑵
Not that for the special case SHSG = SHSH = I, the Eq. (2) degenerates to the the following
case:
K%0)(G, H) = CGAG ∙ K(IR)(G H) ∙ AHCH.
This equation is exactly the same as the equation Eq. (1) of the exact case. See Fact 4.2 for
why they are equivalent.
4 Our techniques : running time
The main contribution of our paper is to show that we can accelerate the computation of
GNTK defined in Du et al. (2019a), while maintaining a similar generalization bound.
In this section we present the techniques that we use to achieve faster running time. We
will provide the generalization bound in the next section. We first present our main running
time theorem.
4
Under review as a conference paper at ICLR 2021
Theorem 4.1 (Main theorem, running time part, Theorem D.1). Consider a GNN with
L Aggregate operations and L Combine operations, and each Combine operation has
R fully-connected layers. We compute the kernel matrix using n graphs n graphs {Gi =
(Vi, Ei)}in=1 with |Vi | ≤ N . Let b = o(N) be the sketch size. Let d ∈ N+ be the dimension of
the feature vectors. The total running time is
O(n2) ∙ (Tmat(N, N,d) + L ∙ Tmat(N N,b) + LR ∙ N2).
Note that we improve the dominating term from N4 to Tmat (N, N, b). We achieve this
improvement using two techniques:
1.	We accelerate the multiplication of a Kronecker product with a vector by decoupling
it into two matrix multiplications of smaller dimensions. In this way we improve
the running time from N4 down to Tmat (N, N, N). We present more details in
Section 4.2.
2.	We further accelerate the two matrix multiplications by using two sketching matri-
ces. In this way, we improve the running time from Tmat (N, N, N) to Tmat (N, N, b).
We present more details in Section 4.3.
4.1	Notations and known facts
Fast matrix multiplication. We use the notation Tmat (n, d, m) to denote the time of
multiplying an n × d matrix with another d × m matrix. Let ω denote the exponent of
matrix multiplication, i.e., Tmat (n, n, n) = nω. The first result shows ω < 3 is Strassen
(1969). The current best exponent is ω ≈ 2.373, due to Williams (2012); Le Gall (2014).
The common belief is ω ≈ 2 in the computational complexity community Cohn et al.
(2005); Williams (2012); Jiang et al. (2020). The following fact is well-known in the fast
matrix multiplication literature Coppersmith (1982); Strassen (1991); Burgisser et al. (1997)
: Tmat (a, b, c) = O(Tmat (a, c, b)) = O(Tmat (c, a, b)) for any positive integers a, b, c.
Kronecker product and vectorization. Given two matrices A ∈ Rn1 ×d1 and B ∈
Rn2 ×d2. We use 0 to denote the Kronecker product, i.e., for C = A 0 B ∈ RnIn2 ×d1 d2, the
(iι + (i2 — 1) ∙ nι,jι + (j2 — 1) ∙ dι)-th entry of C is AiIjI Bi2,j2, ∀iι ∈ [nι],i2 ∈ [n2],jι ∈
[d1], j2 ∈ [d2]. For any give matrix H ∈ Rd1 ×d2 , we use h = vec(H) ∈ Rd1 d2 to denote the
vector such that hj1+(j2-i)∙d1 = HjIj2, ∀jι ∈ [di], j2 ∈ [d2].
4.2	Speedup via Kronecker product equivalence
We make the following observation about kronecker product and vectorization. Proof is
delayed to Section E.
Fact 4.2 (Equivalence between two matrix products and Kronecker product then ma-
trix-vector multiplication). Given matrices A ∈ Rn1 ×d1 , B ∈ Rn2 ×d2, and H ∈ Rd1 ×d2 ,
We have Vec(AHBT) = (A 0 B) ∙ vec(H).
In GNTK, when computing the l-th Aggregate operation for l ∈ [L], we need to compute
a product (A 0 B) ∙ Vec(H) with sizes A, B, H ∈ RN×n. Note that A = CGAg, B =
CHAH, H = K(l-1,R) (G, H) (see Eq. (1) in Section 3.1). Computing this product naively
takes O(N 4) time, since the Kronecker product A 0 B already has size N2 × N2 . This is
the O(N4) term of Du et al. (2019a).
In our experiments, We instead compute AHBt, which takes O(Tmat(N,N,N)) time. And
this is how we get our first improvement in running time.
4.3	Speedup via sketching matrices
The following lemma shows that the sketched version of matrix multiplication approximates
the exact matrix multiplication. This justifies why we can use sketching matrices to speed
up calculation.
5
Under review as a conference paper at ICLR 2021
Lemma 4.3 (Informal version of Lemma 5.4). Given n2 matrices H1,1, •一,Hn,n ∈ RN×N
and n matrices Ai, ∙∙ ∙ , An ∈ RN×N. Let Si ∈ Rb×N denote a random matrix where each
entry is + 圭 or — -√ζ, each with probability 2. Then with high probability, We have the
following guarantee: for all i, j ∈ [n],
AirSirSiHijSjSjAj ≈ AirHi,jAj.
Note that the sizes of the matrices are Ai, Aj, Hi,j ∈ RN×N, and Si, Sj ∈ Rb×N. They corre-
Spond to Ai = CGiAGi,Aj = CGj AGj, Hij = K(l-1,R)(Gi, Gj) (see Eq. (2) in Section 3.2).
Directly computing AJHijAj takes Tmat(N,Ν,Ν) time.
After adding two sketching matrices, using a certain ordering of computation, we can avoid
the time-consuming step of multiplying two N × N matrices. More specifically, we compute
AJSJSiHijSjSjrAj in the following order:
•	Computing AJSJ and SjAj both takes Tmat(N,Ν,b) time.
•	Computing S% ∙ Hij takes Tmat(b,N,N) time.
•	Computing (SiHij) ∙ Sjr takes Tmat(b,N,b) time.
•	Computing (AJSJ) ∙ (SiHijSj) takes Tmat(N, b, b) time.
•	Computing (AjSjSiHi,jSj) ∙ (SjAj) takes Tmat(N,b,N) time.
Thus, we improve the running time from Tmat(N, N, N) to Tmat(N, N, b).
5	Our techniques : error analysis
In this section, we prove that even though adding the sketching matrices in the GNTK
formula will introduce some error, this error can be bounded, and we can still prove a
similar generalization bound as that of Du et al. (2019a).
Theorem 5.1 (Informal version of Theorem C.4). For each i ∈ [n], if the labels {yi}in=1
SatiSfy y = αι ∑u∈v①u, 31 + ΣT=ι α21 ∑u∈v(hu, β2i>2l, where h = Cu Σv∈n("”{“} hv,
α1,α2,α4, •一α2τ ∈ R, β1,β2,β4, •一,β2τ ∈ Rd, and under the assumptions of Assump-
tion C.6, and if we further have the conditions that
τ
4 ∙ α1∣∣β1∣∣2 + E4√π(2l — 1) ∙ α21∣∣β21∣∣2 = o(n), N = o(√n),
l=1
then the generalization error of the approximate GNTK can be upper bounded by
Lo(fgntk) =	E [2(fgntk(G), y)] S O(1∕nc), where constant C ∈ (0,1).
(G,y)〜D
We use a standard generalization bound of kernel methods of Bartlett & Mendelson (2002)
(Theorem C.1) which shows that in order to prove a generalization bound, it suffices to
upper bound IIyIlKK — and tr[K]. We present our bound on IlyIIKK-ι. The bound on tr[K] is
simpler. For the full version of the proofs, please see Section C.
Lemma 5.2 (Informal version of bound on IlyIlKK-ι). Under Assumption C.6, We have
τ
IlyIlK-1 ≤ 4∣α1∣∙ ∣∣β1∣∣2 + E4√π(2l - 1)|a2i| ∙ ^21112”
l=1
We provide a high-level proof sketch here. We first compute all the variables in the approxi-
mate GNTK formula to get a close-form formula of K . Then combining with the assumption
on the labels y, We show that IlyIIKK-1 is upper bounded by
IIyIlKK-1 ≤ (4α2 ∙ βτH ∙ (HJH)-1 ∙ HJe)1/2,
6
Under review as a conference paper at ICLR 2021
where H, H ∈ Rd×n are two matrices such that ∀i, j ∈ [n],
[H TH]i,j = INiClAGi HGi ∙ HGj AGjCGj1Nj,
[HTH]i,j = INiCTiATi(STiSGi)HG ∙ HGj (STjSGj)AGjCGj lNj.
NeXt We show that HtH is a PSD approximation of H H.
Claim 5.3 (PSD approximation). We have (1 - 110)HTH W HTH W (1 + *)HtH.
Note that using this claim, we have
IIyilK-ι ≤ (4α2 ∙ βτH ∙ (HtH)-1 ∙ HTe)1/2 ≤ (8α2 ∙ βτH ∙ (HTH)-1 ∙ HTβ)1/2 ≤ 4 ∙ α∣∣βg
which finishes the proof. Now it remains to prove the claim. We prove it by using the
following lemma which upper bounds the error of adding two sketching matrices. The proof
of this lemma is deferred to Section E.
Lemma 5.4 (Error bound of adding two sketching matrices). Let R ∈ Rb1 ×n, S ∈ Rb2 ×n
be two independent AMS matrices Alon et al. (1999). Let β = O(log1.5 n). For any matrix
A ∈ Rn×n and any vectors g, h ∈ Rn, the following holds with probability 1 - 1/poly(n)
IgTRTRASTSh - gTAh1 ≤ WHg"2Mh"2 + WHgTA"2"h"2+ √P2 W网2M"F.
Using this lemma and the assumption of sketching sizes in the lemma statement, we can
prove the following coordinate-wise upper bound:
|[HTH]i,j - [HTH]i,j| ≤ 10 ∙ [HTH]i,j.
Then We can upper bound IlHtH 一 HTH∣∣2 ≤ 10∣∣HTH∣∣2, which proves the claim. Thus
we finish the proof.
6	Experiments
In this section, we evaluate our proposed GNTK acceleration algorithm on various graph
classification tasks. More details about the experiment setup can be found in Section F of
the supplementary material.
Datasets. We test our method on 7 benchmark graph classification datasets, including 3
social networking dataset (COLLAB, IMDBBINARY, IMDBMULTI) and 4 bioinformatics
datasets (PTC, NCL1, MUTAG and PROTEINS) Yanardag & Vishwanathan (2015). For
bioinformatics dataset, each node has its categorical features as input feature h to the
algorithm. For social network dataset where nodes have no input feature, we use degree of
each node as its feature to better represent its structural information. The dataset statistics
are shown in Table 2.
Baselines. We compare our proposed results with a number of state-of-the-art baselines
for graph classification: (1) State-of-the-art deep graph neural networks architectures, in-
cluding Graph Convolution Network (GCN) Kipf & Welling (2017), GraphSAGE Hamilton
et al. (2017), PATCHY-SAN Niepert et al. (2016), Deep Graph CNN (DGCNN) Zhang
et al. (2018a) and Graph Isomorphism Network (GIN) Xu et al. (2018a). (2) Kernel based
methods, including the WL subtree kernel Shervashidze et al. (2011), Anonymous Walk
Embeddings (AWL) Ivanov & Burnaev (2018), and RetGK Zhang et al. (2018b). (3) Graph
neural tangent kernel (GNTK) Du et al. (2019a). For deep learning methods, GNTK,
RetGK and AWL, we report accuracy reported in the original papers. For WL subtree, we
report the accuracy of the implementation used in Xu et al. (2018a).
Results. We perform 10-fold cross validation and report the mean and standard deviation
of accuracy. We show our performance by comparing with state-of-the-art Graph learning
methods, including the original GNTK method. The accuracy is shown in Table 2 and
7
Under review as a conference paper at ICLR 2021
Datasets	COLLAB	IMDB-B	IMDB-M	PTC	NCI1	MUTAG	PROTEINS
# of graphs	5000	1000	1500	344	4110	188	1113
# of classes	3	2	3	2	2	2	2
Avg # of nodes	74.5	19.8	13.0	25.5	29.8	17.9	39.1
GCN	79.0 ± 1.8	74.0 ± 3.4	51.9 ± 3.8	64.2 ± 4.3	80.2 ± 2.0	85.6 ± 5.8	76.0 ± 3.2
GraphSAGE	-	72.3 ± 5.3	50.9 ± 2.2	63.9 ± 7.7	77.7 ± 1.5	85.1 ± 7.6	75.9 ± 3.2
PATCHY-SAN	72.6 ± 2.2	71.0 ± 2.2	45.2 ± 2.8	60.0 ± 4.8	78.6 ± 1.9	92.6 ± 4.2	75.9 ± 2.8
DGCNN	73.7	70.0	47.8	58.6	74.4	85.8	75.5
GIN	80.2 ± 1.9	75.1 ± 5.1	52.3 ± 2.8	64.6 ± 7.0	82.7 ± 1.7	89.4 ± 5.6	76.2 ± 2.8
WL Subtree	78.9 ± 1.9	73.8 ± 3.9	50.9 ± 3.8	59.9 ± 4.3	86.0 ± 1.8	90.4 ± 5.7	75.0 ± 3.1
AWL	73.9 ± 1.9	74.5 ± 5.9	51.5 ± 3.6	-	-	87.9 ± 9.8	-
RetGK	81.0 ± 0.3	71.9 ± 1.0	47.7 ± 0.3	62.5 ± 1.6	84.5 ± 0.2	90.3 ± 1.1	75.8 ± 0.6
GNTK	83.6 ± 1.0	76.9 ± 3.6	52.8 ± 4.6	67.9 ± 6.9	84.2 ± 1.5	90.0 ± 8.5	75.6 ± 4.2
Ours	83.6 ± 1.0	76.9 ± 3.6	52.8 ± 4.6	67.9 ± 6.9	84.2 ± 1.5	90.0 ± 8.5	75.6 ± 4.2
Table 2: Classification accuracy (%) for graph classification datasets with matrix de-
coupling. We report the result of our proposed method, optimizing on original GNTK model.
Datasets	COLLAB	IMDB-B	IMDB-M	PTC	NCI1	MUTAG	PROTEINS
GNTK	> 24 hrs	546.4	686.0	46.5	10, 084.7	8.0	1, 392.0
Ours	4, 523.0 (> 19×)	90.7 (6×)	112.5 (6.1×)	13.5 (3.4×)	7, 446.8 (1.4×)	3.0 (2.7×)	782.7 (1.8×)
Table 3: Running time analysis for our matrix decoupling method (in seconds). We
report the kernel calculation time between the original GNTK method and our accelerated model.
running time is shown in Table 3. Our matrix decoupling method (MD) doesn’t harm the
result of GNTK while significantly accelerates the learning time of neural tangent kernel.
Our proposed method achieves multiple times of improvements for all the datasets. In par-
ticular, on COLLAB, our method achieves more than 19 times of learning time acceleration.
We observe that the improvement of our method depends on the sizes of the graphs. For
large-scale dataset like COLLAB, we achieve highest acceleration because matrix multiplica-
tion dominates the overall calculation time. And for bioinformatics datasets where number
of nodes is relatively small, the improvement is not as prominent. Note that we only show
the running time comparison between our method and the original GNTK method, because
other state-of-the-art deep GNN methods takes significantly longer to learn via gradient
descent Du et al. (2019a). Analysis of our sketching method can be found in Section F of
the supplementary material.
7	Conclusion
Graph Neural Networks (GNNs) have become the most important method for machine learn-
ing on graph data (e.g., social networks, protein structures), but training GNNs efficiently
is a major challenge. One alternative method is Graph Neural Tangent Kernel (GNTK), a
kernel method that is equivalent to train infinitely wide multi-layer GNNs using gradient de-
scent. GNTK’s parameters can be solved directly in a single step, avoiding time-consuming
gradient descent. Because of this, GNTK has become the state-of-the-art method to achieve
high training speed without compromising accuracy. Unfortunately, GNTK still takes hours
to days to train on real-world graphs because it has a computation bottleneck of O(N4),
where N denotes the number of nodes in the graph. We present two techniques to mitigate
this bottleneck: (1) We use a novel matrix decoupling method to reduce matrix dimensions
during the kernel solving. This allows us to reduce this dominated computation bottleneck
from O(N 4) to O(N 3). (2) We apply sketching to further reduce the bottleneck to o(Nω),
where ω ≈ 2.373 is the exponent of current matrix multiplication. We demonstrate that our
approaches speed up kernel learning by up to 19× on real-world benchmark datasets.
8
Under review as a conference paper at ICLR 2021
References
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. On the convergence rate of training recurrent
neural networks. In NeurIPS, 2019a.
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song. A convergence theory for deep learning via
over-parameterization. In ICML, 2019b.
Noga Alon, Yossi Matias, and Mario Szegedy. The space complexity of approximating the
frequency moments. Journal of Computer and system sciences, 58(1):137-147, 1999.
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis
of optimization and generalization for overparameterized two-layer neural networks. In
International Conference on Machine Learning (ICML), pp. 322-332. https://arxiv.
org/abs/1901.08584, 2019a.
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and Ruosong
Wang. On exact computation with an infinitely wide neural net. In NeurIPS. arXiv
preprint arXiv:1904.11955, 2019b.
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker,
and Amir Zandieh. Random fourier features for kernel ridge regression: Approximation
bounds and statistical guarantees. In ICML. https://arxiv.org/pdf/1804.09893.pdf,
2017.
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya Velingker,
and Amir Zandieh. A universal sampling method for reconstructing signals with simple
fourier transforms. In STOC. https://arxiv.org/pdf/1812.08723.pdf, 2019.
Peter L Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk
bounds and structural results. Journal of Machine Learning Research, 3(Nov):463-482,
2002.
Jan van den Brand, Binghui Peng, Zhao Song, and Omri Weinstein. Training (over-
parametrized) neural networks in near-linear time. arXiv preprint arXiv:2006.11648,
2020.
Peter Burgisserj Michael Clausen, and Mohammad A Shokrollahi. Algebraic complexity
theory, volume 315. Springer Science & Business Media, 1997.
Henry Cohn, Robert Kleinberg, Balazs Szegedy, and Christopher Umans. Group-theoretic
algorithms for matrix multiplication. In 46th Annual IEEE Symposium on Foundations
of Computer Science (FOCS’05), pp. 379-388. IEEE, 2005.
Don Coppersmith. Rapid multiplication of rectangular matrices. SIAM Journal on Com-
puting, 11(3):467-471, 1982.
Simon S Du, Kangcheng Hou, Russ R Salakhutdinov, Barnabas Poczos, Ruosong Wang,
and Keyulu Xu. Graph neural tangent kernel: Fusing graph neural networks with graph
kernels. In Advances in Neural Information Processing Systems (NeurIPS), pp. 5723-
5733. https://arxiv.org/abs/1905.13192, 2019a.
Simon S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh. Gradient descent provably
optimizes over-parameterized neural networks. In ICLR. arXiv preprint arXiv:1810.02054,
2019b.
Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large
graphs. In Advances in neural information processing systems (NeurIPS), pp. 1024-1034.
https://arxiv.org/pdf/1706.02216.pdf, 2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers:
Surpassing human-level performance on imagenet classification. In Proceedings of the
IEEE international conference on computer vision, pp. 1026-1034, 2015.
9
Under review as a conference paper at ICLR 2021
Sergey Ivanov and Evgeny Burnaev. Anonymous walk embeddings. arXiv preprint
arXiv:1805.11921, 2018.
Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence
and generalization in neural networks. In Advances in neural information processing
systems (NeurIPS), pp. 8571—8580, 2018.
Shunhua Jiang, Zhao Song, Omri Weinstein, and Hengjie Zhang. Faster dynamic matrix
inverse for faster lps. arXiv preprint arXiv:2004.07470, 2020.
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional
networks. In International Conference on Learning Representations (ICLR). https://
arxiv.org/pdf/1609.02907.pdf, 2017.
Franqois Le Gall. Powers of tensors and fast matrix multiplication. In Proceedings of the 39th
international symposium on symbolic and algebraic computation (ISSAC), pp. 296—303.
ACM, 2014.
Jason D Lee, Ruoqi Shen, Zhao Song, Mengdi Wang, and Zheng Yu. Generalized leverage
score sampling for neural networks. In NeurIPS, 2020.
Yuanzhi Li and Yingyu Liang. Learning overparameterized neural networks via stochastic
gradient descent on structured data. In NeurIPS, 2018.
Mathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural
networks for graphs. In International conference on machine learning, pp. 2014-2023,
2016.
Samet Oymak and Mahdi Soltanolkotabi. Towards moderate overparameterization: global
convergence guarantees for training shallow neural networks. In IEEE Journal on Selected
Areas in Information Theory. https://arxiv.org/pdf/1902.04674.pdf, 2020.
Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector machines,
regularization, optimization, and beyond. Adaptive Computation and Machine Learning
series, 2018.
Nino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and
Karsten M Borgwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning
Research, 12(9), 2011.
Zhao Song and Xin Yang. Quadratic suffices for over-parametrization via matrix chernoff
bound. In arXiv preprint. https://arxiv.org/pdf/1906.03593.pdf, 2019.
Volker Strassen. Gaussian elimination is not optimal. 1969.
Volker Strassen. Degeneration and complexity of bilinear maps: some asymptotic spectra.
J. reine angew. Math, 413:127-180, 1991.
Xinshuo Weng, Yongxin Wang, Yunze Man, and Kris M Kitani. Gnn3dmot: Graph neural
network for 3d multi-ob ject tracking with 2d-3d multi-feature learning. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 6499-6508,
2020.
Virginia Vassilevska Williams. Multiplying matrices faster than coppersmith-winograd. In
Proceedings of the forty-fourth annual ACM symposium on Theory of computing (STOC),
pp. 887-898. ACM, 2012.
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph
neural networks? In International Conference on Learning Representations (ICLR).
https://arxiv.org/abs/1810.00826, 2018a.
Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi Kawarabayashi, and
Stefanie Jegelka. Representation learning on graphs with jumping knowledge networks.
In International Conference on Machine Learning, pp. 5453-5462, 2018b.
10
Under review as a conference paper at ICLR 2021
Pinar Yanardag and SVN Vishwanathan. Deep graph kernels. In Proceedings of the 21th
ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.
1365-1374, 2015.
Carl Yang, Jieyu Zhang, Haonan Wang, Sha Li, Myungwan Kim, Matt Walker, Yiou Xiao,
and Jiawei Han. Relation learning on social networks with multi-modal graph edge varia-
tional autoencoders. In Proceedings of the 13th International Conference on Web Search
and Data Mining, pp. 699-707, 2020a.
Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd: Point-based 3d single stage ob ject
detector. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pp. 11040-11048, 2020b.
Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure
Leskovec. Graph convolutional neural networks for web-scale recommender systems. In
Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery
& Data Mining, pp. 974-983, 2018.
Xiang Yue, Zhen Wang, Jingong Huang, Srinivasan Parthasarathy, Soheil Moosavinasab,
Yungui Huang, Simon M Lin, Wen Zhang, Ping Zhang, and Huan Sun. Graph embedding
on biomedical networks: methods, applications and evaluations. Bioinformatics, 36(4):
1241-1251, 2020.
Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learn-
ing architecture for graph classification. In Thirty-Second AAAI Conference on Artificial
Intelligence, 2018a.
Zhen Zhang, Mianzhi Wang, Yijian Xiang, Yan Huang, and Arye Nehorai. Retgk: Graph
kernels based on return probabilities of random walks. In Advances in Neural Information
Processing Systems, pp. 3964-3974, 2018b.
Marinka Zitnik and Jure Leskovec. Predicting multicellular function through multi-layer
tissue networks. Bioinformatics, 33(14):i190-i198, 2017.
11