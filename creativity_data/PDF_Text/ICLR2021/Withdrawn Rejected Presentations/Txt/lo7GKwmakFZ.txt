Under review as a conference paper at ICLR 2021
Average	Reward Reinforcement Learning
with Monotonic Policy Improvement
Anonymous authors
Paper under double-blind review
Ab stract
In continuing control tasks, an agent’s average reward per time step is a more natural
performance measure compared to the commonly used discounting framework
since it can better capture an agent’s long-term behavior. We derive a novel lower
bound on the difference of the long-term average reward for two policies. The
lower bound depends on the average divergence between the policies and on the
so-called Kemeny constant, which measures to what degree the unichain Markov
chains associated with the policies are well-connected. We also show that previous
work based on the discounted return (Schulman et al., 2015; Achiam et al., 2017)
results in a non-meaningful lower bound in the average reward setting. Based on
our lower bound, we develop an iterative procedure which produces a sequence of
monotonically improved policies for the average reward criterion. When combined
with Deep Reinforcement Learning (DRL) methods, the procedure leads to scalable
and efficient algorithms for maximizing the agent’s average reward performance.
Empirically we demonstrate the effectiveness of our method on continuing control
tasks and show how discounting can lead to unsatisfactory performance.
1	Introduction
The goal of Reinforcement Learning (RL) is to build agents that can learn high-performing behaviors
through trial-and-error interactions with the environment. Broadly speaking, modern RL tackles two
kinds of problems: episodic tasks and continuing tasks. In episodic tasks, the agent-environment
interaction can be broken into separate distinct episodes, and the performance of the agent is simply
the sum of the rewards accrued within an episode. Examples of episodic tasks include training an
agent to learn to play Go (Silver et al., 2016; 2018) or Atari video games (Mnih et al., 2013), where
the episode terminates when the game ends. In continuing tasks, such as controlling robots with long
operating lifespans (Peters & Schaal, 2008; Schulman et al., 2015; Haarnoja et al., 2018), there is
no natural separation of episodes and the agent-environment interaction continues indefinitely. The
performance of an agent in a continuing task is more difficult to quantify since even for bounded
reward functions, the total sum of rewards is typically infinite.
One way of making the long-term reward objective meaningful for continuing tasks is to apply
discounting, i.e., We maximize the discounted sum of rewards r0 + γrι + γ2r2 + ∙…for some
discount factor γ ∈ (0, 1). This is guaranteed to be finite for any bounded reward function. However
the discounted objective biases the optimal policy to choose actions that lead to high near-term
performance rather than to high long-term performance. Such an objective — while useful in certain
applications — is not appropriate when the goal is optimize long-term behavior. As argued in
Chapter 10 of Sutton & Barto (2018) and in Naik et al. (2019), a more natural objective is to use the
average reward received by an agent over every time-step. While the average reward setting has been
extensively studied in the classical Markov Decision Process literature (Howard, 1960; Blackwell,
1962; Veinott, 1966; Bertsekas et al., 1995), it is much less commonly used in reinforcement learning.
An important open question is whether recent advances in RL for the discounted reward criterion can
be naturally generalized to the average reward setting.
One major source of difficulty with modern DRL algorithms lies in controlling the step-size for
policy updates. In order to have better control over step-sizes, Schulman et al. (2015) constructed a
lower bound on the difference between the expected discounted return for two arbitrary policies π
and π0 . The bound is a function of the divergence between these two policies and the discount factor.
1
Under review as a conference paper at ICLR 2021
Schulman et al. (2015) showed that iteratively maximizing this lower bound generates a sequence of
monotonically improved policies in terms of their discounted return.
In this paper, we first show that the policy improvement theorem from Schulman et al. (2015) results
in a non-meaningful bound in the average reward case. We then derive a novel result which lower
bounds the difference of the average rewards based on the divergence of the policies. The bound
depends on the average divergence between the policies and on the so-called Kemeny constant,
which measures to what degree the unichain Markov chains associated with the policies are well-
connected. We show that iteratively maximizing this lower bound guarantees monotonic average
reward policy improvement. Similar to the discounted case, the problem of maximizing the lower
bound can be approximated with DRL algorithms which can be optimized using samples collected
in the environment. We describe in detail two such algorithms: Average Reward TRPO (ATRPO)
and Average Cost CPO (ACPO), which are average reward versions of algorithms based on the
discounted criterion (Schulman et al., 2015; Achiam et al., 2017). Using the MuJoCo simulated
robotic benchmark, we carry out extensive experiments with the ATRPO algorithm and show that
it is more effective than their discounted counterparts for these continuing control tasks. To our
knowledge, this is one of the first paper to address DRL using the long-term average reward criterion.
2	Preliminaries
Consider a Markov Decision Process (MDP)(SUtton & Barto, 2018) (S, A, P, r, μ) where the state
space S and action space A are assumed to be finite. The transition probability is denoted by
P : S ×A×S → [0,1], the bounded reward function r : S ×A→ [rmin, rmax], and μ : S → [0,1]
is the initial state distribution. Let π = {π(a∣s) : S ∈ S, a ∈ A} be a stationary policy, and Π is the
set of all stationary policies. Here we discuss the two objective formulations for continuing control
tasks: the average reward approach and discounted reward approach.
Average Reward Approach
In this paper, we will focus exclusively on unichain MDPs, which is when the Markov chain
corresponding to every policy contains only one recurrent class and a finite but possibly empty set of
transient states. The average reward objective is defined as:
1	N-1
P⑺= Jim — E Er(St,at) = E [r(s,a)].	⑴
N→∞ N T~π δ—s~d∏
_ t=0	a~π
Here d∏(S) := IimN→∞ N PN-I P(St = s∣∏) = limt→∞ P(St = s∣∏) is the stationary state
distribution under policy π, T = (so, ao,...,) is a sample trajectory. We use T ~ π to indicate that
the trajectory is sampled from policy π, i.e. s0 〜μ, at 〜∏(∙∣St), and St+ι 〜P(∙∣St, at). In the
unichain case, the average reward ρ(π) is state-independent for any policy π (Bertsekas et al., 1995).
We express the average-reward value function as Vπ(S) := Eτ~π	t∞=0(r(St, at) - ρ(π))S0 = S
and action-value function as	Qπ (S,	a)	:=	Eτ~π	t∞=0(r(St, at)	- ρ(π))S0 = S,	a0	= a . We
define the average reward advantage function as Aπ (S, a) := Qπ (S, a) - Vπ(S).
Discounted Reward Approach
For some discount factor γ ∈ (0, 1), the discounted reward objective is defined as
ργ(π)
E
τ~π
∞
γtr(St, at)
t=0
Tl---- E	[r(S,a)].
1 - γ s~dπ,γ
a~π
(2)
where d∏,γ (S) := (1 一 γ) Ε∞=0 γtP (St = s∣∏) is known as the future discounted state visitation
distribution under policy π. Note that unlike the average reward objective, the discounted objective
depends on the initial state distribution μ. It can be easily shown that d∏,γ(S) → d∏ (S) for all S as
γ → 1. The discounted value function is defined as Vγπ (S) := Eτ~π	t∞=0 γtr(St, at)S0 = S and
discounted action-value function Qγπ (S, a) := Eτ~π Pt∞=0 γtr(St, at)S0 = S, a0 = a . Finally,
the discounted advantage function is defined as Aγπ (S, a) := Qγπ (S, a) 一 Vγπ (S).
2
Under review as a conference paper at ICLR 2021
It is well-known that limγ→1(1 - γ)ργ(π) = ρ(π), implying that the discounted and average reward
objectives are equivalent in the limit as γ approaches 1 (Blackwell, 1962). We will further discuss
the relationship between the discounted and average reward value functions in the supplementary
materials and prove that limγ→1 Aγπ(s, a) = Aπ(s, a) (see Corollary A.1).
3 Montonically Improvement Guarantees for Discounted RL
In many modern RL literature (Schulman et al., 2015; 2017; Abdolmaleki et al., 2018; Vuong et al.,
2019), algorithms iteratively update policies within a local region, i.e., at iteration k we find policy
πk+1 by maximizing ργ (π) within some region D(π, πk) ≤ δ for some divergence measure D. This
approach allows us to control the step-size of each update using different choices of D and δ which
can lead to better sample efficiency (Peters & Schaal, 2008). Schulman et al. (2015) derived a policy
improvement bound based on a specific choice of D :
Pγ(∏k+ι) - Pγ(∏k) ≥ 1—— S〜E [A∏k(s,a)] - C ∙ max[Dτv(∏k+ι k ∏k)[s]]	(3)
a~∏k++ι
where DTV(∏0 k ∏)[s] ：= 2 £ɑ ∣∏0(a∣s) 一 π(a∣s)∣ is the total variation divergence for policies π
and π 0 , and C is some constant which does not depend on the divergence term DTV . Schulman
et al. (2015) showed that by choosing πk+1 such that the right hand side of (3) is maximized, we are
guaranteed to have ργ (πk+1) ≥ ργ (πk). This provided the theoretical foundation for an entire class
of scalable policy optimization algorithms based on efficiently maximizing the right-hand-side of (3)
(Schulman et al., 2015; 2017; Wu et al., 2017; Abdolmaleki et al., 2018; Vuong et al., 2019).
A natural question arises here is whether the iterative procedure described by Schulman et al. (2015)
also guarantees improvement w.r.t. the average reward. Since the discounted and average reward
objectives are equivalent when γ → 1, one may assume that we can also lower bound the policy
performance difference of the average reward objective by letting γ → 1 for the bounds in Schulman
et al. (2015). Unfortunately this results in a non-meaningful bound. We will demonstrate this through
a similar policy improvement bound from Achiam et al. (2017) based on the average divergence but a
similar argument can be made for the original bound from Schulman et al. (2015) (see supplementary
material for proof and discussion).
Proposition 1. Consider the following bound from Achiam et al. (2017)
Dπ-,γ(π0) ≤ ργ(π0) - ργ(π) ≤ Dπ+,γ(π0)	(4)
where
DnY(∏0) = ɪ E [π0(≠)An(s,a)l ± 7^2γ⅛ E [DTV(∏0 k ∏)[s]]
π,γ	1 一 Y S〜d∏ π(a∣s) Y	(1 一 Y)2 S〜d∏
a〜π
and EY = maxs ∣Ea〜∏o[A；(s, a)]∣. We have:
li→m1(1 一 Y)Dπ±,Y(π0) = ±∞
(5)
Since limY→1(1 一 Y)(ρY (π0) 一 ρY(π)) = ρ(π0) 一 ρ(π), Proposition 1 says (4) becomes trivial when
used on the average reward. This result is discouraging as it shows that the policy improvement
guarantee from Schulman et al. (2015) does not appear to generalize to the average reward setting.
In the next section, we will derive an alternative policy improvement bound for the average reward
objective which can be used to generate monotonically improved policies w.r.t. the average reward.
4 Main Results
4.1 Average Reward Policy Improvement Theorem
Let d∏ ∈ R|S| be the probability column vector whose components are d∏(s), Pn ∈ RlSl×lSl be the
transition matrix under policy π whose (s, s0) component is Pn(Sls) = Pa P(s0∣s, a)π(a∣s), and
Pn = limt→∞ Pn be the limiting distribution for the transition matrix. We use |卜||? to denote the
3
Under review as a conference paper at ICLR 2021
1	N-1
ρ(π) - ρ(π) = ιim 丙 E, X An (st, at)
N→∞ N T 〜n0	/ J
t=0
operator norm of a matrix. In particular ∣∣∙kι and ∣∣∙k∞ are the maximum absolute column sum and
maximum absolute row sum of a matrix respectively (Horn & Johnson, 2012).
Suppose we have a new policy π0 obtained via some update rule from the current policy π. Similar to
the discounted case, we would like to measure their performance difference ρ(π0) - ρ(π) using an
expression which depends on π and some divergence metric between the two policies. The following
identity shows that ρ(π0) - ρ(π) can be expressed using the advantange function of π.
Lemma 1. For any two stochastic policies π and π0:
E [Aπ(s, a)]	(6)
S 〜d∏0
0
a〜π
Lemma 1 is the an extension of the well-known policy difference lemma from Kakade & Langford
(2002) to the average reward case. A similar result was proved by Neu et al. (2010) and Even-Dar
et al. (2009). For completeness, We will provide a proof based on the Bellman equation as well
as a simpler alternative proof in the supplementary material. Note that this expression depends on
samples drawn from π0 . However we can show through the following lemma that when dπ and dπ0
are "close," we can evaluate the expression in (6) using samples from dπ (see supplementary material
for proof).
Lemma 2. For any two stochastic policies π and π0, the following bound holds:
E	[Aπ(s,	a)]	- 2DTV(dπ0	∣	dπ)	≤	ρ(π0) -	ρ(π)	≤ E	[Aπ(s, a)] + 2DTV(dπ0	∣	dπ)	(7)
S〜d∏	S〜d∏
00
a〜π	a〜π
where E = maxs ∣Ea〜∏o(a∣s)[Aπ(s,a)]∣∙
Lemma 2 shows us how policy improvement is related to the stationary distribution underlying each
policy. In order to study how policy improvement is connected to changes in the actual policies
themselves, we need to analyze the relationship between changes in the policies and changes in
stationary distributions. It turns out that the sensitivity of the stationary distributions in relation to the
policies is related to the structure of the underlying Markov chain.
Let Mπ ∈ RlSl×lSl be the mean first passage time matrix whose elements Mπ (s, s0) is the expected
number of steps it takes to reach state s0 from s under policy π. The matrix Mπ (s, s0) can be
calculated via (Theorem 4.4.7 of Kemeny & Snell (1960))
Mπ(s,s0) = (I-Zπ + EZdπg)Dπ	(8)
where Zπ = (I 一 P∏ + Pn )-1 is known as the fundamental matrix ofthe Markov chain (Kemeny &
Snell, 1960), E is a square matrix consisting of all ones. The subscript ‘dg’ for some square matrix
A refers to a diagonal matrix whose elements are the diagonals of A. Dn ∈ RlSl×lSl is a diagonal
matrix whose elements are 1∕d∏(s). One important property of mean first passage time is that given
some policy π:
κn = X dn(s0)Mn(s, s0)	(9)
S0
is a constant independent of the starting state s. This result is known as the random target lemma
(Aldous & Fill, 1995). The constant κn is sometimes referred to as Kemeny’s constant (Grinstead
& Snell, 2012). This constant can be interpreted as the mean number of steps it takes to get to any
goal state weighted by the steady-distribution of the goal states. This weighted mean does not depend
on the starting state, as mentioned just above. The constant uses a single number to summarize how
“well-connected” a Markov chain is. It can also be shown that κn = trace(Zn) (Grinstead & Snell,
2012). We then have the following result which connects the sensitivity of the stationary distribution
to changes to the policy.
Lemma 3. The divergence between the stationary distributions dn and dn0 can be upper bounded by
the average divergence between policies π and π0 as follows:
DTV(dn0 ∣ dn) ≤ (κn0 一 1) E [DTV(π0 ∣ π)[s]]	(10)
S〜d∏
4
Under review as a conference paper at ICLR 2021
We wish to point out here that Achiam et al. (2017) showed a similar result to Lemma 3 in the
discounted case where the change in dπ,γ can be bounded in terms of the change in policy up to a
multiplicative constant which only depends on the discount factor. In the discounted case, this is
possible since a discounted MDP is like a finite-horizon MDP problem; in fact, it can be shown to be
equivalent to a related finite horizon problem (Proposition 5.3.1, Puterman (1994)). The discount
factor can be used to control the effective horizon where larger discount factors correspond to longer
horizons. In fact, it can be easily shown that the multiplicative factor from Achiam et al. (2017)
goes to infinity as γ → 1, meaning that the bound is not useful for long horizon problems. In the
average reward setting, the sensitivity of the stationary distribution with respect to the policy can vary
depending on the chain structure and long-term behavior of the underlying Markov chain. This means
that it is only natural that the multiplicative constant in Lemma 3 depends on the transition matrix.
This result is also highly intuitive, For very “well-connected” Markov chains where an agent can
easily and quickly get to any state, this constant is relatively small and the stationary distributions
are not sensitive to small changes in policy. On the other hand, for Markov chains that are “weakly
connected,” where on average, it can take a long time to get to some recurrent state in the state space,
the factor can become very large. In this case small changes in the policy can have a large impact on
the resulting stationary distributions.
The following theorem connects the average reward performance of two policies and their average
divergence.
Theorem 1. For any two stochastic policies π and π0, the following bounds hold:
ρ(∏0)-ρ(∏) ≤ E [π0(als)An(s,a)]+2ξ E [DIV(π0k π)[s]]	(11)
S 〜d∏ π(a∣s)	S 〜d∏
a〜π l	」
ρ(∏0)-ρ(∏)	≥ E	[π0%An (s,α)l	-	2ξ	E	[DTV(π0	k	π)[s]]	(12)
S 〜d∏ π(a∣s)	S 〜d∏
a〜π
where ξ = (κπ0 一 1) max§ Ea〜∏o ∣Aπ(s, a)|.
Proof. Combine the bounds from Lemma 2 and Lemma 3. Then rewrite the expectation for Aπ (s, a)
as an expectation w.r.t. ∏ using importance sampling gives Us the desired bound.	□
The right-hand-side of the bounds in Theorem 1 are guaranteed to be finite. Similar to the discounted
case, the multiplicative factor ξ provides a theoretical guidance on the step-sizes for policy updates
(Schulman et al., 2015). The bound in Theorem 1 is given in terms of the TV divergence, however the
KL divergence is more commonly used in practice. Vuong et al. (2019) compared various divergence
measures and showed that the KL has superior empirical performance. The relationship between the
TV divergence and KL divergence is given by PinSker's inequality (Tsybakov, 2008), which says that
for any two distributions P and q: DTV(P k q) ≤，Dkl (PIlq) /2. We can then show that
E DTV(∏0k ∏)[s]] ≤ E PDKL (∏0k∏)[s]∕2] ≤ J E [Dkl (∏0k∏)][s]]∕2	(13)
S 〜d∏	S 〜d∏	S 〜d∏
where the second inequality comes from Jensen’s inequality. The inequality in (13) shows
that the bounds in Theorem 1 still hold when ES〜dπ[DrV(∏0 k ∏)[s]] is substituted with
PES〜d∏ [Dkl (∏0k∏)][s]∕2.
4.2 Approximate Policy Iteration
One direct consequence of Theorem 1 is that iteratively maximizing the right-hand-side of (12)
generates a monotonically improving sequence of policies w.r.t. the average reward objective.
Algorithm 1 gives an approximate policy iteration algorithm that produces such a sequence of policies.
Proposition 2. Given an initial policy π0, Algorithm 1 is guaranteed to generate a sequence of
policies ∏1,∏2,... such that ρ(∏0) ≤ ρ(∏ι) ≤ ρ(∏2) ≤ ,•：
Proof. At iteration k, ES〜d^ ,a〜∏ [Aπk (s, a)] = 0, ES〜d% [Dkl (∏k∏k) [s]] = 0 for π = ∏k. By
Equation (14) and Theorem 1, ρ(∏k+ι) 一 ρ(∏k) ≥ 0.	□
5
Under review as a conference paper at ICLR 2021
Algorithm 1 Approximate Policy Iteration for Average Reward Objective
Initialize: π0
1:	for k = 0, 1, 2, . . . do
2:	Policy evaluation step: evaluate Aπk (s, a) for all s, a.
3:	Policy improvement step:
Πk+1 = argmax ( E [Aπk (s, a)] - ξ 2 E [Dkl (∏k∏k) [s]]
π	∖ s~dπk	sJ s~dπk
∖ a 〜π	V
(14)
where ξ = (κπ - 1)max§ Ea〜∏ ∣Aπk (s, a)|
However, Algorithm 1 is difficult to implement in practice since it requires exact knowledge of the
advantage function and transition matrix. Furthermore, calculating the term ξ is impractical for
high dimensional problems. In the next section, we will introduce a sample-based algorithm which
approximates the update rule given in Equation (14).
5	Practical Applications
As we have noted in the previous section, Algorithm 1 is not practical for problems with large state
and action spaces and thus cannot be naively applied directly. In this section, we will discuss how
Algorithm 1 and Theorem 1 can be used in practice to create algorithms which can effectively solve
high dimensional DRL problems. In the Appendix C, we will also discuss how Theorem 1 can be
used to solve DRL problems with safety constraints.
5.1	Average Reward Trust Region Policy Optimization
For DRL problems, it is common to consider some parameterized policy class ΠΘ ⊆ Π. Our goal is
to devise a computationally tractable version of Algorithm 1 for policies in ΠΘ, i.e., given a policy
πθk at iteration k, how do we obtain the best possible πθk+1 ? We can rewrite the unconstrained
optimization problem in (14) as a constrained problem:
maximize E [Aπθk (s,a)] s.t. Dkl(∏θ ∣∣ ∏θk) ≤ δ	(15)
∏θ ∈∏θ	s~d∏θ^
a〜∏θ
where Dkl(∏θ ∣∣ ∏θ%):= Es〜d∏° [Dkl (∏θk∏θk)[s]]. The constraint set {∏θ ∈ ∏θ : Dkl(∏θ k
θk
πθk ) ≤ δ} is called the trust region set. This problem can be regarded as an average reward variant of
TRPO from Schulman et al. (2015). Note that the advantage function in (15) is the average reward
advantage function introduced in Section 2. When we set πθk+1 to be the optimal solution to (15),
πθk+1 can be shown to have the following performance guarantee:
Proposition 3. Let πθk+1 be the optimal solution to (15) for some πθk ∈ ΠΘ. The policy performance
difference between πθk+1 and πθk can be lower bounded by
P(πθk+1) - ρ(πθk) ≥ -ξπθk+1 √2δ	(16)
where ξπθk+1 = (κπθk+1 — 1)maxs Ea〜n@仁十ɪ ∣Aπθk(s,a)∣.
Proof. Since Dkl(∏Θx k ∏θk) = 0, ∏θk is a feasible solution. The objective value is 0 for ∏θ = ∏θk.
The bound follows from (12)and (13) where the average KL is bounded by δ.	□
Several algorithms have been proposed for efficiently solving the discounted version of (15): Schul-
man et al. (2015) and Wu et al. (2017) converts (15) into a convex problem via Taylor approximations;
another approach is to first solve (15) in the nonparametric policy space and then project the result
back into the parameter space (Abdolmaleki et al., 2018; Vuong et al., 2019). These algorithms can
be adapted for the average reward case and are theoretically justified via Theorem 1 and Proposition
3. One notable difference compared to the discounted case is the estimation of the critic, as discussed
in the next section and in the Appendix D.
6
Under review as a conference paper at ICLR 2021
5.2	Implementation
In this section, we discuss how the average reward version of the TRPO algorithm (Schulman et al.,
2015) — which we will refer to as ATRPO — can be implemented in practice. Algorithm 2 provides
a basic outline of the ATRPO algorithm.
Algorithm 2 Average Reward TRPO(ATRPO)
Initialize: Policy parameters θo, value net parameters φo, learning rate α.
1:	for k = 0,1, 2,…do
2:	Collect a sample trajectory {st, at, st+1, rt}, t = 1, . . . , N from the environment using πθk .
3:	Calculate sample average reward of πθk via ρ = Nn P屋 rt.
4:	for t = 1, 2, . . . , N do
5:	Get target Vttarget = rt - ρ + Vφk (st+1)
6:	Get advantage estimate A(st,at) = r - P + Vφk (st+ι) - V^ (St)
7:	Update critic by
φk + 1 - φk - αVφL(φk)
where
1N
L(φk) = 2 XMk (st) - Vtan2
t=1
8:	Use A(st, at) to update θk using TRPO policy update (Schulman et al., 2015).
The major difference between the TRPO algorithm and the ATRPO algorithm is how the target for
the critic and the advantage function are calculated. Importantly, simply letting γ → 1 in TRPO does
not lead to Algorithm 2. This subtle but important difference leads to a significant improvement in
sample efficiency, as shown in the section on experimental results.
In Algorithm 2, for illustrative purposes, we use the average reward one-step bootstrapped estimate
for the target of the critic and the advantage function. In practice, we instead use an average reward
version of the Generalized Advantage Estimator (GAE) from Schulman et al. (2016). In short, GAE
uses a tunable eligibility trace parameter λ to act as a trade-off between the Monte Carlo estimate and
the bootstrapped estimate. In the Appendix D we provide more detail on how GAE can be generalized
to the average reward case.
6	Related Work
Dynamic programming algorithms for finding the optimal average reward policies have been well-
studied (Howard, 1960; Blackwell, 1962; Veinott, 1966). In contrary to our method which is based on
the policy gradient approach, several Q-learning-like algorithms for problems with unknown dynamics
have been proposed, such as R-Learning (Schwartz, 1993), RVI Q-Learning (Abounadi et al., 2001),
and CSV-Learning (Yang et al., 2016). Mahadevan (1996) conducted a thorough empirical analysis
of the R-Learning algorithm. We note that much of the previous work on average reward RL focuses
on the tabular setting without function approximations, and the theoretical properties of many of
these Q-learning-based algorithm are not well understood (in particular R-learning). More recently,
POLITEX updates policies using a Boltzmann distribution over the sum of action-value function
estimates of the previous policies (Abbasi-Yadkori et al., 2019) and Wei et al. (2020) introduced
a model-free algorithm for optimizing the average reward of weakly-communicating MDPs. Both
methods are shown to have theoretical guarantees under the tabular setting.
For policy gradient methods, Baxter & Bartlett (2001) showed that if 1/(1 - γ) is large compared to
the mixing time of the Markov Chain induced by the MDP, then the gradient of ργ (π) can accurately
approximate the gradient of ρ(π). Kakade (2001) extended upon this result and provided an error
bound on using an optimal discounted policy to maximize the average reward. In contrast, our work
directly deals with using policy gradient methods for the average reward objective and provides
theoretical guidance on the optimal step size for each policy update.
Policy improvement bounds have been extensively explored in the discounted case. The results from
Schulman et al. (2015) is an extension of Kakade & Langford (2002) which restricted the policy class
7
Under review as a conference paper at ICLR 2021
to a mixture of policies. Pirotta et al. (2013) also proposed an alternative generalization to Kakade
& Langford (2002). Achiam et al. (2017) improved upon Schulman et al. (2015) by replacing the
maximum divergence with the average divergence.
7	Experiments
Recently, DRL algorithms such as TRPO have proven to be successful for episodic high-dimensional
tasks. In our experiments, we wish to study whether for continuing-control tasks, the policy trained
with ATRPO can out-perform the policies trained with TRPO with different discount factors.
Our design goal for the experiments is to simulate continuing-control tasks where the agent can interact
with the environment indefinitely. We consider three tasks (Ant, HalfCheetah, and Humanoid) from
the MuJoCo physical simulator (Todorov et al., 2012) implemented in the OpenAI gym (Brockman
et al., 2016). The natural goal is to train the agents to run as fast as possible without falling. However
the standard MuJoCo tasks are episodic tasks which terminate when the agent falls. We convert
these tasks into continuing control tasks via the following: when the agent falls, the agent incurs
a large cost for falling, but then continues the trajectory from a random start state. We use these
continuing-control tasks for both training and evaluation for both ATRPO and TRPO. More details
on the environment can be found in Appendix F.
One point we wish to emphasize regarding the experiments is that even though the MuJoCo benchmark
is commonly trained using the discounted objective (see e.g. Schulman et al. (2015), Wu et al. (2017),
Schulman et al. (2017), Abdolmaleki et al. (2018), Vuong et al. (2019)), it is always evaluated using
the undiscounted objective. This is because the undiscounted objective more naturally describes the
goals of the MuJoCo agents (e.g., an agent’s performance w.r.t. the reward signal should be equally
important at time step 1000 as it is at time step 1). In the case of TRPO (and similarly many other
DRL algorithms), discounting is used during training often for mathematical and computational
convenience. Prior to our work, there has been no theoretical or empirical evidence to support
applying trust region methods to the average reward. In this section, we demonstrate that when
the actual objective we want to evaluate is undiscounted, discounting, as is commonly done, is
unnecessary and may lead to suboptimal performance.
HalfCheetah-v3
Humanoid-v3
4	6	8
ion Samples
-0- ATRPO -Δ- TRPO (y= 0.90)	-θ- TRPO (y= 0.95)	TRPO (y= 0.99)	→- TRPO (y= 0.999)	-M- TRPO (y= 0.9999)
Figure 1: Learning curves comparing ATRPO and TRPO with different discount factors. The solid
lines represent the average reward of trajectories of fixed length of 10,000 time steps averaged over the
last 50 trajectories. The results are averaged over 10 random seeds and the shaded region represents
one standard deviation.
During training, we collect one trajectory of a fixed length of 10,000 using the current policy.1
We then use this data to update the critic and policy networks (see Algorithm 2). This gives us
a new policy and critic which we then use to repeat the above process. In Figure 1, we plot the
1In the original OpenAI gym version of MuJoCo, episode lengths are capped at 1000 (see https:
//github.com/openai/gym/blob/master/gym/envs/__init__.py). We removed this cap to
allow for arbitrarily long time horizons.
8
Under review as a conference paper at ICLR 2021
training curves of ATRPO and of TRPO for different discount factors. Detailed specifications and
hyperparameter settings can be found in Appendix F.
Figure 1 shows that ATRPO improves performance by 5.0%, 32.8%, 26.7% on HalfCheetah, Ant
and Humanoid respectively over TRPO with its best discount factors. One point worth noting is
that increasing the discount factor does not necessarily lead to better performance of TRPO. A
larger discount factor in principle enables the algorithm to seek a policy that performs well for the
average-reward criterion. But, unfortunately, a larger discount factor can also increase the variance of
the gradient estimator (Zhao et al., 2011; Schulman et al., 2016) and degrade generalization (Amit
et al., 2020). Moreover, algorithms with discounting become unstable as γ → 1 (Naik et al., 2019).
The discount factor therefore serves as a hyperparameter which can be tuned to improve performance.
This is supported by the observation that the optimal discount factor is different for each environment
(0.999, 0.99, 0.95 for HalfCheetah, Ant, and Humanoid respectively), where choosing a suboptimal
discount factor can have significant consequences. (For Ant and Humanoid, the optimal discount
factor is 33.9% and 65.6% better than the second best discount factor.) We have shown here that
using the average reward criterion not only delivers superior performance but also obviates the need
to tune the discount factor.
To further support our conclusion, we will also compare ATRPO and TRPO using an alternative
evaluation protocol. In this protocol, after every one million samples of training we run 10 separate
evaluation trajectories of fixed length 10,000 time steps using the current policy with no exploration.
The random seeds used for evaluation are different from those used in training. Figure 2 shows the
average reward of these trajectories, Once again ATRPO provides superior performance.
TRPO (y= 0.95)	-TRFO (γ= 0.99)
Humanoid-v3
HalfCheetah-v3
-" ATRPO	-*- TRPO (y= 0.90)
Ant-v3
TRPO
(yr= D.999)	-X- TRFO (γ= 0.9999)
Million Samples
Figure 2: Comparing performance on evaluation trajectories of length 10,000. For each random seed
used in training, we use a different unseen random seed to run 10 test trajectories after every 1 million
samples of training. The solid line is averaged over these unseen random seeds. The shaded area is
one standard deviation.
8	Conclusion
In this paper, we introduced a novel policy improvement bound for the average reward criterion. The
bound is based on the average divergence between two policies and Kemeny’s constant. We showed
that previous existing policy improvement bounds for the discounted case results in a non-meaningful
bound for the average reward objective. Our work provided the theoretical justification and the
means to generalize the popular trust-region based algorithms to the average reward setting. We
demonstrated through a series of experiments that our method is highly effective on high-dimensional
continuing control tasks. In particular, we showed that when the natural objective of the task is
undiscounted, discounting can lead to suboptimal behavior. To the best of our knowledge, we are one
of the first to address how DRL methods can be used to learn undiscounted continuing control tasks
with large state and action spaces.
9
Under review as a conference paper at ICLR 2021
References
Yasin Abbasi-Yadkori, Peter Bartlett, KUsh Bhatia, Nevena Lazic, Csaba Szepesvari, and Gellert
Weisz. Politex: Regret bounds for policy iteration using expert prediction. In International
Conference on Machine Learning, pp. 3692-3702, 2019.
Abbas Abdolmaleki, Jost Tobias Springenberg, YUval Tassa, Remi MUnos, Nicolas Heess, and Martin
Riedmiller. MaximUm a posteriori policy optimisation. International Conference on Learning
Representation (ICLR), 2018.
Jinane AboUnadi, D Bertsekas, and Vivek S Borkar. Learning algorithms for markov decision
processes with average cost. SIAM Journal on Control and Optimization, 40(3):681-698, 2001.
JoshUa Achiam. UC Berkeley CS 285 (Fall 2017), Advanced Policy Gradients,
2017. URL: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/
lecture_13_advanced_pg.pdf. Last visited on 2020/05/24.
JoshUa Achiam, David Held, Aviv Tamar, and Pieter Abbeel. Constrained policy optimization. In
Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 22-31.
JMLR. org, 2017.
David AldoUs and James Fill. Reversible markov chains and random walks on graphs, 1995.
Eitan Altman. Constrained Markov decision processes, volUme 7. CRC Press, 1999.
Ron Amit, Ron Meir, and Kamil Ciosek. DiscoUnt factor as a regUlarizer in reinforcement learning.
In International conference on machine learning, 2020.
Jonathan Baxter and Peter L Bartlett. Infinite-horizon policy-gradient estimation. Journal of Artificial
Intelligence Research, 15:319-350, 2001.
Dimitri P Bertsekas, Dimitri P Bertsekas, Dimitri P Bertsekas, and Dimitri P Bertsekas. Dynamic
programming and optimal control, volUme 1,2. Athena scientific Belmont, MA, 1995.
David Blackwell. Discrete dynamic programming. The Annals of Mathematical Statistics, pp.
719-726, 1962.
Greg Brockman, Vicki CheUng, LUdwig Pettersson, Jonas Schneider, John SchUlman, Jie Tang, and
Wojciech Zaremba. Openai gym, 2016.
Grace E Cho and Carl D Meyer. Comparison of pertUrbation boUnds for the stationary distribUtion of
a markov chain. Linear Algebra and its Applications, 335(1-3):137-150, 2001.
Eyal Even-Dar, Sham M Kakade, and Yishay MansoUr. Online markov decision processes. Mathe-
matics of Operations Research, 34(3):726-736, 2009.
Charles Miller Grinstead and James LaUrie Snell. Introduction to probability. American Mathematical
Soc., 2012.
TUomas Haarnoja, AUrick ZhoU, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy
maximUm entropy deep reinforcement learning with a stochastic actor. International Conference
on Machine Learning (ICML), 2018.
Roger A Horn and Charles R Johnson. Matrix analysis. Cambridge University press, 2012.
Ronald A Howard. Dynamic programming and markov processes. John Wiley, 1960.
Jeffrey J HUnter. Stationary distribUtions and mean first passage times of pertUrbed markov chains.
Linear Algebra and its Applications, 410:217-243, 2005.
Sham Kakade. Optimizing average reward Using discoUnted rewards. In International Conference on
Computational Learning Theory, pp. 605-615. Springer, 2001.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In
International Conference on Machine Learning, volUme 2, pp. 267-274, 2002.
10
Under review as a conference paper at ICLR 2021
L.C.M. Kallenberg. Linear Programming and Finite Markovian Control Problems. Centrum Voor
Wiskunde en Informatica, 1983.
J.G. Kemeny and I.J. Snell. Finite Markov Chains. Van Nostrand, New Jersey, 1960.
Erich L Lehmann and George Casella. Theory of point estimation. Springer Science & Business
Media, 2006.
Sridhar Mahadevan. Average reward reinforcement learning: Foundations, algorithms, and empirical
results. Machine learning, 22(1-3):159-195, 1996.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. NIPS Deep
Learning Workshop, 2013.
Abhishek Naik, Roshan Shariff, Niko Yasui, and Richard S Sutton. Discounted reinforcement
learning is not an optimization problem. NeurIPS Optimization Foundations for Reinforcement
Learning Workshop, 2019.
Gergely Neu, Andras Antos, AndrgS Gyorgy, and CSaba Szepesvdri. Online markov decision
processes under bandit feedback. In Advances in Neural Information Processing Systems, pp.
1804-1812, 2010.
Jan Peters and Stefan Schaal. Reinforcement learning of motor skills with policy gradients. Neural
networks, 21(4):682-697, 2008.
Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy iteration.
In International Conference on Machine Learning, pp. 307-315, 2013.
Martin L Puterman. Markov Decision Processes: Discrete Stochastic Dynamic Programming. John
Wiley & Sons, 1994.
Keith W Ross. Constrained markov decision processes with queueing applications. Dissertation
Abstracts International Part B: Science and Engineering[DISS. ABST. INT. PT. B- SCI. & ENG.],,
46(4), 1985.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policy optimization. In International Conference on Machine Learning, pp. 1889-1897, 2015.
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional
continuous control using generalized advantage estimation. International Conference on Learning
Representations (ICLR), 2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.
Anton Schwartz. A reinforcement learning method for maximizing undiscounted rewards. In
Proceedings of the tenth international conference on machine learning, volume 298, pp. 298-305,
1993.
David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,
Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering
the game of go with deep neural networks and tree search. nature, 529(7587):484, 2016.
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez,
Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore Graepel, et al. A general reinforcement
learning algorithm that masters chess, shogi, and go through self-play. Science, 362(6419):
1140-1144, 2018.
Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.
Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient meth-
ods for reinforcement learning with function approximation. In Advances in neural information
processing systems, pp. 1057-1063, 2000.
11
Under review as a conference paper at ICLR 2021
Chen Tessler, Daniel J Mankowitz, and Shie Mannor. Reward constrained policy optimization.
International Conference on Learning Representation (ICLR), 2019.
Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.
In 2012IEEE/RSJ International Conference on Intelligent Robots and Systems,pp. 5026-5033.
IEEE, 2012.
Alexandre B Tsybakov. Introduction to nonparametric estimation. Springer Science & Business
Media, 2008.
Arthur F Veinott. On finding optimal policies in discrete dynamic programming with no discounting.
The Annals of Mathematical Statistics, 37(5):1284-1294, 1966.
Quan Vuong, Yiming Zhang, and Keith W Ross. Supervised policy update for deep reinforcement
learning. In International Conference on Learning Representation (ICLR), 2019.
Chen-Yu Wei, Mehdi Jafarnia-Jahromi, Haipeng Luo, Hiteshi Sharma, and Rahul Jain. Model-
free reinforcement learning in infinite-horizon average-reward markov decision processes. In
International conference on machine learning, 2020.
Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy Ba. Scalable trust-region
method for deep reinforcement learning using kronecker-factored approximation. In Advances in
neural information processing systems (NIPS), pp. 5285-5294, 2017.
Shangdong Yang, Yang Gao, Bo An, Hao Wang, and Xingguo Chen. Efficient average reward
reinforcement learning using constant shifting values. In AAAI, pp. 2258-2264, 2016.
Yiming Zhang, Quan Vuong, and Keith Ross. First order constrained optimization in policy space.
Advances in Neural Information Processing Systems, 33, 2020.
Tingting Zhao, Hirotaka Hachiya, Gang Niu, and Masashi Sugiyama. Analysis and improvement of
policy gradient estimation. In Advances in Neural Information Processing Systems, pp. 262-270,
2011.
12