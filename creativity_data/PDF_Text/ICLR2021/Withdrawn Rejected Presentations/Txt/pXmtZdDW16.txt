Under review as a conference paper at ICLR 2021
Embedding a random graph via GNN: mean-
field inference theory and RL applications to
NP-Hard multi-robot/machine scheduling
Anonymous authors
Paper under double-blind review
Ab stract
We develop a theory for embedding a random graph using graph neural networks
(GNN) and illustrate its capability to solve NP-hard scheduling problems. We
apply the theory to address the challenge of developing a near-optimal learning
algorithm to solve the NP-hard problem of scheduling multiple robots/machines
with time-varying rewards. In particular, we consider a class of reward collection
problems called Multi-Robot Reward Collection (MRRC). Such MRRC problems
well model ride-sharing, pickup-and-delivery, and a variety of related problems.
We consider the classic identical parallel machine scheduling problem (IPMS) in
the Appendix.
For the theory, we first observe that MRRC system state can be represented as an
extension of probabilistic graphical models (PGMs), which we refer to as random
PGMs. We then develop a mean-field inference method for random PGMs. We
prove that a simple modification of a typical GNN embedding is sufficient to embed
a random graph even when the edge presence probabilities are interdependent.
Our theory enables a two-step hierarchical inference for precise and transferable
Q-function estimation for MRRC and IPMS. For scalable computation, we show
that transferability of Q-function estimation enables us to design a polynomial time
algorithm with 1 - 1/e optimality bound. Experimental results on solving NP-hard
MRRC problems (and IMPS in the Appendix) highlight the near-optimality and
transferability of the proposed methods.
1	Introduction
Consider a set of identical robots seeking to serve a set of spatially distributed tasks. Each task is
given an initial age (which then increases linearly in time). Greater rewards are given to younger
tasks when service is complete according to a predetermined reward rule. We focus on NP-hard
scheduling problems possessing constraints such as ‘no possibility of two robots assigned to a task at
once’. Such problems prevail in operations research, e.g., dispatching vehicles to deliver customers
in a city or scheduling machines in a factory. Impossibility results in asynchronous communication1
[Fischer et al. (1985)] make these problems inherently centralized.
Learning-based scheduling methods for single-robot NP-hard problems. structure2vec (Dai
et al. (2016)) is a popular Graphical Neural Network (GNN) derived from the fixed point iteration
of PGM based mean-field inference. Recently, Dai et al. (2017) showed that structure2vec can
construct a solution for Traveling Salesman Problem (TSP). A partial solution to TSP was considered
as an intermediate state, and the state was represented using a heuristically constructed probabilistic
graphical model (PGM). This GNN was used to infer the Q-function, which they exploit to select the
next assignment. While their choice of PGM was entirely heuristic, their approach achieved near-
optimality and transferability of their trained single-robot scheduling algorithm to new single-robot
scheduling problems with an unseen number of tasks. Those successes were restricted to single-robot
problems except for special cases when the problem can be modeled as a variant of single-robot TSP
via multiple successive journeys of a single robot, c.f., (Nazari et al. (2018); Kool et al. (2018)).
1Due to this limitation, multi-agent (decentralized) methods are rarely used in industries (e.g., factories).
1
Under review as a conference paper at ICLR 2021
Proposed methods and contributions. The present paper explores the possibility of near-optimally
solving multi-robot, multi-task NP-hard scheduling problems with time-dependent rewards using
a learning-based algorithm. This is achieved by first extending the probabilistic graphical model
(PGM)-based mean-field inference theory in Dai et al. (2016) for random PGM. We next consider a
seemingly naive two-step heuristic: (i) approximate each edge’s presence probability and (ii) apply a
typical GNN encoder with probabilistic adjustments. We subsequently provide theoretical results that
justify this approach. We call structure2vec (Dai et al. (2016)) combined with this heuristic as random
structure2vec. After observing that each state of a robot scheduling problem can be represented as
a random PGM, we use random structure2vec to design a transferable (to different size problems)
reinforcement learning method that is, to the best of our knowledge, the first to learn near-optimal
NP-hard multi-robot/machine scheduling with time-dependent rewards. Experiments yield 97%
optimality for MRRC problems in a deterministic environment with linearly-varying rewards. This
performance is well extended to experiments with stochastic traveling time.
Figure 1: Representing a ridesharing/pickup and delivery problem as an MRRC problem
2	Multi-robot scheduling problem formulation
We formulate a multi-robot reward collection problem (MRRC) as a discrete-time, discrete-state
(DTDS) sequential decision-making problem. (For a closely related continuous-time and continuous-
state (CTCS) formulation of IPMS problems, see Appendix A.1). For the DTDS formulation, time
advances in fixed increments ∆ and each such time-step is considered a decision epoch. (For the
CTCS formulation, the times when a robot arrives at a task or completes a task are considered as
decision epochs.) At such decision epochs (which occur at every time-step in our discrete-time
model), we reassign all available robots to remaining tasks. We use k to index the decision epochs
and let tk denote the time that epoch k occurs (in discrete-time tk = k ∙ ∆). We assume that at
each decision epoch, we are newly given the duration of time required for a robot to complete each
task, which we call task completion time. Such task completion times may be constants or random
variables, and in either case, they are determined by current state (e.g., locations of the robot and the
task) at each epoch.2 We consider initial tasks as nodes in a fully connected graph. For the edge from
task p to task q, we denote as pT,Tq . The edge weight assigned is the task completion time for a robot
that has just completed task p to subsequently complete task q. Let ETT, WTT be the set of all pT,Tq
and the set of corresponding weights. All elements of WTT are multiples of ∆).
State. The state stk at time tk is represented as Gtk , WtRT , αtk . Gt is a directed bipartite graph
(R ∪ Ttk , EtRkT ) where R is the set of all robots, Ttk is the set of all remaining unserved tasks at
time step tk. The set EtRT consists of all directed edges from robots to unserved tasks at time tk. To
each edge is associated a weight equal to the task completion time. Let WtRkT denote the set of all
such weights for all edges at tk (either constants or random variables, of which values are restricted
to multiples of ∆ in the the DTDS system). For example, iR,pT ∈ EtRT is an edge indicating robot
i is assigned to serve task p. To this edge a task completion time is assigned according to current
locations of the robot i and task p. Each task is given an initial age which increases linearly with time
(a multiple of ∆ for DTDS). Let αtk = {ηtpk ∈ R|p ∈ Ttk } denote the set of ages where ηtpk indicates
the age of task p at time-step tk. We denote the set of possible states as S.
It is intuitively clear how MRRC can directly model problems with stationary tasks. MRRC can also
model problems such as ride-sharing or package delivery problems in which the robot location at
2In the later case, our method only requires samples of the random variables; distributions are not required.
2
Under review as a conference paper at ICLR 2021
the start of the task is different than at the end. Consider pickup and delivery tasks as illustrated in
Figure 1. Task 1, denoted as τ1, is to pickup from A and deliver to location B. The weight assigned
to the edge 2T,T1 is the task completion time for a robot who has just completed task 2, and is thus
located at C, who subsequently completes task 1. The traveling distance to task 1 (C → A) is 4 and
the delivery distance (A → B) is 3, so the task completion time is 2T,T1 = 3 + 4 = 7. In the middle
image in Figure 1, state stk (robots nodes, task nodes, arcs from robots to tasks and their weights,
and ages), the system ETT (arcs between task nodes) and their weights WTT are depicted.
Joint assignment. Once a robot has reached a task, it will conduct it until completion. Otherwise,
we allow reassignment prior to arrival. Thus, available robots can change their assignments whenever
a decision epoch occurs. A joint assignment of robots to tasks at current state stk = Gtk , WtRT , αtk ,
denoted as atk , should satisfy: (i) no two robots can be assigned to the same task, and (ii) a robot may
only remain without assignment when the number of robots exceeds the number of remaining tasks.
Thus, a joint assignment atk is the set of edges in a maximal bipartite matching of the bipartite graph
Gtk. The action space Atk is depends upon stk, as it is defined as the set of all maximal bipartite
matchings in Gtk. A policy π is defined as π(stk) = atk, where stk ∈ S and atk ∈ Atk.
Transition function and reward. In the hierarchical control literature, our assignment is termed a
macro-action. In pursuit of the macro-action, robots may make multiple sequential micro-actions
to serve the task. The transition probability associated with a macro-action is derived from the
transition probabilities associated with micro-actions [Omidshafiei et al. (2017)]. For a joint macro-
action, assume there is an induced joint micro-action denoted as ut ∈ U with associated transition
probabilities P (st+1 |st, ut) : St ×Ut × St → [0, 1]. Omidshafiei et al. (2017) proves we can calculate
the corresponding ‘extended transition function’ P0	stk+1	|stk , atk	:	Stk	×	Atk	× Stk	→	[0, 1].
When a task is served, a reward is given according to a predetermined reward function that computes
rewards according to the task’s age at the time of service. Note that the state and assignment
information stk, atk and stk+1 are thus sufficient to determine the reward at decision epoch tk+1. As
such we denote the reward function as R(stk , atk , stk+1) : Stk × Atk × Stk 7→ R.
Objective.	Given an initial	state	st0	∈ S, the MRRC seeks to maximize the	sum
of expected	rewards through	time	by	optimizing an assignment policy ∏* as ∏*	=
argmaxEπ,P0 Pk∞=0 R stk, π(stk), stk+1 |st0 .
3	S cheduling by Inferencing with a Random PGM
3.1	Background on probabilistic graphical model (PGM) and structure2vec
PGM. Given random variables X = {Xk}, suppose that we can factor the joint distribution p (X)
as P (X)=1 Qi φi (Di) where φi(Di) denotes a marginal distribution or conditional distribution
associated with a set of random variables Di ; Z is a normalizing constant. Then {Xk } is called a
probabilistic graphical model (PGM). In a PGM, Di is called a clique and φi (Di) is called a clique
potential for Di . When we write simply φi , suppressing Di , Di is called the scope of φi .
Mean-field inference with PGM. A popular use of PGM is PGM-based mean-field inference.
Suppose that X = {{Yk}, {Hj}}, where we are interested in the inference of {Hj} given {Yk}. For
the inference problem, our interest will be calculating p ({Hj = hj} | {Yk = yk}) but the calculation
might not be tractable. In mean-field inference, we instead find a surrogate distribution q({Hj =
hj}) = j qj(hj) with smallest Kullback-Leibler distance to p ({Hj = hj} | {Yk = yk}). This
surrogate distribution is then used to conduct the inference. Hereafter, for convenience, we suppress
explicit mention of the random variable, for example, we write p(hj) for p(Hj = hj). Koller &
Friedman (2009) shows that when we are given a PGM, the q({hj}) can be obtained by a fixed point
equation. Despite the usefulness of this approach, we are not often directly given the PGM.
Structure2vec. In some problems such as molecule classification problems, data is given as graphs.
For such special cases, [Dai et al. (2016)] suggests that this graph structure information may be
enough to conduct a mean-field inference when combined with Graph Neural Network (GNN). Let
US first embed P (hj | {yk}) to a vector μj using the equation μj = JH φ (hj) P (hj | {ykD dhj.
Suppose that our problem has a special PGM structure that joint distribution is proportional to some
factorization Qk∈V φ (hk, yk) Qi,j∈V φ (hi, hj), where V denotes the set of vertex indexes. Then
3
Under review as a conference paper at ICLR 2021
according to [Dai et al. (2016)], the embedding of the fixed point iteration operation of PGM-based
mean-field inference corresponds to a neural network operation μi = σ(W1yi + W2 Pj=i μj') (σ
denotes Relu function and W denotes parameters of neural networks). We can therefore use {μk } to
solve the original inference problem instead ofp ({hk} | {yj}) or q({hk}). Note that their suggested
neural network operation is similar to the network structure of Graph Convolutional Networks [Kipf
& Welling (2017)], a popular GNN-based graph embedding method. This observation enables one to
interpret GNN-based graph embedding methods as mean-field inference using PGM.
臭 a IFwandWTT ∖ p({H»|{/}) inference
l⅛α⅛λf and W χ with Random PGM
Estimating ∖
X Q(St，at) Z
Figure 2: State representation and main inference procedure
Random graph
Embedding
3.2	Inference with random PGM and random graph embedding with GNN
Random PGM. Denote the set of all random variables in the inference problem as X = {Xi}.
Suppose that the set of all possible PGMs on X, denoted as GX, is prior knowledge (e.g., for a robot
scheduling problem, PGM is often a specific Bayesian Network - see Appendix A.2). A random PGM
on X is then defined as {GX, P} where P : GX 7→ [0, 1] is the probability measure for a realization
of an element of GX . Note that the inference of P will be difficult. To avoid this task, we start by
defining semi-cliques. Suppose that we are given the set of all possible cliques on X as CX . Only a
few cliques in CX will be actually realized as an element of PGM according to P and become real
cliques. So we call the elements Dm ∈ CX as semi-cliques. Note that if we are given P then we can
easily calculate the presence probability pm of semi-clique Dm as pm = PG∈G P(G)1Dm∈G.
Mean-field inference with random PGM. The following theorem extends mean-field inference
with PGM (Koller & Friedman (2009)) to mean-field inference with random PGM. It shows that we
only need to infer the presence probability of each semi-clique in the random PGM, not P .
Theorem 1. Random PGM based mean field inference. Suppose we are given a random PGM on
X = {Xk }. Also, assume that we know presence probability {pm } for all semi-cliques Dm ∈ CX.
The surrogate distribution {qk (xk)} in mean-field inference is locally optimal only if qk (xk) =
z1k exp {Pm:Xk∈Dm PmE(Dm-{Xk})〜q [ln Φm (Dm, Xk)]} where Zk is a normalizing constant and
φm is the clique potential for clique m. (For the proof see Appendix A.3.)
Random structure2vec. From Theorem 1, we can develop a random structure2vec corresponding
to a random PGM with ({Hk}, {Yk}). That is, we can combine (i) the fixed point equation of
the mean field approximation for qk (hk) (Theorem 1) and (ii) the injective embedding for μi =
JH φ(Hi)p(hi |yi)dhi to come up with parameterized fixed point equation for μk (see Figure 2). As
in Dai et al. (2016), we restrict our discussion to the case where there are semi-cliques between two
random variables. In this case, the notation we use for Dm and pm is Dij and pij .
Lemma 1. Structure2vec for random PGM. Given a random PGM on X = ({Hk}, {Y⅛}). As [Dai
et al. (2016)], suppose that our problem has a PGM structure with joint distribution proportional to
some factorization k φ (hk, yk)	i,j φ (hi, hj). Assume that the presence probabilities {pij} for all
pairwise semi-cliques Dij ∈ CX are given. Then fixed point equation in Theorem 1 forp({Hk}|{yk})
is embedded to generate the fixed point equation μk = σ
The proof of Lemma 1 can be found in Appendix A.4.
(WIyk + W2 Pj=k pkjμj).
Remarks. Note that inference of P is in general a difficult task. One implication of Theorem 1
is that we transformed a difficult inference task to a simple inference task: inferring the presence
4
Under review as a conference paper at ICLR 2021
probability of each semi-clique. (See Appendix A.5 for the algorithm that conducts this task). In
addition, Lemma 1 provides a theoretical justification to ignore the interdependencies among edge
presences when embedding a random graph using GNN. When graph edges are not explicitly given
or known to be random, the simplest heuristic one can imagine is to separately infer the presence
probabilities of all edges and adjust the weights of GNN’s message propagation. According to Lemma
1, possible interdependencies among edges would not affect the quality of such heuristic’s inference.
Figure 3: Illustration of overall pipeline of our method
4 SCHEDULING MRRC WITH Random structure2vec
As illustrated in Appendix A.2, MRRC problems with no randomness induces Bayesian networks
with factorization k φ (hk, yk) i,j φ (hi, hj). Therefore, according to Lemma 1, we are justified
to use random structure2vec to design a method to learn solutions to our MRRC problems.
4.1	DESIGNING Q-FUNCTION ESTIMATOR HAVING ORDER-TRANSFERABILITY
Intuitively, local graph information around node k is embedded into the structure2vec output vector
μk [Dai et al. (2016)]. Using this intuition, We propose a two-step sequential and hierarchical State-
embedding neural network using random structure2vec that is designed to achieve what we will later
call order-transferable Q-function estimation. This allows problem-size transferable Q-learning, i.e.,
the neural network parameter θ, trained to calculate Qθm that approximates the Q-function Qm for an
m-robot scheduling problem, can be well used to solve n-robot scheduling problems (n 6= m). For
brevity, we assume task completion times are deterministic. For the detailed algorithm with random
task completion times, see Appendix A.6. The following procedure is illustrated in Figure 3.
Step 1. Distance Embedding. The first structure2vec layer embeds information of robot locations
around each task k, i.e. local graph structure around each task k with respect to robots, to each μk
(superscript 1 denotes the outcome of first layer). For the input of the first structure2vec layer ({xk }
in Lemma 1), we only use robot assignment information (if k is an assigned task, we set the value of
xk to task completion time of assignment (a duration); if k is not an assigned task:, we set xk = 0).
Step 2. Value Embedding. The second structure2vec layer embeds how much value is likely in the
local graph around task k to μk. Recall that the output vectors of the first StruCture2vec layer, {μk },
carry information about the graph structure of robots locally around each task. For each task k, we
concatenate task k,s age ηk with μk to get "? and use {μ? } as the input ({xk } in Lemma 1) to the
second StruCture2vec layer. Denote the outcome of second structure2vec layer as {μk}.
Step 3. Computing Qθ (stk , atk). To derive Qθ (stk , atk), we aggregate the embedding vectors for
all nodes by μ2 = Pk μk to obtain one global vector μ2 to embed the value affinity of the global
graph. We then use a neural network to map μ2 into Qθ (Stk ,atk).
Let us provide the intuition related to problem-size transferability of Q-learning. Step 1 above,
transferability is trivial; the inference problem is a scale-free task loCally around eaCh node. For
Step 2, consider the ratio of robots to tasks. The overall value affinity embedding will be underesti-
mated if this ratio in the training environment is smaller than this ratio in the testing environment;
5
Under review as a conference paper at ICLR 2021
overestimated overall otherwise. The intuition is that this over/under-estimation does not matter in
Q-learning [van Hasselt et al. (2015)] as long as the order of Q-function value among actions are
the same. That is, as long as the best assignments chosen are the same, i.e., argmaxat Qn(stk , atk)
= argmaxat Qθn (stk, atk), the magnitude of imprecision |Qn(stk, atk) - Qθn (stk, atk)| does not
matter. We call this property order-transferability of Q-function estimator with θ.
4.2	Order transferability-enabled auction for scalable computation
Learning-based heuristics for solving NP-hard problems have recently received attention due to their
fast computation speed for large size NP-hard problems [Dai et al. (2017)]. However, this advantage
disappears for Q-learning methods when faced with large action spaces [Lillicrap et al. (2015)]. For
multi-robot/machine scheduling problems, the set of all multi-robot assignments at each decision
epoch is the action space; it grows exponentially as the number of robots and tasks increases. As
such, the computational requirement of the argmaxa Q(stk , atk) operation increases exponentially.
atk	k k
In this section, we demonstrate how order transferability of Q-function estimation enables us to design
a polynomial-time algorithm with a provable performance guarantee (1 - 1/e optimality) to substitute
for the argmax operation. We call this algorithm an order transferability-enabled auction-based
policy (OTAP) and denote it as πQθ , where the Qθ indicates that the Q-function estimator with current
parameter θ is used during the auction.
4.2.1	Order transferability-enabled auction-based policy (OTAP)
We continue to use the notation introduced in section 4.1. Recall that state stk = (Gtk , αtk) where
Gtk = (R ∪ Ttk , EtRkT). OTAP finds an assignment at, the edge set of a maximal bipartite matching
in the bipartite graph Gtk, after N = max (|R|, |Tt|) iterations of Bidding and Consensus phases.
Bidding phase. In the nth bidding phase, initially all robots know M(θn-1), the ordered set of n - 1
robot-task edges in EtRT determined by the previous n - 1 iterations. An unassigned robot i ignores
all others unassigned and calculates Qθn(stk , M(θn-1) ∪
{iRpT }) for each unassigned task p as if
those k robots (robot i together with all robots assigned tasks in the previous n - 1 iterations) only
exist in the future and will serve all remaining tasks. (Here, iRpT ∈ EtRkT is the edge corresponding
to assigning robot i to task j at decision epoch tk .) If task ` has the highest value, robot i bids
{eRT ,Qθ(st, Mθn-D ∪{ERT})} to the centralized auctioneer. Since the number of ignored robots
varies at each iteration, transferability of Q-function inference is crucial.
Consensus phase. At nth consensus phase, the centralized auctioneer finds the bid with the best bid
value, say {erT* ,Qθ(st, M；n-1) ∪{erT* })}. (Here i* and p* denote the best robot task pair.) Denote
ERT* =: m；n). The centralized auctioneer updates the shared ordered set M；n) = M；n-1) ∪ m；n).
These two phases iterate until we reach M(θN) = {m(θ1), . . . , m(θN)}. This M(θN) is chosen as the
joint assignment aζ at time step tk. That is, ∏Qθ (Stk) = a⅛ ∙ The computational complexity for
computing πQθ is O (|R| |Ttk |) and is only polynomial (See Appendix A.8.1).
Provable performance bound of OTAP.
Let the true Q-functions for OTAP be {Qn}nN=1. Denote the outcome of OTAP with these true
Q-functions as M(N) = {m(1), . . . , m(N)}.
Lemma 2. Ifthe Q-function approximator has order transferability, then M(N) = M；N).
For any decision epoch tk , let M denote a set of robot-task pairs (a subset of EtRkT). For any robot-
task pair m ∈ ERT, define ∆(m | M) := QlM∪{m}l (Stk, M ∪ {m}) - QMI (Stk, M) as the the
marginal value (under the true Q-functions) of adding robot-task pair m ∈ EtRT . Note, we allow
“adding” m ∈ M for mathematical convenience in the subsequent proof. In that case, we have
∆(m | M) = 0, m ∈ M.
Theorem 2 Suppose that the Q-function approximation with the parameter value θ exhibits order
transferability. Denote M；N) as the result of OTAP using {Qn}N=1 and let M* = argmax。=
6
Under review as a conference paper at ICLR 2021
Q|atk | (stk , atk). If ∆(m | M) ≥ 0, ∀M ⊂ EtRkT, ∀m ∈ EtRkT, and the marginal value of adding one
robot diminishes as the number of robots increases, i.e., ∆(m | M) ≤ ∆(m | N), ∀N ⊂ M ⊂ EtRT,
∀m ∈ EtRT, then the result of OTAP is at least better than 1 - 1/e of an optimal assignment. That is,
QN(Stk, MθN))≥QlM*1 (Stk, M*)(1 - 1/e).
For proofs of Lemma 2 and Theorem 2, see Appendix A.7 and A.8.
4.2.2 Auction-fitted Q-iteration framework and exploration
Auction-fitted Q-iteration. We incorporate OTAP into a fitted Q-iteration, i.e., we find θ that
empirically minimizes E∏qθ ,s%+ι〜p，[Qθ (Sk,aQ - [r (s®, a®) + γQθ (sk+ι,∏Q° (sk+ι))]]. Please
note that this method’s rigorous fixed point analysis is the scope of subsequent future research.
Exploration. How can we conduct exploration in the auction-fitted Q-iteration framework? Un-
fortunately, we cannot use an -greedy method since: (i) an arbitrary random deviation in a joint
assignment often induces a catastrophic failure [Maffioli (1986)], and (ii) the joint assignment space,
which is complex and combinatorial, is difficult to explore efficiently with such an arbitrary random
exploration policy. In learning the parameters θ for Qθ (Sk, ak), we use the exploration strategy that
perturbs the parameters θ randomly to actively explore the joint assignment space with TAP. While
this method was originally developed for policy-gradient based methods [Plappert et al. (2017)],
exploration in parameter space is useful in our auction-fitted Q-iteration since it generates a reasonable
combination of assignments.
5	Experiments and results
We focus on DTDS MRRC problem in the main paper and now elaborate upon our reasoning. As
discussed in section 2, the formulation of MRRC problem assumes that task completion times are
given as prior knowledge. Recall that in stochastic environment, task completion times are given
as random variables or sets of samples. In a simulation experiment standpoint, one must generate
such a dataset of task completion times before she can discuss algorithms to solve MRRC problems.
However, it is extremely difficult to generate a reasonable distribution of task completion times
under continuous state continuous time (CTCS) environment [Bertsekas (2014); Omidshafiei et al.
(2017)]. For example, finding an optimal control for stochastic routing problems under a CTCS
environment is in general intractable unless you discretize the space and time so that you transform
CTCS environment to get an approximate DTDS environment [Kushner & Dupuis (2013)].
Despite this difficulty, stochastic environment experiment is important since one of the main benefits
of learning-based heuristics is its capability to tractably solve stochastic scheduling problems [Rossi
et al. (2018)]. Therefore, in this paper, we focus on experiments under DTDS environment and
target to show that our algorithm’s performance for deterministic environments extends to stochastic
environments. Since there is no standard dataset for MRRC problems, we deliberately created a
grid-world environment that generates nontrivial task completion time distributions with minimizing
the selection bias. The idea we took was to use a complex maze (see Figure 3) generator of Neller et al.
(2010) (code provided in Appendix 10) and compare it with the baselines. (For CTCS environment
experiment under deterministic environment, refer IPMS experiments (Appendix A.1)). We avoided
over-fitting by randomly generating a new maze for every training and testing experiment with initial
task/robot locations also randomly chosen, only fixing the problem size while doing that.
To generate the task completion times, Dijkstra’s algorithm and dynamic programming were used
for deterministic and stochastic environments, respectively. To minimize artificiality, the simplest
MRRC problem is considered as follows. In the deterministic environment, robots always succeed
in their movement. In the stochastic environment, a robot makes its intended move with a certain
probability. (Cells with a dot: success with 55%, every other direction with 15% each. Cells without
a dot: 70% and 10%, respectively.) A task is considered served when a robot reaches it. We consider
two reward rules: linearly decaying rewards f(age) = max{200 - age, 0} and nonlinearly decaying
rewards f (age) = λage with λ = 0.99, where age is the task age when served. The initial age of tasks
are uniformly distributed in the interval [0, 100]. Throughout, the performance measure used is ρ =
(%rewards collected by the proposed method/reward collected by the baseline). The baselines are:
• %Optimal: Gurobi was used for problems with the deterministic environment and linear rewards.
Gurobi Optimization (2019) was allowed a 60-min time limit to search for an optimal solution.
7
Under review as a conference paper at ICLR 2021
•	Ekici et al: For deterministic environments with linear rewards, an up-to-date, fast heuristic for
MRRC (Ekici & Retharekar (2013)) was used (it claims 93% optimality for 50 tasks and 4 robots).
•	Sequential Greedy Algorithm (SGA): To our knowledge, there is no literature addressing MRRC
with stochastic environments or exponential rewards. Instead, we construct an indirect baseline using
a general-purpose multi-robot task allocation algorithm called SGA (Han-Lim Choi et al. (2009)). We
will provide our performance divided by SGA performance as %SGA. We will see that the %SGA in
the deterministic linear-reward case is maintained for other cases.
Performance test. We tested the performance under four environments: deterministic/linear rewards,
deterministic/nonlinear rewards, stochastic/linear rewards, stochastic/nonlinear rewards. See Table 1.
For linear/deterministic rewards, our method achieves near-optimality with 3% fewer rewards than
optimal on average. The standard deviation for ρ is provided in parentheses. For others, we see that
the %SGA ratio for linear/deterministic is well maintained in stochastic or nonlinear environments.
Due to dynamic programming computation complexity of dataset generation, we only consider 8
robots/50 tasks at maximum. Larger size problems were considered in IPMS experiments.
Table 1: Performance test (50 trials of training for each cases)
Reward	Environment	Baseline	Testing size : Robot (R) / Task (T)						
			2R/20T	3R/20T	3R/30T	5R/30T	5R/40T	8R/40T	8R/50T
	Deterministic	%Optimal (GUrobi 60min)	98.31	97.50	97.80	95.35	96.99	96.11	96.85
Linear		%Ekisi et al.	99.86-	97.50	-^118.33^^	110.42	105.14	104.63	120.16
		%SGA	-1373-	-120.6	129.7	110.4	123.0	119.9	119.8
	Stochastic	%SGA	-130.9-	-115.7	122.8	115.6	122.3	113.3	115.9
Nonlinear	Deterministic	%SGA	-111.5~	-118.1	118.0	110.9	118.7	111.2	112.6
	Stochastic	%SGA	110.8	117.4	119.7	111.9	120.0	110.4	112.4
Transferability test. Table 2 shows comprehensive transferability test results. The rows indicate
training conditions, while the columns indicate testing conditions. The results in the diagonal cells in
red (cells with the same training size and testing size) serve as baselines (direct testing). The results
in the off-diagonal show the results for the transferability testing, and demonstrate how the algorithms
trained with different problem size perform well on test problems. We can see that lower-direction
transfer tests (trained with larger size problems and tested with smaller size problems) show only a
small loss in performance. For upper-direction transfer tests (trained with smaller size problems and
tested with larger size problems), the performance loss was up 4 percent.
Scalability analysis. For scalability considerations, including computational analysis of OTAP and
training data complexity, see Appendix A.8.1.
Table 2: Transferability test (50 trials of training for each cases, linear & deterministic env.)
	TeSting SiZe			: Robot (R) / Task (T)			
Training size (Robot(R)/Task(T))	2R/20T	3R/20T	3R/30T	5R/30T	5R/40T	8R/40T	8R/50T
2R/20T	98.31	93.61	97.31	92.16	92.83	90.94	93.44
3R/20T	95.98	97.50	96.11	93.64	91.75	91.60	92.77
3R/30T	94.16	96.17	97.80	94.79	93.19	93.14	93.28
5R/30T	97.83	94.89	96.43	95.35	93.28	92.63	92.40
5R/40T	97.39	94.69	95.22	93.15	96.99	94.96	93.65
8R/40T	95.44	94.43	93.48	93.93	96.41	96.11	95.24
8R/50T	95.69	96.68	97.35	94.02	94.50	94.86	96.85
6	Concluding Remarks
We developed a theory of random PGM-based mean-field inference method and provided a theoretical
justification for a simple modification of popular GNN methods to embed a random graph. This
theory was motivated from addressing the challenge of developing a near-optimal learning-based
algorithm for solving NP-hard multi-robot/machine scheduling problems. While precise inference
of Q-function is required to address this challenge, the two-layer random structure2vec embedding
procedure we suggested has shown an empirical success. We further address inscalability problem of
Q-learning methods for multi-robot/machine scheduling problem by suggesting a polynomial-time
assignment algorithm with a provable performance guarantee.
8
Under review as a conference paper at ICLR 2021
References
Dimitri P Bertsekas. Approximate dynamic programming. 2014.
Hanjun Dai, Bo Dai, and Le Song. Discriminative Embeddings of Latent Variable Models for
StrUctUredData. 48:1-23,2016. doi: 1603.05629.
Hanjun Dai, Elias B Khalil, Yuyu Zhang, Bistra Dilkina, and Le Song. Learning Combinatorial
Optimization Algorithms over Graphs. (Nips), 2017.
Ali Ekici and Anand Retharekar. MUltiple agents maximUm collection problem with time dependent
rewards. Computers and Industrial Engineering, 64(4):1009-1018, 2013. ISSN 03608352. doi: 10.
1016/j.cie.2013.01.010. URL http://dx.doi.org/10.1016/j.cie.2013.01.010.
Michael J. Fischer, Nancy A. Lynch, and Michael S. Paterson. Impossibility of distribUted consensUs
with one faUlty process. Journal of the ACM (JACM), 32(2):374-382, apr 1985. ISSN 0004-5411.
doi: 10.1145/3149.214121. URL http://dl.acm.org/doi/10.1145/3149.214121.
Google. Google OR-Tools, 2012. URL https://developers.google.com/
optimization/.
LLC GUrobi Optimization. GUrobi optimizer reference manUal, 2019. URL http://www.gurobi.
com.
Han-Lim Choi, LUc BrUnet, and J.P. How. ConsensUs-Based Decentralized AUctions for RobUst Task
Allocation. IEEE Transactions on Robotics, 25(4):912-926, aUg 2009. ISSN 1552-3098. doi:
10.1109/TRO.2009.2022423.
Thomas N. Kipf and Max Welling. Semi-sUpervised classification with graph convolUtional networks,
Feb 2017. URL https://arxiv.org/abs/1609.02907.
Daphne Koller and Nir Friedman. Probabilistic graphical models : principles and techniques, page
449-453. The MIT Press, 1st edition, 2009. ISBN 9780262013192.
WoUter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve roUting problems! arXiv
preprint arXiv:1803.08475, 2018.
M E KUrz, R G Askin, M E KUrzy, and R G Askiny. HeUristic schedUling of parallel machines
with seqUence-dependent set-Up times. International Journal of Production Research, 39(16):
3747-3769, 2001. ISSN 0020-7543. doi: 10.1080/00207540110064938.
Harold KUshner and PaUl G DUpUis. Numerical methods for stochastic control problems in continuous
time, volUme 24. Springer Science & BUsiness Media, 2013.
Timothy P Lillicrap, Jonathan J HUnt, Alexander Pritzel, Nicolas Heess, Tom Erez, YUval Tassa,
David Silver, and Daan Wierstra. ContinUoUs control with deep reinforcement learning. sep 2015.
Francesco Maffioli. Randomized algorithms in combinatorial optimization: A sUrvey. Dis-
crete Applied Mathematics, 14(2):157 - 170, 1986. ISSN 0166-218X. doi: https://doi.org/
10.1016/0166-218X(86)90058-2. URL http://www.sciencedirect.com/science/
article/pii/0166218X86900582.
Mohammadreza Nazari, Afshin Oroojlooy, Lawrence V. Snyder, and Martin Takac. Reinforcement
Learning for Solving the Vehicle RoUting Problem. feb 2018.
Todd Neller, John DeNero, Dan Klein, Sven Koenig, William Yeoh, Xiaoming Zheng, Kenny
Daniel, Alex Nash, Zachary Dodds, GiUseppe Carenini, David Poole, and Chris Brooks. Model
AI Assignments. Proceedings of the Twenty-Fourth AAAI Conference on Artificial Intelligence
(AAAI-10), pp. 1919-1921, 2010.
Shayegan Omidshafiei, Ali-Akbar Agha-Mohammadi, Christopher Amato, Shih-YUan LiU,
Jonathan P How, and John Vian. Decentralized control of mUlti-robot partially observable Markov
decision processes Using belief space macro-actions. The International Journal of Robotics
Research, 36(2):231-258, 2017. doi: 10.1177/0278364917692864.
9
Under review as a conference paper at ICLR 2021
Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor, Richard Y. Chen, Xi Chen,
Tamim Asfour, Pieter Abbeel, and Marcin Andrychowicz. Parameter Space Noise for Exploration.
pp.1-18, 2017.
Federico Rossi, Saptarshi Bandyopadhyay, Michael Wolf, and Marco Pavone. Review of Multi-Agent
Algorithms for Collective Behavior: a Structural Taxonomy. IFAC-PapersOnLine, 51(12):112-117,
2018. ISSN 24058963. doi: 10.1016/j.ifacol.2018.07.097.
Hado van Hasselt, Arthur Guez, and David Silver. Deep Reinforcement Learning with Double
Q-learning. 2015.
A Appendix
A. 1 Identical parallel machine scheduling problem (IPMS) with makespan
MINIMIZATION OBJECTIVE
A.1.1 Formulation
IPMS is a problem defined in continuous state/continuous time space. Once service of a task i begins,
it requires a deterministic duration of time τi for a machine to complete - we call this the processing
time. Machines are all identical, which means processing time of each tasks among machines are all
the same. Processing times of each tasks are all different. Before a machine can start processing a
task, it is required to first setup for the task. In this paper, we discuss IPMS with ‘sequence-dependent
setup times’. In this case, a machine must conduct a setup prior to serving each task. The duration of
this setup depends on the current task i and the task j that was previously served on that machine
- we call this the setup time. The completion time for each task is thus the sum of the setup time
and processing time. Under this setting, we solve the IPMS problem for make-span minimization as
discussed in [Kurz et al. (2001)]. That is, we seek to minimize the total time spent from the start time
to the completion of the last task. IPMS problem’s sequential decision making problem formulation
resembles that of MRRC with continuous-time and continuous-space. That is, every time there is
a finished task, we make assignment decision for a free machine. We call this times as ‘decision
epochs’ and express them as an ordered set (t1, t2, . . . , tk, . . . ). Abusing this notation slightly, we
use (∙)tk = (∙)k. This problem can be cast as a Markov Decision Problem (MDP) whose state, action,
and reward are defined as follows:
State. The state stk at time tk is represented as Gtk , WtRT , tk . Gt is a directed bipartite graph
(R ∪ Ttk, EtRT) where R is the set of all machines, Ttk is the set of all remaining unserved tasks at
time step tk. The set EtRkT consists of all directed edges from machines to unserved tasks at time tk.
To each edge is associated a weight equal to the task completion time. Let WtRT denote the set of all
such weights for all edges at tk (either constants or random variables and restricted to multiples of
∆ in the the DTDS system). For example, iR,pT ∈ EtRkT is an edge indicating machine i is assigned
to serve task p. To this edge a random variable denoting the task completion time (a duration) is
assigned. Each task is given an initial age which increases linearly with time (a multiple of ∆ for
DTDS). Let αtk = {ηtpk ∈ R|p ∈ Ttk } denote the set of ages where ηtpk indicates the age of task p at
time-step tk. We denote the set of possible states as S.
Action. Defined the same as MRRC with continuous state/time space.
Reward. Let’s denote the time between decision epoch k and decision epoch k+ 1 as Tk = tk - tk-1.
One can easily see that Tk is completely determined by sk, ak and sk+1. Therefore, we can denote
the reward we get with sk, ak and sk+1 as T (sk, ak, sk+1).
Transition probabilities. The transition probability P 0 is defined the same as MRRC problem.
Objective. We can now define an assignment policy φ as a function that maps a state sk to action ak.
Given s0 initial state, an IPMS problem with makespan minimization objective can be expressed as a
problem of finding an optimal assignment policy φ* such that
∞
φ* = argmin E∏,po ET (Sk,ak,Sk+ι) ∣so .
10
Under review as a conference paper at ICLR 2021
Table 3: IPMS test results for makespan minimization with deterministic task completion time (our
algorithm / best Google OR tool result)
Makespan minimization		# Machines			
		3	5	7	10
	50	106.7%	117.0%	119.8%	116.7%
# Tasks	~y5~	105.2%	109.6%	113.9%	111.3%
	"T00^	100.7%	111.0%	109.1%	109.0%
A.1.2 Experiments
For IPMS, we test it with continuous time, continuous state environment. While there have been many
learning-based methods proposed for (single) robot scheduling problems, to the best our knowledge
our method is the first learning method to claim scalable performance among machine-scheduling
problems. Hence, in this case, we focus on showing comparable performance for large problems,
instead of attempting to show the superiority of our method compared with heuristics specifically
designed for IPMS (actually no heuristic was specifically designed to solve our exact problem
(makespan minimization, sequence-dependent setup with no restriction on setup times))
For each task, processing times is determined using uniform [16, 64]. For every (task i, task j)
ordered pair, a unique setup time is determined using uniform [0, 32]. As illustrated in Appendix
A.1, we want to minimize make-span. As a benchmark for IPMS, we use Google OR-Tools library
Google (2012). This library provides metaheuristics such as Greedy Descent, Guided Local Search,
Simulated Annealing, Tabu Search. We compare our algorithm’s result with the heuristic with the
best result for each experiment. We consider cases with 3, 5, 7, 10 machines and 50, 75, 100 jobs.
The results are provided in Appendix Table 3. Makespan obtained by our method divided by the
makespan obtained in the baseline is provided. Although our method has limitations in problems
with a small number of tasks, it shows comparable performance to a large number of tasks and shows
its value as the first learning-based machine scheduling method that achieves scalable performance.
Figure 4: Representing MRRC as a random Bayesian Network
A.2 Bayesian Network representation
Here we illustrate that robot scheduling problem randomly induces a random Bayesian Network from
state st. See figure 4. Given starting state st and action at, a person can repeat a random experiment of
“sequential decision making using policy φ”. In this random experiment, We can define events 'How
robots serve all remaining tasks in which sequence’. We call such an event a ‘scenario’. For example,
suppose that at time-step t we are given robots {A, B}, tasks {1, 2, 3, 4, 5}, and policy φ. One
possible scenario S * can be {robot A serves task 3 → 1 → 2 and robot B serves task 5 → 4}. Define
random variable {{Hj} a task characteristic, e.g. ‘The time when task k is serviced’. The question
is, ‘Given a scenario S*, what is the relationship among random variables {Hk}’ {yk} (inputs in
section 4.1)? Recall that in our sequential decision making formulation we are given all the ‘task
completion time’ information in the st description. Note that, task completion time is only dependent
11
Under review as a conference paper at ICLR 2021
on the previous task and assigned task. In our example above, under scenario S * 'when task 2 is
served’ is only dependent on ‘when task 1 is served’. That is, P(H2∖H1,H3, S*) = P(H2∣H1, S*).
This relationship is called ‘conditional independence’. Given a scenario S*, every relationship
among {Hi|S*} can be expressed using this kind of relationship among random variables. A
graph with this special relationship is called ‘Bayesian Network’ [Koller & Friedman (2009)], a
probabilistic graphical model. Therefore, under a fixed scenario S*, this problem’s joint distribution
can be assumed to be factored as PGM structure Qk φ (hk|yk) Qi,j φ (hi|hj) where yk is the inputs
considered in section 4.1 and Hi denoting the time task i is served.
A.3 Proof of Theorem 1.
We first define necessary definitions for our proof. Given a random PGM {GX , P}, a PGM is chosen
among GX , the set of all possible PGMs on X . The set of semi-cliques is denoted as CX . As
discussed in the main text, if we are given P then we can easily calculate the presence probability pm
of semi-clique Dm as pm = PG∈GX P(G)1Dm∈G.
For each semi-clique Di in CX , define a binary random variable V i : F 7→ {0, 1} with value 0
for the factorization that does not include semi-clique Di and value 1 for the factorization that
include semi-clique Di. Let V be a random vector V = V 1, V 2, . . . , V |CX| . Then we can express
P(Xι,...,Xn∣V) (X Qi=X | [φi (Di)] V i. We denote [φi (Di)] V i as ψ(Di).
Now we prove Theorem 1.
In mean-field inference, we want to find a distribution Q (X1, . . . , Xn) = Qin=1 Qi(Xi) such that
the cross-entropy between it and a target distribution is minimized. Following the notation in Koller
& Friedman (2009), the mean field inference problem can written as the following optimization
problem.
min
Q
s.t.
D	Qi|P(X1,...,Xn|V))
X Qi (xi) = 1 ∀i
xi
Here D ( i Qi | P (X1, . . . , Xn|V)) can be expressed as D ( i Qi | P (X1, . . . , Xn|V))
EQ[ln(QiQi)] -EQ[ln(P(X1,...,Xn|V))].
12
Under review as a conference paper at ICLR 2021
Note that
Eq [ln (P (Xi,…,Xn|V))] = EQ ln (Ini=Xlψi (Di, V)
=EQ ln(1 Yl ψi (Di,V))j
"1Cx l	]
=Eq X Viln (Φi (Di))l — EqE(Z)]
|CX|
=X Eq [Viln (φi (Di))] - EQ[ln(Z)]
i=1
|CX|
=X EVi [Eq [Viln (φi (Di)) |Vi]] — EqE(Z)]
i=1
|CX|
=X P(Vi = 1) [Eq [ln (φi (Di))]] — EQ[ln(Z)]
i=1
|CX|
=Xpi [Eq [ln (φi (Di))]] — EqE(Z)].
i=1
Hence, the above optimization problem can be written as
max
Q
|CX|	n
Eq X Pi ln (φi (Di)) + EQ X (ln Qi)
i=1	i=1
(1)
s.t.	Qi (xi) = 1	∀i
In Koller & Friedman (2009), the fixed point equation is derived by solving an analogous equation to
(1) without the presence of the pi . Theorem 1 follows by proceeding as in Koller & Friedman (2009)
with straightforward accounting for pi .
A.4 Proof of Lemma 1.
Since we assume semi-cliques are only between two random variables, we can denote CX = {Dij }
and presence probabilities as {pij} where i, j are node indexes. Denote the set of nodes as V.
From here, we follow the approach of Dai et al. (2016) and assume that the joint distribution of
random variables can be written as
P ({Hk} ,{Xk}) X Y ψi (Hk∣Xk) Y ψi (Hk∣Hi).
k∈V	k,i∈V
Expanding the fixed-point equation for the mean field inference from Theorem 1, we obtain:
Qk (hk) =
Zrexp 1	X	E(Di-{Hk })~Q [lnψi(Hk = hk∣Di)] }
k	(ψi:Hk ∈Di
=-71exp{lnφ (Hk = hk |xk) +
Zk
X	PkiQi(hi)lnφ(Hk = hk|Hi)dhi}.
13
Under review as a conference paper at ICLR 2021
This fixed-point equation for Qk (hk) is a function of {Qj (hj )}j6=k such that
Qk (hk) = f hk,xk, {pkj Qj (hj)}j6=k .
As in Dai et al. (2016), this equation can be expressed as a Hilbert space embedding of the form
μk = T ◦ (xk, {pkjμj }j=i),
where μk indicates a vector that encodes Qk (hk). In this paper, We use the nonlinear mapping T
(based on a neural network form ) suggested in Dai et al. (2016):
μk = σ WiXk + W2 Epkjμj
j6=k
A.5 Simple presence probability inference method used for MRRC
Denote ages of task i, j as agei, agej. Note that if we generate M samples of ij as{eikj}kM=1, then
MM PM=I f (ej, age%, agej) is an unbiased and consistent estimator of E[f (Eij, age%, agej)]. The
corresponding neural network-based inference is as follows: for each sample k, for each task i and
task j, we form a vector of Uj = (ej, age%, agej-) and compute gj = PM=I MW1(relu(W2Uj).
We obtain {pij } from {gij } using softmax.
The pseducode implementation is as follows: In lines 1 and 2, the likelihood of the existence of a
directed edge from each node m to node n is computed by calculating W1 relU W2Ukmn and
averaging over the M samples. In lines 3 and 4, we use the soft-max function to obtain pm,n .
1	For m, n ∈ V do
2	gmn = MM PM=I WI (relu ”2成八))
3	For m, n ∈ V do
egmn /τ
4	pm,n = Pjev egmn/T .
A.6 Complete algorithm of section 4.1 with task completion time as a random
VARIABLE
We combine random sampling and inference procedure suggested in section and Figure 3. Denote the
set of task with a robot assigned to it as TA. Denote a task in TA as t% and the robot assigned to t%
as rti. The corresponding edge in ERT for this assignment is Ert ti. The key idea is to use samples
of Ert ti to generate N number of sampled Q(s, a) value and average them to get the estimate of
E(Q(s, a)). First, for l = 1 . . . N we conduct the following procedure. For each task t% in TA, we
sample one data el 2 3 4 5 6 7rt ti . Using those samples and {p%j }, we follow the whole procedure illustrated
in section 4.1 to get Q(s, a)l. Second, we get the average of {Q(s, a)l}ll==1N to get the estimate of
E(Q(s, a)), N Pl=N Q(s, a)l.
The complete algorithm of section 4.1 with task completion time as a random variable is given as
below.
1 age% = age of node i
2 The set of nodes for assigned tasks ≡ TA
3 Initialize {μ(0)}, {γ(0)}
4 for l = 1 to N :
5	for t% ∈ T:
5	if t% ∈ TA do:
6	sample elrt ti from Ert ti
7	x% = elrti ti
9 else: x% = 0
10 for t = 1 to T1 do
11 for i ∈ V do
14
Under review as a conference paper at ICLR 2021
12
13
14
15
16
17
18
19
20
Ii = Pj∈v Pji μ(t-1)
μ(t) = relu (W3li + W4Xi)
eι = Concatenate ("”,age)
for t = 1 to T2 do
for i ∈ V do
li = Pj∈Vpjiγj(t-1)
Yjt = relu (W5li + W6μi)
Ql = W7 Pi∈V γi(T)
N PN=ι Qi
Qavg
A.7 Proof of Lemma 2
Statement: Denote result of OTAP using true Q-functions {Q(n)} as M(N) = {m(1) . . . m(N)}. If
Q-function approximation method has order transferability, then M(N) = M(θN) holds.
Proof. Recall that we say Q-function approximation method has order transferability if
argmaxa Qn (stk, atk) = argmaxa Qθn (stk, atk). We prove by induction.
tk	tk
Base case: For n = 0, M(0) = φ = M(θ0).
For n > 0, suppose that M(n) = M(θn) holds, i.e. m(j) = m(θj) for 1 ≤ j ≤ n. Then according to
n + 1th step OTEP operation,
m(n+1t = argmaXm Qn+1 (stfc, M(n) ∪ {m})
=argmaxm Qn+1 (Stk, M(n) ∪ {m}) (v Order transferability assumption)
=argmaXm Qn+1 (Stk, Mθnt ∪ {m}) (v induction argument)
(n+1)
= mθ	.
Therefore, M(n+1) = M(n) ∪ {m(n+1)} = M(θn) ∪ {m(θn+1)} = M(θn+1).
A.8 Proof of Theorem 2
Statement: Denote N = max (|R|, |Tt|).
Suppose that Q-function approximation method has order transferability. Denote M(θN) as
the result of OTAP using {Qn} and M* as argmaXa乜 Q (Stk,atk)∙ If 1) the marginal value
of adding one robot is positive, i.e. Q|M|+1 (Stk, M ∪ {m}) - Q|M| (Stk, M) ≥ 0 for all
M ⊂ EtRT and 2) the marginal value of adding one robot diminishes as the robot number increases,
i.e., Q|M|+1(Stk,M ∪ {m}) - Q|M|(Stk,M) ≤ Q|N|+1(Stk,N ∪ {m}) - Q|N|(Stk,N) for
N ⊂ M ⊂ EtRT, for all m ∈ EtRT, then the result of OTAP is at least better than 1 - 1/e of optimal
assignment, i.e., QN(Stk, MθN)) ≥ Q|M*1 (Stk,M*)(1 - 1/e) ∙
Proof. From the assumption 1) that the marginal value of adding one robot is nonnegative, without
loss of generality, We can consider M* with |M*| = N in the further proof procedure. Denote
M*={m⑴",m⑵*,..., m(n)*} and denote M；N) = {m；1),m；2),..., m；N)}.
For notation simplicity, define ∆(m | M) =: QlM∪{m}l (St, M ∪ {m}) — QMI (St, M).
Then the optimal value OPT = QN(Stk, M*) ≤ OMn'M"⑶忆,M，’ ∪M*)
=Qn(Stk, Mθn)) + PN=1 ∆(mj)* | Mθn) ∪{m(1)*,…，mjτ)*})
≤ Qn(Stk , M(θn)) + PjN=1 ∆(m(j)* | M(θn))(v condition 2 - decreasing marginal value condition)
≤ Qn(Stk, M(θn)) + PjN=1 ∆(m(θn+1) | M(θn))
(v OTAP chooses m(θn+1) = argmaXm Qθn+1 St, M(θn) ∪ {m} and
argmaXm Qθn+1 St, M(θn) ∪ {m} = argmaXm Qn St, M(θn) ∪ {m} from Lemma 2)
=Qn(Stk,M(θn))+N∆(m(θn+1) | M(θn)).
15
Under review as a conference paper at ICLR 2021
Therefore, ∆(mθn+1) | M；(n))) ≥ N(OPT - Qn(Stk, MP.
Note that OPT - Qn(stk, M；n)) denotes current iteration (= nth) outcome M；n)’s size of SUb-
optimality compared to OPT. Denote OPT - Qn(stk, M(θn)) =: βn. Then since Q0 (stk, φ) = 0,
βo = OPT. Therefore, We have ∆(mθn+1) | M；(n))) ≥ Nβn
Also, note that ∆(m(θn+1) | M(θn)) = Qn+1(st, M(θn) ∪ {m(θn+1)}) - Qn(st, M(θn))
=Qn+1(st,M(θn+1))-Qn(st,M(θn)) = (OPT-Qn(st,M(θn))-(OPT-Qn+1(st,M(θn+1)))
= βn - βn+1 .
Therefore, βn - βn+ι ≥ N βn, i.e., βn+ι ≤ βn(1 - N).
This implies OPT - Qn(Stk, MgN)) = βN ≤ β0(1 - N)N = OPT(1 - N)n and thus we get
Qn(Stk, MgN)) = OPT(1 - (1 - N)N)〜OPT(1 - 1) as N → ∞.
A.8.1 Scalability analysis
Computational complexity. MRRC can be formulated as a semi-MDP (SMDP) based multi-robot
planning problem (e.g., Omidshafiei et al. (2017)). This problem’s complexity with R robots and
T tasks and maximum H time horizon is O((R!/T !(R - T)!)H). For example, Omidshafiei et al.
(2017) state that a problem with only 13 task completion times (‘TMA nodes’ in their language)
possessed a policy space with cardinality 5.622 * 1017. In our proposed method, this complexity is
addressed by a combination of two complexities: computational complexity and training complexity.
For computational complexity of joint assignment decision at each timestep, it is O(|R||T |3) =
O((1) × (2) × (3) × (4) + (5)) where (1) - (5) are as follows.
(1)	# of Q-function computation required in one time-step = O(|R||T |): Shown in section 4.2
(2)	# of mean-field inference in one Q-function computation = 2 (constant): Two embed-
ding steps (Distance embedding, Value embedding) each needs one mean-field inference
procedure
(3)	# of structure2vec propagation operation in one mean-field inference= O(|T|2): There is
one structure2vec operation from a task to another task and therefore the total number of
operations is |T | × (|T | - 1).
(4)	# of neural net computation for each structure2vec propagation operation=C (constant): This
is only dependent on the hyperparameter size of neural network and does not increase as
number of robots or tasks.
(5)	# of neural net computation for inference of random PGM=O(|T |2) As an offline stage, we
infer the semi-clique presence probability for every possible directed edge, i.e. from a task
to another task using algorithm introduced in Appendix 6. This algorithm complexity is
O(|T| × (|T| - 1)) = O(|T|2).
Training data efficiency. Training efficiency also is required to obtain scalability. To quantify this we
measured the training time required to achieve 93% optimality. As before, we consider a deterministic
environment with linear rewards and compare with the exact optimum. Table 4 demonstrates that
training time many not necessarily increase with problem size.
Table 4: Training complexity (mean of 20 trials of training, linear & deterministic env.)
Linear & Deterministic		Testing Size : Robot (R)/ Task (T)							
	2R/20T	3R/20T	3R/30T	5R/30T	5R/40T	8R/40T	8R/50T
Performance with full training	98.31 ^^	97.50	97:80^^	95.35	96.99	96.11	96.85
# Training for 93 optimality	19261.2	61034.0	^^99032.7	48675.3	48217.5	45360.0	47244.2
A.9 Code for the experiment
For the entire codes used for experiments, please go to the following Google drive link for the codes.
16